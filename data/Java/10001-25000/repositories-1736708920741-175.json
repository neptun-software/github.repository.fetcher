{
  "metadata": {
    "timestamp": 1736708920741,
    "page": 175,
    "hasNextPage": false,
    "endCursor": "Y3Vyc29yOjE3NQ==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "opensearch-project/OpenSearch",
      "stars": 10051,
      "defaultBranch": "main",
      "files": [
        {
          "name": ".ci",
          "type": "tree",
          "content": null
        },
        {
          "name": ".dir-locals.el",
          "type": "blob",
          "size": 3.2626953125,
          "content": "((java-mode\n  .\n  ((eval\n    .\n    (progn\n      (defun my/point-in-defun-declaration-p ()\n        (let ((bod (save-excursion (c-beginning-of-defun)\n                                   (point))))\n          (<= bod\n              (point)\n              (save-excursion (goto-char bod)\n                              (re-search-forward \"{\")\n                              (point)))))\n\n      (defun my/is-string-concatenation-p ()\n        \"Returns true if the previous line is a string concatenation\"\n        (save-excursion\n          (let ((start (point)))\n            (forward-line -1)\n            (if (re-search-forward \" \\\\\\+$\" start t) t nil))))\n\n      (defun my/inside-java-lambda-p ()\n        \"Returns true if point is the first statement inside of a lambda\"\n        (save-excursion\n          (c-beginning-of-statement-1)\n          (let ((start (point)))\n            (forward-line -1)\n            (if (search-forward \" -> {\" start t) t nil))))\n\n      (defun my/trailing-paren-p ()\n        \"Returns true if point is a training paren and semicolon\"\n        (save-excursion\n          (end-of-line)\n          (let ((endpoint (point)))\n            (beginning-of-line)\n            (if (re-search-forward \"[ ]*);$\" endpoint t) t nil))))\n\n      (defun my/prev-line-call-with-no-args-p ()\n        \"Return true if the previous line is a function call with no arguments\"\n        (save-excursion\n          (let ((start (point)))\n            (forward-line -1)\n            (if (re-search-forward \".($\" start t) t nil))))\n\n      (defun my/arglist-cont-nonempty-indentation (arg)\n        (if (my/inside-java-lambda-p)\n            '+\n          (if (my/is-string-concatenation-p)\n              16\n            (unless (my/point-in-defun-declaration-p) '++))))\n\n      (defun my/statement-block-intro (arg)\n        (if (and (c-at-statement-start-p) (my/inside-java-lambda-p)) 0 '+))\n\n      (defun my/block-close (arg)\n        (if (my/inside-java-lambda-p) '- 0))\n\n      (defun my/arglist-close (arg) (if (my/trailing-paren-p) 0 '--))\n\n      (defun my/arglist-intro (arg)\n        (if (my/prev-line-call-with-no-args-p) '++ 0))\n\n      (c-set-offset 'inline-open           0)\n      (c-set-offset 'topmost-intro-cont    '+)\n      (c-set-offset 'statement-block-intro 'my/statement-block-intro)\n      (c-set-offset 'block-close           'my/block-close)\n      (c-set-offset 'knr-argdecl-intro     '+)\n      (c-set-offset 'substatement-open     '+)\n      (c-set-offset 'substatement-label    '+)\n      (c-set-offset 'case-label            '+)\n      (c-set-offset 'label                 '+)\n      (c-set-offset 'statement-case-open   '+)\n      (c-set-offset 'statement-cont        '++)\n      (c-set-offset 'arglist-intro         'my/arglist-intro)\n      (c-set-offset 'arglist-cont-nonempty '(my/arglist-cont-nonempty-indentation c-lineup-arglist))\n      (c-set-offset 'arglist-close         'my/arglist-close)\n      (c-set-offset 'inexpr-class          0)\n      (c-set-offset 'access-label          0)\n      (c-set-offset 'inher-intro           '++)\n      (c-set-offset 'inher-cont            '++)\n      (c-set-offset 'brace-list-intro      '+)\n      (c-set-offset 'func-decl-cont        '++)\n      ))\n   (c-basic-offset . 4)\n   (c-comment-only-line-offset . (0 . 0))\n   (fill-column . 140)\n   (fci-rule-column . 140)\n   (compile-command . \"gradle compileTestJava\"))))\n"
        },
        {
          "name": ".editorconfig",
          "type": "blob",
          "size": 0.3662109375,
          "content": "# EditorConfig: http://editorconfig.org/\n\nroot = true\n\n[*]\ncharset = utf-8\ntrim_trailing_whitespace = true\ninsert_final_newline = true\nindent_style = space\n\n[*.gradle]\nindent_size = 2\n\n[*.groovy]\nindent_size = 4\n\n[*.java]\nindent_size = 4\n\n[*.json]\nindent_size = 2\n\n[*.py]\nindent_size = 2\n\n[*.sh]\nindent_size = 2\n\n[*.{yml,yaml}]\nindent_size = 2\n\n[*.{xsd,xml}]\nindent_size = 4\n"
        },
        {
          "name": ".gitattributes",
          "type": "blob",
          "size": 0.1728515625,
          "content": "* text eol=lf\n*.jar binary\n*.bat binary\n*.zip binary\n*.exe binary\n*.epub binary\n*.pdf binary\n*.vsdx binary\n*.doc binary\n*.bcfks binary\n*.crt binary\n*.p12 binary\n*.txt text=auto\n"
        },
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 1.13671875,
          "content": "\n# intellij files\n.idea/\n*.iml\n*.ipr\n*.iws\nbuild-idea/\nout/\n\n# include shared intellij config\n!.idea/inspectionProfiles/Project_Default.xml\n!.idea/runConfigurations/Debug_OpenSearch.xml\n!.idea/vcs.xml\n!.idea/icon.svg\n\n# These files are generated in the main tree by annotation processors\nbenchmarks/src/main/generated/*\nbenchmarks/bin/*\nbenchmarks/build-eclipse-default/*\nserver/bin/*\nserver/build-eclipse-default/*\ntest/framework/build-eclipse-default/*\n\n# eclipse files\n.project\n.classpath\n.settings\nbuild-eclipse/\n\n# netbeans files\nnb-configuration.xml\nnbactions.xml\n\n# gradle stuff\n.gradle/\nbuild/\n\n# vscode stuff\n.vscode/\n\n# testing stuff\n**/.local*\n.vagrant/\n/logs/\n\n# osx stuff\n.DS_Store\n\n# default folders in which the create_bwc_index.py expects to find old es versions in\n/backwards\n/dev-tools/backwards\n\n# needed in case docs build is run...maybe we can configure doc build to generate files under build?\nhtml_docs\n\n# random old stuff that we should look at the necessity of...\n/tmp/\neclipse-build\n\n# projects using testfixtures\ntestfixtures_shared/\n\n# These are generated from .ci/jobs.t\n.ci/jobs/\n\n# build files generated\ndoc-tools/missing-doclet/bin/"
        },
        {
          "name": ".idea",
          "type": "tree",
          "content": null
        },
        {
          "name": ".lychee.excludes",
          "type": "blob",
          "size": 0.572265625,
          "content": "http://bitbucket.org/jpbarrette/moman/overview/\nhttp://eid-applet.googlecode.com/\nhttp://opensource.adobe.com/wiki/display/cmap/Downloads\nhttp://project.carrot2.org/license.html\nhttp://source.icu-project.org/\nhttp://site.icu-project.org/\nhttp://www.icu-project.org/\nhttp://snapshot/ \nhttp://viewvc.jboss.org/cgi-bin/viewvc.cgi/jbosscache/experimental/jsr166/\nhttp://www.eclipse.org/jetty/downloads.php\nhttp://www.ecma-international.org/publications/files/ECMA-ST/Ecma%20PATENT/Patent%20statements%20ok/ECMA-376%20*\nhttp://www.unicode.org/Public/PROGRAMS/CVTUTF\nhttp://sgjp.pl/morfeusz/\n"
        },
        {
          "name": ".whitesource",
          "type": "blob",
          "size": 0.98828125,
          "content": "{\n  \"scanSettings\": {\n    \"configMode\": \"AUTO\",\n    \"configExternalURL\": \"\",\n    \"projectToken\": \"\",\n    \"baseBranches\": []\n  },\n  \"scanSettingsSAST\": {\n    \"enableScan\": false,\n    \"scanPullRequests\": false,\n    \"incrementalScan\": true,\n    \"baseBranches\": [],\n    \"snippetSize\": 10\n  },\n  \"checkRunSettings\": {\n    \"vulnerableCheckRunConclusionLevel\": \"failure\",\n    \"displayMode\": \"diff\",\n    \"useMendCheckNames\": true\n  },\n  \"checkRunSettingsSAST\": {\n    \"checkRunConclusionLevel\": \"failure\",\n    \"severityThreshold\": \"high\"\n  },\n  \"issueSettings\": {\n    \"minSeverityLevel\": \"LOW\",\n    \"issueType\": \"DEPENDENCY\"\n  },\n  \"issueSettingsSAST\": {\n    \"minSeverityLevel\": \"high\",\n    \"issueType\": \"repo\"\n  },\n  \"remediateSettings\": {\n    \"workflowRules\": {\n      \"enabled\": true\n    }\n  },\n   \"imageSettings\":{\n       \"imageTracing\":{\n           \"enableImageTracingPR\": false,\n           \"addRepositoryCoordinate\": false,\n           \"addDockerfilePath\": false,\n           \"addMendIdentifier\": false\n       }\n   }\n}"
        },
        {
          "name": "ADMINS.md",
          "type": "blob",
          "size": 0.5380859375,
          "content": "## Admins\n\n| Admin           | GitHub ID                               | Affiliation |\n| --------------- | --------------------------------------- | ----------- |\n| Henri Yandell   | [hyandell](https://github.com/hyandell) | Amazon      |\n\n[This document](https://github.com/opensearch-project/.github/blob/main/ADMINS.md) explains what admins do in this repo. and how they should be doing it. If you're interested in becoming a maintainer, see [MAINTAINERS](MAINTAINERS.md). If you're interested in contributing, see [CONTRIBUTING](CONTRIBUTING.md).\n"
        },
        {
          "name": "CHANGELOG-3.0.md",
          "type": "blob",
          "size": 5.0966796875,
          "content": "# CHANGELOG\nAll notable changes to this project are documented in this file.\n\nThe format is based on [Keep a Changelog](https://keepachangelog.com/en/1.0.0/), and this project adheres to [Semantic Versioning](https://semver.org/spec/v2.0.0.html). See the [CONTRIBUTING guide](./CONTRIBUTING.md#Changelog) for instructions on how to add changelog entries.\n\n## [Unreleased 3.0]\n### Added\n- Support for HTTP/2 (server-side) ([#3847](https://github.com/opensearch-project/OpenSearch/pull/3847))\n- Allow mmap to use new JDK-19 preview APIs in Apache Lucene 9.4+ ([#5151](https://github.com/opensearch-project/OpenSearch/pull/5151))\n- Add events correlation engine plugin ([#6854](https://github.com/opensearch-project/OpenSearch/issues/6854))\n- Implement on behalf of token passing for extensions ([#8679](https://github.com/opensearch-project/OpenSearch/pull/8679), [#10664](https://github.com/opensearch-project/OpenSearch/pull/10664))\n- Provide service accounts tokens to extensions ([#9618](https://github.com/opensearch-project/OpenSearch/pull/9618))\n- GHA to verify checklist items completion in PR descriptions ([#10800](https://github.com/opensearch-project/OpenSearch/pull/10800))\n- Allow to pass the list settings through environment variables (like [], [\"a\", \"b\", \"c\"], ...) ([#10625](https://github.com/opensearch-project/OpenSearch/pull/10625))\n- Views, simplify data access and manipulation by providing a virtual layer over one or more indices ([#11957](https://github.com/opensearch-project/OpenSearch/pull/11957))\n\n### Dependencies\n\n### Changed\n- Changed locale provider from COMPAT to CLDR  ([#14345](https://github.com/opensearch-project/OpenSearch/pull/14345))\n- Migrate client transports to Apache HttpClient / Core 5.x ([#4459](https://github.com/opensearch-project/OpenSearch/pull/4459))\n- Change http code on create index API with bad input raising NotXContentException from 500 to 400 ([#4773](https://github.com/opensearch-project/OpenSearch/pull/4773))\n- Improve summary error message for invalid setting updates ([#4792](https://github.com/opensearch-project/OpenSearch/pull/4792))\n- Return 409 Conflict HTTP status instead of 503 on failure to concurrently execute snapshots ([#8986](https://github.com/opensearch-project/OpenSearch/pull/5855))\n- Add task completion count in search backpressure stats API ([#10028](https://github.com/opensearch-project/OpenSearch/pull/10028/))\n- Deprecate CamelCase `PathHierarchy` tokenizer name in favor to lowercase `path_hierarchy` ([#10894](https://github.com/opensearch-project/OpenSearch/pull/10894))\n- Breaking change: Do not request \"search_pipelines\" metrics by default in NodesInfoRequest ([#12497](https://github.com/opensearch-project/OpenSearch/pull/12497))\n\n### Deprecated\n\n### Removed\n- Remove deprecated code to add node name into log pattern of log4j property file ([#4568](https://github.com/opensearch-project/OpenSearch/pull/4568))\n- Unused object and import within TransportClusterAllocationExplainAction ([#4639](https://github.com/opensearch-project/OpenSearch/pull/4639))\n- Remove LegacyESVersion.V_7_0_* and V_7_1_* Constants ([#2768](https://https://github.com/opensearch-project/OpenSearch/pull/2768))\n- Remove LegacyESVersion.V_7_2_ and V_7_3_ Constants ([#4702](https://github.com/opensearch-project/OpenSearch/pull/4702))\n- Always auto release the flood stage block ([#4703](https://github.com/opensearch-project/OpenSearch/pull/4703))\n- Remove LegacyESVersion.V_7_4_ and V_7_5_ Constants ([#4704](https://github.com/opensearch-project/OpenSearch/pull/4704))\n- Remove Legacy Version support from Snapshot/Restore Service ([#4728](https://github.com/opensearch-project/OpenSearch/pull/4728))\n- Remove deprecated serialization logic from pipeline aggs ([#4847](https://github.com/opensearch-project/OpenSearch/pull/4847))\n- Remove unused private methods ([#4926](https://github.com/opensearch-project/OpenSearch/pull/4926))\n- Remove LegacyESVersion.V_7_8_ and V_7_9_ Constants ([#4855](https://github.com/opensearch-project/OpenSearch/pull/4855))\n- Remove LegacyESVersion.V_7_6_ and V_7_7_ Constants ([#4837](https://github.com/opensearch-project/OpenSearch/pull/4837))\n- Remove LegacyESVersion.V_7_10_ Constants ([#5018](https://github.com/opensearch-project/OpenSearch/pull/5018))\n- Remove Version.V_1_ Constants ([#5021](https://github.com/opensearch-project/OpenSearch/pull/5021))\n- Remove custom Map, List and Set collection classes ([#6871](https://github.com/opensearch-project/OpenSearch/pull/6871))\n- Remove `index.store.hybrid.mmap.extensions` setting in favor of `index.store.hybrid.nio.extensions` setting ([#9392](https://github.com/opensearch-project/OpenSearch/pull/9392))\n\n### Fixed\n- Fix 'org.apache.hc.core5.http.ParseException: Invalid protocol version' under JDK 16+ ([#4827](https://github.com/opensearch-project/OpenSearch/pull/4827))\n- Fix compression support for h2c protocol ([#4944](https://github.com/opensearch-project/OpenSearch/pull/4944))\n- Don't over-allocate in HeapBufferedAsyncEntityConsumer in order to consume the response ([#9993](https://github.com/opensearch-project/OpenSearch/pull/9993))\n\n### Security\n\n[Unreleased 3.0]: https://github.com/opensearch-project/OpenSearch/compare/2.x...HEAD\n"
        },
        {
          "name": "CHANGELOG.md",
          "type": "blob",
          "size": 12.3974609375,
          "content": "# CHANGELOG\nAll notable changes to this project are documented in this file.\n\nThe format is based on [Keep a Changelog](https://keepachangelog.com/en/1.0.0/), and this project adheres to [Semantic Versioning](https://semver.org/spec/v2.0.0.html). See the [CONTRIBUTING guide](./CONTRIBUTING.md#Changelog) for instructions on how to add changelog entries.\n\n## [Unreleased 2.x]\n### Added\n- Latency and Memory allocation improvements to Multi Term Aggregation queries ([#14993](https://github.com/opensearch-project/OpenSearch/pull/14993))\n- Add support for restoring from snapshot with search replicas ([#16111](https://github.com/opensearch-project/OpenSearch/pull/16111))\n- Ensure support of the transport-nio by security plugin ([#16474](https://github.com/opensearch-project/OpenSearch/pull/16474))\n- Add logic in master service to optimize performance and retain detailed logging for critical cluster operations. ([#14795](https://github.com/opensearch-project/OpenSearch/pull/14795))\n- Add Setting to adjust the primary constraint weights ([#16471](https://github.com/opensearch-project/OpenSearch/pull/16471))\n- Switch from `buildSrc/version.properties` to Gradle version catalog (`gradle/libs.versions.toml`) to enable dependabot to perform automated upgrades on common libs ([#16284](https://github.com/opensearch-project/OpenSearch/pull/16284))\n- Increase segrep pressure checkpoint default limit to 30 ([#16577](https://github.com/opensearch-project/OpenSearch/pull/16577/files))\n- Add dynamic setting allowing size > 0 requests to be cached in the request cache ([#16483](https://github.com/opensearch-project/OpenSearch/pull/16483))\n- Support installing plugin SNAPSHOTs with SNASPHOT distribution ([#16581](https://github.com/opensearch-project/OpenSearch/pull/16581))\n- Make IndexStoreListener a pluggable interface ([#16583](https://github.com/opensearch-project/OpenSearch/pull/16583))\n- Support for keyword fields in star-tree index ([#16233](https://github.com/opensearch-project/OpenSearch/pull/16233))\n- Add a flag in QueryShardContext to differentiate inner hit query ([#16600](https://github.com/opensearch-project/OpenSearch/pull/16600))\n- Add vertical scaling and SoftReference for snapshot repository data cache ([#16489](https://github.com/opensearch-project/OpenSearch/pull/16489))\n- [Workload Management] Add Workload Management IT ([#16359](https://github.com/opensearch-project/OpenSearch/pull/16359))\n- Support prefix list for remote repository attributes([#16271](https://github.com/opensearch-project/OpenSearch/pull/16271))\n- Add new configuration setting `synonym_analyzer`, to the `synonym` and `synonym_graph` filters, enabling the specification of a custom analyzer for reading the synonym file ([#16488](https://github.com/opensearch-project/OpenSearch/pull/16488)).\n- Add stats for remote publication failure and move download failure stats to remote methods([#16682](https://github.com/opensearch-project/OpenSearch/pull/16682/))\n- Update script supports java.lang.String.sha1() and java.lang.String.sha256() methods ([#16923](https://github.com/opensearch-project/OpenSearch/pull/16923))\n- Added a precaution to handle extreme date values during sorting to prevent `arithmetic_exception: long overflow` ([#16812](https://github.com/opensearch-project/OpenSearch/pull/16812)).\n- Add search replica stats to segment replication stats API ([#16678](https://github.com/opensearch-project/OpenSearch/pull/16678))\n- Introduce a setting to disable download of full cluster state from remote on term mismatch([#16798](https://github.com/opensearch-project/OpenSearch/pull/16798/))\n- Added ability to retrieve value from DocValues in a flat_object filed([#16802](https://github.com/opensearch-project/OpenSearch/pull/16802))\n- Introduce framework for auxiliary transports and an experimental gRPC transport plugin ([#16534](https://github.com/opensearch-project/OpenSearch/pull/16534))\n- Changes to support IP field in star tree indexing([#16641](https://github.com/opensearch-project/OpenSearch/pull/16641/))\n- Support object fields in star-tree index([#16728](https://github.com/opensearch-project/OpenSearch/pull/16728/))\n- Support searching from doc_value using termQueryCaseInsensitive/termQuery in flat_object/keyword field([#16974](https://github.com/opensearch-project/OpenSearch/pull/16974/))\n\n### Dependencies\n- Bump `com.google.cloud:google-cloud-core-http` from 2.23.0 to 2.47.0 ([#16504](https://github.com/opensearch-project/OpenSearch/pull/16504))\n- Bump `google-auth-library-oauth2-http` from 1.7.0 to 1.29.0 in /plugins/repository-gcs ([#16520](https://github.com/opensearch-project/OpenSearch/pull/16520))\n- Bump `com.azure:azure-storage-common` from 12.25.1 to 12.28.0 ([#16521](https://github.com/opensearch-project/OpenSearch/pull/16521), [#16808](https://github.com/opensearch-project/OpenSearch/pull/16808))\n- Bump `com.google.apis:google-api-services-compute` from v1-rev20240407-2.0.0 to v1-rev20241105-2.0.0 ([#16502](https://github.com/opensearch-project/OpenSearch/pull/16502), [#16548](https://github.com/opensearch-project/OpenSearch/pull/16548), [#16613](https://github.com/opensearch-project/OpenSearch/pull/16613))\n- Bump `com.azure:azure-storage-blob` from 12.23.0 to 12.28.1 ([#16501](https://github.com/opensearch-project/OpenSearch/pull/16501))\n- Bump `org.apache.hadoop:hadoop-minicluster` from 3.4.0 to 3.4.1 ([#16550](https://github.com/opensearch-project/OpenSearch/pull/16550))\n- Bump `org.apache.xmlbeans:xmlbeans` from 5.2.1 to 5.3.0 ([#16612](https://github.com/opensearch-project/OpenSearch/pull/16612), [#16854](https://github.com/opensearch-project/OpenSearch/pull/16854))\n- Bump `com.nimbusds:nimbus-jose-jwt` from 9.41.1 to 9.47 ([#16611](https://github.com/opensearch-project/OpenSearch/pull/16611), [#16807](https://github.com/opensearch-project/OpenSearch/pull/16807))\n- Bump `lycheeverse/lychee-action` from 2.0.2 to 2.2.0 ([#16610](https://github.com/opensearch-project/OpenSearch/pull/16610), [#16897](https://github.com/opensearch-project/OpenSearch/pull/16897))\n- Bump `me.champeau.gradle.japicmp` from 0.4.4 to 0.4.5 ([#16614](https://github.com/opensearch-project/OpenSearch/pull/16614))\n- Bump `mockito` from 5.14.1 to 5.14.2, `objenesis` from 3.2 to 3.3 and `bytebuddy` from 1.15.4 to 1.15.10 ([#16655](https://github.com/opensearch-project/OpenSearch/pull/16655))\n- Bump `Netty` from 4.1.114.Final to 4.1.115.Final ([#16661](https://github.com/opensearch-project/OpenSearch/pull/16661))\n- Bump `org.xerial.snappy:snappy-java` from 1.1.10.6 to 1.1.10.7 ([#16665](https://github.com/opensearch-project/OpenSearch/pull/16665))\n- Bump `codecov/codecov-action` from 4 to 5 ([#16667](https://github.com/opensearch-project/OpenSearch/pull/16667))\n- Bump `org.apache.logging.log4j:log4j-core` from 2.24.1 to 2.24.3 ([#16718](https://github.com/opensearch-project/OpenSearch/pull/16718), [#16858](https://github.com/opensearch-project/OpenSearch/pull/16858))\n- Bump `jackson` from 2.17.2 to 2.18.2 ([#16733](https://github.com/opensearch-project/OpenSearch/pull/16733))\n- Bump `ch.qos.logback:logback-classic` from 1.2.13 to 1.5.15 ([#16716](https://github.com/opensearch-project/OpenSearch/pull/16716), [#16898](https://github.com/opensearch-project/OpenSearch/pull/16898))\n- Bump `com.azure:azure-identity` from 1.13.2 to 1.14.2 ([#16778](https://github.com/opensearch-project/OpenSearch/pull/16778))\n- Bump Apache Lucene from 9.12.0 to 9.12.1 ([#16846](https://github.com/opensearch-project/OpenSearch/pull/16846))\n- Bump `com.gradle.develocity` from 3.18.2 to 3.19 ([#16855](https://github.com/opensearch-project/OpenSearch/pull/16855))\n- Bump `org.jline:jline` from 3.27.1 to 3.28.0 ([#16857](https://github.com/opensearch-project/OpenSearch/pull/16857))\n- Bump `com.azure:azure-core` from 1.51.0 to 1.54.1 ([#16856](https://github.com/opensearch-project/OpenSearch/pull/16856))\n- Bump `com.nimbusds:oauth2-oidc-sdk` from 11.19.1 to 11.20.1 ([#16895](https://github.com/opensearch-project/OpenSearch/pull/16895))\n- Bump `com.netflix.nebula.ospackage-base` from 11.10.0 to 11.10.1 ([#16896](https://github.com/opensearch-project/OpenSearch/pull/16896))\n- Bump `com.microsoft.azure:msal4j` from 1.17.2 to 1.18.0 ([#16918](https://github.com/opensearch-project/OpenSearch/pull/16918))\n- Bump `org.apache.commons:commons-text` from 1.12.0 to 1.13.0 ([#16919](https://github.com/opensearch-project/OpenSearch/pull/16919))\n- Bump `ch.qos.logback:logback-core` from 1.5.12 to 1.5.16 ([#16951](https://github.com/opensearch-project/OpenSearch/pull/16951))\n- Bump `com.azure:azure-core-http-netty` from 1.15.5 to 1.15.7 ([#16952](https://github.com/opensearch-project/OpenSearch/pull/16952))\n- Bump `opentelemetry` from 1.41.0 to 1.46.0 ([#16700](https://github.com/opensearch-project/OpenSearch/pull/16700))\n- Bump `opentelemetry-semconv` from 1.27.0-alpha to 1.29.0-alpha ([#16700](https://github.com/opensearch-project/OpenSearch/pull/16700))\n\n### Changed\n- Indexed IP field supports `terms_query` with more than 1025 IP masks [#16391](https://github.com/opensearch-project/OpenSearch/pull/16391)\n- Make entries for dependencies from server/build.gradle to gradle version catalog ([#16707](https://github.com/opensearch-project/OpenSearch/pull/16707))\n- Allow extended plugins to be optional ([#16909](https://github.com/opensearch-project/OpenSearch/pull/16909))\n- Use the correct type to widen the sort fields when merging top docs ([#16881](https://github.com/opensearch-project/OpenSearch/pull/16881))\n- Limit reader writer separation to remote store enabled clusters [#16760](https://github.com/opensearch-project/OpenSearch/pull/16760)\n\n### Deprecated\n- Performing update operation with default pipeline or final pipeline is deprecated ([#16712](https://github.com/opensearch-project/OpenSearch/pull/16712))\n\n### Removed\n\n### Fixed\n- Fix get index settings API doesn't show `number_of_routing_shards` setting when it was explicitly set ([#16294](https://github.com/opensearch-project/OpenSearch/pull/16294))\n- Revert changes to upload remote state manifest using minimum codec version([#16403](https://github.com/opensearch-project/OpenSearch/pull/16403))\n- Ensure index templates are not applied to system indices ([#16418](https://github.com/opensearch-project/OpenSearch/pull/16418))\n- Remove resource usages object from search response headers ([#16532](https://github.com/opensearch-project/OpenSearch/pull/16532))\n- Support retrieving doc values of unsigned long field ([#16543](https://github.com/opensearch-project/OpenSearch/pull/16543))\n- Fix rollover alias supports restored searchable snapshot index([#16483](https://github.com/opensearch-project/OpenSearch/pull/16483))\n- Fix permissions error on scripted query against remote snapshot ([#16544](https://github.com/opensearch-project/OpenSearch/pull/16544))\n- Fix `doc_values` only (`index:false`) IP field searching for masks ([#16628](https://github.com/opensearch-project/OpenSearch/pull/16628))\n- Fix stale cluster state custom file deletion ([#16670](https://github.com/opensearch-project/OpenSearch/pull/16670))\n- [Tiered Caching] Fix bug in cache stats API ([#16560](https://github.com/opensearch-project/OpenSearch/pull/16560))\n- Bound the size of cache in deprecation logger ([16702](https://github.com/opensearch-project/OpenSearch/issues/16702))\n- Ensure consistency of system flag on IndexMetadata after diff is applied ([#16644](https://github.com/opensearch-project/OpenSearch/pull/16644))\n- Skip remote-repositories validations for node-joins when RepositoriesService is not in sync with cluster-state ([#16763](https://github.com/opensearch-project/OpenSearch/pull/16763))\n- Fix case insensitive and escaped query on wildcard ([#16827](https://github.com/opensearch-project/OpenSearch/pull/16827))\n- Fix _list/shards API failing when closed indices are present ([#16606](https://github.com/opensearch-project/OpenSearch/pull/16606))\n- Fix remote shards balance ([#15335](https://github.com/opensearch-project/OpenSearch/pull/15335))\n- Always use `constant_score` query for `match_only_text` field ([#16964](https://github.com/opensearch-project/OpenSearch/pull/16964))\n- Fix Shallow copy snapshot failures on closed index ([#16868](https://github.com/opensearch-project/OpenSearch/pull/16868))\n- Fix multi-value sort for unsigned long ([#16732](https://github.com/opensearch-project/OpenSearch/pull/16732))\n- The `phone-search` analyzer no longer emits the tel/sip prefix, international calling code, extension numbers and unformatted input as a token ([#16993](https://github.com/opensearch-project/OpenSearch/pull/16993))\n\n### Security\n\n[Unreleased 2.x]: https://github.com/opensearch-project/OpenSearch/compare/2.18...2.x\n"
        },
        {
          "name": "CODE_OF_CONDUCT.md",
          "type": "blob",
          "size": 2.203125,
          "content": "\nThis code of conduct applies to all spaces provided by the OpenSource project including in code, documentation, issue trackers, mailing lists, chat channels, wikis, blogs, social media and any other communication channels used by the project.\n\n\n**Our open source communities endeavor to:**\n\n* Be Inclusive: We are committed to being a community where everyone can join and contribute. This means using inclusive and welcoming language.\n* Be Welcoming: We are committed to maintaining a safe space for everyone to be able to contribute.\n* Be Respectful: We are committed to encouraging differing viewpoints, accepting constructive criticism and work collaboratively towards decisions that help the project grow. Disrespectful and unacceptable behavior will not be tolerated.\n* Be Collaborative: We are committed to supporting what is best for our community and users. When we build anything for the benefit of the project, we should document the work we do and communicate to others on how this affects their work.\n\n\n**Our Responsibility. As contributors, members, or bystanders we each individually have the responsibility to behave professionally and respectfully at all times. Disrespectful and unacceptable behaviors include, but are not limited to:**\n\n* The use of violent threats, abusive, discriminatory, or derogatory language;\n* Offensive comments related to gender, gender identity and expression, sexual orientation, disability, mental illness, race, political or religious affiliation;\n* Posting of sexually explicit or violent content;\n* The use of sexualized language and unwelcome sexual attention or advances;\n* Public or private harassment of any kind;\n* Publishing private information, such as physical or electronic address, without permission;\n* Other conduct which could reasonably be considered inappropriate in a professional setting;\n* Advocating for or encouraging any of the above behaviors.\n* Enforcement and Reporting Code of Conduct Issues:\n\nInstances of abusive, harassing, or otherwise unacceptable behavior may be reported. [Contact us](mailto:opensource-codeofconduct@amazon.com). All complaints will be reviewed and investigated and will result in a response that is deemed necessary and appropriate to the circumstances.\n"
        },
        {
          "name": "CONTRIBUTING.md",
          "type": "blob",
          "size": 17.224609375,
          "content": "- [Contributing to OpenSearch](#contributing-to-opensearch)\n- [First Things First](#first-things-first)\n- [Ways to Contribute](#ways-to-contribute)\n    - [Bug Reports](#bug-reports)\n    - [Feature Requests](#feature-requests)\n    - [Documentation Changes](#documentation-changes)\n    - [Contributing Code](#contributing-code)\n- [Developer Certificate of Origin](#developer-certificate-of-origin)\n- [Changelog](#changelog)\n- [Review Process](#review-process)\n    - [Tips for Success](#tips)\n- [Troubleshooting Failing Builds](#troubleshooting-failing-builds)\n\n# Contributing to OpenSearch\n\nOpenSearch is a community project built and maintained by people just like **you**. We're glad you're interested in helping out. There are several different ways you can do it, but before we talk about that, let's talk about how to get started.\n\n## First Things First\n\n1. **When in doubt, open an issue** - For almost any type of contribution, the first step is opening an issue. Even if you think you already know what the solution is, writing down a description of the problem you're trying to solve will help everyone get context when they review your pull request. If it's truly a trivial change (e.g. spelling error), you can skip this step -- but as the subject says, when in doubt, [open an issue](https://github.com/opensearch-project/OpenSearch/issues).\n\n2. **Only submit your own work**  (or work you have sufficient rights to submit) - Please make sure that any code or documentation you submit is your work or you have the rights to submit. We respect the intellectual property rights of others, and as part of contributing, we'll ask you to sign your contribution with a \"Developer Certificate of Origin\" (DCO) that states you have the rights to submit this work and you understand we'll use your contribution. There's more information about this topic in the [DCO section](#developer-certificate-of-origin).\n\n## Ways to Contribute\n\n**Please note:** OpenSearch is a fork of [Elasticsearch 7.10.2](https://github.com/elastic/elasticsearch). If you do find references to Elasticsearch (outside of attributions and copyrights!) please [open an issue](https://github.com/opensearch-project/OpenSearch/issues).\n\n### Bug Reports\n\nUgh! Bugs!\n\nA bug is when software behaves in a way that you didn't expect and the developer didn't intend. To help us understand what's going on, we first want to make sure you're working from the latest version. Please make sure you're testing against the [latest version](https://github.com/opensearch-project/OpenSearch).\n\nOnce you've confirmed that the bug still exists in the latest version, you'll want to check the bug is not something we already know about. A good way to figure this out is to search for your bug on the [open issues GitHub page](https://github.com/opensearch-project/OpenSearch/issues).\n\nIf you've upgraded to the latest version and you can't find it in our open issues list, then you'll need to tell us how to reproduce it. To make the behavior as clear as possible, please provide your steps as `curl` commands which we can copy and paste into a terminal to run it locally, for example:\n\n```sh\n# delete the index\ncurl -X DELETE localhost:9200/test\n\n# insert a document\ncurl -x PUT localhost:9200/test/test/1 -d '{\n \"title\": \"test document\"\n}'\n\n# this should return XXXX but instead returns YYYY\ncurl ....\n```\n\nProvide as much information as you can. You may think that the problem lies with your query, when actually it depends on how your data is indexed. The easier it is for us to recreate your problem, the faster it is likely to be fixed. It is generally always helpful to provide the basic details of your cluster configuration alongside your reproduction steps.\n\n### Feature Requests\n\nIf you've thought of a way that OpenSearch could be better, we want to hear about it. We track feature requests using GitHub, so please feel free to open an issue which describes the feature you would like to see, why you need it, and how it should work. After opening an issue, the fastest way to see your change made is to open a pull request following the requested changes you detailed in your issue. You can learn more about opening a pull request in the [contributing code section](#contributing-code).\n\n### Documentation Changes\n\nIf you would like to contribute to the documentation, please do so in the [documentation-website](https://github.com/opensearch-project/documentation-website) repo. To contribute javadocs, please first check [OpenSearch#221](https://github.com/opensearch-project/OpenSearch/issues/221).\n\n### Contributing Code\n\nAs with other types of contributions, the first step is to [**open an issue on GitHub**](https://github.com/opensearch-project/OpenSearch/issues/new/choose). Opening an issue before you make changes makes sure that someone else isn't already working on that particular problem. It also lets us all work together to find the right approach before you spend a bunch of time on a PR. So again, when in doubt, open an issue.\n\nAdditionally, here are a few guidelines to help you decide whether a particular feature should be included in OpenSearch.\n\n**Is your feature important to most users of OpenSearch?**\n\nIf you believe that a feature is going to fulfill a need for most users of OpenSearch, then it belongs in OpenSearch. However, we don't want every feature built into the core server. If the feature requires additional permissions or brings in extra dependencies it should instead be included as a module in core.\n\n**Is your feature a common dependency across multiple plugins?**\n\nDoes this feature contain functionality that cuts across multiple plugins? If so, this most likely belongs in OpenSearch as a core module or plugin.\n\nOnce you've opened an issue, check out our [Developer Guide](./DEVELOPER_GUIDE.md) for instructions on how to get started.\n\n## Developer Certificate of Origin\n\nOpenSearch is an open source product released under the Apache 2.0 license (see either [the Apache site](https://www.apache.org/licenses/LICENSE-2.0) or the [LICENSE.txt file](./LICENSE.txt)). The Apache 2.0 license allows you to freely use, modify, distribute, and sell your own products that include Apache 2.0 licensed software.\n\nWe respect intellectual property rights of others and we want to make sure all incoming contributions are correctly attributed and licensed. A Developer Certificate of Origin (DCO) is a lightweight mechanism to do that.\n\nThe DCO is a declaration attached to every contribution made by every developer. In the commit message of the contribution, the developer simply adds a `Signed-off-by` statement and thereby agrees to the DCO, which you can find below or at [DeveloperCertificate.org](http://developercertificate.org/).\n\n```\nDeveloper's Certificate of Origin 1.1\n\nBy making a contribution to this project, I certify that:\n\n(a) The contribution was created in whole or in part by me and I\n    have the right to submit it under the open source license\n    indicated in the file; or\n\n(b) The contribution is based upon previous work that, to the\n    best of my knowledge, is covered under an appropriate open\n    source license and I have the right under that license to\n    submit that work with modifications, whether created in whole\n    or in part by me, under the same open source license (unless\n    I am permitted to submit under a different license), as\n    Indicated in the file; or\n\n(c) The contribution was provided directly to me by some other\n    person who certified (a), (b) or (c) and I have not modified\n    it.\n\n(d) I understand and agree that this project and the contribution\n    are public and that a record of the contribution (including\n    all personal information I submit with it, including my\n    sign-off) is maintained indefinitely and may be redistributed\n    consistent with this project or the open source license(s)\n    involved.\n ```\nWe require that every contribution to OpenSearch is signed with a Developer Certificate of Origin. Additionally, please use your real name. We do not accept anonymous contributors nor those utilizing pseudonyms.\n\nEach commit must include a DCO which looks like this\n\n```\nSigned-off-by: Jane Smith <jane.smith@email.com>\n```\nYou may type this line on your own when writing your commit messages. However, if your user.name and user.email are set in your git configs, you can use `-s` or `--signoff` to add the `Signed-off-by` line to the end of the commit message.\n\n## Changelog\n\nOpenSearch maintains version specific changelog by enforcing a change to the ongoing [CHANGELOG](CHANGELOG.md) file adhering to the [Keep A Changelog](https://keepachangelog.com/en/1.0.0/) format. The purpose of the changelog is for the contributors and maintainers to incrementally build the release notes throughout the development process to avoid a painful and error-prone process of attempting to compile the release notes at release time. On each release the \"unreleased\" entries of the changelog are moved to the appropriate release notes document in the `./release-notes` folder. Also, incrementally building the changelog provides a concise, human-readable list of significant features that have been added to the unreleased version under development.\n\n### Which changes require a CHANGELOG entry?\nChangelogs are intended for operators/administrators, developers integrating with libraries and APIs, and end-users interacting with OpenSearch Dashboards and/or the REST API (collectively referred to as \"user\"). In short, any change that a user of OpenSearch might want to be aware of should be included in the changelog. The changelog is _not_ intended to replace the git commit log that developers of OpenSearch itself rely upon. The following are some examples of changes that should be in the changelog:\n\n- A newly added feature\n- A fix for a user-facing bug\n- Dependency updates\n- Fixes for security issues\n\nThe following are some examples where a changelog entry is not necessary:\n\n- Adding, modifying, or fixing tests\n- An incremental PR for a larger feature (such features should include _one_ changelog entry for the feature)\n- Documentation changes or code refactoring\n- Build-related changes\n\nAny PR that does not include a changelog entry will result in a failure of the validation workflow in GitHub. If the contributor and maintainers agree that no changelog entry is required, then the `skip-changelog` label can be applied to the PR which will result in the workflow passing.\n\n### How to add my changes to [CHANGELOG](CHANGELOG.md)?\n\nAdding in the change is two step process:\n1. Add your changes to the corresponding section within the CHANGELOG file with dummy pull request information, publish the PR\n2. Update the entry for your change in [`CHANGELOG.md`](CHANGELOG.md) and make sure that you reference the pull request there.\n\n### Where should I put my CHANGELOG entry?\nPlease review the [branching strategy](https://github.com/opensearch-project/.github/blob/main/RELEASING.md#opensearch-branching) document. The changelog on the `main` branch will contain **two files**: `CHANGELOG.md` which corresponds to unreleased changes intended for the _next minor_ release and `CHANGELOG-3.0.md` which correspond to unreleased changes intended for the _next major_ release. Your entry should go into file corresponding to the version it is intended to be released in. In practice, most changes to `main` will be backported to the next minor release so most entries will be in the `CHANGELOG.md` file.\n\nThe following examples assume the _next major_ release on main is 3.0, then _next minor_ release is 2.5, and the _current_ release is 2.4.\n\n- **Add a new feature to release in next minor:** Add a changelog entry to `[Unreleased 2.x]` in CHANGELOG.md on main, then backport to 2.x (including the changelog entry).\n- **Introduce a breaking API change to release in next major:** Add a changelog entry to `[Unreleased 3.0]` to CHANGELOG-3.0.md on main, do not backport.\n- **Upgrade a dependency to fix a CVE:** Add a changelog entry to `[Unreleased 2.x]` on main, then backport to 2.x (including the changelog entry), then backport to 2.4 and ensure the changelog entry is added to `[Unreleased 2.4.1]`.\n\n## Review Process\n\nWe deeply appreciate everyone who takes the time to make a contribution. We will review all contributions as quickly as possible. As a reminder, [opening an issue](https://github.com/opensearch-project/OpenSearch/issues/new/choose) discussing your change before you make it is the best way to smooth the PR process. This will prevent a rejection because someone else is already working on the problem, or because the solution is incompatible with the architectural direction.\n\nDuring the PR process, expect that there will be some back-and-forth. Please try to respond to comments in a timely fashion, and if you don't wish to continue with the PR, let us know. If a PR takes too many iterations for its complexity or size, we may reject it. Additionally, if you stop responding we may close the PR as abandoned. In either case, if you feel this was done in error, please add a comment on the PR.\n\nIf we accept the PR, a [maintainer](MAINTAINERS.md) will merge your change and usually take care of backporting it to appropriate branches ourselves.\n\nIf we reject the PR, we will close the pull request with a comment explaining why. This decision isn't always final: if you feel we have misunderstood your intended change or otherwise think that we should reconsider then please continue the conversation with a comment on the PR and we'll do our best to address any further points you raise.\n\n### Tips for Success (#tips)\n\nWe have a lot of mechanisms to help expedite towards an accepted PR. Here are some tips for success:\n1. *Minimize BWC guarantees*: The first PR review cycle heavily focuses on the public facing APIs. This is what we have to \"guarantee\" as non-breaking for [bwc across major versions](./DEVELOPER_GUIDE.md#backwards-compatibility).\n2. *Do not copy non-compliant code*: Ensure that code is APLv2 compatible. This means that you have not copied any code from other sources unless that code is also APLv2 compatible.\n3. *Utilize feature flags*: Features that are safeguarded behind feature flags are more likely to be merged and backported, as they come with an additional layer of protection. Refer to this [example PR](https://github.com/opensearch-project/OpenSearch/pull/4959) for implementation details.\n4. *Use appropriate Java tags*:\n    - `@opensearch.internal`: Marks internal classes subject to rapid changes.\n    - `@opensearch.api`: Marks public-facing API classes with backward compatibility guarantees.\n    - `@opensearch.experimental`: Indicates rapidly changing [experimental code](./DEVELOPER_GUIDE.md#experimental-development).\n5. *Employ sandbox for significant core changes*: Any new features or enhancements that make changes to core classes (e.g., search phases, codecs, or specialized lucene APIs) are more likely to. be merged if they are sandboxed. This can only be enabled on the java CLI (`-Dsandbox.enabled=true`).\n6. *Micro-benchmark critical path*: This is a lesser known mechanism, but if you have critical path changes you're afraid will impact performance (the changes touch the garbage collector, heap, direct memory, or CPU) then including a [microbenchmark](https://github.com/opensearch-project/OpenSearch/tree/main/benchmarks) with your PR (and jfr or flamegraph results in the description) is a *GREAT IDEA* and will help expedite the review process.\n7. *Test rigorously*: Ensure thorough testing ([OpenSearchTestCase](./test/framework/src/main/java/org/opensearch/test/OpenSearchTestCase.java) for unit tests, [OpenSearchIntegTestCase](./test/framework/src/main/java/org/opensearch/test/OpenSearchIntegTestCase.java) for integration & cluster tests, [OpenSearchRestTestCase](./test/framework/src/main/java/org/opensearch/test/rest/OpenSearchRestTestCase.java) for testing REST endpoint interfaces, and yaml tests with [ClientYamlTestSuiteIT](./rest-api-spec/src/yamlRestTest/java/org/opensearch/test/rest/ClientYamlTestSuiteIT.java) for REST integration tests)\n\nIn general, adding more guardrails to your changes increases the likelihood of swift PR acceptance. We can always relax these guard rails in smaller followup PRs. Reverting a GA feature is much more difficult. Check out the [DEVELOPER_GUIDE](./DEVELOPER_GUIDE.md#submitting-changes) for more useful tips.\n\n## Troubleshooting Failing Builds\n\nThe OpenSearch testing framework offers many capabilities but exhibits significant complexity (it does lot of randomization internally to cover as many edge cases and variations as possible). Unfortunately, this posses a challenge by making it harder to discover important issues/bugs in straightforward way and may lead to so called flaky tests - the tests which flip randomly from success to failure without any code changes.\n\nIf your pull request reports a failing test(s) on one of the checks, please:\n- look if there is an existing [issue](https://github.com/opensearch-project/OpenSearch/issues) reported for the test in question\n- if not, please make sure this is not caused by your changes, run the failing test(s) locally for some time\n- if you are sure the failure is not related, please open a new [bug](https://github.com/opensearch-project/OpenSearch/issues/new?assignees=&labels=bug%2C+untriaged&projects=&template=bug_template.md&title=%5BBUG%5D) with `flaky-test` label\n- add a comment referencing the issue(s) or bug report(s) to your pull request explaining the failing build(s)\n- as a bonus point, try to contribute by fixing the flaky test(s)\n"
        },
        {
          "name": "DEVELOPER_GUIDE.md",
          "type": "blob",
          "size": 36.376953125,
          "content": "- [Developer Guide](#developer-guide)\n  - [Getting Started](#getting-started)\n    - [Git Clone OpenSearch Repo](#git-clone-opensearch-repo)\n    - [Install Prerequisites](#install-prerequisites)\n      - [JDK](#jdk)\n      - [Custom Runtime JDK](#custom-runtime-jdk)\n      - [Windows](#windows)\n      - [Docker](#docker)\n    - [Build](#build)\n      - [Generated Code](#generated-code)\n    - [Run Tests](#run-tests)\n    - [Run OpenSearch](#run-opensearch)\n  - [Use an Editor](#use-an-editor)\n    - [IntelliJ IDEA](#intellij-idea)\n      - [Remote development using JetBrains Gateway](#remote-development-using-jetbrains-gateway)\n    - [Visual Studio Code](#visual-studio-code)\n    - [Eclipse](#eclipse)\n  - [Project Layout](#project-layout)\n    - [`distribution`](#distribution)\n    - [`libs`](#libs)\n    - [`modules`](#modules)\n    - [`plugins`](#plugins)\n    - [`sandbox`](#sandbox)\n    - [`qa`](#qa)\n    - [`server`](#server)\n    - [`test`](#test)\n  - [Java Language Formatting Guidelines](#java-language-formatting-guidelines)\n  - [Adding Dependencies](#adding-dependencies)\n    - [Editor / IDE Support](#editor--ide-support)\n    - [Formatting Failures](#formatting-failures)\n  - [Gradle Build](#gradle-build)\n    - [Configurations](#configurations)\n      - [implementation](#implementation)\n      - [api](#api)\n      - [runtimeOnly](#runtimeonly)\n      - [compileOnly](#compileonly)\n      - [testImplementation](#testimplementation)\n    - [Gradle Plugins](#gradle-plugins)\n      - [Distribution Download Plugin](#distribution-download-plugin)\n    - [Creating fat-JAR of a Module](#creating-fat-jar-of-a-module)\n  - [Components](#components)\n    - [Build Libraries & Interfaces](#build-libraries--interfaces)\n    - [Clients & Libraries](#clients--libraries)\n    - [Plugins](#plugins-1)\n    - [Indexing & Search](#indexing--search)\n    - [Aggregations](#aggregations)\n    - [Distributed Framework](#distributed-framework)\n  - [Misc](#misc)\n    - [Git Secrets](#git-secrets)\n      - [Installation](#installation)\n      - [Configuration](#configuration)\n    - [Submitting Changes](#submitting-changes)\n    - [Backwards Compatibility](#backwards-compatibility)\n      - [Data](#data)\n      - [Developer API](#developer-api)\n      - [User API](#user-api)\n      - [Experimental Development](#experimental-development)\n      - [API Compatibility Checks](#api-compatibility-checks)\n    - [Backports](#backports)\n    - [LineLint](#linelint)\n    - [Lucene Snapshots](#lucene-snapshots)\n    - [Flaky Tests](#flaky-tests)\n    - [Gradle Check Metrics Dashboard](#gradle-check-metrics-dashboard)\n\n# Developer Guide\n\nSo you want to contribute code to OpenSearch? Excellent! We're glad you're here. Here's what you need to do.\n\n## Getting Started\n\n### Git Clone OpenSearch Repo\n\nFork [opensearch-project/OpenSearch](https://github.com/opensearch-project/OpenSearch) and clone locally, e.g. `git clone https://github.com/[your username]/OpenSearch.git`.\n\n### Install Prerequisites\n\n#### JDK\n\nOpenSearch recommends building with the [Temurin/Adoptium](https://adoptium.net/temurin/releases/) distribution. JDK 11 is the minimum supported, and JDK-23 is the newest supported. You must have a supported JDK installed with the environment variable `JAVA_HOME` referencing the path to Java home for your JDK installation, e.g. `JAVA_HOME=/usr/lib/jvm/jdk-21`. \n\nDownload Java 11 from [here](https://adoptium.net/releases.html?variant=openjdk11). \n\n\nIn addition, certain backward compatibility tests check out and compile the previous major version of OpenSearch, and therefore require installing [JDK 11](https://adoptium.net/temurin/releases/?version=11) and [JDK 17](https://adoptium.net/temurin/releases/?version=17) and setting the `JAVA11_HOME` and `JAVA17_HOME` environment variables. More to that, since 8.10 release, Gradle has deprecated the usage of the any JDKs below JDK-16. For smooth development experience, the recommendation is to install at least [JDK 17](https://adoptium.net/temurin/releases/?version=17) or [JDK 21](https://adoptium.net/temurin/releases/?version=21). If you still want to build with JDK-11 only, please add `-Dorg.gradle.warning.mode=none` when invoking any Gradle build task from command line, for example:\n\n```\n./gradlew check -Dorg.gradle.warning.mode=none\n```\n\nBy default, the test tasks use bundled JDK runtime, configured in version catalog [gradle/libs.versions.toml](gradle/libs.versions.toml), and set to JDK 23 (non-LTS).\n\n```\nbundled_jdk_vendor = adoptium\nbundled_jdk = 23.0.1+11\n```\n\n#### Custom Runtime JDK\n\nOther kind of test tasks (integration, cluster, etc.) use the same runtime as `JAVA_HOME`. However, the build also supports compiling with one version of JDK, and testing on a different version. To do this, set `RUNTIME_JAVA_HOME` pointing to the Java home of another JDK installation, e.g. `RUNTIME_JAVA_HOME=/usr/lib/jvm/jdk-14`. Alternatively, the runtime JDK version could be provided as the command line argument, using combination of `runtime.java=<major JDK version>` property and `JAVA<major JDK version>_HOME` environment variable, for example `./gradlew -Druntime.java=17 ...` (in this case, the tooling expects `JAVA17_HOME` environment variable to be set).\n\n#### Windows\n\nOn Windows, set `_JAVA_OPTIONS: -Xmx4096M`. You may also need to set `LongPathsEnabled=0x1` under `Computer\\HKEY_LOCAL_MACHINE\\SYSTEM\\CurrentControlSet\\Control\\FileSystem`.\n\n#### Docker\n\nDownload and install [Docker](https://docs.docker.com/install/), required for building OpenSearch artifacts, and executing certain test suites.\n\nOn Windows, [use Docker Desktop 3.6](https://docs.docker.com/desktop/windows/release-notes/3.x/). See [OpenSearch#1425](https://github.com/opensearch-project/OpenSearch/issues/1425) for workarounds and issues with Docker Desktop 4.1.1.\n\n### Build\n\nTo build all distributions of OpenSearch, run:\n\n```\n./gradlew assemble\n```\n\nTo build a distribution to run on your local platform, run:\n\n```\n./gradlew localDistro\n```\n\nAll distributions built will be under `distributions/archives`.\n\n#### Generated Code\n\nOpenSearch uses code generators like [Protobuf](https://protobuf.dev/).\nOpenSearch build system already takes a dependency of generating code from protobuf, incase you run into compilation errors, run:\n\n```\n./gradlew generateProto\n```\n\nGenerated code in OpenSearch is used to establish cross version compatibility communication for API contracts within OpenSearch.\n\n### Run Tests\n\nOpenSearch uses a Gradle wrapper for its build. Run `gradlew` on Unix systems, or `gradlew.bat` on Windows in the root of the repository.\n\nStart by running the test suite with `gradlew check`. This should complete without errors.\n\n```\n./gradlew check\n\n=======================================\nOpenSearch Build Hamster says Hello!\n  Gradle Version        : 6.6.1\n  OS Info               : Linux 5.4.0-1037-aws (amd64)\n  JDK Version           : 11 (JDK)\n  JAVA_HOME             : /usr/lib/jvm/java-11-openjdk-amd64\n=======================================\n\n...\n\nBUILD SUCCESSFUL in 14m 50s\n2587 actionable tasks: 2450 executed, 137 up-to-date\n```\n\nIf the full test suite fails you may want to start with a smaller set.\n\n```\n./gradlew precommit\n```\n\n### Run OpenSearch\n\nRun OpenSearch using `gradlew run`.\n\n```\n./gradlew run\n```\n\n[Plugins](plugins/) may be installed by passing a `-PinstalledPlugins` property:\n\n```bash\n./gradlew run -PinstalledPlugins=\"['plugin1', 'plugin2']\"\n```\n\nThat will build OpenSearch and start it, writing its log above Gradle's status message. We log a lot of stuff on startup, specifically these lines tell you that OpenSearch is ready.\n\n```\n[2020-05-29T14:50:35,167][INFO ][o.e.h.AbstractHttpServerTransport] [runTask-0] publish_address {127.0.0.1:9200}, bound_addresses {[::1]:9200}, {127.0.0.1:9200}\n[2020-05-29T14:50:35,169][INFO ][o.e.n.Node               ] [runTask-0] started\n```\n\nIt's typically easier to wait until the console stops scrolling, and then run `curl` in another window to check if OpenSearch instance is running.\n\n```bash\ncurl localhost:9200\n\n{\n  \"name\" : \"runTask-0\",\n  \"cluster_name\" : \"runTask\",\n  \"cluster_uuid\" : \"oX_S6cxGSgOr_mNnUxO6yQ\",\n  \"version\" : {\n    \"number\" : \"1.0.0-SNAPSHOT\",\n    \"build_type\" : \"tar\",\n    \"build_hash\" : \"0ba0e7cc26060f964fcbf6ee45bae53b3a9941d0\",\n    \"build_date\" : \"2021-04-16T19:45:44.248303Z\",\n    \"build_snapshot\" : true,\n    \"lucene_version\" : \"8.7.0\",\n    \"minimum_wire_compatibility_version\" : \"6.8.0\",\n    \"minimum_index_compatibility_version\" : \"6.0.0-beta1\"\n  }\n}\n```\n\nUse `-Dtests.opensearch.` to pass additional settings to the running instance. For example, to enable OpenSearch to listen on an external IP address pass `-Dtests.opensearch.http.host`. Make sure your firewall or security policy allows external connections for this to work.\n\n```bash\n./gradlew run -Dtests.opensearch.http.host=0.0.0.0\n```\n\n## Use an Editor\n\n### IntelliJ IDEA\n\nWhen importing into IntelliJ you will need to define an appropriate JDK. The convention is that **this SDK should be named \"11\"**, and the project import will detect it automatically. For more details on defining an SDK in IntelliJ please refer to [this documentation](https://www.jetbrains.com/help/idea/sdk.html#define-sdk). Note that SDK definitions are global, so you can add the JDK from any project, or after project import. Importing with a missing JDK will still work, IntelliJ will report a problem and will refuse to build until resolved.\n\nYou can import the OpenSearch project into IntelliJ IDEA as follows.\n\n1. Select **File > Open**\n2. In the subsequent dialog navigate to the root `build.gradle` file\n3. In the subsequent dialog select **Open as Project**\n\n#### Remote development using JetBrains Gateway\n\n[JetBrains Gateway](https://www.jetbrains.com/remote-development/gateway/) enables development, testing and debugging on remote machines like development servers.\n\n1. On the local development machine, download and install the latest thin client from the [JetBrains Gateway page](https://www.jetbrains.com/remote-development/gateway/).\n2. Create a new connection to the remote server and install an IntelliJ server support using [these instructions](https://www.jetbrains.com/help/idea/remote-development-starting-page.html#connect_to_rd_ij).\n\nFollow the [IntelliJ IDEA instructions](#intellij-idea) post a successful connection.\n\n### Visual Studio Code\n\nFollow links in the [Java Tutorial](https://code.visualstudio.com/docs/java/java-tutorial) to install the coding pack and extensions for Java, Gradle tasks, etc. Open the source code directory.\n\n### Eclipse\n\nWhen importing to Eclipse, you need to have [Eclipse Buildship](https://projects.eclipse.org/projects/tools.buildship) plugin installed and, preferably, have JDK 11 set as default JRE in **Preferences -> Java -> Installed JREs**. Once this is done, generate Eclipse projects using Gradle wrapper:\n\n    ./gradlew eclipse\n\nYou can now import the OpenSearch project into Eclipse as follows.\n\n1. Select **File > Import -> Existing Gradle Project**\n2. In the subsequent dialog navigate to the root of `build.gradle` file\n3. In the subsequent dialog, if JDK 11 is not set as default JRE, please make sure to check **[Override workspace settings]**, keep **[Gradle Wrapper]** and provide the correct path to JDK11 using **[Java Home]** property under **[Advanced Options]**. Otherwise, you may run into cryptic import failures and only top level project is going to be imported.\n4. In the subsequent dialog, you should see **[Gradle project structure]** populated, please click **[Finish]** to complete the import\n\n**Note:** it may look non-intuitive why one needs to use Gradle wrapper and then import existing Gradle project (in general, **File > Import -> Existing Gradle Project** should be enough). Practically, as it stands now, Eclipse Buildship plugin does not import OpenSearch project dependencies correctly but does work in conjunction with Gradle wrapper.\n\n## Project Layout\n\nThis repository is split into many top level directories. The most important ones are:\n\n### `distribution`\n\nBuilds our tar and zip archives and our rpm and deb packages. There are several flavors of the distributions, with the classifier included in the name of the final deliverable (archive or package):\n - default (no classifier), the distribution with bundled JDK\n - `-no-jdk-` - the distribution without bundled JDK/JRE, assumes the JDK/JRE is going to be pre-installed on the target systems\n - `-jre-` - the distribution bundled with JRE (smaller footprint), supported as experimental feature for some platforms\n\n### `libs`\n\nLibraries used to build other parts of the project. These are meant to be internal rather than general purpose. We have no plans to\n[semver](https://semver.org/) their APIs or accept feature requests for them. We publish them to maven central because they are dependencies of our plugin test framework, high level rest client, and jdbc driver but they really aren't general purpose enough to *belong* in maven central. We're still working out what to do here.\n\n### `modules`\n\nFeatures that are shipped with OpenSearch by default but are not built in to the server. We typically separate features from the server because they require permissions that we don't believe *all* of OpenSearch should have or because they depend on libraries that we don't believe *all* of OpenSearch should depend on.\n\nFor example, reindex requires the `connect` permission so it can perform reindex-from-remote but we don't believe that the *all* of OpenSearch should have the \"connect\". For another example, Painless is implemented using antlr4 and asm and we don't believe that *all* of OpenSearch should have access to them.\n\n### `plugins`\n\nOpenSearch plugins. We decide that a feature should be a plugin rather than shipped as a module because we feel that it is only important to a subset of users, especially if it requires extra dependencies.\n\nThe canonical example of this is the ICU analysis plugin. It is important for folks who want the fairly language neutral ICU analyzer but the library to implement the analyzer is 11MB so we don't ship it with OpenSearch by default.\n\nAnother example is the `discovery-gce` plugin. It is *vital* to folks running in [GCP](https://cloud.google.com/) but useless otherwise and it depends on a dozen extra jars.\n\n### `sandbox`\n\nThis is where the community can add experimental features in to OpenSearch. There are three directories inside the sandbox - `libs`, `modules` and `plugins` - which mirror the subdirectories in the project root and have the same guidelines for deciding on where a new feature goes. The artifacts from `libs` and `modules` will be automatically included in the **snapshot** distributions. Once a certain feature is deemed worthy to be included in the OpenSearch release, it will be promoted to the corresponding subdirectory in the project root. **Note**: The sandbox code do not have any other guarantees such as backwards compatibility or long term support and can be removed at any time.\n\nTo exclude the modules from snapshot distributions, use the `sandbox.enabled` system property.\n\n    ./gradlew assemble -Dsandbox.enabled=false\n\n### `qa`\n\nHonestly this is kind of in flux and we're not 100% sure where we'll end up. We welcome your thoughts and help.\n\nRight now the directory contains the following.\n\n* Tests that require multiple modules or plugins to work.\n* Tests that form a cluster made up of multiple versions of OpenSearch like full cluster restart, rolling restarts, and mixed version tests.\n* Tests that test the OpenSearch clients in \"interesting\" places like the `wildfly` project.\n* Tests that test OpenSearch in funny configurations like with ingest disabled.\n* Tests that need to do strange things like install plugins that thrown uncaught `Throwable`s or add a shutdown hook.\n\nBut we're not convinced that all of these things *belong* in the qa directory. We're fairly sure that tests that require multiple modules or plugins to work should just pick a \"home\" plugin. We're fairly sure that the multi-version tests *do* belong in qa. Beyond that, we're not sure. If you want to add a new qa project, open a PR and be ready to discuss options.\n\n### `server`\n\nThe server component of OpenSearch that contains all of the modules and plugins. Right now things like the high level rest client depend on the server but we'd like to fix that in the future.\n\n### `test`\n\nOur test framework and test fixtures. We use the test framework for testing the server, the plugins, and modules, and pretty much everything else. We publish the test framework so folks who develop OpenSearch plugins can use it to test the plugins. The test fixtures are external processes that we start before running specific tests that rely on them.\n\nFor example, we have an hdfs test that uses mini-hdfs to test our repository-hdfs plugin.\n\n## Java Language Formatting Guidelines\n\nJava files in the OpenSearch codebase are formatted with the Eclipse JDT formatter, using the [Spotless Gradle](https://github.com/diffplug/spotless/tree/master/plugin-gradle) plugin. This plugin is configured on a project-by-project basis, via `build.gradle` in the root of the repository. So long as at least one project is configured, the formatting check can be run explicitly with:\n\n    ./gradlew spotlessJavaCheck\n\nThe code can be formatted with:\n\n    ./gradlew spotlessApply\n\nThese tasks can also be run for specific subprojects, e.g.\n\n    ./gradlew server:spotlessJavaCheck\n\nPlease follow these formatting guidelines:\n\n* Java indent is 4 spaces\n* Line width is 140 characters\n* Lines of code surrounded by `// tag::NAME` and `// end::NAME` comments are included in the documentation and should only be 76 characters wide not counting leading indentation. Such regions of code are not formatted automatically as it is not possible to change the line length rule of the formatter for part of a file. Please format such sections sympathetically with the rest of the code, while keeping lines to maximum length of 76 characters.\n* Wildcard imports (`import foo.bar.baz.*`) are forbidden and will cause the build to fail.\n* If *absolutely* necessary, you can disable formatting for regions of code with the `// tag::NAME` and `// end::NAME` directives, but note that these are intended for use in documentation, so please make it clear what you have done, and only do this where the benefit clearly outweighs the decrease in consistency.\n* Note that JavaDoc and block comments i.e. `/* ... */` are not formatted, but line comments i.e `// ...` are.\n* There is an implicit rule that negative boolean expressions should use the form `foo == false` instead of `!foo` for better readability of the code. While this isn't strictly enforced, it might get called out in PR reviews as something to change.\n\n## Adding Dependencies\n\nWhen adding a new dependency or removing an existing dependency via any `build.gradle` (that are not in the test scope), update the dependency LICENSE and library SHAs.\n\nFor example, after adding `api \"org.slf4j:slf4j-api:${versions.slf4j}\"` to [plugins/discovery-ec2/build.gradle](plugins/discovery-ec2/build.gradle), copy the library `LICENSE.txt` and `NOTICE.txt` to `plugins/discovery-ec2/licenses/slf4j-api-LICENSE.txt` and `plugins/discovery-ec2/licenses/slf4j-api-NOTICE.txt`, then run the following to generate `plugins/discovery-ec2/licenses/slf4j-api-1.7.36.jar.sha1`.\n\n```\n./gradlew :plugins:discovery-ec2:updateSHAs\n```\n\nEnsure that `./gradlew :plugins:discovery-ec2:check` passes before submitting changes.\n\n### Editor / IDE Support\n\nIntelliJ IDEs can [import](https://blog.jetbrains.com/idea/2014/01/intellij-idea-13-importing-code-formatter-settings-from-eclipse/) the [settings file](buildSrc/formatterConfig.xml), and / or use the [Eclipse Code Formatter](https://plugins.jetbrains.com/plugin/6546-eclipse-code-formatter)\nplugin.\n\nYou can also tell Spotless to [format a specific file](https://github.com/diffplug/spotless/tree/master/plugin-gradle#can-i-apply-spotless-to-specific-files) from the command line.\n\n### Formatting Failures\n\nSometimes Spotless will report a \"misbehaving rule which can't make up its mind\" and will recommend enabling the `paddedCell()` setting. If you enabled this settings and run the format check again, Spotless will write files to `$PROJECT/build/spotless-diagnose-java/` to aid diagnosis. It writes different copies of the formatted files, so that you can see how they\ndiffer and infer what is the problem.\n\nThe `paddedCell()` option is disabled for normal operation in order to detect any misbehaviour. You can enable the option from the command line by running Gradle with `-Dspotless.paddedcell`.\n\n> Note: if you have imported the project into IntelliJ IDEA the project will be automatically configured to add the correct license header to new source files based on the source location.\n\n## Gradle Build\n\nWe use Gradle to build OpenSearch because it is flexible enough to not only build and package OpenSearch, but also orchestrate all of the ways that we have to test OpenSearch.\n\n### Configurations\n\nGradle organizes dependencies and build artifacts into \"configurations\" and allows you to use these configurations arbitrarily. Here are some of the most common configurations in our build and how we use them:\n\n#### implementation\n\nDependencies that are used by the project at compile and runtime but are not exposed as a compile dependency to other dependent projects. Dependencies added to the `implementation` configuration are considered an implementation detail that can be changed at a later date without affecting any dependent projects.\n\n#### api\n\nDependencies that are used as compile and runtime dependencies of a project and are considered part of the external api of the project.\n\n#### runtimeOnly\n\nDependencies that not on the classpath at compile time but are on the classpath at runtime. We mostly use this configuration to make sure that we do not accidentally compile against dependencies of our dependencies also known as \"transitive\" dependencies\".\n\n#### compileOnly\n\nCode that is on the classpath at compile time but that should not be shipped with the project because it is \"provided\" by the runtime\nsomehow. OpenSearch plugins use this configuration to include dependencies that are bundled with OpenSearch's server.\n\n#### testImplementation\n\nCode that is on the classpath for compiling tests that are part of this project but not production code. The canonical example\nof this is `junit`.\n\n### Gradle Plugins\n\n#### Distribution Download Plugin\n\nThe Distribution Download plugin downloads the latest version of OpenSearch by default, and supports overriding this behavior by setting `customDistributionUrl`.\n```\n./gradlew integTest -PcustomDistributionUrl=\"https://ci.opensearch.org/ci/dbc/bundle-build/1.2.0/1127/linux/x64/dist/opensearch-1.2.0-linux-x64.tar.gz\"\n```\n\n### Creating fat-JAR of a Module\n\nA fat-JAR (or an uber-JAR) is the JAR, which contains classes from all the libraries, on which your project depends and, of course, the classes of current project.\n\nThere might be cases where a developer would like to add some custom logic to the code of a module (or multiple modules) and generate a fat-JAR that can be directly used by the dependency management tool. For example, in [#3665](https://github.com/opensearch-project/OpenSearch/pull/3665) a developer wanted to provide a tentative patch as a fat-JAR to a consumer for changes made in the high level REST client.\n\nUse [Gradle Shadow plugin](https://imperceptiblethoughts.com/shadow/).\nAdd the following to the `build.gradle` file of the module for which you want to create the fat-JAR, e.g. `client/rest-high-level/build.gradle`:\n\n```\napply plugin: 'com.github.johnrengelman.shadow'\n```\n\nRun the `shadowJar` command using:\n```\n./gradlew :client:rest-high-level:shadowJar\n```\n\nThis will generate a fat-JAR in the `build/distributions` folder of the module, e.g. .`/client/rest-high-level/build/distributions/opensearch-rest-high-level-client-1.4.0-SNAPSHOT.jar`.\n\nYou can further customize your fat-JAR by customising the plugin, More information about shadow plugin can be found [here](https://imperceptiblethoughts.com/shadow/).\n\nTo use the generated JAR, install the JAR locally, e.g.\n```\nmvn install:install-file -Dfile=src/main/resources/opensearch-rest-high-level-client-1.4.0-SNAPSHOT.jar -DgroupId=org.opensearch.client -DartifactId=opensearch-rest-high-level-client -Dversion=1.4.0-SNAPSHOT -Dpackaging=jar -DgeneratePom=true\n```\n\nRefer the installed JAR as any other maven artifact, e.g.\n\n```\n<dependency>\n    <groupId>org.opensearch.client</groupId>\n    <artifactId>opensearch-rest-high-level-client</artifactId>\n    <version>1.4.0-SNAPSHOT</version>\n</dependency>\n```\n\n## Components\n\nAs you work in the OpenSearch repo you may notice issues getting labeled with component labels.  It's a housekeeping task to help group together similar pieces of work.  You can pretty much ignore it, but if you're curious, here's what the different labels mean:\n\n### Build Libraries & Interfaces\n\nTasks to make sure the build tasks are useful and packaging and distribution are easy.\n\nIncludes:\n\n- Gradle for the Core tasks\n- Groovy scripts\n- build-tools\n- Versioning interfaces\n- Compatibility\n- Javadoc enforcement\n\n### Clients & Libraries\n\nAPIs and communication mechanisms for external connections to OpenSearch.  This includes the library directory in OpenSearch (a set of common functions).\n\nIncludes:\n\n- Transport layer\n- High Level and low level Rest Client\n- CLI\n\n### Plugins\n\nAnything touching the plugin infrastructure within core OpenSearch.\n\nIncludes:\n\n- API\n- SPI\n- Plugin interfaces\n\n\n### Indexing & Search\n\nThe critical path of indexing and search, including:  Measure index and search, performance, Improving the performance of indexing and search, ensure synchronization OpenSearch APIs with upstream Lucene change (e.g. new field types, changing doc values and codex).\n\nIncludes:\n\n- Lucene Structures\n- FieldMappers\n- QueryBuilders\n- DocValues\n\n### Aggregations\n\nMaking sure OpenSearch can be used as a compute engine.\n\nIncludes:\n\n- APIs (suggest supporting a formal API)\n- Framework\n\n### Distributed Framework\n\nWork to make sure that OpenSearch can scale in a distributed manner.\n\nIncludes:\n\n- Nodes (Cluster Manager, Data, Compute, Ingest, Discovery, etc.)\n- Replication & Merge Policies (Document, Segment level)\n- Snapshot/Restore (repositories; S3, Azure, GCP, NFS)\n- Translog (e.g., OpenSearch, Kafka, Kinesis)\n- Shard Strategies\n- Circuit Breakers\n\n## Misc\n\n### Git Secrets\n\nSecurity is our top priority. Avoid checking in credentials.\n\n#### Installation\n\nInstall [awslabs/git-secrets](https://github.com/awslabs/git-secrets) by running the following commands.\n```\ngit clone https://github.com/awslabs/git-secrets.git\ncd git-secrets\nmake install\n```\n\n#### Configuration\n\nYou can configure git secrets per repository, you need to change the directory to the root of the repository and run the following command.\n```\ngit secrets --install\n Installed commit-msg hook to .git/hooks/commit-msg\n Installed pre-commit hook to .git/hooks/pre-commit\n Installed prepare-commit-msg hook to .git/hooks/prepare-commit-msg\n```\nThen, you need to apply patterns for git-secrets, you can install the AWS standard patterns by running the following command.\n```\ngit secrets --register-aws\n```\n\n### Submitting Changes\n\nSee [CONTRIBUTING](CONTRIBUTING.md).\n\n### Backwards Compatibility\n\nOpenSearch strives for a smooth and easy upgrade experience that is resilient to data loss and corruption while minimizing downtime and ensuring integration with\nexternal systems does not unexpectedly break.\n\nTo provide these guarantees each version must be designed and developed with [forward compatibility](https://en.wikipedia.org/wiki/Forward_compatibility) in mind.\nOpenSearch addresses backward and forward compatibility at three different levels: 1. Data, 2. Developer API, 3. User API. These levels and the developer mechanisms\nto ensure backwards compatibility are provided below.\n\n#### Data\n\nThe data level consists of index and application data file formats. OpenSearch guarantees file formats and indexes are compatible only back to the first release of\nthe previous major version. If on disk formats or encodings need to be changed (including index data, cluster state, or any other persisted data) developers must\nuse Version checks accordingly (e.g., `Version.onOrAfter`, `Version.before`) to guarantee backwards compatibility.\n\n#### Developer API\n\nThe Developer API consists of interfaces and foundation software implementations that enable external users to develop new OpenSearch features. This includes obvious\ncomponents such as the Plugin and Extension frameworks and less obvious components such as REST Action Handlers. When developing a new feature of OpenSearch it is\nimportant to explicitly mark which implementation components may, or may not, be extended by external implementations. For example, all new API classes with\n`@PublicApi` annotation (or documented as `@opensearch.api`) signal that the new component may be extended by an external implementation and therefore provide\nbackwards compatibility guarantees. Similarly, any class explicitly marked with the `@InternalApi` (or documented as `@opensearch.internal`) annotation, or not\nexplicitly marked by an annotation should not be extended by external implementation components as it does not guarantee backwards compatibility and may change at\nany time. The `@DeprecatedApi` annotation could also be added to any classes annotated with `@PublicApi` (or documented as `@opensearch.api`) or their methods that\nare either changed (with replacement) or planned to be removed across major versions.\n\nThe APIs which are designated to be public but have not been stabilized yet should be marked with `@ExperimentalApi` (or documented as `@opensearch.experimental`)\nannotation. The presence of this annotation signals that API may change at any time (major, minor or even patch releases). In general, the classes annotated with\n`@PublicApi` may expose other classes or methods annotated with `@ExperimentalApi`, in such cases the backward compatibility guarantees would not apply to latter\n(see please [Experimental Development](#experimental-development) for more details).\n\n#### User API\n\nThe User API consists of integration specifications (e.g., [Query Domain Specific Language](https://opensearch.org/docs/latest/opensearch/query-dsl/index/),\n[field mappings](https://opensearch.org/docs/latest/opensearch/mappings/)) and endpoints (e.g., [`_search`](https://opensearch.org/docs/latest/api-reference/search/),\n[`_cat`](https://opensearch.org/docs/latest/api-reference/cat/index/)) users rely on to integrate and use OpenSearch. Backwards compatibility is critical to the\nUser API, therefore OpenSearch commits to using [semantic versioning](https://opensearch.org/blog/what-is-semver/) for all User facing APIs. To support this\ndevelopers must leverage `Version` checks for any user facing endpoints or API specifications that change across minor versions. Developers must also inform\nusers of any changes by adding the `>breaking` label on Pull Requests, adding an entry to the [CHANGELOG](https://github.com/opensearch-project/OpenSearch/blob/main/CHANGELOG.md)\nand a log message to the OpenSearch deprecation log files using the `DeprecationLogger`.\n\n#### Experimental Development\n\nRapidly developing new features often benefit from several release cycles before committing to an official and long term supported (LTS) API. To enable this cycle OpenSearch\nuses an Experimental Development process leveraging [Feature Flags](https://featureflags.io/feature-flags/). This allows a feature to be developed using the same process as\na LTS feature but with additional guard rails and communication mechanisms to signal to the users and development community the feature is not yet stable, may change in a future\nrelease, or be removed altogether. Any Developer or User APIs implemented along with the experimental feature should be marked with `@ExperimentalApi` (or documented as\n`@opensearch.experimental`) annotation to signal the implementation is not subject to LTS and does not follow backwards compatibility guidelines.\n\n#### API Compatibility Checks\n\nThe compatibility checks for public APIs are performed using [japicmp](https://siom79.github.io/japicmp/) and are available as separate Gradle tasks (those are run on demand at the moment):\n\n```\n./gradlew japicmp\n```\n\nBy default, the API compatibility checks are run against the latest released version of the OpenSearch, however the target version to compare to could be provided using system property during the build, fe.:\n\n```\n./gradlew japicmp  -Djapicmp.compare.version=2.14.0-SNAPSHOT\n```\n\n### Backports\n\nThe Github workflow in [`backport.yml`](.github/workflows/backport.yml) creates backport PRs automatically when the original PR with an appropriate label `backport <backport-branch-name>` is merged to main with the backport workflow run successfully on the PR. For example, if a PR on main needs to be backported to `1.x` branch, add a label `backport 1.x` to the PR and make sure the backport workflow runs on the PR along with other checks. Once this PR is merged to main, the workflow will create a backport PR to the `1.x` branch.\n\n### LineLint\n\nA linter in [`code-hygiene.yml`](.github/workflows/code-hygiene.yml) that validates simple newline and whitespace rules in all sorts of files. It can:\n- Recursively check a directory tree for files that do not end in a newline\n- Automatically fix these files by adding a newline or trimming extra newlines.\n\nRules are defined in `.linelint.yml`.\n\nExecuting the binary will automatically search the local directory tree for linting errors.\n\n    linelint .\n\nPass a list of files or directories to limit your search.\n\n    linelint README.md LICENSE\n\n### Lucene Snapshots\n\nThe Github workflow in [lucene-snapshots.yml](.github/workflows/lucene-snapshots.yml) is a GitHub workflow executable by maintainers to build a top-down snapshot build of Lucene.\nThese snapshots are available to test compatibility with upcoming changes to Lucene by updating the version at [version.properties](buildsrc/version.properties) with the `version-snapshot-sha` version. Example: `lucene = 10.0.0-snapshot-2e941fc`.\nNote that these snapshots do not follow the Maven [naming convention](https://maven.apache.org/guides/getting-started/index.html#what-is-a-snapshot-version) with a (case sensitive) SNAPSHOT suffix, so these artifacts are considered \"releases\" by build systems such as the `mavenContent` repository filter in Gradle or `releases` artifact policies in Maven.\n\n### Flaky Tests\n\nIf you encounter a test failure locally or in CI that is seemingly unrelated to the change in your pull request, it may be a known flaky test or a new test failure. OpenSearch has a very large test suite with long running, often failing (flaky), integration tests. Such individual tests are labelled as [Flaky Random Test Failure](https://github.com/opensearch-project/OpenSearch/issues?q=is%3Aopen+is%3Aissue+label%3A%22flaky-test%22). Your help is wanted fixing these!\n\nThe automation [gradle-check-flaky-test-detector](https://build.ci.opensearch.org/job/gradle-check-flaky-test-detector/), which runs in OpenSearch public Jenkins, identifies failing flaky issues that are part of post-merge actions. Once a flaky test is identified, the automation creates an issue with detailed report that includes links to all relevant commits, the Gradle check build log, the test report, and pull requests that are impacted with the flaky test failures. This automation leverages data from the [OpenSearch Metrics Project](https://github.com/opensearch-project/opensearch-metrics) to establish a baseline for creating the issue and updating the flaky test report. For all flaky test issues created by automation, visit this [link](https://github.com/opensearch-project/OpenSearch/issues?q=is%3Aissue+is%3Aopen+label%3A%3Etest-failure+author%3Aopensearch-ci-bot).\n\nIf you still see a failing test that is not part of the post merge actions, please do:\n\n* Follow failed CI links, and locate the failing test(s) or use the [Gradle Check Metrics Dashboard](#gradle-check-metrics-dashboard).\n* Copy-paste the failure into a comment of your PR.\n* Search through issues using the name of the failed test for whether this is a known flaky test.\n* If no existing issue is found, open one.\n* Retry CI via the GitHub UX or by pushing an update to your PR.\n\n\n### Gradle Check Metrics Dashboard\n\nTo get the comprehensive insights and analysis of the Gradle Check test failures, visit the [OpenSearch Gradle Check Metrics Dashboard](https://metrics.opensearch.org/_dashboards/app/dashboards#/view/e5e64d40-ed31-11ee-be99-69d1dbc75083). This dashboard is part of the [OpenSearch Metrics Project](https://github.com/opensearch-project/opensearch-metrics) initiative. The dashboard contains multiple data points that can help investigate and resolve flaky failures. Additionally, this dashboard can be used to drill down, slice, and dice the data using multiple supported filters, which further aids in troubleshooting and resolving issues.\n"
        },
        {
          "name": "LICENSE.txt",
          "type": "blob",
          "size": 11.0908203125,
          "content": "                                 Apache License\n                           Version 2.0, January 2004\n                        http://www.apache.org/licenses/\n\n   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n\n   1. Definitions.\n\n      \"License\" shall mean the terms and conditions for use, reproduction,\n      and distribution as defined by Sections 1 through 9 of this document.\n\n      \"Licensor\" shall mean the copyright owner or entity authorized by\n      the copyright owner that is granting the License.\n\n      \"Legal Entity\" shall mean the union of the acting entity and all\n      other entities that control, are controlled by, or are under common\n      control with that entity. For the purposes of this definition,\n      \"control\" means (i) the power, direct or indirect, to cause the\n      direction or management of such entity, whether by contract or\n      otherwise, or (ii) ownership of fifty percent (50%) or more of the\n      outstanding shares, or (iii) beneficial ownership of such entity.\n\n      \"You\" (or \"Your\") shall mean an individual or Legal Entity\n      exercising permissions granted by this License.\n\n      \"Source\" form shall mean the preferred form for making modifications,\n      including but not limited to software source code, documentation\n      source, and configuration files.\n\n      \"Object\" form shall mean any form resulting from mechanical\n      transformation or translation of a Source form, including but\n      not limited to compiled object code, generated documentation,\n      and conversions to other media types.\n\n      \"Work\" shall mean the work of authorship, whether in Source or\n      Object form, made available under the License, as indicated by a\n      copyright notice that is included in or attached to the work\n      (an example is provided in the Appendix below).\n\n      \"Derivative Works\" shall mean any work, whether in Source or Object\n      form, that is based on (or derived from) the Work and for which the\n      editorial revisions, annotations, elaborations, or other modifications\n      represent, as a whole, an original work of authorship. For the purposes\n      of this License, Derivative Works shall not include works that remain\n      separable from, or merely link (or bind by name) to the interfaces of,\n      the Work and Derivative Works thereof.\n\n      \"Contribution\" shall mean any work of authorship, including\n      the original version of the Work and any modifications or additions\n      to that Work or Derivative Works thereof, that is intentionally\n      submitted to Licensor for inclusion in the Work by the copyright owner\n      or by an individual or Legal Entity authorized to submit on behalf of\n      the copyright owner. For the purposes of this definition, \"submitted\"\n      means any form of electronic, verbal, or written communication sent\n      to the Licensor or its representatives, including but not limited to\n      communication on electronic mailing lists, source code control systems,\n      and issue tracking systems that are managed by, or on behalf of, the\n      Licensor for the purpose of discussing and improving the Work, but\n      excluding communication that is conspicuously marked or otherwise\n      designated in writing by the copyright owner as \"Not a Contribution.\"\n\n      \"Contributor\" shall mean Licensor and any individual or Legal Entity\n      on behalf of whom a Contribution has been received by Licensor and\n      subsequently incorporated within the Work.\n\n   2. Grant of Copyright License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      copyright license to reproduce, prepare Derivative Works of,\n      publicly display, publicly perform, sublicense, and distribute the\n      Work and such Derivative Works in Source or Object form.\n\n   3. Grant of Patent License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      (except as stated in this section) patent license to make, have made,\n      use, offer to sell, sell, import, and otherwise transfer the Work,\n      where such license applies only to those patent claims licensable\n      by such Contributor that are necessarily infringed by their\n      Contribution(s) alone or by combination of their Contribution(s)\n      with the Work to which such Contribution(s) was submitted. If You\n      institute patent litigation against any entity (including a\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\n      or a Contribution incorporated within the Work constitutes direct\n      or contributory patent infringement, then any patent licenses\n      granted to You under this License for that Work shall terminate\n      as of the date such litigation is filed.\n\n   4. Redistribution. You may reproduce and distribute copies of the\n      Work or Derivative Works thereof in any medium, with or without\n      modifications, and in Source or Object form, provided that You\n      meet the following conditions:\n\n      (a) You must give any other recipients of the Work or\n          Derivative Works a copy of this License; and\n\n      (b) You must cause any modified files to carry prominent notices\n          stating that You changed the files; and\n\n      (c) You must retain, in the Source form of any Derivative Works\n          that You distribute, all copyright, patent, trademark, and\n          attribution notices from the Source form of the Work,\n          excluding those notices that do not pertain to any part of\n          the Derivative Works; and\n\n      (d) If the Work includes a \"NOTICE\" text file as part of its\n          distribution, then any Derivative Works that You distribute must\n          include a readable copy of the attribution notices contained\n          within such NOTICE file, excluding those notices that do not\n          pertain to any part of the Derivative Works, in at least one\n          of the following places: within a NOTICE text file distributed\n          as part of the Derivative Works; within the Source form or\n          documentation, if provided along with the Derivative Works; or,\n          within a display generated by the Derivative Works, if and\n          wherever such third-party notices normally appear. The contents\n          of the NOTICE file are for informational purposes only and\n          do not modify the License. You may add Your own attribution\n          notices within Derivative Works that You distribute, alongside\n          or as an addendum to the NOTICE text from the Work, provided\n          that such additional attribution notices cannot be construed\n          as modifying the License.\n\n      You may add Your own copyright statement to Your modifications and\n      may provide additional or different license terms and conditions\n      for use, reproduction, or distribution of Your modifications, or\n      for any such Derivative Works as a whole, provided Your use,\n      reproduction, and distribution of the Work otherwise complies with\n      the conditions stated in this License.\n\n   5. Submission of Contributions. Unless You explicitly state otherwise,\n      any Contribution intentionally submitted for inclusion in the Work\n      by You to the Licensor shall be under the terms and conditions of\n      this License, without any additional terms or conditions.\n      Notwithstanding the above, nothing herein shall supersede or modify\n      the terms of any separate license agreement you may have executed\n      with Licensor regarding such Contributions.\n\n   6. Trademarks. This License does not grant permission to use the trade\n      names, trademarks, service marks, or product names of the Licensor,\n      except as required for reasonable and customary use in describing the\n      origin of the Work and reproducing the content of the NOTICE file.\n\n   7. Disclaimer of Warranty. Unless required by applicable law or\n      agreed to in writing, Licensor provides the Work (and each\n      Contributor provides its Contributions) on an \"AS IS\" BASIS,\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n      implied, including, without limitation, any warranties or conditions\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n      PARTICULAR PURPOSE. You are solely responsible for determining the\n      appropriateness of using or redistributing the Work and assume any\n      risks associated with Your exercise of permissions under this License.\n\n   8. Limitation of Liability. In no event and under no legal theory,\n      whether in tort (including negligence), contract, or otherwise,\n      unless required by applicable law (such as deliberate and grossly\n      negligent acts) or agreed to in writing, shall any Contributor be\n      liable to You for damages, including any direct, indirect, special,\n      incidental, or consequential damages of any character arising as a\n      result of this License or out of the use or inability to use the\n      Work (including but not limited to damages for loss of goodwill,\n      work stoppage, computer failure or malfunction, or any and all\n      other commercial damages or losses), even if such Contributor\n      has been advised of the possibility of such damages.\n\n   9. Accepting Warranty or Additional Liability. While redistributing\n      the Work or Derivative Works thereof, You may choose to offer,\n      and charge a fee for, acceptance of support, warranty, indemnity,\n      or other liability obligations and/or rights consistent with this\n      License. However, in accepting such obligations, You may act only\n      on Your own behalf and on Your sole responsibility, not on behalf\n      of any other Contributor, and only if You agree to indemnify,\n      defend, and hold each Contributor harmless for any liability\n      incurred by, or claims asserted against, such Contributor by reason\n      of your accepting any such warranty or additional liability.\n\n   END OF TERMS AND CONDITIONS\n\n   APPENDIX: How to apply the Apache License to your work.\n\n      To apply the Apache License to your work, attach the following\n      boilerplate notice, with the fields enclosed by brackets \"[]\"\n      replaced with your own identifying information. (Don't include\n      the brackets!)  The text should be enclosed in the appropriate\n      comment syntax for the file format. We also recommend that a\n      file or class name and description of purpose be included on the\n      same \"printed page\" as the copyright notice for easier\n      identification within third-party archives.\n\n   Copyright [yyyy] [name of copyright owner]\n\n   Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.\n"
        },
        {
          "name": "MAINTAINERS.md",
          "type": "blob",
          "size": 4.083984375,
          "content": "## Overview\n\nThis document contains a list of maintainers in this repo. See [opensearch-project/.github/RESPONSIBILITIES.md](https://github.com/opensearch-project/.github/blob/main/RESPONSIBILITIES.md#maintainer-responsibilities) that explains what the role of maintainer means, what maintainers do in this and other repos, and how they should be doing it. If you're interested in contributing, and becoming a maintainer, see [CONTRIBUTING](CONTRIBUTING.md).\n\n## Current Maintainers\n\n| Maintainer               | GitHub ID                                               | Affiliation |\n|--------------------------|---------------------------------------------------------|-------------|\n| Anas Alkouz              | [anasalkouz](https://github.com/anasalkouz)             | Amazon      |\n| Andrew Ross              | [andrross](https://github.com/andrross)                 | Amazon      |\n| Andriy Redko             | [reta](https://github.com/reta)                         | Aiven       |\n| Ankit Jain               | [jainankitk](https://github.com/jainankitk)             | Amazon      |\n| Ashish Singh             | [ashking94](https://github.com/ashking94)               | Amazon      |\n| Bukhtawar Khan           | [Bukhtawar](https://github.com/Bukhtawar)               | Amazon      |\n| Charlotte Henkle         | [CEHENKLE](https://github.com/CEHENKLE)                 | Amazon      |\n| Dan Widdis               | [dbwiddis](https://github.com/dbwiddis)                 | Amazon      |\n| Daniel \"dB.\" Doubrovkine | [dblock](https://github.com/dblock)                     | Amazon      |\n| Gao Binlong              | [gaobinlong](https://github.com/gaobinlong)             | Amazon      |\n| Gaurav Bafna             | [gbbafna](https://github.com/gbbafna)                   | Amazon      |\n| Jay Deng                 | [jed326](https://github.com/jed326)                     | Amazon      |\n| Kunal Kotwani            | [kotwanikunal](https://github.com/kotwanikunal)         | Amazon      |\n| Varun Bansal             | [linuxpi](https://github.com/linuxpi)                   | Amazon      |\n| Marc Handalian           | [mch2](https://github.com/mch2)                         | Amazon      |\n| Michael Froh             | [msfroh](https://github.com/msfroh)                     | Amazon      |\n| Nick Knize               | [nknize](https://github.com/nknize)                     | Lucenia     |\n| Owais Kazi               | [owaiskazi19](https://github.com/owaiskazi19)           | Amazon      |\n| Peter Nied               | [peternied](https://github.com/peternied)               | Amazon      |\n| Rishikesh Pasham         | [Rishikesh1159](https://github.com/Rishikesh1159)       | Amazon      |\n| Sachin Kale              | [sachinpkale](https://github.com/sachinpkale)           | Amazon      |\n| Sarat Vemulapalli        | [saratvemulapalli](https://github.com/saratvemulapalli) | Amazon      |\n| Shweta Thareja           | [shwetathareja](https://github.com/shwetathareja)       | Amazon      |\n| Sorabh Hamirwasia        | [sohami](https://github.com/sohami)                     | Amazon      |\n| Vacha Shah               | [VachaShah](https://github.com/VachaShah)               | Amazon      |\n\n## Emeritus\n\n| Maintainer             | GitHub ID                                   | Affiliation |\n| ---------------------- |-------------------------------------------- | ----------- |\n| Megha Sai Kavikondala  | [meghasaik](https://github.com/meghasaik)   | Amazon      |\n| Xue Zhou               | [xuezhou25](https://github.com/xuezhou25)   | Amazon      |\n| Kartik Ganesh          | [kartg](https://github.com/kartg)           | Amazon      |\n| Abbas Hussain          | [abbashus](https://github.com/abbashus)     | Meta        |\n| Himanshu Setia         | [setiah](https://github.com/setiah)         | Amazon      |\n| Ryan Bogan             | [ryanbogan](https://github.com/ryanbogan)   | Amazon      |\n| Rabi Panda             | [adnapibar](https://github.com/adnapibar)   | Independent |\n| Tianli Feng            | [tlfeng](https://github.com/tlfeng)         | Amazon      |\n| Suraj Singh            | [dreamer-89](https://github.com/dreamer-89) | Amazon      |\n"
        },
        {
          "name": "NOTICE.txt",
          "type": "blob",
          "size": 0.4931640625,
          "content": "OpenSearch (https://opensearch.org/)\nCopyright OpenSearch Contributors\n\nThis product includes software developed by\nElasticsearch (http://www.elastic.co).\nCopyright 2009-2018 Elasticsearch\n\nThis product includes software developed by The Apache Software\nFoundation (http://www.apache.org/).\n\nThis product includes software developed by\nJoda.org (http://www.joda.org/).\n\nThis product includes software developed by\nMorten Haraldsen (ethlo) (https://github.com/ethlo) under the Apache License, version 2.0.\n"
        },
        {
          "name": "PERFORMANCE_BENCHMARKS.md",
          "type": "blob",
          "size": 7.6240234375,
          "content": "# README: Running Performance Benchmarks on Pull Requests\n\n## Overview\n\n`benchmark-pull-request` GitHub Actions workflow is designed to automatically run performance benchmarks on a pull request when a specific comment is made on the pull request. This ensures that performance benchmarks are consistently and accurately applied to code changes, helping maintain the performance standards of the repository.\n\n## Workflow Trigger\n\nThe workflow is triggered when a new comment is created on a pull request. Specifically, it checks for the presence of the `\"run-benchmark-test\"` keyword in the comment body. If this keyword is detected, the workflow proceeds to run the performance benchmarks.\n\n## Key Steps in the Workflow\n\n1. **Check Comment Format and Configuration:**\n    - Validates the format of the comment to ensure it contains the required `\"run-benchmark-test\"` keyword and is in json format.\n    - Extracts the benchmark configuration ID from the comment and verifies if it exists in the `benchmark-config.json` file.\n    - Checks if the extracted configuration ID is supported for the current OpenSearch major version.\n\n2. **Post Invalid Format Comment:**\n    - If the comment format is invalid or the configuration ID is not supported, a comment is posted on the pull request indicating the problem, and the workflow fails.\n\n3. **Manual Approval (if necessary):**\n    - Fetches the list of approvers from the `.github/CODEOWNERS` file.\n    - If the commenter is not one of the maintainers, a manual approval request is created. The workflow pauses until an approver approves or denies the benchmark run by commenting appropriate word on the issue.\n    - The issue for approval request is auto-closed once the approver is done adding appropriate comment\n\n4. **Build and Assemble OpenSearch:**\n    - Builds and assembles (x64-linux tar) the OpenSearch distribution from the pull request code changes.\n\n5. **Upload to S3:**\n    - Configures AWS credentials and uploads the assembled OpenSearch distribution to an S3 bucket for further use in benchmarking.\n    - The S3 bucket is fronted by cloudfront to only allow downloads.\n    - The lifecycle policy on the S3 bucket will delete the uploaded artifacts after 30-days.\n\n6. **Trigger Jenkins Workflow:**\n    - Triggers a Jenkins workflow to run the benchmark tests using a webhook token.\n\n7. **Update Pull Request with Job URL:**\n    - Posts a comment on the pull request with the URL of the Jenkins job. The final benchmark results will be posted once the job completes.\n    - To learn about how benchmark job works see https://github.com/opensearch-project/opensearch-build/tree/main/src/test_workflow#benchmarking-tests\n\n## How to Use This Workflow\n\n1. **Ensure `benchmark-config.json` is Up-to-Date:**\n    - The `benchmark-config.json` file should contain valid benchmark configurations with supported major versions and cluster-benchmark configurations.\n\n2. **Add the Workflow to Your Repository:**\n    - Save the workflow YAML file (typically named `benchmark.yml`) in the `.github/workflows` directory of your repository.\n\n3. **Make a Comment to Trigger the Workflow:**\n    - On any pull request issue, make a comment containing the keyword `\"run-benchmark-test\"` along with the configuration ID. For example:\n      ```json\n      {\"run-benchmark-test\": \"id_1\"}\n      ```\n\n4. **Monitor Workflow Progress:**\n    - The workflow will validate the comment, check for approval (if necessary), build the OpenSearch distribution, and trigger the Jenkins job.\n    - A comment will be posted on the pull request with the URL of the Jenkins job. You can monitor the progress and final results there as well.\n\n## Example Comment Format\n\nTo run the benchmark with configuration ID `id_1`, post the following comment on the pull request issue:\n```json\n{\"run-benchmark-test\": \"id_1\"}\n```\n\n## How to add a new benchmark configuration\n\nThe benchmark-config.json file accepts the following schema.\n```json\n{\n  \"id_<number>\": {\n    \"description\": \"Short description of the configuration\",\n    \"supported_major_versions\": [\"2\", \"3\"],\n    \"cluster-benchmark-configs\": {\n      \"SINGLE_NODE_CLUSTER\": \"Use single node cluster for benchmarking, accepted values are \\\"true\\\" or \\\"false\\\"\",\n      \"MIN_DISTRIBUTION\": \"Use OpenSearch min distribution, should always be \\\"true\\\"\",\n      \"MANAGER_NODE_COUNT\": \"For multi-node cluster tests, number of cluster manager nodes, empty value defaults to 3.\",\n      \"DATA_NODE_COUNT\": \"For multi-node cluster tests, number of data nodes, empty value defaults to 2.\",\n      \"DATA_INSTANCE_TYPE\": \"EC2 instance type for data node, empty defaults to r5.xlarge.\",\n      \"DATA_NODE_STORAGE\": \"Data node ebs block storage size, empty value defaults to 100Gb\",\n      \"JVM_SYS_PROPS\": \"A comma-separated list of key=value pairs that will be added to jvm.options as JVM system properties\",\n      \"ADDITIONAL_CONFIG\": \"Additional space delimited opensearch.yml config parameters. e.g., `search.concurrent_segment_search.enabled:true`\",\n      \"TEST_WORKLOAD\": \"The workload name from OpenSearch Benchmark Workloads. https://github.com/opensearch-project/opensearch-benchmark-workloads. Default is nyc_taxis\",\n      \"WORKLOAD_PARAMS\": \"With this parameter you can inject variables into workloads, e.g.{\\\"number_of_replicas\\\":\\\"0\\\",\\\"number_of_shards\\\":\\\"3\\\"}. See https://opensearch.org/docs/latest/benchmark/reference/commands/command-flags/#workload-params\",\n      \"EXCLUDE_TASKS\": \"Defines a comma-separated list of test procedure tasks not to run. e.g. type:search, see https://opensearch.org/docs/latest/benchmark/reference/commands/command-flags/#exclude-tasks\",\n      \"INCLUDE_TASKS\": \"Defines a comma-separated list of test procedure tasks to run. By default, all tasks listed in a test procedure array are run. See https://opensearch.org/docs/latest/benchmark/reference/commands/command-flags/#include-tasks\",\n      \"TEST_PROCEDURE\": \"Defines a test procedure to use. e.g., `append-no-conflicts,significant-text`. Uses default if none provided. See https://opensearch.org/docs/latest/benchmark/reference/commands/command-flags/#test-procedure\",\n      \"CAPTURE_NODE_STAT\": \"Enable opensearch-benchmark node-stats telemetry to capture system level metrics like cpu, jvm etc., see https://opensearch.org/docs/latest/benchmark/reference/telemetry/#node-stats\"\n    },\n    \"cluster_configuration\": {\n      \"size\": \"Single-Node/Multi-Node\",\n      \"data_instance_config\": \"data-instance-config, e.g., 4vCPU, 32G Mem, 16G Heap\"\n    }\n  }\n}\n```\nTo add a new test configuration that are suitable to your changes please create a new PR to add desired cluster and benchmark configurations.\n\n## How to compare my results against baseline?\n\nApart from just running benchmarks the user will also be interested in how their change is performing against current OpenSearch distribution with the exactly same cluster and benchmark configurations.\nThe user can refer to https://s12d.com/basline-dashboards (WIP) to access baseline data for their workload, this data is generated by our nightly benchmark runs on latest build distribution artifacts for 3.0 and 2.x.\nIn the future, we will add the [compare](https://opensearch.org/docs/latest/benchmark/reference/commands/compare/) feature of opensearch-benchmark to run comparison and publish data on the PR as well.\n\n## Notes\n\n- Ensure all required secrets (e.g., `GITHUB_TOKEN`, `UPLOAD_ARCHIVE_ARTIFACT_ROLE`, `ARCHIVE_ARTIFACT_BUCKET_NAME`, `JENKINS_PR_BENCHMARK_GENERIC_WEBHOOK_TOKEN`) are properly set in the repository secrets.\n- The `CODEOWNERS` file should list the GitHub usernames of approvers for the benchmark process.\n\nBy following these instructions, repository maintainers can ensure consistent and automated performance benchmarking for all code changes introduced via pull requests.\n\n\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 4.841796875,
          "content": "<img src=\"https://opensearch.org/assets/img/opensearch-logo-themed.svg\" height=\"64px\">\n\n[![Chat](https://img.shields.io/badge/chat-on%20forums-blue)](https://forum.opensearch.org/c/opensearch/)\n[![Documentation](https://img.shields.io/badge/documentation-reference-blue)](https://opensearch.org/docs/latest/opensearch/index/)\n[![Code Coverage](https://codecov.io/gh/opensearch-project/OpenSearch/branch/main/graph/badge.svg)](https://codecov.io/gh/opensearch-project/OpenSearch)\n[![Untriaged Issues](https://img.shields.io/github/issues/opensearch-project/OpenSearch/untriaged?labelColor=red)](https://github.com/opensearch-project/OpenSearch/issues?q=is%3Aissue+is%3Aopen+label%3A\"untriaged\")\n[![Security Vulnerabilities](https://img.shields.io/github/issues/opensearch-project/OpenSearch/security%20vulnerability?labelColor=red)](https://github.com/opensearch-project/OpenSearch/issues?q=is%3Aissue+is%3Aopen+label%3A\"security%20vulnerability\")\n[![Open Issues](https://img.shields.io/github/issues/opensearch-project/OpenSearch)](https://github.com/opensearch-project/OpenSearch/issues)\n[![Open Pull Requests](https://img.shields.io/github/issues-pr/opensearch-project/OpenSearch)](https://github.com/opensearch-project/OpenSearch/pulls)\n[![2.19.0 Open Issues](https://img.shields.io/github/issues/opensearch-project/OpenSearch/v2.19.0)](https://github.com/opensearch-project/OpenSearch/issues?q=is%3Aissue+is%3Aopen+label%3A\"v2.19.0\")\n[![2.18.1 Open Issues](https://img.shields.io/github/issues/opensearch-project/OpenSearch/v2.18.1)](https://github.com/opensearch-project/OpenSearch/issues?q=is%3Aissue+is%3Aopen+label%3A\"v2.18.1\")\n[![3.0.0 Open Issues](https://img.shields.io/github/issues/opensearch-project/OpenSearch/v3.0.0)](https://github.com/opensearch-project/OpenSearch/issues?q=is%3Aissue+is%3Aopen+label%3A\"v3.0.0\")\n[![GHA gradle check](https://github.com/opensearch-project/OpenSearch/actions/workflows/gradle-check.yml/badge.svg)](https://github.com/opensearch-project/OpenSearch/actions/workflows/gradle-check.yml)\n[![GHA validate pull request](https://github.com/opensearch-project/OpenSearch/actions/workflows/wrapper.yml/badge.svg)](https://github.com/opensearch-project/OpenSearch/actions/workflows/wrapper.yml)\n[![GHA precommit](https://github.com/opensearch-project/OpenSearch/actions/workflows/precommit.yml/badge.svg)](https://github.com/opensearch-project/OpenSearch/actions/workflows/precommit.yml)\n[![Jenkins gradle check job](https://img.shields.io/jenkins/build?jobUrl=https%3A%2F%2Fbuild.ci.opensearch.org%2Fjob%2Fgradle-check%2F&label=Jenkins%20Gradle%20Check)](https://build.ci.opensearch.org/job/gradle-check/)\n\n- [Welcome!](#welcome)\n- [Project Resources](#project-resources)\n- [Code of Conduct](#code-of-conduct)\n- [Security](#security)\n- [License](#license)\n- [Copyright](#copyright)\n- [Trademark](#trademark)\n\n## Welcome!\n\n**OpenSearch** is [a community-driven, open source fork](https://aws.amazon.com/blogs/opensource/introducing-opensearch/) of [Elasticsearch](https://en.wikipedia.org/wiki/Elasticsearch) and [Kibana](https://en.wikipedia.org/wiki/Kibana) following the [license change](https://blog.opensource.org/the-sspl-is-not-an-open-source-license/) in early 2021. We're looking to sustain (and evolve!) a search and analytics suite for the multitude of businesses who are dependent on the rights granted by the original, [Apache v2.0 License](LICENSE.txt).\n\n## Project Resources\n\n* [Project Website](https://opensearch.org/)\n* [Downloads](https://opensearch.org/downloads.html)\n* [Documentation](https://opensearch.org/docs/)\n* Need help? Try [Forums](https://discuss.opendistrocommunity.dev/)\n* [Project Principles](https://opensearch.org/#principles)\n* [Contributing to OpenSearch](CONTRIBUTING.md)\n* [Maintainer Responsibilities](MAINTAINERS.md)\n* [Release Management](RELEASING.md)\n* [Admin Responsibilities](ADMINS.md)\n* [Testing](TESTING.md)\n* [Security](SECURITY.md)\n\n## Code of Conduct\n\nThis project has adopted the [Amazon Open Source Code of Conduct](CODE_OF_CONDUCT.md). For more information see the [Code of Conduct FAQ](https://aws.github.io/code-of-conduct-faq), or contact [opensource-codeofconduct@amazon.com](mailto:opensource-codeofconduct@amazon.com) with any additional questions or comments.\n\n## Security\nIf you discover a potential security issue in this project we ask that you notify OpenSearch Security directly via email to security@opensearch.org. Please do **not** create a public GitHub issue.\n\n## License\n\nThis project is licensed under the [Apache v2.0 License](LICENSE.txt).\n\n## Copyright\n\nCopyright OpenSearch Contributors. See [NOTICE](NOTICE.txt) for details.\n\n## Trademark\n\nOpenSearch is a registered trademark of Amazon Web Services.\n\nOpenSearch includes certain Apache-licensed Elasticsearch code from Elasticsearch B.V. and other source code. Elasticsearch B.V. is not the source of that other source code. ELASTICSEARCH is a registered trademark of Elasticsearch B.V.\n"
        },
        {
          "name": "RELEASING.md",
          "type": "blob",
          "size": 0.1591796875,
          "content": "## Releasing\n\nThis project follows [OpenSearch project branching, labelling, and releasing](https://github.com/opensearch-project/.github/blob/main/RELEASING.md).\n"
        },
        {
          "name": "SECURITY.md",
          "type": "blob",
          "size": 0.2216796875,
          "content": "## Reporting a Vulnerability\n\nIf you discover a potential security issue in this project we ask that you notify OpenSearch Security directly via email to security@opensearch.org. Please do **not** create a public GitHub issue.\n"
        },
        {
          "name": "TESTING.md",
          "type": "blob",
          "size": 34.482421875,
          "content": "OpenSearch uses [jUnit](https://junit.org/junit5/) for testing, it also uses randomness in the tests, that can be set using a seed. The following is a cheatsheet of options for running the tests for OpenSearch.\n\n- [Requirements](#requirements)\n- [Creating packages](#creating-packages)\n  - [Running OpenSearch from a checkout](#running-opensearch-from-a-checkout)\n    - [Launching and debugging from an IDE](#launching-and-debugging-from-an-ide)\n    - [Other useful arguments](#other-useful-arguments)\n  - [Test case filtering](#test-case-filtering)\n  - [Seed and repetitions](#seed-and-repetitions)\n  - [Repeats *all* tests of ClassName N times](#repeats-all-tests-of-classname-n-times)\n  - [Repeats *all* tests of ClassName N times](#repeats-all-tests-of-classname-n-times-1)\n  - [Repeats a given test N times](#repeats-a-given-test-n-times)\n  - [Test groups](#test-groups)\n  - [Load balancing and caches](#load-balancing-and-caches)\n  - [Test compatibility](#test-compatibility)\n  - [Retries](#retries)\n  - [Miscellaneous](#miscellaneous)\n- [Running verification tasks](#running-verification-tasks)\n- [Testing the REST layer](#testing-the-rest-layer)\n  - [Running REST Tests Against An External Cluster](#running-rest-tests-against-an-external-cluster)\n  - [Debugging REST Tests](#debugging-rest-tests)\n- [Testing packaging](#testing-packaging)\n  - [Testing packaging on Windows](#testing-packaging-on-windows)\n  - [Testing VMs are disposable](#testing-vms-are-disposable)\n  - [Iterating on packaging tests](#iterating-on-packaging-tests)\n- [Testing backwards compatibility](#testing-backwards-compatibility)\n  - [BWC Testing against a specific remote/branch](#bwc-testing-against-a-specific-remotebranch)\n  - [BWC Testing with security](#bwc-testing-with-security)\n    - [Skip fetching latest](#skip-fetching-latest)\n- [How to write good tests?](#how-to-write-good-tests)\n  - [Base classes for test cases](#base-classes-for-test-cases)\n  - [Good practices](#good-practices)\n    - [What kind of tests should I write?](#what-kind-of-tests-should-i-write)\n    - [Refactor code to make it easier to test](#refactor-code-to-make-it-easier-to-test)\n  - [Bad practices](#bad-practices)\n    - [Use randomized-testing for coverage](#use-randomized-testing-for-coverage)\n    - [Abuse randomization in multi-threaded tests](#abuse-randomization-in-multi-threaded-tests)\n    - [Use `Thread.sleep`](#use-threadsleep)\n    - [Expect a specific segment topology](#expect-a-specific-segment-topology)\n    - [Leave environment in an unstable state after test](#leave-environment-in-an-unstable-state-after-test)\n- [Test coverage analysis](#test-coverage-analysis)\n- [Testing with plugins](#testing-with-plugins)\n- [Environment misc](#environment-misc)\n\n# Requirements\n\nYou will need the following pieces of software to run these tests:\n\n- Docker & Docker Compose\n- Vagrant\n- JDK 11\n- Gradle\n\n# Creating packages\n\nTo create a distribution without running the tests, run the following:\n\n    ./gradlew assemble\n\nTo create a platform-specific build, use the following depending on your operating system:\n\n    ./gradlew :distribution:archives:linux-tar:assemble\n    ./gradlew :distribution:archives:darwin-tar:assemble\n    ./gradlew :distribution:archives:windows-zip:assemble\n\n## Running OpenSearch from a checkout\n\nIn order to run OpenSearch from source without building a package, you can run it using Gradle:\n\n    ./gradlew run\n\n### Launching and debugging from an IDE\n\n**NOTE:** If you have imported the project into IntelliJ according to the instructions in [DEVELOPER_GUIDE.md](DEVELOPER_GUIDE.md#intellij-idea), then a debug run configuration named **Debug OpenSearch** will be created for you and configured appropriately.\n\nTo run OpenSearch in debug mode,\n\n1. Start the `Debug OpenSearch` in IntelliJ by pressing the debug icon.\n2. From a terminal run the following `./gradlew run --debug-jvm`. You can also run this task in IntelliJ.\n\nThis will instruct all JVMs (including any that run cli tools such as creating the keyring or adding users) to suspend and initiate a debug connection on port incrementing from `5005`. As such, the IDE needs to be instructed to listen for connections on this port. Since we might run multiple JVMs as part of configuring and starting the cluster, it's recommended to configure the IDE to initiate multiple listening attempts. In case of IntelliJ, this option is called \"Auto restart\" and needs to be checked. In case of Eclipse, \"Connection limit\" setting needs to be configured with a greater value (ie 10 or more).\n\n### Other useful arguments\n\n-   In order to start a node with a different max heap space add: `-Dtests.heap.size=4G`\n-   In order to disable assertions add: `-Dtests.asserts=false`\n-   In order to use a custom data directory: `--data-dir=/tmp/foo`\n-   In order to preserve data in between executions: `--preserve-data`\n-   In order to remotely attach a debugger to the process: `--debug-jvm`\n-   In order to set a different keystore password: `--keystore-password yourpassword`\n-   In order to set an OpenSearch setting, provide a setting with the following prefix: `-Dtests.opensearch.`\n-   In order to enable stack trace of the MockSpanData during testing, add: `-Dtests.telemetry.span.stack_traces=true` (Storing stack traces alongside span data can be useful for comprehensive debugging and performance optimization during testing, as it provides insights into the exact code paths and execution sequences, facilitating efficient issue identification and resolution. Note: Enabling this might lead to OOM issues while running ITs)\n\n## Test case filtering\n\nTo be able to run a single test you need to specify the module where you're running the tests from.\n\nExample: `./gradlew server:test --tests \"*.ReplicaShardBatchAllocatorTests.testNoAsyncFetchData\"`\n\nRun a single test case (variants)\n\n    ./gradlew module:test --tests org.opensearch.package.ClassName\n    ./gradlew module:test --tests org.opensearch.package.ClassName.testName\n    ./gradlew module:test --tests \"*.ClassName\"\n\nRun all tests in a package and its sub-packages\n\n    ./gradlew module:test --tests \"org.opensearch.package.*\"\n\nRun any test methods that contain *esi* (e.g.: .r*esi*ze.)\n\n    ./gradlew module:test --tests \"*esi*\"\n\nRun all tests that are waiting for a bugfix (disabled by default)\n\n    ./gradlew test -Dtests.filter=@awaitsfix\n\n## Seed and repetitions\n\nRun with a given seed (seed is a hex-encoded long).\n\n    ./gradlew test -Dtests.seed=DEADBEEF\n\n## Repeats *all* tests of ClassName N times\n\nEvery test repetition will have a different method seed (derived from a single random master seed).\n\n    ./gradlew test -Dtests.iters=N -Dtests.class=*.ClassName\n\n## Repeats *all* tests of ClassName N times\n\nEvery test repetition will have exactly the same master (0xdead) and method-level (0xbeef) seed.\n\n    ./gradlew test -Dtests.iters=N -Dtests.class=*.ClassName -Dtests.seed=DEAD:BEEF\n\n## Repeats a given test N times\n\nNote that individual test repetitions are passed suffixes, such as: `testFoo[0]`, `testFoo[1]`, etc. Thus using `testmethod` or `tests.method` ending in a glob is necessary to ensure iterations are run.\n\n    ./gradlew test -Dtests.iters=N -Dtests.class=*.ClassName -Dtests.method=mytest*\n\nRepeats N times but skips any tests after the first failure or M initial failures.\n\n    ./gradlew test -Dtests.iters=N -Dtests.failfast=true -Dtestcase=...\n    ./gradlew test -Dtests.iters=N -Dtests.maxfailures=M -Dtestcase=...\n\n## Test groups\n\nTest groups can be enabled or disabled (true/false).\n\nDefault value provided below in \\[brackets\\].\n\n    ./gradlew test -Dtests.awaitsfix=[false] - known issue (@AwaitsFix)\n\n## Load balancing and caches\n\nBy default, the tests run on multiple processes using all the available cores on all available CPUs. Not including hyper-threading. If you want to explicitly specify the number of JVMs you can do so on the command line:\n\n    ./gradlew test -Dtests.jvms=8\n\nOr in `~/.gradle/gradle.properties`:\n\n    systemProp.tests.jvms=8\n\nIt's difficult to pick the \"right\" number here. Hypercores dont count for CPU intensive tests, and you should leave some slack for JVM-internal threads like the garbage collector. And you have to have enough RAM to handle each JVM.\n\n## Test compatibility\n\nIt is possible to provide a version that allows to adapt the tests' behaviour to older features or bugs that have been changed or fixed in the meantime.\n\n    ./gradlew test -Dtests.compatibility=1.0.0\n\n## Retries\n\nThe goal of tests is to be completely deterministic such that any test failure can be easily and reliably reproduced. However, the reality is that many OpenSearch integration tests have non-deterministic behavior which results in rare test failures that cannot be easily reproduced even using the same random test seed. To mitigate the pain of frequent non-reproducible test failures, limited retries have been introduced using the Gradle [test-retry](https://plugins.gradle.org/plugin/org.gradle.test-retry) plugin. The known flaky tests are explicitly listed in the test-retry configuration of the build.gradle file. This is intended as a temporary mitigation for existing flakiness, and as such new tests should not be added to the retry list. Any new addition to the retry list must provide a thorough rationale as to why adding retries is the right thing to do as opposed to fixing the underlying flakiness. Existing flaky tests are tracked in GitHub with the [Flaky Random Test Failure](https://github.com/opensearch-project/OpenSearch/issues?q=is%3Aopen+is%3Aissue+label%3A%22flaky-test%22) label.\n\n## Miscellaneous\n\nRun all tests without stopping on errors (inspect log files).\n\n    ./gradlew test -Dtests.haltonfailure=false\n\nRun more verbose output (JVM parameters, etc.).\n\n    ./gradlew test -verbose\n\nChange the default suite timeout to 5 seconds for all tests (note the exclamation mark).\n\n    ./gradlew test -Dtests.timeoutSuite=5000! ...\n\nChange the logging level of OpenSearch (not Gradle)\n\n    ./gradlew test -Dtests.opensearch.logger.level=DEBUG\n\nPrint all the logging output from the test runs to the command line even if tests are passing.\n\n    ./gradlew test -Dtests.output=true\n\nConfigure the heap size.\n\n    ./gradlew test -Dtests.heap.size=512m\n\nPass arbitrary jvm arguments.\n\n    # specify heap dump path\n    ./gradlew test -Dtests.jvm.argline=\"-XX:HeapDumpPath=/path/to/heapdumps\"\n    # enable gc logging\n    ./gradlew test -Dtests.jvm.argline=\"-verbose:gc\"\n    # enable security debugging\n    ./gradlew test -Dtests.jvm.argline=\"-Djava.security.debug=access,failure\"\n\n# Running verification tasks\n\nTo run all verification tasks, including static checks, unit tests, and integration tests:\n\n    ./gradlew check\n\nNote that this will also run the unit tests and precommit tasks first. If you want to just run the in memory cluster integration tests (because you are debugging them):\n\n    ./gradlew internalClusterTest\n\nTo run a specific set of tests.\n\n    ./gradlew :server:internalClusterTest --tests \"org.opensearch.common.settings.FallbackSettingsIT.*\"\n\nIf you want to just run the precommit checks:\n\n    ./gradlew precommit\n\nSome of these checks will require `docker-compose` installed for bringing up test fixtures. If its not present those checks will be skipped automatically.\n\n# Testing the REST layer\n\nThe REST layer is tested through specific tests that are executed against a cluster that is configured and initialized via Gradle. The tests themselves can be written in either Java or with a YAML based DSL.\n\nYAML based REST tests should be preferred since these are shared between clients. The YAML based tests describe the operations to be executed, and the obtained results that need to be tested.\n\nThe YAML tests support various operators defined in the [rest-api-spec](/rest-api-spec/src/main/resources/rest-api-spec/test/README.md) and adhere to the [OpenSearch REST API JSON specification](/rest-api-spec/README.md). In order to run the YAML tests, the relevant API specification needs to be on the test classpath. Any gradle project that has support for REST tests will get the primary API on its class path. However, to better support Gradle incremental builds, it is recommended to explicitly declare which parts of the API the tests depend upon.\n\nFor example:\n\n    restResources {\n      restApi {\n        includeCore '_common', 'indices', 'index', 'cluster', 'nodes', 'get', 'ingest'\n      }\n    }\n\nThe REST tests are run automatically when executing the \"./gradlew check\" command. To run only the YAML REST tests use the following command (modules and plugins may also include YAML REST tests):\n\n    ./gradlew :rest-api-spec:yamlRestTest\n\nA specific test case can be run with the following command:\n\n    ./gradlew ':rest-api-spec:yamlRestTest' \\\n      --tests \"org.opensearch.test.rest.ClientYamlTestSuiteIT\" \\\n      -Dtests.method=\"test {p0=cat.segments/10_basic/Help}\"\n\nThe YAML REST tests support all the options provided by the randomized runner, plus the following:\n\n-   `tests.rest.suite`: comma separated paths of the test suites to be run (by default loaded from /rest-api-spec/test). It is possible to run only a subset of the tests providing a sub-folder or even a single yaml file (the default /rest-api-spec/test prefix is optional when files are loaded from classpath) e.g. `-Dtests.rest.suite=index,get,create/10_with_id`\n\n-   `tests.rest.denylist`: comma separated globs that identify tests that are denylisted and need to be skipped e.g. `-Dtests.rest.denylist=index/**/Index document,get/10_basic/**`\n\nJava REST tests can be run with the \"javaRestTest\" task.\n\nFor example :\n\n    ./gradlew :modules:mapper-extras:javaRestTest\n\n    ./gradlew ':modules:mapper-extras:javaRestTest' \\\n      --tests \"org.opensearch.index.mapper.TokenCountFieldMapperIntegrationIT.testSearchByTokenCount {storeCountedFields=true loadCountedFields=false}\"\n\nyamlRestTests and javaRestTests are easy to identify, since they are found in a respective source directory. However, there are some more specialized REST tests that use custom task names. These are usually found in \"qa\" projects commonly use the \"integTest\" task.\n\nIf in doubt about which command to use, simply run &lt;gradle path&gt;:check\n\n## Running REST Tests Against An External Cluster\n\nNote that the REST tests, like all the integration tests, can be run against an external cluster by specifying the following properties `tests.cluster`, `tests.rest.cluster`, `tests.clustername`. Use a comma separated list of node properties for the multi-node cluster.\n\nFor example :\n\n    ./gradlew :rest-api-spec:yamlRestTest \\\n      -Dtests.cluster=localhost:9200 -Dtests.rest.cluster=localhost:9200 -Dtests.clustername=opensearch\n\n## Debugging REST Tests\n\nYou can launch a local OpenSearch cluster in debug mode following [Launching and debugging from an IDE](#launching-and-debugging-from-an-ide), and run your REST tests against that following [Running REST Tests Against An External Cluster](#running-rest-tests-against-an-external-cluster).\n\n# Testing packaging\n\nThe packaging tests use Vagrant virtual machines or cloud instances to verify that installing and running OpenSearch distributions works correctly on supported operating systems. These tests should really only be run on ephemeral systems because theyre destructive; that is, these tests install and remove packages and freely modify system settings, so you will probably regret it if you execute them on your development machine.\n\nWhen you run a packaging test, Gradle will set up the target VM and mount your repository directory in the VM. Once this is done, a Gradle task will issue a Vagrant command to run a **nested** Gradle task on the VM. This nested Gradle runs the actual \"destructive\" test classes.\n\n1.  Install Virtual Box and Vagrant.\n\n2.  (Optional) Install [vagrant-cachier](https://github.com/fgrehm/vagrant-cachier) to squeeze a bit more performance out of the process (Note: as of 2021, vagrant-cachier is unmaintained):\n\n        vagrant plugin install vagrant-cachier\n\n3.  You can run all the OS packaging tests with `./gradlew packagingTest`. This task includes our legacy `bats` tests.\n\n    To run only the OS tests that are written in Java, run `.gradlew distroTest`, will cause Gradle to build the tar, zip, and deb packages and all the plugins. It will then run the tests on every available system. This will take a very long time.\n\n    Fortunately, the various systems under test have their own Gradle tasks under `qa/os`. To find out what packaging combinations can be tested on a system, run the `tasks` task. For example:\n\n        ./gradlew :qa:os:ubuntu-1804:tasks\n\n    If you want a quick test of the tarball and RPM packaging for Centos 7, you would run:\n\n        ./gradlew :qa:os:centos-7:distroTest.rpm :qa:os:centos-7:distroTest.linux-archive\n\nNote that if you interrupt Gradle in the middle of running these tasks, any boxes started will remain running, and youll have to stop them manually with `./gradlew --stop` or `vagrant halt`.\n\nAll the regular vagrant commands should just work, so you can get a shell in a VM running trusty by running `vagrant up ubuntu-1604 --provider virtualbox && vagrant ssh ubuntu-1604`.\n\nThese are the linux flavors supported, all of which we provide images for\n\n-   ubuntu-1604 aka xenial\n-   ubuntu-1804 aka bionic beaver\n-   debian-8 aka jessie\n-   debian-9 aka stretch, the current debian stable distribution\n-   centos-6\n-   centos-7\n-   rhel-8\n-   fedora-28\n-   fedora-29\n-   oel-6 aka Oracle Enterprise Linux 6\n-   oel-7 aka Oracle Enterprise Linux 7\n-   sles-12\n-   opensuse-42 aka Leap\n\nWere missing the following from the support matrix because there are no high quality boxes available in vagrant atlas:\n\n-   sles-11\n\n## Testing packaging on Windows\n\nThe packaging tests also support Windows Server 2012R2 and Windows Server 2016. Unfortunately were not able to provide boxes for them in open source use because of licensing issues. Any Virtualbox image that has WinRM and Powershell enabled for remote users should work.\n\nTesting on Windows requires the [vagrant-winrm](https://github.com/criteo/vagrant-winrm) plugin.\n\n    vagrant plugin install vagrant-winrm\n\nSpecify the image IDs of the Windows boxes to gradle with the following project properties. They can be set in `~/.gradle/gradle.properties` such as\n\n    vagrant.windows-2012r2.id=my-image-id\n    vagrant.windows-2016.id=another-image-id\n\nor passed on the command line such as `-Pvagrant.windows-2012r2.id=my-image-id` or `-Pvagrant.windows-2016=another-image-id`\n\nThese properties are required for Windows support in all gradle tasks that handle packaging tests. Either or both may be specified.\n\nIf youre running vagrant commands outside of gradle, specify the Windows boxes with the environment variables.\n\n-   `VAGRANT_WINDOWS_2012R2_BOX`\n-   `VAGRANT_WINDOWS_2016_BOX`\n\n## Testing VMs are disposable\n\nIts important to think of VMs like cattle. If they become lame you just shoot them and let vagrant reprovision them. Say youve hosed your precise VM:\n\n    vagrant ssh ubuntu-1604 -c 'sudo rm -rf /bin'; echo oops\n\nAll youve got to do to get another one is\n\n    vagrant destroy -f ubuntu-1604 && vagrant up ubuntu-1604 --provider virtualbox\n\nThe whole process takes a minute and a half on a modern laptop, two and a half without vagrant-cachier.\n\nSome vagrant commands will work on all VMs at once:\n\n    vagrant halt\n    vagrant destroy -f\n\n`vagrant up` would normally start all the VMs, but weve prevented that because thatd consume a ton of ram.\n\n## Iterating on packaging tests\n\nBecause our packaging tests are capable of testing many combinations of OS (e.g., Windows, Linux, etc.), package type (e.g., zip file, RPM, etc.) and so forth, its faster to develop against smaller subsets of the tests. For example, to run tests for the default archive distribution on Fedora 28:\n\n    ./gradlew :qa:os:fedora-28:distroTest.linux-archive\n\nThese test tasks can use the `--tests`, `--info`, and `--debug` parameters just like non-OS tests can. For example:\n\n    ./gradlew :qa:os:fedora-28:distroTest.linux-archive --tests \"com.opensearch.packaging.test.ArchiveTests\"\n\n# Testing backwards compatibility\n\nBackwards compatibility tests exist to test upgrading from each supported version to the current version.\n\nThe test can be run for any versions which the current version will be compatible with. Tests are run for released versions download the distributions from the artifact repository, see [DistributionDownloadPlugin](./buildSrc/src/main/java/org/opensearch/gradle/DistributionDownloadPlugin.java) for the repository location. Tests are run for versions that are not yet released automatically check out the branch and build from source to get the distributions, see [BwcVersions](./buildSrc/src/main/java/org/opensearch/gradle/BwcVersions.java) and [distribution/bwc/build.gradle](./distribution/bwc/build.gradle) for more information.\n\nThe minimum JDK versions for runtime and compiling need to be installed, and environment variables `JAVAx_HOME`, such as `JAVA8_HOME`, pointing to the JDK installations are required to run the tests against unreleased versions, since the distributions are created by building from source. The required JDK versions for each branch are located at [.ci/java-versions.properties](.ci/java-versions.properties), see [BwcSetupExtension](./buildSrc/src/main/java/org/opensearch/gradle/internal/BwcSetupExtension.java) for more information.\n\nTo run all the backwards compatibility tests use:\n\n    ./gradlew bwcTest\n\nA specific version can be tested as well. For example, to test bwc with version 5.3.2 run:\n\n    ./gradlew v5.3.2#bwcTest\n\nUse -Dtest.class and -Dtests.method to run a specific bwcTest test. For example to test a rolling upgrade from 7.7.0:\n\n    ./gradlew :qa:rolling-upgrade:v7.7.0#bwcTest \\\n     -Dtests.class=org.opensearch.upgrades.RecoveryIT \\\n     -Dtests.method=testHistoryUUIDIsGenerated\n\nUse `-PcustomDistributionDownloadType=bundle` to run the bwcTest against the test cluster with latest CI distribution bundle set up for the specified version; this property is default to min and exclusive choices between `bundle` and `min`:\n\n    ./gradlew bwcTest -PcustomDistributionDownloadType=bundle\n\nWhen running `./gradlew check`, minimal bwc checks are also run against compatible versions that are not yet released.\n\n## BWC Testing against a specific remote/branch\n\nSometimes a backward compatibility change spans two versions. A common case is a new functionality that needs a BWC bridge in an unreleased versioned of a release branch (for example, 5.x). To test the changes, you can instruct Gradle to build the BWC version from a another remote/branch combination instead of pulling the release branch from GitHub. You do so using the `bwc.remote` and `bwc.refspec.BRANCH` system properties:\n\n    ./gradlew check -Dbwc.remote=${remote} -Dbwc.refspec.5.x=index_req_bwc_5.x\n\nThe branch needs to be available on the remote that the BWC makes of the repository you run the tests from. Using the remote is a handy trick to make sure that a branch is available and is up to date in the case of multiple runs.\n\nExample:\n\nSay you need to make a change to `main` and have a BWC layer in `5.x`. You will need to: . Create a branch called `index_req_change` off your remote `${remote}`. This will contain your change. . Create a branch called `index_req_bwc_5.x` off `5.x`. This will contain your bwc layer. . Push both branches to your remote repository. . Run the tests with `./gradlew check -Dbwc.remote=${remote} -Dbwc.refspec.5.x=index_req_bwc_5.x`.\n\n## BWC Testing with security\n\nYou may want to run BWC tests for a secure OpenSearch cluster. In order to do this, you will need to follow a few additional steps:\n\n1. Clone the OpenSearch Security repository from https://github.com/opensearch-project/security.\n2. Get both the old version of the Security plugin (the version you wish to come from) and the new version of the Security plugin (the version you wish to go to). This can be done either by fetching the maven artifact with a command like `wget https://repo1.maven.org/maven2/org/opensearch/plugin/opensearch-security/<TARGET_VERSION>.0/opensearch-security-<TARGET_VERSION>.0.zip` or by running `./gradlew assemble` from the base of the Security repository.\n3. Move both of the Security artifacts into new directories at the path `/security/bwc-test/src/test/resources/<TARGET_VERSION>.0`. You should end up with two different directories in `/security/bwc-test/src/test/resources/`, one named the old version and one the new version.\n4. Run the following command from the base of the Security repository:\n\n```\n  ./gradlew -p bwc-test clean bwcTestSuite \\\n  -Dtests.security.manager=false \\\n  -Dtests.opensearch.http.protocol=https \\\n  -Dtests.opensearch.username=admin \\\n  -Dtests.opensearch.password=admin \\\n  -PcustomDistributionUrl=\"/OpenSearch/distribution/archives/linux-tar/build/distributions/opensearch-min-<TARGET_VERSION>-SNAPSHOT-linux-x64.tar.gz\" \\\n  -i\n```\n\n`-Dtests.security.manager=false` handles access issues when attempting to read the certificates from the file system.\n`-Dtests.opensearch.http.protocol=https` tells the wait for cluster startup task to do the right thing.\n`-PcustomDistributionUrl=...` uses a custom build of the distribution of OpenSearch. This is unnecessary when running against standard/unmodified OpenSearch core distributions.\n\n### Skip fetching latest\n\nFor some BWC testing scenarios, you want to use the local clone of the repository without fetching latest. For these use cases, you can set the system property `tests.bwc.git_fetch_latest` to `false` and the BWC builds will skip fetching the latest from the remote.\n\n# How to write good tests?\n\n## Base classes for test cases\n\nThere are multiple base classes for tests:\n\n-   **`OpenSearchTestCase`**: The base class of all tests. It is typically extended directly by unit tests.\n-   **`OpenSearchSingleNodeTestCase`**: This test case sets up a cluster that has a single node.\n-   **`OpenSearchIntegTestCase`**: An integration test case that creates a cluster that might have multiple nodes.\n-   **`OpenSearchRestTestCase`**: An integration tests that interacts with an external cluster via the REST API. This is used for Java based REST tests.\n-   **`OpenSearchClientYamlSuiteTestCase`** : A subclass of `OpenSearchRestTestCase` used to run YAML based REST tests.\n\n## Good practices\n\n### What kind of tests should I write?\n\nUnit tests are the preferred way to test some functionality: most of the time they are simpler to understand, more likely to reproduce, and unlikely to be affected by changes that are unrelated to the piece of functionality that is being tested.\n\nThe reason why `OpenSearchSingleNodeTestCase` exists is that all our components used to be very hard to set up in isolation, which had led us to having a number of integration tests but close to no unit tests. `OpenSearchSingleNodeTestCase` is a workaround for this issue which provides an easy way to spin up a node and get access to components that are hard to instantiate like `IndicesService`. Whenever practical, you should prefer unit tests.\n\nFinally, if the functionality under test needs to be run in a cluster, there are two test classes to consider:\n  * `OpenSearchRestTestCase` will connect to an external cluster. This is a good option if the tests cases don't rely on a specific configuration of the test cluster. A test cluster is set up as part of the Gradle task running integration tests, and test cases using this class can connect to it. The configuration of the cluster is provided in the Gradle files.\n  * `OpenSearchIntegTestCase` will create a local cluster as part of each test case. The configuration of the cluster is controlled by the test class. This is a good option if different tests cases depend on different cluster configurations, as it would be impractical (and limit parallelization) to keep re-configuring (and re-starting) the external cluster for each test case. A good example of when this class might come in handy is for testing security features, where different cluster configurations are needed to fully test each one.\n\nIn short, most new functionality should come with unit tests, and optionally integration tests using either an external cluster or a local one if there's a need for more specific cluster configurations, as those are more costly and harder to maintain/debug.\n\n### Refactor code to make it easier to test\n\nUnfortunately, a large part of our code base is still hard to unit test. Sometimes because some classes have lots of dependencies that make them hard to instantiate. Sometimes because API contracts make tests hard to write. Code refactors that make functionality easier to unit test are encouraged.\n\n## Bad practices\n\n### Use randomized-testing for coverage\n\nIn general, randomization should be used for parameters that are not expected to affect the behavior of the functionality that is being tested. For instance the number of shards should not impact `date_histogram` aggregations, and the choice of the `store` type (`niofs` vs `mmapfs`) does not affect the results of a query. Such randomization helps improve confidence that we are not relying on implementation details of one component or specifics of some setup.\n\nHowever, it should not be used for coverage. For instance if you are testing a piece of functionality that enters different code paths depending on whether the index has 1 shards or 2+ shards, then we shouldnt just test against an index with a random number of shards: there should be one test for the 1-shard case, and another test for the 2+ shards case.\n\n### Abuse randomization in multi-threaded tests\n\nMulti-threaded tests are often not reproducible due to the fact that there is no guarantee on the order in which operations occur across threads. Adding randomization to the mix usually makes things worse and should be done with care.\n\n### Use `Thread.sleep`\n\n`Thread.sleep()` is almost always a bad idea because it is very difficult to know that you've waited long enough. Using primitives like `waitUntil` or `assertBusy`, which use Thread.sleep internally, is okay to wait for a specific condition. However, it is almost always better to instrument your code with concurrency primitives like a `CountDownLatch` that will allow you to deterministically wait for a specific condition, without waiting longer than necessary that will happen with a polling approach used by `assertBusy`.\n\nExample:\n- [PrimaryShardAllocatorIT](https://github.com/opensearch-project/OpenSearch/blob/7ffcd6500e0bd5956cef5c289ee66d9f99d533fc/server/src/internalClusterTest/java/org/opensearch/gateway/ReplicaShardAllocatorIT.java#L208-L235): This test is using two latches: one to wait for a recovery to start and one to block that recovery so that it can deterministically test things that happen during a recovery.\n\n### Expect a specific segment topology\n\nBy design, OpenSearch integration tests will vary how the merge policy works because in almost all scenarios you should not depend on a specific segment topology (in the real world your code will see a huge diversity of indexing workloads with OpenSearch merging things in the background all the time!). If you do in fact need to care about the segment topology (e.g. for testing statistics that might vary slightly depending on number of segments), then you must take care to ensure that segment topology is deterministic by doing things like disabling background refreshes, force merging after indexing data, etc.\n\nExample:\n- [SegmentReplicationResizeRequestIT](https://github.com/opensearch-project/OpenSearch/blob/f715ee1a485e550802accc1c2e3d8101208d4f0b/server/src/internalClusterTest/java/org/opensearch/indices/replication/SegmentReplicationResizeRequestIT.java#L102-L109): This test disables refreshes to prevent interfering with the segment replication behavior under test.\n\n### Leave environment in an unstable state after test\n\nThe default test case will ensure that no open file handles or running threads are left after tear down. You must ensure that all resources are cleaned up at the end of each test case, or else the cleanup may end up racing with the tear down logic in the base test class in a way that is very difficult to reproduce.\n\nExample:\n- [AwarenessAttributeDecommissionIT](https://github.com/opensearch-project/OpenSearch/blob/main/server/src/internalClusterTest/java/org/opensearch/cluster/coordination/AwarenessAttributeDecommissionIT.java#L951): Recommissions any decommissioned nodes at the end of the test to ensure the after-test checks succeed.\n\n# Test coverage analysis\n\nThe code coverage report can be generated through Gradle with [JaCoCo plugin](https://docs.gradle.org/current/userguide/jacoco_plugin.html).\n\nFor unit test:\n\n    ./gradlew codeCoverageReportForUnitTest\n\nFor integration test:\n\n    ./gradlew codeCoverageReportForIntegrationTest\n\nFor the combined tests (unit and integration):\n\n    ./gradlew codeCoverageReport\n\nTo generate coverage report for the combined tests after `check` task:\n\n    ./gradlew check -Dtests.coverage=true\n\nThe code coverage report will be generated in `build/codeCoverageReport`, `build/codeCoverageReportForUnitTest` or `build/codeCoverageReportForIntegrationTest` correspondingly.\n\nThe report will be in XML format only by default, but you can add the following parameter for HTML and CSV format.\n\n- To generate report in HTML format: `-Dtests.coverage.report.html=true`\n- To generate report in CSV format: `-Dtests.coverage.report.csv=true`\n- To NOT generate report in XML format: `-Dtests.coverage.report.xml=false`\n\nFor example, to generate code coverage report in HTML format and not in XML format:\n\n    ./gradlew codeCoverageReport -Dtests.coverage.report.html=true -Dtests.coverage.report.xml=false\n\nApart from using Gradle, it is also possible to gain insight in code coverage using IntelliJs built-in coverage analysis tool that can measure coverage upon executing specific tests. Eclipse may also be able to do the same using the EclEmma plugin.\n\nPlease read your IDE documentation for how to attach a debugger to a JVM process.\n\n# Testing with plugins\n\nTo test a plugin with a custom build of OpenSearch, build OpenSearch and use the `customDistributionUrl` setting supported by each plugin to override the OpenSearch distribution.\n\nFor example, in your OpenSearch repository assemble a custom distribution.\n\n    ./gradlew :distribution:archives:linux-tar:assemble\n\nThen in your plugin repository, substitute in your OpenSearch build\n\n    ./gradlew run -PcustomDistributionUrl=\"<OPENSEARCH-REPO-PATH>/distribution/archives/linux-tar/build/distributions/opensearch-min-3.0.0-SNAPSHOT-linux-x64.tar.gz\"\n\n# Environment misc\n\nThere is a known issue with macOS localhost resolve strategy that can cause some integration tests to fail. This is because integration tests have timings for cluster formation, discovery, etc. that can be exceeded if name resolution takes a long time. To fix this, make sure you have your computer name (as returned by `hostname`) inside `/etc/hosts`, e.g.:\n\n    127.0.0.1       localhost OpenSearchMBP.local\n    255.255.255.255 broadcasthost\n    ::1             localhost OpenSearchMBP.local`\n"
        },
        {
          "name": "TRIAGING.md",
          "type": "blob",
          "size": 11.6552734375,
          "content": "<img src=\"https://opensearch.org/assets/img/opensearch-logo-themed.svg\" height=\"64px\">\n\nThe maintainers of the OpenSearch Repo seek to promote an inclusive and engaged community of contributors. In order to facilitate this, weekly triage meetings are open-to-all and attendance is encouraged for anyone who hopes to contribute, discuss an issue, or learn more about the project. There are several weekly triage meetings scoped to the following component areas: Search, Storage, and Cluster Manager. To learn more about contributing to the OpenSearch Repo visit the [Contributing](./CONTRIBUTING.md) documentation.\n\n### Do I need to attend for my issue to be addressed/triaged?\n\nAttendance is not required for your issue to be triaged or addressed.  If not accepted the issue will be updated with a comment for next steps.  All new issues are triaged weekly.\n\nYou can track if your issue was triaged by watching your GitHub notifications for updates.\n\n### What happens if my issue does not get covered this time?\n\nEach meeting we seek to address all new issues. However, should we run out of time before your issue is discussed, you are always welcome to attend the next meeting or to follow up on the issue post itself.\n\n### How do I join a Triage meeting?\n\n Check the [OpenSearch Meetup Group](https://www.meetup.com/opensearch/) for the latest schedule and details for joining each meeting. Each component area has its own meetup series: [Search](https://www.meetup.com/opensearch/events/300929493/), [Storage](https://www.meetup.com/opensearch/events/299907409/), [Cluster Manager](https://www.meetup.com/opensearch/events/301082218/), and [Indexing](https://www.meetup.com/opensearch/events/301734024/).\n\nAfter joining the virtual meeting, you can enable your video / voice to join the discussion.  If you do not have a webcam or microphone available, you can still join in via the text chat.\n\nIf you have an issue you'd like to bring forth please prepare a link to the issue so it can be presented and viewed by everyone in the meeting.\n\n### Is there an agenda for each week?\n\nMeeting structure may vary slightly, but the general structure is as follows:\n\n1. **Initial Gathering:** Feel free to turn on your video and engage in informal conversation. Shortly, a volunteer triage [facilitator](#what-is-the-role-of-the-facilitator) will begin the meeting and share their screen.\n2. **Record Attendees:** The facilitator will request attendees to share their GitHub profile links. These links will be collected and assembled into a [tag](#how-do-triage-facilitator-tag-comments-during-the-triage-meeting) to annotate comments during the meeting.\n3. **Announcements:** Any announcements will be made at the beginning of the meeting.\n4. **Review of New Issues:** We start by reviewing all untriaged issues. Each meeting has a label-based search to find relevant issues:\n   - [Search](https://github.com/opensearch-project/OpenSearch/issues?q=is%3Aissue+is%3Aopen+label%3Auntriaged+label%3A%22Search%22%2C%22Search%3ARemote+Search%22%2C%22Search%3AResiliency%22%2C%22Search%3APerformance%22%2C%22Search%3ARelevance%22%2C%22Search%3AAggregations%22%2C%22Search%3AQuery+Capabilities%22%2C%22Search%3AQuery+Insights%22%2C%22Search%3ASearchable+Snapshots%22%2C%22Search%3AUser+Behavior+Insights%22)\n   - [Indexing](https://github.com/opensearch-project/OpenSearch/issues?q=is%3Aissue+is%3Aopen+label%3Auntriaged+label%3A%22Indexing%3AReplication%22%2C%22Indexing%22%2C%22Indexing%3APerformance%22%2C%22Indexing+%26+Search%22%2C)\n   - [Storage](https://github.com/opensearch-project/OpenSearch/issues?q=is%3Aissue+is%3Aopen+label%3Auntriaged+label%3AStorage%2C%22Storage%3AResiliency%22%2C%22Storage%3APerformance%22%2C%22Storage%3ASnapshots%22%2C%22Storage%3ARemote%22%2C%22Storage%3ADurability%22)\n   - [Cluster Manager](https://github.com/opensearch-project/OpenSearch/issues?q=is%3Aissue+is%3Aopen+label%3Auntriaged+label%3A%22Cluster+Manager%22%2C%22ClusterManager%3ARemoteState%22%2C%22ShardManagement%3AResiliency%22%2C%22ShardManagement%3AInsights%22%2C%22ShardManagement%3ASizing%22%2C%22ShardManagement%3APerformance%22%2C%22ShardManagement%3APlacement%22%2C%22ShardManagement%3ARouting%22)\n   - [Core](https://github.com/opensearch-project/OpenSearch/issues?q=is%3Aissue+is%3Aopen+label%3Auntriaged+-label%3A%22Search%22%2C%22Search%3ARemote+Search%22%2C%22Search%3AResiliency%22%2C%22Search%3APerformance%22%2C%22Search%3ARelevance%22%2C%22Search%3AAggregations%22%2C%22Search%3AQuery+Capabilities%22%2C%22Search%3AQuery+Insights%22%2C%22Search%3ASearchable+Snapshots%22%2C%22Search%3AUser+Behavior+Insights%22%2C%22Storage%22%2C%22Storage%3AResiliency%22%2C%22Storage%3APerformance%22%2C%22Storage%3ASnapshots%22%2C%22Storage%3ARemote%22%2C%22Storage%3ADurability%22%2C%22Cluster+Manager%22%2C%22ClusterManager%3ARemoteState%22%2C%22ShardManagement%3AResiliency%22%2C%22ShardManagement%3AInsights%22%2C%22ShardManagement%3ASizing%22%2C%22ShardManagement%3APerformance%22%2C%22ShardManagement%3APlacement%22%2C%22ShardManagement%3ARouting%22%2C%22Indexing%3AReplication%22%2C%22Indexing%22%2C%22Indexing%3APerformance%22%2C%22Indexing+%26+Search%22)\n5. **Attendee Requests:** An opportunity for any meeting member to request consideration of an issue or pull request.\n6. **Open Discussion:** Attendees can bring up any topics not already covered by filed issues or pull requests.\n7. **Review of Old Untriaged Issues:** Look at all [untriaged issues older than 14 days](https://peternied.github.io/redirect/issue_search.html?owner=opensearch-project&repo=OpenSearch&tag=untriaged&created-since-days=14) to prevent issues from falling through the cracks.\n\n### What is the role of the facilitator?\n\nThe facilitator is crucial in driving the meeting, ensuring a smooth flow of issues into OpenSearch for future contributions. They maintain the meeting's agenda, solicit input from attendees, and record outcomes using the triage tag as items are discussed.\n\n### Do I need to have already contributed to the project to attend a triage meeting?\n\nNo prior contributions are required. All interested individuals are welcome and encouraged to attend. Triage meetings offer a fantastic opportunity for new contributors to understand the project and explore various contribution avenues.\n\n### What if I have an issue that is almost a duplicate, should I open a new one to be triaged?\n\nYou can always open an [issue](https://github.com/opensearch-project/OpenSearch/issues/new/choose) including one that you think may be a duplicate. If you believe your issue is similar but distinct from an existing one, you are encouraged to file it and explain the differences during the triage meeting.\n\n### What if I have follow-up questions on an issue?\n\nIf you have an existing issue you would like to discuss, you can always comment on the issue itself. Alternatively, you are welcome to come to the triage meeting to discuss.\n\n### Is this meeting a good place to get help setting up features on my OpenSearch instance?\n\nWhile we are always happy to help the community, the best resource for implementation questions is [the OpenSearch forum](https://forum.opensearch.org/).\n\nThere you can find answers to many common questions as well as speak with implementation experts.\n\n### What are the issue labels associated with triaging?\n\nYes, there are several labels that are used to identify the 'state' of issues filed in OpenSearch .\n| Label         | When Applied         | Meaning                                                                                                                                 |\n|---------------|----------------------|-----------------------------------------------------------------------------------------------------------------------------------------|\n| `Untriaged` | When issues are created or re-opened. | Issues labeled as 'Untriaged' require the attention of the repository maintainers and may need to be prioritized for quicker resolution. It's crucial to keep the count of 'Untriaged' labels low to ensure all potential security issues are addressed in a timely manner. See [SECURITY.md](https://github.com/opensearch-project/OpenSearch/blob/main/SECURITY.md) for more details on handling these issues. |\n| `Help Wanted` | Anytime. | Issues marked as 'Help Wanted' signal that they are actionable and not the current focus of the project maintainers. Community contributions are especially encouraged for these issues. |\n| `Good First Issue` | Anytime. | Issues labeled as 'Good First Issue' are small in scope and can be resolved with a single pull request. These are recommended starting points for newcomers looking to make their first contributions. |\n\n### What are the typical outcomes of a triaged issue?\n\n| Outcome      | Label            | Description                                                                                                                                                                                      | Canned Response                                                                                                                                                           |\n|--------------|------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| Accepted     | `-untriaged`     | The issue has the details needed to be directed towards area owners.                                                                                                                            | \"Thanks for filing this issue, please feel free to submit a pull request.\"                                                                                                 |\n| Rejected     | N/A              | The issue will be closed with a reason for why it was rejected. Reasons might include lack of details, or being outside the scope of the project.                                               | \"Thanks for creating this issue; however, it isn't being accepted due to {REASON}. Please feel free to open a new issue after addressing the reason.\"                                |\n| Area Triage  | `+{AREALABEL}`  | OpenSearch has many different areas. If it's unclear whether an issue should be accepted, it will be labeled with the area and an owner will be @mentioned for follow-up.                        | \"Thanks for creating this issue; the triage meeting was unsure if this issue should be accepted, @{PERSON} or someone from the area please review and then accept or reject this issue?\" |\n| Transfer     | N/A              | If the issue applies to another repository within the OpenSearch Project, it will be transferred accordingly.                                                                                    | \"@opensearch-project/triage, can you please transfer this issue to project {REPOSITORY}.\" Or, if someone at the meeting has permissions, they can start the transfer.        |\n\n### Is this where I should bring up potential security vulnerabilities?\n\nDue to the sensitive nature of security vulnerabilities, please report all potential vulnerabilities directly by following the steps outlined on the [SECURITY.md](https://github.com/opensearch-project/OpenSearch/blob/main/SECURITY.md) document.\n\n### How do triage facilitator tag comments during the triage meeting?\n\nDuring the triage meeting, facilitators should use the tag _[Triage - attendees [1](#Profile_link) [2](#Profile_link)]_ to indicate a collective decision. This ensures contributors know the decision came from the meeting rather than an individual and identifies participants for any follow-up queries.\n\nThis tag should not be used outside triage meetings.\n"
        },
        {
          "name": "Vagrantfile",
          "type": "blob",
          "size": 15.654296875,
          "content": "#\n# SPDX-License-Identifier: Apache-2.0\n#\n# The OpenSearch Contributors require contributions made to\n# this file be licensed under the Apache-2.0 license or a\n# compatible open source license.\n#\n\n# -*- mode: ruby -*-\n# vim: ft=ruby ts=2 sw=2 sts=2 et:\n\n# This Vagrantfile exists to test packaging. Read more about its use in the\n# vagrant section in TESTING.md.\n\n# Licensed to Elasticsearch under one or more contributor\n# license agreements. See the NOTICE file distributed with\n# this work for additional information regarding copyright\n# ownership. Elasticsearch licenses this file to you under\n# the Apache License, Version 2.0 (the \"License\"); you may\n# not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\n#\n# Modifications Copyright OpenSearch Contributors. See\n# GitHub history for details.\n#\n\ndefine_opts = {\n  autostart: false\n}.freeze\n\nVagrant.configure(2) do |config|\n\n  config.vm.provider 'virtualbox' do |vbox|\n    # Give the box more memory and cpu because our tests are beasts!\n    vbox.memory = Integer(ENV['VAGRANT_MEMORY'] || 8192)\n    vbox.cpus = Integer(ENV['VAGRANT_CPUS'] || 4)\n\n    # see https://github.com/hashicorp/vagrant/issues/9524\n    vbox.customize [\"modifyvm\", :id, \"--audio\", \"none\"]\n  end\n\n  # Switch the default share for the project root from /vagrant to\n  # /opensearch because /vagrant is confusing when there is a project inside\n  # the opensearch project called vagrant....\n  config.vm.synced_folder '.', '/vagrant', disabled: true\n  config.vm.synced_folder '.', '/opensearch'\n  # TODO: make these syncs work for windows!!!\n  config.vm.synced_folder \"#{Dir.home}/.vagrant/gradle/caches/jars-3\", \"/root/.gradle/caches/jars-3\",\n      create: true,\n      owner: \"vagrant\"\n  config.vm.synced_folder \"#{Dir.home}/.vagrant/gradle/caches/modules-2\", \"/root/.gradle/caches/modules-2\",\n      create: true,\n      owner: \"vagrant\"\n  config.vm.synced_folder \"#{Dir.home}/.gradle/wrapper\", \"/root/.gradle/wrapper\",\n      create: true,\n      owner: \"vagrant\"\n\n  # Expose project directory. Note that VAGRANT_CWD may not be the same as Dir.pwd\n  PROJECT_DIR = ENV['VAGRANT_PROJECT_DIR'] || Dir.pwd\n  config.vm.synced_folder PROJECT_DIR, '/project'\n\n  'ubuntu-1604'.tap do |box|\n    config.vm.define box, define_opts do |config|\n      config.vm.box = 'elastic/ubuntu-16.04-x86_64'\n      deb_common config, box, extra: <<-SHELL\n        # Install Jayatana so we can work around it being present.\n        [ -f /usr/share/java/jayatanaag.jar ] || install jayatana\n      SHELL\n      ubuntu_docker config\n    end\n  end\n  'ubuntu-1804'.tap do |box|\n    config.vm.define box, define_opts do |config|\n      config.vm.box = 'elastic/ubuntu-18.04-x86_64'\n      deb_common config, box, extra: <<-SHELL\n       # Install Jayatana so we can work around it being present.\n       [ -f /usr/share/java/jayatanaag.jar ] || install jayatana\n      SHELL\n      ubuntu_docker config\n    end\n  end\n  # Wheezy's backports don't contain Openjdk 8 and the backflips\n  # required to get the sun jdk on there just aren't worth it. We have\n  # jessie and stretch for testing debian and it works fine.\n  'debian-8'.tap do |box|\n    config.vm.define box, define_opts do |config|\n      config.vm.box = 'elastic/debian-8-x86_64'\n      deb_common config, box, extra: <<-SHELL\n        # this sometimes gets a bad ip, and doesn't appear to be needed\n        rm -f /etc/apt/sources.list.d/http_debian_net_debian.list\n      SHELL\n    end\n  end\n  'debian-9'.tap do |box|\n    config.vm.define box, define_opts do |config|\n      config.vm.box = 'elastic/debian-9-x86_64'\n      deb_common config, box\n      deb_docker config\n    end\n  end\n  'centos-6'.tap do |box|\n    config.vm.define box, define_opts do |config|\n      config.vm.box = 'elastic/centos-6-x86_64'\n      rpm_common config, box\n    end\n  end\n  'centos-7'.tap do |box|\n    config.vm.define box, define_opts do |config|\n      config.vm.box = 'elastic/centos-7-x86_64'\n      rpm_common config, box\n      rpm_docker config\n    end\n  end\n  'oel-6'.tap do |box|\n    config.vm.define box, define_opts do |config|\n      config.vm.box = 'elastic/oraclelinux-6-x86_64'\n      rpm_common config, box\n    end\n  end\n  'oel-7'.tap do |box|\n    config.vm.define box, define_opts do |config|\n      config.vm.box = 'elastic/oraclelinux-7-x86_64'\n      rpm_common config, box\n    end\n  end\n  'fedora-28'.tap do |box|\n    config.vm.define box, define_opts do |config|\n      config.vm.box = 'elastic/fedora-28-x86_64'\n      dnf_common config, box\n      dnf_docker config\n    end\n  end\n  'fedora-29'.tap do |box|\n    config.vm.define box, define_opts do |config|\n      config.vm.box = 'elastic/fedora-29-x86_64'\n      dnf_common config, box\n      dnf_docker config\n    end\n  end\n  'sles-12'.tap do |box|\n    config.vm.define box, define_opts do |config|\n      config.vm.box = 'elastic/sles-12-x86_64'\n      sles_common config, box\n    end\n  end\n  'rhel-8'.tap do |box|\n    config.vm.define box, define_opts do |config|\n      config.vm.box = 'elastic/rhel-8-x86_64'\n      rpm_common config, box\n    end\n  end\n\n  windows_2012r2_box = ENV['VAGRANT_WINDOWS_2012R2_BOX']\n  if windows_2012r2_box && windows_2012r2_box.empty? == false\n    'windows-2012r2'.tap do |box|\n      config.vm.define box, define_opts do |config|\n        config.vm.box = windows_2012r2_box\n        windows_common config, box\n      end\n    end\n  end\n\n  windows_2016_box = ENV['VAGRANT_WINDOWS_2016_BOX']\n  if windows_2016_box && windows_2016_box.empty? == false\n    'windows-2016'.tap do |box|\n      config.vm.define box, define_opts do |config|\n        config.vm.box = windows_2016_box\n        windows_common config, box\n      end\n    end\n  end\nend\n\ndef deb_common(config, name, extra: '')\n  # http://foo-o-rama.com/vagrant--stdin-is-not-a-tty--fix.html\n  config.vm.provision 'fix-no-tty', type: 'shell' do |s|\n      s.privileged = false\n      s.inline = \"sudo sed -i '/tty/!s/mesg n/tty -s \\\\&\\\\& mesg n/' /root/.profile\"\n  end\n  extra_with_lintian = <<-SHELL\n    #{extra}\n    install lintian\n  SHELL\n  linux_common(\n    config,\n    name,\n    update_command: 'apt-get update',\n    update_tracking_file: '/var/cache/apt/archives/last_update',\n    install_command: 'apt-get install -y --force-yes',\n    extra: extra_with_lintian\n  )\nend\n\ndef ubuntu_docker(config)\n  config.vm.provision 'install Docker using apt', type: 'shell', inline: <<-SHELL\n    # Install packages to allow apt to use a repository over HTTPS\n    apt-get install -y --force-yes \\\n      apt-transport-https \\\n      ca-certificates \\\n      curl \\\n      gnupg2 \\\n      software-properties-common\n\n    # Add Dockers official GPG key\n    curl -fsSL https://download.docker.com/linux/ubuntu/gpg | apt-key add -\n\n    # Set up the stable Docker repository\n    add-apt-repository \\\n      \"deb [arch=amd64] https://download.docker.com/linux/ubuntu \\\n      $(lsb_release -cs) \\\n      stable\"\n\n    # Install Docker. Unlike Fedora and CentOS, this also start the daemon.\n    apt-get update\n    apt-get install -y --force-yes docker-ce docker-ce-cli containerd.io\n\n    # Add vagrant to the Docker group, so that it can run commands\n    usermod -aG docker vagrant\n\n    # Enable IPv4 forwarding\n    sed -i '/net.ipv4.ip_forward/s/^#//' /etc/sysctl.conf\n    systemctl restart networking\n  SHELL\nend\n\n\ndef deb_docker(config)\n  config.vm.provision 'install Docker using apt', type: 'shell', inline: <<-SHELL\n    # Install packages to allow apt to use a repository over HTTPS\n    apt-get install -y --force-yes \\\n      apt-transport-https \\\n      ca-certificates \\\n      curl \\\n      gnupg2 \\\n      software-properties-common\n\n    # Add Dockers official GPG key\n    curl -fsSL https://download.docker.com/linux/debian/gpg | apt-key add -\n\n    # Set up the stable Docker repository\n    add-apt-repository \\\n      \"deb [arch=amd64] https://download.docker.com/linux/debian \\\n      $(lsb_release -cs) \\\n      stable\"\n\n    # Install Docker. Unlike Fedora and CentOS, this also start the daemon.\n    apt-get update\n    apt-get install -y --force-yes docker-ce docker-ce-cli containerd.io\n\n    # Add vagrant to the Docker group, so that it can run commands\n    usermod -aG docker vagrant\n  SHELL\nend\n\ndef rpm_common(config, name)\n  linux_common(\n    config,\n    name,\n    update_command: 'yum check-update',\n    update_tracking_file: '/var/cache/yum/last_update',\n    install_command: 'yum install -y'\n  )\nend\n\ndef rpm_docker(config)\n  config.vm.provision 'install Docker using yum', type: 'shell', inline: <<-SHELL\n    # Install prerequisites\n    yum install -y yum-utils device-mapper-persistent-data lvm2\n\n    # Add repository\n    yum-config-manager -y --add-repo https://download.docker.com/linux/centos/docker-ce.repo\n\n    # Install Docker\n    yum install -y docker-ce docker-ce-cli containerd.io\n\n    # Start Docker\n    systemctl enable --now docker\n\n    # Add vagrant to the Docker group, so that it can run commands\n    usermod -aG docker vagrant\n  SHELL\nend\n\ndef dnf_common(config, name)\n  # Autodetect doesn't work....\n  if Vagrant.has_plugin?('vagrant-cachier')\n    config.cache.auto_detect = false\n    config.cache.enable :generic, { :cache_dir => '/var/cache/dnf' }\n  end\n  linux_common(\n    config,\n    name,\n    update_command: 'dnf check-update',\n    update_tracking_file: '/var/cache/dnf/last_update',\n    install_command: 'dnf install -y',\n    install_command_retries: 5\n  )\nend\n\ndef dnf_docker(config)\n  config.vm.provision 'install Docker using dnf', type: 'shell', inline: <<-SHELL\n    # Install prerequisites\n    dnf -y install dnf-plugins-core\n\n    # Add repository\n    dnf config-manager --add-repo https://download.docker.com/linux/fedora/docker-ce.repo\n\n    # Install Docker\n    dnf install -y docker-ce docker-ce-cli containerd.io\n\n    # Start Docker\n    systemctl enable --now docker\n\n    # Add vagrant to the Docker group, so that it can run commands\n    usermod -aG docker vagrant\n  SHELL\nend\n\ndef suse_common(config, name, extra: '')\n  linux_common(\n    config,\n    name,\n    update_command: 'zypper --non-interactive list-updates',\n    update_tracking_file: '/var/cache/zypp/packages/last_update',\n    install_command: 'zypper --non-interactive --quiet install --no-recommends',\n    extra: extra\n  )\nend\n\ndef sles_common(config, name)\n  extra = <<-SHELL\n    zypper rr systemsmanagement_puppet puppetlabs-pc1 devel_tools_scm\n    zypper ar http://download.opensuse.org/distribution/12.3/repo/oss/ oss\n    zypper --non-interactive  --gpg-auto-import-keys refresh\n    zypper --non-interactive install git-core\n    # choose to \"ignore some dependencies\" of expect, which has a problem with tcl...\n    zypper --non-interactive install --force-resolution expect\n  SHELL\n  suse_common config, name, extra: extra\nend\n\n# Configuration needed for all linux boxes\n# @param config Vagrant's config object. Required.\n# @param name [String] The box name. Required.\n# @param update_command [String] The command used to update the package\n#   manager. Required. Think `apt-get update`.\n# @param update_tracking_file [String] The location of the file tracking the\n#   last time the update command was run. Required. Should be in a place that\n#   is cached by vagrant-cachier.\n# @param install_command [String] The command used to install a package.\n#   Required. Think `apt-get install #{package}`.\n# @param install_command_retries [Integer] Number of times to retry\n#   a failed install command\n# @param extra [String] Additional script to run before installing\n#   dependencies\n#\ndef linux_common(config,\n                 name,\n                 update_command: 'required',\n                 update_tracking_file: 'required',\n                 install_command: 'required',\n                 install_command_retries: 0,\n                 extra: '')\n\n  raise ArgumentError, 'update_command is required' if update_command == 'required'\n  raise ArgumentError, 'update_tracking_file is required' if update_tracking_file == 'required'\n  raise ArgumentError, 'install_command is required' if install_command == 'required'\n\n  if Vagrant.has_plugin?('vagrant-cachier')\n    config.cache.scope = :box\n  end\n\n  config.vm.provision 'markerfile', type: 'shell', inline: <<-SHELL\n    touch /etc/is_vagrant_vm\n    touch /is_vagrant_vm # for consistency between linux and windows\n  SHELL\n\n  # This prevents leftovers from previous tests using the\n  # same VM from messing up the current test\n  config.vm.provision 'clean opensearch installs in tmp', run: 'always', type: 'shell', inline: <<-SHELL\n    rm -rf /tmp/opensearch*\n  SHELL\n\n  sh_set_prompt config, name\n  sh_install_deps(\n    config,\n    update_command,\n    update_tracking_file,\n    install_command,\n    install_command_retries,\n    extra\n  )\nend\n\n# Sets up a consistent prompt for all users. Or tries to. The VM might\n# contain overrides for root and vagrant but this attempts to work around\n# them by re-source-ing the standard prompt file.\ndef sh_set_prompt(config, name)\n  config.vm.provision 'set prompt', type: 'shell', inline: <<-SHELL\n      cat \\<\\<PROMPT > /etc/profile.d/opensearch_prompt.sh\nexport PS1='#{name}:\\\\w$ '\nPROMPT\n      grep 'source /etc/profile.d/opensearch_prompt.sh' ~/.bashrc |\n        cat \\<\\<SOURCE_PROMPT >> ~/.bashrc\n# Replace the standard prompt with a consistent one\nsource /etc/profile.d/opensearch_prompt.sh\nSOURCE_PROMPT\n      grep 'source /etc/profile.d/opensearch_prompt.sh' ~vagrant/.bashrc |\n        cat \\<\\<SOURCE_PROMPT >> ~vagrant/.bashrc\n# Replace the standard prompt with a consistent one\nsource /etc/profile.d/opensearch_prompt.sh\nSOURCE_PROMPT\n  SHELL\nend\n\ndef sh_install_deps(config,\n                    update_command,\n                    update_tracking_file,\n                    install_command,\n                    install_command_retries,\n                    extra)\n  config.vm.provision 'install dependencies', type: 'shell', inline:  <<-SHELL\n    set -e\n    set -o pipefail\n\n    # Retry install command up to $2 times, if failed\n    retry_installcommand() {\n      n=0\n      while true; do\n        #{install_command} $1 && break\n        let n=n+1\n        if [ $n -ge $2 ]; then\n          echo \"==> Exhausted retries to install $1\"\n          return 1\n        fi\n        echo \"==> Retrying installing $1, attempt $((n+1))\"\n        # Add a small delay to increase chance of metalink providing updated list of mirrors\n        sleep 5\n      done\n    }\n\n    installed() {\n      command -v $1 2>&1 >/dev/null\n    }\n\n    install() {\n      # Only apt-get update if we haven't in the last day\n      if [ ! -f #{update_tracking_file} ] || [ \"x$(find #{update_tracking_file} -mtime +0)\" == \"x#{update_tracking_file}\" ]; then\n        echo \"==> Updating repository\"\n        #{update_command} || true\n        touch #{update_tracking_file}\n      fi\n      echo \"==> Installing $1\"\n      if [ #{install_command_retries} -eq 0 ]\n      then\n        #{install_command} $1\n      else\n        retry_installcommand $1 #{install_command_retries}\n      fi\n    }\n\n    ensure() {\n      installed $1 || install $1\n    }\n\n    #{extra}\n\n    ensure tar\n    ensure curl\n    ensure unzip\n    ensure rsync\n    ensure expect\n\n    cat \\<\\<SUDOERS_VARS > /etc/sudoers.d/opensearch_vars\nDefaults   env_keep += \"JAVA_HOME\"\nDefaults   env_keep += \"SYSTEM_JAVA_HOME\"\nDefaults   env_keep += \"OPENSEARCH_JAVA_HOME\"\nSUDOERS_VARS\n    chmod 0440 /etc/sudoers.d/opensearch_vars\n  SHELL\nend\n\ndef windows_common(config, name)\n  config.vm.provision 'set prompt', type: 'shell', inline: <<-SHELL\n    $ErrorActionPreference = \"Stop\"\n    $ps_prompt = 'function Prompt { \"#{name}:$($ExecutionContext.SessionState.Path.CurrentLocation)>\" }'\n    $ps_prompt | Out-File $PsHome/Microsoft.PowerShell_profile.ps1\n  SHELL\nend\n"
        },
        {
          "name": "benchmarks",
          "type": "tree",
          "content": null
        },
        {
          "name": "build.gradle",
          "type": "blob",
          "size": 30.6015625,
          "content": "/*\n * SPDX-License-Identifier: Apache-2.0\n *\n * The OpenSearch Contributors require contributions made to\n * this file be licensed under the Apache-2.0 license or a\n * compatible open source license.\n *\n * Modifications Copyright OpenSearch Contributors. See\n * GitHub history for details.\n */\n\n/*\n * Licensed to Elasticsearch under one or more contributor\n * license agreements. See the NOTICE file distributed with\n * this work for additional information regarding copyright\n * ownership. Elasticsearch licenses this file to you under\n * the Apache License, Version 2.0 (the \"License\"); you may\n * not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *    http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing,\n * software distributed under the License is distributed on an\n * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n * KIND, either express or implied.  See the License for the\n * specific language governing permissions and limitations\n * under the License.\n */\n\nimport java.nio.charset.StandardCharsets;\nimport java.io.ByteArrayOutputStream;\n\nimport com.avast.gradle.dockercompose.tasks.ComposePull\nimport com.github.jengelman.gradle.plugins.shadow.ShadowPlugin\nimport de.thetaphi.forbiddenapis.gradle.ForbiddenApisPlugin\nimport org.apache.tools.ant.taskdefs.condition.Os\nimport org.opensearch.gradle.BuildPlugin\nimport org.opensearch.gradle.Version\nimport org.opensearch.gradle.VersionProperties\nimport org.opensearch.gradle.info.BuildParams\nimport org.opensearch.gradle.plugin.PluginBuildPlugin\nimport org.opensearch.gradle.tar.SymbolicLinkPreservingTar\nimport org.gradle.plugins.ide.eclipse.model.AccessRule\nimport org.gradle.plugins.ide.eclipse.model.EclipseJdt\nimport org.gradle.plugins.ide.eclipse.model.SourceFolder\nimport org.gradle.api.Project;\nimport org.gradle.process.ExecResult;\nimport org.opensearch.gradle.CheckCompatibilityTask\n\nimport static org.opensearch.gradle.util.GradleUtils.maybeConfigure\n\nplugins {\n  id 'lifecycle-base'\n  id 'opensearch.docker-support'\n  id 'opensearch.global-build-info'\n  id \"com.diffplug.spotless\" version \"6.25.0\" apply false\n  id \"test-report-aggregation\"\n  id 'jacoco-report-aggregation'\n}\n\napply from: 'gradle/build-complete.gradle'\napply from: 'gradle/runtime-jdk-provision.gradle'\napply from: 'gradle/ide.gradle'\napply from: 'gradle/forbidden-dependencies.gradle'\napply from: 'gradle/formatting.gradle'\napply from: 'gradle/local-distribution.gradle'\napply from: 'gradle/fips.gradle'\napply from: 'gradle/run.gradle'\napply from: 'gradle/missing-javadoc.gradle'\napply from: 'gradle/code-coverage.gradle'\n\n// Disable unconditional publishing of build scans\ndevelocity {\n  buildScan {\n    publishing.onlyIf { false }\n  }\n}\n\n// common maven publishing configuration\nallprojects {\n  group = 'org.opensearch'\n  version = VersionProperties.getOpenSearch()\n  description = \"OpenSearch subproject ${project.path}\"\n}\n\nconfigure(allprojects - project(':distribution:archives:integ-test-zip')) {\n  project.pluginManager.withPlugin('nebula.maven-base-publish') {\n    if (project.pluginManager.hasPlugin('opensearch.build') == false) {\n      throw new GradleException(\"Project ${path} publishes a pom but doesn't apply the build plugin.\")\n    }\n  }\n}\n\nsubprojects {\n  // Default to the apache license\n  project.ext.licenseName = 'The Apache Software License, Version 2.0'\n  project.ext.licenseUrl = 'http://www.apache.org/licenses/LICENSE-2.0.txt'\n\n  // we only use maven publish to add tasks for pom generation\n  plugins.withType(MavenPublishPlugin).whenPluginAdded {\n    publishing {\n      publications {\n        // add license information to generated poms\n        all {\n          pom.withXml { XmlProvider xml ->\n            Node node = xml.asNode()\n            node.appendNode('inceptionYear', '2021')\n\n            Node license = node.appendNode('licenses').appendNode('license')\n            license.appendNode('name', project.licenseName)\n            license.appendNode('url', project.licenseUrl)\n            license.appendNode('distribution', 'repo')\n\n            Node developer = node.appendNode('developers').appendNode('developer')\n            developer.appendNode('name', 'OpenSearch')\n            developer.appendNode('url', 'https://github.com/opensearch-project/OpenSearch')\n          }\n        }\n      }\n      repositories {\n        maven {\n          name = 'test'\n          url = \"${rootProject.buildDir}/local-test-repo\"\n        }\n        maven {\n          name = 'Snapshots'\n          url = 'https://aws.oss.sonatype.org/content/repositories/snapshots'\n          credentials {\n            username = \"$System.env.SONATYPE_USERNAME\"\n            password = \"$System.env.SONATYPE_PASSWORD\"\n          }\n        }\n      }\n    }\n  }\n\n  plugins.withType(BuildPlugin).whenPluginAdded {\n    project.licenseFile = project.rootProject.file('licenses/APACHE-LICENSE-2.0.txt')\n    project.noticeFile = project.rootProject.file('NOTICE.txt')\n  }\n}\n\ntasks.register(\"updateCIBwcVersions\") {\n  doLast {\n    File yml = file(\".ci/bwcVersions\")\n    yml.text = \"\"\n    yml << \"BWC_VERSION:\\n\"\n    BuildParams.bwcVersions.indexCompatible.each {\n      yml << \"  - \\\"$it\\\"\\n\"\n    }\n  }\n}\n\n// build metadata from previous build, contains eg hashes for bwc builds\nString buildMetadataValue = System.getenv('BUILD_METADATA')\nif (buildMetadataValue == null) {\n  buildMetadataValue = ''\n}\nMap<String, String> buildMetadataMap = buildMetadataValue.tokenize(';').collectEntries {\n  def (String key, String value) = it.split('=')\n  return [key, value]\n}\n\n// See please https://docs.gradle.org/8.11/userguide/service_injection.html#execoperations\ninterface InjectedExecOps {\n  @Inject ExecOperations getExecOps()\n}\n\n/**\n * Using 'git' command line (if available), tries to fetch the commit date of the current revision\n * @return commit date of the current revision or 0 if it is not available\n */\nlong gitRevisionDate = {\n  def execOps = project.objects.newInstance(InjectedExecOps)\n  // Try to get last commit date as Unix timestamp\n  try (ByteArrayOutputStream stdout = new ByteArrayOutputStream()) {\n     ExecResult result = execOps.execOps.exec(spec -> {\n        spec.setIgnoreExitValue(true);\n        spec.setStandardOutput(stdout);\n        spec.commandLine(\"git\", \"log\", \"-1\", \"--format=%ct\");\n     });\n\n    if (result.getExitValue() == 0) {\n      return Long.parseLong(stdout.toString(StandardCharsets.UTF_8).replaceAll(\"\\\\s\", \"\")) * 1000; /* seconds to millis */\n    }\n  } catch (IOException | GradleException | NumberFormatException ex) {\n    /* fall back to default Unix epoch timestamp */\n  }\n\n  return 0;\n}()\n\n\n// injecting groovy property variables into all projects\nallprojects {\n  project.ext {\n    // for ide hacks...\n    isEclipse = System.getProperty(\"eclipse.launcher\") != null ||   // Detects gradle launched from Eclipse's IDE\n      System.getProperty(\"eclipse.application\") != null ||    // Detects gradle launched from the Eclipse compiler server\n      gradle.startParameter.taskNames.contains('eclipse') ||  // Detects gradle launched from the command line to do eclipse stuff\n      gradle.startParameter.taskNames.contains('cleanEclipse')\n\n    buildMetadata = buildMetadataMap\n  }\n}\n\ntasks.register(\"verifyVersions\") {\n  doLast {\n    if (gradle.startParameter.isOffline()) {\n      throw new GradleException(\"Must run in online mode to verify versions\")\n    }\n    // Read the list from maven central.\n    // Fetch the metadata and parse the xml into Version instances because it's more straight forward here\n    // rather than bwcVersion ( VersionCollection ).\n    new URL('https://repo1.maven.org/maven2/org/opensearch/opensearch/maven-metadata.xml').openStream().withStream { s ->\n      BuildParams.bwcVersions.compareToAuthoritative(\n        new XmlParser().parse(s)\n          .versioning.versions.version\n          .collect { it.text() }.findAll { it ==~ /\\d+\\.\\d+\\.\\d+/ }\n          .collect { Version.fromString(it) }\n      )\n    }\n    String ciYml = file(\".ci/bwcVersions\").text\n    BuildParams.bwcVersions.indexCompatible.each {\n      if (ciYml.contains(\"\\\"$it\\\"\\n\") == false) {\n        throw new Exception(\".ci/bwcVersions is outdated, run `./gradlew updateCIBwcVersions` and check in the results\");\n      }\n    }\n  }\n}\n\n/*\n * When adding backcompat behavior that spans major versions, temporarily\n * disabling the backcompat tests is necessary. This flag controls\n * the enabled state of every bwc task. It should be set back to true\n * after the backport of the backcompat code is complete.\n */\n\nboolean bwc_tests_enabled = true\n\n/* place an issue link here when committing bwc changes */\nString bwc_tests_disabled_issue = \"\"\n\n/* there's no existing MacOS release, therefore disable bcw tests */\nif (Os.isFamily(Os.FAMILY_MAC)) {\n  bwc_tests_enabled = false\n  bwc_tests_disabled_issue = \"https://github.com/opensearch-project/OpenSearch/issues/4173\"\n}\n\nif (bwc_tests_enabled == false) {\n  if (bwc_tests_disabled_issue.isEmpty()) {\n    throw new GradleException(\"bwc_tests_disabled_issue must be set when bwc_tests_enabled == false\")\n  }\n  println \"========================= WARNING =========================\"\n  println \"         Backwards compatibility tests are disabled!\"\n  println \"See ${bwc_tests_disabled_issue}\"\n  println \"===========================================================\"\n}\nif (project.gradle.startParameter.taskNames.find { it.startsWith(\"checkPart\") } != null) {\n  // Disable BWC tests for checkPart* tasks as it's expected that this will run un it's own check\n  bwc_tests_enabled = false\n}\n\nsubprojects {\n  ext.bwc_tests_enabled = bwc_tests_enabled\n}\n\ntasks.register(\"verifyBwcTestsEnabled\") {\n  doLast {\n    if (bwc_tests_enabled == false) {\n      throw new GradleException('Bwc tests are disabled. They must be re-enabled after completing backcompat behavior backporting.')\n    }\n  }\n}\n\ntasks.register(\"branchConsistency\") {\n  description 'Ensures this branch is internally consistent. For example, that versions constants match released versions.'\n  group 'Verification'\n  dependsOn \":verifyVersions\", \":verifyBwcTestsEnabled\"\n}\n\nallprojects {\n  // configure compiler options\n  tasks.withType(JavaCompile).configureEach { JavaCompile compile ->\n    options.fork = true\n\n    configure(options.forkOptions) {\n      memoryMaximumSize = project.property('options.forkOptions.memoryMaximumSize')\n    }\n\n    // See please https://bugs.openjdk.java.net/browse/JDK-8209058\n    if (BuildParams.runtimeJavaVersion > JavaVersion.VERSION_11) {\n      compile.options.compilerArgs << '-Werror'\n    }\n    compile.options.compilerArgs << '-Xlint:auxiliaryclass'\n    compile.options.compilerArgs << '-Xlint:cast'\n    compile.options.compilerArgs << '-Xlint:classfile'\n    compile.options.compilerArgs << '-Xlint:dep-ann'\n    compile.options.compilerArgs << '-Xlint:divzero'\n    compile.options.compilerArgs << '-Xlint:empty'\n    compile.options.compilerArgs << '-Xlint:exports'\n    compile.options.compilerArgs << '-Xlint:fallthrough'\n    compile.options.compilerArgs << '-Xlint:finally'\n    compile.options.compilerArgs << '-Xlint:module'\n    compile.options.compilerArgs << '-Xlint:opens'\n    compile.options.compilerArgs << '-Xlint:overloads'\n    compile.options.compilerArgs << '-Xlint:overrides'\n    compile.options.compilerArgs << '-Xlint:-processing'\n    compile.options.compilerArgs << '-Xlint:rawtypes'\n    compile.options.compilerArgs << '-Xlint:removal'\n    compile.options.compilerArgs << '-Xlint:requires-automatic'\n    compile.options.compilerArgs << '-Xlint:requires-transitive-automatic'\n    compile.options.compilerArgs << '-Xlint:static'\n    compile.options.compilerArgs << '-Xlint:unchecked'\n    compile.options.compilerArgs << '-Xlint:varargs'\n    compile.options.compilerArgs << '-Xlint:preview'\n    // TODO: disabled warnings: path, serial, options, deprecation, try\n    // -path because gradle will send in paths that don't always exist.\n    // -missing because we have tons of missing @returns and @param.\n    // -serial because we don't use java serialization.\n    compile.options.compilerArgs << '-Xdoclint:accessibility'\n    compile.options.compilerArgs << '-Xdoclint:html'\n    compile.options.compilerArgs << '-Xdoclint:reference'\n    compile.options.compilerArgs << '-Xdoclint:syntax'\n  }\n\n  // ignore missing javadocs\n  tasks.withType(Javadoc).configureEach { Javadoc javadoc ->\n    // the -quiet here is because of a bug in gradle, in that adding a string option\n    // by itself is not added to the options. By adding quiet, both this option and\n    // the \"value\" -quiet is added, separated by a space. This is ok since the javadoc\n    // command already adds -quiet, so we are just duplicating it\n    // see https://discuss.gradle.org/t/add-custom-javadoc-option-that-does-not-take-an-argument/5959\n    javadoc.options.encoding = 'UTF8'\n    javadoc.options.addStringOption('Xdoclint:all,-missing', '-quiet')\n    boolean failOnJavadocWarning = project.ext.has('failOnJavadocWarning') ? project.ext.get('failOnJavadocWarning') : true\n    if (failOnJavadocWarning) {\n      javadoc.options.addStringOption('Xwerror', '-quiet')\n    }\n    javadoc.options.tags = [\"opensearch.internal\", \"opensearch.api\", \"opensearch.experimental\"]\n    javadoc.options.addStringOption(\"-release\", java.targetCompatibility.majorVersion)\n  }\n\n  // support for reproducible builds\n  tasks.withType(AbstractArchiveTask).configureEach { task ->\n    // ignore file timestamps\n    // be consistent in archive file order\n    task.preserveFileTimestamps = false\n    task.reproducibleFileOrder = true\n    if (task instanceof SymbolicLinkPreservingTar) {\n      // Replace file timestamps with latest Git revision date (if available)\n      task.lastModifiedTimestamp = gitRevisionDate\n    }\n  }\n\n  project.afterEvaluate {\n    // Handle javadoc dependencies across projects. Order matters: the linksOffline for\n    // org.opensearch:opensearch must be the last one or all the links for the\n    // other packages (e.g org.opensearch.client) will point to server rather than\n    // their own artifacts.\n    if (project.plugins.hasPlugin(BuildPlugin) || project.plugins.hasPlugin(PluginBuildPlugin)) {\n      String artifactsHost = VersionProperties.getOpenSearch().endsWith(\"-SNAPSHOT\")\n        ? \"https://artifacts.opensearch.org/snapshots/\"\n        : \"https://artifacts.opensearch.org/releases/\"\n      Closure sortClosure = { a, b -> b.group <=> a.group }\n      Closure depJavadocClosure = { shadowed, dep ->\n        if ((dep instanceof ProjectDependency) == false) {\n          return\n        }\n        Project upstreamProject = project.project(dep.path)\n        if (upstreamProject == null) {\n          return\n        }\n        if (shadowed) {\n          /*\n           * Include the source of shadowed upstream projects so we don't\n           * have to publish their javadoc.\n           */\n          project.evaluationDependsOn(upstreamProject.path)\n          project.javadoc.source += upstreamProject.javadoc.source\n          /*\n           * Instead we need the upstream project's javadoc classpath so\n           * we don't barf on the classes that it references.\n           */\n          project.javadoc.classpath += upstreamProject.javadoc.classpath\n        } else {\n          // Link to non-shadowed dependant projects\n          project.javadoc.dependsOn \"${upstreamProject.path}:javadoc\"\n          String externalLinkName = upstreamProject.base.archivesName\n          String artifactPath = dep.group.replaceAll('\\\\.', '/') + '/' + externalLinkName.replaceAll('\\\\.', '/') + '/' + dep.version\n          String projectRelativePath = project.relativePath(upstreamProject.buildDir)\n          project.javadoc.options.linksOffline artifactsHost + \"/javadoc/\" + artifactPath, \"${projectRelativePath}/docs/javadoc/\"\n        }\n      }\n      boolean hasShadow = project.plugins.hasPlugin(ShadowPlugin)\n      project.configurations.implementation.dependencies\n        .findAll()\n        .toSorted(sortClosure)\n        .each({ c -> depJavadocClosure(hasShadow, c) })\n      project.configurations.compileOnly.dependencies\n        .findAll()\n        .toSorted(sortClosure)\n        .each({ c -> depJavadocClosure(false, c) })\n      if (hasShadow) {\n        // include any dependencies for shadow JAR projects that are *not* bundled in the shadow JAR\n        project.configurations.shadow.dependencies\n          .findAll()\n          .toSorted(sortClosure)\n          .each({ c -> depJavadocClosure(false, c) })\n      }\n    }\n  }\n}\n\n// Ensure similar tasks in dependent projects run first. The projectsEvaluated here is\n// important because, while dependencies.all will pickup future dependencies,\n// it is not necessarily true that the task exists in both projects at the time\n// the dependency is added.\ngradle.projectsEvaluated {\n  allprojects {\n    project.tasks.withType(JavaForkOptions) {\n      maxHeapSize = project.property('options.forkOptions.memoryMaximumSize')\n    }\n\n    if (project.path == ':test:framework') {\n      // :test:framework:test cannot run before and after :server:test\n      return\n    }\n    if (tasks.findByPath('test') != null && tasks.findByPath('integTest') != null) {\n      integTest.mustRunAfter test\n    }\n\n    project.tasks.withType(Test) { task ->\n      if (task != null) {\n        if (BuildParams.runtimeJavaVersion > JavaVersion.VERSION_17) {\n          task.jvmArgs += [\"-Djava.security.manager=allow\"]\n        }\n        if (BuildParams.runtimeJavaVersion >= JavaVersion.VERSION_20) {\n          task.jvmArgs += [\"--add-modules=jdk.incubator.vector\"]\n        }\n      }\n    }\n\n    configurations.matching { it.canBeResolved }.all { Configuration configuration ->\n      dependencies.matching { it instanceof ProjectDependency }.all { ProjectDependency dep ->\n        Project upstreamProject = project.project(dep.path)\n        if (upstreamProject != null) {\n          if (project.path == upstreamProject.path) {\n            // TODO: distribution integ tests depend on themselves (!), fix that\n            return\n          }\n          for (String taskName : ['test', 'integTest']) {\n            Task task = project.tasks.findByName(taskName)\n            Task upstreamTask = upstreamProject.tasks.findByName(taskName)\n            if (task != null && upstreamTask != null) {\n              task.shouldRunAfter(upstreamTask)\n            }\n          }\n        }\n      }\n    }\n  }\n\n  dependencies {\n    subprojects.findAll { it.pluginManager.hasPlugin('java') }.forEach {\n      testReportAggregation it\n    }\n    subprojects.findAll { it.pluginManager.hasPlugin('jacoco') }.forEach {\n      jacocoAggregation it\n    }\n  }\n}\n\n// test retry configuration\nsubprojects {\n  tasks.withType(Test).configureEach {\n    develocity.testRetry {\n      if (BuildParams.isCi()) {\n        maxRetries = 3\n        maxFailures = 10\n      }\n      failOnPassedAfterRetry = false\n      filter {\n        includeClasses.add(\"org.opensearch.action.admin.cluster.node.tasks.ResourceAwareTasksTests\")\n        includeClasses.add(\"org.opensearch.action.admin.cluster.tasks.PendingTasksBlocksIT\")\n        includeClasses.add(\"org.opensearch.action.admin.indices.create.CreateIndexIT\")\n        includeClasses.add(\"org.opensearch.action.admin.indices.create.ShrinkIndexIT\")\n        includeClasses.add(\"org.opensearch.aliases.IndexAliasesIT\")\n        includeClasses.add(\"org.opensearch.backwards.MixedClusterClientYamlTestSuiteIT\")\n        includeClasses.add(\"org.opensearch.blocks.SimpleBlocksIT\")\n        includeClasses.add(\"org.opensearch.client.PitIT\")\n        includeClasses.add(\"org.opensearch.client.ReindexIT\")\n        includeClasses.add(\"org.opensearch.cluster.ClusterHealthIT\")\n        includeClasses.add(\"org.opensearch.cluster.allocation.AwarenessAllocationIT\")\n        includeClasses.add(\"org.opensearch.cluster.allocation.ClusterRerouteIT\")\n        includeClasses.add(\"org.opensearch.cluster.coordination.AwarenessAttributeDecommissionIT\")\n        includeClasses.add(\"org.opensearch.cluster.metadata.IndexGraveyardTests\")\n        includeClasses.add(\"org.opensearch.cluster.routing.MovePrimaryFirstTests\")\n        includeClasses.add(\"org.opensearch.cluster.routing.allocation.decider.DiskThresholdDeciderIT\")\n        includeClasses.add(\"org.opensearch.common.util.concurrent.QueueResizableOpenSearchThreadPoolExecutorTests\")\n        includeClasses.add(\"org.opensearch.gateway.RecoveryFromGatewayIT\")\n        includeClasses.add(\"org.opensearch.gateway.ReplicaShardAllocatorIT\")\n        includeClasses.add(\"org.opensearch.http.SearchRestCancellationIT\")\n        includeClasses.add(\"org.opensearch.http.netty4.Netty4HttpServerTransportTests\")\n        includeClasses.add(\"org.opensearch.index.IndexServiceTests\")\n        includeClasses.add(\"org.opensearch.index.IndexSettingsTests\")\n        includeClasses.add(\"org.opensearch.index.SegmentReplicationPressureIT\")\n        includeClasses.add(\"org.opensearch.index.ShardIndexingPressureIT\")\n        includeClasses.add(\"org.opensearch.index.ShardIndexingPressureSettingsIT\")\n        includeClasses.add(\"org.opensearch.index.reindex.BulkByScrollResponseTests\")\n        includeClasses.add(\"org.opensearch.index.reindex.DeleteByQueryBasicTests\")\n        includeClasses.add(\"org.opensearch.index.reindex.UpdateByQueryBasicTests\")\n        includeClasses.add(\"org.opensearch.index.shard.IndexShardIT\")\n        includeClasses.add(\"org.opensearch.index.shard.RemoteIndexShardTests\")\n        includeClasses.add(\"org.opensearch.index.shard.RemoteStoreRefreshListenerTests\")\n        includeClasses.add(\"org.opensearch.index.translog.RemoteFSTranslogTests\")\n        includeClasses.add(\"org.opensearch.indices.DateMathIndexExpressionsIntegrationIT\")\n        includeClasses.add(\"org.opensearch.indices.replication.RemoteStoreReplicationSourceTests\")\n        includeClasses.add(\"org.opensearch.indices.replication.SegmentReplicationAllocationIT\")\n        includeClasses.add(\"org.opensearch.indices.replication.SegmentReplicationIT\")\n        includeClasses.add(\"org.opensearch.indices.replication.SegmentReplicationRelocationIT\")\n        includeClasses.add(\"org.opensearch.indices.replication.SegmentReplicationTargetServiceTests\")\n        includeClasses.add(\"org.opensearch.indices.state.CloseWhileRelocatingShardsIT\")\n        includeClasses.add(\"org.opensearch.monitor.fs.FsHealthServiceTests\")\n        includeClasses.add(\"org.opensearch.recovery.ReplicationCollectionTests\")\n        includeClasses.add(\"org.opensearch.remotestore.CreateRemoteIndexClusterDefaultDocRep\")\n        includeClasses.add(\"org.opensearch.remotestore.CreateRemoteIndexIT\")\n        includeClasses.add(\"org.opensearch.remotestore.CreateRemoteIndexTranslogDisabledIT\")\n        includeClasses.add(\"org.opensearch.remotestore.RemoteStoreBackpressureIT\")\n        includeClasses.add(\"org.opensearch.remotestore.RemoteStoreIT\")\n        includeClasses.add(\"org.opensearch.remotestore.RemoteStoreRefreshListenerIT\")\n        includeClasses.add(\"org.opensearch.remotestore.RemoteStoreStatsIT\")\n        includeClasses.add(\"org.opensearch.remotestore.SegmentReplicationRemoteStoreIT\")\n        includeClasses.add(\"org.opensearch.remotestore.SegmentReplicationUsingRemoteStoreIT\")\n        includeClasses.add(\"org.opensearch.remotestore.multipart.RemoteStoreMultipartIT\")\n        includeClasses.add(\"org.opensearch.repositories.azure.AzureBlobContainerRetriesTests\")\n        includeClasses.add(\"org.opensearch.repositories.azure.AzureBlobStoreRepositoryTests\")\n        includeClasses.add(\"org.opensearch.repositories.gcs.GoogleCloudStorageBlobContainerRetriesTests\")\n        includeClasses.add(\"org.opensearch.repositories.gcs.GoogleCloudStorageBlobStoreRepositoryTests\")\n        includeClasses.add(\"org.opensearch.repositories.s3.S3BlobStoreRepositoryTests\")\n        includeClasses.add(\"org.opensearch.search.ConcurrentSegmentSearchTimeoutIT\")\n        includeClasses.add(\"org.opensearch.search.SearchTimeoutIT\")\n        includeClasses.add(\"org.opensearch.search.SearchWeightedRoutingIT\")\n        includeClasses.add(\"org.opensearch.search.aggregations.bucket.DoubleTermsIT\")\n        includeClasses.add(\"org.opensearch.search.aggregations.bucket.terms.StringTermsIT\")\n        includeClasses.add(\"org.opensearch.search.aggregations.metrics.CardinalityIT\")\n        includeClasses.add(\"org.opensearch.search.backpressure.SearchBackpressureIT\")\n        includeClasses.add(\"org.opensearch.search.basic.SearchWithRandomIOExceptionsIT\")\n        includeClasses.add(\"org.opensearch.search.pit.DeletePitMultiNodeIT\")\n        includeClasses.add(\"org.opensearch.smoketest.SmokeTestMultiNodeClientYamlTestSuiteIT\")\n        includeClasses.add(\"org.opensearch.snapshots.CloneSnapshotIT\")\n        includeClasses.add(\"org.opensearch.snapshots.DedicatedClusterSnapshotRestoreIT\")\n        includeClasses.add(\"org.opensearch.snapshots.RestoreSnapshotIT\")\n        includeClasses.add(\"org.opensearch.snapshots.SnapshotStatusApisIT\")\n        includeClasses.add(\"org.opensearch.test.rest.ClientYamlTestSuiteIT\")\n        includeClasses.add(\"org.opensearch.upgrade.DetectEsInstallationTaskTests\")\n        includeClasses.add(\"org.opensearch.cluster.MinimumClusterManagerNodesIT\")\n      }\n    }\n  }\n}\n\n// eclipse configuration\nallprojects {\n  apply plugin: 'eclipse'\n\n  // Name all the non-root projects after their path so that paths get grouped together when imported into eclipse.\n  if (path != ':') {\n    eclipse.project.name = path\n    if (Os.isFamily(Os.FAMILY_WINDOWS)) {\n      eclipse.project.name = eclipse.project.name.replace(':', '_')\n    }\n  }\n\n  plugins.withType(JavaBasePlugin) {\n    eclipse.classpath.defaultOutputDir = file('build-eclipse')\n    eclipse.classpath.file.whenMerged { classpath ->\n      // give each source folder a unique corresponding output folder\n      int i = 0;\n      classpath.entries.findAll { it instanceof SourceFolder }.each { folder ->\n        i++;\n        folder.output = \"build-eclipse/\" + i\n      }\n    }\n  }\n  /*\n   * Allow accessing com/sun/net/httpserver in projects that have\n   * configured forbidden apis to allow it.\n   */\n  plugins.withType(ForbiddenApisPlugin) {\n    eclipse.classpath.file.whenMerged { classpath ->\n      if (false == forbiddenApisTest.bundledSignatures.contains('jdk-non-portable')) {\n        classpath.entries\n          .findAll { it.kind == \"con\" && it.toString().contains(\"org.eclipse.jdt.launching.JRE_CONTAINER\") }\n          .each {\n            it.accessRules.add(new AccessRule(\"accessible\", \"com/sun/net/httpserver/*\"))\n          }\n      }\n    }\n  }\n\n  File licenseHeaderFile\n  licenseHeaderFile = new File(project.rootDir, 'buildSrc/src/main/resources/license-headers/license-header.txt')\n\n  String lineSeparator = Os.isFamily(Os.FAMILY_WINDOWS) ? '\\\\\\\\r\\\\\\\\n' : '\\\\\\\\n'\n  String licenseHeader = licenseHeaderFile.getText('UTF-8').replace(System.lineSeparator(), lineSeparator)\n  tasks.register('copyEclipseSettings', Copy) {\n    mustRunAfter 'wipeEclipseSettings'\n    // TODO: \"package this up\" for external builds\n    from new File(project.rootDir, 'buildSrc/src/main/resources/eclipse.settings')\n    into '.settings'\n    filter { it.replaceAll('@@LICENSE_HEADER_TEXT@@', licenseHeader) }\n  }\n  // otherwise .settings is not nuked entirely\n  tasks.register('wipeEclipseSettings', Delete) {\n    delete '.settings'\n  }\n  tasks.named('cleanEclipse') { dependsOn 'wipeEclipseSettings' }\n  // otherwise the eclipse merging is *super confusing*\n  tasks.named('eclipse') { dependsOn 'cleanEclipse', 'copyEclipseSettings' }\n\n  afterEvaluate {\n    tasks.findByName(\"eclipseJdt\")?.configure {\n      dependsOn 'copyEclipseSettings'\n    }\n  }\n}\n\nwrapper {\n  distributionType = 'ALL'\n  doLast {\n    def sha256Sum = new String(new URL(getDistributionUrl() + \".sha256\").bytes)\n    propertiesFile << \"distributionSha256Sum=${sha256Sum}\\n\"\n    println \"Added checksum to wrapper properties\"\n    // Update build-tools to reflect the Gradle upgrade\n    // TODO: we can remove this once we have tests to make sure older versions work.\n    project(':build-tools').file('src/main/resources/minimumGradleVersion').text = gradleVersion + \"\\n\"\n    println \"Updated minimum Gradle Version\"\n  }\n}\n\ngradle.projectsEvaluated {\n  subprojects {\n    /*\n     * Remove assemble/dependenciesInfo on all qa projects because we don't\n     * need to publish artifacts for them.\n     */\n    if (project.name.equals('qa') || project.path.contains(':qa:')) {\n      maybeConfigure(project.tasks, 'assemble') {\n        it.enabled = false\n      }\n      maybeConfigure(project.tasks, 'dependenciesInfo') {\n        it.enabled = false\n      }\n    }\n  }\n  // Having the same group and name for distinct projects causes Gradle to consider them equal when resolving\n  // dependencies leading to hard to debug failures. Run a check across all project to prevent this from happening.\n  // see: https://github.com/gradle/gradle/issues/847\n  Map coordsToProject = [:]\n  project.allprojects.forEach { p ->\n    String coords = \"${p.group}:${p.name}\"\n    if (false == coordsToProject.putIfAbsent(coords, p)) {\n      throw new GradleException(\n        \"Detected that two projects: ${p.path} and ${coordsToProject[coords].path} \" +\n          \"have the same name and group: ${coords}. \" +\n          \"This doesn't currently work correctly in Gradle, see: \" +\n          \"https://github.com/gradle/gradle/issues/847\"\n      )\n    }\n  }\n}\n\nallprojects {\n  tasks.register('resolveAllDependencies', org.opensearch.gradle.ResolveAllDependencies) {\n    configs = project.configurations\n    if (project.path.contains(\"fixture\")) {\n      dependsOn tasks.withType(ComposePull)\n    }\n  }\n\n  // helper task to print direct dependencies of a single task\n  project.tasks.addRule(\"Pattern: <taskName>Dependencies\") { String taskName ->\n    if (taskName.endsWith(\"Dependencies\") == false) {\n      return\n    }\n    if (project.tasks.findByName(taskName) != null) {\n      return\n    }\n    String realTaskName = taskName.substring(0, taskName.length() - \"Dependencies\".length())\n    Task realTask = project.tasks.findByName(realTaskName)\n    if (realTask == null) {\n      return\n    }\n    project.tasks.register(taskName) {\n      doLast {\n        println(\"${realTask.path} dependencies:\")\n        for (Task dep : realTask.getTaskDependencies().getDependencies(realTask)) {\n          println(\"  - ${dep.path}\")\n        }\n      }\n    }\n  }\n\n  def checkPart1 = tasks.register('checkPart1')\n  def checkPart2 = tasks.register('checkPart2')\n  plugins.withId('lifecycle-base') {\n    checkPart1.configure { dependsOn 'check' }\n  }\n}\n\nsubprojects {\n  project.ext.disableTasks = { String... tasknames ->\n    for (String taskname : tasknames) {\n      project.tasks.named(taskname).configure { onlyIf { false } }\n    }\n  }\n}\n\nreporting {\n  reports {\n    testAggregateTestReport(AggregateTestReport) {\n      testType = TestSuiteType.UNIT_TEST\n    }\n  }\n}\n\n// Enable XML test reports for Jenkins integration\ntasks.withType(TestTaskReports).configureEach {\n  junitXml.enabled = true\n}\n\ntasks.named(JavaBasePlugin.CHECK_TASK_NAME) {\n  dependsOn tasks.named('testAggregateTestReport', TestReport)\n}\n\ntasks.register('checkCompatibility', CheckCompatibilityTask) {\n  description = 'Checks the compatibility with child components'\n}\n\nallprojects { project ->\n  project.afterEvaluate {\n    if (project.tasks.findByName('publishToMavenLocal')) {\n      checkCompatibility.dependsOn(project.tasks.publishToMavenLocal)\n    }\n  }\n}\n"
        },
        {
          "name": "buildSrc",
          "type": "tree",
          "content": null
        },
        {
          "name": "client",
          "type": "tree",
          "content": null
        },
        {
          "name": "codecov.yml",
          "type": "blob",
          "size": 0.2763671875,
          "content": "codecov:\n  require_ci_to_pass: yes\n\nignore:\n  - \"test\"\n  - \"benchmarks\"\n\ncoverage:\n  precision: 2\n  round: down\n  range: \"70...100\"\n  status:\n    project:\n      default:\n        target: 70%    # the required coverage value\n        threshold: 1%  # the leniency in hitting the target\n"
        },
        {
          "name": "dev-tools",
          "type": "tree",
          "content": null
        },
        {
          "name": "distribution",
          "type": "tree",
          "content": null
        },
        {
          "name": "doc-tools",
          "type": "tree",
          "content": null
        },
        {
          "name": "docs",
          "type": "tree",
          "content": null
        },
        {
          "name": "gradle.properties",
          "type": "blob",
          "size": 1.4384765625,
          "content": "#\n# SPDX-License-Identifier: Apache-2.0\n#\n# The OpenSearch Contributors require contributions made to\n# this file be licensed under the Apache-2.0 license or a\n# compatible open source license.\n#\n# Modifications Copyright OpenSearch Contributors. See\n# GitHub history for details.\n#\n\n# Enable build caching\norg.gradle.caching=true\norg.gradle.warning.mode=none\norg.gradle.parallel=true\norg.gradle.jvmargs=-Xmx3g -XX:+HeapDumpOnOutOfMemoryError -Xss2m \\\n    --add-exports jdk.compiler/com.sun.tools.javac.api=ALL-UNNAMED \\\n    --add-exports jdk.compiler/com.sun.tools.javac.file=ALL-UNNAMED \\\n    --add-exports jdk.compiler/com.sun.tools.javac.parser=ALL-UNNAMED \\\n    --add-exports jdk.compiler/com.sun.tools.javac.tree=ALL-UNNAMED \\\n    --add-exports jdk.compiler/com.sun.tools.javac.util=ALL-UNNAMED\noptions.forkOptions.memoryMaximumSize=3g\n\n# Disable Gradle Enterprise Gradle plugin's test retry\nsystemProp.develocity.testretry.enabled.enabled=false\n\n# Disable duplicate project id detection\n# See https://docs.gradle.org/current/userguide/upgrading_version_6.html#duplicate_project_names_may_cause_publication_to_fail\nsystemProp.org.gradle.dependency.duplicate.project.detection=false\n\n# Enforce the build to fail on deprecated gradle api usage\nsystemProp.org.gradle.warning.mode=fail\n\nsystemProp.jdk.tls.client.protocols=TLSv1.2,TLSv1.3\n\n# jvm args for faster test execution by default\nsystemProp.tests.jvm.argline=-XX:TieredStopAtLevel=1 -XX:ReservedCodeCacheSize=64m\n"
        },
        {
          "name": "gradle",
          "type": "tree",
          "content": null
        },
        {
          "name": "gradlew",
          "type": "blob",
          "size": 8.556640625,
          "content": "#!/bin/sh\n\n#\n# Copyright  2015-2021 the original authors.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n# SPDX-License-Identifier: Apache-2.0\n#\n\n##############################################################################\n#\n#   Gradle start up script for POSIX generated by Gradle.\n#\n#   Important for running:\n#\n#   (1) You need a POSIX-compliant shell to run this script. If your /bin/sh is\n#       noncompliant, but you have some other compliant shell such as ksh or\n#       bash, then to run this script, type that shell name before the whole\n#       command line, like:\n#\n#           ksh Gradle\n#\n#       Busybox and similar reduced shells will NOT work, because this script\n#       requires all of these POSIX shell features:\n#         * functions;\n#         * expansions $var, ${var}, ${var:-default}, ${var+SET},\n#           ${var#prefix}, ${var%suffix}, and $( cmd );\n#         * compound commands having a testable exit status, especially case;\n#         * various built-in commands including command, set, and ulimit.\n#\n#   Important for patching:\n#\n#   (2) This script targets any POSIX shell, so it avoids extensions provided\n#       by Bash, Ksh, etc; in particular arrays are avoided.\n#\n#       The \"traditional\" practice of packing multiple parameters into a\n#       space-separated string is a well documented source of bugs and security\n#       problems, so this is (mostly) avoided, by progressively accumulating\n#       options in \"$@\", and eventually passing that to Java.\n#\n#       Where the inherited environment variables (DEFAULT_JVM_OPTS, JAVA_OPTS,\n#       and GRADLE_OPTS) rely on word-splitting, this is performed explicitly;\n#       see the in-line comments for details.\n#\n#       There are tweaks for specific operating systems such as AIX, CygWin,\n#       Darwin, MinGW, and NonStop.\n#\n#   (3) This script is generated from the Groovy template\n#       https://github.com/gradle/gradle/blob/HEAD/platforms/jvm/plugins-application/src/main/resources/org/gradle/api/internal/plugins/unixStartScript.txt\n#       within the Gradle project.\n#\n#       You can find Gradle at https://github.com/gradle/gradle/.\n#\n##############################################################################\n\n# Attempt to set APP_HOME\n\n# Resolve links: $0 may be a link\napp_path=$0\n\n# Need this for daisy-chained symlinks.\nwhile\n    APP_HOME=${app_path%\"${app_path##*/}\"}  # leaves a trailing /; empty if no leading path\n    [ -h \"$app_path\" ]\ndo\n    ls=$( ls -ld \"$app_path\" )\n    link=${ls#*' -> '}\n    case $link in             #(\n      /*)   app_path=$link ;; #(\n      *)    app_path=$APP_HOME$link ;;\n    esac\ndone\n\n# This is normally unused\n# shellcheck disable=SC2034\nAPP_BASE_NAME=${0##*/}\n# Discard cd standard output in case $CDPATH is set (https://github.com/gradle/gradle/issues/25036)\nAPP_HOME=$( cd -P \"${APP_HOME:-./}\" > /dev/null && printf '%s\n' \"$PWD\" ) || exit\n\n# Use the maximum available, or set MAX_FD != -1 to use that value.\nMAX_FD=maximum\n\nwarn () {\n    echo \"$*\"\n} >&2\n\ndie () {\n    echo\n    echo \"$*\"\n    echo\n    exit 1\n} >&2\n\n# OS specific support (must be 'true' or 'false').\ncygwin=false\nmsys=false\ndarwin=false\nnonstop=false\ncase \"$( uname )\" in                #(\n  CYGWIN* )         cygwin=true  ;; #(\n  Darwin* )         darwin=true  ;; #(\n  MSYS* | MINGW* )  msys=true    ;; #(\n  NONSTOP* )        nonstop=true ;;\nesac\n\nCLASSPATH=$APP_HOME/gradle/wrapper/gradle-wrapper.jar\n\n\n# Determine the Java command to use to start the JVM.\nif [ -n \"$JAVA_HOME\" ] ; then\n    if [ -x \"$JAVA_HOME/jre/sh/java\" ] ; then\n        # IBM's JDK on AIX uses strange locations for the executables\n        JAVACMD=$JAVA_HOME/jre/sh/java\n    else\n        JAVACMD=$JAVA_HOME/bin/java\n    fi\n    if [ ! -x \"$JAVACMD\" ] ; then\n        die \"ERROR: JAVA_HOME is set to an invalid directory: $JAVA_HOME\n\nPlease set the JAVA_HOME variable in your environment to match the\nlocation of your Java installation.\"\n    fi\nelse\n    JAVACMD=java\n    if ! command -v java >/dev/null 2>&1\n    then\n        die \"ERROR: JAVA_HOME is not set and no 'java' command could be found in your PATH.\n\nPlease set the JAVA_HOME variable in your environment to match the\nlocation of your Java installation.\"\n    fi\nfi\n\n# Increase the maximum file descriptors if we can.\nif ! \"$cygwin\" && ! \"$darwin\" && ! \"$nonstop\" ; then\n    case $MAX_FD in #(\n      max*)\n        # In POSIX sh, ulimit -H is undefined. That's why the result is checked to see if it worked.\n        # shellcheck disable=SC2039,SC3045\n        MAX_FD=$( ulimit -H -n ) ||\n            warn \"Could not query maximum file descriptor limit\"\n    esac\n    case $MAX_FD in  #(\n      '' | soft) :;; #(\n      *)\n        # In POSIX sh, ulimit -n is undefined. That's why the result is checked to see if it worked.\n        # shellcheck disable=SC2039,SC3045\n        ulimit -n \"$MAX_FD\" ||\n            warn \"Could not set maximum file descriptor limit to $MAX_FD\"\n    esac\nfi\n\n# Collect all arguments for the java command, stacking in reverse order:\n#   * args from the command line\n#   * the main class name\n#   * -classpath\n#   * -D...appname settings\n#   * --module-path (only if needed)\n#   * DEFAULT_JVM_OPTS, JAVA_OPTS, and GRADLE_OPTS environment variables.\n\n# For Cygwin or MSYS, switch paths to Windows format before running java\nif \"$cygwin\" || \"$msys\" ; then\n    APP_HOME=$( cygpath --path --mixed \"$APP_HOME\" )\n    CLASSPATH=$( cygpath --path --mixed \"$CLASSPATH\" )\n\n    JAVACMD=$( cygpath --unix \"$JAVACMD\" )\n\n    # Now convert the arguments - kludge to limit ourselves to /bin/sh\n    for arg do\n        if\n            case $arg in                                #(\n              -*)   false ;;                            # don't mess with options #(\n              /?*)  t=${arg#/} t=/${t%%/*}              # looks like a POSIX filepath\n                    [ -e \"$t\" ] ;;                      #(\n              *)    false ;;\n            esac\n        then\n            arg=$( cygpath --path --ignore --mixed \"$arg\" )\n        fi\n        # Roll the args list around exactly as many times as the number of\n        # args, so each arg winds up back in the position where it started, but\n        # possibly modified.\n        #\n        # NB: a `for` loop captures its iteration list before it begins, so\n        # changing the positional parameters here affects neither the number of\n        # iterations, nor the values presented in `arg`.\n        shift                   # remove old arg\n        set -- \"$@\" \"$arg\"      # push replacement arg\n    done\nfi\n\n\n# Add default JVM options here. You can also use JAVA_OPTS and GRADLE_OPTS to pass JVM options to this script.\nDEFAULT_JVM_OPTS='\"-Xmx64m\" \"-Xms64m\"'\n\n# Collect all arguments for the java command:\n#   * DEFAULT_JVM_OPTS, JAVA_OPTS, JAVA_OPTS, and optsEnvironmentVar are not allowed to contain shell fragments,\n#     and any embedded shellness will be escaped.\n#   * For example: A user cannot expect ${Hostname} to be expanded, as it is an environment variable and will be\n#     treated as '${Hostname}' itself on the command line.\n\nset -- \\\n        \"-Dorg.gradle.appname=$APP_BASE_NAME\" \\\n        -classpath \"$CLASSPATH\" \\\n        org.gradle.wrapper.GradleWrapperMain \\\n        \"$@\"\n\n# Stop when \"xargs\" is not available.\nif ! command -v xargs >/dev/null 2>&1\nthen\n    die \"xargs is not available\"\nfi\n\n# Use \"xargs\" to parse quoted args.\n#\n# With -n1 it outputs one arg per line, with the quotes and backslashes removed.\n#\n# In Bash we could simply go:\n#\n#   readarray ARGS < <( xargs -n1 <<<\"$var\" ) &&\n#   set -- \"${ARGS[@]}\" \"$@\"\n#\n# but POSIX shell has neither arrays nor command substitution, so instead we\n# post-process each arg (as a line of input to sed) to backslash-escape any\n# character that might be a shell metacharacter, then use eval to reverse\n# that process (while maintaining the separation between arguments), and wrap\n# the whole thing up as a single \"set\" statement.\n#\n# This will of course break if any of these variables contains a newline or\n# an unmatched quote.\n#\n\neval \"set -- $(\n        printf '%s\\n' \"$DEFAULT_JVM_OPTS $JAVA_OPTS $GRADLE_OPTS\" |\n        xargs -n1 |\n        sed ' s~[^-[:alnum:]+,./:=@_]~\\\\&~g; ' |\n        tr '\\n' ' '\n    )\" '\"$@\"'\n\nexec \"$JAVACMD\" \"$@\"\n"
        },
        {
          "name": "gradlew.bat",
          "type": "blob",
          "size": 2.896484375,
          "content": "@rem\r\n@rem Copyright 2015 the original author or authors.\r\n@rem\r\n@rem Licensed under the Apache License, Version 2.0 (the \"License\");\r\n@rem you may not use this file except in compliance with the License.\r\n@rem You may obtain a copy of the License at\r\n@rem\r\n@rem      https://www.apache.org/licenses/LICENSE-2.0\r\n@rem\r\n@rem Unless required by applicable law or agreed to in writing, software\r\n@rem distributed under the License is distributed on an \"AS IS\" BASIS,\r\n@rem WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n@rem See the License for the specific language governing permissions and\r\n@rem limitations under the License.\r\n@rem\r\n@rem SPDX-License-Identifier: Apache-2.0\r\n@rem\r\n\r\n@if \"%DEBUG%\"==\"\" @echo off\r\n@rem ##########################################################################\r\n@rem\r\n@rem  Gradle startup script for Windows\r\n@rem\r\n@rem ##########################################################################\r\n\r\n@rem Set local scope for the variables with windows NT shell\r\nif \"%OS%\"==\"Windows_NT\" setlocal\r\n\r\nset DIRNAME=%~dp0\r\nif \"%DIRNAME%\"==\"\" set DIRNAME=.\r\n@rem This is normally unused\r\nset APP_BASE_NAME=%~n0\r\nset APP_HOME=%DIRNAME%\r\n\r\n@rem Resolve any \".\" and \"..\" in APP_HOME to make it shorter.\r\nfor %%i in (\"%APP_HOME%\") do set APP_HOME=%%~fi\r\n\r\n@rem Add default JVM options here. You can also use JAVA_OPTS and GRADLE_OPTS to pass JVM options to this script.\r\nset DEFAULT_JVM_OPTS=\"-Xmx64m\" \"-Xms64m\"\r\n\r\n@rem Find java.exe\r\nif defined JAVA_HOME goto findJavaFromJavaHome\r\n\r\nset JAVA_EXE=java.exe\r\n%JAVA_EXE% -version >NUL 2>&1\r\nif %ERRORLEVEL% equ 0 goto execute\r\n\r\necho. 1>&2\r\necho ERROR: JAVA_HOME is not set and no 'java' command could be found in your PATH. 1>&2\r\necho. 1>&2\r\necho Please set the JAVA_HOME variable in your environment to match the 1>&2\r\necho location of your Java installation. 1>&2\r\n\r\ngoto fail\r\n\r\n:findJavaFromJavaHome\r\nset JAVA_HOME=%JAVA_HOME:\"=%\r\nset JAVA_EXE=%JAVA_HOME%/bin/java.exe\r\n\r\nif exist \"%JAVA_EXE%\" goto execute\r\n\r\necho. 1>&2\r\necho ERROR: JAVA_HOME is set to an invalid directory: %JAVA_HOME% 1>&2\r\necho. 1>&2\r\necho Please set the JAVA_HOME variable in your environment to match the 1>&2\r\necho location of your Java installation. 1>&2\r\n\r\ngoto fail\r\n\r\n:execute\r\n@rem Setup the command line\r\n\r\nset CLASSPATH=%APP_HOME%\\gradle\\wrapper\\gradle-wrapper.jar\r\n\r\n\r\n@rem Execute Gradle\r\n\"%JAVA_EXE%\" %DEFAULT_JVM_OPTS% %JAVA_OPTS% %GRADLE_OPTS% \"-Dorg.gradle.appname=%APP_BASE_NAME%\" -classpath \"%CLASSPATH%\" org.gradle.wrapper.GradleWrapperMain %*\r\n\r\n:end\r\n@rem End local scope for the variables with windows NT shell\r\nif %ERRORLEVEL% equ 0 goto mainEnd\r\n\r\n:fail\r\nrem Set variable GRADLE_EXIT_CONSOLE if you need the _script_ return code instead of\r\nrem the _cmd.exe /c_ return code!\r\nset EXIT_CODE=%ERRORLEVEL%\r\nif %EXIT_CODE% equ 0 set EXIT_CODE=1\r\nif not \"\"==\"%GRADLE_EXIT_CONSOLE%\" exit %EXIT_CODE%\r\nexit /b %EXIT_CODE%\r\n\r\n:mainEnd\r\nif \"%OS%\"==\"Windows_NT\" endlocal\r\n\r\n:omega\r\n"
        },
        {
          "name": "libs",
          "type": "tree",
          "content": null
        },
        {
          "name": "licenses",
          "type": "tree",
          "content": null
        },
        {
          "name": "modules",
          "type": "tree",
          "content": null
        },
        {
          "name": "plugins",
          "type": "tree",
          "content": null
        },
        {
          "name": "qa",
          "type": "tree",
          "content": null
        },
        {
          "name": "release-notes",
          "type": "tree",
          "content": null
        },
        {
          "name": "rest-api-spec",
          "type": "tree",
          "content": null
        },
        {
          "name": "sandbox",
          "type": "tree",
          "content": null
        },
        {
          "name": "scripts",
          "type": "tree",
          "content": null
        },
        {
          "name": "server",
          "type": "tree",
          "content": null
        },
        {
          "name": "settings.gradle",
          "type": "blob",
          "size": 5.216796875,
          "content": "/*\n * SPDX-License-Identifier: Apache-2.0\n *\n * The OpenSearch Contributors require contributions made to\n * this file be licensed under the Apache-2.0 license or a\n * compatible open source license.\n *\n * Modifications Copyright OpenSearch Contributors. See\n * GitHub history for details.\n */\n\nplugins {\n  id \"com.gradle.develocity\" version \"3.19\"\n}\n\next.disableBuildCache = hasProperty('DISABLE_BUILD_CACHE') || System.getenv().containsKey('DISABLE_BUILD_CACHE')\n\nbuildCache {\n  local {\n    enabled = !disableBuildCache\n  }\n}\n\nrootProject.name = \"OpenSearch\"\n\ninclude 'doc-tools'\nincludeBuild(\"doc-tools/missing-doclet\")\n\nList projects = [\n  'build-tools',\n  'build-tools:reaper',\n  'rest-api-spec',\n  'docs',\n  'client:rest',\n  'client:rest-high-level',\n  'client:sniffer',\n  'client:test',\n  'client:client-benchmark-noop-api-plugin',\n  'client:benchmark',\n  'benchmarks',\n  'distribution:archives:integ-test-zip',\n  'distribution:archives:windows-zip',\n  'distribution:archives:no-jdk-windows-zip',\n  'distribution:archives:darwin-tar',\n  'distribution:archives:darwin-arm64-tar',\n  'distribution:archives:no-jdk-darwin-arm64-tar',\n  'distribution:archives:no-jdk-darwin-tar',\n  'distribution:archives:freebsd-tar',\n  'distribution:archives:no-jdk-freebsd-tar',\n  'distribution:archives:linux-arm64-tar',\n  'distribution:archives:no-jdk-linux-arm64-tar',\n  'distribution:archives:linux-s390x-tar',\n  'distribution:archives:linux-ppc64le-tar',\n  'distribution:archives:no-jdk-linux-ppc64le-tar',\n  'distribution:archives:linux-tar',\n  'distribution:archives:no-jdk-linux-tar',\n  'distribution:archives:jre-linux-tar',\n  'distribution:docker',\n  'distribution:docker:docker-arm64-build-context',\n  'distribution:docker:docker-arm64-export',\n  'distribution:docker:docker-s390x-export',\n  'distribution:docker:docker-ppc64le-export',\n  'distribution:docker:docker-build-context',\n  'distribution:docker:docker-export',\n  'distribution:packages:arm64-deb',\n  'distribution:packages:no-jdk-arm64-deb',\n  'distribution:packages:deb',\n  'distribution:packages:no-jdk-deb',\n  'distribution:packages:arm64-rpm',\n  'distribution:packages:no-jdk-arm64-rpm',\n  'distribution:packages:rpm',\n  'distribution:packages:no-jdk-rpm',\n  'distribution:bwc:bugfix',\n  'distribution:bwc:maintenance',\n  'distribution:bwc:minor',\n  'distribution:bwc:staged',\n  'distribution:tools:java-version-checker',\n  'distribution:tools:launchers',\n  'distribution:tools:plugin-cli',\n  'distribution:tools:keystore-cli',\n  'distribution:tools:upgrade-cli',\n  'server',\n  'server:cli',\n  'test:framework',\n  'test:fixtures:azure-fixture',\n  'test:fixtures:gcs-fixture',\n  'test:fixtures:hdfs-fixture',\n  'test:fixtures:krb5kdc-fixture',\n  'test:fixtures:minio-fixture',\n  'test:fixtures:old-elasticsearch',\n  'test:fixtures:s3-fixture',\n  'test:logger-usage',\n  'test:telemetry'\n]\n\n/**\n * Iterates over sub directories, looking for build.gradle, and adds a project if found\n * for that dir with the given path prefix. Note that this requires each level\n * of the dir hierarchy to have a build.gradle. Otherwise we would have to iterate\n * all files/directories in the source tree to find all projects.\n */\nvoid addSubProjects(String path, File dir) {\n  if (dir.isDirectory() == false) return;\n  if (dir.name == 'buildSrc') return;\n  if (new File(dir, 'build.gradle').exists() == false) return;\n  if (findProject(dir) != null) return;\n\n  final String projectName = \"${path}:${dir.name}\"\n  include projectName\n  if (path.isEmpty() || path.startsWith(':example-plugins')) {\n    project(projectName).projectDir = dir\n  }\n  for (File subdir : dir.listFiles()) {\n    addSubProjects(projectName, subdir)\n  }\n}\n\n\n// include example plugins first, so adding plugin dirs below won't muck with :example-plugins\nFile examplePluginsDir = new File(rootProject.projectDir, 'plugins/examples')\nfor (File example : examplePluginsDir.listFiles()) {\n  if (example.isDirectory() == false) continue;\n  if (example.name.startsWith('build') || example.name.startsWith('.')) continue;\n  addSubProjects(':example-plugins', example)\n}\nproject(':example-plugins').projectDir = new File(rootProject.projectDir, 'plugins/examples')\n\naddSubProjects('', new File(rootProject.projectDir, 'libs'))\naddSubProjects('', new File(rootProject.projectDir, 'modules'))\naddSubProjects('', new File(rootProject.projectDir, 'plugins'))\naddSubProjects('', new File(rootProject.projectDir, 'sandbox'))\naddSubProjects('', new File(rootProject.projectDir, 'qa'))\naddSubProjects('test', new File(rootProject.projectDir, 'test/external-modules'))\n\nList startTasks = gradle.startParameter.taskNames\n\ninclude projects.toArray(new String[0])\n\nproject(':build-tools').projectDir = new File(rootProject.projectDir, 'buildSrc')\nproject(':build-tools:reaper').projectDir = new File(rootProject.projectDir, 'buildSrc/reaper')\n\nproject(\":libs\").children.each { libsProject ->\n  libsProject.name = \"opensearch-${libsProject.name}\"\n}\n\nproject(\":test:external-modules\").children.each { testProject ->\n  testProject.name = \"test-${testProject.name}\"\n}\n\n// look for extra plugins for opensearch\nFile extraProjects = new File(rootProject.projectDir.parentFile, \"${rootProject.projectDir.name}-extra\")\nif (extraProjects.exists()) {\n  for (File extraProjectDir : extraProjects.listFiles()) {\n    addSubProjects('', extraProjectDir)\n  }\n}\n"
        },
        {
          "name": "test",
          "type": "tree",
          "content": null
        },
        {
          "name": "whitesource.config",
          "type": "blob",
          "size": 9.5791015625,
          "content": "###############################################################\n# WhiteSource Unified-Agent configuration file\n###############################################################\n# GENERAL SCAN MODE: Files and Package Managers\n###############################################################\n# Organization vitals\n######################\n\n#apiKey='${wss_apikey}'\napiKey=\n#userKey is required if WhiteSource administrator has enabled \"Enforce user level access\" option\n#userKey=\n#requesterEmail=user@provider.com\n\nprojectName=\nprojectVersion=\nprojectToken=\n#projectTag= key:value\n\nproductName=\nproductVersion=\nproductToken=\n\n#projectPerFolder=true\n#projectPerFolderIncludes=\n#projectPerFolderExcludes=\n\n#wss.connectionTimeoutMinutes=60\n\n# Change the below URL to your WhiteSource server.\n# Use the 'WhiteSource Server URL' which can be retrieved\n# from your 'Profile' page on the 'Server URLs' panel.\n# Then, add the '/agent' path to it.\nwss.url=https://saas.whitesourcesoftware.com/agent\n#wss.url=https://app.whitesourcesoftware.com/agent\n#wss.url=https://app-eu.whitesourcesoftware.com/agent\n\n############\n# Policies #\n############\ncheckPolicies=false\nforceCheckAllDependencies=false\nforceUpdate=false\nforceUpdate.failBuildOnPolicyViolation=false\n#updateInventory=false\n\n###########\n# General #\n###########\n#offline=false\n#updateType=APPEND\n#ignoreSourceFiles=true\n#scanComment=\n#failErrorLevel=ALL\n#requireKnownSha1=false\n\n#generateProjectDetailsJson=true\n#generateScanReport=true\n#scanReportTimeoutMinutes=10\n#scanReportFilenameFormat=\n\n#analyzeFrameworks=true\n#analyzeFrameworksReference=\n\n#updateEmptyProject=false\n\n#log.files.level=\n#log.files.maxFileSize=\n#log.files.maxFilesCount=\n#log.files.path=\n\n########################################\n# Package Manager Dependency resolvers #\n########################################\nresolveAllDependencies=false\n#excludeDependenciesFromNodes=.*commons-io.*,.*maven-model\n\n#npm.resolveDependencies=false\n#npm.ignoreSourceFiles=false\n#npm.includeDevDependencies=true\n#npm.runPreStep=true\n#npm.ignoreNpmLsErrors=true\n#npm.ignoreScripts=true\n#npm.yarnProject=true\n#npm.accessToken=\n#npm.identifyByNameAndVersion=true\n#npm.yarn.frozenLockfile=true\n#npm.resolveMainPackageJsonOnly=true\n#npm.removeDuplicateDependencies=false\n#npm.resolveAdditionalDependencies=true\n#npm.failOnNpmLsErrors =\n#npm.projectNameFromDependencyFile = true\n#npm.resolveGlobalPackages=true\n#npm.resolveLockFile=true\n\n#bower.resolveDependencies=false\n#bower.ignoreSourceFiles=true\n#bower.runPreStep=true\n\n#nuget.resolvePackagesConfigFiles=false\n#nuget.resolveCsProjFiles=false\n#nuget.resolveDependencies=false\n#nuget.restoreDependencies=true\n#nuget.preferredEnvironment=\n#nuget.packagesDirectory=\n#nuget.ignoreSourceFiles=false\n#nuget.runPreStep=true\n#nuget.resolveNuspecFiles=false\n#nuget.resolveAssetsFiles=true\n\n#python.resolveDependencies=false\n#python.ignoreSourceFiles=false\n#python.ignorePipInstallErrors=true\n#python.installVirtualenv=true\n#python.resolveHierarchyTree=false\n#python.requirementsFileIncludes=requirements.txt\n#python.resolveSetupPyFiles=true\n#python.runPipenvPreStep=true\n#python.pipenvDevDependencies=true\n#python.IgnorePipenvInstallErrors=true\n#python.resolveGlobalPackages=true\n#python.localPackagePathsToInstall=/path/to/local/dependency.egg, /path/to/local/dependency.zip\n#python.resolvePipEditablePackages\n#python.path=/path/to/python\n#python.pipPath=/path/to/pip\n#python.runPoetryPreStep=true\n#python.includePoetryDevDependencies=true\n\n#maven.ignoredScopes=test provided\n#maven.resolveDependencies=false\n#maven.ignoreSourceFiles=true\n#maven.aggregateModules=true\n#maven.ignorePomModules=false\n#maven.runPreStep=true\n#maven.ignoreMvnTreeErrors=true\n#maven.environmentPath=\n#maven.m2RepositoryPath=\n#maven.downloadMissingDependencies=false\n#maven.additionalArguments=\n#maven.projectNameFromDependencyFile=true\n\nresolveAllDependencies=false\narchiveExtractionDepth=7\nfollowSymbolicLinks=true\ngradle.resolveDependencies=true\ngradle.aggregateModules=true\ngradle.preferredEnvironment=wrapper\ngradle.excludeModules=./qa/*\nmaven.resolveDependencies=true\nmaven.runPreStep=true\nmaven.aggregateModules=true\nmaven.ignoredScopes=None\nhtml.resolveDependencies=true\nnpm.resolveDependencies=true\nnpm.runPreStep=true\nnpm.yarnProject=true\ngo.resolveDependencies=true\ngo.collectDependenciesAtRuntime=true\ngo.dependencyManager=\npython.resolveDependencies=true\npython.ignoreSourceFiles=true\npython.runPipenvPreStep=true\npython.pipenvDevDependencies=true\npython.requirementsFileIncludes=dev-requirements.txt\npython.installVirtualenv=true\nruby.resolveDependencies=true\nruby.ignoreSourceFiles=false\n\n#gradle.ignoredScopes=\n#gradle.resolveDependencies=true\n#gradle.runAssembleCommand=true\n#gradle.runPreStep=true\n#gradle.ignoreSourceFiles=true\n#gradle.aggregateModules=true\n#gradle.preferredEnvironment=wrapper\n#gradle.localRepositoryPath=\n#gradle.wrapperPath=\n#gradle.downloadMissingDependencies=false\n#gradle.additionalArguments=\n#gradle.includedScopes=\n#gradle.excludeModules=\n#gradle.includeModules=\n#gradle.includedConfigurations=\n#gradle.ignoredConfigurations=\n\n#paket.resolveDependencies=false\n#paket.ignoredGroups=\n#paket.ignoreSourceFiles=false\n#paket.runPreStep=true\n#paket.exePath=\n\n#go.resolveDependencies=false\n#go.collectDependenciesAtRuntime=true\n#go.dependencyManager=\n#go.ignoreSourceFiles=true\n#go.glide.ignoreTestPackages=false\n#go.gogradle.enableTaskAlias=true\n\n#ruby.resolveDependencies=false\n#ruby.ignoreSourceFiles=false\n#ruby.installMissingGems=true\n#ruby.runBundleInstall=true\n#ruby.overwriteGemFile=true\n\n#sbt.resolveDependencies=false\n#sbt.ignoreSourceFiles=true\n#sbt.aggregateModules=true\n#sbt.runPreStep=true\n#sbt.includedScopes=\n\n#php.resolveDependencies=false\n#php.runPreStep=true\n#php.includeDevDependencies=true\n\n#html.resolveDependencies=false\n\n#cocoapods.resolveDependencies=false\n#cocoapods.runPreStep=true\n#cocoapods.ignoreSourceFiles=false\n\n#hex.resolveDependencies=false\n#hex.runPreStep=true\n#hex.ignoreSourceFiles=false\n#hex.aggregateModules=true\n\n#ant.resolveDependencies=false\n#ant.pathIdIncludes=.*\n#ant.external.parameters=\n\n#r.resolveDependencies=false\n#r.runPreStep=true\n#r.ignoreSourceFiles=false\n#r.cranMirrorUrl=\n#r.packageManager=None\n\n#cargo.resolveDependencies=false\n#cargo.runPreStep=true\n#cargo.ignoreSourceFiles=false\n\n#haskell.resolveDependencies=false\n#haskell.runPreStep=true\n#haskell.ignoreSourceFiles=false\n#haskell.ignorePreStepErrors=true\n\n#ocaml.resolveDependencies=false\n#ocaml.runPrepStep=true\n#ocaml.ignoreSourceFiles=false\n#ocaml.switchName=\n#ocaml.ignoredScopes=none\n#ocaml.aggregateModules=true\n\n#bazel.resolveDependencies=false\n#bazel.runPrepStep=true\n\n###########################################################################################\n# Includes/Excludes Glob patterns - Please use only one exclude line and one include line #\n###########################################################################################\nincludes=**/*.cc **/*.zip **/*.cpp **/*.c **/*.swf **/*.tgz **/*.h **/*.js **/*.hpp **/*.py **/*.gzip **/*.cs **/*.rb **/*.exe **/*.gz **/*.pl **/*.cxx **/*.c++ **/*.hxx **/*.jar **/*.java **/*.go **/*.mod **/*.sum **/*.rb\n#includes=**/*.m **/*.mm  **/*.js **/*.php\n#includes=**/*.jar\n#includes=**/*.gem **/*.rb\n#includes=**/*.dll **/*.cs **/*.nupkg\n#includes=**/*.tgz **/*.deb **/*.gzip **/*.rpm **/*.tar.bz2\n#includes=**/*.zip **/*.tar.gz **/*.egg **/*.whl **/*.py\n\n#Exclude file extensions or specific directories by adding **/*.<extension> or **/<excluded_dir>/**\nexcludes=**/*sources.jar **/*javadoc.jar\n\ncase.sensitive.glob=false\nfollowSymbolicLinks=true\n\n######################\n# Archive properties #\n######################\n#archiveExtractionDepth=2\n#archiveIncludes=**/*.war **/*.ear\n#archiveExcludes=**/*sources.jar\n\n##############\n# SCAN MODES #\n##############\n\n# Docker images\n################\n#docker.scanImages=true\n#docker.includes=.*.*\n#docker.excludes=\n#docker.pull.enable=true\n#docker.pull.images=.*.*\n#docker.pull.maxImages=10\n#docker.pull.tags=.*.*\n#docker.pull.digest=\n#docker.delete.force=true\n#docker.login.sudo=false\n#docker.projectNameFormat={repositoryNameAndTag|repositoryName|default}\n#docker.scanTarFiles=true\n\n#docker.aws.enable=true\n#docker.aws.registryIds=\n\n#docker.azure.enable=true\n#docker.azure.userName=\n#docker.azure.userPassword=\n#docker.azure.registryNames=\n#docker.azure.authenticationType=containerRegistry\n#docker.azure.registryAuthenticationParameters=<registry1UserName>:<registry1Password> <registry2UserName>:<registry2Password>\n\n#docker.gcr.enable=true\n#docker.gcr.account=\n#docker.gcr.repositories=\n\n#docker.artifactory.enable=true\n#docker.artifactory.url=\n#docker.artifactory.pullUrl=\n#docker.artifactory.userName=\n#docker.artifactory.userPassword=\n#docker.artifactory.repositoriesNames=\n#docker.artifactory.dockerAccessMethod=\n\n#docker.hub.enabled=true\n#docker.hub.userName=\n#docker.hub.userPassword=\n#docker.hub.organizationsNames=\n\n# Docker containers\n####################\n#docker.scanContainers=true\n#docker.containerIncludes=.*.*\n#docker.containerExcludes=\n\n# Linux package manager settings\n################################\n#scanPackageManager=true\n\n# Serverless settings\n######################\n#serverless.provider=\n#serverless.scanFunctions=true\n#serverless.includes=\n#serverless.excludes=\n#serverless.region=\n#serverless.maxFunctions=10\n\n# Artifactory settings\n########################\n#artifactory.enableScan=true\n#artifactory.url=\n#artifactory.accessToken=\n#artifactory.repoKeys=\n#artifactory.userName=\n#artifactory.userPassword=\n\n##################\n# Proxy settings #\n##################\n#proxy.host=\n#proxy.port=\n#proxy.user=\n#proxy.pass=\n\n################\n# SCM settings #\n################\n#scm.type=\n#scm.user=\n#scm.pass=\n#scm.ppk=\n#scm.url=\n#scm.branch=\n#scm.tag=\n#scm.npmInstall=\n#scm.npmInstallTimeoutMinutes=\n#scm.repositoriesFile=\n"
        }
      ]
    }
  ]
}