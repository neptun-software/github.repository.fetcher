{
  "metadata": {
    "timestamp": 1736708860085,
    "page": 62,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjcw",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "alibaba/DataX",
      "stars": 16150,
      "defaultBranch": "master",
      "files": [
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 2.03515625,
          "content": "# Created by .ignore support plugin (hsz.mobi)\n.DS_Store\n.AppleDouble\n.LSOverride\nIcon\n._*\n.DocumentRevisions-V100\n.fseventsd\n.Spotlight-V100\n.TemporaryItems\n.Trashes\n.VolumeIcon.icns\n.com.apple.timemachine.donotpresent\n.AppleDB\n.AppleDesktop\nNetwork Trash Folder\nTemporary Items\n.apdisk\n*.class\n*.log\n*.ctxt\n.mtj.tmp/\n*.jar\n*.war\n*.nar\n*.ear\n*.zip\n*.tar.gz\n*.rar\nhs_err_pid*\n.idea/**/workspace.xml\n.idea/**/tasks.xml\n.idea/**/dictionaries\n.idea/**/shelf\n.idea/**/dataSources/\n.idea/**/dataSources.ids\n.idea/**/dataSources.local.xml\n.idea/**/sqlDataSources.xml\n.idea/**/dynamic.xml\n.idea/**/uiDesigner.xml\n.idea/**/dbnavigator.xml\n.idea/**/gradle.xml\n.idea/**/libraries\ncmake-build-debug/\ncmake-build-release/\n.idea/**/mongoSettings.xml\n*.iws\nout/\n.idea_modules/\natlassian-ide-plugin.xml\n.idea/replstate.xml\ncom_crashlytics_export_strings.xml\ncrashlytics.properties\ncrashlytics-build.properties\nfabric.properties\n.idea/httpRequests\ntarget/\npom.xml.tag\npom.xml.releaseBackup\npom.xml.versionsBackup\npom.xml.next\nrelease.properties\ndependency-reduced-pom.xml\nbuildNumber.properties\n.mvn/timing.properties\n!/.mvn/wrapper/maven-wrapper.jar\n.idea\n*.iml\nout\ngen### Python template\n__pycache__/\n*.py[cod]\n*$py.class\n*.so\n.Python\nbuild/\ndevelop-eggs/\ndist/\ndownloads/\neggs/\n.eggs/\nlib/\nlib64/\nparts/\nsdist/\nvar/\nwheels/\n*.egg-info/\n.installed.cfg\n*.egg\nMANIFEST\n*.manifest\n*.spec\npip-log.txt\npip-delete-this-directory.txt\nhtmlcov/\n.tox/\n.coverage\n.coverage.*\n.cache\nnosetests.xml\ncoverage.xml\n*.cover\n.hypothesis/\n.pytest_cache/\n*.mo\n*.pot\n*.log\nlocal_settings.py\ndb.sqlite3\ninstance/\n.webassets-cache\n.scrapy\ndocs/_build/\ntarget/\n.ipynb_checkpoints\n.python-version\ncelerybeat-schedule\n*.sage.py\n.env\n.venv\nenv/\nvenv/\nENV/\nenv.bak/\nvenv.bak/\n.spyderproject\n.spyproject\n.ropeproject\n/site\n.mypy_cache/\n.metadata\nbin/\ntmp/\n*.tmp\n*.bak\n*.swp\n*~.nib\nlocal.properties\n.settings/\n.loadpath\n.recommenders\n.externalToolBuilders/\n*.launch\n*.pydevproject\n.cproject\n.autotools\n.factorypath\n.buildpath\n.target\n.tern-project\n.texlipse\n.springBeans\n.recommenders/\n.cache-main\n.scala_dependencies\n.worksheet\n"
        },
        {
          "name": "NOTICE",
          "type": "blob",
          "size": 2.7734375,
          "content": "========================================================\nDataX 是阿里云 DataWorks数据集成 的开源版本，在阿里巴巴集团内被广泛使用的离线数据同步工具/平台。DataX 实现了包括 MySQL、Oracle、OceanBase、SqlServer、Postgre、HDFS、Hive、ADS、HBase、TableStore(OTS)、MaxCompute(ODPS)、Hologres、DRDS 等各种异构数据源之间高效的数据同步功能。\n\nDataX is an open source offline data synchronization tool / platform widely used in Alibaba group and other companies. DataX implements efficient data synchronization between heterogeneous data sources including mysql, Oracle, oceanbase, sqlserver, postgre, HDFS, hive, ads, HBase, tablestore (OTS), maxcompute (ODPs), hologres, DRDS, etc.\n\nCopyright 1999-2022 Alibaba Group Holding Ltd.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n     http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n\n===================================================================\n文级别引用，按许可证\nThis product contains various third-party components under other open source licenses.\nThis section summarizes those components and their licenses. \nGNU Lesser General Public License\n--------------------------------------\nopentsdbreader/src/main/java/com/alibaba/datax/plugin/reader/conn/CliQuery.java\nopentsdbreader/src/main/java/com/alibaba/datax/plugin/reader/conn/Connection4TSDB.java\nopentsdbreader/src/main/java/com/alibaba/datax/plugin/reader/conn/DataPoint4TSDB.java\nopentsdbreader/src/main/java/com/alibaba/datax/plugin/reader/conn/DumpSeries.java\nopentsdbreader/src/main/java/com/alibaba/datax/plugin/reader/conn/OpenTSDBConnection.java\nopentsdbreader/src/main/java/com/alibaba/datax/plugin/reader/conn/OpenTSDBDump.java\nopentsdbreader/src/main/java/com/alibaba/datax/plugin/reader/opentsdbreader/Constant.java\nopentsdbreader/src/main/java/com/alibaba/datax/plugin/reader/opentsdbreader/Key.java\nopentsdbreader/src/main/java/com/alibaba/datax/plugin/reader/opentsdbreader/OpenTSDBReader.java\nopentsdbreader/src/main/java/com/alibaba/datax/plugin/reader/opentsdbreader/OpenTSDBReaderErrorCode.java\nopentsdbreader/src/main/java/com/alibaba/datax/plugin/reader/util/HttpUtils.java\nopentsdbreader/src/main/java/com/alibaba/datax/plugin/reader/util/TSDBUtils.java\nopentsdbreader/src/main/java/com/alibaba/datax/plugin/reader/util/TimeUtils.java\n===================================================================\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 21.5947265625,
          "content": "![Datax-logo](https://github.com/alibaba/DataX/blob/master/images/DataX-logo.jpg)\n\n# DataX\n\n[![Leaderboard](https://img.shields.io/badge/DataX-%E6%9F%A5%E7%9C%8B%E8%B4%A1%E7%8C%AE%E6%8E%92%E8%A1%8C%E6%A6%9C-orange)](https://opensource.alibaba.com/contribution_leaderboard/details?projectValue=datax)\n\nDataX 是阿里云 [DataWorks数据集成](https://www.aliyun.com/product/bigdata/ide) 的开源版本，在阿里巴巴集团内被广泛使用的离线数据同步工具/平台。DataX 实现了包括 MySQL、Oracle、OceanBase、SqlServer、Postgre、HDFS、Hive、ADS、HBase、TableStore(OTS)、MaxCompute(ODPS)、Hologres、DRDS, databend 等各种异构数据源之间高效的数据同步功能。\n\n# DataX 商业版本\n阿里云DataWorks数据集成是DataX团队在阿里云上的商业化产品，致力于提供复杂网络环境下、丰富的异构数据源之间高速稳定的数据移动能力，以及繁杂业务背景下的数据同步解决方案。目前已经支持云上近3000家客户，单日同步数据超过3万亿条。DataWorks数据集成目前支持离线50+种数据源，可以进行整库迁移、批量上云、增量同步、分库分表等各类同步解决方案。2020年更新实时同步能力，支持10+种数据源的读写任意组合。提供MySQL，Oracle等多种数据源到阿里云MaxCompute，Hologres等大数据引擎的一键全增量同步解决方案。\n\n商业版本参见：  https://www.aliyun.com/product/bigdata/ide\n\n\n# Features\n\nDataX本身作为数据同步框架，将不同数据源的同步抽象为从源头数据源读取数据的Reader插件，以及向目标端写入数据的Writer插件，理论上DataX框架可以支持任意数据源类型的数据同步工作。同时DataX插件体系作为一套生态系统, 每接入一套新数据源该新加入的数据源即可实现和现有的数据源互通。\n\n\n\n# DataX详细介绍\n\n##### 请参考：[DataX-Introduction](https://github.com/alibaba/DataX/blob/master/introduction.md)\n\n\n\n# Quick Start\n\n##### Download [DataX下载地址](https://datax-opensource.oss-cn-hangzhou.aliyuncs.com/202308/datax.tar.gz)\n\n\n##### 请点击：[Quick Start](https://github.com/alibaba/DataX/blob/master/userGuid.md)\n\n\n\n# Support Data Channels \n\nDataX目前已经有了比较全面的插件体系，主流的RDBMS数据库、NOSQL、大数据计算系统都已经接入，目前支持数据如下图，详情请点击：[DataX数据源参考指南](https://github.com/alibaba/DataX/wiki/DataX-all-data-channels)\n\n| 类型               | 数据源                          | Reader(读) | Writer(写) |                                                                                                                       文档                                                                                                                       |\n|--------------|---------------------------|:---------:|:---------:|:----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------:|\n| RDBMS 关系型数据库 | MySQL                           |     √      |     √      |                                       [读](https://github.com/alibaba/DataX/blob/master/mysqlreader/doc/mysqlreader.md) 、[写](https://github.com/alibaba/DataX/blob/master/mysqlwriter/doc/mysqlwriter.md)                                       |\n|                    | Oracle                          |     √      |     √      |                                     [读](https://github.com/alibaba/DataX/blob/master/oraclereader/doc/oraclereader.md) 、[写](https://github.com/alibaba/DataX/blob/master/oraclewriter/doc/oraclewriter.md)                                     |\n|                    | OceanBase                       |     √      |     √      | [读](https://open.oceanbase.com/docs/community/oceanbase-database/V3.1.0/use-datax-to-full-migration-data-to-oceanbase) 、[写](https://open.oceanbase.com/docs/community/oceanbase-database/V3.1.0/use-datax-to-full-migration-data-to-oceanbase) |\n|                    | SQLServer                       |     √      |     √      |                               [读](https://github.com/alibaba/DataX/blob/master/sqlserverreader/doc/sqlserverreader.md) 、[写](https://github.com/alibaba/DataX/blob/master/sqlserverwriter/doc/sqlserverwriter.md)                               |\n|                    | PostgreSQL                      |     √      |     √      |                             [读](https://github.com/alibaba/DataX/blob/master/postgresqlreader/doc/postgresqlreader.md) 、[写](https://github.com/alibaba/DataX/blob/master/postgresqlwriter/doc/postgresqlwriter.md)                             |\n|                    | DRDS                            |     √      |     √      |                                         [读](https://github.com/alibaba/DataX/blob/master/drdsreader/doc/drdsreader.md) 、[写](https://github.com/alibaba/DataX/blob/master/drdswriter/doc/drdswriter.md)                                         |\n|                    | Kingbase                        |     √      |     √      |                                         [读](https://github.com/alibaba/DataX/blob/master/drdsreader/doc/drdsreader.md) 、[写](https://github.com/alibaba/DataX/blob/master/drdswriter/doc/drdswriter.md)                                         |\n|                    | 通用RDBMS(支持所有关系型数据库) |     √      |     √      |                                       [读](https://github.com/alibaba/DataX/blob/master/rdbmsreader/doc/rdbmsreader.md) 、[写](https://github.com/alibaba/DataX/blob/master/rdbmswriter/doc/rdbmswriter.md)                                       |\n| 阿里云数仓数据存储 | ODPS                            |     √      |     √      |                                         [读](https://github.com/alibaba/DataX/blob/master/odpsreader/doc/odpsreader.md) 、[写](https://github.com/alibaba/DataX/blob/master/odpswriter/doc/odpswriter.md)                                         |\n|                    | ADB                             |            |     √      |                                                                             [写](https://github.com/alibaba/DataX/blob/master/adbmysqlwriter/doc/adbmysqlwriter.md)                                                                             |\n|                    | ADS                             |            |     √      |                                                                                  [写](https://github.com/alibaba/DataX/blob/master/adswriter/doc/adswriter.md)                                                                                  |\n|                    | OSS                             |     √      |     √      |                                           [读](https://github.com/alibaba/DataX/blob/master/ossreader/doc/ossreader.md) 、[写](https://github.com/alibaba/DataX/blob/master/osswriter/doc/osswriter.md)                                           |\n|                    | OCS                             |            |     √      |                                                                                  [写](https://github.com/alibaba/DataX/blob/master/ocswriter/doc/ocswriter.md)                                                                                  |\n|                    | Hologres                        |            |     √      |                                                                         [写](https://github.com/alibaba/DataX/blob/master/hologresjdbcwriter/doc/hologresjdbcwriter.md)                                                                         |\n|                    | AnalyticDB For PostgreSQL       |            |     √      |                                                                                                                       写                                                                                                                        |\n| 阿里云中间件       | datahub                         |     √      |     √      |                                                                                                                      读 、写                                                                                                                      |\n|                    | SLS                             |     √      |     √      |                                                                                                                      读 、写                                                                                                                      |\n| 图数据库           | 阿里云 GDB                      |     √      |     √      |                                           [读](https://github.com/alibaba/DataX/blob/master/gdbreader/doc/gdbreader.md) 、[写](https://github.com/alibaba/DataX/blob/master/gdbwriter/doc/gdbwriter.md)                                           |\n|                    | Neo4j                           |            |     √      |                                                                                [写](https://github.com/alibaba/DataX/blob/master/neo4jwriter/doc/neo4jwriter.md)                                                                                |\n| NoSQL数据存储      | OTS                             |     √      |     √      |                                           [读](https://github.com/alibaba/DataX/blob/master/otsreader/doc/otsreader.md) 、[写](https://github.com/alibaba/DataX/blob/master/otswriter/doc/otswriter.md)                                           |\n|                    | Hbase0.94                       |     √      |     √      |                               [读](https://github.com/alibaba/DataX/blob/master/hbase094xreader/doc/hbase094xreader.md) 、[写](https://github.com/alibaba/DataX/blob/master/hbase094xwriter/doc/hbase094xwriter.md)                               |\n|                    | Hbase1.1                        |     √      |     √      |                                 [读](https://github.com/alibaba/DataX/blob/master/hbase11xreader/doc/hbase11xreader.md) 、[写](https://github.com/alibaba/DataX/blob/master/hbase11xwriter/doc/hbase11xwriter.md)                                 |\n|                    | Phoenix4.x                      |     √      |     √      |                           [读](https://github.com/alibaba/DataX/blob/master/hbase11xsqlreader/doc/hbase11xsqlreader.md) 、[写](https://github.com/alibaba/DataX/blob/master/hbase11xsqlwriter/doc/hbase11xsqlwriter.md)                           |\n|                    | Phoenix5.x                      |     √      |     √      |                           [读](https://github.com/alibaba/DataX/blob/master/hbase20xsqlreader/doc/hbase20xsqlreader.md) 、[写](https://github.com/alibaba/DataX/blob/master/hbase20xsqlwriter/doc/hbase20xsqlwriter.md)                           |\n|                    | MongoDB                         |     √      |     √      |                                   [读](https://github.com/alibaba/DataX/blob/master/mongodbreader/doc/mongodbreader.md) 、[写](https://github.com/alibaba/DataX/blob/master/mongodbwriter/doc/mongodbwriter.md)                                   |\n|                    | Cassandra                       |     √      |     √      |                               [读](https://github.com/alibaba/DataX/blob/master/cassandrareader/doc/cassandrareader.md) 、[写](https://github.com/alibaba/DataX/blob/master/cassandrawriter/doc/cassandrawriter.md)                               |\n| 数仓数据存储       | StarRocks                       |     √      |     √      |                                                                          读 、[写](https://github.com/alibaba/DataX/blob/master/starrockswriter/doc/starrockswriter.md)                                                                           |\n|                    | ApacheDoris                     |            |     √      |                                                                                [写](https://github.com/alibaba/DataX/blob/master/doriswriter/doc/doriswriter.md)                                                                                |\n|                    | ClickHouse                      |     √      |     √      |                              [读](https://github.com/alibaba/DataX/blob/master/clickhousereader/doc/clickhousereader.md) 、[写](https://github.com/alibaba/DataX/blob/master/clickhousewriter/doc/clickhousewriter.md)                               |\n|                    | Databend                        |            |     √      |                                                                             [写](https://github.com/alibaba/DataX/blob/master/databendwriter/doc/databendwriter.md)                                                                             |\n|                    | Hive                            |     √      |     √      |                                         [读](https://github.com/alibaba/DataX/blob/master/hdfsreader/doc/hdfsreader.md) 、[写](https://github.com/alibaba/DataX/blob/master/hdfswriter/doc/hdfswriter.md)                                         |\n|                    | kudu                            |            |     √      |                                                                                 [写](https://github.com/alibaba/DataX/blob/master/hdfswriter/doc/hdfswriter.md)                                                                                 |\n|                    | selectdb                        |            |     √      |                                                                             [写](https://github.com/alibaba/DataX/blob/master/selectdbwriter/doc/selectdbwriter.md)                                                                             |\n| 无结构化数据存储   | TxtFile                         |     √      |     √      |                                   [读](https://github.com/alibaba/DataX/blob/master/txtfilereader/doc/txtfilereader.md) 、[写](https://github.com/alibaba/DataX/blob/master/txtfilewriter/doc/txtfilewriter.md)                                   |\n|                    | FTP                             |     √      |     √      |                                           [读](https://github.com/alibaba/DataX/blob/master/ftpreader/doc/ftpreader.md) 、[写](https://github.com/alibaba/DataX/blob/master/ftpwriter/doc/ftpwriter.md)                                           |\n|                    | HDFS                            |     √      |     √      |                                         [读](https://github.com/alibaba/DataX/blob/master/hdfsreader/doc/hdfsreader.md) 、[写](https://github.com/alibaba/DataX/blob/master/hdfswriter/doc/hdfswriter.md)                                         |\n|                    | Elasticsearch                   |            |     √      |                                                                        [写](https://github.com/alibaba/DataX/blob/master/elasticsearchwriter/doc/elasticsearchwriter.md)                                                                        |\n| 时间序列数据库     | OpenTSDB                        |     √      |            |                                                                             [读](https://github.com/alibaba/DataX/blob/master/opentsdbreader/doc/opentsdbreader.md)                                                                             |\n|                    | TSDB                            |     √      |     √      |                                       [读](https://github.com/alibaba/DataX/blob/master/tsdbreader/doc/tsdbreader.md) 、[写](https://github.com/alibaba/DataX/blob/master/tsdbwriter/doc/tsdbhttpwriter.md)                                       |\n|                    | TDengine                        |     √      |     √      |                              [读](https://github.com/alibaba/DataX/blob/master/tdenginereader/doc/tdenginereader-CN.md) 、[写](https://github.com/alibaba/DataX/blob/master/tdenginewriter/doc/tdenginewriter-CN.md)                              |\n\n# 阿里云DataWorks数据集成\n\n目前DataX的已有能力已经全部融和进阿里云的数据集成，并且比DataX更加高效、安全，同时数据集成具备DataX不具备的其它高级特性和功能。可以理解为数据集成是DataX的全面升级的商业化用版本，为企业可以提供稳定、可靠、安全的数据传输服务。与DataX相比，数据集成主要有以下几大突出特点：\n\n支持实时同步：\n\n- 功能简介：https://help.aliyun.com/document_detail/181912.html\n- 支持的数据源：https://help.aliyun.com/document_detail/146778.html\n- 支持数据处理：https://help.aliyun.com/document_detail/146777.html\n\n离线同步数据源种类大幅度扩充：\n\n- 新增比如：DB2、Kafka、Hologres、MetaQ、SAPHANA、达梦等等，持续扩充中\n- 离线同步支持的数据源：https://help.aliyun.com/document_detail/137670.html\n- 具备同步解决方案：\n    - 解决方案系统：https://help.aliyun.com/document_detail/171765.html\n    - 一键全增量：https://help.aliyun.com/document_detail/175676.html\n    - 整库迁移：https://help.aliyun.com/document_detail/137809.html\n    - 批量上云：https://help.aliyun.com/document_detail/146671.html\n    - 更新更多能力请访问：https://help.aliyun.com/document_detail/137663.html\n    -\n\n# 我要开发新的插件\n\n请点击：[DataX插件开发宝典](https://github.com/alibaba/DataX/blob/master/dataxPluginDev.md)\n\n# 重要版本更新说明\n\nDataX 后续计划月度迭代更新，也欢迎感兴趣的同学提交 Pull requests，月度更新内容如下。\n\n- [datax_v202309]（https://github.com/alibaba/DataX/releases/tag/datax_v202309)\n  - 支持Phoenix 同步数据添加 where条件\n  - 支持华为 GuassDB读写插件\n  - 修复ClickReader 插件运行报错 Can't find bundle for base name\n  - 增加 DataX调试模块\n  - 修复 orc空文件报错问题\n  - 优化obwriter性能\n  - txtfilewriter 增加导出为insert语句功能支持\n  - HdfsReader/HdfsWriter 支持parquet读写能力\n  \n- [datax_v202308]（https://github.com/alibaba/DataX/releases/tag/datax_v202308)\n  - OTS 插件更新\n  - databend 插件更新\n  - Oceanbase驱动修复\n\n\n- [datax_v202306]（https://github.com/alibaba/DataX/releases/tag/datax_v202306)\n  - 精简代码\n  - 新增插件（neo4jwriter、clickhousewriter）\n  - 优化插件、修复问题（oceanbase、hdfs、databend、txtfile）\n\n\n- [datax_v202303]（https://github.com/alibaba/DataX/releases/tag/datax_v202303)\n  - 精简代码\n  - 新增插件（adbmysqlwriter、databendwriter、selectdbwriter）\n  - 优化插件、修复问题（sqlserver、hdfs、cassandra、kudu、oss）\n  - fastjson 升级到 fastjson2\n\n- [datax_v202210]（https://github.com/alibaba/DataX/releases/tag/datax_v202210)\n  - 涉及通道能力更新（OceanBase、Tdengine、Doris等）\n\n- [datax_v202209]（https://github.com/alibaba/DataX/releases/tag/datax_v202209)\n    - 涉及通道能力更新（MaxCompute、Datahub、SLS等）、安全漏洞更新、通用打包更新等\n\n- [datax_v202205]（https://github.com/alibaba/DataX/releases/tag/datax_v202205)\n    - 涉及通道能力更新（MaxCompute、Hologres、OSS、Tdengine等）、安全漏洞更新、通用打包更新等\n\n\n# 项目成员\n\n核心Contributions: 言柏 、枕水、秋奇、青砾、一斅、云时\n\n感谢天烬、光戈、祁然、巴真、静行对DataX做出的贡献。\n\n# License\n\nThis software is free to use under the Apache License [Apache license](https://github.com/alibaba/DataX/blob/master/license.txt).\n\n# \n请及时提出issue给我们。请前往：[DataxIssue](https://github.com/alibaba/DataX/issues)\n\n# 开源版DataX企业用户\n\n![Datax-logo](https://github.com/alibaba/DataX/blob/master/images/datax-enterprise-users.jpg)\n\n```\n长期招聘 联系邮箱：datax@alibabacloud.com\n【JAVA开发职位】\n职位名称：JAVA资深开发工程师/专家/高级专家\n工作年限 : 2年以上\n学历要求 : 本科（如果能力靠谱，这些都不是条件）\n期望层级 : P6/P7/P8\n\n岗位描述：\n    1. 负责阿里云大数据平台（数加）的开发设计。 \n    2. 负责面向政企客户的大数据相关产品开发；\n    3. 利用大规模机器学习算法挖掘数据之间的联系，探索数据挖掘技术在实际场景中的产品应用 ；\n    4. 一站式大数据开发平台\n    5. 大数据任务调度引擎\n    6. 任务执行引擎\n    7. 任务监控告警\n    8. 海量异构数据同步\n\n岗位要求：\n    1. 拥有3年以上JAVA Web开发经验；\n    2. 熟悉Java的基础技术体系。包括JVM、类装载、线程、并发、IO资源管理、网络；\n    3. 熟练使用常用Java技术框架、对新技术框架有敏锐感知能力；深刻理解面向对象、设计原则、封装抽象；\n    4. 熟悉HTML/HTML5和JavaScript；熟悉SQL语言；\n    5. 执行力强，具有优秀的团队合作精神、敬业精神；\n    6. 深刻理解设计模式及应用场景者加分；\n    7. 具有较强的问题分析和处理能力、比较强的动手能力，对技术有强烈追求者优先考虑；\n    8. 对高并发、高稳定可用性、高性能、大数据处理有过实际项目及产品经验者优先考虑；\n    9. 有大数据产品、云产品、中间件技术解决方案者优先考虑。\n````\n\n用户咨询支持：\n\n钉钉群目前暂时受到了一些管控策略影响，建议大家有问题优先在这里提交问题 Issue，DataX研发和社区会定期回答Issue中的问题，知识库丰富后也能帮助到后来的使用者。\n\n\n\n"
        },
        {
          "name": "adbmysqlwriter",
          "type": "tree",
          "content": null
        },
        {
          "name": "adbpgwriter",
          "type": "tree",
          "content": null
        },
        {
          "name": "adswriter",
          "type": "tree",
          "content": null
        },
        {
          "name": "cassandrareader",
          "type": "tree",
          "content": null
        },
        {
          "name": "cassandrawriter",
          "type": "tree",
          "content": null
        },
        {
          "name": "clickhousereader",
          "type": "tree",
          "content": null
        },
        {
          "name": "clickhousewriter",
          "type": "tree",
          "content": null
        },
        {
          "name": "common",
          "type": "tree",
          "content": null
        },
        {
          "name": "core",
          "type": "tree",
          "content": null
        },
        {
          "name": "databendwriter",
          "type": "tree",
          "content": null
        },
        {
          "name": "datahubreader",
          "type": "tree",
          "content": null
        },
        {
          "name": "datahubwriter",
          "type": "tree",
          "content": null
        },
        {
          "name": "datax-example",
          "type": "tree",
          "content": null
        },
        {
          "name": "datax-opensource-dingding.png",
          "type": "blob",
          "size": 110.1474609375,
          "content": null
        },
        {
          "name": "dataxPluginDev.md",
          "type": "blob",
          "size": 19.3466796875,
          "content": "# DataX插件开发宝典\n\n本文面向DataX插件开发人员，尝试尽可能全面地阐述开发一个DataX插件所经过的历程，力求消除开发者的困惑，让插件开发变得简单。\n\n## 一、开发之前\n\n>  路走对了，就不怕远。✓\n>  路走远了，就不管对不对。✕\n\n当你打开这篇文档，想必已经不用在此解释什么是`DataX`了。那下一个问题便是：\n\n###  `DataX`为什么要使用插件机制？\n\n从设计之初，`DataX`就把异构数据源同步作为自身的使命，为了应对不同数据源的差异、同时提供一致的同步原语和扩展能力，`DataX`自然而然地采用了`框架` + `插件` 的模式：\n\n- 插件只需关心数据的读取或者写入本身。\n- 而同步的共性问题，比如：类型转换、性能、统计，则交由框架来处理。\n\n作为插件开发人员，则需要关注两个问题：\n\n1. 数据源本身的读写数据正确性。\n2. 如何与框架沟通、合理正确地使用框架。\n\n###  开工前需要想明白的问题\n\n就插件本身而言，希望在您动手coding之前，能够回答我们列举的这些问题，不然路走远了发现没走对，就尴尬了。\n\n## 二、插件视角看框架\n\n### 逻辑执行模型\n\n插件开发者不用关心太多，基本只需要关注特定系统读和写，以及自己的代码在逻辑上是怎样被执行的，哪一个方法是在什么时候被调用的。在此之前，需要明确以下概念：\n\n- `Job`: `Job`是DataX用以描述从一个源头到一个目的端的同步作业，是DataX数据同步的最小业务单元。比如：从一张mysql的表同步到odps的一个表的特定分区。\n- `Task`: `Task`是为最大化而把`Job`拆分得到的最小执行单元。比如：读一张有1024个分表的mysql分库分表的`Job`，拆分成1024个读`Task`，用若干个并发执行。\n- `TaskGroup`:  描述的是一组`Task`集合。在同一个`TaskGroupContainer`执行下的`Task`集合称之为`TaskGroup`\n- `JobContainer`:  `Job`执行器，负责`Job`全局拆分、调度、前置语句和后置语句等工作的工作单元。类似Yarn中的JobTracker\n- `TaskGroupContainer`: `TaskGroup`执行器，负责执行一组`Task`的工作单元，类似Yarn中的TaskTracker。\n\n简而言之， **`Job`拆分成`Task`，在分别在框架提供的容器中执行，插件只需要实现`Job`和`Task`两部分逻辑**。\n\n### 物理执行模型\n\n框架为插件提供物理上的执行能力（线程）。`DataX`框架有三种运行模式：\n\n- `Standalone`: 单进程运行，没有外部依赖。\n- `Local`: 单进程运行，统计信息、错误信息汇报到集中存储。\n- `Distrubuted`: 分布式多进程运行，依赖`DataX Service`服务。\n\n当然，上述三种模式对插件的编写而言没有什么区别，你只需要避开一些小错误，插件就能够在单机/分布式之间无缝切换了。\n当`JobContainer`和`TaskGroupContainer`运行在同一个进程内时，就是单机模式（`Standalone`和`Local`）；当它们分布在不同的进程中执行时，就是分布式（`Distributed`）模式。\n\n是不是很简单？\n\n### 编程接口\n\n那么，`Job`和`Task`的逻辑应是怎么对应到具体的代码中的？\n\n首先，插件的入口类必须扩展`Reader`或`Writer`抽象类，并且实现分别实现`Job`和`Task`两个内部抽象类，`Job`和`Task`的实现必须是 **内部类** 的形式，原因见 **加载原理** 一节。以Reader为例：\n\n```java\npublic class SomeReader extends Reader {\n    public static class Job extends Reader.Job {\n\n        @Override\n        public void init() {\n        }\n\t\t\n\t\t@Override\n\t\tpublic void prepare() {\n        }\n\n        @Override\n        public List<Configuration> split(int adviceNumber) {\n            return null;\n        }\n\n        @Override\n        public void post() {\n        }\n\n        @Override\n        public void destroy() {\n        }\n\n    }\n\n    public static class Task extends Reader.Task {\n\n        @Override\n        public void init() {\n        }\n\t\t\n\t\t@Override\n\t\tpublic void prepare() {\n        }\n\n        @Override\n        public void startRead(RecordSender recordSender) {\n        }\n\n        @Override\n        public void post() {\n        }\n\n        @Override\n        public void destroy() {\n        }\n    }\n}\n```\n\n`Job`接口功能如下：\n- `init`: Job对象初始化工作，此时可以通过`super.getPluginJobConf()`获取与本插件相关的配置。读插件获得配置中`reader`部分，写插件获得`writer`部分。\n- `prepare`: 全局准备工作，比如odpswriter清空目标表。\n- `split`: 拆分`Task`。参数`adviceNumber`框架建议的拆分数，一般是运行时所配置的并发度。值返回的是`Task`的配置列表。\n- `post`: 全局的后置工作，比如mysqlwriter同步完影子表后的rename操作。\n- `destroy`: Job对象自身的销毁工作。\n\n`Task`接口功能如下：\n- `init`：Task对象的初始化。此时可以通过`super.getPluginJobConf()`获取与本`Task`相关的配置。这里的配置是`Job`的`split`方法返回的配置列表中的其中一个。\n- `prepare`：局部的准备工作。\n- `startRead`: 从数据源读数据，写入到`RecordSender`中。`RecordSender`会把数据写入连接Reader和Writer的缓存队列。\n- `startWrite`：从`RecordReceiver`中读取数据，写入目标数据源。`RecordReceiver`中的数据来自Reader和Writer之间的缓存队列。\n- `post`: 局部的后置工作。\n- `destroy`: Task象自身的销毁工作。\n\n需要注意的是：\n- `Job`和`Task`之间一定不能有共享变量，因为分布式运行时不能保证共享变量会被正确初始化。两者之间只能通过配置文件进行依赖。\n- `prepare`和`post`在`Job`和`Task`中都存在，插件需要根据实际情况确定在什么地方执行操作。\n\n框架按照如下的顺序执行`Job`和`Task`的接口：\n\n![DataXReaderWriter (2)](https://github.com/alibaba/DataX/blob/master/images/plugin_dev_guide_1.png)\n\n上图中，黄色表示`Job`部分的执行阶段，蓝色表示`Task`部分的执行阶段，绿色表示框架执行阶段。\n\n相关类关系如下：\n\n![DataX](https://github.com/alibaba/DataX/blob/master/images/plugin_dev_guide_2.png)\n\n### 插件定义\n\n代码写好了，有没有想过框架是怎么找到插件的入口类的？框架是如何加载插件的呢？\n\n在每个插件的项目中，都有一个`plugin.json`文件，这个文件定义了插件的相关信息，包括入口类。例如：\n\n```json\n{\n    \"name\": \"mysqlwriter\",\n    \"class\": \"com.alibaba.datax.plugin.writer.mysqlwriter.MysqlWriter\",\n    \"description\": \"Use Jdbc connect to database, execute insert sql.\",\n    \"developer\": \"alibaba\"\n}\n```\n\n- `name`: 插件名称，大小写敏感。框架根据用户在配置文件中指定的名称来搜寻插件。 **十分重要** 。\n- `class`: 入口类的全限定名称，框架通过反射插件入口类的实例。**十分重要** 。\n- `description`: 描述信息。\n- `developer`: 开发人员。\n\n### 打包发布\n\n`DataX`使用`assembly`打包，`assembly`的使用方法请咨询谷哥或者度娘。打包命令如下：\n\n```bash\nmvn clean package -DskipTests assembly:assembly\n```\n\n`DataX`插件需要遵循统一的目录结构：\n\n```\n${DATAX_HOME}\n|-- bin       \n|   `-- datax.py\n|-- conf\n|   |-- core.json\n|   `-- logback.xml\n|-- lib\n|   `-- datax-core-dependencies.jar\n`-- plugin\n    |-- reader\n    |   `-- mysqlreader\n    |       |-- libs\n    |       |   `-- mysql-reader-plugin-dependencies.jar\n    |       |-- mysqlreader-0.0.1-SNAPSHOT.jar\n    |       `-- plugin.json\n    `-- writer\n        |-- mysqlwriter\n        |   |-- libs\n        |   |   `-- mysql-writer-plugin-dependencies.jar\n        |   |-- mysqlwriter-0.0.1-SNAPSHOT.jar\n        |   `-- plugin.json\n        |-- oceanbasewriter\n        `-- odpswriter\n```\n\n- `${DATAX_HOME}/bin`:  可执行程序目录。\n- `${DATAX_HOME}/conf`:  框架配置目录。\n- `${DATAX_HOME}/lib`:  框架依赖库目录。\n- `${DATAX_HOME}/plugin`:  插件目录。\n\n插件目录分为`reader`和`writer`子目录，读写插件分别存放。插件目录规范如下：\n\n- `${PLUGIN_HOME}/libs`: 插件的依赖库。\n- `${PLUGIN_HOME}/plugin-name-version.jar`: 插件本身的jar。\n- `${PLUGIN_HOME}/plugin.json`: 插件描述文件。\n\n尽管框架加载插件时，会把`${PLUGIN_HOME}`下所有的jar放到`classpath`，但还是推荐依赖库的jar和插件本身的jar分开存放。\n\n注意：\n**插件的目录名字必须和`plugin.json`中定义的插件名称一致。**\n\n### 配置文件\n\n`DataX`使用`json`作为配置文件的格式。一个典型的`DataX`任务配置如下：\n\n```json\n{\n  \"job\": {\n    \"content\": [\n      {\n        \"reader\": {\n          \"name\": \"odpsreader\",\n          \"parameter\": {\n            \"accessKey\": \"\",\n            \"accessId\": \"\",\n            \"column\": [\"\"],\n            \"isCompress\": \"\",\n            \"odpsServer\": \"\",\n            \"partition\": [\n              \"\"\n            ],\n            \"project\": \"\",\n            \"table\": \"\",\n            \"tunnelServer\": \"\"\n          }\n        },\n        \"writer\": {\n          \"name\": \"oraclewriter\",\n          \"parameter\": {\n            \"username\": \"\",\n            \"password\": \"\",\n            \"column\": [\"*\"],\n            \"connection\": [\n              {\n                \"jdbcUrl\": \"\",\n                \"table\": [\n                  \"\"\n                ]\n              }\n            ]\n          }\n        }\n      }\n    ]\n  }\n}\n```\n\n`DataX`框架有`core.json`配置文件，指定了框架的默认行为。任务的配置里头可以指定框架中已经存在的配置项，而且具有更高的优先级，会覆盖`core.json`中的默认值。\n\n**配置中`job.content.reader.parameter`的value部分会传给`Reader.Job`；`job.content.writer.parameter`的value部分会传给`Writer.Job`** ，`Reader.Job`和`Writer.Job`可以通过`super.getPluginJobConf()`来获取。\n\n`DataX`框架支持对特定的配置项进行RSA加密，例子中以`*`开头的项目便是加密后的值。 **配置项加密解密过程对插件是透明，插件仍然以不带`*`的key来查询配置和操作配置项** 。\n\n#### 如何设计配置参数\n\n> 配置文件的设计是插件开发的第一步！\n\n任务配置中`reader`和`writer`下`parameter`部分是插件的配置参数，插件的配置参数应当遵循以下原则：\n\n- 驼峰命名：所有配置项采用驼峰命名法，首字母小写，单词首字母大写。\n- 正交原则：配置项必须正交，功能没有重复，没有潜规则。\n- 富类型：合理使用json的类型，减少无谓的处理逻辑，减少出错的可能。\n    - 使用正确的数据类型。比如，bool类型的值使用`true`/`false`，而非`\"yes\"`/`\"true\"`/`0`等。\n    - 合理使用集合类型，比如，用数组替代有分隔符的字符串。\n- 类似通用：遵守同一类型的插件的习惯，比如关系型数据库的`connection`参数都是如下结构：\n\n    ```json\n    {\n      \"connection\": [\n        {\n          \"table\": [\n            \"table_1\",\n            \"table_2\"\n          ],\n          \"jdbcUrl\": [\n            \"jdbc:mysql://127.0.0.1:3306/database_1\",\n            \"jdbc:mysql://127.0.0.2:3306/database_1_slave\"\n          ]\n        },\n        {\n          \"table\": [\n            \"table_3\",\n            \"table_4\"\n          ],\n          \"jdbcUrl\": [\n            \"jdbc:mysql://127.0.0.3:3306/database_2\",\n            \"jdbc:mysql://127.0.0.4:3306/database_2_slave\"\n          ]\n        }\n      ]\n    }\n    ``` \n- ...\n\n#### 如何使用`Configuration`类\n\n为了简化对json的操作，`DataX`提供了简单的DSL配合`Configuration`类使用。\n\n`Configuration`提供了常见的`get`, `带类型get`，`带默认值get`，`set`等读写配置项的操作，以及`clone`, `toJSON`等方法。配置项读写操作都需要传入一个`path`做为参数，这个`path`就是`DataX`定义的DSL。语法有两条：\n\n1. 子map用`.key`表示，`path`的第一个点省略。\n2. 数组元素用`[index]`表示。\n\n比如操作如下json：\n\n```json\n{\n  \"a\": {\n    \"b\": {\n      \"c\": 2\n    },\n    \"f\": [\n      1,\n      2,\n      {\n        \"g\": true,\n        \"h\": false\n      },\n      4\n    ]\n  },\n  \"x\": 4\n}\n```\n\n比如调用`configuration.get(path)`方法，当path为如下值的时候得到的结果为：\n\n- `x`：`4`\n- `a.b.c`：`2`\n- `a.b.c.d`：`null`\n- `a.b.f[0]`：`1`\n- `a.b.f[2].g`：`true`\n\n注意，因为插件看到的配置只是整个配置的一部分。使用`Configuration`对象时，需要注意当前的根路径是什么。\n\n更多`Configuration`的操作请参考`ConfigurationTest.java`。\n\n### 插件数据传输\n\n跟一般的`生产者-消费者`模式一样，`Reader`插件和`Writer`插件之间也是通过`channel`来实现数据的传输的。`channel`可以是内存的，也可能是持久化的，插件不必关心。插件通过`RecordSender`往`channel`写入数据，通过`RecordReceiver`从`channel`读取数据。\n\n\n`channel`中的一条数据为一个`Record`的对象，`Record`中可以放多个`Column`对象，这可以简单理解为数据库中的记录和列。\n\n`Record`有如下方法：\n\n```java\npublic interface Record {\n    // 加入一个列，放在最后的位置\n    void addColumn(Column column);\n    // 在指定下标处放置一个列\n    void setColumn(int i, final Column column);\n    // 获取一个列\n    Column getColumn(int i);\n    // 转换为json String\n    String toString();\n    // 获取总列数\n    int getColumnNumber();\n    // 计算整条记录在内存中占用的字节数\n    int getByteSize();\n}\n```\n\n因为`Record`是一个接口，`Reader`插件首先调用`RecordSender.createRecord()`创建一个`Record`实例，然后把`Column`一个个添加到`Record`中。\n\n`Writer`插件调用`RecordReceiver.getFromReader()`方法获取`Record`，然后把`Column`遍历出来，写入目标存储中。当`Reader`尚未退出，传输还在进行时，如果暂时没有数据`RecordReceiver.getFromReader()`方法会阻塞直到有数据。如果传输已经结束，会返回`null`，`Writer`插件可以据此判断是否结束`startWrite`方法。\n\n`Column`的构造和操作，我们在《类型转换》一节介绍。\n\n### 类型转换\n\n为了规范源端和目的端类型转换操作，保证数据不失真，DataX支持六种内部数据类型：\n\n- `Long`：定点数(Int、Short、Long、BigInteger等)。\n- `Double`：浮点数(Float、Double、BigDecimal(无限精度)等)。\n- `String`：字符串类型，底层不限长，使用通用字符集(Unicode)。\n- `Date`：日期类型。\n- `Bool`：布尔值。\n- `Bytes`：二进制，可以存放诸如MP3等非结构化数据。\n\n\n对应地，有`DateColumn`、`LongColumn`、`DoubleColumn`、`BytesColumn`、`StringColumn`和`BoolColumn`六种`Column`的实现。\n\n\n`Column`除了提供数据相关的方法外，还提供一系列以`as`开头的数据类型转换转换方法。\n\n![Columns](https://github.com/alibaba/DataX/blob/master/images/plugin_dev_guide_3.png)\n\n\nDataX的内部类型在实现上会选用不同的java类型：\n\n| 内部类型 | 实现类型 | 备注 |\n| ----- | -------- | ----- |\n| Date  | java.util.Date |     |\n| Long  | java.math.BigInteger|  使用无限精度的大整数，保证不失真   |\n| Double| java.lang.String| 用String表示，保证不失真 |\n| Bytes | byte[]|  |\n| String|  java.lang.String   |     |\n| Bool  | java.lang.Boolean   | |\n\n类型之间相互转换的关系如下：\n\n| from\\to     |   Date  |  Long  | Double | Bytes   | String | Bool   |\n| -----   | -------- | ----- | ------ | -------- | ----- |  ----- |\n| Date    |    -     |  使用毫秒时间戳  |   不支持  |    不支持      |   使用系统配置的date/time/datetime格式转换    |  不支持  |\n| Long    |  作为毫秒时间戳构造Date    |   -   | BigInteger转为BigDecimal，然后BigDecimal.doubleValue()       |    不支持      |  BigInteger.toString()    | 0为false，否则true   |\n| Double  |  不支持   | 内部String构造BigDecimal，然后BigDecimal.longValue()   |    -   |     不支持     |  直接返回内部String    |        |\n| Bytes   |  不支持   | 不支持 | 不支持  |    -     |  按照`common.column.encoding`配置的编码转换为String，默认`utf-8`  |  不支持  |\n| String  | 按照配置的date/time/datetime/extra格式解析 |  用String构造BigDecimal，然后取longValue()  |   用String构造BigDecimal，然后取doubleValue(),会正确处理`NaN`/`Infinity`/`-Infinity`  |   按照`common.column.encoding`配置的编码转换为byte[]，默认`utf-8`     |    -  |    \"true\"为`true`, \"false\"为`false`，大小写不敏感。其他字符串不支持    |\n| Bool    |    不支持  |  `true`为`1L`，否则`0L`     |        | `true`为`1.0`，否则`0.0`   |  不支持  |    -   |\n\n\n### 脏数据处理\n\n#### 什么是脏数据？\n\n目前主要有三类脏数据：\n\n1. Reader读到不支持的类型、不合法的值。\n1. 不支持的类型转换，比如：`Bytes`转换为`Date`。\n2. 写入目标端失败，比如：写mysql整型长度超长。\n\n#### 如何处理脏数据\n\n在`Reader.Task`和`Writer.Task`中，通过`AbstractTaskPlugin.getTaskPluginCollector()`可以拿到一个`TaskPluginCollector`，它提供了一系列`collectDirtyRecord`的方法。当脏数据出现时，只需要调用合适的`collectDirtyRecord`方法，把被认为是脏数据的`Record`传入即可。\n\n用户可以在任务的配置中指定脏数据限制条数或者百分比限制，当脏数据超出限制时，框架会结束同步任务，退出。插件需要保证脏数据都被收集到，其他工作交给框架就好。\n\n### 加载原理\n\n\n1. 框架扫描`plugin/reader`和`plugin/writer`目录，加载每个插件的`plugin.json`文件。\n2. 以`plugin.json`文件中`name`为key，索引所有的插件配置。如果发现重名的插件，框架会异常退出。\n3. 用户在插件中在`reader`/`writer`配置的`name`字段指定插件名字。框架根据插件的类型（`reader`/`writer`）和插件名称去插件的路径下扫描所有的jar，加入`classpath`。\n4. 根据插件配置中定义的入口类，框架通过反射实例化对应的`Job`和`Task`对象。\n\n### 编写测试用例\n1. 在datax-example工程下新建新的插件测试模块,调用`ExampleContainer.start(jobPath)`方法来检测你的代码逻辑是否正确。[datax-example使用](https://github.com/alibaba/DataX/blob/master/datax-example/doc/README.md)\n\n\n## 三、Last but not Least\n\n> 文档是工程师的良知。\n\n每个插件都必须在`DataX`官方wiki中有一篇文档，文档需要包括但不限于以下内容：\n\n1. **快速介绍**：介绍插件的使用场景，特点等。\n2. **实现原理**：介绍插件实现的底层原理，比如`mysqlwriter`通过`insert into`和`replace into`来实现插入，`tair`插件通过tair客户端实现写入。\n3. **配置说明**\n\t- 给出典型场景下的同步任务的json配置文件。\n\t- 介绍每个参数的含义、是否必选、默认值、取值范围和其他约束。\n4. **类型转换**\n    - 插件是如何在实际的存储类型和`DataX`的内部类型之间进行转换的。\n    - 以及是否存在特殊处理。\n5. **性能报告**\n\t- 软硬件环境，系统版本，java版本，CPU、内存等。\n\t- 数据特征，记录大小等。\n\t- 测试参数集（多组），系统参数（比如并发数），插件参数（比如batchSize）\n\t- 不同参数下同步速度（Rec/s, MB/s），机器负载（load, cpu）等，对数据源压力（load, cpu, mem等）。\n6. **约束限制**：是否存在其他的使用限制条件。\n7. **FAQ**：用户经常会遇到的问题。\n"
        },
        {
          "name": "dorisreader",
          "type": "tree",
          "content": null
        },
        {
          "name": "doriswriter",
          "type": "tree",
          "content": null
        },
        {
          "name": "drdsreader",
          "type": "tree",
          "content": null
        },
        {
          "name": "drdswriter",
          "type": "tree",
          "content": null
        },
        {
          "name": "elasticsearchwriter",
          "type": "tree",
          "content": null
        },
        {
          "name": "ftpreader",
          "type": "tree",
          "content": null
        },
        {
          "name": "ftpwriter",
          "type": "tree",
          "content": null
        },
        {
          "name": "gaussdbreader",
          "type": "tree",
          "content": null
        },
        {
          "name": "gaussdbwriter",
          "type": "tree",
          "content": null
        },
        {
          "name": "gdbreader",
          "type": "tree",
          "content": null
        },
        {
          "name": "gdbwriter",
          "type": "tree",
          "content": null
        },
        {
          "name": "hbase094xreader",
          "type": "tree",
          "content": null
        },
        {
          "name": "hbase094xwriter",
          "type": "tree",
          "content": null
        },
        {
          "name": "hbase11xreader",
          "type": "tree",
          "content": null
        },
        {
          "name": "hbase11xsqlreader",
          "type": "tree",
          "content": null
        },
        {
          "name": "hbase11xsqlwriter",
          "type": "tree",
          "content": null
        },
        {
          "name": "hbase11xwriter",
          "type": "tree",
          "content": null
        },
        {
          "name": "hbase20xsqlreader",
          "type": "tree",
          "content": null
        },
        {
          "name": "hbase20xsqlwriter",
          "type": "tree",
          "content": null
        },
        {
          "name": "hdfsreader",
          "type": "tree",
          "content": null
        },
        {
          "name": "hdfswriter",
          "type": "tree",
          "content": null
        },
        {
          "name": "hologresjdbcwriter",
          "type": "tree",
          "content": null
        },
        {
          "name": "images",
          "type": "tree",
          "content": null
        },
        {
          "name": "introduction.md",
          "type": "blob",
          "size": 12.9482421875,
          "content": "# 阿里云开源离线同步工具DataX3.0介绍\n\n## 一. DataX\b3.0概览\n\n​\tDataX 是一个异构数据源离线同步工具，致力于实现包括关系型数据库(MySQL、Oracle等)、HDFS、Hive、ODPS、HBase、FTP等各种异构数据源之间稳定高效的数据同步功能。\n\n![datax_why_new](https://cloud.githubusercontent.com/assets/1067175/17879841/93b7fc1c-6927-11e6-8cda-7cf8420fc65f.png)\n\n\n\n- #### 设计理念\n\n  为了解决异构数据源同步问题，DataX将复杂的网状的同步链路变成了星型数据链路，DataX作为中间传输载体负责连接各种数据源。当需要接入一个新的数据源的时候，只需要将此数据源对接到DataX，便能跟已有的数据源做到无缝数据同步。\n\n- #### 当前使用现状\n\n  DataX在阿里巴巴集团内被广泛使用，承担了所有大数据的离线同步业务，并已持续稳定运行了6年之久。目前每天完成同步8w多道作业，每日传输数据量超过300TB。\n\n此前已经开源DataX1.0版本，此次介绍为阿里云开源全新版本DataX3.0，有了更多更强大的功能和更好的使用体验。Github主页地址：https://github.com/alibaba/DataX\n\n## 二、DataX3.0框架设计\t\n\n![datax_framework_new](https://cloud.githubusercontent.com/assets/1067175/17879884/ec7e36f4-6927-11e6-8f5f-ffc43d6a468b.png)\n\nDataX本身作为离线数据同步框架，采用Framework + plugin架构构建。将数据源读取和写入抽象成为Reader/Writer插件，纳入到整个同步框架中。\n\n- Reader：Reader\b为数据采集模块，负责采集数据源的数据，将数据发送给Framework。\n- Writer： Writer为数据写入模块，负责不断向Framework取数据，并将数据写入到目的端。\n- Framework：Framework用于连接reader和writer，作为两者的数据传输通道，并处理缓冲，流控，并发，数据转换等核心技术问题。\n\n## 三. DataX3.0插件体系\n\n​\t经过几年积累，DataX目前已经有了比较全面的插件体系，主流的RDBMS数据库、NOSQL、大数据计算系统都已经接入。DataX目前支持数据如下：\n\n| 类型           | 数据源        | Reader(读) | Writer(写) |文档|\n| ------------ | ---------- | :-------: | :-------: |:-------: |\n| RDBMS 关系型数据库 | MySQL      |     √     |     √     |[读](https://github.com/alibaba/DataX/blob/master/mysqlreader/doc/mysqlreader.md) 、[写](https://github.com/alibaba/DataX/blob/master/mysqlwriter/doc/mysqlwriter.md)|\n|              | Oracle     |     √     |     √     |[读](https://github.com/alibaba/DataX/blob/master/oraclereader/doc/oraclereader.md) 、[写](https://github.com/alibaba/DataX/blob/master/oraclewriter/doc/oraclewriter.md)|\n|              | OceanBase  |     √     |     √     |[读](https://open.oceanbase.com/docs/community/oceanbase-database/V3.1.0/use-datax-to-full-migration-data-to-oceanbase) 、[写](https://open.oceanbase.com/docs/community/oceanbase-database/V3.1.0/use-datax-to-full-migration-data-to-oceanbase)|\n|              | SQLServer  |     √     |     √     |[读](https://github.com/alibaba/DataX/blob/master/sqlserverreader/doc/sqlserverreader.md) 、[写](https://github.com/alibaba/DataX/blob/master/sqlserverwriter/doc/sqlserverwriter.md)|\n|              | PostgreSQL |     √     |     √     |[读](https://github.com/alibaba/DataX/blob/master/postgresqlreader/doc/postgresqlreader.md) 、[写](https://github.com/alibaba/DataX/blob/master/postgresqlwriter/doc/postgresqlwriter.md)|\n|              | DRDS |     √     |     √     |[读](https://github.com/alibaba/DataX/blob/master/drdsreader/doc/drdsreader.md) 、[写](https://github.com/alibaba/DataX/blob/master/drdswriter/doc/drdswriter.md)|\n|              | 达梦         |     √     |     √     |[读]() 、[写]()|\n|              | 通用RDBMS(支持所有关系型数据库)         |     √     |     √     |[读]() 、[写]()|\n| 阿里云数仓数据存储    | ODPS       |     √     |     √     |[读](https://github.com/alibaba/DataX/blob/master/odpsreader/doc/odpsreader.md) 、[写](https://github.com/alibaba/DataX/blob/master/odpsswriter/doc/odpswriter.md)|\n|              | ADS        |           |     √     |[写](https://github.com/alibaba/DataX/blob/master/adswriter/doc/adswriter.md)|\n|              | OSS        |     √     |     √     |[读](https://github.com/alibaba/DataX/blob/master/ossreader/doc/ossreader.md) 、[写](https://github.com/alibaba/DataX/blob/master/osswriter/doc/osswriter.md)|\n|              | OCS        |     √     |     √     |[读](https://github.com/alibaba/DataX/blob/master/ocsreader/doc/ocsreader.md) 、[写](https://github.com/alibaba/DataX/blob/master/ocswriter/doc/ocswriter.md)|\n| NoSQL数据存储    | OTS        |     √     |     √     |[读](https://github.com/alibaba/DataX/blob/master/otsreader/doc/otsreader.md) 、[写](https://github.com/alibaba/DataX/blob/master/otswriter/doc/otswriter.md)|\n|              | Hbase0.94  |     √     |     √     |[读](https://github.com/alibaba/DataX/blob/master/hbase094xreader/doc/hbase094xreader.md) 、[写](https://github.com/alibaba/DataX/blob/master/hbase094xwriter/doc/hbase094xwriter.md)|\n|              | Hbase1.1   |     √     |     √     |[读](https://github.com/alibaba/DataX/blob/master/hbase11xreader/doc/hbase11xreader.md) 、[写](https://github.com/alibaba/DataX/blob/master/hbase11xwriter/doc/hbase11xwriter.md)|\n|              | MongoDB    |     √     |     √     |[读](https://github.com/alibaba/DataX/blob/master/mongoreader/doc/mongoreader.md) 、[写](https://github.com/alibaba/DataX/blob/master/mongowriter/doc/mongowriter.md)|\n|              | Hive       |     √     |     √     |[读](https://github.com/alibaba/DataX/blob/master/hdfsreader/doc/hdfsreader.md) 、[写](https://github.com/alibaba/DataX/blob/master/hdfswriter/doc/hdfswriter.md)|\n| 无结构化数据存储     | TxtFile    |     √     |     √     |[读](https://github.com/alibaba/DataX/blob/master/txtfilereader/doc/txtfilereader.md) 、[写](https://github.com/alibaba/DataX/blob/master/txtfilewriter/doc/txtfilewriter.md)|\n|              | FTP        |     √     |     √     |[读](https://github.com/alibaba/DataX/blob/master/ftpreader/doc/ftpreader.md) 、[写](https://github.com/alibaba/DataX/blob/master/ftpwriter/doc/ftpwriter.md)|\n|              | HDFS       |     √     |     √     |[读](https://github.com/alibaba/DataX/blob/master/hdfsreader/doc/hdfsreader.md) 、[写](https://github.com/alibaba/DataX/blob/master/hdfswriter/doc/hdfswriter.md)|\n|              | Elasticsearch       |         |     √     |[写](https://github.com/alibaba/DataX/blob/master/elasticsearchwriter/doc/elasticsearchwriter.md)|\n\n\nDataX Framework提供了简单的接口与插件交互，提供简单的插件接入机制，只需要任意加上一种插件，就能无缝对接其他数据源。详情请看：[DataX数据源指南](https://github.com/alibaba/DataX/wiki/DataX-all-data-channels)\n\n## \b四、DataX3.0核心架构\n\nDataX 3.0 开源版本支持单机多线程模式完成同步作业运行，本小节按一个DataX作业生命周期的时序图，从整体架构设计非常简要说明DataX各个模块相互关系。\n\n![datax_arch](https://cloud.githubusercontent.com/assets/1067175/17850849/aa6c95a8-6891-11e6-94b7-39f0ab5af3b4.png)\n\n#### 核心模块介绍：\n\n1. DataX完成单个数据同步的作业，我们称之为Job，DataX接受到一个Job之后，将启动一个进程来完成整个作业同步过程。DataX Job模块是单个作业的中枢管理节点，承担了数据清理、子任务切分(将单一作业计算转化为多个子Task)、TaskGroup管理等功能。\n2. DataXJob启动后，会根据不同的源端切分策略，将Job切分成多个小的Task(子任务)，以便于并发执行。Task便是DataX作业的最小单元，每一个Task都会负责一部分数据的同步工作。\n3. 切分多个Task之后，DataX Job会调用Scheduler模块，根据配置的并发数据量，将拆分成的Task重新组合，组装成TaskGroup(任务组)。每一个TaskGroup负责以一定的并发运行完毕分配好的所有Task，默认单个任务组的并发数量为5。\n4. 每一个Task都由TaskGroup负责启动，Task启动后，会固定启动Reader—>Channel—>Writer的线程来完成任务同步工作。\n5. DataX作业运行起来之后， Job监控并等待多个TaskGroup模块任务完成，等待所有TaskGroup任务完成后Job成功退出。否则，异常退出，进程退出值非0\n\n#### DataX调度流程：\n\n举例来说，用户提交了一个DataX作业，并且配置了20个并发，目的是将一个100张分表的mysql数据同步到odps里面。\tDataX的调度决策思路是：\n\n1. DataXJob根据分库分表切分成了100个Task。\n2. 根据20个并发，DataX计算共需要分配4个TaskGroup。\n3. 4个TaskGroup平分切分好的100个Task，每一个TaskGroup负责以5个并发共计运行25个Task。\n\n## 五、DataX 3.0六大核心优势\n\n- #### 可靠的数据质量监控\n\n  - 完美解决数据传输个别类型失真问题\n\n     DataX旧版对于部分数据类型(比如时间戳)传输一直存在毫秒阶段等数据失真情况，新版本DataX3.0已经做到支持所有的强数据类型，每一种插件都有自己的数据类型转换策略，让数据可以完整无损的传输到目的端。\n\n  - 提供作业全链路的流量、数据量\b运行时监控\n\n     DataX3.0运行过程中可以将作业本身状态、数据流量、数据速度、执行进度等信息进行全面的展示，让用户可以实时了解作业状态。并可在作业执行过程中智能判断源端和目的端的速度对比情况，给予用户更多性能排查信息。\n\n  - 提供脏数据探测\n\n     在大量数据的传输过程中，必定会由于各种原因导致很多数据传输报错(比如类型转换错误)，这种数据DataX认为就是脏数据。DataX目前可以实现脏数据精确过滤、识别、采集、展示，为用户提供多种的脏数据处理模式，让用户准确把控数据质量大关！\n\n- #### 丰富的数据转换功能\n\n  DataX作为一个服务于大数据的ETL工具，除了提供数据快照搬迁功能之外，还提供了丰富数据转换的功能，让数据在传输过程中可以轻松完成数据脱敏，补全，过滤等数据转换功能，另外还提供了自动groovy函数，让用户自定义转换函数。详情请看DataX3的transformer详细介绍。\n\n- #### 精准的速度控制\n\n  还在为同步过程对在线存储压力影响而担心吗？新版本DataX3.0提供了包括通道(并发)、记录流、字节流三种流控模式，可以随意控制你的作业速度，让你的作业在库可以承受的范围内达到最佳的同步速度。\n\n  ```json\n  \"speed\": {\n     \"channel\": 5,\n     \"byte\": 1048576,\n     \"record\": 10000\n  }\n  ```\n\n- #### 强劲的同步性能\n\n  DataX3.0每一种读插件都有一种或多种切分策略，都能将作业合理切分成多个Task并行执行，单机多线程执行模型可以让DataX速度随并发成线性增长。在源端和目的端性能都足够的情况下，单个作业一定可以打满网卡。另外，DataX团队对所有的已经接入的插件都做了极致的性能优化，并且做了完整的性能测试。性能测试相关详情可以参照每单个数据源的详细介绍：[DataX数据源指南](https://github.com/alibaba/DataX/wiki/DataX-all-data-channels)\n\n- #### 健壮的容错机制\n\n  DataX作业是极易受外部因素的干扰，网络闪断、数据源不稳定等因素很容易让同步到一半的作业报错停止。因此稳定性是DataX的基本要求，在DataX 3.0的设计中，重点完善了框架和插件的稳定性。目前DataX3.0可以做到线程级别、进程级别(暂时未开放)、作业级别多层次局部/全局的重试，保证用户的作业稳定运行。\n\n  - 线程内部重试\n\n     DataX的核心插件都经过团队的全盘review，不同的网络交互方式都有不同的重试策略。\n\n  - 线程级别重试\n\n     目前DataX已经可以实现TaskFailover，针对于中间失败的Task，DataX框架可以做到整个Task级别的重新调度。\n\n- #### 极简的使用体验\n\n  - 易用\n\n     下载即可用，支持linux和windows，只需要短短几步骤就可以完成数据的传输。请点击：[Quick Start](https://github.com/alibaba/DataX/wiki/Quick-Start)\n\n  - 详细\n\n     DataX在运行日志中打印了大量信息，其中包括传输速度，Reader、Writer性能，进程CPU，JVM和GC情况等等。\n\n    - 传输过程中打印传输速度、进度等\t\n\n      ![datax_run_speed](https://cloud.githubusercontent.com/assets/1067175/17850877/d1612c0a-6891-11e6-9970-d6693c15ef24.png)\n\n    - 传输过程中会打印进程相关的CPU、JVM等\n\n      ![datax_run_cpu](https://cloud.githubusercontent.com/assets/1067175/17850903/ee63c2fe-6891-11e6-9056-97d7e3d13d8d.png)\n\n    - 在任务结束之后，打印总体运行情况\n\n      ![datax_end_info](https://cloud.githubusercontent.com/assets/1067175/17850930/0484d3ac-6892-11e6-9c1d-b102ad210a32.png)\n"
        },
        {
          "name": "kingbaseesreader",
          "type": "tree",
          "content": null
        },
        {
          "name": "kingbaseeswriter",
          "type": "tree",
          "content": null
        },
        {
          "name": "kuduwriter",
          "type": "tree",
          "content": null
        },
        {
          "name": "license.txt",
          "type": "blob",
          "size": 0.5595703125,
          "content": "Copyright 1999-2022 Alibaba Group Holding Ltd.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n     http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n"
        },
        {
          "name": "loghubreader",
          "type": "tree",
          "content": null
        },
        {
          "name": "loghubwriter",
          "type": "tree",
          "content": null
        },
        {
          "name": "mongodbreader",
          "type": "tree",
          "content": null
        },
        {
          "name": "mongodbwriter",
          "type": "tree",
          "content": null
        },
        {
          "name": "mysqlreader",
          "type": "tree",
          "content": null
        },
        {
          "name": "mysqlwriter",
          "type": "tree",
          "content": null
        },
        {
          "name": "neo4jwriter",
          "type": "tree",
          "content": null
        },
        {
          "name": "obhbasereader",
          "type": "tree",
          "content": null
        },
        {
          "name": "obhbasewriter",
          "type": "tree",
          "content": null
        },
        {
          "name": "oceanbasev10reader",
          "type": "tree",
          "content": null
        },
        {
          "name": "oceanbasev10writer",
          "type": "tree",
          "content": null
        },
        {
          "name": "ocswriter",
          "type": "tree",
          "content": null
        },
        {
          "name": "odpsreader",
          "type": "tree",
          "content": null
        },
        {
          "name": "odpswriter",
          "type": "tree",
          "content": null
        },
        {
          "name": "opentsdbreader",
          "type": "tree",
          "content": null
        },
        {
          "name": "oraclereader",
          "type": "tree",
          "content": null
        },
        {
          "name": "oraclewriter",
          "type": "tree",
          "content": null
        },
        {
          "name": "oscarwriter",
          "type": "tree",
          "content": null
        },
        {
          "name": "ossreader",
          "type": "tree",
          "content": null
        },
        {
          "name": "osswriter",
          "type": "tree",
          "content": null
        },
        {
          "name": "otsreader",
          "type": "tree",
          "content": null
        },
        {
          "name": "otsstreamreader",
          "type": "tree",
          "content": null
        },
        {
          "name": "otswriter",
          "type": "tree",
          "content": null
        },
        {
          "name": "package.xml",
          "type": "blob",
          "size": 18.4697265625,
          "content": "<assembly\n        xmlns=\"http://maven.apache.org/plugins/maven-assembly-plugin/assembly/1.1.0\"\n        xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n        xsi:schemaLocation=\"http://maven.apache.org/plugins/maven-assembly-plugin/assembly/1.1.0 http://maven.apache.org/xsd/assembly-1.1.0.xsd\">\n    <id></id>\n    <formats>\n        <format>tar.gz</format>\n        <format>dir</format>\n    </formats>\n    <includeBaseDirectory>false</includeBaseDirectory>\n    <fileSets>\n        <fileSet>\n            <directory>transformer/target/datax/</directory>\n            <includes>\n                <include>**/*.*</include>\n            </includes>\n            <outputDirectory>datax</outputDirectory>\n        </fileSet>\n        <fileSet>\n            <directory>core/target/datax/</directory>\n            <includes>\n                <include>**/*.*</include>\n            </includes>\n            <outputDirectory>datax</outputDirectory>\n        </fileSet>\n\n        <!-- reader -->\n        <fileSet>\n            <directory>mysqlreader/target/datax/</directory>\n            <includes>\n                <include>**/*.*</include>\n            </includes>\n            <outputDirectory>datax</outputDirectory>\n        </fileSet>\n        <fileSet>\n            <directory>oceanbasev10reader/target/datax/</directory>\n            <includes>\n                <include>**/*.*</include>\n            </includes>\n            <outputDirectory>datax</outputDirectory>\n        </fileSet>\n        <fileSet>\n            <directory>obhbasereader/target/datax/</directory>\n            <includes>\n                <include>**/*.*</include>\n            </includes>\n            <outputDirectory>datax</outputDirectory>\n        </fileSet>\n        <fileSet>\n            <directory>drdsreader/target/datax/</directory>\n            <includes>\n                <include>**/*.*</include>\n            </includes>\n            <outputDirectory>datax</outputDirectory>\n        </fileSet>\n        <fileSet>\n            <directory>oraclereader/target/datax/</directory>\n            <includes>\n                <include>**/*.*</include>\n            </includes>\n            <outputDirectory>datax</outputDirectory>\n        </fileSet>\n        <fileSet>\n            <directory>sqlserverreader/target/datax/</directory>\n            <includes>\n                <include>**/*.*</include>\n            </includes>\n            <outputDirectory>datax</outputDirectory>\n        </fileSet>\n        <fileSet>\n            <directory>postgresqlreader/target/datax/</directory>\n            <includes>\n                <include>**/*.*</include>\n            </includes>\n            <outputDirectory>datax</outputDirectory>\n        </fileSet>\n        <fileSet>\n            <directory>kingbaseesreader/target/datax/</directory>\n            <includes>\n                <include>**/*.*</include>\n            </includes>\n            <outputDirectory>datax</outputDirectory>\n        </fileSet>\n        <fileSet>\n            <directory>rdbmsreader/target/datax/</directory>\n            <includes>\n                <include>**/*.*</include>\n            </includes>\n            <outputDirectory>datax</outputDirectory>\n        </fileSet>\n\n        <fileSet>\n            <directory>odpsreader/target/datax/</directory>\n            <includes>\n                <include>**/*.*</include>\n            </includes>\n            <outputDirectory>datax</outputDirectory>\n        </fileSet>\n        <fileSet>\n            <directory>otsreader/target/datax/</directory>\n            <includes>\n                <include>**/*.*</include>\n            </includes>\n            <outputDirectory>datax</outputDirectory>\n        </fileSet>\n        <fileSet>\n            <directory>otsstreamreader/target/datax/</directory>\n            <includes>\n                <include>**/*.*</include>\n            </includes>\n            <outputDirectory>datax</outputDirectory>\n        </fileSet>\n        <fileSet>\n            <directory>txtfilereader/target/datax/</directory>\n            <includes>\n                <include>**/*.*</include>\n            </includes>\n            <outputDirectory>datax</outputDirectory>\n        </fileSet>\n        <fileSet>\n            <directory>ossreader/target/datax/</directory>\n            <includes>\n                <include>**/*.*</include>\n            </includes>\n            <outputDirectory>datax</outputDirectory>\n        </fileSet>\n        <fileSet>\n            <directory>mongodbreader/target/datax/</directory>\n            <includes>\n                <include>**/*.*</include>\n            </includes>\n            <outputDirectory>datax</outputDirectory>\n        </fileSet>\n        <fileSet>\n            <directory>tdenginereader/target/datax/</directory>\n            <includes>\n                <include>**/*.*</include>\n            </includes>\n            <outputDirectory>datax</outputDirectory>\n        </fileSet>\n        <fileSet>\n            <directory>streamreader/target/datax/</directory>\n            <includes>\n                <include>**/*.*</include>\n            </includes>\n            <outputDirectory>datax</outputDirectory>\n        </fileSet>\n         <fileSet>\n            <directory>ftpreader/target/datax/</directory>\n            <includes>\n                <include>**/*.*</include>\n            </includes>\n            <outputDirectory>datax</outputDirectory>\n        </fileSet>\n        <fileSet>\n            <directory>clickhousereader/target/datax/</directory>\n            <includes>\n                <include>**/*.*</include>\n            </includes>\n            <outputDirectory>datax</outputDirectory>\n        </fileSet>\n        <fileSet>\n            <directory>hdfsreader/target/datax/</directory>\n            <includes>\n                <include>**/*.*</include>\n            </includes>\n            <outputDirectory>datax</outputDirectory>\n        </fileSet>\n        <fileSet>\n            <directory>hbase11xreader/target/datax/</directory>\n            <includes>\n                <include>**/*.*</include>\n            </includes>\n            <outputDirectory>datax</outputDirectory>\n        </fileSet>\n        <fileSet>\n            <directory>hbase094xreader/target/datax/</directory>\n            <includes>\n                <include>**/*.*</include>\n            </includes>\n            <outputDirectory>datax</outputDirectory>\n        </fileSet>\n        <fileSet>\n            <directory>opentsdbreader/target/datax/</directory>\n            <includes>\n                <include>**/*.*</include>\n            </includes>\n            <outputDirectory>datax</outputDirectory>\n        </fileSet>\n        <fileSet>\n            <directory>cassandrareader/target/datax/</directory>\n            <includes>\n                <include>**/*.*</include>\n            </includes>\n            <outputDirectory>datax</outputDirectory>\n        </fileSet>\n        <fileSet>\n            <directory>gdbreader/target/datax/</directory>\n            <includes>\n                <include>**/*.*</include>\n            </includes>\n            <outputDirectory>datax</outputDirectory>\n        </fileSet>\n        <fileSet>\n            <directory>hbase11xsqlreader/target/datax/</directory>\n            <includes>\n                <include>**/*.*</include>\n            </includes>\n            <outputDirectory>datax</outputDirectory>\n        </fileSet>\n        <fileSet>\n            <directory>hbase20xsqlreader/target/datax/</directory>\n            <includes>\n                <include>**/*.*</include>\n            </includes>\n            <outputDirectory>datax</outputDirectory>\n        </fileSet>\n        <fileSet>\n            <directory>tsdbreader/target/datax/</directory>\n            <includes>\n                <include>**/*.*</include>\n            </includes>\n            <outputDirectory>datax</outputDirectory>\n        </fileSet>\n        <fileSet>\n            <directory>datahubreader/target/datax/</directory>\n            <includes>\n                <include>**/*.*</include>\n            </includes>\n            <outputDirectory>datax</outputDirectory>\n        </fileSet>\n        <fileSet>\n            <directory>loghubreader/target/datax/</directory>\n            <includes>\n                <include>**/*.*</include>\n            </includes>\n            <outputDirectory>datax</outputDirectory>\n        </fileSet>\n        <fileSet>\n            <directory>starrocksreader/target/datax/</directory>\n            <includes>\n                <include>**/*.*</include>\n            </includes>\n            <outputDirectory>datax</outputDirectory>\n        </fileSet>\n        <fileSet>\n            <directory>dorisreader/target/datax/</directory>\n            <includes>\n                <include>**/*.*</include>\n            </includes>\n            <outputDirectory>datax</outputDirectory>\n        </fileSet>\n        <fileSet>\n            <directory>sybasereader/target/datax/</directory>\n            <includes>\n                <include>**/*.*</include>\n            </includes>\n            <outputDirectory>datax</outputDirectory>\n        </fileSet>\n\n        <!-- writer -->\n        <fileSet>\n            <directory>mysqlwriter/target/datax/</directory>\n            <includes>\n                <include>**/*.*</include>\n            </includes>\n            <outputDirectory>datax</outputDirectory>\n        </fileSet>\n        <fileSet>\n            <directory>tdenginewriter/target/datax/</directory>\n            <includes>\n                <include>**/*.*</include>\n            </includes>\n            <outputDirectory>datax</outputDirectory>\n        </fileSet>\n        <fileSet>\n            <directory>starrockswriter/target/datax/</directory>\n            <includes>\n                <include>**/*.*</include>\n            </includes>\n            <outputDirectory>datax</outputDirectory>\n        </fileSet>\n        <fileSet>\n            <directory>drdswriter/target/datax/</directory>\n            <includes>\n                <include>**/*.*</include>\n            </includes>\n            <outputDirectory>datax</outputDirectory>\n        </fileSet>\n        <fileSet>\n            <directory>odpswriter/target/datax/</directory>\n            <includes>\n                <include>**/*.*</include>\n            </includes>\n            <outputDirectory>datax</outputDirectory>\n        </fileSet>\n        <fileSet>\n            <directory>doriswriter/target/datax/</directory>\n            <includes>\n                <include>**/*.*</include>\n            </includes>\n            <outputDirectory>datax</outputDirectory>\n        </fileSet>\n        <fileSet>\n            <directory>txtfilewriter/target/datax/</directory>\n            <includes>\n                <include>**/*.*</include>\n            </includes>\n            <outputDirectory>datax</outputDirectory>\n        </fileSet>\n        <fileSet>\n            <directory>ftpwriter/target/datax/</directory>\n            <includes>\n                <include>**/*.*</include>\n            </includes>\n            <outputDirectory>datax</outputDirectory>\n        </fileSet>\n        <fileSet>\n            <directory>osswriter/target/datax/</directory>\n            <includes>\n                <include>**/*.*</include>\n            </includes>\n            <outputDirectory>datax</outputDirectory>\n        </fileSet>\n        <fileSet>\n            <directory>adswriter/target/datax/</directory>\n            <includes>\n                <include>**/*.*</include>\n            </includes>\n            <outputDirectory>datax</outputDirectory>\n        </fileSet>\n        <fileSet>\n            <directory>streamwriter/target/datax/</directory>\n            <includes>\n                <include>**/*.*</include>\n            </includes>\n            <outputDirectory>datax</outputDirectory>\n        </fileSet>\n        <fileSet>\n            <directory>otswriter/target/datax/</directory>\n            <includes>\n                <include>**/*.*</include>\n            </includes>\n            <outputDirectory>datax</outputDirectory>\n        </fileSet>\n        <fileSet>\n            <directory>mongodbwriter/target/datax/</directory>\n            <includes>\n                <include>**/*.*</include>\n            </includes>\n            <outputDirectory>datax</outputDirectory>\n        </fileSet>\n\t\t<fileSet>\n            <directory>oraclewriter/target/datax/</directory>\n            <includes>\n                <include>**/*.*</include>\n            </includes>\n            <outputDirectory>datax</outputDirectory>\n        </fileSet>\n        <fileSet>\n            <directory>sqlserverwriter/target/datax/</directory>\n            <includes>\n                <include>**/*.*</include>\n            </includes>\n            <outputDirectory>datax</outputDirectory>\n        </fileSet>\n\t\t<fileSet>\n            <directory>postgresqlwriter/target/datax/</directory>\n            <includes>\n                <include>**/*.*</include>\n            </includes>\n            <outputDirectory>datax</outputDirectory>\n        </fileSet>\n        <fileSet>\n            <directory>kingbaseeswriter/target/datax/</directory>\n            <includes>\n                <include>**/*.*</include>\n            </includes>\n            <outputDirectory>datax</outputDirectory>\n        </fileSet>\n        <fileSet>\n            <directory>rdbmswriter/target/datax/</directory>\n            <includes>\n                <include>**/*.*</include>\n            </includes>\n            <outputDirectory>datax</outputDirectory>\n        </fileSet>\n        <fileSet>\n            <directory>ocswriter/target/datax/</directory>\n            <includes>\n                <include>**/*.*</include>\n            </includes>\n            <outputDirectory>datax</outputDirectory>\n        </fileSet>\n        <fileSet>\n            <directory>hdfswriter/target/datax/</directory>\n            <includes>\n                <include>**/*.*</include>\n            </includes>\n            <outputDirectory>datax</outputDirectory>\n        </fileSet>\n        <fileSet>\n            <directory>hbase11xwriter/target/datax/</directory>\n            <includes>\n                <include>**/*.*</include>\n            </includes>\n            <outputDirectory>datax</outputDirectory>\n        </fileSet>\n        <fileSet>\n            <directory>hbase094xwriter/target/datax/</directory>\n            <includes>\n                <include>**/*.*</include>\n            </includes>\n            <outputDirectory>datax</outputDirectory>\n        </fileSet>\n        <fileSet>\n            <directory>hbase11xsqlwriter/target/datax/</directory>\n            <includes>\n                <include>**/*.*</include>\n            </includes>\n            <outputDirectory>datax</outputDirectory>\n        </fileSet>\n        <fileSet>\n            <directory>elasticsearchwriter/target/datax/</directory>\n            <includes>\n                <include>**/*.*</include>\n            </includes>\n            <outputDirectory>datax</outputDirectory>\n        </fileSet>\n        <fileSet>\n            <directory>hbase20xsqlwriter/target/datax/</directory>\n            <includes>\n                <include>**/*.*</include>\n            </includes>\n            <outputDirectory>datax</outputDirectory>\n        </fileSet>\n        <fileSet>\n            <directory>tsdbwriter/target/datax/</directory>\n            <includes>\n                <include>**/*.*</include>\n            </includes>\n            <outputDirectory>datax</outputDirectory>\n        </fileSet>\n        <fileSet>\n            <directory>adbpgwriter/target/datax/</directory>\n            <includes>\n                <include>**/*.*</include>\n            </includes>\n            <outputDirectory>datax</outputDirectory>\n        </fileSet>\n        <fileSet>\n            <directory>cassandrawriter/target/datax/</directory>\n            <includes>\n                <include>**/*.*</include>\n            </includes>\n            <outputDirectory>datax</outputDirectory>\n        </fileSet>\n        <fileSet>\n            <directory>clickhousewriter/target/datax/</directory>\n            <includes>\n                <include>**/*.*</include>\n            </includes>\n            <outputDirectory>datax</outputDirectory>\n        </fileSet>\n        <fileSet>\n            <directory>databendwriter/target/datax/</directory>\n            <includes>\n                <include>**/*.*</include>\n            </includes>\n            <outputDirectory>datax</outputDirectory>\n        </fileSet>\n        <fileSet>\n            <directory>oscarwriter/target/datax/</directory>\n            <includes>\n                <include>**/*.*</include>\n            </includes>\n            <outputDirectory>datax</outputDirectory>\n        </fileSet>\n        <fileSet>\n            <directory>oceanbasev10writer/target/datax/</directory>\n            <includes>\n                <include>**/*.*</include>\n            </includes>\n            <outputDirectory>datax</outputDirectory>\n        </fileSet>\n        <fileSet>\n            <directory>obhbasewriter/target/datax/</directory>\n            <includes>\n                <include>**/*.*</include>\n            </includes>\n            <outputDirectory>datax</outputDirectory>\n        </fileSet>\n        <fileSet>\n            <directory>gdbwriter/target/datax/</directory>\n            <includes>\n                <include>**/*.*</include>\n            </includes>\n            <outputDirectory>datax</outputDirectory>\n        </fileSet>\n        <fileSet>\n            <directory>kuduwriter/target/datax/</directory>\n            <includes>\n                <include>**/*.*</include>\n            </includes>\n            <outputDirectory>datax</outputDirectory>\n        </fileSet>\n        <fileSet>\n            <directory>hologresjdbcwriter/target/datax/</directory>\n            <includes>\n                <include>**/*.*</include>\n            </includes>\n            <outputDirectory>datax</outputDirectory>\n        </fileSet>\n        <fileSet>\n            <directory>datahubwriter/target/datax/</directory>\n            <includes>\n                <include>**/*.*</include>\n            </includes>\n            <outputDirectory>datax</outputDirectory>\n        </fileSet>\n        <fileSet>\n            <directory>loghubwriter/target/datax/</directory>\n            <includes>\n                <include>**/*.*</include>\n            </includes>\n            <outputDirectory>datax</outputDirectory>\n        </fileSet>\n        <fileSet>\n            <directory>selectdbwriter/target/datax/</directory>\n            <includes>\n                <include>**/*.*</include>\n            </includes>\n            <outputDirectory>datax</outputDirectory>\n        </fileSet>\n        <fileSet>\n            <directory>neo4jwriter/target/datax/</directory>\n            <includes>\n                <include>**/*.*</include>\n            </includes>\n            <outputDirectory>datax</outputDirectory>\n        </fileSet>\n        <fileSet>\n            <directory>sybasewriter/target/datax/</directory>\n            <includes>\n                <include>**/*.*</include>\n            </includes>\n            <outputDirectory>datax</outputDirectory>\n        </fileSet>\n    </fileSets>\n</assembly>\n"
        },
        {
          "name": "plugin-rdbms-util",
          "type": "tree",
          "content": null
        },
        {
          "name": "plugin-unstructured-storage-util",
          "type": "tree",
          "content": null
        },
        {
          "name": "pom.xml",
          "type": "blob",
          "size": 10.759765625,
          "content": "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<project xmlns=\"http://maven.apache.org/POM/4.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\">\n    <modelVersion>4.0.0</modelVersion>\n\n    <groupId>com.alibaba.datax</groupId>\n    <artifactId>datax-all</artifactId>\n    <version>0.0.1-SNAPSHOT</version>\n    <dependencies>\n        <dependency>\n            <groupId>org.hamcrest</groupId>\n            <artifactId>hamcrest-core</artifactId>\n            <version>1.3</version>\n        </dependency>\n    </dependencies>\n\n    <name>datax-all</name>\n    <packaging>pom</packaging>\n\n    <properties>\n        <jdk-version>1.8</jdk-version>\n        <datax-project-version>0.0.1-SNAPSHOT</datax-project-version>\n        <commons-lang3-version>3.3.2</commons-lang3-version>\n        <commons-configuration-version>1.10</commons-configuration-version>\n        <commons-cli-version>1.2</commons-cli-version>\n        <fastjson-version>2.0.23</fastjson-version>\n        <guava-version>16.0.1</guava-version>\n        <diamond.version>3.7.2.1-SNAPSHOT</diamond.version>\n\n        <!--slf4j 1.7.10 和 logback-classic 1.0.13 是好基友 -->\n        <slf4j-api-version>1.7.10</slf4j-api-version>\n        <logback-classic-version>1.0.13</logback-classic-version>\n        <commons-io-version>2.4</commons-io-version>\n        <junit-version>4.13.1</junit-version>\n        <tddl.version>5.1.22-1</tddl.version>\n        <swift-version>1.0.0</swift-version>\n\n        <project-sourceEncoding>UTF-8</project-sourceEncoding>\n        <project.build.sourceEncoding>UTF-8</project.build.sourceEncoding>\n        <project.reporting.outputEncoding>UTF-8</project.reporting.outputEncoding>\n        <maven.compiler.encoding>UTF-8</maven.compiler.encoding>\n        <mysql.driver.version>5.1.47</mysql.driver.version>\n    </properties>\n\n    <modules>\n        <module>common</module>\n        <module>core</module>\n        <module>transformer</module>\n\n        <!-- reader -->\n        <module>mysqlreader</module>\n        <module>drdsreader</module>\n        <module>sqlserverreader</module>\n        <module>postgresqlreader</module>\n        <module>kingbaseesreader</module>\n        <module>oraclereader</module>\n        <module>cassandrareader</module>\n        <module>oceanbasev10reader</module>\n        <module>obhbasereader</module>\n        <module>rdbmsreader</module>\n\n        <module>odpsreader</module>\n        <module>otsreader</module>\n        <module>otsstreamreader</module>\n        <module>hbase11xreader</module>\n        <module>hbase094xreader</module>\n        <module>hbase11xsqlreader</module>\n        <module>hbase20xsqlreader</module>\n\n        <module>ossreader</module>\n        <module>hdfsreader</module>\n        <module>ftpreader</module>\n        <module>txtfilereader</module>\n        <module>streamreader</module>\n        <module>clickhousereader</module>\n\n        <module>mongodbreader</module>\n        <module>tdenginereader</module>\n        <module>gdbreader</module>\n        <module>tsdbreader</module>\n        <module>opentsdbreader</module>\n        <module>loghubreader</module>\n        <module>datahubreader</module>\n        <module>starrocksreader</module>\n        <module>sybasereader</module>\n        <module>dorisreader</module>\n        <!-- writer -->\n        <module>mysqlwriter</module>\n        <module>starrockswriter</module>\n        <module>drdswriter</module>\n        <module>databendwriter</module>\n        <module>oraclewriter</module>\n        <module>sqlserverwriter</module>\n        <module>postgresqlwriter</module>\n        <module>kingbaseeswriter</module>\n        <module>adswriter</module>\n        <module>oceanbasev10writer</module>\n        <module>obhbasewriter</module>\n        <module>adbpgwriter</module>\n        <module>hologresjdbcwriter</module>\n        <module>rdbmswriter</module>\n\n\n        <module>odpswriter</module>\n        <module>osswriter</module>\n        <module>otswriter</module>\n        <module>hbase11xwriter</module>\n        <module>hbase094xwriter</module>\n        <module>hbase11xsqlwriter</module>\n        <module>hbase20xsqlwriter</module>\n        <module>kuduwriter</module>\n        <module>ftpwriter</module>\n        <module>hdfswriter</module>\n        <module>txtfilewriter</module>\n        <module>streamwriter</module>\n\n        <module>elasticsearchwriter</module>\n        <module>mongodbwriter</module>\n        <module>tdenginewriter</module>\n        <module>ocswriter</module>\n        <module>tsdbwriter</module>\n        <module>gdbwriter</module>\n        <module>oscarwriter</module>\n        <module>loghubwriter</module>\n        <module>datahubwriter</module>\n        <module>cassandrawriter</module>\n        <module>clickhousewriter</module>\n        <module>doriswriter</module>\n        <module>selectdbwriter</module>\n        <module>adbmysqlwriter</module>\n        <module>sybasewriter</module>\n        <module>neo4jwriter</module>\n        <!-- common support module -->\n        <module>plugin-rdbms-util</module>\n        <module>plugin-unstructured-storage-util</module>\n        <module>gaussdbreader</module>\n        <module>gaussdbwriter</module>\n        <module>datax-example</module>\n\n    </modules>\n\n    <dependencyManagement>\n        <dependencies>\n            <dependency>\n                <groupId>org.apache.commons</groupId>\n                <artifactId>commons-lang3</artifactId>\n                <version>${commons-lang3-version}</version>\n            </dependency>\n            <dependency>\n                <groupId>com.alibaba.fastjson2</groupId>\n                <artifactId>fastjson2</artifactId>\n                <version>${fastjson-version}</version>\n            </dependency>\n            <!--<dependency>\n                <groupId>com.google.guava</groupId>\n                <artifactId>guava</artifactId>\n                <version>${guava-version}</version>\n            </dependency>-->\n            <dependency>\n                <groupId>commons-io</groupId>\n                <artifactId>commons-io</artifactId>\n                <version>${commons-io-version}</version>\n            </dependency>\n            <dependency>\n                <groupId>org.slf4j</groupId>\n                <artifactId>slf4j-api</artifactId>\n                <version>${slf4j-api-version}</version>\n            </dependency>\n            <dependency>\n                <groupId>ch.qos.logback</groupId>\n                <artifactId>logback-classic</artifactId>\n                <version>${logback-classic-version}</version>\n            </dependency>\n\n            <dependency>\n                <groupId>com.taobao.tddl</groupId>\n                <artifactId>tddl-client</artifactId>\n                <version>${tddl.version}</version>\n                <exclusions>\n                    <exclusion>\n                        <groupId>com.google.guava</groupId>\n                        <artifactId>guava</artifactId>\n                    </exclusion>\n                    <exclusion>\n                        <groupId>com.taobao.diamond</groupId>\n                        <artifactId>diamond-client</artifactId>\n                    </exclusion>\n                </exclusions>\n            </dependency>\n\n            <dependency>\n                <groupId>com.taobao.diamond</groupId>\n                <artifactId>diamond-client</artifactId>\n                <version>${diamond.version}</version>\n            </dependency>\n\n            <dependency>\n                <groupId>com.alibaba.search.swift</groupId>\n                <artifactId>swift_client</artifactId>\n                <version>${swift-version}</version>\n            </dependency>\n\n            <dependency>\n                <groupId>junit</groupId>\n                <artifactId>junit</artifactId>\n                <version>${junit-version}</version>\n            </dependency>\n\n            <dependency>\n                <groupId>org.mockito</groupId>\n                <artifactId>mockito-all</artifactId>\n                <version>1.9.5</version>\n                <scope>test</scope>\n            </dependency>\n            <dependency>\n                <groupId>org.apache.logging.log4j</groupId>\n                <artifactId>log4j-api</artifactId>\n                <version>2.17.1</version>\n            </dependency>\n\n            <dependency>\n                <groupId>org.apache.logging.log4j</groupId>\n                <artifactId>log4j-core</artifactId>\n                <version>2.17.1</version>\n            </dependency>\n        </dependencies>\n    </dependencyManagement>\n\n    <repositories>\n        <repository>\n            <id>central</id>\n            <name>Nexus aliyun</name>\n            <url>https://maven.aliyun.com/repository/central</url>\n            <releases>\n                <enabled>true</enabled>\n            </releases>\n            <snapshots>\n                <enabled>true</enabled>\n            </snapshots>\n        </repository>\n        <repository>\n            <id>spring</id>\n            <name>spring</name>\n            <url>https://maven.aliyun.com/repository/spring</url>\n            <releases>\n                <enabled>true</enabled>\n            </releases>\n            <snapshots>\n                <enabled>true</enabled>\n            </snapshots>\n        </repository>\n    </repositories>\n\n    <pluginRepositories>\n        <pluginRepository>\n            <id>central</id>\n            <name>Nexus aliyun</name>\n            <url>https://maven.aliyun.com/repository/central</url>\n            <releases>\n                <enabled>true</enabled>\n            </releases>\n            <snapshots>\n                <enabled>true</enabled>\n            </snapshots>\n        </pluginRepository>\n    </pluginRepositories>\n\n    <build>\n        <resources>\n            <resource>\n                <directory>src/main/java</directory>\n                <includes>\n                    <include>**/*.properties</include>\n                </includes>\n            </resource>\n        </resources>\n        <plugins>\n            <plugin>\n                <artifactId>maven-assembly-plugin</artifactId>\n                <configuration>\n                    <finalName>datax</finalName>\n                    <descriptors>\n                        <descriptor>package.xml</descriptor>\n                    </descriptors>\n                </configuration>\n                <executions>\n                    <execution>\n                        <id>make-assembly</id>\n                        <phase>package</phase>\n                    </execution>\n                </executions>\n            </plugin>\n            <plugin>\n                <groupId>org.apache.maven.plugins</groupId>\n                <artifactId>maven-compiler-plugin</artifactId>\n                <version>2.3.2</version>\n                <configuration>\n                    <source>${jdk-version}</source>\n                    <target>${jdk-version}</target>\n                    <encoding>${project-sourceEncoding}</encoding>\n                </configuration>\n            </plugin>\n        </plugins>\n    </build>\n</project>\n"
        },
        {
          "name": "postgresqlreader",
          "type": "tree",
          "content": null
        },
        {
          "name": "postgresqlwriter",
          "type": "tree",
          "content": null
        },
        {
          "name": "rdbmsreader",
          "type": "tree",
          "content": null
        },
        {
          "name": "rdbmswriter",
          "type": "tree",
          "content": null
        },
        {
          "name": "rpm",
          "type": "tree",
          "content": null
        },
        {
          "name": "selectdbwriter",
          "type": "tree",
          "content": null
        },
        {
          "name": "sqlserverreader",
          "type": "tree",
          "content": null
        },
        {
          "name": "sqlserverwriter",
          "type": "tree",
          "content": null
        },
        {
          "name": "starrocksreader",
          "type": "tree",
          "content": null
        },
        {
          "name": "starrockswriter",
          "type": "tree",
          "content": null
        },
        {
          "name": "streamreader",
          "type": "tree",
          "content": null
        },
        {
          "name": "streamwriter",
          "type": "tree",
          "content": null
        },
        {
          "name": "sybasereader",
          "type": "tree",
          "content": null
        },
        {
          "name": "sybasewriter",
          "type": "tree",
          "content": null
        },
        {
          "name": "tdenginereader",
          "type": "tree",
          "content": null
        },
        {
          "name": "tdenginewriter",
          "type": "tree",
          "content": null
        },
        {
          "name": "transformer",
          "type": "tree",
          "content": null
        },
        {
          "name": "tsdbreader",
          "type": "tree",
          "content": null
        },
        {
          "name": "tsdbwriter",
          "type": "tree",
          "content": null
        },
        {
          "name": "txtfilereader",
          "type": "tree",
          "content": null
        },
        {
          "name": "txtfilewriter",
          "type": "tree",
          "content": null
        },
        {
          "name": "userGuid.md",
          "type": "blob",
          "size": 5.609375,
          "content": "# DataX\n\nDataX 是阿里巴巴集团内被广泛使用的离线数据同步工具/平台，实现包括 MySQL、SQL Server、Oracle、PostgreSQL、HDFS、Hive、HBase、OTS、ODPS 等各种异构数据源之间高效的数据同步功能。\n\n# Features\n\nDataX本身作为数据同步框架，将不同数据源的同步抽象为从源头数据源读取数据的Reader插件，以及向目标端写入数据的Writer插件，理论上DataX框架可以支持任意数据源类型的数据同步工作。同时DataX插件体系作为一套生态系统, 每接入一套新数据源该新加入的数据源即可实现和现有的数据源互通。\n\n# System Requirements\n\n- Linux\n- [JDK(1.8以上，推荐1.8) ](http://www.oracle.com/technetwork/cn/java/javase/downloads/index.html) \n- [Python(2或3都可以) ](https://www.python.org/downloads/)\n- [Apache Maven 3.x](https://maven.apache.org/download.cgi) (Compile DataX)\n\n# Quick Start\n\n* 工具部署\n  \n  * 方法一、直接下载DataX工具包：[DataX下载地址](https://datax-opensource.oss-cn-hangzhou.aliyuncs.com/202309/datax.tar.gz)\n    \n    下载后解压至本地某个目录，进入bin目录，即可运行同步作业：\n    \n    ``` shell\n    $ cd  {YOUR_DATAX_HOME}/bin\n    $ python datax.py {YOUR_JOB.json}\n    ```\n    自检脚本：\n    python {YOUR_DATAX_HOME}/bin/datax.py {YOUR_DATAX_HOME}/job/job.json\n  * 方法二、下载DataX源码，自己编译：[DataX源码](https://github.com/alibaba/DataX)\n    \n    (1)、下载DataX源码：\n    \n    ``` shell\n    $ git clone git@github.com:alibaba/DataX.git\n    ```\n    \n    (2)、通过maven打包：\n    \n    ``` shell\n    $ cd  {DataX_source_code_home}\n    $ mvn -U clean package assembly:assembly -Dmaven.test.skip=true\n    ```\n    \n    打包成功，日志显示如下：\n    \n    ``` \n    [INFO] BUILD SUCCESS\n    [INFO] -----------------------------------------------------------------\n    [INFO] Total time: 08:12 min\n    [INFO] Finished at: 2015-12-13T16:26:48+08:00\n    [INFO] Final Memory: 133M/960M\n    [INFO] -----------------------------------------------------------------\n    ```\n    \n    打包成功后的DataX包位于 {DataX_source_code_home}/target/datax/datax/ ，结构如下：\n    \n    ``` shell\n    $ cd  {DataX_source_code_home}\n    $ ls ./target/datax/datax/\n    bin\t\tconf\t\tjob\t\tlib\t\tlog\t\tlog_perf\tplugin\t\tscript\t\ttmp\n    ```\n\n\n* 配置示例：从stream读取数据并打印到控制台\n  \n  * 第一步、创建作业的配置文件（json格式）\n    \n    可以通过命令查看配置模板： python datax.py -r {YOUR_READER} -w {YOUR_WRITER}\n    \n    ``` shell\n    $ cd  {YOUR_DATAX_HOME}/bin\n    $  python datax.py -r streamreader -w streamwriter\n    DataX (UNKNOWN_DATAX_VERSION), From Alibaba !\n    Copyright (C) 2010-2015, Alibaba Group. All Rights Reserved.\n    Please refer to the streamreader document:\n        https://github.com/alibaba/DataX/blob/master/streamreader/doc/streamreader.md \n    \n    Please refer to the streamwriter document:\n         https://github.com/alibaba/DataX/blob/master/streamwriter/doc/streamwriter.md \n     \n    Please save the following configuration as a json file and  use\n         python {DATAX_HOME}/bin/datax.py {JSON_FILE_NAME}.json \n    to run the job.\n    \n    {\n        \"job\": {\n            \"content\": [\n                {\n                    \"reader\": {\n                        \"name\": \"streamreader\", \n                        \"parameter\": {\n                            \"column\": [], \n                            \"sliceRecordCount\": \"\"\n                        }\n                    }, \n                    \"writer\": {\n                        \"name\": \"streamwriter\", \n                        \"parameter\": {\n                            \"encoding\": \"\", \n                            \"print\": true\n                        }\n                    }\n                }\n            ], \n            \"setting\": {\n                \"speed\": {\n                    \"channel\": \"\"\n                }\n            }\n        }\n    }\n    ```\n    \n    根据模板配置json如下：\n    \n    ``` json\n    #stream2stream.json\n    {\n      \"job\": {\n        \"content\": [\n          {\n            \"reader\": {\n              \"name\": \"streamreader\",\n              \"parameter\": {\n                \"sliceRecordCount\": 10,\n                \"column\": [\n                  {\n                    \"type\": \"long\",\n                    \"value\": \"10\"\n                  },\n                  {\n                    \"type\": \"string\",\n                    \"value\": \"hello，你好，世界-DataX\"\n                  }\n                ]\n              }\n            },\n            \"writer\": {\n              \"name\": \"streamwriter\",\n              \"parameter\": {\n                \"encoding\": \"UTF-8\",\n                \"print\": true\n              }\n            }\n          }\n        ],\n        \"setting\": {\n          \"speed\": {\n            \"channel\": 5\n           }\n        }\n      }\n    }\n    ```\n    \n  * 第二步：启动DataX\n    \n    ``` shell\n    $ cd {YOUR_DATAX_DIR_BIN}\n    $ python datax.py ./stream2stream.json \n    ```\n    \n    同步结束，显示日志如下：\n    \n    ``` shell\n    ...\n    2015-12-17 11:20:25.263 [job-0] INFO  JobContainer - \n    任务启动时刻                    : 2015-12-17 11:20:15\n    任务结束时刻                    : 2015-12-17 11:20:25\n    任务总计耗时                    :                 10s\n    任务平均流量                    :              205B/s\n    记录写入速度                    :              5rec/s\n    读出记录总数                    :                  50\n    读写失败总数                    :                   0\n    ```\n\n# Contact us\n\nGoogle Groups: [DataX-user](https://github.com/alibaba/DataX)\n\n\n\n\n"
        }
      ]
    }
  ]
}