{
  "metadata": {
    "timestamp": 1736708962259,
    "page": 6,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjEw",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "elastic/elasticsearch",
      "stars": 71272,
      "defaultBranch": "main",
      "files": [
        {
          "name": ".backportrc.json",
          "type": "blob",
          "size": 0.4111328125,
          "content": "{\n  \"upstream\" : \"elastic/elasticsearch\",\n  \"targetBranchChoices\" : [ \"main\", \"8.x\", \"8.17\", \"8.16\", \"8.15\", \"8.14\", \"8.13\", \"8.12\", \"8.11\", \"8.10\", \"8.9\", \"8.8\", \"8.7\", \"8.6\", \"8.5\", \"8.4\", \"8.3\", \"8.2\", \"8.1\", \"8.0\", \"7.17\", \"6.8\" ],\n  \"targetPRLabels\" : [ \"backport\" ],\n  \"branchLabelMapping\" : {\n    \"^v9.0.0$\" : \"main\",\n    \"^v8.18.0$\" : \"8.x\",\n    \"^v(\\\\d+).(\\\\d+).\\\\d+(?:-(?:alpha|beta|rc)\\\\d+)?$\" : \"$1.$2\"\n  }\n}\n"
        },
        {
          "name": ".buildkite",
          "type": "tree",
          "content": null
        },
        {
          "name": ".ci",
          "type": "tree",
          "content": null
        },
        {
          "name": ".dir-locals.el",
          "type": "blob",
          "size": 3.2626953125,
          "content": "((java-mode\n  .\n  ((eval\n    .\n    (progn\n      (defun my/point-in-defun-declaration-p ()\n        (let ((bod (save-excursion (c-beginning-of-defun)\n                                   (point))))\n          (<= bod\n              (point)\n              (save-excursion (goto-char bod)\n                              (re-search-forward \"{\")\n                              (point)))))\n\n      (defun my/is-string-concatenation-p ()\n        \"Returns true if the previous line is a string concatenation\"\n        (save-excursion\n          (let ((start (point)))\n            (forward-line -1)\n            (if (re-search-forward \" \\\\\\+$\" start t) t nil))))\n\n      (defun my/inside-java-lambda-p ()\n        \"Returns true if point is the first statement inside of a lambda\"\n        (save-excursion\n          (c-beginning-of-statement-1)\n          (let ((start (point)))\n            (forward-line -1)\n            (if (search-forward \" -> {\" start t) t nil))))\n\n      (defun my/trailing-paren-p ()\n        \"Returns true if point is a training paren and semicolon\"\n        (save-excursion\n          (end-of-line)\n          (let ((endpoint (point)))\n            (beginning-of-line)\n            (if (re-search-forward \"[ ]*);$\" endpoint t) t nil))))\n\n      (defun my/prev-line-call-with-no-args-p ()\n        \"Return true if the previous line is a function call with no arguments\"\n        (save-excursion\n          (let ((start (point)))\n            (forward-line -1)\n            (if (re-search-forward \".($\" start t) t nil))))\n\n      (defun my/arglist-cont-nonempty-indentation (arg)\n        (if (my/inside-java-lambda-p)\n            '+\n          (if (my/is-string-concatenation-p)\n              16\n            (unless (my/point-in-defun-declaration-p) '++))))\n\n      (defun my/statement-block-intro (arg)\n        (if (and (c-at-statement-start-p) (my/inside-java-lambda-p)) 0 '+))\n\n      (defun my/block-close (arg)\n        (if (my/inside-java-lambda-p) '- 0))\n\n      (defun my/arglist-close (arg) (if (my/trailing-paren-p) 0 '--))\n\n      (defun my/arglist-intro (arg)\n        (if (my/prev-line-call-with-no-args-p) '++ 0))\n\n      (c-set-offset 'inline-open           0)\n      (c-set-offset 'topmost-intro-cont    '+)\n      (c-set-offset 'statement-block-intro 'my/statement-block-intro)\n      (c-set-offset 'block-close           'my/block-close)\n      (c-set-offset 'knr-argdecl-intro     '+)\n      (c-set-offset 'substatement-open     '+)\n      (c-set-offset 'substatement-label    '+)\n      (c-set-offset 'case-label            '+)\n      (c-set-offset 'label                 '+)\n      (c-set-offset 'statement-case-open   '+)\n      (c-set-offset 'statement-cont        '++)\n      (c-set-offset 'arglist-intro         'my/arglist-intro)\n      (c-set-offset 'arglist-cont-nonempty '(my/arglist-cont-nonempty-indentation c-lineup-arglist))\n      (c-set-offset 'arglist-close         'my/arglist-close)\n      (c-set-offset 'inexpr-class          0)\n      (c-set-offset 'access-label          0)\n      (c-set-offset 'inher-intro           '++)\n      (c-set-offset 'inher-cont            '++)\n      (c-set-offset 'brace-list-intro      '+)\n      (c-set-offset 'func-decl-cont        '++)\n      ))\n   (c-basic-offset . 4)\n   (c-comment-only-line-offset . (0 . 0))\n   (fill-column . 140)\n   (fci-rule-column . 140)\n   (compile-command . \"gradle compileTestJava\"))))\n"
        },
        {
          "name": ".editorconfig",
          "type": "blob",
          "size": 8.990234375,
          "content": "# EditorConfig: http://editorconfig.org/\n\n# Options specific to IntelliJ are prefixed with `ij_`\n\nroot = true\n\n[*]\ncharset = utf-8\ntrim_trailing_whitespace = true\ninsert_final_newline = true\nindent_style = space\n\n[*.gradle]\nij_continuation_indent_size = 2\nindent_size = 2\nmax_line_length = 150\n\n[*.md]\nmax_line_length = 80\n\n[*.groovy]\nindent_size = 4\nij_continuation_indent_size = 4\nmax_line_length = 140\nij_groovy_class_count_to_use_import_on_demand = 999\nij_groovy_names_count_to_use_import_on_demand = 999\nij_groovy_imports_layout = *,|,com.**,|,org.**,|,java.**,javax.**,|,$*\n\n[{*.gradle,*.groovy}]\nij_groovy_align_group_field_declarations = false\nij_groovy_align_multiline_array_initializer_expression = false\nij_groovy_align_multiline_assignment = false\nij_groovy_align_multiline_binary_operation = false\nij_groovy_align_multiline_chained_methods = false\nij_groovy_align_multiline_extends_list = false\nij_groovy_align_multiline_for = true\nij_groovy_align_multiline_list_or_map = true\nij_groovy_align_multiline_method_parentheses = false\nij_groovy_align_multiline_parameters = true\nij_groovy_align_multiline_parameters_in_calls = false\nij_groovy_align_multiline_resources = true\nij_groovy_align_multiline_ternary_operation = false\nij_groovy_align_multiline_throws_list = false\nij_groovy_align_named_args_in_map = true\nij_groovy_align_throws_keyword = false\nij_groovy_array_initializer_new_line_after_left_brace = true\nij_groovy_array_initializer_right_brace_on_new_line = true\nij_groovy_array_initializer_wrap = on_every_item\nij_groovy_assert_statement_wrap = on_every_item\nij_groovy_assignment_wrap = on_every_item\nij_groovy_binary_operation_wrap = normal\nij_groovy_blank_lines_after_class_header = 0\nij_groovy_blank_lines_after_imports = 1\nij_groovy_blank_lines_after_package = 1\nij_groovy_blank_lines_around_class = 1\nij_groovy_blank_lines_around_field = 0\nij_groovy_blank_lines_around_field_in_interface = 0\nij_groovy_blank_lines_around_method = 1\nij_groovy_blank_lines_around_method_in_interface = 1\nij_groovy_blank_lines_before_imports = 1\nij_groovy_blank_lines_before_method_body = 0\nij_groovy_blank_lines_before_package = 0\nij_groovy_block_brace_style = end_of_line\nij_groovy_block_comment_at_first_column = true\nij_groovy_call_parameters_new_line_after_left_paren = true\nij_groovy_call_parameters_right_paren_on_new_line = true\nij_groovy_call_parameters_wrap = on_every_item\nij_groovy_catch_on_new_line = false\nij_groovy_class_annotation_wrap = split_into_lines\nij_groovy_class_brace_style = end_of_line\nij_groovy_do_while_brace_force = always\nij_groovy_else_on_new_line = false\nij_groovy_enum_constants_wrap = on_every_item\nij_groovy_extends_keyword_wrap = normal\nij_groovy_extends_list_wrap = on_every_item\nij_groovy_field_annotation_wrap = split_into_lines\nij_groovy_finally_on_new_line = false\nij_groovy_for_brace_force = always\nij_groovy_for_statement_new_line_after_left_paren = false\nij_groovy_for_statement_right_paren_on_new_line = false\nij_groovy_for_statement_wrap = off\nij_groovy_if_brace_force = never\nij_groovy_import_annotation_wrap = 2\nij_groovy_imports_layout = *,com.**,|,org.**,|,java.**,javax.**,|,$*\nij_groovy_indent_case_from_switch = true\nij_groovy_indent_label_blocks = true\nij_groovy_insert_inner_class_imports = false\nij_groovy_keep_blank_lines_before_right_brace = 2\nij_groovy_keep_blank_lines_in_code = 2\nij_groovy_keep_blank_lines_in_declarations = 2\nij_groovy_keep_control_statement_in_one_line = true\nij_groovy_keep_first_column_comment = true\nij_groovy_keep_indents_on_empty_lines = false\nij_groovy_keep_line_breaks = true\nij_groovy_keep_multiple_expressions_in_one_line = false\nij_groovy_keep_simple_blocks_in_one_line = false\nij_groovy_keep_simple_classes_in_one_line = true\nij_groovy_keep_simple_lambdas_in_one_line = true\nij_groovy_keep_simple_methods_in_one_line = true\nij_groovy_label_indent_absolute = false\nij_groovy_label_indent_size = 0\nij_groovy_lambda_brace_style = end_of_line\nij_groovy_layout_static_imports_separately = true\nij_groovy_line_comment_add_space = false\nij_groovy_line_comment_at_first_column = true\nij_groovy_method_annotation_wrap = split_into_lines\nij_groovy_method_brace_style = end_of_line\nij_groovy_method_call_chain_wrap = off\nij_groovy_method_parameters_new_line_after_left_paren = false\nij_groovy_method_parameters_right_paren_on_new_line = false\nij_groovy_method_parameters_wrap = on_every_item\nij_groovy_modifier_list_wrap = false\nij_groovy_names_count_to_use_import_on_demand = 3\nij_groovy_parameter_annotation_wrap = off\nij_groovy_parentheses_expression_new_line_after_left_paren = false\nij_groovy_parentheses_expression_right_paren_on_new_line = false\nij_groovy_prefer_parameters_wrap = false\nij_groovy_resource_list_new_line_after_left_paren = false\nij_groovy_resource_list_right_paren_on_new_line = false\nij_groovy_resource_list_wrap = off\nij_groovy_space_after_assert_separator = true\nij_groovy_space_after_colon = true\nij_groovy_space_after_comma = true\nij_groovy_space_after_comma_in_type_arguments = true\nij_groovy_space_after_for_semicolon = true\nij_groovy_space_after_quest = true\nij_groovy_space_after_type_cast = true\nij_groovy_space_before_annotation_parameter_list = false\nij_groovy_space_before_array_initializer_left_brace = true\nij_groovy_space_before_assert_separator = false\nij_groovy_space_before_catch_keyword = true\nij_groovy_space_before_catch_left_brace = true\nij_groovy_space_before_catch_parentheses = true\nij_groovy_space_before_class_left_brace = true\nij_groovy_space_before_closure_left_brace = true\nij_groovy_space_before_colon = true\nij_groovy_space_before_comma = false\nij_groovy_space_before_do_left_brace = true\nij_groovy_space_before_else_keyword = true\nij_groovy_space_before_else_left_brace = true\nij_groovy_space_before_finally_keyword = true\nij_groovy_space_before_finally_left_brace = true\nij_groovy_space_before_for_left_brace = true\nij_groovy_space_before_for_parentheses = true\nij_groovy_space_before_for_semicolon = false\nij_groovy_space_before_if_left_brace = true\nij_groovy_space_before_if_parentheses = true\nij_groovy_space_before_method_call_parentheses = false\nij_groovy_space_before_method_left_brace = true\nij_groovy_space_before_method_parentheses = false\nij_groovy_space_before_quest = true\nij_groovy_space_before_switch_left_brace = true\nij_groovy_space_before_switch_parentheses = true\nij_groovy_space_before_synchronized_left_brace = true\nij_groovy_space_before_synchronized_parentheses = true\nij_groovy_space_before_try_left_brace = true\nij_groovy_space_before_try_parentheses = true\nij_groovy_space_before_while_keyword = true\nij_groovy_space_before_while_left_brace = true\nij_groovy_space_before_while_parentheses = true\nij_groovy_space_in_named_argument = true\nij_groovy_space_in_named_argument_before_colon = false\nij_groovy_space_within_empty_array_initializer_braces = false\nij_groovy_space_within_empty_method_call_parentheses = false\nij_groovy_spaces_around_additive_operators = true\nij_groovy_spaces_around_assignment_operators = true\nij_groovy_spaces_around_bitwise_operators = true\nij_groovy_spaces_around_equality_operators = true\nij_groovy_spaces_around_lambda_arrow = true\nij_groovy_spaces_around_logical_operators = true\nij_groovy_spaces_around_multiplicative_operators = true\nij_groovy_spaces_around_regex_operators = true\nij_groovy_spaces_around_relational_operators = true\nij_groovy_spaces_around_shift_operators = true\nij_groovy_spaces_within_annotation_parentheses = false\nij_groovy_spaces_within_array_initializer_braces = true\nij_groovy_spaces_within_braces = true\nij_groovy_spaces_within_brackets = false\nij_groovy_spaces_within_cast_parentheses = false\nij_groovy_spaces_within_catch_parentheses = false\nij_groovy_spaces_within_for_parentheses = false\nij_groovy_spaces_within_gstring_injection_braces = false\nij_groovy_spaces_within_if_parentheses = false\nij_groovy_spaces_within_list_or_map = false\nij_groovy_spaces_within_method_call_parentheses = false\nij_groovy_spaces_within_method_parentheses = false\nij_groovy_spaces_within_parentheses = false\nij_groovy_spaces_within_switch_parentheses = false\nij_groovy_spaces_within_synchronized_parentheses = false\nij_groovy_spaces_within_try_parentheses = false\nij_groovy_spaces_within_tuple_expression = false\nij_groovy_spaces_within_while_parentheses = false\nij_groovy_special_else_if_treatment = true\nij_groovy_ternary_operation_wrap = on_every_item\nij_groovy_throws_keyword_wrap = off\nij_groovy_throws_list_wrap = on_every_item\nij_groovy_use_flying_geese_braces = false\nij_groovy_use_fq_class_names = false\nij_groovy_use_fq_class_names_in_javadoc = true\nij_groovy_use_relative_indents = false\nij_groovy_use_single_class_imports = true\nij_groovy_variable_annotation_wrap = off\nij_groovy_while_brace_force = always\nij_groovy_while_on_new_line = false\nij_groovy_wrap_long_lines = false\n\n[*.java]\nindent_size = 4\nmax_line_length = 140\nij_java_class_count_to_use_import_on_demand = 999\nij_java_names_count_to_use_import_on_demand = 999\nij_java_imports_layout = *,|,com.**,|,org.**,|,java.**,|,javax.**,|,$*\n\n[*.json]\nindent_size = 2\n\n[*.py]\nindent_size = 2\n\n[*.sh]\nindent_size = 2\n\n[*.{yml,yaml}]\nindent_size = 2\n\n[*.{xsd,xml}]\nindent_size = 4\n\n[*.{csv,sql}-spec]\ntrim_trailing_whitespace = false\n"
        },
        {
          "name": ".git-blame-ignore-revs",
          "type": "blob",
          "size": 0.7763671875,
          "content": "# This file contains SHAs of changes that should be ignored when e.g.\n# running `git blame` on a file. Do not edit any of the existing commits.\n#\n# Use this file by running:\n#\n#     git blame --ignore-revs-file=.git-blame-ignore-revs <file>\n#\n# or by configuring `blame.ignoreRevsFile`. The latter ought to also work\n# with IDEs such as IntelliJ.\n\n\n# Format snapshot / restore directories in server\n1afe4b914301a23fa37c41c78185b7575a431cc4\n\n# Format more snapshot / restore relate projects\n559c4e6ef4f9173bbb59043bacd0ac36c7281040\n\n# Format aggregations and related code (server and x-pack)\nd71544976608bdb53fa4d29521fb328e1033ee2f\n\n# Reformatting of whole codebase\n12ad399c488f0cc60e19b5e1b29c6d569cb4351a\n\n# Reformat after upgrading to Spotless 6.17.0\nfe1083f6c5f29e1fc28b82c1b71be9b7c1ec7a73\n"
        },
        {
          "name": ".gitattributes",
          "type": "blob",
          "size": 0.9736328125,
          "content": "CHANGELOG.asciidoc  merge=union\n\n# These files contain expected text output, and should not be changed on\n# Windows\nbuild-tools-internal/src/test/resources/org/elasticsearch/gradle/internal/release/*.asciidoc text eol=lf\n\nx-pack/plugin/esql/compute/src/main/generated/** linguist-generated=true\nx-pack/plugin/esql/compute/src/main/generated-src/** linguist-generated=true\nx-pack/plugin/esql/src/main/antlr/*.tokens linguist-generated=true\nx-pack/plugin/esql/src/main/java/org/elasticsearch/xpack/esql/parser/*.interp linguist-generated=true\nx-pack/plugin/esql/src/main/java/org/elasticsearch/xpack/esql/parser/EsqlBaseLexer*.java linguist-generated=true\nx-pack/plugin/esql/src/main/java/org/elasticsearch/xpack/esql/parser/EsqlBaseParser*.java linguist-generated=true\nx-pack/plugin/esql/src/main/generated/** linguist-generated=true\n\n# ESQL functions docs are autogenerated. More information at `docs/reference/esql/functions/README.md`\ndocs/reference/esql/functions/*/** linguist-generated=true\n\n"
        },
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 1.3203125,
          "content": "\n# intellij files\n.idea/\n*.iml\n*.ipr\n*.iws\nbuild-idea/\n# Eclipse and Intellij put there build files in \"out\"\nout/\n\n# include shared intellij config\n!.idea/eclipseCodeFormatter.xml\n!.idea/externalDependencies.xml\n!.idea/inspectionProfiles/Project_Default.xml\n!.idea/runConfigurations/\n!.idea/scopes/x_pack.xml\n\n# These files are generated in the main tree by IntelliJ\nbenchmarks/src/main/generated/*\n\n# eclipse files\n.project\n.classpath\n.settings\n# We don't use this any more, but we'll keep it around in gitignore for a while so we don't accidentally commit it\nbuild-eclipse/\n\n# netbeans files\nnb-configuration.xml\nnbactions.xml\n\n# gradle stuff\n.gradle/\nbuild/\n\n# vscode stuff\n.vscode/\n\n# vs stuff\n.vs/\n\n# testing stuff\n**/.local*\n.vagrant/\n/logs/\n\n# osx stuff\n.DS_Store\n\n# default folders in which the create_bwc_index.py expects to find old es versions in\n/backwards\n/dev-tools/backwards\n\n# needed in case docs build is run...maybe we can configure doc build to generate files under build?\nhtml_docs\n\n# random old stuff that we should look at the necessity of...\n/tmp/\neclipse-build\n\n# projects using testfixtures\ntestfixtures_shared/\n\n# These are generated from .ci/jobs.t, .ci/templates.t and .ci/views.t\n.ci/jobs/\n.ci/templates/\n.ci/views/\n\n# Generated\ncheckstyle_ide.xml\nx-pack/plugin/esql/src/main/generated-src/generated/\n\n# JEnv\n.java-version\n"
        },
        {
          "name": ".idea",
          "type": "tree",
          "content": null
        },
        {
          "name": "BUILDING.md",
          "type": "blob",
          "size": 12.9775390625,
          "content": "Building Elasticsearch with Gradle\n=============================\n\nElasticsearch is built using the [Gradle](https://gradle.org/) open source build tools.\n\nThis document provides a general guidelines for using and working on the Elasticsearch build logic.\n\n## Build logic organisation\n\nThe Elasticsearch project contains 3 build-related projects that are included into the Elasticsearch build as a [composite build](https://docs.gradle.org/current/userguide/composite_builds.html).\n\n### `build-conventions`\n\nThis project contains build conventions that are applied to all Elasticsearch projects.\n\n### `build-tools`\n\nThis project contains all build logic that we publish for third party Elasticsearch plugin authors.\nWe provide the following plugins:\n\n- `elasticsearch.esplugin` - A Gradle plugin for building an elasticsearch plugin.\n- `elasticsearch.testclusters` - A Gradle plugin for setting up es clusters for testing within a build.\n\nThis project is published as part of the Elasticsearch release and accessible by\n`org.elasticsearch.gradle:build-tools:<versionNumber>`.\nThese build tools are also used by the `elasticsearch-hadoop` project maintained by elastic.\n\n### `build-tools-internal`\n\nThis project contains all Elasticsearch project specific build logic that is not meant to be shared\nwith other internal or external projects.\n\n## Build guidelines\n\nThis is an intentionally small set of guidelines to build users and authors\nto ensure we keep the build consistent. We also publish Elasticsearch build logic\nas `build-tools` to be usable by thirdparty Elasticsearch plugin authors. This is\nalso used by other elastic teams like `elasticsearch-hadoop`.\nBreaking changes should therefore be avoided and an appropriate deprecation cycle\nshould be followed.\n\n### Stay up to date\n\nThe Elasticsearch build usually uses the latest Gradle GA release. We stay as close to the\nlatest Gradle releases as possible. In certain cases an update is blocked by a breaking behaviour\nin Gradle. We're usually in contact with the Gradle team here or working on a fix\nin our build logic to resolve this.\n\n**The Elasticsearch build will fail if any deprecated Gradle API is used.**\n\n### Follow Gradle best practices\n\nTony Robalik has compiled a good list of rules that aligns with ours when it comes to writing and maintaining Elasticsearch\nGradle build logic at http://autonomousapps.com/blog/rules-for-gradle-plugin-authors.html.\nOur current build does not yet tick off all those rules everywhere but the ultimate goal is to follow these principles.\nThe reasons for following those rules besides better readability or maintenance are also the goal to support newer Gradle\nfeatures that we will benefit from in terms of performance and reliability.\nE.g. [configuration-cache support](https://github.com/elastic/elasticsearch/issues/57918), [Project Isolation]([https://gradle.github.io/configuration-cache/#project_isolation) or\n[predictive test selection](https://gradle.com/gradle-enterprise-solutions/predictive-test-selection/)\n\n### Make a change in the build\n\nThere are a few guidelines to follow that should make your life easier to make changes to the Elasticsearch build.\nPlease add a member of the `es-delivery` team as a reviewer if you're making non-trivial changes to the build.\n\n#### Adding or updating a dependency\n\nWe rely on [Gradle dependency verification](https://docs.gradle.org/current/userguide/dependency_verification.html) to mitigate the security risks and avoid integrating compromised dependencies.\n\nThis requires to have third party dependencies and their checksums listed in `gradle/verification-metadata.xml`.\n\nFor updated or newly added dependencies you need to add an entry to this verification file or update the existing one:\n```\n      <component group=\"asm\" name=\"asm\" version=\"3.1\">\n         <artifact name=\"asm-3.1.jar\">\n            <sha256 value=\"333ff5369043975b7e031b8b27206937441854738e038c1f47f98d072a20437a\" origin=\"official site\"/>\n         </artifact>\n      </component>\n```\n\nIn case of updating a dependency, ensure to remove the unused entry of the outdated dependency manually from the `verification-metadata.xml` file.\n\nYou can also automate the generation of this entry by running your build using the `--write-verification-metadata` commandline option:\n```\n>./gradlew --write-verification-metadata sha256 precommit\n```\n\nThe `--write-verification-metadata` Gradle option is generally able to resolve reachable configurations,\nbut we use detached configurations for a certain set of plugins and tasks. Therefore, please ensure you run this option with a task that\nuses the changed dependencies. In most cases, `precommit` or `check` are good candidates.\n\nWe prefer sha256 checksums as md5 and sha1 are not considered safe anymore these days. The generated entry\nwill have the `origin` attribute been set to `Generated by Gradle`.\n\n>A manual confirmation of the Gradle generated checksums is currently not mandatory.\n>If you want to add a level of verification you can manually confirm the checksum (e.g. by looking it up on the website of the library)\n>Please replace the content of the `origin` attribute by `official site` in that case.\n>\n\n#### Custom plugin and task implementations\n\nBuild logic that is used across multiple subprojects should be considered to be moved into a Gradle plugin with according Gradle task implementation.\nElasticsearch specific build logic is located in the `build-tools-internal` subproject including integration tests.\n\n- Gradle plugins and Tasks should be written in Java\n- We use a groovy and spock for setting up Gradle integration tests.\n  (see https://github.com/elastic/elasticsearch/blob/main/build-tools/src/testFixtures/groovy/org/elasticsearch/gradle/fixtures/AbstractGradleFuncTest.groovy)\n\n#### Declaring tasks\n\nThe Elasticsearch build makes use of the [task avoidance API](https://docs.gradle.org/current/userguide/task_configuration_avoidance.html) to keep the configuration time of the build low.\n\nWhen declaring tasks (in build scripts or custom plugins) this means that we want to _register_ a task like:\n\n    tasks.register('someTask') { ... }\n\ninstead of eagerly _creating_ the task:\n\n    task someTask { ... }\n\nThe major difference between these two syntaxes is, that the configuration block of a registered task will only be executed when the task is actually created due to the build requires that task to run. The configuration block of an eagerly created tasks will be executed immediately.\n\nBy actually doing less in the Gradle configuration time as only creating tasks that are requested as part of the build and by only running the configurations for those requested tasks, using the task avoidance api contributes a major part in keeping our build fast.\n\n#### Registering test clusters\n\nWhen using the Elasticsearch test cluster plugin we want to use (similar to the task avoidance API) a Gradle API to create domain objects lazy or only if required by the build.\nTherefore we register test cluster by using the following syntax:\n\n    def someClusterProvider = testClusters.register('someCluster') { ... }\n\nThis registers a potential testCluster named `somecluster` and provides a provider instance, but doesn't create it yet nor configures it. This makes the Gradle configuration phase more efficient by\ndoing less.\n\nTo wire this registered cluster into a `TestClusterAware` task (e.g. `RestIntegTest`) you can resolve the actual cluster from the provider instance:\n\n    tasks.register('someClusterTest', RestIntegTestTask) {\n        useCluster someClusterProvider\n        nonInputProperties.systemProperty 'tests.leader_host', \"${-> someClusterProvider.get().getAllHttpSocketURI().get(0)}\"\n    }\n\n#### Adding integration tests\n\nAdditional integration tests for a certain Elasticsearch modules that are specific to certain cluster configuration can be declared in a separate so called `qa` subproject of your module.\n\nThe benefit of a dedicated project for these tests are:\n- `qa` projects are dedicated two specific use-cases and easier to maintain\n- It keeps the specific test logic separated from the common test logic.\n- You can run those tests in parallel to other projects of the build.\n\n#### Using test fixtures\n\nSometimes we want to share test fixtures to set up the code under test across multiple projects. There are basically two ways doing so.\n\nIdeally we would use the build-in [java-test-fixtures](https://docs.gradle.org/current/userguide/java_testing.html#sec:java_test_fixtures) Gradle plugin.\nThis plugin relies on having a separate sourceSet for the test fixtures code.\n\nIn the Elasticsearch codebase we have test fixtures and actual tests within the same sourceSet. Therefore we introduced the `elasticsearch.internal-test-artifact` plugin to provides another build artifact of your project based on the `test` sourceSet.\n\n\nThis artifact can be resolved by the consumer project as shown in the example below:\n\n```\ndependencies {\n  //add the test fixtures of `:providing-project` to testImplementation configuration.\n  testImplementation(testArtifact(project(\":fixture-providing-project')))\n}\n```\n\nThis test artifact mechanism makes use of the concept of [component capabilities](https://docs.gradle.org/current/userguide/component_capabilities.html)\nsimilar to how the Gradle build-in `java-test-fixtures` plugin works.\n\n`testArtifact` is a shortcut declared in the Elasticsearch build. Alternatively you can declare the dependency via\n\n```\ndependencies {\n  testImplementation(project(\":fixture-providing-project')) {\n    requireCapabilities(\"org.elasticsearch.gradle:fixture-providing-project-test-artifacts\")\n  }\n}\n```\n\n## FAQ\n\n### How do I test a development version of a third party dependency?\n\nTo test an unreleased development version of a third party dependency you have several options.\n\n#### How to use a Maven based third party dependency via `mavenlocal`?\n\n1. Clone the third party repository locally\n2. Run `mvn install` to install copy into your `~/.m2/repository` folder.\n3. Add this to the root build script:\n\n```\n   allprojects {\n     repositories {\n         mavenLocal()\n     }\n   }\n   ```\n4. Update the version in your dependency declaration accordingly (likely a snapshot version)\n5. Run the Gradle build as needed\n\n#### How to use a Maven built based third party dependency with JitPack repository?\n\nhttps://jitpack.io is an adhoc repository that supports building Maven projects transparently in the background when\nresolving unreleased snapshots from a GitHub repository. This approach also works as temporally solution\nand is compliant with our CI builds.\n\n1. Add the JitPack repository to the root build file:\n```\n   allprojects {\n     repositories {\n        maven { url \"https://jitpack.io\" }\n     }\n   }\n```\n2. Add the dependency in the following format\n```\ndependencies {\n  implementation 'com.github.User:Repo:Tag'\n}\n```\n\nAs version you could also use a certain short commit hash or `main-SNAPSHOT`.\nIn addition to snapshot builds JitPack supports building Pull Requests. Simply use PR<NR>-SNAPSHOT as the version.\n\n3. Run the Gradle build as needed. Keep in mind the initial resolution might take a bit longer as this needs to be built\nby JitPack in the background before we can resolve the adhoc built dependency.\n\n---\n\n**NOTE**\n\nYou should only use that approach locally or on a developer branch for production dependencies as we do\nnot want to ship unreleased libraries into our releases.\n---\n\n#### How to use a custom third party artifact?\n\nFor third party libraries that are not built with Maven (e.g. Ant) or provided as a plain jar artifact we can leverage\na flat directory repository that resolves artifacts from a flat directory on your filesystem.\n\n1. Put the jar artifact with the format `artifactName-version.jar` into a directory named `localRepo` (you have to create this manually)\n2. Declare a flatDir repository in your root build.gradle file (That ensures all projects have the flatDir repository declared and also the projects consuming the project you tweaked can resolve that local dependency)\n\n```\nallprojects {\n  repositories {\n      flatDir {\n          dirs 'localRepo'\n      }\n  }\n}\n```\n\n3. Update the dependency declaration of the artifact in question to match the custom build version. For a file named e.g. `jmxri-1.2.1.jar` the\n  dependency definition would be `x:jmxri:1.2.1` as it the group information is ignored on flatdir repositories you can replace the `x` in group name:\n\n  ```\n  dependencies {\n      implementation 'x:jmxri:1.2.1'\n  }\n  ```\n4. Run the Gradle build as needed with `--write-verification-metadata` to ensure the Gradle dependency verification does not fail on your custom dependency.\n\n---\n**NOTE**\n\nAs Gradle prefers to use modules whose descriptor has been created from real meta-data rather than being generated,\nflat directory repositories cannot be used to override artifacts with real meta-data from other repositories declared in the build.\nFor example, if Gradle finds only `jmxri-1.2.1.jar` in a flat directory repository, but `jmxri-1.2.1.pom` in another repository\nthat supports meta-data, it will use the second repository to provide the module.\nTherefore, it is recommended to declare a version that is not resolvable from public repositories we use (e.g. Maven Central)\n---\n"
        },
        {
          "name": "CHANGELOG.md",
          "type": "blob",
          "size": 0.166015625,
          "content": "# Elasticsearch Changelog\n\nPlease see the [release notes](https://www.elastic.co/guide/en/elasticsearch/reference/current/es-release-notes.html) in the reference manual.\n"
        },
        {
          "name": "CONTRIBUTING.md",
          "type": "blob",
          "size": 47.9013671875,
          "content": "Contributing to Elasticsearch\n=============================\n\nElasticsearch is a free and open project and we love to receive contributions from our community — you! There are many ways to contribute, from writing tutorials or blog posts, improving the documentation, submitting bug reports and feature requests or writing code which can be incorporated into Elasticsearch itself.\n\nIf you want to be rewarded for your contributions, sign up for the [Elastic Contributor Program](https://www.elastic.co/community/contributor). Each time you\nmake a valid contribution, you’ll earn points that increase your chances of winning prizes and being recognized as a top contributor.\n\nBug reports\n-----------\n\nIf you think you have found a bug in Elasticsearch, first make sure that you are testing against the [latest version of Elasticsearch](https://www.elastic.co/downloads/elasticsearch) - your issue may already have been fixed. If not, search our [issues list](https://github.com/elastic/elasticsearch/issues) on GitHub in case a similar issue has already been opened.\n\nIt is very helpful if you can prepare a reproduction of the bug. In other words, provide a small test case which we can run to confirm your bug. It makes it easier to find the problem and to fix it. Test cases should be provided as `curl` commands which we can copy and paste into a terminal to run it locally, for example:\n\n```sh\n# delete the index\ncurl -XDELETE localhost:9200/test\n\n# insert a document\ncurl -XPUT localhost:9200/test/test/1 -d '{\n \"title\": \"test document\"\n}'\n\n# this should return XXXX but instead returns YYY\ncurl ....\n```\n\nProvide as much information as you can. You may think that the problem lies with your query, when actually it depends on how your data is indexed. The easier it is for us to recreate your problem, the faster it is likely to be fixed.\n\nFeature requests\n----------------\n\nIf you find yourself wishing for a feature that doesn't exist in Elasticsearch, you are probably not alone. There are bound to be others out there with similar needs. Many of the features that Elasticsearch has today have been added because our users saw the need.\nOpen an issue on our [issues list](https://github.com/elastic/elasticsearch/issues) on GitHub which describes the feature you would like to see, why you need it, and how it should work.\n\nContributing code and documentation changes\n-------------------------------------------\n\nIf you would like to contribute a new feature or a bug fix to Elasticsearch,\nplease discuss your idea first on the GitHub issue. If there is no GitHub issue\nfor your idea, please open one. It may be that somebody is already working on\nit, or that there are particular complexities that you should know about before\nstarting the implementation. There are often a number of ways to fix a problem\nand it is important to find the right approach before spending time on a PR\nthat cannot be merged.\n\nWe add the `help wanted` label to existing GitHub issues for which community\ncontributions are particularly welcome, and we use the `good first issue` label\nto mark issues that we think will be suitable for new contributors.\n\nThe process for contributing to any of the [Elastic repositories](https://github.com/elastic/) is similar. Details for individual projects can be found below.\n\n### Fork and clone the repository\n\nYou will need to fork the main Elasticsearch code or documentation repository and clone it to your local machine. See\n[GitHub help page](https://help.github.com/articles/fork-a-repo) for help.\n\nFurther instructions for specific projects are given below.\n\n### Tips for code changes\nFollowing these tips prior to raising a pull request will speed up the review\ncycle.\n\n* Add appropriate unit tests (details on writing tests can be found in the\n  [TESTING](TESTING.asciidoc) file)\n* Add integration tests, if applicable\n* Make sure the code you add follows the [formatting guidelines](#java-language-formatting-guidelines)\n* Lines that are not part of your change should not be edited (e.g. don't format\n  unchanged lines, don't reorder existing imports)\n* Add the appropriate [license headers](#license-headers) to any new files\n* For contributions involving the Elasticsearch build you can find details about the build setup in the\n  [BUILDING](BUILDING.md) file\n\n### Submitting your changes\n\nOnce your changes and tests are ready to submit for review:\n\n1. Test your changes\n\n    Run the test suite to make sure that nothing is broken. See the\n[TESTING](TESTING.asciidoc) file for help running tests.\n\n2. Sign the Contributor License Agreement\n\n    Please make sure you have signed our [Contributor License Agreement](https://www.elastic.co/contributor-agreement/). We are not asking you to assign copyright to us, but to give us the right to distribute your code without restriction. We ask this of all contributors in order to assure our users of the origin and continuing existence of the code. You only need to sign the CLA once.\n\n3. Rebase your changes\n\n    Update your local repository with the most recent code from the main Elasticsearch repository, and rebase your branch on top of the latest main branch. We prefer your initial changes to be squashed into a single commit. Later, if we ask you to make changes, add them as separate commits.  This makes them easier to review.  As a final step before merging we will either ask you to squash all commits yourself or we'll do it for you.\n\n4. Submit a pull request\n\n    Push your local changes to your forked copy of the repository and [submit a pull request](https://help.github.com/articles/using-pull-requests). In the pull request, choose a title which sums up the changes that you have made, and in the body provide more details about what your changes do. Also mention the number of the issue where discussion has taken place, eg \"Closes #123\".\n\nThen sit back and wait. There will probably be discussion about the pull request and, if any changes are needed, we would love to work with you to get your pull request merged into Elasticsearch. A yaml changelog entry will be automatically created, there is no need for external contributors to manually edit it, unless requested by the reviewer.\n\nPlease adhere to the general guideline that you should never force push\nto a publicly shared branch. Once you have opened your pull request, you\nshould consider your branch publicly shared. Instead of force pushing\nyou can just add incremental commits; this is generally easier on your\nreviewers. If you need to pick up changes from main, you can merge\nmain into your branch. A reviewer might ask you to rebase a\nlong-running pull request in which case force pushing is okay for that\nrequest. Note that squashing at the end of the review process should\nalso not be done, that can be done when the pull request is [integrated\nvia GitHub](https://github.com/blog/2141-squash-your-commits).\n\nContributing to the Elasticsearch codebase\n------------------------------------------\n\n**Repository:** [https://github.com/elastic/elasticsearch](https://github.com/elastic/elasticsearch)\n\nJDK 21 is required to build Elasticsearch. You must have a JDK 21 installation\nwith the environment variable `JAVA_HOME` referencing the path to Java home for\nyour JDK 21 installation.\n\nElasticsearch uses the Gradle wrapper for its build. You can execute Gradle\nusing the wrapper via the `gradlew` script on Unix systems or `gradlew.bat`\nscript on Windows in the root of the repository. The examples below show the\nusage on Unix.\n\nWe support development in [IntelliJ IDEA] versions 2020.1 and onwards.\n\n[Docker](https://docs.docker.com/install/) is required for building some Elasticsearch artifacts and executing certain test suites. You can run Elasticsearch without building all the artifacts with:\n\n    ./gradlew :run\n\nThat'll spend a while building Elasticsearch and then it'll start Elasticsearch,\nwriting its log above Gradle's status message. We log a lot of stuff on startup,\nspecifically these lines tell you that Elasticsearch is ready:\n\n    [2020-05-29T14:50:35,167][INFO ][o.e.h.AbstractHttpServerTransport] [runTask-0] publish_address {127.0.0.1:9200}, bound_addresses {[::1]:9200}, {127.0.0.1:9200}\n    [2020-05-29T14:50:35,169][INFO ][o.e.n.Node               ] [runTask-0] started\n\nBut to be honest it's typically easier to wait until the console stops scrolling\nand then run `curl` in another window like this:\n\n    curl -u elastic:password localhost:9200\n\nTo send requests to this Elasticsearch instance, either use the built-in `elastic`\nuser and password as above or use the pre-configured `elastic-admin` user:\n\n    curl -u elastic-admin:elastic-password localhost:9200\n\nSecurity can also be disabled altogether:\n\n    ./gradlew :run -Dtests.es.xpack.security.enabled=false\n\nThe definition of this Elasticsearch cluster can be found [here](build-tools-internal/src/main/groovy/elasticsearch.run.gradle).\n\n### Importing the project into IntelliJ IDEA\n\nThe minimum IntelliJ IDEA version required to import the Elasticsearch project is 2020.1.\nElasticsearch builds using Java 21. When importing into IntelliJ you will need\nto define an appropriate SDK. The convention is that **this SDK should be named\n\"21\"** so that the project import will detect it automatically. For more details\non defining an SDK in IntelliJ please refer to [their documentation](https://www.jetbrains.com/help/idea/sdk.html#define-sdk).\nSDK definitions are global, so you can add the JDK from any project, or after\nproject import. Importing with a missing JDK will still work, IntelliJ will\nsimply report a problem and will refuse to build until resolved.\n\nYou can import the Elasticsearch project into IntelliJ IDEA via:\n\n - Select **File > Open**\n - In the subsequent dialog navigate to the root `build.gradle` file\n - In the subsequent dialog select **Open as Project**\n\n#### Checkstyle\n\nIf you have the [Checkstyle] plugin installed, you can configure IntelliJ to\ncheck the Elasticsearch code. However, the Checkstyle configuration file does\nnot work by default with the IntelliJ plugin, so instead an IDE-specific config\nfile is generated automatically after IntelliJ finishes syncing. You can\nmanually generate the file with `./gradlew configureIdeCheckstyle` in case\nit is removed due to a `./gradlew clean` or other action.\n\nIntelliJ should be automatically configured to use the generated rules after\nimport via the `.idea/checkstyle-idea.xml` configuration file. No further\naction is required.\n\n#### Formatting\n\nElasticsearch code is automatically formatted with [Spotless], backed by the\nEclipse formatter. You can do the same in IntelliJ with the\n[Eclipse Code Formatter] so that you can apply the correct formatting directly in\nyour IDE.  The configuration for the plugin is held in\n`.idea/eclipseCodeFormatter.xml` and should be automatically applied, but manual\ninstructions are below in case you need them.\n\n   1. Open **Preferences > Other Settings > Eclipse Code Formatter**\n   2. Click \"Use the Eclipse Code Formatter\"\n   3. Under \"Eclipse formatter config\", select \"Eclipse workspace/project\n      folder or config file\"\n   4. Click \"Browse\", and navigate to the file `build-conventions/formatterConfig.xml`\n   5. **IMPORTANT** - make sure \"Optimize Imports\" is **NOT** selected.\n   6. Click \"OK\"\n   7. Optional: If you like to format code changes on save automatically, open\n      **Preferences > Tools > Actions on Save** and check \"Reformat Code\", making sure to\n      configure Java files.\n\nAlternative manual steps for IntelliJ.\n\n   1. Open **File > Settings/Preferences > Code Style > Java**\n   2. Gear icon > Import Scheme > Eclipse XML Profile\n   3. Navigate to the file `build-conventions/formatterConfig.xml`\n   4. Click \"OK\"\n\n### REST endpoint conventions\n\nElasticsearch typically uses singular nouns rather than plurals in URLs.\nFor example:\n\n    /_ingest/pipeline\n    /_ingest/pipeline/{id}\n\nbut not:\n\n    /_ingest/pipelines\n    /_ingest/pipelines/{id}\n\nYou may find counterexamples, but new endpoints should use the singular\nform.\n\n### Java language formatting guidelines\n\nJava files in the Elasticsearch codebase are automatically formatted using\nthe [Spotless Gradle] plugin. All new projects are automatically formatted,\nwhile existing projects are gradually being opted-in. The formatting check\nis run automatically via the `precommit` task, but it can be run explicitly with:\n\n    ./gradlew spotlessJavaCheck\n\nIt is usually more useful, and just as fast, to just reformat the project. You\ncan do this with:\n\n    ./gradlew spotlessApply\n\nThese tasks can also be run for specific subprojects, e.g.\n\n    ./gradlew server:spotlessJavaCheck\n\nPlease follow these formatting guidelines:\n\n* Java indent is 4 spaces\n* Line width is 140 characters\n* Lines of code surrounded by `// tag::NAME` and `// end::NAME` comments are included\n  in the documentation and should only be 76 characters wide not counting\n  leading indentation. Such regions of code are not formatted automatically as\n  it is not possible to change the line length rule of the formatter for\n  part of a file. Please format such sections sympathetically with the rest\n  of the code, while keeping lines to maximum length of 76 characters.\n* Wildcard imports (`import foo.bar.baz.*`) are forbidden and will cause\n  the build to fail.\n* If *absolutely* necessary, you can disable formatting for regions of code\n  with the `// tag::noformat` and `// end::noformat` directives, but\n  only do this where the benefit clearly outweighs the decrease in formatting\n  consistency.\n* Note that Javadoc and block comments i.e. `/* ... */` are not formatted,\n  but line comments i.e. `// ...` are.\n* Negative boolean expressions must use the form `foo == false` instead of\n  `!foo` for better readability of the code. This is enforced via\n  Checkstyle. Conversely, you should not write e.g. `if (foo == true)`, but\n  just `if (foo)`.\n\n#### Editor / IDE support\n\nIntelliJ IDEs can\n[import](https://blog.jetbrains.com/idea/2014/01/intellij-idea-13-importing-code-formatter-settings-from-eclipse/)\nthe same settings file, and / or use the [Eclipse Code Formatter] plugin.\n\nYou can also tell Spotless to [format a specific\nfile](https://github.com/diffplug/spotless/tree/main/plugin-gradle#can-i-apply-spotless-to-specific-files)\nfrom the command line.\n\n### Javadoc\n\nGood Javadoc can help with navigating and understanding code. Elasticsearch\nhas some guidelines around when to write Javadoc and when not to, but note\nthat we don't want to be overly prescriptive. The intent of these guidelines\nis to be helpful, not to turn writing code into a chore.\n\n#### The short version\n\n   1. Always add Javadoc to new code.\n   2. Add Javadoc to existing code if you can.\n   3. Document the \"why\", not the \"how\", unless that's important to the\n      \"why\".\n   4. Don't document anything trivial or obvious (e.g. getters and\n      setters). In other words, the Javadoc should add some value.\n\n#### The long version\n\n   1. If you add a new Java package, please also add package-level\n      Javadoc that explains what the package is for. This can just be a\n      reference to a more foundational / parent package if appropriate. An\n      example would be a package hierarchy for a new feature or plugin -\n      the package docs could explain the purpose of the feature, any\n      caveats, and possibly some examples of configuration and usage.\n   2. New classes and interfaces must have class-level Javadoc that\n      describes their purpose. There are a lot of classes in the\n      Elasticsearch repository, and it's easier to navigate when you\n      can quickly find out what is the purpose of a class. This doesn't\n      apply to inner classes or interfaces, unless you expect them to be\n      explicitly used outside their parent class.\n   3. New public methods must have Javadoc, because they form part of the\n      contract between the class and its consumers. Similarly, new abstract\n      methods must have Javadoc because they are part of the contract\n      between a class and its subclasses. It's important that contributors\n      know why they need to implement a method, and the Javadoc should make\n      this clear. You don't need to document a method if it's overriding an\n      abstract method (either from an abstract superclass or an interface),\n      unless your implementation is doing something \"unexpected\" e.g. deviating\n      from the intent of the original method.\n   4. Following on from the above point, please add docs to existing public\n      methods if you are editing them, or to abstract methods if you can.\n   5. Non-public, non-abstract methods don't require Javadoc, but if you feel\n      that adding some would make it easier for other developers to\n      understand the code, or why it's written in a particular way, then please\n      do so.\n   6. Properties don't need to have Javadoc, but please add some if there's\n      something useful to say.\n   7. Javadoc should not go into low-level implementation details unless\n      this is critical to understanding the code e.g. documenting the\n      subtleties of the implementation of a private method. The point here\n      is that implementations will change over time, and the Javadoc is\n      less likely to become out-of-date if it only talks about\n      the purpose of the code, not what it does.\n   8. Examples in Javadoc can be very useful, so feel free to add some if\n      you can reasonably do so i.e. if it takes a whole page of code to set\n      up an example, then Javadoc probably isn't the right place for it.\n      Longer or more elaborate examples are probably better suited\n      to the package docs.\n   9. Test methods are a good place to add Javadoc, because you can use it\n      to succinctly describe e.g. preconditions, actions and expectations\n      of the test, more easily that just using the test name alone. Please\n      consider documenting your tests in this way.\n   10. Sometimes you shouldn't add Javadoc:\n       1. Where it adds no value, for example where a method's\n          implementation is trivial such as with getters and setters, or a\n          method just delegates to another object.\n       2. However, you should still add Javadoc if there are caveats around\n          calling a method that are not immediately obvious from reading the\n          method's implementation in isolation.\n       3. You can omit Javadoc for simple classes, e.g. where they are a\n          simple container for some data. However, please consider whether a\n          reader might still benefit from some additional background, for\n          example about why the class exists at all.\n   11. Not all comments need to be Javadoc. Sometimes it will make more\n       sense to add comments in a method's body, for example due to important\n       implementation decisions or \"gotchas\". As a general guide, if some\n       information forms part of the contract between a method and its callers,\n       then it should go in the Javadoc, otherwise you might consider using\n       regular comments in the code. Remember as well that Elasticsearch\n       has extensive [user documentation](./docs), and it is not the role\n       of Javadoc to replace that.\n        * If a method's performance is \"unexpected\" then it's good to call that\n           out in the Javadoc. This is especially helpful if the method is usually fast but sometimes\n           very slow (shakes fist at caching).\n   12. Please still try to make class, method or variable names as\n       descriptive and concise as possible, as opposed to relying solely on\n       Javadoc to describe something.\n   13. Use `@link` to add references to related resources in the codebase. Or\n       outside the code base.\n       1. `@see` is much more limited than `@link`. You can use it but most of\n          the time `@link` flows better.\n   14. If you need help writing Javadoc, just ask!\n\nFinally, use your judgement! Base your decisions on what will help other\ndevelopers - including yourself, when you come back to some code\n3 months in the future, having forgotten how it works.\n\n### License headers\n\nWe require license headers on all Java files. With the exception of the\ntop-level `x-pack` directory, all contributed code should have the following\nlicense header unless instructed otherwise:\n\n    /*\n     * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n     * or more contributor license agreements. Licensed under the \"Elastic License\n     * 2.0\", the \"GNU Affero General Public License v3.0 only\", and the \"Server Side\n     * Public License v 1\"; you may not use this file except in compliance with, at\n     * your election, the \"Elastic License 2.0\", the \"GNU Affero General Public\n     * License v3.0 only\", or the \"Server Side Public License, v 1\".\n     */\n\nThe top-level `x-pack` directory contains code covered by the [Elastic\nlicense](licenses/ELASTIC-LICENSE-2.0.txt). Community contributions to this code are\nwelcome, and should have the following license header unless instructed\notherwise:\n\n    /*\n     * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n     * or more contributor license agreements. Licensed under the Elastic License\n     * 2.0; you may not use this file except in compliance with the Elastic License\n     * 2.0.\n     */\n\nIt is important that the only code covered by the Elastic licence is contained\nwithin the top-level `x-pack` directory. The build will fail its pre-commit\nchecks if contributed code does not have the appropriate license headers.\n\n> **NOTE:** If you have imported the project into IntelliJ IDEA the project will\n> be automatically configured to add the correct license header to new source\n> files based on the source location.\n\n### Type-checking, generics and casting\n\nYou should try to write code that does not require suppressing any warnings from\nthe compiler, e.g. suppressing type-checking, raw generics, and so on. However,\nthis isn't always possible or practical. In such cases, you should use the\n`@SuppressWarnings` annotations to silence the compiler warning, trying to keep\nthe scope of the suppression as small as possible. Where a piece of code\nrequires a lot of suppressions, it may be better to apply a single suppression\nat a higher level e.g. at the method or even class level. Use your judgement.\n\nThere are also cases where the compiler simply refuses to accept an assignment\nor cast of any kind, because it lacks the information to know that the types are\nOK. In such cases, you can use\nthe [`Types.forciblyCast`](libs/core/src/main/java/org/elasticsearch/core/Types.java)\nutility method. As the name suggests, you can coerce any type to any other type,\nso please use it as a last resort.\n\n### Logging\n\nThe Elasticsearch server logs are vitally useful for diagnosing problems in a\nrunning cluster. You should make sure that your contribution uses logging\nappropriately: log enough detail to inform users about key events and help them\nunderstand what happened when things go wrong without logging so much detail\nthat the logs fill up with noise and the useful signal is lost.\n\nElasticsearch uses Log4J for logging. In most cases you should log via a\n`Logger` named after the class that is writing the log messages, which you can\ndo by declaring a static field of the class. For example:\n\n    class Foo {\n        private static final Logger logger = LogManager.getLogger(Foo.class);\n    }\n\nIn rare situations you may want to configure your `Logger` slightly\ndifferently, perhaps specifying a different class or maybe using one of the\nmethods on `org.elasticsearch.common.logging.Loggers` instead.\n\nIf the log message includes values from your code then you must use\nplaceholders rather than constructing the string yourself using simple\nconcatenation. Consider wrapping the values in `[...]` to help distinguish them\nfrom the static part of the message:\n\n    logger.debug(\"operation failed [{}] times in [{}]ms\", failureCount, elapsedMillis);\n\nYou can also pass in an exception to log it including its stack trace, and any\ncauses and their causes, as well as any suppressed exceptions and so on:\n\n    logger.debug(\"operation failed\", exception);\n\nIf you wish to use placeholders and an exception at the same time, construct a\n`Supplier<String>` and use `org.elasticsearch.core.Strings.format`\n- note java.util.Formatter syntax\n\n    logger.debug(() -> Strings.format(\"failed at offset [%s]\", offset), exception);\n\nYou can also use a `java.util.Supplier<String>` to avoid constructing\nexpensive messages that will usually be discarded:\n\n    logger.debug(() -> \"rarely seen output [\" + expensiveMethod() + \"]\");\n\nLogging is an important behaviour of the system and sometimes deserves its own\nunit tests, especially if there is complex logic for computing what is logged\nand when to log it. You can use a `org.elasticsearch.test.MockLogAppender` to\nmake assertions about the logs that are being emitted.\n\nLogging is a powerful diagnostic technique, but it is not the only possibility.\nYou should also consider exposing some information about your component via an\nAPI instead of in logs. For instance, you can implement APIs to report its\ncurrent status, various statistics, and maybe even details of recent failures.\n\n#### Log levels\n\nEach log message is written at a particular _level_. By default, Elasticsearch\nwill suppress messages at the two most verbose levels, `TRACE` and `DEBUG`, and\nwill output messages at all other levels. Users can configure which levels of\nmessage are written by each logger at runtime, but you should expect everyone\nto run with the default configuration almost all the time and choose your\nlevels accordingly.\n\nThe guidance in this section is subjective in some areas. When in doubt,\ndiscuss your choices with reviewers.\n\n##### `TRACE`\n\nThis is the most verbose level, disabled by default, and it is acceptable if it\ngenerates a very high volume of logs. The target audience of `TRACE` logs\ncomprises developers who are trying to deeply understand some unusual runtime\nbehaviour of a system. For instance `TRACE` logs may be useful when\nunderstanding an unexpected interleaving of concurrent actions or some\nunexpected consequences of a delayed response from a remote node.\n\n`TRACE` logs will normally only make sense when read alongside the code, and\ntypically they will be read as a whole sequence of messages rather than in\nisolation. For example, the `InternalClusterInfoService` uses `TRACE` logs to\nrecord certain key events in its periodic refresh process:\n\n    logger.trace(\"starting async refresh\");\n    // ...\n    logger.trace(\"received node stats response\");\n    // ...\n    logger.trace(\"received indices stats response\");\n    // ...\n    logger.trace(\"stats all received, computing cluster info and notifying listeners\");\n    // ...\n    logger.trace(\"notifying [{}] of new cluster info\", listener);\n\nEven though `TRACE` logs may be very verbose, you should still exercise some\njudgement when deciding when to use them. In many cases it will be easier to\nunderstand the behaviour of the system using tests or by analysing the code\nitself rather than by trawling through hundreds of trivial log messages.\n\nIt may not be easy, or even possible, to obtain `TRACE` logs from a production\nsystem. Therefore they are not appropriate for information that you would\nnormally expect to be useful in diagnosing problems in production.\n\n##### `DEBUG`\n\nThis is the next least verbose level and is also disabled by default. The\ntarget audience of this level typically comprises users or developers who are\ntrying to diagnose an unexpected problem in a production system, perhaps to\nhelp determine whether a fault lies within Elasticsearch or elsewhere.\n\nUsers should expect to be able to enable `DEBUG` logging on their production\nsystems for a whole subsystem for an extended period of time without\noverwhelming the system or filling up their disks with logs, so it is important\nto limit the volume of messages logged at this level. On the other hand, these\nmessages must still provide enough detail to diagnose the sorts of problems\nthat you expect Elasticsearch to encounter. In some cases it works well to\ncollect information over a period of time and then log a complete summary,\nrather than recording every step of a process in its own message.\n\nFor example, the `Coordinator` uses `DEBUG` logs to record a change in mode,\nincluding various internal details for context, because this event is fairly\nrare but not important enough to notify users by default:\n\n    logger.debug(\n        \"{}: coordinator becoming CANDIDATE in term {} (was {}, lastKnownLeader was [{}])\",\n        method,\n        getCurrentTerm(),\n        mode,\n        lastKnownLeader\n    );\n\nIt's possible that the reader of `DEBUG` logs is also reading the code, but\nthat is less likely than for `TRACE` logs. Strive to avoid terminology that\nonly makes sense when reading the code, and also aim for messages at this level\nto be self-contained rather than intending them to be read as a sequence.\n\nIt's often useful to log exceptions and other deviations from the \"happy path\"\nat `DEBUG` level. Exceptions logged at `DEBUG` should generally include the\ncomplete stack trace.\n\n##### `INFO`\n\nThis is the next least verbose level, and the first level that is enabled by\ndefault. It is appropriate for recording important events in the life of the\ncluster, such as an index being created or deleted or a snapshot starting or\ncompleting. Users will mostly ignore log messages at `INFO` level, but may use\nthese messages to construct a high-level timeline of events leading up to an\nincident.\n\nFor example, the `MetadataIndexTemplateService` uses `INFO` logs to record when\nan index template is created or updated:\n\n    logger.info(\n        \"{} index template [{}] for index patterns {}\",\n        existing == null ? \"adding\" : \"updating\",\n        name,\n        template.indexPatterns()\n    );\n\n`INFO`-level logging is enabled by default so its target audience is the\ngeneral population of users and administrators. You should use user-facing\nterminology and ensure that messages at this level are self-contained. In\ngeneral, you shouldn't log unusual events, particularly exceptions with stack\ntraces, at `INFO` level. If the event is relatively benign then use `DEBUG`,\nwhereas if the user should be notified then use `WARN`.\n\nBear in mind that users will be reading the logs when they're trying to\ndetermine why their node is not behaving the way they expect. If a log message\nsounds like an error then some users will interpret it as one, even if it is\nlogged at `INFO` level. Where possible, `INFO` messages should prefer factual\nover judgemental language, for instance saying `Did not find ...` rather than\n`Failed to find ...`.\n\n##### `WARN`\n\nThis is the next least verbose level, and is also enabled by default. Ideally a\nhealthy cluster will emit no `WARN`-level logs, but this is the appropriate\nlevel for recording events that the cluster administrator should investigate,\nor which indicate a bug. Some production environments require the cluster to\nemit no `WARN`-level logs during acceptance testing, so you must ensure that\nany logs at this level really do indicate a problem that needs addressing.\n\nAs with the `INFO` level, you should use user-facing terminology at the `WARN`\nlevel, and also ensure that messages are self-contained. Strive to make them\nactionable too since you should be logging at this level when the user should\ntake some investigative action.\n\nFor example, the `DiskThresholdMonitor` uses `WARN` logs to record that a disk\nthreshold has been breached:\n\n    logger.warn(\n        \"flood stage disk watermark [{}] exceeded on {}, all indices on this node will be marked read-only\",\n        diskThresholdSettings.describeFloodStageThreshold(total, false),\n        usage\n    );\n\nUnlike at the `INFO` level, it is often appropriate to log an exception,\ncomplete with stack trace, at `WARN` level. Although the stack trace may not be\nuseful to the user, it may contain information that is vital for a developer to\nfully understand the problem and its wider context.\n\nIn a situation where occasional transient failures are expected and handled,\nbut a persistent failure requires the user's attention, consider implementing a\nmechanism to detect that a failure is unacceptably persistent and emit a\ncorresponding `WARN` log. For example, it may be helpful to log every tenth\nconsecutive failure at `WARN` level, or log at `WARN` if an operation has not\ncompleted within a certain time limit. This is much more user-friendly than\nfailing persistently and silently by default and requiring the user to enable\n`DEBUG` logging to investigate the problem.\n\nIf an exception occurs as a direct result of a request received from a client\nthen it should only be logged as a `WARN` if the server administrator is the\nright person to address it. In most cases the server administrator cannot do\nanything about faulty client requests, and the person running the client is\noften unable to see the server logs, so you should include the exception in the\nresponse back to the client and not log a warning. Bear in mind that clients\nmay submit requests at a high rate, so any per-request logging can easily flood\nthe logs.\n\n##### `ERROR`\n\nThis is the next least verbose level after `WARN`. In theory, it is possible for\nusers to suppress messages at `WARN` and below, believing this to help them\nfocus on the most important `ERROR` messages, but in practice in Elasticsearch\nthis will hide so much useful information that the resulting logs will be\nuseless, so we do not expect users to do this kind of filtering.\n\nOn the other hand, users may be familiar with the `ERROR` level from elsewhere.\nLog4J for instance documents this level as meaning \"an error in the\napplication, possibly recoverable\". The implication here is that the error is\npossibly _not_ recoverable too, and we do encounter users that get very worried\nby logs at `ERROR` level for this reason.\n\nTherefore you should try and avoid logging at `ERROR` level unless the error\nreally does indicate that Elasticsearch is now running in a degraded state from\nwhich it will not recover. For instance, the `FsHealthService` uses `ERROR`\nlogs to record that the data path failed some basic health checks and hence the\nnode cannot continue to operate as a member of the cluster:\n\n    logger.error(() -> \"health check of [\" + path + \"] failed\", ex);\n\nErrors like this should be very rare. When in doubt, prefer `WARN` to `ERROR`.\n\n### Versioning Elasticsearch\n\nThere are various concepts used to identify running node versions,\nand the capabilities and compatibility of those nodes. For more information,\nsee `docs/internal/Versioning.md`\n\n### Creating a distribution\n\nRun all build commands from within the root directory:\n\n    cd elasticsearch/\n\nTo build a darwin-tar distribution, run this command:\n\n    ./gradlew -p distribution/archives/darwin-tar assemble\n\nYou will find the distribution under:\n\n    ./distribution/archives/darwin-tar/build/distributions/\n\nTo create all build artifacts (e.g., plugins and Javadocs) as well as\ndistributions in all formats, run this command:\n\n    ./gradlew assemble\n\n> **NOTE:** Running the task above will fail if you don't have an available\n> Docker installation.\n\nThe package distributions (Debian and RPM) can be found under:\n\n    ./distribution/packages/(deb|rpm|oss-deb|oss-rpm)/build/distributions/\n\nThe archive distributions (tar and zip) can be found under:\n\n    ./distribution/archives/(darwin-tar|linux-tar|windows-zip|oss-darwin-tar|oss-linux-tar|oss-windows-zip)/build/distributions/\n\n### Running the full test suite\n\nBefore submitting your changes, run the test suite to make sure that nothing is broken, with:\n\n    ./gradlew check\n\nIf your changes affect only the documentation, run:\n\n    ./gradlew -p docs check\n\nFor more information about testing code examples in the documentation, see\nhttps://github.com/elastic/elasticsearch/blob/main/docs/README.asciidoc\n\n### Only running failed tests\n\nWhen you open your pull-request it may be approved for review. If so, the full\ntest suite is run within Elasticsearch's CI environment. If a test fails,\nyou can see how to run that particular test by searching for the `REPRODUCE`\nstring in the CI's console output.\n\nElasticsearch's testing suite takes advantage of randomized testing. Consequently,\na test that passes locally, may actually fail later due to random settings\nor data input. To make tests repeatable, a `REPRODUCE` line in CI will also include\nthe `-Dtests.seed` parameter.\n\nWhen running locally, Gradle does its best to take advantage of cached results.\nSo, if the code is unchanged, running the same test with the same `-Dtests.seed`\nrepeatedly may not actually run the test if it has passed with that seed\n in the previous execution. A way around this is to pass a separate parameter\nto adjust the command options seen by Gradle.\nA simple option may be to add the parameter `-Dtests.timestamp=$(date +%s)`\nwhich will give the current time stamp as a parameter, thus making the parameters\nsent to Gradle unique and bypassing the cache.\n\n### Project layout\n\nThis repository is split into many top level directories. The most important\nones are:\n\n#### `docs`\nDocumentation for the project.\n\n#### `distribution`\nBuilds our tar and zip archives and our rpm and deb packages.\n\n#### `libs`\nLibraries used to build other parts of the project. These are meant to be\ninternal rather than general purpose. We have no plans to\n[semver](https://semver.org/) their APIs or accept feature requests for them.\nWe publish them to Maven Central because they are dependencies of our plugin\ntest framework, high level rest client, and jdbc driver, but they really aren't\ngeneral purpose enough to *belong* in Maven Central. We're still working out\nwhat to do here.\n\n#### `modules`\nFeatures that are shipped with Elasticsearch by default but are not built in to\nthe server. We typically separate features from the server because they require\npermissions that we don't believe *all* of Elasticsearch should have or because\nthey depend on libraries that we don't believe *all* of Elasticsearch should\ndepend on.\n\nFor example, reindex requires the `connect` permission so it can perform\nreindex-from-remote, but we don't believe that the *all* of Elasticsearch should\nhave the \"connect\". For another example, Painless is implemented using antlr4\nand asm and we don't believe that *all* of Elasticsearch should have access to\nthem.\n\n#### `plugins`\nOfficially supported plugins to Elasticsearch. We decide that a feature should\nbe a plugin rather than shipped as a module because we feel that it is only\nimportant to a subset of users, especially if it requires extra dependencies.\n\nThe canonical example of this is the ICU analysis plugin. It is important for\nfolks who want the fairly language neutral ICU analyzer but the library to\nimplement the analyzer is 11MB so we don't ship it with Elasticsearch by\ndefault.\n\nAnother example is the `discovery-gce` plugin. It is *vital* to folks running\nin [GCP](https://cloud.google.com/) but useless otherwise and it depends on a\ndozen extra jars.\n\n#### `qa`\nHonestly this is kind of in flux and we're not 100% sure where we'll end up.\nRight now the directory contains\n* Tests that require multiple modules or plugins to work\n* Tests that form a cluster made up of multiple versions of Elasticsearch like\nfull cluster restart, rolling restarts, and mixed version tests\n* Tests that test the Elasticsearch clients in \"interesting\" places like the\n`wildfly` project.\n* Tests that test Elasticsearch in funny configurations like with ingest\ndisabled\n* Tests that need to do strange things like install plugins that thrown\nuncaught `Throwable`s or add a shutdown hook\nBut we're not convinced that all of these things *belong* in the qa directory.\nWe're fairly sure that tests that require multiple modules or plugins to work\nshould just pick a \"home\" plugin. We're fairly sure that the multi-version\ntests *do* belong in qa. Beyond that, we're not sure. If you want to add a new\nqa project, open a PR and be ready to discuss options.\n\n#### `server`\nThe server component of Elasticsearch that contains all of the modules and\nplugins. Right now things like the high level rest client depend on the server,\nbut we'd like to fix that in the future.\n\n#### `test`\nOur test framework and test fixtures. We use the test framework for testing the\nserver, the plugins, and modules, and pretty much everything else. We publish\nthe test framework so folks who develop Elasticsearch plugins can use it to\ntest the plugins. The test fixtures are external processes that we start before\nrunning specific tests that rely on them.\n\nFor example, we have an hdfs test that uses mini-hdfs to test our\nrepository-hdfs plugin.\n\n#### `x-pack`\nCommercially licensed code that integrates with the rest of Elasticsearch. The\n`docs` subdirectory functions just like the top level `docs` subdirectory and\nthe `qa` subdirectory functions just like the top level `qa` subdirectory. The\n`plugin` subdirectory contains the x-pack module which runs inside the\nElasticsearch process.\n\n### Gradle build\n\nWe use Gradle to build Elasticsearch because it is flexible enough to not only\nbuild and package Elasticsearch, but also orchestrate all of the ways that we\nhave to test Elasticsearch.\n\n#### Configurations\n\nGradle organizes dependencies and build artifacts into \"configurations\" and\nallows you to use these configurations arbitrarily. Here are some of the most\ncommon configurations in our build and how we use them:\n\n<dl>\n<dt>`implementation`</dt><dd>Dependencies that are used by the project\nat compile and runtime but are not exposed as a compile dependency to other dependent projects.\nDependencies added to the `implementation` configuration are considered an implementation detail\nthat can be changed at a later date without affecting any dependent projects.</dd>\n\n<dt>`api`</dt><dd>Dependencies that are used as compile and runtime dependencies of a project\n and are considered part of the external api of the project.</dd>\n\n<dt>`runtimeOnly`</dt><dd>Dependencies that not on the classpath at compile time but\nare on the classpath at runtime. We mostly use this configuration to make sure that\nwe do not accidentally compile against dependencies of our dependencies also\nknown as \"transitive\" dependencies\".</dd>\n\n<dt>`compileOnly`</dt><dd>Code that is on the classpath at compile time but that\nshould not be shipped with the project because it is \"provided\" by the runtime\nsomehow. Elasticsearch plugins use this configuration to include dependencies\nthat are bundled with Elasticsearch's server.</dd>\n\n<dt>`testImplementation`</dt><dd>Code that is on the classpath for compiling tests\nthat are part of this project but not production code. The canonical example\nof this is `junit`.</dd>\n</dl>\n\n\nReviewing and accepting your contribution\n-----------------------------------------\n\nWe review every contribution carefully to ensure that the change is of high\nquality and fits well with the rest of the Elasticsearch codebase. If accepted,\nwe will merge your change and usually take care of backporting it to\nappropriate branches ourselves.\n\nWe really appreciate everyone who is interested in contributing to\nElasticsearch and regret that we sometimes have to reject contributions even\nwhen they might appear to make genuine improvements to the system. Reviewing\ncontributions can be a very time-consuming task, yet the team is small and our\ntime is very limited. In some cases the time we would need to spend on reviews\nwould outweigh the benefits of a change by preventing us from working on other\nmore beneficial changes instead.\n\nPlease discuss your change in a GitHub issue before spending much time on its\nimplementation. We sometimes have to reject contributions that duplicate other\nefforts, take the wrong approach to solving a problem, or solve a problem which\ndoes not need solving. An up-front discussion often saves a good deal of wasted\ntime in these cases.\n\nWe normally immediately reject isolated PRs that only perform simple\nrefactorings or otherwise \"tidy up\" certain aspects of the code. We think the\nbenefits of this kind of change are very small, and in our experience it is not\nworth investing the substantial effort needed to review them. This especially\nincludes changes suggested by tools.\n\nWe normally immediately reject PRs which target platforms or system\nconfigurations that are not in the [official support\nmatrix](https://www.elastic.co/support/matrix). We choose to support particular\nplatforms with care because we must work to ensure that every Elasticsearch\nrelease works completely on every platform, and we must spend time\ninvestigating test failures and performance regressions there too. We cannot\ndetermine whether PRs which target unsupported platforms or configurations meet\nour quality standards, nor can we guarantee that the change they introduce will\ncontinue to work in future releases. We do not want Elasticsearch to suddenly\nstop working on a particular platform after an upgrade.\n\nWe sometimes reject contributions due to the low quality of the submission\nsince low-quality submissions tend to take unreasonable effort to review\nproperly. Quality is rather subjective so it is hard to describe exactly how to\navoid this, but there are some basic steps you can take to reduce the chances\nof rejection. Follow the guidelines listed above when preparing your changes.\nYou should add tests that correspond with your changes, and your PR should pass\naffected test suites too. It makes it much easier to review if your code is\nformatted correctly and does not include unnecessary extra changes.\n\nWe sometimes reject contributions if we find ourselves performing many review\niterations without making enough progress. Some iteration is expected,\nparticularly on technically complicated changes, and there's no fixed limit on\nthe acceptable number of review cycles since it depends so much on the nature\nof the change. You can help to reduce the number of iterations by reviewing\nyour contribution yourself or in your own team before asking us for a review.\nYou may be surprised how many comments you can anticipate and address by taking\na short break and then carefully looking over your changes again.\n\nWe expect you to follow up on review comments somewhat promptly, but recognise\nthat everyone has many priorities for their time and may not be able to respond\nfor several days. We will understand if you find yourself without the time to\ncomplete your contribution, but please let us know that you have stopped\nworking on it. We will try to send you a reminder if we haven't heard from you\nin a while, but may end up closing your PR if you do not respond for too long.\n\nIf your contribution is rejected we will close the pull request with a comment\nexplaining why. This decision isn't always final: if you feel we have\nmisunderstood your intended change or otherwise think that we should reconsider\nthen please continue the conversation with a comment on the pull request and\nwe'll do our best to address any further points you raise.\n\nContributing as part of a class\n-------------------------------\nIn general Elasticsearch is happy to accept contributions that were created as\npart of a class but strongly advise against making the contribution as part of\nthe class. So if you have code you wrote for a class feel free to submit it.\n\nPlease, please, please do not assign contributing to Elasticsearch as part of a\nclass. If you really want to assign writing code for Elasticsearch as an\nassignment then the code contributions should be made to your private clone and\nopening PRs against the primary Elasticsearch clone must be optional, fully\nvoluntary, not for a grade, and without any deadlines.\n\nBecause:\n\n* While the code review process is likely very educational, it can take wildly\nvarying amounts of time depending on who is available, where the change is, and\nhow deep the change is. There is no way to predict how long it will take unless\nwe rush.\n* We do not rush reviews without a very, very good reason. Class deadlines\naren't a good enough reason for us to rush reviews.\n* We deeply discourage opening a PR you don't intend to work through the entire\ncode review process because it wastes our time.\n* We don't have the capacity to absorb an entire class full of new contributors,\nespecially when they are unlikely to become long time contributors.\n\nFinally, we require that you run `./gradlew check` before submitting a\nnon-documentation contribution. This is mentioned above, but it is worth\nrepeating in this section because it has come up in this context.\n\n[IntelliJ IDEA]: https://www.jetbrains.com/idea/\n[Checkstyle]: https://plugins.jetbrains.com/plugin/1065-checkstyle-idea\n[Spotless]: https://github.com/diffplug/spotless\n[Eclipse Code Formatter]: https://plugins.jetbrains.com/plugin/6546-eclipse-code-formatter\n[Spotless Gradle]: https://github.com/diffplug/spotless/tree/main/plugin-gradle\n"
        },
        {
          "name": "LICENSE.txt",
          "type": "blob",
          "size": 0.6533203125,
          "content": "Source code in this repository is covered by (i) a triple license under the \"GNU\nAffero General Public License v3.0 only\", \"the Server Side Public License, v 1\",\nand the \"Elastic License 2.0\", or (ii) an \"Apache License 2.0\" compatible\nlicense or (iii) solely under the \"Elastic License 2.0\", in each case, as noted\nin the applicable header. The default throughout the repository is a triple\nlicense under the \"GNU Affero General Public License v3.0 only\", \"the Server\nSide Public License, v 1\", and the \"Elastic License 2.0\", unless the header\nspecifies another license. Code that is licensed solely under the \"Elastic\nLicense 2.0\" is found only in the x-pack folder.\n"
        },
        {
          "name": "NOTICE.txt",
          "type": "blob",
          "size": 0.146484375,
          "content": "Elasticsearch\nCopyright 2009-2024 Elasticsearch\n\nThis product includes software developed by The Apache Software\nFoundation (http://www.apache.org/).\n"
        },
        {
          "name": "README.asciidoc",
          "type": "blob",
          "size": 9.7998046875,
          "content": "= Elasticsearch\n\nElasticsearch is a distributed search and analytics engine, scalable data store and vector database optimized for speed and relevance on production-scale workloads. Elasticsearch is the foundation of Elastic's open Stack platform. Search in near real-time over massive datasets, perform vector searches, integrate with generative AI applications, and much more.\n\nUse cases enabled by Elasticsearch include:\n\n* https://www.elastic.co/search-labs/blog/articles/retrieval-augmented-generation-rag[Retrieval Augmented Generation (RAG)]\n* https://www.elastic.co/search-labs/blog/categories/vector-search[Vector search]\n* Full-text search\n* Logs\n* Metrics\n* Application performance monitoring (APM)\n* Security logs\n\n\\... and more!\n\nTo learn more about Elasticsearch's features and capabilities, see our\nhttps://www.elastic.co/products/elasticsearch[product page].\n\nTo access information on https://www.elastic.co/search-labs/blog/categories/ml-research[machine learning innovations] and the latest https://www.elastic.co/search-labs/blog/categories/lucene[Lucene contributions from Elastic], more information can be found in https://www.elastic.co/search-labs[Search Labs].\n\n[[get-started]]\n== Get started\n\nThe simplest way to set up Elasticsearch is to create a managed deployment with\nhttps://www.elastic.co/cloud/as-a-service[Elasticsearch Service on Elastic\nCloud].\n\nIf you prefer to install and manage Elasticsearch yourself, you can download\nthe latest version from\nhttps://www.elastic.co/downloads/elasticsearch[elastic.co/downloads/elasticsearch].\n\n=== Run Elasticsearch locally\n\n////\nIMPORTANT: This content is replicated in the Elasticsearch repo. See `run-elasticsearch-locally.asciidoc`.\nEnsure both files are in sync.\n\nhttps://github.com/elastic/start-local is the source of truth.\n////\n\n[WARNING]\n====\nDO NOT USE THESE INSTRUCTIONS FOR PRODUCTION DEPLOYMENTS.\n\nThis setup is intended for local development and testing only.\n====\n\nQuickly set up Elasticsearch and Kibana in Docker for local development or testing, using the https://github.com/elastic/start-local?tab=readme-ov-file#-try-elasticsearch-and-kibana-locally[`start-local` script].\n\nℹ️ For more detailed information about the `start-local` setup, refer to the https://github.com/elastic/start-local[README on GitHub].\n\n==== Prerequisites\n\n- If you don't have Docker installed, https://www.docker.com/products/docker-desktop[download and install Docker Desktop] for your operating system.\n- If you're using Microsoft Windows, then install https://learn.microsoft.com/en-us/windows/wsl/install[Windows Subsystem for Linux (WSL)].\n\n==== Trial license\nThis setup comes with a one-month trial license that includes all Elastic features.\n\nAfter the trial period, the license reverts to *Free and open - Basic*.\nRefer to https://www.elastic.co/subscriptions[Elastic subscriptions] for more information.\n\n==== Run `start-local`\n\nTo set up Elasticsearch and Kibana locally, run the `start-local` script:\n\n[source,sh]\n----\ncurl -fsSL https://elastic.co/start-local | sh\n----\n// NOTCONSOLE\n\nThis script creates an `elastic-start-local` folder containing configuration files and starts both Elasticsearch and Kibana using Docker.\n\nAfter running the script, you can access Elastic services at the following endpoints:\n\n* *Elasticsearch*: http://localhost:9200\n* *Kibana*: http://localhost:5601\n\nThe script generates a random password for the `elastic` user, which is displayed at the end of the installation and stored in the `.env` file.\n\n[CAUTION]\n====\nThis setup is for local testing only. HTTPS is disabled, and Basic authentication is used for Elasticsearch. For security, Elasticsearch and Kibana are accessible only through `localhost`.\n====\n\n==== API access\n\nAn API key for Elasticsearch is generated and stored in the `.env` file as `ES_LOCAL_API_KEY`.\nUse this key to connect to Elasticsearch with a https://www.elastic.co/guide/en/elasticsearch/client/index.html[programming language client] or the https://www.elastic.co/guide/en/elasticsearch/reference/current/rest-apis.html[REST API].\n\nFrom the `elastic-start-local` folder, check the connection to Elasticsearch using `curl`:\n\n[source,sh]\n----\nsource .env\ncurl $ES_LOCAL_URL -H \"Authorization: ApiKey ${ES_LOCAL_API_KEY}\"\n----\n// NOTCONSOLE\n\n=== Send requests to Elasticsearch\n\nYou send data and other requests to Elasticsearch through REST APIs.\nYou can interact with Elasticsearch using any client that sends HTTP requests,\nsuch as the https://www.elastic.co/guide/en/elasticsearch/client/index.html[Elasticsearch\nlanguage clients] and https://curl.se[curl].\n\n==== Using curl\n\nHere's an example curl command to create a new Elasticsearch index, using basic auth:\n\n[source,sh]\n----\ncurl -u elastic:$ELASTIC_PASSWORD \\\n  -X PUT \\\n  http://localhost:9200/my-new-index \\\n  -H 'Content-Type: application/json'\n----\n// NOTCONSOLE\n\n==== Using a language client\n\nTo connect to your local dev Elasticsearch cluster with a language client, you can use basic authentication with the `elastic` username and the password you set in the environment variable.\n\nYou'll use the following connection details:\n\n* **Elasticsearch endpoint**: `http://localhost:9200`\n* **Username**: `elastic`\n* **Password**: `$ELASTIC_PASSWORD` (Value you set in the environment variable)\n\nFor example, to connect with the Python `elasticsearch` client:\n\n[source,python]\n----\nimport os\nfrom elasticsearch import Elasticsearch\n\nusername = 'elastic'\npassword = os.getenv('ELASTIC_PASSWORD') # Value you set in the environment variable\n\nclient = Elasticsearch(\n    \"http://localhost:9200\",\n    basic_auth=(username, password)\n)\n\nprint(client.info())\n----\n\n==== Using the Dev Tools Console\n\nKibana's developer console provides an easy way to experiment and test requests.\nTo access the console, open Kibana, then go to **Management** > **Dev Tools**.\n\n**Add data**\n\nYou index data into Elasticsearch by sending JSON objects (documents) through the REST APIs.\nWhether you have structured or unstructured text, numerical data, or geospatial data,\nElasticsearch efficiently stores and indexes it in a way that supports fast searches.\n\nFor timestamped data such as logs and metrics, you typically add documents to a\ndata stream made up of multiple auto-generated backing indices.\n\nTo add a single document to an index, submit an HTTP post request that targets the index.\n\n----\nPOST /customer/_doc/1\n{\n  \"firstname\": \"Jennifer\",\n  \"lastname\": \"Walters\"\n}\n----\n\nThis request automatically creates the `customer` index if it doesn't exist,\nadds a new document that has an ID of 1, and\nstores and indexes the `firstname` and `lastname` fields.\n\nThe new document is available immediately from any node in the cluster.\nYou can retrieve it with a GET request that specifies its document ID:\n\n----\nGET /customer/_doc/1\n----\n\nTo add multiple documents in one request, use the `_bulk` API.\nBulk data must be newline-delimited JSON (NDJSON).\nEach line must end in a newline character (`\\n`), including the last line.\n\n----\nPUT customer/_bulk\n{ \"create\": { } }\n{ \"firstname\": \"Monica\",\"lastname\":\"Rambeau\"}\n{ \"create\": { } }\n{ \"firstname\": \"Carol\",\"lastname\":\"Danvers\"}\n{ \"create\": { } }\n{ \"firstname\": \"Wanda\",\"lastname\":\"Maximoff\"}\n{ \"create\": { } }\n{ \"firstname\": \"Jennifer\",\"lastname\":\"Takeda\"}\n----\n\n**Search**\n\nIndexed documents are available for search in near real-time.\nThe following search matches all customers with a first name of _Jennifer_\nin the `customer` index.\n\n----\nGET customer/_search\n{\n  \"query\" : {\n    \"match\" : { \"firstname\": \"Jennifer\" }\n  }\n}\n----\n\n**Explore**\n\nYou can use Discover in Kibana to interactively search and filter your data.\nFrom there, you can start creating visualizations and building and sharing dashboards.\n\nTo get started, create a _data view_ that connects to one or more Elasticsearch indices,\ndata streams, or index aliases.\n\n. Go to **Management > Stack Management > Kibana > Data Views**.\n. Select **Create data view**.\n. Enter a name for the data view and a pattern that matches one or more indices,\nsuch as _customer_.\n. Select **Save data view to Kibana**.\n\nTo start exploring, go to **Analytics > Discover**.\n\n[[upgrade]]\n== Upgrade\n\nTo upgrade from an earlier version of Elasticsearch, see the\nhttps://www.elastic.co/guide/en/elasticsearch/reference/current/setup-upgrade.html[Elasticsearch upgrade\ndocumentation].\n\n[[build-source]]\n== Build from source\n\nElasticsearch uses https://gradle.org[Gradle] for its build system.\n\nTo build a distribution for your local OS and print its output location upon\ncompletion, run:\n----\n./gradlew localDistro\n----\n\nTo build a distribution for another platform, run the related command:\n----\n./gradlew :distribution:archives:linux-tar:assemble\n./gradlew :distribution:archives:darwin-tar:assemble\n./gradlew :distribution:archives:windows-zip:assemble\n----\n\nDistributions are output to `distribution/archives`.\n\nTo run the test suite, see xref:TESTING.asciidoc[TESTING].\n\n[[docs]]\n== Documentation\n\nFor the complete Elasticsearch documentation visit\nhttps://www.elastic.co/guide/en/elasticsearch/reference/current/index.html[elastic.co].\n\nFor information about our documentation processes, see the\nxref:docs/README.asciidoc[docs README].\n\n[[examples]]\n== Examples and guides\n\nThe https://github.com/elastic/elasticsearch-labs[`elasticsearch-labs`] repo contains executable Python notebooks, sample apps, and resources to test out Elasticsearch for vector search, hybrid search and generative AI use cases.\n\n\n[[contribute]]\n== Contribute\n\nFor contribution guidelines, see xref:CONTRIBUTING.md[CONTRIBUTING].\n\n[[questions]]\n== Questions? Problems? Suggestions?\n\n* To report a bug or request a feature, create a\nhttps://github.com/elastic/elasticsearch/issues/new/choose[GitHub Issue]. Please\nensure someone else hasn't created an issue for the same topic.\n\n* Need help using Elasticsearch? Reach out on the\nhttps://discuss.elastic.co[Elastic Forum] or https://ela.st/slack[Slack]. A\nfellow community member or Elastic engineer will be happy to help you out.\n"
        },
        {
          "name": "REST_API_COMPATIBILITY.md",
          "type": "blob",
          "size": 16.8544921875,
          "content": "#  REST API compatibility developers guide\n\nREST API compatibility is intended to minimize the impact for breaking changes applied to the REST API. Compatibility is implemented to strike a balance between preserving the REST API contract across major versions while still allowing for breaking changes. URL paths, parameters, HTTP verbs, request bodies and response bodies are all covered by REST API compatibility.\n\n### Example use case\n\nThe recommended procedure to upgrade Elasticsearch to a new major version is to first upgrade Elasticsearch (the server), then upgrade any of the consumers/clients. This implies that the consumers/clients during and immediately after an upgrade may still be attempting to communicate with Elasticsearch using the prior major version's REST API contract.  During this time, without compatibility, if any of consumers/clients continue to call into one of the REST API's that include breaking changes, the consumer/client can also break.  Compatibility is a best attempt to honor the spirit of the original API call even after the breaking change has been applied.\n\nFor example, assume a REST request requires a consumer to send a \"limit\" parameter in the body of the request. Elasticsearch (the server) is upgraded to the next major version and that major version made a non-passive change that removes \"limit\" in favor of a \"minimum\" and \"maximum\" fields to enable some new functionality to also support a new lower limit. After the major version upgrade Elasticsearch only knows about \"minimum\" and \"maximum\", but the consumer/client (still on the older version of the client) only knows about \"limit\". With REST API compatibility the consumer/client can continue to send \"limit\" and Elasticsearch will honor that request using the value of \"limit\" as the \"maximum\".  Since the prior version had no notion of \"minimum\",  mapping \"limit\" -> \"maximum\" is the best attempt to honor the spirit of the request while still allowing the API and underlying behavior to evolve.  A warning will be emitted to let the consumer/client know Elasticsearch applied compatibility.\n\n### Workflow\n\nREST API compatibility is opt-in per request using a specialized value for the `Accept` or `Content-Type` HTTP header.  The intent is that this header may be sent prior to a major version upgrade resulting in no differences with the standard header values when the client and server match versions. For example, assume the consumer/client using the REST API for Elasticsearch version 7.  If the client sends the specialized header value(s) for compatibility with version 7, then it will have no effect when talking with Elasticsearch version 7. However, once Elasticsearch is upgraded to version 8, and the compatibility with version 7 headers are sent, then Elasticsearch version 8 attempts to honor the version 7 REST API contract. This allows Elasticsearch to be upgraded to version 8 while the clients can generally remain on version 7. It is not always guaranteed to fix all upgrade issues but should provide an additional level of confidence during/post upgrade.\n\nBreaking changes always follow a life-cycle of deprecation (documentation and warnings) in the current version before implementing the breaking change in the next major version. This give users an opportunity to adopt the non-deprecated variants in the current version. It is the still recommended approach to stop the usage of all deprecated functionality prior to a major version upgrade. However, in practice this can be a difficult, error prone, and a time consuming task. So it also recommended that consumers/clients send the specialized header to enable REST API compatibility before an upgrade to help catch any missed usages of deprecated functionality.\n\n### Specialized header\n\nREST API compatibility is enabled per request using a specialized HTTP header.\n\nFor a request without a body the following Accept header is required to request REST API compatibility:\n\n```javascript\nAccept: \"application/vnd.elasticsearch+json;compatible-with=7\"\n```\n\nIf the request also has a body the following is also necessary\n\n```javascript\nContent-Type: \"application/vnd.elasticsearch+json;compatible-with=7\"\n```\n\nThe headers mirrors the 4 supported media types for both the `Accept` and `Content-Type` headers.\n\n```javascript\n\"application/vnd.elasticsearch+json;compatible-with=7\"\n\"application/vnd.elasticsearch+yaml;compatible-with=7\"\n\"application/vnd.elasticsearch+smile;compatible-with=7\"\n\"application/vnd.elasticsearch+cbor;compatible-with=7\"\n```\n\nThe version request is found in compatible-with=7. When sent to a version 7 server will do nothing special. When sent to a version 8 server will apply compatiblity if needed. When sent to a version 9 server will fail since compatibility is only supported across 1 major version.\n\nA consumer/client may not mix \"compatible-with\" versions between headers. Attempts to mix and match compatible versions between `Accept` and `Content-Type` headers will result in an error.  A consumer/client may mix the media type (i.e. send `yaml` and get back `json`).\n\n\n## Introducing breaking changes to the REST API\n\nThe remainder of this guide is intended to help Elasticsearch developers understand how to introduce breaking changes to the REST API with support for REST API compatibility.\n\n### When to apply\n\nAny changes that touch the URL path,  URL parameters, HTTP verbs, the shape or response code for the non-error response, the shape of the request should account for REST API compatibility.  REST API compatibility is first introduced with version 8 with compatibility back to version 7.  The 7.x branch of code has some minimal support to allow for easy back porting and to allow for the reading of the header on a version 7 cluster. There are no plans to provide compatibility back to version 6.\n\nREST API compatibility is not the same as a fully version-ed API. It is a best attempt find a compatible way to honor the prior major version REST API contract. There will be cases where it is not possible to apply a compatibility. In those cases a meaningful error message should be emitted.\n\n### When not to apply\n\nSettings, SQL, scripting, CAT APIs, and errors messages all touch the REST API but are not covered by REST API compatibility.\n\n### Implementation\n\nThere are 4 primary integration points with REST API compatibility.\n\n*  Serializing responses to xContent\n*  De-serializing requests from xContent\n*  URL path, URL parameters, and HTTP verbs\n*  Testing for compatibility\n\n### Serializing\n\nEmitting a response back to the client is generally done via the `ToXContent#toXContent` method. The requested compatibility can be found in the `XContentBuilder` object allowing for conditional logic based on a specific request. For example:\n\n```java\n    if (builder.getRestApiVersion() == RestApiVersion.V_7) {\n        builder.field(\"limit\", max);\n    } else {\n        builder.field(\"maximum,\", max);\n        builder.field(\"minimum\", min);\n    }\n```\n\nIn some cases the `ToXContent#toXContent` is also used to serialize the values to the cluster state.  In these cases some refactoring or additional runtime validation is necessary to prevent the REST API conditional serialization from persisting in cluster state.\n\nIn places where `ToXContent#toXContent` is not used for serialization, then the requested compatibility can also be found in the `RestRequest` object and used accordingly. It is generally discouraged to push the requested compatibility version down through the transport actions to keep the REST compatibility a REST layer concern.\n\n### De-serializing\n\nAccepting a payload from a prior version's REST request is a bit more difficult than sending a compatible response. There are a couple different approaches for de-serialization, however, in general all of them use a request specific parser.  `XContentParser` has a `getRestApiVersion()` method that could be used in manner that is near identical to serialization. However, in practice most parsers are invoked via a `ConstructingObjectParser` or an `ObjectParser` and quite often they are daisy chained together via the `NamedXContentRegistry`.\n\nBoth `ConstructingObjectParser` and `ObjectParser` use `ParseField`'s to statically declare the relationship between a named key and value pairing. A parser via it's `ParseField` can be made version aware, meaning that the parser will match on the incoming payload if the requested compatibility is the correct version.\n\nFor example:\n\n```java\nPARSER.declareInt(MyPojo::setMax, new ParseField(\"maximum\", \"limit\").forRestApiVersion(RestApiVersion.equalTo(RestApiVersion.V_7)));\nPARSER.declareInt(MyPojo::setMax, new ParseField(\"maximum\").forRestApiVersion(RestApiVersion.onOrAfter(RestApiVersion.V_8)));\n```\n\nThe above example is for code that live in the version 8 branch of code. In this example, `limit` has been deprecated in version 7 and removed in version 8.  The above code reads use the `maximum` value from the request for both version 7 and version 8. However, if compatibility is requested it will also allow `limit` in the payload.  If `limit` is used a warning will be emitted.\n\nThe version in `forRestApiVersion` is reference to when the declaration is valid. Assuming version 8 is the main branch and all changes start in the main branch then get back ported. The above text is what would be applicable for the v8 branch of code. The first line of code is essentially ignored except for when compatibility with version 7 is requested. When back-porting this change to the 7.x branch, the first line would be identical, and the second line would be omitted.\n\nThe above strategy works well for single fields, but could get overly complex very fast for large multiple field changes. For more complex de-serialization changes there is also support to construct a `NamedXContentRegistry` with some \"normal\" entries as well some entries that are only applied when compatibility with the prior version is requested. The syntax is very similar where can express the desired version from the required `ParseField` when adding an entry to the `NamedXContentRegistry`.\n\nAdditionally, there is a pseudo standard method `fromXContent` which also generally has access to the `XContentParser` for that request which can be used to conditionally change how to parse the input.\n\n```java\nprivate static final ParseField limitField = new ParseField(\"maximum\", \"limit\").forRestApiVersion(RestApiVersion.equalTo(RestApiVersion.V_7));\n\n//call to fromXContent\nMyExample.fromXContent(XContentType.JSON.xContent().createParser(XContentParserConfiguration.EMPTY.withDeprecationHandler(LoggingDeprecationHandler.INSTANCE).withRestApiVersion(request.getRestApiVersion()), \" { \\\"limit\\\" : 99 }\"));\n\n//contents of a fromXContent\nwhile ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {\n    if (token == XContentParser.Token.FIELD_NAME) {\n        currentFieldName = parser.currentName();\n    } else if (token.isValue()) {\n        if (limitField.match(currentFieldName, LoggingDeprecationHandler.INSTANCE)) {\n            if (parser.getRestApiVersion().matches(RestApiVersion.onOrAfter(RestApiVersion.V_8))\n                && \"maximum\".equals(currentFieldName) == false) {\n                throw new IllegalArgumentException(\"invalid parameter [limit], use [maximum] instead\");\n            } else {\n                value = parser.intValue();\n            }\n        }\n    }\n}\n```\n### URL\n\nPaths are declared via a Route. Routes are composed of the HTTP verb and the relative path. Optionally they can declare a deprecated verb/path combination and the REST API version in which they were deprecated.\n\nFor example:\n\n```java\n   Route.builder(GET, \"_mypath/{foo}/{bar}\").deprecated(MY_DEPRECATION_MESSAGE, RestApiVersion.V_7).build(),\n```\nNote: you can safely backport this route declaration to 7.x branch.\nThe above declares that `GET _mypath/foo/bar` is deprecated in version 7. When this path is called in a version 7 server, it will emit the deprecation warning. When this path is called in a version 8 server, it will throw a 404 error unless REST API compatibility is requested. Only then will the path be honored and will also emit a warning message.\n\n\nHTTP parameters (i.e. ?user=alice) are also covered by REST API compatibility and must be at least consumed as to not cause an error. For example if you remove a parameter in version 8, you must at least read that parameter and emit a warning when version 7 compatibility is requested.\n\nFor example:\n\n```java\nif (request.getRestApiVersion() == RestApiVersion.V_7 && request.hasParam(\"limit\")) {\n    deprecationLogger.compatibleCritical(\"limit_parameter_deprecation\",\n                      \"Deprecated parameter [limit] used, replaced by [maximum and minimum]\");\n    setMax(request.param(\"limit\"));\n }\n```\n\nThe above code checks the request's compatible version and if the request has the parameter in question. In this case the deprecation warning is not automatic and requires the developer to manually log the warning. `request.param` is also required since it consumes the value as to avoid the error of unconsumed parameters.\n\n### Testing\n\nThe primary means of testing compatibility is via the prior major version's YAML REST tests. The build system will download the latest prior version of the YAML rest tests and execute them against the current cluster version. Prior to execution the tests will be transformed by injecting the correct headers to enable compatibility as well as other custom changes to the tests to allow the tests to pass. These customizations are configured via the build.gradle and happen just prior to test execution. Since the compatibility tests are manipulated version of the tests stored in Github (via the past major version), it is important to find the local (on disk) version for troubleshooting compatibility tests.\n\nThe tests are wired into the `check` task, so that is the easiest way to test locally prior to committing.  More specifically the task is called `yamlRestCompatTest`. These behave nearly identical to it's non-compat `yamlRestTest` task. The only variance is that the tests are sourced from the prior version branch and the tests go through a transformation phase before execution. The transformation task is `yamlRestCompatTestTransform`.\n\nFor example:\n\n```bash\n./gradlew :rest-api-spec:yamlRestCompatTest\n```\n\nSince these are a variation of backward compatibility testing, the entire suite of compatibility tests will be skipped anytime the backward compatibility testing is disabled. Since the source code for these tests live in a branch of code, disabling a specific test should be done via the transformation task configuration in build.gradle (i.e. `yamlRestCompatTestTransform`).\n\nIn some cases the prior version of the YAML REST tests are not sufficient to fully test changes. This can happen when the prior version has insufficient test coverage. In those cases, you can simply add more testing to the prior version or you can add custom REST tests that will run along side of the other compatibility tests. These custom tests can be found in the `yamlRestCompatTest` sourceset. Custom REST tests for compatibility will not be modified prior to execution, so the correct headers need to be manually added.\n\n### Developer's workflow\n\nThere should not be much, if any, deviation in a developers normal workflow to introduce and back-port changes. Changes should be applied in main, then back ported as needed.\n\nMost of the compatibility will work correctly when back-porting as-is, but some care is needed that the logic is correct for that version when back-porting.  For example, both the route (URL) and field (de-serialization) declarations with version awareness will behave differently if the declared version is the current version or the prior version. This allows the same line of code to be back ported as-is with differing behavior.  Additionally the compatible version is always populated (even when not requested, defaulting to the current version), so conditional logic comparing against a specific version is safe across branches.\n\nMixed clusters are not explicitly tested since the change should be applied at the REST (coordinating node) layer.\n\n### Troubleshooting compatibility test failures\n\nBy far the most common reason that compatibility tests can seemingly randomly fail is that your main branch is out of date with the upstream main. For this reason, it always suggested to ensure that your PR branch is up to date.\n\nTest failure reproduction lines should behave identical to the non-compatible variant. However, to assure you are referencing the correct line number when reading the test, be sure to look at the line number from the transformed test on disk.  Generally the fully transformed tests can be found at `build/restResources/compat/yamlTests/transformed/rest-api-spec/test/*`.\n\nMuting compatibility tests should be done via a test transform. A per test skip or a file match can be used to skip the tests.\n\n```groovy\n\ntasks.named(\"yamlRestCompatTestTransform\").configure({ task ->\n  task.skipTestsByFilePattern(\"**/cat*/*.yml\", \"Cat API are not supported\")\n  task.skipTest(\"bulk/10_basic/Array of objects\", \"Muted due failures. See #12345\")\n})\n\n```\n\n\n\n\n\n\n\n\n\n\n\n\n\n"
        },
        {
          "name": "TESTING.asciidoc",
          "type": "blob",
          "size": 35.4482421875,
          "content": "[[TestingFrameworkCheatsheet]]\n= Testing\n\n[partintro]\n\nElasticsearch uses JUnit for testing. It also generated random inputs into\ntests, either using a random seed, or one that is set via a system\nproperty. The following is a cheatsheet of options for running the\nElasticsearch tests.\n\n== Creating packages\n\nTo build a distribution for your local OS and print its output location upon completion, run:\n\n-----------------------------\n./gradlew localDistro\n-----------------------------\n\nTo create a platform-specific build, use the following depending on your\noperating system:\n\n-----------------------------\n./gradlew :distribution:archives:linux-tar:assemble\n./gradlew :distribution:archives:darwin(-aarch64)-tar:assemble\n./gradlew :distribution:archives:windows-zip:assemble\n-----------------------------\n\nYou can build a Docker image with:\n\n-----------------------------\n./gradlew build(Aarch64)DockerImage\n-----------------------------\n\nNote: you almost certainly don't want to run `./gradlew assemble` as this\nwill attempt build every single Elasticsearch distribution.\n\n=== Running Elasticsearch from a checkout\n\nIn order to run Elasticsearch from source without building a package, you can\nrun it using Gradle:\n\n-------------------------------------\n./gradlew run\n-------------------------------------\n\n==== Launching and debugging from an IDE\n\nIf you want to run and debug Elasticsearch from your IDE, the `./gradlew run` task\nsupports a remote debugging option. Run the following from your terminal:\n\n---------------------------------------------------------------------------\n./gradlew run --debug-jvm\n---------------------------------------------------------------------------\n\nNext start the \"Debug Elasticsearch\" run configuration in IntelliJ. This will enable the IDE to connect to the process and allow debug functionality.\n\n\nAs such the IDE needs to be instructed to listen for connections on the debug port.\nSince we might run multiple JVMs as part of configuring and starting the cluster it's\nrecommended to configure the IDE to initiate multiple listening attempts. In case of IntelliJ, this option\nis called \"Auto restart\" and needs to be checked.\n\nNOTE: If you have imported the project into IntelliJ according to the instructions in\nlink:/CONTRIBUTING.md#importing-the-project-into-intellij-idea[CONTRIBUTING.md] then a debug run configuration\nnamed \"Debug Elasticsearch\" will be created for you and configured appropriately.\n\n===== Debugging the CLI launcher\n\nThe gradle task does not start the Elasticsearch server process directly; like in the Elasticsearch distribution,\nthe job of starting the server process is delegated to a launcher CLI tool. If you need to debug the launcher itself,\nadd the following option to the `run` task:\n---------------------------------------------------------------------------\n./gradlew run --debug-cli-jvm\n---------------------------------------------------------------------------\nThis option can be specified in isolation or combined with `--debug-jvm`. Since the CLI launcher lifespan may overlap\nwith the server process lifespan, the CLI launcher process will be started on a different port (5107 for the first node,\n5108 and following for additional cluster nodes).\n\nAs with the `--debug-jvm` command, the IDE needs to be instructed to listen for connections on the debug port.\nYou need to configure and start an appropriate Remote JVM Debug configuration, e.g. by cloning and editing\nthe \"Debug Elasticsearch\" run configuration to point to the correct debug port.\n\n==== Disabling assertions\n\nWhen running Elasticsearch with `./gradlew run`, assertions are enabled by\ndefault. To disable them, add the following command line option:\n\n-------------------------\n-Dtests.jvm.argline=\"-da -dsa\"\n-------------------------\n\n==== Distribution\n\nBy default a node is started with the zip distribution.\nIn order to start with a different distribution use the `-Drun.distribution` argument.\n\nTo for example start the open source distribution:\n\n-------------------------------------\n./gradlew run -Drun.distribution=oss\n-------------------------------------\n\n==== License type\n\nBy default a node is started with the `basic` license type.\nIn order to start with a different license type use the `-Drun.license_type` argument.\n\nIn order to start a node with a trial license execute the following command:\n\n-------------------------------------\n./gradlew run -Drun.license_type=trial\n-------------------------------------\n\nThis enables security and other paid features and adds a superuser with the username: `elastic-admin` and\npassword: `elastic-password`.\n\n==== Other useful arguments\n\n- In order to start a node with a different max heap space add: `-Dtests.heap.size=4G`\n- In order to use a custom data directory: `--data-dir=/tmp/foo`\n- In order to preserve data in between executions: `--preserve-data`\n- In order to remotely attach a debugger to the server process: `--debug-jvm`\n- In order to remotely attach a debugger to the CLI launcher process: `--debug-cli-jvm`\n- In order to set a different keystore password: `--keystore-password`\n- In order to set an Elasticsearch setting, provide a setting with the following prefix: `-Dtests.es.`\n- In order to pass a JVM setting, e.g. to disable assertions: `-Dtests.jvm.argline=\"-da\"`\n- In order to use HTTPS: ./gradlew run --https\n- In order to start a mock logging APM server on port 9999 and configure ES cluster to connect to it,\nuse `./gradlew run --with-apm-server`\n\n==== Customizing the test cluster for ./gradlew run\n\nYou may need to customize the cluster configuration for the ./gradlew run task.\nThe settings can be set via the command line, but other options require updating the task itself.\nYou can simply find the task in the source code and configure it there.\n(The task is currently defined in build-tools-internal/src/main/groovy/elasticsearch.run.gradle)\nHowever, this requires modifying a source controlled file and is subject to accidental commits.\nAlternatively, you can use a Gradle init script to inject custom build logic with the -I flag to configure this task locally.\n\nFor example:\n\nTo use a custom certificate for HTTPS with `./gradlew run`, you can do the following.\nCreate a file (for example ~/custom-run.gradle) with the following contents:\n-------------------------------------\nrootProject {\n  if(project.name == 'elasticsearch') {\n    afterEvaluate {\n        testClusters.matching { it.name == \"runTask\"}.configureEach {\n          extraConfigFile 'http.p12', file(\"<path/to/file>/http.p12\")\n        }\n    }\n  }\n}\n-------------------------------------\nNow tell Gradle to use this init script:\n-------------------------------------\n./gradlew run -I ~/custom-run.gradle \\\n-Dtests.es.xpack.security.http.ssl.enabled=true \\\n-Dtests.es.xpack.security.http.ssl.keystore.path=http.p12\n-------------------------------------\n\nNow the http.p12 file will be placed in the config directory of the running cluster and available for use.\nAssuming you have the http.ssl.keystore setup correctly, you can now use HTTPS with ./gradlew run without the risk\nof accidentally committing your local configurations.\n\n==== Multiple nodes in the test cluster for ./gradlew run\n\nAnother desired customization for ./gradlew run might be to run multiple\nnodes with different setting for each node. For example, you may want to debug a coordinating only node that fans out\nto one or more data nodes. To do this, increase the numberOfNodes and add specific configuration for each\nof the nodes.  For example, the following will instruct the first node (:9200) to be a coordinating only node,\nand all other nodes to be master, data_hot, data_content nodes.\n-------------------------------------\ntestClusters.register(\"runTask\") {\n     ...\n    numberOfNodes = 2\n    def cluster = testClusters.named(\"runTask\").get()\n    cluster.getNodes().each { node ->\n      node.setting('cluster.initial_master_nodes', cluster.getLastNode().getName())\n      node.setting('node.roles', '[master,data_hot,data_content]')\n    }\n    cluster.getFirstNode().setting('node.roles', '[]')\n   ...\n}\n-------------------------------------\n\nYou can also place this config in custom init script (see above) to avoid accidental commits.\nIf you are passing in the --debug-jvm flag with multiple nodes, you will need multiple remote debuggers running. One\nfor each node listening at port 5007, 5008, 5009, and so on. Ensure that each remote debugger has auto restart enabled.\n\n==== Manually testing cross cluster search\n\nUse ./gradlew run-ccs to launch 2 clusters wired together for the purposes of cross cluster search.\nFor example send a search request \"my_remote_cluster:*/_search\" to the querying cluster (:9200) to query data\nin the fulfilling cluster.\n\nIf you are passing in the --debug-jvm flag, you will need two remote debuggers running. One at port 5007 and another\none at port 5008. Ensure that each remote debugger has auto restart enabled.\n\n=== Test case filtering.\n\nYou can run a single test, provided that you specify the Gradle project. See the documentation on\nhttps://docs.gradle.org/current/userguide/userguide_single.html#simple_name_pattern[simple name pattern filtering].\n\nRun a single test case in the `server` project:\n\n----------------------------------------------------------\n./gradlew :server:test --tests org.elasticsearch.package.ClassName\n----------------------------------------------------------\n\nRun all tests in a package and its sub-packages:\n\n----------------------------------------------------\n./gradlew :server:test --tests 'org.elasticsearch.package.*'\n----------------------------------------------------\n\nRun all tests that are waiting for a bugfix (disabled by default)\n\n------------------------------------------------\n./gradlew test -Dtests.filter=@awaitsfix\n------------------------------------------------\n\n=== Seed and repetitions.\n\nRun with a given seed (seed is a hex-encoded long).\n\n------------------------------\n./gradlew test -Dtests.seed=DEADBEEF\n------------------------------\n\n=== Repeats _all_ tests of ClassName N times.\n\nEvery test repetition will have a different method seed\n(derived from a single random master seed).\n\n--------------------------------------------------\n./gradlew :server:test -Dtests.iters=N --tests org.elasticsearch.package.ClassName\n--------------------------------------------------\n\n=== Repeats _all_ tests of ClassName N times.\n\nEvery test repetition will have exactly the same master (0xdead) and\nmethod-level (0xbeef) seed.\n\n------------------------------------------------------------------------\n./gradlew :server:test -Dtests.iters=N -Dtests.seed=DEAD:BEEF --tests org.elasticsearch.package.ClassName\n------------------------------------------------------------------------\n\n=== Repeats a given test N times\n\n(note the filters - individual test repetitions are given suffixes,\nie: testFoo[0], testFoo[1], etc... so using testmethod or tests.method\nending in a glob is necessary to ensure iterations are run).\n\n-------------------------------------------------------------------------\n./gradlew :server:test -Dtests.iters=N --tests org.elasticsearch.package.ClassName.methodName\n-------------------------------------------------------------------------\n\nRepeats N times but skips any tests after the first failure or M initial failures.\n\n-------------------------------------------------------------\n./gradlew test -Dtests.iters=N -Dtests.failfast=true ...\n./gradlew test -Dtests.iters=N -Dtests.maxfailures=M ...\n-------------------------------------------------------------\n\n=== Test groups.\n\nTest groups can be enabled or disabled (true/false).\n\nDefault value provided below in [brackets].\n\n------------------------------------------------------------------\n./gradlew test -Dtests.awaitsfix=[false] - known issue (@AwaitsFix)\n------------------------------------------------------------------\n\n=== Load balancing and caches.\n\nBy default the tests run on multiple processes using all the available cores on all\navailable CPUs. Not including hyper-threading.\nIf you want to explicitly specify the number of JVMs you can do so on the command\nline:\n\n----------------------------\n./gradlew test -Dtests.jvms=8\n----------------------------\n\nOr in `~/.gradle/gradle.properties`:\n\n----------------------------\nsystemProp.tests.jvms=8\n----------------------------\n\nIt's difficult to pick the \"right\" number here. Hypercores don't count for CPU\nintensive tests and you should leave some slack for JVM-internal threads like\nthe garbage collector. And you have to have enough RAM to handle each JVM.\n\n=== Test compatibility.\n\nIt is possible to provide a version that allows to adapt the tests behaviour\nto older features or bugs that have been changed or fixed in the meantime.\n\n-----------------------------------------\n./gradlew test -Dtests.compatibility=1.0.0\n-----------------------------------------\n\n\n=== Miscellaneous.\n\nRun all tests without stopping on errors (inspect log files).\n\n-----------------------------------------\n./gradlew test -Dtests.haltonfailure=false\n-----------------------------------------\n\nRun more verbose output (slave JVM parameters, etc.).\n\n----------------------\n./gradlew test -verbose\n----------------------\n\nChange the default suite timeout to 5 seconds for all\ntests (note the exclamation mark).\n\n---------------------------------------\n./gradlew test -Dtests.timeoutSuite=5000! ...\n---------------------------------------\n\nChange the logging level of ES (not Gradle)\n\n--------------------------------\n./gradlew test -Dtests.es.logger.level=DEBUG\n--------------------------------\n\nPrint all the logging output from the test runs to the commandline\neven if tests are passing.\n\n------------------------------\n./gradlew test -Dtests.output=always\n------------------------------\n\nConfigure the heap size.\n\n------------------------------\n./gradlew test -Dtests.heap.size=512m\n------------------------------\n\nPass arbitrary jvm arguments.\n\n------------------------------\n# specify heap dump path\n./gradlew test -Dtests.jvm.argline=\"-XX:HeapDumpPath=/path/to/heapdumps\"\n# enable gc logging\n./gradlew test -Dtests.jvm.argline=\"-verbose:gc\"\n# enable security debugging\n./gradlew test -Dtests.jvm.argline=\"-Djava.security.debug=access,failure\"\n------------------------------\n\nPass build arguments.\n\n------------------------------\n# Run tests against a release build. License key must be provided, but usually can be anything.\n./gradlew test -Dbuild.snapshot=false -Dlicense.key=\"x-pack/license-tools/src/test/resources/public.key\"\n------------------------------\n\n== Running verification tasks\n\nTo run all verification tasks, including static checks, unit tests, and integration tests:\n\n---------------------------------------------------------------------------\n./gradlew check\n---------------------------------------------------------------------------\n\nNote that this will also run the unit tests and precommit tasks first. If you want to just\nrun the in memory cluster integration tests (because you are debugging them):\n\n---------------------------------------------------------------------------\n./gradlew internalClusterTest\n---------------------------------------------------------------------------\n\nIf you want to just run the precommit checks:\n\n---------------------------------------------------------------------------\n./gradlew precommit\n---------------------------------------------------------------------------\n\nSome of these checks will require `docker-compose` installed for bringing up\ntest fixtures. If it's not present those checks will be skipped automatically.\nThe host running Docker (or VM if you're using Docker Desktop) needs 4GB of\nmemory or some of the containers will fail to start. You can tell that you\nare short of memory if containers are exiting quickly after starting with\ncode 137 (128 + 9, where 9 means SIGKILL).\n\n== Debugging tests\n\nIf you would like to debug your tests themselves, simply pass the `--debug-jvm`\nflag to the testing task and connect a debugger on the default port of `5005`.\n\n---------------------------------------------------------------------------\n./gradlew :server:test --debug-jvm\n---------------------------------------------------------------------------\n\nFor REST tests, if you'd like to debug the Elasticsearch server itself, and\nnot your test code, use the `--debug-server-jvm` flag and use the\n\"Debug Elasticsearch\" run configuration in IntelliJ to listen on the default\nport of `5007`.\n\n---------------------------------------------------------------------------\n./gradlew :rest-api-spec:yamlRestTest --debug-server-jvm\n---------------------------------------------------------------------------\n\nNOTE: In the case of test clusters using multiple nodes, multiple debuggers\nwill need to be attached on incrementing ports. For example, for a 3 node\ncluster ports `5007`, `5008`, and `5009` will attempt to attach to a listening\ndebugger. You can use the \"Debug Elasticsearch (node 2)\" and \"(node 3)\" run\nconfigurations should you need to debug a multi-node cluster.\n\nYou can also use a combination of both flags to debug both tests and server.\nThis is only applicable to Java REST tests.\n\n---------------------------------------------------------------------------\n./gradlew :modules:kibana:javaRestTest --debug-jvm --debug-server-jvm\n---------------------------------------------------------------------------\n\n== Testing the REST layer\n\nThe REST layer is tested through specific tests that are executed against\na cluster that is configured and initialized via Gradle. The tests\nthemselves can be written in either Java or with a YAML based DSL.\n\nYAML based REST tests should be preferred since these are shared between all\nthe elasticsearch official clients. The YAML based tests describe the\noperations to be executed and the obtained results that need to be tested.\n\nThe YAML tests support various operators defined in the link:/rest-api-spec/src/yamlRestTest/resources/rest-api-spec/test/README.asciidoc[rest-api-spec] and adhere to the link:/rest-api-spec/README.markdown[Elasticsearch REST API JSON specification]\nIn order to run the YAML tests, the relevant API specification needs\nto be on the test classpath. Any gradle project that has support for REST\ntests will get the primary API on it's class path. However, to better support\nGradle incremental builds, it is recommended to explicitly declare which\nparts of the API the tests depend upon.\n\nFor example:\n---------------------------------------------------------------------------\nrestResources {\n  restApi {\n    includeCore '_common', 'indices', 'index', 'cluster', 'nodes', 'get', 'ingest'\n  }\n}\n---------------------------------------------------------------------------\n\nYAML REST tests that include x-pack specific APIs need to explicitly declare\nwhich APIs are required through a similar `includeXpack` configuration.\n\nThe REST tests are run automatically when executing the \"./gradlew check\" command. To run only the\nYAML REST tests use the following command (modules and plugins may also include YAML REST tests):\n\n---------------------------------------------------------------------------\n./gradlew :rest-api-spec:yamlRestTest\n---------------------------------------------------------------------------\n\nA specific test case can be run with the following command:\n\n---------------------------------------------------------------------------\n./gradlew ':rest-api-spec:yamlRestTest' \\\n  --tests \"org.elasticsearch.test.rest.ClientYamlTestSuiteIT\" \\\n  -Dtests.method=\"test {yaml=cat.segments/10_basic/Help}\"\n---------------------------------------------------------------------------\n\nYou can run a group of YAML test by using wildcards:\n\n---------------------------------------------------------------------------\n./gradlew :rest-api-spec:yamlRestTest \\\n  --tests \"org.elasticsearch.test.rest.ClientYamlTestSuiteIT.test {yaml=index/*/*}\"\n---------------------------------------------------------------------------\n\nor\n\n---------------------------------------------------------------------------\n./gradlew :rest-api-spec:yamlRestTest \\\n  --tests org.elasticsearch.test.rest.ClientYamlTestSuiteIT -Dtests.method=\"test {yaml=cat.segments/10_basic/*}\"\n---------------------------------------------------------------------------\n\nThe latter method is preferable when the YAML suite name contains `.` (period).\n\nNote that if the selected test via the `--tests` filter is not a valid test, i.e., the YAML test\nrunner is not able to parse and load it, you might get an error message indicating that the test\nwas not found. In such cases, running the whole suite without using the `--tests` could show more\nspecific error messages about why the test runner is not able to parse or load a certain test.\n\nThe YAML REST tests support all the options provided by the randomized runner, plus the following:\n\n* `tests.rest.blacklist`: comma separated globs that identify tests that are\nblacklisted and need to be skipped\ne.g. -Dtests.rest.blacklist=index/*/Index document,get/10_basic/*\n\nJava REST tests can be run with the \"javaRestTest\" task.\n\nFor example :\n---------------------------------------------------------------------------\n./gradlew :modules:mapper-extras:javaRestTest\n---------------------------------------------------------------------------\n\nA specific test case can be run with the following syntax (fqn.test {params}):\n\n---------------------------------------------------------------------------\n./gradlew ':modules:mapper-extras:javaRestTest' \\\n  --tests \"org.elasticsearch.index.mapper.TokenCountFieldMapperIntegrationIT.testSearchByTokenCount {storeCountedFields=true loadCountedFields=false}\"\n---------------------------------------------------------------------------\n\nyamlRestTest's and javaRestTest's are easy to identify, since they are found in a\nrespective source directory. However, there are some more specialized REST tests\nthat use custom task names. These are usually found in \"qa\" projects commonly\nuse the \"integTest\" task.\n\nIf in doubt about which command to use, simply run <gradle path>:check\n\n== Testing packaging\n\nThe packaging tests are run on different build vm cloud instances to verify\nthat installing and running Elasticsearch distributions works correctly on\nsupported operating systems. These tests should really only be run on ephemeral\nsystems because they're destructive; that is, these tests install and remove\npackages and freely modify system settings, so you will probably regret it if\nyou execute them on your development machine.\n\n=== Reproducing packaging tests\n\nTo reproduce or debug packaging tests failures we recommend using using our provided https://github.com/elastic/elasticsearch-infra/blob/master/buildkite-tools/README.md[*buildkite tools*]\n\n== Testing backwards compatibility\n\nBackwards compatibility tests exist to test upgrading from each supported version\nto the current version. To run them all use:\n\n-------------------------------------------------\n./gradlew bwcTest\n-------------------------------------------------\n\nA specific version can be tested as well. For example, to test bwc with\nversion 5.3.2 run:\n\n-------------------------------------------------\n./gradlew v5.3.2#bwcTest\n-------------------------------------------------\n\nUse -Dtests.class and -Dtests.method to run a specific bwcTest test.\nFor example to run a specific tests from the x-pack rolling upgrade from 7.7.0:\n-------------------------------------------------\n./gradlew :x-pack:qa:rolling-upgrade:v7.7.0#bwcTest \\\n -Dtests.class=org.elasticsearch.upgrades.UpgradeClusterClientYamlTestSuiteIT \\\n -Dtests.method=\"test {p0=*/40_ml_datafeed_crud/*}\"\n-------------------------------------------------\n\nTests are ran for versions that are not yet released but with which the current version will be compatible with.\nThese are automatically checked out and built from source.\nSee link:./build-tools-internal/src/main/java/org/elasticsearch/gradle/BwcVersions.java[BwcVersions]\nand link:./distribution/bwc/build.gradle[distribution/bwc/build.gradle]\nfor more information.\n\nWhen running `./gradlew check`, minimal bwc checks are also run against compatible versions that are not yet released.\n\n==== BWC Testing against a specific remote/branch\n\nSometimes a backward compatibility change spans two versions.\nA common case is a new functionality that needs a BWC bridge in an unreleased versioned of a release branch (for example, 5.x).\nAnother use case, since the introduction of serverless, is to test BWC against main in addition to the other released branches.\nTo do so, specify the `bwc.refspec` remote and branch to use for the BWC build as `origin/main`.\nTo test against main, you will also need to create a new version in link:./server/src/main/java/org/elasticsearch/Version.java[Version.java],\nincrement `elasticsearch` in link:./build-tools-internal/version.properties[version.properties], and hard-code the `project.version` for ml-cpp\nin link:./x-pack/plugin/ml/build.gradle[ml/build.gradle].\n\nIn general, to test the changes, you can instruct Gradle to build the BWC version from another remote/branch combination instead of pulling the release branch from GitHub.\nYou do so using the `bwc.refspec.{VERSION}` system property:\n\n-------------------------------------------------\n./gradlew check -Dtests.bwc.refspec.8.15=origin/main\n-------------------------------------------------\n\nThe branch needs to be available on the remote that the BWC makes of the\nrepository you run the tests from. Using the remote is a handy trick to make\nsure that a branch is available and is up to date in the case of multiple runs.\n\nExample:\n\nSay you need to make a change to `main` and have a BWC layer in `5.x`. You\nwill need to:\n. Create a branch called `index_req_change` off your remote `${remote}`. This\nwill contain your change.\n. Create a branch called `index_req_bwc_5.x` off `5.x`. This will contain your bwc layer.\n. Push both branches to your remote repository.\n. Run the tests with `./gradlew check -Dbwc.remote=${remote} -Dbwc.refspec.5.x=index_req_bwc_5.x`.\n\n==== Skip fetching latest\n\nFor some BWC testing scenarios, you want to use the local clone of the\nrepository without fetching latest. For these use cases, you can set the system\nproperty `tests.bwc.git_fetch_latest` to `false` and the BWC builds will skip\nfetching the latest from the remote.\n\n== Testing in FIPS 140-2 mode\n\nWe have a CI matrix job that periodically runs all our tests with the JVM configured\nto be FIPS 140-2 compliant with the use of the BouncyCastle FIPS approved Security Provider.\nFIPS 140-2 imposes certain requirements that affect how our tests should be set up or what\ncan be tested. This section summarizes what one needs to take into consideration so that\ntests won't fail when run in fips mode.\n\n=== Muting tests in FIPS 140-2 mode\n\nIf the following limitations cannot be observed, or there is a need to actually test some use\ncase that is not available/allowed in fips mode, the test can be muted. For unit tests or Java\nrest tests one can use\n\n------------------------------------------------\nassumeFalse(\"Justification why this cannot be run in FIPS mode\", inFipsJvm());\n------------------------------------------------\n\nFor specific YAML rest tests one can use\n\n------------------------------------------------\n- skip:\n    features: fips_140\n    reason: \"Justification why this cannot be run in FIPS mode\"\n------------------------------------------------\n\nFor disabling entire types of tests for subprojects, one can use for example:\n\n------------------------------------------------\nif (buildParams.inFipsJvm) {\n  // This test cluster is using a BASIC license and FIPS 140 mode is not supported in BASIC\n  tasks.named(\"javaRestTest\").configure{enabled = false }\n}\n------------------------------------------------\n\nin `build.gradle`.\n\n=== Limitations\n\nThe following should be taken into consideration when writing new tests or adjusting existing ones:\n\n==== TLS\n\n`JKS` and `PKCS#12` keystores cannot be used in FIPS mode. If the test depends on being able to use\na keystore, it can be muted when needed ( see `ESTestCase#inFipsJvm` ). Alternatively, one can use\nPEM encoded files for keys and certificates for the tests or for setting up TLS in a test cluster.\nAlso, when in FIPS 140 mode, hostname verification for TLS cannot be turned off so if you are using\n`*.verification_mode: none` , you'd need to mute the test in fips mode.\n\nWhen using TLS, ensure that private keys used are longer than 2048 bits, or mute the test in fips mode.\n\n==== Password hashing algorithm\n\nTest clusters are configured with `xpack.security.fips_mode.enabled` set to true. This means that\nFIPS 140-2 related bootstrap checks are enabled and the test cluster will fail to form if the\npassword hashing algorithm is set to something else than a PBKDF2 based one. You can delegate the choice\nof algorithm to i.e. `SecurityIntegTestCase#getFastStoredHashAlgoForTests` if you don't mind the\nactual algorithm used, or depend on default values for the test cluster nodes.\n\n==== Password length\n\nWhile using `pbkdf2` as the password hashing algorithm, FIPS 140-2 imposes a requirement that\npasswords are longer than 14 characters. You can either ensure that all test user passwords in\nyour test are longer than 14 characters and use i.e. `SecurityIntegTestCase#getFastStoredHashAlgoForTests`\nto randomly select a hashing algorithm, or use `pbkdf2_stretch` that doesn't have the same\nlimitation.\n\n==== Keystore Password\n\nIn FIPS 140-2 mode, the elasticsearch keystore needs to be password protected with a password\nof appropriate length. This is handled automatically in `fips.gradle` and the keystore is unlocked\non startup by the test clusters tooling in order to have secure settings available. However, you\nmight need to take into consideration that the keystore is password-protected with `keystore-password`\nif you need to interact with it in a test.\n\n== How to write good tests?\n\n=== Base classes for test cases\n\nThere are multiple base classes for tests:\n\n* **`ESTestCase`**: The base class of all tests. It is typically extended\n  directly by unit tests.\n* **`ESSingleNodeTestCase`**: This test case sets up a cluster that has a\n  single node.\n* **`ESIntegTestCase`**: An integration test case that creates a cluster that\n  might have multiple nodes.\n* **`ESRestTestCase`**: An integration tests that interacts with an external\n  cluster via the REST API. This is used for Java based REST tests.\n* **`ESClientYamlSuiteTestCase` **: A subclass of `ESRestTestCase` used to run\n  YAML based REST tests.\n\n=== Good practices\n\n==== What kind of tests should I write?\n\nUnit tests are the preferred way to test some functionality: most of the time\nthey are simpler to understand, more likely to reproduce, and unlikely to be\naffected by changes that are unrelated to the piece of functionality that is\nbeing tested.\n\nThe reason why `ESSingleNodeTestCase` exists is that all our components used to\nbe very hard to set up in isolation, which had led us to having a number of\nintegration tests but close to no unit tests. `ESSingleNodeTestCase` is a\nworkaround for this issue which provides an easy way to spin up a node and get\naccess to components that are hard to instantiate like `IndicesService`.\nWhenever practical, you should prefer unit tests.\n\nMany tests extend `ESIntegTestCase`, mostly because this is how most tests used\nto work in the early days of Elasticsearch. However the complexity of these\ntests tends to make them hard to debug. Whenever the functionality that is\nbeing tested isn't intimately dependent on how Elasticsearch behaves as a\ncluster, it is recommended to write unit tests or REST tests instead.\n\nIn short, most new functionality should come with unit tests, and optionally\nREST tests to test integration.\n\n==== Refactor code to make it easier to test\n\nUnfortunately, a large part of our code base is still hard to unit test.\nSometimes because some classes have lots of dependencies that make them hard to\ninstantiate. Sometimes because API contracts make tests hard to write. Code\nrefactors that make functionality easier to unit test are encouraged. If this\nsounds very abstract to you, you can have a look at\nhttps://github.com/elastic/elasticsearch/pull/16610[this pull request] for\ninstance, which is a good example. It refactors `IndicesRequestCache` in such\na way that:\n - it no longer depends on objects that are hard to instantiate such as\n   `IndexShard` or `SearchContext`,\n - time-based eviction is applied on top of the cache rather than internally,\n   which makes it easier to assert on what the cache is expected to contain at\n   a given time.\n\n=== Bad practices\n\n==== Use randomized-testing for coverage\n\nIn general, randomization should be used for parameters that are not expected\nto affect the behavior of the functionality that is being tested. For instance\nthe number of shards should not impact `date_histogram` aggregations, and the\nchoice of the `store` type (`niofs` vs `mmapfs`) does not affect the results of\na query. Such randomization helps improve confidence that we are not relying on\nimplementation details of one component or specifics of some setup.\n\nHowever it should not be used for coverage. For instance if you are testing a\npiece of functionality that enters different code paths depending on whether\nthe index has 1 shards or 2+ shards, then we shouldn't just test against an\nindex with a random number of shards: there should be one test for the 1-shard\ncase, and another test for the 2+ shards case.\n\n==== Abuse randomization in multi-threaded tests\n\nMulti-threaded tests are often not reproducible due to the fact that there is\nno guarantee on the order in which operations occur across threads. Adding\nrandomization to the mix usually makes things worse and should be done with\ncare.\n\n== Test coverage analysis\n\nGenerating test coverage reports for Elasticsearch is currently not possible through Gradle.\nHowever, it _is_ possible to gain insight in code coverage using IntelliJ's built-in coverage\nanalysis tool that can measure coverage upon executing specific tests.\n\nTest coverage reporting used to be possible with JaCoCo when Elasticsearch was using Maven\nas its build system. Since the switch to Gradle though, this is no longer possible, seeing as\nthe code currently used to build Elasticsearch does not allow JaCoCo to recognize its tests.\nFor more information on this, see the discussion in https://github.com/elastic/elasticsearch/issues/28867[issue #28867].\n\n== Building with extra plugins\nAdditional plugins may be built alongside elasticsearch, where their\ndependency on elasticsearch will be substituted with the local elasticsearch\nbuild. To add your plugin, create a directory called elasticsearch-extra as\na sibling of elasticsearch. Checkout your plugin underneath elasticsearch-extra\nand the build will automatically pick it up. You can verify the plugin is\nincluded as part of the build by checking the projects of the build.\n\n---------------------------------------------------------------------------\n./gradlew projects\n---------------------------------------------------------------------------\n\n== Environment misc\n\nThere is a known issue with macOS localhost resolve strategy that can cause\nsome integration tests to fail. This is because integration tests have timings\nfor cluster formation, discovery, etc. that can be exceeded if name resolution\ntakes a long time.\nTo fix this, make sure you have your computer name (as returned by `hostname`)\ninside `/etc/hosts`, e.g.:\n....\n127.0.0.1       localhost ElasticMBP.local\n255.255.255.255 broadcasthost\n::1             localhost ElasticMBP.local`\n....\n\n== Benchmarking\n\nFor changes that might affect the performance characteristics of Elasticsearch\nyou should also run macrobenchmarks. We maintain a macrobenchmarking tool\ncalled https://github.com/elastic/rally[Rally]\nwhich you can use to measure the performance impact. It comes with a set of\ndefault benchmarks that we also\nhttps://elasticsearch-benchmarks.elastic.co/[run every night]. To get started,\nplease see https://esrally.readthedocs.io/en/stable/[Rally's documentation].\n\n== Test doc builds\n\nThe Elasticsearch docs are in AsciiDoc format. You can test and build the docs\nlocally using the Elasticsearch documentation build process. See\nhttps://github.com/elastic/docs.\n"
        },
        {
          "name": "TRACING.md",
          "type": "blob",
          "size": 7.65234375,
          "content": "# Tracing in Elasticsearch\n\nElasticsearch is instrumented using the [OpenTelemetry][otel] API, which allows\nES developers to gather traces and analyze what Elasticsearch is doing.\n\n## How is tracing implemented?\n\nThe Elasticsearch server code contains a [tracing][tracing] package, which is\nan abstraction over the OpenTelemetry API. All locations in the code that\nperform instrumentation and tracing must use these abstractions.\n\nSeparately, there is the [apm](./modules/apm) module, which works with the\nOpenTelemetry API directly to record trace data.  Underneath the OTel API, we\nuse Elastic's [APM agent for Java][agent], which attaches at runtime to the\nElasticsearch JVM and removes the need for Elasticsearch to hard-code the use of\nan OTel implementation. Note that while it is possible to programmatically start\nthe APM agent, the Security Manager permissions required make this essentially\nimpossible.\n\n## How is tracing configured?\n\nYou must supply configuration and credentials for the APM server (see below).\nIn your `elasticsearch.yml` add the following configuration:\n\n```\ntelemetry.tracing.enabled: true\ntelemetry.agent.server_url: https://<your-apm-server>:443\n```\n\nWhen using a secret token to authenticate with the APM server, you must add it to the Elasticsearch keystore under `telemetry.secret_token`. For example, execute:\n\n    bin/elasticsearch-keystore add telemetry.secret_token\n\nthen enter the token when prompted. If you are using API keys, change the keystore key name to `telemetry.api_key`.\n\nAll APM settings live under `telemetry`. Tracing related settings go under `telemetry.tracing` and settings\nrelated to the Java agent go under `telemetry.agent`. Anything you set under there will be propagated to\nthe agent.\n\nFor agent settings that can be changed dynamically, you can use the cluster\nsettings REST API. For example, to change the sampling rate:\n\n    curl -XPUT \\\n      -H \"Content-type: application/json\" \\\n      -u \"$USERNAME:$PASSWORD\" \\\n      -d '{ \"persistent\": { \"telemetry.agent.transaction_sample_rate\": \"0.75\" } }' \\\n      https://localhost:9200/_cluster/settings\n\n\n### More details about configuration\n\nFor context, the APM agent pulls configuration from [multiple\nsources][agent-config], with a hierarchy that means, for example, that options\nset in the config file cannot be overridden via system properties.\n\nNow, in order to send tracing data to the APM server, ES needs to be configured with\neither a `secret_key` or an `api_key`. We could configure these in the agent via\nsystem properties, but then their values would be available to any Java code in\nElasticsearch that can read system properties.\n\nInstead, when Elasticsearch bootstraps itself, it compiles all APM settings\ntogether, including any `secret_key` or `api_key` values from the ES keystore,\nand writes out a temporary APM config file containing all static configuration\n(i.e. values that cannot change after the agent starts).  This file is deleted\nas soon as possible after ES starts up. Settings that are not sensitive and can\nbe changed dynamically are configured via system properties. Calls to the ES\nsettings REST API are translated into system property writes, which the agent\nlater picks up and applies.\n\n## Where is tracing data sent?\n\nYou need to have an APM server running somewhere. For example, you can create a\ndeployment in [Elastic Cloud](https://www.elastic.co/cloud/) with Elastic's APM\nintegration.\n\n## What do we trace?\n\nWe primarily trace \"tasks\". The tasks framework in Elasticsearch allows work to\nbe scheduled for execution, cancelled, executed in a different thread pool, and\nso on. Tracing a task results in a \"span\", which represents the execution of the\ntask in the tracing system. We also instrument REST requests, which are not (at\npresent) modelled by tasks.\n\nA span can be associated with a parent span, which allows all spans in, for\nexample, a REST request to be grouped together. Spans can track work across\ndifferent Elasticsearch nodes.\n\nElasticsearch also supports distributed tracing via [W3c Trace Context][w3c]\nheaders. If clients of Elasticsearch send these headers with their requests,\nthen that data will be forwarded to the APM server in order to yield a trace\nacross systems.\n\nIn rare circumstances, it is possible to avoid tracing a task using\n`TaskManager#register(String,String,TaskAwareRequest,boolean)`. For example,\nMachine Learning uses tasks to record which models are loaded on each node. Such\ntasks are long-lived and are not suitable candidates for APM tracing.\n\n## Thread contexts and nested spans\n\nWhen a span is started, Elasticsearch tracks information about that span in the\ncurrent [thread context][thread-context].  If a new thread context is created,\nthen the current span information must not be propagated but instead renamed, so\nthat (1) it doesn't interfere when new trace information is set in the context,\nand (2) the previous trace information is available to establish a parent /\nchild span relationship.  This is done with `ThreadContext#newTraceContext()`.\n\nSometimes we need to detach new spans from their parent. For example, creating\nan index starts some related background tasks, but these shouldn't be associated\nwith the REST request, otherwise all the background task spans will be\nassociated with the REST request for as long as Elasticsearch is running.\n`ThreadContext` provides the `clearTraceContext`() method for this purpose.\n\n## How to I trace something that isn't a task?\n\nFirst work out if you can turn it into a task. No, really.\n\nIf you can't do that, you'll need to ensure that your class can get access to a\n`Tracer` instance (this is available to inject, or you'll need to pass it when\nyour class is created). Then you need to call the appropriate methods on the\ntracer when a span should start and end. You'll also need to manage the creation\nof new trace contexts when child spans need to be created.\n\n## What additional attributes should I set?\n\nThat's up to you. Be careful not to capture anything that could leak sensitive\nor personal information.\n\n## What is \"scope\" and when should I use it?\n\nUsually you won't need to.\n\nThat said, sometimes you may want more details to be captured about a particular\nsection of code. You can think of \"scope\" as representing the currently active\ntracing context. Using scope allows the APM agent to do the following:\n\n* Enables automatic correlation between the \"active span\" and logging, where\n  logs have also been captured.\n* Enables capturing any exceptions thrown when the span is active, and linking\n  those exceptions to the span\n* Allows the sampling profiler to be used as it allows samples to be linked to\n  the active span (if any), so the agent can automatically get extra spans\n  without manual instrumentation.\n\nHowever, a scope must be closed in the same thread in which it was opened, which\ncannot be guaranteed when using tasks, making scope largely useless to\nElasticsearch.\n\nIn the OpenTelemetry documentation, spans, scope and context are fairly\nstraightforward to use, since `Scope` is an `AutoCloseable` and so can be\neasily created and cleaned up use try-with-resources blocks. Unfortunately,\nElasticsearch is a complex piece of software, and also extremely asynchronous,\nso the typical OpenTelemetry examples do not work.\n\nNonetheless, it is possible to manually use scope where we need more detail by\nexplicitly opening a scope via the `Tracer`.\n\n\n[otel]: https://opentelemetry.io/\n[thread-context]: ./server/src/main/java/org/elasticsearch/common/util/concurrent/ThreadContext.java\n[w3c]: https://www.w3.org/TR/trace-context/\n[tracing]: ./server/src/main/java/org/elasticsearch/telemetry\n[agent-config]: https://www.elastic.co/guide/en/apm/agent/java/master/configuration.html\n[agent]: https://www.elastic.co/guide/en/apm/agent/java/current/index.html\n"
        },
        {
          "name": "Vagrantfile",
          "type": "blob",
          "size": 15.0615234375,
          "content": "# -*- mode: ruby -*-\n# vim: ft=ruby ts=2 sw=2 sts=2 et:\n\n# This Vagrantfile exists to test packaging. Read more about its use in the\n# vagrant section in TESTING.asciidoc.\n\n # Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n # or more contributor license agreements. Licensed under the \"Elastic License\n # 2.0\", the \"GNU Affero General Public License v3.0 only\", and the \"Server Side\n # Public License v 1\"; you may not use this file except in compliance with, at\n # your election, the \"Elastic License 2.0\", the \"GNU Affero General Public\n # License v3.0 only\", or the \"Server Side Public License, v 1\".\n\ndefine_opts = {\n  autostart: false\n}.freeze\n\nVagrant.configure(2) do |config|\n\n  config.vm.provider 'virtualbox' do |vbox|\n    # Give the box more memory and cpu because our tests are beasts!\n    vbox.memory = Integer(ENV['VAGRANT_MEMORY'] || 8192)\n    vbox.cpus = Integer(ENV['VAGRANT_CPUS'] || 4)\n\n    # see https://github.com/hashicorp/vagrant/issues/9524\n    vbox.customize [\"modifyvm\", :id, \"--audio\", \"none\"]\n  end\n\n  # Switch the default share for the project root from /vagrant to\n  # /elasticsearch because /vagrant is confusing when there is a project inside\n  # the elasticsearch project called vagrant....\n  config.vm.synced_folder '.', '/vagrant', disabled: true\n  config.vm.synced_folder '.', '/elasticsearch'\n  # TODO: make these syncs work for windows!!!\n  config.vm.synced_folder \"#{Dir.home}/.vagrant/gradle/caches/jars-3\", \"/root/.gradle/caches/jars-3\",\n      create: true,\n      owner: \"vagrant\"\n  config.vm.synced_folder \"#{Dir.home}/.vagrant/gradle/caches/modules-2\", \"/root/.gradle/caches/modules-2\",\n      create: true,\n      owner: \"vagrant\"\n  config.vm.synced_folder \"#{Dir.home}/.gradle/wrapper\", \"/root/.gradle/wrapper\",\n      create: true,\n      owner: \"vagrant\"\n\n  # Expose project directory. Note that VAGRANT_CWD may not be the same as Dir.pwd\n  PROJECT_DIR = ENV['VAGRANT_PROJECT_DIR'] || Dir.pwd\n  config.vm.synced_folder PROJECT_DIR, '/project'\n\n  'ubuntu-1604'.tap do |box|\n    config.vm.define box, define_opts do |config|\n      config.vm.box = 'elastic/ubuntu-16.04-x86_64'\n      deb_common config, box, extra: <<-SHELL\n        # Install Jayatana so we can work around it being present.\n        [ -f /usr/share/java/jayatanaag.jar ] || install jayatana\n      SHELL\n      ubuntu_docker config\n    end\n  end\n  'ubuntu-1804'.tap do |box|\n    config.vm.define box, define_opts do |config|\n      config.vm.box = 'elastic/ubuntu-18.04-x86_64'\n      deb_common config, box, extra: <<-SHELL\n       # Install Jayatana so we can work around it being present.\n       [ -f /usr/share/java/jayatanaag.jar ] || install jayatana\n      SHELL\n      ubuntu_docker config\n    end\n  end\n  'debian-8'.tap do |box|\n    config.vm.define box, define_opts do |config|\n      config.vm.box = 'elastic/debian-8-x86_64'\n      deb_common config, box, extra: <<-SHELL\n        # this sometimes gets a bad ip, and doesn't appear to be needed\n        rm -f /etc/apt/sources.list.d/http_debian_net_debian.list\n      SHELL\n    end\n  end\n  'debian-9'.tap do |box|\n    config.vm.define box, define_opts do |config|\n      config.vm.box = 'elastic/debian-9-x86_64'\n      deb_common config, box\n      deb_docker config\n    end\n  end\n  'centos-6'.tap do |box|\n    config.vm.define box, define_opts do |config|\n      config.vm.box = 'elastic/centos-6-x86_64'\n      rpm_common config, box\n    end\n  end\n  'centos-7'.tap do |box|\n    config.vm.define box, define_opts do |config|\n      config.vm.box = 'elastic/centos-7-x86_64'\n      rpm_common config, box\n      rpm_docker config\n    end\n  end\n  'oel-6'.tap do |box|\n    config.vm.define box, define_opts do |config|\n      config.vm.box = 'elastic/oraclelinux-6-x86_64'\n      rpm_common config, box\n    end\n  end\n  'oel-7'.tap do |box|\n    config.vm.define box, define_opts do |config|\n      config.vm.box = 'elastic/oraclelinux-7-x86_64'\n      rpm_common config, box\n    end\n  end\n  'fedora-28'.tap do |box|\n    config.vm.define box, define_opts do |config|\n      config.vm.box = 'elastic/fedora-28-x86_64'\n      dnf_common config, box\n      dnf_docker config\n    end\n  end\n  'fedora-29'.tap do |box|\n    config.vm.define box, define_opts do |config|\n      config.vm.box = 'elastic/fedora-29-x86_64'\n      dnf_common config, box\n      dnf_docker config\n    end\n  end\n  'opensuse-42'.tap do |box|\n    config.vm.define box, define_opts do |config|\n      config.vm.box = 'elastic/opensuse-42-x86_64'\n      suse_common config, box\n    end\n  end\n  'sles-12'.tap do |box|\n    config.vm.define box, define_opts do |config|\n      config.vm.box = 'elastic/sles-12-x86_64'\n      sles_common config, box\n    end\n  end\n  'rhel-8'.tap do |box|\n    config.vm.define box, define_opts do |config|\n      config.vm.box = 'elastic/rhel-8-x86_64'\n      rpm_common config, box\n    end\n  end\n\n  windows_2012r2_box = ENV['VAGRANT_WINDOWS_2012R2_BOX']\n  if windows_2012r2_box && windows_2012r2_box.empty? == false\n    'windows-2012r2'.tap do |box|\n      config.vm.define box, define_opts do |config|\n        config.vm.box = windows_2012r2_box\n        windows_common config, box\n      end\n    end\n  end\n\n  windows_2016_box = ENV['VAGRANT_WINDOWS_2016_BOX']\n  if windows_2016_box && windows_2016_box.empty? == false\n    'windows-2016'.tap do |box|\n      config.vm.define box, define_opts do |config|\n        config.vm.box = windows_2016_box\n        windows_common config, box\n      end\n    end\n  end\nend\n\ndef deb_common(config, name, extra: '')\n  # http://foo-o-rama.com/vagrant--stdin-is-not-a-tty--fix.html\n  config.vm.provision 'fix-no-tty', type: 'shell' do |s|\n      s.privileged = false\n      s.inline = \"sudo sed -i '/tty/!s/mesg n/tty -s \\\\&\\\\& mesg n/' /root/.profile\"\n  end\n  extra_with_lintian = <<-SHELL\n    #{extra}\n    install lintian\n  SHELL\n  linux_common(\n    config,\n    name,\n    update_command: 'apt-get update',\n    update_tracking_file: '/var/cache/apt/archives/last_update',\n    install_command: 'apt-get install -y --force-yes',\n    extra: extra_with_lintian\n  )\nend\n\ndef ubuntu_docker(config)\n  config.vm.provision 'install Docker using apt', type: 'shell', inline: <<-SHELL\n    # Install packages to allow apt to use a repository over HTTPS\n    apt-get install -y --force-yes \\\n      apt-transport-https \\\n      ca-certificates \\\n      curl \\\n      gnupg2 \\\n      software-properties-common\n\n    # Add Docker’s official GPG key\n    curl -fsSL https://download.docker.com/linux/ubuntu/gpg | apt-key add -\n\n    # Set up the stable Docker repository\n    add-apt-repository \\\n      \"deb [arch=amd64] https://download.docker.com/linux/ubuntu \\\n      $(lsb_release -cs) \\\n      stable\"\n\n    # Install Docker. Unlike Fedora and CentOS, this also start the daemon.\n    apt-get update\n    apt-get install -y --force-yes docker-ce docker-ce-cli containerd.io\n\n    # Add vagrant to the Docker group, so that it can run commands\n    usermod -aG docker vagrant\n\n    # Enable IPv4 forwarding\n    sed -i '/net.ipv4.ip_forward/s/^#//' /etc/sysctl.conf\n    systemctl restart networking\n  SHELL\nend\n\n\ndef deb_docker(config)\n  config.vm.provision 'install Docker using apt', type: 'shell', inline: <<-SHELL\n    # Install packages to allow apt to use a repository over HTTPS\n    apt-get install -y --force-yes \\\n      apt-transport-https \\\n      ca-certificates \\\n      curl \\\n      gnupg2 \\\n      software-properties-common\n\n    # Add Docker’s official GPG key\n    curl -fsSL https://download.docker.com/linux/debian/gpg | apt-key add -\n\n    # Set up the stable Docker repository\n    add-apt-repository \\\n      \"deb [arch=amd64] https://download.docker.com/linux/debian \\\n      $(lsb_release -cs) \\\n      stable\"\n\n    # Install Docker. Unlike Fedora and CentOS, this also start the daemon.\n    apt-get update\n    apt-get install -y --force-yes docker-ce docker-ce-cli containerd.io\n\n    # Add vagrant to the Docker group, so that it can run commands\n    usermod -aG docker vagrant\n  SHELL\nend\n\ndef rpm_common(config, name)\n  linux_common(\n    config,\n    name,\n    update_command: 'yum check-update',\n    update_tracking_file: '/var/cache/yum/last_update',\n    install_command: 'yum install -y'\n  )\nend\n\ndef rpm_docker(config)\n  config.vm.provision 'install Docker using yum', type: 'shell', inline: <<-SHELL\n    # Install prerequisites\n    yum install -y yum-utils device-mapper-persistent-data lvm2\n\n    # Add repository\n    yum-config-manager -y --add-repo https://download.docker.com/linux/centos/docker-ce.repo\n\n    # Install Docker\n    yum install -y docker-ce docker-ce-cli containerd.io\n\n    # Start Docker\n    systemctl enable --now docker\n\n    # Add vagrant to the Docker group, so that it can run commands\n    usermod -aG docker vagrant\n  SHELL\nend\n\ndef dnf_common(config, name)\n  # Autodetect doesn't work....\n  if Vagrant.has_plugin?('vagrant-cachier')\n    config.cache.auto_detect = false\n    config.cache.enable :generic, { :cache_dir => '/var/cache/dnf' }\n  end\n  linux_common(\n    config,\n    name,\n    update_command: 'dnf check-update',\n    update_tracking_file: '/var/cache/dnf/last_update',\n    install_command: 'dnf install -y',\n    install_command_retries: 5\n  )\nend\n\ndef dnf_docker(config)\n  config.vm.provision 'install Docker using dnf', type: 'shell', inline: <<-SHELL\n    # Install prerequisites\n    dnf -y install dnf-plugins-core\n\n    # Add repository\n    dnf config-manager --add-repo https://download.docker.com/linux/fedora/docker-ce.repo\n\n    # Install Docker\n    dnf install -y docker-ce docker-ce-cli containerd.io\n\n    # Start Docker\n    systemctl enable --now docker\n\n    # Add vagrant to the Docker group, so that it can run commands\n    usermod -aG docker vagrant\n  SHELL\nend\n\ndef suse_common(config, name, extra: '')\n  linux_common(\n    config,\n    name,\n    update_command: 'zypper --non-interactive list-updates',\n    update_tracking_file: '/var/cache/zypp/packages/last_update',\n    install_command: 'zypper --non-interactive --quiet install --no-recommends',\n    extra: extra\n  )\nend\n\ndef sles_common(config, name)\n  extra = <<-SHELL\n    zypper rr systemsmanagement_puppet puppetlabs-pc1 devel_tools_scm\n    zypper ar http://download.opensuse.org/distribution/12.3/repo/oss/ oss\n    zypper --non-interactive  --gpg-auto-import-keys refresh\n    zypper --non-interactive install git-core\n    # choose to \"ignore some dependencies\" of expect, which has a problem with tcl...\n    zypper --non-interactive install --force-resolution expect\n  SHELL\n  suse_common config, name, extra: extra\nend\n\n# Configuration needed for all linux boxes\n# @param config Vagrant's config object. Required.\n# @param name [String] The box name. Required.\n# @param update_command [String] The command used to update the package\n#   manager. Required. Think `apt-get update`.\n# @param update_tracking_file [String] The location of the file tracking the\n#   last time the update command was run. Required. Should be in a place that\n#   is cached by vagrant-cachier.\n# @param install_command [String] The command used to install a package.\n#   Required. Think `apt-get install #{package}`.\n# @param install_command_retries [Integer] Number of times to retry\n#   a failed install command\n# @param extra [String] Additional script to run before installing\n#   dependencies\n#\ndef linux_common(config,\n                 name,\n                 update_command: 'required',\n                 update_tracking_file: 'required',\n                 install_command: 'required',\n                 install_command_retries: 0,\n                 extra: '')\n\n  raise ArgumentError, 'update_command is required' if update_command == 'required'\n  raise ArgumentError, 'update_tracking_file is required' if update_tracking_file == 'required'\n  raise ArgumentError, 'install_command is required' if install_command == 'required'\n\n  if Vagrant.has_plugin?('vagrant-cachier')\n    config.cache.scope = :box\n  end\n\n  config.vm.provision 'markerfile', type: 'shell', inline: <<-SHELL\n    touch /etc/is_vagrant_vm\n    touch /is_vagrant_vm # for consistency between linux and windows\n  SHELL\n\n  # This prevents leftovers from previous tests using the\n  # same VM from messing up the current test\n  config.vm.provision 'clean es installs in tmp', type: 'shell', inline: <<-SHELL\n    rm -rf /tmp/elasticsearch*\n  SHELL\n\n  sh_set_prompt config, name\n  sh_install_deps(\n    config,\n    update_command,\n    update_tracking_file,\n    install_command,\n    install_command_retries,\n    extra\n  )\nend\n\n# Sets up a consistent prompt for all users. Or tries to. The VM might\n# contain overrides for root and vagrant but this attempts to work around\n# them by re-source-ing the standard prompt file.\ndef sh_set_prompt(config, name)\n  config.vm.provision 'set prompt', type: 'shell', inline: <<-SHELL\n      cat \\<\\<PROMPT > /etc/profile.d/elasticsearch_prompt.sh\nexport PS1='#{name}:\\\\w$ '\nPROMPT\n      grep 'source /etc/profile.d/elasticsearch_prompt.sh' ~/.bashrc |\n        cat \\<\\<SOURCE_PROMPT >> ~/.bashrc\n# Replace the standard prompt with a consistent one\nsource /etc/profile.d/elasticsearch_prompt.sh\nSOURCE_PROMPT\n      grep 'source /etc/profile.d/elasticsearch_prompt.sh' ~vagrant/.bashrc |\n        cat \\<\\<SOURCE_PROMPT >> ~vagrant/.bashrc\n# Replace the standard prompt with a consistent one\nsource /etc/profile.d/elasticsearch_prompt.sh\nSOURCE_PROMPT\n  SHELL\nend\n\ndef sh_install_deps(config,\n                    update_command,\n                    update_tracking_file,\n                    install_command,\n                    install_command_retries,\n                    extra)\n  config.vm.provision 'install dependencies', type: 'shell', inline:  <<-SHELL\n    set -e\n    set -o pipefail\n\n    # Retry install command up to $2 times, if failed\n    retry_installcommand() {\n      n=0\n      while true; do\n        #{install_command} $1 && break\n        let n=n+1\n        if [ $n -ge $2 ]; then\n          echo \"==> Exhausted retries to install $1\"\n          return 1\n        fi\n        echo \"==> Retrying installing $1, attempt $((n+1))\"\n        # Add a small delay to increase chance of metalink providing updated list of mirrors\n        sleep 5\n      done\n    }\n\n    installed() {\n      command -v $1 2>&1 >/dev/null\n    }\n\n    install() {\n      # Only apt-get update if we haven't in the last day\n      if [ ! -f #{update_tracking_file} ] || [ \"x$(find #{update_tracking_file} -mtime +0)\" == \"x#{update_tracking_file}\" ]; then\n        echo \"==> Updating repository\"\n        #{update_command} || true\n        touch #{update_tracking_file}\n      fi\n      echo \"==> Installing $1\"\n      if [ #{install_command_retries} -eq 0 ]\n      then\n        #{install_command} $1\n      else\n        retry_installcommand $1 #{install_command_retries}\n      fi\n    }\n\n    ensure() {\n      installed $1 || install $1\n    }\n\n    #{extra}\n\n    ensure tar\n    ensure curl\n    ensure unzip\n    ensure rsync\n    ensure expect\n\n    cat \\<\\<SUDOERS_VARS > /etc/sudoers.d/elasticsearch_vars\nDefaults   env_keep += \"ES_JAVA_HOME\"\nDefaults   env_keep += \"JAVA_HOME\"\nDefaults   env_keep += \"SYSTEM_JAVA_HOME\"\nSUDOERS_VARS\n    chmod 0440 /etc/sudoers.d/elasticsearch_vars\n  SHELL\nend\n\ndef windows_common(config, name)\n  config.vm.provision 'set prompt', type: 'shell', inline: <<-SHELL\n    $ErrorActionPreference = \"Stop\"\n    $ps_prompt = 'function Prompt { \"#{name}:$($ExecutionContext.SessionState.Path.CurrentLocation)>\" }'\n    $ps_prompt | Out-File $PsHome/Microsoft.PowerShell_profile.ps1\n  SHELL\nend\n"
        },
        {
          "name": "benchmarks",
          "type": "tree",
          "content": null
        },
        {
          "name": "branches.json",
          "type": "blob",
          "size": 0.3017578125,
          "content": "{\n  \"notice\": \"This file is not maintained outside of the main branch and should only be used for tooling.\",\n  \"branches\": [\n    {\n      \"branch\": \"main\"\n    },\n    {\n      \"branch\": \"8.16\"\n    },\n    {\n      \"branch\": \"8.17\"\n    },\n    {\n      \"branch\": \"8.x\"\n    },\n    {\n      \"branch\": \"7.17\"\n    }\n  ]\n}\n"
        },
        {
          "name": "build-conventions",
          "type": "tree",
          "content": null
        },
        {
          "name": "build-tools-internal",
          "type": "tree",
          "content": null
        },
        {
          "name": "build-tools",
          "type": "tree",
          "content": null
        },
        {
          "name": "build.gradle",
          "type": "blob",
          "size": 17.92578125,
          "content": "/*\n * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n * or more contributor license agreements. Licensed under the \"Elastic License\n * 2.0\", the \"GNU Affero General Public License v3.0 only\", and the \"Server Side\n * Public License v 1\"; you may not use this file except in compliance with, at\n * your election, the \"Elastic License 2.0\", the \"GNU Affero General Public\n * License v3.0 only\", or the \"Server Side Public License, v 1\".\n */\n\n\nimport de.thetaphi.forbiddenapis.gradle.ForbiddenApisPlugin\nimport com.avast.gradle.dockercompose.tasks.ComposePull\nimport com.fasterxml.jackson.databind.JsonNode\nimport com.fasterxml.jackson.databind.ObjectMapper\n\nimport org.elasticsearch.gradle.DistributionDownloadPlugin\nimport org.elasticsearch.gradle.Version\nimport org.elasticsearch.gradle.internal.BaseInternalPluginBuildPlugin\nimport org.elasticsearch.gradle.internal.ResolveAllDependencies\nimport org.elasticsearch.gradle.util.GradleUtils\nimport org.gradle.plugins.ide.eclipse.model.AccessRule\n\nimport java.nio.file.Files\n\nimport static java.nio.file.StandardCopyOption.REPLACE_EXISTING\nimport static org.elasticsearch.gradle.util.GradleUtils.maybeConfigure\n\nbuildscript {\n  repositories {\n    mavenCentral()\n  }\n}\n\nplugins {\n  id 'lifecycle-base'\n  id 'elasticsearch.docker-support'\n  id 'elasticsearch.global-build-info'\n  id 'elasticsearch.build-complete'\n  id 'elasticsearch.build-scan'\n  id 'elasticsearch.jdk-download'\n  id 'elasticsearch.internal-distribution-download'\n  id 'elasticsearch.runtime-jdk-provision'\n  id 'elasticsearch.ide'\n  id 'elasticsearch.forbidden-dependencies'\n  id 'elasticsearch.local-distribution'\n  id 'elasticsearch.fips'\n  id 'elasticsearch.internal-testclusters'\n  id 'elasticsearch.run'\n  id 'elasticsearch.run-ccs'\n  id 'elasticsearch.release-tools'\n  id 'elasticsearch.versions'\n}\n\n/**\n * This is a convenient method for declaring test artifact dependencies provided by the internal\n * test artifact plugin. It replaces basically the longer dependency notation with explicit capability\n * declaration like this:\n *\n * testImplementation(project(xpackModule('repositories-metering-api'))) {\n *    capabilities {\n *         requireCapability(\"org.elasticsearch.gradle:repositories-metering-api-test-artifacts\")\n *    }\n * }\n *\n * */\next.testArtifact = { p, String name = \"test\" ->\n  def projectDependency = p.dependencies.create(p)\n  projectDependency.capabilities {\n    requireCapabilities(\"org.elasticsearch.gradle:${projectDependency.name}-${name}-artifacts\")\n  };\n}\n\nclass StepExpansion {\n  String templatePath\n  List<Version> versions\n  String variable\n}\n\nclass ListExpansion {\n  List<Version> versions\n  String variable\n}\n\n// Filters out intermediate patch releases to reduce the load of CI testing\ndef filterIntermediatePatches = { List<Version> versions ->\n  versions.groupBy { \"${it.major}.${it.minor}\" }.values().collect { it.max() }\n}\n\ntasks.register(\"updateCIBwcVersions\") {\n  def writeVersions = { File file, List<Version> versions ->\n    file.text = \"\"\n    file << \"BWC_VERSION:\\n\"\n    versions.each {\n      file << \"  - \\\"$it\\\"\\n\"\n    }\n  }\n\n  def writeBuildkitePipeline = { String outputFilePath,\n                                 String pipelineTemplatePath,\n                                 List<ListExpansion> listExpansions,\n                                 List<StepExpansion> stepExpansions = [] ->\n    def outputFile = file(outputFilePath)\n    def pipelineTemplate = file(pipelineTemplatePath)\n\n    def pipeline = pipelineTemplate.text\n\n    listExpansions.each { expansion ->\n      def listString = \"[\" + expansion.versions.collect { \"\\\"${it}\\\"\" }.join(\", \") + \"]\"\n      pipeline = pipeline.replaceAll('\\\\$' + expansion.variable, listString)\n    }\n\n    stepExpansions.each { expansion ->\n      def steps = \"\"\n      expansion.versions.each {\n        steps += \"\\n\" + file(expansion.templatePath).text.replaceAll('\\\\$BWC_VERSION', it.toString())\n      }\n      pipeline = pipeline.replaceAll(' *\\\\$' + expansion.variable, steps)\n    }\n\n    outputFile.text = \"# This file is auto-generated. See ${pipelineTemplatePath}\\n\" + pipeline\n  }\n\n  // Writes a Buildkite pipelime from a template, and replaces $BWC_LIST with an array of versions\n  // Useful for writing a list of versions in a matrix configuration\n  def expandBwcList = { String outputFilePath, String pipelineTemplatePath, List<Version> versions ->\n    writeBuildkitePipeline(outputFilePath, pipelineTemplatePath, [new ListExpansion(versions: versions, variable: \"BWC_LIST\")])\n  }\n\n  // Writes a Buildkite pipeline from a template, and replaces $BWC_STEPS with a list of steps, one for each version\n  // Useful when you need to configure more versions than are allowed in a matrix configuration\n  def expandBwcSteps = { String outputFilePath, String pipelineTemplatePath, String stepTemplatePath, List<Version> versions ->\n    writeBuildkitePipeline(\n      outputFilePath,\n      pipelineTemplatePath,\n      [],\n      [new StepExpansion(templatePath: stepTemplatePath, versions: versions, variable: \"BWC_STEPS\")]\n    )\n  }\n\n  doLast {\n    writeVersions(file(\".ci/bwcVersions\"), filterIntermediatePatches(buildParams.bwcVersions.indexCompatible))\n    writeVersions(file(\".ci/snapshotBwcVersions\"), filterIntermediatePatches(buildParams.bwcVersions.unreleasedIndexCompatible))\n    expandBwcList(\n      \".buildkite/pipelines/intake.yml\",\n      \".buildkite/pipelines/intake.template.yml\",\n      filterIntermediatePatches(buildParams.bwcVersions.unreleasedIndexCompatible)\n    )\n    writeBuildkitePipeline(\n      \".buildkite/pipelines/periodic.yml\",\n      \".buildkite/pipelines/periodic.template.yml\",\n      [\n        new ListExpansion(versions: filterIntermediatePatches(buildParams.bwcVersions.unreleasedIndexCompatible), variable: \"BWC_LIST\"),\n      ],\n      [\n        new StepExpansion(\n          templatePath: \".buildkite/pipelines/periodic.bwc.template.yml\",\n          versions: filterIntermediatePatches(buildParams.bwcVersions.indexCompatible),\n          variable: \"BWC_STEPS\"\n        ),\n      ]\n    )\n\n    expandBwcSteps(\n      \".buildkite/pipelines/periodic-packaging.yml\",\n      \".buildkite/pipelines/periodic-packaging.template.yml\",\n      \".buildkite/pipelines/periodic-packaging.bwc.template.yml\",\n      filterIntermediatePatches(buildParams.bwcVersions.indexCompatible)\n    )\n  }\n}\n\ntasks.register(\"verifyVersions\") {\n  def verifyCiYaml = { File file, List<Version> versions ->\n    String ciYml = file.text\n    versions.each {\n      if (ciYml.contains(\"\\\"$it\\\"\\n\") == false) {\n        throw new Exception(\"${file} is outdated, run `./gradlew updateCIBwcVersions` and check in the results\")\n      }\n    }\n  }\n  doLast {\n    if (gradle.startParameter.isOffline()) {\n      throw new GradleException(\"Must run in online mode to verify versions\")\n    }\n    // Read the list from maven central.\n    // Fetch the metadata and parse the xml into Version instances because it's more straight forward here\n    // rather than bwcVersion ( VersionCollection ).\n    new URL('https://repo1.maven.org/maven2/org/elasticsearch/elasticsearch/maven-metadata.xml').openStream().withStream { s ->\n      buildParams.bwcVersions.compareToAuthoritative(\n        new XmlParser().parse(s)\n          .versioning.versions.version\n          .collect { it.text() }.findAll { it ==~ /\\d+\\.\\d+\\.\\d+/ }\n          .collect { Version.fromString(it) }\n      )\n    }\n    verifyCiYaml(file(\".ci/bwcVersions\"), filterIntermediatePatches(buildParams.bwcVersions.indexCompatible))\n    verifyCiYaml(file(\".ci/snapshotBwcVersions\"), buildParams.bwcVersions.unreleasedIndexCompatible)\n\n    // Make sure backport bot config file is up to date\n    JsonNode backportConfig = new ObjectMapper().readTree(file(\".backportrc.json\"))\n    buildParams.bwcVersions.forPreviousUnreleased { unreleasedVersion ->\n      boolean valid = backportConfig.get(\"targetBranchChoices\").elements().any { branchChoice ->\n        if (branchChoice.isObject()) {\n          return branchChoice.get(\"name\").textValue() == unreleasedVersion.branch\n        } else {\n          return branchChoice.textValue() == unreleasedVersion.branch\n        }\n      }\n      if (valid == false) {\n        throw new GradleException(\"No branch choice exists for development branch ${unreleasedVersion.branch} in .backportrc.json.\")\n      }\n    }\n    String versionMapping = backportConfig.get(\"branchLabelMapping\").fields().find { it.value.textValue() == 'main' }.key\n    String expectedMapping = \"^v${versions.elasticsearch.replaceAll('-SNAPSHOT', '')}\\$\"\n    if (versionMapping != expectedMapping) {\n      throw new GradleException(\n        \"Backport label mapping for branch 'main' is '${versionMapping}' but should be \" +\n          \"'${expectedMapping}'. Update .backportrc.json.\"\n      )\n    }\n  }\n}\n\n// TODO: This flag existed as a mechanism to disable bwc tests during a backport. It is no\n// longer used for that purpose, but instead a way to run only functional tests. We should\n// rework the functionalTests task to be more explicit about which tasks it wants to run\n// so that that this flag is no longer needed.\nboolean bwc_tests_enabled = true\nif (project.gradle.startParameter.taskNames.any { it.startsWith(\"checkPart\") || it == 'functionalTests' }) {\n  // Disable BWC tests for checkPart* tasks and platform support tests as it's expected that this will run on it's own check\n  bwc_tests_enabled = false\n}\n\nsubprojects { proj ->\n  apply plugin: 'elasticsearch.base'\n}\n\nallprojects {\n  // We disable this plugin for now till we shaked out the issues we see\n  // e.g. see https://github.com/elastic/elasticsearch/issues/72169\n  // apply plugin:'elasticsearch.internal-test-rerun'\n\n  plugins.withType(BaseInternalPluginBuildPlugin).whenPluginAdded {\n    project.dependencies {\n      compileOnly project(\":server\")\n      testImplementation project(\":test:framework\")\n    }\n  }\n\n  // injecting groovy property variables into all projects\n  project.ext {\n    // for ide hacks...\n    isEclipse = providers.systemProperty(\"eclipse.launcher\").isPresent() ||   // Detects gradle launched from Eclipse's IDE\n      providers.systemProperty(\"eclipse.application\").isPresent() ||    // Detects gradle launched from the Eclipse compiler server\n      gradle.startParameter.taskNames.contains('eclipse') ||  // Detects gradle launched from the command line to do eclipse stuff\n      gradle.startParameter.taskNames.contains('cleanEclipse')\n  }\n\n  ext.bwc_tests_enabled = bwc_tests_enabled\n\n  // eclipse configuration\n  apply plugin: 'elasticsearch.eclipse'\n\n  /*\n   * Allow accessing com/sun/net/httpserver in projects that have\n   * configured forbidden apis to allow it.\n   */\n  plugins.withType(ForbiddenApisPlugin) {\n    eclipse.classpath.file.whenMerged { classpath ->\n      if (false == forbiddenApisTest.bundledSignatures.contains('jdk-non-portable')) {\n        classpath.entries\n          .findAll { it.kind == \"con\" && it.toString().contains(\"org.eclipse.jdt.launching.JRE_CONTAINER\") }\n          .each {\n            it.accessRules.add(new AccessRule(\"accessible\", \"com/sun/net/httpserver/*\"))\n          }\n      }\n    }\n  }\n\n  tasks.register('resolveAllDependencies', ResolveAllDependencies) {\n    def ignoredPrefixes = [DistributionDownloadPlugin.ES_DISTRO_CONFIG_PREFIX, \"jdbcDriver\"]\n    configs = project.configurations.matching { config -> ignoredPrefixes.any { config.name.startsWith(it) } == false }\n    resolveJavaToolChain = true\n    if (project.path.contains(\"fixture\")) {\n      dependsOn tasks.withType(ComposePull)\n    }\n    if (project.path.contains(\":distribution:docker\")) {\n      enabled = false\n    }\n    if (project.path.contains(\":libs:cli\")) {\n      // ensure we resolve p2 dependencies for the spotless eclipse formatter\n      dependsOn \"spotlessJavaCheck\"\n    }\n  }\n\n  plugins.withId('lifecycle-base') {\n    if (project.path.startsWith(\":x-pack:\")) {\n      if (project.path.contains(\"security\") || project.path.contains(\":ml\")) {\n        tasks.register('checkPart4') { dependsOn 'check' }\n      } else if (project.path == \":x-pack:plugin\" || project.path.contains(\"ql\") || project.path.contains(\"smoke-test\")) {\n        tasks.register('checkPart3') { dependsOn 'check' }\n      } else if (project.path.contains(\"multi-node\")) {\n        tasks.register('checkPart5') { dependsOn 'check' }\n      } else {\n        tasks.register('checkPart2') { dependsOn 'check' }\n      }\n    } else {\n      tasks.register('checkPart1') { dependsOn 'check' }\n    }\n\n    tasks.register('functionalTests') { dependsOn 'check' }\n  }\n\n  /*\n   * Remove assemble/dependenciesInfo on all qa projects because we don't\n   * need to publish artifacts for them.\n   */\n  if (project.name.equals('qa') || project.path.contains(':qa:')) {\n    maybeConfigure(project.tasks, 'assemble') {\n      it.enabled = false\n    }\n    maybeConfigure(project.tasks, 'dependenciesInfo') {\n      it.enabled = false\n    }\n  }\n\n  project.afterEvaluate {\n    // Ensure similar tasks in dependent projects run first. The projectsEvaluated here is\n    // important because, while dependencies.all will pickup future dependencies,\n    // it is not necessarily true that the task exists in both projects at the time\n    // the dependency is added.\n    if (project.path == ':test:framework') {\n      // :test:framework:test cannot run before and after :server:test\n      return\n    }\n    tasks.matching { it.name.equals('integTest') }.configureEach { integTestTask ->\n      integTestTask.mustRunAfter tasks.matching { it.name.equals(\"test\") }\n    }\n\n/*    configurations.matching { it.canBeResolved }.all { Configuration configuration ->\n      dependencies.matching { it instanceof ProjectDependency }.all { ProjectDependency dep ->\n        Project upstreamProject = dep.dependencyProject\n        if (project.path != upstreamProject?.path) {\n          for (String taskName : ['test', 'integTest']) {\n            project.tasks.matching { it.name == taskName }.configureEach { task ->\n              task.shouldRunAfter(upstreamProject.tasks.matching { upStreamTask -> upStreamTask.name == taskName })\n            }\n          }\n        }\n      }\n    }*/\n  }\n\n  apply plugin: 'elasticsearch.formatting'\n}\n\n\ntasks.register(\"verifyBwcTestsEnabled\") {\n  doLast {\n    if (bwc_tests_enabled == false) {\n      throw new GradleException('Bwc tests are disabled. They must be re-enabled after completing backcompat behavior backporting.')\n    }\n  }\n}\n\ntasks.register(\"branchConsistency\") {\n  description = 'Ensures this branch is internally consistent. For example, that versions constants match released versions.'\n  group = 'Verification'\n  dependsOn \":verifyVersions\", \":verifyBwcTestsEnabled\"\n}\n\ntasks.named(\"wrapper\").configure {\n  distributionType = 'ALL'\n  def minimumGradleVersionFile = project.file('build-tools-internal/src/main/resources/minimumGradleVersion')\n  doLast {\n    // copy wrapper properties file to build-tools-internal to allow seamless idea integration\n    def file = new File(\"build-tools-internal/gradle/wrapper/gradle-wrapper.properties\")\n    Files.copy(getPropertiesFile().toPath(), file.toPath(), REPLACE_EXISTING)\n    // copy wrapper properties file to plugins/examples to allow seamless idea integration\n    def examplePluginsWrapperProperties = new File(\"plugins/examples/gradle/wrapper/gradle-wrapper.properties\")\n    Files.copy(getPropertiesFile().toPath(), examplePluginsWrapperProperties.toPath(), REPLACE_EXISTING)\n    // Update build-tools to reflect the Gradle upgrade\n    // TODO: we can remove this once we have tests to make sure older versions work.\n    minimumGradleVersionFile.text = gradleVersion\n    println \"Updated minimum Gradle Version\"\n  }\n}\n\ngradle.projectsEvaluated {\n  // Having the same group and name for distinct projects causes Gradle to consider them equal when resolving\n  // dependencies leading to hard to debug failures. Run a check across all project to prevent this from happening.\n  // see: https://github.com/gradle/gradle/issues/847\n  Map coordsToProject = [:]\n  project.allprojects.forEach { p ->\n    String coords = \"${p.group}:${p.name}\"\n    if (false == coordsToProject.putIfAbsent(coords, p)) {\n      throw new GradleException(\n        \"Detected that two projects: ${p.path} and ${coordsToProject[coords].path} \" +\n          \"have the same name and group: ${coords}. \" +\n          \"This doesn't currently work correctly in Gradle, see: \" +\n          \"https://github.com/gradle/gradle/issues/847\"\n      )\n    }\n  }\n}\n\ntasks.named(\"validateChangelogs\").configure {\n  def triggeredTaskNames = gradle.startParameter.taskNames\n  onlyIf {\n    triggeredTaskNames.any { it.startsWith(\"checkPart\") || it == 'functionalTests' } == false\n  }\n}\n\ntasks.named(\"precommit\") {\n  dependsOn gradle.includedBuild('build-tools').task(':precommit')\n  dependsOn gradle.includedBuild('build-tools-internal').task(':precommit')\n}\n\ntasks.named(\"checkPart1\").configure {\n  dependsOn gradle.includedBuild('build-tools').task(':check')\n  dependsOn gradle.includedBuild('build-tools-internal').task(':check')\n}\n\ntasks.named(\"assemble\").configure {\n  dependsOn gradle.includedBuild('build-tools').task(':assemble')\n}\n\ntasks.named(\"cleanEclipse\").configure {\n  dependsOn gradle.includedBuild('build-conventions').task(':cleanEclipse')\n  dependsOn gradle.includedBuild('build-tools').task(':cleanEclipse')\n  dependsOn gradle.includedBuild('build-tools-internal').task(':cleanEclipse')\n}\n\ntasks.named(\"eclipse\").configure {\n  dependsOn gradle.includedBuild('build-conventions').task(':eclipse')\n  dependsOn gradle.includedBuild('build-tools').task(':eclipse')\n  dependsOn gradle.includedBuild('build-tools-internal').task(':eclipse')\n}\n\ntasks.register(\"buildReleaseArtifacts\").configure {\n  group = 'build'\n  description = 'Builds all artifacts required for release manager'\n\n  dependsOn allprojects.findAll {\n    it.path.startsWith(':distribution:docker') == false\n      && it.path.startsWith(':ml-cpp') == false\n      && it.path.startsWith(':distribution:bwc') == false\n      && it.path.startsWith(':test:fixture') == false\n  }\n    .collect { GradleUtils.findByName(it.tasks, 'assemble') }\n    .findAll { it != null }\n}\n\ntasks.register(\"spotlessApply\").configure {\n  dependsOn gradle.includedBuild('build-tools').task(':spotlessApply')\n  dependsOn gradle.includedBuild('build-tools').task(':reaper:spotlessApply')\n  dependsOn gradle.includedBuild('build-tools-internal').task(':spotlessApply')\n}\n"
        },
        {
          "name": "catalog-info.yaml",
          "type": "blob",
          "size": 9.4794921875,
          "content": "---\n# yaml-language-server: $schema=https://gist.githubusercontent.com/elasticmachine/988b80dae436cafea07d9a4a460a011d/raw/e57ee3bed7a6f73077a3f55a38e76e40ec87a7cf/rre.schema.json\napiVersion: backstage.io/v1alpha1\nkind: Resource\nmetadata:\n  name: buildkite-pipeline-elasticsearch-update-serverless-submodule\n  description: Update elasticsearch submodule in elasticsearch-serverless\n  links:\n    - title: Pipeline\n      url: https://buildkite.com/elastic/elasticsearch-update-serverless-submodule\nspec:\n  type: buildkite-pipeline\n  system: buildkite\n  owner: group:elasticsearch-team\n  implementation:\n    apiVersion: buildkite.elastic.dev/v1\n    kind: Pipeline\n    metadata:\n      description: \":elasticsearch: Update elasticsearch submodule in elasticsearch-serverless\"\n      name: elasticsearch / update serverless submodule\n    spec:\n      repository: elastic/elasticsearch\n      pipeline_file: .buildkite/update-es-serverless.yml\n      teams:\n        elasticsearch-team: {}\n        ml-core: {}\n        everyone:\n          access_level: READ_ONLY\n      provider_settings:\n        trigger_mode: none\n      schedules:\n        daily promotion:\n          branch: main\n          cronline: \"@daily\"\n---\n# yaml-language-server: $schema=https://gist.githubusercontent.com/elasticmachine/988b80dae436cafea07d9a4a460a011d/raw/e57ee3bed7a6f73077a3f55a38e76e40ec87a7cf/rre.schema.json\napiVersion: backstage.io/v1alpha1\nkind: Resource\nmetadata:\n  name: buildkite-pipeline-elasticsearch-check-serverless-submodule\n  description: Validate elasticsearch changes against serverless\n  links:\n    - title: Pipeline\n      url: https://buildkite.com/elastic/elasticsearch-check-serverless-submodule\nspec:\n  type: buildkite-pipeline\n  system: buildkite\n  owner: group:elasticsearch-team\n  implementation:\n    apiVersion: buildkite.elastic.dev/v1\n    kind: Pipeline\n    metadata:\n      description: \":elasticsearch: Validate elasticsearch changes against serverless\"\n      name: elasticsearch / check serverless submodule\n    spec:\n      repository: elastic/elasticsearch\n      pipeline_file: .buildkite/check-es-serverless.yml\n      branch_configuration: main\n      teams:\n        elasticsearch-team: {}\n        ml-core: {}\n        everyone:\n          access_level: READ_ONLY\n      provider_settings:\n        build_pull_requests: false\n        publish_commit_status: false\n---\n# yaml-language-server: $schema=https://gist.githubusercontent.com/elasticmachine/988b80dae436cafea07d9a4a460a011d/raw/e57ee3bed7a6f73077a3f55a38e76e40ec87a7cf/rre.schema.json\napiVersion: backstage.io/v1alpha1\nkind: Resource\nmetadata:\n  name: buildkite-pipeline-elasticsearch-periodic\n  description: Elasticsearch tests and checks that are run a few times daily\n  links:\n    - title: Pipeline\n      url: https://buildkite.com/elastic/elasticsearch-periodic\nspec:\n  type: buildkite-pipeline\n  system: buildkite\n  owner: group:elasticsearch-team\n  implementation:\n    apiVersion: buildkite.elastic.dev/v1\n    kind: Pipeline\n    metadata:\n      description: \":elasticsearch: Tests and checks that are run a few times daily\"\n      name: elasticsearch / periodic\n    spec:\n      repository: elastic/elasticsearch\n      pipeline_file: .buildkite/pipelines/periodic.yml\n      branch_configuration: main\n      teams:\n        elasticsearch-team: {}\n        ml-core: {}\n        everyone:\n          access_level: BUILD_AND_READ\n      provider_settings:\n        build_branches: false\n        build_pull_requests: false\n        publish_commit_status: false\n        trigger_mode: none\n---\n# yaml-language-server: $schema=https://gist.githubusercontent.com/elasticmachine/988b80dae436cafea07d9a4a460a011d/raw/e57ee3bed7a6f73077a3f55a38e76e40ec87a7cf/rre.schema.json\napiVersion: backstage.io/v1alpha1\nkind: Resource\nmetadata:\n  name: buildkite-pipeline-elasticsearch-lucene-snapshot-build\n  description: Builds a new lucene snapshot, uploads, updates the lucene_snapshot branch in ES, runs tests\n  links:\n    - title: Pipeline\n      url: https://buildkite.com/elastic/elasticsearch-lucene-snapshot-build\nspec:\n  type: buildkite-pipeline\n  system: buildkite\n  owner: group:elasticsearch-team\n  implementation:\n    apiVersion: buildkite.elastic.dev/v1\n    kind: Pipeline\n    metadata:\n      description: \":elasticsearch: Builds a new lucene snapshot and tests it\"\n      name: elasticsearch / lucene-snapshot / build-and-update\n    spec:\n      repository: elastic/elasticsearch\n      pipeline_file: .buildkite/pipelines/lucene-snapshot/build-snapshot.yml\n      env:\n        ELASTIC_SLACK_NOTIFICATIONS_ENABLED: \"true\"\n        SLACK_NOTIFICATIONS_CHANNEL: \"#lucene\"\n        SLACK_NOTIFICATIONS_ALL_BRANCHES: \"true\"\n      branch_configuration: lucene_snapshot\n      default_branch: lucene_snapshot\n      teams:\n        elasticsearch-team: {}\n        ml-core: {}\n        everyone:\n          access_level: BUILD_AND_READ\n      provider_settings:\n        build_branches: false\n        build_pull_requests: false\n        publish_commit_status: false\n        trigger_mode: none\n      schedules:\n        Periodically on lucene_snapshot:\n          branch: lucene_snapshot\n          cronline: \"0 2 * * * America/New_York\"\n          message: \"Builds a new lucene snapshot 1x per day\"\n---\n# yaml-language-server: $schema=https://gist.githubusercontent.com/elasticmachine/988b80dae436cafea07d9a4a460a011d/raw/e57ee3bed7a6f73077a3f55a38e76e40ec87a7cf/rre.schema.json\napiVersion: backstage.io/v1alpha1\nkind: Resource\nmetadata:\n  name: buildkite-pipeline-elasticsearch-lucene-snapshot-update-branch\n  description: Merge main into the lucene_snapshot branch, and run tests\n  links:\n    - title: Pipeline\n      url: https://buildkite.com/elastic/elasticsearch-lucene-snapshot-update-branch\nspec:\n  type: buildkite-pipeline\n  system: buildkite\n  owner: group:elasticsearch-team\n  implementation:\n    apiVersion: buildkite.elastic.dev/v1\n    kind: Pipeline\n    metadata:\n      description: \":elasticsearch: Merges main into lucene_snapshot branch and runs tests\"\n      name: elasticsearch / lucene-snapshot / update-branch\n    spec:\n      repository: elastic/elasticsearch\n      pipeline_file: .buildkite/pipelines/lucene-snapshot/update-branch.yml\n      env:\n        ELASTIC_SLACK_NOTIFICATIONS_ENABLED: \"true\"\n        SLACK_NOTIFICATIONS_CHANNEL: \"#lucene\"\n        SLACK_NOTIFICATIONS_ALL_BRANCHES: \"true\"\n      default_branch: lucene_snapshot\n      teams:\n        elasticsearch-team: {}\n        ml-core: {}\n        everyone:\n          access_level: BUILD_AND_READ\n      provider_settings:\n        build_branches: false\n        build_pull_requests: false\n        publish_commit_status: false\n        trigger_mode: none\n      schedules:\n        Periodically on lucene_snapshot:\n          branch: lucene_snapshot\n          cronline: \"0 6 * * * America/New_York\"\n          message: \"Merges main into lucene_snapshot branch 1x per day\"\n---\n# yaml-language-server: $schema=https://gist.githubusercontent.com/elasticmachine/988b80dae436cafea07d9a4a460a011d/raw/e57ee3bed7a6f73077a3f55a38e76e40ec87a7cf/rre.schema.json\napiVersion: backstage.io/v1alpha1\nkind: Resource\nmetadata:\n  name: buildkite-pipeline-elasticsearch-lucene-snapshot-tests\n  description: Runs tests against lucene_snapshot branch\n  links:\n    - title: Pipeline\n      url: https://buildkite.com/elastic/elasticsearch-lucene-snapshot-tests\nspec:\n  type: buildkite-pipeline\n  system: buildkite\n  owner: group:elasticsearch-team\n  implementation:\n    apiVersion: buildkite.elastic.dev/v1\n    kind: Pipeline\n    metadata:\n      description: \":elasticsearch: Runs tests against lucene_snapshot branch\"\n      name: elasticsearch / lucene-snapshot / tests\n    spec:\n      repository: elastic/elasticsearch\n      pipeline_file: .buildkite/pipelines/lucene-snapshot/run-tests.yml\n      env:\n        ELASTIC_SLACK_NOTIFICATIONS_ENABLED: \"true\"\n        SLACK_NOTIFICATIONS_CHANNEL: \"#lucene\"\n        SLACK_NOTIFICATIONS_ALL_BRANCHES: \"true\"\n      branch_configuration: lucene_snapshot\n      default_branch: lucene_snapshot\n      teams:\n        elasticsearch-team: {}\n        ml-core: {}\n        everyone:\n          access_level: BUILD_AND_READ\n      provider_settings:\n        build_branches: false\n        build_pull_requests: false\n        publish_commit_status: false\n        trigger_mode: none\n      schedules:\n        Periodically on lucene_snapshot:\n          branch: lucene_snapshot\n          cronline: \"0 9,12,15,18 * * * America/New_York\"\n          message: \"Runs tests against lucene_snapshot branch several times per day\"\n---\n# yaml-language-server: $schema=https://gist.githubusercontent.com/elasticmachine/988b80dae436cafea07d9a4a460a011d/raw/e57ee3bed7a6f73077a3f55a38e76e40ec87a7cf/rre.schema.json\napiVersion: backstage.io/v1alpha1\nkind: Resource\nmetadata:\n  name: buildkite-pipeline-elasticsearch-ecs-dynamic-template-tests\n  description: Runs ECS dynamic template tests against main branch\n  links:\n    - title: Pipeline\n      url: https://buildkite.com/elastic/elasticsearch-ecs-dynamic-template-tests\nspec:\n  type: buildkite-pipeline\n  system: buildkite\n  owner: group:elasticsearch-team\n  implementation:\n    apiVersion: buildkite.elastic.dev/v1\n    kind: Pipeline\n    metadata:\n      description: \":elasticsearch: ECS dynamic template tests against main branch\"\n      name: elasticsearch / ecs-dynamic-templates / tests\n    spec:\n      repository: elastic/elasticsearch\n      pipeline_file: .buildkite/pipelines/ecs-dynamic-template-tests.yml\n      provider_settings:\n        trigger_mode: none\n      teams:\n        elasticsearch-team: {}\n        ml-core: {}\n        everyone:\n          access_level: READ_ONLY\n      schedules:\n        Daily:\n          branch: main\n          cronline: \"0 12 * * * America/New_York\"\n"
        },
        {
          "name": "client",
          "type": "tree",
          "content": null
        },
        {
          "name": "dev-tools",
          "type": "tree",
          "content": null
        },
        {
          "name": "distribution",
          "type": "tree",
          "content": null
        },
        {
          "name": "docs-mdx",
          "type": "tree",
          "content": null
        },
        {
          "name": "docs",
          "type": "tree",
          "content": null
        },
        {
          "name": "gradle.properties",
          "type": "blob",
          "size": 1.1650390625,
          "content": "org.gradle.welcome=never\norg.gradle.warning.mode=none\norg.gradle.parallel=true\n# We need to declare --add-exports to make spotless working seamlessly with jdk16\norg.gradle.jvmargs=-XX:+HeapDumpOnOutOfMemoryError -Xss2m  --add-exports jdk.compiler/com.sun.tools.javac.util=ALL-UNNAMED --add-exports jdk.compiler/com.sun.tools.javac.file=ALL-UNNAMED --add-exports jdk.compiler/com.sun.tools.javac.parser=ALL-UNNAMED --add-exports jdk.compiler/com.sun.tools.javac.tree=ALL-UNNAMED --add-exports jdk.compiler/com.sun.tools.javac.api=ALL-UNNAMED --add-opens java.base/java.time=ALL-UNNAMED\n\n# Enforce the build to fail on deprecated gradle api usage\nsystemProp.org.gradle.warning.mode=fail\n\n# forcing to use TLS1.2 to avoid failure in vault\n# see https://github.com/hashicorp/vault/issues/8750#issuecomment-631236121\nsystemProp.jdk.tls.client.protocols=TLSv1.2\n\n# java homes resolved by environment variables\norg.gradle.java.installations.auto-detect=false\n\n# log some dependency verification info to console\norg.gradle.dependency.verification.console=verbose\n\n# allow user to specify toolchain via the RUNTIME_JAVA_HOME environment variable\norg.gradle.java.installations.fromEnv=RUNTIME_JAVA_HOME\n"
        },
        {
          "name": "gradle",
          "type": "tree",
          "content": null
        },
        {
          "name": "gradlew",
          "type": "blob",
          "size": 8.5576171875,
          "content": "#!/bin/sh\n\n#\n# Copyright © 2015-2021 the original authors.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n# SPDX-License-Identifier: Apache-2.0\n#\n\n##############################################################################\n#\n#   Gradle start up script for POSIX generated by Gradle.\n#\n#   Important for running:\n#\n#   (1) You need a POSIX-compliant shell to run this script. If your /bin/sh is\n#       noncompliant, but you have some other compliant shell such as ksh or\n#       bash, then to run this script, type that shell name before the whole\n#       command line, like:\n#\n#           ksh Gradle\n#\n#       Busybox and similar reduced shells will NOT work, because this script\n#       requires all of these POSIX shell features:\n#         * functions;\n#         * expansions «$var», «${var}», «${var:-default}», «${var+SET}»,\n#           «${var#prefix}», «${var%suffix}», and «$( cmd )»;\n#         * compound commands having a testable exit status, especially «case»;\n#         * various built-in commands including «command», «set», and «ulimit».\n#\n#   Important for patching:\n#\n#   (2) This script targets any POSIX shell, so it avoids extensions provided\n#       by Bash, Ksh, etc; in particular arrays are avoided.\n#\n#       The \"traditional\" practice of packing multiple parameters into a\n#       space-separated string is a well documented source of bugs and security\n#       problems, so this is (mostly) avoided, by progressively accumulating\n#       options in \"$@\", and eventually passing that to Java.\n#\n#       Where the inherited environment variables (DEFAULT_JVM_OPTS, JAVA_OPTS,\n#       and GRADLE_OPTS) rely on word-splitting, this is performed explicitly;\n#       see the in-line comments for details.\n#\n#       There are tweaks for specific operating systems such as AIX, CygWin,\n#       Darwin, MinGW, and NonStop.\n#\n#   (3) This script is generated from the Groovy template\n#       https://github.com/gradle/gradle/blob/HEAD/platforms/jvm/plugins-application/src/main/resources/org/gradle/api/internal/plugins/unixStartScript.txt\n#       within the Gradle project.\n#\n#       You can find Gradle at https://github.com/gradle/gradle/.\n#\n##############################################################################\n\n# Attempt to set APP_HOME\n\n# Resolve links: $0 may be a link\napp_path=$0\n\n# Need this for daisy-chained symlinks.\nwhile\n    APP_HOME=${app_path%\"${app_path##*/}\"}  # leaves a trailing /; empty if no leading path\n    [ -h \"$app_path\" ]\ndo\n    ls=$( ls -ld \"$app_path\" )\n    link=${ls#*' -> '}\n    case $link in             #(\n      /*)   app_path=$link ;; #(\n      *)    app_path=$APP_HOME$link ;;\n    esac\ndone\n\n# This is normally unused\n# shellcheck disable=SC2034\nAPP_BASE_NAME=${0##*/}\n# Discard cd standard output in case $CDPATH is set (https://github.com/gradle/gradle/issues/25036)\nAPP_HOME=$( cd -P \"${APP_HOME:-./}\" > /dev/null && printf '%s\\n' \"$PWD\" ) || exit\n\n# Use the maximum available, or set MAX_FD != -1 to use that value.\nMAX_FD=maximum\n\nwarn () {\n    echo \"$*\"\n} >&2\n\ndie () {\n    echo\n    echo \"$*\"\n    echo\n    exit 1\n} >&2\n\n# OS specific support (must be 'true' or 'false').\ncygwin=false\nmsys=false\ndarwin=false\nnonstop=false\ncase \"$( uname )\" in                #(\n  CYGWIN* )         cygwin=true  ;; #(\n  Darwin* )         darwin=true  ;; #(\n  MSYS* | MINGW* )  msys=true    ;; #(\n  NONSTOP* )        nonstop=true ;;\nesac\n\nCLASSPATH=$APP_HOME/gradle/wrapper/gradle-wrapper.jar\n\n\n# Determine the Java command to use to start the JVM.\nif [ -n \"$JAVA_HOME\" ] ; then\n    if [ -x \"$JAVA_HOME/jre/sh/java\" ] ; then\n        # IBM's JDK on AIX uses strange locations for the executables\n        JAVACMD=$JAVA_HOME/jre/sh/java\n    else\n        JAVACMD=$JAVA_HOME/bin/java\n    fi\n    if [ ! -x \"$JAVACMD\" ] ; then\n        die \"ERROR: JAVA_HOME is set to an invalid directory: $JAVA_HOME\n\nPlease set the JAVA_HOME variable in your environment to match the\nlocation of your Java installation.\"\n    fi\nelse\n    JAVACMD=java\n    if ! command -v java >/dev/null 2>&1\n    then\n        die \"ERROR: JAVA_HOME is not set and no 'java' command could be found in your PATH.\n\nPlease set the JAVA_HOME variable in your environment to match the\nlocation of your Java installation.\"\n    fi\nfi\n\n# Increase the maximum file descriptors if we can.\nif ! \"$cygwin\" && ! \"$darwin\" && ! \"$nonstop\" ; then\n    case $MAX_FD in #(\n      max*)\n        # In POSIX sh, ulimit -H is undefined. That's why the result is checked to see if it worked.\n        # shellcheck disable=SC2039,SC3045\n        MAX_FD=$( ulimit -H -n ) ||\n            warn \"Could not query maximum file descriptor limit\"\n    esac\n    case $MAX_FD in  #(\n      '' | soft) :;; #(\n      *)\n        # In POSIX sh, ulimit -n is undefined. That's why the result is checked to see if it worked.\n        # shellcheck disable=SC2039,SC3045\n        ulimit -n \"$MAX_FD\" ||\n            warn \"Could not set maximum file descriptor limit to $MAX_FD\"\n    esac\nfi\n\n# Collect all arguments for the java command, stacking in reverse order:\n#   * args from the command line\n#   * the main class name\n#   * -classpath\n#   * -D...appname settings\n#   * --module-path (only if needed)\n#   * DEFAULT_JVM_OPTS, JAVA_OPTS, and GRADLE_OPTS environment variables.\n\n# For Cygwin or MSYS, switch paths to Windows format before running java\nif \"$cygwin\" || \"$msys\" ; then\n    APP_HOME=$( cygpath --path --mixed \"$APP_HOME\" )\n    CLASSPATH=$( cygpath --path --mixed \"$CLASSPATH\" )\n\n    JAVACMD=$( cygpath --unix \"$JAVACMD\" )\n\n    # Now convert the arguments - kludge to limit ourselves to /bin/sh\n    for arg do\n        if\n            case $arg in                                #(\n              -*)   false ;;                            # don't mess with options #(\n              /?*)  t=${arg#/} t=/${t%%/*}              # looks like a POSIX filepath\n                    [ -e \"$t\" ] ;;                      #(\n              *)    false ;;\n            esac\n        then\n            arg=$( cygpath --path --ignore --mixed \"$arg\" )\n        fi\n        # Roll the args list around exactly as many times as the number of\n        # args, so each arg winds up back in the position where it started, but\n        # possibly modified.\n        #\n        # NB: a `for` loop captures its iteration list before it begins, so\n        # changing the positional parameters here affects neither the number of\n        # iterations, nor the values presented in `arg`.\n        shift                   # remove old arg\n        set -- \"$@\" \"$arg\"      # push replacement arg\n    done\nfi\n\n\n# Add default JVM options here. You can also use JAVA_OPTS and GRADLE_OPTS to pass JVM options to this script.\nDEFAULT_JVM_OPTS='\"-Xmx64m\" \"-Xms64m\"'\n\n# Collect all arguments for the java command:\n#   * DEFAULT_JVM_OPTS, JAVA_OPTS, JAVA_OPTS, and optsEnvironmentVar are not allowed to contain shell fragments,\n#     and any embedded shellness will be escaped.\n#   * For example: A user cannot expect ${Hostname} to be expanded, as it is an environment variable and will be\n#     treated as '${Hostname}' itself on the command line.\n\nset -- \\\n        \"-Dorg.gradle.appname=$APP_BASE_NAME\" \\\n        -classpath \"$CLASSPATH\" \\\n        org.gradle.wrapper.GradleWrapperMain \\\n        \"$@\"\n\n# Stop when \"xargs\" is not available.\nif ! command -v xargs >/dev/null 2>&1\nthen\n    die \"xargs is not available\"\nfi\n\n# Use \"xargs\" to parse quoted args.\n#\n# With -n1 it outputs one arg per line, with the quotes and backslashes removed.\n#\n# In Bash we could simply go:\n#\n#   readarray ARGS < <( xargs -n1 <<<\"$var\" ) &&\n#   set -- \"${ARGS[@]}\" \"$@\"\n#\n# but POSIX shell has neither arrays nor command substitution, so instead we\n# post-process each arg (as a line of input to sed) to backslash-escape any\n# character that might be a shell metacharacter, then use eval to reverse\n# that process (while maintaining the separation between arguments), and wrap\n# the whole thing up as a single \"set\" statement.\n#\n# This will of course break if any of these variables contains a newline or\n# an unmatched quote.\n#\n\neval \"set -- $(\n        printf '%s\\n' \"$DEFAULT_JVM_OPTS $JAVA_OPTS $GRADLE_OPTS\" |\n        xargs -n1 |\n        sed ' s~[^-[:alnum:]+,./:=@_]~\\\\&~g; ' |\n        tr '\\n' ' '\n    )\" '\"$@\"'\n\nexec \"$JAVACMD\" \"$@\"\n"
        },
        {
          "name": "gradlew.bat",
          "type": "blob",
          "size": 2.896484375,
          "content": "@rem\r\n@rem Copyright 2015 the original author or authors.\r\n@rem\r\n@rem Licensed under the Apache License, Version 2.0 (the \"License\");\r\n@rem you may not use this file except in compliance with the License.\r\n@rem You may obtain a copy of the License at\r\n@rem\r\n@rem      https://www.apache.org/licenses/LICENSE-2.0\r\n@rem\r\n@rem Unless required by applicable law or agreed to in writing, software\r\n@rem distributed under the License is distributed on an \"AS IS\" BASIS,\r\n@rem WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n@rem See the License for the specific language governing permissions and\r\n@rem limitations under the License.\r\n@rem\r\n@rem SPDX-License-Identifier: Apache-2.0\r\n@rem\r\n\r\n@if \"%DEBUG%\"==\"\" @echo off\r\n@rem ##########################################################################\r\n@rem\r\n@rem  Gradle startup script for Windows\r\n@rem\r\n@rem ##########################################################################\r\n\r\n@rem Set local scope for the variables with windows NT shell\r\nif \"%OS%\"==\"Windows_NT\" setlocal\r\n\r\nset DIRNAME=%~dp0\r\nif \"%DIRNAME%\"==\"\" set DIRNAME=.\r\n@rem This is normally unused\r\nset APP_BASE_NAME=%~n0\r\nset APP_HOME=%DIRNAME%\r\n\r\n@rem Resolve any \".\" and \"..\" in APP_HOME to make it shorter.\r\nfor %%i in (\"%APP_HOME%\") do set APP_HOME=%%~fi\r\n\r\n@rem Add default JVM options here. You can also use JAVA_OPTS and GRADLE_OPTS to pass JVM options to this script.\r\nset DEFAULT_JVM_OPTS=\"-Xmx64m\" \"-Xms64m\"\r\n\r\n@rem Find java.exe\r\nif defined JAVA_HOME goto findJavaFromJavaHome\r\n\r\nset JAVA_EXE=java.exe\r\n%JAVA_EXE% -version >NUL 2>&1\r\nif %ERRORLEVEL% equ 0 goto execute\r\n\r\necho. 1>&2\r\necho ERROR: JAVA_HOME is not set and no 'java' command could be found in your PATH. 1>&2\r\necho. 1>&2\r\necho Please set the JAVA_HOME variable in your environment to match the 1>&2\r\necho location of your Java installation. 1>&2\r\n\r\ngoto fail\r\n\r\n:findJavaFromJavaHome\r\nset JAVA_HOME=%JAVA_HOME:\"=%\r\nset JAVA_EXE=%JAVA_HOME%/bin/java.exe\r\n\r\nif exist \"%JAVA_EXE%\" goto execute\r\n\r\necho. 1>&2\r\necho ERROR: JAVA_HOME is set to an invalid directory: %JAVA_HOME% 1>&2\r\necho. 1>&2\r\necho Please set the JAVA_HOME variable in your environment to match the 1>&2\r\necho location of your Java installation. 1>&2\r\n\r\ngoto fail\r\n\r\n:execute\r\n@rem Setup the command line\r\n\r\nset CLASSPATH=%APP_HOME%\\gradle\\wrapper\\gradle-wrapper.jar\r\n\r\n\r\n@rem Execute Gradle\r\n\"%JAVA_EXE%\" %DEFAULT_JVM_OPTS% %JAVA_OPTS% %GRADLE_OPTS% \"-Dorg.gradle.appname=%APP_BASE_NAME%\" -classpath \"%CLASSPATH%\" org.gradle.wrapper.GradleWrapperMain %*\r\n\r\n:end\r\n@rem End local scope for the variables with windows NT shell\r\nif %ERRORLEVEL% equ 0 goto mainEnd\r\n\r\n:fail\r\nrem Set variable GRADLE_EXIT_CONSOLE if you need the _script_ return code instead of\r\nrem the _cmd.exe /c_ return code!\r\nset EXIT_CODE=%ERRORLEVEL%\r\nif %EXIT_CODE% equ 0 set EXIT_CODE=1\r\nif not \"\"==\"%GRADLE_EXIT_CONSOLE%\" exit %EXIT_CODE%\r\nexit /b %EXIT_CODE%\r\n\r\n:mainEnd\r\nif \"%OS%\"==\"Windows_NT\" endlocal\r\n\r\n:omega\r\n"
        },
        {
          "name": "libs",
          "type": "tree",
          "content": null
        },
        {
          "name": "licenses",
          "type": "tree",
          "content": null
        },
        {
          "name": "modules",
          "type": "tree",
          "content": null
        },
        {
          "name": "muted-tests.yml",
          "type": "blob",
          "size": 18.3056640625,
          "content": "tests:\n- class: \"org.elasticsearch.client.RestClientSingleHostIntegTests\"\n  issue: \"https://github.com/elastic/elasticsearch/issues/102717\"\n  method: \"testRequestResetAndAbort\"\n- class: org.elasticsearch.smoketest.WatcherYamlRestIT\n  method: test {p0=watcher/usage/10_basic/Test watcher usage stats output}\n  issue: https://github.com/elastic/elasticsearch/issues/112189\n- class: org.elasticsearch.ingest.geoip.IngestGeoIpClientYamlTestSuiteIT\n  issue: https://github.com/elastic/elasticsearch/issues/111497\n- class: org.elasticsearch.packaging.test.PackagesSecurityAutoConfigurationTests\n  method: test20SecurityNotAutoConfiguredOnReInstallation\n  issue: https://github.com/elastic/elasticsearch/issues/112635\n- class: org.elasticsearch.xpack.sql.qa.single_node.JdbcSqlSpecIT\n  method: test {case-functions.testSelectInsertWithLcaseAndLengthWithOrderBy}\n  issue: https://github.com/elastic/elasticsearch/issues/112642\n- class: org.elasticsearch.xpack.sql.qa.single_node.JdbcSqlSpecIT\n  method: test {case-functions.testUcaseInline1}\n  issue: https://github.com/elastic/elasticsearch/issues/112641\n- class: org.elasticsearch.xpack.sql.qa.single_node.JdbcSqlSpecIT\n  method: test {case-functions.testUpperCasingTheSecondLetterFromTheRightFromFirstName}\n  issue: https://github.com/elastic/elasticsearch/issues/112640\n- class: org.elasticsearch.xpack.sql.qa.single_node.JdbcSqlSpecIT\n  method: test {case-functions.testUcaseInline3}\n  issue: https://github.com/elastic/elasticsearch/issues/112643\n- class: org.elasticsearch.xpack.sql.qa.security.JdbcSqlSpecIT\n  method: test {case-functions.testUcaseInline1}\n  issue: https://github.com/elastic/elasticsearch/issues/112641\n- class: org.elasticsearch.xpack.sql.qa.security.JdbcSqlSpecIT\n  method: test {case-functions.testUcaseInline3}\n  issue: https://github.com/elastic/elasticsearch/issues/112643\n- class: org.elasticsearch.xpack.sql.qa.security.JdbcSqlSpecIT\n  method: test {case-functions.testUpperCasingTheSecondLetterFromTheRightFromFirstName}\n  issue: https://github.com/elastic/elasticsearch/issues/112640\n- class: org.elasticsearch.xpack.sql.qa.security.JdbcSqlSpecIT\n  method: test {case-functions.testSelectInsertWithLcaseAndLengthWithOrderBy}\n  issue: https://github.com/elastic/elasticsearch/issues/112642\n- class: org.elasticsearch.packaging.test.WindowsServiceTests\n  method: test30StartStop\n  issue: https://github.com/elastic/elasticsearch/issues/113160\n- class: org.elasticsearch.packaging.test.WindowsServiceTests\n  method: test33JavaChanged\n  issue: https://github.com/elastic/elasticsearch/issues/113177\n- class: org.elasticsearch.packaging.test.WindowsServiceTests\n  method: test80JavaOptsInEnvVar\n  issue: https://github.com/elastic/elasticsearch/issues/113219\n- class: org.elasticsearch.packaging.test.WindowsServiceTests\n  method: test81JavaOptsInJvmOptions\n  issue: https://github.com/elastic/elasticsearch/issues/113313\n- class: org.elasticsearch.backwards.MixedClusterClientYamlTestSuiteIT\n  method: test {p0=mtermvectors/10_basic/Tests catching other exceptions per item}\n  issue: https://github.com/elastic/elasticsearch/issues/113325\n- class: org.elasticsearch.xpack.transform.integration.TransformIT\n  method: testStopWaitForCheckpoint\n  issue: https://github.com/elastic/elasticsearch/issues/106113\n- class: org.elasticsearch.kibana.KibanaThreadPoolIT\n  method: testBlockedThreadPoolsRejectUserRequests\n  issue: https://github.com/elastic/elasticsearch/issues/113939\n- class: org.elasticsearch.xpack.inference.TextEmbeddingCrudIT\n  method: testPutE5Small_withPlatformAgnosticVariant\n  issue: https://github.com/elastic/elasticsearch/issues/113983\n- class: org.elasticsearch.xpack.inference.TextEmbeddingCrudIT\n  method: testPutE5WithTrainedModelAndInference\n  issue: https://github.com/elastic/elasticsearch/issues/114023\n- class: org.elasticsearch.xpack.inference.TextEmbeddingCrudIT\n  method: testPutE5Small_withPlatformSpecificVariant\n  issue: https://github.com/elastic/elasticsearch/issues/113950\n- class: org.elasticsearch.xpack.remotecluster.RemoteClusterSecurityWithApmTracingRestIT\n  method: testTracingCrossCluster\n  issue: https://github.com/elastic/elasticsearch/issues/112731\n- class: org.elasticsearch.xpack.inference.DefaultEndPointsIT\n  method: testInferDeploysDefaultE5\n  issue: https://github.com/elastic/elasticsearch/issues/115361\n- class: org.elasticsearch.xpack.restart.MLModelDeploymentFullClusterRestartIT\n  method: testDeploymentSurvivesRestart {cluster=UPGRADED}\n  issue: https://github.com/elastic/elasticsearch/issues/115528\n- class: org.elasticsearch.xpack.shutdown.NodeShutdownIT\n  method: testStalledShardMigrationProperlyDetected\n  issue: https://github.com/elastic/elasticsearch/issues/115697\n- class: org.elasticsearch.xpack.test.rest.XPackRestIT\n  method: test {p0=transform/transforms_start_stop/Verify start transform reuses destination index}\n  issue: https://github.com/elastic/elasticsearch/issues/115808\n- class: org.elasticsearch.search.StressSearchServiceReaperIT\n  method: testStressReaper\n  issue: https://github.com/elastic/elasticsearch/issues/115816\n- class: org.elasticsearch.xpack.application.connector.ConnectorIndexServiceTests\n  issue: https://github.com/elastic/elasticsearch/issues/116087\n- class: org.elasticsearch.xpack.test.rest.XPackRestIT\n  method: test {p0=transform/transforms_start_stop/Test start already started transform}\n  issue: https://github.com/elastic/elasticsearch/issues/98802\n- class: org.elasticsearch.action.search.SearchPhaseControllerTests\n  method: testProgressListener\n  issue: https://github.com/elastic/elasticsearch/issues/116149\n- class: org.elasticsearch.search.basic.SearchWithRandomDisconnectsIT\n  method: testSearchWithRandomDisconnects\n  issue: https://github.com/elastic/elasticsearch/issues/116175\n- class: org.elasticsearch.xpack.deprecation.DeprecationHttpIT\n  method: testDeprecatedSettingsReturnWarnings\n  issue: https://github.com/elastic/elasticsearch/issues/108628\n- class: org.elasticsearch.xpack.shutdown.NodeShutdownIT\n  method: testAllocationPreventedForRemoval\n  issue: https://github.com/elastic/elasticsearch/issues/116363\n- class: org.elasticsearch.reservedstate.service.RepositoriesFileSettingsIT\n  method: testSettingsApplied\n  issue: https://github.com/elastic/elasticsearch/issues/116694\n- class: org.elasticsearch.xpack.security.authc.ldap.ActiveDirectoryGroupsResolverTests\n  issue: https://github.com/elastic/elasticsearch/issues/116182\n- class: org.elasticsearch.xpack.test.rest.XPackRestIT\n  method: test {p0=snapshot/20_operator_privileges_disabled/Operator only settings can be set and restored by non-operator user when operator privileges is disabled}\n  issue: https://github.com/elastic/elasticsearch/issues/116775\n- class: org.elasticsearch.search.basic.SearchWithRandomIOExceptionsIT\n  method: testRandomDirectoryIOExceptions\n  issue: https://github.com/elastic/elasticsearch/issues/114824\n- class: org.elasticsearch.xpack.apmdata.APMYamlTestSuiteIT\n  method: test {yaml=/10_apm/Test template reinstallation}\n  issue: https://github.com/elastic/elasticsearch/issues/116445\n- class: org.elasticsearch.xpack.inference.DefaultEndPointsIT\n  method: testMultipleInferencesTriggeringDownloadAndDeploy\n  issue: https://github.com/elastic/elasticsearch/issues/117208\n- class: org.elasticsearch.ingest.geoip.EnterpriseGeoIpDownloaderIT\n  method: testEnterpriseDownloaderTask\n  issue: https://github.com/elastic/elasticsearch/issues/115163\n- class: org.elasticsearch.versioning.ConcurrentSeqNoVersioningIT\n  method: testSeqNoCASLinearizability\n  issue: https://github.com/elastic/elasticsearch/issues/117249\n- class: org.elasticsearch.discovery.ClusterDisruptionIT\n  method: testAckedIndexing\n  issue: https://github.com/elastic/elasticsearch/issues/117024\n- class: org.elasticsearch.xpack.test.rest.XPackRestIT\n  method: test {p0=snapshot/10_basic/Create a source only snapshot and then restore it}\n  issue: https://github.com/elastic/elasticsearch/issues/117295\n- class: org.elasticsearch.xpack.inference.DefaultEndPointsIT\n  method: testInferDeploysDefaultElser\n  issue: https://github.com/elastic/elasticsearch/issues/114913\n- class: org.elasticsearch.xpack.inference.InferenceRestIT\n  method: test {p0=inference/40_semantic_text_query/Query a field that uses the default ELSER 2 endpoint}\n  issue: https://github.com/elastic/elasticsearch/issues/117027\n- class: org.elasticsearch.xpack.inference.InferenceRestIT\n  method: test {p0=inference/30_semantic_text_inference/Calculates embeddings using the default ELSER 2 endpoint}\n  issue: https://github.com/elastic/elasticsearch/issues/117349\n- class: org.elasticsearch.xpack.inference.InferenceRestIT\n  method: test {p0=inference/30_semantic_text_inference_bwc/Calculates embeddings using the default ELSER 2 endpoint}\n  issue: https://github.com/elastic/elasticsearch/issues/117349\n- class: org.elasticsearch.xpack.test.rest.XPackRestIT\n  method: test {p0=transform/transforms_reset/Test reset running transform}\n  issue: https://github.com/elastic/elasticsearch/issues/117473\n- class: org.elasticsearch.search.ccs.CrossClusterIT\n  method: testCancel\n  issue: https://github.com/elastic/elasticsearch/issues/108061\n- class: org.elasticsearch.test.rest.yaml.CcsCommonYamlTestSuiteIT\n  method: test {p0=search.highlight/50_synthetic_source/text multi unified from vectors}\n  issue: https://github.com/elastic/elasticsearch/issues/117815\n- class: org.elasticsearch.xpack.esql.plugin.ClusterRequestTests\n  method: testFallbackIndicesOptions\n  issue: https://github.com/elastic/elasticsearch/issues/117937\n- class: org.elasticsearch.xpack.ml.integration.RegressionIT\n  method: testTwoJobsWithSameRandomizeSeedUseSameTrainingSet\n  issue: https://github.com/elastic/elasticsearch/issues/117805\n- class: org.elasticsearch.xpack.remotecluster.CrossClusterEsqlRCS2UnavailableRemotesIT\n  method: testEsqlRcs2UnavailableRemoteScenarios\n  issue: https://github.com/elastic/elasticsearch/issues/117419\n- class: org.elasticsearch.xpack.inference.DefaultEndPointsIT\n  method: testInferDeploysDefaultRerank\n  issue: https://github.com/elastic/elasticsearch/issues/118184\n- class: org.elasticsearch.xpack.esql.action.EsqlActionTaskIT\n  method: testCancelRequestWhenFailingFetchingPages\n  issue: https://github.com/elastic/elasticsearch/issues/118193\n- class: org.elasticsearch.packaging.test.ArchiveTests\n  method: test44AutoConfigurationNotTriggeredOnNotWriteableConfDir\n  issue: https://github.com/elastic/elasticsearch/issues/118208\n- class: org.elasticsearch.packaging.test.ArchiveTests\n  method: test51AutoConfigurationWithPasswordProtectedKeystore\n  issue: https://github.com/elastic/elasticsearch/issues/118212\n- class: org.elasticsearch.datastreams.DataStreamsClientYamlTestSuiteIT\n  method: test {p0=data_stream/120_data_streams_stats/Multiple data stream}\n  issue: https://github.com/elastic/elasticsearch/issues/118217\n- class: org.elasticsearch.action.search.SearchQueryThenFetchAsyncActionTests\n  method: testBottomFieldSort\n  issue: https://github.com/elastic/elasticsearch/issues/118214\n- class: org.elasticsearch.xpack.remotecluster.CrossClusterEsqlRCS1UnavailableRemotesIT\n  method: testEsqlRcs1UnavailableRemoteScenarios\n  issue: https://github.com/elastic/elasticsearch/issues/118350\n- class: org.elasticsearch.xpack.searchablesnapshots.RetrySearchIntegTests\n  method: testSearcherId\n  issue: https://github.com/elastic/elasticsearch/issues/118374\n- class: org.elasticsearch.xpack.esql.action.EsqlActionBreakerIT\n  issue: https://github.com/elastic/elasticsearch/issues/118238\n- class: org.elasticsearch.reservedstate.service.FileSettingsServiceTests\n  method: testInvalidJSON\n  issue: https://github.com/elastic/elasticsearch/issues/116521\n- class: org.elasticsearch.xpack.ccr.rest.ShardChangesRestIT\n  method: testShardChangesNoOperation\n  issue: https://github.com/elastic/elasticsearch/issues/118800\n- class: org.elasticsearch.smoketest.DocsClientYamlTestSuiteIT\n  method: test {yaml=reference/indices/shard-stores/line_150}\n  issue: https://github.com/elastic/elasticsearch/issues/118896\n- class: org.elasticsearch.cluster.service.MasterServiceTests\n  method: testThreadContext\n  issue: https://github.com/elastic/elasticsearch/issues/118914\n- class: org.elasticsearch.xpack.security.authc.AuthenticationServiceTests\n  method: testInvalidToken\n  issue: https://github.com/elastic/elasticsearch/issues/119019\n- class: org.elasticsearch.xpack.security.authc.ldap.ActiveDirectoryRunAsIT\n  issue: https://github.com/elastic/elasticsearch/issues/115727\n- class: org.elasticsearch.smoketest.DocsClientYamlTestSuiteIT\n  method: test {yaml=reference/search/search-your-data/retrievers-examples/line_98}\n  issue: https://github.com/elastic/elasticsearch/issues/119155\n- class: org.elasticsearch.xpack.esql.action.EsqlNodeFailureIT\n  method: testFailureLoadingFields\n  issue: https://github.com/elastic/elasticsearch/issues/118000\n- class: org.elasticsearch.index.mapper.AbstractShapeGeometryFieldMapperTests\n  method: testCartesianBoundsBlockLoader\n  issue: https://github.com/elastic/elasticsearch/issues/119201\n- class: org.elasticsearch.xpack.test.rest.XPackRestIT\n  method: test {p0=transform/transforms_start_stop/Test start/stop/start transform}\n  issue: https://github.com/elastic/elasticsearch/issues/119508\n- class: org.elasticsearch.smoketest.MlWithSecurityIT\n  method: test {yaml=ml/sparse_vector_search/Test sparse_vector search with query vector and pruning config}\n  issue: https://github.com/elastic/elasticsearch/issues/119548\n- class: org.elasticsearch.index.engine.LuceneSyntheticSourceChangesSnapshotTests\n  method: testSkipNonRootOfNestedDocuments\n  issue: https://github.com/elastic/elasticsearch/issues/119553\n- class: org.elasticsearch.xpack.ml.integration.ForecastIT\n  method: testOverflowToDisk\n  issue: https://github.com/elastic/elasticsearch/issues/117740\n- class: org.elasticsearch.xpack.security.authc.ldap.MultiGroupMappingIT\n  issue: https://github.com/elastic/elasticsearch/issues/119599\n- class: org.elasticsearch.search.profile.dfs.DfsProfilerIT\n  method: testProfileDfs\n  issue: https://github.com/elastic/elasticsearch/issues/119711\n- class: org.elasticsearch.xpack.esql.optimizer.LocalPhysicalPlanOptimizerTests\n  method: testSingleMatchFunctionFilterPushdownWithStringValues {default}\n  issue: https://github.com/elastic/elasticsearch/issues/119720\n- class: org.elasticsearch.xpack.esql.optimizer.LocalPhysicalPlanOptimizerTests\n  method: testSingleMatchFunctionPushdownWithCasting {default}\n  issue: https://github.com/elastic/elasticsearch/issues/119722\n- class: org.elasticsearch.xpack.esql.optimizer.LocalPhysicalPlanOptimizerTests\n  method: testSingleMatchOperatorFilterPushdownWithStringValues {default}\n  issue: https://github.com/elastic/elasticsearch/issues/119721\n- class: org.elasticsearch.xpack.inference.action.filter.ShardBulkInferenceActionFilterIT\n  method: testBulkOperations {p0=false}\n  issue: https://github.com/elastic/elasticsearch/issues/119901\n- class: org.elasticsearch.xpack.inference.InferenceCrudIT\n  method: testGetServicesWithCompletionTaskType\n  issue: https://github.com/elastic/elasticsearch/issues/119959\n- class: org.elasticsearch.lucene.RollingUpgradeSearchableSnapshotIndexCompatibilityIT\n  method: testSearchableSnapshotUpgrade {p0=[9.0.0, 8.18.0, 8.18.0]}\n  issue: https://github.com/elastic/elasticsearch/issues/119978\n- class: org.elasticsearch.lucene.RollingUpgradeSearchableSnapshotIndexCompatibilityIT\n  method: testSearchableSnapshotUpgrade {p0=[9.0.0, 9.0.0, 8.18.0]}\n  issue: https://github.com/elastic/elasticsearch/issues/119979\n- class: org.elasticsearch.lucene.RollingUpgradeSearchableSnapshotIndexCompatibilityIT\n  method: testMountSearchableSnapshot {p0=[9.0.0, 8.18.0, 8.18.0]}\n  issue: https://github.com/elastic/elasticsearch/issues/119550\n- class: org.elasticsearch.lucene.RollingUpgradeSearchableSnapshotIndexCompatibilityIT\n  method: testMountSearchableSnapshot {p0=[9.0.0, 9.0.0, 8.18.0]}\n  issue: https://github.com/elastic/elasticsearch/issues/119980\n- class: org.elasticsearch.index.codec.vectors.es816.ES816HnswBinaryQuantizedVectorsFormatTests\n  method: testRandomExceptions\n  issue: https://github.com/elastic/elasticsearch/issues/119981\n- class: org.elasticsearch.multi_cluster.MultiClusterYamlTestSuiteIT\n  issue: https://github.com/elastic/elasticsearch/issues/119983\n- class: org.elasticsearch.lucene.RollingUpgradeSearchableSnapshotIndexCompatibilityIT\n  method: testMountSearchableSnapshot {p0=[9.0.0, 9.0.0, 9.0.0]}\n  issue: https://github.com/elastic/elasticsearch/issues/119989\n- class: org.elasticsearch.lucene.RollingUpgradeSearchableSnapshotIndexCompatibilityIT\n  method: testSearchableSnapshotUpgrade {p0=[9.0.0, 9.0.0, 9.0.0]}\n  issue: https://github.com/elastic/elasticsearch/issues/119990\n- class: org.elasticsearch.xpack.test.rest.XPackRestIT\n  method: test {p0=transform/transforms_unattended/Test unattended put and start}\n  issue: https://github.com/elastic/elasticsearch/issues/120019\n\n# Examples:\n#\n#  Mute a single test case in a YAML test suite:\n#  - class: org.elasticsearch.analysis.common.CommonAnalysisClientYamlTestSuiteIT\n#    method: test {yaml=analysis-common/30_tokenizers/letter}\n#    issue: https://github.com/elastic/elasticsearch/...\n#\n#  Mute several methods of a Java test:\n#  - class: org.elasticsearch.common.CharArraysTests\n#    methods:\n#      - testCharsBeginsWith\n#      - testCharsToBytes\n#      - testConstantTimeEquals\n#    issue: https://github.com/elastic/elasticsearch/...\n#\n#  Mute an entire test class:\n#  - class: org.elasticsearch.common.unit.TimeValueTests\n#    issue: https://github.com/elastic/elasticsearch/...\n#\n#  Mute a single method in a test class:\n#  - class: org.elasticsearch.xpack.esql.expression.function.scalar.convert.ToIPTests\n#    method: testCrankyEvaluateBlockWithoutNulls\n#    issue: https://github.com/elastic/elasticsearch/...\n#\n#  Mute a single test in an ES|QL csv-spec test file:\n#  - class: \"org.elasticsearch.xpack.esql.CsvTests\"\n#    method: \"test {union_types.MultiIndexIpStringStatsInline}\"\n#    issue: \"https://github.com/elastic/elasticsearch/...\"\n#  Note that this mutes for the unit-test-like CsvTests only.\n#  Muting all the integration tests can be done using the class \"org.elasticsearch.xpack.esql.**\".\n#  Consider however, that some tests are named as \"test {file.test SYNC}\" and \"ASYNC\" in the integration tests.\n#  To mute all 3 tests safely everywhere use:\n#  - class: \"org.elasticsearch.xpack.esql.**\"\n#    method: \"test {union_types.MultiIndexIpStringStatsInline}\"\n#    issue: \"https://github.com/elastic/elasticsearch/...\"\n#  - class: \"org.elasticsearch.xpack.esql.**\"\n#    method: \"test {union_types.MultiIndexIpStringStatsInline *}\"\n#    issue: \"https://github.com/elastic/elasticsearch/...\"\n"
        },
        {
          "name": "plugins",
          "type": "tree",
          "content": null
        },
        {
          "name": "qa",
          "type": "tree",
          "content": null
        },
        {
          "name": "renovate.json",
          "type": "blob",
          "size": 1.4208984375,
          "content": "{\n  \"$schema\": \"https://docs.renovatebot.com/renovate-schema.json\",\n  \"extends\": [\n    \"github>elastic/renovate-config:only-chainguard\",\n    \":disableDependencyDashboard\"\n  ],\n  \"schedule\": [\n    \"after 1pm on tuesday\"\n  ],\n  \"labels\": [\">non-issue\", \":Delivery/Packaging\", \"Team:Delivery\", \"auto-merge-without-approval\"],\n  \"baseBranches\": [\"main\", \"8.x\", \"8.17\", \"8.16\"],\n  \"packageRules\": [\n    {\n      \"groupName\": \"wolfi (versioned)\",\n      \"groupSlug\": \"wolfi-versioned\",\n      \"description\": \"Override the `groupSlug` to create a non-special-character branch name\",\n      \"matchDatasources\": [\n        \"docker\"\n      ],\n      \"matchPackagePatterns\": [\n        \"^docker.elastic.co/wolfi/chainguard-base$\"\n      ]\n    }\n  ],\n  \"customManagers\": [\n    {\n      \"description\": \"Extract Wolfi images from elasticsearch DockerBase configuration\",\n      \"customType\": \"regex\",\n      \"fileMatch\": [\n        \"build\\\\-tools\\\\-internal\\\\/src\\\\/main\\\\/java\\\\/org\\\\/elasticsearch\\\\/gradle\\\\/internal\\\\/DockerBase\\\\.java$\"\n      ],\n      \"matchStrings\": [\n        \"\\\\s*\\\"?(?<depName>[^\\\\s:@\\\"]+)(?::(?<currentValue>[-a-zA-Z0-9.]+))?(?:@(?<currentDigest>sha256:[a-zA-Z0-9]+))?\\\"?\"\n      ],\n      \"currentValueTemplate\": \"{{#if currentValue}}{{{currentValue}}}{{else}}latest{{/if}}\",\n      \"autoReplaceStringTemplate\": \"{{{depName}}}{{#if newValue}}:{{{newValue}}}{{/if}}{{#if newDigest}}@{{{newDigest}}}{{/if}}\\\"\",\n      \"datasourceTemplate\": \"docker\"\n    }\n  ]\n}\n"
        },
        {
          "name": "rest-api-spec",
          "type": "tree",
          "content": null
        },
        {
          "name": "server",
          "type": "tree",
          "content": null
        },
        {
          "name": "settings.gradle",
          "type": "blob",
          "size": 5.2265625,
          "content": "import org.elasticsearch.gradle.internal.toolchain.OracleOpenJdkToolchainResolver\nimport org.elasticsearch.gradle.internal.toolchain.ArchivedOracleJdkToolchainResolver\nimport org.elasticsearch.gradle.internal.toolchain.AdoptiumJdkToolchainResolver\n\npluginManagement {\n  repositories {\n    mavenCentral()\n    gradlePluginPortal()\n  }\n\n  includeBuild \"build-conventions\"\n  includeBuild \"build-tools\"\n  includeBuild \"build-tools-internal\"\n}\n\nplugins {\n  id \"com.gradle.develocity\" version \"3.18.1\"\n  id 'elasticsearch.java-toolchain'\n}\n\nenableFeaturePreview \"STABLE_CONFIGURATION_CACHE\"\n\nrootProject.name = \"elasticsearch\"\n\ndependencyResolutionManagement {\n  versionCatalogs {\n    buildLibs {\n      from(files(\"gradle/build.versions.toml\"))\n    }\n  }\n}\n\ntoolchainManagement {\n  jvm {\n    javaRepositories {\n      repository('bundledOracleOpendJdk') {\n        resolverClass = OracleOpenJdkToolchainResolver\n      }\n      repository('adoptiumJdks') {\n        resolverClass = AdoptiumJdkToolchainResolver\n      }\n      repository('archivedOracleJdks') {\n        resolverClass = ArchivedOracleJdkToolchainResolver\n      }\n    }\n  }\n}\n\nList projects = [\n  'rest-api-spec',\n  'docs',\n  'client:rest',\n  'client:sniffer',\n  'client:test',\n  'client:client-benchmark-noop-api-plugin',\n  'client:benchmark',\n  'benchmarks',\n  'distribution:archives:integ-test-zip',\n  'distribution:archives:windows-zip',\n  'distribution:archives:darwin-tar',\n  'distribution:archives:darwin-aarch64-tar',\n  'distribution:archives:linux-aarch64-tar',\n  'distribution:archives:linux-tar',\n  'distribution:docker',\n  'distribution:docker:cloud-ess-docker-export',\n  'distribution:docker:cloud-ess-docker-aarch64-export',\n  'distribution:docker:docker-aarch64-export',\n  'distribution:docker:docker-export',\n  'distribution:docker:ironbank-docker-aarch64-export',\n  'distribution:docker:ironbank-docker-export',\n  'distribution:docker:wolfi-docker-aarch64-export',\n  'distribution:docker:wolfi-docker-export',\n  'distribution:packages:aarch64-deb',\n  'distribution:packages:deb',\n  'distribution:packages:aarch64-rpm',\n  'distribution:packages:rpm',\n  'distribution:bwc:bugfix',\n  'distribution:bwc:bugfix2',\n  'distribution:bwc:maintenance',\n  'distribution:bwc:minor',\n  'distribution:bwc:staged',\n  'distribution:tools:java-version-checker',\n  'distribution:tools:cli-launcher',\n  'distribution:tools:server-cli',\n  'distribution:tools:windows-service-cli',\n  'distribution:tools:plugin-cli',\n  'distribution:tools:keystore-cli',\n  'distribution:tools:geoip-cli',\n  'distribution:tools:ansi-console',\n  'server',\n  'test:framework',\n  'test:fixtures:aws-fixture-utils',\n  'test:fixtures:aws-sts-fixture',\n  'test:fixtures:azure-fixture',\n  'test:fixtures:ec2-imds-fixture',\n  'test:fixtures:gcs-fixture',\n  'test:fixtures:hdfs-fixture',\n  'test:fixtures:krb5kdc-fixture',\n  'test:fixtures:minio-fixture',\n  'test:fixtures:old-elasticsearch',\n  'test:fixtures:s3-fixture',\n  'test:fixtures:testcontainer-utils',\n  'test:fixtures:geoip-fixture',\n  'test:fixtures:url-fixture',\n  'test:logger-usage',\n  'test:test-clusters',\n  'test:x-content',\n  'test:yaml-rest-runner',\n  'test:metadata-extractor',\n  'test:immutable-collections-patch'\n]\n\n/**\n * Iterates over sub directories, looking for build.gradle, and adds a project if found\n * for that dir with the given path prefix. Note that this requires each level\n * of the dir hierarchy to have a build.gradle. Otherwise we would have to iterate\n * all files/directories in the source tree to find all projects.\n */\nvoid addSubProjects(String path, File dir) {\n  if (dir.isDirectory() == false) return;\n  if (dir.name == 'buildSrc') return;\n  if (new File(dir, 'build.gradle').exists() == false) return;\n  if (new File(dir, 'settings.gradle').exists()) return;\n  if (findProject(dir) != null) return;\n\n  final String projectName = \"${path}:${dir.name}\"\n\n  // This project has a problem with availability of Docker images after\n  // release. Disabling individual tasks is tricky because it uses test\n  // fixtures, so instead just skip the project entirely until we can\n  // work out a way forward.\n  if (projectName.equals(\":qa:apm\")) {\n    return\n  }\n\n  include projectName\n  if (path.isEmpty() || path.startsWith(':example-plugins')) {\n    project(projectName).projectDir = dir\n  }\n  for (File subdir : dir.listFiles()) {\n    addSubProjects(projectName, subdir)\n  }\n}\n\naddSubProjects('', new File(rootProject.projectDir, 'libs'))\naddSubProjects('', new File(rootProject.projectDir, 'modules'))\naddSubProjects('', new File(rootProject.projectDir, 'plugins'))\naddSubProjects('', new File(rootProject.projectDir, 'qa'))\naddSubProjects('test', new File(rootProject.projectDir, 'test/external-modules'))\naddSubProjects('', new File(rootProject.projectDir, 'x-pack'))\naddSubProjects('', new File(rootProject.projectDir, 'x-pack/libs'))\n\ninclude projects.toArray(new String[0])\n\nproject(\":libs:native:libraries\").name = \"native-libraries\"\n\nproject(\":test:external-modules\").children.each { testProject ->\n  testProject.name = \"test-${testProject.name}\"\n}\n\n// look for extra plugins for elasticsearch\nFile extraProjects = new File(rootProject.projectDir.parentFile, \"${rootProject.projectDir.name}-extra\")\nif (extraProjects.exists()) {\n  for (File extraProjectDir : extraProjects.listFiles()) {\n    addSubProjects('', extraProjectDir)\n  }\n}\n"
        },
        {
          "name": "test",
          "type": "tree",
          "content": null
        },
        {
          "name": "updatecli-compose.yaml",
          "type": "blob",
          "size": 0.65625,
          "content": "# Config file for `updatecli compose ...`.\n# https://www.updatecli.io/docs/core/compose/\npolicies:\n  - name: Handle ironbank bumps\n    policy: ghcr.io/elastic/oblt-updatecli-policies/ironbank/templates:0.3.0@sha256:b0c841d8fb294e6b58359462afbc83070dca375ac5dd0c5216c8926872a98bb1\n    values:\n      - .github/updatecli/values.d/scm.yml\n      - .github/updatecli/values.d/ironbank.yml\n  - name: Update Updatecli policies\n    policy: ghcr.io/updatecli/policies/autodiscovery/updatecli:0.8.0@sha256:99e9e61b501575c2c176c39f2275998d198b590a3f6b1fe829f7315f8d457e7f\n    values:\n      - .github/updatecli/values.d/scm.yml\n      - .github/updatecli/values.d/updatecli-compose.yml\n"
        },
        {
          "name": "x-pack",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}