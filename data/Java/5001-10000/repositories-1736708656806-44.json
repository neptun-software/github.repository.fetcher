{
  "metadata": {
    "timestamp": 1736708656806,
    "page": 44,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjUw",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "nathanmarz/storm",
      "stars": 8824,
      "defaultBranch": "moved-to-apache",
      "files": [
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.2958984375,
          "content": ".lein-repl-history\n/classes\n/lib\ndeploy/lib\ndeploy/logs\n.emacs-project\n*.jar\nbin/jzmq\n.DS_Store\npom.xml\ndeploy/classes\n*.fyc\n*.rbc\n*.pyc\nCHILD\nCHILDMAKER\nNANNY\n\\#project.clj\\#\n.\\#project.clj\n.lein-failures\n_release\n*.zip\n*.tar.gz\n.lein-deps-sum\n*.iml\ntarget\n/.project/\n/.lein-plugins/\n*.ipr\n*.iws\n.idea\n"
        },
        {
          "name": "CHANGELOG.md",
          "type": "blob",
          "size": 28.720703125,
          "content": "## 0.9.0.1\n* Update build configuration to force compatibility with Java 1.6\n\n## 0.9.0\n* Fixed a netty client issue where sleep times for reconnection could be negative (thanks brndnmtthws)\n* Fixed an issue that would cause storm-netty unit tests to fail\n\n## 0.9.0-rc3\n* Added configuration to limit ShellBolt internal _pendingWrites queue length (thanks xiaokang)\n* Fixed a a netty client issue where sleep times for reconnection could be negative (thanks brndnmtthws)\n* Fixed a display issue with system stats in Storm UI (thanks d2r)\n* Nimbus now does worker heartbeat timeout checks as soon as heartbeats are updated (thanks d2r)\n* The logviewer now determines log file location by examining the logback configuration (thanks strongh)\n* Allow tick tuples to work with the system bolt (thanks xumingming)\n* Add default configuration values for the netty transport and the ability to configure the number of worker threads (thanks revans2)\n* Added timeout to unit tests to prevent a situation where tests would hang indefinitely (thanks d2r)\n* Fixed and issue in the system bolt where local mode would not be detected accurately (thanks miofthena)\n\n## 0.9.0-rc2 \n\n* Fixed `storm jar` command to work properly when STORM_JAR_JVM_OPTS is not specified (thanks roadkill001)\n\n## 0.9.0-rc1\n\n * All logging now done with slf4j\n * Replaced log4j logging system with logback\n * Logs are now limited to 1GB per worker (configurable via logging configuration file)\n * Build upgraded to leiningen 2.0\n * Revamped Trident spout interfaces to support more dynamic spouts, such as a spout who reads from a changing set of brokers\n * How tuples are serialized is now pluggable (thanks anfeng)\n * Added blowfish encryption based tuple serialization (thanks anfeng)\n * Have storm fall back to installed storm.yaml (thanks revans2)\n * Improve error message when Storm detects bundled storm.yaml to show the URL's for offending resources (thanks revans2)\n * Nimbus throws NotAliveException instead of FileNotFoundException from various query methods when topology is no longer alive (thanks revans2)\n * Escape HTML and Javascript appropriately in Storm UI (thanks d2r)\n * Storm's Zookeeper client now uses bounded exponential backoff strategy on failures\n * Automatically drain and log error stream of multilang subprocesses\n * Append component name to thread name of running executors so that logs are easier to read\n * Messaging system used for passing messages between workers is now pluggable (thanks anfeng)\n * Netty implementation of messaging (thanks anfeng)\n * Include topology id, worker port, and worker id in properties for worker processes, useful for logging (thanks d2r)\n * Tick tuples can now be scheduled using floating point seconds (thanks tscurtu)\n * Added log viewer daemon and links from UI to logviewers (thanks xiaokang)\n * DRPC server childopts now configurable (thanks strongh)\n * Default number of ackers to number of workers, instead of just one (thanks lyogavin)\n * Validate that Storm configs are of proper types/format/structure (thanks d2r)\n * FixedBatchSpout will now replay batches appropriately on batch failure (thanks ptgoetz)\n * Can set JAR_JVM_OPTS env variable to add jvm options when calling 'storm jar' (thanks srmelody)\n * Throw error if batch id for transaction is behind the batch id in the opaque value (thanks mrflip)\n * Sort topologies by name in UI (thanks jaked)\n * Added LoggingMetricsConsumer to log all metrics to a file, by default not enabled (thanks mrflip)\n * Add prepare(Map conf) method to TopologyValidator (thanks ankitoshniwal)\n * Bug fix: Supervisor provides full path to workers to logging config rather than relative path (thanks revans2) \n * Bug fix: Call ReducerAggregator#init properly when used within persistentAggregate (thanks lorcan)\n * Bug fix: Set component-specific configs correctly for Trident spouts\n\n## 0.8.3 (unreleased)\n\n * Revert zmq layer to not rely on multipart messages to fix issue reported by some users\n * Bug fix: Fix TransactionalMap and OpaqueMap to correctly do multiple updates to the same key in the same batch\n * Bug fix: Fix race condition between supervisor and Nimbus that could lead to stormconf.ser errors and infinite crashing of supervisor\n * Bug fix: Fix default scheduler to always reassign workers in a constrained topology when there are dead executors\n * Bug fix: Fix memory leak in Trident LRUMemoryMapState due to concurrency issue with LRUMap (thanks jasonjckn)\n * Bug fix: Properly ignore NoNodeExists exceptions when deleting old transaction states\n\n## 0.8.2\n\n * Added backtype.storm.scheduler.IsolationScheduler. This lets you run topologies that are completely isolated at the machine level. Configure Nimbus to isolate certain topologies, and how many machines to give to each of those topologies, with the isolation.scheduler.machines config in Nimbus's storm.yaml. Topologies run on the cluster that are not listed there will share whatever remaining machines there are on the cluster after machines are allocated to the listed topologies.\n * Storm UI now uses nimbus.host to find Nimbus rather than always using localhost (thanks Frostman)\n * Added report-error! to Clojure DSL\n * Automatically throttle errors sent to Zookeeper/Storm UI when too many are reported in a time interval (all errors are still logged) Configured with TOPOLOGY_MAX_ERROR_REPORT_PER_INTERVAL and TOPOLOGY_ERROR_THROTTLE_INTERVAL_SECS\n * Kryo instance used for serialization can now be controlled via IKryoFactory interface and TOPOLOGY_KRYO_FACTORY config\n * Add ability to plug in custom code into Nimbus to allow/disallow topologies to be submitted via NIMBUS_TOPOLOGY_VALIDATOR config\n * Added TOPOLOGY_TRIDENT_BATCH_EMIT_INTERVAL_MILLIS config to control how often a batch can be emitted in a Trident topology. Defaults to 500 milliseconds. This is used to prevent too much load from being placed on Zookeeper in the case that batches are being processed super quickly.\n * Log any topology submissions errors in nimbus.log\n * Add static helpers in Config when using regular maps\n * Make Trident much more memory efficient during failures by immediately removing state for failed attempts when a more recent attempt is seen\n * Add ability to name portions of a Trident computation and have those names appear in the Storm UI\n * Show Nimbus and topology configurations through Storm UI (thanks rnfein)\n * Added ITupleCollection interface for TridentState's and TupleCollectionGet QueryFunction for getting the full contents of a state. MemoryMapState and LRUMemoryMapState implement this\n * Can now submit a topology in inactive state. Storm will wait to call open/prepare on the spouts/bolts until it is first activated.\n * Can now activate, deactive, rebalance, and kill topologies from the Storm UI (thanks Frostman)\n * Can now use --config option to override which yaml file from ~/.storm to use for the config (thanks tjun)\n * Redesigned the pluggable resource scheduler (INimbus, ISupervisor) interfaces to allow for much simpler integrations\n * Added prepare method to IScheduler\n * Added \"throws Exception\" to TestJob interface\n * Added reportError to multilang protocol and updated Python and Ruby adapters to use it (thanks Lazyshot)\n * Number tuples executed now tracked and shown in Storm UI\n * Added ReportedFailedException which causes a batch to fail without killing worker and reports the error to the UI\n * Execute latency now tracked and shown in Storm UI\n * Adding testTuple methods for easily creating Tuple instances to Testing API (thanks xumingming)\n * Trident now throws an error during construction of a topology when try to select fields that don't exist in a stream (thanks xumingming)\n * Compute the capacity of a bolt based on execute latency and #executed over last 10 minutes and display in UI\n * Storm UI displays exception instead of blank page when there's an error rendering the page (thanks Frostman)\n * Added MultiScheme interface (thanks sritchie)\n * Added MockTridentTuple for testing (thanks emblem)\n * Add whitelist methods to Cluster to allow only a subset of hosts to be revealed as available slots\n * Updated Trident Debug filter to take in an identifier to use when logging (thanks emblem)\n * Number of DRPC server worker threads now customizable (thanks xiaokang)\n * DRPC server now uses a bounded queue for requests to prevent being overloaded with requests (thanks xiaokang)\n * Add __hash__ method to all generated Python Thrift objects so that Python code can read Nimbus stats which use Thrift objects as dict keys\n * Bug fix: Fix for bug that could cause topology to hang when ZMQ blocks sending to a worker that got reassigned\n * Bug fix: Fix deadlock bug due to variant of dining philosophers problem. Spouts now use an overflow buffer to prevent blocking and guarantee that it can consume the incoming queue of acks/fails.\n * Bug fix: Fix race condition in supervisor that would lead to supervisor continuously crashing due to not finding \"stormconf.ser\" file for an already killed topology\n * Bug fix: bin/storm script now displays a helpful error message when an invalid command is specified\n * Bug fix: fixed NPE when emitting during emit method of Aggregator\n * Bug fix: URLs with periods in them in Storm UI now route correctly\n * Bug fix: Fix occasional cascading worker crashes due when a worker dies due to not removing connections from connection cache appropriately\n  \n## 0.8.1\n\n * Exposed Storm's unit testing facilities via the backtype.storm.Testing class. Notable functions are Testing/withLocalCluster and Testing/completeTopology (thanks xumingming)\n * Implemented pluggable spout wait strategy that is invoked when a spout emits nothing from nextTuple or when a spout hits the MAX_SPOUT_PENDING limit\n * Spouts now have a default wait strategy of a 1 millisecond sleep\n * Changed log level of \"Failed message\" logging to DEBUG\n * Deprecated LinearDRPCTopologyBuilder, TimeCacheMap, and transactional topologies\n * During \"storm jar\", whether topology is already running or not is checked before submitting jar to save time (thanks jasonjckn)\n * Added BaseMultiReducer class to Trident that provides empty implementations of prepare and cleanup\n * Added Negate builtin operation to reverse a Filter\n * Added topology.kryo.decorators config that allows functions to be plugged in to customize Kryo (thanks jasonjckn)\n * Enable message timeouts when using LocalCluster\n * Multilang subprocesses can set \"need_task_ids\" to false when emitting tuples to tell Storm not to send task ids back (performance optimization) (thanks barrywhart)\n * Add contains method on Tuple (thanks okapies)\n * Added ISchemableSpout interface\n * Bug fix: When an item is consumed off an internal buffer, the entry on the buffer is nulled to allow GC to happen on that data\n * Bug fix: Helper class for Trident MapStates now clear their read cache when a new commit happens, preventing updates from spilling over from a failed batch attempt to the next attempt\n * Bug fix: Fix NonTransactionalMap to take in an IBackingMap for regular values rather than TransactionalValue (thanks sjoerdmulder)\n * Bug fix: Fix NPE when no input fields given for regular Aggregator\n * Bug fix: Fix IndexOutOfBoundsExceptions when a bolt for global aggregation had a parallelism greater than 1 (possible with splitting, stateQuerying, and multiReduce)\n * Bug fix: Fix \"fields size\" error that would sometimes occur when splitting a stream with multiple eaches\n * Bug fix: Fix bug where a committer spout (including opaque spouts) could cause Trident batches to fail\n * Bug fix: Fix Trident bug where multiple groupings on same stream would cause tuples to be duplicated to all consumers\n * Bug fix: Fixed error when repartitioning stream twice in a row without any operations in between\n * Bug fix: Fix rare bug in supervisor where it would continuously fail to clean up workers because the worker was already partially cleaned up\n * Bug fix: Fix emitDirect in storm.py\n\n## 0.8.0\n\n * Added Trident, the new high-level abstraction for intermixing high throughput, stateful stream processing with low-latency distributed querying\n * Added executor abstraction between workers and tasks. Workers = processes, executors = threads that run many tasks from the same spout or bolt.\n * Pluggable scheduler (thanks xumingming)\n * Eliminate explicit storage of task->component in Zookeeper\n * Number of workers can be dynamically changed at runtime through rebalance command and -n switch\n * Number of executors for a component can be dynamically changed at runtime through rebalance command and -e switch (multiple -e switches allowed)\n * Use worker heartbeats instead of task heartbeats (thanks xumingming)\n * UI performance for topologies with many executors/tasks much faster due to optimized usage of Zookeeper (10x improvement)\n * Added button to show/hide system stats (e.g., acker component and stream stats) from the Storm UI (thanks xumingming)\n * Stats are tracked on a per-executor basis instead of per-task basis\n * Major optimization for unreliable spouts and unanchored tuples (will use far less CPU)\n * Revamped internals of Storm to use LMAX disruptor for internal queuing. Dramatic reductions in contention and CPU usage.\n * Numerous micro-optimizations all throughout the codebase to reduce CPU usage.\n * Optimized internals of Storm to use much fewer threads - two fewer threads per spout and one fewer thread per acker.\n * Removed error method from task hooks (to be re-added at a later time)\n * Validate that subscriptions come from valid components and streams, and if it's a field grouping that the schema is correct (thanks xumingming)\n * MemoryTransactionalSpout now works in cluster mode\n * Only track errors on a component by component basis to reduce the amount stored in zookeeper (to speed up UI). A side effect of this change is the removal of the task page in the UI.\n * Add TOPOLOGY-TICK-TUPLE-FREQ-SECS config to have Storm automatically send \"tick\" tuples to a bolt's execute method coming from the __system component and __tick stream at the configured frequency. Meant to be used as a component-specific configuration.\n * Upgrade Kryo to v2.17\n * Tuple is now an interface and is much cleaner. The Clojure DSL helpers have been moved to TupleImpl\n * Added shared worker resources. Storm provides a shared ExecutorService thread pool by default. The number of threads in the pool can be configured with topology.worker.shared.thread.pool.size\n * Improve CustomStreamGrouping interface to make it more flexible by providing more information\n * Enhanced INimbus interface to allow for forced schedulers and better integration with global scheduler\n * Added assigned method to ISupervisor so it knows exactly what's running and not running\n * Custom serializers can now have one of four constructors: (), (Kryo), (Class), or (Kryo, Class)\n * Disallow \":\", \".\", and \"\\\" from topology names\n * Errors in multilang subprocesses that go to stderr will be captured and logged to the worker logs (thanks vinodc)\n * Workers detect and warn for missing outbound connections from assignment, drop messages for which there's no outbound connection\n * Zookeeper connection timeout is now configurable (via storm.zookeeper.connection.timeout config)\n * Storm is now less aggressive about halting process when there are Zookeeper errors, preferring to wait until client calls return exceptions.\n * Can configure Zookeeper authentication for Storm's Zookeeper clients via \"storm.zookeeper.auth.scheme\" and \"storm.zookeeper.auth.payload\" configs\n * Supervisors only download code for topologies assigned to them\n * Include task id information in task hooks (thanks velvia)\n * Use execvp to spawn daemons (replaces the python launcher process) (thanks ept)\n * Expanded INimbus/ISupervisor interfaces to provide more information (used in Storm/Mesos integration)\n * Bug fix: Realize task ids when worker heartbeats to supervisor. Some users were hitting deserialization problems here in very rare cases (thanks herberteuler)\n * Bug fix: Fix bug where a topology's status would get corrupted to true if nimbus is restarted while status is rebalancing\n\n## 0.7.4\n\n * Bug fix: Disallow slashes in topology names since it causes Nimbus to break by affecting local filesystem and zookeeper paths\n * Bug fix: Prevent slow loading tasks from causing worker timeouts by launching the heartbeat thread before tasks are loaded\n\n## 0.7.3\n\n * Changed debug level of \"Failed message\" logging to DEBUG\n * Bug fix: Fixed critical regression in 0.7.2 that could cause workers to timeout to the supervisors or to Nimbus. 0.7.2 moved all system tasks to the same thread, so if one took a long time it would block the other critical tasks. Now different system tasks run on different threads.\n\n## 0.7.2\n\nNOTE: The change from 0.7.0 in which OutputCollector no longer assumes immutable inputs has been reverted to support optimized sending of tuples to colocated tasks\n\n * Messages sent to colocated tasks are sent in-memory, skipping serialization (useful in conjunction with localOrShuffle grouping) (thanks xumingming)\n * Upgrade to Clojure 1.4 (thanks sorenmacbeth)\n * Exposed INimbus and ISupervisor interfaces for running Storm on different resource frameworks (like Mesos).\n * Can override the hostname that supervisors report using \"storm.local.hostname\" config.\n * Make request timeout within DRPC server configurable via \"drpc.request.timeout.secs\"\n * Added \"storm list\" command to show running topologies at the command line (thanks xumingming)\n * Storm UI displays the release version (thanks xumingming)\n * Added reportError to BasicOutputCollector\n * Added reportError to BatchOutputCollector\n * Added close method to OpaqueTransactionalSpout coordinator\n * Added \"storm dev-zookeeper\" command for launching a local zookeeper server. Useful for testing a one node Storm cluster locally. Zookeeper dir configured with \"dev.zookeeper.path\"\n * Use new style classes for Python multilang adapter (thanks hellp)\n * Added \"storm version\" command\n * Heavily refactored and simplified the supervisor and worker code\n * Improved error message when duplicate config files found on classpath\n * Print the host and port of Nimbus when using the storm command line client\n * Include as much of currently read output as possible when pipe to subprocess is broken in multilang components\n * Lower supervisor worker start timeout to 120 seconds\n * More debug logging in supervisor\n * \"nohup\" no longer used by supervisor to launch workers (unnecessary)\n * Throw helpful error message if StormSubmitter used without using storm client script\n * Add Values class as a default serialization\n * Bug fix: give absolute piddir to subprocesses (so that relative paths can be used for storm local dir)\n * Bug fix: Fixed critical bug in transactional topologies where a batch would be considered successful even if the batch didn't finish\n * Bug fix: Fixed critical bug in opaque transactional topologies that would lead to duplicate messages when using pipelining\n * Bug fix: Workers will now die properly if a ShellBolt subprocess dies (thanks tomo)\n * Bug fix: Hide the BasicOutputCollector#getOutputter method, since it shouldn't be a publicly available method\n * Bug fix: Zookeeper in local mode now always gets an unused port. This will eliminate conflicts with other local mode processes or other Zookeeper instances on a local machine. (thanks xumingming)\n * Bug fix: Fixed NPE in CoordinatedBolt it tuples emitted, acked, or failed for a request id that has already timed out. (thanks xumingming)\n * Bug fix: UI no longer errors for topologies with no assigned tasks (thanks xumingming)\n * Bug fix: emitDirect on SpoutOutputCollector now works\n * Bug fix: Fixed NPE when giving null parallelism hint for spout in TransactionalTopologyBuilder (thanks xumingming)\n\n## 0.7.1\n\n * Implemented shell spout (thanks tomo)\n * Shell bolts can now asynchronously emit/ack messages (thanks tomo)\n * Added hooks for when a tuple is emitted, acked, or failed in bolts or spouts.\n * Added activate and deactivate lifecycle methods on spouts. Spouts start off deactivated.\n * Added isReady method to ITransactionalSpout$Coordinator to give the ability to delay the creation of new batches\n * Generalized CustomStreamGrouping to return the target tasks rather than the indices. Also parameterized custom groupings with TopologyContext. (not backwards compatible)\n * Added localOrShuffle grouping that will send to tasks in the same worker process if possible, or do a shuffle grouping otherwise.\n * Removed parameter from TopologyContext#maxTopologyMessageTimeout (simplification).\n * Storm now automatically sets TOPOLOGY_NAME in the config passed to the bolts and spouts to the name of the topology.\n * Added TOPOLOGY_AUTO_TASK_HOOKS config to automatically add hooks into every spout/bolt for the topology.\n * Added ability to override configs at the command line. These config definitions have the highest priority.\n * Error thrown if invalid (not json-serializable) topology conf used.\n * bin/storm script can now be symlinked (thanks gabrielgrant)\n * Socket timeout for DRPCClient is now configurable\n * Added getThisWorkerPort() method to TopologyContext\n * Added better error checking in Fields (thanks git2samus)\n * Improved Clojure DSL to allow destructuring in bolt/spout methods\n * Added Nimbus stats methods to LocalCluster (thanks KasperMadsen)\n * Added rebalance, activate, deactivate, and killTopologyWithOpts methods to LocalCluster\n * Added custom stream groupings to LinearDRPC API\n * Simplify multilang protocol to use json for all messages (thanks tomoj)\n * Bug fix: Fixed string encoding in ShellBolt protocol to be UTF-8 (thanks nicoo)\n * Bug fix: Fixed race condition in FeederSpout that could lead to dropped messages\n * Bug fix: Quoted arguments with spaces now work properly with storm client script\n * Bug fix: Workers start properly when topology name has spaces\n * Bug fix: UI works properly when there are spaces in topology or spout/bolt names (thanks xiaokang)\n * Bug fix: Tuple$Seq now returns correct count (thanks travisfw)\n\n## 0.7.0\n\n * Transactional topologies: a new higher level abstraction that enables exactly-once messaging semantics for most computations. Documented on the wiki.\n * Component-specific configurations: Can now set configurations on a per-spout or per-bolt basis. \n * New batch bolt abstraction that simplifies the processing of batches in DRPC or transactional topologies. A new batch bolt is created per batch and they are automatically cleaned up.\n * Introduction of base classes for various bolt and spout types. These base classes are in the backtype.storm.topology.base package and provide empty implementations for commonly unused methods\n * CoordinatedBolt generalized to handle non-linear topologies. This will make it easy to implement a non-linear DRPC topology abstraction.\n * Can customize the JVM options for Storm UI with new ui.childopts config\n * BigIntegers are now serializable by default\n * All bolts/spouts now emit a system stream (id \"__system\"). Currently it only emits startup events, but may emit other events in the future.\n * Optimized tuple trees for batch processing in DRPC and transactional topologies. Only the coordination tuples are anchored. OutputCollector#fail still works because CoordinatedBolt will propagate the fail to all other tuples in the batch. \n * CoordinatedBolt moved to backtype.storm.coordination package\n * Clojure test framework significantly more composable\n * Massive internal refactorings and simplifications, including changes to the Thrift definition for storm topologies.\n * Optimized acking system. Bolts with zero or more than one consumer used to send an additional ack message. Now those are no longer sent.\n * Changed interface of CustomStreamGrouping to receive a List<Object> rather than a Tuple.\n * Added \"storm.zookeeper.retry.times\" and \"storm.zookeeper.retry.interval\" configs (thanks killme2008)\n * Added \"storm help\" and \"storm help {cmd}\" to storm script (thanks kachayev)\n * Logging now always goes to logs/ in the Storm directory, regardless of where you launched the daemon (thanks haitaoyao)\n * Improved Clojure DSL: can emit maps and Tuples implement the appropriate interfaces to integrate with Clojure's seq functions (thanks schleyfox)\n * Added \"ui.childopts\" config (thanks ddillinger)\n * Bug fix: OutputCollector no longer assumes immutable inputs [NOTE: this was reverted in 0.7.2 because it conflicts with sending tuples to colocated tasks without serialization]\n * Bug fix: DRPC topologies now throw a proper error when no DRPC servers are configured instead of NPE (thanks danharvey)\n * Bug fix: Fix local mode so multiple topologies can be run on one LocalCluster\n * Bug fix: \"storm supervisor\" now uses supervisor.childopts instead of nimbus.childopts (thanks ddillinger)\n * Bug fix: supervisor.childopts and nimbus.childopts can now contain whitespace. Previously only the first token was taken from the string\n * Bug fix: Make TopologyContext \"getThisTaskIndex\" and \"getComponentTasks\" consistent\n * Bug fix: Fix NoNodeException that would pop up with task heartbeating under heavy load\n * Bug fix: Catch InterruptedExceptions appropriately in local mode so shutdown always works properly\n\n## 0.6.2\n\n * Automatically delete old files in Nimbus's inbox. Configurable with \"nimbus.cleanup.inbox.freq.secs\" and \"nimbus.inbox.jar.expiration.secs\"\n * Redirect System.out and System.err to log4j\n * Added \"topology.worker.child.opts\" config, for topology-configurable worker options.\n * Use Netflix's Curator library for Zookeeper communication. Workers now reconnect to Zookeeper rather than crash when there's a disconnection.\n * Bug fix: DRPC server no longer hangs with too many concurrent requests. DPRC server now requires two ports: \"drpc.port\" and \"drpc.invocations.port\"\n * Bug fix: Multilang resources are now extracted from the relevant jar on the classpath when appropriate. Previously an error would be thrown if the resources/ dir was in a jar in local mode.\n * Bug fix: Fix race condition in unit testing where time simulation fails to detect that Storm cluster is waiting due to threads that are not alive\n * Bug fix: Fix deadlock in Nimbus that could be triggered by a kill command.\n\n## 0.6.1\n\n * storm client \"activate\" and \"deactivate\" commands\n * storm client \"rebalance\" command\n * Nimbus will automatically detect and cleanup corrupt topologies (this would previously give an error of the form \"file storm...ser cannot be found\").\n * \"storm\" client will not run unless it's being used from a release. \n * Topology jar path now passed in using a java property rather than an environment variable.\n * LD\\_LIBRARY\\_PATH environment variable is now set on worker processes appropriately.\n * Replaced jvyaml with snakeyaml. UTF-8 YAML files should now work properly. \n * Upgraded httpclient, httpcore, and commons-codec dependencies.\n\n## 0.6.0\n\n * New serialization system based on Kryo\n * Component and stream ids are now strings\n * Pluggable stream groupings\n * Storm now chooses an unused port for Zookeeper in local mode instead of crashing when 2181 was in use.\n * Better support for defining topologies in non-JVM languages. The Thrift structure for topologies now allows you to specify components using a Java class name and a list of arguments to that class's constructor.\n * Bug fix: errors during the preparation phase of spouts or bolts will be reported to the Storm UI \n * Bug fix: Fixed bugs related to LinearDRPC topologies where the last bolt implements FinishedCallback \n * Bug fix: String greater than 64K will now serialize properly \n * Generalized type of anchors in OutputCollector methods to Collection from List. \n * Improved logging throughout.\n * In the \"worker.childopts\" config, %ID% will be replaced by the worker port. \n * Significant internal refactorings to clean up the codebase. \n\n## 0.5.4\n\n * LinearDRPCTopologyBuilder, a polished DRPC implementation, \n * Improved custom serialization support. no longer need to provide \"token\" ids. \n * Fallback on Java serialization by default. Can be turned off by setting \"topology.fall.back.on.java.serialization\" to false. \n * Improved \"storm kill\" command. Can override the wait time with \"-w\" flag.\n * Display topology status in Storm UI\n * Changed Thrift namespace to avoid conflicts\n * Better error messages throughout\n * Storm UI port is configurable through \"ui.port\" \n * Minor improvements to Clojure DSL \n\n## 0.5.3\n\n * Nimbus and supervisor daemons can now share a local dir. \n * Greatly improved Clojure DSL for creating topologies.\n * Increased the default timeouts for startup of workers and tasks.\n * Added the commands \"localconfvalue\", \"remoteconfvalue\", and \"repl\" to the storm script.\n * Better error message when \"storm jar\" can't find the nimbus host in the configuration. \n\n## 0.5.2\n\n * No longer need any native dependencies to run Storm in local mode. Storm now uses a pure Java messaging system in local mode\n * Fixed logging configurations so that logging is no longer suppressed when including the Storm release jars on the classpath in local mode. \n\n## 0.5.1\n\n * Changed ISerialization's \"accept\" interface to not annotate the Class with the generic type\n * Made Config class implement Map and added helper methods for setting common configs\n \n## 0.5.0\n \n * Initial release!\n"
        },
        {
          "name": "KEYS",
          "type": "blob",
          "size": 1.1669921875,
          "content": "pub   2048R/E80B8FFD 2012-03-05 [expires: 2018-03-05]\nuid                  P. Taylor Goetz <ptgoetz@gmail.com>\nsig 3        E80B8FFD 2012-03-05  P. Taylor Goetz <ptgoetz@gmail.com>\n\n-----BEGIN PGP PUBLIC KEY BLOCK-----\nVersion: GnuPG/MacGPG2 v2.0.20 (Darwin)\nComment: GPGTools - http://gpgtools.org\n\nmQENBE9VAVUBCADwWjI9USSW4xx45L0KSeHiu+rT1t2eolKx+yxxfMC9QJWb1uGt\nWCKG2zb2lk6DBej2/vF6v6EA6d+esOZfmSZazkd61q0INyimuxi0PBHEjipWD/f3\nuj87ylGY6WbhQjv60eRlQLMH5Md7zGtzUQGmi7BlogTiwWvcYGvYjmkpk6AyGrE2\n9VhJrtRMXpX53V1iL79Z8QR6l688oyuxV3OmPVQMJADtqbXMrDiHk+nSpVuZT5gm\nCA3Fl5zfq7RdsPLrJeNDNM+sL0IuKiFX5U2RVuXF3G4BWoBoHtot8ZG01YhKP7gG\n/7l2fLd5q/sytCcahT7uLTG/rIC829tFvjMvABEBAAG0I1AuIFRheWxvciBHb2V0\neiA8cHRnb2V0ekBnbWFpbC5jb20+iQE+BBMBAgAoBQJPVQFVAhsvBQkLSIaABgsJ\nCAcDAgYVCAIJCgsEFgIDAQIeAQIXgAAKCRCN4Dli6AuP/bqmB/9/U1AzfpMFJ/dY\nnoqCY2yEYV54Bm6e59qlYUifPEFCMKULB5IzMdyou2DYoUrJquHTYdsHUBTr8cuN\n4wVnro8AsryNXjo8oFmE9JwrrO6jE5GLt1OTvri+e0MYgvb08Fk54aZg/zXTcNNS\npIdkbLDBj/RL5jdflKAFuYKSsIEaj0bCvECoR1CRPfTJX2XtPDzRTP28ccRu/pEz\n2I588JSZ/RSjqk9DW2Mh75g1CBocRLp90qhW9jUoCkZb0Pis8jnm5gkcHYOz5Hpr\nqPzxjZOlMD+cLkP9Geo0+Gs13tt3rwBgIE0l/mPdRltPBbQ9xXORoMlGHtZlXZrn\nqSx4e87y\n=RfYX\n-----END PGP PUBLIC KEY BLOCK-----\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 14.8974609375,
          "content": "\n                                 Apache License\n                           Version 2.0, January 2004\n                        http://www.apache.org/licenses/\n\n   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n\n   1. Definitions.\n\n      \"License\" shall mean the terms and conditions for use, reproduction,\n      and distribution as defined by Sections 1 through 9 of this document.\n\n      \"Licensor\" shall mean the copyright owner or entity authorized by\n      the copyright owner that is granting the License.\n\n      \"Legal Entity\" shall mean the union of the acting entity and all\n      other entities that control, are controlled by, or are under common\n      control with that entity. For the purposes of this definition,\n      \"control\" means (i) the power, direct or indirect, to cause the\n      direction or management of such entity, whether by contract or\n      otherwise, or (ii) ownership of fifty percent (50%) or more of the\n      outstanding shares, or (iii) beneficial ownership of such entity.\n\n      \"You\" (or \"Your\") shall mean an individual or Legal Entity\n      exercising permissions granted by this License.\n\n      \"Source\" form shall mean the preferred form for making modifications,\n      including but not limited to software source code, documentation\n      source, and configuration files.\n\n      \"Object\" form shall mean any form resulting from mechanical\n      transformation or translation of a Source form, including but\n      not limited to compiled object code, generated documentation,\n      and conversions to other media types.\n\n      \"Work\" shall mean the work of authorship, whether in Source or\n      Object form, made available under the License, as indicated by a\n      copyright notice that is included in or attached to the work\n      (an example is provided in the Appendix below).\n\n      \"Derivative Works\" shall mean any work, whether in Source or Object\n      form, that is based on (or derived from) the Work and for which the\n      editorial revisions, annotations, elaborations, or other modifications\n      represent, as a whole, an original work of authorship. For the purposes\n      of this License, Derivative Works shall not include works that remain\n      separable from, or merely link (or bind by name) to the interfaces of,\n      the Work and Derivative Works thereof.\n\n      \"Contribution\" shall mean any work of authorship, including\n      the original version of the Work and any modifications or additions\n      to that Work or Derivative Works thereof, that is intentionally\n      submitted to Licensor for inclusion in the Work by the copyright owner\n      or by an individual or Legal Entity authorized to submit on behalf of\n      the copyright owner. For the purposes of this definition, \"submitted\"\n      means any form of electronic, verbal, or written communication sent\n      to the Licensor or its representatives, including but not limited to\n      communication on electronic mailing lists, source code control systems,\n      and issue tracking systems that are managed by, or on behalf of, the\n      Licensor for the purpose of discussing and improving the Work, but\n      excluding communication that is conspicuously marked or otherwise\n      designated in writing by the copyright owner as \"Not a Contribution.\"\n\n      \"Contributor\" shall mean Licensor and any individual or Legal Entity\n      on behalf of whom a Contribution has been received by Licensor and\n      subsequently incorporated within the Work.\n\n   2. Grant of Copyright License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      copyright license to reproduce, prepare Derivative Works of,\n      publicly display, publicly perform, sublicense, and distribute the\n      Work and such Derivative Works in Source or Object form.\n\n   3. Grant of Patent License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      (except as stated in this section) patent license to make, have made,\n      use, offer to sell, sell, import, and otherwise transfer the Work,\n      where such license applies only to those patent claims licensable\n      by such Contributor that are necessarily infringed by their\n      Contribution(s) alone or by combination of their Contribution(s)\n      with the Work to which such Contribution(s) was submitted. If You\n      institute patent litigation against any entity (including a\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\n      or a Contribution incorporated within the Work constitutes direct\n      or contributory patent infringement, then any patent licenses\n      granted to You under this License for that Work shall terminate\n      as of the date such litigation is filed.\n\n   4. Redistribution. You may reproduce and distribute copies of the\n      Work or Derivative Works thereof in any medium, with or without\n      modifications, and in Source or Object form, provided that You\n      meet the following conditions:\n\n      (a) You must give any other recipients of the Work or\n          Derivative Works a copy of this License; and\n\n      (b) You must cause any modified files to carry prominent notices\n          stating that You changed the files; and\n\n      (c) You must retain, in the Source form of any Derivative Works\n          that You distribute, all copyright, patent, trademark, and\n          attribution notices from the Source form of the Work,\n          excluding those notices that do not pertain to any part of\n          the Derivative Works; and\n\n      (d) If the Work includes a \"NOTICE\" text file as part of its\n          distribution, then any Derivative Works that You distribute must\n          include a readable copy of the attribution notices contained\n          within such NOTICE file, excluding those notices that do not\n          pertain to any part of the Derivative Works, in at least one\n          of the following places: within a NOTICE text file distributed\n          as part of the Derivative Works; within the Source form or\n          documentation, if provided along with the Derivative Works; or,\n          within a display generated by the Derivative Works, if and\n          wherever such third-party notices normally appear. The contents\n          of the NOTICE file are for informational purposes only and\n          do not modify the License. You may add Your own attribution\n          notices within Derivative Works that You distribute, alongside\n          or as an addendum to the NOTICE text from the Work, provided\n          that such additional attribution notices cannot be construed\n          as modifying the License.\n\n      You may add Your own copyright statement to Your modifications and\n      may provide additional or different license terms and conditions\n      for use, reproduction, or distribution of Your modifications, or\n      for any such Derivative Works as a whole, provided Your use,\n      reproduction, and distribution of the Work otherwise complies with\n      the conditions stated in this License.\n\n   5. Submission of Contributions. Unless You explicitly state otherwise,\n      any Contribution intentionally submitted for inclusion in the Work\n      by You to the Licensor shall be under the terms and conditions of\n      this License, without any additional terms or conditions.\n      Notwithstanding the above, nothing herein shall supersede or modify\n      the terms of any separate license agreement you may have executed\n      with Licensor regarding such Contributions.\n\n   6. Trademarks. This License does not grant permission to use the trade\n      names, trademarks, service marks, or product names of the Licensor,\n      except as required for reasonable and customary use in describing the\n      origin of the Work and reproducing the content of the NOTICE file.\n\n   7. Disclaimer of Warranty. Unless required by applicable law or\n      agreed to in writing, Licensor provides the Work (and each\n      Contributor provides its Contributions) on an \"AS IS\" BASIS,\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n      implied, including, without limitation, any warranties or conditions\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n      PARTICULAR PURPOSE. You are solely responsible for determining the\n      appropriateness of using or redistributing the Work and assume any\n      risks associated with Your exercise of permissions under this License.\n\n   8. Limitation of Liability. In no event and under no legal theory,\n      whether in tort (including negligence), contract, or otherwise,\n      unless required by applicable law (such as deliberate and grossly\n      negligent acts) or agreed to in writing, shall any Contributor be\n      liable to You for damages, including any direct, indirect, special,\n      incidental, or consequential damages of any character arising as a\n      result of this License or out of the use or inability to use the\n      Work (including but not limited to damages for loss of goodwill,\n      work stoppage, computer failure or malfunction, or any and all\n      other commercial damages or losses), even if such Contributor\n      has been advised of the possibility of such damages.\n\n   9. Accepting Warranty or Additional Liability. While redistributing\n      the Work or Derivative Works thereof, You may choose to offer,\n      and charge a fee for, acceptance of support, warranty, indemnity,\n      or other liability obligations and/or rights consistent with this\n      License. However, in accepting such obligations, You may act only\n      on Your own behalf and on Your sole responsibility, not on behalf\n      of any other Contributor, and only if You agree to indemnify,\n      defend, and hold each Contributor harmless for any liability\n      incurred by, or claims asserted against, such Contributor by reason\n      of your accepting any such warranty or additional liability.\n\n   END OF TERMS AND CONDITIONS\n\n   APPENDIX: How to apply the Apache License to your work.\n\n      To apply the Apache License to your work, attach the following\n      boilerplate notice, with the fields enclosed by brackets \"[]\"\n      replaced with your own identifying information. (Don't include\n      the brackets!)  The text should be enclosed in the appropriate\n      comment syntax for the file format. We also recommend that a\n      file or class name and description of purpose be included on the\n      same \"printed page\" as the copyright notice for easier\n      identification within third-party archives.\n\n   Copyright [yyyy] [name of copyright owner]\n\n   Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.\n\n-----------------------------------------------------------------------\n\n\nFor jQuery 1.6.2 (storm-core/src/ui/public/js/jquery-1.6.2.min.js)\n\nCopyright (c) 2009 John Resig, http://jquery.com/\n\nPermission is hereby granted, free of charge, to any person obtaining\na copy of this software and associated documentation files (the\n\"Software\"), to deal in the Software without restriction, including\nwithout limitation the rights to use, copy, modify, merge, publish,\ndistribute, sublicense, and/or sell copies of the Software, and to\npermit persons to whom the Software is furnished to do so, subject to\nthe following conditions:\n\nThe above copyright notice and this permission notice shall be\nincluded in all copies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND,\nEXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF\nMERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\nNONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE\nLIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION\nOF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION\nWITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\n-----------------------------------------------------------------------\n\nFor jQuery Cookies 2.2.0 (storm-core/src/ui/public/js/jquery.cookies.2.2.0.min.js)\n\nCopyright (c) 2005 - 2010, James Auldridge\n\nPermission is hereby granted, free of charge, to any person obtaining\na copy of this software and associated documentation files (the\n\"Software\"), to deal in the Software without restriction, including\nwithout limitation the rights to use, copy, modify, merge, publish,\ndistribute, sublicense, and/or sell copies of the Software, and to\npermit persons to whom the Software is furnished to do so, subject to\nthe following conditions:\n\nThe above copyright notice and this permission notice shall be\nincluded in all copies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND,\nEXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF\nMERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\nNONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE\nLIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION\nOF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION\nWITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\n-----------------------------------------------------------------------\n\nFor jQuery TableSorter 2.0.5b (storm-core/src/ui/public/js/jquery.tablesorter.min.js)\n\nCopyright (c) 2007 Christian Bach\nExamples and docs at: http://tablesorter.com\nDual licensed under the MIT and GPL licenses:\nhttp://www.opensource.org/licenses/mit-license.php\nhttp://www.gnu.org/licenses/gpl.html\n\nMIT license selected:\n\nCopyright (c) 2007 Christian Bach\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in\nall copies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\nTHE SOFTWARE."
        },
        {
          "name": "MODULES",
          "type": "blob",
          "size": 0.044921875,
          "content": "storm-console-logging\nstorm-core\nstorm-netty\n\n"
        },
        {
          "name": "NOTICE",
          "type": "blob",
          "size": 0.373046875,
          "content": "Apache Storm\nCopyright 2013 The Apache Software Foundation\n\nThis product includes software developed at\nThe Apache Software Foundation (http://www.apache.org/).\n\n\nThis product includes software developed by Nathan Marz\nCopyright 2011-2013 Nathan Marz\n\n\nThis product includes software developed by Yahoo! Inc. (www.yahoo.com)\nCopyright © 2012-2013 Yahoo! Inc.  All rights reserved.\n"
        },
        {
          "name": "README.markdown",
          "type": "blob",
          "size": 2.9521484375,
          "content": "## IMPORTANT NOTE!!!\nStorm has Moved to Apache. The official Storm git repository is now hosted by Apache, and is mirrored on github here:\n\n[https://github.com/apache/incubator-storm](https://github.com/apache/incubator-storm)\n\n\n### Contributing\nSource code contributions can be submitted either by [sumitting a pull request](https://github.com/apache/incubator-storm/pulls) or by creating an issue in [JIRA](https://issues.apache.org/jira/browse/STORM) and attaching patches.\n\n### Migrating Git Repos from nathanmarz/storm to apache/incubator-storm\nIf you have an existing fork/clone of nathanmarz/storm, you can migrate to apache/incubator-storm by doing the following:\n\n1. Create a new fork of [apache/incubator-storm]()\n2. Point your existing clone to the new fork:\n\n\n\t\tgit remote remove origin\n\t\tgit remote add origin git@github.com:username/incubator-storm.git\n\n\n\n### Issue Tracking\nThe official issue tracker for Storm is Apache JIRA:\n\n[https://issues.apache.org/jira/browse/STORM](https://issues.apache.org/jira/browse/STORM)\n\n\n\n### User Mailing List\nStorm users should send messages and subscribe to [user@storm.incubator.apache.org](mailto:user@storm.incubator.apache.org).\n\nYou can subscribe to this list by sending an email to [user-subscribe@storm.incubator.apache.org](mailto:user-subscribe@storm.incubator.apache.org). Likewise, you can cancel a subscription by sending an email to [user-unsubscribe@storm.incubator.apache.org](mailto:user-unsubscribe@storm.incubator.apache.org).\n\nYou can view the archives of the mailing list [here](http://mail-archives.apache.org/mod_mbox/incubator-storm-user/).\n\n### Developer Mailing List\nStorm developers should send messages and subscribe to [dev@storm.incubator.apache.org](mailto:dev@storm.incubator.apache.org).\n\nYou can subscribe to this list by sending an email to [dev-subscribe@storm.incubator.apache.org](mailto:dev-subscribe@storm.incubator.apache.org). Likewise, you can cancel a subscription by sending an email to [dev-unsubscribe@storm.incubator.apache.org](mailto:dev-unsubscribe@storm.incubator.apache.org).\n\nYou can view the archives of the mailing list [here](http://mail-archives.apache.org/mod_mbox/incubator-storm-dev/).\n\n### Which list should I send/subscribe to?\nIf you are using a pre-built binary distribution of Storm, then chances are you should send questions, comments, storm-related announcements, etc. to [user@storm.apache.incubator.org](user@storm.apache.incubator.org). \n\nIf you are building storm from source, developing new features, or otherwise hacking storm source code, then [dev@storm.incubator.apache.org](dev@storm.incubator.apache.org) is more appropriate. \n\n### What will happen with storm-user@googlegroups.com?\nAll existing messages will remain archived there, and can be accessed/searched [here](https://groups.google.com/forum/#!forum/storm-user).\n\nNew messages sent to storm-user@googlegroups.com will either be rejected/bounced or replied to with a message to direct the email to the appropriate Apache-hosted group.\n"
        },
        {
          "name": "TODO",
          "type": "blob",
          "size": 9.7255859375,
          "content": "Use cases:\n\n1. number of steps between 2 people in a graph (topology with cycles?)\n\n\n#################\n\n* Repackage jzmq and zmq as a leiningen \"native dep\"\n       - this might be good, since the native dep can package builds for all different systems/os's?\n\n\n* Deploy design:\n\n- storm swap {name} {jar} {class}\n- it's allowed to use resources equal to current running topology plus number of free resources\n- starts in deactivated mode\n- add TOPOLOGY_STARTUP_TIME config for the delay until nimbus activates a topology after launching it\n- for swap, after the startup time, deactivate the other topology, wait the TOPOLOGY_MESSAGE_TIMEOUT_SECS, and then activate the other topology\n- should be able to decrease the message timeout for killing or swapping (add optional thrift parameter) -- or just make it part of the config?\n- add killWithOptions, swap, swapWithOptions\n\n* Storm UI, stats, debugging, diagnosis tools\n-- need to be able to hide system streams/components from the calculations (another query param and should be default)\n-- need to optimize (slowness is probably on nimbus end of querying zk, consider adding heartbeat caching into nimbus)\n-- add margins\n-- add titles so its easier to distinguish the various pages\n-- right align all table columns except for the leftmost\n\t\t\n* Unit test the core pieces that have stabilized their APIs\n\n- process simulator\n- virtual ports\n- supervisor\n- utils\n- test worker/tasks\n\n* implement pseudo-distributed mode -- this is for testing the distributed parts of the code\n  - perhaps i can use pallet/vmfest for this\n\n* Need integration tests that run on an actual storm cluster (scp code/process code/zookeeper code not tested in unit tests)\n\n* bolts with none grouping can be pushed into a bolt. e.g. A -> B -> C\n     A -> D -> E\n     \nIf A -> B and A -> D are shuffle grouping = none, and B -> C and D -> E are not, then both can be run in A, b's branch goes to C and D's branch goes to E\n\n\n* Failure design\n\nAdd fail method to outputcollector\nFail sends fail message to Acker for those anchors, which sends fail message back to spout.\nWhenever spout fails a tuple, it emits it in its failure stream...\n\nAdd fail method to drpc... Causes blocked thread to throw exception\n\n* Have worker heartbeat with its task ids, nimbus verifies - if wrong, reassign tasks?\n- detect and ignore stray tasks\nEach worker can choose a unique id for itself when heart beating\n- nimbus deletes those that aren't in topology\n\n* Subscriptions design\n\n-- new kind of spout: \"subscription spout\"\n   --> goal is to sync it's data across the tasks that subscribe to its streams\n   --> after doing a grouping, remembers what task it sent the tuple to (regardless of grouping). if a task dies, it knows its subscriptions and asks to be resynced\n   --> normal operation is to push to tasks, but pull done when a task starts up (b/c previous task died or something)\n   --> need to be able to add tuples to subscription or take tuples away (this is protocol with who you're subscribing to - e.g. rocket)\n   --> subscriptions can only happen in a spout because it requires persistent state\n   --> when subscription spout task dies, it polls the source (e.g. rocket) for all the subscription info\n   --> ideally you'd set things up to have one subscription spout per rocket server\n   --> TODO: Need some way to delete subscriptions -> part of tuple or extra metadata on tuple (extra metadata seems cleaner)\n        --> add isSubscription() method to Tuple as well as a getSubscriptionType() [which returns ADD or REMOVE]\n   --> when a spout starts up, it also needs to push all of its subscription info\n   --> acks are irrelevant for subscription tuples -- how should acks be managed as an abstraction?\n        -- maybe the synchronized state is done for you -- you just access the state directly and receive a callback whenever it changes?\n        -- so don't use tuples...\n   --> subscriptions break all the abstractions, perhaps I should generalize spouts and factor acking as a library on top of storm. subscriptions would just be another kind of library? -> no, it seems to break abstractions anyway (like keeping task -> tuples in memory)\n   --> maybe call it \"syncspout\"\n   --> if just do syncing (don't expose tuples directly?)\n   --> have a \"SubscribedState\" class that takes care of indexing/etc. --> expose it through topologycontext?\n      -- need a way to distinguish between states of different streams\n      -- has \"add\" and \"remove\" methods\n      -- bolt can give a statemanager object that implements add and remove in the prepare method\n      -- add(Tuple tuple)\n      -- remove(Tuple tuple)\n   --> synchronize protocol (when spout or source of data dies):\n      --> send how many tuples are going to be sent\n      --> send the tuples\n      --> OR: pack everything together into a single message (could be hard b/c where tuples are supposed to go is abstracted away)\n      --> tie everything together with a unique ID\n      --> once task receives everything, has info needed to remove tuples\n   --> statespout should do long-polling with timeout\n   --> to do subscriptions, the state should contain something like [url, subscriber]. some bolt appends subscriber to tuples, group by subscriber, and send info back\n        --> how to to fields grouping with an even distribution?\n   -->  ********* tasks need to block on startup until they're synchronized *********\n          --> send sync messages in a loop until it's synchronized\n          --> add a task.synchronize.poll.freq.secs config (default to 10 seconds)\n          --> need to buffer other messages as topology is waiting for synchronization messages (use disk?)\n   --> could use acking system to know if a piece of state gets fully synchronized and communicate this with user\n      --> perhaps expose this through a special stream? (the state status stream -> similar to failure streams)\n   --> should be able to do updates of existing state\n      --> use case: have a knob that you can set externally\n      --> this isn't really any better than just using zookeeper directly\n   \n   \n_myState = context.setSubscribedState(_myState)\n\nStateSpout {\n  //does a timeout long poll and emits new add or remove state tuples (add and remove on the output collector)\n  nextTuple(StateSpoutOutputCollector) //collector has add and remove methods add(id, tuple). remove(id)\n  //emits all the tuples into the output collector (in the background, will also send ids and counts to tasks so they know how to synchronize)\n  //called on startup\n  //collector can have a synchronize method in case the source of data (e.g., rocket) craps out\n  synchronize(SynchronizationOutputCollector) //collector only has add(id, tuple) method\n}\n\n//task startup (in prepare method) [this is automatic]\nfor(int taskId: statespoutids) {\n  emitDirect(SYNC_STREAM, tuple())\n}\n\nstatespout synchronization():\n  id = uuid()\n  //getAlLStateTuples calls synchronize on the spout to get the tuples\n  for(Tuple t: getAllStateTuplesFromSource()) {\n    List tasks = emit(cons(id, t));\n    .. keep track of id -> tasks -> count\n    for(task: all output tasks) {\n      emitDirect(task, id, count)\n    } \n  }\n\nfor synchronization to work, task needs to keep track of which tasks sent it tuples, and compare against only that set on synchronization\n\nNeed a way to propogate information back up the topology - \"subscriptions\"\ne.g. browser -> rocket -> bolt -> bolt -> bolt. \n\nexample: #retweets for a subscribed set of tweet ids\n\nstorm topology\n\n -> tweet spout (A) -> group on original id -> count (B) -> rocket\n\nsubscriptions: rocket -> count (B) tweet id (need to group) -> spout (need to go to all)\n\n-- how does it work when stuff dies downstream or upstream? do people ask what the subscriptions are? or do you push your subscriptions up? a combination?\n\n-- maybe subscriptions are a \"constant\" spout? e..g, continuously emits and refreshes to make sure every task has the tuple. this seem amporphous and hard to implement... nimbus would need to refire all constant spouts whenever there's a reassignment that affects the flow of data. subscriptions seem more natural\n\n-- subscriptions are a special kind of stream that are driven by being asked to send it. e..g, rocket is a spout that emits subscription/unsubscription tuples. they only send it when they get something new, or are asked as to what all the subscriptions are\n\n-- maybe you just need a system stream to know when tasks are created. when you see that a downstream task is created, you know to fire subscriptions to it if its subscribed to your subscriptions stream? - how does this interplay with all the grouping types... you almost want to do a grouping and only send what to tasks that would have received. spouts would need to be able to subscribe to streams as well\n\n(use 'backtype.storm.testing)\n;;(start-simulating-time!)\n(def cluster (mk-local-storm-cluster))\n(use 'backtype.storm.bootstrap) (bootstrap)\n(import '[backtype.storm.testing TestWordCounter TestWordSpout TestGlobalCount TestAggregatesCounter])\n(def spout (feeder-spout [\"word\"]))\n(def topology (thrift/mk-topology\n                    {1 (thrift/mk-spout-spec spout :parallelism-hint 3)}\n                    {2 (thrift/mk-bolt-spec {1 [\"word\"]} (TestWordCounter.) :parallelism-hint 4)\n                     3 (thrift/mk-bolt-spec {1 :global} (TestGlobalCount.))\n                     4 (thrift/mk-bolt-spec {2 :global} (TestAggregatesCounter.))\n                     }))\n(submit-local-topology (:nimbus cluster) \"test\" {TOPOLOGY-WORKERS 4 TOPOLOGY-DEBUG true} topology)\n\n\n* clean up project\n  - remove log4j dir and instead generate it in the deploy (it's only used in bin/storm -> create a console one and put into bin/)\n  - include system component / stream information in the topologycontext and clean up system specific code all over the place\n\n* Very rare errors\n\nweird nullptr exceptions:\n(tasks i) on send-fn\nno virtual port socket for outbound task (in worker)\n\n"
        },
        {
          "name": "VERSION",
          "type": "blob",
          "size": 0.025390625,
          "content": "0.9.1-incubating-SNAPSHOT\n"
        },
        {
          "name": "bin",
          "type": "tree",
          "content": null
        },
        {
          "name": "conf",
          "type": "tree",
          "content": null
        },
        {
          "name": "logback",
          "type": "tree",
          "content": null
        },
        {
          "name": "project.clj",
          "type": "blob",
          "size": 1.4873046875,
          "content": ";; Licensed to the Apache Software Foundation (ASF) under one\n;; or more contributor license agreements.  See the NOTICE file\n;; distributed with this work for additional information\n;; regarding copyright ownership.  The ASF licenses this file\n;; to you under the Apache License, Version 2.0 (the\n;; \"License\"); you may not use this file except in compliance\n;; with the License.  You may obtain a copy of the License at\n;;\n;; http:;; www.apache.org/licenses/LICENSE-2.0\n;;\n;; Unless required by applicable law or agreed to in writing, software\n;; distributed under the License is distributed on an \"AS IS\" BASIS,\n;; WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n;; See the License for the specific language governing permissions and\n;; limitations under the License.\n(def VERSION (.trim (slurp \"VERSION\")))\n(def MODULES (-> \"MODULES\" slurp (.split \"\\n\")))\n(def DEPENDENCIES (for [m MODULES] [(symbol (str \"storm/\" m)) VERSION]))\n\n(eval `(defproject storm/storm ~VERSION\n  :url \"http://storm-project.net\"\n  :description \"Distributed and fault-tolerant realtime computation\"\n  :license {:name \"Eclipse Public License - Version 1.0\" :url \"https://github.com/nathanmarz/storm/blob/master/LICENSE.html\"}\n  :mailing-list {:name \"Storm user mailing list\"\n                 :archive \"https://groups.google.com/group/storm-user\"\n                 :post \"storm-user@googlegroups.com\"}\n  :dependencies [~@DEPENDENCIES]\n  :plugins [[~'lein-sub \"0.2.1\"]]  \n  :min-lein-version \"2.0.0\"\n  :sub [~@MODULES]\n  ))\n"
        },
        {
          "name": "storm-console-logging",
          "type": "tree",
          "content": null
        },
        {
          "name": "storm-core",
          "type": "tree",
          "content": null
        },
        {
          "name": "storm-lib",
          "type": "tree",
          "content": null
        },
        {
          "name": "storm-netty",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}