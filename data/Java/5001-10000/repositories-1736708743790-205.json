{
  "metadata": {
    "timestamp": 1736708743790,
    "page": 205,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjIxMA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "hibernate/hibernate-orm",
      "stars": 6040,
      "defaultBranch": "main",
      "files": [
        {
          "name": ".git-blame-ignore-revs",
          "type": "blob",
          "size": 0.1396484375,
          "content": "# Initial commits that enabled spotless automatic formatting\n59731c089e9d649a663c81d9622b54557d6aba37\n4d63ffd98cdead513ab0be0fa23f9787423092d3\n"
        },
        {
          "name": ".gitattributes",
          "type": "blob",
          "size": 0.7099609375,
          "content": "# By default, detect text files automatically, and use whatever line terminators make sense for the OS\n* text=auto eol=lf\n\n# Java files are text, and we want Java-friendly readable hunk headers for diff\n*.java text diff=java eol=lf\n\n# Force LF/CRLF format for files that are known to require it.\n*.sh text eol=lf\n*.bat text eol=crlf\n\n# For some reason the above is not enough, in particular for gradlew.bat,\n# as some commands (git status, git add --renormalize) will still change its line endings to LF.\n# So, we explicitly tell git not to mess with *.bat line endings.\n# It's annoying as git won't show diffs for these files anymore,\n# but that's the best I could come up with after an hour of head-scratching.\n*.bat binary\n\n"
        },
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.701171875,
          "content": "# Typically *NIX text editors, by default, append '~' to files on saving to make backups\n*~\n\n# Gradle work directory and caches\n.gradle\n.gradletasknamecache\n\n# Build output directies\n/target\n*/target\n*/**/target\n/build\n*/build\ntestdb\nlib\n\n# IntelliJ specific files/directories\nout\n# See .idea/.gitignore for more precise rules in that directory\n*.ipr\n*.iws\n*.iml\natlassian-ide-plugin.xml\n\n# Eclipse specific files/directories\n.classpath\n.project\n.settings\n.metadata\nbin\n\n# NetBeans specific files/directories\n.nbattrs\n\n# Miscellaneous\n*.log\n.clover\n.DS_Store\n\n# JBoss Transactions\nObjectStore\n\n# Profiler and heap dumps\n*.jps\n*.hprof\n/.nb-gradle/\n\n# Vim\n*.swp\n*.swo\n\n# SDKman, used by some module maintainers\n.sdkmanrc"
        },
        {
          "name": ".idea",
          "type": "tree",
          "content": null
        },
        {
          "name": ".mailmap",
          "type": "blob",
          "size": 7.4814453125,
          "content": "#\n# This is a .mailmap file : see gitmailmap for format specification and documentation.\n#\n# We use this as helper metadata to recognize that sometimes contributors change\n# email addresses, preferred names, or perhaps have had mistakes in their git\n# configuration at the time of contributing a patch and wich to correct them.\n# This can be useful in various ways, such as to allow contributors to let others\n# know their preferred name and contact point should they change in the future.\n#\n# This file is taken into account by various `git log` options; for example use:\n# # git log --mailmap --format=\"%aN %aE\" | sort | uniq\n# to produce a list of contributors which applies the normalization rules from this file;\n# this is also a reasonable strategy to verify effectiveness of updates to this file.\n#\n# # MAINTENANCE\n# Feel free to provide mappings for your own git identifiers, should you wish to\n# update your own preferred name and/or email address.\n# Bear in mind that in doing so, you're making such information public.\n#\n# The team might occasionally try to keep this file up to date, but we won't always\n# have the necessary information to do so.\n# For example, if two records have the same email address we can assume they refer to the\n# same person, and we might be able to guess which name format is preferred, but\n# we might not absolute certainty about the preference.\n# On the other hand, if two records show the same name but different email addresses\n# we won't know for sure if they are not two different persons happening to have the\n# same name; even if they are the same person we wouldn't be able to tell which one\n# is their preferred email address.\n# In particular we would prefer to be cautious to not choose an email address which is\n# potentially no longer in use, so we'd rather keep both records rather than risk\n# no longer being able to get in touch.\n# For all these reasons, please only add a mapping to this file if you are the person\n# affected by the record, or if you know with certainty that the mapping is correct.\n#\n# # IMPORTANT !\n# Do not add personal information here which was not explicitly contributed by the person:\n# you might have got information about a more suitable email address of someone listed here\n# but you can't publish such information here without their permission.\n# We only use this file to re-map, and reorganize information which was already contributed,\n# even if this comes at a potential risk of maintaining information which is no longer\n# accurate.\n# This also comes with no guarantees whatsoever, in fact I would suggest to never\n# assume any information here is accurate.\n\nAdam Warski <adam@warski.org> adamw <adam@warski.org>\nAlex Snaps <alex.snaps@gmail.com> <alex.snaps@shopify.com>\nAndrea Boriero <andrea@hibernate.org> <dreborier@gmail.com>\nAndrej Golovnin <andrej.golovnin@googlemail.com> Andrej Golovnin <andrej.golovnin@gmail.com>\nAndrig Miller <andy.miller@jboss.com> andrigtmiller <andy.miller@jboss.com>\nAndrig Miller <andy.miller@jboss.com> andy.miller <andy.miller@jboss.com>\nBarry LaFond <blafond@redhat.com> blafond <blafond@redhat.com>\nBoris Korogvich <b.korogvich@gmail.com> VEINHORN <b.korogvich@gmail.com>\nBrett Meyer <brmeyer@redhat.com> <brett@3riverdev.com>\nBrett Meyer <brmeyer@redhat.com> brmeyer <brmeyer@BRMEYER-W7E.home>\nBrett Meyer <brmeyer@redhat.com> brmeyer <brmeyer@redhat.com>\nBrian Stansberry <brian.stansberry@redhat.com> <brian.stansberry@jboss.com>\nChris Cranford <chris@hibernate.org> <ccranfor@ccranfor.lan>\nChris Cranford <chris@hibernate.org> <ccranfor@redhat.com>\nChris Cranford <chris@hibernate.org> Chris Canford <chris@hibernate.org>\nChris Cranford <chris@hibernate.org> <chris@hibernate.org> Naros\nChris Cranford <chris@hibernate.org> <crancran@gmail.com>\nChris Cranford <chris@hibernate.org> cranforc <chris.cranford@setech.com>\nChristoph Dreis <christoph.dreis@freenet.de> ChristophDreis <christoph.dreis@freenet.de>\nDave Repshas <David.Repshas@Teradata.com> drepshas <David.Repshas@Teradata.com>\nDavid M. Carr <david@carrclan.us> davidmc24 <david@carrclan.us>\nDavid M. Carr <dcarr@commercehub.com> dcarr <dcarr@commercehub.com>\nEric Dalquist <eric.dalquist@gmail.com> edalquist <eric.dalquist@gmail.com>\nErik-Berndt Scheper <erik.berndt.scheper@gmail.com> <schepeer@sogeti.nl>\nGail Badner <gail@redhat.com> Gail Badner <gbadner@redhat.com>\nGail Badner <gail@redhat.com> <gbadner@gbadner.fedora>\nGail Badner <gail@redhat.com> gbadner <gbadner@redhat.com>\nGalder Zamarreño <galder.zamarreno@redhat.com> <galder@jboss.org>\nGalder Zamarreño <galder.zamarreno@redhat.com> <galder@zamarreno.com>\nGalder Zamarreño <galder.zamarreno@redhat.com> <galder@zamarreno.com>\nGalder Zamarreño <galder.zamarreno@redhat.com> <galder@zamarreno.com>\nGalder Zamarreño <galder.zamarreno@redhat.com> <galder.zamarreno@jboss.com>\nGavin King <gavin@hibernate.org> <gavin@ceylon-lang.org>\nGavin King <gavin@hibernate.org> Gavin <gavin@hibernate.org>\nGreg Luck <gluck@gregluck.com> Greg Luck <gluck@Greg-Lucks-Laptop.local>\nGuillaume Smet <guillaume@hibernate.org> Guillaume Smet <guillaume.smet@gmail.com>\nHardy Ferentschik <hardy@hibernate.org> <hardy@ferentschik.de>\nHardy Ferentschik <hardy@hibernate.org> <hibernate@ferentschik.de>\nHarsh Panchal <panchal.harsh18@gmail.com> BOOTMGR <panchal.harsh18@gmail.com>\nHernán Chanfreau <hchanfreau@gmail.com> hernan <hchanfreau@gmail.com>\nJaikiran Pai <jaikiran.pai@gmail.com> Jaikiran <jaikiran@users.noreply.github.com>\nJeremy Whiting <jwhiting@redhat.com> Jeremy Whiting <whitingjr@hotmail.com>\nJohn O'Hara <johara@redhat.com> johara <johnaohara80@gmail.com>\nJohn O'Hara <johara@redhat.com> <johara@localhost.localdomain>\nJohn O'Hara <johara@redhat.com> JohnOhara <johara@redhat.com>\nJohn O'Hara <johara@redhat.com> John OHara <johara@redhat.com>\nJohn Verhaeg <jverhaeg@redhat.com> John Verhaeg <john.verhaeg@gmail.com>\nJohn Verhaeg <jverhaeg@redhat.com> JPAV <jverhaeg@redhat.com>\nLoïc LEFEVRE <loic.lefevre@gmail.com> LLEFEVRE <loic.lefevre@gmail.com>\nLuis Barreiro <lbarreiro@redhat.com> barreiro <lbbbarreiro@gmail.com>\nLukasz Antoniak <lukasz.antoniak@gmail.com> lukasz-antoniak <lukasz.antoniak@gmail.com>\nMarco Belladelli <marco@hibernate.org> marco <marco.belladelli@semenda.it>\nMarco Belladelli <marco@hibernate.org> <mbellade@redhat.com>\nMisty Stanley-Jones <misty@redhat.com> misty <misty@redhat.com>\nMisty Stanley-Jones <misty@redhat.com> <mstanley@cheezel.(none)>\nMisty Stanley-Jones <misty@redhat.com> mstanleyjones <misty@redhat.com>\nNathan Xu <nathan.qingyang.xu@gmail.com> <nathan.xu@procor.com>\nNathan Xu <nathan.qingyang.xu@gmail.com> <nathan_xu@ultimatesoftware.com>\nOliver Breidenbach <obr@xima.de> obr <obr@xima.de>\nRadim Vansa <rvansa@redhat.com> rvansa <rvansa@redhat.com> \nSanne Grinovero <sanne@hibernate.org> <sanne.grinovero@gmail.com>\nScott Marlow <smarlow@redhat.com> Scott Marlow <scott.marlow@gmail.com>\nScott Marlow <smarlow@redhat.com> smarlow <smarlow@redhat.com>\nSimeon <simeon.malchev@gmail.com> simeonmalchev <simeon.malchev@gmail.com>\nStåle W. Pedersen <stale.pedersen@jboss.org> <stalep@gmail.com>\nSteve Ebersole <steve@hibernate.org> <steve@apollo.(none)>\nSteve Ebersole <steve@hibernate.org> <steve@t510.(none)>\nStrong Liu <stliu@hibernate.org> <stliu@redhat.com>\nVlad Mihalcea <mihalcea.vlad@gmail.com> <mihalcea.vlad@gmail.com>\nVlad Mihalcea <mihalcea.vlad@gmail.com> <mih_vlad@yahoo.com>\nVlad Mihalcea <mihalcea.vlad@gmail.com> <Vlad Mihalcea>\nVlad Mihalcea <mihalcea.vlad@gmail.com> <vlad@vladmihalcea.com>\nYoann Rodière <yoann@hibernate.org> Yoann Rodière <yoann.rodiere@openwide.fr>\nYoann Rodière <yoann@hibernate.org> Yoann Rodière <yrodiere@redhat.com>\n\n"
        },
        {
          "name": ".release",
          "type": "tree",
          "content": null
        },
        {
          "name": "CONTRIBUTING.md",
          "type": "blob",
          "size": 5.7216796875,
          "content": "# Contributing\n\nContributions from the community are essential in keeping Hibernate (and any Open Source\nproject really) strong and successful.  \n\n# Legal\n\nAll original contributions to Hibernate are licensed under the \n[GNU Lesser General Public License (LGPL)](https://www.gnu.org/licenses/old-licenses/lgpl-2.1.txt), \nversion 2.1 or later, or, if another license is specified as governing the file or directory being \nmodified, such other license.\n\nThe LGPL text is included verbatim in the [lgpl.txt](lgpl.txt) file in the root directory of the ORM repository.\n\nAll contributions are subject to the [Developer Certificate of Origin (DCO)](https://developercertificate.org/).  \n\nThe DCO text is available verbatim in the [dco.txt](dco.txt) file in the root directory of the ORM repository.\n\n\n## Guidelines\n\nWhile we try to keep requirements for contributing to a minimum, there are a few guidelines \nwe ask that you mind.\n\nFor code contributions, these guidelines include:\n* Respect the project code style - find templates for [IntelliJ IDEA](https://hibernate.org/community/contribute/intellij-idea/) or [Eclipse](https://hibernate.org/community/contribute/eclipse-ide/)\n* Have a corresponding JIRA [issue](https://hibernate.atlassian.net/browse/HHH) and be sure to include the key for this JIRA issue in your commit messages.\n* Have a set of appropriate tests.  \n  \tFor your convenience, a [set of test templates](https://github.com/hibernate/hibernate-test-case-templates/tree/main/orm) have been made available.\n  \t\n\tWhen submitting bug reports, the tests should reproduce the initially reported bug and illustrate that your solution addresses the issue.\n\tFor features/enhancements, the tests should demonstrate that the feature works as intended.  \n    \tIn both cases, be sure to incorporate your tests into the project to protect against possible regressions.\n* If applicable, documentation should be updated to reflect the introduced changes\n* The code compiles and the tests pass (`./gradlew clean build`)\n\nFor documentation contributions, mainly to respect the project code style, especially in regards \nto the use of tabs - as mentioned above, code style templates are available for both IntelliJ IDEA and Eclipse\nIDEs.  Ideally, these contributions would also have a corresponding JIRA issue, although this \nis less necessary for documentation contributions.\n\n\n## Getting Started\n\nIf you are just getting started with Git, GitHub, and/or contributing to Hibernate via\nGitHub there are a few pre-requisite steps to follow:\n\n* Make sure you have a [Hibernate JIRA account](https://hibernate.atlassian.net)\n* Make sure you have a [GitHub account](https://github.com/signup/free)\n* [Fork](https://help.github.com/articles/fork-a-repo) the Hibernate repository.  As discussed in\nthe linked page, this also includes:\n    * [set up your local git install](https://help.github.com/articles/set-up-git) \n    * clone your fork\n* Instruct git to ignore certain commits when using `git blame`. From the directory of your local clone, run this: `git config blame.ignoreRevsFile .git-blame-ignore-revs`\n* See the wiki pages for setting up your IDE, whether you use \n[IntelliJ IDEA](https://hibernate.org/community/contribute/intellij-idea/)\nor [Eclipse](https://hibernate.org/community/contribute/eclipse-ide/)<sup>(1)</sup>.\n\n\n## Create the working (topic) branch\n\nCreate a [topic branch](https://git-scm.com/book/en/Git-Branching-Branching-Workflows#Topic-Branches) \non which you will work.  The convention is to incorporate the JIRA issue key in the name of this branch,\nalthough this is more of a mnemonic strategy than a hard-and-fast rule - but doing so helps:\n* Remember what each branch is for \n* Isolate the work from other contributions you may be working on\n\n_If there is not already a JIRA issue covering the work you want to do, create one._\n  \nAssuming you will be working from the `main` branch and working\non the JIRA HHH-123 : `git checkout -b HHH-123 main`\n\n\n## Code\n\nDo your thing!\n\n\n## Commit\n\n* Make commits of logical units\n* Be sure to start each commit message using the **JIRA issue key**, this is how JIRA will pick\nup the related commits and display them on the JIRA issue\n* Make sure you have added the necessary tests for your changes\n* Run _all_ the tests to ensure nothing else was accidentally broken\n\n_Before committing, if you want to pull in the latest upstream changes (highly\nappreciated btw), please use rebasing rather than merging.  Merging creates\n\"merge commits\" that invariably muck up the project timeline._\n\n## Submit\n\n* Push your changes to the topic branch in your fork of the repository\n* Initiate a [pull request](https://help.github.com/articles/creating-a-pull-request)\n* Once your pull request has been submitted you can verify that the pull request has been properly linked to its corresponding Jira issue by confirming that the issue status is now _Waiting for Review_ and that clicking on the _Recent rule runs_ _Refresh_ button on the right side of the issue displays a _Pull Request (ORM)_ entry.\n\nIt is important that this topic branch of your fork:\n\n* Is isolated to just the work on this one JIRA issue, or multiple issues if they are\n\trelated and also fixed/implemented by this work.  The main point is to not push\n\tcommits for more than one PR to a single branch - GitHub PRs are linked to\n\ta branch rather than specific commits\n* remain until the PR is closed.  Once the underlying branch is deleted the corresponding\n\tPR will be closed, if not already, and the changes will be lost\n\n# Notes\n<sup>(1)</sup> Gradle `eclipse` plugin is no longer supported, so the recommended way to import the project in your IDE is with the proper IDE tools/plugins. Don't try to run `./gradlew clean eclipse --refresh-dependencies` from the command line as you'll get an error because `eclipse` no longer exists\n"
        },
        {
          "name": "Jenkinsfile",
          "type": "blob",
          "size": 14.048828125,
          "content": "/*\n * Hibernate, Relational Persistence for Idiomatic Java\n *\n * License: GNU Lesser General Public License (LGPL), version 2.1 or later.\n * See the lgpl.txt file in the root directory or <http://www.gnu.org/licenses/lgpl-2.1.html>.\n */\n\nimport groovy.transform.Field\nimport io.jenkins.blueocean.rest.impl.pipeline.PipelineNodeGraphVisitor\nimport io.jenkins.blueocean.rest.impl.pipeline.FlowNodeWrapper\nimport org.jenkinsci.plugins.workflow.support.steps.build.RunWrapper\n\n/*\n * See https://github.com/hibernate/hibernate-jenkins-pipeline-helpers\n */\n@Library('hibernate-jenkins-pipeline-helpers') _\nimport org.hibernate.jenkins.pipeline.helpers.job.JobHelper\n\n@Field final String DEFAULT_JDK_VERSION = '17'\n@Field final String DEFAULT_JDK_TOOL = \"OpenJDK ${DEFAULT_JDK_VERSION} Latest\"\n@Field final String NODE_PATTERN_BASE = 'Worker&&Containers'\n@Field List<BuildEnvironment> environments\n\nthis.helper = new JobHelper(this)\n\nhelper.runWithNotification {\nstage('Configure') {\n\trequireApprovalForPullRequest 'hibernate'\n\n\tthis.environments = [\n//\t\tnew BuildEnvironment( dbName: 'h2' ),\n//\t\tnew BuildEnvironment( dbName: 'hsqldb' ),\n//\t\tnew BuildEnvironment( dbName: 'derby' ),\n//\t\tnew BuildEnvironment( dbName: 'mysql' ),\n//\t\tnew BuildEnvironment( dbName: 'mariadb' ),\n//\t\tnew BuildEnvironment( dbName: 'postgresql' ),\n//\t\tnew BuildEnvironment( dbName: 'edb' ),\n//\t\tnew BuildEnvironment( dbName: 'oracle' ),\n//\t\tnew BuildEnvironment( dbName: 'db2' ),\n//\t\tnew BuildEnvironment( dbName: 'mssql' ),\n//\t\tnew BuildEnvironment( dbName: 'sybase' ),\n// Don't build with HANA by default, but only do it nightly until we receive a 3rd instance\n// \t\tnew BuildEnvironment( dbName: 'hana_cloud', dbLockableResource: 'hana-cloud', dbLockResourceAsHost: true ),\n\t\tnew BuildEnvironment( node: 's390x' ),\n\t\tnew BuildEnvironment( dbName: 'tidb', node: 'tidb',\n\t\t\t\tnotificationRecipients: 'tidb_hibernate@pingcap.com' ),\n\t\t// We want to enable preview features when testing newer builds of OpenJDK:\n\t\t// even if we don't use these features, just enabling them can cause side effects\n\t\t// and it's useful to test that.\n\t\tnew BuildEnvironment( testJdkVersion: '21', testJdkLauncherArgs: '--enable-preview' ),\n\t\tnew BuildEnvironment( testJdkVersion: '23', testJdkLauncherArgs: '--enable-preview' ),\n\t\tnew BuildEnvironment( testJdkVersion: '24', testJdkLauncherArgs: '--enable-preview' ),\n\t\t// The following JDKs aren't supported by Hibernate ORM out-of-the box yet:\n\t\t// they require the use of -Dnet.bytebuddy.experimental=true.\n\t\t// Make sure to remove that argument as soon as possible\n\t\t// -- generally that requires upgrading bytebuddy after the JDK goes GA.\n\t\tnew BuildEnvironment( testJdkVersion: '25', testJdkLauncherArgs: '--enable-preview -Dnet.bytebuddy.experimental=true' )\n\t];\n\n\tif ( env.CHANGE_ID ) {\n\t\tif ( pullRequest.labels.contains( 'cockroachdb' ) ) {\n\t\t\tthis.environments.add( new BuildEnvironment( dbName: 'cockroachdb', node: 'cockroachdb', longRunning: true ) )\n\t\t}\n\t\tif ( pullRequest.labels.contains( 'hana' ) ) {\n\t\t\tthis.environments.add( new BuildEnvironment( dbName: 'hana_cloud', dbLockableResource: 'hana-cloud', dbLockResourceAsHost: true ) )\n\t\t}\n\t\tif ( pullRequest.labels.contains( 'sybase' ) ) {\n\t\t\tthis.environments.add( new BuildEnvironment( dbName: 'sybase_jconn' ) )\n\t\t}\n\t}\n\n\thelper.configure {\n\t\tfile 'job-configuration.yaml'\n\t\t// We don't require the following, but the build helper plugin apparently does\n\t\tjdk {\n\t\t\tdefaultTool DEFAULT_JDK_TOOL\n\t\t}\n\t\tmaven {\n\t\t\tdefaultTool 'Apache Maven 3.8'\n\t\t}\n\t}\n\tproperties([\n\t\t\tbuildDiscarder(\n\t\t\t\t\tlogRotator(daysToKeepStr: '30', numToKeepStr: '10')\n\t\t\t),\n\t\t\t// If two builds are about the same branch or pull request,\n\t\t\t// the older one will be aborted when the newer one starts.\n\t\t\tdisableConcurrentBuilds(abortPrevious: true),\n\t\t\thelper.generateNotificationProperty()\n\t])\n}\n\n// Avoid running the pipeline on branch indexing\nif (currentBuild.getBuildCauses().toString().contains('BranchIndexingCause')) {\n  \tprint \"INFO: Build skipped due to trigger being Branch Indexing\"\n\tcurrentBuild.result = 'NOT_BUILT'\n  \treturn\n}\n\nstage('Build') {\n\tMap<String, Closure> executions = [:]\n\tMap<String, Map<String, String>> state = [:]\n\tenvironments.each { BuildEnvironment buildEnv ->\n\t\t// Don't build environments for newer JDKs when this is a PR\n\t\tif ( helper.scmSource.pullRequest && buildEnv.testJdkVersion ) {\n\t\t\treturn\n\t\t}\n\t\tstate[buildEnv.tag] = [:]\n\t\texecutions.put(buildEnv.tag, {\n\t\t\trunBuildOnNode(buildEnv.node ?: NODE_PATTERN_BASE) {\n\t\t\t\tdef testJavaHome\n\t\t\t\tif ( buildEnv.testJdkVersion ) {\n\t\t\t\t\ttestJavaHome = tool(name: \"OpenJDK ${buildEnv.testJdkVersion} Latest\", type: 'jdk')\n\t\t\t\t}\n\t\t\t\tdef javaHome = tool(name: DEFAULT_JDK_TOOL, type: 'jdk')\n\t\t\t\t// Use withEnv instead of setting env directly, as that is global!\n\t\t\t\t// See https://github.com/jenkinsci/pipeline-plugin/blob/master/TUTORIAL.md\n\t\t\t\twithEnv([\"JAVA_HOME=${javaHome}\", \"PATH+JAVA=${javaHome}/bin\"]) {\n\t\t\t\t\tstate[buildEnv.tag]['additionalOptions'] = '-PmavenMirror=nexus-load-balancer-c4cf05fd92f43ef8.elb.us-east-1.amazonaws.com'\n\t\t\t\t\tif ( testJavaHome ) {\n\t\t\t\t\t\tstate[buildEnv.tag]['additionalOptions'] = state[buildEnv.tag]['additionalOptions'] +\n\t\t\t\t\t\t\t\t\" -Ptest.jdk.version=${buildEnv.testJdkVersion} -Porg.gradle.java.installations.paths=${javaHome},${testJavaHome}\"\n\t\t\t\t\t}\n\t\t\t\t\tif ( buildEnv.testJdkLauncherArgs ) {\n\t\t\t\t\t\tstate[buildEnv.tag]['additionalOptions'] = state[buildEnv.tag]['additionalOptions'] +\n\t\t\t\t\t\t\t\t\" -Ptest.jdk.launcher.args='${buildEnv.testJdkLauncherArgs}'\"\n\t\t\t\t\t}\n\t\t\t\t\tif ( buildEnv.node ) {\n\t\t\t\t\t\tstate[buildEnv.tag]['additionalOptions'] = state[buildEnv.tag]['additionalOptions'] +\n\t\t\t\t\t\t\t\t\" -Pci.node=${buildEnv.node}\"\n\t\t\t\t\t}\n\t\t\t\t\tstate[buildEnv.tag]['containerName'] = null;\n\t\t\t\t\tstage('Checkout') {\n\t\t\t\t\t\tcheckout scm\n\t\t\t\t\t}\n\t\t\t\t\ttryFinally({\n\t\t\t\t\t\tstage('Start database') {\n\t\t\t\t\t\t\tswitch (buildEnv.dbName) {\n\t\t\t\t\t\t\t\tcase \"edb\":\n\t\t\t\t\t\t\t\t\tdocker.image('quay.io/enterprisedb/edb-postgres-advanced:15.4-3.3-postgis').pull()\n\t\t\t\t\t\t\t\t\tsh \"./docker_db.sh edb\"\n\t\t\t\t\t\t\t\t\tstate[buildEnv.tag]['containerName'] = \"edb\"\n\t\t\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\t\t\tcase \"sybase_jconn\":\n\t\t\t\t\t\t\t\t\tdocker.withRegistry('https://index.docker.io/v1/', 'hibernateci.hub.docker.com') {\n\t\t\t\t\t\t\t\t\t\tdocker.image('nguoianphu/docker-sybase').pull()\n\t\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t\t\tsh \"./docker_db.sh sybase\"\n\t\t\t\t\t\t\t\t\tstate[buildEnv.tag]['containerName'] = \"sybase\"\n\t\t\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\t\t\tcase \"cockroachdb\":\n\t\t\t\t\t\t\t\t\tdocker.withRegistry('https://index.docker.io/v1/', 'hibernateci.hub.docker.com') {\n\t\t\t\t\t\t\t\t\t\tdocker.image('cockroachdb/cockroach:v23.1.12').pull()\n\t\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t\t\tsh \"./docker_db.sh cockroachdb\"\n\t\t\t\t\t\t\t\t\tstate[buildEnv.tag]['containerName'] = \"cockroach\"\n\t\t\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t}\n\t\t\t\t\t\tstage('Test') {\n\t\t\t\t\t\t\tString args = \"${buildEnv.additionalOptions ?: ''} ${state[buildEnv.tag]['additionalOptions'] ?: ''}\"\n\t\t\t\t\t\t\twithEnv([\"RDBMS=${buildEnv.dbName}\"]) {\n\t\t\t\t\t\t\t\ttryFinally({\n\t\t\t\t\t\t\t\t\tif (buildEnv.dbLockableResource == null) {\n\t\t\t\t\t\t\t\t\t\twithCredentials([file(credentialsId: 'sybase-jconnect-driver', variable: 'jconnect_driver')]) {\n\t\t\t\t\t\t\t\t\t\t\tsh 'cp -f $jconnect_driver ./drivers/jconn4.jar'\n\t\t\t\t\t\t\t\t\t\t\ttimeout( [time: buildEnv.longRunning ? 480 : 120, unit: 'MINUTES'] ) {\n\t\t\t\t\t\t\t\t\t\t\t\tciBuild buildEnv, args\n\t\t\t\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t\t\telse {\n\t\t\t\t\t\t\t\t\t\tlock(label: buildEnv.dbLockableResource, quantity: 1, variable: 'LOCKED_RESOURCE') {\n\t\t\t\t\t\t\t\t\t\t\tif ( buildEnv.dbLockResourceAsHost ) {\n\t\t\t\t\t\t\t\t\t\t\t\targs += \" -DdbHost=${LOCKED_RESOURCE}\"\n\t\t\t\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t\t\t\t\ttimeout( [time: buildEnv.longRunning ? 480 : 120, unit: 'MINUTES'] ) {\n\t\t\t\t\t\t\t\t\t\t\t\tciBuild buildEnv, args\n\t\t\t\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t\t}, {\n\t\t\t\t\t\t\t\t\tjunit '**/target/test-results/test/*.xml,**/target/test-results/testKitTest/*.xml'\n\t\t\t\t\t\t\t\t})\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t}\n\t\t\t\t\t}, { // Finally\n\t\t\t\t\t\tif ( state[buildEnv.tag]['containerName'] != null ) {\n\t\t\t\t\t\t\tsh \"docker rm -f ${state[buildEnv.tag]['containerName']}\"\n\t\t\t\t\t\t}\n\t\t\t\t\t\t// Skip this for PRs\n\t\t\t\t\t\tif ( !env.CHANGE_ID && buildEnv.notificationRecipients != null ) {\n\t\t\t\t\t\t\thandleNotifications(currentBuild, buildEnv)\n\t\t\t\t\t\t}\n\t\t\t\t\t})\n\t\t\t\t}\n\t\t\t}\n\t\t})\n\t}\n\tparallel(executions)\n}\n\n} // End of helper.runWithNotification\n\n// Job-specific helpers\n\nclass BuildEnvironment {\n\tString testJdkVersion\n\tString testJdkLauncherArgs\n\tString dbName = 'h2'\n\tString node\n\tString dbLockableResource\n\tboolean dbLockResourceAsHost\n\tString additionalOptions\n\tString notificationRecipients\n\tboolean longRunning\n\n\tString toString() { getTag() }\n\tString getTag() { \"${node ? node + \"_\" : ''}${testJdkVersion ? 'jdk_' + testJdkVersion + '_' : '' }${dbName}\" }\n\tString getRdbms() { dbName.contains(\"_\") ? dbName.substring(0, dbName.indexOf('_')) : dbName }\n}\n\nvoid runBuildOnNode(String label, Closure body) {\n\tnode( label ) {\n\t\tpruneDockerContainers()\n        tryFinally(body, { // Finally\n        \t// If this is a PR, we clean the workspace at the end\n        \tif ( env.CHANGE_BRANCH != null ) {\n        \t\tcleanWs()\n        \t}\n        \tpruneDockerContainers()\n        })\n\t}\n}\n\nvoid ciBuild(buildEnv, String args) {\n\tif ( !helper.scmSource.pullRequest ) {\n\t\t// Not a PR: we can pass credentials to the build, allowing it to populate the build cache\n\t\t// and to publish build scans directly.\n\n\t\t// On untrusted nodes, we use the same access key as for PRs:\n\t\t// it has limited access, essentially it can only push build scans.\n\t\tdef develocityCredentialsId = buildEnv.node ? 'ge.hibernate.org-access-key-pr' : 'ge.hibernate.org-access-key'\n\n\t\twithCredentials([string(credentialsId: develocityCredentialsId,\n\t\t\t\tvariable: 'DEVELOCITY_ACCESS_KEY')]) {\n\t\t\twithGradle { // withDevelocity, actually: https://plugins.jenkins.io/gradle/#plugin-content-capturing-build-scans-from-jenkins-pipeline\n\t\t\t\tsh \"./ci/build.sh $args\"\n\t\t\t}\n\t\t}\n\t}\n\telse if ( buildEnv.node != 's390x' ) { // We couldn't get the code below to work on s390x for some reason.\n\t\t// Pull request: we can't pass credentials to the build, since we'd be exposing secrets to e.g. tests.\n\t\t// We do the build first, then publish the build scan separately.\n\t\ttryFinally({\n\t\t\tsh \"./ci/build.sh $args\"\n\t\t}, { // Finally\n\t\t\twithCredentials([string(credentialsId: 'ge.hibernate.org-access-key-pr',\n\t\t\t\t\tvariable: 'DEVELOCITY_ACCESS_KEY')]) {\n\t\t\t\twithGradle { // withDevelocity, actually: https://plugins.jenkins.io/gradle/#plugin-content-capturing-build-scans-from-jenkins-pipeline\n\t\t\t\t\t// Don't fail a build if publishing fails\n\t\t\t\t\tsh './gradlew buildScanPublishPrevious || true'\n\t\t\t\t}\n\t\t\t}\n\t\t})\n\t}\n\telse {\n\t\t// Don't do build scans\n\t\tsh \"./ci/build.sh $args\"\n\t}\n}\n\nvoid pruneDockerContainers() {\n\tif ( !sh( script: 'command -v docker || true', returnStdout: true ).trim().isEmpty() ) {\n\t\tsh 'docker container prune -f || true'\n\t\tsh 'docker image prune -f || true'\n\t\tsh 'docker network prune -f || true'\n\t\tsh 'docker volume prune -f || true'\n\t}\n}\n\nvoid handleNotifications(currentBuild, buildEnv) {\n\tdef currentResult = getParallelResult(currentBuild, buildEnv.tag)\n\tboolean success = currentResult == 'SUCCESS' || currentResult == 'UNKNOWN'\n\tdef previousResult = currentBuild.previousBuild == null ? null : getParallelResult(currentBuild.previousBuild, buildEnv.tag)\n\n\t// Ignore success after success\n\tif ( !( success && previousResult == 'SUCCESS' ) ) {\n\t\tdef subject\n\t\tdef body\n\t\tif ( success ) {\n\t\t\tif ( previousResult != 'SUCCESS' && previousResult != null ) {\n\t\t\t\tsubject = \"${env.JOB_NAME} - Build ${env.BUILD_NUMBER} - Fixed\"\n\t\t\t\tbody = \"\"\"<p>${env.JOB_NAME} - Build ${env.BUILD_NUMBER} - Fixed:</p>\n\t\t\t\t\t<p>Check console output at <a href='${env.BUILD_URL}'>${env.BUILD_URL}</a> to view the results.</p>\"\"\"\n\t\t\t}\n\t\t\telse {\n\t\t\t\tsubject = \"${env.JOB_NAME} - Build ${env.BUILD_NUMBER} - Success\"\n\t\t\t\tbody = \"\"\"<p>${env.JOB_NAME} - Build ${env.BUILD_NUMBER} - Success:</p>\n\t\t\t\t\t<p>Check console output at <a href='${env.BUILD_URL}'>${env.BUILD_URL}</a> to view the results.</p>\"\"\"\n\t\t\t}\n\t\t}\n\t\telse if (currentBuild.rawBuild.getActions(jenkins.model.InterruptedBuildAction.class).isEmpty()) {\n\t\t\t// If there are interrupted build actions, this means the build was cancelled, probably superseded\n\t\t\t// Thanks to https://issues.jenkins.io/browse/JENKINS-43339 for the \"hack\" to determine this\n\t\t\tif ( currentResult == 'FAILURE' ) {\n\t\t\t\tif ( previousResult != null && previousResult == \"FAILURE\" ) {\n\t\t\t\t\tsubject = \"${env.JOB_NAME} - Build ${env.BUILD_NUMBER} - Still failing\"\n\t\t\t\t\tbody = \"\"\"<p>${env.JOB_NAME} - Build ${env.BUILD_NUMBER} - Still failing:</p>\n\t\t\t\t\t\t<p>Check console output at <a href='${env.BUILD_URL}'>${env.BUILD_URL}</a> to view the results.</p>\"\"\"\n\t\t\t\t}\n\t\t\t\telse {\n\t\t\t\t\tsubject = \"${env.JOB_NAME} - Build ${env.BUILD_NUMBER} - Failure\"\n\t\t\t\t\tbody = \"\"\"<p>${env.JOB_NAME} - Build ${env.BUILD_NUMBER} - Failure:</p>\n\t\t\t\t\t\t<p>Check console output at <a href='${env.BUILD_URL}'>${env.BUILD_URL}</a> to view the results.</p>\"\"\"\n\t\t\t\t}\n\t\t\t}\n\t\t\telse {\n\t\t\t\tsubject = \"${env.JOB_NAME} - Build ${env.BUILD_NUMBER} - ${currentResult}\"\n\t\t\t\tbody = \"\"\"<p>${env.JOB_NAME} - Build ${env.BUILD_NUMBER} - ${currentResult}:</p>\n\t\t\t\t\t<p>Check console output at <a href='${env.BUILD_URL}'>${env.BUILD_URL}</a> to view the results.</p>\"\"\"\n\t\t\t}\n\t\t}\n\n\t\temailext(\n\t\t\t\tsubject: subject,\n\t\t\t\tbody: body,\n\t\t\t\tto: buildEnv.notificationRecipients\n\t\t)\n\t}\n}\n\n@NonCPS\nString getParallelResult( RunWrapper build, String parallelBranchName ) {\n    def visitor = new PipelineNodeGraphVisitor( build.rawBuild )\n    def branch = visitor.pipelineNodes.find{ it.type == FlowNodeWrapper.NodeType.PARALLEL && parallelBranchName == it.displayName }\n    if ( branch == null ) {\n    \techo \"Couldn't find parallel branch name '$parallelBranchName'. Available parallel branch names:\"\n\t\tvisitor.pipelineNodes.findAll{ it.type == FlowNodeWrapper.NodeType.PARALLEL }.each{\n\t\t\techo \" - ${it.displayName}\"\n\t\t}\n    \treturn null;\n    }\n    return branch.status.result\n}\n\n// try-finally construct that properly suppresses exceptions thrown in the finally block.\nstatic def tryFinally(Closure main, Closure ... finallies) {\n\tdef mainFailure = null\n\ttry {\n\t\tmain()\n\t}\n\tcatch (Throwable t) {\n\t\tmainFailure = t\n\t\tthrow t\n\t}\n\tfinally {\n\t\tfinallies.each {it ->\n\t\t\ttry {\n\t\t\t\tit()\n\t\t\t}\n\t\t\tcatch (Throwable t) {\n\t\t\t\tif ( mainFailure ) {\n\t\t\t\t\tmainFailure.addSuppressed( t )\n\t\t\t\t}\n\t\t\t\telse {\n\t\t\t\t\tmainFailure = t\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\tif ( mainFailure ) { // We may reach here if only the \"finally\" failed\n\t\tthrow mainFailure\n\t}\n}\n"
        },
        {
          "name": "MAINTAINERS.md",
          "type": "blob",
          "size": 5.712890625,
          "content": "Guide for maintainers of Hibernate ORM\n====\n\nThis guide is intended for maintainers of Hibernate ORM,\ni.e. anybody with direct push access to the git repository.\n\n## Contributing\n\nSee [CONTRIBUTING.md](CONTRIBUTING.md).\n\n## Continuous integration\n\nContinuous integration is split across two platforms:\n\n* GitHub Actions at https://github.com/hibernate/hibernate-orm/actions\n* a self-hosted Jenkins instance at https://ci.hibernate.org.\n\n### GitHub Actions workflows\n\nTODO: describe the workflows available.\n\n### Jenkins main pipeline\n\nhttps://ci.hibernate.org/job/hibernate-orm-pipeline/\n\nThis job takes care of testing additional DBs for:\n\n* Primary branch builds\n* Pull request builds\n\nIt is generally triggered on push,\nbut can also be triggered manually,\nwhich is particularly useful to test more environments on a pull request.\n\nSee [Jenkinsfile](Jenkinsfile) for the job definition.\n\n### Release pipeline\n\nhttps://ci.hibernate.org/job/hibernate-orm-release/\n\nThis job takes care of releases. It is triggered manually.\n\nSee [ci/release/Jenkinsfile](ci/release/Jenkinsfile) for the job definition.\n\nSee [Releasing](#releasing) for more information.\n\n## <a id=\"releasing\"></a> Releasing\n\n### Automated releases\n\nOn select maintenance branches (`6.2`, `6.4`, ...),\nmicro releases (`x.y.1`, `x.y.2`, ...) are performed as soon as you push to that branch.\n\nMake sure to assign fix versions properly before merging pull requests.\n\nNo announcements are expected for such releases:\nneither through X, blog posts, or email.\n\n### Manual releases\n\nOn `main` and some maintenance branches (`6.5`, ...),\nautomated releases are disabled.\n\nYou must perform releases by manually triggering a CI job.\n\n#### Preparing the release\n\nIn any case, before the release:\n\n* Check that everything has been pushed to the upstream repository.\n* Check that the [CI jobs](#continuous-integration) for the branch you want to release are green.\n* Check Jira [Releases](https://hibernate.atlassian.net/projects/HHH?selectedItem=com.atlassian.jira.jira-projects-plugin%3Arelease-page):\n  * Check that the release you are about to publish exists in Jira.\n  * Check there are no outstanding issues assigned to that release.\n  * Check there are no resolved/closed issues in the corresponding \"work-in-progress version\"\n    (e.g. `6.6`, `6.6-next`, ... naming convention may vary);\n    if there are, you might want to assign them to your release.\n\n**If it is a new major or minor release**, before the release:\n\n* Reset the migration guide to include only information relevant to the new major or minor.\n\n**If it's a `.CR` or `.Final` release**, before the release:\n\n* Check that the [migration guide](documentation/src/main/asciidoc/migration/index.adoc) is up to date.\n  In particular, check the git history for API/SPI changes\n  and document them in the migration guide.\n\n#### Performing the release\n\nOnce you trigger the CI job, it automatically pushes artifacts to the\n[OSSRH Maven Repository](https://repo1.maven.org/maven2/org/hibernate/orm/),\nand the documentation to [docs.jboss.org](https://docs.jboss.org/hibernate/orm/).\n\n* Do *not* mark the Jira Release as \"released\" or close issues,\n  the release job does it for you.\n* Do *not* update the repository (in particular changelog.txt and README.md), \n  the release job does it for you.\n* Trigger the release on CI:\n  * Go to CI, to [the \"hibernate-orm-release\" CI job](https://ci.hibernate.org/job/hibernate-orm-release/).\n  * Click the \"run\" button (the green triangle on top of a clock, to the right) next to the branch you want to release.\n  * **Be careful** when filling the form with the build parameters.\n    Note only `RELEASE_VERSION` is absolutely necessary.\n  * Note that for new branches where the job has never run, the first run may not ask for parameters and thus may fail:\n    that's expected, just run it again.\n\nAfter the job succeeds:\n\n* Update [hibernate.org](https://github.com/hibernate/hibernate.org) if necessary:\n  * If it is a new major or minor release, add a `_data/projects/orm/releases/series.yml` file\n    and a `orm/releases/<version>/index.adoc` file.\n  * Adjust the release file in `_data/projects/orm/releases`: use a meaningful summary and set `announcement_url` to the blog post, if any.\n  * Depending on which series you want to have displayed,\n    make sure to adjust the `status`/`displayed` attributes of the `series.yml` file of the old series.\n  * Push to the production branch.\n* Check that the artifacts are available on Maven Central:\n  https://repo1.maven.org/maven2/org/hibernate/orm/hibernate-core/.\n  They should appear after a few minutes, sometimes a few hours.\n* Make sure a GitHub release got created and that everything looks ok.\n\n\n#### Announcing the release\n\n* Send an email to `hibernate-announce@lists.jboss.org` and CC `hibernate-dev@lists.jboss.org`.\n* Tweet about the release via the `@Hibernate` account.\n\n#### Updating depending projects\n\nIf you just released the latest stable, you will need to update other projects:\n\n* Approve and merge automatic updates that dependabot will send (it might take ~24h):\n  * In the [test case templates](https://github.com/hibernate/hibernate-test-case-templates/tree/master/orm).\n  * In the [demos](https://github.com/hibernate/hibernate-demos/tree/master/hibernate-orm).\n* **If it's a `.Final` release**, upgrade the Hibernate ORM dependency manually:\n  * In the [Quarkus BOM](https://github.com/quarkusio/quarkus/blob/main/bom/application/pom.xml).\n  * In any other relevant project.\n\n#### Updating Hibernate ORM\n\nIn any case:\n\n* Reset [release_notes.md](release_notes.md).\n\n**If it is a new major or minor release**:\n\n* Reset the migration guide on the `main` branch if you forgot about it when preparing the release.\n* Create a maintenance branch for the previous series, if necessary; see [branching](branching.adoc).\n"
        },
        {
          "name": "README.adoc",
          "type": "blob",
          "size": 7.029296875,
          "content": "Hibernate ORM is a powerful object/relational mapping solution for Java, and makes it easy to develop persistence logic for applications, libraries, and frameworks.\n\nHibernate implements JPA, the standard API for object/relational persistence in Java, but also offers an extensive set of features and APIs which go beyond the specification.\n\nSee https://hibernate.org/orm/[Hibernate.org] for more information.\n\nimage:https://ci.hibernate.org/job/hibernate-orm-pipeline/job/main/badge/icon[Build Status,link=https://ci.hibernate.org/job/hibernate-orm-pipeline/job/main/]\nimage:https://img.shields.io/badge/Revved%20up%20by-Develocity-06A0CE?logo=Gradle&labelColor=02303A[link=https://ge.hibernate.org/scans]\n\n== Continuous Integration\n\nHibernate uses both https://jenkins-ci.org[Jenkins] and https://github.com/features/actions[GitHub Actions]\nfor its CI needs. See\n\n* https://ci.hibernate.org/view/ORM/[Jenkins Jobs]\n* https://github.com/hibernate/hibernate-orm/actions[GitHub Actions Jobs]\n\n== Building from sources\n\nThe build requires at least Java 11 and at most Java 17.\n\nHibernate uses https://gradle.org[Gradle] as its build tool. See the _Gradle Primer_ section below if you are new to\nGradle.\n\nContributors should read the link:CONTRIBUTING.md[Contributing Guide].\n\nSee the guides for setting up https://hibernate.org/community/contribute/intellij-idea/[IntelliJ] or\nhttps://hibernate.org/community/contribute/eclipse-ide/[Eclipse] as your development environment.\n\n== Gradle Primer\n\nThe Gradle build tool has amazing documentation.  2 in particular that are indispensable:\n\n* https://docs.gradle.org/current/userguide/userguide_single.html[Gradle User Guide] is a typical user guide in that\nit follows a topical approach to describing all of the capabilities of Gradle.\n* https://docs.gradle.org/current/dsl/index.html[Gradle DSL Guide] is unique and excellent in quickly\ngetting up to speed on certain aspects of Gradle.\n\nWe will cover the basics developers and contributors new to Gradle need to know to get productive quickly.\n\nNOTE: The project defines a https://docs.gradle.org/current/userguide/gradle_wrapper.html[Gradle Wrapper].\nThe rest of the section will assume execution through the wrapper.\n\n=== Executing Tasks\n\nGradle uses the concept of build tasks (equivalent to Ant targets or Maven phases/goals). You can get a list of\navailable tasks via \n\n----\ngradle tasks\n----\n\nTo execute a task across all modules, simply perform that task from the root directory. Gradle will visit each\nsub-project and execute that task if the sub-project defines it. To execute a task in a specific module you can\neither:\n\n. `cd` into that module directory and execute the task\n. name the \"task path\". For example, to run the tests for the _hibernate-core_ module from the root directory\nyou could say `gradle hibernate-core:test`\n\n=== Common tasks\n\nThe common tasks you might use in building Hibernate include:\n\n* _build_ - Assembles (jars) and tests this project\n* _compile_ - Performs all compilation tasks including staging resources from both main and test\n* _jar_ - Generates a jar archive with all the compiled classes\n* _test_ - Runs the tests\n* _publishToMavenLocal_ - Installs the project jar to your local maven cache (aka ~/.m2/repository). Note that Gradle\nnever uses this, but it can be useful for testing your build with other local Maven-based builds.\n* _clean_ - Cleans the build directory\n\n== Testing and databases\n\nTesting against a specific database can be achieved in 2 different ways:\n\n=== Using the \"Matrix Testing Plugin\" for Gradle.\n\nComing later…\n\n=== Using \"profiles\"\n\nThe Hibernate build defines several database testing \"profiles\" in `databases.gradle`. These\nprofiles can be activated by name using the `db` build property which can be passed either as\na JVM system prop (`-D`) or as a Gradle project property (`-P`). Examples below use the Gradle\nproject property approach.\n\n----\ngradle clean build -Pdb=pgsql\n----\n\nTo run a test from your IDE, you need to ensure the property expansions happen.\nUse the following command:\n\n----\ngradle clean compile -Pdb=pgsql\n----\n\n__NOTE: If you are running tests against a JDBC driver that is not available via Maven central be sure to\nadd these drivers to your local Maven repo cache (~/.m2/repository) or (better) add it to a personal Maven repo server__\n\n=== Running database-specific tests from the IDE using \"profiles\"\n\nYou can run any test on any particular database that is configured in a `databases.gradle` profile.\n\nAll you have to do is run the following command:\n\n----\n./gradlew setDataBase -Pdb=pgsql\n----\n\nor you can use the shortcut version: \n\n----\n./gradlew sDB -Pdb=pgsql\n----\n\nYou can do this from the module which you are interested in testing or from the `hibernate-orm` root folder.\n\nAfterward, just pick any test from the IDE and run it as usual. Hibernate will pick the database configuration from the `hibernate.properties`\nfile that was set up by the `setDataBase` Gradle task.\n\n=== Starting test databases locally as docker containers\n\nYou don't have to install all databases locally to be able to test against them in case you have docker available.\nThe script `docker_db.sh` allows you to start a pre-configured database which can be used for testing.\n\nAll you have to do is run the following command:\n\n----\n./docker_db.sh postgresql\n----\n\nomitting the argument will print a list of possible options.\n\nWhen the database is properly started, you can run tests with special profiles that are suffixed with `_ci`\ne.g. `pgsql_ci` for PostgreSQL. By using the system property `dbHost` you can configure the IP address of your docker host.\n\nThe command for running tests could look like the following:\n\n----\n./gradlew test -Pdb=pgsql_ci \"-DdbHost=192.168.99.100\"\n----\n\nThe following table illustrates a list of commands for various databases that can be tested locally.\n\n|===\n|Database |`docker_db.sh` |Gradle command\n\n|H2\n|-\n|`./gradlew test -Pdb=h2`\n\n|HSQLDB\n|-\n|`./gradlew test -Pdb=hsqldb`\n\n|Apache Derby\n|-\n|`./gradlew test -Pdb=derby`\n\n|MySQL\n|`./docker_db.sh mysql`\n|`./gradlew test -Pdb=mysql_ci`\n\n|MariaDB\n|`./docker_db.sh mariadb`\n|`./gradlew test -Pdb=mariadb_ci`\n\n|PostgreSQL\n|`./docker_db.sh postgresql`\n|`./gradlew test -Pdb=pgsql_ci`\n\n|EnterpriseDB\n|`./docker_db.sh edb`\n|`./gradlew test -Pdb=edb_ci`\n\n|Oracle\n|`./docker_db.sh oracle`\n|`./gradlew test -Pdb=oracle_ci`\n\n|DB2\n|`./docker_db.sh db2`\n|`./gradlew test -Pdb=db2_ci`\n\n|SQL Server\n|`./docker_db.sh mssql`\n|`./gradlew test -Pdb=mssql_ci`\n\n|Sybase ASE (jTDS)\n|`./docker_db.sh sybase`\n|`./gradlew test -Pdb=sybase_ci`\n\n|Sybase ASE (jConnect)\n|`./docker_db.sh sybase`\n|`./gradlew test -Pdb=sybase_jconn_ci`\n\n|SAP HANA\n|`./docker_db.sh hana`\n|`./gradlew test -Pdb=hana_ci`\n\n|CockroachDB\n|`./docker_db.sh cockroachdb`\n|`./gradlew test -Pdb=cockroachdb`\n\n|TiDB\n|`./docker_db.sh tidb`\n|`./gradlew test -Pdb=tidb`\n\n|Informix\n|`./docker_db.sh informix`\n|`./gradlew test -Pdb=informix`\n|===\n\nTo stop a container started by `docker`, use the command\n\n[source]\n----\ndocker stop $container_name\n----\n\nNOTE:: Substitute `podman` command for `docker` if using `podman`\n\nE.g., to stop the mariadb container\n\n[source]\n----\ndocker stop mariadb\n----\n"
        },
        {
          "name": "annotation-descriptor-generator",
          "type": "tree",
          "content": null
        },
        {
          "name": "branching.adoc",
          "type": "blob",
          "size": 1.759765625,
          "content": "= ORM Branching\n\nDescribes the paradigm used for branching within the ORM project\n\n[[branches]]\n== The Branches\n\n* `main` is where we do \"latest stable\" development. Which specific release family this targets is dependent upon the \"critical mass\" discussion in <<process>>.\n* \"Dedicated release branches\" (`5.6`, `6.0`, `6.1`, `6.2`, ...) represent previous, no longer supported releases. Branched for posterity.\n* PR branches for new features, improvements, disruptive bugfixes, etc target main\n* PR branches for performance improvements, security fixes and bugfixes target the affected minor branches (which could be main for a short period of time)\n\n[[process]]\n== The Process\n\nProcess (using 6.3 -> 6.4 as an example):\n\n* As mentioned, all new features, improvements, disruptive bugfixes, etc. are developed on topic branches (PR) against main. Based on sprint planning, these will be given a priority and target a particular major/minor release.\n* Once we have critical mass for topic branches targeting 6.4:\n    * `main` will be branched as `6.3` and a 6.3.x release will be done.\n    * The finished topic branches will be integrated into main and a 6.4.0 Alpha (or Beta or CR)[1] release will be done.\n* A bot will cherry-pick changes made to the latest \"dedicated release branch\" (here, 6.3) and create a PR against main (on the assumption that these changes might be needed there as well). TBD if we want to auto-apply these PRs on successful build.\n* PRs against older \"dedicated release branches\", will first be rebased to the latest stable branch and applied. We'll decide between 6.3 and main based mostly on ; if 6.3, the bot will pick it up.\n\n[1] Historically I am not a huge fan of a full Alpha/Beta/CR cycle for minor releases, usually just doing CRs. But open to convincing otherwise."
        },
        {
          "name": "build.gradle",
          "type": "blob",
          "size": 2.7353515625,
          "content": "/*\n * Hibernate, Relational Persistence for Idiomatic Java\n *\n * License: GNU Lesser General Public License (LGPL), version 2.1 or later.\n * See the lgpl.txt file in the root directory or <http://www.gnu.org/licenses/lgpl-2.1.html>.\n */\n\nbuildscript {\n//\trepositories {\n//\t\tmavenCentral()\n//\t}\n\n\tdependencies {\n//\t\tclasspath 'org.asciidoctor:asciidoctor-gradle-plugin:1.5.7'\n\t\tclasspath buildscriptLibs.forbiddenapis\n\t\tclasspath 'org.junit.platform:junit-platform-gradle-plugin:1.0.1'\n\t}\n}\n\n\nplugins {\n\tid \"local.module\"\n\n\tid \"org.hibernate.build.version-injection\" version \"2.0.0\" apply false\n\tid 'org.hibernate.matrix-test' version '3.1.1' apply false\n\tid 'org.hibernate.orm.database-service' apply false\n\tid 'biz.aQute.bnd' version '7.0.0' apply false\n\n\tid 'com.diffplug.spotless' version '6.25.0'\n\tid 'org.checkerframework' version '0.6.40'\n\tid 'org.hibernate.orm.build.jdks'\n\n\tid 'io.github.gradle-nexus.publish-plugin' version '2.0.0'\n\n\tid 'idea'\n\tid 'org.jetbrains.gradle.plugin.idea-ext' version '1.0'\n\tid 'eclipse'\n\tid \"com.dorongold.task-tree\" version \"2.1.1\"\n}\n\n\n// ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n// Releasing\n\ntasks.register( 'releasePrepare' ) {\n\tgroup \"release-prepare\"\n\tdescription \"Scripted release 'Release Prepare' stage.  \" +\n\t\t\t\"Includes various checks as to the publish-ability of the project: testing, generation, etc.  \" +\n\t\t\t\"Sub-projects register their own `releasePrepare` to hook into this stage.\"\n\t// See `:release:releasePrepare` which does a lot of heavy lifting here\n}\n\ntasks.register( 'releasePerform' ) {\n\tgroup \"release-perform\"\n\tdescription \"Scripted release 'Release Perform' stage.  \" +\n\t\t\t\"Generally this entails publishing artifacts to various servers.  \" +\n\t\t\t\"Sub-projects register their own `releasePerform` to hook into this stage.\"\n\t// See `:release:releasePerform` which does a lot of heavy lifting here\n}\n\n\nnexusPublishing {\n\trepositories {\n\t\tsonatype()\n\t}\n}\n\n\n// ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n// CI Build Task\n\ntasks.register('ciBuild') {\n\tdescription = \"The task performed when one of the 'main' jobs are triggered on the \" +\n\t\t\t\"CI server.  Just as above, relies on the fact that subprojects will \" +\n\t\t\t\"appropriately define a release task themselves if they have any tasks \" +\n\t\t\t\"which should be performed from these CI jobs\"\n}\n\n\n\n// ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n// Misc...\n\nwrapper {\n\t// To upgrade the version of gradle used in the wrapper, run:\n\t//     ./gradlew wrapper --gradle-version NEW_VERSION\n\n\t// uncomment locally if you need to debug build scripts.\n\t// in such cases, having the sources helps\n\t//distributionType = Wrapper.DistributionType.ALL\n}\n\n\nidea {\n\tmodule {\n\t\tname = \"hibernate-orm\"\n\t}\n}\n"
        },
        {
          "name": "changelog.txt",
          "type": "blob",
          "size": 15.966796875,
          "content": "Hibernate 6 Changelog\n=======================\n\nNote: Please refer to JIRA to learn more about each issue.\n\nChanges in 7.0.0.Beta3 (December 05, 2024)\n------------------------------------------------------------------------------------------------------------------------\n\nhttps://hibernate.atlassian.net/projects/HHH/versions/32364\n\n** Bug\n    * [HHH-18912] - Fix ORM release process\n    * [HHH-18881] - In MySQL, array of dates are not converted correctly\n    * [HHH-18872] - ConcreteProxy type not restored from 2LC when loading a ManyToOne\n    * [HHH-18862] - Group by error due to subselect using foreign key reference instead of primary key in HQL query\n    * [HHH-18859] - slice operator and @ElementCollection\n    * [HHH-18851] - ArrayContainsArgumentTypeResolver wrongly infers array type for needle argument\n    * [HHH-18850] - createCountQuery with Hibernate 6.6.2\n    * [HHH-18848] - JAR for org.hibernate.orm:hibernate-scan-jandex:7.0.0.Beta2 at Maven Central\n    * [HHH-18842] - Regression: CollectionType.replace() breaks if target is PersistentCollection, but not instance of Collection (e.g. PersistentMap)\n    * [HHH-18832] - Bytecode enhancement skipped for entities with \"compute-only\" @Transient properties\n    * [HHH-18830] - extraneous SQL UPDATE statements for unowned collection with @OrderColumn\n    * [HHH-18826] - mappedBy validation in Processor\n    * [HHH-18765] - Error in the booleanarray_to_string auxiliary function \n    * [HHH-18709] - CriteriaUpdate involving JSON field containing Map<String, Object> results in SemanticException\n    * [HHH-18705] - Hibernate processor creates bad TypedReferenceQuery when @Entity have name attribute\n    * [HHH-18692] - Hibernate attempts to close batched statements multiple times \n    * [HHH-18629] - Inconsistent column alias generated while result class is used for placeholder\n    * [HHH-18610] - \"SQLGrammarException: Unable to find column position by name:\" when using Single Table Inheritance with a strict JDBC driver such as PostgreSQL\n    * [HHH-18583] - Joined + discriminator inheritance treat in where clause not restricting to subtype\n    * [HHH-18274] - Problems with generics in queries; proposed partial solution\n    * [HHH-18069] - NullPointerException when unioning partition results\n    * [HHH-17838] - @OneToOne relationship + @Embeddable keys + FetchType.LAZY fail in most recent version\n    * [HHH-16054] - JPA / Hibernate, duplicate pkey error when updating entity that is a subclass of a base class that uses IdClass for composite primary key\n    * [HHH-14119] - IN clause parameter padding not working for criteria query in conjunction with LiteralHandlingMode.BIND\n\n** Improvement\n    * [HHH-18875] - Stop using `Array.newInstance` in `org.hibernate.internal.util.collections.StandardStack`\n    * [HHH-18861] - Improve GitHub release announcement body for automated releases\n    * [HHH-18847] - Organize the org.hibernate.query.results package\n    * [HHH-18844] - Run preVerifyRelease task as part of h2 CI job\n    * [HHH-18841] - Make `_identifierMapper` property added for a IdClass synthetic\n    * [HHH-18840] - detect and report incorrect usage of @OrderColumn, @MapKeyColumn, and @MapKey\n    * [HHH-18683] - The method Metamodel#entity(String) should throw IllegalArgumentException for non-entities\n    * [HHH-18534] - Remove the org.hibernate.boot.models.categorize package\n    * [HHH-17246] - Guard against Sybase being configured for truncating trailing zeros.\n    * [HHH-16160] - XML aggregate support for more databases\n    * [HHH-14020] - Allow Hibernate Types to have access to ServiceRegistry during initialization\n    * [HHH-7913] - Catalog and schema replacement in <subselect> / @Subselect\n\n** New Feature\n    * [HHH-18644] - New and improved hibernate-maven-plugin\n\n** Remove Feature\n    * [HHH-18843] - remove deprecated @OrderBy annotation\n\n** Sub-task\n    * [HHH-18804] - Add XML aggregate support for HANA\n    * [HHH-18803] - Add XML aggregate support for DB2\n    * [HHH-18802] - Add XML aggregate support for SQL Server\n    * [HHH-18801] - Add XML aggregate support for Sybase ASE\n    * [HHH-18800] - Add XML aggregate support for PostgreSQL\n    * [HHH-18799] - Add XML aggregate support for Oracle\n\n** Task\n    * [HHH-18906] - Allow specifying UnsupportedEnhancementStrategy for Hibernate testing\n    * [HHH-18866] - Fix more failing tests on CockroachDB\n    * [HHH-18854] - Changes for Hibernate Reactive 3.0 integration\n    * [HHH-18678] - Use specific tasks for CI builds\n\n\nChanges in 7.0.0.Beta2 (November 13, 2024)\n------------------------------------------------------------------------------------------------------------------------\n\nhttps://hibernate.atlassian.net/projects/HHH/versions/32358\n\n** Bug\n    * [HHH-18816] - Error when rendering the fk-side of an association in an exists subquery\n    * [HHH-18808] - HqlParser.g4 outputs wrong token for `nakedIdentifier` and `identifier` when keyword is used\n    * [HHH-18807] - a bug in HqlLexer.g4\n    * [HHH-18806] - handling of nationalized strings on Sybase / jTDS\n    * [HHH-18773] - Multiple selections of same alias triggers possible non-threadsafe access to the session\n    * [HHH-18770] - NPE when using the JFR integration with JFR disabled\n    * [HHH-18764] - Class cast exception when using non basic type as identifier and in an embedded field using a natural ID\n    * [HHH-18761] - named query method generation for @NamedQuery on entity\n    * [HHH-18739] - Do not support join queries when using Mysql\n    * [HHH-18738] - Schema of database sequence is not configured if xml mapping is used\n    * [HHH-18730] - Multi-column association in aggregate component doesn't work\n    * [HHH-18720] - Type check on select columns in union all gives SemanticException when there is a null column\n    * [HHH-18719] - Previous row state reuse can provide detached entities to the consumer\n    * [HHH-18712] - Warning about attempts to update an immutable entity for normal (not immutable) entity\n    * [HHH-18703] - JoinedSubclassEntityPersister#getTableNameForColumn KO\n    * [HHH-18702] - Exception using @EmbeddedId with @OneToMany that refers to an alternate key column\n    * [HHH-18699] - Correctly handle @Id and @Version fields in query validation in Hibernate Processor\n    * [HHH-18697] - JPA 3.2 spec compliance for uppercasing of names in Hibernate Processor\n    * [HHH-18696] - @Find method for single @NaturalId field\n    * [HHH-18692] - Hibernate attempts to close batched statements multiple times \n    * [HHH-18689] - 'FULL' query cache sometimes incomplete\n    * [HHH-18681] - InterpretationException executing subquery in case-when : o.h.query.sqm.tree.select.SqmSelection.getExpressible() is null\n    * [HHH-18675] - Self-referencing many-to-many relation on generic entity gives NullPointerException in mapping\n    * [HHH-18671] - Fix setting name (spelling)\n    * [HHH-18669] - NullPointerException in the AgroalConnectionProvider\n    * [HHH-18667] - Annotation processor leaks - OOME when used in Eclipse IDE\n    * [HHH-18662] - Attribute not mentioned in orm.xml ends up not being mapped in Hibernate ORM 7\n    * [HHH-18658] - Inner join prevents finding an entity instance referencing an empty map\n    * [HHH-18647] - SemanticException when using createCriteriaInsertValues to insert into foreign key column\n    * [HHH-18645] - AssertionError in AbstractBatchEntitySelectFetchInitializer#registerToBatchFetchQueue\n    * [HHH-18642] - DB2: select from new table with identity column not working when missing read permission\n    * [HHH-18635] - Avoid using `bigdatetime` column type on Sybase jconn when not necessary\n    * [HHH-18632] - Concurrency issue with AbstractEntityPersister#nonLazyPropertyLoadPlansByName\n    * [HHH-18631] - AssertionError when loading an entity after removing another, associated entity\n    * [HHH-18628] - Regression: Unable to determine TableReference\n    * [HHH-18626] - @Id annotation in @Embeddable class results in AssertionFailure\n    * [HHH-18617] - Fetching unowned side of bidirectional OneToOne mappings including tenant identifier triggers EntityFilteredException\n    * [HHH-18608] - NPE in EntityInitializerImpl.resolveInstanceSubInitializers\n    * [HHH-18596] - ValueHandlingMode hack in query pagination\n    * [HHH-18585] - exposure of internal types via Dialect\n    * [HHH-18582] - Mapping array of arrays with @JdbcTypeCode(SqlTypes.ARRAY) causes NPE\n    * [HHH-18581] - Performance degradation from Hibernate 5 to 6 on NativeQuery\n    * [HHH-18575] - IN predicate with numeric/decimal parameter types leads to Binding is multi-valued; illegal call to #getBindValue\n    * [HHH-18571] - Entities and collections with batch size 1 are treated as batchable\n    * [HHH-18570] - Invalid SQL when filter contains identifier named date\n    * [HHH-18565] - Bytecode enhancement, assertion error on reloading *toOne entities\n    * [HHH-18564] - Literal expressions using AttributeConverters stopped working in hibernate 6\n    * [HHH-18561] - Informix primary key constraint syntax error\n    * [HHH-18560] - DB2iDialect executes incompatible query in combination with @AuditJoinTable mapping\n    * [HHH-18558] - Informix UUID type support\n\n\nChanges in 7.0.0.Beta1 (August 01, 2024)\n------------------------------------------------------------------------------------------------------------------------\n\nhttps://hibernate.atlassian.net/projects/HHH/versions/32319\n\n** Bug\n    * [HHH-18314] - dialect for Db2 claims to be NationalizationSupport.EXPLICIT but never generates DDL with NCHAR/NVARCHAR\n\n** Improvement\n    * [HHH-18453] - Fix Java code block highlighting in User Guide\n    * [HHH-18448] - Add cast and notEqualTo methods to JpaExpression and SqmExpression\n    * [HHH-18441] - Create extension to PersistenceConfiguration\n    * [HHH-18440] - Rewrite the Bootstrapping chapter in the User Guide\n    * [HHH-18412] - Upgrade JBoss Logging Tools (processor) to 3.0.1.Final\n    * [HHH-18393] - Upgrade JBoss Logging Tools (processor) to 3.0.0.Final\n    * [HHH-18316] - use utf8mb4 instead of utf8 a.k.a utf8mb3 on MySQL\n    * [HHH-18097] - Replace `java.io.Closeable` with `java.lang.AutoCloseable`\n    * [HHH-18009] - Consolidate JdbcObserver and ConnectionObserver into JdbcEventHandler\n    * [HHH-17720] - Add common JAXB contracts for named queries\n\n** New Feature\n    * [HHH-18304] - Transform hbm.xml key-many-to-one references\n    * [HHH-18281] - Transform <filter-def/> and <filter/>\n    * [HHH-18266] - HbmXmlTransformer hbm inverse\n    * [HHH-18265] - HbmXmlTransformer transform hbm <key column=\"\"/> \n    * [HHH-18264] - HbmXmlTransformer collection classification\n    * [HHH-18060] - HbmXmlTransformer work\n    * [HHH-17979] - Add @PropertyRef\n\n** Remove Feature\n    * [HHH-18452] - Remove deprecated org.hibernate.Interceptor methods\n    * [HHH-18449] - Remove deprecated Integrator#integrate form\n    * [HHH-18444] - Remove deprecate Session#refresh methods\n    * [HHH-18443] - Drop SessionFactoryBuilder#enableJpaListCompliance\n    * [HHH-18442] - Drop DynamicInsert#value and DynamicUpdate#value\n    * [HHH-18437] - Remove deprecations from JdbcSessionContext\n    * [HHH-18428] - Remove Session#delete\n    * [HHH-18199] - Remove @Where and @WhereJoinTable\n    * [HHH-18196] - Remove Session#save / Session#update / Session#saveOrUpdate\n    * [HHH-18195] - Remove @SelectBeforeUpdate\n    * [HHH-18194] - Remove @Proxy\n    * [HHH-18193] - Remove @Polymorphism\n    * [HHH-18191] - Remove @LazyToOne\n    * [HHH-18190] - Remove @LazyCollection\n    * [HHH-18189] - Remove @IndexColumn\n    * [HHH-18188] - Remove GenerationTime and its uses\n    * [HHH-18186] - Remove @GeneratorType\n    * [HHH-18184] - Remove CacheModeType and its uses\n    * [HHH-17697] - Remove deprecated annotations\n\n** Sub-task\n    * [HHH-18197] - Remove @Table\n    * [HHH-18192] - Remove @Loader\n    * [HHH-18187] - Remove @Index\n    * [HHH-18185] - Remove @ForeignKey\n    * [HHH-18075] - Transform property-ref\n    * [HHH-17888] - Remove support for MariaDB versions older than 10.5\n\n** Task\n    * [HHH-18397] - Transform \"foreign\" generators\n    * [HHH-18396] - Transform property-ref pointing to a to-one attribute\n    * [HHH-18394] - Fix transformation of nested subclass mappings\n    * [HHH-18037] - Move DerbyDialect to hibernate-community-dialects\n    * [HHH-18010] - Investigate ConnectionObserver and friends\n    * [HHH-17583] - Cleanup for 7.0\n    * [HHH-17448] - Add newly standard column annotation attributes to Hibernate column annotations\n\n\nChanges in 7.0.0.Alpha3 (June 14, 2024)\n------------------------------------------------------------------------------------------------------------------------\n\nhttps://hibernate.atlassian.net/projects/HHH/versions/32304\n\n** Bug\n    * [HHH-18135] - GenerationTypeStrategy implementations always throw UnsupportedOperationException\n    * [HHH-18081] - XML  <secondary-table/> element is not added to JdkClassDetails\n    * [HHH-11937] - Remove warnings about \"empty composites\" being experimental when feature is stabilized\n    * [HHH-11936] - Stabilize \"empty composites\" feature\n\n** New Feature\n    * [HHH-18231] - SPI for persistence XML parsing\n    * [HHH-18057] - Support for JPA 3.2 column options\n    * [HHH-18056] - Support for JPA 32 table options\n    * [HHH-18055] - Support for JPA 3.2 table comment\n    * [HHH-18054] - Support for JPA 3.2 @CheckConstraint\n    * [HHH-16153] - Support JPA 3.2 `@EnumeratedValue`\n\n** Remove Feature\n    * [HHH-18222] - remove hibernate.create_empty_composites.enabled in Hibernate 7\n    * [HHH-18207] - remove deprecated Dialects\n    * [HHH-18139] - remove IdentifierGeneratorFactory and related code\n\n** Sub-task\n    * [HHH-18095] - Transform hbm.xml column read/write fragments\n    * [HHH-18072] - Transform hbm.xml not-found\n\n** Task\n    * [HHH-18127] - Leverage hibernate-models Annotation-as-Class\n    * [HHH-18096] - Support for JPA 3.2 database generator options\n\n\nChanges in 7.0.0.Alpha2 (May 03, 2024)\n------------------------------------------------------------------------------------------------------------------------\n\nhttps://hibernate.atlassian.net/projects/HHH/versions/32280\n\n** Bug\n    * [HHH-18053] - duration arithmetic with fractional seconds\n    * [HHH-18049] - Handle <exclude-default-listeners/> and <exclude-superclass-listeners/>\n    * [HHH-18042] - ConstructorResults defined in XML are not applied\n    * [HHH-18041] - With SharedCacheMode.DISABLE_SELECTIVE entities with cacheable false should not be cached\n    * [HHH-18039] - EntityListeners defined in XML should replace those from annotations, not add to\n    * [HHH-18038] - Fall back to persistence-unit name as SessionFactory name\n    * [HHH-18036] - Retrieving java.sql.Date from Oracle contains unwanted milliseconds\n    * [HHH-18028] - TCK test failure with attribute converter and Embeddable\n    * [HHH-18018] - Derby implementation for 'right' function wrongly passes parameter to 'length'\n\n** Improvement\n    * [HHH-18048] - Split notions of SessionFactory name and SessionFactory JNDI name\n    * [HHH-18005] - Remove AnnotationDescriptor#createUsage method calls that rely on lambdas for configuration\n    * [HHH-18003] - Create a PersistenceUnitDescriptor wrapper around JPA 3.2 PersistenceConfiguration\n    * [HHH-18000] - Remove XmlProcessingHelper methods for creating AnnotationUsage instances\n\n** New Feature\n    * [HHH-18025] - RefreshOptions & LockOptions for Hibernate 7\n    * [HHH-18001] - FindOptions for Hibernate 7\n\n** Task\n    * [HHH-18043] - Change SQL Server default timestamp precision to 7\n    * [HHH-18035] - Change Oracle default timestamp precision to 9\n    * [HHH-17982] - Setup JPA 3.2 TCK testing automation for ORM 7\n\n\nChanges in 7.0.0.Alpha1 (April 16, 2024)\n------------------------------------------------------------------------------------------------------------------------\n\nhttps://hibernate.atlassian.net/projects/HHH/versions/32214\n\n** Deprecation\n    * [HHH-17441] - Deprecate @Comment\n\n** New Feature\n    * [HHH-17460] - Ongoing JPA 3.2 work\n    * [HHH-17459] - Allow resolution callbacks on select o.h.mapping objects\n\n** Remove Feature\n    * [HHH-17961] - Drop support for hibernate.mapping.precedence\n    * [HHH-17894] - Remove AdditionalJaxbMappingProducer\n    * [HHH-17893] - Remove MetadataContributor\n    * [HHH-17892] - Remove @Persister\n\n** Task\n    * [HHH-17444] - Ongoing JPA 32 work\n"
        },
        {
          "name": "checkerstubs",
          "type": "tree",
          "content": null
        },
        {
          "name": "ci",
          "type": "tree",
          "content": null
        },
        {
          "name": "databases",
          "type": "tree",
          "content": null
        },
        {
          "name": "dco.txt",
          "type": "blob",
          "size": 1.38671875,
          "content": "Developer Certificate of Origin\nVersion 1.1\n\nCopyright (C) 2004, 2006 The Linux Foundation and its contributors.\n1 Letterman Drive\nSuite D4700\nSan Francisco, CA, 94129\n\nEveryone is permitted to copy and distribute verbatim copies of this\nlicense document, but changing it is not allowed.\n\n\nDeveloper's Certificate of Origin 1.1\n\nBy making a contribution to this project, I certify that:\n\n(a) The contribution was created in whole or in part by me and I\n    have the right to submit it under the open source license\n    indicated in the file; or\n\n(b) The contribution is based upon previous work that, to the best\n    of my knowledge, is covered under an appropriate open source\n    license and I have the right under that license to submit that\n    work with modifications, whether created in whole or in part\n    by me, under the same open source license (unless I am\n    permitted to submit under a different license), as indicated\n    in the file; or\n\n(c) The contribution was provided directly to me by some other\n    person who certified (a), (b) or (c) and I have not modified\n    it.\n\n(d) I understand and agree that this project and the contribution\n    are public and that a record of the contribution (including all\n    personal information I submit with it, including my sign-off) is\n    maintained indefinitely and may be redistributed consistent with\n    this project or the open source license(s) involved."
        },
        {
          "name": "design",
          "type": "tree",
          "content": null
        },
        {
          "name": "dialects.adoc",
          "type": "blob",
          "size": 3.8193359375,
          "content": "= Dialects\n\nA dialect is a class that provides information about the specifics of a database and translators for the SQL dialect of the database.\n\n== Supported dialects\n\nHibernate supports a wide range of dialects out of the box. The following is list of officially supported databases:\n\n* Apache Derby\n* Cockroach\n* Google Spanner\n* H2\n* HSQLDB\n* IBM DB2 LUW\n* IBM DB2 iSeries\n* IBM DB2 z/OS\n* MariaDB\n* MySQL\n* Oracle\n* PostgreSQL\n* Postgres Plus\n* SAP HANA\n* SQL Server\n* Sybase ASE\n\nUsually, Hibernate supports at least the database version that is also still supported by the respective vendor.\nIn many cases though, Hibernate supports even older versions of the databases,\nbut the support for these versions is not guaranteed.\n\nApart from the Hibernate team supported dialects, there are also community dialects.\n\n== Community dialects\n\nAs of Hibernate 6.0, the Hibernate team decided to provide a clear way forward for community contributed dialects.\nThe `hibernate-core` artifact had many legacy dialects before 6.0 that were only tested and maintained on a best effort basis.\n\nMore and more database vendors requested to integrate a dialect for their database and even provided a PR with a dialect,\nbut the Hibernate team didn't want to add new dialects for databases that might not have a wide adoption\nor any automated testing into the `hibernate-core` artifact. Even though the dialect was supposedly maintained by the vendor,\nthe Hibernate team was burdened with reviewing questions, issues and PRs that relate to these dialects.\n\nTo give database vendors and the community a clear way forward, the Hibernate team decided to introduce a new artifact,\ncalled `hibernate-community-dialects` which is the new home for dialects that are maintained by vendors or individuals.\nStarting with Hibernate 6.0 the `hibernate-core` artifact will only contain dialects that are supported and tested by the Hibernate team.\nAll the legacy dialects are moved to the `hibernate-community-dialects` artifact to have a clear separation based on the quality of the dialect.\n\nIssues with dialects in the `hibernate-community-dialects` are usually not considered by the Hibernate team,\nas the community is responsible for providing fixes and improving the dialects for newer database versions or ORM capabilities.\n\n== Requirements for moving to hibernate-core\n\nIf a database vendor wants their database dialect to be included in the `hibernate-core` artifact,\nseveral requirements have to be fulfilled:\n\n* The vendor must provide access to a dedicated database server that can be used for testing\n* The vendor must provide contact details to at least one employee who is mainly responsible for the maintenance of the dialect\n* The responsible employee of the vendor must actively monitor and react to failures of the testsuite against the respective database\n* The responsible employee of the vendor must ensure the testsuite is configured correctly in order for it to succeed on the respective database\n* If the responsible employee of the vendor leaves the company, the vendor must provide contact details to a new responsible employee\n\nIn case the responsible employee is unreachable for a longer period or issues with the dialect are not attended to in a timely manner,\nthe Hibernate team will move the dialect back to the `hibernate-community-dialects` artifact.\n\nThe requirements for the database server are:\n\n* JDK 8 installed through e.g. `sudo yum install -y java-1.8.0-openjdk-devel`\n* JDK 11 installed through e.g. `sudo yum install -y java-11-openjdk-devel`\n* Git installed through e.g. `sudo yum install -y git`\n* Access to the database through non-confidential credentials\n* Access via SSH through confidential credentials\n\nGet in touch with the Hibernate team on https://hibernate.zulipchat.com/#narrow/stream/132096-hibernate-user[Zulip]\nif you want to request the move of your dialect to hibernate-core."
        },
        {
          "name": "docker_db.sh",
          "type": "blob",
          "size": 45.064453125,
          "content": "#! /bin/bash\n\nif command -v docker > /dev/null; then\n  CONTAINER_CLI=$(command -v docker)\n  HEALTCHECK_PATH=\"{{.State.Health.Status}}\"\n  PRIVILEGED_CLI=\"\"\nelse\n  CONTAINER_CLI=$(command -v podman)\n  HEALTCHECK_PATH=\"{{.State.Healthcheck.Status}}\"\n  # Only use sudo for podman\n  if command -v sudo > /dev/null; then\n    PRIVILEGED_CLI=\"sudo\"\n  else\n    PRIVILEGED_CLI=\"\"\n  fi\nfi\n\nmysql() {\n  mysql_8_2\n}\n\nmysql_8_0() {\n    $CONTAINER_CLI rm -f mysql || true\n    $CONTAINER_CLI run --name mysql -e MYSQL_USER=hibernate_orm_test -e MYSQL_PASSWORD=hibernate_orm_test -e MYSQL_ROOT_PASSWORD=hibernate_orm_test -e MYSQL_DATABASE=hibernate_orm_test -e MYSQL_ROOT_PASSWORD=hibernate_orm_test -p3306:3306 -d ${DB_IMAGE_MYSQL_8_0:-docker.io/mysql:8.0.31} --character-set-server=utf8mb4 --collation-server=utf8mb4_0900_as_cs --skip-character-set-client-handshake --log-bin-trust-function-creators=1 --lower_case_table_names=2\n    # Give the container some time to start\n    OUTPUT=\n    n=0\n    until [ \"$n\" -ge 5 ]\n    do\n        # Need to access STDERR. Thanks for the snippet https://stackoverflow.com/a/56577569/412446\n        { OUTPUT=\"$( { $CONTAINER_CLI logs mysql; } 2>&1 1>&3 3>&- )\"; } 3>&1;\n        if [[ $OUTPUT == *\"ready for connections\"* ]]; then\n          break;\n        fi\n        n=$((n+1))\n        echo \"Waiting for MySQL to start...\"\n        sleep 3\n    done\n    if [ \"$n\" -ge 5 ]; then\n      echo \"MySQL failed to start and configure after 15 seconds\"\n    else\n      echo \"MySQL successfully started\"\n    fi\n}\n\nmysql_8_1() {\n    $CONTAINER_CLI rm -f mysql || true\n    $CONTAINER_CLI run --name mysql -e MYSQL_USER=hibernate_orm_test -e MYSQL_PASSWORD=hibernate_orm_test -e MYSQL_ROOT_PASSWORD=hibernate_orm_test -e MYSQL_DATABASE=hibernate_orm_test -e MYSQL_ROOT_PASSWORD=hibernate_orm_test -p3306:3306 -d ${DB_IMAGE_MYSQL_8_1:-docker.io/mysql:8.1.0} --character-set-server=utf8mb4 --collation-server=utf8mb4_0900_as_cs --skip-character-set-client-handshake --log-bin-trust-function-creators=1 --lower_case_table_names=2\n    # Give the container some time to start\n    OUTPUT=\n    n=0\n    until [ \"$n\" -ge 5 ]\n    do\n        # Need to access STDERR. Thanks for the snippet https://stackoverflow.com/a/56577569/412446\n        { OUTPUT=\"$( { $CONTAINER_CLI logs mysql; } 2>&1 1>&3 3>&- )\"; } 3>&1;\n        if [[ $OUTPUT == *\"ready for connections\"* ]]; then\n          break;\n        fi\n        n=$((n+1))\n        echo \"Waiting for MySQL to start...\"\n        sleep 3\n    done\n    if [ \"$n\" -ge 5 ]; then\n      echo \"MySQL failed to start and configure after 15 seconds\"\n    else\n      echo \"MySQL successfully started\"\n    fi\n}\n\nmysql_8_2() {\n    $CONTAINER_CLI rm -f mysql || true\n    $CONTAINER_CLI run --name mysql -e MYSQL_USER=hibernate_orm_test -e MYSQL_PASSWORD=hibernate_orm_test -e MYSQL_ROOT_PASSWORD=hibernate_orm_test -e MYSQL_DATABASE=hibernate_orm_test -e MYSQL_ROOT_PASSWORD=hibernate_orm_test -p3306:3306 -d ${DB_IMAGE_MYSQL_8_2:-docker.io/mysql:8.2.0} --character-set-server=utf8mb4 --collation-server=utf8mb4_0900_as_cs --skip-character-set-client-handshake --log-bin-trust-function-creators=1 --lower_case_table_names=2\n    # Give the container some time to start\n    OUTPUT=\n    n=0\n    until [ \"$n\" -ge 5 ]\n    do\n        # Need to access STDERR. Thanks for the snippet https://stackoverflow.com/a/56577569/412446\n        { OUTPUT=\"$( { $CONTAINER_CLI logs mysql; } 2>&1 1>&3 3>&- )\"; } 3>&1;\n        if [[ $OUTPUT == *\"ready for connections\"* ]]; then\n          break;\n        fi\n        n=$((n+1))\n        echo \"Waiting for MySQL to start...\"\n        sleep 3\n    done\n    if [ \"$n\" -ge 5 ]; then\n      echo \"MySQL failed to start and configure after 15 seconds\"\n    else\n      echo \"MySQL successfully started\"\n    fi\n}\n\nmariadb() {\n  mariadb_11_7\n}\n\nmariadb_wait_until_start()\n{\n    n=0\n    until [ \"$n\" -ge 5 ]\n    do\n        if $CONTAINER_CLI exec mariadb healthcheck.sh --connect --innodb_initialized; then\n          break;\n        fi\n        n=$((n+1))\n        echo \"Waiting for MariaDB to start...\"\n        sleep 3\n    done\n    if $CONTAINER_CLI exec mariadb healthcheck.sh --connect --innodb_initialized; then\n      echo \"MariaDB successfully started\"\n    else\n      echo \"MariaDB failed to start and configure after 15 seconds\"\n    fi\n}\n\nmariadb_10_5() {\n    $CONTAINER_CLI rm -f mariadb || true\n    $CONTAINER_CLI run --name mariadb -e MARIADB_USER=hibernate_orm_test -e MARIADB_PASSWORD=hibernate_orm_test -e MARIADB_DATABASE=hibernate_orm_test -e MARIADB_ROOT_PASSWORD=hibernate_orm_test -p3306:3306 -d ${DB_IMAGE_MARIADB_10_5:-docker.io/mariadb:10.5.25} --character-set-server=utf8mb4 --collation-server=utf8mb4_bin --skip-character-set-client-handshake --lower_case_table_names=2\n    mariadb_wait_until_start\n}\n\nmariadb_10_11() {\n    $CONTAINER_CLI rm -f mariadb || true\n    $CONTAINER_CLI run --name mariadb -e MARIADB_USER=hibernate_orm_test -e MARIADB_PASSWORD=hibernate_orm_test -e MARIADB_DATABASE=hibernate_orm_test -e MARIADB_ROOT_PASSWORD=hibernate_orm_test -p3306:3306 -d ${DB_IMAGE_MARIADB_10_11:-docker.io/mariadb:10.11.8} --character-set-server=utf8mb4 --collation-server=utf8mb4_bin --skip-character-set-client-handshake --lower_case_table_names=2\n    mariadb_wait_until_start\n}\n\nmariadb_11_1() {\n    $CONTAINER_CLI rm -f mariadb || true\n    $CONTAINER_CLI run --name mariadb -e MARIADB_USER=hibernate_orm_test -e MARIADB_PASSWORD=hibernate_orm_test -e MARIADB_DATABASE=hibernate_orm_test -e MARIADB_ROOT_PASSWORD=hibernate_orm_test -p3306:3306 -d ${DB_IMAGE_MARIADB_11_1:-docker.io/mariadb:11.1.2} --character-set-server=utf8mb4 --collation-server=utf8mb4_bin --skip-character-set-client-handshake --lower_case_table_names=2\n    mariadb_wait_until_start\n}\n\nmariadb_11_4() {\n    $CONTAINER_CLI rm -f mariadb || true\n    $CONTAINER_CLI run --name mariadb -e MARIADB_USER=hibernate_orm_test -e MARIADB_PASSWORD=hibernate_orm_test -e MARIADB_DATABASE=hibernate_orm_test -e MARIADB_ROOT_PASSWORD=hibernate_orm_test -p3306:3306 -d ${DB_IMAGE_MARIADB_11_4:-docker.io/mariadb:11.4.2} --character-set-server=utf8mb4 --collation-server=utf8mb4_bin --skip-character-set-client-handshake --lower_case_table_names=2\n    mariadb_wait_until_start\n}\n\nmariadb_11_7() {\n    $CONTAINER_CLI rm -f mariadb || true\n    $CONTAINER_CLI run --name mariadb -e MARIADB_USER=hibernate_orm_test -e MARIADB_PASSWORD=hibernate_orm_test -e MARIADB_DATABASE=hibernate_orm_test -e MARIADB_ROOT_PASSWORD=hibernate_orm_test -p3306:3306 -d ${DB_IMAGE_MARIADB_11_7:-docker.io/mariadb:11.7-rc} --character-set-server=utf8mb4 --collation-server=utf8mb4_bin --skip-character-set-client-handshake --lower_case_table_names=2\n    mariadb_wait_until_start\n}\n\nmariadb_verylatest() {\n    $CONTAINER_CLI rm -f mariadb || true\n    $CONTAINER_CLI run --name mariadb -e MARIADB_USER=hibernate_orm_test -e MARIADB_PASSWORD=hibernate_orm_test -e MARIADB_DATABASE=hibernate_orm_test -e MARIADB_ROOT_PASSWORD=hibernate_orm_test -p3306:3306 -d ${DB_IMAGE_MARIADB_VERYLATEST:-quay.io/mariadb-foundation/mariadb-devel:verylatest} --character-set-server=utf8mb4 --collation-server=utf8mb4_bin --skip-character-set-client-handshake --lower_case_table_names=2\n    mariadb_wait_until_start\n}\n\npostgresql() {\n  postgresql_16\n}\n\npostgresql_12() {\n    $CONTAINER_CLI rm -f postgres || true\n    $CONTAINER_CLI run --name postgres -e POSTGRES_USER=hibernate_orm_test -e POSTGRES_PASSWORD=hibernate_orm_test -e POSTGRES_DB=hibernate_orm_test -p5432:5432 -d ${DB_IMAGE_POSTGRESQL_12:-docker.io/postgis/postgis:12-3.4}\n    $CONTAINER_CLI exec postgres bash -c '/usr/share/postgresql-common/pgdg/apt.postgresql.org.sh -y && apt install -y postgresql-12-pgvector && psql -U hibernate_orm_test -d hibernate_orm_test -c \"create extension vector;\"'\n}\n\npostgresql_13() {\n    $CONTAINER_CLI rm -f postgres || true\n    $CONTAINER_CLI run --name postgres -e POSTGRES_USER=hibernate_orm_test -e POSTGRES_PASSWORD=hibernate_orm_test -e POSTGRES_DB=hibernate_orm_test -p5432:5432 -d ${DB_IMAGE_POSTGRESQL_13:-docker.io/postgis/postgis:13-3.1}\n    $CONTAINER_CLI exec postgres bash -c '/usr/share/postgresql-common/pgdg/apt.postgresql.org.sh -y && apt install -y postgresql-13-pgvector && psql -U hibernate_orm_test -d hibernate_orm_test -c \"create extension vector;\"'\n}\n\npostgresql_14() {\n    $CONTAINER_CLI rm -f postgres || true\n    $CONTAINER_CLI run --name postgres -e POSTGRES_USER=hibernate_orm_test -e POSTGRES_PASSWORD=hibernate_orm_test -e POSTGRES_DB=hibernate_orm_test -p5432:5432 -d ${DB_IMAGE_POSTGRESQL_14:-docker.io/postgis/postgis:14-3.3}\n    $CONTAINER_CLI exec postgres bash -c '/usr/share/postgresql-common/pgdg/apt.postgresql.org.sh -y && apt install -y postgresql-14-pgvector && psql -U hibernate_orm_test -d hibernate_orm_test -c \"create extension vector;\"'\n}\n\npostgresql_15() {\n    $CONTAINER_CLI rm -f postgres || true\n    $CONTAINER_CLI run --name postgres -e POSTGRES_USER=hibernate_orm_test -e POSTGRES_PASSWORD=hibernate_orm_test -e POSTGRES_DB=hibernate_orm_test -p5432:5432 --tmpfs /pgtmpfs:size=131072k -d ${DB_IMAGE_POSTGRESQL_15:-docker.io/postgis/postgis:15-3.3} \\\n      -c fsync=off -c synchronous_commit=off -c full_page_writes=off -c shared_buffers=256MB -c maintenance_work_mem=256MB -c max_wal_size=1GB -c checkpoint_timeout=1d\n    $CONTAINER_CLI exec postgres bash -c '/usr/share/postgresql-common/pgdg/apt.postgresql.org.sh -y && apt install -y postgresql-15-pgvector && psql -U hibernate_orm_test -d hibernate_orm_test -c \"create extension vector;\"'\n}\n\npostgresql_16() {\n    $CONTAINER_CLI rm -f postgres || true\n    $CONTAINER_CLI run --name postgres -e POSTGRES_USER=hibernate_orm_test -e POSTGRES_PASSWORD=hibernate_orm_test -e POSTGRES_DB=hibernate_orm_test -p5432:5432 --tmpfs /pgtmpfs:size=131072k -d ${DB_IMAGE_POSTGRESQL_16:-docker.io/postgis/postgis:16-3.4} \\\n      -c fsync=off -c synchronous_commit=off -c full_page_writes=off -c shared_buffers=256MB -c maintenance_work_mem=256MB -c max_wal_size=1GB -c checkpoint_timeout=1d\n    $CONTAINER_CLI exec postgres bash -c '/usr/share/postgresql-common/pgdg/apt.postgresql.org.sh -y && apt install -y postgresql-16-pgvector && psql -U hibernate_orm_test -d hibernate_orm_test -c \"create extension vector;\"'\n}\n\nedb() {\n    edb_16\n}\n\nedb_12() {\n    $CONTAINER_CLI rm -f edb || true\n    # We need to build a derived image because the existing image is mainly made for use by a kubernetes operator\n    (cd edb; $CONTAINER_CLI build -t edb-test:12 -f edb12.Dockerfile .)\n    $CONTAINER_CLI run --name edb -e POSTGRES_USER=hibernate_orm_test -e POSTGRES_PASSWORD=hibernate_orm_test -e POSTGRES_DB=hibernate_orm_test -p 5444:5444 -d edb-test:12\n}\n\nedb_14() {\n    $CONTAINER_CLI rm -f edb || true\n    # We need to build a derived image because the existing image is mainly made for use by a kubernetes operator\n    (cd edb; $CONTAINER_CLI build -t edb-test:14 -f edb14.Dockerfile .)\n    $CONTAINER_CLI run --name edb -e POSTGRES_USER=hibernate_orm_test -e POSTGRES_PASSWORD=hibernate_orm_test -e POSTGRES_DB=hibernate_orm_test -p 5444:5444 -d edb-test:14\n}\n\nedb_15() {\n    $CONTAINER_CLI rm -f edb || true\n    # We need to build a derived image because the existing image is mainly made for use by a kubernetes operator\n    (cd edb; $CONTAINER_CLI build -t edb-test:15 -f edb15.Dockerfile .)\n    $CONTAINER_CLI run --name edb -e POSTGRES_USER=hibernate_orm_test -e POSTGRES_PASSWORD=hibernate_orm_test -e POSTGRES_DB=hibernate_orm_test -p 5444:5444 -d edb-test:15\n}\n\nedb_16() {\n    $CONTAINER_CLI rm -f edb || true\n    # We need to build a derived image because the existing image is mainly made for use by a kubernetes operator\n    (cd edb; $CONTAINER_CLI build -t edb-test:16 -f edb16.Dockerfile .)\n    $CONTAINER_CLI run --name edb -e POSTGRES_USER=hibernate_orm_test -e POSTGRES_PASSWORD=hibernate_orm_test -e POSTGRES_DB=hibernate_orm_test -p 5444:5444 -d edb-test:16\n}\n\ndb2() {\n  db2_11_5\n}\n\ndb2_11_5() {\n    $PRIVILEGED_CLI $CONTAINER_CLI rm -f db2 || true\n    $PRIVILEGED_CLI $CONTAINER_CLI run --name db2 --privileged -e DB2INSTANCE=orm_test -e DB2INST1_PASSWORD=orm_test -e DBNAME=orm_test -e LICENSE=accept -e AUTOCONFIG=false -e ARCHIVE_LOGS=false -e TO_CREATE_SAMPLEDB=false -e REPODB=false -p 50000:50000 -d ${DB_IMAGE_DB2_11_5:-icr.io/db2_community/db2:11.5.9.0}\n    # Give the container some time to start\n    OUTPUT=\n    while [[ $OUTPUT != *\"INSTANCE\"* ]]; do\n        echo \"Waiting for DB2 to start...\"\n        sleep 10\n        OUTPUT=$($PRIVILEGED_CLI $CONTAINER_CLI logs db2 2>&1)\n    done\n    $PRIVILEGED_CLI $CONTAINER_CLI exec -t db2 su - orm_test bash -c \". /database/config/orm_test/sqllib/db2profile; /database/config/orm_test/sqllib/bin/db2 'connect to orm_test'; /database/config/orm_test/sqllib/bin/db2 'CREATE USER TEMPORARY TABLESPACE usr_tbsp MANAGED BY AUTOMATIC STORAGE'\"\n}\n\ndb2_10_5() {\n    $PRIVILEGED_CLI $CONTAINER_CLI rm -f db2 || true\n    # The sha represents the tag 10.5.0.5-3.10.0\n    $PRIVILEGED_CLI $CONTAINER_CLI run --name db2 --privileged -e DB2INST1_PASSWORD=db2inst1-pwd -e LICENSE=accept -p 50000:50000 -d ${DB_IMAGE_DB2_10_5:-quay.io/hibernate/db2express-c@sha256:a499afd9709a1f69fb41703e88def9869955234c3525547e2efc3418d1f4ca2b} db2start\n    # Give the container some time to start\n    OUTPUT=\n    while [[ $OUTPUT != *\"DB2START\"* ]]; do\n        echo \"Waiting for DB2 to start...\"\n        sleep 10\n        OUTPUT=$($PRIVILEGED_CLI $CONTAINER_CLI logs db2 2>&1)\n    done\n    $PRIVILEGED_CLI $CONTAINER_CLI exec -t db2 su - db2inst1 bash -c \"/home/db2inst1/sqllib/bin/db2 create database orm_test &&\n    /home/db2inst1/sqllib/bin/db2 'connect to orm_test' &&\n    /home/db2inst1/sqllib/bin/db2 'CREATE BUFFERPOOL BP8K pagesize 8K' &&\n    /home/db2inst1/sqllib/bin/db2 'CREATE SYSTEM TEMPORARY TABLESPACE STB_8 PAGESIZE 8K BUFFERPOOL BP8K' &&\n    /home/db2inst1/sqllib/bin/db2 'CREATE BUFFERPOOL BP16K pagesize 16K' &&\n    /home/db2inst1/sqllib/bin/db2 'CREATE SYSTEM TEMPORARY TABLESPACE STB_16 PAGESIZE 16K BUFFERPOOL BP16K' &&\n    /home/db2inst1/sqllib/bin/db2 'CREATE BUFFERPOOL BP32K pagesize 32K' &&\n    /home/db2inst1/sqllib/bin/db2 'CREATE SYSTEM TEMPORARY TABLESPACE STB_32 PAGESIZE 32K BUFFERPOOL BP32K' &&\n    /home/db2inst1/sqllib/bin/db2 'CREATE USER TEMPORARY TABLESPACE usr_tbsp MANAGED BY AUTOMATIC STORAGE'\"\n}\n\ndb2_spatial() {\n    $PRIVILEGED_CLI $CONTAINER_CLI rm -f db2spatial || true\n    temp_dir=$(mktemp -d)\n    cat <<EOF >${temp_dir}/ewkt.sql\ncreate or replace function db2gse.asewkt(geometry db2gse.st_geometry)\nreturns clob(2G)\nspecific db2gse.asewkt1\nlanguage sql\ndeterministic\nno external action\nreads sql data\nreturn 'srid=' || varchar(db2gse.st_srsid(geometry)) || ';' || db2gse.st_astext(geometry)\n;\n\n-- Create SQL function to create a geometry from EWKT format\ncreate or replace function db2gse.geomfromewkt(instring varchar(32000))\nreturns db2gse.st_geometry\nspecific db2gse.fromewkt1\nlanguage sql\ndeterministic\nno external action\nreads sql data\nreturn db2gse.st_geometry(\nsubstr(instring,posstr(instring,';')+1, length(instring) - posstr(instring,';')),\ninteger(substr(instring,posstr(instring,'=')+1,posstr(instring,';')-(posstr(instring,'=')+1)))\n)\n;\n-- Create a DB2 transform group to return and accept EWKT\nCREATE TRANSFORM FOR db2gse.ST_Geometry EWKT (\n       FROM SQL WITH FUNCTION db2gse.asewkt(db2gse.ST_Geometry),\n       TO   SQL WITH FUNCTION db2gse.geomfromewkt(varchar(32000)) )\n\t;\n\n-- Redefine the default DB2_PROGRAM to return and accept EWKT instead of WKT\nDROP TRANSFORM DB2_PROGRAM FOR db2gse.ST_Geometry;\nCREATE TRANSFORM FOR db2gse.ST_Geometry DB2_PROGRAM (\n       FROM SQL WITH FUNCTION db2gse.asewkt(db2gse.ST_Geometry),\n       TO   SQL WITH FUNCTION db2gse.geomfromewkt(varchar(32000)) )\n;\nEOF\n    $PRIVILEGED_CLI $CONTAINER_CLI run --name db2spatial --privileged -e DB2INSTANCE=orm_test -e DB2INST1_PASSWORD=orm_test -e DBNAME=orm_test -e LICENSE=accept -e AUTOCONFIG=false -e ARCHIVE_LOGS=false -e TO_CREATE_SAMPLEDB=false -e REPODB=false \\\n        -v ${temp_dir}:/conf  \\\n        -p 50000:50000 -d ${DB_IMAGE_DB2_SPATIAL:-docker.io/ibmcom/db2:11.5.5.0}\n\n    # Give the container some time to start\n    OUTPUT=\n    while [[ $OUTPUT != *\"Setup has completed.\"* ]]; do\n        echo \"Waiting for DB2 to start...\"\n        sleep 10\n        OUTPUT=$($PRIVILEGED_CLI $CONTAINER_CLI logs db2spatial 2>&1)\n    done\n    sleep 10\n    echo \"Enabling spatial extender\"\n    $PRIVILEGED_CLI $CONTAINER_CLI exec -t db2spatial su - orm_test bash -c \"/database/config/orm_test/sqllib/db2profile && /database/config/orm_test/sqllib/bin/db2se enable_db orm_test\"\n    echo \"Installing required transform group\"\n    $PRIVILEGED_CLI $CONTAINER_CLI exec -t db2spatial su - orm_test bash -c \"/database/config/orm_test/sqllib/db2profile && /database/config/orm_test/sqllib/bin/db2 'connect to orm_test' && /database/config/orm_test/sqllib/bin/db2 -tvf /conf/ewkt.sql\"\n\n}\n\nmssql() {\n  mssql_2022\n}\n\nmssql_2017() {\n    $CONTAINER_CLI rm -f mssql || true\n    #This sha256 matches a specific tag of mcr.microsoft.com/mssql/server:2017-latest :\n    $CONTAINER_CLI run --name mssql -d -p 1433:1433 -e \"SA_PASSWORD=Hibernate_orm_test\" -e ACCEPT_EULA=Y ${DB_IMAGE_MSSQL_2017:-mcr.microsoft.com/mssql/server@sha256:7d194c54e34cb63bca083542369485c8f4141596805611e84d8c8bab2339eede}\n    sleep 5\n    n=0\n    until [ \"$n\" -ge 5 ]\n    do\n        # We need a database that uses a non-lock based MVCC approach\n        # https://github.com/microsoft/homebrew-mssql-release/issues/2#issuecomment-682285561\n        $CONTAINER_CLI exec mssql bash -c 'echo \"create database hibernate_orm_test collate SQL_Latin1_General_CP1_CS_AS; alter database hibernate_orm_test set READ_COMMITTED_SNAPSHOT ON\" | /opt/mssql-tools/bin/sqlcmd -S localhost -U sa -P Hibernate_orm_test -i /dev/stdin' && break\n        echo \"Waiting for SQL Server to start...\"\n        n=$((n+1))\n        sleep 5\n    done\n    if [ \"$n\" -ge 5 ]; then\n      echo \"SQL Server failed to start and configure after 25 seconds\"\n    else\n      echo \"SQL Server successfully started\"\n    fi\n}\n\nmssql_2022() {\n    $CONTAINER_CLI rm -f mssql || true\n    #This sha256 matches a specific tag of 2022-CU12-ubuntu-22.04 (https://mcr.microsoft.com/en-us/product/mssql/server/tags):\n    $CONTAINER_CLI run --name mssql -d -p 1433:1433 -e \"SA_PASSWORD=Hibernate_orm_test\" -e ACCEPT_EULA=Y ${DB_IMAGE_MSSQL_2022:-mcr.microsoft.com/mssql/server@sha256:b94071acd4612bfe60a73e265097c2b6388d14d9d493db8f37cf4479a4337480}\n    sleep 5\n    n=0\n    until [ \"$n\" -ge 5 ]\n    do\n        # We need a database that uses a non-lock based MVCC approach\n        # https://github.com/microsoft/homebrew-mssql-release/issues/2#issuecomment-682285561\n        $CONTAINER_CLI exec mssql bash -c 'echo \"create database hibernate_orm_test collate SQL_Latin1_General_CP1_CS_AS; alter database hibernate_orm_test set READ_COMMITTED_SNAPSHOT ON\" | /opt/mssql-tools/bin/sqlcmd -S localhost -U sa -P Hibernate_orm_test -i /dev/stdin' && break\n        echo \"Waiting for SQL Server to start...\"\n        n=$((n+1))\n        sleep 5\n    done\n    if [ \"$n\" -ge 5 ]; then\n      echo \"SQL Server failed to start and configure after 25 seconds\"\n    else\n      echo \"SQL Server successfully started\"\n    fi\n}\n\nsybase() {\n    $CONTAINER_CLI rm -f sybase || true\n    # Yup, that sucks, but on ubuntu we need to use -T11889 as per: https://github.com/DataGrip/docker-env/issues/12\n    $CONTAINER_CLI run -d -p 9000:5000 -p 9001:5001 --name sybase --entrypoint /bin/bash ${DB_IMAGE_SYBASE:-docker.io/nguoianphu/docker-sybase} -c \"source /opt/sybase/SYBASE.sh\n/opt/sybase/ASE-16_0/bin/dataserver \\\n-d/opt/sybase/data/master.dat \\\n-e/opt/sybase/ASE-16_0/install/MYSYBASE.log \\\n-c/opt/sybase/ASE-16_0/MYSYBASE.cfg \\\n-M/opt/sybase/ASE-16_0 \\\n-N/opt/sybase/ASE-16_0/sysam/MYSYBASE.properties \\\n-i/opt/sybase \\\n-sMYSYBASE \\\n-T11889\nRET=\\$?\nexit 0\n\"\n\n    sybase_check() {\n    $CONTAINER_CLI exec sybase bash -c \"source /opt/sybase/SYBASE.sh;\n/opt/sybase/OCS-16_0/bin/isql -Usa -P myPassword -S MYSYBASE <<EOF\nSelect name from sysdatabases where status2 & 48 > 0\ngo\nquit\nEOF\n\"\n}\n    START_STATUS=0\n    j=1\n    while (( $j < 30 )); do\n      echo \"Waiting for Sybase to start...\"\n      sleep 1\n      j=$((j+1))\n      START_STATUS=$(sybase_check | grep '(0 rows affected)' | wc -c)\n      if (( $START_STATUS > 0 )); then\n        break\n      fi\n    done\n    if (( $j == 30 )); then\n      echo \"Failed starting Sybase\"\n      $CONTAINER_CLI ps -a\n      $CONTAINER_CLI logs sybase\n      sybase_check\n      exit 1\n    fi\n\n    export SYBASE_DB=hibernate_orm_test\n    export SYBASE_USER=hibernate_orm_test\n    export SYBASE_PASSWORD=hibernate_orm_test\n    $CONTAINER_CLI exec sybase bash -c \"source /opt/sybase/SYBASE.sh;\ncat <<-EOSQL > init1.sql\nuse master\ngo\ndisk resize name='master', size='256m'\ngo\ncreate database $SYBASE_DB on master = '96m'\ngo\nsp_dboption $SYBASE_DB, \\\"single user\\\", true\ngo\nalter database $SYBASE_DB log on master = '50m'\ngo\nuse $SYBASE_DB\ngo\nexec sp_extendsegment logsegment, $SYBASE_DB, master\ngo\nuse master\ngo\nsp_dboption $SYBASE_DB, \\\"single user\\\", false\ngo\nuse $SYBASE_DB\ngo\ncheckpoint\ngo\nuse master\ngo\ncreate login $SYBASE_USER with password $SYBASE_PASSWORD\ngo\nexec sp_configure 'enable xml', 1\ngo\nexec sp_dboption $SYBASE_DB, 'abort tran on log full', true\ngo\nexec sp_dboption $SYBASE_DB, 'allow nulls by default', true\ngo\nexec sp_dboption $SYBASE_DB, 'ddl in tran', true\ngo\nexec sp_dboption $SYBASE_DB, 'trunc log on chkpt', true\ngo\nexec sp_dboption $SYBASE_DB, 'full logging for select into', true\ngo\nexec sp_dboption $SYBASE_DB, 'full logging for alter table', true\ngo\nsp_dboption $SYBASE_DB, \\\"select into\\\", true\ngo\nsp_dboption tempdb, 'ddl in tran', true\ngo\nEOSQL\n\n/opt/sybase/OCS-16_0/bin/isql -Usa -P myPassword -S MYSYBASE -i ./init1.sql\n\necho =============== CREATING DB ==========================\ncat <<-EOSQL > init2.sql\nuse $SYBASE_DB\ngo\nsp_adduser '$SYBASE_USER', '$SYBASE_USER', null\ngo\ngrant create default to $SYBASE_USER\ngo\ngrant create table to $SYBASE_USER\ngo\ngrant create view to $SYBASE_USER\ngo\ngrant create rule to $SYBASE_USER\ngo\ngrant create function to $SYBASE_USER\ngo\ngrant create procedure to $SYBASE_USER\ngo\ncommit\ngo\nEOSQL\n\n/opt/sybase/OCS-16_0/bin/isql -Usa -P myPassword -S MYSYBASE -i ./init2.sql\"\n    echo \"Sybase successfully started\"\n}\n\noracle_setup() {\n    HEALTHSTATUS=\n    until [ \"$HEALTHSTATUS\" == \"healthy\" ];\n    do\n        echo \"Waiting for Oracle to start...\"\n        sleep 5;\n        # On WSL, health-checks intervals don't work for Podman, so run them manually\n        if ! command -v docker > /dev/null; then\n          $PRIVILEGED_CLI $CONTAINER_CLI healthcheck run oracle > /dev/null\n        fi\n        HEALTHSTATUS=\"`$PRIVILEGED_CLI $CONTAINER_CLI inspect -f $HEALTCHECK_PATH oracle`\"\n        HEALTHSTATUS=${HEALTHSTATUS##+( )} #Remove longest matching series of spaces from the front\n        HEALTHSTATUS=${HEALTHSTATUS%%+( )} #Remove longest matching series of spaces from the back\n    done\n    sleep 2;\n    echo \"Oracle successfully started\"\n    # We increase file sizes to avoid online resizes as that requires lots of CPU which is restricted in XE\n    $PRIVILEGED_CLI $CONTAINER_CLI exec oracle bash -c \"source /home/oracle/.bashrc; bash -c \\\"\ncat <<EOF | \\$ORACLE_HOME/bin/sqlplus / as sysdba\nset timing on\n-- Remove DISABLE_OOB parameter from Listener configuration and restart it\n!echo Enabling OOB for Listener...\n!echo NAMES.DIRECTORY_PATH=\\(EZCONNECT,TNSNAMES\\) > /opt/oracle/oradata/dbconfig/XE/sqlnet.ora\n!lsnrctl reload\n\n-- Increasing redo logs\nalter database add logfile group 4 '\\$ORACLE_BASE/oradata/XE/redo04.log' size 500M reuse;\nalter database add logfile group 5 '\\$ORACLE_BASE/oradata/XE/redo05.log' size 500M reuse;\nalter database add logfile group 6 '\\$ORACLE_BASE/oradata/XE/redo06.log' size 500M reuse;\nalter system switch logfile;\nalter system switch logfile;\nalter system switch logfile;\nalter system checkpoint;\nalter database drop logfile group 1;\nalter database drop logfile group 2;\nalter database drop logfile group 3;\n!rm \\$ORACLE_BASE/oradata/XE/redo01.log\n!rm \\$ORACLE_BASE/oradata/XE/redo02.log\n!rm \\$ORACLE_BASE/oradata/XE/redo03.log\n\n-- Increasing SYSAUX data file\nalter database datafile '\\$ORACLE_BASE/oradata/XE/sysaux01.dbf' resize 600M;\n\n-- Modifying database init parameters\nalter system set open_cursors=1000 sid='*' scope=both;\nalter system set session_cached_cursors=500 sid='*' scope=spfile;\nalter system set db_securefile=ALWAYS sid='*' scope=spfile;\nalter system set dispatchers='(PROTOCOL=TCP)(SERVICE=XEXDB)(DISPATCHERS=0)' sid='*' scope=spfile;\nalter system set recyclebin=OFF sid='*' SCOPE=SPFILE;\n\n-- Comment the 2 next lines to be able to use Diagnostics Pack features\nalter system set sga_target=0m sid='*' scope=both;\n-- alter system set statistics_level=BASIC sid='*' scope=spfile;\n\n-- Restart the database\nSHUTDOWN IMMEDIATE;\nSTARTUP MOUNT;\nALTER DATABASE OPEN;\n\n-- Switch to the XEPDB1 pluggable database\nalter session set container=xepdb1;\n\n-- Modify XEPDB1 datafiles and tablespaces\nalter database datafile '\\$ORACLE_BASE/oradata/XE/XEPDB1/system01.dbf' resize 320M;\nalter database datafile '\\$ORACLE_BASE/oradata/XE/XEPDB1/sysaux01.dbf' resize 360M;\nalter database datafile '\\$ORACLE_BASE/oradata/XE/XEPDB1/undotbs01.dbf' resize 400M;\nalter database datafile '\\$ORACLE_BASE/oradata/XE/XEPDB1/undotbs01.dbf' autoextend on next 16M;\nalter database tempfile '\\$ORACLE_BASE/oradata/XE/XEPDB1/temp01.dbf' resize 400M;\nalter database tempfile '\\$ORACLE_BASE/oradata/XE/XEPDB1/temp01.dbf' autoextend on next 16M;\nalter database datafile '\\$ORACLE_BASE/oradata/XE/XEPDB1/users01.dbf' resize 100M;\nalter database datafile '\\$ORACLE_BASE/oradata/XE/XEPDB1/users01.dbf' autoextend on next 16M;\nalter tablespace USERS nologging;\nalter tablespace SYSTEM nologging;\nalter tablespace SYSAUX nologging;\n\ncreate user hibernate_orm_test identified by hibernate_orm_test quota unlimited on users;\ngrant all privileges to hibernate_orm_test;\nEOF\\\"\"\n}\n\noracle_free_setup() {\n    HEALTHSTATUS=\n    until [ \"$HEALTHSTATUS\" == \"healthy\" ];\n    do\n        echo \"Waiting for Oracle Free to start...\"\n        sleep 5;\n        # On WSL, health-checks intervals don't work for Podman, so run them manually\n        if ! command -v docker > /dev/null; then\n          $PRIVILEGED_CLI $CONTAINER_CLI healthcheck run oracle > /dev/null\n        fi\n        HEALTHSTATUS=\"`$PRIVILEGED_CLI $CONTAINER_CLI inspect -f $HEALTCHECK_PATH oracle`\"\n        HEALTHSTATUS=${HEALTHSTATUS##+( )} #Remove longest matching series of spaces from the front\n        HEALTHSTATUS=${HEALTHSTATUS%%+( )} #Remove longest matching series of spaces from the back\n    done\n    sleep 2;\n    echo \"Oracle successfully started\"\n    # We increase file sizes to avoid online resizes as that requires lots of CPU which is restricted in XE\n    $PRIVILEGED_CLI $CONTAINER_CLI exec oracle bash -c \"source /home/oracle/.bashrc; bash -c \\\"\ncat <<EOF | \\$ORACLE_HOME/bin/sqlplus / as sysdba\nset timing on\n-- Remove DISABLE_OOB parameter from Listener configuration and restart it\n!echo Enabling OOB for Listener...\n!echo NAMES.DIRECTORY_PATH=\\(EZCONNECT,TNSNAMES\\) > /opt/oracle/oradata/dbconfig/FREE/sqlnet.ora\n!lsnrctl reload\n-- Increasing redo logs\nalter database add logfile group 4 '\\$ORACLE_BASE/oradata/FREE/redo04.log' size 500M reuse;\nalter database add logfile group 5 '\\$ORACLE_BASE/oradata/FREE/redo05.log' size 500M reuse;\nalter database add logfile group 6 '\\$ORACLE_BASE/oradata/FREE/redo06.log' size 500M reuse;\nalter system switch logfile;\nalter system switch logfile;\nalter system switch logfile;\nalter system checkpoint;\nalter database drop logfile group 1;\nalter database drop logfile group 2;\nalter database drop logfile group 3;\n!rm \\$ORACLE_BASE/oradata/FREE/redo01.log\n!rm \\$ORACLE_BASE/oradata/FREE/redo02.log\n!rm \\$ORACLE_BASE/oradata/FREE/redo03.log\n\n-- Increasing SYSAUX data file\nalter database datafile '\\$ORACLE_BASE/oradata/FREE/sysaux01.dbf' resize 600M;\n\n-- Modifying database init parameters\nalter system set open_cursors=1000 sid='*' scope=both;\nalter system set session_cached_cursors=500 sid='*' scope=spfile;\nalter system set db_securefile=ALWAYS sid='*' scope=spfile;\nalter system set dispatchers='(PROTOCOL=TCP)(SERVICE=FREEXDB)(DISPATCHERS=0)' sid='*' scope=spfile;\nalter system set recyclebin=OFF sid='*' SCOPE=SPFILE;\n\n-- Comment the 2 next lines to be able to use Diagnostics Pack features\nalter system set sga_target=0m sid='*' scope=both;\n-- alter system set statistics_level=BASIC sid='*' scope=spfile;\n\n-- Restart the database\nSHUTDOWN IMMEDIATE;\nSTARTUP MOUNT;\nALTER DATABASE OPEN;\n\n-- Switch to the FREEPDB1 pluggable database\nalter session set container=freepdb1;\n\n-- Modify FREEPDB1 datafiles and tablespaces\nalter database datafile '\\$ORACLE_BASE/oradata/FREE/FREEPDB1/system01.dbf' resize 320M;\nalter database datafile '\\$ORACLE_BASE/oradata/FREE/FREEPDB1/sysaux01.dbf' resize 360M;\nalter database datafile '\\$ORACLE_BASE/oradata/FREE/FREEPDB1/undotbs01.dbf' resize 400M;\nalter database datafile '\\$ORACLE_BASE/oradata/FREE/FREEPDB1/undotbs01.dbf' autoextend on next 16M;\nalter database tempfile '\\$ORACLE_BASE/oradata/FREE/FREEPDB1/temp01.dbf' resize 400M;\nalter database tempfile '\\$ORACLE_BASE/oradata/FREE/FREEPDB1/temp01.dbf' autoextend on next 16M;\nalter database datafile '\\$ORACLE_BASE/oradata/FREE/FREEPDB1/users01.dbf' resize 100M;\nalter database datafile '\\$ORACLE_BASE/oradata/FREE/FREEPDB1/users01.dbf' autoextend on next 16M;\nalter tablespace USERS nologging;\nalter tablespace SYSTEM nologging;\nalter tablespace SYSAUX nologging;\n\ncreate user hibernate_orm_test identified by hibernate_orm_test quota unlimited on users;\ngrant all privileges to hibernate_orm_test;\nEOF\\\"\"\n}\n\ndisable_userland_proxy() {\n  if [[ \"$HEALTCHECK_PATH\" == \"{{.State.Health.Status}}\" ]]; then\n    if [[ ! -f /etc/docker/daemon.json ]]; then\n      echo \"Didn't find /etc/docker/daemon.json but need to disable userland-proxy...\"\n      echo \"Stopping docker...\"\n      sudo service docker stop\n      echo \"Creating /etc/docker/daemon.json...\"\n      sudo bash -c \"echo '{\\\"userland-proxy\\\": false}' > /etc/docker/daemon.json\"\n      echo \"Starting docker...\"\n      sudo service docker start\n      echo \"Docker successfully started with userland proxies disabled\"\n    elif ! grep -q userland-proxy /etc/docker/daemon.json; then\n      echo \"Userland proxy is still enabled in /etc/docker/daemon.json, but need to disable it...\"\n      export docker_daemon_json=$(</etc/docker/daemon.json)\n      echo \"Stopping docker...\"\n      sudo service docker stop\n      echo \"Updating /etc/docker/daemon.json...\"\n      sudo bash -c \"export docker_daemon_json='$docker_daemon_json'; echo \\\"\\${docker_daemon_json/\\}/,}\\\\\\\"userland-proxy\\\\\\\": false}\\\" > /etc/docker/daemon.json\"\n      echo \"Starting docker...\"\n      sudo service docker start\n      echo \"Service status:\"\n      sudo journalctl -xeu docker.service\n      echo \"Docker successfully started with userland proxies disabled\"\n    fi\n  fi\n}\n\noracle_atps() {\n  echo \"Managing Oracle Autonomous Database...\"\n  export INFO=$(curl -s -k -L -X GET \"https://api.atlas-controller.oraclecloud.com/ords/atlas/admin/database?type=autonomous2&hostname=`hostname`\" -H 'accept: application/json')\n  export HOST=$(echo $INFO | jq -r '.database' | jq -r '.host')\n  export SERVICE=$(echo $INFO | jq -r '.database' | jq -r '.service')\n  export PASSWORD=$(echo $INFO | jq -r '.database' | jq -r '.password')\n\n  curl -k -s -X POST \"https://${HOST}.oraclevcn.com:8443/ords/admin/_/sql\" -H 'content-type: application/sql' -H 'accept: application/json' -basic -u admin:${PASSWORD} --data-ascii \"create user hibernate_orm_test_$RUNID identified by \\\"Oracle_19_Password\\\" DEFAULT TABLESPACE DATA TEMPORARY TABLESPACE TEMP;alter user hibernate_orm_test_$RUNID quota unlimited on data;grant CREATE SESSION, RESOURCE, CREATE VIEW, CREATE SYNONYM, CREATE ANY INDEX, EXECUTE ANY TYPE to hibernate_orm_test_$RUNID;\"\n}\n\noracle_atps_tls() {\n  echo \"Managing Oracle Autonomous Database...\"\n  export INFO=$(curl -s -k -L -X GET \"https://api.atlas-controller.oraclecloud.com/ords/atlas/admin/database?type=autonomous&hostname=`hostname`\" -H 'accept: application/json')\n  export HOST=$(echo $INFO | jq -r '.database' | jq -r '.host')\n  export SERVICE=$(echo $INFO | jq -r '.database' | jq -r '.service')\n  export PASSWORD=$(echo $INFO | jq -r '.database' | jq -r '.password')\n\n  curl -s -X POST \"https://${HOST}.oraclecloudapps.com/ords/admin/_/sql\" -H 'content-type: application/sql' -H 'accept: application/json' -basic -u admin:${PASSWORD} --data-ascii \"create user hibernate_orm_test_$RUNID identified by \\\"Oracle_19_Password\\\" DEFAULT TABLESPACE DATA TEMPORARY TABLESPACE TEMP;alter user hibernate_orm_test_$RUNID quota unlimited on data;grant CREATE SESSION, RESOURCE, CREATE VIEW, CREATE SYNONYM, CREATE ANY INDEX, EXECUTE ANY TYPE to hibernate_orm_test_$RUNID;\"\n}\n\noracle_db19c() {\n  echo \"Managing Oracle Database 19c...\"\n  export INFO=$(curl -s -k -L -X GET \"https://api.atlas-controller.oraclecloud.com/ords/atlas/admin/database?type=db19c&hostname=`hostname`\" -H 'accept: application/json')\n  export HOST=$(echo $INFO | jq -r '.database' | jq -r '.host')\n  export SERVICE=$(echo $INFO | jq -r '.database' | jq -r '.service')\n  export PASSWORD=$(echo $INFO | jq -r '.database' | jq -r '.password')\n\n/home/opc/sqlcl/bin/sql -s system/$PASSWORD@$HOST:1521/$SERVICE <<EOF\n    create user hibernate_orm_test_$RUNID identified by \"Oracle_19_Password\" DEFAULT TABLESPACE USERS TEMPORARY TABLESPACE TEMP;\n    alter user hibernate_orm_test_$RUNID quota unlimited on users;\n    grant CREATE SESSION, RESOURCE, CREATE VIEW, CREATE SYNONYM, CREATE ANY INDEX, EXECUTE ANY TYPE to hibernate_orm_test_$RUNID;\nEOF\n\n}\n\noracle_db21c() {\n  echo \"Managing Oracle Database 21c...\"\n  export INFO=$(curl -s -k -L -X GET \"https://api.atlas-controller.oraclecloud.com/ords/atlas/admin/database?type=db21c&hostname=`hostname`\" -H 'accept: application/json')\n  export HOST=$(echo $INFO | jq -r '.database' | jq -r '.host')\n  export SERVICE=$(echo $INFO | jq -r '.database' | jq -r '.service')\n  export PASSWORD=$(echo $INFO | jq -r '.database' | jq -r '.password')\n\n/home/opc/sqlcl/bin/sql -s system/$PASSWORD@$HOST:1521/$SERVICE <<EOF\n    create user hibernate_orm_test_$RUNID identified by \"Oracle_21_Password\" DEFAULT TABLESPACE USERS TEMPORARY TABLESPACE TEMP;\n    alter user hibernate_orm_test_$RUNID quota unlimited on users;\n    grant CREATE SESSION, RESOURCE, CREATE VIEW, CREATE SYNONYM, CREATE ANY INDEX, EXECUTE ANY TYPE to hibernate_orm_test_$RUNID;\nEOF\n}\n\noracle_db23c() {\n  echo \"Managing Oracle Database 23c...\"\n  export INFO=$(curl -s -k -L -X GET \"https://api.atlas-controller.oraclecloud.com/ords/atlas/admin/database?type=db23c&hostname=`hostname`\" -H 'accept: application/json')\n  export HOST=$(echo $INFO | jq -r '.database' | jq -r '.host')\n  export SERVICE=$(echo $INFO | jq -r '.database' | jq -r '.service')\n  export PASSWORD=$(echo $INFO | jq -r '.database' | jq -r '.password')\n\n/home/opc/sqlcl/bin/sql -s system/$PASSWORD@$HOST:1521/$SERVICE <<EOF\n    create user hibernate_orm_test_$RUNID identified by \"Oracle_23_Password\" DEFAULT TABLESPACE USERS TEMPORARY TABLESPACE TEMP;\n    alter user hibernate_orm_test_$RUNID quota unlimited on users;\n    grant DB_DEVELOPER_ROLE to hibernate_orm_test_$RUNID;\nEOF\n}\n\noracle() {\n  oracle_23\n}\n\noracle_18() {\n    $PRIVILEGED_CLI $CONTAINER_CLI rm -f oracle || true\n    disable_userland_proxy\n    # We need to use the defaults\n    # SYSTEM/Oracle18\n    $PRIVILEGED_CLI $CONTAINER_CLI run --name oracle -d -p 1521:1521 -e ORACLE_PASSWORD=Oracle18 \\\n       --cap-add cap_net_raw \\\n       --health-cmd healthcheck.sh \\\n       --health-interval 5s \\\n       --health-timeout 5s \\\n       --health-retries 10 \\\n       ${DB_IMAGE_ORACLE_21:-docker.io/gvenzl/oracle-xe:18.4.0}\n    oracle_setup\n}\n\noracle_21() {\n    $PRIVILEGED_CLI $CONTAINER_CLI rm -f oracle || true\n    disable_userland_proxy\n    # We need to use the defaults\n    # SYSTEM/Oracle18\n    $PRIVILEGED_CLI $CONTAINER_CLI run --name oracle -d -p 1521:1521 -e ORACLE_PASSWORD=Oracle18 \\\n       --cap-add cap_net_raw \\\n       --health-cmd healthcheck.sh \\\n       --health-interval 5s \\\n       --health-timeout 5s \\\n       --health-retries 10 \\\n       ${DB_IMAGE_ORACLE_21:-docker.io/gvenzl/oracle-xe:21.3.0}\n    oracle_setup\n}\n\noracle_23() {\n    $PRIVILEGED_CLI $CONTAINER_CLI rm -f oracle || true\n    disable_userland_proxy\n    # We need to use the defaults\n    # SYSTEM/Oracle18\n    $PRIVILEGED_CLI $CONTAINER_CLI run --name oracle -d -p 1521:1521 -e ORACLE_PASSWORD=Oracle18 \\\n       --health-cmd healthcheck.sh \\\n       --health-interval 5s \\\n       --health-timeout 5s \\\n       --health-retries 10 \\\n       ${DB_IMAGE_ORACLE_23:-docker.io/gvenzl/oracle-free:23}\n    oracle_free_setup\n}\n\nhana() {\n    temp_dir=$(mktemp -d)\n    echo '{\"master_password\" : \"H1bernate_test\"}' >$temp_dir/password.json\n    chmod -R 777 $temp_dir\n    $PRIVILEGED_CLI $CONTAINER_CLI rm -f hana || true\n    $PRIVILEGED_CLI $CONTAINER_CLI run -d --name hana -p 39013:39013 -p 39017:39017 -p 39041-39045:39041-39045 -p 1128-1129:1128-1129 -p 59013-59014:59013-59014 \\\n      --memory=8g \\\n      --ulimit nofile=1048576:1048576 \\\n      --sysctl kernel.shmmax=1073741824 \\\n      --sysctl net.ipv4.ip_local_port_range='40000 60999' \\\n      --sysctl kernel.shmmni=4096 \\\n      --sysctl kernel.shmall=8388608 \\\n      -v $temp_dir:/config:Z \\\n      ${DB_IMAGE_HANA:-docker.io/saplabs/hanaexpress:2.00.076.00.20240701.1} \\\n      --passwords-url file:///config/password.json \\\n      --agree-to-sap-license\n    # Give the container some time to start\n    OUTPUT=\n    while [[ $OUTPUT != *\"Startup finished\"* ]]; do\n        echo \"Waiting for HANA to start...\"\n        sleep 10\n        OUTPUT=$($PRIVILEGED_CLI $CONTAINER_CLI logs hana 2>&1)\n    done\n    echo \"HANA successfully started\"\n}\n\ncockroachdb() {\n  cockroachdb_24_1\n}\n\ncockroachdb_24_1() {\n  $CONTAINER_CLI rm -f cockroach || true\n  LOG_CONFIG=\"\nsinks:\n  stderr:\n    channels: all\n    filter: ERROR\n    redact: false\n    exit-on-error: true\n\"\n  $CONTAINER_CLI run -d --name=cockroach -m 6g -p 26257:26257 -p 8080:8080 ${DB_IMAGE_COCKROACHDB_24_1:-cockroachdb/cockroach:v24.1.5} start-single-node \\\n    --insecure --store=type=mem,size=0.25 --advertise-addr=localhost --log=\"$LOG_CONFIG\"\n  OUTPUT=\n  while [[ $OUTPUT != *\"CockroachDB node starting\"* ]]; do\n        echo \"Waiting for CockroachDB to start...\"\n        sleep 10\n        # Note we need to redirect stderr to stdout to capture the logs\n        OUTPUT=$($CONTAINER_CLI logs cockroach 2>&1)\n  done\n  echo \"Enabling experimental box2d operators and some optimized settings for running the tests\"\n  #settings documented in https://www.cockroachlabs.com/docs/v24.1/local-testing#use-a-local-single-node-cluster-with-in-memory-storage\n  $CONTAINER_CLI exec cockroach bash -c \"cat <<EOF | ./cockroach sql --insecure\nSET CLUSTER SETTING sql.spatial.experimental_box2d_comparison_operators.enabled = on;\nSET CLUSTER SETTING kv.range_merge.queue_interval = '50ms';\nSET CLUSTER SETTING jobs.registry.interval.gc = '30s';\nSET CLUSTER SETTING jobs.registry.interval.cancel = '180s';\nSET CLUSTER SETTING jobs.retention_time = '15s';\nSET CLUSTER SETTING sql.stats.automatic_collection.enabled = false;\nSET CLUSTER SETTING kv.range_split.by_load_merge_delay = '5s';\nALTER RANGE default CONFIGURE ZONE USING \"gc.ttlseconds\" = 600;\nALTER DATABASE system CONFIGURE ZONE USING \"gc.ttlseconds\" = 600;\n\nquit\nEOF\n\"\n  echo \"Cockroachdb successfully started\"\n}\n\n\ncockroachdb_23_1() {\n  $CONTAINER_CLI rm -f cockroach || true\n  LOG_CONFIG=\"\nsinks:\n  stderr:\n    channels: all\n    filter: ERROR\n    redact: false\n    exit-on-error: true\n\"\n  $CONTAINER_CLI run -d --name=cockroach -m 6g -p 26257:26257 -p 8080:8080 ${DB_IMAGE_COCKROACHDB_23_1:-docker.io/cockroachdb/cockroach:v23.1.28} start-single-node \\\n    --insecure --store=type=mem,size=0.25 --advertise-addr=localhost --log=\"$LOG_CONFIG\"\n  OUTPUT=\n  while [[ $OUTPUT != *\"CockroachDB node starting\"* ]]; do\n        echo \"Waiting for CockroachDB to start...\"\n        sleep 10\n        # Note we need to redirect stderr to stdout to capture the logs\n        OUTPUT=$($CONTAINER_CLI logs cockroach 2>&1)\n  done\n  echo \"Enabling experimental box2d operators and some optimized settings for running the tests\"\n  #settings documented in https://www.cockroachlabs.com/docs/v22.1/local-testing.html#use-a-local-single-node-cluster-with-in-memory-storage\n  $CONTAINER_CLI exec cockroach bash -c \"cat <<EOF | ./cockroach sql --insecure\nSET CLUSTER SETTING sql.spatial.experimental_box2d_comparison_operators.enabled = on;\nSET CLUSTER SETTING kv.raft_log.disable_synchronization_unsafe = true;\nSET CLUSTER SETTING kv.range_merge.queue_interval = '50ms';\nSET CLUSTER SETTING jobs.registry.interval.gc = '30s';\nSET CLUSTER SETTING jobs.registry.interval.cancel = '180s';\nSET CLUSTER SETTING jobs.retention_time = '15s';\nSET CLUSTER SETTING sql.stats.automatic_collection.enabled = false;\nSET CLUSTER SETTING kv.range_split.by_load_merge_delay = '5s';\nSET CLUSTER SETTING sql.defaults.serial_normalization = 'sql_sequence_cached';\nALTER RANGE default CONFIGURE ZONE USING \"gc.ttlseconds\" = 600;\nALTER DATABASE system CONFIGURE ZONE USING \"gc.ttlseconds\" = 600;\n\nquit\nEOF\n\"\n  echo \"Cockroachdb successfully started\"\n\n}\n\ntidb() {\n  tidb_5_4\n}\n\ntidb_5_4() {\n    $CONTAINER_CLI rm -f tidb || true\n    $CONTAINER_CLI network rm -f tidb_network || true\n    $CONTAINER_CLI network create tidb_network\n    $CONTAINER_CLI run --name tidb -p4000:4000 -d --network tidb_network ${DB_IMAGE_TIDB_5_4:-docker.io/pingcap/tidb:v5.4.3}\n    # Give the container some time to start\n    OUTPUT=\n    n=0\n    until [ \"$n\" -ge 5 ]\n    do\n        OUTPUT=$($CONTAINER_CLI logs tidb 2>&1)\n        if [[ $OUTPUT == *\"server is running\"* ]]; then\n          break;\n        fi\n        n=$((n+1))\n        echo \"Waiting for TiDB to start...\"\n        sleep 3\n    done\n    $CONTAINER_CLI run -it --rm --network tidb_network docker.io/mysql:8.2.0 mysql -htidb -P4000 -uroot -e \"create database hibernate_orm_test; create user 'hibernate_orm_test' identified by 'hibernate_orm_test'; grant all on hibernate_orm_test.* to 'hibernate_orm_test';\"\n    if [ \"$n\" -ge 5 ]; then\n      echo \"TiDB failed to start and configure after 15 seconds\"\n    else\n      echo \"TiDB successfully started\"\n    fi\n}\n\ninformix() {\n  informix_14_10\n}\n\ninformix_14_10() {\n    temp_dir=$(mktemp -d)\n    echo \"ALLOW_NEWLINE 1\" >$temp_dir/onconfig.mod\n    chmod 777 -R $temp_dir\n    $PRIVILEGED_CLI $CONTAINER_CLI rm -f informix || true\n    $PRIVILEGED_CLI $CONTAINER_CLI run --name informix --privileged -p 9088:9088 -v $temp_dir:/opt/ibm/config -e LICENSE=accept -e GL_USEGLU=1 -d ${DB_IMAGE_INFORMIX_14_10:-icr.io/informix/informix-developer-database:14.10.FC9W1DE}\n    echo \"Starting Informix. This can take a few minutes\"\n    # Give the container some time to start\n    OUTPUT=\n    n=0\n    until [ \"$n\" -ge 5 ]\n    do\n        OUTPUT=$($PRIVILEGED_CLI $CONTAINER_CLI logs informix 2>&1)\n        if [[ $OUTPUT == *\"Server Started\"* ]]; then\n          sleep 15\n          $PRIVILEGED_CLI $CONTAINER_CLI exec informix bash -l -c \"export DB_LOCALE=en_US.utf8;export CLIENT_LOCALE=en_US.utf8;echo \\\"execute function task('create dbspace from storagepool', 'datadbs', '100 MB', '4');execute function task('create sbspace from storagepool', 'sbspace', '20 M', '0');create database dev in datadbs with log;\\\" > post_init.sql;dbaccess sysadmin post_init.sql\"\n          break;\n        fi\n        n=$((n+1))\n        echo \"Waiting for Informix to start...\"\n        sleep 30\n    done\n    if [ \"$n\" -ge 5 ]; then\n      echo \"Informix failed to start and configure after 5 minutes\"\n    else\n      echo \"Informix successfully started\"\n    fi\n}\n\ninformix_12_10() {\n    $PRIVILEGED_CLI $CONTAINER_CLI rm -f informix || true\n    $PRIVILEGED_CLI $CONTAINER_CLI run --name informix --privileged -p 9088:9088 -e LICENSE=accept -e GL_USEGLU=1 -d ${DB_IMAGE_INFORMIX_12_10:-ibmcom/informix-developer-database:12.10.FC12W1DE}\n    echo \"Starting Informix. This can take a few minutes\"\n    # Give the container some time to start\n    OUTPUT=\n    n=0\n    until [ \"$n\" -ge 5 ]\n    do\n        OUTPUT=$($PRIVILEGED_CLI $CONTAINER_CLI logs informix 2>&1)\n        if [[ $OUTPUT == *\"login Information\"* ]]; then\n          sleep 15\n          $PRIVILEGED_CLI $CONTAINER_CLI exec informix bash -l -c \"export DB_LOCALE=en_US.utf8;export CLIENT_LOCALE=en_US.utf8;echo \\\"execute function task('create dbspace from storagepool', 'datadbs', '100 MB', '4');execute function task('create sbspace from storagepool', 'sbspace', '20 M', '0');create database dev in datadbs with log;\\\" > post_init.sql;dbaccess sysadmin post_init.sql\"\n          break;\n        fi\n        n=$((n+1))\n        echo \"Waiting for Informix to start...\"\n        sleep 30\n    done\n    if [ \"$n\" -ge 5 ]; then\n      echo \"Informix failed to start and configure after 5 minutes\"\n    else\n      echo \"Informix successfully started\"\n    fi\n}\n\nif [ -z ${1} ]; then\n    echo \"No db name provided\"\n    echo \"Provide one of:\"\n    echo -e \"\\tcockroachdb\"\n    echo -e \"\\tcockroachdb_24_1\"\n    echo -e \"\\tcockroachdb_23_1\"\n    echo -e \"\\tdb2\"\n    echo -e \"\\tdb2_11_5\"\n    echo -e \"\\tdb2_10_5\"\n    echo -e \"\\tdb2_spatial\"\n    echo -e \"\\tedb\"\n    echo -e \"\\tedb_16\"\n    echo -e \"\\tedb_15\"\n    echo -e \"\\tedb_14\"\n    echo -e \"\\tedb_12\"\n    echo -e \"\\thana\"\n    echo -e \"\\tmariadb\"\n    echo -e \"\\tmariadb_verylatest\"\n    echo -e \"\\tmariadb_11_7\"\n    echo -e \"\\tmariadb_11_4\"\n    echo -e \"\\tmariadb_11_1\"\n    echo -e \"\\tmariadb_10_11\"\n    echo -e \"\\tmariadb_10_5\"\n    echo -e \"\\tmssql\"\n    echo -e \"\\tmssql_2022\"\n    echo -e \"\\tmssql_2017\"\n    echo -e \"\\tmysql\"\n    echo -e \"\\tmysql_8_2\"\n    echo -e \"\\tmysql_8_1\"\n    echo -e \"\\tmysql_8_0\"\n    echo -e \"\\toracle\"\n    echo -e \"\\toracle_23\"\n    echo -e \"\\toracle_21\"\n    echo -e \"\\tpostgresql\"\n    echo -e \"\\tpostgresql_16\"\n    echo -e \"\\tpostgresql_15\"\n    echo -e \"\\tpostgresql_14\"\n    echo -e \"\\tpostgresql_13\"\n    echo -e \"\\tpostgresql_12\"\n    echo -e \"\\tsybase\"\n    echo -e \"\\ttidb\"\n    echo -e \"\\ttidb_5_4\"\n    echo -e \"\\informix\"\n    echo -e \"\\informix_14_10\"\n    echo -e \"\\informix_12_10\"\nelse\n    ${1}\nfi\n"
        },
        {
          "name": "documentation",
          "type": "tree",
          "content": null
        },
        {
          "name": "drivers",
          "type": "tree",
          "content": null
        },
        {
          "name": "edb",
          "type": "tree",
          "content": null
        },
        {
          "name": "etc",
          "type": "tree",
          "content": null
        },
        {
          "name": "gradle.properties",
          "type": "blob",
          "size": 2.1435546875,
          "content": "db=h2\n\n# Keep all these properties in sync unless you know what you are doing!\n# We set '-Dlog4j2.disableJmx=true' to prevent classloader leaks triggered by the logger.\n# (Some of these settings need to be repeated in the test.jvmArgs blocks of each module)\norg.gradle.jvmargs=-Dlog4j2.disableJmx -Xmx2g -XX:MaxMetaspaceSize=256m -XX:+HeapDumpOnOutOfMemoryError -Duser.language=en -Duser.country=US -Duser.timezone=UTC -Dfile.encoding=UTF-8\ntoolchain.compiler.jvmargs=-Dlog4j2.disableJmx=true -Xmx2g -XX:MaxMetaspaceSize=256m -XX:+HeapDumpOnOutOfMemoryError -Duser.language=en -Duser.country=US -Duser.timezone=UTC -Dfile.encoding=UTF-8\ntoolchain.javadoc.jvmargs=-Dlog4j2.disableJmx=true -Xmx2g -XX:MaxMetaspaceSize=256m -XX:+HeapDumpOnOutOfMemoryError -Duser.language=en -Duser.country=US -Duser.timezone=UTC -Dfile.encoding=UTF-8\ntoolchain.launcher.jvmargs=-Dlog4j2.disableJmx=true -Xmx2g -XX:MaxMetaspaceSize=448m -XX:+HeapDumpOnOutOfMemoryError -Duser.language=en -Duser.country=US -Duser.timezone=UTC -Dfile.encoding=UTF-8\n\norg.gradle.parallel=true\n\n# enable Gradle's Task Cache.  worst case:\n# > rm -rf ~/.gradle/caches/build-cache-1\norg.gradle.caching=true\n\n# JDK auto-detection is not quite ready yet in Gradle 6.7.\n# On Fedora in particular, if you have the package java-1.8.0-openjdk-headless-1.8.0.265.b01-1.fc32.x86_64 installed,\n# Gradle will look for the Java binaries in /usr/lib/jvm/java-1.8.0-openjdk-1.8.0.265.b01-1.fc32.x86_64/bin/java\n# but it won't find it and will fail.\n# It's just a JRE, so it's perfectly normal that the JDK is not present;\n# the JRE is under /usr/lib/jvm/java-1.8.0-openjdk-1.8.0.265.b01-1.fc32.x86_64/jre\norg.gradle.java.installations.auto-detect=false\n# We can't rely on Gradle's auto-download of JDKs as it doesn't support EA releases.\n# See https://github.com/gradle/gradle/blob/fc7ea24f3c525d8d12a4346eb0f15976a6be9414/subprojects/platform-jvm/src/main/java/org/gradle/jvm/toolchain/install/internal/AdoptOpenJdkRemoteBinary.java#L114\norg.gradle.java.installations.auto-download=false\n\n# externalized definition of JDK versions so that they are available in both Project (build.gradle) and Settings (settings.gradle)\norm.jdk.base=17\norm.jdk.max=22"
        },
        {
          "name": "gradle",
          "type": "tree",
          "content": null
        },
        {
          "name": "gradlew",
          "type": "blob",
          "size": 8.48828125,
          "content": "#!/bin/sh\n\n#\n# Copyright © 2015-2021 the original authors.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n##############################################################################\n#\n#   Gradle start up script for POSIX generated by Gradle.\n#\n#   Important for running:\n#\n#   (1) You need a POSIX-compliant shell to run this script. If your /bin/sh is\n#       noncompliant, but you have some other compliant shell such as ksh or\n#       bash, then to run this script, type that shell name before the whole\n#       command line, like:\n#\n#           ksh Gradle\n#\n#       Busybox and similar reduced shells will NOT work, because this script\n#       requires all of these POSIX shell features:\n#         * functions;\n#         * expansions «$var», «${var}», «${var:-default}», «${var+SET}»,\n#           «${var#prefix}», «${var%suffix}», and «$( cmd )»;\n#         * compound commands having a testable exit status, especially «case»;\n#         * various built-in commands including «command», «set», and «ulimit».\n#\n#   Important for patching:\n#\n#   (2) This script targets any POSIX shell, so it avoids extensions provided\n#       by Bash, Ksh, etc; in particular arrays are avoided.\n#\n#       The \"traditional\" practice of packing multiple parameters into a\n#       space-separated string is a well documented source of bugs and security\n#       problems, so this is (mostly) avoided, by progressively accumulating\n#       options in \"$@\", and eventually passing that to Java.\n#\n#       Where the inherited environment variables (DEFAULT_JVM_OPTS, JAVA_OPTS,\n#       and GRADLE_OPTS) rely on word-splitting, this is performed explicitly;\n#       see the in-line comments for details.\n#\n#       There are tweaks for specific operating systems such as AIX, CygWin,\n#       Darwin, MinGW, and NonStop.\n#\n#   (3) This script is generated from the Groovy template\n#       https://github.com/gradle/gradle/blob/HEAD/subprojects/plugins/src/main/resources/org/gradle/api/internal/plugins/unixStartScript.txt\n#       within the Gradle project.\n#\n#       You can find Gradle at https://github.com/gradle/gradle/.\n#\n##############################################################################\n\n# Attempt to set APP_HOME\n\n# Resolve links: $0 may be a link\napp_path=$0\n\n# Need this for daisy-chained symlinks.\nwhile\n    APP_HOME=${app_path%\"${app_path##*/}\"}  # leaves a trailing /; empty if no leading path\n    [ -h \"$app_path\" ]\ndo\n    ls=$( ls -ld \"$app_path\" )\n    link=${ls#*' -> '}\n    case $link in             #(\n      /*)   app_path=$link ;; #(\n      *)    app_path=$APP_HOME$link ;;\n    esac\ndone\n\n# This is normally unused\n# shellcheck disable=SC2034\nAPP_BASE_NAME=${0##*/}\n# Discard cd standard output in case $CDPATH is set (https://github.com/gradle/gradle/issues/25036)\nAPP_HOME=$( cd \"${APP_HOME:-./}\" > /dev/null && pwd -P ) || exit\n\n# Use the maximum available, or set MAX_FD != -1 to use that value.\nMAX_FD=maximum\n\nwarn () {\n    echo \"$*\"\n} >&2\n\ndie () {\n    echo\n    echo \"$*\"\n    echo\n    exit 1\n} >&2\n\n# OS specific support (must be 'true' or 'false').\ncygwin=false\nmsys=false\ndarwin=false\nnonstop=false\ncase \"$( uname )\" in                #(\n  CYGWIN* )         cygwin=true  ;; #(\n  Darwin* )         darwin=true  ;; #(\n  MSYS* | MINGW* )  msys=true    ;; #(\n  NONSTOP* )        nonstop=true ;;\nesac\n\nCLASSPATH=$APP_HOME/gradle/wrapper/gradle-wrapper.jar\n\n\n# Determine the Java command to use to start the JVM.\nif [ -n \"$JAVA_HOME\" ] ; then\n    if [ -x \"$JAVA_HOME/jre/sh/java\" ] ; then\n        # IBM's JDK on AIX uses strange locations for the executables\n        JAVACMD=$JAVA_HOME/jre/sh/java\n    else\n        JAVACMD=$JAVA_HOME/bin/java\n    fi\n    if [ ! -x \"$JAVACMD\" ] ; then\n        die \"ERROR: JAVA_HOME is set to an invalid directory: $JAVA_HOME\n\nPlease set the JAVA_HOME variable in your environment to match the\nlocation of your Java installation.\"\n    fi\nelse\n    JAVACMD=java\n    if ! command -v java >/dev/null 2>&1\n    then\n        die \"ERROR: JAVA_HOME is not set and no 'java' command could be found in your PATH.\n\nPlease set the JAVA_HOME variable in your environment to match the\nlocation of your Java installation.\"\n    fi\nfi\n\n# Increase the maximum file descriptors if we can.\nif ! \"$cygwin\" && ! \"$darwin\" && ! \"$nonstop\" ; then\n    case $MAX_FD in #(\n      max*)\n        # In POSIX sh, ulimit -H is undefined. That's why the result is checked to see if it worked.\n        # shellcheck disable=SC2039,SC3045\n        MAX_FD=$( ulimit -H -n ) ||\n            warn \"Could not query maximum file descriptor limit\"\n    esac\n    case $MAX_FD in  #(\n      '' | soft) :;; #(\n      *)\n        # In POSIX sh, ulimit -n is undefined. That's why the result is checked to see if it worked.\n        # shellcheck disable=SC2039,SC3045\n        ulimit -n \"$MAX_FD\" ||\n            warn \"Could not set maximum file descriptor limit to $MAX_FD\"\n    esac\nfi\n\n# Collect all arguments for the java command, stacking in reverse order:\n#   * args from the command line\n#   * the main class name\n#   * -classpath\n#   * -D...appname settings\n#   * --module-path (only if needed)\n#   * DEFAULT_JVM_OPTS, JAVA_OPTS, and GRADLE_OPTS environment variables.\n\n# For Cygwin or MSYS, switch paths to Windows format before running java\nif \"$cygwin\" || \"$msys\" ; then\n    APP_HOME=$( cygpath --path --mixed \"$APP_HOME\" )\n    CLASSPATH=$( cygpath --path --mixed \"$CLASSPATH\" )\n\n    JAVACMD=$( cygpath --unix \"$JAVACMD\" )\n\n    # Now convert the arguments - kludge to limit ourselves to /bin/sh\n    for arg do\n        if\n            case $arg in                                #(\n              -*)   false ;;                            # don't mess with options #(\n              /?*)  t=${arg#/} t=/${t%%/*}              # looks like a POSIX filepath\n                    [ -e \"$t\" ] ;;                      #(\n              *)    false ;;\n            esac\n        then\n            arg=$( cygpath --path --ignore --mixed \"$arg\" )\n        fi\n        # Roll the args list around exactly as many times as the number of\n        # args, so each arg winds up back in the position where it started, but\n        # possibly modified.\n        #\n        # NB: a `for` loop captures its iteration list before it begins, so\n        # changing the positional parameters here affects neither the number of\n        # iterations, nor the values presented in `arg`.\n        shift                   # remove old arg\n        set -- \"$@\" \"$arg\"      # push replacement arg\n    done\nfi\n\n\n# Add default JVM options here. You can also use JAVA_OPTS and GRADLE_OPTS to pass JVM options to this script.\nDEFAULT_JVM_OPTS='\"-Xmx64m\" \"-Xms64m\"'\n\n# Collect all arguments for the java command:\n#   * DEFAULT_JVM_OPTS, JAVA_OPTS, JAVA_OPTS, and optsEnvironmentVar are not allowed to contain shell fragments,\n#     and any embedded shellness will be escaped.\n#   * For example: A user cannot expect ${Hostname} to be expanded, as it is an environment variable and will be\n#     treated as '${Hostname}' itself on the command line.\n\nset -- \\\n        \"-Dorg.gradle.appname=$APP_BASE_NAME\" \\\n        -classpath \"$CLASSPATH\" \\\n        org.gradle.wrapper.GradleWrapperMain \\\n        \"$@\"\n\n# Stop when \"xargs\" is not available.\nif ! command -v xargs >/dev/null 2>&1\nthen\n    die \"xargs is not available\"\nfi\n\n# Use \"xargs\" to parse quoted args.\n#\n# With -n1 it outputs one arg per line, with the quotes and backslashes removed.\n#\n# In Bash we could simply go:\n#\n#   readarray ARGS < <( xargs -n1 <<<\"$var\" ) &&\n#   set -- \"${ARGS[@]}\" \"$@\"\n#\n# but POSIX shell has neither arrays nor command substitution, so instead we\n# post-process each arg (as a line of input to sed) to backslash-escape any\n# character that might be a shell metacharacter, then use eval to reverse\n# that process (while maintaining the separation between arguments), and wrap\n# the whole thing up as a single \"set\" statement.\n#\n# This will of course break if any of these variables contains a newline or\n# an unmatched quote.\n#\n\neval \"set -- $(\n        printf '%s\\n' \"$DEFAULT_JVM_OPTS $JAVA_OPTS $GRADLE_OPTS\" |\n        xargs -n1 |\n        sed ' s~[^-[:alnum:]+,./:=@_]~\\\\&~g; ' |\n        tr '\\n' ' '\n    )\" '\"$@\"'\n\nexec \"$JAVACMD\" \"$@\"\n"
        },
        {
          "name": "gradlew.bat",
          "type": "blob",
          "size": 2.80078125,
          "content": "@rem\r\n@rem Copyright 2015 the original author or authors.\r\n@rem\r\n@rem Licensed under the Apache License, Version 2.0 (the \"License\");\r\n@rem you may not use this file except in compliance with the License.\r\n@rem You may obtain a copy of the License at\r\n@rem\r\n@rem      https://www.apache.org/licenses/LICENSE-2.0\r\n@rem\r\n@rem Unless required by applicable law or agreed to in writing, software\r\n@rem distributed under the License is distributed on an \"AS IS\" BASIS,\r\n@rem WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n@rem See the License for the specific language governing permissions and\r\n@rem limitations under the License.\r\n@rem\r\n\r\n@if \"%DEBUG%\"==\"\" @echo off\r\n@rem ##########################################################################\r\n@rem\r\n@rem  Gradle startup script for Windows\r\n@rem\r\n@rem ##########################################################################\r\n\r\n@rem Set local scope for the variables with windows NT shell\r\nif \"%OS%\"==\"Windows_NT\" setlocal\r\n\r\nset DIRNAME=%~dp0\r\nif \"%DIRNAME%\"==\"\" set DIRNAME=.\r\n@rem This is normally unused\r\nset APP_BASE_NAME=%~n0\r\nset APP_HOME=%DIRNAME%\r\n\r\n@rem Resolve any \".\" and \"..\" in APP_HOME to make it shorter.\r\nfor %%i in (\"%APP_HOME%\") do set APP_HOME=%%~fi\r\n\r\n@rem Add default JVM options here. You can also use JAVA_OPTS and GRADLE_OPTS to pass JVM options to this script.\r\nset DEFAULT_JVM_OPTS=\"-Xmx64m\" \"-Xms64m\"\r\n\r\n@rem Find java.exe\r\nif defined JAVA_HOME goto findJavaFromJavaHome\r\n\r\nset JAVA_EXE=java.exe\r\n%JAVA_EXE% -version >NUL 2>&1\r\nif %ERRORLEVEL% equ 0 goto execute\r\n\r\necho.\r\necho ERROR: JAVA_HOME is not set and no 'java' command could be found in your PATH.\r\necho.\r\necho Please set the JAVA_HOME variable in your environment to match the\r\necho location of your Java installation.\r\n\r\ngoto fail\r\n\r\n:findJavaFromJavaHome\r\nset JAVA_HOME=%JAVA_HOME:\"=%\r\nset JAVA_EXE=%JAVA_HOME%/bin/java.exe\r\n\r\nif exist \"%JAVA_EXE%\" goto execute\r\n\r\necho.\r\necho ERROR: JAVA_HOME is set to an invalid directory: %JAVA_HOME%\r\necho.\r\necho Please set the JAVA_HOME variable in your environment to match the\r\necho location of your Java installation.\r\n\r\ngoto fail\r\n\r\n:execute\r\n@rem Setup the command line\r\n\r\nset CLASSPATH=%APP_HOME%\\gradle\\wrapper\\gradle-wrapper.jar\r\n\r\n\r\n@rem Execute Gradle\r\n\"%JAVA_EXE%\" %DEFAULT_JVM_OPTS% %JAVA_OPTS% %GRADLE_OPTS% \"-Dorg.gradle.appname=%APP_BASE_NAME%\" -classpath \"%CLASSPATH%\" org.gradle.wrapper.GradleWrapperMain %*\r\n\r\n:end\r\n@rem End local scope for the variables with windows NT shell\r\nif %ERRORLEVEL% equ 0 goto mainEnd\r\n\r\n:fail\r\nrem Set variable GRADLE_EXIT_CONSOLE if you need the _script_ return code instead of\r\nrem the _cmd.exe /c_ return code!\r\nset EXIT_CODE=%ERRORLEVEL%\r\nif %EXIT_CODE% equ 0 set EXIT_CODE=1\r\nif not \"\"==\"%GRADLE_EXIT_CONSOLE%\" exit %EXIT_CODE%\r\nexit /b %EXIT_CODE%\r\n\r\n:mainEnd\r\nif \"%OS%\"==\"Windows_NT\" endlocal\r\n\r\n:omega\r\n"
        },
        {
          "name": "hibernate-agroal",
          "type": "tree",
          "content": null
        },
        {
          "name": "hibernate-c3p0",
          "type": "tree",
          "content": null
        },
        {
          "name": "hibernate-community-dialects",
          "type": "tree",
          "content": null
        },
        {
          "name": "hibernate-core",
          "type": "tree",
          "content": null
        },
        {
          "name": "hibernate-envers",
          "type": "tree",
          "content": null
        },
        {
          "name": "hibernate-graalvm",
          "type": "tree",
          "content": null
        },
        {
          "name": "hibernate-hikaricp",
          "type": "tree",
          "content": null
        },
        {
          "name": "hibernate-integrationtest-java-modules",
          "type": "tree",
          "content": null
        },
        {
          "name": "hibernate-jcache",
          "type": "tree",
          "content": null
        },
        {
          "name": "hibernate-jfr",
          "type": "tree",
          "content": null
        },
        {
          "name": "hibernate-micrometer",
          "type": "tree",
          "content": null
        },
        {
          "name": "hibernate-platform",
          "type": "tree",
          "content": null
        },
        {
          "name": "hibernate-scan-jandex",
          "type": "tree",
          "content": null
        },
        {
          "name": "hibernate-spatial",
          "type": "tree",
          "content": null
        },
        {
          "name": "hibernate-testing",
          "type": "tree",
          "content": null
        },
        {
          "name": "hibernate-ucp",
          "type": "tree",
          "content": null
        },
        {
          "name": "hibernate-vector",
          "type": "tree",
          "content": null
        },
        {
          "name": "hibernate_logo.gif",
          "type": "blob",
          "size": 1.421875,
          "content": null
        },
        {
          "name": "javadoc",
          "type": "tree",
          "content": null
        },
        {
          "name": "lgpl.txt",
          "type": "blob",
          "size": 25.908203125,
          "content": "                  GNU LESSER GENERAL PUBLIC LICENSE\n                       Version 2.1, February 1999\n\n Copyright (C) 1991, 1999 Free Software Foundation, Inc.\n 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301  USA\n Everyone is permitted to copy and distribute verbatim copies\n of this license document, but changing it is not allowed.\n\n[This is the first released version of the Lesser GPL.  It also counts\n as the successor of the GNU Library Public License, version 2, hence\n the version number 2.1.]\n\n                            Preamble\n\n  The licenses for most software are designed to take away your\nfreedom to share and change it.  By contrast, the GNU General Public\nLicenses are intended to guarantee your freedom to share and change\nfree software--to make sure the software is free for all its users.\n\n  This license, the Lesser General Public License, applies to some\nspecially designated software packages--typically libraries--of the\nFree Software Foundation and other authors who decide to use it.  You\ncan use it too, but we suggest you first think carefully about whether\nthis license or the ordinary General Public License is the better\nstrategy to use in any particular case, based on the explanations below.\n\n  When we speak of free software, we are referring to freedom of use,\nnot price.  Our General Public Licenses are designed to make sure that\nyou have the freedom to distribute copies of free software (and charge\nfor this service if you wish); that you receive source code or can get\nit if you want it; that you can change the software and use pieces of\nit in new free programs; and that you are informed that you can do\nthese things.\n\n  To protect your rights, we need to make restrictions that forbid\ndistributors to deny you these rights or to ask you to surrender these\nrights.  These restrictions translate to certain responsibilities for\nyou if you distribute copies of the library or if you modify it.\n\n  For example, if you distribute copies of the library, whether gratis\nor for a fee, you must give the recipients all the rights that we gave\nyou.  You must make sure that they, too, receive or can get the source\ncode.  If you link other code with the library, you must provide\ncomplete object files to the recipients, so that they can relink them\nwith the library after making changes to the library and recompiling\nit.  And you must show them these terms so they know their rights.\n\n  We protect your rights with a two-step method: (1) we copyright the\nlibrary, and (2) we offer you this license, which gives you legal\npermission to copy, distribute and/or modify the library.\n\n  To protect each distributor, we want to make it very clear that\nthere is no warranty for the free library.  Also, if the library is\nmodified by someone else and passed on, the recipients should know\nthat what they have is not the original version, so that the original\nauthor's reputation will not be affected by problems that might be\nintroduced by others.\n\f\n  Finally, software patents pose a constant threat to the existence of\nany free program.  We wish to make sure that a company cannot\neffectively restrict the users of a free program by obtaining a\nrestrictive license from a patent holder.  Therefore, we insist that\nany patent license obtained for a version of the library must be\nconsistent with the full freedom of use specified in this license.\n\n  Most GNU software, including some libraries, is covered by the\nordinary GNU General Public License.  This license, the GNU Lesser\nGeneral Public License, applies to certain designated libraries, and\nis quite different from the ordinary General Public License.  We use\nthis license for certain libraries in order to permit linking those\nlibraries into non-free programs.\n\n  When a program is linked with a library, whether statically or using\na shared library, the combination of the two is legally speaking a\ncombined work, a derivative of the original library.  The ordinary\nGeneral Public License therefore permits such linking only if the\nentire combination fits its criteria of freedom.  The Lesser General\nPublic License permits more lax criteria for linking other code with\nthe library.\n\n  We call this license the \"Lesser\" General Public License because it\ndoes Less to protect the user's freedom than the ordinary General\nPublic License.  It also provides other free software developers Less\nof an advantage over competing non-free programs.  These disadvantages\nare the reason we use the ordinary General Public License for many\nlibraries.  However, the Lesser license provides advantages in certain\nspecial circumstances.\n\n  For example, on rare occasions, there may be a special need to\nencourage the widest possible use of a certain library, so that it becomes\na de-facto standard.  To achieve this, non-free programs must be\nallowed to use the library.  A more frequent case is that a free\nlibrary does the same job as widely used non-free libraries.  In this\ncase, there is little to gain by limiting the free library to free\nsoftware only, so we use the Lesser General Public License.\n\n  In other cases, permission to use a particular library in non-free\nprograms enables a greater number of people to use a large body of\nfree software.  For example, permission to use the GNU C Library in\nnon-free programs enables many more people to use the whole GNU\noperating system, as well as its variant, the GNU/Linux operating\nsystem.\n\n  Although the Lesser General Public License is Less protective of the\nusers' freedom, it does ensure that the user of a program that is\nlinked with the Library has the freedom and the wherewithal to run\nthat program using a modified version of the Library.\n\n  The precise terms and conditions for copying, distribution and\nmodification follow.  Pay close attention to the difference between a\n\"work based on the library\" and a \"work that uses the library\".  The\nformer contains code derived from the library, whereas the latter must\nbe combined with the library in order to run.\n\f\n                  GNU LESSER GENERAL PUBLIC LICENSE\n   TERMS AND CONDITIONS FOR COPYING, DISTRIBUTION AND MODIFICATION\n\n  0. This License Agreement applies to any software library or other\nprogram which contains a notice placed by the copyright holder or\nother authorized party saying it may be distributed under the terms of\nthis Lesser General Public License (also called \"this License\").\nEach licensee is addressed as \"you\".\n\n  A \"library\" means a collection of software functions and/or data\nprepared so as to be conveniently linked with application programs\n(which use some of those functions and data) to form executables.\n\n  The \"Library\", below, refers to any such software library or work\nwhich has been distributed under these terms.  A \"work based on the\nLibrary\" means either the Library or any derivative work under\ncopyright law: that is to say, a work containing the Library or a\nportion of it, either verbatim or with modifications and/or translated\nstraightforwardly into another language.  (Hereinafter, translation is\nincluded without limitation in the term \"modification\".)\n\n  \"Source code\" for a work means the preferred form of the work for\nmaking modifications to it.  For a library, complete source code means\nall the source code for all modules it contains, plus any associated\ninterface definition files, plus the scripts used to control compilation\nand installation of the library.\n\n  Activities other than copying, distribution and modification are not\ncovered by this License; they are outside its scope.  The act of\nrunning a program using the Library is not restricted, and output from\nsuch a program is covered only if its contents constitute a work based\non the Library (independent of the use of the Library in a tool for\nwriting it).  Whether that is true depends on what the Library does\nand what the program that uses the Library does.\n\n  1. You may copy and distribute verbatim copies of the Library's\ncomplete source code as you receive it, in any medium, provided that\nyou conspicuously and appropriately publish on each copy an\nappropriate copyright notice and disclaimer of warranty; keep intact\nall the notices that refer to this License and to the absence of any\nwarranty; and distribute a copy of this License along with the\nLibrary.\n\n  You may charge a fee for the physical act of transferring a copy,\nand you may at your option offer warranty protection in exchange for a\nfee.\n\f\n  2. You may modify your copy or copies of the Library or any portion\nof it, thus forming a work based on the Library, and copy and\ndistribute such modifications or work under the terms of Section 1\nabove, provided that you also meet all of these conditions:\n\n    a) The modified work must itself be a software library.\n\n    b) You must cause the files modified to carry prominent notices\n    stating that you changed the files and the date of any change.\n\n    c) You must cause the whole of the work to be licensed at no\n    charge to all third parties under the terms of this License.\n\n    d) If a facility in the modified Library refers to a function or a\n    table of data to be supplied by an application program that uses\n    the facility, other than as an argument passed when the facility\n    is invoked, then you must make a good faith effort to ensure that,\n    in the event an application does not supply such function or\n    table, the facility still operates, and performs whatever part of\n    its purpose remains meaningful.\n\n    (For example, a function in a library to compute square roots has\n    a purpose that is entirely well-defined independent of the\n    application.  Therefore, Subsection 2d requires that any\n    application-supplied function or table used by this function must\n    be optional: if the application does not supply it, the square\n    root function must still compute square roots.)\n\nThese requirements apply to the modified work as a whole.  If\nidentifiable sections of that work are not derived from the Library,\nand can be reasonably considered independent and separate works in\nthemselves, then this License, and its terms, do not apply to those\nsections when you distribute them as separate works.  But when you\ndistribute the same sections as part of a whole which is a work based\non the Library, the distribution of the whole must be on the terms of\nthis License, whose permissions for other licensees extend to the\nentire whole, and thus to each and every part regardless of who wrote\nit.\n\nThus, it is not the intent of this section to claim rights or contest\nyour rights to work written entirely by you; rather, the intent is to\nexercise the right to control the distribution of derivative or\ncollective works based on the Library.\n\nIn addition, mere aggregation of another work not based on the Library\nwith the Library (or with a work based on the Library) on a volume of\na storage or distribution medium does not bring the other work under\nthe scope of this License.\n\n  3. You may opt to apply the terms of the ordinary GNU General Public\nLicense instead of this License to a given copy of the Library.  To do\nthis, you must alter all the notices that refer to this License, so\nthat they refer to the ordinary GNU General Public License, version 2,\ninstead of to this License.  (If a newer version than version 2 of the\nordinary GNU General Public License has appeared, then you can specify\nthat version instead if you wish.)  Do not make any other change in\nthese notices.\n\f\n  Once this change is made in a given copy, it is irreversible for\nthat copy, so the ordinary GNU General Public License applies to all\nsubsequent copies and derivative works made from that copy.\n\n  This option is useful when you wish to copy part of the code of\nthe Library into a program that is not a library.\n\n  4. You may copy and distribute the Library (or a portion or\nderivative of it, under Section 2) in object code or executable form\nunder the terms of Sections 1 and 2 above provided that you accompany\nit with the complete corresponding machine-readable source code, which\nmust be distributed under the terms of Sections 1 and 2 above on a\nmedium customarily used for software interchange.\n\n  If distribution of object code is made by offering access to copy\nfrom a designated place, then offering equivalent access to copy the\nsource code from the same place satisfies the requirement to\ndistribute the source code, even though third parties are not\ncompelled to copy the source along with the object code.\n\n  5. A program that contains no derivative of any portion of the\nLibrary, but is designed to work with the Library by being compiled or\nlinked with it, is called a \"work that uses the Library\".  Such a\nwork, in isolation, is not a derivative work of the Library, and\ntherefore falls outside the scope of this License.\n\n  However, linking a \"work that uses the Library\" with the Library\ncreates an executable that is a derivative of the Library (because it\ncontains portions of the Library), rather than a \"work that uses the\nlibrary\".  The executable is therefore covered by this License.\nSection 6 states terms for distribution of such executables.\n\n  When a \"work that uses the Library\" uses material from a header file\nthat is part of the Library, the object code for the work may be a\nderivative work of the Library even though the source code is not.\nWhether this is true is especially significant if the work can be\nlinked without the Library, or if the work is itself a library.  The\nthreshold for this to be true is not precisely defined by law.\n\n  If such an object file uses only numerical parameters, data\nstructure layouts and accessors, and small macros and small inline\nfunctions (ten lines or less in length), then the use of the object\nfile is unrestricted, regardless of whether it is legally a derivative\nwork.  (Executables containing this object code plus portions of the\nLibrary will still fall under Section 6.)\n\n  Otherwise, if the work is a derivative of the Library, you may\ndistribute the object code for the work under the terms of Section 6.\nAny executables containing that work also fall under Section 6,\nwhether or not they are linked directly with the Library itself.\n\f\n  6. As an exception to the Sections above, you may also combine or\nlink a \"work that uses the Library\" with the Library to produce a\nwork containing portions of the Library, and distribute that work\nunder terms of your choice, provided that the terms permit\nmodification of the work for the customer's own use and reverse\nengineering for debugging such modifications.\n\n  You must give prominent notice with each copy of the work that the\nLibrary is used in it and that the Library and its use are covered by\nthis License.  You must supply a copy of this License.  If the work\nduring execution displays copyright notices, you must include the\ncopyright notice for the Library among them, as well as a reference\ndirecting the user to the copy of this License.  Also, you must do one\nof these things:\n\n    a) Accompany the work with the complete corresponding\n    machine-readable source code for the Library including whatever\n    changes were used in the work (which must be distributed under\n    Sections 1 and 2 above); and, if the work is an executable linked\n    with the Library, with the complete machine-readable \"work that\n    uses the Library\", as object code and/or source code, so that the\n    user can modify the Library and then relink to produce a modified\n    executable containing the modified Library.  (It is understood\n    that the user who changes the contents of definitions files in the\n    Library will not necessarily be able to recompile the application\n    to use the modified definitions.)\n\n    b) Use a suitable shared library mechanism for linking with the\n    Library.  A suitable mechanism is one that (1) uses at run time a\n    copy of the library already present on the user's computer system,\n    rather than copying library functions into the executable, and (2)\n    will operate properly with a modified version of the library, if\n    the user installs one, as long as the modified version is\n    interface-compatible with the version that the work was made with.\n\n    c) Accompany the work with a written offer, valid for at\n    least three years, to give the same user the materials\n    specified in Subsection 6a, above, for a charge no more\n    than the cost of performing this distribution.\n\n    d) If distribution of the work is made by offering access to copy\n    from a designated place, offer equivalent access to copy the above\n    specified materials from the same place.\n\n    e) Verify that the user has already received a copy of these\n    materials or that you have already sent this user a copy.\n\n  For an executable, the required form of the \"work that uses the\nLibrary\" must include any data and utility programs needed for\nreproducing the executable from it.  However, as a special exception,\nthe materials to be distributed need not include anything that is\nnormally distributed (in either source or binary form) with the major\ncomponents (compiler, kernel, and so on) of the operating system on\nwhich the executable runs, unless that component itself accompanies\nthe executable.\n\n  It may happen that this requirement contradicts the license\nrestrictions of other proprietary libraries that do not normally\naccompany the operating system.  Such a contradiction means you cannot\nuse both them and the Library together in an executable that you\ndistribute.\n\f\n  7. You may place library facilities that are a work based on the\nLibrary side-by-side in a single library together with other library\nfacilities not covered by this License, and distribute such a combined\nlibrary, provided that the separate distribution of the work based on\nthe Library and of the other library facilities is otherwise\npermitted, and provided that you do these two things:\n\n    a) Accompany the combined library with a copy of the same work\n    based on the Library, uncombined with any other library\n    facilities.  This must be distributed under the terms of the\n    Sections above.\n\n    b) Give prominent notice with the combined library of the fact\n    that part of it is a work based on the Library, and explaining\n    where to find the accompanying uncombined form of the same work.\n\n  8. You may not copy, modify, sublicense, link with, or distribute\nthe Library except as expressly provided under this License.  Any\nattempt otherwise to copy, modify, sublicense, link with, or\ndistribute the Library is void, and will automatically terminate your\nrights under this License.  However, parties who have received copies,\nor rights, from you under this License will not have their licenses\nterminated so long as such parties remain in full compliance.\n\n  9. You are not required to accept this License, since you have not\nsigned it.  However, nothing else grants you permission to modify or\ndistribute the Library or its derivative works.  These actions are\nprohibited by law if you do not accept this License.  Therefore, by\nmodifying or distributing the Library (or any work based on the\nLibrary), you indicate your acceptance of this License to do so, and\nall its terms and conditions for copying, distributing or modifying\nthe Library or works based on it.\n\n  10. Each time you redistribute the Library (or any work based on the\nLibrary), the recipient automatically receives a license from the\noriginal licensor to copy, distribute, link with or modify the Library\nsubject to these terms and conditions.  You may not impose any further\nrestrictions on the recipients' exercise of the rights granted herein.\nYou are not responsible for enforcing compliance by third parties with\nthis License.\n\f\n  11. If, as a consequence of a court judgment or allegation of patent\ninfringement or for any other reason (not limited to patent issues),\nconditions are imposed on you (whether by court order, agreement or\notherwise) that contradict the conditions of this License, they do not\nexcuse you from the conditions of this License.  If you cannot\ndistribute so as to satisfy simultaneously your obligations under this\nLicense and any other pertinent obligations, then as a consequence you\nmay not distribute the Library at all.  For example, if a patent\nlicense would not permit royalty-free redistribution of the Library by\nall those who receive copies directly or indirectly through you, then\nthe only way you could satisfy both it and this License would be to\nrefrain entirely from distribution of the Library.\n\nIf any portion of this section is held invalid or unenforceable under any\nparticular circumstance, the balance of the section is intended to apply,\nand the section as a whole is intended to apply in other circumstances.\n\nIt is not the purpose of this section to induce you to infringe any\npatents or other property right claims or to contest validity of any\nsuch claims; this section has the sole purpose of protecting the\nintegrity of the free software distribution system which is\nimplemented by public license practices.  Many people have made\ngenerous contributions to the wide range of software distributed\nthrough that system in reliance on consistent application of that\nsystem; it is up to the author/donor to decide if he or she is willing\nto distribute software through any other system and a licensee cannot\nimpose that choice.\n\nThis section is intended to make thoroughly clear what is believed to\nbe a consequence of the rest of this License.\n\n  12. If the distribution and/or use of the Library is restricted in\ncertain countries either by patents or by copyrighted interfaces, the\noriginal copyright holder who places the Library under this License may add\nan explicit geographical distribution limitation excluding those countries,\nso that distribution is permitted only in or among countries not thus\nexcluded.  In such case, this License incorporates the limitation as if\nwritten in the body of this License.\n\n  13. The Free Software Foundation may publish revised and/or new\nversions of the Lesser General Public License from time to time.\nSuch new versions will be similar in spirit to the present version,\nbut may differ in detail to address new problems or concerns.\n\nEach version is given a distinguishing version number.  If the Library\nspecifies a version number of this License which applies to it and\n\"any later version\", you have the option of following the terms and\nconditions either of that version or of any later version published by\nthe Free Software Foundation.  If the Library does not specify a\nlicense version number, you may choose any version ever published by\nthe Free Software Foundation.\n\f\n  14. If you wish to incorporate parts of the Library into other free\nprograms whose distribution conditions are incompatible with these,\nwrite to the author to ask for permission.  For software which is\ncopyrighted by the Free Software Foundation, write to the Free\nSoftware Foundation; we sometimes make exceptions for this.  Our\ndecision will be guided by the two goals of preserving the free status\nof all derivatives of our free software and of promoting the sharing\nand reuse of software generally.\n\n                            NO WARRANTY\n\n  15. BECAUSE THE LIBRARY IS LICENSED FREE OF CHARGE, THERE IS NO\nWARRANTY FOR THE LIBRARY, TO THE EXTENT PERMITTED BY APPLICABLE LAW.\nEXCEPT WHEN OTHERWISE STATED IN WRITING THE COPYRIGHT HOLDERS AND/OR\nOTHER PARTIES PROVIDE THE LIBRARY \"AS IS\" WITHOUT WARRANTY OF ANY\nKIND, EITHER EXPRESSED OR IMPLIED, INCLUDING, BUT NOT LIMITED TO, THE\nIMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\nPURPOSE.  THE ENTIRE RISK AS TO THE QUALITY AND PERFORMANCE OF THE\nLIBRARY IS WITH YOU.  SHOULD THE LIBRARY PROVE DEFECTIVE, YOU ASSUME\nTHE COST OF ALL NECESSARY SERVICING, REPAIR OR CORRECTION.\n\n  16. IN NO EVENT UNLESS REQUIRED BY APPLICABLE LAW OR AGREED TO IN\nWRITING WILL ANY COPYRIGHT HOLDER, OR ANY OTHER PARTY WHO MAY MODIFY\nAND/OR REDISTRIBUTE THE LIBRARY AS PERMITTED ABOVE, BE LIABLE TO YOU\nFOR DAMAGES, INCLUDING ANY GENERAL, SPECIAL, INCIDENTAL OR\nCONSEQUENTIAL DAMAGES ARISING OUT OF THE USE OR INABILITY TO USE THE\nLIBRARY (INCLUDING BUT NOT LIMITED TO LOSS OF DATA OR DATA BEING\nRENDERED INACCURATE OR LOSSES SUSTAINED BY YOU OR THIRD PARTIES OR A\nFAILURE OF THE LIBRARY TO OPERATE WITH ANY OTHER SOFTWARE), EVEN IF\nSUCH HOLDER OR OTHER PARTY HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH\nDAMAGES.\n\n                     END OF TERMS AND CONDITIONS\n\f\n           How to Apply These Terms to Your New Libraries\n\n  If you develop a new library, and you want it to be of the greatest\npossible use to the public, we recommend making it free software that\neveryone can redistribute and change.  You can do so by permitting\nredistribution under these terms (or, alternatively, under the terms of the\nordinary General Public License).\n\n  To apply these terms, attach the following notices to the library.  It is\nsafest to attach them to the start of each source file to most effectively\nconvey the exclusion of warranty; and each file should have at least the\n\"copyright\" line and a pointer to where the full notice is found.\n\n    <one line to give the library's name and a brief idea of what it does.>\n    Copyright (C) <year>  <name of author>\n\n    This library is free software; you can redistribute it and/or\n    modify it under the terms of the GNU Lesser General Public\n    License as published by the Free Software Foundation; either\n    version 2.1 of the License, or (at your option) any later version.\n\n    This library is distributed in the hope that it will be useful,\n    but WITHOUT ANY WARRANTY; without even the implied warranty of\n    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU\n    Lesser General Public License for more details.\n\n    You should have received a copy of the GNU Lesser General Public\n    License along with this library; if not, write to the Free Software\n    Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301  USA\n\nAlso add information on how to contact you by electronic and paper mail.\n\nYou should also get your employer (if you work as a programmer) or your\nschool, if any, to sign a \"copyright disclaimer\" for the library, if\nnecessary.  Here is a sample; alter the names:\n\n  Yoyodyne, Inc., hereby disclaims all copyright interest in the\n  library `Frob' (a library for tweaking knobs) written by James Random Hacker.\n\n  <signature of Ty Coon>, 1 April 1990\n  Ty Coon, President of Vice\n\nThat's all there is to it!\n"
        },
        {
          "name": "local-build-plugins",
          "type": "tree",
          "content": null
        },
        {
          "name": "migration-guide.adoc",
          "type": "blob",
          "size": 26.6845703125,
          "content": "= 7.0 Migration Guide\n:toc:\n:toclevels: 4\n:docsBase: https://docs.jboss.org/hibernate/orm\n:versionDocBase: {docsBase}/7.0\n:userGuideBase: {versionDocBase}/userguide/html_single/Hibernate_User_Guide.html\n:javadocsBase: {versionDocBase}/javadocs\n\n\nThis guide discusses migration to Hibernate ORM version 7.0. For migration from\nearlier versions, see any other pertinent migration guides as well.\n\n[[jpa-32]]\n== Jakarta Persistence 3.2\n\n7.0 migrates to Jakarta Persistence 3.2 which is fairly disruptive, mainly around:\n\n* Type parameters:\n  ** Affects much of the Criteria API - especially roots, joins, paths\n  ** Affects much of the Graph API -\n    *** `org.hibernate.graph.Graph.addAttributeNode(java.lang.String)` defines a return while\n                `jakarta.persistence.Graph.addAttributeNode(java.lang.String)` does not.\n* New JPA features colliding with previous Hibernate extension features:\n  ** `Nulls` (JPA) v. `NullPrecedence` (Hibernate), including JPA's new `Order#getNullPrecedence()` returning `Nulls`\n        colliding with Hibernate's `SqmSortSpecification#getNullPrecedence` returning `NullPrecedence`.  Hibernate's form\n        was renamed to `SqmSortSpecification#getHibernateNullPrecedence` to avoid the collision.\n  ** `SchemaManager` is now also a JPA contract exposed as `EntityManagerFactory#getSchemaManager` which leads to type issues for\n        Hibernate's `SessionFactory#getSchemaManager`.  Hibernate's `SchemaManager` now extends the new JPA `SchemaManager`.\n        But that is a bytecode incompatibility.\n  ** JPA has added support in its Graph API for things Hibernate has supported for some time.  Some of those are collisions\n        requiring changes to the Hibernate API.\n  ** `Transaction#getTimeout`.  JPA 3.2 adds `#getTimeout` but uses `Integer` whereas Hibernate has historically used `int`.  Note that this raises the possibility of a `NullPointerException` during migration if, e.g., performing direct comparisons on the timeout value against an in (auto unboxing).\n\nSee this https://in.relation.to/2024/04/01/jakarta-persistence-3/[blog post] for a good discussion of the changes in Jakarta Persistence 3.2.\n\n\n[[hibernate-models]]\n== Hibernate Models\n\nFor many years Hibernate has used the Hibernate Commons Annotations (HCANN) library for handling various low-level tasks\nrelated to understanding the structure of an application domain model, reading annotations and weaving in XML\nmapping documents.\n\nHowever, HCANN suffers from a number of limitations that continued to be problematic.  And given\nthe use of HCANN across multiple projects, doing the needed refactoring was simply not possible.\n\nThe https://github.com/hibernate/hibernate-models[Hibernate Models] project was developed to be a better alternative\nto HCANN.  Hibernate Models is essentially an abstraction over reflection (`Type`, `Class`, `Member`, ...) and\nannotations.  Check out its project page for complete details.\n\n7.0 uses Hibernate Models in place of HCANN.\n\nNOTE: Currently, the `hibernate-envers` module still uses HCANN.  That will change during continued 7.x development.\n\n\n\n[[model-validation]]\n== Domain Model Validations\n\n7.0 adds many more checks about illegal use of annotations.\n\n[[PersistentAttributeType]]\n=== PersistentAttributeType\n\nAs of 7.0, Hibernate applies much better validation of an attribute specifying multiple PersistentAttributeTypes.\nJakarta Persistence 3.2 has clarified this in the specification.  E.g., the following examples are all now illegal -\n\n[source,java]\n----\n@Basic\n@ManyToOne\nprivate Employee manager;\n----\n\nor\n\n[source,java]\n----\n@Lob\n@ManyToOne\nprivate Employee manager;\n----\n\n\n[[misplaced-annotations]]\n=== Misplaced Annotations\n\n7.0 does much more in-depth checking that annotations appear in the proper place.  While previous versions\ndid not necessarily throw errors, in most cases these annotations were simply ignored.\n\nFor example, this code now results in an error:\n\n[source,java]\n----\n@Entity\nclass Book {\n    // specifies FIELD access, properties should not be annotated\n    @Id\n    Integer id;\n\n    // previously ignored, this is an error now\n    @Column(name=\"category\")\n    String getType() { ... }\n}\n----\n\n[[id-generators]]\n=== Identifier Generators\n\nStarting in 7.0 it is no longer valid to combine `GenerationType#SEQUENCE` with anything other than\n`@SequenceGenerator` nor `GenerationType#TABLE` with anything other than `@TableGenerator`.  Previous\nversions did not validate this particularly well.\n\n\n[[java-beans]]\n=== JavaBean Conventions\n\nPrevious versions allowed some questionable (at best) attribute naming patterns.\nFor example, this property declaration is no longer allowed:\n\n[source,java]\n----\n@Basic\nString isDefault();\n----\n\n[[envers-rev-types]]\n== Hibernate Envers and custom revision entities\n\nUsers that wanted to customize the `@RevisionEntity` used by Envers could do so by extending one on the four default revision entity types:\n\n[source]\n----\norg.hibernate.envers.DefaultRevisionEntity\norg.hibernate.envers.DefaultTrackingModifiedEntitiesRevisionEntity\norg.hibernate.envers.enhanced.SequenceIdRevisionEntity\norg.hibernate.envers.enhanced.SequenceIdTrackingModifiedEntitiesRevisionEntity\n----\n\nThese types are annotated with `@MappedSuperclass` to enable this custom extension. When no custom revision entity was specified, though,\nthe same class was mapped as an entity type by Envers internals. This caused problems when dealing with the domain metamodel and static\nmetamodel aspect of these types, so we chose to create *new separate classes* annotated `@MappedSuperclass` from which revision entities,\nmeaning the default ones as well as yours, *should extend from*. These types are (in the same order):\n\n[source]\n----\norg.hibernate.envers.RevisionMapping\norg.hibernate.envers.TrackingModifiedEntitiesRevisionMapping\norg.hibernate.envers.enhanced.SequenceIdRevisionMapping\norg.hibernate.envers.enhanced.SequenceIdTrackingModifiedEntitiesRevisionMapping\n----\n\nAlso, you can now write HQL queries using the simple class name of default revision entities to retrieve all revision information.\nFind out more in link:{user-guide-url}#envers-querying-revision-info[this user guide chapter].\n\n[[create-query]]\n== Queries with implicit `select` list and no explicit result type\n\nIn previous versions, Hibernate allowed a query with no `select` list to be passed to the overload of `createQuery()` with no explicit result type parameter, for example:\n\n[source,java]\nList query =\n        session.createQuery(\"from X, Y\")\n                .getResultList()\n\nor:\n\n[source,java]\nList query =\n        session.createQuery(\"from X join y\")\n                .getResultList()\n\nThe select list was inferred based on the `from` clause.\n\nIn Hibernate 6 we decided to deprecate this overload of `createQuery()`, since:\n\n- it returns a raw type, resulting in compiler warnings in client code, and\n- the second query is truly ambiguous, with no obviously intuitive interpretation.\n\nAs of Hibernate 7, the method is remains deprecated, and potentially-ambiguous queries _are no longer accepted_.\nMigration paths include:\n\n1. explicitly specify the `select` list,\n2. add `X.class` or `Object[].class` as a second argument, to disambiguate the interpretation of the query, or\n3. in the case where the query should return exactly one entity, explicitly assign the alias `this` to that entity.\n\nFor example, the queries above may be migrated via:\n\n[source,java]\nList<Object[]> result =\n        session.createQuery(\"from X, Y\", Object[].class)\n                .getResultList()\n\nor:\n\n[source,java]\nList<X> result =\n        session.createQuery(\"from X join y\", X.class)\n                .getResultList()\n\n\n[[proxy-annotation]]\n== Replace `@Proxy`\n\nApplications will need to replace usages of the removed `@Proxy` annotation.\n\n`@Proxy#proxyClass` has no direct replacement, but was also never needed/useful.\n\nHere we focus on `@Proxy#lazy` attribute which, again, was hardly ever useful.\nBy default (true), Hibernate would proxy an entity when possible and when asked for.\n\"Asked for\" includes calls to `Session#getReference` and lazy associations.\nAll such cases though are already controllable by the application.\n\n* Instead of `Session#getReference`, use `Session#find`\n* Use eager association fetching, for example,\n** `FetchType.EAGER` (the default for to-one associations anyway), possibly combined with `@Fetch`,\n** `EntityGraph`, or a\n** `@FetchProfile`.\n\nThe effect can also often be mitigated using Hibernate's bytecode-based laziness (possibly combined with `@ConcreteProxy`).\n\n\n[[flush-persist]]\n== Session flush and persist\n\nThe removal of `CascadeType.SAVE_UPDATE` slightly changes the persist and flush behaviour to conform with Jakarta Persistence.\n\nPersisting a transient entity or flushing a manged entity with an associated detached entity having the association annotated with `cascade = CascadeType.ALL` or `cascade = CascadeType.PERSIST` throws now an `jakarta.persistence.EntityExistsException` if the detached entity has not been re-associated with the Session.\n\nTo re-associate the detached entity with the Session the `Session#merge` method can be used.\n\nConsider the following model\n\n[source,java]\n----\n@Entity\nclass Parent {\n\t...\n\n\t@OneToMany(cascade = CascadeType.ALL, mappedBy = \"parent\", orphanRemoval = true)\n\t@LazyCollection(value = LazyCollectionOption.EXTRA)\n\tprivate Set<Child> children = new HashSet<>();\n\n\tpublic void addChild(Child child) {\n\t\tchildren.add( child );\n\t\tchild.setParent( this );\n\t}\n}\n\n@Entity\nclass Child {\n\t...\n\n\t@ManyToOne\n\tprivate Parent parent;\n}\n----\n\nAssuming we have `c1` as a detached `Child`, the following code will now result in `jakarta.persistence.EntityExistsException` being thrown at flush time:\n\n[source,java]\n----\nParent parent = session.get( Parent.class, parentId );\nparent.addChild( c1 );\n----\n\nInstead, `c1` must first be re-associated with the Session using merge:\n\n\n[source,java]\n----\nParent parent = session.get( Parent.class, parentId );\nChild merged = session.merge( c1 );\nparent.addChild( merged );\n----\n\n\n[[refresh-lock-deteached]]\n== Refreshing/locking detached entities\n\nTraditionally, Hibernate allowed detached entities to be refreshed. However, Jakarta Persistence prohibits this practice and specifies that an `IllegalArgumentException` should be thrown instead. Hibernate now fully aligns with the JPA specification in this regard.\n\nAlong the same line of thought, also acquiring a lock on a detached entity is no longer allowed.\n\nTo this effect the `hibernate.allow_refresh_detached_entity`, which allowed Hibernate's legacy refresh behaviour to be invoked, has been removed.\n\n\n[[auto-cascade-persist]]\n== Cascading persistence for `@Id` and `@MapsId` fields\n\nPreviously Hibernate automatically enabled `cascade=PERSIST` for association fields annotated `@Id` or `@MapsId`.\nThis was undocumented and unexpected behavior, and arguably against the intent of the Persistence specification.\n\nExisting code which relies on this behavior should be modified by addition of explicit `cascade=PERSIST` to the association field.\n\n\n[[enum-checks]]\n== Enums and Check Constraints\n\nHibernate previously added support for generating check constraints for enums mapped using `@Enumerated`\nas part of schema generation.  7.0 adds the same capability for enums mapped using an `AttributeConverter`,\nby asking the converter to convert all the enum constants on start up.\n\n[[datetime-native]]\n== Date and time types returned by native queries\n\nIn the absence of a `@SqlResultSetMapping`, previous versions of Hibernate used `java.sql` types (`Date`, `Time`, `Timestamp`) to represent date/time types returned by a native query.\nIn 7.0, such queries return types defined by `java.time` (`LocalDate`, `LocalTime`, `LocalDateTime`) by default.\nThe previous behavior may be recovered by setting `hibernate.query.native.prefer_jdbc_datetime_types` to `true`.\n\n\n[[ddl-implicit-datatype-timestamp]]\n== Default precision for `timestamp` on some databases\n\nThe default precision for Oracle timestamps was changed to 9, i.e. nanosecond precision.\nThe default precision for SQL Server timestamps was changed to 7, i.e. 100 nanosecond precision.\n\nNote that these changes only affect DDL generation.\n\n[[array-mapping-changes-on-db2-sap-hana-sql-server-and-sybase-ase]]\n== Array mapping changes on DB2, SAP HANA, SQL Server and Sybase ASE\n\nOn DB2, SAP HANA, SQL Server and Sybase ASE, basic arrays now map to the `SqlTypes.XML_ARRAY` type code,\nwhereas previously, the dialect mapped arrays to `SqlTypes.VARBINARY`.\nThe `SqlTypes.XML_ARRAY` type uses the `xml` DDL type which enables using arrays in other features through the various XML functions.\n\nThe migration requires to read data and re-save it. Note that XML support on Sybase ASE is not enabled by default\nand requires to run `sp_configure 'enable xml', 1`.\n\nTo retain backwards compatibility, configure the setting `hibernate.type.preferred_array_jdbc_type` to `VARBINARY`.\n\n[[array-mapping-changes-on-mysql-mariadb]]\n== Array mapping changes on MySQL/MariaDB\n\nOn MySQL and MariaDB, basic arrays now map to the `SqlTypes.JSON_ARRAY` type code,\nwhereas previously, the dialect mapped arrays to `SqlTypes.VARBINARY`.\nThe `SqlTypes.JSON_ARRAY` type uses the `json` DDL type which enables using arrays in other features through the various JSON functions.\n\nThe migration requires to read data and re-save it.\n\nTo retain backwards compatibility, configure the setting `hibernate.type.preferred_array_jdbc_type` to `VARBINARY`.\n\n[[xml-format-mapper-changes]]\n== XML `FormatMapper` changes\n\nPrevious versions of Hibernate ORM used an undefined/provider-specific format for serialization/deserialization of\ncollections, maps and byte arrays to/from XML, which was not portable.\n\nXML `FormatMapper` implementations now use a portable format for collections, maps, and byte arrays.\nThis change is necessary to allow mapping basic arrays as `SqlTypes.XML_ARRAY`.\n\nThe migration requires to read data and re-save it.\n\nTo retain backwards compatibility, configure the setting `hibernate.type.xml_format_mapper.legacy_format` to `true`.\n\n[[sf-name]]\n== SessionFactory Name (and JNDI)\n\nHibernate defines `SessionFactory#getName` (specified via `cfg.xml` or  `hibernate.session_factory_name`) which is used to\nhelp with (de)serializing a `SessionFactory`.  It is also, unless `hibernate.session_factory_name_is_jndi` is set to `false`,\nused in biding the `SessionFactory` into JNDI.\n\nThis `SessionFactory#getName` method pre-dates Jakarta Persistence (and JPA).  It now implements `EntityManagerFactory#getName`\ninherited from Jakarta Persistence, which states that this name should come from the persistence-unit name.\nTo align with Jakarta Persistence (the 3.2 TCK tests this), Hibernate now considers the persistence-unit name if no\n`hibernate.session_factory_name` is specified.\n\nHowever, because `hibernate.session_factory_name` is also a trigger to attempt to bind the SessionFactory into JNDI,\nthis change to consider persistence-unit name, means that each `SessionFactory` created through Jakarta Persistence now\nhas a name and Hibernate attempts to bind it to JNDI.\n\nTo work around this we have introduced a new `hibernate.session_factory_jndi_name` setting that can be used to explicitly\nspecify a name for JNDI binding.  The new behavior is as follows (assuming `hibernate.session_factory_name_is_jndi` is not explicitly configured):\n\n* If `hibernate.session_factory_jndi_name` is specified, the name is used to bind into JNDI\n* If `hibernate.session_factory_name` is specified, the name is used to bind into JNDI\n\nHibernate can use the persistence-unit name for binding into JNDI as well, but `hibernate.session_factory_name_is_jndi`\nmust be explicitly set to true.\n\n[[configurable-generators]]\n== Configurable generators\n\nThe signature of the `Configurable#configure` method changed from accepting just a `ServiceRegistry` instance to the new `GeneratorCreationContext` interface, which exposes a lot more useful information when configuring the generator itself. The old signature has been deprecated for removal, so you should migrate any custom `Configurable` generator implementation to the new one.\n\n[[stateless-session-cache]]\n== `StatelessSession` and second-level cache\n\nPreviously, stateless sessions never interacted with the second-level cache.\nThis reflected their original intended role in bulk processing.\nWith the advent of Jakarta Data and Hibernate Data Repositories, the responsibilities of `StatelessSession` have now expanded, and this behavior is no longer appropriate.\n\nThus, a stateless session now makes use of the second-level cache by default.\nTo completely bypass the second-level cache, recovering the previous behavior, call `setCacheMode(CacheMode.IGNORE)`.\n\nIt's often important to explicitly disable puts to the second-level cache in code which performs bulk processing.\nSet the cache mode to `GET` or configure `jakarta.persistence.cache.storeMode` to `BYPASS`.\n\n[[stateless-session-jdbc-batching]]\n== JDBC batching with `StatelessSession`\n\nAutomatic JDBC batching has the side effect of delaying the execution of the batched operation, and this undermines the synchronous nature of operations performed through a stateless session.\nIn Hibernate 7, the configuration property `hibernate.jdbc.batch_size` now has no effect on a stateless session.\nAutomatic batching may be enabled by explicitly calling `setJdbcBatchSize()`.\nHowever, the preferred approach is to explicitly batch operations via `insertMultiple()`, `updateMultiple()`, or `deleteMultiple()`.\n\n[[criteria-implicit-treat]]\n== Criteria API and inheritance subtypes attributes\n\nIt was previously possible to use the string version of the `jakarta.persistence.criteria.Path#get` and `jakarta.persistence.criteria.From#join` methods with names of attributes defined in an inheritance subtype of the type represented by the path expression. This was handled internally by implicitly treating the path as the subtype which defines said attribute. Since Hibernate 7.0, aligning with the JPA specification, the Criteria API will no longer allow retrieving subtype attributes this way, and it's going to require an explicit `jakarta.persistence.criteria.CriteriaBuilder#treat` to be called on the path first to downcast it to the subtype which defines the attribute.\n\nImplicit treats are still going to be applied when an HQL query dereferences a path belonging to an inheritance subtype.\n\n[[hbm-transform]]\n== hbm.xml Transformation\n\nHibernate's legacy `hbm.xml` mapping schema has been deprecated for quite some time, replaced by a new `mapping.xml`\nschema.  In 7.0, this `mapping.xml` is stabilized and we now offer a transformation of `hbm.xml` files into `mapping.xml` files.\n\nThis tool is available as both -\n\n* build-time transformation (currently only offered as a Gradle plugin)\n* run-time transformation, using `hibernate.transform_hbm_xml.enabled=true`\n\nBuild-time transformation is preferred.\n\n[NOTE]\n====\nInitial versions of the transformation processed one file at a time.\nThis is now done across the entire set of `hbm.xml` files at once.\nWhile most users will never see this change, it might impact integrations which tie-in to XML processing.\n====\n\n[[mysql-varchar]]\n== Default DDL type for `char` and `Character` on MySQL\n\nPreviously, `char` and `Character` fields were, by default, mapped to `char(1)` columns by the schema export tool.\nHowever, MySQL treats a `char(1)` containing a single space as an empty string, resulting in broken behavior for some HQL and SQL functions.\nNow, `varchar(1)` is used by default.\n\n[[unowned-order-column]]\n== `@OrderColumn` in unowned `@OneToMany` associations\n\nIn an unowned (`mappedBy`) one-to-many association, an `@OrderColumn` should, in principle, also be mapped by a field of the associated entity, and the value of the order column should be determined by the value of this field, not by the position in the list.\n\nPreviously, since version 4.1, https://hibernate.atlassian.net/issues/HHH-18830[Hibernate would issue superfluous SQL `UPDATE` statements] to set the value of the order column based on the state of the unowned collection.\nThis was incorrect according to the JPA specification, and inconsistent with the natural semantics of Hibernate.\n\nIn Hibernate 7, these SQL `UPDATE` statements only occur if the `@OrderColumn` is _not_ also mapped by a field of the entity.\n\n[[pools]]\n== Connection pools\n\nSince Vibur and Proxool are no longer actively developed, support for these connection pools was removed.\nUse Agroal or HikariCP instead.\n\n== Converters incompatible with annotations\n\nJPA ``AttributeConverter``s are incompatible with the annotations `@Id`, `@Version`, `@Enumerated`, `@Embedded`, `@Temporal` and all association-mapping annotations.\nPreviously, any converter applied to an attribute with an incompatible annotation was simply ignored.\nHibernate now reports an error in this situation.\n\nThis includes auto-applied converters.\nTo suppress the error for an auto-applied converter, use `@Convert(disableConversion=true)`.\n\n== `org.hibernate.graph` package\n\nThe `EntityGraph` API was enhanced in JPA 3.2, and made much more useful.\nThe incubating package `org.hibernate.graph` contains extensions to that API, which have been significantly impacted by the migration to JPA 3.2, and by the additional of new functionality.\nFurthermore, some legacy operations were declared with incorrect generic type signatures (by both JPA, and by Hibernate).\n\nThis package has been significantly re-engineered, and the impact of this effort includes:\n\n- some breaking changes to type signatures, and\n- a number of deprecations of legacy operations which are now covered by JPA.\n\nAlso, a key subgraph now always refers to a `Map` key, and never to an entity id.\n\nWe encourage migration to the use of the new JPA-standard operations.\n\n== Deprecations\n\n* `@Comment` is deprecated in favor of the JPA 3.2 `comment` members\n* `@Comment` is deprecated in favor of the JPA 3.2 `@CheckConstraint` and `check` members\n* `NullPrecedence` is deprecated in favor of JPA 3.2 `Nulls`\n* `@FractionalSeconds` is deprecated in favor of JPA 3.2 `secondPrecision`\n* `DynamicParameterizedType` is deprecated\n* `Session.get()` is deprecated in favor of `find()`\n* `@Cascade` and `org.hibernate.annotations.CascadeType` are deprecated in favor of JPA `cascade` and `CascadeType`\n* `org.hibernate.annotations.FlushModeType` is deprecated in favor of `QueryFlushMode`\n\n[[cleanup]]\n== Cleanup\n\n* Annotations\n** Removed `@Persister`\n** Removed `@Proxy` -- see <<proxy-annotation>>\n** Removed `@SelectBeforeUpdate`\n** Removed `@DynamicInsert#value` and `@DynamicUpdate#value`\n** Removed `@Loader`\n** Removed `@Table` -> use JPA `@Table`\n** Removed `@Where` and `@WhereJoinTable` -> use `@SQLRestriction` or `@SQLJoinTableRestriction`\n** Removed `@OrderBy` -> use `@SQLOrder` or JPA `@OrderBy`\n** Removed `@ForeignKey` -> use JPA `@ForeignKey`\n** Removed `@Index` -> use JPA `@Index`\n** Removed `@IndexColumn` -> use JPA `@OrderColumn`\n** Removed `@GeneratorType` (and `GenerationTime`, etc)\n** Removed `@LazyToOne`\n** Removed `@LazyCollection`\n** Replaced uses of `CacheModeType` with `CacheMode`\n** Removed `@TestForIssue` (for testing purposes) -> use `org.hibernate.testing.orm.junit.JiraKey` and `org.hibernate.testing.orm.junit.JiraKeyGroup`\n** Removed `@Cache.include` -> use `@Cache.includeLazy`\n\n* Classes/interfaces\n** Removed `SqmQualifiedJoin` (all joins are qualified)\n** Removed `AdditionalJaxbMappingProducer` -> `AdditionalMappingContributor`\n** Removed `MetadataContributor` -> `AdditionalMappingContributor`\n** Removed `EmptyInterceptor` -> implement `org.hibernate.Interceptor` directly\n\n* Behavior\n** Removed `org.hibernate.Session#save` in favor of `org.hibernate.Session#persist`\n** Removed `org.hibernate.Session#saveOrUpdate` in favor `#persist` if the entity is transient or `#merge` if the entity is detached.\n** Removed `org.hibernate.Session#update` in favor of `org.hibernate.Session.merge`\n** Removed `org.hibernate.annotations.CascadeType.SAVE_UPDATE` in favor of `org.hibernate.annotations.CascadeType.PERSIST` + `org.hibernate.annotations.CascadeType.MERGE`\n** Removed `org.hibernate.Session#delete` in favor of `org.hibernate.Session#remove`\n** Removed `org.hibernate.annotations.CascadeType.DELETE` in favor of `org.hibernate.annotations.CascadeType#REMOVE`\n** Removed `org.hibernate.Session#refresh(String entityName, Object object)` in favor of `org.hibernate.Session#refresh(Object object)`\n** Removed `org.hibernate.Session#refresh(String entityName, Object object, LockOptions lockOptions)` in favor of `org.hibernate.Session#refresh(Object object, LockOptions lockOptions)`\n** Removed `org.hibernate.integrator.spi.Integrator#integrate(Metadata,SessionFactoryImplementor,SessionFactoryServiceRegistry)` in favor of `org.hibernate.integrator.spi.Integrator#integrate(Metadata,BootstrapContext,SessionFactoryImplementor)`\n** Removed `org.hibernate.Interceptor#onLoad(Object, Serializable, Object[] , String[] , Type[] )` in favour of `org.hibernate.Interceptor#onLoad(Object, Object, Object[], String[], Type[] )`\n** Removed `org.hibernate.Interceptor#onFlushDirty(Object, Serializable, Object[] , Object[], String[] , Type[] )` in favour of `org.hibernate.Interceptor#onLoad(Object, Object, Object[], Object[], String[] , Type[] )`\n** Removed `org.hibernate.Interceptor#onSave(Object, Serializable, Object[], String[], Type[])` in favour of `org.hibernate.Interceptor#onSave(Object, Object, Object[], String[], Type[])`\n** Removed `org.hibernate.Interceptor#onDelete(Object, Serializable, Object[], String[], Type[])` in favour of `org.hibernate.Interceptor#onDelete(Object, Serializable, Object[], String[], Type[])`\n** Removed `org.hibernate.Interceptor#onCollectionRecreate(Object, Serializable)` in favour of `org.hibernate.Interceptor#onCollectionRecreate(Object, Object)`\n** Removed `org.hibernate.Interceptor#onCollectionRemove(Object, Serializable)` in favour of `org.hibernate.Interceptor#onCollectionRemove(Object, Object)`\n** Removed `org.hibernate.Interceptor#onCollectionUpdate(Object, Serializable)` in favour of `org.hibernate.Interceptor#onCollectionUpdate(Object, Object)`\n** Removed `org.hibernate.Interceptor#findDirty(Object, Serializable, Object[], Object[], String[], Type[])` in favour of `org.hibernate.Interceptor#findDirty(Object, Object, Object[], Object[], String[], Type[])`\n** Removed `org.hibernate.Interceptor#getEntity(String, Serializable)` in favour of `org.hibernate.Interceptor#getEntity(String, Serializable)`\n** Removed `org.hibernate.metamodel.spi.MetamodelImplementor` in favor of `org.hibernate.metamodela.MappingMetmodel` or `org.hibernate.metamodel.model.domain.JpaMetamodel`\n** Removed `org.hibernate.Metamodel` in favor of `org.hibernate.metamodel.model.domain.JpaMetamodel`\n** Removed `NaturalIdLoadAccess.using(Map)` and `NaturalIdMultiLoadAccess.compoundValue()` in favor of `Map.of()`\n\n* Settings\n** Removed `hibernate.mapping.precedence` and friends\n** Removed `hibernate.allow_refresh_detached_entity`\n\n\n[[reorg]]\n== Reorganize Packages (for api/spi/internal, etc)\n\n* Re-organized the `org.hibernate.query.results` package\n\n\n[[todo]]\n== Todos (dev)\n\n* Look for `todo (jpa 3.2)` comments\n* Look for `todo (7.0)` comments"
        },
        {
          "name": "nightly.Jenkinsfile",
          "type": "blob",
          "size": 15.9638671875,
          "content": "/*\n * Hibernate, Relational Persistence for Idiomatic Java\n *\n * License: GNU Lesser General Public License (LGPL), version 2.1 or later.\n * See the lgpl.txt file in the root directory or <http://www.gnu.org/licenses/lgpl-2.1.html>.\n */\n\nimport groovy.transform.Field\nimport io.jenkins.blueocean.rest.impl.pipeline.PipelineNodeGraphVisitor\nimport io.jenkins.blueocean.rest.impl.pipeline.FlowNodeWrapper\nimport org.jenkinsci.plugins.workflow.support.steps.build.RunWrapper\n\n/*\n * See https://github.com/hibernate/hibernate-jenkins-pipeline-helpers\n */\n@Library('hibernate-jenkins-pipeline-helpers') _\nimport org.hibernate.jenkins.pipeline.helpers.job.JobHelper\n\n@Field final String DEFAULT_JDK_VERSION = '17'\n@Field final String DEFAULT_JDK_TOOL = \"OpenJDK ${DEFAULT_JDK_VERSION} Latest\"\n@Field final String NODE_PATTERN_BASE = 'Worker&&Containers'\n@Field List<BuildEnvironment> environments\n\nthis.helper = new JobHelper(this)\n\nhelper.runWithNotification {\nstage('Configure') {\n\tthis.environments = [\n\t\t// Minimum supported versions\n\t\tnew BuildEnvironment( dbName: 'hsqldb_2_6' ),\n\t\tnew BuildEnvironment( dbName: 'mysql_8_0' ),\n\t\tnew BuildEnvironment( dbName: 'mariadb_10_5' ),\n\t\tnew BuildEnvironment( dbName: 'postgresql_12' ),\n\t\tnew BuildEnvironment( dbName: 'edb_12' ),\n\t\tnew BuildEnvironment( dbName: 'db2_10_5', longRunning: true ),\n\t\tnew BuildEnvironment( dbName: 'mssql_2017' ), // Unfortunately there is no SQL Server 2008 image, so we have to test with 2017\n// \t\tnew BuildEnvironment( dbName: 'sybase_16' ), // There only is a Sybase ASE 16 image, so no pint in testing that nightly\n\t\tnew BuildEnvironment( dbName: 'sybase_jconn' ),\n\t\t// Long running databases\n\t\tnew BuildEnvironment( dbName: 'cockroachdb', node: 'cockroachdb', longRunning: true ),\n\t\tnew BuildEnvironment( dbName: 'hana_cloud', dbLockableResource: 'hana-cloud', dbLockResourceAsHost: true )\n\t];\n\n\thelper.configure {\n\t\tfile 'job-configuration.yaml'\n\t\t// We don't require the following, but the build helper plugin apparently does\n\t\tjdk {\n\t\t\tdefaultTool DEFAULT_JDK_TOOL\n\t\t}\n\t\tmaven {\n\t\t\tdefaultTool 'Apache Maven 3.8'\n\t\t}\n\t}\n\tproperties([\n\t\t\tbuildDiscarder(\n\t\t\t\t\tlogRotator(daysToKeepStr: '30', numToKeepStr: '10')\n\t\t\t),\n\t\t\trateLimitBuilds(throttle: [count: 1, durationName: 'day', userBoost: true]),\n\t\t\t// If two builds are about the same branch or pull request,\n\t\t\t// the older one will be aborted when the newer one starts.\n\t\t\tdisableConcurrentBuilds(abortPrevious: true),\n\t\t\thelper.generateNotificationProperty()\n\t])\n}\n\n// Avoid running the pipeline on branch indexing\nif (currentBuild.getBuildCauses().toString().contains('BranchIndexingCause')) {\n  \tprint \"INFO: Build skipped due to trigger being Branch Indexing\"\n\tcurrentBuild.result = 'NOT_BUILT'\n  \treturn\n}\n\nstage('Build') {\n\tMap<String, Closure> executions = [:]\n\tMap<String, Map<String, String>> state = [:]\n\tenvironments.each { BuildEnvironment buildEnv ->\n\t\t// Don't build environments for newer JDKs when this is a PR\n\t\tif ( helper.scmSource.pullRequest && buildEnv.testJdkVersion ) {\n\t\t\treturn\n\t\t}\n\t\tstate[buildEnv.tag] = [:]\n\t\texecutions.put(buildEnv.tag, {\n\t\t\trunBuildOnNode(buildEnv.node ?: NODE_PATTERN_BASE) {\n\t\t\t\tdef testJavaHome\n\t\t\t\tif ( buildEnv.testJdkVersion ) {\n\t\t\t\t\ttestJavaHome = tool(name: \"OpenJDK ${buildEnv.testJdkVersion} Latest\", type: 'jdk')\n\t\t\t\t}\n\t\t\t\tdef javaHome = tool(name: DEFAULT_JDK_TOOL, type: 'jdk')\n\t\t\t\t// Use withEnv instead of setting env directly, as that is global!\n\t\t\t\t// See https://github.com/jenkinsci/pipeline-plugin/blob/master/TUTORIAL.md\n\t\t\t\twithEnv([\"JAVA_HOME=${javaHome}\", \"PATH+JAVA=${javaHome}/bin\"]) {\n\t\t\t\t\tstate[buildEnv.tag]['additionalOptions'] = ''\n\t\t\t\t\tif ( testJavaHome ) {\n\t\t\t\t\t\tstate[buildEnv.tag]['additionalOptions'] = state[buildEnv.tag]['additionalOptions'] +\n\t\t\t\t\t\t\t\t\" -Ptest.jdk.version=${buildEnv.testJdkVersion} -Porg.gradle.java.installations.paths=${javaHome},${testJavaHome}\"\n\t\t\t\t\t}\n\t\t\t\t\tif ( buildEnv.testJdkLauncherArgs ) {\n\t\t\t\t\t\tstate[buildEnv.tag]['additionalOptions'] = state[buildEnv.tag]['additionalOptions'] +\n\t\t\t\t\t\t\t\t\" -Ptest.jdk.launcher.args=${buildEnv.testJdkLauncherArgs}\"\n\t\t\t\t\t}\n\t\t\t\t\tstate[buildEnv.tag]['containerName'] = null;\n\t\t\t\t\tstage('Checkout') {\n\t\t\t\t\t\tcheckout scm\n\t\t\t\t\t}\n\t\t\t\t\ttryFinally({\n\t\t\t\t\t\tstage('Start database') {\n\t\t\t\t\t\t\tswitch (buildEnv.dbName) {\n\t\t\t\t\t\t\t\tcase \"hsqldb_2_6\":\n\t\t\t\t\t\t\t\t\tstate[buildEnv.tag]['additionalOptions'] = state[buildEnv.tag]['additionalOptions'] +\n\t\t\t\t\t\t\t\t\t\t\" -Pgradle.libs.versions.hsqldb=2.6.1\"\n\t\t\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\t\t\tcase \"mysql_8_0\":\n\t\t\t\t\t\t\t\t\tdocker.withRegistry('https://index.docker.io/v1/', 'hibernateci.hub.docker.com') {\n\t\t\t\t\t\t\t\t\t\tdocker.image('mysql:8.0.31').pull()\n\t\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t\t\tsh \"./docker_db.sh mysql_8_0\"\n\t\t\t\t\t\t\t\t\tstate[buildEnv.tag]['containerName'] = \"mysql\"\n\t\t\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\t\t\tcase \"mariadb_10_5\":\n\t\t\t\t\t\t\t\t\tdocker.withRegistry('https://index.docker.io/v1/', 'hibernateci.hub.docker.com') {\n\t\t\t\t\t\t\t\t\t\tdocker.image('mariadb:10.5.25').pull()\n\t\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t\t\tsh \"./docker_db.sh mariadb_10_5\"\n\t\t\t\t\t\t\t\t\tstate[buildEnv.tag]['containerName'] = \"mariadb\"\n\t\t\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\t\t\tcase \"postgresql_12\":\n\t\t\t\t\t\t\t\t\t// use the postgis image to enable the PGSQL GIS (spatial) extension\n\t\t\t\t\t\t\t\t\tdocker.withRegistry('https://index.docker.io/v1/', 'hibernateci.hub.docker.com') {\n\t\t\t\t\t\t\t\t\t\tdocker.image('postgis/postgis:12-3.4').pull()\n\t\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t\t\tsh \"./docker_db.sh postgresql_12\"\n\t\t\t\t\t\t\t\t\tstate[buildEnv.tag]['containerName'] = \"postgres\"\n\t\t\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\t\t\tcase \"edb_12\":\n\t\t\t\t\t\t\t\t\tdocker.image('quay.io/enterprisedb/edb-postgres-advanced:12.16-3.3-postgis').pull()\n\t\t\t\t\t\t\t\t\tsh \"./docker_db.sh edb_12\"\n\t\t\t\t\t\t\t\t\tstate[buildEnv.tag]['containerName'] = \"edb\"\n\t\t\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\t\t\tcase \"db2_10_5\":\n\t\t\t\t\t\t\t\t\tdocker.withRegistry('https://quay.io', 'hibernate.quay.io') {\n\t\t\t\t\t\t\t\t\t\tdocker.image('hibernate/db2express-c@sha256:a499afd9709a1f69fb41703e88def9869955234c3525547e2efc3418d1f4ca2b').pull()\n\t\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t\t\tsh \"./docker_db.sh db2_10_5\"\n\t\t\t\t\t\t\t\t\tstate[buildEnv.tag]['containerName'] = \"db2\"\n\t\t\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\t\t\tcase \"mssql_2017\":\n\t\t\t\t\t\t\t\t\tdocker.image('mcr.microsoft.com/mssql/server@sha256:7d194c54e34cb63bca083542369485c8f4141596805611e84d8c8bab2339eede').pull()\n\t\t\t\t\t\t\t\t\tsh \"./docker_db.sh mssql_2017\"\n\t\t\t\t\t\t\t\t\tstate[buildEnv.tag]['containerName'] = \"mssql\"\n\t\t\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\t\t\tcase \"sybase_jconn\":\n\t\t\t\t\t\t\t\t\tdocker.withRegistry('https://index.docker.io/v1/', 'hibernateci.hub.docker.com') {\n\t\t\t\t\t\t\t\t\t\tdocker.image('nguoianphu/docker-sybase').pull()\n\t\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t\t\tsh \"./docker_db.sh sybase\"\n\t\t\t\t\t\t\t\t\tstate[buildEnv.tag]['containerName'] = \"sybase\"\n\t\t\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\t\t\tcase \"cockroachdb\":\n\t\t\t\t\t\t\t\t\tdocker.withRegistry('https://index.docker.io/v1/', 'hibernateci.hub.docker.com') {\n\t\t\t\t\t\t\t\t\t\tdocker.image('cockroachdb/cockroach:v23.1.12').pull()\n\t\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t\t\tsh \"./docker_db.sh cockroachdb\"\n\t\t\t\t\t\t\t\t\tstate[buildEnv.tag]['containerName'] = \"cockroach\"\n\t\t\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t}\n\t\t\t\t\t\tstage('Test') {\n\t\t\t\t\t\t\tString args = \"${buildEnv.additionalOptions ?: ''} ${state[buildEnv.tag]['additionalOptions'] ?: ''}\"\n\t\t\t\t\t\t\twithEnv([\"RDBMS=${buildEnv.dbName}\"]) {\n\t\t\t\t\t\t\t\ttryFinally({\n\t\t\t\t\t\t\t\t\tif (buildEnv.dbLockableResource == null) {\n\t\t\t\t\t\t\t\t\t\twithCredentials([file(credentialsId: 'sybase-jconnect-driver', variable: 'jconnect_driver')]) {\n\t\t\t\t\t\t\t\t\t\t\tsh 'cp -f $jconnect_driver ./drivers/jconn4.jar'\n\t\t\t\t\t\t\t\t\t\t\ttimeout( [time: buildEnv.longRunning ? 480 : 120, unit: 'MINUTES'] ) {\n\t\t\t\t\t\t\t\t\t\t\t\tciBuild buildEnv, args\n\t\t\t\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t\t\telse {\n\t\t\t\t\t\t\t\t\t\tlock(label: buildEnv.dbLockableResource, quantity: 1, variable: 'LOCKED_RESOURCE') {\n\t\t\t\t\t\t\t\t\t\t\tif ( buildEnv.dbLockResourceAsHost ) {\n\t\t\t\t\t\t\t\t\t\t\t\targs += \" -DdbHost=${LOCKED_RESOURCE}\"\n\t\t\t\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t\t\t\t\ttimeout( [time: buildEnv.longRunning ? 480 : 120, unit: 'MINUTES'] ) {\n\t\t\t\t\t\t\t\t\t\t\t\tciBuild buildEnv, args\n\t\t\t\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t\t}, { // Finally\n\t\t\t\t\t\t\t\t\tjunit '**/target/test-results/test/*.xml,**/target/test-results/testKitTest/*.xml'\n\t\t\t\t\t\t\t\t})\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t}\n\t\t\t\t\t}, { // Finally\n\t\t\t\t\t\tif ( state[buildEnv.tag]['containerName'] != null ) {\n\t\t\t\t\t\t\tsh \"docker rm -f ${state[buildEnv.tag]['containerName']}\"\n\t\t\t\t\t\t}\n\t\t\t\t\t\t// Skip this for PRs\n\t\t\t\t\t\tif ( !env.CHANGE_ID && buildEnv.notificationRecipients != null ) {\n\t\t\t\t\t\t\thandleNotifications(currentBuild, buildEnv)\n\t\t\t\t\t\t}\n\t\t\t\t\t})\n\t\t\t\t}\n\t\t\t}\n\t\t})\n\t}\n\t// Don't run additional checks when this is a PR\n\tif ( !helper.scmSource.pullRequest ) {\n\t\texecutions.put('Reproducible build check', {\n\t\t\trunBuildOnNode(NODE_PATTERN_BASE) {\n\t\t\t\tdef javaHome = tool(name: DEFAULT_JDK_TOOL, type: 'jdk')\n\t\t\t\t// Use withEnv instead of setting env directly, as that is global!\n\t\t\t\t// See https://github.com/jenkinsci/pipeline-plugin/blob/master/TUTORIAL.md\n\t\t\t\twithEnv([\"JAVA_HOME=${javaHome}\", \"PATH+JAVA=${javaHome}/bin\"]) {\n\t\t\t\t\tstage('Checkout') {\n\t\t\t\t\t\tcheckout scm\n\t\t\t\t\t}\n\t\t\t\t\tstage('Test') {\n\t\t\t\t\t\twithGradle {\n\t\t\t\t\t\t\tdef tempDir = pwd(tmp: true)\n\t\t\t\t\t\t\tdef repo1 = tempDir + '/repo1'\n\t\t\t\t\t\t\tdef repo2 = tempDir + '/repo2'\n\t\t\t\t\t\t\t// build Hibernate ORM two times without any cache and \"publish\" the resulting artifacts to different maven repositories\n\t\t\t\t\t\t\t// so that we can compare them afterwards:\n\t\t\t\t\t\t\tsh \"./gradlew --no-daemon clean publishToMavenLocal --no-build-cache -Dmaven.repo.local=${repo1}\"\n\t\t\t\t\t\t\tsh \"./gradlew --no-daemon clean publishToMavenLocal --no-build-cache -Dmaven.repo.local=${repo2}\"\n\n\t\t\t\t\t\t\tsh \"sh ci/compare-build-results.sh ${repo1} ${repo2}\"\n\t\t\t\t\t\t\tsh \"cat .buildcompare\"\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t})\n\t\texecutions.put('Strict JAXP configuration', {\n\t\t\trunBuildOnNode(NODE_PATTERN_BASE) {\n\t\t\t\t// we want to test with JDK 23 where the strict settings were introduced\n\t\t\t\tdef testJavaHome = tool(name: \"OpenJDK 23 Latest\", type: 'jdk')\n\t\t\t\tdef javaHome = tool(name: DEFAULT_JDK_TOOL, type: 'jdk')\n\t\t\t\t// Use withEnv instead of setting env directly, as that is global!\n\t\t\t\t// See https://github.com/jenkinsci/pipeline-plugin/blob/master/TUTORIAL.md\n\t\t\t\twithEnv([\"JAVA_HOME=${javaHome}\", \"PATH+JAVA=${javaHome}/bin\"]) {\n\t\t\t\t\tstage('Checkout') {\n\t\t\t\t\t\tcheckout scm\n\t\t\t\t\t}\n\t\t\t\t\tstage('Test') {\n\t\t\t\t\t\twithGradle {\n\t\t\t\t\t\t\tdef tempDir = pwd(tmp: true)\n\t\t\t\t\t\t\tdef jaxpStrictProperties = tempDir + '/jaxp-strict.properties'\n\t\t\t\t\t\t\tdef jaxpStrictTemplate = testJavaHome + '/conf/jaxp-strict.properties.template'\n\n\t\t\t\t\t\t\techo 'Copy strict JAXP configuration properties.'\n\t\t\t\t\t\t\tsh \"cp $jaxpStrictTemplate $jaxpStrictProperties\"\n\n\t\t\t\t\t\t\t// explicitly calling toString here to prevent Jenkins failures like:\n\t\t\t\t\t\t\t//  > Scripts not permitted to use method groovy.lang.GroovyObject invokeMethod java.lang.String java.lang.Object (org.codehaus.groovy.runtime.GStringImpl positive)\n\t\t\t\t\t\t\tString args = (\"-Ptest.jdk.version=23 -Porg.gradle.java.installations.paths=${javaHome},${testJavaHome}\"\n\t\t\t\t\t\t\t\t+ \" -Ptest.jdk.launcher.args=\\\"-Djava.xml.config.file=${jaxpStrictProperties}\\\"\").toString()\n\n\t\t\t\t\t\t\ttimeout( [time: 60, unit: 'MINUTES'] ) {\n\t\t\t\t\t\t\t\tciBuild(args)\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t})\n\t}\n\tparallel(executions)\n}\n\n} // End of helper.runWithNotification\n\n// Job-specific helpers\n\nclass BuildEnvironment {\n\tString testJdkVersion\n\tString testJdkLauncherArgs\n\tString dbName = 'h2'\n\tString node\n\tString dbLockableResource\n\tboolean dbLockResourceAsHost\n\tString additionalOptions\n\tString notificationRecipients\n\tboolean longRunning\n\n\tString toString() { getTag() }\n\tString getTag() { \"${node ? node + \"_\" : ''}${testJdkVersion ? 'jdk_' + testJdkVersion + '_' : '' }${dbName}\" }\n}\n\nvoid runBuildOnNode(String label, Closure body) {\n\tnode( label ) {\n\t\tpruneDockerContainers()\n    tryFinally(body, {\n      // If this is a PR, we clean the workspace at the end\n      if ( env.CHANGE_BRANCH != null ) {\n        cleanWs()\n      }\n      pruneDockerContainers()\n    })\n\t}\n}\n\nvoid ciBuild(buildEnv, String args) {\n  // On untrusted nodes, we use the same access key as for PRs:\n  // it has limited access, essentially it can only push build scans.\n  def develocityCredentialsId = buildEnv.node ? 'ge.hibernate.org-access-key-pr' : 'ge.hibernate.org-access-key'\n\n  ciBuild(develocityCredentialsId, args)\n}\n\nvoid ciBuild(String args) {\n  ciBuild('ge.hibernate.org-access-key-pr', args)\n}\n\nvoid ciBuild(String develocityCredentialsId, String args) {\n  withCredentials([string(credentialsId: develocityCredentialsId,\n      variable: 'DEVELOCITY_ACCESS_KEY')]) {\n    withGradle { // withDevelocity, actually: https://plugins.jenkins.io/gradle/#plugin-content-capturing-build-scans-from-jenkins-pipeline\n      sh \"./ci/build.sh $args\"\n    }\n  }\n}\n\nvoid pruneDockerContainers() {\n\tif ( !sh( script: 'command -v docker || true', returnStdout: true ).trim().isEmpty() ) {\n\t\tsh 'docker container prune -f || true'\n\t\tsh 'docker image prune -f || true'\n\t\tsh 'docker network prune -f || true'\n\t\tsh 'docker volume prune -f || true'\n\t}\n}\n\nvoid handleNotifications(currentBuild, buildEnv) {\n\tdef currentResult = getParallelResult(currentBuild, buildEnv.tag)\n\tboolean success = currentResult == 'SUCCESS' || currentResult == 'UNKNOWN'\n\tdef previousResult = currentBuild.previousBuild == null ? null : getParallelResult(currentBuild.previousBuild, buildEnv.tag)\n\n\t// Ignore success after success\n\tif ( !( success && previousResult == 'SUCCESS' ) ) {\n\t\tdef subject\n\t\tdef body\n\t\tif ( success ) {\n\t\t\tif ( previousResult != 'SUCCESS' && previousResult != null ) {\n\t\t\t\tsubject = \"${env.JOB_NAME} - Build ${env.BUILD_NUMBER} - Fixed\"\n\t\t\t\tbody = \"\"\"<p>${env.JOB_NAME} - Build ${env.BUILD_NUMBER} - Fixed:</p>\n\t\t\t\t\t<p>Check console output at <a href='${env.BUILD_URL}'>${env.BUILD_URL}</a> to view the results.</p>\"\"\"\n\t\t\t}\n\t\t\telse {\n\t\t\t\tsubject = \"${env.JOB_NAME} - Build ${env.BUILD_NUMBER} - Success\"\n\t\t\t\tbody = \"\"\"<p>${env.JOB_NAME} - Build ${env.BUILD_NUMBER} - Success:</p>\n\t\t\t\t\t<p>Check console output at <a href='${env.BUILD_URL}'>${env.BUILD_URL}</a> to view the results.</p>\"\"\"\n\t\t\t}\n\t\t}\n\t\telse if (currentBuild.rawBuild.getActions(jenkins.model.InterruptedBuildAction.class).isEmpty()) {\n\t\t\t// If there are interrupted build actions, this means the build was cancelled, probably superseded\n\t\t\t// Thanks to https://issues.jenkins.io/browse/JENKINS-43339 for the \"hack\" to determine this\n\t\t\tif ( currentResult == 'FAILURE' ) {\n\t\t\t\tif ( previousResult != null && previousResult == \"FAILURE\" ) {\n\t\t\t\t\tsubject = \"${env.JOB_NAME} - Build ${env.BUILD_NUMBER} - Still failing\"\n\t\t\t\t\tbody = \"\"\"<p>${env.JOB_NAME} - Build ${env.BUILD_NUMBER} - Still failing:</p>\n\t\t\t\t\t\t<p>Check console output at <a href='${env.BUILD_URL}'>${env.BUILD_URL}</a> to view the results.</p>\"\"\"\n\t\t\t\t}\n\t\t\t\telse {\n\t\t\t\t\tsubject = \"${env.JOB_NAME} - Build ${env.BUILD_NUMBER} - Failure\"\n\t\t\t\t\tbody = \"\"\"<p>${env.JOB_NAME} - Build ${env.BUILD_NUMBER} - Failure:</p>\n\t\t\t\t\t\t<p>Check console output at <a href='${env.BUILD_URL}'>${env.BUILD_URL}</a> to view the results.</p>\"\"\"\n\t\t\t\t}\n\t\t\t}\n\t\t\telse {\n\t\t\t\tsubject = \"${env.JOB_NAME} - Build ${env.BUILD_NUMBER} - ${currentResult}\"\n\t\t\t\tbody = \"\"\"<p>${env.JOB_NAME} - Build ${env.BUILD_NUMBER} - ${currentResult}:</p>\n\t\t\t\t\t<p>Check console output at <a href='${env.BUILD_URL}'>${env.BUILD_URL}</a> to view the results.</p>\"\"\"\n\t\t\t}\n\t\t}\n\n\t\temailext(\n\t\t\t\tsubject: subject,\n\t\t\t\tbody: body,\n\t\t\t\tto: buildEnv.notificationRecipients\n\t\t)\n\t}\n}\n\n@NonCPS\nString getParallelResult( RunWrapper build, String parallelBranchName ) {\n    def visitor = new PipelineNodeGraphVisitor( build.rawBuild )\n    def branch = visitor.pipelineNodes.find{ it.type == FlowNodeWrapper.NodeType.PARALLEL && parallelBranchName == it.displayName }\n    if ( branch == null ) {\n    \techo \"Couldn't find parallel branch name '$parallelBranchName'. Available parallel branch names:\"\n\t\tvisitor.pipelineNodes.findAll{ it.type == FlowNodeWrapper.NodeType.PARALLEL }.each{\n\t\t\techo \" - ${it.displayName}\"\n\t\t}\n    \treturn null;\n    }\n    return branch.status.result\n}\n\n// try-finally construct that properly suppresses exceptions thrown in the finally block.\ndef tryFinally(Closure main, Closure ... finallies) {\n\tdef mainFailure = null\n\ttry {\n\t\tmain()\n\t}\n\tcatch (Throwable t) {\n\t\tmainFailure = t\n\t\tthrow t\n\t}\n\tfinally {\n\t\tfinallies.each {it ->\n\t\t\ttry {\n\t\t\t\tit()\n\t\t\t}\n\t\t\tcatch (Throwable t) {\n\t\t\t\tif ( mainFailure ) {\n\t\t\t\t\tmainFailure.addSuppressed( t )\n\t\t\t\t}\n\t\t\t\telse {\n\t\t\t\t\tmainFailure = t\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\tif ( mainFailure ) { // We may reach here if only the \"finally\" failed\n\t\tthrow mainFailure\n\t}\n}\n"
        },
        {
          "name": "release",
          "type": "tree",
          "content": null
        },
        {
          "name": "release_notes.md",
          "type": "blob",
          "size": 4.6494140625,
          "content": "\n### <a name=\"jpa-32\"></a> Jakarta Persistence 3.2\n\n7.0 migrates to Jakarta Persistence 3.2 which can be fairly disruptive.\nSee the [Migration Guide](https://docs.jboss.org/hibernate/orm/7.0/migration-guide/migration-guide.html#jpa-32) for details.\n\nSee [this blog post](https://in.relation.to/2024/04/01/jakarta-persistence-3/) for a summary of the changes in 3.2\n\n- [TCK Results](https://ci.hibernate.org/view/ORM/job/hibernate-orm-tck-3.2/job/wip%252F7.0/24/) with Java 17\n- [TCK Results](https://ci.hibernate.org/view/ORM/job/hibernate-orm-tck-3.2/job/wip%252F7.0/25/) with Java 21\n\n### <a name=\"java-17\"></a> Java 17\n\nVersion 3.2 of Jakarta Persistence requires Java 17.  \nHibernate 7.0 therefore baselines on Java 17 whereas previous versions baseline on Java 11.\n\n### <a name=\"model-validations\"></a> Domain Model Validations\n\n7.0 does much more validation of an application's domain model and especially its mapping details, e.g.\n\n* illegal combinations such as `@Basic` and `@ManyToOne` on the same attribute\n* misplaced annotations such as an annotated getter method with FIELD access\n* stricter following of JavaBean conventions\n\nSee the [Migration Guide](https://docs.jboss.org/hibernate/orm/7.0/migration-guide/migration-guide.html#model-validation) for details.\n\n\n### <a name=\"mapping-xml\"></a> mapping.xsd\n\nHibernate 7.0 provides a new XSD that represents an \"extension\" of the Jakarta Persistence orm.xsd weaving in Hibernate-specific mapping features.  \nThe namespace for this extended mapping is `http://www.hibernate.org/xsd/orm/mapping`\n\nFor applications using Hibernate's legacy `hbm.xml` format, we provide a tool to help with the transformation.\nSee the [Migration Guide](https://docs.jboss.org/hibernate/orm/7.0/migration-guide/migration-guide.html#hbm-transform) for details.\n\n\n### <a name=\"hibernate-models\"></a> Hibernate Models\n\n7.0 migrates from [Hibernate Commons Annotations](https://github.com/hibernate/hibernate-commons-annotations/) (HCANN) to the new [Hibernate Models](https://github.com/hibernate/hibernate-models) project for low-level processing of an application domain model, reading annotations and weaving in XML mapping documents.\nSee the [Migration Guide](https://docs.jboss.org/hibernate/orm/7.0/migration-guide/migration-guide.html#hibernate-models) for details.\n\n\n### <a name=\"json-and-xml-functions\"></a> JSON and XML functions\n\nSupport for most of the JSON and XML functions that the SQL standard specifies was added to HQL/Criteria.\nThe implementations retain the SQL standard semantics and will throw an error if emulation on a database is impossible.\n\nNew functions include:\n\n* construction functions like `json_array()`, `json_object()`, `xmlelement()` and `xmlforest()`\n* query functions like `json_value()`, `json_query()` and `xmlquery()`\n* aggregation functions like `json_agg()`, `json_object_agg()` and `xmlagg()`\n* manipulation functions like `json_set()`, `json_mergepatch()`\n* any many more\n\n> The functions are incubating/tech-preview - to use them in HQL it is necessary to enable the `hibernate.query.hql.json_functions_enabled` and `hibernate.query.hql.xml_functions_enabled` configuration settings.\n\n\n### <a name=\"set-returning-functions\"></a> Set-returning Functions\n\nA set-returning function is a new type of function that can return rows and is exclusive to the `from` clause.\nThe concept is known in many different database SQL dialects and is sometimes referred to as table valued function or table function.\n\nCustom set-returning functions can be registered via a `FunctionContributor`.\nOut-of-the-box, some common set-returning functions are already supported or emulated\n\n* `unnest()` - allows to turn an array into rows\n* `generate_series()` - can be used to create a series of values as rows\n* `json_table()` - turns a JSON document into rows\n* `xmltable()` - turns an XML document into rows\n\n### <a name=\"any-discriminator\"></a> @AnyDiscriminatorImplicitValues\n\nThe new  `@AnyDiscriminatorImplicitValues` offers 2 related improvements for the mapping of discriminator values\nfor `@Any` and `ManyToAny` associations.\n\nFirst, it allows control over how Hibernate determines the discriminator value to store in the database for\nimplicit discriminator mappings.  Historically, Hibernate would always use the full name of the associated\nentity.\n\nSecond, it allows mixing of explicit and implicit value strategies.\n\nSee the [Migration Guide](https://docs.jboss.org/hibernate/orm/7.0/userguide/html_single/Hibernate_User_Guide.html#associations-any) for details.\n\n### <a name=cleanup\"></a> Clean-up\n\nA lot of deprecated contracts and behavior has been removed.\nSee the [Migration Guide](https://docs.jboss.org/hibernate/orm/7.0/migration-guide/migration-guide.html#cleanup) for details.\n\n"
        },
        {
          "name": "rules",
          "type": "tree",
          "content": null
        },
        {
          "name": "settings.gradle",
          "type": "blob",
          "size": 18.083984375,
          "content": "/*\n * Hibernate, Relational Persistence for Idiomatic Java\n *\n * License: GNU Lesser General Public License (LGPL), version 2.1 or later.\n * See the lgpl.txt file in the root directory or <http://www.gnu.org/licenses/lgpl-2.1.html>.\n */\n\npluginManagement {\n    repositories {\n        gradlePluginPortal()\n        maven {\n            name = 'localPluginRepository'\n            url = uri( \"${gradle.gradleUserHomeDir}/tmp/plugins\" )\n        }\n    }\n\n    includeBuild 'local-build-plugins'\n}\n\nplugins {\n    id 'org.hibernate.orm.build.env-settings'\n    id 'org.hibernate.orm.build.jdks-settings'\n    id 'com.gradle.develocity' version '3.17.6'\n    id 'com.gradle.common-custom-user-data-gradle-plugin' version '2.0.2'\n}\n\ndependencyResolutionManagement {\n    repositories {\n        if ( rootProject.hasProperty( \"mavenMirror\" ) ) {\n            url( rootProject.property( \"mavenMirror\" ) )\n        }\n        mavenCentral()\n        if (System.getProperty('JPA_PREVIEW') != null) {\n\t\t\tmaven {\n\t\t\t\turl \"https://jakarta.oss.sonatype.org/content/repositories/releases/\"\n\t\t\t}\n            // Needed for the SNAPSHOT versions of Jakarta Persistence\n            maven {\n                url \"https://jakarta.oss.sonatype.org/content/repositories/snapshots/\"\n            }\n\t\t}\n\n        mavenLocal()\n\n        //Allow loading additional dependencies from a local path;\n        //useful to load JDBC drivers which can not be distributed in public.\n        if (System.env['ADDITIONAL_REPO'] != null) {\n            flatDir {\n                dirs \"${System.env.ADDITIONAL_REPO}\"\n            }\n        }\n    }\n\n    pluginManagement {\n        repositories {\n            gradlePluginPortal()\n        }\n    }\n\n    versionCatalogs {\n        jdks {\n            // see gradle.properties (or set with -D/-P)\n            var String baseJdk = jdkVersions.baseline\n            var String maxJdk = jdkVersions.max\n\n            version \"baseline\", baseJdk\n            version \"compatible\", \"17 or 21\"\n            version \"jdbc\", \"4.2\" // Bundled with JDK 11\n\n            // Gradle does bytecode transformation on tests.\n            // You can't use bytecode higher than what Gradle supports, even with toolchains.\n            version \"maxSupportedBytecode\", maxJdk\n        }\n        libs {\n            def antlrVersion = version \"antlr\", \"4.13.2\"\n            // WARNING: When upgrading to a version of bytebuddy that supports a new bytecode version,\n            // make sure to remove the now unnecessary net.bytebuddy.experimental=true in relevant CI jobs (Jenkinsfile).\n            def byteBuddyVersion = version \"byteBuddy\", \"1.15.11\"\n            def classmateVersion = version \"classmate\", \"1.7.0\"\n            def geolatteVersion = version \"geolatte\", \"1.9.1\"\n            def hibernateModelsVersion = version \"hibernateModels\", \"0.9.3\"\n            def jandexVersion = version \"jandex\", \"3.2.3\"\n            def jacksonVersion = version \"jackson\", \"2.18.2\"\n            def jbossLoggingVersion = version \"jbossLogging\", \"3.6.1.Final\"\n            def jbossLoggingToolVersion = version \"jbossLoggingTool\", \"3.0.3.Final\"\n\n            def agroalVersion = version \"agroal\", \"2.5\"\n            def c3poVersion = version \"c3p0\", \"0.10.1\"\n            def hikaricpVersion = version \"hikaricp\", \"6.2.1\"\n            def ucpVersion = version \"ucp\", \"23.6.0.24.10\"\n\n            def jcacheVersion = version \"jcache\", \"1.1.1\"\n            def ehcache3Version = version \"ehcache3\", \"3.10.8\"\n\n            def micrometerVersion = version \"micrometer\", \"1.14.1\"\n\n            def antVersion = version \"ant\", \"1.10.15\"\n\n            library( \"antlr\", \"org.antlr\", \"antlr4\" ).versionRef( antlrVersion )\n            library( \"antlrRuntime\", \"org.antlr\", \"antlr4-runtime\" ).versionRef( antlrVersion)\n\n            library( \"byteBuddy\", \"net.bytebuddy\", \"byte-buddy\" ).versionRef( byteBuddyVersion )\n            library( \"byteBuddyAgent\", \"net.bytebuddy\", \"byte-buddy-agent\" ).versionRef( byteBuddyVersion )\n\n            library( \"logging\", \"org.jboss.logging\", \"jboss-logging\" ).versionRef( jbossLoggingVersion )\n            library( \"loggingAnnotations\", \"org.jboss.logging\", \"jboss-logging-annotations\" ).versionRef( jbossLoggingToolVersion )\n            library( \"loggingProcessor\", \"org.jboss.logging\", \"jboss-logging-processor\" ).versionRef( jbossLoggingToolVersion )\n\n            library( \"hibernateModels\", \"org.hibernate.models\", \"hibernate-models\" ).versionRef( hibernateModelsVersion )\n            library( \"hibernateModelsJandex\", \"org.hibernate.models\", \"hibernate-models-jandex\" ).versionRef( hibernateModelsVersion )\n            library( \"jandex\", \"io.smallrye\", \"jandex\" ).versionRef( jandexVersion )\n            library( \"classmate\", \"com.fasterxml\", \"classmate\" ).versionRef( classmateVersion )\n\n            library( \"jackson\", \"com.fasterxml.jackson.core\", \"jackson-databind\" ).versionRef( jacksonVersion )\n            library( \"jacksonXml\", \"com.fasterxml.jackson.dataformat\", \"jackson-dataformat-xml\" ).versionRef( jacksonVersion )\n            library( \"jacksonJsr310\", \"com.fasterxml.jackson.datatype\", \"jackson-datatype-jsr310\" ).versionRef( jacksonVersion )\n\n            library( \"agroal\", \"io.agroal\", \"agroal-api\" ).versionRef( agroalVersion )\n            library( \"agroalPool\", \"io.agroal\", \"agroal-pool\" ).versionRef( agroalVersion )\n            library( \"c3p0\", \"com.mchange\", \"c3p0\" ).versionRef( c3poVersion )\n            library( \"hikaricp\", \"com.zaxxer\", \"HikariCP\" ).versionRef( hikaricpVersion )\n            library( \"ucp\", \"com.oracle.database.jdbc\", \"ucp11\" ).versionRef( ucpVersion )\n\n            library( \"ojdbc11\", \"com.oracle.database.jdbc\", \"ojdbc11\" ).versionRef( ucpVersion )\n\n            library( \"geolatte\", \"org.geolatte\", \"geolatte-geom\" ).versionRef( geolatteVersion )\n\n            library( \"jcache\", \"javax.cache\", \"cache-api\" ).versionRef( jcacheVersion )\n            library( \"ehcache3\", \"org.ehcache\", \"ehcache\" ).versionRef( ehcache3Version )\n\n            library( \"micrometer\", \"io.micrometer\", \"micrometer-core\" ).versionRef( micrometerVersion )\n\n            library( \"ant\", \"org.apache.ant\", \"ant\" ).versionRef( antVersion )\n        }\n        jakartaLibs {\n            // `jakartaJpaVersion` comes from the local-build-plugins to allow for command-line overriding of the JPA version to use\n            def jpaVersion = version \"jpa\", \"${jakartaJpaVersion}\"\n\n            def annotationVersion = version \"annotation\", \"3.0.0\"\n            def cdiVersion = version \"cdi\", \"4.1.0\"\n            def injectVersion = version \"inject\", \"2.0.1\"\n            def interceptorsVersion = version \"interceptors\", \"2.2.0\"\n            def dataVersion = version \"data\", \"1.0.1\"\n            def jaccVersion = version \"jacc\", \"3.0.0\"\n            def jaxbApiVersion = version \"jaxbApi\", \"4.0.2\"\n            def jaxbRuntimeVersion = version \"jaxbRuntime\", \"4.0.5\"\n            def jsonbApiVersion = version \"jsonbApi\", \"3.0.1\"\n            def jsonbRuntimeVersion = version \"jsonbRuntime\", \"3.0.4\"\n            def jtaVersion = version \"jta\", \"2.0.1\"\n            def validationVersion = version \"validation\", \"3.1.0\"\n            def xjcVersion = version \"jaxbRuntime\", jaxbRuntimeVersion\n\n            library( \"jpa\", \"jakarta.persistence\", \"jakarta.persistence-api\" ).versionRef( jpaVersion )\n            library( \"jta\", \"jakarta.transaction\", \"jakarta.transaction-api\" ).versionRef( jtaVersion )\n\n            library( \"validation\", \"jakarta.validation\", \"jakarta.validation-api\" ).versionRef( validationVersion )\n            library( \"jacc\", \"jakarta.authorization\", \"jakarta.authorization-api\" ).versionRef( jaccVersion )\n            library( \"cdi\", \"jakarta.enterprise\", \"jakarta.enterprise.cdi-api\" ).versionRef( cdiVersion )\n            library( \"annotation\", \"jakarta.annotation\", \"jakarta.annotation-api\" ).versionRef( annotationVersion )\n            library( \"interceptors\", \"jakarta.interceptor\", \"jakarta.interceptor-api\" ).versionRef( interceptorsVersion )\n            library( \"data\", \"jakarta.data\", \"jakarta.data-api\" ).versionRef( dataVersion )\n            library( \"jsonbApi\", \"jakarta.json.bind\", \"jakarta.json.bind-api\" ).versionRef( jsonbApiVersion )\n            library( \"jsonb\", \"org.eclipse\", \"yasson\" ).versionRef( jsonbRuntimeVersion )\n            library( \"inject\", \"jakarta.inject\", \"jakarta.inject-api\" ).versionRef( injectVersion )\n            library( \"jaxbApi\", \"jakarta.xml.bind\", \"jakarta.xml.bind-api\" ).versionRef( jaxbApiVersion )\n            library( \"jaxb\", \"org.glassfish.jaxb\", \"jaxb-runtime\" ).versionRef( jaxbRuntimeVersion )\n            library( \"xjc\", \"org.glassfish.jaxb\", \"jaxb-xjc\" ).versionRef( xjcVersion )\n        }\n        testLibs {\n            def junit5Version = version \"junit5\", \"5.11.3\"\n            def junit4Version = version \"junit4\", \"4.13.2\"\n            def junit5LauncherVersion = version \"junit5Launcher\", \"1.11.3\"\n\n            def assertjVersion = version \"assertj\", \"3.26.3\"\n            def hamcrestVersion = version \"hamcrest\", \"3.0\"\n            def bytemanVersion = version \"byteman\", \"4.0.24\"\n            def jbossJtaVersion = version \"jbossJta\", \"7.1.0.Final\"\n            def jbossTxSpiVersion = version \"jbossTxSpi\", \"8.0.0.Final\"\n            def log4jVersion = version \"log4j\", \"2.24.2\"\n            def mockitoVersion = version \"mockito\", \"5.14.2\"\n            def shrinkwrapVersion = version \"shrinkwrap\", \"1.2.6\"\n            def shrinkwrapDescriptorsVersion = version \"shrinkwrapDescriptors\", \"2.0.0\"\n            def weldVersion = version \"weld\", \"5.1.3.Final\"\n            def wildFlyTxnClientVersion = version \"wildFlyTxnClient\", \"2.0.0.Final\"\n\n            def jfrUnitVersion = version \"jfrUnit\", \"1.0.0.Alpha2\"\n\n            def hibernateValidatorVersion = version \"hibernateValidator\", \"9.0.0.Beta3\"\n\n            library( \"validator\", \"org.hibernate.validator\", \"hibernate-validator\" ).versionRef( hibernateValidatorVersion )\n\n            library( \"junit5Api\", \"org.junit.jupiter\", \"junit-jupiter-api\" ).versionRef( junit5Version )\n            library( \"junit5Engine\", \"org.junit.jupiter\", \"junit-jupiter-engine\" ).versionRef( junit5Version )\n            library( \"junit5Params\", \"org.junit.jupiter\", \"junit-jupiter-params\" ).versionRef( junit5Version )\n            library( \"junit4Engine\", \"org.junit.vintage\", \"junit-vintage-engine\" ).versionRef( junit5Version )\n            library( \"junit5Launcher\", \"org.junit.platform\", \"junit-platform-launcher\" ).versionRef( junit5LauncherVersion )\n            library( \"junit4\", \"junit\", \"junit\" ).versionRef( junit4Version )\n\n            library( \"assertjCore\", \"org.assertj\", \"assertj-core\" ).versionRef( assertjVersion )\n            library( \"assertjApi\", \"org.assertj\", \"assertj-core-api\" ).versionRef( assertjVersion )\n            library( \"hamcrest\", \"org.hamcrest\", \"hamcrest\" ).versionRef( hamcrestVersion )\n\n            library( \"log4j2\", \"org.apache.logging.log4j\", \"log4j-core\" ).versionRef( log4jVersion )\n\n            library( \"shrinkwrap\", \"org.jboss.shrinkwrap\", \"shrinkwrap-impl-base\" ).versionRef( shrinkwrapVersion )\n            library( \"shrinkwrapDescriptors\", \"org.jboss.shrinkwrap.descriptors\", \"shrinkwrap-descriptors-impl-javaee\" ).versionRef( shrinkwrapDescriptorsVersion )\n\n            library( \"bytemanBmunit\", \"org.jboss.byteman\", \"byteman-bmunit\" ).versionRef( bytemanVersion )\n\n            library( \"mockito\", \"org.mockito\", \"mockito-core\" ).versionRef( mockitoVersion )\n\n            library( \"jbossJta\", \"org.jboss.narayana.jta\", \"narayana-jta\" ).versionRef( jbossJtaVersion )\n            library( \"jbossTxSpi\", \"org.jboss\", \"jboss-transaction-spi\" ).versionRef( jbossTxSpiVersion )\n            library( \"wildFlyTxnClient\", \"org.wildfly.transaction\", \"wildfly-transaction-client-jakarta\" ).versionRef( wildFlyTxnClientVersion )\n\n            library( \"weld\", \"org.jboss.weld.se\", \"weld-se-shaded\" ).versionRef( weldVersion )\n\n            library( \"jfrUnit\", \"org.moditect.jfrunit\", \"jfrunit-core\" ).versionRef( jfrUnitVersion )\n        }\n        jdbcLibs {\n            def h2Version = version \"h2\", overrideableVersion( \"gradle.libs.versions.h2\", \"2.3.232\" )\n\n            def db2Version = version \"db2\", \"11.5.9.0\"\n            // Latest Derby version 10.16.1.1 only supports JDK 17+, but 10.15.2 should be compatible\n            def derbyVersion = version \"derby\", overrideableVersion( \"gradle.libs.versions.derby\", \"10.15.2.0\" )\n            def firebirdVersion = version \"firebird\", \"4.0.8.java11\"\n            def hanaVersion = version \"hana\", \"2.22.12\"\n            def h2gisVersion = version \"h2gis\", overrideableVersion( \"gradle.libs.versions.h2gis\", \"2.2.3\" )\n            def hsqldbVersion = version \"hsqldb\", overrideableVersion( \"gradle.libs.versions.hsqldb\", \"2.7.4\" )\n            def informixVersion = version \"informix\", \"4.50.11\"\n            def mariadbVersion = version \"mariadb\", \"3.5.1\"\n            def mssqlVersion = version \"mssql\", \"12.8.1.jre11\"\n            def mysqlVersion = version \"mysql\", \"9.1.0\"\n            def oracleVersion = version \"oracle\", \"23.4.0.24.05\"\n            def pgsqlVersion = version \"pgsql\", \"42.7.4\"\n            def sybaseVersion = version \"sybase\", \"1.3.1\"\n            def tidbVersion = version \"tidb\", mysqlVersion\n            def altibaseVersion = version \"altibase\", \"7.3.0.0.3\"\n\n            library( \"h2\", \"com.h2database\", \"h2\" ).versionRef( h2Version )\n            library( \"h2gis\", \"org.orbisgis\", \"h2gis\" ).versionRef( h2gisVersion )\n            library( \"hsqldb\", \"org.hsqldb\", \"hsqldb\" ).versionRef( hsqldbVersion )\n            library( \"derby\", \"org.apache.derby\", \"derby\" ).versionRef( derbyVersion )\n            library( \"derbyTools\", \"org.apache.derby\", \"derbytools\" ).versionRef( derbyVersion )\n            library( \"postgresql\", \"org.postgresql\", \"postgresql\" ).versionRef( pgsqlVersion )\n            library( \"cockroachdb\", \"org.postgresql\", \"postgresql\" ).versionRef( pgsqlVersion )\n            library( \"mysql\", \"com.mysql\", \"mysql-connector-j\" ).versionRef( mysqlVersion )\n            library( \"tidb\", \"com.mysql\", \"mysql-connector-j\" ).versionRef( tidbVersion )\n            library( \"mariadb\", \"org.mariadb.jdbc\", \"mariadb-java-client\" ).versionRef( mariadbVersion )\n            library( \"oracle\", \"com.oracle.database.jdbc\", \"ojdbc11\" ).versionRef( oracleVersion )\n            library( \"oracleXml\", \"com.oracle.database.xml\", \"xdb\" ).versionRef( oracleVersion )\n            library( \"oracleXmlParser\", \"com.oracle.database.xml\", \"xmlparserv2\" ).versionRef( oracleVersion )\n            library( \"mssql\", \"com.microsoft.sqlserver\", \"mssql-jdbc\" ).versionRef( mssqlVersion )\n            library( \"db2\", \"com.ibm.db2\", \"jcc\" ).versionRef( db2Version )\n            library( \"hana\", \"com.sap.cloud.db.jdbc\", \"ngdbc\" ).versionRef( hanaVersion )\n            library( \"sybase\", \"net.sourceforge.jtds\", \"jtds\" ).versionRef( sybaseVersion )\n            library( \"informix\", \"com.ibm.informix\", \"jdbc\" ).versionRef( informixVersion )\n            library( \"firebird\", \"org.firebirdsql.jdbc\", \"jaybird\" ).versionRef( firebirdVersion )\n            library( \"altibase\", \"com.altibase\", \"altibase-jdbc\" ).versionRef( altibaseVersion )\n        }\n        mavenLibs {\n            def mavenCoreVersion = version \"mavenCore\", \"3.9.9\"\n            def mavenVersion = version \"maven\", \"3.9.9\"\n            def mavenPluginToolsVersion = version \"mavenPluginTools\", \"3.15.1\"\n\n            library( \"mavenCore\", \"org.apache.maven\", \"maven-core\" ).versionRef( mavenCoreVersion )\n            library( \"mavenArtifact\", \"org.apache.maven\", \"maven-artifact\" ).versionRef( mavenVersion )\n            library( \"mavenPlugin\", \"org.apache.maven\", \"maven-plugin-api\" ).versionRef( mavenVersion )\n            library( \"mavenPluginTools\", \"org.apache.maven.plugin-tools\", \"maven-plugin-annotations\" ).versionRef( mavenPluginToolsVersion )\n        }\n        buildscriptLibs {\n            def forbiddenapisversion = version \"forbiddenapis\", \"3.8\"\n\n            library( \"forbiddenapis\", \"de.thetaphi\", \"forbiddenapis\" ).versionRef( forbiddenapisversion )\n        }\n    }\n}\n\nString overrideableVersion(String settingName, String defaultVersion) {\n    String overridden = settings.ext.find( settingName )\n    if ( overridden != null ) {\n        return overridden\n    }\n    return defaultVersion\n}\n\nrootProject.name = 'hibernate-orm'\n\napply from: file( 'gradle/gradle-develocity.gradle' )\n\nif ( !JavaVersion.current().java11Compatible ) {\n    throw new GradleException( \"Gradle must be run with Java 11 or later\" )\n}\n\nbuildCache {\n    local {\n        // do not use local build cache for CI jobs, period!\n        enabled = !settings.ext.isCiEnvironment\n    }\n    remote(develocity.buildCache) {\n        enabled = settings.ext.useRemoteCache\n        // Check access key presence to avoid build cache errors on PR builds when access key is not present\n        def accessKey = System.getenv(\"DEVELOCITY_ACCESS_KEY\")\n        push = settings.ext.populateRemoteBuildCache && accessKey\n    }\n}\n\ninclude 'hibernate-core'\ninclude 'hibernate-testing'\n\ninclude 'hibernate-envers'\ninclude 'hibernate-spatial'\n\ninclude 'hibernate-platform'\n\ninclude 'hibernate-community-dialects'\ninclude 'hibernate-vector'\n\ninclude 'hibernate-c3p0'\ninclude 'hibernate-hikaricp'\ninclude 'hibernate-agroal'\ninclude 'hibernate-ucp'\n\ninclude 'hibernate-jcache'\n\ninclude 'hibernate-micrometer'\ninclude 'hibernate-graalvm'\ninclude 'hibernate-integrationtest-java-modules'\n\ninclude 'documentation'\ninclude 'release'\n\ninclude 'annotation-descriptor-generator'\n\n// Not all JDK implementations support JFR\nif ( \"OpenJDK Runtime Environment\".equals( System.getProperty( \"java.runtime.name\" ) ) ) {\n    include 'hibernate-jfr'\n}\n\ninclude 'hibernate-scan-jandex'\n\ninclude 'metamodel-generator'\nproject(':metamodel-generator').projectDir = new File(rootProject.projectDir, \"tooling/metamodel-generator\")\nproject(':metamodel-generator').name = 'hibernate-processor'\n\ninclude 'hibernate-gradle-plugin'\nproject(':hibernate-gradle-plugin').projectDir = new File(rootProject.projectDir, \"tooling/hibernate-gradle-plugin\")\n\ninclude 'hibernate-maven-plugin'\nproject(':hibernate-maven-plugin').projectDir = new File(rootProject.projectDir, \"tooling/hibernate-maven-plugin\")\n\ninclude 'hibernate-ant'\nproject(':hibernate-ant').projectDir = new File(rootProject.projectDir, \"tooling/hibernate-ant\")\n\nrootProject.children.each { project ->\n    project.buildFileName = \"${project.name}.gradle\"\n    assert project.projectDir.isDirectory()\n    assert project.buildFile.exists()\n    assert project.buildFile.isFile()\n}\ninclude 'hibernate-platform'\n\n\n"
        },
        {
          "name": "shared",
          "type": "tree",
          "content": null
        },
        {
          "name": "spotless.license.java",
          "type": "blob",
          "size": 0.0986328125,
          "content": "/*\n * SPDX-License-Identifier: LGPL-2.1-or-later\n * Copyright Red Hat Inc. and Hibernate Authors\n */\n"
        },
        {
          "name": "tck",
          "type": "tree",
          "content": null
        },
        {
          "name": "test-case-guide.adoc",
          "type": "blob",
          "size": 9.9189453125,
          "content": "= Test Case Guide\n:toc:\n\nThis is meant as a guide for writing test cases to be attached to bug reports in the Hibernate Jira.  Really most of the information here works just as well when asking for help on community help channels (forums, IRC, HipChat, etc).\n\n\n== Write a good test\n\nThere are a number of tenants that make up a good test case as opposed to a poor one.  In fact there are a few guides for this across the web including (http://stackoverflow.com/help/mcve[MCVE]) and (http://sscce.org/[SSCCE]).  These guides all assert the same ideas albeit using different terms.  Given the ubiquity of StackOverflow and the fact that the MCVE guidelines were written specifically for StackOverflow, we will use those terms here as we assume most developers have seen them before:\n\n* (M)inimal - Provide just the minimal information needed.  If second level caching is irrelevant to the bug report then the test should not use second level caching.  If entity inheritance is irrelevant then do not use it in the test.  If your application uses Spring Data, remove Spring Data from the test.\n* \\(C)omplete - Provide all information needed to reproduce the problem.  If a bug only occurs when using bytecode enhancement, then the test should include bytecode enhancement.  In other words the test should be self-contained.\n* (V)erifiable - The test should actually reproduce the problem being reported.\n\n\n[[junit5]]\n== JUnit 5 extensions\n\nJUnit 5 offers better support for integration, compared to JUnit 4, via https://junit.org/junit5/docs/current/user-guide/#extensions[extensions].  Hibernate builds on those concepts in its `hibernate-testing` module allowing set up of test fixtures using annotations.  The following sections describe the Hibernate extensions.\n\nNOTE: The extensions exist in the `org.hibernate.testing.orm.junit` package, as opposed to the older `org.hibernate.testing.junit4` package used with JUnit 4.\n\n\n[[junit5-service-registry]]\n=== ServiceRegistryExtension\n\nManages a `ServiceRegistry` as part of the test lifecycle.  2 in fact, depending on the annotation(s) used.\n\n`@BootstrapServiceRegistry`:: configures Hibernate's bootstrap `BootstrapServiceRegistry` which manages class-loading, etc.  `@BootstrapServiceRegistry` is used to provide Java services and Hibernate `Integrator` implementations for the test.\n`@ServiceRegistry`:: configures Hibernate's standard `StandardServiceRegistry`.  `@ServiceRegistry` is used to provide settings, contributors, services, etc.\n\nAlso exposes `ServiceRegistryScope` via JUnit 5 `ParameterResolver`.  `ServiceRegistryScope` allows\naccess to the managed `ServiceRegistry` from tests and callbacks.\n\n```\n@BootstrapServiceRegistry(\n    javaServices=@JavaServices(\n        role=TypeContributions.class,\n        impls=CustomTypeContributions.class\n    ),\n    ...\n)\n@ServiceRegistry(\n    settings=@Setting(\n            name=\"hibernate.show_sql\",\n            value=\"true\"\n    ),\n    services=@Service(\n            role=ConnectionProvider.class,\n            impl=CustomConnectionProvider.class\n    ),\n    ...\n)\nclass TheTest {\n    @Test void testIt(ServiceRegistryScope scope) {\n        StandardServiceRegistry reg = scope.getRegistry();\n        ...\n    }\n}\n```\n\n\n[[junit5-domain-model]]\n=== DomainModelExtension\n\nManages the domain model for the test as part of its lifecycle.\n\n`@DomainModel`:: defines the sources of the domain model used in the test - type contributions, managed classes, XML mappings, etc.\n\nIf available, this extension uses the `ServiceRegistry` instances available from <<junit5-service-registry>>.\n\nExposes `DomainModelScope` via JUnit5 `ParameterResolver`, allowing access to details about the domain model from the `org.hibernate.mapping` \"boot model\".\n\n\n```\n@DomainModel(\n    standardDomainModels=StandardDomainModel.ANIMAL,\n    annotatedClasses={Entity1.class, Entity2.class},\n    xmlMappings=\"resource/path/to/my-mapping.xml\",\n    ...\n)\nclass TheTest {\n    @Test void testIt(DomainModelScope scope) {\n        MetadataImplementor meta = scope.getDomainModel();\n        ...\n\n        PersistentClass entityMapping = scope.getEntityBinding(Entity1.class);\n        ...\n\n        scope.withHierarchy(Entity1.class, (entityMapping) -> {\n            ...\n        }\n    }\n}\n```\n\n\n=== SessionFactoryExtension\n\nManages a Hibernate `SessionFactory` as part of the test lifecycle.\n\n`@SessionFactory`:: is used to configure the runtime aspects of the `SessionFactory` fixture.\n\nIf available, uses the `ServiceRegistry` instances available from <<junit5-service-registry>> as well\nas the domain model defined by <<junit5-domain-model>>.\n\nExposes `SessionFactoryScope` via JUnit5 `ParameterResolver`.\n\n```\n@SessionFactory(\n    generateStatistics=true,\n    exportSchema=true,\n    useCollectingStatementInspector=true,\n    ...\n)\nclass TheTest {\n    @Test void testIt(SessionFactoryScope scope) {\n        SQLStatementInspector sqlCollector = scope.getCollectingStatementInspector();\n        sqlCollector.clear();\n\n        scope.inTransaction( (session) -> {\n            ...\n            assertThat(sqlCollector.getSqlQueries()).isEmpty();\n        } );\n\n        Entity1 e = scope.fromTransaction( (session) -> {\n            Entity1 it = session.find(Entity1.class, id);\n            ...\n            return it;\n        } );\n    }\n}\n```\n\n=== DialectFilterExtension\n\nAllows filtering tests based on Dialect used.  Implemented as a JUnit `ExecutionCondition` which is used to dynamically determine whether a test should be run.  Used in conjunction with:\n\n`@RequiresDialect`:: says to only run this test for the given Dialect(s).\n`@SkipForDialect`:: says to skip this test for the given Dialect(s).\n\n=== ExpectedExceptionExtension\n\nUsed with `@ExpectedException` to allow testing that an excepted exception occurs as the \"success\" condition.\n\n```\n@DomainModel(...)\n@SessionFactory(...)\nclass TheTest {\n    @Test\n    @ExpectedException(UnknownEntityTypeException.class)\n    void testIt(SessionFactoryScope) {\n        scope.inTransaction( (session) -> {\n            // Should fail as MyEmbeddable is not an entity\n            session.find(MyEmbeddable.class, 1);\n        } );\n    }\n}\n```\n\n\n=== FailureExpectedExtension\n\nUsed with `@FailureExpected` to indicate that a test is (currently) expected to fail.  You might use this, e.g., for a test that is the reproducer for a bug report before working on it.  It basically just flips the success/failure condition.  In fact, a test marked with `@FailureExpected` will be marked a failure if it succeeds.\n\n```\n@Test\n@JiraKey(\"HHH-123456789\")\n@FailureExpected\nvoid bugReproducer(...) {...}\n```\n\n\n=== LoggingInspectionsExtension and MessageKeyInspectionExtension\n\nBoth are used for testing log messages.\n\n`@LoggingInspections`:: used to watch more than one \"message key\".\n`MessageKeyInspection`:: used to watch a single \"message key\".\n\n\n=== EntityManagerExtension\n\nUsed in conjunction with `@Jpa` to build tests with an `EntityManagerFactory` fixture.\n\nSince Hibernate's `SessionFactory` *is a* `EntityManagerFactory`, `@BootstrapServiceRegistry`, `@ServiceRegistry`, `@DomainModel` and `@SessionFactory` can also be used to perform tests with a (`SessionFactory` as a) `EntityManagerFactory` fixture.\n\nThe distinction with `@Jpa` is that `EntityManagerExtension` uses the JPA-defined bootstrap APIs.  How the\n`SessionFactory` is built is the difference.\n\n\n== JUnit 4\n\nHistorically, Hibernate used JUnit 4 for its test suite.  Since the release of https://junit.org/junit5/[JUnit 5], we've moved to using the testing approach outlined in <<junit5>>.  However, many existing tests still use the legacy JUnit 4 based infrastructure (boilerplate) based on \"test templates\".\n\n\n=== Test templates\n\nThe Hibernate team maintains a set of \"test templates\" intended to help developers write tests.  These test templates are maintained in GitHub @ https://github.com/hibernate/hibernate-test-case-templates/tree/main/orm[hibernate-test-case-templates]\n\n* If you want to use the Hibernate native API, you should follow the instructions from http://in.relation.to/2015/06/26/hibernate-test-case-templates/[this article].\n* If you want to use JPA, you should use the JPA templates that were detailed in http://in.relation.to/2016/01/14/hibernate-jpa-test-case-template/[this article].\n\nNOTE: the test templates are generally not a good starting point for problems building the SessionFactory/EntityManager.  In JUnit terms they manage the SessionFactory/EntityManager as set-up and teardown constructs._\n\n=== Annotations\n\nWhen using \"test templates\" you can annotate a  single test or a whole test class with one of the following  annotations:\n\n* FailureExpected - allows to skip a single test or all tests of a class, because test failures are expected. The test will actually run, but not lead to an error report. In fact if a test is marked with `@FailureExpected` and the test actually succeeds, an error occurs. As a parameter to this annotation a jira key is required.\n* NotImplementedYet - test classes or methods annotated with @NotImplementedYet will run but not fail if the feature(s) that are being tested are not implemented yet for the current version. Optionally, a message and a version that is expected to have the feature already implemented can be provided as parameters.\n* RequiresDialect - tests methods/classes annotated with `@RequiresDialect` will only run if the current Dialect matches the one specified as annotation parameter. You can also specify a comment and/or jira key explaining why this test requires a certain dialect\n* RequiresDialectFeature - tests methods/classes annotated with `@RequiresDialectFeature` will only run if the current Dialect offers the specified feature. Examples for this features are `SupportsSequences`, `SupportsExpectedLobUsagePattern` or `SupportsIdentityColumns`. You can add more features if you need to. Have a look at `DialectChecks`.\n* SkipForDialect - tests methods/classes annotated with `@SkipForDialect` will not run if  the current Dialect matches the one specified as annotation  parameter. You can also specify a comment and/or jira key explaining why  this test has to be skipped for the Dialect.\n"
        },
        {
          "name": "tooling",
          "type": "tree",
          "content": null
        },
        {
          "name": "utilities.gradle",
          "type": "blob",
          "size": 1.5849609375,
          "content": "\napply plugin: UtilitiesPlugin\n\n/*\n * Hibernate, Relational Persistence for Idiomatic Java\n *\n * License: GNU Lesser General Public License (LGPL), version 2.1 or later.\n * See the lgpl.txt file in the root directory or <http://www.gnu.org/licenses/lgpl-2.1.html>.\n */\nclass UtilitiesPlugin implements Plugin<Project> {\n    def void apply(Project project) {\n        project.convention.plugins.utilities = new UtilitiesPluginDef()\n    }\n}\n\nclass UtilitiesPluginDef {\n    @SuppressWarnings(\"GrUnnecessarySemicolon\")\n    public String determinePackageName(SourceDirectorySet sourceDirectorySet, File javaFile) {\n        final javaFileAbsolutePath = javaFile.absolutePath;\n        for ( File sourceDirectory : sourceDirectorySet.srcDirs ) {\n            final String sourceDirectoryAbsolutePath = sourceDirectory.absolutePath;\n            if ( javaFileAbsolutePath.startsWith( sourceDirectoryAbsolutePath ) ) {\n                final String javaFileRelativePath = javaFileAbsolutePath.substring(\n                        sourceDirectoryAbsolutePath.length() + 1,\n                        javaFileAbsolutePath.lastIndexOf( File.separator )\n                );\n                return javaFileRelativePath.replace( File.separator, \".\" );\n            }\n        }\n        throw new RuntimeException( \"ugh\" );\n    }\n\n    String java9ModuleName(Project project) {\n        String name = project.name\n\n        // alternative is to just use the full project name (don't drop the 'hibernate-' prefix)\n\n        if ( name.startsWith( 'hibernate-' ) ) {\n            name = name.drop( 'hibernate-'.length() )\n        }\n\n        return name\n    }\n}"
        }
      ]
    }
  ]
}