{
  "metadata": {
    "timestamp": 1736708675545,
    "page": 78,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjgw",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "apache/beam",
      "stars": 7946,
      "defaultBranch": "master",
      "files": [
        {
          "name": ".asf.yaml",
          "type": "blob",
          "size": 3.220703125,
          "content": "# Licensed to the Apache Software Foundation (ASF) under one or more\n# contributor license agreements. See the NOTICE file distributed with\n# this work for additional information regarding copyright ownership.\n# The ASF licenses this file to You under the Apache License, Version 2.0\n# (the \"License\"); you may not use this file except in compliance with\n# the License. You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# This file configures github and jira notifications based on\n# https://s.apache.org/asfyaml-notify\n\ngithub:\n  description: \"Apache Beam is a unified programming model for Batch and Streaming data processing.\"\n  homepage: https://beam.apache.org/\n  labels:\n    - batch\n    - beam\n    - big-data\n    - golang\n    - java\n    - python\n    - sql\n    - streaming\n  features:\n    # Enable issue management\n    issues: true\n    # Enable projects for project management boards\n    projects: true\n\n  # Give some users issue triage permissions\n  collaborators:\n    - Amar3tto\n    - mrshakirov\n    - akashorabek\n\n  enabled_merge_buttons:\n    squash: true\n    merge: true\n    rebase: false\n\n  protected_branches:\n    master: {}\n    release-2.62.0: {}\n    release-2.61.0: {}\n    release-2.60.0: {}\n    release-2.59.0: {}\n    release-2.58.1: {}\n    release-2.58.0: {}\n    release-2.57.0: {}\n    release-2.56.0: {}\n    release-2.55.1: {}\n    release-2.55.0: {}\n    release-2.54.0: {}\n    release-2.53.0: {}\n    release-2.52.0: {}\n    release-2.51.0: {}\n    release-2.50.0: {}\n    release-2.49.0: {}\n    release-2.48.0: {}\n    release-2.47.0: {}\n    release-2.46.0: {}\n    release-2.45.0: {}\n    release-2.44.0: {}\n    release-2.43.0: {}\n    release-2.42.0: {}\n    release-2.41.0: {}\n    release-2.40.0: {}\n    release-2.39.0: {}\n    release-2.38.0: {}\n    release-2.37.0: {}\n    release-2.36.0: {}\n    release-2.35.0: {}\n    release-2.34.0: {}\n    release-2.33.0: {}\n    release-2.32.0: {}\n    release-2.31.0: {}\n    release-2.30.0: {}\n    release-2.29.0: {}\n    release-2.28.0: {}\n    release-2.27.0: {}\n    release-2.26.0: {}\n    release-2.25.0: {}\n    release-2.24.0: {}\n    release-2.23.0: {}\n    release-2.22.0: {}\n    release-2.21.0: {}\n    release-2.20.0: {}\n    release-2.19.0: {}\n    release-2.18.0: {}\n    release-2.17.0: {}\n    release-2.16.0: {}\n    release-2.15.0: {}\n    release-2.14.0: {}\n    release-2.13.0: {}\n    release-2.12.0: {}\n    release-2.11.0: {}\n    release-2.10.0: {}\n    release-2.8.0: {}\n    release-2.8.0: {}\n    release-2.7.0: {}\n    release-2.6.0: {}\n    release-2.5.0: {}\n    release-2.4.0: {}\n    release-2.3.0: {}\n    release-2.2.0: {}\n    release-2.1.1: {}\n    release-2.1.0: {}\n    release-0.6.0: {}\n    release-0.5.0: {}\n    release-0.4.0: {}\n    release-0.4.0-incubating: {}\n    release-0.3.0-incubating: {}\n    release-0.2.0-incubating: {}\n    release-0.1.0-incubating: {}\n\nnotifications:\n  commits: commits@beam.apache.org\n  issues: github@beam.apache.org\n  pullrequests: github@beam.apache.org\n"
        },
        {
          "name": ".gitattributes",
          "type": "blob",
          "size": 0.9453125,
          "content": "# The default behavior, which overrides 'core.autocrlf', is to use Git's\n# built-in heuristics to determine whether a particular file is text or binary.\n# Text files are automatically normalized to the user's platforms.\n* text=auto\n\n# Explicitly declare text files that should always be normalized and converted\n# to native line endings.\n.gitattributes text\n.gitignore text\nLICENSE text\nDockerfile text\n*.avsc text\n*.go text\n*.html text\n*.java text\n*.md text\n*.properties text\n*.proto text\n*.py text\n*.sh text\n*.xml text\n*.yml text\n\n# Declare files that will always have CRLF line endings on checkout.\n# *.sln text eol=crlf\n\n# Explicitly denote all files that are truly binary and should not be modified.\n# *.jpg binary\n\n# Declare files that should be ignored when creating an archive of the\n# git repository\n.gitignore export-ignore\n.gitattributes export-ignore\n/gradlew* export-ignore\n**/gradle export-ignore\n\n# Website is not part of archive\n/website export-ignore\n"
        },
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 3.7216796875,
          "content": "# NOTE: if you modify this file, you probably need to modify the file set that\n# is an input to 'maven-assembly-plugin' that generates source distribution.\n# This is typically in files named 'src.xml' throughout this repository.\n\n# Ignore any offline repositories the user may have created.\n**/offline-repository/**/*\n\n# Ignore files generated by the Gradle build process.\n**/.gradle/**/*\n**/.gogradle/**/*\n**/.nb-gradle/**/*\n**/gogradle.lock\n**/build/**/*\n.test-infra/**/vendor/**/*\nsdks/**/vendor/**/*\nrunners/**/vendor/**/*\n**/.gradletasknamecache\n**/generated/*\n/go.mod\n/go.sum\n\n# Ignore sources generated into the main tree\n**/src/main/generated/**\n**/src/test/generated_tests/**\n\n# Ignore files generated by the Maven build process.\n**/bin/**/*\n**/dependency-reduced-pom.xml\n**/target/**/*\n\n# Ignore generated archetypes\nsdks/java/maven-archetypes/examples/src/main/resources/archetype-resources/src/\nsdks/java/maven-archetypes/examples-java8/src/main/resources/archetype-resources/src/\nsdks/java/maven-archetypes/gcp-bom-examples/src/main/resources/archetype-resources/src/\nsdks/java/maven-archetypes/examples/src/main/resources/archetype-resources/sample.txt\n\n# Ignore files generated by the Python build process.\n**/*.pyc\n**/*.pyo\n**/*.pyd\n**/*.egg-info/\n**/.eggs/\n**/nose-*.egg/\n**/.tox/**/*\n**/dist/**/*\n**/distribute-*/**/*\n**/env/**/*\n**/.mypy_cache\n**/.dmypy.json\n**/.hypothesis\nsdks/python/**/*.c\nsdks/python/**/*.so\nsdks/python/**/*.egg\nsdks/python/LICENSE\nsdks/python/NOTICE\nsdks/python/README.md\nsdks/python/apache_beam/transforms/xlang/*\nsdks/python/apache_beam/portability/api/*\nsdks/python/apache_beam/yaml/docs/*\nsdks/python/nosetests*.xml\nsdks/python/pytest*.xml\nsdks/python/postcommit_requirements.txt\nsdks/python/.coverage\nsdks/python/coverage.xml\n\n# Ignore IntelliJ files.\n**/.idea/**/*\n**/*.iml\n**/*.ipr\n**/*.iws\n**/out/**/*\n\n# Ignore Eclipse files.\n**/.classpath\n**/.project\n**/.factorypath\n**/.checkstyle\n**/.fbExcludeFilterFile\n**/.apt_generated/**/*\n**/.settings/**/*\n**/.gitignore\n\n# Ignore Visual Studio Code files.\n**/.vscode/**/*\n\n# Hotspot VM leaves this log in a non-target directory when java crashes\n**/hs_err_pid*.log\n\n# Ignore files that end with '~', since they are most likely auto-save files\n# produced by a text editor.\n**/*~\n\n# Ignore MacOSX files.\n**/.DS_Store/**/*\n**/.DS_Store\n\n# Ignore Jupyter notebook checkpoints.\n**/.ipynb_checkpoints/**/*\n\n# Ignore python venv\n**/venv\n\n# pyenv\n.python-version\n\n# NOTE: if you modify this file, you probably need to modify the file set that\n# is an input to 'maven-assembly-plugin' that generates source distribution.\n# This is typically in files named 'src.xml' throughout this repository.\n\n# JetBrains Education files\n!**/study_project.xml\n**/.coursecreator/**/*\n\n.pytest_cache\n.pytest_cache/**/*\n\n# Hugo\nwebsite/www/node_modules\nwebsite/www/dist\nwebsite/www/site/resources\nwebsite/www/site/github_samples\nwebsite/www/site/code_samples\nwebsite/www/site/_config_branch_repo.toml\nwebsite/www/yarn-error.log\n!website/www/site/content\n\n# Node\n**/node_modules\n\n# Dart/Flutter\n**/.dart_tool\n**/.flutter-plugins\n**/.flutter-plugins-dependencies\n**/.packages\n**/generated_plugin_registrant.dart\nplayground/frontend/playground_components/pubspec.lock\nplayground/frontend/playground_components/test/tools/extract_symbols_java/dependencies\nplayground/frontend/playground_components_dev/pubspec.lock\n\n# Ignore Beam Playground Terraform\nplayground/cloudfunction.zip\n\n# Ignore Terraform related\n**/.terraform/\n**/*.tfstate\n**/*.tfstate.*\n\n# Ignore Katas auto-generated files\n**/*-remote-info.yaml\n\n# Ignore .test-infra/mock-apis related files\n.test-infra/mock-apis/**/charts/\n\n# Ignore .test-infra/metrics/github_runs_prefetcher/code.zip\n# as its generated with terraform\n.test-infra/metrics/sync/github/github_runs_prefetcher/code.zip\n"
        },
        {
          "name": ".gitmodules",
          "type": "blob",
          "size": 0.4072265625,
          "content": "[submodule \"website/www/site/themes/docsy\"]\n\tpath = website/www/site/themes/docsy\n\turl = https://github.com/google/docsy.git\n[submodule \".github/actions/cancel-workflow-runs\"]\n\tpath = .github/actions/cancel-workflow-runs\n\turl = https://github.com/potiuk/cancel-workflow-runs\n[submodule \".github/actions/github-push-action\"]\n\tpath = .github/actions/github-push-action\n\turl = https://github.com/ad-m/github-push-action\n"
        },
        {
          "name": ".mailmap",
          "type": "blob",
          "size": 22.595703125,
          "content": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# License); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an AS IS BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nAbbass Marouni <amarouni@talend.com>\nAbdullah Bashir <mabdullah353@gmail.com>\nAdam Horky <adam.horky@firma.seznam.cz>\nAdam Horky <horky.adam@gmail.com>\nAhmet Altay <aaltay@gmail.com>\nAhmet Altay <altay@altay-macbookpro2.roam.corp.google.com>\nAhmet Altay <altay@google.com>\nAlan Myrvold <alan@Alans-MacBook.local>\nAlan Myrvold <alan.myrvold@comcast.net>\nAlan Myrvold <amyrvold@amyrvold-macbookpro.roam.corp.google.com>\nAlan Myrvold <amyrvold@google.com>\nAleksandr Kokhaniukov <alexander.kohanyukov@gmail.com>\nAlex Amato <ajamato@ajamato2016.sea.corp.google.com>\nAlex Amato <ajamato@ajamato-linux0.sea.corp.google.com>\nAlex Amato <ajamato@google.com>\nAlexander Dejanovski <alex@thelastpickle.com>\nAlexander Hoem Rosbach <alexander@rosbach.no>\nAlexey Diomin <diominay@gmail.com>\nAlexey Romanenko <33895511+aromanenko-dev@users.noreply.github.com>\nAlexey Romanenko <aromanenko-dev@gmail.com>\nAlexey Romanenko <aromanenko.dev@gmail.com>\nAlex Filatov <alex-filatov@users.noreply.github.com>\nAlex Van Boxel <alex@vanboxel.be>\nAljoscha Krettek <aljoscha.krettek@gmail.com>\nAmit Sela <amitsela33@gmail.com>\nAmy Unruh <amyu@google.com>\nAnders Johnson <andersjohnson@google.com>\nAndrea Foegler <foegler@foegler-macbookpro3.roam.corp.google.com>\nAndrea Foegler <foegler@google.com>\nAndreas Ehrencrona <andreas.ehrencrona@velik.it>\nandreich <andreich@google.com>\nAndrew Brampton <bramp@google.com>\nAndrew Fulton <afulton@me.com>\nAndrew Fulton <andrew@flumaion.com>\nAndrew Martin <amartin@spotify.com>\nAndrew Martin <andrewsmartin.mg@gmail.com>\nAndrew Pilloud <apilloud@google.com>\nAndrew Pilloud <apilloud@users.noreply.github.com>\nAneesh Naman <aneeshnaman@google.com>\nAnil Muppalla <anil@spotify.com>\nAnkit Jhalaria <ajhalaria@godaddy.com>\nAnkur Goenka <angoenka@users.noreply.github.com>\nAnkur Goenka <ankurgoenka@gmail.com>\nAnkur Goenka <goenka@goenka.svl.corp.google.com>\nAntonio D'souza <adsouza@gmail.com>\nAnton Kedin <33067037+akedin@users.noreply.github.com>\nAnton Kedin <kedin@google.com>\nAnton Kedin <kedin@kedin-macbookpro.roam.corp.google.com>\nArnaud <arnaudfournier921@gmail.com>\nArnaudFnr <arnaudfournier021@gmail.com>\nArnaud Fournier <arnaudfournier921@gmail.com>\nArun Sethia <sethia.arun@gmail.com>\nAsha Rostamianfar <arostami@google.com>\nAustin.Bennett <austin.bennett@mbp.Academy-AP>\nAviem Zur <aviemzur@gmail.com>\nAxel Magnuson <axelmagn@gmail.com>\nBatkhuyag Batsaikhan <batbat@google.com>\nBen Chambers <bchambers@bchambers-macbookpro2.roam.corp.google.com>\nBen Chambers <bchambers@google.com>\nBen Chambers <bjchambers@users.noreply.github.com>\nBen Sidhom <bsidhom@gmail.com>\nBen Sidhom <sidhom@google.com>\nBen Song <songb@google.com>\nBenson Margulies <bimargulies@bimargulies.sea.corp.google.com>\nBenson Margulies <bimargulies@google.com>\nBill Neubauer <wcn@google.com>\nBingfeng Shu <44354306+BingfengShu@users.noreply.github.com>\nBingfeng Shu <bshu@bshu.svl.corp.google.com>\nBorisa Zivkovic <borisa.zivkovic@huawei.com>\nBoyuan Zhang <36090911+boyuanzz@users.noreply.github.com>\nBoyuan Zhang <boyuan@google.com>\nBoyuan Zhang <boyuanz@google.com>\nBraden Bassingthwaite <bbassingthwaite@vendasta.com>\nBrian Foo <bfoo@bfoo-macbookpro.roam.corp.google.com>\nBrian Hulette <bhulette@google.com>\nBrian Hulette <hulettbh@gmail.com>\nBrian Martin <brianmartin@gmail.com>\nBrian Quinlan <brian@sweetapp.com>\nCade Markegard <cademarkegard@gmail.com>\nCao Manh Dat <datcm@apache.org>\nCarl McGraw <carlm@accretivetg.com>\nCarlos Alonso <carlos.alonso@cabify.com>\nChaim Turkel <cyturel@gmail.com>\nChamikara Jayalath <chamikara@apache.org>\nChamikara Jayalath <chamikara@chamikara-linux.svl.corp.google.com>\nChamikara Jayalath <chamikara@google.com>\nChandni Singh <chandni.singh@capitalone.com>\nChang chen <baibaichen@gmail.com>\nCharles Chen <ccy@google.com>\nCharles Chen <charlesccychen@users.noreply.github.com>\nChen Bin <bchen@talend.com>\nChikanaga Tomoyuki <t-chikanaga@groovenauts.jp>\nChinmay Kolhatkar <chinmay@apache.org>\nChris Broadfoot <cbro@golang.org>\nChristian Hudon <chrish@pianocktail.org>\nChristian Schneider <chris@die-schneider.net>\nChuan Yu Foo <1147435+chuanyu@users.noreply.github.com>\nChuan Yu Foo <cyfoo@google.com>\nCindy Kuhn <ckuhn@google.com>\nCody Schroeder <schroederc@google.com>\nColin Phipps <cph@moria.org.uk>\nColin Phipps <fipsy@google.com>\ncolinreid <colinreid@google.com>\nColm O hEigeartaigh <coheigea@apache.org>\nColm O hEigeartaigh <coheigea@users.noreply.github.com>\nConnell O'Callahan <40410951+connelloG@users.noreply.github.com>\nCory Brzycki <supercclank@gmail.com>\nCory Brzycki <supercclank@google.com>\nCraig Chambers <45049052+CraigChambersG@users.noreply.github.com>\nCraig Chambers <chambers@google.com>\nCraig Citro <craigcitro@google.com>\nCristian <me@cristian.io>\nDamien Gouyette <damien.gouyette@gmail.com>\nDan Duong <danduong@google.com>\nDaniel Halperin <daniel@halper.in>\nDaniel Halperin <dhalperi@google.com>\nDaniel Halperin <dhalperi@users.noreply.github.com>\nDaniel Kulp <dkulp@apache.org>\nDaniel Mills <millsd@google.com>\nDaniel Mills <millsd@millsd.sea.corp.google.com>\nDaniel Norberg <dano@spotify.com>\nDaniel Oliveira <daniel.o.programmer@gmail.com>\nDaniel Oliveira <younghoono@gmail.com>\nDan Ringwalt <ringwalt@google.com>\nDariusz Aniszewski <dariusz@aniszewski.eu>\nDariusz Aniszewski <dariusz.aniszewski@polidea.com>\nDat Tran <dattran@sentifi.com>\nDavid Alves <david.alves@cloudera.com>\nDavidB <david.billings@gandlake.com>\nDavid Cavazos <davido262@gmail.com>\nDavid Cavazos <dcavazos@google.com>\nDavid Desberg <david.desberg@uber.com>\nDavid Hrbacek <david.hrbacek@firma.seznam.cz>\nDavid Moravek <david.moravek@firma.seznam.cz>\nDavid Moravek <david.moravek@gmail.com>\nDavid Rieber <drieber@google.com>\nDavid Sabater <david.sabater@gmail.com>\nDavid Sabater Dinter <david.sabater@gmail.com>\nDavid Volquartz Lebech <david@lebech.info>\nDavid Yan <davidyan@apache.org>\nDavor Bonaci <davorbonaci@users.noreply.github.com>\nDavor Bonaci <davor@davor-macbookair.roam.corp.google.com>\nDavor Bonaci <davor@google.com>\nDawid Wysakowicz <dawid@getindata.com>\nDennis Huo <dhuo@google.com>\nDerek Perez <pzd@google.com>\nDevin Donnelly <ddonnelly@google.com>\nDevon Meunier <devon.meunier@shopify.com>\nDipti Kulkarni <dipti_dkulkarni@persistent.co.in>\nDmytro Ivanov <dimon.ivan@gmail.com>\nDusan Rychnovsky <dusan.rychnovsky@firma.seznam.cz>\nDustin Rhodes <dcrhodes@google.com>\nEd Hartwell Goose <ed@mention-me.com>\nElliott Brossard <elliottb@google.com>\nEric Anderson <eandersonm@gmail.com>\nEric Beach <ebeach@google.com>\nEric Roshan-Eisner <ede@alum.mit.edu>\nEric Roshan-Eisner <edre@google.com>\nEtienne Chauchot and Jean-Baptiste Onofré <echauchot@gmail.com+jbonofre@apache.org>\nEtienne Chauchot <echauchot@apache.org>\nEtienne Chauchot <echauchot@gmail.com>\nEugene Kirpichov <ekirpichov@gmail.com>\nEugene Kirpichov <kirpichov@google.com>\nExprosed <larryruili@gmail.com>\nFabien Rousseau <fabien.rousseau@happn.com>\nFlavio Fiszman <flaviocf@flaviocf-macbookpro.roam.corp.google.com>\nFlavio Fiszman <flaviocf@google.com>\nFrances Perry <fjp@google.com>\nFrances Perry <francesperry@users.noreply.github.com>\nFrank Yellin <fy@fyellin.com>\nGareth Western <gareth@garethwestern.com>\nGarrett Jones <garrettjonesgoogle@users.noreply.github.com>\nGaurav Gupta <gaugupt3@cisco.com>\nGeet Kumar <geet.kumar75@gmail.com>\nGeet Kumar <gkumar7@users.noreply.github.com>\nGene Peters <gene@telligent-data.com>\nGergely Novak <gnovak@hortonworks.com>\nGleb Kanterov <gleb@spotify.com>\nGleb Kanterov <kanterov@users.noreply.github.com>\nGlenn Ammons <ammons@google.com>\nGrzegorz Kołakowski <grzegorz.kolakowski@getindata.com>\nGuillaume Blaquiere <guillaume.blaquiere@hrsys.Fr>\nGus Katsiapis <katsiapis@katsiapis-linux.mtv.corp.google.com>\nHadar Hod <hadarh@google.com>\nHai Lu <halu@linkedin.com>\nHarch Vardhan <ananvay@google.com>\nHarsh Vardhan <ananvay2000@yahoo.com>\nHeejong Lee <heejong@gmail.com>\nHenning Rohde <herohde@google.com>\nHenning Rohde <herohde@seekerror.org>\nHenry Deist <hdeist@google.com>\nHenry Suryawirawan <hsuryawirawan@google.com>\nHolden Karau <holdenkarau@google.com>\nHolden Karau <holden@pigscanfly.ca>\nHolden Karau <holden@us.ibm.com>\nHo Tien Vu <ho0001vu@gmail.com>\nHuygaa Batsaikhan <batbat@batbat-linuxworkstation.sea.corp.google.com>\nHuygaa Batsaikhan <batbat@google.com>\nIan Zhou <ianzhou@google.com>\nIgor Bernstein <igorbernstein@google.com>\nIlya Figotin <ifigotin@gmail.com>\nIlya Ganelin <ilya.ganelin@capitalone.com>\nInnocent Djiofack <djiofack007@gmail.com>\nIsmaël Mejía <iemejia@apache.org>\nIsmaël Mejía <iemejia@gmail.com>\nItamar Ostricher <itamarost@gmail.com>\nJack Hsueh <jhsueh@opengov.com>\nJacky <jackyq2015@gmail.com>\nJacob Marble <jmarble@kochava.com>\nJakob Homan <jghoman@gmail.com>\nJames Malone <jamalone@gmail.com>\nJames Malone <jamesmalone@google.com>\nJames Xu <xumingmingv@gmail.com>\nJan Lukavsky <jan.lukavsky@firma.seznam.cz>\nJan Lukavsky <jan.lukavsky@o2.cz>\nJan Lukavsky <je.ik@seznam.cz>\nJan Lukavský <je.ik@seznam.cz>\nJára Vaněk <jaromir.vanek2@firma.seznam.cz>\nJaromir Vanek <vanek.jaromir@gmail.com>\nJason Dobry <jdobry@google.com>\nJason Kuster <jason@google.com>\nJason Kuster <jasonkuster@google.com>\nJason White <jason.white@shopify.com>\nJavier Antonio Gonzalez Trejo <javier.antonio.gonzalez.trejo@gmail.com>\nJavier Moreno <bluelephant@gmail.com>\nJean-Baptiste Onofré <jb@nanthrax.net>\nJean-Baptiste Onofré <jbonofre@apache.org>\nJean-Philippe Martin <jpmartin@google.com>\nJeff Gardner <gardnerj@google.com>\nJeff Klukas <jeff@klukas.net>\nJeffrey Scott Keone Payne <jeffkpayne@gmail.com>\nJeremie Lenfant-Engelmann <jeremiele@google.com>\nJeremy Hurwitz <hurwitz@google.com>\nJeremy Lewi <jlewi@google.com>\nJeremy Weinstein <jeremydw@gmail.com>\nJeroen Steggink <jsteggink@users.noreply.github.com>\nJesse Anderson <jesse@smokinghand.com>\nJianfeng Qian <Jianfeng.Qian@outlook.com>\nJiJun Tang <tangjijun@yhd.com>\nJingsong Li <lzljs3620320@aliyun.com>\nJins George <jins.george@aeris.net>\nJoachim van der Herten <joachim.vanderherten@ugent.be>\nJoão Cabrita <kewne@protonmail.com>\nJoar Wandborg <joar@wandborg.se>\nJoey Baruch <joey.baruch@gmail.com>\nJohn MacMillan <johnmac@ca.ibm.com>\nJosh <joshformangornall@gmail.com>\nJoshua Litt <joshualitt@google.com>\nJoshua Litt <joshualitt@joshualitt.mtv.corp.google.com>\nJosh Wills <josh.wills@gmail.com>\nJosh Wills <jwills@cloudera.com>\nJozef Vilcek <Jozef.Vilcek@sizmek.com>\nJozef Vilcek <jozo.vilcek@gmail.com>\nJuan Rael <juan@qlogic.io>\nJulien Phalip <jphalip@gmail.com>\nJulien Tournay <boudhevil@gmail.com>\nJuliet Hougland <juliet@cloudera.com>\nJustin Tumale <fjetumale@gmail.com>\nKadir Cetinkaya <kadircet@google.com>\nKai Jiang <jiangkai@gmail.com>\nKamil Szewczyk <szewinho@gmail.com>\nKaren Nino <knino@google.com>\nKasia Kucharczyk <2536609+kkucharc@users.noreply.github.com>\nKasia Kucharczyk <katarzyna.kucharczyk@polidea.com>\nKeiji Yoshida <keijiyoshida.mail@gmail.com>\nKeisuke Kondo <keisuke.kondo@istellar.jp>\nKeith McNeill <mcneill@anibla.net>\nKelly Westbrooks <kwestbrooks@google.com>\nKengo Seki <sekikn@apache.org>\nKenneth Jung <kmj@google.com>\nKenneth Knowles <kenn@apache.org>\nKenneth Knowles <kenn@kennknowles.com>\nKenneth Knowles <klk@google.com>\nKevin Graney <kmg@google.com>\nKevin Graney <nanonet@gmail.com>\nKevin Peterson <kpeterson@nestlabs.com>\nKevin Si <kevinsi@google.com>\nKevin Sookocheff <kevin.sookocheff@workiva.com>\nKirill Kozlov <kozlov.k.e@gmail.com>\nKobi Salant <kobi.salant@gmail.com>\nKobi Salant <ksalant@payapal.com>\nKostas Kloudas <kkloudas@gmail.com>\nKris Hildrum <hildrum@google.com>\nKrzysztof Trubalski <k.trubalski@ocado.com>\nKurt Kluever <kak@google.com>\nKyle Weaver <kcweaver@google.com>\nKyle Winkelman <kyle.winkelman@optum.com>\nLara Schmidt <laraschmidt@google.com>\nLeen Toelen <leen.toelen@tomtom.com>\nLeen Toelen <toelen@gmail.com>\nLevi Bowman <lbowman@gmail.com>\nLiam Miller-Cushon <cushon@google.com>\nLiang Zhang <liangzhang@google.com>\nLogan HAUSPIE <logan.hauspie.pro@gmail.com>\nLorenzo Caggioni <lorenzo.caggioni@gmail.com>\nLucas Amorim <lucasamorim@Lucass-MacBook-Pro.local>\nLucas Amorim <lucasamorim@Lucass-MBP.hitronhub.home>\nLucas Amorim <lucasamorim@protonmail.com>\nLuis Enrique Ortíz Ramirez <luisortramirez2211@gmail.com>\nLuis Osa <luis.osa.gdc@gmail.com>\nLukas Drbal <lukas.drbal@gmail.com>\nLukasz Cwik <lcwik@google.com>\nLukasz Cwik <lukecwik@gmail.com>\nŁukasz Gajowy <lukasz.gajowy@gmail.com>\nŁukasz Gajowy <lukasz.gajowy@polidea.com>\nLuke Cwik <lcwik@google.com>\nLuke Cwik <lcwik@visitor-lcwik.wat.corp.google.com>\nLuke Cwik <lukecwik@gmail.com>\nLuke Zhu <luke.l.zhu@gmail.com>\nLuke Zhu <luke_zhu@brown.edu>\nMagnus Runesson <magru@spotify.com>\nMagnus Runesson <M.Runesson@gmail.com>\nMairbek Khadikov <mairbek@google.com>\nMairbek Khadikov <mkhadikov@gmail.com>\nMalo Denielou <malo@google.com>\nManuel Fahndrich <fahndrich@google.com>\nManu Zhang <owenzhang1990@gmail.com>\nManu Zhang <OwenZhang1990@gmail.com>\nMarco Buccini <mbuccini@talend.com>\nMarek Simunek <marek.simunek@firma.seznam.cz>\nMarek Simunek <marek-simunek@seznam.cz>\nMaria Garcia Herrero <mariagh@google.com>\nMaría García Herrero <mariagh@mariagh.svl.corp.google.com>\nMarian Dvorsky <mariand@google.com>\nMaria Python <mariapython@users.noreply.github.com>\nMark Daoust <markdaoust@google.com>\nMark Liu <markflyhigh@users.noreply.github.com>\nMark Liu <markliu@google.com>\nMark Liu <markliu@markliu0.mtv.corp.google.com>\nMark Liu <markliu@markliu-macbookpro.roam.corp.google.com>\nMark Liu <markliu@markliu.svl.corp.google.com>\nMark Shields <markshields@google.com>\nMārtiņš Kalvāns <martins.kalvans@gmail.com>\nMartin Suchanek <mrtn@nrd.io>\nMárton Elek <elek@users.noreply.github.com>\nMathieu Blanchard <mathieu.blanchard@happn.fr>\nMatt Austern <austern@google.com>\nMatthew Jones <mlety2@gmail.com>\nMatthias Baetens <baetensmatthias@gmail.com>\nMatthias Feys <matthiasfeys@gmail.com>\nMatthias Feys <matthiasfeys@hotmail.com>\nMatthias Feys <matthias@ml6.eu>\nMatthias Wessendorf <matzew@apache.org>\nMatt Lang <mattlang@google.com>\nMatt Lee <mattl@users.noreply.github.com>\nMaximilian Michels <max@posteo.de>\nMaximilian Michels <mxm@apache.org>\nMax <max@posteo.de>\nMax Shytikov <mshytikov@gmail.com>\nMelissa Pashniak <melissapa@google.com>\nMergebot <mergebot@apache.org>\nMicah Wylde <micah@micahw.com>\nMicah Wylde <mwylde@lyft.com>\nMichael Luckey <25622840+adude3141@users.noreply.github.com>\nMichael Luckey <michael.luckey@ext.gfk.com>\nMichal Walenia <32354134+mwalenia@users.noreply.github.com>\nMichal Walenia <michal.walenia@polidea.com>\nMike Pedersen <mike@mikepedersen.dk>\nMike Pedersen <noctune9@gmail.com>\nMikhail Gryzykhin <12602502+Ardagan@users.noreply.github.com>\nMikhail Gryzykhin <gryzykhin.mikhail@gmail.com>\nMikhail Gryzykhin <migryz@google.com>\nMikhail Shmulyan <mshmulyan@google.com>\nMiles Saul <msaul@google.com>\nMiles Saul <msaul@msaul0.wat.corp.google.com>\nMitch Shanklin <mshanklin@google.com>\nMotty Gruda <Mottyg1@gmail.com>\nNathan Howell <nhowell@godaddy.com>\nNawaid Shamim <nawaid.shamim@bbc.co.uk>\nNeda Mirian <nedam@google.com>\nNeda Mirian <neda.mirian@gmail.com>\nNeelesh Srinivas Salian <nsalian@cloudera.com>\nNeville Li <neville.lyh@gmail.com>\nNeville Li <neville@spotify.com>\nNiel Markwick <nielm@google.com>\nNiel Markwick <nielm@users.noreply.github.com>\nNiels Basjes <nbasjes@bol.com>\nNiels Basjes <niels@basjes.nl>\nNigel Kilmer <nkilmer@google.com>\nOle Langbehn <ole.langbehn@inoio.de>\nOndrej Kvasnicka <ondrej.kvasnicka@firma.seznam.cz>\nPablo Estrada <pabloem@google.com>\nPablo Estrada <pabloem@users.noreply.github.com>\nPascal Gula <pascal.gula@gmail.com>\nPastuszka Przemysław <pastuszka.przemyslaw@gmail.com>\nPaul Gerver <pfgerver@gmail.com>\nPaul Gerver <pgerver@us.ibm.com>\nPaulVelthuis93 <paulvelthuis93@gmail.com>\nPavel Slechta <pavel.slechta@firma.seznam.cz>\nPawel Kaczmarczyk <pawel.pk.kaczmarczyk@gmail.com>\nPawel Kaczmarczyk <p.kaczmarczyk@ocado.com>\nPei He <hepei.hp@alibaba-inc.com>\nPei He <hepeimail@gmail.com>\nPei He <pei@apache.org>\nPei He <peihe0@gmail.com>\nPei He <peihe@google.com>\nPei He <peihe@users.noreply.github.com>\nPeter Gergo Barna <pbarna@hortonworks.com>\nPetr Novotnik <petr.novotnik@firma.seznam.cz>\nPetr Shevtsov <petr.shevtsov@gmail.com>\nPramod Immaneni <pramod@datatorrent.com>\nPrateek Chanda <prateekkol21@gmail.com>\nPrem Kumar Karunakaran <p.karunakaran@metrosystems.net>\nRadhika S Kulkarni <radhika_kulkarni1@persistent.co.in>\nRafael Fernández <rfernand@google.com>\nRafal Wojdyla <rav@spotify.com>\nRafal Wojdyla <ravwojdyla@gmail.com>\nRaghu Angadi <rangadi@apache.org>\nRaghu Angadi <rangadi@google.com>\nRahul Sabbineni <duraza@users.noreply.github.com>\nRenat <regata@users.noreply.github.com>\nReuven Lax <relax@google.com>\nReuven Lax <relax@relax-macbookpro2.roam.corp.google.com>\nReuven Lax <relax@relax-macbookpro.roam.corp.google.com>\nRezan Achmad <rezanachmad@gmail.com>\nReza Rokni <7542791+rezarokni@users.noreply.github.com>\nReza Rokni <rezarokni@google.com>\nRobbe Sneyders <robbe.sneyders@gmail.com>\nRobbe Sneyders <robbe.sneyders@ml6.eu>\nRob Earhart <earhart@gmail.com>\nRob Earhart <earhart@google.com>\nRobert Bradshaw <robertwb@gmail.com>\nRobert Bradshaw <robertwb@google.com>\nRobert Burke <lostluck@users.noreply.github.com>\nRobert Burke <rober@frantil.com>\nRobert Burke <robert@frantil.com>\nRoberto Congiu <rcongiu@agentace.com>\nRobin Qiu <robinyq@rodete-desktop-imager.corp.google.com>\nRodrigo Benenson <rodrigo.benenson@gmail.com>\nRomain Yon <yonromai@users.noreply.github.com>\nRong Ou <rong.ou@gmail.com>\nRoy Lenferink <lenferinkroy@gmail.com>\nRui Wang <amaliujia@163.com>\nRui Wang <amaliujia@gmail.com>\nRui Wang <amaliujia@users.noreply.github.com>\nRune Fevang <fevang@exabel.com>\nRuoyu Liu <ruoyu@google.com>\nRuoyun Huang <huangry@gmail.com>\nRyan Culbertson <ryan@spotify.com>\nRyan Niemocienski <niemo@google.com>\nRyan Skraba <ryan@skraba.com>\nRyan Williams <ryan.blake.williams@gmail.com>\nsabhyankar <abhyankar@gmail.com>\nSam McVeety <sam.mcveety@gmail.com>\nSam McVeety <sgmc@google.com>\nSam Rohde <rohde.samuel@gmail.com>\nSam Waggoner <samuel.waggoner@healthsparq.com>\nSam Whittle <samuelw@google.com>\nSam Whittle <scwhittle@users.noreply.github.com>\nSandeep Deshmukh <sandeep@datatorrent.com>\nSandeep Parikh <sandeep@clusterbeep.org>\nScott Wegner <scott@apache.com>\nScott Wegner <scott@apache.org>\nScott Wegner <swegner2@gmail.com>\nScott Wegner <swegner@google.com>\nScott Wegner <swegner@outlook.com>\nSean O'Keefe <seano314@users.noreply.github.com>\nSean Owen <sowen@cloudera.com>\nSean Owen <srowen@gmail.com>\nSela <ansela@paypal.com>\nSergei Lebedev <s.lebedev@criteo.com>\nSergey Beryozkin <sberyozkin@gmail.com>\nSergio Fernández <sergio@wikier.org>\nSergiy Byelozyorov <sergiyb@chromium.org>\nSeshadri Chakkravarthy <sesh.cr@gmail.com>\nSeunghyun Lee <shlee0605@gmail.com>\nShashank Prabhakara <shashank@infoworks.io>\nShinsuke Sugaya <shinsuke@apache.org>\nShnitz <andrewktan@gmail.com>\nSilviu Calinoiu <silviuc@google.com>\nSimon Plovyt <40612002+splovyt@users.noreply.github.com>\nSindy Li <qinyeli@qinyeli.svl.corp.google.com>\nSindy Li <qinyeli@umich.edu>\nSlava Chernyak <chernyak@google.com>\nSlaven Bilac <slaven@google.com>\nSokolovMS <m.s.sokolov.92@gmail.com>\nSolomon Duskis <sduskis@google.com>\nSourabh Bajaj <sb2nov@gmail.com>\nSourabh Bajaj <sourabhbajaj@google.com>\nStas Levin <staslevin@apache.org>\nStas Levin <staslevin@gmail.com>\nStas Levin <staslev@users.noreply.github.com>\nStefano Baghino <stefano@baghino.me>\nStepan Kadlec <stepan.kadlec@oracle.com>\nStephan Ewen <sewen@apache.org>\nStephan Hoyer <shoyer@google.com>\nStephen Gildea <gildea@google.com>\nStephen Lumenta <stephen.lumenta@gmail.com>\nStephen Sisk <sisk@google.com>\nStephen Sisk <ssisk@users.noreply.github.com>\nSteve Niemitz <sniemitz@twitter.com>\nSteve Wheeler <stevewheeler@google.com>\nStijn Decubber <stijn.decubber@ml6.eu>\nSumit Chawla <sumichaw@cisco.com>\nSunil Pedapudi <skpedapudi@gmail.com>\nTaro Murao <taro.murao@gmail.com>\nTed Yu <yuzhihong@gmail.com>\nTeng Peng <josephtengpeng@gmail.com>\nTheodore Siu <theosiu@theosiu-macbookpro24.roam.corp.google.com>\nThomas Groh <tgroh@google.com>\nThomas Groh <tgroh@users.noreply.github.com>\nThomas Weise <thw@apache.org>\nThomas Weise <tweise@lyft.com>\nThomas Weise <tweise@users.noreply.github.com>\nTianyang Hu <htyleo@gmail.com>\nTibor Kiss <tibor.kiss@gmail.com>\nTim Robertson <timrobertson100@gmail.com>\nTim Robertson <timrobertson100@gmial.com>\nTim Sears <sears.tim@gmail.com>\nTobias Feldhaus <tobias.feldhaus@localsearch.ch>\nTomas Novak <tomas.novak@firma.seznam.cz>\nTomas Roos <ptomasroos@gmail.com>\nTom Haines <thomas.haines@practiceinsight.io>\nTom White <tom@cloudera.com>\nTudor Marian <tudorm@google.com>\nTyler Akidau <takidau@apache.org>\nTyler Akidau <takidau@google.com>\nUdi Meiri <ehudm@google.com>\nUdi Meiri (Ehud) <udim@users.noreply.github.com>\nUdi Meiri <udim@users.noreply.github.com>\nUri Silberstein <uri.silberstein@gmail.com>\nUwe Jugel <uwe.jugel@lovoo.com>\nVaclav Plajt <vaclav.plajt@firma.seznam.cz>\nVaclav Plajt <vaclav.plajt@gmail.com>\nValentyn Tymofieiev <valentyn@google.com>\nValient Gough <vgough@google.com>\nVarun Dhussa <varundhussa@google.com>\nVassil Kolarov <vas@vas.io>\nVikas Kedigehalli <vikasrk@google.com>\nVitalii Tverdokhlib <vitaliytv@nitralabs.com>\nVladisav Jelisavcic <vj@apache.org>\nVojtech Janota <vojtech.janota@oracle.com>\nWard Van Assche <ward@piesync.com>\nWesley Tanaka <wtanaka@yahoo.com>\n성준영 <wnsdud1861@gmail.com>\nWon Wook SONG <wonook@apache.org>\nWout Scheepers <Wout.Scheepers@vente-exclusive.com>\nwslulciuc <willy@bounceexchange.com>\nXin Wang <xinwang@apache.org>\nXinyu Liu <xiliu@linkedin.com>\nXinyu Liu <xiliu@xiliu-ld1.linkedin.biz>\nXinyu Liu <xinyuliu.us@gmail.com>\nYifan Zou <35050780+yifanzou@users.noreply.github.com>\nYifan Zou <yifanzou@google.com>\nYifan Zou <yifanzou@rodete-desktop-imager.corp.google.com>\nYifan Zou <yifanzou@yifanzou-linuxworkstation.sea.corp.google.com>\nYifan Zou <yifanzou@yifanzou-macbookpro.roam.corp.google.com>\nYounghee Kwon <younghee.kwon@gmail.com>\nYuan (Terry) Tang <terrytangyuan@gmail.com>\nYueyang Qiu <robinyqiu@gmail.com>\nYunqing Zhou <zhouyunqing@zhouyunqing-macbookpro3.roam.corp.google.com>\nYunqing Zhou <zhouyunqing@zhouyunqing-macbookpro.roam.corp.google.com>\nZang <szang@lm-sea-11001278.corp.ebay.com>\nZhuo Peng <1835738+brills@users.noreply.github.com>\nzhuoyao <zhuoyao@google.com>\nZohar Yahav <zoy@giggles.nyc.corp.google.com>\nZohar Yahav <zoy@smtp.corp.google.com>\nZongwei Zhou <zongweiz@google.com>\nZur, Aviem <azur@paypal.com>\n波特 <haozhi.shz@alibaba-inc.com>\n琨瑜 <yiyan.lyy@alibaba-inc.com>\n"
        },
        {
          "name": ".pre-commit-config.yaml",
          "type": "blob",
          "size": 2.0341796875,
          "content": "# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nrepos:\n  - repo: https://github.com/pre-commit/mirrors-yapf\n    # this rev is a release tag in the repo above and corresponds with a yapf\n    # version. make sure this matches the version of yapf in tox.ini.\n    rev: v0.29.0\n    hooks:\n      - id: yapf\n        files: ^sdks/python/apache_beam/\n        # keep these in sync with sdks/python/.yapfignore  and run_pylint.sh\n        exclude: &exclude >\n            (?x)^(\n                sdks/python/apache_beam/io/gcp/internal/clients/bigquery/bigquery_v2_client.py|\n                sdks/python/apache_beam/io/gcp/internal/clients/bigquery/bigquery_v2_messages.py|\n                sdks/python/apache_beam/runners/dataflow/internal/clients/dataflow/dataflow_v1b3_client.py|\n                sdks/python/apache_beam/runners/dataflow/internal/clients/dataflow/dataflow_v1b3_messages.py|\n                sdks/python/apache_beam/io/gcp/internal/clients/storage/storage_v1_client.py|\n                sdks/python/apache_beam/io/gcp/internal/clients/storage/storage_v1_messages.py|\n                sdks/python/apache_beam/coders/proto2_coder_test_messages_pb2.py|\n                sdks/python/apache_beam/portability/api/.*pb2.*.py\n            )$\n\n  - repo: https://github.com/pycqa/pylint\n    # this rev is a release tag in the repo above and corresponds with a pylint\n    # version. make sure this matches the version of pylint in tox.ini.\n    rev: v2.17.5\n    hooks:\n      - id: pylint\n        args: [\"--rcfile=sdks/python/.pylintrc\"]\n        files: ^sdks/python/apache_beam/\n        exclude: *exclude\n"
        },
        {
          "name": ".test-infra",
          "type": "tree",
          "content": null
        },
        {
          "name": ".yamllint.yml",
          "type": "blob",
          "size": 0.943359375,
          "content": "# Licensed to the Apache Software Foundation (ASF) under one or more\n# contributor license agreements. See the NOTICE file distributed with\n# this work for additional information regarding copyright ownership.\n# The ASF licenses this file to You under the Apache License, Version 2.0\n# (the \"License\"); you may not use this file except in compliance with\n# the License. You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nextends: default\n\nrules:\n  # Change line length from error to warning\n  line-length:\n    level: warning\n\n  # Disabling the document-start error messages\n  document-start:\n    present: false\n"
        },
        {
          "name": "CHANGES.md",
          "type": "blob",
          "size": 145.3916015625,
          "content": "<!--\n    Licensed to the Apache Software Foundation (ASF) under one\n    or more contributor license agreements.  See the NOTICE file\n    distributed with this work for additional information\n    regarding copyright ownership.  The ASF licenses this file\n    to you under the Apache License, Version 2.0 (the\n    \"License\"); you may not use this file except in compliance\n    with the License.  You may obtain a copy of the License at\n\n      http://www.apache.org/licenses/LICENSE-2.0\n\n    Unless required by applicable law or agreed to in writing,\n    software distributed under the License is distributed on an\n    \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n    KIND, either express or implied.  See the License for the\n    specific language governing permissions and limitations\n    under the License.\n-->\n<!-- Template -->\n<!--\n# [2.XX.X] - Unreleased\n\n## Highlights\n\n* New highly anticipated feature X added to Python SDK ([#X](https://github.com/apache/beam/issues/X)).\n* New highly anticipated feature Y added to Java SDK ([#Y](https://github.com/apache/beam/issues/Y)).\n\n## I/Os\n\n* Support for X source added (Java/Python) ([#X](https://github.com/apache/beam/issues/X)).\n\n## New Features / Improvements\n\n* X feature added (Java/Python) ([#X](https://github.com/apache/beam/issues/X)).\n\n## Breaking Changes\n\n* X behavior was changed ([#X](https://github.com/apache/beam/issues/X)).\n\n## Deprecations\n\n* X behavior is deprecated and will be removed in X versions ([#X](https://github.com/apache/beam/issues/X)).\n\n## Bugfixes\n\n* Fixed X (Java/Python) ([#X](https://github.com/apache/beam/issues/X)).\n\n## Security Fixes\n* Fixed (CVE-YYYY-NNNN)[https://www.cve.org/CVERecord?id=CVE-YYYY-NNNN] (Java/Python/Go) ([#X](https://github.com/apache/beam/issues/X)).\n\n## Known Issues\n\n* ([#X](https://github.com/apache/beam/issues/X)).\n-->\n\n# [2.63.0] - Unreleased\n\n## Highlights\n\n* New highly anticipated feature X added to Python SDK ([#X](https://github.com/apache/beam/issues/X)).\n* New highly anticipated feature Y added to Java SDK ([#Y](https://github.com/apache/beam/issues/Y)).\n\n## I/Os\n\n* Support for X source added (Java/Python) ([#X](https://github.com/apache/beam/issues/X)).\n\n## New Features / Improvements\n\n* X feature added (Java/Python) ([#X](https://github.com/apache/beam/issues/X)).\n* Upgraded to protobuf 4 (Java) ([#33192](https://github.com/apache/beam/issues/33192)).\n* [GCSIO] Added retry logic to each batch method of the GCS IO (Python) ([#33539](https://github.com/apache/beam/pull/33539))\n\n## Breaking Changes\n\n* AWS V1 I/Os have been removed (Java). As part of this, x-lang Python Kinesis I/O has been updated to consume the V2 IO and it also no longer supports setting producer_properties ([#33430](https://github.com/apache/beam/issues/33430)).\n* Upgraded to protobuf 4 (Java) ([#33192](https://github.com/apache/beam/issues/33192)), but forced Debezium IO to use protobuf 3 ([#33541](https://github.com/apache/beam/issues/33541) because Debezium clients are not protobuf 4 compatible. This may cause conflicts when using clients which are only compatible with protobuf 4.\n\n## Deprecations\n\n* X behavior is deprecated and will be removed in X versions ([#X](https://github.com/apache/beam/issues/X)).\n\n## Bugfixes\n\n* Fix data loss issues when reading gzipped files with TextIO (Python) ([#18390](https://github.com/apache/beam/issues/18390), [#31040](https://github.com/apache/beam/issues/31040)).\n* Fixed X (Java/Python) ([#X](https://github.com/apache/beam/issues/X)).\n* [BigQueryIO] Fixed an issue where Storage Write API sometimes doesn't pick up auto-schema updates ([#33231](https://github.com/apache/beam/pull/33231))\n\n## Security Fixes\n* Fixed (CVE-YYYY-NNNN)[https://www.cve.org/CVERecord?id=CVE-YYYY-NNNN] (Java/Python/Go) ([#X](https://github.com/apache/beam/issues/X)).\n\n## Known Issues\n\n* ([#X](https://github.com/apache/beam/issues/X)).\n\n# [2.62.0] - Unreleased\n\n## I/Os\n\n* gcs-connector config options can be set via GcsOptions (Java) ([#32769](https://github.com/apache/beam/pull/32769)).\n* [Managed Iceberg] Support partitioning by time (year, month, day, hour) for types `date`, `time`, `timestamp`, and `timestamp(tz)` ([#32939](https://github.com/apache/beam/pull/32939))\n* Upgraded the default version of Hadoop dependencies to 3.4.1. Hadoop 2.10.2 is still supported (Java) ([#33011](https://github.com/apache/beam/issues/33011)).\n* [BigQueryIO] Create managed BigLake tables dynamically ([#33125](https://github.com/apache/beam/pull/33125))\n\n## New Features / Improvements\n\n* Added support for stateful processing in Spark Runner for streaming pipelines. Timer functionality is not yet supported and will be implemented in a future release ([#33237](https://github.com/apache/beam/issues/33237)).\n* The datetime module is now available for use in jinja templatization for yaml.\n* Improved batch performance of SparkRunner's GroupByKey ([#20943](https://github.com/apache/beam/pull/20943)).\n* Support OnWindowExpiration in Prism ([#32211](https://github.com/apache/beam/issues/32211)).\n  * This enables initial Java GroupIntoBatches support.\n* Support OrderedListState in Prism ([#32929](https://github.com/apache/beam/issues/32929)).\n\n## Breaking Changes\n\n* Upgraded ZetaSQL to 2024.11.1 ([#32902](https://github.com/apache/beam/pull/32902)). Java11+ is now needed if Beam's ZetaSQL component is used.\n\n## Bugfixes\n\n* Fixed EventTimeTimer ordering in Prism. ([#32222](https://github.com/apache/beam/issues/32222)).\n* [Managed Iceberg] Fixed a bug where DataFile metadata was assigned incorrect partition values ([#33549](https://github.com/apache/beam/pull/33549)).\n\n## Security Fixes\n\n* Fixed (CVE-2024-47561)[https://www.cve.org/CVERecord?id=CVE-2024-47561] (Java) by upgrading Avro version to 1.11.4\n\n# [2.61.0] - 2024-11-25\n\n## Highlights\n\n* [Python] Introduce Managed Transforms API ([#31495](https://github.com/apache/beam/pull/31495))\n* Flink 1.19 support added ([#32648](https://github.com/apache/beam/pull/32648))\n\n## I/Os\n\n* [Managed Iceberg] Support creating tables if needed ([#32686](https://github.com/apache/beam/pull/32686))\n* [Managed Iceberg] Now available in Python SDK ([#31495](https://github.com/apache/beam/pull/31495))\n* [Managed Iceberg] Add support for TIMESTAMP, TIME, and DATE types ([#32688](https://github.com/apache/beam/pull/32688))\n* BigQuery CDC writes are now available in Python SDK, only supported when using StorageWrite API at least once mode ([#32527](https://github.com/apache/beam/issues/32527))\n* [Managed Iceberg] Allow updating table partition specs during pipeline runtime ([#32879](https://github.com/apache/beam/pull/32879))\n* Added BigQueryIO as a Managed IO ([#31486](https://github.com/apache/beam/pull/31486))\n* Support for writing to [Solace messages queues](https://solace.com/) (`SolaceIO.Write`) added (Java) ([#31905](https://github.com/apache/beam/issues/31905)).\n\n## New Features / Improvements\n\n* Added support for read with metadata in MqttIO (Java) ([#32195](https://github.com/apache/beam/issues/32195))\n* Added support for processing events which use a global sequence to \"ordered\" extension (Java) ([#32540](https://github.com/apache/beam/pull/32540))\n* Add new meta-transform FlattenWith and Tee that allow one to introduce branching\n  without breaking the linear/chaining style of pipeline construction.\n* Use Prism as a fallback to the Python Portable runner when running a pipeline with the Python Direct runner ([#32876](https://github.com/apache/beam/pull/32876))\n\n## Deprecations\n\n* Removed support for Flink 1.15 and 1.16\n* Removed support for Python 3.8\n\n## Bugfixes\n\n* (Java) Fixed tearDown not invoked when DoFn throws on Portable Runners ([#18592](https://github.com/apache/beam/issues/18592), [#31381](https://github.com/apache/beam/issues/31381)).\n* (Java) Fixed protobuf error with MapState.remove() in Dataflow Streaming Java Legacy Runner without Streaming Engine ([#32892](https://github.com/apache/beam/issues/32892)).\n* Adding flag to support conditionally disabling auto-commit in JdbcIO ReadFn ([#31111](https://github.com/apache/beam/issues/31111))\n* (Python) Fixed BigQuery Enrichment bug that can lead to multiple conditions returning duplicate rows, batching returning incorrect results and conditions not scoped by row during batching ([#32780](https://github.com/apache/beam/pull/32780)).\n\n## Known Issues\n\n* [Managed Iceberg] DataFile metadata is assigned incorrect partition values ([#33497](https://github.com/apache/beam/issues/33497)).\n  * Fixed in 2.62.0\n\n# [2.60.0] - 2024-10-17\n\n## Highlights\n\n* Added support for using vLLM in the RunInference transform (Python) ([#32528](https://github.com/apache/beam/issues/32528))\n* [Managed Iceberg] Added support for streaming writes ([#32451](https://github.com/apache/beam/pull/32451))\n* [Managed Iceberg] Added auto-sharding for streaming writes ([#32612](https://github.com/apache/beam/pull/32612))\n* [Managed Iceberg] Added support for writing to dynamic destinations ([#32565](https://github.com/apache/beam/pull/32565))\n\n## I/Os\n\n* PubsubIO can validate that the Pub/Sub topic exists before running the Read/Write pipeline (Java) ([#32465](https://github.com/apache/beam/pull/32465))\n\n## New Features / Improvements\n\n* Dataflow worker can install packages from Google Artifact Registry Python repositories (Python) ([#32123](https://github.com/apache/beam/issues/32123)).\n* Added support for Zstd codec in SerializableAvroCodecFactory (Java) ([#32349](https://github.com/apache/beam/issues/32349))\n* Added support for using vLLM in the RunInference transform (Python) ([#32528](https://github.com/apache/beam/issues/32528))\n* Prism release binaries and container bootloaders are now being built with the latest Go 1.23 patch. ([#32575](https://github.com/apache/beam/pull/32575))\n* Prism\n  * Prism now supports Bundle Finalization. ([#32425](https://github.com/apache/beam/pull/32425))\n* Significantly improved performance of Kafka IO reads that enable [commitOffsetsInFinalize](https://beam.apache.org/releases/javadoc/current/org/apache/beam/sdk/io/kafka/KafkaIO.Read.html#commitOffsetsInFinalize--) by removing the data reshuffle from SDF implementation.  ([#31682](https://github.com/apache/beam/pull/31682)).\n* Added support for dynamic writing in MqttIO (Java) ([#19376](https://github.com/apache/beam/issues/19376))\n* Optimized Spark Runner parDo transform evaluator (Java) ([#32537](https://github.com/apache/beam/issues/32537))\n* [Managed Iceberg] More efficient manifest file writes/commits ([#32666](https://github.com/apache/beam/issues/32666))\n\n## Breaking Changes\n\n* In Python, assert_that now throws if it is not in a pipeline context instead of silently succeeding ([#30771](https://github.com/apache/beam/pull/30771))\n* In Python and YAML, ReadFromJson now override the dtype from None to\n  an explicit False.  Most notably, string values like `\"123\"` are preserved\n  as strings rather than silently coerced (and possibly truncated) to numeric\n  values.  To retain the old behavior, pass `dtype=True` (or any other value\n  accepted by `pandas.read_json`).\n* Users of KafkaIO  Read transform that enable [commitOffsetsInFinalize](https://beam.apache.org/releases/javadoc/current/org/apache/beam/sdk/io/kafka/KafkaIO.Read.html#commitOffsetsInFinalize--) might encounter pipeline graph  compatibility issues when updating the pipeline. To mitigate, set the `updateCompatibilityVersion` option to the SDK version used for the original pipeline, example `--updateCompatabilityVersion=2.58.1`\n\n## Deprecations\n\n* Python 3.8 is reaching EOL and support is being removed in Beam 2.61.0. The 2.60.0 release will warn users\nwhen running on 3.8. ([#31192](https://github.com/apache/beam/issues/31192))\n\n## Bugfixes\n\n* (Java) Fixed custom delimiter issues in TextIO ([#32249](https://github.com/apache/beam/issues/32249), [#32251](https://github.com/apache/beam/issues/32251)).\n* (Java, Python, Go) Fixed PeriodicSequence backlog bytes reporting, which was preventing Dataflow Runner autoscaling from functioning properly ([#32506](https://github.com/apache/beam/issues/32506)).\n* (Java) Fix improper decoding of rows with schemas containing nullable fields when encoded with a schema with equal encoding positions but modified field order. ([#32388](https://github.com/apache/beam/issues/32388)).\n* (Java) Skip close on bundles in BigtableIO.Read ([#32661](https://github.com/apache/beam/pull/32661), [#32759](https://github.com/apache/beam/pull/32759)).\n\n## Known Issues\n\n* BigQuery Enrichment (Python):  The following issues are present when using the BigQuery enrichment transform ([#32780](https://github.com/apache/beam/pull/32780)):\n  * Duplicate Rows: Multiple conditions may be applied incorrectly, leading to the duplication of rows in the output.\n  * Incorrect Results with Batched Requests: Conditions may not be correctly scoped to individual rows within the batch, potentially causing inaccurate results.\n  * Fixed in 2.61.0.\n* [Managed Iceberg] DataFile metadata is assigned incorrect partition values ([#33497](https://github.com/apache/beam/issues/33497)).\n  * Fixed in 2.62.0\n\n# [2.59.0] - 2024-09-11\n\n## Highlights\n\n* Added support for setting a configureable timeout when loading a model and performing inference in the [RunInference](https://beam.apache.org/documentation/ml/inference-overview/) transform using [with_exception_handling](https://beam.apache.org/releases/pydoc/current/apache_beam.ml.inference.base.html#apache_beam.ml.inference.base.RunInference.with_exception_handling) ([#32137](https://github.com/apache/beam/issues/32137))\n* Initial experimental support for using Prism with the Java and Python SDKs\n  * Prism is presently targeting local testing usage, or other small scale execution.\n  * For Java, use 'PrismRunner', or 'TestPrismRunner' as an argument to the `--runner` flag.\n  * For Python, use 'PrismRunner' as an argument to the `--runner` flag.\n  * Go already uses Prism as the default local runner.\n\n## I/Os\n\n* Improvements to the performance of BigqueryIO when using withPropagateSuccessfulStorageApiWrites(true) method (Java) ([#31840](https://github.com/apache/beam/pull/31840)).\n* [Managed Iceberg] Added support for writing to partitioned tables ([#32102](https://github.com/apache/beam/pull/32102))\n* Update ClickHouseIO to use the latest version of the ClickHouse JDBC driver ([#32228](https://github.com/apache/beam/issues/32228)).\n* Add ClickHouseIO dedicated User-Agent ([#32252](https://github.com/apache/beam/issues/32252)).\n\n## New Features / Improvements\n\n* BigQuery endpoint can be overridden via PipelineOptions, this enables BigQuery emulators (Java) ([#28149](https://github.com/apache/beam/issues/28149)).\n* Go SDK Minimum Go Version updated to 1.21 ([#32092](https://github.com/apache/beam/pull/32092)).\n* [BigQueryIO] Added support for withFormatRecordOnFailureFunction() for STORAGE_WRITE_API and STORAGE_API_AT_LEAST_ONCE methods (Java) ([#31354](https://github.com/apache/beam/issues/31354)).\n* Updated Go protobuf package to new version (Go) ([#21515](https://github.com/apache/beam/issues/21515)).\n* Added support for setting a configureable timeout when loading a model and performing inference in the [RunInference](https://beam.apache.org/documentation/ml/inference-overview/) transform using [with_exception_handling](https://beam.apache.org/releases/pydoc/current/apache_beam.ml.inference.base.html#apache_beam.ml.inference.base.RunInference.with_exception_handling) ([#32137](https://github.com/apache/beam/issues/32137))\n* Adds OrderedListState support for Java SDK via FnApi.\n* Initial support for using Prism from the Python and Java SDKs.\n\n## Bugfixes\n\n* Fixed incorrect service account impersonation flow for Python pipelines using BigQuery IOs ([#32030](https://github.com/apache/beam/issues/32030)).\n* Auto-disable broken and meaningless `upload_graph` feature when using Dataflow Runner V2 ([#32159](https://github.com/apache/beam/issues/32159)).\n* (Python) Upgraded google-cloud-storage to version 2.18.2 to fix a data corruption issue ([#32135](https://github.com/apache/beam/pull/32135)).\n* (Go) Fix corruption on State API writes. ([#32245](https://github.com/apache/beam/issues/32245)).\n\n## Known Issues\n\n* Prism is under active development and does not yet support all pipelines. See [#29650](https://github.com/apache/beam/issues/29650) for progress.\n   * In the 2.59.0 release, Prism passes most runner validations tests with the exceptions of pipelines using the following features:\n   OrderedListState, OnWindowExpiry (eg. GroupIntoBatches), CustomWindows, MergingWindowFns, Trigger and WindowingStrategy associated features, Bundle Finalization, Looping Timers, and some Coder related issues such as with Python combiner packing, and Java Schema transforms, and heterogenous flatten coders. Processing Time timers do not yet have real time support.\n   * If your pipeline is having difficulty with the Python or Java direct runners, but runs well on Prism, please let us know.\n\n* Java file-based IOs read or write lots (100k+) files could experience slowness and/or broken metrics visualization on Dataflow UI [#32649](https://github.com/apache/beam/issues/32649).\n* BigQuery Enrichment (Python):  The following issues are present when using the BigQuery enrichment transform ([#32780](https://github.com/apache/beam/pull/32780)):\n  * Duplicate Rows: Multiple conditions may be applied incorrectly, leading to the duplication of rows in the output.\n  * Incorrect Results with Batched Requests: Conditions may not be correctly scoped to individual rows within the batch, potentially causing inaccurate results.\n  * Fixed in 2.61.0.\n* [Managed Iceberg] DataFile metadata is assigned incorrect partition values ([#33497](https://github.com/apache/beam/issues/33497)).\n  * Fixed in 2.62.0\n\n# [2.58.1] - 2024-08-15\n\n## New Features / Improvements\n\n* Fixed issue where KafkaIO Records read with `ReadFromKafkaViaSDF` are redistributed and may contain duplicates regardless of the configuration. This affects Java pipelines with Dataflow v2 runner and xlang pipelines reading from Kafka, ([#32196](https://github.com/apache/beam/issues/32196))\n\n## Known Issues\n\n* Large Dataflow graphs using runner v2, or pipelines explicitly enabling the `upload_graph` experiment, will fail at construction time ([#32159](https://github.com/apache/beam/issues/32159)).\n* Python pipelines that run with 2.53.0-2.58.0 SDKs and read data from GCS might be affected by a data corruption issue ([#32169](https://github.com/apache/beam/issues/32169)). The issue will be fixed in 2.59.0 ([#32135](https://github.com/apache/beam/pull/32135)). To work around this, update the google-cloud-storage package to version 2.18.2 or newer.\n* BigQuery Enrichment (Python):  The following issues are present when using the BigQuery enrichment transform ([#32780](https://github.com/apache/beam/pull/32780)):\n  * Duplicate Rows: Multiple conditions may be applied incorrectly, leading to the duplication of rows in the output.\n  * Incorrect Results with Batched Requests: Conditions may not be correctly scoped to individual rows within the batch, potentially causing inaccurate results.\n  * Fixed in 2.61.0.\n\n# [2.58.0] - 2024-08-06\n\n## Highlights\n\n* Support for [Solace](https://solace.com/) source (`SolaceIO.Read`) added (Java) ([#31440](https://github.com/apache/beam/issues/31440)).\n\n## New Features / Improvements\n\n* Multiple RunInference instances can now share the same model instance by setting the model_identifier parameter (Python) ([#31665](https://github.com/apache/beam/issues/31665)).\n* Added options to control the number of Storage API multiplexing connections ([#31721](https://github.com/apache/beam/pull/31721))\n* [BigQueryIO] Better handling for batch Storage Write API when it hits AppendRows throughput quota ([#31837](https://github.com/apache/beam/pull/31837))\n* [IcebergIO] All specified catalog properties are passed through to the connector ([#31726](https://github.com/apache/beam/pull/31726))\n* Removed a 3rd party LGPL dependency from the Go SDK ([#31765](https://github.com/apache/beam/issues/31765)).\n* Support for MapState and SetState when using Dataflow Runner v1 with Streaming Engine (Java) ([[#18200](https://github.com/apache/beam/issues/18200)])\n\n## Breaking Changes\n\n* [IcebergIO] IcebergCatalogConfig was changed to support specifying catalog properties in a key-store fashion ([#31726](https://github.com/apache/beam/pull/31726))\n* [SpannerIO] Added validation that query and table cannot be specified at the same time for SpannerIO.read(). Previously withQuery overrides withTable, if set ([#24956](https://github.com/apache/beam/issues/24956)).\n\n## Bugfixes\n\n* [BigQueryIO] Fixed a bug in batch Storage Write API that frequently exhausted concurrent connections quota ([#31710](https://github.com/apache/beam/pull/31710))\n* Fixed a logging issue where Python worker dependency installation logs sometimes were not emitted in a timely manner ([#31977](https://github.com/apache/beam/pull/31977))\n\n## Known Issues\n\n* Large Dataflow graphs using runner v2, or pipelines explicitly enabling the `upload_graph` experiment, will fail at construction time ([#32159](https://github.com/apache/beam/issues/32159)).\n* Python pipelines that run with 2.53.0-2.58.0 SDKs and read data from GCS might be affected by a data corruption issue ([#32169](https://github.com/apache/beam/issues/32169)). The issue will be fixed in 2.59.0 ([#32135](https://github.com/apache/beam/pull/32135)). To work around this, update the google-cloud-storage package to version 2.18.2 or newer.\n* [KafkaIO] Records read with `ReadFromKafkaViaSDF` are redistributed and may contain duplicates regardless of the configuration. This affects Java pipelines with Dataflow v2 runner and xlang pipelines reading from Kafka, ([#32196](https://github.com/apache/beam/issues/32196))\n* BigQuery Enrichment (Python):  The following issues are present when using the BigQuery enrichment transform ([#32780](https://github.com/apache/beam/pull/32780)):\n  * Duplicate Rows: Multiple conditions may be applied incorrectly, leading to the duplication of rows in the output.\n  * Incorrect Results with Batched Requests: Conditions may not be correctly scoped to individual rows within the batch, potentially causing inaccurate results.\n  * Fixed in 2.61.0.\n\n# [2.57.0] - 2024-06-26\n\n## Highlights\n\n* Apache Beam adds Python 3.12 support ([#29149](https://github.com/apache/beam/issues/29149)).\n* Added FlinkRunner for Flink 1.18 ([#30789](https://github.com/apache/beam/issues/30789)).\n\n## I/Os\n\n* Ensure that BigtableIO closes the reader streams ([#31477](https://github.com/apache/beam/issues/31477)).\n\n## New Features / Improvements\n\n* Added Feast feature store handler for enrichment transform (Python) ([#30957](https://github.com/apache/beam/issues/30964)).\n* BigQuery per-worker metrics are reported by default for Streaming Dataflow Jobs (Java) ([#31015](https://github.com/apache/beam/pull/31015))\n* Adds `inMemory()` variant of Java List and Map side inputs for more efficient lookups when the entire side input fits into memory.\n* Beam YAML now supports the jinja templating syntax.\n  Template variables can be passed with the (json-formatted) `--jinja_variables` flag.\n* DataFrame API now supports pandas 2.1.x and adds 12 more string functions for Series.([#31185](https://github.com/apache/beam/pull/31185)).\n* Added BigQuery handler for enrichment transform (Python) ([#31295](https://github.com/apache/beam/pull/31295))\n* Disable soft delete policy when creating the default bucket for a project (Java) ([#31324](https://github.com/apache/beam/pull/31324)).\n* Added `DoFn.SetupContextParam` and `DoFn.BundleContextParam` which can be used\n  as a python `DoFn.process`, `Map`, or `FlatMap` parameter to invoke a context\n  manager per DoFn setup or bundle (analogous to using `setup`/`teardown`\n  or `start_bundle`/`finish_bundle` respectively.)\n* Go SDK Prism Runner\n  * Pre-built Prism binaries are now part of the release and are available via the Github release page. ([#29697](https://github.com/apache/beam/issues/29697)).\n  * ProcessingTime is now handled synthetically with TestStream pipelines and Non-TestStream pipelines, for fast test pipeline execution by default. ([#30083](https://github.com/apache/beam/issues/30083)).\n    * Prism does NOT yet support \"real time\" execution for this release.\n* Improve processing for large elements to reduce the chances for exceeding 2GB protobuf limits (Python)([https://github.com/apache/beam/issues/31607]).\n\n## Breaking Changes\n\n* Java's View.asList() side inputs are now optimized for iterating rather than\n  indexing when in the global window.\n  This new implementation still supports all (immutable) List methods as before,\n  but some of the random access methods like get() and size() will be slower.\n  To use the old implementation one can use View.asList().withRandomAccess().\n* SchemaTransforms implemented with TypedSchemaTransformProvider now produce a\n  configuration Schema with snake_case naming convention\n  ([#31374](https://github.com/apache/beam/pull/31374)). This will make the following\n  cases problematic:\n  * Running a pre-2.57.0 remote SDK pipeline containing a 2.57.0+ Java SchemaTransform,\n    and vice versa:\n  * Running a 2.57.0+ remote SDK pipeline containing a pre-2.57.0 Java SchemaTransform\n  * All direct uses of Python's [SchemaAwareExternalTransform](https://github.com/apache/beam/blob/a998107a1f5c3050821eef6a5ad5843d8adb8aec/sdks/python/apache_beam/transforms/external.py#L381)\n    should be updated to use new snake_case parameter names.\n* Upgraded Jackson Databind to 2.15.4 (Java) ([#26743](https://github.com/apache/beam/issues/26743)).\n  jackson-2.15 has known breaking changes. An important one is it imposed a buffer limit for parser.\n  If your custom PTransform/DoFn are affected, refer to [#31580](https://github.com/apache/beam/pull/31580) for mitigation.\n\n## Known Issues\n\n* Large Dataflow graphs using runner v2, or pipelines explicitly enabling the `upload_graph` experiment, will fail at construction time ([#32159](https://github.com/apache/beam/issues/32159)).\n* Python pipelines that run with 2.53.0-2.58.0 SDKs and read data from GCS might be affected by a data corruption issue ([#32169](https://github.com/apache/beam/issues/32169)). The issue will be fixed in 2.59.0 ([#32135](https://github.com/apache/beam/pull/32135)). To work around this, update the google-cloud-storage package to version 2.18.2 or newer.\n* BigQuery Enrichment (Python):  The following issues are present when using the BigQuery enrichment transform ([#32780](https://github.com/apache/beam/pull/32780)):\n  * Duplicate Rows: Multiple conditions may be applied incorrectly, leading to the duplication of rows in the output.\n  * Incorrect Results with Batched Requests: Conditions may not be correctly scoped to individual rows within the batch, potentially causing inaccurate results.\n  * Fixed in 2.61.0.\n\n# [2.56.0] - 2024-05-01\n\n## Highlights\n\n* Added FlinkRunner for Flink 1.17, removed support for Flink 1.12 and 1.13. Previous version of Pipeline running on Flink 1.16 and below can be upgraded to 1.17, if the Pipeline is first updated to Beam 2.56.0 with the same Flink version. After Pipeline runs with Beam 2.56.0, it should be possible to upgrade to FlinkRunner with Flink 1.17. ([#29939](https://github.com/apache/beam/issues/29939))\n* New Managed I/O Java API ([#30830](https://github.com/apache/beam/pull/30830)).\n* New Ordered Processing PTransform added for processing order-sensitive stateful data ([#30735](https://github.com/apache/beam/pull/30735)).\n\n## I/Os\n\n* Upgraded Avro version to 1.11.3, kafka-avro-serializer and kafka-schema-registry-client versions to 7.6.0 (Java) ([#30638](https://github.com/apache/beam/pull/30638)).\n  The newer Avro package is known to have breaking changes. If you are affected, you can keep pinned to older Avro versions which are also tested with Beam.\n* Iceberg read/write support is available through the new Managed I/O Java API ([#30830](https://github.com/apache/beam/pull/30830)).\n\n## New Features / Improvements\n\n* Added ability to control the exact number of models loaded across processes by RunInference. This may be useful for pipelines with tight memory constraints ([#31052](https://github.com/apache/beam/pull/31052))\n* Profiling of Cythonized code has been disabled by default. This might improve performance for some Python pipelines ([#30938](https://github.com/apache/beam/pull/30938)).\n* Bigtable enrichment handler now accepts a custom function to build a composite row key. (Python) ([#30974](https://github.com/apache/beam/issues/30975)).\n\n## Breaking Changes\n\n* Default consumer polling timeout for KafkaIO.Read was increased from 1 second to 2 seconds. Use KafkaIO.read().withConsumerPollingTimeout(Duration duration) to configure this timeout value when necessary ([#30870](https://github.com/apache/beam/issues/30870)).\n* Python Dataflow users no longer need to manually specify --streaming for pipelines using unbounded sources such as ReadFromPubSub.\n\n## Bugfixes\n\n* Fixed locking issue when shutting down inactive bundle processors. Symptoms of this issue include slowness or stuckness in long-running jobs (Python) ([#30679](https://github.com/apache/beam/pull/30679)).\n* Fixed logging issue that caused silecing the pip output when installing of dependencies provided in `--requirements_file` (Python).\n* Fixed pipeline stuckness issue by disallowing versions of grpcio that can cause the stuckness (Python) ([#30867](https://github.com/apache/beam/issues/30867)).\n\n## Known Issues\n\n* The beam interactive runner does not correctly run on flink ([#31168](https://github.com/apache/beam/issues/31168)).\n* When using the Flink runner from Python, 1.17 is not supported and 1.12/13 do not work correctly. Support for 1.17 will be added in 2.57.0, and the ability to choose 1.12/13 will be cleaned up and fully removed in 2.57.0 as well ([#31168](https://github.com/apache/beam/issues/31168)).\n* Large Dataflow graphs using runner v2, or pipelines explicitly enabling the `upload_graph` experiment, will fail at construction time ([#32159](https://github.com/apache/beam/issues/32159)).\n* Python pipelines that run with 2.53.0-2.58.0 SDKs and read data from GCS might be affected by a data corruption issue ([#32169](https://github.com/apache/beam/issues/32169)). The issue will be fixed in 2.59.0 ([#32135](https://github.com/apache/beam/pull/32135)). To work around this, update the google-cloud-storage package to version 2.18.2 or newer.\n\n# [2.55.1] - 2024-04-08\n\n## Bugfixes\n\n* Fixed issue that broke WriteToJson in languages other than Java (X-lang) ([#30776](https://github.com/apache/beam/issues/30776)).\n\n# [2.55.0] - 2024-03-25\n\n## Highlights\n\n* The Python SDK will now include automatically generated wrappers for external Java transforms! ([#29834](https://github.com/apache/beam/pull/29834))\n\n## I/Os\n\n* Added support for handling bad records to BigQueryIO ([#30081](https://github.com/apache/beam/pull/30081)).\n  * Full Support for Storage Read and Write APIs\n  * Partial Support for File Loads (Failures writing to files supported, failures loading files to BQ unsupported)\n  * No Support for Extract or Streaming Inserts\n* Added support for handling bad records to PubSubIO ([#30372](https://github.com/apache/beam/pull/30372)).\n  * Support is not available for handling schema mismatches, and enabling error handling for writing to pubsub topics with schemas is not recommended\n* `--enableBundling` pipeline option for BigQueryIO DIRECT_READ is replaced by `--enableStorageReadApiV2`. Both were considered experimental and may subject to change (Java) ([#26354](https://github.com/apache/beam/issues/26354)).\n\n## New Features / Improvements\n\n* Allow writing clustered and not time partitioned BigQuery tables (Java) ([#30094](https://github.com/apache/beam/pull/30094)).\n* Redis cache support added to RequestResponseIO and Enrichment transform (Python) ([#30307](https://github.com/apache/beam/pull/30307))\n* Merged sdks/java/fn-execution and runners/core-construction-java into the main SDK. These artifacts were never meant for users, but noting\n  that they no longer exist. These are steps to bring portability into the core SDK alongside all other core functionality.\n* Added Vertex AI Feature Store handler for Enrichment transform (Python) ([#30388](https://github.com/apache/beam/pull/30388))\n\n## Breaking Changes\n\n* Arrow version was bumped to 15.0.0 from 5.0.0 ([#30181](https://github.com/apache/beam/pull/30181)).\n* Go SDK users who build custom worker containers may run into issues with the move to distroless containers as a base (see Security Fixes).\n  * The issue stems from distroless containers lacking additional tools, which current custom container processes may rely on.\n  * See https://beam.apache.org/documentation/runtime/environments/#from-scratch-go for instructions on building and using a custom container.\n* Python SDK has changed the default value for the `--max_cache_memory_usage_mb` pipeline option from 100 to 0. This option was first introduced in 2.52.0 SDK. This change restores the behavior of 2.51.0 SDK, which does not use the state cache. If your pipeline uses iterable side inputs views, consider increasing the cache size by setting the option manually. ([#30360](https://github.com/apache/beam/issues/30360)).\n\n## Bugfixes\n\n* Fixed SpannerIO.readChangeStream to support propagating credentials from pipeline options\n  to the getDialect calls for authenticating with Spanner (Java) ([#30361](https://github.com/apache/beam/pull/30361)).\n* Reduced the number of HTTP requests in GCSIO function calls (Python) ([#30205](https://github.com/apache/beam/pull/30205))\n\n## Security Fixes\n\n* Go SDK base container image moved to distroless/base-nossl-debian12, reducing vulnerable container surface to kernel and glibc ([#30011](https://github.com/apache/beam/pull/30011)).\n\n## Known Issues\n\n* In Python pipelines, when shutting down inactive bundle processors, shutdown logic can overaggressively hold the lock, blocking acceptance of new work. Symptoms of this issue include slowness or stuckness in long-running jobs. Fixed in 2.56.0 ([#30679](https://github.com/apache/beam/pull/30679)).\n* WriteToJson broken in languages other than Java (X-lang) ([#30776](https://github.com/apache/beam/issues/30776)).\n* Python pipelines might occasionally become stuck due to a regression in grpcio ([#30867](https://github.com/apache/beam/issues/30867)). The issue manifests frequently with Bigtable IO connector, but might also affect other GCP connectors. Fixed in 2.56.0.\n* Python pipelines that run with 2.53.0-2.58.0 SDKs and read data from GCS might be affected by a data corruption issue ([#32169](https://github.com/apache/beam/issues/32169)). The issue will be fixed in 2.59.0 ([#32135](https://github.com/apache/beam/pull/32135)). To work around this, update the google-cloud-storage package to version 2.18.2 or newer.\n\n# [2.54.0] - 2024-02-14\n\n## Highlights\n\n* [Enrichment Transform](https://s.apache.org/enrichment-transform) along with GCP BigTable handler added to Python SDK ([#30001](https://github.com/apache/beam/pull/30001)).\n* Beam Java Batch pipelines run on Google Cloud Dataflow will default to the Portable (Runner V2)[https://cloud.google.com/dataflow/docs/runner-v2] starting with this version. (All other languages are already on Runner V2.)\n    * This change is still rolling out to the Dataflow service, see (Runner V2 documentation)[https://cloud.google.com/dataflow/docs/runner-v2] for how to enable or disable it intentionally.\n\n## I/Os\n\n* Added support for writing to BigQuery dynamic destinations with Python's Storage Write API ([#30045](https://github.com/apache/beam/pull/30045))\n* Adding support for Tuples DataType in ClickHouse (Java) ([#29715](https://github.com/apache/beam/pull/29715)).\n* Added support for handling bad records to FileIO, TextIO, AvroIO ([#29670](https://github.com/apache/beam/pull/29670)).\n* Added support for handling bad records to BigtableIO ([#29885](https://github.com/apache/beam/pull/29885)).\n\n## New Features / Improvements\n\n* [Enrichment Transform](https://s.apache.org/enrichment-transform) along with GCP BigTable handler added to Python SDK ([#30001](https://github.com/apache/beam/pull/30001)).\n\n## Breaking Changes\n\n* N/A\n\n## Deprecations\n\n* N/A\n\n## Bugfixes\n\n* Fixed a memory leak affecting some Go SDK since 2.46.0. ([#28142](https://github.com/apache/beam/pull/28142))\n\n## Security Fixes\n\n* N/A\n\n## Known Issues\n\n* Some Python pipelines that run with 2.52.0-2.54.0 SDKs and use large materialized side inputs might be affected by a performance regression. To restore the prior behavior on these SDK versions, supply the `--max_cache_memory_usage_mb=0` pipeline option. ([#30360](https://github.com/apache/beam/issues/30360)).\n* Python pipelines that run with 2.53.0-2.54.0 SDKs and perform file operations on GCS might be affected by excess HTTP requests. This could lead to a performance regression or a permission issue. ([#28398](https://github.com/apache/beam/issues/28398))\n* In Python pipelines, when shutting down inactive bundle processors, shutdown logic can overaggressively hold the lock, blocking acceptance of new work. Symptoms of this issue include slowness or stuckness in long-running jobs. Fixed in 2.56.0 ([#30679](https://github.com/apache/beam/pull/30679)).\n* Python pipelines that run with 2.53.0-2.58.0 SDKs and read data from GCS might be affected by a data corruption issue ([#32169](https://github.com/apache/beam/issues/32169)). The issue will be fixed in 2.59.0 ([#32135](https://github.com/apache/beam/pull/32135)). To work around this, update the google-cloud-storage package to version 2.18.2 or newer.\n\n# [2.53.0] - 2024-01-04\n\n## Highlights\n\n* Python streaming users that use 2.47.0 and newer versions of Beam should update to version 2.53.0, which fixes a known issue: ([#27330](https://github.com/apache/beam/issues/27330)).\n\n## I/Os\n\n* TextIO now supports skipping multiple header lines (Java) ([#17990](https://github.com/apache/beam/issues/17990)).\n* Python GCSIO is now implemented with GCP GCS Client instead of apitools ([#25676](https://github.com/apache/beam/issues/25676))\n* Added support for handling bad records to KafkaIO (Java) ([#29546](https://github.com/apache/beam/pull/29546))\n* Add support for generating text embeddings in MLTransform for Vertex AI and Hugging Face Hub models.([#29564](https://github.com/apache/beam/pull/29564))\n* NATS IO connector added (Go) ([#29000](https://github.com/apache/beam/issues/29000)).\n* Adding support for LowCardinality (Java) ([#29533](https://github.com/apache/beam/pull/29533)).\n\n## New Features / Improvements\n\n* The Python SDK now type checks `collections.abc.Collections` types properly. Some type hints that were erroneously allowed by the SDK may now fail. ([#29272](https://github.com/apache/beam/pull/29272))\n* Running multi-language pipelines locally no longer requires Docker.\n  Instead, the same (generally auto-started) subprocess used to perform the\n  expansion can also be used as the cross-language worker.\n* Framework for adding Error Handlers to composite transforms added in Java ([#29164](https://github.com/apache/beam/pull/29164)).\n* Python 3.11 images now include google-cloud-profiler ([#29561](https://github.com/apache/beam/pull/29651)).\n\n## Deprecations\n\n* Euphoria DSL is deprecated and will be removed in a future release (not before 2.56.0) ([#29451](https://github.com/apache/beam/issues/29451))\n\n## Bugfixes\n\n* (Python) Fixed sporadic crashes in streaming pipelines that affected some users of 2.47.0 and newer SDKs ([#27330](https://github.com/apache/beam/issues/27330)).\n* (Python) Fixed a bug that caused MLTransform to drop identical elements in the output PCollection ([#29600](https://github.com/apache/beam/issues/29600)).\n\n## Security Fixes\n\n* Upgraded to go 1.21.5 to build, fixing [CVE-2023-45285](https://security-tracker.debian.org/tracker/CVE-2023-45285) and [CVE-2023-39326](https://security-tracker.debian.org/tracker/CVE-2023-39326)\n\n## Known Issues\n\n* Potential race condition causing NPE in DataflowExecutionStateSampler in Dataflow Java Streaming pipelines ([#29987](https://github.com/apache/beam/issues/29987)).\n* Some Python pipelines that run with 2.52.0-2.54.0 SDKs and use large materialized side inputs might be affected by a performance regression. To restore the prior behavior on these SDK versions, supply the `--max_cache_memory_usage_mb=0` pipeline option. ([#30360](https://github.com/apache/beam/issues/30360)).\n* Python pipelines that run with 2.53.0-2.54.0 SDKs and perform file operations on GCS might be affected by excess HTTP requests. This could lead to a performance regression or a permission issue. ([#28398](https://github.com/apache/beam/issues/28398))\n* In Python pipelines, when shutting down inactive bundle processors, shutdown logic can overaggressively hold the lock, blocking acceptance of new work. Symptoms of this issue include slowness or stuckness in long-running jobs. Fixed in 2.56.0 ([#30679](https://github.com/apache/beam/pull/30679)).\n* Python pipelines that run with 2.53.0-2.58.0 SDKs and read data from GCS might be affected by a data corruption issue ([#32169](https://github.com/apache/beam/issues/32169)). The issue will be fixed in 2.59.0 ([#32135](https://github.com/apache/beam/pull/32135)). To work around this, update the google-cloud-storage package to version 2.18.2 or newer.\n\n# [2.52.0] - 2023-11-17\n\n## Highlights\n\n* Previously deprecated Avro-dependent code (Beam Release 2.46.0) has been finally removed from Java SDK \"core\" package.\nPlease, use `beam-sdks-java-extensions-avro` instead. This will allow to easily update Avro version in user code without\npotential breaking changes in Beam \"core\" since the Beam Avro extension already supports the latest Avro versions and\nshould handle this. ([#25252](https://github.com/apache/beam/issues/25252)).\n* Publishing Java 21 SDK container images now supported as part of Apache Beam release process. ([#28120](https://github.com/apache/beam/issues/28120))\n  * Direct Runner and Dataflow Runner support running pipelines on Java21 (experimental until tests fully setup). For other runners (Flink, Spark, Samza, etc) support status depend on runner projects.\n\n## New Features / Improvements\n\n* Add `UseDataStreamForBatch` pipeline option to the Flink runner. When it is set to true, Flink runner will run batch\n  jobs using the DataStream API. By default the option is set to false, so the batch jobs are still executed\n  using the DataSet API.\n* `upload_graph` as one of the Experiments options for DataflowRunner is no longer required when the graph is larger than 10MB for Java SDK ([PR#28621](https://github.com/apache/beam/pull/28621)).\n* Introduced a pipeline option `--max_cache_memory_usage_mb` to configure state and side input cache size. The cache has been enabled to a default of 100 MB. Use `--max_cache_memory_usage_mb=X` to provide cache size for the user state API and side inputs. ([#28770](https://github.com/apache/beam/issues/28770)).\n* Beam YAML stable release. Beam pipelines can now be written using YAML and leverage the Beam YAML framework which includes a preliminary set of IO's and turnkey transforms. More information can be found in the YAML root folder and in the [README](https://github.com/apache/beam/blob/master/sdks/python/apache_beam/yaml/README.md).\n\n\n## Breaking Changes\n\n* `org.apache.beam.sdk.io.CountingSource.CounterMark` uses custom `CounterMarkCoder` as a default coder since all Avro-dependent\nclasses finally moved to `extensions/avro`. In case if it's still required to use `AvroCoder` for `CounterMark`, then,\nas a workaround, a copy of \"old\" `CountingSource` class should be placed into a project code and used directly\n([#25252](https://github.com/apache/beam/issues/25252)).\n* Renamed `host` to `firestoreHost` in `FirestoreOptions` to avoid potential conflict of command line arguments (Java) ([#29201](https://github.com/apache/beam/pull/29201)).\n* Transforms which use `SnappyCoder` are update incompatible with previous versions of the same transform (Java) on some runners. This includes PubSubIO's read ([#28655](https://github.com/apache/beam/pull/28655#issuecomment-2407839769)).\n\n## Bugfixes\n\n* Fixed \"Desired bundle size 0 bytes must be greater than 0\" in Java SDK's BigtableIO.BigtableSource when you have more cores than bytes to read (Java) [#28793](https://github.com/apache/beam/issues/28793).\n* `watch_file_pattern` arg of the [RunInference](https://github.com/apache/beam/blob/104c10b3ee536a9a3ea52b4dbf62d86b669da5d9/sdks/python/apache_beam/ml/inference/base.py#L997) arg had no effect prior to 2.52.0. To use the behavior of arg `watch_file_pattern` prior to 2.52.0, follow the documentation at https://beam.apache.org/documentation/ml/side-input-updates/ and use `WatchFilePattern` PTransform as a SideInput. ([#28948](https://github.com/apache/beam/pulls/28948))\n* `MLTransform` doesn't output artifacts such as min, max and quantiles. Instead, `MLTransform` will add a feature to output these artifacts as human readable format - [#29017](https://github.com/apache/beam/issues/29017). For now, to use the artifacts such as min and max that were produced by the eariler `MLTransform`, use `read_artifact_location` of `MLTransform`, which reads artifacts that were produced earlier in a different `MLTransform` ([#29016](https://github.com/apache/beam/pull/29016/))\n* Fixed a memory leak, which affected some long-running Python pipelines: [#28246](https://github.com/apache/beam/issues/28246).\n\n## Security Fixes\n* Fixed [CVE-2023-39325](https://www.cve.org/CVERecord?id=CVE-2023-39325) (Java/Python/Go) ([#29118](https://github.com/apache/beam/issues/29118)).\n* Mitigated [CVE-2023-47248](https://nvd.nist.gov/vuln/detail/CVE-2023-47248)  (Python) [#29392](https://github.com/apache/beam/issues/29392).\n\n## Known issues\n\n* MLTransform drops the identical elements in the output PCollection. For any duplicate elements, a single element will be emitted downstream. ([#29600](https://github.com/apache/beam/issues/29600)).\n* Some Python pipelines that run with 2.52.0-2.54.0 SDKs and use large materialized side inputs might be affected by a performance regression. To restore the prior behavior on these SDK versions, supply the `--max_cache_memory_usage_mb=0` pipeline option. (Python) ([#30360](https://github.com/apache/beam/issues/30360)).\n* Users who lauch Python pipelines in an environment without internet access and use the `--setup_file` pipeline option might experience an increase in pipeline submission time. This has been fixed in 2.56.0 ([#31070](https://github.com/apache/beam/pull/31070)).\n* Transforms which use `SnappyCoder` are update incompatible with previous versions of the same transform (Java) on some runners. This includes PubSubIO's read ([#28655](https://github.com/apache/beam/pull/28655#issuecomment-2407839769)).\n\n# [2.51.0] - 2023-10-03\n\n## New Features / Improvements\n\n* In Python, [RunInference](https://beam.apache.org/documentation/sdks/python-machine-learning/#why-use-the-runinference-api) now supports loading many models in the same transform using a [KeyedModelHandler](https://beam.apache.org/documentation/sdks/python-machine-learning/#use-a-keyed-modelhandler) ([#27628](https://github.com/apache/beam/issues/27628)).\n* In Python, the [VertexAIModelHandlerJSON](https://beam.apache.org/releases/pydoc/current/apache_beam.ml.inference.vertex_ai_inference.html#apache_beam.ml.inference.vertex_ai_inference.VertexAIModelHandlerJSON) now supports passing in inference_args. These will be passed through to the Vertex endpoint as parameters.\n* Added support to run `mypy` on user pipelines ([#27906](https://github.com/apache/beam/issues/27906))\n* Python SDK worker start-up logs and crash logs are now captured by a buffer and logged at appropriate levels via Beam logging API. Dataflow Runner users might observe that most `worker-startup` log content is now captured by the `worker` logger. Users who relied on `print()` statements for logging might notice that some logs don't flush before pipeline succeeds - we strongly advise to use `logging` package instead of `print()` statements for logging. ([#28317](https://github.com/apache/beam/pull/28317))\n\n\n## Breaking Changes\n\n* Removed fastjson library dependency for Beam SQL. Table property is changed to be based on jackson ObjectNode (Java) ([#24154](https://github.com/apache/beam/issues/24154)).\n* Removed TensorFlow from Beam Python container images [PR](https://github.com/apache/beam/pull/28424). If you have been negatively affected by this change, please comment on [#20605](https://github.com/apache/beam/issues/20605).\n* Removed the parameter `t reflect.Type` from `parquetio.Write`. The element type is derived from the input PCollection (Go) ([#28490](https://github.com/apache/beam/issues/28490))\n* Refactor BeamSqlSeekableTable.setUp adding a parameter joinSubsetType. [#28283](https://github.com/apache/beam/issues/28283)\n\n\n## Bugfixes\n\n* Fixed exception chaining issue in GCS connector (Python) ([#26769](https://github.com/apache/beam/issues/26769#issuecomment-1700422615)).\n* Fixed streaming inserts exception handling, GoogleAPICallErrors are now retried according to retry strategy and routed to failed rows where appropriate rather than causing a pipeline error (Python) ([#21080](https://github.com/apache/beam/issues/21080)).\n* Fixed a bug in Python SDK's cross-language Bigtable sink that mishandled records that don't have an explicit timestamp set: [#28632](https://github.com/apache/beam/issues/28632).\n\n\n## Security Fixes\n* Python containers updated, fixing [CVE-2021-30474](https://nvd.nist.gov/vuln/detail/CVE-2021-30474), [CVE-2021-30475](https://nvd.nist.gov/vuln/detail/CVE-2021-30475), [CVE-2021-30473](https://nvd.nist.gov/vuln/detail/CVE-2021-30473), [CVE-2020-36133](https://nvd.nist.gov/vuln/detail/CVE-2020-36133), [CVE-2020-36131](https://nvd.nist.gov/vuln/detail/CVE-2020-36131), [CVE-2020-36130](https://nvd.nist.gov/vuln/detail/CVE-2020-36130), and [CVE-2020-36135](https://nvd.nist.gov/vuln/detail/CVE-2020-36135)\n* Used go 1.21.1 to build, fixing [CVE-2023-39320](https://security-tracker.debian.org/tracker/CVE-2023-39320)\n\n## Known Issues\n\n* Long-running Python pipelines might experience a memory leak: [#28246](https://github.com/apache/beam/issues/28246).\n* Python pipelines using BigQuery Storage Read API might need to pin `fastavro`\n  dependency to 1.8.3 or earlier on some runners that don't use Beam Docker containers: [#28811](https://github.com/apache/beam/issues/28811)\n* MLTransform drops the identical elements in the output PCollection. For any duplicate elements, a single element will be emitted downstream. ([#29600](https://github.com/apache/beam/issues/29600)).\n\n\n# [2.50.0] - 2023-08-30\n\n## Highlights\n\n* Spark 3.2.2 is used as default version for Spark runner ([#23804](https://github.com/apache/beam/issues/23804)).\n* The Go SDK has a new default local runner, called Prism ([#24789](https://github.com/apache/beam/issues/24789)).\n* All Beam released container images are now [multi-arch images](https://cloud.google.com/kubernetes-engine/docs/how-to/build-multi-arch-for-arm#what_is_a_multi-arch_image) that support both x86 and ARM CPU architectures.\n\n## I/Os\n\n* Java KafkaIO now supports picking up topics via topicPattern ([#26948](https://github.com/apache/beam/pull/26948))\n* Support for read from Cosmos DB Core SQL API ([#23604](https://github.com/apache/beam/issues/23604))\n* Upgraded to HBase 2.5.5 for HBaseIO. (Java) ([#27711](https://github.com/apache/beam/issues/19554))\n* Added support for GoogleAdsIO source (Java) ([#27681](https://github.com/apache/beam/pull/27681)).\n\n## New Features / Improvements\n\n* The Go SDK now requires Go 1.20 to build. ([#27558](https://github.com/apache/beam/issues/27558))\n* The Go SDK has a new default local runner, Prism. ([#24789](https://github.com/apache/beam/issues/24789)).\n  * Prism is a portable runner that executes each transform independantly, ensuring coders.\n  * At this point it supercedes the Go direct runner in functionality. The Go direct runner is now deprecated.\n  * See https://github.com/apache/beam/blob/master/sdks/go/pkg/beam/runners/prism/README.md for the goals and features of Prism.\n* Hugging Face Model Handler for RunInference added to Python SDK. ([#26632](https://github.com/apache/beam/pull/26632))\n* Hugging Face Pipelines support for RunInference added to Python SDK. ([#27399](https://github.com/apache/beam/pull/27399))\n* Vertex AI Model Handler for RunInference now supports private endpoints ([#27696](https://github.com/apache/beam/pull/27696))\n* MLTransform transform added with support for common ML pre/postprocessing operations ([#26795](https://github.com/apache/beam/pull/26795))\n* Upgraded the Kryo extension for the Java SDK to Kryo 5.5.0. This brings in bug fixes, performance improvements, and serialization of Java 14 records. ([#27635](https://github.com/apache/beam/issues/27635))\n* All Beam released container images are now [multi-arch images](https://cloud.google.com/kubernetes-engine/docs/how-to/build-multi-arch-for-arm#what_is_a_multi-arch_image) that support both x86 and ARM CPU architectures. ([#27674](https://github.com/apache/beam/issues/27674)). The multi-arch container images include:\n  * All versions of Go, Python, Java and Typescript SDK containers.\n  * All versions of Flink job server containers.\n  * Java and Python expansion service containers.\n  * Transform service controller container.\n  * Spark3 job server container.\n* Added support for batched writes to AWS SQS for improved throughput (Java, AWS 2).([#21429](https://github.com/apache/beam/issues/21429))\n\n## Breaking Changes\n\n* Python SDK: Legacy runner support removed from Dataflow, all pipelines must use runner v2.\n* Python SDK: Dataflow Runner will no longer stage Beam SDK from PyPI in the `--staging_location` at pipeline submission. Custom container images that are not based on Beam's default image must include Apache Beam installation.([#26996](https://github.com/apache/beam/issues/26996))\n\n## Deprecations\n\n* The Go Direct Runner is now Deprecated. It remains available to reduce migration churn.\n  * Tests can be set back to the direct runner by overriding TestMain: `func TestMain(m *testing.M) { ptest.MainWithDefault(m, \"direct\") }`\n  * It's recommended to fix issues seen in tests using Prism, as they can also happen on any portable runner.\n  * Use the generic register package for your pipeline DoFns to ensure pipelines function on portable runners, like prism.\n  * Do not rely on closures or using package globals for DoFn configuration. They don't function on portable runners.\n\n## Bugfixes\n\n* Fixed DirectRunner bug in Python SDK where GroupByKey gets empty PCollection and fails when pipeline option `direct_num_workers!=1`.([#27373](https://github.com/apache/beam/pull/27373))\n* Fixed BigQuery I/O bug when estimating size on queries that utilize row-level security ([#27474](https://github.com/apache/beam/pull/27474))\n\n## Known Issues\n\n* Long-running Python pipelines might experience a memory leak: [#28246](https://github.com/apache/beam/issues/28246).\n* Python Pipelines using BigQuery IO or `orjson` dependency might experience segmentation faults or get stuck: [#28318](https://github.com/apache/beam/issues/28318).\n* Beam Python containers rely on a version of Debian/aom that has several security vulnerabilities: [CVE-2021-30474](https://nvd.nist.gov/vuln/detail/CVE-2021-30474), [CVE-2021-30475](https://nvd.nist.gov/vuln/detail/CVE-2021-30475), [CVE-2021-30473](https://nvd.nist.gov/vuln/detail/CVE-2021-30473), [CVE-2020-36133](https://nvd.nist.gov/vuln/detail/CVE-2020-36133), [CVE-2020-36131](https://nvd.nist.gov/vuln/detail/CVE-2020-36131), [CVE-2020-36130](https://nvd.nist.gov/vuln/detail/CVE-2020-36130), and [CVE-2020-36135](https://nvd.nist.gov/vuln/detail/CVE-2020-36135)\n* Python SDK's cross-language Bigtable sink mishandles records that don't have an explicit timestamp set: [#28632](https://github.com/apache/beam/issues/28632). To avoid this issue, set explicit timestamps for all records before writing to Bigtable.\n* Python SDK worker start-up logs, particularly PIP dependency installations, that are not logged at warning or higher are suppressed. This suppression is reverted in 2.51.0.\n* MLTransform drops the identical elements in the output PCollection. For any duplicate elements, a single element will be emitted downstream. ([#29600](https://github.com/apache/beam/issues/29600)).\n\n# [2.49.0] - 2023-07-17\n\n\n## I/Os\n\n* Support for Bigtable Change Streams added in Java `BigtableIO.ReadChangeStream` ([#27183](https://github.com/apache/beam/issues/27183))\n\n## New Features / Improvements\n\n* Allow prebuilding large images when using `--prebuild_sdk_container_engine=cloud_build`, like images depending on `tensorflow` or `torch` ([#27023](https://github.com/apache/beam/pull/27023)).\n* Disabled `pip` cache when installing packages on the workers. This reduces the size of prebuilt Python container images ([#27035](https://github.com/apache/beam/pull/27035)).\n* Select dedicated avro datum reader and writer (Java) ([#18874](https://github.com/apache/beam/issues/18874)).\n* Timer API for the Go SDK (Go) ([#22737](https://github.com/apache/beam/issues/22737)).\n\n## Deprecations\n\n* Removed Python 3.7 support. ([#26447](https://github.com/apache/beam/issues/26447))\n\n## Bugfixes\n\n* Fixed KinesisIO `NullPointerException` when a progress check is made before the reader is started (IO) ([#23868](https://github.com/apache/beam/issues/23868))\n\n## Known Issues\n\n* Long-running Python pipelines might experience a memory leak: [#28246](https://github.com/apache/beam/issues/28246).\n* Python pipelines using the `--impersonate_service_account` option with BigQuery IOs might fail on Dataflow ([#32030](https://github.com/apache/beam/issues/32030)). This is fixed in 2.59.0 release.\n\n\n# [2.48.0] - 2023-05-31\n\n## Highlights\n\n* \"Experimental\" annotation cleanup: the annotation and concept have been removed from Beam to avoid\n  the misperception of code as \"not ready\". Any proposed breaking changes will be subject to\n  case-by-case pro/con decision making (and generally avoided) rather than using the \"Experimental\"\n  to allow them.\n\n## I/Os\n\n* Added rename for GCS and copy for local filesystem (Go) ([#25779](https://github.com/apache/beam/issues/26064)).\n* Added support for enhanced fan-out in KinesisIO.Read (Java) ([#19967](https://github.com/apache/beam/issues/19967)).\n  * This change is not compatible with Flink savepoints created by Beam 2.46.0 applications which had KinesisIO sources.\n* Added textio.ReadWithFilename transform (Go) ([#25812](https://github.com/apache/beam/issues/25812)).\n* Added fileio.MatchContinuously transform (Go) ([#26186](https://github.com/apache/beam/issues/26186)).\n\n## New Features / Improvements\n\n* Allow passing service name for google-cloud-profiler (Python) ([#26280](https://github.com/apache/beam/issues/26280)).\n* Dead letter queue support added to RunInference in Python ([#24209](https://github.com/apache/beam/issues/24209)).\n* Support added for defining pre/postprocessing operations on the RunInference transform ([#26308](https://github.com/apache/beam/issues/26308))\n* Adds a Docker Compose based transform service that can be used to discover and use portable Beam transforms ([#26023](https://github.com/apache/beam/pull/26023)).\n\n## Breaking Changes\n\n* Passing a tag into MultiProcessShared is now required in the Python SDK ([#26168](https://github.com/apache/beam/issues/26168)).\n* CloudDebuggerOptions is removed (deprecated in Beam v2.47.0) for Dataflow runner as the Google Cloud Debugger service is [shutting down](https://cloud.google.com/debugger/docs/deprecations). (Java) ([#25959](https://github.com/apache/beam/issues/25959)).\n* AWS 2 client providers (deprecated in Beam [v2.38.0](#2380---2022-04-20)) are finally removed ([#26681](https://github.com/apache/beam/issues/26681)).\n* AWS 2 SnsIO.writeAsync (deprecated in Beam v2.37.0 due to risk of data loss) was finally removed ([#26710](https://github.com/apache/beam/issues/26710)).\n* AWS 2 coders (deprecated in Beam v2.43.0 when adding Schema support for AWS Sdk Pojos) are finally removed ([#23315](https://github.com/apache/beam/issues/23315)).\n\n## Deprecations\n\n\n## Bugfixes\n\n* Fixed Java bootloader failing with Too Long Args due to long classpaths, with a pathing jar. (Java) ([#25582](https://github.com/apache/beam/issues/25582)).\n\n## Known Issues\n\n* PubsubIO writes will throw *SizeLimitExceededException* for any message above 100 bytes, when used in batch (bounded) mode. (Java) ([#27000](https://github.com/apache/beam/issues/27000)).\n* Long-running Python pipelines might experience a memory leak: [#28246](https://github.com/apache/beam/issues/28246).\n* Python SDK's cross-language Bigtable sink mishandles records that don't have an explicit timestamp set: [#28632](https://github.com/apache/beam/issues/28632). To avoid this issue, set explicit timestamps for all records before writing to Bigtable.\n\n\n# [2.47.0] - 2023-05-10\n\n## Highlights\n\n* Apache Beam adds Python 3.11 support ([#23848](https://github.com/apache/beam/issues/23848)).\n\n## I/Os\n\n* BigQuery Storage Write API is now available in Python SDK via cross-language ([#21961](https://github.com/apache/beam/issues/21961)).\n* Added HbaseIO support for writing RowMutations (ordered by rowkey) to Hbase (Java) ([#25830](https://github.com/apache/beam/issues/25830)).\n* Added fileio transforms MatchFiles, MatchAll and ReadMatches (Go) ([#25779](https://github.com/apache/beam/issues/25779)).\n* Add integration test for JmsIO + fix issue with multiple connections (Java) ([#25887](https://github.com/apache/beam/issues/25887)).\n\n## New Features / Improvements\n\n* The Flink runner now supports Flink 1.16.x ([#25046](https://github.com/apache/beam/issues/25046)).\n* Schema'd PTransforms can now be directly applied to Beam dataframes just like PCollections.\n  (Note that when doing multiple operations, it may be more efficient to explicitly chain the operations\n  like `df | (Transform1 | Transform2 | ...)` to avoid excessive conversions.)\n* The Go SDK adds new transforms periodic.Impulse and periodic.Sequence that extends support\n  for slowly updating side input patterns. ([#23106](https://github.com/apache/beam/issues/23106))\n* Several Google client libraries in Python SDK dependency chain were updated to latest available major versions. ([#24599](https://github.com/apache/beam/pull/24599))\n\n## Breaking Changes\n\n* If a main session fails to load, the pipeline will now fail at worker startup. ([#25401](https://github.com/apache/beam/issues/25401)).\n* Python pipeline options will now ignore unparsed command line flags prefixed with a single dash. ([#25943](https://github.com/apache/beam/issues/25943)).\n* The SmallestPerKey combiner now requires keyword-only arguments for specifying optional parameters, such as `key` and `reverse`. ([#25888](https://github.com/apache/beam/issues/25888)).\n\n## Deprecations\n\n* Cloud Debugger support and its pipeline options are deprecated and will be removed in the next Beam version,\n  in response to the Google Cloud Debugger service [turning down](https://cloud.google.com/debugger/docs/deprecations). (Java) ([#25959](https://github.com/apache/beam/issues/25959)).\n\n## Bugfixes\n\n* BigQuery sink in STORAGE_WRITE_API mode in batch pipelines could result in data consistency issues during the handling of other unrelated transient errors for Beam SDKs 2.35.0 - 2.46.0 (inclusive). For more details see: https://github.com/apache/beam/issues/26521\n\n## Known Issues\n\n* The google-cloud-profiler dependency was accidentally removed from Beam's Python Docker\n  Image [#26998](https://github.com/apache/beam/issues/26698). [Dataflow Docker images](https://cloud.google.com/dataflow/docs/concepts/sdk-worker-dependencies) still preinstall this dependency.\n* Long-running Python pipelines might experience a memory leak: [#28246](https://github.com/apache/beam/issues/28246).\n\n# [2.46.0] - 2023-03-10\n\n## Highlights\n\n* Java SDK containers migrated to [Eclipse Temurin](https://hub.docker.com/_/eclipse-temurin)\n  as a base. This change migrates away from the deprecated [OpenJDK](https://hub.docker.com/_/openjdk)\n  container. Eclipse Temurin is currently based upon Ubuntu 22.04 while the OpenJDK\n  container was based upon Debian 11.\n* RunInference PTransform will accept model paths as SideInputs in Python SDK. ([#24042](https://github.com/apache/beam/issues/24042))\n* RunInference supports ONNX runtime in Python SDK ([#22972](https://github.com/apache/beam/issues/22972))\n* Tensorflow Model Handler for RunInference in Python SDK ([#25366](https://github.com/apache/beam/issues/25366))\n* Java SDK modules migrated to use `:sdks:java:extensions:avro` ([#24748](https://github.com/apache/beam/issues/24748))\n\n## I/Os\n\n* Added in JmsIO a retry policy for failed publications (Java) ([#24971](https://github.com/apache/beam/issues/24971)).\n* Support for `LZMA` compression/decompression of text files added to the Python SDK ([#25316](https://github.com/apache/beam/issues/25316))\n* Added ReadFrom/WriteTo Csv/Json as top-level transforms to the Python SDK.\n\n## New Features / Improvements\n\n* Add UDF metrics support for Samza portable mode.\n* Option for SparkRunner to avoid the need of SDF output to fit in memory ([#23852](https://github.com/apache/beam/issues/23852)).\n  This helps e.g. with ParquetIO reads. Turn the feature on by adding experiment `use_bounded_concurrent_output_for_sdf`.\n* Add `WatchFilePattern` transform, which can be used as a side input to the RunInference PTransfrom to watch for model updates using a file pattern. ([#24042](https://github.com/apache/beam/issues/24042))\n* Add support for loading TorchScript models with `PytorchModelHandler`. The TorchScript model path can be\n  passed to PytorchModelHandler using `torch_script_model_path=<path_to_model>`. ([#25321](https://github.com/apache/beam/pull/25321))\n* The Go SDK now requires Go 1.19 to build. ([#25545](https://github.com/apache/beam/pull/25545))\n* The Go SDK now has an initial native Go implementation of a portable Beam Runner called Prism. ([#24789](https://github.com/apache/beam/pull/24789))\n  * For more details and current state see https://github.com/apache/beam/tree/master/sdks/go/pkg/beam/runners/prism.\n\n## Breaking Changes\n\n* The deprecated SparkRunner for Spark 2 (see [2.41.0](#2410---2022-08-23)) was removed ([#25263](https://github.com/apache/beam/pull/25263)).\n* Python's BatchElements performs more aggressive batching in some cases,\n  capping at 10 second rather than 1 second batches by default and excluding\n  fixed cost in this computation to better handle cases where the fixed cost\n  is larger than a single second. To get the old behavior, one can pass\n  `target_batch_duration_secs_including_fixed_cost=1` to BatchElements.\n* Dataflow runner enables sibling SDK protocol for Python pipelines using custom containers on Beam 2.46.0 and newer SDKs.\n  If your Python pipeline starts to stall after you switch to 2.46.0 and you use a custom container, please verify\n  that your custom container does not include artifacts from older Beam SDK releases. In particular, check in your `Dockerfile`\n  that the Beam container entrypoint and/or Beam base image version match the Beam SDK version used at job submission.\n\n## Deprecations\n\n* Avro related classes are deprecated in module `beam-sdks-java-core` and will be eventually removed. Please, migrate to a new module `beam-sdks-java-extensions-avro` instead by importing the classes from `org.apache.beam.sdk.extensions.avro` package.\n  For the sake of migration simplicity, the relative package path and the whole class hierarchy of Avro related classes in new module is preserved the same as it was before.\n  For example, import `org.apache.beam.sdk.extensions.avro.coders.AvroCoder` class instead of`org.apache.beam.sdk.coders.AvroCoder`. ([#24749](https://github.com/apache/beam/issues/24749)).\n\n## Bugfixes\n\n\n# [2.45.0] - 2023-02-15\n\n## I/Os\n\n* Support for X source added (Java/Python) ([#X](https://github.com/apache/beam/issues/X)).\n* MongoDB IO connector added (Go) ([#24575](https://github.com/apache/beam/issues/24575)).\n\n## New Features / Improvements\n\n* RunInference Wrapper with Sklearn Model Handler support added in Go SDK ([#24497](https://github.com/apache/beam/issues/23382)).\n* Adding override of allowed TLS algorithms (Java), now maintaining the disabled/legacy algorithms\n  present in 2.43.0 (up to 1.8.0_342, 11.0.16, 17.0.2 for respective Java versions). This is accompanied\n  by an explicit re-enabling of TLSv1 and TLSv1.1 for Java 8 and Java 11.\n* Add UDF metrics support for Samza portable mode.\n\n## Breaking Changes\n\n* Portable Java pipelines, Go pipelines, Python streaming pipelines, and portable Python batch\n  pipelines on Dataflow are required to use Runner V2. The `disable_runner_v2`,\n  `disable_runner_v2_until_2023`, `disable_prime_runner_v2` experiments will raise an error during\n  pipeline construction. You can no longer specify the Dataflow worker jar override. Note that\n  non-portable Java jobs and non-portable Python batch jobs are not impacted. ([#24515](https://github.com/apache/beam/issues/24515)).\n* Beam now requires `pyarrow>=3` and `pandas>=1.4.3` since older versions are not compatible with `numpy==1.24.0`.\n\n## Bugfixes\n\n* Avoids Cassandra syntax error when user-defined query has no where clause in it (Java) ([#24829](https://github.com/apache/beam/issues/24829)).\n* Fixed JDBC connection failures (Java) during handshake due to deprecated TLSv1(.1) protocol for the JDK. ([#24623](https://github.com/apache/beam/issues/24623))\n* Fixed Python BigQuery Batch Load write may truncate valid data when deposition sets to WRITE_TRUNCATE and incoming data is large (Python) ([#24623](https://github.com/apache/beam/issues/24535)).\n\n# [2.44.0] - 2023-01-12\n\n## I/Os\n\n* Support for Bigtable sink (Write and WriteBatch) added (Go) ([#23324](https://github.com/apache/beam/issues/23324)).\n* S3 implementation of the Beam filesystem (Go) ([#23991](https://github.com/apache/beam/issues/23991)).\n* Support for SingleStoreDB source and sink added (Java) ([#22617](https://github.com/apache/beam/issues/22617)).\n* Added support for DefaultAzureCredential authentication in Azure Filesystem (Python) ([#24210](https://github.com/apache/beam/issues/24210)).\n\n## New Features / Improvements\n\n* Beam now provides a portable \"runner\" that can render pipeline graphs with\n  graphviz.  See `python -m apache_beam.runners.render --help` for more details.\n* Local packages can now be used as dependencies in the requirements.txt file, rather\n  than requiring them to be passed separately via the `--extra_package` option\n  (Python) ([#23684](https://github.com/apache/beam/pull/23684)).\n* Pipeline Resource Hints now supported via `--resource_hints` flag (Go) ([#23990](https://github.com/apache/beam/pull/23990)).\n* Make Python SDK containers reusable on portable runners by installing dependencies to temporary venvs ([BEAM-12792](https://issues.apache.org/jira/browse/BEAM-12792), [#16658](https://github.com/apache/beam/pull/16658)).\n* RunInference model handlers now support the specification of a custom inference function in Python ([#22572](https://github.com/apache/beam/issues/22572)).\n* Support for `map_windows` urn added to Go SDK ([#24307](https://github.apache/beam/pull/24307)).\n\n## Breaking Changes\n\n* `ParquetIO.withSplit` was removed since splittable reading has been the default behavior since 2.35.0. The effect of\n  this change is to drop support for non-splittable reading (Java)([#23832](https://github.com/apache/beam/issues/23832)).\n* `beam-sdks-java-extensions-google-cloud-platform-core` is no longer a\n  dependency of the Java SDK Harness. Some users of a portable runner (such as Dataflow Runner v2)\n  may have an undeclared dependency on this package (for example using GCS with\n  TextIO) and will now need to declare the dependency.\n* `beam-sdks-java-core` is no longer a dependency of the Java SDK Harness. Users of a portable\n  runner (such as Dataflow Runner v2) will need to provide this package and its dependencies.\n* Slices now use the Beam Iterable Coder. This enables cross language use, but breaks pipeline updates\n  if a Slice type is used as a PCollection element or State API element. (Go)[#24339](https://github.com/apache/beam/issues/24339)\n* If you activated a virtual environment in your custom container image, this environment might no longer be activated, since a new environment will be created (see the note about [BEAM-12792](https://issues.apache.org/jira/browse/BEAM-12792) above).\n  To work around, install dependencies into the default (global) python environment. When using poetry you may need to use `poetry config virtualenvs.create false` before installing deps, see an example in: [#25085](https://github.com/apache/beam/issues/25085).\n  If you were negatively impacted by this change and cannot find a workaround, feel free to chime in on [#16658](https://github.com/apache/beam/pull/16658).\n  To disable this behavior, you could upgrade to Beam 2.48.0 and set an environment variable\n  `ENV RUN_PYTHON_SDK_IN_DEFAULT_ENVIRONMENT=1` in your Dockerfile.\n\n## Deprecations\n\n* X behavior is deprecated and will be removed in X versions ([#X](https://github.com/apache/beam/issues/X)).\n\n## Bugfixes\n\n* Fixed X (Java/Python) ([#X](https://github.com/apache/beam/issues/X)).\n* Fixed JmsIO acknowledgment issue (Java) ([#20814](https://github.com/apache/beam/issues/20814))\n* Fixed Beam SQL CalciteUtils (Java) and Cross-language JdbcIO (Python) did not support JDBC CHAR/VARCHAR, BINARY/VARBINARY logical types ([#23747](https://github.com/apache/beam/issues/23747), [#23526](https://github.com/apache/beam/issues/23526)).\n* Ensure iterated and emitted types are used with the generic register package are registered with the type and schema registries.(Go) ([#23889](https://github.com/apache/beam/pull/23889))\n\n\n# [2.43.0] - 2022-11-17\n\n## Highlights\n\n* Python 3.10 support in Apache Beam ([#21458](https://github.com/apache/beam/issues/21458)).\n* An initial implementation of a runner that allows us to run Beam pipelines on Dask. Try it out and give us feedback! (Python) ([#18962](https://github.com/apache/beam/issues/18962)).\n\n## I/Os\n\n* Decreased TextSource CPU utilization by 2.3x (Java) ([#23193](https://github.com/apache/beam/issues/23193)).\n* Fixed bug when using SpannerIO with RuntimeValueProvider options (Java) ([#22146](https://github.com/apache/beam/issues/22146)).\n* Fixed issue for unicode rendering on WriteToBigQuery ([#22312](https://github.com/apache/beam/issues/22312))\n* Remove obsolete variants of BigQuery Read and Write, always using Beam-native variant\n  ([#23564](https://github.com/apache/beam/issues/23564) and [#23559](https://github.com/apache/beam/issues/23559)).\n* Bumped google-cloud-spanner dependency version to 3.x for Python SDK ([#21198](https://github.com/apache/beam/issues/21198)).\n\n## New Features / Improvements\n\n* Dataframe wrapper added in Go SDK via Cross-Language (with automatic expansion service). (Go) ([#23384](https://github.com/apache/beam/issues/23384)).\n* Name all Java threads to aid in debugging ([#23049](https://github.com/apache/beam/issues/23049)).\n* An initial implementation of a runner that allows us to run Beam pipelines on Dask. (Python) ([#18962](https://github.com/apache/beam/issues/18962)).\n* Allow configuring GCP OAuth scopes via pipeline options. This unblocks usages of Beam IOs that require additional scopes.\n  For example, this feature makes it possible to access Google Drive backed tables in BigQuery ([#23290](https://github.com/apache/beam/issues/23290)).\n* An example for using Python RunInference from Java ([#23290](https://github.com/apache/beam/pull/23619)).\n* Data can now be read from BigQuery and directly plumbed into a DeferredDataframe in the Dataframe API. Users no longer have to re-specify the schema in this case ([#22907](https://github.com/apache/beam/pull/22907)).\n\n## Breaking Changes\n\n* CoGroupByKey transform in Python SDK has changed the output typehint. The typehint component representing grouped values changed from List to Iterable,\n  which more accurately reflects the nature of the arbitrarily large output collection. [#21556](https://github.com/apache/beam/issues/21556) Beam users may see an error on transforms downstream from CoGroupByKey. Users must change methods expecting a List to expect an Iterable going forward. See [document](https://docs.google.com/document/d/1RIzm8-g-0CyVsPb6yasjwokJQFoKHG4NjRUcKHKINu0) for information and fixes.\n* The PortableRunner for Spark assumes Spark 3 as default Spark major version unless configured otherwise using `--spark_version`.\n  Spark 2 support is deprecated and will be removed soon ([#23728](https://github.com/apache/beam/issues/23728)).\n\n## Bugfixes\n\n* Fixed Python cross-language JDBC IO Connector cannot read or write rows containing Numeric/Decimal type values ([#19817](https://github.com/apache/beam/issues/19817)).\n\n# [2.42.0] - 2022-10-17\n\n## Highlights\n\n* Added support for stateful DoFns to the Go SDK.\n* Added support for [Batched\n  DoFns](https://beam.apache.org/documentation/programming-guide/#batched-dofns)\n  to the Python SDK.\n\n## New Features / Improvements\n\n* Added support for Zstd compression to the Python SDK.\n* Added support for Google Cloud Profiler to the Go SDK.\n* Added support for stateful DoFns to the Go SDK.\n\n## Breaking Changes\n\n* The Go SDK's Row Coder now uses a different single-precision float encoding for float32 types to match Java's behavior ([#22629](https://github.com/apache/beam/issues/22629)).\n\n## Bugfixes\n\n* Fixed Python cross-language JDBC IO Connector cannot read or write rows containing Timestamp type values [#19817](https://github.com/apache/beam/issues/19817).\n* Fixed `AfterProcessingTime` behavior in Python's `DirectRunner` to match Java ([#23071](https://github.com/apache/beam/issues/23071))\n\n## Known Issues\n\n* Go SDK doesn't yet support Slowly Changing Side Input pattern ([#23106](https://github.com/apache/beam/issues/23106))\n\n# [2.41.0] - 2022-08-23\n\n## I/Os\n\n* Projection Pushdown optimizer is now on by default for streaming, matching the behavior of batch pipelines since 2.38.0. If you encounter a bug with the optimizer, please file an issue and disable the optimizer using pipeline option `--experiments=disable_projection_pushdown`.\n\n## New Features / Improvements\n\n* Previously available in Java sdk, Python sdk now also supports logging level overrides per module. ([#18222](https://github.com/apache/beam/issues/18222)).\n* Added support for accessing GCP PubSub Message ordering keys (Java) ([BEAM-13592](https://issues.apache.org/jira/browse/BEAM-13592))\n\n## Breaking Changes\n\n* Projection Pushdown optimizer may break Dataflow upgrade compatibility for optimized pipelines when it removes unused fields. If you need to upgrade and encounter a compatibility issue, disable the optimizer using pipeline option `--experiments=disable_projection_pushdown`.\n\n## Deprecations\n\n* Support for Spark 2.4.x is deprecated and will be dropped with the release of Beam 2.44.0 or soon after (Spark runner) ([#22094](https://github.com/apache/beam/issues/22094)).\n* The modules [amazon-web-services](https://github.com/apache/beam/tree/master/sdks/java/io/amazon-web-services) and\n  [kinesis](https://github.com/apache/beam/tree/master/sdks/java/io/kinesis) for AWS Java SDK v1 are deprecated\n  in favor of [amazon-web-services2](https://github.com/apache/beam/tree/master/sdks/java/io/amazon-web-services2)\n  and will be eventually removed after a few Beam releases (Java) ([#21249](https://github.com/apache/beam/issues/21249)).\n\n## Bugfixes\n\n* Fixed a condition where retrying queries would yield an incorrect cursor in the Java SDK Firestore Connector ([#22089](https://github.com/apache/beam/issues/22089)).\n* Fixed plumbing allowed lateness in Go SDK. It was ignoring the user set value earlier and always used to set to 0. ([#22474](https://github.com/apache/beam/issues/22474)).\n\n\n# [2.40.0] - 2022-06-25\n\n## Highlights\n\n* Added [RunInference](https://s.apache.org/inference-sklearn-pytorch) API, a framework agnostic transform for inference. With this release, PyTorch and Scikit-learn are supported by the transform.\n    See also example at apache_beam/examples/inference/pytorch_image_classification.py\n\n## I/Os\n\n* Upgraded to Hive 3.1.3 for HCatalogIO. Users can still provide their own version of Hive. (Java) ([Issue-19554](https://github.com/apache/beam/issues/19554)).\n\n## New Features / Improvements\n\n* Go SDK users can now use generic registration functions to optimize their DoFn execution. ([BEAM-14347](https://issues.apache.org/jira/browse/BEAM-14347))\n* Go SDK users may now write self-checkpointing Splittable DoFns to read from streaming sources. ([BEAM-11104](https://issues.apache.org/jira/browse/BEAM-11104))\n* Go SDK textio Reads have been moved to Splittable DoFns exclusively. ([BEAM-14489](https://issues.apache.org/jira/browse/BEAM-14489))\n* Pipeline drain support added for Go SDK has now been tested. ([BEAM-11106](https://issues.apache.org/jira/browse/BEAM-11106))\n* Go SDK users can now see heap usage, sideinput cache stats, and active process bundle stats in Worker Status. ([BEAM-13829](https://issues.apache.org/jira/browse/BEAM-13829))\n\n## Breaking Changes\n\n* The Go Sdk now requires a minimum version of 1.18 in order to support generics ([BEAM-14347](https://issues.apache.org/jira/browse/BEAM-14347)).\n* synthetic.SourceConfig field types have changed to int64 from int for better compatibility with Flink's use of Logical types in Schemas (Go) ([BEAM-14173](https://issues.apache.org/jira/browse/BEAM-14173))\n* Default coder updated to compress sources used with `BoundedSourceAsSDFWrapperFn` and `UnboundedSourceAsSDFWrapper`.\n\n## Bugfixes\n* Fixed Java expansion service to allow specific files to stage ([BEAM-14160](https://issues.apache.org/jira/browse/BEAM-14160)).\n* Fixed Elasticsearch connection when using both ssl and username/password (Java) ([BEAM-14000](https://issues.apache.org/jira/browse/BEAM-14000))\n\n# [2.39.0] - 2022-05-25\n\n## Highlights\n\n* Watermark estimation is now supported in the Go SDK ([BEAM-11105](https://issues.apache.org/jira/browse/BEAM-11105)).\n* Support for impersonation credentials added to dataflow runner in the Java and Python SDK ([BEAM-14014](https://issues.apache.org/jira/browse/BEAM-14014)).\n* Implemented Apache PulsarIO ([BEAM-8218](https://issues.apache.org/jira/browse/BEAM-8218)).\n\n## I/Os\n\n* JmsIO gains the ability to map any kind of input to any subclass of `javax.jms.Message` (Java) ([BEAM-16308](https://issues.apache.org/jira/browse/BEAM-16308)).\n* JmsIO introduces the ability to write to dynamic topics (Java) ([BEAM-16308](https://issues.apache.org/jira/browse/BEAM-16308)).\n  * A `topicNameMapper` must be set to extract the topic name from the input value.\n  * A `valueMapper` must be set to convert the input value to JMS message.\n* Reduce number of threads spawned by BigqueryIO StreamingInserts (\n  [BEAM-14283](https://issues.apache.org/jira/browse/BEAM-14283)).\n* Implemented Apache PulsarIO ([BEAM-8218](https://issues.apache.org/jira/browse/BEAM-8218)).\n\n\n## New Features / Improvements\n\n* Support for flink scala 2.12, because most of the libraries support version 2.12 onwards. ([beam-14386](https://issues.apache.org/jira/browse/BEAM-14386))\n* 'Manage Clusters' JupyterLab extension added for users to configure usage of Dataproc clusters managed by Interactive Beam (Python) ([BEAM-14130](https://issues.apache.org/jira/browse/BEAM-14130)).\n* Pipeline drain support added for Go SDK ([BEAM-11106](https://issues.apache.org/jira/browse/BEAM-11106)). **Note: this feature is not yet fully validated and should be treated as experimental in this release.**\n* `DataFrame.unstack()`, `DataFrame.pivot() ` and  `Series.unstack()`\n  implemented for DataFrame API ([BEAM-13948](https://issues.apache.org/jira/browse/BEAM-13948), [BEAM-13966](https://issues.apache.org/jira/browse/BEAM-13966)).\n* Support for impersonation credentials added to dataflow runner in the Java and Python SDK ([BEAM-14014](https://issues.apache.org/jira/browse/BEAM-14014)).\n* Implemented Jupyterlab extension for managing Dataproc clusters ([BEAM-14130](https://issues.apache.org/jira/browse/BEAM-14130)).\n* ExternalPythonTransform API added for easily invoking Python transforms from\n  Java ([BEAM-14143](https://issues.apache.org/jira/browse/BEAM-14143)).\n* Added Add support for Elasticsearch 8.x ([BEAM-14003](https://issues.apache.org/jira/browse/BEAM-14003)).\n* Shard aware Kinesis record aggregation (AWS Sdk v2), ([BEAM-14104](https://issues.apache.org/jira/browse/BEAM-14104)).\n* Upgrade to ZetaSQL 2022.04.1 ([BEAM-14348](https://issues.apache.org/jira/browse/BEAM-14348)).\n* Fixed ReadFromBigQuery cannot be used with the interactive runner ([BEAM-14112](https://issues.apache.org/jira/browse/BEAM-14112)).\n\n\n## Breaking Changes\n\n* Unused functions `ShallowCloneParDoPayload()`, `ShallowCloneSideInput()`, and `ShallowCloneFunctionSpec()` have been removed from the Go SDK's pipelinex package ([BEAM-13739](https://issues.apache.org/jira/browse/BEAM-13739)).\n* JmsIO requires an explicit `valueMapper` to be set ([BEAM-16308](https://issues.apache.org/jira/browse/BEAM-16308)). You can use the `TextMessageMapper` to convert `String` inputs to JMS `TestMessage`s:\n```java\n  JmsIO.<String>write()\n        .withConnectionFactory(jmsConnectionFactory)\n        .withValueMapper(new TextMessageMapper());\n```\n* Coders in Python are expected to inherit from Coder. ([BEAM-14351](https://issues.apache.org/jira/browse/BEAM-14351)).\n* New abstract method `metadata()` added to io.filesystem.FileSystem in the\n  Python SDK. ([BEAM-14314](https://issues.apache.org/jira/browse/BEAM-14314))\n\n## Deprecations\n\n* Flink 1.11 is no longer supported ([BEAM-14139](https://issues.apache.org/jira/browse/BEAM-14139)).\n* Python 3.6 is no longer supported ([BEAM-13657](https://issues.apache.org/jira/browse/BEAM-13657)).\n\n## Bugfixes\n\n* Fixed Java Spanner IO NPE when ProjectID not specified in template executions (Java) ([BEAM-14405](https://issues.apache.org/jira/browse/BEAM-14405)).\n* Fixed potential NPE in BigQueryServicesImpl.getErrorInfo (Java) ([BEAM-14133](https://issues.apache.org/jira/browse/BEAM-14133)).\n\n\n# [2.38.0] - 2022-04-20\n\n## I/Os\n* Introduce projection pushdown optimizer to the Java SDK ([BEAM-12976](https://issues.apache.org/jira/browse/BEAM-12976)). The optimizer currently only works on the [BigQuery Storage API](https://beam.apache.org/documentation/io/built-in/google-bigquery/#storage-api), but more I/Os will be added in future releases. If you encounter a bug with the optimizer, please file a JIRA and disable the optimizer using pipeline option `--experiments=disable_projection_pushdown`.\n* A new IO for Neo4j graph databases was added. ([BEAM-1857](https://issues.apache.org/jira/browse/BEAM-1857))  It has the ability to update nodes and relationships using UNWIND statements and to read data using cypher statements with parameters.\n* `amazon-web-services2` has reached feature parity and is finally recommended over the earlier `amazon-web-services` and `kinesis` modules (Java). These will be deprecated in one of the next releases ([BEAM-13174](https://issues.apache.org/jira/browse/BEAM-13174)).\n  * Long outstanding write support for `Kinesis` was added ([BEAM-13175](https://issues.apache.org/jira/browse/BEAM-13175)).\n  * Configuration was simplified and made consistent across all IOs, including the usage of `AwsOptions` ([BEAM-13563](https://issues.apache.org/jira/browse/BEAM-13563), [BEAM-13663](https://issues.apache.org/jira/browse/BEAM-13663), [BEAM-13587](https://issues.apache.org/jira/browse/BEAM-13587)).\n  * Additionally, there's a long list of recent improvements and fixes to\n    `S3` Filesystem ([BEAM-13245](https://issues.apache.org/jira/browse/BEAM-13245), [BEAM-13246](https://issues.apache.org/jira/browse/BEAM-13246), [BEAM-13441](https://issues.apache.org/jira/browse/BEAM-13441), [BEAM-13445](https://issues.apache.org/jira/browse/BEAM-13445), [BEAM-14011](https://issues.apache.org/jira/browse/BEAM-14011)),\n    `DynamoDB` IO ([BEAM-13209](https://issues.apache.org/jira/browse/BEAM-13009), [BEAM-13209](https://issues.apache.org/jira/browse/BEAM-13209)),\n    `SQS` IO ([BEAM-13631](https://issues.apache.org/jira/browse/BEAM-13631), [BEAM-13510](https://issues.apache.org/jira/browse/BEAM-13510)) and others.\n\n## New Features / Improvements\n\n* Pipeline dependencies supplied through `--requirements_file` will now be staged to the runner using binary distributions (wheels) of the PyPI packages for linux_x86_64 platform ([BEAM-4032](https://issues.apache.org/jira/browse/BEAM-4032)). To restore the behavior to use source distributions, set pipeline option `--requirements_cache_only_sources`. To skip staging the packages at submission time, set pipeline option `--requirements_cache=skip` (Python).\n* The Flink runner now supports Flink 1.14.x ([BEAM-13106](https://issues.apache.org/jira/browse/BEAM-13106)).\n* Interactive Beam now supports remotely executing Flink pipelines on Dataproc (Python) ([BEAM-14071](https://issues.apache.org/jira/browse/BEAM-14071)).\n\n## Breaking Changes\n\n* (Python) Previously `DoFn.infer_output_types` was expected to return `Iterable[element_type]` where `element_type` is the PCollection elemnt type. It is now expected to return `element_type`. Take care if you have overriden `infer_output_type` in a `DoFn` (this is not common). See [BEAM-13860](https://issues.apache.org/jira/browse/BEAM-13860).\n* (`amazon-web-services2`) The types of `awsRegion` / `endpoint` in `AwsOptions` changed from String to `Region` / `URI` ([BEAM-13563](https://issues.apache.org/jira/browse/BEAM-13563)).\n\n## Deprecations\n\n* Beam 2.38.0 will be the last minor release to support Flink 1.11.\n* (`amazon-web-services2`) Client providers (`withXYZClientProvider()`) as well as IO specific `RetryConfiguration`s are deprecated, instead use `withClientConfiguration()` or `AwsOptions` to configure AWS IOs / clients.\n  Custom implementations of client providers shall be replaced with a respective `ClientBuilderFactory` and configured through `AwsOptions` ([BEAM-13563](https://issues.apache.org/jira/browse/BEAM-13563)).\n\n## Bugfixes\n\n* Fix S3 copy for large objects (Java) ([BEAM-14011](https://issues.apache.org/jira/browse/BEAM-14011))\n* Fix quadratic behavior of pipeline canonicalization (Go) ([BEAM-14128](https://issues.apache.org/jira/browse/BEAM-14128))\n  * This caused unnecessarily long pre-processing times before job submission for large complex pipelines.\n* Fix `pyarrow` version parsing (Python)([BEAM-14235](https://issues.apache.org/jira/browse/BEAM-14235))\n\n## Known Issues\n\n* Some pipelines that use Java SpannerIO may raise a NPE when the project ID is not specified ([BEAM-14405](https://issues.apache.org/jira/browse/BEAM-14405))\n\n# [2.37.0] - 2022-03-04\n\n## Highlights\n* Java 17 support for Dataflow ([BEAM-12240](https://issues.apache.org/jira/browse/BEAM-12240)).\n  * Users using Dataflow Runner V2 may see issues with state cache due to inaccurate object sizes ([BEAM-13695](https://issues.apache.org/jira/browse/BEAM-13695)).\n  * ZetaSql is currently unsupported ([issue](https://github.com/google/zetasql/issues/89)).\n* Python 3.9 support in Apache Beam ([BEAM-12000](https://issues.apache.org/jira/browse/BEAM-12000)).\n\n## I/Os\n\n* Go SDK now has wrappers for the following Cross Language Transforms from Java, along with automatic expansion service startup for each.\n    *  JDBCIO ([BEAM-13293](https://issues.apache.org/jira/browse/BEAM-13293)).\n    *  Debezium ([BEAM-13761](https://issues.apache.org/jira/browse/BEAM-13761)).\n    *  BeamSQL ([BEAM-13683](https://issues.apache.org/jira/browse/BEAM-13683)).\n    *  BiqQuery ([BEAM-13732](https://issues.apache.org/jira/browse/BEAM-13732)).\n    *  KafkaIO now also has automatic expansion service startup. ([BEAM-13821](https://issues.apache.org/jira/browse/BEAM-13821)).\n\n## New Features / Improvements\n\n* DataFrame API now supports pandas 1.4.x ([BEAM-13605](https://issues.apache.org/jira/browse/BEAM-13605)).\n* Go SDK DoFns can now observe trigger panes directly ([BEAM-13757](https://issues.apache.org/jira/browse/BEAM-13757)).\n* Added option to specify a caching directory in Interactive Beam (Python) ([BEAM-13685](https://issues.apache.org/jira/browse/BEAM-13685)).\n* Added support for caching batch pipelines to GCS in Interactive Beam (Python) ([BEAM-13734](https://issues.apache.org/jira/browse/BEAM-13734)).\n\n## Breaking Changes\n\n## Deprecations\n\n## Bugfixes\n\n## Known Issues\n\n* On rare occations, Python Datastore source may swallow some exceptions. Users are adviced to upgrade to Beam 2.38.0 or later ([BEAM-14282](https://issues.apache.org/jira/browse/BEAM-14282))\n* On rare occations, Python GCS source may swallow some exceptions. Users are adviced to upgrade to Beam 2.38.0 or later ([BEAM-14282](https://issues.apache.org/jira/browse/BEAM-14282))\n\n# [2.36.0] - 2022-02-07\n\n## I/Os\n\n* Support for stopReadTime on KafkaIO SDF (Java).([BEAM-13171](https://issues.apache.org/jira/browse/BEAM-13171)).\n* Added ability to register URI schemes to use the S3 protocol via FileIO using amazon-web-services2 (amazon-web-services already had this ability). ([BEAM-12435](https://issues.apache.org/jira/brows/BEAM-12435), [BEAM-13245](https://issues.apache.org/jira/brows/BEAM-13245)).\n\n## New Features / Improvements\n\n* Added support for cloudpickle as a pickling library for Python SDK ([BEAM-8123](https://issues.apache.org/jira/browse/BEAM-8123)). To use cloudpickle, set pipeline option: --pickler_lib=cloudpickle\n* Added option to specify triggering frequency when streaming to BigQuery (Python) ([BEAM-12865](https://issues.apache.org/jira/browse/BEAM-12865)).\n* Added option to enable caching uploaded artifacts across job runs for Python Dataflow jobs ([BEAM-13459](https://issues.apache.org/jira/browse/BEAM-13459)).  To enable, set pipeline option: --enable_artifact_caching, this will be enabled by default in a future release.\n\n## Breaking Changes\n\n* Updated the jedis from 3.x to 4.x to Java RedisIO. If you are using RedisIO and using jedis directly, please refer to [this page](https://github.com/redis/jedis/blob/v4.0.0/docs/3to4.md) to update it. ([BEAM-12092](https://issues.apache.org/jira/browse/BEAM-12092)).\n* Datatype of timestamp fields in `SqsMessage` for AWS IOs for SDK v2 was changed from `String` to `long`, visibility of all fields was fixed from `package private` to `public` [BEAM-13638](https://issues.apache.org/jira/browse/BEAM-13638).\n\n## Bugfixes\n\n* Properly check output timestamps on elements output from DoFns, timers, and onWindowExpiration in Java [BEAM-12931](https://issues.apache.org/jira/browse/BEAM-12931).\n* Fixed a bug with DeferredDataFrame.xs when used with a non-tuple key\n  ([BEAM-13421](https://issues.apache.org/jira/browse/BEAM-13421])).\n\n# Known Issues\n\n* Users may encounter an unexpected java.lang.ArithmeticException when outputting a timestamp\n  for an element further than allowedSkew from an allowed DoFN skew set to a value more than\n  Integer.MAX_VALUE.\n* On rare occations, Python Datastore source may swallow some exceptions. Users are adviced to upgrade to Beam 2.38.0 or later ([BEAM-14282](https://issues.apache.org/jira/browse/BEAM-14282))\n* On rare occations, Python GCS source may swallow some exceptions. Users are adviced to upgrade to Beam 2.38.0 or later ([BEAM-14282](https://issues.apache.org/jira/browse/BEAM-14282))\n* On rare occations, Java SpannerIO source may swallow some exceptions. Users are adviced to upgrade to Beam 2.37.0 or later ([BEAM-14005](https://issues.apache.org/jira/browse/BEAM-14005))\n\n# [2.35.0] - 2021-12-29\n\n## Highlights\n\n* MultiMap side inputs are now supported by the Go SDK ([BEAM-3293](https://issues.apache.org/jira/browse/BEAM-3293)).\n* Side inputs are supported within Splittable DoFns for Dataflow Runner V1 and Dataflow Runner V2. ([BEAM-12522](https://issues.apache.org/jira/browse/BEAM-12522)).\n* Upgrades Log4j version used in test suites (Apache Beam testing environment only, not for end user consumption) to 2.17.0([BEAM-13434](https://issues.apache.org/jira/browse/BEAM-13434)).\n    Note that Apache Beam versions do not depend on the Log4j 2 dependency (log4j-core) impacted by [CVE-2021-44228](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-44228).\n    However we urge users to update direct and indirect dependencies (if any) on Log4j 2 to the latest version by updating their build configuration and redeploying impacted pipelines.\n\n## I/Os\n\n* We changed the data type for ranges in `JdbcIO.readWithPartitions` from `int` to `long` ([BEAM-13149](https://issues.apache.org/jira/browse/BEAM-13149)).\n    This is a relatively minor breaking change, which we're implementing to improve the usability of the transform without increasing cruft.\n    This transform is relatively new, so we may implement other breaking changes in the future to improve its usability.\n* Side inputs are supported within Splittable DoFns for Dataflow Runner V1 and Dataflow Runner V2. ([BEAM-12522](https://issues.apache.org/jira/browse/BEAM-12522)).\n\n## New Features / Improvements\n\n* Added custom delimiters to Python TextIO reads ([BEAM-12730](https://issues.apache.org/jira/browse/BEAM-12730)).\n* Added escapechar parameter to Python TextIO reads ([BEAM-13189](https://issues.apache.org/jira/browse/BEAM-13189)).\n* Splittable reading is enabled by default while reading data with ParquetIO ([BEAM-12070](https://issues.apache.org/jira/browse/BEAM-12070)).\n* DoFn Execution Time metrics added to Go ([BEAM-13001](https://issues.apache.org/jira/browse/BEAM-13001)).\n* Cross-bundle side input caching is now available in the Go SDK for runners that support the feature by setting the EnableSideInputCache hook ([BEAM-11097](https://issues.apache.org/jira/browse/BEAM-11097)).\n* Upgraded the GCP Libraries BOM version to 24.0.0 and associated dependencies ([BEAM-11205](\n  https://issues.apache.org/jira/browse/BEAM-11205)). For Google Cloud client library versions set by this BOM,\n  see [this table](https://storage.googleapis.com/cloud-opensource-java-dashboard/com.google.cloud/libraries-bom/24.0.0/artifact_details.html).\n* Removed avro-python3 dependency in AvroIO. Fastavro has already been our Avro library of choice on Python 3. Boolean use_fastavro is left for api compatibility, but will have no effect.([BEAM-13016](https://github.com/apache/beam/pull/15900)).\n* MultiMap side inputs are now supported by the Go SDK ([BEAM-3293](https://issues.apache.org/jira/browse/BEAM-3293)).\n* Remote packages can now be downloaded from locations supported by apache_beam.io.filesystems. The files will be downloaded on Stager and uploaded to staging location. For more information, see [BEAM-11275](https://issues.apache.org/jira/browse/BEAM-11275)\n\n## Breaking Changes\n\n* A new URN convention was adopted for cross-language transforms and existing URNs were updated. This may break advanced use-cases, for example, if a custom expansion service is used to connect diffrent Beam Java and Python versions. ([BEAM-12047](https://issues.apache.org/jira/browse/BEAM-12047)).\n* The upgrade to Calcite 1.28.0 introduces a breaking change in the SUBSTRING function in SqlTransform, when used with the Calcite dialect ([BEAM-13099](https://issues.apache.org/jira/browse/BEAM-13099), [CALCITE-4427](https://issues.apache.org/jira/browse/CALCITE-4427)).\n* ListShards (with DescribeStreamSummary) is used instead of DescribeStream to list shards in Kinesis streams (AWS SDK v2). Due to this change, as mentioned in [AWS documentation](https://docs.aws.amazon.com/kinesis/latest/APIReference/API_ListShards.html), for fine-grained IAM policies it is required to update them to allow calls to ListShards and DescribeStreamSummary APIs. For more information, see [Controlling Access to Amazon Kinesis Data Streams](https://docs.aws.amazon.com/streams/latest/dev/controlling-access.html) ([BEAM-13233](https://issues.apache.org/jira/browse/BEAM-13233)).\n\n## Deprecations\n\n* Non-splittable reading is deprecated while reading data with ParquetIO ([BEAM-12070](https://issues.apache.org/jira/browse/BEAM-12070)).\n\n## Bugfixes\n\n* Properly map main input windows to side input windows by default (Go)\n  ([BEAM-11087](https://issues.apache.org/jira/browse/BEAM-11087)).\n* Fixed data loss when writing to DynamoDB without setting deduplication key names (Java)\n  ([BEAM-13009](https://issues.apache.org/jira/browse/BEAM-13009)).\n* Go SDK Examples now have types and functions registered. (Go) ([BEAM-5378](https://issues.apache.org/jira/browse/BEAM-5378))\n\n## Known Issues\n\n* Users of beam-sdks-java-io-hcatalog (and beam-sdks-java-extensions-sql-hcatalog) must take care to override the transitive log4j dependency when they add a hive dependency ([BEAM-13499](https://issues.apache.org/jira/browse/BEAM-13499)).\n* On rare occations, Python Datastore source may swallow some exceptions. Users are adviced to upgrade to Beam 2.38.0 or later ([BEAM-14282](https://issues.apache.org/jira/browse/BEAM-14282))\n* On rare occations, Python GCS source may swallow some exceptions. Users are adviced to upgrade to Beam 2.38.0 or later ([BEAM-14282](https://issues.apache.org/jira/browse/BEAM-14282))\n* On rare occations, Java SpannerIO source may swallow some exceptions. Users are adviced to upgrade to Beam 2.37.0 or later ([BEAM-14005](https://issues.apache.org/jira/browse/BEAM-14005))\n\n# [2.34.0] - 2021-11-11\n\n## Highlights\n\n* The Beam Java API for Calcite SqlTransform is no longer experimental ([BEAM-12680](https://issues.apache.org/jira/browse/BEAM-12680)).\n* Python's ParDo (Map, FlatMap, etc.) transforms now suport a `with_exception_handling` option for easily ignoring bad records and implementing the dead letter pattern.\n\n## I/Os\n\n* `ReadFromBigQuery` and `ReadAllFromBigQuery` now run queries with BATCH priority by default. The `query_priority` parameter is introduced to the same transforms to allow configuring the query priority (Python) ([BEAM-12913](https://issues.apache.org/jira/browse/BEAM-12913)).\n* [EXPERIMENTAL] Support for [BigQuery Storage Read API](https://cloud.google.com/bigquery/docs/reference/storage) added to `ReadFromBigQuery`. The newly introduced `method` parameter can be set as `DIRECT_READ` to use the Storage Read API. The default is `EXPORT` which invokes a BigQuery export request. (Python) ([BEAM-10917](https://issues.apache.org/jira/browse/BEAM-10917)).\n* [EXPERIMENTAL] Added `use_native_datetime` parameter to `ReadFromBigQuery` to configure the return type of [DATETIME](https://cloud.google.com/bigquery/docs/reference/standard-sql/data-types#datetime_type) fields when using `ReadFromBigQuery`. This parameter can *only* be used when `method = DIRECT_READ`(Python) ([BEAM-10917](https://issues.apache.org/jira/browse/BEAM-10917)).\n* [EXPERIMENTAL] Added support for writing to [Redis Streams](https://redis.io/topics/streams-intro) as a sink in RedisIO ([BEAM-13159](https://issues.apache.org/jira/browse/BEAM-13159))\n\n## New Features / Improvements\n\n* Upgraded to Calcite 1.26.0 ([BEAM-9379](https://issues.apache.org/jira/browse/BEAM-9379)).\n* Added a new `dataframe` extra to the Python SDK that tracks `pandas` versions\n  we've verified compatibility with. We now recommend installing Beam with `pip\n  install apache-beam[dataframe]` when you intend to use the DataFrame API\n  ([BEAM-12906](https://issues.apache.org/jira/browse/BEAM-12906)).\n* Added an [example](https://github.com/cometta/python-apache-beam-spark) of deploying Python Apache Beam job with Spark Cluster\n\n## Breaking Changes\n\n* SQL Rows are no longer flattened ([BEAM-5505](https://issues.apache.org/jira/browse/BEAM-5505)).\n* [Go SDK] beam.TryCrossLanguage's signature now matches beam.CrossLanguage. Like other Try functions it returns an error instead of panicking. ([BEAM-9918](https://issues.apache.org/jira/browse/BEAM-9918)).\n* [BEAM-12925](https://jira.apache.org/jira/browse/BEAM-12925) was fixed. It used to silently pass incorrect null data read from JdbcIO. Pipelines affected by this will now start throwing failures instead of silently passing incorrect data.\n\n## Bugfixes\n\n* Fixed error while writing multiple DeferredFrames to csv (Python) ([BEAM-12701](https://issues.apache.org/jira/browse/BEAM-12701)).\n* Fixed error when importing the DataFrame API with pandas 1.0.x installed ([BEAM-12945](https://issues.apache.org/jira/browse/BEAM-12945)).\n* Fixed top.SmallestPerKey implementation in the Go SDK ([BEAM-12946](https://issues.apache.org/jira/browse/BEAM-12946)).\n\n## Known Issues\n\n* On rare occations, Python Datastore source may swallow some exceptions. Users are adviced to upgrade to Beam 2.38.0 or later ([BEAM-14282](https://issues.apache.org/jira/browse/BEAM-14282))\n* On rare occations, Python GCS source may swallow some exceptions. Users are adviced to upgrade to Beam 2.38.0 or later ([BEAM-14282](https://issues.apache.org/jira/browse/BEAM-14282))\n* On rare occations, Java SpannerIO source may swallow some exceptions. Users are adviced to upgrade to Beam 2.37.0 or later ([BEAM-14005](https://issues.apache.org/jira/browse/BEAM-14005))\n\n# [2.33.0] - 2021-10-07\n\n## Highlights\n\n* Go SDK is no longer experimental, and is officially part of the Beam release process.\n  * Matching Go SDK containers are published on release.\n  * Batch usage is well supported, and tested on Flink, Spark, and the Python Portable Runner.\n    * SDK Tests are also run against Google Cloud Dataflow, but this doesn't indicate reciprical support.\n  * The SDK supports Splittable DoFns, Cross Language transforms, and most Beam Model basics.\n  * Go Modules are now used for dependency management.\n    * This is a breaking change, see Breaking Changes for resolution.\n    * Easier path to contribute to the Go SDK, no need to set up a GO\\_PATH.\n    * Minimum Go version is now Go v1.16\n  * See the announcement blogpost for full information once published.\n\n<!--\n## I/Os\n\n* Support for X source added (Java/Python) ([BEAM-X](https://issues.apache.org/jira/browse/BEAM-X)).\n-->\n\n## New Features / Improvements\n\n* Projection pushdown in SchemaIO ([BEAM-12609](https://issues.apache.org/jira/browse/BEAM-12609)).\n* Upgrade Flink runner to Flink versions 1.13.2, 1.12.5 and 1.11.4 ([BEAM-10955](https://issues.apache.org/jira/browse/BEAM-10955)).\n\n## Breaking Changes\n\n* Go SDK pipelines require new import paths to use this release due to migration to Go Modules.\n  * `go.mod` files will need to change to require `github.com/apache/beam/sdks/v2`.\n  * Code depending on beam imports need to include v2 on the module path.\n    * Fix by'v2' to the import paths, turning  `.../sdks/go/...` to `.../sdks/v2/go/...`\n  * No other code change should be required to use v2.33.0 of the Go SDK.\n* Since release 2.30.0, \"The AvroCoder changes for BEAM-2303 \\[changed\\] the reader/writer from the Avro ReflectDatum* classes to the SpecificDatum* classes\" (Java). This default behavior change has been reverted in this release. Use the `useReflectApi` setting to control it ([BEAM-12628](https://issues.apache.org/jira/browse/BEAM-12628)).\n\n## Deprecations\n\n* Python GBK will stop supporting unbounded PCollections that have global windowing and a default trigger in Beam 2.34. This can be overriden with `--allow_unsafe_triggers`. ([BEAM-9487](https://issues.apache.org/jira/browse/BEAM-9487)).\n* Python GBK will start requiring safe triggers or the `--allow_unsafe_triggers` flag starting with Beam 2.34. ([BEAM-9487](https://issues.apache.org/jira/browse/BEAM-9487)).\n\n## Bug fixes\n\n* Workaround to not delete orphaned files to avoid missing events when using Python WriteToFiles in streaming pipeline ([BEAM-12950](https://issues.apache.org/jira/browse/BEAM-12950)))\n\n## Known Issues\n\n* Spark 2.x users will need to update Spark's Jackson runtime dependencies (`spark.jackson.version`) to at least version 2.9.2, due to Beam updating its dependencies.\n* Go SDK jobs may produce \"Failed to deduce Step from MonitoringInfo\" messages following successful job execution. The messages are benign and don't indicate job failure. These are due to not yet handling PCollection metrics.\n* On rare occations, Python GCS source may swallow some exceptions. Users are adviced to upgrade to Beam 2.38.0 or later ([BEAM-14282](https://issues.apache.org/jira/browse/BEAM-14282))\n\n# [2.32.0] - 2021-08-25\n\n## Highlights\n* The [Beam DataFrame\n  API](https://beam.apache.org/documentation/dsls/dataframes/overview/) is no\n  longer experimental! We've spent the time since the [2.26.0 preview\n  announcement](https://beam.apache.org/blog/dataframe-api-preview-available/)\n  implementing the most frequently used pandas operations\n  ([BEAM-9547](https://issues.apache.org/jira/browse/BEAM-9547)), improving\n  [documentation](https://beam.apache.org/releases/pydoc/current/apache_beam.dataframe.html)\n  and [error messages](https://issues.apache.org/jira/browse/BEAM-12028),\n  adding\n  [examples](https://github.com/apache/beam/tree/master/sdks/python/apache_beam/examples/dataframe),\n  integrating DataFrames with [interactive\n  Beam](https://beam.apache.org/releases/pydoc/current/apache_beam.runners.interactive.interactive_beam.html),\n  and of course finding and fixing\n  [bugs](https://issues.apache.org/jira/issues/?jql=project%3DBEAM%20AND%20issuetype%3DBug%20AND%20status%3DResolved%20AND%20component%3Ddsl-dataframe).\n  Leaving experimental just means that we now have high confidence in the API\n  and recommend its use for production workloads. We will continue to improve\n  the API, guided by your\n  [feedback](https://beam.apache.org/community/contact-us/).\n\n\n## I/Os\n\n* New experimental Firestore connector in Java SDK, providing sources and sinks to Google Cloud Firestore ([BEAM-8376](https://issues.apache.org/jira/browse/BEAM-8376)).\n* Added ability to use JdbcIO.Write.withResults without statement and preparedStatementSetter. ([BEAM-12511](https://issues.apache.org/jira/browse/BEAM-12511))\n- Added ability to register URI schemes to use the S3 protocol via FileIO. ([BEAM-12435](https://issues.apache.org/jira/browse/BEAM-12435)).\n* Respect number of shards set in SnowflakeWrite batch mode. ([BEAM-12715](https://issues.apache.org/jira/browse/BEAM-12715))\n* Java SDK: Update Google Cloud Healthcare IO connectors from using v1beta1 to using the GA version.\n\n## New Features / Improvements\n\n* Add support to convert Beam Schema to Avro Schema for JDBC LogicalTypes:\n  `VARCHAR`, `NVARCHAR`, `LONGVARCHAR`, `LONGNVARCHAR`, `DATE`, `TIME`\n  (Java)([BEAM-12385](https://issues.apache.org/jira/browse/BEAM-12385)).\n* Reading from JDBC source by partitions (Java) ([BEAM-12456](https://issues.apache.org/jira/browse/BEAM-12456)).\n* PubsubIO can now write to a dead-letter topic after a parsing error (Java)([BEAM-12474](https://issues.apache.org/jira/browse/BEAM-12474)).\n* New append-only option for Elasticsearch sink (Java) [BEAM-12601](https://issues.apache.org/jira/browse/BEAM-12601)\n* DatastoreIO: Write and delete operations now follow automatic gradual ramp-up, in line with best practices (Java/Python) ([BEAM-12260](https://issues.apache.org/jira/browse/BEAM-12260), [BEAM-12272](https://issues.apache.org/jira/browse/BEAM-12272)).\n\n## Breaking Changes\n\n* ListShards (with DescribeStreamSummary) is used instead of DescribeStream to list shards in Kinesis streams. Due to this change, as mentioned in [AWS documentation](https://docs.aws.amazon.com/kinesis/latest/APIReference/API_ListShards.html), for fine-grained IAM policies it is required to update them to allow calls to ListShards and DescribeStreamSummary APIs. For more information, see [Controlling Access to Amazon Kinesis Data Streams](https://docs.aws.amazon.com/streams/latest/dev/controlling-access.html) ([BEAM-12225](https://issues.apache.org/jira/browse/BEAM-12225)).\n\n## Deprecations\n\n* Python GBK will stop supporting unbounded PCollections that have global windowing and a default trigger in Beam 2.33. This can be overriden with `--allow_unsafe_triggers`. ([BEAM-9487](https://issues.apache.org/jira/browse/BEAM-9487)).\n* Python GBK will start requiring safe triggers or the `--allow_unsafe_triggers` flag starting with Beam 2.33. ([BEAM-9487](https://issues.apache.org/jira/browse/BEAM-9487)).\n\n## Bugfixes\n\n* Fixed race condition in RabbitMqIO causing duplicate acks (Java) ([BEAM-6516](https://issues.apache.org/jira/browse/BEAM-6516)))\n\n## Known Issues\n* On rare occations, Python GCS source may swallow some exceptions. Users are adviced to upgrade to Beam 2.38.0 or later ([BEAM-14282](https://issues.apache.org/jira/browse/BEAM-14282))\n\n# [2.31.0] - 2021-07-08\n\n## I/Os\n\n* Fixed bug in ReadFromBigQuery when a RuntimeValueProvider is used as value of table argument (Python) ([BEAM-12514](https://issues.apache.org/jira/browse/BEAM-12514)).\n\n## New Features / Improvements\n\n* `CREATE FUNCTION` DDL statement added to Calcite SQL syntax. `JAR` and `AGGREGATE` are now reserved keywords. ([BEAM-12339](https://issues.apache.org/jira/browse/BEAM-12339)).\n* Flink 1.13 is now supported by the Flink runner ([BEAM-12277](https://issues.apache.org/jira/browse/BEAM-12277)).\n* Python `TriggerFn` has a new `may_lose_data` method to signal potential data loss. Default behavior assumes safe (necessary for backwards compatibility). See Deprecations for potential impact of overriding this. ([BEAM-9487](https://issues.apache.org/jira/browse/BEAM-9487)).\n\n## Breaking Changes\n\n* Python Row objects are now sensitive to field order. So `Row(x=3, y=4)` is no\n  longer considered equal to `Row(y=4, x=3)` (BEAM-11929).\n* Kafka Beam SQL tables now ascribe meaning to the LOCATION field; previously\n  it was ignored if provided.\n* `TopCombineFn` disallow `compare` as its argument (Python) ([BEAM-7372](https://issues.apache.org/jira/browse/BEAM-7372)).\n* Drop support for Flink 1.10 ([BEAM-12281](https://issues.apache.org/jira/browse/BEAM-12281)).\n\n## Deprecations\n\n* Python GBK will stop supporting unbounded PCollections that have global windowing and a default trigger in Beam 2.33. This can be overriden with `--allow_unsafe_triggers`. ([BEAM-9487](https://issues.apache.org/jira/browse/BEAM-9487)).\n* Python GBK will start requiring safe triggers or the `--allow_unsafe_triggers` flag starting with Beam 2.33. ([BEAM-9487](https://issues.apache.org/jira/browse/BEAM-9487)).\n\n# [2.30.0] - 2021-06-09\n\n## I/Os\n\n* Allow splitting apart document serialization and IO for ElasticsearchIO\n* Support Bulk API request size optimization through addition of ElasticsearchIO.Write.withStatefulBatches\n\n## New Features / Improvements\n\n* Added capability to declare resource hints in Java and Python SDKs ([BEAM-2085](https://issues.apache.org/jira/browse/BEAM-2085)).\n* Added Spanner IO Performance tests for read and write. (Python) ([BEAM-10029](https://issues.apache.org/jira/browse/BEAM-10029)).\n* Added support for accessing GCP PubSub Message ordering keys, message IDs and message publish timestamp (Python) ([BEAM-7819](https://issues.apache.org/jira/browse/BEAM-7819)).\n* DataFrame API: Added support for collecting DataFrame objects in interactive Beam ([BEAM-11855](https://issues.apache.org/jira/browse/BEAM-11855))\n* DataFrame API: Added [apache_beam.examples.dataframe](https://github.com/apache/beam/tree/master/sdks/python/apache_beam/examples/dataframe) module ([BEAM-12024](https://issues.apache.org/jira/browse/BEAM-12024))\n* Upgraded the GCP Libraries BOM version to 20.0.0 ([BEAM-11205](https://issues.apache.org/jira/browse/BEAM-11205)).\n  For Google Cloud client library versions set by this BOM, see [this table](https://storage.googleapis.com/cloud-opensource-java-dashboard/com.google.cloud/libraries-bom/20.0.0/artifact_details.html).\n\n## Breaking Changes\n\n* Drop support for Flink 1.8 and 1.9 ([BEAM-11948](https://issues.apache.org/jira/browse/BEAM-11948)).\n* MongoDbIO: Read.withFilter() and Read.withProjection() are removed since they are deprecated since\n  Beam 2.12.0 ([BEAM-12217](https://issues.apache.org/jira/browse/BEAM-12217)).\n* RedisIO.readAll() was removed since it was deprecated since Beam 2.13.0. Please use\n  RedisIO.readKeyPatterns() for the equivalent functionality.\n  ([BEAM-12214](https://issues.apache.org/jira/browse/BEAM-12214)).\n* MqttIO.create() with clientId constructor removed because it was deprecated since Beam\n  2.13.0 ([BEAM-12216](https://issues.apache.org/jira/browse/BEAM-12216)).\n\n# [2.29.0] - 2021-04-29\n\n## Highlights\n\n* Spark Classic and Portable runners officially support Spark 3 ([BEAM-7093](https://issues.apache.org/jira/browse/BEAM-7093)).\n* Official Java 11 support for most runners (Dataflow, Flink, Spark) ([BEAM-2530](https://issues.apache.org/jira/browse/BEAM-2530)).\n* DataFrame API now supports GroupBy.apply ([BEAM-11628](https://issues.apache.org/jira/browse/BEAM-11628)).\n\n## I/Os\n\n* Added support for S3 filesystem on AWS SDK V2 (Java) ([BEAM-7637](https://issues.apache.org/jira/browse/BEAM-7637))\n\n## New Features / Improvements\n\n* DataFrame API now supports pandas 1.2.x ([BEAM-11531](https://issues.apache.org/jira/browse/BEAM-11531)).\n* Multiple DataFrame API bugfixes ([BEAM-12071](https://issues.apache.org/jira/browse/BEAM-12071), [BEAM-11929](https://issues.apache.org/jira/browse/BEAM-11929))\n\n## Breaking Changes\n\n* Deterministic coding enforced for GroupByKey and Stateful DoFns.  Previously non-deterministic coding was allowed, resulting in keys not properly being grouped in some cases. ([BEAM-11719](https://issues.apache.org/jira/browse/BEAM-11719))\n  To restore the old behavior, one can register `FakeDeterministicFastPrimitivesCoder` with\n  `beam.coders.registry.register_fallback_coder(beam.coders.coders.FakeDeterministicFastPrimitivesCoder())`\n  or use the `allow_non_deterministic_key_coders` pipeline option.\n\n## Deprecations\n\n* Support for Flink 1.8 and 1.9 will be removed in the next release (2.30.0) ([BEAM-11948](https://issues.apache.org/jira/browse/BEAM-11948)).\n\n# [2.28.0] - 2021-02-22\n\n## Highlights\n* Many improvements related to Parquet support ([BEAM-11460](https://issues.apache.org/jira/browse/BEAM-11460), [BEAM-8202](https://issues.apache.org/jira/browse/BEAM-8202), and [BEAM-11526](https://issues.apache.org/jira/browse/BEAM-11526))\n* Hash Functions in BeamSQL ([BEAM-10074](https://issues.apache.org/jira/browse/BEAM-10074))\n* Hash functions in ZetaSQL ([BEAM-11624](https://issues.apache.org/jira/browse/BEAM-11624))\n* Create ApproximateDistinct using HLL Impl ([BEAM-10324](https://issues.apache.org/jira/browse/BEAM-10324))\n\n## I/Os\n\n* SpannerIO supports using BigDecimal for Numeric fields ([BEAM-11643](https://issues.apache.org/jira/browse/BEAM-11643))\n* Add Beam schema support to ParquetIO ([BEAM-11526](https://issues.apache.org/jira/browse/BEAM-11526))\n* Support ParquetTable Writer ([BEAM-8202](https://issues.apache.org/jira/browse/BEAM-8202))\n* GCP BigQuery sink (streaming inserts) uses runner determined sharding ([BEAM-11408](https://issues.apache.org/jira/browse/BEAM-11408))\n* PubSub support types: TIMESTAMP, DATE, TIME, DATETIME ([BEAM-11533](https://issues.apache.org/jira/browse/BEAM-11533))\n\n## New Features / Improvements\n\n* ParquetIO add methods _readGenericRecords_ and _readFilesGenericRecords_ can read files with an unknown schema. See [PR-13554](https://github.com/apache/beam/pull/13554) and ([BEAM-11460](https://issues.apache.org/jira/browse/BEAM-11460))\n* Added support for thrift in KafkaTableProvider ([BEAM-11482](https://issues.apache.org/jira/browse/BEAM-11482))\n* Added support for HadoopFormatIO to skip key/value clone ([BEAM-11457](https://issues.apache.org/jira/browse/BEAM-11457))\n* Support Conversion to GenericRecords in Convert.to transform ([BEAM-11571](https://issues.apache.org/jira/browse/BEAM-11571)).\n* Support writes for Parquet Tables in Beam SQL ([BEAM-8202](https://issues.apache.org/jira/browse/BEAM-8202)).\n* Support reading Parquet files with unknown schema ([BEAM-11460](https://issues.apache.org/jira/browse/BEAM-11460))\n* Support user configurable Hadoop Configuration flags for ParquetIO ([BEAM-11527](https://issues.apache.org/jira/browse/BEAM-11527))\n* Expose commit_offset_in_finalize and timestamp_policy to ReadFromKafka ([BEAM-11677](https://issues.apache.org/jira/browse/BEAM-11677))\n* S3 options does not provided to boto3 client while using FlinkRunner and Beam worker pool container ([BEAM-11799](https://issues.apache.org/jira/browse/BEAM-11799))\n* HDFS not deduplicating identical configuration paths ([BEAM-11329](https://issues.apache.org/jira/browse/BEAM-11329))\n* Hash Functions in BeamSQL ([BEAM-10074](https://issues.apache.org/jira/browse/BEAM-10074))\n* Create ApproximateDistinct using HLL Impl ([BEAM-10324](https://issues.apache.org/jira/browse/BEAM-10324))\n* Add Beam schema support to ParquetIO ([BEAM-11526](https://issues.apache.org/jira/browse/BEAM-11526))\n* Add a Deque Encoder ([BEAM-11538](https://issues.apache.org/jira/browse/BEAM-11538))\n* Hash functions in ZetaSQL ([BEAM-11624](https://issues.apache.org/jira/browse/BEAM-11624))\n* Refactor ParquetTableProvider ([](https://issues.apache.org/jira/browse/))\n* Add JVM properties to JavaJobServer ([BEAM-8344](https://issues.apache.org/jira/browse/BEAM-8344))\n* Single source of truth for supported Flink versions ([](https://issues.apache.org/jira/browse/))\n* Use metric for Python BigQuery streaming insert API latency logging ([BEAM-11018](https://issues.apache.org/jira/browse/BEAM-11018))\n* Use metric for Java BigQuery streaming insert API latency logging ([BEAM-11032](https://issues.apache.org/jira/browse/BEAM-11032))\n* Upgrade Flink runner to Flink versions 1.12.1 and 1.11.3 ([BEAM-11697](https://issues.apache.org/jira/browse/BEAM-11697))\n* Upgrade Beam base image to use Tensorflow 2.4.1 ([BEAM-11762](https://issues.apache.org/jira/browse/BEAM-11762))\n* Create Beam GCP BOM ([BEAM-11665](https://issues.apache.org/jira/browse/BEAM-11665))\n\n## Breaking Changes\n\n* The Java artifacts \"beam-sdks-java-io-kinesis\", \"beam-sdks-java-io-google-cloud-platform\", and\n  \"beam-sdks-java-extensions-sql-zetasql\" declare Guava 30.1-jre dependency (It was 25.1-jre in Beam 2.27.0).\n  This new Guava version may introduce dependency conflicts if your project or dependencies rely\n  on removed APIs. If affected, ensure to use an appropriate Guava version via `dependencyManagement` in Maven and\n  `force` in Gradle.\n\n\n# [2.27.0] - 2021-01-08\n\n## I/Os\n* ReadFromMongoDB can now be used with MongoDB Atlas (Python) ([BEAM-11266](https://issues.apache.org/jira/browse/BEAM-11266).)\n* ReadFromMongoDB/WriteToMongoDB will mask password in display_data (Python) ([BEAM-11444](https://issues.apache.org/jira/browse/BEAM-11444).)\n* Support for X source added (Java/Python) ([BEAM-X](https://issues.apache.org/jira/browse/BEAM-X)).\n* There is a new transform `ReadAllFromBigQuery` that can receive multiple requests to read data from BigQuery at pipeline runtime. See [PR 13170](https://github.com/apache/beam/pull/13170), and [BEAM-9650](https://issues.apache.org/jira/browse/BEAM-9650).\n\n## New Features / Improvements\n\n* Beam modules that depend on Hadoop are now tested for compatibility with Hadoop 3 ([BEAM-8569](https://issues.apache.org/jira/browse/BEAM-8569)). (Hive/HCatalog pending)\n* Publishing Java 11 SDK container images now supported as part of Apache Beam release process. ([BEAM-8106](https://issues.apache.org/jira/browse/BEAM-8106))\n* Added Cloud Bigtable Provider extension to Beam SQL ([BEAM-11173](https://issues.apache.org/jira/browse/BEAM-11173), [BEAM-11373](https://issues.apache.org/jira/browse/BEAM-11373))\n* Added a schema provider for thrift data ([BEAM-11338](https://issues.apache.org/jira/browse/BEAM-11338))\n* Added combiner packing pipeline optimization to Dataflow runner. ([BEAM-10641](https://issues.apache.org/jira/browse/BEAM-10641))\n* Support for the Deque structure by adding a coder ([BEAM-11538](https://issues.apache.org/jira/browse/BEAM-11538))\n\n## Breaking Changes\n\n* HBaseIO hbase-shaded-client dependency should be now provided by the users ([BEAM-9278](https://issues.apache.org/jira/browse/BEAM-9278)).\n* `--region` flag in amazon-web-services2 was replaced by `--awsRegion` ([BEAM-11331](https://issues.apache.org/jira/projects/BEAM/issues/BEAM-11331)).\n\n# [2.26.0] - 2020-12-11\n\n## Highlights\n\n* Splittable DoFn is now the default for executing the Read transform for Java based runners (Spark with bounded pipelines) in addition to existing runners from the 2.25.0 release (Direct, Flink, Jet, Samza, Twister2). The expected output of the Read transform is unchanged. Users can opt-out using `--experiments=use_deprecated_read`. The Apache Beam community is looking for feedback for this change as the community is planning to make this change permanent with no opt-out. If you run into an issue requiring the opt-out, please send an e-mail to [user@beam.apache.org](mailto:user@beam.apache.org) specifically referencing BEAM-10670 in the subject line and why you needed to opt-out. (Java) ([BEAM-10670](https://issues.apache.org/jira/browse/BEAM-10670))\n\n## I/Os\n\n* Java BigQuery streaming inserts now have timeouts enabled by default. Pass `--HTTPWriteTimeout=0` to revert to the old behavior. ([BEAM-6103](https://issues.apache.org/jira/browse/BEAM-6103))\n* Added support for Contextual Text IO (Java), a version of text IO that provides metadata about the records ([BEAM-10124](https://issues.apache.org/jira/browse/BEAM-10124)). Support for this IO is currently experimental. Specifically, **there are no update-compatibility guarantees** for streaming jobs with this IO between current future verisons of Apache Beam SDK.\n\n## New Features / Improvements\n* Added support for avro payload format in Beam SQL Kafka Table ([BEAM-10885](https://issues.apache.org/jira/browse/BEAM-10885))\n* Added support for json payload format in Beam SQL Kafka Table ([BEAM-10893](https://issues.apache.org/jira/browse/BEAM-10893))\n* Added support for protobuf payload format in Beam SQL Kafka Table ([BEAM-10892](https://issues.apache.org/jira/browse/BEAM-10892))\n* Added support for avro payload format in Beam SQL Pubsub Table ([BEAM-5504](https://issues.apache.org/jira/browse/BEAM-5504))\n* Added option to disable unnecessary copying between operators in Flink Runner (Java) ([BEAM-11146](https://issues.apache.org/jira/browse/BEAM-11146))\n* Added CombineFn.setup and CombineFn.teardown to Python SDK. These methods let you initialize the CombineFn's state before any of the other methods of the CombineFn is executed and clean that state up later on. If you are using Dataflow, you need to enable Dataflow Runner V2 by passing `--experiments=use_runner_v2` before using this feature. ([BEAM-3736](https://issues.apache.org/jira/browse/BEAM-3736))\n* Added support for NestedValueProvider for the Python SDK ([BEAM-10856](https://issues.apache.org/jira/browse/BEAM-10856)).\n\n## Breaking Changes\n\n* BigQuery's DATETIME type now maps to Beam logical type org.apache.beam.sdk.schemas.logicaltypes.SqlTypes.DATETIME\n* Pandas 1.x is now required for dataframe operations.\n\n## Known Issues\n\n* Non-idempotent combiners built via `CombineFn.from_callable()` or `CombineFn.maybe_from_callable()` can lead to incorrect behavior. ([BEAM-11522](https://issues.apache.org/jira/browse/BEAM-11522)).\n\n\n# [2.25.0] - 2020-10-23\n\n## Highlights\n\n* Splittable DoFn is now the default for executing the Read transform for Java based runners (Direct, Flink, Jet, Samza, Twister2). The expected output of the Read transform is unchanged. Users can opt-out using `--experiments=use_deprecated_read`. The Apache Beam community is looking for feedback for this change as the community is planning to make this change permanent with no opt-out. If you run into an issue requiring the opt-out, please send an e-mail to [user@beam.apache.org](mailto:user@beam.apache.org) specifically referencing BEAM-10670 in the subject line and why you needed to opt-out. (Java) ([BEAM-10670](https://issues.apache.org/jira/browse/BEAM-10670))\n\n## I/Os\n\n* Added cross-language support to Java's KinesisIO, now available in the Python module `apache_beam.io.kinesis` ([BEAM-10138](https://issues.apache.org/jira/browse/BEAM-10138), [BEAM-10137](https://issues.apache.org/jira/browse/BEAM-10137)).\n* Update Snowflake JDBC dependency for SnowflakeIO ([BEAM-10864](https://issues.apache.org/jira/browse/BEAM-10864))\n* Added cross-language support to Java's SnowflakeIO.Write, now available in the Python module `apache_beam.io.snowflake` ([BEAM-9898](https://issues.apache.org/jira/browse/BEAM-9898)).\n* Added delete function to Java's `ElasticsearchIO#Write`. Now, Java's ElasticsearchIO can be used to selectively delete documents using `withIsDeleteFn` function ([BEAM-5757](https://issues.apache.org/jira/browse/BEAM-5757)).\n* Java SDK: Added new IO connector for InfluxDB - InfluxDbIO ([BEAM-2546](https://issues.apache.org/jira/browse/BEAM-2546)).\n* Config options added for Python's S3IO ([BEAM-9094](https://issues.apache.org/jira/browse/BEAM-9094))\n\n## New Features / Improvements\n\n* Support for repeatable fields in JSON decoder for `ReadFromBigQuery` added. (Python) ([BEAM-10524](https://issues.apache.org/jira/browse/BEAM-10524))\n* Added an opt-in, performance-driven runtime type checking system for the Python SDK ([BEAM-10549](https://issues.apache.org/jira/browse/BEAM-10549)).\n    More details will be in an upcoming [blog post](https://beam.apache.org/blog/python-performance-runtime-type-checking/index.html).\n* Added support for Python 3 type annotations on PTransforms using typed PCollections ([BEAM-10258](https://issues.apache.org/jira/browse/BEAM-10258)).\n    More details will be in an upcoming [blog post](https://beam.apache.org/blog/python-improved-annotations/index.html).\n* Improved the Interactive Beam API where recording streaming jobs now start a long running background recording job. Running ib.show() or ib.collect() samples from the recording ([BEAM-10603](https://issues.apache.org/jira/browse/BEAM-10603)).\n* In Interactive Beam, ib.show() and ib.collect() now have \"n\" and \"duration\" as parameters. These mean read only up to \"n\" elements and up to \"duration\" seconds of data read from the recording ([BEAM-10603](https://issues.apache.org/jira/browse/BEAM-10603)).\n* Initial preview of [Dataframes](https://s.apache.org/simpler-python-pipelines-2020#slide=id.g905ac9257b_1_21) support.\n    See also example at apache_beam/examples/wordcount_dataframe.py\n* Fixed support for type hints on `@ptransform_fn` decorators in the Python SDK.\n  ([BEAM-4091](https://issues.apache.org/jira/browse/BEAM-4091))\n  This has not enabled by default to preserve backwards compatibility; use the\n  `--type_check_additional=ptransform_fn` flag to enable. It may be enabled by\n  default in future versions of Beam.\n\n## Breaking Changes\n\n* Python 2 and Python 3.5 support dropped ([BEAM-10644](https://issues.apache.org/jira/browse/BEAM-10644), [BEAM-9372](https://issues.apache.org/jira/browse/BEAM-9372)).\n* Pandas 1.x allowed.  Older version of Pandas may still be used, but may not be as well tested.\n\n## Deprecations\n\n* Python transform ReadFromSnowflake has been moved from `apache_beam.io.external.snowflake` to `apache_beam.io.snowflake`. The previous path will be removed in the future versions.\n\n## Known Issues\n\n* Dataflow streaming timers once against not strictly time ordered when set earlier mid-bundle, as the fix for  [BEAM-8543](https://issues.apache.org/jira/browse/BEAM-8543) introduced more severe bugs and has been rolled back.\n* Default compressor change breaks dataflow python streaming job update compatibility. Please use python SDK version <= 2.23.0 or > 2.25.0 if job update is critical.([BEAM-11113](https://issues.apache.org/jira/browse/BEAM-11113))\n\n\n# [2.24.0] - 2020-09-18\n\n## Highlights\n\n* Apache Beam 2.24.0 is the last release with Python 2 and Python 3.5\n  support.\n\n## I/Os\n\n* New overloads for BigtableIO.Read.withKeyRange() and BigtableIO.Read.withRowFilter()\n  methods that take ValueProvider as a parameter (Java) ([BEAM-10283](https://issues.apache.org/jira/browse/BEAM-10283)).\n* The WriteToBigQuery transform (Python) in Dataflow Batch no longer relies on BigQuerySink by default. It relies on\n  a new, fully-featured transform based on file loads into BigQuery. To revert the behavior to the old implementation,\n  you may use `--experiments=use_legacy_bq_sink`.\n* Add cross-language support to Java's JdbcIO, now available in the Python module `apache_beam.io.jdbc` ([BEAM-10135](https://issues.apache.org/jira/browse/BEAM-10135), [BEAM-10136](https://issues.apache.org/jira/browse/BEAM-10136)).\n* Add support of AWS SDK v2 for KinesisIO.Read (Java) ([BEAM-9702](https://issues.apache.org/jira/browse/BEAM-9702)).\n* Add streaming support to SnowflakeIO in Java SDK ([BEAM-9896](https://issues.apache.org/jira/browse/BEAM-9896))\n* Support reading and writing to Google Healthcare DICOM APIs in Python SDK ([BEAM-10601](https://issues.apache.org/jira/browse/BEAM-10601))\n* Add dispositions for SnowflakeIO.write ([BEAM-10343](https://issues.apache.org/jira/browse/BEAM-10343))\n* Add cross-language support to SnowflakeIO.Read now available in the Python module `apache_beam.io.external.snowflake` ([BEAM-9897](https://issues.apache.org/jira/browse/BEAM-9897)).\n\n## New Features / Improvements\n\n* Shared library for simplifying management of large shared objects added to Python SDK. An example use case is sharing a large TF model object across threads ([BEAM-10417](https://issues.apache.org/jira/browse/BEAM-10417)).\n* Dataflow streaming timers are not strictly time ordered when set earlier mid-bundle ([BEAM-8543](https://issues.apache.org/jira/browse/BEAM-8543)).\n* OnTimerContext should not create a new one when processing each element/timer in FnApiDoFnRunner ([BEAM-9839](https://issues.apache.org/jira/browse/BEAM-9839))\n* Key should be available in @OnTimer methods (Spark Runner) ([BEAM-9850](https://issues.apache.org/jira/browse/BEAM-9850))\n\n## Breaking Changes\n\n* WriteToBigQuery transforms now require a GCS location to be provided through either\n  custom_gcs_temp_location in the constructor of WriteToBigQuery or the fallback option\n  --temp_location, or pass method=\"STREAMING_INSERTS\" to WriteToBigQuery ([BEAM-6928](https://issues.apache.org/jira/browse/BEAM-6928)).\n* Python SDK now understands `typing.FrozenSet` type hints, which are not interchangeable with `typing.Set`. You may need to update your pipelines if type checking fails. ([BEAM-10197](https://issues.apache.org/jira/browse/BEAM-10197))\n\n## Known issues\n\n* When a timer fires but is reset prior to being executed, a watermark hold may be leaked, causing a stuck pipeline [BEAM-10991](https://issues.apache.org/jira/browse/BEAM-10991).\n* Default compressor change breaks dataflow python streaming job update compatibility. Please use python SDK version <= 2.23.0 or > 2.25.0 if job update is critical.([BEAM-11113](https://issues.apache.org/jira/browse/BEAM-11113))\n\n# [2.23.0] - 2020-06-29\n\n## Highlights\n\n* Twister2 Runner ([BEAM-7304](https://issues.apache.org/jira/browse/BEAM-7304)).\n* Python 3.8 support ([BEAM-8494](https://issues.apache.org/jira/browse/BEAM-8494)).\n\n## I/Os\n\n* Support for reading from Snowflake added (Java) ([BEAM-9722](https://issues.apache.org/jira/browse/BEAM-9722)).\n* Support for writing to Splunk added (Java) ([BEAM-8596](https://issues.apache.org/jira/browse/BEAM-8596)).\n* Support for assume role added (Java) ([BEAM-10335](https://issues.apache.org/jira/browse/BEAM-10335)).\n* A new transform to read from BigQuery has been added: `apache_beam.io.gcp.bigquery.ReadFromBigQuery`. This transform\n  is experimental. It reads data from BigQuery by exporting data to Avro files, and reading those files. It also supports\n  reading data by exporting to JSON files. This has small differences in behavior for Time and Date-related fields. See\n  Pydoc for more information.\n\n## New Features / Improvements\n\n* Update Snowflake JDBC dependency and add application=beam to connection URL ([BEAM-10383](https://issues.apache.org/jira/browse/BEAM-10383)).\n\n## Breaking Changes\n\n* `RowJson.RowJsonDeserializer`, `JsonToRow`, and `PubsubJsonTableProvider` now accept \"implicit\n  nulls\" by default when deserializing JSON (Java) ([BEAM-10220](https://issues.apache.org/jira/browse/BEAM-10220)).\n  Previously nulls could only be represented with explicit null values, as in\n  `{\"foo\": \"bar\", \"baz\": null}`, whereas an implicit null like `{\"foo\": \"bar\"}` would raise an\n  exception. Now both JSON strings will yield the same result by default. This behavior can be\n  overridden with `RowJson.RowJsonDeserializer#withNullBehavior`.\n* Fixed a bug in `GroupIntoBatches` experimental transform in Python to actually group batches by key.\n  This changes the output type for this transform ([BEAM-6696](https://issues.apache.org/jira/browse/BEAM-6696)).\n\n## Deprecations\n\n* Remove Gearpump runner. ([BEAM-9999](https://issues.apache.org/jira/browse/BEAM-9999))\n* Remove Apex runner. ([BEAM-9999](https://issues.apache.org/jira/browse/BEAM-9999))\n* RedisIO.readAll() is deprecated and will be removed in 2 versions, users must use RedisIO.readKeyPatterns() as a replacement ([BEAM-9747](https://issues.apache.org/jira/browse/BEAM-9747)).\n\n## Known Issues\n\n* Fixed X (Java/Python) ([BEAM-X](https://issues.apache.org/jira/browse/BEAM-X)).\n\n# [2.22.0] - 2020-06-08\n\n## Highlights\n\n## I/Os\n\n* Basic Kafka read/write support for DataflowRunner (Python) ([BEAM-8019](https://issues.apache.org/jira/browse/BEAM-8019)).\n* Sources and sinks for Google Healthcare APIs (Java)([BEAM-9468](https://issues.apache.org/jira/browse/BEAM-9468)).\n* Support for writing to Snowflake added (Java) ([BEAM-9894](https://issues.apache.org/jira/browse/BEAM-9894)).\n\n## New Features / Improvements\n\n* `--workerCacheMB` flag is supported in Dataflow streaming pipeline ([BEAM-9964](https://issues.apache.org/jira/browse/BEAM-9964))\n* `--direct_num_workers=0` is supported for FnApi runner. It will set the number of threads/subprocesses to number of cores of the machine executing the pipeline ([BEAM-9443](https://issues.apache.org/jira/browse/BEAM-9443)).\n* Python SDK now has experimental support for SqlTransform ([BEAM-8603](https://issues.apache.org/jira/browse/BEAM-8603)).\n* Add OnWindowExpiration method to Stateful DoFn ([BEAM-1589](https://issues.apache.org/jira/browse/BEAM-1589)).\n* Added PTransforms for Google Cloud DLP (Data Loss Prevention) services integration ([BEAM-9723](https://issues.apache.org/jira/browse/BEAM-9723)):\n    * Inspection of data,\n    * Deidentification of data,\n    * Reidentification of data.\n* Add a more complete I/O support matrix in the documentation site ([BEAM-9916](https://issues.apache.org/jira/browse/BEAM-9916)).\n* Upgrade Sphinx to 3.0.3 for building PyDoc.\n* Added a PTransform for image annotation using Google Cloud AI image processing service\n([BEAM-9646](https://issues.apache.org/jira/browse/BEAM-9646))\n* Dataflow streaming timers are not strictly time ordered when set earlier mid-bundle ([BEAM-8543](https://issues.apache.org/jira/browse/BEAM-8543)).\n\n## Breaking Changes\n\n* The Python SDK now requires `--job_endpoint` to be set when using `--runner=PortableRunner` ([BEAM-9860](https://issues.apache.org/jira/browse/BEAM-9860)). Users seeking the old default behavior should set `--runner=FlinkRunner` instead.\n\n## Deprecations\n\n## Known Issues\n\n\n# [2.21.0] - 2020-05-27\n\n## Highlights\n\n## I/Os\n* Python: Deprecated module `apache_beam.io.gcp.datastore.v1` has been removed\nas the client it uses is out of date and does not support Python 3\n([BEAM-9529](https://issues.apache.org/jira/browse/BEAM-9529)).\nPlease migrate your code to use\n[apache_beam.io.gcp.datastore.**v1new**](https://beam.apache.org/releases/pydoc/current/apache_beam.io.gcp.datastore.v1new.datastoreio.html).\nSee the updated\n[datastore_wordcount](https://github.com/apache/beam/blob/master/sdks/python/apache_beam/examples/cookbook/datastore_wordcount.py)\nfor example usage.\n* Python SDK: Added integration tests and updated batch write functionality for Google Cloud Spanner transform ([BEAM-8949](https://issues.apache.org/jira/browse/BEAM-8949)).\n\n## New Features / Improvements\n* Python SDK will now use Python 3 type annotations as pipeline type hints.\n([#10717](https://github.com/apache/beam/pull/10717))\n\n    If you suspect that this feature is causing your pipeline to fail, calling\n    `apache_beam.typehints.disable_type_annotations()` before pipeline creation\n    will disable is completely, and decorating specific functions (such as\n    `process()`) with `@apache_beam.typehints.no_annotations` will disable it\n    for that function.\n\n    More details will be in\n    [Ensuring Python Type Safety](https://beam.apache.org/documentation/sdks/python-type-safety/)\n    and an upcoming\n    [blog post](https://beam.apache.org/blog/python-typing/index.html).\n\n* Java SDK: Introducing the concept of options in Beam Schemas. These options add extra\ncontext to fields and schemas. This replaces the current Beam metadata that is present\nin a FieldType only, options are available in fields and row schemas. Schema options are\nfully typed and can contain complex rows. *Remark: Schema aware is still experimental.*\n([BEAM-9035](https://issues.apache.org/jira/browse/BEAM-9035))\n* Java SDK: The protobuf extension is fully schema aware and also includes protobuf option\nconversion to beam schema options. *Remark: Schema aware is still experimental.*\n([BEAM-9044](https://issues.apache.org/jira/browse/BEAM-9044))\n* Added ability to write to BigQuery via Avro file loads (Python) ([BEAM-8841](https://issues.apache.org/jira/browse/BEAM-8841))\n\n    By default, file loads will be done using JSON, but it is possible to\n    specify the temp_file_format parameter to perform file exports with AVRO.\n    AVRO-based file loads work by exporting Python types into Avro types, so\n    to switch to Avro-based loads, you will need to change your data types\n    from Json-compatible types (string-type dates and timestamp, long numeric\n    values as strings) into Python native types that are written to Avro\n    (Python's date, datetime types, decimal, etc). For more information\n    see https://cloud.google.com/bigquery/docs/loading-data-cloud-storage-avro#avro_conversions.\n* Added integration of Java SDK with Google Cloud AI VideoIntelligence service\n([BEAM-9147](https://issues.apache.org/jira/browse/BEAM-9147))\n* Added integration of Java SDK with Google Cloud AI natural language processing API\n([BEAM-9634](https://issues.apache.org/jira/browse/BEAM-9634))\n* `docker-pull-licenses` tag was introduced. Licenses/notices of third party dependencies will be added to the docker images when `docker-pull-licenses` was set.\n  The files are added to `/opt/apache/beam/third_party_licenses/`.\n  By default, no licenses/notices are added to the docker images. ([BEAM-9136](https://issues.apache.org/jira/browse/BEAM-9136))\n\n\n## Breaking Changes\n\n* Dataflow runner now requires the `--region` option to be set, unless a default value is set in the environment ([BEAM-9199](https://issues.apache.org/jira/browse/BEAM-9199)). See [here](https://cloud.google.com/dataflow/docs/concepts/regional-endpoints) for more details.\n* HBaseIO.ReadAll now requires a PCollection of HBaseIO.Read objects instead of HBaseQuery objects ([BEAM-9279](https://issues.apache.org/jira/browse/BEAM-9279)).\n* ProcessContext.updateWatermark has been removed in favor of using a WatermarkEstimator ([BEAM-9430](https://issues.apache.org/jira/browse/BEAM-9430)).\n* Coder inference for PCollection of Row objects has been disabled ([BEAM-9569](https://issues.apache.org/jira/browse/BEAM-9569)).\n* Go SDK docker images are no longer released until further notice.\n\n## Deprecations\n* Java SDK: Beam Schema FieldType.getMetadata is now deprecated and is replaced by the Beam\nSchema Options, it will be removed in version `2.23.0`. ([BEAM-9704](https://issues.apache.org/jira/browse/BEAM-9704))\n* The `--zone` option in the Dataflow runner is now deprecated. Please use `--worker_zone` instead. ([BEAM-9716](https://issues.apache.org/jira/browse/BEAM-9716))\n\n## Known Issues\n\n\n# [2.20.0] - 2020-04-15\n\n## Highlights\n\n\n## I/Os\n\n* Java SDK: Adds support for Thrift encoded data via ThriftIO. ([BEAM-8561](https://issues.apache.org/jira/browse/BEAM-8561))\n* Java SDK: KafkaIO supports schema resolution using Confluent Schema Registry. ([BEAM-7310](https://issues.apache.org/jira/browse/BEAM-7310))\n* Java SDK: Add Google Cloud Healthcare IO connectors: HL7v2IO and FhirIO ([BEAM-9468](https://issues.apache.org/jira/browse/BEAM-9468))\n* Python SDK: Support for Google Cloud Spanner. This is an experimental module for reading and writing data from Google Cloud Spanner ([BEAM-7246](https://issues.apache.org/jira/browse/BEAM-7246)).\n* Python SDK: Adds support for standard HDFS URLs (with server name). ([#10223](https://github.com/apache/beam/pull/10223)).\n\n\n## New Features / Improvements\n\n* New AnnotateVideo & AnnotateVideoWithContext PTransform's that integrates GCP Video Intelligence functionality. (Python) ([BEAM-9146](https://issues.apache.org/jira/browse/BEAM-9146))\n* New AnnotateImage & AnnotateImageWithContext PTransform's for element-wise & batch image annotation using Google Cloud Vision API. (Python) ([BEAM-9247](https://issues.apache.org/jira/browse/BEAM-9247))\n* Added a PTransform for inspection and deidentification of text using Google Cloud DLP. (Python) ([BEAM-9258](https://issues.apache.org/jira/browse/BEAM-9258))\n* New AnnotateText PTransform that integrates Google Cloud Natural Language functionality (Python) ([BEAM-9248](https://issues.apache.org/jira/browse/BEAM-9248))\n* _ReadFromBigQuery_ now supports value providers for the query string (Python) ([BEAM-9305](https://issues.apache.org/jira/browse/BEAM-9305))\n* Direct runner for FnApi supports further parallelism (Python) ([BEAM-9228](https://issues.apache.org/jira/browse/BEAM-9228))\n* Support for _@RequiresTimeSortedInput_ in Flink and Spark (Java) ([BEAM-8550](https://issues.apache.org/jira/browse/BEAM-8550))\n\n## Breaking Changes\n\n* ReadFromPubSub(topic=<topic>) in Python previously created a subscription under the same project as the topic. Now it will create the subscription under the project specified in pipeline_options. If the project is not specified in pipeline_options, then it will create the subscription under the same project as the topic. ([BEAM-3453](https://issues.apache.org/jira/browse/BEAM-3453)).\n* SpannerAccessor in Java is now package-private to reduce API surface. `SpannerConfig.connectToSpanner` has been moved to `SpannerAccessor.create`. ([BEAM-9310](https://issues.apache.org/jira/browse/BEAM-9310)).\n* ParquetIO hadoop dependency should be now provided by the users ([BEAM-8616](https://issues.apache.org/jira/browse/BEAM-8616)).\n* Docker images will be deployed to\n  [apache/beam](https://hub.docker.com/search?q=apache%2Fbeam&type=image) repositories from 2.20. They\n used to be deployed to\n [apachebeam](https://hub.docker.com/search?q=apachebeam&type=image) repository.\n ([BEAM-9063](https://issues.apache.org/jira/browse/BEAM-9093))\n* PCollections now have tags inferred from the result type (e.g. the keys of a dict or index of a tuple).  Users may expect the old implementation which gave PCollection output ids a monotonically increasing id. To go back to the old implementation, use the `force_generated_pcollection_output_ids` experiment.\n\n## Deprecations\n\n## Bugfixes\n\n* Fixed numpy operators in ApproximateQuantiles (Python) ([BEAM-9579](https://issues.apache.org/jira/browse/BEAM-9579)).\n* Fixed exception when running in IPython notebook (Python) ([BEAM-X9277](https://issues.apache.org/jira/browse/BEAM-9277)).\n* Fixed Flink uberjar job termination bug. ([BEAM-9225](https://issues.apache.org/jira/browse/BEAM-9225))\n* Fixed SyntaxError in process worker startup ([BEAM-9503](https://issues.apache.org/jira/browse/BEAM-9503))\n* Key should be available in @OnTimer methods (Java) ([BEAM-1819](https://issues.apache.org/jira/browse/BEAM-1819)).\n\n## Known Issues\n\n* ([BEAM-X](https://issues.apache.org/jira/browse/BEAM-X)).\n* ([BEAM-9322](https://issues.apache.org/jira/browse/BEAM-9322)).\n* Python SDK `pre_optimize=all` experiment may cause error ([BEAM-9445](https://issues.apache.org/jira/browse/BEAM-9445))\n\n# [2.19.0] - 2020-01-31\n\n- For versions 2.19.0 and older release notes are available on [Apache Beam Blog](https://beam.apache.org/blog/).\n"
        },
        {
          "name": "CI.md",
          "type": "blob",
          "size": 20.4658203125,
          "content": "<!--\n    Licensed to the Apache Software Foundation (ASF) under one\n    or more contributor license agreements.  See the NOTICE file\n    distributed with this work for additional information\n    regarding copyright ownership.  The ASF licenses this file\n    to you under the Apache License, Version 2.0 (the\n    \"License\"); you may not use this file except in compliance\n    with the License.  You may obtain a copy of the License at\n\n      http://www.apache.org/licenses/LICENSE-2.0\n\n    Unless required by applicable law or agreed to in writing,\n    software distributed under the License is distributed on an\n    \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n    KIND, either express or implied.  See the License for the\n    specific language governing permissions and limitations\n    under the License.\n-->\n\n# Apache Beam\n\n## CI Environment\n\nContinuous Integration is important component of making Apache Beam robust and stable.\n\nOur execution environment for CI is the [GitHub Actions](https://github.com/features/actions).\nSee [.github/workflow/README](.github/workflow/README.md) for trigger phrase,\nstatus and link of all GHA jobs.\n\nGitHub Actions (GHA) are very well integrated with GitHub code and Workflow and\nit has evolved fast in 2019/2020 to become a fully-fledged CI environment, easy\nto use and develop for, so we decided to use it firstly for a few workflows then\nmigrated all workflows previously run on Jenkins.\n\nFor this reason there are mainly two types of GHA workflows running\n- Self-hosted runner GHAs. These were mifrated from Jenkins with workflow name\n  prefix (beam_*.yml) as well as new workflows added following the same naming\n  convention, including PreCommit, PostCommit, LoadTest, PerformanceTest, and\n  several infrastructure jobs. See [.github/workflow/README](.github/workflow/README.md)\n  for the complete list.\n- GitHub-hosted runner GHAs. Most of these jobs exercises the workflow in different\n  OS (linux, macOS, Windows). They were added prior to Jenkins migration.\n  Some Linux jobs later migrated to use self-hosted runner.\n\n## GitHub Actions\n\nThis section applies to GitHub-hosted runner GHAs. New workflows unless intended\nto run on a matrix of OS should refer to Self-hosted runner GHAs [.github/workflow/README](.github/workflow/README.md).\n\n### GitHub actions run types\n\nThe following GA CI Job runs are currently run for Apache Beam, and each of the runs have different\npurpose and context.\n\n#### Pull request run\n\nThose runs are results of PR from the forks made by contributors. Most builds for Apache Beam fall\ninto this category. They are executed in the context of the \"Fork\", not main\nBeam Code Repository which means that they have only \"read\" permission to all the GitHub resources\n(container registry, code repository). This is necessary as the code in those PRs (including CI job\ndefinition) might be modified by people who are not committers for the Apache Beam Code Repository.\n\nThe main purpose of those jobs is to check if PR builds cleanly, if the test run properly and if\nthe PR is ready to review and merge.\n\n#### Direct Push/Merge Run\n\nThose runs are results of direct pushes done by the committers or as result of merge of a Pull Request\nby the committers. Those runs execute in the context of the Apache Beam Code Repository and have also\nwrite permission for GitHub resources (container registry, code repository).\nThe main purpose for the run is to check if the code after merge still holds all the assertions - like\nwhether it still builds, all tests are green.\n\nThis is needed because some of the conflicting changes from multiple PRs might cause build and test failures\nafter merge even if they do not fail in isolation.\n\n#### Scheduled runs\n\nThose runs are results of (nightly) triggered job - only for `master` branch. The\nmain purpose of the job is to check if there was no impact of external dependency changes on the Apache\nBeam code (for example transitive dependencies released that fail the build). Another reason for the nightly\nbuild is that the builds tags most recent master with `nightly-master`.\n\nAll runs consist of the same jobs, but the jobs behave slightly differently or they are skipped in different\nrun categories. Here is a summary of the run categories with regards of the jobs they are running.\nThose jobs often have matrix run strategy which runs several different variations of the jobs\n(with different platform type / Python version to run for example)\n\n### Google Cloud Platform Credentials\n\nSome of the jobs require variables stored as [GitHub Secrets](https://docs.github.com/en/actions/configuring-and-managing-workflows/creating-and-storing-encrypted-secrets)\nto perform operations on Google Cloud Platform.\nThese variables are:\n * `GCP_PROJECT_ID` - ID of the Google Cloud project. For example: `apache-beam-testing`.\n * `GCP_REGION` - Region of the bucket and dataflow jobs. For example: `us-central1`.\n * `GCP_TESTING_BUCKET` - Name of the bucket where temporary files for Dataflow tests will be stored. For example: `beam-github-actions-tests`.\n * `GCP_PYTHON_WHEELS_BUCKET` - Name of the bucket where python source distribution and wheels will be stored. For example: `beam-wheels-staging`.\n * `GCP_SA_EMAIL` - Service account email address. This is usually of the format `<name>@<project-id>.iam.gserviceaccount.com`.\n * `GCP_SA_KEY` - Service account key. This key should be created and encoded as a Base64 string (eg. `cat my-key.json | base64` on macOS).\n\nService Account shall have following permissions ([IAM roles](https://cloud.google.com/iam/docs/understanding-roles)):\n * Storage Admin (roles/storage.admin)\n * Dataflow Admin (roles/dataflow.admin)\n * Artifact Registry writer (roles/artifactregistry.createOnPush)\n * Big Query Data Editor (roles/bigquery.dataEditor)\n * Service Account User (roles/iam.serviceAccountUser)\n\n### Workflows\n\n#### Build python source distribution and wheels - [build_wheels.yml](.github/workflows/build_wheels.yml)\n\n| Job                                             | Description                                                                                                                                                                                                                                                        | Pull Request Run | Direct Push/Merge Run | Scheduled Run | Requires GCP Credentials |\n|-------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|------------------|-----------------------|---------------|--------------------------|\n| Check GCP variables                             | Checks that GCP variables are set. Jobs which required them depend on the output of this job.                                                                                                                                                                      | Yes              | Yes                   | Yes           | Yes/No                   |\n| Build python source distribution                | Builds python source distribution and uploads it to artifacts. Artifacts from release branch are used in release process ([`build_release_candidate.sh`](release/src/main/scripts/build_release_candidate.sh))                                                     | Yes              | Yes                   | Yes           | -                        |\n| Prepare GCS                                     | Clears target path on GCS if already exists.                                                                                                                                                                                                                       | -                | Yes                   | Yes           | Yes                      |\n| Upload python source distribution to GCS bucket | Uploads python source distribution to GCS bucket for path unique for specific workflow run.                                                                                                                                                                        | -                | Yes                   | Yes           | Yes                      |\n| Build python wheels on linux/macos/windows      | Builds python wheels on linux/macos/windows platform with usage of `cibuildwheel` and uploads it to artifacts. Artifacts from release branch are used in release process ( [ `build_release_candidate.sh` ](release/src/main/scripts/build_release_candidate.sh) ) | Yes              | Yes                   | Yes           | -                        |\n| Upload python wheels to GCS bucket              | Uploads python wheels to GCS bucket for path unique for specific workflow run. Additionally uploads workflow run data.                                                                                                                                             | -                | Yes                   | Yes           | Yes                      |\n| List files on Google Cloud Storage Bucket       | Lists files on GCS for verification purpose.                                                                                                                                                                                                                       | -                | Yes                   | Yes           | Yes                      |\n| Branch repo nightly                             | Branch repo with `nightly-master` if build python source distribution and python wheels finished successfully.                                                                                                                                                     | -                | -                     | Yes           | -                        |\n\n#### Python tests - [python_tests.yml](.github/workflows/python_tests.yml)\n\n| Job                              | Description                                                                                                           | Pull Request Run | Direct Push/Merge Run | Scheduled Run | Requires GCP Credentials |\n|----------------------------------|-----------------------------------------------------------------------------------------------------------------------|------------------|-----------------------|---------------|--------------------------|\n| Check GCP variables              | Checks that GCP variables are set. Jobs which required them depend on the output of this job.                         | Yes              | Yes                   | Yes           | Yes/No                   |\n| Build python source distribution | Builds python source distribution and uploads it to artifacts. Artifacts are used in `Python Wordcount Dataflow` job. | -                | Yes                   | Yes           | Yes                      |\n| Python Unit Tests                | Runs python unit tests.                                                                                               | Yes              | Yes                   | Yes           | -                        |\n| Python Wordcount Direct Runner   | Runs python WordCount example with Direct Runner.                                                                     | Yes              | Yes                   | Yes           | -                        |\n| Python Wordcount Dataflow        | Runs python WordCount example with DataFlow Runner.                                                                   | -                | Yes                   | Yes           | Yes                      |\n\n#### Java tests - [java_tests.yml](.github/workflows/java_tests.yml)\n\n| Job                          | Description                                                                                   | Pull Request Run | Direct Push/Merge Run | Scheduled Run | Requires GCP Credentials |\n|------------------------------|-----------------------------------------------------------------------------------------------|------------------|-----------------------|---------------|--------------------------|\n| Check GCP variables          | Checks that GCP variables are set. Jobs which required them depend on the output of this job. | Yes              | Yes                   | Yes           | Yes/No                   |\n| Java Unit Tests              | Runs Java unit tests.                                                                         | Yes              | Yes                   | Yes           | -                        |\n| Java Wordcount Direct Runner | Runs Java WordCount example with Direct Runner.                                               | Yes              | Yes                   | Yes           | -                        |\n| Java Wordcount Dataflow      | Runs Java WordCount example with DataFlow Runner.                                             | -                | Yes                   | Yes           | Yes                      |\n\n### Release Preparation and Validation Workflows\n\n#### Start Snapshot Build - [start_snapshot_build.yml](.github/workflows/start_snapshot_build.yml)\n| Job                   | Description                                                             | Pull Request Run | Direct Push/Merge Run | Scheduled Run | Requires GCP Credentials |\n|-----------------------|-------------------------------------------------------------------------|------------------|-----------------------|---------------|--------------------------|\n| Start Snapshot Build  | Creates PR against apache:master and triggers a job to build a snapshot | No               | No                    | No            | No                       |\n\n#### Choose RC Commit - [choose_rc_commit.yml](.github/workflows/choose_rc_commit.yml)\n\n| Job              | Description                                                                                         | Pull Request Run | Direct Push/Merge Run | Scheduled Run | Requires GCP Credentials |\n|------------------|-----------------------------------------------------------------------------------------------------|------------------|-----------------------|---------------|--------------------------|\n| Choose RC Commit | Chooses a commit to be the basis of a release candidate and pushes a new tagged commit for that RC. | No               | No                    | No            | No                       |\n\n#### Cut Release Branch - [verify_release_build.yml](.github/workflows/cut_release_branch.yml)\n| Job                   | Description                                                | Pull Request Run | Direct Push/Merge Run | Scheduled Run | Requires GCP Credentials |\n|-----------------------|------------------------------------------------------------|------------------|-----------------------|---------------|--------------------------|\n| Update Master         | Update Apache Beam master branch with next release version | No               | No                    | No            | No                       |\n| Update Release Branch | Cut release branch for current development version         | No               | No                    | No            | No                       |\n\n#### Verify Release Build - [verify_release_build.yml](.github/workflows/verify_release_build.yml)\n\n| Job                          | Description                                                                                   | Pull Request Run | Direct Push/Merge Run | Scheduled Run | Requires GCP Credentials |\n|------------------------------|-----------------------------------------------------------------------------------------------|------------------|-----------------------|---------------|--------------------------|\n| Verify Release Build         | Verifies full life cycle of Gradle Build and all PostCommit/PreCommit tests against Release Branch on CI.                   | No               | No                    | No            | No                       |\n\n#### Git tag Release Version - [git_tag_released_version.yml](.github/workflows/git_tag_released_version.yml)\n\n| Job                             | Description                                                                                                    | Pull Request Run | Direct Push/Merge Run | Scheduled Run | Requires GCP Credentials |\n|---------------------------------|----------------------------------------------------------------------------------------------------------------|------------------|-----------------------|---------------|--------------------------|\n| Git Tag Release Version         | Create and push a new tag for the released version by copying the tag for the final release candidate.         | No               | No                    | No            | No                       |\n\n#### Run RC Validation - [run_rc_validation.yml](.github/workflows/run_rc_validation.yml)\n\n| Job                          | Description                                                                                   | Pull Request Run | Direct Push/Merge Run | Scheduled Run | Requires GCP Credentials |\n|------------------------------|-----------------------------------------------------------------------------------------------|------------------|-----------------------|---------------|--------------------------|\n| Python Release Candidate     | Comment on PR to trigger Python ReleaseCandidate Jenkins job.                                 | No               | No                    | No            | No                       |\n| Python XLang SQL Taxi        | Runs Python XLang SQL Taxi with DataflowRunner                                                | No               | No                    | No            | Yes                      |\n| Python XLang Kafka           | Runs Python XLang Kafka Taxi with DataflowRunner                                              | No               | No                    | No            | Yes                      |\n| Direct Runner Leaderboard    | Runs Python Leaderboard with DirectRunner                                                     | No               | No                    | No            | Yes                      |\n| Direct Runner GameStats      | Runs Python GameStats with DirectRunner.                                                      | No               | No                    | No            | Yes                      |\n| Dataflow Runner Leaderboard  | Runs Python Leaderboard with DataflowRunner                                                   | No               | No                    | No            | Yes                      |\n| Dataflow Runner GameStats    | Runs Python GameStats with DataflowRunner                                                     | No               | No                    | No            | Yes                      |\n\n### All migrated workflows run based on the following triggers\n\n| Description | Pull Request Run | Direct Push/Merge Run | Scheduled Run | Workflow Dispatch |\n|-------------|------------------|-----------------------|---------------|-------------------|\n| PostCommit  | No               | Yes                   | Yes           | Yes               |\n| PreCommit   | Yes              | Yes                   | Yes           | Yes               |\n\n### PreCommit Workflows\n\n| Workflow                                                                         | Description             | Requires GCP Credentials  |\n|----------------------------------------------------------------------------------|-------------------------|---------------------------|\n| [job-precommit-placeholder.yml](.github/workflows/job-precommit-placeholder.yml) | Description placeholder | Yes/No                    |\n\n### PostCommit Workflows\n\n| Workflow                                                                           | Description             | Requires GCP Credentials |\n|------------------------------------------------------------------------------------|-------------------------|--------------------------|\n| [job-postcommit-placeholder.yml](.github/workflows/job-postcommit-placeholder.yml) | Description placeholder | Yes/No                   |\n\n### GitHub Action Tips\n\n* All migrated workflows get executed on **pre-configured self-hosted** runners. For this reason, GCP credentials are **only** needed when running the workflows in a different runner.\n* If you introduce changes to the workflow it is possible that your changes will not be present in the check run triggered in Pull Request.\nIn this case please attach link to the modified workflow run executed on your fork.\n* Possible timeouts with macOS runner - existing issue: [(X) This check failed - sometimes happens on macOS runner #841](https://github.com/actions/virtual-environments/issues/841)\n* [GitHub Actions Documentation](https://docs.github.com/en/actions)\n"
        },
        {
          "name": "CONTRIBUTING.md",
          "type": "blob",
          "size": 14.716796875,
          "content": "<!--\n    Licensed to the Apache Software Foundation (ASF) under one\n    or more contributor license agreements.  See the NOTICE file\n    distributed with this work for additional information\n    regarding copyright ownership.  The ASF licenses this file\n    to you under the Apache License, Version 2.0 (the\n    \"License\"); you may not use this file except in compliance\n    with the License.  You may obtain a copy of the License at\n\n      http://www.apache.org/licenses/LICENSE-2.0\n\n    Unless required by applicable law or agreed to in writing,\n    software distributed under the License is distributed on an\n    \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n    KIND, either express or implied.  See the License for the\n    specific language governing permissions and limitations\n    under the License.\n-->\n\n# Contributing to Beam\n\nThere are many ways to contribute to Beam, just one of which is by contributing code.\nFor a full list of ways to contribute and get plugged into Beam, see the\n[Beam Contribution Guide](https://beam.apache.org/contribute/)\n\n## Code Contributions\n\n*Before opening a pull request*, review the Beam contribution guide below.\nIt lists steps that are required before creating a PR and provides tips for\ngetting started. In particular, consider the following:\n\n- Have you searched for existing, related Issues and pull requests?\n- Have you shared your intent by creating an issue and commenting that you plan to take it on?\n- If the change is large, have you discussed it on the dev@ mailing list?\n- Is the change being proposed clearly explained and motivated?\n\nThese steps and instructions on getting started are outlined below as well.\n\n### Prerequisites\n\n- A [GitHub](https://github.com/) account.\n- A Linux, macOS, or Microsoft Windows development environment.\n- Java JDK 8 installed.\n- [Go](https://golang.org) 1.16.0 or later installed.\n- [Docker](https://www.docker.com/) installed for some tasks including building worker containers and testing changes to this website locally.\n- For SDK Development:\n  - Python 3.x interpreters. You will need Python interpreters for all Python versions supported by Beam.\n    Interpreters should be installed and available in shell via `python3.x` commands. For more information, see:\n    Python installation tips in [Developer Wiki](https://cwiki.apache.org/confluence/display/BEAM/Python+Tips#PythonTips-InstallingPythoninterpreters).\n- For large contributions, a signed [Individual Contributor License.\n  Agreement](https://www.apache.org/licenses/icla.pdf) (ICLA) to the Apache\n  Software Foundation (ASF).\n\n### Share Your Intent\n1. Find or create an issue in the [Beam repo](https://github.com/apache/beam/issues/new/choose).\n   Tracking your work in an issue will avoid duplicated or conflicting work, and provide\n   a place for notes. Later, your pull request will be linked to the issue as well.\n2. Comment \".take-issue\" on the issue. This will cause the issue to be assigned to you.\n   When you've completed the issue, you can close it by commenting \".close-issue\".\n   If you are a committer and would like to assign an issue to a non-committer, they must comment\n   on the issue first; please tag the user asking them to do so or to comment \"\\`.take-issue\\`\".\n   The command will be ignored if it is surrounded by `\\`` markdown characters.\n3. If your change is large or it is your first change, it is a good idea to\n   [discuss it on the dev@beam.apache.org mailing list](https://beam.apache.org/community/contact-us/).\n4. For large changes create a design doc\n   ([template](https://s.apache.org/beam-design-doc-template),\n   [examples](https://s.apache.org/beam-design-docs)) and email it to the [dev@beam.apache.org mailing list](https://beam.apache.org/community/contact-us/).\n\n### Setup Your Environment and Learn About Language Specific Setup\n\nBefore you begin, check out the Wiki pages. There are many useful tips about [Git](https://cwiki.apache.org/confluence/display/BEAM/Git+Tips), [Go](https://cwiki.apache.org/confluence/display/BEAM/Go+Tips), [Gradle](https://cwiki.apache.org/confluence/display/BEAM/Gradle+Tips), [Java](https://cwiki.apache.org/confluence/display/BEAM/Java+Tips), [Python](https://cwiki.apache.org/confluence/display/BEAM/Python+Tips), etc.\n\n#### Configuration Options\nYou have two options for configuring your development environment:\n- Local:\n  - Manually installing the [prerequisites](https://beam.apache.org/contribute/#prerequisites).\n  - Using the automated script for Linux and macOS.\n- Container-based: using a [Docker](https://www.docker.com/) image.\n\n##### Local: Debian-based Distribution\n\n###### Manual steps\n\nTo install these in a Debian-based distribution:\n1. Execute:\n    ```\n    sudo apt-get install \\\n       openjdk-8-jdk \\\n       python-setuptools \\\n       python-pip \\\n       virtualenv \\\n       tox \\\n       docker-ce\n    ```\n2. On some systems, like Ubuntu 20.04, install these:\n    ```\n    pip3 install grpcio-tools mypy-protobuf\n    ```\n3. If you develop in GO:\n  1. Install [Go](https://golang.org/doc/install).\n  2. Check BEAM repo is in: `$GOPATH/src/github.com/apache/`\n  3. At the end, it should look like this: `$GOPATH/src/github.com/apache/beam`\n4. Once Go is installed, install goavro:\n    ```\n    $ export GOPATH=`pwd`/sdks/go/examples/.gogradle/project_gopath\n    $ go get github.com/linkedin/goavro/v2\n    ```\n**Important**: gLinux users should configure their machines for sudoless Docker.\n\n###### Automated script for Linux and macOS\n\nYou can install these in a Debian-based distribution for Linux or macOs using the [local-env-setup.sh](https://github.com/apache/beam/blob/master/local-env-setup.sh) script, which is part of the Beam repo. It contains:\n\n* pip3 packages\n* go packages\n* goavro\n* JDK 8\n* Python\n* Docker\n\nTo install execute:\n```\n./local-env-setup.sh\n```\n\n##### Container: Docker-based\n\nAlternatively, you can use the Docker based local development environment to wrap your clone of the Beam repo\ninto a container meeting the requirements above.\n\nYou can start this container using the [start-build-env.sh](https://github.com/apache/beam/blob/master/start-build-env.sh) script which is part of the Beam repo.\n\nExecute:\n```\n./start-build-env.sh\n```\n\n#### Development Setup {#development-setup}\n\n1. Check [Git workflow tips](https://cwiki.apache.org/confluence/display/BEAM/Git+Tips) if you need help with git forking, cloning, branching, committing, pull requests, and squashing commits.\n\n2. Make a fork of https://github.com/apache/beam repo.\n\n3. Clone the forked repository. You can download it anywhere you like.\n    ```\n    $ mkdir -p ~/path/to/your/folder\n    $ cd ~/path/to/your/folder\n    $ git clone https://github.com/forked/apache/beam\n    $ cd beam\n    ```\n   For **Go development**:\n\n   We recommend putting it in your `$GOPATH` (`$HOME/go` by default on Unix systems).\n\n   Clone the repo, and update your branch as normal:\n    ```\n    $ git clone https://github.com/apache/beam.git\n    $ cd beam\n    $ git remote add <GitHub_user> git@github.com:<GitHub_user>/beam.git\n    $ git fetch --all\n    ```\n\n   Get or Update all the Go SDK dependencies:\n    ```\n    $ go get -u ./...\n    ```\n\n4. Check the environment was set up correctly.\n\n   **Option 1**: validate the Go, Java, and Python environments:\n\n   **Important**: Make sure you have activated Python development.\n    ```\n    ./gradlew :checkSetup\n    ```\n   **Option 2**: Run independent checks:\n  - For **Go development**:\n      ```\n      export GOLANG_PROTOBUF_REGISTRATION_CONFLICT=ignore./gradlew :sdks:go:examples:wordCount\n      ```\n  - For **Python development**:\n      ```\n      ./gradlew :sdks:python:wordCount\n      ```\n  - For **Java development**:\n      ```\n      ./gradlew :examples:java:wordCount\n     ```\n\n5. Familiarize yourself with gradle and the project structure.\n\n   At the root of the git repository, run:\n    ```\n    $ ./gradlew projects\n    ```\n   Examine the available tasks in a project. For the default set of tasks, use:\n    ```\n    $ ./gradlew tasks\n    ```\n   For a given module, use:\n    ```\n    $ ./gradlew -p sdks/java/io/cassandra tasks\n    ```\n   For an exhaustive list of tasks, use:\n    ```\n    $ ./gradlew tasks --all\n    ```\n\n6. Make sure you can build and run tests.\n\n   Since Beam is a large project, usually, you will want to limit testing to the particular module you are working on. Gradle will build just the necessary things to run those tests. For example:\n    ```\n    $ ./gradlew -p sdks/go check\n    $ ./gradlew -p sdks/java/io/cassandra check\n    $ ./gradlew -p runners/flink check\n    ```\n\n7. Now you may want to set up your preferred IDE and other aspects of your development\n   environment. See the Developers' wiki for tips, guides, and FAQs on:\n  - [IntelliJ](https://cwiki.apache.org/confluence/display/BEAM/Using+IntelliJ+IDE)\n  - [Java](https://cwiki.apache.org/confluence/display/BEAM/Java+Tips)\n  - [Python](https://cwiki.apache.org/confluence/display/BEAM/Python+Tips)\n  - [Go](https://cwiki.apache.org/confluence/display/BEAM/Go+Tips)\n  - [Website](https://cwiki.apache.org/confluence/display/BEAM/Website+Tips)\n  - [Gradle](https://cwiki.apache.org/confluence/display/BEAM/Gradle+Tips)\n  - [Jenkins](https://cwiki.apache.org/confluence/display/BEAM/Jenkins+Tips)\n  - [FAQ](https://cwiki.apache.org/confluence/display/BEAM/Contributor+FAQ)\n\n### Create a Pull Request\n\n1. Make your code change. Every source file needs to include the Apache license header. Every new dependency needs to\n   have an open source license [compatible](https://www.apache.org/legal/resolved.html#criteria) with Apache.\n\n2. Add unit tests for your change.\n\n3. Use descriptive commit messages that make it easy to identify changes and provide a clear history.\n\n4. When your change is ready to be reviewed and merged, create a pull request.\n\n5. Link to the issue you are addressing in your pull request.\n\n6. The pull request and any changes pushed to it will trigger [pre-commit\n   jobs](https://cwiki.apache.org/confluence/display/BEAM/Contribution+Testing+Guide#ContributionTestingGuide-Pre-commit). If a test fails and appears unrelated to your\n   change, you can cause tests to be re-run by adding a single line comment on your\n   PR:\n    ```\n    retest this please\n    ```\nPull request template has a link to a [catalog of trigger phrases](https://github.com/apache/beam/blob/master/.test-infra/jenkins/README.md)\nthat start various post-commit tests suites. Use these sparingly because post-commit tests consume shared development resources.\n\n### Review Process and Releases\n\n#### Get Reviewed\n\nYour pull requests should automatically have reviewers assigned within a few hours of opening it.\nIf that doesn't happen for some reason, you can also request a review yourself.\n\n1. Pull requests can only be merged by a\n   [Beam committer](https://home.apache.org/phonebook.html?pmc=beam).\n   To find a committer for your area, either:\n  - look for similar code merges, or\n  - ask on [dev@beam.apache.org](https://beam.apache.org/community/contact-us/)\n\n   Use `R: @username` in the pull request to notify a reviewer.\n\n2. If you don't get any response in 3 business days, email the [dev@beam.apache.org mailing list](https://beam.apache.org/community/contact-us/) to ask for someone to look at your pull request.\n\n#### Make the Reviewer’s Job Easier\n\n1. Provide context for your changes in the associated issue and/or PR description.\n\n2. Avoid huge mega-changes.\n\n3. Review feedback typically leads to follow-up changes. It is easier to review follow-up changes when they are added as additional \"fixup\" commits to the\n   existing PR/branch. This allows reviewer(s) to track the incremental progress and focus on new changes,\n   and keeps comment threads attached to the code.\n   Please refrain from squashing new commits into reviewed commits before review is completed.\n   Because squashing reviewed and unreviewed commits often makes it harder to\n   see the difference between the review iterations, reviewers may ask you to unsquash new changes.\n\n4. After review is complete and the PR is accepted, fixup commits should be squashed (see [Git workflow tips](https://cwiki.apache.org/confluence/display/BEAM/Git+Tips)).\n   Beam committers [can squash](https://beam.apache.org/contribute/committer-guide/#merging-it)\n   all commits in the PR during merge, however if a PR has a mixture of independent changes that should not be squashed, and fixup commits,\n   then the PR author should help squashing fixup commits to maintain a clean commit history.\n\n#### Apache Beam Releases\n\nApache Beam makes minor releases every 6 weeks. Apache Beam has a\n[calendar](https://calendar.google.com/calendar/embed?src=0p73sl034k80oob7seouanigd0%40group.calendar.google.com) for\ncutting the next release branch. Your change needs to be checked into master before the release branch is cut\nto make the next release.\n\n#### Stale Pull Requests\n\nThe community will close stale pull requests in order to keep the project\nhealthy. A pull request becomes stale after its author fails to respond to\nactionable comments for 60 days.  Author of a closed pull request is welcome to\nreopen the same pull request again in the future.\n\n### Troubleshooting\n\nIf you run into any issues, check out the [contribution FAQ](https://cwiki.apache.org/confluence/display/BEAM/Contributor+FAQ) or ask on the [dev@ mailing list](https://beam.apache.org/community/contact-us/) or [#beam channel of the ASF Slack](https://beam.apache.org/community/contact-us/).\n\nIf you didn't find the information you were looking for in this guide, please\n[reach out to the Beam community](https://beam.apache.org/community/contact-us/).\n\n</div>\n\n## Find Efforts to Contribute to\nA great way to contribute is to join an existing effort. If you want to get involved but don’t have a project in mind, check our [list of open starter tasks](https://s.apache.org/beam-starter-tasks).\nFor the most intensive efforts, check out the [roadmap](https://beam.apache.org/roadmap/).\n\n## Contributing to the Developer Documentation\n\nNew contributors are often best equipped to find gaps in the developer documentation.\nIf you'd like to contribute to our documentation, either open a PR in the Beam repo with\nthe proposed changes or make edits to the [Beam wiki](https://cwiki.apache.org/confluence/display/BEAM/Apache+Beam).\n\nBy default, everyone has access to the wiki. If you wish to contribute changes,\nplease create an account and request edit access on the dev@beam.apache.org mailing list (include your Wiki account user ID).\n\n## Additional Resources\nPlease see Beam developers’ [Wiki Contributor FAQ](https://cwiki.apache.org/confluence/display/BEAM/Contributor+FAQ) for more information.\n\nIf you are contributing a ```PTransform``` to Beam, we have an extensive [PTransform Style Guide](https://beam.apache.org/contribute/ptransform-style-guide).\n\nIf you are contributing a Runner to Beam, refer to the [Runner authoring guide](https://beam.apache.org/contribute/runner-guide/).\n\nReview [design documents](https://s.apache.org/beam-design-docs).\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 21.3173828125,
          "content": "\n                                 Apache License\n                           Version 2.0, January 2004\n                        http://www.apache.org/licenses/\n\n   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n\n   1. Definitions.\n\n      \"License\" shall mean the terms and conditions for use, reproduction,\n      and distribution as defined by Sections 1 through 9 of this document.\n\n      \"Licensor\" shall mean the copyright owner or entity authorized by\n      the copyright owner that is granting the License.\n\n      \"Legal Entity\" shall mean the union of the acting entity and all\n      other entities that control, are controlled by, or are under common\n      control with that entity. For the purposes of this definition,\n      \"control\" means (i) the power, direct or indirect, to cause the\n      direction or management of such entity, whether by contract or\n      otherwise, or (ii) ownership of fifty percent (50%) or more of the\n      outstanding shares, or (iii) beneficial ownership of such entity.\n\n      \"You\" (or \"Your\") shall mean an individual or Legal Entity\n      exercising permissions granted by this License.\n\n      \"Source\" form shall mean the preferred form for making modifications,\n      including but not limited to software source code, documentation\n      source, and configuration files.\n\n      \"Object\" form shall mean any form resulting from mechanical\n      transformation or translation of a Source form, including but\n      not limited to compiled object code, generated documentation,\n      and conversions to other media types.\n\n      \"Work\" shall mean the work of authorship, whether in Source or\n      Object form, made available under the License, as indicated by a\n      copyright notice that is included in or attached to the work\n      (an example is provided in the Appendix below).\n\n      \"Derivative Works\" shall mean any work, whether in Source or Object\n      form, that is based on (or derived from) the Work and for which the\n      editorial revisions, annotations, elaborations, or other modifications\n      represent, as a whole, an original work of authorship. For the purposes\n      of this License, Derivative Works shall not include works that remain\n      separable from, or merely link (or bind by name) to the interfaces of,\n      the Work and Derivative Works thereof.\n\n      \"Contribution\" shall mean any work of authorship, including\n      the original version of the Work and any modifications or additions\n      to that Work or Derivative Works thereof, that is intentionally\n      submitted to Licensor for inclusion in the Work by the copyright owner\n      or by an individual or Legal Entity authorized to submit on behalf of\n      the copyright owner. For the purposes of this definition, \"submitted\"\n      means any form of electronic, verbal, or written communication sent\n      to the Licensor or its representatives, including but not limited to\n      communication on electronic mailing lists, source code control systems,\n      and issue tracking systems that are managed by, or on behalf of, the\n      Licensor for the purpose of discussing and improving the Work, but\n      excluding communication that is conspicuously marked or otherwise\n      designated in writing by the copyright owner as \"Not a Contribution.\"\n\n      \"Contributor\" shall mean Licensor and any individual or Legal Entity\n      on behalf of whom a Contribution has been received by Licensor and\n      subsequently incorporated within the Work.\n\n   2. Grant of Copyright License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      copyright license to reproduce, prepare Derivative Works of,\n      publicly display, publicly perform, sublicense, and distribute the\n      Work and such Derivative Works in Source or Object form.\n\n   3. Grant of Patent License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      (except as stated in this section) patent license to make, have made,\n      use, offer to sell, sell, import, and otherwise transfer the Work,\n      where such license applies only to those patent claims licensable\n      by such Contributor that are necessarily infringed by their\n      Contribution(s) alone or by combination of their Contribution(s)\n      with the Work to which such Contribution(s) was submitted. If You\n      institute patent litigation against any entity (including a\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\n      or a Contribution incorporated within the Work constitutes direct\n      or contributory patent infringement, then any patent licenses\n      granted to You under this License for that Work shall terminate\n      as of the date such litigation is filed.\n\n   4. Redistribution. You may reproduce and distribute copies of the\n      Work or Derivative Works thereof in any medium, with or without\n      modifications, and in Source or Object form, provided that You\n      meet the following conditions:\n\n      (a) You must give any other recipients of the Work or\n          Derivative Works a copy of this License; and\n\n      (b) You must cause any modified files to carry prominent notices\n          stating that You changed the files; and\n\n      (c) You must retain, in the Source form of any Derivative Works\n          that You distribute, all copyright, patent, trademark, and\n          attribution notices from the Source form of the Work,\n          excluding those notices that do not pertain to any part of\n          the Derivative Works; and\n\n      (d) If the Work includes a \"NOTICE\" text file as part of its\n          distribution, then any Derivative Works that You distribute must\n          include a readable copy of the attribution notices contained\n          within such NOTICE file, excluding those notices that do not\n          pertain to any part of the Derivative Works, in at least one\n          of the following places: within a NOTICE text file distributed\n          as part of the Derivative Works; within the Source form or\n          documentation, if provided along with the Derivative Works; or,\n          within a display generated by the Derivative Works, if and\n          wherever such third-party notices normally appear. The contents\n          of the NOTICE file are for informational purposes only and\n          do not modify the License. You may add Your own attribution\n          notices within Derivative Works that You distribute, alongside\n          or as an addendum to the NOTICE text from the Work, provided\n          that such additional attribution notices cannot be construed\n          as modifying the License.\n\n      You may add Your own copyright statement to Your modifications and\n      may provide additional or different license terms and conditions\n      for use, reproduction, or distribution of Your modifications, or\n      for any such Derivative Works as a whole, provided Your use,\n      reproduction, and distribution of the Work otherwise complies with\n      the conditions stated in this License.\n\n   5. Submission of Contributions. Unless You explicitly state otherwise,\n      any Contribution intentionally submitted for inclusion in the Work\n      by You to the Licensor shall be under the terms and conditions of\n      this License, without any additional terms or conditions.\n      Notwithstanding the above, nothing herein shall supersede or modify\n      the terms of any separate license agreement you may have executed\n      with Licensor regarding such Contributions.\n\n   6. Trademarks. This License does not grant permission to use the trade\n      names, trademarks, service marks, or product names of the Licensor,\n      except as required for reasonable and customary use in describing the\n      origin of the Work and reproducing the content of the NOTICE file.\n\n   7. Disclaimer of Warranty. Unless required by applicable law or\n      agreed to in writing, Licensor provides the Work (and each\n      Contributor provides its Contributions) on an \"AS IS\" BASIS,\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n      implied, including, without limitation, any warranties or conditions\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n      PARTICULAR PURPOSE. You are solely responsible for determining the\n      appropriateness of using or redistributing the Work and assume any\n      risks associated with Your exercise of permissions under this License.\n\n   8. Limitation of Liability. In no event and under no legal theory,\n      whether in tort (including negligence), contract, or otherwise,\n      unless required by applicable law (such as deliberate and grossly\n      negligent acts) or agreed to in writing, shall any Contributor be\n      liable to You for damages, including any direct, indirect, special,\n      incidental, or consequential damages of any character arising as a\n      result of this License or out of the use or inability to use the\n      Work (including but not limited to damages for loss of goodwill,\n      work stoppage, computer failure or malfunction, or any and all\n      other commercial damages or losses), even if such Contributor\n      has been advised of the possibility of such damages.\n\n   9. Accepting Warranty or Additional Liability. While redistributing\n      the Work or Derivative Works thereof, You may choose to offer,\n      and charge a fee for, acceptance of support, warranty, indemnity,\n      or other liability obligations and/or rights consistent with this\n      License. However, in accepting such obligations, You may act only\n      on Your own behalf and on Your sole responsibility, not on behalf\n      of any other Contributor, and only if You agree to indemnify,\n      defend, and hold each Contributor harmless for any liability\n      incurred by, or claims asserted against, such Contributor by reason\n      of your accepting any such warranty or additional liability.\n\n   END OF TERMS AND CONDITIONS\n\n   APPENDIX: How to apply the Apache License to your work.\n\n      To apply the Apache License to your work, attach the following\n      boilerplate notice, with the fields enclosed by brackets \"[]\"\n      replaced with your own identifying information. (Don't include\n      the brackets!)  The text should be enclosed in the appropriate\n      comment syntax for the file format. We also recommend that a\n      file or class name and description of purpose be included on the\n      same \"printed page\" as the copyright notice for easier\n      identification within third-party archives.\n\n   Copyright [yyyy] [name of copyright owner]\n\n   Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.\n\n   A part of several convenience binary distributions of this software is licensed as follows:\n\n   Google Protobuf:\n     Copyright 2008 Google Inc.  All rights reserved.\n\n     Redistribution and use in source and binary forms, with or without\n     modification, are permitted provided that the following conditions are\n     met:\n\n         * Redistributions of source code must retain the above copyright\n     notice, this list of conditions and the following disclaimer.\n         * Redistributions in binary form must reproduce the above\n     copyright notice, this list of conditions and the following disclaimer\n     in the documentation and/or other materials provided with the\n     distribution.\n         * Neither the name of Google Inc. nor the names of its\n     contributors may be used to endorse or promote products derived from\n     this software without specific prior written permission.\n\n     THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS\n     \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT\n     LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR\n     A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT\n     OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,\n     SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT\n     LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,\n     DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY\n     THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n     (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n     OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\n     Code generated by the Protocol Buffer compiler is owned by the owner\n     of the input file used when generating it.  This code is not\n     standalone and requires a support library to be linked with it.  This\n     support library is itself covered by the above license.\n\n   jsr-305:\n    Copyright (c) 2007-2009, JSR305 expert group\n    All rights reserved.\n\n    https://opensource.org/licenses/BSD-3-Clause\n\n    Redistribution and use in source and binary forms, with or without\n    modification, are permitted provided that the following conditions are met:\n\n        * Redistributions of source code must retain the above copyright notice,\n          this list of conditions and the following disclaimer.\n        * Redistributions in binary form must reproduce the above copyright notice,\n          this list of conditions and the following disclaimer in the documentation\n          and/or other materials provided with the distribution.\n        * Neither the name of the JSR305 expert group nor the names of its\n          contributors may be used to endorse or promote products derived from\n          this software without specific prior written permission.\n\n    THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\n    AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n    THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n    ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE\n    LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\n    CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\n    SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\n    INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\n    CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\n    ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\n    POSSIBILITY OF SUCH DAMAGE.\n\n   janino-compiler:\n    Janino - An embedded Java[TM] compiler\n\n    Copyright (c) 2001-2016, Arno Unkrig\n    Copyright (c) 2015-2016  TIBCO Software Inc.\n    All rights reserved.\n\n    Redistribution and use in source and binary forms, with or without\n    modification, are permitted provided that the following conditions\n    are met:\n\n       1. Redistributions of source code must retain the above copyright\n          notice, this list of conditions and the following disclaimer.\n       2. Redistributions in binary form must reproduce the above\n          copyright notice, this list of conditions and the following\n          disclaimer in the documentation and/or other materials\n          provided with the distribution.\n       3. Neither the name of JANINO nor the names of its contributors\n          may be used to endorse or promote products derived from this\n          software without specific prior written permission.\n\n    THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\n    AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n    IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n    ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDERS OR CONTRIBUTORS BE\n    LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\n    CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\n    SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\n    INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER\n    IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n    OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN\n    IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\n   jline:\n    Copyright (c) 2002-2016, the original author or authors.\n    All rights reserved.\n\n    http://www.opensource.org/licenses/bsd-license.php\n\n    Redistribution and use in source and binary forms, with or\n    without modification, are permitted provided that the following\n    conditions are met:\n\n    Redistributions of source code must retain the above copyright\n    notice, this list of conditions and the following disclaimer.\n\n    Redistributions in binary form must reproduce the above copyright\n    notice, this list of conditions and the following disclaimer\n    in the documentation and/or other materials provided with\n    the distribution.\n\n    Neither the name of JLine nor the names of its contributors\n    may be used to endorse or promote products derived from this\n    software without specific prior written permission.\n\n    THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS\n    \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING,\n    BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY\n    AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO\n    EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE\n    FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY,\n    OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n    PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,\n    DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED\n    AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT\n    LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING\n    IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED\n    OF THE POSSIBILITY OF SUCH DAMAGE.\n\n   sqlline:\n    SQLLine - Shell for issuing SQL to relational databases via JDBC\n\n    Copyright (c) 2002,2003,2004,2005,2006,2007 Marc Prud'hommeaux\n    Copyright (c) 2004-2010 The Eigenbase Project\n    Copyright (c) 2013-2017 Julian Hyde\n    All rights reserved.\n\n    ===============================================================================\n\n    Licensed under the Modified BSD License (the \"License\"); you may not\n    use this file except in compliance with the License. You may obtain a\n    copy of the License at:\n\n    http://opensource.org/licenses/BSD-3-Clause\n\n    Redistribution and use in source and binary forms,\n    with or without modification, are permitted provided\n    that the following conditions are met:\n\n    (1) Redistributions of source code must retain the above copyright\n        notice, this list of conditions and the following disclaimer.\n\n    (2) Redistributions in binary form must reproduce the above copyright\n        notice, this list of conditions and the following disclaimer in the\n        documentation and/or other materials provided with the\n        distribution.\n\n    (3) The name of the author may not be used to endorse or promote\n        products derived from this software without specific prior written\n        permission.\n\n    THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS\n    \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT\n    LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR\n    A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT\n    OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,\n    SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT\n    LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,\n    DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY\n    THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n    (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n    OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\n   slf4j:\n    Copyright (c) 2004-2017 QOS.ch\n    All rights reserved.\n\n    Permission is hereby granted, free  of charge, to any person obtaining\n    a  copy  of this  software  and  associated  documentation files  (the\n    \"Software\"), to  deal in  the Software without  restriction, including\n    without limitation  the rights to  use, copy, modify,  merge, publish,\n    distribute,  sublicense, and/or sell  copies of  the Software,  and to\n    permit persons to whom the Software  is furnished to do so, subject to\n    the following conditions:\n\n    The  above  copyright  notice  and  this permission  notice  shall  be\n    included in all copies or substantial portions of the Software.\n\n    THE  SOFTWARE IS  PROVIDED  \"AS  IS\", WITHOUT  WARRANTY  OF ANY  KIND,\n    EXPRESS OR  IMPLIED, INCLUDING  BUT NOT LIMITED  TO THE  WARRANTIES OF\n    MERCHANTABILITY,    FITNESS    FOR    A   PARTICULAR    PURPOSE    AND\n    NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE\n    LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION\n    OF CONTRACT, TORT OR OTHERWISE,  ARISING FROM, OUT OF OR IN CONNECTION\n    WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE\n\nSee the adjacent LICENSE.python file, if present, for additional licenses that\napply to parts of Apache Beam Python."
        },
        {
          "name": "LICENSE.python",
          "type": "blob",
          "size": 12.654296875,
          "content": "\n################################################################################\nCPython LICENSE. Source:\nhttps://github.com/python/cpython/blob/81574b80e92554adf75c13fa42415beb8be383cb/LICENSE\n\nA. HISTORY OF THE SOFTWARE\n==========================\n\nPython was created in the early 1990s by Guido van Rossum at Stichting\nMathematisch Centrum (CWI, see http://www.cwi.nl) in the Netherlands\nas a successor of a language called ABC.  Guido remains Python's\nprincipal author, although it includes many contributions from others.\n\nIn 1995, Guido continued his work on Python at the Corporation for\nNational Research Initiatives (CNRI, see http://www.cnri.reston.va.us)\nin Reston, Virginia where he released several versions of the\nsoftware.\n\nIn May 2000, Guido and the Python core development team moved to\nBeOpen.com to form the BeOpen PythonLabs team.  In October of the same\nyear, the PythonLabs team moved to Digital Creations, which became\nZope Corporation.  In 2001, the Python Software Foundation (PSF, see\nhttps://www.python.org/psf/) was formed, a non-profit organization\ncreated specifically to own Python-related Intellectual Property.\nZope Corporation was a sponsoring member of the PSF.\n\nAll Python releases are Open Source (see http://www.opensource.org for\nthe Open Source Definition).  Historically, most, but not all, Python\nreleases have also been GPL-compatible; the table below summarizes\nthe various releases.\n\n    Release         Derived     Year        Owner       GPL-\n                    from                                compatible? (1)\n\n    0.9.0 thru 1.2              1991-1995   CWI         yes\n    1.3 thru 1.5.2  1.2         1995-1999   CNRI        yes\n    1.6             1.5.2       2000        CNRI        no\n    2.0             1.6         2000        BeOpen.com  no\n    1.6.1           1.6         2001        CNRI        yes (2)\n    2.1             2.0+1.6.1   2001        PSF         no\n    2.0.1           2.0+1.6.1   2001        PSF         yes\n    2.1.1           2.1+2.0.1   2001        PSF         yes\n    2.1.2           2.1.1       2002        PSF         yes\n    2.1.3           2.1.2       2002        PSF         yes\n    2.2 and above   2.1.1       2001-now    PSF         yes\n\nFootnotes:\n\n(1) GPL-compatible doesn't mean that we're distributing Python under\n    the GPL.  All Python licenses, unlike the GPL, let you distribute\n    a modified version without making your changes open source.  The\n    GPL-compatible licenses make it possible to combine Python with\n    other software that is released under the GPL; the others don't.\n\n(2) According to Richard Stallman, 1.6.1 is not GPL-compatible,\n    because its license has a choice of law clause.  According to\n    CNRI, however, Stallman's lawyer has told CNRI's lawyer that 1.6.1\n    is \"not incompatible\" with the GPL.\n\nThanks to the many outside volunteers who have worked under Guido's\ndirection to make these releases possible.\n\nB. TERMS AND CONDITIONS FOR ACCESSING OR OTHERWISE USING PYTHON\n===============================================================\n\nPYTHON SOFTWARE FOUNDATION LICENSE VERSION 2\n--------------------------------------------\n\n1. This LICENSE AGREEMENT is between the Python Software Foundation\n(\"PSF\"), and the Individual or Organization (\"Licensee\") accessing and\notherwise using this software (\"Python\") in source or binary form and\nits associated documentation.\n\n2. Subject to the terms and conditions of this License Agreement, PSF hereby\ngrants Licensee a nonexclusive, royalty-free, world-wide license to reproduce,\nanalyze, test, perform and/or display publicly, prepare derivative works,\ndistribute, and otherwise use Python alone or in any derivative version,\nprovided, however, that PSF's License Agreement and PSF's notice of copyright,\ni.e., \"Copyright (c) 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010,\n2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018 Python Software Foundation; All\nRights Reserved\" are retained in Python alone or in any derivative version\nprepared by Licensee.\n\n3. In the event Licensee prepares a derivative work that is based on\nor incorporates Python or any part thereof, and wants to make\nthe derivative work available to others as provided herein, then\nLicensee hereby agrees to include in any such work a brief summary of\nthe changes made to Python.\n\n4. PSF is making Python available to Licensee on an \"AS IS\"\nbasis.  PSF MAKES NO REPRESENTATIONS OR WARRANTIES, EXPRESS OR\nIMPLIED.  BY WAY OF EXAMPLE, BUT NOT LIMITATION, PSF MAKES NO AND\nDISCLAIMS ANY REPRESENTATION OR WARRANTY OF MERCHANTABILITY OR FITNESS\nFOR ANY PARTICULAR PURPOSE OR THAT THE USE OF PYTHON WILL NOT\nINFRINGE ANY THIRD PARTY RIGHTS.\n\n5. PSF SHALL NOT BE LIABLE TO LICENSEE OR ANY OTHER USERS OF PYTHON\nFOR ANY INCIDENTAL, SPECIAL, OR CONSEQUENTIAL DAMAGES OR LOSS AS\nA RESULT OF MODIFYING, DISTRIBUTING, OR OTHERWISE USING PYTHON,\nOR ANY DERIVATIVE THEREOF, EVEN IF ADVISED OF THE POSSIBILITY THEREOF.\n\n6. This License Agreement will automatically terminate upon a material\nbreach of its terms and conditions.\n\n7. Nothing in this License Agreement shall be deemed to create any\nrelationship of agency, partnership, or joint venture between PSF and\nLicensee.  This License Agreement does not grant permission to use PSF\ntrademarks or trade name in a trademark sense to endorse or promote\nproducts or services of Licensee, or any third party.\n\n8. By copying, installing or otherwise using Python, Licensee\nagrees to be bound by the terms and conditions of this License\nAgreement.\n\nBEOPEN.COM LICENSE AGREEMENT FOR PYTHON 2.0\n-------------------------------------------\n\nBEOPEN PYTHON OPEN SOURCE LICENSE AGREEMENT VERSION 1\n\n1. This LICENSE AGREEMENT is between BeOpen.com (\"BeOpen\"), having an\noffice at 160 Saratoga Avenue, Santa Clara, CA 95051, and the\nIndividual or Organization (\"Licensee\") accessing and otherwise using\nthis software in source or binary form and its associated\ndocumentation (\"the Software\").\n\n2. Subject to the terms and conditions of this BeOpen Python License\nAgreement, BeOpen hereby grants Licensee a non-exclusive,\nroyalty-free, world-wide license to reproduce, analyze, test, perform\nand/or display publicly, prepare derivative works, distribute, and\notherwise use the Software alone or in any derivative version,\nprovided, however, that the BeOpen Python License is retained in the\nSoftware, alone or in any derivative version prepared by Licensee.\n\n3. BeOpen is making the Software available to Licensee on an \"AS IS\"\nbasis.  BEOPEN MAKES NO REPRESENTATIONS OR WARRANTIES, EXPRESS OR\nIMPLIED.  BY WAY OF EXAMPLE, BUT NOT LIMITATION, BEOPEN MAKES NO AND\nDISCLAIMS ANY REPRESENTATION OR WARRANTY OF MERCHANTABILITY OR FITNESS\nFOR ANY PARTICULAR PURPOSE OR THAT THE USE OF THE SOFTWARE WILL NOT\nINFRINGE ANY THIRD PARTY RIGHTS.\n\n4. BEOPEN SHALL NOT BE LIABLE TO LICENSEE OR ANY OTHER USERS OF THE\nSOFTWARE FOR ANY INCIDENTAL, SPECIAL, OR CONSEQUENTIAL DAMAGES OR LOSS\nAS A RESULT OF USING, MODIFYING OR DISTRIBUTING THE SOFTWARE, OR ANY\nDERIVATIVE THEREOF, EVEN IF ADVISED OF THE POSSIBILITY THEREOF.\n\n5. This License Agreement will automatically terminate upon a material\nbreach of its terms and conditions.\n\n6. This License Agreement shall be governed by and interpreted in all\nrespects by the law of the State of California, excluding conflict of\nlaw provisions.  Nothing in this License Agreement shall be deemed to\ncreate any relationship of agency, partnership, or joint venture\nbetween BeOpen and Licensee.  This License Agreement does not grant\npermission to use BeOpen trademarks or trade names in a trademark\nsense to endorse or promote products or services of Licensee, or any\nthird party.  As an exception, the \"BeOpen Python\" logos available at\nhttp://www.pythonlabs.com/logos.html may be used according to the\npermissions granted on that web page.\n\n7. By copying, installing or otherwise using the software, Licensee\nagrees to be bound by the terms and conditions of this License\nAgreement.\n\n\nCNRI LICENSE AGREEMENT FOR PYTHON 1.6.1\n---------------------------------------\n\n1. This LICENSE AGREEMENT is between the Corporation for National\nResearch Initiatives, having an office at 1895 Preston White Drive,\nReston, VA 20191 (\"CNRI\"), and the Individual or Organization\n(\"Licensee\") accessing and otherwise using Python 1.6.1 software in\nsource or binary form and its associated documentation.\n\n2. Subject to the terms and conditions of this License Agreement, CNRI\nhereby grants Licensee a nonexclusive, royalty-free, world-wide\nlicense to reproduce, analyze, test, perform and/or display publicly,\nprepare derivative works, distribute, and otherwise use Python 1.6.1\nalone or in any derivative version, provided, however, that CNRI's\nLicense Agreement and CNRI's notice of copyright, i.e., \"Copyright (c)\n1995-2001 Corporation for National Research Initiatives; All Rights\nReserved\" are retained in Python 1.6.1 alone or in any derivative\nversion prepared by Licensee.  Alternately, in lieu of CNRI's License\nAgreement, Licensee may substitute the following text (omitting the\nquotes): \"Python 1.6.1 is made available subject to the terms and\nconditions in CNRI's License Agreement.  This Agreement together with\nPython 1.6.1 may be located on the Internet using the following\nunique, persistent identifier (known as a handle): 1895.22/1013.  This\nAgreement may also be obtained from a proxy server on the Internet\nusing the following URL: http://hdl.handle.net/1895.22/1013\".\n\n3. In the event Licensee prepares a derivative work that is based on\nor incorporates Python 1.6.1 or any part thereof, and wants to make\nthe derivative work available to others as provided herein, then\nLicensee hereby agrees to include in any such work a brief summary of\nthe changes made to Python 1.6.1.\n\n4. CNRI is making Python 1.6.1 available to Licensee on an \"AS IS\"\nbasis.  CNRI MAKES NO REPRESENTATIONS OR WARRANTIES, EXPRESS OR\nIMPLIED.  BY WAY OF EXAMPLE, BUT NOT LIMITATION, CNRI MAKES NO AND\nDISCLAIMS ANY REPRESENTATION OR WARRANTY OF MERCHANTABILITY OR FITNESS\nFOR ANY PARTICULAR PURPOSE OR THAT THE USE OF PYTHON 1.6.1 WILL NOT\nINFRINGE ANY THIRD PARTY RIGHTS.\n\n5. CNRI SHALL NOT BE LIABLE TO LICENSEE OR ANY OTHER USERS OF PYTHON\n1.6.1 FOR ANY INCIDENTAL, SPECIAL, OR CONSEQUENTIAL DAMAGES OR LOSS AS\nA RESULT OF MODIFYING, DISTRIBUTING, OR OTHERWISE USING PYTHON 1.6.1,\nOR ANY DERIVATIVE THEREOF, EVEN IF ADVISED OF THE POSSIBILITY THEREOF.\n\n6. This License Agreement will automatically terminate upon a material\nbreach of its terms and conditions.\n\n7. This License Agreement shall be governed by the federal\nintellectual property law of the United States, including without\nlimitation the federal copyright law, and, to the extent such\nU.S. federal law does not apply, by the law of the Commonwealth of\nVirginia, excluding Virginia's conflict of law provisions.\nNotwithstanding the foregoing, with regard to derivative works based\non Python 1.6.1 that incorporate non-separable material that was\npreviously distributed under the GNU General Public License (GPL), the\nlaw of the Commonwealth of Virginia shall govern this License\nAgreement only as to issues arising under or with respect to\nParagraphs 4, 5, and 7 of this License Agreement.  Nothing in this\nLicense Agreement shall be deemed to create any relationship of\nagency, partnership, or joint venture between CNRI and Licensee.  This\nLicense Agreement does not grant permission to use CNRI trademarks or\ntrade name in a trademark sense to endorse or promote products or\nservices of Licensee, or any third party.\n\n8. By clicking on the \"ACCEPT\" button where indicated, or by copying,\ninstalling or otherwise using Python 1.6.1, Licensee agrees to be\nbound by the terms and conditions of this License Agreement.\n\n        ACCEPT\n\n\nCWI LICENSE AGREEMENT FOR PYTHON 0.9.0 THROUGH 1.2\n--------------------------------------------------\n\nCopyright (c) 1991 - 1995, Stichting Mathematisch Centrum Amsterdam,\nThe Netherlands.  All rights reserved.\n\nPermission to use, copy, modify, and distribute this software and its\ndocumentation for any purpose and without fee is hereby granted,\nprovided that the above copyright notice appear in all copies and that\nboth that copyright notice and this permission notice appear in\nsupporting documentation, and that the name of Stichting Mathematisch\nCentrum or CWI not be used in advertising or publicity pertaining to\ndistribution of the software without specific, written prior\npermission.\n\nSTICHTING MATHEMATISCH CENTRUM DISCLAIMS ALL WARRANTIES WITH REGARD TO\nTHIS SOFTWARE, INCLUDING ALL IMPLIED WARRANTIES OF MERCHANTABILITY AND\nFITNESS, IN NO EVENT SHALL STICHTING MATHEMATISCH CENTRUM BE LIABLE\nFOR ANY SPECIAL, INDIRECT OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES\nWHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN AN\nACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING OUT\nOF OR IN CONNECTION WITH THE USE OR PERFORMANCE OF THIS SOFTWARE.\n\n"
        },
        {
          "name": "NOTICE",
          "type": "blob",
          "size": 0.314453125,
          "content": "Apache Beam\nCopyright 2016-2018 The Apache Software Foundation\n\nThis product includes software developed at\nThe Apache Software Foundation (http://www.apache.org/).\n\nBased on source code originally developed by\nGoogle (http://www.google.com/).\n\nThis product includes software developed at\nGoogle (http://www.google.com/).\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 8.5439453125,
          "content": "<!--\n    Licensed to the Apache Software Foundation (ASF) under one\n    or more contributor license agreements.  See the NOTICE file\n    distributed with this work for additional information\n    regarding copyright ownership.  The ASF licenses this file\n    to you under the Apache License, Version 2.0 (the\n    \"License\"); you may not use this file except in compliance\n    with the License.  You may obtain a copy of the License at\n\n      http://www.apache.org/licenses/LICENSE-2.0\n\n    Unless required by applicable law or agreed to in writing,\n    software distributed under the License is distributed on an\n    \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n    KIND, either express or implied.  See the License for the\n    specific language governing permissions and limitations\n    under the License.\n-->\n\n# Apache Beam\n\n[Apache Beam](http://beam.apache.org/) is a unified model for defining both batch and streaming data-parallel processing pipelines, as well as a set of language-specific SDKs for constructing pipelines and Runners for executing them on distributed processing backends, including [Apache Flink](http://flink.apache.org/), [Apache Spark](http://spark.apache.org/), [Google Cloud Dataflow](http://cloud.google.com/dataflow/), and [Hazelcast Jet](https://jet.hazelcast.org/).\n\n## Status\n\n[![Maven Version](https://maven-badges.herokuapp.com/maven-central/org.apache.beam/beam-sdks-java-core/badge.svg)](http://search.maven.org/#search|gav|1|g:\"org.apache.beam\")\n[![PyPI version](https://badge.fury.io/py/apache-beam.svg)](https://badge.fury.io/py/apache-beam)\n[![Go version](https://pkg.go.dev/badge/github.com/apache/beam/sdks/v2/go.svg)](https://pkg.go.dev/github.com/apache/beam/sdks/v2/go)\n[![Python coverage](https://codecov.io/gh/apache/beam/branch/master/graph/badge.svg)](https://codecov.io/gh/apache/beam)\n[![Build python source distribution and wheels](https://github.com/apache/beam/workflows/Build%20python%20source%20distribution%20and%20wheels/badge.svg?branch=master&event=schedule)](https://github.com/apache/beam/actions?query=workflow%3A%22Build+python+source+distribution+and+wheels%22+branch%3Amaster+event%3Aschedule)\n[![Python tests](https://github.com/apache/beam/workflows/Python%20tests/badge.svg?branch=master&event=schedule)](https://github.com/apache/beam/actions?query=workflow%3A%22Python+Tests%22+branch%3Amaster+event%3Aschedule)\n[![Java tests](https://github.com/apache/beam/workflows/Java%20Tests/badge.svg?branch=master&event=schedule)](https://github.com/apache/beam/actions?query=workflow%3A%22Java+Tests%22+branch%3Amaster+event%3Aschedule)\n\n## Overview\n\nBeam provides a general approach to expressing [embarrassingly parallel](https://en.wikipedia.org/wiki/Embarrassingly_parallel) data processing pipelines and supports three categories of users, each of which have relatively disparate backgrounds and needs.\n\n1. _End Users_: Writing pipelines with an existing SDK, running it on an existing runner. These users want to focus on writing their application logic and have everything else just work.\n2. _SDK Writers_: Developing a Beam SDK targeted at a specific user community (Java, Python, Scala, Go, R, graphical, etc). These users are language geeks and would prefer to be shielded from all the details of various runners and their implementations.\n3. _Runner Writers_: Have an execution environment for distributed processing and would like to support programs written against the Beam Model. Would prefer to be shielded from details of multiple SDKs.\n\n### The Beam Model\n\nThe model behind Beam evolved from several internal Google data processing projects, including [MapReduce](http://research.google.com/archive/mapreduce.html), [FlumeJava](http://research.google.com/pubs/pub35650.html), and [Millwheel](http://research.google.com/pubs/pub41378.html). This model was originally known as the “[Dataflow Model](http://www.vldb.org/pvldb/vol8/p1792-Akidau.pdf)”.\n\nTo learn more about the Beam Model (though still under the original name of Dataflow), see the World Beyond Batch: [Streaming 101](https://www.oreilly.com/ideas/the-world-beyond-batch-streaming-101) and [Streaming 102](https://www.oreilly.com/ideas/the-world-beyond-batch-streaming-102) posts on O’Reilly’s Radar site, and the [VLDB 2015 paper](http://www.vldb.org/pvldb/vol8/p1792-Akidau.pdf).\n\nThe key concepts in the Beam programming model are:\n\n* `PCollection`: represents a collection of data, which could be bounded or unbounded in size.\n* `PTransform`: represents a computation that transforms input PCollections into output PCollections.\n* `Pipeline`: manages a directed acyclic graph of PTransforms and PCollections that is ready for execution.\n* `PipelineRunner`: specifies where and how the pipeline should execute.\n\n### SDKs\n\nBeam supports multiple language-specific SDKs for writing pipelines against the Beam Model.\n\nCurrently, this repository contains SDKs for Java, Python and Go.\n\nHave ideas for new SDKs or DSLs? See the [sdk-ideas label](https://github.com/apache/beam/issues?q=is%3Aopen+is%3Aissue+label%3Asdk-ideas).\n\n### Runners\n\nBeam supports executing programs on multiple distributed processing backends through PipelineRunners. Currently, the following PipelineRunners are available:\n\n- The `DirectRunner` runs the pipeline on your local machine.\n- The `PrismRunner` runs the pipeline on your local machine using Beam Portability.\n- The `DataflowRunner` submits the pipeline to the [Google Cloud Dataflow](http://cloud.google.com/dataflow/).\n- The `FlinkRunner` runs the pipeline on an Apache Flink cluster. The code has been donated from [dataArtisans/flink-dataflow](https://github.com/dataArtisans/flink-dataflow) and is now part of Beam.\n- The `SparkRunner` runs the pipeline on an Apache Spark cluster.\n- The `JetRunner` runs the pipeline on a Hazelcast Jet cluster. The code has been donated from [hazelcast/hazelcast-jet](https://github.com/hazelcast/hazelcast-jet) and is now part of Beam.\n- The `Twister2Runner` runs the pipeline on a Twister2 cluster. The code has been donated from [DSC-SPIDAL/twister2](https://github.com/DSC-SPIDAL/twister2) and is now part of Beam.\n\nHave ideas for new Runners? See the [runner-ideas label](https://github.com/apache/beam/issues?q=is%3Aopen+is%3Aissue+label%3Arunner-ideas).\n\n\nInstructions for building and testing Beam itself\nare in the [contribution guide](./CONTRIBUTING.md).\n\n## 📚 Learn More\n\nHere are some resources actively maintained by the Beam community to help you get started:\n<table>\n<thead>\n  <tr>\n      <th><b>Resource</b></th>\n      <th><b>Details</b></th>\n  </tr>\n</thead>\n<tbody>\n  <tr>\n    <td><a href=\"https://beam.apache.org\" target=\"_blank\" rel=\"noopener noreferrer\">Apache Beam Website</a></td>\n    <td>Our website discussing the project, and it's specifics.</td>\n  </tr>\n  <tr>\n    <td><a href=\"https://beam.apache.org/get-started/quickstart-java\" target=\"_blank\" rel=\"noopener noreferrer\">Java Quickstart</a></td>\n    <td>A guide to getting started with the Java SDK.</td>\n  </tr>\n  <tr>\n    <td><a href=\"https://beam.apache.org/get-started/quickstart-py\" target=\"_blank\" rel=\"noopener noreferrer\">Python Quickstart</a></td>\n    <td>A guide to getting started with the Python SDK.</td>\n  </tr>\n  <tr>\n    <td><a href=\"https://beam.apache.org/get-started/quickstart-go\" target=\"_blank\" rel=\"noopener noreferrer\">Go Quickstart </a></td>\n    <td>A guide to getting started with the Go SDK.</td>\n  </tr>\n  <tr>\n    <td><a href=\"https://tour.beam.apache.org/\" target=\"_blank\" rel=\"noopener noreferrer\">Tour of Beam </a></td>\n    <td>A comprehensive, interactive learning experience covering Beam concepts in depth.</td>\n  </tr>\n  <tr>\n    <td><a href=\"https://www.cloudskillsboost.google/course_templates/724\" target=\"_blank\" rel=\"noopener noreferrer\">Beam Quest </a></td>\n    <td>A certification granted by Google Cloud, certifying proficiency in Beam.</td>\n  </tr>\n  <tr>\n    <td><a href=\"https://s.apache.org/beam-community-metrics\" target=\"_blank\" rel=\"noopener noreferrer\">Community Metrics </a></td>\n    <td>Beam's Git Community Metrics.</td>\n  </tr>\n</tbody>\n</table>\n\n## Contact Us\n\nTo get involved with Apache Beam:\n\n* [Subscribe to](https://beam.apache.org/community/contact-us/#:~:text=Subscribe%20and%20Unsubscribe) or e-mail the [user@beam.apache.org](http://mail-archives.apache.org/mod_mbox/beam-user/) list.\n* [Subscribe to](https://beam.apache.org/community/contact-us/#:~:text=Subscribe%20and%20Unsubscribe) or e-mail the [dev@beam.apache.org](http://mail-archives.apache.org/mod_mbox/beam-dev/) list.\n* [Join ASF Slack](https://s.apache.org/slack-invite) on [#beam channel](https://s.apache.org/beam-slack-channel)\n* [Report an issue](https://github.com/apache/beam/issues/new/choose).\n"
        },
        {
          "name": "assembly.xml",
          "type": "blob",
          "size": 6.3564453125,
          "content": "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<!--\n    Licensed to the Apache Software Foundation (ASF) under one or more\n    contributor license agreements.  See the NOTICE file distributed with\n    this work for additional information regarding copyright ownership.\n    The ASF licenses this file to You under the Apache License, Version 2.0\n    (the \"License\"); you may not use this file except in compliance with\n    the License.  You may obtain a copy of the License at\n       http://www.apache.org/licenses/LICENSE-2.0\n    Unless required by applicable law or agreed to in writing, software\n    distributed under the License is distributed on an \"AS IS\" BASIS,\n    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n    See the License for the specific language governing permissions and\n    limitations under the License.\n-->\n\n<!-- Please refer to https://github.com/apache/maven-resources/blob/trunk/apache-source-release-assembly-descriptor/src/main/resources/assemblies/source-shared.xml before making changes to this file. -->\n\n<assembly xmlns=\"http://maven.apache.org/plugins/maven-assembly-plugin/assembly/1.1.3\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n          xsi:schemaLocation=\"http://maven.apache.org/plugins/maven-assembly-plugin/assembly/1.1.3 http://maven.apache.org/xsd/assembly-1.1.3.xsd\">\n\n    <id>source</id>\n\n    <formats>\n        <format>zip</format>\n    </formats>\n\n    <fileSets>\n        <!-- main project directory structure -->\n        <fileSet>\n            <directory>.</directory>\n            <outputDirectory></outputDirectory>\n            <useDefaultExcludes>true</useDefaultExcludes>\n            <excludes>\n                <!-- build output -->\n                <exclude>%regex[(?!((?!${project.build.directory}/)[^/]+/)*src/).*${project.build.directory}.*]</exclude>\n\n                <!-- NOTE: Most of the following excludes should not be required\n                     if the standard release process is followed. This is because the\n                     release plugin checks out project sources into a location like\n                     target/checkout, then runs the build from there. The result is\n                     a source-release archive that comes from a pretty clean directory\n                     structure.\n\n                     HOWEVER, if the release plugin is configured to run extra goals\n                     or generate a project website, it's definitely possible that some\n                     of these files will be present. So, it's safer to exclude them.\n                -->\n\n                <!-- IDEs -->\n                <exclude>%regex[(?!((?!${project.build.directory}/)[^/]+/)*src/)(.*/)?maven-eclipse\\.xml]</exclude>\n                <exclude>%regex[(?!((?!${project.build.directory}/)[^/]+/)*src/)(.*/)?\\.project]</exclude>\n                <exclude>%regex[(?!((?!${project.build.directory}/)[^/]+/)*src/)(.*/)?\\.classpath]</exclude>\n                <exclude>%regex[(?!((?!${project.build.directory}/)[^/]+/)*src/)(.*/)?[^/]*\\.iws]</exclude>\n                <exclude>%regex[(?!((?!${project.build.directory}/)[^/]+/)*src/)(.*/)?\\.idea(/.*)?]</exclude>\n                <exclude>%regex[(?!((?!${project.build.directory}/)[^/]+/)*src/)(.*/)?out(/.*)?]</exclude>\n                <exclude>%regex[(?!((?!${project.build.directory}/)[^/]+/)*src/)(.*/)?[^/]*\\.ipr]</exclude>\n                <exclude>%regex[(?!((?!${project.build.directory}/)[^/]+/)*src/)(.*/)?[^/]*\\.iml]</exclude>\n                <exclude>%regex[(?!((?!${project.build.directory}/)[^/]+/)*src/)(.*/)?\\.settings(/.*)?]</exclude>\n                <exclude>%regex[(?!((?!${project.build.directory}/)[^/]+/)*src/)(.*/)?\\.externalToolBuilders(/.*)?]</exclude>\n                <exclude>%regex[(?!((?!${project.build.directory}/)[^/]+/)*src/)(.*/)?\\.deployables(/.*)?]</exclude>\n                <exclude>%regex[(?!((?!${project.build.directory}/)[^/]+/)*src/)(.*/)?\\.wtpmodules(/.*)?]</exclude>\n\n                <!-- misc -->\n                <exclude>%regex[(?!((?!${project.build.directory}/)[^/]+/)*src/)(.*/)?cobertura\\.ser]</exclude>\n\n                <!-- files generated by the Gradle build process. -->\n                <exclude>**/.gradle/**</exclude>\n                <exclude>**/.gogradle/**</exclude>\n                <exclude>**/build/**</exclude>\n                <exclude>**/vendor/**</exclude>\n                <exclude>**/out/**</exclude>\n                <exclude>**/.gradletasknamecache</exclude>\n\n                <!-- files generated by the Maven build process. -->\n                <exclude>**/bin/**</exclude>\n                <exclude>**/dependency-reduced-pom.xml</exclude>\n\n                <!-- file generated for archetypes -->\n                <exclude>sdks/java/maven-archetypes/examples/src/main/resources/archetype-resources/src/</exclude>\n                <exclude>sdks/java/maven-archetypes/examples-java8/src/main/resources/archetype-resources/src/</exclude>\n\n                <!-- files generated by the Python build process. -->\n                <exclude>**/*.py[cod]</exclude>\n                <exclude>**/*.egg-info/</exclude>\n                <exclude>**/.eggs/**</exclude>\n                <exclude>**/nose-*.egg/**</exclude>\n                <exclude>**/.tox/**</exclude>\n                <exclude>**/dist/**</exclude>\n                <exclude>**/distribute-*/**</exclude>\n                <exclude>**/env/**</exclude>\n                <exclude>sdks/python/**/*.c</exclude>\n                <exclude>sdks/python/**/*.so</exclude>\n                <exclude>sdks/python/**/*.eggs/**</exclude>\n                <exclude>sdks/python/LICENSE</exclude>\n                <exclude>sdks/python/NOTICE</exclude>\n                <exclude>sdks/python/README.md</exclude>\n                <exclude>sdks/python/apache_beam/portability/api/*pb2*.*</exclude>\n\n                <!-- release-plugin temp files -->\n                <exclude>%regex[(?!((?!${project.build.directory}/)[^/]+/)*src/)(.*/)?pom\\.xml\\.releaseBackup]</exclude>\n                <exclude>%regex[(?!((?!${project.build.directory}/)[^/]+/)*src/)(.*/)?release\\.properties]</exclude>\n\n                <!-- code review related -->\n                <exclude>**/OWNERS</exclude>\n            </excludes>\n        </fileSet>\n        <!-- license, readme, etc. calculated at build time -->\n        <fileSet>\n            <directory>${project.build.directory}/maven-shared-archive-resources/META-INF</directory>\n            <outputDirectory></outputDirectory>\n        </fileSet>\n    </fileSets>\n\n</assembly>\n"
        },
        {
          "name": "build.gradle.kts",
          "type": "blob",
          "size": 28.216796875,
          "content": "/*\n * Licensed to the Apache Software Foundation (ASF) under one\n * or more contributor license agreements.  See the NOTICE file\n * distributed with this work for additional information\n * regarding copyright ownership.  The ASF licenses this file\n * to you under the Apache License, Version 2.0 (the\n * License); you may not use this file except in compliance\n * with the License.  You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an AS IS BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\nplugins {\n  base\n  // Apply one top level rat plugin to perform any required license enforcement analysis\n  id(\"org.nosphere.apache.rat\") version \"0.8.1\"\n  // Enable gradle-based release management\n  id(\"net.researchgate.release\") version \"2.8.1\"\n  id(\"org.apache.beam.module\")\n  id(\"org.sonarqube\") version \"3.0\"\n}\n\n/*************************************************************************************************/\n// Configure the root project\n\ntasks.rat {\n  // Set input directory to that of the root project instead of the CWD. This\n  // makes .gitignore rules (added below) work properly.\n  inputDir.set(project.rootDir)\n\n  val exclusions = mutableListOf(\n    // Ignore files we track but do not distribute\n    \"**/.github/**/*\",\n    \"**/.gitkeep\",\n    \"gradlew\",\n    \"gradlew.bat\",\n    \"gradle/wrapper/gradle-wrapper.properties\",\n\n    \"**/package-list\",\n    \"**/test.avsc\",\n    \"**/user.avsc\",\n    \"**/test/resources/**/*.txt\",\n    \"**/test/resources/**/*.csv\",\n    \"**/test/**/.placeholder\",\n\n    // Default eclipse excludes neglect subprojects\n\n    // Proto/grpc generated wrappers\n    \"**/apache_beam/portability/api/**/*_pb2*.py\",\n    \"**/go/pkg/beam/**/*.pb.go\",\n    \"**/mock-apis/**/*.pb.go\",\n\n    // Ignore go.sum files, which don't permit headers\n    \"**/go.sum\",\n\n    // Ignore Go test data files\n    \"**/go/data/**\",\n\n    // VCF test files\n    \"**/apache_beam/testing/data/vcf/*\",\n\n    // JDBC package config files\n    \"**/META-INF/services/java.sql.Driver\",\n\n    // Website build files\n    \"**/Gemfile.lock\",\n    \"**/Rakefile\",\n    \"**/.htaccess\",\n    \"website/www/site/assets/scss/_bootstrap.scss\",\n    \"website/www/site/assets/scss/bootstrap/**/*\",\n    \"website/www/site/assets/js/**/*\",\n    \"website/www/site/static/images/mascot/*.ai\",\n    \"website/www/site/static/js/bootstrap*.js\",\n    \"website/www/site/static/js/bootstrap/**/*\",\n    \"website/www/site/themes\",\n    \"website/www/yarn.lock\",\n    \"website/www/package.json\",\n    \"website/www/site/static/js/hero/lottie-light.min.js\",\n    \"website/www/site/static/js/keen-slider.min.js\",\n    \"website/www/site/assets/scss/_keen-slider.scss\",\n\n    // Release automation files\n    \"release/src/main/scripts/*.txt\",\n\n    // Ignore ownership files\n    \"ownership/**/*\",\n    \"**/OWNERS\",\n\n    // Ignore CPython LICENSE file\n    \"LICENSE.python\",\n\n    // Json doesn't support comments.\n    \"**/*.json\",\n\n    // Katas files\n    \"learning/katas/**/course-info.yaml\",\n    \"learning/katas/**/task-info.yaml\",\n    \"learning/katas/**/course-remote-info.yaml\",\n    \"learning/katas/**/section-remote-info.yaml\",\n    \"learning/katas/**/lesson-remote-info.yaml\",\n    \"learning/katas/**/task-remote-info.yaml\",\n    \"learning/katas/**/*.txt\",\n\n    // Tour Of Beam learning-content metadata and its samples\n    \"learning/tour-of-beam/**/content-info.yaml\",\n    \"learning/tour-of-beam/**/module-info.yaml\",\n    \"learning/tour-of-beam/**/group-info.yaml\",\n    \"learning/tour-of-beam/**/unit-info.yaml\",\n    \"learning/tour-of-beam/backend/samples/**/*.md\",\n\n    // Tour Of Beam example logs\n    \"learning/tour-of-beam/learning-content/**/*.log\",\n\n    // Tour Of Beam example txt files\n    \"learning/tour-of-beam/learning-content/**/*.txt\",\n\n    // Tour Of Beam example csv files\n    \"learning/tour-of-beam/learning-content/**/*.csv\",\n\n    // Tour Of Beam backend autogenerated Datastore indexes\n    \"learning/tour-of-beam/backend/internal/storage/index.yaml\",\n\n    // Tour Of Beam backend autogenerated Playground GRPC API stubs and mocks\n    \"learning/tour-of-beam/backend/playground_api/api/v1/api.pb.go\",\n    \"learning/tour-of-beam/backend/playground_api/api/v1/api_grpc.pb.go\",\n    \"learning/tour-of-beam/backend/playground_api/api/v1/mock.go\",\n\n    // Playground backend autogenerated GRPC API stubs and mocks\n    \"playground/backend/internal/api/v1/api.pb.go\",\n    \"playground/backend/internal/api/v1/api_grpc.pb.go\",\n\n    // Playground infrastructure autogenerated GRPC API stubs and mocks\n    \"playground/infrastructure/api/v1/api_pb2.py\",\n    \"playground/infrastructure/api/v1/api_pb2.pyi\",\n    \"playground/infrastructure/api/v1/api_pb2_grpc.py\",\n\n    // test p8 file for SnowflakeIO\n    \"sdks/java/io/snowflake/src/test/resources/invalid_test_rsa_key.p8\",\n    \"sdks/java/io/snowflake/src/test/resources/valid_encrypted_test_rsa_key.p8\",\n    \"sdks/java/io/snowflake/src/test/resources/valid_unencrypted_test_rsa_key.p8\",\n\n    // Mockito extensions\n    \"sdks/java/io/amazon-web-services2/src/test/resources/mockito-extensions/org.mockito.plugins.MockMaker\",\n    \"sdks/java/io/azure/src/test/resources/mockito-extensions/org.mockito.plugins.MockMaker\",\n    \"sdks/java/extensions/ml/src/test/resources/mockito-extensions/org.mockito.plugins.MockMaker\",\n\n    // JupyterLab extensions\n    \"sdks/python/apache_beam/runners/interactive/extensions/apache-beam-jupyterlab-sidepanel/yarn.lock\",\n\n    // Autogenerated apitools clients.\n    \"sdks/python/apache_beam/runners/dataflow/internal/clients/*/**/*.py\",\n\n    // Sample text file for Java quickstart\n    \"sdks/java/maven-archetypes/examples/sample.txt\",\n\n    // Ignore Flutter autogenerated files for Playground\n    \"playground/frontend/**/*.g.dart\",\n    \"playground/frontend/**/*.g.yaml\",\n    \"playground/frontend/**/*.gen.dart\",\n    \"playground/frontend/**/*.golden.yaml\",\n    \"playground/frontend/**/*.mocks.dart\",\n    \"playground/frontend/.metadata\",\n    \"playground/frontend/pubspec.lock\",\n\n    // Ignore Flutter autogenerated files for Playground Components\n    \"playground/frontend/**/*.pb.dart\",\n    \"playground/frontend/**/*.pbenum.dart\",\n    \"playground/frontend/**/*.pbgrpc.dart\",\n    \"playground/frontend/**/*.pbjson.dart\",\n    \"playground/frontend/playground_components/.metadata\",\n    \"playground/frontend/playground_components/pubspec.lock\",\n\n    // Ignore Flutter autogenerated files for Tour of Beam\n    \"learning/tour-of-beam/frontend/**/*.g.dart\",\n    \"learning/tour-of-beam/frontend/**/*.gen.dart\",\n    \"learning/tour-of-beam/frontend/.metadata\",\n    \"learning/tour-of-beam/frontend/pubspec.lock\",\n    \"learning/tour-of-beam/frontend/lib/firebase_options.dart\",\n\n    // Ignore .gitkeep file\n    \"**/.gitkeep\",\n\n    // Ignore Flutter localization .arb files (doesn't support comments)\n    \"playground/frontend/lib/l10n/**/*.arb\",\n\n    // Ignore LICENSES copied onto containers\n    \"sdks/java/container/license_scripts/manual_licenses\",\n    \"sdks/python/container/license_scripts/manual_licenses\",\n\n    // Ignore autogenrated proto files.\n    \"sdks/typescript/src/apache_beam/proto/**/*.ts\",\n\n    // Ignore typesciript package management.\n    \"sdks/typescript/package-lock.json\",\n    \"sdks/typescript/node_modules/**/*\",\n\n    // Ignore buf autogenerated files.\n    \"**/buf.lock\",\n\n    // Ignore poetry autogenerated files.\n    \"**/poetry.lock\",\n\n    // DuetAI training prompts\n    \"learning/prompts/**/*.md\",\n\n    // Ignore terraform lock files\n    \"**/.terraform.lock.hcl\"\n  )\n\n  // Add .gitignore excludes to the Apache Rat exclusion list. We re-create the behavior\n  // of the Apache Maven Rat plugin since the Apache Ant Rat plugin doesn't do this\n  // automatically.\n  val gitIgnore = project(\":\").file(\".gitignore\")\n  if (gitIgnore.exists()) {\n    val gitIgnoreExcludes = gitIgnore.readLines().filter { it.isNotEmpty() && !it.startsWith(\"#\") }\n    exclusions.addAll(gitIgnoreExcludes)\n  }\n\n  verbose.set(true)\n  failOnError.set(true)\n  setExcludes(exclusions)\n}\ntasks.check.get().dependsOn(tasks.rat)\n\n// Define root pre/post commit tasks simplifying what is needed\n// to be specified on the commandline when executing locally.\n// This indirection also makes Jenkins use the branch of the PR\n// for the test definitions.\ntasks.register(\"javaPreCommit\") {\n  // We need to list the model/* builds since sdks/java/core doesn't\n  // depend on any of the model.\n  dependsOn(\":model:pipeline:build\")\n  dependsOn(\":model:job-management:build\")\n  dependsOn(\":model:fn-execution:build\")\n  dependsOn(\":sdks:java:core:buildNeeded\")\n\n  // Inline :sdks:java:core:buildDependents so we can carve out pieces at a time\n  dependsOn(\":beam-validate-runner:build\")\n  dependsOn(\":examples:java:build\")\n  dependsOn(\":examples:java:preCommit\")\n  dependsOn(\":examples:java:twitter:build\")\n  dependsOn(\":examples:java:twitter:preCommit\")\n  dependsOn(\":examples:multi-language:build\")\n  dependsOn(\":model:fn-execution:build\")\n  dependsOn(\":model:job-management:build\")\n  dependsOn(\":model:pipeline:build\")\n  dependsOn(\":runners:core-java:build\")\n  dependsOn(\":runners:direct-java:build\")\n  dependsOn(\":runners:direct-java:needsRunnerTests\")\n  dependsOn(\":runners:extensions-java:metrics:build\")\n  // lowest supported flink version\n  var flinkVersions = project.ext.get(\"allFlinkVersions\") as Array<*>\n  dependsOn(\":runners:flink:${flinkVersions[0]}:build\")\n  dependsOn(\":runners:flink:${flinkVersions[0]}:job-server:build\")\n  dependsOn(\":runners:google-cloud-dataflow-java:build\")\n  dependsOn(\":runners:google-cloud-dataflow-java:examples-streaming:build\")\n  dependsOn(\":runners:google-cloud-dataflow-java:examples:build\")\n  dependsOn(\":runners:google-cloud-dataflow-java:worker:build\")\n  dependsOn(\":runners:google-cloud-dataflow-java:worker:windmill:build\")\n  dependsOn(\":runners:java-fn-execution:build\")\n  dependsOn(\":runners:java-job-service:build\")\n  dependsOn(\":runners:jet:build\")\n  dependsOn(\":runners:local-java:build\")\n  dependsOn(\":runners:portability:java:build\")\n  dependsOn(\":runners:prism:java:build\")\n  dependsOn(\":runners:samza:build\")\n  dependsOn(\":runners:samza:job-server:build\")\n  dependsOn(\":runners:spark:3:build\")\n  dependsOn(\":runners:spark:3:job-server:build\")\n  dependsOn(\":runners:twister2:build\")\n  dependsOn(\":sdks:java:build-tools:build\")\n  dependsOn(\":sdks:java:container:java8:docker\")\n  dependsOn(\":sdks:java:core:build\")\n  dependsOn(\":sdks:java:core:jmh:build\")\n  dependsOn(\":sdks:java:expansion-service:build\")\n  dependsOn(\":sdks:java:expansion-service:app:build\")\n  dependsOn(\":sdks:java:extensions:arrow:build\")\n  dependsOn(\":sdks:java:extensions:avro:build\")\n  dependsOn(\":sdks:java:extensions:combiners:build\")\n  dependsOn(\":sdks:java:extensions:euphoria:build\")\n  dependsOn(\":sdks:java:extensions:google-cloud-platform-core:build\")\n  dependsOn(\":sdks:java:extensions:jackson:build\")\n  dependsOn(\":sdks:java:extensions:join-library:build\")\n  dependsOn(\":sdks:java:extensions:kryo:build\")\n  dependsOn(\":sdks:java:extensions:ml:build\")\n  dependsOn(\":sdks:java:extensions:protobuf:build\")\n  dependsOn(\":sdks:java:extensions:python:build\")\n  dependsOn(\":sdks:java:extensions:sbe:build\")\n  dependsOn(\":sdks:java:extensions:schemaio-expansion-service:build\")\n  dependsOn(\":sdks:java:extensions:sketching:build\")\n  dependsOn(\":sdks:java:extensions:sorter:build\")\n  dependsOn(\":sdks:java:extensions:timeseries:build\")\n  dependsOn(\":sdks:java:extensions:zetasketch:build\")\n  dependsOn(\":sdks:java:harness:build\")\n  dependsOn(\":sdks:java:harness:jmh:build\")\n  dependsOn(\":sdks:java:io:bigquery-io-perf-tests:build\")\n  dependsOn(\":sdks:java:io:common:build\")\n  dependsOn(\":sdks:java:io:contextualtextio:build\")\n  dependsOn(\":sdks:java:io:expansion-service:build\")\n  dependsOn(\":sdks:java:io:file-based-io-tests:build\")\n  dependsOn(\":sdks:java:io:sparkreceiver:2:build\")\n  dependsOn(\":sdks:java:io:synthetic:build\")\n  dependsOn(\":sdks:java:io:xml:build\")\n  dependsOn(\":sdks:java:javadoc:allJavadoc\")\n  dependsOn(\":sdks:java:managed:build\")\n  dependsOn(\":sdks:java:testing:expansion-service:build\")\n  dependsOn(\":sdks:java:testing:jpms-tests:build\")\n  dependsOn(\":sdks:java:testing:load-tests:build\")\n  dependsOn(\":sdks:java:testing:nexmark:build\")\n  dependsOn(\":sdks:java:testing:test-utils:build\")\n  dependsOn(\":sdks:java:testing:tpcds:build\")\n  dependsOn(\":sdks:java:testing:watermarks:build\")\n  dependsOn(\":sdks:java:transform-service:build\")\n  dependsOn(\":sdks:java:transform-service:app:build\")\n  dependsOn(\":sdks:java:transform-service:launcher:build\")\n}\n\n// a precommit task build multiple IOs (except those splitting into single jobs)\ntasks.register(\"javaioPreCommit\") {\n  dependsOn(\":sdks:java:io:amqp:build\")\n  dependsOn(\":sdks:java:io:cassandra:build\")\n  dependsOn(\":sdks:java:io:csv:build\")\n  dependsOn(\":sdks:java:io:cdap:build\")\n  dependsOn(\":sdks:java:io:clickhouse:build\")\n  dependsOn(\":sdks:java:io:debezium:expansion-service:build\")\n  dependsOn(\":sdks:java:io:debezium:build\")\n  dependsOn(\":sdks:java:io:elasticsearch:build\")\n  dependsOn(\":sdks:java:io:file-schema-transform:build\")\n  dependsOn(\":sdks:java:io:google-ads:build\")\n  dependsOn(\":sdks:java:io:hbase:build\")\n  dependsOn(\":sdks:java:io:hcatalog:build\")\n  dependsOn(\":sdks:java:io:influxdb:build\")\n  dependsOn(\":sdks:java:io:jdbc:build\")\n  dependsOn(\":sdks:java:io:jms:build\")\n  dependsOn(\":sdks:java:io:kafka:build\")\n  dependsOn(\":sdks:java:io:kafka:upgrade:build\")\n  dependsOn(\":sdks:java:io:kudu:build\")\n  dependsOn(\":sdks:java:io:mongodb:build\")\n  dependsOn(\":sdks:java:io:mqtt:build\")\n  dependsOn(\":sdks:java:io:neo4j:build\")\n  dependsOn(\":sdks:java:io:parquet:build\")\n  dependsOn(\":sdks:java:io:rabbitmq:build\")\n  dependsOn(\":sdks:java:io:redis:build\")\n  dependsOn(\":sdks:java:io:rrio:build\")\n  dependsOn(\":sdks:java:io:singlestore:build\")\n  dependsOn(\":sdks:java:io:solr:build\")\n  dependsOn(\":sdks:java:io:splunk:build\")\n  dependsOn(\":sdks:java:io:thrift:build\")\n  dependsOn(\":sdks:java:io:tika:build\")\n}\n\n// a precommit task testing additional supported flink versions not covered by\n// the main Java PreCommit (lowest supported version)\ntasks.register(\"flinkPreCommit\") {\n  var flinkVersions = project.ext.get(\"allFlinkVersions\") as Array<*>\n  for (version in flinkVersions.slice(1..flinkVersions.size - 1)) {\n    dependsOn(\":runners:flink:${version}:build\")\n    dependsOn(\":runners:flink:${version}:job-server:build\")\n  }\n}\n\ntasks.register(\"sqlPreCommit\") {\n  dependsOn(\":sdks:java:extensions:sql:preCommit\")\n  dependsOn(\":sdks:java:extensions:sql:buildDependents\")\n  dependsOn(\":sdks:java:extensions:sql:datacatalog:build\")\n  dependsOn(\":sdks:java:extensions:sql:expansion-service:build\")\n  dependsOn(\":sdks:java:extensions:sql:hcatalog:build\")\n  dependsOn(\":sdks:java:extensions:sql:jdbc:build\")\n  dependsOn(\":sdks:java:extensions:sql:jdbc:preCommit\")\n  dependsOn(\":sdks:java:extensions:sql:perf-tests:build\")\n  dependsOn(\":sdks:java:extensions:sql:shell:build\")\n  dependsOn(\":sdks:java:extensions:sql:udf-test-provider:build\")\n  dependsOn(\":sdks:java:extensions:sql:udf:build\")\n  dependsOn(\":sdks:java:extensions:sql:zetasql:build\")\n}\n\ntasks.register(\"javaPreCommitPortabilityApi\") {\n  dependsOn(\":runners:google-cloud-dataflow-java:worker:build\")\n}\n\ntasks.register(\"javaPostCommit\") {\n  dependsOn(\":sdks:java:extensions:google-cloud-platform-core:postCommit\")\n  dependsOn(\":sdks:java:extensions:zetasketch:postCommit\")\n  dependsOn(\":sdks:java:extensions:ml:postCommit\")\n}\n\ntasks.register(\"javaPostCommitSickbay\") {\n  dependsOn(\":runners:samza:validatesRunnerSickbay\")\n  for (version in project.ext.get(\"allFlinkVersions\") as Array<*>) {\n    dependsOn(\":runners:flink:${version}:validatesRunnerSickbay\")\n  }\n  dependsOn(\":runners:spark:3:job-server:validatesRunnerSickbay\")\n  dependsOn(\":runners:direct-java:validatesRunnerSickbay\")\n  dependsOn(\":runners:portability:java:validatesRunnerSickbay\")\n}\n\ntasks.register(\"javaHadoopVersionsTest\") {\n  dependsOn(\":sdks:java:io:hadoop-common:hadoopVersionsTest\")\n  dependsOn(\":sdks:java:io:hadoop-file-system:hadoopVersionsTest\")\n  dependsOn(\":sdks:java:io:hadoop-format:hadoopVersionsTest\")\n  dependsOn(\":sdks:java:io:hcatalog:hadoopVersionsTest\")\n  dependsOn(\":sdks:java:io:iceberg:hadoopVersionsTest\")\n  dependsOn(\":sdks:java:io:parquet:hadoopVersionsTest\")\n  dependsOn(\":sdks:java:extensions:sorter:hadoopVersionsTest\")\n  dependsOn(\":runners:spark:3:hadoopVersionsTest\")\n}\n\ntasks.register(\"javaAvroVersionsTest\") {\n  dependsOn(\":sdks:java:extensions:avro:avroVersionsTest\")\n}\n\ntasks.register(\"sqlPostCommit\") {\n  dependsOn(\":sdks:java:extensions:sql:postCommit\")\n  dependsOn(\":sdks:java:extensions:sql:jdbc:postCommit\")\n  dependsOn(\":sdks:java:extensions:sql:datacatalog:postCommit\")\n  dependsOn(\":sdks:java:extensions:sql:hadoopVersionsTest\")\n}\n\ntasks.register(\"goPreCommit\") {\n  // Ensure the Precommit builds run after the tests, in order to avoid the\n  // flake described in BEAM-11918. This is done by splitting them into two\n  // tasks and using \"mustRunAfter\" to enforce ordering.\n  dependsOn(\":goPrecommitTest\")\n  dependsOn(\":goPrecommitBuild\")\n}\n\ntasks.register(\"goPrecommitTest\") {\n  dependsOn(\":sdks:go:goTest\")\n}\n\ntasks.register(\"goPrecommitBuild\") {\n  mustRunAfter(\":goPrecommitTest\")\n\n  dependsOn(\":sdks:go:goBuild\")\n  dependsOn(\":sdks:go:examples:goBuild\")\n  dependsOn(\":sdks:go:test:goBuild\")\n\n  // Ensure all container Go boot code builds as well.\n  dependsOn(\":sdks:java:container:goBuild\")\n  dependsOn(\":sdks:python:container:goBuild\")\n  dependsOn(\":sdks:go:container:goBuild\")\n}\n\ntasks.register(\"goPortablePreCommit\") {\n  dependsOn(\":sdks:go:test:ulrValidatesRunner\")\n}\n\ntasks.register(\"goPrismPreCommit\") {\n  dependsOn(\":sdks:go:test:prismValidatesRunner\")\n}\n\ntasks.register(\"goPostCommitDataflowARM\") {\n  dependsOn(\":sdks:go:test:dataflowValidatesRunnerARM64\")\n}\n\ntasks.register(\"goPostCommit\") {\n  dependsOn(\":sdks:go:test:dataflowValidatesRunner\")\n}\n\ntasks.register(\"playgroundPreCommit\") {\n  dependsOn(\":playground:lintProto\")\n  dependsOn(\":playground:backend:precommit\")\n  dependsOn(\":playground:frontend:precommit\")\n}\n\ntasks.register(\"pythonPreCommit\") {\n  dependsOn(\":sdks:python:test-suites:tox:pycommon:preCommitPyCommon\")\n  dependsOn(\":sdks:python:test-suites:tox:py39:preCommitPy39\")\n  dependsOn(\":sdks:python:test-suites:tox:py310:preCommitPy310\")\n  dependsOn(\":sdks:python:test-suites:tox:py311:preCommitPy311\")\n  dependsOn(\":sdks:python:test-suites:tox:py312:preCommitPy312\")\n}\n\ntasks.register(\"pythonPreCommitIT\") {\n  dependsOn(\":sdks:python:test-suites:tox:pycommon:preCommitPyCommon\")\n  dependsOn(\":sdks:python:test-suites:dataflow:preCommitIT\")\n}\n\ntasks.register(\"pythonDocsPreCommit\") {\n  dependsOn(\":sdks:python:test-suites:tox:pycommon:docs\")\n}\n\ntasks.register(\"pythonDockerBuildPreCommit\") {\n  dependsOn(\":sdks:python:container:py39:docker\")\n  dependsOn(\":sdks:python:container:py310:docker\")\n  dependsOn(\":sdks:python:container:py311:docker\")\n  dependsOn(\":sdks:python:container:py312:docker\")\n}\n\ntasks.register(\"pythonLintPreCommit\") {\n  dependsOn(\":sdks:python:test-suites:tox:pycommon:linter\")\n}\n\ntasks.register(\"pythonFormatterPreCommit\") {\n  dependsOn(\"sdks:python:test-suites:tox:pycommon:formatter\")\n}\n\ntasks.register(\"python39PostCommit\") {\n  dependsOn(\":sdks:python:test-suites:dataflow:py39:postCommitIT\")\n  dependsOn(\":sdks:python:test-suites:direct:py39:postCommitIT\")\n  dependsOn(\":sdks:python:test-suites:direct:py39:hdfsIntegrationTest\")\n  dependsOn(\":sdks:python:test-suites:direct:py39:azureIntegrationTest\")\n  dependsOn(\":sdks:python:test-suites:portable:py39:postCommitPy39\")\n  // TODO (https://github.com/apache/beam/issues/23966)\n  // Move this to Python 3.10 test suite once tfx-bsl has python 3.10 wheel.\n  dependsOn(\":sdks:python:test-suites:direct:py39:inferencePostCommitIT\")\n}\n\ntasks.register(\"python310PostCommit\") {\n  dependsOn(\":sdks:python:test-suites:dataflow:py310:postCommitIT\")\n  dependsOn(\":sdks:python:test-suites:direct:py310:postCommitIT\")\n  dependsOn(\":sdks:python:test-suites:portable:py310:postCommitPy310\")\n  // TODO: https://github.com/apache/beam/issues/22651\n  // The default container uses Python 3.10. The goal here is to\n  // duild Docker images for TensorRT tests during run time for python versions\n  // other than 3.10 and add these tests in other python postcommit suites.\n  dependsOn(\":sdks:python:test-suites:dataflow:py310:inferencePostCommitIT\")\n}\n\ntasks.register(\"python311PostCommit\") {\n  dependsOn(\":sdks:python:test-suites:dataflow:py311:postCommitIT\")\n  dependsOn(\":sdks:python:test-suites:direct:py311:postCommitIT\")\n  dependsOn(\":sdks:python:test-suites:direct:py311:hdfsIntegrationTest\")\n  dependsOn(\":sdks:python:test-suites:portable:py311:postCommitPy311\")\n}\n\ntasks.register(\"python312PostCommit\") {\n  dependsOn(\":sdks:python:test-suites:dataflow:py312:postCommitIT\")\n  dependsOn(\":sdks:python:test-suites:direct:py312:postCommitIT\")\n  dependsOn(\":sdks:python:test-suites:direct:py312:hdfsIntegrationTest\")\n  dependsOn(\":sdks:python:test-suites:portable:py312:postCommitPy312\")\n  dependsOn(\":sdks:python:test-suites:dataflow:py312:inferencePostCommitITPy312\")\n}\n\ntasks.register(\"portablePythonPreCommit\") {\n  dependsOn(\":sdks:python:test-suites:portable:py39:preCommitPy39\")\n  dependsOn(\":sdks:python:test-suites:portable:py312:preCommitPy312\")\n}\n\ntasks.register(\"pythonSparkPostCommit\") {\n  dependsOn(\":sdks:python:test-suites:portable:py39:sparkValidatesRunner\")\n  dependsOn(\":sdks:python:test-suites:portable:py312:sparkValidatesRunner\")\n}\n\ntasks.register(\"websitePreCommit\") {\n  dependsOn(\":website:preCommit\")\n}\n\ntasks.register(\"communityMetricsPreCommit\") {\n  dependsOn(\":beam-test-infra-metrics:preCommit\")\n}\n\ntasks.register(\"communityMetricsProber\") {\n  dependsOn(\":beam-test-infra-metrics:checkProber\")\n}\n\ntasks.register(\"javaExamplesDataflowPrecommit\") {\n  dependsOn(\":runners:google-cloud-dataflow-java:examples:preCommit\")\n  dependsOn(\":runners:google-cloud-dataflow-java:examples-streaming:preCommit\")\n  dependsOn(\":runners:google-cloud-dataflow-java:examplesJavaRunnerV2PreCommit\")\n}\n\ntasks.register(\"whitespacePreCommit\") {\n  // TODO(https://github.com/apache/beam/issues/20209): Find a better way to specify the tasks without hardcoding py version.\n  dependsOn(\":sdks:python:test-suites:tox:py39:archiveFilesToLint\")\n  dependsOn(\":sdks:python:test-suites:tox:py39:unpackFilesToLint\")\n  dependsOn(\":sdks:python:test-suites:tox:py39:whitespacelint\")\n}\n\ntasks.register(\"typescriptPreCommit\") {\n  // TODO(https://github.com/apache/beam/issues/20209): Find a better way to specify the tasks without hardcoding py version.\n  dependsOn(\":sdks:python:test-suites:tox:py39:eslint\")\n  dependsOn(\":sdks:python:test-suites:tox:py39:jest\")\n}\n\ntasks.register(\"pushAllRunnersDockerImages\") {\n  dependsOn(\":runners:spark:3:job-server:container:docker\")\n  for (version in project.ext.get(\"allFlinkVersions\") as Array<*>) {\n    dependsOn(\":runners:flink:${version}:job-server-container:docker\")\n  }\n\n  doLast {\n    if (project.hasProperty(\"prune-images\")) {\n      exec {\n        executable(\"docker\")\n        args(\"system\", \"prune\", \"-a\", \"--force\")\n      }\n    }\n  }\n}\n\ntasks.register(\"pushAllSdkDockerImages\") {\n  // Enforce ordering to allow the prune step to happen between runs.\n  // This will ensure we don't use up too much space (especially in CI environments)\n  mustRunAfter(\":pushAllRunnersDockerImages\")\n\n  dependsOn(\":sdks:java:container:pushAll\")\n  dependsOn(\":sdks:python:container:pushAll\")\n  dependsOn(\":sdks:go:container:pushAll\")\n  dependsOn(\":sdks:typescript:container:pushAll\")\n\n  doLast {\n    if (project.hasProperty(\"prune-images\")) {\n      exec {\n        executable(\"docker\")\n        args(\"system\", \"prune\", \"-a\", \"--force\")\n      }\n    }\n  }\n}\n\ntasks.register(\"pushAllXlangDockerImages\") {\n  // Enforce ordering to allow the prune step to happen between runs.\n  // This will ensure we don't use up too much space (especially in CI environments)\n  mustRunAfter(\":pushAllSdkDockerImages\")\n\n  dependsOn(\":sdks:java:expansion-service:container:docker\")\n  dependsOn(\":sdks:java:transform-service:controller-container:docker\")\n  dependsOn(\":sdks:python:expansion-service-container:docker\")\n\n  doLast {\n    if (project.hasProperty(\"prune-images\")) {\n      exec {\n        executable(\"docker\")\n        args(\"system\", \"prune\", \"-a\", \"--force\")\n      }\n    }\n  }\n}\n\ntasks.register(\"pushAllDockerImages\") {\n  dependsOn(\":pushAllRunnersDockerImages\")\n  dependsOn(\":pushAllSdkDockerImages\")\n  dependsOn(\":pushAllXlangDockerImages\")\n}\n\n// Use this task to validate the environment set up for Go, Python and Java\ntasks.register(\"checkSetup\") {\n  dependsOn(\":sdks:go:examples:wordCount\")\n  dependsOn(\":sdks:python:wordCount\")\n  dependsOn(\":examples:java:wordCount\")\n}\n\n// if not disabled make spotlessApply dependency of compileJava and compileTestJava\nval disableSpotlessCheck: String by project\nval isSpotlessDisabled = project.hasProperty(\"disableSpotlessCheck\") &&\n        disableSpotlessCheck == \"true\"\nif (!isSpotlessDisabled) {\n  subprojects {\n    afterEvaluate {\n      tasks.findByName(\"spotlessApply\")?.let {\n        listOf(\"compileJava\", \"compileTestJava\").forEach {\n          t -> tasks.findByName(t)?.let { f -> f.dependsOn(\"spotlessApply\") }\n        }\n      }\n    }\n  }\n}\n\n// Generates external transform config\nproject.tasks.register(\"generateExternalTransformsConfig\") {\n  dependsOn(\":sdks:python:generateExternalTransformsConfig\")\n}\n\n// Configure the release plugin to do only local work; the release manager determines what, if\n// anything, to push. On failure, the release manager can reset the branch without pushing.\nrelease {\n  revertOnFail = false\n  tagTemplate = \"v${version}\"\n  // workaround from https://github.com/researchgate/gradle-release/issues/281#issuecomment-466876492\n  release {\n    with (propertyMissing(\"git\") as net.researchgate.release.GitAdapter.GitConfig) {\n      requireBranch = \"release-.*|master\"\n      pushToRemote = \"\"\n    }\n  }\n}\n\n// Reports linkage errors across multiple Apache Beam artifact ids.\n//\n// To use (from the root of project):\n//    ./gradlew -Ppublishing -PjavaLinkageArtifactIds=artifactId1,artifactId2,... :checkJavaLinkage\n//\n// For example:\n//    ./gradlew -Ppublishing -PjavaLinkageArtifactIds=beam-sdks-java-core,beam-sdks-java-io-jdbc :checkJavaLinkage\n//\n// Note that this task publishes artifacts into your local Maven repository.\nif (project.hasProperty(\"javaLinkageArtifactIds\")) {\n  if (!project.hasProperty(\"publishing\")) {\n    throw GradleException(\"You can only check linkage of Java artifacts if you specify -Ppublishing on the command line as well.\")\n  }\n\n  val linkageCheckerJava by configurations.creating\n  dependencies {\n    linkageCheckerJava(\"com.google.cloud.tools:dependencies:1.5.6\")\n  }\n\n  // We need to evaluate all the projects first so that we can find depend on all the\n  // publishMavenJavaPublicationToMavenLocal tasks below.\n  for (p in rootProject.subprojects) {\n    if (p.path != project.path) {\n      evaluationDependsOn(p.path)\n    }\n  }\n\n  project.tasks.register<JavaExec>(\"checkJavaLinkage\") {\n    dependsOn(project.getTasksByName(\"publishMavenJavaPublicationToMavenLocal\", true /* recursively */))\n    classpath = linkageCheckerJava\n    mainClass.value(\"com.google.cloud.tools.opensource.classpath.LinkageCheckerMain\")\n    val javaLinkageArtifactIds: String = project.property(\"javaLinkageArtifactIds\") as String? ?: \"\"\n    var arguments = arrayOf(\"-a\", javaLinkageArtifactIds.split(\",\").joinToString(\",\") {\n      if (it.contains(\":\")) {\n        \"${project.ext.get(\"mavenGroupId\")}:${it}\"\n      } else {\n        // specify the version if not provided\n        \"${project.ext.get(\"mavenGroupId\")}:${it}:${project.version}\"\n      }\n    })\n\n    // Exclusion file filters out existing linkage errors before a change\n    if (project.hasProperty(\"javaLinkageWriteBaseline\")) {\n      arguments += \"--output-exclusion-file\"\n      arguments += project.property(\"javaLinkageWriteBaseline\") as String\n    } else if (project.hasProperty(\"javaLinkageReadBaseline\")) {\n      arguments += \"--exclusion-file\"\n      arguments += project.property(\"javaLinkageReadBaseline\") as String\n    }\n    args(*arguments)\n    doLast {\n      println(\"NOTE: This task published artifacts into your local Maven repository. You may want to remove them manually.\")\n    }\n  }\n}\nif (project.hasProperty(\"testJavaVersion\")) {\n  var testVer = project.property(\"testJavaVersion\")\n\n  tasks.getByName(\"javaPreCommitPortabilityApi\").dependsOn(\":sdks:java:testing:test-utils:verifyJavaVersion$testVer\")\n  tasks.getByName(\"javaExamplesDataflowPrecommit\").dependsOn(\":sdks:java:testing:test-utils:verifyJavaVersion$testVer\")\n} else {\n  allprojects {\n    tasks.withType(Test::class).configureEach {\n      exclude(\"**/JvmVerification.class\")\n    }\n  }\n}\n"
        },
        {
          "name": "buildSrc",
          "type": "tree",
          "content": null
        },
        {
          "name": "contributor-docs",
          "type": "tree",
          "content": null
        },
        {
          "name": "dev-support",
          "type": "tree",
          "content": null
        },
        {
          "name": "examples",
          "type": "tree",
          "content": null
        },
        {
          "name": "gradle.properties",
          "type": "blob",
          "size": 1.7763671875,
          "content": "################################################################################\n#  Licensed to the Apache Software Foundation (ASF) under one\n#  or more contributor license agreements.  See the NOTICE file\n#  distributed with this work for additional information\n#  regarding copyright ownership.  The ASF licenses this file\n#  to you under the Apache License, Version 2.0 (the\n#  \"License\"); you may not use this file except in compliance\n#  with the License.  You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an \"AS IS\" BASIS,\n#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#  See the License for the specific language governing permissions and\n# limitations under the License.\n################################################################################\norg.gradle.caching=true\norg.gradle.parallel=true\norg.gradle.daemon=false\norg.gradle.configureondemand=true\norg.gradle.jvmargs=-Xss10240k\norg.gradle.vfs.watch=true\nofflineRepositoryRoot=offline-repository\nsigning.gnupg.executable=gpg\nsigning.gnupg.useLegacyGpg=true\n\n# WARNING! Many Beam modules use a custom Gradle plugin that overrides the\n# project version defined here in the `apply` function in\n# buildSrc/src/main/groovy/org/apache/beam/gradle/BeamModulePlugin.groovy.\n# To build a custom Beam version make sure you change it in both places, see\n# https://github.com/apache/beam/issues/21302.\nversion=2.63.0-SNAPSHOT\nsdk_version=2.63.0.dev\n\njavaVersion=1.8\n\ndocker_image_default_repo_root=apache\ndocker_image_default_repo_prefix=beam_\n\n# supported flink versions\nflink_versions=1.17,1.18,1.19\n# supported python versions\npython_versions=3.9,3.10,3.11,3.12\n"
        },
        {
          "name": "gradle",
          "type": "tree",
          "content": null
        },
        {
          "name": "gradlew",
          "type": "blob",
          "size": 8.48828125,
          "content": "#!/bin/sh\n\n#\n# Copyright © 2015-2021 the original authors.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n##############################################################################\n#\n#   Gradle start up script for POSIX generated by Gradle.\n#\n#   Important for running:\n#\n#   (1) You need a POSIX-compliant shell to run this script. If your /bin/sh is\n#       noncompliant, but you have some other compliant shell such as ksh or\n#       bash, then to run this script, type that shell name before the whole\n#       command line, like:\n#\n#           ksh Gradle\n#\n#       Busybox and similar reduced shells will NOT work, because this script\n#       requires all of these POSIX shell features:\n#         * functions;\n#         * expansions «$var», «${var}», «${var:-default}», «${var+SET}»,\n#           «${var#prefix}», «${var%suffix}», and «$( cmd )»;\n#         * compound commands having a testable exit status, especially «case»;\n#         * various built-in commands including «command», «set», and «ulimit».\n#\n#   Important for patching:\n#\n#   (2) This script targets any POSIX shell, so it avoids extensions provided\n#       by Bash, Ksh, etc; in particular arrays are avoided.\n#\n#       The \"traditional\" practice of packing multiple parameters into a\n#       space-separated string is a well documented source of bugs and security\n#       problems, so this is (mostly) avoided, by progressively accumulating\n#       options in \"$@\", and eventually passing that to Java.\n#\n#       Where the inherited environment variables (DEFAULT_JVM_OPTS, JAVA_OPTS,\n#       and GRADLE_OPTS) rely on word-splitting, this is performed explicitly;\n#       see the in-line comments for details.\n#\n#       There are tweaks for specific operating systems such as AIX, CygWin,\n#       Darwin, MinGW, and NonStop.\n#\n#   (3) This script is generated from the Groovy template\n#       https://github.com/gradle/gradle/blob/HEAD/subprojects/plugins/src/main/resources/org/gradle/api/internal/plugins/unixStartScript.txt\n#       within the Gradle project.\n#\n#       You can find Gradle at https://github.com/gradle/gradle/.\n#\n##############################################################################\n\n# Attempt to set APP_HOME\n\n# Resolve links: $0 may be a link\napp_path=$0\n\n# Need this for daisy-chained symlinks.\nwhile\n    APP_HOME=${app_path%\"${app_path##*/}\"}  # leaves a trailing /; empty if no leading path\n    [ -h \"$app_path\" ]\ndo\n    ls=$( ls -ld \"$app_path\" )\n    link=${ls#*' -> '}\n    case $link in             #(\n      /*)   app_path=$link ;; #(\n      *)    app_path=$APP_HOME$link ;;\n    esac\ndone\n\n# This is normally unused\n# shellcheck disable=SC2034\nAPP_BASE_NAME=${0##*/}\n# Discard cd standard output in case $CDPATH is set (https://github.com/gradle/gradle/issues/25036)\nAPP_HOME=$( cd \"${APP_HOME:-./}\" > /dev/null && pwd -P ) || exit\n\n# Use the maximum available, or set MAX_FD != -1 to use that value.\nMAX_FD=maximum\n\nwarn () {\n    echo \"$*\"\n} >&2\n\ndie () {\n    echo\n    echo \"$*\"\n    echo\n    exit 1\n} >&2\n\n# OS specific support (must be 'true' or 'false').\ncygwin=false\nmsys=false\ndarwin=false\nnonstop=false\ncase \"$( uname )\" in                #(\n  CYGWIN* )         cygwin=true  ;; #(\n  Darwin* )         darwin=true  ;; #(\n  MSYS* | MINGW* )  msys=true    ;; #(\n  NONSTOP* )        nonstop=true ;;\nesac\n\nCLASSPATH=$APP_HOME/gradle/wrapper/gradle-wrapper.jar\n\n\n# Determine the Java command to use to start the JVM.\nif [ -n \"$JAVA_HOME\" ] ; then\n    if [ -x \"$JAVA_HOME/jre/sh/java\" ] ; then\n        # IBM's JDK on AIX uses strange locations for the executables\n        JAVACMD=$JAVA_HOME/jre/sh/java\n    else\n        JAVACMD=$JAVA_HOME/bin/java\n    fi\n    if [ ! -x \"$JAVACMD\" ] ; then\n        die \"ERROR: JAVA_HOME is set to an invalid directory: $JAVA_HOME\n\nPlease set the JAVA_HOME variable in your environment to match the\nlocation of your Java installation.\"\n    fi\nelse\n    JAVACMD=java\n    if ! command -v java >/dev/null 2>&1\n    then\n        die \"ERROR: JAVA_HOME is not set and no 'java' command could be found in your PATH.\n\nPlease set the JAVA_HOME variable in your environment to match the\nlocation of your Java installation.\"\n    fi\nfi\n\n# Increase the maximum file descriptors if we can.\nif ! \"$cygwin\" && ! \"$darwin\" && ! \"$nonstop\" ; then\n    case $MAX_FD in #(\n      max*)\n        # In POSIX sh, ulimit -H is undefined. That's why the result is checked to see if it worked.\n        # shellcheck disable=SC2039,SC3045\n        MAX_FD=$( ulimit -H -n ) ||\n            warn \"Could not query maximum file descriptor limit\"\n    esac\n    case $MAX_FD in  #(\n      '' | soft) :;; #(\n      *)\n        # In POSIX sh, ulimit -n is undefined. That's why the result is checked to see if it worked.\n        # shellcheck disable=SC2039,SC3045\n        ulimit -n \"$MAX_FD\" ||\n            warn \"Could not set maximum file descriptor limit to $MAX_FD\"\n    esac\nfi\n\n# Collect all arguments for the java command, stacking in reverse order:\n#   * args from the command line\n#   * the main class name\n#   * -classpath\n#   * -D...appname settings\n#   * --module-path (only if needed)\n#   * DEFAULT_JVM_OPTS, JAVA_OPTS, and GRADLE_OPTS environment variables.\n\n# For Cygwin or MSYS, switch paths to Windows format before running java\nif \"$cygwin\" || \"$msys\" ; then\n    APP_HOME=$( cygpath --path --mixed \"$APP_HOME\" )\n    CLASSPATH=$( cygpath --path --mixed \"$CLASSPATH\" )\n\n    JAVACMD=$( cygpath --unix \"$JAVACMD\" )\n\n    # Now convert the arguments - kludge to limit ourselves to /bin/sh\n    for arg do\n        if\n            case $arg in                                #(\n              -*)   false ;;                            # don't mess with options #(\n              /?*)  t=${arg#/} t=/${t%%/*}              # looks like a POSIX filepath\n                    [ -e \"$t\" ] ;;                      #(\n              *)    false ;;\n            esac\n        then\n            arg=$( cygpath --path --ignore --mixed \"$arg\" )\n        fi\n        # Roll the args list around exactly as many times as the number of\n        # args, so each arg winds up back in the position where it started, but\n        # possibly modified.\n        #\n        # NB: a `for` loop captures its iteration list before it begins, so\n        # changing the positional parameters here affects neither the number of\n        # iterations, nor the values presented in `arg`.\n        shift                   # remove old arg\n        set -- \"$@\" \"$arg\"      # push replacement arg\n    done\nfi\n\n\n# Add default JVM options here. You can also use JAVA_OPTS and GRADLE_OPTS to pass JVM options to this script.\nDEFAULT_JVM_OPTS='\"-Xmx64m\" \"-Xms64m\"'\n\n# Collect all arguments for the java command:\n#   * DEFAULT_JVM_OPTS, JAVA_OPTS, JAVA_OPTS, and optsEnvironmentVar are not allowed to contain shell fragments,\n#     and any embedded shellness will be escaped.\n#   * For example: A user cannot expect ${Hostname} to be expanded, as it is an environment variable and will be\n#     treated as '${Hostname}' itself on the command line.\n\nset -- \\\n        \"-Dorg.gradle.appname=$APP_BASE_NAME\" \\\n        -classpath \"$CLASSPATH\" \\\n        org.gradle.wrapper.GradleWrapperMain \\\n        \"$@\"\n\n# Stop when \"xargs\" is not available.\nif ! command -v xargs >/dev/null 2>&1\nthen\n    die \"xargs is not available\"\nfi\n\n# Use \"xargs\" to parse quoted args.\n#\n# With -n1 it outputs one arg per line, with the quotes and backslashes removed.\n#\n# In Bash we could simply go:\n#\n#   readarray ARGS < <( xargs -n1 <<<\"$var\" ) &&\n#   set -- \"${ARGS[@]}\" \"$@\"\n#\n# but POSIX shell has neither arrays nor command substitution, so instead we\n# post-process each arg (as a line of input to sed) to backslash-escape any\n# character that might be a shell metacharacter, then use eval to reverse\n# that process (while maintaining the separation between arguments), and wrap\n# the whole thing up as a single \"set\" statement.\n#\n# This will of course break if any of these variables contains a newline or\n# an unmatched quote.\n#\n\neval \"set -- $(\n        printf '%s\\n' \"$DEFAULT_JVM_OPTS $JAVA_OPTS $GRADLE_OPTS\" |\n        xargs -n1 |\n        sed ' s~[^-[:alnum:]+,./:=@_]~\\\\&~g; ' |\n        tr '\\n' ' '\n    )\" '\"$@\"'\n\nexec \"$JAVACMD\" \"$@\"\n"
        },
        {
          "name": "gradlew.bat",
          "type": "blob",
          "size": 2.7109375,
          "content": "@rem\n@rem Copyright 2015 the original author or authors.\n@rem\n@rem Licensed under the Apache License, Version 2.0 (the \"License\");\n@rem you may not use this file except in compliance with the License.\n@rem You may obtain a copy of the License at\n@rem\n@rem      https://www.apache.org/licenses/LICENSE-2.0\n@rem\n@rem Unless required by applicable law or agreed to in writing, software\n@rem distributed under the License is distributed on an \"AS IS\" BASIS,\n@rem WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n@rem See the License for the specific language governing permissions and\n@rem limitations under the License.\n@rem\n\n@if \"%DEBUG%\"==\"\" @echo off\n@rem ##########################################################################\n@rem\n@rem  Gradle startup script for Windows\n@rem\n@rem ##########################################################################\n\n@rem Set local scope for the variables with windows NT shell\nif \"%OS%\"==\"Windows_NT\" setlocal\n\nset DIRNAME=%~dp0\nif \"%DIRNAME%\"==\"\" set DIRNAME=.\n@rem This is normally unused\nset APP_BASE_NAME=%~n0\nset APP_HOME=%DIRNAME%\n\n@rem Resolve any \".\" and \"..\" in APP_HOME to make it shorter.\nfor %%i in (\"%APP_HOME%\") do set APP_HOME=%%~fi\n\n@rem Add default JVM options here. You can also use JAVA_OPTS and GRADLE_OPTS to pass JVM options to this script.\nset DEFAULT_JVM_OPTS=\"-Xmx64m\" \"-Xms64m\"\n\n@rem Find java.exe\nif defined JAVA_HOME goto findJavaFromJavaHome\n\nset JAVA_EXE=java.exe\n%JAVA_EXE% -version >NUL 2>&1\nif %ERRORLEVEL% equ 0 goto execute\n\necho.\necho ERROR: JAVA_HOME is not set and no 'java' command could be found in your PATH.\necho.\necho Please set the JAVA_HOME variable in your environment to match the\necho location of your Java installation.\n\ngoto fail\n\n:findJavaFromJavaHome\nset JAVA_HOME=%JAVA_HOME:\"=%\nset JAVA_EXE=%JAVA_HOME%/bin/java.exe\n\nif exist \"%JAVA_EXE%\" goto execute\n\necho.\necho ERROR: JAVA_HOME is set to an invalid directory: %JAVA_HOME%\necho.\necho Please set the JAVA_HOME variable in your environment to match the\necho location of your Java installation.\n\ngoto fail\n\n:execute\n@rem Setup the command line\n\nset CLASSPATH=%APP_HOME%\\gradle\\wrapper\\gradle-wrapper.jar\n\n\n@rem Execute Gradle\n\"%JAVA_EXE%\" %DEFAULT_JVM_OPTS% %JAVA_OPTS% %GRADLE_OPTS% \"-Dorg.gradle.appname=%APP_BASE_NAME%\" -classpath \"%CLASSPATH%\" org.gradle.wrapper.GradleWrapperMain %*\n\n:end\n@rem End local scope for the variables with windows NT shell\nif %ERRORLEVEL% equ 0 goto mainEnd\n\n:fail\nrem Set variable GRADLE_EXIT_CONSOLE if you need the _script_ return code instead of\nrem the _cmd.exe /c_ return code!\nset EXIT_CODE=%ERRORLEVEL%\nif %EXIT_CODE% equ 0 set EXIT_CODE=1\nif not \"\"==\"%GRADLE_EXIT_CONSOLE%\" exit %EXIT_CODE%\nexit /b %EXIT_CODE%\n\n:mainEnd\nif \"%OS%\"==\"Windows_NT\" endlocal\n\n:omega\n"
        },
        {
          "name": "it",
          "type": "tree",
          "content": null
        },
        {
          "name": "learning",
          "type": "tree",
          "content": null
        },
        {
          "name": "local-env-setup.sh",
          "type": "blob",
          "size": 4.9384765625,
          "content": "#!/usr/bin/env bash\n\n# Licensed to the Apache Software Foundation (ASF) under one or more\n# contributor license agreements.  See the NOTICE file distributed with\n# this work for additional information regarding copyright ownership.\n# The ASF licenses this file to You under the Apache License, Version 2.0\n# (the \"License\"); you may not use this file except in compliance with\n# the License.  You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\ndarwin_install_pip3_packages() {\n    echo \"Installing setuptools grpcio-tools\"\n    pip3 install setuptools grpcio-tools\n    echo \"Installing mypy-protobuf\"\n    pip3 install --user mypy-protobuf\n}\n\ninstall_go_packages(){\n        echo \"Installing goavro\"\n        go mod init beam-runtime && go get github.com/linkedin/goavro/v2\n        # As we are using bash, we are assuming .bashrc exists.\n        grep -qxF \"export GOPATH=${PWD}/sdks/go/examples/.gogradle/project_gopath\" ~/.bashrc\n        gopathExists=$?\n        if [ $gopathExists -ne 0 ]; then\n            export GOPATH=${PWD}/sdks/go/examples/.gogradle/project_gopath && echo \"GOPATH was set for this session to '$GOPATH'. Make sure to add this path to your ~/.bashrc file after the execution of this script.\"\n        fi\n}\n\nkernelname=$(uname -s)\n\n# Running on Linux\nif [ \"$kernelname\" = \"Linux\" ]; then\n    # Assuming Debian based Linux and the prerequisites in https://beam.apache.org/contribute/ are met:\n    apt-get update\n\n    echo \"Installing dependencies listed in pkglist file\"\n    apt-get install -y $(grep -v '^#' dev-support/docker/pkglist | cat) # pulling dependencies from pkglist file\n\n    type -P python3 > /dev/null 2>&1\n    python3Exists=$?\n    type -P pip3 > /dev/null 2>&1\n    pip3Exists=$?\n    if [ $python3Exists -eq 0  -a $pip3Exists -eq 0 ]; then\n        echo \"Installing grpcio-tools mypy-protobuf\"\n        pip3 install grpcio-tools mypy-protobuf\n    else\n        echo \"Python3 and pip3 are required but failed to install. Install them manually and rerun the script.\"\n        exit\n    fi\n\n    for ver in 3.9 3.10 3.11 3.12 3; do\n        apt install --yes python$ver-venv\n    done\n\n    type -P go > /dev/null 2>&1\n    goExists=$?\n    if [ $goExists -eq 0 ]; then\n        install_go_packages\n    else\n        echo \"Go is required. Install it manually from https://golang.org/doc/install and rerun the script.\"\n        exit\n    fi\n\n# Running on Mac\nelif [ \"$kernelname\" = \"Darwin\" ]; then\n    # Check for Homebrew, install if we don't have it\n    type -P brew > /dev/null 2>&1\n    brewExists=$?\n    if [ $brewExists -ne 0 ]; then\n        echo \"Installing homebrew\"\n        /bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"\n    fi\n\n    # Update homebrew recipes\n    echo \"Updating brew\"\n    brew update\n\n    # Assuming we are using brew\n    if brew ls --versions openjdk@8 > /dev/null; then\n        echo \"openjdk@8 already installed. Skipping\"\n    else\n        echo \"Installing openjdk@8\"\n        brew install openjdk@8\n    fi\n    for ver in 3.9 3.10 3.11 3.12; do\n      if brew ls --versions python@$ver > /dev/null; then\n          echo \"python@$ver already installed. Skipping\"\n          brew info python@$ver\n      else\n          echo \"Installing python@$ver\"\n          brew install python@$ver\n      fi\n      if [ ! $(type -P python$ver) > /dev/null 2>&1 ]; then\n          # For some python packages, brew does not add symlinks...\n          # TODO: Consider using pyenv to manage multiple installations of Python.\n          ln -s /usr/local/opt/python@$ver/bin/python3 /usr/local/bin/python$ver\n      fi\n    done\n\n    ls -l /usr/local/bin/python*\n\n    type -P python3 > /dev/null 2>&1\n    python3Exists=$?\n    type -P pip3 > /dev/null 2>&1\n    pip3Exists=$?\n    if [ $python3Exists -eq 0  -a $pip3Exists -eq 0 ]; then\n        darwin_install_pip3_packages\n    else\n        echo \"Python3 and pip3 are required but failed to install. Install them manually and rerun the script.\"\n        exit\n    fi\n\n    type -P tox > /dev/null 2>&1\n    toxExists=$?\n    if [ $toxExists -eq 0 ]; then\n        echo \"tox already installed. Skipping\"\n    else\n        echo \"Installing tox\"\n        brew install tox\n    fi\n\n    type -P docker > /dev/null 2>&1\n    dockerExists=$?\n    if [ $dockerExists -eq 0 ]; then\n        echo \"docker already installed. Skipping\"\n    else\n        echo \"Installing docker\"\n        brew install --cask docker\n    fi\n\n    type -P go > /dev/null 2>&1\n    goExists=$?\n    if [ $goExists -eq 0 ]; then\n        install_go_packages\n    else\n        echo \"Go is required. Install it manually from https://golang.org/doc/install and rerun the script.\"\n        exit\n    fi\n\nelse echo \"Unrecognized Kernel Name: $kernelname\"\nfi\n"
        },
        {
          "name": "model",
          "type": "tree",
          "content": null
        },
        {
          "name": "playground",
          "type": "tree",
          "content": null
        },
        {
          "name": "plugins",
          "type": "tree",
          "content": null
        },
        {
          "name": "release",
          "type": "tree",
          "content": null
        },
        {
          "name": "runners",
          "type": "tree",
          "content": null
        },
        {
          "name": "scripts",
          "type": "tree",
          "content": null
        },
        {
          "name": "sdks",
          "type": "tree",
          "content": null
        },
        {
          "name": "settings.gradle.kts",
          "type": "blob",
          "size": 13.830078125,
          "content": "/*\n * Licensed to the Apache Software Foundation (ASF) under one\n * or more contributor license agreements.  See the NOTICE file\n * distributed with this work for additional information\n * regarding copyright ownership.  The ASF licenses this file\n * to you under the Apache License, Version 2.0 (the\n * License); you may not use this file except in compliance\n * with the License.  You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an AS IS BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\nimport com.gradle.enterprise.gradleplugin.internal.extension.BuildScanExtensionWithHiddenFeatures\n\npluginManagement {\n  plugins {\n     id(\"org.javacc.javacc\") version \"3.0.3\" // enable the JavaCC parser generator\n  }\n}\n\nplugins {\n  id(\"com.gradle.develocity\") version \"3.19\"\n  id(\"com.gradle.common-custom-user-data-gradle-plugin\") version \"2.0.1\"\n}\n\n\n// JENKINS_HOME and BUILD_ID set automatically during Jenkins execution\nval isJenkinsBuild = arrayOf(\"JENKINS_HOME\", \"BUILD_ID\").all { System.getenv(it) != null }\n// GITHUB_REPOSITORY and GITHUB_RUN_ID set automatically during Github Actions run\nval isGithubActionsBuild = arrayOf(\"GITHUB_REPOSITORY\", \"GITHUB_RUN_ID\").all { System.getenv(it) != null }\nval isCi = isJenkinsBuild || isGithubActionsBuild\n\ndevelocity {\n  server = \"https://ge.apache.org\"\n\n  buildScan {\n    uploadInBackground = !isCi\n    publishing.onlyIf { it.isAuthenticated }\n    obfuscation {\n      ipAddresses { addresses -> addresses.map { \"0.0.0.0\" } }\n    }\n  }\n}\n\nbuildCache {\n  local {\n    isEnabled = true\n  }\n  remote<HttpBuildCache> {\n    url = uri(\"https://beam-cache.apache.org/cache/\")\n    isAllowUntrustedServer = false\n    credentials {\n      username = System.getenv(\"GRADLE_ENTERPRISE_CACHE_USERNAME\")\n      password = System.getenv(\"GRADLE_ENTERPRISE_CACHE_PASSWORD\")\n    }\n    isEnabled = !System.getenv(\"GRADLE_ENTERPRISE_CACHE_USERNAME\").isNullOrBlank()\n    isPush = isCi && !System.getenv(\"GRADLE_ENTERPRISE_CACHE_USERNAME\").isNullOrBlank()\n  }\n}\n\nrootProject.name = \"beam\"\n\ninclude(\":release\")\ninclude(\":release:go-licenses:go\")\ninclude(\":release:go-licenses:java\")\ninclude(\":release:go-licenses:py\")\n\ninclude(\":examples:java\")\ninclude(\":examples:java:twitter\")\ninclude(\":examples:java:cdap\")\ninclude(\":examples:java:cdap:hubspot\")\ninclude(\":examples:java:cdap:salesforce\")\ninclude(\":examples:java:cdap:servicenow\")\ninclude(\":examples:java:cdap:zendesk\")\ninclude(\":examples:java:webapis\")\ninclude(\":examples:kotlin\")\ninclude(\":examples:multi-language\")\ninclude(\":learning\")\ninclude(\":learning:tour-of-beam\")\ninclude(\":learning:tour-of-beam:frontend\")\ninclude(\":learning:tour-of-beam:terraform\")\ninclude(\":model:fn-execution\")\ninclude(\":model:job-management\")\ninclude(\":model:pipeline\")\ninclude(\":playground\")\ninclude(\":playground:backend\")\ninclude(\":playground:frontend\")\ninclude(\":playground:frontend:playground_components\")\ninclude(\":playground:frontend:playground_components:tools:extract_symbols_java\")\ninclude(\":playground:backend:containers\")\ninclude(\":playground:backend:containers:java\")\ninclude(\":playground:backend:containers:go\")\ninclude(\":playground:backend:containers:python\")\ninclude(\":playground:backend:containers:router\")\ninclude(\":playground:backend:containers:scio\")\ninclude(\":playground:terraform\")\ninclude(\":playground:kafka-emulator\")\n\ninclude(\":it:cassandra\")\ninclude(\":it:common\")\ninclude(\":it:conditions\")\ninclude(\":it:elasticsearch\")\ninclude(\":it:google-cloud-platform\")\ninclude(\":it:jdbc\")\ninclude(\":it:kafka\")\ninclude(\":it:testcontainers\")\ninclude(\":it:truthmatchers\")\ninclude(\":it:mongodb\")\ninclude(\":it:splunk\")\ninclude(\":it:neo4j\")\ninclude(\":runners:core-java\")\ninclude(\":runners:direct-java\")\ninclude(\":runners:extensions-java:metrics\")\n/* Begin Flink Runner related settings */\n/* When updating these versions, please make sure that the following files are updated as well:\n  * FLINK_VERSIONS in .github/actions/setup-default-test-properties/test-properties.json\n  * flink_versions in sdks/go/examples/wasm/README.md\n  * PUBLISHED_FLINK_VERSIONS in sdks/python/apache_beam/options/pipeline_options.py\n  * PUBLISHED_FLINK_VERSIONS in sdks/typescript/src/apache_beam/runners/flink.ts\n  * verify versions in website/www/site/content/en/documentation/runners/flink.md\n  * verify version in sdks/python/apache_beam/runners/interactive/interactive_beam.py\n */\n// Flink 1.17\ninclude(\":runners:flink:1.17\")\ninclude(\":runners:flink:1.17:job-server\")\ninclude(\":runners:flink:1.17:job-server-container\")\n// Flink 1.18\ninclude(\":runners:flink:1.18\")\ninclude(\":runners:flink:1.18:job-server\")\ninclude(\":runners:flink:1.18:job-server-container\")\n// Flink 1.19\ninclude(\":runners:flink:1.19\")\ninclude(\":runners:flink:1.19:job-server\")\ninclude(\":runners:flink:1.19:job-server-container\")\n/* End Flink Runner related settings */\ninclude(\":runners:twister2\")\ninclude(\":runners:google-cloud-dataflow-java\")\ninclude(\":runners:google-cloud-dataflow-java:arm\")\ninclude(\":runners:google-cloud-dataflow-java:examples\")\ninclude(\":runners:google-cloud-dataflow-java:examples-streaming\")\ninclude(\":runners:java-fn-execution\")\ninclude(\":runners:java-job-service\")\ninclude(\":runners:jet\")\ninclude(\":runners:local-java\")\ninclude(\":runners:portability:java\")\ninclude(\":runners:prism\")\ninclude(\":runners:prism:java\")\ninclude(\":runners:spark:3\")\ninclude(\":runners:spark:3:job-server\")\ninclude(\":runners:spark:3:job-server:container\")\ninclude(\":runners:samza\")\ninclude(\":runners:samza:job-server\")\ninclude(\":sdks:go\")\ninclude(\":sdks:go:container\")\ninclude(\":sdks:go:examples\")\ninclude(\":sdks:go:test\")\ninclude(\":sdks:go:test:load\")\ninclude(\":sdks:java:bom\")\ninclude(\":sdks:java:bom:gcp\")\ninclude(\":sdks:java:build-tools\")\ninclude(\":sdks:java:container\")\ninclude(\":sdks:java:container:agent\")\ninclude(\":sdks:java:container:java8\")\ninclude(\":sdks:java:container:java11\")\ninclude(\":sdks:java:container:java17\")\ninclude(\":sdks:java:container:java21\")\ninclude(\":sdks:java:core\")\ninclude(\":sdks:java:core:jmh\")\ninclude(\":sdks:java:expansion-service\")\ninclude(\":sdks:java:expansion-service:container\")\ninclude(\":sdks:java:expansion-service:app\")\ninclude(\":sdks:java:extensions:arrow\")\ninclude(\":sdks:java:extensions:avro\")\ninclude(\":sdks:java:extensions:euphoria\")\ninclude(\":sdks:java:extensions:kryo\")\ninclude(\":sdks:java:extensions:google-cloud-platform-core\")\ninclude(\":sdks:java:extensions:jackson\")\ninclude(\":sdks:java:extensions:join-library\")\ninclude(\":sdks:java:extensions:ml\")\ninclude(\":sdks:java:extensions:ordered\")\ninclude(\":sdks:java:extensions:protobuf\")\ninclude(\":sdks:java:extensions:python\")\ninclude(\":sdks:java:extensions:sbe\")\ninclude(\":sdks:java:extensions:schemaio-expansion-service\")\ninclude(\":sdks:java:extensions:sketching\")\ninclude(\":sdks:java:extensions:sorter\")\ninclude(\":sdks:java:extensions:sql\")\ninclude(\":sdks:java:extensions:sql:payloads\")\ninclude(\":sdks:java:extensions:sql:perf-tests\")\ninclude(\":sdks:java:extensions:sql:jdbc\")\ninclude(\":sdks:java:extensions:sql:shell\")\ninclude(\":sdks:java:extensions:sql:hcatalog\")\ninclude(\":sdks:java:extensions:sql:datacatalog\")\ninclude(\":sdks:java:extensions:sql:zetasql\")\ninclude(\":sdks:java:extensions:sql:expansion-service\")\ninclude(\":sdks:java:extensions:sql:udf\")\ninclude(\":sdks:java:extensions:sql:udf-test-provider\")\ninclude(\":sdks:java:extensions:timeseries\")\ninclude(\":sdks:java:extensions:zetasketch\")\ninclude(\":sdks:java:harness\")\ninclude(\":sdks:java:harness:jmh\")\ninclude(\":sdks:java:io:amazon-web-services2\")\ninclude(\":sdks:java:io:amazon-web-services2:expansion-service\")\ninclude(\":sdks:java:io:amqp\")\ninclude(\":sdks:java:io:azure\")\ninclude(\":sdks:java:io:azure-cosmos\")\ninclude(\":sdks:java:io:cassandra\")\ninclude(\":sdks:java:io:clickhouse\")\ninclude(\":sdks:java:io:common\")\ninclude(\":sdks:java:io:contextualtextio\")\ninclude(\":sdks:java:io:debezium\")\ninclude(\":sdks:java:io:debezium:expansion-service\")\ninclude(\":sdks:java:io:elasticsearch\")\ninclude(\":sdks:java:io:elasticsearch-tests:elasticsearch-tests-7\")\ninclude(\":sdks:java:io:elasticsearch-tests:elasticsearch-tests-8\")\ninclude(\":sdks:java:io:elasticsearch-tests:elasticsearch-tests-common\")\ninclude(\":sdks:java:io:expansion-service\")\ninclude(\":sdks:java:io:file-based-io-tests\")\ninclude(\":sdks:java:io:bigquery-io-perf-tests\")\ninclude(\":sdks:java:io:cdap\")\ninclude(\":sdks:java:io:csv\")\ninclude(\":sdks:java:io:file-schema-transform\")\ninclude(\":sdks:java:io:google-ads\")\ninclude(\":sdks:java:io:google-cloud-platform\")\ninclude(\":sdks:java:io:google-cloud-platform:expansion-service\")\ninclude(\":sdks:java:io:hadoop-common\")\ninclude(\":sdks:java:io:hadoop-file-system\")\ninclude(\":sdks:java:io:hadoop-format\")\ninclude(\":sdks:java:io:hbase\")\ninclude(\":sdks:java:io:hcatalog\")\ninclude(\":sdks:java:io:jdbc\")\ninclude(\":sdks:java:io:jms\")\ninclude(\":sdks:java:io:json\")\ninclude(\":sdks:java:io:kafka\")\ninclude(\":sdks:java:io:kafka:upgrade\")\ninclude(\":sdks:java:io:kudu\")\ninclude(\":sdks:java:io:mongodb\")\ninclude(\":sdks:java:io:mqtt\")\ninclude(\":sdks:java:io:neo4j\")\ninclude(\":sdks:java:io:parquet\")\ninclude(\":sdks:java:io:pulsar\")\ninclude(\":sdks:java:io:rabbitmq\")\ninclude(\":sdks:java:io:redis\")\ninclude(\":sdks:java:io:rrio\")\ninclude(\":sdks:java:io:solr\")\ninclude(\":sdks:java:io:sparkreceiver:2\")\ninclude(\":sdks:java:io:snowflake\")\ninclude(\":sdks:java:io:snowflake:expansion-service\")\ninclude(\":sdks:java:io:splunk\")\ninclude(\":sdks:java:io:thrift\")\ninclude(\":sdks:java:io:tika\")\ninclude(\":sdks:java:io:xml\")\ninclude(\":sdks:java:io:synthetic\")\ninclude(\":sdks:java:io:influxdb\")\ninclude(\":sdks:java:io:singlestore\")\ninclude(\":sdks:java:javadoc\")\ninclude(\":sdks:java:maven-archetypes:examples\")\ninclude(\":sdks:java:maven-archetypes:gcp-bom-examples\")\ninclude(\":sdks:java:maven-archetypes:starter\")\ninclude(\":sdks:java:testing:nexmark\")\ninclude(\":sdks:java:testing:expansion-service\")\ninclude(\":sdks:java:testing:jpms-tests\")\ninclude(\":sdks:java:testing:kafka-service\")\ninclude(\":sdks:java:testing:load-tests\")\ninclude(\":sdks:java:testing:test-utils\")\ninclude(\":sdks:java:testing:tpcds\")\ninclude(\":sdks:java:testing:watermarks\")\ninclude(\":sdks:java:transform-service\")\ninclude(\":sdks:java:transform-service:app\")\ninclude(\":sdks:java:transform-service:launcher\")\ninclude(\":sdks:java:transform-service:controller-container\")\ninclude(\":sdks:python\")\ninclude(\":sdks:python:apache_beam:testing:load_tests\")\ninclude(\":sdks:python:apache_beam:testing:benchmarks:nexmark\")\ninclude(\":sdks:python:container\")\ninclude(\":sdks:python:container:py39\")\ninclude(\":sdks:python:container:py310\")\ninclude(\":sdks:python:container:py311\")\ninclude(\":sdks:python:container:py312\")\ninclude(\":sdks:python:expansion-service-container\")\ninclude(\":sdks:python:test-suites:dataflow\")\ninclude(\":sdks:python:test-suites:dataflow:py39\")\ninclude(\":sdks:python:test-suites:dataflow:py310\")\ninclude(\":sdks:python:test-suites:dataflow:py311\")\ninclude(\":sdks:python:test-suites:dataflow:py312\")\ninclude(\":sdks:python:test-suites:direct\")\ninclude(\":sdks:python:test-suites:direct:py39\")\ninclude(\":sdks:python:test-suites:direct:py310\")\ninclude(\":sdks:python:test-suites:direct:py311\")\ninclude(\":sdks:python:test-suites:direct:py312\")\ninclude(\":sdks:python:test-suites:direct:xlang\")\ninclude(\":sdks:python:test-suites:portable:py39\")\ninclude(\":sdks:python:test-suites:portable:py310\")\ninclude(\":sdks:python:test-suites:portable:py311\")\ninclude(\":sdks:python:test-suites:portable:py312\")\ninclude(\":sdks:python:test-suites:tox:pycommon\")\ninclude(\":sdks:python:test-suites:tox:py39\")\ninclude(\":sdks:python:test-suites:tox:py310\")\ninclude(\":sdks:python:test-suites:tox:py311\")\ninclude(\":sdks:python:test-suites:tox:py312\")\ninclude(\":sdks:python:test-suites:xlang\")\ninclude(\":sdks:typescript\")\ninclude(\":sdks:typescript:container\")\ninclude(\":vendor:grpc-1_60_1\")\ninclude(\":vendor:calcite-1_28_0\")\ninclude(\":vendor:guava-32_1_2-jre\")\ninclude(\":website\")\ninclude(\":runners:google-cloud-dataflow-java:worker\")\ninclude(\":runners:google-cloud-dataflow-java:worker:windmill\")\n// no dots allowed for project paths\ninclude(\"beam-test-infra-metrics\")\nproject(\":beam-test-infra-metrics\").projectDir = file(\".test-infra/metrics\")\ninclude(\"beam-test-infra-mock-apis\")\nproject(\":beam-test-infra-mock-apis\").projectDir = file(\".test-infra/mock-apis\")\ninclude(\"beam-test-tools\")\nproject(\":beam-test-tools\").projectDir = file(\".test-infra/tools\")\ninclude(\"beam-test-jenkins\")\nproject(\":beam-test-jenkins\").projectDir = file(\".test-infra/jenkins\")\ninclude(\"beam-test-gha\")\nproject(\":beam-test-gha\").projectDir = file(\".github\")\ninclude(\"beam-validate-runner\")\nproject(\":beam-validate-runner\").projectDir = file(\".test-infra/validate-runner\")\ninclude(\"com.google.api.gax.batching\")\ninclude(\"sdks:java:io:kafka:kafka-312\")\nfindProject(\":sdks:java:io:kafka:kafka-312\")?.name = \"kafka-312\"\ninclude(\"sdks:java:io:kafka:kafka-251\")\nfindProject(\":sdks:java:io:kafka:kafka-251\")?.name = \"kafka-251\"\ninclude(\"sdks:java:io:kafka:kafka-241\")\nfindProject(\":sdks:java:io:kafka:kafka-241\")?.name = \"kafka-241\"\ninclude(\"sdks:java:io:kafka:kafka-231\")\nfindProject(\":sdks:java:io:kafka:kafka-231\")?.name = \"kafka-231\"\ninclude(\"sdks:java:io:kafka:kafka-222\")\nfindProject(\":sdks:java:io:kafka:kafka-222\")?.name = \"kafka-222\"\ninclude(\"sdks:java:io:kafka:kafka-211\")\nfindProject(\":sdks:java:io:kafka:kafka-211\")?.name = \"kafka-211\"\ninclude(\"sdks:java:io:kafka:kafka-201\")\nfindProject(\":sdks:java:io:kafka:kafka-201\")?.name = \"kafka-201\"\ninclude(\"sdks:java:managed\")\nfindProject(\":sdks:java:managed\")?.name = \"managed\"\ninclude(\"sdks:java:io:iceberg\")\nfindProject(\":sdks:java:io:iceberg\")?.name = \"iceberg\"\ninclude(\"sdks:java:io:solace\")\nfindProject(\":sdks:java:io:solace\")?.name = \"solace\"\ninclude(\"sdks:java:extensions:combiners\")\nfindProject(\":sdks:java:extensions:combiners\")?.name = \"combiners\"\ninclude(\"sdks:java:io:iceberg:hive\")\nfindProject(\":sdks:java:io:iceberg:hive\")?.name = \"hive\"\ninclude(\"sdks:java:io:iceberg:bqms\")\nfindProject(\":sdks:java:io:iceberg:bqms\")?.name = \"bqms\"\n"
        },
        {
          "name": "start-build-env.sh",
          "type": "blob",
          "size": 5.6435546875,
          "content": "#!/usr/bin/env bash\n\n# Licensed to the Apache Software Foundation (ASF) under one or more\n# contributor license agreements.  See the NOTICE file distributed with\n# this work for additional information regarding copyright ownership.\n# The ASF licenses this file to You under the Apache License, Version 2.0\n# (the \"License\"); you may not use this file except in compliance with\n# the License.  You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nset -e               # exit on error\n\ncd \"$(dirname \"$0\")\" # connect to root\n\nDOCKER_DIR=dev-support/docker\nDOCKER_FILE=\"${DOCKER_DIR}/Dockerfile\"\n\nCONTAINER_NAME=beam-dev-${USER}-$$\n\nif [ ! -x \"$(command -v docker)\" ] ; then\n  echo \"Unable to locate docker\"\n  echo \"This Beam Build environment is based upon docker so you must install it first.\"\n  exit 1;\nfi\n\ndocker build -t beam-build -f $DOCKER_FILE $DOCKER_DIR\n\nUSER_NAME=${SUDO_USER:=$USER}\nUSER_ID=$(id -u \"${USER_NAME}\")\n\nif [ \"$(uname -s)\" = \"Darwin\" ]; then\n  GROUP_ID=100\n  if (dscl . -read /Groups/docker 2>/dev/null); then \n      DOCKER_GROUP_ID=$(dscl . -read /Groups/docker| awk '($1 == \"PrimaryGroupID:\") { print $2 }')\n    else\n      # if Docker post-install steps to manage as non-root user not performed - will use dummy gid\n      DOCKER_GROUP_ID=1000\n      echo \"`tput setaf 3`Please take a look post-install steps, to manage Docker as non-root user\"\n      echo \"`tput setaf 2`https://docs.docker.com/engine/install/linux-postinstall`tput sgr0`\"\n  fi\nfi\n\nif [ \"$(uname -s)\" = \"Linux\" ]; then\n  DOCKER_GROUP_ID=$(getent group docker | cut -d':' -f3)\n  GROUP_ID=$(id -g \"${USER_NAME}\")\n  # man docker-run\n  # When using SELinux, mounted directories may not be accessible\n  # to the container. To work around this, with Docker prior to 1.7\n  # one needs to run the \"chcon -Rt svirt_sandbox_file_t\" command on\n  # the directories. With Docker 1.7 and later the z mount option\n  # does this automatically.\n  if command -v selinuxenabled >/dev/null && selinuxenabled; then\n    DCKR_VER=$(docker -v|\n    awk '$1 == \"Docker\" && $2 == \"version\" {split($3,ver,\".\");print ver[1]\".\"ver[2]}')\n    DCKR_MAJ=${DCKR_VER%.*}\n    DCKR_MIN=${DCKR_VER#*.}\n    if [ \"${DCKR_MAJ}\" -eq 1 ] && [ \"${DCKR_MIN}\" -ge 7 ] ||\n        [ \"${DCKR_MAJ}\" -gt 1 ]; then\n      V_OPTS=:z\n    else\n      for d in \"${PWD}\" \"${HOME}/.m2\"; do\n        ctx=$(stat --printf='%C' \"$d\"|cut -d':' -f3)\n        if [ \"$ctx\" != svirt_sandbox_file_t ] && [ \"$ctx\" != container_file_t ]; then\n          printf 'INFO: SELinux is enabled.\\n'\n          printf '\\tMounted %s may not be accessible to the container.\\n' \"$d\"\n          printf 'INFO: If so, on the host, run the following command:\\n'\n          printf '\\t# chcon -Rt svirt_sandbox_file_t %s\\n' \"$d\"\n        fi\n      done\n    fi\n  fi\nfi\n\n# Set the home directory in the Docker container.\nDOCKER_HOME_DIR=${DOCKER_HOME_DIR:-/home/${USER_NAME}}\n\ndocker build -t \"beam-build-${USER_ID}\" - <<UserSpecificDocker\nFROM beam-build\nRUN rm -f /var/log/faillog /var/log/lastlog\nRUN groupadd --non-unique -g ${GROUP_ID} ${USER_NAME}\nRUN groupmod -g ${DOCKER_GROUP_ID} docker\nRUN useradd -g ${GROUP_ID} -G docker -u ${USER_ID} -k /root -m ${USER_NAME} -d \"${DOCKER_HOME_DIR}\"\nRUN echo \"${USER_NAME} ALL=NOPASSWD: ALL\" > \"/etc/sudoers.d/beam-build-${USER_ID}\"\nENV HOME \"${DOCKER_HOME_DIR}\"\nENV GOPATH ${DOCKER_HOME_DIR}/beam/sdks/go/examples/.gogradle/project_gopath\n# This next command still runs as root causing the ~/.cache/go-build to be owned by root\nRUN go mod init beam-build-${USER_ID} && go get github.com/linkedin/goavro/v2\nRUN chown -R ${USER_NAME}:${GROUP_ID} ${DOCKER_HOME_DIR}/.cache\nUserSpecificDocker\n\necho \"\"\necho \"Docker image build completed.\"\necho \"==============================================================================================\"\necho \"\"\n\n# If this env variable is empty, docker will be started\n# in non interactive mode\nDOCKER_INTERACTIVE_RUN=${DOCKER_INTERACTIVE_RUN-\"-i -t\"}\n\nDOCKER_SOCKET_MOUNT=\"\"\nif [ -S /var/run/docker.sock ];\nthen\n  DOCKER_SOCKET_MOUNT=\"-v /var/run/docker.sock:/var/run/docker.sock${V_OPTS:-}\"\n  echo \"Enabling Docker support with the docker build environment.\"\nelse\n  echo \"There is NO Docker support with the docker build environment.\"\nfi\n\nCOMMAND=( \"$@\" )\nif [ $# -eq 0 ];\nthen\n  COMMAND=( \"bash\" )\nfi\n\n[ -d \"${HOME}/.beam_docker_build_env/.gradle\" ] || mkdir -p \"${HOME}/.beam_docker_build_env/.gradle\"\n\n# By mapping the .m2 directory you can do an mvn install from\n# within the container and use the result on your normal\n# system.  And this also is a significant speedup in subsequent\n# builds because the dependencies are downloaded only once.\ndocker run --rm=true ${DOCKER_INTERACTIVE_RUN}                         \\\n           --name \"${CONTAINER_NAME}\"                                  \\\n           --network=host                                              \\\n           -v \"${HOME}/.m2:${DOCKER_HOME_DIR}/.m2${V_OPTS:-}\"          \\\n           -v \"${HOME}/.gnupg:${DOCKER_HOME_DIR}/.gnupg${V_OPTS:-}\"    \\\n           -v \"${HOME}/.beam_docker_build_env/.gradle:${DOCKER_HOME_DIR}/.gradle${V_OPTS:-}\"  \\\n           -v \"${PWD}:${DOCKER_HOME_DIR}/beam${V_OPTS:-}\"              \\\n           -w \"${DOCKER_HOME_DIR}/beam\"                                \\\n           ${DOCKER_SOCKET_MOUNT}                                      \\\n           -u \"${USER_ID}\"                                             \\\n           \"beam-build-${USER_ID}\" \"${COMMAND[@]}\"\n"
        },
        {
          "name": "vendor",
          "type": "tree",
          "content": null
        },
        {
          "name": "website",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}