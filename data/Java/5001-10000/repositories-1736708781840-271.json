{
  "metadata": {
    "timestamp": 1736708781840,
    "page": 271,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjI4MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "apache/hudi",
      "stars": 5550,
      "defaultBranch": "master",
      "files": [
        {
          "name": ".asf.yaml",
          "type": "blob",
          "size": 1.5390625,
          "content": "#\n#  Licensed to the Apache Software Foundation (ASF) under one\n#  or more contributor license agreements.  See the NOTICE file\n#  distributed with this work for additional information\n#  regarding copyright ownership.  The ASF licenses this file\n#  to you under the Apache License, Version 2.0 (the\n#  \"License\"); you may not use this file except in compliance\n#  with the License.  You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an \"AS IS\" BASIS,\n#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#  See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\ngithub:\n  description: \"Upserts, Deletes And Incremental Processing on Big Data.\"\n  homepage: https://hudi.apache.org/\n  labels:\n    - hudi\n    - apachehudi\n    - datalake\n    - incremental-processing\n    - bigdata\n    - stream-processing\n    - data-integration\n    - apachespark\n    - apacheflink\n  features:\n    wiki: true\n    issues: true\n    projects: true\n    discussions: true\n  enabled_merge_buttons:\n    squash:  true\n    merge:   false\n    rebase:  false\n  protected_branches:\n    master:\n      required_pull_request_reviews:\n        required_approving_review_count: 1\n  collaborators:\n    - ad1happy2go\nnotifications:\n  commits:      commits@hudi.apache.org\n  issues:       commits@hudi.apache.org\n  pullrequests: commits@hudi.apache.org\n  jira_options: link label\n"
        },
        {
          "name": ".codecov.yml",
          "type": "blob",
          "size": 3.3388671875,
          "content": "# Licensed to the Apache Software Foundation (ASF) under one or more\n# contributor license agreements.  See the NOTICE file distributed with\n# this work for additional information regarding copyright ownership.\n# The ASF licenses this file to You under the Apache License, Version 2.0\n# (the \"License\"); you may not use this file except in compliance with\n# the License.  You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# For more configuration details:\n# https://docs.codecov.io/docs/codecov-yaml\n\n# Check if this file is valid by running in bash:\n# curl -X POST --data-binary @.codecov.yml https://codecov.io/validate\n\ncoverage:\n  precision: 2\n  round: down\n  range: \"50...100\"\n  status:\n    project: # settings affecting project coverage\n      enabled: yes\n\n    # do not run coverage on patch nor changes\n    patch: no\n    changes: no\n\n# Ignoring Paths\n# --------------\n# which folders/files to ignore\nignore:\n  - \"hudi-common/src/main/java/org/apache/hudi/avro/model/*\"\n  - \"hudi-common/src/main/java/org/apache/hudi/avro/MercifulJsonConverter.java\"\n  - \"hudi-common/src/main/java/org/apache/hudi/common/HoodieJsonPayload\"\n  - \"hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieCleaner.java\"\n  - \"hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieCompactionAdminTool.java\"\n  - \"hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieCompactor.java\"\n  - \"hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotCopier.java\"\n  - \"hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieWithTimelineServer.java\"\n  - \"hudi-utilities/src/main/java/org/apache/hudi/utilities/UpgradePayloadFromUberToApache.java\"\n  - \"hudi-utilities/src/main/java/org/apache/hudi/utilities/perf/TimelineServerPerf.java\"\n  - \"hudi-utilities/src/main/java/org/apache/hudi/utilities/HDFSParquetImporter.java\"\n  - \"hudi-utilities/src/main/java/org/apache/hudi/utilities/HiveIncrementalPuller.java\"\n  - \"hudi-utilities/src/main/java/org/apache/hudi/utilities/adhoc/UpgradePayloadFromUberToApache.java\"\n  - \"hudi-client/src/main/java/org/apache/hudi/metrics/JmxMetricsReporter.java\"\n  - \"hudi-client/src/main/java/org/apache/hudi/metrics/JmxReporterServer.java\"\n  - \"hudi-client/src/main/java/org/apache/hudi/metrics/MetricsGraphiteReporter.java\"\n  - \"hudi-hadoop-mr/src/main/java/com/uber/hoodie/hadoop/HoodieInputFormat.java\"\n  - \"hudi-hadoop-mr/src/main/java/com/uber/hoodie/hadoop/realtime/HoodieRealtimeInputFormat.java\"\n\ncomment: false\n\nflags:\n  hudicli:\n    paths:\n      - hudi-cli/src/main/\n  hudiclient:\n    paths:\n      - hudi-client/src/main/\n  hudicommon:\n    paths:\n      - hudi-common/src/main/\n  hudiexamples:\n    paths:\n      - hudi-examples/src/main/\n  hudihadoopmr:\n    paths:\n      - hudi-hadoop-mr/src/main/\n  hudihivesync:\n    paths:\n      - hudi-hive-sync/src/main/\n  hudiintegtest:\n    paths:\n      - hudi-integ-test/src/main/\n  hudispark:\n    paths:\n      - hudi-spark/src/main/\n  huditimelineservice:\n    paths:\n      - hudi-timeline-service/src/main/\n  hudiutilities:\n    paths:\n      - hudi-utilities/src/main/\n"
        },
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 1.1533203125,
          "content": "# Directories #\n/build/\ntarget/\nmetastore_db/\n.metals/\n.mvn/\n.vscode/\n\n# OS Files #\n.DS_Store\n \n*.class\n.java-version\n \n# Package Files #\n*.jar\n# But not these files...\n!/hoodie-cli/lib/dnl/utils/textutils/0.3.3/textutils-0.3.3.jar\n*.war\n*.ear\n*.db\n*.patch \n \n######################\n# OSX\n######################\n \n.DS_Store\n \n# Thumbnails\n._*\n \n######################\n# Eclipse\n######################\n \n*.pydevproject\n.project\n.metadata\ntmp/**\ntmp/**/*\n*.tmp\n*.bak\n*.swp\n*.pyc\n*~.nib\nlocal.properties\n.classpath\n.settings/\n.loadpath\n/src/main/resources/rebel.xml\n# External tool builders\n.externalToolBuilders/\n \n# Locally stored \"Eclipse launch configurations\"\n*.launch\n \n# CDT-specific\n.cproject\n \n# PDT-specific\n.buildpath\n\n#######################################\n# IntelliJ specific files/directories #\n#######################################\n.out\n.idea/*\n!.idea/vcs.xml\n!.idea/icon.png\n*.ipr\n*.iws\n*.iml\n*.log\n\n#######################################\n# Maven\n#######################################\ndependency-reduced-pom.xml\n\n\n#######################################\n# Docker\n#######################################\nhudi-integ-test/compose_env\nnode_modules\npackage-lock.json\n"
        },
        {
          "name": ".idea",
          "type": "tree",
          "content": null
        },
        {
          "name": "Dockerfile",
          "type": "blob",
          "size": 1.0595703125,
          "content": "# Licensed to the Apache Software Foundation (ASF) under one or more\n# contributor license agreements.  See the NOTICE file distributed with\n# this work for additional information regarding copyright ownership.\n# The ASF licenses this file to You under the Apache License, Version 2.0\n# (the \"License\"); you may not use this file except in compliance with\n# the License.  You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# Use a home made image as the base, which includes:\n# utuntu:latest\n# git\n# thrift\n# maven\n# java8\nFROM apachehudi/hudi-ci-bundle-validation-base:azure_ci_test_base_new\n\nCMD [\"java\", \"-version\"]\n\n# Set the working directory to /app\nWORKDIR /hudi\n\n# Copy git repo into the working directory\nCOPY . /hudi\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 18.7451171875,
          "content": "\n                                 Apache License\n                           Version 2.0, January 2004\n                        http://www.apache.org/licenses/\n\n   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n\n   1. Definitions.\n\n      \"License\" shall mean the terms and conditions for use, reproduction,\n      and distribution as defined by Sections 1 through 9 of this document.\n\n      \"Licensor\" shall mean the copyright owner or entity authorized by\n      the copyright owner that is granting the License.\n\n      \"Legal Entity\" shall mean the union of the acting entity and all\n      other entities that control, are controlled by, or are under common\n      control with that entity. For the purposes of this definition,\n      \"control\" means (i) the power, direct or indirect, to cause the\n      direction or management of such entity, whether by contract or\n      otherwise, or (ii) ownership of fifty percent (50%) or more of the\n      outstanding shares, or (iii) beneficial ownership of such entity.\n\n      \"You\" (or \"Your\") shall mean an individual or Legal Entity\n      exercising permissions granted by this License.\n\n      \"Source\" form shall mean the preferred form for making modifications,\n      including but not limited to software source code, documentation\n      source, and configuration files.\n\n      \"Object\" form shall mean any form resulting from mechanical\n      transformation or translation of a Source form, including but\n      not limited to compiled object code, generated documentation,\n      and conversions to other media types.\n\n      \"Work\" shall mean the work of authorship, whether in Source or\n      Object form, made available under the License, as indicated by a\n      copyright notice that is included in or attached to the work\n      (an example is provided in the Appendix below).\n\n      \"Derivative Works\" shall mean any work, whether in Source or Object\n      form, that is based on (or derived from) the Work and for which the\n      editorial revisions, annotations, elaborations, or other modifications\n      represent, as a whole, an original work of authorship. For the purposes\n      of this License, Derivative Works shall not include works that remain\n      separable from, or merely link (or bind by name) to the interfaces of,\n      the Work and Derivative Works thereof.\n\n      \"Contribution\" shall mean any work of authorship, including\n      the original version of the Work and any modifications or additions\n      to that Work or Derivative Works thereof, that is intentionally\n      submitted to Licensor for inclusion in the Work by the copyright owner\n      or by an individual or Legal Entity authorized to submit on behalf of\n      the copyright owner. For the purposes of this definition, \"submitted\"\n      means any form of electronic, verbal, or written communication sent\n      to the Licensor or its representatives, including but not limited to\n      communication on electronic mailing lists, source code control systems,\n      and issue tracking systems that are managed by, or on behalf of, the\n      Licensor for the purpose of discussing and improving the Work, but\n      excluding communication that is conspicuously marked or otherwise\n      designated in writing by the copyright owner as \"Not a Contribution.\"\n\n      \"Contributor\" shall mean Licensor and any individual or Legal Entity\n      on behalf of whom a Contribution has been received by Licensor and\n      subsequently incorporated within the Work.\n\n   2. Grant of Copyright License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      copyright license to reproduce, prepare Derivative Works of,\n      publicly display, publicly perform, sublicense, and distribute the\n      Work and such Derivative Works in Source or Object form.\n\n   3. Grant of Patent License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      (except as stated in this section) patent license to make, have made,\n      use, offer to sell, sell, import, and otherwise transfer the Work,\n      where such license applies only to those patent claims licensable\n      by such Contributor that are necessarily infringed by their\n      Contribution(s) alone or by combination of their Contribution(s)\n      with the Work to which such Contribution(s) was submitted. If You\n      institute patent litigation against any entity (including a\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\n      or a Contribution incorporated within the Work constitutes direct\n      or contributory patent infringement, then any patent licenses\n      granted to You under this License for that Work shall terminate\n      as of the date such litigation is filed.\n\n   4. Redistribution. You may reproduce and distribute copies of the\n      Work or Derivative Works thereof in any medium, with or without\n      modifications, and in Source or Object form, provided that You\n      meet the following conditions:\n\n      (a) You must give any other recipients of the Work or\n          Derivative Works a copy of this License; and\n\n      (b) You must cause any modified files to carry prominent notices\n          stating that You changed the files; and\n\n      (c) You must retain, in the Source form of any Derivative Works\n          that You distribute, all copyright, patent, trademark, and\n          attribution notices from the Source form of the Work,\n          excluding those notices that do not pertain to any part of\n          the Derivative Works; and\n\n      (d) If the Work includes a \"NOTICE\" text file as part of its\n          distribution, then any Derivative Works that You distribute must\n          include a readable copy of the attribution notices contained\n          within such NOTICE file, excluding those notices that do not\n          pertain to any part of the Derivative Works, in at least one\n          of the following places: within a NOTICE text file distributed\n          as part of the Derivative Works; within the Source form or\n          documentation, if provided along with the Derivative Works; or,\n          within a display generated by the Derivative Works, if and\n          wherever such third-party notices normally appear. The contents\n          of the NOTICE file are for informational purposes only and\n          do not modify the License. You may add Your own attribution\n          notices within Derivative Works that You distribute, alongside\n          or as an addendum to the NOTICE text from the Work, provided\n          that such additional attribution notices cannot be construed\n          as modifying the License.\n\n      You may add Your own copyright statement to Your modifications and\n      may provide additional or different license terms and conditions\n      for use, reproduction, or distribution of Your modifications, or\n      for any such Derivative Works as a whole, provided Your use,\n      reproduction, and distribution of the Work otherwise complies with\n      the conditions stated in this License.\n\n   5. Submission of Contributions. Unless You explicitly state otherwise,\n      any Contribution intentionally submitted for inclusion in the Work\n      by You to the Licensor shall be under the terms and conditions of\n      this License, without any additional terms or conditions.\n      Notwithstanding the above, nothing herein shall supersede or modify\n      the terms of any separate license agreement you may have executed\n      with Licensor regarding such Contributions.\n\n   6. Trademarks. This License does not grant permission to use the trade\n      names, trademarks, service marks, or product names of the Licensor,\n      except as required for reasonable and customary use in describing the\n      origin of the Work and reproducing the content of the NOTICE file.\n\n   7. Disclaimer of Warranty. Unless required by applicable law or\n      agreed to in writing, Licensor provides the Work (and each\n      Contributor provides its Contributions) on an \"AS IS\" BASIS,\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n      implied, including, without limitation, any warranties or conditions\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n      PARTICULAR PURPOSE. You are solely responsible for determining the\n      appropriateness of using or redistributing the Work and assume any\n      risks associated with Your exercise of permissions under this License.\n\n   8. Limitation of Liability. In no event and under no legal theory,\n      whether in tort (including negligence), contract, or otherwise,\n      unless required by applicable law (such as deliberate and grossly\n      negligent acts) or agreed to in writing, shall any Contributor be\n      liable to You for damages, including any direct, indirect, special,\n      incidental, or consequential damages of any character arising as a\n      result of this License or out of the use or inability to use the\n      Work (including but not limited to damages for loss of goodwill,\n      work stoppage, computer failure or malfunction, or any and all\n      other commercial damages or losses), even if such Contributor\n      has been advised of the possibility of such damages.\n\n   9. Accepting Warranty or Additional Liability. While redistributing\n      the Work or Derivative Works thereof, You may choose to offer,\n      and charge a fee for, acceptance of support, warranty, indemnity,\n      or other liability obligations and/or rights consistent with this\n      License. However, in accepting such obligations, You may act only\n      on Your own behalf and on Your sole responsibility, not on behalf\n      of any other Contributor, and only if You agree to indemnify,\n      defend, and hold each Contributor harmless for any liability\n      incurred by, or claims asserted against, such Contributor by reason\n      of your accepting any such warranty or additional liability.\n\n   END OF TERMS AND CONDITIONS\n\n   APPENDIX: How to apply the Apache License to your work.\n\n      To apply the Apache License to your work, attach the following\n      boilerplate notice, with the fields enclosed by brackets \"[]\"\n      replaced with your own identifying information. (Don't include\n      the brackets!)  The text should be enclosed in the appropriate\n      comment syntax for the file format. We also recommend that a\n      file or class name and description of purpose be included on the\n      same \"printed page\" as the copyright notice for easier\n      identification within third-party archives.\n\n   Copyright [yyyy] [name of copyright owner]\n\n   Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.\n\n--------------------------------------------------------------------------------\n\nThis product includes code from Apache Hive.\n\n* org.apache.hadoop.hive.ql.io.CombineHiveInputFormat copied to org.apache.hudi.hadoop.hive.HoodieCombineHiveInputFormat\n* org.apache.hadoop.hive.serde2.ColumnProjectionUtils copied and modified to org.apache.hudi.hadoop.HoodieColumnProjectionUtils\n\nCopyright: 2011-2019 The Apache Software Foundation\nHome page: http://hive.apache.org/\nLicense: http://www.apache.org/licenses/LICENSE-2.0\n\n--------------------------------------------------------------------------------\n\nThis product includes code from Apache SystemML.\n\n* org.apache.hudi.client.utils.LazyIterableIterator  adapted from org/apache/sysml/runtime/instructions/spark/data/LazyIterableIterator\n\nCopyright: 2015-2018 The Apache Software Foundation\nHome page: https://systemml.apache.org/\nLicense: http://www.apache.org/licenses/LICENSE-2.0\n\n--------------------------------------------------------------------------------\n\nThis product includes code from https://github.com/twitter/commons/blob/master/src/java/com/twitter/common/objectsize/ObjectSizeCalculator.java with the following license\n\n=================================================================================================\n Copyright 2011 Twitter, Inc.\n -------------------------------------------------------------------------------------------------\n Licensed under the Apache License, Version 2.0 (the \"License\");\n you may not use this work except in compliance with the License.\n You may obtain a copy of the License in the LICENSE file, or at:\n\n  http://www.apache.org/licenses/LICENSE-2.0\n\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an \"AS IS\" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n=================================================================================================\n\nThis product includes code from Apache Spark\n\n* org.apache.hudi.AvroConversionHelper copied from classes in org/apache/spark/sql/avro package\n\n* org.apache.hudi.HoodieSparkUtils.scala copied some methods from org.apache.spark.deploy.SparkHadoopUtil.scala\n\nCopyright: 2014 and onwards The Apache Software Foundation\nHome page: http://spark.apache.org/\nLicense: http://www.apache.org/licenses/LICENSE-2.0\n\n--------------------------------------------------------------------------------\n\nThis product includes code from https://github.com/big-data-europe/README\n\n* docker/hoodie/hadoop/base/entrypoint.sh copied from https://github.com/big-data-europe/docker-hadoop/blob/master/base/entrypoint.sh \n\nwhich is under the MIT license (https://github.com/big-data-europe/README#license)\n\nThe MIT License (MIT)\n\nCopyright (c) 2015 TENFORCE BVBA, INSTITUT FUR ANGEWANDTE INFORMATIK EV,\nNATIONAL CENTER FOR SCIENTIFIC RESEARCH \"DEMOKRITOS\", SEMANTIC WEB COMPANY\nGMBH, FRAUNHOFER-GESELLSCHAFT ZUR FOERDERUNG DER ANGEWANDTEN FORSCHUNG E.V,\nETHNIKO KAI KAPODISTRIAKO PANEPISTIMIO ATHINON, GEIE ERCIM\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n\n-------------------------------------------------------------------------------\n\nThis product includes code from Apache Hadoop\n\n* org.apache.hudi.common.bloom.InternalDynamicBloomFilter.java adapted from org.apache.hadoop.util.bloom.DynamicBloomFilter.java\n\n* org.apache.hudi.common.bloom.InternalFilter.java adapted from org.apache.hadoop.util.bloom.Filter.java\n  and org.apache.hadoop.io.Writable.java\n\n* org.apache.hudi.common.bloom.InternalBloomFilter adapted from org.apache.hadoop.util.bloom.BloomFilter.java\n\n* org.apache.hudi.common.bloom.Key.java adapted from org.apache.hadoop.util.bloom.Key.java\n\n* org.apache.hudi.common.bloom.HashFunction.java ported from org.apache.hadoop.util.bloom.HashFunction.java\n\n* org.apache.hudi.common.util.hash.Hash.java ported from org.apache.hadoop.util.hash.Hash.java\n\n* org.apache.hudi.common.util.hash.JenkinsHash.java ported from org.apache.hadoop.util.hash.JenkinsHash.java\n\n* org.apache.hudi.common.util.hash.MurmurHash.java ported from org.apache.hadoop.util.hash.MurmurHash.java\n\nwith the following license\n\nCopyright (c) 2005, European Commission project OneLab under contract 034819 (http://www.one-lab.org)\n\n  Licensed to the Apache Software Foundation (ASF) under one\n  or more contributor license agreements.  See the NOTICE file\n  distributed with this work for additional information\n  regarding copyright ownership.  The ASF licenses this file\n  to you under the Apache License, Version 2.0 (the\n  \"License\"); you may not use this file except in compliance\n  with the License.  You may obtain a copy of the License at\n\n      http://www.apache.org/licenses/LICENSE-2.0\n\n  Unless required by applicable law or agreed to in writing, software\n  distributed under the License is distributed on an \"AS IS\" BASIS,\n  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n  See the License for the specific language governing permissions and\n  limitations under the License.\n\n -------------------------------------------------------------------------------\n\n This product includes code from Apache Cassandra\n\n * org.apache.hudi.common.util.BufferedRandomAccessFile adapted from org.apache.cassandra.io.BufferedRandomAccessFile\n\n Copyright: 2011-2019 The Apache Software Foundation\n Home page: http://cassandra.apache.org/\n License: http://www.apache.org/licenses/LICENSE-2.0\n\n -------------------------------------------------------------------------------\n\n This product includes code from Apache commons-lang\n\n * org.apache.hudi.common.util.collection.Pair adapted from org.apache.commons.lang3.tuple.Pair\n\n Copyright 2001-2020 The Apache Software Foundation\n\n Home page: https://commons.apache.org/proper/commons-lang/\n License: http://www.apache.org/licenses/LICENSE-2.0\n\n -------------------------------------------------------------------------------\n\n This product includes code from StreamSets Data Collector\n\n  * com.streamsets.pipeline.lib.util.avroorc.AvroToOrcRecordConverter copied and modified to org.apache.hudi.common.util.AvroOrcUtils\n  * com.streamsets.pipeline.lib.util.avroorc.AvroToOrcSchemaConverter copied and modified to org.apache.hudi.common.util.AvroOrcUtils\n\n  Copyright 2018 StreamSets Inc.\n\n  Home page: https://github.com/streamsets/datacollector-oss\n  License: http://www.apache.org/licenses/LICENSE-2.0\n\n -------------------------------------------------------------------------------\n\n This product includes code from Apache Avro\n\n * org.apache.hudi.avro.JsonEncoder adapted from org.apache.avro.io.JsonEncoder\n\n Copyright: 2010-2019 The Apache Software Foundation\n Home page: https://avro.apache.org\n License: http://www.apache.org/licenses/LICENSE-2.0\n"
        },
        {
          "name": "NOTICE",
          "type": "blob",
          "size": 9.9521484375,
          "content": "Apache Hudi\nCopyright 2019-2020 The Apache Software Foundation\n\nThis product includes software developed at\nThe Apache Software Foundation (http://www.apache.org/).\n\n--------------------------------------------------------------------------------\n\nThis product includes code from Apache Hive, which includes the following in\nits NOTICE file:\n\n  Apache Hive\n  Copyright 2008-2018 The Apache Software Foundation\n\n  This product includes software developed by The Apache Software\n  Foundation (http://www.apache.org/).\n\n  This project includes software licensed under the JSON license.\n\n--------------------------------------------------------------------------------\n\nThis product includes code from Apache SystemML, which includes the following in\nits NOTICE file:\n\n  Apache SystemML\n  Copyright [2015-2018] The Apache Software Foundation\n\n  This product includes software developed at\n  The Apache Software Foundation (http://www.apache.org/).\n\n--------------------------------------------------------------------------------\n\nThis product includes code from Apache Spark, which includes the following in\nits NOTICE file:\n\n  Apache Spark\n  Copyright 2014 and onwards The Apache Software Foundation.\n\n  This product includes software developed at\n  The Apache Software Foundation (http://www.apache.org/).\n\n\n  Export Control Notice\n  ---------------------\n\n  This distribution includes cryptographic software. The country in which you currently reside may have\n  restrictions on the import, possession, use, and/or re-export to another country, of encryption software.\n  BEFORE using any encryption software, please check your country's laws, regulations and policies concerning\n  the import, possession, or use, and re-export of encryption software, to see if this is permitted. See\n  <http://www.wassenaar.org/> for more information.\n\n  The U.S. Government Department of Commerce, Bureau of Industry and Security (BIS), has classified this\n  software as Export Commodity Control Number (ECCN) 5D002.C.1, which includes information security software\n  using or performing cryptographic functions with asymmetric algorithms. The form and manner of this Apache\n  Software Foundation distribution makes it eligible for export under the License Exception ENC Technology\n  Software Unrestricted (TSU) exception (see the BIS Export Administration Regulations, Section 740.13) for\n  both object code and source code.\n\n  The following provides more details on the included cryptographic software:\n\n  This software uses Apache Commons Crypto (https://commons.apache.org/proper/commons-crypto/) to\n  support authentication, and encryption and decryption of data sent across the network between\n  services.\n\n\n  Metrics\n  Copyright 2010-2013 Coda Hale and Yammer, Inc.\n\n  This product includes software developed by Coda Hale and Yammer, Inc.\n\n  This product includes code derived from the JSR-166 project (ThreadLocalRandom, Striped64,\n  LongAdder), which was released with the following comments:\n\n      Written by Doug Lea with assistance from members of JCP JSR-166\n      Expert Group and released to the public domain, as explained at\n      http://creativecommons.org/publicdomain/zero/1.0/\n\n--------------------------------------------------------------------------------\n\nPortions of this software were developed at\nTwitter, Inc (https://twitter.com/).\n\n--------------------------------------------------------------------------------\n\nThis product includes software from the docker-hadoop project\n * Copyright https://github.com/big-data-europe/docker-hadoop/\n * Licensed under the MIT License;\n\n--------------------------------------------------------------------------------\n\nThis product includes code from Apache Hadoop, which includes the following in\nits NOTICE file:\n\n  Apache Hadoop\n  Copyright 2006 and onwards The Apache Software Foundation.\n\n  This product includes software developed at\n  The Apache Software Foundation (http://www.apache.org/).\n\n  Export Control Notice\n  ---------------------\n\n  This distribution includes cryptographic software.  The country in\n  which you currently reside may have restrictions on the import,\n  possession, use, and/or re-export to another country, of\n  encryption software.  BEFORE using any encryption software, please\n  check your country's laws, regulations and policies concerning the\n  import, possession, or use, and re-export of encryption software, to\n  see if this is permitted.  See <http://www.wassenaar.org/> for more\n  information.\n\n  The U.S. Government Department of Commerce, Bureau of Industry and\n  Security (BIS), has classified this software as Export Commodity\n  Control Number (ECCN) 5D002.C.1, which includes information security\n  software using or performing cryptographic functions with asymmetric\n  algorithms.  The form and manner of this Apache Software Foundation\n  distribution makes it eligible for export under the License Exception\n  ENC Technology Software Unrestricted (TSU) exception (see the BIS\n  Export Administration Regulations, Section 740.13) for both object\n  code and source code.\n\n  The following provides more details on the included cryptographic software:\n\n  This software uses the SSL libraries from the Jetty project written\n  by mortbay.org.\n  Hadoop Yarn Server Web Proxy uses the BouncyCastle Java\n  cryptography APIs written by the Legion of the Bouncy Castle Inc.\n\n--------------------------------------------------------------------------------\n\nThis product includes code from Apache Cassandra, which includes the following in\nits NOTICE file:\n\n  Apache Cassandra\n  Copyright 2009-2020 The Apache Software Foundation\n\n  This product includes software developed by The Apache Software\n  Foundation (http://www.apache.org/).\n\n--------------------------------------------------------------------------------\n\nThis product includes code from Apache commons-lang, which includes the following in\nits NOTICE file:\n\n  Apache commons-lang\n  Copyright 2001-2020 The Apache Software Foundation\n\n  This product includes software developed at\n  The Apache Software Foundation (http://www.apache.org/).\n\n--------------------------------------------------------------------------------\n\nThis product includes code from StreamSets Data Collector, which includes the following in\nits NOTICE file:\n\n  StreamSets datacollector-oss\n  Copyright 2018 StreamSets Inc.\n\n  This product includes software developed at\n  StreamSets (http://www.streamsets.com/).\n\n--------------------------------------------------------------------------------\n\nThis product includes code from hilbert-curve project\n * Copyright https://github.com/davidmoten/hilbert-curve\n * Licensed under the Apache-2.0 License\n\n--------------------------------------------------------------------------------\n\nThis product includes code from Apache Avro, which includes the following in\nits NOTICE file:\n\n  Apache Avro\n  Copyright 2010-2019 The Apache Software Foundation\n\n  This product includes software developed at\n  The Apache Software Foundation (https://www.apache.org/).\n\n  NUnit license acknowledgement:\n\n  | Portions Copyright © 2002-2012 Charlie Poole or Copyright © 2002-2004 James\n  | W. Newkirk, Michael C. Two, Alexei A. Vorontsov or Copyright © 2000-2002\n  | Philip A. Craig\n\n  Based upon the representations of upstream licensors, it is understood that\n  portions of the mapreduce API included in the Java implementation are licensed\n  from various contributors under one or more contributor license agreements to\n  Odiago, Inc. and were then contributed by Odiago to Apache Avro, which has now\n  made them available under the Apache 2.0 license. The original file header text\n  is:\n\n  | Licensed to Odiago, Inc. under one or more contributor license\n  | agreements.  See the NOTICE file distributed with this work for\n  | additional information regarding copyright ownership.  Odiago, Inc.\n  | licenses this file to you under the Apache License, Version 2.0\n  | (the \"License\"); you may not use this file except in compliance\n  | with the License.  You may obtain a copy of the License at\n  |\n  |     https://www.apache.org/licenses/LICENSE-2.0\n  |\n  | Unless required by applicable law or agreed to in writing, software\n  | distributed under the License is distributed on an \"AS IS\" BASIS,\n  | WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n  | implied.  See the License for the specific language governing\n  | permissions and limitations under the License.\n\n  The Odiago NOTICE at the time of the contribution:\n\n  | This product includes software developed by Odiago, Inc.\n  | (https://www.wibidata.com).\n\n  Apache Ivy includes the following in its NOTICE file:\n\n  | Apache Ivy\n  | Copyright 2007-2010 The Apache Software Foundation\n  |\n  | This product includes software developed by\n  | The Apache Software Foundation (https://www.apache.org/).\n  |\n  | Portions of Ivy were originally developed by\n  | Jayasoft SARL (http://www.jayasoft.fr/)\n  | and are licensed to the Apache Software Foundation under the\n  | \"Software Grant License Agreement\"\n  |\n  | SSH and SFTP support is provided by the JCraft JSch package,\n  | which is open source software, available under\n  | the terms of a BSD style license.\n  | The original software and related information is available\n  | at http://www.jcraft.com/jsch/.\n\n  Apache Log4Net includes the following in its NOTICE file:\n\n  | Apache log4net\n  | Copyright 2004-2015 The Apache Software Foundation\n  |\n  | This product includes software developed at\n  | The Apache Software Foundation (https://www.apache.org/).\n\n  csharp reflect serializers were contributed by Pitney Bowes Inc.\n\n  | Copyright 2019 Pitney Bowes Inc.\n  | Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License.\n  | You may obtain a copy of the License at https://www.apache.org/licenses/LICENSE-2.0.\n  | Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS,\n  | WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n  | See the License for the specific language governing permissions and limitations under the License.\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 10.8466796875,
          "content": "\n<!--\n  Licensed to the Apache Software Foundation (ASF) under one or more\n  contributor license agreements.  See the NOTICE file distributed with\n  this work for additional information regarding copyright ownership.\n  The ASF licenses this file to You under the Apache License, Version 2.0\n  (the \"License\"); you may not use this file except in compliance with\n  the License.  You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n  Unless required by applicable law or agreed to in writing, software\n  distributed under the License is distributed on an \"AS IS\" BASIS,\n  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n  See the License for the specific language governing permissions and\n  limitations under the License.\n-->\n\n# Apache Hudi\n\nApache Hudi is an open data lakehouse platform, built on a high-performance open table format \nto ingest, index, store, serve, transform and manage your data across multiple cloud data environments.\n\n<img src=\"https://hudi.apache.org/assets/images/hudi-logo-medium.png\" alt=\"Hudi logo\" height=\"80px\" align=\"right\" />\n\n<https://hudi.apache.org/>\n\n[![Build](https://github.com/apache/hudi/actions/workflows/bot.yml/badge.svg)](https://github.com/apache/hudi/actions/workflows/bot.yml)\n[![Test](https://dev.azure.com/apachehudi/hudi-oss-ci/_apis/build/status/apachehudi-ci.hudi-mirror?branchName=master)](https://dev.azure.com/apachehudi/hudi-oss-ci/_build/latest?definitionId=5&branchName=master)\n[![License](https://img.shields.io/badge/license-Apache%202-4EB1BA.svg)](https://www.apache.org/licenses/LICENSE-2.0.html)\n[![Maven Central](https://maven-badges.herokuapp.com/maven-central/org.apache.hudi/hudi/badge.svg)](http://search.maven.org/#search%7Cga%7C1%7Cg%3A%22org.apache.hudi%22)\n![GitHub commit activity](https://img.shields.io/github/commit-activity/m/apache/hudi)\n[![Join on Slack](https://img.shields.io/badge/slack-%23hudi-72eff8?logo=slack&color=48c628&label=Join%20on%20Slack)](https://join.slack.com/t/apache-hudi/shared_invite/zt-2ggm1fub8-_yt4Reu9djwqqVRFC7X49g)\n[![Twitter Follow](https://img.shields.io/twitter/follow/ApacheHudi)](https://twitter.com/apachehudi)\n[![Follow Linkedin](https://img.shields.io/badge/apache%E2%80%93hudi-0077B5?style=for-the-badge&logo=linkedin&logoColor=white&label=Follow)](https://www.linkedin.com/company/apache-hudi/?viewAsMember=true)\n\n## Features\n\nHudi stores all data and metadata on cloud storage in open formats, providing the following features across different aspects.\n\n### Ingestion\n\n* Built-in ingestion tools for Apache Spark/Apache Flink users.\n* Supports half-dozen file formats, database change logs and streaming data systems.\n* Connect sink for Apache Kafka, to bring external data sources.\n\n### Storage\n\n* Optimized storage format, supporting row & columnar data.\n* Timeline metadata to track history of changes\n* Automatically manages file sizes, layout using statistics\n* Savepoints for data versioning and recovery\n* Schema tracking and evolution.\n\n### Indexing\n\n* Scalable indexing subsystem to speed up snapshot queries, maintained automatically by writes.\n* Tracks file listings, column-level and partition-level statistics to help plan queries efficiently.\n* Record-level indexing mechanisms built on row-oriented file formats and bloom filters.\n* Logical partitioning on tables, using expression indexes to decouple from physical partitioning on storage.\n\n### Writing\n\n* Atomically commit data with rollback/restore support.\n* Fast upsert/delete support leveraging record-level indexes.\n* Snapshot isolation between writer & queries.\n* Optimistic concurrency control to implement relational data model, with Read-Modify-Write style consistent writes.\n* Non-blocking concurrency control, to implement streaming data model, with support for out-of-order, late data handling.\n\n### Queries\n\nHudi supports different types of queries, on top of a single table. \n\n* **Snapshot Query** - Provides a view of the table, as of the latest committed state, accelerated with indexes as applicable.\n* **Incremental Query** - Provides latest value of records inserted/updated, since a given point in time of the table. Can be used to \"diff\" table states between two points in time.\n* **Change-Data-Capture Query** - Provides a change stream with records inserted or updated or deleted since a point in time or between two points in time. Provides both before and after images for each change record.\n* **Time-Travel Query** - Provides a view of the table, as of a given point in time.\n* **Read Optimized Query** - Provides excellent snapshot query performance via purely columnar storage (e.g. [Parquet](https://parquet.apache.org/)), when used with a compaction policy to provide a transaction boundary.\n\n### Table Management\n\n* Automatic, hands-free table services runtime integrated into Spark/Flink writers or operated independently. \n* Configurable scheduling strategies with built-in failure handling, for all table services.\n* Cleaning older versions and time-to-live management to expire older data, reclaim storage space.\n* Clustering and space-filling curve algorithms to optimize data layout with pluggable scheduling strategies.\n* Asynchronous compaction of row oriented data into columnar formats, for efficient streaming writers.\n* Consistent index building in face of ongoing queries or writers.\n* Catalog sync with Apache Hive Metastore, AWS Glue, Google BigQuery, Apache XTable and more.\n\nLearn more about Hudi at [https://hudi.apache.org](https://hudi.apache.org)\n\n## Building Apache Hudi from source\n\nPrerequisites for building Apache Hudi:\n\n* Unix-like system (like Linux, Mac OS X)\n* Java 8, 11 or 17\n* Git\n* Maven (>=3.6.0)\n\n```\n# Checkout code and build\ngit clone https://github.com/apache/hudi.git && cd hudi\nmvn clean package -DskipTests\n\n# Start command\nspark-3.5.0-bin-hadoop3/bin/spark-shell \\\n  --jars `ls packaging/hudi-spark-bundle/target/hudi-spark3.5-bundle_2.12-*.*.*-SNAPSHOT.jar` \\\n  --conf 'spark.serializer=org.apache.spark.serializer.KryoSerializer' \\\n  --conf 'spark.sql.extensions=org.apache.spark.sql.hudi.HoodieSparkSessionExtension' \\\n  --conf 'spark.sql.catalog.spark_catalog=org.apache.spark.sql.hudi.catalog.HoodieCatalog' \\\n  --conf 'spark.kryo.registrator=org.apache.spark.HoodieSparkKryoRegistrar'\n```\n\nTo build for integration tests that include `hudi-integ-test-bundle`, use `-Dintegration-tests`.\n\nTo build the Javadoc for all Java and Scala classes:\n```\n# Javadoc generated under target/site/apidocs\nmvn clean javadoc:aggregate -Pjavadocs\n```\n\n### Build with different Spark versions\n\nThe default Spark 3.x version, corresponding to `spark3` profile is\n3.5.3. The default Scala version is 2.12. Scala 2.13 is supported for Spark 3.5 and above.\n\nRefer to the table below for building with different Spark and Scala versions.\n\n| Maven build options       | Expected Spark bundle jar name               | Notes                                            |\n|:--------------------------|:---------------------------------------------|:-------------------------------------------------|\n| (empty)                   | hudi-spark3.5-bundle_2.12                    | For Spark 3.5.x and Scala 2.12 (default options) |\n| `-Dspark3.3`              | hudi-spark3.3-bundle_2.12                    | For Spark 3.3.2+ and Scala 2.12                  |\n| `-Dspark3.4`              | hudi-spark3.4-bundle_2.12                    | For Spark 3.4.x and Scala 2.12                   |\n| `-Dspark3.5 -Dscala-2.12` | hudi-spark3.5-bundle_2.12                    | For Spark 3.5.x and Scala 2.12 (same as default) |\n| `-Dspark3.5 -Dscala-2.13` | hudi-spark3.5-bundle_2.13                    | For Spark 3.5.x and Scala 2.13                   |\n| `-Dspark3`                | hudi-spark3-bundle_2.12 (legacy bundle name) | For Spark 3.5.x and Scala 2.12                   |\n\nPlease note that only Spark-related bundles, i.e., `hudi-spark-bundle`, `hudi-utilities-bundle`,\n`hudi-utilities-slim-bundle`, can be built using `scala-2.13` profile. Hudi Flink bundle cannot be built\nusing `scala-2.13` profile. To build these bundles on Scala 2.13, use the following command:\n\n```\n# Build against Spark 3.5.x and Scala 2.13\nmvn clean package -DskipTests -Dspark3.5 -Dscala-2.13 -pl packaging/hudi-spark-bundle,packaging/hudi-utilities-bundle,packaging/hudi-utilities-slim-bundle -am\n```\n\nFor example,\n```\n# Build against Spark 3.5.x\nmvn clean package -DskipTests\n\n# Build against Spark 3.4.x\nmvn clean package -DskipTests -Dspark3.4\n```\n\n#### What about \"spark-avro\" module?\n\nStarting from versions 0.11, Hudi no longer requires `spark-avro` to be specified using `--packages`\n\n### Build with different Flink versions\n\nThe default Flink version supported is 1.20. The default Flink 1.20.x version, corresponding to `flink1.20` profile is 1.20.0.\nFlink is Scala-free since 1.15.x, there is no need to specify the Scala version for Flink 1.15.x and above versions.\nRefer to the table below for building with different Flink and Scala versions.\n\n| Maven build options | Expected Flink bundle jar name | Notes                            |\n|:--------------------|:-------------------------------|:---------------------------------|\n| (empty)             | hudi-flink1.20-bundle          | For Flink 1.20 (default options) |\n| `-Dflink1.20`       | hudi-flink1.20-bundle          | For Flink 1.20 (same as default) |\n| `-Dflink1.19`       | hudi-flink1.19-bundle          | For Flink 1.19                   |\n| `-Dflink1.18`       | hudi-flink1.18-bundle          | For Flink 1.18                   |\n| `-Dflink1.17`       | hudi-flink1.17-bundle          | For Flink 1.17                   |\n| `-Dflink1.16`       | hudi-flink1.16-bundle          | For Flink 1.16                   |\n| `-Dflink1.15`       | hudi-flink1.15-bundle          | For Flink 1.15                   |\n| `-Dflink1.14`       | hudi-flink1.14-bundle          | For Flink 1.14                   |\n\nFor example,\n```\n# Build against Flink 1.15.x\nmvn clean package -DskipTests -Dflink1.15\n```\n\n## Running Tests\n\nUnit tests can be run with maven profile `unit-tests`.\n```\nmvn -Punit-tests test\n```\n\nFunctional tests, which are tagged with `@Tag(\"functional\")`, can be run with maven profile `functional-tests`.\n```\nmvn -Pfunctional-tests test\n```\n\nIntegration tests can be run with maven profile `integration-tests`.\n```\nmvn -Pintegration-tests verify\n```\n\nTo run tests with spark event logging enabled, define the Spark event log directory. This allows visualizing test DAG and stages using Spark History Server UI.\n```\nmvn -Punit-tests test -DSPARK_EVLOG_DIR=/path/for/spark/event/log\n```\n\n## Quickstart\n\nPlease visit [https://hudi.apache.org/docs/quick-start-guide.html](https://hudi.apache.org/docs/quick-start-guide.html) to quickly explore Hudi's capabilities using spark-shell. \n\n## Contributing\n\nPlease check out our [contribution guide](https://hudi.apache.org/contribute/how-to-contribute) to learn more about how to contribute.\nFor code contributions, please refer to the [developer setup](https://hudi.apache.org/contribute/developer-setup).\n"
        },
        {
          "name": "azure-pipelines-20230430.yml",
          "type": "blob",
          "size": 15.416015625,
          "content": "# Licensed to the Apache Software Foundation (ASF) under one or more\n# contributor license agreements.  See the NOTICE file distributed with\n# this work for additional information regarding copyright ownership.\n# The ASF licenses this file to You under the Apache License, Version 2.0\n# (the \"License\"); you may not use this file except in compliance with\n# the License.  You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# NOTE:\n# This config file defines how Azure CI runs tests with Spark 2.4 and Flink 1.18 profiles.\n# PRs will need to keep in sync with master's version to trigger the CI runs.\n\ntrigger:\n  branches:\n    include:\n      - '*'  # must quote since \"*\" is a YAML reserved character; we want a string\n\npool:\n  vmImage: 'ubuntu-22.04'\n\nparameters:\n  - name: job1Modules\n    type: object\n    default:\n      - 'hudi-common'\n      - 'hudi-hadoop-common'\n      - 'hudi-client/hudi-spark-client'\n  - name: job2UTModules\n    type: object\n    default:\n      - 'hudi-flink-datasource'\n      - 'hudi-flink-datasource/hudi-flink'\n      - 'hudi-flink-datasource/hudi-flink1.14.x'\n      - 'hudi-flink-datasource/hudi-flink1.15.x'\n      - 'hudi-flink-datasource/hudi-flink1.16.x'\n      - 'hudi-flink-datasource/hudi-flink1.17.x'\n      - 'hudi-flink-datasource/hudi-flink1.18.x'\n      - 'hudi-flink-datasource/hudi-flink1.19.x'\n      - 'hudi-flink-datasource/hudi-flink1.20.x'\n  - name: job2FTModules\n    type: object\n    default:\n      - 'hudi-common'\n      - 'hudi-hadoop-common'\n      - 'hudi-flink-datasource'\n      - 'hudi-flink-datasource/hudi-flink'\n      - 'hudi-flink-datasource/hudi-flink1.14.x'\n      - 'hudi-flink-datasource/hudi-flink1.15.x'\n      - 'hudi-flink-datasource/hudi-flink1.16.x'\n      - 'hudi-flink-datasource/hudi-flink1.17.x'\n      - 'hudi-flink-datasource/hudi-flink1.18.x'\n      - 'hudi-flink-datasource/hudi-flink1.19.x'\n      - 'hudi-flink-datasource/hudi-flink1.20.x'\n      - 'hudi-client/hudi-spark-client'\n      - 'hudi-spark-datasource/hudi-spark'\n  - name: job34UTModules\n    type: object\n    default:\n      - 'hudi-spark-datasource'\n      - 'hudi-spark-datasource/hudi-spark'\n      - 'hudi-spark-datasource/hudi-spark3.5.x'\n      - 'hudi-spark-datasource/hudi-spark3-common'\n      - 'hudi-spark-datasource/hudi-spark-common'\n  - name: job6UTModules\n    type: object\n    default:\n      - '!hudi-hadoop-mr'\n      - '!hudi-client/hudi-java-client'\n      - '!hudi-client/hudi-spark-client'\n      - '!hudi-cli'\n      - '!hudi-common'\n      - '!hudi-hadoop-common'\n      - '!hudi-examples'\n      - '!hudi-examples/hudi-examples-common'\n      - '!hudi-examples/hudi-examples-flink'\n      - '!hudi-examples/hudi-examples-java'\n      - '!hudi-examples/hudi-examples-spark'\n      - '!hudi-flink-datasource'\n      - '!hudi-flink-datasource/hudi-flink'\n      - '!hudi-flink-datasource/hudi-flink1.14.x'\n      - '!hudi-flink-datasource/hudi-flink1.15.x'\n      - '!hudi-flink-datasource/hudi-flink1.16.x'\n      - '!hudi-flink-datasource/hudi-flink1.17.x'\n      - '!hudi-flink-datasource/hudi-flink1.18.x'\n      - '!hudi-flink-datasource/hudi-flink1.19.x'\n      - '!hudi-flink-datasource/hudi-flink1.20.x'\n      - '!hudi-spark-datasource'\n      - '!hudi-spark-datasource/hudi-spark'\n      - '!hudi-spark-datasource/hudi-spark3.5.x'\n      - '!hudi-spark-datasource/hudi-spark3-common'\n      - '!hudi-spark-datasource/hudi-spark-common'\n  - name: job6FTModules\n    type: object\n    default:\n      - '!hudi-client/hudi-spark-client'\n      - '!hudi-cli'\n      - '!hudi-common'\n      - '!hudi-hadoop-common'\n      - '!hudi-examples'\n      - '!hudi-examples/hudi-examples-common'\n      - '!hudi-examples/hudi-examples-flink'\n      - '!hudi-examples/hudi-examples-java'\n      - '!hudi-examples/hudi-examples-spark'\n      - '!hudi-flink-datasource'\n      - '!hudi-flink-datasource/hudi-flink'\n      - '!hudi-flink-datasource/hudi-flink1.14.x'\n      - '!hudi-flink-datasource/hudi-flink1.15.x'\n      - '!hudi-flink-datasource/hudi-flink1.16.x'\n      - '!hudi-flink-datasource/hudi-flink1.17.x'\n      - '!hudi-flink-datasource/hudi-flink1.18.x'\n      - '!hudi-flink-datasource/hudi-flink1.19.x'\n      - '!hudi-flink-datasource/hudi-flink1.20.x'\n      - '!hudi-spark-datasource/hudi-spark'\n  - name: job4HudiSparkDmlOthersWildcardSuites\n    type: object\n    default:\n      - 'org.apache.hudi'\n      - 'org.apache.spark.hudi'\n      - 'org.apache.spark.sql.avro'\n      - 'org.apache.spark.sql.execution'\n      - 'org.apache.spark.sql.hudi.analysis'\n      - 'org.apache.spark.sql.hudi.command'\n      - 'org.apache.spark.sql.hudi.common'\n      - 'org.apache.spark.sql.hudi.dml'\n\nvariables:\n  BUILD_PROFILES: '-Dscala-2.12 -Dspark3.5 -Dflink1.18'\n  PLUGIN_OPTS: '-Dcheckstyle.skip=true -Drat.skip=true -Djacoco.skip=true -ntp -B -V -Pwarn-log -Dorg.slf4j.simpleLogger.log.org.apache.maven.plugins.shade=warn -Dorg.slf4j.simpleLogger.log.org.apache.maven.plugins.dependency=warn'\n  MVN_OPTS_INSTALL: '-T 3 -Phudi-platform-service -DskipTests $(BUILD_PROFILES) $(PLUGIN_OPTS) -Dmaven.wagon.httpconnectionManager.ttlSeconds=25 -Dmaven.wagon.http.retryHandler.count=5'\n  MVN_OPTS_TEST: '-fae -Pwarn-log $(BUILD_PROFILES) $(PLUGIN_OPTS)'\n  JAVA_MVN_TEST_FILTER: '-DwildcardSuites=skipScalaTests -DfailIfNoTests=false'\n  SCALA_MVN_TEST_FILTER: '-Dtest=skipJavaTests -DfailIfNoTests=false'\n  JOB1_MODULES: ${{ join(',',parameters.job1Modules) }}\n  JOB2_UT_MODULES: ${{ join(',',parameters.job2UTModules) }}\n  JOB2_FT_MODULES: ${{ join(',',parameters.job2FTModules) }}\n  JOB34_MODULES: ${{ join(',',parameters.job34UTModules) }}\n  JOB3_SPARK_DDL_WILDCARD_SUITES: 'org.apache.spark.sql.hudi.ddl'\n  JOB6_SPARK_PROCEDURE_WILDCARD_SUITES: 'org.apache.spark.sql.hudi.procedure'\n  JOB4_SPARK_DML_OTHERS_WILDCARD_SUITES: ${{ join(',',parameters.job4HudiSparkDmlOthersWildcardSuites) }}\n  JOB6_UT_MODULES: ${{ join(',',parameters.job6UTModules) }}\n  JOB6_FT_MODULES: ${{ join(',',parameters.job6FTModules) }}\n\nstages:\n  - stage: test\n    variables:\n      - name: DOCKER_BUILDKIT\n        value: 1\n    jobs:\n      - job: UT_FT_1\n        displayName: UT common & client/spark-client\n        timeoutInMinutes: '90'\n        steps:\n          - task: Maven@4\n            displayName: maven install\n            inputs:\n              mavenPomFile: 'pom.xml'\n              goals: 'clean install'\n              options: $(MVN_OPTS_INSTALL) -pl $(JOB1_MODULES) -am\n              publishJUnitResults: false\n              jdkVersionOption: '1.8'\n          - task: Maven@4\n            displayName: UT common & client/spark-client\n            inputs:\n              mavenPomFile: 'pom.xml'\n              goals: 'test'\n              options: $(MVN_OPTS_TEST) -Punit-tests -pl $(JOB1_MODULES)\n              publishJUnitResults: true\n              testResultsFiles: '**/surefire-reports/TEST-*.xml'\n              jdkVersionOption: '1.8'\n              mavenOptions: '-Xmx4g'\n          - script: |\n              grep \"testcase\" */target/surefire-reports/*.xml */*/target/surefire-reports/*.xml | awk -F'\"' ' { print $6,$4,$2 } ' | sort -nr | head -n 100\n            displayName: Top 100 long-running testcases\n      - job: UT_FT_2\n        displayName: UT flink & FT common & flink & spark-client & hudi-spark\n        timeoutInMinutes: '120'\n        steps:\n          - task: Maven@4\n            displayName: maven install\n            inputs:\n              mavenPomFile: 'pom.xml'\n              goals: 'clean install'\n              options: $(MVN_OPTS_INSTALL) -pl $(JOB2_FT_MODULES) -am\n              publishJUnitResults: false\n              jdkVersionOption: '1.8'\n          - task: Maven@4\n            displayName: UT flink\n            inputs:\n              mavenPomFile: 'pom.xml'\n              goals: 'test'\n              options: $(MVN_OPTS_TEST) -Punit-tests -pl $(JOB2_UT_MODULES)\n              publishJUnitResults: false\n              jdkVersionOption: '1.8'\n              mavenOptions: '-Xmx4g'\n          - task: Maven@4\n            displayName: FT common & flink & client/spark-client & hudi-spark-datasource/hudi-spark\n            inputs:\n              mavenPomFile: 'pom.xml'\n              goals: 'test'\n              options: $(MVN_OPTS_TEST) -Pfunctional-tests -pl $(JOB2_FT_MODULES)\n              publishJUnitResults: true\n              testResultsFiles: '**/surefire-reports/TEST-*.xml'\n              jdkVersionOption: '1.8'\n              mavenOptions: '-Xmx4g'\n          - script: |\n              grep \"testcase\" */target/surefire-reports/*.xml */*/target/surefire-reports/*.xml | awk -F'\"' ' { print $6,$4,$2 } ' | sort -nr | head -n 100\n            displayName: Top 100 long-running testcases\n      - job: UT_FT_3\n        displayName: UT spark-datasource Java Tests & DDL\n        timeoutInMinutes: '120'\n        steps:\n          - task: Maven@4\n            displayName: maven install\n            inputs:\n              mavenPomFile: 'pom.xml'\n              goals: 'clean install'\n              options: $(MVN_OPTS_INSTALL) -pl $(JOB34_MODULES) -am\n              publishJUnitResults: false\n              jdkVersionOption: '1.8'\n          - task: Maven@4\n            displayName: Java UT spark-datasource\n            inputs:\n              mavenPomFile: 'pom.xml'\n              goals: 'test'\n              options: $(MVN_OPTS_TEST) -Punit-tests $(JAVA_MVN_TEST_FILTER) -pl $(JOB34_MODULES)\n              publishJUnitResults: false\n              jdkVersionOption: '1.8'\n              mavenOptions: '-Xmx4g'\n          - task: Maven@4\n            displayName: Scala UT spark-datasource DDL\n            inputs:\n              mavenPomFile: 'pom.xml'\n              goals: 'test'\n              options: $(MVN_OPTS_TEST) -Punit-tests $(SCALA_MVN_TEST_FILTER) -DwildcardSuites=\"$(JOB3_SPARK_DDL_WILDCARD_SUITES)\" -pl $(JOB34_MODULES)\n              publishJUnitResults: true\n              testResultsFiles: '**/surefire-reports/TEST-*.xml'\n              jdkVersionOption: '1.8'\n              mavenOptions: '-Xmx4g'\n          - script: |\n              grep \"testcase\" */target/surefire-reports/*.xml */*/target/surefire-reports/*.xml | awk -F'\"' ' { print $6,$4,$2 } ' | sort -nr | head -n 100\n            displayName: Top 100 long-running testcases\n      - job: UT_FT_4\n        displayName: UT spark-datasource DML & others\n        timeoutInMinutes: '120'\n        steps:\n          - task: Maven@4\n            displayName: maven install\n            inputs:\n              mavenPomFile: 'pom.xml'\n              goals: 'clean install'\n              options: $(MVN_OPTS_INSTALL) -pl $(JOB34_MODULES) -am\n              publishJUnitResults: false\n              jdkVersionOption: '1.8'\n          - task: Maven@4\n            displayName: Scala UT spark-datasource DML & others\n            inputs:\n              mavenPomFile: 'pom.xml'\n              goals: 'test'\n              options: $(MVN_OPTS_TEST) -Punit-tests $(SCALA_MVN_TEST_FILTER) -DwildcardSuites=\"$(JOB4_SPARK_DML_OTHERS_WILDCARD_SUITES)\" -pl $(JOB34_MODULES)\n              publishJUnitResults: true\n              testResultsFiles: '**/surefire-reports/TEST-*.xml'\n              jdkVersionOption: '1.8'\n              mavenOptions: '-Xmx4g'\n          - script: |\n              grep \"testcase\" */target/surefire-reports/*.xml */*/target/surefire-reports/*.xml | awk -F'\"' ' { print $6,$4,$2 } ' | sort -nr | head -n 100\n            displayName: Top 100 long-running testcases\n      - job: UT_FT_5\n        displayName: UT FT Hudi Streamer\n        timeoutInMinutes: '90'\n        steps:\n          - task: Docker@2\n            displayName: \"login to docker hub\"\n            inputs:\n              command: \"login\"\n              containerRegistry: \"apachehudi-docker-hub\"\n          - task: Docker@2\n            displayName: \"load repo into image\"\n            inputs:\n              containerRegistry: 'apachehudi-docker-hub'\n              repository: 'apachehudi/hudi-ci-bundle-validation-base'\n              command: 'build'\n              Dockerfile: '**/Dockerfile'\n              ImageName: $(Build.BuildId)\n          - task: Docker@2\n            displayName: \"UT FT other modules\"\n            inputs:\n              containerRegistry: 'apachehudi-docker-hub'\n              repository: 'apachehudi/hudi-ci-bundle-validation-base'\n              command: 'run'\n              arguments: >\n                -v $(Build.SourcesDirectory):/hudi\n                -i docker.io/apachehudi/hudi-ci-bundle-validation-base:$(Build.BuildId)\n                /bin/bash -c \"mvn clean install $(MVN_OPTS_INSTALL) -Phudi-platform-service -Pthrift-gen-source -pl hudi-utilities -am\n                && mvn test  $(MVN_OPTS_TEST) -Punit-tests -Dtest=\"Test*DeltaStreamer*\" -DfailIfNoTests=false -pl hudi-utilities\n                && mvn test  $(MVN_OPTS_TEST) -Pfunctional-tests -Dtest=\"Test*DeltaStreamer*\" -DfailIfNoTests=false -pl hudi-utilities\"\n          - task: PublishTestResults@2\n            displayName: 'Publish Test Results'\n            inputs:\n              testResultsFormat: 'JUnit'\n              testResultsFiles: '**/surefire-reports/TEST-*.xml'\n              searchFolder: '$(Build.SourcesDirectory)'\n              failTaskOnFailedTests: true\n          - script: |\n              grep \"testcase\" */target/surefire-reports/*.xml */*/target/surefire-reports/*.xml | awk -F'\"' ' { print $6,$4,$2 } ' | sort -nr | head -n 100\n            displayName: Top 100 long-running testcases\n      - job: UT_FT_6\n        displayName: UT FT other modules\n        timeoutInMinutes: '90'\n        steps:\n          - task: Docker@2\n            displayName: \"login to docker hub\"\n            inputs:\n              command: \"login\"\n              containerRegistry: \"apachehudi-docker-hub\"\n          - task: Docker@2\n            displayName: \"load repo into image\"\n            inputs:\n              containerRegistry: 'apachehudi-docker-hub'\n              repository: 'apachehudi/hudi-ci-bundle-validation-base'\n              command: 'build'\n              Dockerfile: '**/Dockerfile'\n              ImageName: $(Build.BuildId)\n          - task: Docker@2\n            displayName: \"UT FT other modules\"\n            inputs:\n              containerRegistry: 'apachehudi-docker-hub'\n              repository: 'apachehudi/hudi-ci-bundle-validation-base'\n              command: 'run'\n              arguments: >\n                -v $(Build.SourcesDirectory):/hudi\n                -i docker.io/apachehudi/hudi-ci-bundle-validation-base:$(Build.BuildId)\n                /bin/bash -c \"mvn clean install $(MVN_OPTS_INSTALL) -Phudi-platform-service -Pthrift-gen-source\n                && mvn test  $(MVN_OPTS_TEST) -Punit-tests $(SCALA_MVN_TEST_FILTER) -DwildcardSuites=\"$(JOB6_SPARK_PROCEDURE_WILDCARD_SUITES)\" -pl $(JOB34_MODULES)\n                && mvn test  $(MVN_OPTS_TEST) -Punit-tests -Dtest=\"!Test*DeltaStreamer*\" -DfailIfNoTests=false -pl $(JOB6_UT_MODULES)\n                && mvn test  $(MVN_OPTS_TEST) -Pfunctional-tests -Dtest=\"!Test*DeltaStreamer*\" -DfailIfNoTests=false -pl $(JOB6_FT_MODULES)\"\n          - task: PublishTestResults@2\n            displayName: 'Publish Test Results'\n            inputs:\n              testResultsFormat: 'JUnit'\n              testResultsFiles: '**/surefire-reports/TEST-*.xml'\n              searchFolder: '$(Build.SourcesDirectory)'\n              failTaskOnFailedTests: true\n          - script: |\n              grep \"testcase\" */target/surefire-reports/*.xml */*/target/surefire-reports/*.xml | awk -F'\"' ' { print $6,$4,$2 } ' | sort -nr | head -n 100\n            displayName: Top 100 long-running testcases\n"
        },
        {
          "name": "conf",
          "type": "tree",
          "content": null
        },
        {
          "name": "dependencies",
          "type": "tree",
          "content": null
        },
        {
          "name": "doap_HUDI.rdf",
          "type": "blob",
          "size": 6.5693359375,
          "content": "<?xml version=\"1.0\"?>\n<?xml-stylesheet type=\"text/xsl\"?>\n<rdf:RDF xml:lang=\"en\"\n         xmlns=\"http://usefulinc.com/ns/doap#\"\n         xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\"\n         xmlns:asfext=\"http://projects.apache.org/ns/asfext#\"\n         xmlns:foaf=\"http://xmlns.com/foaf/0.1/\">\n<!--\n    Licensed to the Apache Software Foundation (ASF) under one or more\n    contributor license agreements.  See the NOTICE file distributed with\n    this work for additional information regarding copyright ownership.\n    The ASF licenses this file to You under the Apache License, Version 2.0\n    (the \"License\"); you may not use this file except in compliance with\n    the License.  You may obtain a copy of the License at\n\n         http://www.apache.org/licenses/LICENSE-2.0\n\n    Unless required by applicable law or agreed to in writing, software\n    distributed under the License is distributed on an \"AS IS\" BASIS,\n    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n    See the License for the specific language governing permissions and\n    limitations under the License.\n-->\n  <Project rdf:about=\"https://hudi.apache.org\">\n    <created>2019-12-31</created>\n    <license rdf:resource=\"http://usefulinc.com/doap/licenses/asl20\" />\n    <name>Apache Hudi</name>\n    <homepage rdf:resource=\"https://hudi.apache.org\" />\n    <asfext:pmc rdf:resource=\"https://hudi.apache.org\" />\n    <shortdesc>high-performance open data lakehouse platform</shortdesc>\n    <description>Hudi brings transactions, stream processing, indexes, mutability and incremental processing to data lakes.</description>\n    <bug-database rdf:resource=\"https://issues.apache.org/jira/browse/HUDI\" />\n    <mailing-list rdf:resource=\"https://hudi.apache.org/community.html\" />\n    <download-page rdf:resource=\"https://hudi.apache.org/community.html\" />\n    <programming-language>Java</programming-language>\n    <programming-language>Scala</programming-language>\n    <category rdf:resource=\"http://projects.apache.org/category/library\" />\n    <release>\n      <Version>\n        <name>Apache Hudi-incubating 0.5.0</name>\n        <created>2019-10-24</created>\n        <revision>0.5.0</revision>\n      </Version>\n    </release>\n    <release>\n      <Version>\n        <name>Apache Hudi-incubating 0.5.1</name>\n        <created>2020-01-31</created>\n        <revision>0.5.1</revision>\n      </Version>\n    </release>\n    <release>\n      <Version>\n        <name>Apache Hudi-incubating 0.5.2</name>\n        <created>2020-03-26</created>\n        <revision>0.5.2</revision>\n      </Version>\n    </release>\n    <release>\n      <Version>\n        <name>Apache Hudi 0.5.3</name>\n        <created>2020-06-16</created>\n        <revision>0.5.3</revision>\n      </Version>\n    </release>\n    <release>\n      <Version>\n        <name>Apache Hudi 0.6.0</name>\n        <created>2020-08-22</created>\n        <revision>0.6.0</revision>\n      </Version>\n    </release>\n    <release>\n      <Version>\n        <name>Apache Hudi 0.7.0</name>\n        <created>2021-01-25</created>\n        <revision>0.7.0</revision>\n      </Version>\n    </release>\n    <release>\n      <Version>\n        <name>Apache Hudi 0.8.0</name>\n        <created>2021-04-06</created>\n        <revision>0.8.0</revision>\n      </Version>\n    </release>\n    <release>\n      <Version>\n        <name>Apache Hudi 0.9.0</name>\n        <created>2021-08-26</created>\n        <revision>0.9.0</revision>\n      </Version>\n    </release>\n    <release>\n      <Version>\n        <name>Apache Hudi 0.10.0</name>\n        <created>2021-12-08</created>\n        <revision>0.10.0</revision>\n      </Version>\n    </release>\n    <release>\n      <Version>\n        <name>Apache Hudi 0.10.1</name>\n        <created>2022-01-26</created>\n        <revision>0.10.1</revision>\n      </Version>\n    </release>\n    <release>\n      <Version>\n        <name>Apache Hudi 0.11.0</name>\n        <created>2022-04-30</created>\n        <revision>0.11.0</revision>\n      </Version>\n    </release>\n    <release>\n      <Version>\n        <name>Apache Hudi 0.11.1</name>\n        <created>2022-06-18</created>\n        <revision>0.11.1</revision>\n      </Version>\n    </release>\n    <release>\n      <Version>\n        <name>Apache Hudi 0.12.0</name>\n        <created>2022-08-16</created>\n        <revision>0.12.0</revision>\n      </Version>\n    </release>\n    <release>\n      <Version>\n        <name>Apache Hudi 0.12.1</name>\n        <created>2022-10-18</created>\n        <revision>0.12.1</revision>\n      </Version>\n    </release>\n    <release>\n      <Version>\n        <name>Apache Hudi 0.12.2</name>\n        <created>2022-12-28</created>\n        <revision>0.12.2</revision>\n      </Version>\n    </release>\n    <release>\n      <Version>\n        <name>Apache Hudi 0.13.0</name>\n        <created>2023-02-25</created>\n        <revision>0.13.0</revision>\n      </Version>\n    </release>\n    <release>\n      <Version>\n        <name>Apache Hudi 0.12.3</name>\n        <created>2023-04-23</created>\n        <revision>0.12.3</revision>\n      </Version>\n    </release>\n    <release>\n      <Version>\n        <name>Apache Hudi 0.13.1</name>\n        <created>2023-05-25</created>\n        <revision>0.13.1</revision>\n      </Version>\n    </release>\n    <release>\n      <Version>\n        <name>Apache Hudi 0.14.0</name>\n        <created>2023-09-28</created>\n        <revision>0.14.0</revision>\n      </Version>\n    </release>\n    <release>\n      <Version>\n        <name>Apache Hudi 1.0.0-beta1</name>\n        <created>2023-11-14</created>\n        <revision>1.0.0-beta1</revision>\n      </Version>\n    </release>\n    <release>\n      <Version> \n        <name>Apache Hudi 0.14.1</name> \n        <created>2024-01-04</created>\n        <revision>0.14.1</revision>\n      </Version>\n    </release>\n    <release>\n      <Version>\n        <name>Apache Hudi 0.15.0</name>\n        <created>2024-06-04</created>\n        <revision>0.15.0</revision>\n      </Version>\n    </release>\n    <release>\n      <Version>\n        <name>Apache Hudi 1.0.0-beta2</name>\n        <created>2024-07-14</created>\n        <revision>1.0.0-beta2</revision>\n      </Version>\n    </release>\n    <release>\n      <Version>\n        <name>Apache Hudi 1.0.0</name>\n        <created>2024-12-11</created>\n        <revision>1.0.0</revision>\n      </Version>\n    </release>\n    <repository>\n      <GitRepository>\n        <location rdf:resource=\"https://github.com/apache/hudi.git\"/>\n        <browse rdf:resource=\"https://github.com/apache/hudi\"/>\n      </GitRepository>\n    </repository>\n    <maintainer>\n      <foaf:Person>\n        <foaf:name>Apache Hudi PMC</foaf:name>\n          <foaf:mbox rdf:resource=\"mailto:dev@hudi.apache.org\"/>\n      </foaf:Person>\n    </maintainer>\n  </Project>\n</rdf:RDF>\n"
        },
        {
          "name": "docker",
          "type": "tree",
          "content": null
        },
        {
          "name": "hudi-aws",
          "type": "tree",
          "content": null
        },
        {
          "name": "hudi-cli",
          "type": "tree",
          "content": null
        },
        {
          "name": "hudi-client",
          "type": "tree",
          "content": null
        },
        {
          "name": "hudi-common",
          "type": "tree",
          "content": null
        },
        {
          "name": "hudi-examples",
          "type": "tree",
          "content": null
        },
        {
          "name": "hudi-flink-datasource",
          "type": "tree",
          "content": null
        },
        {
          "name": "hudi-gcp",
          "type": "tree",
          "content": null
        },
        {
          "name": "hudi-hadoop-common",
          "type": "tree",
          "content": null
        },
        {
          "name": "hudi-hadoop-mr",
          "type": "tree",
          "content": null
        },
        {
          "name": "hudi-integ-test",
          "type": "tree",
          "content": null
        },
        {
          "name": "hudi-io",
          "type": "tree",
          "content": null
        },
        {
          "name": "hudi-kafka-connect",
          "type": "tree",
          "content": null
        },
        {
          "name": "hudi-platform-service",
          "type": "tree",
          "content": null
        },
        {
          "name": "hudi-spark-datasource",
          "type": "tree",
          "content": null
        },
        {
          "name": "hudi-sync",
          "type": "tree",
          "content": null
        },
        {
          "name": "hudi-tests-common",
          "type": "tree",
          "content": null
        },
        {
          "name": "hudi-timeline-service",
          "type": "tree",
          "content": null
        },
        {
          "name": "hudi-utilities",
          "type": "tree",
          "content": null
        },
        {
          "name": "packaging",
          "type": "tree",
          "content": null
        },
        {
          "name": "pom.xml",
          "type": "blob",
          "size": 101.333984375,
          "content": "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n\n<!--\n  Licensed to the Apache Software Foundation (ASF) under one or more\n  contributor license agreements.  See the NOTICE file distributed with\n  this work for additional information regarding copyright ownership.\n  The ASF licenses this file to You under the Apache License, Version 2.0\n  (the \"License\"); you may not use this file except in compliance with\n  the License.  You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n  Unless required by applicable law or agreed to in writing, software\n  distributed under the License is distributed on an \"AS IS\" BASIS,\n  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n  See the License for the specific language governing permissions and\n  limitations under the License.\n-->\n\n<project xmlns=\"http://maven.apache.org/POM/4.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\">\n  <modelVersion>4.0.0</modelVersion>\n\n  <parent>\n    <groupId>org.apache</groupId>\n    <artifactId>apache</artifactId>\n    <version>21</version>\n  </parent>\n\n  <groupId>org.apache.hudi</groupId>\n  <artifactId>hudi</artifactId>\n  <packaging>pom</packaging>\n  <version>1.1.0-SNAPSHOT</version>\n  <description>Apache Hudi brings stream style processing on big data</description>\n  <url>https://github.com/apache/hudi</url>\n  <name>Hudi</name>\n\n  <modules>\n    <module>hudi-common</module>\n    <module>hudi-cli</module>\n    <module>hudi-client</module>\n    <module>hudi-aws</module>\n    <module>hudi-gcp</module>\n    <module>hudi-hadoop-common</module>\n    <module>hudi-hadoop-mr</module>\n    <module>hudi-io</module>\n    <module>hudi-spark-datasource</module>\n    <module>hudi-timeline-service</module>\n    <module>hudi-utilities</module>\n    <module>hudi-sync</module>\n    <module>packaging/hudi-hadoop-mr-bundle</module>\n    <module>packaging/hudi-datahub-sync-bundle</module>\n    <module>packaging/hudi-hive-sync-bundle</module>\n    <module>packaging/hudi-aws-bundle</module>\n    <module>packaging/hudi-gcp-bundle</module>\n    <module>packaging/hudi-spark-bundle</module>\n    <module>packaging/hudi-presto-bundle</module>\n    <module>packaging/hudi-utilities-bundle</module>\n    <module>packaging/hudi-utilities-slim-bundle</module>\n    <module>packaging/hudi-timeline-server-bundle</module>\n    <module>packaging/hudi-trino-bundle</module>\n    <module>hudi-examples</module>\n    <module>hudi-flink-datasource</module>\n    <module>hudi-kafka-connect</module>\n    <module>packaging/hudi-flink-bundle</module>\n    <module>packaging/hudi-kafka-connect-bundle</module>\n    <module>packaging/hudi-cli-bundle</module>\n    <module>hudi-tests-common</module>\n  </modules>\n\n  <licenses>\n    <license>\n      <name>Apache License, Version 2.0</name>\n      <url>http://www.apache.org/licenses/LICENSE-2.0.txt</url>\n      <distribution>repo</distribution>\n    </license>\n  </licenses>\n\n  <organization>\n    <name>The Apache Software Foundation</name>\n    <url>https://www.apache.org</url>\n  </organization>\n\n  <properties>\n    <maven-jar-plugin.version>3.2.0</maven-jar-plugin.version>\n    <maven-surefire-plugin.version>2.22.2</maven-surefire-plugin.version>\n    <maven-failsafe-plugin.version>2.22.2</maven-failsafe-plugin.version>\n    <maven-shade-plugin.version>3.4.0</maven-shade-plugin.version>\n    <maven-javadoc-plugin.version>3.1.1</maven-javadoc-plugin.version>\n    <maven-compiler-plugin.version>3.8.0</maven-compiler-plugin.version>\n    <maven-deploy-plugin.version>2.4</maven-deploy-plugin.version>\n    <genjavadoc-plugin.version>0.15</genjavadoc-plugin.version>\n    <build-helper-maven-plugin.version>1.7</build-helper-maven-plugin.version>\n    <maven-enforcer-plugin.version>3.0.0-M1</maven-enforcer-plugin.version>\n    <maven-docker-plugin.version>0.45.0</maven-docker-plugin.version>\n\n    <java.version>8</java.version>\n    <kryo.shaded.version>4.0.2</kryo.shaded.version>\n    <fasterxml.spark3.version>2.10.0</fasterxml.spark3.version>\n    <fasterxml.version>${fasterxml.spark3.version}</fasterxml.version>\n    <fasterxml.jackson.databind.version>${fasterxml.spark3.version}</fasterxml.jackson.databind.version>\n    <fasterxml.jackson.module.scala.version>${fasterxml.spark3.version}</fasterxml.jackson.module.scala.version>\n    <fasterxml.jackson.dataformat.yaml.version>${fasterxml.spark3.version}</fasterxml.jackson.dataformat.yaml.version>\n    <kafka.version>2.0.0</kafka.version>\n    <pulsar.version>3.0.2</pulsar.version>\n    <pulsar.spark.version>${pulsar.spark.scala12.version}</pulsar.spark.version>\n    <pulsar.spark.scala12.version>3.1.1.4</pulsar.spark.scala12.version>\n    <pulsar.spark.scala13.version>3.4.1.1</pulsar.spark.scala13.version>\n    <confluent.version>5.5.0</confluent.version>\n    <glassfish.version>2.17</glassfish.version>\n    <glassfish.el.version>3.0.1-b12</glassfish.el.version>\n    <parquet.version>1.10.1</parquet.version>\n    <junit.jupiter.version>5.8.2</junit.jupiter.version>\n    <junit.vintage.version>5.8.2</junit.vintage.version>\n    <junit.platform.version>1.8.2</junit.platform.version>\n    <mockito.jupiter.version>3.12.4</mockito.jupiter.version>\n    <log4j2.version>2.17.2</log4j2.version>\n    <slf4j.version>1.7.36</slf4j.version>\n    <joda.version>2.9.9</joda.version>\n    <hadoop.version>2.10.2</hadoop.version>\n    <hive.groupid>org.apache.hive</hive.groupid>\n    <hive.version>2.3.4</hive.version>\n    <hive.parquet.version>1.10.1</hive.parquet.version>\n    <hive.avro.version>1.11.4</hive.avro.version>\n    <presto.version>0.273</presto.version>\n    <trino.version>390</trino.version>\n    <hive.exec.classifier>core</hive.exec.classifier>\n    <metrics.version>4.1.1</metrics.version>\n    <orc.spark.version>1.6.0</orc.spark.version>\n    <orc.flink.version>1.5.6</orc.flink.version>\n    <roaringbitmap.version>0.9.47</roaringbitmap.version>\n    <airlift.version>0.27</airlift.version>\n    <tally.version>0.13.0</tally.version>\n    <prometheus.version>0.8.0</prometheus.version>\n    <aws.sdk.httpclient.version>4.5.13</aws.sdk.httpclient.version>\n    <aws.sdk.httpcore.version>4.4.13</aws.sdk.httpcore.version>\n    <httpcore.version>4.4.16</httpcore.version>\n    <httpclient.version>4.5.14</httpclient.version>\n    <spark.version>${spark3.version}</spark.version>\n    <spark3.version>3.5.1</spark3.version>\n    <sparkbundle.version></sparkbundle.version>\n    <flink1.20.version>1.20.0</flink1.20.version>\n    <flink1.19.version>1.19.1</flink1.19.version>\n    <flink1.18.version>1.18.1</flink1.18.version>\n    <flink1.17.version>1.17.1</flink1.17.version>\n    <flink1.16.version>1.16.2</flink1.16.version>\n    <flink1.15.version>1.15.1</flink1.15.version>\n    <flink1.14.version>1.14.5</flink1.14.version>\n    <flink.version>${flink1.20.version}</flink.version>\n    <hudi.flink.module>hudi-flink1.20.x</hudi.flink.module>\n    <flink.bundle.version>1.20</flink.bundle.version>\n    <!-- This is fixed to match with version from flink-avro -->\n    <flink.avro.version>1.11.4</flink.avro.version>\n    <flink.format.parquet.version>1.13.1</flink.format.parquet.version>\n    <!-- check kafka version -->\n    <flink.connector.kafka.version>3.2.0-1.19</flink.connector.kafka.version> \n    <flink.runtime.artifactId>flink-runtime</flink.runtime.artifactId>\n    <flink.table.runtime.artifactId>flink-table-runtime</flink.table.runtime.artifactId>\n    <flink.table.planner.artifactId>flink-table-planner_2.12</flink.table.planner.artifactId>\n    <flink.parquet.artifactId>flink-parquet</flink.parquet.artifactId>\n    <flink.statebackend.rocksdb.artifactId>flink-statebackend-rocksdb</flink.statebackend.rocksdb.artifactId>\n    <flink.test.utils.artifactId>flink-test-utils</flink.test.utils.artifactId>\n    <flink.streaming.java.artifactId>flink-streaming-java</flink.streaming.java.artifactId>\n    <flink.clients.artifactId>flink-clients</flink.clients.artifactId>\n    <flink.connector.kafka.artifactId>flink-connector-kafka</flink.connector.kafka.artifactId>\n    <flink.hadoop.compatibility.artifactId>flink-hadoop-compatibility_2.12</flink.hadoop.compatibility.artifactId>\n    <rocksdbjni.version>7.5.3</rocksdbjni.version>\n    <spark33.version>3.3.4</spark33.version>\n    <spark34.version>3.4.3</spark34.version>\n    <spark35.version>3.5.4</spark35.version>\n    <hudi.spark.module>hudi-spark3.5.x</hudi.spark.module>\n    <hudi.spark.common.module>hudi-spark3-common</hudi.spark.common.module>\n    <avro.version>1.11.4</avro.version>\n    <bijection-avro.version>0.9.8</bijection-avro.version>\n    <caffeine.version>2.9.1</caffeine.version>\n    <commons.io.version>2.11.0</commons.io.version>\n    <scala12.version>2.12.15</scala12.version>\n    <scala13.version>2.13.8</scala13.version>\n    <scala.version>${scala12.version}</scala.version>\n    <scala.collection-compat.version>2.8.1</scala.collection-compat.version>\n    <scala.binary.version>2.12</scala.binary.version>\n    <apache-rat-plugin.version>0.16.1</apache-rat-plugin.version>\n    <scala-maven-plugin.version>3.3.1</scala-maven-plugin.version>\n    <scalatest.spark3.version>3.1.0</scalatest.spark3.version>\n    <scalatest.version>${scalatest.spark3.version}</scalatest.version>\n    <surefire-log4j.file>log4j2-surefire.properties</surefire-log4j.file>\n    <thrift.version>0.13.0</thrift.version>\n    <javalin.version>4.6.7</javalin.version>\n    <jetty.version>9.4.53.v20231009</jetty.version>\n    <htrace.version>3.1.0-incubating</htrace.version>\n    <hbase.version>2.4.13</hbase.version>\n    <h2.version>1.4.199</h2.version>\n    <awaitility.version>3.1.2</awaitility.version>\n    <skipTests>false</skipTests>\n    <skipUTs>${skipTests}</skipUTs>\n    <skipFTs>${skipTests}</skipFTs>\n    <skipITs>${skipTests}</skipITs>\n    <skip.hudi-spark3.unit.tests>${skipTests}</skip.hudi-spark3.unit.tests>\n    <skipDocker>${skipTests}</skipDocker>\n    <project.build.sourceEncoding>UTF-8</project.build.sourceEncoding>\n    <main.basedir>${project.basedir}</main.basedir>\n    <spark.bundle.hive.scope>provided</spark.bundle.hive.scope>\n    <spark.bundle.hive.shade.prefix/>\n    <utilities.bundle.hive.scope>provided</utilities.bundle.hive.scope>\n    <utilities.bundle.hive.shade.prefix/>\n    <argLine>-Xmx2g -Xms128m</argLine>\n    <jacoco.version>0.8.8</jacoco.version>\n    <presto.bundle.bootstrap.scope>compile</presto.bundle.bootstrap.scope>\n    <presto.bundle.bootstrap.shade.prefix>org.apache.hudi.</presto.bundle.bootstrap.shade.prefix>\n    <trino.bundle.bootstrap.scope>compile</trino.bundle.bootstrap.scope>\n    <trino.bundle.bootstrap.shade.prefix>org.apache.hudi.</trino.bundle.bootstrap.shade.prefix>\n    <shadeSources>true</shadeSources>\n    <zk-curator.version>2.7.1</zk-curator.version>\n    <disruptor.version>3.4.2</disruptor.version>\n    <antlr.version>4.8</antlr.version>\n    <aws.sdk.version>2.25.69</aws.sdk.version>\n    <proto.version>3.25.5</proto.version>\n    <protoc.version>3.25.5</protoc.version>\n    <dynamodb.lockclient.version>1.2.0</dynamodb.lockclient.version>\n    <zookeeper.version>3.5.7</zookeeper.version>\n    <openjdk.jol.version>0.16</openjdk.jol.version>\n    <google.cloud.pubsub.version>1.120.0</google.cloud.pubsub.version>\n    <gcp-libraries-bom.version>26.15.0</gcp-libraries-bom.version>\n    <gcs.connector.version>hadoop2-2.2.7</gcs.connector.version>\n    <dynamodb-local.port>8000</dynamodb-local.port>\n    <dynamodb-local.endpoint>http://localhost:${dynamodb-local.port}</dynamodb-local.endpoint>\n    <moto.port>5002</moto.port>\n    <moto.endpoint>http://localhost:${moto.port}</moto.endpoint>\n    <springboot.version>2.7.3</springboot.version>\n    <spring.shell.version>2.1.1</spring.shell.version>\n    <snappy.version>1.1.10.7</snappy.version>\n  </properties>\n\n  <scm>\n    <connection>scm:git:git@github.com:apache/hudi.git</connection>\n    <developerConnection>scm:git:git@github.com:apache/hudi.git</developerConnection>\n    <url>git@github.com:apache/hudi.git</url>\n    <tag>HEAD</tag>\n  </scm>\n\n  <issueManagement>\n    <system>JIRA</system>\n    <url>https://issues.apache.org/jira/browse/HUDI</url>\n  </issueManagement>\n\n  <mailingLists>\n    <mailingList>\n      <name>Dev Mailing List</name>\n      <post>dev@hudi.apache.org</post>\n      <subscribe>dev-subscribe@hudi.apache.org</subscribe>\n      <unsubscribe>dev-unsubscribe@hudi.apache.org</unsubscribe>\n    </mailingList>\n    <mailingList>\n      <name>User Mailing List</name>\n      <post>users@hudi.apache.org</post>\n      <subscribe>users-subscribe@hudi.apache.org</subscribe>\n      <unsubscribe>users-unsubscribe@hudi.apache.org</unsubscribe>\n    </mailingList>\n    <mailingList>\n      <name>Commits Mailing List</name>\n      <post>commits@hudi.apache.org</post>\n      <subscribe>commits-subscribe@hudi.apache.org</subscribe>\n      <unsubscribe>commits-unsubscribe@hudi.apache.org</unsubscribe>\n    </mailingList>\n  </mailingLists>\n\n  <build>\n    <plugins>\n      <plugin>\n        <groupId>org.apache.maven.plugins</groupId>\n        <artifactId>maven-source-plugin</artifactId>\n        <version>2.2.1</version>\n        <executions>\n          <execution>\n            <id>attach-sources</id>\n            <goals>\n              <goal>jar-no-fork</goal>\n            </goals>\n          </execution>\n        </executions>\n      </plugin>\n      <plugin>\n        <groupId>org.apache.maven.plugins</groupId>\n        <artifactId>maven-checkstyle-plugin</artifactId>\n        <version>3.4.0</version>\n        <dependencies>\n          <dependency>\n            <groupId>com.puppycrawl.tools</groupId>\n            <artifactId>checkstyle</artifactId>\n            <version>9.3</version>\n          </dependency>\n        </dependencies>\n        <configuration>\n          <!-- Set consoleOutput to true to see minor checkstyle issues -->\n          <consoleOutput>false</consoleOutput>\n          <configLocation>style/checkstyle.xml</configLocation>\n          <suppressionsLocation>style/checkstyle-suppressions.xml</suppressionsLocation>\n          <suppressionsFileExpression>checkstyle.suppressions.file</suppressionsFileExpression>\n          <failOnViolation>true</failOnViolation>\n          <violationSeverity>warning</violationSeverity>\n          <includeTestSourceDirectory>true</includeTestSourceDirectory>\n          <sourceDirectories>\n            <sourceDirectory>${project.build.sourceDirectory}</sourceDirectory>\n          </sourceDirectories>\n          <!-- NOTE: This property is only available in Maven >= 3.3.1 -->\n          <propertyExpansion>basedir=${maven.multiModuleProjectDirectory}</propertyExpansion>\n          <excludes>**\\/generated-sources\\/,org/apache/hudi/metaserver/thrift/*</excludes>\n        </configuration>\n        <executions>\n          <execution>\n            <phase>compile</phase>\n            <goals>\n              <goal>check</goal>\n            </goals>\n          </execution>\n        </executions>\n      </plugin>\n<!--\n      See https://jira.apache.org/jira/browse/HUDI-304\n      <plugin>\n        <groupId>com.diffplug.spotless</groupId>\n        <artifactId>spotless-maven-plugin</artifactId>\n        <version>1.24.3</version>\n        <configuration>\n          <java>\n            <eclipse>\n              <file>${main.basedir}/style/eclipse-java-google-style.xml</file>\n              <version>4.10.0</version>\n            </eclipse>\n          </java>\n          <scala>\n            <trimTrailingWhitespace />\n          </scala>\n        </configuration>\n        <executions>\n          <execution>\n            <id>spotless-check</id>\n            <phase>compile</phase>\n            <goals>\n              <goal>check</goal>\n            </goals>\n          </execution>\n        </executions>\n      </plugin>\n-->\n      <plugin>\n        <groupId>org.apache.maven.plugins</groupId>\n        <artifactId>maven-compiler-plugin</artifactId>\n        <version>${maven-compiler-plugin.version}</version>\n        <configuration>\n          <source>${java.version}</source>\n          <target>${java.version}</target>\n        </configuration>\n      </plugin>\n      <plugin>\n        <groupId>org.apache.maven.plugins</groupId>\n        <artifactId>maven-release-plugin</artifactId>\n        <version>2.5.3</version>\n        <configuration>\n          <autoVersionSubmodules>true</autoVersionSubmodules>\n          <useReleaseProfile>false</useReleaseProfile>\n          <releaseProfiles>release,integration-tests</releaseProfiles>\n          <goals>deploy</goals>\n        </configuration>\n      </plugin>\n      <plugin>\n        <groupId>org.apache.maven.plugins</groupId>\n        <artifactId>maven-deploy-plugin</artifactId>\n        <version>${maven-deploy-plugin.version}</version>\n        <executions>\n          <execution>\n            <id>default-deploy</id>\n            <phase>deploy</phase>\n            <goals>\n              <goal>deploy</goal>\n            </goals>\n          </execution>\n        </executions>\n      </plugin>\n      <plugin>\n        <groupId>org.apache.maven.plugins</groupId>\n        <artifactId>maven-failsafe-plugin</artifactId>\n        <version>${maven-failsafe-plugin.version}</version>\n        <configuration>\n          <skip>${skipITs}</skip>\n          <argLine>@{argLine}</argLine>\n          <useSystemClassLoader>false</useSystemClassLoader>\n          <systemPropertyVariables>\n            <log4j.configurationFile>${surefire-log4j.file}</log4j.configurationFile>\n          </systemPropertyVariables>\n        </configuration>\n      </plugin>\n      <plugin>\n        <groupId>org.apache.maven.plugins</groupId>\n        <artifactId>maven-enforcer-plugin</artifactId>\n        <version>${maven-enforcer-plugin.version}</version>\n        <executions>\n          <execution>\n            <id>enforce-logging</id>\n            <goals>\n              <goal>enforce</goal>\n            </goals>\n            <configuration>\n              <rules>\n                <bannedDependencies>\n                  <excludes>\n                    <exclude>org.sl4fj:slf4j-simple</exclude>\n                    <exclude>org.sl4fj:slf4j-jdk14</exclude>\n                    <exclude>org.sl4fj:slf4j-nop</exclude>\n                    <exclude>org.sl4fj:slf4j-jcl</exclude>\n                    <exclude>log4j:log4j</exclude>\n                    <exclude>ch.qos.logback:logback-classic</exclude>\n                    <!-- NOTE: We're banning any HBase deps versions other than the approved ${hbase.version},\n                               which is aimed at preventing the classpath collisions w/ transitive deps usually) -->\n                    <exclude>org.apache.hbase:hbase-common:*</exclude>\n                    <exclude>org.apache.hbase:hbase-client:*</exclude>\n                    <exclude>org.apache.hbase:hbase-server:*</exclude>\n                    <!--To upgrade snappy because pre 1.1.8.2 does not work on m1 mac-->\n                    <exclude>org.xerial.snappy:snappy-java:*</exclude>\n                  </excludes>\n                  <includes>\n                    <include>org.slf4j:slf4j-simple:*:*:test</include>\n                    <include>org.apache.hbase:hbase-common:${hbase.version}</include>\n                    <include>org.apache.hbase:hbase-client:${hbase.version}</include>\n                    <include>org.apache.hbase:hbase-server:${hbase.version}</include>\n                    <!--To upgrade snappy because pre 1.1.8.2 does not work on m1 mac-->\n                    <include>org.xerial.snappy:snappy-java:${snappy.version}</include>\n                  </includes>\n                </bannedDependencies>\n              </rules>\n            </configuration>\n          </execution>\n        </executions>\n      </plugin>\n      <plugin>\n        <groupId>org.jacoco</groupId>\n        <artifactId>jacoco-maven-plugin</artifactId>\n        <version>${jacoco.version}</version>\n      </plugin>\n      <plugin>\n        <groupId>io.fabric8</groupId>\n        <artifactId>docker-maven-plugin</artifactId>\n        <version>${maven-docker-plugin.version}</version>\n        <configuration>\n          <skip>${skipDocker}</skip>\n        </configuration>\n      </plugin>\n      <plugin>\n        <groupId>org.apache.maven.plugins</groupId>\n        <artifactId>maven-shade-plugin</artifactId>\n        <version>${maven-shade-plugin.version}</version>\n        <configuration>\n          <!-- common to all bundles -->\n          <artifactSet>\n            <includes>\n              <include>org.apache.hudi:hudi-io</include>\n              <include>io.airlift:aircompressor</include>\n              <!-- org.apache.httpcomponents -->\n              <include>org.apache.httpcomponents:httpclient</include>\n              <include>org.apache.httpcomponents:httpcore</include>\n              <include>org.apache.httpcomponents:fluent-hc</include>\n              <!-- hbase -->\n              <include>org.apache.hbase:hbase-client</include>\n              <include>org.apache.hbase:hbase-common</include>\n              <include>org.apache.hbase:hbase-hadoop-compat</include>\n              <include>org.apache.hbase:hbase-hadoop2-compat</include>\n              <include>org.apache.hbase:hbase-metrics</include>\n              <include>org.apache.hbase:hbase-metrics-api</include>\n              <include>org.apache.hbase:hbase-protocol</include>\n              <include>org.apache.hbase:hbase-protocol-shaded</include>\n              <include>org.apache.hbase:hbase-server</include>\n              <include>org.apache.hbase.thirdparty:hbase-shaded-miscellaneous</include>\n              <include>org.apache.hbase.thirdparty:hbase-shaded-netty</include>\n              <include>org.apache.hbase.thirdparty:hbase-shaded-protobuf</include>\n              <include>org.apache.hbase.thirdparty:hbase-unsafe</include>\n              <include>org.apache.htrace:htrace-core4</include>\n              <!-- roaring bitmap -->\n              <include>org.roaringbitmap:RoaringBitmap</include>\n              <!-- afterburner module for jackson performance -->\n              <include>com.fasterxml.jackson.module:jackson-module-afterburner</include>\n              <include>com.fasterxml.jackson.module:jackson-module-scala_${scala.binary.version}</include>\n              <!-- native HFile reader uses protobuf -->\n              <include>com.google.protobuf:protobuf-java</include>\n            </includes>\n          </artifactSet>\n          <relocations>\n            <!-- org.apache.httpcomponents -->\n            <relocation>\n              <pattern>org.apache.http.</pattern>\n              <shadedPattern>org.apache.hudi.org.apache.http.</shadedPattern>\n            </relocation>\n            <!-- hbase -->\n            <relocation>\n              <pattern>org.apache.hadoop.hbase.</pattern>\n              <shadedPattern>org.apache.hudi.org.apache.hadoop.hbase.</shadedPattern>\n              <excludes>\n                <exclude>org.apache.hadoop.hbase.KeyValue$KeyComparator</exclude>\n              </excludes>\n            </relocation>\n            <relocation>\n              <pattern>org.apache.hbase.</pattern>\n              <shadedPattern>org.apache.hudi.org.apache.hbase.</shadedPattern>\n            </relocation>\n            <relocation>\n              <pattern>org.apache.htrace.</pattern>\n              <shadedPattern>org.apache.hudi.org.apache.htrace.</shadedPattern>\n            </relocation>\n            <!-- hbase\n                 The classes below in org.apache.hadoop.metrics2 package come from\n                 hbase-hadoop-compat and hbase-hadoop2-compat, which have to be shaded one by one,\n                 instead of shading all classes under org.apache.hadoop.metrics2 including ones\n                 from hadoop. -->\n            <relocation>\n              <pattern>org.apache.hadoop.metrics2.MetricHistogram</pattern>\n              <shadedPattern>org.apache.hudi.org.apache.hadoop.metrics2.MetricHistogram\n              </shadedPattern>\n            </relocation>\n            <relocation>\n              <pattern>org.apache.hadoop.metrics2.MetricsExecutor</pattern>\n              <shadedPattern>org.apache.hudi.org.apache.hadoop.metrics2.MetricsExecutor\n              </shadedPattern>\n            </relocation>\n            <relocation>\n              <pattern>org.apache.hadoop.metrics2.impl.JmxCacheBuster</pattern>\n              <shadedPattern>org.apache.hudi.org.apache.hadoop.metrics2.impl.JmxCacheBuster\n              </shadedPattern>\n            </relocation>\n            <relocation>\n              <pattern>org.apache.hadoop.metrics2.lib.DefaultMetricsSystemHelper</pattern>\n              <shadedPattern>\n                org.apache.hudi.org.apache.hadoop.metrics2.lib.DefaultMetricsSystemHelper\n              </shadedPattern>\n            </relocation>\n            <relocation>\n              <pattern>org.apache.hadoop.metrics2.lib.DynamicMetricsRegistry</pattern>\n              <shadedPattern>org.apache.hudi.org.apache.hadoop.metrics2.lib.DynamicMetricsRegistry\n              </shadedPattern>\n            </relocation>\n            <relocation>\n              <pattern>org.apache.hadoop.metrics2.lib.MetricsExecutorImpl</pattern>\n              <shadedPattern>org.apache.hudi.org.apache.hadoop.metrics2.lib.MetricsExecutorImpl\n              </shadedPattern>\n            </relocation>\n            <relocation>\n              <pattern>org.apache.hadoop.metrics2.lib.MutableFastCounter</pattern>\n              <shadedPattern>org.apache.hudi.org.apache.hadoop.metrics2.lib.MutableFastCounter\n              </shadedPattern>\n            </relocation>\n            <relocation>\n              <pattern>org.apache.hadoop.metrics2.lib.MutableHistogram</pattern>\n              <shadedPattern>org.apache.hudi.org.apache.hadoop.metrics2.lib.MutableHistogram\n              </shadedPattern>\n            </relocation>\n            <relocation>\n              <pattern>org.apache.hadoop.metrics2.lib.MutableRangeHistogram</pattern>\n              <shadedPattern>org.apache.hudi.org.apache.hadoop.metrics2.lib.MutableRangeHistogram\n              </shadedPattern>\n            </relocation>\n            <relocation>\n              <pattern>org.apache.hadoop.metrics2.lib.MutableSizeHistogram</pattern>\n              <shadedPattern>org.apache.hudi.org.apache.hadoop.metrics2.lib.MutableSizeHistogram\n              </shadedPattern>\n            </relocation>\n            <relocation>\n              <pattern>org.apache.hadoop.metrics2.lib.MutableTimeHistogram</pattern>\n              <shadedPattern>org.apache.hudi.org.apache.hadoop.metrics2.lib.MutableTimeHistogram\n              </shadedPattern>\n            </relocation>\n            <relocation>\n              <pattern>org.apache.hadoop.metrics2.util.MetricQuantile</pattern>\n              <shadedPattern>org.apache.hudi.org.apache.hadoop.metrics2.util.MetricQuantile\n              </shadedPattern>\n            </relocation>\n            <relocation>\n              <pattern>org.apache.hadoop.metrics2.util.MetricSampleQuantiles</pattern>\n              <shadedPattern>org.apache.hudi.org.apache.hadoop.metrics2.util.MetricSampleQuantiles\n              </shadedPattern>\n            </relocation>\n            <relocation>\n              <pattern>org.roaringbitmap.</pattern>\n              <shadedPattern>org.apache.hudi.org.roaringbitmap.</shadedPattern>\n            </relocation>\n            <relocation>\n              <pattern>com.fasterxml.jackson.module</pattern>\n              <shadedPattern>org.apache.hudi.com.fasterxml.jackson.module\n              </shadedPattern>\n            </relocation>\n            <relocation>\n              <pattern>com.google.protobuf.</pattern>\n              <shadedPattern>org.apache.hudi.com.google.protobuf.</shadedPattern>\n            </relocation>\n          </relocations>\n        </configuration>\n      </plugin>\n    </plugins>\n\n    <pluginManagement>\n      <plugins>\n        <plugin>\n          <groupId>org.apache.maven.plugins</groupId>\n          <artifactId>maven-surefire-plugin</artifactId>\n          <version>${maven-surefire-plugin.version}</version>\n          <configuration>\n            <rerunFailingTestsCount>3</rerunFailingTestsCount>\n            <argLine>@{argLine}</argLine>\n            <trimStackTrace>false</trimStackTrace>\n            <systemPropertyVariables>\n              <log4j.configurationFile>${surefire-log4j.file}</log4j.configurationFile>\n            </systemPropertyVariables>\n            <useSystemClassLoader>false</useSystemClassLoader>\n            <forkedProcessExitTimeoutInSeconds>30</forkedProcessExitTimeoutInSeconds>\n          </configuration>\n        </plugin>\n        <plugin>\n          <groupId>org.scalatest</groupId>\n          <artifactId>scalatest-maven-plugin</artifactId>\n          <version>2.2.0</version>\n          <configuration>\n            <skipTests>${skipUTs}</skipTests>\n            <reportsDirectory>${project.build.directory}/surefire-reports</reportsDirectory>\n            <junitxml>.</junitxml>\n            <filereports>TestSuite.txt</filereports>\n            <systemProperties>\n              <log4j.configurationFile>${surefire-log4j.file}</log4j.configurationFile>\n            </systemProperties>\n          </configuration>\n          <executions>\n            <execution>\n              <id>test</id>\n              <goals>\n                <goal>test</goal>\n              </goals>\n            </execution>\n          </executions>\n        </plugin>\n        <plugin>\n          <groupId>org.apache.maven.plugins</groupId>\n          <artifactId>maven-jar-plugin</artifactId>\n          <version>${maven-jar-plugin.version}</version>\n        </plugin>\n        <plugin>\n          <groupId>net.alchim31.maven</groupId>\n          <artifactId>scala-maven-plugin</artifactId>\n          <version>${scala-maven-plugin.version}</version>\n          <configuration>\n            <checkMultipleScalaVersions>false</checkMultipleScalaVersions>\n          </configuration>\n        </plugin>\n        <plugin>\n          <groupId>org.apache.maven.plugins</groupId>\n          <artifactId>maven-compiler-plugin</artifactId>\n        </plugin>\n        <plugin>\n          <!-- excludes are inherited -->\n          <groupId>org.apache.rat</groupId>\n          <artifactId>apache-rat-plugin</artifactId>\n          <version>${apache-rat-plugin.version}</version>\n          <configuration>\n            <excludeSubProjects>false</excludeSubProjects>\n            <numUnapprovedLicenses>0</numUnapprovedLicenses>\n            <licenses>\n              <!-- Enforce this license:\n                   Licensed to the Apache Software Foundation (ASF) under one\n                   or more contributor license agreements.  See the NOTICE file\n                   distributed with this work for additional information\n                   regarding copyright ownership.  The ASF licenses this file\n                   to you under the Apache License, Version 2.0 (the\n                   \"License\"); you may not use this file except in compliance\n                   with the License.  You may obtain a copy of the License at\n                   http://www.apache.org/licenses/LICENSE-2.0\n                   Unless required by applicable law or agreed to in writing,\n                   software distributed under the License is distributed on an\n                   \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n                   KIND, either express or implied.  See the License for the\n                   specific language governing permissions and limitations\n                   under the License.\n              -->\n              <license implementation=\"org.apache.rat.analysis.license.SimplePatternBasedLicense\">\n                <licenseFamilyCategory>AL2 </licenseFamilyCategory>\n                <licenseFamilyName>Apache License 2.0</licenseFamilyName>\n                <notes />\n                <patterns>\n                  <pattern>Licensed to the Apache Software Foundation (ASF) under one</pattern>\n                </patterns>\n              </license>\n            </licenses>\n            <licenseFamilies>\n              <licenseFamily implementation=\"org.apache.rat.license.SimpleLicenseFamily\">\n                <familyName>Apache License 2.0</familyName>\n              </licenseFamily>\n            </licenseFamilies>\n            <excludes>\n              <exclude>NOTICE</exclude>\n              <exclude>DISCLAIMER</exclude>\n              <exclude>**/.*</exclude>\n              <exclude>**/emptyFile</exclude>\n              <exclude>**/*.json</exclude>\n              <exclude>**/*.hfile</exclude>\n              <exclude>**/*.log</exclude>\n              <exclude>**/*.sqltemplate</exclude>\n              <exclude>**/compose_env</exclude>\n              <exclude>**/main/resources/version.txt</exclude>\n              <exclude>**/*NOTICE*</exclude>\n              <exclude>**/*LICENSE*</exclude>\n              <exclude>**/dependency-reduced-pom.xml</exclude>\n              <exclude>**/test/resources/*.data</exclude>\n              <exclude>**/test/resources/*.commit</exclude>\n              <exclude>**/test/resources/**/*.txt</exclude>\n              <exclude>**/test/resources/**/*.avsc</exclude>\n              <exclude>**/target/**</exclude>\n              <exclude>**/generated-sources/**</exclude>\n              <exclude>.github/**</exclude>\n              <exclude>**/banner.txt</exclude>\n              <!-- local files not in version control -->\n              <exclude>**/*.iml</exclude>\n              <exclude>.mvn/**</exclude>\n            </excludes>\n          </configuration>\n          <executions>\n            <execution>\n              <phase>compile</phase>\n              <goals>\n                <goal>check</goal>\n              </goals>\n            </execution>\n          </executions>\n        </plugin>\n        <plugin>\n          <groupId>org.apache.avro</groupId>\n          <artifactId>avro-maven-plugin</artifactId>\n          <version>${avro.version}</version>\n          <executions>\n            <execution>\n              <phase>generate-sources</phase>\n              <goals>\n                <goal>schema</goal>\n              </goals>\n              <configuration>\n                <sourceDirectory>${project.basedir}/src/main/avro/</sourceDirectory>\n                <outputDirectory>${project.build.directory}/generated-sources/src/main/java/\n                </outputDirectory>\n                <stringType>String</stringType>\n              </configuration>\n            </execution>\n          </executions>\n        </plugin>\n        <plugin>\n          <groupId>org.scalastyle</groupId>\n          <artifactId>scalastyle-maven-plugin</artifactId>\n          <version>1.0.0</version>\n          <configuration>\n            <verbose>false</verbose>\n            <failOnViolation>true</failOnViolation>\n            <includeTestSourceDirectory>true</includeTestSourceDirectory>\n            <failOnWarning>false</failOnWarning>\n            <sourceDirectory>${project.basedir}/src/main/scala</sourceDirectory>\n            <testSourceDirectory>${project.basedir}/src/test/scala</testSourceDirectory>\n            <configLocation>${main.basedir}/style/scalastyle.xml</configLocation>\n            <outputEncoding>UTF-8</outputEncoding>\n          </configuration>\n          <executions>\n            <execution>\n              <phase>compile</phase>\n              <goals>\n                <goal>check</goal>\n              </goals>\n            </execution>\n          </executions>\n        </plugin>\n        <plugin>\n          <groupId>com.github.os72</groupId>\n          <artifactId>protoc-jar-maven-plugin</artifactId>\n          <version>3.11.4</version>\n          <executions>\n            <execution>\n              <id>proto-compile</id>\n              <phase>generate-sources</phase>\n              <goals>\n                <goal>run</goal>\n              </goals>\n              <configuration>\n                <inputDirectories>\n                  <include>src/main/resources</include>\n                </inputDirectories>\n              </configuration>\n            </execution>\n            <execution>\n              <id>proto-test-compile</id>\n              <phase>generate-test-sources</phase>\n              <goals>\n                <goal>run</goal>\n              </goals>\n              <configuration>\n                <addSources>test</addSources>\n                <inputDirectories>\n                  <include>src/test/resources</include>\n                </inputDirectories>\n              </configuration>\n            </execution>\n          </executions>\n          <configuration>\n            <protocArtifact>com.google.protobuf:protoc:${proto.version}</protocArtifact>\n            <protocVersion>${protoc.version}</protocVersion>\n            <includeStdTypes>true</includeStdTypes>\n          </configuration>\n        </plugin>\n      </plugins>\n    </pluginManagement>\n  </build>\n\n  <dependencyManagement>\n    <dependencies>\n      <!-- Scala -->\n      <dependency>\n        <groupId>org.scala-lang.modules</groupId>\n        <artifactId>scala-collection-compat_${scala.binary.version}</artifactId>\n        <version>${scala.collection-compat.version}</version>\n      </dependency>\n\n      <dependency>\n        <groupId>org.openjdk.jol</groupId>\n        <artifactId>jol-core</artifactId>\n        <version>${openjdk.jol.version}</version>\n      </dependency>\n\n      <!-- Logging -->\n      <!-- NOTE: All the following deps have to have \"provided\" scope to make sure these are not conflicting\n           w/ implementations that are using Hudi as a library. For ex, all Spark < 3.3 are still relying on Log4j1\n           and therefore if we be bringing Log4j2 bridge for V1 on the classpath (log4j-1.2-api), it'll fail w/\n           `ClassNotFoundException`, since the bridge would be expecting Log4j2 impl be present -->\n      <dependency>\n        <groupId>org.apache.logging.log4j</groupId>\n        <artifactId>log4j-api</artifactId>\n        <version>${log4j2.version}</version>\n        <scope>provided</scope>\n      </dependency>\n      <dependency>\n        <groupId>org.apache.logging.log4j</groupId>\n        <artifactId>log4j-core</artifactId>\n        <version>${log4j2.version}</version>\n        <scope>provided</scope>\n      </dependency>\n      <dependency>\n        <groupId>org.apache.logging.log4j</groupId>\n        <artifactId>log4j-1.2-api</artifactId>\n        <version>${log4j2.version}</version>\n        <scope>provided</scope>\n      </dependency>\n      <dependency>\n        <groupId>org.apache.logging.log4j</groupId>\n        <artifactId>log4j-slf4j-impl</artifactId>\n        <version>${log4j2.version}</version>\n        <scope>provided</scope>\n      </dependency>\n      <dependency>\n        <groupId>org.slf4j</groupId>\n        <artifactId>slf4j-api</artifactId>\n        <version>${slf4j.version}</version>\n        <scope>provided</scope>\n      </dependency>\n      <dependency>\n        <groupId>org.slf4j</groupId>\n        <artifactId>jul-to-slf4j</artifactId>\n        <version>${slf4j.version}</version>\n        <scope>provided</scope>\n      </dependency>\n\n      <!-- Fasterxml -->\n      <dependency>\n        <groupId>com.fasterxml.jackson.core</groupId>\n        <artifactId>jackson-annotations</artifactId>\n        <version>${fasterxml.version}</version>\n      </dependency>\n      <dependency>\n        <groupId>com.fasterxml.jackson.core</groupId>\n        <artifactId>jackson-core</artifactId>\n        <version>${fasterxml.version}</version>\n      </dependency>\n      <dependency>\n        <groupId>com.fasterxml.jackson.core</groupId>\n        <artifactId>jackson-databind</artifactId>\n        <version>${fasterxml.jackson.databind.version}</version>\n      </dependency>\n      <dependency>\n        <groupId>com.fasterxml.jackson.datatype</groupId>\n        <artifactId>jackson-datatype-guava</artifactId>\n        <version>${fasterxml.version}</version>\n      </dependency>\n      <!-- This one is necessary to support Java 8 Date/Time types (required for Jackson >= 2.13) -->\n      <dependency>\n        <groupId>com.fasterxml.jackson.datatype</groupId>\n        <artifactId>jackson-datatype-jsr310</artifactId>\n        <version>${fasterxml.version}</version>\n      </dependency>\n      <dependency>\n        <groupId>com.fasterxml.jackson.module</groupId>\n        <artifactId>jackson-module-scala_${scala.binary.version}</artifactId>\n        <version>${fasterxml.jackson.module.scala.version}</version>\n      </dependency>\n      <!-- Provides performance improvements with json serialization/deserialization -->\n      <dependency>\n        <groupId>com.fasterxml.jackson.module</groupId>\n        <artifactId>jackson-module-afterburner</artifactId>\n        <version>${fasterxml.version}</version>\n      </dependency>\n\n      <dependency>\n        <groupId>com.fasterxml.jackson.module</groupId>\n        <artifactId>jackson-module-afterburner</artifactId>\n        <version>${fasterxml.jackson.databind.version}</version>\n      </dependency>\n\n      <!-- Glassfish -->\n      <dependency>\n        <groupId>org.glassfish.jersey.core</groupId>\n        <artifactId>jersey-server</artifactId>\n        <version>${glassfish.version}</version>\n      </dependency>\n      <dependency>\n        <groupId>org.glassfish.jersey.connectors</groupId>\n        <artifactId>jersey-apache-connector</artifactId>\n        <version>${glassfish.version}</version>\n      </dependency>\n      <dependency>\n        <groupId>org.glassfish.jersey.containers</groupId>\n        <artifactId>jersey-container-servlet-core</artifactId>\n        <version>${glassfish.version}</version>\n      </dependency>\n      <dependency>\n        <groupId>org.glassfish</groupId>\n        <artifactId>javax.el</artifactId>\n        <version>${glassfish.el.version}</version>\n        <scope>provided</scope>\n      </dependency>\n      <dependency>\n        <groupId>org.glassfish.jersey.ext</groupId>\n        <artifactId>jersey-bean-validation</artifactId>\n        <version>${glassfish.version}</version>\n      </dependency>\n\n      <!-- Avro -->\n      <dependency>\n        <groupId>org.apache.avro</groupId>\n        <artifactId>avro</artifactId>\n        <version>${avro.version}</version>\n        <exclusions>\n          <exclusion>\n            <groupId>org.xerial.snappy</groupId>\n            <artifactId>snappy-java</artifactId>\n          </exclusion>\n        </exclusions>\n        <scope>provided</scope>\n      </dependency>\n\n      <!-- airlift -->\n      <dependency>\n        <groupId>io.airlift</groupId>\n        <artifactId>aircompressor</artifactId>\n        <version>${airlift.version}</version>\n      </dependency>\n\n      <!-- Snappy -->\n      <dependency>\n        <groupId>org.xerial.snappy</groupId>\n        <artifactId>snappy-java</artifactId>\n        <version>${snappy.version}</version>\n      </dependency>\n\n      <!-- caffeine -->\n      <dependency>\n        <groupId>com.github.ben-manes.caffeine</groupId>\n        <artifactId>caffeine</artifactId>\n        <version>${caffeine.version}</version>\n      </dependency>\n\n      <!-- Parquet -->\n      <dependency>\n        <groupId>org.apache.parquet</groupId>\n        <artifactId>parquet-avro</artifactId>\n        <version>${parquet.version}</version>\n        <scope>provided</scope>\n        <exclusions>\n          <exclusion>\n            <groupId>log4j</groupId>\n            <artifactId>log4j</artifactId>\n          </exclusion>\n        </exclusions>\n      </dependency>\n\n      <!-- Orc -->\n      <dependency>\n        <groupId>org.apache.orc</groupId>\n        <artifactId>orc-core</artifactId>\n        <version>${orc.spark.version}</version>\n        <scope>compile</scope>\n      </dependency>\n\n      <!-- RoaringBitmap -->\n      <dependency>\n        <groupId>org.roaringbitmap</groupId>\n        <artifactId>RoaringBitmap</artifactId>\n        <version>${roaringbitmap.version}</version>\n      </dependency>\n\n      <!-- Spark -->\n      <dependency>\n        <groupId>org.apache.spark</groupId>\n        <artifactId>spark-core_${scala.binary.version}</artifactId>\n        <version>${spark.version}</version>\n        <scope>provided</scope>\n        <exclusions>\n          <exclusion>\n            <groupId>org.slf4j</groupId>\n            <artifactId>*</artifactId>\n          </exclusion>\n          <exclusion>\n            <groupId>log4j</groupId>\n            <artifactId>log4j</artifactId>\n          </exclusion>\n        </exclusions>\n      </dependency>\n      <dependency>\n        <groupId>org.apache.spark</groupId>\n        <artifactId>spark-sql_${scala.binary.version}</artifactId>\n        <version>${spark.version}</version>\n        <scope>provided</scope>\n      </dependency>\n      <dependency>\n        <groupId>org.apache.spark</groupId>\n        <artifactId>spark-hive_${scala.binary.version}</artifactId>\n        <version>${spark.version}</version>\n        <scope>provided</scope>\n        <exclusions>\n          <exclusion>\n            <groupId>log4j</groupId>\n            <artifactId>apache-log4j-extras</artifactId>\n          </exclusion>\n        </exclusions>\n      </dependency>\n      <dependency>\n        <groupId>org.apache.spark</groupId>\n        <artifactId>spark-sql_${scala.binary.version}</artifactId>\n        <classifier>tests</classifier>\n        <version>${spark.version}</version>\n        <scope>test</scope>\n      </dependency>\n      <dependency>\n        <groupId>org.apache.spark</groupId>\n        <artifactId>spark-core_${scala.binary.version}</artifactId>\n        <classifier>tests</classifier>\n        <version>${spark.version}</version>\n        <scope>test</scope>\n        <exclusions>\n          <exclusion>\n            <groupId>org.slf4j</groupId>\n            <artifactId>*</artifactId>\n          </exclusion>\n          <exclusion>\n            <groupId>log4j</groupId>\n            <artifactId>log4j</artifactId>\n          </exclusion>\n        </exclusions>\n      </dependency>\n      <dependency>\n        <groupId>org.apache.spark</groupId>\n        <artifactId>spark-catalyst_${scala.binary.version}</artifactId>\n        <classifier>tests</classifier>\n        <version>${spark.version}</version>\n        <scope>test</scope>\n      </dependency>\n      <dependency>\n        <groupId>org.apache.spark</groupId>\n        <artifactId>spark-hive_${scala.binary.version}</artifactId>\n        <classifier>tests</classifier>\n        <version>${spark.version}</version>\n        <scope>test</scope>\n        <exclusions>\n          <exclusion>\n            <groupId>log4j</groupId>\n            <artifactId>apache-log4j-extras</artifactId>\n          </exclusion>\n        </exclusions>\n      </dependency>\n\n      <!-- Flink -->\n      <dependency>\n        <groupId>org.apache.flink</groupId>\n        <artifactId>${flink.streaming.java.artifactId}</artifactId>\n        <version>${flink.version}</version>\n        <scope>provided</scope>\n      </dependency>\n      <dependency>\n        <groupId>org.apache.flink</groupId>\n        <artifactId>${flink.clients.artifactId}</artifactId>\n        <version>${flink.version}</version>\n        <scope>provided</scope>\n      </dependency>\n      <dependency>\n        <groupId>org.apache.flink</groupId>\n        <artifactId>${flink.connector.kafka.artifactId}</artifactId>\n        <version>${flink.connector.kafka.version}</version>\n        <scope>provided</scope>\n      </dependency>\n\n      <!-- Dropwizard Metrics -->\n      <dependency>\n        <groupId>io.dropwizard.metrics</groupId>\n        <artifactId>metrics-graphite</artifactId>\n        <version>${metrics.version}</version>\n      </dependency>\n      <dependency>\n        <groupId>io.dropwizard.metrics</groupId>\n        <artifactId>metrics-core</artifactId>\n        <version>${metrics.version}</version>\n      </dependency>\n      <dependency>\n        <groupId>io.dropwizard.metrics</groupId>\n        <artifactId>metrics-jmx</artifactId>\n        <version>${metrics.version}</version>\n      </dependency>\n      <dependency>\n        <groupId>io.prometheus</groupId>\n        <artifactId>simpleclient</artifactId>\n        <version>${prometheus.version}</version>\n      </dependency>\n      <dependency>\n        <groupId>io.prometheus</groupId>\n        <artifactId>simpleclient_httpserver</artifactId>\n        <version>${prometheus.version}</version>\n      </dependency>\n      <dependency>\n        <groupId>io.prometheus</groupId>\n        <artifactId>simpleclient_dropwizard</artifactId>\n        <version>${prometheus.version}</version>\n      </dependency>\n      <dependency>\n        <groupId>io.prometheus</groupId>\n        <artifactId>simpleclient_pushgateway</artifactId>\n        <version>${prometheus.version}</version>\n      </dependency>\n      <dependency>\n        <groupId>com.uber.m3</groupId>\n        <artifactId>tally-m3</artifactId>\n        <version>${tally.version}</version>\n      </dependency>\n      <dependency>\n        <groupId>com.uber.m3</groupId>\n        <artifactId>tally-core</artifactId>\n        <version>${tally.version}</version>\n      </dependency>\n\n      <dependency>\n        <groupId>com.beust</groupId>\n        <artifactId>jcommander</artifactId>\n        <version>1.78</version>\n      </dependency>\n\n      <dependency>\n        <groupId>joda-time</groupId>\n        <artifactId>joda-time</artifactId>\n        <version>${joda.version}</version>\n      </dependency>\n\n      <dependency>\n        <groupId>xerces</groupId>\n        <artifactId>xercesImpl</artifactId>\n        <version>2.12.2</version>\n      </dependency>\n\n      <dependency>\n        <groupId>xalan</groupId>\n        <artifactId>xalan</artifactId>\n        <version>2.7.3</version>\n      </dependency>\n\n      <dependency>\n        <groupId>org.rocksdb</groupId>\n        <artifactId>rocksdbjni</artifactId>\n        <version>${rocksdbjni.version}</version>\n      </dependency>\n\n      <!-- Httpcomponents -->\n      <dependency>\n        <groupId>org.apache.httpcomponents</groupId>\n        <artifactId>fluent-hc</artifactId>\n        <version>${httpclient.version}</version>\n      </dependency>\n      <dependency>\n        <groupId>org.apache.httpcomponents</groupId>\n        <artifactId>httpcore</artifactId>\n        <version>${httpcore.version}</version>\n      </dependency>\n      <dependency>\n        <groupId>org.apache.httpcomponents</groupId>\n        <artifactId>httpclient</artifactId>\n        <version>${httpclient.version}</version>\n      </dependency>\n\n      <!-- Hadoop -->\n      <dependency>\n        <groupId>org.apache.hadoop</groupId>\n        <artifactId>hadoop-client</artifactId>\n        <version>${hadoop.version}</version>\n        <scope>provided</scope>\n        <exclusions>\n          <exclusion>\n            <groupId>javax.servlet</groupId>\n            <artifactId>servlet-api</artifactId>\n          </exclusion>\n          <exclusion>\n            <groupId>javax.xml.bind</groupId>\n            <artifactId>jaxb-api</artifactId>\n          </exclusion>\n          <exclusion>\n            <groupId>org.slf4j</groupId>\n            <artifactId>slf4j-log4j12</artifactId>\n          </exclusion>\n        </exclusions>\n      </dependency>\n      <dependency>\n        <groupId>org.apache.hadoop</groupId>\n        <artifactId>hadoop-common</artifactId>\n        <version>${hadoop.version}</version>\n        <scope>provided</scope>\n        <exclusions>\n          <exclusion>\n            <groupId>jdk.tools</groupId>\n            <artifactId>jdk.tools</artifactId>\n          </exclusion>\n          <exclusion>\n            <groupId>javax.xml.bind</groupId>\n            <artifactId>jaxb-api</artifactId>\n          </exclusion>\n          <exclusion>\n            <groupId>org.slf4j</groupId>\n            <artifactId>slf4j-log4j12</artifactId>\n          </exclusion>\n          <exclusion>\n            <groupId>log4j</groupId>\n            <artifactId>log4j</artifactId>\n          </exclusion>\n        </exclusions>\n      </dependency>\n      <dependency>\n        <groupId>org.apache.hadoop</groupId>\n        <artifactId>hadoop-hdfs</artifactId>\n        <version>${hadoop.version}</version>\n        <scope>provided</scope>\n        <exclusions>\n          <exclusion>\n            <groupId>log4j</groupId>\n            <artifactId>log4j</artifactId>\n          </exclusion>\n        </exclusions>\n      </dependency>\n      <dependency>\n        <groupId>org.apache.hadoop</groupId>\n        <artifactId>hadoop-auth</artifactId>\n        <version>${hadoop.version}</version>\n        <scope>provided</scope>\n        <exclusions>\n          <exclusion>\n            <groupId>org.slf4j</groupId>\n            <artifactId>slf4j-log4j12</artifactId>\n          </exclusion>\n          <exclusion>\n            <groupId>log4j</groupId>\n            <artifactId>log4j</artifactId>\n          </exclusion>\n        </exclusions>\n      </dependency>\n      <dependency>\n        <groupId>org.apache.hadoop</groupId>\n        <artifactId>hadoop-mapreduce-client-core</artifactId>\n        <version>${hadoop.version}</version>\n        <scope>provided</scope>\n        <exclusions>\n          <exclusion>\n            <groupId>javax.xml.bind</groupId>\n            <artifactId>jaxb-api</artifactId>\n          </exclusion>\n          <exclusion>\n            <groupId>org.slf4j</groupId>\n            <artifactId>slf4j-log4j12</artifactId>\n          </exclusion>\n          <exclusion>\n            <groupId>log4j</groupId>\n            <artifactId>log4j</artifactId>\n          </exclusion>\n        </exclusions>\n      </dependency>\n      <dependency>\n        <groupId>org.apache.hadoop</groupId>\n        <artifactId>hadoop-mapreduce-client-common</artifactId>\n        <version>${hadoop.version}</version>\n        <scope>provided</scope>\n        <exclusions>\n          <exclusion>\n            <groupId>javax.xml.bind</groupId>\n            <artifactId>jaxb-api</artifactId>\n          </exclusion>\n          <exclusion>\n            <groupId>org.slf4j</groupId>\n            <artifactId>slf4j-log4j12</artifactId>\n          </exclusion>\n          <exclusion>\n            <groupId>log4j</groupId>\n            <artifactId>log4j</artifactId>\n          </exclusion>\n        </exclusions>\n      </dependency>\n      <dependency>\n        <groupId>org.apache.hadoop</groupId>\n        <artifactId>hadoop-hdfs</artifactId>\n        <classifier>tests</classifier>\n        <scope>test</scope>\n        <version>${hadoop.version}</version>\n        <exclusions>\n          <exclusion>\n            <groupId>log4j</groupId>\n            <artifactId>log4j</artifactId>\n          </exclusion>\n        </exclusions>\n      </dependency>\n      <dependency>\n        <groupId>org.apache.hadoop</groupId>\n        <artifactId>hadoop-common</artifactId>\n        <classifier>tests</classifier>\n        <version>${hadoop.version}</version>\n        <exclusions>\n          <exclusion>\n            <groupId>jdk.tools</groupId>\n            <artifactId>jdk.tools</artifactId>\n          </exclusion>\n          <exclusion>\n            <groupId>javax.xml.bind</groupId>\n            <artifactId>jaxb-api</artifactId>\n          </exclusion>\n          <exclusion>\n            <groupId>org.slf4j</groupId>\n            <artifactId>slf4j-log4j12</artifactId>\n          </exclusion>\n          <exclusion>\n            <groupId>log4j</groupId>\n            <artifactId>log4j</artifactId>\n          </exclusion>\n        </exclusions>\n      </dependency>\n\n      <!-- Hive -->\n      <dependency>\n        <groupId>${hive.groupid}</groupId>\n        <artifactId>hive-service</artifactId>\n        <version>${hive.version}</version>\n        <scope>provided</scope>\n        <exclusions>\n          <exclusion>\n            <groupId>javax.mail</groupId>\n            <artifactId>*</artifactId>\n          </exclusion>\n          <exclusion>\n            <groupId>org.eclipse.jetty.aggregate</groupId>\n            <artifactId>*</artifactId>\n          </exclusion>\n          <exclusion>\n            <groupId>org.pentaho</groupId>\n            <artifactId>*</artifactId>\n          </exclusion>\n          <exclusion>\n            <groupId>org.apache.logging.log4j</groupId>\n            <artifactId>*</artifactId>\n          </exclusion>\n          <exclusion>\n            <groupId>org.slf4j</groupId>\n            <artifactId>slf4j-api</artifactId>\n          </exclusion>\n          <exclusion>\n            <groupId>org.slf4j</groupId>\n            <artifactId>slf4j-log4j12</artifactId>\n          </exclusion>\n          <exclusion>\n            <groupId>log4j</groupId>\n            <artifactId>log4j</artifactId>\n          </exclusion>\n          <exclusion>\n            <groupId>org.apache.hbase</groupId>\n            <artifactId>hbase-common</artifactId>\n          </exclusion>\n          <exclusion>\n            <groupId>org.apache.hbase</groupId>\n            <artifactId>*</artifactId>\n          </exclusion>\n        </exclusions>\n      </dependency>\n      <dependency>\n        <groupId>${hive.groupid}</groupId>\n        <artifactId>hive-shims</artifactId>\n        <version>${hive.version}</version>\n        <scope>provided</scope>\n        <exclusions>\n          <exclusion>\n            <groupId>javax.mail</groupId>\n            <artifactId>mail</artifactId>\n          </exclusion>\n          <exclusion>\n            <groupId>javax.xml.bind</groupId>\n            <artifactId>jaxb-api</artifactId>\n          </exclusion>\n          <exclusion>\n            <groupId>org.eclipse.jetty.aggregate</groupId>\n            <artifactId>*</artifactId>\n          </exclusion>\n          <exclusion>\n            <groupId>org.pentaho</groupId>\n            <artifactId>*</artifactId>\n          </exclusion>\n        </exclusions>\n      </dependency>\n      <dependency>\n        <groupId>${hive.groupid}</groupId>\n        <artifactId>hive-jdbc</artifactId>\n        <version>${hive.version}</version>\n        <scope>provided</scope>\n        <exclusions>\n          <exclusion>\n            <groupId>javax.mail</groupId>\n            <artifactId>mail</artifactId>\n          </exclusion>\n          <exclusion>\n            <groupId>org.eclipse.jetty.aggregate</groupId>\n            <artifactId>*</artifactId>\n          </exclusion>\n          <exclusion>\n            <groupId>org.slf4j</groupId>\n            <artifactId>slf4j-log4j12</artifactId>\n          </exclusion>\n          <exclusion>\n            <groupId>log4j</groupId>\n            <artifactId>log4j</artifactId>\n          </exclusion>\n        </exclusions>\n      </dependency>\n      <dependency>\n        <groupId>${hive.groupid}</groupId>\n        <artifactId>hive-serde</artifactId>\n        <version>${hive.version}</version>\n        <scope>provided</scope>\n        <exclusions>\n          <exclusion>\n            <groupId>javax.mail</groupId>\n            <artifactId>mail</artifactId>\n          </exclusion>\n        </exclusions>\n      </dependency>\n      <dependency>\n        <groupId>${hive.groupid}</groupId>\n        <artifactId>hive-metastore</artifactId>\n        <version>${hive.version}</version>\n        <scope>provided</scope>\n        <exclusions>\n          <exclusion>\n            <groupId>javax.transaction</groupId>\n            <artifactId>jta</artifactId>\n          </exclusion>\n          <exclusion>\n            <groupId>javax.transaction</groupId>\n            <artifactId>transaction-api</artifactId>\n          </exclusion>\n          <exclusion>\n            <groupId>javax.mail</groupId>\n            <artifactId>mail</artifactId>\n          </exclusion>\n          <exclusion>\n            <groupId>org.eclipse.jetty.aggregate</groupId>\n            <artifactId>*</artifactId>\n          </exclusion>\n          <exclusion>\n            <groupId>log4j</groupId>\n            <artifactId>log4j</artifactId>\n          </exclusion>\n          <exclusion>\n            <groupId>org.apache.hbase</groupId>\n            <artifactId>*</artifactId>\n          </exclusion>\n        </exclusions>\n      </dependency>\n      <dependency>\n        <groupId>${hive.groupid}</groupId>\n        <artifactId>hive-common</artifactId>\n        <version>${hive.version}</version>\n        <scope>provided</scope>\n        <exclusions>\n          <exclusion>\n            <groupId>javax.mail</groupId>\n            <artifactId>mail</artifactId>\n          </exclusion>\n          <exclusion>\n            <groupId>org.eclipse.jetty.aggregate</groupId>\n            <artifactId>*</artifactId>\n          </exclusion>\n          <exclusion>\n            <groupId>org.apache.logging.log4j</groupId>\n            <artifactId>*</artifactId>\n          </exclusion>\n          <exclusion>\n            <groupId>org.slf4j</groupId>\n            <artifactId>slf4j-log4j12</artifactId>\n          </exclusion>\n          <exclusion>\n            <groupId>log4j</groupId>\n            <artifactId>log4j</artifactId>\n          </exclusion>\n          <exclusion>\n            <groupId>org.apache.hbase</groupId>\n            <artifactId>*</artifactId>\n          </exclusion>\n        </exclusions>\n      </dependency>\n      <dependency>\n        <groupId>${hive.groupid}</groupId>\n        <artifactId>hive-exec</artifactId>\n        <version>${hive.version}</version>\n        <scope>provided</scope>\n        <classifier>${hive.exec.classifier}</classifier>\n        <exclusions>\n          <exclusion>\n            <groupId>javax.mail</groupId>\n            <artifactId>mail</artifactId>\n          </exclusion>\n          <exclusion>\n            <groupId>org.eclipse.jetty.aggregate</groupId>\n            <artifactId>*</artifactId>\n          </exclusion>\n          <exclusion>\n            <groupId>org.pentaho</groupId>\n            <artifactId>*</artifactId>\n          </exclusion>\n          <exclusion>\n            <groupId>org.apache.logging.log4j</groupId>\n            <artifactId>*</artifactId>\n          </exclusion>\n          <exclusion>\n            <groupId>org.slf4j</groupId>\n            <artifactId>slf4j-log4j12</artifactId>\n          </exclusion>\n          <exclusion>\n            <groupId>log4j</groupId>\n            <artifactId>log4j</artifactId>\n          </exclusion>\n          <exclusion>\n            <groupId>log4j</groupId>\n            <artifactId>apache-log4j-extras</artifactId>\n          </exclusion>\n          <exclusion>\n            <groupId>org.apache.hbase</groupId>\n            <artifactId>*</artifactId>\n          </exclusion>\n        </exclusions>\n      </dependency>\n      <dependency>\n        <groupId>org.apache.hive</groupId>\n        <artifactId>hive-exec</artifactId>\n        <version>${hive.version}</version>\n        <scope>provided</scope>\n        <exclusions>\n          <exclusion>\n            <groupId>commons-lang</groupId>\n            <artifactId>commons-lang</artifactId>\n          </exclusion>\n          <exclusion>\n            <groupId>org.apache.commons</groupId>\n            <artifactId>commons-lang3</artifactId>\n          </exclusion>\n          <exclusion>\n            <artifactId>guava</artifactId>\n            <groupId>com.google.guava</groupId>\n          </exclusion>\n          <exclusion>\n            <groupId>org.eclipse.jetty.aggregate</groupId>\n            <artifactId>*</artifactId>\n          </exclusion>\n          <exclusion>\n            <groupId>javax.mail</groupId>\n            <artifactId>mail</artifactId>\n          </exclusion>\n          <exclusion>\n            <groupId>org.apache.zookeeper</groupId>\n            <artifactId>zookeeper</artifactId>\n          </exclusion>\n          <exclusion>\n            <groupId>org.pentaho</groupId>\n            <artifactId>*</artifactId>\n          </exclusion>\n          <exclusion>\n            <groupId>com.esotericsoftware</groupId>\n            <artifactId>kryo-shaded</artifactId>\n          </exclusion>\n          <exclusion>\n            <groupId>org.apache.logging.log4j</groupId>\n            <artifactId>*</artifactId>\n          </exclusion>\n          <exclusion>\n            <groupId>log4j</groupId>\n            <artifactId>log4j</artifactId>\n          </exclusion>\n          <exclusion>\n            <groupId>log4j</groupId>\n            <artifactId>apache-log4j-extras</artifactId>\n          </exclusion>\n          <exclusion>\n            <groupId>org.apache.hbase</groupId>\n            <artifactId>*</artifactId>\n          </exclusion>\n        </exclusions>\n      </dependency>\n\n      <dependency>\n        <groupId>com.facebook.presto</groupId>\n        <artifactId>presto-jdbc</artifactId>\n        <version>${presto.version}</version>\n      </dependency>\n\n      <dependency>\n        <groupId>io.trino</groupId>\n        <artifactId>trino-jdbc</artifactId>\n        <version>${trino.version}</version>\n      </dependency>\n\n      <!-- Zookeeper -->\n      <dependency>\n        <groupId>org.apache.curator</groupId>\n        <artifactId>curator-framework</artifactId>\n        <version>${zk-curator.version}</version>\n        <exclusions>\n          <exclusion>\n            <groupId>org.slf4j</groupId>\n            <artifactId>slf4j-log4j12</artifactId>\n          </exclusion>\n          <exclusion>\n            <groupId>log4j</groupId>\n            <artifactId>log4j</artifactId>\n          </exclusion>\n        </exclusions>\n      </dependency>\n\n      <dependency>\n        <groupId>org.apache.curator</groupId>\n        <artifactId>curator-client</artifactId>\n        <version>${zk-curator.version}</version>\n      </dependency>\n\n      <dependency>\n        <groupId>org.apache.curator</groupId>\n        <artifactId>curator-recipes</artifactId>\n        <version>${zk-curator.version}</version>\n      </dependency>\n\n      <!-- Protobuf -->\n      <dependency>\n        <groupId>com.google.protobuf</groupId>\n        <artifactId>protobuf-java</artifactId>\n        <version>${proto.version}</version>\n      </dependency>\n      <dependency>\n        <groupId>com.google.protobuf</groupId>\n        <artifactId>protobuf-java-util</artifactId>\n        <version>${proto.version}</version>\n      </dependency>\n\n      <!-- Junit 5 -->\n      <dependency>\n        <groupId>org.junit.jupiter</groupId>\n        <artifactId>junit-jupiter-api</artifactId>\n        <version>${junit.jupiter.version}</version>\n        <scope>test</scope>\n      </dependency>\n      <dependency>\n        <groupId>org.junit.jupiter</groupId>\n        <artifactId>junit-jupiter-engine</artifactId>\n        <version>${junit.jupiter.version}</version>\n        <scope>test</scope>\n      </dependency>\n      <dependency>\n        <groupId>org.junit.vintage</groupId>\n        <artifactId>junit-vintage-engine</artifactId>\n        <version>${junit.vintage.version}</version>\n        <scope>test</scope>\n      </dependency>\n      <dependency>\n        <groupId>org.junit.jupiter</groupId>\n        <artifactId>junit-jupiter-params</artifactId>\n        <version>${junit.jupiter.version}</version>\n        <scope>test</scope>\n      </dependency>\n      <dependency>\n        <groupId>org.mockito</groupId>\n        <artifactId>mockito-junit-jupiter</artifactId>\n        <scope>test</scope>\n        <version>${mockito.jupiter.version}</version>\n      </dependency>\n      <dependency>\n        <groupId>org.mockito</groupId>\n        <artifactId>mockito-inline</artifactId>\n        <scope>test</scope>\n        <version>${mockito.jupiter.version}</version>\n      </dependency>\n      <dependency>\n        <groupId>org.junit.platform</groupId>\n        <artifactId>junit-platform-runner</artifactId>\n        <version>${junit.platform.version}</version>\n        <scope>test</scope>\n      </dependency>\n      <dependency>\n        <groupId>org.junit.platform</groupId>\n        <artifactId>junit-platform-suite-api</artifactId>\n        <version>${junit.platform.version}</version>\n        <scope>test</scope>\n      </dependency>\n      <dependency>\n        <groupId>org.junit.platform</groupId>\n        <artifactId>junit-platform-commons</artifactId>\n        <version>${junit.platform.version}</version>\n        <scope>test</scope>\n      </dependency>\n\n      <!-- Kryo -->\n      <dependency>\n        <groupId>com.esotericsoftware</groupId>\n        <artifactId>kryo-shaded</artifactId>\n        <version>${kryo.shaded.version}</version>\n        <scope>provided</scope>\n      </dependency>\n\n      <dependency>\n        <!--Used to test execution in task executor after de-serializing-->\n        <groupId>com.esotericsoftware</groupId>\n        <artifactId>kryo</artifactId>\n        <version>4.0.0</version>\n        <scope>test</scope>\n      </dependency>\n\n      <!-- Other Utils -->\n      <dependency>\n        <groupId>org.apache.flink</groupId>\n        <artifactId>flink-test-utils_${scala.binary.version}</artifactId>\n        <version>${flink.version}</version>\n        <scope>test</scope>\n        <exclusions>\n          <exclusion>\n            <groupId>org.apache.logging.log4j</groupId>\n            <artifactId>*</artifactId>\n          </exclusion>\n          <exclusion>\n            <groupId>org.junit.jupiter</groupId>\n            <artifactId>*</artifactId>\n          </exclusion>\n        </exclusions>\n      </dependency>\n\n      <!--  Spring Boot  -->\n      <dependency>\n        <groupId>org.springframework.boot</groupId>\n        <artifactId>spring-boot-starter-test</artifactId>\n        <version>${springboot.version}</version>\n        <scope>test</scope>\n        <exclusions>\n          <exclusion>\n            <groupId>org.springframework.boot</groupId>\n            <artifactId>spring-boot-starter-logging</artifactId>\n          </exclusion>\n        </exclusions>\n      </dependency>\n      <dependency>\n        <groupId>org.springframework.shell</groupId>\n        <artifactId>spring-shell-starter</artifactId>\n        <version>${spring.shell.version}</version>\n        <exclusions>\n          <exclusion>\n            <groupId>com.google.guava</groupId>\n            <artifactId>guava</artifactId>\n          </exclusion>\n        </exclusions>\n      </dependency>\n\n      <!-- Confluent -->\n      <dependency>\n        <groupId>io.confluent</groupId>\n        <artifactId>kafka-avro-serializer</artifactId>\n        <version>${confluent.version}</version>\n      </dependency>\n      <dependency>\n        <groupId>io.confluent</groupId>\n        <artifactId>common-config</artifactId>\n        <version>${confluent.version}</version>\n      </dependency>\n      <dependency>\n        <groupId>io.confluent</groupId>\n        <artifactId>common-utils</artifactId>\n        <version>${confluent.version}</version>\n      </dependency>\n      <dependency>\n        <groupId>io.confluent</groupId>\n        <artifactId>kafka-schema-registry-client</artifactId>\n        <version>${confluent.version}</version>\n      </dependency>\n      <dependency>\n        <groupId>io.confluent</groupId>\n        <artifactId>kafka-protobuf-serializer</artifactId>\n        <version>${confluent.version}</version>\n      </dependency>\n      <dependency>\n        <groupId>io.confluent</groupId>\n        <artifactId>kafka-json-schema-serializer</artifactId>\n        <version>${confluent.version}</version>\n        <exclusions>\n          <exclusion>\n            <groupId>com.kjetland</groupId>\n            <artifactId>mbknor-jackson-jsonschema_2.12</artifactId>\n          </exclusion>\n        </exclusions>\n      </dependency>\n      <dependency>\n        <groupId>com.kjetland</groupId>\n        <artifactId>mbknor-jackson-jsonschema_${scala.binary.version}</artifactId>\n        <version>1.0.39</version>\n      </dependency>\n    </dependencies>\n  </dependencyManagement>\n  <repositories>\n    <repository>\n      <id>Maven Central</id>\n      <name>Maven Repository</name>\n      <url>https://repo.maven.apache.org/maven2</url>\n      <releases>\n        <enabled>true</enabled>\n      </releases>\n      <snapshots>\n        <enabled>false</enabled>\n      </snapshots>\n    </repository>\n    <repository>\n      <id>cloudera-repo-releases</id>\n      <url>https://repository.cloudera.com/artifactory/public/</url>\n      <releases>\n        <enabled>true</enabled>\n      </releases>\n      <snapshots>\n        <enabled>false</enabled>\n      </snapshots>\n    </repository>\n    <repository>\n      <id>confluent</id>\n      <url>https://packages.confluent.io/maven/</url>\n    </repository>\n  </repositories>\n\n  <profiles>\n    <profile>\n      <id>release</id>\n      <activation>\n        <property>\n          <name>deployArtifacts</name>\n          <value>true</value>\n        </property>\n      </activation>\n      <build>\n        <plugins>\n          <plugin>\n            <groupId>org.apache.maven.plugins</groupId>\n            <artifactId>maven-source-plugin</artifactId>\n            <version>2.2.1</version>\n            <executions>\n              <execution>\n                <id>attach-sources</id>\n                <goals>\n                  <goal>jar-no-fork</goal>\n                </goals>\n              </execution>\n            </executions>\n          </plugin>\n          <plugin>\n            <groupId>org.apache.maven.plugins</groupId>\n            <artifactId>maven-javadoc-plugin</artifactId>\n            <version>${maven-javadoc-plugin.version}</version>\n            <executions>\n              <execution>\n                <id>attach-javadocs</id>\n                <goals>\n                  <goal>jar</goal>\n                </goals>\n              </execution>\n            </executions>\n            <configuration>\n              <doclint>none</doclint>\n            </configuration>\n          </plugin>\n          <plugin>\n            <groupId>org.apache.maven.plugins</groupId>\n            <artifactId>maven-gpg-plugin</artifactId>\n            <version>1.4</version>\n            <executions>\n              <execution>\n                <id>sign-artifacts</id>\n                <phase>verify</phase>\n                <goals>\n                  <goal>sign</goal>\n                </goals>\n              </execution>\n            </executions>\n          </plugin>\n        </plugins>\n      </build>\n    </profile>\n    <profile>\n      <id>warn-log</id>\n      <activation>\n        <property>\n          <name>env.HUDI_QUIETER_LOGGING</name>\n        </property>\n      </activation>\n      <properties>\n        <surefire-log4j.file>log4j2-surefire-quiet.properties</surefire-log4j.file>\n      </properties>\n    </profile>\n    <profile>\n      <id>unit-tests</id>\n      <properties>\n        <skipUTs>false</skipUTs>\n        <skipFTs>true</skipFTs>\n        <skipITs>true</skipITs>\n      </properties>\n      <build>\n        <plugins>\n          <plugin>\n            <groupId>org.apache.maven.plugins</groupId>\n            <artifactId>maven-surefire-plugin</artifactId>\n            <version>${maven-surefire-plugin.version}</version>\n            <configuration combine.self=\"append\">\n              <skip>${skipUTs}</skip>\n              <forkedProcessExitTimeoutInSeconds>120</forkedProcessExitTimeoutInSeconds>\n              <excludedGroups>functional</excludedGroups>\n              <excludes>\n                <exclude>**/*FunctionalTestSuite.java</exclude>\n                <exclude>**/IT*.java</exclude>\n                <exclude>**/testsuite/**/Test*.java</exclude>\n              </excludes>\n            </configuration>\n          </plugin>\n          <plugin>\n            <groupId>org.jacoco</groupId>\n            <artifactId>jacoco-maven-plugin</artifactId>\n            <executions>\n              <execution>\n                <goals>\n                  <goal>prepare-agent</goal>\n                </goals>\n              </execution>\n              <execution>\n                <id>post-unit-tests</id>\n                <phase>test</phase>\n                <goals>\n                  <goal>report</goal>\n                </goals>\n                <configuration>\n                  <outputDirectory>${project.reporting.outputDirectory}/jacoco-ut</outputDirectory>\n                </configuration>\n              </execution>\n            </executions>\n          </plugin>\n        </plugins>\n      </build>\n    </profile>\n    <profile>\n      <id>functional-tests</id>\n      <properties>\n        <skipUTs>true</skipUTs>\n        <skipFTs>false</skipFTs>\n        <skipITs>true</skipITs>\n      </properties>\n      <build>\n        <plugins>\n          <plugin>\n            <groupId>org.apache.maven.plugins</groupId>\n            <artifactId>maven-surefire-plugin</artifactId>\n            <version>${maven-surefire-plugin.version}</version>\n            <dependencies>\n              <dependency>\n                <groupId>org.apache.maven.surefire</groupId>\n                <artifactId>surefire-junit47</artifactId>\n                <version>${maven-surefire-plugin.version}</version>\n              </dependency>\n            </dependencies>\n            <configuration combine.self=\"append\">\n              <skip>${skipFTs}</skip>\n              <forkCount>1</forkCount>\n              <reuseForks>true</reuseForks>\n              <includes>\n                <include>**/*FunctionalTestSuite.java</include>\n              </includes>\n            </configuration>\n          </plugin>\n          <plugin>\n            <groupId>org.jacoco</groupId>\n            <artifactId>jacoco-maven-plugin</artifactId>\n            <executions>\n              <execution>\n                <goals>\n                  <goal>prepare-agent</goal>\n                </goals>\n              </execution>\n              <execution>\n                <id>post-functional-tests</id>\n                <phase>test</phase>\n                <goals>\n                  <goal>report</goal>\n                </goals>\n                <configuration>\n                  <outputDirectory>${project.reporting.outputDirectory}/jacoco-ft</outputDirectory>\n                </configuration>\n              </execution>\n            </executions>\n          </plugin>\n        </plugins>\n      </build>\n    </profile>\n    <profile>\n      <id>hudi-platform-service</id>\n      <activation>\n        <property>\n          <name>deployArtifacts</name>\n          <value>true</value>\n        </property>\n      </activation>\n      <modules>\n        <module>hudi-platform-service</module>\n        <module>packaging/hudi-metaserver-server-bundle</module>\n      </modules>\n    </profile>\n    <profile>\n      <id>integration-tests</id>\n      <activation>\n        <property>\n          <name>deployArtifacts</name>\n          <value>true</value>\n        </property>\n      </activation>\n      <modules>\n        <module>docker/hoodie/hadoop</module>\n        <module>hudi-integ-test</module>\n        <module>packaging/hudi-integ-test-bundle</module>\n      </modules>\n      <properties>\n        <skipUTs>true</skipUTs>\n        <skipFTs>true</skipFTs>\n        <skipITs>${skipTests}</skipITs>\n      </properties>\n      <build>\n        <plugins>\n          <plugin>\n            <groupId>org.apache.maven.plugins</groupId>\n            <artifactId>maven-surefire-plugin</artifactId>\n            <version>${maven-surefire-plugin.version}</version>\n            <configuration combine.self=\"override\">\n              <skip>${skipUTs}</skip>\n              <useSystemClassLoader>false</useSystemClassLoader>\n            </configuration>\n          </plugin>\n          <plugin>\n            <groupId>org.apache.maven.plugins</groupId>\n            <artifactId>maven-failsafe-plugin</artifactId>\n            <configuration combine.self=\"override\">\n              <skip>${skipITs}</skip>\n              <includes>\n                <include>**/IT*.java</include>\n              </includes>\n              <systemPropertyVariables>\n                <dynamodb-local.endpoint>${dynamodb-local.endpoint}</dynamodb-local.endpoint>\n                <log4j.configurationFile>${surefire-log4j.file}</log4j.configurationFile>\n              </systemPropertyVariables>\n              <useSystemClassLoader>false</useSystemClassLoader>\n            </configuration>\n            <executions>\n              <execution>\n                <phase>integration-test</phase>\n                <goals>\n                  <goal>integration-test</goal>\n                </goals>\n              </execution>\n              <execution>\n                <id>verify-integration-test</id>\n                <phase>verify</phase>\n                <goals>\n                  <goal>verify</goal>\n                </goals>\n              </execution>\n            </executions>\n          </plugin>\n        </plugins>\n      </build>\n    </profile>\n    <profile>\n      <id>javadocs</id>\n      <build>\n        <plugins>\n          <plugin>\n            <groupId>org.apache.maven.plugins</groupId>\n            <artifactId>maven-compiler-plugin</artifactId>\n            <configuration>\n              <source>${java.version}</source>\n              <target>${java.version}</target>\n            </configuration>\n          </plugin>\n          <plugin>\n            <groupId>net.alchim31.maven</groupId>\n            <artifactId>scala-maven-plugin</artifactId>\n            <version>${scala-maven-plugin.version}</version>\n            <executions>\n              <execution>\n                <id>doc</id>\n                <phase>generate-sources</phase>\n                <goals>\n                  <goal>compile</goal>\n                </goals>\n                <configuration>\n                  <excludes>\n                    <exclude>${project.basedir}/src/main/scala</exclude>\n                  </excludes>\n                  <checkMultipleScalaVersions>false</checkMultipleScalaVersions>\n                </configuration>\n              </execution>\n            </executions>\n            <configuration>\n              <args>\n                <arg>-P:genjavadoc:out=${project.build.directory}/genjavadoc</arg>\n              </args>\n              <compilerPlugins>\n                <compilerPlugin>\n                  <groupId>com.typesafe.genjavadoc</groupId>\n                  <artifactId>genjavadoc-plugin_${scala.version}</artifactId>\n                  <version>${genjavadoc-plugin.version}</version>\n                </compilerPlugin>\n              </compilerPlugins>\n              <excludes>\n                <exclude>**/*.scala</exclude>\n              </excludes>\n            </configuration>\n          </plugin>\n          <plugin>\n            <groupId>org.codehaus.mojo</groupId>\n            <artifactId>build-helper-maven-plugin</artifactId>\n            <version>${build-helper-maven-plugin.version}</version>\n            <executions>\n              <execution>\n                <phase>generate-sources</phase>\n                <goals>\n                  <goal>add-source</goal>\n                </goals>\n                <configuration>\n                  <sources>\n                    <source>${project.build.directory}/genjavadoc</source>\n                  </sources>\n                </configuration>\n              </execution>\n            </executions>\n          </plugin>\n          <plugin>\n            <groupId>org.apache.maven.plugins</groupId>\n            <artifactId>maven-javadoc-plugin</artifactId>\n            <version>${maven-javadoc-plugin.version}</version>\n            <executions>\n              <execution>\n                <id>aggregate</id>\n                <goals>\n                  <goal>aggregate</goal>\n                </goals>\n              </execution>\n            </executions>\n            <configuration>\n              <!-- Turn off the javadoc doclint for now due to incomplete javadoc in the source\n              <doclint>all,-missing</doclint>\n              -->\n              <doclint>none</doclint>\n              <detectLinks>true</detectLinks>\n              <links>\n                <link>https://avro.apache.org/docs/${avro.version}/api/java</link>\n                <link>https://docs.spring.io/spring-shell/docs/1.2.0.RELEASE</link>\n                <link>https://fasterxml.github.io/jackson-databind/javadoc/2.6</link>\n                <link>https://hadoop.apache.org/docs/r${hadoop.version}/api</link>\n                <link>https://hbase.apache.org/2.4/apidocs</link>\n                <link>https://hive.apache.org/javadocs/r2.3.6/api</link>\n                <link>https://javadoc.io/static/io.javalin/javalin/2.3.0</link>\n                <link>https://javadoc.io/doc/org.apache.parquet/parquet-avro/${parquet.version}</link>\n                <link>https://javadoc.io/static/org.apache.parquet/parquet-hadoop/${parquet.version}</link>\n                <link>https://logging.apache.org/log4j/1.2/apidocs</link>\n                <link>https://metrics.dropwizard.io/4.1.0/apidocs</link>\n                <link>https://spark.apache.org/docs/${spark.version}/api/java</link>\n              </links>\n              <sourceFileExcludes>\n                <!--\n                    Exclude the generated java files with the static reference to\n                    the singleton instance of the Scala object, to avoid redundancy in javadoc\n                -->\n                <exclude>**/*$.java</exclude>\n              </sourceFileExcludes>\n            </configuration>\n          </plugin>\n        </plugins>\n      </build>\n    </profile>\n    <profile>\n      <id>scala-2.12</id>\n      <properties>\n        <scala.version>${scala12.version}</scala.version>\n        <scala.binary.version>2.12</scala.binary.version>\n        <pulsar.spark.version>${pulsar.spark.scala12.version}</pulsar.spark.version>\n      </properties>\n      <activation>\n        <property>\n          <name>scala-2.12</name>\n        </property>\n      </activation>\n      <build>\n        <plugins>\n          <plugin>\n            <groupId>org.apache.maven.plugins</groupId>\n            <artifactId>maven-enforcer-plugin</artifactId>\n            <version>${maven-enforcer-plugin.version}</version>\n            <executions>\n              <execution>\n                <id>enforce-versions</id>\n                <goals>\n                  <goal>enforce</goal>\n                </goals>\n                <configuration>\n                  <rules>\n                    <bannedDependencies>\n                      <excludes combine.children=\"append\">\n                        <exclude>*:*_2.11</exclude>\n                      </excludes>\n                    </bannedDependencies>\n                  </rules>\n                </configuration>\n              </execution>\n            </executions>\n          </plugin>\n        </plugins>\n      </build>\n    </profile>\n    <profile>\n      <id>scala-2.13</id>\n      <properties>\n        <scala.version>${scala13.version}</scala.version>\n        <scala.binary.version>2.13</scala.binary.version>\n        <pulsar.spark.version>${pulsar.spark.scala13.version}</pulsar.spark.version>\n      </properties>\n      <activation>\n        <property>\n          <name>scala-2.13</name>\n        </property>\n      </activation>\n      <build>\n        <plugins>\n          <plugin>\n            <groupId>org.apache.maven.plugins</groupId>\n            <artifactId>maven-enforcer-plugin</artifactId>\n            <version>${maven-enforcer-plugin.version}</version>\n            <executions>\n              <execution>\n                <id>enforce-versions</id>\n                <goals>\n                  <goal>enforce</goal>\n                </goals>\n                <configuration>\n                  <rules>\n                    <bannedDependencies>\n                      <excludes combine.children=\"append\">\n                        <exclude>*:*_2.11</exclude>\n                        <exclude>*:*_2.12</exclude>\n                      </excludes>\n                    </bannedDependencies>\n                  </rules>\n                </configuration>\n              </execution>\n            </executions>\n          </plugin>\n        </plugins>\n      </build>\n    </profile>\n\n    <!-- \"spark3\" is an alias for \"spark3.5\" -->\n    <!-- NOTE: This profile is deprecated and soon will be removed -->\n    <profile>\n      <id>spark3</id>\n      <properties>\n        <spark3.version>${spark35.version}</spark3.version>\n        <spark.version>${spark3.version}</spark.version>\n        <sparkbundle.version>3</sparkbundle.version>\n        <scala12.version>2.12.18</scala12.version>\n        <scala.version>${scala12.version}</scala.version>\n        <scala.binary.version>2.12</scala.binary.version>\n        <hudi.spark.module>hudi-spark3.5.x</hudi.spark.module>\n        <!-- This glob has to include hudi-spark3-common, hudi-spark3.2plus-common -->\n        <hudi.spark.common.module>hudi-spark3-common</hudi.spark.common.module>\n        <scalatest.version>${scalatest.spark3.version}</scalatest.version>\n        <kafka.version>3.4.1</kafka.version>\n        <hive.storage.version>2.8.1</hive.storage.version>\n        <!-- NOTE: Some Hudi modules require standalone Parquet/Orc/etc file-format dependency (hudi-hive-sync,\n                   hudi-hadoop-mr, for ex). Since these Hudi modules might be used from w/in the execution engine(s)\n                   bringing these file-formats as dependencies as well, we need to make sure that versions are\n                   synchronized to avoid classpath ambiguity -->\n        <parquet.version>1.13.1</parquet.version>\n        <orc.spark.version>1.9.1</orc.spark.version>\n        <avro.version>1.11.4</avro.version>\n        <antlr.version>4.9.3</antlr.version>\n        <fasterxml.spark3.version>2.15.2</fasterxml.spark3.version>\n        <fasterxml.version>${fasterxml.spark3.version}</fasterxml.version>\n        <fasterxml.jackson.databind.version>${fasterxml.spark3.version}</fasterxml.jackson.databind.version>\n        <fasterxml.jackson.module.scala.version>${fasterxml.spark3.version}</fasterxml.jackson.module.scala.version>\n        <fasterxml.jackson.dataformat.yaml.version>${fasterxml.spark3.version}\n        </fasterxml.jackson.dataformat.yaml.version>\n        <pulsar.spark.version>${pulsar.spark.scala12.version}</pulsar.spark.version>\n        <log4j2.version>2.20.0</log4j2.version>\n        <slf4j.version>2.0.7</slf4j.version>\n        <skipITs>true</skipITs>\n      </properties>\n      <modules>\n        <module>hudi-spark-datasource/hudi-spark3.5.x</module>\n        <module>hudi-spark-datasource/hudi-spark3-common</module>\n      </modules>\n      <dependencies>\n        <dependency>\n          <groupId>org.slf4j</groupId>\n          <artifactId>slf4j-log4j12</artifactId>\n          <version>${slf4j.version}</version>\n          <scope>test</scope>\n        </dependency>\n        <dependency>\n          <groupId>${hive.groupid}</groupId>\n          <artifactId>hive-storage-api</artifactId>\n          <version>${hive.storage.version}</version>\n        </dependency>\n      </dependencies>\n      <activation>\n        <property>\n          <name>spark3</name>\n        </property>\n      </activation>\n    </profile>\n\n    <profile>\n      <id>spark3.3</id>\n      <properties>\n        <spark3.version>${spark33.version}</spark3.version>\n        <spark.version>${spark3.version}</spark.version>\n        <sparkbundle.version>3.3</sparkbundle.version>\n        <scala12.version>2.12.15</scala12.version>\n        <scala.version>${scala12.version}</scala.version>\n        <scala.binary.version>2.12</scala.binary.version>\n        <scalatest.version>${scalatest.spark3.version}</scalatest.version>\n        <hudi.spark.module>hudi-spark3.3.x</hudi.spark.module>\n        <!-- This glob has to include hudi-spark3-common, hudi-spark3.2plus-common -->\n        <hudi.spark.common.module>hudi-spark3-common</hudi.spark.common.module>\n        <kafka.version>2.8.1</kafka.version>\n        <!-- NOTE: Some Hudi modules require standalone Parquet/Orc/etc file-format dependency (hudi-hive-sync,\n                   hudi-hadoop-mr, for ex). Since these Hudi modules might be used from w/in the execution engine(s)\n                   bringing these file-formats as dependencies as well, we need to make sure that versions are\n                   synchronized to avoid classpath ambiguity -->\n        <parquet.version>1.12.2</parquet.version>\n        <orc.spark.version>1.7.8</orc.spark.version>\n        <avro.version>1.11.4</avro.version>\n        <antlr.version>4.8</antlr.version>\n        <fasterxml.spark3.version>2.13.3</fasterxml.spark3.version>\n        <fasterxml.version>${fasterxml.spark3.version}</fasterxml.version>\n        <fasterxml.jackson.databind.version>${fasterxml.spark3.version}</fasterxml.jackson.databind.version>\n        <fasterxml.jackson.module.scala.version>${fasterxml.spark3.version}</fasterxml.jackson.module.scala.version>\n        <fasterxml.jackson.dataformat.yaml.version>${fasterxml.spark3.version}</fasterxml.jackson.dataformat.yaml.version>\n        <pulsar.spark.version>${pulsar.spark.scala12.version}</pulsar.spark.version>\n        <skipITs>true</skipITs>\n      </properties>\n      <modules>\n        <module>hudi-spark-datasource/hudi-spark3.3.x</module>\n        <module>hudi-spark-datasource/hudi-spark3-common</module>\n      </modules>\n      <activation>\n        <property>\n          <name>spark3.3</name>\n        </property>\n      </activation>\n    </profile>\n\n    <profile>\n      <id>spark3.4</id>\n      <properties>\n        <spark3.version>${spark34.version}</spark3.version>\n        <spark.version>${spark3.version}</spark.version>\n        <sparkbundle.version>3.4</sparkbundle.version>\n        <scala12.version>2.12.17</scala12.version>\n        <scala.version>${scala12.version}</scala.version>\n        <scala.binary.version>2.12</scala.binary.version>\n        <hudi.spark.module>hudi-spark3.4.x</hudi.spark.module>\n        <!-- This glob has to include hudi-spark3-common, hudi-spark3.2plus-common -->\n        <hudi.spark.common.module>hudi-spark3-common</hudi.spark.common.module>\n        <scalatest.version>${scalatest.spark3.version}</scalatest.version>\n        <kafka.version>3.3.2</kafka.version>\n        <!-- NOTE: Some Hudi modules require standalone Parquet/Orc/etc file-format dependency (hudi-hive-sync,\n                   hudi-hadoop-mr, for ex). Since these Hudi modules might be used from w/in the execution engine(s)\n                   bringing these file-formats as dependencies as well, we need to make sure that versions are\n                   synchronized to avoid classpath ambiguity -->\n        <parquet.version>1.12.3</parquet.version>\n        <orc.spark.version>1.8.3</orc.spark.version>\n        <avro.version>1.11.4</avro.version>\n        <antlr.version>4.9.3</antlr.version>\n        <fasterxml.spark3.version>2.14.2</fasterxml.spark3.version>\n        <fasterxml.version>${fasterxml.spark3.version}</fasterxml.version>\n        <fasterxml.jackson.databind.version>${fasterxml.spark3.version}</fasterxml.jackson.databind.version>\n        <fasterxml.jackson.module.scala.version>${fasterxml.spark3.version}</fasterxml.jackson.module.scala.version>\n        <fasterxml.jackson.dataformat.yaml.version>${fasterxml.spark3.version}</fasterxml.jackson.dataformat.yaml.version>\n        <pulsar.spark.version>${pulsar.spark.scala12.version}</pulsar.spark.version>\n        <log4j2.version>2.19.0</log4j2.version>\n        <slf4j.version>2.0.6</slf4j.version>\n        <skipITs>true</skipITs>\n      </properties>\n      <modules>\n        <module>hudi-spark-datasource/hudi-spark3.4.x</module>\n        <module>hudi-spark-datasource/hudi-spark3-common</module>\n      </modules>\n      <dependencies>\n        <dependency>\n          <groupId>org.slf4j</groupId>\n          <artifactId>slf4j-log4j12</artifactId>\n          <version>${slf4j.version}</version>\n          <scope>test</scope>\n        </dependency>\n      </dependencies>\n      <activation>\n        <property>\n          <name>spark3.4</name>\n        </property>\n      </activation>\n    </profile>\n\n    <profile>\n      <id>spark3.5</id>\n      <properties>\n        <spark3.version>${spark35.version}</spark3.version>\n        <spark.version>${spark3.version}</spark.version>\n        <sparkbundle.version>3.5</sparkbundle.version>\n        <scala12.version>2.12.18</scala12.version>\n        <scala13.version>2.13.8</scala13.version>\n        <hudi.spark.module>hudi-spark3.5.x</hudi.spark.module>\n        <!-- This glob has to include hudi-spark3-common, hudi-spark3.2plus-common -->\n        <hudi.spark.common.module>hudi-spark3-common</hudi.spark.common.module>\n        <scalatest.version>${scalatest.spark3.version}</scalatest.version>\n        <kafka.version>3.4.1</kafka.version>\n        <hive.storage.version>2.8.1</hive.storage.version>\n        <!-- NOTE: Some Hudi modules require standalone Parquet/Orc/etc file-format dependency (hudi-hive-sync,\n                   hudi-hadoop-mr, for ex). Since these Hudi modules might be used from w/in the execution engine(s)\n                   bringing these file-formats as dependencies as well, we need to make sure that versions are\n                   synchronized to avoid classpath ambiguity -->\n        <parquet.version>1.13.1</parquet.version>\n        <orc.spark.version>1.9.1</orc.spark.version>\n        <avro.version>1.11.4</avro.version>\n        <antlr.version>4.9.3</antlr.version>\n        <fasterxml.spark3.version>2.15.2</fasterxml.spark3.version>\n        <fasterxml.version>${fasterxml.spark3.version}</fasterxml.version>\n        <fasterxml.jackson.databind.version>${fasterxml.spark3.version}</fasterxml.jackson.databind.version>\n        <fasterxml.jackson.module.scala.version>${fasterxml.spark3.version}</fasterxml.jackson.module.scala.version>\n        <fasterxml.jackson.dataformat.yaml.version>${fasterxml.spark3.version}</fasterxml.jackson.dataformat.yaml.version>\n        <log4j2.version>2.20.0</log4j2.version>\n        <slf4j.version>2.0.7</slf4j.version>\n        <skipITs>false</skipITs>\n      </properties>\n      <modules>\n        <module>hudi-spark-datasource/hudi-spark3.5.x</module>\n        <module>hudi-spark-datasource/hudi-spark3-common</module>\n      </modules>\n      <dependencies>\n        <dependency>\n          <groupId>org.slf4j</groupId>\n          <artifactId>slf4j-log4j12</artifactId>\n          <version>${slf4j.version}</version>\n          <scope>test</scope>\n        </dependency>\n        <dependency>\n          <groupId>${hive.groupid}</groupId>\n          <artifactId>hive-storage-api</artifactId>\n          <version>${hive.storage.version}</version>\n        </dependency>\n      </dependencies>\n      <activation>\n        <activeByDefault>true</activeByDefault>\n        <property>\n          <name>spark3.5</name>\n        </property>\n      </activation>\n    </profile>\n\n    <profile>\n      <id>flink1.20</id>\n      <properties>\n        <orc.flink.version>1.5.6</orc.flink.version>\n        <flink.avro.version>1.11.4</flink.avro.version>\n        <flink.format.parquet.version>1.13.1</flink.format.parquet.version>\n      </properties>\n      <activation>\n        <property>\n          <name>flink1.20</name>\n        </property>\n      </activation>\n    </profile>\n    <profile>\n      <id>flink1.19</id>\n      <properties>\n        <flink.version>${flink1.19.version}</flink.version>\n        <hudi.flink.module>hudi-flink1.19.x</hudi.flink.module>\n        <flink.bundle.version>1.19</flink.bundle.version>\n        <orc.flink.version>1.5.6</orc.flink.version>\n        <flink.avro.version>1.11.4</flink.avro.version>\n        <flink.format.parquet.version>1.13.1</flink.format.parquet.version>\n        <flink.connector.kafka.version>3.2.0-1.19</flink.connector.kafka.version>\n      </properties>\n      <activation>\n        <property>\n          <name>flink1.19</name>\n        </property>\n      </activation>\n    </profile>\n    <profile>\n      <id>flink1.18</id>\n      <properties>\n        <flink.version>${flink1.18.version}</flink.version>\n        <hudi.flink.module>hudi-flink1.18.x</hudi.flink.module>\n        <flink.bundle.version>1.18</flink.bundle.version>\n        <orc.flink.version>1.5.6</orc.flink.version>\n        <flink.avro.version>1.11.4</flink.avro.version>\n        <flink.format.parquet.version>1.13.1</flink.format.parquet.version>\n        <flink.connector.kafka.version>3.2.0-1.18</flink.connector.kafka.version>\n      </properties>\n      <activation>\n        <property>\n          <name>flink1.18</name>\n        </property>\n      </activation>\n    </profile>\n    <profile>\n      <id>flink1.17</id>\n      <properties>\n        <flink.version>${flink1.17.version}</flink.version>\n        <hudi.flink.module>hudi-flink1.17.x</hudi.flink.module>\n        <flink.bundle.version>1.17</flink.bundle.version>\n        <orc.flink.version>1.5.6</orc.flink.version>\n        <flink.avro.version>1.11.4</flink.avro.version>\n        <flink.format.parquet.version>1.12.3</flink.format.parquet.version>\n        <flink.connector.kafka.version>${flink1.17.version}</flink.connector.kafka.version>\n      </properties>\n      <activation>\n        <property>\n          <name>flink1.17</name>\n        </property>\n      </activation>\n    </profile>\n    <profile>\n      <id>flink1.16</id>\n      <properties>\n        <flink.version>${flink1.16.version}</flink.version>\n        <hudi.flink.module>hudi-flink1.16.x</hudi.flink.module>\n        <flink.bundle.version>1.16</flink.bundle.version>\n        <orc.flink.version>1.5.6</orc.flink.version>\n        <flink.avro.version>1.11.4</flink.avro.version>\n        <flink.format.parquet.version>1.12.2</flink.format.parquet.version>\n        <flink.connector.kafka.version>${flink1.16.version}</flink.connector.kafka.version>\n      </properties>\n      <activation>\n        <property>\n          <name>flink1.16</name>\n        </property>\n      </activation>\n    </profile>\n    <profile>\n    <id>flink1.15</id>\n      <properties>\n        <flink.version>${flink1.15.version}</flink.version>\n        <hudi.flink.module>hudi-flink1.15.x</hudi.flink.module>\n        <flink.bundle.version>1.15</flink.bundle.version>\n        <orc.flink.version>1.5.6</orc.flink.version>\n        <flink.avro.version>1.11.4</flink.avro.version>\n        <flink.format.parquet.version>1.12.2</flink.format.parquet.version>\n        <flink.connector.kafka.version>${flink1.15.version}</flink.connector.kafka.version>\n      </properties>\n      <activation>\n        <property>\n          <name>flink1.15</name>\n        </property>\n      </activation>\n    </profile>\n    <profile>\n      <id>flink1.14</id>\n      <properties>\n        <flink.version>${flink1.14.version}</flink.version>\n        <hudi.flink.module>hudi-flink1.14.x</hudi.flink.module>\n        <flink.bundle.version>1.14</flink.bundle.version>\n        <orc.flink.version>1.5.6</orc.flink.version>\n        <flink.avro.version>1.11.4</flink.avro.version>\n        <flink.table.runtime.artifactId>flink-table-runtime_${scala.binary.version}</flink.table.runtime.artifactId>\n        <flink.table.planner.artifactId>flink-table-planner_${scala.binary.version}</flink.table.planner.artifactId>\n        <flink.parquet.artifactId>flink-parquet_${scala.binary.version}</flink.parquet.artifactId>\n        <flink.statebackend.rocksdb.artifactId>flink-statebackend-rocksdb_${scala.binary.version}</flink.statebackend.rocksdb.artifactId>\n        <flink.test.utils.artifactId>flink-test-utils_${scala.binary.version}</flink.test.utils.artifactId>\n        <flink.streaming.java.artifactId>flink-streaming-java_${scala.binary.version}</flink.streaming.java.artifactId>\n        <flink.clients.artifactId>flink-clients_${scala.binary.version}</flink.clients.artifactId>\n        <flink.connector.kafka.artifactId>flink-connector-kafka_${scala.binary.version}</flink.connector.kafka.artifactId>\n        <flink.hadoop.compatibility.artifactId>flink-hadoop-compatibility_${scala.binary.version}</flink.hadoop.compatibility.artifactId>\n        <flink.format.parquet.version>1.11.1</flink.format.parquet.version>\n        <flink.connector.kafka.version>${flink1.14.version}</flink.connector.kafka.version>\n      </properties>\n      <activation>\n        <property>\n          <name>flink1.14</name>\n        </property>\n      </activation>\n    </profile>\n\n    <profile>\n      <id>skipShadeSources</id>\n      <properties>\n        <shadeSources>false</shadeSources>\n      </properties>\n      <activation>\n        <property>\n          <name>skipShadeSources</name>\n        </property>\n      </activation>\n    </profile>\n\n    <profile>\n      <id>java17</id>\n      <properties>\n        <argLine>-Xmx2g --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djol.magicFieldOffset=true</argLine>\n      </properties>\n      <activation>\n        <property>\n          <name>java17</name>\n        </property>\n      </activation>\n    </profile>\n\n  </profiles>\n\n</project>\n"
        },
        {
          "name": "release",
          "type": "tree",
          "content": null
        },
        {
          "name": "rfc",
          "type": "tree",
          "content": null
        },
        {
          "name": "scripts",
          "type": "tree",
          "content": null
        },
        {
          "name": "style",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}