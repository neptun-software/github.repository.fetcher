{
  "metadata": {
    "timestamp": 1736708776364,
    "page": 261,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjI3MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "apache/hive",
      "stars": 5608,
      "defaultBranch": "master",
      "files": [
        {
          "name": ".asf.yaml",
          "type": "blob",
          "size": 1.23046875,
          "content": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\ngithub:\n  description: \"Apache Hive\"\n  homepage: https://hive.apache.org/\n  labels:\n    - hive\n    - java\n    - database\n    - sql\n    - apache\n    - big-data\n    - hadoop\n  features:\n    wiki: false\n    issues: false\n    projects: false\n  enabled_merge_buttons:\n    squash:  true\n    merge:   false\n    rebase:  false\nnotifications:\n  commits:      commits@hive.apache.org\n  issues:       gitbox@hive.apache.org\n  pullrequests: gitbox@hive.apache.org\n  jira_options: link label\n"
        },
        {
          "name": ".checkstyle",
          "type": "blob",
          "size": 1.2958984375,
          "content": "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<!--\n  Licensed to the Apache Software Foundation (ASF) under one\n  or more contributor license agreements.  See the NOTICE file\n  distributed with this work for additional information\n  regarding copyright ownership.  The ASF licenses this file\n  to you under the Apache License, Version 2.0 (the\n  \"License\"); you may not use this file except in compliance\n  with the License.  You may obtain a copy of the License at\n \n      http://www.apache.org/licenses/LICENSE-2.0\n \n  Unless required by applicable law or agreed to in writing, software\n  distributed under the License is distributed on an \"AS IS\" BASIS,\n  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n  See the License for the specific language governing permissions and\n  limitations under the License.\n--> \n\n<fileset-config file-format-version=\"1.2.0\" simple-config=\"false\">\n  <local-check-config name=\"Hive Checkstyle\" location=\"checkstyle/checkstyle.xml\" type=\"project\" description=\"\">\n    <additional-data name=\"protect-config-file\" value=\"true\"/>\n  </local-check-config>\n  <fileset name=\"Hive Java Source Files\" enabled=\"true\" check-config-name=\"Hive Checkstyle\" local=\"true\">\n    <file-match-pattern match-pattern=\"^(?!(build|ant)).*java$\" include-pattern=\"true\"/>\n  </fileset>\n</fileset-config>\n"
        },
        {
          "name": ".gitattributes",
          "type": "blob",
          "size": 0.5166015625,
          "content": "# Auto detect text files and perform LF normalization\n*        text=auto\n\n*.cs     text diff=csharp\n*.java   text diff=java\n*.html   text diff=html\n*.py     text diff=python\n*.pl     text diff=perl\n*.pm     text diff=perl\n*.css    text\n*.js     text\n*.sql    text\n*.q      text\n*.q.out  text diff\n\n*.sh     text eol=lf\n\n#test files, use lf so that size is same on windows as well\ndata/files/*.dat    text eol=lf\n\n*.bat    text eol=crlf\n*.cmd    text eol=crlf\n*.csproj text merge=union eol=crlf\n*.sln    text merge=union eol=crlf\n"
        },
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.3828125,
          "content": "build\nbuild-eclipse\n.arc_jira_lib\n.classpath*\n.externalToolBuilders\n.project\n.settings\n.factorypath\n*.launch\n*.metadata\n*~\ncommon/src/gen\n.idea\n*.iml\n*.ipr\n*.iws\n*.swp\n.arc\ntarget/\nconf/hive-default.xml.template\n.DS_Store\n.factorypath\nstandalone-metastore/metastore-common/src/gen/version\nkafka-handler/src/test/gen\n**/.vscode/\n/.recommenders/\ndependency-reduced-pom.xml\n**/.mvn/.develocity*\n"
        },
        {
          "name": ".mvn",
          "type": "tree",
          "content": null
        },
        {
          "name": ".reviewboardrc",
          "type": "blob",
          "size": 1.2734375,
          "content": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n# \n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\n# This file makes it easier to post review requests to ReviewBoard.\n# Assuming you already have RBTools installed you can post a review\n# request with the following command:\n#\n# % rbt post\n#\n# The RBTools docs (including installation instructions) are located here:\n# http://www.reviewboard.org/docs/rbtools\n#\n# Note: This may not work correctly if $HOME/.reviewboardrc exists.\n\nREPOSITORY='hive-git'\nREVIEWBOARD_URL='https://reviews.apache.org'\nTRACKING_BRANCH='origin/master'\nTARGET_GROUPS='hive'\nGUESS_FIELDS='yes'\n"
        },
        {
          "name": "Jenkinsfile",
          "type": "blob",
          "size": 12.7265625,
          "content": "/*\n * Licensed to the Apache Software Foundation (ASF) under one\n * or more contributor license agreements.  See the NOTICE file\n * distributed with this work for additional information\n * regarding copyright ownership.  The ASF licenses this file\n * to you under the Apache License, Version 2.0 (the\n * \"License\"); you may not use this file except in compliance\n * with the License.  You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\ndef discardDaysToKeep = '365'\ndef discardNumToKeep = '' // Unlimited\nif (env.BRANCH_NAME != 'master') {\n  discardDaysToKeep = '60'\n  discardNumToKeep = '5'\n}\nproperties([\n    buildDiscarder(logRotator(daysToKeepStr: discardDaysToKeep, numToKeepStr: discardNumToKeep)),\n    // max 5 build/branch/day\n    rateLimitBuilds(throttle: [count: 5, durationName: 'day', userBoost: true]),\n    // do not run multiple testruns on the same branch\n    disableConcurrentBuilds(),\n    parameters([\n        string(name: 'SPLIT', defaultValue: '22', description: 'Number of buckets to split tests into.'),\n        string(name: 'OPTS', defaultValue: '', description: 'additional maven opts'),\n    ])\n])\n\nthis.prHead = null;\ndef checkPrHead() {\n  if(env.CHANGE_ID) {\n    println(\"checkPrHead - prHead:\" + prHead)\n    println(\"checkPrHead - prHead2:\" + pullRequest.head)\n    if (prHead == null) {\n      prHead = pullRequest.head;\n    } else {\n      if(prHead != pullRequest.head) {\n        currentBuild.result = 'ABORTED'\n        error('Found new changes on PR; aborting current build')\n      }\n    }\n  }\n}\ncheckPrHead()\n\ndef setPrLabel(String prLabel) {\n  if (env.CHANGE_ID) {\n   def mapping=[\n    \"SUCCESS\":\"tests passed\",\n    \"UNSTABLE\":\"tests unstable\",\n    \"FAILURE\":\"tests failed\",\n    \"PENDING\":\"tests pending\",\n   ]\n   def newLabels = []\n   for( String l : pullRequest.labels )\n     newLabels.add(l)\n   for( String l : mapping.keySet() )\n     newLabels.remove(mapping[l])\n   newLabels.add(mapping[prLabel])\n   echo ('' +newLabels)\n   pullRequest.labels=newLabels\n  }\n}\n\nsetPrLabel(\"PENDING\");\n\ndef executorNode(run) {\n  hdbPodTemplate {\n    timeout(time: 12, unit: 'HOURS') {\n      node(POD_LABEL) {\n        container('hdb') {\n          run()\n        }\n      }\n    }\n  }\n}\n\ndef buildHive(args) {\n  configFileProvider([configFile(fileId: 'artifactory', variable: 'SETTINGS')]) {\n    withEnv([\"MULTIPLIER=$params.MULTIPLIER\",\"M_OPTS=$params.OPTS\"]) {\n      sh '''#!/bin/bash -e\nset -x\n. /etc/profile.d/confs.sh\nexport USER=\"`whoami`\"\nexport MAVEN_OPTS=\"-Xmx2g\"\nexport -n HIVE_CONF_DIR\ncp $SETTINGS .git/settings.xml\nOPTS=\" -s $PWD/.git/settings.xml -B -Dtest.groups= \"\nOPTS+=\" -Pitests,qsplits,dist,errorProne\"\nOPTS+=\" -Dorg.slf4j.simpleLogger.log.org.apache.maven.plugin.surefire.SurefirePlugin=INFO\"\nOPTS+=\" -Dmaven.repo.local=$PWD/.git/m2\"\ngit config extra.mavenOpts \"$OPTS\"\nOPTS=\" $M_OPTS -Dmaven.test.failure.ignore \"\nif [ -s inclusions.txt ]; then OPTS+=\" -Dsurefire.includesFile=$PWD/inclusions.txt\";fi\nif [ -s exclusions.txt ]; then OPTS+=\" -Dsurefire.excludesFile=$PWD/exclusions.txt\";fi\nmvn $OPTS '''+args+'''\ndu -h --max-depth=1\ndf -h\n'''\n    }\n  }\n}\n\ndef sonarAnalysis(args) {\n  withCredentials([string(credentialsId: 'sonar', variable: 'SONAR_TOKEN')]) {\n      def mvnCmd = \"\"\"mvn org.sonarsource.scanner.maven:sonar-maven-plugin:3.9.1.2184:sonar \\\n      -Dsonar.organization=apache \\\n      -Dsonar.projectKey=apache_hive \\\n      -Dsonar.host.url=https://sonarcloud.io \\\n      \"\"\"+args+\" -DskipTests -Dit.skipTests -Dmaven.javadoc.skip\"\n\n      sh \"\"\"#!/bin/bash -e\n      sw java 17 && . /etc/profile.d/java.sh\n      export MAVEN_OPTS=-Xmx5G\n      \"\"\"+mvnCmd\n  }\n}\n\ndef hdbPodTemplate(closure) {\n  podTemplate(\n  containers: [\n    containerTemplate(name: 'hdb', image: 'wecharyu/hive-dev-box:executor', ttyEnabled: true, command: 'tini -- cat',\n        alwaysPullImage: true,\n        resourceRequestCpu: '1800m',\n        resourceLimitCpu: '8000m',\n        resourceRequestMemory: '6400Mi',\n        resourceLimitMemory: '12000Mi',\n        envVars: [\n            envVar(key: 'DOCKER_HOST', value: 'tcp://localhost:2375')\n        ]\n    ),\n    containerTemplate(name: 'dind', image: 'docker:18.05-dind',\n        alwaysPullImage: true,\n        privileged: true,\n    ),\n  ],\n  volumes: [\n    emptyDirVolume(mountPath: '/var/lib/docker', memory: false),\n  ], yaml:'''\nspec:\n  securityContext:\n    fsGroup: 1000\n  tolerations:\n    - key: \"type\"\n      operator: \"Equal\"\n      value: \"slave\"\n      effect: \"PreferNoSchedule\"\n    - key: \"type\"\n      operator: \"Equal\"\n      value: \"slave\"\n      effect: \"NoSchedule\"\n  nodeSelector:\n    type: slave\n''') {\n    closure();\n  }\n}\n\ndef jobWrappers(closure) {\n  def finalLabel=\"FAILURE\";\n  try {\n    // allocate 1 precommit token for the execution\n    lock(label:'hive-precommit', quantity:1, variable: 'LOCKED_RESOURCE')  {\n      timestamps {\n        echo env.LOCKED_RESOURCE\n        checkPrHead()\n        closure()\n      }\n    }\n    finalLabel=currentBuild.currentResult\n  } finally {\n    setPrLabel(finalLabel)\n  }\n}\n\ndef saveWS() {\n  sh '''#!/bin/bash -e\n    tar --exclude=archive.tar -cf archive.tar .\n    ls -l archive.tar\n    rsync -rltDq --stats archive.tar rsync://rsync/data/$LOCKED_RESOURCE'''\n}\n\ndef loadWS() {\n  sh '''#!/bin/bash -e\n    rsync -rltDq --stats rsync://rsync/data/$LOCKED_RESOURCE archive.tar\n    time tar -xf archive.tar\n    rm archive.tar\n'''\n}\n\ndef saveFile(name) {\n  sh \"\"\"#!/bin/bash -e\n    rsync -rltDq --stats ${name} rsync://rsync/data/$LOCKED_RESOURCE.${name}\"\"\"\n}\n\ndef loadFile(name) {\n  sh \"\"\"#!/bin/bash -e\n    rsync -rltDq --stats rsync://rsync/data/$LOCKED_RESOURCE.${name} ${name}\"\"\"\n}\n\n\njobWrappers {\n\n  def splits\n  executorNode {\n    container('hdb') {\n      stage('Checkout') {\n        if(env.CHANGE_ID) {\n          checkout([\n            $class: 'GitSCM',\n            branches: scm.branches,\n            doGenerateSubmoduleConfigurations: scm.doGenerateSubmoduleConfigurations,\n            extensions: scm.extensions,\n            userRemoteConfigs: scm.userRemoteConfigs + [[\n              name: 'origin',\n              refspec: scm.userRemoteConfigs[0].refspec+ \" +refs/heads/${CHANGE_TARGET}:refs/remotes/origin/target\",\n              url: scm.userRemoteConfigs[0].url,\n              credentialsId: scm.userRemoteConfigs[0].credentialsId\n            ]],\n          ])\n\t  sh '''#!/bin/bash\nset -e\necho \"@@@ patches in the PR but not on target ($CHANGE_TARGET)\"\ngit log --oneline origin/target..HEAD\necho \"@@@ patches on target but not in the PR\"\ngit log --oneline HEAD..origin/target\necho \"@@@ merging target\"\ngit config --global user.email \"you@example.com\"\ngit config --global user.name \"Your Name\"\ngit merge origin/target\n'''\n\n        } else {\n          checkout scm\n        }\n      }\n      stage('Prechecks') {\n        def spotbugsProjects = [\n            \":hive-common\",\n            \":hive-shims\",\n            \":hive-storage-api\",\n            \":hive-standalone-metastore-common\",\n            \":hive-service-rpc\"\n        ]\n        sh '''#!/bin/bash\nset -e\nxmlstarlet edit -L `find . -name pom.xml`\ngit diff\nn=`git diff | wc -l`\nif [ $n != 0 ]; then\n  echo \"!!! incorrectly formatted pom.xmls detected; see above!\" >&2\n  exit 1\nfi\n'''\n        buildHive(\"-Pspotbugs -pl \" + spotbugsProjects.join(\",\") + \" -am test-compile com.github.spotbugs:spotbugs-maven-plugin:4.0.0:check\")\n      }\n      stage('Compile') {\n        buildHive(\"install -Dtest=noMatches\")\n      }\n      checkPrHead()\n      stage('Upload') {\n        saveWS()\n        sh '''#!/bin/bash -e\n            # make parallel-test-execution plugins source scanner happy ~ better results for 1st run\n            find . -name '*.java'|grep /Test|grep -v src/test/java|grep org/apache|while read f;do t=\"`echo $f|sed 's|.*org/apache|happy/src/test/java/org/apache|'`\";mkdir -p  \"${t%/*}\";touch \"$t\";done\n        '''\n        splits = splitTests parallelism: count(Integer.parseInt(params.SPLIT)), generateInclusions: true, estimateTestsFromFiles: true\n      }\n    }\n  }\n\n  def branches = [:]\n  for (def d in ['derby','postgres',/*'mysql','oracle'*/]) {\n    def dbType=d\n    def splitName = \"init@$dbType\"\n    branches[splitName] = {\n      executorNode {\n        stage('Prepare') {\n            loadWS();\n        }\n        stage('init-metastore') {\n           withEnv([\"dbType=$dbType\"]) {\n             sh '''#!/bin/bash -e\nset -x\necho 127.0.0.1 dev_$dbType | sudo tee -a /etc/hosts\n. /etc/profile.d/confs.sh\nsw hive-dev $PWD\nexport DOCKER_NETWORK=host\nexport DBNAME=metastore\nreinit_metastore $dbType\ntime docker rm -f dev_$dbType || true\n'''\n          }\n        }\n      }\n    }\n  }\n  branches['nightly-check'] = {\n      executorNode {\n        stage('Prepare') {\n            loadWS();\n        }\n        stage('Build') {\n            sh '''#!/bin/bash\nset -e\ndev-support/nightly\n'''\n            buildHive(\"install -Dtest=noMatches -Pdist -pl packaging -am\")\n        }\n        stage('Verify') {\n            sh '''#!/bin/bash\nset -e\ntar -xzf packaging/target/apache-hive-*-nightly-*-src.tar.gz\n'''\n            buildHive(\"install -Dtest=noMatches -Pdist -f apache-hive-*-nightly-*/pom.xml\")\n        }\n      }\n  }\n  branches['sonar'] = {\n      executorNode {\n          if(env.BRANCH_NAME == 'master') {\n              stage('Prepare') {\n                  loadWS();\n              }\n              stage('Sonar') {\n                  sonarAnalysis(\"-Dsonar.branch.name=${BRANCH_NAME}\")\n              }\n          } else if(env.CHANGE_ID) {\n              stage('Prepare') {\n                  loadWS();\n              }\n              stage('Sonar') {\n                  sonarAnalysis(\"\"\"-Dsonar.pullrequest.github.repository=apache/hive \\\n                                   -Dsonar.pullrequest.key=${CHANGE_ID} \\\n                                   -Dsonar.pullrequest.branch=${CHANGE_BRANCH} \\\n                                   -Dsonar.pullrequest.base=${CHANGE_TARGET} \\\n                                   -Dsonar.pullrequest.provider=GitHub\"\"\")\n              }\n          } else {\n              echo \"Skipping sonar analysis, we only run it on PRs and on the master branch, found ${env.BRANCH_NAME}\"\n          }\n      }\n  }\n  for (int i = 0; i < splits.size(); i++) {\n    def num = i\n    def split = splits[num]\n    def splitName=String.format(\"split-%02d\",num+1)\n    branches[splitName] = {\n      executorNode {\n        stage('Prepare') {\n            loadWS();\n            writeFile file: (split.includes ? \"inclusions.txt\" : \"exclusions.txt\"), text: split.list.join(\"\\n\")\n            writeFile file: (split.includes ? \"exclusions.txt\" : \"inclusions.txt\"), text: ''\n            sh '''echo \"@INC\";cat inclusions.txt;echo \"@EXC\";cat exclusions.txt;echo \"@END\"'''\n        }\n        try {\n          stage('Test') {\n            buildHive(\"org.apache.maven.plugins:maven-antrun-plugin:run@{define-classpath,setup-test-dirs,setup-metastore-scripts} org.apache.maven.plugins:maven-surefire-plugin:test -q\")\n          }\n        } finally {\n          stage('PostProcess') {\n            try {\n              sh \"\"\"#!/bin/bash -e\n                FAILED_FILES=`find . -name \"TEST*xml\" -exec grep -l \"<failure\" {} \\\\; 2>/dev/null | head -n 10`\n                for a in \\$FAILED_FILES\n                do\n                  RENAME_TMP=`echo \\$a | sed s/TEST-//g`\n                  mv \\${RENAME_TMP/.xml/-output.txt} \\${RENAME_TMP/.xml/-output-save.txt}\n                done\n                # remove all output.txt files\n                find . -name '*output.txt' -path '*/surefire-reports/*' -exec unlink \"{}\" \\\\;\n              \"\"\"\n            } finally {\n              def fn=\"${splitName}.tgz\"\n              sh \"\"\"#!/bin/bash -e\n              tar -czf ${fn} --files-from  <(find . -path '*/surefire-reports/*')\"\"\"\n              saveFile(fn)\n              junit '**/TEST-*.xml'\n            }\n          }\n        }\n      }\n    }\n  }\n  branches['javadoc-check'] = {\n    executorNode {\n      stage('Prepare') {\n          loadWS();\n      }\n      stage('Generate javadoc') {\n          sh \"\"\"#!/bin/bash -e\nmvn install javadoc:javadoc javadoc:aggregate -DskipTests -pl '!itests/hive-jmh,!itests/util'\n\"\"\"\n      }\n    }\n  }\n  try {\n    stage('Testing') {\n      parallel branches\n    }\n  } finally {\n    stage('Archive') {\n      executorNode {\n        for (int i = 0; i < splits.size(); i++) {\n          def num = i\n          def splitName=String.format(\"split-%02d\",num+1)\n          def fn=\"${splitName}.tgz\"\n          loadFile(fn)\n          sh(\"\"\"#!/bin/bash -e\n              mkdir ${splitName}\n              tar xzf ${fn} -C ${splitName}\n              unlink ${fn}\"\"\")\n        }\n        sh(\"\"\"#!/bin/bash -e\n        tar czf test-results.tgz split*\"\"\")\n        archiveArtifacts artifacts: \"**/test-results.tgz\"\n      }\n    }\n  }\n}\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 19.8173828125,
          "content": "\n                                 Apache License\n                           Version 2.0, January 2017\n                        http://www.apache.org/licenses/\n\n   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n\n   1. Definitions.\n\n      \"License\" shall mean the terms and conditions for use, reproduction,\n      and distribution as defined by Sections 1 through 9 of this document.\n\n      \"Licensor\" shall mean the copyright owner or entity authorized by\n      the copyright owner that is granting the License.\n\n      \"Legal Entity\" shall mean the union of the acting entity and all\n      other entities that control, are controlled by, or are under common\n      control with that entity. For the purposes of this definition,\n      \"control\" means (i) the power, direct or indirect, to cause the\n      direction or management of such entity, whether by contract or\n      otherwise, or (ii) ownership of fifty percent (50%) or more of the\n      outstanding shares, or (iii) beneficial ownership of such entity.\n\n      \"You\" (or \"Your\") shall mean an individual or Legal Entity\n      exercising permissions granted by this License.\n\n      \"Source\" form shall mean the preferred form for making modifications,\n      including but not limited to software source code, documentation\n      source, and configuration files.\n\n      \"Object\" form shall mean any form resulting from mechanical\n      transformation or translation of a Source form, including but\n      not limited to compiled object code, generated documentation,\n      and conversions to other media types.\n\n      \"Work\" shall mean the work of authorship, whether in Source or\n      Object form, made available under the License, as indicated by a\n      copyright notice that is included in or attached to the work\n      (an example is provided in the Appendix below).\n\n      \"Derivative Works\" shall mean any work, whether in Source or Object\n      form, that is based on (or derived from) the Work and for which the\n      editorial revisions, annotations, elaborations, or other modifications\n      represent, as a whole, an original work of authorship. For the purposes\n      of this License, Derivative Works shall not include works that remain\n      separable from, or merely link (or bind by name) to the interfaces of,\n      the Work and Derivative Works thereof.\n\n      \"Contribution\" shall mean any work of authorship, including\n      the original version of the Work and any modifications or additions\n      to that Work or Derivative Works thereof, that is intentionally\n      submitted to Licensor for inclusion in the Work by the copyright owner\n      or by an individual or Legal Entity authorized to submit on behalf of\n      the copyright owner. For the purposes of this definition, \"submitted\"\n      means any form of electronic, verbal, or written communication sent\n      to the Licensor or its representatives, including but not limited to\n      communication on electronic mailing lists, source code control systems,\n      and issue tracking systems that are managed by, or on behalf of, the\n      Licensor for the purpose of discussing and improving the Work, but\n      excluding communication that is conspicuously marked or otherwise\n      designated in writing by the copyright owner as \"Not a Contribution.\"\n\n      \"Contributor\" shall mean Licensor and any individual or Legal Entity\n      on behalf of whom a Contribution has been received by Licensor and\n      subsequently incorporated within the Work.\n\n   2. Grant of Copyright License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      copyright license to reproduce, prepare Derivative Works of,\n      publicly display, publicly perform, sublicense, and distribute the\n      Work and such Derivative Works in Source or Object form.\n\n   3. Grant of Patent License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      (except as stated in this section) patent license to make, have made,\n      use, offer to sell, sell, import, and otherwise transfer the Work,\n      where such license applies only to those patent claims licensable\n      by such Contributor that are necessarily infringed by their\n      Contribution(s) alone or by combination of their Contribution(s)\n      with the Work to which such Contribution(s) was submitted. If You\n      institute patent litigation against any entity (including a\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\n      or a Contribution incorporated within the Work constitutes direct\n      or contributory patent infringement, then any patent licenses\n      granted to You under this License for that Work shall terminate\n      as of the date such litigation is filed.\n\n   4. Redistribution. You may reproduce and distribute copies of the\n      Work or Derivative Works thereof in any medium, with or without\n      modifications, and in Source or Object form, provided that You\n      meet the following conditions:\n\n      (a) You must give any other recipients of the Work or\n          Derivative Works a copy of this License; and\n\n      (b) You must cause any modified files to carry prominent notices\n          stating that You changed the files; and\n\n      (c) You must retain, in the Source form of any Derivative Works\n          that You distribute, all copyright, patent, trademark, and\n          attribution notices from the Source form of the Work,\n          excluding those notices that do not pertain to any part of\n          the Derivative Works; and\n\n      (d) If the Work includes a \"NOTICE\" text file as part of its\n          distribution, then any Derivative Works that You distribute must\n          include a readable copy of the attribution notices contained\n          within such NOTICE file, excluding those notices that do not\n          pertain to any part of the Derivative Works, in at least one\n          of the following places: within a NOTICE text file distributed\n          as part of the Derivative Works; within the Source form or\n          documentation, if provided along with the Derivative Works; or,\n          within a display generated by the Derivative Works, if and\n          wherever such third-party notices normally appear. The contents\n          of the NOTICE file are for informational purposes only and\n          do not modify the License. You may add Your own attribution\n          notices within Derivative Works that You distribute, alongside\n          or as an addendum to the NOTICE text from the Work, provided\n          that such additional attribution notices cannot be construed\n          as modifying the License.\n\n      You may add Your own copyright statement to Your modifications and\n      may provide additional or different license terms and conditions\n      for use, reproduction, or distribution of Your modifications, or\n      for any such Derivative Works as a whole, provided Your use,\n      reproduction, and distribution of the Work otherwise complies with\n      the conditions stated in this License.\n\n   5. Submission of Contributions. Unless You explicitly state otherwise,\n      any Contribution intentionally submitted for inclusion in the Work\n      by You to the Licensor shall be under the terms and conditions of\n      this License, without any additional terms or conditions.\n      Notwithstanding the above, nothing herein shall supersede or modify\n      the terms of any separate license agreement you may have executed\n      with Licensor regarding such Contributions.\n\n   6. Trademarks. This License does not grant permission to use the trade\n      names, trademarks, service marks, or product names of the Licensor,\n      except as required for reasonable and customary use in describing the\n      origin of the Work and reproducing the content of the NOTICE file.\n\n   7. Disclaimer of Warranty. Unless required by applicable law or\n      agreed to in writing, Licensor provides the Work (and each\n      Contributor provides its Contributions) on an \"AS IS\" BASIS,\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n      implied, including, without limitation, any warranties or conditions\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n      PARTICULAR PURPOSE. You are solely responsible for determining the\n      appropriateness of using or redistributing the Work and assume any\n      risks associated with Your exercise of permissions under this License.\n\n   8. Limitation of Liability. In no event and under no legal theory,\n      whether in tort (including negligence), contract, or otherwise,\n      unless required by applicable law (such as deliberate and grossly\n      negligent acts) or agreed to in writing, shall any Contributor be\n      liable to You for damages, including any direct, indirect, special,\n      incidental, or consequential damages of any character arising as a\n      result of this License or out of the use or inability to use the\n      Work (including but not limited to damages for loss of goodwill,\n      work stoppage, computer failure or malfunction, or any and all\n      other commercial damages or losses), even if such Contributor\n      has been advised of the possibility of such damages.\n\n   9. Accepting Warranty or Additional Liability. While redistributing\n      the Work or Derivative Works thereof, You may choose to offer,\n      and charge a fee for, acceptance of support, warranty, indemnity,\n      or other liability obligations and/or rights consistent with this\n      License. However, in accepting such obligations, You may act only\n      on Your own behalf and on Your sole responsibility, not on behalf\n      of any other Contributor, and only if You agree to indemnify,\n      defend, and hold each Contributor harmless for any liability contains\n      incurred by, or claims asserted against, such Contributor by reason\n      of your accepting any such warranty or additional liability.\n\n   END OF TERMS AND CONDITIONS\n\n   APPENDIX: How to apply the Apache License to your work.\n\n      To apply the Apache License to your work, attach the following\n      boilerplate notice, with the fields enclosed by brackets \"[]\"\n      replaced with your own identifying information. (Don't include\n      the brackets!)  The text should be enclosed in the appropriate\n      comment syntax for the file format. We also recommend that a\n      file or class name and description of purpose be included on the\n      same \"printed page\" as the copyright notice for easier\n      identification within third-party archives.\n\n   Copyright [yyyy] [name of copyright owner]\n\n   Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.\n\n\nAPACHE HIVE SUBCOMPONENTS:\n\nThe Apache Hive project contains subcomponents with separate copyright\nnotices and license terms. Your use of the source code for the these\nsubcomponents is subject to the terms and conditions of the following\nlicenses.\n\nFor org.apache.hadoop.hive.llap.daemon.impl.PriorityBlockingDeque class:\n\nThe BSD 3-Clause License\n\nCopyright (c) 2007, Aviad Ben Dov\n\nAll rights reserved.\n\nRedistribution and use in source and binary forms, with or without modification,\nare permitted provided that the following conditions are met:\n\n1. Redistributions of source code must retain the above copyright notice, this list\nof conditions and the following disclaimer.\n2. Redistributions in binary form must reproduce the above copyright notice, this\nlist of conditions and the following disclaimer in the documentation and/or other\nmaterials provided with the distribution.\n3. Neither the name of Infomancers, Ltd. nor the names of its contributors may be\nused to endorse or promote products derived from this software without specific\nprior written permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS\n\"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT\nLIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR\nA PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR\nCONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\nEXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\nPROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR\nPROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF\nLIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING\nNEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS\nSOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\nFor org.apache.hadoop.hive.serde2.lazy.fast.StringToDouble class:\n\nThe file is under the BSD 3-Clause License\n\nFor jquery.min.js:\n\nCopyright OpenJS Foundation and other contributors, https://openjsf.org/\n\nPermission is hereby granted, free of charge, to any person obtaining\na copy of this software and associated documentation files (the\n\"Software\"), to deal in the Software without restriction, including\nwithout limitation the rights to use, copy, modify, merge, publish,\ndistribute, sublicense, and/or sell copies of the Software, and to\npermit persons to whom the Software is furnished to do so, subject to\nthe following conditions:\n\nThe above copyright notice and this permission notice shall be\nincluded in all copies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND,\nEXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF\nMERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\nNONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE\nLIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION\nOF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION\nWITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\nFor jquery.sparkline.min.js:\n\nLicense: New BSD License (3-clause)\n\nCopyright (c) 2012, Splunk Inc.\nAll rights reserved.\n\nRedistribution and use in source and binary forms, with or without modification,\nare permitted provided that the following conditions are met:\n\n    * Redistributions of source code must retain the above copyright notice,\n      this list of conditions and the following disclaimer.\n    * Redistributions in binary form must reproduce the above copyright notice,\n      this list of conditions and the following disclaimer in the documentation\n      and/or other materials provided with the distribution.\n    * Neither the name of Splunk Inc nor the names of its contributors may\n      be used to endorse or promote products derived from this software without\n      specific prior written permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND ANY\nEXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES\nOF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT\nSHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,\nSPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT\nOF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)\nHOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,\nOR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS\nSOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\nFor json.human.js/json.human.css:\n\nCopyright (c) 2016 Mariano Guerra\n\nPermission is hereby granted, free of charge, to any person obtaining\na copy of this software and associated documentation files (the\n\"Software\"), to deal in the Software without restriction, including\nwithout limitation the rights to use, copy, modify, merge, publish,\ndistribute, sublicense, and/or sell copies of the Software, and to\npermit persons to whom the Software is furnished to do so, subject to\nthe following conditions:\n\nThe above copyright notice and this permission notice shall be\nincluded in all copies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND,\nEXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF\nMERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\nNONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE\nLIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION\nOF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION\nWITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\nFor argparse.py:\n\nargparse is (c) 2006-2009 Steven J. Bethard <steven.bethard@gmail.com>.\n\nThe argparse module was contributed to Python as of Python 2.7 and thus\nwas licensed under the Python license. Same license applies to all files in\nthe argparse package project.\n\nFor details about the Python License, please see doc/Python-License.txt.\n\nPYTHON SOFTWARE FOUNDATION LICENSE VERSION 2\n--------------------------------------------\n\n1. This LICENSE AGREEMENT is between the Python Software Foundation\n(\"PSF\"), and the Individual or Organization (\"Licensee\") accessing and\notherwise using this software (\"Python\") in source or binary form and\nits associated documentation.\n\n2. Subject to the terms and conditions of this License Agreement, PSF hereby\ngrants Licensee a nonexclusive, royalty-free, world-wide license to reproduce,\nanalyze, test, perform and/or display publicly, prepare derivative works,\ndistribute, and otherwise use Python alone or in any derivative version,\nprovided, however, that PSF's License Agreement and PSF's notice of copyright,\ni.e., \"Copyright (c) 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010\nPython Software Foundation; All Rights Reserved\" are retained in Python alone or\nin any derivative version prepared by Licensee.\n\n3. In the event Licensee prepares a derivative work that is based on\nor incorporates Python or any part thereof, and wants to make\nthe derivative work available to others as provided herein, then\nLicensee hereby agrees to include in any such work a brief summary of\nthe changes made to Python.\n\n4. PSF is making Python available to Licensee on an \"AS IS\"\nbasis.  PSF MAKES NO REPRESENTATIONS OR WARRANTIES, EXPRESS OR\nIMPLIED.  BY WAY OF EXAMPLE, BUT NOT LIMITATION, PSF MAKES NO AND\nDISCLAIMS ANY REPRESENTATION OR WARRANTY OF MERCHANTABILITY OR FITNESS\nFOR ANY PARTICULAR PURPOSE OR THAT THE USE OF PYTHON WILL NOT\nINFRINGE ANY THIRD PARTY RIGHTS.\n\n5. PSF SHALL NOT BE LIABLE TO LICENSEE OR ANY OTHER USERS OF PYTHON\nFOR ANY INCIDENTAL, SPECIAL, OR CONSEQUENTIAL DAMAGES OR LOSS AS\nA RESULT OF MODIFYING, DISTRIBUTING, OR OTHERWISE USING PYTHON,\nOR ANY DERIVATIVE THEREOF, EVEN IF ADVISED OF THE POSSIBILITY THEREOF.\n\n6. This License Agreement will automatically terminate upon a material\nbreach of its terms and conditions.\n\n7. Nothing in this License Agreement shall be deemed to create any\nrelationship of agency, partnership, or joint venture between PSF and\nLicensee.  This License Agreement does not grant permission to use PSF\ntrademarks or trade name in a trademark sense to endorse or promote\nproducts or services of Licensee, or any third party.\n\n8. By copying, installing or otherwise using Python, Licensee\nagrees to be bound by the terms and conditions of this License\nAgreement.\n\nFor **/fonts/glyphicons-halflings*:\n\nGLYPHICONS Halflings in Bootstrap in previous version 1.9.2\nGLYPHICONS Halflings font is also released as an extension of a Bootstrap\nwww.getbootstrap.com for free and it is released under the same license as Bootstrap.\n\nThe Bootstrap version used in Hive is Licensed under the Apache License v2.0 so does the glyphicons.\n\nFor more: https://glyphicons.com/old/license.html#old-halfings-bootstrap\n"
        },
        {
          "name": "LICENSE-binary",
          "type": "blob",
          "size": 0.1943359375,
          "content": "This product bundles dependencies under multiple licenses.\nThe bundled dependencies are listed in licenses.xml file and copies\nof their respective licenses can be found under the licenses directory.\n"
        },
        {
          "name": "NOTICE",
          "type": "blob",
          "size": 0.1611328125,
          "content": "Apache Hive\nCopyright 2008-2023 The Apache Software Foundation\n\nThis product includes software developed at\nThe Apache Software Foundation (http://www.apache.org/).\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 5.154296875,
          "content": "<!--\n{% comment %}\nLicensed to the Apache Software Foundation (ASF) under one or more\ncontributor license agreements.  See the NOTICE file distributed with\nthis work for additional information regarding copyright ownership.\nThe ASF licenses this file to you under the Apache License, Version 2.0\n(the \"License\"); you may not use this file except in compliance with\nthe License.  You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n{% endcomment %}\n-->\nApache Hive (TM)\n================\n[![Master Build Status](https://travis-ci.org/apache/hive.svg?branch=master)](https://travis-ci.org/apache/hive/branches)\n[![Maven Central](https://maven-badges.herokuapp.com/maven-central/org.apache.hive/hive/badge.svg)](http://search.maven.org/#search%7Cga%7C1%7Cg%3A%22org.apache.hive%22)\n\nThe Apache Hive (TM) data warehouse software facilitates reading,\nwriting, and managing large datasets residing in distributed storage\nusing SQL. Built on top of Apache Hadoop (TM), it provides:\n\n* Tools to enable easy access to data via SQL, thus enabling data\n  warehousing tasks such as extract/transform/load (ETL), reporting,\n  and data analysis\n\n* A mechanism to impose structure on a variety of data formats\n\n* Access to files stored either directly in Apache HDFS (TM) or in other\n  data storage systems such as Apache HBase (TM)\n\n* Query execution using Apache Hadoop MapReduce or Apache Tez frameworks.\n\nHive provides standard SQL functionality, including many of the later\n2003 and 2011 features for analytics.  These include OLAP functions,\nsubqueries, common table expressions, and more.  Hive's SQL can also be\nextended with user code via user defined functions (UDFs), user defined\naggregates (UDAFs), and user defined table functions (UDTFs).\n\nHive users can choose between Apache Hadoop MapReduce or Apache Tez\nframeworks as their execution backend. Note that MapReduce framework\nhas been deprecated since Hive 2, and Apache Tez is recommended. MapReduce\nis a mature framework that is proven at large scales. However, MapReduce\nis a purely batch framework, and queries using it may experience\nhigher latencies (tens of seconds), even over small datasets. Apache\nTez is designed for interactive query, and has substantially reduced\noverheads versus MapReduce.\n\nUsers are free to switch back and forth between these frameworks\nat any time. In each case, Hive is best suited for use cases\nwhere the amount of data processed is large enough to require a\ndistributed system.\n\nHive is not designed for online transaction processing. It is best used\nfor traditional data warehousing tasks.  Hive is designed to maximize\nscalability (scale out with more machines added dynamically to the Hadoop\ncluster), performance, extensibility, fault-tolerance, and\nloose-coupling with its input formats.\n\n\nGeneral Info\n============\n\nFor the latest information about Hive, please visit out website at:\n\n  http://hive.apache.org/\n\n\nGetting Started\n===============\n\n- Installation Instructions and a quick tutorial:\n  https://cwiki.apache.org/confluence/display/Hive/GettingStarted\n\n- Instructions to build Hive from source:\n  https://cwiki.apache.org/confluence/display/Hive/GettingStarted#GettingStarted-BuildingHivefromSource\n\n- A longer tutorial that covers more features of HiveQL:\n  https://cwiki.apache.org/confluence/display/Hive/Tutorial\n\n- The HiveQL Language Manual:\n  https://cwiki.apache.org/confluence/display/Hive/LanguageManual\n\n\nRequirements\n============\n\nJava\n------\n\n| Hive Version  | Java Version  |\n| ------------- |:-------------:|\n| Hive 1.0      | Java 6        |\n| Hive 1.1      | Java 6        |\n| Hive 1.2      | Java 7        |\n| Hive 2.x      | Java 7        |\n| Hive 3.x      | Java 8        |\n| Hive 4.x      | Java 8        |\n\n\nHadoop\n------\n\n- Hadoop 1.x, 2.x\n- Hadoop 3.x (Hive 3.x)\n\n\nUpgrading from older versions of Hive\n=====================================\n\n- Hive includes changes to the MetaStore schema. If\n  you are upgrading from an earlier version of Hive it is imperative\n  that you upgrade the MetaStore schema by running the appropriate\n  schema upgrade scripts located in the scripts/metastore/upgrade\n  directory.\n\n- We have provided upgrade scripts for MySQL, PostgreSQL, Oracle,\n  Microsoft SQL Server, and Derby databases. If you are using a\n  different database for your MetaStore you will need to provide\n  your own upgrade script.\n\nUseful mailing lists\n====================\n\n1. user@hive.apache.org - To discuss and ask usage questions. Send an\n   empty email to user-subscribe@hive.apache.org in order to subscribe\n   to this mailing list.\n\n2. dev@hive.apache.org - For discussions about code, design and features.\n   Send an empty email to dev-subscribe@hive.apache.org in order to\n   subscribe to this mailing list.\n\n3. commits@hive.apache.org - In order to monitor commits to the source\n   repository. Send an empty email to commits-subscribe@hive.apache.org\n   in order to subscribe to this mailing list.\n"
        },
        {
          "name": "RELEASE_NOTES.txt",
          "type": "blob",
          "size": 163.9521484375,
          "content": "Release Notes - Hive - Version 3.1.0\n\n** Sub-task\n    * [HIVE-12192] - Hive should carry out timestamp computations in UTC\n    * [HIVE-17227] - Incremental replication load should create tasks in execution phase rather than semantic phase \n    * [HIVE-17657] - export/import for MM tables is broken\n    * [HIVE-18193] - Migrate existing ACID tables to use write id per table rather than global transaction id\n    * [HIVE-18748] - Rename table impacts the ACID behavior as table names are not updated in meta-tables.\n    * [HIVE-18840] - CachedStore: Prioritize loading of recently accessed tables during prewarm\n    * [HIVE-18946] - Fix columnstats merge NPE\n    * [HIVE-18988] - Support bootstrap replication of ACID tables\n    * [HIVE-19009] - Retain and use runtime statistics during hs2 lifetime\n    * [HIVE-19096] - query result cache interferes with explain analyze \n    * [HIVE-19126] - CachedStore: Use memory estimation to limit cache size during prewarm\n    * [HIVE-19128] - Update golden files for spark perf tests\n    * [HIVE-19135] - Need tool to allow admins to create catalogs and move existing dbs to catalog during upgrade\n    * [HIVE-19141] - TestNegativeCliDriver insert_into_notnull_constraint, insert_into_acid_notnull failing\n    * [HIVE-19159] - TestMTQueries.testMTQueries1 failure\n    * [HIVE-19164] - TestMetastoreVersion failures\n    * [HIVE-19171] - Persist runtime statistics in metastore\n    * [HIVE-19193] - TestActivePassiveHA fails\n    * [HIVE-19194] - TestDruidStorageHandler fails\n    * [HIVE-19195] - Fix flaky tests and cleanup testconfiguration to run llap specific tests in llap only.\n    * [HIVE-19196] - TestTriggersMoveWorkloadManager is flaky\n    * [HIVE-19206] - Automatic memory management for open streaming writers\n    * [HIVE-19209] - Streaming ingest record writers should accept input stream\n    * [HIVE-19210] - Create separate module for streaming ingest\n    * [HIVE-19211] - New streaming ingest API and support for dynamic partitioning\n    * [HIVE-19214] - High throughput ingest ORC format\n    * [HIVE-19222] - TestNegativeCliDriver tests are failing due to \"java.lang.OutOfMemoryError: GC overhead limit exceeded\"\n    * [HIVE-19243] - Upgrade hadoop.version to 3.1.0\n    * [HIVE-19252] - TestJdbcWithMiniKdcCookie.testCookieNegative is failing consistently\n    * [HIVE-19274] - Add an OpTreeSignature persistence checker hook\n    * [HIVE-19332] - Disable compute.query.using.stats for external table\n    * [HIVE-19336] - Disable SMB/Bucketmap join for external tables\n    * [HIVE-19340] - Disable timeout of transactions opened by replication task at target cluster\n    * [HIVE-19347] - TestTriggersWorkloadManager tests are failing consistently\n    * [HIVE-19348] -  org.apache.hadoop.hive.ql.plan.mapping.TestOperatorCmp are failing\n    * [HIVE-19366] - Vectorization causing TestStreaming.testStreamBucketingMatchesRegularBucketing to fail\n    * [HIVE-19374] - Parse and process ALTER TABLE SET OWNER command syntax\n    * [HIVE-19409] - Disable incremental rewriting with outdated materialized views\n    * [HIVE-19472] - HiveStreamingConnection swallows exception on partition creation\n    * [HIVE-19494] - Accept shade prefix during reflective instantiation of output format\n    * [HIVE-19495] - Arrow SerDe itest failure\n    * [HIVE-19499] - Bootstrap REPL LOAD shall add tasks to create checkpoints for db/tables/partitions.\n    * [HIVE-19500] - Prevent multiple selectivity estimations for the same variable in conjuctions\n    * [HIVE-19562] - Flaky test: TestMiniSparkOnYarn FileNotFoundException in spark-submit\n    * [HIVE-19598] - Add Acid V1 to V2 upgrade module\n    * [HIVE-19637] - Add slow test report script to testutils\n    * [HIVE-19688] - Make catalogs updatable\n    * [HIVE-19727] - Fix Signature matching of table aliases\n    * [HIVE-19739] - Bootstrap REPL LOAD to use checkpoints to validate and skip the loaded data/metadata.\n    * [HIVE-19758] - Set hadoop.version=3.1.0 in standalone-metastore\n    * [HIVE-19768] - Utility to convert tables to conform to Hive strict managed tables mode\n    * [HIVE-19799] - remove jasper dependency\n    * [HIVE-19815] - Repl dump should not propagate the checkpoint and repl source properties\n    * [HIVE-19851] - upgrade jQuery version\n    * [HIVE-19852] - update jackson to latest\n    * [HIVE-19868] - Add support for float aggregator\n    * [HIVE-19892] - Disable query results cache for for HiveServer2 doAs=true\n    * [HIVE-19923] - Follow up of HIVE-19615, use UnaryFunction instead of prefix\n\n\n** Bug\n    * [HIVE-15190] - Field names are not preserved in ORC files written with ACID\n    * [HIVE-18434] - Type is not determined correctly for comparison between decimal column and string constant\n    * [HIVE-18816] - CREATE TABLE (ACID) doesn't work with TIMESTAMPLOCALTZ column type\n    * [HIVE-19016] - Vectorization and Parquet: Disable vectorization for nested complex types\n    * [HIVE-19054] - Function replication shall use \"hive.repl.replica.functions.root.dir\" as root\n    * [HIVE-19108] - Vectorization and Parquet: Turning on vectorization in parquet_ppd_decimal.q causes Wrong Query Results\n    * [HIVE-19109] - Vectorization: Enabling vectorization causes TestCliDriver delete_orig_table.q to produce Wrong Results\n    * [HIVE-19110] - Vectorization: Enabling vectorization causes TestContribCliDriver udf_example_arraymapstruct.q to produce Wrong Results\n    * [HIVE-19118] - Vectorization: Turning on vectorization in escape_crlf produces wrong results\n    * [HIVE-19120] - catalog not properly set for some tables in SQL upgrade scripts\n    * [HIVE-19131] - DecimalColumnStatsMergerTest comparison review\n    * [HIVE-19155] - Day time saving cause Druid inserts to fail with org.apache.hive.druid.io.druid.java.util.common.UOE: Cannot add overlapping segments\n    * [HIVE-19157] - Assert that Insert into Druid Table fails if the publishing of metadata by HS2 fails\n    * [HIVE-19167] - Map data type doesn't keep the order of the key/values pairs as read (Part 2, The Sequel or SQL)   \n    * [HIVE-19168] - Ranger changes for llap commands\n    * [HIVE-19186] - Multi Table INSERT statements query has a flaw for partitioned table when INSERT INTO and INSERT OVERWRITE are used\n    * [HIVE-19200] - Vectorization: Disable vectorization for LLAP I/O when a non-VECTORIZED_INPUT_FILE_FORMAT mode is needed (i.e. rows) and data type conversion is needed\n    * [HIVE-19219] - Incremental REPL DUMP should throw error if requested events are cleaned-up.\n    * [HIVE-19230] - Schema column width inconsistency in Oracle \n    * [HIVE-19231] - Beeline generates garbled output when using UnsupportedTerminal\n    * [HIVE-19237] - Only use an operatorId once in a plan\n    * [HIVE-19247] - StatsOptimizer: Missing stats fast-path for Date\n    * [HIVE-19248] - REPL LOAD couldn't copy file from source CM path and also doesn't throw error if file copy fails.\n    * [HIVE-19258] - add originals support to MM tables (and make the conversion a metadata only operation)\n    * [HIVE-19264] - Vectorization: Reenable vectorization in vector_adaptor_usage_mode.q\n    * [HIVE-19269] - Vectorization: Turn On by Default\n    * [HIVE-19275] - Vectorization: Defer Wrong Results / Execution Failures when Vectorization turned on\n    * [HIVE-19277] - Active/Passive HA web endpoints does not allow cross origin requests\n    * [HIVE-19312] - MM tables don't work with BucketizedHIF\n    * [HIVE-19317] - Handle schema evolution from int like types to decimal\n    * [HIVE-19327] - qroupby_rollup_empty.q fails for insert-only transactional tables\n    * [HIVE-19331] - Repl load config in \"with\" clause not pass to Context.getStagingDir\n    * [HIVE-19350] - Vectorization: Turn off vectorization for explainuser_1.q / spark_explainuser_1\n    * [HIVE-19352] - Vectorization: Disable vectorization for org.apache.hive.jdbc.TestJdbcDriver2.testResultSetMetaData\n    * [HIVE-19357] - Vectorization: assert_true HiveException erroneously gets suppressed to NULL\n    * [HIVE-19358] - CBO decorrelation logic should generate Hive operators\n    * [HIVE-19365] - Index on COMPLETED_TXN_COMPONENTS in Metastore RDBMS has different names in different scripts\n    * [HIVE-19370] - Issue: ADD Months function on timestamp datatype fields in hive\n    * [HIVE-19381] - Function replication in cloud fail when download resource from AWS\n    * [HIVE-19382] - Acquire locks before generating valid transaction list for some operations\n    * [HIVE-19384] - Vectorization: IfExprTimestamp* do not handle NULLs correctly\n    * [HIVE-19385] - Optional hive env variable to redirect bin/hive to use Beeline\n    * [HIVE-19389] - Schematool: For Hive's Information Schema, use embedded HS2 as default\n    * [HIVE-19410] - don't create serde reader in LLAP if there's no cache\n    * [HIVE-19418] - add background stats updater similar to compactor\n    * [HIVE-19423] - REPL LOAD creates staging directory in source dump directory instead of table data location\n    * [HIVE-19433] - HiveJoinPushTransitivePredicatesRule hangs\n    * [HIVE-19435] - Incremental replication cause data loss if a table is dropped followed by create and insert-into with different partition type.\n    * [HIVE-19454] - Test failure : org.apache.hadoop.hive.ql.TestTxnCommands2.testNonAcidToAcidConversion1 fails with java.lang.AssertionError\n    * [HIVE-19460] - Improve stats estimations for NOT IN operator\n    * [HIVE-19463] - TezTask - getting groups may fail (PartialGroupNameException in some tests)\n    * [HIVE-19467] - Make storage format configurable for temp tables created using LLAP external client\n    * [HIVE-19474] - Decimal type should be casted as part of the CTAS or INSERT Clause.\n    * [HIVE-19479] - encoded stream seek is incorrect for 0-length RGs in LLAP IO\n    * [HIVE-19481] - Tablesample uses incorrect logic to pick files corresponding to buckets.\n    * [HIVE-19485] - dump directory for non native tables should not be created\n    * [HIVE-19493] - VectorUDFDateDiffColCol copySelected does not handle nulls correctly\n    * [HIVE-19496] - Check untar folder\n    * [HIVE-19498] - Vectorization: CAST expressions produce wrong results\n    * [HIVE-19504] - Change default value for hive.auto.convert.join.shuffle.max.size property\n    * [HIVE-19516] - TestNegative merge_negative_5 and mm_concatenate are causing timeouts\n    * [HIVE-19529] - Vectorization: Date/Timestamp NULL issues\n    * [HIVE-19557] - stats: filters for dates are not taking advantage of min/max values\n    * [HIVE-19565] - Vectorization: Fix NULL / Wrong Results issues in STRING Functions\n    * [HIVE-19567] - Fix flakiness in TestTriggers\n    * [HIVE-19569] - alter table db1.t1 rename db2.t2 generates MetaStoreEventListener.onDropTable()\n    * [HIVE-19575] - TestAutoPurgeTables seems flaky\n    * [HIVE-19577] - CREATE TEMPORARY TABLE LIKE  and INSERT generate output format mismatch errors\n    * [HIVE-19578] - HLL merges tempList on every add\n    * [HIVE-19588] - Several invocation of file listing when creating VectorizedOrcAcidRowBatchReader\n    * [HIVE-19589] - Disable TestAutoPurge tests and annotate TestTriggersWorkloadManager with retry\n    * [HIVE-19590] - mask stats in llap_smb\n    * [HIVE-19592] - TestWorkloadManager - add retry for now\n    * [HIVE-19594] - Add custom tmp folders to tests to avoid collisions\n    * [HIVE-19595] - Regenerate webui port in MiniHS2\n    * [HIVE-19604] - Incorrect Handling of Boolean in DruidSerde\n    * [HIVE-19605] - TAB_COL_STATS table has no index on db/table name\n    * [HIVE-19608] - disable flaky tests 2\n    * [HIVE-19613] - GenericUDTFGetSplits should handle fetch task with temp table rewrite\n    * [HIVE-19614] - GenericUDTFGetSplits does not honor ORDER BY\n    * [HIVE-19615] - Proper handling of is null and not is null predicate when pushed to Druid\n    * [HIVE-19619] - Allow comparisons between doubles and bigints\n    * [HIVE-19629] - Enable Decimal64 reader after orc version upgrade\n    * [HIVE-19631] - reduce epic locking in AbstractService\n    * [HIVE-19632] - Remove webapps directory from standalone jar\n    * [HIVE-19639] - a transactional Hive table cannot be imported as an external table\n    * [HIVE-19643] - MM table conversion doesn't need full ACID structure checks\n    * [HIVE-19644] - change WM syntax to avoid conflicts with identifiers starting with a number\n    * [HIVE-19646] - Filesystem closed error in HiveProtoLoggingHook\n    * [HIVE-19660] - update branch-3 to be version 3.1 and fix storage-api mismatch\n    * [HIVE-19675] - Cast to timestamps on Druid time column leads to an exception\n    * [HIVE-19677] - Disable sample6.q\n    * [HIVE-19680] - Push down limit is not applied for Druid storage handler.\n    * [HIVE-19684] - Hive stats optimizer wrongly uses stats against non native tables\n    * [HIVE-19687] - Export table on acid partitioned table is failing\n    * [HIVE-19690] - multi-insert query with multiple GBY, and distinct in only some branches can produce incorrect results\n    * [HIVE-19691] - Start SessionState in materialized views registry\n    * [HIVE-19695] - Year Month Day extraction functions need to add an implicit cast for column that are String types\n    * [HIVE-19697] - TestReOptimization#testStatCachingMetaStore is flaky\n    * [HIVE-19698] - TestAMReporter#testMultipleAM is flaky\n    * [HIVE-19700] - Workaround for JLine issue with UnsupportedTerminal\n    * [HIVE-19713] - itests/hive-jmh should not reference a concreate storage-api version\n    * [HIVE-19723] - Arrow serde: \"Unsupported data type: Timestamp(NANOSECOND, null)\"\n    * [HIVE-19726] - ORC date PPD is broken\n    * [HIVE-19728] - beeline with USE_BEELINE_FOR_HIVE_CLI fails when trying to set hive.aux.jars.path\n    * [HIVE-19734] - Beeline: When beeline-site.xml is present, beeline does not honor -n (username) and -p (password) arguments\n    * [HIVE-19744] - In Beeline if -u is specified the default connection should not be tried at all\n    * [HIVE-19750] - Initialize NEXT_WRITE_ID. NWI_NEXT on converting an existing table to full acid\n    * [HIVE-19753] - Strict managed tables mode in Hive\n    * [HIVE-19754] - vector_decimal_2 failing on branch-3\n    * [HIVE-19755] - insertsel_fail.q.out needs to be updated on branch-3\n    * [HIVE-19762] - Druid Queries containing Joins gives wrong results. \n    * [HIVE-19771] - allowNullColumnForMissingStats should not be false when column stats are estimated\n    * [HIVE-19772] - Streaming ingest V2 API can generate invalid orc file if interrupted\n    * [HIVE-19773] - CBO exception while running queries with tables that are not present in materialized views\n    * [HIVE-19777] - NPE in TezSessionState\n    * [HIVE-19789] - reenable orc_llap test\n    * [HIVE-19793] - disable LLAP IO batch-to-row wrapper for ACID deletes/updates\n    * [HIVE-19794] - Disable removing order by from subquery in GenericUDTFGetSplits\n    * [HIVE-19796] - Push Down TRUNC Fn to Druid Storage Handler\n    * [HIVE-19801] - JDBC: Add some missing classes to jdbc standalone jar and remove hbase classes\n    * [HIVE-19808] - GenericUDTFGetSplits should support ACID reads in the temp. table read path\n    * [HIVE-19810] - StorageHandler fail to ship jars in Tez intermittently\n    * [HIVE-19813] - SessionState.start don't have to be synchronized\n    * [HIVE-19817] - Hive streaming API + dynamic partitioning + json/regex writer does not work\n    * [HIVE-19826] - OrcRawRecordMerger doesn't work for more than one file in non vectorized case\n    * [HIVE-19827] - hiveserver2 startup should provide a way to override TEZ_CONF_DIR\n    * [HIVE-19833] - reduce LLAP IO min allocation to match ORC variable CB size\n    * [HIVE-19837] - Setting to have different default location for external tables\n    * [HIVE-19838] - simplify & fix ColumnizedDeleteEventRegistry load loop\n    * [HIVE-19853] - Arrow serializer needs to create a TimeStampMicroTZVector instead of TimeStampMicroVector\n    * [HIVE-19857] - Set 3.1.0 for sys db version\n    * [HIVE-19859] - Inspect lock components for DBHiveLock while verifying whether transaction list is valid\n    * [HIVE-19861] - Fix temp table path generation for acid table export\n    * [HIVE-19862] - Postgres init script has a glitch around UNIQUE_DATABASE\n    * [HIVE-19864] - Address TestTriggersWorkloadManager flakiness\n    * [HIVE-19866] - improve LLAP cache purge\n    * [HIVE-19869] - Remove double formatting bug followup of HIVE-19382\n    * [HIVE-19872] - hive-schema-3.1.0.hive.sql is missing on master and branch-3\n    * [HIVE-19873] - Cleanup operation log on query cancellation after some delay\n    * [HIVE-19875] - increase LLAP IO queue size for perf\n    * [HIVE-19876] - Multiple fixes for Driver.isValidTxnListState\n    * [HIVE-19877] - Remove setting hive.execution.engine as mr in HiveStreamingConnection\n    * [HIVE-19879] - Remove unused calcite sql operator.\n    * [HIVE-19884] - Invalidation cache may throw NPE when there is no data in table used by materialized view\n    * [HIVE-19889] - Wrong results due to PPD of non deterministic functions with CBO\n    * [HIVE-19890] - ACID: Inherit bucket-id from original ROW_ID for delete deltas\n    * [HIVE-19898] - Disable TransactionalValidationListener when the table is not in the Hive catalog\n    * [HIVE-19903] - Disable temporary insert-only transactional table\n    * [HIVE-19904] - Load data rewrite into Tez job fails for ACID\n    * [HIVE-19908] - Block Insert Overwrite with Union All on full CRUD ACID tables using HIVE_UNION_SUBDIR_\n    * [HIVE-19912] - Schema evolution checks prints a log line in INFO mode for each vectorized rowbatch, impacts performance\n    * [HIVE-19917] - Export of full CRUD transactional table fails if table is not in default database\n    * [HIVE-19920] - Schematool fails in embedded mode when auth is on\n    * [HIVE-19921] - Fix perf duration and queue name in HiveProtoLoggingHook\n    * [HIVE-19938] - Upgrade scripts for information schema\n    * [HIVE-19941] - Row based Filters added via Hive Ranger policies are not pushed to druid\n    * [HIVE-19946] - VectorizedRowBatchCtx.recordIdColumnVector cannot be shared between different JVMs\n    * [HIVE-19951] - Vectorization: Need to disable encoded LLAP I/O for ORC when there is data type conversion  (Schema Evolution)\n    * [HIVE-19956] - Include yarn registry classes to jdbc standalone jar\n    * [HIVE-19964] - Apply resource plan fails if trigger expression has quotes\n    * [HIVE-19965] - Make HiveEndPoint use IMetaStoreClient.add_partition\n    * [HIVE-19972] - Followup to HIVE-19928 : Fix the check for managed table\n    * [HIVE-19973] - Enable materialized view rewriting by default\n    * [HIVE-19980] - GenericUDTFGetSplits fails when order by query returns 0 rows\n    * [HIVE-19997] - Batches for TestMiniDruidCliDriver\n    * [HIVE-20002] - Shipping jdbd-storage-handler dependency jars in LLAP\n    * [HIVE-20010] - Fix create view over literals\n    * [HIVE-20076] - ACID: Fix Synthetic ROW__ID generation for vectorized orc readers\n\n\n** New Feature\n    * [HIVE-18739] - Add support for Import/Export from Acid table\n    * [HIVE-19307] - Support ArrowOutputStream in LlapOutputFormatService\n\n\n** Improvement\n    * [HIVE-17824] - msck repair table should drop the missing partitions from metastore\n    * [HIVE-18079] - Statistics: Allow HyperLogLog to be merged to the lowest-common-denominator bit-size\n    * [HIVE-18394] - Materialized view: \"Create Materialized View\" should default to rewritable ones\n    * [HIVE-18410] - [Performance][Avro] Reading flat Avro tables is very expensive in Hive\n    * [HIVE-18743] - CREATE TABLE on S3 data can be extremely slow. DO_NOT_UPDATE_STATS workaround is buggy.\n    * [HIVE-18792] - Allow standard compliant syntax for insert on partitioned tables\n    * [HIVE-18866] - Semijoin and analyze: Implement a Long -> Hash64 vector fast-path\n    * [HIVE-19027] - Make materializations invalidation cache work with multiple active remote metastores\n    * [HIVE-19161] - Add authorizations to information schema\n    * [HIVE-19228] - Remove commons-httpclient 3.x usage\n    * [HIVE-19259] - Create view on tables having union all fail with \"Table not found\"\n    * [HIVE-19344] - Change default value of msck.repair.batch.size \n    * [HIVE-19390] - Useless error messages logged for dummy table stats\n    * [HIVE-19415] - Support CORS for all HS2 web endpoints\n    * [HIVE-19421] - Upgrade version of Jetty to 9.3.20.v20170531\n    * [HIVE-19440] - Make StorageBasedAuthorizer work with information schema\n    * [HIVE-19462] - Fix mapping for char_length function to enable pushdown to Druid. \n    * [HIVE-19464] - Upgrade Parquet to 1.10.0\n    * [HIVE-19465] - Upgrade ORC to 1.5.0\n    * [HIVE-19466] - Update constraint violation error message\n    * [HIVE-19490] - Locking on Insert into for non native and managed tables.\n    * [HIVE-19534] - Allow implementations to access member variables of AbstractRecordWriter\n    * [HIVE-19560] - Retry test runner and retry rule for flaky tests\n    * [HIVE-19572] - Add option to mask stats and data size in q files\n    * [HIVE-19586] - Optimize Count(distinct X) pushdown based on the storage capabilities \n    * [HIVE-19669] - Upgrade ORC to 1.5.1\n    * [HIVE-19682] - Provide option for GenericUDTFGetSplits to return only schema metadata\n    * [HIVE-19775] - Schematool should use HS2 embedded mode in privileged auth mode\n    * [HIVE-19776] - HiveServer2.startHiveServer2 retries of start has concurrency issues\n    * [HIVE-19824] - Improve online datasize estimations for MapJoins\n    * [HIVE-19885] - Druid Kafka Ingestion - Allow user to set kafka consumer properties via table properties\n    * [HIVE-20102] - Add a couple of additional tests for query parsing\n    * [HIVE-20135] - Fix incompatible change in TimestampColumnVector to default to UTC\n\n** Test\n    * [HIVE-19271] - TestMiniLlapLocalCliDriver default_constraint and check_constraint failing\n    * [HIVE-19515] - TestRpc.testServerPort is consistently failing\n    * [HIVE-19555] - Enable TestMiniLlapLocalCliDriver#tez_dynpart_hashjoin_1.q and TestMiniLlapLocalCliDriver#tez_vector_dynpart_hashjoin_1.q\n    * [HIVE-19573] - Fix flaky TestMiniLlapLocalCliDriver#explainuser_4.q\n    * [HIVE-19612] - Add option to mask lineage in q files\n    * [HIVE-19617] - Rename test tables to avoid collisions during execution in batches\n    * [HIVE-19620] - Change tmp directory used by PigServer in HCat tests\n    * [HIVE-19626] - Change tmp staging mapred directory for CliDriver\n    * [HIVE-19654] - Change tmp staging mapred directory for TestBlobstoreCliDriver\n    * [HIVE-19655] - Mask stats for TestMiniLlapLocalCliDriver#smb_mapjoin_15\n    * [HIVE-19699] - Re-enable TestReOptimization\n    * [HIVE-19706] - Disable TestJdbcWithMiniHS2#testHttpRetryOnServerIdleTimeout\n    * [HIVE-19731] - Change staging tmp directory used by TestHCatLoaderComplexSchema\n    * [HIVE-20123] - Fix masking tests after HIVE-19617\n\n\n** Task\n    * [HIVE-18875] - Enable SMB Join by default in Tez\n    * [HIVE-19134] - Update copyright NOTICE and fix rat check failures\n    * [HIVE-19140] - Update metastore upgrade scripts to prepare for 3.1.0 development\n    * [HIVE-19257] - HIVE-19157 commit references wrong jira\n    * [HIVE-19306] - Arrow batch serializer\n    * [HIVE-19308] - Provide an Arrow stream reader for external LLAP clients \n    * [HIVE-19323] - Create metastore SQL install and upgrade scripts for 3.1\n    * [HIVE-19488] - Enable CM root based on db parameter, identifying a db as source of replication.\n    * [HIVE-19509] - Disable tests that are failing continuously\n    * [HIVE-19512] - If parallel execution is enabled, metastore is throwing out of sequence error.\n    * [HIVE-19708] - Repl copy retrying with cm path even if the failure is due to network issue\n    * [HIVE-19725] - Add ability to dump non-native tables in replication metadata dump\n    * [HIVE-19880] - Repl Load to return recoverable vs non-recoverable error codes\n    * [HIVE-19881] - Allow metadata-only dump for database which are not source of replication\n    * [HIVE-19928] - Load Data for managed tables should set the owner of loaded files to a configurable user\n    * [HIVE-19978] - Backport HIVE-18037 to branch-3\n\n\nRelease Notes - Hive - Version 3.0.0\n\n** Sub-task\n    * [HIVE-11133] - Support hive.explain.user for Spark\n    * [HIVE-11418] - Dropping a database in an encryption zone with CASCADE and trash enabled fails\n    * [HIVE-13567] - Enable auto-gather column stats by default\n    * [HIVE-13583] - E061-14: Search Conditions\n    * [HIVE-13673] - LLAP: handle case where no service instance is found on the host specified in the input split\n    * [HIVE-14412] - Add timestamp with time zone\n    * [HIVE-14487] - Add REBUILD statement for materialized views\n    * [HIVE-14495] - Add SHOW MATERIALIZED VIEWS statement\n    * [HIVE-14498] - Freshness period for query rewriting using materialized views\n    * [HIVE-14518] - Support 'having' translation for Druid GroupBy queries\n    * [HIVE-14747] - Remove JAVA paths from profiles by sending them from ptest-client\n    * [HIVE-14947] - Add support for Acid 2 in Merge\n    * [HIVE-15016] - Run tests with Hadoop 3.0.0-beta1\n    * [HIVE-15018] - ALTER rewriting flag in materialized view \n    * [HIVE-15051] - Test framework integration with findbugs, rat checks etc.\n    * [HIVE-15173] - Allow dec as an alias for decimal\n    * [HIVE-15212] - merge branch into master\n    * [HIVE-15326] - Hive shims report Unrecognized Hadoop major version number: 3.0.0-alpha2-SNAPSHOT\n    * [HIVE-15436] - Enhancing metastore APIs to retrieve only materialized views\n    * [HIVE-15490] - REPL LOAD & DUMP support for INSERT events with change management\n    * [HIVE-15619] - Column pruner should handle DruidQuery\n    * [HIVE-15642] - Replicate Insert Overwrites, Dynamic Partition Inserts and Loads\n    * [HIVE-15673] - Allow multiple queries with disjunction\n    * [HIVE-15705] - Event replication for constraints\n    * [HIVE-15725] - Make it possible to run checkstyle for a specific module\n    * [HIVE-15758] - Allow correlated scalar subqueries with aggregates which has non-equi join predicates\n    * [HIVE-15834] - Add unit tests for org.json usage on master\n    * [HIVE-15899] - Make CTAS with acid target table and insert into acid_tbl select ... union all ... work\n    * [HIVE-15939] - Make cast expressions comply more to sql2011\n    * [HIVE-15982] - Support the width_bucket function\n    * [HIVE-15986] - Support \"is [not] distinct from\"\n    * [HIVE-16171] - Support replication of truncate table\n    * [HIVE-16186] - REPL DUMP shows last event ID of the database even if we use LIMIT option.\n    * [HIVE-16197] - Incremental insert into a partitioned table doesn't get replicated.\n    * [HIVE-16207] - Add support for Complex Types in Fast SerDe\n    * [HIVE-16228] - Support subqueries in complex expression in SELECT clause\n    * [HIVE-16256] - Flaky test: TestCliDriver.testCliDriver[comments]\n    * [HIVE-16266] - Enable function metadata to be written during bootstrap\n    * [HIVE-16267] - Enable bootstrap function metadata to be loaded in repl load\n    * [HIVE-16268] - enable incremental repl dump to handle functions metadata\n    * [HIVE-16269] - enable incremental function dump to be loaded via repl load \n    * [HIVE-16272] - support for drop function in incremental replication \n    * [HIVE-16276] - Fix NoSuchMethodError: com.amazonaws.services.s3.transfer.TransferManagerConfiguration.setMultipartUploadThreshold(I)V\n    * [HIVE-16294] - Support snapshot for truncate table\n    * [HIVE-16312] - Flaky test: TestHCatClient.testTransportFailure\n    * [HIVE-16313] - Flaky test: TestBeeLineDriver[drop_with_concurrency]\n    * [HIVE-16320] - Flaky test: TestBeeLineDriver.testCliDriver[escape_comments]\n    * [HIVE-16330] - Improve plans for scalar subquery with aggregates\n    * [HIVE-16344] - Test and support replication of exchange partition\n    * [HIVE-16372] - Enable DDL statement for non-native tables (add/remove table properties)\n    * [HIVE-16400] - Fix the MDC reference to use slf4j rather than log4j\n    * [HIVE-16416] - Service: move constants out from HiveAuthFactory\n    * [HIVE-16467] - Flaky test: TestCliDriver.testCliDriver[vector_order_null]\n    * [HIVE-16488] - Support replicating into existing db if the db is empty\n    * [HIVE-16493] - Skip column stats when colStats is empty\n    * [HIVE-16504] - Addition of binary licenses broke rat check\n    * [HIVE-16530] - Add HS2 operation logs and improve logs for REPL commands\n    * [HIVE-16532] - HIVE on hadoop 3 build failed due to hdfs client/server jar separation\n    * [HIVE-16535] - Hive fails to build from source code tarball\n    * [HIVE-16542] - make merge that targets acid 2.0 table fail-fast \n    * [HIVE-16555] - Add a new thrift API call for get_metastore_uuid\n    * [HIVE-16556] - Modify schematool scripts to initialize and create METASTORE_DB_PROPERTIES table\n    * [HIVE-16566] - Set column stats default as true when creating new tables/partitions\n    * [HIVE-16568] - Support complex types in external LLAP InputFormat\n    * [HIVE-16579] - CachedStore: improvements to partition col stats caching and cache column stats for unpartitioned table\n    * [HIVE-16586] - Fix Unit test failures when CachedStore is enabled\n    * [HIVE-16591] - DR for function Binaries on HDFS \n    * [HIVE-16600] - Refactor SetSparkReducerParallelism#needSetParallelism to enable parallel order by in multi_insert cases\n    * [HIVE-16601] - Display Session Id and Query Name / Id in Spark UI\n    * [HIVE-16617] - Clean up javadoc from errors in module hive-shims\n    * [HIVE-16618] - Clean up javadoc from errors in module hive-common\n    * [HIVE-16619] - Clean up javadoc from errors in module hive-serde\n    * [HIVE-16628] - Fix query25 when it uses a mix of MergeJoin and MapJoin\n    * [HIVE-16637] - Improve end-of-data checking for LLAP input format\n    * [HIVE-16642] - New Events created as part of replv2 potentially break replv1\n    * [HIVE-16644] - Hook Change Manager to Insert Overwrite\n    * [HIVE-16647] - Improve the validation output to make the output to stderr and stdout more consistent\n    * [HIVE-16651] - LlapProtocolClientProxy stack trace when using llap input format\n    * [HIVE-16652] - LlapInputFormat: Seeing \"output error\" WARN message\n    * [HIVE-16653] - Mergejoin should give itself a correct tag\n    * [HIVE-16672] - Parquet vectorization doesn't work for tables with partition info\n    * [HIVE-16684] - Bootstrap REPL DUMP shouldn't fail when table is dropped after fetching the table names.\n    * [HIVE-16686] - repl invocations of distcp needs additional handling\n    * [HIVE-16688] - Make sure Alter Table to set transaction=true acquires X lock\n    * [HIVE-16691] - Add test for more datatypes for LlapInputFormat\n    * [HIVE-16697] - Schema table validator should return a sorted list of missing tables \n    * [HIVE-16702] - Use LazyBinarySerDe for LLAP InputFormat\n    * [HIVE-16706] - Bootstrap REPL DUMP shouldn't fail when a partition is dropped/renamed when dump in progress.\n    * [HIVE-16714] - make Task Dependency on Repl Load more intuitive\n    * [HIVE-16715] - Clean up javadoc from errors in modules llap-client, metastore, spark-client\n    * [HIVE-16722] - Converting bucketed non-acid table to acid should perform validation\n    * [HIVE-16727] - REPL DUMP for insert event should't fail if the table is already dropped.\n    * [HIVE-16729] - Improve location validator to check for blank paths.\n    * [HIVE-16747] - Remove YETUS*.sh files after a YETUS release\n    * [HIVE-16748] - Integreate YETUS to Pre-Commit\n    * [HIVE-16750] - Support change management for rename table/partition.\n    * [HIVE-16764] - Support numeric as same as decimal\n    * [HIVE-16765] - ParquetFileReader should be closed to avoid resource leak\n    * [HIVE-16774] - Support position in ORDER BY when using SELECT *\n    * [HIVE-16775] - Fix HiveFilterAggregateTransposeRule when filter is always false\n    * [HIVE-16779] - CachedStore leak PersistenceManager resources\n    * [HIVE-16782] - Flaky Test: TestMiniLlapLocalCliDriver[subquery_scalar]\n    * [HIVE-16785] - Ensure replication actions are idempotent if any series of events are applied again.\n    * [HIVE-16797] - Enhance HiveFilterSetOpTransposeRule to remove union branches\n    * [HIVE-16813] - Incremental REPL LOAD should load the events in the same sequence as it is dumped.\n    * [HIVE-16827] - Merge stats task and column stats task into a single task\n    * [HIVE-16837] - MetadataOnly optimizer conflicts with count distinct rewrite\n    * [HIVE-16838] - Improve plans for subqueries with non-equi co-related predicates\n    * [HIVE-16848] - NPE during CachedStore refresh\n    * [HIVE-16892] - Move creation of _files from ReplCopyTask to analysis phase for boostrap replication \n    * [HIVE-16893] - move replication dump related work in semantic analysis phase to execution phase using a task\n    * [HIVE-16895] -  Multi-threaded execution of bootstrap dump of partitions\n    * [HIVE-16896] - move replication load related work in semantic analysis phase to execution phase using a task\n    * [HIVE-16901] - Distcp optimization - One distcp per ReplCopyTask \n    * [HIVE-16912] - Improve table validator's performance against Oracle\n    * [HIVE-16926] - LlapTaskUmbilicalExternalClient should not start new umbilical server for every fragment request\n    * [HIVE-16974] - Change the sort key for the schema tool validator to be <ID>\n    * [HIVE-16981] - hive.optimize.bucketingsorting should compare the schema before removing RS\n    * [HIVE-16990] - REPL LOAD should update last repl ID only after successful copy of data files.\n    * [HIVE-16992] - LLAP: monitoring and better default lambda for LRFU policy\n    * [HIVE-16996] - Add HLL as an alternative to FM sketch to compute stats\n    * [HIVE-16997] - Extend object store to store and use bit vectors\n    * [HIVE-16998] - Add config to enable HoS DPP only for map-joins\n    * [HIVE-17005] - Ensure REPL DUMP and REPL LOAD are authorized properly\n    * [HIVE-17021] - Support replication of concatenate operation.\n    * [HIVE-17087] - Remove unnecessary HoS DPP trees during map-join conversion\n    * [HIVE-17091] - \"Timed out getting readerEvents\" error from external LLAP client\n    * [HIVE-17100] - Improve HS2 operation logs for REPL commands.\n    * [HIVE-17112] - Reduce logging in HiveSparkClientFactory and RemoteHiveSparkClient\n    * [HIVE-17132] - Add InterfaceAudience and InterfaceStability annotations for UDF APIs\n    * [HIVE-17137] - Fix javolution conflict\n    * [HIVE-17153] - Flaky test: TestMiniSparkOnYarnCliDriver[spark_dynamic_partition_pruning]\n    * [HIVE-17157] - Add InterfaceAudience and InterfaceStability annotations for ObjectInspector APIs\n    * [HIVE-17167] - Create metastore specific configuration tool\n    * [HIVE-17168] - Create separate module for stand alone metastore\n    * [HIVE-17170] - Move thrift generated code to stand alone metastore\n    * [HIVE-17178] - Spark Partition Pruning Sink Operator can't target multiple Works\n    * [HIVE-17183] - Disable rename operations during bootstrap dump\n    * [HIVE-17185] - TestHiveMetaStoreStatsMerge.testStatsMerge is failing\n    * [HIVE-17195] - Long chain of tasks created by REPL LOAD shouldn't cause stack corruption.\n    * [HIVE-17196] - CM: ReplCopyTask should retain the original file names even if copied from CM path.\n    * [HIVE-17205] - add functional support for unbucketed tables\n    * [HIVE-17212] - Dynamic add partition by insert shouldn't generate INSERT event.\n    * [HIVE-17214] - check/fix conversion of unbucketed non-acid to acid\n    * [HIVE-17215] - Streaming Ingest API writing unbucketed tables\n    * [HIVE-17216] - Additional qtests for HoS DPP\n    * [HIVE-17224] - Move JDO classes to standalone metastore\n    * [HIVE-17225] - HoS DPP pruning sink ops can target parallel work objects\n    * [HIVE-17241] - Change metastore classes to not use the shims\n    * [HIVE-17247] - HoS DPP: UDFs on the partition column side does not evaluate correctly\n    * [HIVE-17256] - add a notion of a guaranteed task to LLAP\n    * [HIVE-17289] - EXPORT and IMPORT shouldn't perform distcp with doAs privileged user.\n    * [HIVE-17292] - Change TestMiniSparkOnYarnCliDriver test configuration to use the configured cores\n    * [HIVE-17297] - allow AM to use LLAP guaranteed tasks\n    * [HIVE-17307] - Change the metastore to not use the metrics code in hive/common\n    * [HIVE-17316] - Use String.startsWith for the hidden configuration variables\n    * [HIVE-17318] - Make Hikari CP configurable using hive properties in hive-site.xml\n    * [HIVE-17319] - Make BoneCp configurable using hive properties in hive-site.xml\n    * [HIVE-17330] - refactor TezSessionPoolManager to separate its multiple functions\n    * [HIVE-17346] - TestMiniSparkOnYarnCliDriver[spark_dynamic_partition_pruning] is failing every time\n    * [HIVE-17347] - TestMiniSparkOnYarnCliDriver[spark_dynamic_partition_pruning_mapjoin_only] is failing every time\n    * [HIVE-17359] - Deal with TypeInfo dependencies in the metastore\n    * [HIVE-17371] - Move tokenstores to metastore module\n    * [HIVE-17375] - stddev_samp,var_samp standard compliance\n    * [HIVE-17380] - refactor LlapProtocolClientProxy to be usable with other protocols\n    * [HIVE-17381] - When we enable Parquet Writer Version V2, hive throws an exception: Unsupported encoding: DELTA_BYTE_ARRAY.\n    * [HIVE-17382] - Change startsWith relation introduced in HIVE-17316\n    * [HIVE-17387] - implement Tez AM registry in Hive\n    * [HIVE-17405] - HoS DPP ConstantPropagate should use ConstantPropagateOption.SHORTCUT\n    * [HIVE-17409] - refactor LLAP ZK registry to make the ZK-registry part reusable\n    * [HIVE-17414] - HoS DPP + Vectorization generates invalid explain plan due to CombineEquivalentWorkResolver\n    * [HIVE-17428] - REPL LOAD of ALTER_PARTITION event doesn't create import tasks if the partition doesn't exist during analyze phase.\n    * [HIVE-17455] - External LLAP client: connection to HS2 should be kept open until explicitly closed\n    * [HIVE-17456] - Set current database for external LLAP interface\n    * [HIVE-17473] - implement workload management pools\n    * [HIVE-17482] - External LLAP client: acquire locks for tables queried directly by LLAP\n    * [HIVE-17488] - Move first set of classes to standalone metastore\n    * [HIVE-17494] - Bootstrap REPL DUMP throws exception if a partitioned table is dropped while reading partitions.\n    * [HIVE-17495] - CachedStore: prewarm improvement (avoid multiple sql calls to read partition column stats), refactoring and caching some aggregate stats\n    * [HIVE-17506] - Fix standalone-metastore pom.xml to not depend on hive's main pom\n    * [HIVE-17508] - Implement global execution triggers based on counters\n    * [HIVE-17514] - Use SHA-256 for cookie signer to improve security\n    * [HIVE-17515] - Use SHA-256 for GenericUDFMaskHash to improve security\n    * [HIVE-17527] - Support replication for rename/move table across database\n    * [HIVE-17528] - Add more q-tests for Hive-on-Spark with Parquet vectorized reader\n    * [HIVE-17534] - Add a config to turn off parquet vectorization\n    * [HIVE-17537] - Move Warehouse class to standalone metastore\n    * [HIVE-17541] - Move testing related methods from MetaStoreUtils to some testing related utility\n    * [HIVE-17566] - Create schema required for workload management.\n    * [HIVE-17581] - Replace some calcite dependencies with native ones\n    * [HIVE-17607] - remove ColumnStatsDesc usage from columnstatsupdatetask\n    * [HIVE-17608] - REPL LOAD should overwrite the data files if exists instead of duplicating it\n    * [HIVE-17617] - Rollup of an empty resultset should contain the grouping of the empty grouping set\n    * [HIVE-17629] - CachedStore - wait for prewarm at use time, not init time\n    * [HIVE-17645] - MM tables patch conflicts with HIVE-17482 (Spark/Acid integration)\n    * [HIVE-17647] - DDLTask.generateAddMmTasks(Table tbl) and other random code should not start transactions\n    * [HIVE-17651] - TableScanOperator might miss vectorization on flag\n    * [HIVE-17652] - retire ANALYZE TABLE ... PARTIALSCAN\n    * [HIVE-17661] - DBTxnManager.acquireLocks() - MM tables should use shared lock for Insert\n    * [HIVE-17671] - TableScanDesc.isAcidTable is restricted to FullAcid tables\n    * [HIVE-17681] - Need to log bootstrap dump progress state property to HS2 logs.\n    * [HIVE-17692] - Block HCat on Acid tables\n    * [HIVE-17696] - Vectorized reader does not seem to be pushing down projection columns in certain code paths\n    * [HIVE-17698] - FileSinkDesk.getMergeInputDirName() uses stmtId=0\n    * [HIVE-17708] - Upgrade surefire to 2.20.1\n    * [HIVE-17728] - TestHCatClient should use hive.metastore.transactional.event.listeners as per recommendation.\n    * [HIVE-17733] - Move RawStore to standalone metastore\n    * [HIVE-17743] - Add InterfaceAudience and InterfaceStability annotations for Thrift generated APIs\n    * [HIVE-17748] - ReplCopyTask doesn't support multi-file CopyWork\n    * [HIVE-17750] - add a flag to automatically create most tables as MM \n    * [HIVE-17756] - Enable subquery related Qtests for Hive on Spark\n    * [HIVE-17757] - REPL LOAD need to use customised configurations to execute distcp/remote copy.\n    * [HIVE-17771] - Implement commands to manage resource plan\n    * [HIVE-17778] - Add support for custom counters in trigger expression\n    * [HIVE-17809] - Implement per pool trigger validation and move sessions across pools\n    * [HIVE-17812] - Move remaining classes that HiveMetaStore depends on \n    * [HIVE-17835] - HS2 Logs print unnecessary stack trace when HoS query is cancelled\n    * [HIVE-17837] - Explicitly check if the HoS Remote Driver has been lost in the RemoteSparkJobMonitor \n    * [HIVE-17841] - implement applying the resource plan\n    * [HIVE-17842] - Run checkstyle on ptest2 module with proper configuration\n    * [HIVE-17850] - can VectorizedOrcAcidRowReader be removed once HIVE-17458 is done?\n    * [HIVE-17856] - MM tables - IOW is not ACID compliant\n    * [HIVE-17858] - MM - some union cases are broken\n    * [HIVE-17874] - Parquet vectorization fails on tables with complex columns when there are no projected columns\n    * [HIVE-17884] - Implement create, alter and drop workload management triggers\n    * [HIVE-17887] - Incremental REPL LOAD with Drop partition event on timestamp type partition column fails.\n    * [HIVE-17888] - Display the reason for query cancellation\n    * [HIVE-17897] - \"repl load\" in bootstrap phase fails when partitions have whitespace\n    * [HIVE-17902] - add notions of default pool and start adding unmanaged mapping\n    * [HIVE-17904] - handle internal Tez AM restart in registry and WM\n    * [HIVE-17905] - propagate background LLAP cluster changes to WM\n    * [HIVE-17906] - use kill query mechanics to kill queries in WM\n    * [HIVE-17907] - enable and apply resource plan commands in HS2\n    * [HIVE-17913] - Cleanup unused methods in Driver\n    * [HIVE-17926] - Support triggers for non-pool sessions\n    * [HIVE-17929] - Use sessionId for HoS Remote Driver Client id\n    * [HIVE-17931] - Implement Parquet vectorization reader for Array type\n    * [HIVE-17933] - make antlr output directory to use a top-level sourceset\n    * [HIVE-17934] - Merging Statistics are promoted to COMPLETE (most of the time)\n    * [HIVE-17945] - Support column projection for index access when using Parquet Vectorization\n    * [HIVE-17950] - Implement resource plan fetching from metastore\n    * [HIVE-17954] - Implement pool, user, group and trigger to pool management API's.\n    * [HIVE-17961] - NPE during initialization of VectorizedParquetRecordReader when input split is null\n    * [HIVE-17967] - Move HiveMetaStore class\n    * [HIVE-17970] - MM LOAD DATA with OVERWRITE doesn't use base_n directory concept\n    * [HIVE-17972] - Implement Parquet vectorization reader for Map type\n    * [HIVE-17980] - Move HiveMetaStoreClient plus a few remaining classes.\n    * [HIVE-17981] - Create a set of builders for Thrift classes\n    * [HIVE-17982] - Move metastore specific itests\n    * [HIVE-17983] - Make the standalone metastore generate tarballs etc.\n    * [HIVE-17990] - Add Thrift and DB storage for Schema Registry objects\n    * [HIVE-17991] - Remove CommandNeedRetryException\n    * [HIVE-17995] - Run checkstyle on standalone-metastore module with proper configuration\n    * [HIVE-17996] - Fix ASF headers\n    * [HIVE-17997] - Add rat plugin and configuration to standalone metastore pom\n    * [HIVE-18002] - add group support for pool mappings\n    * [HIVE-18003] - add explicit jdbc connection string args for mappings\n    * [HIVE-18004] - investigate deriving app name from JDBC connection for pool mapping\n    * [HIVE-18005] - Improve size estimation for array() to be not 0\n    * [HIVE-18025] - Push resource plan changes to tez/unmanaged sessions\n    * [HIVE-18028] - fix WM based on cluster smoke test; add logging\n    * [HIVE-18029] - beeline - support proper usernames based on the URL arg\n    * [HIVE-18031] - Support replication for Alter Database operation.\n    * [HIVE-18034] - Improving logging with HoS executors spend lots of time in GC\n    * [HIVE-18036] - Stats: Remove usage of clone() methods\n    * [HIVE-18053] - Support different table types for MVs\n    * [HIVE-18056] - CachedStore: Have a whitelist/blacklist config to allow selective caching of tables/partitions and allow read while prewarming\n    * [HIVE-18057] - remove PostExecute / PreExecute hook support\n    * [HIVE-18063] - Make CommandProcessorResponse an exception instead of a return class\n    * [HIVE-18071] - add HS2 jmx information about pools and current resource plan\n    * [HIVE-18072] - fix various WM bugs based on cluster testing - part 2\n    * [HIVE-18073] - AM may assert when its guaranteed task count is reduced\n    * [HIVE-18075] - verify commands on a cluster\n    * [HIVE-18076] - killquery doesn't actually work for non-trigger WM kills\n    * [HIVE-18078] - WM getSession needs some retry logic\n    * [HIVE-18084] - Upgrade checkstyle version to support lambdas\n    * [HIVE-18085] - Run checkstyle on storage-api module with proper configuration\n    * [HIVE-18088] - Add WM event traces at query level for debugging\n    * [HIVE-18092] - Fix exception on tables handled by HBaseHandler if columnsstats are auto-gathered\n    * [HIVE-18093] - Improve logging when HoS application is killed\n    * [HIVE-18095] - add a unmanaged flag to triggers (applies to container based sessions)\n    * [HIVE-18096] - add a user-friendly show plan command\n    * [HIVE-18125] - Support arbitrary file names in input to Load Data\n    * [HIVE-18133] - Parametrize TestTxnNoBuckets wrt Vectorization\n    * [HIVE-18134] - some alter resource plan fixes\n    * [HIVE-18138] - Fix columnstats problem in case schema evolution\n    * [HIVE-18141] - Fix StatsUtils.combineRange to combine intervals\n    * [HIVE-18149] - Stats: rownum estimation from datasize underestimates in most cases\n    * [HIVE-18153] - refactor reopen and file management in TezTask\n    * [HIVE-18161] - Remove hive.stats.atomic\n    * [HIVE-18163] - Stats: create materialized view should also collect stats\n    * [HIVE-18170] - User mapping not initialized correctly on start\n    * [HIVE-18179] - Implement validate resource plan (part 1)\n    * [HIVE-18187] - Add jamon generated-sources as source folder\n    * [HIVE-18190] - Consider looking at ORC file schema rather than using _metadata_acid file\n    * [HIVE-18192] - Introduce WriteID per table rather than using global transaction ID\n    * [HIVE-18193] - Migrate existing ACID tables to use write id per table rather than global transaction id\n    * [HIVE-18202] - Automatically migrate hbase.table.name to hbase.mapreduce.hfileoutputformat.table.name for hbase-based table\n    * [HIVE-18203] - change the way WM is enabled and allow dropping the last resource plan\n    * [HIVE-18209] - Fix API call in VectorizedListColumnReader to get value from BytesColumnVector\n    * [HIVE-18211] - Support to read multiple level definition for Map type in Parquet file\n    * [HIVE-18212] - Make sure Yetus check always has a full log\n    * [HIVE-18214] - Flaky test: TestSparkClient\n    * [HIVE-18222] - Update checkstyle rules to be less peeky\n    * [HIVE-18224] - Introduce interface above driver\n    * [HIVE-18229] - add the unmanaged mapping command\n    * [HIVE-18230] - create plan like plan, and replace plan commands for easy modification\n    * [HIVE-18235] - Columnstats gather on mm tables: re-enable disabled test\n    * [HIVE-18237] - missing results for insert_only table after DP insert\n    * [HIVE-18238] - Driver execution may not have configuration changing sideeffects \n    * [HIVE-18245] - clean up acid_vectorization_original.q\n    * [HIVE-18257] - implement scheduling policy configuration instead of hardcoding fair scheduling\n    * [HIVE-18273] - add LLAP-level counters for WM\n    * [HIVE-18274] - add AM level metrics for WM\n    * [HIVE-18275] - add HS2-level WM metrics\n    * [HIVE-18286] - java.lang.ClassCastException: org.apache.hadoop.hive.ql.exec.vector.DoubleColumnVector cannot be cast to org.apache.hadoop.hive.ql.exec.vector.LongColumnVector\n    * [HIVE-18288] - merge/concat not supported on Acid table\n    * [HIVE-18294] - add switch to make acid table the default\n    * [HIVE-18315] - update tests use non-acid tables\n    * [HIVE-18317] - Improve error messages in TransactionalValidationListerner\n    * [HIVE-18323] - Vectorization: add the support of timestamp in VectorizedPrimitiveColumnReader for parquet\n    * [HIVE-18366] - Update HBaseSerDe to use hbase.mapreduce.hfileoutputformat.table.name instead of hbase.table.name as the table name property\n    * [HIVE-18368] - Improve Spark Debug RDD Graph\n    * [HIVE-18372] - Create testing infra to test different HMS instances\n    * [HIVE-18389] - Print out Spark Web UI URL to the console log\n    * [HIVE-18411] - Fix ArrayIndexOutOfBoundsException for VectorizedListColumnReader\n    * [HIVE-18418] - clean up plugin between DAGs\n    * [HIVE-18437] - use plan parallelism for the default pool if both are present\n    * [HIVE-18438] - WM RP: it's impossible to unset things\n    * [HIVE-18443] - Ensure git gc finished in ptest prep phase before copying repo\n    * [HIVE-18457] - improve show plan output (triggers, mappings)\n    * [HIVE-18458] - Workload manager initializes even when interactive queue is not set\n    * [HIVE-18468] - Create tests to cover alterPartition and renamePartition methods\n    * [HIVE-18478] - Data files deleted from temp table should not be recycled to CM path\n    * [HIVE-18479] - Create tests to cover dropPartition methods\n    * [HIVE-18480] - Create tests for function related methods\n    * [HIVE-18481] - Create tests for table related methods (get, list, exists)\n    * [HIVE-18483] - Create tests to cover getPartition(s) methods\n    * [HIVE-18484] - Create tests to cover listPartition(s) methods\n    * [HIVE-18486] - Create tests to cover add partition methods\n    * [HIVE-18489] - Automatically migrate s3n URIs to s3a URIs\n    * [HIVE-18495] - JUnit rule to enable Driver level testing\n    * [HIVE-18496] - Create tests to cover add/alter/drop index methods\n    * [HIVE-18498] - Create tests to cover get and list index methods\n    * [HIVE-18509] - Create tests for table manipulation related methods (create, alter, drop)\n    * [HIVE-18511] - Fix generated checkstyle errors\n    * [HIVE-18536] - IOW + DP is broken for insert-only ACID\n    * [HIVE-18541] - Secure HS2 web UI with PAM\n    * [HIVE-18542] - Create tests to cover getTableMeta method\n    * [HIVE-18544] - Create tests to cover appendPartition methods\n    * [HIVE-18550] - Keep the hbase table name property as hbase.table.name\n    * [HIVE-18553] - Support schema evolution in Parquet Vectorization reader\n    * [HIVE-18566] - Create tests to cover adding partitions from PartitionSpec\n    * [HIVE-18580] - Create tests to cover exchange partitions\n    * [HIVE-18596] - Synchronize value of hive.spark.client.connect.timeout across unit tests\n    * [HIVE-18609] - Results cache invalidation based on ACID table updates\n    * [HIVE-18633] - Service discovery for Active/Passive HA mode\n    * [HIVE-18635] - Generalize hook dispatch logics in Driver\n    * [HIVE-18651] - Expose additional Spark metrics\n    * [HIVE-18663] - Logged Spark Job Id contains a UUID instead of the actual id\n    * [HIVE-18672] - Printed state in RemoteSparkJobMonitor is ambiguous\n    * [HIVE-18673] - ErrorMsg.SPARK_JOB_MONITOR_TIMEOUT isn't formatted correctly\n    * [HIVE-18677] - SparkClientImpl usage of SessionState.LogHelper doesn't respect isSilent value\n    * [HIVE-18679] - create/replicate open transaction event\n    * [HIVE-18703] - Make Operator comparision to be based on some primitive\n    * [HIVE-18715] - Remove index support from metastore\n    * [HIVE-18720] - Replicate Commit Txn operation (without writes)\n    * [HIVE-18745] - Fix MetaStore creation in tests, so multiple MetaStores can be started on the same machine\n    * [HIVE-18747] - Cleaner for TXN_TO_WRITE_ID table entries using MIN_HISTORY_LEVEL.\n    * [HIVE-18749] - Need to replace transactionId with writeId in RecordIdentifier and other relevant contexts.\n    * [HIVE-18750] - Exchange partition should be disabled on ACID/Insert-only tables with per table write ID.\n    * [HIVE-18751] - ACID table scan through get_splits UDF doesn't receive ValidWriteIdList configuration.\n    * [HIVE-18753] - Correct methods and variables names which uses writeId instead of transactionId.\n    * [HIVE-18755] - Modifications to the metastore for catalogs\n    * [HIVE-18765] - SparkClientImpl swallows exception messages from the RemoteDriver\n    * [HIVE-18771] - Refactor tests, so only 1 MetaStore instance will be started per test class and test configuration\n    * [HIVE-18781] - Create/Replicate Open, Commit (without writes) and Abort Txn events\n    * [HIVE-18805] - Add ConstantPropagate before stats annotation\n    * [HIVE-18824] - ValidWriteIdList config should be defined on tables which has to collect stats after insert\n    * [HIVE-18830] - RemoteSparkJobMonitor failures are logged twice\n    * [HIVE-18832] - Support change management for trashing data files from ACID tables.\n    * [HIVE-18840] - CachedStore: Prioritize loading of recently accessed tables during prewarm\n    * [HIVE-18846] - Query results cache: Allow queries to refer to the pending results of a query that has not finished yet\n    * [HIVE-18855] - Fix unit test TestMiniLlapLocalCliDriver.testCliDriver[results_cache_1]\n    * [HIVE-18861] - druid-hdfs-storage is pulling in hadoop-aws-2.7.x and aws SDK, creating classpath problems on hadoop 3.x\n    * [HIVE-18864] - ValidWriteIdList snapshot seems incorrect if obtained after allocating writeId by current transaction.\n    * [HIVE-18899] - Separate FetchWork required for each query that uses the results cache\n    * [HIVE-18909] - Metrics for results cache\n    * [HIVE-18926] - Imporve operator-tree matching\n    * [HIVE-18946] - Fix columnstats merge NPE\n    * [HIVE-18961] - Error in results cache when query has identifiers with spaces\n    * [HIVE-18982] - Provide a CLI option to manually trigger failover\n    * [HIVE-18988] - Support bootstrap replication of ACID tables\n    * [HIVE-18994] - Handle client connections on failover\n    * [HIVE-19009] - Retain and use runtime statistics during hs2 lifetime\n    * [HIVE-19031] - Mark duplicate configs in HiveConf as deprecated\n    * [HIVE-19083] - Make partition clause optional for INSERT\n    * [HIVE-19089] - Create/Replicate Allocate write-id event\n    * [HIVE-19112] - Support Analyze table for partitioned tables without partition spec\n    * [HIVE-19126] - CachedStore: Use memory estimation to limit cache size during prewarm\n    * [HIVE-19127] - Concurrency fixes in QueryResultsCache\n    * [HIVE-19128] - Update golden files for spark perf tests\n    * [HIVE-19129] - Support DEFAULT keyword with MERGE\n    * [HIVE-19135] - Need tool to allow admins to create catalogs and move existing dbs to catalog during upgrade\n    * [HIVE-19138] - Results cache: allow queries waiting on pending cache entries to check cache again if pending query fails\n    * [HIVE-19141] - TestNegativeCliDriver insert_into_notnull_constraint, insert_into_acid_notnull failing\n    * [HIVE-19144] - TestSparkCliDriver:subquery_scalar - golden file needs to be udpated\n    * [HIVE-19145] - Stabilize statsoptimizer.q test\n    * [HIVE-19146] - Delete dangling q.out \n    * [HIVE-19147] - Fix PerfCliDrivers: Tpcds30T missed CAT_NAME change\n    * [HIVE-19153] - Update golden files for few tests\n    * [HIVE-19154] - Poll notification events to invalidate the results cache\n    * [HIVE-19156] - TestMiniLlapLocalCliDriver.vectorized_dynamic_semijoin_reduction.q is broken\n    * [HIVE-19159] - TestMTQueries.testMTQueries1 failure\n    * [HIVE-19164] - TestMetastoreVersion failures\n    * [HIVE-19171] - Persist runtime statistics in metastore\n    * [HIVE-19175] - TestMiniLlapLocalCliDriver.testCliDriver update_access_time_non_current_db failing\n    * [HIVE-19178] - TestMiniTezCliDriver.testCliDriver[explainanalyze_5] failure\n    * [HIVE-19193] - TestActivePassiveHA fails\n    * [HIVE-19194] - TestDruidStorageHandler fails\n    * [HIVE-19195] - Fix flaky tests and cleanup testconfiguration to run llap specific tests in llap only.\n    * [HIVE-19196] - TestTriggersMoveWorkloadManager is flaky\n    * [HIVE-19197] - TestReplicationScenarios is flaky\n    * [HIVE-19206] - Automatic memory management for open streaming writers\n    * [HIVE-19209] - Streaming ingest record writers should accept input stream\n    * [HIVE-19210] - Create separate module for streaming ingest\n    * [HIVE-19211] - New streaming ingest API and support for dynamic partitioning\n    * [HIVE-19214] - High throughput ingest ORC format\n    * [HIVE-19222] - TestNegativeCliDriver tests are failing due to \"java.lang.OutOfMemoryError: GC overhead limit exceeded\"\n    * [HIVE-19232] - results_cache_invalidation2 is failing\n    * [HIVE-19274] - Add an OpTreeSignature persistence checker hook\n    * [HIVE-19319] - RuntimeStats fixes\n    * [HIVE-19322] - broken test: TestNegativeMinimrCliDriver#testCliDriver[minimr_broken_pipe]\n    * [HIVE-19335] - Disable runtime filtering (semijoin reduction opt with bloomfilter) for external tables\n    * [HIVE-19346] - TestMiniLlapLocalCliDriver.testCliDriver[materialized_view_create_rewrite_5] failling\n    * [HIVE-19347] - TestTriggersWorkloadManager tests are failing consistently\n    * [HIVE-19348] -  org.apache.hadoop.hive.ql.plan.mapping.TestOperatorCmp are failing\n    * [HIVE-19371] - Add table ownerType to HMS thrift API\n    * [HIVE-19372] - Add table ownerType to JDO/SQL and ObjectStore\n    * [HIVE-19374] - Parse and process ALTER TABLE SET OWNER command syntax\n    * [HIVE-19400] - Adjust Hive 1.0 to 2.0 conversion utility to the upgrade\n    * [HIVE-19471] - bucket_map_join_tez1 and  bucket_map_join_tez2 are failing\n    * [HIVE-19472] - HiveStreamingConnection swallows exception on partition creation\n    * [HIVE-19494] - Accept shade prefix during reflective instantiation of output format\n\n\n** Bug\n    * [HIVE-4577] - hive CLI can't handle hadoop dfs command  with space and quotes.\n    * [HIVE-6348] - Order by/Sort by in subquery\n    * [HIVE-6590] - Hive does not work properly with boolean partition columns (wrong results and inserts to incorrect HDFS path)\n    * [HIVE-6990] - Direct SQL fails when the explicit schema setting is different from the default one\n    * [HIVE-8937] - fix description of hive.security.authorization.sqlstd.confwhitelist.* params\n    * [HIVE-9815] - Metastore column\"SERDE_PARAMS\".\"PARAM_VALUE\"  limited to 4000 bytes\n    * [HIVE-10616] - TypeInfoUtils doesn't handle DECIMAL with just precision specified\n    * [HIVE-10865] - Beeline needs to support DELIMITER command\n    * [HIVE-11064] - ALTER TABLE CASCADE ERROR unbalanced calls to openTransaction/commitTransaction\n    * [HIVE-11266] - count(*) wrong result based on table statistics for external tables\n    * [HIVE-11297] - Combine op trees for partition info generating tasks\n    * [HIVE-11609] - Capability to add a filter to hbase scan via composite key doesn't work\n    * [HIVE-12408] - SQLStdAuthorizer should not require external table creator to be owner of directory, in addition to rw permissions\n    * [HIVE-12425] - OrcRecordUpdater.close(true) leaves the file open\n    * [HIVE-12631] - LLAP IO: support ORC ACID tables\n    * [HIVE-12719] - As a hive user, I am facing issues using permanent UDAF's.\n    * [HIVE-12734] - Remove redundancy in HiveConfs serialized to UDFContext\n    * [HIVE-13000] - Hive returns useless parsing error \n    * [HIVE-13652] - Import table change order of dynamic partitions\n    * [HIVE-14032] - INSERT OVERWRITE command failed with case sensitive partition key names\n    * [HIVE-14052] - Cleanup structures when external clients use LLAP\n    * [HIVE-14077] - add implicit decimal arithmetic q test, fix issues if found \n    * [HIVE-14455] - upgrade httpclient, httpcore to match updated hadoop dependency\n    * [HIVE-14560] - Support exchange partition between s3 and hdfs tables\n    * [HIVE-14564] - Column Pruning generates out of order columns in SelectOperator which cause ArrayIndexOutOfBoundsException.\n    * [HIVE-14678] - Hive-on-MR deprecation warning  is not diplayed when engine is set to capital letter 'MR'\n    * [HIVE-14731] - Use Tez cartesian product edge in Hive (unpartitioned case only)\n    * [HIVE-14792] - AvroSerde reads the remote schema-file at least once per mapper, per table reference.\n    * [HIVE-14813] - Make TransactionBatchImpl.toString() include state of each txn: commit/abort\n    * [HIVE-14988] - Support INSERT OVERWRITE into a partition on transactional tables\n    * [HIVE-15077] - Acid LockManager is unfair\n    * [HIVE-15104] - Hive on Spark generate more shuffle data than hive on mr\n    * [HIVE-15144] - JSON.org license is now CatX\n    * [HIVE-15160] - Can't order by an unselected column\n    * [HIVE-15176] - Small typo in hiveserver2 webui\n    * [HIVE-15249] - HIve 2.1.0 is throwing InvalidObjectException(message:Invalid column type name is too long\n    * [HIVE-15267] - Make query length calculation logic more accurate in TxnUtils.needNewQuery()\n    * [HIVE-15343] - Spelling errors in logging and exceptions for beeline, common, hbase-handler, hcatalog, llap-server, orc, serde and shims\n    * [HIVE-15344] - Spelling errors in logging and exceptions for metastore and service directories\n    * [HIVE-15442] - Driver.java has a redundancy  code\n    * [HIVE-15483] - Database and table name is case sensitive when used in show grant\n    * [HIVE-15504] - ArrayIndexOutOfBoundsException in GenericUDFTrunc::initialize\n    * [HIVE-15515] - Remove the docs directory\n    * [HIVE-15552] - Unable to coalesce DATE and TIMESTAMP types\n    * [HIVE-15630] - add operation handle before operation.run instead of after operation.run\n    * [HIVE-15632] - Hive/Druid integration: Incorrect result - Limit on timestamp disappears\n    * [HIVE-15635] - Hive/Druid integration: timeseries query shows all days, even if no data\n    * [HIVE-15636] - Hive/Druid integration: wrong semantics of topN query limit with granularity\n    * [HIVE-15637] - Hive/Druid integration: wrong semantics of groupBy query limit with granularity\n    * [HIVE-15639] - Hive/Druid integration: wrong semantics for ordering within groupBy queries\n    * [HIVE-15680] - Incorrect results when hive.optimize.index.filter=true and same ORC table is referenced twice in query\n    * [HIVE-15724] - getPrimaryKeys and getForeignKeys in metastore does not normalize db and table name\n    * [HIVE-15739] - Incorrect exception message in PartExprEvalUtils\n    * [HIVE-15761] - ObjectStore.getNextNotification could return an empty NotificationEventResponse causing TProtocolException \n    * [HIVE-15767] - Hive On Spark is not working on secure clusters from Oozie\n    * [HIVE-15829] - LLAP text cache: disable memory tracking on the writer\n    * [HIVE-15883] - HBase mapped table in Hive insert fail for decimal\n    * [HIVE-15995] - Syncing metastore table with serde schema\n    * [HIVE-16007] - When the query does not complie the LogRunnable never stops\n    * [HIVE-16025] - Where IN clause throws exception\n    * [HIVE-16026] - Generated query will timeout and/or kill the druid cluster.\n    * [HIVE-16027] - <timestamp> BETWEEN <string> AND <string> must cast to TIMESTMAP\n    * [HIVE-16044] - LLAP: Shuffle Handler keep-alive connections are closed from the server side\n    * [HIVE-16053] - Remove newRatio from llap JAVA_OPTS_BASE\n    * [HIVE-16057] - SchemaTool ignores --passWord argument if hadoop.security.credential.provider.path is configured\n    * [HIVE-16061] - When hive.async.log.enabled is set to true, some output is not printed to the beeline console\n    * [HIVE-16077] - UPDATE/DELETE fails with numBuckets > numReducers\n    * [HIVE-16113] - PartitionPruner::removeNonPartCols needs to handle AND/OR cases\n    * [HIVE-16117] - SortProjectTransposeRule should check for monotonicity preserving CAST\n    * [HIVE-16125] - Split work between reducers.\n    * [HIVE-16130] - Remove jackson classes from hive-jdbc standalone jar\n    * [HIVE-16147] - Rename a partitioned table should not drop its partition columns stats\n    * [HIVE-16174] - Update MetricsConstant.WAITING_COMPILE_OPS metric when we acquire lock failed in Driver\n    * [HIVE-16177] - non Acid to acid conversion doesn't handle _copy_N files\n    * [HIVE-16188] - beeline should block the connection if given invalid database name.\n    * [HIVE-16193] - Hive show compactions not reflecting the status of the application\n    * [HIVE-16213] - ObjectStore can leak Queries when rollbackTransaction throws an exception\n    * [HIVE-16219] - metastore notification_log contains serialized message with  non functional fields\n    * [HIVE-16222] - add a setting to disable row.serde for specific formats; enable for others\n    * [HIVE-16225] - Memory leak in webhcat service (FileSystem CACHE entries)\n    * [HIVE-16233] - llap: Query failed with AllocatorOutOfMemoryException\n    * [HIVE-16254] - metadata for values temporary tables for INSERTs are getting replicated during bootstrap\n    * [HIVE-16275] - Vectorization: Add ReduceSink support for TopN (in specialized native classes)\n    * [HIVE-16282] - Semijoin: Disable slow-start for the bloom filter aggregate task\n    * [HIVE-16287] - Alter table partition rename with location - moves partition back to hive warehouse\n    * [HIVE-16290] - Stats: StatsRulesProcFactory::evaluateComparator estimates are wrong when minValue == filterValue\n    * [HIVE-16291] - Hive fails when unions a parquet table with itself\n    * [HIVE-16296] - use LLAP executor count to configure reducer auto-parallelism\n    * [HIVE-16298] - Add config to specify multi-column joins have correlated columns\n    * [HIVE-16299] - MSCK REPAIR TABLE should enforce partition key order when adding unknown partitions\n    * [HIVE-16302] - Add junit dependency to hive-shims-common to compile with Hadoop 2.8+\n    * [HIVE-16305] - Additional Datanucleus ClassLoaderResolverImpl leaks causing HS2 OOM\n    * [HIVE-16307] - add IO memory usage report to LLAP UI\n    * [HIVE-16308] - PreExecutePrinter and PostExecutePrinter should log to INFO level instead of ERROR\n    * [HIVE-16309] - Hive Test Commands failure should be printed in hive.log in addition to stderr\n    * [HIVE-16315] - Describe table doesn't show num of partitions\n    * [HIVE-16316] - Prepare master branch for 3.0.0 development.\n    * [HIVE-16317] - CASE .. NULL in JOIN condition can trigger SemanticException\n    * [HIVE-16318] - LLAP cache: address some issues in 2.2/2.3\n    * [HIVE-16319] - LLAP: Better handling of an empty wait queue, should try scheduling checks\n    * [HIVE-16321] - Possible deadlock in metastore with Acid enabled\n    * [HIVE-16323] - HS2 JDOPersistenceManagerFactory.pmCache leaks after HIVE-14204\n    * [HIVE-16324] - Truncate table should not work when EXTERNAL property of table is true\n    * [HIVE-16325] - Tez session refresh based on a time interval fails\n    * [HIVE-16328] - HoS: more aggressive mapjoin optimization when hive.spark.use.ts.stats.for.mapjoin is true\n    * [HIVE-16329] - TopN: use local executor info for LLAP memory checks\n    * [HIVE-16333] - remove the redundant symbol \"\\\" to appear red in sublime text 3\n    * [HIVE-16335] - Beeline user HS2 connection file should use /etc/hive/conf instead of /etc/conf/hive\n    * [HIVE-16336] - Rename hive.spark.use.file.size.for.mapjoin to hive.spark.use.ts.stats.for.mapjoin\n    * [HIVE-16341] - Tez Task Execution Summary has incorrect input record counts on some operators\n    * [HIVE-16347] - HiveMetastoreChecker should skip listing partitions which are not valid when hive.msck.path.validation is set to skip or ignore\n    * [HIVE-16353] - Jetty 9 upgrade breaks hive master LLAP\n    * [HIVE-16357] - Failed folder creation when creating a new table is reported incorrectly\n    * [HIVE-16363] - QueryLifeTimeHooks should catch parse exceptions\n    * [HIVE-16368] - Unexpected java.lang.ArrayIndexOutOfBoundsException from query with LaterView Operation for hive on MR.\n    * [HIVE-16369] - Vectorization: Support PTF (Part 1: No Custom Window Framing -- Default Only)\n    * [HIVE-16380] - removing global test dependency of jsonassert\n    * [HIVE-16384] - Remove jdk7 build from travis\n    * [HIVE-16385] - StatsNoJobTask could exit early before all partitions have been processed\n    * [HIVE-16388] - LLAP: Log rotation for daemon, history and gc files\n    * [HIVE-16389] - Allow HookContext to access SQLOperationDisplay\n    * [HIVE-16390] - LLAP IO should take job config into account; also LLAP config should load defaults\n    * [HIVE-16393] - Fix visibility of CodahaleReporter interface\n    * [HIVE-16394] - HoS does not support queue name change in middle of session\n    * [HIVE-16396] - Sync storage-api version in pom.xml\n    * [HIVE-16399] - create an index for tc_txnid in TXN_COMPONENTS\n    * [HIVE-16402] - Upgrade to Hadoop 2.8.0\n    * [HIVE-16403] - LLAP UI shows the wrong number of executors\n    * [HIVE-16404] - Renaming of public classes in Calcite 12 breeaking druid integration\n    * [HIVE-16406] - Remove unwanted interning when creating PartitionDesc\n    * [HIVE-16409] - TestEventHandlerFactory  has lacked the ASF header\n    * [HIVE-16413] - Create table as select does not check ownership of the location\n    * [HIVE-16421] - Runtime filtering breaks user-level explain\n    * [HIVE-16422] - Should kill running Spark Jobs when a query is cancelled.\n    * [HIVE-16425] - Vectorization: unload old hashtables before reloadHashTable\n    * [HIVE-16427] - Fix multi-insert query and write qtests\n    * [HIVE-16433] - Not nullify variable \"rj\" to avoid NPE due to race condition in ExecDriver.\n    * [HIVE-16436] - Response times in \"Task Execution Summary\" at the end of the job is not correct\n    * [HIVE-16448] - Vectorization: Vectorized order_null.q fails with deserialize EOF exception below TEZ ReduceRecordSource.processVectorGroup\n    * [HIVE-16450] - Some metastore operations are not retried even with desired underlining exceptions\n    * [HIVE-16451] - Race condition between HiveStatement.getQueryLog and HiveStatement.runAsyncOnServer\n    * [HIVE-16459] - Forward channelInactive to RpcDispatcher\n    * [HIVE-16461] - DagUtils checks local resource size on the remote fs\n    * [HIVE-16462] - Vectorization: Enabling hybrid grace disables specialization of all reduce side joins\n    * [HIVE-16465] - NullPointer Exception when enable vectorization for Parquet file format\n    * [HIVE-16468] - BeeLineDriver should be able to run tests against an externally created cluster\n    * [HIVE-16471] - Add metrics for \"waiting compilation time\"\n    * [HIVE-16473] - Hive-on-Tez may fail to write to an HBase table\n    * [HIVE-16482] - Druid Ser/Des need to use dimension output name\n    * [HIVE-16483] - HoS should populate split related configurations to HiveConf\n    * [HIVE-16485] - Enable outputName for RS operator in explain formatted\n    * [HIVE-16487] - Serious Zookeeper exception is logged when a race condition happens\n    * [HIVE-16491] - CBO cant handle join involving complex types in on condition\n    * [HIVE-16494] - udaf percentile_approx() may fail on CBO\n    * [HIVE-16497] - FileUtils. isActionPermittedForFileHierarchy, isOwnerOfFileHierarchy file system operations should be impersonated\n    * [HIVE-16507] - Hive Explain User-Level may print out \"Vertex dependency in root stage\" twice\n    * [HIVE-16510] - Vectorization: Add vectorized PTF tests in preparation for HIVE-16369\n    * [HIVE-16511] - CBO looses inner casts on constants of complex type\n    * [HIVE-16513] - width_bucket issues\n    * [HIVE-16518] - Insert override for druid does not replace all existing segments\n    * [HIVE-16519] - Fix exception thrown by checkOutputSpecs\n    * [HIVE-16523] - VectorHashKeyWrapper hash code for strings is not so good\n    * [HIVE-16524] - Remove the redundant item type in hiveserver2.jsp and QueryProfileTmpl.jamon\n    * [HIVE-16533] - Vectorization: Avoid evaluating empty groupby keys\n    * [HIVE-16534] - Add capability to tell aborted transactions apart from open transactions in ValidTxnList\n    * [HIVE-16538] - TestExecDriver fails if run after TestOperators#testScriptOperator\n    * [HIVE-16539] - Add PTF tests for blobstores\n    * [HIVE-16545] - LLAP: bug in arena size determination logic\n    * [HIVE-16546] - LLAP: Fail map join tasks if hash table memory exceeds threshold\n    * [HIVE-16547] - LLAP: may not unlock buffers in some cases\n    * [HIVE-16553] - Change default value for hive.tez.bigtable.minsize.semijoin.reduction\n    * [HIVE-16554] - ACID: Make HouseKeeperService threads daemon\n    * [HIVE-16557] - Vectorization: Specialize ReduceSink empty key case\n    * [HIVE-16559] - Parquet schema evolution for partitioned tables may break if table and partition serdes differ\n    * [HIVE-16562] - Issues with nullif / fetch task\n    * [HIVE-16563] - Alter table partition set location should use fully qualified path for non-default FS\n    * [HIVE-16572] - Rename a partition should not drop its column stats\n    * [HIVE-16573] - In-place update for HoS can't be disabled\n    * [HIVE-16576] - Fix encoding of intervals when fetching select query candidates from druid\n    * [HIVE-16577] - Syntax error in the metastore init scripts for mssql\n    * [HIVE-16578] - Semijoin Hints should use column name, if provided for partition key check\n    * [HIVE-16581] -  a bug in HIVE-16523\n    * [HIVE-16584] - Warning messages should use LogHelper.printInfo instead of printing to the infoStream directly\n    * [HIVE-16588] - Resource leak by druid http client\n    * [HIVE-16589] - Vectorization: Support Complex Types and GroupBy modes PARTIAL2, FINAL, and COMPLETE  for AVG, VARIANCE\n    * [HIVE-16590] - Make initializing dag names in SparkWork thread safe for parallel compilation (HIVE-13512)\n    * [HIVE-16592] - Vectorization: Long hashCodes should bit-mix into lower bits\n    * [HIVE-16593] - SparkClientFactory.stop may prevent JVM from exiting\n    * [HIVE-16598] - LlapServiceDriver - create directories and warn of errors\n    * [HIVE-16599] - NPE in runtime filtering cost when handling SMB Joins\n    * [HIVE-16603] - Enforce foreign keys to refer to primary keys or unique keys\n    * [HIVE-16607] - ColumnStatsAutoGatherContext regenerates HiveConf.HIVEQUERYID\n    * [HIVE-16609] - col='__HIVE_DEFAULT_PARTITION__' condition in select statement may produce wrong result\n    * [HIVE-16610] - Semijoin Hint : Should be able to handle more than one hint per alias\n    * [HIVE-16613] - SaslClientHandler.sendHello is eating exceptions\n    * [HIVE-16625] - Extra '\\0' characters in the output, when SeparatedValuesOutputFormat is used and the quoting is disabled\n    * [HIVE-16633] - username for ATS data shall always be the uid who submit the job\n    * [HIVE-16634] - LLAP Use a pool of connections to a single AM from a daemon\n    * [HIVE-16640] - The ASF Headers have some errors in some class\n    * [HIVE-16645] - Commands.java has missed the catch statement and has some code format errors\n    * [HIVE-16646] - Alias in transform ... as clause shouldn't be case sensitive\n    * [HIVE-16654] - Optimize a combination of avg(), sum(), count(distinct) etc\n    * [HIVE-16658] - TestTimestampTZ.java has missed the ASF header\n    * [HIVE-16659] - Query plan should reflect hive.spark.use.groupby.shuffle\n    * [HIVE-16660] - Not able to add partition for views in hive when sentry is enabled\n    * [HIVE-16665] - Race condition in Utilities.GetInputPathsCallable --> createDummyFileForEmptyPartition\n    * [HIVE-16667] - PostgreSQL metastore handling of CLOB types for COLUMNS_V2.TYPE_NAME and other field is incorrect\n    * [HIVE-16671] - LLAP IO: BufferUnderflowException may happen in very rare(?) cases due to ORC end-of-CB estimation\n    * [HIVE-16675] - Fix ConcurrentModificationException in SparkClientImpl#startDriver\n    * [HIVE-16677] - CTAS with no data fails in Druid\n    * [HIVE-16678] - Truncate on temporary table fails with \"table not found\" error.\n    * [HIVE-16679] - Missing ASF header on properties file in ptest2 project\n    * [HIVE-16689] - Correlated scalar subquery with comparison to constant in predicate fails\n    * [HIVE-16692] - LLAP: Keep alive connection in shuffle handler should not be closed until entire data is flushed out\n    * [HIVE-16693] - beeline \"source\" command freezes if you have a comment in it?\n    * [HIVE-16696] - Fix JoinCondDesc explain string\n    * [HIVE-16698] - HoS should avoid mapjoin optimization in case of union and using table stats\n    * [HIVE-16703] - Hive may add the same file to the session and vertex in Tez\n    * [HIVE-16708] - Exception while renewing a Delegation Token\n    * [HIVE-16721] - Inconsistent behavior in dealing with Timestamp stats\n    * [HIVE-16724] - increase session timeout for LLAP ZK token manager\n    * [HIVE-16730] - Vectorization: Schema Evolution for Text Vectorization / Complex Types\n    * [HIVE-16731] - Vectorization: Make \"CASE WHEN (day_name='Sunday') THEN column1 ELSE null end\" that involves a column name or expression THEN or ELSE vectorize\n    * [HIVE-16732] - Transactional tables should block LOAD DATA \n    * [HIVE-16737] - LLAP: Shuffle handler TCP listen queue overflows\n    * [HIVE-16738] - Notification ID generation in DBNotification might not be unique across HS2 instances.\n    * [HIVE-16742] - cap the number of reducers for LLAP at the configured value\n    * [HIVE-16743] - BitSet set() is incorrectly used in TxnUtils.createValidCompactTxnList()\n    * [HIVE-16744] - LLAP index update may be broken after ORC switch\n    * [HIVE-16745] - Syntax error in 041-HIVE-16556.mysql.sql script\n    * [HIVE-16746] - Reduce number of index lookups for same table in IndexWhereTaskDispatcher\n    * [HIVE-16751] - Support different types for grouping columns in GroupBy Druid queries\n    * [HIVE-16755] - LLAP IO: incorrect assert may trigger in tests\n    * [HIVE-16756] - Vectorization: LongColModuloLongColumn throws \"java.lang.ArithmeticException: / by zero\"\n    * [HIVE-16757] - Use of deprecated getRows() instead of new estimateRowCount(RelMetadataQuery..) has serious performance impact\n    * [HIVE-16761] - LLAP IO: SMB joins fail elevator \n    * [HIVE-16769] - Possible hive service startup due to the existing file /tmp/stderr\n    * [HIVE-16776] - Strange cast behavior for table backed by druid\n    * [HIVE-16777] - LLAP: Use separate tokens and UGI instances when an external client is used\n    * [HIVE-16778] - LLAP IO: better refcount management\n    * [HIVE-16780] - Case \"multiple sources, single key\" in spark_dynamic_pruning.q fails \n    * [HIVE-16784] - Missing lineage information when hive.blobstore.optimizations.enabled is true\n    * [HIVE-16788] - ODBC call SQLForeignKeys leads to NPE if you use PK arguments rather than FK arguments\n    * [HIVE-16793] - Scalar sub-query: sq_count_check not required if gby keys are constant\n    * [HIVE-16801] - Vectorization: throwExpandError should be an immediate fatal \n    * [HIVE-16803] - Alter table change column comment should not try to get column stats for update\n    * [HIVE-16804] - Semijoin hint : Needs support for target table.\n    * [HIVE-16808] - WebHCat statusdir parameter doesn't properly handle Unicode characters when using relative path\n    * [HIVE-16820] - TezTask may not shut down correctly before submit\n    * [HIVE-16821] - Vectorization: support Explain Analyze in vectorized mode\n    * [HIVE-16824] - PrimaryToReplicaResourceFunctionTest.java has missed the ASF header\n    * [HIVE-16826] - Improvements for SeparatedValuesOutputFormat\n    * [HIVE-16828] - With CBO enabled, Query on partitioned views throws IndexOutOfBoundException\n    * [HIVE-16832] - duplicate ROW__ID possible in multi insert into transactional table\n    * [HIVE-16835] - Addendum to HIVE-16745\n    * [HIVE-16844] - Fix Connection leak in ObjectStore when new Conf object is used\n    * [HIVE-16845] - INSERT OVERWRITE a table with dynamic partitions on S3 fails with NPE\n    * [HIVE-16846] - TestJdbcWithMiniHS2#testHttpHeaderSize test case is not testing in HTTP mode\n    * [HIVE-16847] - LLAP queue order issue\n    * [HIVE-16851] - Scalar subquery with group by missing sq_count_check UDF\n    * [HIVE-16854] - SparkClientFactory is locked too aggressively\n    * [HIVE-16864] - add validation to stream position search in LLAP IO\n    * [HIVE-16869] - Hive returns wrong result when predicates on non-existing columns are pushed down to Parquet reader\n    * [HIVE-16871] - CachedStore.get_aggr_stats_for has side affect\n    * [HIVE-16875] - Query against view with partitioned child on HoS fails with privilege exception.\n    * [HIVE-16876] - HoS: Make Rpc configs immutable at runtime\n    * [HIVE-16877] - NPE when issue query like alter table ... cascade onto non-partitioned table \n    * [HIVE-16886] - HMS log notifications may have duplicated event IDs if multiple HMS are running concurrently\n    * [HIVE-16888] - Upgrade Calcite to 1.13 and Avatica to 1.10\n    * [HIVE-16898] - Validation of source file after distcp in repl load \n    * [HIVE-16902] - investigate \"failed to remove operation log\" errors\n    * [HIVE-16903] - LLAP: Fix config name issue in SHUFFLE_MANAGE_OS_CACHE\n    * [HIVE-16908] - Failures in TestHcatClient due to HIVE-16844\n    * [HIVE-16910] - RpcConfiguration - Improper Cast From Long To Int\n    * [HIVE-16915] - partition column count is not determined correctly in LLAP IO non-vectorized wrapper\n    * [HIVE-16918] - Skip ReplCopyTask distcp for _metadata copying. Also enable -pb for distcp\n    * [HIVE-16920] - remove useless uri.getScheme() from EximUtil\n    * [HIVE-16922] - Typo in serde.thrift: COLLECTION_DELIM = \"colelction.delim\"\n    * [HIVE-16927] - LLAP: Slider takes down all daemons when some daemons fail repeatedly\n    * [HIVE-16930] - HoS should verify the value of Kerberos principal and keytab file before adding them to spark-submit command parameters\n    * [HIVE-16935] - Hive should strip comments from input before choosing which CommandProcessor to run.\n    * [HIVE-16937] - INFORMATION_SCHEMA usability: everything is currently a string\n    * [HIVE-16938] - INFORMATION_SCHEMA usability: difficult to access # of table records\n    * [HIVE-16939] - metastore error: 'export: -Dproc_metastore : not a valid identifier'\n    * [HIVE-16942] - INFORMATION_SCHEMA: schematool for setting it up is not idempotent\n    * [HIVE-16943] - MoveTask should separate src FileSystem from dest FileSystem \n    * [HIVE-16947] - Semijoin Reduction : Task cycle created due to multiple semijoins in conjunction with hashjoin\n    * [HIVE-16948] - Invalid explain when running dynamic partition pruning query in Hive On Spark\n    * [HIVE-16949] - Leak of threads from Get-Input-Paths and Get-Input-Summary thread pool\n    * [HIVE-16954] - LLAP IO: better debugging\n    * [HIVE-16958] - Setting hive.merge.sparkfiles=true will retrun an error when generating parquet databases \n    * [HIVE-16960] - Hive throws an ugly error exception when HDFS sticky bit is set\n    * [HIVE-16961] - Hive on Spark leaks spark application in case user cancels query and closes session\n    * [HIVE-16964] - _orc_acid_version file is missing\n    * [HIVE-16965] - SMB join may produce incorrect results\n    * [HIVE-16973] - Fetching of Delegation tokens (Kerberos) for AccumuloStorageHandler fails in HS2\n    * [HIVE-16975] - Vectorization: Fully vectorize CAST date as TIMESTAMP so VectorUDFAdaptor is now used\n    * [HIVE-16978] - HoS: add current thread ID to the log redirector for the RemoteDriver\n    * [HIVE-16982] - WebUI \"Show Query\" tab prints \"UNKNOWN\" instead of explaining configuration option\n    * [HIVE-16985] - LLAP IO: enable SMB join in elevator after the former is fixed\n    * [HIVE-16991] - HiveMetaStoreClient needs a 2-arg constructor for backwards compatibility\n    * [HIVE-17002] - decimal (binary) is not working when creating external table for hbase\n    * [HIVE-17006] - LLAP: Parquet caching v1\n    * [HIVE-17007] - NPE introduced by HIVE-16871\n    * [HIVE-17008] - Fix boolean flag switchup in DropTableEvent\n    * [HIVE-17010] - Fix the overflow problem of Long type in SetSparkReducerParallelism\n    * [HIVE-17013] - Delete request with a subquery based on select over a view\n    * [HIVE-17050] - Multiline queries that have comment in middle fail when executed via \"beeline -e\"\n    * [HIVE-17052] - Remove logging of predicate filters\n    * [HIVE-17066] - Query78 filter wrong estimatation is generating bad plan\n    * [HIVE-17067] - LLAP: Add http endpoint to provide system level configurations\n    * [HIVE-17069] - Refactor OrcRawRecrodMerger.ReaderPair\n    * [HIVE-17070] - remove .orig files from src\n    * [HIVE-17073] - Incorrect result with vectorization and SharedWorkOptimizer\n    * [HIVE-17076] - typo in itests/src/test/resources/testconfiguration.properties\n    * [HIVE-17079] - LLAP: Use FQDN by default for work submission\n    * [HIVE-17083] - DagUtils overwrites any credentials already added\n    * [HIVE-17085] - ORC file merge/concatenation should do full schema check\n    * [HIVE-17086] - LLAP: JMX Metric for max file descriptors used so far\n    * [HIVE-17088] - HS2 WebUI throws a NullPointerException when opened\n    * [HIVE-17090] - spark.only.query.files are not being run by ptest\n    * [HIVE-17093] - LLAP ssl configs need to be localized to talk to a wire encrypted hdfs\n    * [HIVE-17095] - Long chain repl loads do not complete in a timely fashion\n    * [HIVE-17097] - Fix SemiJoinHint parsing in SemanticAnalyzer\n    * [HIVE-17098] - Race condition in Hbase tables\n    * [HIVE-17099] - Update golden files for spark.only.query.files\n    * [HIVE-17109] - Remove calls to RelMetadataQuery.instance() after Calcite 1.13 upgrade\n    * [HIVE-17110] - BucketCodec should enforce value ranges\n    * [HIVE-17111] - Add TestLocalSparkCliDriver\n    * [HIVE-17113] - Duplicate bucket files can get written to table by runaway task\n    * [HIVE-17114] - HoS: Possible skew in shuffling when data is not really skewed\n    * [HIVE-17115] - MetaStoreUtils.getDeserializer doesn't catch the java.lang.ClassNotFoundException\n    * [HIVE-17116] - Vectorization: Add infrastructure for vectorization of ROW__ID struct\n    * [HIVE-17117] - Metalisteners are not notified when threadlocal metaconf is cleanup \n    * [HIVE-17128] - Operation Logging leaks file descriptors as the log4j Appender is never closed\n    * [HIVE-17144] - export of temporary tables not working and it seems to be using distcp rather than filesystem copy\n    * [HIVE-17147] - Vectorization: Add code for testing MapJoin operator in isolation and measuring its performance with JMH\n    * [HIVE-17148] - Incorrect result for Hive join query with COALESCE in WHERE condition\n    * [HIVE-17149] - Hdfs directory is not cleared if partition creation failed on HMS\n    * [HIVE-17150] - CREATE INDEX execute HMS out-of-transaction listener calls inside a transaction\n    * [HIVE-17152] - Improve security of random generator for HS2 cookies\n    * [HIVE-17155] - findConfFile() in HiveConf.java has some issues with the conf path\n    * [HIVE-17169] - Avoid extra call to KeyProvider::getMetadata()\n    * [HIVE-17172] - add ordering checks to DiskRangeList\n    * [HIVE-17176] - Add ASF header for LlapAllocatorBuffer.java\n    * [HIVE-17177] - move TestSuite.java to the right position\n    * [HIVE-17181] - HCatOutputFormat should expose complete output-schema (including partition-keys) for dynamic-partitioning MR jobs\n    * [HIVE-17184] - Unexpected new line in beeline output when running with -f option\n    * [HIVE-17188] - ObjectStore runs out of memory for large batches of addPartitions().\n    * [HIVE-17189] - Fix backwards incompatibility in HiveMetaStoreClient\n    * [HIVE-17208] - Repl dump should pass in db/table information to authorization API\n    * [HIVE-17209] - ObjectCacheFactory should return null when tez shared object registry is not setup\n    * [HIVE-17213] - HoS: file merging doesn't work for union all\n    * [HIVE-17217] - SMB Join : Assert if paths are different in TezGroupedSplit in KeyValueInputMerger\n    * [HIVE-17218] - Canonical-ize hostnames for Hive metastore, and HS2 servers.\n    * [HIVE-17220] - Bloomfilter probing in semijoin reduction is thrashing L1 dcache\n    * [HIVE-17222] - Llap: Iotrace throws  java.lang.UnsupportedOperationException with IncompleteCb\n    * [HIVE-17228] - Bump tez version to 0.9.0\n    * [HIVE-17233] - Set \"mapred.input.dir.recursive\" for HCatInputFormat-based jobs.\n    * [HIVE-17235] - Add ORC Decimal64 Serialization/Deserialization (Part 1)\n    * [HIVE-17240] - Function ACOS(2) and ASIN(2) should be null\n    * [HIVE-17254] - Skip updating AccessTime of recycled files in ReplChangeManager\n    * [HIVE-17257] - Hive should merge empty files\n    * [HIVE-17258] - Incorrect log messages in the Hive.java\n    * [HIVE-17259] - Hive JDBC does not recognize UNIONTYPE columns\n    * [HIVE-17260] - Typo: exception has been created and lost in the ThriftJDBCBinarySerDe\n    * [HIVE-17265] - Cache merged column stats from retrieved partitions\n    * [HIVE-17267] - Make HMS Notification Listeners typesafe\n    * [HIVE-17268] - WebUI / QueryPlan: query plan is sometimes null when explain output conf is on\n    * [HIVE-17270] - Qtest results show wrong number of executors\n    * [HIVE-17272] - when hive.vectorized.execution.enabled is true, query on empty partitioned table fails with NPE\n    * [HIVE-17274] - RowContainer spills for timestamp column throws exception\n    * [HIVE-17275] - Auto-merge fails on writes of UNION ALL output to ORC file with dynamic partitioning\n    * [HIVE-17276] - Check max shuffle size when converting to dynamically partitioned hash join\n    * [HIVE-17277] - HiveMetastoreClient Log name is wrong\n    * [HIVE-17280] - Data loss in CONCATENATE ORC created by Spark\n    * [HIVE-17281] - LLAP external client not properly handling KILLED notification that occurs when a fragment is rejected\n    * [HIVE-17283] - Enable parallel edges of semijoin along with mapjoins\n    * [HIVE-17285] - Fixes for bit vector retrievals and merging\n    * [HIVE-17286] - Avoid expensive String serialization/deserialization for bitvectors\n    * [HIVE-17290] - Should use equals() rather than == to compare strings\n    * [HIVE-17298] - export when running distcp for large number of files should not run as privileged user \n    * [HIVE-17301] - Make JSONMessageFactory.getTObj method thread safe\n    * [HIVE-17302] - ReduceRecordSource should not add batch string to Exception message\n    * [HIVE-17303] - Missmatch between roaring bitmap library used by druid and the one coming from tez\n    * [HIVE-17305] - New insert overwrite dynamic partitions qtest need to have the golden file regenerated\n    * [HIVE-17309] - alter partition onto a table not in current database throw InvalidOperationException\n    * [HIVE-17311] - Numeric overflow in the HiveConf\n    * [HIVE-17313] - Potentially possible 'case fall through' in the ObjectInspectorConverters\n    * [HIVE-17314] - LazySimpleSerializeWrite.writeString() contains if with an empty body\n    * [HIVE-17321] - HoS: analyze ORC table doesn't compute raw data size when noscan/partialscan is not specified\n    * [HIVE-17322] - Serialise BeeLine qtest execution to prevent flakyness\n    * [HIVE-17327] - LLAP IO: restrict native file ID usage to default FS to avoid hypothetical collisions when HDFS federation is used\n    * [HIVE-17331] - Path must be used as key type of the pathToAliases\n    * [HIVE-17333] - Schema changes in HIVE-12274 for Oracle may not work for upgrade\n    * [HIVE-17336] - Missing class 'org.apache.hadoop.hive.hbase.HiveHBaseTableInputFormat' from Hive on Spark when inserting into hbase based table\n    * [HIVE-17338] - Utilities.get*Tasks multiple methods duplicate code\n    * [HIVE-17344] - LocalCache element memory usage is not calculated properly.\n    * [HIVE-17348] - Remove unnecessary GenSparkUtils.java.orig file\n    * [HIVE-17351] - use new slider package installation command in run.sh\n    * [HIVE-17352] - HiveSever2 error with \"Illegal Operation state transition from CLOSED to FINISHED\"\n    * [HIVE-17354] - Fix \"alter view\" for incremental replication\n    * [HIVE-17356] - Missing ASF headers 3 classes\n    * [HIVE-17357] - Plugin jars are not properly added for LocalHiveSparkClient\n    * [HIVE-17360] - Tez session reopen appears to use a wrong conf object\n    * [HIVE-17364] - Add unit test to \"alter view\" replication\n    * [HIVE-17365] - Druid CTAS should support CHAR/VARCHAR type\n    * [HIVE-17367] - IMPORT table doesn't load from data dump if a metadata-only dump was already imported.\n    * [HIVE-17368] - DBTokenStore fails to connect in Kerberos enabled remote HMS environment\n    * [HIVE-17372] - update druid dependency to druid 0.10.1\n    * [HIVE-17377] - SharedWorkOptimizer might not iterate through TS operators deterministically\n    * [HIVE-17378] - CBO: HiveReduceExpressionsWithStatsRule can operate on IS_NULL and IS_NOT_NULL\n    * [HIVE-17385] - Fix incremental repl error for non-native tables\n    * [HIVE-17389] - Yetus is always failing on rat checks\n    * [HIVE-17391] - Compaction fails if there is an empty value in tblproperties\n    * [HIVE-17392] - SharedWorkOptimizer might merge TS operators filtered by not equivalent semijoin operators\n    * [HIVE-17393] - AMReporter need hearbeat every external 'AM'\n    * [HIVE-17394] - AvroSerde is regenerating TypeInfo objects for each nullable Avro field for every row\n    * [HIVE-17401] - Hive session idle timeout doesn't function properly\n    * [HIVE-17403] - Fail concatenation for unmanaged and transactional tables\n    * [HIVE-17410] - repl load task during subsequent DAG generation does not start from the last partition processed\n    * [HIVE-17411] - LLAP IO may incorrectly release a refcount in some rare cases\n    * [HIVE-17412] - Add \"-- SORT_QUERY_RESULTS\" for spark_vectorized_dynamic_partition_pruning.q\n    * [HIVE-17413] - predicate involving CAST affects value returned by the SELECT statement\n    * [HIVE-17415] - Hit error \"SemanticException View xxx is corresponding to LIMIT, rather than a SelectOperator.\" in Hive queries\n    * [HIVE-17417] - LazySimple Timestamp is very expensive\n    * [HIVE-17419] - ANALYZE TABLE...COMPUTE STATISTICS FOR COLUMNS command shows computed stats for masked tables\n    * [HIVE-17420] - bootstrap - get replid before object dump\n    * [HIVE-17421] - Clear incorrect stats after replication\n    * [HIVE-17429] - Hive JDBC doesn't return rows when querying Impala\n    * [HIVE-17450] - rename TestTxnCommandsBase \n    * [HIVE-17452] - HPL/SQL function variable block is not initialized\n    * [HIVE-17453] - Missing ASF headers 2 classes\n    * [HIVE-17457] - IOW Acid Insert Overwrite when the transaction fails\n    * [HIVE-17459] - View deletion operation failed to replicate on target cluster\n    * [HIVE-17460] - `insert overwrite` should support table schema evolution (e.g. add columns)\n    * [HIVE-17463] - ORC: include orc-shims in hive-exec.jar\n    * [HIVE-17464] - Fix to be able to disable max shuffle size DHJ config\n    * [HIVE-17465] - Statistics: Drill-down filters don't reduce row-counts progressively\n    * [HIVE-17468] - Shade and package appropriate jackson version for druid storage handler\n    * [HIVE-17471] - Vectorization: Enable hive.vectorized.row.identifier.enabled to true by default\n    * [HIVE-17472] - Drop-partition for multi-level partition fails, if data does not exist.\n    * [HIVE-17475] - Disable mapjoin using hint\n    * [HIVE-17479] - Staging directories do not get cleaned up for update/delete queries\n    * [HIVE-17483] - HS2 kill command to kill queries using query id\n    * [HIVE-17485] - Hive-Druid table on indexing for few segments- DruidRecordWriter.pushSegments throws ArrayIndexOutOfBoundsException\n    * [HIVE-17489] - Separate client-facing and server-side Kerberos principals, to support HA\n    * [HIVE-17496] - Bootstrap repl is not cleaning up staging dirs\n    * [HIVE-17504] - Skip ACID table for replication\n    * [HIVE-17510] - Make comparison of filter predicates in q files deterministic\n    * [HIVE-17512] - Not use doAs if distcp privileged user same as user running hive\n    * [HIVE-17522] - cleanup old 'repl dump' dirs\n    * [HIVE-17523] - Insert into druid table  hangs Hive server2 in an infinite loop\n    * [HIVE-17529] - Bucket Map Join : Sets incorrect edge type causing execution failure\n    * [HIVE-17530] - ClassCastException when converting uniontype\n    * [HIVE-17535] - Select 1 EXCEPT Select 1 fails with NPE\n    * [HIVE-17553] - CBO wrongly type cast decimal literal to int\n    * [HIVE-17554] - Occurr java.lang.ArithmeticException: / by zero at hplsql component\n    * [HIVE-17556] - The test udf_mask_hash.q is failing\n    * [HIVE-17558] - Skip non-native/temporary tables for constraint related scenarios\n    * [HIVE-17560] - HiveMetastore doesn't start in secure cluster if repl change manager is enabled\n    * [HIVE-17563] - CodahaleMetrics.JsonFileReporter is not updating hive.service.metrics.file.location\n    * [HIVE-17568] - HiveJoinPushTransitivePredicatesRule may exchange predicates which are not valid on the other branch\n    * [HIVE-17571] - update sql standard authorization config whitelist to include distcp options for replication\n    * [HIVE-17576] - Improve progress-reporting in TezProcessor\n    * [HIVE-17582] - Followup of HIVE-15708\n    * [HIVE-17584] - fix mapred.job.queue.name in sql standard authorization config whitelist\n    * [HIVE-17585] - Improve thread safety when loading dynamic partitions in parallel\n    * [HIVE-17588] - LlapRowRecordReader doing name-based field lookup for every column of every row\n    * [HIVE-17594] - Unit format error in Copy.java\n    * [HIVE-17595] - Correct DAG for updating the last.repl.id for a database during bootstrap load\n    * [HIVE-17601] - improve error handling in LlapServiceDriver\n    * [HIVE-17602] - Explain plan not working\n    * [HIVE-17610] - LLAP IO: an exception in exception handling can hide the original exception\n    * [HIVE-17613] - remove object pools for short, same-thread allocations\n    * [HIVE-17615] - Task.executeTask has to be thread safe for parallel execution\n    * [HIVE-17619] - Exclude avatica-core.jar dependency from avatica shaded jar\n    * [HIVE-17620] - Use the default MR scratch directory (HDFS) in the only case when hive.blobstore.optimizations.enabled=true AND isFinalJob=true\n    * [HIVE-17621] - Hive-site settings are ignored during HCatInputFormat split-calculation\n    * [HIVE-17623] - Fix Select query Fix Double column serde and some refactoring\n    * [HIVE-17624] - MapredLocakTask running in separate JVM could throw ClassNotFoundException \n    * [HIVE-17625] - Replication: update hive.repl.partitions.dump.parallelism to 100\n    * [HIVE-17627] - Use druid scan query instead of the select query.\n    * [HIVE-17628] - always use fully qualified path for tables/partitions/etc.\n    * [HIVE-17633] - Make it possible to override the query results directory in TestBeeLineDriver\n    * [HIVE-17635] - Add unit tests to CompactionTxnHandler and use PreparedStatements for queries\n    * [HIVE-17639] - don't reuse planner context when re-parsing the query\n    * [HIVE-17643] - recent WM changes broke reopen due to spurious overloads\n    * [HIVE-17644] - directSQL errors out on key constraints until the DB is initialized\n    * [HIVE-17649] - Export/Import: Move export data write to a task\n    * [HIVE-17653] - Druid storage handler CTAS with boolean type columns fails. \n    * [HIVE-17659] - get_token thrift call fails for DBTokenStore in remote HMS mode\n    * [HIVE-17664] - Refactor and add new tests\n    * [HIVE-17665] - Update netty-all to latest 4.0.x.Final\n    * [HIVE-17679] - http-generic-click-jacking for WebHcat server\n    * [HIVE-17682] - Vectorization: IF stmt produces wrong results\n    * [HIVE-17690] - Add distcp.options.p* in sql standard authorization config whitelist\n    * [HIVE-17701] - Added restriction to historic queries on web UI\n    * [HIVE-17702] - incorrect isRepeating handling in decimal reader in ORC\n    * [HIVE-17706] - Add a possibility to run the BeeLine tests on the default database\n    * [HIVE-17715] - Exception when pushing postaggregates into Druid\n    * [HIVE-17720] - Bitvectors are not shown in describe statement on beeline\n    * [HIVE-17721] - with Postgres rdbms for metastore and dbnotification enabled, hive DDL SQL query fails \n    * [HIVE-17723] - Update Accumulo drive q.out files\n    * [HIVE-17725] - Fix misnamed tests which are not run during precommit runs. \n    * [HIVE-17726] - Using exists may lead to incorrect results\n    * [HIVE-17731] - add a backward compat option for external users to HIVE-11985\n    * [HIVE-17735] - ObjectStore.addNotificationEvent is leaking queries\n    * [HIVE-17746] - Regenerate spark_explainuser_1.q.out\n    * [HIVE-17749] - Multiple class have missed the ASF header\n    * [HIVE-17758] - NOTIFICATION_SEQUENCE_LOCK_RETRY_SLEEP_INTERVAL.defaultLongVal is -1\n    * [HIVE-17761] - Deprecate hive.druid.select.distribute property for Druid\n    * [HIVE-17762] - Exclude older jackson-annotation.jar from druid-handler shaded jar\n    * [HIVE-17764] - alter view fails when hive.metastore.disallow.incompatible.col.type.changes set to true\n    * [HIVE-17765] - expose Hive keywords \n    * [HIVE-17777] - Add maven coordinates in itests/pom.xml\n    * [HIVE-17781] - Map MR settings to Tez settings via DeprecatedKeys\n    * [HIVE-17782] - Inconsistent cast behavior from string to numeric types with regards to leading/trailing spaces\n    * [HIVE-17785] - Encription tests are not running\n    * [HIVE-17792] - Enable Bucket Map Join when there are extra keys other than bucketed columns\n    * [HIVE-17795] - Add distribution management tag in pom\n    * [HIVE-17798] - When replacing the src table names in BeeLine testing, the table names shouldn't be changed to lower case\n    * [HIVE-17803] - With Pig multi-query, 2 HCatStorers writing to the same table will trample each other's outputs\n    * [HIVE-17804] - Vectorization: Bug erroneously causes match for 1st row in batch (SelectStringColLikeStringScalar)\n    * [HIVE-17806] - Create directory for metrics file if it doesn't exist\n    * [HIVE-17807] - Execute maven commands in batch mode for ptests\n    * [HIVE-17813] - hive.exec.move.files.from.source.dir does not work with partitioned tables\n    * [HIVE-17815] - prevent OOM with Atlas Hive hook \n    * [HIVE-17817] - Stabilize crossproduct warning message output order\n    * [HIVE-17822] - Provide an option to skip shading of jars\n    * [HIVE-17825] - Socket not closed when trying to read files to copy over in replication from metadata\n    * [HIVE-17826] - Error writing to RandomAccessFile after operation log is closed\n    * [HIVE-17828] - Metastore: mysql upgrade scripts to 3.0.0 is broken\n    * [HIVE-17829] - ArrayIndexOutOfBoundsException - HBASE-backed tables with Avro schema in Hive2\n    * [HIVE-17830] - dbnotification fails to work with rdbms other than postgres\n    * [HIVE-17831] - HiveSemanticAnalyzerHookContext does not update the HiveOperation after sem.analyze() is called\n    * [HIVE-17832] - Allow hive.metastore.disallow.incompatible.col.type.changes to be changed in metastore\n    * [HIVE-17833] - Publish split generation counters\n    * [HIVE-17834] - Fix flaky triggers test\n    * [HIVE-17836] - Persisting nulls in bit vector field fails for postgres backed metastore\n    * [HIVE-17839] - Cannot generate thrift definitions in standalone-metastore\n    * [HIVE-17843] - UINT32 Parquet columns are handled as signed INT32-s, silently reading incorrect data\n    * [HIVE-17845] - insert fails if target table columns are not lowercase\n    * [HIVE-17853] - RetryingMetaStoreClient loses UGI impersonation-context when reconnecting after timeout\n    * [HIVE-17864] - PTestClient cannot start during Precommit tests\n    * [HIVE-17867] - Exception in windowing functions with TIMESTAMP WITH LOCAL TIME ZONE type\n    * [HIVE-17868] - Make queries in spark_local_queries.q have deterministic output\n    * [HIVE-17872] - Ignoring schema autostart doesn't work (HIVE-14152 used the wrong setting)\n    * [HIVE-17873] - External LLAP client: allow same handleID to be used more than once\n    * [HIVE-17882] - Resource plan retrieval looks incorrect\n    * [HIVE-17891] - HIVE-13076 uses create table if not exists for the postgres script\n    * [HIVE-17900] - analyze stats on columns triggered by Compactor generates malformed SQL with > 1 partition column\n    * [HIVE-17908] - LLAP External client not correctly handling killTask for pending requests\n    * [HIVE-17918] - NPE during semijoin reduction optimization when LLAP caching disabled\n    * [HIVE-17936] - Dynamic Semijoin Reduction : markSemiJoinForDPP marks unwanted semijoin branches\n    * [HIVE-17937] - llap_acid_fast test is flaky\n    * [HIVE-17939] - Bucket map join not being selected when bucketed tables is missing bucket files\n    * [HIVE-17942] - HiveAlterHandler not using conf from HMS Handler\n    * [HIVE-17952] - Fix license headers to avoid dangling javadoc warnings\n    * [HIVE-17953] - Metrics should move to destination atomically\n    * [HIVE-17963] - Fix for HIVE-17113 can be improved for non-blobstore filesystems\n    * [HIVE-17966] - org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveArrayInspector - Review\n    * [HIVE-17973] - Fix small bug in multi_insert_union_src.q\n    * [HIVE-17976] - HoS: don't set output collector if there's no data to process\n    * [HIVE-17978] - Shared work optimizer may leave useless operator branches in the plan\n    * [HIVE-17994] - Vectorization: Serialization bottlenecked on irrelevant hashmap lookup\n    * [HIVE-18001] - InvalidObjectException while creating Primary Key constraint on partition key column\n    * [HIVE-18006] - Optimize memory footprint of HLLDenseRegister\n    * [HIVE-18007] - Address maven warnings\n    * [HIVE-18012] - fix ct_noperm_loc test\n    * [HIVE-18016] - org.apache.hadoop.hive.ql.util.ResourceDownloader - Review\n    * [HIVE-18017] - HS2 materialized view registry init loading all tables from metastore\n    * [HIVE-18026] - Hive webhcat principal configuration optimization\n    * [HIVE-18046] - Metastore: default IS_REWRITE_ENABLED=false instead of NULL\n    * [HIVE-18050] - LlapServiceDriver shoud split HIVE_AUX_JARS_PATH by ':' instead of ','\n    * [HIVE-18054] -  Make Lineage work with concurrent queries on a Session\n    * [HIVE-18060] - UpdateInputAccessTimeHook fails for non-current database\n    * [HIVE-18067] - Remove extraneous golden files\n    * [HIVE-18068] - Upgrade to Calcite 1.15\n    * [HIVE-18069] - MetaStoreDirectSql to get tables has misplaced comma\n    * [HIVE-18077] - Vectorization: Add string conversion case for UDFToDouble\n    * [HIVE-18090] - acid heartbeat fails when metastore is connected via hadoop credential\n    * [HIVE-18109] - fix identifier usage in parser\n    * [HIVE-18111] - Fix temp path for Spark DPP sink\n    * [HIVE-18124] -  clean up isAcidTable() API vs isInsertOnlyTable()\n    * [HIVE-18127] - Do not strip '--' comments from shell commands issued from CliDriver\n    * [HIVE-18136] - WorkloadManagerMxBean is missing the Apache license header\n    * [HIVE-18146] - Vectorization: VectorMapJoinOperator Decimal64ColumnVector key/value cast bug\n    * [HIVE-18147] - Tests can fail with java.net.BindException: Address already in use\n    * [HIVE-18148] - NPE in SparkDynamicPartitionPruningResolver\n    * [HIVE-18150] - Upgrade Spark Version to 2.2.0\n    * [HIVE-18151] - LLAP external client: Better error message propagation during submission failures\n    * [HIVE-18157] - Vectorization : Insert in bucketed table is broken with vectorization\n    * [HIVE-18160] - Jar localization during session initialization is slow\n    * [HIVE-18166] - Result of hive.query.string is encoded.\n    * [HIVE-18188] - Fix TestSSL failures in master\n    * [HIVE-18189] - Order by position does not work when cbo is disabled\n    * [HIVE-18191] - Vectorization: Add validation of TableScanOperator (gather statistics) back\n    * [HIVE-18194] - Migrate existing ACID tables to use write id per table rather than global transaction id\n    * [HIVE-18195] - Hive schema broken on postgres\n    * [HIVE-18196] - Druid Mini Cluster to run Qtests integrations tests.\n    * [HIVE-18198] - TablePropertyEnrichmentOptimizer.java is missing the Apache license header\n    * [HIVE-18207] - Fix the test failure for TestCliDriver#vector_complex_join\n    * [HIVE-18208] - SMB Join : Fix the unit tests to run SMB Joins.\n    * [HIVE-18210] - create resource plan allows duplicates\n    * [HIVE-18213] - Tests: YARN Minicluster times out if the disks are >90% full\n    * [HIVE-18220] - Workload Management tables have broken constraints defined on postgres schema\n    * [HIVE-18228] - Azure credential properties should be added to the HiveConf hidden list\n    * [HIVE-18232] - Packaging: add dfs-init script in package target\n    * [HIVE-18240] - support getClientInfo/setClientInfo in JDBC\n    * [HIVE-18241] - Query with LEFT SEMI JOIN producing wrong result\n    * [HIVE-18248] - Clean up parameters\n    * [HIVE-18250] - CBO gets turned off with duplicates in RR error\n    * [HIVE-18254] - Use proper AVG Calcite primitive instead of Other_FUNCTION\n    * [HIVE-18255] - spark-client jar should be prefixed with hive-\n    * [HIVE-18258] - Vectorization: Reduce-Side GROUP BY MERGEPARTIAL with duplicate columns is broken\n    * [HIVE-18263] - Ptest execution are multiple times slower sometimes due to dying executor slaves\n    * [HIVE-18266] - LLAP: /system references wrong file for THP\n    * [HIVE-18269] - LLAP: Fast llap io with slow processing pipeline can lead to OOM\n    * [HIVE-18271] - Druid Insert into fails with exception when committing files\n    * [HIVE-18290] - hbase backed table creation fails where no column comments present\n    * [HIVE-18293] - Hive is failing to compact tables contained within a folder that is not owned by identity running HiveMetaStore\n    * [HIVE-18298] - Fix TestReplicationScenarios.testConstraints\n    * [HIVE-18299] - DbNotificationListener fail on mysql with \"select for update\"\n    * [HIVE-18306] - Fix spark smb tests\n    * [HIVE-18309] - qtests: smb_mapjoin_19.q breaks bucketsortoptimize_insert_2.q\n    * [HIVE-18310] - Test 'vector_reduce_groupby_duplicate_cols.q' is misspelled in testconfiguration.properties\n    * [HIVE-18311] - Enable smb_mapjoin_8.q for cli driver\n    * [HIVE-18314] - qtests: semijoin_hint.q breaks hybridgrace_hashjoin_2.q\t\n    * [HIVE-18316] - HiveEndPoint should only work with full acid tables\n    * [HIVE-18318] - LLAP record reader should check interrupt even when not blocking\n    * [HIVE-18321] - Support REBUILD for MVs backed by custom storage handlers\n    * [HIVE-18326] - LLAP Tez scheduler - only preempt tasks if there's a dependency between them\n    * [HIVE-18330] - Fix TestMsgBusConnection - doesn't test tests the original intention\n    * [HIVE-18331] - Renew the Kerberos ticket used by Druid Query runner\n    * [HIVE-18335] - Vectorization : Check bounds of array before the allocation in VectorMapJoinFastBytesHashTable\n    * [HIVE-18341] - Add repl load support for adding \"raw\" namespace for TDE with same encryption keys\n    * [HIVE-18352] - introduce a METADATAONLY option while doing REPL DUMP to allow integrations of other tools \n    * [HIVE-18353] - CompactorMR should call jobclient.close() to trigger cleanup\n    * [HIVE-18355] - Add builder for metastore Thrift classes missed in the first pass - FunctionBuilder\n    * [HIVE-18356] - Fixing license headers in checkstyle\n    * [HIVE-18359] - Extend grouping set limits from int to long\n    * [HIVE-18360] - NPE in TezSessionState\n    * [HIVE-18365] - netty-all jar is not present in the llap tarball\n    * [HIVE-18367] - Describe Extended output is truncated on a table with an explicit row format containing tabs or newlines.\n    * [HIVE-18370] - standalone-metastore gen dir contains two annotation/package-info.java which causes IDEA build fail\n    * [HIVE-18379] - ALTER TABLE authorization_part SET PROPERTIES (\"PARTITIONL_LEVEL_PRIVILEGE\"=\"TRUE\"); fails when authorization_part is MicroManaged table.\n    * [HIVE-18380] - ALTER TABLE CONCATENATE is not supported on Micro-managed table\n    * [HIVE-18383] - Qtests: running all cases from TestNegativeCliDriver results in OOMs\n    * [HIVE-18384] - ConcurrentModificationException in log4j2.x library\n    * [HIVE-18385] - mergejoin fails with java.lang.IllegalStateException\n    * [HIVE-18390] - IndexOutOfBoundsException when query a  partitioned view in ColumnPruner \n    * [HIVE-18393] - Error returned when some other type is read as string from parquet tables\n    * [HIVE-18413] - Grouping of an empty result set may only contain null values\n    * [HIVE-18414] - upgrade to tez-0.9.1\n    * [HIVE-18416] - Initial support for TABLE function\n    * [HIVE-18417] - better error handling in TezSessionState cleanup\n    * [HIVE-18419] - CliDriver loads different hive-site.xml into HiveConf and MetastoreConf\n    * [HIVE-18420] - LLAP IO: InputStream may return 0 bytes\n    * [HIVE-18421] - Vectorized execution handles overflows in a different manner than non-vectorized execution\n    * [HIVE-18422] - Vectorized input format should not be used when vectorized input format is excluded and row.serde is enabled\n    * [HIVE-18426] - Memory leak in RoutingAppender for every hive operation\n    * [HIVE-18429] - Compaction should handle a case when it produces no output\n    * [HIVE-18430] - Add new determinism category for runtime constants (current_date, current_timestamp)\n    * [HIVE-18442] - HoS: No FileSystem for scheme: nullscan\n    * [HIVE-18445] - qtests: auto_join25.q fails permanently\n    * [HIVE-18447] - JDBC: Provide a way for JDBC users to pass cookie info via connection string\n    * [HIVE-18450] - Support TABLE function in CBO\n    * [HIVE-18452] - work around HADOOP-15171\n    * [HIVE-18456] - Add some tests for HIVE-18367 to check that the table information contains the query correctly\n    * [HIVE-18459] - hive-exec.jar leaks contents fb303.jar into classpath\n    * [HIVE-18465] - Hive metastore schema initialization failing on postgres\n    * [HIVE-18467] - support whole warehouse dump / load + create/drop database events\n    * [HIVE-18472] - Beeline gives log4j warnings\n    * [HIVE-18473] - Infer timezone information correctly in DruidSerde\n    * [HIVE-18482] - Copy-paste error in the RelOptHiveTable\n    * [HIVE-18488] - LLAP ORC readers are missing some null checks\n    * [HIVE-18490] - Query with EXISTS and NOT EXISTS with non-equi predicate can produce wrong result\n    * [HIVE-18492] - Wrong argument in the WorkloadManager.resetAndQueryKill()\n    * [HIVE-18494] - Regression: from HIVE-18069, the metastore directsql is getting disabled\n    * [HIVE-18499] - Amend point lookup tests to check for data\n    * [HIVE-18500] - annoying exceptions from LLAP Jmx view in the logs\n    * [HIVE-18501] - Typo in beeline code\n    * [HIVE-18504] - Hive is throwing InvalidObjectException(message:Invalid column type name is too long.\n    * [HIVE-18506] - LlapBaseInputFormat - negative array index\n    * [HIVE-18507] - AccumuloIndexedOutputFormat.AccumuloRecordWriter.close() - typo in the condition\n    * [HIVE-18513] - Query results caching\n    * [HIVE-18514] - add service output for ranger to WM DDL operations\n    * [HIVE-18517] - Vectorization: Fix VectorMapOperator to accept VRBs and check vectorized flag correctly to support LLAP Caching\n    * [HIVE-18518] - Upgrade druid version to 0.11.0\n    * [HIVE-18519] - do not create materialized CTEs with ACID/MM\n    * [HIVE-18521] - Vectorization: query failing in reducer VectorUDAFAvgDecimalPartial2 java.lang.ClassCastException StructTypeInfo --> DecimalTypeInfo\n    * [HIVE-18523] - Fix summary row in case there are no inputs\n    * [HIVE-18524] - Vectorization: Execution failure related to non-standard embedding of IfExprConditionalFilter inside VectorUDFAdaptor (Revert HIVE-17139)\n    * [HIVE-18529] - Vectorization: Add a debug config option to disable scratch column reuse\n    * [HIVE-18530] - Replication should skip MM table (for now)\n    * [HIVE-18531] - Vectorization: Vectorized PTF operator should not set the initial type infos\n    * [HIVE-18546] - Remove unnecessary code introduced in HIVE-14498\n    * [HIVE-18547] - WM: trigger test may fail\n    * [HIVE-18548] - Fix log4j import\n    * [HIVE-18551] - Vectorization: VectorMapOperator tries to write too many vector columns for Hybrid Grace\n    * [HIVE-18554] - Fix false positive test ql.io.parquet.TestHiveSchemaConverter.testMap \n    * [HIVE-18557] - q.outs: fix issues caused by q.out_spark files\n    * [HIVE-18558] - Upgrade orc version to 1.4.2\n    * [HIVE-18562] - Vectorization: CHAR/VARCHAR conversion in VectorDeserializeRow is broken\n    * [HIVE-18567] - ObjectStore.getPartitionNamesNoTxn doesn't handle max param properly\n    * [HIVE-18569] - Hive Druid indexing not dealing with decimals in correct way.\n    * [HIVE-18571] - stats issues for MM tables; ACID doesn't check state for CTAS\n    * [HIVE-18573] - Use proper Calcite operator instead of UDFs\n    * [HIVE-18574] - LLAP: Ship netty3 as part of LLAP install tarball\n    * [HIVE-18575] - ACID properties usage in jobconf is ambiguous for MM tables\n    * [HIVE-18577] - SemanticAnalyzer.validate has some pointless metastore calls\n    * [HIVE-18578] - Some class has missed the ASF header\n    * [HIVE-18579] - Changes from HIVE-18495 introduced import paths from shaded jars\n    * [HIVE-18585] - Return type for udfs should be determined using Hive inference rules instead of Calcite\n    * [HIVE-18587] - insert DML event may attempt to calculate a checksum on directories\n    * [HIVE-18589] - java.io.IOException: Not enough history available\n    * [HIVE-18590] - Assertion error on transitive join inference in the presence of NOT NULL constraint\n    * [HIVE-18595] - UNIX_TIMESTAMP  UDF fails when type is Timestamp with local timezone\n    * [HIVE-18597] - LLAP: Always package the log4j2 API jar for org.apache.log4j\n    * [HIVE-18599] - Transactions: Fix CTAS on Micromanaged tables\n    * [HIVE-18600] - Vectorization: Top-Level Vector Expression Scratch Column Deallocation\n    * [HIVE-18601] - Support Power platform by updating protoc-jar-maven-plugin version\n    * [HIVE-18606] - CTAS on empty table throws NPE from org.apache.hadoop.hive.ql.exec.MoveTask\n    * [HIVE-18607] - HBase HFile write does strange things\n    * [HIVE-18610] - Performance: ListKeyWrapper does not check for hashcode equals, before comparing members\n    * [HIVE-18611] - Avoid memory allocation of aggregation buffer during stats computation \n    * [HIVE-18612] - Build subprocesses under Yetus in Ptest use 1.7 jre instead of 1.8\n    * [HIVE-18613] - Extend JsonSerDe to support BINARY type\n    * [HIVE-18614] - Fix sys db creation in Hive\n    * [HIVE-18616] - work around HADOOP-15171 p2\n    * [HIVE-18617] - Workload management Action parser does not generate the correct pool path.\n    * [HIVE-18622] - Vectorization: IF Statements, Comparisons, and more do not handle NULLs correctly\n    * [HIVE-18626] - Repl load \"with\" clause does not pass config to tasks\n    * [HIVE-18627] - PPD: Handle FLOAT boxing differently for single/double precision constants\n    * [HIVE-18628] - Make tez dag status check interval configurable\n    * [HIVE-18631] - Hive metastore schema initialization failing on mysql\n    * [HIVE-18637] - WorkloadManagent Event Summary leaving subscribedCounters and currentCounters fields empty\n    * [HIVE-18638] - Triggers for multi-pool move, failing to initiate the move event\n    * [HIVE-18641] - Remove MCreationMetadata from MTable class\n    * [HIVE-18642] - incorrect assertion in TezSessionPool for WM\n    * [HIVE-18643] - don't check for archived partitions for ACID ops\n    * [HIVE-18645] - invalid url address in README.txt from module hbase-handler\n    * [HIVE-18646] - Update errata.txt for HIVE-18617\n    * [HIVE-18647] - Cannot create table: \"message:Exception thrown when executing query : SELECT DISTINCT..\"\n    * [HIVE-18653] - Fix TestOperators test failure in master\n    * [HIVE-18658] - WM: allow not specifying scheduling policy when creating a pool\n    * [HIVE-18659] - add acid version marker to acid files/directories\n    * [HIVE-18660] - PCR doesn't distinguish between partition and virtual columns\n    * [HIVE-18662] - hive.acid.key.index is missing entries\n    * [HIVE-18665] - LLAP: Ignore cache-affinity if the LLAP IO elevator is disabled\n    * [HIVE-18666] - Materialized view: \"create materialized enable rewrite\" should fail if rewriting is not possible\n    * [HIVE-18667] - Materialized views: rewrites should be triggered without checks if the time.window=-1\n    * [HIVE-18671] - lock not released after Hive on Spark query was cancelled\n    * [HIVE-18674] - update Hive to use ORC 1.4.3\n    * [HIVE-18675] - make HIVE_LOCKS.HL_TXNID NOT NULL\n    * [HIVE-18678] - fix exim for MM tables and reinstante the test\n    * [HIVE-18680] - FieldTrimmer missing opportunity with SortLimit operators\n    * [HIVE-18686] - Installation on Postgres and Oracle broken\n    * [HIVE-18688] - Vectorization: Vectorizer Reason shouldn't be part of work-plan\n    * [HIVE-18693] - Snapshot Isolation does not work for Micromanaged table when a insert transaction is aborted\n    * [HIVE-18695] - fix TestAccumuloCliDriver.testCliDriver[accumulo_queries]\n    * [HIVE-18697] - The HiveMetastore.exchange_partitions method throws FileNotFoundException if the given partition doesn't exist in the source table\n    * [HIVE-18698] - Fix TestMiniLlapLocalCliDriver#testCliDriver[bucket_map_join_tez1]\n    * [HIVE-18699] - Check for duplicate partitions in HiveMetastore.exchange_partitions\n    * [HIVE-18708] - Vectorization: Delay out-of-tree fixups till whole work is vectorized\n    * [HIVE-18713] - Optimize: Transform IN clauses to = when there's only one element\n    * [HIVE-18717] - Avoid transitive dependency on jetty 6.x\n    * [HIVE-18733] - Missing break in CommonFastHashTable\n    * [HIVE-18737] - add an option to disable LLAP IO ACID for non-original files\n    * [HIVE-18738] - LLAP IO ACID - includes handling is broken\n    * [HIVE-18742] - Vectorization acid/inputformat check should allow NullRowsInputFormat/OneNullRowInputFormat\n    * [HIVE-18757] - LLAP IO for text fails for empty files\n    * [HIVE-18759] - Remove unconnected q.out-s\n    * [HIVE-18764] - ELAPSED_TIME resource plan setting is not getting honored\n    * [HIVE-18775] - HIVE-17983 missed deleting metastore/scripts/upgrade/derby/hive-schema-3.0.0.derby.sql\n    * [HIVE-18776] - MaterializationsInvalidationCache loading causes race condition in the metastore\n    * [HIVE-18777] - Add Authorization interface to support information_schema integration with external authorization\n    * [HIVE-18783] - ALTER TABLE post-commit listener does not include the transactional listener responses \n    * [HIVE-18788] - Clean up inputs in JDBC PreparedStatement\n    * [HIVE-18789] - Disallow embedded element in UDFXPathUtil\n    * [HIVE-18791] - Fix TestJdbcWithMiniHS2#testHttpHeaderSize\n    * [HIVE-18794] - Repl load \"with\" clause does not pass config to tasks for non-partition tables\n    * [HIVE-18796] - fix TestSSL\n    * [HIVE-18813] - Fix qtest mapjoin_hook.q\n    * [HIVE-18815] - Remove unused feature in HPL/SQL\n    * [HIVE-18816] - CREATE TABLE (ACID) doesn't work with TIMESTAMPLOCALTZ column type\n    * [HIVE-18817] - ArrayIndexOutOfBounds exception during read of ACID table.\n    * [HIVE-18818] - Alter table add constraint unique fails with direct sql set to false\n    * [HIVE-18820] - Operation doesn't always clean up log4j for operation log\n    * [HIVE-18826] - fix TestEncryptedHDFSCliDriver.testCliDriver[encryption_move_tbl]\n    * [HIVE-18828] - improve error handling for codecs in LLAP IO\n    * [HIVE-18833] - Auto Merge fails when \"insert into directory as orcfile\"\n    * [HIVE-18837] - add a flag and disable some object pools in LLAP until further testing\n    * [HIVE-18858] - System properties in job configuration not resolved when submitting MR job\n    * [HIVE-18859] - Incorrect handling of thrift metastore exceptions\n    * [HIVE-18863] - trunc() calls itself trunk() in an error message\n    * [HIVE-18877] - HiveSchemaTool.validateSchemaTables() should wrap a SQLException when rethrowing\n    * [HIVE-18879] - Disallow embedded element in UDFXPathUtil needs to work if xercesImpl.jar in classpath\n    * [HIVE-18886] - ACID: NPE on unexplained mysql exceptions \n    * [HIVE-18888] - Replace synchronizedMap with ConcurrentHashMap\n    * [HIVE-18889] - update all parts of Hive to use the same Guava version\n    * [HIVE-18892] - Fix NPEs in HiveMetastore.exchange_partitions method\n    * [HIVE-18898] - Fix NPEs in HiveMetastore.dropPartition method\n    * [HIVE-18907] - Create utility to fix acid key index issue from HIVE-18817\n    * [HIVE-18918] - Bad error message in CompactorMR.lanuchCompactionJob()\n    * [HIVE-18919] - remove separate keytab setting for ZK in LLAP\n    * [HIVE-18925] - Hive doesn't work when JVM is America/Bahia_Banderas time zone\n    * [HIVE-18933] - disable ORC codec pool for now; remove clone\n    * [HIVE-18944] - Groupping sets position is set incorrectly during DPP\n    * [HIVE-18950] - DESCRIBE EXTENDED missing details of default constraint\n    * [HIVE-18951] - Fix the llapdump usage error in llapdump.sh\n    * [HIVE-18955] - HoS: Unable to create Channel from class NioServerSocketChannel\n    * [HIVE-18962] - add WM task state to Tez AM heartbeat\n    * [HIVE-18963] - JDBC: Provide an option to simplify beeline usage by supporting default and named URL for beeline\n    * [HIVE-18965] - HIVE-17990 didn't update derby SQL scripts\n    * [HIVE-18967] - Standalone metastore SQL upgrade scripts do not properly set schema version\n    * [HIVE-18968] - LLAP: report guaranteed tasks count in AM registry to check for consistency\n    * [HIVE-18970] - improve AM WM metrics for use in Grafana and such\n    * [HIVE-18971] - add HS2 WM metrics for use in Grafana and such\n    * [HIVE-18972] - beeline command suggestion to kill job deprecated\n    * [HIVE-18975] - NPE when inserting NULL value in structure and array with HBase table\n    * [HIVE-18976] - Add ability to setup Druid Kafka Ingestion from Hive\n    * [HIVE-18990] - Hive doesn't close Tez session properly\n    * [HIVE-18991] - Drop database cascade doesn't work with materialized views\n    * [HIVE-18992] - enable synthetic file IDs by default in LLAP\n    * [HIVE-19003] - metastoreconf logs too much on info level\n    * [HIVE-19007] - Support REPL LOAD from primary using replica connection configurations received through WITH clause.\n    * [HIVE-19012] - Support builds for ARM and PPC arch\n    * [HIVE-19014] - utilize YARN-8028 (queue ACL check) in Hive Tez session pool\n    * [HIVE-19017] - Add util function to determine if 2 ValidWriteIdLists are at the same committed ID\n    * [HIVE-19018] - beeline -e now requires semicolon even when used with query from command line\n    * [HIVE-19019] - Vectorization: When vectorized, orc_merge_incompat_schema.q throws HiveException \"Not implemented yet\" from VectorExpressionWriterMap\n    * [HIVE-19021] - WM counters are not properly propagated from LLAP to AM\n    * [HIVE-19024] - Vectorization: Disable complex type constants for VectorUDFAdaptor\n    * [HIVE-19030] - Update Wiki with new rules for Load Data\n    * [HIVE-19032] - Vectorization: Disable GROUP BY aggregations with DISTINCT\n    * [HIVE-19035] - Vectorization: Disable exotic STRUCT field reference form\n    * [HIVE-19036] - Fix whitespace error in testconfiguration.properties after HIVE-14032\n    * [HIVE-19037] - Vectorization: Miscellaneous cleanup\n    * [HIVE-19038] - LLAP: Service loader throws \"Provider not found\" exception if hive-llap-server is in class path while loading tokens\n    * [HIVE-19042] - set MALLOC_ARENA_MAX for LLAP\n    * [HIVE-19043] - Vectorization: LazySimpleDeserializeRead fewer fields handling is broken for Complex Types\n    * [HIVE-19047] - Only the first init file is interpreted\n    * [HIVE-19050] - DBNotificationListener does not catch exceptions in the cleaner thread\n    * [HIVE-19052] - Vectorization: Disable Vector Pass-Thru SMB MapJoin in the presence of old-style MR FilterMaps\n    * [HIVE-19054] - Function replication shall use \"hive.repl.replica.functions.root.dir\" as root\n    * [HIVE-19055] - WM alter may fail if the name is not changed\n    * [HIVE-19056] - IllegalArgumentException in FixAcidKeyIndex when ORC file has 0 rows\n    * [HIVE-19057] - Query result caching cannot be disabled by client\n    * [HIVE-19061] - WM needs to output an event for allocation update\n    * [HIVE-19062] - Update constraint_partition_columns.q.out\n    * [HIVE-19065] - Metastore client compatibility check should include syncMetaStoreClient\n    * [HIVE-19071] - WM: backup resource plans cannot be used without quoted idenitifiers\n    * [HIVE-19072] - incorrect token handling for LLAP plugin endpoint\n    * [HIVE-19073] - StatsOptimizer may mangle constant columns\n    * [HIVE-19074] - Vectorization: Add llap vectorization_div0.q.out Q output file\n    * [HIVE-19075] - Fix NPE when trying to drop or get DB with null name\n    * [HIVE-19080] - Fix travis build\n    * [HIVE-19085] - FastHiveDecimal abs(0) sets sign to +ve\n    * [HIVE-19099] - HIVE-18755 forgot to update derby install script in metastore\n    * [HIVE-19100] - investigate TestStreaming failures\n    * [HIVE-19102] - Vectorization: Suppress known Q file bugs\n    * [HIVE-19105] - HIVE-18781 broke WarehouseInstance\n    * [HIVE-19108] - Vectorization and Parquet: Turning on vectorization in parquet_ppd_decimal.q causes Wrong Query Results\n    * [HIVE-19116] - Vectorization: Vector Map data type doesn't keep the order of the key/values pairs as read\n    * [HIVE-19119] - Fix the TestAppendPartitions tests which are failing in the pre-commit runs\n    * [HIVE-19120] - catalog not properly set for some tables in SQL upgrade scripts\n    * [HIVE-19121] - Fix HiveSchemaTool validation for databases that don't support schema\n    * [HIVE-19124] - implement a basic major compactor for MM tables\n    * [HIVE-19130] - NPE is thrown when REPL LOAD applied drop partition event.\n    * [HIVE-19131] - DecimalColumnStatsMergerTest comparison review\n    * [HIVE-19137] - orcfiledump doesn't print hive.acid.version value\n    * [HIVE-19151] - Update expected result for some TestNegativeCliDriver tests\n    * [HIVE-19155] - Day time saving cause Druid inserts to fail with org.apache.hive.druid.io.druid.java.util.common.UOE: Cannot add overlapping segments\n    * [HIVE-19157] - Assert that Insert into Druid Table fails if the publishing of metadata by HS2 fails\n    * [HIVE-19167] - Map data type doesn't keep the order of the key/values pairs as read (Part 2, The Sequel or SQL)   \n    * [HIVE-19168] - Ranger changes for llap commands\n    * [HIVE-19186] - Multi Table INSERT statements query has a flaw for partitioned table when INSERT INTO and INSERT OVERWRITE are used\n    * [HIVE-19187] - Update Druid Storage Handler to Druid 0.12.0\n    * [HIVE-19191] - Assertion error while running materialized view rewriting\n    * [HIVE-19200] - Vectorization: Disable vectorization for LLAP I/O when a non-VECTORIZED_INPUT_FILE_FORMAT mode is needed (i.e. rows) and data type conversion is needed\n    * [HIVE-19215] - JavaUtils.AnyIdDirFilter ignores base_n directories\n    * [HIVE-19219] - Incremental REPL DUMP should throw error if requested events are cleaned-up.\n    * [HIVE-19224] - incorrect token handling for LLAP plugin endpoint - part 2\n    * [HIVE-19226] - Extend storage-api to print timestamp values in UTC\n    * [HIVE-19230] - Schema column width inconsistency in Oracle \n    * [HIVE-19231] - Beeline generates garbled output when using UnsupportedTerminal\n    * [HIVE-19233] - Add utility for acid 1.0 to 2.0 migration\n    * [HIVE-19240] - backport HIVE-17645 to 3.0\n    * [HIVE-19247] - StatsOptimizer: Missing stats fast-path for Date\n    * [HIVE-19248] - REPL LOAD couldn't copy file from source CM path and also doesn't throw error if file copy fails.\n    * [HIVE-19249] - Replication: WITH clause is not passing the configuration to Task correctly in all cases\n    * [HIVE-19260] - Streaming Ingest API doesn't normalize db.table names\n    * [HIVE-19264] - Vectorization: Reenable vectorization in vector_adaptor_usage_mode.q\n    * [HIVE-19269] - Vectorization: Turn On by Default\n    * [HIVE-19275] - Vectorization: Defer Wrong Results / Execution Failures when Vectorization turned on\n    * [HIVE-19277] - Active/Passive HA web endpoints does not allow cross origin requests\n    * [HIVE-19280] - Invalid error messages for UPDATE/DELETE on insert-only transactional tables\n    * [HIVE-19281] - incorrect protocol name for LLAP AM plugin\n    * [HIVE-19282] - don't nest delta directories inside LB directories for ACID tables\n    * [HIVE-19298] - Fix operator tree of CTAS for Druid Storage Handler\n    * [HIVE-19310] - Metastore: MetaStoreDirectSql.ensureDbInit has some slow DN calls which might need to be run only in test env\n    * [HIVE-19315] - Test failure org.apache.hadoop.hive.ql.lockmgr.TestDbTxnManager2#testWriteSetTracking3\n    * [HIVE-19324] - improve YARN queue check error message in Tez pool\n    * [HIVE-19327] - qroupby_rollup_empty.q fails for insert-only transactional tables\n    * [HIVE-19330] - multi_insert_partitioned.q fails with \"src table does not exist\" message.\n    * [HIVE-19331] - Repl load config in \"with\" clause not pass to Context.getStagingDir\n    * [HIVE-19338] - isExplicitAnalyze method may be incorrect in BasicStatsTask\n    * [HIVE-19339] - Regenerate alltypesorc file with latest ORC\n    * [HIVE-19350] - Vectorization: Turn off vectorization for explainuser_1.q / spark_explainuser_1\n    * [HIVE-19352] - Vectorization: Disable vectorization for org.apache.hive.jdbc.TestJdbcDriver2.testResultSetMetaData\n    * [HIVE-19361] - Backport HIVE-18910 to branch -3\n    * [HIVE-19362] - enable LLAP cache affinity by default\n    * [HIVE-19363] - remove cryptic metrics from LLAP IO output\n    * [HIVE-19365] - Index on COMPLETED_TXN_COMPONENTS in Metastore RDBMS has different names in different scripts\n    * [HIVE-19367] - Load Data should fail for empty Parquet files.\n    * [HIVE-19381] - Function replication in cloud fail when download resource from AWS\n    * [HIVE-19383] - Add ArrayList$SubList kryo serializer\n    * [HIVE-19384] - Vectorization: IfExprTimestamp* do not handle NULLs correctly\n    * [HIVE-19386] - Move TABLE_BUCKETING_VERSION to hive_metastore.thrift\n    * [HIVE-19394] - WM_TRIGGER trigger creation failed with type cast from Integer to Boolean \n    * [HIVE-19396] - HiveOperation is incorrectly set for analyze statement\n    * [HIVE-19410] - don't create serde reader in LLAP if there's no cache\n    * [HIVE-19420] - Support LOAD from SeqFile to ORC table\n    * [HIVE-19423] - REPL LOAD creates staging directory in source dump directory instead of table data location\n    * [HIVE-19433] - HiveJoinPushTransitivePredicatesRule hangs\n    * [HIVE-19435] - Incremental replication cause data loss if a table is dropped followed by create and insert-into with different partition type.\n    * [HIVE-19446] - QueryCache: Transaction lists needed for pending cache entries\n    * [HIVE-19474] - Decimal type should be casted as part of the CTAS or INSERT Clause.\n    * [HIVE-19476] - Fix failures in TestReplicationScenariosAcidTables, TestReplicationOnHDFSEncryptedZones and TestCopyUtils\n    * [HIVE-19477] - Hiveserver2 in http mode not emitting metric default.General.open_connections\n    * [HIVE-19479] - encoded stream seek is incorrect for 0-length RGs in LLAP IO\n    * [HIVE-19483] - Metastore cleaner tasks that run periodically are created more than once\n    * [HIVE-19506] - Test suites timing out\n\n\n** New Feature\n    * [HIVE-1010] - Implement INFORMATION_SCHEMA in Hive\n    * [HIVE-8838] - Support Parquet through HCatalog\n    * [HIVE-15229] - 'like any' and 'like all' operators in hive\n    * [HIVE-15434] - Add UDF to allow interrogation of uniontype values\n    * [HIVE-15571] - Support Insert into for druid storage handler\n    * [HIVE-15691] - Create StrictRegexWriter to work with RegexSerializer for Flume Hive Sink\n    * [HIVE-15996] - Implement multiargument GROUPING function\n    * [HIVE-16281] - Upgrade master branch to JDK8\n    * [HIVE-16452] - Database UUID for metastore DB\n    * [HIVE-16520] - Cache hive metadata in metastore\n    * [HIVE-16575] - Support for 'UNIQUE' and 'NOT NULL' constraints\n    * [HIVE-16602] - Implement shared scans with Tez\n    * [HIVE-16605] - Enforce NOT NULL constraints\n    * [HIVE-16643] - BeeLine tests output should keep the PREHOOK/POSTHOOK Input/Output orderdering\n    * [HIVE-16917] - HiveServer2 guard rails - Limit concurrent connections from user\n    * [HIVE-17089] - make acid 2.0 the default\n    * [HIVE-17160] - Adding kerberos Authorization to the Druid hive integration\n    * [HIVE-17361] - Support LOAD DATA for transactional tables\n    * [HIVE-17366] - Constraint replication in bootstrap\n    * [HIVE-17432] - Enable join and aggregate materialized view rewriting\n    * [HIVE-17466] - Metastore API to list unique partition-key-value combinations\n    * [HIVE-17481] - LLAP workload management\n    * [HIVE-17626] - Query reoptimization using cached runtime statistics\n    * [HIVE-17710] - LockManager should only lock Managed tables\n    * [HIVE-17717] - Enable rule to push post-aggregations into Druid\n    * [HIVE-18281] - HiveServer2 HA for LLAP and Workload Manager\n    * [HIVE-18347] - Allow pluggable dynamic lookup of Hive Metastores from HiveServer2\n    * [HIVE-18361] - Extend shared work optimizer to reuse computation beyond work boundaries\n    * [HIVE-18373] - Make it easier to search for column name in a table\n    * [HIVE-18726] - Implement DEFAULT constraint\n    * [HIVE-18739] - Add support for Import/Export from Acid table\n    * [HIVE-18814] - Support Add Partition For Acid tables\n    * [HIVE-18835] - JDBC standalone jar download link in ambari\n    * [HIVE-18841] - Support authorization of UDF usage in hive\n    * [HIVE-18953] - Implement CHECK constraint\n    * [HIVE-19059] - Support DEFAULT keyword with INSERT and UPDATE\n\n\n** Improvement\n    * [HIVE-8472] - Add ALTER DATABASE SET LOCATION\n    * [HIVE-9447] - Metastore: inefficient Oracle query for removing unused column descriptors when add/drop table/partition\n    * [HIVE-12274] - Increase width of columns used for general configuration in the metastore.\n    * [HIVE-12299] - Hive Column Data Type definition in schema limited to 4000 characters - too small\n    * [HIVE-12636] - Ensure that all queries (with DbTxnManager) run in a transaction\n    * [HIVE-13842] - Expose ability to set number of connections in the pool in TxnHandler\n    * [HIVE-14069] - update curator version to 2.12.0 \n    * [HIVE-14145] - Too small length of column 'PARAM_VALUE' in table 'SERDE_PARAMS'\n    * [HIVE-14389] - Beeline should not output query and prompt to stdout\n    * [HIVE-14786] - Beeline displays binary column data as string instead of byte array\n    * [HIVE-15053] - Beeline#addlocaldriver - reduce classpath scanning\n    * [HIVE-15300] - Reuse table information in SemanticAnalyzer::getMetaData to reduce compilation time\n    * [HIVE-15393] - Update Guava version\n    * [HIVE-15396] - Basic Stats are not collected when for managed tables with LOCATION specified\n    * [HIVE-15433] - setting hive.warehouse.subdir.inherit.perms in HIVE won't overwrite it in hive configuration\n    * [HIVE-15616] - Improve contents of qfile test output\n    * [HIVE-15631] - Optimize for hive client logs , you can filter the log for each session itself.\n    * [HIVE-15665] - LLAP: OrcFileMetadata objects in cache can impact heap usage\n    * [HIVE-15726] - Reenable indentation checks to checkstyle\n    * [HIVE-15786] - Provide additional information from the llapstatus command\n    * [HIVE-15795] - Support Accumulo Index Tables in Hive Accumulo Connector\n    * [HIVE-15880] - Allow insert overwrite and truncate table query to use auto.purge table property\n    * [HIVE-16049] - upgrade to jetty 9\n    * [HIVE-16075] - MetaStore needs to reinitialize log4j to allow log specific settings via hiveconf take effect \n    * [HIVE-16079] - HS2: high memory pressure due to duplicate Properties objects\n    * [HIVE-16084] - SHOW COMPACTIONS should display CompactionID\n    * [HIVE-16143] - Improve msck repair batching\n    * [HIVE-16152] - TestBeeLineDriver logging improvements\n    * [HIVE-16164] - Provide mechanism for passing HMS notification ID between transactional and non-transactional listeners.\n    * [HIVE-16206] - Make Codahale metrics reporters pluggable\n    * [HIVE-16242] - Run BeeLine tests parallel\n    * [HIVE-16285] - Servlet for dynamically configuring log levels\n    * [HIVE-16297] - Improving hive logging configuration variables\n    * [HIVE-16311] - Improve the performance for FastHiveDecimalImpl.fastDivide\n    * [HIVE-16334] - Query lock contains the query string, which can cause OOM on ZooKeeper\n    * [HIVE-16340] - Allow Kerberos + SSL connections to HMS\n    * [HIVE-16343] - LLAP: Publish YARN's ProcFs based memory usage to metrics for monitoring\n    * [HIVE-16345] - BeeLineDriver should be able to run qtest files which are using default database tables\n    * [HIVE-16356] - Table#validateColumns should avoid checking exhaustively for matches in a list\n    * [HIVE-16360] - Improve \"No delta files or original files found to compact in\" message\n    * [HIVE-16371] - Add bitmap selection strategy for druid storage handler\n    * [HIVE-16383] - Switch to HikariCP as default connection pooling\n    * [HIVE-16386] - Add debug logging to describe why runtime filtering semijoins are removed\n    * [HIVE-16423] - Add hint to enforce semi join optimization\n    * [HIVE-16426] - Query cancel: improve the way to handle files\n    * [HIVE-16429] - Should call invokeFailureHooks in handleInterruption to track failed query execution due to interrupted command.\n    * [HIVE-16430] - Add log to show the cancelled query id when cancelOperation is called.\n    * [HIVE-16431] - Support Parquet StatsNoJobTask for Spark & Tez engine\n    * [HIVE-16441] - De-duplicate semijoin branches in n-way joins\n    * [HIVE-16449] - BeeLineDriver should handle query result sorting\n    * [HIVE-16456] - Kill spark job when InterruptedException happens or driverContext.isShutdown is true.\n    * [HIVE-16460] - In the console output, show vertex list in topological order instead of an alphabetical sort\n    * [HIVE-16501] - Add rej/orig to .gitignore ; remove *.orig files\n    * [HIVE-16503] - LLAP: Oversubscribe memory for noconditional task size\n    * [HIVE-16527] - Support outer and mixed reference aggregates in windowed functions\n    * [HIVE-16536] - Various improvements in TestPerfCliDriver\n    * [HIVE-16550] - Semijoin Hints should be able to skip the optimization if needed.\n    * [HIVE-16552] - Limit the number of tasks a Spark job may contain\n    * [HIVE-16571] - HiveServer2: Prefer LIFO over round-robin for Tez session reuse\n    * [HIVE-16582] - HashTableLoader should log info about the input, rows, size etc.\n    * [HIVE-16594] - Add more tests for BeeLineDriver\n    * [HIVE-16595] - fix syntax in Hplsql.g4\n    * [HIVE-16604] - Use [NOT] ENFORCED for column constraint characteristics\n    * [HIVE-16614] - Support \"set local time zone\" statement\n    * [HIVE-16635] - Progressbar: Use different timeouts for running queries\n    * [HIVE-16639] - LLAP: Derive shuffle thread counts and keep-alive connections from instance count\n    * [HIVE-16663] - String Caching For Rows\n    * [HIVE-16700] - Log ZK discovery info (hostname & port) for HTTP mode when connection is established\n    * [HIVE-16711] - Remove property_id column from metastore_db_properties table\n    * [HIVE-16712] - StringBuffer v.s. StringBuilder\n    * [HIVE-16717] - Extend shared scan optimizer to handle partitions\n    * [HIVE-16723] - Enable configurable MetaStoreSchemaInfo \n    * [HIVE-16736] - General Improvements to BufferedRows\n    * [HIVE-16754] - LLAP: Print hive version info on llap daemon startup\n    * [HIVE-16758] - Better Select Number of Replications\n    * [HIVE-16759] - Add table type information to HMS log notifications\n    * [HIVE-16771] - Schematool should use MetastoreSchemaInfo to get the metastore schema version from database\n    * [HIVE-16799] - Control the max number of task for a stage in a spark job\n    * [HIVE-16805] - Utilities isEmptyPath Logging Too Chatty and Uses Bad Format\n    * [HIVE-16809] - Improve filter condition for correlated subqueries\n    * [HIVE-16811] - Estimate statistics in absence of stats\n    * [HIVE-16833] - Review org.apache.hive.jdbc.HiveMetaDataResultSet\n    * [HIVE-16834] - Review org.apache.hadoop.hive.serde2.ByteStream\n    * [HIVE-16853] - Minor org.apache.hadoop.hive.ql.exec.HashTableSinkOperator Improvement\n    * [HIVE-16855] - org.apache.hadoop.hive.ql.exec.mr.HashTableLoader Improvements\n    * [HIVE-16856] - Allow For Customization Of Buffer Size In MapJoinTableContainerSerDe\n    * [HIVE-16857] - SparkPartitionPruningSinkOperator Buffer Size\n    * [HIVE-16858] - Accumulo Utils Improvements\n    * [HIVE-16866] - existing available UDF is used in TestReplicationScenariosAcrossInstances#testDropFunctionIncrementalReplication \n    * [HIVE-16867] - Extend shared scan optimizer to reuse computation from other operators\n    * [HIVE-16873] - Remove Thread Cache From Logging\n    * [HIVE-16880] - Remove ArrayList Instantiation For Empty Arrays\n    * [HIVE-16881] - Make extractSqlBoolean More Consistent\n    * [HIVE-16885] - Non-equi Joins: Filter clauses should be pushed into the ON clause\n    * [HIVE-16890] - org.apache.hadoop.hive.serde2.io.HiveVarcharWritable - Adds Superfluous Wrapper\n    * [HIVE-16900] - optimization to give distcp a list of input files to copy to a destination target directory during repl load\n    * [HIVE-16911] - Upgrade groovy version to 2.4.11\n    * [HIVE-16914] - Change HiveMetaStoreClient to AutoCloseable\n    * [HIVE-16933] - ORA-00060: deadlock detected while waiting on commit\n    * [HIVE-16934] - Transform COUNT(x) into COUNT() when x is not nullable\n    * [HIVE-16945] - Add method to compare Operators \n    * [HIVE-16955] - General Improvements To org.apache.hadoop.hive.metastore.MetaStoreUtils\n    * [HIVE-16962] - Better error msg for Hive on Spark in case user cancels query and closes session\n    * [HIVE-16969] - Improvement performance of MapOperator for Parquet\n    * [HIVE-16970] - General Improvements To org.apache.hadoop.hive.metastore.cache.CacheUtils\n    * [HIVE-16989] - Fix some issues identified by lgtm.com\n    * [HIVE-17000] - Upgrade Hive to PARQUET 1.9.0\n    * [HIVE-17022] - Add mode in lock debug statements\n    * [HIVE-17036] - Lineage: Minor CPU/Mem optimization for lineage transform\n    * [HIVE-17037] - Use 1-to-1 Tez edge to avoid unnecessary input data shuffle\n    * [HIVE-17048] - Pass HiveOperation info to HiveSemanticAnalyzerHook through HiveSemanticAnalyzerHookContext\n    * [HIVE-17054] - Expose SQL database constraints to Calcite\n    * [HIVE-17072] - Make the parallelized timeout configurable in BeeLine tests\n    * [HIVE-17078] - Add more logs to MapredLocalTask\n    * [HIVE-17125] - Lineage: Generate lineage information on need basis when atlas hook is enabled\n    * [HIVE-17139] - Conditional expressions optimization: skip the expression evaluation if the condition is not satisfied for vectorization engine.\n    * [HIVE-17174] - LLAP: ShuffleHandler: optimize fadvise calls for broadcast edge\n    * [HIVE-17194] - JDBC: Implement Gzip compression for HTTP mode\n    * [HIVE-17229] - HiveMetastore HMSHandler locks during initialization, even though its static variable threadPool is not null\n    * [HIVE-17237] - HMS wastes 26.4% of memory due to dup strings in metastore.api.Partition.parameters\n    * [HIVE-17251] - Remove usage of org.apache.pig.ResourceStatistics#setmBytes method in HCatLoader\n    * [HIVE-17253] - Adding SUMMARY statement to HPL/SQL\n    * [HIVE-17263] - Reduce debug logging for S3 tables\n    * [HIVE-17288] - LlapOutputFormatService: Increase netty event loop threads\n    * [HIVE-17308] - Improvement in join cardinality estimation\n    * [HIVE-17329] - ensure acid side file is not overwritten\n    * [HIVE-17340] - TxnHandler.checkLock() - reduce number of SQL statements\n    * [HIVE-17341] - DbTxnManger.startHeartbeat() - randomize initial delay\n    * [HIVE-17362] - The MAX_PREWARM_TIME should be configurable on HoS\n    * [HIVE-17376] - Upgrade snappy version to 1.1.4\n    * [HIVE-17400] - Estimate stats in absence of stats for complex types\n    * [HIVE-17408] - replication distcp should only be invoked if number of files AND file size cross configured limits\n    * [HIVE-17422] - Skip non-native/temporary tables for all major table/partition related scenarios\n    * [HIVE-17426] - Execution framework in hive to run tasks in parallel\n    * [HIVE-17458] - VectorizedOrcAcidRowBatchReader doesn't handle 'original' files\n    * [HIVE-17493] - Improve PKFK cardinality estimation in Physical planning\n    * [HIVE-17513] - Refactor PathUtils to not contain instance fields\n    * [HIVE-17519] - Transpose column stats display\n    * [HIVE-17536] - StatsUtil::getBasicStatForTable doesn't distinguish b/w absence of statistics or zero stats\n    * [HIVE-17538] - Enhance estimation of stats to estimate even if only one column is missing stats\n    * [HIVE-17542] - Make HoS CombineEquivalentWorkResolver Configurable\n    * [HIVE-17543] - Enable PerfCliDriver for HoS\n    * [HIVE-17550] - Remove unreferenced q.out-s\n    * [HIVE-17569] - Compare filtered output files in BeeLine tests\n    * [HIVE-17578] - Create a TableRef object for Table/Partition\n    * [HIVE-17587] - Remove unnecessary filter from getPartitionsFromPartitionIds call\n    * [HIVE-17604] - Add druid properties to conf white list\n    * [HIVE-17606] - Improve security for DB notification related APIs\n    * [HIVE-17609] - Tool to manipulate delegation tokens\n    * [HIVE-17611] - Add new LazyBinary SerDe for faster writes\n    * [HIVE-17614] - Notification_sequence initialization using SQL statement which is compatible with Mysql 5.1\n    * [HIVE-17631] - upgrade orc to 1.4.1\n    * [HIVE-17669] - Cache to optimize SearchArgument deserialization\n    * [HIVE-17732] - Minor Improvements - org.apache.hive.hcatalog.data.JsonSerDe.java\n    * [HIVE-17740] - HiveConf - Use SLF4J Parameterization  \n    * [HIVE-17742] - AccumuloIndexedOutputFormat Use SLF4J\n    * [HIVE-17747] - HMS DropTableMessage should include the full table object\n    * [HIVE-17766] - Support non-equi LEFT SEMI JOIN\n    * [HIVE-17767] - Rewrite correlated EXISTS/IN subqueries into LEFT SEMI JOIN\n    * [HIVE-17787] - Apply more filters on the BeeLine test output files (follow-up on HIVE-17569)\n    * [HIVE-17793] - Parameterize Logging Messages\n    * [HIVE-17799] - Add Ellipsis For Truncated Query In Hive Lock\n    * [HIVE-17805] - SchemaTool validate locations should not return exit 1\n    * [HIVE-17824] - msck repair table should drop the missing partitions from metastore\n    * [HIVE-17847] - Exclude net.hydromatic:aggdesigner-algorithm jar as compile and runtime dependency\n    * [HIVE-17870] - Update NoDeleteRollingFileAppender to use Log4j2 api\n    * [HIVE-17871] - Add non nullability flag to druid time column\n    * [HIVE-17877] - HoS: combine equivalent DPP sink works\n    * [HIVE-17898] - Explain plan output enhancement\n    * [HIVE-17901] - org.apache.hadoop.hive.ql.exec.Utilities - Use Logging Parameterization and More\n    * [HIVE-17911] - org.apache.hadoop.hive.metastore.ObjectStore - Tune Up\n    * [HIVE-17912] - org.apache.hadoop.hive.metastore.security.DBTokenStore - Parameterize Logging\n    * [HIVE-17932] - Remove option to control partition level basic stats fetching\n    * [HIVE-17962] - org.apache.hadoop.hive.metastore.security.MemoryTokenStore - Parameterize Logging\n    * [HIVE-17964] - HoS: some spark configs doesn't require re-creating a session\n    * [HIVE-17965] - Remove HIVELIMITTABLESCANPARTITION support\n    * [HIVE-17969] - Metastore to alter table in batches of partitions when renaming table\n    * [HIVE-17988] - Replace patch utility usage with git apply in ptest\n    * [HIVE-18008] - Add optimization rule to remove gby from right side of left semi-join\n    * [HIVE-18009] - Multiple lateral view query is slow on hive on spark\n    * [HIVE-18010] - Update hbase version\n    * [HIVE-18023] - Redact the expression in lineage info\n    * [HIVE-18043] - Vectorization: Support List type in MapWork\n    * [HIVE-18048] - Vectorization: Support Struct type with vectorization\n    * [HIVE-18051] - qfiles: dataset support\n    * [HIVE-18061] - q.outs: be more selective with masking hdfs paths\n    * [HIVE-18123] - Explain formatted improve column expression map display\n    * [HIVE-18158] - Remove OrcRawRecordMerger.ReaderPairAcid.statementId\n    * [HIVE-18159] - Vectorization: Support Map type in MapWork\n    * [HIVE-18173] - Improve plans for correlated subqueries with non-equi predicate\n    * [HIVE-18185] - update insert_values_orig_table_use_metadata.q.out\n    * [HIVE-18246] - Replace toString with getExprString in AbstractOperatorDesc::getColumnExprMapForExplain\n    * [HIVE-18251] - Loosen restriction for some checks\n    * [HIVE-18259] - Automatic cleanup of invalidation cache for materialized views\n    * [HIVE-18283] - Better error message and error code for HoS exceptions\n    * [HIVE-18342] - Remove LinkedList from HiveAlterHandler.java\n    * [HIVE-18343] - Remove LinkedList from ColumnStatsSemanticAnalyzer.java\n    * [HIVE-18344] - Remove LinkedList from SharedWorkOptimizer.java\n    * [HIVE-18386] - Create dummy materialized views registry and make it configurable\n    * [HIVE-18387] - Minimize time that REBUILD locks the materialized view\n    * [HIVE-18410] - [Performance][Avro] Reading flat Avro tables is very expensive in Hive\n    * [HIVE-18423] - Support pushing computation from the optimizer for JDBC storage handler tables\n    * [HIVE-18448] - Drop Support For Indexes From Apache Hive\n    * [HIVE-18462] - Explain formatted for queries with map join has columnExprMap with unformatted column name\n    * [HIVE-18510] - Enable running checkstyle on test sources as well\n    * [HIVE-18540] -  remove logic for wide terminal to display in-place updates\n    * [HIVE-18543] - Add print sessionid in console\n    * [HIVE-18552] - Split hive.strict.checks.large.query into two configs\n    * [HIVE-18564] - Add a mapper to make plan transformations more easily understandable\n    * [HIVE-18586] - Upgrade Derby to 10.14.1.0\n    * [HIVE-18625] - SessionState Not Checking For Directory Creation Result\n    * [HIVE-18654] - Add Hiveserver2 specific HADOOP_OPTS environment variable \n    * [HIVE-18706] - Ensure each Yetus execution has its own separate working dir\n    * [HIVE-18716] - Delete unnecessary parameters from TaskFactory\n    * [HIVE-18718] - Integer like types throws error when there is a mismatch\n    * [HIVE-18727] - Update GenericUDFEnforceNotNullConstraint to throw an ERROR instead of Exception on failure\n    * [HIVE-18730] - Use LLAP as execution engine for Druid mini Cluster Tests\n    * [HIVE-18743] - CREATE TABLE on S3 data can be extremely slow. DO_NOT_UPDATE_STATS workaround is buggy.\n    * [HIVE-18770] - Additional tests and fixes for materialized view rewriting\n    * [HIVE-18780] - Improve schema discovery For Druid Storage Handler\n    * [HIVE-18793] - Round udf should support variable as second argument\n    * [HIVE-18797] - ExprConstNodeDesc's getExprString should put appropriate qualifier with literals\n    * [HIVE-18808] - Make compaction more robust when stats update fails\n    * [HIVE-18825] - Define ValidTxnList before starting query optimization\n    * [HIVE-18839] - Implement incremental rebuild for materialized views (only insert operations in source tables)\n    * [HIVE-18848] - Improve readability of filter conditions in explain plan when CBO is run\n    * [HIVE-18857] - Store default value text instead of default value expression in metastore\n    * [HIVE-18878] - Lower MoveTask Lock Logging to Debug\n    * [HIVE-18901] - Lower ResourceDownloader Logging to Debug\n    * [HIVE-18979] - Enable AggregateReduceFunctionsRule from Calcite\n    * [HIVE-18984] - Make time window configurable per materialized view\n    * [HIVE-18995] - Vectorization: Add option to suppress \"Execution mode: vectorized\" for testing purposes\n    * [HIVE-19001] - ALTER TABLE ADD CONSTRAINT support for CHECK constraint\n    * [HIVE-19033] - Provide an option to purge LLAP IO cache\n    * [HIVE-19070] - Add More Test To Druid Mini Cluster  queries.\n    * [HIVE-19092] - Somne improvement in bin shell scripts\n    * [HIVE-19161] - Add authorizations to information schema\n    * [HIVE-19288] - Implement protobuf logging hive hook.\n    * [HIVE-19344] - Change default value of msck.repair.batch.size \n    * [HIVE-19415] - Support CORS for all HS2 web endpoints\n    * [HIVE-19466] - Update constraint violation error message\n    * [HIVE-19534] - Allow implementations to access member variables of AbstractRecordWriter\n\n** Test\n    * [HIVE-13843] - Re-enable the HoS tests disabled in HIVE-13402\n    * [HIVE-15538] - Test HIVE-13884 with more complex query predicates\n    * [HIVE-16288] - Add blobstore tests for ORC and RCFILE file formats\n    * [HIVE-16359] - Update golden file for subquery_select.q\n    * [HIVE-16415] - Add tests covering single inserts of zero rows\n    * [HIVE-16454] - Add blobstore tests for inserting empty into dynamic partition/list bucket tables & inserting cross blobstore tables\n    * [HIVE-16540] - dynamic_semijoin_user_level is failing on MiniLlap\n    * [HIVE-16636] - TestPerfCli driver is missing query24\n    * [HIVE-16664] - Add join related Hive blobstore tests\n    * [HIVE-16673] - Test for HIVE-16413\n    * [HIVE-16831] - Add unit tests for NPE fixes in HIVE-12054\n    * [HIVE-17034] - The spark tar for itests is downloaded every time if md5sum is not installed\n    * [HIVE-17190] - Schema changes for bitvectors for unpartitioned tables\n    * [HIVE-17246] - Add having related blobstore query test\n    * [HIVE-17430] - Add LOAD DATA test for blobstores\n    * [HIVE-17636] - Add multiple_agg.q test for blobstores\n    * [HIVE-17729] - Add Database & Explain related blobstore tests\n    * [HIVE-17789] - Flaky test: TestSessionManagerMetrics.testAbandonedSessionMetrics has timing related problems\n    * [HIVE-17820] - Add buckets.q test for blobstores\n    * [HIVE-18041] - Add SORT_QUERY_RESULTS to subquery_multi\n    * [HIVE-18089] - Update golden files for few tests\n    * [HIVE-18100] - Some tests time out\n    * [HIVE-18186] - Fix wrong assertion in TestHiveMetaStoreAlterColumnPar test\n    * [HIVE-18260] - Add test case scenarios for materialized views invalidation cache and registry\n    * [HIVE-18327] - Remove the unnecessary HiveConf dependency for MiniHiveKdc\n    * [HIVE-18485] - Add more unit tests for hive.strict.checks.* properties\n    * [HIVE-18588] - Add 'checkin' profile that runs slower tests in standalone-metastore\n    * [HIVE-18867] - create_with_constraints_duplicate_name and default_constraint_invalid_default_value_length failing \n    * [HIVE-19060] - Fix the TestAppendPartitions.testAppendPartitionNullPartValues\n    * [HIVE-19123] - TestNegativeCliDriver nopart_insert failing\n    * [HIVE-19143] - Update golden files for negative tests\n    * [HIVE-19271] - TestMiniLlapLocalCliDriver default_constraint and check_constraint failing\n\n** Wish\n    * [HIVE-17540] - remove feature: describe pretty\n\n** Task\n    * [HIVE-15708] - Upgrade calcite version to 1.12\n    * [HIVE-16058] - Disable falling back to non-cbo for SemanticException for tests\n    * [HIVE-16392] - Remove hive.warehouse.subdir.inherit.perms and all permissions inheritance logic\n    * [HIVE-16395] - ConcurrentModificationException on config object in HoS\n    * [HIVE-16411] - Revert HIVE-15199\n    * [HIVE-16474] - Upgrade Druid version to 0.10\n    * [HIVE-17107] - Upgrade Yetus to 0.5.0\n    * [HIVE-17234] - Remove HBase metastore from master\n    * [HIVE-17425] - Change MetastoreConf.ConfVars internal members to be private\n    * [HIVE-17480] - repl dump sub dir should use UUID instead of timestamp\n    * [HIVE-17521] - Improve defaults for few runtime configs\n    * [HIVE-17544] - Provide classname info for function authorization\n    * [HIVE-17672] - Upgrade Calcite version to 1.14\n    * [HIVE-17857] - Upgrade to orc 1.4\n    * [HIVE-18131] - Truncate table for Acid tables\n    * [HIVE-18272] - Fix check-style violations in subquery code\n    * [HIVE-18433] - Upgrade version of com.fasterxml.jackson\n    * [HIVE-18436] - Upgrade to Spark 2.3.0\n    * [HIVE-18560] - qtests: QTestUtil refactor/split - QOutProcessor\n    * [HIVE-18598] - Disallow NOT NULL constraints to be ENABLED/ENFORCED with EXTERNAL table\n    * [HIVE-18754] - REPL STATUS should support 'with' clause\n    * [HIVE-18917] - Add spark.home to hive.conf.restricted.list\n    * [HIVE-18957] - Upgrade Calcite version to 1.16.0\n    * [HIVE-18959] - Avoid creating extra pool of threads within LLAP\n    * [HIVE-18993] - Use Druid Expressions\n    * [HIVE-19049] - Add support for Alter table add columns for Druid\n    * [HIVE-19091] - [Hive 3.0.0 Release] Rat check failure fixes\n    * [HIVE-19134] - Update copyright NOTICE and fix rat check failures\n    * [HIVE-19172] - NPE due to null EnvironmentContext in DDLTask\n    * [HIVE-19173] - Add Storage Handler runtime information as part of DESCRIBE EXTENDED\n    * [HIVE-19184] - Hive 3.0.0 release branch preparation\n    * [HIVE-19257] - HIVE-19157 commit references wrong jira\n    * [HIVE-19309] - Add Arrow dependencies to LlapServiceDriver\n    * [HIVE-19311] - Partition and bucketing support for “load data” statement\n    * [HIVE-19451] - Druid Query Execution fails with ClassNotFoundException org.antlr.v4.runtime.CharStream\n    * [HIVE-19491] - Branch-3 Start using storage-api 2.6.1 once available.\n\n\nRelease Notes - Hive - Version 2.3.0\n\n** Sub-task\n    * [HIVE-14807] - analyze table compute statistics fails due to presence of Infinity value in double column\n    * [HIVE-15556] - Replicate views\n    * [HIVE-16186] - REPL DUMP shows last event ID of the database even if we use LIMIT option.\n    * [HIVE-16249] - With column stats, mergejoin.q throws NPE\n    * [HIVE-16293] - Column pruner should continue to work when SEL has more than 1 child\n    * [HIVE-16387] - Fix failing test org.apache.hive.jdbc.TestJdbcDriver2.testResultSetMetaData\n    * [HIVE-16440] - Fix failing test columnstats_partlvl_invalid_values when autogather column stats is on\n    * [HIVE-16504] - Addition of binary licenses broke rat check\n    * [HIVE-16535] - Hive fails to build from source code tarball\n    * [HIVE-16537] - Add missing AL files\n\n\n\n\n\n\n\n** Bug\n    * [HIVE-9815] - Metastore column\"SERDE_PARAMS\".\"PARAM_VALUE\"  limited to 4000 bytes\n    * [HIVE-14077] - add implicit decimal arithmetic q test, fix issues if found \n    * [HIVE-14801] - improve TestPartitionNameWhitelistValidation stability\n    * [HIVE-15035] - Clean up Hive licenses for binary distribution\n    * [HIVE-15249] - HIve 2.1.0 is throwing InvalidObjectException(message:Invalid column type name is too long\n    * [HIVE-15829] - LLAP text cache: disable memory tracking on the writer\n    * [HIVE-15923] - Hive default partition causes errors in get partitions\n    * [HIVE-16007] - When the query does not complie the LogRunnable never stops\n    * [HIVE-16188] - beeline should block the connection if given invalid database name.\n    * [HIVE-16193] - Hive show compactions not reflecting the status of the application\n    * [HIVE-16219] - metastore notification_log contains serialized message with  non functional fields\n    * [HIVE-16231] - Parquet timestamp may be stored differently since HIVE-12767\n    * [HIVE-16274] - Support tuning of NDV of columns using lower/upper bounds\n    * [HIVE-16287] - Alter table partition rename with location - moves partition back to hive warehouse\n    * [HIVE-16301] - Preparing for 2.3 development.\n    * [HIVE-16305] - Additional Datanucleus ClassLoaderResolverImpl leaks causing HS2 OOM\n    * [HIVE-16308] - PreExecutePrinter and PostExecutePrinter should log to INFO level instead of ERROR\n    * [HIVE-16310] - Get the output operators of Reducesink when vectorization is on\n    * [HIVE-16315] - Describe table doesn't show num of partitions\n    * [HIVE-16318] - LLAP cache: address some issues in 2.2/2.3\n    * [HIVE-16321] - Possible deadlock in metastore with Acid enabled\n    * [HIVE-16336] - Rename hive.spark.use.file.size.for.mapjoin to hive.spark.use.ts.stats.for.mapjoin\n    * [HIVE-16341] - Tez Task Execution Summary has incorrect input record counts on some operators\n    * [HIVE-16366] - Hive 2.3 release planning\n    * [HIVE-16380] - removing global test dependency of jsonassert\n    * [HIVE-16385] - StatsNoJobTask could exit early before all partitions have been processed\n    * [HIVE-16390] - LLAP IO should take job config into account; also LLAP config should load defaults\n    * [HIVE-16403] - LLAP UI shows the wrong number of executors\n    * [HIVE-16459] - Forward channelInactive to RpcDispatcher\n    * [HIVE-16461] - DagUtils checks local resource size on the remote fs\n    * [HIVE-16465] - NullPointer Exception when enable vectorization for Parquet file format\n    * [HIVE-16473] - Hive-on-Tez may fail to write to an HBase table\n    * [HIVE-16519] - Fix exception thrown by checkOutputSpecs\n    * [HIVE-16545] - LLAP: bug in arena size determination logic\n    * [HIVE-16547] - LLAP: may not unlock buffers in some cases\n\n\n\n\n\n** Improvement\n    * [HIVE-12274] - Increase width of columns used for general configuration in the metastore.\n    * [HIVE-12299] - Hive Column Data Type definition in schema limited to 4000 characters - too small\n    * [HIVE-14145] - Too small length of column 'PARAM_VALUE' in table 'SERDE_PARAMS'\n    * [HIVE-15880] - Allow insert overwrite and truncate table query to use auto.purge table property\n    * [HIVE-16115] - Stop printing progress info from operation logs with beeline progress bar\n    * [HIVE-16164] - Provide mechanism for passing HMS notification ID between transactional and non-transactional listeners.\n\n\n\n\n** New Feature\n    * [HIVE-15434] - Add UDF to allow interrogation of uniontype values\n    * [HIVE-15691] - Create StrictRegexWriter to work with RegexSerializer for Flume Hive Sink\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n** Test\n    * [HIVE-16288] - Add blobstore tests for ORC and RCFILE file formats\n    * [HIVE-16415] - Add tests covering single inserts of zero rows\n    * [HIVE-16454] - Add blobstore tests for inserting empty into dynamic partition/list bucket tables & inserting cross blobstore tables\n\n\n\n"
        },
        {
          "name": "accumulo-handler",
          "type": "tree",
          "content": null
        },
        {
          "name": "beeline",
          "type": "tree",
          "content": null
        },
        {
          "name": "bin",
          "type": "tree",
          "content": null
        },
        {
          "name": "checkstyle",
          "type": "tree",
          "content": null
        },
        {
          "name": "classification",
          "type": "tree",
          "content": null
        },
        {
          "name": "cli",
          "type": "tree",
          "content": null
        },
        {
          "name": "common",
          "type": "tree",
          "content": null
        },
        {
          "name": "conf",
          "type": "tree",
          "content": null
        },
        {
          "name": "contrib",
          "type": "tree",
          "content": null
        },
        {
          "name": "data",
          "type": "tree",
          "content": null
        },
        {
          "name": "dev-support",
          "type": "tree",
          "content": null
        },
        {
          "name": "druid-handler",
          "type": "tree",
          "content": null
        },
        {
          "name": "errata.txt",
          "type": "blob",
          "size": 11.2646484375,
          "content": "Commits with the wrong or no JIRA referenced:\n\ngit commit                               branch     jira       url\n1d18fd9a27cfbe7f6d735e422d6d9f79f07c3c22 master     HIVE-23848 https://issues.apache.org/jira/browse/HIVE-23848\n1aec6a39424942f5dc7974413ad5ea2829985a73 master     HIVE-19826 https://issues.apache.org/jira/browse/HIVE-19826\n7981d38cbf6ef43384f5fb51b1560d7cf5add729 branch-3   HIVE-19826 https://issues.apache.org/jira/browse/HIVE-19826\n233884620af67e6af72b60629f799a69f5823eb2 master     HIVE-18627 https://issues.apache.org/jira/browse/HIVE-18627\neb0034c0cdcc5f10fd5d7382e2caf787a8003e7a master     HIVE-17420 https://issues.apache.org/jira/browse/HIVE-17420\nf1aae85f197de09d4b86143f7f13d5aa21d2eb85 master     HIVE-16431 https://issues.apache.org/jira/browse/HIVE-16431\ncbab5b29f26ceb3d4633ade9647ce8bcb2f020a0 master     HIVE-16422 https://issues.apache.org/jira/browse/HIVE-16422\ne6143de2b0c3f53d32db8a743119e3a8080d4f85 master     HIVE-16425 https://issues.apache.org/jira/browse/HIVE-16425\n3f90794d872e90c29a068f16cdf3f45b1cf52c74 master     HIVE-15579 https://issues.apache.org/jira/browse/HIVE-15579\n5a576b6fbf1680ab4dd8f275cad484a2614ef2c1 master     HIVE-10391 https://issues.apache.org/jira/browse/HIVE-10391\n582f4e1bc39b9605d11f762480b29561a44688ae llap       HIVE-10217 https://issues.apache.org/jira/browse/HIVE-10217\n8981f365bf0cf921bc0ac2ff8914df44ca2f7de7 master     HIVE-10500 https://issues.apache.org/jira/browse/HIVE-10500\n09100831adff7589ee48e735a4beac6ebb25cb3e master     HIVE-10885 https://issues.apache.org/jira/browse/HIVE-10885\nf3ab5fda6af57afff31c29ad048d906fd095d5fb branch-1.2 HIVE-10885 https://issues.apache.org/jira/browse/HIVE-10885\ndcf21cd6fa98fb5db01ef661bb3b9f94d9ca2d15 master     HIVE-10021 https://issues.apache.org/jira/browse/HIVE-10021\n9763c9dd31bd5939db3ca50e75bb97955b411f6d master     HIVE-11536 https://issues.apache.org/jira/browse/HIVE-11536\n52a934f911c63fda5d69cb6036cb4e917c799259 llap       HIVE-11871 https://issues.apache.org/jira/browse/HIVE-11871\n1d0881e04e1aa9dd10dde8425427f29a53bee97a llap       HIVE-12125 https://issues.apache.org/jira/browse/HIVE-12125\n7f21a4254dff893ff6d882ab66e018075a37d484 llap       HIVE-12126 https://issues.apache.org/jira/browse/HIVE-12126\na4e32580f7dd0d7cda08695af0a1feae6b175709 llap       HIVE-12127 https://issues.apache.org/jira/browse/HIVE-12127\naebb82888e53676312a46587577ac67ad0b78579 llap       HIVE-12128 https://issues.apache.org/jira/browse/HIVE-12128\nb0860a48b75069dd24a413dca701a2685577c1cf llap       HIVE-12129 https://issues.apache.org/jira/browse/HIVE-12129\nebb8fec397e098702e85ae68919af63f6cad2ac0 llap       HIVE-12130 https://issues.apache.org/jira/browse/HIVE-12130\nb7c53456dfa49ec08952f2f1237dffb59bd3e8a9 llap       HIVE-12131 https://issues.apache.org/jira/browse/HIVE-12131\n00045de70f85f7e8c843bc7bee7846339c9781b4 llap       HIVE-12132 https://issues.apache.org/jira/browse/HIVE-12132\n3dc2dd9c195e5985712e4fba968f12fdb1c5ec2e llap       HIVE-12133 https://issues.apache.org/jira/browse/HIVE-12133\nac52a81f72c8ce26f13554a682ee7ae41a6a7015 llap       HIVE-12134 https://issues.apache.org/jira/browse/HIVE-12134\nb4df77b013d7e5b33c4a3eddee0c1d009e2f117a llap       HIVE-12135 https://issues.apache.org/jira/browse/HIVE-12135\nd4db62fb5f6779d9989f9a8153f1771895255982 llap       HIVE-12136 https://issues.apache.org/jira/browse/HIVE-12136\na12191237578abbaafb35934d094dbf1278d1412 llap       HIVE-12137 https://issues.apache.org/jira/browse/HIVE-12137\n255b3a6fc621b96f32fad68437b3d8caf04823ec llap       HIVE-12138 https://issues.apache.org/jira/browse/HIVE-12138\n29cd5c8d7817eb0618e4f115329882d2c4a20417 llap       HIVE-12139 https://issues.apache.org/jira/browse/HIVE-12139\nf314e577c1f411ee5b434ba8d81e30e937e40c68 llap       HIVE-12140 https://issues.apache.org/jira/browse/HIVE-12140\n4bbaca8b33ed31cb67862f7c815ea7b6bbe5a2b4 llap       HIVE-12141 https://issues.apache.org/jira/browse/HIVE-12141\n289863694d242a16cdd5e8ed82bc8b4ef460bfdc llap       HIVE-12142 https://issues.apache.org/jira/browse/HIVE-12142\nc4b2a13a5e9bccadf1ca430c2a110cbe5d68a66b llap       HIVE-12143 https://issues.apache.org/jira/browse/HIVE-12143\n8b6c34eaa02f87806e6567a27baeb56c74f94926 llap       HIVE-12144 https://issues.apache.org/jira/browse/HIVE-12144\n7b5f80f1273f82f864ff4a36c7d640021e7d3d6a llap       HIVE-12145 https://issues.apache.org/jira/browse/HIVE-12145\n7ebf999e2fdee5de00a145653a1e58de53650602 llap       HIVE-12146 https://issues.apache.org/jira/browse/HIVE-12146\ne6b1556e39f81dc2861f612733b2ba61c17ff698 llap       HIVE-12147 https://issues.apache.org/jira/browse/HIVE-12147\nb8acbb6ef7b97502b772569641917e6ef973b25e llap       HIVE-12147 https://issues.apache.org/jira/browse/HIVE-12147\n35c18a3d9b67013bf8cb2185a27391390aacc1e4 llap       HIVE-12147 https://issues.apache.org/jira/browse/HIVE-12147\na31d7c8a5346fe3f26ad241e4be17a09a41583dc llap       HIVE-12147 https://issues.apache.org/jira/browse/HIVE-12147\n08969c8517861d820ed353db7b1e98e9f1799d64 llap       HIVE-12147 https://issues.apache.org/jira/browse/HIVE-12147\n7dc3cf966745feaffef742a2ea4d74c89d44e766 llap       HIVE-12147 https://issues.apache.org/jira/browse/HIVE-12147\nf11222583c2b62248832010fbb7181eee369fbca llap       HIVE-12147 https://issues.apache.org/jira/browse/HIVE-12147\na7f77a73d89ee6503d5671258f74f5d7183d805e llap       HIVE-12147 https://issues.apache.org/jira/browse/HIVE-12147\n35a314bc1bb2e7e7b29232cb63d1c5adbe26234e llap       HIVE-12147 https://issues.apache.org/jira/browse/HIVE-12147\n5feb58db2c99627cb41a747a097a0ec4b019d60c llap       HIVE-12147 https://issues.apache.org/jira/browse/HIVE-12147\n2dadf56692fe33e1a67f162e57ba9d36bd26b84a llap       HIVE-12147 https://issues.apache.org/jira/browse/HIVE-12147\ne6965be3df0c74061c44e0a6aee5f74ce9d7c113 llap       HIVE-12147 https://issues.apache.org/jira/browse/HIVE-12147\n76432fbe2e463c20b0230839366f8e35a0948f0f llap       HIVE-12147 https://issues.apache.org/jira/browse/HIVE-12147\n42acf2b77b0f160629d9457774f5b109bc0b1fbe llap       HIVE-12147 https://issues.apache.org/jira/browse/HIVE-12147\n0737a4a5aed16374e1ee504f147c96ecc6636f6a llap       HIVE-12147 https://issues.apache.org/jira/browse/HIVE-12147\nd487f800a09936562dd41b6d8a039904c14dfaff llap       HIVE-12147 https://issues.apache.org/jira/browse/HIVE-12147\nccb63a31cb5f9003221341bad592080918627565 llap       HIVE-12147 https://issues.apache.org/jira/browse/HIVE-12147\n74c4bdfeb03f2119e01a439cc86e384ddd2bfcde llap       HIVE-12147 https://issues.apache.org/jira/browse/HIVE-12147\n371c2ba38cfd90feca4be2878daf030cf8a85bfb llap       HIVE-12147 https://issues.apache.org/jira/browse/HIVE-12147\n52589882b8a69577f38dbe64a1b64e51bb5f6b52 llap       HIVE-12147 https://issues.apache.org/jira/browse/HIVE-12147\nf9bb03441c2e50c31d29582083d467e32bc5e088 llap       HIVE-12147 https://issues.apache.org/jira/browse/HIVE-12147\ne5bea303829174a4999b03bbcee5b0ad57a3bcf3 llap       HIVE-12147 https://issues.apache.org/jira/browse/HIVE-12147\n94f3b0590ac749b2f13c2841d0b3c47c16c5d8b7 llap       HIVE-12147 https://issues.apache.org/jira/browse/HIVE-12147\n69886819281100327ffb9527a001a7956ffc8daf llap       HIVE-12148 https://issues.apache.org/jira/browse/HIVE-12148\n055ed8dc679d0f59645f2cf1b118ab125e24d4f5 llap       HIVE-12148 https://issues.apache.org/jira/browse/HIVE-12148\n05630792e1adf24e1ead0a3b03fcf0d4af689909 llap       HIVE-12149 https://issues.apache.org/jira/browse/HIVE-12149\n744dc9c36dd50d2c7ef8b54a76aba1d4109f1b23 llap       HIVE-12149 https://issues.apache.org/jira/browse/HIVE-12149\n7775f7cbad687ee39b78538c38bb0a5c0329e076 llap       HIVE-12150 https://issues.apache.org/jira/browse/HIVE-12150\n541fcbe720df8c62e3bd4e00311c9a8c95bb12a4 llap       HIVE-12150 https://issues.apache.org/jira/browse/HIVE-12150\n53094ba7190b326d32be5e43ed4d992823c5dd4e llap       HIVE-12150 https://issues.apache.org/jira/browse/HIVE-12150\n0f556fe3723ebb67dc22793fbfa4cc0e2e248f35 llap       HIVE-12150 https://issues.apache.org/jira/browse/HIVE-12150\n4104b2c35eaac2669e862f6703dc003e94aba0f6 llap       HIVE-12151 https://issues.apache.org/jira/browse/HIVE-12151\nac4baea04ffc801bd2c972d7628deba0eb9ae4a8 llap       HIVE-12151 https://issues.apache.org/jira/browse/HIVE-12151\n200749619c929474333c5d540eadd3751d7ecb19 llap       HIVE-12151 https://issues.apache.org/jira/browse/HIVE-12151\nf27bcd9ca3a4296625079e2caf7408e855a197db llap       HIVE-12151 https://issues.apache.org/jira/browse/HIVE-12151\n9b3756902e5d70f36540d11b50234c3d9a2adb39 llap       HIVE-12151 https://issues.apache.org/jira/browse/HIVE-12151\n7d1ea695819ccdcaa86efe8d095323b5007df7f1 llap       HIVE-12151 https://issues.apache.org/jira/browse/HIVE-12151\n2dfd8457b7ee415f1b28c5de2650b3f2457f20ea llap       HIVE-12151 https://issues.apache.org/jira/browse/HIVE-12151\nfc6be8faf5c97901ccad33edca8f8f80023b308a llap       HIVE-12151 https://issues.apache.org/jira/browse/HIVE-12151\n20acdb661a12f1bc472633d89428917275b6364d llap       HIVE-12151 https://issues.apache.org/jira/browse/HIVE-12151\n3a2e8ee7e47bd31745dfc5f6a29c602e09747f24 llap       HIVE-12152 https://issues.apache.org/jira/browse/HIVE-12152\n8ed270cb9d8a9c49cccf99402ca92e3df3304d9f llap       HIVE-12152 https://issues.apache.org/jira/browse/HIVE-12152\nc6565f5d65da9ed5cb452db7e313d0ce7abc1105 llap       HIVE-9729  https://issues.apache.org/jira/browse/HIVE-9729\nd8298e1c85a515150562b0df68af89c18c468638 llap       HIVE-9418  https://issues.apache.org/jira/browse/HIVE-9418\n034280ce070d812f1eb312567a974a8720943647 master     HIVE-12272 https://issues.apache.org/jira/browse/HIVE-12272\n36e855084da833915dfe6c34f74e19352b64fde9 master     HIVE-12826 https://issues.apache.org/jira/browse/HIVE-12826\n9cab4414caf1bba2eb1852536a9d3676ba7eab21 master     HIVE-12827 https://issues.apache.org/jira/browse/HIVE-12827\n3734d5b674b4e8de9c0cc751650aee3194bfb93a branch-1   HIVE-12827 https://issues.apache.org/jira/browse/HIVE-12827\n22df7a8441ca85ad7f64e5191d4675f2f36a0664 master     HIVE-14182 https://issues.apache.org/jira/browse/HIVE-14182\n223350894fe5aa653668e9f39e43218e514f2b24 master     HIVE-14182 https://issues.apache.org/jira/browse/HIVE-14182\n5c58dceeaf662b6314eedb9afa01a2896657ef77 master     HIVE-14182 https://issues.apache.org/jira/browse/HIVE-14182\nd16d4f1bcc43d6ebcab0eaf5bc635fb88b60be5f master     HIVE-9423  https://issues.apache.org/jira/browse/HIVE-9423\n130617443bb05d79c18420c0c4e903a76da3651c master     HIVE-14909 https://issues.apache.org/jira/browse/HIVE-14909\n6dace60af4b6ab4d5200310a0ad94c4530c2bec3 master     HIVE-13335 https://issues.apache.org/jira/browse/HIVE-13335\n5facfbb863366d7a661c21c57011b8dbe43f52e0 master     HIVE-16307 https://issues.apache.org/jira/browse/HIVE-16307\n1c3039333ba71665e8b954fbee88188757bb4050 master     HIVE-16743 https://issues.apache.org/jira/browse/HIVE-16743\ne7081035bb9768bc014f0aba11417418ececbaf0 master     HIVE-17109 https://issues.apache.org/jira/browse/HIVE-17109\nf33db1f68c68b552b9888988f818c03879749461 master     HIVE-18617 https://issues.apache.org/jira/browse/HIVE-18617\n1eea5a80ded2df33d57b2296b3bed98cb18383fd master     HIVE-19157 https://issues.apache.org/jira/browse/HIVE-19157\n4853a44b2fcfa702d23965ab0d3835b6b57954c4 master     HIVE-21823 https://issues.apache.org/jira/browse/HIVE-21823\nbd432c9203584c531f51559d8c97c398202f0794 master     HIVE-22267 https://issues.apache.org/jira/browse/HIVE-22267\na0ccbff838afb440461a4d6df335f824c1dccbcc master     HIVE-21327 https://issues.apache.org/jira/browse/HIVE-21327\n23bc5469daad53b6839a86cd2c9db3d1d1c86d2e master     HIVE-24423 https://issues.apache.org/jira/browse/HIVE-24423\n"
        },
        {
          "name": "hbase-handler",
          "type": "tree",
          "content": null
        },
        {
          "name": "hcatalog",
          "type": "tree",
          "content": null
        },
        {
          "name": "hplsql",
          "type": "tree",
          "content": null
        },
        {
          "name": "iceberg",
          "type": "tree",
          "content": null
        },
        {
          "name": "itests",
          "type": "tree",
          "content": null
        },
        {
          "name": "jdbc-handler",
          "type": "tree",
          "content": null
        },
        {
          "name": "jdbc",
          "type": "tree",
          "content": null
        },
        {
          "name": "kafka-handler",
          "type": "tree",
          "content": null
        },
        {
          "name": "kudu-handler",
          "type": "tree",
          "content": null
        },
        {
          "name": "lib",
          "type": "tree",
          "content": null
        },
        {
          "name": "llap-client",
          "type": "tree",
          "content": null
        },
        {
          "name": "llap-common",
          "type": "tree",
          "content": null
        },
        {
          "name": "llap-ext-client",
          "type": "tree",
          "content": null
        },
        {
          "name": "llap-server",
          "type": "tree",
          "content": null
        },
        {
          "name": "llap-tez",
          "type": "tree",
          "content": null
        },
        {
          "name": "metastore",
          "type": "tree",
          "content": null
        },
        {
          "name": "packaging",
          "type": "tree",
          "content": null
        },
        {
          "name": "parser",
          "type": "tree",
          "content": null
        },
        {
          "name": "pom.xml",
          "type": "blob",
          "size": 80.6572265625,
          "content": "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<!--\n  Licensed under the Apache License, Version 2.0 (the \"License\");\n  you may not use this file except in compliance with the License.\n  You may obtain a copy of the License at\n\n      http://www.apache.org/licenses/LICENSE-2.0\n\n  Unless required by applicable law or agreed to in writing, software\n  distributed under the License is distributed on an \"AS IS\" BASIS,\n  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n  See the License for the specific language governing permissions and\n  limitations under the License.\n-->\n<project xmlns=\"http://maven.apache.org/POM/4.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\">\n  <modelVersion>4.0.0</modelVersion>\n  <parent>\n    <groupId>org.apache</groupId>\n    <artifactId>apache</artifactId>\n    <version>23</version>\n  </parent>\n  <groupId>org.apache.hive</groupId>\n  <artifactId>hive</artifactId>\n  <version>4.1.0-SNAPSHOT</version>\n  <packaging>pom</packaging>\n  <name>Hive</name>\n  <url>https://hive.apache.org</url>\n  <inceptionYear>2008</inceptionYear>\n  <modules>\n    <module>storage-api</module>\n    <module>accumulo-handler</module>\n    <module>vector-code-gen</module>\n    <module>beeline</module>\n    <module>classification</module>\n    <module>cli</module>\n    <module>common</module>\n    <module>contrib</module>\n    <module>druid-handler</module>\n    <module>hbase-handler</module>\n    <module>jdbc-handler</module>\n    <module>hcatalog</module>\n    <module>hplsql</module>\n    <module>jdbc</module>\n    <module>metastore</module>\n    <module>parser</module>\n    <module>udf</module>\n    <module>ql</module>\n    <module>serde</module>\n    <module>service-rpc</module>\n    <module>service</module>\n    <module>streaming</module>\n    <module>llap-common</module>\n    <module>llap-client</module>\n    <module>llap-ext-client</module>\n    <module>llap-tez</module>\n    <module>llap-server</module>\n    <module>shims</module>\n    <module>kudu-handler</module>\n    <module>testutils</module>\n    <module>packaging</module>\n    <module>standalone-metastore</module>\n    <module>kafka-handler</module>\n    <module>iceberg</module>\n  </modules>\n  <properties>\n    <standalone-metastore.version>4.1.0-SNAPSHOT</standalone-metastore.version>\n    <hive.version.shortname>4.1.0</hive.version.shortname>\n    <!-- Build Properties -->\n    <maven.compiler.source>1.8</maven.compiler.source>\n    <maven.compiler.target>1.8</maven.compiler.target>\n    <maven.compiler.useIncrementalCompilation>false</maven.compiler.useIncrementalCompilation>\n    <maven.repo.local>${settings.localRepository}</maven.repo.local>\n    <hive.path.to.root>.</hive.path.to.root>\n    <hive.jdbc.driver.classifier>standalone</hive.jdbc.driver.classifier>\n    <checkstyle.conf.dir>${basedir}/${hive.path.to.root}/checkstyle</checkstyle.conf.dir>\n    <!-- SonarCloud-related variables -->\n    <sonar.moduleKey>${project.groupId}:${project.artifactId}</sonar.moduleKey>\n    <!-- Test Properties -->\n    <test.extra.path/>\n    <!--suppress UnresolvedMavenProperty -->\n    <test.hive.hadoop.classpath>${maven.test.classpath}</test.hive.hadoop.classpath>\n    <test.log4j.scheme>file://</test.log4j.scheme>\n    <test.tmp.dir>${project.build.directory}/tmp</test.tmp.dir>\n    <test.conf.dir>${project.build.directory}/testconf</test.conf.dir>\n    <test.tmp.dir.uri>file://${test.tmp.dir}</test.tmp.dir.uri>\n    <!-- Determines the log level of the console logger, hive.log is independent of this-->\n    <test.console.log.level>INFO</test.console.log.level>\n    <test.warehouse.dir>${project.build.directory}/warehouse</test.warehouse.dir>\n    <test.local.warehouse.dir>${project.build.directory}/localfs/warehouse</test.local.warehouse.dir>\n    <test.warehouse.scheme>pfile://</test.warehouse.scheme>\n    <!-- To add additional exclude patterns set this property -->\n    <test.excludes.additional/>\n    <!-- Plugin and Plugin Dependency Versions -->\n    <ant.contrib.version>1.0b3</ant.contrib.version>\n    <maven.test.jvm.args>-Xmx2048m -DJETTY_AVAILABLE_PROCESSORS=4 -Duser.country=US</maven.test.jvm.args>\n    <maven.checkstyle.plugin.version>2.17</maven.checkstyle.plugin.version>\n    <maven.build-helper.plugin.version>3.4.0</maven.build-helper.plugin.version>\n    <maven.eclipse.plugin.version>2.10</maven.eclipse.plugin.version>\n    <maven.exec.plugin.version>3.1.0</maven.exec.plugin.version>\n    <maven.versions.plugin.version>2.16.0</maven.versions.plugin.version>\n    <maven.shade.plugin.version>3.5.0</maven.shade.plugin.version>\n    <maven.surefire.plugin.version>3.5.1</maven.surefire.plugin.version>\n    <maven.cyclonedx.plugin.version>2.7.10</maven.cyclonedx.plugin.version>\n    <maven.license.plugin.version>2.3.0</maven.license.plugin.version>\n    <!-- Library Dependency Versions -->\n    <accumulo.version>1.10.4</accumulo.version>\n    <ant.version>1.10.13</ant.version>\n    <antlr.version>3.5.2</antlr.version>\n    <!-- Make sure to sync it with standalone-metastore/pom.xml -->\n    <antlr4.version>4.9.3</antlr4.version>\n    <apache-directory-server.version>1.5.7</apache-directory-server.version>\n    <!-- Include arrow for LlapOutputFormatService -->\n    <arrow.version>16.0.0</arrow.version>\n    <avatica.version>1.12.0</avatica.version>\n    <avro.version>1.11.4</avro.version>\n    <bcprov-jdk18on.version>1.78</bcprov-jdk18on.version>\n    <calcite.version>1.25.0</calcite.version>\n    <datanucleus-api-jdo.version>5.2.8</datanucleus-api-jdo.version>\n    <datanucleus-core.version>5.2.10</datanucleus-core.version>\n    <datanucleus-jdo.version>3.2.0-release</datanucleus-jdo.version>\n    <datanucleus-rdbms.version>5.2.10</datanucleus-rdbms.version>\n    <commons-cli.version>1.5.0</commons-cli.version>\n    <commons-codec.version>1.15</commons-codec.version>\n    <commons-collections.version>3.2.2</commons-collections.version>\n    <commons-collections4.version>4.1</commons-collections4.version>\n    <commons-compress.version>1.26.0</commons-compress.version>\n    <commons-configuration.version>1.10</commons-configuration.version>\n    <commons-exec.version>1.1</commons-exec.version>\n    <commons-io.version>2.14.0</commons-io.version>\n    <commons-lang3.version>3.12.0</commons-lang3.version>\n    <commons-math3.version>3.6.1</commons-math3.version>\n    <commons-dbcp2.version>2.12.0</commons-dbcp2.version>\n    <commons-text.version>1.10.0</commons-text.version>\n    <derby.version>10.14.2.0</derby.version>\n    <dropwizard.version>3.1.0</dropwizard.version>\n    <dropwizard-metrics-hadoop-metrics2-reporter.version>0.1.2</dropwizard-metrics-hadoop-metrics2-reporter.version>\n    <druid.version>0.17.1</druid.version>\n    <esri.version>2.2.4</esri.version>\n    <flatbuffers.version>1.12.0</flatbuffers.version>\n    <guava.version>22.0</guava.version>\n    <groovy.version>2.4.21</groovy.version>\n    <h2database.version>2.2.220</h2database.version>\n    <hadoop.version>3.3.6</hadoop.version>\n    <hadoop.bin.path>${basedir}/${hive.path.to.root}/testutils/hadoop</hadoop.bin.path>\n    <hamcrest.version>1.3</hamcrest.version>\n    <hbase.version>2.5.6-hadoop3</hbase.version>\n    <hppc.version>0.7.2</hppc.version>\n    <!-- required for logging test to avoid including hbase which pulls disruptor transitively -->\n    <disruptor.version>3.3.7</disruptor.version>\n    <hikaricp.version>4.0.3</hikaricp.version>\n    <!-- httpcomponents are not always in version sync -->\n    <httpcomponents.client.version>4.5.13</httpcomponents.client.version>\n    <httpcomponents.core.version>4.4.13</httpcomponents.core.version>\n    <ivy.version>2.5.2</ivy.version>\n    <jackson.version>2.16.1</jackson.version>\n    <jamon.plugin.version>2.3.4</jamon.plugin.version>\n    <jamon-runtime.version>2.4.1</jamon-runtime.version>\n    <javax-servlet.version>3.1.0</javax-servlet.version>\n    <javolution.version>5.5.1</javolution.version>\n    <jettison.version>1.5.4</jettison.version>\n    <jetty.version>9.4.45.v20220203</jetty.version>\n    <jersey.version>1.19.4</jersey.version>\n    <jline.version>2.14.6</jline.version>\n    <jms.version>2.0.2</jms.version>\n    <joda.version>2.9.9</joda.version>\n    <jodd.version>6.0.0</jodd.version>\n    <json.version>1.8</json.version>\n    <junit.version>4.13.2</junit.version>\n    <junit.jupiter.version>5.11.2</junit.jupiter.version>\n    <junit.vintage.version>5.11.2</junit.vintage.version>\n    <kafka.version>2.5.0</kafka.version>\n    <kryo.version>5.5.0</kryo.version>\n    <reflectasm.version>1.11.9</reflectasm.version>\n    <kudu.version>1.12.0</kudu.version>\n    <!-- Leaving libfb303 at 0.9.3 regardless of libthrift: As per THRIFT-4613 The Apache Thrift project does not publish items related to fb303 at this point -->\n    <libfb303.version>0.9.3</libfb303.version>\n    <libthrift.version>0.16.0</libthrift.version>\n    <log4j2.version>2.18.0</log4j2.version>\n    <mariadb.version>2.5.0</mariadb.version>\n    <mssql.version>6.2.1.jre8</mssql.version>\n    <mysql.version>8.2.0</mysql.version>\n    <postgres.version>42.7.3</postgres.version>\n    <oracle.version>21.3.0.0</oracle.version>\n    <opencsv.version>5.9</opencsv.version>\n    <orc.version>1.9.4</orc.version>\n    <otel.version>1.42.0</otel.version>\n    <mockito-core.version>3.4.4</mockito-core.version>\n    <mockito-inline.version>4.11.0</mockito-inline.version>\n    <mina.version>2.0.0-M5</mina.version>\n    <netty.version>4.1.116.Final</netty.version>\n    <netty3.version>3.10.5.Final</netty3.version>\n    <!-- used by druid storage handler -->\n    <pac4j-saml.version>4.5.5</pac4j-saml.version>\n    <paranamer.version>2.8</paranamer.version>\n    <parquet.version>1.14.4</parquet.version>\n    <pig.version>0.16.0</pig.version>\n    <plexus.version>1.5.6</plexus.version>\n    <protobuf.version>3.25.5</protobuf.version>\n    <rat.version>0.16.1</rat.version>\n    <roaringbit.version>1.2.1</roaringbit.version>\n    <stax.version>1.0.1</stax.version>\n    <slf4j.version>1.7.30</slf4j.version>\n    <ST4.version>4.0.4</ST4.version>\n    <storage-api.version>4.1.0-SNAPSHOT</storage-api.version>\n    <tez.version>0.10.4</tez.version>\n    <super-csv.version>2.2.0</super-csv.version>\n    <tempus-fugit.version>1.1</tempus-fugit.version>\n    <snappy.version>1.1.10.4</snappy.version>\n    <wadl-resourcedoc-doclet.version>1.4</wadl-resourcedoc-doclet.version>\n    <velocity.version>2.3</velocity.version>\n    <xerces.version>2.12.2</xerces.version>\n    <xmlsec.version>2.3.4</xmlsec.version>\n    <zookeeper.version>3.8.4</zookeeper.version>\n    <jpam.version>1.1</jpam.version>\n    <felix.version>2.4.0</felix.version>\n    <curator.version>5.2.0</curator.version>\n    <jsr305.version>3.0.0</jsr305.version>\n    <gson.version>2.9.0</gson.version>\n    <jjwt.version>0.10.5</jjwt.version>\n    <re2j.version>1.2</re2j.version>\n    <rs-api.version>2.0.1</rs-api.version>\n    <json-path.version>2.9.0</json-path.version>\n    <janino.version>3.1.12</janino.version>\n    <datasketches.version>1.2.0</datasketches.version>\n    <spotbugs.version>4.0.3</spotbugs.version>\n    <validation-api.version>1.1.0.Final</validation-api.version>\n    <aws-secretsmanager-caching.version>1.0.1</aws-secretsmanager-caching.version>\n    <aws-java-sdk.version>1.12.499</aws-java-sdk.version>\n    <jansi.version>2.4.0</jansi.version>\n    <!-- If upgrading, upgrade atlas as well in ql/pom.xml, which brings in some springframework dependencies transitively -->\n    <spring.version>5.3.39</spring.version>\n    <spring.ldap.version>2.4.1</spring.ldap.version>\n    <project.build.outputTimestamp>2024-01-01T00:00:00Z</project.build.outputTimestamp>\n  </properties>\n  <repositories>\n    <!-- This needs to be removed before checking in-->\n    <repository>\n      <id>central</id>\n      <name>central</name>\n      <url>https://repo.maven.apache.org/maven2</url>\n      <layout>default</layout>\n      <releases>\n        <enabled>true</enabled>\n        <checksumPolicy>warn</checksumPolicy>\n      </releases>\n    </repository>\n    <repository>\n      <id>repository-release</id>\n      <url>https://repository.apache.org/content/repositories/releases/</url>\n      <releases>\n        <enabled>true</enabled>\n      </releases>\n      <snapshots>\n        <enabled>true</enabled>\n      </snapshots>\n    </repository>\n    <repository>\n      <!-- shibboleth doesn't publish artifacts on maven central\n        See https://wiki.shibboleth.net/confluence/display/DEV/Use+of+Maven+Central -->\n      <id>shibboleth</id>\n      <url>https://build.shibboleth.net/nexus/content/groups/public</url>\n      <releases>\n        <enabled>true</enabled>\n        <checksumPolicy>warn</checksumPolicy>\n      </releases>\n      <snapshots>\n        <enabled>false</enabled>\n      </snapshots>\n    </repository>\n  </repositories>\n  <dependencyManagement>\n    <dependencies>\n      <!-- dependencies are always listed in sorted order by groupId, artifactId -->\n      <dependency>\n        <groupId>com.amazonaws</groupId>\n        <artifactId>aws-java-sdk-bundle</artifactId>\n        <version>${aws-java-sdk.version}</version>\n        <exclusions>\n          <exclusion>\n            <groupId>io.netty</groupId>\n            <artifactId>*</artifactId>\n          </exclusion>\n        </exclusions>\n      </dependency>\n      <dependency>\n        <groupId>com.amazonaws.secretsmanager</groupId>\n        <artifactId>aws-secretsmanager-caching-java</artifactId>\n        <version>${aws-secretsmanager-caching.version}</version>\n        <exclusions>\n          <exclusion>\n            <groupId>com.amazonaws</groupId>\n            <artifactId>aws-java-sdk-secretsmanager</artifactId>\n          </exclusion>\n        </exclusions>\n      </dependency>\n      <dependency>\n        <groupId>com.esotericsoftware</groupId>\n        <artifactId>kryo</artifactId>\n        <version>${kryo.version}</version>\n      </dependency>\n      <dependency>\n        <groupId>com.esotericsoftware</groupId>\n        <artifactId>reflectasm</artifactId>\n        <version>${reflectasm.version}</version>\n      </dependency>\n      <dependency>\n        <groupId>com.google.guava</groupId>\n        <artifactId>guava</artifactId>\n        <version>${guava.version}</version>\n      </dependency>\n      <dependency>\n        <groupId>com.google.protobuf</groupId>\n        <artifactId>protobuf-java</artifactId>\n        <version>${protobuf.version}</version>\n      </dependency>\n      <dependency>\n        <groupId>com.google.code.tempus-fugit</groupId>\n        <artifactId>tempus-fugit</artifactId>\n        <version>${tempus-fugit.version}</version>\n        <exclusions>\n          <exclusion>\n            <groupId>org.hamcrest</groupId>\n            <artifactId>hamcrest-core</artifactId>\n          </exclusion>\n        </exclusions>\n      </dependency>\n      <dependency>\n        <groupId>com.zaxxer</groupId>\n        <artifactId>HikariCP</artifactId>\n        <version>${hikaricp.version}</version>\n      </dependency>\n      <dependency>\n        <groupId>com.thoughtworks.paranamer</groupId>\n        <artifactId>paranamer</artifactId>\n        <version>${paranamer.version}</version>\n      </dependency>\n      <dependency>\n        <groupId>org.apache.parquet</groupId>\n        <artifactId>parquet</artifactId>\n        <version>${parquet.version}</version>\n      </dependency>\n      <dependency>\n        <groupId>org.apache.parquet</groupId>\n        <artifactId>parquet-column</artifactId>\n        <version>${parquet.version}</version>\n        <classifier>tests</classifier>\n      </dependency>\n      <dependency>\n        <groupId>org.apache.parquet</groupId>\n        <artifactId>parquet-hadoop-bundle</artifactId>\n        <version>${parquet.version}</version>\n      </dependency>\n      <dependency>\n        <groupId>com.sun.jersey</groupId>\n        <artifactId>jersey-core</artifactId>\n        <version>${jersey.version}</version>\n      </dependency>\n      <dependency>\n        <groupId>com.sun.jersey</groupId>\n        <artifactId>jersey-json</artifactId>\n        <version>${jersey.version}</version>\n      </dependency>\n      <dependency>\n        <groupId>com.sun.jersey</groupId>\n        <artifactId>jersey-server</artifactId>\n        <version>${jersey.version}</version>\n      </dependency>\n      <dependency>\n        <groupId>com.sun.jersey.contribs</groupId>\n        <artifactId>wadl-resourcedoc-doclet</artifactId>\n        <version>${wadl-resourcedoc-doclet.version}</version>\n      </dependency>\n      <dependency>\n        <groupId>com.sun.jersey</groupId>\n        <artifactId>jersey-servlet</artifactId>\n        <version>${jersey.version}</version>\n      </dependency>\n      <dependency>\n        <groupId>commons-cli</groupId>\n        <artifactId>commons-cli</artifactId>\n        <version>${commons-cli.version}</version>\n      </dependency>\n      <dependency>\n        <groupId>commons-codec</groupId>\n        <artifactId>commons-codec</artifactId>\n        <version>${commons-codec.version}</version>\n      </dependency>\n      <dependency>\n        <groupId>commons-collections</groupId>\n        <artifactId>commons-collections</artifactId>\n        <version>${commons-collections.version}</version>\n      </dependency>\n      <dependency>\n        <groupId>org.apache.commons</groupId>\n        <artifactId>commons-collections4</artifactId>\n        <version>${commons-collections4.version}</version>\n      </dependency>\n      <dependency>\n        <groupId>commons-io</groupId>\n        <artifactId>commons-io</artifactId>\n        <version>${commons-io.version}</version>\n      </dependency>\n      <dependency>\n        <groupId>org.apache.commons</groupId>\n        <artifactId>commons-dbcp2</artifactId>\n        <version>${commons-dbcp2.version}</version>\n      </dependency>\n      <dependency>\n        <groupId>org.apache.commons</groupId>\n        <artifactId>commons-math3</artifactId>\n        <version>${commons-math3.version}</version>\n      </dependency>\n      <dependency>\n        <groupId>io.jsonwebtoken</groupId>\n        <artifactId>jjwt-api</artifactId>\n        <version>${jjwt.version}</version>\n      </dependency>\n      <dependency>\n        <groupId>io.jsonwebtoken</groupId>\n        <artifactId>jjwt-impl</artifactId>\n        <version>${jjwt.version}</version>\n      </dependency>\n      <dependency>\n        <groupId>io.jsonwebtoken</groupId>\n        <artifactId>jjwt-jackson</artifactId>\n        <version>${jjwt.version}</version>\n      </dependency>\n      <dependency>\n        <groupId>io.netty</groupId>\n        <artifactId>netty-all</artifactId>\n        <version>${netty.version}</version>\n      </dependency>\n      <dependency>\n        <groupId>jakarta.jms</groupId>\n        <artifactId>jakarta.jms-api</artifactId>\n        <version>${jms.version}</version>\n      </dependency>\n      <dependency>\n        <groupId>javolution</groupId>\n        <artifactId>javolution</artifactId>\n        <version>${javolution.version}</version>\n      </dependency>\n      <dependency>\n        <groupId>jline</groupId>\n        <artifactId>jline</artifactId>\n        <version>${jline.version}</version>\n      </dependency>\n      <dependency>\n        <groupId>joda-time</groupId>\n        <artifactId>joda-time</artifactId>\n        <version>${joda.version}</version>\n      </dependency>\n      <dependency>\n        <groupId>junit</groupId>\n        <artifactId>junit</artifactId>\n        <version>${junit.version}</version>\n      </dependency>\n      <dependency>\n        <groupId>org.junit.jupiter</groupId>\n        <artifactId>junit-jupiter-engine</artifactId>\n        <version>${junit.jupiter.version}</version>\n      </dependency>\n      <dependency>\n        <groupId>org.junit.jupiter</groupId>\n        <artifactId>junit-jupiter-params</artifactId>\n        <version>${junit.jupiter.version}</version>\n      </dependency>\n      <dependency>\n        <groupId>org.junit.vintage</groupId>\n        <artifactId>junit-vintage-engine</artifactId>\n        <version>${junit.vintage.version}</version>\n      </dependency>\n      <dependency>\n        <groupId>org.apache.commons</groupId>\n        <artifactId>commons-text</artifactId>\n        <version>${commons-text.version}</version>\n      </dependency>\n      <dependency>\n        <groupId>org.apache.logging.log4j</groupId>\n        <artifactId>log4j-1.2-api</artifactId>\n        <version>${log4j2.version}</version>\n      </dependency>\n      <dependency>\n        <groupId>org.apache.logging.log4j</groupId>\n        <artifactId>log4j-web</artifactId>\n        <version>${log4j2.version}</version>\n      </dependency>\n      <dependency>\n        <groupId>org.apache.logging.log4j</groupId>\n        <artifactId>log4j-slf4j-impl</artifactId>\n        <version>${log4j2.version}</version>\n      </dependency>\n      <dependency>\n        <groupId>org.antlr</groupId>\n        <artifactId>antlr-runtime</artifactId>\n        <version>${antlr.version}</version>\n      </dependency>\n      <dependency>\n        <groupId>org.antlr</groupId>\n        <artifactId>ST4</artifactId>\n        <version>${ST4.version}</version>\n      </dependency>\n      <dependency>\n        <groupId>org.apache.commons</groupId>\n        <artifactId>commons-compress</artifactId>\n        <version>${commons-compress.version}</version>\n      </dependency>\n      <dependency>\n        <groupId>org.apache.commons</groupId>\n        <artifactId>commons-exec</artifactId>\n        <version>${commons-exec.version}</version>\n      </dependency>\n      <dependency>\n        <groupId>org.apache.accumulo</groupId>\n        <artifactId>accumulo-core</artifactId>\n        <version>${accumulo.version}</version>\n      </dependency>\n      <dependency>\n        <groupId>org.apache.accumulo</groupId>\n        <artifactId>accumulo-fate</artifactId>\n        <version>${accumulo.version}</version>\n      </dependency>\n      <dependency>\n        <groupId>org.apache.accumulo</groupId>\n        <artifactId>accumulo-minicluster</artifactId>\n        <version>${accumulo.version}</version>\n      </dependency>\n      <dependency>\n        <groupId>org.apache.accumulo</groupId>\n        <artifactId>accumulo-start</artifactId>\n        <version>${accumulo.version}</version>\n      </dependency>\n      <dependency>\n        <groupId>org.apache.accumulo</groupId>\n        <artifactId>accumulo-trace</artifactId>\n        <version>${accumulo.version}</version>\n      </dependency>\n      <dependency>\n        <groupId>org.apache.calcite.avatica</groupId>\n        <artifactId>avatica</artifactId>\n        <version>${avatica.version}</version>\n      </dependency>\n      <dependency>\n        <groupId>org.apache.calcite.avatica</groupId>\n        <artifactId>avatica-core</artifactId>\n        <version>${avatica.version}</version>\n      </dependency>\n      <dependency>\n        <groupId>org.apache.calcite.avatica</groupId>\n        <artifactId>avatica-metrics</artifactId>\n        <version>${avatica.version}</version>\n      </dependency>\n      <dependency>\n        <groupId>org.apache.calcite.avatica</groupId>\n        <artifactId>avatica-server</artifactId>\n        <version>${avatica.version}</version>\n      </dependency>\n      <dependency>\n        <groupId>org.apache.avro</groupId>\n        <artifactId>avro</artifactId>\n        <version>${avro.version}</version>\n      </dependency>\n      <dependency>\n        <groupId>org.apache.avro</groupId>\n        <artifactId>avro-mapred</artifactId>\n        <version>${avro.version}</version>\n        <exclusions>\n          <exclusion>\n            <groupId>org.mortbay.jetty</groupId>\n            <artifactId>jetty-util</artifactId>\n          </exclusion>\n          <exclusion>\n            <groupId>org.mortbay.jetty</groupId>\n            <artifactId>servlet-api</artifactId>\n          </exclusion>\n          <exclusion>\n            <groupId>io.netty</groupId>\n            <artifactId>netty</artifactId>\n          </exclusion>\n        </exclusions>\n      </dependency>\n      <dependency>\n        <groupId>org.apache.httpcomponents</groupId>\n        <artifactId>httpclient</artifactId>\n        <version>${httpcomponents.client.version}</version>\n      </dependency>\n      <dependency>\n        <groupId>org.apache.httpcomponents</groupId>\n        <artifactId>httpcore</artifactId>\n        <version>${httpcomponents.core.version}</version>\n      </dependency>\n      <dependency>\n        <groupId>org.apache.velocity</groupId>\n        <artifactId>velocity-engine-core</artifactId>\n        <version>${velocity.version}</version>\n      </dependency>\n      <dependency>\n        <groupId>stax</groupId>\n        <artifactId>stax-api</artifactId>\n        <version>${stax.version}</version>\n      </dependency>\n      <dependency>\n        <groupId>org.apache.calcite</groupId>\n        <artifactId>calcite-core</artifactId>\n        <version>${calcite.version}</version>\n      </dependency>\n      <dependency>\n        <groupId>org.apache.calcite</groupId>\n        <artifactId>calcite-linq4j</artifactId>\n        <version>${calcite.version}</version>\n      </dependency>\n      <dependency>\n        <groupId>org.apache.calcite</groupId>\n        <artifactId>calcite-druid</artifactId>\n        <version>${calcite.version}</version>\n      </dependency>\n      <dependency>\n        <groupId>org.apache.curator</groupId>\n        <artifactId>curator-test</artifactId>\n        <version>${curator.version}</version>\n        <scope>test</scope>\n        <exclusions>\n          <exclusion>\n            <groupId>org.junit.jupiter</groupId>\n            <artifactId>junit-jupiter-api</artifactId>\n          </exclusion>\n        </exclusions>\n      </dependency>\n      <dependency>\n        <groupId>org.apache.datasketches</groupId>\n        <artifactId>datasketches-hive</artifactId>\n        <version>${datasketches.version}</version>\n        <exclusions>\n          <exclusion>\n            <groupId>org.slf4j</groupId>\n            <artifactId>slf4j-simple</artifactId>\n          </exclusion>\n        </exclusions>\n      </dependency>\n      <dependency>\n        <groupId>org.apache.orc</groupId>\n        <artifactId>orc-core</artifactId>\n        <version>${orc.version}</version>\n        <exclusions>\n          <exclusion>\n            <groupId>org.apache.hadoop</groupId>\n            <artifactId>hadoop-common</artifactId>\n          </exclusion>\n          <exclusion>\n            <groupId>org.apache.hive</groupId>\n            <artifactId>hive-storage-api</artifactId>\n          </exclusion>\n        </exclusions>\n      </dependency>\n      <dependency>\n        <groupId>org.apache.hive</groupId>\n        <artifactId>hive-storage-api</artifactId>\n        <version>${storage-api.version}</version>\n        <exclusions>\n          <exclusion>\n            <groupId>org.apache.zookeeper</groupId>\n            <artifactId>zookeeper</artifactId>\n          </exclusion>\n          <exclusion>\n            <groupId>org.apache.curator</groupId>\n            <artifactId>curator-client</artifactId>\n          </exclusion>\n          <exclusion>\n            <groupId>org.apache.curator</groupId>\n            <artifactId>curator-recipes</artifactId>\n          </exclusion>\n        </exclusions>\n      </dependency>\n      <dependency>\n        <groupId>org.apache.pig</groupId>\n        <artifactId>pig</artifactId>\n        <version>${pig.version}</version>\n      </dependency>\n      <dependency>\n        <groupId>org.apache.thrift</groupId>\n        <artifactId>libfb303</artifactId>\n        <version>${libfb303.version}</version>\n      </dependency>\n      <dependency>\n        <groupId>org.apache.thrift</groupId>\n        <artifactId>libthrift</artifactId>\n        <version>${libthrift.version}</version>\n      </dependency>\n      <dependency>\n        <groupId>org.apache.zookeeper</groupId>\n        <artifactId>zookeeper</artifactId>\n        <version>${zookeeper.version}</version>\n        <exclusions>\n          <exclusion>\n            <groupId>ch.qos.logback</groupId>\n            <artifactId>logback-classic</artifactId>\n          </exclusion>\n          <exclusion>\n            <groupId>ch.qos.logback</groupId>\n            <artifactId>logback-core</artifactId>\n          </exclusion>\n          <exclusion>\n            <groupId>org.slf4j</groupId>\n            <artifactId>slf4j-log4j12</artifactId>\n          </exclusion>\n          <exclusion>\n            <groupId>org.slf4j</groupId>\n            <artifactId>slf4j-reload4j</artifactId>\n          </exclusion>\n          <exclusion>\n            <groupId>ch.qos.reload4j</groupId>\n            <artifactId>reload4j</artifactId>\n          </exclusion>\n          <exclusion>\n            <groupId>commons-logging</groupId>\n            <artifactId>commons-logging</artifactId>\n          </exclusion>\n          <exclusion>\n            <groupId>org.apache.httpcomponents</groupId>\n            <artifactId>httpcore</artifactId>\n          </exclusion>\n          <exclusion>\n            <groupId>org.apache.httpcomponents</groupId>\n            <artifactId>httpclient</artifactId>\n          </exclusion>\n          <exclusion>\n            <groupId>io.netty</groupId>\n            <artifactId>netty-all</artifactId>\n          </exclusion>\n        </exclusions>\n      </dependency>\n      <dependency>\n        <groupId>org.apache.curator</groupId>\n        <artifactId>curator-client</artifactId>\n        <version>${curator.version}</version>\n      </dependency>\n      <dependency>\n        <groupId>org.apache.curator</groupId>\n        <artifactId>curator-framework</artifactId>\n        <version>${curator.version}</version>\n      </dependency>\n      <dependency>\n        <groupId>org.apache.curator</groupId>\n        <artifactId>curator-recipes</artifactId>\n        <version>${curator.version}</version>\n      </dependency>\n      <dependency>\n        <groupId>org.codehaus.groovy</groupId>\n        <artifactId>groovy-all</artifactId>\n        <version>${groovy.version}</version>\n      </dependency>\n      <dependency>\n        <groupId>com.fasterxml.jackson</groupId>\n        <artifactId>jackson-bom</artifactId>\n        <version>${jackson.version}</version>\n        <type>pom</type>\n        <scope>import</scope>\n      </dependency>\n      <dependency>\n        <groupId>org.codehaus.jettison</groupId>\n        <artifactId>jettison</artifactId>\n        <version>${jettison.version}</version>\n        <exclusions>\n          <exclusion>\n            <groupId>stax</groupId>\n            <artifactId>stax-api</artifactId>\n          </exclusion>\n        </exclusions>\n      </dependency>\n      <dependency>\n        <groupId>org.eclipse.jetty</groupId>\n        <artifactId>jetty-rewrite</artifactId>\n        <version>${jetty.version}</version>\n      </dependency>\n      <dependency>\n        <groupId>org.eclipse.jetty</groupId>\n        <artifactId>jetty-server</artifactId>\n        <version>${jetty.version}</version>\n      </dependency>\n      <dependency>\n        <groupId>org.eclipse.jetty</groupId>\n        <artifactId>jetty-servlet</artifactId>\n        <version>${jetty.version}</version>\n      </dependency>\n      <dependency>\n        <groupId>org.eclipse.jetty</groupId>\n        <artifactId>jetty-runner</artifactId>\n        <version>${jetty.version}</version>\n      </dependency>\n      <dependency>\n        <groupId>org.eclipse.jetty</groupId>\n        <artifactId>jetty-webapp</artifactId>\n        <version>${jetty.version}</version>\n      </dependency>\n      <dependency>\n        <groupId>org.eclipse.jetty</groupId>\n        <artifactId>jetty-http</artifactId>\n        <version>${jetty.version}</version>\n      </dependency>\n      <dependency>\n        <groupId>org.eclipse.jetty</groupId>\n        <artifactId>jetty-util</artifactId>\n        <version>${jetty.version}</version>\n      </dependency>\n      <dependency>\n        <groupId>javax.servlet</groupId>\n        <artifactId>javax.servlet-api</artifactId>\n        <version>${javax-servlet.version}</version>\n      </dependency>\n      <dependency>\n        <groupId>org.datanucleus</groupId>\n        <artifactId>datanucleus-api-jdo</artifactId>\n        <version>${datanucleus-api-jdo.version}</version>\n      </dependency>\n      <dependency>\n        <groupId>org.datanucleus</groupId>\n        <artifactId>datanucleus-core</artifactId>\n        <version>${datanucleus-core.version}</version>\n      </dependency>\n      <dependency>\n        <groupId>org.datanucleus</groupId>\n        <artifactId>datanucleus-rdbms</artifactId>\n        <version>${datanucleus-rdbms.version}</version>\n      </dependency>\n      <dependency>\n        <groupId>org.datanucleus</groupId>\n        <artifactId>javax.jdo</artifactId>\n        <version>${datanucleus-jdo.version}</version>\n      </dependency>\n      <dependency>\n        <groupId>org.pac4j</groupId>\n        <artifactId>pac4j-saml-opensamlv3</artifactId>\n        <version>${pac4j-saml.version}</version>\n        <exclusions>\n          <exclusion>\n            <groupId>com.google.code.findbugs</groupId>\n            <artifactId>jsr305</artifactId>\n          </exclusion>\n          <exclusion>\n            <groupId>ch.qos.logback</groupId>\n            <artifactId>logback-classic</artifactId>\n          </exclusion>\n          <exclusion>\n            <groupId>xalan</groupId>\n            <artifactId>xalan</artifactId>\n          </exclusion>\n          <exclusion>\n            <groupId>org.springframework</groupId>\n            <artifactId>spring-core</artifactId>\n          </exclusion>\n          <exclusion>\n            <groupId>dom4j</groupId>\n            <artifactId>dom4j</artifactId>\n          </exclusion>\n          <exclusion>\n            <groupId>commons-collections</groupId>\n            <artifactId>commons-collections</artifactId>\n          </exclusion>\n          <exclusion>\n            <groupId>org.slf4j</groupId>\n            <artifactId>*</artifactId>\n          </exclusion>\n          <exclusion>\n            <groupId>org.jboss.logging</groupId>\n            <artifactId>*</artifactId>\n          </exclusion>\n          <exclusion>\n            <groupId>org.hibernate</groupId>\n            <artifactId>*</artifactId>\n          </exclusion>\n          <exclusion>\n            <groupId>org.hibernate.javax.persistence</groupId>\n            <artifactId>*</artifactId>\n          </exclusion>\n          <exclusion>\n            <groupId>org.springframework</groupId>\n            <artifactId>*</artifactId>\n          </exclusion>\n          <exclusion>\n            <groupId>org.javassist</groupId>\n            <artifactId>javassist</artifactId>\n          </exclusion>\n          <exclusion>\n            <!-- The dependency included in pac4j is old and has known CVEs.\n            We exclude it from here and add a separate dependency down below.-->\n            <groupId>org.bouncycastle</groupId>\n            <artifactId>org.bouncycastle</artifactId>\n          </exclusion>\n          <exclusion>\n            <groupId>org.apache.santuario</groupId>\n            <artifactId>xmlsec</artifactId>\n          </exclusion>\n        </exclusions>\n      </dependency>\n      <dependency>\n        <groupId>org.bouncycastle</groupId>\n        <artifactId>bcprov-jdk18on</artifactId>\n        <version>${bcprov-jdk18on.version}</version>\n      </dependency>\n      <dependency>\n        <groupId>org.apache.santuario</groupId>\n        <artifactId>xmlsec</artifactId>\n        <version>${xmlsec.version}</version>\n        <exclusions>\n          <exclusion>\n            <groupId>com.fasterxml.woodstox</groupId>\n            <artifactId>woodstox-core</artifactId>\n          </exclusion>\n        </exclusions>\n      </dependency>\n      <dependency>\n        <groupId>com.tdunning</groupId>\n        <artifactId>json</artifactId>\n        <version>${json.version}</version>\n      </dependency>\n      <dependency>\n        <groupId>org.slf4j</groupId>\n        <artifactId>slf4j-api</artifactId>\n        <version>${slf4j.version}</version>\n      </dependency>\n      <dependency>\n        <groupId>xerces</groupId>\n        <artifactId>xercesImpl</artifactId>\n        <version>${xerces.version}</version>\n      </dependency>\n      <dependency>\n        <groupId>org.apache.hadoop</groupId>\n        <artifactId>hadoop-client</artifactId>\n        <version>${hadoop.version}</version>\n        <exclusions>\n          <exclusion>\n            <groupId>commons-logging</groupId>\n            <artifactId>commons-logging</artifactId>\n          </exclusion>\n        </exclusions>\n      </dependency>\n      <dependency>\n        <groupId>org.apache.hadoop</groupId>\n        <artifactId>hadoop-auth</artifactId>\n        <version>${hadoop.version}</version>\n        <exclusions>\n          <exclusion>\n            <groupId>commons-logging</groupId>\n            <artifactId>commons-logging</artifactId>\n          </exclusion>\n          <exclusion>\n            <groupId>org.apache.zookeeper</groupId>\n            <artifactId>zookeeper</artifactId>\n          </exclusion>\n          <exclusion>\n            <groupId>org.apache.curator</groupId>\n            <artifactId>curator-framework</artifactId>\n          </exclusion>\n          <exclusion>\n            <groupId>org.apache.curator</groupId>\n            <artifactId>curator-test</artifactId>\n          </exclusion>\n        </exclusions>\n      </dependency>\n      <dependency>\n        <groupId>org.apache.hadoop</groupId>\n        <artifactId>hadoop-common</artifactId>\n        <version>${hadoop.version}</version>\n        <exclusions>\n          <exclusion>\n            <groupId>org.slf4j</groupId>\n            <artifactId>slf4j-log4j12</artifactId>\n          </exclusion>\n          <exclusion>\n            <groupId>org.slf4j</groupId>\n            <artifactId>slf4j-reload4j</artifactId>\n          </exclusion>\n          <exclusion>\n            <groupId>ch.qos.reload4j</groupId>\n            <artifactId>reload4j</artifactId>\n          </exclusion>\n          <exclusion>\n            <groupId>commons-logging</groupId>\n            <artifactId>commons-logging</artifactId>\n          </exclusion>\n          <exclusion>\n            <groupId>org.apache.httpcomponents</groupId>\n            <artifactId>httpcore</artifactId>\n          </exclusion>\n          <exclusion>\n            <groupId>org.apache.httpcomponents</groupId>\n            <artifactId>httpclient</artifactId>\n          </exclusion>\n          <exclusion>\n            <groupId>org.apache.zookeeper</groupId>\n            <artifactId>zookeeper</artifactId>\n          </exclusion>\n          <exclusion>\n            <groupId>org.apache.curator</groupId>\n            <artifactId>curator-test</artifactId>\n          </exclusion>\n          <exclusion>\n            <groupId>org.apache.curator</groupId>\n            <artifactId>curator-client</artifactId>\n          </exclusion>\n          <exclusion>\n            <groupId>org.apache.curator</groupId>\n            <artifactId>curator-recipes</artifactId>\n          </exclusion>\n          <exclusion>\n            <groupId>javax.servlet.jsp</groupId>\n            <artifactId>jsp-api</artifactId>\n          </exclusion>\n        </exclusions>\n      </dependency>\n      <dependency>\n        <groupId>org.apache.hadoop</groupId>\n        <artifactId>hadoop-hdfs</artifactId>\n        <version>${hadoop.version}</version>\n        <exclusions>\n          <exclusion>\n            <groupId>io.netty</groupId>\n            <artifactId>netty-all</artifactId>\n          </exclusion>\n        </exclusions>\n      </dependency>\n      <dependency>\n        <groupId>org.apache.hadoop</groupId>\n        <artifactId>hadoop-mapreduce-client-jobclient</artifactId>\n        <version>${hadoop.version}</version>\n        <exclusions>\n          <exclusion>\n            <groupId>org.slf4j</groupId>\n            <artifactId>slf4j-log4j12</artifactId>\n          </exclusion>\n          <exclusion>\n            <groupId>org.slf4j</groupId>\n            <artifactId>slf4j-reload4j</artifactId>\n          </exclusion>\n          <exclusion>\n            <groupId>ch.qos.reload4j</groupId>\n            <artifactId>reload4j</artifactId>\n          </exclusion>\n          <exclusion>\n            <groupId>commons-logging</groupId>\n            <artifactId>commons-logging</artifactId>\n          </exclusion>\n          <exclusion>\n            <groupId>com.codahale.metrics</groupId>\n            <artifactId>metrics-core</artifactId>\n          </exclusion>\n          <exclusion>\n            <groupId>io.netty</groupId>\n            <artifactId>netty-all</artifactId>\n          </exclusion>\n          <exclusion>\n            <groupId>io.netty</groupId>\n            <artifactId>netty</artifactId>\n          </exclusion>\n        </exclusions>\n      </dependency>\n      <dependency>\n        <groupId>org.apache.hadoop</groupId>\n        <artifactId>hadoop-mapreduce-client-common</artifactId>\n        <version>${hadoop.version}</version>\n        <exclusions>\n          <exclusion>\n            <groupId>org.slf4j</groupId>\n            <artifactId>slf4j-log4j12</artifactId>\n          </exclusion>\n          <exclusion>\n            <groupId>org.slf4j</groupId>\n            <artifactId>slf4j-reload4j</artifactId>\n          </exclusion>\n          <exclusion>\n            <groupId>ch.qos.reload4j</groupId>\n            <artifactId>reload4j</artifactId>\n          </exclusion>\n          <exclusion>\n            <groupId>commons-logging</groupId>\n            <artifactId>commons-logging</artifactId>\n          </exclusion>\n          <exclusion>\n            <groupId>io.netty</groupId>\n            <artifactId>netty-all</artifactId>\n          </exclusion>\n          <exclusion>\n            <groupId>io.netty</groupId>\n            <artifactId>netty</artifactId>\n          </exclusion>\n        </exclusions>\n      </dependency>\n      <dependency>\n        <groupId>org.apache.hadoop</groupId>\n        <artifactId>hadoop-mapreduce-client-core</artifactId>\n        <version>${hadoop.version}</version>\n        <exclusions>\n          <exclusion>\n            <groupId>org.slf4j</groupId>\n            <artifactId>slf4j-log4j12</artifactId>\n          </exclusion>\n          <exclusion>\n            <groupId>org.slf4j</groupId>\n            <artifactId>slf4j-reload4j</artifactId>\n          </exclusion>\n          <exclusion>\n            <groupId>ch.qos.reload4j</groupId>\n            <artifactId>reload4j</artifactId>\n          </exclusion>\n          <exclusion>\n            <groupId>org.jline</groupId>\n            <artifactId>jline</artifactId>\n          </exclusion>\n          <exclusion>\n            <groupId>commons-logging</groupId>\n            <artifactId>commons-logging</artifactId>\n          </exclusion>\n          <exclusion>\n            <groupId>io.netty</groupId>\n            <artifactId>netty-all</artifactId>\n          </exclusion>\n          <exclusion>\n            <groupId>io.netty</groupId>\n            <artifactId>netty</artifactId>\n          </exclusion>\n        </exclusions>\n      </dependency>\n      <dependency>\n        <groupId>org.apache.hadoop</groupId>\n        <artifactId>hadoop-minikdc</artifactId>\n        <version>${hadoop.version}</version>\n        <exclusions>\n          <exclusion>\n            <groupId>io.netty</groupId>\n            <artifactId>netty-all</artifactId>\n          </exclusion>\n          <exclusion>\n            <groupId>org.slf4j</groupId>\n            <artifactId>slf4j-log4j12</artifactId>\n          </exclusion>\n          <exclusion>\n            <groupId>org.slf4j</groupId>\n            <artifactId>slf4j-reload4j</artifactId>\n          </exclusion>\n          <exclusion>\n            <groupId>ch.qos.reload4j</groupId>\n            <artifactId>reload4j</artifactId>\n          </exclusion>\n          <exclusion>\n            <groupId>commons-logging</groupId>\n            <artifactId>commons-logging</artifactId>\n          </exclusion>\n        </exclusions>\n      </dependency>\n      <dependency>\n        <groupId>org.apache.hadoop</groupId>\n        <artifactId>hadoop-yarn-api</artifactId>\n        <version>${hadoop.version}</version>\n      </dependency>\n      <dependency>\n        <groupId>org.apache.hadoop</groupId>\n        <artifactId>hadoop-yarn-client</artifactId>\n        <version>${hadoop.version}</version>\n        <exclusions>\n          <exclusion>\n            <groupId>org.jline</groupId>\n            <artifactId>jline</artifactId>\n          </exclusion>\n        </exclusions>\n      </dependency>\n      <dependency>\n        <groupId>org.apache.hadoop</groupId>\n        <artifactId>hadoop-yarn-common</artifactId>\n        <version>${hadoop.version}</version>\n      </dependency>\n      <dependency>\n        <groupId>org.apache.hadoop</groupId>\n        <artifactId>hadoop-yarn-registry</artifactId>\n        <version>${hadoop.version}</version>\n      </dependency>\n      <dependency>\n        <groupId>org.apache.hadoop</groupId>\n        <artifactId>hadoop-yarn-server-web-common</artifactId>\n        <version>${hadoop.version}</version>\n      </dependency>\n      <dependency>\n        <groupId>org.apache.hadoop</groupId>\n        <artifactId>hadoop-yarn-server-web-proxy</artifactId>\n        <version>${hadoop.version}</version>\n        <exclusions>\n          <exclusion>\n            <groupId>io.netty</groupId>\n            <artifactId>netty-all</artifactId>\n          </exclusion>\n        </exclusions>\n      </dependency>\n      <dependency>\n        <groupId>org.apache.hbase</groupId>\n        <artifactId>hbase-common</artifactId>\n        <version>${hbase.version}</version>\n      </dependency>\n      <dependency>\n        <groupId>org.apache.hbase</groupId>\n        <artifactId>hbase-client</artifactId>\n        <version>${hbase.version}</version>\n      </dependency>\n      <dependency>\n        <groupId>org.apache.hbase</groupId>\n        <artifactId>hbase-hadoop-compat</artifactId>\n        <version>${hbase.version}</version>\n      </dependency>\n      <dependency>\n        <groupId>org.apache.hbase</groupId>\n        <artifactId>hbase-hadoop2-compat</artifactId>\n        <version>${hbase.version}</version>\n        <exclusions>\n          <exclusion>\n            <groupId>javax.servlet</groupId>\n            <artifactId>servlet-api</artifactId>\n          </exclusion>\n          <exclusion>\n            <groupId>javax.servlet.jsp</groupId>\n            <artifactId>jsp-api</artifactId>\n          </exclusion>\n          <exclusion>\n            <groupId>org.jruby</groupId>\n            <artifactId>jruby-complete</artifactId>\n          </exclusion>\n          <exclusion>\n            <groupId>io.netty</groupId>\n            <artifactId>netty-all</artifactId>\n          </exclusion>\n          <exclusion>\n            <groupId>io.netty</groupId>\n            <artifactId>netty</artifactId>\n          </exclusion>\n          <exclusion>\n            <groupId>com.sun.jersey</groupId>\n            <artifactId>jersey-core</artifactId>\n          </exclusion>\n          <exclusion>\n            <groupId>com.sun.jersey</groupId>\n            <artifactId>jersey-json</artifactId>\n          </exclusion>\n          <exclusion>\n            <groupId>com.sun.jersey</groupId>\n            <artifactId>jersey-server</artifactId>\n          </exclusion>\n          <exclusion>\n            <groupId>com.codahale.metrics</groupId>\n            <artifactId>metrics-core</artifactId>\n          </exclusion>\n        </exclusions>\n      </dependency>\n      <dependency>\n        <groupId>org.apache.hbase</groupId>\n        <artifactId>hbase-server</artifactId>\n        <version>${hbase.version}</version>\n        <exclusions>\n          <exclusion>\n            <groupId>org.glassfish.web</groupId>\n            <artifactId>javax.servlet.jsp</artifactId>\n          </exclusion>\n        </exclusions>\n      </dependency>\n      <dependency>\n        <groupId>org.apache.hbase</groupId>\n        <artifactId>hbase-mapreduce</artifactId>\n        <version>${hbase.version}</version>\n      </dependency>\n      <dependency>\n        <groupId>org.apache.hbase</groupId>\n        <artifactId>hbase-zookeeper</artifactId>\n        <classifier>tests</classifier>\n        <version>${hbase.version}</version>\n      </dependency>\n      <dependency>\n        <groupId>org.apache.hadoop</groupId>\n        <artifactId>hadoop-minicluster</artifactId>\n        <version>${hadoop.version}</version>\n      </dependency>\n      <dependency>\n        <groupId>org.jamon</groupId>\n        <artifactId>jamon-runtime</artifactId>\n        <version>${jamon-runtime.version}</version>\n      </dependency>\n      <dependency>\n        <groupId>org.xerial.snappy</groupId>\n        <artifactId>snappy-java</artifactId>\n        <version>${snappy.version}</version>\n      </dependency>\n      <dependency>\n        <groupId>com.google.re2j</groupId>\n        <artifactId>re2j</artifactId>\n        <version>${re2j.version}</version>\n      </dependency>\n      <dependency>\n        <groupId>com.jayway.jsonpath</groupId>\n        <artifactId>json-path</artifactId>\n        <version>${json-path.version}</version>\n        <scope>runtime</scope>\n      </dependency>\n      <dependency>\n        <groupId>org.codehaus.janino</groupId>\n        <artifactId>commons-compiler</artifactId>\n        <version>${janino.version}</version>\n        <scope>runtime</scope>\n      </dependency>\n      <dependency>\n        <groupId>org.codehaus.janino</groupId>\n        <artifactId>janino</artifactId>\n        <version>${janino.version}</version>\n        <scope>runtime</scope>\n      </dependency>\n      <dependency>\n        <groupId>org.apache.tez</groupId>\n        <artifactId>tez-runtime-internals</artifactId>\n        <version>${tez.version}</version>\n      </dependency>\n      <dependency>\n        <groupId>org.apache.tez</groupId>\n        <artifactId>tez-runtime-library</artifactId>\n        <version>${tez.version}</version>\n        <exclusions>\n          <exclusion>\n            <groupId>io.netty</groupId>\n            <artifactId>netty</artifactId>\n          </exclusion>\n        </exclusions>\n      </dependency>\n      <dependency>\n        <groupId>org.apache.tez</groupId>\n        <artifactId>tez-api</artifactId>\n        <version>${tez.version}</version>\n        <exclusions>\n          <exclusion>\n            <groupId>com.sun.jersey</groupId>\n            <artifactId>jersey-json</artifactId>\n          </exclusion>\n          <exclusion>\n            <groupId>com.sun.jersey</groupId>\n            <artifactId>jersey-client</artifactId>\n          </exclusion>\n        </exclusions>\n      </dependency>\n      <dependency>\n        <groupId>org.apache.tez</groupId>\n        <artifactId>tez-dag</artifactId>\n        <version>${tez.version}</version>\n      </dependency>\n      <dependency>\n        <groupId>org.apache.tez</groupId>\n        <artifactId>tez-mapreduce</artifactId>\n        <version>${tez.version}</version>\n        <optional>true</optional>\n        <exclusions>\n          <exclusion>\n            <groupId>io.netty</groupId>\n            <artifactId>netty</artifactId>\n          </exclusion>\n          <exclusion>\n            <groupId>org.apache.hadoop</groupId>\n            <artifactId>hadoop-common</artifactId>\n          </exclusion>\n          <exclusion>\n            <groupId>org.apache.hadoop</groupId>\n            <artifactId>hadoop-mapreduce-client-core</artifactId>\n          </exclusion>\n          <exclusion>\n            <groupId>org.apache.hadoop</groupId>\n            <artifactId>hadoop-mapreduce-client-jobclient</artifactId>\n          </exclusion>\n          <exclusion>\n            <groupId>org.apache.hadoop</groupId>\n            <artifactId>hadoop-mapreduce-client-common</artifactId>\n          </exclusion>\n          <exclusion>\n            <groupId>org.apache.hadoop</groupId>\n            <artifactId>hadoop-hdfs</artifactId>\n          </exclusion>\n          <exclusion>\n            <groupId>org.apache.hadoop</groupId>\n            <artifactId>hadoop-yarn-client</artifactId>\n          </exclusion>\n        </exclusions>\n      </dependency>\n      <dependency>\n        <groupId>org.apache.tez</groupId>\n        <artifactId>tez-common</artifactId>\n        <version>${tez.version}</version>\n      </dependency>\n      <dependency>\n        <groupId>org.apache.tez</groupId>\n        <artifactId>tez-protobuf-history-plugin</artifactId>\n        <version>${tez.version}</version>\n        <exclusions>\n          <exclusion>\n            <groupId>org.slf4j</groupId>\n            <artifactId>slf4j-log4j12</artifactId>\n          </exclusion>\n          <exclusion>\n            <groupId>commons-logging</groupId>\n            <artifactId>commons-logging</artifactId>\n          </exclusion>\n          <exclusion>\n            <groupId>org.apache.hadoop</groupId>\n            <artifactId>hadoop-common</artifactId>\n          </exclusion>\n          <exclusion>\n            <groupId>org.apache.hadoop</groupId>\n            <artifactId>hadoop-mapreduce-client-core</artifactId>\n          </exclusion>\n          <exclusion>\n            <groupId>org.apache.hadoop</groupId>\n            <artifactId>hadoop-mapreduce-client-jobclient</artifactId>\n          </exclusion>\n          <exclusion>\n            <groupId>org.apache.hadoop</groupId>\n            <artifactId>hadoop-mapreduce-client-common</artifactId>\n          </exclusion>\n          <exclusion>\n            <groupId>org.apache.hadoop</groupId>\n            <artifactId>hadoop-hdfs</artifactId>\n          </exclusion>\n          <exclusion>\n            <groupId>org.apache.hadoop</groupId>\n            <artifactId>hadoop-yarn-client</artifactId>\n          </exclusion>\n        </exclusions>\n      </dependency>\n      <dependency>\n        <groupId>org.springframework</groupId>\n        <artifactId>spring-jdbc</artifactId>\n        <version>${spring.version}</version>\n      </dependency>\n      <dependency>\n        <groupId>org.springframework</groupId>\n        <artifactId>spring-core</artifactId>\n        <version>${spring.version}</version>\n      </dependency>\n      <!-- JDBC drivers -->\n      <dependency>\n        <groupId>com.microsoft.sqlserver</groupId>\n        <artifactId>mssql-jdbc</artifactId>\n        <version>${mssql.version}</version>\n        <scope>runtime</scope>\n      </dependency>\n      <dependency>\n        <groupId>com.oracle.database.jdbc</groupId>\n        <artifactId>ojdbc8</artifactId>\n        <version>${oracle.version}</version>\n        <scope>runtime</scope>\n      </dependency>\n      <dependency>\n        <groupId>com.mysql</groupId>\n        <artifactId>mysql-connector-j</artifactId>\n        <version>${mysql.version}</version>\n        <scope>runtime</scope>\n      </dependency>\n      <dependency>\n        <groupId>org.apache.derby</groupId>\n        <artifactId>derby</artifactId>\n        <version>${derby.version}</version>\n        <scope>runtime</scope>\n      </dependency>\n      <dependency>\n        <groupId>org.mariadb.jdbc</groupId>\n        <artifactId>mariadb-java-client</artifactId>\n        <version>${mariadb.version}</version>\n        <scope>runtime</scope>\n      </dependency>\n      <dependency>\n        <groupId>org.postgresql</groupId>\n        <artifactId>postgresql</artifactId>\n        <version>${postgres.version}</version>\n        <scope>runtime</scope>\n      </dependency>\n    </dependencies>\n  </dependencyManagement>\n  <dependencies>\n    <!-- dependencies are always listed in sorted order by groupId, artifactId -->\n    <!-- global dependencies -->\n    <dependency>\n      <groupId>org.slf4j</groupId>\n      <artifactId>slf4j-api</artifactId>\n    </dependency>\n  </dependencies>\n  <build>\n    <pluginManagement>\n      <plugins>\n        <!-- plugins are always listed in sorted order by groupId, artifactId -->\n        <plugin>\n          <groupId>org.antlr</groupId>\n          <artifactId>antlr3-maven-plugin</artifactId>\n          <version>${antlr.version}</version>\n        </plugin>\n        <plugin>\n          <groupId>org.apache.avro</groupId>\n          <artifactId>avro-maven-plugin</artifactId>\n          <version>${avro.version}</version>\n        </plugin>\n        <plugin>\n          <groupId>org.apache.maven.plugins</groupId>\n          <artifactId>maven-antrun-plugin</artifactId>\n          <dependencies>\n            <dependency>\n              <groupId>ant-contrib</groupId>\n              <artifactId>ant-contrib</artifactId>\n              <version>${ant.contrib.version}</version>\n              <exclusions>\n                <exclusion>\n                  <groupId>ant</groupId>\n                  <artifactId>ant</artifactId>\n                </exclusion>\n              </exclusions>\n            </dependency>\n          </dependencies>\n        </plugin>\n        <plugin>\n          <groupId>org.apache.maven.plugins</groupId>\n          <artifactId>maven-eclipse-plugin</artifactId>\n          <version>${maven.eclipse.plugin.version}</version>\n          <configuration>\n            <downloadJavadocs>false</downloadJavadocs>\n            <downloadSources>true</downloadSources>\n            <buildOutputDirectory>target/eclipse/classes</buildOutputDirectory>\n            <workspaceActiveCodeStyleProfileName>Hive</workspaceActiveCodeStyleProfileName>\n            <workspaceCodeStylesURL>${basedir}/dev-support/eclipse-styles.xml</workspaceCodeStylesURL>\n          </configuration>\n        </plugin>\n        <plugin>\n          <groupId>org.apache.maven.plugins</groupId>\n          <artifactId>maven-checkstyle-plugin</artifactId>\n          <version>${maven.checkstyle.plugin.version}</version>\n        </plugin>\n        <plugin>\n          <groupId>org.apache.rat</groupId>\n          <artifactId>apache-rat-plugin</artifactId>\n          <version>${rat.version}</version>\n        </plugin>\n        <plugin>\n          <groupId>org.codehaus.mojo</groupId>\n          <artifactId>versions-maven-plugin</artifactId>\n          <version>${maven.versions.plugin.version}</version>\n        </plugin>\n        <plugin>\n          <groupId>org.apache.maven.plugins</groupId>\n          <artifactId>maven-surefire-plugin</artifactId>\n          <version>${maven.surefire.plugin.version}</version>\n        </plugin>\n        <plugin>\n          <groupId>org.apache.felix</groupId>\n          <artifactId>maven-bundle-plugin</artifactId>\n          <version>${felix.version}</version>\n        </plugin>\n        <plugin>\n          <groupId>org.apache.maven.plugins</groupId>\n          <artifactId>maven-shade-plugin</artifactId>\n          <version>${maven.shade.plugin.version}</version>\n        </plugin>\n        <plugin>\n          <groupId>org.codehaus.mojo</groupId>\n          <artifactId>build-helper-maven-plugin</artifactId>\n          <version>${maven.build-helper.plugin.version}</version>\n        </plugin>\n        <plugin>\n          <groupId>org.codehaus.mojo</groupId>\n          <artifactId>exec-maven-plugin</artifactId>\n          <version>${maven.exec.plugin.version}</version>\n        </plugin>\n      </plugins>\n    </pluginManagement>\n    <plugins>\n      <!-- plugins are always listed in sorted order by groupId, artifactId -->\n      <plugin>\n        <groupId>org.apache.maven.plugins</groupId>\n        <artifactId>maven-antrun-plugin</artifactId>\n        <executions>\n          <execution>\n            <id>define-classpath</id>\n            <phase>process-resources</phase>\n            <goals>\n              <goal>run</goal>\n            </goals>\n            <configuration>\n              <exportAntProperties>true</exportAntProperties>\n              <target>\n                <property name=\"maven.test.classpath\" refid=\"maven.test.classpath\"/>\n              </target>\n            </configuration>\n          </execution>\n          <execution>\n            <id>setup-test-dirs</id>\n            <phase>process-test-resources</phase>\n            <goals>\n              <goal>run</goal>\n            </goals>\n            <configuration>\n              <target>\n                <delete dir=\"${test.tmp.dir}\"/>\n                <delete dir=\"${test.conf.dir}\"/>\n                <delete dir=\"${test.warehouse.dir}\"/>\n                <mkdir dir=\"${test.tmp.dir}\"/>\n                <mkdir dir=\"${test.warehouse.dir}\"/>\n                <mkdir dir=\"${test.conf.dir}\"/>\n                <!-- copies hive-site.xml so it can be modified -->\n                <copy todir=\"${test.conf.dir}\">\n                  <fileset dir=\"${basedir}/${hive.path.to.root}/data/conf/\"/>\n                </copy>\n              </target>\n            </configuration>\n          </execution>\n        </executions>\n      </plugin>\n      <plugin>\n        <groupId>org.apache.maven.plugins</groupId>\n        <artifactId>maven-clean-plugin</artifactId>\n        <configuration>\n          <filesets>\n            <fileset>\n              <directory>./</directory>\n              <includes>\n                <include>datanucleus.log</include>\n                <include>derby.log</include>\n              </includes>\n              <followSymlinks>false</followSymlinks>\n            </fileset>\n            <fileset>\n              <directory>build</directory>\n              <followSymlinks>false</followSymlinks>\n            </fileset>\n          </filesets>\n        </configuration>\n      </plugin>\n      <plugin>\n        <groupId>org.apache.maven.plugins</groupId>\n        <artifactId>maven-checkstyle-plugin</artifactId>\n        <configuration>\n          <configLocation>${checkstyle.conf.dir}/checkstyle.xml</configLocation>\n          <propertyExpansion>config_loc=${checkstyle.conf.dir}</propertyExpansion>\n          <includeTestSourceDirectory>true</includeTestSourceDirectory>\n        </configuration>\n      </plugin>\n      <plugin>\n        <groupId>org.apache.maven.plugins</groupId>\n        <artifactId>maven-enforcer-plugin</artifactId>\n        <dependencies>\n          <dependency>\n            <groupId>de.skuzzle.enforcer</groupId>\n            <artifactId>restrict-imports-enforcer-rule</artifactId>\n            <version>0.9.0</version>\n          </dependency>\n        </dependencies>\n        <executions>\n          <execution>\n            <id>enforce-no-snapshots</id>\n            <goals>\n              <goal>enforce</goal>\n            </goals>\n            <configuration>\n              <rules>\n                <requireReleaseDeps>\n                  <message>Release builds are not allowed to have SNAPSHOT depenendencies</message>\n                  <searchTransitive>true</searchTransitive>\n                  <onlyWhenRelease>true</onlyWhenRelease>\n                </requireReleaseDeps>\n              </rules>\n              <fail>true</fail>\n            </configuration>\n          </execution>\n          <execution>\n            <id>enforce-banned-dependencies-licenses</id>\n            <goals>\n              <goal>enforce</goal>\n            </goals>\n            <configuration>\n              <rules>\n                <bannedDependencies>\n                  <excludes>\n                    <!--LGPL licenced library-->\n                    <exclude>com.google.code.findbugs:annotations</exclude>\n                  </excludes>\n                  <message>A banned license dependency was found!</message>\n                </bannedDependencies>\n              </rules>\n              <fail>true</fail>\n            </configuration>\n          </execution>\n          <execution>\n            <id>enforce-banned-dependencies-logging</id>\n            <goals>\n              <goal>enforce</goal>\n            </goals>\n            <configuration>\n              <rules>\n                <bannedDependencies>\n                  <excludes>\n                    <!-- Move to SLF4J -->\n                    <exclude>commons-logging:commons-logging</exclude>\n                    <exclude>ch.qos.reload4j:reload4j</exclude>\n                  </excludes>\n                  <searchTransitive>false</searchTransitive>\n                  <message>A banned logging dependency was found!</message>\n                </bannedDependencies>\n                <bannedDependencies>\n                  <excludes>\n                    <exclude>log4j:log4j</exclude>\n                  </excludes>\n                  <searchTransitive>true</searchTransitive>\n                  <message>Banned log4j:log4j dependency/transitive dependency was found!</message>\n                </bannedDependencies>\n              </rules>\n              <fail>true</fail>\n            </configuration>\n          </execution>\n          <execution>\n            <id>check-banned-imports</id>\n            <phase>initialize</phase>\n            <goals>\n              <goal>enforce</goal>\n            </goals>\n            <configuration>\n              <rules>\n                <restrictImports implementation=\"de.skuzzle.enforcer.restrictimports.RestrictImports\">\n                  <reason>Do not use shaded imports</reason>\n                  <bannedImports>\n                    <bannedImport>**.shaded.**</bannedImport>\n                    <bannedImport>jersey.repackaged.com.google.**</bannedImport>\n                    <bannedImport>org.codehaus.jackson.**</bannedImport>\n                    <bannedImport>org.apache.hive.com.**</bannedImport>\n                    <bannedImport>org.apache.hive.org.**</bannedImport>\n                  </bannedImports>\n                  <allowedImports>\n                    <allowedImport>org.apache.hadoop.hbase.shaded.protobuf.**</allowedImport>\n                  </allowedImports>\n                  <includeTestCode>true</includeTestCode>\n                </restrictImports>\n                <restrictImports implementation=\"de.skuzzle.enforcer.restrictimports.RestrictImports\">\n                  <reason>Do not use commons-lang</reason>\n                  <bannedImports>\n                    <bannedImport>org.apache.commons.lang.**</bannedImport>\n                  </bannedImports>\n                  <includeTestCode>true</includeTestCode>\n                </restrictImports>\n                <restrictImports implementation=\"de.skuzzle.enforcer.restrictimports.RestrictImports\">\n                  <reason>Do not use commons-logging; use slf4j</reason>\n                  <bannedImports>\n                    <bannedImport>org.apache.commons.logging.**</bannedImport>\n                  </bannedImports>\n                  <includeTestCode>true</includeTestCode>\n                </restrictImports>\n              </rules>\n            </configuration>\n          </execution>\n        </executions>\n      </plugin>\n      <plugin>\n        <groupId>org.apache.maven.plugins</groupId>\n        <artifactId>maven-surefire-plugin</artifactId>\n        <configuration>\n          <enableOutErrElements>false</enableOutErrElements>\n          <excludes>\n            <exclude>**/TestSerDe.java</exclude>\n            <exclude>**/TestHiveMetaStore.java</exclude>\n            <exclude>**/ql/exec/vector/util/*.java</exclude>\n            <exclude>**/ql/exec/vector/udf/legacy/*.java</exclude>\n            <exclude>**/ql/exec/vector/udf/generic/*.java</exclude>\n            <exclude>**/TestHiveServer2Concurrency.java</exclude>\n            <exclude>${test.excludes.additional}</exclude>\n          </excludes>\n          <redirectTestOutputToFile>true</redirectTestOutputToFile>\n          <reuseForks>false</reuseForks>\n          <failIfNoSpecifiedTests>false</failIfNoSpecifiedTests>\n          <argLine>${maven.test.jvm.args} -Xshare:off</argLine>\n          <trimStackTrace>false</trimStackTrace>\n          <additionalClasspathElements>\n            <additionalClasspathElement>${test.conf.dir}</additionalClasspathElement>\n            <additionalClasspathElement>${basedir}/${hive.path.to.root}/conf</additionalClasspathElement>\n          </additionalClasspathElements>\n          <environmentVariables>\n            <HIVE_CLUSTER_ID>hive-test-cluster-id-env</HIVE_CLUSTER_ID>\n            <TZ>US/Pacific</TZ>\n            <LANG>en_US.UTF-8</LANG>\n            <HADOOP_CLASSPATH>${test.conf.dir}:${basedir}/${hive.path.to.root}/conf</HADOOP_CLASSPATH>\n            <HIVE_HADOOP_TEST_CLASSPATH>${test.hive.hadoop.classpath}</HIVE_HADOOP_TEST_CLASSPATH>\n            <PATH>${env.PATH}${test.extra.path}</PATH>\n          </environmentVariables>\n          <systemPropertyVariables>\n            <build.dir>${project.build.directory}</build.dir>\n            <!-- required by zk test ClientBase -->\n            <build.test.dir>${test.tmp.dir}</build.test.dir>\n            <!-- required by a few tests to find the derby jar -->\n            <derby.version>${derby.version}</derby.version>\n            <derby.stream.error.file>${test.tmp.dir}/derby.log</derby.stream.error.file>\n            <hadoop.bin.path>${hadoop.bin.path}</hadoop.bin.path>\n            <!-- required by Hadoop's JobHistory -->\n            <hadoop.log.dir>${test.tmp.dir}</hadoop.log.dir>\n            <hive.root>${basedir}/${hive.path.to.root}/</hive.root>\n            <hive.version>${project.version}</hive.version>\n            <!-- required for hive-exec jar path and tests which reference a jar -->\n            <maven.local.repository>${maven.repo.local}</maven.local.repository>\n            <mapred.job.tracker>local</mapred.job.tracker>\n            <log4j.configurationFile>${test.log4j.scheme}${test.conf.dir}/hive-log4j2.properties</log4j.configurationFile>\n            <hive.test.console.log.level>${test.console.log.level}</hive.test.console.log.level>\n            <hive.cluster.id>hive-test-cluster-id-cli</hive.cluster.id>\n            <log4j.debug>true</log4j.debug>\n            <!-- don't dirty up /tmp -->\n            <java.io.tmpdir>${test.tmp.dir}</java.io.tmpdir>\n            <!-- Hadoop's minidfs class uses this -->\n            <test.build.data>${test.tmp.dir}</test.build.data>\n            <!-- required by QTestUtil -->\n            <test.data.files>${basedir}/${hive.path.to.root}/data/files</test.data.files>\n            <test.data.dir>${basedir}/${hive.path.to.root}/data/files</test.data.dir>\n            <test.tmp.dir>${test.tmp.dir}</test.tmp.dir>\n            <test.tmp.dir.uri>${test.tmp.dir.uri}</test.tmp.dir.uri>\n            <test.dfs.mkdir>${test.dfs.mkdir}</test.dfs.mkdir>\n            <test.output.overwrite>${test.output.overwrite}</test.output.overwrite>\n            <test.warehouse.dir>${test.warehouse.scheme}${test.warehouse.dir}</test.warehouse.dir>\n            <test.local.warehouse.dir>${test.warehouse.scheme}${test.local.warehouse.dir}</test.local.warehouse.dir>\n            <java.net.preferIPv4Stack>true</java.net.preferIPv4Stack>\n            <!-- EnforceReadOnlyTables hook and QTestUtil -->\n            <test.src.tables/>\n            <java.security.krb5.conf>${test.conf.dir}/krb5.conf</java.security.krb5.conf>\n            <hadoop.version>${hadoop.version}</hadoop.version>\n            <qfile>${qfile}</qfile>\n            <initScript>${initScript}</initScript>\n            <clustermode>${clustermode}</clustermode>\n            <qfile_regex>${qfile_regex}</qfile_regex>\n            <run_disabled>${run_disabled}</run_disabled>\n          </systemPropertyVariables>\n        </configuration>\n      </plugin>\n      <plugin>\n        <groupId>org.apache.rat</groupId>\n        <artifactId>apache-rat-plugin</artifactId>\n        <executions>\n          <execution>\n            <phase>process-resources</phase>\n            <goals>\n              <goal>check</goal>\n            </goals>\n          </execution>\n        </executions>\n        <configuration>\n          <excludes>\n            <eclude>.gitattributes</eclude>\n            <exclude>*.patch</exclude>\n            <exclude>**/pull_request_template.md</exclude>\n            <exclude>data/**</exclude>\n            <exclude>checkstyle/**</exclude>\n            <exclude>docs/Gemfile</exclude>\n            <exclude>bin/**</exclude>\n            <exclude>**/*.iml</exclude>\n            <exclude>**/*.txt</exclude>\n            <exclude>**/*.log</exclude>\n            <exclude>**/.factorypath</exclude>\n            <exclude>**/.classpath</exclude>\n            <exclude>**/.project</exclude>\n            <exclude>**/.settings/**</exclude>\n            <exclude>**/package-info.java</exclude>\n            <exclude>**/*.properties</exclude>\n            <exclude>**/*.q</exclude>\n            <exclude>**/*.q.out</exclude>\n            <exclude>**/*.q.out_*</exclude>\n            <exclude>**/test/**/*.xml</exclude>\n            <exclude>**/dependency-reduced-pom.xml</exclude>\n            <exclude>**/*json</exclude>\n            <exclude>**/gen/**</exclude>\n            <exclude>**/target/**</exclude>\n            <exclude>**/scripts/**</exclude>\n            <exclude>**/resources/**</exclude>\n            <exclude>**/*.rc</exclude>\n            <exclude>**/*.rcfile</exclude>\n            <exclude>**/*.qv</exclude>\n            <exclude>**/*.out</exclude>\n            <exclude>**/RecordTestObj.java</exclude>\n            <exclude>**/*.m</exclude>\n            <exclude>**/gen-java/**</exclude>\n            <exclude>**/testdata/**</exclude>\n            <exclude>**/test/org/apache/hadoop/hive/hbase/avro/**</exclude>\n            <exclude>**/avro_test.avpr</exclude>\n            <exclude>**/xmlReport.pl</exclude>\n            <exclude>**/*.html</exclude>\n            <exclude>**/sit</exclude>\n            <exclude>**/test/queries/**/*.sql</exclude>\n            <exclude>**/patchprocess/**</exclude>\n            <exclude>**/metastore_db/**</exclude>\n            <exclude>**/test/resources/**/*.ldif</exclude>\n            <exclude>hcatalog/core/mapred/**/part-m*</exclude>\n            <exclude>hcatalog/core/mapred/**/*_SUCCESS*</exclude>\n            <exclude>**/PriorityBlockingDeque.java</exclude>\n            <exclude>**/StringToDouble.java</exclude>\n            <exclude>LICENSE-binary</exclude>\n          </excludes>\n        </configuration>\n      </plugin>\n      <plugin>\n        <groupId>org.jamon</groupId>\n        <artifactId>jamon-maven-plugin</artifactId>\n        <version>${jamon.plugin.version}</version>\n      </plugin>\n    </plugins>\n  </build>\n  <profiles>\n    <profile>\n      <id>thriftif</id>\n      <build>\n        <plugins>\n          <plugin>\n            <groupId>org.codehaus.mojo</groupId>\n            <artifactId>exec-maven-plugin</artifactId>\n            <executions>\n              <execution>\n                <id>check-thrift-version</id>\n                <phase>generate-sources</phase>\n                <goals>\n                  <goal>exec</goal>\n                </goals>\n                <configuration>\n                  <executable>sh</executable>\n                  <workingDirectory>${basedir}</workingDirectory>\n                  <arguments>\n                    <argument>-c</argument>\n                    <argument>${thrift.home}/bin/thrift -version | fgrep 'Thrift version ${libthrift.version}' &amp;&amp; exit 0;\n                      echo \"=================================================================================\";\n                      echo \"========== [FATAL] Build is configured to require Thrift version ${libthrift.version} =========\";\n                      echo \"========== Currently installed: \";\n                      ${thrift.home}/bin/thrift -version;\n                      echo \"=================================================================================\";\n                      exit 1\n                    </argument>\n                  </arguments>\n                </configuration>\n              </execution>\n            </executions>\n          </plugin>\n          <plugin>\n            <groupId>org.apache.maven.plugins</groupId>\n            <artifactId>maven-antrun-plugin</artifactId>\n            <executions>\n              <execution>\n                <id>generate-thrift-sources</id>\n                <phase>generate-sources</phase>\n                <configuration>\n                  <target>\n                    <taskdef name=\"for\" classname=\"net.sf.antcontrib.logic.ForTask\" classpathref=\"maven.plugin.classpath\"/>\n                    <property name=\"thrift.args\" value=\"-I ${thrift.home} --gen java:beans,generated_annotations=undated --gen cpp --gen php --gen py --gen rb\"/>\n                    <property name=\"thrift.gen.dir\" value=\"${basedir}/src/gen/thrift\"/>\n                    <delete dir=\"${thrift.gen.dir}\"/>\n                    <mkdir dir=\"${thrift.gen.dir}\"/>\n                    <for param=\"thrift.file\">\n                      <path>\n                        <fileset dir=\".\" includes=\"if/*.thrift,if/test/*.thrift,src/main/thrift/*.thrift\"/>\n                      </path>\n                      <sequential>\n                        <echo message=\"Generating Thrift code for @{thrift.file}\"/>\n                        <exec executable=\"${thrift.home}/bin/thrift\" failonerror=\"true\" dir=\".\">\n                          <arg line=\"${thrift.args} -strict -I ${basedir}/include -I ${basedir}/.. -o ${thrift.gen.dir} @{thrift.file} \"/>\n                        </exec>\n                      </sequential>\n                    </for>\n                  </target>\n                </configuration>\n                <goals>\n                  <goal>run</goal>\n                </goals>\n              </execution>\n            </executions>\n          </plugin>\n          <plugin>\n            <groupId>org.apache.maven.plugins</groupId>\n            <artifactId>maven-enforcer-plugin</artifactId>\n            <executions>\n              <execution>\n                <id>enforce-property</id>\n                <goals>\n                  <goal>enforce</goal>\n                </goals>\n                <configuration>\n                  <rules>\n                    <requireProperty>\n                      <property>thrift.home</property>\n                    </requireProperty>\n                  </rules>\n                  <fail>true</fail>\n                </configuration>\n              </execution>\n            </executions>\n          </plugin>\n        </plugins>\n      </build>\n    </profile>\n    <profile>\n      <id>sources</id>\n      <build>\n        <plugins>\n          <plugin>\n            <groupId>org.apache.maven.plugins</groupId>\n            <artifactId>maven-source-plugin</artifactId>\n            <executions>\n              <execution>\n                <id>attach-sources</id>\n                <goals>\n                  <goal>jar</goal>\n                </goals>\n              </execution>\n            </executions>\n          </plugin>\n        </plugins>\n      </build>\n    </profile>\n    <profile>\n      <id>javadoc</id>\n      <build>\n        <plugins>\n          <plugin>\n            <groupId>org.apache.maven.plugins</groupId>\n            <artifactId>maven-javadoc-plugin</artifactId>\n            <configuration>\n              <doclint>none</doclint>\n              <useStandardDocletOptions>false</useStandardDocletOptions>\n            </configuration>\n            <executions>\n              <execution>\n                <id>attach-javadocs</id>\n                <goals>\n                  <goal>jar</goal>\n                </goals>\n              </execution>\n            </executions>\n          </plugin>\n        </plugins>\n      </build>\n    </profile>\n    <profile>\n      <id>spotbugs</id>\n      <build>\n        <plugins>\n          <!-- Execute as: com.github.spotbugs:spotbugs-maven-plugin:4.0.0:spotbugs -->\n          <plugin>\n            <groupId>com.github.spotbugs</groupId>\n            <artifactId>spotbugs-maven-plugin</artifactId>\n            <version>4.0.0</version>\n            <dependencies>\n              <!-- Specify the version of spotbugs -->\n              <dependency>\n                <groupId>com.github.spotbugs</groupId>\n                <artifactId>spotbugs</artifactId>\n                <version>${spotbugs.version}</version>\n              </dependency>\n            </dependencies>\n            <configuration>\n              <fork>true</fork>\n              <maxHeap>2048</maxHeap>\n              <jvmArgs>-Djava.awt.headless=true -Xmx2048m -Xms512m</jvmArgs>\n              <excludeFilterFile>${basedir}/${hive.path.to.root}/spotbugs/spotbugs-exclude.xml</excludeFilterFile>\n            </configuration>\n          </plugin>\n        </plugins>\n      </build>\n      <reporting>\n        <plugins>\n          <plugin>\n            <groupId>com.github.spotbugs</groupId>\n            <artifactId>spotbugs-maven-plugin</artifactId>\n            <version>4.0.0</version>\n            <configuration>\n              <fork>true</fork>\n              <maxHeap>2048</maxHeap>\n              <jvmArgs>-Djava.awt.headless=true -Xmx2048m -Xms512m</jvmArgs>\n              <excludeFilterFile>${basedir}/${hive.path.to.root}/spotbugs/spotbugs-exclude.xml</excludeFilterFile>\n            </configuration>\n          </plugin>\n        </plugins>\n      </reporting>\n    </profile>\n    <profile>\n      <!-- Windows-specific settings to allow unit tests to work -->\n      <id>windows-test</id>\n      <activation>\n        <os>\n          <family>Windows</family>\n        </os>\n      </activation>\n      <build>\n        <plugins>\n          <!-- maven.test.classpath (used for HIVE_HADOOP_TEST_CLASSPATH) exceeds the 8K Windows -->\n          <!-- command shell limit which causes tests which call hadoop command to fail.         -->\n          <!-- Workaround is to copy all necessary jars to a single location to shorten the      -->\n          <!-- the length of the environment variable.                                           -->\n          <plugin>\n            <groupId>org.apache.maven.plugins</groupId>\n            <artifactId>maven-dependency-plugin</artifactId>\n            <version>2.8</version>\n            <executions>\n              <execution>\n                <id>copy-dependencies</id>\n                <phase>package</phase>\n                <goals>\n                  <goal>copy-dependencies</goal>\n                </goals>\n                <configuration>\n                  <outputDirectory>${project.build.directory}/deplibs/</outputDirectory>\n                  <overWriteReleases>false</overWriteReleases>\n                  <overWriteSnapshots>false</overWriteSnapshots>\n                  <overWriteIfNewer>true</overWriteIfNewer>\n                </configuration>\n              </execution>\n            </executions>\n          </plugin>\n        </plugins>\n      </build>\n      <properties>\n        <hadoop.bin.path>${basedir}/${hive.path.to.root}/testutils/hadoop.cmd</hadoop.bin.path>\n        <!--suppress UnresolvedMavenProperty -->\n        <test.extra.path>;${env.HADOOP_HOME}/bin</test.extra.path>\n        <test.hive.hadoop.classpath>${project.build.directory}/deplibs/*</test.hive.hadoop.classpath>\n        <test.tmp.dir.uri>file:///${test.tmp.dir}</test.tmp.dir.uri>\n        <test.log4j.scheme>file:/</test.log4j.scheme>\n      </properties>\n    </profile>\n    <profile>\n      <id>itests</id>\n      <modules>\n        <module>itests</module>\n      </modules>\n    </profile>\n    <profile>\n      <id>customhbase</id>\n      <activation>\n        <property>\n          <name>hbase.version</name>\n        </property>\n      </activation>\n    </profile>\n    <profile>\n      <id>dist</id>\n      <build>\n        <plugins>\n          <plugin>\n            <groupId>org.cyclonedx</groupId>\n            <artifactId>cyclonedx-maven-plugin</artifactId>\n            <version>${maven.cyclonedx.plugin.version}</version>\n            <executions>\n              <execution>\n                <phase>package</phase>\n                <goals>\n                  <goal>makeBom</goal>\n                </goals>\n              </execution>\n            </executions>\n          </plugin>\n        </plugins>\n      </build>\n    </profile>\n  </profiles>\n</project>\n"
        },
        {
          "name": "ql",
          "type": "tree",
          "content": null
        },
        {
          "name": "serde",
          "type": "tree",
          "content": null
        },
        {
          "name": "service-rpc",
          "type": "tree",
          "content": null
        },
        {
          "name": "service",
          "type": "tree",
          "content": null
        },
        {
          "name": "shims",
          "type": "tree",
          "content": null
        },
        {
          "name": "spotbugs",
          "type": "tree",
          "content": null
        },
        {
          "name": "standalone-metastore",
          "type": "tree",
          "content": null
        },
        {
          "name": "storage-api",
          "type": "tree",
          "content": null
        },
        {
          "name": "streaming",
          "type": "tree",
          "content": null
        },
        {
          "name": "testutils",
          "type": "tree",
          "content": null
        },
        {
          "name": "udf",
          "type": "tree",
          "content": null
        },
        {
          "name": "vector-code-gen",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}