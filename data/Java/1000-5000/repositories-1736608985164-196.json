{
  "metadata": {
    "timestamp": 1736608985164,
    "page": 196,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjIwMA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "zendesk/maxwell",
      "stars": 4066,
      "defaultBranch": "master",
      "files": [
        {
          "name": ".circleci",
          "type": "tree",
          "content": null
        },
        {
          "name": ".dockerignore",
          "type": "blob",
          "size": 0.01171875,
          "content": ".git\ntarget\n"
        },
        {
          "name": ".editorconfig",
          "type": "blob",
          "size": 0.0791015625,
          "content": "[*.java]\ncharset=utf-8\nend_of_line=lf\ninsert_final_newline=true\nindent_style=tab\n"
        },
        {
          "name": ".gitattributes",
          "type": "blob",
          "size": 0.015625,
          "content": "*.min.js binary\n"
        },
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.185546875,
          "content": "data/*.yaml\nconfig/*.yml\n/target\n/config.properties\n/maxwell.position\n/.idea\n/*.iml\n.*.sw*\n.DS_Store\n/test.log\n.sw?\n/config\n.classpath\n.project\n.settings/\n.vscode/\n.factorypath\n.sts4-cache/\n"
        },
        {
          "name": "CHANGELOG.md",
          "type": "blob",
          "size": 53.6123046875,
          "content": "# Maxwell changelog\n\n### [v1.42.2](https://github.com/zendesk/maxwell/releases/tag/v1.42.2)\n\n- update jdk on docker image\n- support rabbitmq + SSL\n- small fixes for latest maria\n- get tests running under mysql 8.4\n- remove Kinesis internal TTL, see #2147 for details\n- support bootstrapping from a replica that doesn't contain the maxwell database\n\n\n\n_Released 2025-01-09_\n\n### [v1.42.1](https://github.com/zendesk/maxwell/releases/tag/v1.42.1)\n\n- bugfix for 1.42.0, mysql 8.0.x and \"SHOW BINARY LOG STATUS\"\n\n\n\n_Released 2024-12-21_\n\n### [v1.42.0](https://github.com/zendesk/maxwell/releases/tag/v1.42.0)\n\n- initial support for mysql 8.4\n- support partitioning for sns and sqs\n- bugfix for maria\n\n\n\n_Released 2024-12-17_\n\n### [v1.41.2](https://github.com/zendesk/maxwell/releases/tag/v1.41.2)\n\n- Owen Derby is the Nick Clarke of Maxwell parser bugs\n\n\n\n_Released 2024-06-05_\n\n### [v1.41.1](https://github.com/zendesk/maxwell/releases/tag/v1.41.1)\n\n- fix 2 parser issues, one mariadb and one \"tablespace\" specific\n- upgrade lz4 dep for security\n\n\n\n_Released 2024-03-24_\n\n### [v1.41.0](https://github.com/zendesk/maxwell/releases/tag/v1.41.0)\n\n- javascript filters are now passed a second, optional dicionary\n  argument which persists between filter invocations.\n\n\n\n_Released 2023-11-30_\n\n### [v1.40.6](https://github.com/zendesk/maxwell/releases/tag/v1.40.6)\n\n- fix 2 parser bugs\n- upgrade jackson for security\n\n\n\n_Released 2023-11-04_\n\n### [v1.40.5](https://github.com/zendesk/maxwell/releases/tag/v1.40.5)\n\n- Fix a bug introduced in v1.40.2 in the kafka producer.\n\n\n\n_Released 2023-09-09_\n\n### [v1.40.4](https://github.com/zendesk/maxwell/releases/tag/v1.40.4)\n\n- add support for mariadb's DROP COLUMN IF EXISTS\n\n\n\n_Released 2023-09-01_\n\n### [v1.40.3](https://github.com/zendesk/maxwell/releases/tag/v1.40.3)\n\n- bugfix for \"rename tables\"\n- bugfix for temporary tables that rollback inside transactions\n- sns+localstack support\n\n\n\n_Released 2023-08-27_\n\n### [v1.40.2](https://github.com/zendesk/maxwell/releases/tag/v1.40.2)\n\n- fix dumb bug in last release\n\n\n\n_Released 2023-06-11_\n\n### [v1.40.0](https://github.com/zendesk/maxwell/releases/tag/v1.40.0)\n\n- add kafka 3.4.0\n- kafka 2.7.0 is now the default kafka library\n- add custom health-check factory jar thing\n\n\n\n_Released 2023-04-02_\n\n### [v1.39.6](https://github.com/zendesk/maxwell/releases/tag/v1.39.6)\n\n- Bugfix issue where SQL query would go missing (#1973)\n- Various parser bugfixes (#1970, #1982, #1987)\n- Fix issue with renaming a primary key column (#1977)\n\n\n\n_Released 2023-03-11_\n\n### [v1.39.5](https://github.com/zendesk/maxwell/releases/tag/v1.39.5)\n\n- a few parser fixes\n\n\n\n_Released 2023-02-08_\n\n### [v1.39.4](https://github.com/zendesk/maxwell/releases/tag/v1.39.4)\n\n- Fix bugs with older versions of mariadb (<10.4)\n\n\n\n_Released 2022-12-07_\n\n### [v1.39.3](https://github.com/zendesk/maxwell/releases/tag/v1.39.3)\n\n- some bugfixes for 1.39.2 and google pubsub\n- couple of security upgrades, including in the docker image\n\n\n\n_Released 2022-12-04_\n\n### [v1.39.2](https://github.com/zendesk/maxwell/releases/tag/v1.39.2)\n\nthis is a bug-fix release.  some upgrades broke maxwell's http interface and there's\na bunch of SQL parser fixes in here.\n\n\n\n_Released 2022-11-02_\n\n### [v1.39.1](https://github.com/zendesk/maxwell/releases/tag/v1.39.1)\n\nThis is a faily major release, including lots of MariaDB support fixes\nand a few months worth of patches.\n\n- GTID support for MariaDB\n- Improved JSON column handling for MariaDB\n- add `--pubsub_message_ordering_key`, thanks Billy Braga\n- add `--pubsub_emulator`, thanks Billy Braga\n- add `--ignore_missing_schema` for otherwise untenable schema situations.\n- handle TABLESPACE related DDL\n\n\n\n_Released 2022-11-02_\n\n### [v1.38.0](https://github.com/zendesk/maxwell/releases/tag/v1.38.0)\n\n- Maxwell gets the ability to talk to bigtable!  I have no idea how well it'll work.  I hope it works for you!\n- upgrade protobuf to fix a rabbitmq issue with booleans, I think.\n- rabbitMQ timeouts on connection\n- other fixes.\n- I can't imagine the security department cares about my naming what with what's going on inside 1019.  I guess we'll see.\n\n\n\n\n_Released 2022-07-29_\n\n### [v1.37.7](https://github.com/zendesk/maxwell/releases/tag/v1.37.7)\n\n - Bump viafoura/metrics-datadog 2.0.0-RC3\n\n\n\n_Released 2022-06-21_\n\n### [v1.37.6](https://github.com/zendesk/maxwell/releases/tag/v1.37.6)\n\n- In non-GTID mode, Verify that the master's server hasn't changed out\n  from underneath us.  thanks Tamin Khan\n\n\n\n_Released 2022-05-12_\n\n### [v1.37.5](https://github.com/zendesk/maxwell/releases/tag/v1.37.5)\n\n- Upgrade binlog-replicator.  pulls in some minor fixes.\n\n\n\n_Released 2022-04-16_\n\n### [v1.37.4](https://github.com/zendesk/maxwell/releases/tag/v1.37.4)\n\n- configure custom producer via environment\n- sns and sqs producers take output config properly\n\n\n\n_Released 2022-04-08_\n\n### [v1.37.3](https://github.com/zendesk/maxwell/releases/tag/v1.37.3)\n\n- fixes for mariadb\n\n\n\n_Released 2022-03-25_\n\n### [v1.37.2](https://github.com/zendesk/maxwell/releases/tag/v1.37.2)\n\n- configurable binlog event queue size\n\n\n\n_Released 2022-03-14_\n\n### [v1.37.1](https://github.com/zendesk/maxwell/releases/tag/v1.37.1)\n\n - upgrade mysql-connector-j \n\n_Released 2022-03-07_\n\n### [v1.37.0](https://github.com/zendesk/maxwell/releases/tag/v1.37.0)\n\n- Change max size of RowMap buffer to unblock high-efficiency producers\n\n\n\n_Released 2022-01-26_\n\n### [v1.36.0](https://github.com/zendesk/maxwell/releases/tag/v1.36.0)\n\n- fix bug where the millionth binlog would kinda sort \"overflow\" and the\n  binlog positions would stop moving.\n- My benefactor here asked that I stopped creating cute release names.\n  The security department, mysteriously.\n\n\n\n_Released 2022-01-23_\n\n### [v1.35.5](https://github.com/zendesk/maxwell/releases/tag/v1.35.5)\n\n- log4j, again and agian.\n\n\n\n_Released 2021-12-29_\n\n### [v1.35.4](https://github.com/zendesk/maxwell/releases/tag/v1.35.4)\n\n- log4j turns 2.17.0, happy birthday\n\n\n\n_Released 2021-12-18_\n\n### [v1.35.3](https://github.com/zendesk/maxwell/releases/tag/v1.35.3)\n\n- log4j vulnerability #2\n\n\n\n_Released 2021-12-15_\n\n### [v1.35.2](https://github.com/zendesk/maxwell/releases/tag/v1.35.2)\n\n- better logging when we can't connect on startup\n\n\n\n_Released 2021-12-12_\n\n### [v1.35.1](https://github.com/zendesk/maxwell/releases/tag/v1.35.1)\n\n- log4j upgrade to upgrade past the giant security hole\n\n\n\n_Released 2021-12-10_\n\n### [v1.35.0](https://github.com/zendesk/maxwell/releases/tag/v1.35.0)\n\n- couple of parser fixes\n- docker builds are now multi-platform\n- replication_reconnection_retries configuration option\n- quote table names in bootstrapper properly\n\n\n\n_Released 2021-11-30_\n\n### [v1.34.1](https://github.com/zendesk/maxwell/releases/tag/v1.34.1)\n\n- support for mysql 8's visible/invisible columns\n- support mariadb's if-exists/if-not-exists for partition management\n- add an index for the http endpoint\n\n\n\n_Released 2021-09-21_\n\n### [v1.34.0](https://github.com/zendesk/maxwell/releases/tag/v1.34.0)\n\n- intern a bunch of objects in our in-memory representation of schema.\n  Saves gobs of memory in cases where one has N copies of the same\n  database.  Note that this changes the API of Columns, should any\n  embedded Maxwell application be using that.\n- go up to BIGINT for maxwell's auto-increment ids\n\n\n\n_Released 2021-07-29_\n\n### [v1.33.1](https://github.com/zendesk/maxwell/releases/tag/v1.33.1)\n\n- properties may now be fetched from a javascript blob in the env\n- RowMap provides access to primary keys\n- fix an odd NPE in mariaDB init\n\n\n\n_Released 2021-06-02_\n\n### [v1.33.0](https://github.com/zendesk/maxwell/releases/tag/v1.33.0)\n\n- Add HTTP endpoint for runtime reconfiguration\n\n\n\n_Released 2021-03-29_\n\n### [v1.32.0](https://github.com/zendesk/maxwell/releases/tag/v1.32.0)\n\n- Amazon SNS producer added, thanks Rober Wittman\n- kafka 2.7.0 supported\n- stackdriver metrics logging available\n\n\n\n_Released 2021-03-17_\n\n### [v1.31.0](https://github.com/zendesk/maxwell/releases/tag/v1.31.0)\n\n- Add producer for NATS streaming server\n\n\n\n_Released 2021-02-11_\n\n### [v1.30.0](https://github.com/zendesk/maxwell/releases/tag/v1.30.0)\n\n- support server-sent heartbeating on the binlog connection via --binlog-heartbeat\n- can connect to rabbitmq by URL, supports SSL connections\n- fix parser bug with multiline SQL\n- target JDK 11 -- we have dropped support for JDK 8\n- ability to send a microsecond timestamp via --output_push_timestamp\n- fixes for odd azure mysql connection failures\n\n\n\n_Released 2021-02-05_\n\n### [v1.29.2](https://github.com/zendesk/maxwell/releases/tag/v1.29.2)\n\n- fix for terrible performance regression in bootstrapping\n\n\n\n_Released 2021-01-27_\n\n### [v1.29.1](https://github.com/zendesk/maxwell/releases/tag/v1.29.1)\n\n- small bugfix release, fixes binlog event type processing in mysql 8\n\n\n\n_Released 2020-12-23_\n\n### [v1.29.0](https://github.com/zendesk/maxwell/releases/tag/v1.29.0)\n\n- High Availability support via jgroups-raft\n- rework --help text\n\n\n\n_Released 2020-12-15_\n\n### [v1.28.2](https://github.com/zendesk/maxwell/releases/tag/v1.28.2)\n\n- fix for encryption parsing error on table creation\n- some logging around memory usage in RowMapBuffer\n\n\n\n_Released 2020-12-02_\n\n### [v1.28.1](https://github.com/zendesk/maxwell/releases/tag/v1.28.1)\n\n- fix http server issue in 1.28.0\n\n\n\n_Released 2020-11-25_\n\n### [v1.28.0](https://github.com/zendesk/maxwell/releases/tag/v1.28.0)\n\n- schema compaction!  with the new --max_schemas option, maxwell will\n  periodically roll up the `maxwell`.`schemas` table, preventing it from\n  growing infinitely long.\n- fix metricsAgeSloMS calculation\n- support SRID columns\n- fix parsing of complex INDEX(CAST()) statements\n- various dependency bumps\n\n\n\n_Released 2020-11-19_\n\n### [v1.27.1](https://github.com/zendesk/maxwell/releases/tag/v1.27.1)\n\n- redis producer gets sentinal support\n- fix a double-reconnect race condition\n- file producer honors javascript row-suppression\n- better error messaging when we lack REPLICATION SLAVE privs\n- miscellaneous dependency bumps\n\n\n\n_Released 2020-08-07_\n\n### [v1.27.0](https://github.com/zendesk/maxwell/releases/tag/v1.27.0)\n\n- better support for empty/null passwords\n- allow bootstrap utility to query replication_host\n- a few library upgrades, notably pubsub and kinesis library\n- bootstrap connection uses jdbc_options properly\n- add logging for when we hit out of sync schema exceptions\n- allow for partitioning by thread_id, thx @gogov\n- fresh and clean documentation\n\n\n\n_Released 2020-06-30_\n\n### [v1.26.4](https://github.com/zendesk/maxwell/releases/tag/v1.26.4)\n\n - support now() function with precision\n\n\n\n_Released 2020-06-08_\n\n### [v1.26.3](https://github.com/zendesk/maxwell/releases/tag/v1.26.3)\n\n- use pooled redis connections, fixes corruption when redis was accessed\nfrom multiple threads (bootstrap/producer), thanks @lucastex\n- fix date handling of '0000-01-01'\n- fix race condition in binlog reconnect logic\n\n\n\n_Released 2020-05-26_\n\n### [v1.26.2](https://github.com/zendesk/maxwell/releases/tag/v1.26.2)\n\n- bootstraps can be scheduled in the future by setting the `started_at`\n  column, thanks @lucastex\n- two mysql 8 fixes; one for a `DEFAULT(function())` parse error, one\n  for supporting DEFAULT ENCRYPTION\n\n\n\n_Released 2020-05-18_\n\n### [v1.26.1](https://github.com/zendesk/maxwell/releases/tag/v1.26.1)\n\n- fixes for redis re-connection login, thanks much @lucastex\n\n\n\n_Released 2020-05-07_\n\n### [v1.26.0](https://github.com/zendesk/maxwell/releases/tag/v1.26.0)\n\n- We now support mysql 8's caching_sha2_password authentication scheme\n- support for converting JSON field names to camelCase\n\n\n\n_Released 2020-05-06_\n\n### [v1.25.3](https://github.com/zendesk/maxwell/releases/tag/v1.25.3)\n\n- fixes memory leak in mysql-binlog-connector\n- fixes exceptions that occur when a connection passes wait_timeout\n\n\n\n_Released 2020-05-02_\n\n### [v1.25.2](https://github.com/zendesk/maxwell/releases/tag/v1.25.2)\n\n- Fixes for a long standing JSON bug in 8.0.19+\n\n\n\n_Released 2020-05-01_\n\n### [v1.25.1](https://github.com/zendesk/maxwell/releases/tag/v1.25.1)\n\n- issue #1457, ALTER DATABASE with implicit database name\n- maxwell now runs on JDK 11 in docker\n- exit with status 2 when we can't find binlog files\n\n\n\n_Released 2020-04-22_\n\n### [v1.25.0](https://github.com/zendesk/maxwell/releases/tag/v1.25.0)\n\n- swap un-maintained snaq.db with C3P0.\n- support eu datadog metrics\n- protect against lost connections during key queries (bootstrapping,\n      heartbeats, postition setting)\n\n\n\n_Released 2020-03-29_\n\n### [v1.24.2](https://github.com/zendesk/maxwell/releases/tag/v1.24.2)\n\n- bugfix parsing errors: compressed columns, exchange partitions,\n  parenthesis-enclosed default values, `drop column foo.t`.\n- add partition-by-random feature.\n- update jackson-databind to get security patch\n- fix redis channel interpolation on RPUSH\n\n\n\n_Released 2020-03-25_\n\n### [v1.24.1](https://github.com/zendesk/maxwell/releases/tag/v1.24.1)\n\n- allow jdbc_options on secondary connections\n- fix a crash in bootstrapping / javascript filters\n- fix a regression in message.publish.age metric\n\n\n\n_Released 2020-01-21_\n\n### [v1.24.0](https://github.com/zendesk/maxwell/releases/tag/v1.24.0)\n\n - add comments field to bootstrapping, thanks Tom Collins\n - fix sql bug with #comments style comments\n\n\n\n_Released 2019-12-14_\n\n### [v1.23.5](https://github.com/zendesk/maxwell/releases/tag/v1.23.5)\n\n - Update bootstrap documentation\n - Bump drop wizard metrics to support Java versions 10+\n\n\n\n_Released 2019-12-12_\n\n### [v1.23.4](https://github.com/zendesk/maxwell/releases/tag/v1.23.4)\n\n- Bump and override dependencies to fix security vulnerabilities.\n- Update redis-key config options\n\n - list changes\n\n\n\n_Released 2019-12-03_\n\n### [v1.23.3](https://github.com/zendesk/maxwell/releases/tag/v1.23.3)\n\n- pubsubDelayMultiplier may now be 1.0\n- allow %{database} and %{topic} interpolation into redis producer\n- docs updates\n- setup default client_id in maxwell-bootstrap util\n\n\n\n_Released 2019-11-21_\n\n### [v1.23.2](https://github.com/zendesk/maxwell/releases/tag/v1.23.2)\n\n- upgrade jackson\n- stop passing maxwell rows through the JS filter.  too dangerous.\n\n\n\n_Released 2019-10-18_\n\n### [v1.23.1](https://github.com/zendesk/maxwell/releases/tag/v1.23.1)\n\n- Add option for XADD (redis streams) operation\n- Add configuration flag for tuning transaction buffer memory\n- sectionalize help text\n\n\n\n_Released 2019-10-12_\n\n### [v1.23.0](https://github.com/zendesk/maxwell/releases/tag/v1.23.0)\n\n- Added AWS FIFO support\n- Add retry and batch settings to pubs producer\n- Add support for age SLO metrics\n\n\n\n_Released 2019-10-08_\n\n### [v1.22.6](https://github.com/zendesk/maxwell/releases/tag/v1.22.6)\n\n- upgrade mysql-connector-java to 8.0.17\n- use a newer docker image as base\n - list changes\n\n\n\n_Released 2019-09-20_\n\n### [v1.22.5](https://github.com/zendesk/maxwell/releases/tag/v1.22.5)\n\n- bugfix for bootstrapping off a split replica that doesn't contain a\n  \"maxwell\" database\n- Fix a parser issue with db.table.column style column names\n\n\n\n_Released 2019-09-06_\n\n### [v1.22.4](https://github.com/zendesk/maxwell/releases/tag/v1.22.4)\n\n - Add row type to fallback message\n - Upgrade jackson-databind\n\n\n\n_Released 2019-08-23_\n\n### [v1.22.3](https://github.com/zendesk/maxwell/releases/tag/v1.22.3)\n\n- fix issue with google pubsub in 1.22.2\n\n\n\n_Released 2019-06-20_\n\n### [v1.22.2](https://github.com/zendesk/maxwell/releases/tag/v1.22.2)\n\n- fix an issue with bootstrapping-on-replicas\n- add --output_primary_keys and --output_primary_key_columns\n- fix a very minor memory leak with blacklists\n\n\n\n_Released 2019-06-18_\n\n### [v1.22.1](https://github.com/zendesk/maxwell/releases/tag/v1.22.1)\n\n- fix crash in rabbit-mq producer\n- better support for maxwell + azure-mysql\n- remove bogus different-host bootstrap check\n- some security upgrades\n\n\n\n_Released 2019-05-28_\n\n### [v1.22.0](https://github.com/zendesk/maxwell/releases/tag/v1.22.0)\n\n- Bootstrapping has been reworked and is now available in all setups,\nincluding those in which the maxwell store is split from the replicator.\n- cleanup and fix a deadlock in the kafka fallback queue logic\n- add .partition_string = to javascript filters\n\n\n\n_Released 2019-04-16_\n\n### [v1.21.1](https://github.com/zendesk/maxwell/releases/tag/v1.21.1)\n\n- Upgrade binlog connector.  Should fix issues around deserialization\nerrors.\n\n\n\n_Released 2019-03-29_\n\n### [v1.21.0](https://github.com/zendesk/maxwell/releases/tag/v1.21.0)\n\n- Bootstrapping output no longer contain binlog positions.  Please update\n  any code that relies on this.\n- Fix 3 parser issues.\n\n\n\n_Released 2019-03-23_\n\n### [v1.20.0](https://github.com/zendesk/maxwell/releases/tag/v1.20.0)\n\n- add support for partitioning by transaction ID thx @hexene\n- add support for a kafka \"fallback\" topic to write to\n  when a message fails to write\n- add UJIS charset support\n- parser bug: multiple strings concatenate to make one default string\n- parser bug: deal with bizarre column renames which are then referenced\n  in AFTER column statements\n\n\n\n_Released 2019-02-28_\n\n### [v1.19.7](https://github.com/zendesk/maxwell/releases/tag/v1.19.7)\n\n- fix a parser error with empty sql comments\n- interpret latin-1 as windows-1252, not iso-whatever, thx @borleaandrei\n\n\n\n_Released 2019-01-25_\n\n### [v1.19.6](https://github.com/zendesk/maxwell/releases/tag/v1.19.6)\n\n- Further fixes for GTID-reconnection issues.\n- Crash sanely when GTID-enabled maxwell is connected to clearly the wrong master,\n  thanks @acampoh\n\n\n\n_Released 2019-01-20_\n\n### [v1.19.5](https://github.com/zendesk/maxwell/releases/tag/v1.19.5)\n\n- Fixes for unreliable connections wrt to GTID events; previously we\n  restart in any old position, now we throw away the current transaction\n  and restart the replicator again at the head of the GTID event.\n\n\n\n_Released 2019-01-15_\n\n### [v1.19.4](https://github.com/zendesk/maxwell/releases/tag/v1.19.4)\n\n- Fixes for a maxwell database not making it through the blacklist\n- Add `output_null_zerodates` parameter to control how we treat\n  '0000-00-00'\n\n\n\n_Released 2019-01-12_\n\n### [v1.19.3](https://github.com/zendesk/maxwell/releases/tag/v1.19.3)\n\n- Add a universal backpressure mechanism.  This should help people who\nwere running into out-of-memory situations while bootstrapping.\n\n\n\n_Released 2018-12-19_\n\n### [v1.19.2](https://github.com/zendesk/maxwell/releases/tag/v1.19.2)\n\n- Include schema_id in bootstrap events\n- add more logging around binlog connector losing connection\n- add retry logic to redis\n- some aws fixes\n- allow pushing JS hashes/arrays into data from js filters\n\n - list changes\n\n\n\n_Released 2018-12-02_\n\n### [v1.19.1](https://github.com/zendesk/maxwell/releases/tag/v1.19.1)\n\n- Handle mysql bit literals in DEFAULT statements\n- blacklist out CREATE ROLE etc\n- upgrade dependencies to pick up security issues\n\n\n\n_Released 2018-11-12_\n\n### [v1.19.0](https://github.com/zendesk/maxwell/releases/tag/v1.19.0)\n\n- mysql 8 support!\n- utf8 enum values are supported now\n- fix #1125, bootstrapping issue for TINYINT(1)\n- fix #1145, nasty bug around SQL blacklist and columns starting with \"begin\"\n- only resume bootstraps that are targeted at this client_id\n- fixes for blacklists and heartbeats.  Did I ever mention blacklists\n  are a terrible idea?\n\n\n\n_Released 2018-10-27_\n\n### [v1.18.0](https://github.com/zendesk/maxwell/releases/tag/v1.18.0)\n\n- memory optimizations for large schemas (especially shareded schemas with lots of duplicates)\n- add support for an http endpoint to support Prometheus metrics\n- allow javascript filters to access the row query object\n- javascript filters now run in the bootstrap process\n- support for non-latin1 column names\n- add `--output_schema_id` option\n- better handling of packet-too-big errors from Kinesis\n- add message.publish.age metric\n\n\n\n_Released 2018-09-15_\n\n### [v1.17.1](https://github.com/zendesk/maxwell/releases/tag/v1.17.1)\n\n- fix a regression around filters + bootstrapping\n- fix a regression around filters + database-only-ddl\n\n\n\n_Released 2018-07-03_\n\n### [v1.17.0](https://github.com/zendesk/maxwell/releases/tag/v1.17.0)\n\nv1.17.0 brings a new level of configurability by allowing you to inject\na bit of javascript into maxwell's processing.  Should be useful!  Also:\n\n- fix regression for Alibaba RDS tables\n\n\n\n_Released 2018-06-28_\n\n### [v1.16.1](https://github.com/zendesk/maxwell/releases/tag/v1.16.1)\n\n- Fix Bootstrapping for JSON columns\n- add --recapture_schema flag for when ya wanna start over\n- add kafka 1.0 libraries, make them default\n\n\n\n_Released 2018-06-21_\n\n### [v1.16.0](https://github.com/zendesk/maxwell/releases/tag/v1.16.0)\n\nv1.16.0 brings a rewrite of Maxwell's filtering system, giving it a\nconcise list of rules that are executed in sequence.  It's now possible\nto exclude tables from a particular database, exclude columns matching a\nvalue, and probably some other use cases.\nSee http://maxwells-daemon.io/config/#filtering for details.\n\n\n\n_Released 2018-06-15_\n\n### [v1.15.0](https://github.com/zendesk/maxwell/releases/tag/v1.15.0)\n\nThis is a bug-fix release, but it's big enough I'm giving it a minor\nversion.\n\n- Fix a very old bug in which DDL rows were writing the *start* of the\nrow into `maxwell.positions`, leading to chaos in some scenarios where\nmaxwell managed to stop on the row and double-process it, as well as to\na few well-meaning patches.\n- Fix the fact that maxwell was outputting \"next-position\" instead of\n\"position\" of a row into JSON.\n- Fix the master-recovery code to store schema that corresponds to the\nstart of a row, and points the replicator at the next-position.\n\nMuch thanks to Tim, Likun and others in sorting this mess out.\n\n\n\n_Released 2018-06-13_\n\n### [v1.14.7](https://github.com/zendesk/maxwell/releases/tag/v1.14.7)\n\n- add RowMap#getRowQuery, thx @saimon7\n- revert alpine-linux docker image fiasco\n- fix RawJSONString not serializable, thx @niuhaifeng\n\n\n\n_Released 2018-06-03_\n\n### [v1.14.6](https://github.com/zendesk/maxwell/releases/tag/v1.14.6)\n\n- Fix docker image\n\n\n\n_Released 2018-05-15_\n\n### [v1.14.5](https://github.com/zendesk/maxwell/releases/tag/v1.14.5)\n\n- reduce docker image footprint\n- add benchmarking framework\n- performance improvements for date/datetime columns\n- fix parser error on UPGRADE PARTITIONING\n\n\n\n_Released 2018-05-15_\n\n### [v1.14.4](https://github.com/zendesk/maxwell/releases/tag/v1.14.4)\n\n - Fix race condition in SchemaCapturer\n\n\n\n_Released 2018-05-07_\n\n### [v1.14.3](https://github.com/zendesk/maxwell/releases/tag/v1.14.3)\n\n- Enable jvm metrics\n\n_Released 2018-05-04_\n\n### [v1.14.2](https://github.com/zendesk/maxwell/releases/tag/v1.14.2)\n\n- fix regression in 1.14.1 around bootstrapping host detection\n- fix heartbeating code around table includes\n\n\n\n_Released 2018-05-02_\n\n### [v1.14.1](https://github.com/zendesk/maxwell/releases/tag/v1.14.1)\n\n- bootstraps can now take a client_id\n- improved config validation for embedded mode\n\n\n\n_Released 2018-05-01_\n\n### [v1.14.0](https://github.com/zendesk/maxwell/releases/tag/v1.14.0)\n\n- new feature `--output_xoffset` to uniquely identify rows within transactions,\n  thx Jens Gyti\n- Bug fixes around \"0000-00-00\" times.\n- Bug fixes around dates pre 1000 AD\n\n\n\n_Released 2018-04-24_\n\n### [v1.13.5](https://github.com/zendesk/maxwell/releases/tag/v1.13.5)\n\n- Support environment variable based configuration\n\n_Released 2018-04-11_\n\n### [v1.13.4](https://github.com/zendesk/maxwell/releases/tag/v1.13.4)\n\n- Added possibility to do not declare the rabbitmq exchange.\n\n_Released 2018-04-03_\n\n### [v1.13.3](https://github.com/zendesk/maxwell/releases/tag/v1.13.3)\n\n\n - Add logging for binlog errors\n - Maven warning fix\n - Do not include current position DDL schema to avoid processing DDL twice\n - Always write null fields in primary key fields\n - Bugfix: fix http_path_prefix command line option issue\n\n_Released 2018-04-03_\n\n### [v1.13.2](https://github.com/zendesk/maxwell/releases/tag/v1.13.2)\n\n- fix a bug with CHARACTER SET = DEFAULT\n- maxwell now eclipse-friendly.\n- configurable bind-address for maxwell's http server\n\n\n\n_Released 2018-03-06_\n\n### [v1.13.1](https://github.com/zendesk/maxwell/releases/tag/v1.13.1)\n\n- redis producer now supports LPUSH, thx @m-denton\n- RowMap can now contain artbitrary attributes for embedded maxwell, thx @jkgeyti\n- bugfix: fix jdbc option parsing when value contains `=`\n- bugfix: apparently the SQS producer was disabled\n- bugfix: fix a situation where adding a second client could cause\n  schemas to become out of sync\n- support for --daemon\n\n\n\n_Released 2018-02-20_\n\n### [v1.13.0](https://github.com/zendesk/maxwell/releases/tag/v1.13.0)\n\n- proper SSL connection support, thanks @cadams5\n- support for including original SQL in insert/update/deletes, thanks @saimon7\n- fixes for float4, float8 and other non-mysql datatypes\n- bump kinesis lib to 0.12.8\n- fix for bug when two databases share a single table\n\n\n\n_Released 2018-02-01_\n\n### [v1.12.0](https://github.com/zendesk/maxwell/releases/tag/v1.12.0)\n\n- Support for injecting a custom producer, thanks @tomcollinsproject\n- New producer for Amazon SQS, thanks @vikrant2mahajan\n- Maxwell can now filter rows based on column values, thanks @finnplay\n- Fixes for the Google Pubsub producer (it was really broken), thanks @finnplay\n- DDL output can now optionally include the source SQL, thanks @sungjuly\n- Support for double-quoted table/database/etc names\n- rabbitmq option for persistent messages, thanks @d-babiak\n- SQL parser bugfix for values like +1.234, thanks @hexene\n\n\n\n_Released 2018-01-09_\n\n### [v1.11.0](https://github.com/zendesk/maxwell/releases/tag/v1.11.0)\n\n     - default kafka client upgrades to 0.11.0.1\n     - fix the encryption issue (https://github.com/zendesk/maxwell/issues/803)\n\n\n\n_Released 2017-11-22_\n\n### [v1.10.9](https://github.com/zendesk/maxwell/releases/tag/v1.10.9)\n\nWe recommend all v1.10.7 and v1.10.8 users upgrade to v1.10.9.\n\n - Add missing Kafka clients\n - Listen and report on binlog connector lifecycle events for better visibility\n - Reduce docker image size\n\n\n\n_Released 2017-10-30_\n\n### [v1.10.8](https://github.com/zendesk/maxwell/releases/tag/v1.10.8)\n\n - Fix docker builds\n - Add Google Cloud Pub/Sub producer\n - RabbitMQ producer enhancements\n\n\n\n_Released 2017-10-12_\n\n### [v1.10.7](https://github.com/zendesk/maxwell/releases/tag/v1.10.7)\n\n- Java 8 upgrade\n- Diagnostic health check endpoint\n- Encryption\n- Documentation update: encryption, kinesis producer, schema storage fundamentals, etc.\n\n\n_Released 2017-10-11_\n\n### [v1.10.6](https://github.com/zendesk/maxwell/releases/tag/v1.10.6)\n\n - Binlog-connector upgrade\n - Bug-fix: when using literal string for an option that accepts Regex, Regex characters are no longer special\n - If master recovery is enabled, Maxwell cleans up old positions for the same server and client id\n\n\n\n_Released 2017-08-14_\n\n### [v1.10.5](https://github.com/zendesk/maxwell/releases/tag/v1.10.5)\n\n- Shyko's binlog-connector is now the default and only replication\nbackend available for maxwell.\n\n\n\n_Released 2017-07-25_\n\n### [v1.10.4](https://github.com/zendesk/maxwell/releases/tag/v1.10.4)\n\nNotable changes:\n\n - Shutdown hardening. If maxwell can't shut down (because the kafka\n   producer is in a bad state and `close()` never terminates, for example),\n   it would previously stall and process no messages. Now, shutdown is run\n   in a separate thread and there is an additional watchdog thread which\n   forcibly kills the maxwell process if it can't shut down within 10\n   seconds.\n - Initial support for running maxwell from java, rather then as its own\n   process. This mode of operation is still experimental, but we'll\n   accept PRs to improve it (thanks Geoff Lywood).\n - Fix incorrect handling of negative (pre-epoch dates) when using\n   binlog_connector mode (thanks Geoff Lywood).\n\n\n\n_Released 2017-07-10_\n\n### [v1.10.3](https://github.com/zendesk/maxwell/releases/tag/v1.10.3)\n\n - tiny release to fix a units error in the `replication.lag` metric\n   (subtracting seconds from milliseconds)\n\n\n\n_Released 2017-06-06_\n\n### [v1.10.2](https://github.com/zendesk/maxwell/releases/tag/v1.10.2)\n\n- added metrics: \"replication.queue.time\" and \"inflightmessages.count\"\n- renamed \"time.overall\" metric to \"message.publish.time\"\n- documentation updates (thanks Chintan Tank)\n\n\n\n_Released 2017-06-04_\n\n### [v1.10.1](https://github.com/zendesk/maxwell/releases/tag/v1.10.1)\n\nThe observable changes in this minor release are a new configuration for Kafka/Kinesis producer to abort processing on publish errors, and support of Kafka 0.10.2. Also a bunch of good refactoring has been done for heartbeat processing. List of changes:   \n\n- Support Kafka 0.10.2   \n- Stop procesing RDS hearbeats   \n- Keep maxwell heartbeat going every 10 seconds when database is quiet   \n- Allow for empty double-quoted string literals for database schema changes   \n- Ignore Kafka/Kinesis producer errors based on new configuration ignore_producer_error\n\n_Released 2017-05-26_\n\n### [v1.10.0](https://github.com/zendesk/maxwell/releases/tag/v1.10.0)\n\nThis is a small release, primarily around a change to how schemas are\nstored. Maxwell now stores the `last_heartbeat_read` with each entry\nin the `schemas` table, making schema management more resilient to\ncases where binlog numbers are reused, but means that you must take\ncare if you need to roll back to an earlier version. If you deploy\nv1.10.0, then roll back to an earlier version, you should manually\nupdate all `schemas`.`last_heartbeat_read` values to `0` before\nredeploying v1.10.0 or higher.\n\nOther minor changes:\n\n  - allow negative default numbers in columns\n  - only store final binlog position if it has changed\n  - blacklist internal aurora table `rds_heartbeat*'\n  - log4j version bump (allows for one entry per line JSON logging)\n\n\n\n_Released 2017-05-09_\n\n### [v1.9.0](https://github.com/zendesk/maxwell/releases/tag/v1.9.0)\n\nMaxwell 1.9 adds one main feature: monitoring support, contributed by\nScott Ferguson. Multiple backends can be configured, read the updated\ndocs for full details.\n\nThere's also some bugfixes:\n\n- filter DDL messages based on config\n- determine newest schema from binlog order, not creation order\n- add task manager to shutdown cleanly on error\n- minor logging improvements\n\n\n\n_Released 2017-04-26_\n\n### [v1.8.2](https://github.com/zendesk/maxwell/releases/tag/v1.8.2)\n\nBugfix release.\n\n- maxwell would crash on a quoted partition name\n- fixes for alters on non-string tables containing VARCHAR\n- use seconds instead of milliseconds for DDL messages\n\n\n\n_Released 2017-04-11_\n\n### [v1.8.1](https://github.com/zendesk/maxwell/releases/tag/v1.8.1)\n\n- performance improves in capturing and restoring schema, thx Joren\n  Minnaert\n- Allow for capturing from a separate mysql host (adds support for using\n  Maxscale as a replication proxy), thx Adam Szkoda\n\n\n_Released 2017-02-20_\n\n### [v1.8.0](https://github.com/zendesk/maxwell/releases/tag/v1.8.0)\n\nIn version 1.8.0 Maxwell gains alpha support for GTID-based positions!\nAll praise due to Henry Cai.\n\n\n_Released 2017-02-14_\n\n### [v1.7.2](https://github.com/zendesk/maxwell/releases/tag/v1.7.2)\n\n- Fix a bug found where maxwell could cache the wrong TABLE_MAP_ID for a\n  binlog event, leading to crashes or in some cases data mismatches.\n\n\n_Released 2017-01-30_\n\n### [v1.7.1](https://github.com/zendesk/maxwell/releases/tag/v1.7.1)\n\n- bootstrapping now can take a `--where` clause\n- performance improvements in the kafka producer\n\n\n_Released 2017-01-24_\n\n### [v1.7.0](https://github.com/zendesk/maxwell/releases/tag/v1.7.0)\n\nMaxwell 1.7 brings 2 major new, alpha features.  The first is Mysql 5.7\nsupport, including JSON column type support and handling of 5.7 SQL, but\n_not_ including GTID support yet.  This is based on porting Maxwell to\nStanley Shyko's binlog-connector library.  Thanks to Stanley for his\namazing support doing this port.\n\nThe second major new feature is a producer for Amazon's Kinesis streams,\nThis was contributed in full by the dogged and persistent Thomas Dziedzic.\nCheck it out with `--producer=kinesis`.\n\nThere's also some bugfixes:\n- Amazon RDS heartbeat events now tick maxwell's position, thx Scott Ferguson\n- allow CHECK() statements inside column definitions\n\n\n_Released 2017-01-07_\n\n### [v1.6.0](https://github.com/zendesk/maxwell/releases/tag/v1.6.0)\n\nThis is mostly a bugfix release, but it gets a minor version bump due to\na single change of behavior: dates and timestamps which mysql may\naccept, but are considered invalid (0000-00-00 is a notable example)\npreviously had inconsistent behavior.  Now we convert these to NULL.\nOther bugfixes:\n- heartbeats have moved into their own table\n- more fixes around alibaba rds\n- ignore DELETE statements that are output for MEMORY tables upon server\n  restart\n- allow pointing maxwell to a pre-existing database\n\n\n_Released 2016-12-29_\n\n### [v1.5.2](https://github.com/zendesk/maxwell/releases/tag/v1.5.2)\n\n- add support for kafka 0.10.1 @ smferguson\n- master recovery: cleanup positions from previous master; prevent\n  errors on flip-back.\n- fix a bug that would trigger in certain cases when dropping a column\n  that was part of the primary-key\n\n\n_Released 2016-12-07_\n\n### [v1.5.1](https://github.com/zendesk/maxwell/releases/tag/v1.5.1)\n\nThis is a bugfix release.\n- fixes for bootstrapping with an alternative maxwell-schema name and an\n  `include_database` filter, thanks Lucian Jones\n- fixes for kafka 0.10 with lz4 compression, thanks Scott Ferguson\n- ignore the RDS table `mysql.ha_health_check` table\n- Get the bootstrapping process to output NULL values.\n- fix a quoting issue in the bootstrap code, thanks @mylesjao.\n\n\n_Released 2016-11-24_\n\n### [v1.5.0](https://github.com/zendesk/maxwell/releases/tag/v1.5.0)\n\n- CHANGE: Kafka producer no longer ships with hard-coded defaults.\n  Please ensure you have \"compression.type\", \"metadata.fetch.timeout.ms\", and \"retries\"\n  configured to your liking.\n- bugfix: fix a regression in handling `ALTER TABLE change c int after b` statements\n- warn on servers with missing server_id\n\n\n_Released 2016-11-07_\n\n### [v1.4.2](https://github.com/zendesk/maxwell/releases/tag/v1.4.2)\n\n- kafka 0.10.0 support, as well as a re-working of the --kafka_version\n  command line option.\n\n\n_Released 2016-11-01_\n\n### [v1.4.1](https://github.com/zendesk/maxwell/releases/tag/v1.4.1)\n\n- support per-table topics, Thanks @smferguson and @sschatts.\n- fix a parser issue with DROP COLUMN CASCADE, thanks @smferguson\n\n\n_Released 2016-10-27_\n\n### [v1.4.0](https://github.com/zendesk/maxwell/releases/tag/v1.4.0)\n\n1.4.0 brings us two nice new features:\n- partition-by-column: see --kafka_partition_columns.  Thanks @smferguson\n- output schema changes as JSON: see --output_ddl.  Thanks @xmlking\n- As well as a fix around race conditions on shutdown.\n\n\n_Released 2016-10-21_\n\n### [v1.3.0](https://github.com/zendesk/maxwell/releases/tag/v1.3.0)\n\n- support for fractional DATETIME, TIME, TIMESTAMP columns, thanks @Dagnan\n- support for outputting server_id & thread_id, thanks @sagiba\n- fix a race condition in bootstrap support\n\n\n_Released 2016-10-03_\n\n### [v1.2.2](https://github.com/zendesk/maxwell/releases/tag/v1.2.2)\n\n- Maxwell will now include by default fields with NULL values (as null\n  fields).  To disable this and restore the old functionality where fields\n  were omitted, pass `--output_nulls=false`\n- Fix an issue with multi-client support where two replicators would\n  ping-pong heartbeats at each other\n- Fix an issue where a client would attempt to recover a position from a\n  mismatched client_id\n- Fix a bug when using CHANGE COLUMN on a primary key\n\n\n_Released 2016-09-23_\n\n### [v1.2.1](https://github.com/zendesk/maxwell/releases/tag/v1.2.1)\n\nThis is a bugfix release.\n- fix a parser bug around ALTER TABLE CHARACTER SET\n- fix bin/maxwell to pull in the proper version of the kafka-clients\n  library\n\n\n_Released 2016-09-15_\n\n### [v1.2.0](https://github.com/zendesk/maxwell/releases/tag/v1.2.0)\n\n1.2.0 is a major release of Maxwell that introduces master recovery\nfeatures; when a slave is promoted to master, Maxwell is now capable of\nrecovering the position.  See the `--master_recovery` flag for more\ndetails.\n\nIt also upgrades the kafka producer library to 0.9.  If you're using\nmaxwell with a kafka 0.8 server, you must now pass the `--kafka0.8` flag\nto maxwell.\n\n\n_Released 2016-09-12_\n\n### [v1.1.6](https://github.com/zendesk/maxwell/releases/tag/v1.1.6)\n\n- minor bugfix in which maxwell with --replay mode was trying to write\n  heartbeats\n\n\n_Released 2016-09-07_\n\n### [v1.1.5](https://github.com/zendesk/maxwell/releases/tag/v1.1.5)\n\n- @dadah89 adds --output_binlog_position to optionally output the\n  position with the row\n- @dadah89 adds --output_commit_info to turn off xid/commit fields\n- maxwell now supports tables with partitions\n- maxwell now supports N maxwells per-server.  see the client_id /\n  replica_server_id options.\n- two parser fixes, for engine=`innodb` and CHARSET ASCII\n- lay the ground work for doing master recovery; we add a heartbeat into\n  the positions table that we can co-ordinate around.\n\n\n_Released 2016-09-04_\n\n### [v1.1.4](https://github.com/zendesk/maxwell/releases/tag/v1.1.4)\n\n- add support for a bunch more charsets (gbk, big5, notably)\n- fix Maxwell's handling of kafka errors - previously we were trying to\n  crash Maxwell by throwing a RuntimeException out of the Kafka\n  Producer, but this was a failure.  Now we log and skip all errors.\n\n\n_Released 2016-08-05_\n\n### [v1.1.3](https://github.com/zendesk/maxwell/releases/tag/v1.1.3)\n\nThis is a bugfix release, which fixes:\n- https://github.com/zendesk/maxwell/issues/376, a problem parsing\n  RENAME INDEX\n- https://github.com/zendesk/maxwell/issues/371, a problem with the\n  SERIAL datatype\n- https://github.com/zendesk/maxwell/issues/362, we now preserve the\n  original casing of columns\n- https://github.com/zendesk/maxwell/issues/373, we were incorrectly\n  expecting heartbeats to work under 5.1\n\n\n_Released 2016-07-14_\n\n### [v1.1.2](https://github.com/zendesk/maxwell/releases/tag/v1.1.2)\n\n- pick up latest mysql-connector-j, fixes #369\n- fix an issue where maxwell could skip ahead positions if a leader failed.\n- rework buffering code to be much kinder to the GC and JVM heap in case\n  of very large transactions / rows inside transactions\n- kinder, gentler help text when you specify an option incorrectly\n\n\n_Released 2016-06-27_\n\n### [v1.1.1](https://github.com/zendesk/maxwell/releases/tag/v1.1.1)\n\n- fixes a race condition setting the binlog position that would get\n  maxwell stuck\n\n\n_Released 2016-05-23_\n\n### [v1.1.0](https://github.com/zendesk/maxwell/releases/tag/v1.1.0)\n\n- much more efficient processing of schema updates storage, especially when dealing with large schemas.\n- @lileeyao added --exclude-columns and the --jdbc_options features\n- @lileeyao added --jdbc_options\n- can now blacklist entire databases\n- new kafka key format available, using a JSON array instead of an object\n- bugfix: unsigned integer columns were captured incorrectly.  1.1 will\n  recapture the schema and attempt to correct the error.\n\n\n_Released 2016-05-20_\n\n### [v1.1.0-pre4](https://github.com/zendesk/maxwell/releases/tag/v1.1.0-pre4)\n\n- Eddie McLean gives some helpful patches around bootstrapping\n- Bugfixes for the patch-up-the-schema code around unsigned ints\n\n\n_Released 2016-05-06_\n\n### [v1.1.0-pre3](https://github.com/zendesk/maxwell/releases/tag/v1.1.0-pre3)\n\n- forgot to include some updates that back-patch unsigned column\n  problems\n\n\n_Released 2016-05-05_\n\n### [v1.1.0-pre2](https://github.com/zendesk/maxwell/releases/tag/v1.1.0-pre2)\n\n- fix performance issues when capturing schema in AWS Aurora\n- fix a bug in capturing unsigned integer columns\n\n\n_Released 2016-05-04_\n\n### [v1.0.1](https://github.com/zendesk/maxwell/releases/tag/v1.0.1)\n\n- fixes a parsing bug with `CURRENT_TIMESTAMP()`\n\n\n_Released 2016-04-12_\n\n### [v1.0.0](https://github.com/zendesk/maxwell/releases/tag/v1.0.0)\n\nSince v0.17.0, Maxwell has gotten:\n- bootstrapping support\n- blacklisting for tables\n- flexible kafka partitioning\n- replication heartbeats\n- GEOMETRY columns\n- a whole lotta lotta bugfixes\n\nand I, Osheroff, think the damn thing is stable enough for a 1.0.  So\nthere.\n\n\n_Released 2016-03-11_\n\n### [v1.0.0-RC3](https://github.com/zendesk/maxwell/releases/tag/v1.0.0-RC3)\n\npull in support for replication heartbeats.  helps in the flakier\nnetwork environs.\n\n\n_Released 2016-03-08_\n\n### [v1.0.0-RC2](https://github.com/zendesk/maxwell/releases/tag/v1.0.0-RC2)\n\n- fixes the way ALTER DATABASE charset= was handled\n- adds proper handling of ALTER TABLE CONVERT TO CHARSET\n\n\n_Released 2016-02-20_\n\n### [v1.0.0-RC1](https://github.com/zendesk/maxwell/releases/tag/v1.0.0-RC1)\n\n- modifications to the way the bootstrap utility works\n- fix a race condition crash bug in bootstrapping\n- fix a parser bug\n\n\n_Released 2016-02-11_\n\n### [v1.0.0-PRE2](https://github.com/zendesk/maxwell/releases/tag/v1.0.0-PRE2)\n\n1.0.0-PRE2 brings in a lot of changes that got merged while we were\ntesting out PRE1.  so, hey.\n- Configurable names for the `maxwell` schema database (Kristian Kaufman)\n- Configurable key (primary key, id, database) into the kafka partition hash function (Kristian Kaufman)\n- Configurable Kafka partition hash function (java hashCode, murmur3) (Kristian Kaufman)\n- support GEOMETRY columns, output as well-known-text\n- add `--blacklist_tables` option to fully ignore excessive schema changes (Nicolas Maquet)\n- bootstrap rows now have 'bootstrap-insert' type\n\n\n_Released 2016-01-30_\n\n### [v1.0.0-PRE1](https://github.com/zendesk/maxwell/releases/tag/v1.0.0-PRE1)\n\n- Here we have the preview release of @nmaquet's excellent work around\n  bootstrapping initial versions of mysql tables.\n\n\n_Released 2016-01-09_\n\n### [v0.17.0](https://github.com/zendesk/maxwell/releases/tag/v0.17.0)\n\nv0.17 is a large bugfix release with one new feature.\n- FEATURE: allow specifying an alternative mysql schema-storage server and\n  replication server\n- BUGFIX: properly handle case-sensitivity by aping the behavior of the\n  master server.  Fixes #230.\n- BUGFIX: parse some forms of CHECK( ... ) statements.  Fixes #203.\n- BUGFIX: many more SQL-parser fixes.  We are mostly through some\n  thousands of lines of SQL produced by mysql-test.\n\n\n_Released 2016-01-07_\n\n### [v0.16.2](https://github.com/zendesk/maxwell/releases/tag/v0.16.2)\n\nThis is a large-ish bugfix release.\n- Support, with reservations, binlog_row_image=MINIMAL\n- parser bug: handle table names that look like floating points\n- parser bug: fix for entity names that have '.', '\\', etc in them\n- handle UPPERCASE encoding names\n- support UCS2 (start trying to operate ok on the mysql-test suite)\n- use ObjectOutputStream.reset to fix memory leaks when buffering to disk\n\n\n_Released 2015-12-16_\n\n### [v0.16.1](https://github.com/zendesk/maxwell/releases/tag/v0.16.1)\n\nThis is a bug-fix-roundup release:\n- support ALTER DATABASE\n- fix a bunch of parse errors: we've started running mysql-test at\n  maxwell and are fixing up failures.\n- some modifications to the overflow-to-disk logic; we buffer the input\n  and output, and we fix a memory leak\n\n\n_Released 2015-12-11_\n\n### [v0.16.0](https://github.com/zendesk/maxwell/releases/tag/v0.16.0)\n\nVersion 0.16.0 introduces a feature where UPDATE statements will now\nshow both the new row image and the old values of the fields that\nchanged.  Thanks @kristiankaufmann\n\n\n_Released 2015-12-10_\n\n### [v0.15.0](https://github.com/zendesk/maxwell/releases/tag/v0.15.0)\n\n- fix a parse problem with indices ordered by ASC/DESC\n\n\n_Released 2015-12-07_\n\n### [v0.15.0-RC1](https://github.com/zendesk/maxwell/releases/tag/v0.15.0-RC1)\n\n- large transactions now buffer to disk instead of crushing maxwell.\n- support ALGORITHM=[algo], LOCK=[lock] for 5.6 alters\n\n\n_Released 2015-12-04_\n\n### [v0.14.6](https://github.com/zendesk/maxwell/releases/tag/v0.14.6)\n\n- fix TIME column support\n- fix parsing on millisecond precision column defintions\n- fix CREATE SCHEMA parsing\n\n\n_Released 2015-11-27_\n\n### [v0.14.5](https://github.com/zendesk/maxwell/releases/tag/v0.14.5)\n\n- handle BOOLEAN columns with true/false defaults\n\n\n_Released 2015-11-25_\n\n### [v0.14.4](https://github.com/zendesk/maxwell/releases/tag/v0.14.4)\n\n- fixes parsing of \"mysql comments\" (`/*! .. */`)\n- More performance improvements, another 10% in a tight loop.\n\n\n_Released 2015-11-24_\n\n### [v0.14.3](https://github.com/zendesk/maxwell/releases/tag/v0.14.3)\n\n- fixes a regression in 0.14.2 that creates duplicate copies of the \"mysql\" database in the schema.\n\n\n_Released 2015-11-23_\n\n### [v0.14.2](https://github.com/zendesk/maxwell/releases/tag/v0.14.2)\n\n- capture the mysql database along with the rest of the schema.  Eliding it was a bad premature optimization that led to crashes when tables in the mysql database changed. \n\n\n_Released 2015-11-20_\n\n### [v0.14.1](https://github.com/zendesk/maxwell/releases/tag/v0.14.1)\n\n- fixes a parser bug around named PRIMARY KEYs.\n\n\n_Released 2015-11-17_\n\n### [v0.14.0](https://github.com/zendesk/maxwell/releases/tag/v0.14.0)\n\nThis release introduces row filters, allowing you to include or exclude tables from maxwell's output based on names or regular expressions.  \n\n\n_Released 2015-11-03_\n\n### [v0.13.1](https://github.com/zendesk/maxwell/releases/tag/v0.13.1)\n\nv0.13.1 is a bug fix of v0.13.0 -- fixes a bug where long rows were truncated. \n\nv0.13.0 contains:\n- Big performance boost for maxwell: 75% faster in some benchmarks\n- @davidsheldon contributed some nice bug fixes around `CREATE TABLE ... IF NOT EXISTS`, which were previously generating new, bogus copies of the schema.\n- we now include a \"scavenger thread\" that will lazily clean out old, deleted schemas.\n\n\n_Released 2015-10-29_\n\n### [v0.13.0](https://github.com/zendesk/maxwell/releases/tag/v0.13.0)\n\nLucky release number 13 brings some reasonably big changes:\n- Big performance boost for maxwell: 75% faster in some benchmarks\n- @davidsheldon contributed some nice bug fixes around `CREATE TABLE ... IF NOT EXISTS`, which were previously generating new, bogus copies of the schema.\n- we now include a \"scavenger thread\" that will lazily clean out old, deleted schemas.\n\n_This release has a pretty bad bug.  do not use._\n\n\n_Released 2015-10-29_\n\n### [v0.12.0](https://github.com/zendesk/maxwell/releases/tag/v0.12.0)\n\n- add support for BIT columns.  \n\n\n_Released 2015-10-16_\n\n### [v0.11.4](https://github.com/zendesk/maxwell/releases/tag/v0.11.4)\n\nthis is another bugfix release that fixes a problem where the replication thread can die in the middle of processing a transaction event.  I really need to fix this at a lower level, ie the open-replicator level.\n\n\n_Released 2015-09-30_\n\n### [v0.11.3](https://github.com/zendesk/maxwell/releases/tag/v0.11.3)\n\nthis is a bugfix release:\n- fix problems with table creation options inside alter statements ( `ALTER TABLE foo auto_increment=10` )\n- fix a host of shutdown-procedure bugs\n\nthe test suite should also be way more reliable, not like you care.\n\n\n_Released 2015-09-29_\n\n### [v0.11.2](https://github.com/zendesk/maxwell/releases/tag/v0.11.2)\n\nThis is a bugfix release.  It includes:\n- soft deletions of maxwell.schemas to fix A->B->A master swapping without creating intense replication delay\n- detect and fail early if we see `binlog_row_image=minimal`\n- kill off maxwell if the position thread dies\n- fix a bug where maxwell could pick up a copy of schema from a different server_id (curse you operator precedence!)\n\n\n_Released 2015-09-18_\n\n### [v0.11.1](https://github.com/zendesk/maxwell/releases/tag/v0.11.1)\n\n- maxwell gets a very minimal pass at detecting when a master has changed, in which it will kill off schemas and positions from a server_id that no longer is valid.  this should prevent the worst of cases.\n\n\n_Released 2015-09-16_\n\n### [v0.11.0](https://github.com/zendesk/maxwell/releases/tag/v0.11.0)\n\nThis release of Maxwell preserves transaction information in the kafka stream by adding a `xid` key in the JSON object, as well as a `commit` key for the final row inside the transaction.\n\nIt also contains a bugfix around server_id handling.\n\n\n_Released 2015-09-15_\n\n### [v0.10.1](https://github.com/zendesk/maxwell/releases/tag/v0.10.1)\n\n- proper support for BLOB, BINARY, VARBINARY columns (base 64 encoded)\n- fix a problem with the SQL parser where specifying encoding or collation in a string column in the wrong order would crash\n- make table option parsing more lenient\n\n\n_Released 2015-09-11_\n\n### [v0.11.0-RC1](https://github.com/zendesk/maxwell/releases/tag/v0.11.0-RC1)\n\n- merge master fixes\n\n\n_Released 2015-09-09_\n\n### [v0.11.0-PRE4](https://github.com/zendesk/maxwell/releases/tag/v0.11.0-PRE4)\n\n- bugfix on v0.11.0-PRE3\n\n\n_Released 2015-09-09_\n\n### [v0.10.0](https://github.com/zendesk/maxwell/releases/tag/v0.10.0)\n\n- Mysql 5.6 checksum support!\n- some more bugfixes with the SQL parser \n\n\n_Released 2015-09-09_\n\n### [v0.11.0-PRE3](https://github.com/zendesk/maxwell/releases/tag/v0.11.0-PRE3)\n\n- handle SAVEPOINT within transactions\n- downgrade unhandled SQL to a warning\n\n\n_Released 2015-09-08_\n\n### [v0.11.0-PRE2](https://github.com/zendesk/maxwell/releases/tag/v0.11.0-PRE2)\n\n- fixes for myISAM \"transactions\"\n\n\n_Released 2015-09-03_\n\n### [v0.11.0-PRE1](https://github.com/zendesk/maxwell/releases/tag/v0.11.0-PRE1)\n\n- fix a server_id bug (was always 1 in maxwell.schemas)\n- JSON output now includes transaction IDs\n\n\n_Released 2015-09-02_\n\n### [v0.10.0-RC4](https://github.com/zendesk/maxwell/releases/tag/v0.10.0-RC4)\n\n- deal with BINARY flag in string column creation.\n\n\n_Released 2015-08-31_\n\n### [v0.9.5](https://github.com/zendesk/maxwell/releases/tag/v0.9.5)\n\n- handle the BINARY flag in column creation\n\n\n_Released 2015-08-31_\n\n### [v0.10.0-RC3](https://github.com/zendesk/maxwell/releases/tag/v0.10.0-RC3)\n\n- handle \"TRUNCATE [TABLE_NAME]\" statements\n\n\n_Released 2015-08-27_\n\n### [v0.10.0-RC2](https://github.com/zendesk/maxwell/releases/tag/v0.10.0-RC2)\n\n- fixes a bug with checksum processing.\n\n\n_Released 2015-08-26_\n\n### [v0.10.0-RC1](https://github.com/zendesk/maxwell/releases/tag/v0.10.0-RC1)\n\n- upgrade to open-replicator 1.3.0-RC1, which brings binlog checksum (and thus easy 5.6.1) support to maxwell.\n\n\n_Released 2015-08-04_\n\n### [v0.9.4](https://github.com/zendesk/maxwell/releases/tag/v0.9.4)\n\n- allow a configurable number (including unlimited) of schemas to be stored\n\n\n_Released 2015-07-27_\n\n### [v0.9.3](https://github.com/zendesk/maxwell/releases/tag/v0.9.3)\n\n- bump open-replicator to 1.2.3, which allows processing of single rows greater than 2^24 bytes\n\n\n_Released 2015-07-14_\n\n### [v0.9.2](https://github.com/zendesk/maxwell/releases/tag/v0.9.2)\n\n- bump open-replicator buffer to 50mb by default\n- log to STDERR, not STDOUT \n- `--output_file` option for file producer\n\n\n_Released 2015-07-10_\n\n### [v0.9.1](https://github.com/zendesk/maxwell/releases/tag/v0.9.1)\n\n- Maxwell is now aware that column names are case-insenstive\n- fix a nasty bug in which maxwell would store the wrong position after it lost its connection to the master.\n\n\n_Released 2015-06-22_\n\n### [v0.9.0](https://github.com/zendesk/maxwell/releases/tag/v0.9.0)\n\nAlso, vanchi is so paranoid he's worried immediately about this. \n\n- mysql 5.6 support (without checksum support, yet)\n- fix a bunch of miscellaneous bugs @akshayi1 found (REAL, BOOL, BOOLEAN types, TRUNCATE TABLE)\n\n\n_Released 2015-06-18_\n\n### [v0.8.1](https://github.com/zendesk/maxwell/releases/tag/v0.8.1)\n\n- minor bugfix release around mysql connections going away.\n\n\n_Released 2015-06-16_\n\n### [v0.8.0](https://github.com/zendesk/maxwell/releases/tag/v0.8.0)\n\n- add \"ts\" field to row output\n- add --config option for passing a different config file\n- support int1, int2, int4, int8 columns\n\n\n_Released 2015-06-09_\n\n### [v0.7.2](https://github.com/zendesk/maxwell/releases/tag/v0.7.2)\n\n- handle inline sql comments\n- ignore more user management SQL\n\n\n_Released 2015-05-29_\n\n### [v0.7.1](https://github.com/zendesk/maxwell/releases/tag/v0.7.1)\n\n- only keep 5 most recent schemas\n\n\n_Released 2015-05-15_\n\n### [v0.7.0](https://github.com/zendesk/maxwell/releases/tag/v0.7.0)\n\n- handle CURRENT_TIMESTAMP parsing properly\n- better binlog position sync behavior\n\n\n_Released 2015-04-28_\n\n### [v0.6.3](https://github.com/zendesk/maxwell/releases/tag/v0.6.3)\n\n- better blacklist for CREATE TRIGGER\n\n\n_Released 2015-04-13_\n\n### [v0.6.2](https://github.com/zendesk/maxwell/releases/tag/v0.6.2)\n\n- maxwell now ignores SAVEPOINT statements.\n\n\n_Released 2015-04-13_\n\n### [v0.6.1](https://github.com/zendesk/maxwell/releases/tag/v0.6.1)\n\n- fixes a bug with parsing length-limited indexes.\n\n\n_Released 2015-04-13_\n\n### [v0.6.0](https://github.com/zendesk/maxwell/releases/tag/v0.6.0)\n\nVersion 0.6.0 has Maxwell outputting a JSON kafka key, so that one can use Kafka's neat \"store the last copy of a key\" retention policy.  It also fixes a couple of bugs in the query parsing path.\n\n\n_Released 2015-04-09_\n\n### [v0.5.0](https://github.com/zendesk/maxwell/releases/tag/v0.5.0)\n\n- maxwell now captures primary keys on tables.  We'll use this to form kafka key names later.\n- maxwell now outputs to a single topic, hashing the data by database name to keep a database's updates in order.\n\n\n_Released 2015-04-06_\n\n### [v0.4.0](https://github.com/zendesk/maxwell/releases/tag/v0.4.0)\n\nv0.4.0 fixes some bugs with long-lived mysql connections by adding connection pooling support.\n\n\n_Released 2015-03-25_\n\n### [v0.3.0](https://github.com/zendesk/maxwell/releases/tag/v0.3.0)\n\nThis version fixes a fairly nasty bug in which the binlog-position flush thread was sharing a connection with the rest of the system, leading to crashes. \n\nIt also enables kafka gzip compression by default.\n\n\n_Released 2015-03-24_\n\n### [v0.2.2](https://github.com/zendesk/maxwell/releases/tag/v0.2.2)\n\nVersion 0.2.2 sets up the LANG environment variable, which fixes a bug in utf-8 handling. \n\n\n_Released 2015-03-22_\n\n### [v0.2.1](https://github.com/zendesk/maxwell/releases/tag/v0.2.1)\n\nversion 0.2.1 makes Maxwell ignore CREATE INDEX ddl statements and others.\n\n\n_Released 2015-03-21_\n\n### [v0.2.0](https://github.com/zendesk/maxwell/releases/tag/v0.2.0)\n\nThis release gets Maxwell storing the last-written binlog position inside the mysql master itself. \n\n\n_Released 2015-03-18_\n\n### [v0.1.4](https://github.com/zendesk/maxwell/releases/tag/v0.1.4)\n\nsupport --position_file param\n\n\n_Released 2015-03-09_\n\n### [v0.1.3](https://github.com/zendesk/maxwell/releases/tag/v0.1.3)\n\nAdds kafka command line options.\n\n\n_Released 2015-03-09_\n\n### [v0.1.1](https://github.com/zendesk/maxwell/releases/tag/v0.1.1)\n\nv0.1.1, a small bugfix release. \n\n\n_Released 2015-03-06_\n\n### [v0.1](https://github.com/zendesk/maxwell/releases/tag/v0.1)\n\nThis is the first possible release of Maxwell that might work.  It includes some exceedingly basic kafka support, and JSON output of binlog deltas.\n\n\n_Released 2015-03-04_\n\n"
        },
        {
          "name": "Dockerfile",
          "type": "blob",
          "size": 0.9833984375,
          "content": "FROM maven:3.9.9-eclipse-temurin-23 AS builder\nENV MAXWELL_VERSION=1.42.2 KAFKA_VERSION=1.0.0\n\n\nRUN apt-get update \\\n    && apt-get -y upgrade \\\n    && apt-get install -y make\n\n# prime so we can have a cached image of the maven deps\nCOPY pom.xml /tmp\nRUN cd /tmp && mvn dependency:resolve\n\nCOPY . /workspace\nRUN cd /workspace \\\n    && KAFKA_VERSION=$KAFKA_VERSION make package MAXWELL_VERSION=$MAXWELL_VERSION \\\n    && mkdir /app \\\n    && mv /workspace/target/maxwell-$MAXWELL_VERSION/maxwell-$MAXWELL_VERSION/* /app/ \\\n    && apt-get clean \\\n    && rm -rf /var/lib/apt/lists/* /tmp/* /var/tmp/* /usr/share/doc/* /workspace/ /root/.m2/ \\\n    && echo \"$MAXWELL_VERSION\" > /REVISION\n\n# Build clean image with non-root priveledge\nFROM openjdk:23-jdk-slim\n\nRUN apt-get update \\\n    && apt-get -y upgrade\n\nCOPY --from=builder /app /app\nCOPY --from=builder /REVISION /REVISION\n\nWORKDIR /app\n\nRUN useradd -u 1000 maxwell -d /app\nRUN chown 1000:1000 /app\n\nUSER 1000\n\nCMD [ \"/bin/bash\", \"-c\", \"bin/maxwell-docker\" ]\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 0.53515625,
          "content": " Copyright 2025 Zendesk\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n   http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n"
        },
        {
          "name": "Makefile",
          "type": "blob",
          "size": 0.630859375,
          "content": "KAFKA_VERSION ?= 2.7.0\nKAFKA_PROFILE = kafka-${KAFKA_VERSION}\nexport JAVA_TOOL_OPTIONS = -Dorg.slf4j.simpleLogger.log.org.apache.maven.cli.transfer.Slf4jMavenTransferListener=warn\n\nall: compile\n\ntest:\n\tmvn -B test -P ${KAFKA_PROFILE}\n\ncompile:\n\tmvn -B compile -P ${KAFKA_PROFILE}\n\nclean:\n\tmvn -B clean\n\ndepclean: clean\n\trm -f $(CLASSPATH)\n\npackage: depclean kafka-0.8.2.2 kafka-0.9.0.1 kafka-0.10.0.1 kafka-0.10.2.1 kafka-0.11.0.1 kafka-1.0.0 kafka-2.7.0 kafka-3.4.0\n\t@# TODO: this is inefficient, we really just want to copy the jars...\n\tmvn package -DskipTests=true\n\nkafka-%:\n\tmvn compile -P kafka-$(*)\n\nFORCE:\ndocs: FORCE\n\tmvn javadoc:javadoc\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 1.0009765625,
          "content": "This is __Maxwell's daemon__, a [change data capture](https://www.confluent.io/blog/how-change-data-capture-works-patterns-solutions-implementation/) application \nthat reads MySQL binlogs and writes data changes as JSON to Kafka, Kinesis, and other streaming platforms.\n\n\n\n[ Download](https://github.com/zendesk/maxwell/releases/download/v1.42.2/maxwell-1.42.2.tar.gz) \\|\n[ Source / Community](https://github.com/zendesk/maxwell) \\|\n[ Getting Started](/quickstart) \\|\n[ Reference](/config)\n\n__What's it for?__\n\n- ETL of all sorts\n- maintaining an audit log of all changes to your database\n- cache building/expiring\n- search indexing \n- inter-service communication\n\n\n__It goes like this:__\n\n```\n  mysql> update `test`.`maxwell` set mycol = 55, daemon = 'Stanislaw Lem';\n  maxwell -> kafka: \n  {\n    \"database\": \"test\",\n    \"table\": \"maxwell\",\n    \"type\": \"update\",\n    \"ts\": 1449786310,\n    \"data\": { \"id\":1, \"daemon\": \"Stanislaw Lem\", \"mycol\": 55 },\n    \"old\": { \"mycol\":, 23, \"daemon\": \"what once was\" }\n  }\n```\n"
        },
        {
          "name": "bin",
          "type": "tree",
          "content": null
        },
        {
          "name": "build",
          "type": "tree",
          "content": null
        },
        {
          "name": "config.properties.example",
          "type": "blob",
          "size": 12.138671875,
          "content": "# tl;dr config\nlog_level=info\n\nproducer=kafka\nkafka.bootstrap.servers=localhost:9092\n\n# mysql login info\nhost=localhost\nuser=maxwell\npassword=maxwell\n\n\n#     *** general ***\n# choose where to produce data to. stdout|file|kafka|kinesis|pubsub|sqs|rabbitmq|redis|bigquery\n#producer=kafka\n\n# set the log level.  note that you can configure things further in log4j2.xml\n#log_level=DEBUG # [DEBUG, INFO, WARN, ERROR]\n\n# if set, maxwell will look up the scoped environment variables, strip off the prefix and inject the configs\n#env_config_prefix=MAXWELL_\n\n#     *** mysql ***\n\n# mysql host to connect to\n#host=hostname\n\n# mysql port to connect to\n#port=3306\n\n# mysql user to connect as.  This user must have REPLICATION SLAVE permissions,\n# as well as full access to the `maxwell` (or schema_database) database\n#user=maxwell\n\n# mysql password\n#password=maxwell\n\n# options to pass into the jdbc connection, given as opt=val&opt2=val2\n#jdbc_options=opt1=100&opt2=hello\n\n# name of the mysql database where maxwell keeps its own state\n#schema_database=maxwell\n\n# whether to use GTID or not for positioning\n#gtid_mode=true\n\n# maxwell will capture an initial \"base\" schema containing all table and column information,\n# and then keep delta-updates on top of that schema.  If you have an inordinate amount of DDL changes,\n# the table containing delta changes will grow unbounded (and possibly too large) over time.  If you\n# enable this option Maxwell will periodically compact its tables.\n#max_schemas=10000\n\n# SSL/TLS options\n# To use VERIFY_CA or VERIFY_IDENTITY, you must set the trust store with Java opts:\n#   -Djavax.net.ssl.trustStore=<truststore> -Djavax.net.ssl.trustStorePassword=<password>\n# or import the MySQL cert into the global Java cacerts.\n# MODE must be one of DISABLED, PREFERRED, REQUIRED, VERIFY_CA, or VERIFY_IDENTITY\n#\n# turns on ssl for the maxwell-store connection, other connections inherit this setting unless specified\n#ssl=DISABLED\n# for binlog-connector\n#replication_ssl=DISABLED\n# for the schema-capture connection, if used\n#schema_ssl=DISABLED\n\n# maxwell can optionally replicate from a different server than where it stores\n# schema and binlog position info.  Specify that different server here:\n\n#replication_host=other\n#replication_user=username\n#replication_password=password\n#replication_port=3306\n\n# This may be useful when using MaxScale's binlog mirroring host.\n# Specifies that Maxwell should capture schema from a different server than\n# it replicates from:\n\n#schema_host=other\n#schema_user=username\n#schema_password=password\n#schema_port=3306\n\n\n#       *** output format ***\n\n# records include binlog position (default false)\n#output_binlog_position=true\n\n# records include a gtid string (default false)\n#output_gtid_position=true\n\n# records include fields with null values (default true).  If this is false,\n# fields where the value is null will be omitted entirely from output.\n#output_nulls=true\n\n# records include server_id (default false)\n#output_server_id=true\n\n# records include thread_id (default false)\n#output_thread_id=true\n\n# records include schema_id (default false)\n#output_schema_id=true\n\n# records include row query, binlog option \"binlog_rows_query_log_events\" must be enabled\" (default false)\n#output_row_query=true\n\n# DML records include list of values that make up a row's primary key (default false)\n#output_primary_keys=true\n\n# DML records include list of columns that make up a row's primary key (default false)\n#output_primary_key_columns=true\n\n# records include commit and xid (default true)\n#output_commit_info=true\n\n# This controls whether maxwell will output JSON information containing\n# DDL (ALTER/CREATE TABLE/ETC) infromation. (default: false)\n# See also: ddl_kafka_topic\n#output_ddl=true\n\n# turns underscore naming style of fields to camel case style in JSON output\n# default is none, which means the field name in JSON is the exact name in MySQL table\n#output_naming_strategy=underscore_to_camelcase\n\n#       *** kafka ***\n\n# list of kafka brokers\n#kafka.bootstrap.servers=hosta:9092,hostb:9092\n\n# kafka topic to write to\n# this can be static, e.g. 'maxwell', or dynamic, e.g. namespace_%{database}_%{table}\n# in the latter case 'database' and 'table' will be replaced with the values for the row being processed\n#kafka_topic=maxwell\n\n# alternative kafka topic to write DDL (alter/create/drop) to.  Defaults to kafka_topic\n#ddl_kafka_topic=maxwell_ddl\n\n# hash function to use.  \"default\" is just the JVM's 'hashCode' function.\n#kafka_partition_hash=default # [default, murmur3]\n\n# how maxwell writes its kafka key.\n#\n# 'hash' looks like:\n# {\"database\":\"test\",\"table\":\"tickets\",\"pk.id\":10001}\n#\n# 'array' looks like:\n# [\"test\",\"tickets\",[{\"id\":10001}]]\n#\n# default: \"hash\"\n#kafka_key_format=hash # [hash, array]\n\n# extra kafka options.  Anything prefixed \"kafka.\" will get\n# passed directly into the kafka-producer's config.\n\n# a few defaults.\n# These are 0.11-specific. They may or may not work with other versions.\nkafka.compression.type=snappy\nkafka.retries=0\nkafka.acks=1\n#kafka.batch.size=16384\n\n\n# kafka+SSL example\n# kafka.security.protocol=SSL\n# kafka.ssl.truststore.location=/var/private/ssl/kafka.client.truststore.jks\n# kafka.ssl.truststore.password=test1234\n# kafka.ssl.keystore.location=/var/private/ssl/kafka.client.keystore.jks\n# kafka.ssl.keystore.password=test1234\n# kafka.ssl.key.password=test1234#\n\n# controls a heuristic check that maxwell may use to detect messages that\n# we never heard back from.  The heuristic check looks for \"stuck\" messages, and\n# will timeout maxwell after this many milliseconds.\n#\n# See https://github.com/zendesk/maxwell/blob/master/src/main/java/com/zendesk/maxwell/producer/InflightMessageList.java\n# if you really want to get into it.\n#producer_ack_timeout=120000 # default 0\n\n\n#           *** partitioning ***\n\n# What part of the data do we partition by?\n#producer_partition_by=database # [database, table, primary_key, transaction_id, thread_id, column]\n\n# specify what fields to partition by when using producer_partition_by=column\n# column separated list.\n#producer_partition_columns=id,foo,bar\n\n# when using producer_partition_by=column, partition by this when\n# the specified column(s) don't exist.\n#producer_partition_by_fallback=database\n\n#            *** kinesis ***\n\n#kinesis_stream=maxwell\n\n# AWS places a 256 unicode character limit on the max key length of a record\n# http://docs.aws.amazon.com/kinesis/latest/APIReference/API_PutRecord.html\n#\n# Setting this option to true enables hashing the key with the md5 algorithm\n# before we send it to kinesis so all the keys work within the key size limit.\n# Values: true, false\n# Default: false\n#kinesis_md5_keys=true\n\n#            *** sqs ***\n\n#sqs_queue_uri=aws_sqs_queue_uri\n\n# The sqs producer will need aws credentials configured in the default\n# root folder and file format. Please check below link on how to do it.\n# http://docs.aws.amazon.com/sdk-for-java/v1/developer-guide/setup-credentials.html\n\n#            *** pub/sub ***\n\n#pubsub_project_id=maxwell\n#pubsub_topic=maxwell\n#ddl_pubsub_topic=maxwell_ddl\n\n#            *** bigquery ***\n\n#bigquery_project_id=myproject\n#bigquery_dataset=mydataset\n#bigquery_table=mytable\n\n#            *** rabbit-mq ***\n\n#rabbitmq_host=rabbitmq_hostname\n#rabbitmq_port=5672\n#rabbitmq_user=guest\n#rabbitmq_pass=guest\n#rabbitmq_virtual_host=/\n#rabbitmq_handshake_timeout=20000\n#rabbitmq_exchange=maxwell\n#rabbitmq_exchange_type=fanout\n#rabbitmq_exchange_durable=false\n#rabbitmq_exchange_autodelete=false\n#rabbitmq_routing_key_template=%db%.%table%\n#rabbitmq_message_persistent=false\n#rabbitmq_declare_exchange=true\n#rabbitmq_use_ssl=false\n\n#           *** redis ***\n\n#redis_host=redis_host\n#redis_port=6379\n#redis_auth=redis_auth\n#redis_database=0\n\n# name of pubsub/list/whatever key to publish to\n#redis_key=maxwell\n\n# this can be static, e.g. 'maxwell', or dynamic, e.g. namespace_%{database}_%{table}\n#redis_pub_channel=maxwell\n# this can be static, e.g. 'maxwell', or dynamic, e.g. namespace_%{database}_%{table}\n#redis_list_key=maxwell\n# this can be static, e.g. 'maxwell', or dynamic, e.g. namespace_%{database}_%{table}\n# Valid values for redis_type = pubsub|lpush. Defaults to pubsub\n\n#redis_type=pubsub\n\n#           *** custom producer ***\n\n# the fully qualified class name for custom ProducerFactory\n# see the following link for more details.\n# http://maxwells-daemon.io/producers/#custom-producer\n#custom_producer.factory=\n\n# custom producer properties can be configured using the custom_producer.* property namespace\n#custom_producer.custom_prop=foo\n\n#          *** filtering ***\n\n# filter rows out of Maxwell's output.  Command separated list of filter-rules, evaluated in sequence.\n# A filter rule is:\n#  <type> \":\" <db> \".\" <tbl> [ \".\" <col> \"=\" <col_val> ]\n#  type    ::= [ \"include\" | \"exclude\" | \"blacklist\" ]\n#  db      ::= [ \"/regexp/\" | \"string\" | \"`string`\" | \"*\" ]\n#  tbl     ::= [ \"/regexp/\" | \"string\" | \"`string`\" | \"*\" ]\n#  col_val ::= \"column_name\"\n#  tbl     ::= [ \"/regexp/\" | \"string\" | \"`string`\" | \"*\" ]\n#\n# See http://maxwells-daemon.io/filtering for more details\n#\n#filter= exclude: *.*, include: foo.*, include: bar.baz, include: foo.bar.col_eg = \"value_to_match\"\n\n\n# If you are running maxwell without permissions to view certain parts of your schema, \n# you may encounter \"Coulndn't find database XXX\" errors.  In *only* those situations, \n# turn this option on and use filters to only include what you need.\n#ignore_missing_schema = false\n\n# javascript filter\n# maxwell can run a bit of javascript for each row if you need very custom filtering/data munging.\n# See http://maxwells-daemon.io/filtering/#javascript_filters for more details\n#\n#javascript=/path/to/javascript_filter_file\n\n#       *** encryption ***\n\n# Encryption mode. Possible values are none, data, and all. (default none)\n#encrypt=none\n\n# Specify the secret key to be used\n#secret_key=RandomInitVector\n\n#       *** monitoring ***\n\n# Maxwell collects metrics via dropwizard. These can be exposed through the\n# base logging mechanism (slf4j), JMX, HTTP or pushed to Datadog.\n# Options: [jmx, slf4j, http, datadog]\n# Supplying multiple is allowed.\n#metrics_type=jmx,slf4j\n\n# The prefix maxwell will apply to all metrics\n#metrics_prefix=MaxwellMetrics # default MaxwellMetrics\n\n# Enable (dropwizard) JVM metrics, default false\n#metrics_jvm=true\n\n# When metrics_type includes slf4j this is the frequency metrics are emitted to the log, in seconds\n#metrics_slf4j_interval=60\n\n# When metrics_type includes http or diagnostic is enabled, this is the port the server will bind to.\n#http_port=8080\n\n# When metrics_type includes http or diagnostic is enabled, this is the http path prefix, default /.\n#http_path_prefix=/some/path/\n\n# ** The following are Datadog specific. **\n# When metrics_type includes datadog this is the way metrics will be reported.\n# Options: [udp, http]\n# Supplying multiple is not allowed.\n#metrics_datadog_type=udp\n\n# datadog tags that should be supplied\n#metrics_datadog_tags=tag1:value1,tag2:value2\n\n# The frequency metrics are pushed to datadog, in seconds\n#metrics_datadog_interval=60\n\n# required if metrics_datadog_type = http\n#metrics_datadog_apikey=API_KEY\n\n# required if metrics_datadog_type = udp\n#metrics_datadog_host=localhost # default localhost\n#metrics_datadog_port=8125 # default 8125\n\n# Maxwell exposes http diagnostic endpoint to check below in parallel:\n# 1. binlog replication lag\n# 2. producer (currently kafka) lag\n\n# To enable Maxwell diagnostic\n#http_diagnostic=true # default false\n\n# Diagnostic check timeout in milliseconds, required if diagnostic = true\n#http_diagnostic_timeout=10000 # default 10000\n\n#    *** misc ***\n\n# maxwell's bootstrapping functionality has a couple of modes.\n#\n# In \"async\" mode, maxwell will output the replication stream while it\n# simultaneously outputs the database to the topic.  Note that it won't\n# output replication data for any tables it is currently bootstrapping -- this\n# data will be buffered and output after the bootstrap is complete.\n#\n# In \"sync\" mode, maxwell stops the replication stream while it\n# outputs bootstrap data.\n#\n# async mode keeps ops live while bootstrapping, but carries the possibility of\n# data loss (due to buffering transactions).  sync mode is safer but you\n# have to stop replication.\n#bootstrapper=async [sync, async, none]\n\n# output filename when using the \"file\" producer\n#output_file=/path/to/file\n"
        },
        {
          "name": "docs",
          "type": "tree",
          "content": null
        },
        {
          "name": "img",
          "type": "tree",
          "content": null
        },
        {
          "name": "kinesis-producer-library.properties.example",
          "type": "blob",
          "size": 10.0205078125,
          "content": "# copied from https://github.com/awslabs/amazon-kinesis-producer/blob/master/java/amazon-kinesis-producer-sample/default_config.properties\n\n# You can load a properties file with\n# KinesisProducerConfiguration.fromPropertiesFile(String)\n#\n# Any fields not found in the properties file will take on default values.\n#\n# The values loaded are checked against any constraints that each respective\n# field may have. If there are invalid values an IllegalArgumentException will\n# be thrown.\n\n\n# Enable aggregation. With aggregation, multiple user records are packed into\n# a single KinesisRecord. If disabled, each user record is sent in its own\n# KinesisRecord.\n#\n# If your records are small, enabling aggregation will allow you to put many\n# more records than you would otherwise be able to for a shard before getting\n# throttled.\n#\n# Default: true\nAggregationEnabled = true\n\n# Maximum number of items to pack into an aggregated record.\n#\n# There should be normally no need to adjust this. If you want to limit the\n# time records spend buffering, look into record_max_buffered_time instead.\n#\n# Default: 4294967295\n# Minimum: 1\n# Maximum (inclusive): 9223372036854775807\nAggregationMaxCount = 4294967295\n\n# Maximum number of bytes to pack into an aggregated Kinesis record.\n#\n# There should be normally no need to adjust this. If you want to limit the\n# time records spend buffering, look into record_max_buffered_time instead.\n#\n# If a record has more data by itself than this limit, it will bypass the\n# aggregator. Note the backend enforces a limit of 50KB on record size. If\n# you set this beyond 50KB, oversize records will be rejected at the backend.\n#\n# Default: 51200\n# Minimum: 64\n# Maximum (inclusive): 1048576\nAggregationMaxSize = 51200\n\n# Maximum number of items to pack into an PutRecords request.\n#\n# There should be normally no need to adjust this. If you want to limit the\n# time records spend buffering, look into record_max_buffered_time instead.\n#\n# Default: 500\n# Minimum: 1\n# Maximum (inclusive): 500\nCollectionMaxCount = 500\n\n# Maximum amount of data to send with a PutRecords request.\n#\n# There should be normally no need to adjust this. If you want to limit the\n# time records spend buffering, look into record_max_buffered_time instead.\n#\n# Records larger than the limit will still be sent, but will not be grouped\n# with others.\n#\n# Default: 5242880\n# Minimum: 52224\n# Maximum (inclusive): 9223372036854775807\nCollectionMaxSize = 5242880\n\n# Timeout (milliseconds) for establishing TLS connections.\n#\n# Default: 6000\n# Minimum: 100\n# Maximum (inclusive): 300000\nConnectTimeout = 6000\n\n# Use a custom Kinesis and CloudWatch endpoint.\n#\n# Mostly for testing use. Note this does not accept protocols or paths, only\n# host names or ip addresses. There is no way to disable TLS. The KPL always\n# connects with TLS.\n#\n# Expected pattern: ^([A-Za-z0-9-\\\\.]+)?$\n# CustomEndpoint = \n\n# If true, throttled puts are not retried. The records that got throttled\n# will be failed immediately upon receiving the throttling error. This is\n# useful if you want to react immediately to any throttling without waiting\n# for the KPL to retry. For example, you can use a different hash key to send\n# the throttled record to a backup shard.\n#\n# If false, the KPL will automatically retry throttled puts. The KPL performs\n# backoff for shards that it has received throttling errors from, and will\n# avoid flooding them with retries. Note that records may fail from\n# expiration (see record_ttl) if they get delayed for too long because of\n# throttling.\n#\n# Default: false\nFailIfThrottled = false\n\n# Minimum level of logs. Messages below the specified level will not be\n# logged. Logs for the native KPL daemon show up on stderr.\n#\n# Default: info\n# Expected pattern: info|warning|error\nLogLevel = info\n\n# Maximum number of connections to open to the backend. HTTP requests are\n# sent in parallel over multiple connections.\n#\n# Setting this too high may impact latency and consume additional resources\n# without increasing throughput.\n#\n# Default: 24\n# Minimum: 1\n# Maximum (inclusive): 256\nMaxConnections = 24\n\n# Controls the granularity of metrics that are uploaded to CloudWatch.\n# Greater granularity produces more metrics.\n#\n# When \"shard\" is selected, metrics are emitted with the stream name and\n# shard id as dimensions. On top of this, the same metric is also emitted\n# with only the stream name dimension, and lastly, without the stream name.\n# This means for a particular metric, 2 streams with 2 shards (each) will\n# produce 7 CloudWatch metrics, one for each shard, one for each stream, and\n# one overall, all describing the same statistics, but at different levels of\n# granularity.\n#\n# When \"stream\" is selected, per shard metrics are not uploaded; when\n# \"global\" is selected, only the total aggregate for all streams and all\n# shards are uploaded.\n#\n# Consider reducing the granularity if you're not interested in shard-level\n# metrics, or if you have a large number of shards.\n#\n# If you only have 1 stream, select \"global\"; the global data will be\n# equivalent to that for the stream.\n#\n# Refer to the metrics documentation for details about each metric.\n#\n# Default: shard\n# Expected pattern: global|stream|shard\nMetricsGranularity = shard\n\n# Controls the number of metrics that are uploaded to CloudWatch.\n#\n# \"none\" disables all metrics.\n#\n# \"summary\" enables the following metrics: UserRecordsPut, KinesisRecordsPut,\n# ErrorsByCode, AllErrors, BufferingTime.\n#\n# \"detailed\" enables all remaining metrics.\n#\n# Refer to the metrics documentation for details about each metric.\n#\n# Default: detailed\n# Expected pattern: none|summary|detailed\nMetricsLevel = detailed\n\n# The namespace to upload metrics under.\n#\n# If you have multiple applications running the KPL under the same AWS\n# account, you should use a different namespace for each application.\n#\n# If you are also using the KCL, you may wish to use the application name you\n# have configured for the KCL as the the namespace here. This way both your\n# KPL and KCL metrics show up under the same namespace.\n#\n# Default: KinesisProducerLibrary\n# Expected pattern: (?!AWS/).{1,255}\nMetricsNamespace = KinesisProducerLibrary\n\n# Delay (in milliseconds) between each metrics upload.\n#\n# For testing only. There is no benefit in setting this lower or higher in\n# production.\n#\n# Default: 60000\n# Minimum: 1\n# Maximum (inclusive): 60000\nMetricsUploadDelay = 60000\n\n# Minimum number of connections to keep open to the backend.\n#\n# There should be no need to increase this in general.\n#\n# Default: 1\n# Minimum: 1\n# Maximum (inclusive): 16\nMinConnections = 1\n\n# Server port to connect to. Only useful with custom_endpoint.\n#\n# Default: 443\n# Minimum: 1\n# Maximum (inclusive): 65535\nPort = 443\n\n# Limits the maximum allowed put rate for a shard, as a percentage of the\n# backend limits.\n#\n# The rate limit prevents the producer from sending data too fast to a shard.\n# Such a limit is useful for reducing bandwidth and CPU cycle wastage from\n# sending requests that we know are going to fail from throttling.\n#\n# Kinesis enforces limits on both the number of records and number of bytes\n# per second. This setting applies to both.\n#\n# The default value of 150% is chosen to allow a single producer instance to\n# completely saturate the allowance for a shard. This is an aggressive\n# setting. If you prefer to reduce throttling errors rather than completely\n# saturate the shard, consider reducing this setting.\n#\n# Default: 150\n# Minimum: 1\n# Maximum (inclusive): 9223372036854775807\nRateLimit = 150\n\n# Maximum amount of itme (milliseconds) a record may spend being buffered\n# before it gets sent. Records may be sent sooner than this depending on the\n# other buffering limits.\n#\n# This setting provides coarse ordering among records - any two records will\n# be reordered by no more than twice this amount (assuming no failures and\n# retries and equal network latency).\n#\n# The library makes a best effort to enforce this time, but cannot guarantee\n# that it will be precisely met. In general, if the CPU is not overloaded,\n# the library will meet this deadline to within 10ms.\n#\n# Failures and retries can additionally increase the amount of time records\n# spend in the KPL. If your application cannot tolerate late records, use the\n# record_ttl setting to drop records that do not get transmitted in time.\n#\n# Setting this too low can negatively impact throughput.\n#\n# Default: 100\n# Maximum (inclusive): 9223372036854775807\nRecordMaxBufferedTime = 100\n\n# Set a time-to-live on records (milliseconds). Records that do not get\n# successfully put within the limit are failed.\n#\n# This setting is useful if your application cannot or does not wish to\n# tolerate late records. Records will still incur network latency after they\n# leave the KPL, so take that into consideration when choosing a value for\n# this setting.\n#\n# If you do not wish to lose records and prefer to retry indefinitely, set\n# record_ttl to a large value like INT_MAX. This has the potential to cause\n# head-of-line blocking if network issues or throttling occur. You can\n# respond to such situations by using the metrics reporting functions of the\n# KPL. You may also set fail_if_throttled to true to prevent automatic\n# retries in case of throttling.\n#\n# Default: 30000\n# Minimum: 100\n# Maximum (inclusive): 9223372036854775807\nRecordTtl = 3600000\n\n# Which region to send records to.\n#\n# If you do not specify the region and are running in EC2, the library will\n# use the region the instance is in.\n#\n# The region is also used to sign requests.\n#\n# Expected pattern: ^([a-z]+-[a-z]+-[0-9])?$\n#Region =\n\n# The maximum total time (milliseconds) elapsed between when we begin a HTTP\n# request and receiving all of the response. If it goes over, the request\n# will be timed-out.\n#\n# Note that a timed-out request may actually succeed at the backend. Retrying\n# then leads to duplicates. Setting the timeout too low will therefore\n# increase the probability of duplicates.\n#\n# Default: 6000\n# Minimum: 100\n# Maximum (inclusive): 600000\nRequestTimeout = 6000\n\n# Verify the endpoint's certificate. Do not disable unless using\n# custom_endpoint for testing. Never disable this in production.\n#\n# Default: true\nVerifyCertificate = true\n\n"
        },
        {
          "name": "output.txt",
          "type": "blob",
          "size": 0,
          "content": ""
        },
        {
          "name": "pom.xml",
          "type": "blob",
          "size": 18.037109375,
          "content": "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<project xmlns=\"http://maven.apache.org/POM/4.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\">\n  <modelVersion>4.0.0</modelVersion>\n\n  <groupId>com.zendesk</groupId>\n  <artifactId>maxwell</artifactId>\n  <version>1.42.2</version>\n  <packaging>jar</packaging>\n\n  <name>maxwell</name>\n  <description>Maxwell's daemon.  Watches mysql, outputs to JSON.</description>\n  <url>http://maxwells-daemon.io</url>\n\n  <licenses>\n    <license>\n      <name>Apache License, Version 2.0</name>\n      <url>http://www.apache.org/licenses/LICENSE-2.0.txt</url>\n      <distribution>repo</distribution>\n    </license>\n  </licenses>\n\n  <developers>\n    <developer>\n      <id>osheroff</id>\n      <name>Ben Osheroff</name>\n      <email>ben@zendesk.com</email>\n      <organization>Zendesk</organization>\n      <organizationUrl>http://www.zendesk.com</organizationUrl>\n      <roles>\n        <role>Project-Administrator</role>\n        <role>Developer</role>\n      </roles>\n    </developer>\n  </developers>\n\n  <scm>\n    <connection>scm:git:git@github.com:zendesk/maxwell.git</connection>\n    <developerConnection>scm:git:git@github.com:zendesk/maxwell.git</developerConnection>\n    <url>git@github.com:zendesk/maxwell.git</url>\n  </scm>\n\n  <properties>\n    <project.build.sourceEncoding>UTF-8</project.build.sourceEncoding>\n    <mockito.version>3.12.4</mockito.version>\n    <opencensus.version>0.28.3</opencensus.version>\n    <aws-java.version>1.12.537</aws-java.version>\n    <jackson.version>2.15.2</jackson.version>\n    <maven.compiler.source>1.8</maven.compiler.source>\n    <maven.compiler.target>1.8</maven.compiler.target>\n  </properties>\n\n  <profiles>\n    <profile>\n      <id>kafka-0.8.2.2</id>\n      <dependencies>\n        <dependency>\n          <groupId>org.apache.kafka</groupId>\n          <artifactId>kafka-clients</artifactId>\n          <version>0.8.2.2</version>\n        </dependency>\n      </dependencies>\n    </profile>\n    <profile>\n      <id>kafka-0.9.0.1</id>\n      <dependencies>\n        <dependency>\n          <groupId>org.apache.kafka</groupId>\n          <artifactId>kafka-clients</artifactId>\n          <version>0.9.0.1</version>\n        </dependency>\n      </dependencies>\n    </profile>\n    <profile>\n      <id>kafka-0.10.0.1</id>\n      <dependencies>\n        <dependency>\n          <groupId>org.apache.kafka</groupId>\n          <artifactId>kafka-clients</artifactId>\n          <version>0.10.0.1</version>\n        </dependency>\n      </dependencies>\n    </profile>\n    <profile>\n      <id>kafka-0.10.2.1</id>\n      <dependencies>\n        <dependency>\n          <groupId>org.apache.kafka</groupId>\n          <artifactId>kafka-clients</artifactId>\n          <version>0.10.2.1</version>\n        </dependency>\n      </dependencies>\n    </profile>\n    <profile>\n      <id>kafka-0.11.0.1</id>\n      <dependencies>\n        <dependency>\n          <groupId>org.apache.kafka</groupId>\n          <artifactId>kafka-clients</artifactId>\n          <version>0.11.0.1</version>\n        </dependency>\n      </dependencies>\n    </profile>\n    <profile>\n      <id>kafka-1.0.0</id>\n      <activation>\n        <activeByDefault>true</activeByDefault>\n      </activation>\n      <dependencies>\n        <dependency>\n          <groupId>org.apache.kafka</groupId>\n          <artifactId>kafka-clients</artifactId>\n          <version>1.0.0</version>\n        </dependency>\n      </dependencies>\n    </profile>\n    <profile>\n      <id>kafka-2.7.0</id>\n      <activation>\n        <activeByDefault>true</activeByDefault>\n      </activation>\n      <dependencies>\n        <dependency>\n          <groupId>org.apache.kafka</groupId>\n          <artifactId>kafka-clients</artifactId>\n          <version>2.7.0</version>\n        </dependency>\n      </dependencies>\n    </profile>\n    <profile>\n      <id>kafka-3.4.0</id>\n      <activation>\n        <activeByDefault>true</activeByDefault>\n      </activation>\n      <dependencies>\n        <dependency>\n          <groupId>org.apache.kafka</groupId>\n          <artifactId>kafka-clients</artifactId>\n          <version>3.4.0</version>\n        </dependency>\n      </dependencies>\n    </profile>\n  </profiles>\n\n  <dependencies>\n    <!-- very important deps -->\n    <dependency>\n      <groupId>com.zendesk</groupId>\n      <artifactId>mysql-binlog-connector-java</artifactId>\n      <version>0.27.4</version>\n    </dependency>\n    <dependency>\n      <groupId>com.mchange</groupId>\n      <artifactId>c3p0</artifactId>\n      <version>0.9.5.5</version>\n    </dependency>\n    <dependency>\n      <groupId>mysql</groupId>\n      <artifactId>mysql-connector-java</artifactId>\n      <version>8.0.28</version>\n    </dependency>\n    <dependency>\n      <groupId>org.antlr</groupId>\n      <artifactId>antlr4-runtime</artifactId>\n      <version>4.8-1</version>\n    </dependency>\n    <dependency>\n      <groupId>net.sf.jopt-simple</groupId>\n      <artifactId>jopt-simple</artifactId>\n      <version>5.0.4</version>\n    </dependency>\n    <dependency>\n      <groupId>com.fasterxml.jackson.core</groupId>\n      <artifactId>jackson-core</artifactId>\n      <version>${jackson.version}</version>\n    </dependency>\n    <dependency>\n      <groupId>com.fasterxml.jackson.core</groupId>\n      <artifactId>jackson-databind</artifactId>\n      <version>${jackson.version}</version>\n    </dependency>\n    <dependency>\n      <groupId>com.fasterxml.jackson.core</groupId>\n      <artifactId>jackson-annotations</artifactId>\n      <version>${jackson.version}</version>\n    </dependency>\n    <dependency>\n      <groupId>org.jgroups</groupId>\n      <artifactId>jgroups-raft</artifactId>\n      <version>1.0.0.Final</version>\n    </dependency>\n    <dependency>\n      <groupId>org.openjdk.nashorn</groupId>\n      <artifactId>nashorn-core</artifactId>\n      <version>15.3</version>\n    </dependency>\n\n    <!-- utils and support libs -->\n    <dependency>\n      <groupId>org.apache.logging.log4j</groupId>\n      <artifactId>log4j-core</artifactId>\n      <version>2.17.1</version>\n    </dependency>\n    <dependency>\n      <groupId>org.apache.logging.log4j</groupId>\n      <artifactId>log4j-slf4j-impl</artifactId>\n      <version>2.12.1</version>\n    </dependency>\n    <dependency>\n      <groupId>org.slf4j</groupId>\n      <artifactId>jul-to-slf4j</artifactId>\n      <version>1.7.30</version>\n    </dependency>\n    <dependency>\n      <groupId>com.djdch.log4j</groupId>\n      <artifactId>log4j-staticshutdown</artifactId>\n      <version>1.1.0</version>\n    </dependency>\n    <dependency>\n      <groupId>org.apache.commons</groupId>\n      <artifactId>commons-lang3</artifactId>\n      <version>3.11</version>\n    </dependency>\n    <dependency>\n      <groupId>commons-codec</groupId>\n      <artifactId>commons-codec</artifactId>\n      <version>1.14</version>\n    </dependency>\n    <dependency>\n      <groupId>com.vividsolutions</groupId>\n      <artifactId>jts</artifactId>\n      <version>1.13</version>\n    </dependency>\n    <dependency>\n      <groupId>org.lz4</groupId>\n      <artifactId>lz4-java</artifactId>\n      <version>1.8.0</version>\n    </dependency>\n    <dependency>\n      <groupId>me.tongfei</groupId>\n      <artifactId>progressbar</artifactId>\n      <version>0.6.0</version>\n    </dependency>\n\n    <!-- producer libs -->\n    <dependency>\n      <groupId>com.google.cloud</groupId>\n      <artifactId>google-cloud-bigquerystorage</artifactId>\n      <version>2.14.2</version>\n    </dependency>\n    <dependency>\n      <groupId>com.google.cloud</groupId>\n      <artifactId>google-cloud-bigquery</artifactId>\n      <version>2.13.3</version>\n    </dependency>\n    <dependency>\n      <groupId>com.amazonaws</groupId>\n      <artifactId>aws-java-sdk-core</artifactId>\n      <version>${aws-java.version}</version>\n    </dependency>\n    <dependency>\n      <groupId>com.amazonaws</groupId>\n      <artifactId>aws-java-sdk-sns</artifactId>\n      <version>${aws-java.version}</version>\n    </dependency>\n    <dependency>\n      <groupId>com.amazonaws</groupId>\n      <artifactId>aws-java-sdk-sqs</artifactId>\n      <version>${aws-java.version}</version>\n    </dependency>\n    <dependency>\n      <groupId>com.amazonaws</groupId>\n      <artifactId>aws-java-sdk-sts</artifactId>\n      <version>${aws-java.version}</version>\n    </dependency>\n    <dependency>\n      <groupId>com.amazonaws</groupId>\n      <artifactId>amazon-kinesis-producer</artifactId>\n      <version>0.15.7</version>\n    </dependency>\n    <dependency>\n      <groupId>javax.xml.bind</groupId>\n      <artifactId>jaxb-api</artifactId>\n      <version>2.3.0</version>\n    </dependency>\n    <dependency>\n      <groupId>com.rabbitmq</groupId>\n      <artifactId>amqp-client</artifactId>\n      <version>5.18.0</version>\n    </dependency>\n    <dependency>\n      <groupId>com.google.cloud</groupId>\n      <artifactId>google-cloud-pubsub</artifactId>\n      <version>1.120.24</version>\n    </dependency>\n    <dependency>\n      <groupId>io.nats</groupId>\n      <artifactId>jnats</artifactId>\n      <version>2.8.0</version>\n    </dependency>\n    <dependency>\n      <groupId>com.google.protobuf</groupId>\n      <artifactId>protobuf-java</artifactId>\n      <version>3.21.7</version>\n    </dependency>\n    <dependency>\n      <groupId>redis.clients</groupId>\n      <artifactId>jedis</artifactId>\n      <version>3.5.1</version>\n    </dependency>\n\n    <!-- metrics -->\n    <dependency>\n      <groupId>io.dropwizard.metrics</groupId>\n      <artifactId>metrics-core</artifactId>\n      <version>4.1.17</version>\n    </dependency>\n    <dependency>\n      <groupId>io.dropwizard.metrics</groupId>\n      <artifactId>metrics-servlets</artifactId>\n      <version>4.1.17</version>\n    </dependency>\n    <dependency>\n      <groupId>io.dropwizard.metrics</groupId>\n      <artifactId>metrics-jmx</artifactId>\n      <version>4.1.17</version>\n    </dependency>\n    <dependency>\n      <groupId>org.eclipse.jetty</groupId>\n      <artifactId>jetty-server</artifactId>\n      <version>10.0.14</version>\n    </dependency>\n    <dependency>\n      <groupId>org.eclipse.jetty</groupId>\n      <artifactId>jetty-servlet</artifactId>\n      <version>10.0.12</version>\n    </dependency>\n    <dependency>\n      <groupId>com.viafoura</groupId>\n      <artifactId>metrics-datadog</artifactId>\n      <version>2.0.0-RC3</version>\n    </dependency>\n    <dependency>\n      <groupId>io.prometheus</groupId>\n      <artifactId>simpleclient_dropwizard</artifactId>\n      <version>0.9.0</version>\n    </dependency>\n    <dependency>\n      <groupId>io.prometheus</groupId>\n      <artifactId>simpleclient_servlet</artifactId>\n      <version>0.9.0</version>\n    </dependency>\n    <!-- OpenCensus used for pushing metrics to stackdriver -->\n    <dependency>\n        <groupId>io.opencensus</groupId>\n        <artifactId>opencensus-contrib-dropwizard</artifactId>\n        <version>${opencensus.version}</version>\n    </dependency>\n    <dependency>\n        <groupId>io.opencensus</groupId>\n        <artifactId>opencensus-api</artifactId>\n        <version>${opencensus.version}</version>\n    </dependency>\n    <dependency>\n        <groupId>io.opencensus</groupId>\n        <artifactId>opencensus-impl</artifactId>\n        <version>${opencensus.version}</version>\n        <scope>runtime</scope>\n    </dependency>\n    <dependency>\n        <groupId>io.opencensus</groupId>\n        <artifactId>opencensus-exporter-stats-stackdriver</artifactId>\n        <version>${opencensus.version}</version>\n    </dependency>\n\n    <!-- test -->\n    <dependency>\n      <groupId>org.hamcrest</groupId>\n      <artifactId>hamcrest-all</artifactId>\n      <version>1.3</version>\n      <scope>test</scope>\n    </dependency>\n    <dependency>\n      <groupId>junit</groupId>\n      <artifactId>junit</artifactId>\n      <version>4.13.1</version>\n      <scope>test</scope>\n    </dependency>\n    <dependency>\n      <groupId>org.mockito</groupId>\n      <artifactId>mockito-core</artifactId>\n      <version>${mockito.version}</version>\n      <scope>test</scope>\n    </dependency>\n    <dependency>\n      <groupId>org.mockito</groupId>\n      <artifactId>mockito-inline</artifactId>\n      <version>${mockito.version}</version>\n      <scope>test</scope>\n    </dependency>\n    <dependency>\n      <groupId>com.github.stefanbirkner</groupId>\n      <artifactId>system-rules</artifactId>\n      <version>1.18.0</version>\n      <scope>test</scope>\n    </dependency>\n\n  </dependencies>\n\n  <build>\n    <plugins>\n      <!-- horrible sonatype plugins -->\n      <plugin>\n        <groupId>org.sonatype.plugins</groupId>\n        <artifactId>nexus-staging-maven-plugin</artifactId>\n        <version>1.6.6</version>\n        <extensions>true</extensions>\n        <configuration>\n          <serverId>ossrh</serverId>\n          <nexusUrl>https://oss.sonatype.org/</nexusUrl>\n          <autoReleaseAfterClose>true</autoReleaseAfterClose>\n          <stagingProgressTimeoutMinutes>10</stagingProgressTimeoutMinutes>\n          <keepStagingRepositoryOnCloseRuleFailure>true</keepStagingRepositoryOnCloseRuleFailure>\n        </configuration>\n      </plugin>\n      <plugin>\n        <groupId>org.apache.maven.plugins</groupId>\n        <artifactId>maven-surefire-plugin</artifactId>\n        <configuration>\n          <!-- Allow Mockito to access the test classe private variables -->\n          <argLine>--add-opens java.base/java.util=ALL-UNNAMED</argLine>\n        </configuration>\n      </plugin>\n      <plugin>\n        <groupId>org.apache.maven.plugins</groupId>\n        <artifactId>maven-gpg-plugin</artifactId>\n        <version>1.5</version>\n        <executions>\n          <execution>\n            <id>sign-artifacts</id>\n            <phase>verify</phase>\n            <goals>\n              <goal>sign</goal>\n            </goals>\n          </execution>\n        </executions>\n      </plugin>\n      <plugin>\n        <groupId>org.apache.maven.plugins</groupId>\n        <artifactId>maven-source-plugin</artifactId>\n        <version>3.0.1</version>\n        <executions>\n          <execution>\n            <id>attach-sources</id>\n            <goals>\n              <goal>jar-no-fork</goal>\n            </goals>\n          </execution>\n        </executions>\n      </plugin>\n      <plugin>\n        <groupId>org.apache.maven.plugins</groupId>\n        <artifactId>maven-javadoc-plugin</artifactId>\n        <version>3.3.1</version>\n        <configuration>\n            <source>11</source>\n        </configuration>\n        <executions>\n          <execution>\n            <id>attach-javadocs</id>\n            <goals>\n              <goal>jar</goal>\n            </goals>\n          </execution>\n        </executions>\n      </plugin>\n      <plugin>\n        <groupId>org.apache.maven.plugins</groupId>\n        <artifactId>maven-compiler-plugin</artifactId>\n        <version>3.8.1</version>\n        <configuration>\n          <release>11</release>\n        </configuration>\n      </plugin>\n      <plugin>\n        <groupId>org.antlr</groupId>\n        <artifactId>antlr4-maven-plugin</artifactId>\n        <version>4.8-1</version>\n        <executions>\n          <execution>\n            <id>antlr</id>\n            <goals>\n              <goal>antlr4</goal>\n            </goals>\n          </execution>\n        </executions>\n      </plugin>\n\n      <plugin>\n        <groupId>org.apache.maven.plugins</groupId>\n        <artifactId>maven-assembly-plugin</artifactId>\n        <version>3.1.0</version>\n        <configuration>\n          <appendAssemblyId>false</appendAssemblyId>\n          <tarLongFileMode>posix</tarLongFileMode>\n          <descriptors>\n            <descriptor>src/main/assembly/assembly.xml</descriptor>\n          </descriptors>\n        </configuration>\n        <executions>\n          <execution>\n            <phase>package</phase>\n            <goals>\n              <goal>single</goal>\n            </goals>\n          </execution>\n        </executions>\n      </plugin>\n\n      <plugin>\n        <groupId>org.apache.maven.plugins</groupId>\n        <artifactId>maven-jar-plugin</artifactId>\n        <version>3.0.2</version>\n        <configuration>\n          <archive>\n            <manifest>\n              <addDefaultImplementationEntries>true</addDefaultImplementationEntries>\n              <addDefaultSpecificationEntries>true</addDefaultSpecificationEntries>\n            </manifest>\n          </archive>\n        </configuration>\n      </plugin>\n\n      <plugin>\n        <artifactId>maven-dependency-plugin</artifactId>\n        <executions>\n          <execution>\n            <phase>compile</phase>\n            <goals>\n              <goal>copy-dependencies</goal>\n            </goals>\n            <configuration>\n              <outputDirectory>${project.build.directory}/lib</outputDirectory>\n              <excludeArtifactIds>kafka-clients</excludeArtifactIds>\n              <includeScope>runtime</includeScope>\n            </configuration>\n          </execution>\n          <execution>\n            <id>kafka-clients</id>\n            <phase>compile</phase>\n            <goals>\n              <goal>copy-dependencies</goal>\n            </goals>\n            <configuration>\n              <outputDirectory>${project.build.directory}/lib/kafka-clients</outputDirectory>\n              <includeArtifactIds>kafka-clients</includeArtifactIds>\n            </configuration>\n          </execution>\n        </executions>\n      </plugin>\n\n    </plugins>\n    <pluginManagement>\n      <plugins>\n        <!--This plugin's configuration is used to store Eclipse m2e settings only. It has no influence on the Maven build itself.-->\n        <plugin>\n          <groupId>org.eclipse.m2e</groupId>\n          <artifactId>lifecycle-mapping</artifactId>\n          <version>1.0.0</version>\n          <configuration>\n            <lifecycleMappingMetadata>\n              <pluginExecutions>\n                <pluginExecution>\n                  <pluginExecutionFilter>\n                    <groupId>org.apache.maven.plugins</groupId>\n                    <artifactId>maven-dependency-plugin</artifactId>\n                    <versionRange>[2.8,)</versionRange>\n                    <goals>\n                      <goal>copy-dependencies</goal>\n                    </goals>\n                  </pluginExecutionFilter>\n                  <action>\n                    <ignore/>\n                  </action>\n                </pluginExecution>\n              </pluginExecutions>\n            </lifecycleMappingMetadata>\n          </configuration>\n        </plugin>\n      </plugins>\n    </pluginManagement>\n  </build>\n</project>\n"
        },
        {
          "name": "raft.xml.example",
          "type": "blob",
          "size": 0.9189453125,
          "content": "<?xml version='1.0' encoding='utf-8'?>\n<config xmlns=\"urn:org:jgroups\"\n        xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n        xsi:schemaLocation=\"urn:org:jgroups http://www.jgroups.org/schema/jgroups.xsd\">\n    <UDP mcast_addr=\"228.8.8.8\" mcast_port=\"${jgroups.udp.mcast_port:45588}\"/>\n    <PING />\n    <MERGE3 />\n    <FD_SOCK/>\n    <FD_ALL/>\n    <VERIFY_SUSPECT timeout=\"1500\"/>\n    <pbcast.NAKACK2 xmit_interval=\"500\"/>\n    <UNICAST3 xmit_interval=\"500\"/>\n    <pbcast.STABLE desired_avg_gossip=\"50000\" max_bytes=\"4M\"/>\n    <raft.NO_DUPES/>\n    <pbcast.GMS print_local_addr=\"true\" join_timeout=\"2000\"/>\n    <UFC max_credits=\"2M\" min_threshold=\"0.4\"/>\n    <MFC max_credits=\"2M\" min_threshold=\"0.4\"/>\n    <FRAG2 frag_size=\"60K\"/>\n    <raft.ELECTION election_min_interval=\"500\" election_max_interval=\"1000\" heartbeat_interval=\"250\"/>\n    <raft.RAFT members=\"A,B,C\" raft_id=\"${raft_id:undefined}\"/>\n    <raft.REDIRECT/>\n</config>\n"
        },
        {
          "name": "src",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}