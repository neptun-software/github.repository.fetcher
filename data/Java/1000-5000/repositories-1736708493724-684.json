{
  "metadata": {
    "timestamp": 1736708493724,
    "page": 684,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjcwMA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "apache/parquet-java",
      "stars": 2685,
      "defaultBranch": "master",
      "files": [
        {
          "name": ".asf.yaml",
          "type": "blob",
          "size": 1.2900390625,
          "content": "#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n#\n# https://cwiki.apache.org/confluence/display/INFRA/git+-+.asf.yaml+features\n#\ngithub:\n  description: \"Apache Parquet Java\"\n  homepage: https://parquet.apache.org/\n  labels:\n    - apache\n    - parquet\n    - parquet-java\n  \n  enabled_merge_buttons:\n    merge: false\n    squash: true\n    rebase: false\n  \n  features:\n    wiki: false\n    issues: true\n    projects: false\n\nnotifications:\n  commits:      commits@parquet.apache.org\n  issues:       issues@parquet.apache.org\n  pullrequests: issues@parquet.apache.org\n  jira_options: link\n"
        },
        {
          "name": ".editorconfig",
          "type": "blob",
          "size": 2.236328125,
          "content": "#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n#\nroot = true\n\n[*]\ncharset = utf-8\nend_of_line = lf\ninsert_final_newline = true\nindent_style = space\ntrim_trailing_whitespace = true\n\n[*.scala]\nindent_size = 2\n\n[*.java]\nindent_size = 2\nmax_line_length = 120\nij_java_space_within_empty_method_parentheses = false\nij_java_blank_lines_before_method_body = 0\nij_java_keep_blank_lines_before_right_brace = 0\nij_java_space_before_annotation_array_initializer_left_brace = false\nij_java_space_before_array_initializer_left_brace = true\nij_java_layout_static_imports_separately = true\nij_java_insert_inner_class_imports = false\nij_java_use_single_class_imports = true\nij_java_continuation_indent_size = 4\nij_java_use_relative_indents = false\nij_java_keep_indents_on_empty_lines = false\nij_java_indent_case_from_switch = true\nij_java_do_not_indent_top_level_class_members = false\nij_java_keep_builder_methods_indents = true\nij_java_line_comment_add_space = true\nij_java_space_after_type_cast = true\nij_java_align_types_in_multi_catch = true\nij_java_space_after_comma_in_type_arguments = true\nij_java_blank_lines_around_method_in_interface = 1\nij_java_method_parameters_wrap = normal\nij_java_keep_simple_lambdas_in_one_line = true\nij_java_replace_sum_lambda_with_method_ref = true\nij_java_spaces_around_lambda_arrow = true\nij_java_method_brace_style = end_of_line\nij_java_align_group_field_declarations = false\nij_java_imports_layout = $*,|,*\n\n[*.yaml]\nindent_size = 2\n\n[*.xml]\nindent_size = 2\n\n# Tab indentation (no size specified)\n[Makefile]\nindent_style = tab\n"
        },
        {
          "name": ".gitattributes",
          "type": "blob",
          "size": 0.81640625,
          "content": "#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n#\n* text eol=lf\n*.png binary\ncore.autocrlf=false\n"
        },
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.197265625,
          "content": "*.class\n.project\n.classpath\n.settings\ntarget\n# Package Files #\n*.jar\n*.war\n*.ear\n*.iml\n*.ipr\n*.iws\n*.orig\n*.rej\ndependency-reduced-pom.xml\n.idea/*\ntarget/\n.cache\n*~\nmvn_install.log\n.vscode/*\n.DS_Store\n\n"
        },
        {
          "name": ".mvn",
          "type": "tree",
          "content": null
        },
        {
          "name": "CHANGES.md",
          "type": "blob",
          "size": 108.3134765625,
          "content": "<!--\n  ~ Licensed to the Apache Software Foundation (ASF) under one\n  ~ or more contributor license agreements.  See the NOTICE file\n  ~ distributed with this work for additional information\n  ~ regarding copyright ownership.  The ASF licenses this file\n  ~ to you under the Apache License, Version 2.0 (the\n  ~ \"License\"); you may not use this file except in compliance\n  ~ with the License.  You may obtain a copy of the License at\n  ~\n  ~   http://www.apache.org/licenses/LICENSE-2.0\n  ~\n  ~ Unless required by applicable law or agreed to in writing,\n  ~ software distributed under the License is distributed on an\n  ~ \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n  ~ KIND, either express or implied.  See the License for the\n  ~ specific language governing permissions and limitations\n  ~ under the License.\n  -->\n\n# Parquet #\n\nFrom 1.14.2 onwards, the Parquet project has migrated from Jira to GitHub, and the changelogs are now [published under Releases](https://github.com/apache/parquet-java/releases).\n\n### Version 1.14.1 ###\n\nRelease Notes - Parquet - Version 1.14.1\n\n#### Bug\n\n*   [PARQUET-2468](https://issues.apache.org/jira/browse/PARQUET-2468) - ParquetMetadata.toPrettyJSON throws exception on file read when LOG.isDebugEnabled()\n*   [PARQUET-2498](https://issues.apache.org/jira/browse/PARQUET-2498) - Hadoop vector IO API doesn't handle empty list of ranges\n\n### Version 1.14.0 ###\n\nRelease Notes - Parquet - Version 1.14.0\n\n#### Bug\n\n*   [PARQUET-2260](https://issues.apache.org/jira/browse/PARQUET-2260) - Bloom filter bytes size shouldn't be larger than maxBytes size in the configuration\n*   [PARQUET-2266](https://issues.apache.org/jira/browse/PARQUET-2266) - Fix support for files without ColumnIndexes\n*   [PARQUET-2276](https://issues.apache.org/jira/browse/PARQUET-2276) - ParquetReader reads do not work with Hadoop version 2.8.5\n*   [PARQUET-2300](https://issues.apache.org/jira/browse/PARQUET-2300) - Update jackson-core 2.13.4 to a version without CVE PRISMA-2023-0067\n*   [PARQUET-2325](https://issues.apache.org/jira/browse/PARQUET-2325) - Fix parquet-cli's dictionary subcommand to work with FIXED_LEN_BYTE_ARRAY\n*   [PARQUET-2329](https://issues.apache.org/jira/browse/PARQUET-2329) - Fix wrong help messages of parquet-cli subcommands\n*   [PARQUET-2330](https://issues.apache.org/jira/browse/PARQUET-2330) - Fix convert-csv to show the correct position of the invalid record\n*   [PARQUET-2332](https://issues.apache.org/jira/browse/PARQUET-2332) - Fix unexpectedly disabled tests to be executed\n*   [PARQUET-2336](https://issues.apache.org/jira/browse/PARQUET-2336) - Add caching key to CodecFactory\n*   [PARQUET-2342](https://issues.apache.org/jira/browse/PARQUET-2342) - Parquet writer produced a corrupted file due to page value count overflow\n*   [PARQUET-2343](https://issues.apache.org/jira/browse/PARQUET-2343) - Fixes NPE when rewriting file with multiple rowgroups\n*   [PARQUET-2348](https://issues.apache.org/jira/browse/PARQUET-2348) - Recompression/Re-encrypt should rewrite bloomfilter\n*   [PARQUET-2354](https://issues.apache.org/jira/browse/PARQUET-2354) - Apparent race condition in CharsetValidator\n*   [PARQUET-2363](https://issues.apache.org/jira/browse/PARQUET-2363) - ParquetRewriter should encrypt the V2 page header\n*   [PARQUET-2365](https://issues.apache.org/jira/browse/PARQUET-2365) - Fixes NPE when rewriting column without column index\n*   [PARQUET-2408](https://issues.apache.org/jira/browse/PARQUET-2408) - Fix license header in .gitattributes\n*   [PARQUET-2420](https://issues.apache.org/jira/browse/PARQUET-2420) - ThriftParquetWriter converts thrift byte to int32 without adding logical type\n*   [PARQUET-2429](https://issues.apache.org/jira/browse/PARQUET-2429) - Direct buffer churn in NonBlockedDecompressor\n*   [PARQUET-2438](https://issues.apache.org/jira/browse/PARQUET-2438) - Fixes minMaxSize for BinaryColumnIndexBuilder\n*   [PARQUET-2442](https://issues.apache.org/jira/browse/PARQUET-2442) - Remove Parquet Site from parquet-mr\n*   [PARQUET-2448](https://issues.apache.org/jira/browse/PARQUET-2448) - parquet-avro does not support nested logical-type for avro <= 1.8\n*   [PARQUET-2449](https://issues.apache.org/jira/browse/PARQUET-2449) - Writing using LocalOutputFile creates a large buffer\n*   [PARQUET-2450](https://issues.apache.org/jira/browse/PARQUET-2450) - ParquetAvroReader throws exception projecting a single field of a repeated record type\n*   [PARQUET-2456](https://issues.apache.org/jira/browse/PARQUET-2456) - avro schema conversion may fail with name conflict when using fixed types\n*   [PARQUET-2457](https://issues.apache.org/jira/browse/PARQUET-2457) - Missing maven-scala-plugin version\n*   [PARQUET-2458](https://issues.apache.org/jira/browse/PARQUET-2458) - Java compiler should use release instead of source/target\n*   [PARQUET-2465](https://issues.apache.org/jira/browse/PARQUET-2465) - Fall back to Hadoop Configuration\n\n#### New Feature\n\n*   [PARQUET-1647](https://issues.apache.org/jira/browse/PARQUET-1647) - Java support for Arrow's float16\n*   [PARQUET-2171](https://issues.apache.org/jira/browse/PARQUET-2171) - Implement vectored IO in parquet file format\n*   [PARQUET-2318](https://issues.apache.org/jira/browse/PARQUET-2318) - Implement a tool to list page headers\n\n#### Improvement\n\n*   [PARQUET-1629](https://issues.apache.org/jira/browse/PARQUET-1629) - Page-level CRC checksum verification for DataPageV2\n*   [PARQUET-1822](https://issues.apache.org/jira/browse/PARQUET-1822) - Parquet without Hadoop dependencies\n*   [PARQUET-1942](https://issues.apache.org/jira/browse/PARQUET-1942) - Bump Apache Arrow 2.0.0\n*   [PARQUET-2060](https://issues.apache.org/jira/browse/PARQUET-2060) - Parquet corruption can cause infinite loop with Snappy\n*   [PARQUET-2212](https://issues.apache.org/jira/browse/PARQUET-2212) - Add ByteBuffer api for decryptors to allow direct memory to be decrypted\n*   [PARQUET-2254](https://issues.apache.org/jira/browse/PARQUET-2254) - Build a BloomFilter with a more precise size\n*   [PARQUET-2263](https://issues.apache.org/jira/browse/PARQUET-2263) - Upgrade maven-shade-plugin to 3.4.1\n*   [PARQUET-2265](https://issues.apache.org/jira/browse/PARQUET-2265) - AvroParquetWriter should default to data supplier model from Configuration\n*   [PARQUET-2267](https://issues.apache.org/jira/browse/PARQUET-2267) - Add dependabot to update dependencies\n*   [PARQUET-2268](https://issues.apache.org/jira/browse/PARQUET-2268) - Bump Thrift to 0.18.1\n*   [PARQUET-2272](https://issues.apache.org/jira/browse/PARQUET-2272) - Bump protobuf-java from 3.17.3 to 3.19.6\n*   [PARQUET-2273](https://issues.apache.org/jira/browse/PARQUET-2273) - Remove Travis from the repository\n*   [PARQUET-2274](https://issues.apache.org/jira/browse/PARQUET-2274) - Remove Yetus\n*   [PARQUET-2275](https://issues.apache.org/jira/browse/PARQUET-2275) - Upgrade `cyclonedx-maven-plugin` to 2.7.6\n*   [PARQUET-2277](https://issues.apache.org/jira/browse/PARQUET-2277) - Bump hadoop.version from 3.2.3 to 3.3.5\n*   [PARQUET-2278](https://issues.apache.org/jira/browse/PARQUET-2278) - Bump re2j from 1.1 to 1.7\n*   [PARQUET-2279](https://issues.apache.org/jira/browse/PARQUET-2279) - Bump slf4j.version from 1.7.22 to 1.7.33\n*   [PARQUET-2280](https://issues.apache.org/jira/browse/PARQUET-2280) - Bump h2 from 2.1.210 to 2.1.214\n*   [PARQUET-2282](https://issues.apache.org/jira/browse/PARQUET-2282) - Dont initialize HadoopCodec\n*   [PARQUET-2283](https://issues.apache.org/jira/browse/PARQUET-2283) - Remove Hadoop HiddenFileFilter\n*   [PARQUET-2290](https://issues.apache.org/jira/browse/PARQUET-2290) - Add CI for Hadoop 2\n*   [PARQUET-2291](https://issues.apache.org/jira/browse/PARQUET-2291) - Remove lingering japicmp exclusions\n*   [PARQUET-2292](https://issues.apache.org/jira/browse/PARQUET-2292) - Improve default SpecificRecord model selection for Avro{Write,Read}Support\n*   [PARQUET-2293](https://issues.apache.org/jira/browse/PARQUET-2293) - Bump guava from 27.0.1-jre to 31.1-jre\n*   [PARQUET-2294](https://issues.apache.org/jira/browse/PARQUET-2294) - Bump fastutil from 8.4.2 to 8.5.12\n*   [PARQUET-2295](https://issues.apache.org/jira/browse/PARQUET-2295) - Bump truth-proto-extension from 1.0 to 1.1.3\n*   [PARQUET-2296](https://issues.apache.org/jira/browse/PARQUET-2296) - Bump easymock from 3.4 to 5.1.0\n*   [PARQUET-2297](https://issues.apache.org/jira/browse/PARQUET-2297) - Encrypted files should not be checked for delta encoding problem\n*   [PARQUET-2301](https://issues.apache.org/jira/browse/PARQUET-2301) - Add missing argument in ParquetRewriter logging\n*   [PARQUET-2302](https://issues.apache.org/jira/browse/PARQUET-2302) - Bump joda-time from 2.9.7 to 2.12.5\n*   [PARQUET-2303](https://issues.apache.org/jira/browse/PARQUET-2303) - Bump cyclonedx-maven-plugin from 2.7.6 to 2.7.9\n*   [PARQUET-2304](https://issues.apache.org/jira/browse/PARQUET-2304) - Bump buildnumber-maven-plugin from 1.1 to 3.1.0\n*   [PARQUET-2305](https://issues.apache.org/jira/browse/PARQUET-2305) - Allow Parquet to Proto conversion even though Target Schema has less fields\n*   [PARQUET-2307](https://issues.apache.org/jira/browse/PARQUET-2307) - Bump zero-allocation-hashing from 0.9 to 0.16\n*   [PARQUET-2308](https://issues.apache.org/jira/browse/PARQUET-2308) - Bump powermock.version from 2.0.2 to 2.0.9\n*   [PARQUET-2309](https://issues.apache.org/jira/browse/PARQUET-2309) - Bump site-maven-plugin from 0.8 to 0.12\n*   [PARQUET-2312](https://issues.apache.org/jira/browse/PARQUET-2312) - Bump snappy-java from 1.1.8.3 to 1.1.10.1 in /parquet-hadoop\n*   [PARQUET-2314](https://issues.apache.org/jira/browse/PARQUET-2314) - Bump jackson.version from 2.15.0 to 2.15.2\n*   [PARQUET-2319](https://issues.apache.org/jira/browse/PARQUET-2319) - Upgrade Avro to version 1.11.2\n*   [PARQUET-2320](https://issues.apache.org/jira/browse/PARQUET-2320) - Bump jackson-databind from 2.14.2 to 2.15.2\n*   [PARQUET-2322](https://issues.apache.org/jira/browse/PARQUET-2322) - Bump h2 from 2.1.214 to 2.2.220 in /parquet-column\n*   [PARQUET-2324](https://issues.apache.org/jira/browse/PARQUET-2324) - Bump cobertura-maven-plugin from 2.5.2 to 2.7\n*   [PARQUET-2326](https://issues.apache.org/jira/browse/PARQUET-2326) - Bump jcommander from 1.72 to 1.82\n*   [PARQUET-2328](https://issues.apache.org/jira/browse/PARQUET-2328) - Add overwrite option to the parquet-cli's rewrite subcommand\n*   [PARQUET-2331](https://issues.apache.org/jira/browse/PARQUET-2331) - Allow convert-csv to take multiple input files\n*   [PARQUET-2333](https://issues.apache.org/jira/browse/PARQUET-2333) - Support bzip2 and xz compressions in the to-avro subcommand\n*   [PARQUET-2334](https://issues.apache.org/jira/browse/PARQUET-2334) - Allow the cat subcommand to take multiple files\n*   [PARQUET-2335](https://issues.apache.org/jira/browse/PARQUET-2335) - Allow the scan subcommand to take multiple files\n*   [PARQUET-2347](https://issues.apache.org/jira/browse/PARQUET-2347) - Add interface layer between Parquet and Hadoop Configuration\n*   [PARQUET-2349](https://issues.apache.org/jira/browse/PARQUET-2349) - Move from deprecated BytesCompressor/Decompressor to BytesInputCompressor/Decompressor\n*   [PARQUET-2357](https://issues.apache.org/jira/browse/PARQUET-2357) - Modest refactor of CapacityByteArrayOutputStream\n*   [PARQUET-2359](https://issues.apache.org/jira/browse/PARQUET-2359) - Simple Parquet Configuration implementation\n*   [PARQUET-2364](https://issues.apache.org/jira/browse/PARQUET-2364) - Encrypt all columns option\n*   [PARQUET-2366](https://issues.apache.org/jira/browse/PARQUET-2366) - Optimize random seek during rewriting\n*   [PARQUET-2368](https://issues.apache.org/jira/browse/PARQUET-2368) - Update japicmp to 1.18.1\n*   [PARQUET-2370](https://issues.apache.org/jira/browse/PARQUET-2370) - Crypto factory activation of \"all column encryption\" mode\n*   [PARQUET-2371](https://issues.apache.org/jira/browse/PARQUET-2371) - Resolve japicmp failure for CI\n*   [PARQUET-2372](https://issues.apache.org/jira/browse/PARQUET-2372) - Avoid unnecessary reading of RowGroup data during rewriting\n*   [PARQUET-2373](https://issues.apache.org/jira/browse/PARQUET-2373) - Improve I/O performance with bloom_filter_length\n*   [PARQUET-2374](https://issues.apache.org/jira/browse/PARQUET-2374) - Add metrics support for parquet file reader\n*   [PARQUET-2375](https://issues.apache.org/jira/browse/PARQUET-2375) - Extend vectorized bit unpacking benchmark for various bit sizes.\n*   [PARQUET-2380](https://issues.apache.org/jira/browse/PARQUET-2380) - Decouple RewriteOptions from Hadoop classes\n*   [PARQUET-2383](https://issues.apache.org/jira/browse/PARQUET-2383) - Bump parquet-format to 2.10.0\n*   [PARQUET-2384](https://issues.apache.org/jira/browse/PARQUET-2384) - Mark toOriginalType as deprecated\n*   [PARQUET-2385](https://issues.apache.org/jira/browse/PARQUET-2385) - Don't initialize CodecFactory in ParquetWriter\n*   [PARQUET-2386](https://issues.apache.org/jira/browse/PARQUET-2386) - More consistent code style in parquet-mr\n*   [PARQUET-2387](https://issues.apache.org/jira/browse/PARQUET-2387) - Simplify `hasFieldsIgnored` expression\n*   [PARQUET-2388](https://issues.apache.org/jira/browse/PARQUET-2388) - Deprecate `CHARSETS` on `PlainValuesWriter`\n*   [PARQUET-2389](https://issues.apache.org/jira/browse/PARQUET-2389) - Remove redundant initializers\n*   [PARQUET-2390](https://issues.apache.org/jira/browse/PARQUET-2390) - Replace anonymouse functions with lambda's\n*   [PARQUET-2391](https://issues.apache.org/jira/browse/PARQUET-2391) - Remove unnecessary unboxing\n*   [PARQUET-2392](https://issues.apache.org/jira/browse/PARQUET-2392) - Remove StringBuilder in `LogicalTypeAnnotation`\n*   [PARQUET-2393](https://issues.apache.org/jira/browse/PARQUET-2393) - Make `ColumnIOCreatorVisitor` static\n*   [PARQUET-2394](https://issues.apache.org/jira/browse/PARQUET-2394) - Use `computeIfAbsent` in `MessageColumnIO`\n*   [PARQUET-2395](https://issues.apache.org/jira/browse/PARQUET-2395) - Prefer `singletonList` over `asList`\n*   [PARQUET-2396](https://issues.apache.org/jira/browse/PARQUET-2396) - Refactor `ColumnIndexBuilder`\n*   [PARQUET-2397](https://issues.apache.org/jira/browse/PARQUET-2397) - Make use of `isEmpty`\n*   [PARQUET-2398](https://issues.apache.org/jira/browse/PARQUET-2398) - Make static variables final\n*   [PARQUET-2399](https://issues.apache.org/jira/browse/PARQUET-2399) - Use deprecated tag in Javadoc\n*   [PARQUET-2400](https://issues.apache.org/jira/browse/PARQUET-2400) - Update Spotless command in PR prompt to include vector plugins\n*   [PARQUET-2401](https://issues.apache.org/jira/browse/PARQUET-2401) - Synchronize on final fields\n*   [PARQUET-2406](https://issues.apache.org/jira/browse/PARQUET-2406) - Remove redundant valueOf calls\n*   [PARQUET-2407](https://issues.apache.org/jira/browse/PARQUET-2407) - Add custom .asf.yaml for finer-grained control of email notifications\n*   [PARQUET-2410](https://issues.apache.org/jira/browse/PARQUET-2410) - Use row count instead of value count to get row count from OffsetIndex\n*   [PARQUET-2413](https://issues.apache.org/jira/browse/PARQUET-2413) - Support custom file footer metadata via ParquetWriter\n*   [PARQUET-2417](https://issues.apache.org/jira/browse/PARQUET-2417) - Update NOTICE\n*   [PARQUET-2419](https://issues.apache.org/jira/browse/PARQUET-2419) - Reduce noisy logging when running test suite\n*   [PARQUET-2422](https://issues.apache.org/jira/browse/PARQUET-2422) - Prevent unwrapping of Hadoop filestreams\n*   [PARQUET-2425](https://issues.apache.org/jira/browse/PARQUET-2425) - AvroSchemaConverter doesn't support non-grouped repeated fields\n*   [PARQUET-2426](https://issues.apache.org/jira/browse/PARQUET-2426) - Add lz4_raw compression to README\n*   [PARQUET-2428](https://issues.apache.org/jira/browse/PARQUET-2428) - Make RawPagesReader support specified columns\n*   [PARQUET-2432](https://issues.apache.org/jira/browse/PARQUET-2432) - Use ByteBufferAllocator instead of hardcoded heap allocation\n*   [PARQUET-2436](https://issues.apache.org/jira/browse/PARQUET-2436) - More optimal memory usage in compression codecs\n*   [PARQUET-2437](https://issues.apache.org/jira/browse/PARQUET-2437) - Avoid flushing at Parquet writes after an exception\n*   [PARQUET-2439](https://issues.apache.org/jira/browse/PARQUET-2439) - Upgrade ZSTD-JNI to 1.5.5-11\n*   [PARQUET-2445](https://issues.apache.org/jira/browse/PARQUET-2445) - Fix log exception when FieldsMarker.visitedIndexes is empty\n*   [PARQUET-2446](https://issues.apache.org/jira/browse/PARQUET-2446) - ProtoParquetWriter Not Support DynamicMessage\n*   [PARQUET-2451](https://issues.apache.org/jira/browse/PARQUET-2451) - Add BYTE_STREAM_SPLIT support for FIXED_LEN_BYTE_ARRAY, INT32 and INT64\n*   [PARQUET-2453](https://issues.apache.org/jira/browse/PARQUET-2453) - Add build-helper-maven-plugin for parquet-column/common module\n*   [PARQUET-2454](https://issues.apache.org/jira/browse/PARQUET-2454) - Invoking flush before closing the output stream in ParquetFileWriter\n*   [PARQUET-2463](https://issues.apache.org/jira/browse/PARQUET-2463) - Bump japicmp to 0.21.0\n\n#### Test\n\n*   [PARQUET-2361](https://issues.apache.org/jira/browse/PARQUET-2361) - Reduce failure rate of unit test testParquetFileWithBloomFilterWithFpp\n\n#### Task\n\n*   [PARQUET-2418](https://issues.apache.org/jira/browse/PARQUET-2418) - Add integration test for BYTE_STREAM_SPLIT\n\n### Version 1.13.1 ###\n\nRelease Notes - Parquet - Version 1.13.1\n\n#### Improvement\n\n*   [PARQUET-2276](https://issues.apache.org/jira/browse/PARQUET-2276) - Bring back support for Hadoop 2.7.3\n*   [PARQUET-2297](https://issues.apache.org/jira/browse/PARQUET-2297) - Skip delta problem check\n*   [PARQUET-2292](https://issues.apache.org/jira/browse/PARQUET-2292) - Improve default SpecificRecord model selection for Avro `{Write,Read}`Support\n*   [PARQUET-2290](https://issues.apache.org/jira/browse/PARQUET-2290) - Add CI for Hadoop 2\n*   [PARQUET-2282](https://issues.apache.org/jira/browse/PARQUET-2282) - Don't initialize HadoopCodec\n*   [PARQUET-2283](https://issues.apache.org/jira/browse/PARQUET-2283) - Remove Hadoop HiddenFileFilter\n*   [PARQUET-2081](https://issues.apache.org/jira/browse/PARQUET-2081) - Fix support for rewriting files without ColumnIndexes\n\n### Version 1.13.0 ###\n\nRelease Notes - Parquet - Version 1.13.0\n\n#### New Feature\n\n*   [PARQUET-1020](https://issues.apache.org/jira/browse/PARQUET-1020) - Add support for Dynamic Messages in parquet-protobuf\n\n#### Task\n\n*   [PARQUET-2230](https://issues.apache.org/jira/browse/PARQUET-2230) - Add a new rewrite command powered by ParquetRewriter\n*   [PARQUET-2228](https://issues.apache.org/jira/browse/PARQUET-2228) - ParquetRewriter supports more than one input file\n*   [PARQUET-2229](https://issues.apache.org/jira/browse/PARQUET-2229) - ParquetRewriter supports masking and encrypting the same column\n*   [PARQUET-2227](https://issues.apache.org/jira/browse/PARQUET-2227) - Refactor different file rewriters to use single implementation\n\n#### Improvement\n\n*   [PARQUET-2258](https://issues.apache.org/jira/browse/PARQUET-2258) - Storing toString fields in FilterPredicate instances can lead to memory pressure\n*   [PARQUET-2252](https://issues.apache.org/jira/browse/PARQUET-2252) - Make some methods public to allow external projects to implement page skipping\n*   [PARQUET-2159](https://issues.apache.org/jira/browse/PARQUET-2159) - Vectorized BytePacker decoder using Java VectorAPI\n*   [PARQUET-2246](https://issues.apache.org/jira/browse/PARQUET-2246) - Add short circuit logic to column index filter\n*   [PARQUET-2226](https://issues.apache.org/jira/browse/PARQUET-2226) - Support merge Bloom Filters\n*   [PARQUET-2224](https://issues.apache.org/jira/browse/PARQUET-2224) - Publish SBOM artifacts\n*   [PARQUET-2208](https://issues.apache.org/jira/browse/PARQUET-2208) - Add details to nested column encryption config doc and exception text\n*   [PARQUET-2195](https://issues.apache.org/jira/browse/PARQUET-2195) - Add scan command to parquet-cli\n*   [PARQUET-2196](https://issues.apache.org/jira/browse/PARQUET-2196) - Support LZ4_RAW codec\n*   [PARQUET-2176](https://issues.apache.org/jira/browse/PARQUET-2176) - Column index/statistics truncation in ParquetWriter\n*   [PARQUET-2197](https://issues.apache.org/jira/browse/PARQUET-2197) - Document uniform encryption\n*   [PARQUET-2191](https://issues.apache.org/jira/browse/PARQUET-2191) - Upgrade Scala to 2.12.17\n*   [PARQUET-2169](https://issues.apache.org/jira/browse/PARQUET-2169) - Upgrade Avro to version 1.11.1\n*   [PARQUET-2155](https://issues.apache.org/jira/browse/PARQUET-2155) - Upgrade protobuf version to 3.17.3\n*   [PARQUET-2158](https://issues.apache.org/jira/browse/PARQUET-2158) - Upgrade Hadoop dependency to version 3.2.0\n*   [PARQUET-2138](https://issues.apache.org/jira/browse/PARQUET-2138) - Add ShowBloomFilterCommand to parquet-cli\n*   [PARQUET-2157](https://issues.apache.org/jira/browse/PARQUET-2157) - Add BloomFilter fpp config\n\n#### Bug\n\n*   [PARQUET-2202](https://issues.apache.org/jira/browse/PARQUET-2202) - Redundant String allocation on the hot path in CapacityByteArrayOutputStream.setByte\n*   [PARQUET-2164](https://issues.apache.org/jira/browse/PARQUET-2164) - CapacityByteArrayOutputStream overflow while writing causes negative row group sizes to be written\n*   [PARQUET-2103](https://issues.apache.org/jira/browse/PARQUET-2103) - Fix crypto exception in print toPrettyJSON\n*   [PARQUET-2251](https://issues.apache.org/jira/browse/PARQUET-2251) - Avoid generating Bloomfilter when all pages of a column are encoded by dictionary\n*   [PARQUET-2243](https://issues.apache.org/jira/browse/PARQUET-2243) - Support zstd-jni in DirectCodecFactory\n*   [PARQUET-2247](https://issues.apache.org/jira/browse/PARQUET-2247) - Fail-fast if CapacityByteArrayOutputStream write overflow\n*   [PARQUET-2241](https://issues.apache.org/jira/browse/PARQUET-2241) - Fix ByteStreamSplitValuesReader with nulls\n*   [PARQUET-2244](https://issues.apache.org/jira/browse/PARQUET-2244) - Fix notIn for columns with null values\n*   [PARQUET-2173](https://issues.apache.org/jira/browse/PARQUET-2173) - Fix parquet build against hadoop 3.3.3+\n*   [PARQUET-2219](https://issues.apache.org/jira/browse/PARQUET-2219) - ParquetFileReader skips empty row group\n*   [PARQUET-2198](https://issues.apache.org/jira/browse/PARQUET-2198) - Updating jackson data bind version to fix CVEs\n*   [PARQUET-2177](https://issues.apache.org/jira/browse/PARQUET-2177) - Fix parquet-cli not to fail showing descriptions\n*   [PARQUET-1711](https://issues.apache.org/jira/browse/PARQUET-1711) - Support recursive proto schemas by limiting recursion depth\n*   [PARQUET-2142](https://issues.apache.org/jira/browse/PARQUET-2142) - parquet-cli without hadoop throws java.lang.NoSuchMethodError on any parquet file access command\n*   [PARQUET-2160](https://issues.apache.org/jira/browse/PARQUET-2160) - Close decompression stream to free off-heap memory in time\n*   [PARQUET-2185](https://issues.apache.org/jira/browse/PARQUET-2185) - ParquetReader constructed using builder fails to read encrypted files\n*   [PARQUET-2167](https://issues.apache.org/jira/browse/PARQUET-2167) - CLI show footer command fails if Parquet file contains date fields\n*   [PARQUET-2134](https://issues.apache.org/jira/browse/PARQUET-2134) - Incorrect type checking in HadoopStreams.wrap\n*   [PARQUET-2161](https://issues.apache.org/jira/browse/PARQUET-2161) - Fix row index generation in combination with range filtering\n*   [PARQUET-2154](https://issues.apache.org/jira/browse/PARQUET-2154) - ParquetFileReader should close its input stream when filterRowGroups throw Exception in constructor\n\n#### Test\n\n*   [PARQUET-2192](https://issues.apache.org/jira/browse/PARQUET-2192) - Add Java 17 build test to GitHub action\n\n### Version 1.12.3 ###\n\nRelease Notes - Parquet - Version 1.12.3\n\n#### New Feature\n\n*   [PARQUET-2117](https://issues.apache.org/jira/browse/PARQUET-2117) - Add rowPosition API in parquet record readers\n\n#### Task\n\n*   [PARQUET-2081](https://issues.apache.org/jira/browse/PARQUET-2081) - Encryption translation tool - Parquet-hadoop\n\n#### Improvement\n\n*   [PARQUET-2040](https://issues.apache.org/jira/browse/PARQUET-2040) - Uniform encryption\n*   [PARQUET-2076](https://issues.apache.org/jira/browse/PARQUET-2076) - Improve Travis CI build Performance\n*   [PARQUET-2105](https://issues.apache.org/jira/browse/PARQUET-2105) - Refactor the test code of creating the test file\n*   [PARQUET-2106](https://issues.apache.org/jira/browse/PARQUET-2106) - BinaryComparator should avoid doing ByteBuffer.wrap in the hot-path\n*   [PARQUET-2112](https://issues.apache.org/jira/browse/PARQUET-2112) - Fix typo in MessageColumnIO\n*   [PARQUET-2121](https://issues.apache.org/jira/browse/PARQUET-2121) - Remove descriptions for the removed modules\n*   [PARQUET-2127](https://issues.apache.org/jira/browse/PARQUET-2127) - Security risk in latest parquet-jackson-1.12.2.jar\n*   [PARQUET-2128](https://issues.apache.org/jira/browse/PARQUET-2128) - Bump Thrift to 0.16.0\n*   [PARQUET-2129](https://issues.apache.org/jira/browse/PARQUET-2129) - Add uncompressedSize to \"meta\" output\n*   [PARQUET-2136](https://issues.apache.org/jira/browse/PARQUET-2136) - File writer construction with encryptor\n\n#### Bug\n\n*   [PARQUET-2101](https://issues.apache.org/jira/browse/PARQUET-2101) - Fix wrong descriptions about the default block size\n*   [PARQUET-2102](https://issues.apache.org/jira/browse/PARQUET-2102) - Typo in ColumnIndexBase toString\n*   [PARQUET-2107](https://issues.apache.org/jira/browse/PARQUET-2107) - Travis failures\n*   [PARQUET-2120](https://issues.apache.org/jira/browse/PARQUET-2120) - parquet-cli dictionary command fails on pages without dictionary encoding\n*   [PARQUET-2144](https://issues.apache.org/jira/browse/PARQUET-2144) - Fix ColumnIndexBuilder for notIn predicate\n*   [PARQUET-2148](https://issues.apache.org/jira/browse/PARQUET-2148) - Enable uniform decryption with plaintext footer\n\n### Version 1.12.2 ###\n\nRelease Notes - Parquet - Version 1.12.2\n\n#### Bug\n\n*   [PARQUET-2094](https://issues.apache.org/jira/browse/PARQUET-2094) - Handle negative values in page headers\n\n### Version 1.12.1 ###\n\nRelease Notes - Parquet - Version 1.12.1\n\n#### Bug\n\n*   [PARQUET-1633](https://issues.apache.org/jira/browse/PARQUET-1633) - Fix integer overflow\n*   [PARQUET-2022](https://issues.apache.org/jira/browse/PARQUET-2022) - ZstdDecompressorStream should close zstdInputStream\n*   [PARQUET-2027](https://issues.apache.org/jira/browse/PARQUET-2027) - Fix calculating directory offset for merge\n*   [PARQUET-2052](https://issues.apache.org/jira/browse/PARQUET-2052) - Integer overflow when writing huge binary using dictionary encoding\n*   [PARQUET-2054](https://issues.apache.org/jira/browse/PARQUET-2054) - fix TCP leaking when calling ParquetFileWriter.appendFile\n*   [PARQUET-2072](https://issues.apache.org/jira/browse/PARQUET-2072) - Do Not Determine Both Min/Max for Binary Stats\n*   [PARQUET-2073](https://issues.apache.org/jira/browse/PARQUET-2073) - Fix estimate remaining row count in ColumnWriteStoreBase.\n*   [PARQUET-2078](https://issues.apache.org/jira/browse/PARQUET-2078) - Failed to read parquet file after writing with the same parquet version\n\n#### Improvement\n\n*   [PARQUET-2064](https://issues.apache.org/jira/browse/PARQUET-2064) - Make Range public accessible in RowRanges\n\n### Version 1.12.0 ###\n\nRelease Notes - Parquet - Version 1.12.0\n\n#### Sub-task\n\n*   [PARQUET-1228](https://issues.apache.org/jira/browse/PARQUET-1228) - parquet-format-structures encryption\n*   [PARQUET-1229](https://issues.apache.org/jira/browse/PARQUET-1229) - parquet-mr code changes for encryption support\n*   [PARQUET-1286](https://issues.apache.org/jira/browse/PARQUET-1286) - Crypto package in parquet-mr\n*   [PARQUET-1328](https://issues.apache.org/jira/browse/PARQUET-1328) - \\[java\\]Bloom filter read/write implementation\n*   [PARQUET-1391](https://issues.apache.org/jira/browse/PARQUET-1391) - \\[java\\] Integrate Bloom filter logic\n*   [PARQUET-1516](https://issues.apache.org/jira/browse/PARQUET-1516) - Store Bloom filters near to footer.\n*   [PARQUET-1740](https://issues.apache.org/jira/browse/PARQUET-1740) - Make ParquetFileReader.getFilteredRecordCount public\n*   [PARQUET-1744](https://issues.apache.org/jira/browse/PARQUET-1744) - Some filters throws ArrayIndexOutOfBoundsException\n*   [PARQUET-1807](https://issues.apache.org/jira/browse/PARQUET-1807) - Encryption: Interop and Function test suite for Java version\n*   [PARQUET-1884](https://issues.apache.org/jira/browse/PARQUET-1884) - Merge encryption branch into master\n*   [PARQUET-1915](https://issues.apache.org/jira/browse/PARQUET-1915) - Add null command\n\n#### Bug\n\n*   [PARQUET-1438](https://issues.apache.org/jira/browse/PARQUET-1438) - \\[C++\\] corrupted files produced on 32-bit architecture (i686)\n*   [PARQUET-1493](https://issues.apache.org/jira/browse/PARQUET-1493) - maven protobuf plugin not work properly\n*   [PARQUET-1455](https://issues.apache.org/jira/browse/PARQUET-1455) - \\[parquet-protobuf\\] Handle \"unknown\" enum values for parquet-protobuf\n*   [PARQUET-1554](https://issues.apache.org/jira/browse/PARQUET-1554) - Compilation error when upgrading Scrooge version\n*   [PARQUET-1599](https://issues.apache.org/jira/browse/PARQUET-1599) - Fix to-avro to respect the overwrite option\n*   [PARQUET-1684](https://issues.apache.org/jira/browse/PARQUET-1684) - \\[parquet-protobuf\\] default protobuf field values are stored as nulls\n*   [PARQUET-1699](https://issues.apache.org/jira/browse/PARQUET-1699) - Could not resolve org.apache.yetus:audience-annotations:0.11.0\n*   [PARQUET-1741](https://issues.apache.org/jira/browse/PARQUET-1741) - APIs backward compatibility issues cause master branch build failure\n*   [PARQUET-1765](https://issues.apache.org/jira/browse/PARQUET-1765) - Invalid filteredRowCount in InternalParquetRecordReader\n*   [PARQUET-1794](https://issues.apache.org/jira/browse/PARQUET-1794) - Random data generation may cause flaky tests\n*   [PARQUET-1803](https://issues.apache.org/jira/browse/PARQUET-1803) - Could not find FilleInputSplit in ParquetInputSplit\n*   [PARQUET-1808](https://issues.apache.org/jira/browse/PARQUET-1808) - SimpleGroup.toString() uses String += and so has poor performance\n*   [PARQUET-1818](https://issues.apache.org/jira/browse/PARQUET-1818) - Fix collision of encryption and bloom filters in format-structure Util\n*   [PARQUET-1850](https://issues.apache.org/jira/browse/PARQUET-1850) - toParquetMetadata method in ParquetMetadataConverter does not set dictionary page offset bit\n*   [PARQUET-1851](https://issues.apache.org/jira/browse/PARQUET-1851) - ParquetMetadataConveter throws NPE in an Iceberg unit test\n*   [PARQUET-1868](https://issues.apache.org/jira/browse/PARQUET-1868) - Parquet reader options toggle for bloom filter toggles dictionary filtering\n*   [PARQUET-1879](https://issues.apache.org/jira/browse/PARQUET-1879) - Apache Arrow can not read a Parquet File written with Parqet-Avro 1.11.0 with a Map field\n*   [PARQUET-1893](https://issues.apache.org/jira/browse/PARQUET-1893) - H2SeekableInputStream readFully() doesn't respect start and len\n*   [PARQUET-1894](https://issues.apache.org/jira/browse/PARQUET-1894) - Please fix the related Shaded Jackson Databind CVEs\n*   [PARQUET-1896](https://issues.apache.org/jira/browse/PARQUET-1896) - \\[Maven\\] parquet-tools build is broken\n*   [PARQUET-1910](https://issues.apache.org/jira/browse/PARQUET-1910) - Parquet-cli is broken after TransCompressionCommand was added\n*   [PARQUET-1917](https://issues.apache.org/jira/browse/PARQUET-1917) - \\[parquet-proto\\] default values are stored in oneOf fields that aren't set\n*   [PARQUET-1920](https://issues.apache.org/jira/browse/PARQUET-1920) - Fix issue with reading parquet files with too large column chunks\n*   [PARQUET-1923](https://issues.apache.org/jira/browse/PARQUET-1923) - parquet-tools 1.11.0: TestSimpleRecordConverter fails with ExceptionInInitializerError on openjdk 15\n*   [PARQUET-1928](https://issues.apache.org/jira/browse/PARQUET-1928) - Interpret Parquet INT96 type as FIXED\\[12\\] AVRO Schema\n*   [PARQUET-1944](https://issues.apache.org/jira/browse/PARQUET-1944) - Unable to download transitive dependency hadoop-lzo\n*   [PARQUET-1947](https://issues.apache.org/jira/browse/PARQUET-1947) - DeprecatedParquetInputFormat in CombineFileInputFormat would produce wrong data\n*   [PARQUET-1949](https://issues.apache.org/jira/browse/PARQUET-1949) - Mark Parquet-1872 with not support bloom filter yet\n*   [PARQUET-1954](https://issues.apache.org/jira/browse/PARQUET-1954) - TCP connection leak in parquet dump\n*   [PARQUET-1963](https://issues.apache.org/jira/browse/PARQUET-1963) - DeprecatedParquetInputFormat in CombineFileInputFormat throw NPE when the first sub-split is empty\n*   [PARQUET-1966](https://issues.apache.org/jira/browse/PARQUET-1966) - Fix build with JDK11 for JDK8\n*   [PARQUET-1970](https://issues.apache.org/jira/browse/PARQUET-1970) - Make minor releases source compatible\n*   [PARQUET-1971](https://issues.apache.org/jira/browse/PARQUET-1971) - Flaky test in github action\n*   [PARQUET-1975](https://issues.apache.org/jira/browse/PARQUET-1975) - Test failure on ARM64 CPU architecture\n*   [PARQUET-1977](https://issues.apache.org/jira/browse/PARQUET-1977) - Invalid data\\_page\\_offset\n*   [PARQUET-1979](https://issues.apache.org/jira/browse/PARQUET-1979) - Optional bloom\\_filter\\_offset is filled if no bloom filter is present\n*   [PARQUET-1984](https://issues.apache.org/jira/browse/PARQUET-1984) - Some tests fail on windows\n*   [PARQUET-1992](https://issues.apache.org/jira/browse/PARQUET-1992) - Cannot build from tarball because of git submodules\n*   [PARQUET-1999](https://issues.apache.org/jira/browse/PARQUET-1999) - NPE might occur if OutputFile is implemented by the client\n\n#### New Feature\n\n*   [PARQUET-41](https://issues.apache.org/jira/browse/PARQUET-41) - Add bloom filters to parquet statistics\n*   [PARQUET-1373](https://issues.apache.org/jira/browse/PARQUET-1373) - Encryption key management tools\n*   [PARQUET-1396](https://issues.apache.org/jira/browse/PARQUET-1396) - Example of using EncryptionPropertiesFactory and DecryptionPropertiesFactory\n*   [PARQUET-1622](https://issues.apache.org/jira/browse/PARQUET-1622) - Add BYTE\\_STREAM\\_SPLIT encoding\n*   [PARQUET-1784](https://issues.apache.org/jira/browse/PARQUET-1784) - Column-wise configuration\n*   [PARQUET-1817](https://issues.apache.org/jira/browse/PARQUET-1817) - Crypto Properties Factory\n*   [PARQUET-1854](https://issues.apache.org/jira/browse/PARQUET-1854) - Properties-Driven Interface to Parquet Encryption\n\n#### Improvement\n\n*   [PARQUET-313](https://issues.apache.org/jira/browse/PARQUET-313) - Implement 3 level list writing rule for Parquet-Thrift\n*   [PARQUET-1528](https://issues.apache.org/jira/browse/PARQUET-1528) - Add JSON support to \\`parquet-tools head\\`\n*   [PARQUET-1593](https://issues.apache.org/jira/browse/PARQUET-1593) - Replace the example usage in parquet-cli's help message with an actually existent subcommand\n*   [PARQUET-1660](https://issues.apache.org/jira/browse/PARQUET-1660) - \\[java\\] Align Bloom filter implementation with format\n*   [PARQUET-1666](https://issues.apache.org/jira/browse/PARQUET-1666) - Remove Unused Modules\n*   [PARQUET-1696](https://issues.apache.org/jira/browse/PARQUET-1696) - Remove unused hadoop-1 profile\n*   [PARQUET-1710](https://issues.apache.org/jira/browse/PARQUET-1710) - Use Objects.requireNonNull\n*   [PARQUET-1723](https://issues.apache.org/jira/browse/PARQUET-1723) - Read From Maps Without Using Contains\n*   [PARQUET-1724](https://issues.apache.org/jira/browse/PARQUET-1724) - Use ConcurrentHashMap for Cache in DictionaryPageReader\n*   [PARQUET-1725](https://issues.apache.org/jira/browse/PARQUET-1725) - Replace Usage of Strings.join with JDK Functionality in ColumnPath Class\n*   [PARQUET-1726](https://issues.apache.org/jira/browse/PARQUET-1726) - Use Java 8 Multi Exception Handling\n*   [PARQUET-1727](https://issues.apache.org/jira/browse/PARQUET-1727) - Do Not Swallow InterruptedException in ParquetLoader\n*   [PARQUET-1728](https://issues.apache.org/jira/browse/PARQUET-1728) - Simplify NullPointerException Handling in AvroWriteSupport\n*   [PARQUET-1729](https://issues.apache.org/jira/browse/PARQUET-1729) - Avoid AutoBoxing in EncodingStats\n*   [PARQUET-1730](https://issues.apache.org/jira/browse/PARQUET-1730) - Use switch Statement in AvroIndexedRecordConverter for Enums\n*   [PARQUET-1731](https://issues.apache.org/jira/browse/PARQUET-1731) - Use JDK 8 Facilities to Simplify FilteringRecordMaterializer\n*   [PARQUET-1732](https://issues.apache.org/jira/browse/PARQUET-1732) - Call toArray With Empty Array\n*   [PARQUET-1735](https://issues.apache.org/jira/browse/PARQUET-1735) - Clean Up parquet-columns Module\n*   [PARQUET-1736](https://issues.apache.org/jira/browse/PARQUET-1736) - Use StringBuilder instead of StringBuffer\n*   [PARQUET-1737](https://issues.apache.org/jira/browse/PARQUET-1737) - Replace Test Class RandomStr with Apache Commons Lang\n*   [PARQUET-1738](https://issues.apache.org/jira/browse/PARQUET-1738) - Remove unused imports in parquet-column\n*   [PARQUET-1743](https://issues.apache.org/jira/browse/PARQUET-1743) - Add equals to BlockSplitBloomFilter\n*   [PARQUET-1749](https://issues.apache.org/jira/browse/PARQUET-1749) - Use Java 8 Streams for Empty PrimitiveIterator\n*   [PARQUET-1750](https://issues.apache.org/jira/browse/PARQUET-1750) - Reduce Memory Usage of RowRanges Class\n*   [PARQUET-1751](https://issues.apache.org/jira/browse/PARQUET-1751) - Fix Protobuf Build Warning\n*   [PARQUET-1756](https://issues.apache.org/jira/browse/PARQUET-1756) - Remove Dependency on Maven Plugin semantic-versioning\n*   [PARQUET-1759](https://issues.apache.org/jira/browse/PARQUET-1759) - InternalParquetRecordReader Use Singleton Set\n*   [PARQUET-1763](https://issues.apache.org/jira/browse/PARQUET-1763) - Add SLF4J to TestCircularReferences\n*   [PARQUET-1764](https://issues.apache.org/jira/browse/PARQUET-1764) - The ParquetProperties constructor parameter list is so long\n*   [PARQUET-1775](https://issues.apache.org/jira/browse/PARQUET-1775) - Deprecate AvroParquetWriter Builder Hadoop Path\n*   [PARQUET-1778](https://issues.apache.org/jira/browse/PARQUET-1778) - Do Not Consider Class for Avro Generic Record Reader\n*   [PARQUET-1782](https://issues.apache.org/jira/browse/PARQUET-1782) - Use Switch Statement in AvroRecordConverter\n*   [PARQUET-1790](https://issues.apache.org/jira/browse/PARQUET-1790) - ParquetFileWriter missing Api for DataPageV2\n*   [PARQUET-1791](https://issues.apache.org/jira/browse/PARQUET-1791) - Add 'prune' command to parquet-tools\n*   [PARQUET-1801](https://issues.apache.org/jira/browse/PARQUET-1801) - Add column index support for 'prune' command in Parquet-tools/cli\n*   [PARQUET-1802](https://issues.apache.org/jira/browse/PARQUET-1802) - CompressionCodec class not found if the codec class is not in the same defining classloader as the CodecFactory class\n*   [PARQUET-1805](https://issues.apache.org/jira/browse/PARQUET-1805) - Refactor the configuration for bloom filters\n*   [PARQUET-1821](https://issues.apache.org/jira/browse/PARQUET-1821) - Add 'column-size' command to parquet-cli and parquet-tools\n*   [PARQUET-1826](https://issues.apache.org/jira/browse/PARQUET-1826) - Document hadoop configuration options\n*   [PARQUET-1827](https://issues.apache.org/jira/browse/PARQUET-1827) - UUID type currently not supported by parquet-mr\n*   [PARQUET-1853](https://issues.apache.org/jira/browse/PARQUET-1853) - Minimize the parquet-avro fastutil shaded jar\n*   [PARQUET-1863](https://issues.apache.org/jira/browse/PARQUET-1863) - Remove use of add-test-source mojo in parquet-protobuf\n*   [PARQUET-1866](https://issues.apache.org/jira/browse/PARQUET-1866) - Replace Hadoop ZSTD with JNI-ZSTD\n*   [PARQUET-1890](https://issues.apache.org/jira/browse/PARQUET-1890) - Upgrade to Avro 1.10.0\n*   [PARQUET-1891](https://issues.apache.org/jira/browse/PARQUET-1891) - Encryption-related light fixes\n*   [PARQUET-1914](https://issues.apache.org/jira/browse/PARQUET-1914) - Allow ProtoParquetReader To Support InputFile\n*   [PARQUET-1924](https://issues.apache.org/jira/browse/PARQUET-1924) - Do not Instantiate a New LongHashFunction\n*   [PARQUET-1926](https://issues.apache.org/jira/browse/PARQUET-1926) - Add LogicalType support to ThriftType.I64Type\n*   [PARQUET-1929](https://issues.apache.org/jira/browse/PARQUET-1929) - Bump Snappy to 1.1.8\n*   [PARQUET-1930](https://issues.apache.org/jira/browse/PARQUET-1930) - Bump Apache Thrift to 0.13.0\n*   [PARQUET-1931](https://issues.apache.org/jira/browse/PARQUET-1931) - Bump Junit 4.13.1\n*   [PARQUET-1932](https://issues.apache.org/jira/browse/PARQUET-1932) - Bump Fastutil to 8.4.2\n*   [PARQUET-1938](https://issues.apache.org/jira/browse/PARQUET-1938) - Option to get KMS details from key material (in key rotation)\n*   [PARQUET-1939](https://issues.apache.org/jira/browse/PARQUET-1939) - Fix RemoteKmsClient API ambiguity\n*   [PARQUET-1940](https://issues.apache.org/jira/browse/PARQUET-1940) - Make KeyEncryptionKey length configurable\n*   [PARQUET-1941](https://issues.apache.org/jira/browse/PARQUET-1941) - Bump Commons CLI from 1.3.1 to 1.4\n*   [PARQUET-1951](https://issues.apache.org/jira/browse/PARQUET-1951) - Allow different strategies to combine key values when merging parquet files\n*   [PARQUET-1952](https://issues.apache.org/jira/browse/PARQUET-1952) - Upgrade Avro to 1.10.1\n*   [PARQUET-1961](https://issues.apache.org/jira/browse/PARQUET-1961) - Bump Jackson to 2.11.4\n*   [PARQUET-1964](https://issues.apache.org/jira/browse/PARQUET-1964) - Properly handle missing/null filter\n*   [PARQUET-1967](https://issues.apache.org/jira/browse/PARQUET-1967) - Upgrade Zstd-jni to 1.4.8-3\n*   [PARQUET-1969](https://issues.apache.org/jira/browse/PARQUET-1969) - Test by GithubAction\n*   [PARQUET-1973](https://issues.apache.org/jira/browse/PARQUET-1973) - Support ZSTD JNI BufferPool\n*   [PARQUET-1988](https://issues.apache.org/jira/browse/PARQUET-1988) - Upgrade to ZSTD 1.4.8-6\n*   [PARQUET-1994](https://issues.apache.org/jira/browse/PARQUET-1994) - Upgrade ZSTD JNI to 1.4.9-1\n\n#### Test\n\n*   [PARQUET-1832](https://issues.apache.org/jira/browse/PARQUET-1832) - Travis fails with too long output\n*   [PARQUET-1980](https://issues.apache.org/jira/browse/PARQUET-1980) - Build and test Apache Parquet on ARM64 CPU architecture\n\n#### Wish\n\n*   [PARQUET-1717](https://issues.apache.org/jira/browse/PARQUET-1717) - parquet-thrift converts Thrift i16 to parquet INT32 instead of INT\\_16\n\n#### Task\n\n*   [PARQUET-1676](https://issues.apache.org/jira/browse/PARQUET-1676) - Remove hive modules\n*   [PARQUET-1703](https://issues.apache.org/jira/browse/PARQUET-1703) - Update API compatibility check\n*   [PARQUET-1796](https://issues.apache.org/jira/browse/PARQUET-1796) - Bump Apache Avro to 1.9.2\n*   [PARQUET-1842](https://issues.apache.org/jira/browse/PARQUET-1842) - Update Jackson Databind version to address CVE\n*   [PARQUET-1844](https://issues.apache.org/jira/browse/PARQUET-1844) - Removed Hadoop transitive dependency on commons-lang\n*   [PARQUET-1895](https://issues.apache.org/jira/browse/PARQUET-1895) - Update jackson-databind\n*   [PARQUET-1898](https://issues.apache.org/jira/browse/PARQUET-1898) - Release parquet-mr 1.12.0\n\n### Version 1.11.0 ###\n\nRelease Notes - Parquet - Version 1.11.0\n\n#### Bug\n\n*   [PARQUET-138](https://issues.apache.org/jira/browse/PARQUET-138) - Parquet should allow a merge between required and optional schemas\n*   [PARQUET-952](https://issues.apache.org/jira/browse/PARQUET-952) - Avro union with single type fails with 'is not a group'\n*   [PARQUET-1128](https://issues.apache.org/jira/browse/PARQUET-1128) - \\[Java\\] Upgrade the Apache Arrow version to 0.8.0 for SchemaConverter\n*   [PARQUET-1281](https://issues.apache.org/jira/browse/PARQUET-1281) - Jackson dependency\n*   [PARQUET-1285](https://issues.apache.org/jira/browse/PARQUET-1285) - \\[Java\\] SchemaConverter should not convert from TimeUnit.SECOND AND TimeUnit.NANOSECOND of Arrow\n*   [PARQUET-1293](https://issues.apache.org/jira/browse/PARQUET-1293) - Build failure when using Java 8 lambda expressions\n*   [PARQUET-1296](https://issues.apache.org/jira/browse/PARQUET-1296) - Travis kills build after 10 minutes, because \"no output was received\"\n*   [PARQUET-1297](https://issues.apache.org/jira/browse/PARQUET-1297) - \\[Java\\] SchemaConverter should not convert from Timestamp(TimeUnit.SECOND) and Timestamp(TimeUnit.NANOSECOND) of Arrow\n*   [PARQUET-1303](https://issues.apache.org/jira/browse/PARQUET-1303) - Avro reflect @Stringable field write error if field not instanceof CharSequence\n*   [PARQUET-1304](https://issues.apache.org/jira/browse/PARQUET-1304) - Release 1.10 contains breaking changes for Hive\n*   [PARQUET-1305](https://issues.apache.org/jira/browse/PARQUET-1305) - Backward incompatible change introduced in 1.8\n*   [PARQUET-1309](https://issues.apache.org/jira/browse/PARQUET-1309) - Parquet Java uses incorrect stats and dictionary filter properties\n*   [PARQUET-1311](https://issues.apache.org/jira/browse/PARQUET-1311) - Update README.md\n*   [PARQUET-1317](https://issues.apache.org/jira/browse/PARQUET-1317) - ParquetMetadataConverter throw NPE\n*   [PARQUET-1341](https://issues.apache.org/jira/browse/PARQUET-1341) - Null count is suppressed when columns have no min or max and use unsigned sort order\n*   [PARQUET-1344](https://issues.apache.org/jira/browse/PARQUET-1344) - Type builders don't honor new logical types\n*   [PARQUET-1368](https://issues.apache.org/jira/browse/PARQUET-1368) - ParquetFileReader should close its input stream for the failure in constructor\n*   [PARQUET-1371](https://issues.apache.org/jira/browse/PARQUET-1371) - Time/Timestamp UTC normalization parameter doesn't work\n*   [PARQUET-1407](https://issues.apache.org/jira/browse/PARQUET-1407) - Data loss on duplicate values with AvroParquetWriter/Reader\n*   [PARQUET-1417](https://issues.apache.org/jira/browse/PARQUET-1417) - BINARY\\_AS\\_SIGNED\\_INTEGER\\_COMPARATOR fails with IOBE for the same arrays with the different length\n*   [PARQUET-1421](https://issues.apache.org/jira/browse/PARQUET-1421) - InternalParquetRecordWriter logs debug messages at the INFO level\n*   [PARQUET-1440](https://issues.apache.org/jira/browse/PARQUET-1440) - Parquet-tools: Decimal values stored in an int32 or int64 in the parquet file aren't displayed with their proper scale\n*   [PARQUET-1441](https://issues.apache.org/jira/browse/PARQUET-1441) - SchemaParseException: Can't redefine: list in AvroIndexedRecordConverter\n*   [PARQUET-1456](https://issues.apache.org/jira/browse/PARQUET-1456) - Use page index, ParquetFileReader throw ArrayIndexOutOfBoundsException\n*   [PARQUET-1460](https://issues.apache.org/jira/browse/PARQUET-1460) - Fix javadoc errors and include javadoc checking in Travis checks\n*   [PARQUET-1461](https://issues.apache.org/jira/browse/PARQUET-1461) - Third party code does not compile after parquet-mr minor version update\n*   [PARQUET-1470](https://issues.apache.org/jira/browse/PARQUET-1470) - Inputstream leakage in ParquetFileWriter.appendFile\n*   [PARQUET-1472](https://issues.apache.org/jira/browse/PARQUET-1472) - Dictionary filter fails on FIXED\\_LEN\\_BYTE\\_ARRAY\n*   [PARQUET-1475](https://issues.apache.org/jira/browse/PARQUET-1475) - DirectCodecFactory's ParquetCompressionCodecException drops a passed in cause in one constructor\n*   [PARQUET-1478](https://issues.apache.org/jira/browse/PARQUET-1478) - Can't read spec compliant, 3-level lists via parquet-proto\n*   [PARQUET-1480](https://issues.apache.org/jira/browse/PARQUET-1480) - INT96 to avro not yet implemented error should mention deprecation\n*   [PARQUET-1485](https://issues.apache.org/jira/browse/PARQUET-1485) - Snappy Decompressor/Compressor may cause direct memory leak\n*   [PARQUET-1488](https://issues.apache.org/jira/browse/PARQUET-1488) - UserDefinedPredicate throw NPE\n*   [PARQUET-1496](https://issues.apache.org/jira/browse/PARQUET-1496) - \\[Java\\] Update Scala for JDK 11 compatibility\n*   [PARQUET-1497](https://issues.apache.org/jira/browse/PARQUET-1497) - \\[Java\\] javax annotations dependency missing for Java 11\n*   [PARQUET-1498](https://issues.apache.org/jira/browse/PARQUET-1498) - \\[Java\\] Add instructions to install thrift via homebrew\n*   [PARQUET-1510](https://issues.apache.org/jira/browse/PARQUET-1510) - Dictionary filter skips null values when evaluating not-equals.\n*   [PARQUET-1514](https://issues.apache.org/jira/browse/PARQUET-1514) - ParquetFileWriter Records Compressed Bytes instead of Uncompressed Bytes\n*   [PARQUET-1527](https://issues.apache.org/jira/browse/PARQUET-1527) - \\[parquet-tools\\] cat command throw java.lang.ClassCastException\n*   [PARQUET-1529](https://issues.apache.org/jira/browse/PARQUET-1529) - Shade fastutil in all modules where used\n*   [PARQUET-1531](https://issues.apache.org/jira/browse/PARQUET-1531) - Page row count limit causes empty pages to be written from MessageColumnIO\n*   [PARQUET-1533](https://issues.apache.org/jira/browse/PARQUET-1533) - TestSnappy() throws OOM exception with Parquet-1485 change\n*   [PARQUET-1534](https://issues.apache.org/jira/browse/PARQUET-1534) - \\[parquet-cli\\] Argument error: Illegal character in opaque part at index 2 on Windows\n*   [PARQUET-1544](https://issues.apache.org/jira/browse/PARQUET-1544) - Possible over-shading of modules\n*   [PARQUET-1550](https://issues.apache.org/jira/browse/PARQUET-1550) - CleanUtil does not work in Java 11\n*   [PARQUET-1555](https://issues.apache.org/jira/browse/PARQUET-1555) - Bump snappy-java to 1.1.7.3\n*   [PARQUET-1596](https://issues.apache.org/jira/browse/PARQUET-1596) - PARQUET-1375 broke parquet-cli's to-avro command\n*   [PARQUET-1600](https://issues.apache.org/jira/browse/PARQUET-1600) - Fix shebang in parquet-benchmarks/run.sh\n*   [PARQUET-1615](https://issues.apache.org/jira/browse/PARQUET-1615) - getRecordWriter shouldn't hardcode CREAT mode when new ParquetFileWriter\n*   [PARQUET-1637](https://issues.apache.org/jira/browse/PARQUET-1637) - Builds are failing because default jdk changed to openjdk11 on Travis\n*   [PARQUET-1644](https://issues.apache.org/jira/browse/PARQUET-1644) - Clean up some benchmark code and docs.\n*   [PARQUET-1691](https://issues.apache.org/jira/browse/PARQUET-1691) - Build fails due to missing hadoop-lzo\n\n#### New Feature\n\n*   [PARQUET-1201](https://issues.apache.org/jira/browse/PARQUET-1201) - Column indexes\n*   [PARQUET-1253](https://issues.apache.org/jira/browse/PARQUET-1253) - Support for new logical type representation\n*   [PARQUET-1388](https://issues.apache.org/jira/browse/PARQUET-1388) - Nanosecond precision time and timestamp - parquet-mr\n\n#### Improvement\n\n*   [PARQUET-1135](https://issues.apache.org/jira/browse/PARQUET-1135) - upgrade thrift and protobuf dependencies\n*   [PARQUET-1280](https://issues.apache.org/jira/browse/PARQUET-1280) - \\[parquet-protobuf\\] Use maven protoc plugin\n*   [PARQUET-1321](https://issues.apache.org/jira/browse/PARQUET-1321) - LogicalTypeAnnotation.LogicalTypeAnnotationVisitor#visit methods should have a return value\n*   [PARQUET-1335](https://issues.apache.org/jira/browse/PARQUET-1335) - Logical type names in parquet-mr are not consistent with parquet-format\n*   [PARQUET-1336](https://issues.apache.org/jira/browse/PARQUET-1336) - PrimitiveComparator should implements Serializable\n*   [PARQUET-1365](https://issues.apache.org/jira/browse/PARQUET-1365) - Don't write page level statistics\n*   [PARQUET-1375](https://issues.apache.org/jira/browse/PARQUET-1375) - Upgrade to supported version of Jackson\n*   [PARQUET-1383](https://issues.apache.org/jira/browse/PARQUET-1383) - Parquet tools should indicate UTC parameter for time/timestamp types\n*   [PARQUET-1390](https://issues.apache.org/jira/browse/PARQUET-1390) - \\[Java\\] Upgrade to Arrow 0.10.0\n*   [PARQUET-1399](https://issues.apache.org/jira/browse/PARQUET-1399) - Move parquet-mr related code from parquet-format\n*   [PARQUET-1410](https://issues.apache.org/jira/browse/PARQUET-1410) - Refactor modules to use the new logical type API\n*   [PARQUET-1414](https://issues.apache.org/jira/browse/PARQUET-1414) - Limit page size based on maximum row count\n*   [PARQUET-1418](https://issues.apache.org/jira/browse/PARQUET-1418) - Run integration tests in Travis\n*   [PARQUET-1435](https://issues.apache.org/jira/browse/PARQUET-1435) - Benchmark filtering column-indexes\n*   [PARQUET-1444](https://issues.apache.org/jira/browse/PARQUET-1444) - Prefer ArrayList over LinkedList\n*   [PARQUET-1445](https://issues.apache.org/jira/browse/PARQUET-1445) - Remove Files.java\n*   [PARQUET-1462](https://issues.apache.org/jira/browse/PARQUET-1462) - Allow specifying new development version in prepare-release.sh\n*   [PARQUET-1466](https://issues.apache.org/jira/browse/PARQUET-1466) - Upgrade to the latest guava 27.0-jre\n*   [PARQUET-1474](https://issues.apache.org/jira/browse/PARQUET-1474) - Less verbose and lower level logging for missing column/offset indexes\n*   [PARQUET-1476](https://issues.apache.org/jira/browse/PARQUET-1476) - Don't emit a warning message for files without new logical type\n*   [PARQUET-1487](https://issues.apache.org/jira/browse/PARQUET-1487) - Do not write original type for timezone-agnostic timestamps\n*   [PARQUET-1489](https://issues.apache.org/jira/browse/PARQUET-1489) - Insufficient documentation for UserDefinedPredicate.keep(T)\n*   [PARQUET-1490](https://issues.apache.org/jira/browse/PARQUET-1490) - Add branch-specific Travis steps\n*   [PARQUET-1492](https://issues.apache.org/jira/browse/PARQUET-1492) - Remove protobuf install in travis build\n*   [PARQUET-1499](https://issues.apache.org/jira/browse/PARQUET-1499) - \\[parquet-mr\\] Add Java 11 to Travis\n*   [PARQUET-1500](https://issues.apache.org/jira/browse/PARQUET-1500) - Remove the Closables\n*   [PARQUET-1502](https://issues.apache.org/jira/browse/PARQUET-1502) - Convert FIXED\\_LEN\\_BYTE\\_ARRAY to arrow type in logicalTypeAnnotation if it is not null\n*   [PARQUET-1503](https://issues.apache.org/jira/browse/PARQUET-1503) - Remove Ints Utility Class\n*   [PARQUET-1504](https://issues.apache.org/jira/browse/PARQUET-1504) - Add an option to convert Parquet Int96 to Arrow Timestamp\n*   [PARQUET-1505](https://issues.apache.org/jira/browse/PARQUET-1505) - Use Java 7 NIO StandardCharsets\n*   [PARQUET-1506](https://issues.apache.org/jira/browse/PARQUET-1506) - Migrate from maven-thrift-plugin to thrift-maven-plugin\n*   [PARQUET-1507](https://issues.apache.org/jira/browse/PARQUET-1507) - Bump Apache Thrift to 0.12.0\n*   [PARQUET-1509](https://issues.apache.org/jira/browse/PARQUET-1509) - Update Docs for Hive Deprecation\n*   [PARQUET-1513](https://issues.apache.org/jira/browse/PARQUET-1513) - HiddenFileFilter Streamline\n*   [PARQUET-1518](https://issues.apache.org/jira/browse/PARQUET-1518) - Bump Jackson2 version of parquet-cli\n*   [PARQUET-1530](https://issues.apache.org/jira/browse/PARQUET-1530) - Remove Dependency on commons-codec\n*   [PARQUET-1542](https://issues.apache.org/jira/browse/PARQUET-1542) - Merge multiple I/O to one time I/O when read footer\n*   [PARQUET-1557](https://issues.apache.org/jira/browse/PARQUET-1557) - Replace deprecated Apache Avro methods\n*   [PARQUET-1558](https://issues.apache.org/jira/browse/PARQUET-1558) - Use try-with-resource in Apache Avro tests\n*   [PARQUET-1576](https://issues.apache.org/jira/browse/PARQUET-1576) - Upgrade to Avro 1.9.0\n*   [PARQUET-1577](https://issues.apache.org/jira/browse/PARQUET-1577) - Remove duplicate license\n*   [PARQUET-1578](https://issues.apache.org/jira/browse/PARQUET-1578) - Introduce Lambdas\n*   [PARQUET-1579](https://issues.apache.org/jira/browse/PARQUET-1579) - Add Github PR template\n*   [PARQUET-1580](https://issues.apache.org/jira/browse/PARQUET-1580) - Page-level CRC checksum verification for DataPageV1\n*   [PARQUET-1601](https://issues.apache.org/jira/browse/PARQUET-1601) - Add zstd support to parquet-cli to-avro\n*   [PARQUET-1604](https://issues.apache.org/jira/browse/PARQUET-1604) - Bump fastutil from 7.0.13 to 8.2.3\n*   [PARQUET-1605](https://issues.apache.org/jira/browse/PARQUET-1605) - Bump maven-javadoc-plugin from 2.9 to 3.1.0\n*   [PARQUET-1606](https://issues.apache.org/jira/browse/PARQUET-1606) - Fix invalid tests scope\n*   [PARQUET-1607](https://issues.apache.org/jira/browse/PARQUET-1607) - Remove duplicate maven-enforcer-plugin\n*   [PARQUET-1616](https://issues.apache.org/jira/browse/PARQUET-1616) - Enable Maven batch mode\n*   [PARQUET-1650](https://issues.apache.org/jira/browse/PARQUET-1650) - Implement unit test to validate column/offset indexes\n*   [PARQUET-1654](https://issues.apache.org/jira/browse/PARQUET-1654) - Remove unnecessary options when building thrift\n*   [PARQUET-1661](https://issues.apache.org/jira/browse/PARQUET-1661) - Upgrade to Avro 1.9.1\n*   [PARQUET-1662](https://issues.apache.org/jira/browse/PARQUET-1662) - Upgrade Jackson to version 2.9.10\n*   [PARQUET-1665](https://issues.apache.org/jira/browse/PARQUET-1665) - Upgrade zstd-jni to 1.4.0-1\n*   [PARQUET-1669](https://issues.apache.org/jira/browse/PARQUET-1669) - Disable compiling all libraries when building thrift\n*   [PARQUET-1671](https://issues.apache.org/jira/browse/PARQUET-1671) - Upgrade Yetus to 0.11.0\n*   [PARQUET-1682](https://issues.apache.org/jira/browse/PARQUET-1682) - Maintain forward compatibility for TIME/TIMESTAMP\n*   [PARQUET-1683](https://issues.apache.org/jira/browse/PARQUET-1683) - Remove unnecessary string converting in readFooter method\n*   [PARQUET-1685](https://issues.apache.org/jira/browse/PARQUET-1685) - Truncate the stored min and max for String statistics to reduce the footer size\n\n#### Test\n\n*   [PARQUET-1536](https://issues.apache.org/jira/browse/PARQUET-1536) - \\[parquet-cli\\] Add simple tests for each command\n\n#### Wish\n\n*   [PARQUET-1552](https://issues.apache.org/jira/browse/PARQUET-1552) - upgrade protoc-jar-maven-plugin to 3.8.0\n*   [PARQUET-1673](https://issues.apache.org/jira/browse/PARQUET-1673) - Upgrade parquet-mr format version to 2.7.0\n\n#### Task\n\n*   [PARQUET-968](https://issues.apache.org/jira/browse/PARQUET-968) - Add Hive/Presto support in ProtoParquet\n*   [PARQUET-1294](https://issues.apache.org/jira/browse/PARQUET-1294) - Update release scripts for the new Apache policy\n*   [PARQUET-1434](https://issues.apache.org/jira/browse/PARQUET-1434) - Release parquet-mr 1.11.0\n*   [PARQUET-1436](https://issues.apache.org/jira/browse/PARQUET-1436) - TimestampMicrosStringifier shows wrong microseconds for timestamps before 1970\n*   [PARQUET-1452](https://issues.apache.org/jira/browse/PARQUET-1452) - Deprecate old logical types API\n*   [PARQUET-1551](https://issues.apache.org/jira/browse/PARQUET-1551) - Support Java 11 - top-level JIRA\n*   [PARQUET-1570](https://issues.apache.org/jira/browse/PARQUET-1570) - Publish 1.11.0 to maven central\n*   [PARQUET-1585](https://issues.apache.org/jira/browse/PARQUET-1585) - Update old external links in the code base\n*   [PARQUET-1645](https://issues.apache.org/jira/browse/PARQUET-1645) - Bump Apache Avro to 1.9.1\n*   [PARQUET-1649](https://issues.apache.org/jira/browse/PARQUET-1649) - Bump Jackson Databind to 2.9.9.3\n*   [PARQUET-1687](https://issues.apache.org/jira/browse/PARQUET-1687) - Update release process\n\n### Version 1.10.1 ###\n\nRelease Notes - Parquet - Version 1.10.1\n\n#### Bug\n\n*   [PARQUET-1510](https://issues.apache.org/jira/browse/PARQUET-1510) \\- Dictionary filter skips null values when evaluating not-equals.\n*   [PARQUET-1309](https://issues.apache.org/jira/browse/PARQUET-1309) \\- Parquet Java uses incorrect stats and dictionary filter properties\n\n### Version 1.10.0 ###\n\nRelease Notes - Parquet - Version 1.10.0\n\n#### Bug\n\n*   [PARQUET-196](https://issues.apache.org/jira/browse/PARQUET-196) \\- parquet-tools command to get rowcount & size\n*   [PARQUET-357](https://issues.apache.org/jira/browse/PARQUET-357) \\- Parquet-thrift generates wrong schema for Thrift binary fields\n*   [PARQUET-765](https://issues.apache.org/jira/browse/PARQUET-765) \\- Upgrade Avro to 1.8.1\n*   [PARQUET-783](https://issues.apache.org/jira/browse/PARQUET-783) \\- H2SeekableInputStream does not close its underlying FSDataInputStream, leading to connection leaks\n*   [PARQUET-786](https://issues.apache.org/jira/browse/PARQUET-786) \\- parquet-tools README incorrectly has 'java jar' instead of 'java -jar'\n*   [PARQUET-791](https://issues.apache.org/jira/browse/PARQUET-791) \\- Predicate pushing down on missing columns should work on UserDefinedPredicate too\n*   [PARQUET-1005](https://issues.apache.org/jira/browse/PARQUET-1005) \\- Fix DumpCommand parsing to allow column projection\n*   [PARQUET-1028](https://issues.apache.org/jira/browse/PARQUET-1028) \\- \\[JAVA\\] When reading old Spark-generated files with INT96, stats are reported as valid when they aren't\n*   [PARQUET-1065](https://issues.apache.org/jira/browse/PARQUET-1065) \\- Deprecate type-defined sort ordering for INT96 type\n*   [PARQUET-1077](https://issues.apache.org/jira/browse/PARQUET-1077) \\- \\[MR\\] Switch to long key ids in KEYs file\n*   [PARQUET-1141](https://issues.apache.org/jira/browse/PARQUET-1141) \\- IDs are dropped in metadata conversion\n*   [PARQUET-1152](https://issues.apache.org/jira/browse/PARQUET-1152) \\- Parquet-thrift doesn't compile with Thrift 0.9.3\n*   [PARQUET-1153](https://issues.apache.org/jira/browse/PARQUET-1153) \\- Parquet-thrift doesn't compile with Thrift 0.10.0\n*   [PARQUET-1156](https://issues.apache.org/jira/browse/PARQUET-1156) \\- dev/merge\\_parquet\\_pr.py problems\n*   [PARQUET-1185](https://issues.apache.org/jira/browse/PARQUET-1185) \\- TestBinary#testBinary unit test fails after PARQUET-1141\n*   [PARQUET-1191](https://issues.apache.org/jira/browse/PARQUET-1191) \\- Type.hashCode() takes originalType into account but Type.equals() does not\n*   [PARQUET-1208](https://issues.apache.org/jira/browse/PARQUET-1208) \\- Occasional endless loop in unit test\n*   [PARQUET-1217](https://issues.apache.org/jira/browse/PARQUET-1217) \\- Incorrect handling of missing values in Statistics\n*   [PARQUET-1246](https://issues.apache.org/jira/browse/PARQUET-1246) \\- Ignore float/double statistics in case of NaN\n*   [PARQUET-1258](https://issues.apache.org/jira/browse/PARQUET-1258) \\- Update scm developer connection to github\n\n#### New Feature\n\n*   [PARQUET-1025](https://issues.apache.org/jira/browse/PARQUET-1025) \\- Support new min-max statistics in parquet-mr\n\n#### Improvement\n\n*   [PARQUET-220](https://issues.apache.org/jira/browse/PARQUET-220) \\- Unnecessary warning in ParquetRecordReader.initialize\n*   [PARQUET-321](https://issues.apache.org/jira/browse/PARQUET-321) \\- Set the HDFS padding default to 8MB\n*   [PARQUET-386](https://issues.apache.org/jira/browse/PARQUET-386) \\- Printing out the statistics of metadata in parquet-tools\n*   [PARQUET-423](https://issues.apache.org/jira/browse/PARQUET-423) \\- Make writing Avro to Parquet less noisy\n*   [PARQUET-755](https://issues.apache.org/jira/browse/PARQUET-755) \\- create parquet-arrow module with schema converter\n*   [PARQUET-777](https://issues.apache.org/jira/browse/PARQUET-777) \\- Add new Parquet CLI tools\n*   [PARQUET-787](https://issues.apache.org/jira/browse/PARQUET-787) \\- Add a size limit for heap allocations when reading\n*   [PARQUET-801](https://issues.apache.org/jira/browse/PARQUET-801) \\- Allow UserDefinedPredicates in DictionaryFilter\n*   [PARQUET-852](https://issues.apache.org/jira/browse/PARQUET-852) \\- Slowly ramp up sizes of byte\\[\\] in ByteBasedBitPackingEncoder\n*   [PARQUET-884](https://issues.apache.org/jira/browse/PARQUET-884) \\- Add support for Decimal datatype to Parquet-Pig record reader\n*   [PARQUET-969](https://issues.apache.org/jira/browse/PARQUET-969) \\- Decimal datatype support for parquet-tools output\n*   [PARQUET-990](https://issues.apache.org/jira/browse/PARQUET-990) \\- More detailed error messages in footer parsing\n*   [PARQUET-1024](https://issues.apache.org/jira/browse/PARQUET-1024) \\- allow for case insensitive parquet-xxx prefix in PR title\n*   [PARQUET-1026](https://issues.apache.org/jira/browse/PARQUET-1026) \\- allow unsigned binary stats when min == max\n*   [PARQUET-1115](https://issues.apache.org/jira/browse/PARQUET-1115) \\- Warn users when misusing parquet-tools merge\n*   [PARQUET-1135](https://issues.apache.org/jira/browse/PARQUET-1135) \\- upgrade thrift and protobuf dependencies\n*   [PARQUET-1142](https://issues.apache.org/jira/browse/PARQUET-1142) \\- Avoid leaking Hadoop API to downstream libraries\n*   [PARQUET-1149](https://issues.apache.org/jira/browse/PARQUET-1149) \\- Upgrade Avro dependancy to 1.8.2\n*   [PARQUET-1170](https://issues.apache.org/jira/browse/PARQUET-1170) \\- Logical-type-based toString for proper representeation in tools/logs\n*   [PARQUET-1183](https://issues.apache.org/jira/browse/PARQUET-1183) \\- AvroParquetWriter needs OutputFile based Builder\n*   [PARQUET-1197](https://issues.apache.org/jira/browse/PARQUET-1197) \\- Log rat failures\n*   [PARQUET-1198](https://issues.apache.org/jira/browse/PARQUET-1198) \\- Bump java source and target to java8\n*   [PARQUET-1215](https://issues.apache.org/jira/browse/PARQUET-1215) \\- Add accessor for footer after a file is closed\n*   [PARQUET-1263](https://issues.apache.org/jira/browse/PARQUET-1263) \\- ParquetReader's builder should use Configuration from the InputFile\n\n#### Task\n\n*   [PARQUET-768](https://issues.apache.org/jira/browse/PARQUET-768) \\- Add Uwe L. Korn to KEYS\n*   [PARQUET-1189](https://issues.apache.org/jira/browse/PARQUET-1189) \\- Release Parquet Java 1.10\n\n### Version 1.9.0 ###\n\n#### Bug\n\n*   [PARQUET-182](https://issues.apache.org/jira/browse/PARQUET-182) - FilteredRecordReader skips rows it shouldn't for schema with optional columns\n*   [PARQUET-212](https://issues.apache.org/jira/browse/PARQUET-212) - Implement nested type read rules in parquet-thrift\n*   [PARQUET-241](https://issues.apache.org/jira/browse/PARQUET-241) - ParquetInputFormat.getFooters() should return in the same order as what listStatus() returns\n*   [PARQUET-305](https://issues.apache.org/jira/browse/PARQUET-305) - Logger instantiated for package org.apache.parquet may be GC-ed\n*   [PARQUET-335](https://issues.apache.org/jira/browse/PARQUET-335) - Avro object model should not require MAP_KEY_VALUE\n*   [PARQUET-340](https://issues.apache.org/jira/browse/PARQUET-340) - totalMemoryPool is truncated to 32 bits\n*   [PARQUET-346](https://issues.apache.org/jira/browse/PARQUET-346) - ThriftSchemaConverter throws for unknown struct or union type\n*   [PARQUET-349](https://issues.apache.org/jira/browse/PARQUET-349) - VersionParser does not handle versions like \"parquet-mr 1.6.0rc4\"\n*   [PARQUET-352](https://issues.apache.org/jira/browse/PARQUET-352) - Add tags to \"created by\" metadata in the file footer\n*   [PARQUET-353](https://issues.apache.org/jira/browse/PARQUET-353) - Compressors not getting recycled while writing parquet files, causing memory leak\n*   [PARQUET-360](https://issues.apache.org/jira/browse/PARQUET-360) - parquet-cat json dump is broken for maps\n*   [PARQUET-363](https://issues.apache.org/jira/browse/PARQUET-363) - Cannot construct empty MessageType for ReadContext.requestedSchema\n*   [PARQUET-367](https://issues.apache.org/jira/browse/PARQUET-367) - \"parquet-cat -j\" doesn't show all records\n*   [PARQUET-372](https://issues.apache.org/jira/browse/PARQUET-372) - Parquet stats can have awkwardly large values\n*   [PARQUET-373](https://issues.apache.org/jira/browse/PARQUET-373) - MemoryManager tests are flaky\n*   [PARQUET-379](https://issues.apache.org/jira/browse/PARQUET-379) - PrimitiveType.union erases original type\n*   [PARQUET-380](https://issues.apache.org/jira/browse/PARQUET-380) - Cascading and scrooge builds fail when using thrift 0.9.0\n*   [PARQUET-385](https://issues.apache.org/jira/browse/PARQUET-385) - PrimitiveType.union accepts fixed_len_byte_array fields with different lengths when strict mode is on\n*   [PARQUET-387](https://issues.apache.org/jira/browse/PARQUET-387) - TwoLevelListWriter does not handle null values in array\n*   [PARQUET-389](https://issues.apache.org/jira/browse/PARQUET-389) - Filter predicates should work with missing columns\n*   [PARQUET-395](https://issues.apache.org/jira/browse/PARQUET-395) - System.out is used as logger in org.apache.parquet.Log\n*   [PARQUET-396](https://issues.apache.org/jira/browse/PARQUET-396) - The builder for AvroParquetReader loses the record type\n*   [PARQUET-400](https://issues.apache.org/jira/browse/PARQUET-400) - Error reading some files after PARQUET-77 bytebuffer read path\n*   [PARQUET-409](https://issues.apache.org/jira/browse/PARQUET-409) - InternalParquetRecordWriter doesn't use min/max row counts\n*   [PARQUET-410](https://issues.apache.org/jira/browse/PARQUET-410) - Fix subprocess hang in merge_parquet_pr.py\n*   [PARQUET-413](https://issues.apache.org/jira/browse/PARQUET-413) - Test failures for Java 8\n*   [PARQUET-415](https://issues.apache.org/jira/browse/PARQUET-415) - ByteBufferBackedBinary serialization is broken\n*   [PARQUET-422](https://issues.apache.org/jira/browse/PARQUET-422) - Fix a potential bug in MessageTypeParser where we ignore and overwrite the initial value of a method parameter\n*   [PARQUET-425](https://issues.apache.org/jira/browse/PARQUET-425) - Fix the bug when predicate contains columns not specified in prejection, to prevent filtering out data improperly\n*   [PARQUET-426](https://issues.apache.org/jira/browse/PARQUET-426) - Throw Exception when predicate contains columns not specified in prejection, to prevent filtering out data improperly\n*   [PARQUET-430](https://issues.apache.org/jira/browse/PARQUET-430) - Change to use Locale parameterized version of String.toUpperCase()/toLowerCase\n*   [PARQUET-431](https://issues.apache.org/jira/browse/PARQUET-431) - Make ParquetOutputFormat.memoryManager volatile\n*   [PARQUET-495](https://issues.apache.org/jira/browse/PARQUET-495) - Fix mismatches in Types class comments\n*   [PARQUET-509](https://issues.apache.org/jira/browse/PARQUET-509) - Incorrect number of args passed to string.format calls\n*   [PARQUET-511](https://issues.apache.org/jira/browse/PARQUET-511) - Integer overflow on counting values in column\n*   [PARQUET-528](https://issues.apache.org/jira/browse/PARQUET-528) - Fix flush() for RecordConsumer and implementations\n*   [PARQUET-529](https://issues.apache.org/jira/browse/PARQUET-529) - Avoid evoking job.toString() in ParquetLoader\n*   [PARQUET-540](https://issues.apache.org/jira/browse/PARQUET-540) - Cascading3 module doesn't build when using thrift 0.9.0\n*   [PARQUET-544](https://issues.apache.org/jira/browse/PARQUET-544) - ParquetWriter.close() throws NullPointerException on second call, improper implementation of Closeable contract\n*   [PARQUET-560](https://issues.apache.org/jira/browse/PARQUET-560) - Incorrect synchronization in SnappyCompressor\n*   [PARQUET-569](https://issues.apache.org/jira/browse/PARQUET-569) - ParquetMetadataConverter offset filter is broken\n*   [PARQUET-571](https://issues.apache.org/jira/browse/PARQUET-571) - Fix potential leak in ParquetFileReader.close()\n*   [PARQUET-580](https://issues.apache.org/jira/browse/PARQUET-580) - Potentially unnecessary creation of large int[] in IntList for columns that aren't used\n*   [PARQUET-581](https://issues.apache.org/jira/browse/PARQUET-581) - Min/max row count for page size check are conflated in some places\n*   [PARQUET-584](https://issues.apache.org/jira/browse/PARQUET-584) - show proper command usage when there's no arguments\n*   [PARQUET-612](https://issues.apache.org/jira/browse/PARQUET-612) - Add compression to FileEncodingIT tests\n*   [PARQUET-623](https://issues.apache.org/jira/browse/PARQUET-623) - DeltaByteArrayReader has incorrect skip behaviour\n*   [PARQUET-642](https://issues.apache.org/jira/browse/PARQUET-642) - Improve performance of ByteBuffer based read / write paths\n*   [PARQUET-645](https://issues.apache.org/jira/browse/PARQUET-645) - DictionaryFilter incorrectly handles null\n*   [PARQUET-651](https://issues.apache.org/jira/browse/PARQUET-651) - Parquet-avro fails to decode array of record with a single field name \"element\" correctly\n*   [PARQUET-660](https://issues.apache.org/jira/browse/PARQUET-660) - Writing Protobuf messages with extensions results in an error or data corruption.\n*   [PARQUET-663](https://issues.apache.org/jira/browse/PARQUET-663) - Link are Broken in README.md\n*   [PARQUET-674](https://issues.apache.org/jira/browse/PARQUET-674) - Add an abstraction to get the length of a stream\n*   [PARQUET-685](https://issues.apache.org/jira/browse/PARQUET-685) - Deprecated ParquetInputSplit constructor passes parameters in the wrong order.\n*   [PARQUET-726](https://issues.apache.org/jira/browse/PARQUET-726) - TestMemoryManager consistently fails\n*   [PARQUET-743](https://issues.apache.org/jira/browse/PARQUET-743) - DictionaryFilters can re-use StreamBytesInput when compressed\n\n#### Improvement\n\n*   [PARQUET-77](https://issues.apache.org/jira/browse/PARQUET-77) - Improvements in ByteBuffer read path\n*   [PARQUET-99](https://issues.apache.org/jira/browse/PARQUET-99) - Large rows cause unnecessary OOM exceptions\n*   [PARQUET-146](https://issues.apache.org/jira/browse/PARQUET-146) - make Parquet compile with java 7 instead of java 6\n*   [PARQUET-318](https://issues.apache.org/jira/browse/PARQUET-318) - Remove unnecessary objectmapper from ParquetMetadata\n*   [PARQUET-327](https://issues.apache.org/jira/browse/PARQUET-327) - Show statistics in the dump output\n*   [PARQUET-341](https://issues.apache.org/jira/browse/PARQUET-341) - Improve write performance with wide schema sparse data\n*   [PARQUET-343](https://issues.apache.org/jira/browse/PARQUET-343) - Caching nulls on group node to improve write performance on wide schema sparse data\n*   [PARQUET-358](https://issues.apache.org/jira/browse/PARQUET-358) - Add support for temporal logical types to AVRO/Parquet conversion\n*   [PARQUET-361](https://issues.apache.org/jira/browse/PARQUET-361) - Add prerelease logic to semantic versions\n*   [PARQUET-384](https://issues.apache.org/jira/browse/PARQUET-384) - Add Dictionary Based Filtering to Filter2 API\n*   [PARQUET-386](https://issues.apache.org/jira/browse/PARQUET-386) - Printing out the statistics of metadata in parquet-tools\n*   [PARQUET-397](https://issues.apache.org/jira/browse/PARQUET-397) - Pig Predicate Pushdown using Filter2 API\n*   [PARQUET-421](https://issues.apache.org/jira/browse/PARQUET-421) - Fix mismatch of javadoc names and method parameters in module encoding, column, and hadoop\n*   [PARQUET-427](https://issues.apache.org/jira/browse/PARQUET-427) - Push predicates into the whole read path\n*   [PARQUET-432](https://issues.apache.org/jira/browse/PARQUET-432) - Complete a todo for method ColumnDescriptor.compareTo()\n*   [PARQUET-460](https://issues.apache.org/jira/browse/PARQUET-460) - Parquet files concat tool\n*   [PARQUET-480](https://issues.apache.org/jira/browse/PARQUET-480) - Update for Cascading 3.0\n*   [PARQUET-484](https://issues.apache.org/jira/browse/PARQUET-484) - Warn when Decimal is stored as INT64 while could be stored as INT32\n*   [PARQUET-543](https://issues.apache.org/jira/browse/PARQUET-543) - Remove BoundedInt encodings\n*   [PARQUET-585](https://issues.apache.org/jira/browse/PARQUET-585) - Slowly ramp up sizes of int[]s in IntList to keep sizes small when data sets are small\n*   [PARQUET-654](https://issues.apache.org/jira/browse/PARQUET-654) - Make record-level filtering optional\n*   [PARQUET-668](https://issues.apache.org/jira/browse/PARQUET-668) - Provide option to disable auto crop feature in DumpCommand output\n*   [PARQUET-727](https://issues.apache.org/jira/browse/PARQUET-727) - Ensure correct version of thrift is used\n*   [PARQUET-740](https://issues.apache.org/jira/browse/PARQUET-740) - Introduce editorconfig\n\n#### New Feature\n\n*   [PARQUET-225](https://issues.apache.org/jira/browse/PARQUET-225) - INT64 support for Delta Encoding\n*   [PARQUET-382](https://issues.apache.org/jira/browse/PARQUET-382) - Add a way to append encoded blocks in ParquetFileWriter\n*   [PARQUET-429](https://issues.apache.org/jira/browse/PARQUET-429) - Enables predicates collecting their referred columns\n*   [PARQUET-548](https://issues.apache.org/jira/browse/PARQUET-548) - Add Java metadata for PageEncodingStats\n*   [PARQUET-669](https://issues.apache.org/jira/browse/PARQUET-669) - Allow reading file footers from input streams when writing metadata files\n\n#### Task\n\n*   [PARQUET-392](https://issues.apache.org/jira/browse/PARQUET-392) - Release Parquet-mr 1.9.0\n*   [PARQUET-404](https://issues.apache.org/jira/browse/PARQUET-404) - Replace git@github.com.apache for HTTPS URL on dev/README.md to avoid permission issues\n*   [PARQUET-696](https://issues.apache.org/jira/browse/PARQUET-696) - Move travis download from google code (defunct) to github\n\n#### Test\n\n*   [PARQUET-355](https://issues.apache.org/jira/browse/PARQUET-355) - Create Integration tests to validate statistics\n*   [PARQUET-378](https://issues.apache.org/jira/browse/PARQUET-378) - Add thoroughly parquet test encodings\n\n\n### Version 1.8.1 ###\n\n#### Bug\n\n*   [PARQUET-331](https://issues.apache.org/jira/browse/PARQUET-331) - Merge script doesn't surface stderr from failed sub processes\n*   [PARQUET-336](https://issues.apache.org/jira/browse/PARQUET-336) - ArrayIndexOutOfBounds in checkDeltaByteArrayProblem\n*   [PARQUET-337](https://issues.apache.org/jira/browse/PARQUET-337) - binary fields inside map/set/list are not handled in parquet-scrooge\n*   [PARQUET-338](https://issues.apache.org/jira/browse/PARQUET-338) - Readme references wrong format of pull request title\n\n#### Improvement\n\n*   [PARQUET-279](https://issues.apache.org/jira/browse/PARQUET-279) - Check empty struct in the CompatibilityChecker util\n\n#### Task\n\n*   [PARQUET-339](https://issues.apache.org/jira/browse/PARQUET-339) - Add Alex Levenson to KEYS file\n\n### Version 1.8.0 ###\n\n#### Bug\n\n*   [PARQUET-151](https://issues.apache.org/jira/browse/PARQUET-151) - Null Pointer exception in parquet.hadoop.ParquetFileWriter.mergeFooters\n*   [PARQUET-152](https://issues.apache.org/jira/browse/PARQUET-152) - Encoding issue with fixed length byte arrays\n*   [PARQUET-164](https://issues.apache.org/jira/browse/PARQUET-164) - Warn when parquet memory manager kicks in\n*   [PARQUET-199](https://issues.apache.org/jira/browse/PARQUET-199) - Add a callback when the MemoryManager adjusts row group size\n*   [PARQUET-201](https://issues.apache.org/jira/browse/PARQUET-201) - Column with OriginalType INT_8 failed at filtering\n*   [PARQUET-227](https://issues.apache.org/jira/browse/PARQUET-227) - Parquet thrift can write unions that have 0 or more than 1 set value\n*   [PARQUET-246](https://issues.apache.org/jira/browse/PARQUET-246) - ArrayIndexOutOfBoundsException with Parquet write version v2\n*   [PARQUET-251](https://issues.apache.org/jira/browse/PARQUET-251) - Binary column statistics error when reuse byte[] among rows\n*   [PARQUET-252](https://issues.apache.org/jira/browse/PARQUET-252) - parquet scrooge support should support nested container type\n*   [PARQUET-254](https://issues.apache.org/jira/browse/PARQUET-254) - Wrong exception message for unsupported INT96 type\n*   [PARQUET-269](https://issues.apache.org/jira/browse/PARQUET-269) - Restore scrooge-maven-plugin to 3.17.0 or greater\n*   [PARQUET-284](https://issues.apache.org/jira/browse/PARQUET-284) - Should use ConcurrentHashMap instead of HashMap in ParquetMetadataConverter\n*   [PARQUET-285](https://issues.apache.org/jira/browse/PARQUET-285) - Implement nested types write rules in parquet-avro\n*   [PARQUET-287](https://issues.apache.org/jira/browse/PARQUET-287) - Projecting unions in thrift causes TExceptions in deserializatoin\n*   [PARQUET-296](https://issues.apache.org/jira/browse/PARQUET-296) - Set master branch version back to 1.8.0-SNAPSHOT\n*   [PARQUET-297](https://issues.apache.org/jira/browse/PARQUET-297) - created_by in file meta data doesn't contain parquet library version\n*   [PARQUET-314](https://issues.apache.org/jira/browse/PARQUET-314) - Fix broken equals implementation(s)\n*   [PARQUET-316](https://issues.apache.org/jira/browse/PARQUET-316) - Run.sh is broken in parquet-benchmarks\n*   [PARQUET-317](https://issues.apache.org/jira/browse/PARQUET-317) - writeMetaDataFile crashes when a relative root Path is used\n*   [PARQUET-320](https://issues.apache.org/jira/browse/PARQUET-320) - Restore semver checks\n*   [PARQUET-324](https://issues.apache.org/jira/browse/PARQUET-324) - row count incorrect if data file has more than 2^31 rows\n*   [PARQUET-325](https://issues.apache.org/jira/browse/PARQUET-325) - Do not target row group sizes if padding is set to 0\n*   [PARQUET-329](https://issues.apache.org/jira/browse/PARQUET-329) - ThriftReadSupport#THRIFT_COLUMN_FILTER_KEY was removed (incompatible change)\n\n#### Improvement\n\n*   [PARQUET-175](https://issues.apache.org/jira/browse/PARQUET-175) - Allow setting of a custom protobuf class when reading parquet file using parquet-protobuf.\n*   [PARQUET-223](https://issues.apache.org/jira/browse/PARQUET-223) - Add Map and List builiders\n*   [PARQUET-245](https://issues.apache.org/jira/browse/PARQUET-245) - Travis CI runs tests even if build fails\n*   [PARQUET-248](https://issues.apache.org/jira/browse/PARQUET-248) - Simplify ParquetWriters's constructors\n*   [PARQUET-253](https://issues.apache.org/jira/browse/PARQUET-253) - AvroSchemaConverter has confusing Javadoc\n*   [PARQUET-259](https://issues.apache.org/jira/browse/PARQUET-259) - Support Travis CI in parquet-cpp\n*   [PARQUET-264](https://issues.apache.org/jira/browse/PARQUET-264) - Update README docs for graduation\n*   [PARQUET-266](https://issues.apache.org/jira/browse/PARQUET-266) - Add support for lists of primitives to Pig schema converter\n*   [PARQUET-272](https://issues.apache.org/jira/browse/PARQUET-272) - Updates docs decscription to match data model\n*   [PARQUET-274](https://issues.apache.org/jira/browse/PARQUET-274) - Updates URLs to link against the apache user instead of Parquet on github\n*   [PARQUET-276](https://issues.apache.org/jira/browse/PARQUET-276) - Updates CONTRIBUTING file with new repo info\n*   [PARQUET-286](https://issues.apache.org/jira/browse/PARQUET-286) - Avro object model should use Utf8\n*   [PARQUET-288](https://issues.apache.org/jira/browse/PARQUET-288) - Add dictionary support to Avro converters\n*   [PARQUET-289](https://issues.apache.org/jira/browse/PARQUET-289) - Allow object models to extend the ParquetReader builders\n*   [PARQUET-290](https://issues.apache.org/jira/browse/PARQUET-290) - Add Avro data model to the reader builder\n*   [PARQUET-306](https://issues.apache.org/jira/browse/PARQUET-306) - Improve alignment between row groups and HDFS blocks\n*   [PARQUET-308](https://issues.apache.org/jira/browse/PARQUET-308) - Add accessor to ParquetWriter to get current data size\n*   [PARQUET-309](https://issues.apache.org/jira/browse/PARQUET-309) - Remove unnecessary compile dependency on parquet-generator\n*   [PARQUET-321](https://issues.apache.org/jira/browse/PARQUET-321) - Set the HDFS padding default to 8MB\n*   [PARQUET-327](https://issues.apache.org/jira/browse/PARQUET-327) - Show statistics in the dump output\n\n#### New Feature\n\n*   [PARQUET-229](https://issues.apache.org/jira/browse/PARQUET-229) - Make an alternate, stricter thrift column projection API\n*   [PARQUET-243](https://issues.apache.org/jira/browse/PARQUET-243) - Add avro-reflect support\n\n#### Task\n\n*   [PARQUET-262](https://issues.apache.org/jira/browse/PARQUET-262) - When 1.7.0 is released, restore semver plugin config\n*   [PARQUET-292](https://issues.apache.org/jira/browse/PARQUET-292) - Release Parquet 1.8.0\n\n### Version 1.7.0 ###\n\n*   [PARQUET-23](https://issues.apache.org/jira/browse/PARQUET-23) - Rename to org.apache.\n\n### Version 1.6.0 ###\n\n####  Bug\n\n*   [PARQUET-3](https://issues.apache.org/jira/browse/PARQUET-3) - tool to merge pull requests based on Spark\n*   [PARQUET-4](https://issues.apache.org/jira/browse/PARQUET-4) - Use LRU caching for footers in ParquetInputFormat.\n*   [PARQUET-8](https://issues.apache.org/jira/browse/PARQUET-8) - [parquet-scrooge] mvn eclipse:eclipse fails on parquet-scrooge\n*   [PARQUET-9](https://issues.apache.org/jira/browse/PARQUET-9) - InternalParquetRecordReader will not read multiple blocks when filtering\n*   [PARQUET-18](https://issues.apache.org/jira/browse/PARQUET-18) - Cannot read dictionary-encoded pages with all null values\n*   [PARQUET-19](https://issues.apache.org/jira/browse/PARQUET-19) - NPE when an empty file is included in a Hive query that uses CombineHiveInputFormat\n*   [PARQUET-21](https://issues.apache.org/jira/browse/PARQUET-21) - Fix reference to 'github-apache' in dev docs\n*   [PARQUET-56](https://issues.apache.org/jira/browse/PARQUET-56) - Added an accessor for the Long column type in example Group\n*   [PARQUET-62](https://issues.apache.org/jira/browse/PARQUET-62) - DictionaryValuesWriter dictionaries are corrupted by user changes.\n*   [PARQUET-63](https://issues.apache.org/jira/browse/PARQUET-63) - Fixed-length columns cannot be dictionary encoded.\n*   [PARQUET-66](https://issues.apache.org/jira/browse/PARQUET-66) - InternalParquetRecordWriter int overflow causes unnecessary memory check warning\n*   [PARQUET-69](https://issues.apache.org/jira/browse/PARQUET-69) - Add committer doc and REVIEWERS files\n*   [PARQUET-70](https://issues.apache.org/jira/browse/PARQUET-70) - PARQUET #36: Pig Schema Storage to UDFContext\n*   [PARQUET-75](https://issues.apache.org/jira/browse/PARQUET-75) - String decode using 'new String' is slow\n*   [PARQUET-80](https://issues.apache.org/jira/browse/PARQUET-80) - upgrade semver plugin version to 0.9.27\n*   [PARQUET-82](https://issues.apache.org/jira/browse/PARQUET-82) - ColumnChunkPageWriteStore assumes pages are smaller than Integer.MAX\\_VALUE\n*   [PARQUET-88](https://issues.apache.org/jira/browse/PARQUET-88) - Fix pre-version enforcement.\n*   [PARQUET-94](https://issues.apache.org/jira/browse/PARQUET-94) - ParquetScroogeScheme constructor ignores klass argument\n*   [PARQUET-96](https://issues.apache.org/jira/browse/PARQUET-96) - parquet.example.data.Group is missing some methods\n*   [PARQUET-97](https://issues.apache.org/jira/browse/PARQUET-97) - ProtoParquetReader builder factory method not static\n*   [PARQUET-101](https://issues.apache.org/jira/browse/PARQUET-101) - Exception when reading data with parquet.task.side.metadata=false\n*   [PARQUET-104](https://issues.apache.org/jira/browse/PARQUET-104) - Parquet writes empty Rowgroup at the end of the file\n*   [PARQUET-106](https://issues.apache.org/jira/browse/PARQUET-106) - Relax InputSplit Protections\n*   [PARQUET-107](https://issues.apache.org/jira/browse/PARQUET-107) - Add option to disable summary metadata aggregation after MR jobs\n*   [PARQUET-114](https://issues.apache.org/jira/browse/PARQUET-114) - Sample NanoTime class serializes and deserializes Timestamp incorrectly\n*   [PARQUET-122](https://issues.apache.org/jira/browse/PARQUET-122) - make parquet.task.side.metadata=true by default\n*   [PARQUET-124](https://issues.apache.org/jira/browse/PARQUET-124) - parquet.hadoop.ParquetOutputCommitter.commitJob() throws parquet.io.ParquetEncodingException\n*   [PARQUET-132](https://issues.apache.org/jira/browse/PARQUET-132) - AvroParquetInputFormat should use a parameterized type\n*   [PARQUET-135](https://issues.apache.org/jira/browse/PARQUET-135) - Input location is not getting set for the getStatistics in ParquetLoader when using two different loaders within a Pig script.\n*   [PARQUET-136](https://issues.apache.org/jira/browse/PARQUET-136) - NPE thrown in StatisticsFilter when all values in a string/binary column trunk are null\n*   [PARQUET-142](https://issues.apache.org/jira/browse/PARQUET-142) - parquet-tools doesn't filter \\_SUCCESS file\n*   [PARQUET-145](https://issues.apache.org/jira/browse/PARQUET-145) - InternalParquetRecordReader.close() should not throw an exception if initialization has failed\n*   [PARQUET-150](https://issues.apache.org/jira/browse/PARQUET-150) - Merge script requires ':' in PR names\n*   [PARQUET-157](https://issues.apache.org/jira/browse/PARQUET-157) - Divide by zero in logging code\n*   [PARQUET-159](https://issues.apache.org/jira/browse/PARQUET-159) - paquet-hadoop tests fail to compile\n*   [PARQUET-162](https://issues.apache.org/jira/browse/PARQUET-162) - ParquetThrift should throw when unrecognized columns are passed to the column projection API\n*   [PARQUET-168](https://issues.apache.org/jira/browse/PARQUET-168) - Wrong command line option description in parquet-tools\n*   [PARQUET-173](https://issues.apache.org/jira/browse/PARQUET-173) - StatisticsFilter doesn't handle And properly\n*   [PARQUET-174](https://issues.apache.org/jira/browse/PARQUET-174) - Fix Java6 compatibility\n*   [PARQUET-176](https://issues.apache.org/jira/browse/PARQUET-176) - Parquet fails to parse schema contains '\\r'\n*   [PARQUET-180](https://issues.apache.org/jira/browse/PARQUET-180) - Parquet-thrift compile issue with 0.9.2.\n*   [PARQUET-184](https://issues.apache.org/jira/browse/PARQUET-184) - Add release scripts and documentation\n*   [PARQUET-186](https://issues.apache.org/jira/browse/PARQUET-186) - Poor performance in SnappyCodec because of string concat in tight loop\n*   [PARQUET-187](https://issues.apache.org/jira/browse/PARQUET-187) - parquet-scrooge doesn't compile under 2.11\n*   [PARQUET-188](https://issues.apache.org/jira/browse/PARQUET-188) - Parquet writes columns out of order (compared to the schema)\n*   [PARQUET-189](https://issues.apache.org/jira/browse/PARQUET-189) - Support building parquet with thrift 0.9.0\n*   [PARQUET-196](https://issues.apache.org/jira/browse/PARQUET-196) - parquet-tools command to get rowcount & size\n*   [PARQUET-197](https://issues.apache.org/jira/browse/PARQUET-197) - parquet-cascading and the mapred API does not create metadata file\n*   [PARQUET-202](https://issues.apache.org/jira/browse/PARQUET-202) - Typo in the connection info in the pom prevents publishing an RC\n*   [PARQUET-207](https://issues.apache.org/jira/browse/PARQUET-207) - ParquetInputSplit end calculation bug\n*   [PARQUET-208](https://issues.apache.org/jira/browse/PARQUET-208) - revert PARQUET-197\n*   [PARQUET-214](https://issues.apache.org/jira/browse/PARQUET-214) - Avro: Regression caused by schema handling\n*   [PARQUET-215](https://issues.apache.org/jira/browse/PARQUET-215) - Parquet Thrift should discard records with unrecognized union members\n*   [PARQUET-216](https://issues.apache.org/jira/browse/PARQUET-216) - Decrease the default page size to 64k\n*   [PARQUET-217](https://issues.apache.org/jira/browse/PARQUET-217) - Memory Manager's min allocation heuristic is not valid for schemas with many columns\n*   [PARQUET-232](https://issues.apache.org/jira/browse/PARQUET-232) - minor compilation issue\n*   [PARQUET-234](https://issues.apache.org/jira/browse/PARQUET-234) - Restore ParquetInputSplit methods from 1.5.0\n*   [PARQUET-235](https://issues.apache.org/jira/browse/PARQUET-235) - Fix compatibility of parquet.metadata with 1.5.0\n*   [PARQUET-236](https://issues.apache.org/jira/browse/PARQUET-236) - Check parquet-scrooge compatibility\n*   [PARQUET-237](https://issues.apache.org/jira/browse/PARQUET-237) - Check ParquetWriter constructor compatibility with 1.5.0\n*   [PARQUET-239](https://issues.apache.org/jira/browse/PARQUET-239) - Make AvroParquetReader#builder() static\n*   [PARQUET-242](https://issues.apache.org/jira/browse/PARQUET-242) - AvroReadSupport.setAvroDataSupplier is broken\n\n####  Improvement\n\n*   [PARQUET-2](https://issues.apache.org/jira/browse/PARQUET-2) - Adding Type Persuasion for Primitive Types\n*   [PARQUET-25](https://issues.apache.org/jira/browse/PARQUET-25) - Pushdown predicates only work with hardcoded arguments\n*   [PARQUET-52](https://issues.apache.org/jira/browse/PARQUET-52) - Improve the encoding fall back mechanism for Parquet 2.0\n*   [PARQUET-57](https://issues.apache.org/jira/browse/PARQUET-57) - Make dev commit script easier to use\n*   [PARQUET-61](https://issues.apache.org/jira/browse/PARQUET-61) - Avoid fixing protocol events when there is not required field missing\n*   [PARQUET-74](https://issues.apache.org/jira/browse/PARQUET-74) - Use thread local decoder cache in Binary toStringUsingUTF8()\n*   [PARQUET-79](https://issues.apache.org/jira/browse/PARQUET-79) - Add thrift streaming API to read metadata\n*   [PARQUET-84](https://issues.apache.org/jira/browse/PARQUET-84) - Add an option to read the rowgroup metadata on the task side.\n*   [PARQUET-87](https://issues.apache.org/jira/browse/PARQUET-87) - Better and unified API for projection pushdown on cascading scheme\n*   [PARQUET-89](https://issues.apache.org/jira/browse/PARQUET-89) - All Parquet CI tests should be run against hadoop-2\n*   [PARQUET-92](https://issues.apache.org/jira/browse/PARQUET-92) - Parallel Footer Read Control\n*   [PARQUET-105](https://issues.apache.org/jira/browse/PARQUET-105) - Refactor and Document Parquet Tools\n*   [PARQUET-108](https://issues.apache.org/jira/browse/PARQUET-108) - Parquet Memory Management in Java\n*   [PARQUET-115](https://issues.apache.org/jira/browse/PARQUET-115) - Pass a filter object to user defined predicate in filter2 api\n*   [PARQUET-116](https://issues.apache.org/jira/browse/PARQUET-116) - Pass a filter object to user defined predicate in filter2 api\n*   [PARQUET-117](https://issues.apache.org/jira/browse/PARQUET-117) - implement the new page format for Parquet 2.0\n*   [PARQUET-119](https://issues.apache.org/jira/browse/PARQUET-119) - add data\\_encodings to ColumnMetaData to enable dictionary based predicate push down\n*   [PARQUET-121](https://issues.apache.org/jira/browse/PARQUET-121) - Allow Parquet to build with Java 8\n*   [PARQUET-128](https://issues.apache.org/jira/browse/PARQUET-128) - Optimize the parquet RecordReader implementation when: A. filterpredicate is pushed down , B. filterpredicate is pushed down on a flat schema\n*   [PARQUET-133](https://issues.apache.org/jira/browse/PARQUET-133) - Upgrade snappy-java to 1.1.1.6\n*   [PARQUET-134](https://issues.apache.org/jira/browse/PARQUET-134) - Enhance ParquetWriter with file creation flag\n*   [PARQUET-140](https://issues.apache.org/jira/browse/PARQUET-140) - Allow clients to control the GenericData object that is used to read Avro records\n*   [PARQUET-141](https://issues.apache.org/jira/browse/PARQUET-141) - improve parquet scrooge integration\n*   [PARQUET-160](https://issues.apache.org/jira/browse/PARQUET-160) - Simplify CapacityByteArrayOutputStream\n*   [PARQUET-165](https://issues.apache.org/jira/browse/PARQUET-165) - A benchmark module for Parquet would be nice\n*   [PARQUET-177](https://issues.apache.org/jira/browse/PARQUET-177) - MemoryManager ensure minimum Column Chunk size\n*   [PARQUET-181](https://issues.apache.org/jira/browse/PARQUET-181) - Scrooge Write Support\n*   [PARQUET-191](https://issues.apache.org/jira/browse/PARQUET-191) - Avro schema conversion incorrectly converts maps with nullable values.\n*   [PARQUET-192](https://issues.apache.org/jira/browse/PARQUET-192) - Avro maps drop null values\n*   [PARQUET-193](https://issues.apache.org/jira/browse/PARQUET-193) - Avro: Implement read compatibility rules for nested types\n*   [PARQUET-203](https://issues.apache.org/jira/browse/PARQUET-203) - Consolidate PathFilter for hidden files\n*   [PARQUET-204](https://issues.apache.org/jira/browse/PARQUET-204) - Directory support for parquet-schema\n*   [PARQUET-210](https://issues.apache.org/jira/browse/PARQUET-210) - JSON output for parquet-cat\n\n####  New Feature\n\n*   [PARQUET-22](https://issues.apache.org/jira/browse/PARQUET-22) - Parquet #13: Backport of HIVE-6938\n*   [PARQUET-49](https://issues.apache.org/jira/browse/PARQUET-49) - Create a new filter API that supports filtering groups of records based on their statistics\n*   [PARQUET-64](https://issues.apache.org/jira/browse/PARQUET-64) - Add new logical types to parquet-column\n*   [PARQUET-123](https://issues.apache.org/jira/browse/PARQUET-123) - Add dictionary support to AvroIndexedRecordReader\n*   [PARQUET-198](https://issues.apache.org/jira/browse/PARQUET-198) - parquet-cascading Add Parquet Avro Scheme\n\n####  Task\n\n*   [PARQUET-50](https://issues.apache.org/jira/browse/PARQUET-50) - Remove items from semver blacklist\n*   [PARQUET-139](https://issues.apache.org/jira/browse/PARQUET-139) - Avoid reading file footers in parquet-avro InputFormat\n*   [PARQUET-190](https://issues.apache.org/jira/browse/PARQUET-190) - Fix an inconsistent Javadoc comment of ReadSupport.prepareForRead\n*   [PARQUET-230](https://issues.apache.org/jira/browse/PARQUET-230) - Add build instructions to the README\n\n### Version 1.5.0 ###\n* ISSUE [399](https://github.com/Parquet/parquet-mr/pull/399): Fixed resetting stats after writePage bug, unit testing of readFooter\n* ISSUE [397](https://github.com/Parquet/parquet-mr/pull/397): Fixed issue with column pruning when using requested schema\n* ISSUE [389](https://github.com/Parquet/parquet-mr/pull/389): Added padding for requested columns not found in file schema\n* ISSUE [392](https://github.com/Parquet/parquet-mr/pull/392): Value stats fixes\n* ISSUE [338](https://github.com/Parquet/parquet-mr/pull/338): Added statistics to Parquet pages and rowGroups\n* ISSUE [351](https://github.com/Parquet/parquet-mr/pull/351): Fix bug #350, fixed length argument out of order.\n* ISSUE [378](https://github.com/Parquet/parquet-mr/pull/378): configure semver to enforce semantic versioning\n* ISSUE [355](https://github.com/Parquet/parquet-mr/pull/355): Add support for DECIMAL type annotation.\n* ISSUE [336](https://github.com/Parquet/parquet-mr/pull/336): protobuf dependency version changed from 2.4.1 to 2.5.0\n* ISSUE [337](https://github.com/Parquet/parquet-mr/pull/337): issue #324, move ParquetStringInspector to org.apache.hadoop.hive.serde...\n\n### Version 1.4.3 ###\n* ISSUE [381](https://github.com/Parquet/parquet-mr/pull/381): fix metadata concurency problem\n\n### Version 1.4.2 ###\n* ISSUE [359](https://github.com/Parquet/parquet-mr/pull/359): Expose values in SimpleRecord\n* ISSUE [335](https://github.com/Parquet/parquet-mr/pull/335): issue #290, hive map conversion to parquet schema\n* ISSUE [365](https://github.com/Parquet/parquet-mr/pull/365): generate splits by min max size, and align to HDFS block when possible\n* ISSUE [353](https://github.com/Parquet/parquet-mr/pull/353): Fix bug: optional enum field causing ScroogeSchemaConverter to fail\n* ISSUE [362](https://github.com/Parquet/parquet-mr/pull/362): Fix output bug during parquet-dump command\n* ISSUE [366](https://github.com/Parquet/parquet-mr/pull/366): do not call schema converter to generate projected schema when projection is not set\n* ISSUE [367](https://github.com/Parquet/parquet-mr/pull/367): make ParquetFileWriter throw IOException in invalid state case\n* ISSUE [352](https://github.com/Parquet/parquet-mr/pull/352): Parquet thrift storer\n* ISSUE [349](https://github.com/Parquet/parquet-mr/pull/349): fix header bug\n\n### Version 1.4.1 ###\n* ISSUE [344](https://github.com/Parquet/parquet-mr/pull/344): select * from parquet hive table containing map columns runs into exception. Issue #341.\n* ISSUE [347](https://github.com/Parquet/parquet-mr/pull/347): set reading length in ThriftBytesWriteSupport to avoid potential OOM cau...\n* ISSUE [346](https://github.com/Parquet/parquet-mr/pull/346): stop using strings and b64 for compressed input splits\n* ISSUE [345](https://github.com/Parquet/parquet-mr/pull/345): set cascading version to 2.5.3\n* ISSUE [342](https://github.com/Parquet/parquet-mr/pull/342): compress kv pairs in ParquetInputSplits\n \n### Version 1.4.0 ###\n* ISSUE [333](https://github.com/Parquet/parquet-mr/pull/333): Compress schemas in split\n* ISSUE [329](https://github.com/Parquet/parquet-mr/pull/329): fix filesystem resolution\n* ISSUE [320](https://github.com/Parquet/parquet-mr/pull/320): Spelling fix\n* ISSUE [319](https://github.com/Parquet/parquet-mr/pull/319): oauth based authentication; fix grep change\n* ISSUE [310](https://github.com/Parquet/parquet-mr/pull/310): Merge parquet tools\n* ISSUE [314](https://github.com/Parquet/parquet-mr/pull/314): Fix avro schema conv for arrays of optional type for #312.\n* ISSUE [311](https://github.com/Parquet/parquet-mr/pull/311): Avro null default values bug\n* ISSUE [316](https://github.com/Parquet/parquet-mr/pull/316): Update poms to use thrift.exectuable property.\n* ISSUE [285](https://github.com/Parquet/parquet-mr/pull/285): [CASCADING] Provide the sink implementation for ParquetTupleScheme\n* ISSUE [264](https://github.com/Parquet/parquet-mr/pull/264): Native Protocol Buffer support\n* ISSUE [293](https://github.com/Parquet/parquet-mr/pull/293): Int96 support\n* ISSUE [313](https://github.com/Parquet/parquet-mr/pull/313): Add hadoop Configuration to Avro and Thrift writers (#295).\n* ISSUE [262](https://github.com/Parquet/parquet-mr/pull/262): Scrooge schema converter and projection pushdown in Scrooge\n* ISSUE [297](https://github.com/Parquet/parquet-mr/pull/297): Ports HIVE-5783 to the parquet-hive module\n* ISSUE [303](https://github.com/Parquet/parquet-mr/pull/303): Avro read schema aliases\n* ISSUE [299](https://github.com/Parquet/parquet-mr/pull/299): Fill in default values for new fields in the Avro read schema\n* ISSUE [298](https://github.com/Parquet/parquet-mr/pull/298): Bugfix reorder thrift fields causing writting nulls\n* ISSUE [289](https://github.com/Parquet/parquet-mr/pull/289): first use current thread's classloader to load a class, if current threa...\n* ISSUE [292](https://github.com/Parquet/parquet-mr/pull/292): Added ParquetWriter() that takes an instance of Hadoop's Configuration.\n* ISSUE [282](https://github.com/Parquet/parquet-mr/pull/282): Avro default read schema\n* ISSUE [280](https://github.com/Parquet/parquet-mr/pull/280): style: junit.framework to org.junit\n* ISSUE [270](https://github.com/Parquet/parquet-mr/pull/270): Make ParquetInputSplit extend FileSplit\n\n### Version 1.3.2 ###\n* ISSUE [271](https://github.com/Parquet/parquet-mr/pull/271): fix bug: last enum index throws DecodingSchemaMismatchException\n* ISSUE [268](https://github.com/Parquet/parquet-mr/pull/268): fixes #265: add semver validation checks to non-bundle builds\n* ISSUE [269](https://github.com/Parquet/parquet-mr/pull/269): Bumps parquet-jackson parent version\n* ISSUE [260](https://github.com/Parquet/parquet-mr/pull/260): Shade jackson only once for all parquet modules\n\n### Version 1.3.1 ###\n* ISSUE [267](https://github.com/Parquet/parquet-mr/pull/267): handler only handle ignored field, exception during will be thrown as Sk...\n* ISSUE [266](https://github.com/Parquet/parquet-mr/pull/266): upgrade parquet-mr to elephant-bird 4.4\n\n### Version 1.3.0 ###\n* ISSUE [258](https://github.com/Parquet/parquet-mr/pull/258): Optimize scan\n* ISSUE [259](https://github.com/Parquet/parquet-mr/pull/259): add delta length byte arrays and delta byte arrays encodings\n* ISSUE [249](https://github.com/Parquet/parquet-mr/pull/249): make summary files read in parallel; improve memory footprint of metadata; avoid unnecessary seek\n* ISSUE [257](https://github.com/Parquet/parquet-mr/pull/257): Create parquet-hadoop-bundle which will eventually replace parquet-hive-bundle\n* ISSUE [253](https://github.com/Parquet/parquet-mr/pull/253): Delta Binary Packing for Int\n* ISSUE [254](https://github.com/Parquet/parquet-mr/pull/254): Add writer version flag to parquet and make initial changes for supported parquet 2.0 encodings\n* ISSUE [256](https://github.com/Parquet/parquet-mr/pull/256): Resolves issue #251 by doing additional checks if Hive returns \"Unknown\" as a version\n* ISSUE [252](https://github.com/Parquet/parquet-mr/pull/252): refactor error handler for BufferedProtocolReadToWrite to be non-static\n\n### Version 1.2.11 ###\n* ISSUE [250](https://github.com/Parquet/parquet-mr/pull/250): pretty_print_json_for_compatibility_checker\n* ISSUE [243](https://github.com/Parquet/parquet-mr/pull/243): add parquet cascading integration documentation\n* ISSUE [248](https://github.com/Parquet/parquet-mr/pull/248): More Hadoop 2 compatibility fixes\n\n### Version 1.2.10 ###\n* ISSUE [247](https://github.com/Parquet/parquet-mr/pull/247): fix bug: when field index is greater than zero\n* ISSUE [244](https://github.com/Parquet/parquet-mr/pull/244): Feature/error handler\n* ISSUE [187](https://github.com/Parquet/parquet-mr/pull/187): Plumb OriginalType\n* ISSUE [245](https://github.com/Parquet/parquet-mr/pull/245): integrate parquet format 2.0\n\n### Version 1.2.9 ###\n* ISSUE [242](https://github.com/Parquet/parquet-mr/pull/242): upgrade elephant-bird version to 4.3\n* ISSUE [240](https://github.com/Parquet/parquet-mr/pull/240): fix loader cache\n* ISSUE [233](https://github.com/Parquet/parquet-mr/pull/233): use latest stable release of cascading: 2.5.1\n* ISSUE [241](https://github.com/Parquet/parquet-mr/pull/241): Update reference to 0.10 in Hive012Binding javadoc\n* ISSUE [239](https://github.com/Parquet/parquet-mr/pull/239): Fix hive map and array inspectors with null containers\n* ISSUE [234](https://github.com/Parquet/parquet-mr/pull/234): optimize chunk scan; fix compressed size\n* ISSUE [237](https://github.com/Parquet/parquet-mr/pull/237): Handle codec not found\n* ISSUE [238](https://github.com/Parquet/parquet-mr/pull/238): fix pom version caused by bad merge\n* ISSUE [235](https://github.com/Parquet/parquet-mr/pull/235): Not write pig meta data only when pig is not avaliable\n* ISSUE [227](https://github.com/Parquet/parquet-mr/pull/227): Breaks parquet-hive up into several submodules, creating infrastructure ...\n* ISSUE [229](https://github.com/Parquet/parquet-mr/pull/229): add changelog tool\n* ISSUE [236](https://github.com/Parquet/parquet-mr/pull/236): Make cascading a provided dependency\n\n### Version 1.2.8 ###\n* ISSUE 228: enable globing files for parquetTupleScheme, refactor unit tests and rem...\n* ISSUE 224: Changing read and write methods in ParquetInputSplit so that they can de...\n\n### Version 1.2.8 ###\n* ISSUE 228: enable globing files for parquetTupleScheme, refactor unit tests and rem...\n* ISSUE 224: Changing read and write methods in ParquetInputSplit so that they can de...\n\n### Version 1.2.7 ###\n* ISSUE 223: refactor encoded values changes and test that resetDictionary works\n* ISSUE 222: fix bug: set raw data size to 0 after reset\n\n### Version 1.2.6 ###\n* ISSUE 221: make pig, hadoop and log4j jars provided\n* ISSUE 220: parquet-hive should ship and uber jar\n* ISSUE 213: group parquet-format version in one property\n* ISSUE 215: Fix Binary.equals().\n* ISSUE 210: ParquetWriter ignores enable dictionary and validating flags.\n* ISSUE 202: Fix requested schema when recreating splits in hive\n* ISSUE 208: Improve dic fall back\n* ISSUE 207: Fix offset\n* ISSUE 206: Create a \"Powered by\" page\n\n### Version 1.2.5 ###\n* ISSUE 204: ParquetLoader.inputFormatCache as WeakHashMap\n* ISSUE 203: add null check for EnumWriteProtocol\n* ISSUE 205: use cascading 2.2.0\n* ISSUE 199: simplify TupleWriteSupport constructor\n* ISSUE 164: Dictionary changes\n* ISSUE 196: Fixes to the Hive SerDe\n* ISSUE 197: RLE decoder reading past the end of the stream\n* ISSUE 188: Added ability to define arbitrary predicate functions\n* ISSUE 194: refactor serde to remove some unecessary boxing and include dictionary awareness\n* ISSUE 190: NPE in DictionaryValuesWriter.\n\n### Version 1.2.4 ###\n* ISSUE 191: Add compatibility checker for ThriftStruct to check for backward compatibility of two thrift structs\n\n### Version 1.2.3 ###\n* ISSUE 186: add parquet-pig-bundle\n* ISSUE 184: Update ParquetReader to take Configuration as a constructor argument.\n* ISSUE 183: Disable the time read counter check in DeprecatedInputFormatTest.\n* ISSUE 182: Fix a maven warning about a missing version number.\n* ISSUE 181: FIXED_LEN_BYTE_ARRAY support\n* ISSUE 180: Support writing Avro records with maps with Utf8 keys\n* ISSUE 179: Added Or/Not logical filters for column predicates\n* ISSUE 172: Add sink support for parquet.cascading.ParquetTBaseScheme\n* ISSUE 169: Support avro records with empty maps and arrays\n* ISSUE 162: Avro schema with empty arrays and maps\n\n### Version 1.2.2 ###\n* ISSUE 175: fix problem with projection pushdown in parquetloader\n* ISSUE 174: improve readability by renaming variables\n* ISSUE 173: make numbers in log messages easy to read in InternalParquetRecordWriter\n* ISSUE 171: add unit test for parquet-scrooge\n* ISSUE 165: distinguish recoverable exception in BufferedProtocolReadToWrite\n* ISSUE 166: support projection when required fields in thrift class are not projected\n\n### Version 1.2.1 ###\n* ISSUE 167: fix oom error dues to bad estimation\n\n### Version 1.2.0 ###\n* ISSUE 154: improve thrift error message\n* ISSUE 161: support schema evolution\n* ISSUE 160: Resource leak in parquet.hadoop.ParquetFileReader.readFooter(Configurati...\n* ISSUE 163: remove debugging code from hot path\n* ISSUE 155: Manual pushdown for thrift read support\n* ISSUE 159: Counter for mapred\n* ISSUE 156: Fix site\n* ISSUE 153: Fix projection required field\n    \n### Version 1.1.1 ###\n* ISSUE 150: add thrift validation on read\n\n### Version 1.1.0 ###\n* ISSUE 149: changing default block size to 128mb  \n* ISSUE 146: Fix and add unit tests for Hive nested types  \n* ISSUE 145: add getStatistics method to parquetloader  \n* ISSUE 144: Map key fields should allow other types than strings  \n* ISSUE 143: Fix empty encoding col metadata  \n* ISSUE 142: Fix total size row group  \n* ISSUE 141: add parquet counters for benchmark  \n* ISSUE 140: Implemented partial schema for GroupReadSupport  \n* ISSUE 138: fix bug of wrong column metadata size  \n* ISSUE 137: ParquetMetadataConverter bug  \n* ISSUE 133: Update plugin versions for maven aether migration - fixes #125  \n* ISSUE 130: Schema validation should not validate the root element's name  \n* ISSUE 127: Adding dictionary encoding for non string types.. #99  \n* ISSUE 125: Unable to build  \n* ISSUE 124: Fix Short and Byte types in Hive SerDe.  \n* ISSUE 123: Fix Snappy compressor in parquet-hadoop.  \n* ISSUE 120: Fix RLE bug with partial literal groups at end of stream.  \n* ISSUE 118: Refactor column reader  \n* ISSUE 115: Map key fields should allow other types than strings  \n* ISSUE 103: Map key fields should allow other types than strings  \n* ISSUE 99: Dictionary encoding for non string types (float  double  int  long  boolean)  \n* ISSUE 47: Add tests for parquet-scrooge and parquet-cascading \n\n### Version 1.0.1 ###\n* ISSUE 126: Unit tests for parquet cascading  \n* ISSUE 121: fix wrong RecordConverter for ParquetTBaseScheme  \n* ISSUE 119: fix compatibility with thrift  remove unused dependency \n\n### Version 1.0.0 ###\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 11.353515625,
          "content": "\n                                 Apache License\n                           Version 2.0, January 2004\n                        http://www.apache.org/licenses/\n\n   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n\n   1. Definitions.\n\n      \"License\" shall mean the terms and conditions for use, reproduction,\n      and distribution as defined by Sections 1 through 9 of this document.\n\n      \"Licensor\" shall mean the copyright owner or entity authorized by\n      the copyright owner that is granting the License.\n\n      \"Legal Entity\" shall mean the union of the acting entity and all\n      other entities that control, are controlled by, or are under common\n      control with that entity. For the purposes of this definition,\n      \"control\" means (i) the power, direct or indirect, to cause the\n      direction or management of such entity, whether by contract or\n      otherwise, or (ii) ownership of fifty percent (50%) or more of the\n      outstanding shares, or (iii) beneficial ownership of such entity.\n\n      \"You\" (or \"Your\") shall mean an individual or Legal Entity\n      exercising permissions granted by this License.\n\n      \"Source\" form shall mean the preferred form for making modifications,\n      including but not limited to software source code, documentation\n      source, and configuration files.\n\n      \"Object\" form shall mean any form resulting from mechanical\n      transformation or translation of a Source form, including but\n      not limited to compiled object code, generated documentation,\n      and conversions to other media types.\n\n      \"Work\" shall mean the work of authorship, whether in Source or\n      Object form, made available under the License, as indicated by a\n      copyright notice that is included in or attached to the work\n      (an example is provided in the Appendix below).\n\n      \"Derivative Works\" shall mean any work, whether in Source or Object\n      form, that is based on (or derived from) the Work and for which the\n      editorial revisions, annotations, elaborations, or other modifications\n      represent, as a whole, an original work of authorship. For the purposes\n      of this License, Derivative Works shall not include works that remain\n      separable from, or merely link (or bind by name) to the interfaces of,\n      the Work and Derivative Works thereof.\n\n      \"Contribution\" shall mean any work of authorship, including\n      the original version of the Work and any modifications or additions\n      to that Work or Derivative Works thereof, that is intentionally\n      submitted to Licensor for inclusion in the Work by the copyright owner\n      or by an individual or Legal Entity authorized to submit on behalf of\n      the copyright owner. For the purposes of this definition, \"submitted\"\n      means any form of electronic, verbal, or written communication sent\n      to the Licensor or its representatives, including but not limited to\n      communication on electronic mailing lists, source code control systems,\n      and issue tracking systems that are managed by, or on behalf of, the\n      Licensor for the purpose of discussing and improving the Work, but\n      excluding communication that is conspicuously marked or otherwise\n      designated in writing by the copyright owner as \"Not a Contribution.\"\n\n      \"Contributor\" shall mean Licensor and any individual or Legal Entity\n      on behalf of whom a Contribution has been received by Licensor and\n      subsequently incorporated within the Work.\n\n   2. Grant of Copyright License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      copyright license to reproduce, prepare Derivative Works of,\n      publicly display, publicly perform, sublicense, and distribute the\n      Work and such Derivative Works in Source or Object form.\n\n   3. Grant of Patent License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      (except as stated in this section) patent license to make, have made,\n      use, offer to sell, sell, import, and otherwise transfer the Work,\n      where such license applies only to those patent claims licensable\n      by such Contributor that are necessarily infringed by their\n      Contribution(s) alone or by combination of their Contribution(s)\n      with the Work to which such Contribution(s) was submitted. If You\n      institute patent litigation against any entity (including a\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\n      or a Contribution incorporated within the Work constitutes direct\n      or contributory patent infringement, then any patent licenses\n      granted to You under this License for that Work shall terminate\n      as of the date such litigation is filed.\n\n   4. Redistribution. You may reproduce and distribute copies of the\n      Work or Derivative Works thereof in any medium, with or without\n      modifications, and in Source or Object form, provided that You\n      meet the following conditions:\n\n      (a) You must give any other recipients of the Work or\n          Derivative Works a copy of this License; and\n\n      (b) You must cause any modified files to carry prominent notices\n          stating that You changed the files; and\n\n      (c) You must retain, in the Source form of any Derivative Works\n          that You distribute, all copyright, patent, trademark, and\n          attribution notices from the Source form of the Work,\n          excluding those notices that do not pertain to any part of\n          the Derivative Works; and\n\n      (d) If the Work includes a \"NOTICE\" text file as part of its\n          distribution, then any Derivative Works that You distribute must\n          include a readable copy of the attribution notices contained\n          within such NOTICE file, excluding those notices that do not\n          pertain to any part of the Derivative Works, in at least one\n          of the following places: within a NOTICE text file distributed\n          as part of the Derivative Works; within the Source form or\n          documentation, if provided along with the Derivative Works; or,\n          within a display generated by the Derivative Works, if and\n          wherever such third-party notices normally appear. The contents\n          of the NOTICE file are for informational purposes only and\n          do not modify the License. You may add Your own attribution\n          notices within Derivative Works that You distribute, alongside\n          or as an addendum to the NOTICE text from the Work, provided\n          that such additional attribution notices cannot be construed\n          as modifying the License.\n\n      You may add Your own copyright statement to Your modifications and\n      may provide additional or different license terms and conditions\n      for use, reproduction, or distribution of Your modifications, or\n      for any such Derivative Works as a whole, provided Your use,\n      reproduction, and distribution of the Work otherwise complies with\n      the conditions stated in this License.\n\n   5. Submission of Contributions. Unless You explicitly state otherwise,\n      any Contribution intentionally submitted for inclusion in the Work\n      by You to the Licensor shall be under the terms and conditions of\n      this License, without any additional terms or conditions.\n      Notwithstanding the above, nothing herein shall supersede or modify\n      the terms of any separate license agreement you may have executed\n      with Licensor regarding such Contributions.\n\n   6. Trademarks. This License does not grant permission to use the trade\n      names, trademarks, service marks, or product names of the Licensor,\n      except as required for reasonable and customary use in describing the\n      origin of the Work and reproducing the content of the NOTICE file.\n\n   7. Disclaimer of Warranty. Unless required by applicable law or\n      agreed to in writing, Licensor provides the Work (and each\n      Contributor provides its Contributions) on an \"AS IS\" BASIS,\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n      implied, including, without limitation, any warranties or conditions\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n      PARTICULAR PURPOSE. You are solely responsible for determining the\n      appropriateness of using or redistributing the Work and assume any\n      risks associated with Your exercise of permissions under this License.\n\n   8. Limitation of Liability. In no event and under no legal theory,\n      whether in tort (including negligence), contract, or otherwise,\n      unless required by applicable law (such as deliberate and grossly\n      negligent acts) or agreed to in writing, shall any Contributor be\n      liable to You for damages, including any direct, indirect, special,\n      incidental, or consequential damages of any character arising as a\n      result of this License or out of the use or inability to use the\n      Work (including but not limited to damages for loss of goodwill,\n      work stoppage, computer failure or malfunction, or any and all\n      other commercial damages or losses), even if such Contributor\n      has been advised of the possibility of such damages.\n\n   9. Accepting Warranty or Additional Liability. While redistributing\n      the Work or Derivative Works thereof, You may choose to offer,\n      and charge a fee for, acceptance of support, warranty, indemnity,\n      or other liability obligations and/or rights consistent with this\n      License. However, in accepting such obligations, You may act only\n      on Your own behalf and on Your sole responsibility, not on behalf\n      of any other Contributor, and only if You agree to indemnify,\n      defend, and hold each Contributor harmless for any liability\n      incurred by, or claims asserted against, such Contributor by reason\n      of your accepting any such warranty or additional liability.\n\n   END OF TERMS AND CONDITIONS\n\n--------------------------------------------------------------------------------\n\nThis product includes code from Apache Avro.\n\nCopyright: 2014 The Apache Software Foundation.\nHome page: https://avro.apache.org/\nLicense: http://www.apache.org/licenses/LICENSE-2.0\n\n--------------------------------------------------------------------------------\n\nThis project includes code from Daniel Lemire's JavaFastPFOR project. The\n\"Lemire\" bit packing source code produced by parquet-generator is derived from\nthe JavaFastPFOR project.\n\nCopyright: 2013 Daniel Lemire\nHome page: http://lemire.me/en/\nProject page: https://github.com/lemire/JavaFastPFOR\nLicense: Apache License Version 2.0 http://www.apache.org/licenses/LICENSE-2.0\n\n--------------------------------------------------------------------------------\n\nThis product includes code from Apache Spark.\n\n* dev/merge_parquet_pr.py is based on Spark's dev/merge_spark_pr.py\n\nCopyright: 2014 The Apache Software Foundation.\nHome page: https://spark.apache.org/\nLicense: http://www.apache.org/licenses/LICENSE-2.0\n\n--------------------------------------------------------------------------------\n\nThis product includes code from Twitter's ElephantBird project.\n\n* parquet-hadoop's UnmaterializableRecordCounter.java includes code from\n  ElephantBird's LzoRecordReader.java\n\nCopyright: 2012-2014 Twitter\nHome page: https://github.com/twitter/elephant-bird\nLicense: http://www.apache.org/licenses/LICENSE-2.0\n\n"
        },
        {
          "name": "NOTICE",
          "type": "blob",
          "size": 3.4794921875,
          "content": "\nApache Parquet Java\nCopyright 2014-2024 The Apache Software Foundation\n\nThis product includes software developed at\nThe Apache Software Foundation (http://www.apache.org/).\n\n--------------------------------------------------------------------------------\n\nThis product includes parquet-tools, initially developed at ARRIS, Inc. with\nthe following copyright notice:\n\n  Copyright 2013 ARRIS, Inc.\n\n  Licensed under the Apache License, Version 2.0 (the \"License\");\n  you may not use this file except in compliance with the License.\n  You may obtain a copy of the License at\n\n  http://www.apache.org/licenses/LICENSE-2.0\n\n  Unless required by applicable law or agreed to in writing, software\n  distributed under the License is distributed on an \"AS IS\" BASIS,\n  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n  See the License for the specific language governing permissions and\n  limitations under the License.\n\n--------------------------------------------------------------------------------\n\nThis product includes parquet-protobuf, initially developed by Lukas Nalezenc\nwith the following copyright notice:\n\n  Copyright 2013 Lukas Nalezenec.\n\n  Licensed under the Apache License, Version 2.0 (the \"License\");\n  you may not use this file except in compliance with the License.\n  You may obtain a copy of the License at\n\n  http://www.apache.org/licenses/LICENSE-2.0\n\n  Unless required by applicable law or agreed to in writing, software\n  distributed under the License is distributed on an \"AS IS\" BASIS,\n  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n  See the License for the specific language governing permissions and\n  limitations under the License.\n\n--------------------------------------------------------------------------------\n\nThis product includes code from Apache Avro, which includes the following in\nits NOTICE file:\n\n  Apache Avro\n  Copyright 2010-2015 The Apache Software Foundation\n\n  This product includes software developed at\n  The Apache Software Foundation (http://www.apache.org/).\n\n--------------------------------------------------------------------------------\n\nThis project includes code from Kite, developed at Cloudera, Inc. with\nthe following copyright notice:\n\n| Copyright 2013 Cloudera Inc.\n|\n| Licensed under the Apache License, Version 2.0 (the \"License\");\n| you may not use this file except in compliance with the License.\n| You may obtain a copy of the License at\n|\n|   http://www.apache.org/licenses/LICENSE-2.0\n|\n| Unless required by applicable law or agreed to in writing, software\n| distributed under the License is distributed on an \"AS IS\" BASIS,\n| WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n| See the License for the specific language governing permissions and\n| limitations under the License.\n\n--------------------------------------------------------------------------------\n\nThis project includes code from Netflix, Inc. with the following copyright\nnotice:\n\n| Copyright 2016 Netflix, Inc.\n|\n| Licensed under the Apache License, Version 2.0 (the \"License\");\n| you may not use this file except in compliance with the License.\n| You may obtain a copy of the License at\n|\n|   http://www.apache.org/licenses/LICENSE-2.0\n|\n| Unless required by applicable law or agreed to in writing, software\n| distributed under the License is distributed on an \"AS IS\" BASIS,\n| WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n| See the License for the specific language governing permissions and\n| limitations under the License.\n\n"
        },
        {
          "name": "PoweredBy.md",
          "type": "blob",
          "size": 2.9775390625,
          "content": "<!--\n  ~ Licensed to the Apache Software Foundation (ASF) under one\n  ~ or more contributor license agreements.  See the NOTICE file\n  ~ distributed with this work for additional information\n  ~ regarding copyright ownership.  The ASF licenses this file\n  ~ to you under the Apache License, Version 2.0 (the\n  ~ \"License\"); you may not use this file except in compliance\n  ~ with the License.  You may obtain a copy of the License at\n  ~\n  ~   http://www.apache.org/licenses/LICENSE-2.0\n  ~\n  ~ Unless required by applicable law or agreed to in writing,\n  ~ software distributed under the License is distributed on an\n  ~ \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n  ~ KIND, either express or implied.  See the License for the\n  ~ specific language governing permissions and limitations\n  ~ under the License.\n  -->\n\nWho's using Parquet?\n======\n(in alphabetical order)\n\n## Cloudera Impala\n\n<blockquote class=\"twitter-tweet\"><p>We shipped Impala 0.7 (<a href=\"http://t.co/wxuV0wYShk\">http://t.co/wxuV0wYShk</a>) - a whole ton of great new features including DDL, Parquet support and partitioned joins!</p>&mdash; Henry Robinson (@HenryR) <a href=\"https://twitter.com/HenryR/statuses/324222874011451392\">April 16, 2013</a></blockquote>\n\n## Criteo\n\n<blockquote class=\"twitter-tweet\"><p>Parquet: Efficient Columnar Storage for Apache <a href=\"https://twitter.com/search?q=%23Hadoop&amp;src=hash\">#Hadoop</a> <a href=\"http://t.co/He1xyv6NC3\">http://t.co/He1xyv6NC3</a> via <a href=\"https://twitter.com/cloudera\">@cloudera</a> - <a href=\"https://twitter.com/search?q=%23Criteo&amp;src=hash\">#Criteo</a> R&amp;D very happy to contribute!</p>&mdash; Julien SIMON (@julsimon) <a href=\"https://twitter.com/julsimon/statuses/312114074911666177\">March 14, 2013</a></blockquote>\n\n## Salesforce.com\n\n<blockquote class=\"twitter-tweet\"><p>&quot;<a href=\"https://twitter.com/ParquetFormat\">@ParquetFormat</a> at <a href=\"http://t.co/lro7m7quuc\">Salesforce.com</a>&quot; <a href=\"http://t.co/IFskqF0FP3\">http://t.co/IFskqF0FP3</a> via <a href=\"https://twitter.com/cloudera\">@cloudera</a></p>&mdash; Twitter Open Source (@TwitterOSS) <a href=\"https://twitter.com/TwitterOSS/statuses/392734610116726784\">October 22, 2013</a></blockquote>\n\n## Stripe\n\n<blockquote class=\"twitter-tweet\"><p>We&#39;re moving basically all of our archival data at <a href=\"https://twitter.com/stripe\">@stripe</a> into <a href=\"https://twitter.com/ParquetFormat\">@ParquetFormat</a>, and I&#39;m super pleased with how well it&#39;s working out.</p>&mdash; Avi Bryant (@avibryant) <a href=\"https://twitter.com/avibryant/statuses/391339949250715648\">October 18, 2013</a></blockquote>\n\n## Twitter\n\n<blockquote class=\"twitter-tweet\"><p>Converting some data to Parquet on the Twitter clusters. I&#39;m seeing a 28% space saving thanks to the compressibility of the column layout.</p>&mdash; Julien Le Dem (@J_) <a href=\"https://twitter.com/J_/statuses/315844725611581441\">March 24, 2013</a></blockquote>\n\n<script async src=\"//platform.twitter.com/widgets.js\" charset=\"utf-8\"></script>\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 11.16796875,
          "content": "<!--\n  ~ Licensed to the Apache Software Foundation (ASF) under one\n  ~ or more contributor license agreements.  See the NOTICE file\n  ~ distributed with this work for additional information\n  ~ regarding copyright ownership.  The ASF licenses this file\n  ~ to you under the Apache License, Version 2.0 (the\n  ~ \"License\"); you may not use this file except in compliance\n  ~ with the License.  You may obtain a copy of the License at\n  ~\n  ~   http://www.apache.org/licenses/LICENSE-2.0\n  ~\n  ~ Unless required by applicable law or agreed to in writing,\n  ~ software distributed under the License is distributed on an\n  ~ \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n  ~ KIND, either express or implied.  See the License for the\n  ~ specific language governing permissions and limitations\n  ~ under the License.\n  -->\n\nParquet Java (formerly Parquet MR) [![CI Hadoop 3](https://github.com/apache/parquet-java/actions/workflows/ci-hadoop3.yml/badge.svg)](https://github.com/apache/parquet-java/actions/workflows/ci-hadoop3.yml)\n======\n\nThis repository contains a Java implementation of [Apache Parquet](https://parquet.apache.org/)\n\nApache Parquet is an open source, column-oriented data file format\ndesigned for efficient data storage and retrieval. It provides high\nperformance compression and encoding schemes to handle complex data in\nbulk and is supported in many programming languages and analytics\ntools.\n\nThe [parquet-format](https://github.com/apache/parquet-format)\nrepository contains the file format specification.\n\nParquet uses the [record shredding and assembly algorithm](https://github.com/julienledem/redelm/wiki/The-striping-and-assembly-algorithms-from-the-Dremel-paper) described in the Dremel paper to represent nested structures.\nYou can find additional details about the format and intended use cases in our [Hadoop Summit 2013 presentation](http://www.slideshare.net/julienledem/parquet-hadoop-summit-2013)\n\n## Building\n\nParquet-Java uses Maven to build and depends on the thrift compiler (protoc is now managed by maven plugin).\n\n### Install Thrift\n\nTo build and install the thrift compiler, run:\n\n```\nwget -nv https://archive.apache.org/dist/thrift/0.21.0/thrift-0.21.0.tar.gz\ntar xzf thrift-0.21.0.tar.gz\ncd thrift-0.21.0\nchmod +x ./configure\n./configure --disable-libs\nsudo make install -j\n```\n\nIf you're on OSX and use homebrew, you can instead install Thrift 0.21.0 with `brew` and ensure that it comes first in your `PATH`.\n\n```\nbrew install thrift\nexport PATH=\"/usr/local/opt/thrift@0.21.0/bin:$PATH\"\n```\n\n### Build Parquet with Maven\n\nOnce protobuf and thrift are available in your path, you can build the project by running:\n\n```\nLC_ALL=C ./mvnw clean install\n```\n\n## Features\n\nParquet is a very active project, and new features are being added quickly. Here are a few features:\n\n\n* Type-specific encoding\n* Hive integration (deprecated)\n* Pig integration\n* Cascading integration (deprecated)\n* Crunch integration\n* Apache Arrow integration\n* Scrooge integration (deprecated)\n* Impala integration (non-nested)\n* Java Map/Reduce API\n* Native Avro support\n* Native Thrift support\n* Native Protocol Buffers support\n* Complex structure support\n* Run-length encoding (RLE)\n* Bit Packing\n* Adaptive dictionary encoding\n* Predicate pushdown\n* Column stats\n* Delta encoding\n* Index pages\n* Scala DSL (deprecated)\n* Java Vector API support (experimental)\n\n## Java Vector API support\n`The feature is experimental and is currently not part of the parquet distribution`.\nParquet-Java has supported Java Vector API to speed up reading, to enable this feature:\n* Java 17+, 64-bit\n* Requiring the CPU to support instruction sets:\n  * avx512vbmi\n  * avx512_vbmi2\n* To build the jars: `./mvnw clean package -P vector-plugins`\n* For Apache Spark to enable this feature:\n  * Build parquet and replace the parquet-encoding-{VERSION}.jar on the spark jars folder\n  * Build parquet-encoding-vector and copy parquet-encoding-vector-{VERSION}.jar to the spark jars folder\n  * Edit spark class#VectorizedRleValuesReader, function#readNextGroup refer to parquet class#ParquetReadRouter, function#readBatchUsing512Vector\n  * Build spark with maven and replace spark-sql_2.12-{VERSION}.jar on the spark jars folder\n\n## Map/Reduce integration\n\n[Input](https://github.com/apache/parquet-java/blob/master/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetInputFormat.java) and [Output](https://github.com/apache/parquet-java/blob/master/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetOutputFormat.java) formats.\nNote that to use an Input or Output format, you need to implement a WriteSupport or ReadSupport class, which will implement the conversion of your object to and from a Parquet schema.\n\nWe've implemented this for 2 popular data formats to provide a clean migration path as well:\n\n### Thrift\nThrift integration is provided by the [parquet-thrift](https://github.com/apache/parquet-java/tree/master/parquet-thrift) sub-project.\n\n### Avro\nAvro conversion is implemented via the [parquet-avro](https://github.com/apache/parquet-java/tree/master/parquet-avro) sub-project.\n\n### Protobuf\nProtobuf conversion is implemented via the [parquet-protobuf](https://github.com/apache/parquet-java/tree/master/parquet-protobuf) sub-project.\n\n### Create your own objects\n* The ParquetOutputFormat can be provided a WriteSupport to write your own objects to an event based RecordConsumer.\n* The ParquetInputFormat can be provided a ReadSupport to materialize your own objects by implementing a RecordMaterializer\n\nSee the APIs:\n* [Record conversion API](https://github.com/apache/parquet-java/tree/master/parquet-column/src/main/java/org/apache/parquet/io/api)\n* [Hadoop API](https://github.com/apache/parquet-java/tree/master/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/api)\n\n## Apache Pig integration\nA [Loader](https://github.com/apache/parquet-java/blob/master/parquet-pig/src/main/java/org/apache/parquet/pig/ParquetLoader.java) and a [Storer](https://github.com/apache/parquet-java/blob/master/parquet-pig/src/main/java/org/apache/parquet/pig/ParquetStorer.java) are provided to read and write Parquet files with Apache Pig\n\nStoring data into Parquet in Pig is simple:\n```\n-- options you might want to fiddle with\nSET parquet.page.size 1048576 -- default. this is your min read/write unit.\nSET parquet.block.size 134217728 -- default. your memory budget for buffering data\nSET parquet.compression lzo -- or you can use none, gzip, snappy\nSTORE mydata into '/some/path' USING parquet.pig.ParquetStorer;\n```\nReading in Pig is also simple:\n```\nmydata = LOAD '/some/path' USING parquet.pig.ParquetLoader();\n```\n\nIf the data was stored using Pig, things will \"just work\". If the data was stored using another method, you will need to provide the Pig schema equivalent to the data you stored (you can also write the schema to the file footer while writing it -- but that's pretty advanced). We will provide a basic automatic schema conversion soon.\n\n## Hive integration\n\nHive integration is provided via the [parquet-hive](https://github.com/apache/parquet-java/tree/master/parquet-hive) sub-project.\n\nHive integration is now deprecated within the Parquet project. It is now maintained by Apache Hive.\n\n## Build\n\nTo run the unit tests: `./mvnw test`\n\nTo build the jars: `./mvnw package`\n\nThe build runs in [GitHub Actions](https://github.com/apache/parquet-java/actions):\n[![Build Status](https://github.com/apache/parquet-java/workflows/Test/badge.svg)](https://github.com/apache/parquet-java/actions)\n\n## Add Parquet as a dependency in Maven\n\nThe current release is version `1.15.0`.\n\n```xml\n  <dependencies>\n    <dependency>\n      <groupId>org.apache.parquet</groupId>\n      <artifactId>parquet-common</artifactId>\n      <version>1.15.0</version>\n    </dependency>\n    <dependency>\n      <groupId>org.apache.parquet</groupId>\n      <artifactId>parquet-encoding</artifactId>\n      <version>1.15.0</version>\n    </dependency>\n    <dependency>\n      <groupId>org.apache.parquet</groupId>\n      <artifactId>parquet-column</artifactId>\n      <version>1.15.0</version>\n    </dependency>\n    <dependency>\n      <groupId>org.apache.parquet</groupId>\n      <artifactId>parquet-hadoop</artifactId>\n      <version>1.15.0</version>\n    </dependency>\n  </dependencies>\n```\n\n### How To Contribute\n\nWe prefer to receive contributions in the form of GitHub pull requests. Please send pull requests against the [parquet-java](https://github.com/apache/parquet-java) Git repository. If you've previously forked Parquet from its old location, you will need to add a remote or update your origin remote to https://github.com/apache/parquet-java.git\n\nIf you are looking for some ideas on what to contribute, check out jira issues for this project labeled [\"pick-me-up\"](https://issues.apache.org/jira/browse/PARQUET-5?jql=project%20%3D%20PARQUET%20and%20labels%20%3D%20pick-me-up%20and%20status%20%3D%20open).\nComment on the issue and/or contact [dev@parquet.apache.org](http://mail-archives.apache.org/mod_mbox/parquet-dev/) with your questions and ideas.\n\nIf youd like to report a bug but dont have time to fix it, you can still post it to our [issue tracker](https://issues.apache.org/jira/browse/PARQUET), or email the mailing list [dev@parquet.apache.org](http://mail-archives.apache.org/mod_mbox/parquet-dev/)\n\nTo contribute a patch:\n\n  1. Break your work into small, single-purpose patches if possible. Its much harder to merge in a large change with a lot of disjoint features.\n  2. Create a JIRA for your patch on the [Parquet Project JIRA](https://issues.apache.org/jira/browse/PARQUET).\n  3. Submit the patch as a GitHub pull request against the master branch. For a tutorial, see the GitHub guides on forking a repo and sending a pull request. Prefix your pull request name with the JIRA name (ex: https://github.com/apache/parquet-java/pull/240).\n  4. Make sure that your code passes the unit tests. You can run the tests with `./mvnw test` in the root directory.\n  5. Add new unit tests for your code.\n\nWe tend to do fairly close readings of pull requests, and you may get a lot of comments. Some common issues that are not code structure related, but still important:\n  * Use 2 spaces for whitespace. Not tabs, not 4 spaces. The number of the spacing shall be 2.\n  * Give your operators some room. Not `a+b` but `a + b` and not `foo(int a,int b)` but `foo(int a, int b)`.\n  * Generally speaking, stick to the [Sun Java Code Conventions](http://www.oracle.com/technetwork/java/javase/documentation/codeconvtoc-136057.html)\n  * Make sure tests pass!\n\nThank you for getting involved!\n\n## Authors and contributors\n\n* [Contributors](https://github.com/apache/parquet-java/graphs/contributors)\n* [Committers](dev/COMMITTERS.md)\n\n## Code of Conduct\n\nWe hold ourselves and the Parquet developer community to two codes of conduct:\n  1. [The Apache Software Foundation Code of Conduct](https://www.apache.org/foundation/policies/conduct.html)\n  2. [The Twitter OSS Code of Conduct](https://github.com/twitter/code-of-conduct/blob/master/code-of-conduct.md)\n\n## Discussions\n* Mailing list: [dev@parquet.apache.org](http://mail-archives.apache.org/mod_mbox/parquet-dev/)\n* Bug tracker: [jira](https://issues.apache.org/jira/browse/PARQUET)\n* Discussions also take place in github pull requests\n\n## License\n\nLicensed under the Apache License, Version 2.0: http://www.apache.org/licenses/LICENSE-2.0\n"
        },
        {
          "name": "changelog.sh",
          "type": "blob",
          "size": 2.126953125,
          "content": "#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n#\n\nOAUTH_FILE=~/.github_oauth_for_changelog\nif [ -f $OAUTH_FILE ]\nthen\n  token=`cat $OAUTH_FILE`\nelse\n  echo \"Please create an oauth token here: https://github.com/settings/tokens/new\"\n  echo \"Then paste it bellow (it will be saved in $OAUTH_FILE):\" >&2\n  read token >&2\n  echo $token > $OAUTH_FILE\n  chmod og-rwx $OAUTH_FILE\nfi\nTOKEN_HEADER=\"Authorization: token $token\"\n\ncurl -f -H \"$TOKEN_HEADER\" -s \"https://api.github.com\" > /dev/null\nif [ $? == 0 ]\nthen\n  echo \"login successful\" >&2\nelse\n  echo \"login failed\" >&2\n  curl -H \"$TOKEN_HEADER\" -s \"https://api.github.com\"\n  echo \"if your OAUTH token needs to be replaced you can delete file $OAUTH_FILE\"\n  exit 1\nfi\n\necho \"# Parquet #\"\n\ngit log | grep -E \"Merge pull request|prepare release\" | while read l\ndo \n  release=`echo $l | grep \"\\[maven-release-plugin\\] prepare release\" | cut -d \"-\" -f 4`\n  PR=`echo $l| grep -E -o \"Merge pull request #[^ ]*\" | cut -d \"#\" -f 2`\n#  echo $l\n  if [ -n \"$release\" ] \n  then \n    echo\n    echo \"### Version $release ###\"\n  fi\n  if [ -n \"$PR\" ]\n  then\n    JSON=`curl -H \"$TOKEN_HEADER\" -s https://api.github.com/repos/Parquet/parquet-mr/pulls/$PR | tr \"\\n\" \" \"`\n    DESC_RAW=$(echo $JSON |  grep -Eo '\"title\":.*?[^\\\\]\",' | cut -d \"\\\"\" -f 4- | head -n 1 | sed -e \"s/\\\\\\\\//g\")\n    DESC=$(echo ${DESC_RAW%\\\",})\n    echo \"* ISSUE [$PR](https://github.com/Parquet/parquet-mr/pull/$PR): ${DESC}\"\n  fi\ndone\n"
        },
        {
          "name": "dev",
          "type": "tree",
          "content": null
        },
        {
          "name": "doc",
          "type": "tree",
          "content": null
        },
        {
          "name": "mvnw",
          "type": "blob",
          "size": 10.4150390625,
          "content": "#!/bin/sh\n# ----------------------------------------------------------------------------\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n# ----------------------------------------------------------------------------\n\n# ----------------------------------------------------------------------------\n# Apache Maven Wrapper startup batch script, version 3.3.2\n#\n# Optional ENV vars\n# -----------------\n#   JAVA_HOME - location of a JDK home dir, required when download maven via java source\n#   MVNW_REPOURL - repo url base for downloading maven distribution\n#   MVNW_USERNAME/MVNW_PASSWORD - user and password for downloading maven\n#   MVNW_VERBOSE - true: enable verbose log; debug: trace the mvnw script; others: silence the output\n# ----------------------------------------------------------------------------\n\nset -euf\n[ \"${MVNW_VERBOSE-}\" != debug ] || set -x\n\n# OS specific support.\nnative_path() { printf %s\\\\n \"$1\"; }\ncase \"$(uname)\" in\nCYGWIN* | MINGW*)\n  [ -z \"${JAVA_HOME-}\" ] || JAVA_HOME=\"$(cygpath --unix \"$JAVA_HOME\")\"\n  native_path() { cygpath --path --windows \"$1\"; }\n  ;;\nesac\n\n# set JAVACMD and JAVACCMD\nset_java_home() {\n  # For Cygwin and MinGW, ensure paths are in Unix format before anything is touched\n  if [ -n \"${JAVA_HOME-}\" ]; then\n    if [ -x \"$JAVA_HOME/jre/sh/java\" ]; then\n      # IBM's JDK on AIX uses strange locations for the executables\n      JAVACMD=\"$JAVA_HOME/jre/sh/java\"\n      JAVACCMD=\"$JAVA_HOME/jre/sh/javac\"\n    else\n      JAVACMD=\"$JAVA_HOME/bin/java\"\n      JAVACCMD=\"$JAVA_HOME/bin/javac\"\n\n      if [ ! -x \"$JAVACMD\" ] || [ ! -x \"$JAVACCMD\" ]; then\n        echo \"The JAVA_HOME environment variable is not defined correctly, so mvnw cannot run.\" >&2\n        echo \"JAVA_HOME is set to \\\"$JAVA_HOME\\\", but \\\"\\$JAVA_HOME/bin/java\\\" or \\\"\\$JAVA_HOME/bin/javac\\\" does not exist.\" >&2\n        return 1\n      fi\n    fi\n  else\n    JAVACMD=\"$(\n      'set' +e\n      'unset' -f command 2>/dev/null\n      'command' -v java\n    )\" || :\n    JAVACCMD=\"$(\n      'set' +e\n      'unset' -f command 2>/dev/null\n      'command' -v javac\n    )\" || :\n\n    if [ ! -x \"${JAVACMD-}\" ] || [ ! -x \"${JAVACCMD-}\" ]; then\n      echo \"The java/javac command does not exist in PATH nor is JAVA_HOME set, so mvnw cannot run.\" >&2\n      return 1\n    fi\n  fi\n}\n\n# hash string like Java String::hashCode\nhash_string() {\n  str=\"${1:-}\" h=0\n  while [ -n \"$str\" ]; do\n    char=\"${str%\"${str#?}\"}\"\n    h=$(((h * 31 + $(LC_CTYPE=C printf %d \"'$char\")) % 4294967296))\n    str=\"${str#?}\"\n  done\n  printf %x\\\\n $h\n}\n\nverbose() { :; }\n[ \"${MVNW_VERBOSE-}\" != true ] || verbose() { printf %s\\\\n \"${1-}\"; }\n\ndie() {\n  printf %s\\\\n \"$1\" >&2\n  exit 1\n}\n\ntrim() {\n  # MWRAPPER-139:\n  #   Trims trailing and leading whitespace, carriage returns, tabs, and linefeeds.\n  #   Needed for removing poorly interpreted newline sequences when running in more\n  #   exotic environments such as mingw bash on Windows.\n  printf \"%s\" \"${1}\" | tr -d '[:space:]'\n}\n\n# parse distributionUrl and optional distributionSha256Sum, requires .mvn/wrapper/maven-wrapper.properties\nwhile IFS=\"=\" read -r key value; do\n  case \"${key-}\" in\n  distributionUrl) distributionUrl=$(trim \"${value-}\") ;;\n  distributionSha256Sum) distributionSha256Sum=$(trim \"${value-}\") ;;\n  esac\ndone <\"${0%/*}/.mvn/wrapper/maven-wrapper.properties\"\n[ -n \"${distributionUrl-}\" ] || die \"cannot read distributionUrl property in ${0%/*}/.mvn/wrapper/maven-wrapper.properties\"\n\ncase \"${distributionUrl##*/}\" in\nmaven-mvnd-*bin.*)\n  MVN_CMD=mvnd.sh _MVNW_REPO_PATTERN=/maven/mvnd/\n  case \"${PROCESSOR_ARCHITECTURE-}${PROCESSOR_ARCHITEW6432-}:$(uname -a)\" in\n  *AMD64:CYGWIN* | *AMD64:MINGW*) distributionPlatform=windows-amd64 ;;\n  :Darwin*x86_64) distributionPlatform=darwin-amd64 ;;\n  :Darwin*arm64) distributionPlatform=darwin-aarch64 ;;\n  :Linux*x86_64*) distributionPlatform=linux-amd64 ;;\n  *)\n    echo \"Cannot detect native platform for mvnd on $(uname)-$(uname -m), use pure java version\" >&2\n    distributionPlatform=linux-amd64\n    ;;\n  esac\n  distributionUrl=\"${distributionUrl%-bin.*}-$distributionPlatform.zip\"\n  ;;\nmaven-mvnd-*) MVN_CMD=mvnd.sh _MVNW_REPO_PATTERN=/maven/mvnd/ ;;\n*) MVN_CMD=\"mvn${0##*/mvnw}\" _MVNW_REPO_PATTERN=/org/apache/maven/ ;;\nesac\n\n# apply MVNW_REPOURL and calculate MAVEN_HOME\n# maven home pattern: ~/.m2/wrapper/dists/{apache-maven-<version>,maven-mvnd-<version>-<platform>}/<hash>\n[ -z \"${MVNW_REPOURL-}\" ] || distributionUrl=\"$MVNW_REPOURL$_MVNW_REPO_PATTERN${distributionUrl#*\"$_MVNW_REPO_PATTERN\"}\"\ndistributionUrlName=\"${distributionUrl##*/}\"\ndistributionUrlNameMain=\"${distributionUrlName%.*}\"\ndistributionUrlNameMain=\"${distributionUrlNameMain%-bin}\"\nMAVEN_USER_HOME=\"${MAVEN_USER_HOME:-${HOME}/.m2}\"\nMAVEN_HOME=\"${MAVEN_USER_HOME}/wrapper/dists/${distributionUrlNameMain-}/$(hash_string \"$distributionUrl\")\"\n\nexec_maven() {\n  unset MVNW_VERBOSE MVNW_USERNAME MVNW_PASSWORD MVNW_REPOURL || :\n  exec \"$MAVEN_HOME/bin/$MVN_CMD\" \"$@\" || die \"cannot exec $MAVEN_HOME/bin/$MVN_CMD\"\n}\n\nif [ -d \"$MAVEN_HOME\" ]; then\n  verbose \"found existing MAVEN_HOME at $MAVEN_HOME\"\n  exec_maven \"$@\"\nfi\n\ncase \"${distributionUrl-}\" in\n*?-bin.zip | *?maven-mvnd-?*-?*.zip) ;;\n*) die \"distributionUrl is not valid, must match *-bin.zip or maven-mvnd-*.zip, but found '${distributionUrl-}'\" ;;\nesac\n\n# prepare tmp dir\nif TMP_DOWNLOAD_DIR=\"$(mktemp -d)\" && [ -d \"$TMP_DOWNLOAD_DIR\" ]; then\n  clean() { rm -rf -- \"$TMP_DOWNLOAD_DIR\"; }\n  trap clean HUP INT TERM EXIT\nelse\n  die \"cannot create temp dir\"\nfi\n\nmkdir -p -- \"${MAVEN_HOME%/*}\"\n\n# Download and Install Apache Maven\nverbose \"Couldn't find MAVEN_HOME, downloading and installing it ...\"\nverbose \"Downloading from: $distributionUrl\"\nverbose \"Downloading to: $TMP_DOWNLOAD_DIR/$distributionUrlName\"\n\n# select .zip or .tar.gz\nif ! command -v unzip >/dev/null; then\n  distributionUrl=\"${distributionUrl%.zip}.tar.gz\"\n  distributionUrlName=\"${distributionUrl##*/}\"\nfi\n\n# verbose opt\n__MVNW_QUIET_WGET=--quiet __MVNW_QUIET_CURL=--silent __MVNW_QUIET_UNZIP=-q __MVNW_QUIET_TAR=''\n[ \"${MVNW_VERBOSE-}\" != true ] || __MVNW_QUIET_WGET='' __MVNW_QUIET_CURL='' __MVNW_QUIET_UNZIP='' __MVNW_QUIET_TAR=v\n\n# normalize http auth\ncase \"${MVNW_PASSWORD:+has-password}\" in\n'') MVNW_USERNAME='' MVNW_PASSWORD='' ;;\nhas-password) [ -n \"${MVNW_USERNAME-}\" ] || MVNW_USERNAME='' MVNW_PASSWORD='' ;;\nesac\n\nif [ -z \"${MVNW_USERNAME-}\" ] && command -v wget >/dev/null; then\n  verbose \"Found wget ... using wget\"\n  wget ${__MVNW_QUIET_WGET:+\"$__MVNW_QUIET_WGET\"} \"$distributionUrl\" -O \"$TMP_DOWNLOAD_DIR/$distributionUrlName\" || die \"wget: Failed to fetch $distributionUrl\"\nelif [ -z \"${MVNW_USERNAME-}\" ] && command -v curl >/dev/null; then\n  verbose \"Found curl ... using curl\"\n  curl ${__MVNW_QUIET_CURL:+\"$__MVNW_QUIET_CURL\"} -f -L -o \"$TMP_DOWNLOAD_DIR/$distributionUrlName\" \"$distributionUrl\" || die \"curl: Failed to fetch $distributionUrl\"\nelif set_java_home; then\n  verbose \"Falling back to use Java to download\"\n  javaSource=\"$TMP_DOWNLOAD_DIR/Downloader.java\"\n  targetZip=\"$TMP_DOWNLOAD_DIR/$distributionUrlName\"\n  cat >\"$javaSource\" <<-END\n\tpublic class Downloader extends java.net.Authenticator\n\t{\n\t  protected java.net.PasswordAuthentication getPasswordAuthentication()\n\t  {\n\t    return new java.net.PasswordAuthentication( System.getenv( \"MVNW_USERNAME\" ), System.getenv( \"MVNW_PASSWORD\" ).toCharArray() );\n\t  }\n\t  public static void main( String[] args ) throws Exception\n\t  {\n\t    setDefault( new Downloader() );\n\t    java.nio.file.Files.copy( java.net.URI.create( args[0] ).toURL().openStream(), java.nio.file.Paths.get( args[1] ).toAbsolutePath().normalize() );\n\t  }\n\t}\n\tEND\n  # For Cygwin/MinGW, switch paths to Windows format before running javac and java\n  verbose \" - Compiling Downloader.java ...\"\n  \"$(native_path \"$JAVACCMD\")\" \"$(native_path \"$javaSource\")\" || die \"Failed to compile Downloader.java\"\n  verbose \" - Running Downloader.java ...\"\n  \"$(native_path \"$JAVACMD\")\" -cp \"$(native_path \"$TMP_DOWNLOAD_DIR\")\" Downloader \"$distributionUrl\" \"$(native_path \"$targetZip\")\"\nfi\n\n# If specified, validate the SHA-256 sum of the Maven distribution zip file\nif [ -n \"${distributionSha256Sum-}\" ]; then\n  distributionSha256Result=false\n  if [ \"$MVN_CMD\" = mvnd.sh ]; then\n    echo \"Checksum validation is not supported for maven-mvnd.\" >&2\n    echo \"Please disable validation by removing 'distributionSha256Sum' from your maven-wrapper.properties.\" >&2\n    exit 1\n  elif command -v sha256sum >/dev/null; then\n    if echo \"$distributionSha256Sum  $TMP_DOWNLOAD_DIR/$distributionUrlName\" | sha256sum -c >/dev/null 2>&1; then\n      distributionSha256Result=true\n    fi\n  elif command -v shasum >/dev/null; then\n    if echo \"$distributionSha256Sum  $TMP_DOWNLOAD_DIR/$distributionUrlName\" | shasum -a 256 -c >/dev/null 2>&1; then\n      distributionSha256Result=true\n    fi\n  else\n    echo \"Checksum validation was requested but neither 'sha256sum' or 'shasum' are available.\" >&2\n    echo \"Please install either command, or disable validation by removing 'distributionSha256Sum' from your maven-wrapper.properties.\" >&2\n    exit 1\n  fi\n  if [ $distributionSha256Result = false ]; then\n    echo \"Error: Failed to validate Maven distribution SHA-256, your Maven distribution might be compromised.\" >&2\n    echo \"If you updated your Maven version, you need to update the specified distributionSha256Sum property.\" >&2\n    exit 1\n  fi\nfi\n\n# unzip and move\nif command -v unzip >/dev/null; then\n  unzip ${__MVNW_QUIET_UNZIP:+\"$__MVNW_QUIET_UNZIP\"} \"$TMP_DOWNLOAD_DIR/$distributionUrlName\" -d \"$TMP_DOWNLOAD_DIR\" || die \"failed to unzip\"\nelse\n  tar xzf${__MVNW_QUIET_TAR:+\"$__MVNW_QUIET_TAR\"} \"$TMP_DOWNLOAD_DIR/$distributionUrlName\" -C \"$TMP_DOWNLOAD_DIR\" || die \"failed to untar\"\nfi\nprintf %s\\\\n \"$distributionUrl\" >\"$TMP_DOWNLOAD_DIR/$distributionUrlNameMain/mvnw.url\"\nmv -- \"$TMP_DOWNLOAD_DIR/$distributionUrlNameMain\" \"$MAVEN_HOME\" || [ -d \"$MAVEN_HOME\" ] || die \"fail to move MAVEN_HOME\"\n\nclean || :\nexec_maven \"$@\"\n"
        },
        {
          "name": "mvnw.cmd",
          "type": "blob",
          "size": 6.75,
          "content": "<# : batch portion\n@REM ----------------------------------------------------------------------------\n@REM Licensed to the Apache Software Foundation (ASF) under one\n@REM or more contributor license agreements.  See the NOTICE file\n@REM distributed with this work for additional information\n@REM regarding copyright ownership.  The ASF licenses this file\n@REM to you under the Apache License, Version 2.0 (the\n@REM \"License\"); you may not use this file except in compliance\n@REM with the License.  You may obtain a copy of the License at\n@REM\n@REM    http://www.apache.org/licenses/LICENSE-2.0\n@REM\n@REM Unless required by applicable law or agreed to in writing,\n@REM software distributed under the License is distributed on an\n@REM \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n@REM KIND, either express or implied.  See the License for the\n@REM specific language governing permissions and limitations\n@REM under the License.\n@REM ----------------------------------------------------------------------------\n\n@REM ----------------------------------------------------------------------------\n@REM Apache Maven Wrapper startup batch script, version 3.3.2\n@REM\n@REM Optional ENV vars\n@REM   MVNW_REPOURL - repo url base for downloading maven distribution\n@REM   MVNW_USERNAME/MVNW_PASSWORD - user and password for downloading maven\n@REM   MVNW_VERBOSE - true: enable verbose log; others: silence the output\n@REM ----------------------------------------------------------------------------\n\n@IF \"%__MVNW_ARG0_NAME__%\"==\"\" (SET __MVNW_ARG0_NAME__=%~nx0)\n@SET __MVNW_CMD__=\n@SET __MVNW_ERROR__=\n@SET __MVNW_PSMODULEP_SAVE=%PSModulePath%\n@SET PSModulePath=\n@FOR /F \"usebackq tokens=1* delims==\" %%A IN (`powershell -noprofile \"& {$scriptDir='%~dp0'; $script='%__MVNW_ARG0_NAME__%'; icm -ScriptBlock ([Scriptblock]::Create((Get-Content -Raw '%~f0'))) -NoNewScope}\"`) DO @(\n  IF \"%%A\"==\"MVN_CMD\" (set __MVNW_CMD__=%%B) ELSE IF \"%%B\"==\"\" (echo %%A) ELSE (echo %%A=%%B)\n)\n@SET PSModulePath=%__MVNW_PSMODULEP_SAVE%\n@SET __MVNW_PSMODULEP_SAVE=\n@SET __MVNW_ARG0_NAME__=\n@SET MVNW_USERNAME=\n@SET MVNW_PASSWORD=\n@IF NOT \"%__MVNW_CMD__%\"==\"\" (%__MVNW_CMD__% %*)\n@echo Cannot start maven from wrapper >&2 && exit /b 1\n@GOTO :EOF\n: end batch / begin powershell #>\n\n$ErrorActionPreference = \"Stop\"\nif ($env:MVNW_VERBOSE -eq \"true\") {\n  $VerbosePreference = \"Continue\"\n}\n\n# calculate distributionUrl, requires .mvn/wrapper/maven-wrapper.properties\n$distributionUrl = (Get-Content -Raw \"$scriptDir/.mvn/wrapper/maven-wrapper.properties\" | ConvertFrom-StringData).distributionUrl\nif (!$distributionUrl) {\n  Write-Error \"cannot read distributionUrl property in $scriptDir/.mvn/wrapper/maven-wrapper.properties\"\n}\n\nswitch -wildcard -casesensitive ( $($distributionUrl -replace '^.*/','') ) {\n  \"maven-mvnd-*\" {\n    $USE_MVND = $true\n    $distributionUrl = $distributionUrl -replace '-bin\\.[^.]*$',\"-windows-amd64.zip\"\n    $MVN_CMD = \"mvnd.cmd\"\n    break\n  }\n  default {\n    $USE_MVND = $false\n    $MVN_CMD = $script -replace '^mvnw','mvn'\n    break\n  }\n}\n\n# apply MVNW_REPOURL and calculate MAVEN_HOME\n# maven home pattern: ~/.m2/wrapper/dists/{apache-maven-<version>,maven-mvnd-<version>-<platform>}/<hash>\nif ($env:MVNW_REPOURL) {\n  $MVNW_REPO_PATTERN = if ($USE_MVND) { \"/org/apache/maven/\" } else { \"/maven/mvnd/\" }\n  $distributionUrl = \"$env:MVNW_REPOURL$MVNW_REPO_PATTERN$($distributionUrl -replace '^.*'+$MVNW_REPO_PATTERN,'')\"\n}\n$distributionUrlName = $distributionUrl -replace '^.*/',''\n$distributionUrlNameMain = $distributionUrlName -replace '\\.[^.]*$','' -replace '-bin$',''\n$MAVEN_HOME_PARENT = \"$HOME/.m2/wrapper/dists/$distributionUrlNameMain\"\nif ($env:MAVEN_USER_HOME) {\n  $MAVEN_HOME_PARENT = \"$env:MAVEN_USER_HOME/wrapper/dists/$distributionUrlNameMain\"\n}\n$MAVEN_HOME_NAME = ([System.Security.Cryptography.MD5]::Create().ComputeHash([byte[]][char[]]$distributionUrl) | ForEach-Object {$_.ToString(\"x2\")}) -join ''\n$MAVEN_HOME = \"$MAVEN_HOME_PARENT/$MAVEN_HOME_NAME\"\n\nif (Test-Path -Path \"$MAVEN_HOME\" -PathType Container) {\n  Write-Verbose \"found existing MAVEN_HOME at $MAVEN_HOME\"\n  Write-Output \"MVN_CMD=$MAVEN_HOME/bin/$MVN_CMD\"\n  exit $?\n}\n\nif (! $distributionUrlNameMain -or ($distributionUrlName -eq $distributionUrlNameMain)) {\n  Write-Error \"distributionUrl is not valid, must end with *-bin.zip, but found $distributionUrl\"\n}\n\n# prepare tmp dir\n$TMP_DOWNLOAD_DIR_HOLDER = New-TemporaryFile\n$TMP_DOWNLOAD_DIR = New-Item -Itemtype Directory -Path \"$TMP_DOWNLOAD_DIR_HOLDER.dir\"\n$TMP_DOWNLOAD_DIR_HOLDER.Delete() | Out-Null\ntrap {\n  if ($TMP_DOWNLOAD_DIR.Exists) {\n    try { Remove-Item $TMP_DOWNLOAD_DIR -Recurse -Force | Out-Null }\n    catch { Write-Warning \"Cannot remove $TMP_DOWNLOAD_DIR\" }\n  }\n}\n\nNew-Item -Itemtype Directory -Path \"$MAVEN_HOME_PARENT\" -Force | Out-Null\n\n# Download and Install Apache Maven\nWrite-Verbose \"Couldn't find MAVEN_HOME, downloading and installing it ...\"\nWrite-Verbose \"Downloading from: $distributionUrl\"\nWrite-Verbose \"Downloading to: $TMP_DOWNLOAD_DIR/$distributionUrlName\"\n\n$webclient = New-Object System.Net.WebClient\nif ($env:MVNW_USERNAME -and $env:MVNW_PASSWORD) {\n  $webclient.Credentials = New-Object System.Net.NetworkCredential($env:MVNW_USERNAME, $env:MVNW_PASSWORD)\n}\n[Net.ServicePointManager]::SecurityProtocol = [Net.SecurityProtocolType]::Tls12\n$webclient.DownloadFile($distributionUrl, \"$TMP_DOWNLOAD_DIR/$distributionUrlName\") | Out-Null\n\n# If specified, validate the SHA-256 sum of the Maven distribution zip file\n$distributionSha256Sum = (Get-Content -Raw \"$scriptDir/.mvn/wrapper/maven-wrapper.properties\" | ConvertFrom-StringData).distributionSha256Sum\nif ($distributionSha256Sum) {\n  if ($USE_MVND) {\n    Write-Error \"Checksum validation is not supported for maven-mvnd. `nPlease disable validation by removing 'distributionSha256Sum' from your maven-wrapper.properties.\"\n  }\n  Import-Module $PSHOME\\Modules\\Microsoft.PowerShell.Utility -Function Get-FileHash\n  if ((Get-FileHash \"$TMP_DOWNLOAD_DIR/$distributionUrlName\" -Algorithm SHA256).Hash.ToLower() -ne $distributionSha256Sum) {\n    Write-Error \"Error: Failed to validate Maven distribution SHA-256, your Maven distribution might be compromised. If you updated your Maven version, you need to update the specified distributionSha256Sum property.\"\n  }\n}\n\n# unzip and move\nExpand-Archive \"$TMP_DOWNLOAD_DIR/$distributionUrlName\" -DestinationPath \"$TMP_DOWNLOAD_DIR\" | Out-Null\nRename-Item -Path \"$TMP_DOWNLOAD_DIR/$distributionUrlNameMain\" -NewName $MAVEN_HOME_NAME | Out-Null\ntry {\n  Move-Item -Path \"$TMP_DOWNLOAD_DIR/$MAVEN_HOME_NAME\" -Destination $MAVEN_HOME_PARENT | Out-Null\n} catch {\n  if (! (Test-Path -Path \"$MAVEN_HOME\" -PathType Container)) {\n    Write-Error \"fail to move MAVEN_HOME\"\n  }\n} finally {\n  try { Remove-Item $TMP_DOWNLOAD_DIR -Recurse -Force | Out-Null }\n  catch { Write-Warning \"Cannot remove $TMP_DOWNLOAD_DIR\" }\n}\n\nWrite-Output \"MVN_CMD=$MAVEN_HOME/bin/$MVN_CMD\"\n"
        },
        {
          "name": "parquet-arrow",
          "type": "tree",
          "content": null
        },
        {
          "name": "parquet-avro",
          "type": "tree",
          "content": null
        },
        {
          "name": "parquet-benchmarks",
          "type": "tree",
          "content": null
        },
        {
          "name": "parquet-cli",
          "type": "tree",
          "content": null
        },
        {
          "name": "parquet-column",
          "type": "tree",
          "content": null
        },
        {
          "name": "parquet-common",
          "type": "tree",
          "content": null
        },
        {
          "name": "parquet-encoding",
          "type": "tree",
          "content": null
        },
        {
          "name": "parquet-format-structures",
          "type": "tree",
          "content": null
        },
        {
          "name": "parquet-generator",
          "type": "tree",
          "content": null
        },
        {
          "name": "parquet-hadoop-bundle",
          "type": "tree",
          "content": null
        },
        {
          "name": "parquet-hadoop",
          "type": "tree",
          "content": null
        },
        {
          "name": "parquet-jackson",
          "type": "tree",
          "content": null
        },
        {
          "name": "parquet-pig-bundle",
          "type": "tree",
          "content": null
        },
        {
          "name": "parquet-pig",
          "type": "tree",
          "content": null
        },
        {
          "name": "parquet-plugins",
          "type": "tree",
          "content": null
        },
        {
          "name": "parquet-protobuf",
          "type": "tree",
          "content": null
        },
        {
          "name": "parquet-thrift",
          "type": "tree",
          "content": null
        },
        {
          "name": "pom.xml",
          "type": "blob",
          "size": 24.19921875,
          "content": "<!--\n  ~ Licensed to the Apache Software Foundation (ASF) under one\n  ~ or more contributor license agreements.  See the NOTICE file\n  ~ distributed with this work for additional information\n  ~ regarding copyright ownership.  The ASF licenses this file\n  ~ to you under the Apache License, Version 2.0 (the\n  ~ \"License\"); you may not use this file except in compliance\n  ~ with the License.  You may obtain a copy of the License at\n  ~\n  ~   http://www.apache.org/licenses/LICENSE-2.0\n  ~\n  ~ Unless required by applicable law or agreed to in writing,\n  ~ software distributed under the License is distributed on an\n  ~ \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n  ~ KIND, either express or implied.  See the License for the\n  ~ specific language governing permissions and limitations\n  ~ under the License.\n  -->\n<project xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns=\"http://maven.apache.org/POM/4.0.0\"\n         xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/maven-v4_0_0.xsd\">\n  <modelVersion>4.0.0</modelVersion>\n\n  <parent>\n    <groupId>org.apache</groupId>\n    <artifactId>apache</artifactId>\n    <version>33</version>\n  </parent>\n\n  <groupId>org.apache.parquet</groupId>\n  <artifactId>parquet</artifactId>\n  <version>1.16.0-SNAPSHOT</version>\n  <packaging>pom</packaging>\n\n  <name>Apache Parquet Java</name>\n  <url>https://parquet.apache.org</url>\n  <description>Parquet is a columnar storage format that supports nested data. This provides the java implementation.</description>\n\n  <scm>\n    <connection>scm:git:git@github.com:apache/parquet-mr.git</connection>\n    <url>scm:git:git@github.com:apache/parquet-mr.git</url>\n    <developerConnection>scm:git:git@github.com:apache/parquet-mr.git</developerConnection>\n    <tag>HEAD</tag>\n  </scm>\n\n  <licenses>\n    <license>\n      <name>The Apache Software License, Version 2.0</name>\n      <url>https://www.apache.org/licenses/LICENSE-2.0.txt</url>\n    </license>\n  </licenses>\n\n  <issueManagement>\n    <system>GitHub</system>\n    <url>https://github.com/apache/parquet-java/issues</url>\n  </issueManagement>\n\n  <mailingLists>\n    <mailingList>\n      <name>Dev Mailing List</name>\n      <post>dev@parquet.apache.org</post>\n      <subscribe>dev-subscribe@parquet.apache.org</subscribe>\n      <unsubscribe>dev-unsubscribe@parquet.apache.org</unsubscribe>\n    </mailingList>\n    <mailingList>\n      <name>Commits Mailing List</name>\n      <post>commits@parquet.apache.org</post>\n      <subscribe>commits-subscribe@parquet.apache.org</subscribe>\n      <unsubscribe>commits-unsubscribe@parquet.apache.org</unsubscribe>\n    </mailingList>\n  </mailingLists>\n\n  <repositories>\n    <repository>\n      <id>jitpack.io</id>\n      <url>https://jitpack.io</url>\n      <name>Jitpack.io repository</name>\n      <!-- needed for brotli-codec -->\n    </repository>\n  </repositories>\n\n  <properties>\n    <maven.compiler.source>1.8</maven.compiler.source>\n    <maven.compiler.target>1.8</maven.compiler.target>\n    <github.global.server>github</github.global.server>\n    <jackson.groupId>com.fasterxml.jackson.core</jackson.groupId>\n    <jackson.datatype.groupId>com.fasterxml.jackson.datatype</jackson.datatype.groupId>\n    <jackson.package>com.fasterxml.jackson</jackson.package>\n    <!-- To upgrade jackson, check the jdk versions inside the jar and include any new versions in the shading in parquet-jackson. -->\n    <jackson.version>2.18.2</jackson.version>\n    <jackson-databind.version>2.18.2</jackson-databind.version>\n    <japicmp.version>0.21.0</japicmp.version>\n    <javax.annotation.version>1.3.2</javax.annotation.version>\n    <spotless.version>2.30.0</spotless.version>\n    <shade.prefix>shaded.parquet</shade.prefix>\n    <!-- Guarantees no newer classes/methods/constants are used by parquet. -->\n    <hadoop.version>3.3.0</hadoop.version>\n    <parquet.format.version>2.10.0</parquet.format.version>\n    <previous.version>1.15.0</previous.version>\n    <thrift.executable>thrift</thrift.executable>\n    <format.thrift.executable>${thrift.executable}</format.thrift.executable>\n    <pig.version>0.16.0</pig.version>\n    <pig.classifier>h2</pig.classifier>\n    <thrift-maven-plugin.version>0.10.0</thrift-maven-plugin.version>\n    <thrift.version>0.21.0</thrift.version>\n    <format.thrift.version>${thrift.version}</format.thrift.version>\n    <fastutil.version>8.5.13</fastutil.version>\n    <semver.api.version>0.9.33</semver.api.version>\n    <slf4j.version>1.7.33</slf4j.version>\n    <avro.version>1.11.4</avro.version>\n    <guava.version>33.2.1-jre</guava.version>\n    <brotli-codec.version>0.1.1</brotli-codec.version>\n    <mockito.version>1.10.19</mockito.version>\n    <powermock.version>2.0.9</powermock.version>\n    <net.openhft.version>0.27ea0</net.openhft.version>\n    <exec-maven-plugin.version>3.5.0</exec-maven-plugin.version>\n\n    <!-- parquet-cli dependencies -->\n    <opencsv.version>2.3</opencsv.version>\n    <jcommander.version>1.82</jcommander.version>\n    <tukaani.version>1.10</tukaani.version>\n    <zstd-jni.version>1.5.6-6</zstd-jni.version>\n    <commons-text.version>1.12.0</commons-text.version>\n    <jsr305.version>3.0.2</jsr305.version>\n    <commons-lang3.version>3.17.0</commons-lang3.version>\n\n    <!-- properties for the profiles -->\n    <surefire.argLine>-XX:MaxJavaStackTraceDepth=8</surefire.argLine>\n    <surefire.logLevel>ERROR</surefire.logLevel>\n\n    <!-- Resource intesive tests are enabled by default but disabled in the CI envrionment -->\n    <enableResourceIntensiveTests>true</enableResourceIntensiveTests>\n\n    <extraJavaTestArgs>\n      -XX:+IgnoreUnrecognizedVMOptions\n      --add-opens=java.base/java.lang=ALL-UNNAMED\n      --add-opens=java.base/java.lang.invoke=ALL-UNNAMED\n      --add-opens=java.base/java.lang.reflect=ALL-UNNAMED\n      --add-opens=java.base/java.io=ALL-UNNAMED\n      --add-opens=java.base/java.net=ALL-UNNAMED\n      --add-opens=java.base/java.nio=ALL-UNNAMED\n      --add-opens=java.base/java.util=ALL-UNNAMED\n      --add-opens=java.base/java.util.concurrent=ALL-UNNAMED\n      --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED\n      --add-opens=java.base/sun.nio.ch=ALL-UNNAMED\n      --add-opens=java.base/sun.nio.cs=ALL-UNNAMED\n      --add-opens=java.base/sun.security.action=ALL-UNNAMED\n      --add-opens=java.base/sun.util.calendar=ALL-UNNAMED\n    </extraJavaTestArgs>\n  </properties>\n\n  <modules>\n    <module>parquet-arrow</module>\n    <module>parquet-avro</module>\n    <module>parquet-benchmarks</module>\n    <module>parquet-cli</module>\n    <module>parquet-column</module>\n    <module>parquet-common</module>\n    <module>parquet-encoding</module>\n    <module>parquet-format-structures</module>\n    <module>parquet-generator</module>\n    <module>parquet-hadoop</module>\n    <module>parquet-jackson</module>\n    <module>parquet-pig</module>\n    <module>parquet-pig-bundle</module>\n    <module>parquet-protobuf</module>\n    <module>parquet-thrift</module>\n    <module>parquet-hadoop-bundle</module>\n  </modules>\n\n  <dependencies>\n    <dependency>\n      <groupId>junit</groupId>\n      <artifactId>junit</artifactId>\n      <version>4.13.2</version>\n      <scope>test</scope>\n    </dependency>\n    <dependency>\n      <groupId>org.easymock</groupId>\n      <artifactId>easymock</artifactId>\n      <version>5.4.0</version>\n      <scope>test</scope>\n    </dependency>\n  </dependencies>\n\n  <dependencyManagement>\n    <dependencies>\n      <dependency>\n        <groupId>org.apache.hadoop</groupId>\n        <artifactId>hadoop-mapreduce-client-core</artifactId>\n        <version>${hadoop.version}</version>\n        <exclusions>\n          <exclusion>\n            <groupId>org.slf4j</groupId>\n            <artifactId>*</artifactId>\n          </exclusion>\n          <exclusion>\n            <groupId>ch.qos.reload4j</groupId>\n            <artifactId>*</artifactId>\n          </exclusion>\n        </exclusions>\n      </dependency>\n      <dependency>\n        <groupId>org.apache.hadoop</groupId>\n        <artifactId>hadoop-client</artifactId>\n        <version>${hadoop.version}</version>\n        <exclusions>\n          <exclusion>\n            <groupId>org.slf4j</groupId>\n            <artifactId>*</artifactId>\n          </exclusion>\n          <exclusion>\n            <groupId>ch.qos.reload4j</groupId>\n            <artifactId>*</artifactId>\n          </exclusion>\n        </exclusions>\n      </dependency>\n      <dependency>\n        <groupId>org.apache.hadoop</groupId>\n        <artifactId>hadoop-common</artifactId>\n        <version>${hadoop.version}</version>\n        <exclusions>\n          <exclusion>\n            <groupId>org.slf4j</groupId>\n            <artifactId>*</artifactId>\n          </exclusion>\n          <exclusion>\n            <groupId>ch.qos.reload4j</groupId>\n            <artifactId>*</artifactId>\n          </exclusion>\n        </exclusions>\n      </dependency>\n    </dependencies>\n  </dependencyManagement>\n\n  <reporting>\n    <plugins>\n      <plugin>\n        <groupId>org.apache.maven.plugins</groupId>\n        <artifactId>maven-javadoc-plugin</artifactId>\n        <reportSets>\n          <reportSet><!-- by default, id = \"default\" -->\n            <reports><!-- select non-aggregate reports -->\n              <report>javadoc</report>\n              <report>test-javadoc</report>\n            </reports>\n          </reportSet>\n          <reportSet><!-- aggregate reportSet, to define in poms having modules -->\n            <id>aggregate</id>\n            <inherited>false</inherited><!-- don't run aggregate in child modules -->\n            <reports>\n              <report>aggregate</report>\n            </reports>\n          </reportSet>\n        </reportSets>\n        <configuration>\n          <sourceFileExcludes>\n            <sourceFileExclude>**/generated-sources/**/*.java</sourceFileExclude>\n          </sourceFileExcludes>\n          <source>8</source>\n          <quiet>true</quiet>\n        </configuration>\n      </plugin>\n      <plugin>\n        <groupId>org.codehaus.mojo</groupId>\n        <artifactId>cobertura-maven-plugin</artifactId>\n        <version>2.7</version>\n        <configuration>\n          <formats>\n            <format>html</format>\n          </formats>\n          <aggregate>true</aggregate>\n          <instrumentation>\n            <ignores>\n              <ignore>java.lang.UnsupportedOperationException.*</ignore>\n            </ignores>\n            <excludes>\n              <exclude>parquet/Log.class</exclude>\n              <exclude>**/*Exception.class</exclude>\n              <exclude>parquet/example/**/*.class</exclude>\n            </excludes>\n          </instrumentation>\n        </configuration>\n      </plugin>\n    </plugins>\n  </reporting>\n\n  <build>\n    <pluginManagement>\n      <plugins>\n        <plugin>\n          <!-- Disable the source artifact from ASF parent -->\n          <artifactId>maven-assembly-plugin</artifactId>\n          <executions>\n            <execution>\n              <id>source-release-assembly</id>\n              <phase>none</phase>\n            </execution>\n          </executions>\n        </plugin>\n\n        <plugin>\n          <artifactId>maven-enforcer-plugin</artifactId>\n          <executions>\n            <execution>\n              <id>enforce-banned-dependencies</id>\n              <goals>\n                <goal>enforce</goal>\n              </goals>\n              <configuration>\n                <rules>\n                  <bannedDependencies>\n                    <excludes>\n                      <exclude>org.slf4j:slf4j-log4j12:*:*:compile</exclude>\n                      <exclude>org.slf4j:slf4j-reload4j:*:*:compile</exclude>\n                    </excludes>\n                  </bannedDependencies>\n                </rules>\n                <fail>true</fail>\n              </configuration>\n            </execution>\n          </executions>\n        </plugin>\n        <plugin>\n          <groupId>org.apache.maven.plugins</groupId>\n          <artifactId>maven-shade-plugin</artifactId>\n          <executions>\n            <execution>\n              <phase>package</phase>\n              <goals>\n                <goal>shade</goal>\n              </goals>\n              <configuration>\n                <minimizeJar>true</minimizeJar>\n                <artifactSet>\n                  <includes>\n                    <include>${jackson.groupId}:*</include>\n                    <include>it.unimi.dsi:fastutil</include>\n                    <include>net.openhft:*</include>\n                  </includes>\n                </artifactSet>\n                <!-- Shade jackson but do not include any class. Let parquet-jackson handle this -->\n                <filters>\n                  <filter>\n                    <artifact>${jackson.groupId}:*</artifact>\n                    <excludes>\n                      <exclude>**</exclude>\n                    </excludes>\n                  </filter>\n                </filters>\n                <relocations>\n                  <relocation>\n                    <pattern>${jackson.package}</pattern>\n                    <shadedPattern>${shade.prefix}.${jackson.package}</shadedPattern>\n                  </relocation>\n                  <relocation>\n                    <pattern>it.unimi.dsi</pattern>\n                    <shadedPattern>${shade.prefix}.it.unimi.dsi</shadedPattern>\n                  </relocation>\n                  <relocation>\n                    <pattern>net.openhft</pattern>\n                    <shadedPattern>${shade.prefix}.net.openhft</shadedPattern>\n                  </relocation>\n                </relocations>\n              </configuration>\n            </execution>\n          </executions>\n        </plugin>\n        <plugin>\n          <groupId>org.apache.maven.plugins</groupId>\n          <artifactId>maven-jar-plugin</artifactId>\n          <configuration>\n            <archive>\n              <manifestEntries>\n                <git-SHA-1>${buildNumber}</git-SHA-1>\n              </manifestEntries>\n            </archive>\n          </configuration>\n          <executions>\n            <execution>\n              <goals>\n                <goal>test-jar</goal>\n              </goals>\n            </execution>\n          </executions>\n        </plugin>\n      </plugins>\n    </pluginManagement>\n    <plugins>\n      <plugin>\n        <groupId>org.apache.maven.plugins</groupId>\n        <artifactId>maven-javadoc-plugin</artifactId>\n        <configuration>\n          <sourceFileExcludes>\n            <sourceFileExclude>**/generated-sources/**/*.java</sourceFileExclude>\n          </sourceFileExcludes>\n          <source>8</source>\n          <quiet>true</quiet>\n        </configuration>\n      </plugin>\n      <plugin>\n        <groupId>com.mycila.maven-license-plugin</groupId>\n        <artifactId>maven-license-plugin</artifactId>\n        <version>1.10.b1</version>\n        <configuration>\n          <header>src/license.txt</header>\n          <strictCheck>true</strictCheck>\n        </configuration>\n        <!--executions>\n          <execution>\n            <phase>test</phase>\n            <goals>\n              <goal>check</goal>\n            </goals>\n          </execution>\n        </executions-->\n      </plugin>\n\n      <plugin>\n        <groupId>org.apache.maven.plugins</groupId>\n        <artifactId>maven-remote-resources-plugin</artifactId>\n        <configuration>\n          <skip>true</skip>\n        </configuration>\n      </plugin>\n\n      <plugin>\n        <groupId>org.apache.maven.plugins</groupId>\n        <artifactId>maven-failsafe-plugin</artifactId>\n        <configuration>\n          <argLine>${extraJavaTestArgs}</argLine>\n        </configuration>\n        <executions>\n          <execution>\n            <goals>\n              <goal>integration-test</goal>\n              <goal>verify</goal>\n            </goals>\n          </execution>\n        </executions>\n      </plugin>\n\n      <plugin>\n        <groupId>org.apache.maven.plugins</groupId>\n        <artifactId>maven-surefire-plugin</artifactId>\n        <configuration>\n          <argLine>${surefire.argLine} ${extraJavaTestArgs}</argLine>\n          <systemPropertyVariables>\n            <!-- Configure Parquet logging during tests\n                 See http://www.slf4j.org/api/org/slf4j/impl/SimpleLogger.html\n                 -->\n            <org.slf4j.simpleLogger.defaultLogLevel>${surefire.logLevel}</org.slf4j.simpleLogger.defaultLogLevel>\n            <org.slf4j.simpleLogger.showDateTime>true</org.slf4j.simpleLogger.showDateTime>\n            <org.slf4j.simpleLogger.dateTimeFormat>YYYY-MM-dd HH:mm:ss</org.slf4j.simpleLogger.dateTimeFormat>\n            <org.slf4j.simpleLogger.showThreadName>false</org.slf4j.simpleLogger.showThreadName>\n            <org.slf4j.simpleLogger.showShortLogName>true</org.slf4j.simpleLogger.showShortLogName>\n\n            <!-- Configure log level for Hadoop -->\n            <hadoop.logLevel>${surefire.logLevel}</hadoop.logLevel>\n            <enableResourceIntensiveTests>${enableResourceIntensiveTests}</enableResourceIntensiveTests>\n          </systemPropertyVariables>\n          <excludes>\n            <exclude>**/benchmark/*.java</exclude>\n          </excludes>\n        </configuration>\n      </plugin>\n\n      <plugin>\n        <groupId>org.codehaus.mojo</groupId>\n        <artifactId>buildnumber-maven-plugin</artifactId>\n        <!-- Warning! 3.2.1 caused issues during verification, 3.2.0 is known to work correctly. -->\n        <version>3.2.0</version>\n        <executions>\n          <execution>\n            <phase>validate</phase>\n            <goals>\n              <goal>create</goal>\n            </goals>\n          </execution>\n        </executions>\n      </plugin>\n\n      <plugin>\n        <groupId>org.apache.rat</groupId>\n        <artifactId>apache-rat-plugin</artifactId>\n        <executions>\n          <execution>\n            <phase>test</phase>\n            <goals>\n              <goal>check</goal>\n            </goals>\n          </execution>\n        </executions>\n        <configuration>\n          <consoleOutput>true</consoleOutput>\n          <excludes>\n            <exclude>.github/PULL_REQUEST_TEMPLATE.md</exclude>\n            <exclude>**/*.parquet</exclude>\n            <exclude>**/*.avro</exclude>\n            <exclude>**/*.json</exclude>\n            <exclude>**/*.avsc</exclude>\n            <exclude>**/*.iml</exclude>\n            <exclude>**/*.log</exclude>\n            <exclude>**/*.md.vm</exclude>\n            <exclude>**/.classpath</exclude>\n            <exclude>**/.project</exclude>\n            <exclude>**/.settings/**</exclude>\n            <exclude>**/build/**</exclude>\n            <exclude>**/target/**</exclude>\n            <exclude>.git/**</exclude>\n            <exclude>.gitignore</exclude>\n            <exclude>.gitmodules</exclude>\n            <exclude>.idea/**</exclude>\n            <exclude>*/jdiff/*.xml</exclude>\n            <exclude>licenses/**</exclude>\n            <exclude>protobuf_install/**</exclude>\n            <exclude>thrift-${thrift.version}/**</exclude>\n            <exclude>thrift-${thrift.version}.tar.gz</exclude>\n            <exclude>**/dependency-reduced-pom.xml</exclude>\n            <exclude>**/*.rej</exclude>\n          </excludes>\n        </configuration>\n      </plugin>\n\n      <!-- https://mvnrepository.com/artifact/com.diffplug.spotless/spotless-maven-plugin -->\n      <plugin>\n        <groupId>com.diffplug.spotless</groupId>\n        <artifactId>spotless-maven-plugin</artifactId>\n        <version>${spotless.version}</version>\n        <configuration>\n          <java>\n            <excludes>\n              <exclude>**/generated-sources/**</exclude>\n            </excludes>\n            <toggleOffOn/>\n            <removeUnusedImports/>\n            <importOrder/>\n            <trimTrailingWhitespace/>\n            <palantirJavaFormat/>\n            <endWithNewline/>\n            <formatAnnotations/>\n            <indent>\n              <tabs>true</tabs>\n              <spacesPerTab>4</spacesPerTab>\n            </indent>\n            <indent>\n              <spaces>true</spaces>\n              <spacesPerTab>2</spacesPerTab>\n            </indent>\n          </java>\n        </configuration>\n        <executions>\n          <execution>\n            <phase>verify</phase>\n            <goals>\n              <goal>check</goal>\n            </goals>\n          </execution>\n        </executions>\n      </plugin>\n\n      <plugin>\n        <groupId>com.github.siom79.japicmp</groupId>\n        <artifactId>japicmp-maven-plugin</artifactId>\n        <version>${japicmp.version}</version>\n        <configuration>\n          <parameter>\n            <oldVersionPattern>${previous.version}</oldVersionPattern>\n            <breakBuildOnSourceIncompatibleModifications>true</breakBuildOnSourceIncompatibleModifications>\n            <onlyModified>true</onlyModified>\n            <ignoreMissingClassesByRegularExpressions>\n              <ignoreMissingClassesByRegularExpression>${shade.prefix}.*</ignoreMissingClassesByRegularExpression>\n            </ignoreMissingClassesByRegularExpressions>\n            <ignoreMissingOldVersion>true</ignoreMissingOldVersion>\n            <excludeModules>\n              <!-- Excluding the following modules because they are not part of the parquet public API -->\n              <excludeModule>parquet-benchmarks</excludeModule>\n              <excludeModule>parquet-cli</excludeModule>\n              <excludeModule>parquet-tools-deprecated</excludeModule>\n              <excludeModule>parquet-format-structures</excludeModule>\n\n              <!-- Excluding the following modules because bundles do not contain any java classes while they still fail the\n                compatibility check because of missing dependencies -->\n              <excludeModule>parquet-hadoop-bundle</excludeModule>\n              <excludeModule>parquet-pig-bundle</excludeModule>\n            </excludeModules>\n            <excludes>\n              <exclude>${shade.prefix}</exclude>\n              <!-- Removal of a protected method in a class that's not supposed to be subclassed by third-party code -->\n              <exclude>org.apache.parquet.column.values.bytestreamsplit.ByteStreamSplitValuesReader#gatherElementDataFromStreams(byte[])</exclude>\n              <!-- Due to the removal of deprecated methods -->\n              <exclude>org.apache.parquet.arrow.schema.SchemaMapping$TypeMappingVisitor#visit(org.apache.parquet.arrow.schema.SchemaMapping$MapTypeMapping)</exclude>\n              <!-- Make static variables final -->\n              <exclude>org.apache.parquet.avro.AvroReadSupport#AVRO_REQUESTED_PROJECTION</exclude>\n              <exclude>org.apache.parquet.avro.AvroReadSupport#AVRO_DATA_SUPPLIER</exclude>\n              <exclude>org.apache.parquet.hadoop.ParquetFileReader#PARQUET_READ_PARALLELISM</exclude>\n            </excludes>\n          </parameter>\n        </configuration>\n        <executions>\n          <execution>\n            <phase>verify</phase>\n            <goals>\n              <goal>cmp</goal>\n            </goals>\n          </execution>\n        </executions>\n      </plugin>\n      <plugin>\n        <groupId>org.apache.maven.plugins</groupId>\n        <artifactId>maven-dependency-plugin</artifactId>\n        <executions>\n          <execution>\n            <goals>\n              <goal>analyze-only</goal>\n            </goals>\n            <configuration>\n              <failOnWarning>true</failOnWarning>\n              <ignoreNonCompile>true</ignoreNonCompile>\n              <ignoredDependencies>\n                <dependency>javax.annotation:javax.annotation-api:jar:1.3.2</dependency>\n              </ignoredDependencies>\n            </configuration>\n          </execution>\n        </executions>\n      </plugin>\n      <plugin>\n        <groupId>org.cyclonedx</groupId>\n        <artifactId>cyclonedx-maven-plugin</artifactId>\n        <version>2.8.0</version>\n        <executions>\n          <execution>\n            <phase>package</phase>\n            <goals>\n              <goal>makeBom</goal>\n            </goals>\n          </execution>\n        </executions>\n      </plugin>\n    </plugins>\n  </build>\n\n  <profiles>\n    <profile>\n      <id>jdk9+</id>\n      <activation>\n        <jdk>[9,)</jdk>\n      </activation>\n      <properties>\n        <!-- release takes precedence over source/target if java version is 9 or higher -->\n        <maven.compiler.release>8</maven.compiler.release>\n      </properties>\n    </profile>\n\n    <!-- Profile for tests to have more output -->\n    <profile>\n      <id>verbose-test</id>\n      <properties>\n        <surefire.logLevel>INFO</surefire.logLevel>\n        <surefire.argLine>-XX:MaxJavaStackTraceDepth=1024</surefire.argLine>\n      </properties>\n    </profile>\n\n    <profile>\n      <id>vector-plugins</id>\n      <modules>\n        <module>parquet-plugins/parquet-encoding-vector</module>\n        <module>parquet-plugins/parquet-plugins-benchmarks</module>\n      </modules>\n    </profile>\n  </profiles>\n</project>\n"
        },
        {
          "name": "src",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}