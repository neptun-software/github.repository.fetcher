{
  "metadata": {
    "timestamp": 1736609114901,
    "page": 388,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjM5MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "OpenHFT/Chronicle-Queue",
      "stars": 3371,
      "defaultBranch": "ea",
      "files": [
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.7177734375,
          "content": "### How to update\n# This is copied from OpenHFT/.gitignore\n# update the original and run OpenHFT/update_gitignore.sh\n\n### Compiled class file\n*.class\n\n### Package Files\n*.jar\n*.war\n*.ear\n\n### Log file\n*.log\n\n### IntelliJ\n*.iml\n*.ipr\n*.iws\n.idea\ncompat_reports\n.attach_pid*\n\n### Virtual machine crash logs, see http://www.java.com/en/download/help/error_hotspot.xml\nhs_err_pid*\n\n### Maven template\ntarget/\npom.xml.tag\npom.xml.releaseBackup\npom.xml.versionsBackup\npom.xml.next\nrelease.properties\n\n### Eclipse template\n*.pydevproject\n.metadata\n.gradle\ntmp/\n*.tmp\n*.bak\n*.swp\n*~.nib\nlocal.properties\n.classpath\n.project\n.settings/\n.loadpath\n\n### Queue files\n*.cq4t\n#*.cq4\n\nbenchmarks/dependency-reduced-pom.xml\n\ndependency-reduced-pom.xml\n"
        },
        {
          "name": "DISCLAIMER.adoc",
          "type": "blob",
          "size": 2.1162109375,
          "content": "== Data\n\nThis product uses Google Analytics, a web analytics service provided by Google, Inc.(\"Google\").\n\nThis product sends usage data and platform information via a secure connection to Google Analytics.\n\nThis product stores a small file with a random client id (UUID) in the user's home directory. Because it is random, it does not contain any personal information.\n\n== Conveyed Properties\nProperties are conveyed over a secure (https) connection and includes the following:\n\n* Event name (e.g. \"started\")\n* Version of the library (e.g. \"5.20.146\")\n* The three top-most external package names (e.g. \"org.apache.commons\")\n* A random client id (UUID) (e.g. \"c38175c7-e822-40ef-a2ff-73a2e888b47c\")\n* The following System Properties:\n  - java.runtime.name (e.g \"Java(TM) SE Runtime Environment\")\n  - java.runtime.version (e.g. \"1.8.0_191-b12\")\n  - os.name (e.g. \"Mac OS X\")\n  - os.arch (e.g. \"x86_64\")\n  - os.version (e.g. \"10.16\")\n  - timezone.default (e.g. \"Europe/London\")\n  - available.processors (e.g. \"16\")\n  - max.memory.gib (e.g \"14\")\n  - java.major.version (e.g. \"8\")\n  - max.direct.memory.gib (e.g. \"14\")\n\nThe following library-specific properties are also conveyed:\n\n* Roll cycle (e.g. `DAILY`)\n* Wire type (e.g. `BINARY`)\n\n== Why is Data Collected?\nKnowing how our products are used allows us to focus our resources more efficiently in our effort to improve our libraries and\nprovide better and more widely adopted solutions for the community.\n\n== System Resources\nData collection is using a separate thread for sending information. The thread itself will not consume CPU resources when idle.\n\n== Open Data Pledge\nChronicle pledges to make aggregated data reports (using collected data) available to the general public at no cost.\nContact info@chronicle.software to obtain a link to these reports.\n\n== Consent\nBy using this product, you consent to the collection, storage and processing of data (sent by this library) by Google and Chronicle in the manner and for the purposes set out above.\n\nIf you do not consent you can do either of the following:\n\nA) Refrain from using this product\n\nOR\n\nB) Contact info@chronicle.software to have this feature removed\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 11.0908203125,
          "content": "                                 Apache License\n                           Version 2.0, January 2004\n                        http://www.apache.org/licenses/\n\n   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n\n   1. Definitions.\n\n      \"License\" shall mean the terms and conditions for use, reproduction,\n      and distribution as defined by Sections 1 through 9 of this document.\n\n      \"Licensor\" shall mean the copyright owner or entity authorized by\n      the copyright owner that is granting the License.\n\n      \"Legal Entity\" shall mean the union of the acting entity and all\n      other entities that control, are controlled by, or are under common\n      control with that entity. For the purposes of this definition,\n      \"control\" means (i) the power, direct or indirect, to cause the\n      direction or management of such entity, whether by contract or\n      otherwise, or (ii) ownership of fifty percent (50%) or more of the\n      outstanding shares, or (iii) beneficial ownership of such entity.\n\n      \"You\" (or \"Your\") shall mean an individual or Legal Entity\n      exercising permissions granted by this License.\n\n      \"Source\" form shall mean the preferred form for making modifications,\n      including but not limited to software source code, documentation\n      source, and configuration files.\n\n      \"Object\" form shall mean any form resulting from mechanical\n      transformation or translation of a Source form, including but\n      not limited to compiled object code, generated documentation,\n      and conversions to other media types.\n\n      \"Work\" shall mean the work of authorship, whether in Source or\n      Object form, made available under the License, as indicated by a\n      copyright notice that is included in or attached to the work\n      (an example is provided in the Appendix below).\n\n      \"Derivative Works\" shall mean any work, whether in Source or Object\n      form, that is based on (or derived from) the Work and for which the\n      editorial revisions, annotations, elaborations, or other modifications\n      represent, as a whole, an original work of authorship. For the purposes\n      of this License, Derivative Works shall not include works that remain\n      separable from, or merely link (or bind by name) to the interfaces of,\n      the Work and Derivative Works thereof.\n\n      \"Contribution\" shall mean any work of authorship, including\n      the original version of the Work and any modifications or additions\n      to that Work or Derivative Works thereof, that is intentionally\n      submitted to Licensor for inclusion in the Work by the copyright owner\n      or by an individual or Legal Entity authorized to submit on behalf of\n      the copyright owner. For the purposes of this definition, \"submitted\"\n      means any form of electronic, verbal, or written communication sent\n      to the Licensor or its representatives, including but not limited to\n      communication on electronic mailing lists, source code control systems,\n      and issue tracking systems that are managed by, or on behalf of, the\n      Licensor for the purpose of discussing and improving the Work, but\n      excluding communication that is conspicuously marked or otherwise\n      designated in writing by the copyright owner as \"Not a Contribution.\"\n\n      \"Contributor\" shall mean Licensor and any individual or Legal Entity\n      on behalf of whom a Contribution has been received by Licensor and\n      subsequently incorporated within the Work.\n\n   2. Grant of Copyright License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      copyright license to reproduce, prepare Derivative Works of,\n      publicly display, publicly perform, sublicense, and distribute the\n      Work and such Derivative Works in Source or Object form.\n\n   3. Grant of Patent License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      (except as stated in this section) patent license to make, have made,\n      use, offer to sell, sell, import, and otherwise transfer the Work,\n      where such license applies only to those patent claims licensable\n      by such Contributor that are necessarily infringed by their\n      Contribution(s) alone or by combination of their Contribution(s)\n      with the Work to which such Contribution(s) was submitted. If You\n      institute patent litigation against any entity (including a\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\n      or a Contribution incorporated within the Work constitutes direct\n      or contributory patent infringement, then any patent licenses\n      granted to You under this License for that Work shall terminate\n      as of the date such litigation is filed.\n\n   4. Redistribution. You may reproduce and distribute copies of the\n      Work or Derivative Works thereof in any medium, with or without\n      modifications, and in Source or Object form, provided that You\n      meet the following conditions:\n\n      (a) You must give any other recipients of the Work or\n          Derivative Works a copy of this License; and\n\n      (b) You must cause any modified files to carry prominent notices\n          stating that You changed the files; and\n\n      (c) You must retain, in the Source form of any Derivative Works\n          that You distribute, all copyright, patent, trademark, and\n          attribution notices from the Source form of the Work,\n          excluding those notices that do not pertain to any part of\n          the Derivative Works; and\n\n      (d) If the Work includes a \"NOTICE\" text file as part of its\n          distribution, then any Derivative Works that You distribute must\n          include a readable copy of the attribution notices contained\n          within such NOTICE file, excluding those notices that do not\n          pertain to any part of the Derivative Works, in at least one\n          of the following places: within a NOTICE text file distributed\n          as part of the Derivative Works; within the Source form or\n          documentation, if provided along with the Derivative Works; or,\n          within a display generated by the Derivative Works, if and\n          wherever such third-party notices normally appear. The contents\n          of the NOTICE file are for informational purposes only and\n          do not modify the License. You may add Your own attribution\n          notices within Derivative Works that You distribute, alongside\n          or as an addendum to the NOTICE text from the Work, provided\n          that such additional attribution notices cannot be construed\n          as modifying the License.\n\n      You may add Your own copyright statement to Your modifications and\n      may provide additional or different license terms and conditions\n      for use, reproduction, or distribution of Your modifications, or\n      for any such Derivative Works as a whole, provided Your use,\n      reproduction, and distribution of the Work otherwise complies with\n      the conditions stated in this License.\n\n   5. Submission of Contributions. Unless You explicitly state otherwise,\n      any Contribution intentionally submitted for inclusion in the Work\n      by You to the Licensor shall be under the terms and conditions of\n      this License, without any additional terms or conditions.\n      Notwithstanding the above, nothing herein shall supersede or modify\n      the terms of any separate license agreement you may have executed\n      with Licensor regarding such Contributions.\n\n   6. Trademarks. This License does not grant permission to use the trade\n      names, trademarks, service marks, or product names of the Licensor,\n      except as required for reasonable and customary use in describing the\n      origin of the Work and reproducing the content of the NOTICE file.\n\n   7. Disclaimer of Warranty. Unless required by applicable law or\n      agreed to in writing, Licensor provides the Work (and each\n      Contributor provides its Contributions) on an \"AS IS\" BASIS,\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n      implied, including, without limitation, any warranties or conditions\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n      PARTICULAR PURPOSE. You are solely responsible for determining the\n      appropriateness of using or redistributing the Work and assume any\n      risks associated with Your exercise of permissions under this License.\n\n   8. Limitation of Liability. In no event and under no legal theory,\n      whether in tort (including negligence), contract, or otherwise,\n      unless required by applicable law (such as deliberate and grossly\n      negligent acts) or agreed to in writing, shall any Contributor be\n      liable to You for damages, including any direct, indirect, special,\n      incidental, or consequential damages of any character arising as a\n      result of this License or out of the use or inability to use the\n      Work (including but not limited to damages for loss of goodwill,\n      work stoppage, computer failure or malfunction, or any and all\n      other commercial damages or losses), even if such Contributor\n      has been advised of the possibility of such damages.\n\n   9. Accepting Warranty or Additional Liability. While redistributing\n      the Work or Derivative Works thereof, You may choose to offer,\n      and charge a fee for, acceptance of support, warranty, indemnity,\n      or other liability obligations and/or rights consistent with this\n      License. However, in accepting such obligations, You may act only\n      on Your own behalf and on Your sole responsibility, not on behalf\n      of any other Contributor, and only if You agree to indemnify,\n      defend, and hold each Contributor harmless for any liability\n      incurred by, or claims asserted against, such Contributor by reason\n      of your accepting any such warranty or additional liability.\n\n   END OF TERMS AND CONDITIONS\n\n   APPENDIX: How to apply the Apache License to your work.\n\n      To apply the Apache License to your work, attach the following\n      boilerplate notice, with the fields enclosed by brackets \"[]\"\n      replaced with your own identifying information. (Don't include\n      the brackets!)  The text should be enclosed in the appropriate\n      comment syntax for the file format. We also recommend that a\n      file or class name and description of purpose be included on the\n      same \"printed page\" as the copyright notice for easier\n      identification within third-party archives.\n\n   Copyright [yyyy] [name of copyright owner]\n\n   Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.\n"
        },
        {
          "name": "MigratingToV4.adoc",
          "type": "blob",
          "size": 3.4755859375,
          "content": "= Migrating Chronicle Logger to Queue V4\nPeter Lawrey\n\nThese are the steps which were taken to migrate Chornicle Logger from using Chronicle Queue v3 to v4.\n\n=== Update the artifactId\n\n.Artifact and package for Chronicle v3.\n[source, xml]\n----\n     <dependency>\n            <groupId>net.openhft</groupId>\n            <artifactId>chronicle</artifactId>\n      </dependency>\n----\n\nto the following artifactId.\n\n.Artifact and package for Chronicle v4.\n[source, xml]\n----\n     <dependency>\n            <groupId>net.openhft</groupId>\n            <artifactId>chronicle-queue</artifactId>\n      </dependency>\n----\n\nNOTE: As Chronicle Queue v3 and v4 use different packages you can use both versions at the same time.\n\n=== Rename the Chronicle class to ChronicleQueue\n\nAs Chronicle is the name of a family of products, the interface is now called ChronicleQueue to distinguish it from ChronicleMap for example.\n\n=== createAppender() has been replaced with acquireAppender()\n\nWe use the term `acquire` to indicate an object will be created only as needed.\nIn this case, the method will create a new `ExcerptAppender` for each thread once.\nA thread cannot have more than one appender in use at once as this would cause a live lock.\n\nThis method no longer throws an `IOException`\n\n=== An Appender cannot be used as Bytes directly.\n\nConflating the `ExcerptAppender` and `Bytes` cause a number of problems\nincluding the use of `flush()` and `close()` which had different meanings between these two classes.\n\nTo use Bytes of an appender you can use either of these two patterns\n\n.Java 7 style try-with-resource block\n[source, java]\n----\n// Writing Java 7 style without a lambda\ntry (DocumentContext dc = appender.writingDocument()) {\n    Bytes<?> bytes = dc.wire().bytes();\n    // write to the bytes\n}\n\n// Reading Java 7 style without a lambda\ntry (DocumentContext dc = appender.readingDocument()) {\n    Bytes<?> bytes = dc.wire().bytes();\n    // read from the bytes\n}\n----\n\n.Java 8 style use of lambdas\n[source, java]\n----\n// Writing Java 8 style with a lambda\nappender.writeBytes(b -> b\n                    .writeXxx(x)\n                    .writeXxx(y));\n\n// Reading Java 8 style with a lambda\ntailer.readBytes(b -> {\n     int x = b.readInt();\n     String y = b.readUtf8();\n});\n----\n\n=== Using the Wire API instead of the Bytes API\n\nThere is two ways to use the Wire API\n\n- as field names and values.\n- a stream of values.\n\nUsing the Wire API to write values means the data is self describing.\nThis allows for changes in type as well as automatic dumping of the data.\nAdding field names allows the data to be more descriptive as well and in any order or added/removed in future.\n\n.Example of using the ValueOut interface\n[source, Java]\n----\ntry (DocumentContext dc = appender.writingDocument()) {\n    ValueOut out = dc.wire().getValueOut();\n    out.int8(CODE);\n    out.int64(timestamp);\n    out.text(message);\n}\n----\n\n=== Changes to the Bytes API.\n\nChronicle Bytes distinguishes between the read and write position, remaining and limit.\n\nWhen reading you want to use\n\n- getter `readPosition()` and setter `readPosition(long)` - position to read next\n- getter `readLimit()` and setter `readLimit(long)` -  last valid readPosition(). It is also the writePosition()\n- readRemaining() is  readLimit() - readPosition();\n\nWhen writing you want to use\n\n- getter `writePosition()` and setter `writePosition(long)` - position to write next\n- getter `writeLimit()` and setter `writeLimit(long)` -  last valid writePosition().\n- writeRemaining() is  writeLimit() - writePosition();\n\n"
        },
        {
          "name": "README.adoc",
          "type": "blob",
          "size": 82.533203125,
          "content": "= Chronicle Queue\nPeter Lawrey, Rob Austin\n:css-signature: demo\n:toc: macro\n:toclevels: 2\n:icons: font\n\nimage:https://maven-badges.herokuapp.com/maven-central/net.openhft/chronicle-queue/badge.svg[caption=\"\",link=https://maven-badges.herokuapp.com/maven-central/net.openhft/chronicle-queue]\nimage:https://javadoc.io/badge2/net.openhft/chronicle-queue/javadoc.svg[link=\"https://www.javadoc.io/doc/net.openhft/chronicle-queue/latest/index.html\"]\n//image:https://javadoc-badge.appspot.com/net.openhft/chronicle-queue.svg?label=javadoc[JavaDoc, link=https://www.javadoc.io/doc/net.openhft/chronicle-queue]\nimage:https://img.shields.io/github/license/OpenHFT/Chronicle-Queue[GitHub]\nimage:https://img.shields.io/gitter/room/OpenHFT/Lobby.svg?style=popout[link=\"https://gitter.im/OpenHFT/Lobby\"]\nimage:https://img.shields.io/badge/release%20notes-subscribe-brightgreen[link=\"https://chronicle.software/release-notes/\"]\nimage:https://sonarcloud.io/api/project_badges/measure?project=OpenHFT_Chronicle-Queue&metric=alert_status[link=\"https://sonarcloud.io/dashboard?id=OpenHFT_Chronicle-Queue\"]\n\nimage::docs/images/Queue_line.png[width=20%]\n\ntoc::[]\n\n== About Chronicle Software\n\nhttps://player.vimeo.com/video/201989439\n\n== Overview\n\nChronicle Queue is a persisted low-latency messaging framework for high performance applications.\n\nThis project covers the Java version of Chronicle Queue.\nA {cpp} version of this project is also available and supports Java/{cpp} interoperability plus additional language bindings e.g. Python.\nIf you are interested in evaluating the {cpp} version please contact sales@chronicle.software.\n\nAt first glance Chronicle Queue can be seen as simply **another queue implementation**.\nHowever, it has major design choices that should be emphasised.\nUsing *off-heap storage*, Chronicle Queue provides an environment where applications do not suffer from Garbage Collection (GC).\nWhen implementing high-performance and memory-intensive applications (you heard the fancy term \"bigdata\"?) in Java, one of the biggest problems is garbage collection.\n\nChronicle Queue allows messages to be added to the end of a queue (\"appended\"), read from the queue (\"tailed\"),\nand also supports random-access seek.\n\n== What Is Chronicle Queue?\n\nYou could consider a Chronicle Queue to be similar to a low latency broker-less durable/persisted topic that can contain messages of different types and sizes.\nChronicle Queue is a distributed unbounded persisted queue that:\n\n* supports asynchronous RMI and Publish/Subscribe interfaces with microsecond latencies.\n* passes messages between JVMs in under a microsecond\n* passes messages between JVMs on different machines via replication in under 10 microseconds\n(<<Chronicle Queue Enterprise Edition,Enterprise feature>>)\n* provides stable, soft real-time latencies into the millions of messages per second for a single thread to one queue; with total ordering of every event.\n\nWhen publishing 40-byte messages, a high percentage of the time we achieve latencies under 1 microsecond.\nThe 99th percentile latency is the worst 1 in 100, and the 99.9th percentile is the worst 1 in 1000 latency.\n\n.Latency to send/receive on the same machine.\n[width=\"60%\",options=\"header\"]\n|=======\n| Batch Size | 10 million events per minute | 60 million events per minute | 100 million events per minute\n| 99%ile | 0.78 &micro;s | 0.78 &micro;s | 1.2 &micro;s\n| 99.9%ile | 1.2 &micro;s | 1.3 &micro;s | 1.5 &micro;s\n|=======\n\n.Latency to send/receive on a second machine.\n[width=\"60%\",options=\"header\"]\n|=======\n| Batch Size | 10 million events per minute | 60 million events per minute | 100 million events per minute\n| 99%ile | 20 &micro;s | 28 &micro;s | 176 &micro;s\n| 99.9%ile | 901 &micro;s | 705 &micro;s | 5,370 &micro;s\n|=======\n\nNOTE: 100 million events per minute is sending an event every 660 nanoseconds; replicated and persisted.\n\nIMPORTANT: This performance is not achieved using a *large cluster of machines*.\nThis is using one thread to publish, and one thread to consume.\n\n=== Design Motivation and Features\n\nChronicle Queue is designed to:\n\n* be a \"record everything store\" which can read with microsecond real-time latency.\nThis supports even the most demanding High Frequency Trading systems.\nHowever, it can be used in any application where the recording of information is a concern.\n\n* support reliable replication with notification to either the appender (writer of message) or a tailer (reader of message), when a message has been successfully replicated.\n\n==== Persistence\n\nChronicle Queue assumes disk space is cheap compared with memory.\nChronicle Queue makes full use of the disk space you have, and so you are not limited by the main memory of your machine.\nIf you use spinning HDD, you can store many TBs of disk space for little cost.\n\nThe only extra software that Chronicle Queue needs to run is the operating system.\nIt doesn't have a broker; instead it uses your operating system to do all the work.\nIf your application dies, the operating system keeps running for seconds longer, so no data is lost; even without replication.\n\nAs Chronicle Queue stores all saved data in memory-mapped files, this has a trivial on-heap overhead, even if you have over 100 TB of data.\n\n==== Efficiency\n\nChronicle put significant effort into achieving very low latency.\nIn other products which focus on support of web applications, latencies of less than 40 milliseconds are fine as they are faster than you can see; for example, the frame rate of cinema is 24 Hz, or about 40 ms.\n\nChronicle Queue aims to achieve latencies of under 40 microseconds for 99% to 99.99% of the time.\nUsing Chronicle Queue without replication, we support applications with latencies below 40 microseconds end-to-end across multiple services.\nOften the 99% latency of Chronicle Queue is entirely dependent on the choice of operating system and hard disk sub-system.\n\n==== Compression\n\nReplication for Chronicle Queue supports Chronicle Wire Enterprise.\nThis supports a real-time compression which calculates the deltas for individual objects, as they are written.\nThis can reduce the size of messages by a factor of 10, or better, without the need for batching; that is, without introducing significant latency.\n\nChronicle Queue also supports LZW, Snappy, and GZIP compression.\nThese formats however add significant latency.\nThese are only useful if you have strict limitations on network bandwidth.\n\n==== Delivery mode semantics\n\nChronicle Queue supports a number of semantics:\n\n- Every message is replayed on restart.\n- Only new messages are played on restart.\n- Restart from any known point using the index of the entry.\n- Replay only the messages you have missed.\nThis is supported directly using the methodReader/methodWriter builders.\n\n==== Using high resolution timings across machines\n\nOn most systems `System.nanoTime()` is roughly the number of nanoseconds since the system last rebooted (although different JVMs may behave differently).\nThis is the same across JVMs on the same machine, but wildly different between machines.\nThe absolute difference when it comes to machines is meaningless.\nHowever, the information can be used to detect outliers; you can't determine what the best latency is, but you can determine how far off the best latencies you are.\nThis is useful if you are focusing on the 99th percentile latencies.\nWe have a class called `RunningMinimum` to obtain timings from different machines, while compensating for a drift in the `nanoTime` between machines.\nThe more often you take measurements, the more accurate this running minimum is.\n\n==== Compacting logs\n\nChronicle Queue manages storage by cycle.\nYou can add a `StoreFileListener` which will notify you when a file is added, and when it is no longer retained.\nYou can move, compress, or delete all the messages for a day, at once.\nNOTE : Unfortunately on Windows, if an IO operation is interrupted, it can close the underlying FileChannel.\n\n==== Avoid Interrupts\n\nDue to performance reasons, we have removed checking for interrupts in the chronicle queue code.\nBecause of this, we recommend that you avoid using chronicle queue with code that generates interrupts.\nIf you can not avoid generating interrupts then we suggest that you create a separate instance of Chronicle Queue per thread.\n\n=== Usage\n\nChronicle Queue is most often used for producer-centric systems where you need to retain a lot of data for days or years. For statistics see https://docs.google.com/spreadsheets/u/1/d/e/2PACX-1vTe-ijX-uRMc86pB1r-qPUIDZmzI0drPQtvUiGiU8p6WEq98HHDO47HXfV_dk_q6Tmhr1fq2pLxLkqv/pubhtml[Usage of Chronicle-Queue]\n\nIMPORTANT: Chronicle Queue does *not* support operating off any network file system, be it NFS, AFS, SAN-based storage or anything else.\nThe reason for this is those file systems do not provide all the required primitives for memory-mapped files Chronicle Queue uses.\nIf any networking is needed (e.g. to make the data accessible to multiple hosts), the only supported way is Chronicle Queue Replication (Enterprise feature).\n\n==== What is a producer-centric system?\n\nMost messaging systems are consumer-centric.\nFlow control is implemented to avoid the consumer ever getting overloaded; even momentarily.\nA common example is a server supporting multiple GUI users.\nThose users might be on different machines (OS and hardware), different qualities of network (latency and bandwidth), doing a variety of other things at different times.\nFor this reason it makes sense for the client consumer to tell the producer when to back off, delaying any data until the consumer is ready to take more data.\n\nChronicle Queue is a producer-centric solution and does everything possible to never push back on the producer, or tell it to slow down.\nThis makes it a powerful tool, providing a big buffer between your system, and an upstream producer over which you have little, or no, control.\n\n==== Market data\n\nMarket data publishers don't give you the option to push back on the producer for long; if at all.\nA few of our users consume data from CME OPRA. This produces peaks of 10 million events per minute, sent as UDP packets without any retry.\nIf you miss, or drop a packet, then it is lost.\nYou have to consume and record those packets as fast as they come to you, with very little buffering in the network adapter.\nFor market data in particular, real time means in a *few microseconds*; it doesn't mean intra-day (during the day).\n\nChronicle Queue is fast and efficient, and has been used to increase the speed that data is passed between threads.\nIn addition, it also keeps a record of every message passed allowing you to significantly reduce the amount of logging that you need to do.\n\n==== Compliance systems\n\nCompliance systems are required by more and more systems these days.\nEveryone has to have them, but no one wants to be slowed down by them.\nBy using Chronicle Queue to buffer data between monitored systems and the compliance system, you don't need to worry about the impact of compliance recording for your monitored systems.\nAgain, Chronicle Queue can support millions of events per-second, per-server, and access data which has been retained for years.\n\n==== Latency sensitive micro-services\n\nChronicle Queue supports low latency IPC (Inter Process Communication) between JVMs on the same machine in the order of magnitude of 1 microsecond; as well as between machines with a typical latency of 10 microseconds for modest throughputs of a few hundred thousands.\nChronicle Queue supports throughputs of millions of events per second, with stable microsecond latencies.\n\nSee https://vanilla-java.github.io/tag/Microservices/[Articles on the use of Chronicle Queue in Microservices]\n\n==== Log replacement\n\nA Chronicle Queue can be used to build state machines.\nAll the information about the state of those components can be reproduced externally, without direct access to the components, or to their state.\nThis significantly reduces the need for additional logging.\nHowever, any logging you do need can be recorded in great detail.\nThis makes enabling `DEBUG` logging in production practical.\nThis is because the cost of logging is very low; less than 10 microseconds.\nLogs can be replicated centrally for log consolidation.\nChronicle Queue is being used to store 100+ TB of data, which can be replayed from any point in time.\n\n==== Lambda Stream Processing\n\nNon-batching streaming components are highly performant, deterministic, and reproducible.\nYou can reproduce bugs which only show up after a million events played in a particular order, with accelerated realistic timings.\nThis makes using Stream processing attractive for systems which need a high degree of quality outcomes.\n\n==== Setting the Roll Cycle of a queue\n\nYou can have different instances of `SingleChronicleQueue` set with different roll cycles.\nHowever, if you have instances of `SingleChronicleQueue` with the same path but different roll cycles, first queue to be created determines the roll cycle, even if the first queue only have a tailer. If the first queue is read-only, it will fail to read the queue if the roll cycle is later different.\n\n== Downloading Chronicle Queue\n\nReleases are available on Maven Central as:\n\n[source,xml]\n----\n<dependency>\n  <groupId>net.openhft</groupId>\n  <artifactId>chronicle-queue</artifactId>\n  <version><!--replace with the latest version, see below--></version>\n</dependency>\n----\n\nSee https://github.com/OpenHFT/Chronicle-Queue/releases[Chronicle Queue Release Notes] and get the http://search.maven.org/#search%7Cga%7C1%7Cg%3A%22net.openhft%22%20AND%20a%3A%22chronicle-queue%22[Latest Version Number].\nSnapshots are available on https://oss.sonatype.org\n\nNOTE: Classes that reside in either of the packages 'internal', 'impl', and 'main' (the latter containing various runnable main methods) and\nany sub-packages are not a part of the public API and *may become subject to change at any time for any reason*. See the respective `package-info.java` files for details.\n\n== Chronicle Queue Versions and Remarkable Changes\n\n=== Changes from Version 4 to Version 5\n\nIn Chronicle Queue v5 tailers are now read-only, in Chronicle Queue v4 we had the concept of lazy indexing, where appenders would not write indexes but instead the indexing could be done by the tailer. We decided to drop lazy indexing in v5; making tailers read-only not only simplifies Chronicle Queue but also allows us to add optimisations elsewhere in the code.\n\nThe locking model of Chronicle Queue was changed in v5, in Chronicle Queue v4 the write lock (to prevent concurrent writes to the queue) exists in the .cq4 file.\nIn v5 this was moved to a single file called a table store (metadata.cq4t).\nThis simplifies the locking code internally as only the table store file has to be inspected.\n\nYou can use Chronicle Queue v5 to read messages written with Chronicle Queue v4, but this is not guaranteed to always work - if, for example,\nyou created your v4 queue with `wireType(WireType.FIELDLESS_BINARY)` then Chronicle Queue v5 will not be able to read the queue's header.\nWe have some tests for v5 reading v4 queues but these are limited and all scenarios may not be supported.\n\nYou cannot use Chronicle Queue v5 to write to Chronicle Queue v4 queues.\n\n=== Changes from Version 3 to Version 4\n\nChronicle Queue v4 is a complete re-write of Chronicle Queue that solves the following issues that existed in v3.\n\n- Without self-describing messages, users had to create their own functionality for dumping messages and long term storage of data.\nWith v4 you don't have to do this, but you can if you wish to.\n- Vanilla Chronicle Queue would create a file per thread. This is fine if the number of threads is controlled, however, many applications have little or no control over how many threads are used and this caused usability problems.\n- The configuration for Indexed and Vanilla Chronicle was entirely in code so the reader had to have the same configuration as the writers and it wasn't always clear what that was.\n- There was no way for the producer to know how much data had been replicated to the a second machine. The only workaround was to replicate data back to the producers.\n- You needed to specify the size of data to reserve before you started to write your message.\n- You needed to do your own locking for the appender when using Indexed Chronicle.\n\n=== Migrating from Chronicle Queue v2 and v3\n\nIn Chronicle Queue v3, everything was in terms of bytes, not wire.\nThere are two ways to use byte in Chronicle Queue v4. You can use the `writeBytes` and `readBytes` methods, or you can get the `bytes()` from the wire.\nFor example:\n\n.Writing and reading bytes using a lambda\n[source,Java]\n----\nappender.writeBytes(b -> b.writeInt(1234).writeDouble(1.111));\n\nboolean present = tailer.readBytes(b -> process(b.readInt(), b.readDouble()));\n----\n\n.Writing to a queue without using a lambda\n[source,Java]\n----\ntry (DocumentContext dc = appender.writingDocument()) {\n    Bytes<?> bytes = dc.wire().bytes();\n    // write to bytes\n}\n\ntry (DocumentContext dc = tailer.readingDocument()) {\n    if (dc.isPresent()) {\n        Bytes<?> bytes = dc.wire().bytes();\n        // read from bytes\n    }\n}\n----\n\n=== Chronicle Queue Enterprise Edition\n\nlink:https://chronicle.software/products/queue-enterprise/[Chronicle Queue Enterprise Edition] is a commercially supported version of our successful open source Chronicle Queue.\nThe open source documentation is extended by the following documents to describe the additional features that are available when you are licenced for Enterprise Edition.\nThese are:\n\n- Encryption of message queues and messages.\nFor more information see link:https://portal.chronicle.software/docs/queue/chronicle-queue/encryption/encryption.html[Encryption documentation].\n- TCP/IP (and optionally UDP) Replication between hosts to ensure real-time backup of all your queue data.\nFor more information see the link:https://portal.chronicle.software/docs/queue/chronicle-queue/replication/replication.html[replication] documentation, the queue replication protocol is covered in link:https://portal.chronicle.software/docs/queue/chronicle-queue/replication/replication-protocol.html[Replication Protocol].\n- Timezone support for daily queue rollover scheduling.\nFor more information see link:https://portal.chronicle.software/docs/queue/chronicle-queue/configuration/timezone_rollover.html[Timezone support].\n- Async mode support to give improved performance at high throughput on slower filesystems.\nFor more information see link:https://portal.chronicle.software/docs/queue/chronicle-queue/async-mode/async_mode.html[async mode] and also link:https://portal.chronicle.software/docs/queue/chronicle-queue/performance/performance-test-results.html[performance].\n- Pre-toucher\nFor improved outliers, see link:https://portal.chronicle.software/docs/queue/chronicle-queue/performance/pretouching.html[Pre-toucher and its configuration]\n\nIn addition, you will be fully supported by our technical experts.\n\nFor more information on Chronicle Queue Enterprise Edition, please contact mailto:sales@chronicle.software[sales@chronicle.software].\n\n== User Guide\n\nA Chronicle Queue is defined by `SingleChronicleQueue.class` that is designed to support:\n\n* rolling files on a daily, weekly or hourly basis,\n\n* concurrent writers on the same machine,\n\n* concurrent readers on the same machine or across multiple machines via TCP replication (With Chronicle Queue Enterprise),\n\n* concurrent readers and writers between link:./docs/FAQ.adoc#containerisation-recommendations[Docker or other containerised workloads]\n\n* zero copy serialization and deserialization,\n\n* millions of writes/reads per second on commodity hardware.\n\nApproximately 5 million messages/second for 96-byte messages on a i7-4790 processor.\nA queue directory structure is as follows:\n\n[source]\n----\nbase-directory /\n   {cycle-name}.cq4       - The default format is yyyyMMdd for daily rolling.\n----\n\nThe format consists of size-prefixed bytes which are formatted using `BinaryWire` or `TextWire`.\nChronicle Queue is designed to be driven from code. You can easily add an interface which suits your needs.\n\nNOTE: Due to fairly low-level operation, Chronicle Queue read/write operations can throw unchecked exceptions. In order to prevent thread death, it might be practical to catch `RuntimeExceptions` and log/analyze them as appropriate.\n\nNOTE: For demonstrations of how Chronicle Queue can be used see link:https://github.com/OpenHFT/Chronicle-Queue-Sample[Chronicle Queue Demo] and for Java documentation see link:https://www.javadoc.io/doc/net.openhft/chronicle-queue/latest/index.html[Chronicle Queue JavaDocs]\n\nIn the following sections, first we introduce some terminology and a quick reference to use Chronicle Queue. Then, we provide a more detailed guide.\n\n=== Key Concepts and Terminology\nChronicle Queue is a persisted journal of messages which supports concurrent writers and readers even across multiple JVMs on the same machine.\nEvery reader sees every message, and a reader can join at any time and still see every message.\n\nNOTE: We deliberately avoid the term *consumer* and instead use *reader* as messages are not consumed/destroyed by reading.\n\nChronicle queue has the following main concepts:\n\n- *Excerpt*\n\nExcerpt is the main data container in a Chronicle Queue. In other words, each Chronicle Queue is composed of excerpts.\nWriting message to a Chronicle Queue means starting a new excerpt, writing message into it, and finishing the excerpt at the end.\n\n- *Appender*\n\nAn appender is the source of messages; something like an iterator in Chronicle environment.\nYou add data appending the current Chronicle Queue. It can perform sequential writes by appending to the end of queue only. There is no way to insert, or delete excerpts.\n\n- *Tailer*\n\nA tailer is an excerpt reader optimized for sequential reads. It can perform sequential and random reads, both forwards and backwards.\nTailers read the next available message each time they are called. The followings are guaranteed in Chronicle Queue:\n\n* for each *appender*, messages are written in the order the appender wrote them.\nMessages by different appenders are interleaved,\n\n* for each *tailer*, it will see every message for a topic in the same order as every other tailer,\n\n* when replicated, every replica has a copy of every message.\n\nChronicle Queue is broker-less. If you need an architecture with a broker, please\ncontact sales@chronicle.software.\n\n- *File rolling and queue files*\n\nChronicle Queue is designed to roll its files depending on the roll cycle chosen when queue is created (see https://github.com/OpenHFT/Chronicle-Queue/blob/ea/src/main/java/net/openhft/chronicle/queue/RollCycles.java[RollCycles]).\nIn other words, a queue file is created for each roll cycle which has extension `cq4`. When the roll cycle reaches the point it should roll, appender will atomically write `EOF` mark at the end of current file to indicate that no other appender should write to this file and no tailer should read further, and instead everyone should use new file.\n\nIf the process was shut down, and restarted later when the roll cycle should be using a new file, an appender will try to locate old files and write an `EOF` mark in them to help tailers reading them.\n\n- *Topics*\n\nEach topic is a directory of queue files.\nIf you have a topic called `mytopic`, the layout could look like this:\n\n[source]\n----\nmytopic/\n    20160710.cq4\n    20160711.cq4\n    20160712.cq4\n    20160713.cq4\n----\n\nTo copy all the data for a single day (or cycle), you can copy the file for that day on to your development machine for replay testing.\n\n** Restrictions on topics and messages\n\nTopics are limited to being strings which can be used as directory names.\nWithin a topic, you can have sub-topics which can be any data type that can be serialized.\nMessages can be any serializable data.\n\nChronicle Queue supports:\n\n* `Serializable` objects, though this is to be avoided as it is not efficient\n\n* `Externalizable` objects is preferred if you wish to use standard Java APIs.\n\n* `byte[]` and `String`\n\n* `Marshallable`; a self describing message which can be written as YAML, Binary YAML, or JSON.\n\n* `BytesMarshallable` which is low-level binary, or text encoding.\n\n=== Quick start\nThis section provides a quick reference for using Chronicle Queue to briefly show how to create, write/read into/from a queue.\n\n- Chronicle Queue construction\n\nCreating an instance of Chronicle Queue is different from just calling a constructor.\nTo create an instance you have to use the `ChronicleQueueBuilder`.\n\n[source,Java]\n----\nString basePath = OS.getTarget() + \"/getting-started\"\nChronicleQueue queue = SingleChronicleQueueBuilder.single(basePath).build();\n----\n\nIn this example we have created an `IndexedChronicle` which creates two `RandomAccessFiles`; one for indexes, and one for data having names relatively:\n\n[source]\n----\n${java.io.tmpdir}/getting-started/{today}.cq4\n----\n\n- Writing to a queue\n\n[source,Java]\n----\n// Obtains an ExcerptAppender\nExcerptAppender appender = queue.acquireAppender();\n\n// Writes: {msg: TestMessage}\nappender.writeDocument(w -> w.write(\"msg\").text(\"TestMessage\"));\n\n// Writes: TestMessage\nappender.writeText(\"TestMessage\");\n----\n\n- Reading from a queue\n\n[source,Java]\n----\n// Creates a tailer\nExcerptTailer tailer = queue.createTailer();\n\ntailer.readDocument(w -> System.out.println(\"msg: \" + w.read(()->\"msg\").text()));\n\nassertEquals(\"TestMessage\", tailer.readText());\n----\nAlso, the `ChronicleQueue.dump()` method can be used to dump the raw contents as a string.\n[source,Java]\n----\nqueue.dump();\n----\n\n- Cleanup\n\nChronicle Queue stores its data off-heap, and it is recommended that you call `close()` once you have finished working with Chronicle Queue, to free resources.\n\nNOTE: No data will be lost if you do this.\nThis is only to clean up resources that were used.\n\n[source,Java]\n----\nqueue.close();\n----\n\n- Putting it all together\n\n[source,Java]\n----\ntry (ChronicleQueue queue = SingleChronicleQueueBuilder.single(\"queue-dir\").build()) {\n    // Obtain an ExcerptAppender\n    ExcerptAppender appender = queue.acquireAppender();\n\n    // Writes: {msg: TestMessage}\n    appender.writeDocument(w -> w.write(\"msg\").text(\"TestMessage\"));\n\n    // Writes: TestMessage\n    appender.writeText(\"TestMessage\");\n\n    ExcerptTailer tailer = queue.createTailer();\n\n    tailer.readDocument(w -> System.out.println(\"msg: \" + w.read(()->\"msg\").text()));\n\n    assertEquals(\"TestMessage\", tailer.readText());\n}\n----\n\n=== Detailed Guide\nYou can configure a Chronicle Queue using its configuration parameters or system properties. In addition, there are different ways of writing/reading into/from a queue such as the use of proxies and using `MethodReader` and `MethodWriter`.\n\n==== Queue configuration\n\nChronicle Queue (CQ) can be configured via a number of methods on the `SingleChronicleQueueBuilder` class.\nA few of the parameters that were most queried by our customers are explained below .\n\n* *RollCycle*\n\nThe `RollCycle` parameter configures the rate at which CQ will roll the underlying queue files.\nFor instance, using the following code snippet will result in the queue files being rolled (i.e. a new file created) every hour:\n\n[source,java]\n----\nChronicleQueue.singleBuilder(queuePath).rollCycle(RollCycles.HOURLY).build()\n----\n\nOnce a queue's roll-cycle has been set, it cannot be changed at a later date.\nAny further instances of `SingleChronicleQueue` configured to use the same path should be configured to use the same roll-cycle,\nand if they are not, then the roll-cycle will be updated to match the persisted roll-cycle.\nIn this case, a warning log message will be printed in order to notify the library user of the situation:\n\n[source,java]\n----\n// Creates a queue with roll-cycle MINUTELY\ntry (ChronicleQueue minuteRollCycleQueue = ChronicleQueue.singleBuilder(queueDir).rollCycle(MINUTELY).build()) {\n\n    // Creates a queue with roll-cycle HOURLY\n    try (ChronicleQueue hourlyRollCycleQueue = ChronicleQueue.singleBuilder(queueDir).rollCycle(HOURLY).build()) {\n\n        try (DocumentContext documentContext = hourlyRollCycleQueue.acquireAppender().writingDocument()) {\n            documentContext.wire().write(\"somekey\").text(\"somevalue\");\n        }\n    }\n    // Now try to append using the queue configured with roll-cycle MINUTELY\n    try (DocumentContext documentContext2 = minuteRollCycleQueue.acquireAppender().writingDocument()) {\n        documentContext2.wire().write(\"otherkey\").text(\"othervalue\");\n    }\n}\n----\n\nconsole output:\n\n[source]\n----\n[main] WARN SingleChronicleQueueBuilder - Overriding roll cycle from HOURLY to MINUTELY.\n----\nThe maximum number of messages that can be stored in a queue file depends on roll cycle. See  https://github.com/OpenHFT/Chronicle-Queue/tree/master/docs/FAQ.adoc[FAQ] for more information on this.\n\nIn Chronicle Queue, the rollover time is based on UTC. The Timezone Rollover Enterprise feature extends Chronicle Queue's ability to specify the time and periodicity of queue rollovers, rather than UTC. For more information see link:https://portal.chronicle.software/docs/queue/chronicle-queue/configuration/timezone_rollover.html[Timezone Queue Rollover].\n\nThe Chronicle Queue `FileUtil` class provides useful methods for managing queue files. See link:docs/managing_roll_files_directly.adoc[Managing Roll Files Directly].\n\n* *wireType*\n\nIt's possible to configure how Chronicle Queue will store the data by explicitly set the `WireType`:\n\n[source,java]\n----\n// Creates a queue at \"queuePath\" and sets the WireType\nSingleChronicleQueueBuilder.builder(queuePath, wireType)\n----\nFor example:\n\n[source,java]\n----\n// Creates a queue with default WireType: BINARY_LIGHT\nChronicleQueue.singleBuilder(queuePath)\n\n// Creates a queue and sets the WireType as FIELDLESS_BINARY\nSingleChronicleQueueBuilder.fieldlessBinary(queuePath)\n\n// Creates a queue and sets the WireType as DEFAULT_ZERO_BINARY\nSingleChronicleQueueBuilder.defaultZeroBinary(queuePath)\n\n// Creates a queue and sets the WireType as DELTA_BINARY\nSingleChronicleQueueBuilder.deltaBinary(queuePath)\n----\n\nAlthough it's possible to explicitly provide WireType when creating a builder, it is discouraged as not all wire types are supported by Chronicle Queue yet.\nIn particular, the following wire types are not supported:\n\n* TEXT (and essentially all based on text, including JSON and CSV)\n\n* RAW\n\n* READ_ANY\n\n* *blockSize*\n\nWhen a queue is read/written, part of the file currently being read/written is mapped to a memory segment.\nThis parameter controls the size of the memory mapping block. You can change this parameter using the method `SingleChronicleQueueBuilder.blockSize(long blockSize)` if it is necessary.\n\nNOTE: You should avoid changing `blockSize` unnecessarily.\n\nIf you are sending large messages then you should set a large `blockSize` i.e. the `blockSize` should be at least four times the message size.\n\nWARNING:  If you use small `blockSize` for large messages you receive an `IllegalStateException` and the write is aborted.\n\nWe recommend that you use the same `blockSize` for each queue instance when replicating queues, the `blockSize` is not written to the queue's metadata, so should ideally be set to the same value when creating your instances of chronicle queue (this is recommended but if you wish to run with a different `blocksize` you can).\n\nTIP: Use the same `blockSize` for each instance of replicated queues.\n\n* *indexSpacing*\n\nThis parameter shows the space between excerpts that are explicitly indexed.\nA higher number means higher sequential write performance but slower random access read.\nThe sequential read performance is not affected by this property.\nFor example, the following default index spacing can be returned:\n\n* 16 (MINUTELY)\n\n* 64 (DAILY)\n\nYou can change this parameter using the method `SingleChronicleQueueBuilder.indexSpacing(int indexSpacing)`.\n\n* *indexCount*\n\nThe size of each index array, as well as the total number of index arrays per queue file.\n\n[NOTE]\nindexCount^2^ is the maximum number of indexed queue entries.\n\nNOTE: See Section link:#excerpt-indexing-in-chronicle-queue[Excerpt indexing in Chronicle Queue] of this User Guide for more information and examples of using indexes.\n\n* *readBufferMode, writeBufferMode*\n\nThese parameters define BufferMode for reads or writes that have the following options:\n\n* `None` - The default (and the only one available for open source users), no buffering;\n\n* `Copy` - used in conjunction with encryption;\n\n* `Asynchronous` - use an asynchronous buffer when reading and/or writing, provided by Chronicle Async Mode.\n\n* *bufferCapacity*\n\nRingBuffer capacity in bytes when using `bufferMode: Asynchronous`\n\n'''\n==== Writing to a queue using an appender\n\nIn Chronicle Queue we refer to the act of writing your data to the Chronicle Queue, as storing an excerpt. This data could be made up from any data type, including text, numbers, or serialised blobs. Ultimately, all your data, regardless of what it is, is stored as a series of bytes.\n\nJust before storing your excerpt, Chronicle Queue reserves a 4-byte header. Chronicle Queue writes the length of your data into this header. This way, when Chronicle Queue comes to read your excerpt, it knows how long each blob of data is. We refer to this 4-byte header, along with your excerpt, as a document. Strictly speaking Chronicle Queue can be used to read and write documents.\n\nNOTE:  Within this 4-byte header we also reserve a few bits for a number of internal operations, such as locking, to make Chronicle Queue thread-safe across both processors and threads.\nThe important thing to note is that because of this, you cant strictly convert the 4 bytes to an integer to find the length of your data blob.\n\nAs stated before, Chronicle Queue uses an *appender* to write to the queue and a *tailer* to read from the queue. Unlike other java queuing solutions, messages are not lost when they are read with a tailer. This is covered in more detail in the section below on \"Reading from a queue using a tailer\".\nTo write data to a Chronicle Queue, you must first create an appender:\n\n[source,Java]\n----\ntry (ChronicleQueue queue = ChronicleQueue.singleBuilder(path + \"/trades\").build()) {\n   final ExcerptAppender appender = queue.acquireAppender();\n}\n----\n\nChronicle Queue uses the following low-level interface to write the data:\n\n[source,Java]\n----\ntry (final DocumentContext dc = appender.writingDocument()) {\n      dc.wire().write().text(your text data);\n}\n----\n\nThe close on the try-with-resources, is the point when the length of the data is written to the header. You can also use the `DocumentContext` to find out the index that your data has just been assigned (see below). You can later use this index to move-to/look up this excerpt. Each Chronicle Queue excerpt has a unique index.\n\n[source,Java]\n----\ntry (final DocumentContext dc = appender.writingDocument()) {\n    dc.wire().write().text(your text data);\n    System.out.println(\"your data was store to index=\"+ dc.index());\n}\n----\n\nThe high-level methods below such as `writeText()` are convenience methods on calling `appender.writingDocument()`, but both approaches essentially do the same thing. The actual code of `writeText(CharSequence text)` looks like this:\n\n[source,Java]\n----\n/**\n * @param text the message to write\n */\nvoid writeText(CharSequence text) {\n    try (DocumentContext dc = writingDocument()) {\n        dc.wire().bytes().append8bit(text);\n    }\n}\n----\n\nSo you have a choice of a number of high-level interfaces, down to a low-level API, to raw memory.\n\nThis is the highest-level API which hides the fact you are writing to messaging at all. The benefit is that you can swap calls to the interface with a real component, or an interface to a different protocol.\n\n[source,Java]\n----\n// using the method writer interface.\nRiskMonitor riskMonitor = appender.methodWriter(RiskMonitor.class);\nfinal LocalDateTime now = LocalDateTime.now(Clock.systemUTC());\nriskMonitor.trade(new TradeDetails(now, \"GBPUSD\", 1.3095, 10e6, Side.Buy, \"peter\"));\n----\n\nYou can write a \"self-describing message\". Such messages can support schema changes. They are also easier to understand when debugging or diagnosing problems.\n\n[source,Java]\n----\n// writing a self describing message\nappender.writeDocument(w -> w.write(\"trade\").marshallable(\n        m -> m.write(\"timestamp\").dateTime(now)\n                .write(\"symbol\").text(\"EURUSD\")\n                .write(\"price\").float64(1.1101)\n                .write(\"quantity\").float64(15e6)\n                .write(\"side\").object(Side.class, Side.Sell)\n                .write(\"trader\").text(\"peter\")));\n----\n\nYou can write \"raw data\" which is self-describing. The types will always be correct; position is the only indication as to the meaning of those values.\n\n[source,Java]\n----\n// writing just data\nappender.writeDocument(w -> w\n        .getValueOut().int32(0x123456)\n        .getValueOut().int64(0x999000999000L)\n        .getValueOut().text(\"Hello World\"));\n----\n\nYou can write \"raw data\" which is not self-describing. Your reader must know what this data means, and the types that were used.\n\n[source,Java]\n----\n// writing raw data\nappender.writeBytes(b -> b\n        .writeByte((byte) 0x12)\n        .writeInt(0x345678)\n        .writeLong(0x999000999000L)\n        .writeUtf8(\"Hello World\"));\n----\n\nBelow, the lowest level way to write data is illustrated. You get an address to raw memory and you can write whatever you want.\n\n[source,Java]\n----\n// Unsafe low level\nappender.writeBytes(b -> {\n    long address = b.address(b.writePosition());\n    Unsafe unsafe = UnsafeMemory.UNSAFE;\n    unsafe.putByte(address, (byte) 0x12);\n    address += 1;\n    unsafe.putInt(address, 0x345678);\n    address += 4;\n    unsafe.putLong(address, 0x999000999000L);\n    address += 8;\n    byte[] bytes = \"Hello World\".getBytes(StandardCharsets.ISO_8859_1);\n    unsafe.copyMemory(bytes, Jvm.arrayByteBaseOffset(), null, address, bytes.length);\n    b.writeSkip(1 + 4 + 8 + bytes.length);\n});\n----\n\nYou can print the contents of the queue. You can see the first two, and last two messages store the same data.\n\n[source,Java]\n----\n// dump the content of the queue\nSystem.out.println(queue.dump());\n----\nprints:\n\n[source,Yaml]\n----\n# position: 262568, header: 0\n--- !!data #binary\ntrade: {\n  timestamp: 2016-07-17T15:18:41.141,\n  symbol: GBPUSD,\n  price: 1.3095,\n  quantity: 10000000.0,\n  side: Buy,\n  trader: peter\n}\n# position: 262684, header: 1\n--- !!data #binary\ntrade: {\n  timestamp: 2016-07-17T15:18:41.141,\n  symbol: EURUSD,\n  price: 1.1101,\n  quantity: 15000000.0,\n  side: Sell,\n  trader: peter\n}\n# position: 262800, header: 2\n--- !!data #binary\n!int 1193046\n168843764404224\nHello World\n# position: 262830, header: 3\n--- !!data #binary\n000402b0       12 78 56 34 00 00  90 99 00 90 99 00 00 0B   xV4 \n000402c0 48 65 6C 6C 6F 20 57 6F  72 6C 64                Hello Wo rld\n# position: 262859, header: 4\n--- !!data #binary\n000402c0                                               12                 \n000402d0 78 56 34 00 00 90 99 00  90 99 00 00 0B 48 65 6C xV4 Hel\n000402e0 6C 6F 20 57 6F 72 6C 64                          lo World\n----\n\n'''\n==== Reading from a queue using a tailer\n\nReading the queue follows the same pattern as writing, except there is a possibility there is not a message when you attempt to read it.\n\n.Start Reading\n[source,Java]\n----\ntry (ChronicleQueue queue = ChronicleQueue.singleBuilder(path + \"/trades\").build()) {\n   final ExcerptTailer tailer = queue.createTailer();\n}\n----\n\nYou can turn each message into a method call based on the content of the message, and have Chronicle Queue automatically deserialize the method arguments. Calling `reader.readOne()` will automatically skip over (filter out) any messages that do not match your method reader.\n\n[source,Java]\n----\n// reading using method calls\nRiskMonitor monitor = System.out::println;\nMethodReader reader = tailer.methodReader(monitor);\n// read one message\nassertTrue(reader.readOne());\n----\n\nYou can decode the message yourself.\n\nNOTE: The names, type, and order of the fields doesn't have to match.\n\n[source,Java]\n----\nassertTrue(tailer.readDocument(w -> w.read(\"trade\").marshallable(\n        m -> {\n            LocalDateTime timestamp = m.read(\"timestamp\").dateTime();\n            String symbol = m.read(\"symbol\").text();\n            double price = m.read(\"price\").float64();\n            double quantity = m.read(\"quantity\").float64();\n            Side side = m.read(\"side\").object(Side.class);\n            String trader = m.read(\"trader\").text();\n            // do something with values.\n        })));\n----\n\nYou can read self-describing data values. This will check the types are correct, and convert as required.\n\n[source,Java]\n----\nassertTrue(tailer.readDocument(w -> {\n    ValueIn in = w.getValueIn();\n    int num = in.int32();\n    long num2 = in.int64();\n    String text = in.text();\n    // do something with values\n}));\n----\n\nYou can read raw data as primitives and strings.\n\n[source,Java]\n----\nassertTrue(tailer.readBytes(in -> {\n    int code = in.readByte();\n    int num = in.readInt();\n    long num2 = in.readLong();\n    String text = in.readUtf8();\n    assertEquals(\"Hello World\", text);\n    // do something with values\n}));\n----\n\nor, you can get the underlying memory address and access the native memory.\n\n[source,Java]\n----\nassertTrue(tailer.readBytes(b -> {\n    long address = b.address(b.readPosition());\n    Unsafe unsafe = UnsafeMemory.UNSAFE;\n    int code = unsafe.getByte(address);\n    address++;\n    int num = unsafe.getInt(address);\n    address += 4;\n    long num2 = unsafe.getLong(address);\n    address += 8;\n    int length = unsafe.getByte(address);\n    address++;\n    byte[] bytes = new byte[length];\n    unsafe.copyMemory(null, address, bytes, Jvm.arrayByteBaseOffset(), bytes.length);\n    String text = new String(bytes, StandardCharsets.UTF_8);\n    assertEquals(\"Hello World\", text);\n    // do something with values\n}));\n\n----\n\nNOTE: Every tailer sees every message.\n\nAn abstraction can be added to filter messages, or assign messages to just one message processor.\nHowever, in general you only need one main tailer for a topic, with possibly, some supporting tailers for monitoring etc.\n\nAs Chronicle Queue doesn't partition its topics, you get total ordering of all messages within that topic.\nAcross topics, there is no guarantee of ordering; if you want to replay deterministically from a system which consumes from multiple topics, we suggest replaying from that system's output.\n\n'''\n==== Tailers and file handlers clean up\n\nChronicle Queue tailers may create file handlers, the file handlers are cleaned up whenever the associated chronicle queue's `close()` method is invoked or whenever the Jvm runs a Garbage Collection.\nIf you are writing your code not have GC pauses and you explicitly want to clean up the file handlers, you can call the following:\n\n```java\n((StoreTailer)tailer).releaseResources()\n```\n\n'''\n==== Using `ExcerptTailer.toEnd()`\n\nIn some applications, it may be necessary to start reading from the end of the queue (e.g. in a restart scenario).\nFor this use-case, `ExcerptTailer` provides the `toEnd()` method.\nWhen the tailer direction is `FORWARD` (by default, or as set by the `ExcerptTailer.direction`\nmethod), then calling `toEnd()` will place the tailer just *after* the last existing record in the queue.\nIn this case, the tailer is now ready for reading any new records appended to the queue.\nUntil any new messages are appended to the queue, there will be no new `DocumentContext`\navailable for reading:\n\n[source,java]\n....\n// this will be false until new messages are appended to the queue\nboolean messageAvailable = tailer.toEnd().readingDocument().isPresent();\n....\n\nIf it is necessary to read backwards through the queue from the end, then the tailer can be set to read backwards:\n\n[source,java]\n....\nExcerptTailer tailer = queue.createTailer();\ntailer.direction(TailerDirection.BACKWARD).toEnd();\n....\n\nWhen reading backwards, then the `toEnd()` method will move the tailer to the last record in the queue.\nIf the queue is not empty, then there will be a\n`DocumentContext` available for reading:\n\n[source,java]\n----\n// this will be true if there is at least one message in the queue\nboolean messageAvailable = tailer.toEnd().direction(TailerDirection.BACKWARD).\n        readingDocument().isPresent();\n----\n\n'''\n==== Restartable tailers\n\nAKA named tailers.\n\nIt can be useful to have a tailer which continues from where it was up to on restart of the application.\n\n[source,Java]\n----\ntry (ChronicleQueue cq = SingleChronicleQueueBuilder.binary(tmp).build()) {\n    ExcerptTailer atailer = cq.createTailer(\"a\");\n    assertEquals(\"test 0\", atailer.readText());\n    assertEquals(\"test 1\", atailer.readText());\n    assertEquals(\"test 2\", atailer.readText()); // <1>\n\n    ExcerptTailer btailer = cq.createTailer(\"b\");\n    assertEquals(\"test 0\", btailer.readText()); // <3>\n}\n\ntry (ChronicleQueue cq = SingleChronicleQueueBuilder.binary(tmp).build()) {\n    ExcerptTailer atailer = cq.createTailer(\"a\");\n    assertEquals(\"test 3\", atailer.readText()); // <2>\n    assertEquals(\"test 4\", atailer.readText());\n    assertEquals(\"test 5\", atailer.readText());\n\n    ExcerptTailer btailer = cq.createTailer(\"b\");\n    assertEquals(\"test 1\", btailer.readText()); // <4>\n}\n----\n<1> Tailer \"a\" last reads message 2\n<2> Tailer \"a\" next reads message 3\n<3> Tailer \"b\" last reads message 0\n<4> Tailer \"b\" next reads message 1\n\nThis is from the `RestartableTailerTest` where there are two tailers, each with a unique name.\nThese tailers store their index within the Queue itself and this index is maintained as the tailer uses `toStart()`, `toEnd()`, `moveToIndex()` or reads a message.\n\nNOTE: The `direction()` is not preserved across restarts, only the next index to be read.\n\nNOTE: The index of a tailer is only progressed when the `DocumentContext.close()` is called.\nIf this is prevented by an error, the same message will be read on each restart.\n\n'''\n==== Command line tools - reading and writing a Chronicle Queue\n\nChronicle Queue stores its data in binary format, with a file extension of `cq4`:\n\n```\n\\\u0001@\u0006header\bSCQStoreE\u0001wireType\bWireTypeBINARYwritePositionroll\bSCQSRoll*length6format\nyyyyMMdd-HHepoch6indexing\fSCQSIndexingNindexCount\u0010indexSpacing\u0004index2IndexlastIndex\nlastAcknowledgedIndexReplicated\u0002recovery\u0012TimedStoreRecovery\u0016timeStamp\n```\n\nThis can often be a bit difficult to read, so it is better to dump the `cq4` files as text. This can also help you fix your production issues, as it gives you the visibility as to what has been stored in the queue, and in what order.\n\nYou can dump the queue to the terminal using `net.openhft.chronicle.queue.main.DumpMain` or `net.openhft.chronicle.queue.ChronicleReaderMain`. `DumpMain` performs a simple dump to the terminal while `ChronicleReaderMain` handles more complex operations, e.g. tailing a queue. They can both be run from the command line in a number of ways described below.\n\n'''\n==== DumpMain\n\nIf you have a project pom file that includes the Chronicle-Queue artifact, you can read a `cq4` file with the following command:\n\n[source, shell script]\n----\n$ mvn exec:java -Dexec.mainClass=\"net.openhft.chronicle.queue.main.DumpMain\" -Dexec.args=\"myqueue\"\n----\n\nIn the above command _myqueue_ is the directory containing your .cq4 files\n\nYou can also set up any dependent files manually. This requires the `chronicle-queue.jar`, from any version 4.5.3 or later, and that all dependent files are present on the class path. The dependent jars are listed below:\n\n```\n$ ls -ltr\ntotal 9920\n-rw-r--r--  1 robaustin  staff   112557 28 Jul 14:52 chronicle-queue-5.20.108.jar\n-rw-r--r--  1 robaustin  staff   209268 28 Jul 14:53 chronicle-bytes-2.20.104.jar\n-rw-r--r--  1 robaustin  staff   136434 28 Jul 14:56 chronicle-core-2.20.114.jar\n-rw-r--r--  1 robaustin  staff    33562 28 Jul 15:03 slf4j-api-1.7.30.jar\n-rw-r--r--  1 robaustin  staff    33562 28 Jul 15:03 slf4j-simple-1.7.30.jar\n-rw-r--r--  1 robaustin  staff   324302 28 Jul 15:04 chronicle-wire-2.20.105.jar\n-rw-r--r--  1 robaustin  staff    35112 28 Jul 15:05 chronicle-threads-2.20.101.jar\n-rw-r--r--  1 robaustin  staff   344235 28 Jul 15:05 affinity-3.20.0.jar\n-rw-r--r--  1 robaustin  staff   124332 28 Jul 15:05 commons-cli-1.4.jar\n-rw-r--r--  1 robaustin  staff  4198400 28 Jul 15:06 19700101-02.cq4\n```\n\nTIP: To find out which version of jars to include please, refer to the link:https://github.com/OpenHFT/OpenHFT/blob/74808dc7f0b55094d4fd6fce1817842baab5b87b/chronicle-bom/pom.xml[`chronicle-bom`].\n\nOnce the dependencies are present on the class path, you can run:\n\n```\n$ java -cp chronicle-queue-5.20.108.jar net.openhft.chronicle.queue.main.DumpMain 19700101-02.cq4\n```\n\nThis will dump the `19700101-02.cq4` file out as text, as shown below:\n\n[source,Yaml]\n----\n!!meta-data #binary\nheader: !SCQStore {\n  wireType: !WireType BINARY,\n  writePosition: 0,\n  roll: !SCQSRoll {\n    length: !int 3600000,\n    format: yyyyMMdd-HH,\n    epoch: !int 3600000\n  },\n  indexing: !SCQSIndexing {\n    indexCount: !short 4096,\n    indexSpacing: 4,\n    index2Index: 0,\n    lastIndex: 0\n  },\n  lastAcknowledgedIndexReplicated: -1,\n  recovery: !TimedStoreRecovery {\n    timeStamp: 0\n  }\n}\n\n...\n# 4198044 bytes remaining\n----\n\nNOTE: The example above does not show any user data, because no user data was written to this example file.\n\nThere is also a script named `dump_queue.sh` located in the `Chonicle-Queue/bin`-folder that gathers the needed dependencies in a shaded jar and uses it to dump the queue with `DumpMain`. The script can be run from the `Chronicle-Queue` root folder like this:\n\n[source, shell script]\n----\n$ ./bin/dump_queue.sh <file path>\n----\n\n'''\n==== Reading a queue using `ChronicleReaderMain`\n\nThe second tool for logging the contents of the chronicle queue is the `ChronicleReaderMain` (in the Chronicle Queue project). As mentioned above, it is able to perform several operations beyond printing the file content to the console. For example, it can be used to tail a queue to detect whenever new messages are added (rather like $tail -f).\n\nBelow is the command line interface used to configure `ChronicleReaderMain`:\n\n----\nusage: ChronicleReaderMain\n -a <binary-arg>                            Argument to pass to binary search class\n -b <binary-search>                         Use this class as a comparator to binary search\n -cbl <content-based-limiter>               Specify a content-based limiter\n -cblArg <content-based-limiter-argument>   Specify an argument for use by the content-based limiter\n -d <directory>                             Directory containing chronicle queue files\n -e <exclude-regex>                         Do not display records containing this regular expression\n -f                                         Tail behaviour - wait for new records to arrive\n -g                                         Show message history (when using method reader)\n -h                                         Print this help and exit\n -i <include-regex>                         Display records containing this regular expression\n -k                                         Read the queue in reverse\n -l                                         Squash each output message into a single line\n -m <max-history>                           Show this many records from the end of the data set\n -n <from-index>                            Start reading from this index (e.g. 0x123ABE)\n -named <named>                             Named tailer ID\n -r <as-method-reader>                      Use when reading from a queue generated using a MethodWriter\n -s                                         Display index\n -w <wire-type>                             Control output i.e. JSON\n -x <max-results>                           Limit the number of results to output\n -z                                         Print timestamps using the local timezone\n----\n\nJust as with `DumpQueue` you need the classes in the example above present on the class path. This can again be achieved by manually adding them and then run:\n\n```\n$ java -cp chronicle-queue-5.20.108.jar net.openhft.chronicle.queue.ChronicleReaderMain -d <directory>\n```\n\nAnother option is to create an Uber Jar using the Maven shade plugin. It is configured as follows:\n\n[source,xml]\n----\n <build>\n    <plugins>\n        <plugin>\n            <groupId>org.apache.maven.plugins</groupId>\n            <artifactId>maven-shade-plugin</artifactId>\n            <executions>\n                <execution>\n                    <phase>package</phase>\n                    <goals>\n                        <goal>shade</goal>\n                    </goals>\n                    <configuration>\n                        <filters>\n                            <filter>\n                                <artifact>*:*</artifact>\n                                <includes>\n                                    <include>net/openhft/**</include>\n                                    <include>software/chronicle/**</include>\n                                </includes>\n                            </filter>\n                        </filters>\n                    </configuration>\n                </execution>\n            </executions>\n        </plugin>\n    </plugins>\n</build>\n\n----\n\nOnce the Uber jar is present, you can run `ChronicleReaderMain` from the command line via:\n\n----\njava -cp \"$UBER_JAR\" net.openhft.chronicle.queue.ChronicleReaderMain \"19700101-02.cq4\"\n----\n\nLastly, there is a script for running the reader named `queue_reader.sh` which again is located in the `Chonicle-Queue/bin`-folder. It automatically gathers the needed dependencies in a shaded jar and uses it to run `ChronicleReaderMain`. The script can be run from the `Chronicle-Queue` root folder like this:\n\n[source, shell script]\n----\n$ ./bin/queue_reader.sh <options>\n----\n\n'''\n==== Writing into a queue using `ChronicleWriter`\n\nIf using `MethodReader` and `MethodWriter` then you can write single-argument method calls to a queue\nusing `net.openhft.chronicle.queue.ChronicleWriterMain` or the shell script `queue_writer.sh` e.g.\n\n[source,bash]\nusage: ChronicleWriterMain files.. -d <directory> [-i <interface>] -m <method>\nMissing required options: m, d\n -d <directory>   Directory containing chronicle queue to write to\n -i <interface>   Interface to write via\n -m <method>      Method name\n\nIf you want to write to the below \"doit\" method\n\n[source,java]\npublic interface MyInterface {\n    void doit(DTO dto);\n}\n\npublic class DTO extends SelfDescribingMarshallable {\n    private int age;\n    private String name;\n}\n\nThen you can call `ChronicleWriterMain -d queue doit x.yaml` with either (or both) of the below Yamls:\n\n[source,yaml]\n{\n  age: 19,\n  name: Henry\n}\n\nor\n[source,yaml]\n!x.y.z.DTO {\n  age: 42,\n  name: Percy\n}\n\nIf `DTO` makes use of custom serialisation then you should specify the interface to write to with `-i`\n\n''''\n==== High level interface for reading/writing\n\nChronicle v4.4+ supports the use of proxies to write and read messages.\nYou start by defining an asynchronous `interface`, where all methods have:\n\n- arguments which are only inputs\n- no return value or exceptions expected.\n\n.A simple asynchronous interface\n[source,Java]\n----\nimport net.openhft.chronicle.wire.SelfDescribingMarshallable;\ninterface MessageListener {\n    void method1(Message1 message);\n\n    void method2(Message2 message);\n}\n\nstatic class Message1 extends SelfDescribingMarshallable {\n    String text;\n\n    public Message1(String text) {\n        this.text = text;\n    }\n}\n\nstatic class Message2 extends SelfDescribingMarshallable {\n    long number;\n\n    public Message2(long number) {\n        this.number = number;\n    }\n}\n----\n\nTo write to the queue you can call a proxy which implements this interface.\n\n[source,Java]\n----\nSingleChronicleQueue queue1 = ChronicleQueue.singleBuilder(path).build();\n\nMessageListener writer1 = queue1.acquireAppender().methodWriter(MessageListener.class);\n\n// call method on the interface to send messages\nwriter1.method1(new Message1(\"hello\"));\nwriter1.method2(new Message2(234));\n----\n\nThese calls produce messages which can be dumped as follows.\n\n[source,yaml]\n----\n# position: 262568, header: 0\n--- !!data #binary\nmethod1: {\n  text: hello\n}\n# position: 262597, header: 1\n--- !!data #binary\nmethod2: {\n  number: !int 234\n}\n----\n\nTo read the messages, you can provide a reader which calls your implementation with the same calls that you made.\n\n[source,Java]\n----\n// a proxy which print each method called on it\nMessageListener processor = ObjectUtils.printAll(MessageListener.class)\n// a queue reader which turns messages into method calls.\nMethodReader reader1 = queue1.createTailer().methodReader(processor);\n\nassertTrue(reader1.readOne());\nassertTrue(reader1.readOne());\nassertFalse(reader1.readOne());\n----\n\nRunning this example prints:\n\n[source]\n----\nmethod1 [!Message1 {\n  text: hello\n}\n]\nmethod2 [!Message2 {\n  number: 234\n}\n]\n----\n\n* For more details see, https://vanilla-java.github.io/2016/03/24/Microservices-in-the-Chronicle-world-Part-2.html[Using Method Reader/Writers] and https://github.com/OpenHFT/Chronicle-Queue/blob/ea/src/test/java/net/openhft/chronicle/queue/MessageReaderWriterTest.java[MessageReaderWriterTest]\n\n''''\n==== Detailed tracing of timings\n\nChronicle Queue supports explicit, or implicit, nanosecond resolution timing for messages as they pass end-to-end over across your system.\nWe support using nano-time across machines, without the need for specialist hardware. To enable this, set the `sourceId` of the queue.\n\n.Enabling high resolution timings\n[source,Java]\n----\nChronicleQueue out = ChronicleQueue.singleBuilder(queuePath)\n        ...\n        .sourceId(1)\n        .build();\n\nSidedMarketDataListener combiner = out.acquireAppender()\n        .methodWriterBuilder(SidedMarketDataListener.class)\n        .get();\n\ncombiner.onSidedPrice(new SidedPrice(\"EURUSD1\", 123456789000L, Side.Sell, 1.1172, 2e6));\n----\n\nA timestamp is added for each read and write as it passes from service to service.\n\n.Downstream message triggered by the event above\n[source,Yaml]\n----\n--- !!data #binary\nhistory: {\n  sources: [\n    1,\n    0x426700000000 # <4>\n  ]\n  timings: [\n    1394278797664704, # <1>\n    1394278822632044, # <2>\n    1394278824073475  # <3>\n  ]\n}\nonTopOfBookPrice: {\n  symbol: EURUSD1,\n  timestamp: 123456789000,\n  buyPrice: NaN,\n  buyQuantity: 0,\n  sellPrice: 1.1172,\n  sellQuantity: 2000000.0\n}\n----\n<1> First write\n<2> First read\n<3> Write of the result of the read.\n<4> What triggered this event.\n\n''''\n==== Excerpt indexing in Chronicle Queue\n\nIn the following section you will find how to work with the excerpt index.\n\n** *Finding the index at the end of a Chronicle Queue*\n\nChronicle Queue appenders are thread-local.\nIn fact when you ask for:\n\n```\nfinal ExcerptAppender appender = queue.acquireAppender();\n```\n\nthe `acquireAppender()` uses a thread-local pool to give you an appender which will be reused to reduce object creation.\nAs such, the method call to:\n\n```\nlong index = appender.lastIndexAppended();\n```\n\nwill only give you the last index appended by this appender; not the last index appended by any appender.\nIf you wish to find the index of the last record written to the queue, then you have to call:\n\n```\nqueue.lastIndex()\n```\nWhich will return the index of the last excerpt present in the queue (or -1 for an empty queue). Note that if the queue is\nbeing written to concurrently it's possible the value may be an under-estimate, as subsequent entries may have been written\neven before it was returned.\n\n** *The number of messages between two indexes*\n\nTo count the number of messages between two indexes you can use:\n\n```\n((SingleChronicleQueue)queue).countExcerpts(<firstIndex>,<lastIndex>);\n```\n\nNOTE: You should avoid calling this method on latency sensitive code, because if the indexes are in different cycles this method may have to access the .cq4 files from the file system.\n\nfor more information on this see :\n\n```\nnet.openhft.chronicle.queue.impl.single.SingleChronicleQueue.countExcerpts\n```\n\n** *Move to a specific message and read it*\n\nThe following example shows how to write 10 messages, then move to the 5th message to read it\n[source,java]\n----\n@Test\npublic void read5thMessageTest() {\n    try (final ChronicleQueue queue = singleBuilder(getTmpDir()).build()) {\n\n        final ExcerptAppender appender = queue.acquireAppender();\n\n        int i = 0;\n        for (int j = 0; j < 10; j++) {\n\n            try (DocumentContext dc = appender.writingDocument()) {\n                dc.wire().write(\"hello\").text(\"world \" + (i++));\n                long indexWritten = dc.index();\n            }\n        }\n\n        // Get the current cycle\n        int cycle;\n        final ExcerptTailer tailer = queue.createTailer();\n        try (DocumentContext documentContext = tailer.readingDocument()) {\n            long index = documentContext.index();\n            cycle = queue.rollCycle().toCycle(index);\n        }\n\n        long index = queue.rollCycle().toIndex(cycle, 5);\n        tailer.moveToIndex(index);\n        try (DocumentContext dc = tailer.readingDocument()) {\n            System.out.println(dc.wire().read(\"hello\").text());\n        }\n }\n}\n----\n\n'''\n==== File retention\n\nYou can add a `StoreFileListener` to notify you when a file is added, or no longer used.\nThis can be used to delete files after a period of time.\nHowever, by default, files are retained forever.\nOur largest users have over 100 TB of data stored in queues.\n\nAppenders and tailers are cheap as they don't even require a TCP connection; they are just a few Java objects.\nThe only thing each tailer retains is an index which is composed from:\n\n* a cycle number.\nFor example, days since epoch, and\n\n* a sequence number within that cycle.\n\nIn the case of a `DAILY` cycle, the sequence number is 32 bits, and the `index = ((long) cycle << 32) | sequenceNumber` providing up to 4 billion entries per day.\nif more messages per day are anticipated, the `XLARGE_DAILY` cycle, for example, provides up 4 trillion entries per day using a 48-bit sequence number.\nPrinting the index in hexadecimal is common in our libraries, to make it easier to see these two components.\n\nRather than partition the queue files across servers, we support each server, storing as much data as you have disk space.\nThis is much more scalable than being limited to the amount of memory space that you have.\nYou can buy a redundant pair of 6TB of enterprise disks very much more cheaply than 6TB of memory.\n\n'''\n==== Monitoring that you have sufficient disk space\n\nChronicle Queue runs a background thread to watch for low disk space (see `net.openhft.chronicle.threads.DiskSpaceMonitor` class) as the JVM can crash when allocating a new memory mapped file if disk space becomes low enough. The disk space monitor checks (for each FileStore you are using Chronicle Queues on):\nthat there is less than 200MB free. If so you will see:\n[source,java]\n----\nJvm.warn().on(getClass(), \"your disk \" + fileStore + \" is almost full, \" +\n        \"warning: chronicle-queue may crash if it runs out of space.\");\n----\t\n\notherwise it will check for the threshold percentage and log out this message:\n[source,java]\n----\nJvm.warn().on(getClass(), \"your disk \" + fileStore\n        + \" is \" + diskSpaceFull + \"% full, \" +\n        \"warning: chronicle-queue may crash if it runs out of space.\");\n----\n\nThe threshold percentage is controlled by the chronicle.disk.monitor.threshold.percent system property. The default value is 0.\n\n'''\n==== File handles and flushing data to the disk\n\nAs mentioned previously Chronicle Queue stores its data off-heap in a .cq4 file.\nSo whenever you wish to append data to this file or read data into this file, chronicle queue will create a file handle .\nTypically, Chronicle Queue will create a new .cq4 file every day.\nHowever, this could be changed so that you can create a new file every hour, every minute or even every second.\n\nIf we create a queue file every second, we would refer to this as SECONDLY rolling.\nOf course, creating a new file every second is a little extreme, but it's a good way to illustrate the following point.\nWhen using secondly rolling, If you had written 10 seconds worth of data and then you wish to read this data, chronicle would have to scan across 10 files.\nTo reduce the creation of the file handles, chronicle queue cashes them lazily and when it comes to writing data to the queue files, care-full consideration must be taken when closing the files, because on most OSs a close of the file, will force any data that has been appended to the file, to be flushed to disk, and if we are not careful this could stall your application.\n\n'''\n==== Pretoucher and its configuration\n\n`Pretoucher` is a class designed to be called from a long-lived thread. The purpose of the Pretoucher\nis to accelerate writing in a queue. Upon invocation of the `execute()` method, this object will pre-touch\npages in the queue's underlying store file, so that they are resident in the page-cache (i.e. loaded from\nstorage) before they are required by appenders to the queue. Resources held by this object will be released when the underlying\nqueue is closed. Alternatively, the `shutdown()` method can be called to close the supplied queue and\nrelease any other resources. Invocation of the `execute()` method after `shutdown()` has been called will\ncause an `IllegalStateException` to be thrown.\n\nThe Pretoucher's configuration parameters (set via the system properties) are as follows:\n\n- `SingleChronicleQueueExcerpts.earlyAcquireNextCycle` (defaults to false): Causes the Pretoucher to create the next cycle file while the queue\nis still writing to the current one in order to mitigate the impact of stalls in the OS when creating new files.\n\nWARNING: `earlyAcquireNextCycle` is off by default and if it is going to be turned on, you should very carefully\nstress test before and after turning it on. Basically what you experience is related to your system.\n\n- `SingleChronicleQueueExcerpts.pretoucherPrerollTimeMs` (defaults to 2,000 milliseconds) The pretoucher will create new cycle files\nthis amount of time in advanced of them being written to. Effectively moves the Pretoucher's notion\nof which cycle is \"current\" into the future by `pretoucherPrerollTimeMs`.\n\n- `SingleChronicleQueueExcerpts.dontWrite` (defaults to false): Tells the Pretoucher to never create cycle files that do not already\nexist. As opposed to the default behaviour where if the Pretoucher runs inside a cycle where no excerpts\nhave been written, it will create the \"current\" cycle file. Obviously enabling this will prevent\n`earlyAcquireNextCycle` from working.\n\n** *Pretoucher usage example*\n\nThe configuration parameters of Pretoucher that were described above should be set via system properties. For example, in the following excerpt `earlyAcquireNextCycle` is set to `true` and `pretoucherPrerollTimeMs` to 100ms.\n[source,java]\n----\nSystem.setProperty(\"SingleChronicleQueueExcerpts.earlyAcquireNextCycle\", \"true\");\nSystem.setProperty(\"SingleChronicleQueueExcerpts.pretoucherPrerollTimeMs\", \"100\");\n----\nThe constructor of Pretoucher takes the name of the queue that this Pretoucher is assigned to and creates a new Pretoucher. Then, by invoking the `execute()` method the Pretoucher starts.\n[source,java]\n----\n// Creates the queue q1 (or q1 is a queue that already exists)\ntry(final SingleChronicleQueue q1 = SingleChronicleQueueBuilder.binary(\"queue-storage-path\").build();\n\n    final Pretoucher pretouch = PretouchUtil.INSTANCE.createPretoucher(q1)){\n    try {\n        pretouch.execute();\n\n    } catch (InvalidEventHandlerException e) {\n        throw Jvm.rethrow(e);\n    }\n}\n----\n\nThe method `close()`, closes the Pretoucher and releases its resources.\n[source,java]\n----\npretouch.close();\n----\n\nNOTE: The Pretoucher is an Enterprise feature\n\n== Performance and Benchmarking\nChronicle Queue can be monitored to obtain latency, throughput, and activity metrics, in real time (that is, within microseconds of the event triggering it).\n\n=== Latency Test for Chronicle Queue Replication\n\nThe following charts show how long it takes to:\n\n- write a 40 byte message to a Chronicle Queue\n- have the write replicated over TCP\n- have the second copy acknowledge receipt of the message\n- have a thread read the acknowledged message\n\nThe test was run for ten minutes, and the distribution of latencies plotted.\n\nimage:https://vanilla-java.github.io/images/Latency-to-993.png[]\n\nNOTE: There is a step in latency at around 10 million message per second; it jumps as the messages start to batch.\nAt rates below this, each message can be sent individually.\n\nThe 99.99 percentile and above are believed to be delays in passing the message over TCP. Further research is needed to prove this.\nThese delays are similar, regardless of the throughput.\nThe 99.9 percentile and 99.93 percentile are a function of how quickly the system can recover after a delay.\nThe higher the throughput, the less headroom the system has to recover from a delay.\n\nimage:https://vanilla-java.github.io/images/Latency-from-993.png[]\n\n=== Double-buffering for contended writes\n\nWhen double-buffering is disabled, all writes to the queue will be serialized based on the write lock acquisition.\nEach time `ExcerptAppender.writingDocument()`\nis called, appender tries to acquire the write lock on the queue, and if it fails to do so it blocks until write lock is unlocked, and in turn locks the queue for itself.\n\nWhen double-buffering is enabled, if appender sees that the write lock is acquired upon call to `ExcerptAppender.writingDocument()` call, it returns immediately with a context pointing to the secondary buffer, and essentially defers lock acquisition until the `context.close()` is called (normally with try-with-resources pattern it is at the end of the try block), allowing user to go ahead writing data, and then essentially doing memcpy on the serialized data (thus reducing cost of serialization).\nBy default, double-buffering is disabled. You can enable double-buffering by calling\n\n```\nSingleChronicleQueueBuilder.doubleBuffer(true);\n```\n\nNOTE: During a write that is buffered, `DocumentContext.index()` will throw an `IndexNotAvailableException`. This is because it is impossible to know the index until the buffer is written back to the queue, which only happens when the `DocumentContext` is closed.\n\nThis is only useful if (majority of) the objects being written to the queue are big enough AND their marshalling is not straight-forward (e.g. BytesMarshallable's marshalling is very efficient and quick and hence double-buffering will only slow things down), and if there's a heavy contention on writes (e.g. 2 or more threads writing a lot of data to the queue at a very high rate).\n\n- Results:\n\nBelow are the benchmark results for various data sizes at the frequency of 10 KHz for a cumbersome message (see `net.openhft.chronicle.queue.bench.QueueContendedWritesJLBHBenchmark`), YMMV - always do your own benchmarks:\n\n* 1 KB\n** Double-buffer disabled:\n+\n----\n-------------------------------- SUMMARY (Concurrent) ------------------------------------------------------------\nPercentile   run1         run2         run3      % Variation\n50:            90.40        90.59        91.17         0.42\n90:           179.52       180.29        97.50        36.14\n99:           187.33       186.69       186.82         0.05\n99.7:         213.57       198.72       217.28         5.86\n------------------------------------------------------------------------------------------------------------------\n-------------------------------- SUMMARY (Concurrent2) -----------------------------------------------------------\nPercentile   run1         run2         run3      % Variation\n50:           179.14       179.26       180.93         0.62\n90:           183.49       183.36       185.92         0.92\n99:           192.19       190.02       215.49         8.20\n99.7:         240.70       228.16       258.88         8.24\n------------------------------------------------------------------------------------------------------------------\n----\n\n** Double-buffer enabled:\n+\n----\n-------------------------------- SUMMARY (Concurrent) ------------------------------------------------------------\nPercentile   run1         run2         run3      % Variation\n50:            86.05        85.60        86.24         0.50\n90:           170.18       169.79       170.30         0.20\n99:           176.83       176.58       177.09         0.19\n99.7:         183.36       185.92       183.49         0.88\n------------------------------------------------------------------------------------------------------------------\n-------------------------------- SUMMARY (Concurrent2) -----------------------------------------------------------\nPercentile   run1         run2         run3      % Variation\n50:            86.24        85.98        86.11         0.10\n90:            89.89        89.44        89.63         0.14\n99:           169.66       169.79       170.05         0.10\n99.7:         175.42       176.32       176.45         0.05\n------------------------------------------------------------------------------------------------------------------\n----\n\n* 4 KB\n** Double-buffer disabled:\n+\n----\n-------------------------------- SUMMARY (Concurrent) ------------------------------------------------------------\nPercentile   run1         run2         run3      % Variation\n50:           691.46       699.65       701.18         0.15\n90:           717.57       722.69       721.15         0.14\n99:           752.90       748.29       748.29         0.00\n99.7:        1872.38      1743.36      1780.22         1.39\n------------------------------------------------------------------------------------------------------------------\n-------------------------------- SUMMARY (Concurrent2) -----------------------------------------------------------\nPercentile   run1         run2         run3      % Variation\n50:           350.59       353.66       353.41         0.05\n90:           691.46       701.18       697.60         0.34\n99:           732.42       733.95       729.34         0.42\n99.7:        1377.79      1279.49      1302.02         1.16\n------------------------------------------------------------------------------------------------------------------\n----\n\n** Double-buffer enabled:\n+\n----\n-------------------------------- SUMMARY (Concurrent) ------------------------------------------------------------\nPercentile   run1         run2         run3      % Variation\n50:           342.40       344.96       344.45         0.10\n90:           357.25       360.32       359.04         0.24\n99:           688.38       691.97       691.46         0.05\n99.7:        1376.77      1480.19      1383.94         4.43\n------------------------------------------------------------------------------------------------------------------\n-------------------------------- SUMMARY (Concurrent2) -----------------------------------------------------------\nPercentile   run1         run2         run3      % Variation\n50:           343.68       345.47       346.24         0.15\n90:           360.06       362.11       363.14         0.19\n99:           694.02       698.62       699.14         0.05\n99.7:        1400.32      1510.91      1435.14         3.40\n------------------------------------------------------------------------------------------------------------------\n----\n\n=== Jitter\n\nIf you wish to tune your code for ultra-low latency, you could take a similar approach to our `QueueReadJitterMain`\n\n[source,java]\n----\nnet.openhft.chronicle.queue.jitter.QueueReadJitterMain\n----\n\nThis code can be considered as a basic stack sampler profiler.\nThis is assuming you base your code on the `net.openhft.chronicle.core.threads.EventLoop`, you can periodically sample the stacks to find a stall.\nIt is recommended to not reduce the sample rate below 50 microseconds as this will produce too much noise\n\nIt is likely to give you finer granularity than a typical profiler.\nAs it is based on a statistical approach of where the stalls are, it takes many samples, to see which code has the highest grouping ( in other words the highest stalls ) and will output a trace that looks like the following :\n\n[console,java]\n----\n28\tat java.util.concurrent.ConcurrentHashMap.putVal(ConcurrentHashMap.java:1012)\n\tat java.util.concurrent.ConcurrentHashMap.put(ConcurrentHashMap.java:1006)\n\tat net.openhft.chronicle.core.util.WeakReferenceCleaner.newCleaner(WeakReferenceCleaner.java:43)\n\tat net.openhft.chronicle.bytes.NativeBytesStore.<init>(NativeBytesStore.java:90)\n\tat net.openhft.chronicle.bytes.MappedBytesStore.<init>(MappedBytesStore.java:31)\n\tat net.openhft.chronicle.bytes.MappedFile$$Lambda$4/1732398722.create(Unknown Source)\n\tat net.openhft.chronicle.bytes.MappedFile.acquireByteStore(MappedFile.java:297)\n\tat net.openhft.chronicle.bytes.MappedFile.acquireByteStore(MappedFile.java:246)\n\n25\tat net.openhft.chronicle.queue.jitter.QueueWriteJitterMain.lambda$main$1(QueueWriteJitterMain.java:58)\n\tat net.openhft.chronicle.queue.jitter.QueueWriteJitterMain$$Lambda$11/967627249.run(Unknown Source)\n\tat java.lang.Thread.run(Thread.java:748)\n\n21\tat java.util.concurrent.ConcurrentHashMap.putVal(ConcurrentHashMap.java:1027)\n\tat java.util.concurrent.ConcurrentHashMap.put(ConcurrentHashMap.java:1006)\n\tat net.openhft.chronicle.core.util.WeakReferenceCleaner.newCleaner(WeakReferenceCleaner.java:43)\n\tat net.openhft.chronicle.bytes.NativeBytesStore.<init>(NativeBytesStore.java:90)\n\tat net.openhft.chronicle.bytes.MappedBytesStore.<init>(MappedBytesStore.java:31)\n\tat net.openhft.chronicle.bytes.MappedFile$$Lambda$4/1732398722.create(Unknown Source)\n\tat net.openhft.chronicle.bytes.MappedFile.acquireByteStore(MappedFile.java:297)\n\tat net.openhft.chronicle.bytes.MappedFile.acquireByteStore(MappedFile.java:246)\n\n14\tat net.openhft.chronicle.queue.jitter.QueueWriteJitterMain.lambda$main$1(QueueWriteJitterMain.java:54)\n\tat net.openhft.chronicle.queue.jitter.QueueWriteJitterMain$$Lambda$11/967627249.run(Unknown Source)\n\tat java.lang.Thread.run(Thread.java:748)\n\n----\n\nfrom this, we can see that most of the samples (on this occasion 28 of them ) were captured in `ConcurrentHashMap.putVal()` if we wish to get finer grain granularity,\nwe will often add a `net.openhft.chronicle.core.Jvm.safepoint` into the code because thread dumps are only reported at safe-points.\n\n- Results:\n\nIn the test described above, the typical latency varied between 14 and 40 microseconds.\nThe 99 percentile varied between 17 and 56 microseconds depending on the throughput being tested.\nNotably, the 99.93% latency varied between 21 microseconds and 41 milliseconds, a factor of 2000.\n\n.Possible throughput results depending on acceptable latencies\n|===\n| Acceptable Latency | Throughput\n| < 30 microseconds 99.3% of the time | 7 million message per second\n| < 20 microseconds 99.9% of the time | 20 million messages per second\n| < 1 milliseconds 99.9% of the time | 50 million messages per second\n| < 60 microseconds 99.3% of the time | 80 million message per second\n|===\n\n=== More Benchmarks\n\nhttps://vanilla-java.github.io/2016/07/09/Batching-and-Low-Latency.html[Batching and Queue Latency]\n\nlink:https://portal.chronicle.software/docs/queue/chronicle-queue/performance/performance-test-results.html[End-to-End latency plots for various message sizes]\n\n=== Chronicle Queue vs Kafka\n\nChronicle Queue is designed to out-perform its rivals such as Kafka.\nChronicle Queue supports over an order-of-magnitude of greater throughput, together with an order-of-magnitude of lower latency, than Apache Kafka.\nWhile Kafka is faster than many of the alternatives, it doesn't match Chronicle Queue's ability to support throughputs of over a million events per second, while simultaneously achieving latencies of 1 to 20 microseconds.\n\nChronicle Queue handles more volume from a single thread to a single partition.\nThis avoids the need for the complexity, and the downsides, of having partitions.\n\nKafka uses an intermediate broker to use the operating system's file system and cache, while Chronicle Queue directly uses the operating system's file system and cache.\nFor comparison see http://kafka.apache.org/documentation.html[Kafka Documentation]\n\n== More Information and Support\n\n=== Links\n\n* https://github.com/OpenHFT/Chronicle-Queue/tree/develop/docs/BigDataAndChronicleQueue.adoc[Big Data and Chronicle Queue] - a detailed description of some techniques utilised by Chronicle Queue\n* https://github.com/OpenHFT/Chronicle-Queue/tree/develop/docs/FAQ.adoc[FAQ] - questions asked by customers\n* https://github.com/OpenHFT/Chronicle-Queue/tree/develop/docs/How_it_works.adoc[How it works] - more depth on how Chronicle Queue is implemented\n* https://github.com/OpenHFT/Chronicle-Queue/tree/develop/docs/utilities.adoc[Utilities] - lists some useful utilities for working with queue files\n\n==== Online support\n\n* http://stackoverflow.com/tags/chronicle/info[Chronicle support on StackOverflow]\n* https://groups.google.com/forum/?hl=en-GB#!forum/java-chronicle[Chronicle support on Google Groups]\n\n== Chronicle Software Release Notes\nhttps://chronicle.software/release-notes[Leave your e-mail] to get information about the latest releases and patches to stay up-to-date. \n"
        },
        {
          "name": "bin",
          "type": "tree",
          "content": null
        },
        {
          "name": "docs",
          "type": "tree",
          "content": null
        },
        {
          "name": "pom.xml",
          "type": "blob",
          "size": 27.4482421875,
          "content": "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<!--\n  ~ Copyright 2016 chronicle.software\n  ~\n  ~ Licensed under the *Apache License, Version 2.0* (the \"License\");\n  ~ you may not use this file except in compliance with the License.\n  ~ You may obtain a copy of the License at\n  ~\n  ~     http://www.apache.org/licenses/LICENSE-2.0\n  ~\n  ~ Unless required by applicable law or agreed to in writing, software\n  ~ distributed under the License is distributed on an \"AS IS\" BASIS,\n  ~ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n  ~ See the License for the specific language governing permissions and\n  ~ limitations under the License.\n  -->\n\n<project xmlns=\"http://maven.apache.org/POM/4.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\">\n    <modelVersion>4.0.0</modelVersion>\n\n    <parent>\n        <groupId>net.openhft</groupId>\n        <artifactId>java-parent-pom</artifactId>\n        <version>1.27ea0</version>\n        <relativePath />\n    </parent>\n\n    <artifactId>chronicle-queue</artifactId>\n    <version>5.27ea1-SNAPSHOT</version>\n    <packaging>bundle</packaging>\n    <name>OpenHFT/Chronicle Queue</name>\n    <description>Java library for persisted low latency messaging (Java 8+)</description>\n    <properties>\n        <zero.cost.assertions>disabled</zero.cost.assertions>\n        <additionalparam>-Xdoclint:none</additionalparam>\n        <sonar.organization>openhft</sonar.organization>\n        <sonar.host.url>https://sonarcloud.io</sonar.host.url>\n    </properties>\n\n    <dependencyManagement>\n        <dependencies>\n            <dependency>\n                <groupId>net.openhft</groupId>\n                <artifactId>third-party-bom</artifactId>\n                <version>3.27ea0</version>\n                <type>pom</type>\n                <scope>import</scope>\n            </dependency>\n            <dependency>\n                <groupId>net.openhft</groupId>\n                <artifactId>chronicle-bom</artifactId>\n                <version>2.27ea-SNAPSHOT</version>\n                <type>pom</type>\n                <scope>import</scope>\n            </dependency>\n        </dependencies>\n    </dependencyManagement>\n\n    <dependencies>\n\n        <dependency>\n            <groupId>net.openhft</groupId>\n            <artifactId>assertions-${zero.cost.assertions}</artifactId>\n            <scope>provided</scope>\n        </dependency>\n\n        <dependency>\n            <groupId>net.openhft</groupId>\n            <artifactId>chronicle-core</artifactId>\n        </dependency>\n\n        <dependency>\n            <groupId>net.openhft</groupId>\n            <artifactId>chronicle-bytes</artifactId>\n        </dependency>\n\n        <dependency>\n            <groupId>net.openhft</groupId>\n            <artifactId>chronicle-wire</artifactId>\n        </dependency>\n\n        <dependency>\n            <groupId>net.openhft</groupId>\n            <artifactId>chronicle-threads</artifactId>\n        </dependency>\n\n        <dependency>\n            <groupId>net.openhft</groupId>\n            <artifactId>affinity</artifactId>\n            <optional>true</optional>\n        </dependency>\n\n        <dependency>\n            <groupId>net.openhft</groupId>\n            <artifactId>jlbh</artifactId>\n            <optional>true</optional>\n        </dependency>\n\n        <dependency>\n            <groupId>commons-cli</groupId>\n            <artifactId>commons-cli</artifactId>\n        </dependency>\n\n        <dependency>\n            <groupId>org.slf4j</groupId>\n            <artifactId>slf4j-api</artifactId>\n        </dependency>\n\n        <dependency>\n            <groupId>org.jetbrains</groupId>\n            <artifactId>annotations</artifactId>\n        </dependency>\n\n        <!-- for testing -->\n        <dependency>\n            <groupId>net.openhft</groupId>\n            <artifactId>chronicle-values</artifactId>\n            <scope>test</scope>\n            <exclusions>\n                <exclusion>\n                    <groupId>com.sun.java</groupId>\n                    <artifactId>tools</artifactId>\n                </exclusion>\n            </exclusions>\n        </dependency>\n\n        <dependency>\n            <groupId>net.openhft</groupId>\n            <artifactId>chronicle-core</artifactId>\n            <type>test-jar</type>\n            <scope>test</scope>\n        </dependency>\n\n        <dependency>\n            <groupId>org.easymock</groupId>\n            <artifactId>easymock</artifactId>\n            <scope>test</scope>\n        </dependency>\n\n        <dependency>\n            <groupId>org.junit.jupiter</groupId>\n            <artifactId>junit-jupiter-api</artifactId>\n            <scope>test</scope>\n        </dependency>\n\n        <dependency>\n            <groupId>org.junit.jupiter</groupId>\n            <artifactId>junit-jupiter-params</artifactId>\n            <scope>test</scope>\n        </dependency>\n\n        <dependency>\n            <groupId>org.junit.vintage</groupId>\n            <artifactId>junit-vintage-engine</artifactId>\n            <scope>test</scope>\n        </dependency>\n\n        <dependency>\n            <groupId>org.slf4j</groupId>\n            <artifactId>slf4j-simple</artifactId>\n            <scope>test</scope>\n        </dependency>\n\n        <dependency>\n            <groupId>net.openhft</groupId>\n            <artifactId>chronicle-test-framework</artifactId>\n            <scope>test</scope>\n        </dependency>\n\n        <dependency>\n            <groupId>org.xerial.snappy</groupId>\n            <artifactId>snappy-java</artifactId>\n            <scope>test</scope>\n        </dependency>\n\n        <dependency>\n            <groupId>org.hsqldb</groupId>\n            <artifactId>hsqldb</artifactId>\n            <scope>test</scope>\n        </dependency>\n\n        <!-- for benchmarks -->\n\n        <dependency>\n            <groupId>org.openjdk.jmh</groupId>\n            <artifactId>jmh-core</artifactId>\n            <scope>test</scope>\n        </dependency>\n        <dependency>\n            <groupId>org.openjdk.jmh</groupId>\n            <artifactId>jmh-generator-annprocess</artifactId>\n            <scope>test</scope>\n        </dependency>\n    </dependencies>\n\n    <repositories>\n        <repository>\n            <id>third-party-release</id>\n            <name>ThirdParty Repository</name>\n            <url>\n                https://nexus.chronicle.software/content/repositories/thirdparty/\n            </url>\n            <releases>\n                <enabled>true</enabled>\n            </releases>\n        </repository>\n        <repository>\n            <id>chronicle-enterprise-snapshots</id>\n            <name>Snapshot Repository</name>\n            <url>\n                https://nexus.chronicle.software/content/repositories/snapshots\n            </url>\n            <snapshots>\n                <enabled>true</enabled>\n            </snapshots>\n        </repository>\n        <repository>\n            <id>chronicle-enterprise-release</id>\n            <url>\n                https://nexus.chronicle.software/content/repositories/releases\n            </url>\n            <releases>\n                <enabled>true</enabled>\n            </releases>\n        </repository>\n    </repositories>\n\n    <build>\n\n        <plugins>\n            <plugin>\n                <groupId>net.openhft</groupId>\n                <artifactId>binary-compatibility-enforcer-plugin</artifactId>\n\n                <executions>\n                    <execution>\n                        <phase>verify</phase>\n                        <goals>\n                            <goal>enforcer</goal>\n                        </goals>\n                        <configuration>\n                            <referenceVersion>5.27ea0</referenceVersion>\n                            <artifactsURI>https://teamcity.chronicle.software/repository/download</artifactsURI>\n                            <binaryCompatibilityPercentageRequired>99.8</binaryCompatibilityPercentageRequired>\n                            <extraOptions>\n                                <!--This skips \"internal\" but checks \"impl\"-->\n                                <extraOption>\n                                    <name>-keep-internal</name>\n                                </extraOption>\n                                <extraOption>\n                                    <name>skip-internal-packages</name>\n                                    <value>.*internal.*</value>\n                                </extraOption>\n                            </extraOptions>\n                        </configuration>\n                    </execution>\n                </executions>\n            </plugin>\n\n            <plugin>\n                <groupId>org.apache.maven.plugins</groupId>\n                <artifactId>maven-surefire-plugin</artifactId>\n                <configuration>\n                    <forkCount>1</forkCount>\n                    <!--                    <reuseForks>false</reuseForks>-->\n                    <runOrder>hourly</runOrder>\n                </configuration>\n            </plugin>\n\n            <plugin>\n                <groupId>org.apache.maven.plugins</groupId>\n                <artifactId>maven-source-plugin</artifactId>\n                <executions>\n                    <execution>\n                        <id>attach-sources</id>\n                        <goals>\n                            <goal>jar</goal>\n                            <goal>test-jar</goal>\n                        </goals>\n                    </execution>\n                </executions>\n            </plugin>\n\n            <plugin>\n                <groupId>org.apache.felix</groupId>\n                <artifactId>maven-bundle-plugin</artifactId>\n                <extensions>true</extensions>\n                <configuration>\n                    <instructions>\n                        <Bundle-SymbolicName>${project.groupId}.${project.artifactId}\n                        </Bundle-SymbolicName>\n                        <Bundle-Name>OpenHFT :: ${project.artifactId}</Bundle-Name>\n                        <Export-Package>net.openhft.chronicle.queue.*</Export-Package>\n                    </instructions>\n                </configuration>\n                <executions>\n                    <!--\n                      This execution makes sure that the manifest is available\n                      when the tests are executed\n                    -->\n                    <execution>\n                        <goals>\n                            <goal>manifest</goal>\n                        </goals>\n                    </execution>\n                </executions>\n            </plugin>\n\n            <!-- benchmark network latency with Wire -->\n            <plugin>\n                <groupId>org.codehaus.mojo</groupId>\n                <artifactId>exec-maven-plugin</artifactId>\n\n                <executions>\n                    <!-- run this first -->\n                    <execution>\n                        <id>Gateway</id>\n                        <goals>\n                            <goal>java</goal>\n                        </goals>\n                        <configuration>\n                            <skip>false</skip>\n                            <mainClass>net.openhft.chronicle.wire.channel.ChronicleGatewayMain</mainClass>\n                            <systemProperties>\n                                <property>\n                                    <key>system.properties</key>\n                                    <value>/dev/null</value>\n                                </property>\n                                <property>\n                                    <key>pauserMode</key>\n                                    <value>busy</value>\n                                </property>\n                            </systemProperties>\n                            <classpathScope>test</classpathScope>\n                        </configuration>\n                    </execution>\n\n                    <!-- PIPING MICROSERVICE LOW LATENCY TEST -->\n                    <!-- run this second with -Durl=tcp://localhost:1248 or the host of your choice -->\n                    <execution>\n                        <id>PipeClient</id>\n                        <goals>\n                            <goal>java</goal>\n                        </goals>\n                        <configuration>\n                            <skip>false</skip>\n                            <mainClass>net.openhft.chronicle.queue.channel.PerfLatencyMain</mainClass>\n                            <systemProperties>\n                                <property>\n                                    <key>system.properties</key>\n                                    <value>/dev/null</value>\n                                </property>\n                            </systemProperties>\n                            <classpathScope>test</classpathScope>\n                        </configuration>\n                    </execution>\n\n                    <!-- PIPING MICROSERVICE LATENCY WITH HIGH THROUGHPUT TEST -->\n                    <!-- run this second with -Durl=tcp://localhost:1248 or the host of your choice -->\n                    <execution>\n                        <id>PipeClient-Throughput</id>\n                        <goals>\n                            <goal>java</goal>\n                        </goals>\n                        <configuration>\n                            <skip>false</skip>\n                            <mainClass>net.openhft.chronicle.queue.channel.PerfLatencyMain</mainClass>\n                            <systemProperties>\n                                <property>\n                                    <key>system.properties</key>\n                                    <value>/dev/null</value>\n                                </property>\n                                <property>\n                                    <key>throughput</key>\n                                    <value>1000000</value>\n                                </property>\n                                <property>\n                                    <key>buffered</key>\n                                    <value>true</value>\n                                </property>\n                            </systemProperties>\n                            <classpathScope>test</classpathScope>\n                        </configuration>\n                    </execution>\n\n                    <!-- PIPING MICROSERVICE LOW LATENCY TEST -->\n                    <!-- run this second with -Durl=tcp://localhost:1248 or the host of your choice -->\n                    <execution>\n                        <id>PipeThroughput</id>\n                        <goals>\n                            <goal>java</goal>\n                        </goals>\n                        <configuration>\n                            <skip>false</skip>\n                            <mainClass>net.openhft.chronicle.queue.channel.PerfThroughputMain</mainClass>\n                            <systemProperties>\n                                <property>\n                                    <key>system.properties</key>\n                                    <value>/dev/null</value>\n                                </property>\n                            </systemProperties>\n                            <classpathScope>test</classpathScope>\n                        </configuration>\n                    </execution>\n\n                </executions>\n            </plugin>\n\n            <plugin>\n                <groupId>org.apache.maven.plugins</groupId>\n                <artifactId>maven-jar-plugin</artifactId>\n                <configuration>\n                    <archive>\n                        <manifest>\n                            <addClasspath>true</addClasspath>\n                        </manifest>\n                    </archive>\n                </configuration>\n                <executions>\n                    <execution>\n                        <goals>\n                            <goal>test-jar</goal>\n                        </goals>\n                    </execution>\n                </executions>\n            </plugin>\n        </plugins>\n        <resources>\n            <resource>\n                <directory>src/main/resources</directory>\n                <filtering>true</filtering>\n                <includes>\n                    <include>**/queue.pom.properties</include>\n                </includes>\n            </resource>\n            <resource>\n                <directory>src/main/resources</directory>\n                <filtering>false</filtering>\n                <excludes>\n                    <exclude>**/queue.pom.properties</exclude>\n                </excludes>\n            </resource>\n        </resources>\n\n    </build>\n\n    <profiles>\n        <profile>\n            <id>assertions</id>\n            <properties>\n                <zero.cost.assertions>enabled</zero.cost.assertions>\n            </properties>\n        </profile>\n\n        <profile>\n            <id>bundled-nexus-staging</id>\n            <repositories>\n                <repository>\n                    <id>chronicle-enterprise-release</id>\n                    <url>https://nexus.chronicle.software/content/repositories/releases</url>\n                    <releases>\n                        <enabled>true</enabled>\n                    </releases>\n                </repository>\n            </repositories>\n        </profile>\n\n        <profile>\n            <id>shade</id>\n            <activation>\n                <property>\n                    <name>build-all</name>\n                    <value>!true</value>\n                </property>\n            </activation>\n            <build>\n                <plugins>\n                    <plugin>\n                        <groupId>org.apache.maven.plugins</groupId>\n                        <artifactId>maven-shade-plugin</artifactId>\n                        <executions>\n                            <execution>\n                                <phase>package</phase>\n                                <goals>\n                                    <goal>shade</goal>\n                                </goals>\n                                <configuration>\n                                    <shadedArtifactAttached>true</shadedArtifactAttached>\n                                    <shadedClassifierName>all</shadedClassifierName>\n                                    <createDependencyReducedPom>false</createDependencyReducedPom>\n                                </configuration>\n                            </execution>\n                        </executions>\n                    </plugin>\n                </plugins>\n            </build>\n        </profile>\n        <profile>\n            <id>sonar</id>\n            <build>\n                <plugins>\n                    <plugin>\n                        <groupId>org.sonarsource.scanner.maven</groupId>\n                        <artifactId>sonar-maven-plugin</artifactId>\n                    </plugin>\n                    <plugin>\n                        <groupId>org.jacoco</groupId>\n                        <artifactId>jacoco-maven-plugin</artifactId>\n                        <executions>\n                            <execution>\n                                <goals>\n                                    <goal>prepare-agent</goal>\n                                </goals>\n                            </execution>\n                            <execution>\n                                <id>report</id>\n                                <phase>prepare-package</phase>\n                                <goals>\n                                    <goal>report</goal>\n                                </goals>\n                            </execution>\n                        </executions>\n                    </plugin>\n                </plugins>\n            </build>\n        </profile>\n\n        <profile>\n            <id>run-benchmarks</id>\n            <properties />\n            <build>\n                <plugins>\n                    <plugin>\n                        <groupId>org.codehaus.mojo</groupId>\n                        <artifactId>exec-maven-plugin</artifactId>\n\n                        <configuration>\n                            <classpathScope>test</classpathScope>\n                            <cleanupDaemonThreads>false</cleanupDaemonThreads>\n                        </configuration>\n                        <executions>\n                            <execution>\n                                <id>ByteArrayJLBHBenchmark</id>\n                                <phase>test</phase>\n                                <goals>\n                                    <goal>exec</goal>\n                                </goals>\n                                <configuration>\n                                    <executable>${java.home}/bin/java</executable>\n                                    <commandlineArgs>${jvm.requiredArgs} -Djvm.resource.tracing=false -classpath %classpath net.openhft.chronicle.queue.bench.ByteArrayJLBHBenchmark\n                                    </commandlineArgs>\n                                </configuration>\n                            </execution>\n                            <execution>\n                                <id>MethodReaderBenchmark</id>\n                                <phase>test</phase>\n                                <goals>\n                                    <goal>exec</goal>\n                                </goals>\n                                <configuration>\n                                    <executable>${java.home}/bin/java</executable>\n                                    <commandlineArgs>${jvm.requiredArgs} -Djvm.resource.tracing=false -classpath %classpath net.openhft.chronicle.queue.bench.MethodReaderBenchmark\n                                    </commandlineArgs>\n                                </configuration>\n                            </execution>\n                            <execution>\n                                <id>MethodReaderSkipBenchmark</id>\n                                <phase>test</phase>\n                                <goals>\n                                    <goal>exec</goal>\n                                </goals>\n                                <configuration>\n                                    <executable>${java.home}/bin/java</executable>\n                                    <commandlineArgs>${jvm.requiredArgs} -Djvm.resource.tracing=false -classpath %classpath net.openhft.chronicle.queue.bench.MethodReaderSkipBenchmark\n                                    </commandlineArgs>\n                                </configuration>\n                            </execution>\n                            <execution>\n                                <id>QueueContendedWritesJLBHBenchmark</id>\n                                <phase>test</phase>\n                                <goals>\n                                    <goal>exec</goal>\n                                </goals>\n                                <configuration>\n                                    <executable>${java.home}/bin/java</executable>\n                                    <commandlineArgs>${jvm.requiredArgs} -Djvm.resource.tracing=false -classpath %classpath net.openhft.chronicle.queue.bench.QueueContendedWritesJLBHBenchmark\n                                    </commandlineArgs>\n                                </configuration>\n                            </execution>\n                            <execution>\n                                <id>QueueLargeMessageJLBHBenchmark</id>\n                                <phase>test</phase>\n                                <goals>\n                                    <goal>exec</goal>\n                                </goals>\n                                <configuration>\n                                    <executable>${java.home}/bin/java</executable>\n                                    <commandlineArgs>${jvm.requiredArgs} -Djvm.resource.tracing=false -classpath %classpath net.openhft.chronicle.queue.bench.QueueLargeMessageJLBHBenchmark\n                                    </commandlineArgs>\n                                </configuration>\n                            </execution>\n                            <execution>\n                                <id>QueueSingleThreadedJLBHBenchmark</id>\n                                <phase>test</phase>\n                                <goals>\n                                    <goal>exec</goal>\n                                </goals>\n                                <configuration>\n                                    <executable>${java.home}/bin/java</executable>\n                                    <commandlineArgs>${jvm.requiredArgs} -Djvm.resource.tracing=false -classpath %classpath net.openhft.chronicle.queue.bench.QueueSingleThreadedJLBHBenchmark\n                                    </commandlineArgs>\n                                </configuration>\n                            </execution>\n                            <execution>\n                                <id>InternalAppenderJLBH</id>\n                                <phase>test</phase>\n                                <goals>\n                                    <goal>exec</goal>\n                                </goals>\n                                <configuration>\n                                    <executable>${java.home}/bin/java</executable>\n                                    <commandlineArgs>${jvm.requiredArgs} -Djvm.resource.tracing=false -classpath %classpath net.openhft.chronicle.queue.bench.InternalAppenderJLBH\n                                    </commandlineArgs>\n                                </configuration>\n                            </execution>\n                            <!-- QueueMultiThreadedJLBHBenchmark was moved to QE -->\n                            <execution>\n                                <id>RollCycleMultiThreadStressTest</id>\n                                <phase>test</phase>\n                                <goals>\n                                    <goal>exec</goal>\n                                </goals>\n                                <configuration>\n                                    <executable>${java.home}/bin/java</executable>\n                                    <commandlineArgs>${jvm.requiredArgs} -Djvm.resource.tracing=false -DtestTime=30 -classpath %classpath net.openhft.chronicle.queue.impl.single.stress.RollCycleMultiThreadStressTest\n                                    </commandlineArgs>\n                                </configuration>\n                            </execution>\n                            <execution>\n                                <id>RollCycleMultiThreadStressReadOnlyTest</id>\n                                <phase>test</phase>\n                                <goals>\n                                    <goal>exec</goal>\n                                </goals>\n                                <configuration>\n                                    <executable>${java.home}/bin/java</executable>\n                                    <commandlineArgs>${jvm.requiredArgs} -Djvm.resource.tracing=false -DtestTime=30 -classpath %classpath net.openhft.chronicle.queue.impl.single.stress.RollCycleMultiThreadStressReadOnlyTest\n                                    </commandlineArgs>\n                                </configuration>\n                            </execution>\n                        </executions>\n                    </plugin>\n                    <plugin>\n                        <groupId>org.apache.maven.plugins</groupId>\n                        <artifactId>maven-surefire-plugin</artifactId>\n                        <configuration>\n                            <!-- Disable unit tests -->\n                            <skip>true</skip>\n                        </configuration>\n                    </plugin>\n                </plugins>\n            </build>\n        </profile>\n    </profiles>\n\n    <scm>\n        <url>scm:git:git@github.com:OpenHFT/Chronicle-Queue.git</url>\n        <connection>scm:git:git@github.com:OpenHFT/Chronicle-Queue.git</connection>\n        <developerConnection>scm:git:git@github.com:OpenHFT/Chronicle-Queue.git\n        </developerConnection>\n        <tag>develop</tag>\n    </scm>\n\n</project>\n"
        },
        {
          "name": "src",
          "type": "tree",
          "content": null
        },
        {
          "name": "system.properties",
          "type": "blob",
          "size": 1.1015625,
          "content": "#\n# Copyright 2016-2022 chronicle.software\n#\n#       https://chronicle.software\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#       http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n# Tracing if resources are closed/released correctly.\njvm.resource.tracing=true\ndisable.resource.warning=true\ndisable.discard.warning=false\n# for profiling\njvm.safepoint.enabled=false\n# temporary option\nwarnAndCloseIfNotClosed=true\ncloseable.warn.secs=0.2\n# reduce logging of the announcer\nchronicle.announcer.disable=true\n# for RollCycleMultiThreadStressTest and descendants to reduce resource consumption\ncores=6\n# report delaying in linear scans\nchronicle.queue.report.linear.scan.latency=true\n"
        },
        {
          "name": "systemProperties.adoc",
          "type": "blob",
          "size": 2.619140625,
          "content": "== System Properties\nBelow, a number of relevant System Properties are listed.\n\nNOTE: With `getBoolean`, returns if a System Property with the provided {@code systemPropertyKey} either exists, is set to \"yes\" or is set to \"true\", see\nhttps://github.com/OpenHFT/Chronicle-Core/blob/351e79ed593fa656c21b4e5a540a3a5831cd06a3/src/main/java/net/openhft/chronicle/core/Jvm.java#L1184[javadoc].\n\n.System properties\n[cols=4*, options=\"header\"]\n|===\n| Property Key | Default | Description | Java Variable Name (Type)\n| chronicle.queue.checkrollcycle | `false` | Setting this property to \"yes\", \"true\" or \"\", prints a warning message every time a roll cycle file is created | _SHOULD_CHECK_CYCLE_ (boolean)\n| chronicle.queue.checkInterrupts | `false` | Setting this property to \"yes\", \"true\" or \"\", overrides any programmatic setting of checkInterrupt which will result in the tailer throwing an InterruptedException rather than rolling back any ongoing operation upon queue close | _checkInterrupts_ (boolean)\n| chronicle.queue.report.linear.scan.latency | `false` | Setting this property to \"yes\", \"true\" or \"\" - when scan time between two indexes exceeds 100 ms - the scan time will be printed | _REPORT_LINEAR_SCAN_ (boolean)\n| chronicle.queue.rollingResourceCache.size | 128 | Determines how many information elements (at most) pertaining to rolled queue files that should be held in memory at any given moment | _CACHE_SIZE_ (int)\n| chronicle.queue.warnSlowAppenderMs | 100 | Triggers warning message if an appender takes longer than default time | _WARN_SLOW_APPENDER_MS_ (int)\n| chronicle.table.store.timeoutMS | 10000 ms | The maximum time allowed when trying to acquire the exclusive table store lock | timeoutMS (long)\n| queue.force.unlock.mode | `LOCKING_PROCESS_DEAD` | See `UnlockMode` enum for allowable values |\n| queue.ignoreIndexingFailure | `false` | Setting this property to \"yes\", \"true\" or \"\", an exception is not thrown if the number of entries exceeds the max number for the current rollcycle | _IGNORE_INDEXING_FAILURE_ (boolean)\n| queue.check.index | `false` | Setting this property to \"yes\", \"true\" or \"\" returns if Chronicle Queue shall assert certain index invariants on various occasions throughout the code. Setting this property to \"\", \"yes\" or \"true\" will enable this feature. Enabling the feature will slow down execution if assertions (-ea) are enabled. | _CHECK_INDEX_ (boolean)\n| SingleChronicleQueueExcerpts.earlyAcquireNextCycle | `false` | Used by the pretoucher to acquire the next cycle file, but does NOT do the roll. Setting this property to \"yes\", \"true\" or \"\" results in acquiring the cycle file early | _EARLY_ACQUIRE_NEXT_CYCLE_ (boolean)\n|===\n"
        }
      ]
    }
  ]
}