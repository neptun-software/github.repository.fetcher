{
  "metadata": {
    "timestamp": 1736708504871,
    "page": 702,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjcyMA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "voldemort/voldemort",
      "stars": 2645,
      "defaultBranch": "master",
      "files": [
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.236328125,
          "content": "classes\ndist\n*.iml\n*.ipr\n*.iws\n*~\n*#\n.#*\ntest-output\nnode.id\nrebalancing.slave.list\nserver.state\n.version\n.temp\n.idea\ndata/\nMETA-INF/MANIFEST.MF\ngc.log\nvoldsys$_*\n.project\n.settings\n.pydevproject\n.gradle\nbuild\nlib\n.project\n.classpath\n*.class\n"
        },
        {
          "name": ".settings",
          "type": "tree",
          "content": null
        },
        {
          "name": "CONTRIBUTORS",
          "type": "blob",
          "size": 0.6435546875,
          "content": "Abhinay Nagpal\nAlex Feinberg\nAntoine Toulme\nAnthony Lauzon\nArunachalam Thirupathi\nBhavani Sudha Saktheeswaran\nBhupesh Bansal\nBrendan Harris\nBruce Ritchie\nChinmay Soman\nChris Riccomini\nClaudio Cherubino\nDain Sundstrom\nDave Brosius\nElias Torres\nEric Evans\nGeir Magnusson Jr.\nIsmael Juma\nJakob Homan\nJames Roper\nJanne Hietam√§ki\nJay Kreps\nJay Wylie\nJonathan Traupman\nJoshua Tuberville\nKirk True\nLei Gao\nMatthew Hayes\nMichael R. Head\nMike Frost\nNeha Narkhede\nPadraig O'Sullivan\nPaul Lindner\nRob Adams\nRoshan Sumbaly\nScott Wheeler\nShannon Zhang\nSiddharth Singh\nTatu Saloranta\nThomas Mueller\nTofig Suleymanov\nVinoth Chandar\nXu Ha\nYair Weinberger\nXu Ha\nZhongjie Wu\n\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 11.091796875,
          "content": "                                 Apache License\n                           Version 2.0, January 2004\n                        http://www.apache.org/licenses/\n\n   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n\n   1. Definitions.\n\n      \"License\" shall mean the terms and conditions for use, reproduction,\n      and distribution as defined by Sections 1 through 9 of this document.\n\n      \"Licensor\" shall mean the copyright owner or entity authorized by\n      the copyright owner that is granting the License.\n\n      \"Legal Entity\" shall mean the union of the acting entity and all\n      other entities that control, are controlled by, or are under common\n      control with that entity. For the purposes of this definition,\n      \"control\" means (i) the power, direct or indirect, to cause the\n      direction or management of such entity, whether by contract or\n      otherwise, or (ii) ownership of fifty percent (50%) or more of the\n      outstanding shares, or (iii) beneficial ownership of such entity.\n\n      \"You\" (or \"Your\") shall mean an individual or Legal Entity\n      exercising permissions granted by this License.\n\n      \"Source\" form shall mean the preferred form for making modifications,\n      including but not limited to software source code, documentation\n      source, and configuration files.\n\n      \"Object\" form shall mean any form resulting from mechanical\n      transformation or translation of a Source form, including but\n      not limited to compiled object code, generated documentation,\n      and conversions to other media types.\n\n      \"Work\" shall mean the work of authorship, whether in Source or\n      Object form, made available under the License, as indicated by a\n      copyright notice that is included in or attached to the work\n      (an example is provided in the Appendix below).\n\n      \"Derivative Works\" shall mean any work, whether in Source or Object\n      form, that is based on (or derived from) the Work and for which the\n      editorial revisions, annotations, elaborations, or other modifications\n      represent, as a whole, an original work of authorship. For the purposes\n      of this License, Derivative Works shall not include works that remain\n      separable from, or merely link (or bind by name) to the interfaces of,\n      the Work and Derivative Works thereof.\n\n      \"Contribution\" shall mean any work of authorship, including\n      the original version of the Work and any modifications or additions\n      to that Work or Derivative Works thereof, that is intentionally\n      submitted to Licensor for inclusion in the Work by the copyright owner\n      or by an individual or Legal Entity authorized to submit on behalf of\n      the copyright owner. For the purposes of this definition, \"submitted\"\n      means any form of electronic, verbal, or written communication sent\n      to the Licensor or its representatives, including but not limited to\n      communication on electronic mailing lists, source code control systems,\n      and issue tracking systems that are managed by, or on behalf of, the\n      Licensor for the purpose of discussing and improving the Work, but\n      excluding communication that is conspicuously marked or otherwise\n      designated in writing by the copyright owner as \"Not a Contribution.\"\n\n      \"Contributor\" shall mean Licensor and any individual or Legal Entity\n      on behalf of whom a Contribution has been received by Licensor and\n      subsequently incorporated within the Work.\n\n   2. Grant of Copyright License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      copyright license to reproduce, prepare Derivative Works of,\n      publicly display, publicly perform, sublicense, and distribute the\n      Work and such Derivative Works in Source or Object form.\n\n   3. Grant of Patent License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      (except as stated in this section) patent license to make, have made,\n      use, offer to sell, sell, import, and otherwise transfer the Work,\n      where such license applies only to those patent claims licensable\n      by such Contributor that are necessarily infringed by their\n      Contribution(s) alone or by combination of their Contribution(s)\n      with the Work to which such Contribution(s) was submitted. If You\n      institute patent litigation against any entity (including a\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\n      or a Contribution incorporated within the Work constitutes direct\n      or contributory patent infringement, then any patent licenses\n      granted to You under this License for that Work shall terminate\n      as of the date such litigation is filed.\n\n   4. Redistribution. You may reproduce and distribute copies of the\n      Work or Derivative Works thereof in any medium, with or without\n      modifications, and in Source or Object form, provided that You\n      meet the following conditions:\n\n      (a) You must give any other recipients of the Work or\n          Derivative Works a copy of this License; and\n\n      (b) You must cause any modified files to carry prominent notices\n          stating that You changed the files; and\n\n      (c) You must retain, in the Source form of any Derivative Works\n          that You distribute, all copyright, patent, trademark, and\n          attribution notices from the Source form of the Work,\n          excluding those notices that do not pertain to any part of\n          the Derivative Works; and\n\n      (d) If the Work includes a \"NOTICE\" text file as part of its\n          distribution, then any Derivative Works that You distribute must\n          include a readable copy of the attribution notices contained\n          within such NOTICE file, excluding those notices that do not\n          pertain to any part of the Derivative Works, in at least one\n          of the following places: within a NOTICE text file distributed\n          as part of the Derivative Works; within the Source form or\n          documentation, if provided along with the Derivative Works; or,\n          within a display generated by the Derivative Works, if and\n          wherever such third-party notices normally appear. The contents\n          of the NOTICE file are for informational purposes only and\n          do not modify the License. You may add Your own attribution\n          notices within Derivative Works that You distribute, alongside\n          or as an addendum to the NOTICE text from the Work, provided\n          that such additional attribution notices cannot be construed\n          as modifying the License.\n\n      You may add Your own copyright statement to Your modifications and\n      may provide additional or different license terms and conditions\n      for use, reproduction, or distribution of Your modifications, or\n      for any such Derivative Works as a whole, provided Your use,\n      reproduction, and distribution of the Work otherwise complies with\n      the conditions stated in this License.\n\n   5. Submission of Contributions. Unless You explicitly state otherwise,\n      any Contribution intentionally submitted for inclusion in the Work\n      by You to the Licensor shall be under the terms and conditions of\n      this License, without any additional terms or conditions.\n      Notwithstanding the above, nothing herein shall supersede or modify\n      the terms of any separate license agreement you may have executed\n      with Licensor regarding such Contributions.\n\n   6. Trademarks. This License does not grant permission to use the trade\n      names, trademarks, service marks, or product names of the Licensor,\n      except as required for reasonable and customary use in describing the\n      origin of the Work and reproducing the content of the NOTICE file.\n\n   7. Disclaimer of Warranty. Unless required by applicable law or\n      agreed to in writing, Licensor provides the Work (and each\n      Contributor provides its Contributions) on an \"AS IS\" BASIS,\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n      implied, including, without limitation, any warranties or conditions\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n      PARTICULAR PURPOSE. You are solely responsible for determining the\n      appropriateness of using or redistributing the Work and assume any\n      risks associated with Your exercise of permissions under this License.\n\n   8. Limitation of Liability. In no event and under no legal theory,\n      whether in tort (including negligence), contract, or otherwise,\n      unless required by applicable law (such as deliberate and grossly\n      negligent acts) or agreed to in writing, shall any Contributor be\n      liable to You for damages, including any direct, indirect, special,\n      incidental, or consequential damages of any character arising as a\n      result of this License or out of the use or inability to use the\n      Work (including but not limited to damages for loss of goodwill,\n      work stoppage, computer failure or malfunction, or any and all\n      other commercial damages or losses), even if such Contributor\n      has been advised of the possibility of such damages.\n\n   9. Accepting Warranty or Additional Liability. While redistributing\n      the Work or Derivative Works thereof, You may choose to offer,\n      and charge a fee for, acceptance of support, warranty, indemnity,\n      or other liability obligations and/or rights consistent with this\n      License. However, in accepting such obligations, You may act only\n      on Your own behalf and on Your sole responsibility, not on behalf\n      of any other Contributor, and only if You agree to indemnify,\n      defend, and hold each Contributor harmless for any liability\n      incurred by, or claims asserted against, such Contributor by reason\n      of your accepting any such warranty or additional liability.\n\n   END OF TERMS AND CONDITIONS\n\n   APPENDIX: How to apply the Apache License to your work.\n\n      To apply the Apache License to your work, attach the following\n      boilerplate notice, with the fields enclosed by brackets \"[]\"\n      replaced with your own identifying information. (Don't include\n      the brackets!)  The text should be enclosed in the appropriate\n      comment syntax for the file format. We also recommend that a\n      file or class name and description of purpose be included on the\n      same \"printed page\" as the copyright notice for easier\n      identification within third-party archives.\n\n   Copyright [yyyy] [name of copyright owner]\n\n   Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.\n\n"
        },
        {
          "name": "NOTES",
          "type": "blob",
          "size": 2.546875,
          "content": "Getting Started\n\nFor the most up-to-date information see http://www.project-voldemort.com\n\n## checkout and build\njkreps@jkreps-md:/tmp > svn co svn+ssh://cm01.corp/lirepo/voldemort/trunk voldemort\njkreps@jkreps-md:/tmp > cd voldemort\njkreps@jkreps-md:/tmp/voldemort > ant\n\n## start single node cluster and connect to table named √ítest√ì\njkreps@jkreps-md:/tmp/voldemort > ./bin/voldemort-server.sh config/single_node_cluster &\njkreps@jkreps-md:/tmp/voldemort> ./bin/voldemort-shell.sh test tcp://localhost:6666\n\n## run some random commands to put and get strings\n> help\nCommands:\nput key value -- Associate the given value with the key.\nget key -- Retrieve the value associated with the key.\ndelete key -- Remove all values associated with the key.\npreflist key -- Get node preference list for given key.\nhelp -- Print this message.\nexit -- Exit from this shell.\n> put \"hello\" \"there\"\n> get \"hello\"\nversion(0:1): \"there\"\n> preflist \"hello\"\nNode 0\nhost:  localhost\nport: 6666\navailable: yes\nlast checked: 4614 ms ago\n\nExample usage in example/java/voldemort/example/VoldemortExample.java.\nExample configurations in config/\n\n\nCode layout\n\nannotations - Helper annotations\nclient - Code specific to the client\ncluster - domain model for a voldemort cluster\nrouting - Code specific to routing requests\nserialization - Code for turning bytes into objects and vice versa\nserver - Code for handling client requests\nstore - All the store implementations used by both client and server\nutils - Helpers!\nversioning - Vector clock stuff\nxml - Code for serializing the configuration data...should probably be move to serialization\n\n\nBackground Resources\n\n- Amazon Dynamo Paper -- http://s3.amazonaws.com/AllThingsDistributed/sosp/amazon-dynamo-sosp2007.pdf\n- http://www.allthingsdistributed.com/2007/12/eventually_consistent.html\n- OpenDHT and Bamboo papers\n- BDB Performance: http://www.oracle.com/technology/products/berkeley-db/pdf/berkeley-db-perf.pdf\n- Origin of vector clocks: http://research.microsoft.com/users/lamport/pubs/time-clocks.pdf\n- Brewer's conjecture: http://citeseer.ist.psu.edu/544596.html\n\nCurrent build is from r19 \n\n\nSupporting other clients\n- Each store is available via all protcols, they are seperated by port\n- Wire format vs. protocol (HTTP vs. Tcp/IP), protocol buffers\n- How to abstract wire format?\n- A given serialization type may or may not be supported by the client language\n\nSocket servers share threadpool? \n\nA WireProtocol takes bytes and creates a voldemort request, and takes objects to create a voldemort response.\nLikewise the client does the opposite\n\nConnector.handleRequest()\n"
        },
        {
          "name": "NOTICE",
          "type": "blob",
          "size": 8.1484375,
          "content": "   =========================================================================\n   ==  NOTICE file corresponding to the section 4d of                     ==\n   ==  the Apache License, Version 2.0                                    ==\n   =========================================================================\n\nThis product includes the libraries\n  Avro \n  commons-codec\n  commons-collections\n  commons-dbcp\n  commons-httpclient\n  commons-io\n  commons-lang\n  commons-logging\n  commons-pool\n  catalina-ant\n  servlet-api\n  velocity\n  xerces\n  libthrift\n  all of which are software developed by The Apache Software Foundation (http://www.apache.org/)\n  \nThis product includes google-collections and protocol buffers, two libraries developed by Google (http://www.google.com, http://code.google.com/p/google-collections/ and http://code.google.com/apis/protocolbuffers/).\n\nThis product includes jdom, a library developed by jdom.org.\n\nThis product includes jopt-simple, a library for parsing command line options (http://jopt-simple.sourceforge.net/).\n\nThis product includes Jackson, a JSON processor (http://jackson.codehaus.org/).\n\nThis product includes ParaNamer, a library that allows the parameter names of non-private methods and constructors to be accessed at runtime (http://paranamer.codehaus.org/).\n\nThis product includes BDB, Java edition, A library developed by Oracle (http://www.oracle.com/database/berkeley-db/je/index.html), which includes\nthe following license information:\n\n=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\nThe following is the license that applies to this copy of the Berkeley\nDB Java Edition software.  For a license to use the Berkeley DB Java\nEdition software under conditions other than those described here, or\nto purchase support for this software, please contact Oracle at\nberkeleydb-info_us@oracle.com.\n\n=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n/*\n * Copyright (c) 2002-2008 Oracle.  All rights reserved.\n *\n * Redistribution and use in source and binary forms, with or without\n * modification, are permitted provided that the following conditions\n * are met:\n * 1. Redistributions of source code must retain the above copyright\n *    notice, this list of conditions and the following disclaimer.\n * 2. Redistributions in binary form must reproduce the above copyright\n *    notice, this list of conditions and the following disclaimer in the\n *    documentation and/or other materials provided with the distribution.\n * 3. Redistributions in any form must be accompanied by information on\n *    how to obtain complete source code for the DB software and any\n *    accompanying software that uses the DB software.  The source code\n *    must either be included in the distribution or be available for no\n *    more than the cost of distribution plus a nominal fee, and must be\n *    freely redistributable under reasonable conditions.  For an\n *    executable file, complete source code means the source code for all\n *    modules it contains.  It does not include source code for modules or\n *    files that typically accompany the major components of the operating\n *    system on which the executable file runs.\n *\n * THIS SOFTWARE IS PROVIDED BY ORACLE ``AS IS'' AND ANY EXPRESS OR\n * IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED\n * WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, OR\n * NON-INFRINGEMENT, ARE DISCLAIMED.  IN NO EVENT SHALL ORACLE BE LIABLE\n * FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\n * CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\n * SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR\n * BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n * WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE\n * OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN\n * IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n */\n=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n/***\n * ASM: a very small and fast Java bytecode manipulation framework\n * Copyright (c) 2000-2005 INRIA, France Telecom\n * All rights reserved.\n *\n * Redistribution and use in source and binary forms, with or without\n * modification, are permitted provided that the following conditions\n * are met:\n * 1. Redistributions of source code must retain the above copyright\n *    notice, this list of conditions and the following disclaimer.\n * 2. Redistributions in binary form must reproduce the above copyright\n *    notice, this list of conditions and the following disclaimer in the\n *    documentation and/or other materials provided with the distribution.\n * 3. Neither the name of the copyright holders nor the names of its\n *    contributors may be used to endorse or promote products derived from\n *    this software without specific prior written permission.\n *\n * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\n * AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n * ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE\n * LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\n * CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\n * SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\n * INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\n * CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\n * ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF\n * THE POSSIBILITY OF SUCH DAMAGE.\n */\n\nThis product includes jetty, developed by mortbay.org.\n\nThis product includes junit, developed by junit.org\n\nThis product includes the COLT library for numerical computation developed by CERN:\n\n    Packages cern.colt* , cern.jet*, cern.clhep\n\n        Copyright (c) 1999 CERN - European Organization for Nuclear Research.\n\n        Permission to use, copy, modify, distribute and sell this software and its documentation for any purpose is hereby granted without fee,\n         provided that the above copyright notice appear in all copies and that both that copyright notice and this permission notice appear in \n         supporting documentation. CERN makes no representations about the suitability of this software for any purpose. It is provided \"as is\" \n         without expressed or implied warranty.\n\nCOLT in turn includes the following packages:\n\n    Packages hep.aida.*\n\n        Written by Pavel Binko, Dino Ferrero Merlino, Wolfgang Hoschek, Tony Johnson, Andreas Pfeiffer, and others. Check the FreeHEP home page \n        for more info. Permission to use and/or redistribute this work is granted under the terms of the LGPL License, with the exception that \n        any usage related to military applications is expressly forbidden. The software and documentation made available under the terms of this \n        license are provided with no warranty. \n\nThis product includes a copy of slf4j made by slf4j.org, which contains the following statement:\n    Copyright (c) 2004-2008 QOS.ch All rights reserved. Permission is hereby granted, free of charge, \n    to any person obtaining a copy of this software and associated documentation files (the \"Software\"), \n    to deal in the Software without restriction, including without limitation the rights to use, copy, modify, \n    merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the \n    Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission \n    notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT \n    WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A \n    PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, \n    DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION \n    WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE. \n    \nThis project includes the the XJDM driver for MongoDB which is available at http://github.com/geir/mongo-java-driver/tree/master\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 5.0419921875,
          "content": "# Voldemort is a distributed key-value storage system #\n\n_N.B.: Voldemort is no longer under development. LinkedIn was the primary maintainer and user of Voldemort, and stopped all production usage in 2018. Most of the Voldemort Read-Only use cases and some of the Read-Write use cases have migrated to [Venice](https://github.com/linkedin/venice), which is actively maintained and also open sourced._\n\n## Overview ##\n\n* Data is automatically replicated over multiple servers across multiple datacenters.\n* Data is automatically partitioned so each server contains only a subset of the total data\n* Server failure is handled transparently\n* Pluggable serialization is supported to allow rich keys and values including lists and tuples with named fields, as well as to integrate with common serialization frameworks like Protocol Buffers, Thrift, and Java Serialization\n* Data items are versioned to maximize data integrity in failure scenarios without compromising availability of the system\n* Each node is independent of other nodes with no central point of failure or coordination\n* Pluggable storage engines, to cater to different workloads\n* SSD Optimized Read Write storage engine, with support for multi-tenancy\n* Built in mechanism to fetch & serve batch computed data from Hadoop \n* Support for pluggable data placement strategies to support things like distribution across data centers that are geographical far apart.\n\nIt is used at LinkedIn by numerous critical services powering a large portion of the site. .\n\n## QuickStart ##\n\n*You can refer to http://www.project-voldemort.com for more info*\n\n### Download Code ###\n\n```bash\ncd ~/workspace\ngit clone https://github.com/voldemort/voldemort.git\ncd voldemort\n./gradlew clean build -x test\n```\n\n### Start Server ###\n\n```\n# in one terminal\nbin/voldemort-server.sh config/single_node_cluster\n```\n\n### Use Client Shell ###\n\nClient shell gives you fast access to the store. We already have a test store defined in the \"single_node_cluster\", whose key and value are both String.\n\n```bash\n# in another terminal\ncd ~/workspace/voldemort\nbin/voldemort-shell.sh test tcp://localhost:6666/\n```\n\nNow you have the the voldemort shell running. You can try these commands in the shell\n\n```\nput \"k1\" \"v1\"\nput \"k2\" \"v2\"\nget \"k1\"\ngetall \"k1\" \"k2\"\ndelete \"k1\"\nget \"k1\"\n```\n\nYou can find more commands by running```help```\n\nWant to dig into the detailed implementation or even contribute to Voldemort? [A quick git guide for people who want to make contributions to Voldemort](https://github.com/voldemort/voldemort/wiki/A-quick-git-guide-for-people-who-want-to-make-contributions-to-Voldemort).\n\n\n## Comparison to relational databases ##\n\nVoldemort is not a relational database, it does not attempt to satisfy arbitrary relations while satisfying ACID properties. Nor is it an object database that attempts to transparently map object reference graphs. Nor does it introduce a new abstraction such as document-orientation. It is basically just a big, distributed, persistent, fault-tolerant hash table. For applications that can use an O/R mapper like ActiveRecord or Hibernate this will provide horizontal scalability and much higher availability but at great loss of convenience. For large applications under internet-type scalability pressure, a system may likely consist of a number of functionally partitioned services or apis, which may manage storage resources across multiple data centers using storage systems which may themselves be horizontally partitioned. For applications in this space, arbitrary in-database joins are already impossible since all the data is not available in any single database. A typical pattern is to introduce a caching layer which will require hashtable semantics anyway. For these applications Voldemort offers a number of advantages:\n\n* Voldemort combines in memory caching with the storage system so that a separate caching tier is not required (instead the storage system itself is just fast).\n* Unlike MySQL replication, both reads and writes scale horizontally\n* Data partioning is transparent, and allows for cluster expansion without rebalancing all data\n* Data replication and placement is decided by a simple API to be able to accommodate a wide range of application specific strategies\n* The storage layer is completely mockable so development and unit testing can be done against a throw-away in-memory storage system without needing a real cluster (or even a real storage system) for simple testing\n\n## Contribution ##\n\nThe source code is available under the Apache 2.0 license. We are actively looking for contributors so if you have ideas, code, bug reports, or fixes you would like to contribute please do so.\n\nFor help please see the [discussion group](http://groups.google.com/group/project-voldemort), or the IRC channel chat.us.freenode.net #voldemort. Bugs and feature requests can be filed on [Github](https://github.com/voldemort/voldemort/issues).\n\n## Special Thanks ##\n\nWe would like to thank [JetBrains](http://www.jetbrains.com) for supporting Voldemort Project by offering open-source license of their [IntelliJ IDE](http://www.jetbrains.com/idea/) to us.\n"
        },
        {
          "name": "bin",
          "type": "tree",
          "content": null
        },
        {
          "name": "build.gradle",
          "type": "blob",
          "size": 22.693359375,
          "content": "apply plugin: 'java'\napply plugin: 'war'\n\nbuildscript {\n    repositories { jcenter() }\n    dependencies {\n        classpath 'com.github.jengelman.gradle.plugins:shadow:1.2.4'\n    }\n}\n\napply plugin: 'com.github.johnrengelman.shadow'\n\ndef String getProjectProperty(String propertyName) {\n    String propertyValue = \"null\"\n    if (hasProperty(propertyName)) {\n        propertyValue = this.properties[propertyName]\n    }\n    else {\n        throw new GradleScriptException(\"PropertyName \" + propertyName + \" is not defined in properties file\")\n    }\n    return propertyValue\n}\ndef projectName = project.name\n\ndef sourceDir = getProjectProperty('src.dir')\ndef distDir = getProjectProperty('dist.dir')\ndef classesDir = getProjectProperty('classes.dir')\ndef javaDir = getProjectProperty('java.dir')\ndef resourcesDir = getProjectProperty('resources.dir')\ndef javaDocDir = getProjectProperty('javadoc.dir')\n\ndef voldTestClassesDir = getProjectProperty('testclasses.dir')\n\ndef commonTestSrcDir = getProjectProperty('commontestsrc.dir')\ndef unitTestSrcDir = getProjectProperty('unittestsrc.dir')\ndef intTestSrcDir = getProjectProperty('inttestsrc.dir')\ndef longTestSrcDir = getProjectProperty('longtestsrc.dir')\n\ndef voldVersion = getProjectProperty('curr.release')\ndef javacVersion = getProjectProperty('javac.version')\n\n//This is the javaCompile variable version. Directly defining 'def version' will override this and cause nightmare\nversion = voldVersion\n\ndef archiveDirectoryName = projectName + '-' + version\ndef archiveDirectoryPath = distDir + \"/\" + archiveDirectoryName\n\ndef javadocEnabled = getProjectProperty('javadoc.enabled').toBoolean()\n\ndef deleteDirectoryContents(directory) {\n    project.file(directory).deleteDir()\n    project.file(directory).mkdirs()\n}\n\ndef gobblinExcludes = {\n    exclude group: 'org.apache.hive'\n    exclude group: 'com.google.protobuf'\n    exclude group: 'org.apache.avro'\n    exclude group: 'com.linkedin.gobblin', module: 'gobblin-hive-registration'\n}\n\nallprojects {\n    sourceCompatibility = javacVersion\n    targetCompatibility = javacVersion\n    compileJava.options.debug = true\n\n    repositories {\n        mavenCentral()\n        maven {\n            // For Hadoop dependencies\n            url \"https://repository.cloudera.com/artifactory/cloudera-repos/\"\n        }\n        maven {\n            // For BDB-Je dependencies\n            url \"http://download.oracle.com/maven/\"\n        }\n    }\n\n    // http://blog.joda.org/2014/02/turning-off-doclint-in-jdk-8-javadoc.html\n    if (JavaVersion.current().java8Compatible) {\n        tasks.withType(Javadoc) {\n            options.addStringOption('Xdoclint:none', '-quiet')\n        }\n    }\n\n    tasks.withType(Javadoc) {\n        enabled = javadocEnabled\n    }\n}\n\nsourceSets {\n    main {\n        java { srcDirs = [javaDir] }\n        resources {\n            srcDirs = [javaDir]\n            include '**/*.xsd'\n        }\n        output.classesDir = classesDir\n        output.resourcesDir = resourcesDir\n    }\n    test {\n        java {\n            srcDirs = [\n                commonTestSrcDir,\n                unitTestSrcDir,\n                intTestSrcDir,\n                longTestSrcDir\n            ]\n        }\n        output.classesDir = voldTestClassesDir\n    }\n}\n\ncompileJava.doLast {\n    project.copy {\n        from (javaDir) { exclude '**/*.java','**/*.html','**/log4j.properties' }\n        into classesDir\n    }\n\n    project.copy {\n        // Theoretically this block can be replaced by including the log4j.properties in main resources.\n        // But that causes the log4j.properties to be present in the voldJar . Not sure what is the\n        // implication of this change, so avoiding it for now.\n        from (javaDir) { include 'log4j.properties' }\n        into resourcesDir\n    }\n}\n\njavadoc {\n    destinationDir = file(javaDocDir)\n}\n\ncompileTestJava.doLast {\n    project.copy {\n        from (commonTestSrcDir) { exclude '**/*.java','**/*.html' }\n        from (unitTestSrcDir) { exclude '**/*.java','**/*.html' }\n        into voldTestClassesDir\n    }\n}\n\ntask testJar(type: Jar) {\n    baseName = projectName + \"-test\"\n    from sourceSets.test.output\n    destinationDir = project.file(distDir)\n}\n\njar {\n    manifest {\n        attributes 'Voldemort-Implementation-Version' : version,\n        'Implementation-Title': 'Voldemort',\n        'Implementation-Version': version,\n        'Implementation-Vendor' :'LinkedIn'\n    }\n    destinationDir = project.file(distDir)\n}\n\n\ntask contribJar(type:Jar) {\n    baseName = projectName + \"-contrib\"\n    from {subprojects*.sourceSets.main.output}\n    destinationDir = project.file(distDir)\n}\n\ntask srcJar(type: Jar, dependsOn: classes) {\n    classifier = 'sources'\n    from sourceSets.main.java.srcDirs\n    destinationDir = project.file(distDir)\n}\n\ntask javadocJar(type: Jar) {\n    enabled = javadocEnabled\n    classifier = 'javadoc'\n    from javadoc\n    destinationDir = file(distDir)\n}\n\ntask bnpJar(dependsOn: shadowJar) {\n    // Just a nicer more self-explanatory name than \"shadowJar\"\n}\n\nartifacts {\n    archives jar\n    archives testJar\n    archives contribJar\n    archives srcJar\n    archives javadocJar\n}\n\nclean {\n    delete(distDir)\n    delete('lib')\n    doLast { deleteDirectoryContents(javaDocDir) }\n}\n\n// Dependencies used by both BnP and Voldemort\n// TODO: Decide if we want to do that for all dependencies, even if they're used just in Voldemort...\n\ndef depAvro = 'org.apache.avro:avro:1.4.0'\ndef depProtoBuf = 'com.google.protobuf:protobuf-java:2.3.0'\ndef depJdom = 'org.jdom:jdom:1.1'\ndef depAzkaban = 'com.linkedin.azkaban:azkaban:2.5.0'\ndef depGuava = 'com.google.guava:guava:14.0.1'\ndef depLog4j = 'log4j:log4j:1.2.15'\ndef depJacksonMapper = 'org.codehaus.jackson:jackson-mapper-asl:1.9.13'\ndef depJoda = 'joda-time:joda-time:1.6'\ndef depTehuti = 'io.tehuti:tehuti:0.7.0'\n\n\nshadowJar {\n    zip64 true  // Required if fatjar has more than 64K files\n    classifier \"bnp\"\n    from sourceSets.main.output, sourceSets.test.output, sourceSets.main.resources\n    from {subprojects*.sourceSets.main.output}\n    from {subprojects*.sourceSets.test.output}\n    from project('gobblin').projectDir\n    dependsOn 'gobblin:shadowJar'\n\n    // Hadoop dependencies are expected to be provided by Azkaban.\n    // If Azkaban is not included in your deployment, you may need to remove the following excludes\n    exclude(\"**/org/apache/hadoop/**\")\n    exclude(\"**/org.apache.hadoop**\")\n\n    // Required when working in an Hadoop 2.x environment\n    dependencies {\n        include(dependency(depAvro))\n        include(dependency(depProtoBuf))\n        include(dependency(depJdom))\n        include(dependency(depAzkaban))\n        include(dependency(depGuava))\n        include(dependency(depLog4j))\n        include(dependency(depJacksonMapper))\n        include(dependency(depJoda))\n        include(dependency(depTehuti))\n    }\n    relocate 'com.google.protobuf', 'voldemort.shadow.2.3.0.com.google.protobuf'\n    relocate 'org.apache.avro', 'voldemort.shadow.1.4.0.org.apache.avro'\n    // TODO: find a way to exclude private lib's BDB-JE which gets pulled into the fat jar...\n}\n\nimport com.github.jengelman.gradle.plugins.shadow.tasks.ShadowJar\ntask protobufJar(type: ShadowJar) {\n    baseName = projectName + \"-protobuf\"\n\n    manifest {\n        attributes 'Voldemort-Implementation-Version' : version,\n        'Implementation-Title': 'Voldemort',\n        'Implementation-Version': version,\n        'Implementation-Vendor' :'LinkedIn'\n    }\n\n    dependencies {\n        include(dependency(depProtoBuf))\n        configurations.runtime.each { exclude(dependency(it)) }\n    }\n\n    configurations = [ project.configurations.runtime ]\n\n    from sourceSets.main.output\n    destinationDir = project.file(distDir)\n\n    relocate 'com.google.protobuf', 'voldemort.shadow.2.3.0.com.google.protobuf'\n    // Since BDB is included via compile files option, shadow does not support\n    // excluding them. https://github.com/johnrengelman/shadow/issues/142\n    // Once shadow supports excluding them, it can be excluded. There are\n    // Complex ways to exclude it, but not worth it.\n    relocate 'com.sleepycat' , 'voldemort.shadow.5.0.88.com.sleepycat'\n}\n\ntask copySources (type: Copy) {\n    from ('.') { include 'bin/*.sh', 'bin/*.bat' , 'bin/*.py' }\n    from ('.') { include  distDir + '/*.jar'}\n    from ('.') { exclude distDir + '/**' ,'bin/**' , 'build/**', '.git/**' , '.gradle/**', 'config/**/data/**' }\n    into archiveDirectoryPath\n}\n\ntask copyDeps(type: Copy) {\n    // note this only copies the dependencies of the root\n    // project into /lib if we start adding compile deps to\n    // subprojects will have to rethink\n    from { configurations.compile }\n    into \"lib\"\n}\n\ntask zip (type: Zip) {\n    dependsOn copySources, copyDeps, contribJar, protobufJar\n    baseName = projectName\n\n    from(distDir) {\n        include archiveDirectoryName + '/bin/**'\n        fileMode = 0755\n    }\n    from(distDir) {\n        include archiveDirectoryName + '/**'\n        exclude archiveDirectoryName + '/bin/**'\n    }\n\n    destinationDir = project.file(distDir)\n}\n\ntask tar (type: Tar) {\n    dependsOn copySources, copyDeps, contribJar, protobufJar\n    compression = Compression.GZIP\n    baseName = projectName\n    extension = \"tar.gz\"\n\n    from(distDir) {\n        include archiveDirectoryName + '/bin/**'\n        fileMode = 0755\n    }\n    from(distDir) {\n        include archiveDirectoryName + '/**'\n        exclude archiveDirectoryName + '/bin/**'\n    }\n\n    destinationDir = project.file(distDir)\n}\n\nwar {\n    dependsOn copyDeps\n    from sourceSets.main.output\n    webXml = project.file('web.xml')\n    destinationDir = project.file(distDir)\n}\n\nassemble.dependsOn copyDeps\n\ncopySources.dependsOn jar\n\nallprojects {\n    tasks.withType(Test) {\n        maxHeapSize = \"8g\"\n        // If ignoreFailures is not set, then merged reports will not be generated\n        // Gradle aborts further tasks on test failure. so if you run junitAll\n        // which runs 3 tests, reports task will never be run on failure cases.\n        ignoreFailures = true\n\n        useJUnit()\n\n        testLogging {\n            events \"started\", \"passed\", \"skipped\", \"failed\"\n            exceptionFormat = 'full'\n            // showStandardStreams = true\n\n            doFirst {\n                def classesSize = candidateClassFiles.files.size()\n                logger.lifecycle(\"{} starts executing {} test classes {}\",\n                        path, classesSize, classesSize > 0? \"(\" + candidateClassFiles.files*.name[0] + \", ...)\" : \"\")\n            }\n\n            //Set reasonable defaults for reports location\n            reports.html.destination = file(\"$buildDir/reports/$name\")\n            reports.junitXml.destination = file(\"$buildDir/$name-results\")\n        }\n        // Makes sure tests aren't marked \"UP-TO-DATE\" after running\n        outputs.upToDateWhen { false }\n    }\n}\n\ntasks.withType(Test) {\n    // note only the root project's tests fork for very test class\n    forkEvery = 1\n\n    // Do not set the max parallelism as there are tests that uses the same port and will\n    // run into bind exceptions.\n\n    //ignoreFailures = gradle.startParameter.continueOnFailure\n\n    //all standard error messages from tests will get routed to 'DEBUG' level messages.\n    //logging.captureStandardError(LogLevel.DEBUG)\n    //all standard output messages from tests will get routed to 'DEBUG' level messages.\n    //logging.captureStandardOutput(LogLevel.DEBUG)\n\n    //Set reasonable defaults classpath and classes dir. They can be reconfigured in an individual task.\n//    it.testClassesDir = sourceSets.test.output.classesDir\n//    classpath = sourceSets.test.runtimeClasspath\n}\n\ntask resetConfig() {\n    doLast {\n        def DirsToDelete = [\".temp\", \".version\", \"data\"]\n        def deleteRecursively\n\n        deleteRecursively = { file ->\n            file.eachFile() {f ->\n                if(f.directory) {\n                    if( DirsToDelete.contains(f.getName()) )\n                    {\n                        println \"deleting ${f.getAbsolutePath()}\"\n                        delete f\n                    }\n                    else\n                    {\n                        deleteRecursively(f)\n                    }\n                }\n            }\n        }\n\n        deleteRecursively (new File(\"config\"))\n    }\n}\n\ntask junit(dependsOn: test)\n\nCollection<String> testClassesFrom(String dir, String include = '**/*Test.*') {\n    //take all *Test.java files found in given dir, make the path relative and replace .java with .class\n    fileTree(dir: dir, includes: [include]).collect { it.absolutePath.replace(\"\\\\\", \"/\").replaceAll(file(dir).absolutePath.replace(\"\\\\\", \"/\") + \"/\", \"\").replaceAll(\".java\\$\", \".class\")}\n}\n\ntest {\n    description = \"Runs acceptance tests\"\n    include testClassesFrom(unitTestSrcDir)\n}\n\ntask junitLong(type: Test) {\n    description = \"Runs long junit tests\"\n    include testClassesFrom(longTestSrcDir)\n}\n\ntask junitInt(type: Test) {\n    description = \"Runs integration tests\"\n    include testClassesFrom(intTestSrcDir)\n}\n\ntask junitRebalance(type: Test) {\n    include testClassesFrom(unitTestSrcDir, '**/*Rebalance*Test.java')\n}\n\ntask junitRebalanceLong(type: Test) {\n    include testClassesFrom(longTestSrcDir, '**/*Rebalance*Test.java')\n}\n\ntask contribJunit(type:TestReport) {\n    // this populated below by depending on all the test tasks found\n    // in the subprojects.\n    destinationDir = file(\"$buildDir/reports/$name\")\n}\n\nsubprojects {\n    tasks.withType(Test) {\n        // hook up the report to teh contrib task and junitAll\n        rootProject.contribJunit.reportOn it\n        rootProject.junitAll.reportOn it\n    }\n}\n\ntask junitAll(type: TestReport) {\n    reportOn test, junitLong\n    destinationDir = file(\"$project.buildDir/reports/$name\")\n}\n\n\ntask aggregatedJunit(type: TestReport) {\n    destinationDir = file(\"$project.buildDir/reports/$name\")\n}\n\nallprojects {\n    tasks.withType(Test) {\n        finalizedBy rootProject.aggregatedJunit\n        doLast { rootProject.aggregatedJunit.reportOn it }\n    }\n}\n\ntask wrapper(type: Wrapper) { gradleVersion = '2.9' }\n\ndependencies {\n    // Avro serialization format\n    compile depAvro\n\n    // INTERNAL_LIBS azkaban version not found\n    // azkaban-common-0.05.jar\n\n    // INTERNAL_LIBS Used for tomcat deployment, not sure if anyone uses it\n    // catalina-ant.jar , version not found in maven central\n\n    // coders decoders containing the Base64,binary encoding\n    compile 'commons-codec:commons-codec:1.4'\n\n    // TRANSITIVE_DEPENDENCY Contrib jar depends on commons-configuration-1.6.jar\n    // commons-configuration instead depends on commons-collection\n    //compile 'commons-collections:commons-collections:3.2.1'\n\n    // Used by MySql storage engine classes\n    // The jar supports database connection pooling\n    compile 'commons-dbcp:commons-dbcp:1.2.2'\n\n    // commons io is used at many places\n    // IOUtils, FileUtils and ByteArrayOutputStream\n    compile 'commons-io:commons-io:2.1'\n\n    // LZF compression strategy for store and tests.\n    compile 'com.ning:compress-lzf:0.9.1'\n\n    // Used all over the place for collections\n    compile depGuava\n\n    // used for readonly store hdfs fetcher.\n    compile 'org.apache.hadoop:hadoop-auth:2.3.0-cdh5.1.5'\n\n    // used at lots of places. Seems like there is some overlap between httpclient and core, but not clear\n    compile 'org.apache.httpcomponents:httpclient:4.1.2'\n\n    // contains both http server and client functionalities. Used for HttpResponse but could be used at more places.\n    compile 'org.apache.httpcomponents:httpcore:4.1.2'\n\n    // JSON mapping library from Java Objects to JSON\n    compile depJacksonMapper\n\n    // JSON processing library\n    compile 'org.codehaus.jackson:jackson-core-asl:1.9.13'\n\n    // Used for reading XML files and Document.\n    compile depJdom\n\n    // Jetty is used for HttpService and tests. Jetty Util is used for QueuedThreadPool class.\n    compile 'org.mortbay.jetty:jetty-util:6.1.18'\n    compile 'org.mortbay.jetty:jetty:6.1.18'\n\n    // A line processing library for command line. No compile time dependency\n    // Used by Voldemort shell\n    compile 'jline:jline:0.9.94'\n\n    // jna is library for invoking native functions\n    // used in the readonly store\n    compile 'net.java.dev.jna:jna:3.2.7'\n\n    // joda time is replacement for Java Date and Time\n    // used in readonly store code.\n    compile depJoda\n\n    // Used for argument command line parsing\n    compile 'net.sf.jopt-simple:jopt-simple:4.6'\n\n    // log4j - logger used in almost all files\n    compile depLog4j\n\n    // used in readonly store and Co-ordinator\n    compile 'javax.mail:mail:1.4.1'\n\n    // Used in co-ordinator and rest services\n    compile 'io.netty:netty:3.5.8.Final'\n\n    // TRANSITIVE_DEPENDENCY Paranamer is a library that allows the parameter names of non-private methods and constructors to be accessed at runtime\n    // Avro has a dependency on paranamer\n    // compile 'com.thoughtworks.paranamer:paranamer:2.1'\n\n    // protobuf is a supported protocol format between voldemort client and server\n    compile depProtoBuf\n\n    // Servlet\n    compile 'javax.servlet:servlet-api:2.5'\n\n    // slf4j is another logging abstraction framework.\n    // It is used by the apache.avro, apache.hadoop and r2 clients\n    compile 'org.slf4j:slf4j-api:1.5.6'\n    compile 'org.slf4j:slf4j-log4j12:1.5.6'\n\n    // snappy is one of the supported compression strategies in voldemort\n    compile 'org.iq80.snappy:snappy:0.2'\n\n    // Velocity is a simple yet powerful Java-based template engine that renders data\n    // from plain Java objects to text, xml, email, SQL, Post Script, HTML etc\n    // Velocity is used for Http Server GUI\n    compile 'org.apache.velocity:velocity:1.6.2'\n\n    // TRANSITIVE_DEPENDENCY Apache XML Parser\n    // used by jdom\n    // compile 'xerces:xercesImpl:2.9.1'\n\n    // BDB-JE from Oracle\n    compile 'com.sleepycat:je:5.0.104'\n\n    // cern library containing high performance Maps for int and double\n    // Currently only used in the tests\n    testCompile 'colt:colt:1.2.0'\n\n    // Used in resource pool perf testing class\n    testCompile 'commons-pool:commons-pool:1.5.2'\n\n    testRuntime 'mysql:mysql-connector-java:5.1.31'\n\n    // Used for unit tests and other automated testing\n    testCompile 'junit:junit:4.6'\n\n    // Mockito is written by our beloved friend Szczepan Faber :)\n    // Mocking framework used in some tests\n    testCompile 'org.mockito:mockito-all:1.8.5'\n\n//    contribCompile sourceSets.main.output\n//    contribCompile sourceSets.test.output\n\n    // declaring contribCompile dependencies as compile dependencies\n    // otherwise while copying dependencies to lib directory\n    // conflict resolution is not done properly across sourceSets\n    // and we end up with 2 versions of few jars like ( log4j, servlet etc. )\n    compile 'commons-configuration:commons-configuration:1.6'\n    compile('org.apache.hadoop:hadoop-core:2.3.0-mr1-cdh5.1.5') {\n        exclude group: 'com.google.protobuf'\n        exclude group: 'org.apache.avro'\n    }\n    compile('org.apache.hadoop:hadoop-common:2.3.0-cdh5.1.5') {\n        exclude group: 'com.google.protobuf'\n        exclude group: 'org.apache.avro'\n    }\n    compile('org.apache.hadoop:hadoop-hdfs:2.3.0-cdh5.1.5') {\n        exclude group: 'com.google.protobuf'\n        exclude group: 'org.apache.avro'\n    }\n\n    compile 'com.linkedin.pegasus:r2:1.8.3'\n    compile 'com.linkedin.pegasus:data:1.8.3'\n    compile 'com.linkedin.pegasus:pegasus-common:1.8.3'\n    compile depAzkaban\n\n    compile 'com.google.code.typica:typica:1.7.2'\n    compile 'com.sna-projects.krati:krati:0.4.9'\n\n    // Metrics\n    compile depTehuti\n    testCompile 'io.tehuti:tehuti:0.7.0:test'\n\n    // Other libs...\n    compile 'org.apache.tomcat:catalina-ant:6.0.43'\n    compile 'org.apache.hadoop:libthrift:0.5.0.0'\n\n    // rocksdb from maven\n    compile 'org.rocksdb:rocksdbjni:3.13.1'\n\n    // Bouncy Castle Libaray\n    compile 'org.bouncycastle:bcprov-jdk15on:1.48'\n\n    // Gobblin\n    compile 'com.linkedin.gobblin:gobblin-runtime:0.11.0', gobblinExcludes\n    compile 'com.linkedin.gobblin:gobblin-data-management:0.11.0', gobblinExcludes\n    compile 'com.linkedin.gobblin:gobblin-throttling-service-client:0.11.0', gobblinExcludes\n}\n\nsubprojects {\n    // this configures all the contrib subprojects\n    // note at the moment there dependencies are still\n    // declared in the dependency block above.\n    apply plugin:'java'\n    sourceCompatibility = javacVersion\n    targetCompatibility = javacVersion\n\n    sourceSets {\n        // note that the contrib projects don't currently have resource\n        // directories so below is actually just a to keep idea & gradle from\n        // thinking that the test resources dir is src/test/resources\n        main {\n            java { srcDirs = ['src/java'] }\n            resources { srcDirs = ['src/resources'] }\n        }\n        test {\n            java { srcDirs = ['test'] }\n            resources { srcDirs = ['testResources'] }\n        }\n    }\n    dependencies {\n        compile rootProject\n        // Used for unit tests and other automated testing\n        testCompile rootProject.sourceSets.test.runtimeClasspath\n    }\n\n    tasks.withType(Test) {\n        // this is required as the test utils expect the config directory\n        // to be at the root of the process working directory.\n        workingDir = rootProject.projectDir\n    }\n}\n\nallprojects {\n    apply plugin: 'idea'\n    apply plugin: 'eclipse'\n\n    eclipse {\n        jdt {\n            sourceCompatibility = targetCompatibility = 1.7\n        }\n        classpath {\n            defaultOutputDir = project.file('classes') // overrides the default of /bin which is where our scripts are\n            downloadSources = true\n            file {\n                whenMerged { classpath ->\n                    // so for some reason the generated .classpath for the contrib projects includes\n                    // two copies of *most* (maybe all) of the libraries from the parent project\n                    // the following de-dupes these\n                    def duplicateLibs = classpath.entries\n                        .findAll { it.kind == 'lib' } // only library entries\n                        .groupBy { it.library }       // index by the library\n                        .findAll { it.value.size() > 1 } // only where there is more that a single entry\n\n                    duplicateLibs.each { k, v ->\n                        // pick one from the list of dupes ..preferring the one without a sourcelib path\n                        // else just take the first\n                        def toRemove = v.find { !it.sourcePath } ?: v.first()\n                        classpath.entries.remove toRemove\n                    }\n                    // also with for no reason contrib projects gets a source path configured\n                    // with the following path.. since this path doesn't exist eclipse complains\n                    classpath.entries.removeAll {\n                        it.kind == 'lib' && it.library.path.endsWith('build/resources/test')\n                    }\n                }\n            }\n        }\n    }\n\n    idea {\n        module {\n            downloadJavadoc = true\n            downloadSources = true\n        }\n    }\n}\n"
        },
        {
          "name": "build.xml",
          "type": "blob",
          "size": 1.724609375,
          "content": "<?xml version=\"1.0\"?>\n\n<project name=\"voldemort\" basedir=\".\" default=\"bail-out\">\n\n  <target name=\"bail-out\">\n    <echo message=\"Ant is not supported anymore. Please use Gradle instead:\" />\n    <echo message=\"./gradlew tasks\" />\n  </target>\n\n  <target name=\"all\" depends=\"bail-out\" />\n\n  <target name=\"clean\" depends=\"bail-out\" />\n\n  <target name=\"build\" depends=\"bail-out\" />\n\n  <target name=\"buildtest\" depends=\"bail-out\" />\n\n  <target name=\"test\" depends=\"bail-out\" />\n\n  <target name=\"jar\" depends=\"bail-out\" />\n\n  <target name=\"srcjar\" depends=\"bail-out\" />\n\n  <target name=\"alljar\" depends=\"bail-out\" />\n\n  <target name=\"war\" depends=\"bail-out\" />\n\n  <target name=\"contrib-build\" depends=\"bail-out\" />\n\n  <target name=\"contrib-jar\" depends=\"bail-out\" />\n\n  <target name=\"contrib-srcjar\" depends=\"bail-out\" />\n\n  <target name=\"contrib-junit\" depends=\"bail-out\" />\n\n  <target name=\"ec2testing-junit\" depends=\"bail-out\" />\n\n  <target name=\"ec2testing-gossip\" depends=\"bail-out\" />\n\n  <target name=\"ec2testing-rebalancing\" depends=\"bail-out\" />\n\n  <target name=\"snapshot\" depends=\"bail-out\" />\n\n  <target name=\"release\" depends=\"bail-out\" />\n\n  <target name=\"hadoop-benchmark-jar\" depends=\"bail-out\" />\n\n  <target name=\"junit\" depends=\"bail-out\" />\n\n  <target name=\"junit-long\" depends=\"bail-out\" />\n\n  <target name=\"junit-rebalance\" depends=\"bail-out\" />\n\n  <target name=\"junit-long-rebalance\" depends=\"bail-out\" />\n\n  <target name=\"junit-test\" depends=\"bail-out\" />\n\n  <target name=\"junit-all\" depends=\"bail-out\" />\n\n  <target name=\"docs\" depends=\"bail-out\" />\n\n  <target name=\"redeploy\" depends=\"bail-out\" />\n\n  <target name=\"list\" depends=\"bail-out\" />\n\n  <target name=\"deploy\" depends=\"bail-out\" />\n\n  <target name=\"undeploy\" depends=\"bail-out\" />\n\n</project>\n"
        },
        {
          "name": "clients",
          "type": "tree",
          "content": null
        },
        {
          "name": "config",
          "type": "tree",
          "content": null
        },
        {
          "name": "contrib",
          "type": "tree",
          "content": null
        },
        {
          "name": "docs",
          "type": "tree",
          "content": null
        },
        {
          "name": "example",
          "type": "tree",
          "content": null
        },
        {
          "name": "gobblin",
          "type": "tree",
          "content": null
        },
        {
          "name": "gradle.properties",
          "type": "blob",
          "size": 1.1220703125,
          "content": "## Main source\nsrc.dir=src\njava.dir=src/java\npython.dir=clients/python\npython.proto.dir=clients/python/voldemort/protocol\nprotobuff.dir=src/proto\nclasses.dir=dist/classes\nresources.dir=dist/resources\ncommontestsrc.dir=test/common\nunittestsrc.dir=test/unit\nlongtestsrc.dir=test/long\ninttestsrc.dir=test/integration\ntestclasses.dir=dist/testclasses\ntestreport.dir=dist/junit-reports\ntesthtml.dir=dist/junit-reports/html\nsingletestreport.dir=dist/junit-single-reports\nsingletesthtml.dir=dist/junit-single-reports/html\nlongtestreport.dir=dist/junit-long-reports\nlongtesthtml.dir=dist/junit-long-reports/html\n\n## Contrib\ncontrib.root.dir=contrib\ncontrib.classes.dir=dist/contrib-classes\ncontribtestreport.dir=dist/contrib-junit-reports\ncontribtesthtml.dir=dist/contrib-junit-reports/html\ncontrib.testclasses.dir=dist/contribtestclasses\n\n## Other dirs\ndocs.dir=docs\njavadoc.dir=docs/javadoc\njavadoc.enabled=false\ndist.dir=dist\nwar.dir=war\n\n## Tomcat\ntomcat.manager.url=http://localhost:8080/manager\ntomcat.manager.username=tomcat\ntomcat.manager.password=tomcat\ntomcat.context=/voldemort\n\n## Java version\njavac.version=1.7\n\n## Release\ncurr.release=1.10.26\n"
        },
        {
          "name": "gradle",
          "type": "tree",
          "content": null
        },
        {
          "name": "gradlew",
          "type": "blob",
          "size": 4.8544921875,
          "content": "#!/usr/bin/env bash\n\n##############################################################################\n##\n##  Gradle start up script for UN*X\n##\n##############################################################################\n\n# Add default JVM options here. You can also use JAVA_OPTS and GRADLE_OPTS to pass JVM options to this script.\nDEFAULT_JVM_OPTS=\"\"\n\nAPP_NAME=\"Gradle\"\nAPP_BASE_NAME=`basename \"$0\"`\n\n# Use the maximum available, or set MAX_FD != -1 to use that value.\nMAX_FD=\"maximum\"\n\nwarn ( ) {\n    echo \"$*\"\n}\n\ndie ( ) {\n    echo\n    echo \"$*\"\n    echo\n    exit 1\n}\n\n# OS specific support (must be 'true' or 'false').\ncygwin=false\nmsys=false\ndarwin=false\ncase \"`uname`\" in\n  CYGWIN* )\n    cygwin=true\n    ;;\n  Darwin* )\n    darwin=true\n    ;;\n  MINGW* )\n    msys=true\n    ;;\nesac\n\n# Attempt to set APP_HOME\n# Resolve links: $0 may be a link\nPRG=\"$0\"\n# Need this for relative symlinks.\nwhile [ -h \"$PRG\" ] ; do\n    ls=`ls -ld \"$PRG\"`\n    link=`expr \"$ls\" : '.*-> \\(.*\\)$'`\n    if expr \"$link\" : '/.*' > /dev/null; then\n        PRG=\"$link\"\n    else\n        PRG=`dirname \"$PRG\"`\"/$link\"\n    fi\ndone\nSAVED=\"`pwd`\"\ncd \"`dirname \\\"$PRG\\\"`/\" >/dev/null\nAPP_HOME=\"`pwd -P`\"\ncd \"$SAVED\" >/dev/null\n\nCLASSPATH=$APP_HOME/gradle/wrapper/gradle-wrapper.jar\n\n# Determine the Java command to use to start the JVM.\nif [ -n \"$JAVA_HOME\" ] ; then\n    if [ -x \"$JAVA_HOME/jre/sh/java\" ] ; then\n        # IBM's JDK on AIX uses strange locations for the executables\n        JAVACMD=\"$JAVA_HOME/jre/sh/java\"\n    else\n        JAVACMD=\"$JAVA_HOME/bin/java\"\n    fi\n    if [ ! -x \"$JAVACMD\" ] ; then\n        die \"ERROR: JAVA_HOME is set to an invalid directory: $JAVA_HOME\n\nPlease set the JAVA_HOME variable in your environment to match the\nlocation of your Java installation.\"\n    fi\nelse\n    JAVACMD=\"java\"\n    which java >/dev/null 2>&1 || die \"ERROR: JAVA_HOME is not set and no 'java' command could be found in your PATH.\n\nPlease set the JAVA_HOME variable in your environment to match the\nlocation of your Java installation.\"\nfi\n\n# Increase the maximum file descriptors if we can.\nif [ \"$cygwin\" = \"false\" -a \"$darwin\" = \"false\" ] ; then\n    MAX_FD_LIMIT=`ulimit -H -n`\n    if [ $? -eq 0 ] ; then\n        if [ \"$MAX_FD\" = \"maximum\" -o \"$MAX_FD\" = \"max\" ] ; then\n            MAX_FD=\"$MAX_FD_LIMIT\"\n        fi\n        ulimit -n $MAX_FD\n        if [ $? -ne 0 ] ; then\n            warn \"Could not set maximum file descriptor limit: $MAX_FD\"\n        fi\n    else\n        warn \"Could not query maximum file descriptor limit: $MAX_FD_LIMIT\"\n    fi\nfi\n\n# For Darwin, add options to specify how the application appears in the dock\nif $darwin; then\n    GRADLE_OPTS=\"$GRADLE_OPTS \\\"-Xdock:name=$APP_NAME\\\" \\\"-Xdock:icon=$APP_HOME/media/gradle.icns\\\"\"\nfi\n\n# For Cygwin, switch paths to Windows format before running java\nif $cygwin ; then\n    APP_HOME=`cygpath --path --mixed \"$APP_HOME\"`\n    CLASSPATH=`cygpath --path --mixed \"$CLASSPATH\"`\n    JAVACMD=`cygpath --unix \"$JAVACMD\"`\n\n    # We build the pattern for arguments to be converted via cygpath\n    ROOTDIRSRAW=`find -L / -maxdepth 1 -mindepth 1 -type d 2>/dev/null`\n    SEP=\"\"\n    for dir in $ROOTDIRSRAW ; do\n        ROOTDIRS=\"$ROOTDIRS$SEP$dir\"\n        SEP=\"|\"\n    done\n    OURCYGPATTERN=\"(^($ROOTDIRS))\"\n    # Add a user-defined pattern to the cygpath arguments\n    if [ \"$GRADLE_CYGPATTERN\" != \"\" ] ; then\n        OURCYGPATTERN=\"$OURCYGPATTERN|($GRADLE_CYGPATTERN)\"\n    fi\n    # Now convert the arguments - kludge to limit ourselves to /bin/sh\n    i=0\n    for arg in \"$@\" ; do\n        CHECK=`echo \"$arg\"|egrep -c \"$OURCYGPATTERN\" -`\n        CHECK2=`echo \"$arg\"|egrep -c \"^-\"`                                 ### Determine if an option\n\n        if [ $CHECK -ne 0 ] && [ $CHECK2 -eq 0 ] ; then                    ### Added a condition\n            eval `echo args$i`=`cygpath --path --ignore --mixed \"$arg\"`\n        else\n            eval `echo args$i`=\"\\\"$arg\\\"\"\n        fi\n        i=$((i+1))\n    done\n    case $i in\n        (0) set -- ;;\n        (1) set -- \"$args0\" ;;\n        (2) set -- \"$args0\" \"$args1\" ;;\n        (3) set -- \"$args0\" \"$args1\" \"$args2\" ;;\n        (4) set -- \"$args0\" \"$args1\" \"$args2\" \"$args3\" ;;\n        (5) set -- \"$args0\" \"$args1\" \"$args2\" \"$args3\" \"$args4\" ;;\n        (6) set -- \"$args0\" \"$args1\" \"$args2\" \"$args3\" \"$args4\" \"$args5\" ;;\n        (7) set -- \"$args0\" \"$args1\" \"$args2\" \"$args3\" \"$args4\" \"$args5\" \"$args6\" ;;\n        (8) set -- \"$args0\" \"$args1\" \"$args2\" \"$args3\" \"$args4\" \"$args5\" \"$args6\" \"$args7\" ;;\n        (9) set -- \"$args0\" \"$args1\" \"$args2\" \"$args3\" \"$args4\" \"$args5\" \"$args6\" \"$args7\" \"$args8\" ;;\n    esac\nfi\n\n# Split up the JVM_OPTS And GRADLE_OPTS values into an array, following the shell quoting and substitution rules\nfunction splitJvmOpts() {\n    JVM_OPTS=(\"$@\")\n}\neval splitJvmOpts $DEFAULT_JVM_OPTS $JAVA_OPTS $GRADLE_OPTS\nJVM_OPTS[${#JVM_OPTS[*]}]=\"-Dorg.gradle.appname=$APP_BASE_NAME\"\n\nexec \"$JAVACMD\" \"${JVM_OPTS[@]}\" -classpath \"$CLASSPATH\" org.gradle.wrapper.GradleWrapperMain \"$@\"\n"
        },
        {
          "name": "gradlew.bat",
          "type": "blob",
          "size": 2.34765625,
          "content": "@if \"%DEBUG%\" == \"\" @echo off\r\n@rem ##########################################################################\r\n@rem\r\n@rem  Gradle startup script for Windows\r\n@rem\r\n@rem ##########################################################################\r\n\r\n@rem Set local scope for the variables with windows NT shell\r\nif \"%OS%\"==\"Windows_NT\" setlocal\r\n\r\n@rem Add default JVM options here. You can also use JAVA_OPTS and GRADLE_OPTS to pass JVM options to this script.\r\nset DEFAULT_JVM_OPTS=\r\n\r\nset DIRNAME=%~dp0\r\nif \"%DIRNAME%\" == \"\" set DIRNAME=.\r\nset APP_BASE_NAME=%~n0\r\nset APP_HOME=%DIRNAME%\r\n\r\n@rem Find java.exe\r\nif defined JAVA_HOME goto findJavaFromJavaHome\r\n\r\nset JAVA_EXE=java.exe\r\n%JAVA_EXE% -version >NUL 2>&1\r\nif \"%ERRORLEVEL%\" == \"0\" goto init\r\n\r\necho.\r\necho ERROR: JAVA_HOME is not set and no 'java' command could be found in your PATH.\r\necho.\r\necho Please set the JAVA_HOME variable in your environment to match the\r\necho location of your Java installation.\r\n\r\ngoto fail\r\n\r\n:findJavaFromJavaHome\r\nset JAVA_HOME=%JAVA_HOME:\"=%\r\nset JAVA_EXE=%JAVA_HOME%/bin/java.exe\r\n\r\nif exist \"%JAVA_EXE%\" goto init\r\n\r\necho.\r\necho ERROR: JAVA_HOME is set to an invalid directory: %JAVA_HOME%\r\necho.\r\necho Please set the JAVA_HOME variable in your environment to match the\r\necho location of your Java installation.\r\n\r\ngoto fail\r\n\r\n:init\r\n@rem Get command-line arguments, handling Windowz variants\r\n\r\nif not \"%OS%\" == \"Windows_NT\" goto win9xME_args\r\nif \"%@eval[2+2]\" == \"4\" goto 4NT_args\r\n\r\n:win9xME_args\r\n@rem Slurp the command line arguments.\r\nset CMD_LINE_ARGS=\r\nset _SKIP=2\r\n\r\n:win9xME_args_slurp\r\nif \"x%~1\" == \"x\" goto execute\r\n\r\nset CMD_LINE_ARGS=%*\r\ngoto execute\r\n\r\n:4NT_args\r\n@rem Get arguments from the 4NT Shell from JP Software\r\nset CMD_LINE_ARGS=%$\r\n\r\n:execute\r\n@rem Setup the command line\r\n\r\nset CLASSPATH=%APP_HOME%\\gradle\\wrapper\\gradle-wrapper.jar\r\n\r\n@rem Execute Gradle\r\n\"%JAVA_EXE%\" %DEFAULT_JVM_OPTS% %JAVA_OPTS% %GRADLE_OPTS% \"-Dorg.gradle.appname=%APP_BASE_NAME%\" -classpath \"%CLASSPATH%\" org.gradle.wrapper.GradleWrapperMain %CMD_LINE_ARGS%\r\n\r\n:end\r\n@rem End local scope for the variables with windows NT shell\r\nif \"%ERRORLEVEL%\"==\"0\" goto mainEnd\r\n\r\n:fail\r\nrem Set variable GRADLE_EXIT_CONSOLE if you need the _script_ return code instead of\r\nrem the _cmd.exe /c_ return code!\r\nif  not \"\" == \"%GRADLE_EXIT_CONSOLE%\" exit 1\r\nexit /b 1\r\n\r\n:mainEnd\r\nif \"%OS%\"==\"Windows_NT\" endlocal\r\n\r\n:omega\r\n"
        },
        {
          "name": "release_notes.txt",
          "type": "blob",
          "size": 53.1123046875,
          "content": "Release 1.10.26 8/30/2017\n* Improved some BnP-related logging.\n* The RO server is now returning old async op IDs idempotently.\n* Added retries for soft errors in AdminClient#waitForCompletion.\n* Fixed issue with too long commands in Windows 10 bat files.\n\nRelease 1.10.25 7/26/2017\n* Updated Gobblin dependency from 0.10.0 to 0.11.0\n\nRelease 1.10.24 7/21/2017\n* Added optional CDN feature to the BnP pipeline.\n* Fixed the client shell's list parsing logic.\n* Fixed a connection leakage due to unclosed adminClient.\n* Improved failure detection in HA mode.\n* Increased javac version to 1.7\n* Fixed various minor problems.\n\nRelease 1.10.23 10/11/2016\n* Added some extra logging when OOM occurs in BnP.\n* Made admin connection/socket timeout configurable in BnP.\n* Changed the DeleteAllFailedFetchStrategy so that it affects all nodes.\n* BnP now kills an async job that it is waiting on if that job times out.\n* Tweaked the AdminClient's currentVersion so that it is not stale.\n* BnP now retries fetches when cluster.xml is stale.\n\nRelease 1.10.22 9/20/2016\n* Fixes BnP resilience to colo failures\n* Fixes indentation issue with python client\n* Adds 'readonly.omit.port' server configuration\n* Fixes data cleanup job\n* Fixes discrepancy on data retention configuration\n\nRelease 1.10.21 8/10/2016\n* Fix admin request timeout issue because HDFSFailedlock takes long time.\n* Provide chunk size suggestion for BnP jobs.\n\nRelease 1.10.20 7/28/2016\n* Reduce the server log level on missing stores\n\nRelease 1.10.19 7/25/2016\n* getAll Quota support\n* meaningful log message, when fetch is disabled\n* Run BnP store verification in parallel.\n* Auto Detect Node Id from Cluster.xml\n* Validate Node Id for Host Match from Cluster.xml\n* Auto update Node Id on cluster.xml update\n* Generate SSH script for running on all hosts in cluster.xml (experimental)\n* Concurrent Modification Exception on InsufficientOperationlNodes Exception\n* Log time taken in verifyOrAddStore\n* Serialize rest-port on cluster.xml\n\nRelease 1.10.18 6/28/2016\n* get/set/delete quota takes QuotaType instead of string\n* Fix NPE when diffMessage is called from Build and Push\n* Missing store, gives a recognizable error message.\n* vadmin tool increased timeout\n* Logging changes for BnP\n* AdminClientPool for pooling adminClients.\n* AdminClient change to detect if cluster is modified\n* Parallel operation support for AddStore/SetQuota\n* Fetch single store for BnP Store Creation\n* Utility method to query only a single store\n* update store validates the store definition change\n\nRelease 1.10.17 6/1/2016\n* Add additional debug info to client registry\n* Fix Admin Connection/Socket timeout from Bnp\n* Add idle connection timeout feature for Voldemort clients.\n* Don't use Properties(properties) constructor in Voldemort\n\nRelease 1.10.16 5/23/2016\n* Lower the node connection errors log to debug level\n* Cleanup the HA state in HDFS directory during online state transition\n* Option to fetch single store in bootstrap\n* Fix Client shell reports exception on closing\n* Print more info when cluster metadata check fails\n* Route the system store to current zone only\n* Reliable way to set the quota for stores.\n* Add retry for Hdfs isFile call\n* Remove support for num.chunks in the Build and Push\n* ReplaceNode CLI test enhancement\n* Include quota check in metadata check\n* DataCleanup job picks up latest information for stores\n* Gradle task tar/zip fixes\n\nRelease 1.10.15 4/18/2016\n* Set-Metadata null Version error\n* Multi module gradle build for voldemort\n* The AdminClient's verifyOrAddStore is vulnerable to connectivity issues.\n\nRelease 1.10.14 3/29/2016\n* Report metrics for Scheduler and Async Service\n* better error message thrown when pushing to a store with storage quota 0\n* Resolved concurrency push conflict\n* Added a configurable flag on the BnP side that can disable store creation\n* Fixed build.gradle\n* Fixed a NPE thrown by the StorageEngineService on startup with views\n* Added server state checking when bringing it back\n* Fixed memory leaking issue in Hdfs FileSystem when fetching\n* AdminClient updates version after updating the data\n* Added second checksum validation when receiving data\n* Added more more aggregatedmetrics for HDFS data\n\nRelease 1.10.13 on 2/8/2016\n* Unit Test fixes for ClientRequestExecutorPool\n* ShadowJar with Protobuf dependency\n* HdfsFetcher error message cleanup\n\nRelease 1.10.12 on 1/11/2016\n* Fixed and cleaned up HadoopStoreWriterTest.\n* Fixed HDFS directory size measurement in HdfsFetcher.\n\nRelease 1.10.11 on 1/8/2016\n* Fixed RO data directory diverged on 1.10.7\n* Fixed BnP assigns keys into correct partitions, resulting in data loss\n* Fixed unknown.host unit test failure\n* Fixed HadoopStoreWriter for multiple chunks bug\n* Added Defensive code for BnP Partition reducer when it sees incorrect data\n\nRelease 1.10.10 on 1/6/2016\n* AdminClient constructor cleanup and respects the timeout of ClientConfig\n\nRelease 1.10.9 on 1/5/2016\n* Fixed a regression affecting Read-Only servers introduced in 1.10.7.\n* Fixed a unit test which failed on Java 8.\n* Made vadmin.sh more resilient to node failures.\n\nRelease 1.10.8 on 1/4/2016\n* Validate compression when a store is pushed via BnP\n* BnP HA pushes fail, when fetch is disabled on a node.\n* Enhanced Meta check which compares the file name and sizes\n* Wait for Scheduler Service to shutdown, before proceeding with shutdown\n* JMX errors during register/unregister for RO Stores\n\nRelease 1.10.7 on 12/15/2015\n* Introduced 'build.primary.replicas.only' mode in BnP.\n* Performance tool can now load custom workloads from external files.\n* Fixed a bug where the server would fail to start if Bouncy Castle is\n  absent from the classpath.\n* Admin operations improvements:\n  * Better error message when server receives unknown admin operation.\n  * Improved the Meta Check operation's default behavior.\n  * BnP fetches now abort immediately when their async operation is killed.\n\nRelease 1.10.6 on 12/08/2015\n* BnP loggin improvements\n* Option to use BouncyCastle for encryption/decryption, instead of JCE\n* JMX Metrics for tracking Server state\n* Enable BnP HA only on Connection errors\n\nRelease 1.10.5 on 11/25/2015\n* Extended voldemort shell to let \"preflist\" command show parition information.\n* Bug fix on python script used to generate cluster.xml.\n* Bug fix updating metadata version in cluster.xml and stores.xml.\n* Read-only improvements and fixes:\n  * Add replace URL of feching file to Voldemort node.\n\nRelease 1.10.4 on 11/11/2015\n* The AdminClient in the VoldemortClientShell is now configurable.\n* Improved error messages in BnP and Read-Only server-side code.\n* Fixed a server-side bug in BnP HA.\n* Made BnP jobs fail explicitly when clusters are inconsistent.\n\nRelease 1.10.3 on 11/04/2015\n* Bug fix and improvement to the Forklift tool.\n* Improved help prompt and configurability of the VoldemortClientShell.\n* Moved BnP's verifyOrAddStore() to the AdminClient.\n* Implement the truncate function in the RocksDB storage engine.\n  * N.B.: Previous data is backwards incompatible, see a0e19c6 for details.\n\nRelease 1.10.2 on 10/30/2015\n* Added a ReadOnlyFileValidator script.\n* Fixed rename operation in HdfsFailedFetchLock (regression fix).\n\nRelease 1.10.1 on 10/27/2015\n* Read-Only server improvements and fixes:\n  * Read-Only server now avoids needlessly recreating symlinks.\n  * Individual file checksums now outputted to stats file.\n  * Removed a harmful set quota admin op from BnP (regression fix).\n  * Added more details to an exception shown in BnP (regression fix).\n\nRelease 1.10.0 on 10/19/2015\n* Many many many fixes and improvements in the 1.9.x series of release! Many thanks to all contributors!\n* Minor improvements since the last patch release (1.9.22):\n  * Upgraded RocksDB to 3.13.1 and added more configurability options for it.\n  * Improvements to the benchmarking tool.\n  * Added a unit test for colliding keys in Read-Only data files.\n  * Added various safe-guards in case Read-Only index/data files become corrupted.\n\nRelease 1.9.22 on 10/08/2015\n* Disk quota for Voldemort Read-Only pipeline\n* Improved debuggability of BnP HA\n* Added heart beat for NIO selector\n* Added additional keepalive settings to avoid client connection leaks\n* Trimmed fat from HadoopUtils\n* Proper interruption of BnP hooks\n\nRelease 1.9.21 on 09/24/2015\n* Kerberos token caching now considered experimental (disabled by default).\n\nRelease 1.9.20 on 09/14/2015\n* BnP/RO improvements:\n  * Various debuggability improvements.\n  * Graceful recovery from incomplete store creation.\n  * HA locking now happens on the server-side.\n  * Removed 'node.id' BnP config which caused a SPoF.\n* Script improvements:\n  * voldemort-shell.sh properly print Avro data.\n  * Forklifting makes a proper schema check.\n  * Binary fetch-entries format.\n* Unit tests improvements:\n  * Longer retry times on ServerTestUtils.startVoldemortServer().\n  * Ignored failing RocksDB test since it is unimplemented.\n  * New forklifting tests.\n* RocksDB iteration logic is now properly implemented.\n* Improved JMX beans management.\n* Fixed the routing strategy of the voldsys$_store_quotas store.\n\nRelease 1.9.19 on 08/12/2015\n* Removed unused things (scala shell, ec2-testing, public-lib directory)\n* Fixed some unit tests.\n* Various BnP improvements:\n  * Improved logging.\n  * Added bnpJar gradle target for bundling BnP with its dependencies.\n  * Added run-bnp.sh script for running without Azkaban.\n* Various Read-Only server-side improvements:\n  * Upgrade to Hadoop 2.\n  * Kerberos authentication clean ups.\n  * Got rid of unused File Descriptor and other MemLock clean ups.\n  * Added STORAGE_SPACE quota API (not enforced yet).\n\nRelease 1.9.18 on 07/20/2015\n* Various BnP improvements:\n  * Introduces of a limited form of High Availability for BnP.\n  * Improves path handling and validation in VoldemortSwapJob.\n  * Fetch throttling now works with block-level compression.\n  * Pushes to multiple colos now happen in parallel.\n  * Some code refactoring and clean ups.\n* Minor bug fixes in scripts:\n  * Fixes a SecurityException when running HadoopStoreJobRunner in Oozie.\n  * Fixes Coordinator start script.\n* EventThrottler code now uses Tehuti.\n\nRelease 1.9.17 on 06/12/2015\n* Robust handling of connection failures\n* Flush the queue on marking a node down\n* Async connect exceptions are remembered based on connection timeout\n\nRelease 1.9.16 on 06/09/2015\n* Fixes unregistration of JMX mbean.\n* Fixes VoldemortConfig bug introduced in 1.9.15.\n* Fixes error reporting in AvroUtils.getSchemaFromPath().\n* Caches the result of duplicate AvroUtils and HadoopUtils calls in BnP.\n\nRelease 1.9.15 on 06/05/2015\n* Unit tests for Serialization\n* Alert if BnP jobs does not progress\n* Add Timeout for ConfigurableSocketStoreFactory\n* HdfsCopyStats outputs object ids, not Hdfs Paths\n* HdfsCopyStatsTest fails intermittently\n\nRelease 1.9.14 on 05/22/2015\n* RO stats feature\n* HdfsFetcher refactoring\n* Hadoop Core and IO commons version upgrade \n* RO fetcher GC performance reduction by 90%\n* Fix quota redistribution on zone shrinkage issue\n* Admin command for meta get ro\n* Store delete command fix\n* Unit test fixes\n\nRelease 1.9.13 on 05/01/2015\n* Non blocking connect and Non blocking Protocol negotiation.\n* Suppress logging ObsoleteVersionExceptions in request handler.\n* Java minimum version is bumped to Java 6.\n\nRelease 1.9.12 on 05/01/2015\n* quota reset on rebalance\n* Fully disabled ant build\n* fix rebalance unit test intermittent failures\n* Changing buffer size of GZIP streams \n* Additional test cases\n\nRelease 1.9.11 on 04/23/2015\n* BuildAndPush job produces compressed files as dictated by Server\n* Voldemort RO Server supports on the fly decompression when compression is\n  enabled\n* Admin request to query Server on supported compression codec\n \nRelease 1.9.10 on 04/15/2015\n* Share Read/Write buffers for Client\n* Share Read/Write buffers for Server on Client Operations\n* Separate Client and Admin Request Handlers\n* Remove extra allocations on Server side while parsing the request\n* Pre-allocate Put request buffer before writing the request\n* Remove one extra byte array allocation on Put, when parsing\n    Vector clocks from the Value\n\nRelease 1.9.9 on 04/10/2015\n* Admin tools improvements and bug fixes:\n  * vadmin.sh now deals gracefully with inconsistent metadata and other issues.\n  * ReplaceNodeCLI now deals gracefully with empty stores.xml.\n  * ReadOnlyReplicationHelperCLI can now act on the local node, regardless of server version.\n* HdfsFetcher imprpvements:\n  * Can fetch individual files to specific local dir (when used from CLI).\n  * Lower memory consumption.\n* BnP improvements:\n  * Added a min.number.of.records config (defaults to 1) to prevent pushing empty stores.\n  * Improved error handling and reporting in BnP's run function.\n* Client-related automated tests expanded and improved.\n* Metadata queries no longer always hit the same node.\n* Initial release of the RocksDB storage engine! (N.B.: still considered experimental).\n\nRelease 1.9.8 on 03/17/2015\n* Fixes a bug in the vadmin restore-from-replica command.\n* Improves the vadmin meta check command.\n* Improves the build's tar target.\n* Adds better logging in the expansion code.\n* Cleans up BnP:\n  * Removes Azkaban dependency as much as possible.\n  * Standardizes on using voldemort.utils.Props as much as possible and adds more utility functions to it.\n  * Deletes VoldemortMultiStoreBuildAndPushJob which is not actively used and suffering from code rot.\n  * Adds Content-Length header support in BnP HttpHook.\n  * Adds safeguards in BnP HttpHook's concurrent code.\n  * Removes System.exit calls from BnP.\n* Cleans up dependencies:\n  * Removed tusk, RestHadoopFetcher and related classes and tests.\n  * Now fetching libthrift, catalina-ant and tehuti from Maven.\n\nRelease 1.9.7 on 03/04/2015\n* Improve Voldemort BuildAndPush\n* Fix bugs in FailureDetector and AdminConnectionVerifier for slop streaming\n* Fix log messages and English typo\n* Fix NIO client thread, selector and admin thread names\n\nRelease 1.9.6 on 02/13/2015\n\n* Bug fix for diverging metadata versions.\n* IntelliJ support in gradle build.\n* Avoid extra allocation on AvroGenericVersioned.\n* Unit test fixes.\n* Dependency upgrades:\n  * Jackson moved to 1.9.13\n  * Azkaban moved to 2.5.0, fetched from Maven Central, and removed from private-lib.\n* Build and Push clean ups and improvements:\n  * Cleaned up config strings into constants.\n  * Sanity checks and error messages for bad avro configs.\n  * Removed proprietary LinkedIn monitoring code.\n  * Added open-source friendly hooks for running arbitrary code in BnP.\n\nRelease 1.9.5 on 01/12/2015\n\n* Version metadata key fix for incorrectly incrementing on each restarts\n* ReplaceNodeCLI fixes and adding post conditions.\n* Tests refactoring and a new class ClientTrafficGenerator for sending traffic\n* Fixed Cluster.equals to do the node state check\n* Enhanced meta check-version to do the auto analysis and give output\n* Readonly replication helper tool to output, to boot strap the dead node\n* Windows batch file equivalents for new command functionalities\n* DeleteKeysCLI tool to delete the serialized keys from Voldemort\n* Krati entries iterator bug fix\n\nRelease 1.9.4 on 12/08/2014\n\n* Add separate failure detector for slop pusher job so that metadata slop.streaming.enabled functions.\n* Rename StoreVerifier to ConnectionVerifier.\n* Create AdminConnectionVerifier and AdminSlopStreamingVerifier for failure detectors under OFFLINE_SERVER.\n\nRelease 1.9.3 on 11/25/2014\n\n* Add support for disabling the client traffic (Offline server) to do maintenance on the server.\n* Add support for enabling/disabling slop streaming, partition streaming, ro fetch independently.\n* VAdmin support for offline server and other switches.\n* ReplaceNodeCLI command to swap a node in the cluster with other voldemort node.\n* Additional logging for failure detector.\n\nRelease 1.9.2 on 10/07/2014\n\n* Fix Coordinator related tests\n\nRelease 1.9.1 on 10/03/2014\n\n* Admin service and client for Coordinator\n* Some refactoring\n* Fix getmetadata shell command\n* Fix origin time in coordinator rest request\n\nRelease 1.9.0 on 09/22/2014\n\n* Updating release notes for 1.9.0\n\nRelease 1.8.16 on 09/04/2014\n\n* QuotaException causes delete failure.\n* bdb native backup fixups\n\nRelease 1.8.15 on 09/02/2014\n\n* Fixed double-counting stat bug.\n* Merged GET/GET_ALL into READ quotas and PUT/DELETE into WRITE quotas.\n* Cleaned up the slop delete code.\n* Fixed the C++ build so it works on more recent OS.\n\nRelease 1.8.14 on 08/26/2014\n\n* Tehuti 0.5 that includes a fix about Histograms\n\nRelease 1.8.13 on 08/25/2014\n\n* Fixes duplicated error message for coordinator\n* Adds zone checks\n* Makes tools that depend on zone 0 to be available\n* Polishes Java client example\n* Makes delete op use getVersion instead of get\n\nRelease 1.8.12 on 08/11/2014\n\n* Speeds up vector clock serialization.\n* Adds a clean up task in the Gradle build that fixes an intermittent issue with builds on mac.\n* Expands test configurations.\n* Some fixes and clean ups in the C++ client.\n* Rewrites RequestCounter so that it leverages the Tehuti metrics lib.\n\nRelease 1.8.11 on 08/01/2014\n\n* Expose http decoder parameters as config\n* Client Parallel request Exceptions are not reported to failure detector\n* ClientRequest Executor has infinite timeout and log cleanup\n* Threshold failure detector bug fixes\n\nRelease 1.8.10 on 07/23/2014\n\n* Registering slops for parallel puts that fail on QuotaExceededException\n\nRelease 1.8.9 on 07/02/2014\n\n* Adding backwards compatibility checks in AdminClient\n\nRelease 1.8.8 on 06/24/2014\n\n* Wrap mime multipart into a mime message to ensure headers are updated in\n  REST ResponseSenders\n* Fix transport client instantiation and HttpClientFactory shutdown\n* Fix build.gradle for setting java builder property in .project file\n\nRelease 1.8.6 on 06/11/2014\n\n* Fix related to Quota metrics (store level instead of aggregate)\n\nRelease 1.8.6 on 06/11/2014\n\n* Fix related to quotas - checking per store instead of aggregate\n\nRelease 1.8.5 on 06/06/2014\n*Gradle related changes\n    - empty settings file\n    - empty voldemort-contrib directory\n    - gradle wrapper to fetch required version\n\nRelease 1.8.4 on 06/03/2014\n\n* Fix RESTClientFactory for a clean shutdown\n* Add check for SocketAndStreams in sendAndReceive\n* Added the ability to query a single store using AdminClient / admin tool\n\nRelease 1.8.3 on 05/27/2014\n\n* Add debug log messages for CoordinatorService\n* Fix NetworkClassLoader bug when running in Windows OS (Carlos Tasada)\n* Add junit support for gradle\n* Fix bind failures while running tests\n* Add overwrite option to fork lift tool\n* Use standard logger in the StreamingClient constructor\n* [python client] Fixed reconnect in _execute_request (S√∏ren Holbech)\n* Fix the admin metadata check command and did same change for old admin command.\n\nRelease 1.8.2 on 05/09/2014\n\n* Bug fix in MetadataStore regarding updation of routing strategy\n\nRelease 1.8.1 on 05/01/2014\n\n* Fix AbstractStoreClientFactory for backwards compatibility\n* Cleaned up logging messages\n* Unit tests for big Binary data\n* Refactored voldemort admin tool - Phase 1 and Phase 2\n\nRelease 1.8.0 on 04/21/2014\n\n* Split the stores.xml into individual stores on Voldemort server\n* Maintaing backwards compatibility of MetadataStore\n* DefaultStoreClient bootstraps using store name (instead of stores.xml)\n* New Admin tool functionality to update stores (instead of replace)\n\nRelease 1.7.3 on 04/16/2014\n\n* Fix RESTClientFactory to return a LazyStoreClient\n\nRelease 1.7.2 on 04/15/2014\n\n* Quota limiting related fix\n* Coordinator related fix\n\nRelease 1.7.1 on 04/10/2014\n\n* Same as 1.7.0\n\nRelease 1.7.0 on 04/09/2014\n\n* Improves BDB coversion process and documentations\n* Adds scala build support and shell (Saurabh Bhatia)\n* Add removeStorageEngine call to StorageConfiguration\n\nRelease 1.6.9 on 03/19/2014\n\n* Fix a big where StreamingClient will throw NPE if not initialized\n* Fix multiple broken tests\n* Modified RO Rebalance Test to cover one more case\n* Adding thin client shell for Voldemort Coordinator\n* Move to tusk-0.0.2\n\nRelease 1.6.8 on 02/21/2014\n\n* Add a tool to dump BDB data to Text and the reverse\n* Fix racing condition problem in CachingStoreClientFactory\n* Testing, client shell and admin tool enhancements from suletm,\n  ahimta, bhsaurabh and Holden Caufield\n* ConsistencyCheckTool enhancement\n* Fix bug in Coordinator for enabling different startup ports\n\nRelease 1.6.7 on 02/07/2014\n\n* Updating the ivy file to refer to tusk-0.0.2 (build fix for nuage)\n\nRelease 1.6.6 on 02/07/2014\n\n* Quota management in Voldemort\n* Creation of read only stores in RO2 format\n* Updation of query key feature in admin tool\n\nRelease 1.6.5 on 02/04/2014\n\n* Adding AdminClient functionality to update store definitions\n\nRelease 1.6.4 on 02/03/2014\n\n* Fix bug when RO server NPE when rolled back\n* RO bug fix caused by RW optimization code\n\nRelease 1.6.3 on 01/24/2014\n\n* Fix KeyVersionFetcher to not depend on nodeid 0\n\nRelease 1.6.2 on 01/10/2014\n\n* Slops stored for nodes no longer in cluster, will die automatically\n* Admin command to selectively purge slops destined to nodes/zones/stores\n* Admin command to atomically update cluster.xml, stores.xml in one go\n* Added admin command show-routing-plan to trace routing table for a key\n* ZoneClipperCLI also adjusts store definitions based on dropped zone\n* Numerous tests around non contiguous node ids/zone ids\n\nRelease 1.6.1 on 12/13/2013\n\n* Avro support for fetch keys/entries admin command\n* Store Clients are now cached by default, within the same factory\n* Rigoruous tests around non contiguous node ids/zone ids\n* ZenStoreClient efficient resource managerment\n\n\nRelease 1.6.0 on 11/26/2013\n\nNOTE: This is an open source release! This release can be downloaded here:\n      https://github.com/voldemort/voldemort/releases\n\nChanges made since 1.5.9:\n\n* Fixes to remove dependency on non contiguous node ids\n* Stats for store client\n\n\nRelease 1.5.9 on 11/05/2013\n\n* Bug fix in AsyncMetadataVersionManager (related to store version tracking)\n\nRelease 1.5.8 on 10/29/2013\n\n* Replace resthdfs jar with tusk.jar\n* Incorporate retries into rest hdfs fetcher\n\nRelease 1.5.7 on 10/28/2013\n\n* Voldemort now uses BDB 5.0.88\n* Support for RO fetches via RestHdfs\n* Fix NPE in Streaming Client\n* Retrying fetches on all exceptions w/ delay\n\nRelease 1.5.6 on 10/18/2013\n\n* Removed getStoreClientFactoryStats from StoreClientFactory interface (fix)\n\nRelease 1.5.5 on 10/18/2013\n\n* Client shell with avro support\n* Fix bind exceptions\n* Fix ObsoleteVersion Exceptions\n* Build once and push to multiple clusters\n* Improvements to failure detector\n* Fix parallel puts\n* Prevent NPE in worker thread when shutdown\n* Bug fixes in repair and prune job\n\nRelease 1.5.3 on 09/19/2013\n\n* Bug fixes (NPE during shutdown\n* Client side visibility changes (Histogram, Connect exception)\n* Adding PruneJob and timebased resolving mechanism to streaming client\n* R2Store cleanup required for the rest client\n* Monitoring improvements (aggregated BDB stats, streaming stats)\n* Monitoring and Config improvements for REST service\n* Removed donor based rebalancing code\n* Removed replica type from the code base\n* Improvements to Rebalance controller\n\nRelease 1.4.7 on 09/04/13\n\nChanges\n* Added the Zone affinity feature on the client side\n* Refactoring the rest package (Rest server and Coordinator)\n* Using standard Netty pipeline in the Coordinator\n* Added monitoring hooks for the Rest Service\n\nRelease 1.4.6 on 07/24/2013\n\nChanges\n* Cleanup of R2Store to work with the REST server\n* Modified ZenStoreClient to avoid creating SystemStores during each\n* re-bootstrap\n* Added slop streaming functionality\n\nRelease 1.4.4 on 07/08/2013\n\nChanges made since 1.4.3\n* Change Repartioner to swap among nodes from all specified zones\n* Added rebalance shuffle, cluster expansion scripts\n\nRelease 1.4.3 on 07/03/2013\n\nNote: Server changes are not backwards compatible. To use new\nrebalancing tooling, servers must be upgraded before hand.\n\n* Rebalance features\n  - Proxying / abortable rebalance\n    - added proxy puts which rebalance safely abortable\n    - improved proxy gets to check locally first and only fetch remote\n      if not yet local\n  - Repartitioning: improved algorithms to optimize partition layout\n  - Plan: avoids cross-zone data movement in most rebalance cases. Only\n    expanding into a new zone requires data movement across zones.\n  - Controller\n    - better / more accurate progress monitoring.\n    - 'proxy pause' before rebalancing. Ensures clients pick up new\n      metadata and allows performance with proxy'ing to be observed\n      before servers start rebalancing.\n  - NOTE: Donor-based rebalance is currently broken by these changes.\n* Rebalance tooling\n  - PartitionBalanceCLI: more nuanced and verbose analysis of cluster\n    balance\n  - RepartitionerCLI: Stand alone tool to determine repartitioning\n  - Stand alone 'repartitioning' scripts for key use cases: new cluster,\n    shuffle (existing cluster), cluster expansion, and zone expansion.\n  - RebalancePlanCLI: Stand alone plan tool to determine cost of a\n    specific rebalance\n  - RebalanceControllerCLI: Stand-alone tool for executing a specific\n    rebalance (plan)\n* Administrivia\n  - java 7 compilation\n  - moved to Guava from Google Collections\n  - better unit test coverage of zoned clusters\n  - added example prod configuration\n  - added tools directory\n  - .gitignore improvements\n* Bug fixes\n  - fixed concurrency bugs in metadata store\n  - fixed vector clock comparisons\n  - fixed many NPEs\n  - write slops in more cases when they are needed\n  - KeySampler/Fetcher handle larger working sets with OOME\n\nChanges made since 1.4.2\n* Minor code fixes\n* Faster creation of BaseStoreRoutingPlan objects in the fast rebalancing path\n\nRelease 1.4.2 on 06/27/2013\n* Fixing costly StoreRoutingPlan object construction\n\nRelease 1.4.0 on 06/21/2013\n* Zone expansion release\n\nRelease 1.3.4 on 06/19/2013\n* Read Write store bug fixes\n  - Rewrite of InMemoryStorageEngine + config to control multiVersionPuts\n  - Fixed a bug in the read-repair logic which was causing unnecessary puts.\n  - Fixed put operation in PipelineRoutedStore to ensure slop is submitted\n  - Fixed VectorClock bug triggered during versioned puts\n* Rebalance improvement\n  - Made it safe to abort rebalance\n  - Proxy puts are established so that 'old' partition is updated and so an aborted rebalance can \"roll back\" without any data loss\n  - Proxy gets & puts are established within the zone making improving performance during rebalance\n* Coordinator\n  - Added monitoring capability in the Coordinator\n\nRelease 1.3.3 on 04/24/2013\n* VoldemortBuildandPush\n  - Fixed bug with schema check\n* Streaming Client\n  - Fixed issue with redundant callback invocation\nRelease 1.3.1 on 03/25/2013\n* HDFSFetcher\n  - Fixed the bug in calculating checksums when we entere a retry loop\n  - refactored per file checksums\n  - added junit test case to simulate intermittent IO exceptions\n* voldemort.client.protocol.admin.AdminClient\n  - Added AdminStoreClient so that AdminClient can do store operations\n    against specific store on a specific node.\n  - Added helper methods for diong put & get for specific node/store\n  - Added voldemort.client.protocol.admin.QueryKeyResult type to\n    simplify QueryKey interface\n* Improved FetchStreamRequestHandler and sub-classes\n  - Renamed all sub-classes: 'FullScan' and 'PartitionScan' prefixes\n    for pertinent stream request handler implementations.\n  - Removed unused skipRecords parameter.\n  - Added recordsPerPartition parameter to fetch limited portion of\n    each partition.\n  - All logic about how many keys to fetch (return to invoker) is\n    server side now.\n* RebalanceCLI\n  - Added many options to help improve the balance of (zoned) clusters.\n  - Analysis of the balance of a cluster is significantly more detailed.\n  - Fixed a bug that reduced the balance of a cluster each time it was\n    expanded.\n  - Many different algorithms for improving cluster balance are\n    implemented in voldemort.utils.RebalanceClusterUtils\n* ConsistencyCheck & ConsistencyFixCLI\n  - New tools for ensuring data durability. These tools are necessary\n    because slop creation can fail during put operations.\n  - ConsistencyCheck determines which keys, if any, lack\n    \"consistency\". I.e., are present on only a subset of the expected\n    partitions.\n  - ConsistencyFix takes a list of bad (inconsistent) keys and makes\n    sure they are present on all expected partitions.\n  - ConsistencyFix also has an interface for repairing \"orphaned\" keys\n    that could result from an aborted rebalance.\n* KeySamplerCLI & KeyVersionFetcherCLI\n  - KeySamplerCLI is a new tool that reads some number of keys for\n    specified partitions/stores.\n  - KeyVersionFetcherCLI is a new tool that, given a key and a store,\n    fetches the version from all nodes that host a partition that\n    ought to store a replica of the key's value.\n  - Together, KeySamplerCLI and KeyVersionFetcherCLI correctly\n    implement the intended functionality of the Entropy tool (for\n    servers that implement either FullScan and PartitionScan fetches).\n  - Entropy tool had been used in the past to verify a sample of keys\n    before and after a rebalance. Entropy tool does not work as\n    intended/expected. This is exacerbated by the partition aware\n    layouts. Instead of trying to fix the Entropy tool, these two new\n    tools were developed. Entropy is deprecated and will eventually be\n    removed from the code base.\n* Substantial refactoring of helper & util methods:\n  - voldemort.cluster.Cluster : added helper methods\n  - voldemort.utils.ClusterInstance : wraps up one Cluster &\n    List<StoreDefinition>\n  - voldemort.utils.Cluster : utils for single Cluster object.\n  - voldemort.utils.NodeUtils : utils for Node object.\n  - voldemort.utils.RebalanceUtils : Many methods moved to more\n    appropriate helper classes\n  - voldemort.utils.StoreDefinitionUtils : utils for StoreDefinition\n    object.\n  - voldemort.utils.StoreInstance : wraps up one Cluster & one\n    StoreDefinition\n* Et cetera\n  - ByteUtils toHexString & from HexString now rely on standard\n    libraries\n  - voldemort.client.AdminFetchTest now tests FullScan and\n    PartitionScan fetches\n  - voldemort.store.routed.ReadRepairerTest annotated all tests with\n    @Test\n\n\nRelease 1.3.0 on 03/08/2013\n\nNOTE: This is an open source release! This release can be downloaded here:\n      http://github.com/voldemort/voldemort/downloads.\n\nChanges made since 1.2.3\n* VoldemortConfig and ClientConfig now contain detailed documentation\n* BDB-JE defaults set to ones in prod@linkedin\n* Bug fixes on kerberos support for Hadoop\n\n\nRelease 1.2.3 on 02/20/2013\n\nChanges made since 1.2.2\n* Added a retry loop and synchronized block while getting Hadoop FS\n* Code cleanup in HdfsFetcher to make it more readable.\n* Throwing explicit exceptions in HdfsFetcher instead of\n  returning null to be more precise in the Azkaban logs.\n\n\nRelease 1.2.2 on 02/19/2013\n\nChanges made since 1.2.1\n* Synchronized the streaming API\n* Fixed some of the streaming API tests.\n\n\nRelease 1.2.1 on 0/30/2013\n\nChanges made since 1.2.0\n* Added a Streaming API and related tests.\n* Refactoring of the admin client apis into functional inner classes\n\n\nRelease 1.2.0 on 01/21/2013\n\nChanges made since 1.1.9\n* Added an Admin API to fetch orphaned key / entries\n* Improved some tests related to streaming API.\n* Correcting commons-codec version in ivy file (1.4)\n\n\nRelease 1.1.9 on 01/15/2013\n\nChanges made since 1.1.8\n* Asynchronous socket checkout improvements\n  * Changed checkout behavior of KeyedResourcePool to only create new\n    connections when there are no resources available (rather than\n    creating new connections until the pool is full)\n  * Changed QueuedKeyedResourcePool.reset behavior to better match\n    KeyedResourcePool (i.e., to not cancel queued asynchronous\n    requests unnecessarily)\n  * Removed (unnecessary) synchronization primitives from keyed resource pool\n  * Reduce granularity of failure detector locking within ThresholdFailureDetector\n* Minor features/improvements\n  * Less verbose logging in the face of expected exceptions and errors\n  * Refactored (Queued)KeyedResourcePoolTest\n* Bug fixes\n  * Fixed possible race condition for resource creation in KeyedResourcePool\n  * More efficient (time & space) and simpler Histogram implementation\n    with improved tests\n\n\nRelease 1.1.8 on 01/14/2013\n\nChanges made since release 1.1.7\n* Enhanced Server Monitoring\n   -- Server NIO layer\n   -- Streaming operations to the server\n   -- BDB storage exception counts\n* Ability to turn off BDB Checkpointing during batch modifications\n* Added ability to delete old checksum files in Build and Push reducer\n* Upgrade Hadoop jar to 1.0.4-p2\n\n\nRelease 1.1.7 on 01/03/2013\n\nNOTE: This release is based off of release 1.1.4\n\nChanges made since release 1.1.4\n* Upgrading Hadoop jar to 1.0.2\n* Added support for Kerberos authentication in HdfsFetcher\n* Extra config parameters for Kerberos config and keytab file\n\n\nNOTE: Release 1.1.5 and 1.1.6 are special client side releases\nnot based off of master. 1.1.5 was rolled back to to a weird bug.\n1.1.6 is a special client side release including Auto-\nbootstrapper and Versioned Avro support.\n\n\nRelease 1.1.4 on 11/29/2012\n\nChanges made since release 1.1.3\n* Added BDB parameters to control LRU behavior in cache & proactive cleaner migration\n* Added a mlock fix for pinning the indexes of RO stores in memory\n\n\nRelease 1.1.3 on 11/28/2012\n\nChanges made since release 1.1.2\n* Fixed a bug in the build and push job, specifically the Mapper\n  that caused collisions\n* Added retry mechanism with the HDFS fetcher for hftp\n\n\nRelease 1.1.2 on 10/31/2012\n\nChanges made since release 1.1.1\n* Reverted a change to voldemort.versioning.Versioned.getVersion() so\n  that a Version is returned as our clients expect.\n\n\nRelease 1.1.1 on 10/30/2012\n\nChanges made since release 1.1.0\n* Fixed connection leak in ClientRequestExecutorFactory\n* Changed client to default to DefaultStoreClient\n\n\nRelease 1.1.0 on 10/19/2012\n\nChanges made since release 1.0.0\n\nIMPORTANT NOTE : This release has significant changes to the BDB storage layer.\nUsers are required to read the bin/PREUPGRADE_FOR_1_1_X_README file\nthoroughly before attempting to upgrade to 1.1.0. The necessary data\nconversion will be done through bin/voldemort-convert-bdb.sh\n\n* Upgrading to JE 4.1.17\n* New data format that handles conflicting updates in Voldemort more\n  efficiently\n* Move data off heap and only use it for Index\n* When scanning, evict whatever you bring in right away.\n* Partition based scan api to dramatically speed up rebalancing & restore\n  using Partition aware scans (you exactly scan whatever you want to fetch)\n* Flexible knobs to control scheduling of DataCleanupJob\n\n\nRelease 1.0.0 on 10/17/2012\n\nNOTE: The large version number jump from 0.96 to 1.0.0 is to\nstandardize on a version number of the sort MAJOR.MINOR.PATCH.  This\nchange is part of our effort to treat internal and open source\nreleases in a much more similar manner. Along these lines, release\nnotes for internal releases (like this one) are committed on the\nmaster branch. We hope this improves transparency as we work towards\nthe next open source release.\n\nChanges made since release 0.96\n\n* Auto bootstrapping: ZenStoreClient and System stores\n  * Added server side system stores for managing metadata\n  * ZenStoreClient interacts with system stores\n  * ZenStoreClient auto bootstraps whenever cluster.xml or stores.xml changes\n  * Added a new routing strategy to route to all with local preference\n  * Added a client-registry for publishing client info and config values\n  * Updated LazyClientStore to try to bootstrap during Init\n  * Modified Failure Detector to work on a shared cluster object reference\n* Avro: schema evolution and read only support\n  * Added new Avro serializer type that supports schema evolution\n  * Added Avro support to read only stores\n  * Added LinkedIn build-and-push Azkaban jobs to build read only stores to contrib\n  * Added a schema backwards compatibility check to VoldemortAdminTool and on server startup to prevent mishaps due to bad schemas\n* Non-blocking IO: Fixed design flaw that blocked in the face of slow servers\n  * Asynchronous operations no longer do a blocking checkout to get a SocketDestination\n  * Added additional stats collection for better visibility into request queues\n* Minor features\n  * Enhanced VoldemortAdminTool to update store metadata version\n  * Enhanced VoldemortAdminTool to work with the new system stores\n  * Added feature to voldemort-shell.sh to dump byte & object arrays\n  * Added a SlowStorageEngine for testing degraded mode performance\n  * Added mechanism to isolate BDB cache usage among stores\n  * Enhanced debug logging (for traffic analysis).\n  * Python client bug fixes (from pull request)\n  * Improved messages in request tracing\n  * Cleaned up help/usage messages within the client shell\n  * Added server config to control socket backlog\n  * Added \"--query-keys\" option to query multiple keys of multiple stores from specific node\n  * Added control to DataCleanupJob Frequency\n  * Unified jmxid as the factory across the board\n* Tools\n  * bin/generate_cluster_xml.py to generate cluster.xml\n  * bin/repeat-junit.sh and bin/repeat-junit-test.sh to repeatedly run tests\n* Bug fixes\n  * Changed getall return behavior to comply with javadoc\n  * Fixed a bug that caused unnecessary serial requests in getall\n  * HFTP performance issue bug fix (fix in byte buffer and copy process)\n  * Fixed a bug that prevented \"--fetch-keys\" and \"--fetch-entries\" in admin tool from showing multiple store results\n  * Fixed problem in sample config that prevented the server from starting\n  * Fixed some intermittent BindException failures across many unit tests\n  * Fixed some intermittent rebalance test failures\n  * Wrapped long running tests with timeouts\n\n\nRelease 0.96 on 09/05/2012\n\nChanges made since 0.90.1\n\n * Monitoring:\n     * Append cluster name to various mbeans for better stats display\n     * Implement average throughput in bytes\n     * Add BDB JE stats\n     * Add 95th and 99th latency tracking\n     * Add stats for ClientRequestExecutorPool\n     * Add error/exception count and max getall count\n     * BDB+ Data cleanup Monitoring changes\n * Rebalancing:\n     * Donor-based rebalancing and post cleanup (see https://github.com/voldemort/voldemort/wiki/Voldemort-Donor-Based-Rebalancing for more details)\n     * Rebalancing integration testing framework (under test/integration/voldemort/rebalance/)\n     * Generate multiple cluster.xml files based on the number specified when running the tool and choose the cluster with the smallest std dev as the final-cluster.xml\n     * Add status output to log for updateEntries (used by rebalancing)\n * Read-only pipeline:\n     * Add hftp and webhdfs support\n     * Read-only bandwidth dynamic throttler\n     * Add minimum throttle limit per store\n     * Add rollback capability to the Admin tool\n * Voldemort-backed stack and index linked list impl\n * Change client requests to not process responses after timeout\n * Modified client request executor timeout to not factor in the NIO selector timeout\n * Added BDB native backup capabalities, checksum verification and incremental backups (well tested, but not yet used in production)\n * Add additional client-side tracing for debugging and consistency analytics\n * Clean up logging during exception at client-side\n * Security exception handling\n * Add snappy to CompressionStrategyFactory\n * Add configurable option to interrupt service being unscheduled\n * Add logging support for tracking ScanPermit owners (for debugging purposes)\n * Add a jmx terminate operation for async jobs\n * Add zone option for restore from replicas\n * Changing the enable.nio.connector to true by default\n * Better disconnection handling for python client\n * Split junit tests into a long and a short test suites\n * Add separate timeouts for different operations (put, get, delete, and getAll\n * Allow getAll to return partial results upon timeout\n * Improved cluster generation tool\n * Added log4j properties folder for junit test\n * Bug fixes:\n     * httpclient 3.x to httpclient 4.x\n     * Fix NPE in listing read-only store versions\n     * Fixed 2 failure detector bugs during rebalancing or node swapping\n     * Fixed a thread leak issue in StreamingSlopPusher\n     * Fixed a NIO bug\n     * Fixed a bug in TimeBasedInconsistency resolver.\n     * Fixed race condition in client socket close\n     * Fixed a potential deadlock issue in ScanPermitWrapper\n     * Fixed a bug where a read returns null (on rare occations) when being concurrent with a write\n     * Fixed a performance bug in HdfsFetcher when hftp is used\n\n\nChanges made since 0.90\n\n * Updated the documentation for Voldemort shell tool in NOTES\n * Added Admin API to perform Bdb data cleanup (repairJob)\n   and corresponding unit tests\n * Fixes in restore from replication, store creation code\n * Improved failure detector configuration. ThresholdFailureDetector\n   is now the default option\n * Multiple Fixes to the Voldemort ruby client\n * Added additional Jmx metrics to expose Bdb environment statistics,\n   caching statistics and Voldemort batch operation statistics\n * Updated default timeout for restore 'from replica' to 365 days\n * New feature in the performance tool: '--use-sample' option enables\n   'read, write back unmodified' transactions in place of writes in\n   the workload (allows for testing read-write transactions on stores\n   with complex schemas)\n * Added the ability to dynamically update cluster.xml and\n   reinitialize the scheduler\n\nRelease 0.90 on 7/10/2011\n\nChanges made since 0.81\n * All upgrading instructions can be found here - https://github.com/voldemort/voldemort/wiki/Upgrading-from-0.81\n * Tooling\n - Single consolidated administrative tool - Got rid of the admin client\n   shell and have a better Voldemort admin tool. More documentation -\n   https://github.com/voldemort/voldemort/wiki/Voldemort-Admin-tool\n\n * Web manager\n - Basic GUI in JRuby ( and Sinatra ), capable of querying for store metadata.\n - Read contrib/web-manager/README.md for more details\n\n * Rebalancing\n - New better rebalancing support with (a) checkpointing (b) progress bar (c) support for RO store rebalancing\n - Documentation - https://github.com/voldemort/voldemort/wiki/Voldemort-Rebalancing\n - In the process solves Issue 203, 288, 305, 307, 311\n\n * Client side changes\n - Pipeline routed store ( with support for topology awareness ) - Changed the default client side routing\n   to use a state machine like system. More documentation - https://github.com/voldemort/voldemort/wiki/RoutedStore-redesign\n - Lazy store client - Some of our clients are very inactive and don't really do many requests. For such\n   clients it doesn't make sense to bootstrap the metadata at startup. Hence from now onwards clients instantiated\n   from SocketStoreClientFactory give a LazyStoreClient which will not bootstrap till the first request is made\n - Caching store client factory - We have also checked-in a new store client factory which we use internally\n   at LinkedIn to cache store clients for stores which we repeatedly.\n - Failure detector - We have fixed some bugs in the ThresholdFailureDetector. This is an improvement over the naive\n   BannageFailureDetector which would aggressively mark nodes down after a single failure. More details -\n   https://github.com/voldemort/voldemort/wiki/Client-side-failure-detector-implementations\n - Issue 219: StoreClient::put returns a Version - You may have to upgrade your clients since now the StoreClient returns\n   a version to be used in successive requests.\n\n * Read-only store changes\n - Admin based store swapper - Till 0.81 we relied on a servlet based fetcher / swapper.\n   Now we support an admin based store swapper which reports progress.\n - Other changes - (a) New directory format (b) Two new data format. Support for iterating over read-only data\n   (c) Checksum of data in .metadata file (d) Some monitoring changes of RO stores\n\n * Jar upgrades\n - Introduction of JNA 3.2.7 - We have introduced JNA 3.2.7 in this release for supporting symbolic links in read-only stores.\n   More details about the new RO format and changes can be found here - https://github.com/voldemort/voldemort/wiki/Upgrading-from-0.81\n - Protocol Buffers 2.3.0 - Upgrading protocol buffers from 2.2.0 to 2.3.0\n - Avro 1.4.0 - Upgraded Avro from 1.3.0 to 1.4.0\n\n * Storage engine\n - Added support for Krati 0.3.6 as a storage engine ( http://sna-projects.com/krati/ ) -\n   https://github.com/voldemort/voldemort/tree/release-090/contrib/krati\n\n * Core features\n - Hinted handoff - https://github.com/voldemort/voldemort/wiki/Hinted-Handoff\n - Experimental support for server side transforms - https://github.com/voldemort/voldemort/wiki/Server-side-transforms-in-Voldemort\n - Topology awareness ( i.e. datacenter / rack ) - https://github.com/voldemort/voldemort/wiki/Topology-awareness-capability\n - Repair Job ( Job which will delete data if the node is not responsible for it )\n\n * Better monitoring\n - Tons of JMX changes ( average bytes, etc ) - More details ( https://github.com/voldemort/voldemort/wiki/JMX-Monitoring )\n - Key distribution generator - Ability to estimate skew of your cluster ( ./bin/run-class.sh voldemort.utils.KeyDistributionGenerator )\n\n * Clients\n - Python - Updated Python client with support for Binary JSON serialized stores ( ./clients/python/README )\n - Ruby - Updated Ruby client ( ./clients/ruby/README.md )\n\nRelease 0.81 on 6/15/2010\n\nChanges made since 0.80.2:\n\n* IMPORTANT: we have upgraded the Hadoop Core jar to v0.20.2. Since\n  this version of Hadoop requires Java 6, in order to retain backwards\n  compatibility with Java 5, you will need to replace this jar with\n  the Hadoop 0.18.* core jar.\n* Multiple donors for Voldemort Rebalancing: speeds up rebalancing, by\n  allowing a single stealer node to transfer partitions from multiple\n  nodes\n* Features for EC2 Testing\n* Read-only Stores: backwards compatibility with 0.70, bug fix in\n  Checksum code\n* Hadoop InputFormat, Pig LoadFunc in Contrib: ability to perform\n  Map/Reduce (either directly via Hadoop or using Pig) over data in\n  Voldemort stores. See:\n  < contrib/hadoop/test/voldemort/hadoop/VoldemortWordCount.java > for\n  an example.\n* Voldemort Performance Tool: performance measurement tool based on\n  YCSB (Yahoo Cloud Serving Benchmark) code. Run\n  < bin/voldemort-performance-tool.sh --help > for more information.\n* Reverted default failure detector implementation back to\n  BannagedPeriodFailureDetector due to potential bugs in the\n  ThresholdFailureDetector\n\nRelease 0.80.2 on 4/27/2010\n\nChanges made since 0.80.1:\n\n* Batched NIO writes (improves performance for AdminClient/Streaming\n  operations when the NIO connector is enabled)\n* VoldemortAdminTool: Added support to specify stores, support to\n  fetch all keys to a binary file, support to save keys in ASCII\n  (JSON) format [experimental], capability to add stores. Run\n  < bin/voldemort-admin-tool.sh --help > for more information.\n* Minor bug fixes for Rebalancing\n* Issue 240: Voldemort fetcher should use different temp directories for\n  different stores\n* Checksum capability during construction of Voldemort read-only stores\n\nRelease 0.80.1 on 3/23/2010\n\nChanges made since 0.80:\n\n* Issue 133: Support for Apache Avro as a serialization format\n* Issue 223: Changed the default client to use\n  TresholdFailureDetector\n* Fixed issue 222: Revised KeyedResourcePool.close(K key) to fix\n  leaking sockets\n* Fixed issue 198: Revised ReadRepairer to use a separate copy of the\n  vector clock, fixing a situation where NoSuchElementException would\n  be thrown\n* Miscellaneous enhancement: support for TCP keep-alives, improved\n  read only store utilities, command line interface to AdminClient,\n  improved load testing tools\n\nRelease 0.80 on 2/18/2010\n\nChanges made since 0.70.1:\n\n* IMPORTANT: backwards compatibility between the client and server has\n  changed. A backwards incompatibility in the wire protocol was found\n  between releases 0.60 to 0.70.1 and releases prior to 0.60. We chose\n  to make 0.80 compatible with 0.57.1 and earlier versions, while\n  introducing an incompatibility with 0.60-0.70.1. What this means is\n  that if you're presently running 0.60 and higher, you would need to\n  upgrade the Voldemort jar files on *all* servers and clients.\n* Upgraded the BDB storage engine to use BerkeleyDB-JE 4.0.92,\n  retaining ability to use BerkeleyDB-JE 3.3.* if desired. IMPORTANT:\n  if one switches from BerkeleyDB-JE 3.3.* to 4.0.92 they will be able\n  to access all of existing data. Once a switch has been made to\n  4.0.92 the data will not be readable by earlier versions of BDB. If\n  there's a chance that a roll back to 3.3.* might be needed, the best\n  course of action will be to make a backup of existing data\n  prior to upgrading.\n  Switching between 3.3.* and 4.0.* would also require rebuilding the\n  class files (e.g., by running \"ant clean && ant release\" after\n  replacing the BDB jar files).\n* Compression support for read-only stores\n* Increased the socket buffer size for transferring read-only\n  stores from Hadoop for improved performance over high-latency links\n* NIO support for the Admin Service, including Streaming\n  Functionality\n* Support for adding stores on the fly via the Admin\n  Service\n* Fixed issue 209: Incorrect object passed to List.contains in\n  RebalanceUtils.getLatestCluster()\n* Fixed issue 211: Unnecessary read repairs during getAll() with more\n  than one key\n* Other enhancements: better CLI for rebalancing, throttling in\n  Admin Service is now based on all disk activity\n\nRelease 0.70.1 on 2/1/2010\n\nChanges made since 0.70:\n\n* Fixed issue 205: if no keys passed to getAll() were in partitions\n  undergoing rebalancing, proxyGetAll() would be called with an\n  empty list even if rebalancing wasn't happening\n\nRelease 0.70 on 1/27/2010\n\nChanges made since 0.60.1:\n\n* A beta of rebalancing (dynamic cluster expansion) support merged\n  into the main branch. See the project's wiki for more information:\n  http://wiki.github.com/voldemort/voldemort/voldemort-rebalancing\n* New failure detector merged into the main branch:\n  http://wiki.github.com/voldemort/voldemort/failure-detection\n* Beta mechanism for restoring all of node's data from replicas on\n  demand. This is an alternative to a more gradual mechanism provided\n  by read-repair: useful when a machine is down for a prolonged period\n  and is then re-inserted into the cluster.\n  Invoked via JMX: the operation is restoreDataFromReplication in the\n  voldemort.server.VoldemortServer MBean, with a mandatory parameter\n  (integer >= 1) indicating the number of transfers to do in parallel.\n* Simple gossip protocol (for cluster metadata) merged\n  into the main branch. Disabled by default: use \"enable.gossip=true\"\n  to enable, use \"gossip.interval.ms\" to set an interval at which gossip\n  occurs (default: 30000 i.e., 30 seconds).\n* Fixed issue 190: add a way of aggregating performance data over\n  all stores\n* Fixed issue 181: stack trackes shouldn't be filled for Obsolete\n  version exception\n\nRelease 0.60.1 on 12/18/2009\n\nMinor changes made since 0.60:\n\n* Better logging in the exception thrown if config/.temp and\n  config/.version are copied\n* Bumping up the version to 0.60.1 in order to release updated\n  archives, fixing an error in the stores.xml for single_node_cluster\n  sample config\n\nRelease 0.60 on 12/15/2009\n\nChanges made since 0.57.1:\n\n* Admin Client/Server API: adds support for streaming-based transfer\n  of entries between nodes, deleting entries on remote nodes,\n  remotely deleting and updating metadata\n* EC2 testing: a way to periodically run integration and performance\n  tests which involve Voldemort instances on different machines\n* Experimental support for views\n* Interpolation search for read-only stores\n* Support for large lists and strings in the JSON serializer\n* LZF compression support\n* Ruby client contributed\n* Fixed issue 170: hanging if a port is used by another process\n* Fixed issue 122: suspicious integer division in\n  RequestCounter.getThroughput\n* Miscellaneous improvements and bug fixes for read-only stores\n* Fixed issue 168: added StorageEngine.keys()\n\nRelease 0.57.1 on 11/27/2009\n\nMinor change made since 0.57:\n\n* Modified build.xml to exclude .git directory from release tarballs/zipfiles\n\nRelease 0.57 on 11/16/2009\n\nThe following changes were made since 0.56:\n\n* Fixed an issue in ReadOnlyEngine's close() method\n* Fixed hidden logging in StorageService\n* Fix for issue 163 (lock mode during get)\n* Exposed bdb environment stats with setFast(false)\n\nRelease 0.56 on 10/26/2009\n\nThe following changes were made since 0.55:\n\n* Fix for issue 164: Changed default bdb.max.logfile.size to 60MB\n* Make file deletes asynchronous in read only store swap\n* Added better debug logs for bdb stats\n* Fixed race condition in AbstractSocketPoolTest\n* Added improved monitoring for bdb stats\n* Added backoff and retry logic in bootstrap code\n* Not logging obsoleteVersionException(s) or counting it in JMX exception count\n* Fixed issue 159\n* C++ client building on OS X\n* Rely only on the number of versions returned to decide whether to retrieve\n  the value for put(K,V)\n* Implemented issue 152: getVersion() API in Store\n\nRelease 0.55 on 10/7/2009\n\nThe following changes were made since 0.52 (in summary):\n\n* Add an event throttler\n* Added DataCleanupJob\n* Protocol buffers at version 2.2.0\n* BDB JE upgraded to 3.3.87\n* Added data compression support (enable by adding\n    <compression>\n       <type>gzip</type>\n    </compression>\n  to value-serializer or key-serializer sections in stores.xml)\n* Added a resource pool/socket pool implementation (no longer using\n  commons-pool)\n* Added server-side NIO support (enabled by setting\n  enable.nio.connector=true\n  in server.properties)\n* Improved the efficiency of the protocol buffers network protocol by using\n  CodedOutputStream/CodedInputStream (zero copy transfers)\n* Fix for issue #21: incorrectness in vector clock inconsistency resolver\n* Upgrade google-collections\n* Support for building read only stores in Hadoop\n* Added a C++ client\n"
        },
        {
          "name": "settings.gradle",
          "type": "blob",
          "size": 0.630859375,
          "content": "rootProject.name = 'voldemort'\n\n// the following dance creates the subprojects (the contrib modules) and\n// fixes the naming so that eclipse doesn't explode\ninclude 'contrib-collections'\nproject(':contrib-collections').projectDir = file('contrib/collections')\n\ninclude 'contrib-hadoop-store-builder'\nproject(':contrib-hadoop-store-builder').projectDir = file('contrib/hadoop-store-builder')\n\ninclude 'contrib-krati'\nproject(':contrib-krati').projectDir = file('contrib/krati')\n\ninclude 'contrib-restclient'\nproject(':contrib-restclient').projectDir = file('contrib/restclient')\n\ninclude 'gobblin'\nproject(':gobblin').projectDir = file('gobblin')\n"
        },
        {
          "name": "src",
          "type": "tree",
          "content": null
        },
        {
          "name": "test",
          "type": "tree",
          "content": null
        },
        {
          "name": "tomcat-tasks.properties",
          "type": "blob",
          "size": 0.41015625,
          "content": "deploy=org.apache.catalina.ant.DeployTask\ninstall=org.apache.catalina.ant.InstallTask\nlist=org.apache.catalina.ant.ListTask\nreload=org.apache.catalina.ant.ReloadTask\nremove=org.apache.catalina.ant.RemoveTask\nresources=org.apache.catalina.ant.ResourcesTask\nroles=org.apache.catalina.ant.RolesTask\nstart=org.apache.catalina.ant.StartTask\nstop=org.apache.catalina.ant.StopTask\nundeploy=org.apache.catalina.ant.UndeployTask\n"
        },
        {
          "name": "voldemort-contrib",
          "type": "tree",
          "content": null
        },
        {
          "name": "voldemort-protobuf",
          "type": "tree",
          "content": null
        },
        {
          "name": "web.xml",
          "type": "blob",
          "size": 1.099609375,
          "content": "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<!DOCTYPE web-app PUBLIC \"-//Sun Microsystems, Inc.//DTD Web Application 2.3//EN\" \"http://java.sun.com/dtd/web-app_2_3.dtd\">\n\n<web-app>\n\n  <listener>\n    <listener-class>voldemort.server.http.VoldemortServletContextListener</listener-class>\n  </listener>\n\n  <servlet>\n    <servlet-name>store-servlet</servlet-name>\n    <servlet-class>voldemort.server.http.StoreServlet</servlet-class>\n  </servlet>\n  \n  <servlet>\n    <servlet-name>admin-servlet</servlet-name>\n    <servlet-class>voldemort.server.http.gui.AdminServlet</servlet-class>\n  </servlet>\n  \n  <servlet>\n    <servlet-name>query-servlet</servlet-name>\n    <servlet-class>voldemort.server.http.gui.QueryServlet</servlet-class>\n  </servlet>\n\n  <servlet-mapping>\n    <servlet-name>admin-servlet</servlet-name>\n    <url-pattern>/admin</url-pattern>\n  </servlet-mapping>\n  \n  <servlet-mapping>\n    <servlet-name>query-servlet</servlet-name>\n    <url-pattern>/query</url-pattern>\n  </servlet-mapping>\n\n  <servlet-mapping>\n    <servlet-name>store-servlet</servlet-name>\n    <url-pattern>/*</url-pattern>\n  </servlet-mapping>\n\n</web-app>\n"
        }
      ]
    }
  ]
}