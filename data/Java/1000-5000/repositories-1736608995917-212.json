{
  "metadata": {
    "timestamp": 1736608995917,
    "page": 212,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjIyMA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "AutoMQ/automq",
      "stars": 4005,
      "defaultBranch": "main",
      "files": [
        {
          "name": ".asf.yaml",
          "type": "blob",
          "size": 1.7958984375,
          "content": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nnotifications:\n  commits:      commits@kafka.apache.org\n  issues:       jira@kafka.apache.org\n  pullrequests: jira@kafka.apache.org\n  jira_options: link label\n\n# This list allows you to trigger builds on pull requests. It can have a maximum of 10 people.\n# https://cwiki.apache.org/confluence/pages/viewpage.action?spaceKey=INFRA&title=Git+-+.asf.yaml+features#Git.asf.yamlfeatures-JenkinsPRwhitelisting\njenkins:\n  github_whitelist:\n    - FrankYang0529\n    - kamalcph\n    - apoorvmittal10\n    - lianetm\n    - brandboat\n    - kirktrue\n    - nizhikov\n    - OmniaGM\n    - dongnuo123\n    - frankvicky\n\n# This list allows you to triage pull requests. It can have a maximum of 10 people.\n# https://cwiki.apache.org/confluence/pages/viewpage.action?spaceKey=INFRA&title=Git+-+.asf.yaml+features#Git.asf.yamlfeatures-AssigningexternalcollaboratorswiththetriageroleonGitHub\ngithub:\n  collaborators:\n    - FrankYang0529\n    - kamalcph\n    - apoorvmittal10\n    - lianetm\n    - brandboat\n    - kirktrue\n    - nizhikov\n    - OmniaGM\n    - dongnuo123\n    - frankvicky"
        },
        {
          "name": ".editorconfig",
          "type": "blob",
          "size": 0.28515625,
          "content": "# https://EditorConfig.org\n\nroot = true\n\n[*]\nindent_style = space\nindent_size = 4\nend_of_line = lf\ncharset = utf-8\ntrim_trailing_whitespace = true\ninsert_final_newline = true\n\n[*.md]\ntrim_trailing_whitespace = false\n\n[*.yml]\nindent_size = 2\n\n[*.scala]\nindent_size = 2\n\n[*.sh]\nindent_size = 2\n"
        },
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.7021484375,
          "content": "dist\n*classes\n*.class\ntarget/\nbuild/\nbuild_eclipse/\nout/\n.gradle/\n.vscode/\nlib_managed/\nsrc_managed/\nproject/boot/\nproject/plugins/project/\npatch-process/*\n.idea\n.svn\n.classpath\n/.metadata\n/.recommenders\n*~\n*#\n.#*\nrat.out\nTAGS\n*.iml\n.project\n.settings\n*.ipr\n*.iws\n.vagrant\nVagrantfile.local\n/logs\n.DS_Store\n\nconfig/server-*\nconfig/zookeeper-*\ngradle/wrapper/*.jar\ngradlew.bat\n\nresults\ntests/results\n.ducktape\ntests/.ducktape\ntests/venv\n.cache\n\ndocs/generated/\n\n.release-settings.json\n\nkafkatest.egg-info/\nsystest/\n*.swp\njmh-benchmarks/generated\njmh-benchmarks/src/main/generated\n**/.jqwik-database\n**/src/generated\n**/src/generated-test\n\nstorage/kafka-tiered-storage/\n\ndocker/test/report_*.html\nkafka.Kafka\n__pycache__\n"
        },
        {
          "name": ".idea",
          "type": "tree",
          "content": null
        },
        {
          "name": "CODE_OF_CONDUCT.md",
          "type": "blob",
          "size": 3.509765625,
          "content": "## Pledge\nIn the interest of fostering an open and welcoming environment, we as contributors and maintainers pledge to make participation in our project and our community a harassment-free experience for everyone, regardless of age, body size, disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, religion, or sexual identity and orientation.\n\n## Our Standards\nExamples of behavior that contributes to creating a positive environment include:\n- Using welcoming and inclusive language\n- Being respectful of differing viewpoints and experiences\n- Gracefully accepting constructive criticism\n- Focusing on what is best for the community\n- Showing empathy towards other community members\n- Respecting the work of others by recognizing acknowledgment/citation requests of original authors\n- Being explicit about how we want our own work to be cited or acknowledged\n\nExamples of unacceptable behavior by participants include:\n- The use of sexualized language or imagery and unwelcome sexual attention or advances\n- Sexist, racist, or otherwise exclusionary jokes\n- Trolling, insulting/derogatory comments, and personal or political attacks\n- Public or private harassment\n- Publishing others' private information, such as a physical or electronic address, without explicit permission\n- Other conduct which could reasonably be considered inappropriate in a professional setting\n\n## Our Responsibilities\nProject maintainers are responsible for clarifying the standards of acceptable behavior and are expected to take appropriate and fair corrective action in response to any instances of unacceptable behavior.\n\nProject maintainers have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, or to ban temporarily or permanently any contributor for other behaviors that they deem inappropriate, threatening, offensive, or harmful.\n\n## Enforcement\nInstances of abusive, harassing, or otherwise unacceptable behavior may be reported by contacting the project maintainers at AutoMQ. All complaints will be reviewed and investigated and will result in a response that is deemed necessary and appropriate to the circumstances. The project team is obligated to maintain confidentiality with regard to the reporter of an incident. Further details of specific enforcement policies may be posted separately.\n\nProject maintainers who do not follow or enforce the Code of Conduct in good faith may face temporary or permanent repercussions as determined by other members of the project’s leadership.\n\n## Scope\nThis Code of Conduct applies both within project spaces and in public spaces when an individual is representing the project or its community. Examples of representing a project or community include using an official project e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event. Representation of a project may be further defined and clarified by the Community Standards Committee.\n\nThis Code of Conduct is to be understood to apply in addition to any institutional codes of conduct governing individuals working within the project.\n\n## Attribution\nThis Code of Conduct is adapted from that of [Airbyte](https://github.com/airbytehq/airbyte), which is adapted from the Contributor Covenant, version 1.4, available at https://www.contributor-covenant.org/version/1/4/code-of-conduct.html\n\n\n"
        },
        {
          "name": "CONTRIBUTING_GUIDE.md",
          "type": "blob",
          "size": 5.740234375,
          "content": "# Contributing to AutoMQ\n\nThank you for your interest in contributing! We love community contributions.\nRead on to learn how to contribute to AutoMQ.\nWe appreciate first time contributors and we are happy to assist you in getting started. In case of questions, just\nreach out to us via [Wechat Group](https://www.automq.com/img/----------------------------1.png)\nor [Slack](https://join.slack.com/t/automq/shared_invite/zt-29h17vye9-thf31ebIVL9oXuRdACnOIA)!\n\nBefore getting started, please review AutoMQ's Code of Conduct. Everyone interacting in Slack or Wechat\nfollow [Code of Conduct](CODE_OF_CONDUCT.md).\n\n## Code Contributions\n\nMost of the issues open for contributions are tagged with 'good first issue.' To claim one, simply reply with 'pick up' in the issue and the AutoMQ maintainers will assign the issue to you. If you have any questions about the 'good first issue' please feel free to ask. We will do our best to clarify any doubts you may have.\nStart with\nthis [tagged good first issue](https://github.com/AutoMQ/automq-for-kafka/issues?q=is%3Aissue+is%3Aopen+label%3A%22good+first+issue%22)\n\nThe usual workflow of code contribution is:\n\n1. Fork the AutoMQ repository.\n2. Clone the repository locally.\n3. Create a branch for your feature/bug fix with the format `{YOUR_USERNAME}/{FEATURE/BUG}` (\n   e.g. `jdoe/source-stock-api-stream-fix`)\n4. Make and commit changes.\n5. Push your local branch to your fork.\n6. Submit a Pull Request so that we can review your changes.\n7. [Link an existing Issue](https://docs.github.com/en/issues/tracking-your-work-with-issues/linking-a-pull-request-to-an-issue)\n   that does not include the `needs triage` label to your Pull Request. A pull request without a linked issue will be\n   closed, otherwise.\n8. Write a PR title and description that follows the [Pull Request Template](PULL_REQUEST_TEMPLATE.md).\n9. An AutoMQ maintainer will trigger the CI tests for you and review the code.\n10. Review and respond to feedback and questions by AutoMQ maintainers.\n11. Merge the contribution.\n\nPull Request reviews are done on a regular basis.\n\n> [!NOTE] \n> Please make sure you respond to our feedback/questions and sign our CLA.\n>\n> Pull Requests without updates will be closed due inactivity.\n\n## Requirement\n\n| Requirement            | Version    |\n|------------------------|------------|\n| Compiling requirements | JDK 17     |\n| Compiling requirements | Scala 2.13 |\n| Running requirements   | JDK 17     |\n\n> Tips: You can refer the [document](https://www.scala-lang.org/download/2.13.12.html) to install Scala 2.13\n\n## Local Debug with IDEA\n\n### Gradle\n\nBuilding AutoMQ is the same as Apache Kafka. Kafka uses Gradle as its project management tool. The management of Gradle projects is based on scripts written in Groovy syntax, and within the Kafka project, the main project management configuration is found in the `build.gradle` file located in the root directory, which serves a similar function to the root POM in Maven projects. Gradle also supports configuring a `build.gradle` for each module separately, but Kafka does not do this; all modules are managed by the build.gradle file in the root directory.\n\nIt is not recommended to manually install Gradle. The gradlew script in the root directory will automatically download Gradle for you, and the version is also specified by the gradlew script.\n\n### Build\n```\n./gradlew jar -x test\n```\n\n### Prepare S3 service\nRefer to this [documentation](https://docs.localstack.cloud/getting-started/installation/) to install `localstack` to mock a local s3 service or use AWS S3 service directly. \n\nIf you are using localstack then create a bucket with the following command:\n```\naws s3api create-bucket --bucket ko3 --endpoint=http://127.0.0.1:4566\n```\n### Modify Configuration\n\nModify the `config/kraft/server.properties` file. The following settings need to be changed:\n\n```\ns3.endpoint=https://s3.amazonaws.com\n\n# The region of S3 service\n# For Aliyun, you have to set the region to aws-global. See https://www.alibabacloud.com/help/zh/oss/developer-reference/use-amazon-s3-sdks-to-access-oss.\ns3.region=us-east-1\n\n# The bucket of S3 service to store data\ns3.bucket=ko3\n```\n> Tips: If you're using localstack, make sure to set the s3.endpoint to http://127.0.0.1:4566, not localhost. Set the region to us-east-1. The bucket should match the one created earlier.\n\n### Format\nGenerated Cluster UUID:\n```\nKAFKA_CLUSTER_ID=\"$(bin/kafka-storage.sh random-uuid)\"\n```\nFormat Metadata Catalog:\n```\nbin/kafka-storage.sh format -t $KAFKA_CLUSTER_ID -c config/kraft/server.properties\n```\n### IDE Start Configuration\n| Item            | Value    |\n|------------------------|------------|\n| Main | core/src/main/scala/kafka/Kafka.scala     |\n| ClassPath | -cp kafka.core.main |\n| VM Options   | -Xmx1 -Xms1G -server -XX:+UseZGC -XX:MaxDirectMemorySize=2G -Dkafka.logs.dir=logs/ -Dlog4j.configuration=file:config/log4j.properties -Dio.netty.leakDetection.level=paranoid    |\n| CLI Arguments | config/kraft/server.properties|\n| Environment | KAFKA_S3_ACCESS_KEY=test;KAFKA_S3_SECRET_KEY=test |\n\n> tips: If you are using localstack, just use any value of access key and secret key. If you are using real S3 service, set `KAFKA_S3_ACCESS_KEY` and `KAFKA_S3_SECRET_KEY` to the real access key and secret key that have read/write permission of S3 service.\n\n\n## Documentation\n\nWe welcome Pull Requests that enhance the grammar, structure, or fix typos in our documentation.\n\n## Engage with the Community\n\nAnother crucial way to contribute is by reporting bugs and helping other users in the community.\n\nYou're welcome to enter\nthe [Community Slack](https://join.slack.com/t/automq/shared_invite/zt-29h17vye9-thf31ebIVL9oXuRdACnOIA) and help other\nusers or report bugs in GitHub.\n\n## Attribution\n\nThis contributing document is adapted from that of [Airbyte](https://github.com/airbytehq/airbyte).\n"
        },
        {
          "name": "HEADER",
          "type": "blob",
          "size": 0.736328125,
          "content": "Licensed to the Apache Software Foundation (ASF) under one or more\ncontributor license agreements.  See the NOTICE file distributed with\nthis work for additional information regarding copyright ownership.\nThe ASF licenses this file to You under the Apache License, Version 2.0\n(the \"License\"); you may not use this file except in compliance with\nthe License.  You may obtain a copy of the License at\n\n   http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n"
        },
        {
          "name": "Jenkinsfile",
          "type": "blob",
          "size": 5.5185546875,
          "content": "/*\n *\n *  Licensed to the Apache Software Foundation (ASF) under one or more\n *  contributor license agreements.  See the NOTICE file distributed with\n *  this work for additional information regarding copyright ownership.\n *  The ASF licenses this file to You under the Apache License, Version 2.0\n *  (the \"License\"); you may not use this file except in compliance with\n *  the License.  You may obtain a copy of the License at\n *\n *      http://www.apache.org/licenses/LICENSE-2.0\n *\n *  Unless required by applicable law or agreed to in writing, software\n *  distributed under the License is distributed on an \"AS IS\" BASIS,\n *  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n *  See the License for the specific language governing permissions and\n *  limitations under the License.\n *\n */\n\ndef doValidation() {\n  // Run all the tasks associated with `check` except for `test` - the latter is executed via `doTest`\n  sh \"\"\"\n    ./retry_zinc ./gradlew -PscalaVersion=$SCALA_VERSION clean check -x test \\\n        --profile --continue -PxmlSpotBugsReport=true -PkeepAliveMode=\"session\"\n  \"\"\"\n}\n\ndef isChangeRequest(env) {\n  env.CHANGE_ID != null && !env.CHANGE_ID.isEmpty()\n}\n\ndef doTest(env, target = \"test\") {\n  sh \"\"\"./gradlew -PscalaVersion=$SCALA_VERSION ${target} \\\n      --profile --continue -PkeepAliveMode=\"session\" -PtestLoggingEvents=started,passed,skipped,failed \\\n      -PignoreFailures=true -PmaxParallelForks=2 -PmaxTestRetries=1 -PmaxTestRetryFailures=10\"\"\"\n  junit '**/build/test-results/**/TEST-*.xml'\n}\n\ndef runTestOnDevBranch(env) {\n  if (!isChangeRequest(env)) {\n    doTest(env)\n  }\n}\n\ndef doStreamsArchetype() {\n  echo 'Verify that Kafka Streams archetype compiles'\n\n  sh '''\n    ./gradlew streams:publishToMavenLocal clients:publishToMavenLocal connect:json:publishToMavenLocal connect:api:publishToMavenLocal \\\n         || { echo 'Could not publish kafka-streams.jar (and dependencies) locally to Maven'; exit 1; }\n  '''\n\n  VERSION = sh(script: 'grep \"^version=\" gradle.properties | cut -d= -f 2', returnStdout: true).trim()\n\n  dir('streams/quickstart') {\n    sh '''\n      mvn clean install -Dgpg.skip  \\\n          || { echo 'Could not `mvn install` streams quickstart archetype'; exit 1; }\n    '''\n\n    dir('test-streams-archetype') {\n      // Note the double quotes for variable interpolation\n      sh \"\"\" \n        echo \"Y\" | mvn archetype:generate \\\n            -DarchetypeCatalog=local \\\n            -DarchetypeGroupId=org.apache.kafka \\\n            -DarchetypeArtifactId=streams-quickstart-java \\\n            -DarchetypeVersion=${VERSION} \\\n            -DgroupId=streams.examples \\\n            -DartifactId=streams.examples \\\n            -Dversion=0.1 \\\n            -Dpackage=myapps \\\n            || { echo 'Could not create new project using streams quickstart archetype'; exit 1; }\n      \"\"\"\n\n      dir('streams.examples') {\n        sh '''\n          mvn compile \\\n              || { echo 'Could not compile streams quickstart archetype project'; exit 1; }\n        '''\n      }\n    }\n  }\n}\n\ndef tryStreamsArchetype() {\n  try {\n    doStreamsArchetype()\n  } catch(err) {\n    echo 'Failed to build Kafka Streams archetype, marking this build UNSTABLE'\n    currentBuild.result = 'UNSTABLE'\n  }\n}\n\n\npipeline {\n  agent none\n  \n  options {\n    disableConcurrentBuilds(abortPrevious: isChangeRequest(env))\n  }\n  \n  stages {\n    stage('Build') {\n      parallel {\n\n        stage('JDK 8 and Scala 2.12') {\n          agent { label 'ubuntu' }\n          tools {\n            jdk 'jdk_1.8_latest'\n            maven 'maven_3_latest'\n          }\n          options {\n            timeout(time: 8, unit: 'HOURS') \n            timestamps()\n          }\n          environment {\n            SCALA_VERSION=2.12\n          }\n          steps {\n            doValidation()\n            doTest(env)\n            tryStreamsArchetype()\n          }\n        }\n\n        stage('JDK 11 and Scala 2.13') {\n          agent { label 'ubuntu' }\n          tools {\n            jdk 'jdk_11_latest'\n          }\n          options {\n            timeout(time: 8, unit: 'HOURS') \n            timestamps()\n          }\n          environment {\n            SCALA_VERSION=2.13\n          }\n          steps {\n            doValidation()\n            runTestOnDevBranch(env)\n            echo 'Skipping Kafka Streams archetype test for Java 11'\n          }\n        }\n\n        stage('JDK 17 and Scala 2.13') {\n          agent { label 'ubuntu' }\n          tools {\n            jdk 'jdk_17_latest'\n          }\n          options {\n            timeout(time: 8, unit: 'HOURS') \n            timestamps()\n          }\n          environment {\n            SCALA_VERSION=2.13\n          }\n          steps {\n            doValidation()\n            runTestOnDevBranch(env)\n            echo 'Skipping Kafka Streams archetype test for Java 17'\n          }\n        }\n\n        stage('JDK 21 and Scala 2.13') {\n          agent { label 'ubuntu' }\n          tools {\n            jdk 'jdk_21_latest'\n          }\n          options {\n            timeout(time: 8, unit: 'HOURS')\n            timestamps()\n          }\n          environment {\n            SCALA_VERSION=2.13\n          }\n          steps {\n            doValidation()\n            doTest(env)\n            echo 'Skipping Kafka Streams archetype test for Java 21'\n          }\n        }\n      }\n    }\n  }\n  \n  post {\n    always {\n      script {\n        if (!isChangeRequest(env)) {\n          node('ubuntu') {\n            step([$class: 'Mailer',\n                 notifyEveryUnstableBuild: true,\n                 recipients: \"dev@kafka.apache.org\",\n                 sendToIndividuals: false])\n          }\n        }\n      }\n    }\n  }\n}\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 1.5185546875,
          "content": "Copyright (c) 2023-2024 AutoMQ HK Limited.\n\nthis software are licensed as follows:\n\n1. Apache Kafka Source and Dependency Licensing:\n   All code in this repository that is forked from Apache Kafka and its\n   dependencies will continue to be licensed under the original Apache Kafka\n   open source license. For detailed licensing information regarding Apache\n   Kafka and its dependencies, please refer to the files under the \"/licenses/\"\n   folder in this repository.\n\n2. S3Stream Component Licensing:\n   The S3Stream component added to this project (specifically referring to all\n   files under the \"/S3Stream/\" directory) is licensed under a revised Business\n   Source License (BSL) by AutoMQ HK Limited, with the specific terms available\n   in the /LICENSE.S3Stream file in this repository. Any dependencies used by\n   the S3Stream component are subject to their respective open source licenses.\n\n3. File-Level License Precedence:\n   For each file in this repository, if the license is explicitly specified in\n   the header of the file, the license stated in the file header shall prevail.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n"
        },
        {
          "name": "LICENSE.S3Stream",
          "type": "blob",
          "size": 4.916015625,
          "content": "License text copyright © 2023 MariaDB plc, All Rights Reserved.\n\"Business Source License\" is a trademark of MariaDB plc.\n\n\nParameters\n\nLicensor:             AutoMQ HK Limited.\nLicensed Work:        AutoMQ Version 1.1.2 or later. The Licensed Work is (c) 2024\n                      AutoMQ HK Limited.\nAdditional Use Grant: You may make production use of the Licensed Work, provided\n                      Your use does not include offering the Licensed Work to third\n                      parties on a hosted or embedded basis in order to compete with\n                      AutoMQ's paid version(s) of the Licensed Work. For purposes\n                      of this license:\n\n                      A \"competitive offering\" is a Product that is offered to third\n                      parties on a paid basis, including through paid support\n                      arrangements, that significantly overlaps with the capabilities\n                      of AutoMQ's paid version(s) of the Licensed Work. If Your\n                      Product is not a competitive offering when You first make it\n                      generally available, it will not become a competitive offering\n                      later due to AutoMQ releasing a new version of the Licensed\n                      Work with additional capabilities. In addition, Products that\n                      are not provided on a paid basis are not competitive.\n\n                      \"Product\" means software that is offered to end users to manage\n                      in their own environments or offered as a service on a hosted\n                      basis.\n\n                      \"Embedded\" means including the source code or executable code\n                      from the Licensed Work in a competitive offering. \"Embedded\"\n                      also means packaging the competitive offering in such a way\n                      that the Licensed Work must be accessed or downloaded for the\n                      competitive offering to operate.\n\n                      Hosting or using the Licensed Work(s) for internal purposes\n                      within an organization is not considered a competitive\n                      offering. AutoMQ considers your organization to include all\n                      of your affiliates under common control.\n\n                      For binding interpretive guidance on using AutoMQ products\n                      under the Business Source License, please visit our FAQ.\n                      (https://www.automq.com/license-faq)\nChange Date:          Change date is four years from release date.\n                      Please see https://github.com/AutoMQ/automq/releases for exact dates\nChange License:       Apache License, Version 2.0\n                      URL: https://www.apache.org/licenses/LICENSE-2.0\n\n\nFor information about alternative licensing arrangements for the Licensed Work,\nplease contact licensing@automq.com.\n\nNotice\n\nBusiness Source License 1.1\n\nTerms\n\nThe Licensor hereby grants you the right to copy, modify, create derivative\nworks, redistribute, and make non-production use of the Licensed Work. The\nLicensor may make an Additional Use Grant, above, permitting limited production use.\n\nEffective on the Change Date, or the fourth anniversary of the first publicly\navailable distribution of a specific version of the Licensed Work under this\nLicense, whichever comes first, the Licensor hereby grants you rights under\nthe terms of the Change License, and the rights granted in the paragraph\nabove terminate.\n\nIf your use of the Licensed Work does not comply with the requirements\ncurrently in effect as described in this License, you must purchase a\ncommercial license from the Licensor, its affiliated entities, or authorized\nresellers, or you must refrain from using the Licensed Work.\n\nAll copies of the original and modified Licensed Work, and derivative works\nof the Licensed Work, are subject to this License. This License applies\nseparately for each version of the Licensed Work and the Change Date may vary\nfor each version of the Licensed Work released by Licensor.\n\nYou must conspicuously display this License on each original or modified copy\nof the Licensed Work. If you receive the Licensed Work in original or\nmodified form from a third party, the terms and conditions set forth in this\nLicense apply to your use of that work.\n\nAny use of the Licensed Work in violation of this License will automatically\nterminate your rights under this License for the current and all other\nversions of the Licensed Work.\n\nThis License does not grant you any right in any trademark or logo of\nLicensor or its affiliates (provided that you may use a trademark or logo of\nLicensor as expressly required by this License).\n\nTO THE EXTENT PERMITTED BY APPLICABLE LAW, THE LICENSED WORK IS PROVIDED ON\nAN \"AS IS\" BASIS. LICENSOR HEREBY DISCLAIMS ALL WARRANTIES AND CONDITIONS,\nEXPRESS OR IMPLIED, INCLUDING (WITHOUT LIMITATION) WARRANTIES OF\nMERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, NON-INFRINGEMENT, AND\nTITLE.\n"
        },
        {
          "name": "NOTICE",
          "type": "blob",
          "size": 1.4677734375,
          "content": "AutoMQ NOTICE\nCopyright 2023-2024, AutoMQ HK Limited.\n\n---------------------------\nApache Kafka NOTICE\n---------------------------\nApache Kafka\nCopyright 2023 The Apache Software Foundation.\n\nThis product includes software developed at\nThe Apache Software Foundation (https://www.apache.org/).\n\nThis distribution has a binary dependency on jersey, which is available under the CDDL\nLicense. The source code of jersey can be found at https://github.com/jersey/jersey/.\n\nThis distribution has a binary test dependency on jqwik, which is available under\nthe Eclipse Public License 2.0. The source code can be found at\nhttps://github.com/jlink/jqwik.\n\nThe streams-scala (streams/streams-scala) module was donated by Lightbend and the original code was copyrighted by them:\nCopyright (C) 2018 Lightbend Inc. <https://www.lightbend.com>\nCopyright (C) 2017-2018 Alexis Seigneurin.\n\nThis project contains the following code copied from Apache Hadoop:\nclients/src/main/java/org/apache/kafka/common/utils/PureJavaCrc32C.java\nSome portions of this file Copyright (c) 2004-2006 Intel Corporation and licensed under the BSD license.\n\nThis project contains the following code copied from Apache Hive:\nstreams/src/main/java/org/apache/kafka/streams/state/internals/Murmur3.java\n\nThis project contains codes copied from, derived from or inspired by Cruise Control (https://github.com/linkedin/cruise-control)\nCopyright 2017, 2018, 2019, 2020, 2021, 2022 LinkedIn Corporation.\nAll Rights Reserved.\nLicense: BSD 2-CLAUSE\n"
        },
        {
          "name": "NOTICE-binary",
          "type": "blob",
          "size": 27.6669921875,
          "content": "AutoMQ Binary NOTICE\nCopyright 2023-2024, AutoMQ HK Limited.\n\n---------------------------\nApache Kafka Binary NOTICE\n---------------------------\nApache Kafka\nCopyright 2021 The Apache Software Foundation.\n\nThis product includes software developed at\nThe Apache Software Foundation (https://www.apache.org/).\n\nThis distribution has a binary dependency on jersey, which is available under the CDDL\nLicense. The source code of jersey can be found at https://github.com/jersey/jersey/.\n\nThis distribution has a binary test dependency on jqwik, which is available under\nthe Eclipse Public License 2.0. The source code can be found at\nhttps://github.com/jlink/jqwik.\n\nThe streams-scala (streams/streams-scala) module was donated by Lightbend and the original code was copyrighted by them:\nCopyright (C) 2018 Lightbend Inc. <https://www.lightbend.com>\nCopyright (C) 2017-2018 Alexis Seigneurin.\n\nThis project contains the following code copied from Apache Hadoop:\nclients/src/main/java/org/apache/kafka/common/utils/PureJavaCrc32C.java\nSome portions of this file Copyright (c) 2004-2006 Intel Corporation and licensed under the BSD license.\n\nThis project contains the following code copied from Apache Hive:\nstreams/src/main/java/org/apache/kafka/streams/state/internals/Murmur3.java\n\n// ------------------------------------------------------------------\n// NOTICE file corresponding to the section 4d of The Apache License,\n// Version 2.0, in this case for\n// ------------------------------------------------------------------\n\n# Notices for Eclipse GlassFish\n\nThis content is produced and maintained by the Eclipse GlassFish project.\n\n* Project home: https://projects.eclipse.org/projects/ee4j.glassfish\n\n## Trademarks\n\nEclipse GlassFish, and GlassFish are trademarks of the Eclipse Foundation.\n\n## Copyright\n\nAll content is the property of the respective authors or their employers. For\nmore information regarding authorship of content, please consult the listed\nsource code repository logs.\n\n## Declared Project Licenses\n\nThis program and the accompanying materials are made available under the terms\nof the Eclipse Public License v. 2.0 which is available at\nhttp://www.eclipse.org/legal/epl-2.0. This Source Code may also be made\navailable under the following Secondary Licenses when the conditions for such\navailability set forth in the Eclipse Public License v. 2.0 are satisfied: GNU\nGeneral Public License, version 2 with the GNU Classpath Exception which is\navailable at https://www.gnu.org/software/classpath/license.html.\n\nSPDX-License-Identifier: EPL-2.0 OR GPL-2.0 WITH Classpath-exception-2.0\n\n## Source Code\n\nThe project maintains the following source code repositories:\n\n* https://github.com/eclipse-ee4j/glassfish-ha-api\n* https://github.com/eclipse-ee4j/glassfish-logging-annotation-processor\n* https://github.com/eclipse-ee4j/glassfish-shoal\n* https://github.com/eclipse-ee4j/glassfish-cdi-porting-tck\n* https://github.com/eclipse-ee4j/glassfish-jsftemplating\n* https://github.com/eclipse-ee4j/glassfish-hk2-extra\n* https://github.com/eclipse-ee4j/glassfish-hk2\n* https://github.com/eclipse-ee4j/glassfish-fighterfish\n\n## Third-party Content\n\nThis project leverages the following third party content.\n\nNone\n\n## Cryptography\n\nContent may contain encryption software. The country in which you are currently\nmay have restrictions on the import, possession, and use, and/or re-export to\nanother country, of encryption software. BEFORE using any encryption software,\nplease check the country's laws, regulations and policies concerning the import,\npossession, or use, and re-export of encryption software, to see if this is\npermitted.\n\n\nApache Yetus - Audience Annotations\nCopyright 2015-2017 The Apache Software Foundation\n\nThis product includes software developed at\nThe Apache Software Foundation (http://www.apache.org/).\n\n\nApache Commons CLI\nCopyright 2001-2017 The Apache Software Foundation\n\nThis product includes software developed at\nThe Apache Software Foundation (http://www.apache.org/).\n\n\nApache Commons Lang\nCopyright 2001-2018 The Apache Software Foundation\n\nThis product includes software developed at\nThe Apache Software Foundation (http://www.apache.org/).\n\n\n# Jackson JSON processor\n\nJackson is a high-performance, Free/Open Source JSON processing library.\nIt was originally written by Tatu Saloranta (tatu.saloranta@iki.fi), and has\nbeen in development since 2007.\nIt is currently developed by a community of developers, as well as supported\ncommercially by FasterXML.com.\n\n## Licensing\n\nJackson core and extension components may licensed under different licenses.\nTo find the details that apply to this artifact see the accompanying LICENSE file.\nFor more information, including possible other licensing options, contact\nFasterXML.com (http://fasterxml.com).\n\n## Credits\n\nA list of contributors may be found from CREDITS file, which is included\nin some artifacts (usually source distributions); but is always available\nfrom the source code management (SCM) system project uses.\n\n\n# Notices for Eclipse Project for JAF\n\nThis content is produced and maintained by the Eclipse Project for JAF project.\n\n* Project home: https://projects.eclipse.org/projects/ee4j.jaf\n\n## Copyright\n\nAll content is the property of the respective authors or their employers. For\nmore information regarding authorship of content, please consult the listed\nsource code repository logs.\n\n## Declared Project Licenses\n\nThis program and the accompanying materials are made available under the terms\nof the Eclipse Distribution License v. 1.0,\nwhich is available at http://www.eclipse.org/org/documents/edl-v10.php.\n\nSPDX-License-Identifier: BSD-3-Clause\n\n## Source Code\n\nThe project maintains the following source code repositories:\n\n* https://github.com/eclipse-ee4j/jaf\n\n## Third-party Content\n\nThis project leverages the following third party content.\n\nJUnit (4.12)\n\n* License: Eclipse Public License\n\n\n# Notices for Jakarta Annotations\n\nThis content is produced and maintained by the Jakarta Annotations project.\n\n * Project home: https://projects.eclipse.org/projects/ee4j.ca\n\n## Trademarks\n\nJakarta Annotations is a trademark of the Eclipse Foundation.\n\n## Declared Project Licenses\n\nThis program and the accompanying materials are made available under the terms\nof the Eclipse Public License v. 2.0 which is available at\nhttp://www.eclipse.org/legal/epl-2.0. This Source Code may also be made\navailable under the following Secondary Licenses when the conditions for such\navailability set forth in the Eclipse Public License v. 2.0 are satisfied: GNU\nGeneral Public License, version 2 with the GNU Classpath Exception which is\navailable at https://www.gnu.org/software/classpath/license.html.\n\nSPDX-License-Identifier: EPL-2.0 OR GPL-2.0 WITH Classpath-exception-2.0\n\n## Source Code\n\nThe project maintains the following source code repositories:\n\n * https://github.com/eclipse-ee4j/common-annotations-api\n\n## Third-party Content\n\n## Cryptography\n\nContent may contain encryption software. The country in which you are currently\nmay have restrictions on the import, possession, and use, and/or re-export to\nanother country, of encryption software. BEFORE using any encryption software,\nplease check the country's laws, regulations and policies concerning the import,\npossession, or use, and re-export of encryption software, to see if this is\npermitted.\n\n\n# Notices for the Jakarta RESTful Web Services Project\n\nThis content is produced and maintained by the **Jakarta RESTful Web Services**\nproject.\n\n* Project home: https://projects.eclipse.org/projects/ee4j.jaxrs\n\n## Trademarks\n\n**Jakarta RESTful Web Services** is a trademark of the Eclipse Foundation.\n\n## Copyright\n\nAll content is the property of the respective authors or their employers. For\nmore information regarding authorship of content, please consult the listed\nsource code repository logs.\n\n## Declared Project Licenses\n\nThis program and the accompanying materials are made available under the terms\nof the Eclipse Public License v. 2.0 which is available at\nhttp://www.eclipse.org/legal/epl-2.0. This Source Code may also be made\navailable under the following Secondary Licenses when the conditions for such\navailability set forth in the Eclipse Public License v. 2.0 are satisfied: GNU\nGeneral Public License, version 2 with the GNU Classpath Exception which is\navailable at https://www.gnu.org/software/classpath/license.html.\n\nSPDX-License-Identifier: EPL-2.0 OR GPL-2.0 WITH Classpath-exception-2.0\n\n## Source Code\n\nThe project maintains the following source code repositories:\n\n* https://github.com/eclipse-ee4j/jaxrs-api\n\n## Third-party Content\n\nThis project leverages the following third party content.\n\njavaee-api (7.0)\n\n* License: Apache-2.0 AND W3C\n\nJUnit (4.11)\n\n* License: Common Public License 1.0\n\nMockito (2.16.0)\n\n* Project: http://site.mockito.org\n* Source: https://github.com/mockito/mockito/releases/tag/v2.16.0\n\n## Cryptography\n\nContent may contain encryption software. The country in which you are currently\nmay have restrictions on the import, possession, and use, and/or re-export to\nanother country, of encryption software. BEFORE using any encryption software,\nplease check the country's laws, regulations and policies concerning the import,\npossession, or use, and re-export of encryption software, to see if this is\npermitted.\n\n\n# Notices for Eclipse Project for JAXB\n\nThis content is produced and maintained by the Eclipse Project for JAXB project.\n\n* Project home: https://projects.eclipse.org/projects/ee4j.jaxb\n\n## Trademarks\n\nEclipse Project for JAXB is a trademark of the Eclipse Foundation.\n\n## Copyright\n\nAll content is the property of the respective authors or their employers. For\nmore information regarding authorship of content, please consult the listed\nsource code repository logs.\n\n## Declared Project Licenses\n\nThis program and the accompanying materials are made available under the terms\nof the Eclipse Distribution License v. 1.0 which is available\nat http://www.eclipse.org/org/documents/edl-v10.php.\n\nSPDX-License-Identifier: BSD-3-Clause\n\n## Source Code\n\nThe project maintains the following source code repositories:\n\n* https://github.com/eclipse-ee4j/jaxb-api\n\n## Third-party Content\n\nThis project leverages the following third party content.\n\nNone\n\n## Cryptography\n\nContent may contain encryption software. The country in which you are currently\nmay have restrictions on the import, possession, and use, and/or re-export to\nanother country, of encryption software. BEFORE using any encryption software,\nplease check the country's laws, regulations and policies concerning the import,\npossession, or use, and re-export of encryption software, to see if this is\npermitted.\n\n\n# Notice for Jersey\nThis content is produced and maintained by the Eclipse Jersey project.\n\n*  Project home: https://projects.eclipse.org/projects/ee4j.jersey\n\n## Trademarks\nEclipse Jersey is a trademark of the Eclipse Foundation.\n\n## Copyright\n\nAll content is the property of the respective authors or their employers. For\nmore information regarding authorship of content, please consult the listed\nsource code repository logs.\n\n## Declared Project Licenses\n\nThis program and the accompanying materials are made available under the terms\nof the Eclipse Public License v. 2.0 which is available at\nhttp://www.eclipse.org/legal/epl-2.0. This Source Code may also be made\navailable under the following Secondary Licenses when the conditions for such\navailability set forth in the Eclipse Public License v. 2.0 are satisfied: GNU\nGeneral Public License, version 2 with the GNU Classpath Exception which is\navailable at https://www.gnu.org/software/classpath/license.html.\n\nSPDX-License-Identifier: EPL-2.0 OR GPL-2.0 WITH Classpath-exception-2.0\n\n## Source Code\nThe project maintains the following source code repositories:\n\n* https://github.com/eclipse-ee4j/jersey\n\n## Third-party Content\n\nAngular JS, v1.6.6\n* License MIT (http://www.opensource.org/licenses/mit-license.php)\n* Project: http://angularjs.org\n* Copyright: (c) 2010-2017 Google, Inc.\n\naopalliance Version 1\n* License: all the source code provided by AOP Alliance is Public Domain.\n* Project: http://aopalliance.sourceforge.net\n* Copyright: Material in the public domain is not protected by copyright\n\nBean Validation API 2.0.2\n* License: Apache License, 2.0\n* Project: http://beanvalidation.org/1.1/\n* Copyright: 2009, Red Hat, Inc. and/or its affiliates, and individual contributors\n* by the @authors tag.\n\nHibernate Validator CDI, 6.1.2.Final\n* License: Apache License, 2.0\n* Project: https://beanvalidation.org/\n* Repackaged in org.glassfish.jersey.server.validation.internal.hibernate\n\nBootstrap v3.3.7\n* License: MIT license (https://github.com/twbs/bootstrap/blob/master/LICENSE)\n* Project: http://getbootstrap.com\n* Copyright: 2011-2016 Twitter, Inc\n\nGoogle Guava Version 18.0\n* License: Apache License, 2.0\n* Copyright (C) 2009 The Guava Authors\n\njavax.inject Version: 1\n* License: Apache License, 2.0\n* Copyright (C) 2009 The JSR-330 Expert Group\n\nJavassist Version 3.25.0-GA\n* License: Apache License, 2.0\n* Project: http://www.javassist.org/\n* Copyright (C) 1999- Shigeru Chiba. All Rights Reserved.\n\nJackson JAX-RS Providers Version 2.10.1\n* License: Apache License, 2.0\n* Project: https://github.com/FasterXML/jackson-jaxrs-providers\n* Copyright: (c) 2009-2011 FasterXML, LLC. All rights reserved unless otherwise indicated.\n\njQuery v1.12.4\n* License: jquery.org/license\n* Project: jquery.org\n* Copyright: (c) jQuery Foundation\n\njQuery Barcode plugin 0.3\n* License: MIT & GPL (http://www.opensource.org/licenses/mit-license.php & http://www.gnu.org/licenses/gpl.html)\n* Project:  http://www.pasella.it/projects/jQuery/barcode\n* Copyright: (c) 2009 Antonello Pasella antonello.pasella@gmail.com\n\nJSR-166 Extension - JEP 266\n* License: CC0\n* No copyright\n* Written by Doug Lea with assistance from members of JCP JSR-166 Expert Group and released to the public domain, as explained at http://creativecommons.org/publicdomain/zero/1.0/\n\nKineticJS, v4.7.1\n* License: MIT license (http://www.opensource.org/licenses/mit-license.php)\n* Project: http://www.kineticjs.com, https://github.com/ericdrowell/KineticJS\n* Copyright: Eric Rowell\n\norg.objectweb.asm Version 8.0\n* License: Modified BSD (http://asm.objectweb.org/license.html)\n* Copyright (c) 2000-2011 INRIA, France Telecom. All rights reserved.\n\norg.osgi.core version 6.0.0\n* License: Apache License, 2.0\n* Copyright (c) OSGi Alliance (2005, 2008). All Rights Reserved.\n\norg.glassfish.jersey.server.internal.monitoring.core\n* License: Apache License, 2.0\n* Copyright (c) 2015-2018 Oracle and/or its affiliates. All rights reserved.\n* Copyright 2010-2013 Coda Hale and Yammer, Inc.\n\nW3.org documents\n* License: W3C License\n* Copyright: Copyright (c) 1994-2001 World Wide Web Consortium, (Massachusetts Institute of Technology, Institut National de Recherche en Informatique et en Automatique, Keio University). All Rights Reserved. http://www.w3.org/Consortium/Legal/\n\n\n==============================================================\n Jetty Web Container\n Copyright 1995-2018 Mort Bay Consulting Pty Ltd.\n==============================================================\n\nThe Jetty Web Container is Copyright Mort Bay Consulting Pty Ltd\nunless otherwise noted.\n\nJetty is dual licensed under both\n\n  * The Apache 2.0 License\n    http://www.apache.org/licenses/LICENSE-2.0.html\n\n      and\n\n  * The Eclipse Public 1.0 License\n    http://www.eclipse.org/legal/epl-v10.html\n\nJetty may be distributed under either license.\n\n------\nEclipse\n\nThe following artifacts are EPL.\n * org.eclipse.jetty.orbit:org.eclipse.jdt.core\n\nThe following artifacts are EPL and ASL2.\n * org.eclipse.jetty.orbit:javax.security.auth.message\n\n\nThe following artifacts are EPL and CDDL 1.0.\n * org.eclipse.jetty.orbit:javax.mail.glassfish\n\n\n------\nOracle\n\nThe following artifacts are CDDL + GPLv2 with classpath exception.\nhttps://glassfish.dev.java.net/nonav/public/CDDL+GPL.html\n\n * javax.servlet:javax.servlet-api\n * javax.annotation:javax.annotation-api\n * javax.transaction:javax.transaction-api\n * javax.websocket:javax.websocket-api\n\n------\nOracle OpenJDK\n\nIf ALPN is used to negotiate HTTP/2 connections, then the following\nartifacts may be included in the distribution or downloaded when ALPN\nmodule is selected.\n\n * java.sun.security.ssl\n\nThese artifacts replace/modify OpenJDK classes.  The modififications\nare hosted at github and both modified and original are under GPL v2 with\nclasspath exceptions.\nhttp://openjdk.java.net/legal/gplv2+ce.html\n\n\n------\nOW2\n\nThe following artifacts are licensed by the OW2 Foundation according to the\nterms of http://asm.ow2.org/license.html\n\norg.ow2.asm:asm-commons\norg.ow2.asm:asm\n\n\n------\nApache\n\nThe following artifacts are ASL2 licensed.\n\norg.apache.taglibs:taglibs-standard-spec\norg.apache.taglibs:taglibs-standard-impl\n\n\n------\nMortBay\n\nThe following artifacts are ASL2 licensed.  Based on selected classes from\nfollowing Apache Tomcat jars, all ASL2 licensed.\n\norg.mortbay.jasper:apache-jsp\n  org.apache.tomcat:tomcat-jasper\n  org.apache.tomcat:tomcat-juli\n  org.apache.tomcat:tomcat-jsp-api\n  org.apache.tomcat:tomcat-el-api\n  org.apache.tomcat:tomcat-jasper-el\n  org.apache.tomcat:tomcat-api\n  org.apache.tomcat:tomcat-util-scan\n  org.apache.tomcat:tomcat-util\n\norg.mortbay.jasper:apache-el\n  org.apache.tomcat:tomcat-jasper-el\n  org.apache.tomcat:tomcat-el-api\n\n\n------\nMortbay\n\nThe following artifacts are CDDL + GPLv2 with classpath exception.\n\nhttps://glassfish.dev.java.net/nonav/public/CDDL+GPL.html\n\norg.eclipse.jetty.toolchain:jetty-schemas\n\n------\nAssorted\n\nThe UnixCrypt.java code implements the one way cryptography used by\nUnix systems for simple password protection.  Copyright 1996 Aki Yoshida,\nmodified April 2001  by Iris Van den Broeke, Daniel Deville.\nPermission to use, copy, modify and distribute UnixCrypt\nfor non-commercial or commercial purposes and without fee is\ngranted provided that the copyright notice appears in all copies.\n\n\nApache log4j\nCopyright 2007 The Apache Software Foundation\n\nThis product includes software developed at\nThe Apache Software Foundation (http://www.apache.org/).\n\n\nMaven Artifact\nCopyright 2001-2019 The Apache Software Foundation\n\nThis product includes software developed at\nThe Apache Software Foundation (http://www.apache.org/).\n\n\nThis product includes software developed by the Indiana University\n  Extreme! Lab (http://www.extreme.indiana.edu/).\n\nThis product includes software developed by\nThe Apache Software Foundation (http://www.apache.org/).\n\nThis product includes software developed by\nThoughtWorks (http://www.thoughtworks.com).\n\nThis product includes software developed by\njavolution (http://javolution.org/).\n\nThis product includes software developed by\nRome (https://rome.dev.java.net/).\n\n\nScala\nCopyright (c) 2002-2020 EPFL\nCopyright (c) 2011-2020 Lightbend, Inc.\n\nScala includes software developed at\nLAMP/EPFL (https://lamp.epfl.ch/) and\nLightbend, Inc. (https://www.lightbend.com/).\n\nLicensed under the Apache License, Version 2.0 (the \"License\").\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n\nThis software includes projects with other licenses -- see `doc/LICENSE.md`.\n\n\nApache ZooKeeper - Server\nCopyright 2008-2021 The Apache Software Foundation\n\nThis product includes software developed at\nThe Apache Software Foundation (http://www.apache.org/).\n\n\nApache ZooKeeper - Jute\nCopyright 2008-2021 The Apache Software Foundation\n\nThis product includes software developed at\nThe Apache Software Foundation (http://www.apache.org/).\n\n\nThe Netty Project\n                            =================\n\nPlease visit the Netty web site for more information:\n\n  * https://netty.io/\n\nCopyright 2014 The Netty Project\n\nThe Netty Project licenses this file to you under the Apache License,\nversion 2.0 (the \"License\"); you may not use this file except in compliance\nwith the License. You may obtain a copy of the License at:\n\n  https://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\nWARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\nLicense for the specific language governing permissions and limitations\nunder the License.\n\nAlso, please refer to each LICENSE.<component>.txt file, which is located in\nthe 'license' directory of the distribution file, for the license terms of the\ncomponents that this product depends on.\n\n-------------------------------------------------------------------------------\nThis product contains the extensions to Java Collections Framework which has\nbeen derived from the works by JSR-166 EG, Doug Lea, and Jason T. Greene:\n\n  * LICENSE:\n    * license/LICENSE.jsr166y.txt (Public Domain)\n  * HOMEPAGE:\n    * http://gee.cs.oswego.edu/cgi-bin/viewcvs.cgi/jsr166/\n    * http://viewvc.jboss.org/cgi-bin/viewvc.cgi/jbosscache/experimental/jsr166/\n\nThis product contains a modified version of Robert Harder's Public Domain\nBase64 Encoder and Decoder, which can be obtained at:\n\n  * LICENSE:\n    * license/LICENSE.base64.txt (Public Domain)\n  * HOMEPAGE:\n    * http://iharder.sourceforge.net/current/java/base64/\n\nThis product contains a modified portion of 'Webbit', an event based\nWebSocket and HTTP server, which can be obtained at:\n\n  * LICENSE:\n    * license/LICENSE.webbit.txt (BSD License)\n  * HOMEPAGE:\n    * https://github.com/joewalnes/webbit\n\nThis product contains a modified portion of 'SLF4J', a simple logging\nfacade for Java, which can be obtained at:\n\n  * LICENSE:\n    * license/LICENSE.slf4j.txt (MIT License)\n  * HOMEPAGE:\n    * https://www.slf4j.org/\n\nThis product contains a modified portion of 'Apache Harmony', an open source\nJava SE, which can be obtained at:\n\n  * NOTICE:\n    * license/NOTICE.harmony.txt\n  * LICENSE:\n    * license/LICENSE.harmony.txt (Apache License 2.0)\n  * HOMEPAGE:\n    * https://archive.apache.org/dist/harmony/\n\nThis product contains a modified portion of 'jbzip2', a Java bzip2 compression\nand decompression library written by Matthew J. Francis. It can be obtained at:\n\n  * LICENSE:\n    * license/LICENSE.jbzip2.txt (MIT License)\n  * HOMEPAGE:\n    * https://code.google.com/p/jbzip2/\n\nThis product contains a modified portion of 'libdivsufsort', a C API library to construct\nthe suffix array and the Burrows-Wheeler transformed string for any input string of\na constant-size alphabet written by Yuta Mori. It can be obtained at:\n\n  * LICENSE:\n    * license/LICENSE.libdivsufsort.txt (MIT License)\n  * HOMEPAGE:\n    * https://github.com/y-256/libdivsufsort\n\nThis product contains a modified portion of Nitsan Wakart's 'JCTools', Java Concurrency Tools for the JVM,\n which can be obtained at:\n\n  * LICENSE:\n    * license/LICENSE.jctools.txt (ASL2 License)\n  * HOMEPAGE:\n    * https://github.com/JCTools/JCTools\n\nThis product optionally depends on 'JZlib', a re-implementation of zlib in\npure Java, which can be obtained at:\n\n  * LICENSE:\n    * license/LICENSE.jzlib.txt (BSD style License)\n  * HOMEPAGE:\n    * http://www.jcraft.com/jzlib/\n\nThis product optionally depends on 'Compress-LZF', a Java library for encoding and\ndecoding data in LZF format, written by Tatu Saloranta. It can be obtained at:\n\n  * LICENSE:\n    * license/LICENSE.compress-lzf.txt (Apache License 2.0)\n  * HOMEPAGE:\n    * https://github.com/ning/compress\n\nThis product optionally depends on 'lz4', a LZ4 Java compression\nand decompression library written by Adrien Grand. It can be obtained at:\n\n  * LICENSE:\n    * license/LICENSE.lz4.txt (Apache License 2.0)\n  * HOMEPAGE:\n    * https://github.com/jpountz/lz4-java\n\nThis product optionally depends on 'lzma-java', a LZMA Java compression\nand decompression library, which can be obtained at:\n\n  * LICENSE:\n    * license/LICENSE.lzma-java.txt (Apache License 2.0)\n  * HOMEPAGE:\n    * https://github.com/jponge/lzma-java\n\nThis product contains a modified portion of 'jfastlz', a Java port of FastLZ compression\nand decompression library written by William Kinney. It can be obtained at:\n\n  * LICENSE:\n    * license/LICENSE.jfastlz.txt (MIT License)\n  * HOMEPAGE:\n    * https://code.google.com/p/jfastlz/\n\nThis product contains a modified portion of and optionally depends on 'Protocol Buffers', Google's data\ninterchange format, which can be obtained at:\n\n  * LICENSE:\n    * license/LICENSE.protobuf.txt (New BSD License)\n  * HOMEPAGE:\n    * https://github.com/google/protobuf\n\nThis product optionally depends on 'Bouncy Castle Crypto APIs' to generate\na temporary self-signed X.509 certificate when the JVM does not provide the\nequivalent functionality.  It can be obtained at:\n\n  * LICENSE:\n    * license/LICENSE.bouncycastle.txt (MIT License)\n  * HOMEPAGE:\n    * https://www.bouncycastle.org/\n\nThis product optionally depends on 'Snappy', a compression library produced\nby Google Inc, which can be obtained at:\n\n  * LICENSE:\n    * license/LICENSE.snappy.txt (New BSD License)\n  * HOMEPAGE:\n    * https://github.com/google/snappy\n\nThis product optionally depends on 'JBoss Marshalling', an alternative Java\nserialization API, which can be obtained at:\n\n  * LICENSE:\n    * license/LICENSE.jboss-marshalling.txt (Apache License 2.0)\n  * HOMEPAGE:\n    * https://github.com/jboss-remoting/jboss-marshalling\n\nThis product optionally depends on 'Caliper', Google's micro-\nbenchmarking framework, which can be obtained at:\n\n  * LICENSE:\n    * license/LICENSE.caliper.txt (Apache License 2.0)\n  * HOMEPAGE:\n    * https://github.com/google/caliper\n\nThis product optionally depends on 'Apache Commons Logging', a logging\nframework, which can be obtained at:\n\n  * LICENSE:\n    * license/LICENSE.commons-logging.txt (Apache License 2.0)\n  * HOMEPAGE:\n    * https://commons.apache.org/logging/\n\nThis product optionally depends on 'Apache Log4J', a logging framework, which\ncan be obtained at:\n\n  * LICENSE:\n    * license/LICENSE.log4j.txt (Apache License 2.0)\n  * HOMEPAGE:\n    * https://logging.apache.org/log4j/\n\nThis product optionally depends on 'Aalto XML', an ultra-high performance\nnon-blocking XML processor, which can be obtained at:\n\n  * LICENSE:\n    * license/LICENSE.aalto-xml.txt (Apache License 2.0)\n  * HOMEPAGE:\n    * http://wiki.fasterxml.com/AaltoHome\n\nThis product contains a modified version of 'HPACK', a Java implementation of\nthe HTTP/2 HPACK algorithm written by Twitter. It can be obtained at:\n\n  * LICENSE:\n    * license/LICENSE.hpack.txt (Apache License 2.0)\n  * HOMEPAGE:\n    * https://github.com/twitter/hpack\n\nThis product contains a modified version of 'HPACK', a Java implementation of\nthe HTTP/2 HPACK algorithm written by Cory Benfield. It can be obtained at:\n\n  * LICENSE:\n    * license/LICENSE.hyper-hpack.txt (MIT License)\n  * HOMEPAGE:\n    * https://github.com/python-hyper/hpack/\n\nThis product contains a modified version of 'HPACK', a Java implementation of\nthe HTTP/2 HPACK algorithm written by Tatsuhiro Tsujikawa. It can be obtained at:\n\n  * LICENSE:\n    * license/LICENSE.nghttp2-hpack.txt (MIT License)\n  * HOMEPAGE:\n    * https://github.com/nghttp2/nghttp2/\n\nThis product contains a modified portion of 'Apache Commons Lang', a Java library\nprovides utilities for the java.lang API, which can be obtained at:\n\n  * LICENSE:\n    * license/LICENSE.commons-lang.txt (Apache License 2.0)\n  * HOMEPAGE:\n    * https://commons.apache.org/proper/commons-lang/\n\n\nThis product contains the Maven wrapper scripts from 'Maven Wrapper', that provides an easy way to ensure a user has everything necessary to run the Maven build.\n\n  * LICENSE:\n    * license/LICENSE.mvn-wrapper.txt (Apache License 2.0)\n  * HOMEPAGE:\n    * https://github.com/takari/maven-wrapper\n\nThis product contains the dnsinfo.h header file, that provides a way to retrieve the system DNS configuration on MacOS.\nThis private header is also used by Apple's open source\n mDNSResponder (https://opensource.apple.com/tarballs/mDNSResponder/).\n\n * LICENSE:\n    * license/LICENSE.dnsinfo.txt (Apple Public Source License 2.0)\n  * HOMEPAGE:\n    * https://www.opensource.apple.com/source/configd/configd-453.19/dnsinfo/dnsinfo.h\n"
        },
        {
          "name": "PULL_REQUEST_TEMPLATE.md",
          "type": "blob",
          "size": 0.556640625,
          "content": "*More detailed description of your change,\nif necessary. The PR title and PR message become\nthe squashed commit message, so use a separate\ncomment to ping reviewers.*\n\n*Summary of testing strategy (including rationale)\nfor the feature or bug fix. Unit and/or integration\ntests are expected for any behaviour change and\nsystem tests should be considered for larger changes.*\n\n### Committer Checklist (excluded from commit message)\n- [ ] Verify design and implementation \n- [ ] Verify test coverage and CI build status\n- [ ] Verify documentation (including upgrade notes)\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 12.205078125,
          "content": "# AutoMQ: A cloud-first alternative of Kafka by decoupling durability to S3 and EBS\n\n\n<div align=\"center\">\n<p align=\"center\">\n  🔥&nbsp <a\n    href=\"https://www.automq.com/quick-start#Cloud?utm_source=github_automq_cloud\"\n    target=\"_blank\"\n  ><b>Free trial of AutoMQ Business Edition</b></a>&nbsp&nbsp&nbsp\n  📑&nbsp <a\n    href=\"https://docs.automq.com/docs/automq-opensource/HSiEwHVfdiO7rWk34vKcVvcvn2Z?utm_source=github\"\n    target=\"_blank\"\n  ><b>Documentation</b></a>&nbsp&nbsp&nbsp\n  📃&nbsp <a\n    href=\"https://www.automq.com/blog/introducing-automq-cloud-native-replacement-of-apache-kafka?utm_source=github\"\n    target=\"_blank\"\n  ><b>AutoMQ Introduction</b></a>\n</p>\n\n\n[![Linkedin Badge](https://img.shields.io/badge/-LinkedIn-blue?style=flat-square&logo=Linkedin&logoColor=white&link=https://www.linkedin.com/company/automq)](https://www.linkedin.com/company/automq)\n[![Twitter URL](https://img.shields.io/twitter/follow/AutoMQ)](https://twitter.com/intent/follow?screen_name=AutoMQ_Lab)\n[![](https://img.shields.io/badge/-%20Wechat%20-red?style=social&logo=discourse)](docs/images/automq-wechat.png)\n[![](https://badgen.net/badge/Slack/Join%20AutoMQ/0abd59?icon=slack)](https://join.slack.com/t/automq/shared_invite/zt-29h17vye9-thf31ebIVL9oXuRdACnOIA)\n[![](https://img.shields.io/badge/AutoMQ%20vs.%20Kafka(Cost)-yellow)](https://www.automq.com/blog/automq-vs-apache-kafka-a-real-aws-cloud-bill-comparison)\n[![](https://img.shields.io/badge/AutoMQ%20vs.%20Kafka(Performance)-orange)](https://docs.automq.com/docs/automq-opensource/IJLQwnVROiS5cUkXfF0cuHnWnNd)\n[![Gurubase](https://img.shields.io/badge/Gurubase-Ask%20AutoMQ%20Guru-006BFF)](https://gurubase.io/g/automq)\n\n<a href=\"https://trendshift.io/repositories/9782\" target=\"_blank\"><img src=\"https://trendshift.io/api/badge/repositories/9782\" alt=\"AutoMQ%2Fautomq | Trendshift\" style=\"width: 250px; height: 55px;\" width=\"250\" height=\"55\"/></a>\n\n---\n\n![](https://img.shields.io/badge/AWS-%E2%9C%85-lightgray?logo=amazonaws)\n![](https://img.shields.io/badge/Google-%E2%9C%85-lightgray?logo=googlecloud)\n![](https://img.shields.io/badge/Azure-%E2%9C%85-lightgray?logo=microsoftazure)\n![](https://img.shields.io/badge/Aliyun-%E2%9C%85-lightgray?logo=alibabacloud)\n</div>\n\n## 📺 Youtube Video Introduction\n<b>Watch this video to learn what is AutoMQ. ⬇️ ⬇️ ⬇️ </b>\n\n[![What is AutoMQ?](https://img.youtube.com/vi/3JQrclZlie4/0.jpg)](https://www.youtube.com/watch?v=3JQrclZlie4)\n\n\n\n## 🍵 AutoMQ vs Other Streaming Platforms\n\n<table>\n  <tr>\n    <th>Feature</th>\n    <th>AutoMQ</th>\n    <th>Apache Kafka</th>\n    <th>Confluent</th>\n    <th>Apache Pulsar</th>\n    <th>Redpanda</th>\n    <th>Warpstream</th>\n  </tr>\n  <tr>\n    <td>Apache Kafka Compatibility[1]</td>\n    <td colspan=\"3\">Native Kafka</td>\n    <td>Non-Kafka</td>\n    <td colspan=\"2\">Kafka Protocol</td>\n  </tr>\n  <tr>\n    <td>Source Code Availability</td>\n    <td>Yes</td>\n    <td>Yes</td>\n    <td>No</td>\n    <td>Yes</td>\n    <td>Yes</td>\n    <td>No</td>\n  </tr>\n  <tr>\n    <td>Stateless Broker</td>\n    <td>Yes</td>\n    <td>No</td>\n    <td>No</td>\n    <td>Yes</td>\n    <td>No</td>\n    <td>Yes</td>\n  </tr>\n  <tr>\n    <td>Publisher Latency(P99)</td>\n    <td colspan=\"5\">Single-digit ms latency</td>\n    <td><a href=\"https://www.warpstream.com/blog/warpstream-benchmarks-and-tco\">> 620ms</a></td>\n  </tr>\n  <tr>\n    <td>Continuous Self-Balancing</td>\n    <td>Yes</td>\n    <td>No</td>\n    <td>Yes</td>\n    <td>Yes</td>\n    <td>Yes</td>\n    <td>Yes</td>\n  </tr>\n  <tr>\n    <td>Scale in/out</td>\n    <td>In seconds</td>\n    <td>In hours/days</td>\n    <td>In hours</td>\n    <td>In hours<br>(scale-in);<br> In seconds<br>(scale-out)</td>\n    <td>In hours<br>In seconds (Enterprise Only)</td>\n    <td>In seconds</td>\n  </tr>\n  <tr>\n    <td>Spot Instance Support</td>\n    <td>Yes</td>\n    <td>No</td>\n    <td>No</td>\n    <td>No</td>\n    <td>No</td>\n    <td>Yes</td>\n  </tr>\n  <tr>\n    <td>Partition Reassignment</td>\n    <td>In seconds</td>\n    <td>In hours/days</td>\n    <td>In hours</td>\n    <td>In seconds</td>\n    <td>In hours<br>In seconds (Enterprise Only)</td>\n    <td>In seconds</td>\n  </tr>\n  <tr>\n    <td>Component</td>\n    <td>Broker</td>\n    <td colspan=\"2\">Broker<br>Zookeeper<br>(Non-KRaft)</td>\n    <td>Broker<br>Zookeeper<br>Bookkeeper<br>Proxy(Optional)</td>\n    <td>Broker</td>\n    <td>Agent<br>MetadataServer</td>\n  </tr>\n  <tr>\n    <td>Durability</td>\n    <td>Guaranteed by S3/EBS[2]</td>\n    <td colspan=\"2\">Guaranteed by ISR </td>\n    <td>Guaranteed by Bookkeeper</td>\n    <td>Guaranteed by Raft</td>\n    <td>Guaranteed by S3</td>\n  </tr>\n  <tr>\n    <td>Inter-AZ Networking Fees</td>\n    <td>No</td>\n    <td colspan=\"4\">Yes</td>\n    <td>No</td>\n  </tr>\n</table>\n\n\n> [1] Apache Kafka Compatibility's definition is coming from this [blog](https://www.kai-waehner.de/blog/2021/05/09/kafka-api-de-facto-standard-event-streaming-like-amazon-s3-object-storage/).\n\n> [2] EBS Durability: On Azure, GCP, and Alibaba Cloud, Regional EBS replicas span multiple AZs. On AWS, ensure durability by double writing to EBS and S3 Express One Zone in different AZs.\n\n## 🔶 Why AutoMQ\n\n- **Cost effective**: The first true cloud-native streaming storage system, designed for optimal cost and efficiency on the cloud. Refer to [this report](https://docs.automq.com/docs/automq-opensource/EV6mwoC95ihwRckMsUKcppnqnJb) to see how we cut Apache Kafka billing by 90% on the cloud.\n- **High Reliability**: Leverage cloud-shared storage services(EBS and S3) to achieve zero RPO, RTO in seconds and 99.999999999% durability.\n- **Serverless**:\n  - Auto Scaling: Monitor cluster metrics and automatically scale in/out to align with your workload, enabling a pay-as-you-go model.\n  - Scaling in seconds: The computing layer (broker) is stateless and can scale in/out within seconds, making AutoMQ a truly serverless solution.\n  - Infinite scalable: Utilize cloud object storage as the primary storage solution, eliminating concerns about storage capacity.\n- **Manage-less**: The built-in auto-balancer component automatically schedules partitions and network traffic between brokers, eliminating manual partition reassignment.\n- **High performance**:\n  - Low latency: Accelerate writing with high-performance EBS as WAL, achieving single-digit millisecond latency.\n  - High throughput: Leverage pre-fetching, batch processing, and parallel technologies to maximize the capabilities of cloud object storage.\n  > Refer to the [AutoMQ Performance White Paper](https://docs.automq.com/docs/automq-opensource/IJLQwnVROiS5cUkXfF0cuHnWnNd) to see how we achieve this.\n- **A superior alternative to Apache Kafka**: 100% compatible with Apache Kafka and does not lose any key features, but cheaper and better.\n\n## ✨Architecture\n\n![image](./docs/images/automq_vs_kafka.gif)\n\nAutoMQ adopts a Shared-Storage architecture, replacing the storage layer of Apache Kafka with a shared streaming storage library called [S3Stream](https://github.com/AutoMQ/automq/tree/main/s3stream) in a storage-compute separation manner, making the Broker completely stateless.\n\nCompared to the classic Kafka Shared-Nothing or Tiered-Storage architectures, AutoMQ's computing layer (Broker) is truly stateless, enabling features such as Auto-Scaling, Self-Balancing, and Partition Reassignment in Seconds that significantly reduce costs and improve efficiency.\n\n## ⛄ Get started with AutoMQ\n\n### Deploy Locally on a Single Host\n```\ncurl https://download.automq.com/community_edition/standalone_deployment/install_run.sh | bash\n```\n\nThe easiest way to run AutoMQ. You can experience features like **Partition Reassignment in Seconds** and **Continuous Self-Balancing** in your local machine. [Learn more](https://docs.automq.com/docs/automq-opensource/EsUBwQei4ilCDjkWb8WcbOZInwc)\n\nThere are more deployment options available:\n- [Deploy on Linux with 5 Nodes](https://docs.automq.com/docs/automq-opensource/IyXrw3lHriVPdQkQLDvcPGQdnNh)\n- [Deploy on Kubernetes(Enterprise Edition Only)](https://docs.automq.com/docs/automq-opensource/KJtLwvdaPi7oznkX3lkcCR7fnte)\n- [Runs on Ceph / MinIO / CubeFS / HDFS](https://docs.automq.com/docs/automq-opensource/RexrwfhKuiGChfk237QcEBIwnND)\n- [Try AutoMQ on Alibaba Cloud Marketplace (Two Weeks Free Trial)](https://market.aliyun.com/products/55530001/cmgj00065841.html)\n- [Try AutoMQ on AWS Marketplace (Two Weeks Free Trial)](https://docs.automq.com/automq-cloud/getting-started/install-byoc-environment/aws/install-env-from-marketplace)\n\n## 💬 Community\nYou can join the following groups or channels to discuss or ask questions about AutoMQ:\n- Ask questions or report a bug by [GitHub Issues](https://github.com/AutoMQ/automq/issues)\n- Discuss about AutoMQ or Kafka by [Slack](https://join.slack.com/t/automq/shared_invite/zt-29h17vye9-thf31ebIVL9oXuRdACnOIA) or [Wechat Group](docs/images/automq-wechat.png)\n\n\n## 👥 How to contribute\nIf you've found a problem with AutoMQ, please open a [GitHub Issues](https://github.com/AutoMQ/automq/issues).\nTo contribute to AutoMQ please see [Code of Conduct](CODE_OF_CONDUCT.md) and [Contributing Guide](CONTRIBUTING_GUIDE.md).\nWe have a list of [good first issues](https://github.com/AutoMQ/automq/issues?q=is%3Aissue+is%3Aopen+label%3A%22good+first+issue%22) that help you to get started, gain experience, and get familiar with our contribution process. To claim one, simply reply with 'pick up' in the issue and the AutoMQ maintainers will assign the issue to you. If you have any questions about the 'good first issue' please feel free to ask. We will do our best to clarify any doubts you may have.\n\n## 👍 AutoMQ Business Edition\nThe business edition of AutoMQ provides a powerful and easy-to-use control plane to help you manage clusters effortlessly. Meanwhile, the control plane is more powerful in terms of availability and observability compared to the community edition.\n\n> You can check the difference between the community and business editions [here](https://www.automq.com/product).\n\n\n<b>Watch the following video and refer to our [docs](https://docs.automq.com/automq-cloud/getting-started/install-byoc-environment/aws/install-env-via-terraform-module) to see how to deploy AutoMQ Business Edition with 2 weeks free license for PoC.</b>\n\n<b> ⬇️ ⬇️ ⬇️ </b>\n\n[![Deploy AutoMQ Business Edition with Terraform](https://img.youtube.com/vi/O40zp81x97w/0.jpg)](https://www.youtube.com/watch?v=O40zp81x97w)\n\n\n\n### Free trial of AutoMQ Business Edition\nTo allow users to experience the capabilities of the AutoMQ business edition without any barriers, click [here](https://www.automq.com/quick-start#Cloud?utm_source=github_automq_cloud) to apply for a no-obligation cluster trial, and note `AutoMQ Cloud Free Trial` in the message input box. We will immediately initialize an AutoMQ Cloud control panel for you soon in the cloud and give you the address of the control panel. Then, you can use the control panel to create a AutoMQ cluster or perform operations like scale in/out. \n\nNo need to bind a credit card, no cost at all. We look forward to receiving valuable feedback from you to make our product better. If you want to proceed with a formal POC, you can also contact us through [Contact Us](https://automq66.feishu.cn/share/base/form/shrcnoqxslhYkujx6ULiMxOqkGh?utm_source=github_poc). We will further support your official POC.\n\n## 👥 AutoMQ Use Cases\n> Here are some of the users who have deployed AutoMQ in their production environments.\n\n[![Click on the link to learn more about AutoMQ user cases.](./docs/images/customer.jpeg \"AutoMQ Uses Case\")](https://www.automq.com/customer)\n\n## 🐱 The relationship with Apache Kafka\n\nAutoMQ is a fork of the open-source [Apache Kafka](https://github.com/apache/kafka). Based on the Apache Kafka codebase, we found an aspect at the LogSegment level, and replaced Kafka's storage layer with our self-developed cloud-first stream storage engine, [S3Stream](https://github.com/AutoMQ/automq/tree/main/s3stream). This engine can provide customers with high-performance, low-cost, and unlimited stream storage capabilities based on cloud storage like EBS WAL and S3. As such, AutoMQ completely retains the code of Kafka's computing layer and is 100% fully compatible with Apache Kafka. We appreciate the work done by the Apache Kafka community and will continue to embrace the Kafka community.\n\n## 🙋 Contact Us\nWant to learn more, [Talk with our product experts](https://automq66.feishu.cn/share/base/form/shrcnoqxslhYkujx6ULiMxOqkGh).\n"
        },
        {
          "name": "Vagrantfile",
          "type": "blob",
          "size": 8.0771484375,
          "content": "# Licensed to the Apache Software Foundation (ASF) under one or more\n# contributor license agreements.  See the NOTICE file distributed with\n# this work for additional information regarding copyright ownership.\n# The ASF licenses this file to You under the Apache License, Version 2.0\n# (the \"License\"); you may not use this file except in compliance with\n# the License.  You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# -*- mode: ruby -*-\n# vi: set ft=ruby :\n\nrequire 'socket'\n\n# Vagrantfile API/syntax version. Don't touch unless you know what you're doing!\nVAGRANTFILE_API_VERSION = \"2\"\n\n# General config\nenable_dns = false\n# Override to false when bringing up a cluster on AWS\nenable_hostmanager = true\nenable_jmx = false\nnum_zookeepers = 1\nnum_brokers = 3\nnum_workers = 0 # Generic workers that get the code, but don't start any services\nram_megabytes = 1280\nbase_box = \"ubuntu/trusty64\"\n\n# EC2\nec2_access_key = ENV['AWS_ACCESS_KEY']\nec2_secret_key = ENV['AWS_SECRET_KEY']\nec2_session_token = ENV['AWS_SESSION_TOKEN']\nec2_keypair_name = nil\nec2_keypair_file = nil\n\nec2_region = \"us-east-1\"\nec2_az = nil # Uses set by AWS\nec2_ami = \"ami-29ebb519\"\nec2_instance_type = \"m3.medium\"\nec2_spot_instance = ENV['SPOT_INSTANCE'] ? ENV['SPOT_INSTANCE'] == 'true' : true\nec2_spot_max_price = \"0.113\"  # On-demand price for instance type\nec2_user = \"ubuntu\"\nec2_instance_name_prefix = \"kafka-vagrant\"\nec2_security_groups = nil\nec2_subnet_id = nil\n# Only override this by setting it to false if you're running in a VPC and you\n# are running Vagrant from within that VPC as well.\nec2_associate_public_ip = nil\nec2_iam_instance_profile_name = nil\n\nebs_volume_type = 'gp3'\n\njdk_major = '8'\njdk_full = '8u202-linux-x64'\n\nlocal_config_file = File.join(File.dirname(__FILE__), \"Vagrantfile.local\")\nif File.exists?(local_config_file) then\n  eval(File.read(local_config_file), binding, \"Vagrantfile.local\")\nend\n\n# override any instance type set by Vagrantfile.local or above via an environment variable\nif ENV['INSTANCE_TYPE'] then\n  ec2_instance_type = ENV['INSTANCE_TYPE']\nend\n\n# choose size based on overridden size\nif ec2_instance_type.start_with?(\"m3\") then\n  ebs_volume_size = 20\nelse\n  ebs_volume_size = 40\nend\n\n# TODO(ksweeney): RAM requirements are not empirical and can probably be significantly lowered.\nVagrant.configure(VAGRANTFILE_API_VERSION) do |config|\n  config.hostmanager.enabled = enable_hostmanager\n  config.hostmanager.manage_host = enable_dns\n  config.hostmanager.include_offline = false\n\n  ## Provider-specific global configs\n  config.vm.provider :virtualbox do |vb,override|\n    override.vm.box = base_box\n\n    override.hostmanager.ignore_private_ip = false\n\n    # Brokers started with the standard script currently set Xms and Xmx to 1G,\n    # plus we need some extra head room.\n    vb.customize [\"modifyvm\", :id, \"--memory\", ram_megabytes.to_s]\n\n    if Vagrant.has_plugin?(\"vagrant-cachier\")\n      override.cache.scope = :box\n    end\n  end\n\n  config.vm.provider :aws do |aws,override|\n    # The \"box\" is specified as an AMI\n    override.vm.box = \"dummy\"\n    override.vm.box_url = \"https://github.com/mitchellh/vagrant-aws/raw/master/dummy.box\"\n\n    cached_addresses = {}\n    # Use a custom resolver that SSH's into the machine and finds the IP address\n    # directly. This lets us get at the private IP address directly, avoiding\n    # some issues with using the default IP resolver, which uses the public IP\n    # address.\n    override.hostmanager.ip_resolver = proc do |vm, resolving_vm|\n      if !cached_addresses.has_key?(vm.name)\n        state_id = vm.state.id\n        if state_id != :not_created && state_id != :stopped && vm.communicate.ready?\n          contents = ''\n          vm.communicate.execute(\"/sbin/ifconfig eth0 | grep 'inet addr' | tail -n 1 | egrep -o '[0-9\\.]+' | head -n 1 2>&1\") do |type, data|\n            contents << data\n          end\n          cached_addresses[vm.name] = contents.split(\"\\n\").first[/(\\d+\\.\\d+\\.\\d+\\.\\d+)/, 1]\n        else\n          cached_addresses[vm.name] = nil\n        end\n      end\n      cached_addresses[vm.name]\n    end\n\n    override.ssh.username = ec2_user\n    override.ssh.private_key_path = ec2_keypair_file\n\n    aws.access_key_id = ec2_access_key\n    aws.secret_access_key = ec2_secret_key\n    aws.session_token = ec2_session_token\n    aws.keypair_name = ec2_keypair_name\n\n    aws.region = ec2_region\n    aws.availability_zone = ec2_az\n    aws.instance_type = ec2_instance_type\n\n    aws.ami = ec2_ami\n    aws.security_groups = ec2_security_groups\n    aws.subnet_id = ec2_subnet_id\n    aws.block_device_mapping = [{ 'DeviceName' => '/dev/sda1', 'Ebs.VolumeType' => ebs_volume_type, 'Ebs.VolumeSize' => ebs_volume_size }]\n    # If a subnet is specified, default to turning on a public IP unless the\n    # user explicitly specifies the option. Without a public IP, Vagrant won't\n    # be able to SSH into the hosts unless Vagrant is also running in the VPC.\n    if ec2_associate_public_ip.nil?\n      aws.associate_public_ip = true unless ec2_subnet_id.nil?\n    else\n      aws.associate_public_ip = ec2_associate_public_ip\n    end\n    aws.region_config ec2_region do |region|\n      region.spot_instance = ec2_spot_instance\n      region.spot_max_price = ec2_spot_max_price\n    end\n    aws.iam_instance_profile_name = ec2_iam_instance_profile_name\n\n    # Exclude some directories that can grow very large from syncing\n    override.vm.synced_folder \".\", \"/vagrant\", type: \"rsync\", rsync__exclude: ['.git', 'core/data/', 'logs/', 'tests/results/', 'results/']\n  end\n\n  def name_node(node, name, ec2_instance_name_prefix)\n    node.vm.hostname = name\n    node.vm.provider :aws do |aws|\n      aws.tags = {\n        'Name' => ec2_instance_name_prefix + \"-\" + Socket.gethostname + \"-\" + name,\n        'JenkinsBuildUrl' => ENV['BUILD_URL']\n      }\n    end\n  end\n\n  def assign_local_ip(node, ip_address)\n    node.vm.provider :virtualbox do |vb,override|\n      override.vm.network :private_network, ip: ip_address\n    end\n  end\n\n  ## Cluster definition\n  zookeepers = []\n  (1..num_zookeepers).each { |i|\n    name = \"zk\" + i.to_s\n    zookeepers.push(name)\n    config.vm.define name do |zookeeper|\n      name_node(zookeeper, name, ec2_instance_name_prefix)\n      ip_address = \"192.168.50.\" + (10 + i).to_s\n      assign_local_ip(zookeeper, ip_address)\n      zookeeper.vm.provision \"shell\", path: \"vagrant/base.sh\", env: {\"JDK_MAJOR\" => jdk_major, \"JDK_FULL\" => jdk_full}\n      zk_jmx_port = enable_jmx ? (8000 + i).to_s : \"\"\n      zookeeper.vm.provision \"shell\", path: \"vagrant/zk.sh\", :args => [i.to_s, num_zookeepers, zk_jmx_port]\n    end\n  }\n\n  (1..num_brokers).each { |i|\n    name = \"broker\" + i.to_s\n    config.vm.define name do |broker|\n      name_node(broker, name, ec2_instance_name_prefix)\n      ip_address = \"192.168.50.\" + (50 + i).to_s\n      assign_local_ip(broker, ip_address)\n      # We need to be careful about what we list as the publicly routable\n      # address since this is registered in ZK and handed out to clients. If\n      # host DNS isn't setup, we shouldn't use hostnames -- IP addresses must be\n      # used to support clients running on the host.\n      zookeeper_connect = zookeepers.map{ |zk_addr| zk_addr + \":2181\"}.join(\",\")\n      broker.vm.provision \"shell\", path: \"vagrant/base.sh\", env: {\"JDK_MAJOR\" => jdk_major, \"JDK_FULL\" => jdk_full}\n      kafka_jmx_port = enable_jmx ? (9000 + i).to_s : \"\"\n      broker.vm.provision \"shell\", path: \"vagrant/broker.sh\", :args => [i.to_s, enable_dns ? name : ip_address, zookeeper_connect, kafka_jmx_port]\n    end\n  }\n\n  (1..num_workers).each { |i|\n    name = \"worker\" + i.to_s\n    config.vm.define name do |worker|\n      name_node(worker, name, ec2_instance_name_prefix)\n      ip_address = \"192.168.50.\" + (100 + i).to_s\n      assign_local_ip(worker, ip_address)\n      worker.vm.provision \"shell\", path: \"vagrant/base.sh\", env: {\"JDK_MAJOR\" => jdk_major, \"JDK_FULL\" => jdk_full}\n    end\n  }\n\nend\n"
        },
        {
          "name": "automq-shell",
          "type": "tree",
          "content": null
        },
        {
          "name": "automq_release.py",
          "type": "blob",
          "size": 7.2470703125,
          "content": "# Licensed to the Apache Software Foundation (ASF) under one or more\n# contributor license agreements.  See the NOTICE file distributed with\n# this work for additional information regarding copyright ownership.\n# The ASF licenses this file to You under the Apache License, Version 2.0\n# (the \"License\"); you may not use this file except in compliance with\n# the License.  You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# !/usr/bin/python3\n\nimport re\nimport subprocess\nimport sys\nimport tempfile\nimport time\nfrom pathlib import Path\nimport argparse\n\nmain_branch = \"develop\"\n\n\ndef fail(msg):\n    print(msg)\n    sys.exit(1)\n\n\ndef print_output(output):\n    if output is None or len(output) == 0:\n        return\n    for line in output.split('\\n'):\n        print(\">\", line)\n\n\ndef cmd(action, cmd_arg, *args, **kwargs):\n    if isinstance(cmd_arg, str) and not kwargs.get(\"shell\", False):\n        cmd_arg = cmd_arg.split()\n    allow_failure = kwargs.pop(\"allow_failure\", False)\n    num_retries = kwargs.pop(\"num_retries\", 0)\n\n    stdin_log = \"\"\n    if \"stdin\" in kwargs and isinstance(kwargs[\"stdin\"], str):\n        stdin_log = \"--> \" + kwargs[\"stdin\"]\n        stdin = tempfile.TemporaryFile()\n        stdin.write(kwargs[\"stdin\"].encode('utf-8'))\n        stdin.seek(0)\n        kwargs[\"stdin\"] = stdin\n\n    print(action, cmd_arg, stdin_log)\n    try:\n        output = subprocess.check_output(cmd_arg, *args, stderr=subprocess.STDOUT, **kwargs)\n        print_output(output.decode('utf-8'))\n    except subprocess.CalledProcessError as e:\n        print_output(e.output.decode('utf-8'))\n\n        if num_retries > 0:\n            kwargs['num_retries'] = num_retries - 1\n            kwargs['allow_failure'] = allow_failure\n            print(\"Retrying... %d remaining retries\" % (num_retries - 1))\n            time.sleep(4. / (num_retries + 1))  # e.g., if retries=3, sleep for 1s, 1.3s, 2s\n            return cmd(action, cmd_arg, *args, **kwargs)\n\n        if allow_failure:\n            return\n\n        print(\"*************************************************\")\n        print(\"*** First command failure occurred here.      ***\")\n        print(\"*************************************************\")\n        fail(\"\")\n\n\ndef cmd_output(cmd, *args, **kwargs):\n    if isinstance(cmd, str):\n        cmd = cmd.split()\n    return subprocess.check_output(cmd, *args, stderr=subprocess.STDOUT, **kwargs).decode('utf-8')\n\n\ndef replace(path, pattern, replacement):\n    updated = []\n    with open(path, 'r') as f:\n        for line in f:\n            updated.append((replacement + '\\n') if line.startswith(pattern) else line)\n\n    with open(path, 'w') as f:\n        for line in updated:\n            f.write(line)\n\n\ndef regexReplace(path, pattern, replacement):\n    updated = []\n    with open(path, 'r') as f:\n        for line in f:\n            updated.append(re.sub(pattern, replacement, line))\n\n    with open(path, 'w') as f:\n        for line in updated:\n            f.write(line)\n\n\ndef is_tool(name):\n    \"\"\"Check whether `name` is on PATH and marked as executable.\"\"\"\n\n    # from whichcraft import which\n    from shutil import which\n\n    return which(name) is not None\n\n\ndef check_tools(tools):\n    \"\"\"Check whether tools are available in the system\"\"\"\n    for tool in tools:\n        if not is_tool(tool):\n            raise Exception(\"%s is not installed\" % tool)\n\n\ndef is_valid_kafka_tag(tag):\n    \"\"\"Check whether the tag is a valid kafka tag\"\"\"\n    pattern = r'^\\d+\\.\\d+\\.\\d+(-rc\\d+)?$'\n    return re.match(pattern, tag) is not None\n\n\ndef is_valid_s3stream_tag(tag):\n    \"\"\"Check whether the tag is a valid s3stream tag\"\"\"\n    pattern = r'^\\d+\\.\\d+\\.\\d+-s3stream(-rc\\d+)?$'\n    return re.match(pattern, tag) is not None\n\n\ndef get_project_path():\n    # Path object for the current file\n    current_file = Path(__file__)\n    # Get the directory of the current file\n    return current_file.parent\n\n\ndef do_release(tag, s3stream_tag):\n    new_branch = f\"release-{tag}-{int(time.time())}\"\n    cmd(f\"Checking out to release branch\", f'git checkout --quiet -b {new_branch}')\n\n    print(\"Updating docker compose\")\n    regexReplace(\"docker/docker-compose.yaml\", \"image: automqinc/automq:.*$\", f\"image: automqinc/automq:{tag}\")\n    replace(\"gradle/dependencies.gradle\", '    branch = \"main\"', f'    require \"{s3stream_tag}\"')\n\n    cmd(\"Committing changes\", [\"git\", \"commit\", \"--all\", \"--gpg-sign\", \"--signoff\", \"--message\", f\"ci: bump version to {tag}\"])\n    cmd(\"Pushing changes\", [\"git\", \"push\", \"--quiet\", \"origin\", new_branch])\n\n    cmd(\"Tagging %s\" % tag, [\"git\", \"tag\", \"--sign\", \"--annotate\", tag, \"--message\", f\"release {tag}\"])\n    cmd(\"Pushing tag %s\" % tag, [\"git\", \"push\", \"--quiet\", \"origin\", tag])\n\n    return new_branch\n\n\ndef tag_exists(tag):\n    maybe_tag = cmd_output(f'git tag --list {tag}').strip()\n    return len(maybe_tag) > 0\n\n\ndef check_before_started(tag, s3stream_tag):\n    check_tools([\"git\"])\n    cmd(\"Fetching latest code\", 'git fetch origin')\n    cmd(\"Checking workspace clean\", 'git diff-index --quiet HEAD --')\n    starting_branch = cmd_output('git rev-parse --abbrev-ref HEAD').strip()\n    if starting_branch != main_branch:\n        fail(f\"You must run this script on the {main_branch} branch. current: {starting_branch}\")\n    cmd(f\"Checking branch {main_branch} up-to-date\", f'git diff origin/{main_branch}...{main_branch} --quiet')\n    cmd(f\"Checking tag {s3stream_tag} exists\", f'git ls-remote --quiet --exit-code --tags git@github.com:AutoMQ/automq-for-rocketmq.git {s3stream_tag}')\n    cmd(f\"Checking tag {tag} not exists in remote\", f'! git ls-remote --quiet --exit-code --tags origin {tag}', shell=True)\n    print(f\"Checking tag {tag} not exists in local\")\n    if tag_exists(tag):\n        fail(f\"Tag {tag} already exists. Please delete it before running this script.\")\n\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser(description='release AutoMQ Kafka')\n    parser.add_argument(\"--kafka-tag\", required=True, type=str,\n                        help=\"release tag, which should be in the format of 'x.y.z' or 'x.y.z-rcN', e.g., 1.2.3 or 11.22.33-rc44\")\n    parser.add_argument(\"--s3stream-tag\", required=True, type=str,\n                        help=\"related s3stream tag, which should have been pushed to the AutoMQ/automq-for-rocketmq repository,\\\n                            and should be in the format of 'x.y.z-s3stream' or 'x.y.z-s3stream-rcN', e.g., 1.2.3-s3stream or 11.22.33-s3stream-rc44\")\n\n    # get tag from command line\n    args = parser.parse_args()\n    tag = args.kafka_tag\n    s3stream_tag = args.s3stream_tag\n    print(f\"=== try to release {tag} ===\")\n    if not is_valid_kafka_tag(tag):\n        fail(f\"Invalid tag {tag}\")\n    if not is_valid_s3stream_tag(s3stream_tag):\n        fail(f\"Invalid s3stream tag {s3stream_tag}\")\n    check_before_started(tag, s3stream_tag)\n    branch = do_release(tag, s3stream_tag)\n    print(f\"=== release {tag} done ===\")\n    print(f\"Please create a PR to merge release branch to {main_branch} visiting:\")\n    print(f\"    https://github.com/AutoMQ/automq-for-kafka/pull/new/{main_branch}...{branch}\")\n"
        },
        {
          "name": "bin",
          "type": "tree",
          "content": null
        },
        {
          "name": "build.gradle",
          "type": "blob",
          "size": 113.2998046875,
          "content": "// Licensed to the Apache Software Foundation (ASF) under one or more\n// contributor license agreements.  See the NOTICE file distributed with\n// this work for additional information regarding copyright ownership.\n// The ASF licenses this file to You under the Apache License, Version 2.0\n// (the \"License\"); you may not use this file except in compliance with\n// the License.  You may obtain a copy of the License at\n//\n//    http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\nimport org.ajoberstar.grgit.Grgit\nimport org.gradle.api.JavaVersion\n\nimport java.nio.charset.StandardCharsets\n\nbuildscript {\n  repositories {\n    mavenCentral()\n  }\n  apply from: \"$rootDir/gradle/dependencies.gradle\"\n\n  dependencies {\n    // For Apache Rat plugin to ignore non-Git files\n    classpath \"org.ajoberstar.grgit:grgit-core:$versions.grgit\"\n  }\n}\n\nplugins {\n  id 'com.github.ben-manes.versions' version '0.48.0'\n  id 'idea'\n  id 'jacoco'\n  id 'java-library'\n  id 'org.owasp.dependencycheck' version '8.2.1'\n  id 'org.nosphere.apache.rat' version \"0.8.1\"\n  id \"io.swagger.core.v3.swagger-gradle-plugin\" version \"${swaggerVersion}\"\n\n  // When updating the spotbugs gradle plugin, check if it already\n  // includes spotbugs version 4.7.4, in which case CVE-2022-42920 can\n  // be dropped from gradle/resources/dependencycheck-suppressions.xml\n  id \"com.github.spotbugs\" version '5.1.3' apply false\n  id 'org.scoverage' version '8.0.3' apply false\n  id 'io.github.goooler.shadow' version '8.1.3' apply false\n  //  Spotless 6.13.0 has issue with Java 21 (see https://github.com/diffplug/spotless/pull/1920), and Spotless 6.14.0+ requires JRE 11\n  //  We are going to drop JDK8 support. Hence, the spotless is upgrade to newest version and be applied only if the build env is compatible with JDK 11.\n  //  spotless 6.15.0+ has issue in runtime with JDK8 even through we define it with `apply:false`. see https://github.com/diffplug/spotless/issues/2156 for more details\n  id 'com.diffplug.spotless' version \"6.14.0\" apply false\n}\n\next {\n  gradleVersion = versions.gradle\n  minJavaVersion = 11\n  buildVersionFileName = \"kafka-version.properties\"\n\n  defaultMaxHeapSize = \"2g\"\n  defaultJvmArgs = [\"-Xss4m\", \"-XX:+UseParallelGC\"]\n\n  // \"JEP 403: Strongly Encapsulate JDK Internals\" causes some tests to fail when they try\n  // to access internals (often via mocking libraries). We use `--add-opens` as a workaround\n  // for now and we'll fix it properly (where possible) via KAFKA-13275.\n  if (JavaVersion.current().isCompatibleWith(JavaVersion.VERSION_16))\n    defaultJvmArgs.addAll(\n      \"--add-opens=java.base/java.io=ALL-UNNAMED\",\n      \"--add-opens=java.base/java.lang=ALL-UNNAMED\",\n      \"--add-opens=java.base/java.nio=ALL-UNNAMED\",\n      \"--add-opens=java.base/java.nio.file=ALL-UNNAMED\",\n      \"--add-opens=java.base/java.util=ALL-UNNAMED\",\n      \"--add-opens=java.base/java.util.concurrent=ALL-UNNAMED\",\n      \"--add-opens=java.base/java.util.regex=ALL-UNNAMED\",\n      \"--add-opens=java.base/java.util.stream=ALL-UNNAMED\",\n      \"--add-opens=java.base/java.text=ALL-UNNAMED\",\n      \"--add-opens=java.base/java.time=ALL-UNNAMED\",\n      \"--add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED\"\n    )\n\n  maxTestForks = project.hasProperty('maxParallelForks') ? maxParallelForks.toInteger() : Runtime.runtime.availableProcessors()\n  maxScalacThreads = project.hasProperty('maxScalacThreads') ? maxScalacThreads.toInteger() :\n      Math.min(Runtime.runtime.availableProcessors(), 8)\n  userIgnoreFailures = project.hasProperty('ignoreFailures') ? ignoreFailures : false\n\n  userMaxTestRetries = project.hasProperty('maxTestRetries') ? maxTestRetries.toInteger() : 0\n  userMaxTestRetryFailures = project.hasProperty('maxTestRetryFailures') ? maxTestRetryFailures.toInteger() : 0\n\n  skipSigning = project.hasProperty('skipSigning') && skipSigning.toBoolean()\n  shouldSign = !skipSigning && !version.endsWith(\"SNAPSHOT\")\n\n  mavenUrl = project.hasProperty('mavenUrl') ? project.mavenUrl : ''\n  mavenUsername = project.hasProperty('mavenUsername') ? project.mavenUsername : ''\n  mavenPassword = project.hasProperty('mavenPassword') ? project.mavenPassword : ''\n\n  userShowStandardStreams = project.hasProperty(\"showStandardStreams\") ? showStandardStreams : null\n\n  userTestLoggingEvents = project.hasProperty(\"testLoggingEvents\") ? Arrays.asList(testLoggingEvents.split(\",\")) : null\n\n  userEnableTestCoverage = project.hasProperty(\"enableTestCoverage\") ? enableTestCoverage : false\n\n  userKeepAliveModeString = project.hasProperty(\"keepAliveMode\") ? keepAliveMode : \"daemon\"\n  userKeepAliveMode = KeepAliveMode.values().find(m -> m.name().toLowerCase().equals(userKeepAliveModeString))\n  if (userKeepAliveMode == null) {\n    def keepAliveValues = KeepAliveMode.values().collect(m -> m.name.toLowerCase())\n    throw new GradleException(\"Unexpected value for keepAliveMode property. Expected one of $keepAliveValues, but received: $userKeepAliveModeString\")\n  }\n\n  // See README.md for details on this option and the reasoning for the default\n  userScalaOptimizerMode = project.hasProperty(\"scalaOptimizerMode\") ? scalaOptimizerMode : \"inline-kafka\"\n  def scalaOptimizerValues = [\"none\", \"method\", \"inline-kafka\", \"inline-scala\"]\n  if (!scalaOptimizerValues.contains(userScalaOptimizerMode))\n    throw new GradleException(\"Unexpected value for scalaOptimizerMode property. Expected one of $scalaOptimizerValues, but received: $userScalaOptimizerMode\")\n\n  generatedDocsDir = new File(\"${project.rootDir}/docs/generated\")\n  repo = file(\"$rootDir/.git\").isDirectory() ? Grgit.open(currentDir: project.getRootDir()) : null\n\n  commitId = determineCommitId()\n\n  addParametersForTests = { name, options ->\n    // -parameters generates arguments with parameter names in TestInfo#getDisplayName.\n    // ref: https://github.com/junit-team/junit5/blob/4c0dddad1b96d4a20e92a2cd583954643ac56ac0/junit-jupiter-params/src/main/java/org/junit/jupiter/params/ParameterizedTest.java#L161-L164\n    if (name == \"compileTestJava\" || name == \"compileTestScala\")\n      options.compilerArgs << \"-parameters\"\n  }\n}\n\nallprojects {\n\n  repositories {\n    mavenCentral()\n    maven {\n      url = uri(\"https://packages.confluent.io/maven/\")\n    }\n  }\n\n  dependencyUpdates {\n    revision=\"release\"\n    resolutionStrategy {\n      componentSelection { rules ->\n        rules.all { ComponentSelection selection ->\n          boolean rejected = ['snap', 'alpha', 'beta', 'rc', 'cr', 'm'].any { qualifier ->\n            selection.candidate.version ==~ /(?i).*[.-]${qualifier}[.\\d-]*/\n          }\n          if (rejected) {\n            selection.reject('Release candidate')\n          }\n        }\n      }\n    }\n  }\n\n  configurations.all {\n    // zinc is the Scala incremental compiler, it has a configuration for its own dependencies\n    // that are unrelated to the project dependencies, we should not change them\n    if (name != \"zinc\") {\n      resolutionStrategy {\n        force(\n          // be explicit about the javassist dependency version instead of relying on the transitive version\n          libs.javassist,\n          // ensure we have a single version in the classpath despite transitive dependencies\n          libs.scalaLibrary,\n          libs.scalaReflect,\n          libs.jacksonAnnotations,\n          // be explicit about the Netty dependency version instead of relying on the version set by\n          // ZooKeeper (potentially older and containing CVEs)\n          libs.nettyHandler,\n          libs.nettyTransportNativeEpoll,\n\t  // be explicit about the reload4j version instead of relying on the transitive versions\n\t  libs.reload4j\n        )\n      }\n    }\n  }\n  task printAllDependencies(type: DependencyReportTask) {}\n\n  tasks.withType(Javadoc) {\n    options.charSet = 'UTF-8'\n    options.docEncoding = 'UTF-8'\n    options.encoding = 'UTF-8'\n    options.memberLevel = JavadocMemberLevel.PUBLIC  // Document only public members/API\n    // Turn off doclint for now, see https://blog.joda.org/2014/02/turning-off-doclint-in-jdk-8-javadoc.html for rationale\n    options.addStringOption('Xdoclint:none', '-quiet')\n    // Javadoc warnings should fail the build in JDK 15+ https://bugs.openjdk.org/browse/JDK-8200363\n    options.addBooleanOption('Werror', JavaVersion.current().isCompatibleWith(JavaVersion.VERSION_15))\n\n    // The URL structure was changed to include the locale after Java 8\n    if (JavaVersion.current().isJava11Compatible())\n      options.links \"https://docs.oracle.com/en/java/javase/${JavaVersion.current().majorVersion}/docs/api/\"\n    else\n      options.links \"https://docs.oracle.com/javase/8/docs/api/\"\n  }\n}\n\ndef determineCommitId() {\n  def takeFromHash = 16\n  if (project.hasProperty('commitId')) {\n    commitId.take(takeFromHash)\n  } else if (repo != null) {\n    repo.head().id.take(takeFromHash)\n  } else {\n    \"unknown\"\n  }\n}\n\n\n\n\napply from: file('wrapper.gradle')\n\nrat.enabled = false\nprintln(\"Starting build with version $version (commit id ${commitId == null ? \"null\" : commitId.take(8)}) using Gradle $gradleVersion, Java ${JavaVersion.current()} and Scala ${versions.scala}\")\nprintln(\"Build properties: maxParallelForks=$maxTestForks, maxScalacThreads=$maxScalacThreads, maxTestRetries=$userMaxTestRetries\")\n\nsubprojects {\n\n  // enable running :dependencies task recursively on all subprojects\n  // eg: ./gradlew allDeps\n  task allDeps(type: DependencyReportTask) {}\n  // enable running :dependencyInsight task recursively on all subprojects\n  // eg: ./gradlew allDepInsight --configuration runtime --dependency com.fasterxml.jackson.core:jackson-databind\n  task allDepInsight(type: DependencyInsightReportTask) {showingAllVariants = false} doLast {}\n\n  apply plugin: 'java-library'\n  apply plugin: 'checkstyle'\n  apply plugin: \"com.github.spotbugs\"\n\n  // We use the shadow plugin for the jmh-benchmarks module and the `-all` jar can get pretty large, so\n  // don't publish it\n  def shouldPublish = !project.name.equals('jmh-benchmarks')\n  def shouldPublishWithShadow = (['clients'].contains(project.name))\n\n  if (shouldPublish) {\n    apply plugin: 'maven-publish'\n    apply plugin: 'signing'\n\n    // Add aliases for the task names used by the maven plugin for backwards compatibility\n    // The maven plugin was replaced by the maven-publish plugin in Gradle 7.0\n    tasks.register('install').configure { dependsOn(publishToMavenLocal) }\n    tasks.register('uploadArchives').configure { dependsOn(publish) }\n  }\n\n  // apply the eclipse plugin only to subprojects that hold code. 'connect' is just a folder.\n  if (!project.name.equals('connect')) {\n    apply plugin: 'eclipse'\n    fineTuneEclipseClasspathFile(eclipse, project)\n  }\n\n  java {\n    consistentResolution {\n      // resolve the compileClasspath and then \"inject\" the result of resolution as strict constraints into the runtimeClasspath\n      useCompileClasspathVersions()\n    }\n  }\n\n  tasks.withType(JavaCompile) {\n    options.encoding = 'UTF-8'\n    options.compilerArgs << \"-Xlint:all\"\n    // temporary exclusions until all the warnings are fixed\n    if (!project.path.startsWith(\":connect\") && !project.path.startsWith(\":storage\"))\n      options.compilerArgs << \"-Xlint:-rawtypes\"\n    options.compilerArgs << \"-Xlint:-serial\"\n    options.compilerArgs << \"-Xlint:-try\"\n    options.compilerArgs << \"-Werror\"\n\n    // --release is the recommended way to select the target release, but it's only supported in Java 9 so we also\n    // set --source and --target via `sourceCompatibility` and `targetCompatibility` a couple of lines below\n    if (JavaVersion.current().isJava9Compatible())\n      options.release = minJavaVersion\n    // --source/--target 8 is deprecated in Java 20, suppress warning until Java 8 support is dropped in Kafka 4.0\n    if (JavaVersion.current().isCompatibleWith(JavaVersion.VERSION_20))\n      options.compilerArgs << \"-Xlint:-options\"\n\n    addParametersForTests(name, options)\n  }\n\n  java {\n    // We should only set this if Java version is < 9 (--release is recommended for >= 9), but the Scala plugin for IntelliJ sets\n    // `-target` incorrectly if this is unset\n    sourceCompatibility = minJavaVersion\n    targetCompatibility = minJavaVersion\n  }\n\n  if (shouldPublish) {\n\n    publishing {\n      repositories {\n        // To test locally, invoke gradlew with `-PmavenUrl=file:///some/local/path`\n        maven {\n          url = mavenUrl\n          credentials {\n            username = mavenUsername\n            password = mavenPassword\n          }\n        }\n      }\n      publications {\n        mavenJava(MavenPublication) {\n          if (!shouldPublishWithShadow) {\n            from components.java\n          } else {\n            apply plugin: 'io.github.goooler.shadow'\n            project.shadow.component(mavenJava)\n\n            // Fix for avoiding inclusion of runtime dependencies marked as 'shadow' in MANIFEST Class-Path.\n            // https://github.com/johnrengelman/shadow/issues/324\n            afterEvaluate {\n              pom.withXml { xml ->\n                if (xml.asNode().get('dependencies') == null) {\n                  xml.asNode().appendNode('dependencies')\n                }\n                def dependenciesNode = xml.asNode().get('dependencies').get(0)\n                project.configurations.shadowed.allDependencies.each {\n                  def dependencyNode = dependenciesNode.appendNode('dependency')\n                  dependencyNode.appendNode('groupId', it.group)\n                  dependencyNode.appendNode('artifactId', it.name)\n                  dependencyNode.appendNode('version', it.version)\n                  dependencyNode.appendNode('scope', 'runtime')\n                }\n              }\n            }\n          }\n\n          afterEvaluate {\n            [\"srcJar\", \"javadocJar\", \"scaladocJar\", \"testJar\", \"testSrcJar\"].forEach { taskName ->\n              def task = tasks.findByName(taskName)\n              if (task != null)\n                artifact task\n            }\n\n            artifactId = base.archivesName.get()\n            pom {\n              name = 'Apache Kafka'\n              url = 'https://kafka.apache.org'\n              licenses {\n                license {\n                  name = 'The Apache License, Version 2.0'\n                  url = 'http://www.apache.org/licenses/LICENSE-2.0.txt'\n                  distribution = 'repo'\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n\n    if (shouldSign) {\n      signing {\n        sign publishing.publications.mavenJava\n      }\n    }\n  }\n\n  def testLoggingEvents = [\"passed\", \"skipped\", \"failed\"]\n  def testShowStandardStreams = false\n  def testExceptionFormat = 'full'\n  // Gradle built-in logging only supports sending test output to stdout, which generates a lot\n  // of noise, especially for passing tests. We really only want output for failed tests. This\n  // hooks into the output and logs it (so we don't have to buffer it all in memory) and only\n  // saves the output for failing tests. Directory and filenames are such that you can, e.g.,\n  // create a Jenkins rule to collect failed test output.\n  def logTestStdout = {\n    def testId = { TestDescriptor descriptor ->\n      \"${descriptor.className}.${descriptor.name}\".toString()\n    }\n\n    def logFiles = new HashMap<String, File>()\n    def logStreams = new HashMap<String, FileOutputStream>()\n    beforeTest { TestDescriptor td ->\n      def tid = testId(td)\n      // truncate the file name if it's too long\n      def logFile = new File(\n              \"${projectDir}/build/reports/testOutput/${tid.substring(0, Math.min(tid.size(),240))}.test.stdout\"\n      )\n      logFile.parentFile.mkdirs()\n      logFiles.put(tid, logFile)\n      logStreams.put(tid, new FileOutputStream(logFile))\n    }\n    onOutput { TestDescriptor td, TestOutputEvent toe ->\n      def tid = testId(td)\n      // Some output can happen outside the context of a specific test (e.g. at the class level)\n      // and beforeTest/afterTest seems to not be invoked for these cases (and similarly, there's\n      // a TestDescriptor hierarchy that includes the thread executing the test, Gradle tasks,\n      // etc). We see some of these in practice and it seems like something buggy in the Gradle\n      // test runner since we see it *before* any tests and it is frequently not related to any\n      // code in the test (best guess is that it is tail output from last test). We won't have\n      // an output file for these, so simply ignore them. If they become critical for debugging,\n      // they can be seen with showStandardStreams.\n      if (td.name == td.className || td.className == null) {\n        // silently ignore output unrelated to specific test methods\n        return\n      } else if (logStreams.get(tid) == null) {\n        println \"WARNING: unexpectedly got output for a test [${tid}]\" +\n                \" that we didn't previously see in the beforeTest hook.\" +\n                \" Message for debugging: [\" + toe.message + \"].\"\n        return\n      }\n      try {\n        logStreams.get(tid).write(toe.message.getBytes(StandardCharsets.UTF_8))\n      } catch (Exception e) {\n        println \"ERROR: Failed to write output for test ${tid}\"\n        e.printStackTrace()\n      }\n    }\n    afterTest { TestDescriptor td, TestResult tr ->\n      def tid = testId(td)\n      try {\n        logStreams.get(tid).close()\n        if (tr.resultType != TestResult.ResultType.FAILURE) {\n          logFiles.get(tid).delete()\n        } else {\n          def file = logFiles.get(tid)\n          println \"${tid} failed, log available in ${file}\"\n        }\n      } catch (Exception e) {\n        println \"ERROR: Failed to close stdout file for ${tid}\"\n        e.printStackTrace()\n      } finally {\n        logFiles.remove(tid)\n        logStreams.remove(tid)\n      }\n    }\n  }\n\n  // The suites are for running sets of tests in IDEs.\n  // Gradle will run each test class, so we exclude the suites to avoid redundantly running the tests twice.\n  def testsToExclude = ['**/*Suite.class']\n\n  test {\n    maxParallelForks = maxTestForks\n    ignoreFailures = userIgnoreFailures\n\n    maxHeapSize = defaultMaxHeapSize\n    jvmArgs = defaultJvmArgs\n\n    testLogging {\n      events = userTestLoggingEvents ?: testLoggingEvents\n      showStandardStreams = userShowStandardStreams ?: testShowStandardStreams\n      exceptionFormat = testExceptionFormat\n      displayGranularity = 0\n    }\n    logTestStdout.rehydrate(delegate, owner, this)()\n\n    exclude testsToExclude\n\n    useJUnitPlatform {\n      includeEngines 'junit-jupiter'\n    }\n\n    retry {\n      maxRetries = userMaxTestRetries\n      maxFailures = userMaxTestRetryFailures\n    }\n\n    // Allows devs to run tests in a loop to debug flaky tests. See README.\n    if (project.hasProperty(\"rerun-tests\")) {\n      outputs.upToDateWhen { false }\n    }\n  }\n\n  tasks.register('S3UnitTest', Test) {\n    description = 'Runs unit tests for Kafka on S3.'\n    dependsOn compileJava\n\n    maxParallelForks = maxTestForks\n    ignoreFailures = userIgnoreFailures\n\n    maxHeapSize = defaultMaxHeapSize\n    jvmArgs = defaultJvmArgs\n\n    testLogging {\n      events = userTestLoggingEvents ?: testLoggingEvents\n      showStandardStreams = userShowStandardStreams ?: testShowStandardStreams\n      exceptionFormat = testExceptionFormat\n      displayGranularity = 0\n    }\n    logTestStdout.rehydrate(delegate, owner, this)()\n\n    exclude testsToExclude\n\n    useJUnitPlatform {\n      includeTags 'S3Unit'\n    }\n\n    retry {\n      maxRetries = userMaxTestRetries\n      maxFailures = userMaxTestRetryFailures\n    }\n  }\n\n  task integrationTest(type: Test, dependsOn: compileJava) {\n    maxParallelForks = maxTestForks\n    ignoreFailures = userIgnoreFailures\n\n    // Increase heap size for integration tests\n    maxHeapSize = \"2560m\"\n    jvmArgs = defaultJvmArgs\n\n\n    testLogging {\n      events = userTestLoggingEvents ?: testLoggingEvents\n      showStandardStreams = userShowStandardStreams ?: testShowStandardStreams\n      exceptionFormat = testExceptionFormat\n      displayGranularity = 0\n    }\n    logTestStdout.rehydrate(delegate, owner, this)()\n\n    exclude testsToExclude\n\n    useJUnitPlatform {\n      includeTags \"integration\"\n      includeEngines 'junit-jupiter'\n    }\n\n    retry {\n      maxRetries = userMaxTestRetries\n      maxFailures = userMaxTestRetryFailures\n    }\n  }\n\n  task unitTest(type: Test, dependsOn: compileJava) {\n    maxParallelForks = maxTestForks\n    ignoreFailures = userIgnoreFailures\n\n    maxHeapSize = defaultMaxHeapSize\n    jvmArgs = defaultJvmArgs\n\n    testLogging {\n      events = userTestLoggingEvents ?: testLoggingEvents\n      showStandardStreams = userShowStandardStreams ?: testShowStandardStreams\n      exceptionFormat = testExceptionFormat\n      displayGranularity = 0\n    }\n    logTestStdout.rehydrate(delegate, owner, this)()\n\n    exclude testsToExclude\n\n    useJUnitPlatform {\n      excludeTags \"integration\"\n      includeEngines 'junit-jupiter'\n    }\n\n    retry {\n      maxRetries = userMaxTestRetries\n      maxFailures = userMaxTestRetryFailures\n    }\n  }\n\n  // remove test output from all test types\n  tasks.withType(Test).all { t ->\n    cleanTest {\n      delete t.reports.junitXml.outputLocation\n      delete t.reports.html.outputLocation\n    }\n  }\n\n  jar {\n    from \"$rootDir/LICENSE\"\n    from \"$rootDir/NOTICE\"\n  }\n\n  task srcJar(type: Jar) {\n    archiveClassifier = 'sources'\n    from \"$rootDir/LICENSE\"\n    from \"$rootDir/NOTICE\"\n    from sourceSets.main.allSource\n  }\n\n  task javadocJar(type: Jar, dependsOn: javadoc) {\n    archiveClassifier = 'javadoc'\n    from \"$rootDir/LICENSE\"\n    from \"$rootDir/NOTICE\"\n    from javadoc.destinationDir\n  }\n\n  task docsJar(dependsOn: javadocJar)\n\n  test.dependsOn('javadoc')\n\n  task systemTestLibs(dependsOn: jar)\n\n  if (!sourceSets.test.allSource.isEmpty()) {\n    task testJar(type: Jar) {\n      archiveClassifier = 'test'\n      from \"$rootDir/LICENSE\"\n      from \"$rootDir/NOTICE\"\n      from sourceSets.test.output\n      // The junit-platform.properties file is used for configuring and customizing the behavior of the JUnit platform.\n      // It should only apply to Kafka's own JUnit tests, and should not exist in the test JAR.\n      // If we include it in the test JAR, it could lead to conflicts with user configurations.\n      exclude 'junit-platform.properties'\n    }\n\n    task testSrcJar(type: Jar, dependsOn: testJar) {\n      archiveClassifier = 'test-sources'\n      from \"$rootDir/LICENSE\"\n      from \"$rootDir/NOTICE\"\n      from sourceSets.test.allSource\n    }\n\n  }\n\n  plugins.withType(ScalaPlugin) {\n\n    scala {\n      zincVersion = versions.zinc\n    }\n\n    task scaladocJar(type:Jar, dependsOn: scaladoc) {\n      archiveClassifier = 'scaladoc'\n      from \"$rootDir/LICENSE\"\n      from \"$rootDir/NOTICE\"\n      from scaladoc.destinationDir\n    }\n\n    //documentation task should also trigger building scala doc jar\n    docsJar.dependsOn scaladocJar\n\n  }\n\n  tasks.withType(ScalaCompile) {\n\n    scalaCompileOptions.keepAliveMode = userKeepAliveMode\n\n    scalaCompileOptions.additionalParameters = [\n      \"-deprecation:false\",\n      \"-unchecked\",\n      \"-encoding\", \"utf8\",\n      \"-Xlog-reflective-calls\",\n      \"-feature\",\n      \"-language:postfixOps\",\n      \"-language:implicitConversions\",\n      \"-language:existentials\",\n      \"-Ybackend-parallelism\", maxScalacThreads.toString(),\n      \"-Xlint:constant\",\n      \"-Xlint:delayedinit-select\",\n      \"-Xlint:doc-detached\",\n      \"-Xlint:missing-interpolator\",\n      \"-Xlint:nullary-unit\",\n      \"-Xlint:option-implicit\",\n      \"-Xlint:package-object-classes\",\n      \"-Xlint:poly-implicit-overload\",\n      \"-Xlint:private-shadow\",\n      \"-Xlint:stars-align\",\n      \"-Xlint:type-parameter-shadow\",\n      \"-Xlint:unused\"\n    ]\n\n    // See README.md for details on this option and the meaning of each value\n    if (userScalaOptimizerMode.equals(\"method\"))\n      scalaCompileOptions.additionalParameters += [\"-opt:l:method\"]\n    else if (userScalaOptimizerMode.startsWith(\"inline-\")) {\n      List<String> inlineFrom = [\"-opt-inline-from:org.apache.kafka.**\"]\n      if (project.name.equals('core'))\n        inlineFrom.add(\"-opt-inline-from:kafka.**\")\n      if (userScalaOptimizerMode.equals(\"inline-scala\"))\n        inlineFrom.add(\"-opt-inline-from:scala.**\")\n\n      scalaCompileOptions.additionalParameters += [\"-opt:l:inline\"]\n      scalaCompileOptions.additionalParameters += inlineFrom\n    }\n\n    if (versions.baseScala != '2.12') {\n      scalaCompileOptions.additionalParameters += [\"-opt-warnings\", \"-Xlint:strict-unsealed-patmat\"]\n      // Scala 2.13.2 introduces compiler warnings suppression, which is a pre-requisite for -Xfatal-warnings\n      scalaCompileOptions.additionalParameters += [\"-Xfatal-warnings\"]\n    }\n\n    // these options are valid for Scala versions < 2.13 only\n    // Scala 2.13 removes them, see https://github.com/scala/scala/pull/6502 and https://github.com/scala/scala/pull/5969\n    if (versions.baseScala == '2.12') {\n      scalaCompileOptions.additionalParameters += [\n        \"-Xlint:by-name-right-associative\",\n        \"-Xlint:nullary-override\",\n        \"-Xlint:unsound-match\"\n      ]\n    }\n\n    // Scalac 2.12 `-release` requires Java 9 or higher, but Scala 2.13 doesn't have that restriction\n    if (versions.baseScala == \"2.13\" || JavaVersion.current().isJava9Compatible())\n      scalaCompileOptions.additionalParameters += [\"-release\", String.valueOf(minJavaVersion)]\n\n    addParametersForTests(name, options)\n\n    configure(scalaCompileOptions.forkOptions) {\n      memoryMaximumSize = defaultMaxHeapSize\n      jvmArgs = defaultJvmArgs\n    }\n  }\n\n  checkstyle {\n    configDirectory = rootProject.layout.projectDirectory.dir(\"checkstyle\")\n    configProperties = checkstyleConfigProperties(\"import-control.xml\")\n    toolVersion = versions.checkstyle\n  }\n\n  configure(checkstyleMain) {\n    group = 'Verification'\n    description = 'Run checkstyle on all main Java sources'\n  }\n\n  configure(checkstyleTest) {\n    group = 'Verification'\n    description = 'Run checkstyle on all test Java sources'\n  }\n\n  test.dependsOn('checkstyleMain', 'checkstyleTest')\n\n  spotbugs {\n    toolVersion = versions.spotbugs\n    excludeFilter = file(\"$rootDir/gradle/spotbugs-exclude.xml\")\n    ignoreFailures = false\n  }\n  test.dependsOn('spotbugsMain')\n\n  tasks.withType(com.github.spotbugs.snom.SpotBugsTask).configureEach {\n    reports.configure {\n      // Continue supporting `xmlFindBugsReport` for compatibility\n      xml.enabled(project.hasProperty('xmlSpotBugsReport') || project.hasProperty('xmlFindBugsReport'))\n      html.enabled(!project.hasProperty('xmlSpotBugsReport') && !project.hasProperty('xmlFindBugsReport'))\n    }\n    maxHeapSize = defaultMaxHeapSize\n    jvmArgs = defaultJvmArgs\n  }\n\n  // Ignore core since its a scala project\n  if (it.path != ':core') {\n    if (userEnableTestCoverage) {\n      apply plugin: \"jacoco\"\n\n      jacoco {\n        toolVersion = versions.jacoco\n      }\n      \n      jacocoTestReport {\n        dependsOn tasks.test\n        sourceSets sourceSets.main\n        reports {\n          html.required = true\n          xml.required = true\n          csv.required = false\n        }\n      }\n\n    }\n  }\n\n  if (userEnableTestCoverage) {\n    def coverageGen = it.path == ':core' ? 'reportTestScoverage' : 'jacocoTestReport'\n    tasks.register('reportCoverage').configure { dependsOn(coverageGen) }\n  }\n\n  dependencyCheck {\n    suppressionFile = \"$rootDir/gradle/resources/dependencycheck-suppressions.xml\"\n    skipProjects = [ \":jmh-benchmarks\", \":trogdor\" ]\n    skipConfigurations = [ \"zinc\" ]\n  }\n  //  the task `removeUnusedImports` is implemented by google-java-format, \n  //  and unfortunately the google-java-format version used by spotless 6.14.0 can't work with JDK 21. \n  //  Hence, we apply spotless tasks only if the env is either JDK11 or JDK17\n  if ((JavaVersion.current().isJava11() || (JavaVersion.current() == JavaVersion.VERSION_17))) {\n    apply plugin: 'com.diffplug.spotless'\n    spotless {\n      java {\n        targetExclude('src/generated/**/*.java','src/generated-test/**/*.java')\n        importOrder('kafka', 'org.apache.kafka', 'com', 'net', 'org', 'java', 'javax', '', '\\\\#')\n        removeUnusedImports()\n      }\n    }\n  }\n}\n\ngradle.taskGraph.whenReady { taskGraph ->\n  taskGraph.getAllTasks().findAll { it.name.contains('spotbugsScoverage') || it.name.contains('spotbugsTest') }.each { task ->\n    task.enabled = false\n  }\n}\n\ndef fineTuneEclipseClasspathFile(eclipse, project) {\n  eclipse.classpath.file {\n    beforeMerged { cp ->\n      cp.entries.clear()\n      // for the core project add the directories defined under test/scala as separate source directories\n      if (project.name.equals('core')) {\n        cp.entries.add(new org.gradle.plugins.ide.eclipse.model.SourceFolder(\"src/test/scala/integration\", null))\n        cp.entries.add(new org.gradle.plugins.ide.eclipse.model.SourceFolder(\"src/test/scala/other\", null))\n        cp.entries.add(new org.gradle.plugins.ide.eclipse.model.SourceFolder(\"src/test/scala/unit\", null))\n      }\n    }\n    whenMerged { cp ->\n      // for the core project exclude the separate sub-directories defined under test/scala. These are added as source dirs above\n      if (project.name.equals('core')) {\n        cp.entries.findAll { it.kind == \"src\" && it.path.equals(\"src/test/scala\") }*.excludes = [\"integration/\", \"other/\", \"unit/\"]\n      }\n      /*\n       * Set all eclipse build output to go to 'build_eclipse' directory. This is to ensure that gradle and eclipse use different\n       * build output directories, and also avoid using the eclipse default of 'bin' which clashes with some of our script directories.\n       * https://discuss.gradle.org/t/eclipse-generated-files-should-be-put-in-the-same-place-as-the-gradle-generated-files/6986/2\n       */\n      cp.entries.findAll { it.kind == \"output\" }*.path = \"build_eclipse\"\n      /*\n       * Some projects have explicitly added test output dependencies. These are required for the gradle build but not required\n       * in Eclipse since the dependent projects are added as dependencies. So clean up these from the generated classpath.\n       */\n      cp.entries.removeAll { it.kind == \"lib\" && it.path.matches(\".*/build/(classes|resources)/test\") }\n    }\n  }\n}\n\ndef checkstyleConfigProperties(configFileName) {\n  [importControlFile: \"$configFileName\"]\n}\n\nif (userEnableTestCoverage) {\n  tasks.register('reportCoverage').configure { dependsOn(subprojects.reportCoverage) }\n}\n\ndef connectPkgs = [\n    'connect:api',\n    'connect:basic-auth-extension',\n    'connect:file',\n    'connect:json',\n    'connect:runtime',\n    'connect:test-plugins',\n    'connect:transforms',\n    'connect:mirror',\n    'connect:mirror-client'\n]\n\ntasks.create(name: \"jarConnect\", dependsOn: connectPkgs.collect { it + \":jar\" }) {}\n\ntasks.create(name: \"testConnect\", dependsOn: connectPkgs.collect { it + \":test\" }) {}\n\nproject(':server') {\n  base {\n    archivesName = \"kafka-server\"\n  }\n\n  dependencies {\n    implementation project(':clients')\n    implementation project(':server-common')\n    implementation project(':storage')\n    implementation project(':group-coordinator')\n    implementation project(':transaction-coordinator')\n    implementation project(':raft')\n    implementation libs.metrics\n    implementation libs.jacksonDatabind\n\n    implementation libs.slf4jApi\n\n    compileOnly libs.reload4j\n\n    testImplementation project(':clients').sourceSets.test.output\n\n    testImplementation libs.mockitoCore\n    testImplementation libs.junitJupiter\n    testImplementation libs.slf4jReload4j\n\n    testRuntimeOnly libs.junitPlatformLanucher\n  }\n\n  task createVersionFile() {\n    def receiptFile = file(\"$buildDir/kafka/$buildVersionFileName\")\n    inputs.property \"commitId\", commitId\n    inputs.property \"version\", version\n    outputs.file receiptFile\n\n    doLast {\n      def data = [\n        commitId: commitId,\n        version: version,\n      ]\n\n      receiptFile.parentFile.mkdirs()\n      def content = data.entrySet().collect { \"$it.key=$it.value\" }.sort().join(\"\\n\")\n      receiptFile.setText(content, \"ISO-8859-1\")\n    }\n  }\n\n  jar {\n    dependsOn createVersionFile\n    from(\"$buildDir\") {\n      include \"kafka/$buildVersionFileName\"\n    }\n  }\n\n  clean.doFirst {\n    delete \"$buildDir/kafka/\"\n  }\n\n  checkstyle {\n    configProperties = checkstyleConfigProperties(\"import-control-server.xml\")\n  }\n\n  javadoc {\n    enabled = false\n  }\n}\n\nproject(':core') {\n  apply plugin: 'scala'\n\n  // scaladoc generation is configured at the sub-module level with an artifacts\n  // block (cf. see streams-scala). If scaladoc generation is invoked explicitly\n  // for the `core` module, this ensures the generated jar doesn't include scaladoc\n  // files since the `core` module doesn't include public APIs.\n  scaladoc {\n    enabled = false\n  }\n  if (userEnableTestCoverage)\n    apply plugin: \"org.scoverage\"\n\n  base {\n    archivesName = \"kafka_${versions.baseScala}\"\n  }\n\n  dependencies {\n    // `core` is often used in users' tests, define the following dependencies as `api` for backwards compatibility\n    // even though the `core` module doesn't expose any public API\n    api project(':clients')\n    api libs.scalaLibrary\n\n    implementation project(':server-common')\n    implementation project(':group-coordinator:group-coordinator-api')\n    implementation project(':group-coordinator')\n    implementation project(':transaction-coordinator')\n    implementation project(':metadata')\n    implementation project(':storage:storage-api')\n    implementation project(':tools:tools-api')\n    implementation project(':raft')\n    implementation project(':storage')\n    implementation project(':server')\n    implementation project(':automq-shell')\n\n    implementation libs.argparse4j\n    implementation libs.commonsValidator\n    implementation libs.jacksonDatabind\n    implementation libs.jacksonModuleScala\n    implementation libs.jacksonDataformatCsv\n    implementation libs.jacksonJDK8Datatypes\n    implementation libs.joptSimple\n    implementation libs.jose4j\n    implementation libs.metrics\n    implementation libs.scalaCollectionCompat\n    implementation libs.scalaJava8Compat\n    // only needed transitively, but set it explicitly to ensure it has the same version as scala-library\n    implementation libs.scalaReflect\n    implementation libs.scalaLogging\n    implementation libs.slf4jApi\n    implementation(libs.zookeeper) {\n      // Dropwizard Metrics are required by ZooKeeper as of v3.6.0,\n      // but the library should *not* be used in Kafka code\n      implementation libs.dropwizardMetrics\n      exclude module: 'slf4j-log4j12'\n      exclude module: 'log4j'\n      // Both Kafka and Zookeeper use slf4j. ZooKeeper moved from log4j to logback in v3.8.0, but Kafka relies on reload4j.\n      // We are removing Zookeeper's dependency on logback so we have a singular logging backend.\n      exclude module: 'logback-classic'\n      exclude module: 'logback-core'\n    }\n    // ZooKeeperMain depends on commons-cli but declares the dependency as `provided`\n    implementation libs.commonsCli\n\n    implementation libs.zstd\n    implementation libs.commonLang\n    implementation libs.nettyHttp2\n    implementation project(':s3stream')\n    implementation libs.guava\n    implementation libs.slf4jBridge\n    implementation libs.slf4jReload4j\n\n    implementation libs.opentelemetryJava8\n    implementation libs.opentelemetryOshi\n    implementation libs.opentelemetrySdk\n    implementation libs.opentelemetrySdkMetrics\n    implementation libs.opentelemetryExporterLogging\n    implementation libs.opentelemetryExporterProm\n    implementation libs.opentelemetryExporterOTLP\n    implementation libs.opentelemetryJmx\n    implementation libs.awsSdkAuth\n\n    implementation(libs.oshi) {\n      exclude group: 'org.slf4j', module: '*'\n    }\n\n    compileOnly libs.reload4j\n\n    testImplementation project(':clients').sourceSets.test.output\n    testImplementation project(':group-coordinator').sourceSets.test.output\n    testImplementation project(':metadata').sourceSets.test.output\n    testImplementation project(':raft').sourceSets.test.output\n    testImplementation project(':server-common').sourceSets.test.output\n    testImplementation project(':storage:storage-api').sourceSets.test.output\n    testImplementation project(':server').sourceSets.test.output\n    testImplementation libs.bcpkix\n    testImplementation libs.mockitoCore\n    testImplementation libs.guava\n    testImplementation(libs.apacheda) {\n      exclude group: 'xml-apis', module: 'xml-apis'\n      // `mina-core` is a transitive dependency for `apacheds` and `apacheda`.\n      // It is safer to use from `apacheds` since that is the implementation.\n      exclude module: 'mina-core'\n    }\n    testImplementation libs.apachedsCoreApi\n    testImplementation libs.apachedsInterceptorKerberos\n    testImplementation libs.apachedsProtocolShared\n    testImplementation libs.apachedsProtocolKerberos\n    testImplementation libs.apachedsProtocolLdap\n    testImplementation libs.apachedsLdifPartition\n    testImplementation libs.apachedsMavibotPartition\n    testImplementation libs.apachedsJdbmPartition\n    testImplementation libs.junitJupiter\n    testImplementation libs.slf4jReload4j\n    testImplementation libs.caffeine\n    testImplementation libs.commonMath3\n    testRuntimeOnly libs.junitPlatformLanucher\n  }\n\n  if (userEnableTestCoverage) {\n    scoverage {\n      scoverageVersion = versions.scoverage\n      if (versions.baseScala == '2.13') {\n        scoverageScalaVersion = '2.13.9' // there's no newer 2.13 artifact, org.scoverage:scalac-scoverage-plugin_2.13.9:2.0.11 is the latest as of now\n      }\n      reportDir = file(\"${rootProject.buildDir}/scoverage\")\n      highlighting = false\n      minimumRate = 0.0\n    }\n  }\n\n  configurations {\n    // manually excludes some unnecessary dependencies\n    implementation.exclude module: 'javax'\n    implementation.exclude module: 'jline'\n    implementation.exclude module: 'jms'\n    implementation.exclude module: 'jmxri'\n    implementation.exclude module: 'jmxtools'\n    implementation.exclude module: 'mail'\n    // To prevent a UniqueResourceException due the same resource existing in both\n    // org.apache.directory.api/api-all and org.apache.directory.api/api-ldap-schema-data\n    testImplementation.exclude module: 'api-ldap-schema-data'\n  }\n\n  tasks.create(name: \"copyDependantLibs\", type: Copy) {\n    from (configurations.testRuntimeClasspath) {\n      include('slf4j-log4j12*')\n      include('reload4j*jar')\n    }\n    from (configurations.runtimeClasspath) {\n      exclude('kafka-clients*')\n    }\n    into \"$buildDir/dependant-libs-${versions.scala}\"\n    duplicatesStrategy 'exclude'\n  }\n\n  task genProtocolErrorDocs(type: JavaExec) {\n    classpath = sourceSets.main.runtimeClasspath\n    mainClass = 'org.apache.kafka.common.protocol.Errors'\n    if( !generatedDocsDir.exists() ) { generatedDocsDir.mkdirs() }\n    standardOutput = new File(generatedDocsDir, \"protocol_errors.html\").newOutputStream()\n  }\n\n  task genProtocolTypesDocs(type: JavaExec) {\n    classpath = sourceSets.main.runtimeClasspath\n    mainClass = 'org.apache.kafka.common.protocol.types.Type'\n    if( !generatedDocsDir.exists() ) { generatedDocsDir.mkdirs() }\n    standardOutput = new File(generatedDocsDir, \"protocol_types.html\").newOutputStream()\n  }\n\n  task genProtocolApiKeyDocs(type: JavaExec) {\n    classpath = sourceSets.main.runtimeClasspath\n    mainClass = 'org.apache.kafka.common.protocol.ApiKeys'\n    if( !generatedDocsDir.exists() ) { generatedDocsDir.mkdirs() }\n    standardOutput = new File(generatedDocsDir, \"protocol_api_keys.html\").newOutputStream()\n  }\n\n  task genProtocolMessageDocs(type: JavaExec) {\n    classpath = sourceSets.main.runtimeClasspath\n    mainClass = 'org.apache.kafka.common.protocol.Protocol'\n    if( !generatedDocsDir.exists() ) { generatedDocsDir.mkdirs() }\n    standardOutput = new File(generatedDocsDir, \"protocol_messages.html\").newOutputStream()\n  }\n\n  task genAdminClientConfigDocs(type: JavaExec) {\n    classpath = sourceSets.main.runtimeClasspath\n    mainClass = 'org.apache.kafka.clients.admin.AdminClientConfig'\n    if( !generatedDocsDir.exists() ) { generatedDocsDir.mkdirs() }\n    standardOutput = new File(generatedDocsDir, \"admin_client_config.html\").newOutputStream()\n  }\n\n  task genProducerConfigDocs(type: JavaExec) {\n    classpath = sourceSets.main.runtimeClasspath\n    mainClass = 'org.apache.kafka.clients.producer.ProducerConfig'\n    if( !generatedDocsDir.exists() ) { generatedDocsDir.mkdirs() }\n    standardOutput = new File(generatedDocsDir, \"producer_config.html\").newOutputStream()\n  }\n\n  task genConsumerConfigDocs(type: JavaExec) {\n    classpath = sourceSets.main.runtimeClasspath\n    mainClass = 'org.apache.kafka.clients.consumer.ConsumerConfig'\n    if( !generatedDocsDir.exists() ) { generatedDocsDir.mkdirs() }\n    standardOutput = new File(generatedDocsDir, \"consumer_config.html\").newOutputStream()\n  }\n\n  task genKafkaConfigDocs(type: JavaExec) {\n    classpath = sourceSets.main.runtimeClasspath\n    mainClass = 'kafka.server.KafkaConfig'\n    if( !generatedDocsDir.exists() ) { generatedDocsDir.mkdirs() }\n    standardOutput = new File(generatedDocsDir, \"kafka_config.html\").newOutputStream()\n  }\n\n  task genTopicConfigDocs(type: JavaExec) {\n    classpath = sourceSets.main.runtimeClasspath\n    mainClass = 'org.apache.kafka.storage.internals.log.LogConfig'\n    if( !generatedDocsDir.exists() ) { generatedDocsDir.mkdirs() }\n    standardOutput = new File(generatedDocsDir, \"topic_config.html\").newOutputStream()\n  }\n\n  task genConsumerMetricsDocs(type: JavaExec) {\n    classpath = sourceSets.test.runtimeClasspath\n    mainClass = 'org.apache.kafka.clients.consumer.internals.ConsumerMetrics'\n    if( !generatedDocsDir.exists() ) { generatedDocsDir.mkdirs() }\n    standardOutput = new File(generatedDocsDir, \"consumer_metrics.html\").newOutputStream()\n  }\n\n  task genProducerMetricsDocs(type: JavaExec) {\n    classpath = sourceSets.test.runtimeClasspath\n    mainClass = 'org.apache.kafka.clients.producer.internals.ProducerMetrics'\n    if( !generatedDocsDir.exists() ) { generatedDocsDir.mkdirs() }\n    standardOutput = new File(generatedDocsDir, \"producer_metrics.html\").newOutputStream()\n  }\n\n  task siteDocsTar(dependsOn: ['genProtocolErrorDocs', 'genProtocolTypesDocs', 'genProtocolApiKeyDocs', 'genProtocolMessageDocs',\n                               'genAdminClientConfigDocs', 'genProducerConfigDocs', 'genConsumerConfigDocs',\n                               'genKafkaConfigDocs', 'genTopicConfigDocs',\n                               ':connect:runtime:genConnectConfigDocs', ':connect:runtime:genConnectTransformationDocs',\n                               ':connect:runtime:genConnectPredicateDocs',\n                               ':connect:runtime:genSinkConnectorConfigDocs', ':connect:runtime:genSourceConnectorConfigDocs',\n                               ':streams:genStreamsConfigDocs', 'genConsumerMetricsDocs', 'genProducerMetricsDocs',\n                               ':connect:runtime:genConnectMetricsDocs', ':connect:runtime:genConnectOpenAPIDocs',\n                               ':connect:mirror:genMirrorSourceConfigDocs', ':connect:mirror:genMirrorCheckpointConfigDocs',\n                               ':connect:mirror:genMirrorHeartbeatConfigDocs', ':connect:mirror:genMirrorConnectorConfigDocs',\n                               ':storage:genRemoteLogManagerConfigDoc', ':storage:genRemoteLogMetadataManagerConfigDoc'], type: Tar) {\n    def prefix = project.findProperty('prefix') ?: ''\n    archiveBaseName = \"${prefix}kafka\"\n\n    archiveClassifier = 'site-docs'\n    compression = Compression.GZIP\n    from project.file(\"$rootDir/docs\")\n    into \"${prefix}kafka-${archiveVersion.get()}-site-docs\"\n    duplicatesStrategy 'exclude'\n  }\n\n  tasks.create(name: \"releaseTarGz\", dependsOn: configurations.archives.artifacts, type: Tar) {\n    def prefix = project.findProperty('prefix') ?: ''\n    archiveBaseName = \"${prefix}kafka\"\n\n    into \"${prefix}kafka-${archiveVersion.get()}\"\n    compression = Compression.GZIP\n    from(project.file(\"$rootDir/bin\")) { into \"bin/\" }\n    from(project.file(\"$rootDir/config\")) { into \"config/\" }\n    from(project.file(\"$rootDir/licenses\")) { into \"licenses/\" }\n    from(project.file(\"$rootDir/docker/docker-compose.yaml\")) { into \"docker/\" }\n    from(project.file(\"$rootDir/docker/telemetry\")) { into \"docker/telemetry/\" }\n    from(project.file(\"$rootDir/LICENSE\")) { into \"\" }\n    from(project.file(\"$rootDir/LICENSE.S3Stream\")) { into \"\" }\n    from \"$rootDir/NOTICE-binary\" rename {String filename -> filename.replace(\"-binary\", \"\")}\n    from(configurations.runtimeClasspath) { into(\"libs/\") }\n    from(configurations.archives.artifacts.files) { into(\"libs/\") }\n    from(project.siteDocsTar) { into(\"site-docs/\") }\n    from(project(':tools').jar) { into(\"libs/\") }\n    from(project(':tools').configurations.runtimeClasspath) { into(\"libs/\") }\n    from(project(':trogdor').jar) { into(\"libs/\") }\n    from(project(':trogdor').configurations.runtimeClasspath) { into(\"libs/\") }\n    from(project(':automq-shell').jar) { into(\"libs/\") }\n    from(project(':automq-shell').configurations.runtimeClasspath) { into(\"libs/\") }\n    from(project(':shell').jar) { into(\"libs/\") }\n    from(project(':shell').configurations.runtimeClasspath) { into(\"libs/\") }\n    from(project(':connect:api').jar) { into(\"libs/\") }\n    from(project(':connect:api').configurations.runtimeClasspath) { into(\"libs/\") }\n    from(project(':connect:runtime').jar) { into(\"libs/\") }\n    from(project(':connect:runtime').configurations.runtimeClasspath) { into(\"libs/\") }\n    from(project(':connect:transforms').jar) { into(\"libs/\") }\n    from(project(':connect:transforms').configurations.runtimeClasspath) { into(\"libs/\") }\n    from(project(':connect:json').jar) { into(\"libs/\") }\n    from(project(':connect:json').configurations.runtimeClasspath) { into(\"libs/\") }\n    from(project(':connect:file').jar) { into(\"libs/\") }\n    from(project(':connect:file').configurations.runtimeClasspath) { into(\"libs/\") }\n    from(project(':connect:basic-auth-extension').jar) { into(\"libs/\") }\n    from(project(':connect:basic-auth-extension').configurations.runtimeClasspath) { into(\"libs/\") }\n    from(project(':connect:mirror').jar) { into(\"libs/\") }\n    from(project(':connect:mirror').configurations.runtimeClasspath) { into(\"libs/\") }\n    from(project(':connect:mirror-client').jar) { into(\"libs/\") }\n    from(project(':connect:mirror-client').configurations.runtimeClasspath) { into(\"libs/\") }\n    from(project(':streams').jar) { into(\"libs/\") }\n    from(project(':streams').configurations.runtimeClasspath) { into(\"libs/\") }\n    from(project(':streams:streams-scala').jar) { into(\"libs/\") }\n    from(project(':streams:streams-scala').configurations.runtimeClasspath) { into(\"libs/\") }\n    from(project(':streams:test-utils').jar) { into(\"libs/\") }\n    from(project(':streams:test-utils').configurations.runtimeClasspath) { into(\"libs/\") }\n    from(project(':streams:examples').jar) { into(\"libs/\") }\n    from(project(':streams:examples').configurations.runtimeClasspath) { into(\"libs/\") }\n    from(project(':tools:tools-api').jar) { into(\"libs/\") }\n    from(project(':tools:tools-api').configurations.runtimeClasspath) { into(\"libs/\") }\n    duplicatesStrategy 'exclude'\n  }\n\n  jar {\n    dependsOn('copyDependantLibs')\n  }\n\n  jar.manifest {\n    attributes(\n      'Version': \"${version}\"\n    )\n  }\n\n  tasks.create(name: \"copyDependantTestLibs\", type: Copy) {\n    from (configurations.testRuntimeClasspath) {\n      include('*.jar')\n    }\n    into \"$buildDir/dependant-testlibs\"\n    //By default gradle does not handle test dependencies between the sub-projects\n    //This line is to include clients project test jar to dependant-testlibs\n    from (project(':clients').testJar ) { \"$buildDir/dependant-testlibs\" }\n    // log4j-appender is not in core dependencies, \n    // so we add it to dependant-testlibs to avoid ClassNotFoundException in running kafka_log4j_appender.py\n    from (project(':log4j-appender').jar ) { \"$buildDir/dependant-testlibs\" }\n    duplicatesStrategy 'exclude'\n  }\n\n  systemTestLibs.dependsOn('jar', 'testJar', 'copyDependantTestLibs')\n\n  checkstyle {\n    configProperties = checkstyleConfigProperties(\"import-control-core.xml\")\n  }\n\n  sourceSets {\n    // Set java/scala source folders in the `scala` block to enable joint compilation\n    main {\n      java {\n        srcDirs = []\n      }\n      scala {\n        srcDirs = [\"src/generated/java\", \"src/main/java\", \"src/main/scala\"]\n      }\n    }\n    test {\n      java {\n        srcDirs = []\n      }\n      scala {\n        srcDirs = [\"src/test/java\", \"src/test/scala\"]\n      }\n    }\n  }\n}\n\nproject(':metadata') {\n  base {\n    archivesName = \"kafka-metadata\"\n  }\n\n  configurations {\n    generator\n  }\n\n  dependencies {\n    implementation project(':server-common')\n    implementation project(':clients')\n    implementation project(':raft')\n    implementation project(':automq-shell')\n\n    implementation libs.metrics\n    implementation libs.nettyCommon\n    implementation libs.nettyHttp2\n    implementation libs.zstd\n    implementation libs.guava\n    implementation libs.awsSdkAuth\n    implementation project(':s3stream')\n\n    implementation libs.jacksonDatabind\n    implementation libs.jacksonJDK8Datatypes\n    implementation libs.metrics\n    compileOnly libs.reload4j\n    testImplementation libs.junitJupiter\n    testImplementation libs.jqwik\n    testImplementation libs.hamcrest\n    testImplementation libs.mockitoCore\n    testImplementation libs.slf4jReload4j\n    testImplementation project(':clients').sourceSets.test.output\n    testImplementation project(':raft').sourceSets.test.output\n    testImplementation project(':server-common').sourceSets.test.output\n\n    testRuntimeOnly libs.junitPlatformLanucher\n\n    generator project(':generator')\n  }\n\n  task processMessages(type:JavaExec) {\n    mainClass = \"org.apache.kafka.message.MessageGenerator\"\n    classpath = configurations.generator\n    args = [ \"-p\", \"org.apache.kafka.common.metadata\",\n             \"-o\", \"src/generated/java/org/apache/kafka/common/metadata\",\n             \"-i\", \"src/main/resources/common/metadata\",\n             \"-m\", \"MessageDataGenerator\", \"JsonConverterGenerator\",\n             \"-t\", \"MetadataRecordTypeGenerator\", \"MetadataJsonConvertersGenerator\"\n           ]\n    inputs.dir(\"src/main/resources/common/metadata\")\n        .withPropertyName(\"messages\")\n        .withPathSensitivity(PathSensitivity.RELATIVE)\n    outputs.cacheIf { true }\n    outputs.dir(\"src/generated/java/org/apache/kafka/common/metadata\")\n  }\n\n  compileJava.dependsOn 'processMessages'\n  srcJar.dependsOn 'processMessages'\n\n  sourceSets {\n    main {\n      java {\n        srcDirs = [\"src/generated/java\", \"src/main/java\"]\n      }\n    }\n    test {\n      java {\n        srcDirs = [\"src/test/java\"]\n      }\n    }\n  }\n\n  javadoc {\n    enabled = false\n  }\n\n  checkstyle {\n    configProperties = checkstyleConfigProperties(\"import-control-metadata.xml\")\n  }\n}\n\nproject(':group-coordinator:group-coordinator-api') {\n  base {\n    archivesName = \"kafka-group-coordinator-api\"\n  }\n\n  dependencies {\n    implementation project(':clients')\n  }\n\n  task createVersionFile() {\n    def receiptFile = file(\"$buildDir/kafka/$buildVersionFileName\")\n    inputs.property \"commitId\", commitId\n    inputs.property \"version\", version\n    outputs.file receiptFile\n\n    doLast {\n      def data = [\n              commitId: commitId,\n              version: version,\n      ]\n\n      receiptFile.parentFile.mkdirs()\n      def content = data.entrySet().collect { \"$it.key=$it.value\" }.sort().join(\"\\n\")\n      receiptFile.setText(content, \"ISO-8859-1\")\n    }\n  }\n\n  sourceSets {\n    main {\n      java {\n        srcDirs = [\"src/main/java\"]\n      }\n    }\n    test {\n      java {\n        srcDirs = [\"src/test/java\"]\n      }\n    }\n  }\n\n  jar {\n    dependsOn createVersionFile\n    from(\"$buildDir\") {\n      include \"kafka/$buildVersionFileName\"\n    }\n  }\n\n  clean.doFirst {\n    delete \"$buildDir/kafka/\"\n  }\n\n  javadoc {\n    include \"**/org/apache/kafka/coordinator/group/api/**\"\n  }\n\n  checkstyle {\n    configProperties = checkstyleConfigProperties(\"import-control-group-coordinator.xml\")\n  }\n}\n\nproject(':group-coordinator') {\n  base {\n    archivesName = \"kafka-group-coordinator\"\n  }\n\n  configurations {\n    generator\n  }\n\n  dependencies {\n    implementation project(':server-common')\n    implementation project(':clients')\n    implementation project(':metadata')\n    implementation project(':group-coordinator:group-coordinator-api')\n    implementation project(':storage')\n    implementation libs.jacksonDatabind\n    implementation libs.jacksonJDK8Datatypes\n    implementation libs.slf4jApi\n    implementation libs.metrics\n\n    testImplementation project(':clients').sourceSets.test.output\n    testImplementation project(':server-common').sourceSets.test.output\n    testImplementation libs.junitJupiter\n    testImplementation libs.mockitoCore\n\n    testRuntimeOnly libs.slf4jReload4j\n    testRuntimeOnly libs.junitPlatformLanucher\n\n    generator project(':generator')\n  }\n\n  sourceSets {\n    main {\n      java {\n        srcDirs = [\"src/generated/java\", \"src/main/java\"]\n      }\n    }\n    test {\n      java {\n        srcDirs = [\"src/test/java\"]\n      }\n    }\n  }\n\n  javadoc {\n    enabled = false\n  }\n\n  checkstyle {\n    configProperties = checkstyleConfigProperties(\"import-control-group-coordinator.xml\")\n  }\n\n  task processMessages(type:JavaExec) {\n    mainClass = \"org.apache.kafka.message.MessageGenerator\"\n    classpath = configurations.generator\n    args = [ \"-p\", \"org.apache.kafka.coordinator.group.generated\",\n             \"-o\", \"src/generated/java/org/apache/kafka/coordinator/group/generated\",\n             \"-i\", \"src/main/resources/common/message\",\n             \"-m\", \"MessageDataGenerator\", \"JsonConverterGenerator\"\n    ]\n    inputs.dir(\"src/main/resources/common/message\")\n        .withPropertyName(\"messages\")\n        .withPathSensitivity(PathSensitivity.RELATIVE)\n    outputs.cacheIf { true }\n    outputs.dir(\"src/generated/java/org/apache/kafka/coordinator/group/generated\")\n  }\n\n  compileJava.dependsOn 'processMessages'\n  srcJar.dependsOn 'processMessages'\n}\n\nproject(':transaction-coordinator') {\n  base {\n    archivesName = \"kafka-transaction-coordinator\"\n  }\n\n  configurations {\n    generator\n  }\n\n  dependencies {\n    implementation libs.jacksonDatabind\n    implementation project(':clients')\n    generator project(':generator')\n  }\n  \n  sourceSets {\n    main {\n      java {\n        srcDirs = [\"src/generated/java\", \"src/main/java\"]\n      }\n    }\n    test {\n      java {\n        srcDirs = [\"src/test/java\"]\n      }\n    }\n  }\n\n  checkstyle {\n    configProperties = checkstyleConfigProperties(\"import-control-transaction-coordinator.xml\")\n  }\n\n  task processMessages(type:JavaExec) {\n    mainClass = \"org.apache.kafka.message.MessageGenerator\"\n    classpath = configurations.generator\n    args = [ \"-p\", \"org.apache.kafka.coordinator.transaction.generated\",\n             \"-o\", \"src/generated/java/org/apache/kafka/coordinator/transaction/generated\",\n             \"-i\", \"src/main/resources/common/message\",\n             \"-m\", \"MessageDataGenerator\", \"JsonConverterGenerator\"\n    ]\n    inputs.dir(\"src/main/resources/common/message\")\n            .withPropertyName(\"messages\")\n            .withPathSensitivity(PathSensitivity.RELATIVE)\n    outputs.cacheIf { true }\n    outputs.dir(\"src/generated/java/org/apache/kafka/coordinator/transaction/generated\")\n  }\n\n  compileJava.dependsOn 'processMessages'\n  srcJar.dependsOn 'processMessages'\n\n  javadoc {\n    enabled = false\n  }\n}\n\nproject(':examples') {\n  base {\n    archivesName = \"kafka-examples\"\n  }\n\n  dependencies {\n    implementation project(':clients')\n  }\n\n  javadoc {\n    enabled = false\n  }\n\n  checkstyle {\n    configProperties = checkstyleConfigProperties(\"import-control-core.xml\")\n  }\n}\n\nproject(':generator') {\n  dependencies {\n    implementation libs.argparse4j\n    implementation libs.jacksonDatabind\n    implementation libs.jacksonJDK8Datatypes\n    implementation libs.jacksonJaxrsJsonProvider\n    testImplementation libs.junitJupiter\n\n    testRuntimeOnly libs.junitPlatformLanucher\n  }\n\n  javadoc {\n    enabled = false\n  }\n}\n\nproject(':clients') {\n  base {\n    archivesName = \"kafka-clients\"\n  }\n\n  configurations {\n    generator\n    shadowed\n  }\n\n  dependencies {\n    implementation libs.zstd\n    implementation libs.lz4\n    implementation libs.snappy\n    implementation libs.slf4jApi\n    implementation libs.opentelemetryProto\n\n    // libraries which should be added as runtime dependencies in generated pom.xml should be defined here:\n    shadowed libs.zstd\n    shadowed libs.lz4\n    shadowed libs.snappy\n    shadowed libs.slf4jApi\n\n    compileOnly libs.jacksonDatabind // for SASL/OAUTHBEARER bearer token parsing\n    compileOnly libs.jacksonJDK8Datatypes\n    compileOnly libs.jose4j          // for SASL/OAUTHBEARER JWT validation; only used by broker\n\n    testImplementation libs.bcpkix\n    testImplementation libs.jacksonJaxrsJsonProvider\n    testImplementation libs.jose4j\n    testImplementation libs.junitJupiter\n    testImplementation libs.reload4j\n    testImplementation libs.mockitoCore\n    testImplementation libs.mockitoJunitJupiter // supports MockitoExtension\n\n    testRuntimeOnly libs.slf4jReload4j\n    testRuntimeOnly libs.jacksonDatabind\n    testRuntimeOnly libs.jacksonJDK8Datatypes\n    testRuntimeOnly libs.junitPlatformLanucher\n\n    generator project(':generator')\n  }\n\n  task createVersionFile() {\n    def receiptFile = file(\"$buildDir/kafka/$buildVersionFileName\")\n    inputs.property \"commitId\", commitId\n    inputs.property \"version\", version\n    outputs.file receiptFile\n\n    doLast {\n      def data = [\n        commitId: commitId,\n        version: version,\n      ]\n\n      receiptFile.parentFile.mkdirs()\n      def content = data.entrySet().collect { \"$it.key=$it.value\" }.sort().join(\"\\n\")\n      receiptFile.setText(content, \"ISO-8859-1\")\n    }\n  }\n\n  shadowJar {\n    dependsOn createVersionFile\n    // archiveClassifier defines the classifier for the shadow jar, the default is 'all'.\n    // We don't want to use the default classifier because it will cause the shadow jar to\n    // overwrite the original jar. We also don't want to use the 'shadow' classifier because\n    // it will cause the shadow jar to be named kafka-clients-shadow.jar. We want to use the\n    // same name as the original jar, kafka-clients.jar.\n    archiveClassifier = null\n    // KIP-714: move shaded dependencies to a shaded location\n    relocate('io.opentelemetry.proto', 'org.apache.kafka.shaded.io.opentelemetry.proto')\n    relocate('com.google.protobuf', 'org.apache.kafka.shaded.com.google.protobuf')\n\n    // dependencies excluded from the final jar, since they are declared as runtime dependencies\n    dependencies {\n      project.configurations.shadowed.allDependencies.each {\n        exclude(dependency(it.group + ':' + it.name))\n      }\n      // exclude proto files from the jar\n      exclude \"**/opentelemetry/proto/**/*.proto\"\n      exclude \"**/google/protobuf/*.proto\"\n    }\n\n    from(\"$buildDir\") {\n      include \"kafka/$buildVersionFileName\"\n    }\n\n    from \"$rootDir/LICENSE\"\n    from \"$rootDir/NOTICE\"\n  }\n\n  jar {\n    enabled false\n    dependsOn 'shadowJar'\n  }\n\n  clean.doFirst {\n    delete \"$buildDir/kafka/\"\n  }\n\n  task processMessages(type:JavaExec) {\n    mainClass = \"org.apache.kafka.message.MessageGenerator\"\n    classpath = configurations.generator\n    args = [ \"-p\", \"org.apache.kafka.common.message\",\n             \"-o\", \"src/generated/java/org/apache/kafka/common/message\",\n             \"-i\", \"src/main/resources/common/message\",\n             \"-t\", \"ApiMessageTypeGenerator\",\n             \"-m\", \"MessageDataGenerator\", \"JsonConverterGenerator\"\n           ]\n    inputs.dir(\"src/main/resources/common/message\")\n        .withPropertyName(\"messages\")\n        .withPathSensitivity(PathSensitivity.RELATIVE)\n    outputs.cacheIf { true }\n    outputs.dir(\"src/generated/java/org/apache/kafka/common/message\")\n  }\n\n  task processTestMessages(type:JavaExec) {\n    mainClass = \"org.apache.kafka.message.MessageGenerator\"\n    classpath = configurations.generator\n    args = [ \"-p\", \"org.apache.kafka.common.message\",\n             \"-o\", \"src/generated-test/java/org/apache/kafka/common/message\",\n             \"-i\", \"src/test/resources/common/message\",\n             \"-m\", \"MessageDataGenerator\", \"JsonConverterGenerator\"\n           ]\n    inputs.dir(\"src/test/resources/common/message\")\n        .withPropertyName(\"testMessages\")\n        .withPathSensitivity(PathSensitivity.RELATIVE)\n    outputs.cacheIf { true }\n    outputs.dir(\"src/generated-test/java/org/apache/kafka/common/message\")\n  }\n\n  sourceSets {\n    main {\n      java {\n        srcDirs = [\"src/generated/java\", \"src/main/java\"]\n      }\n    }\n    test {\n      java {\n        srcDirs = [\"src/generated-test/java\", \"src/test/java\"]\n      }\n    }\n  }\n\n  compileJava.dependsOn 'processMessages'\n  srcJar.dependsOn 'processMessages'\n\n  compileTestJava.dependsOn 'processTestMessages'\n\n  javadoc {\n    include \"**/org/apache/kafka/clients/admin/*\"\n    include \"**/org/apache/kafka/clients/consumer/*\"\n    include \"**/org/apache/kafka/clients/producer/*\"\n    include \"**/org/apache/kafka/common/*\"\n    include \"**/org/apache/kafka/common/acl/*\"\n    include \"**/org/apache/kafka/common/annotation/*\"\n    include \"**/org/apache/kafka/common/errors/*\"\n    include \"**/org/apache/kafka/common/header/*\"\n    include \"**/org/apache/kafka/common/metrics/*\"\n    include \"**/org/apache/kafka/common/metrics/stats/*\"\n    include \"**/org/apache/kafka/common/quota/*\"\n    include \"**/org/apache/kafka/common/resource/*\"\n    include \"**/org/apache/kafka/common/serialization/*\"\n    include \"**/org/apache/kafka/common/config/*\"\n    include \"**/org/apache/kafka/common/config/provider/*\"\n    include \"**/org/apache/kafka/common/security/auth/*\"\n    include \"**/org/apache/kafka/common/security/plain/*\"\n    include \"**/org/apache/kafka/common/security/scram/*\"\n    include \"**/org/apache/kafka/common/security/token/delegation/*\"\n    include \"**/org/apache/kafka/common/security/oauthbearer/*\"\n    include \"**/org/apache/kafka/common/security/oauthbearer/secured/*\"\n    include \"**/org/apache/kafka/server/authorizer/*\"\n    include \"**/org/apache/kafka/server/policy/*\"\n    include \"**/org/apache/kafka/server/quota/*\"\n    include \"**/org/apache/kafka/server/telemetry/*\"\n  }\n}\n\nproject(':raft') {\n  base {\n    archivesName = \"kafka-raft\"\n  }\n\n  configurations {\n    generator\n  }\n\n  dependencies {\n    implementation project(':server-common')\n    implementation project(':clients')\n    implementation libs.slf4jApi\n    implementation libs.jacksonDatabind\n\n    testImplementation project(':server-common')\n    testImplementation project(':server-common').sourceSets.test.output\n    testImplementation project(':clients')\n    testImplementation project(':clients').sourceSets.test.output\n    testImplementation libs.junitJupiter\n    testImplementation libs.mockitoCore\n    testImplementation libs.jqwik\n\n    testRuntimeOnly libs.slf4jReload4j\n    testRuntimeOnly libs.junitPlatformLanucher\n\n    generator project(':generator')\n  }\n\n  task createVersionFile() {\n    def receiptFile = file(\"$buildDir/kafka/$buildVersionFileName\")\n    inputs.property \"commitId\", commitId\n    inputs.property \"version\", version\n    outputs.file receiptFile\n\n    doLast {\n      def data = [\n        commitId: commitId,\n        version: version,\n      ]\n\n      receiptFile.parentFile.mkdirs()\n      def content = data.entrySet().collect { \"$it.key=$it.value\" }.sort().join(\"\\n\")\n      receiptFile.setText(content, \"ISO-8859-1\")\n    }\n  }\n\n  task processMessages(type:JavaExec) {\n    mainClass = \"org.apache.kafka.message.MessageGenerator\"\n    classpath = configurations.generator\n    args = [ \"-p\", \"org.apache.kafka.raft.generated\",\n             \"-o\", \"src/generated/java/org/apache/kafka/raft/generated\",\n             \"-i\", \"src/main/resources/common/message\",\n             \"-m\", \"MessageDataGenerator\", \"JsonConverterGenerator\"]\n    inputs.dir(\"src/main/resources/common/message\")\n        .withPropertyName(\"messages\")\n        .withPathSensitivity(PathSensitivity.RELATIVE)\n    outputs.cacheIf { true }\n    outputs.dir(\"src/generated/java/org/apache/kafka/raft/generated\")\n  }\n\n  sourceSets {\n    main {\n      java {\n        srcDirs = [\"src/generated/java\", \"src/main/java\"]\n      }\n    }\n    test {\n      java {\n        srcDirs = [\"src/test/java\"]\n      }\n    }\n  }\n\n  compileJava.dependsOn 'processMessages'\n  srcJar.dependsOn 'processMessages'\n\n  jar {\n    dependsOn createVersionFile\n    from(\"$buildDir\") {\n        include \"kafka/$buildVersionFileName\"\n    }\n  }\n\n  test {\n    useJUnitPlatform {\n      includeEngines 'jqwik', 'junit-jupiter'\n    }\n  }\n\n  clean.doFirst {\n    delete \"$buildDir/kafka/\"\n  }\n\n  javadoc {\n    enabled = false\n  }\n}\n\nproject(':server-common') {\n  base {\n    archivesName = \"kafka-server-common\"\n  }\n\n  dependencies {\n    api project(':clients')\n    implementation libs.slf4jApi\n    implementation libs.metrics\n    implementation libs.joptSimple\n    implementation libs.jacksonDatabind\n    implementation libs.pcollections\n    implementation libs.opentelemetrySdk\n    implementation project(':s3stream')\n\n    testImplementation project(':clients')\n    testImplementation project(':clients').sourceSets.test.output\n    testImplementation libs.junitJupiter\n    testImplementation libs.mockitoCore\n    testImplementation libs.hamcrest\n\n    testRuntimeOnly libs.slf4jReload4j\n    testRuntimeOnly libs.junitPlatformLanucher\n  }\n\n  task createVersionFile() {\n    def receiptFile = file(\"$buildDir/kafka/$buildVersionFileName\")\n    inputs.property \"commitId\", commitId\n    inputs.property \"version\", version\n    outputs.file receiptFile\n\n    doLast {\n      def data = [\n              commitId: commitId,\n              version: version,\n      ]\n\n      receiptFile.parentFile.mkdirs()\n      def content = data.entrySet().collect { \"$it.key=$it.value\" }.sort().join(\"\\n\")\n      receiptFile.setText(content, \"ISO-8859-1\")\n    }\n  }\n\n  jar {\n    dependsOn createVersionFile\n    from(\"$buildDir\") {\n      include \"kafka/$buildVersionFileName\"\n    }\n  }\n\n  clean.doFirst {\n    delete \"$buildDir/kafka/\"\n  }\n\n  checkstyle {\n    configProperties = checkstyleConfigProperties(\"import-control-server-common.xml\")\n  }\n\n  javadoc {\n    enabled = false\n  }\n}\n\nproject(':storage:storage-api') {\n  base {\n    archivesName = \"kafka-storage-api\"\n  }\n\n  dependencies {\n    implementation project(':clients')\n    implementation project(':server-common')\n    implementation libs.metrics\n    implementation libs.slf4jApi\n\n    testImplementation project(':clients')\n    testImplementation project(':clients').sourceSets.test.output\n    testImplementation libs.junitJupiter\n    testImplementation libs.mockitoCore\n\n    testRuntimeOnly libs.slf4jReload4j\n    testRuntimeOnly libs.junitPlatformLanucher\n  }\n\n  task createVersionFile() {\n    def receiptFile = file(\"$buildDir/kafka/$buildVersionFileName\")\n    inputs.property \"commitId\", commitId\n    inputs.property \"version\", version\n    outputs.file receiptFile\n\n    doLast {\n      def data = [\n              commitId: commitId,\n              version: version,\n      ]\n\n      receiptFile.parentFile.mkdirs()\n      def content = data.entrySet().collect { \"$it.key=$it.value\" }.sort().join(\"\\n\")\n      receiptFile.setText(content, \"ISO-8859-1\")\n    }\n  }\n\n  sourceSets {\n    main {\n      java {\n        srcDirs = [\"src/main/java\"]\n      }\n    }\n    test {\n      java {\n        srcDirs = [\"src/test/java\"]\n      }\n    }\n  }\n\n  jar {\n    dependsOn createVersionFile\n    from(\"$buildDir\") {\n      include \"kafka/$buildVersionFileName\"\n    }\n  }\n\n  clean.doFirst {\n    delete \"$buildDir/kafka/\"\n  }\n\n  javadoc {\n    include \"**/org/apache/kafka/server/log/remote/storage/*\"\n  }\n\n  checkstyle {\n    configProperties = checkstyleConfigProperties(\"import-control-storage.xml\")\n  }\n}\n\nproject(':storage') {\n  base {\n    archivesName = \"kafka-storage\"\n  }\n\n  configurations {\n    generator\n  }\n\n  dependencies {\n    implementation project(':storage:storage-api')\n    implementation project(':server-common')\n    implementation project(':clients')\n    implementation project(':transaction-coordinator')\n    implementation(libs.caffeine) {\n      exclude group: 'org.checkerframework', module: 'checker-qual'\n    }\n    implementation libs.slf4jApi\n    implementation libs.jacksonDatabind\n    implementation libs.metrics\n\n    testImplementation project(':clients')\n    testImplementation project(':clients').sourceSets.test.output\n    testImplementation project(':core')\n    testImplementation project(':core').sourceSets.test.output\n    testImplementation project(':server')\n    testImplementation project(':server-common')\n    testImplementation project(':server-common').sourceSets.test.output\n    testImplementation libs.hamcrest\n    testImplementation libs.junitJupiter\n    testImplementation libs.mockitoCore\n    testImplementation libs.bcpkix\n\n    testRuntimeOnly libs.slf4jReload4j\n    testRuntimeOnly libs.junitPlatformLanucher\n\n    generator project(':generator')\n  }\n\n  task createVersionFile() {\n    def receiptFile = file(\"$buildDir/kafka/$buildVersionFileName\")\n    inputs.property \"commitId\", commitId\n    inputs.property \"version\", version\n    outputs.file receiptFile\n\n    doLast {\n      def data = [\n              commitId: commitId,\n              version: version,\n      ]\n\n      receiptFile.parentFile.mkdirs()\n      def content = data.entrySet().collect { \"$it.key=$it.value\" }.sort().join(\"\\n\")\n      receiptFile.setText(content, \"ISO-8859-1\")\n    }\n  }\n\n  task processMessages(type:JavaExec) {\n    mainClass = \"org.apache.kafka.message.MessageGenerator\"\n    classpath = configurations.generator\n    args = [ \"-p\", \"org.apache.kafka.server.log.remote.metadata.storage.generated\",\n             \"-o\", \"src/generated/java/org/apache/kafka/server/log/remote/metadata/storage/generated\",\n             \"-i\", \"src/main/resources/message\",\n             \"-m\", \"MessageDataGenerator\", \"JsonConverterGenerator\",\n             \"-t\", \"MetadataRecordTypeGenerator\", \"MetadataJsonConvertersGenerator\" ]\n    inputs.dir(\"src/main/resources/message\")\n        .withPropertyName(\"messages\")\n        .withPathSensitivity(PathSensitivity.RELATIVE)\n    outputs.cacheIf { true }\n    outputs.dir(\"src/generated/java/org/apache/kafka/server/log/remote/metadata/storage/generated\")\n  }\n\n  task genRemoteLogManagerConfigDoc(type: JavaExec) {\n    classpath = sourceSets.main.runtimeClasspath\n    mainClass = 'org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig'\n    if( !generatedDocsDir.exists() ) { generatedDocsDir.mkdirs() }\n    standardOutput = new File(generatedDocsDir, \"remote_log_manager_config.html\").newOutputStream()\n  }\n\n  task genRemoteLogMetadataManagerConfigDoc(type: JavaExec) {\n    classpath = sourceSets.main.runtimeClasspath\n    mainClass = 'org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManagerConfig'\n    if( !generatedDocsDir.exists() ) { generatedDocsDir.mkdirs() }\n    standardOutput = new File(generatedDocsDir, \"remote_log_metadata_manager_config.html\").newOutputStream()\n  }\n\n  sourceSets {\n    main {\n      java {\n        srcDirs = [\"src/generated/java\", \"src/main/java\"]\n      }\n    }\n    test {\n      java {\n        srcDirs = [\"src/test/java\"]\n      }\n    }\n  }\n\n  compileJava.dependsOn 'processMessages'\n  srcJar.dependsOn 'processMessages'\n\n  jar {\n    dependsOn createVersionFile\n    from(\"$buildDir\") {\n      include \"kafka/$buildVersionFileName\"\n    }\n  }\n\n  clean.doFirst {\n    delete \"$buildDir/kafka/\"\n  }\n\n  javadoc {\n    enabled = false\n  }\n\n  checkstyle {\n    configProperties = checkstyleConfigProperties(\"import-control-storage.xml\")\n  }\n}\n\nproject(':s3stream') {\n  archivesBaseName = \"s3stream\"\n\n  checkstyle {\n    configProperties = checkstyleConfigProperties(\"import-control-automq.xml\")\n  }\n\n  dependencies {\n    implementation libs.awsSdk\n    implementation libs.awsSdkNetty\n    // In gradle, we need to specify the classifier for netty-tcnative\n    // see\n    // https://netty.io/wiki/forked-tomcat-native.html\n    // https://github.com/netty/netty-tcnative/issues/716\n    // https://github.com/grpc/grpc-java/pull/9027\n    runtimeOnly (libs.nettyTcnativeBoringSsl) {\n      artifact {\n        classifier = \"linux-x86_64\"\n      }\n    }\n    runtimeOnly (libs.nettyTcnativeBoringSsl) {\n      artifact {\n        classifier = \"linux-aarch_64\"\n      }\n    }\n    runtimeOnly (libs.nettyTcnativeBoringSsl) {\n      artifact {\n        classifier = \"osx-x86_64\"\n      }\n    }\n    runtimeOnly (libs.nettyTcnativeBoringSsl) {\n      artifact {\n        classifier = \"osx-aarch_64\"\n      }\n    }\n    implementation libs.nettyBuffer\n    implementation libs.bucket4j\n    implementation libs.commonLang\n    implementation libs.slf4jApi\n    implementation libs.argparse4j\n    implementation libs.jna\n    implementation libs.guava\n    implementation libs.jacksonDatabind\n    implementation libs.opentelemetrySdk\n    implementation 'io.opentelemetry.instrumentation:opentelemetry-instrumentation-annotations:1.32.0'\n    implementation 'org.aspectj:aspectjrt:1.9.20.1'\n    implementation 'org.aspectj:aspectjweaver:1.9.20.1'\n    implementation 'com.github.jnr:jnr-posix:3.1.19'\n    implementation 'com.yammer.metrics:metrics-core:2.2.0'\n    implementation 'commons-codec:commons-codec:1.17.0'\n    implementation 'org.hdrhistogram:HdrHistogram:2.2.2'\n    implementation 'software.amazon.awssdk.crt:aws-crt:0.30.8'\n\n    testImplementation 'org.slf4j:slf4j-simple:2.0.9'\n    testImplementation 'org.junit.jupiter:junit-jupiter:5.10.0'\n    testImplementation 'org.mockito:mockito-core:5.5.0'\n    testImplementation 'org.mockito:mockito-junit-jupiter:5.5.0'\n    testImplementation 'org.awaitility:awaitility:4.2.1'\n  }\n\n  test {\n    jvmArgs '--add-opens=java.base/java.nio=ALL-UNNAMED', '-Dio.netty.tryReflectionSetAccessible=true'\n  }\n}\n\nproject(':tools:tools-api') {\n  base {\n    archivesName = \"kafka-tools-api\"\n  }\n\n  dependencies {\n    implementation project(':clients')\n    testImplementation libs.junitJupiter\n    testRuntimeOnly libs.junitPlatformLanucher\n  }\n\n  task createVersionFile() {\n    def receiptFile = file(\"$buildDir/kafka/$buildVersionFileName\")\n    inputs.property \"commitId\", commitId\n    inputs.property \"version\", version\n    outputs.file receiptFile\n\n    doLast {\n      def data = [\n              commitId: commitId,\n              version: version,\n      ]\n\n      receiptFile.parentFile.mkdirs()\n      def content = data.entrySet().collect { \"$it.key=$it.value\" }.sort().join(\"\\n\")\n      receiptFile.setText(content, \"ISO-8859-1\")\n    }\n  }\n\n  sourceSets {\n    main {\n      java {\n        srcDirs = [\"src/main/java\"]\n      }\n    }\n    test {\n      java {\n        srcDirs = [\"src/test/java\"]\n      }\n    }\n  }\n\n  jar {\n    dependsOn createVersionFile\n    from(\"$buildDir\") {\n      include \"kafka/$buildVersionFileName\"\n    }\n  }\n\n  clean.doFirst {\n    delete \"$buildDir/kafka/\"\n  }\n\n  javadoc {\n    include \"**/org/apache/kafka/tools/api/*\"\n  }\n}\n\nproject(':tools') {\n  base {\n    archivesName = \"kafka-tools\"\n  }\n\n  dependencies {\n    implementation (project(':clients')){\n      exclude group: 'org.slf4j', module: '*'\n    }\n    implementation (project(':server-common')){\n      exclude group: 'org.slf4j', module: '*'\n    }\n    implementation (project(':log4j-appender')){\n      exclude group: 'org.slf4j', module: '*'\n    }\n    // AutoMQ inject start\n    implementation project(':automq-shell')\n    implementation libs.kafkaAvroSerializer\n    // AutoMQ inject end\n\n    implementation project(':storage')\n    implementation project(':connect:runtime')\n    implementation project(':tools:tools-api')\n    implementation project(':transaction-coordinator')\n    implementation libs.argparse4j\n    implementation libs.jacksonDatabind\n    implementation libs.jacksonDataformatCsv\n    implementation libs.jacksonJDK8Datatypes\n    implementation libs.slf4jApi\n    implementation libs.slf4jReload4j\n    implementation libs.joptSimple\n    implementation libs.awsSdkAuth\n    implementation libs.hdrHistogram\n    implementation libs.spotbugsAnnotations\n    implementation libs.guava\n\n    // for SASL/OAUTHBEARER JWT validation\n    implementation (libs.jose4j){\n      exclude group: 'org.slf4j', module: '*'\n    }\n    implementation libs.jacksonJaxrsJsonProvider\n    implementation project(':s3stream')\n    implementation libs.commonio\n\n\n    testImplementation (project(':clients')){\n      exclude group: 'org.slf4j', module: '*'\n    }\n    testImplementation project(':clients').sourceSets.test.output\n    testImplementation (project(':core')){\n      exclude group: 'org.slf4j', module: '*'\n    }\n    testImplementation project(':core').sourceSets.test.output\n    testImplementation (project(':server-common')){\n      exclude group: 'org.slf4j', module: '*'\n    }\n\n    implementation libs.jose4j                    // for SASL/OAUTHBEARER JWT validation\n    implementation libs.jacksonJaxrsJsonProvider\n\n    testImplementation project(':clients')\n    testImplementation project(':clients').sourceSets.test.output\n    testImplementation project(':server')\n    testImplementation project(':server').sourceSets.test.output\n    testImplementation project(':core')\n    testImplementation project(':core').sourceSets.test.output\n    testImplementation project(':server-common')\n    testImplementation project(':server-common').sourceSets.test.output\n    testImplementation project(':connect:api')\n    testImplementation project(':connect:runtime')\n    testImplementation project(':connect:runtime').sourceSets.test.output\n    testImplementation project(':storage:storage-api').sourceSets.main.output\n    testImplementation project(':group-coordinator')\n    testImplementation libs.junitJupiter\n    testImplementation libs.mockitoCore\n    testImplementation libs.mockitoJunitJupiter // supports MockitoExtension\n    testImplementation libs.bcpkix // required by the clients test module, but we have to specify it explicitly as gradle does not include the transitive test dependency automatically\n    testImplementation(libs.jfreechart) {\n      exclude group: 'junit', module: 'junit'\n    }\n    testImplementation libs.reload4j\n\n    testRuntimeOnly libs.junitPlatformLanucher\n  }\n\n  javadoc {\n    enabled = false\n  }\n\n  tasks.create(name: \"copyDependantLibs\", type: Copy) {\n    from (configurations.testRuntimeClasspath) {\n      include('slf4j-log4j12*')\n      include('reload4j*jar')\n    }\n    from (configurations.runtimeClasspath) {\n      exclude('kafka-clients*')\n    }\n    into \"$buildDir/dependant-libs-${versions.scala}\"\n    duplicatesStrategy 'exclude'\n  }\n\n  jar {\n    dependsOn 'copyDependantLibs'\n  }\n}\n\nproject(':trogdor') {\n  base {\n    archivesName = \"trogdor\"\n  }\n\n  dependencies {\n    implementation project(':clients')\n    implementation libs.argparse4j\n    implementation libs.jacksonDatabind\n    implementation libs.jacksonJDK8Datatypes\n    implementation libs.slf4jApi\n\n    implementation libs.jacksonJaxrsJsonProvider\n    implementation libs.jerseyContainerServlet\n    implementation libs.jerseyHk2\n    implementation libs.jaxbApi // Jersey dependency that was available in the JDK before Java 9\n    implementation libs.activation // Jersey dependency that was available in the JDK before Java 9\n    implementation libs.jettyServer\n    implementation libs.jettyServlet\n    implementation libs.jettyServlets\n\n    testImplementation project(':clients')\n    testImplementation libs.junitJupiter\n    testImplementation project(':clients').sourceSets.test.output\n    testImplementation libs.mockitoCore\n\n    testRuntimeOnly libs.slf4jReload4j\n    testRuntimeOnly libs.junitPlatformLanucher\n  }\n\n  javadoc {\n    enabled = false\n  }\n\n  tasks.create(name: \"copyDependantLibs\", type: Copy) {\n    from (configurations.testRuntimeClasspath) {\n      include('slf4j-log4j12*')\n      include('reload4j*jar')\n    }\n    from (configurations.runtimeClasspath) {\n      exclude('kafka-clients*')\n    }\n    into \"$buildDir/dependant-libs-${versions.scala}\"\n    duplicatesStrategy 'exclude'\n  }\n\n  jar {\n    dependsOn 'copyDependantLibs'\n  }\n}\n\nproject(':shell') {\n  base {\n    archivesName = \"kafka-shell\"\n  }\n\n  dependencies {\n    implementation libs.argparse4j\n    implementation libs.jacksonDatabind\n    implementation libs.jacksonJDK8Datatypes\n    implementation libs.jline\n    implementation libs.slf4jApi\n    implementation project(':server-common')\n    implementation project(':clients')\n    implementation project(':core')\n    implementation project(':metadata')\n    implementation project(':raft')\n\n    implementation libs.jose4j                    // for SASL/OAUTHBEARER JWT validation\n    implementation libs.jacksonJaxrsJsonProvider\n\n    implementation project(':s3stream')\n\n    testImplementation project(':clients')\n    testImplementation project(':clients').sourceSets.test.output\n    testImplementation project(':core')\n    testImplementation project(':core').sourceSets.test.output\n    testImplementation project(':server-common')\n    testImplementation project(':server-common').sourceSets.test.output\n    testImplementation libs.junitJupiter\n\n    testRuntimeOnly libs.slf4jReload4j\n    testRuntimeOnly libs.junitPlatformLanucher\n  }\n\n  javadoc {\n    enabled = false\n  }\n\n  tasks.create(name: \"copyDependantLibs\", type: Copy) {\n    from (configurations.testRuntimeClasspath) {\n      include('jline-*jar')\n    }\n    from (configurations.runtimeClasspath) {\n      include('jline-*jar')\n    }\n    into \"$buildDir/dependant-libs-${versions.scala}\"\n    duplicatesStrategy 'exclude'\n  }\n\n  jar {\n    dependsOn 'copyDependantLibs'\n  }\n}\n\nproject(':streams') {\n  base {\n    archivesName = \"kafka-streams\"\n  }\n\n  ext.buildStreamsVersionFileName = \"kafka-streams-version.properties\"\n\n  configurations {\n    generator\n  }\n\n  dependencies {\n    api project(':clients')\n    // `org.rocksdb.Options` is part of Kafka Streams public api via `RocksDBConfigSetter`\n    api libs.rocksDBJni\n\n    implementation libs.slf4jApi\n    implementation libs.jacksonAnnotations\n    implementation libs.jacksonDatabind\n\n    // testCompileOnly prevents streams from exporting a dependency on test-utils, which would cause a dependency cycle\n    testCompileOnly project(':streams:test-utils')\n\n    testImplementation project(':clients').sourceSets.test.output\n    testImplementation project(':server')\n    testImplementation project(':core')\n    testImplementation project(':tools')\n    testImplementation project(':core').sourceSets.test.output\n    testImplementation project(':storage')\n    testImplementation project(':group-coordinator')\n    testImplementation project(':transaction-coordinator')\n    testImplementation project(':server-common')\n    testImplementation project(':server-common').sourceSets.test.output\n    testImplementation project(':server')\n    testImplementation libs.reload4j\n    testImplementation libs.junitJupiter\n    testImplementation libs.bcpkix\n    testImplementation libs.hamcrest\n    testImplementation libs.mockitoCore\n    testImplementation libs.mockitoJunitJupiter // supports MockitoExtension\n    testImplementation libs.junitPlatformSuiteEngine // supports suite test\n    testImplementation project(':group-coordinator')\n\n    testRuntimeOnly project(':streams:test-utils')\n    testRuntimeOnly libs.slf4jReload4j\n    testRuntimeOnly libs.junitPlatformLanucher\n\n    generator project(':generator')\n  }\n\n  task processMessages(type:JavaExec) {\n    mainClass = \"org.apache.kafka.message.MessageGenerator\"\n    classpath = configurations.generator\n    args = [ \"-p\", \"org.apache.kafka.streams.internals.generated\",\n             \"-o\", \"src/generated/java/org/apache/kafka/streams/internals/generated\",\n             \"-i\", \"src/main/resources/common/message\",\n             \"-m\", \"MessageDataGenerator\"\n           ]\n    inputs.dir(\"src/main/resources/common/message\")\n        .withPropertyName(\"messages\")\n        .withPathSensitivity(PathSensitivity.RELATIVE)\n    outputs.cacheIf { true }\n    outputs.dir(\"src/generated/java/org/apache/kafka/streams/internals/generated\")\n  }\n\n  sourceSets {\n    main {\n      java {\n        srcDirs = [\"src/generated/java\", \"src/main/java\"]\n      }\n    }\n    test {\n      java {\n        srcDirs = [\"src/test/java\"]\n      }\n    }\n  }\n\n  compileJava.dependsOn 'processMessages'\n  srcJar.dependsOn 'processMessages'\n\n  javadoc {\n    include \"**/org/apache/kafka/streams/**\"\n    exclude \"**/org/apache/kafka/streams/internals/**\", \"**/org/apache/kafka/streams/**/internals/**\"\n  }\n\n  tasks.create(name: \"copyDependantLibs\", type: Copy) {\n    from (configurations.runtimeClasspath) {\n      exclude('kafka-clients*')\n    }\n    into \"$buildDir/dependant-libs-${versions.scala}\"\n    duplicatesStrategy 'exclude'\n  }\n\n  task createStreamsVersionFile() {\n    def receiptFile = file(\"$buildDir/kafka/$buildStreamsVersionFileName\")\n    inputs.property \"commitId\", commitId\n    inputs.property \"version\", version\n    outputs.file receiptFile\n\n    doLast {\n      def data = [\n              commitId: commitId,\n              version: version,\n      ]\n\n      receiptFile.parentFile.mkdirs()\n      def content = data.entrySet().collect { \"$it.key=$it.value\" }.sort().join(\"\\n\")\n      receiptFile.setText(content, \"ISO-8859-1\")\n    }\n  }\n\n  jar {\n    dependsOn 'createStreamsVersionFile'\n    from(\"$buildDir\") {\n      include \"kafka/$buildStreamsVersionFileName\"\n    }\n    dependsOn 'copyDependantLibs'\n  }\n\n  systemTestLibs {\n    dependsOn testJar\n  }\n\n  task genStreamsConfigDocs(type: JavaExec) {\n    classpath = sourceSets.main.runtimeClasspath\n    mainClass = 'org.apache.kafka.streams.StreamsConfig'\n    if( !generatedDocsDir.exists() ) { generatedDocsDir.mkdirs() }\n    standardOutput = new File(generatedDocsDir, \"streams_config.html\").newOutputStream()\n  }\n\n  task testAll(\n    dependsOn: [\n            ':streams:test',\n            ':streams:test-utils:test',\n            ':streams:streams-scala:test',\n            ':streams:upgrade-system-tests-0100:test',\n            ':streams:upgrade-system-tests-0101:test',\n            ':streams:upgrade-system-tests-0102:test',\n            ':streams:upgrade-system-tests-0110:test',\n            ':streams:upgrade-system-tests-10:test',\n            ':streams:upgrade-system-tests-11:test',\n            ':streams:upgrade-system-tests-20:test',\n            ':streams:upgrade-system-tests-21:test',\n            ':streams:upgrade-system-tests-22:test',\n            ':streams:upgrade-system-tests-23:test',\n            ':streams:upgrade-system-tests-24:test',\n            ':streams:upgrade-system-tests-25:test',\n            ':streams:upgrade-system-tests-26:test',\n            ':streams:upgrade-system-tests-27:test',\n            ':streams:upgrade-system-tests-28:test',\n            ':streams:upgrade-system-tests-30:test',\n            ':streams:upgrade-system-tests-31:test',\n            ':streams:upgrade-system-tests-32:test',\n            ':streams:upgrade-system-tests-33:test',\n            ':streams:upgrade-system-tests-34:test',\n            ':streams:upgrade-system-tests-35:test',\n            ':streams:upgrade-system-tests-36:test',\n            ':streams:upgrade-system-tests-37:test',\n            ':streams:examples:test'\n    ]\n  )\n}\n\nproject(':streams:streams-scala') {\n  apply plugin: 'scala'\n\n  base {\n    archivesName = \"kafka-streams-scala_${versions.baseScala}\"\n  }\n\n  dependencies {\n    api project(':streams')\n\n    api libs.scalaLibrary\n    if ( versions.baseScala == '2.12' ) {\n      // Scala-Collection-Compat isn't required when compiling with Scala 2.13 or later,\n      // and having it in the dependencies could lead to classpath conflicts in Scala 3\n      // projects that use kafka-streams-kafka_2.13 (because we don't have a Scala 3 version yet)\n      // but also pull in scala-collection-compat_3 via another dependency.\n      // So we make sure to not include it in the dependencies.\n      api libs.scalaCollectionCompat\n    }\n    testImplementation project(':group-coordinator')\n    testImplementation project(':core')\n    testImplementation project(':core').sourceSets.test.output\n    testImplementation project(':server-common').sourceSets.test.output\n    testImplementation project(':streams').sourceSets.test.output\n    testImplementation project(':clients').sourceSets.test.output\n    testImplementation project(':streams:test-utils')\n\n    testImplementation libs.junitJupiter\n    testImplementation libs.mockitoCore\n    testImplementation libs.mockitoJunitJupiter // supports MockitoExtension\n    testImplementation libs.hamcrest\n    testRuntimeOnly libs.slf4jReload4j\n    testRuntimeOnly libs.junitPlatformLanucher\n  }\n\n  javadoc {\n    include \"**/org/apache/kafka/streams/scala/**\"\n  }\n\n  scaladoc {\n    scalaDocOptions.additionalParameters = [\"-no-link-warnings\"]\n  }\n\n  tasks.create(name: \"copyDependantLibs\", type: Copy) {\n    from (configurations.runtimeClasspath) {\n      exclude('kafka-streams*')\n    }\n    into \"$buildDir/dependant-libs-${versions.scala}\"\n    duplicatesStrategy 'exclude'\n  }\n\n  jar {\n    dependsOn 'copyDependantLibs'\n  }\n\n  // spotless 6.14 requires Java 11 at runtime\n  if (JavaVersion.current().isJava11Compatible()) {\n    apply plugin: 'com.diffplug.spotless'\n    spotless {\n      scala {\n        target '**/*.scala'\n        scalafmt(\"$versions.scalafmt\").configFile('../../checkstyle/.scalafmt.conf').scalaMajorVersion(versions.baseScala)\n        licenseHeaderFile '../../checkstyle/java.header', 'package'\n      }\n    }\n  }\n}\n\nproject(':streams:test-utils') {\n  base {\n    archivesName = \"kafka-streams-test-utils\"\n  }\n\n  dependencies {\n    api project(':streams')\n    api project(':clients')\n\n    implementation libs.slf4jApi\n\n    testImplementation project(':clients').sourceSets.test.output\n    testImplementation libs.junitJupiter\n    testImplementation libs.mockitoCore\n    testImplementation libs.hamcrest\n\n    testRuntimeOnly libs.slf4jReload4j\n    testRuntimeOnly libs.junitPlatformLanucher\n  }\n\n  tasks.create(name: \"copyDependantLibs\", type: Copy) {\n    from (configurations.runtimeClasspath) {\n      exclude('kafka-streams*')\n    }\n    into \"$buildDir/dependant-libs-${versions.scala}\"\n    duplicatesStrategy 'exclude'\n  }\n\n  jar {\n    dependsOn 'copyDependantLibs'\n  }\n\n}\n\nproject(':streams:examples') {\n  base {\n    archivesName = \"kafka-streams-examples\"\n  }\n\n  dependencies {\n    // this dependency should be removed after we unify data API\n    implementation(project(':connect:json')) {\n      // this transitive dependency is not used in Streams, and it breaks SBT builds\n      exclude module: 'javax.ws.rs-api'\n    }\n\n    implementation project(':streams')\n\n    implementation libs.slf4jReload4j\n\n    testImplementation project(':streams:test-utils')\n    testImplementation project(':clients').sourceSets.test.output // for org.apache.kafka.test.IntegrationTest\n    testImplementation libs.junitJupiter\n    testImplementation libs.hamcrest\n\n    testRuntimeOnly libs.junitPlatformLanucher\n  }\n\n  javadoc {\n    enabled = false\n  }\n\n  tasks.create(name: \"copyDependantLibs\", type: Copy) {\n    from (configurations.runtimeClasspath) {\n      exclude('kafka-streams*')\n    }\n    into \"$buildDir/dependant-libs-${versions.scala}\"\n    duplicatesStrategy 'exclude'\n  }\n\n  jar {\n    dependsOn 'copyDependantLibs'\n  }\n}\n\nproject(':streams:upgrade-system-tests-0100') {\n  base {\n    archivesName = \"kafka-streams-upgrade-system-tests-0100\"\n  }\n\n  dependencies {\n    testImplementation(libs.kafkaStreams_0100) {\n      exclude group: 'org.slf4j', module: 'slf4j-log4j12'\n      exclude group: 'log4j', module: 'log4j'\n    }\n    testRuntimeOnly libs.junitJupiter\n  }\n\n  systemTestLibs {\n    dependsOn testJar\n  }\n}\n\nproject(':streams:upgrade-system-tests-0101') {\n  base {\n    archivesName = \"kafka-streams-upgrade-system-tests-0101\"\n  }\n\n  dependencies {\n    testImplementation(libs.kafkaStreams_0101) {\n      exclude group: 'org.slf4j', module: 'slf4j-log4j12'\n      exclude group: 'log4j', module: 'log4j'\n    }\n    testRuntimeOnly libs.junitJupiter\n  }\n\n  systemTestLibs {\n    dependsOn testJar\n  }\n}\n\nproject(':streams:upgrade-system-tests-0102') {\n  base {\n    archivesName = \"kafka-streams-upgrade-system-tests-0102\"\n  }\n\n  dependencies {\n    testImplementation libs.kafkaStreams_0102\n    testRuntimeOnly libs.junitJupiter\n  }\n\n  systemTestLibs {\n    dependsOn testJar\n  }\n}\n\nproject(':streams:upgrade-system-tests-0110') {\n  base{\n    archivesName = \"kafka-streams-upgrade-system-tests-0110\"\n  }\n\n  dependencies {\n    testImplementation libs.kafkaStreams_0110\n    testRuntimeOnly libs.junitJupiter\n  }\n\n  systemTestLibs {\n    dependsOn testJar\n  }\n}\n\nproject(':streams:upgrade-system-tests-10') {\n  base {\n    archivesName = \"kafka-streams-upgrade-system-tests-10\"\n  }\n\n  dependencies {\n    testImplementation libs.kafkaStreams_10\n    testRuntimeOnly libs.junitJupiter\n  }\n\n  systemTestLibs {\n    dependsOn testJar\n  }\n}\n\nproject(':streams:upgrade-system-tests-11') {\n  base {\n    archivesName = \"kafka-streams-upgrade-system-tests-11\"\n  }\n\n  dependencies {\n    testImplementation libs.kafkaStreams_11\n    testRuntimeOnly libs.junitJupiter\n  }\n\n  systemTestLibs {\n    dependsOn testJar\n  }\n}\n\nproject(':streams:upgrade-system-tests-20') {\n  base {\n    archivesName = \"kafka-streams-upgrade-system-tests-20\"\n  }\n\n  dependencies {\n    testImplementation libs.kafkaStreams_20\n    testRuntimeOnly libs.junitJupiter\n  }\n\n  systemTestLibs {\n    dependsOn testJar\n  }\n}\n\nproject(':streams:upgrade-system-tests-21') {\n  base {\n    archivesName = \"kafka-streams-upgrade-system-tests-21\"\n  }\n\n  dependencies {\n    testImplementation libs.kafkaStreams_21\n    testRuntimeOnly libs.junitJupiter\n  }\n\n  systemTestLibs {\n    dependsOn testJar\n  }\n}\n\nproject(':streams:upgrade-system-tests-22') {\n  base {\n    archivesName = \"kafka-streams-upgrade-system-tests-22\"\n  }\n\n  dependencies {\n    testImplementation libs.kafkaStreams_22\n    testRuntimeOnly libs.junitJupiter\n  }\n\n  systemTestLibs {\n    dependsOn testJar\n  }\n}\n\nproject(':streams:upgrade-system-tests-23') {\n  base {\n    archivesName = \"kafka-streams-upgrade-system-tests-23\"\n  }\n\n  dependencies {\n    testImplementation libs.kafkaStreams_23\n    testRuntimeOnly libs.junitJupiter\n  }\n\n  systemTestLibs {\n    dependsOn testJar\n  }\n}\n\nproject(':streams:upgrade-system-tests-24') {\n  base {\n    archivesName = \"kafka-streams-upgrade-system-tests-24\"\n  }\n\n  dependencies {\n    testImplementation libs.kafkaStreams_24\n    testRuntimeOnly libs.junitJupiter\n  }\n\n  systemTestLibs {\n    dependsOn testJar\n  }\n}\n\nproject(':streams:upgrade-system-tests-25') {\n  base {\n    archivesName = \"kafka-streams-upgrade-system-tests-25\"\n  }\n\n  dependencies {\n    testImplementation libs.kafkaStreams_25\n    testRuntimeOnly libs.junitJupiter\n  }\n\n  systemTestLibs {\n    dependsOn testJar\n  }\n}\n\nproject(':streams:upgrade-system-tests-26') {\n  base {\n    archivesName = \"kafka-streams-upgrade-system-tests-26\"\n  }\n\n  dependencies {\n    testImplementation libs.kafkaStreams_26\n    testRuntimeOnly libs.junitJupiter\n  }\n\n  systemTestLibs {\n    dependsOn testJar\n  }\n}\n\nproject(':streams:upgrade-system-tests-27') {\n  base {\n    archivesName = \"kafka-streams-upgrade-system-tests-27\"\n  }\n\n  dependencies {\n    testImplementation libs.kafkaStreams_27\n    testRuntimeOnly libs.junitJupiter\n  }\n\n  systemTestLibs {\n    dependsOn testJar\n  }\n}\n\nproject(':streams:upgrade-system-tests-28') {\n  base {\n    archivesName = \"kafka-streams-upgrade-system-tests-28\"\n  }\n\n  dependencies {\n    testImplementation libs.kafkaStreams_28\n    testRuntimeOnly libs.junitJupiter\n  }\n\n  systemTestLibs {\n    dependsOn testJar\n  }\n}\n\nproject(':streams:upgrade-system-tests-30') {\n  base {\n    archivesName = \"kafka-streams-upgrade-system-tests-30\"\n  }\n\n  dependencies {\n    testImplementation libs.kafkaStreams_30\n    testRuntimeOnly libs.junitJupiter\n  }\n\n  systemTestLibs {\n    dependsOn testJar\n  }\n}\n\nproject(':streams:upgrade-system-tests-31') {\n  base {\n    archivesName = \"kafka-streams-upgrade-system-tests-31\"\n  }\n\n  dependencies {\n    testImplementation libs.kafkaStreams_31\n    testRuntimeOnly libs.junitJupiter\n  }\n\n  systemTestLibs {\n    dependsOn testJar\n  }\n}\n\nproject(':streams:upgrade-system-tests-32') {\n  base {\n    archivesName = \"kafka-streams-upgrade-system-tests-32\"\n  }\n\n  dependencies {\n    testImplementation libs.kafkaStreams_32\n    testRuntimeOnly libs.junitJupiter\n  }\n\n  systemTestLibs {\n    dependsOn testJar\n  }\n}\n\nproject(':streams:upgrade-system-tests-33') {\n  base {\n    archivesName = \"kafka-streams-upgrade-system-tests-33\"\n  }\n\n  dependencies {\n    testImplementation libs.kafkaStreams_33\n    testRuntimeOnly libs.junitJupiter\n  }\n\n  systemTestLibs {\n    dependsOn testJar\n  }\n}\n\nproject(':streams:upgrade-system-tests-34') {\n  base {\n    archivesName = \"kafka-streams-upgrade-system-tests-34\"\n  }\n\n  dependencies {\n    testImplementation libs.kafkaStreams_34\n    testRuntimeOnly libs.junitJupiter\n  }\n\n  systemTestLibs {\n    dependsOn testJar\n  }\n}\n\nproject(':streams:upgrade-system-tests-35') {\n  base {\n    archivesName = \"kafka-streams-upgrade-system-tests-35\"\n  }\n\n  dependencies {\n    testImplementation libs.kafkaStreams_35\n    testRuntimeOnly libs.junitJupiter\n  }\n\n  systemTestLibs {\n    dependsOn testJar\n  }\n}\n\nproject(':streams:upgrade-system-tests-36') {\n  base {\n    archivesName = \"kafka-streams-upgrade-system-tests-36\"\n  }\n\n  dependencies {\n    testImplementation libs.kafkaStreams_36\n    testRuntimeOnly libs.junitJupiter\n  }\n\n  systemTestLibs {\n    dependsOn testJar\n  }\n}\n\nproject(':streams:upgrade-system-tests-37') {\n  base {\n    archivesName = \"kafka-streams-upgrade-system-tests-37\"\n  }\n\n  dependencies {\n    testImplementation libs.kafkaStreams_37\n    testRuntimeOnly libs.junitJupiter\n  }\n\n  systemTestLibs {\n    dependsOn testJar\n  }\n}\n\nproject(':jmh-benchmarks') {\n\n  apply plugin: 'io.github.goooler.shadow'\n\n  shadowJar {\n    archiveBaseName = 'kafka-jmh-benchmarks'\n  }\n\n  dependencies {\n    implementation(project(':core')) {\n      // jmh requires jopt 4.x while `core` depends on 5.0, they are not binary compatible\n      exclude group: 'net.sf.jopt-simple', module: 'jopt-simple'\n    }\n    implementation project(':server-common')\n    implementation project(':server')\n    implementation project(':raft')\n    implementation project(':clients')\n    implementation project(':group-coordinator')\n    implementation project(':group-coordinator:group-coordinator-api')\n    implementation project(':metadata')\n    implementation project(':storage')\n    implementation project(':streams')\n    implementation project(':core')\n    implementation project(':connect:api')\n    implementation project(':connect:transforms')\n    implementation project(':connect:json')\n    implementation project(':clients').sourceSets.test.output\n    implementation project(':core').sourceSets.test.output\n    implementation project(':server-common').sourceSets.test.output\n\n    implementation libs.jmhCore\n    annotationProcessor libs.jmhGeneratorAnnProcess\n    implementation libs.jmhCoreBenchmarks\n    implementation libs.jacksonDatabind\n    implementation libs.metrics\n    implementation libs.mockitoCore\n    implementation libs.slf4jReload4j\n    implementation libs.scalaLibrary\n    implementation libs.scalaJava8Compat\n  }\n\n  tasks.withType(JavaCompile) {\n    // Suppress warning caused by code generated by jmh: `warning: [cast] redundant cast to long`\n    options.compilerArgs << \"-Xlint:-cast\"\n  }\n\n  jar {\n    manifest {\n      attributes \"Main-Class\": \"org.openjdk.jmh.Main\"\n    }\n  }\n\n  checkstyle {\n    configProperties = checkstyleConfigProperties(\"import-control-jmh-benchmarks.xml\")\n  }\n\n  task jmh(type: JavaExec, dependsOn: [':jmh-benchmarks:clean', ':jmh-benchmarks:shadowJar']) {\n\n    mainClass = \"-jar\"\n\n    doFirst {\n      if (System.getProperty(\"jmhArgs\")) {\n          args System.getProperty(\"jmhArgs\").split(' ')\n      }\n      args = [shadowJar.archivePath, *args]\n    }\n  }\n\n  javadoc {\n     enabled = false\n  }\n}\n\nproject(':log4j-appender') {\n  base {\n    archivesName = \"kafka-log4j-appender\"\n  }\n\n  dependencies {\n    implementation project(':clients')\n    implementation libs.slf4jReload4j\n\n    testImplementation project(':clients').sourceSets.test.output\n    testImplementation libs.junitJupiter\n    testImplementation libs.hamcrest\n    testImplementation libs.mockitoCore\n\n    testRuntimeOnly libs.junitPlatformLanucher\n  }\n\n  javadoc {\n    enabled = false\n  }\n\n}\n\nproject(':connect:api') {\n  base {\n    archivesName = \"connect-api\"\n  }\n\n  dependencies {\n    api project(':clients')\n    implementation libs.slf4jApi\n    implementation libs.jaxrsApi\n\n    testImplementation libs.junitJupiter\n    testRuntimeOnly libs.junitPlatformLanucher\n    testRuntimeOnly libs.slf4jReload4j\n    testImplementation project(':clients').sourceSets.test.output\n  }\n\n  javadoc {\n    include \"**/org/apache/kafka/connect/**\" // needed for the `aggregatedJavadoc` task\n  }\n\n  tasks.create(name: \"copyDependantLibs\", type: Copy) {\n    from (configurations.testRuntimeClasspath) {\n      include('slf4j-log4j12*')\n      include('reload4j*jar')\n    }\n    from (configurations.runtimeClasspath) {\n      exclude('kafka-clients*')\n      exclude('connect-*')\n    }\n    into \"$buildDir/dependant-libs\"\n    duplicatesStrategy 'exclude'\n  }\n\n  jar {\n    dependsOn copyDependantLibs\n  }\n}\n\nproject(':connect:transforms') {\n  base {\n    archivesName = \"connect-transforms\"\n  }\n\n  dependencies {\n    api project(':connect:api')\n\n    implementation libs.slf4jApi\n\n    testImplementation libs.junitJupiter\n\n    testRuntimeOnly libs.slf4jReload4j\n    testRuntimeOnly libs.junitPlatformLanucher\n    testImplementation project(':clients').sourceSets.test.output\n  }\n\n  javadoc {\n    enabled = false\n  }\n\n  tasks.create(name: \"copyDependantLibs\", type: Copy) {\n    from (configurations.testRuntimeClasspath) {\n      include('slf4j-log4j12*')\n      include('reload4j*jar')\n    }\n    from (configurations.runtimeClasspath) {\n      exclude('kafka-clients*')\n      exclude('connect-*')\n    }\n    into \"$buildDir/dependant-libs\"\n    duplicatesStrategy 'exclude'\n  }\n\n  jar {\n    dependsOn copyDependantLibs\n  }\n}\n\nproject(':connect:json') {\n  base {\n    archivesName = \"connect-json\"\n  }\n\n  dependencies {\n    api project(':connect:api')\n\n    api libs.jacksonDatabind\n    api libs.jacksonJDK8Datatypes\n    api libs.jacksonAfterburner\n\n    implementation libs.slf4jApi\n\n    testImplementation libs.junitJupiter\n\n    testRuntimeOnly libs.slf4jReload4j\n    testRuntimeOnly libs.junitPlatformLanucher\n    testImplementation project(':clients').sourceSets.test.output\n  }\n\n  javadoc {\n    enabled = false\n  }\n\n  tasks.create(name: \"copyDependantLibs\", type: Copy) {\n    from (configurations.testRuntimeClasspath) {\n      include('slf4j-log4j12*')\n      include('reload4j*jar')\n    }\n    from (configurations.runtimeClasspath) {\n      exclude('kafka-clients*')\n      exclude('connect-*')\n    }\n    into \"$buildDir/dependant-libs\"\n    duplicatesStrategy 'exclude'\n  }\n\n  jar {\n    dependsOn copyDependantLibs\n  }\n}\n\nproject(':connect:runtime') {\n  configurations {\n    swagger\n  }\n\n  base {\n    archivesName = \"connect-runtime\"\n  }\n\n  dependencies {\n    // connect-runtime is used in tests, use `api` for modules below for backwards compatibility even though\n    // applications should generally not depend on `connect-runtime`\n    api project(':connect:api')\n    api project(':clients')\n    api project(':connect:json')\n    api project(':connect:transforms')\n\n    implementation libs.slf4jApi\n    implementation libs.reload4j\n    implementation libs.jose4j                    // for SASL/OAUTHBEARER JWT validation\n    implementation libs.jacksonAnnotations\n    implementation libs.jacksonJaxrsJsonProvider\n    implementation libs.jerseyContainerServlet\n    implementation libs.jerseyHk2\n    implementation libs.jaxbApi // Jersey dependency that was available in the JDK before Java 9\n    implementation libs.activation // Jersey dependency that was available in the JDK before Java 9\n    implementation libs.jettyServer\n    implementation libs.jettyServlet\n    implementation libs.jettyServlets\n    implementation libs.jettyClient\n    implementation libs.reflections\n    implementation libs.mavenArtifact\n    implementation libs.swaggerAnnotations\n\n    // We use this library to generate OpenAPI docs for the REST API, but we don't want or need it at compile\n    // or run time. So, we add it to a separate configuration, which we use later on during docs generation\n    swagger libs.swaggerJaxrs2\n\n    testImplementation project(':clients').sourceSets.test.output\n    testImplementation project(':core')\n    testImplementation project(':metadata')\n    testImplementation project(':server-common')\n    testImplementation project(':core').sourceSets.test.output\n    testImplementation project(':server-common')\n    testImplementation project(':server')\n    testImplementation project(':group-coordinator')\n    testImplementation project(':storage')\n    testImplementation project(':connect:test-plugins')\n    testImplementation project(':server-common').sourceSets.test.output\n\n    testImplementation libs.junitJupiter\n    testImplementation libs.mockitoCore\n    testImplementation libs.hamcrest\n    testImplementation libs.mockitoJunitJupiter\n    testImplementation libs.httpclient\n\n    testRuntimeOnly libs.slf4jReload4j\n    testRuntimeOnly libs.bcpkix\n    testRuntimeOnly libs.junitPlatformLanucher\n  }\n\n  javadoc {\n    enabled = false\n  }\n\n  tasks.create(name: \"copyDependantLibs\", type: Copy) {\n    from (configurations.testRuntimeClasspath) {\n      // No need to copy log4j since the module has an explicit dependency on that\n      include('slf4j-log4j12*')\n    }\n    from (configurations.runtimeClasspath) {\n      exclude('kafka-clients*')\n      exclude('connect-*')\n    }\n    into \"$buildDir/dependant-libs\"\n    duplicatesStrategy 'exclude'\n  }\n\n  jar {\n    dependsOn copyDependantLibs\n  }\n\n  task genConnectConfigDocs(type: JavaExec) {\n    classpath = sourceSets.main.runtimeClasspath\n    mainClass = 'org.apache.kafka.connect.runtime.distributed.DistributedConfig'\n    if( !generatedDocsDir.exists() ) { generatedDocsDir.mkdirs() }\n    standardOutput = new File(generatedDocsDir, \"connect_config.html\").newOutputStream()\n  }\n\n  task genSinkConnectorConfigDocs(type: JavaExec) {\n    classpath = sourceSets.main.runtimeClasspath\n    mainClass = 'org.apache.kafka.connect.runtime.SinkConnectorConfig'\n    if( !generatedDocsDir.exists() ) { generatedDocsDir.mkdirs() }\n    standardOutput = new File(generatedDocsDir, \"sink_connector_config.html\").newOutputStream()\n  }\n\n  task genSourceConnectorConfigDocs(type: JavaExec) {\n    classpath = sourceSets.main.runtimeClasspath\n    mainClass = 'org.apache.kafka.connect.runtime.SourceConnectorConfig'\n    if( !generatedDocsDir.exists() ) { generatedDocsDir.mkdirs() }\n    standardOutput = new File(generatedDocsDir, \"source_connector_config.html\").newOutputStream()\n  }\n\n  task genConnectTransformationDocs(type: JavaExec) {\n    classpath = sourceSets.main.runtimeClasspath\n    mainClass = 'org.apache.kafka.connect.tools.TransformationDoc'\n    if( !generatedDocsDir.exists() ) { generatedDocsDir.mkdirs() }\n    standardOutput = new File(generatedDocsDir, \"connect_transforms.html\").newOutputStream()\n  }\n\n  task genConnectPredicateDocs(type: JavaExec) {\n    classpath = sourceSets.main.runtimeClasspath\n    mainClass = 'org.apache.kafka.connect.tools.PredicateDoc'\n    if( !generatedDocsDir.exists() ) { generatedDocsDir.mkdirs() }\n    standardOutput = new File(generatedDocsDir, \"connect_predicates.html\").newOutputStream()\n  }\n\n  task genConnectMetricsDocs(type: JavaExec) {\n    classpath = sourceSets.main.runtimeClasspath\n    mainClass = 'org.apache.kafka.connect.runtime.ConnectMetrics'\n    if( !generatedDocsDir.exists() ) { generatedDocsDir.mkdirs() }\n    standardOutput = new File(generatedDocsDir, \"connect_metrics.html\").newOutputStream()\n  }\n\n  task setVersionInOpenAPISpec(type: Copy) {\n    from \"$rootDir/gradle/openapi.template\"\n    into \"$buildDir/resources/docs\"\n    rename ('openapi.template', 'openapi.yaml')\n    expand(kafkaVersion: \"$rootProject.version\")\n  }\n\n  task genConnectOpenAPIDocs(type: io.swagger.v3.plugins.gradle.tasks.ResolveTask, dependsOn: setVersionInOpenAPISpec) {\n    classpath = sourceSets.main.runtimeClasspath\n\n    buildClasspath = classpath + configurations.swagger\n    outputFileName = 'connect_rest'\n    outputFormat = 'YAML'\n    prettyPrint = 'TRUE'\n    sortOutput = 'TRUE'\n    openApiFile = file(\"$buildDir/resources/docs/openapi.yaml\")\n    resourcePackages = ['org.apache.kafka.connect.runtime.rest.resources']\n    if( !generatedDocsDir.exists() ) { generatedDocsDir.mkdirs() }\n    outputDir = file(generatedDocsDir)\n  }\n\n}\n\nproject(':connect:file') {\n  base {\n    archivesName = \"connect-file\"\n  }\n\n  dependencies {\n    implementation project(':connect:api')\n    implementation libs.slf4jApi\n\n    testImplementation libs.junitJupiter\n    testImplementation libs.mockitoCore\n\n    testRuntimeOnly libs.slf4jReload4j\n    testRuntimeOnly libs.junitPlatformLanucher\n    testImplementation project(':clients').sourceSets.test.output\n    testImplementation project(':connect:runtime')\n    testImplementation project(':connect:runtime').sourceSets.test.output\n    testImplementation project(':core')\n    testImplementation project(':core').sourceSets.test.output\n    testImplementation project(':server-common').sourceSets.test.output\n  }\n\n  javadoc {\n    enabled = false\n  }\n\n  tasks.create(name: \"copyDependantLibs\", type: Copy) {\n    from (configurations.testRuntimeClasspath) {\n      include('slf4j-log4j12*')\n      include('reload4j*jar')\n    }\n    from (configurations.runtimeClasspath) {\n      exclude('kafka-clients*')\n      exclude('connect-*')\n    }\n    into \"$buildDir/dependant-libs\"\n    duplicatesStrategy 'exclude'\n  }\n\n  jar {\n    dependsOn copyDependantLibs\n  }\n}\n\nproject(':connect:basic-auth-extension') {\n  base {\n    archivesName = \"connect-basic-auth-extension\"\n  }\n\n  dependencies {\n    implementation project(':connect:api')\n    implementation libs.slf4jApi\n    implementation libs.jaxrsApi\n    implementation libs.jaxAnnotationApi\n\n    testImplementation libs.bcpkix\n    testImplementation libs.mockitoCore\n    testImplementation libs.junitJupiter\n    testImplementation project(':clients').sourceSets.test.output\n\n    testRuntimeOnly libs.slf4jReload4j\n    testRuntimeOnly libs.jerseyContainerServlet\n    testRuntimeOnly libs.junitPlatformLanucher\n  }\n\n  javadoc {\n    enabled = false\n  }\n\n  tasks.create(name: \"copyDependantLibs\", type: Copy) {\n    from (configurations.testRuntimeClasspath) {\n      include('slf4j-log4j12*')\n      include('reload4j*jar')\n    }\n    from (configurations.runtimeClasspath) {\n      exclude('kafka-clients*')\n      exclude('connect-*')\n    }\n    into \"$buildDir/dependant-libs\"\n    duplicatesStrategy 'exclude'\n  }\n\n  jar {\n    dependsOn copyDependantLibs\n  }\n}\n\nproject(':connect:mirror') {\n  base {\n    archivesName = \"connect-mirror\"\n  }\n\n  dependencies {\n    implementation project(':connect:api')\n    implementation project(':connect:runtime')\n    implementation project(':connect:mirror-client')\n    implementation project(':clients')\n\n    implementation libs.argparse4j\n    implementation libs.jacksonAnnotations\n    implementation libs.slf4jApi\n    implementation libs.jacksonAnnotations\n    implementation libs.jacksonJaxrsJsonProvider\n    implementation libs.jerseyContainerServlet\n    implementation libs.jerseyHk2\n    implementation libs.jaxbApi // Jersey dependency that was available in the JDK before Java 9\n    implementation libs.activation // Jersey dependency that was available in the JDK before Java 9\n    implementation libs.jettyServer\n    implementation libs.jettyServlet\n    implementation libs.jettyServlets\n    implementation libs.jettyClient\n    implementation libs.swaggerAnnotations\n\n    testImplementation libs.junitJupiter\n    testImplementation libs.reload4j\n    testImplementation libs.mockitoCore\n    testImplementation project(':clients').sourceSets.test.output\n    testImplementation project(':connect:runtime').sourceSets.test.output\n    testImplementation project(':core')\n    testImplementation project(':core').sourceSets.test.output\n    testImplementation project(':server')\n    testImplementation project(':server-common').sourceSets.test.output\n\n    testRuntimeOnly project(':connect:runtime')\n    testRuntimeOnly libs.slf4jReload4j\n    testRuntimeOnly libs.bcpkix\n    testRuntimeOnly libs.junitPlatformLanucher\n  }\n\n  javadoc {\n    enabled = false\n  }\n\n  tasks.create(name: \"copyDependantLibs\", type: Copy) {\n    from (configurations.testRuntimeClasspath) {\n      include('slf4j-log4j12*')\n      include('reload4j*jar')\n    }\n    from (configurations.runtimeClasspath) {\n      exclude('kafka-clients*')\n      exclude('connect-*')\n    }\n    into \"$buildDir/dependant-libs\"\n    duplicatesStrategy 'exclude'\n  }\n\n  task genMirrorConnectorConfigDocs(type: JavaExec) {\n    classpath = sourceSets.main.runtimeClasspath\n    mainClass = 'org.apache.kafka.connect.mirror.MirrorConnectorConfig'\n    if( !generatedDocsDir.exists() ) { generatedDocsDir.mkdirs() }\n    standardOutput = new File(generatedDocsDir, \"mirror_connector_config.html\").newOutputStream()\n  }\n\n  task genMirrorSourceConfigDocs(type: JavaExec) {\n    classpath = sourceSets.main.runtimeClasspath\n    mainClass = 'org.apache.kafka.connect.mirror.MirrorSourceConfig'\n    if( !generatedDocsDir.exists() ) { generatedDocsDir.mkdirs() }\n    standardOutput = new File(generatedDocsDir, \"mirror_source_config.html\").newOutputStream()\n  }\n\n  task genMirrorCheckpointConfigDocs(type: JavaExec) {\n    classpath = sourceSets.main.runtimeClasspath\n    mainClass = 'org.apache.kafka.connect.mirror.MirrorCheckpointConfig'\n    if( !generatedDocsDir.exists() ) { generatedDocsDir.mkdirs() }\n    standardOutput = new File(generatedDocsDir, \"mirror_checkpoint_config.html\").newOutputStream()\n  }\n\n  task genMirrorHeartbeatConfigDocs(type: JavaExec) {\n    classpath = sourceSets.main.runtimeClasspath\n    mainClass = 'org.apache.kafka.connect.mirror.MirrorHeartbeatConfig'\n    if( !generatedDocsDir.exists() ) { generatedDocsDir.mkdirs() }\n    standardOutput = new File(generatedDocsDir, \"mirror_heartbeat_config.html\").newOutputStream()\n  }\n\n  jar {\n    dependsOn copyDependantLibs\n  }\n}\n\nproject(':connect:mirror-client') {\n  base {\n    archivesName = \"connect-mirror-client\"\n  }\n\n  dependencies {\n    implementation project(':clients')\n    implementation libs.slf4jApi\n\n    testImplementation libs.junitJupiter\n    testImplementation project(':clients').sourceSets.test.output\n\n    testRuntimeOnly libs.slf4jReload4j\n    testRuntimeOnly libs.junitPlatformLanucher\n  }\n\n  javadoc {\n    enabled = true\n  }\n\n  tasks.create(name: \"copyDependantLibs\", type: Copy) {\n    from (configurations.testRuntimeClasspath) {\n      include('slf4j-log4j12*')\n      include('reload4j*jar')\n    }\n    from (configurations.runtimeClasspath) {\n      exclude('kafka-clients*')\n      exclude('connect-*')\n    }\n    into \"$buildDir/dependant-libs\"\n    duplicatesStrategy 'exclude'\n  }\n\n  jar {\n    dependsOn copyDependantLibs\n  }\n}\n\nproject(':connect:test-plugins') {\n  base {\n    archivesName = \"connect-test-plugins\"\n  }\n\n  dependencies {\n    api project(':connect:api')\n\n    implementation project(':server-common')\n    implementation libs.slf4jApi\n    implementation libs.jacksonDatabind\n  }\n}\n\ntask aggregatedJavadoc(type: Javadoc, dependsOn: compileJava) {\n  def projectsWithJavadoc = subprojects.findAll { it.javadoc.enabled }\n  source = projectsWithJavadoc.collect { it.sourceSets.main.allJava }\n  classpath = files(projectsWithJavadoc.collect { it.sourceSets.main.compileClasspath })\n  includes = projectsWithJavadoc.collectMany { it.javadoc.getIncludes() }\n  excludes = projectsWithJavadoc.collectMany { it.javadoc.getExcludes() }\n}\n"
        },
        {
          "name": "checkstyle",
          "type": "tree",
          "content": null
        },
        {
          "name": "clients",
          "type": "tree",
          "content": null
        },
        {
          "name": "config",
          "type": "tree",
          "content": null
        },
        {
          "name": "connect",
          "type": "tree",
          "content": null
        },
        {
          "name": "core",
          "type": "tree",
          "content": null
        },
        {
          "name": "doap_Kafka.rdf",
          "type": "blob",
          "size": 3.01171875,
          "content": "<?xml version=\"1.0\"?>\n<?xml-stylesheet type=\"text/xsl\"?>\n<rdf:RDF xml:lang=\"en\"\n         xmlns=\"http://usefulinc.com/ns/doap#\"\n         xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\"\n         xmlns:asfext=\"http://projects.apache.org/ns/asfext#\"\n         xmlns:foaf=\"http://xmlns.com/foaf/0.1/\">\n<!--\n    Licensed to the Apache Software Foundation (ASF) under one or more\n    contributor license agreements.  See the NOTICE file distributed with\n    this work for additional information regarding copyright ownership.\n    The ASF licenses this file to You under the Apache License, Version 2.0\n    (the \"License\"); you may not use this file except in compliance with\n    the License.  You may obtain a copy of the License at\n\n         http://www.apache.org/licenses/LICENSE-2.0\n\n    Unless required by applicable law or agreed to in writing, software\n    distributed under the License is distributed on an \"AS IS\" BASIS,\n    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n    See the License for the specific language governing permissions and\n    limitations under the License.\n-->\n  <Project rdf:about=\"https://kafka.apache.org/\">\n    <created>2014-04-12</created>\n    <license rdf:resource=\"http://usefulinc.com/doap/licenses/asl20\" />\n    <name>Apache Kafka</name>\n    <homepage rdf:resource=\"https://kafka.apache.org/\" />\n    <asfext:pmc rdf:resource=\"https://kafka.apache.org\" />\n    <shortdesc>Apache Kafka is a distributed, fault tolerant, publish-subscribe messaging.</shortdesc>\n    <description>A single Kafka broker can handle hundreds of megabytes of reads and writes per second from thousands of clients. Kafka is designed to allow a single cluster to serve as the central data backbone for a large organization. It can be elastically and transparently expanded without downtime. Data streams are partitioned and spread over a cluster of machines to allow data streams larger than the capability of any single machine and to allow clusters of co-ordinated consumers. Kafka has a modern cluster-centric design that offers strong durability and fault-tolerance guarantees. Messages are persisted on disk and replicated within the cluster to prevent data loss. Each broker can handle terabytes of messages without performance impact.</description>\n    <bug-database rdf:resource=\"https://issues.apache.org/jira/browse/KAFKA\" />\n    <mailing-list rdf:resource=\"https://kafka.apache.org/contact.html\" />\n    <download-page rdf:resource=\"https://kafka.apache.org/downloads.html\" />\n    <programming-language>Scala</programming-language>\n    <category rdf:resource=\"http://projects.apache.org/category/big-data\" />\n    <repository>\n      <SVNRepository>\n        <location rdf:resource=\"https://gitbox.apache.org/repos/asf/kafka.git\"/>\n        <browse rdf:resource=\"https://github.com/apache/kafka\"/>\n      </SVNRepository>\n    </repository>\n    <maintainer>\n      <foaf:Person>\n        <foaf:name>Jun Rao</foaf:name>\n          <foaf:mbox rdf:resource=\"mailto:junrao@apache.org\"/>\n      </foaf:Person>\n    </maintainer>\n  </Project>\n</rdf:RDF>\n"
        },
        {
          "name": "docker",
          "type": "tree",
          "content": null
        },
        {
          "name": "docs",
          "type": "tree",
          "content": null
        },
        {
          "name": "examples",
          "type": "tree",
          "content": null
        },
        {
          "name": "generator",
          "type": "tree",
          "content": null
        },
        {
          "name": "gradle.properties",
          "type": "blob",
          "size": 1.4609375,
          "content": "# Licensed to the Apache Software Foundation (ASF) under one or more\n# contributor license agreements.  See the NOTICE file distributed with\n# this work for additional information regarding copyright ownership.\n# The ASF licenses this file to You under the Apache License, Version 2.0\n# (the \"License\"); you may not use this file except in compliance with\n# the License.  You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\ngroup=org.apache.kafka\n# NOTE: When you change this version number, you should also make sure to update\n# the version numbers in\n#  - docs/js/templateData.js\n#  - tests/kafkatest/__init__.py\n#  - tests/kafkatest/version.py (variable DEV_VERSION)\n#  - kafka-merge-pr.py\n#  - streams/quickstart/pom.xml\n#  - streams/quickstart/java/src/main/resources/archetype-resources/pom.xml\n#  - streams/quickstart/java/pom.xml\nversion=3.9.0-SNAPSHOT\nscalaVersion=2.13.14\n# Adding swaggerVersion in gradle.properties to have a single version in place for swagger\n# New version of Swagger 2.2.14 requires minimum JDK 11.\nswaggerVersion=2.2.8\ntask=build\norg.gradle.jvmargs=-Xmx2g -Xss4m -XX:+UseParallelGC\norg.gradle.parallel=true\n"
        },
        {
          "name": "gradle",
          "type": "tree",
          "content": null
        },
        {
          "name": "gradlew",
          "type": "blob",
          "size": 8.962890625,
          "content": "#!/bin/sh\n\n#\n# Copyright © 2015-2021 the original authors.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n##############################################################################\n#\n#   Gradle start up script for POSIX generated by Gradle.\n#\n#   Important for running:\n#\n#   (1) You need a POSIX-compliant shell to run this script. If your /bin/sh is\n#       noncompliant, but you have some other compliant shell such as ksh or\n#       bash, then to run this script, type that shell name before the whole\n#       command line, like:\n#\n#           ksh Gradle\n#\n#       Busybox and similar reduced shells will NOT work, because this script\n#       requires all of these POSIX shell features:\n#         * functions;\n#         * expansions «$var», «${var}», «${var:-default}», «${var+SET}»,\n#           «${var#prefix}», «${var%suffix}», and «$( cmd )»;\n#         * compound commands having a testable exit status, especially «case»;\n#         * various built-in commands including «command», «set», and «ulimit».\n#\n#   Important for patching:\n#\n#   (2) This script targets any POSIX shell, so it avoids extensions provided\n#       by Bash, Ksh, etc; in particular arrays are avoided.\n#\n#       The \"traditional\" practice of packing multiple parameters into a\n#       space-separated string is a well documented source of bugs and security\n#       problems, so this is (mostly) avoided, by progressively accumulating\n#       options in \"$@\", and eventually passing that to Java.\n#\n#       Where the inherited environment variables (DEFAULT_JVM_OPTS, JAVA_OPTS,\n#       and GRADLE_OPTS) rely on word-splitting, this is performed explicitly;\n#       see the in-line comments for details.\n#\n#       There are tweaks for specific operating systems such as AIX, CygWin,\n#       Darwin, MinGW, and NonStop.\n#\n#   (3) This script is generated from the Groovy template\n#       https://github.com/gradle/gradle/blob/HEAD/subprojects/plugins/src/main/resources/org/gradle/api/internal/plugins/unixStartScript.txt\n#       within the Gradle project.\n#\n#       You can find Gradle at https://github.com/gradle/gradle/.\n#\n##############################################################################\n\n# Attempt to set APP_HOME\n\n# Resolve links: $0 may be a link\napp_path=$0\n\n# Need this for daisy-chained symlinks.\nwhile\n    APP_HOME=${app_path%\"${app_path##*/}\"}  # leaves a trailing /; empty if no leading path\n    [ -h \"$app_path\" ]\ndo\n    ls=$( ls -ld \"$app_path\" )\n    link=${ls#*' -> '}\n    case $link in             #(\n      /*)   app_path=$link ;; #(\n      *)    app_path=$APP_HOME$link ;;\n    esac\ndone\n\n# This is normally unused\n# shellcheck disable=SC2034\nAPP_BASE_NAME=${0##*/}\n# Discard cd standard output in case $CDPATH is set (https://github.com/gradle/gradle/issues/25036)\nAPP_HOME=$( cd \"${APP_HOME:-./}\" > /dev/null && pwd -P ) || exit\n\n# Use the maximum available, or set MAX_FD != -1 to use that value.\nMAX_FD=maximum\n\nwarn () {\n    echo \"$*\"\n} >&2\n\ndie () {\n    echo\n    echo \"$*\"\n    echo\n    exit 1\n} >&2\n\n# OS specific support (must be 'true' or 'false').\ncygwin=false\nmsys=false\ndarwin=false\nnonstop=false\ncase \"$( uname )\" in                #(\n  CYGWIN* )         cygwin=true  ;; #(\n  Darwin* )         darwin=true  ;; #(\n  MSYS* | MINGW* )  msys=true    ;; #(\n  NONSTOP* )        nonstop=true ;;\nesac\n\n\n# Loop in case we encounter an error.\nfor attempt in 1 2 3; do\n  if [ ! -e \"$APP_HOME/gradle/wrapper/gradle-wrapper.jar\" ]; then\n    if ! curl -s -S --retry 3 -L -o \"$APP_HOME/gradle/wrapper/gradle-wrapper.jar\" \"https://raw.githubusercontent.com/gradle/gradle/v8.8.0/gradle/wrapper/gradle-wrapper.jar\"; then\n      rm -f \"$APP_HOME/gradle/wrapper/gradle-wrapper.jar\"\n      # Pause for a bit before looping in case the server throttled us.\n      sleep 5\n      continue\n    fi\n  fi\ndone\n\nCLASSPATH=$APP_HOME/gradle/wrapper/gradle-wrapper.jar\n\n\n# Determine the Java command to use to start the JVM.\nif [ -n \"$JAVA_HOME\" ] ; then\n    if [ -x \"$JAVA_HOME/jre/sh/java\" ] ; then\n        # IBM's JDK on AIX uses strange locations for the executables\n        JAVACMD=$JAVA_HOME/jre/sh/java\n    else\n        JAVACMD=$JAVA_HOME/bin/java\n    fi\n    if [ ! -x \"$JAVACMD\" ] ; then\n        die \"ERROR: JAVA_HOME is set to an invalid directory: $JAVA_HOME\n\nPlease set the JAVA_HOME variable in your environment to match the\nlocation of your Java installation.\"\n    fi\nelse\n    JAVACMD=java\n    if ! command -v java >/dev/null 2>&1\n    then\n        die \"ERROR: JAVA_HOME is not set and no 'java' command could be found in your PATH.\n\nPlease set the JAVA_HOME variable in your environment to match the\nlocation of your Java installation.\"\n    fi\nfi\n\n# Increase the maximum file descriptors if we can.\nif ! \"$cygwin\" && ! \"$darwin\" && ! \"$nonstop\" ; then\n    case $MAX_FD in #(\n      max*)\n        # In POSIX sh, ulimit -H is undefined. That's why the result is checked to see if it worked.\n        # shellcheck disable=SC2039,SC3045\n        MAX_FD=$( ulimit -H -n ) ||\n            warn \"Could not query maximum file descriptor limit\"\n    esac\n    case $MAX_FD in  #(\n      '' | soft) :;; #(\n      *)\n        # In POSIX sh, ulimit -n is undefined. That's why the result is checked to see if it worked.\n        # shellcheck disable=SC2039,SC3045\n        ulimit -n \"$MAX_FD\" ||\n            warn \"Could not set maximum file descriptor limit to $MAX_FD\"\n    esac\nfi\n\n# Collect all arguments for the java command, stacking in reverse order:\n#   * args from the command line\n#   * the main class name\n#   * -classpath\n#   * -D...appname settings\n#   * --module-path (only if needed)\n#   * DEFAULT_JVM_OPTS, JAVA_OPTS, and GRADLE_OPTS environment variables.\n\n# For Cygwin or MSYS, switch paths to Windows format before running java\nif \"$cygwin\" || \"$msys\" ; then\n    APP_HOME=$( cygpath --path --mixed \"$APP_HOME\" )\n    CLASSPATH=$( cygpath --path --mixed \"$CLASSPATH\" )\n\n    JAVACMD=$( cygpath --unix \"$JAVACMD\" )\n\n    # Now convert the arguments - kludge to limit ourselves to /bin/sh\n    for arg do\n        if\n            case $arg in                                #(\n              -*)   false ;;                            # don't mess with options #(\n              /?*)  t=${arg#/} t=/${t%%/*}              # looks like a POSIX filepath\n                    [ -e \"$t\" ] ;;                      #(\n              *)    false ;;\n            esac\n        then\n            arg=$( cygpath --path --ignore --mixed \"$arg\" )\n        fi\n        # Roll the args list around exactly as many times as the number of\n        # args, so each arg winds up back in the position where it started, but\n        # possibly modified.\n        #\n        # NB: a `for` loop captures its iteration list before it begins, so\n        # changing the positional parameters here affects neither the number of\n        # iterations, nor the values presented in `arg`.\n        shift                   # remove old arg\n        set -- \"$@\" \"$arg\"      # push replacement arg\n    done\nfi\n\n\n# Add default JVM options here. You can also use JAVA_OPTS and GRADLE_OPTS to pass JVM options to this script.\nDEFAULT_JVM_OPTS='\"-Xmx64m\" \"-Xms64m\"'\n\n# Collect all arguments for the java command:\n#   * DEFAULT_JVM_OPTS, JAVA_OPTS, JAVA_OPTS, and optsEnvironmentVar are not allowed to contain shell fragments,\n#     and any embedded shellness will be escaped.\n#   * For example: A user cannot expect ${Hostname} to be expanded, as it is an environment variable and will be\n#     treated as '${Hostname}' itself on the command line.\n\nset -- \\\n        \"-Dorg.gradle.appname=$APP_BASE_NAME\" \\\n        -classpath \"$CLASSPATH\" \\\n        org.gradle.wrapper.GradleWrapperMain \\\n        \"$@\"\n\n# Stop when \"xargs\" is not available.\nif ! command -v xargs >/dev/null 2>&1\nthen\n    die \"xargs is not available\"\nfi\n\n# Use \"xargs\" to parse quoted args.\n#\n# With -n1 it outputs one arg per line, with the quotes and backslashes removed.\n#\n# In Bash we could simply go:\n#\n#   readarray ARGS < <( xargs -n1 <<<\"$var\" ) &&\n#   set -- \"${ARGS[@]}\" \"$@\"\n#\n# but POSIX shell has neither arrays nor command substitution, so instead we\n# post-process each arg (as a line of input to sed) to backslash-escape any\n# character that might be a shell metacharacter, then use eval to reverse\n# that process (while maintaining the separation between arguments), and wrap\n# the whole thing up as a single \"set\" statement.\n#\n# This will of course break if any of these variables contains a newline or\n# an unmatched quote.\n#\n\neval \"set -- $(\n        printf '%s\\n' \"$DEFAULT_JVM_OPTS $JAVA_OPTS $GRADLE_OPTS\" |\n        xargs -n1 |\n        sed ' s~[^-[:alnum:]+,./:=@_]~\\\\&~g; ' |\n        tr '\\n' ' '\n    )\" '\"$@\"'\n\nexec \"$JAVACMD\" \"$@\"\n"
        },
        {
          "name": "gradlew.bat",
          "type": "blob",
          "size": 2.78125,
          "content": "@rem\n@rem Copyright 2015 the original author or authors.\n@rem\n@rem Licensed under the Apache License, Version 2.0 (the \"License\");\n@rem you may not use this file except in compliance with the License.\n@rem You may obtain a copy of the License at\n@rem\n@rem      https://www.apache.org/licenses/LICENSE-2.0\n@rem\n@rem Unless required by applicable law or agreed to in writing, software\n@rem distributed under the License is distributed on an \"AS IS\" BASIS,\n@rem WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n@rem See the License for the specific language governing permissions and\n@rem limitations under the License.\n@rem\n\n@if \"%DEBUG%\"==\"\" @echo off\n@rem ##########################################################################\n@rem\n@rem  Gradle startup script for Windows\n@rem\n@rem ##########################################################################\n\n@rem Set local scope for the variables with windows NT shell\nif \"%OS%\"==\"Windows_NT\" setlocal\n\nset DIRNAME=%~dp0\nif \"%DIRNAME%\"==\"\" set DIRNAME=.\n@rem This is normally unused\nset APP_BASE_NAME=%~n0\nset APP_HOME=%DIRNAME%\n\n@rem Resolve any \".\" and \"..\" in APP_HOME to make it shorter.\nfor %%i in (\"%APP_HOME%\") do set APP_HOME=%%~fi\n\n@rem Add default JVM options here. You can also use JAVA_OPTS and GRADLE_OPTS to pass JVM options to this script.\nset DEFAULT_JVM_OPTS=-Dfile.encoding=UTF-8 \"-Xmx64m\" \"-Xms64m\"\n\n@rem Find java.exe\nif defined JAVA_HOME goto findJavaFromJavaHome\n\nset JAVA_EXE=java.exe\n%JAVA_EXE% -version >NUL 2>&1\nif %ERRORLEVEL% equ 0 goto execute\n\necho. 1>&2\necho ERROR: JAVA_HOME is not set and no 'java' command could be found in your PATH. 1>&2\necho. 1>&2\necho Please set the JAVA_HOME variable in your environment to match the 1>&2\necho location of your Java installation. 1>&2\n\ngoto fail\n\n:findJavaFromJavaHome\nset JAVA_HOME=%JAVA_HOME:\"=%\nset JAVA_EXE=%JAVA_HOME%/bin/java.exe\n\nif exist \"%JAVA_EXE%\" goto execute\n\necho. 1>&2\necho ERROR: JAVA_HOME is set to an invalid directory: %JAVA_HOME% 1>&2\necho. 1>&2\necho Please set the JAVA_HOME variable in your environment to match the 1>&2\necho location of your Java installation. 1>&2\n\ngoto fail\n\n:execute\n@rem Setup the command line\n\nset CLASSPATH=%APP_HOME%\\gradle\\wrapper\\gradle-wrapper.jar\n\n\n@rem Execute Gradle\n\"%JAVA_EXE%\" %DEFAULT_JVM_OPTS% %JAVA_OPTS% %GRADLE_OPTS% \"-Dorg.gradle.appname=%APP_BASE_NAME%\" -classpath \"%CLASSPATH%\" org.gradle.wrapper.GradleWrapperMain %*\n\n:end\n@rem End local scope for the variables with windows NT shell\nif %ERRORLEVEL% equ 0 goto mainEnd\n\n:fail\nrem Set variable GRADLE_EXIT_CONSOLE if you need the _script_ return code instead of\nrem the _cmd.exe /c_ return code!\nset EXIT_CODE=%ERRORLEVEL%\nif %EXIT_CODE% equ 0 set EXIT_CODE=1\nif not \"\"==\"%GRADLE_EXIT_CONSOLE%\" exit %EXIT_CODE%\nexit /b %EXIT_CODE%\n\n:mainEnd\nif \"%OS%\"==\"Windows_NT\" endlocal\n\n:omega\n"
        },
        {
          "name": "gradlewAll",
          "type": "blob",
          "size": 0.9423828125,
          "content": "#!/usr/bin/env sh\n\n# Licensed to the Apache Software Foundation (ASF) under one or more\n# contributor license agreements.  See the NOTICE file distributed with\n# this work for additional information regarding copyright ownership.\n# The ASF licenses this file to You under the Apache License, Version 2.0\n# (the \"License\"); you may not use this file except in compliance with\n# the License.  You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# Convenient way to invoke a gradle command with all Scala versions supported\n# by default\n./gradlew \"$@\" -PscalaVersion=2.12 && ./gradlew \"$@\" -PscalaVersion=2.13\n\n"
        },
        {
          "name": "group-coordinator",
          "type": "tree",
          "content": null
        },
        {
          "name": "jmh-benchmarks",
          "type": "tree",
          "content": null
        },
        {
          "name": "kafka-merge-pr.py",
          "type": "blob",
          "size": 19.234375,
          "content": "#!/usr/bin/env python\n\n#\n# Licensed to the Apache Software Foundation (ASF) under one or more\n# contributor license agreements.  See the NOTICE file distributed with\n# this work for additional information regarding copyright ownership.\n# The ASF licenses this file to You under the Apache License, Version 2.0\n# (the \"License\"); you may not use this file except in compliance with\n# the License.  You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n# Utility for creating well-formed pull request merges and pushing them to Apache. This script is a modified version\n# of the one created by the Spark project (https://github.com/apache/spark/blob/master/dev/merge_spark_pr.py).\n#\n# Usage: ./kafka-merge-pr.py (see config env vars below)\n#\n# This utility assumes you already have local a kafka git folder and that you\n# have added remotes corresponding to both:\n# (i) the github apache kafka mirror and\n# (ii) the apache kafka git repo.\n\nimport json\nimport os\nimport re\nimport subprocess\nimport sys\nimport urllib\n\ntry:\n    import jira.client\n    JIRA_IMPORTED = True\nexcept ImportError:\n    JIRA_IMPORTED = False\n\nPROJECT_NAME = \"kafka\"\n\nCAPITALIZED_PROJECT_NAME = \"kafka\".upper()\n\n# Location of the local git repository\nREPO_HOME = os.environ.get(\"%s_HOME\" % CAPITALIZED_PROJECT_NAME, os.getcwd())\n# Remote name which points to the GitHub site\nPR_REMOTE_NAME = os.environ.get(\"PR_REMOTE_NAME\", \"apache-github\")\n# Remote name where we want to push the changes to (GitHub by default, but Apache Git would work if GitHub is down)\nPUSH_REMOTE_NAME = os.environ.get(\"PUSH_REMOTE_NAME\", \"apache-github\")\n# ASF JIRA username\nJIRA_USERNAME = os.environ.get(\"JIRA_USERNAME\", \"\")\n# ASF JIRA password\nJIRA_PASSWORD = os.environ.get(\"JIRA_PASSWORD\", \"\")\n# OAuth key used for issuing requests against the GitHub API. If this is not defined, then requests\n# will be unauthenticated. You should only need to configure this if you find yourself regularly\n# exceeding your IP's unauthenticated request rate limit. You can create an OAuth key at\n# https://github.com/settings/tokens. This script only requires the \"public_repo\" scope.\nGITHUB_OAUTH_KEY = os.environ.get(\"GITHUB_OAUTH_KEY\")\n\nGITHUB_USER = os.environ.get(\"GITHUB_USER\", \"apache\")\nGITHUB_BASE = \"https://github.com/%s/%s/pull\" % (GITHUB_USER, PROJECT_NAME)\nGITHUB_API_BASE = \"https://api.github.com/repos/%s/%s\" % (GITHUB_USER, PROJECT_NAME)\nJIRA_BASE = \"https://issues.apache.org/jira/browse\"\nJIRA_API_BASE = \"https://issues.apache.org/jira\"\n# Prefix added to temporary branches\nTEMP_BRANCH_PREFIX = \"PR_TOOL\"\n\nDEV_BRANCH_NAME = \"trunk\"\n\nDEFAULT_FIX_VERSION = os.environ.get(\"DEFAULT_FIX_VERSION\", \"3.9.0\")\n\nORIGINAL_HEAD = \"\"\n\ndef get_json(url):\n    try:\n        request = urllib.request.Request(url)\n        if GITHUB_OAUTH_KEY:\n            request.add_header('Authorization', 'token %s' % GITHUB_OAUTH_KEY)\n        return json.load(urllib.request.urlopen(request))\n    except urllib.error.HTTPError as error:\n        if \"X-RateLimit-Remaining\" in error.headers and error.headers[\"X-RateLimit-Remaining\"] == '0':\n            print(\"Exceeded the GitHub API rate limit; see the instructions in \" +\n                  \"kafka-merge-pr.py to configure an OAuth token for making authenticated \" +\n                  \"GitHub requests.\")\n        else:\n            print(\"Unable to fetch URL, exiting: %s\" % url)\n        sys.exit(-1)\n\n\ndef fail(msg):\n    print(msg)\n    clean_up()\n    sys.exit(-1)\n\n\ndef run_cmd(cmd):\n    print(cmd)\n    if isinstance(cmd, list):\n        result = subprocess.check_output(cmd)\n    else:\n        result = subprocess.check_output(cmd.split(\" \"))\n\n    return result.decode(\"utf-8\")\n\n\ndef continue_maybe(prompt):\n    result = input(\"\\n%s (y/n): \" % prompt)\n    if result.lower() != \"y\":\n        fail(\"Okay, exiting\")\n\n\ndef clean_up():\n    if ORIGINAL_HEAD != get_current_branch():\n        print(\"Restoring head pointer to %s\" % ORIGINAL_HEAD)\n        run_cmd(\"git checkout %s\" % ORIGINAL_HEAD)\n\n    branches = run_cmd(\"git branch\").replace(\" \", \"\").split(\"\\n\")\n\n    for branch in filter(lambda x: x.startswith(TEMP_BRANCH_PREFIX), branches):\n        print(\"Deleting local branch %s\" % branch)\n        run_cmd(\"git branch -D %s\" % branch)\n\n\ndef get_current_branch():\n    return run_cmd(\"git rev-parse --abbrev-ref HEAD\").replace(\"\\n\", \"\")\n\n\n# merge the requested PR and return the merge hash\ndef merge_pr(pr_num, target_ref, title, body, pr_repo_desc):\n    pr_branch_name = \"%s_MERGE_PR_%s\" % (TEMP_BRANCH_PREFIX, pr_num)\n    target_branch_name = \"%s_MERGE_PR_%s_%s\" % (TEMP_BRANCH_PREFIX, pr_num, target_ref.upper())\n    run_cmd(\"git fetch %s pull/%s/head:%s\" % (PR_REMOTE_NAME, pr_num, pr_branch_name))\n    run_cmd(\"git fetch %s %s:%s\" % (PUSH_REMOTE_NAME, target_ref, target_branch_name))\n    run_cmd(\"git checkout %s\" % target_branch_name)\n\n    had_conflicts = False\n    try:\n        run_cmd(['git', 'merge', pr_branch_name, '--squash'])\n    except Exception as error:\n        msg = \"Error merging: %s\\nWould you like to manually fix-up this merge?\" % error\n        continue_maybe(msg)\n        msg = \"Okay, please fix any conflicts and 'git add' conflicting files... Finished?\"\n        continue_maybe(msg)\n        had_conflicts = True\n\n    commit_authors = run_cmd(\n        ['git', 'log', 'HEAD..%s' % pr_branch_name, '--pretty=format:%an <%ae>']).split(\"\\n\")\n    distinct_authors = sorted(set(commit_authors), key=commit_authors.count, reverse=True)\n    primary_author = input(\n        \"Enter primary author in the format of \\\"name <email>\\\" [%s]: \" %\n        distinct_authors[0])\n    if primary_author == \"\":\n        primary_author = distinct_authors[0]\n\n    reviewers = input(\n        \"Enter reviewers in the format of \\\"name1 <email1>, name2 <email2>\\\": \").strip()\n\n    run_cmd(['git', 'log', 'HEAD..%s' % pr_branch_name, '--pretty=format:%h [%an] %s']).split(\"\\n\")\n\n    merge_message_flags = []\n\n    merge_message_flags += [\"-m\", title]\n\n    if body is not None:\n        # Remove \"Committer Checklist\" section\n        checklist_index = body.find(\"### Committer Checklist\")\n        if checklist_index != -1:\n            body = body[:checklist_index].rstrip()\n        # Remove @ symbols from the body to avoid triggering e-mails to people every time someone creates a\n        # public fork of the project.\n        body = body.replace(\"@\", \"\")\n        merge_message_flags += [\"-m\", body]\n\n    authors = \"\\n\".join([\"Author: %s\" % a for a in distinct_authors])\n\n    merge_message_flags += [\"-m\", authors]\n\n    if reviewers != \"\":\n        merge_message_flags += [\"-m\", \"Reviewers: %s\" % reviewers]\n\n    if had_conflicts:\n        committer_name = run_cmd(\"git config --get user.name\").strip()\n        committer_email = run_cmd(\"git config --get user.email\").strip()\n        message = \"This patch had conflicts when merged, resolved by\\nCommitter: %s <%s>\" % (\n            committer_name, committer_email)\n        merge_message_flags += [\"-m\", message]\n\n    # The string \"Closes #%s\" string is required for GitHub to correctly close the PR\n    close_line = \"Closes #%s from %s\" % (pr_num, pr_repo_desc)\n    merge_message_flags += [\"-m\", close_line]\n\n    run_cmd(['git', 'commit', '--author=\"%s\"' % primary_author] + merge_message_flags)\n\n    continue_maybe(\"Merge complete (local ref %s). Push to %s?\" % (\n        target_branch_name, PUSH_REMOTE_NAME))\n\n    try:\n        run_cmd('git push %s %s:%s' % (PUSH_REMOTE_NAME, target_branch_name, target_ref))\n    except Exception as error:\n        clean_up()\n        fail(\"Exception while pushing: %s\" % error)\n\n    merge_hash = run_cmd(\"git rev-parse %s\" % target_branch_name)[:8]\n    clean_up()\n    print(\"Pull request #%s merged!\" % pr_num)\n    print(\"Merge hash: %s\" % merge_hash)\n    return merge_hash\n\n\ndef cherry_pick(pr_num, merge_hash, default_branch):\n    pick_ref = input(\"Enter a branch name [%s]: \" % default_branch)\n    if pick_ref == \"\":\n        pick_ref = default_branch\n\n    pick_branch_name = \"%s_PICK_PR_%s_%s\" % (TEMP_BRANCH_PREFIX, pr_num, pick_ref.upper())\n\n    run_cmd(\"git fetch %s %s:%s\" % (PUSH_REMOTE_NAME, pick_ref, pick_branch_name))\n    run_cmd(\"git checkout %s\" % pick_branch_name)\n\n    try:\n        run_cmd(\"git cherry-pick -sx %s\" % merge_hash)\n    except Exception as error:\n        msg = \"Error cherry-picking: %s\\nWould you like to manually fix-up this merge?\" % error\n        continue_maybe(msg)\n        msg = \"Okay, please fix any conflicts and finish the cherry-pick. Finished?\"\n        continue_maybe(msg)\n\n    continue_maybe(\"Pick complete (local ref %s). Push to %s?\" % (\n        pick_branch_name, PUSH_REMOTE_NAME))\n\n    try:\n        run_cmd('git push %s %s:%s' % (PUSH_REMOTE_NAME, pick_branch_name, pick_ref))\n    except Exception as error:\n        clean_up()\n        fail(\"Exception while pushing: %s\" % error)\n\n    pick_hash = run_cmd(\"git rev-parse %s\" % pick_branch_name)[:8]\n    clean_up()\n\n    print(\"Pull request #%s picked into %s!\" % (pr_num, pick_ref))\n    print(\"Pick hash: %s\" % pick_hash)\n    return pick_ref\n\n\ndef fix_version_from_branch(branch, versions):\n    # Note: Assumes this is a sorted (newest->oldest) list of un-released versions\n    if branch == DEV_BRANCH_NAME:\n        versions = filter(lambda x: x == DEFAULT_FIX_VERSION, versions)\n        if len(versions) > 0:\n            return versions[0]\n\n        return None\n\n    versions = filter(lambda x: x.startswith(branch), versions)\n    if len(versions) > 0:\n        return versions[-1]\n\n    return None\n\n\ndef resolve_jira_issue(merge_branches, comment, default_jira_id=\"\"):\n    asf_jira = jira.client.JIRA({'server': JIRA_API_BASE},\n                                basic_auth=(JIRA_USERNAME, JIRA_PASSWORD))\n\n    jira_id = input(\"Enter a JIRA id [%s]: \" % default_jira_id)\n    if jira_id == \"\":\n        jira_id = default_jira_id\n\n    try:\n        issue = asf_jira.issue(jira_id)\n    except Exception as error:\n        fail(\"ASF JIRA could not find %s\\n%s\" % (jira_id, error))\n\n    cur_status = issue.fields.status.name\n    cur_summary = issue.fields.summary\n    cur_assignee = issue.fields.assignee\n    if cur_assignee is None:\n        cur_assignee = \"NOT ASSIGNED!!!\"\n    else:\n        cur_assignee = cur_assignee.displayName\n\n    if cur_status in (\"Resolved\", \"Closed\"):\n        fail(\"JIRA issue %s already has status '%s'\" % (jira_id, cur_status))\n    print(\"=== JIRA %s ===\" % jira_id)\n    print(\"summary\\t\\t%s\\nassignee\\t%s\\nstatus\\t\\t%s\\nurl\\t\\t%s/%s\\n\" % (\n        cur_summary, cur_assignee, cur_status, JIRA_BASE, jira_id))\n\n    versions = asf_jira.project_versions(CAPITALIZED_PROJECT_NAME)\n    versions = sorted(versions, key=lambda x: x.name, reverse=True)\n    versions = filter(lambda x: x.raw['released'] is False, versions)\n\n    version_names = map(lambda x: x.name, versions)\n    default_fix_versions = map(lambda x: fix_version_from_branch(x, version_names), merge_branches)\n    default_fix_versions = filter(lambda x: x is not None, default_fix_versions)\n    default_fix_versions = \",\".join(default_fix_versions)\n\n    fix_versions = input(\"Enter comma-separated fix version(s) [%s]: \" % default_fix_versions)\n    if fix_versions == \"\":\n        fix_versions = default_fix_versions\n    fix_versions = fix_versions.replace(\" \", \"\").split(\",\")\n\n    def get_version_json(version_str):\n        return next(filter(lambda v: v.name == version_str, versions)).raw\n\n    jira_fix_versions = map(get_version_json, fix_versions)\n\n    resolve = next(filter(lambda a: a['name'] == \"Resolve Issue\", asf_jira.transitions(jira_id)))\n    resolution = next(filter(lambda r: r.raw['name'] == \"Fixed\", asf_jira.resolutions()))\n    asf_jira.transition_issue(\n        jira_id, resolve[\"id\"], fixVersions=jira_fix_versions,\n        comment=comment, resolution={'id': resolution.raw['id']})\n\n    print(\"Successfully resolved %s with fixVersions=%s!\" % (jira_id, fix_versions))\n\n\ndef resolve_jira_issues(title, merge_branches, comment):\n    jira_ids = re.findall(\"%s-[0-9]{4,5}\" % CAPITALIZED_PROJECT_NAME, title)\n\n    if not jira_ids:\n        resolve_jira_issue(merge_branches, comment)\n    for jira_id in jira_ids:\n        resolve_jira_issue(merge_branches, comment, jira_id)\n\n\ndef standardize_jira_ref(text):\n    \"\"\"\n    Standardize the jira reference commit message prefix to \"PROJECT_NAME-XXX; Issue\"\n\n    >>> standardize_jira_ref(\"%s-5954; Top by key\" % CAPITALIZED_PROJECT_NAME)\n    'KAFKA-5954; Top by key'\n    >>> standardize_jira_ref(\"%s-5821; ParquetRelation2 CTAS should check if delete is successful\" % PROJECT_NAME)\n    'KAFKA-5821; ParquetRelation2 CTAS should check if delete is successful'\n    >>> standardize_jira_ref(\"%s-4123 [WIP] Show new dependencies added in pull requests\" % PROJECT_NAME)\n    'KAFKA-4123; [WIP] Show new dependencies added in pull requests'\n    >>> standardize_jira_ref(\"%s  5954: Top by key\" % PROJECT_NAME)\n    'KAFKA-5954; Top by key'\n    >>> standardize_jira_ref(\"%s-979 a LRU scheduler for load balancing in TaskSchedulerImpl\" % PROJECT_NAME)\n    'KAFKA-979; a LRU scheduler for load balancing in TaskSchedulerImpl'\n    >>> standardize_jira_ref(\"%s-1094 Support MiMa for reporting binary compatibility across versions.\" % CAPITALIZED_PROJECT_NAME)\n    'KAFKA-1094; Support MiMa for reporting binary compatibility across versions.'\n    >>> standardize_jira_ref(\"[WIP] %s-1146; Vagrant support\" % CAPITALIZED_PROJECT_NAME)\n    'KAFKA-1146; [WIP] Vagrant support'\n    >>> standardize_jira_ref(\"%s-1032. If Yarn app fails before registering, app master stays aroun...\" % PROJECT_NAME)\n    'KAFKA-1032; If Yarn app fails before registering, app master stays aroun...'\n    >>> standardize_jira_ref(\"%s-6250 %s-6146 %s-5911: Types are now reserved words in DDL parser.\" % (PROJECT_NAME, PROJECT_NAME, CAPITALIZED_PROJECT_NAME))\n    'KAFKA-6250 KAFKA-6146 KAFKA-5911; Types are now reserved words in DDL parser.'\n    >>> standardize_jira_ref(\"Additional information for users building from source code\")\n    'Additional information for users building from source code'\n    \"\"\"\n    jira_refs = []\n    components = []\n\n    # Extract JIRA ref(s):\n    pattern = re.compile(r'(%s[-\\s]*[0-9]{3,6})+' % CAPITALIZED_PROJECT_NAME, re.IGNORECASE)\n    for ref in pattern.findall(text):\n        # Add brackets, replace spaces with a dash, & convert to uppercase\n        jira_refs.append(re.sub(r'\\s+', '-', ref.upper()))\n        text = text.replace(ref, '')\n\n    # Extract project name component(s):\n    # Look for alphanumeric chars, spaces, dashes, periods, and/or commas\n    pattern = re.compile(r'(\\[[\\w\\s,-\\.]+\\])', re.IGNORECASE)\n    for component in pattern.findall(text):\n        components.append(component.upper())\n        text = text.replace(component, '')\n\n    # Cleanup any remaining symbols:\n    pattern = re.compile(r'^\\W+(.*)', re.IGNORECASE)\n    if pattern.search(text) is not None:\n        text = pattern.search(text).groups()[0]\n\n    # Assemble full text (JIRA ref(s), module(s), remaining text)\n    jira_prefix = ' '.join(jira_refs).strip()\n    if jira_prefix:\n        jira_prefix = jira_prefix + \"; \"\n    clean_text = jira_prefix + ' '.join(components).strip() + \" \" + text.strip()\n\n    # Replace multiple spaces with a single space, e.g. if no jira refs and/or components were included\n    clean_text = re.sub(r'\\s+', ' ', clean_text.strip())\n\n    return clean_text\n\n\ndef main():\n    global ORIGINAL_HEAD\n    ORIGINAL_HEAD = get_current_branch()\n\n    branches = get_json(\"%s/branches\" % GITHUB_API_BASE)\n    branch_names = filter(lambda x: x[0].isdigit(), [x['name'] for x in branches])\n    # Assumes branch names can be sorted lexicographically\n    latest_branch = sorted(branch_names, reverse=True)[0]\n\n    pr_num = input(\"Which pull request would you like to merge? (e.g. 34): \")\n    pull_request = get_json(\"%s/pulls/%s\" % (GITHUB_API_BASE, pr_num))\n    pr_events = get_json(\"%s/issues/%s/events\" % (GITHUB_API_BASE, pr_num))\n\n    url = pull_request[\"url\"]\n\n    pr_title = pull_request[\"title\"]\n    commit_title = input(\"Commit title [%s]: \" % pr_title)\n    if commit_title == \"\":\n        commit_title = pr_title\n\n    # Decide whether to use the modified title or not\n    modified_title = standardize_jira_ref(commit_title)\n    if modified_title != commit_title:\n        print(\"I've re-written the title as follows to match the standard format:\")\n        print(\"Original: %s\" % commit_title)\n        print(\"Modified: %s\" % modified_title)\n        result = input(\"Would you like to use the modified title? (y/n): \")\n        if result.lower() == \"y\":\n            commit_title = modified_title\n            print(\"Using modified title:\")\n        else:\n            print(\"Using original title:\")\n        print(commit_title)\n\n    body = pull_request[\"body\"]\n    target_ref = pull_request[\"base\"][\"ref\"]\n    user_login = pull_request[\"user\"][\"login\"]\n    base_ref = pull_request[\"head\"][\"ref\"]\n    pr_repo_desc = \"%s/%s\" % (user_login, base_ref)\n\n    # Merged pull requests don't appear as merged in the GitHub API;\n    # Instead, they're closed by asfgit.\n    merge_commits = \\\n        [event for event in pr_events if event[\"actor\"][\"login\"] == \"asfgit\" and event[\"event\"] == \"closed\"]\n\n    if merge_commits:\n        merge_hash = merge_commits[0][\"commit_id\"]\n        message = get_json(\"%s/commits/%s\" % (GITHUB_API_BASE, merge_hash))[\"commit\"][\"message\"]\n\n        print(\"Pull request %s has already been merged, assuming you want to backport\" % pr_num)\n        commit_is_downloaded = run_cmd(\n            ['git', 'rev-parse', '--quiet', '--verify', \"%s^{commit}\" % merge_hash]).strip() != \"\"\n        if not commit_is_downloaded:\n            fail(\"Couldn't find any merge commit for #%s, you may need to update HEAD.\" % pr_num)\n\n        print(\"Found commit %s:\\n%s\" % (merge_hash, message))\n        cherry_pick(pr_num, merge_hash, latest_branch)\n        sys.exit(0)\n\n    if not bool(pull_request[\"mergeable\"]):\n        msg = \"Pull request %s is not mergeable in its current form.\\n\" % pr_num + \\\n            \"Continue? (experts only!)\"\n        continue_maybe(msg)\n\n    print(\"\\n=== Pull Request #%s ===\" % pr_num)\n    print(\"PR title\\t%s\\nCommit title\\t%s\\nSource\\t\\t%s\\nTarget\\t\\t%s\\nURL\\t\\t%s\" % (\n        pr_title, commit_title, pr_repo_desc, target_ref, url))\n    continue_maybe(\"Proceed with merging pull request #%s?\" % pr_num)\n\n    merged_refs = [target_ref]\n\n    merge_hash = merge_pr(pr_num, target_ref, commit_title, body, pr_repo_desc)\n\n    pick_prompt = \"Would you like to pick %s into another branch?\" % merge_hash\n    while input(\"\\n%s (y/n): \" % pick_prompt).lower() == \"y\":\n        merged_refs = merged_refs + [cherry_pick(pr_num, merge_hash, latest_branch)]\n\n    if JIRA_IMPORTED:\n        if JIRA_USERNAME and JIRA_PASSWORD:\n            continue_maybe(\"Would you like to update an associated JIRA?\")\n            jira_comment = \"Issue resolved by pull request %s\\n[%s/%s]\" % (pr_num, GITHUB_BASE, pr_num)\n            resolve_jira_issues(commit_title, merged_refs, jira_comment)\n        else:\n            print(\"JIRA_USERNAME and JIRA_PASSWORD not set\")\n            print(\"Exiting without trying to close the associated JIRA.\")\n    else:\n        print(\"Could not find jira-python library. Run 'sudo pip install jira' to install.\")\n        print(\"Exiting without trying to close the associated JIRA.\")\n\n\nif __name__ == \"__main__\":\n    import doctest\n    (FAILURE_COUNT, TEST_COUNT) = doctest.testmod()\n    if FAILURE_COUNT:\n        sys.exit(-1)\n\n    main()\n"
        },
        {
          "name": "licenses",
          "type": "tree",
          "content": null
        },
        {
          "name": "log4j-appender",
          "type": "tree",
          "content": null
        },
        {
          "name": "metadata",
          "type": "tree",
          "content": null
        },
        {
          "name": "raft",
          "type": "tree",
          "content": null
        },
        {
          "name": "release",
          "type": "tree",
          "content": null
        },
        {
          "name": "retry_zinc",
          "type": "blob",
          "size": 2.099609375,
          "content": "#!/bin/bash\n# Licensed to the Apache Software Foundation (ASF) under one or more\n# contributor license agreements.  See the NOTICE file distributed with\n# this work for additional information regarding copyright ownership.\n# The ASF licenses this file to You under the Apache License, Version 2.0\n# (the \"License\"); you may not use this file except in compliance with\n# the License.  You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# Hacky workaround for https://github.com/gradle/gradle/issues/3777\n# There is currently no configurable timeout, so we retry builds jenkins when we can't get a lock on the zinc compiler cache\n# Hopefully we can remove this in the future, but this will save us from having to manually rebuild for the time being.\n# Example:\n# [2021-10-19T17:25:07.234Z] * What went wrong:\n# [2021-10-19T17:25:07.234Z] Execution failed for task ':streams:streams-scala:compileScala'.\n# [2021-10-19T17:25:07.234Z] > Timeout waiting to lock zinc-1.3.5_2.13.6_8 compiler cache (/home/jenkins/.gradle/caches/7.0.2/zinc-1.3.5_2.13.6_8). It is currently in use by another Gradle instance.\n# [2021-10-19T17:25:07.234Z]   Owner PID: 3999\n# [2021-10-19T17:25:07.234Z]   Our PID: 3973\n# [2021-10-19T17:25:07.234Z]   Owner Operation: \n# [2021-10-19T17:25:07.234Z]   Our operation: \n# [2021-10-19T17:25:07.234Z]   Lock file: /home/jenkins/.gradle/caches/7.0.2/zinc-1.3.5_2.13.6_8/zinc-1.3.5_2.13.6_8.lock\n\nset -uf -o pipefail\nmkdir -p logs\nretryable=1\nwhile [[ \"$retryable\" != 0 ]]; do\n\tretryable=0\n\n\t\"$@\" 2>&1 | tee logs/buildoutput.log\n\tcommandReturnCode=$?\n\n\tif [ $commandReturnCode -ne 0 ]; then\n\t\tif grep \"Timeout waiting to lock zinc\" logs/buildoutput.log; then\n\t\t\tretryable=1\n\t\t\techo 'Retrying due to zinc lock timeout'\n\t\t\tcontinue\n\t\telse\n\t\t\texit $commandReturnCode\n\t\tfi\n\tfi\ndone\n"
        },
        {
          "name": "reviewers.py",
          "type": "blob",
          "size": 3.2666015625,
          "content": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n#\n# Licensed to the Apache Software Foundation (ASF) under one or more\n# contributor license agreements.  See the NOTICE file distributed with\n# this work for additional information regarding copyright ownership.\n# The ASF licenses this file to You under the Apache License, Version 2.0\n# (the \"License\"); you may not use this file except in compliance with\n# the License.  You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n\nfrom collections import defaultdict\nimport os\nimport re\n\n\ndef prompt_for_user():\n    while True:\n        try:\n            user_input = input(\"\\nName or email (case insensitive): \")\n        except (KeyboardInterrupt, EOFError):\n            return None\n        clean_input = user_input.strip().lower()\n        if clean_input != \"\":\n            return clean_input\n\n\nif __name__ == \"__main__\":\n    print(\"Utility to help generate 'Reviewers' string for Pull Requests. Use Ctrl+D or Ctrl+C to exit\")\n\n    command = r\"git log | grep 'Reviewers\\|Author'\"\n    stream = os.popen(command)\n    lines = stream.readlines()\n    all_reviewers = defaultdict(int)\n    for line in lines:\n        stripped = line.strip().lstrip(\"Reviewers: \").lstrip(\"Author: \")\n        reviewers = stripped.split(\",\")\n        for reviewer in reviewers:\n            all_reviewers[reviewer.strip()] += 1\n    parsed_reviewers = []\n\n    for item in all_reviewers.items():\n        patterns = r\"(?P<name>.*)\\s<(?P<email>.*)>\"\n        m = re.match(patterns, item[0])\n        if m is not None and len(m.groups()) == 2:\n            if item[1] > 2:\n                parsed_reviewers.append((m.group(\"name\"), m.group(\"email\"), item[1]))\n\n    selected_reviewers = []\n    while True:\n        if selected_reviewers:\n            print(f\"Reviewers so far: {selected_reviewers}\")\n        user_input = prompt_for_user()\n        if user_input is None:\n            break\n        candidates = []\n        for reviewer, email, count in parsed_reviewers:\n            if reviewer.lower().startswith(user_input) or email.lower().startswith(user_input):\n                candidates.append((reviewer, email, count))\n            if len(candidates) == 10:\n                break\n        if not candidates:\n            continue\n\n        print(\"\\nPossible matches (in order of most recent):\")\n        for i, candidate in zip(range(10), candidates):\n            print(f\"[{i+1}] {candidate[0]} {candidate[1]} ({candidate[2]})\")\n\n        try:\n            selection_input = input(\"\\nMake a selection: \")\n            selected_candidate = candidates[int(selection_input)-1]\n            selected_reviewers.append(selected_candidate)\n        except (EOFError, KeyboardInterrupt):\n            break\n        except (ValueError, IndexError):\n            print(\"Invalid selection\")\n            continue\n\n    if selected_reviewers:\n        out = \"\\n\\nReviewers: \"\n        out += \", \".join([f\"{name} <{email}>\" for name, email, _ in selected_reviewers])\n        out += \"\\n\"\n        print(out)\n\n\n"
        },
        {
          "name": "s3stream",
          "type": "tree",
          "content": null
        },
        {
          "name": "server-common",
          "type": "tree",
          "content": null
        },
        {
          "name": "server",
          "type": "tree",
          "content": null
        },
        {
          "name": "settings.gradle",
          "type": "blob",
          "size": 3.3701171875,
          "content": "// Licensed to the Apache Software Foundation (ASF) under one or more\n// contributor license agreements.  See the NOTICE file distributed with\n// this work for additional information regarding copyright ownership.\n// The ASF licenses this file to You under the Apache License, Version 2.0\n// (the \"License\"); you may not use this file except in compliance with\n// the License.  You may obtain a copy of the License at\n//\n//    http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\nplugins {\n    id 'com.gradle.enterprise' version '3.14.1'\n    id 'com.gradle.common-custom-user-data-gradle-plugin' version '1.11.1'\n}\n\ndef isGithubActions = System.getenv('GITHUB_ACTIONS') != null\ndef isJenkins = System.getenv('JENKINS_URL') != null\ndef isCI = isGithubActions || isJenkins\n\ngradleEnterprise {\n    server = \"https://ge.apache.org\"\n    buildScan {\n        capture { taskInputFiles = true }\n        uploadInBackground = !isCI\n        publishAlways()\n        publishIfAuthenticated()\n        obfuscation {\n            // This obfuscates the IP addresses of the build machine in the build scan.\n            // Alternatively, the build scan will provide the hostname for troubleshooting host-specific issues.\n            ipAddresses { addresses -> addresses.collect { address -> \"0.0.0.0\"} }\n        }\n    }\n}\n\nbuildCache {\n    local {\n        enabled = !isCI\n    }\n\n    remote(gradleEnterprise.buildCache) {\n        enabled = false\n    }\n}\n\ninclude 'clients',\n    'connect:api',\n    'connect:basic-auth-extension',\n    'connect:file',\n    'connect:json',\n    'connect:mirror',\n    'connect:mirror-client',\n    'connect:runtime',\n    'connect:test-plugins',\n    'connect:transforms',\n    'core',\n    'examples',\n    'generator',\n    'group-coordinator',\n    'group-coordinator:group-coordinator-api',\n    'jmh-benchmarks',\n    'log4j-appender',\n    'metadata',\n    'raft',\n    'server',\n    'server-common',\n    'shell',\n    'storage',\n    'storage:api',\n    'streams',\n    'streams:examples',\n    'streams:streams-scala',\n    'streams:test-utils',\n    'streams:upgrade-system-tests-0100',\n    'streams:upgrade-system-tests-0101',\n    'streams:upgrade-system-tests-0102',\n    'streams:upgrade-system-tests-0110',\n    'streams:upgrade-system-tests-10',\n    'streams:upgrade-system-tests-11',\n    'streams:upgrade-system-tests-20',\n    'streams:upgrade-system-tests-21',\n    'streams:upgrade-system-tests-22',\n    'streams:upgrade-system-tests-23',\n    'streams:upgrade-system-tests-24',\n    'streams:upgrade-system-tests-25',\n    'streams:upgrade-system-tests-26',\n    'streams:upgrade-system-tests-27',\n    'streams:upgrade-system-tests-28',\n    'streams:upgrade-system-tests-30',\n    'streams:upgrade-system-tests-31',\n    'streams:upgrade-system-tests-32',\n    'streams:upgrade-system-tests-33',\n    'streams:upgrade-system-tests-34',\n    'streams:upgrade-system-tests-35',\n    'streams:upgrade-system-tests-36',\n    'streams:upgrade-system-tests-37',\n    'tools',\n    'tools:tools-api',\n    'transaction-coordinator',\n    'trogdor',\n    's3stream',\n    'automq-shell'\n\nproject(\":storage:api\").name = \"storage-api\"\nrootProject.name = 'kafka'\n"
        },
        {
          "name": "shell",
          "type": "tree",
          "content": null
        },
        {
          "name": "storage",
          "type": "tree",
          "content": null
        },
        {
          "name": "streams",
          "type": "tree",
          "content": null
        },
        {
          "name": "tests",
          "type": "tree",
          "content": null
        },
        {
          "name": "tools",
          "type": "tree",
          "content": null
        },
        {
          "name": "transaction-coordinator",
          "type": "tree",
          "content": null
        },
        {
          "name": "trogdor",
          "type": "tree",
          "content": null
        },
        {
          "name": "vagrant",
          "type": "tree",
          "content": null
        },
        {
          "name": "wrapper.gradle",
          "type": "blob",
          "size": 3.7138671875,
          "content": "/*\n * Licensed to the Apache Software Foundation (ASF) under one\n * or more contributor license agreements.  See the NOTICE file\n * distributed with this work for additional information\n * regarding copyright ownership.  The ASF licenses this file\n * to you under the Apache License, Version 2.0 (the\n * \"License\"); you may not use this file except in compliance\n * with the License.  You may obtain a copy of the License at\n *\n *   http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing,\n * software distributed under the License is distributed on an\n * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n * KIND, either express or implied.  See the License for the\n * specific language governing permissions and limitations\n * under the License.\n */\n\n// This file contains tasks for the gradle wrapper generation.\n\n// Ensure the wrapper script is generated based on the version defined in the project\n// and not the version installed on the machine running the task.\n// Read more about the wrapper here: https://docs.gradle.org/current/userguide/gradle_wrapper.html\nwrapper {\n    gradleVersion = project.gradleVersion\n    distributionType = Wrapper.DistributionType.ALL\n}\n\n// Custom task to inject support for downloading the gradle wrapper jar if it doesn't exist.\n// This allows us to avoid checking in the jar to our repository.\n// Additionally adds a license header to the wrapper while editing the file contents.\ntask bootstrapWrapper() {\n    // In the doLast block so this runs when the task is called and not during project configuration.\n    doLast {\n        def wrapperBasePath = \"\\$APP_HOME/gradle/wrapper\"\n        def wrapperJarPath = wrapperBasePath + \"/gradle-wrapper.jar\"\n\n        // Add a trailing zero to the version if needed.\n        def fullVersion = project.gradleVersion.count(\".\") == 1 ? \"${project.gradleVersion}.0\" : versions.gradle\n        // Leverages the wrapper jar checked into the gradle project on github because the jar isn't\n        // available elsewhere. Using raw.githubusercontent.com instead of github.com because\n        // github.com servers deprecated TLSv1/TLSv1.1 support some time ago, so older versions\n        // of curl (built against OpenSSL library that doesn't support TLSv1.2) would fail to\n        // fetch the jar.\n        def wrapperBaseUrl = \"https://raw.githubusercontent.com/gradle/gradle/v$fullVersion/gradle/wrapper\"\n        def wrapperJarUrl = wrapperBaseUrl + \"/gradle-wrapper.jar\"\n\n        def bootstrapString = \"\"\"\n      # Loop in case we encounter an error.\n      for attempt in 1 2 3; do\n        if [ ! -e \"$wrapperJarPath\" ]; then\n          if ! curl -s -S --retry 3 -L -o \"$wrapperJarPath\" \"$wrapperJarUrl\"; then\n            rm -f \"$wrapperJarPath\"\n            # Pause for a bit before looping in case the server throttled us.\n            sleep 5\n            continue\n          fi\n        fi\n      done\n      \"\"\".stripIndent()\n\n        def wrapperScript = wrapper.scriptFile\n        def wrapperLines = wrapperScript.readLines()\n        wrapperScript.withPrintWriter { out ->\n            def bootstrapWritten = false\n            wrapperLines.each { line ->\n                // Print the wrapper bootstrap before the first usage of the wrapper jar.\n                if (!bootstrapWritten && line.contains(\"gradle-wrapper.jar\")) {\n                    out.println(bootstrapString)\n                    bootstrapWritten = true\n                }\n                out.print(line)\n                out.println()\n            }\n        }\n    }\n}\nwrapper.finalizedBy bootstrapWrapper\n\n// Remove the generated batch file since we don't test building in the Windows environment.\ntask removeWindowsScript(type: Delete) {\n    delete \"$rootDir/gradlew.bat\"\n}\nwrapper.finalizedBy removeWindowsScript\n"
        }
      ]
    }
  ]
}