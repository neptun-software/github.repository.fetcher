{
  "metadata": {
    "timestamp": 1736609013661,
    "page": 238,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjI0MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "databricks/learning-spark",
      "stars": 3892,
      "defaultBranch": "master",
      "files": [
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.236328125,
          "content": "*.class\n*.log\n\n# sbt specific\ndist/*\ntarget/\nlib_managed/\nsrc_managed/\nproject/boot/\nproject/plugins/project/\nsbt/*.jar\nmini-complete-example/sbt/*.jar\n\n# Scala-IDE specific\n.scala_dependencies\n\n#Emacs\n*~\n\n#ignore the metastore\nmetastore_db/*"
        },
        {
          "name": ".travis.yml",
          "type": "blob",
          "size": 0.3095703125,
          "content": "language: scala\nscala:\n   - 2.10.4\n# Install R\nbefore_install:\n  - curl -OL http://raw.github.com/craigcitro/r-travis/master/scripts/travis-tool.sh\n  - chmod 755 ./travis-tool.sh\n  - ./travis-tool.sh bootstrap\ninstall:\n  - ./travis-tool.sh install_deps\nbefore_script:\n   - ./setup-project\nscript:\n   - ./build-project"
        },
        {
          "name": "DESCRIPTION",
          "type": "blob",
          "size": 0.259765625,
          "content": "Package: learning-spark-examples\nVersion: 0.1\nDepends: Imap\nLicense: MIT License\nDescription: Examples for the learning spark book.\nTitle: Examples for the learning spark book.\nAuthor@R: c(person(\"Holden Karau\", role = c(\"aut\", \"cre\"), email=\"holden@pigscanfly.ca\"))"
        },
        {
          "name": "LICENSE.md",
          "type": "blob",
          "size": 1.154296875,
          "content": "Copyright (C) 2014 Holden Karau and respective authors. The learning spark examples are licensed under the [MIT license](http://opensource.org/licenses/MIT). \n\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE."
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 1.583984375,
          "content": "[![buildstatus](https://travis-ci.org/holdenk/learning-spark-examples.svg?branch=master)](https://travis-ci.org/holdenk/learning-spark-examples)\nExamples for Learning Spark\n===============\nExamples for the Learning Spark book. These examples require a number of libraries and as such have long build files. We have also added a stand alone example with minimal dependencies and a small build file\nin the mini-complete-example directory.\n\n\nThese examples have been updated to run against Spark 1.3 so they may\nbe slightly different than the versions in your copy of \"Learning Spark\".\n\nRequirements\n==\n* JDK 1.7 or higher\n* Scala 2.10.3\n- scala-lang.org\n* Spark 1.3\n* Protobuf compiler\n- On debian you can install with sudo apt-get install protobuf-compiler\n* R & the CRAN package Imap are required for the ChapterSixExample\n* The Python examples require urllib3\n\nPython examples\n===\n\nFrom spark just run ./bin/pyspark ./src/python/[example]\n\nSpark Submit\n===\n\nYou can also create an assembly jar with all of the dependencies for running either the java or scala\nversions of the code and run the job with the spark-submit script\n\n./sbt/sbt assembly OR mvn package\ncd $SPARK_HOME; ./bin/spark-submit   --class com.oreilly.learningsparkexamples.[lang].[example] ../learning-spark-examples/target/scala-2.10/learning-spark-examples-assembly-0.0.1.jar\n\n[![Learning Spark](http://akamaicovers.oreilly.com/images/0636920028512/cat.gif)](http://www.jdoqocy.com/click-7645222-11260198?url=http%3A%2F%2Fshop.oreilly.com%2Fproduct%2F0636920028512.do%3Fcmp%3Daf-strata-books-videos-product_cj_9781449358600_%2525zp&cjsku=0636920028512)"
        },
        {
          "name": "bin",
          "type": "tree",
          "content": null
        },
        {
          "name": "build-project",
          "type": "blob",
          "size": 0.3974609375,
          "content": "#!/usr/bin/env sh\nset -e\nset -x\n# Do our mini example first\ncd mini-complete-example\n./sbt/sbt clean compile package\n./sbt/sbt clean\necho $PWD && mvn clean && mvn compile\ncd ..\n# Run the tests\nexport SPARK_HOME=./spark-1.3.1-bin-hadoop1/\n./sbt/sbt compile package assembly\necho $?\ntime ./run-all-examples\necho $?\necho \"done\"\n# Try and build with maven, skip for now\n#mvn clean && mvn compile && mvn package\n"
        },
        {
          "name": "build.sbt",
          "type": "blob",
          "size": 2.94921875,
          "content": "import AssemblyKeys._\n\nassemblySettings\n\nname := \"learning-spark-examples\"\n\nversion := \"0.0.1\"\n\nscalaVersion := \"2.10.4\"\n\njavacOptions ++= Seq(\"-source\", \"1.7\", \"-target\", \"1.7\")\n\n// protocol buffer support\nseq(sbtprotobuf.ProtobufPlugin.protobufSettings: _*)\n\n// additional libraries\nlibraryDependencies ++= Seq(\n  \"org.apache.spark\" %% \"spark-core\" % \"1.3.1\" % \"provided\",\n  \"org.apache.spark\" %% \"spark-sql\" % \"1.3.1\",\n  \"org.apache.spark\" %% \"spark-hive\" % \"1.3.1\",\n  \"org.apache.spark\" %% \"spark-streaming\" % \"1.3.1\",\n  \"org.apache.spark\" %% \"spark-streaming-kafka\" % \"1.3.1\",\n  \"org.apache.spark\" %% \"spark-streaming-flume\" % \"1.3.1\",\n  \"org.apache.spark\" %% \"spark-mllib\" % \"1.3.1\",\n  \"org.apache.commons\" % \"commons-lang3\" % \"3.0\",\n  \"org.eclipse.jetty\"  % \"jetty-client\" % \"8.1.14.v20131031\",\n  \"com.typesafe.play\" % \"play-json_2.10\" % \"2.2.1\",\n  \"com.fasterxml.jackson.core\" % \"jackson-databind\" % \"2.3.3\",\n  \"com.fasterxml.jackson.module\" % \"jackson-module-scala_2.10\" % \"2.3.3\",\n  \"org.elasticsearch\" % \"elasticsearch-hadoop-mr\" % \"2.0.0.RC1\",\n  \"net.sf.opencsv\" % \"opencsv\" % \"2.0\",\n  \"com.twitter.elephantbird\" % \"elephant-bird\" % \"4.5\",\n  \"com.twitter.elephantbird\" % \"elephant-bird-core\" % \"4.5\",\n  \"com.hadoop.gplcompression\" % \"hadoop-lzo\" % \"0.4.17\",\n  \"mysql\" % \"mysql-connector-java\" % \"5.1.31\",\n  \"com.datastax.spark\" %% \"spark-cassandra-connector\" % \"1.0.0-rc5\",\n  \"com.datastax.spark\" %% \"spark-cassandra-connector-java\" % \"1.0.0-rc5\",\n  \"com.github.scopt\" %% \"scopt\" % \"3.2.0\",\n  \"org.scalatest\" %% \"scalatest\" % \"2.2.1\" % \"test\",\n  \"com.holdenkarau\" %% \"spark-testing-base\" % \"0.0.1\" % \"test\"\n)\n\nresolvers ++= Seq(\n  \"JBoss Repository\" at \"http://repository.jboss.org/nexus/content/repositories/releases/\",\n  \"Spray Repository\" at \"http://repo.spray.cc/\",\n  \"Cloudera Repository\" at \"https://repository.cloudera.com/artifactory/cloudera-repos/\",\n  \"Akka Repository\" at \"http://repo.akka.io/releases/\",\n  \"Twitter4J Repository\" at \"http://twitter4j.org/maven2/\",\n  \"Apache HBase\" at \"https://repository.apache.org/content/repositories/releases\",\n  \"Twitter Maven Repo\" at \"http://maven.twttr.com/\",\n  \"scala-tools\" at \"https://oss.sonatype.org/content/groups/scala-tools\",\n  \"Typesafe repository\" at \"http://repo.typesafe.com/typesafe/releases/\",\n  \"Second Typesafe repo\" at \"http://repo.typesafe.com/typesafe/maven-releases/\",\n  \"Mesosphere Public Repository\" at \"http://downloads.mesosphere.io/maven\",\n  Resolver.sonatypeRepo(\"public\")\n)\n\nmergeStrategy in assembly <<= (mergeStrategy in assembly) { (old) =>\n  {\n    case m if m.toLowerCase.endsWith(\"manifest.mf\") => MergeStrategy.discard\n    case m if m.startsWith(\"META-INF\") => MergeStrategy.discard\n    case PathList(\"javax\", \"servlet\", xs @ _*) => MergeStrategy.first\n    case PathList(\"org\", \"apache\", xs @ _*) => MergeStrategy.first\n    case PathList(\"org\", \"jboss\", xs @ _*) => MergeStrategy.first\n    case \"about.html\"  => MergeStrategy.rename\n    case \"reference.conf\" => MergeStrategy.concat\n    case _ => MergeStrategy.first\n  }\n}\n"
        },
        {
          "name": "files",
          "type": "tree",
          "content": null
        },
        {
          "name": "mini-complete-example",
          "type": "tree",
          "content": null
        },
        {
          "name": "pom.xml",
          "type": "blob",
          "size": 4.3896484375,
          "content": "<project>\n  <groupId>com.oreilly.learningsparkexamples</groupId>\n  <artifactId>java</artifactId>\n  <modelVersion>4.0.0</modelVersion>\n  <name>examples</name>\n  <packaging>jar</packaging>\n  <version>0.0.2</version>\n  <repositories>\n    <repository>\n      <id>Akka repository</id>\n      <url>http://repo.akka.io/releases</url>\n    </repository>\n    <repository>\n      <id>scala-tools</id>\n      <url>https://oss.sonatype.org/content/groups/scala-tools</url>\n    </repository>\n    <repository>\n      <id>apache</id>\n      <url>https://repository.apache.org/content/repositories/releases</url>\n    </repository>\n    <repository>\n      <id>twitter</id>\n      <url>http://maven.twttr.com/</url>\n    </repository>\n    <repository>\n      <id>central2</id>\n      <url>http://central.maven.org/maven2/</url>\n    </repository>\n  </repositories>\n  <dependencies>\n    <dependency> <!-- Spark dependency -->\n      <groupId>org.apache.spark</groupId>\n      <artifactId>spark-core_2.10</artifactId>\n      <version>1.3.1</version>\n      <scope>provided</scope>\n    </dependency>\n    <dependency> <!-- Spark dependency -->\n      <groupId>org.apache.spark</groupId>\n      <artifactId>spark-sql_2.10</artifactId>\n      <version>1.3.1</version>\n      <scope>provided</scope>\n    </dependency>\n    <dependency> <!-- Spark dependency -->\n      <groupId>org.apache.spark</groupId>\n      <artifactId>spark-hive_2.10</artifactId>\n      <version>1.3.1</version>\n      <scope>provided</scope>\n    </dependency>\n    <dependency> <!-- Spark dependency -->\n      <groupId>org.apache.spark</groupId>\n      <artifactId>spark-streaming_2.10</artifactId>\n      <version>1.3.1</version>\n    </dependency>\n    <dependency> <!-- Spark dependency -->\n      <groupId>org.apache.spark</groupId>\n      <artifactId>spark-streaming-kafka_2.10</artifactId>\n      <version>1.3.1</version>\n    </dependency>\n    <dependency> <!-- Spark dependency -->\n      <groupId>org.apache.spark</groupId>\n      <artifactId>spark-mllib_2.10</artifactId>\n      <version>1.3.1</version>\n    </dependency>\n    <dependency> <!-- Cassandra -->\n      <groupId>com.datastax.spark</groupId>\n      <artifactId>spark-cassandra-connector_2.10</artifactId>\n      <version>1.0.0-rc5</version>\n    </dependency>\n    <dependency> <!-- Cassandra -->\n      <groupId>com.datastax.spark</groupId>\n      <artifactId>spark-cassandra-connector-java_2.10</artifactId>\n      <version>1.0.0-rc5</version>\n    </dependency>\n    <dependency> <!-- Elastic search connector -->\n      <groupId>org.elasticsearch</groupId>\n      <artifactId>elasticsearch-hadoop-mr</artifactId>\n      <version>2.0.0.RC1</version>\n    </dependency>\n    <dependency> <!-- Jetty demmo -->\n      <groupId>org.eclipse.jetty</groupId>\n      <artifactId>jetty-client</artifactId>\n      <version>8.1.14.v20131031</version>\n    </dependency>\n    <dependency>\n      <groupId>com.fasterxml.jackson.core</groupId>\n      <artifactId>jackson-databind</artifactId>\n      <version>2.3.3</version>\n    </dependency>\n    <dependency>\n      <groupId>org.apache.commons</groupId>\n      <artifactId>commons-lang3</artifactId>\n      <version>3.0</version>\n    </dependency>\n    <dependency>\n      <groupId>net.sf.opencsv</groupId>\n      <artifactId>opencsv</artifactId>\n      <version>2.0</version>\n    </dependency>\n    <dependency>\n      <groupId>org.scalatest</groupId>\n      <artifactId>scalatest_${scala.binary.version}</artifactId>\n      <version>2.2.1</version>\n    </dependency>\n  </dependencies>\n  <properties>\n    <java.version>1.7</java.version>\n    <scala.binary.version>2.10</scala.binary.version>\n  </properties>\n  <build>\n    <pluginManagement>\n      <plugins>\n        <plugin>\n\t  <groupId>org.apache.maven.plugins</groupId>\n          <artifactId>maven-compiler-plugin</artifactId>\n          <version>3.1</version>\n          <configuration>\n            <source>${java.version}</source>\n            <target>${java.version}</target>\n          </configuration>\n\t</plugin>\n\t<plugin>\n          <groupId>org.apache.maven.plugins</groupId>\n          <artifactId>maven-assembly-plugin</artifactId>\n          <version>2.2.2</version>\n          <!-- The configuration of the plugin -->\n          <configuration>\n            <!-- Specifies the configuration file of the assembly plugin -->\n            <descriptors>\n              <descriptor>src/main/assembly/assembly.xml</descriptor>\n            </descriptors>\n          </configuration>\n        </plugin>\n      </plugins>\n    </pluginManagement>\n  </build>\n</project>\n"
        },
        {
          "name": "project",
          "type": "tree",
          "content": null
        },
        {
          "name": "run-all-examples",
          "type": "blob",
          "size": 4.4560546875,
          "content": "#!/usr/bin/env bash\n# This script is used to run all of the examples. Mostly to be used by travis for testing\n# Output the commands we run\nset -x\n# If any command fails, fail\nset -e\n# Build everything\n./sbt/sbt compile package assembly > sbtlog || (echo \"sbt failed\" && cat ./sbtlog && exit 1)\nKAFKA_ROOT=./kafka_2.9.2-0.8.1.1\nSPARK_SUBMIT_SCRIPT=$SPARK_HOME/bin/spark-submit\nASSEMBLY_JAR=./target/scala-2.10/learning-spark-examples-assembly-0.0.1.jar\n# Mini cleanup\nrm -rf /tmp/py; mkdir -p /tmp/py\nrm -rf /tmp/java; mkdir -p /tmp/java\nrm -rf /tmp/scala; mkdir -p /tmp/scala\n# setup cassandra\n# cqlsh --file ./files/cqlsh_setup &\n# Scala\necho \"Running Scala programs\"\n$SPARK_SUBMIT_SCRIPT --class com.oreilly.learningsparkexamples.scala.LoadJsonWithSparkSQL $ASSEMBLY_JAR local ./files/pandainfo.json \n$SPARK_SUBMIT_SCRIPT --class com.oreilly.learningsparkexamples.scala.ChapterSixExample  $ASSEMBLY_JAR local ./files/callsigns ./files/callsigns /tmp/scala/ch6out\nTWITTER_DATA=./files/testweet.json\n$SPARK_SUBMIT_SCRIPT --class com.oreilly.learningsparkexamples.scala.SparkSQLTwitter  $ASSEMBLY_JAR  \"$TWITTER_DATA\"  /tmp/scala/tweetout\n#$SPARK_SUBMIT_SCRIPT --class com.oreilly.learningsparkexamples.scala.BasicQueryCassandra $ASSEMBLY_JAR local localhost\necho \"Running Scala streaming program\"\n./bin/fakelogs.sh &\nsleep 1\n$SPARK_SUBMIT_SCRIPT --class com.oreilly.learningsparkexamples.scala.StreamingLogInput $ASSEMBLY_JAR local[4]\necho \"Running Scala Kafka streaming example\"\n$SPARK_SUBMIT_SCRIPT --master local[4] --class com.oreilly.learningsparkexamples.scala.KafkaInput $ASSEMBLY_JAR localhost:2181 spark-readers pandas 1 &\nKAFKA_PID=$!\nsleep 1\necho \"panda\\nerror panda\" | $KAFKA_ROOT/bin/kafka-console-producer.sh --broker-list localhost:9092 --topic pandas\nwait $KAFKA_PID\necho \"Running Scala Flume example\"\n$SPARK_SUBMIT_SCRIPT --master local[4] --class com.oreilly.learningsparkexamples.scala.FlumeInput $ASSEMBLY_JAR localhost 7788 &\nFLUME_PID=$!\nsleep 1\necho \"panda\\nerror panda\\n\" | nc localhost 44444\nsleep 3\necho \"panda2\\nerror panda2\\n\" | nc localhost 44444\nwait $FLUME_PID\n# Python\necho \"Running Python programs\"\n$SPARK_SUBMIT_SCRIPT ./src/python/AvgMapPartitions.py local\n$SPARK_SUBMIT_SCRIPT ./src/python/BasicAvg.py local\n$SPARK_SUBMIT_SCRIPT ./src/python/BasicFilterMap.py local\n$SPARK_SUBMIT_SCRIPT ./src/python/BasicKeyValueMapFilter.py local\n$SPARK_SUBMIT_SCRIPT ./src/python/BasicMapPartitions.py local\n$SPARK_SUBMIT_SCRIPT ./src/python/BasicMap.py local\n$SPARK_SUBMIT_SCRIPT ./src/python/ChapterSixExample.py local ./files/callsigns /tmp/py/pandaout\n$SPARK_SUBMIT_SCRIPT ./src/python/SparkSQLTwitter.py ./files/testweet.json /tmp/py/tweetout\n$SPARK_SUBMIT_SCRIPT ./src/python/LoadCsv.py local ./files/favourite_animals.csv /tmp/py/panda_lovers.csv\n$SPARK_SUBMIT_SCRIPT ./src/python/MakeHiveTable.py local ./files/int_string.csv pandaplural\n# Temporarily disabled due to API changes\n#$SPARK_SUBMIT_SCRIPT ./src/python/LoadHive.py local pandaplural\n$SPARK_SUBMIT_SCRIPT ./src/python/LoadJson.py local ./files/pandainfo.json /tmp/py/loadjsonout\n$SPARK_SUBMIT_SCRIPT ./src/python/PerKeyAvg.py local\n$SPARK_SUBMIT_SCRIPT ./src/python/RemoveOutliers.py local\n$SPARK_SUBMIT_SCRIPT ./src/python/WordCount.py local\n$SPARK_SUBMIT_SCRIPT ./src/python/MakeParquetFile.py local ./files/favourite_animals.csv /tmp/py/favouriteanimal_parquet\n$SPARK_SUBMIT_SCRIPT ./src/python/QueryParquetFile.py local /tmp/py/favouriteanimal_parquet\n\n# Java\necho \"Running Java programs\"\n$SPARK_SUBMIT_SCRIPT --class com.oreilly.learningsparkexamples.java.LoadJsonWithSparkSQL $ASSEMBLY_JAR local ./files/pandainfo.json\n$SPARK_SUBMIT_SCRIPT --class com.oreilly.learningsparkexamples.java.ChapterSixExample $ASSEMBLY_JAR local ./files/callsigns ./files/callsigns /tmp/java/ch6out\n./sbt/sbt assembly && $SPARK_SUBMIT_SCRIPT --class com.oreilly.learningsparkexamples.java.SparkSQLTwitter  $ASSEMBLY_JAR  ./files/testweet.json  /tmp/java/tweetout\n#$SPARK_SUBMIT_SCRIPT --class com.oreilly.learningsparkexamples.java.BasicQueryCassandra $ASSEMBLY_JAR local localhost\necho \"Running Java streaming program\"\n./bin/fakelogs.sh &\nsleep 1\n$SPARK_SUBMIT_SCRIPT --class com.oreilly.learningsparkexamples.java.StreamingLogInput $ASSEMBLY_JAR local[4]\nsleep 5\necho \"Running Java Kafka streaming example\"\n$SPARK_SUBMIT_SCRIPT --class com.oreilly.learningsparkexamples.java.KafkaInput $ASSEMBLY_JAR localhost:2181 spark-java-readers\necho \"panda\\nerror panda\" | $KAFKA_ROOT/bin/kafka-console-producer.sh --broker-list localhost:9092 --topic pandas\n\necho \"Done running all programs :)\"\n"
        },
        {
          "name": "sbt",
          "type": "tree",
          "content": null
        },
        {
          "name": "setup-project",
          "type": "blob",
          "size": 2.4794921875,
          "content": "#!/usr/bin/env bash\nset -x\nset -e\nset -o pipefail\nsudo apt-get install -y axel time\necho \"Downloading misc tools\"\nsudo rm -f /etc/apt/sources.list.d/cassandra.sources.list\necho \"deb http://debian.datastax.com/community stable main\" | sudo tee -a /etc/apt/sources.list.d/cassandra.sources.list\ncurl -L http://debian.datastax.com/debian/repo_key | sudo apt-key add -\nsudo apt-get update > aptlog &\nAPT_GET_UPDATE_PID=$!\naxel http://d3kbcqa49mib13.cloudfront.net/spark-1.3.1-bin-hadoop1.tgz  > sparkdl &\nSPARK_DL_PID=$!\naxel http://mirrors.ibiblio.org/apache/kafka/0.8.1.1/kafka_2.9.2-0.8.1.1.tgz > kafkadl &\nKAFKA_DL_PID=$!\naxel http://mirror.cogentco.com/pub/apache/flume/1.5.0.1/apache-flume-1.5.0.1-bin.tar.gz > flumedl &\nFLUME_DL_PID=$!\nwait $SPARK_DL_PID\nsudo mkdir -p /etc/apt/sources.list.d/\necho \"install urllib3\"\nsudo pip install urllib3\nwait $SPARK_DL_PID || echo \"Spark DL finished early\"\ntar -xf spark-1.3.1-bin-hadoop1.tgz\nwait $APT_GET_UPDATE_PID\necho \"Installing protobuf\"\nsudo apt-get install protobuf-compiler\necho $?\n# Set up cassandra\necho \"Waiting for apt-get update to finish\"\nwait $APT_GET_UPDATE_PID || echo \"apt-get update finished early\"\necho \"Setting up dsc (cassandra)\"\nsleep 1;\n#sudo apt-get -y --force-yes remove cassandra cassandra-tools\n#sudo rm -rf /etc/security/limits.d/cassandra.conf || echo \"No cassandra security conf\"\n#yes | sudo apt-get -y --force-yes install  dsc21 > dscinstall.log\n#yes | sudo apt-get -y --force-yes install  cassandra-tools > ctoolsinstall.log\necho \"Starting cassandra\"\nsudo /etc/init.d/cassandra start\necho $?\necho \"set up hive directories\"\nexport IAM=`whoami`\nsudo mkdir -p /user/hive && sudo chown -R $IAM /user/hive\necho \"done with setup\"\n# Set up kafka\necho \"Setting up kafka\"\nwait $KAFKA_DL_PID || echo \"Kafka DL finished early\"\ntar -xzf kafka_2.9.2-0.8.1.1.tgz\ncd kafka_2.9.2-0.8.1.1\necho \"Starting zookeeper\"\n./bin/zookeeper-server-start.sh config/zookeeper.properties &\necho \"Starting kafka\"\nsleep 5\n./bin/kafka-server-start.sh config/server.properties &\nsleep 5\n# publish a pandas topic to kafka\n./bin/kafka-topics.sh --zookeeper localhost:2181 --topic pandas --partition 1 --replication-factor 1 --create\n./bin/kafka-topics.sh --zookeeper localhost:2181 --topic logs --partition 1 --replication-factor 1 --create\ncd ..\n\n# set up flume\nwait $FLUME_DL_PID || echo \"Flume DL finished early\"\necho \"Setting up flume\"\ntar -xf apache-flume-1.5.0.1-bin.tar.gz\ncd apache-flume-1.5.0.1-bin\n./bin/flume-ng agent -n panda --conf-file ../files/flumeconf.cfg &\ndisown $!\ncd ..\necho $?\n"
        },
        {
          "name": "src",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}