{
  "metadata": {
    "timestamp": 1736608878077,
    "page": 52,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjYw",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "claritylab/lucida",
      "stars": 4807,
      "defaultBranch": "master",
      "files": [
        {
          "name": ".dockerignore",
          "type": "blob",
          "size": 0.0205078125,
          "content": ".git\nkuberlucida.tar\n"
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.5830078125,
          "content": "# Compiled Object files\n*.slo\n*.lo\n*.o\n*.obj\n*.class\n\n# Precompiled Headers\n*.gch\n*.pch\n\n# Compiled Dynamic libraries\n*.so\n!libindri_jni.so\n*.dylib\n*.dll\n\n# Fortran module files\n*.mod\n\n# Compiled Static libraries\n*.lai\n*.la\n*.a\n*.lib\n\n# Executables\n*.exe\n*.out\n*.app\n*.log\n\n# Compiled python\n*.pyc\n\n# Swap files\n*.swp\n\n# Generated by git merge\n*.orig\n\n# Datasets/databases/other\n*.pb.*\n*.pb\n*.pickle\nsmall_db\ngen-*\n*.tar.gz\n*.tar.bz2\n\n# Others\ntools/caffe/\ntools/opencv-2.4.9/\ntools/protobuf-2.5.0/\ntools/thrift-0.9.3/\ntools/fbthrift/\ntools/mongo-c-driver-1.3.0/\ntools/mongo-cxx-driver/\n.DS_Store\n"
        },
        {
          "name": "CONTRIBUTING.md",
          "type": "blob",
          "size": 0.54296875,
          "content": "# Contributing\n\nWe would love to have your help in making Lucida better.\n\n## Pull Requests\n\nWe actively welcome your pull requests.\n\n1. Fork the repo and create your branch from `master`.\n2. If you have changed the workflow, update the documentation.\n3. Make sure your code lints.\n\n## Issues\n\nWe use GitHub issues to track public bugs. Please ensure your description is\nclear and has sufficient instructions to be able to reproduce the issue.\n\n## License\n\nBy contributing to Lucida, you agree that your contribution will be licensed\nunder its BSD license.\n"
        },
        {
          "name": "Dockerfile",
          "type": "blob",
          "size": 1.34765625,
          "content": "####\n# Builds the lucida base image\nFROM ubuntu:14.04\n\n#### environment variables\nENV LUCIDAROOT /usr/local/lucida/lucida\nENV THRIFT_ROOT /usr/local/lucida/tools/thrift-$THRIFT_VERSION\nENV LD_LIBRARY_PATH /usr/local/lib\nENV CAFFE /usr/local/lucida/tools/caffe/distribute\nENV CPU_ONLY 1 # for caffe\n\nENV OPENCV_VERSION 2.4.9\nENV THRIFT_VERSION 0.9.3\nENV THREADS 4\nENV PROTOBUF_VERSION 2.5.0\nENV JAVA_VERSION 8\nENV JAVA_TOOL_OPTIONS=-Dfile.encoding=UTF8 \n\n## common package installations\nRUN sed 's/main$/main universe/' -i /etc/apt/sources.list\nRUN apt-get update\nRUN apt-get install -y make\n\n## install lucida\nRUN mkdir -p /usr/local/lucida\nADD . /usr/local/lucida\nWORKDIR \"/usr/local/lucida/tools\"\nRUN /bin/bash apt_deps.sh\nRUN /bin/bash install_python.sh\nRUN /bin/bash install_java.sh\nRUN /bin/bash install_opencv.sh\nRUN /bin/bash install_thrift.sh\nRUN /bin/bash install_fbthrift.sh\nRUN /bin/bash install_mongodb.sh\nWORKDIR \"/usr/local/lucida/lucida\"\nRUN /usr/bin/make\nRUN /bin/bash commandcenter/apache/install_apache.sh\nRUN mkdir -p /etc/letsencrypt/live/host\n\n### function docker-flush(){\n###     dockerlist=$(docker ps -a -q)\n###     if [ \"${dockerlist}\" != \"\" ]; then\n###         for d in ${dockerlist}; do\n###             echo \"***** ${d}\"\n###             docker stop ${d} 2>&1 > /dev/null\n###             docker rm ${d} 2>&1 > /dev/null\n###         done\n###     fi\n### }\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 1.505859375,
          "content": "BSD License\n\nFor Sirius software\n\nCopyright (c) 2015, University of Michigan. All rights reserved.\n\nRedistribution and use in source and binary forms, with or without modification,\nare permitted provided that the following conditions are met:\n\n * Redistributions of source code must retain the above copyright notice, this\n   list of conditions and the following disclaimer.\n\n * Redistributions in binary form must reproduce the above copyright notice,\n   this list of conditions and the following disclaimer in the documentation\n   and/or other materials provided with the distribution.\n\n * Neither the name University of Michigan nor the names of its contributors may\n   be used to endorse or promote products derived from this software without\n   specific prior written permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND\nANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED\nWARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\nDISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR\nANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES\n(INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;\nLOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON\nANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n(INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS\nSOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n"
        },
        {
          "name": "Makefile",
          "type": "blob",
          "size": 0.6416015625,
          "content": "DOCKER_CONTAINER=lucida\nVERSION=latest\n\nifeq ($(THREADS),)\n  THREADS=$(shell grep -c ^processor /proc/cpuinfo)\nendif\n\ninclude ./Makefile.common\n\n.PHONY: docker local\n\n## build docker environment\ndocker:\n\tdocker build -t $(DOCKER_CONTAINER):$(VERSION) .\n\n## build local environment\nexport LD_LIBRARY_PATH=/usr/local/lib\n\nlocal:\n\tcd tools && sudo make && cd ../lucida && make\n\nstart_all:\n\tcd tools && ./start_all_tmux.sh\n\nstart_all_secure:\n\tcd tools && ./start_all_tmux.sh secure\n\nstart_test_all:\n\tcd tools && ./start_all_tmux.sh test\n\nall_service:\n\tcd lucida && make all\n\nclean_all_service:\n\tcd lucida && make clean\n\nclean_all_tools:\n\tcd tools && make clean\n"
        },
        {
          "name": "Makefile.common",
          "type": "blob",
          "size": 0.48828125,
          "content": "UNDER_TEST=1\n\n.PHONY: default all debug test clean\n\ndefault: all\n\nall: all-sub all-here\ndebug: debug-sub debug-here\ntest: test-sub test-here\nclean: clean-sub clean-here\n\nall-sub:\n\t$(foreach var,$(SUBDIRS),$(MAKE) -C $(var) all || exit 1;)\n\ndebug-sub:\n\t$(foreach var,$(SUBDIRS),$(MAKE) -C $(var) debug || exit 1;)\n\ntest-sub:\n\t$(foreach var,$(TESTSUBDIRS),$(MAKE) -C $(var) test || exit 1;)\n\nclean-sub:\n\t$(foreach var,$(SUBDIRS),$(MAKE) -C $(var) clean || exit 1;)\n\nall-here::\ntest-here::\nclean-here::\n"
        },
        {
          "name": "PATENTS",
          "type": "blob",
          "size": 1.4609375,
          "content": "Additional Grant of Patent Rights\n\n\"Software\" means the Sirius software distributed by University of Michigan.\n\nUniversity of Michigan hereby grants you a perpetual, worldwide, royalty-free,\nnon-exclusive, irrevocable (subject to the termination provision below) license\nunder any rights in any patent claims owned by University of Michigan, to make,\nhave made, use, sell, offer to sell, import, and otherwise transfer the\nSoftware. For avoidance of doubt, no license is granted under University of\nMichiganâ€™s rights in any patent claims that are infringed by (i) modifications\nto the Software made by you or a third party, or (ii) the Software in\ncombination with any software or other technology provided by you or a third\nparty.\n\nThe license granted hereunder will terminate, automatically and without notice,\nfor anyone that makes any claim (including by filing any lawsuit, assertion or\nother action) alleging (a) direct, indirect, or contributory infringement or\ninducement to infringe any patent: (i) by University of Michigan or any of its\nsubsidiaries or affiliates, whether or not such claim is related to the\nSoftware, (ii) by any party if such claim arises in whole or in part from any\nsoftware, product or service of University of Michigan or any of its\nsubsidiaries or affiliates, whether or not such claim is related to the\nSoftware, or (iii) by any party relating to the Software; or (b) that any right\nin any patent claim of University of Michigan is invalid or unenforceable.\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 23.9482421875,
          "content": "# Lucida\n\nLucida is a speech and vision based intelligent personal assistant inspired by\n[Sirius](http://sirius.clarity-lab.org).\nVisit [our website](http://lucida.ai) for tutorial, and\n[Lucida-users](http://groups.google.com/forum/#!forum/lucida-users) for help.\nThe project is released under [BSD\nlicense](LICENSE), except certain submodules contain their own specific\nlicensing information. We would love to have your help on improving Lucida, and\nsee [CONTRIBUTING](CONTRIBUTING.md) for more details.\n\n## Overview\n\n- `lucida`: back-end services and command center (CMD). \n  Currently, there are 7 categories of back-end services:\n  \"ASR\" (automatic speech recognition), \"IMM\" (image matching), \"QA\" (question answering),\n  \"CA\" (calendar events retrieval), \"IMC\" (image classification), \"FACE\" (facial recognition),\n  and \"DIG\" (digit recognition).\n\n  You can delete or replace these services with your own, or you can simply add a new service.\n  For example, if you know some better ASR implementation,\n  have an interesting image captioning end-to-end system,\n  or have access to a quality machine translation algorithm,\n  please read the section \"How to Add Your Own Service into Lucida?\" below.\n  \n  The command center determines which services are needed based on the user input,\n  sends requests to them, and returns response to the user.\n  In the following diagram, the user asks a query that needs the following three services: ASR, IMM, and QA.\n  The \"cloud\" behind each box means the Docker container(s) running on the host machine(s).\n\n  <p align=\"center\">\n    <img src=\"high_level.png\" alt=\"\" width=\"600\" />\n  </p>\n\n- `tools`: dependencies necessary for compiling Lucida.\n  Due to the fact that services share some common dependencies,\n  all services should be compiled after these dependencies are installed.\n  The advantage of a central point of dependencies is that the total size of compiled services is minimized;\n  the disadvantage is that it makes deleting a service from Lucida non-trivial -- you have to remove its dependencies in `tools`. \n\n## Lucida Local Development\n\n  If you want to make contributions to Lucida, please build it locally:\n\n- From this directory, type: `make local`. This will run scripts in `tools/` to\n  install all the required dependencies. After that, it will compile back-end services\n  in `lucida/`.\n\n- Important note for Ubuntu 16.04 users: please read [note #1](tools/README.md).\n\n- If for some reason you need to compile part of it (e.g. one back-end service),\n  make sure to set the following environment variable as set in [`Makefile`](Makefile):\n\n  ```\n  export LD_LIBRARY_PATH=/usr/local/lib\n  ```\n  You can add it permanently to your bash profile.\n\n- Start all services:\n\n  ```\n  make start_all\n  ```\n\n  This will spawn a terminal window (`gnome-terminal`) for each service as well as the command center.\n  Once they all start running,\n  open your browser and visit `http://localhost:3000/`.\n  Check out the [`tutorial`](tutorial.pdf) for usage and sample questions.\n  \n  Currently, the command center receives the user input in the form of HTTP requests sent from your browser,\n  but in future we can support other forms of input. \n\n## Lucida Docker Deployment\n\n  If you want to use Lucida as a web application, please deploy using\n  [Docker](https://www.docker.com/) and [Kubernetes](http://kubernetes.io/):\n\n- Install Docker: refer to\n  [https://docs.docker.com/engine/installation/](https://docs.docker.com/engine/installation/).\n\n- Navigate to [`tools/deploy/`](tools/deploy) and follow the instructions there.\n\n- Once done, check out the [`tutorial`](tutorial.pdf) for usage and sample questions.\n\n## REST API for command center\n\nThe REST API is in active development and may change drastically. It currently supports only infer and learn. Other features may be added later.\nAn [example client](lucida/botframework-interface) for botframework is available. Information on how to use the API can be found in the [wiki](https://github.com/claritylab/lucida/wiki/REST-API)\n\n## Design Notes -- How to Add Your Own Service into Lucida?\n\n### Back-end Communication\n\nThrift is an RPC framework with the advantages of being efficient and language-neutral. \nIt was originally developed by Facebook and now developed by both the open-source community (Apache Thrift) and Facebook.\nWe use both Apache Thrift and Facebook Thrift because Facebook Thrift has a fully asynchronous C++ server but does not support\nJava very well. Also, Apache Thrift seems to be more popular.\nTherefore, we recommend using Apache Thrift for services written in Python and Java,\nand Facebook Thrift for services written in C++.\nHowever, you can choose either one for your own service as long as you follow the steps below.\n\nOne disadvantage about Thrift is that the interface has to be pre-defined and implemented by each service. \nIf the interface changes, all services have to re-implement the interface. \nWe try to avoid changing the interface by careful design, but if you really need to adapt the interface for your need,\nfeel free to modify, but make sure that all services implement and use the new interface.\n\n### Detailed Instructions\n\nYou need to configure the command center (CMD) besides implementing the Thrift interface\nin order to add your own service into Lucida. Let's break it down into two steps:\n\n1. Implement the Thrift interface jointly defined in `lucida/lucidaservice.thrift` and `lucida/lucidatypes.thrift`.\n\n   1. [`lucida/lucidaservice.thrift`](lucida/lucidaservice.thrift)\n\n      ```\n      include \"lucidatypes.thrift\"\n      service LucidaService {\n         void create(1:string LUCID, 2:lucidatypes.QuerySpec spec);\n         void learn(1:string LUCID, 2:lucidatypes.QuerySpec knowledge);\n         string infer(1:string LUCID, 2:lucidatypes.QuerySpec query);\n      }\n      ```\n    \n      The basic functionalities that your service needs to provide are called `create`, `learn`, and `infer`. \n      They all take in the same type of parameters, a `string` representing the Lucida user ID (`LUCID`),\n      and a `QuerySpec` defined in `lucida/lucidatypes.thrift`. \n      The command center invokes these three procedures implemented by your service,\n      and services can also invoke these procedures on each other to achieve communication.\n      Thus the typical data flow looks like this:\n\n      ```Command Center (CMD) -> Your Own Service (YOS)```\n\n      But it also can be like this:\n\n      ```Command Center (CMD) -> Your Own Service 0 (YOS0) -> Your Own Service 1 (YOS1) -> Your Own Service 2 (YOS2)```\n\n      In this scenario, make sure to implement the asynchronous Thrift interface.\n      If YOS0 implements the asynchronous Thrift interface,\n      it won't block on waiting for the response from YOS1.\n      If YOS0 implements the synchronous Thrift interface, it cannot make progress until\n      YOS1 returns the response, so the operating system will perform a thread context switch,\n      and let the current thread sleep until YOS1 returns. \n      See section 3 of step 1 for implementation details.\n\n      `create`: create an intelligent instance based on supplied LUCID.\n      It gives services a chance to warm up the pipeline,\n      but our current services do not need that. Therefore, the command center does not send `create` request at this point.\n      If your service needs to warm up for each user, make sure to modify the command center which is detailed in step 2.\n\n      `learn`: tell the intelligent instance to learn new knowledge based on data supplied in the query,\n      which usually means the training process.\n      Although it has be implemented, you can choose to do nothing in the function body\n      if your service cannot learn new knowledge. For example, it may be hard to retrain a DNN model, so the facial recognition\n      service simply prints a message when it receives a learn request.\n      Otherwise, consider using a database system to store the new knowledge.\n      Currently, we use MongoDB to store the text and image knowledge.\n      You need to tell the command center whether to send a learn request\n      to your service or not, which is detailed in step 2.\n\n      `infer`: ask the intelligence to infer using the data supplied in the query,\n      which usually means the predicting process.\n\n      Notice all the three functions take in `QuerySpec` as their second parameters,\n      so let's see what `QuerySpec` means for each function.\n    \n   2. [`lucida/lucidatypes.thrift`](lucida/lucidatypes.thrift):\n\n      ```\n      struct QueryInput {\n          1: string type;\n          2: list<string> data;\n          3: list<string> tags;\n      }\n      struct QuerySpec {\n          1: string name;\n          2: list<QueryInput> content;\n      }\n      ```\n    \n      A `QuerySpec` has a name, which is `create` for `create`, `knowledge` for `learn`, and `query` for `infer`. \n      A `QuerySpec` also has a list of `QueryInput` called `content`, which is the data payload. \n      A `QueryInput` consists of a `type`, a list of `data`, and a list of `tags`. \n\n      * If the function call is `learn`:\n\n      One `QueryInput` is constructed by the command center currently,\n      but you should still iterate through all `QueryInput`s in case for change in future.\n      For `QueryInput`, `type` can be `text` for plain text, `url` for url address to extract text from, `image` for image,\n      or `unlearn` (undo learn) for the reverse process of learn.\n      Here is our assumptions: a service can handle either text or image,\n      and if it can handle text,\n      the types your service should handle are `text`, `url`, and `unlearn`, \n      and if it can handle image,\n      the types your service should handle are `image` and `unlearn`.\n      See step 2 for details on how to specify the type of knowledge that your service can learn.\n      If `type` is `text` or `url`, `data[i]` is the `i`th piece of text or url as new knowledge\n      and `tags[i]` is the id of the `i`th piece of text generated by a hash function in the command center;\n      if `type` is `image`, `data[i]` is the `i`th image as new knowledge\n      (notice that it is the actual string representation of an image and thus can very long),\n      and `tags[i]` is the label/name of the `i`th image received from the front end;\n      if `type` is `unlearn`, `data` should be ignored by your service (usually a list of an empty string),\n      and `tags[i]` is the `i`th id of the text or the image label to delete,\n      based on whether the service can handle text or image.\n\n      * If the function call is `infer`:\n\n      Each `QueryInput` in `content` corresponds to one service (CMD is not considered to be a service)\n      in the service graph, a connected directed acyclic graph (DAG) describing all services that are needed for the query.\n      Thus, for the following service graph, two `QueryInput`s are present in `content`, each being a node in the graph:\n\n      ```Command Center (CMD) -> Your Own Service 0 (YOS0) -> Your Own Service 1 (YOS1)```\n\n      For `QueryInput`, `type` can be `text` for plain text, or `image` for image (no `url` for `infer`).\n      See step 2 for details on how to specify the type of query that your service can process.\n      If `type` is `text`, `data[0]` is the `0`th piece of text;\n      if `type` is `image`, `data[0]` is the `0`th image. There is only one string in `data`.\n      `tags` have different meanings from `learn` and are of the following format:\n\n      ```\n      [host, port, <size of the following list>, <list of integers>]\n      ```\n\n      . `tags` in the `i`th `QueryInput` in `content` describe the location of the `i`th node and its relation to other nodes.\n      By location, we mean that the `host:port` specifies the location of the `i`th node.\n      By relation to other nodes, we mean that the list of integers specifies the indices of nodes that the `i`th node points to.\n\n      Therefore, the above service graph results in a `QuerySpec` that looks like this:\n\n      ```\n      { name: \"query\", \n      content: [ \n      { type: \"text\",\n      data: [ \"What's the speed of light?\" ],\n      tags: [\"localhost\", \"8083\", \"1\", \"1\"] },\n      { type: \"text\",\n      data: [ \"What's the speed of light?\" ],\n      tags: [\"localhost\", \"9090\", \"0\"] } ] }\n      ```\n\n      . We can define arbitrarily complicated service graphs. For example, for the following service graph:\n\n      ![Alt text](service_graph_0.png?raw=true \"Service Graph\")\n\n      , the resulting `QuerySpec` may look like this, assuming YOSX is running at 909X:\n\n      ```\n      { name: \"query\", \n      content: [\n      { type: \"image\",\n      data: [ \"1234567...abcdefg\" ],\n      tags: [\"localhost\", \"9090\", \"1\", 2\"] },\n      { type: \"image\",\n      data: [ \"1234567...abcdefg\" ],\n      tags: [\"localhost\", \"9091\", \"2\", \"2\", \"3\"] },\n      { type: \"text\",\n      data: [ \"Which person in my family is in this image?\" ],\n      tags: [\"localhost\", \"9092\", \"0\"] },\n      { type: \"image\",\n      data: [ \"1234567...abcdefg\" ],\n      tags: [\"localhost\", \"9093\", \"0\"] } ] }\n      ```\n\n      . Notice that if the order of `QueryInput` in `content` is rearranged, the resulting `QuerySpec`\n      still corresponds to the same graph.\n      In fact, there are `2^(N)` valid `QuerySpec`s for a given graph, and you need to define only one of them in\n      the configuration file of the command center. Notice that the starting nodes, YOS0 and YOS1, need to be specified separately,\n      so that the command center knows where to send the request(s) to. If more than one starting nodes are specified,\n      the command center simply concatenates the results returned from all of them.\n      See step 2 for more details on how to specify service graphs and starting nodes.\n\n      The command center guarantees to send a valid `QuerySpec`,\n      but your service is responsible for parsing the graph, further sending the request(s) to other service(s),\n      and returning the response(s) to the service(s) it is pointed to by.\n      `tags` simply provide the information of all nodes involved in the service graph.\n      That being said, suppose YOS0 deliberately does not send the request to YOS2,\n      and YOS2 is written in a way that it cannot return to YOS1 without processing requests from both YOS0 and YOS1.\n      Then YOS2 is stalled, which leads to YOS1 waiting for YOS2, and the command center waiting for YOS1.\n      Each service is also allowed to ignore or modify the graph if that is necessary, but that should be done with caution.\n\n      Although the service graph can be very complicated, it is usually very simple. At least for the current services,\n      the most complicated graph looks like this:\n\n      ```Command Center (CMD) -> IMM -> QA```\n\n      . Thus, some services can ignore the `tags` given the fact that they know they are always the only node in the graph.\n\n   3. Here are the code examples that you can use for your own service:\n\n      If it is written in C++, refer to the code in [`lucida/imagematching/opencv_imm/server/`]\n      (lucida/imagematching/opencv_imm/server/).\n      Look at `Makefile` for how to generate Thrift stubs which are the abstract base classes your handlers need to inherit.\n      Notice that the interface is implemented in `IMMHandler.h` and `IMMHandler.cpp`,\n      and the entry point (which uses a multi-threaded server provided by Thrift) is in `IMMServer.cpp`.\n\n      If it is written in Java, refer to the code in [`lucida/calendar/src/main/java/calendar/`]\n      (lucida/calendar/src/main/java/calendar/) and [`lucida/calendar/`](lucida/calendar/).\n      Look at `Makefile` for how to generate Thrift stubs which are the interfaces your handlers need to implement.\n      Notice that the interface is implemented in `CAServiceHandler.java`,\n      and the entry point (which uses a multi-threaded server provided by Thrift) is in `CalendarDaemon.java`.\n\n      If it is written in other programming languages, please refer to [the official tutorial](https://thrift.apache.org/tutorial/).\n    \n   4. Here is a list of what you need to do for step 1: \n    \n      * Add a thrift wrapper which typically consists of a Thrift handler implementing the Thrift interfaces,\n        and a daemon program which is the entry point of your service.\n        Refer to the code examples mentioned above.\n\n      * Modify your `Makefile` so that it uses the Thrift compiler to generate Thrift stubs code.\n        Following the style of the existing `Makefile`s is recommended.\n\n      * Test your service.\n\n      * (optional) Modify `tools` if you choose to put the dependencies of your service in this central point.\n\n      * (Local development) Modify the top-level [`Makefile`](Makefile) and [`lucida/Makefile`](lucida/Makefile)\n        so that `make local` and `make start_all` include your service.\n\n      * (Docker deployment) Create a Dockerfile image for your service, or merge it into the top-level [`Dockerfile`](Dockerfile)\n       and add Kubernetes `yaml` scripts for your service into [`tools/deploy/`](tools/deploy/).\n\n2. Configure the command center. \n\n   [`lucida/commandcenter/controllers/Config.py`](lucida/commandcenter/controllers/Config.py)\n   is the only file you must modify,\n   but you may also need to add sample queries to [`lucida/commandcenter/data/`](lucida/commandcenter/data/)\n   as training data for the query classifier.\n  \n   1. Modify the configuration file [`lucida/commandcenter/controllers/Config.py`](lucida/commandcenter/controllers/Config.py).\n    \n      ```\n      SERVICES = { \n        'IMM' : Service('IMM', 8082, 'image', 'image'),  # image matching\n        'QA' : Service('QA', 8083, 'text', 'text'), # question answering\n        'CA' : Service('CA', 8084, 'text', None), # calendar\n        }\n\n      CLASSIFIER_DESCRIPTIONS = { \n        'text' : { 'class_QA' :  Graph([Node('QA')]),\n                   'class_CA' : Graph([Node('CA')]) },\n        'image' : { 'class_IMM' : Graph([Node('IMM')]) },\n        'text_image' : { 'class_QA': Graph([Node('QA')]),\n                         'class_IMM' : Graph([Node('IMM')]), \n                         'class_IMM_QA' : Graph([Node('IMM', [1]), Node('QA')]) } \n        }\n      ```\n\n      * `SERVICES`\n\n      `SERVICES` is a dictionary from service name to service object.\n      A service object is constructed this way:\n\n      ```Service(name, port, input_type, learn_type)```\n\n      , where `name` is the name of the service, `port` is the port number,\n      `input_type` is the type of query that the service can handle which is either `text` or `image`,\n      and `learn_type` is the type of knowledge that the service can learn which is either`text`, `image`, or `None`.\n      Recall from step 1 that if `learn_type` is specified as `text`, your service will receive `type` of `text`, `url`, and `unlearn`,\n      and if `learn_type` is specified as `image`, your service will receive `type` of `image` and `unlearn`.\n\n      Notice that you do not need to specify the IP address of your service. By default it is `localhost`,\n      but if you use Kubernetes and run each service behind a Kubernetes `Service`,\n      Kubernetes dynamically assigns an IP address to the `Service` and sets an environment variable\n      `<SERVICE_NAME>_PORT_<PORT>_TCP_ADDR` for each running container in the cluster.\n      This implies that the Kubernetes `Service` defined in the `yaml` file\n      should have the same name as the service defined in this Python file.\n      All of the current Kubernetes scripts in [`tools/deploy/`](tools/deploy/) follow this naming convention.\n\n      For example, if you create a Kubernetes `Service` called `imm` and run your `IMM` container behind it,\n      the command center in the same cluster has the following environment variable\n\n      ```\n      IMM_PORT_8082_TCP_ADDR\n      ```\n\n      set to something like `10.0.0.92`. This IP address will be `tags[0]` in the `QueryInput` as described in step 1.\n\n      Notice that `ASR` (automatic speech recognition) is not listed here.\n      The reason is that we currently use [kaldi gstremer server] (https://github.com/alumae/kaldi-gstreamer-server)\n      which directly receives real-time voice query from the front end through web socket in the Docker mode.\n\n      * `CLASSIFIER_DESCRIPTIONS`\n\n      `CLASSIFIER_DESCRIPTIONS` is a dictionary from input type to possible prediction results.\n      Internally, the command center uses a query classifier that predicts the needed services\n      based on both the input type (transcription text, image, or both) and the content of transcription text if present.\n      In the above example, if the user only gives voice input without image, the input type is `text`,\n      and the prediction result can be either `class_QA` or `class_CA`.\n      If the query is classified as `class_QA` which means generic QA style questions whose training data is\n      in [`lucida/commandcenter/data/class_QA.txt`](lucida/commandcenter/data/class_QA.txt),\n      the services are needed are represented in a `Graph` with one `Node`, i.e. service `QA`.\n      If you want to replace the current QA implementation with your own,\n      you can still use the training data, and only modify the service graph to be:\n\n      ```\n      'text_image' : { 'class_QA': Graph([Node('NAME_OF_MY_OWN_QA_SERVICE')]), ...\n      ```\n\n      However, if you need to define another set of questions that a user can ask, e.g. questions about image captioning,\n      refer to the next section on how to add training data.\n\n      Notice that a service `Graph` object is constructed with a list of `Node`\n      Each `Node` is constructed with the service name and an optional list of node indices in the list\n      that the current node points to. By default, it is an empty list.\n      For example, `'class_IMM_QA' : Graph([Node('IMM', [1]), Node('QA')])` means that the prediction result `class_IMM_QA`\n      needs the following services:\n\n      ```\n      IMM -> QA\n      ```\n\n      Notice that we do not define which nodes the current node is pointed to by, so we do not know which node is pointed to\n      by the command center. Thus, we need to specify the starting nodes separately.\n      This is done by an optional second parameter in the constructor of `Graph`:\n\n      ```\n      def __init__(self, node_list, starting_indices=[0]):\n      ```\n\n      As you see, by default, the command center assumes the 0th node in the node list is the starting node.\n      Thus, a `QuerySpec` like the following will be sent to the `IMM` service:\n\n      ```\n      { name: \"query\", \n      content: [\n      { type: \"image\",\n      data: [ \"1234567...abcdefg\" ],\n      tags: [\"localhost\", \"8082\", \"1\", 1\"] },\n      { type: \"text\",\n      data: [ \"How old is this person?\" ],\n      tags: [\"localhost\", \"8083\", \"0\"] } ] }\n      ```\n\n      The `IMM` service receives this `QuerySpec` along with `LUCID`, and is responsible for further sending\n      the request to the `QA` service. The `IMM` service is allowed to modify the `QuerySpec` and\n      send a reconstructed `QuerySpec` to `QA`, but as long as `IMM` finally returns a `string` to the\n      command center, it is fine. This degree of flexibility opens up opportunities for complicated communication\n      between your services. Usually, a graph with one node suffices, because that one node may be an entry point\n      to a cluster of your services, which may have complicated feedback loops and use a different communication mechanism.\n\n      Notice that there is only one possible prediction result if the user only gives image input: `class_IMM`,\n      because the query classifier only works on text input.\n      However, you can still send the image to multiple services like this:\n\n      ```\n      ''image' : { 'class_IMM' : Graph([Node('IMM'), Node('IMC'), Node('IMAGE_CAPTIONING')], [0, 1, 2]) }, ...\n      ```\n\n      Be aware that there are other parameters that you can change in this configuration file,\n      which are pretty self-explanatory in their names and comments.\n    \n   2. Add training data for your own query class.\n  \n      We already prepare some sample training data in [`lucida/commandcenter/data/`](lucida/commandcenter/data/),\n      but if you need to define a custom type of query that your service can handle,\n      you should create the following file in the above directory:\n\n      ```\n      class_<NAME_OF_YOUR_QUERY_CLASS>.txt\n      ```\n\n      , and have at least 40 pieces of text in it, each being one way to ask about the same question.\n    \n"
        },
        {
          "name": "high_level.png",
          "type": "blob",
          "size": 873.6591796875,
          "content": null
        },
        {
          "name": "lucida",
          "type": "tree",
          "content": null
        },
        {
          "name": "service_graph_0.png",
          "type": "blob",
          "size": 54.0009765625,
          "content": null
        },
        {
          "name": "tools",
          "type": "tree",
          "content": null
        },
        {
          "name": "tutorial.pdf",
          "type": "blob",
          "size": 804.9091796875,
          "content": null
        },
        {
          "name": "tutorial.pptx",
          "type": "blob",
          "size": 739.6259765625,
          "content": null
        }
      ]
    }
  ]
}