{
  "metadata": {
    "timestamp": 1736557422010,
    "page": 428,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjQ1MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "GokuMohandas/Made-With-ML",
      "stars": 37923,
      "defaultBranch": "main",
      "files": [
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 1.02,
          "content": "# Data\nlogs/\nstores/\nmlflow/\nresults/\nworkspaces/\nefs/\n\n# VSCode\n.vscode/\n.idea\n\n# Byte-compiled / optimized / DLL files\n__pycache__/\n*.py[cod]\n*$py.class\n\n# C extensions\n*.so\n\n# Distribution / packaging\n.Python\nbuild/\ndevelop-eggs/\ndist/\ndownloads/\neggs/\n.eggs/\nlib/\nlib64/\nparts/\nsdist/\nvar/\nwheels/\npip-wheel-metadata/\nshare/python-wheels/\n*.egg-info/\n.installed.cfg\n*.egg\nMANIFEST\n\n# PyInstaller\n*.manifest\n*.spec\n\n# Installer logs\npip-log.txt\npip-delete-this-directory.txt\n\n# Unit test / coverage reports\nhtmlcov/\n.tox/\n.nox/\n.coverage\n.coverage.*\n.cache\nnosetests.xml\ncoverage.xml\n*.cover\n*.py,cover\n.hypothesis/\n.pytest_cache/\n\n# Flask:\ninstance/\n.webassets-cache\n\n# Scrapy:\n.scrapy\n\n# Sphinx\ndocs/_build/\n\n# PyBuilder\ntarget/\n\n# IPython\n.ipynb_checkpoints\nprofile_default/\nipython_config.py\n\n# pyenv\n.python-version\n\n# PEP 582\n__pypackages__/\n\n# Celery\ncelerybeat-schedule\ncelerybeat.pid\n\n# Environment\n.env\n.venv\nenv/\nvenv/\nENV/\nenv.bak/\nvenv.bak/\n\n# mkdocs\nsite/\n\n# Airflow\nairflow/airflow.db\n\n# MacOS\n.DS_Store\n\n# Clean up\n.trash/\n"
        },
        {
          "name": ".pre-commit-config.yaml",
          "type": "blob",
          "size": 0.61,
          "content": "# See https://pre-commit.com for more information\n# See https://pre-commit.com/hooks.html for more hooks\nrepos:\n-   repo: https://github.com/pre-commit/pre-commit-hooks\n    rev: v4.5.0\n    hooks:\n    -   id: trailing-whitespace\n    -   id: end-of-file-fixer\n    -   id: check-merge-conflict\n    -   id: check-yaml\n    -   id: check-added-large-files\n        args: ['--maxkb=1000']\n        exclude: \"notebooks\"\n    -   id: check-yaml\n        exclude: \"mkdocs.yml\"\n-   repo: local\n    hooks:\n    -   id: clean\n        name: clean\n        entry: make\n        args: [\"clean\"]\n        language: system\n        pass_filenames: false\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 1.04,
          "content": "MIT License\n\nCopyright (c) 2023 Made With ML\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n"
        },
        {
          "name": "Makefile",
          "type": "blob",
          "size": 0.41,
          "content": "# Makefile\nSHELL = /bin/bash\n\n# Styling\n.PHONY: style\nstyle:\n\tblack .\n\tflake8\n\tpython3 -m isort .\n\tpyupgrade\n\n# Cleaning\n.PHONY: clean\nclean: style\n\tpython notebooks/clear_cell_nums.py\n\tfind . -type f -name \"*.DS_Store\" -ls -delete\n\tfind . | grep -E \"(__pycache__|\\.pyc|\\.pyo)\" | xargs rm -rf\n\tfind . | grep -E \".pytest_cache\" | xargs rm -rf\n\tfind . | grep -E \".ipynb_checkpoints\" | xargs rm -rf\n\trm -rf .coverage*\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 23.01,
          "content": "<div align=\"center\">\n<h1><img width=\"30\" src=\"https://madewithml.com/static/images/rounded_logo.png\">&nbsp;<a href=\"https://madewithml.com/\">Made With ML</a></h1>\nDesign ¬∑ Develop ¬∑ Deploy ¬∑ Iterate\n<br>\nJoin 40K+ developers in learning how to responsibly deliver value with ML.\n    <br>\n</div>\n\n<br>\n\n<div align=\"center\">\n    <a target=\"_blank\" href=\"https://madewithml.com/\"><img src=\"https://img.shields.io/badge/Subscribe-40K-brightgreen\"></a>&nbsp;\n    <a target=\"_blank\" href=\"https://github.com/GokuMohandas/Made-With-ML\"><img src=\"https://img.shields.io/github/stars/GokuMohandas/Made-With-ML.svg?style=social&label=Star\"></a>&nbsp;\n    <a target=\"_blank\" href=\"https://www.linkedin.com/in/goku\"><img src=\"https://img.shields.io/badge/style--5eba00.svg?label=LinkedIn&logo=linkedin&style=social\"></a>&nbsp;\n    <a target=\"_blank\" href=\"https://twitter.com/GokuMohandas\"><img src=\"https://img.shields.io/twitter/follow/GokuMohandas.svg?label=Follow&style=social\"></a>\n    <br>\n    üî•&nbsp; Among the <a href=\"https://github.com/GokuMohandas/Made-With-ML\" target=\"_blank\">top ML repositories</a> on GitHub\n</div>\n\n<br>\n<hr>\n\n## Lessons\n\nLearn how to combine machine learning with software engineering to design, develop, deploy and iterate on production-grade ML applications.\n\n- Lessons: https://madewithml.com/\n- Code: [GokuMohandas/Made-With-ML](https://github.com/GokuMohandas/Made-With-ML)\n\n<a href=\"https://madewithml.com/#course\">\n  <img src=\"https://madewithml.com/static/images/lessons.png\" alt=\"lessons\">\n</a>\n\n## Overview\n\nIn this course, we'll go from experimentation (design + development) to production (deployment + iteration). We'll do this iteratively by motivating the components that will enable us to build a *reliable* production system.\n\n<blockquote>\n  <img width=20 src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/0/09/YouTube_full-color_icon_%282017%29.svg/640px-YouTube_full-color_icon_%282017%29.svg.png\">&nbsp; Be sure to watch the video below for a quick overview of what we'll be building.\n</blockquote>\n\n<div align=\"center\">\n  <a href=\"https://youtu.be/AWgkt8H8yVo\"><img src=\"https://img.youtube.com/vi/AWgkt8H8yVo/0.jpg\" alt=\"Course overview video\"></a>\n</div>\n\n<br>\n\n- **üí° First principles**: before we jump straight into the code, we develop a first principles understanding for every machine learning concept.\n- **üíª Best practices**: implement software engineering best practices as we develop and deploy our machine learning models.\n- **üìà Scale**: easily scale ML workloads (data, train, tune, serve) in Python without having to learn completely new languages.\n- **‚öôÔ∏è MLOps**: connect MLOps components (tracking, testing, serving, orchestration, etc.) as we build an end-to-end machine learning system.\n- **üöÄ Dev to Prod**: learn how to quickly and reliably go from development to production without any changes to our code or infra management.\n- **üêô CI/CD**: learn how to create mature CI/CD workflows to continuously train and deploy better models in a modular way that integrates with any stack.\n\n## Audience\n\nMachine learning is not a separate industry, instead, it's a powerful way of thinking about data that's not reserved for any one type of person.\n\n- **üë©‚Äçüíª All developers**: whether software/infra engineer or data scientist, ML is increasingly becoming a key part of the products that you'll be developing.\n- **üë©‚Äçüéì College graduates**: learn the practical skills required for industry and bridge gap between the university curriculum and what industry expects.\n- **üë©‚Äçüíº Product/Leadership**: who want to develop a technical foundation so that they can build amazing (and reliable) products powered by machine learning.\n\n## Set up\n\nBe sure to go through the [course](https://madewithml/#course) for a much more detailed walkthrough of the content on this repository. We will have instructions for both local laptop and Anyscale clusters for the sections below, so be sure to toggle the ‚ñ∫ dropdown based on what you're using (Anyscale instructions will be toggled on by default). If you do want to run this course with Anyscale, where we'll provide the **structure**, **compute (GPUs)** and **community** to learn everything in one day, join our next upcoming live cohort ‚Üí [sign up here](https://4190urw86oh.typeform.com/madewithml)!\n\n### Cluster\n\nWe'll start by setting up our cluster with the environment and compute configurations.\n\n<details>\n  <summary>Local</summary><br>\n  Your personal laptop (single machine) will act as the cluster, where one CPU will be the head node and some of the remaining CPU will be the worker nodes. All of the code in this course will work in any personal laptop though it will be slower than executing the same workloads on a larger cluster.\n</details>\n\n<details open>\n  <summary>Anyscale</summary><br>\n\n  We can create an [Anyscale Workspace](https://docs.anyscale.com/develop/workspaces/get-started) using the [webpage UI](https://console.anyscale.com/o/madewithml/workspaces/add/blank).\n\n  ```md\n  - Workspace name: `madewithml`\n  - Project: `madewithml`\n  - Cluster environment name: `madewithml-cluster-env`\n  # Toggle `Select from saved configurations`\n  - Compute config: `madewithml-cluster-compute-g5.4xlarge`\n  ```\n\n  > Alternatively, we can use the [CLI](https://docs.anyscale.com/reference/anyscale-cli) to create the workspace via `anyscale workspace create ...`\n\n</details>\n\n<details>\n  <summary>Other (cloud platforms, K8s, on-prem)</summary><br>\n\n  If you don't want to do this course locally or via Anyscale, you have the following options:\n\n  - On [AWS and GCP](https://docs.ray.io/en/latest/cluster/vms/index.html#cloud-vm-index). Community-supported Azure and Aliyun integrations also exist.\n  - On [Kubernetes](https://docs.ray.io/en/latest/cluster/kubernetes/index.html#kuberay-index), via the officially supported KubeRay project.\n  - Deploy Ray manually [on-prem](https://docs.ray.io/en/latest/cluster/vms/user-guides/launching-clusters/on-premises.html#on-prem) or onto platforms [not listed here](https://docs.ray.io/en/latest/cluster/vms/user-guides/community/index.html#ref-cluster-setup).\n\n</details>\n\n### Git setup\n\nCreate a repository by following these instructions: [Create a new repository](https://github.com/new) ‚Üí name it `Made-With-ML` ‚Üí Toggle `Add a README file` (**very important** as this creates a `main` branch) ‚Üí Click `Create repository` (scroll down)\n\nNow we're ready to clone the repository that has all of our code:\n\n```bash\ngit clone https://github.com/GokuMohandas/Made-With-ML.git .\n```\n\n### Credentials\n\n```bash\ntouch .env\n```\n```bash\n# Inside .env\nGITHUB_USERNAME=\"CHANGE_THIS_TO_YOUR_USERNAME\"  # ‚Üê CHANGE THIS\n```\n```bash\nsource .env\n```\n\n### Virtual environment\n\n<details>\n  <summary>Local</summary><br>\n\n  ```bash\n  export PYTHONPATH=$PYTHONPATH:$PWD\n  python3 -m venv venv  # recommend using Python 3.10\n  source venv/bin/activate  # on Windows: venv\\Scripts\\activate\n  python3 -m pip install --upgrade pip setuptools wheel\n  python3 -m pip install -r requirements.txt\n  pre-commit install\n  pre-commit autoupdate\n  ```\n\n  > Highly recommend using Python `3.10` and using [pyenv](https://github.com/pyenv/pyenv) (mac) or [pyenv-win](https://github.com/pyenv-win/pyenv-win) (windows).\n\n</details>\n\n<details open>\n  <summary>Anyscale</summary><br>\n\n  Our environment with the appropriate Python version and libraries is already all set for us through the cluster environment we used when setting up our Anyscale Workspace. So we just need to run these commands:\n  ```bash\n  export PYTHONPATH=$PYTHONPATH:$PWD\n  pre-commit install\n  pre-commit autoupdate\n  ```\n\n</details>\n\n## Notebook\n\nStart by exploring the [jupyter notebook](notebooks/madewithml.ipynb) to interactively walkthrough the core machine learning workloads.\n\n<div align=\"center\">\n  <img src=\"https://madewithml.com/static/images/mlops/systems-design/workloads.png\">\n</div>\n\n<details>\n  <summary>Local</summary><br>\n\n  ```bash\n  # Start notebook\n  jupyter lab notebooks/madewithml.ipynb\n```\n\n</details>\n\n<details open>\n  <summary>Anyscale</summary><br>\n\n  Click on the Jupyter icon &nbsp;<img width=15 src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/3/38/Jupyter_logo.svg/1200px-Jupyter_logo.svg.png\">&nbsp; at the top right corner of our Anyscale Workspace page and this will open up our JupyterLab instance in a new tab. Then navigate to the `notebooks` directory and open up the `madewithml.ipynb` notebook.\n\n</details>\n\n\n## Scripts\n\nNow we'll execute the same workloads using the clean Python scripts following software engineering best practices (testing, documentation, logging, serving, versioning, etc.) The code we've implemented in our notebook will be refactored into the following scripts:\n\n```bash\nmadewithml\n‚îú‚îÄ‚îÄ config.py\n‚îú‚îÄ‚îÄ data.py\n‚îú‚îÄ‚îÄ evaluate.py\n‚îú‚îÄ‚îÄ models.py\n‚îú‚îÄ‚îÄ predict.py\n‚îú‚îÄ‚îÄ serve.py\n‚îú‚îÄ‚îÄ train.py\n‚îú‚îÄ‚îÄ tune.py\n‚îî‚îÄ‚îÄ utils.py\n```\n\n**Note**: Change the `--num-workers`, `--cpu-per-worker`, and `--gpu-per-worker` input argument values below based on your system's resources. For example, if you're on a local laptop, a reasonable configuration would be `--num-workers 6 --cpu-per-worker 1 --gpu-per-worker 0`.\n\n### Training\n```bash\nexport EXPERIMENT_NAME=\"llm\"\nexport DATASET_LOC=\"https://raw.githubusercontent.com/GokuMohandas/Made-With-ML/main/datasets/dataset.csv\"\nexport TRAIN_LOOP_CONFIG='{\"dropout_p\": 0.5, \"lr\": 1e-4, \"lr_factor\": 0.8, \"lr_patience\": 3}'\npython madewithml/train.py \\\n    --experiment-name \"$EXPERIMENT_NAME\" \\\n    --dataset-loc \"$DATASET_LOC\" \\\n    --train-loop-config \"$TRAIN_LOOP_CONFIG\" \\\n    --num-workers 1 \\\n    --cpu-per-worker 3 \\\n    --gpu-per-worker 1 \\\n    --num-epochs 10 \\\n    --batch-size 256 \\\n    --results-fp results/training_results.json\n```\n\n### Tuning\n```bash\nexport EXPERIMENT_NAME=\"llm\"\nexport DATASET_LOC=\"https://raw.githubusercontent.com/GokuMohandas/Made-With-ML/main/datasets/dataset.csv\"\nexport TRAIN_LOOP_CONFIG='{\"dropout_p\": 0.5, \"lr\": 1e-4, \"lr_factor\": 0.8, \"lr_patience\": 3}'\nexport INITIAL_PARAMS=\"[{\\\"train_loop_config\\\": $TRAIN_LOOP_CONFIG}]\"\npython madewithml/tune.py \\\n    --experiment-name \"$EXPERIMENT_NAME\" \\\n    --dataset-loc \"$DATASET_LOC\" \\\n    --initial-params \"$INITIAL_PARAMS\" \\\n    --num-runs 2 \\\n    --num-workers 1 \\\n    --cpu-per-worker 3 \\\n    --gpu-per-worker 1 \\\n    --num-epochs 10 \\\n    --batch-size 256 \\\n    --results-fp results/tuning_results.json\n```\n\n### Experiment tracking\n\nWe'll use [MLflow](https://mlflow.org/) to track our experiments and store our models and the [MLflow Tracking UI](https://www.mlflow.org/docs/latest/tracking.html#tracking-ui) to view our experiments. We have been saving our experiments to a local directory but note that in an actual production setting, we would have a central location to store all of our experiments. It's easy/inexpensive to spin up your own MLflow server for all of your team members to track their experiments on or use a managed solution like [Weights & Biases](https://wandb.ai/site), [Comet](https://www.comet.ml/), etc.\n\n```bash\nexport MODEL_REGISTRY=$(python -c \"from madewithml import config; print(config.MODEL_REGISTRY)\")\nmlflow server -h 0.0.0.0 -p 8080 --backend-store-uri $MODEL_REGISTRY\n```\n\n<details>\n  <summary>Local</summary><br>\n\n  If you're running this notebook on your local laptop then head on over to <a href=\"http://localhost:8080/\" target=\"_blank\">http://localhost:8080/</a> to view your MLflow dashboard.\n\n</details>\n\n<details open>\n  <summary>Anyscale</summary><br>\n\n  If you're on <a href=\"https://docs.anyscale.com/develop/workspaces/get-started\" target=\"_blank\">Anyscale Workspaces</a>, then we need to first expose the port of the MLflow server. Run the following command on your Anyscale Workspace terminal to generate the public URL to your MLflow server.\n\n  ```bash\n  APP_PORT=8080\n  echo https://$APP_PORT-port-$ANYSCALE_SESSION_DOMAIN\n  ```\n\n</details>\n\n### Evaluation\n```bash\nexport EXPERIMENT_NAME=\"llm\"\nexport RUN_ID=$(python madewithml/predict.py get-best-run-id --experiment-name $EXPERIMENT_NAME --metric val_loss --mode ASC)\nexport HOLDOUT_LOC=\"https://raw.githubusercontent.com/GokuMohandas/Made-With-ML/main/datasets/holdout.csv\"\npython madewithml/evaluate.py \\\n    --run-id $RUN_ID \\\n    --dataset-loc $HOLDOUT_LOC \\\n    --results-fp results/evaluation_results.json\n```\n```json\n{\n  \"timestamp\": \"June 09, 2023 09:26:18 AM\",\n  \"run_id\": \"6149e3fec8d24f1492d4a4cabd5c06f6\",\n  \"overall\": {\n    \"precision\": 0.9076136428670714,\n    \"recall\": 0.9057591623036649,\n    \"f1\": 0.9046792827719773,\n    \"num_samples\": 191.0\n  },\n...\n```\n\n### Inference\n```bash\nexport EXPERIMENT_NAME=\"llm\"\nexport RUN_ID=$(python madewithml/predict.py get-best-run-id --experiment-name $EXPERIMENT_NAME --metric val_loss --mode ASC)\npython madewithml/predict.py predict \\\n    --run-id $RUN_ID \\\n    --title \"Transfer learning with transformers\" \\\n    --description \"Using transformers for transfer learning on text classification tasks.\"\n```\n```json\n[{\n  \"prediction\": [\n    \"natural-language-processing\"\n  ],\n  \"probabilities\": {\n    \"computer-vision\": 0.0009767753,\n    \"mlops\": 0.0008223939,\n    \"natural-language-processing\": 0.99762577,\n    \"other\": 0.000575123\n  }\n}]\n```\n\n### Serving\n\n<details>\n  <summary>Local</summary><br>\n\n  ```bash\n  # Start\n  ray start --head\n  ```\n\n  ```bash\n  # Set up\n  export EXPERIMENT_NAME=\"llm\"\n  export RUN_ID=$(python madewithml/predict.py get-best-run-id --experiment-name $EXPERIMENT_NAME --metric val_loss --mode ASC)\n  python madewithml/serve.py --run_id $RUN_ID\n  ```\n\n  Once the application is running, we can use it via cURL, Python, etc.:\n\n  ```python\n  # via Python\n  import json\n  import requests\n  title = \"Transfer learning with transformers\"\n  description = \"Using transformers for transfer learning on text classification tasks.\"\n  json_data = json.dumps({\"title\": title, \"description\": description})\n  requests.post(\"http://127.0.0.1:8000/predict\", data=json_data).json()\n  ```\n\n  ```bash\n  ray stop  # shutdown\n  ```\n\n</details>\n\n<details open>\n  <summary>Anyscale</summary><br>\n\n  In Anyscale Workspaces, Ray is already running so we don't have to manually start/shutdown like we have to do locally.\n\n  ```bash\n  # Set up\n  export EXPERIMENT_NAME=\"llm\"\n  export RUN_ID=$(python madewithml/predict.py get-best-run-id --experiment-name $EXPERIMENT_NAME --metric val_loss --mode ASC)\n  python madewithml/serve.py --run_id $RUN_ID\n  ```\n\n  Once the application is running, we can use it via cURL, Python, etc.:\n\n  ```python\n  # via Python\n  import json\n  import requests\n  title = \"Transfer learning with transformers\"\n  description = \"Using transformers for transfer learning on text classification tasks.\"\n  json_data = json.dumps({\"title\": title, \"description\": description})\n  requests.post(\"http://127.0.0.1:8000/predict\", data=json_data).json()\n  ```\n\n</details>\n\n### Testing\n```bash\n# Code\npython3 -m pytest tests/code --verbose --disable-warnings\n\n# Data\nexport DATASET_LOC=\"https://raw.githubusercontent.com/GokuMohandas/Made-With-ML/main/datasets/dataset.csv\"\npytest --dataset-loc=$DATASET_LOC tests/data --verbose --disable-warnings\n\n# Model\nexport EXPERIMENT_NAME=\"llm\"\nexport RUN_ID=$(python madewithml/predict.py get-best-run-id --experiment-name $EXPERIMENT_NAME --metric val_loss --mode ASC)\npytest --run-id=$RUN_ID tests/model --verbose --disable-warnings\n\n# Coverage\npython3 -m pytest tests/code --cov madewithml --cov-report html --disable-warnings  # html report\npython3 -m pytest tests/code --cov madewithml --cov-report term --disable-warnings  # terminal report\n```\n\n## Production\n\nFrom this point onwards, in order to deploy our application into production, we'll need to either be on Anyscale or on a [cloud VM](https://docs.ray.io/en/latest/cluster/vms/index.html#cloud-vm-index) / [on-prem](https://docs.ray.io/en/latest/cluster/vms/user-guides/launching-clusters/on-premises.html#on-prem) cluster you manage yourself (w/ Ray). If not on Anyscale, the commands will be [slightly different](https://docs.ray.io/en/latest/cluster/running-applications/job-submission/index.html) but the concepts will be the same.\n\n> If you don't want to set up all of this yourself, we highly recommend joining our [upcoming live cohort](https://4190urw86oh.typeform.com/madewithml){:target=\"_blank\"} where we'll provide an environment with all of this infrastructure already set up for you so that you just focused on the machine learning.\n\n<div align=\"center\">\n  <img src=\"https://madewithml.com/static/images/mlops/jobs_and_services/manual.png\">\n</div>\n\n### Authentication\n\nThese credentials below are **automatically** set for us if we're using Anyscale Workspaces. We **do not** need to set these credentials explicitly on Workspaces but we do if we're running this locally or on a cluster outside of where our Anyscale Jobs and Services are configured to run.\n\n``` bash\nexport ANYSCALE_HOST=https://console.anyscale.com\nexport ANYSCALE_CLI_TOKEN=$YOUR_CLI_TOKEN  # retrieved from Anyscale credentials page\n```\n\n### Cluster environment\n\nThe cluster environment determines **where** our workloads will be executed (OS, dependencies, etc.) We've already created this [cluster environment](./deploy/cluster_env.yaml) for us but this is how we can create/update one ourselves.\n\n```bash\nexport CLUSTER_ENV_NAME=\"madewithml-cluster-env\"\nanyscale cluster-env build deploy/cluster_env.yaml --name $CLUSTER_ENV_NAME\n```\n\n### Compute configuration\n\nThe compute configuration determines **what** resources our workloads will be executes on. We've already created this [compute configuration](./deploy/cluster_compute.yaml) for us but this is how we can create it ourselves.\n\n```bash\nexport CLUSTER_COMPUTE_NAME=\"madewithml-cluster-compute-g5.4xlarge\"\nanyscale cluster-compute create deploy/cluster_compute.yaml --name $CLUSTER_COMPUTE_NAME\n```\n\n### Anyscale jobs\n\nNow we're ready to execute our ML workloads. We've decided to combine them all together into one [job](./deploy/jobs/workloads.yaml) but we could have also created separate jobs for each workload (train, evaluate, etc.) We'll start by editing the `$GITHUB_USERNAME` slots inside our [`workloads.yaml`](./deploy/jobs/workloads.yaml) file:\n```yaml\nruntime_env:\n  working_dir: .\n  upload_path: s3://madewithml/$GITHUB_USERNAME/jobs  # <--- CHANGE USERNAME (case-sensitive)\n  env_vars:\n    GITHUB_USERNAME: $GITHUB_USERNAME  # <--- CHANGE USERNAME (case-sensitive)\n```\n\nThe `runtime_env` here specifies that we should upload our current `working_dir` to an S3 bucket so that all of our workers when we execute an Anyscale Job have access to the code to use. The `GITHUB_USERNAME` is used later to save results from our workloads to S3 so that we can retrieve them later (ex. for serving).\n\nNow we're ready to submit our job to execute our ML workloads:\n```bash\nanyscale job submit deploy/jobs/workloads.yaml\n```\n\n### Anyscale Services\n\nAnd after our ML workloads have been executed, we're ready to launch our serve our model to production. Similar to our Anyscale Jobs configs, be sure to change the `$GITHUB_USERNAME` in [`serve_model.yaml`](./deploy/services/serve_model.yaml).\n\n```yaml\nray_serve_config:\n  import_path: deploy.services.serve_model:entrypoint\n  runtime_env:\n    working_dir: .\n    upload_path: s3://madewithml/$GITHUB_USERNAME/services  # <--- CHANGE USERNAME (case-sensitive)\n    env_vars:\n      GITHUB_USERNAME: $GITHUB_USERNAME  # <--- CHANGE USERNAME (case-sensitive)\n```\n\nNow we're ready to launch our service:\n```bash\n# Rollout service\nanyscale service rollout -f deploy/services/serve_model.yaml\n\n# Query\ncurl -X POST -H \"Content-Type: application/json\" -H \"Authorization: Bearer $SECRET_TOKEN\" -d '{\n  \"title\": \"Transfer learning with transformers\",\n  \"description\": \"Using transformers for transfer learning on text classification tasks.\"\n}' $SERVICE_ENDPOINT/predict/\n\n# Rollback (to previous version of the Service)\nanyscale service rollback -f $SERVICE_CONFIG --name $SERVICE_NAME\n\n# Terminate\nanyscale service terminate --name $SERVICE_NAME\n```\n\n### CI/CD\n\nWe're not going to manually deploy our application every time we make a change. Instead, we'll automate this process using GitHub Actions!\n\n<div align=\"center\">\n  <img src=\"https://madewithml.com/static/images/mlops/cicd/cicd.png\">\n</div>\n\n1. Create a new github branch to save our changes to and execute CI/CD workloads:\n```bash\ngit remote set-url origin https://github.com/$GITHUB_USERNAME/Made-With-ML.git  # <-- CHANGE THIS to your username\ngit checkout -b dev\n```\n\n2. We'll start by adding the necessary credentials to the [`/settings/secrets/actions`](https://github.com/GokuMohandas/Made-With-ML/settings/secrets/actions) page of our GitHub repository.\n\n``` bash\nexport ANYSCALE_HOST=https://console.anyscale.com\nexport ANYSCALE_CLI_TOKEN=$YOUR_CLI_TOKEN  # retrieved from https://console.anyscale.com/o/madewithml/credentials\n```\n\n3. Now we can make changes to our code (not on `main` branch) and push them to GitHub. But in order to push our code to GitHub, we'll need to first authenticate with our credentials before pushing to our repository:\n\n```bash\ngit config --global user.name $GITHUB_USERNAME  # <-- CHANGE THIS to your username\ngit config --global user.email you@example.com  # <-- CHANGE THIS to your email\ngit add .\ngit commit -m \"\"  # <-- CHANGE THIS to your message\ngit push origin dev\n```\n\nNow you will be prompted to enter your username and password (personal access token). Follow these steps to get personal access token: [New GitHub personal access token](https://github.com/settings/tokens/new) ‚Üí Add a name ‚Üí Toggle `repo` and `workflow` ‚Üí Click `Generate token` (scroll down) ‚Üí Copy the token and paste it when prompted for your password.\n\n4. Now we can start a PR from this branch to our `main` branch and this will trigger the [workloads workflow](/.github/workflows/workloads.yaml). If the workflow (Anyscale Jobs) succeeds, this will produce comments with the training and evaluation results directly on the PR.\n\n<div align=\"center\">\n  <img src=\"https://madewithml.com/static/images/mlops/cicd/comments.png\">\n</div>\n\n5. If we like the results, we can merge the PR into the `main` branch. This will trigger the [serve workflow](/.github/workflows/serve.yaml) which will rollout our new service to production!\n\n### Continual learning\n\nWith our CI/CD workflow in place to deploy our application, we can now focus on continually improving our model. It becomes really easy to extend on this foundation to connect to scheduled runs (cron), [data pipelines](https://madewithml.com/courses/mlops/data-engineering/), drift detected through [monitoring](https://madewithml.com/courses/mlops/monitoring/), [online evaluation](https://madewithml.com/courses/mlops/evaluation/#online-evaluation), etc. And we can easily add additional context such as comparing any experiment with what's currently in production (directly in the PR even), etc.\n\n<div align=\"center\">\n  <img src=\"https://madewithml.com/static/images/mlops/cicd/continual.png\">\n</div>\n\n## FAQ\n\n### Jupyter notebook kernels\n\nIssues with configuring the notebooks with jupyter? By default, jupyter will use the kernel with our virtual environment but we can also manually add it to jupyter:\n```bash\npython3 -m ipykernel install --user --name=venv\n```\nNow we can open up a notebook ‚Üí Kernel (top menu bar) ‚Üí Change Kernel ‚Üí `venv`. To ever delete this kernel, we can do the following:\n```bash\njupyter kernelspec list\njupyter kernelspec uninstall venv\n```\n"
        },
        {
          "name": "datasets",
          "type": "tree",
          "content": null
        },
        {
          "name": "deploy",
          "type": "tree",
          "content": null
        },
        {
          "name": "docs",
          "type": "tree",
          "content": null
        },
        {
          "name": "madewithml",
          "type": "tree",
          "content": null
        },
        {
          "name": "mkdocs.yml",
          "type": "blob",
          "size": 0.51,
          "content": "site_name: Made With ML\nsite_url: https://madewithml.com/\nrepo_url: https://github.com/GokuMohandas/Made-With-ML/\nnav:\n  - Home: index.md\n  - madewithml:\n    - data: madewithml/data.md\n    - models: madewithml/models.md\n    - train: madewithml/train.md\n    - tune: madewithml/tune.md\n    - evaluate: madewithml/evaluate.md\n    - predict: madewithml/predict.md\n    - serve: madewithml/serve.md\n    - utils: madewithml/utils.md\ntheme: readthedocs\nplugins:\n  - mkdocstrings\nwatch:\n  - .  # reload docs for any file changes\n"
        },
        {
          "name": "notebooks",
          "type": "tree",
          "content": null
        },
        {
          "name": "pyproject.toml",
          "type": "blob",
          "size": 0.81,
          "content": "# Black formatting\n[tool.black]\nline-length = 150\ninclude = '\\.pyi?$'\nexclude = '''\n/(\n      .eggs         # exclude a few common directories in the\n    | .git          # root of the project\n    | .hg\n    | .mypy_cache\n    | .tox\n    | venv\n    | _build\n    | buck-out\n    | build\n    | dist\n  )/\n'''\n\n# iSort\n[tool.isort]\nprofile = \"black\"\nline_length = 79\nmulti_line_output = 3\ninclude_trailing_comma = true\nvirtual_env = \"venv\"\n\n[tool.flake8]\nexclude = \"venv\"\nignore = [\"E501\", \"W503\", \"E226\"]\n# E501: Line too long\n# W503: Line break occurred before binary operator\n# E226: Missing white space around arithmetic operator\n\n[tool.pyupgrade]\npy39plus = true\n\n# Pytest\n[tool.pytest.ini_options]\ntestpaths = [\"tests\"]\npython_files = \"test_*.py\"\n\n# Pytest cov\n[tool.coverage.run]\nomit=[\"madewithml/evaluate.py\", \"madewithml/serve.py\"]\n"
        },
        {
          "name": "requirements.txt",
          "type": "blob",
          "size": 0.67,
          "content": "# Default\nhyperopt==0.2.7\nipywidgets>=8\nmatplotlib==3.7.1\nmlflow==2.3.1\nnltk==3.8.1\nnumpy==1.24.3\nnumpyencoder==0.3.0\npandas==2.0.1\npython-dotenv==1.0.0\nray[air]==2.7.0\nscikit-learn==1.2.2\nsnorkel==0.9.9\nSQLAlchemy==1.4.48\ntorch==2.0.0\ntransformers==4.28.1\n\n# Notebook\ncleanlab==2.3.1\njupyterlab==3.6.3\nlime==0.2.0.1\nseaborn==0.12.2\nwordcloud==1.9.2\n\n# Documentation\nmkdocs==1.4.2\nmkdocstrings==0.21.2\nmkdocstrings[python]>=0.18\n\n# Styling\nblack==23.3.0\nflake8==6.0.0\nFlake8-pyproject==1.2.3\nisort==5.12.0\npyupgrade==3.3.2\n\n# Testing\ngreat-expectations==0.16.5\npytest==7.3.1\npytest-cov==4.0.0\n\n# Development\nfastapi==0.95.2\npre-commit==3.2.2\ntyper==0.9.0\n\n# Deployment\nanyscale==0.5.131\n"
        },
        {
          "name": "tests",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}