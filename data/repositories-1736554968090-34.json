{
  "metadata": {
    "timestamp": 1736554968090,
    "page": 34,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjQw",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "huggingface/transformers",
      "stars": 137417,
      "defaultBranch": "main",
      "files": [
        {
          "name": ".circleci",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitattributes",
          "type": "blob",
          "size": 0.05,
          "content": "*.py\teol=lf\n*.rst\teol=lf\n*.md\teol=lf\n*.mdx   eol=lf"
        },
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 1.76,
          "content": "# Initially taken from Github's Python gitignore file\n\n# Byte-compiled / optimized / DLL files\n__pycache__/\n*.py[cod]\n*$py.class\n\n# C extensions\n*.so\n\n# tests and logs\ntests/fixtures/cached_*_text.txt\nlogs/\nlightning_logs/\nlang_code_data/\n\n# Distribution / packaging\n.Python\nbuild/\ndevelop-eggs/\ndist/\ndownloads/\neggs/\n.eggs/\nlib/\nlib64/\nparts/\nsdist/\nvar/\nwheels/\n*.egg-info/\n.installed.cfg\n*.egg\nMANIFEST\n\n# PyInstaller\n#  Usually these files are written by a python script from a template\n#  before PyInstaller builds the exe, so as to inject date/other infos into it.\n*.manifest\n*.spec\n\n# Installer logs\npip-log.txt\npip-delete-this-directory.txt\n\n# Unit test / coverage reports\nhtmlcov/\n.tox/\n.nox/\n.coverage\n.coverage.*\n.cache\nnosetests.xml\ncoverage.xml\n*.cover\n.hypothesis/\n.pytest_cache/\n\n# Translations\n*.mo\n*.pot\n\n# Django stuff:\n*.log\nlocal_settings.py\ndb.sqlite3\n\n# Flask stuff:\ninstance/\n.webassets-cache\n\n# Scrapy stuff:\n.scrapy\n\n# Sphinx documentation\ndocs/_build/\n\n# PyBuilder\ntarget/\n\n# Jupyter Notebook\n.ipynb_checkpoints\n\n# IPython\nprofile_default/\nipython_config.py\n\n# pyenv\n.python-version\n\n# celery beat schedule file\ncelerybeat-schedule\n\n# SageMath parsed files\n*.sage.py\n\n# Environments\n.env\n.venv\nenv/\nvenv/\nENV/\nenv.bak/\nvenv.bak/\n\n# Spyder project settings\n.spyderproject\n.spyproject\n\n# Rope project settings\n.ropeproject\n\n# mkdocs documentation\n/site\n\n# mypy\n.mypy_cache/\n.dmypy.json\ndmypy.json\n\n# Pyre type checker\n.pyre/\n\n# vscode\n.vs\n.vscode\n\n# Pycharm\n.idea\n\n# TF code\ntensorflow_code\n\n# Models\nproc_data\n\n# examples\nruns\n/runs_old\n/wandb\n/examples/runs\n/examples/**/*.args\n/examples/rag/sweep\n\n# data\n/data\nserialization_dir\n\n# emacs\n*.*~\ndebug.env\n\n# vim\n.*.swp\n\n#ctags\ntags\n\n# pre-commit\n.pre-commit*\n\n# .lock\n*.lock\n\n# DS_Store (MacOS)\n.DS_Store\n\n# ruff\n.ruff_cache\n"
        },
        {
          "name": "CITATION.cff",
          "type": "blob",
          "size": 2.28,
          "content": "cff-version: \"1.2.0\"\r\ndate-released: 2020-10\r\nmessage: \"If you use this software, please cite it using these metadata.\"\r\ntitle: \"Transformers: State-of-the-Art Natural Language Processing\"\r\nurl: \"https://github.com/huggingface/transformers\"\r\nauthors: \r\n  - family-names: Wolf\r\n    given-names: Thomas\r\n  - family-names: Debut\r\n    given-names: Lysandre\r\n  - family-names: Sanh\r\n    given-names: Victor\r\n  - family-names: Chaumond\r\n    given-names: Julien\r\n  - family-names: Delangue\r\n    given-names: Clement\r\n  - family-names: Moi\r\n    given-names: Anthony\r\n  - family-names: Cistac\r\n    given-names: Perric\r\n  - family-names: Ma\r\n    given-names: Clara\r\n  - family-names: Jernite\r\n    given-names: Yacine\r\n  - family-names: Plu\r\n    given-names: Julien\r\n  - family-names: Xu\r\n    given-names: Canwen\r\n  - family-names: \"Le Scao\"\r\n    given-names: Teven\r\n  - family-names: Gugger\r\n    given-names: Sylvain\r\n  - family-names: Drame\r\n    given-names: Mariama\r\n  - family-names: Lhoest\r\n    given-names: Quentin\r\n  - family-names: Rush\r\n    given-names: \"Alexander M.\"\r\npreferred-citation:\r\n  type: conference-paper\r\n  authors:\r\n  - family-names: Wolf\r\n    given-names: Thomas\r\n  - family-names: Debut\r\n    given-names: Lysandre\r\n  - family-names: Sanh\r\n    given-names: Victor\r\n  - family-names: Chaumond\r\n    given-names: Julien\r\n  - family-names: Delangue\r\n    given-names: Clement\r\n  - family-names: Moi\r\n    given-names: Anthony\r\n  - family-names: Cistac\r\n    given-names: Perric\r\n  - family-names: Ma\r\n    given-names: Clara\r\n  - family-names: Jernite\r\n    given-names: Yacine\r\n  - family-names: Plu\r\n    given-names: Julien\r\n  - family-names: Xu\r\n    given-names: Canwen\r\n  - family-names: \"Le Scao\"\r\n    given-names: Teven\r\n  - family-names: Gugger\r\n    given-names: Sylvain\r\n  - family-names: Drame\r\n    given-names: Mariama\r\n  - family-names: Lhoest\r\n    given-names: Quentin\r\n  - family-names: Rush\r\n    given-names: \"Alexander M.\"\r\n  booktitle: \"Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations\"\r\n  month: 10\r\n  start: 38\r\n  end: 45\r\n  title: \"Transformers: State-of-the-Art Natural Language Processing\"\r\n  year: 2020\r\n  publisher: \"Association for Computational Linguistics\"\r\n  url: \"https://www.aclweb.org/anthology/2020.emnlp-demos.6\"\r\n  address: \"Online\"\r\n"
        },
        {
          "name": "CODEOWNERS",
          "type": "blob",
          "size": 21.43,
          "content": "# Top-level rules are matched only if nothing else matches\n* @Rocketknight1 @ArthurZucker # if no one is pinged based on the other rules, he will do the dispatch\n**.md @stevhliu\ndocs/ @stevhliu\n/benchmark/ @McPatate\n/docker/ @ydshieh @ArthurZucker\n\n# More high-level globs catch cases when specific rules later don't apply\n/src/transformers/models/*/*processing* @molbap @yonigozlan @qubvel\n/src/transformers/models/*/image_processing* @qubvel\n/src/transformers/models/*/image_processing_*_fast* @yonigozlan\n/src/transformers/**/*_tokenization* @ArthurZucker\n\n# Owners of subsections of the library\n/src/transformers/generation/ @gante\n/src/transformers/pipeline/ @Rocketknight1 @yonigozlan\n/src/transformers/integrations/ @SunMarc @MekkCyber @muellerzr\n/src/transformers/quantizers/ @SunMarc @MekkCyber\n/src/transformers/tests/ @ydshieh\n/src/transformers/tests/generation/ @gante\n/src/transformers/models/auto/ @ArthurZucker\n/src/transformers/utils/ @ArthurZucker @Rocketknight1\n/src/transformers/loss/ @ArthurZucker\n/src/transformers/onnx/ @michaelbenayoun\n\n# Specific files come after the sections/globs, so they take priority\n/.circleci/config.yml @ArthurZucker @ydshieh\n/utils/tests_fetcher.py @ydshieh\ntrainer.py @muellerzr @SunMarc\ntrainer_utils.py @muellerzr @SunMarc\n/utils/modular_model_converter.py @Cyrilvallez @ArthurZucker\n\n# Owners of individual models are specific / high priority, and so they come last\n# mod* captures modeling and modular files\n\n# Text models\n/src/transformers/models/albert/mod*_albert* @ArthurZucker\n/src/transformers/models/bamba/mod*_bamba* @ArthurZucker\n/src/transformers/models/bart/mod*_bart* @ArthurZucker\n/src/transformers/models/barthez/mod*_barthez* @ArthurZucker\n/src/transformers/models/bartpho/mod*_bartpho* @ArthurZucker\n/src/transformers/models/bert/mod*_bert* @ArthurZucker\n/src/transformers/models/bert_generation/mod*_bert_generation* @ArthurZucker\n/src/transformers/models/bert_japanese/mod*_bert_japanese* @ArthurZucker\n/src/transformers/models/bertweet/mod*_bertweet* @ArthurZucker\n/src/transformers/models/big_bird/mod*_big_bird* @ArthurZucker\n/src/transformers/models/bigbird_pegasus/mod*_bigbird_pegasus* @ArthurZucker\n/src/transformers/models/biogpt/mod*_biogpt* @ArthurZucker\n/src/transformers/models/blenderbot/mod*_blenderbot* @ArthurZucker\n/src/transformers/models/blenderbot_small/mod*_blenderbot_small* @ArthurZucker\n/src/transformers/models/bloom/mod*_bloom* @ArthurZucker\n/src/transformers/models/bort/mod*_bort* @ArthurZucker\n/src/transformers/models/byt5/mod*_byt5* @ArthurZucker\n/src/transformers/models/camembert/mod*_camembert* @ArthurZucker\n/src/transformers/models/canine/mod*_canine* @ArthurZucker\n/src/transformers/models/codegen/mod*_codegen* @ArthurZucker\n/src/transformers/models/code_llama/mod*_code_llama* @ArthurZucker\n/src/transformers/models/cohere/mod*_cohere* @ArthurZucker\n/src/transformers/models/cohere2/mod*_cohere2* @ArthurZucker\n/src/transformers/models/convbert/mod*_convbert* @ArthurZucker\n/src/transformers/models/cpm/mod*_cpm* @ArthurZucker\n/src/transformers/models/cpmant/mod*_cpmant* @ArthurZucker\n/src/transformers/models/ctrl/mod*_ctrl* @ArthurZucker\n/src/transformers/models/dbrx/mod*_dbrx* @ArthurZucker\n/src/transformers/models/deberta/mod*_deberta* @ArthurZucker\n/src/transformers/models/deberta_v2/mod*_deberta_v2* @ArthurZucker\n/src/transformers/models/dialogpt/mod*_dialogpt* @ArthurZucker\n/src/transformers/models/diffllama/mod*_diffllama* @ArthurZucker\n/src/transformers/models/distilbert/mod*_distilbert* @ArthurZucker\n/src/transformers/models/dpr/mod*_dpr* @ArthurZucker\n/src/transformers/models/electra/mod*_electra* @ArthurZucker\n/src/transformers/models/encoder_decoder/mod*_encoder_decoder* @ArthurZucker\n/src/transformers/models/ernie/mod*_ernie* @ArthurZucker\n/src/transformers/models/ernie_m/mod*_ernie_m* @ArthurZucker\n/src/transformers/models/esm/mod*_esm* @ArthurZucker\n/src/transformers/models/falcon/mod*_falcon* @ArthurZucker\n/src/transformers/models/falcon3/mod*_falcon3* @ArthurZucker\n/src/transformers/models/falcon_mamba/mod*_falcon_mamba* @ArthurZucker\n/src/transformers/models/fastspeech2_conformer/mod*_fastspeech2_conformer* @ArthurZucker\n/src/transformers/models/flan_t5/mod*_flan_t5* @ArthurZucker\n/src/transformers/models/flan_ul2/mod*_flan_ul2* @ArthurZucker\n/src/transformers/models/flaubert/mod*_flaubert* @ArthurZucker\n/src/transformers/models/fnet/mod*_fnet* @ArthurZucker\n/src/transformers/models/fsmt/mod*_fsmt* @ArthurZucker\n/src/transformers/models/funnel/mod*_funnel* @ArthurZucker\n/src/transformers/models/fuyu/mod*_fuyu* @ArthurZucker\n/src/transformers/models/gemma/mod*_gemma* @ArthurZucker\n/src/transformers/models/gemma2/mod*_gemma2* @ArthurZucker\n/src/transformers/models/glm/mod*_glm* @ArthurZucker\n/src/transformers/models/openai_gpt/mod*_openai_gpt* @ArthurZucker\n/src/transformers/models/gpt_neo/mod*_gpt_neo* @ArthurZucker\n/src/transformers/models/gpt_neox/mod*_gpt_neox* @ArthurZucker\n/src/transformers/models/gpt_neox_japanese/mod*_gpt_neox_japanese* @ArthurZucker\n/src/transformers/models/gptj/mod*_gptj* @ArthurZucker\n/src/transformers/models/gpt2/mod*_gpt2* @ArthurZucker\n/src/transformers/models/gpt_bigcode/mod*_gpt_bigcode* @ArthurZucker\n/src/transformers/models/gptsan_japanese/mod*_gptsan_japanese* @ArthurZucker\n/src/transformers/models/gpt_sw3/mod*_gpt_sw3* @ArthurZucker\n/src/transformers/models/granite/mod*_granite* @ArthurZucker\n/src/transformers/models/granitemoe/mod*_granitemoe* @ArthurZucker\n/src/transformers/models/herbert/mod*_herbert* @ArthurZucker\n/src/transformers/models/ibert/mod*_ibert* @ArthurZucker\n/src/transformers/models/jamba/mod*_jamba* @ArthurZucker\n/src/transformers/models/jetmoe/mod*_jetmoe* @ArthurZucker\n/src/transformers/models/jukebox/mod*_jukebox* @ArthurZucker\n/src/transformers/models/led/mod*_led* @ArthurZucker\n/src/transformers/models/llama/mod*_llama* @ArthurZucker @Cyrilvallez \n/src/transformers/models/longformer/mod*_longformer* @ArthurZucker\n/src/transformers/models/longt5/mod*_longt5* @ArthurZucker\n/src/transformers/models/luke/mod*_luke* @ArthurZucker\n/src/transformers/models/m2m_100/mod*_m2m_100* @ArthurZucker\n/src/transformers/models/madlad_400/mod*_madlad_400* @ArthurZucker\n/src/transformers/models/mamba/mod*_mamba* @ArthurZucker\n/src/transformers/models/mamba2/mod*_mamba2* @ArthurZucker\n/src/transformers/models/marian/mod*_marian* @ArthurZucker\n/src/transformers/models/markuplm/mod*_markuplm* @ArthurZucker\n/src/transformers/models/mbart/mod*_mbart* @ArthurZucker\n/src/transformers/models/mega/mod*_mega* @ArthurZucker\n/src/transformers/models/megatron_bert/mod*_megatron_bert* @ArthurZucker\n/src/transformers/models/megatron_gpt2/mod*_megatron_gpt2* @ArthurZucker\n/src/transformers/models/mistral/mod*_mistral* @ArthurZucker\n/src/transformers/models/mixtral/mod*_mixtral* @ArthurZucker\n/src/transformers/models/mluke/mod*_mluke* @ArthurZucker\n/src/transformers/models/mobilebert/mod*_mobilebert* @ArthurZucker\n/src/transformers/models/modernbert/mod*_modernbert* @ArthurZucker\n/src/transformers/models/mpnet/mod*_mpnet* @ArthurZucker\n/src/transformers/models/mpt/mod*_mpt* @ArthurZucker\n/src/transformers/models/mra/mod*_mra* @ArthurZucker\n/src/transformers/models/mt5/mod*_mt5* @ArthurZucker\n/src/transformers/models/mvp/mod*_mvp* @ArthurZucker\n/src/transformers/models/myt5/mod*_myt5* @ArthurZucker\n/src/transformers/models/nemotron/mod*_nemotron* @ArthurZucker\n/src/transformers/models/nezha/mod*_nezha* @ArthurZucker\n/src/transformers/models/nllb/mod*_nllb* @ArthurZucker\n/src/transformers/models/nllb_moe/mod*_nllb_moe* @ArthurZucker\n/src/transformers/models/nystromformer/mod*_nystromformer* @ArthurZucker\n/src/transformers/models/olmo/mod*_olmo* @ArthurZucker\n/src/transformers/models/olmo2/mod*_olmo2* @ArthurZucker\n/src/transformers/models/olmoe/mod*_olmoe* @ArthurZucker\n/src/transformers/models/open_llama/mod*_open_llama* @ArthurZucker\n/src/transformers/models/opt/mod*_opt* @ArthurZucker\n/src/transformers/models/pegasus/mod*_pegasus* @ArthurZucker\n/src/transformers/models/pegasus_x/mod*_pegasus_x* @ArthurZucker\n/src/transformers/models/persimmon/mod*_persimmon* @ArthurZucker\n/src/transformers/models/phi/mod*_phi* @ArthurZucker\n/src/transformers/models/phi3/mod*_phi3* @ArthurZucker\n/src/transformers/models/phimoe/mod*_phimoe* @ArthurZucker\n/src/transformers/models/phobert/mod*_phobert* @ArthurZucker\n/src/transformers/models/plbart/mod*_plbart* @ArthurZucker\n/src/transformers/models/prophetnet/mod*_prophetnet* @ArthurZucker\n/src/transformers/models/qdqbert/mod*_qdqbert* @ArthurZucker\n/src/transformers/models/qwen2/mod*_qwen2* @ArthurZucker\n/src/transformers/models/qwen2_moe/mod*_qwen2_moe* @ArthurZucker\n/src/transformers/models/rag/mod*_rag* @ArthurZucker\n/src/transformers/models/realm/mod*_realm* @ArthurZucker\n/src/transformers/models/recurrent_gemma/mod*_recurrent_gemma* @ArthurZucker\n/src/transformers/models/reformer/mod*_reformer* @ArthurZucker\n/src/transformers/models/rembert/mod*_rembert* @ArthurZucker\n/src/transformers/models/retribert/mod*_retribert* @ArthurZucker\n/src/transformers/models/roberta/mod*_roberta* @ArthurZucker\n/src/transformers/models/roberta_prelayernorm/mod*_roberta_prelayernorm* @ArthurZucker\n/src/transformers/models/roc_bert/mod*_roc_bert* @ArthurZucker\n/src/transformers/models/roformer/mod*_roformer* @ArthurZucker\n/src/transformers/models/rwkv/mod*_rwkv* @ArthurZucker\n/src/transformers/models/splinter/mod*_splinter* @ArthurZucker\n/src/transformers/models/squeezebert/mod*_squeezebert* @ArthurZucker\n/src/transformers/models/stablelm/mod*_stablelm* @ArthurZucker\n/src/transformers/models/starcoder2/mod*_starcoder2* @ArthurZucker\n/src/transformers/models/switch_transformers/mod*_switch_transformers* @ArthurZucker\n/src/transformers/models/t5/mod*_t5* @ArthurZucker\n/src/transformers/models/t5v1.1/mod*_t5v1.1* @ArthurZucker\n/src/transformers/models/tapex/mod*_tapex* @ArthurZucker\n/src/transformers/models/transfo_xl/mod*_transfo_xl* @ArthurZucker\n/src/transformers/models/ul2/mod*_ul2* @ArthurZucker\n/src/transformers/models/umt5/mod*_umt5* @ArthurZucker\n/src/transformers/models/xmod/mod*_xmod* @ArthurZucker\n/src/transformers/models/xglm/mod*_xglm* @ArthurZucker\n/src/transformers/models/xlm/mod*_xlm* @ArthurZucker\n/src/transformers/models/xlm_prophetnet/mod*_xlm_prophetnet* @ArthurZucker\n/src/transformers/models/xlm_roberta/mod*_xlm_roberta* @ArthurZucker\n/src/transformers/models/xlm_roberta_xl/mod*_xlm_roberta_xl* @ArthurZucker\n/src/transformers/models/xlm_v/mod*_xlm_v* @ArthurZucker\n/src/transformers/models/xlnet/mod*_xlnet* @ArthurZucker\n/src/transformers/models/yoso/mod*_yoso* @ArthurZucker\n/src/transformers/models/zamba/mod*_zamba* @ArthurZucker\n\n# Vision models\n/src/transformers/models/beit/mod*_beit* @amyeroberts @qubvel\n/src/transformers/models/bit/mod*_bit* @amyeroberts @qubvel\n/src/transformers/models/conditional_detr/mod*_conditional_detr* @amyeroberts @qubvel\n/src/transformers/models/convnext/mod*_convnext* @amyeroberts @qubvel\n/src/transformers/models/convnextv2/mod*_convnextv2* @amyeroberts @qubvel\n/src/transformers/models/cvt/mod*_cvt* @amyeroberts @qubvel\n/src/transformers/models/deformable_detr/mod*_deformable_detr* @amyeroberts @qubvel\n/src/transformers/models/deit/mod*_deit* @amyeroberts @qubvel\n/src/transformers/models/depth_anything/mod*_depth_anything* @amyeroberts @qubvel\n/src/transformers/models/depth_anything_v2/mod*_depth_anything_v2* @amyeroberts @qubvel\n/src/transformers/models/deta/mod*_deta* @amyeroberts @qubvel\n/src/transformers/models/detr/mod*_detr* @amyeroberts @qubvel\n/src/transformers/models/dinat/mod*_dinat* @amyeroberts @qubvel\n/src/transformers/models/dinov2/mod*_dinov2* @amyeroberts @qubvel\n/src/transformers/models/dinov2_with_registers/mod*_dinov2_with_registers* @amyeroberts @qubvel\n/src/transformers/models/dit/mod*_dit* @amyeroberts @qubvel\n/src/transformers/models/dpt/mod*_dpt* @amyeroberts @qubvel\n/src/transformers/models/efficientformer/mod*_efficientformer* @amyeroberts @qubvel\n/src/transformers/models/efficientnet/mod*_efficientnet* @amyeroberts @qubvel\n/src/transformers/models/focalnet/mod*_focalnet* @amyeroberts @qubvel\n/src/transformers/models/glpn/mod*_glpn* @amyeroberts @qubvel\n/src/transformers/models/hiera/mod*_hiera* @amyeroberts @qubvel\n/src/transformers/models/ijepa/mod*_ijepa* @amyeroberts @qubvel\n/src/transformers/models/imagegpt/mod*_imagegpt* @amyeroberts @qubvel\n/src/transformers/models/levit/mod*_levit* @amyeroberts @qubvel\n/src/transformers/models/mask2former/mod*_mask2former* @amyeroberts @qubvel\n/src/transformers/models/maskformer/mod*_maskformer* @amyeroberts @qubvel\n/src/transformers/models/mobilenet_v1/mod*_mobilenet_v1* @amyeroberts @qubvel\n/src/transformers/models/mobilenet_v2/mod*_mobilenet_v2* @amyeroberts @qubvel\n/src/transformers/models/mobilevit/mod*_mobilevit* @amyeroberts @qubvel\n/src/transformers/models/mobilevitv2/mod*_mobilevitv2* @amyeroberts @qubvel\n/src/transformers/models/nat/mod*_nat* @amyeroberts @qubvel\n/src/transformers/models/poolformer/mod*_poolformer* @amyeroberts @qubvel\n/src/transformers/models/pvt/mod*_pvt* @amyeroberts @qubvel\n/src/transformers/models/pvt_v2/mod*_pvt_v2* @amyeroberts @qubvel\n/src/transformers/models/regnet/mod*_regnet* @amyeroberts @qubvel\n/src/transformers/models/resnet/mod*_resnet* @amyeroberts @qubvel\n/src/transformers/models/rt_detr/mod*_rt_detr* @amyeroberts @qubvel\n/src/transformers/models/segformer/mod*_segformer* @amyeroberts @qubvel\n/src/transformers/models/seggpt/mod*_seggpt* @amyeroberts @qubvel\n/src/transformers/models/superpoint/mod*_superpoint* @amyeroberts @qubvel\n/src/transformers/models/swiftformer/mod*_swiftformer* @amyeroberts @qubvel\n/src/transformers/models/swin/mod*_swin* @amyeroberts @qubvel\n/src/transformers/models/swinv2/mod*_swinv2* @amyeroberts @qubvel\n/src/transformers/models/swin2sr/mod*_swin2sr* @amyeroberts @qubvel\n/src/transformers/models/table_transformer/mod*_table_transformer* @amyeroberts @qubvel\n/src/transformers/models/textnet/mod*_textnet* @amyeroberts @qubvel\n/src/transformers/models/timm_wrapper/mod*_timm_wrapper* @amyeroberts @qubvel\n/src/transformers/models/upernet/mod*_upernet* @amyeroberts @qubvel\n/src/transformers/models/van/mod*_van* @amyeroberts @qubvel\n/src/transformers/models/vit/mod*_vit* @amyeroberts @qubvel\n/src/transformers/models/vit_hybrid/mod*_vit_hybrid* @amyeroberts @qubvel\n/src/transformers/models/vitdet/mod*_vitdet* @amyeroberts @qubvel\n/src/transformers/models/vit_mae/mod*_vit_mae* @amyeroberts @qubvel\n/src/transformers/models/vitmatte/mod*_vitmatte* @amyeroberts @qubvel\n/src/transformers/models/vit_msn/mod*_vit_msn* @amyeroberts @qubvel\n/src/transformers/models/vitpose/mod*_vitpose* @amyeroberts @qubvel\n/src/transformers/models/yolos/mod*_yolos* @amyeroberts @qubvel\n/src/transformers/models/zoedepth/mod*_zoedepth* @amyeroberts @qubvel\n\n# Audio models\n/src/transformers/models/audio_spectrogram_transformer/mod*_audio_spectrogram_transformer* @eustlb\n/src/transformers/models/bark/mod*_bark* @eustlb\n/src/transformers/models/clap/mod*_clap* @eustlb\n/src/transformers/models/dac/mod*_dac* @eustlb\n/src/transformers/models/encodec/mod*_encodec* @eustlb\n/src/transformers/models/hubert/mod*_hubert* @eustlb\n/src/transformers/models/mctct/mod*_mctct* @eustlb\n/src/transformers/models/mimi/mod*_mimi* @eustlb\n/src/transformers/models/mms/mod*_mms* @eustlb\n/src/transformers/models/moshi/mod*_moshi* @eustlb\n/src/transformers/models/musicgen/mod*_musicgen* @eustlb\n/src/transformers/models/musicgen_melody/mod*_musicgen_melody* @eustlb\n/src/transformers/models/pop2piano/mod*_pop2piano* @eustlb\n/src/transformers/models/seamless_m4t/mod*_seamless_m4t* @eustlb\n/src/transformers/models/seamless_m4t_v2/mod*_seamless_m4t_v2* @eustlb\n/src/transformers/models/sew/mod*_sew* @eustlb\n/src/transformers/models/sew_d/mod*_sew_d* @eustlb\n/src/transformers/models/speech_to_text/mod*_speech_to_text* @eustlb\n/src/transformers/models/speech_to_text_2/mod*_speech_to_text_2* @eustlb\n/src/transformers/models/speecht5/mod*_speecht5* @eustlb\n/src/transformers/models/unispeech/mod*_unispeech* @eustlb\n/src/transformers/models/unispeech_sat/mod*_unispeech_sat* @eustlb\n/src/transformers/models/univnet/mod*_univnet* @eustlb\n/src/transformers/models/vits/mod*_vits* @eustlb\n/src/transformers/models/wav2vec2/mod*_wav2vec2* @eustlb\n/src/transformers/models/wav2vec2_bert/mod*_wav2vec2_bert* @eustlb\n/src/transformers/models/wav2vec2_conformer/mod*_wav2vec2_conformer* @eustlb\n/src/transformers/models/wav2vec2_phoneme/mod*_wav2vec2_phoneme* @eustlb\n/src/transformers/models/wavlm/mod*_wavlm* @eustlb\n/src/transformers/models/whisper/mod*_whisper* @eustlb\n/src/transformers/models/xls_r/mod*_xls_r* @eustlb\n/src/transformers/models/xlsr_wav2vec2/mod*_xlsr_wav2vec2* @eustlb\n\n# Video models\n/src/transformers/models/timesformer/mod*_timesformer* @Rocketknight1\n/src/transformers/models/videomae/mod*_videomae* @Rocketknight1\n/src/transformers/models/vivit/mod*_vivit* @Rocketknight1\n\n# Multimodal models\n/src/transformers/models/align/mod*_align* @zucchini-nlp\n/src/transformers/models/altclip/mod*_altclip* @zucchini-nlp\n/src/transformers/models/aria/mod*_aria* @zucchini-nlp\n/src/transformers/models/blip/mod*_blip* @zucchini-nlp\n/src/transformers/models/blip_2/mod*_blip_2* @zucchini-nlp\n/src/transformers/models/bridgetower/mod*_bridgetower* @zucchini-nlp\n/src/transformers/models/bros/mod*_bros* @zucchini-nlp\n/src/transformers/models/chameleon/mod*_chameleon* @zucchini-nlp\n/src/transformers/models/chinese_clip/mod*_chinese_clip* @zucchini-nlp\n/src/transformers/models/clip/mod*_clip* @zucchini-nlp\n/src/transformers/models/clipseg/mod*_clipseg* @zucchini-nlp\n/src/transformers/models/clvp/mod*_clvp* @zucchini-nlp\n/src/transformers/models/colpali/mod*_colpali* @zucchini-nlp @yonigozlan \n/src/transformers/models/data2vec/mod*_data2vec* @zucchini-nlp\n/src/transformers/models/deplot/mod*_deplot* @zucchini-nlp\n/src/transformers/models/donut/mod*_donut* @zucchini-nlp\n/src/transformers/models/flava/mod*_flava* @zucchini-nlp\n/src/transformers/models/git/mod*_git* @zucchini-nlp\n/src/transformers/models/grounding_dino/mod*_grounding_dino* @qubvel\n/src/transformers/models/groupvit/mod*_groupvit* @zucchini-nlp\n/src/transformers/models/idefics/mod*_idefics* @zucchini-nlp\n/src/transformers/models/idefics2/mod*_idefics2* @zucchini-nlp\n/src/transformers/models/idefics3/mod*_idefics3* @zucchini-nlp\n/src/transformers/models/instructblip/mod*_instructblip* @zucchini-nlp\n/src/transformers/models/instructblipvideo/mod*_instructblipvideo* @zucchini-nlp\n/src/transformers/models/kosmos_2/mod*_kosmos_2* @zucchini-nlp\n/src/transformers/models/layoutlm/mod*_layoutlm* @NielsRogge\n/src/transformers/models/layoutlmv2/mod*_layoutlmv2* @NielsRogge\n/src/transformers/models/layoutlmv3/mod*_layoutlmv3* @NielsRogge\n/src/transformers/models/layoutxlm/mod*_layoutxlm* @NielsRogge\n/src/transformers/models/lilt/mod*_lilt* @zucchini-nlp\n/src/transformers/models/llava/mod*_llava* @zucchini-nlp @arthurzucker\n/src/transformers/models/llava_next/mod*_llava_next* @zucchini-nlp\n/src/transformers/models/llava_next_video/mod*_llava_next_video* @zucchini-nlp\n/src/transformers/models/llava_onevision/mod*_llava_onevision* @zucchini-nlp\n/src/transformers/models/lxmert/mod*_lxmert* @zucchini-nlp\n/src/transformers/models/matcha/mod*_matcha* @zucchini-nlp\n/src/transformers/models/mgp_str/mod*_mgp_str* @zucchini-nlp\n/src/transformers/models/mllama/mod*_mllama* @zucchini-nlp\n/src/transformers/models/nougat/mod*_nougat* @NielsRogge\n/src/transformers/models/omdet_turbo/mod*_omdet_turbo* @qubvel @yonigozlan\n/src/transformers/models/oneformer/mod*_oneformer* @zucchini-nlp\n/src/transformers/models/owlvit/mod*_owlvit* @qubvel\n/src/transformers/models/owlv2/mod*_owlv2* @qubvel\n/src/transformers/models/paligemma/mod*_paligemma* @zucchini-nlp @molbap\n/src/transformers/models/perceiver/mod*_perceiver* @zucchini-nlp\n/src/transformers/models/pix2struct/mod*_pix2struct* @zucchini-nlp\n/src/transformers/models/pixtral/mod*_pixtral* @zucchini-nlp @ArthurZucker \n/src/transformers/models/qwen2_audio/mod*_qwen2_audio* @zucchini-nlp @ArthurZucker \n/src/transformers/models/qwen2_vl/mod*_qwen2_vl* @zucchini-nlp @ArthurZucker \n/src/transformers/models/sam/mod*_sam* @zucchini-nlp @ArthurZucker \n/src/transformers/models/siglip/mod*_siglip* @zucchini-nlp\n/src/transformers/models/speech_encoder_decoder/mod*_speech_encoder_decoder* @zucchini-nlp\n/src/transformers/models/tapas/mod*_tapas* @NielsRogge\n/src/transformers/models/trocr/mod*_trocr* @zucchini-nlp\n/src/transformers/models/tvlt/mod*_tvlt* @zucchini-nlp\n/src/transformers/models/tvp/mod*_tvp* @zucchini-nlp\n/src/transformers/models/udop/mod*_udop* @zucchini-nlp\n/src/transformers/models/video_llava/mod*_video_llava* @zucchini-nlp\n/src/transformers/models/vilt/mod*_vilt* @zucchini-nlp\n/src/transformers/models/vipllava/mod*_vipllava* @zucchini-nlp\n/src/transformers/models/vision_encoder_decoder/mod*_vision_encoder_decoder* @Rocketknight1\n/src/transformers/models/vision_text_dual_encoder/mod*_vision_text_dual_encoder* @Rocketknight1\n/src/transformers/models/visual_bert/mod*_visual_bert* @zucchini-nlp\n/src/transformers/models/xclip/mod*_xclip* @zucchini-nlp\n\n# Reinforcement learning models\n/src/transformers/models/decision_transformer/mod*_decision_transformer* @Rocketknight1\n/src/transformers/models/trajectory_transformer/mod*_trajectory_transformer* @Rocketknight1\n\n# Time series models\n/src/transformers/models/autoformer/mod*_autoformer* @Rocketknight1\n/src/transformers/models/informer/mod*_informer* @Rocketknight1\n/src/transformers/models/patchtsmixer/mod*_patchtsmixer* @Rocketknight1\n/src/transformers/models/patchtst/mod*_patchtst* @Rocketknight1\n/src/transformers/models/time_series_transformer/mod*_time_series_transformer* @Rocketknight1\n\n# Graph models\n/src/transformers/models/graphormer/mod*_graphormer* @clefourrier\n\n# Finally, files with no owners that shouldn't generate pings, usually automatically generated and checked in the CI\nutils/dummy*"
        },
        {
          "name": "CODE_OF_CONDUCT.md",
          "type": "blob",
          "size": 5.36,
          "content": "\n# Contributor Covenant Code of Conduct\n\n## Our Pledge\n\nWe as members, contributors, and leaders pledge to make participation in our\ncommunity a harassment-free experience for everyone, regardless of age, body\nsize, visible or invisible disability, ethnicity, sex characteristics, gender\nidentity and expression, level of experience, education, socio-economic status,\nnationality, personal appearance, race, caste, color, religion, or sexual\nidentity and orientation.\n\nWe pledge to act and interact in ways that contribute to an open, welcoming,\ndiverse, inclusive, and healthy community.\n\n## Our Standards\n\nExamples of behavior that contributes to a positive environment for our\ncommunity include:\n\n* Demonstrating empathy and kindness toward other people\n* Being respectful of differing opinions, viewpoints, and experiences\n* Giving and gracefully accepting constructive feedback\n* Accepting responsibility and apologizing to those affected by our mistakes,\n  and learning from the experience\n* Focusing on what is best not just for us as individuals, but for the overall\n  community\n\nExamples of unacceptable behavior include:\n\n* The use of sexualized language or imagery, and sexual attention or advances of\n  any kind\n* Trolling, insulting or derogatory comments, and personal or political attacks\n* Public or private harassment\n* Publishing others' private information, such as a physical or email address,\n  without their explicit permission\n* Other conduct which could reasonably be considered inappropriate in a\n  professional setting\n\n## Enforcement Responsibilities\n\nCommunity leaders are responsible for clarifying and enforcing our standards of\nacceptable behavior and will take appropriate and fair corrective action in\nresponse to any behavior that they deem inappropriate, threatening, offensive,\nor harmful.\n\nCommunity leaders have the right and responsibility to remove, edit, or reject\ncomments, commits, code, wiki edits, issues, and other contributions that are\nnot aligned to this Code of Conduct, and will communicate reasons for moderation\ndecisions when appropriate.\n\n## Scope\n\nThis Code of Conduct applies within all community spaces, and also applies when\nan individual is officially representing the community in public spaces.\nExamples of representing our community include using an official e-mail address,\nposting via an official social media account, or acting as an appointed\nrepresentative at an online or offline event.\n\n## Enforcement\n\nInstances of abusive, harassing, or otherwise unacceptable behavior may be\nreported to the community leaders responsible for enforcement at\nfeedback@huggingface.co.\nAll complaints will be reviewed and investigated promptly and fairly.\n\nAll community leaders are obligated to respect the privacy and security of the\nreporter of any incident.\n\n## Enforcement Guidelines\n\nCommunity leaders will follow these Community Impact Guidelines in determining\nthe consequences for any action they deem in violation of this Code of Conduct:\n\n### 1. Correction\n\n**Community Impact**: Use of inappropriate language or other behavior deemed\nunprofessional or unwelcome in the community.\n\n**Consequence**: A private, written warning from community leaders, providing\nclarity around the nature of the violation and an explanation of why the\nbehavior was inappropriate. A public apology may be requested.\n\n### 2. Warning\n\n**Community Impact**: A violation through a single incident or series of\nactions.\n\n**Consequence**: A warning with consequences for continued behavior. No\ninteraction with the people involved, including unsolicited interaction with\nthose enforcing the Code of Conduct, for a specified period of time. This\nincludes avoiding interactions in community spaces as well as external channels\nlike social media. Violating these terms may lead to a temporary or permanent\nban.\n\n### 3. Temporary Ban\n\n**Community Impact**: A serious violation of community standards, including\nsustained inappropriate behavior.\n\n**Consequence**: A temporary ban from any sort of interaction or public\ncommunication with the community for a specified period of time. No public or\nprivate interaction with the people involved, including unsolicited interaction\nwith those enforcing the Code of Conduct, is allowed during this period.\nViolating these terms may lead to a permanent ban.\n\n### 4. Permanent Ban\n\n**Community Impact**: Demonstrating a pattern of violation of community\nstandards, including sustained inappropriate behavior, harassment of an\nindividual, or aggression toward or disparagement of classes of individuals.\n\n**Consequence**: A permanent ban from any sort of public interaction within the\ncommunity.\n\n## Attribution\n\nThis Code of Conduct is adapted from the [Contributor Covenant][homepage],\nversion 2.1, available at\n[https://www.contributor-covenant.org/version/2/1/code_of_conduct.html][v2.1].\n\nCommunity Impact Guidelines were inspired by\n[Mozilla's code of conduct enforcement ladder][Mozilla CoC].\n\nFor answers to common questions about this code of conduct, see the FAQ at\n[https://www.contributor-covenant.org/faq][FAQ]. Translations are available at\n[https://www.contributor-covenant.org/translations][translations].\n\n[homepage]: https://www.contributor-covenant.org\n[v2.1]: https://www.contributor-covenant.org/version/2/1/code_of_conduct.html\n[Mozilla CoC]: https://github.com/mozilla/diversity\n[FAQ]: https://www.contributor-covenant.org/faq\n[translations]: https://www.contributor-covenant.org/translations\n"
        },
        {
          "name": "CONTRIBUTING.md",
          "type": "blob",
          "size": 18.47,
          "content": "<!---\nCopyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n-->\n\n# Contribute to 🤗 Transformers\n\nEveryone is welcome to contribute, and we value everybody's contribution. Code\ncontributions are not the only way to help the community. Answering questions, helping\nothers, and improving the documentation are also immensely valuable.\n\nIt also helps us if you spread the word! Reference the library in blog posts\nabout the awesome projects it made possible, shout out on Twitter every time it has\nhelped you, or simply ⭐️ the repository to say thank you.\n\nHowever you choose to contribute, please be mindful and respect our\n[code of conduct](https://github.com/huggingface/transformers/blob/main/CODE_OF_CONDUCT.md).\n\n**This guide was heavily inspired by the awesome [scikit-learn guide to contributing](https://github.com/scikit-learn/scikit-learn/blob/main/CONTRIBUTING.md).**\n\n## Ways to contribute\n\nThere are several ways you can contribute to 🤗 Transformers:\n\n* Fix outstanding issues with the existing code.\n* Submit issues related to bugs or desired new features.\n* Implement new models.\n* Contribute to the examples or to the documentation.\n\nIf you don't know where to start, there is a special [Good First\nIssue](https://github.com/huggingface/transformers/contribute) listing. It will give you a list of\nopen issues that are beginner-friendly and help you start contributing to open-source. The best way to do that is to open a Pull Request and link it to the issue that you'd like to work on. We try to give priority to opened PRs as we can easily track the progress of the fix, and if the contributor does not have time anymore, someone else can take the PR over.\n\nFor something slightly more challenging, you can also take a look at the [Good Second Issue](https://github.com/huggingface/transformers/labels/Good%20Second%20Issue) list. In general though, if you feel like you know what you're doing, go for it and we'll help you get there! 🚀\n\n> All contributions are equally valuable to the community. 🥰\n\n## Fixing outstanding issues\n\nIf you notice an issue with the existing code and have a fix in mind, feel free to [start contributing](#create-a-pull-request) and open a Pull Request!\n\n## Submitting a bug-related issue or feature request\n\nDo your best to follow these guidelines when submitting a bug-related issue or a feature\nrequest. It will make it easier for us to come back to you quickly and with good\nfeedback.\n\n### Did you find a bug?\n\nThe 🤗 Transformers library is robust and reliable thanks to users who report the problems they encounter.\n\nBefore you report an issue, we would really appreciate it if you could **make sure the bug was not\nalready reported** (use the search bar on GitHub under Issues). Your issue should also be related to bugs in the library itself, and not your code. If you're unsure whether the bug is in your code or the library, please ask in the [forum](https://discuss.huggingface.co/) or on our [discord](https://discord.com/invite/hugging-face-879548962464493619) first. This helps us respond quicker to fixing issues related to the library versus general questions.\n\n> [!TIP]\n> We have a [docs bot](https://huggingface.co/spaces/huggingchat/hf-docs-chat), and we highly encourage you to ask all your questions there. There is always a chance your bug can be fixed with a simple flag 👾🔫\n\nOnce you've confirmed the bug hasn't already been reported, please include the following information in your issue so we can quickly resolve it:\n\n* Your **OS type and version** and **Python**, **PyTorch** and\n  **TensorFlow** versions when applicable.\n* A short, self-contained, code snippet that allows us to reproduce the bug in\n  less than 30s.\n* The *full* traceback if an exception is raised.\n* Attach any other additional information, like screenshots, you think may help.\n\nTo get the OS and software versions automatically, run the following command:\n\n```bash\ntransformers-cli env\n```\n\nYou can also run the same command from the root of the repository:\n\n```bash\npython src/transformers/commands/transformers_cli.py env\n```\n\n### Do you want a new feature?\n\nIf there is a new feature you'd like to see in 🤗 Transformers, please open an issue and describe:\n\n1. What is the *motivation* behind this feature? Is it related to a problem or frustration with the library? Is it a feature related to something you need for a project? Is it something you worked on and think it could benefit the community?\n\n   Whatever it is, we'd love to hear about it!\n\n2. Describe your requested feature in as much detail as possible. The more you can tell us about it, the better we'll be able to help you.\n3. Provide a *code snippet* that demonstrates the features usage.\n4. If the feature is related to a paper, please include a link.\n\nIf your issue is well written we're already 80% of the way there by the time you create it.\n\nWe have added [templates](https://github.com/huggingface/transformers/tree/main/templates) to help you get started with your issue.\n\n## Do you want to implement a new model?\n\nNew models are constantly released and if you want to implement a new model, please provide the following information:\n\n* A short description of the model and a link to the paper.\n* Link to the implementation if it is open-sourced.\n* Link to the model weights if they are available.\n\nIf you are willing to contribute the model yourself, let us know so we can help you add it to 🤗 Transformers!\n\nWe have a technical guide for [how to add a model to 🤗 Transformers](https://huggingface.co/docs/transformers/add_new_model).\n\n## Do you want to add documentation?\n\nWe're always looking for improvements to the documentation that make it more clear and accurate. Please let us know how the documentation can be improved such as typos and any content that is missing, unclear or inaccurate. We'll be happy to make the changes or help you make a contribution if you're interested!\n\nFor more details about how to generate, build, and write the documentation, take a look at the documentation [README](https://github.com/huggingface/transformers/tree/main/docs).\n\n## Create a Pull Request\n\nBefore writing any code, we strongly advise you to search through the existing PRs or\nissues to make sure nobody is already working on the same thing. If you are\nunsure, it is always a good idea to open an issue to get some feedback.\n\nYou will need basic `git` proficiency to contribute to\n🤗 Transformers. While `git` is not the easiest tool to use, it has the greatest\nmanual. Type `git --help` in a shell and enjoy! If you prefer books, [Pro\nGit](https://git-scm.com/book/en/v2) is a very good reference.\n\nYou'll need **[Python 3.9](https://github.com/huggingface/transformers/blob/main/setup.py#L449)** or above to contribute to 🤗 Transformers. Follow the steps below to start contributing:\n\n1. Fork the [repository](https://github.com/huggingface/transformers) by\n   clicking on the **[Fork](https://github.com/huggingface/transformers/fork)** button on the repository's page. This creates a copy of the code\n   under your GitHub user account.\n\n2. Clone your fork to your local disk, and add the base repository as a remote:\n\n   ```bash\n   git clone git@github.com:<your Github handle>/transformers.git\n   cd transformers\n   git remote add upstream https://github.com/huggingface/transformers.git\n   ```\n\n3. Create a new branch to hold your development changes:\n\n   ```bash\n   git checkout -b a-descriptive-name-for-my-changes\n   ```\n\n   🚨 **Do not** work on the `main` branch!\n\n4. Set up a development environment by running the following command in a virtual environment:\n\n   ```bash\n   pip install -e \".[dev]\"\n   ```\n\n   If 🤗 Transformers was already installed in the virtual environment, remove\n   it with `pip uninstall transformers` before reinstalling it in editable\n   mode with the `-e` flag.\n\n   Depending on your OS, and since the number of optional dependencies of Transformers is growing, you might get a\n   failure with this command. If that's the case make sure to install the Deep Learning framework you are working with\n   (PyTorch, TensorFlow and/or Flax) then do:\n\n   ```bash\n   pip install -e \".[quality]\"\n   ```\n\n   which should be enough for most use cases.\n\n5. Develop the features in your branch.\n\n   As you work on your code, you should make sure the test suite\n   passes. Run the tests impacted by your changes like this:\n\n   ```bash\n   pytest tests/<TEST_TO_RUN>.py\n   ```\n\n   For more information about tests, check out the\n   [Testing](https://huggingface.co/docs/transformers/testing) guide.\n\n   🤗 Transformers relies on `black` and `ruff` to format its source code\n   consistently. After you make changes, apply automatic style corrections and code verifications\n   that can't be automated in one go with:\n\n   ```bash\n   make fixup\n   ```\n\n   This target is also optimized to only work with files modified by the PR you're working on.\n\n   If you prefer to run the checks one after the other, the following command applies the\n   style corrections:\n\n   ```bash\n   make style\n   ```\n\n   🤗 Transformers also uses `ruff` and a few custom scripts to check for coding mistakes. Quality\n   controls are run by the CI, but you can run the same checks with:\n\n   ```bash\n   make quality\n   ```\n\n   Finally, we have a lot of scripts to make sure we don't forget to update\n   some files when adding a new model. You can run these scripts with:\n\n   ```bash\n   make repo-consistency\n   ```\n\n   To learn more about those checks and how to fix any issues with them, check out the\n   [Checks on a Pull Request](https://huggingface.co/docs/transformers/pr_checks) guide.\n\n   If you're modifying documents under the `docs/source` directory, make sure the documentation can still be built. This check will also run in the CI when you open a pull request. To run a local check\n   make sure you install the documentation builder:\n\n   ```bash\n   pip install \".[docs]\"\n   ```\n\n   Run the following command from the root of the repository:\n\n   ```bash\n   doc-builder build transformers docs/source/en --build_dir ~/tmp/test-build\n   ```\n\n   This will build the documentation in the `~/tmp/test-build` folder where you can inspect the generated\n   Markdown files with your favorite editor. You can also preview the docs on GitHub when you open a pull request.\n\n   Once you're happy with your changes, add the changed files with `git add` and\n   record your changes locally with `git commit`:\n\n   ```bash\n   git add modified_file.py\n   git commit\n   ```\n\n   Please remember to write [good commit\n   messages](https://chris.beams.io/posts/git-commit/) to clearly communicate the changes you made!\n\n   To keep your copy of the code up to date with the original\n   repository, rebase your branch on `upstream/branch` *before* you open a pull request or if requested by a maintainer:\n\n   ```bash\n   git fetch upstream\n   git rebase upstream/main\n   ```\n\n   Push your changes to your branch:\n\n   ```bash\n   git push -u origin a-descriptive-name-for-my-changes\n   ```\n\n   If you've already opened a pull request, you'll need to force push with the `--force` flag. Otherwise, if the pull request hasn't been opened yet, you can just push your changes normally.\n\n6. Now you can go to your fork of the repository on GitHub and click on **Pull Request** to open a pull request. Make sure you tick off all the boxes on our [checklist](#pull-request-checklist) below. When you're ready, you can send your changes to the project maintainers for review.\n\n7. It's ok if maintainers request changes, it happens to our core contributors\n   too! So everyone can see the changes in the pull request, work in your local\n   branch and push the changes to your fork. They will automatically appear in\n   the pull request.\n\n### Pull request checklist\n\n☐ The pull request title should summarize your contribution.<br>\n☐ If your pull request addresses an issue, please mention the issue number in the pull\nrequest description to make sure they are linked (and people viewing the issue know you\nare working on it).<br>\n☐ To indicate a work in progress please prefix the title with `[WIP]`. These are\nuseful to avoid duplicated work, and to differentiate it from PRs ready to be merged.<br>\n☐ Make sure existing tests pass.<br>\n☐ If adding a new feature, also add tests for it.<br>\n   - If you are adding a new model, make sure you use\n     `ModelTester.all_model_classes = (MyModel, MyModelWithLMHead,...)` to trigger the common tests.\n   - If you are adding new `@slow` tests, make sure they pass using\n     `RUN_SLOW=1 python -m pytest tests/models/my_new_model/test_my_new_model.py`.\n   - If you are adding a new tokenizer, write tests and make sure\n     `RUN_SLOW=1 python -m pytest tests/models/{your_model_name}/test_tokenization_{your_model_name}.py` passes.\n   - CircleCI does not run the slow tests, but GitHub Actions does every night!<br>\n\n☐ All public methods must have informative docstrings (see\n[`modeling_bert.py`](https://github.com/huggingface/transformers/blob/main/src/transformers/models/bert/modeling_bert.py)\nfor an example).<br>\n☐ Due to the rapidly growing repository, don't add any images, videos and other\nnon-text files that'll significantly weigh down the repository. Instead, use a Hub\nrepository such as [`hf-internal-testing`](https://huggingface.co/hf-internal-testing)\nto host these files and reference them by URL. We recommend placing documentation\nrelated images in the following repository:\n[huggingface/documentation-images](https://huggingface.co/datasets/huggingface/documentation-images).\nYou can open a PR on this dataset repository and ask a Hugging Face member to merge it.\n\nFor more information about the checks run on a pull request, take a look at our [Checks on a Pull Request](https://huggingface.co/docs/transformers/pr_checks) guide.\n\n### Tests\n\nAn extensive test suite is included to test the library behavior and several examples. Library tests can be found in\nthe [tests](https://github.com/huggingface/transformers/tree/main/tests) folder and examples tests in the\n[examples](https://github.com/huggingface/transformers/tree/main/examples) folder.\n\nWe like `pytest` and `pytest-xdist` because it's faster. From the root of the\nrepository, specify a *path to a subfolder or a test file* to run the test:\n\n```bash\npython -m pytest -n auto --dist=loadfile -s -v ./tests/models/my_new_model\n```\n\nSimilarly, for the `examples` directory, specify a *path to a subfolder or test file* to run the test. For example, the following command tests the text classification subfolder in the PyTorch `examples` directory:\n\n```bash\npip install -r examples/xxx/requirements.txt  # only needed the first time\npython -m pytest -n auto --dist=loadfile -s -v ./examples/pytorch/text-classification\n```\n\nIn fact, this is actually how our `make test` and `make test-examples` commands are implemented (not including the `pip install`)!\n\nYou can also specify a smaller set of tests in order to test only the feature\nyou're working on.\n\nBy default, slow tests are skipped but you can set the `RUN_SLOW` environment variable to\n`yes` to run them. This will download many gigabytes of models so make sure you\nhave enough disk space, a good internet connection or a lot of patience!\n\n<Tip warning={true}>\n\nRemember to specify a *path to a subfolder or a test file* to run the test. Otherwise, you'll run all the tests in the `tests` or `examples` folder, which will take a very long time!\n\n</Tip>\n\n```bash\nRUN_SLOW=yes python -m pytest -n auto --dist=loadfile -s -v ./tests/models/my_new_model\nRUN_SLOW=yes python -m pytest -n auto --dist=loadfile -s -v ./examples/pytorch/text-classification\n```\n\nLike the slow tests, there are other environment variables available which are not enabled by default during testing:\n- `RUN_CUSTOM_TOKENIZERS`: Enables tests for custom tokenizers.\n- `RUN_PT_FLAX_CROSS_TESTS`: Enables tests for PyTorch + Flax integration.\n- `RUN_PT_TF_CROSS_TESTS`: Enables tests for TensorFlow + PyTorch integration.\n\nMore environment variables and additional information can be found in the [testing_utils.py](https://github.com/huggingface/transformers/blob/main/src/transformers/testing_utils.py).\n\n🤗 Transformers uses `pytest` as a test runner only. It doesn't use any\n`pytest`-specific features in the test suite itself.\n\nThis means `unittest` is fully supported. Here's how to run tests with\n`unittest`:\n\n```bash\npython -m unittest discover -s tests -t . -v\npython -m unittest discover -s examples -t examples -v\n```\n\n### Style guide\n\nFor documentation strings, 🤗 Transformers follows the [Google Python Style Guide](https://google.github.io/styleguide/pyguide.html).\nCheck our [documentation writing guide](https://github.com/huggingface/transformers/tree/main/docs#writing-documentation---specification)\nfor more information.\n\n### Develop on Windows\n\nOn Windows (unless you're working in [Windows Subsystem for Linux](https://learn.microsoft.com/en-us/windows/wsl/) or WSL), you need to configure git to transform Windows `CRLF` line endings to Linux `LF` line endings:\n\n```bash\ngit config core.autocrlf input\n```\n\nOne way to run the `make` command on Windows is with MSYS2:\n\n1. [Download MSYS2](https://www.msys2.org/), and we assume it's installed in `C:\\msys64`.\n2. Open the command line `C:\\msys64\\msys2.exe` (it should be available from the **Start** menu).\n3. Run in the shell: `pacman -Syu` and install `make` with `pacman -S make`.\n4. Add `C:\\msys64\\usr\\bin` to your PATH environment variable.\n\nYou can now use `make` from any terminal (PowerShell, cmd.exe, etc.)! 🎉\n\n### Sync a forked repository with upstream main (the Hugging Face repository)\n\nWhen updating the main branch of a forked repository, please follow these steps to avoid pinging the upstream repository which adds reference notes to each upstream PR, and sends unnecessary notifications to the developers involved in these PRs.\n\n1. When possible, avoid syncing with the upstream using a branch and PR on the forked repository. Instead, merge directly into the forked main.\n2. If a PR is absolutely necessary, use the following steps after checking out your branch:\n\n   ```bash\n   git checkout -b your-branch-for-syncing\n   git pull --squash --no-commit upstream main\n   git commit -m '<your message without GitHub references>'\n   git push --set-upstream origin your-branch-for-syncing\n   ```\n"
        },
        {
          "name": "ISSUES.md",
          "type": "blob",
          "size": 18.37,
          "content": "<!---\nCopyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n-->\n\n# How To Request Support\n\nThis is an Open Source Project so please be mindful that like in any other project of this kind there is no obligation to answer all requests for help.\n\nHowever, we want to encourage you to ask for help whenever you think it's needed! We are happy about every question we get because it allows us to better understand your needs, possible misunderstandings, and most importantly a way for you to help us make this library better. That being said, this document's main purpose is to provide guidelines at how you can formulate your requests to increase your chances to be understood and to get support.\n\nThere are two main venues to receive support: [the forums](https://discuss.huggingface.co/) and [the GitHub issues](https://github.com/huggingface/transformers/issues).\n\n## The Forums\n\n[The user forums](https://discuss.huggingface.co/) are supported by the wide community of the library users and backed up by developers when needed.\n\nIf you have a difficulty with deploying this library or some questions, or you'd like to discuss a new feature, please first consider discussing those things at the forums. Only when you feel your subject matter has been crystalized and you still need support from the library developers do proceed to file an [issue](https://github.com/huggingface/transformers/issues).\n\nIn particular all \"Please explain\" questions or objectively very user-specific feature requests belong to the forums. Here are some example of such questions:\n\n* \"I would like to use a BertModel within a RL-Agent for a customer support service. How can I use a BertForMaskedLM in my ChatBotModel?\"\n\n* \"Could you please explain why T5 has no positional embedding matrix under T5Model?\"\n\n* \"How should I set my generation parameters for translation?\"\n\n* \"How to train T5 on De->En translation?\"\n\n\n## The GitHub Issues\n\nEverything which hints at a bug should be opened as an [issue](https://github.com/huggingface/transformers/issues).\n\nYou are not required to read the following guidelines before opening an issue. However, if you notice that your issue doesn't get any replies, chances are that the developers have one or several difficulties with its quality. In this case, reading the following points and adjusting your issue accordingly could help.\n\n1. Before posting an issue, first search for already posted issues, since chances are someone has already asked a similar question before you.\n\n    If you use Google your search query should be:\n\n    ```\n    \"huggingface\" \"transformers\" your query\n    ```\n\n    The first two quoted words tell Google to limit the search to the context of the Huggingface Transformers. The remainder is your query - most commonly this would be the error message the software fails with. We will go deeper into details shortly.\n\n    The results of such a query will typically match GitHub issues, Hugging Face forums, StackExchange, and blogs.\n\n    If you find relevant hints, you may choose to continue the discussion there if you have follow up questions.\n\n    If what you found is similar but doesn't quite answer your problem, please, post a new issue and do include links to similar issues or forum discussions you may have found.\n\n    Let's look at some examples:\n\n    The error message, often referred to as an assertion, tells us what went wrong. Here is an example of an assertion:\n\n   ```python\n   Traceback (most recent call last):\n     File \"<string>\", line 1, in <module>\n     File \"/transformers/src/transformers/__init__.py\", line 34, in <module>\n       from . import dependency_versions_check\n     File \"/transformers/src/transformers/dependency_versions_check.py\", line 34, in <module>\n       from .utils import is_tokenizers_available\n     File \"/transformers/src/transformers/utils/import_utils.py\", line 40, in <module>\n       from tqdm.auto import tqdm\n    ModuleNotFoundError: No module named 'tqdm.auto'\n    ```\n\n   and it typically includes a traceback, so that we can see the full stack of calls the program made before it fails. This gives us the context to know why the program failed.\n\n   Going back to the above example. If you received this error search, look at the very last line of the error which is:\n\n   ```python\n    ModuleNotFoundError: No module named 'tqdm.auto'\n    ```\n\n    And now we can use it to do the searching on your favorite search engine:\n\n    1. first for `\"huggingface\" \"transformers\" \"ModuleNotFoundError: No module named 'tqdm.auto'\"`\n    2. if you don't find relevant results, then search for just `\"ModuleNotFoundError: No module named 'tqdm.auto'\"`\n    3. and finally if nothing still comes up, then remove the outside quotes: `ModuleNotFoundError: No module named 'tqdm.auto'`\n\n   If the error includes any messages that include bits unique to your filesystem, always remove those in the search query since other users will not have the same filesystem as yours. For example:\n\n   ```bash\n   python -c 'open(\"/tmp/wrong_path.txt\", \"r\")'\n   Traceback (most recent call last):\n     File \"<string>\", line 1, in <module>\n   FileNotFoundError: [Errno 2] No such file or directory: '/tmp/wrong_path.txt'\n   ```\n   Here you'd search for just: `\"FileNotFoundError: [Errno 2] No such file or directory\"`\n\n   If the local information that you removed were inside the error message and you removed them you may need to remove double quotes since your query is no longer exact. So if the error message was something like:\n\n   ```bash\n      ValueError: '/tmp/wrong_path.txt' cannot be found\n   ```\n\n   then you'd search for `\"ValueError\" \"cannot be found\"`\n\n   As you search you will notice that when you don't use quotes often the search engines will return a variety of unrelated hits, which may or may not be what you want.\n\n   Experiment with different ways and find which approach gives the most satisfactory results.\n\n2. Keep the issue short, providing the information that you think will aid the developers to understand your situation. Put yourself in the shoes of the person who has never seen your code or knows anything about your custom setup. This mental exercise will help to develop an intuition to what/what not to share\"\n\n3. If there is a software failure, always provide the full traceback, for example:\n\n   ```python\n   $ python -c 'import transformers'\n   Traceback (most recent call last):\n     File \"<string>\", line 1, in <module>\n     File \"/transformers/src/transformers/__init__.py\", line 34, in <module>\n       from . import dependency_versions_check\n     File \"/transformers/src/transformers/dependency_versions_check.py\", line 34, in <module>\n       from .utils import is_tokenizers_available\n     File \"/transformers/src/transformers/utils/import_utils.py\", line 40, in <module>\n       from tqdm.auto import tqdm\n   ModuleNotFoundError: No module named 'tqdm.auto'\n   ```\n\n   As compared to providing just the last line of the error message, e.g.:\n   ```python\n   ModuleNotFoundError: No module named 'tqdm.auto'\n   ```\n   which is not sufficient.\n\n   If your application is running on more than one GPU (e.g. under `DistributedDataParallel`) and typically getting every log and traceback printed multiple times, please make sure that you paste only one copy of it. At times the traceback from parallel processes may get interleaved - so either disentangle these or change the loggers to log only for `local_rank==0` so that only one process logs things.\n\n4. When quoting a traceback, command line instructions and any type of code always enclose it in triple backticks inside the editor window, that is:\n\n   ````\n   ```\n   git clone https://github.com/huggingface/transformers\n   cd transformers\n   pip install .\n   ```\n   ````\n\n   If it's a command line with a long argument list, please consider breaking it down using backslashes and new lines. Here is an example of a good command line quote:\n\n   ```bash\n    cd examples/seq2seq\n    torchrun --nproc_per_node=2 ./finetune_trainer.py \\\n    --model_name_or_path sshleifer/distill-mbart-en-ro-12-4 --data_dir wmt_en_ro \\\n    --output_dir output_dir --overwrite_output_dir \\\n    --do_train --n_train 500 --num_train_epochs 1 \\\n    --per_device_train_batch_size 1  --freeze_embeds \\\n    --src_lang en_XX --tgt_lang ro_RO --task translation \\\n    --fp16\n   ```\n\n   If you don't break it up, one has to scroll horizontally which often makes it quite difficult to quickly see what's happening.\n\n   The backslashes allow us to copy the command directly into the console to run it, without needing to edit it.\n\n5. Include only the important information that you think will help the developer to quickly identify the problem.\n\n   For example applications often create huge amounts of logs. Ask yourself whether providing all or parts of the log is useful.\n\n   Pasting a 100-1000 lines of log into the issue is an immediate turn off, since it will take a lot of time to figure out where the pertinent parts of the log are.\n\n   Attaching a full log can be helpful if it's done as an attachment, if it's enclosed in the following html code in the comment editor window:\n\n   ```\n   <details>\n   <summary>Full log</summary>\n   <pre>\n\n   many\n   lines\n   go\n   here\n\n   </pre>\n   </details>\n   ```\n\n   which would result in the following entry, which can be opened if desired, but otherwise takes little space.\n\n   <details>\n   <summary>Full log</summary>\n   <pre>\n   many\n   lines\n   go\n   here\n   </pre>\n   </details>\n\n    You could also provide a link to a pastebin service, but this is less beneficial since those links tend to expire quickly and future readers of your issue might not be able to access that log file anymore and may lack some context.\n\n6. If this is an issue in your code, do try to reduce that code to a minimal example that still demonstrates the problem. Please ask at the forums if you have a hard time figuring how to do that. Please realize that we don't have the luxury of having time to try and understand all of your custom code.\n\n   If you really tried to make a short reproducible code but couldn't figure it out, it might be that having a traceback will give the developer enough information to know what's going on. But if it is not enough and we can't reproduce the problem, we can't really solve it.\n\n   Do not despair if you can't figure it out from the beginning, just share what you can and perhaps someone else will be able to help you at the forums.\n\n   If your setup involves any custom datasets, the best way to help us reproduce the problem is to create a [Google Colab notebook](https://colab.research.google.com/) that demonstrates the issue and once you verify that the issue still exists, include a link to that notebook in the Issue. Just make sure that you don't copy and paste the location bar url of the open notebook - as this is private and we won't be able to open it. Instead, you need to click on `Share` in the right upper corner of the notebook, select `Get Link` and then copy and paste the public link it will give to you.\n\n7. If you forked off some of this project's code or example applications, please, do not ask us to go into your code repository and figure out what you may have done. The code is already very complex and unless there is an easy way to do a diff and it's a small diff, it won't be possible to find someone with time on their hands to make a lengthy investigation. Albeit, you might find someone at the forums who will be generous to do this for you.\n\n8. Before reporting an issue, first, always try to update your environment to the latest official version of this library. We have no resources to go and debug older revisions, which could easily have bugs that have been fixed in the latest released version.\n\n   We understand that this is not always possible, especially when APIs change, in which case file an issue against the highest library version your environment can support.\n\n   Of course, if you upgrade the library, always retest that the problem is still there.\n\n9. Please do not ask us to reproduce an issue with your custom data, since we don't have it. So, either you should use some existing dataset supported by HF datasets or you need to supply a code that generates a small sample on the fly, or some another quick and simple way to get it.\n\n   Please do not send us any non-public domain data that may require a license or a permission to be used.\n\n10. Do not tag multiple developers on the issue unless you know this is expected, either because you asked them and they gave you an explicit permission to tag them or the issue template instructs you to do so.\n\n   The \"who to tag for what domain\" part of the issue template is there to help users direct their questions to the right developers who are designated maintainers of project's specific domains. They can then decide at their own discretion to tag other developers if they feel it'd help move the issue forward.\n\n   We currently don't have a triage service and we trust your capacity to identify the right domain and thus the persons to tag in your issue. If you are not sure, please use the forums to ask for guidance.\n\n   When in doubt, err on the side of not tagging a given person. If you tag multiple people out of context or permission don't be surprised if you get no response at all. Please remember that every time you tag someone, they get a notification and you're taking their time without their permission. Please be sensitive to that.\n\n   If you got helped by one of the developers in the past please don't tag them in future issues, unless they are listed in the issue template for the domain you are asking about or that developer gave you an explicit permission to tag them in future issues.\n\n   If you see a certain developer doing multiple and/or recent commits into a specific area of the project that you feel is relevant to your issue, it is not a good reason to tag them. Various developers may be fixing things that prevent them from moving forward, but often their work is focused on a totally different domain. And while they may or may not know how to help you with the problem at hand, it would benefit the whole community much more if they focus on the domain of their unique expertise.\n\n11. Use the Edit button. Take your time, and re-read and improve the wording and formatting to make your posts and comments as easy to understand as possible.\n\n    Avoid posting multiple comments in a row, as each comment generates a notification for the developers tagged in that issue. If you happened to post multiple comments in a row, and nobody followed up yet - consider merging those into one or a few comments while editing the combined content to be coherent.\n\n    If you choose to edit your older comments after others posted follow up comments you need to be aware that your modifications might not be noticed, so if it's not a typo fixing, try to write a new comment flagging that something has been changed in the previous comments.\n\n    For example, the very first comment is the most important one. If while the thread unfolds you realize that things aren't as they seemed to you originally you may want to edit the first post to reflect the up-to-date understanding of the issue at hand so that it helps those who read your issue in the future quickly understand what's going on and not need to sift through dozens of comments. It also helps to indicate that the post was edited. So, those reading the thread later can understand why there might be certain discontinuity in the information flow.\n\n    Use bullets and items if you have lists of items and the outcome improves overall readability.\n\n    Use backticks to refer to class and function names, e.g. `BartModel` and `generate` as these stand out and improve the speed of a reader's comprehension.\n\n    Try not use italics and bold text too much as these often make the text more difficult to read.\n\n\n12. If you are cross-referencing a specific comment in a given thread or another issue, always link to that specific comment, rather than using the issue link. If you do the latter it could be quite impossible to find which specific comment you're referring to.\n\n    To get the link to the specific comment do not copy the url from the location bar of your browser, but instead, click the `...` icon in the upper right corner of the comment and then select \"Copy Link\".\n\n    For example the first link is a link to an issue, and the second to a specific comment in the same issue:\n\n    1. https://github.com/huggingface/transformers/issues/9257\n    2. https://github.com/huggingface/transformers/issues/9257#issuecomment-749945162\n\n\n13. If you are replying to a last comment, it's totally fine to make your reply with just your comment in it. The readers can follow the information flow here.\n\n    But if you're replying to a comment that happened some comments back it's always a good practice to quote just the relevant lines you're replying it. The `>` is used for quoting, or you can always use the menu to do so. For example your editor box will look like:\n\n    ```\n    > How big is your gpu cluster?\n\n    Our cluster is made of 256 gpus.\n    ```\n\n    If you are addressing multiple comments, quote the relevant parts of each before your answer. Some people use the same comment to do multiple replies, others separate them into separate comments. Either way works. The latter approach helps for linking to a specific comment.\n\nIn general the best way to figure out what works the best is learn from issues posted by other people - see which issues get great responses and which get little to no response - observe what the posters who received great responses did differently from those who did not.\n\nThank you for reading this somewhat lengthy document. We would like to conclude that these are not absolute rules, but a friendly advice that will help maximize the chances for us to understand what you are trying to communicate, reproduce the problem then resolve it to your satisfaction and the benefit of the whole community.\n\nIf after reading this document there are remaining questions on how and why or there is a need for further elucidation, please, don't hesitate to ask your question in [this thread](https://discuss.huggingface.co/t/how-to-request-support/3128).\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 11.15,
          "content": "Copyright 2018- The Hugging Face team. All rights reserved.\n\n                                 Apache License\n                           Version 2.0, January 2004\n                        http://www.apache.org/licenses/\n\n   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n\n   1. Definitions.\n\n      \"License\" shall mean the terms and conditions for use, reproduction,\n      and distribution as defined by Sections 1 through 9 of this document.\n\n      \"Licensor\" shall mean the copyright owner or entity authorized by\n      the copyright owner that is granting the License.\n\n      \"Legal Entity\" shall mean the union of the acting entity and all\n      other entities that control, are controlled by, or are under common\n      control with that entity. For the purposes of this definition,\n      \"control\" means (i) the power, direct or indirect, to cause the\n      direction or management of such entity, whether by contract or\n      otherwise, or (ii) ownership of fifty percent (50%) or more of the\n      outstanding shares, or (iii) beneficial ownership of such entity.\n\n      \"You\" (or \"Your\") shall mean an individual or Legal Entity\n      exercising permissions granted by this License.\n\n      \"Source\" form shall mean the preferred form for making modifications,\n      including but not limited to software source code, documentation\n      source, and configuration files.\n\n      \"Object\" form shall mean any form resulting from mechanical\n      transformation or translation of a Source form, including but\n      not limited to compiled object code, generated documentation,\n      and conversions to other media types.\n\n      \"Work\" shall mean the work of authorship, whether in Source or\n      Object form, made available under the License, as indicated by a\n      copyright notice that is included in or attached to the work\n      (an example is provided in the Appendix below).\n\n      \"Derivative Works\" shall mean any work, whether in Source or Object\n      form, that is based on (or derived from) the Work and for which the\n      editorial revisions, annotations, elaborations, or other modifications\n      represent, as a whole, an original work of authorship. For the purposes\n      of this License, Derivative Works shall not include works that remain\n      separable from, or merely link (or bind by name) to the interfaces of,\n      the Work and Derivative Works thereof.\n\n      \"Contribution\" shall mean any work of authorship, including\n      the original version of the Work and any modifications or additions\n      to that Work or Derivative Works thereof, that is intentionally\n      submitted to Licensor for inclusion in the Work by the copyright owner\n      or by an individual or Legal Entity authorized to submit on behalf of\n      the copyright owner. For the purposes of this definition, \"submitted\"\n      means any form of electronic, verbal, or written communication sent\n      to the Licensor or its representatives, including but not limited to\n      communication on electronic mailing lists, source code control systems,\n      and issue tracking systems that are managed by, or on behalf of, the\n      Licensor for the purpose of discussing and improving the Work, but\n      excluding communication that is conspicuously marked or otherwise\n      designated in writing by the copyright owner as \"Not a Contribution.\"\n\n      \"Contributor\" shall mean Licensor and any individual or Legal Entity\n      on behalf of whom a Contribution has been received by Licensor and\n      subsequently incorporated within the Work.\n\n   2. Grant of Copyright License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      copyright license to reproduce, prepare Derivative Works of,\n      publicly display, publicly perform, sublicense, and distribute the\n      Work and such Derivative Works in Source or Object form.\n\n   3. Grant of Patent License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      (except as stated in this section) patent license to make, have made,\n      use, offer to sell, sell, import, and otherwise transfer the Work,\n      where such license applies only to those patent claims licensable\n      by such Contributor that are necessarily infringed by their\n      Contribution(s) alone or by combination of their Contribution(s)\n      with the Work to which such Contribution(s) was submitted. If You\n      institute patent litigation against any entity (including a\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\n      or a Contribution incorporated within the Work constitutes direct\n      or contributory patent infringement, then any patent licenses\n      granted to You under this License for that Work shall terminate\n      as of the date such litigation is filed.\n\n   4. Redistribution. You may reproduce and distribute copies of the\n      Work or Derivative Works thereof in any medium, with or without\n      modifications, and in Source or Object form, provided that You\n      meet the following conditions:\n\n      (a) You must give any other recipients of the Work or\n          Derivative Works a copy of this License; and\n\n      (b) You must cause any modified files to carry prominent notices\n          stating that You changed the files; and\n\n      (c) You must retain, in the Source form of any Derivative Works\n          that You distribute, all copyright, patent, trademark, and\n          attribution notices from the Source form of the Work,\n          excluding those notices that do not pertain to any part of\n          the Derivative Works; and\n\n      (d) If the Work includes a \"NOTICE\" text file as part of its\n          distribution, then any Derivative Works that You distribute must\n          include a readable copy of the attribution notices contained\n          within such NOTICE file, excluding those notices that do not\n          pertain to any part of the Derivative Works, in at least one\n          of the following places: within a NOTICE text file distributed\n          as part of the Derivative Works; within the Source form or\n          documentation, if provided along with the Derivative Works; or,\n          within a display generated by the Derivative Works, if and\n          wherever such third-party notices normally appear. The contents\n          of the NOTICE file are for informational purposes only and\n          do not modify the License. You may add Your own attribution\n          notices within Derivative Works that You distribute, alongside\n          or as an addendum to the NOTICE text from the Work, provided\n          that such additional attribution notices cannot be construed\n          as modifying the License.\n\n      You may add Your own copyright statement to Your modifications and\n      may provide additional or different license terms and conditions\n      for use, reproduction, or distribution of Your modifications, or\n      for any such Derivative Works as a whole, provided Your use,\n      reproduction, and distribution of the Work otherwise complies with\n      the conditions stated in this License.\n\n   5. Submission of Contributions. Unless You explicitly state otherwise,\n      any Contribution intentionally submitted for inclusion in the Work\n      by You to the Licensor shall be under the terms and conditions of\n      this License, without any additional terms or conditions.\n      Notwithstanding the above, nothing herein shall supersede or modify\n      the terms of any separate license agreement you may have executed\n      with Licensor regarding such Contributions.\n\n   6. Trademarks. This License does not grant permission to use the trade\n      names, trademarks, service marks, or product names of the Licensor,\n      except as required for reasonable and customary use in describing the\n      origin of the Work and reproducing the content of the NOTICE file.\n\n   7. Disclaimer of Warranty. Unless required by applicable law or\n      agreed to in writing, Licensor provides the Work (and each\n      Contributor provides its Contributions) on an \"AS IS\" BASIS,\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n      implied, including, without limitation, any warranties or conditions\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n      PARTICULAR PURPOSE. You are solely responsible for determining the\n      appropriateness of using or redistributing the Work and assume any\n      risks associated with Your exercise of permissions under this License.\n\n   8. Limitation of Liability. In no event and under no legal theory,\n      whether in tort (including negligence), contract, or otherwise,\n      unless required by applicable law (such as deliberate and grossly\n      negligent acts) or agreed to in writing, shall any Contributor be\n      liable to You for damages, including any direct, indirect, special,\n      incidental, or consequential damages of any character arising as a\n      result of this License or out of the use or inability to use the\n      Work (including but not limited to damages for loss of goodwill,\n      work stoppage, computer failure or malfunction, or any and all\n      other commercial damages or losses), even if such Contributor\n      has been advised of the possibility of such damages.\n\n   9. Accepting Warranty or Additional Liability. While redistributing\n      the Work or Derivative Works thereof, You may choose to offer,\n      and charge a fee for, acceptance of support, warranty, indemnity,\n      or other liability obligations and/or rights consistent with this\n      License. However, in accepting such obligations, You may act only\n      on Your own behalf and on Your sole responsibility, not on behalf\n      of any other Contributor, and only if You agree to indemnify,\n      defend, and hold each Contributor harmless for any liability\n      incurred by, or claims asserted against, such Contributor by reason\n      of your accepting any such warranty or additional liability.\n\n   END OF TERMS AND CONDITIONS\n\n   APPENDIX: How to apply the Apache License to your work.\n\n      To apply the Apache License to your work, attach the following\n      boilerplate notice, with the fields enclosed by brackets \"[]\"\n      replaced with your own identifying information. (Don't include\n      the brackets!)  The text should be enclosed in the appropriate\n      comment syntax for the file format. We also recommend that a\n      file or class name and description of purpose be included on the\n      same \"printed page\" as the copyright notice for easier\n      identification within third-party archives.\n\n   Copyright [yyyy] [name of copyright owner]\n\n   Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.\n"
        },
        {
          "name": "Makefile",
          "type": "blob",
          "size": 4.1,
          "content": ".PHONY: deps_table_update modified_only_fixup extra_style_checks quality style fixup fix-copies test test-examples benchmark\n\n# make sure to test the local checkout in scripts and not the pre-installed one (don't use quotes!)\nexport PYTHONPATH = src\n\ncheck_dirs := examples tests src utils\n\nexclude_folders :=  \"\"\n\nmodified_only_fixup:\n\t$(eval modified_py_files := $(shell python utils/get_modified_files.py $(check_dirs)))\n\t@if test -n \"$(modified_py_files)\"; then \\\n\t\techo \"Checking/fixing $(modified_py_files)\"; \\\n\t\truff check $(modified_py_files) --fix --exclude $(exclude_folders); \\\n\t\truff format $(modified_py_files) --exclude $(exclude_folders);\\\n\telse \\\n\t\techo \"No library .py files were modified\"; \\\n\tfi\n\n# Update src/transformers/dependency_versions_table.py\n\ndeps_table_update:\n\t@python setup.py deps_table_update\n\ndeps_table_check_updated:\n\t@md5sum src/transformers/dependency_versions_table.py > md5sum.saved\n\t@python setup.py deps_table_update\n\t@md5sum -c --quiet md5sum.saved || (printf \"\\nError: the version dependency table is outdated.\\nPlease run 'make fixup' or 'make style' and commit the changes.\\n\\n\" && exit 1)\n\t@rm md5sum.saved\n\n# autogenerating code\n\nautogenerate_code: deps_table_update\n\n# Check that the repo is in a good state\n\nrepo-consistency:\n\tpython utils/check_copies.py\n\tpython utils/check_modular_conversion.py\n\tpython utils/check_table.py\n\tpython utils/check_dummies.py\n\tpython utils/check_repo.py\n\tpython utils/check_inits.py\n\tpython utils/check_config_docstrings.py\n\tpython utils/check_config_attributes.py\n\tpython utils/check_doctest_list.py\n\tpython utils/update_metadata.py --check-only\n\tpython utils/check_docstrings.py\n\tpython utils/check_support_list.py\n\n# this target runs checks on all files\n\nquality:\n\t@python -c \"from transformers import *\" || (echo '🚨 import failed, this means you introduced unprotected imports! 🚨'; exit 1)\n\truff check $(check_dirs) setup.py conftest.py\n\truff format --check $(check_dirs) setup.py conftest.py\n\tpython utils/sort_auto_mappings.py --check_only\n\tpython utils/check_doc_toc.py\n\tpython utils/check_docstrings.py --check_all\n\n\n# Format source code automatically and check is there are any problems left that need manual fixing\n\nextra_style_checks:\n\tpython utils/sort_auto_mappings.py\n\tpython utils/check_doc_toc.py --fix_and_overwrite\n\n# this target runs checks on all files and potentially modifies some of them\n\nstyle:\n\truff check $(check_dirs) setup.py conftest.py --fix --exclude $(exclude_folders)\n\truff format $(check_dirs) setup.py conftest.py --exclude $(exclude_folders)\n\t${MAKE} autogenerate_code\n\t${MAKE} extra_style_checks\n\n# Super fast fix and check target that only works on relevant modified files since the branch was made\n\nfixup: modified_only_fixup extra_style_checks autogenerate_code repo-consistency\n\n# Make marked copies of snippets of codes conform to the original\n\nfix-copies:\n\tpython utils/check_copies.py --fix_and_overwrite\n\tpython utils/check_modular_conversion.py  --fix_and_overwrite\n\tpython utils/check_table.py --fix_and_overwrite\n\tpython utils/check_dummies.py --fix_and_overwrite\n\tpython utils/check_doctest_list.py --fix_and_overwrite\n\tpython utils/check_docstrings.py --fix_and_overwrite\n\n# Run tests for the library\n\ntest:\n\tpython -m pytest -n auto --dist=loadfile -s -v ./tests/\n\n# Run tests for examples\n\ntest-examples:\n\tpython -m pytest -n auto --dist=loadfile -s -v ./examples/pytorch/\n\n# Run benchmark\n\nbenchmark:\n\tpython3 benchmark/benchmark.py --config-dir benchmark/config --config-name generation --commit=diff backend.model=google/gemma-2b backend.cache_implementation=null,static backend.torch_compile=false,true --multirun\n\n# Run tests for SageMaker DLC release\n\ntest-sagemaker: # install sagemaker dependencies in advance with pip install .[sagemaker]\n\tTEST_SAGEMAKER=True python -m pytest -n auto  -s -v ./tests/sagemaker\n\n\n# Release stuff\n\npre-release:\n\tpython utils/release.py\n\npre-patch:\n\tpython utils/release.py --patch\n\npost-release:\n\tpython utils/release.py --post_release\n\npost-patch:\n\tpython utils/release.py --post_release --patch\n\nbuild-release:\n\trm -rf dist\n\trm -rf build\n\tpython setup.py bdist_wheel\n\tpython setup.py sdist\n\tpython utils/check_build.py\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 22.64,
          "content": "<!---\nCopyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n-->\n\n<p align=\"center\">\n  <picture>\n    <source media=\"(prefers-color-scheme: dark)\" srcset=\"https://huggingface.co/datasets/huggingface/documentation-images/raw/main/transformers-logo-dark.svg\">\n    <source media=\"(prefers-color-scheme: light)\" srcset=\"https://huggingface.co/datasets/huggingface/documentation-images/raw/main/transformers-logo-light.svg\">\n    <img alt=\"Hugging Face Transformers Library\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/raw/main/transformers-logo-light.svg\" width=\"352\" height=\"59\" style=\"max-width: 100%;\">\n  </picture>\n  <br/>\n  <br/>\n</p>\n\n<p align=\"center\">\n    <a href=\"https://circleci.com/gh/huggingface/transformers\"><img alt=\"Build\" src=\"https://img.shields.io/circleci/build/github/huggingface/transformers/main\"></a>\n    <a href=\"https://github.com/huggingface/transformers/blob/main/LICENSE\"><img alt=\"GitHub\" src=\"https://img.shields.io/github/license/huggingface/transformers.svg?color=blue\"></a>\n    <a href=\"https://huggingface.co/docs/transformers/index\"><img alt=\"Documentation\" src=\"https://img.shields.io/website/http/huggingface.co/docs/transformers/index.svg?down_color=red&down_message=offline&up_message=online\"></a>\n    <a href=\"https://github.com/huggingface/transformers/releases\"><img alt=\"GitHub release\" src=\"https://img.shields.io/github/release/huggingface/transformers.svg\"></a>\n    <a href=\"https://github.com/huggingface/transformers/blob/main/CODE_OF_CONDUCT.md\"><img alt=\"Contributor Covenant\" src=\"https://img.shields.io/badge/Contributor%20Covenant-v2.0%20adopted-ff69b4.svg\"></a>\n    <a href=\"https://zenodo.org/badge/latestdoi/155220641\"><img src=\"https://zenodo.org/badge/155220641.svg\" alt=\"DOI\"></a>\n</p>\n\n<h4 align=\"center\">\n    <p>\n        <b>English</b> |\n        <a href=\"https://github.com/huggingface/transformers/blob/main/i18n/README_zh-hans.md\">简体中文</a> |\n        <a href=\"https://github.com/huggingface/transformers/blob/main/i18n/README_zh-hant.md\">繁體中文</a> |\n        <a href=\"https://github.com/huggingface/transformers/blob/main/i18n/README_ko.md\">한국어</a> |\n        <a href=\"https://github.com/huggingface/transformers/blob/main/i18n/README_es.md\">Español</a> |\n        <a href=\"https://github.com/huggingface/transformers/blob/main/i18n/README_ja.md\">日本語</a> |\n        <a href=\"https://github.com/huggingface/transformers/blob/main/i18n/README_hd.md\">हिन्दी</a> |\n        <a href=\"https://github.com/huggingface/transformers/blob/main/i18n/README_ru.md\">Русский</a> |\n        <a href=\"https://github.com/huggingface/transformers/blob/main/i18n/README_pt-br.md\">Рortuguês</a> |\n        <a href=\"https://github.com/huggingface/transformers/blob/main/i18n/README_te.md\">తెలుగు</a> |\n        <a href=\"https://github.com/huggingface/transformers/blob/main/i18n/README_fr.md\">Français</a> |\n        <a href=\"https://github.com/huggingface/transformers/blob/main/i18n/README_de.md\">Deutsch</a> |\n        <a href=\"https://github.com/huggingface/transformers/blob/main/i18n/README_vi.md\">Tiếng Việt</a> |\n        <a href=\"https://github.com/huggingface/transformers/blob/main/i18n/README_ar.md\">العربية</a> |\n        <a href=\"https://github.com/huggingface/transformers/blob/main/i18n/README_ur.md\">اردو</a> |\n    </p>\n</h4>\n\n<h3 align=\"center\">\n    <p>State-of-the-art Machine Learning for JAX, PyTorch and TensorFlow</p>\n</h3>\n\n<h3 align=\"center\">\n    <a href=\"https://hf.co/course\"><img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/course_banner.png\"></a>\n</h3>\n\n🤗 Transformers provides thousands of pretrained models to perform tasks on different modalities such as text, vision, and audio.\n\nThese models can be applied on:\n\n* 📝 Text, for tasks like text classification, information extraction, question answering, summarization, translation, and text generation, in over 100 languages.\n* 🖼️ Images, for tasks like image classification, object detection, and segmentation.\n* 🗣️ Audio, for tasks like speech recognition and audio classification.\n\nTransformer models can also perform tasks on **several modalities combined**, such as table question answering, optical character recognition, information extraction from scanned documents, video classification, and visual question answering.\n\n🤗 Transformers provides APIs to quickly download and use those pretrained models on a given text, fine-tune them on your own datasets and then share them with the community on our [model hub](https://huggingface.co/models). At the same time, each python module defining an architecture is fully standalone and can be modified to enable quick research experiments.\n\n🤗 Transformers is backed by the three most popular deep learning libraries — [Jax](https://jax.readthedocs.io/en/latest/), [PyTorch](https://pytorch.org/) and [TensorFlow](https://www.tensorflow.org/) — with a seamless integration between them. It's straightforward to train your models with one before loading them for inference with the other.\n\n## Online demos\n\nYou can test most of our models directly on their pages from the [model hub](https://huggingface.co/models). We also offer [private model hosting, versioning, & an inference API](https://huggingface.co/pricing) for public and private models.\n\nHere are a few examples:\n\nIn Natural Language Processing:\n- [Masked word completion with BERT](https://huggingface.co/google-bert/bert-base-uncased?text=Paris+is+the+%5BMASK%5D+of+France)\n- [Named Entity Recognition with Electra](https://huggingface.co/dbmdz/electra-large-discriminator-finetuned-conll03-english?text=My+name+is+Sarah+and+I+live+in+London+city)\n- [Text generation with Mistral](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2)\n- [Natural Language Inference with RoBERTa](https://huggingface.co/FacebookAI/roberta-large-mnli?text=The+dog+was+lost.+Nobody+lost+any+animal)\n- [Summarization with BART](https://huggingface.co/facebook/bart-large-cnn?text=The+tower+is+324+metres+%281%2C063+ft%29+tall%2C+about+the+same+height+as+an+81-storey+building%2C+and+the+tallest+structure+in+Paris.+Its+base+is+square%2C+measuring+125+metres+%28410+ft%29+on+each+side.+During+its+construction%2C+the+Eiffel+Tower+surpassed+the+Washington+Monument+to+become+the+tallest+man-made+structure+in+the+world%2C+a+title+it+held+for+41+years+until+the+Chrysler+Building+in+New+York+City+was+finished+in+1930.+It+was+the+first+structure+to+reach+a+height+of+300+metres.+Due+to+the+addition+of+a+broadcasting+aerial+at+the+top+of+the+tower+in+1957%2C+it+is+now+taller+than+the+Chrysler+Building+by+5.2+metres+%2817+ft%29.+Excluding+transmitters%2C+the+Eiffel+Tower+is+the+second+tallest+free-standing+structure+in+France+after+the+Millau+Viaduct)\n- [Question answering with DistilBERT](https://huggingface.co/distilbert/distilbert-base-uncased-distilled-squad?text=Which+name+is+also+used+to+describe+the+Amazon+rainforest+in+English%3F&context=The+Amazon+rainforest+%28Portuguese%3A+Floresta+Amaz%C3%B4nica+or+Amaz%C3%B4nia%3B+Spanish%3A+Selva+Amaz%C3%B3nica%2C+Amazon%C3%ADa+or+usually+Amazonia%3B+French%3A+For%C3%AAt+amazonienne%3B+Dutch%3A+Amazoneregenwoud%29%2C+also+known+in+English+as+Amazonia+or+the+Amazon+Jungle%2C+is+a+moist+broadleaf+forest+that+covers+most+of+the+Amazon+basin+of+South+America.+This+basin+encompasses+7%2C000%2C000+square+kilometres+%282%2C700%2C000+sq+mi%29%2C+of+which+5%2C500%2C000+square+kilometres+%282%2C100%2C000+sq+mi%29+are+covered+by+the+rainforest.+This+region+includes+territory+belonging+to+nine+nations.+The+majority+of+the+forest+is+contained+within+Brazil%2C+with+60%25+of+the+rainforest%2C+followed+by+Peru+with+13%25%2C+Colombia+with+10%25%2C+and+with+minor+amounts+in+Venezuela%2C+Ecuador%2C+Bolivia%2C+Guyana%2C+Suriname+and+French+Guiana.+States+or+departments+in+four+nations+contain+%22Amazonas%22+in+their+names.+The+Amazon+represents+over+half+of+the+planet%27s+remaining+rainforests%2C+and+comprises+the+largest+and+most+biodiverse+tract+of+tropical+rainforest+in+the+world%2C+with+an+estimated+390+billion+individual+trees+divided+into+16%2C000+species)\n- [Translation with T5](https://huggingface.co/google-t5/t5-base?text=My+name+is+Wolfgang+and+I+live+in+Berlin)\n\nIn Computer Vision:\n- [Image classification with ViT](https://huggingface.co/google/vit-base-patch16-224)\n- [Object Detection with DETR](https://huggingface.co/facebook/detr-resnet-50)\n- [Semantic Segmentation with SegFormer](https://huggingface.co/nvidia/segformer-b0-finetuned-ade-512-512)\n- [Panoptic Segmentation with Mask2Former](https://huggingface.co/facebook/mask2former-swin-large-coco-panoptic)\n- [Depth Estimation with Depth Anything](https://huggingface.co/docs/transformers/main/model_doc/depth_anything)\n- [Video Classification with VideoMAE](https://huggingface.co/docs/transformers/model_doc/videomae)\n- [Universal Segmentation with OneFormer](https://huggingface.co/shi-labs/oneformer_ade20k_dinat_large)\n\nIn Audio:\n- [Automatic Speech Recognition with Whisper](https://huggingface.co/openai/whisper-large-v3)\n- [Keyword Spotting with Wav2Vec2](https://huggingface.co/superb/wav2vec2-base-superb-ks)\n- [Audio Classification with Audio Spectrogram Transformer](https://huggingface.co/MIT/ast-finetuned-audioset-10-10-0.4593)\n\nIn Multimodal tasks:\n- [Table Question Answering with TAPAS](https://huggingface.co/google/tapas-base-finetuned-wtq)\n- [Visual Question Answering with ViLT](https://huggingface.co/dandelin/vilt-b32-finetuned-vqa)\n- [Image captioning with LLaVa](https://huggingface.co/llava-hf/llava-1.5-7b-hf)\n- [Zero-shot Image Classification with SigLIP](https://huggingface.co/google/siglip-so400m-patch14-384)\n- [Document Question Answering with LayoutLM](https://huggingface.co/impira/layoutlm-document-qa)\n- [Zero-shot Video Classification with X-CLIP](https://huggingface.co/docs/transformers/model_doc/xclip)\n- [Zero-shot Object Detection with OWLv2](https://huggingface.co/docs/transformers/en/model_doc/owlv2)\n- [Zero-shot Image Segmentation with CLIPSeg](https://huggingface.co/docs/transformers/model_doc/clipseg)\n- [Automatic Mask Generation with SAM](https://huggingface.co/docs/transformers/model_doc/sam)\n\n\n## 100 projects using Transformers\n\nTransformers is more than a toolkit to use pretrained models: it's a community of projects built around it and the\nHugging Face Hub. We want Transformers to enable developers, researchers, students, professors, engineers, and anyone\nelse to build their dream projects.\n\nIn order to celebrate the 100,000 stars of transformers, we have decided to put the spotlight on the\ncommunity, and we have created the [awesome-transformers](./awesome-transformers.md) page which lists 100\nincredible projects built in the vicinity of transformers.\n\nIf you own or use a project that you believe should be part of the list, please open a PR to add it!\n\n## Serious about AI in your organisation? Build faster with the Hugging Face Enterprise Hub.\n\n<a target=\"_blank\" href=\"https://huggingface.co/enterprise\">\n    <img alt=\"Hugging Face Enterprise Hub\" src=\"https://github.com/user-attachments/assets/247fb16d-d251-4583-96c4-d3d76dda4925\">\n</a><br>\n\n## Quick tour\n\nTo immediately use a model on a given input (text, image, audio, ...), we provide the `pipeline` API. Pipelines group together a pretrained model with the preprocessing that was used during that model's training. Here is how to quickly use a pipeline to classify positive versus negative texts:\n\n```python\n>>> from transformers import pipeline\n\n# Allocate a pipeline for sentiment-analysis\n>>> classifier = pipeline('sentiment-analysis')\n>>> classifier('We are very happy to introduce pipeline to the transformers repository.')\n[{'label': 'POSITIVE', 'score': 0.9996980428695679}]\n```\n\nThe second line of code downloads and caches the pretrained model used by the pipeline, while the third evaluates it on the given text. Here, the answer is \"positive\" with a confidence of 99.97%.\n\nMany tasks have a pre-trained `pipeline` ready to go, in NLP but also in computer vision and speech. For example, we can easily extract detected objects in an image:\n\n``` python\n>>> import requests\n>>> from PIL import Image\n>>> from transformers import pipeline\n\n# Download an image with cute cats\n>>> url = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/coco_sample.png\"\n>>> image_data = requests.get(url, stream=True).raw\n>>> image = Image.open(image_data)\n\n# Allocate a pipeline for object detection\n>>> object_detector = pipeline('object-detection')\n>>> object_detector(image)\n[{'score': 0.9982201457023621,\n  'label': 'remote',\n  'box': {'xmin': 40, 'ymin': 70, 'xmax': 175, 'ymax': 117}},\n {'score': 0.9960021376609802,\n  'label': 'remote',\n  'box': {'xmin': 333, 'ymin': 72, 'xmax': 368, 'ymax': 187}},\n {'score': 0.9954745173454285,\n  'label': 'couch',\n  'box': {'xmin': 0, 'ymin': 1, 'xmax': 639, 'ymax': 473}},\n {'score': 0.9988006353378296,\n  'label': 'cat',\n  'box': {'xmin': 13, 'ymin': 52, 'xmax': 314, 'ymax': 470}},\n {'score': 0.9986783862113953,\n  'label': 'cat',\n  'box': {'xmin': 345, 'ymin': 23, 'xmax': 640, 'ymax': 368}}]\n```\n\nHere, we get a list of objects detected in the image, with a box surrounding the object and a confidence score. Here is the original image on the left, with the predictions displayed on the right:\n\n<h3 align=\"center\">\n    <a><img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/coco_sample.png\" width=\"400\"></a>\n    <a><img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/coco_sample_post_processed.png\" width=\"400\"></a>\n</h3>\n\nYou can learn more about the tasks supported by the `pipeline` API in [this tutorial](https://huggingface.co/docs/transformers/task_summary).\n\nIn addition to `pipeline`, to download and use any of the pretrained models on your given task, all it takes is three lines of code. Here is the PyTorch version:\n```python\n>>> from transformers import AutoTokenizer, AutoModel\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-uncased\")\n>>> model = AutoModel.from_pretrained(\"google-bert/bert-base-uncased\")\n\n>>> inputs = tokenizer(\"Hello world!\", return_tensors=\"pt\")\n>>> outputs = model(**inputs)\n```\n\nAnd here is the equivalent code for TensorFlow:\n```python\n>>> from transformers import AutoTokenizer, TFAutoModel\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-uncased\")\n>>> model = TFAutoModel.from_pretrained(\"google-bert/bert-base-uncased\")\n\n>>> inputs = tokenizer(\"Hello world!\", return_tensors=\"tf\")\n>>> outputs = model(**inputs)\n```\n\nThe tokenizer is responsible for all the preprocessing the pretrained model expects and can be called directly on a single string (as in the above examples) or a list. It will output a dictionary that you can use in downstream code or simply directly pass to your model using the ** argument unpacking operator.\n\nThe model itself is a regular [Pytorch `nn.Module`](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) or a [TensorFlow `tf.keras.Model`](https://www.tensorflow.org/api_docs/python/tf/keras/Model) (depending on your backend) which you can use as usual. [This tutorial](https://huggingface.co/docs/transformers/training) explains how to integrate such a model into a classic PyTorch or TensorFlow training loop, or how to use our `Trainer` API to quickly fine-tune on a new dataset.\n\n## Why should I use transformers?\n\n1. Easy-to-use state-of-the-art models:\n    - High performance on natural language understanding & generation, computer vision, and audio tasks.\n    - Low barrier to entry for educators and practitioners.\n    - Few user-facing abstractions with just three classes to learn.\n    - A unified API for using all our pretrained models.\n\n1. Lower compute costs, smaller carbon footprint:\n    - Researchers can share trained models instead of always retraining.\n    - Practitioners can reduce compute time and production costs.\n    - Dozens of architectures with over 400,000 pretrained models across all modalities.\n\n1. Choose the right framework for every part of a model's lifetime:\n    - Train state-of-the-art models in 3 lines of code.\n    - Move a single model between TF2.0/PyTorch/JAX frameworks at will.\n    - Seamlessly pick the right framework for training, evaluation, and production.\n\n1. Easily customize a model or an example to your needs:\n    - We provide examples for each architecture to reproduce the results published by its original authors.\n    - Model internals are exposed as consistently as possible.\n    - Model files can be used independently of the library for quick experiments.\n\n## Why shouldn't I use transformers?\n\n- This library is not a modular toolbox of building blocks for neural nets. The code in the model files is not refactored with additional abstractions on purpose, so that researchers can quickly iterate on each of the models without diving into additional abstractions/files.\n- The training API is not intended to work on any model but is optimized to work with the models provided by the library. For generic machine learning loops, you should use another library (possibly, [Accelerate](https://huggingface.co/docs/accelerate)).\n- While we strive to present as many use cases as possible, the scripts in our [examples folder](https://github.com/huggingface/transformers/tree/main/examples) are just that: examples. It is expected that they won't work out-of-the-box on your specific problem and that you will be required to change a few lines of code to adapt them to your needs.\n\n## Installation\n\n### With pip\n\nThis repository is tested on Python 3.9+, Flax 0.4.1+, PyTorch 2.0+, and TensorFlow 2.6+.\n\nYou should install 🤗 Transformers in a [virtual environment](https://docs.python.org/3/library/venv.html). If you're unfamiliar with Python virtual environments, check out the [user guide](https://packaging.python.org/guides/installing-using-pip-and-virtual-environments/).\n\nFirst, create a virtual environment with the version of Python you're going to use and activate it.\n\nThen, you will need to install at least one of Flax, PyTorch, or TensorFlow.\nPlease refer to [TensorFlow installation page](https://www.tensorflow.org/install/), [PyTorch installation page](https://pytorch.org/get-started/locally/#start-locally) and/or [Flax](https://github.com/google/flax#quick-install) and [Jax](https://github.com/google/jax#installation) installation pages regarding the specific installation command for your platform.\n\nWhen one of those backends has been installed, 🤗 Transformers can be installed using pip as follows:\n\n```bash\npip install transformers\n```\n\nIf you'd like to play with the examples or need the bleeding edge of the code and can't wait for a new release, you must [install the library from source](https://huggingface.co/docs/transformers/installation#installing-from-source).\n\n### With conda\n\n🤗 Transformers can be installed using conda as follows:\n\n```shell script\nconda install conda-forge::transformers\n```\n\n> **_NOTE:_** Installing `transformers` from the `huggingface` channel is deprecated.\n\nFollow the installation pages of Flax, PyTorch or TensorFlow to see how to install them with conda.\n\n> **_NOTE:_**  On Windows, you may be prompted to activate Developer Mode in order to benefit from caching. If this is not an option for you, please let us know in [this issue](https://github.com/huggingface/huggingface_hub/issues/1062).\n\n## Model architectures\n\n**[All the model checkpoints](https://huggingface.co/models)** provided by 🤗 Transformers are seamlessly integrated from the huggingface.co [model hub](https://huggingface.co/models), where they are uploaded directly by [users](https://huggingface.co/users) and [organizations](https://huggingface.co/organizations).\n\nCurrent number of checkpoints: ![](https://img.shields.io/endpoint?url=https://huggingface.co/api/shields/models&color=brightgreen)\n\n🤗 Transformers currently provides the following architectures: see [here](https://huggingface.co/docs/transformers/model_summary) for a high-level summary of each them.\n\nTo check if each model has an implementation in Flax, PyTorch or TensorFlow, or has an associated tokenizer backed by the 🤗 Tokenizers library, refer to [this table](https://huggingface.co/docs/transformers/index#supported-frameworks).\n\nThese implementations have been tested on several datasets (see the example scripts) and should match the performance of the original implementations. You can find more details on performance in the Examples section of the [documentation](https://github.com/huggingface/transformers/tree/main/examples).\n\n\n## Learn more\n\n| Section | Description |\n|-|-|\n| [Documentation](https://huggingface.co/docs/transformers/) | Full API documentation and tutorials |\n| [Task summary](https://huggingface.co/docs/transformers/task_summary) | Tasks supported by 🤗 Transformers |\n| [Preprocessing tutorial](https://huggingface.co/docs/transformers/preprocessing) | Using the `Tokenizer` class to prepare data for the models |\n| [Training and fine-tuning](https://huggingface.co/docs/transformers/training) | Using the models provided by 🤗 Transformers in a PyTorch/TensorFlow training loop and the `Trainer` API |\n| [Quick tour: Fine-tuning/usage scripts](https://github.com/huggingface/transformers/tree/main/examples) | Example scripts for fine-tuning models on a wide range of tasks |\n| [Model sharing and uploading](https://huggingface.co/docs/transformers/model_sharing) | Upload and share your fine-tuned models with the community |\n\n## Citation\n\nWe now have a [paper](https://www.aclweb.org/anthology/2020.emnlp-demos.6/) you can cite for the 🤗 Transformers library:\n```bibtex\n@inproceedings{wolf-etal-2020-transformers,\n    title = \"Transformers: State-of-the-Art Natural Language Processing\",\n    author = \"Thomas Wolf and Lysandre Debut and Victor Sanh and Julien Chaumond and Clement Delangue and Anthony Moi and Pierric Cistac and Tim Rault and Rémi Louf and Morgan Funtowicz and Joe Davison and Sam Shleifer and Patrick von Platen and Clara Ma and Yacine Jernite and Julien Plu and Canwen Xu and Teven Le Scao and Sylvain Gugger and Mariama Drame and Quentin Lhoest and Alexander M. Rush\",\n    booktitle = \"Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations\",\n    month = oct,\n    year = \"2020\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://www.aclweb.org/anthology/2020.emnlp-demos.6\",\n    pages = \"38--45\"\n}\n```\n"
        },
        {
          "name": "SECURITY.md",
          "type": "blob",
          "size": 2.31,
          "content": "# Security Policy\n\n## Hugging Face Hub, remote artefacts, and remote code\n\nTransformers is open-source software that is tightly coupled to the Hugging Face Hub. While you have the ability to use it\noffline with pre-downloaded model weights, it provides a very simple way to download, use, and manage models locally.\n\nWhen downloading artefacts that have been uploaded by others on any platform, you expose yourself to risks. Please\nread below for the security recommendations in order to keep your runtime and local environment safe.\n\n### Remote artefacts\n\nModels uploaded on the Hugging Face Hub come in different formats. We heavily recommend uploading and downloading\nmodels in the [`safetensors`](https://github.com/huggingface/safetensors) format (which is the default prioritized\nby the transformers library), as developed specifically to prevent arbitrary code execution on your system.\n\nTo avoid loading models from unsafe formats(e.g. [pickle](https://docs.python.org/3/library/pickle.html), you should use the `use_safetensors` parameter. If doing so, in the event that no .safetensors file is present, transformers will error when loading the model.\n\n### Remote code\n\n#### Modeling\n\nTransformers supports many model architectures, but is also the bridge between your Python runtime and models that\nare stored in model repositories on the Hugging Face Hub.\n\nThese models require the `trust_remote_code=True` parameter to be set when using them; please **always** verify\nthe content of the modeling files when using this argument. We recommend setting a revision in order to ensure you\nprotect yourself from updates on the repository.\n\n#### Tools\n\nThrough the `Agent` framework, remote tools can be downloaded to be used by the Agent. You're to specify these tools\nyourself, but please keep in mind that their code will be run on your machine if the Agent chooses to run them.\n\nPlease inspect the code of the tools before passing them to the Agent to protect your runtime and local setup.\n\n## Reporting a Vulnerability\n\nFeel free to submit vulnerability reports to [security@huggingface.co](mailto:security@huggingface.co), where someone from the HF security team will review and recommend next steps. If reporting a vulnerability specific to open source, please note [Huntr](https://huntr.com) is a vulnerability disclosure program for open source software.\n"
        },
        {
          "name": "awesome-transformers.md",
          "type": "blob",
          "size": 38.19,
          "content": "# Awesome projects built with Transformers\n\nThis page lists awesome projects built on top of Transformers. Transformers is more than a toolkit to use pretrained\nmodels: it's a community of projects built around it and the Hugging Face Hub. We want Transformers to enable\ndevelopers, researchers, students, professors, engineers, and anyone else to build their dream projects.\n\nIn this list, we showcase incredibly impactful and novel projects that have pushed the field forward. We celebrate\n100 of these projects as we reach the milestone of 100k stars as a community; but we're very open to pull requests\nadding other projects to the list. If you believe a project should be here and it's not, then please, open a PR \nto add it.\n\n## [gpt4all](https://github.com/nomic-ai/gpt4all)\n\n[gpt4all](https://github.com/nomic-ai/gpt4all) is an ecosystem of open-source chatbots trained on massive collections of clean assistant data including code, stories and dialogue. It offers open-source, large language models such as LLaMA and GPT-J trained in an assistant-style.\n\nKeywords: Open-source, LLaMa, GPT-J, instruction, assistant\n\n## [recommenders](https://github.com/microsoft/recommenders)\n\nThis repository contains examples and best practices for building recommendation systems, provided as Jupyter notebooks. It goes over several aspects required to build efficient recommendation systems: data preparation, modeling, evaluation, model selection & optimization, as well as operationalization\n\nKeywords: Recommender systems, AzureML\n\n## [IOPaint](https://github.com/Sanster/IOPaint)\n\nImage inpainting tool powered by Stable Diffusion. Remove any unwanted object, defect, people from your pictures or erase and replace anything on your pictures.\n\nKeywords: inpainting, SD, Stable Diffusion\n\n## [flair](https://github.com/flairNLP/flair)\n\nFLAIR is a powerful PyTorch NLP framework, convering several important tasks: NER, sentiment-analysis, part-of-speech tagging, text and document embeddings, among other things.\n\nKeywords: NLP, text embedding, document embedding, biomedical, NER, PoS, sentiment-analysis\n\n## [mindsdb](https://github.com/mindsdb/mindsdb)\n\nMindsDB is a low-code ML platform, which automates and integrates several ML frameworks into the data stack as \"AI Tables\" to streamline the integration of AI into applications, making it accessible to developers of all skill levels.\n\nKeywords: Database, low-code, AI table\n\n## [langchain](https://github.com/hwchase17/langchain)\n\n[langchain](https://github.com/hwchase17/langchain) is aimed at assisting in the development of apps merging both LLMs and other sources of knowledge. The library allows chaining calls to applications, creating a sequence across many tools.\n\nKeywords: LLMs, Large Language Models, Agents, Chains\n\n## [LlamaIndex](https://github.com/jerryjliu/llama_index)\n\n[LlamaIndex](https://github.com/jerryjliu/llama_index) is a project that provides a central interface to connect your LLM's with external data. It provides various kinds of indices and retreival mechanisms to perform different LLM tasks and obtain knowledge-augmented results.\n\nKeywords: LLMs, Large Language Models, Data Retrieval, Indices, Knowledge Augmentation \n\n## [ParlAI](https://github.com/facebookresearch/ParlAI)\n\n[ParlAI](https://github.com/facebookresearch/ParlAI) is a python framework for sharing, training and testing dialogue models, from open-domain chitchat, to task-oriented dialogue, to visual question answering. It provides more than 100 datasets under the same API, a large zoo of pretrained models, a set of agents, and has several integrations.\n\nKeywords: Dialogue, Chatbots, VQA, Datasets, Agents\n\n## [sentence-transformers](https://github.com/UKPLab/sentence-transformers)\n\nThis framework provides an easy method to compute dense vector representations for sentences, paragraphs, and images. The models are based on transformer networks like BERT / RoBERTa / XLM-RoBERTa etc. and achieve state-of-the-art performance in various task. Text is embedding in vector space such that similar text is close and can efficiently be found using cosine similarity.\n\nKeywords: Dense vector representations, Text embeddings, Sentence embeddings\n\n## [ludwig](https://github.com/ludwig-ai/ludwig)\n\nLudwig is a declarative machine learning framework that makes it easy to define machine learning pipelines using a simple and flexible data-driven configuration system. Ludwig is targeted at a wide variety of AI tasks. It provides a data-driven configuration system, training, prediction, and evaluation scripts, as well as a programmatic API.\n\nKeywords: Declarative, Data-driven, ML Framework\n\n## [InvokeAI](https://github.com/invoke-ai/InvokeAI)\n\n[InvokeAI](https://github.com/invoke-ai/InvokeAI) is an engine for Stable Diffusion models, aimed at professionals, artists, and enthusiasts. It leverages the latest AI-driven technologies through CLI as well as a WebUI.\n\nKeywords: Stable-Diffusion, WebUI, CLI\n\n## [PaddleNLP](https://github.com/PaddlePaddle/PaddleNLP)\n\n[PaddleNLP](https://github.com/PaddlePaddle/PaddleNLP) is an easy-to-use and powerful NLP library particularly targeted at the Chinese languages. It has support for multiple pre-trained model zoos, and supports a wide-range of NLP tasks from research to industrial applications.\n\nKeywords: NLP, Chinese, Research, Industry\n\n## [stanza](https://github.com/stanfordnlp/stanza)\n\nThe Stanford NLP Group's official Python NLP library. It contains support for running various accurate natural language processing tools on 60+ languages and for accessing the Java Stanford CoreNLP software from Python.\n\nKeywords: NLP, Multilingual, CoreNLP\n\n## [DeepPavlov](https://github.com/deeppavlov/DeepPavlov)\n\n[DeepPavlov](https://github.com/deeppavlov/DeepPavlov) is an open-source conversational AI library. It is designed for the development of production ready chat-bots and complex conversational systems, as well as research in the area of NLP and, particularly, of dialog systems.\n\nKeywords: Conversational, Chatbot, Dialog\n\n## [alpaca-lora](https://github.com/tloen/alpaca-lora)\n\nAlpaca-lora contains code for reproducing the Stanford Alpaca results using low-rank adaptation (LoRA). The repository provides training (fine-tuning) as well as generation scripts.\n\nKeywords: LoRA, Parameter-efficient fine-tuning\n\n## [imagen-pytorch](https://github.com/lucidrains/imagen-pytorch)\n\nAn open-source Implementation of Imagen, Google's closed-source Text-to-Image Neural Network that beats DALL-E2. As of release, it is the new SOTA for text-to-image synthesis.\n\nKeywords: Imagen, Text-to-image\n\n## [adapters](https://github.com/adapter-hub/adapters)\n\n[adapters](https://github.com/adapter-hub/adapters) is an extension of HuggingFace's Transformers library, integrating adapters into state-of-the-art language models by incorporating AdapterHub, a central repository for pre-trained adapter modules. It is a drop-in replacement for transformers, which is regularly updated to stay up-to-date with the developments of transformers.\n\nKeywords: Adapters, LoRA, Parameter-efficient fine-tuning, Hub\n\n## [NeMo](https://github.com/NVIDIA/NeMo)\n\nNVIDIA [NeMo](https://github.com/NVIDIA/NeMo) is a conversational AI toolkit built for researchers working on automatic speech recognition (ASR), text-to-speech synthesis (TTS), large language models (LLMs), and natural language processing (NLP). The primary objective of [NeMo](https://github.com/NVIDIA/NeMo) is to help researchers from industry and academia to reuse prior work (code and pretrained models) and make it easier to create new https://developer.nvidia.com/conversational-ai#started.\n\nKeywords: Conversational, ASR, TTS, LLMs, NLP\n\n## [Runhouse](https://github.com/run-house/runhouse)\n\n[Runhouse](https://github.com/run-house/runhouse) allows to send code and data to any of your compute or data infra, all in Python, and continue to interact with them normally from your existing code and environment. Runhouse developers mention:\n\n> Think of it as an expansion pack to your Python interpreter that lets it take detours to remote machines or manipulate remote data.\n\nKeywords: MLOps, Infrastructure, Data storage, Modeling\n\n## [MONAI](https://github.com/Project-MONAI/MONAI)\n\n[MONAI](https://github.com/Project-MONAI/MONAI) is a PyTorch-based, open-source framework for deep learning in healthcare imaging, part of PyTorch Ecosystem. Its ambitions are:\n- developing a community of academic, industrial and clinical researchers collaborating on a common foundation;\n- creating state-of-the-art, end-to-end training workflows for healthcare imaging;\n- providing researchers with the optimized and standardized way to create and evaluate deep learning models.\n\nKeywords: Healthcare imaging, Training, Evaluation\n\n## [simpletransformers](https://github.com/ThilinaRajapakse/simpletransformers)\n\nSimple Transformers lets you quickly train and evaluate Transformer models. Only 3 lines of code are needed to initialize, train, and evaluate a model. It supports a wide variety of NLP tasks.\n\nKeywords: Framework, simplicity, NLP\n\n## [JARVIS](https://github.com/microsoft/JARVIS)\n\n[JARVIS](https://github.com/microsoft/JARVIS) is a system attempting to merge LLMs such as GPT-4 with the rest of the open-source ML community: leveraging up to 60 downstream models in order to perform tasks identified by the LLM.\n\nKeywords: LLM, Agents, HF Hub\n\n## [transformers.js](https://xenova.github.io/transformers.js/)\n\n[transformers.js](https://xenova.github.io/transformers.js/) is a JavaScript library targeted at running models from transformers directly within the browser.\n\nKeywords: Transformers, JavaScript, browser\n\n## [bumblebee](https://github.com/elixir-nx/bumblebee)\n\nBumblebee provides pre-trained Neural Network models on top of Axon, a neural networks library for the Elixir language. It includes integration with 🤗 Models, allowing anyone to download and perform Machine Learning tasks with few lines of code.\n\nKeywords: Elixir, Axon\n\n## [argilla](https://github.com/argilla-io/argilla)\n\nArgilla is an open-source platform providing advanced NLP labeling, monitoring, and workspaces. It is compatible with many open source ecosystems such as Hugging Face, Stanza, FLAIR, and others.\n\nKeywords: NLP, Labeling, Monitoring, Workspaces\n\n## [haystack](https://github.com/deepset-ai/haystack)\n\nHaystack is an open source NLP framework to interact with your data using Transformer models and LLMs. It offers production-ready tools to quickly build complex decision making, question answering, semantic search, text generation applications, and more.\n\nKeywords: NLP, Framework, LLM\n\n## [spaCy](https://github.com/explosion/spaCy)\n\n[spaCy](https://github.com/explosion/spaCy) is a library for advanced Natural Language Processing in Python and Cython. It's built on the very latest research, and was designed from day one to be used in real products. It offers support for transformers models through its third party package, spacy-transformers.\n\nKeywords: NLP, Framework\n\n## [speechbrain](https://github.com/speechbrain/speechbrain)\n\nSpeechBrain is an open-source and all-in-one conversational AI toolkit based on PyTorch.\nThe goal is to create a single, flexible, and user-friendly toolkit that can be used to easily develop state-of-the-art speech technologies, including systems for speech recognition, speaker recognition, speech enhancement, speech separation, language identification, multi-microphone signal processing, and many others.\n\nKeywords: Conversational, Speech\n\n## [skorch](https://github.com/skorch-dev/skorch)\n\nSkorch is a scikit-learn compatible neural network library that wraps PyTorch. It has support for models within transformers, and tokenizers from tokenizers.\n\nKeywords: Scikit-Learn, PyTorch\n\n## [bertviz](https://github.com/jessevig/bertviz)\n\nBertViz is an interactive tool for visualizing attention in Transformer language models such as BERT, GPT2, or T5. It can be run inside a Jupyter or Colab notebook through a simple Python API that supports most Huggingface models.\n\nKeywords: Visualization, Transformers\n\n## [mesh-transformer-jax](https://github.com/kingoflolz/mesh-transformer-jax)\n\n[mesh-transformer-jax](https://github.com/kingoflolz/mesh-transformer-jax) is a haiku library using the xmap/pjit operators in JAX for model parallelism of transformers. This library is designed for scalability up to approximately 40B parameters on TPUv3s. It was the library used to train the GPT-J model.\n\nKeywords: Haiku, Model parallelism, LLM, TPU\n\n## [deepchem](https://github.com/deepchem/deepchem)\n\nDeepChem aims to provide a high quality open-source toolchain that democratizes the use of deep-learning in drug discovery, materials science, quantum chemistry, and biology.\n\nKeywords: Drug discovery, Materials Science, Quantum Chemistry, Biology\n\n## [OpenNRE](https://github.com/thunlp/OpenNRE)\n\nAn Open-Source Package for Neural Relation Extraction (NRE). It is targeted at a wide range of users, from newcomers to relation extraction, to developers, researchers, or students.\n\nKeywords: Neural Relation Extraction, Framework\n\n## [pycorrector](https://github.com/shibing624/pycorrector)\n\nPyCorrector is a Chinese Text Error Correction Tool. It uses a language model to detect errors, pinyin feature and shape feature to correct Chinese text errors. it can be used for Chinese Pinyin and stroke input method.\n\nKeywords: Chinese, Error correction tool, Language model, Pinyin\n\n## [nlpaug](https://github.com/makcedward/nlpaug)\n\nThis python library helps you with augmenting nlp for machine learning projects. It is a lightweight library featuring synthetic data generation for improving model performance, support for audio and text, and compatibility with several ecosystems (scikit-learn, pytorch, tensorflow).\n\nKeywords: Data augmentation, Synthetic data generation, Audio, NLP\n\n## [dream-textures](https://github.com/carson-katri/dream-textures)\n\n[dream-textures](https://github.com/carson-katri/dream-textures) is a library targeted at bringing stable-diffusion support within Blender. It supports several use-cases, such as image generation, texture projection, inpainting/outpainting, ControlNet, and upscaling.\n\nKeywords: Stable-Diffusion, Blender\n\n## [seldon-core](https://github.com/SeldonIO/seldon-core)\n\nSeldon core converts your ML models (Tensorflow, Pytorch, H2o, etc.) or language wrappers (Python, Java, etc.) into production REST/GRPC microservices.\nSeldon handles scaling to thousands of production machine learning models and provides advanced machine learning capabilities out of the box including Advanced Metrics, Request Logging, Explainers, Outlier Detectors, A/B Tests, Canaries and more.\n\nKeywords: Microservices, Modeling, Language wrappers\n\n## [open_model_zoo](https://github.com/openvinotoolkit/open_model_zoo)\n\nThis repository includes optimized deep learning models and a set of demos to expedite development of high-performance deep learning inference applications. Use these free pre-trained models instead of training your own models to speed-up the development and production deployment process.\n\nKeywords: Optimized models, Demos\n\n## [ml-stable-diffusion](https://github.com/apple/ml-stable-diffusion)\n\nML-Stable-Diffusion is a repository by Apple bringing Stable Diffusion support to Core ML, on Apple Silicon devices. It supports stable diffusion checkpoints hosted on the Hugging Face Hub.\n\nKeywords: Stable Diffusion, Apple Silicon, Core ML\n\n## [stable-dreamfusion](https://github.com/ashawkey/stable-dreamfusion)\n\nStable-Dreamfusion is a pytorch implementation of the text-to-3D model Dreamfusion, powered by the Stable Diffusion text-to-2D model.\n\nKeywords: Text-to-3D, Stable Diffusion\n\n## [txtai](https://github.com/neuml/txtai)\n \n[txtai](https://github.com/neuml/txtai) is an open-source platform for semantic search and workflows powered by language models. txtai builds embeddings databases, which are a union of vector indexes and relational databases enabling similarity search with SQL. Semantic workflows connect language models together into unified applications.\n\nKeywords: Semantic search, LLM\n\n## [djl](https://github.com/deepjavalibrary/djl)\n\nDeep Java Library (DJL) is an open-source, high-level, engine-agnostic Java framework for deep learning. DJL is designed to be easy to get started with and simple to use for developers. DJL provides a native Java development experience and functions like any other regular Java library. DJL offers [a Java binding](https://github.com/deepjavalibrary/djl/tree/master/extensions/tokenizers) for HuggingFace Tokenizers and easy conversion toolkit for HuggingFace model to deploy in Java.\n\nKeywords: Java, Framework\n\n## [lm-evaluation-harness](https://github.com/EleutherAI/lm-evaluation-harness/)\n\nThis project provides a unified framework to test generative language models on a large number of different evaluation tasks. It has support for more than 200 tasks, and supports different ecosystems: HF Transformers, GPT-NeoX, DeepSpeed, as well as the OpenAI API.\n\nKeywords: LLM, Evaluation, Few-shot\n\n## [gpt-neox](https://github.com/EleutherAI/gpt-neox)\n\nThis repository records EleutherAI's library for training large-scale language models on GPUs. The framework is based on NVIDIA's Megatron Language Model and has been augmented with techniques from DeepSpeed as well as some novel optimizations. It is focused on training multi-billion-parameter models.\n\nKeywords: Training, LLM, Megatron, DeepSpeed\n\n## [muzic](https://github.com/microsoft/muzic)\n\nMuzic is a research project on AI music that empowers music understanding and generation with deep learning and artificial intelligence. Muzic was created by researchers from Microsoft Research Asia.\n\nKeywords: Music understanding, Music generation\n\n## [dalle-flow](https://github.com/jina-ai/dalle-flow)\n\nDALL·E Flow is an interactive workflow for generating high-definition images from a text prompt. Itt leverages DALL·E-Mega, GLID-3 XL, and Stable Diffusion to generate image candidates, and then calls CLIP-as-service to rank the candidates w.r.t. the prompt.\nThe preferred candidate is fed to GLID-3 XL for diffusion, which often enriches the texture and background. Finally, the candidate is upscaled to 1024x1024 via SwinIR.\n\nKeywords: High-definition image generation, Stable Diffusion, DALL-E Mega, GLID-3 XL, CLIP, SwinIR\n\n## [lightseq](https://github.com/bytedance/lightseq)\n\nLightSeq is a high performance training and inference library for sequence processing and generation implemented in CUDA. It enables highly efficient computation of modern NLP and CV models such as BERT, GPT, Transformer, etc. It is therefore best useful for machine translation, text generation, image classification, and other sequence related tasks.\n\nKeywords: Training, Inference, Sequence Processing, Sequence Generation\n\n## [LaTeX-OCR](https://github.com/lukas-blecher/LaTeX-OCR)\n\nThe goal of this project is to create a learning based system that takes an image of a math formula and returns corresponding LaTeX code.\n\nKeywords: OCR, LaTeX, Math formula\n\n## [open_clip](https://github.com/mlfoundations/open_clip)\n\nOpenCLIP is an open source implementation of OpenAI's CLIP.\n\nThe goal of this repository is to enable training models with contrastive image-text supervision, and to investigate their properties such as robustness to distribution shift. \nThe starting point is an implementation of CLIP that matches the accuracy of the original CLIP models when trained on the same dataset. \n\nSpecifically, a ResNet-50 model trained with this codebase on OpenAI's 15 million image subset of YFCC achieves 32.7% top-1 accuracy on ImageNet.\n\nKeywords: CLIP, Open-source, Contrastive, Image-text\n\n## [dalle-playground](https://github.com/saharmor/dalle-playground)\n\nA playground to generate images from any text prompt using Stable Diffusion and Dall-E mini.\n\nKeywords: WebUI, Stable Diffusion, Dall-E mini\n\n## [FedML](https://github.com/FedML-AI/FedML)\n\n[FedML](https://github.com/FedML-AI/FedML) is a federated learning and analytics library enabling secure and collaborative machine learning on decentralized data anywhere at any scale.\n\nIt supports large-scale cross-silo federated learning, and cross-device federated learning on smartphones/IoTs, and research simulation.\n\nKeywords: Federated Learning, Analytics, Collaborative ML, Decentralized\n\n## [gpt-code-clippy](https://github.com/CodedotAl/gpt-code-clippy)\n\nGPT-Code-Clippy (GPT-CC) is an open source version of GitHub Copilot, a language model -- based on GPT-3, called GPT-Codex -- that is fine-tuned on publicly available code from GitHub.\n\nKeywords: LLM, Code\n\n## [TextAttack](https://github.com/QData/TextAttack)\n\n[TextAttack](https://github.com/QData/TextAttack) 🐙 is a Python framework for adversarial attacks, data augmentation, and model training in NLP.\n\nKeywords: Adversarial attacks, Data augmentation, NLP\n\n## [OpenPrompt](https://github.com/thunlp/OpenPrompt)\n\nPrompt-learning is a paradigm to adapt pre-trained language models (PLMs) to downstream NLP tasks, which modify the input text with a textual template and directly uses PLMs to conduct pre-trained tasks. This library provides a standard, flexible and extensible framework to deploy the prompt-learning pipeline. [OpenPrompt](https://github.com/thunlp/OpenPrompt) supports loading PLMs directly from https://github.com/huggingface/transformers.\n\n## [text-generation-webui](https://github.com/oobabooga/text-generation-webui/)\n\n[text-generation-webui](https://github.com/oobabooga/text-generation-webui/) is a Gradio Web UI for running Large Language Models like LLaMA, llama.cpp, GPT-J, Pythia, OPT, and GALACTICA.\n\nKeywords: LLM, WebUI\n\n## [libra](https://github.com/Palashio/libra)\n\nAn ergonomic machine learning [libra](https://github.com/Palashio/libra)ry for non-technical users. It focuses on ergonomics and on ensuring that training a model is as simple as it can be.\n\nKeywords: Ergonomic, Non-technical\n\n## [alibi](https://github.com/SeldonIO/alibi)\n\nAlibi is an open source Python library aimed at machine learning model inspection and interpretation. The focus of the library is to provide high-quality implementations of black-box, white-box, local and global explanation methods for classification and regression models.\n\nKeywords: Model inspection, Model interpretation, Black-box, White-box\n\n## [tortoise-tts](https://github.com/neonbjb/tortoise-tts)\n\nTortoise is a text-to-speech program built with the following priorities: strong multi-voice capabilities, and highly realistic prosody and intonation.\n\nKeywords: Text-to-speech\n\n## [flower](https://github.com/adap/flower)\n\nFlower (flwr) is a framework for building federated learning systems. The design of Flower is based on a few guiding principles: customizability, extendability, framework agnosticity, and ease-of-use.\n\nKeywords: Federated learning systems, Customizable, Extendable, Framework-agnostic, Simplicity\n\n## [fast-bert](https://github.com/utterworks/fast-bert)\n\nFast-Bert is a deep learning library that allows developers and data scientists to train and deploy BERT and XLNet based models for natural language processing tasks beginning with Text Classification. It is aimed at simplicity.\n\nKeywords: Deployment, BERT, XLNet\n\n## [towhee](https://github.com/towhee-io/towhee)\n\nTowhee makes it easy to build neural data processing pipelines for AI applications. We provide hundreds of models, algorithms, and transformations that can be used as standard pipeline building blocks. Users can use Towhee's Pythonic API to build a prototype of their pipeline and automatically optimize it for production-ready environments.\n\nKeywords: Data processing pipeline, Optimization\n\n## [alibi-detect](https://github.com/SeldonIO/alibi-detect)\n\nAlibi Detect is an open source Python library focused on outlier, adversarial and drift detection. The package aims to cover both online and offline detectors for tabular data, text, images and time series. Both TensorFlow and PyTorch backends are supported for drift detection.\n\nKeywords: Adversarial, Outlier, Drift detection\n\n## [FARM](https://github.com/deepset-ai/FARM)\n\n[FARM](https://github.com/deepset-ai/FARM) makes Transfer Learning with BERT & Co simple, fast and enterprise-ready. It's built upon transformers and provides additional features to simplify the life of developers: Parallelized preprocessing, highly modular design, multi-task learning, experiment tracking, easy debugging and close integration with AWS SageMaker.\n\nKeywords: Transfer Learning, Modular design, Multi-task learning, Experiment tracking\n\n## [aitextgen](https://github.com/minimaxir/aitextgen)\n\nA robust Python tool for text-based AI training and generation using OpenAI's GPT-2 and EleutherAI's GPT Neo/GPT-3 architecture.\n[aitextgen](https://github.com/minimaxir/aitextgen) is a Python package that leverages PyTorch, Hugging Face Transformers and pytorch-lightning with specific optimizations for text generation using GPT-2, plus many added features.\n\nKeywords: Training, Generation\n\n## [diffgram](https://github.com/diffgram/diffgram)\n\nDiffgram aims to integrate human supervision into platforms. We support your team programmatically changing the UI (Schema, layout, etc.) like in Streamlit. This means that you can collect and annotate timely data from users. In other words, we are the platform behind your platform, an integrated part of your application, to ship new & better AI products faster.\n\nKeywords: Human supervision, Platform\n\n## [ecco](https://github.com/jalammar/ecco)\n\nExplain, analyze, and visualize NLP language models. Ecco creates interactive visualizations directly in Jupyter notebooks explaining the behavior of Transformer-based language models (like GPT2, BERT, RoBERTA, T5, and T0).\n\nKeywords: Model explainability\n\n## [s3prl](https://github.com/s3prl/s3prl)\n\n[s3prl](https://github.com/s3prl/s3prl) stands for Self-Supervised Speech Pre-training and Representation Learning. Self-supervised speech pre-trained models are called upstream in this toolkit, and are utilized in various downstream tasks.\n\nKeywords: Speech, Training\n\n## [ru-dalle](https://github.com/ai-forever/ru-dalle)\n\nRuDALL-E aims to be similar to DALL-E, targeted to Russian.\n\nKeywords: DALL-E, Russian\n\n## [DeepKE](https://github.com/zjunlp/DeepKE)\n\n[DeepKE](https://github.com/zjunlp/DeepKE) is a knowledge extraction toolkit for knowledge graph construction supporting cnSchema，low-resource, document-level and multimodal scenarios for entity, relation and attribute extraction.\n\nKeywords: Knowledge Extraction, Knowledge Graphs\n\n## [Nebuly](https://github.com/nebuly-ai/nebuly)\n\nNebuly is the next-generation platform to monitor and optimize your AI costs in one place. The platform connects to all your AI cost sources (compute, API providers, AI software licenses, etc) and centralizes them in one place to give you full visibility on a model basis. The platform also provides optimization recommendations and a co-pilot model that can guide during the optimization process. The platform builds on top of the open-source tools allowing you to optimize the different steps of your AI stack to squeeze out the best possible cost performances.\n\nKeywords: Optimization, Performance, Monitoring\n\n## [imaginAIry](https://github.com/brycedrennan/imaginAIry)\n\nOffers a CLI and a Python API to generate images with Stable Diffusion. It has support for many tools, like image structure control (controlnet), instruction-based image edits (InstructPix2Pix), prompt-based masking (clipseg), among others.\n\nKeywords: Stable Diffusion, CLI, Python API\n\n## [sparseml](https://github.com/neuralmagic/sparseml)\n\nSparseML is an open-source model optimization toolkit that enables you to create inference-optimized sparse models using pruning, quantization, and distillation algorithms. Models optimized with SparseML can then be exported to the ONNX and deployed with DeepSparse for GPU-class performance on CPU hardware.\n\nKeywords: Model optimization, Pruning, Quantization, Distillation\n\n## [opacus](https://github.com/pytorch/opacus)\n\nOpacus is a library that enables training PyTorch models with differential privacy. It supports training with minimal code changes required on the client, has little impact on training performance, and allows the client to online track the privacy budget expended at any given moment.\n\nKeywords: Differential privacy\n\n## [LAVIS](https://github.com/salesforce/LAVIS)\n\n[LAVIS](https://github.com/salesforce/LAVIS) is a Python deep learning library for LAnguage-and-VISion intelligence research and applications. This library aims to provide engineers and researchers with a one-stop solution to rapidly develop models for their specific multimodal scenarios, and benchmark them across standard and customized datasets. It features a unified interface design to access\n\nKeywords: Multimodal, NLP, Vision\n\n## [buzz](https://github.com/chidiwilliams/buzz)\n\nBuzz transcribes and translates audio offline on your personal computer. Powered by OpenAI's Whisper.\n\nKeywords: Audio transcription, Translation\n\n## [rust-bert](https://github.com/guillaume-be/rust-bert)\n\nRust-native state-of-the-art Natural Language Processing models and pipelines. Port of Hugging Face's Transformers library, using the tch-rs crate and pre-processing from rust-tokenizers. Supports multi-threaded tokenization and GPU inference. This repository exposes the model base architecture, task-specific heads and ready-to-use pipelines.\n\nKeywords: Rust, BERT, Inference\n\n## [EasyNLP](https://github.com/alibaba/EasyNLP)\n\n[EasyNLP](https://github.com/alibaba/EasyNLP) is an easy-to-use NLP development and application toolkit in PyTorch, first released inside Alibaba in 2021. It is built with scalable distributed training strategies and supports a comprehensive suite of NLP algorithms for various NLP applications. [EasyNLP](https://github.com/alibaba/EasyNLP) integrates knowledge distillation and few-shot learning for landing large pre-trained models, together with various popular multi-modality pre-trained models. It provides a unified framework of model training, inference, and deployment for real-world applications.\n\nKeywords: NLP, Knowledge distillation, Few-shot learning, Multi-modality, Training, Inference, Deployment\n\n## [TurboTransformers](https://github.com/Tencent/TurboTransformers)\n\nA fast and user-friendly runtime for transformer inference (Bert, Albert, GPT2, Decoders, etc) on CPU and GPU.\n\nKeywords: Optimization, Performance\n\n## [hivemind](https://github.com/learning-at-home/hivemind)\n\nHivemind is a PyTorch library for decentralized deep learning across the Internet. Its intended usage is training one large model on hundreds of computers from different universities, companies, and volunteers.\n\nKeywords: Decentralized training\n\n## [docquery](https://github.com/impira/docquery)\n\nDocQuery is a library and command-line tool that makes it easy to analyze semi-structured and unstructured documents (PDFs, scanned images, etc.) using large language models (LLMs). You simply point DocQuery at one or more documents and specify a question you want to ask. DocQuery is created by the team at Impira.\n\nKeywords: Semi-structured documents, Unstructured documents, LLM, Document Question Answering\n\n## [CodeGeeX](https://github.com/THUDM/CodeGeeX)\n\n[CodeGeeX](https://github.com/THUDM/CodeGeeX) is a large-scale multilingual code generation model with 13 billion parameters, pre-trained on a large code corpus of more than 20 programming languages. It has several unique features:\n- Multilingual code generation\n- Crosslingual code translation\n- Is a customizable programming assistant\n\nKeywords: Code Generation Model\n\n## [ktrain](https://github.com/amaiya/ktrain)\n\n[ktrain](https://github.com/amaiya/ktrain) is a lightweight wrapper for the deep learning library TensorFlow Keras (and other libraries) to help build, train, and deploy neural networks and other machine learning models. Inspired by ML framework extensions like fastai and ludwig, [ktrain](https://github.com/amaiya/ktrain) is designed to make deep learning and AI more accessible and easier to apply for both newcomers and experienced practitioners.\n\nKeywords: Keras wrapper, Model building, Training, Deployment\n\n## [FastDeploy](https://github.com/PaddlePaddle/FastDeploy)\n\n[FastDeploy](https://github.com/PaddlePaddle/FastDeploy) is an Easy-to-use and High Performance AI model deployment toolkit for Cloud, Mobile and Edge with packageout-of-the-box and unified experience, endend-to-end optimization for over fire160+ Text, Vision, Speech and Cross-modal AI models. Including image classification, object detection, OCR, face detection, matting, pp-tracking, NLP, stable diffusion, TTS and other tasks to meet developers' industrial deployment needs for multi-scenario, multi-hardware and multi-platform.\n\nKeywords: Model deployment, CLoud, Mobile, Edge\n\n## [underthesea](https://github.com/undertheseanlp/underthesea)\n\n[underthesea](https://github.com/undertheseanlp/underthesea) is a Vietnamese NLP toolkit. Underthesea is a suite of open source Python modules data sets and tutorials supporting research and development in Vietnamese Natural Language Processing. We provides extremely easy API to quickly apply pretrained NLP models to your Vietnamese text, such as word segmentation, part-of-speech tagging (PoS), named entity recognition (NER), text classification and dependency parsing.\n\nKeywords: Vietnamese, NLP\n\n## [hasktorch](https://github.com/hasktorch/hasktorch)\n\nHasktorch is a library for tensors and neural networks in Haskell. It is an independent open source community project which leverages the core C++ libraries shared by PyTorch.\n\nKeywords: Haskell, Neural Networks\n\n## [donut](https://github.com/clovaai/donut)\n\nDonut, or Document understanding transformer, is a new method of document understanding that utilizes an OCR-free end-to-end Transformer model.\n\nDonut does not require off-the-shelf OCR engines/APIs, yet it shows state-of-the-art performances on various visual document understanding tasks, such as visual document classification or information extraction (a.k.a. document parsing).\n\nKeywords: Document Understanding\n\n## [transformers-interpret](https://github.com/cdpierse/transformers-interpret)\n\nTransformers Interpret is a model explainability tool designed to work exclusively with the transformers package.\n\nIn line with the philosophy of the Transformers package Transformers Interpret allows any transformers model to be explained in just two lines. Explainers are available for both text and computer vision models. Visualizations are also available in notebooks and as savable png and html files\n\nKeywords: Model interpretation, Visualization\n\n## [mlrun](https://github.com/mlrun/mlrun)\n\nMLRun is an open MLOps platform for quickly building and managing continuous ML applications across their lifecycle. MLRun integrates into your development and CI/CD environment and automates the delivery of production data, ML pipelines, and online applications, significantly reducing engineering efforts, time to production, and computation resources. With MLRun, you can choose any IDE on your local machine or on the cloud. MLRun breaks the silos between data, ML, software, and DevOps/MLOps teams, enabling collaboration and fast continuous improvements.\n\nKeywords: MLOps\n\n## [FederatedScope](https://github.com/alibaba/FederatedScope)\n\n[FederatedScope](https://github.com/alibaba/FederatedScope) is a comprehensive federated learning platform that provides convenient usage and flexible customization for various federated learning tasks in both academia and industry. Based on an event-driven architecture, [FederatedScope](https://github.com/alibaba/FederatedScope) integrates rich collections of functionalities to satisfy the burgeoning demands from federated learning, and aims to build up an easy-to-use platform for promoting learning safely and effectively.\n\nKeywords: Federated learning, Event-driven\n\n## [pythainlp](https://github.com/PyThaiNLP/pythainlp)\n\nPyThaiNLP is a Python package for text processing and linguistic analysis, similar to NLTK with focus on Thai language.\n\nKeywords: Thai, NLP, NLTK\n\n## [FlagAI](https://github.com/FlagAI-Open/FlagAI)\n\n[FlagAI](https://github.com/FlagAI-Open/FlagAI) (Fast LArge-scale General AI models) is a fast, easy-to-use and extensible toolkit for large-scale model. Our goal is to support training, fine-tuning, and deployment of large-scale models on various downstream tasks with multi-modality.\n\nKeywords: Large models, Training, Fine-tuning, Deployment, Multi-modal\n\n## [pyserini](https://github.com/castorini/pyserini)\n\n[pyserini](https://github.com/castorini/pyserini) is a Python toolkit for reproducible information retrieval research with sparse and dense representations. Retrieval using sparse representations is provided via integration with the group's Anserini IR toolkit. Retrieval using dense representations is provided via integration with Facebook's Faiss library.\n\nKeywords: IR, Information Retrieval, Dense, Sparse\n\n## [baal](https://github.com/baal-org/baal)\n\n[baal](https://github.com/baal-org/baal) is an active learning library that supports both industrial applications and research usecases. [baal](https://github.com/baal-org/baal) currently supports Monte-Carlo Dropout, MCDropConnect, deep ensembles, and semi-supervised learning.\n\nKeywords: Active Learning, Research, Labeling\n\n## [cleanlab](https://github.com/cleanlab/cleanlab)\n\n[cleanlab](https://github.com/cleanlab/cleanlab) is the standard data-centric AI package for data quality and machine learning with messy, real-world data and labels. For text, image, tabular, audio (among others) datasets, you can use cleanlab to automatically: detect data issues (outliers, label errors, near duplicates, etc), train robust ML models, infer consensus + annotator-quality for multi-annotator data, suggest data to (re)label next (active learning).\n\nKeywords: Data-Centric AI, Data Quality, Noisy Labels, Outlier Detection, Active Learning  \n\n## [BentoML](https://github.com/bentoml/BentoML)\n\n[BentoML](https://github.com/bentoml) is the unified framework for building, shipping, and scaling production-ready AI applications incorporating traditional ML, pre-trained AI models, Generative and Large Language Models. \nAll Hugging Face models and pipelines can be seamlessly integrated into BentoML applications, enabling the running of models on the most suitable hardware and independent scaling based on usage.\n\nKeywords: BentoML, Framework, Deployment, AI Applications\n\n## [LLaMA Factory](https://github.com/hiyouga/LLaMA-Factory)\n\n[LLaMA Factory](https://github.com/hiyouga/LLaMA-Factory) offers a user-friendly fine-tuning framework that incorporates PEFT. The repository includes training(fine-tuning) and inference examples for LLaMA-2, BLOOM, Falcon, Baichuan, Qwen, and other LLMs. A ChatGLM version is also available in [ChatGLM-Efficient-Tuning](https://github.com/hiyouga/ChatGLM-Efficient-Tuning).\n\nKeywords: PEFT, fine-tuning, LLaMA-2, ChatGLM, Qwen\n\n"
        },
        {
          "name": "benchmark",
          "type": "tree",
          "content": null
        },
        {
          "name": "conftest.py",
          "type": "blob",
          "size": 5.08,
          "content": "# Copyright 2020 The HuggingFace Team. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# tests directory-specific settings - this file is run automatically\n# by pytest before any tests are run\n\nimport doctest\nimport sys\nimport warnings\nfrom os.path import abspath, dirname, join\n\nimport _pytest\nimport pytest\n\nfrom transformers.testing_utils import HfDoctestModule, HfDocTestParser\n\n\nNOT_DEVICE_TESTS = {\n    \"test_tokenization\",\n    \"test_processor\",\n    \"test_processing\",\n    \"test_beam_constraints\",\n    \"test_configuration_utils\",\n    \"test_data_collator\",\n    \"test_trainer_callback\",\n    \"test_trainer_utils\",\n    \"test_feature_extraction\",\n    \"test_image_processing\",\n    \"test_image_processor\",\n    \"test_image_transforms\",\n    \"test_optimization\",\n    \"test_retrieval\",\n    \"test_config\",\n    \"test_from_pretrained_no_checkpoint\",\n    \"test_keep_in_fp32_modules\",\n    \"test_gradient_checkpointing_backward_compatibility\",\n    \"test_gradient_checkpointing_enable_disable\",\n    \"test_save_load_fast_init_from_base\",\n    \"test_fast_init_context_manager\",\n    \"test_fast_init_tied_embeddings\",\n    \"test_save_load_fast_init_to_base\",\n    \"test_torch_save_load\",\n    \"test_initialization\",\n    \"test_forward_signature\",\n    \"test_model_get_set_embeddings\",\n    \"test_model_main_input_name\",\n    \"test_correct_missing_keys\",\n    \"test_tie_model_weights\",\n    \"test_can_use_safetensors\",\n    \"test_load_save_without_tied_weights\",\n    \"test_tied_weights_keys\",\n    \"test_model_weights_reload_no_missing_tied_weights\",\n    \"test_pt_tf_model_equivalence\",\n    \"test_mismatched_shapes_have_properly_initialized_weights\",\n    \"test_matched_shapes_have_loaded_weights_when_some_mismatched_shapes_exist\",\n    \"test_model_is_small\",\n    \"test_tf_from_pt_safetensors\",\n    \"test_flax_from_pt_safetensors\",\n    \"ModelTest::test_pipeline_\",  # None of the pipeline tests from PipelineTesterMixin (of which XxxModelTest inherits from) are running on device\n    \"ModelTester::test_pipeline_\",\n    \"/repo_utils/\",\n    \"/utils/\",\n    \"/agents/\",\n}\n\n# allow having multiple repository checkouts and not needing to remember to rerun\n# `pip install -e '.[dev]'` when switching between checkouts and running tests.\ngit_repo_path = abspath(join(dirname(__file__), \"src\"))\nsys.path.insert(1, git_repo_path)\n\n# silence FutureWarning warnings in tests since often we can't act on them until\n# they become normal warnings - i.e. the tests still need to test the current functionality\nwarnings.simplefilter(action=\"ignore\", category=FutureWarning)\n\n\ndef pytest_configure(config):\n    config.addinivalue_line(\n        \"markers\", \"is_pt_tf_cross_test: mark test to run only when PT and TF interactions are tested\"\n    )\n    config.addinivalue_line(\n        \"markers\", \"is_pt_flax_cross_test: mark test to run only when PT and FLAX interactions are tested\"\n    )\n    config.addinivalue_line(\"markers\", \"is_pipeline_test: mark test to run only when pipelines are tested\")\n    config.addinivalue_line(\"markers\", \"is_staging_test: mark test to run only in the staging environment\")\n    config.addinivalue_line(\"markers\", \"accelerate_tests: mark test that require accelerate\")\n    config.addinivalue_line(\"markers\", \"agent_tests: mark the agent tests that are run on their specific schedule\")\n    config.addinivalue_line(\"markers\", \"not_device_test: mark the tests always running on cpu\")\n\n\ndef pytest_collection_modifyitems(items):\n    for item in items:\n        if any(test_name in item.nodeid for test_name in NOT_DEVICE_TESTS):\n            item.add_marker(pytest.mark.not_device_test)\n\n\ndef pytest_addoption(parser):\n    from transformers.testing_utils import pytest_addoption_shared\n\n    pytest_addoption_shared(parser)\n\n\ndef pytest_terminal_summary(terminalreporter):\n    from transformers.testing_utils import pytest_terminal_summary_main\n\n    make_reports = terminalreporter.config.getoption(\"--make-reports\")\n    if make_reports:\n        pytest_terminal_summary_main(terminalreporter, id=make_reports)\n\n\ndef pytest_sessionfinish(session, exitstatus):\n    # If no tests are collected, pytest exists with code 5, which makes the CI fail.\n    if exitstatus == 5:\n        session.exitstatus = 0\n\n\n# Doctest custom flag to ignore output.\nIGNORE_RESULT = doctest.register_optionflag(\"IGNORE_RESULT\")\n\nOutputChecker = doctest.OutputChecker\n\n\nclass CustomOutputChecker(OutputChecker):\n    def check_output(self, want, got, optionflags):\n        if IGNORE_RESULT & optionflags:\n            return True\n        return OutputChecker.check_output(self, want, got, optionflags)\n\n\ndoctest.OutputChecker = CustomOutputChecker\n_pytest.doctest.DoctestModule = HfDoctestModule\ndoctest.DocTestParser = HfDocTestParser\n"
        },
        {
          "name": "docker",
          "type": "tree",
          "content": null
        },
        {
          "name": "docs",
          "type": "tree",
          "content": null
        },
        {
          "name": "examples",
          "type": "tree",
          "content": null
        },
        {
          "name": "hubconf.py",
          "type": "blob",
          "size": 8.47,
          "content": "# Copyright 2020 The HuggingFace Team. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport os\nimport sys\n\n\nSRC_DIR = os.path.join(os.path.dirname(__file__), \"src\")\nsys.path.append(SRC_DIR)\n\n\nfrom transformers import (\n    AutoConfig,\n    AutoModel,\n    AutoModelForCausalLM,\n    AutoModelForMaskedLM,\n    AutoModelForQuestionAnswering,\n    AutoModelForSequenceClassification,\n    AutoTokenizer,\n    add_start_docstrings,\n)\n\n\ndependencies = [\"torch\", \"numpy\", \"tokenizers\", \"filelock\", \"requests\", \"tqdm\", \"regex\", \"sentencepiece\", \"sacremoses\", \"importlib_metadata\", \"huggingface_hub\"]\n\n\n@add_start_docstrings(AutoConfig.__doc__)\ndef config(*args, **kwargs):\n    r\"\"\"\n                # Using torch.hub !\n                import torch\n\n                config = torch.hub.load('huggingface/transformers', 'config', 'google-bert/bert-base-uncased')  # Download configuration from huggingface.co and cache.\n                config = torch.hub.load('huggingface/transformers', 'config', './test/bert_saved_model/')  # E.g. config (or model) was saved using `save_pretrained('./test/saved_model/')`\n                config = torch.hub.load('huggingface/transformers', 'config', './test/bert_saved_model/my_configuration.json')\n                config = torch.hub.load('huggingface/transformers', 'config', 'google-bert/bert-base-uncased', output_attentions=True, foo=False)\n                assert config.output_attentions == True\n                config, unused_kwargs = torch.hub.load('huggingface/transformers', 'config', 'google-bert/bert-base-uncased', output_attentions=True, foo=False, return_unused_kwargs=True)\n                assert config.output_attentions == True\n                assert unused_kwargs == {'foo': False}\n\n            \"\"\"\n\n    return AutoConfig.from_pretrained(*args, **kwargs)\n\n\n@add_start_docstrings(AutoTokenizer.__doc__)\ndef tokenizer(*args, **kwargs):\n    r\"\"\"\n        # Using torch.hub !\n        import torch\n\n        tokenizer = torch.hub.load('huggingface/transformers', 'tokenizer', 'google-bert/bert-base-uncased')    # Download vocabulary from huggingface.co and cache.\n        tokenizer = torch.hub.load('huggingface/transformers', 'tokenizer', './test/bert_saved_model/')  # E.g. tokenizer was saved using `save_pretrained('./test/saved_model/')`\n\n    \"\"\"\n\n    return AutoTokenizer.from_pretrained(*args, **kwargs)\n\n\n@add_start_docstrings(AutoModel.__doc__)\ndef model(*args, **kwargs):\n    r\"\"\"\n            # Using torch.hub !\n            import torch\n\n            model = torch.hub.load('huggingface/transformers', 'model', 'google-bert/bert-base-uncased')    # Download model and configuration from huggingface.co and cache.\n            model = torch.hub.load('huggingface/transformers', 'model', './test/bert_model/')  # E.g. model was saved using `save_pretrained('./test/saved_model/')`\n            model = torch.hub.load('huggingface/transformers', 'model', 'google-bert/bert-base-uncased', output_attentions=True)  # Update configuration during loading\n            assert model.config.output_attentions == True\n            # Loading from a TF checkpoint file instead of a PyTorch model (slower)\n            config = AutoConfig.from_pretrained('./tf_model/bert_tf_model_config.json')\n            model = torch.hub.load('huggingface/transformers', 'model', './tf_model/bert_tf_checkpoint.ckpt.index', from_tf=True, config=config)\n\n        \"\"\"\n\n    return AutoModel.from_pretrained(*args, **kwargs)\n\n\n@add_start_docstrings(AutoModelForCausalLM.__doc__)\ndef modelForCausalLM(*args, **kwargs):\n    r\"\"\"\n        # Using torch.hub !\n        import torch\n\n        model = torch.hub.load('huggingface/transformers', 'modelForCausalLM', 'openai-community/gpt2')    # Download model and configuration from huggingface.co and cache.\n        model = torch.hub.load('huggingface/transformers', 'modelForCausalLM', './test/saved_model/')  # E.g. model was saved using `save_pretrained('./test/saved_model/')`\n        model = torch.hub.load('huggingface/transformers', 'modelForCausalLM', 'openai-community/gpt2', output_attentions=True)  # Update configuration during loading\n        assert model.config.output_attentions == True\n        # Loading from a TF checkpoint file instead of a PyTorch model (slower)\n        config = AutoConfig.from_pretrained('./tf_model/gpt_tf_model_config.json')\n        model = torch.hub.load('huggingface/transformers', 'modelForCausalLM', './tf_model/gpt_tf_checkpoint.ckpt.index', from_tf=True, config=config)\n\n    \"\"\"\n    return AutoModelForCausalLM.from_pretrained(*args, **kwargs)\n\n\n@add_start_docstrings(AutoModelForMaskedLM.__doc__)\ndef modelForMaskedLM(*args, **kwargs):\n    r\"\"\"\n            # Using torch.hub !\n            import torch\n\n            model = torch.hub.load('huggingface/transformers', 'modelForMaskedLM', 'google-bert/bert-base-uncased')    # Download model and configuration from huggingface.co and cache.\n            model = torch.hub.load('huggingface/transformers', 'modelForMaskedLM', './test/bert_model/')  # E.g. model was saved using `save_pretrained('./test/saved_model/')`\n            model = torch.hub.load('huggingface/transformers', 'modelForMaskedLM', 'google-bert/bert-base-uncased', output_attentions=True)  # Update configuration during loading\n            assert model.config.output_attentions == True\n            # Loading from a TF checkpoint file instead of a PyTorch model (slower)\n            config = AutoConfig.from_pretrained('./tf_model/bert_tf_model_config.json')\n            model = torch.hub.load('huggingface/transformers', 'modelForMaskedLM', './tf_model/bert_tf_checkpoint.ckpt.index', from_tf=True, config=config)\n\n        \"\"\"\n\n    return AutoModelForMaskedLM.from_pretrained(*args, **kwargs)\n\n\n@add_start_docstrings(AutoModelForSequenceClassification.__doc__)\ndef modelForSequenceClassification(*args, **kwargs):\n    r\"\"\"\n            # Using torch.hub !\n            import torch\n\n            model = torch.hub.load('huggingface/transformers', 'modelForSequenceClassification', 'google-bert/bert-base-uncased')    # Download model and configuration from huggingface.co and cache.\n            model = torch.hub.load('huggingface/transformers', 'modelForSequenceClassification', './test/bert_model/')  # E.g. model was saved using `save_pretrained('./test/saved_model/')`\n            model = torch.hub.load('huggingface/transformers', 'modelForSequenceClassification', 'google-bert/bert-base-uncased', output_attentions=True)  # Update configuration during loading\n            assert model.config.output_attentions == True\n            # Loading from a TF checkpoint file instead of a PyTorch model (slower)\n            config = AutoConfig.from_pretrained('./tf_model/bert_tf_model_config.json')\n            model = torch.hub.load('huggingface/transformers', 'modelForSequenceClassification', './tf_model/bert_tf_checkpoint.ckpt.index', from_tf=True, config=config)\n\n        \"\"\"\n\n    return AutoModelForSequenceClassification.from_pretrained(*args, **kwargs)\n\n\n@add_start_docstrings(AutoModelForQuestionAnswering.__doc__)\ndef modelForQuestionAnswering(*args, **kwargs):\n    r\"\"\"\n        # Using torch.hub !\n        import torch\n\n        model = torch.hub.load('huggingface/transformers', 'modelForQuestionAnswering', 'google-bert/bert-base-uncased')    # Download model and configuration from huggingface.co and cache.\n        model = torch.hub.load('huggingface/transformers', 'modelForQuestionAnswering', './test/bert_model/')  # E.g. model was saved using `save_pretrained('./test/saved_model/')`\n        model = torch.hub.load('huggingface/transformers', 'modelForQuestionAnswering', 'google-bert/bert-base-uncased', output_attentions=True)  # Update configuration during loading\n        assert model.config.output_attentions == True\n        # Loading from a TF checkpoint file instead of a PyTorch model (slower)\n        config = AutoConfig.from_pretrained('./tf_model/bert_tf_model_config.json')\n        model = torch.hub.load('huggingface/transformers', 'modelForQuestionAnswering', './tf_model/bert_tf_checkpoint.ckpt.index', from_tf=True, config=config)\n\n    \"\"\"\n    return AutoModelForQuestionAnswering.from_pretrained(*args, **kwargs)\n"
        },
        {
          "name": "i18n",
          "type": "tree",
          "content": null
        },
        {
          "name": "model_cards",
          "type": "tree",
          "content": null
        },
        {
          "name": "notebooks",
          "type": "tree",
          "content": null
        },
        {
          "name": "pyproject.toml",
          "type": "blob",
          "size": 1.4,
          "content": "[tool.coverage.run]\nsource = [\"transformers\"]\nomit = [\n    \"*/convert_*\",\n    \"*/__main__.py\"\n]\n\n[tool.coverage.report]\nexclude_lines = [\n    \"pragma: no cover\",\n    \"raise\",\n    \"except\",\n    \"register_parameter\"\n]\n\n[tool.ruff]\nline-length = 119\n\n[tool.ruff.lint]\n# Never enforce `E501` (line length violations).\nignore = [\"C901\", \"E501\", \"E741\", \"F402\", \"F823\" ]\nselect = [\"C\", \"E\", \"F\", \"I\", \"W\"]\n\n# Ignore import violations in all `__init__.py` files.\n[tool.ruff.lint.per-file-ignores]\n\"__init__.py\" = [\"E402\", \"F401\", \"F403\", \"F811\"]\n\"src/transformers/file_utils.py\" = [\"F401\"]\n\"src/transformers/utils/dummy_*.py\" = [\"F401\"]\n\n[tool.ruff.lint.isort]\nlines-after-imports = 2\nknown-first-party = [\"transformers\"]\n\n[tool.ruff.format]\n# Like Black, use double quotes for strings.\nquote-style = \"double\"\n\n# Like Black, indent with spaces, rather than tabs.\nindent-style = \"space\"\n\n# Like Black, respect magic trailing commas.\nskip-magic-trailing-comma = false\n\n# Like Black, automatically detect the appropriate line ending.\nline-ending = \"auto\"\n\n[tool.pytest.ini_options]\naddopts = \"--doctest-glob='**/*.md'\"\ndoctest_optionflags=\"NUMBER NORMALIZE_WHITESPACE ELLIPSIS\"\nmarkers = [\n    \"flash_attn_test: marks tests related to flash attention (deselect with '-m \\\"not flash_attn_test\\\"')\",\n    \"bitsandbytes: select (or deselect with `not`) bitsandbytes integration tests\",\n    \"generate: marks tests that use the GenerationTesterMixin\"\n]\n"
        },
        {
          "name": "read_video.py",
          "type": "blob",
          "size": 1.62,
          "content": "import numpy as np\nimport cv2\nimport requests\nfrom yt_dlp import YoutubeDL\nfrom contextlib import redirect_stdout\nfrom pathlib import Path\nimport io\nimport imageio.v3 as iio\n\n\nurl = \"https://test-videos.co.uk/vids/bigbuckbunny/mp4/h264/720/Big_Buck_Bunny_720_10s_10MB.mp4\"\nvid = cv2.VideoCapture(url)\n# ret, frame = vid.read()\n\nwhile(True):\n    # Capture frame-by-frame\n    ret, frame = vid.read()\n    #print cap.isOpened(), ret\n    if frame is not None:\n        pass\n        # print(frame.shape)\n    else:\n        break\n\nprint(vid.isOpened(), frame is not None)\n\nbuffer = io.BytesIO(requests.get(url).content)\nvideo = buffer.getvalue()\nframes = iio.imread(video, index=None)\nprint(frames.shape)\n\n\n\n\n\nyoutube_id = \"https://www.youtube.com/watch?v=BaW_jenozKc\"\n\nctx = {\n    \"outtmpl\": \"-\",\n    'logtostderr': True\n}\n\nbuffer = io.BytesIO()\nwith redirect_stdout(buffer), YoutubeDL(ctx) as foo:\n    foo.download([youtube_id])\n# Path(f\"vi.mp4\").write_bytes(buffer.getvalue())\n\nvideo = buffer.getvalue()\nprint(type(video))\nframes = iio.imread(video, index=None)\nprint(frames.shape)\n\n\nimport decord\nfile_obj = io.BytesIO(video)\ncontainer = decord.VideoReader(file_obj)\nprint(container[2].shape)\n\n# print(np.frombuffer(video, dtype=np.uint8).shape)\n# img_array = np.asarray(bytearray(video), dtype=np.uint8)\n# im = cv2.imdecode(img_array, cv2.IMREAD_UNCHANGED)\n\n\n\nimport av\n\nfile_obj = io.BytesIO(video)\ncontainer = av.open(file_obj)\ncontainer.seek(0)\nframes = []\nfor i, frame in enumerate(container.decode(video=0)):\n    if i > 10:\n        break\n    if i >= 0:\n        frames.append(frame)\nout = np.stack([x.to_ndarray(format=\"rgb24\") for x in frames])\nprint(out.shape)\n"
        },
        {
          "name": "run.py",
          "type": "blob",
          "size": 5.03,
          "content": "import av\nimport torch\nimport decord\nfrom decord import VideoReader, cpu\n\nimport numpy as np\nfrom PIL import Image\nfrom huggingface_hub import hf_hub_download\nfrom transformers import LlavaNextVideoProcessor, LlavaNextVideoForConditionalGeneration, SiglipImageProcessor\n\nmodel_id = \"/raid/raushan/llava-next-video-qwen-7b\"\n\nmodel = LlavaNextVideoForConditionalGeneration.from_pretrained(\n    model_id, \n    torch_dtype=torch.bfloat16, \n    low_cpu_mem_usage=True, \n).to(0)\n\nprocessor = LlavaNextVideoProcessor.from_pretrained(model_id, torch_dtype=torch.bfloat16)\nimg_proc = SiglipImageProcessor.from_pretrained(\"google/siglip-so400m-patch14-384\")\n\nimage = Image.open(\"/raid/raushan/image.png\")\n\n\ndef load_video(video_path, max_frames_num,fps=1,force_sample=False):\n\n    vr = VideoReader(video_path)\n    total_frame_num = len(vr)\n    video_time = total_frame_num / vr.get_avg_fps()\n    fps = round(vr.get_avg_fps()/fps)\n    frame_idx = [i for i in range(0, len(vr), fps)]\n    frame_time = [i/fps for i in frame_idx]\n    if len(frame_idx) > max_frames_num or force_sample:\n        sample_fps = max_frames_num\n        uniform_sampled_frames = np.linspace(0, total_frame_num - 1, sample_fps, dtype=int)\n        frame_idx = uniform_sampled_frames.tolist()\n        frame_time = [i/vr.get_avg_fps() for i in frame_idx]\n    frame_time = \",\".join([f\"{i:.2f}s\" for i in frame_time])\n    spare_frames = vr.get_batch(frame_idx).asnumpy()\n    print(spare_frames.shape)\n    return spare_frames,frame_time,video_time\n\n\ndef read_video_pyav(container, indices):\n    '''\n    Decode the video with PyAV decoder.\n    Args:\n        container (`av.container.input.InputContainer`): PyAV container.\n        indices (`List[int]`): List of frame indices to decode.\n    Returns:\n        result (np.ndarray): np array of decoded frames of shape (num_frames, height, width, 3).\n    '''\n    frames = []\n    container.seek(0)\n    start_index = indices[0]\n    end_index = indices[-1]\n    for i, frame in enumerate(container.decode(video=0)):\n        if i > end_index:\n            break\n        if i >= start_index and i in indices:\n            frames.append(frame)\n    return np.stack([x.to_ndarray(format=\"rgb24\") for x in frames])\n\n\n# define a chat history and use `apply_chat_template` to get correctly formatted prompt\n# Each value in \"content\" has to be a list of dicts with types (\"text\", \"image\", \"video\") \n# <|im_start|>system\n# You are a helpful assistant.<|im_end|>\n# <|im_start|>user\n# <image>Time farmes are this moments and we ahev 64 frames\n# Please describe this video in detail.<|im_end|>\n# <|im_start|>assistant\n\nconversation = [\n    {\n\n        \"role\": \"system\",\n        \"content\": [\n            {\"type\": \"text\", \"text\": \"You are a helpful assistant.\"},\n            ],\n    },\n    {\n\n        \"role\": \"user\",\n        \"content\": [\n            {\"type\": \"text\", \"text\": \"The video lasts for 19.97 seconds, and 64 frames are uniformly sampled from it. These frames are located at 0.00s,0.30s,0.60s,0.93s,1.23s,1.57s,1.87s,2.20s,2.50s,2.83s,3.13s,3.47s,3.77s,4.10s,4.40s,4.73s,5.03s,5.37s,5.67s,6.00s,6.30s,6.63s,6.93s,7.27s,7.57s,7.90s,8.20s,8.53s,8.83s,9.17s,9.47s,9.80s,10.10s,10.43s,10.73s,11.07s,11.37s,11.70s,12.00s,12.33s,12.63s,12.97s,13.27s,13.60s,13.90s,14.23s,14.53s,14.87s,15.17s,15.50s,15.80s,16.13s,16.43s,16.77s,17.07s,17.40s,17.70s,18.03s,18.33s,18.67s,18.97s,19.30s,19.60s,19.93s.Please answer the following questions related to this video.\\nPlease describe this video in detail.\"},\n            {\"type\": \"video\"},\n            ],\n    },\n]\n\nprompt = processor.apply_chat_template(conversation, add_generation_prompt=True)\nprompt = \"<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n<|im_start|>user\\n<video>The video lasts for 19.97 seconds, and 64 frames are uniformly sampled from it. These frames are located at 0.00s,0.30s,0.60s,0.93s,1.23s,1.57s,1.87s,2.20s,2.50s,2.83s,3.13s,3.47s,3.77s,4.10s,4.40s,4.73s,5.03s,5.37s,5.67s,6.00s,6.30s,6.63s,6.93s,7.27s,7.57s,7.90s,8.20s,8.53s,8.83s,9.17s,9.47s,9.80s,10.10s,10.43s,10.73s,11.07s,11.37s,11.70s,12.00s,12.33s,12.63s,12.97s,13.27s,13.60s,13.90s,14.23s,14.53s,14.87s,15.17s,15.50s,15.80s,16.13s,16.43s,16.77s,17.07s,17.40s,17.70s,18.03s,18.33s,18.67s,18.97s,19.30s,19.60s,19.93s.Please answer the following questions related to this video.\\nPlease describe this video in detail.<|im_end|>\\n<|im_start|>assistant\"\n\nvideo_path = \"/raid/raushan/karate.mp4\" # hf_hub_download(repo_id=\"raushan-testing-hf/videos-test\", filename=\"sample_demo_1.mp4\", repo_type=\"dataset\")\ncontainer = av.open(video_path)\n\n# sample uniformly 8 frames from the video, can sample more for longer videos\ntotal_frames = container.streams.video[0].frames\nindices = np.arange(0, total_frames, total_frames / 64).astype(int)\nclip = read_video_pyav(container, indices)\n\nclip, frame_time,video_time = load_video(video_path, max_frames_num=64, force_sample=True)\ninputs_video = processor(text=prompt, videos=clip, return_tensors=\"pt\").to(device=model.device, dtype=torch.bfloat16)\n\noutput = model.generate(**inputs_video, max_new_tokens=100, do_sample=False)\nprint(processor.decode(output[0][2:], skip_special_tokens=True))\n"
        },
        {
          "name": "scripts",
          "type": "tree",
          "content": null
        },
        {
          "name": "setup.py",
          "type": "blob",
          "size": 16.37,
          "content": "# Copyright 2021 The HuggingFace Team. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"\nSimple check list from AllenNLP repo: https://github.com/allenai/allennlp/blob/main/setup.py\n\nTo create the package for pypi.\n\n1. Create the release branch named: v<RELEASE>-release, for example v4.19-release. For a patch release checkout the\n   current release branch.\n\n   If releasing on a special branch, copy the updated README.md on the main branch for the commit you will make\n   for the post-release and run `make fix-copies` on the main branch as well.\n\n2. Run `make pre-release` (or `make pre-patch` for a patch release) and commit these changes with the message:\n   \"Release: <VERSION>\" and push.\n\n3. Go back to the main branch and run `make post-release` then `make fix-copies`. Commit these changes with the\n   message \"v<NEXT_VERSION>.dev.0\" and push to main.\n\n# If you were just cutting the branch in preparation for a release, you can stop here for now.\n\n4. Wait for the tests on the release branch to be completed and be green (otherwise revert and fix bugs)\n\n5. On the release branch, add a tag in git to mark the release: \"git tag v<VERSION> -m 'Adds tag v<VERSION> for pypi' \"\n   Push the tag to git: git push --tags origin v<RELEASE>-release\n\n6. Build both the sources and the wheel. Do not change anything in setup.py between\n   creating the wheel and the source distribution (obviously).\n\n   Run `make build-release`. This will build the release and do some sanity checks for you. If this ends with an error\n   message, you need to fix things before going further.\n\n   You should now have a /dist directory with both .whl and .tar.gz source versions.\n\n7. Check that everything looks correct by uploading the package to the pypi test server:\n\n   twine upload dist/* -r testpypi\n   (pypi suggest using twine as other methods upload files via plaintext.)\n   You may have to specify the repository url, use the following command then:\n   twine upload dist/* -r testpypi --repository-url=https://test.pypi.org/legacy/\n\n   Check that you can install it in a virtualenv by running:\n   pip install -i https://testpypi.python.org/pypi transformers\n\n   Check you can run the following commands:\n   python -c \"from transformers import pipeline; classifier = pipeline('text-classification'); print(classifier('What a nice release'))\"\n   python -c \"from transformers import *\"\n   python utils/check_build.py --check_lib\n\n   If making a patch release, double check the bug you are patching is indeed resolved.\n\n8. Upload the final version to actual pypi:\n   twine upload dist/* -r pypi\n\n9. Copy the release notes from RELEASE.md to the tag in github once everything is looking hunky-dory.\n\"\"\"\n\nimport os\nimport re\nimport shutil\nfrom pathlib import Path\n\nfrom setuptools import Command, find_packages, setup\n\n\n# Remove stale transformers.egg-info directory to avoid https://github.com/pypa/pip/issues/5466\nstale_egg_info = Path(__file__).parent / \"transformers.egg-info\"\nif stale_egg_info.exists():\n    print(\n        (\n            \"Warning: {} exists.\\n\\n\"\n            \"If you recently updated transformers to 3.0 or later, this is expected,\\n\"\n            \"but it may prevent transformers from installing in editable mode.\\n\\n\"\n            \"This directory is automatically generated by Python's packaging tools.\\n\"\n            \"I will remove it now.\\n\\n\"\n            \"See https://github.com/pypa/pip/issues/5466 for details.\\n\"\n        ).format(stale_egg_info)\n    )\n    shutil.rmtree(stale_egg_info)\n\n\n# IMPORTANT:\n# 1. all dependencies should be listed here with their version requirements if any\n# 2. once modified, run: `make deps_table_update` to update src/transformers/dependency_versions_table.py\n_deps = [\n    \"Pillow>=10.0.1,<=15.0\",\n    \"accelerate>=0.26.0\",\n    \"av==9.2.0\",  # Latest version of PyAV (10.0.0) has issues with audio stream.\n    \"beautifulsoup4\",\n    \"blobfile\",\n    \"codecarbon>=2.8.1\",\n    \"cookiecutter==1.7.3\",\n    \"dataclasses\",\n    \"datasets!=2.5.0\",\n    \"deepspeed>=0.9.3\",\n    \"diffusers\",\n    \"dill<0.3.5\",\n    \"evaluate>=0.2.0\",\n    \"faiss-cpu\",\n    \"fastapi\",\n    \"filelock\",\n    \"flax>=0.4.1,<=0.7.0\",\n    \"fsspec<2023.10.0\",\n    \"ftfy\",\n    \"fugashi>=1.0\",\n    \"GitPython<3.1.19\",\n    \"hf-doc-builder>=0.3.0\",\n    \"huggingface-hub>=0.24.0,<1.0\",\n    \"importlib_metadata\",\n    \"ipadic>=1.0.0,<2.0\",\n    \"isort>=5.5.4\",\n    \"jax>=0.4.1,<=0.4.13\",\n    \"jaxlib>=0.4.1,<=0.4.13\",\n    \"jieba\",\n    \"jinja2>=3.1.0\",\n    \"kenlm\",\n    # Keras pin - this is to make sure Keras 3 doesn't destroy us. Remove or change when we have proper support.\n    \"keras>2.9,<2.16\",\n    \"keras-nlp>=0.3.1,<0.14.0\",  # keras-nlp 0.14 doesn't support keras 2, see pin on keras.\n    \"librosa\",\n    \"nltk<=3.8.1\",\n    \"natten>=0.14.6,<0.15.0\",\n    \"numpy>=1.17\",\n    \"onnxconverter-common\",\n    \"onnxruntime-tools>=1.4.2\",\n    \"onnxruntime>=1.4.0\",\n    \"opencv-python\",\n    \"optimum-benchmark>=0.3.0\",\n    \"optuna\",\n    \"optax>=0.0.8,<=0.1.4\",\n    \"packaging>=20.0\",\n    \"parameterized\",\n    \"phonemizer\",\n    \"protobuf\",\n    \"psutil\",\n    \"pyyaml>=5.1\",\n    \"pydantic\",\n    \"pytest>=7.2.0,<8.0.0\",\n    \"pytest-asyncio\",\n    \"pytest-timeout\",\n    \"pytest-xdist\",\n    \"python>=3.9.0\",\n    \"ray[tune]>=2.7.0\",\n    \"regex!=2019.12.17\",\n    \"requests\",\n    \"rhoknp>=1.1.0,<1.3.1\",\n    \"rjieba\",\n    \"rouge-score!=0.0.7,!=0.0.8,!=0.1,!=0.1.1\",\n    \"ruff==0.5.1\",\n    \"sacrebleu>=1.4.12,<2.0.0\",\n    \"sacremoses\",\n    \"safetensors>=0.4.1\",\n    \"sagemaker>=2.31.0\",\n    \"schedulefree>=1.2.6\",\n    \"scikit-learn\",\n    \"scipy<1.13.0\",  # SciPy >= 1.13.0 is not supported with the current jax pin (`jax>=0.4.1,<=0.4.13`)\n    \"sentencepiece>=0.1.91,!=0.1.92\",\n    \"sigopt\",\n    \"starlette\",\n    \"sudachipy>=0.6.6\",\n    \"sudachidict_core>=20220729\",\n    \"tensorboard\",\n    # TensorFlow pin. When changing this value, update examples/tensorflow/_tests_requirements.txt accordingly\n    \"tensorflow-cpu>2.9,<2.16\",\n    \"tensorflow>2.9,<2.16\",\n    \"tensorflow-text<2.16\",\n    \"tensorflow-probability<0.24\",\n    \"tf2onnx\",\n    \"timeout-decorator\",\n    \"tiktoken\",\n    \"timm<=1.0.11\",\n    \"tokenizers>=0.21,<0.22\",\n    \"torch>=2.0\",\n    \"torchaudio\",\n    \"torchvision\",\n    \"pyctcdecode>=0.4.0\",\n    \"tqdm>=4.27\",\n    \"unidic>=1.0.2\",\n    \"unidic_lite>=1.0.7\",\n    \"urllib3<2.0.0\",\n    \"uvicorn\",\n    \"pytest-rich\",\n    \"libcst\",\n    \"rich\",\n]\n\n\n# this is a lookup table with items like:\n#\n# tokenizers: \"tokenizers==0.9.4\"\n# packaging: \"packaging\"\n#\n# some of the values are versioned whereas others aren't.\ndeps = {b: a for a, b in (re.findall(r\"^(([^!=<>~ ]+)(?:[!=<>~ ].*)?$)\", x)[0] for x in _deps)}\n\n# since we save this data in src/transformers/dependency_versions_table.py it can be easily accessed from\n# anywhere. If you need to quickly access the data from this table in a shell, you can do so easily with:\n#\n# python -c 'import sys; from transformers.dependency_versions_table import deps; \\\n# print(\" \".join([ deps[x] for x in sys.argv[1:]]))' tokenizers datasets\n#\n# Just pass the desired package names to that script as it's shown with 2 packages above.\n#\n# If transformers is not yet installed and the work is done from the cloned repo remember to add `PYTHONPATH=src` to the script above\n#\n# You can then feed this for example to `pip`:\n#\n# pip install -U $(python -c 'import sys; from transformers.dependency_versions_table import deps; \\\n# print(\" \".join([deps[x] for x in sys.argv[1:]]))' tokenizers datasets)\n#\n\n\ndef deps_list(*pkgs):\n    return [deps[pkg] for pkg in pkgs]\n\n\nclass DepsTableUpdateCommand(Command):\n    \"\"\"\n    A custom distutils command that updates the dependency table.\n    usage: python setup.py deps_table_update\n    \"\"\"\n\n    description = \"build runtime dependency table\"\n    user_options = [\n        # format: (long option, short option, description).\n        (\"dep-table-update\", None, \"updates src/transformers/dependency_versions_table.py\"),\n    ]\n\n    def initialize_options(self):\n        pass\n\n    def finalize_options(self):\n        pass\n\n    def run(self):\n        entries = \"\\n\".join([f'    \"{k}\": \"{v}\",' for k, v in deps.items()])\n        content = [\n            \"# THIS FILE HAS BEEN AUTOGENERATED. To update:\",\n            \"# 1. modify the `_deps` dict in setup.py\",\n            \"# 2. run `make deps_table_update``\",\n            \"deps = {\",\n            entries,\n            \"}\",\n            \"\",\n        ]\n        target = \"src/transformers/dependency_versions_table.py\"\n        print(f\"updating {target}\")\n        with open(target, \"w\", encoding=\"utf-8\", newline=\"\\n\") as f:\n            f.write(\"\\n\".join(content))\n\n\nextras = {}\n\nextras[\"ja\"] = deps_list(\"fugashi\", \"ipadic\", \"unidic_lite\", \"unidic\", \"sudachipy\", \"sudachidict_core\", \"rhoknp\")\nextras[\"sklearn\"] = deps_list(\"scikit-learn\")\n\nextras[\"tf\"] = deps_list(\"tensorflow\", \"onnxconverter-common\", \"tf2onnx\", \"tensorflow-text\", \"keras-nlp\")\nextras[\"tf-cpu\"] = deps_list(\n    \"keras\",\n    \"tensorflow-cpu\",\n    \"onnxconverter-common\",\n    \"tf2onnx\",\n    \"tensorflow-text\",\n    \"keras-nlp\",\n    \"tensorflow-probability\",\n)\n\nextras[\"torch\"] = deps_list(\"torch\", \"accelerate\")\nextras[\"accelerate\"] = deps_list(\"accelerate\")\n\nif os.name == \"nt\":  # windows\n    extras[\"retrieval\"] = deps_list(\"datasets\")  # faiss is not supported on windows\n    extras[\"flax\"] = []  # jax is not supported on windows\nelse:\n    extras[\"retrieval\"] = deps_list(\"faiss-cpu\", \"datasets\")\n    extras[\"flax\"] = deps_list(\"jax\", \"jaxlib\", \"flax\", \"optax\", \"scipy\")\n\nextras[\"tokenizers\"] = deps_list(\"tokenizers\")\nextras[\"ftfy\"] = deps_list(\"ftfy\")\nextras[\"onnxruntime\"] = deps_list(\"onnxruntime\", \"onnxruntime-tools\")\nextras[\"onnx\"] = deps_list(\"onnxconverter-common\", \"tf2onnx\") + extras[\"onnxruntime\"]\nextras[\"modelcreation\"] = deps_list(\"cookiecutter\")\n\nextras[\"sagemaker\"] = deps_list(\"sagemaker\")\nextras[\"deepspeed\"] = deps_list(\"deepspeed\") + extras[\"accelerate\"]\nextras[\"optuna\"] = deps_list(\"optuna\")\nextras[\"ray\"] = deps_list(\"ray[tune]\")\nextras[\"sigopt\"] = deps_list(\"sigopt\")\n\nextras[\"integrations\"] = extras[\"optuna\"] + extras[\"ray\"] + extras[\"sigopt\"]\n\nextras[\"serving\"] = deps_list(\"pydantic\", \"uvicorn\", \"fastapi\", \"starlette\")\nextras[\"audio\"] = deps_list(\"librosa\", \"pyctcdecode\", \"phonemizer\", \"kenlm\")\n# `pip install \".[speech]\"` is deprecated and `pip install \".[torch-speech]\"` should be used instead\nextras[\"speech\"] = deps_list(\"torchaudio\") + extras[\"audio\"]\nextras[\"torch-speech\"] = deps_list(\"torchaudio\") + extras[\"audio\"]\nextras[\"tf-speech\"] = extras[\"audio\"]\nextras[\"flax-speech\"] = extras[\"audio\"]\nextras[\"vision\"] = deps_list(\"Pillow\")\nextras[\"timm\"] = deps_list(\"timm\")\nextras[\"torch-vision\"] = deps_list(\"torchvision\") + extras[\"vision\"]\nextras[\"natten\"] = deps_list(\"natten\")\nextras[\"codecarbon\"] = deps_list(\"codecarbon\")\nextras[\"video\"] = deps_list(\"av\")\n\nextras[\"sentencepiece\"] = deps_list(\"sentencepiece\", \"protobuf\")\nextras[\"tiktoken\"] = deps_list(\"tiktoken\", \"blobfile\")\nextras[\"testing\"] = (\n    deps_list(\n        \"pytest\",\n        \"pytest-asyncio\",\n        \"pytest-rich\",\n        \"pytest-xdist\",\n        \"timeout-decorator\",\n        \"parameterized\",\n        \"psutil\",\n        \"datasets\",\n        \"dill\",\n        \"evaluate\",\n        \"pytest-timeout\",\n        \"ruff\",\n        \"sacrebleu\",\n        \"rouge-score\",\n        \"nltk\",\n        \"GitPython\",\n        \"sacremoses\",\n        \"rjieba\",\n        \"beautifulsoup4\",\n        \"tensorboard\",\n        \"pydantic\",\n        \"sentencepiece\",\n    )\n    + extras[\"retrieval\"]\n    + extras[\"modelcreation\"]\n)\n\nextras[\"deepspeed-testing\"] = extras[\"deepspeed\"] + extras[\"testing\"] + extras[\"optuna\"] + extras[\"sentencepiece\"]\nextras[\"ruff\"] = deps_list(\"ruff\")\nextras[\"quality\"] = deps_list(\"datasets\", \"isort\", \"ruff\", \"GitPython\", \"urllib3\", \"libcst\", \"rich\")\n\nextras[\"all\"] = (\n    extras[\"tf\"]\n    + extras[\"torch\"]\n    + extras[\"flax\"]\n    + extras[\"sentencepiece\"]\n    + extras[\"tokenizers\"]\n    + extras[\"torch-speech\"]\n    + extras[\"vision\"]\n    + extras[\"integrations\"]\n    + extras[\"timm\"]\n    + extras[\"torch-vision\"]\n    + extras[\"codecarbon\"]\n    + extras[\"accelerate\"]\n    + extras[\"video\"]\n)\n\n\nextras[\"dev-torch\"] = (\n    extras[\"testing\"]\n    + extras[\"torch\"]\n    + extras[\"sentencepiece\"]\n    + extras[\"tokenizers\"]\n    + extras[\"torch-speech\"]\n    + extras[\"vision\"]\n    + extras[\"integrations\"]\n    + extras[\"timm\"]\n    + extras[\"torch-vision\"]\n    + extras[\"codecarbon\"]\n    + extras[\"quality\"]\n    + extras[\"ja\"]\n    + extras[\"sklearn\"]\n    + extras[\"modelcreation\"]\n    + extras[\"onnxruntime\"]\n)\nextras[\"dev-tensorflow\"] = (\n    extras[\"testing\"]\n    + extras[\"tf\"]\n    + extras[\"sentencepiece\"]\n    + extras[\"tokenizers\"]\n    + extras[\"vision\"]\n    + extras[\"quality\"]\n    + extras[\"sklearn\"]\n    + extras[\"modelcreation\"]\n    + extras[\"onnx\"]\n    + extras[\"tf-speech\"]\n)\nextras[\"dev\"] = (\n    extras[\"all\"] + extras[\"testing\"] + extras[\"quality\"] + extras[\"ja\"] + extras[\"sklearn\"] + extras[\"modelcreation\"]\n)\n\nextras[\"torchhub\"] = deps_list(\n    \"filelock\",\n    \"huggingface-hub\",\n    \"importlib_metadata\",\n    \"numpy\",\n    \"packaging\",\n    \"protobuf\",\n    \"regex\",\n    \"requests\",\n    \"sentencepiece\",\n    \"torch\",\n    \"tokenizers\",\n    \"tqdm\",\n)\n\nextras[\"agents\"] = deps_list(\n    \"diffusers\", \"accelerate\", \"datasets\", \"torch\", \"sentencepiece\", \"opencv-python\", \"Pillow\"\n)\n\nextras[\"benchmark\"] = deps_list(\"optimum-benchmark\")\n\n# when modifying the following list, make sure to update src/transformers/dependency_versions_check.py\ninstall_requires = [\n    deps[\"filelock\"],  # filesystem locks, e.g., to prevent parallel downloads\n    deps[\"huggingface-hub\"],\n    deps[\"numpy\"],\n    deps[\"packaging\"],  # utilities from PyPA to e.g., compare versions\n    deps[\"pyyaml\"],  # used for the model cards metadata\n    deps[\"regex\"],  # for OpenAI GPT\n    deps[\"requests\"],  # for downloading models over HTTPS\n    deps[\"tokenizers\"],\n    deps[\"safetensors\"],\n    deps[\"tqdm\"],  # progress bars in model download and training scripts\n]\n\nsetup(\n    name=\"transformers\",\n    version=\"4.49.0.dev0\",  # expected format is one of x.y.z.dev0, or x.y.z.rc1 or x.y.z (no to dashes, yes to dots)\n    author=\"The Hugging Face team (past and future) with the help of all our contributors (https://github.com/huggingface/transformers/graphs/contributors)\",\n    author_email=\"transformers@huggingface.co\",\n    description=\"State-of-the-art Machine Learning for JAX, PyTorch and TensorFlow\",\n    long_description=open(\"README.md\", \"r\", encoding=\"utf-8\").read(),\n    long_description_content_type=\"text/markdown\",\n    keywords=\"NLP vision speech deep learning transformer pytorch tensorflow jax BERT GPT-2 Wav2Vec2 ViT\",\n    license=\"Apache 2.0 License\",\n    url=\"https://github.com/huggingface/transformers\",\n    package_dir={\"\": \"src\"},\n    packages=find_packages(\"src\"),\n    include_package_data=True,\n    package_data={\"\": [\"**/*.cu\", \"**/*.cpp\", \"**/*.cuh\", \"**/*.h\", \"**/*.pyx\"]},\n    zip_safe=False,\n    extras_require=extras,\n    entry_points={\"console_scripts\": [\"transformers-cli=transformers.commands.transformers_cli:main\"]},\n    python_requires=\">=3.9.0\",\n    install_requires=list(install_requires),\n    classifiers=[\n        \"Development Status :: 5 - Production/Stable\",\n        \"Intended Audience :: Developers\",\n        \"Intended Audience :: Education\",\n        \"Intended Audience :: Science/Research\",\n        \"License :: OSI Approved :: Apache Software License\",\n        \"Operating System :: OS Independent\",\n        \"Programming Language :: Python :: 3\",\n        \"Programming Language :: Python :: 3.9\",\n        \"Programming Language :: Python :: 3.10\",\n        \"Topic :: Scientific/Engineering :: Artificial Intelligence\",\n    ],\n    cmdclass={\"deps_table_update\": DepsTableUpdateCommand},\n)\n\nextras[\"tests_torch\"] = deps_list()\nextras[\"tests_tf\"] = deps_list()\nextras[\"tests_flax\"] = deps_list()\nextras[\"tests_torch_and_tf\"] = deps_list()\nextras[\"tests_torch_and_flax\"] = deps_list()\nextras[\"tests_hub\"] = deps_list()\nextras[\"tests_pipelines_torch\"] = deps_list()\nextras[\"tests_pipelines_tf\"] = deps_list()\nextras[\"tests_onnx\"] = deps_list()\nextras[\"tests_examples_torch\"] = deps_list()\nextras[\"tests_examples_tf\"] = deps_list()\nextras[\"tests_custom_tokenizers\"] = deps_list()\nextras[\"tests_exotic_models\"] = deps_list()\nextras[\"consistency\"] = deps_list()\n"
        },
        {
          "name": "src",
          "type": "tree",
          "content": null
        },
        {
          "name": "templates",
          "type": "tree",
          "content": null
        },
        {
          "name": "tests",
          "type": "tree",
          "content": null
        },
        {
          "name": "utils",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}