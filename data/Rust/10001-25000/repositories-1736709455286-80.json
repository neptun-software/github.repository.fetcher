{
  "metadata": {
    "timestamp": 1736709455286,
    "page": 80,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjgw",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "quickwit-oss/tantivy",
      "stars": 12479,
      "defaultBranch": "main",
      "files": [
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.16796875,
          "content": "tantivy.iml\n.cargo\nproptest-regressions\n*.swp\ntarget\ntarget/debug\n.vscode\ntarget/release\nCargo.lock\nbenchmark\n.DS_Store\n*.bk\n.idea\ntrace.dat\ncargo-timing*\ncontrol\nvariable\n"
        },
        {
          "name": "ARCHITECTURE.md",
          "type": "blob",
          "size": 16.5947265625,
          "content": "# Tantivy\n\n## What is tantivy?\n\nTantivy is a library that is meant to build search engines. Although it is by no means a port of Lucene, its architecture is strongly inspired by it. If you are familiar with Lucene, you may be struck by the overlapping vocabulary.\nThis is not fortuitous.\n\nTantivy's bread and butter is to address the problem of full-text search :\n\nGiven a large set of textual documents, and a text query, return the K-most relevant documents in a very efficient way. To execute these queries rapidly, the tantivy needs to build an index beforehand. The relevance score implemented in the tantivy is not configurable. Tantivy uses the same score as the default similarity used in Lucene / Elasticsearch, called [BM25](https://en.wikipedia.org/wiki/Okapi_BM25).\n\nBut tantivy's scope does not stop there. Numerous features are required to power rich-search applications. For instance, one may want to:\n\n- compute the count of documents matching a query in the different section of an e-commerce website,\n- display an average price per meter square for a real estate search engine,\n- take into account historical user data to rank documents in a specific way,\n- or even use tantivy to power an OLAP database.\n\nA more abstract description of the problem space tantivy is trying to address is the following.\n\nIngest a large set of documents, create an index that makes it possible to\nrapidly select all documents matching a given predicate (also known as a query) and\ncollect some information about them ([See collector](#collector-define-what-to-do-with-matched-documents)).\n\nRoughly speaking the design is following these guiding principles:\n\n- Search should be O(1) in memory.\n- Indexing should be O(1) in memory. (In practice it is just sublinear)\n- Search should be as fast as possible\n\nThis comes at the cost of the dynamicity of the index: while it is possible to add, and delete documents from our corpus, the tantivy is designed to handle these updates in large batches.\n\n## [core/](src/core): Index, segments, searchers\n\nCore contains all of the high-level code to make it possible to create an index, add documents, delete documents and commit.\n\nThis is both the most high-level part of tantivy, the least performance-sensitive one, the seemingly most mundane code... And paradoxically the most complicated part.\n\n### Index and Segments\n\nA tantivy index is a collection of smaller independent immutable segments.\nEach segment contains its own independent set of data structures.\n\nA segment is identified by a segment id that is in fact a UUID.\nThe file of a segment has the format\n\n ```segment-id . ext```\n\nThe extension signals which data structure (or [`SegmentComponent`](src/index/segment_component.rs)) is stored in the file.\n\nA small `meta.json` file is in charge of keeping track of the list of segments, as well as the schema.\n\nOn commit, one segment per indexing thread is written to disk, and the `meta.json` is then updated atomically.\n\nFor a better idea of how indexing works, you may read the [following blog post](https://fulmicoton.com/posts/behold-tantivy-part2/).\n\n### Deletes\n\nDeletes happen by deleting a \"term\". Tantivy does not offer any notion of primary id, so it is up to the user to use a field in their schema as if it was a primary id, and delete the associated term if they want to delete only one specific document.\n\nOn commit, tantivy will find all of the segments with documents matching this existing term and remove from [alive bitset file](src/fastfield/alive_bitset.rs) that represents the bitset of the alive document ids.\nLike all segment files, this file is immutable. Because it is possible to have more than one alive bitset file at a given instant, the alive bitset filename has the format ```segment_id . commit_opstamp . del```.\n\nAn opstamp is simply an incremental id that identifies any operation applied to the index. For instance, performing a commit or adding a document.\n\n### DocId\n\nWithin a segment, all documents are identified by a DocId that ranges within `[0, max_doc)`.\nwhere `max_doc` is the number of documents in the segment, (deleted or not). Having such a compact `DocId` space is key to the compression of our data structures.\n\nThe DocIds are simply allocated in the order documents are added to the index.\n\n### Merges\n\nIn separate threads, tantivy's index writer search for opportunities to merge segments.\nThe point of segment merge is to:\n\n- eventually get rid of tombstoned documents\n- reduce the otherwise ever-growing number of segments.\n\nIndeed, while having several segments instead of one does not hurt search too much, having hundreds can have a measurable impact on the search performance.\n\n### Searcher\n\nThe user of the library usually does not need to know about the existence of Segments.\nSearching is done through an object called a [`Searcher`](src/core/searcher.rs), that captures a\nsnapshot of the index at one point of time, by holding a list of [SegmentReader](src/core/segment_reader.rs).\n\nIn other words, regardless of commits, file garbage collection, or segment merge that might happen, as long as the user holds and reuse the same [Searcher](src/core/searcher.rs), search will happen on an immutable snapshot of the index.\n\n## [directory/](src/directory): Where should the data be stored?\n\nTantivy, like Lucene, abstracts the place where the data should be stored in a key-trait\ncalled [`Directory`](src/directory/directory.rs).\nContrary to Lucene however, \"files\" are quite different from some kind of `io::Read` object.\nCheck out [`src/directory/directory.rs`](src/directory/directory.rs) trait for more details.\n\nTantivy ships two main directory implementation: the `MmapDirectory` and the `RamDirectory`,\nbut users can extend tantivy with their own implementation.\n\n## [schema/](src/schema): What are documents?\n\nTantivy's document follows a very strict schema, decided before building any index.\n\nThe schema defines all of the fields that the indexes [`Document`](src/schema/document/mod.rs) may and should contain, their types (`text`, `i64`, `u64`, `Date`, ...) as well as how it should be indexed / represented in tantivy.\n\nDepending on the type of the field, you can decide to\n\n- put it in the docstore\n- store it as a fast field\n- index it\n\nPractically, tantivy will push values associated with this type to up to 3 respective\ndata structures.\n\n*Limitations*\n\nAs of today, tantivy's schema imposes a 1:1 relationship between a field that is being ingested and a field represented in the search index. In sophisticated search application, it is fairly common to want to index a field twice using different tokenizers, or to index the concatenation of several fields together into one field.\n\nThis is not something tantivy supports, and it is up to the user to duplicate field / concatenate fields before feeding them to tantivy.\n\n## General information about these data structures\n\nAll data structures in tantivy, have:\n\n- a writer\n- a serializer\n- a reader\n\nThe writer builds an in-memory representation of a batch of documents. This representation is not searchable. It is just meant as an intermediary mutable representation, to which we can sequentially add\nthe document of a batch. At the end of the batch (or if a memory limit is reached), this representation\nis then converted into an on-disk immutable representation, that is extremely compact.\nThis conversion is done by the serializer.\n\nFinally, the reader is in charge of offering an API to read on this on-disk read-only representation.\nIn tantivy, readers are designed to require very little anonymous memory. The data is read straight from an mmapped file, and loading an index is as fast as mmapping its files.\n\n## [store/](src/store): Here is my DocId, Gimme my document\n\nThe docstore is a row-oriented storage that, for each document, stores a subset of the fields\nthat are marked as stored in the schema. The docstore is compressed using a general-purpose algorithm\nlike LZ4.\n\n**Useful for**\n\nIn search engines, it is often used to display search results.\nOnce the top 10 documents have been identified, we fetch them from the store, and display them or their snippet on the search result page (aka SERP).\n\n**Not useful for**\n\nFetching a document from the store is typically a \"slow\" operation. It usually consists in\n\n- searching into a compact tree-like data structure to find the position of the right block.\n- decompressing a small block\n- returning the document from this block.\n\nIt is NOT meant to be called for every document matching a query.\n\nAs a rule of thumb, if you hit the docstore more than 100 times per search query, you are probably misusing tantivy.\n\n## [fastfield/](src/fastfield): Here is my DocId, Gimme my value\n\nFast fields are stored in a column-oriented storage that allows for random access.\nThe only compression applied is bitpacking. The column comes with two meta data.\nThe minimum value in the column and the number of bits per doc.\n\nFetching a value for a `DocId` is then as simple as computing\n\n```rust\nmin_value + fetch_bits(num_bits * doc_id..num_bits * (doc_id+1))\n```\n\nThis operation just requires one memory fetch.\nBecause, DocSets are scanned through in order (DocId are iterated in a sorted manner) which\nalso help locality.\n\nIn Lucene's jargon, fast fields are called DocValues.\n\n**Useful for**\n\nThey are typically integer values that are useful to either rank or compute aggregate over\nall of the documents matching a query (aka [DocSet](src/docset.rs)).\n\nFor instance, one could define a function to combine upvotes with tantivy's internal relevancy score.\nThis can be done by fetching a fast field during scoring.\nOne could also compute the mean price of the items matching a query in an e-commerce website.\nThis can be done by fetching a fast field in a collector.\nFinally one could decide to post-filter a docset to remove docset with a price within a specific range.\nIf the ratio of filtered out documents is not too low, an efficient way to do this is to fetch the price and apply the filter on the collector side.\n\nAside from integer values, it is also possible to store an actual byte payload.\nFor advanced search engine, it is possible to store all of the features required for learning-to-rank in a byte payload, access it during search, and apply the learning-to-rank model.\n\nFinally facets are a specific kind of fast field, and the associated source code is in [`fastfield/facet_reader.rs`](src/fastfield/facet_reader.rs).\n\n# The inverted search index\n\nThe inverted index is the core part of full-text search.\nWhen presented a new document with the text field \"Hello, happy tax payer!\", tantivy breaks it into a list of so-called tokens. In addition to just splitting these strings into tokens, it might also do different kinds of operations like dropping the punctuation, converting the character to lowercase, apply stemming, etc. Tantivy makes it possible to configure the operations to be applied in the schema (tokenizer/ is the place where these operations are implemented).\n\nFor instance, the default tokenizer of tantivy would break our text into: `[hello, happy, tax, payer]`.\nThe document will therefore be registered in the inverted index as containing the terms\n`[text:hello, text:happy, text:tax, text:payer]`.\n\nThe role of the inverted index is, when given a term, gives us in return a very fast iterator over the sorted doc ids that match the term.\n\nSuch an iterator is called a posting list. In addition to giving us `DocId`, they can also give us optionally the number of occurrence of the term for each document, also called term frequency or TF.\n\nThese iterators being sorted by DocId, one can create an iterator over the document containing `text:tax AND text:payer`, `(text:tax AND text:payer) OR (text:contribuable)` or any boolean expression.\n\nIn order to represent the function\n```Term ⟶ Posting```\n\nThe inverted index actually consists of two data structures chained together.\n\n- [Term](src/schema/term.rs) ⟶ [TermInfo](src/postings/term_info.rs) is addressed by the term dictionary.\n- [TermInfo](src/postings/term_info.rs) ⟶ [Posting](src/postings/postings.rs) is addressed by the posting lists.\n\nWhere [TermInfo](src/postings/term_info.rs) is an object containing some meta data about a term.\n\n## [termdict/](src/termdict): Here is a term, give me the [TermInfo](src/postings/term_info.rs)\n\nTantivy's term dictionary is mainly in charge of supplying the function\n\n[Term](src/schema/term.rs) ⟶ [TermInfo](src/postings/term_info.rs)\n\nIt is itself broken into two parts.\n\n- [Term](src/schema/term.rs) ⟶ [TermOrdinal](src/termdict/mod.rs) is addressed by a finite state transducer, implemented by the fst crate.\n- [TermOrdinal](src/termdict/mod.rs) ⟶ [TermInfo](src/postings/term_info.rs) is addressed by the term info store.\n\n## [postings/](src/postings): Iterate over documents... very fast\n\nA posting list makes it possible to store a sorted list of doc ids and for each doc store\na term frequency as well.\n\nThe posting lists are stored in a separate file. The [TermInfo](src/postings/term_info.rs) contains an offset into that file and a number of documents for the given posting list. Both are required and sufficient to read the posting list.\n\nThe posting list is organized in block of 128 documents.\nOne block of doc ids is followed by one block of term frequencies.\n\nThe doc ids are delta encoded and bitpacked.\nThe term frequencies are bitpacked.\n\nBecause the number of docs is rarely a multiple of 128, the last block may contain an arbitrary number of docs between 1 and 127 documents. We then use variable int encoding instead of bitpacking.\n\n## [positions/](src/positions): Where are my terms within the documents?\n\nPhrase queries make it possible to search for documents containing a specific sequence of terms.\nFor instance, when the phrase query \"the art of war\" does not match \"the war of art\".\nTo make it possible, it is possible to specify in the schema that a field should store positions in addition to being indexed.\n\nThe token positions of all of the terms are then stored in a separate file with the extension `.pos`.\nThe [TermInfo](src/postings/term_info.rs) gives an offset (expressed in position this time) in this file. As we iterate through the docset,\nwe advance the position reader by the number of term frequencies of the current document.\n\n## [fieldnorm/](src/fieldnorm): Here is my doc, how many tokens in this field?\n\nThe [BM25](https://en.wikipedia.org/wiki/Okapi_BM25) formula also requires to know the number of tokens stored in a specific field for a given document. We store this information on one byte per document in the fieldnorm.\nThe fieldnorm is therefore compressed. Values up to 40 are encoded unchanged.\n\n## [tokenizer/](src/tokenizer): How should we process text?\n\nText processing is key to a good search experience.\nSplits or normalize your text too much, and the search results will have a less precision and a higher recall.\nDo not normalize, or under split your text, you will end up with a higher precision and a lesser recall.\n\nText processing can be configured by selecting an off-the-shelf [`Tokenizer`](./src/tokenizer/tokenizer.rs) or implementing your own to first split the text into tokens, and then chain different [`TokenFilter`](src/tokenizer/tokenizer.rs)'s to it.\n\nTantivy's comes with few tokenizers, but external crates are offering advanced tokenizers, such as [Lindera](https://crates.io/crates/lindera) for Japanese.\n\n## [query/](src/query): Define and compose queries\n\nThe [Query](src/query/query.rs) trait defines what a query is.\nDue to the necessity for some queries to compute some statistics over the entire index, and because the\nindex is composed of several `SegmentReader`, the path from transforming a `Query` to an iterator over documents is slightly convoluted, but fundamentally, this is what a Query is.\n\nThe iterator over a document comes with some scoring function. The resulting trait is called a\n[Scorer](src/query/scorer.rs) and is specific to a segment.\n\nDifferent queries can be combined using the [BooleanQuery](src/query/boolean_query/).\nTantivy comes with different types of queries and can be extended by implementing\nthe `Query`, `Weight`, and `Scorer` traits.\n\n## [collector](src/collector): Define what to do with matched documents\n\nCollectors define how to aggregate the documents matching a query, in the broadest sense possible.\nThe search will push matched documents one by one, calling their\n`fn collect(doc: DocId, score: Score);` method.\n\nUsers may implement their own collectors by implementing the [Collector](src/collector/mod.rs) trait.\n\n## [query-grammar](query-grammar): Defines the grammar of the query parser\n\nWhile the [QueryParser](src/query/query_parser/query_parser.rs) struct is located in the `query/` directory, the actual parser combinator used to convert user queries into an AST is in an external crate called `query-grammar`. This part was externalized to lighten the work of the compiler.\n"
        },
        {
          "name": "AUTHORS",
          "type": "blob",
          "size": 0.3046875,
          "content": "# This is the list of authors of tantivy for copyright purposes.\nPaul Masurel\nLaurentiu Nicola\nDru Sellers\nAshley Mannix\nMichael J. Curry\nJason Wolfe\n# As an employee of Google I am required to add Google LLC\n# in the list of authors, but this project is not affiliated to Google\n# in any other way.\nGoogle LLC \n"
        },
        {
          "name": "CHANGELOG.md",
          "type": "blob",
          "size": 48.31640625,
          "content": "Tantivy 0.23 - Unreleased\n================================\nTantivy 0.23 will be backwards compatible with indices created with v0.22 and v0.21. The new minimum rust version will be 1.75.\n\n#### Bugfixes\n- fix potential endless loop in merge [#2457](https://github.com/quickwit-oss/tantivy/pull/2457)(@PSeitz)\n- fix bug that causes out-of-order sstable key. [#2445](https://github.com/quickwit-oss/tantivy/pull/2445)(@fulmicoton)\n- fix ReferenceValue API flaw [#2372](https://github.com/quickwit-oss/tantivy/pull/2372)(@PSeitz)\n- fix `OwnedBytes` debug panic [#2512](https://github.com/quickwit-oss/tantivy/pull/2512)(@b41sh)\n\n#### Breaking API Changes\n- remove index sorting [#2434](https://github.com/quickwit-oss/tantivy/pull/2434)(@PSeitz)\n\n#### Features/Improvements\n- **Aggregation**\n    - Support for cardinality aggregation [#2337](https://github.com/quickwit-oss/tantivy/pull/2337) [#2446](https://github.com/quickwit-oss/tantivy/pull/2446) (@raphaelcoeffic @PSeitz)\n    - Support for extended stats aggregation [#2247](https://github.com/quickwit-oss/tantivy/pull/2247)(@giovannicuccu)\n    - Add Key::I64 and Key::U64 variants in aggregation to avoid f64 precision issues [#2468](https://github.com/quickwit-oss/tantivy/pull/2468)(@PSeitz)\n    - Faster term aggregation fetch terms [#2447](https://github.com/quickwit-oss/tantivy/pull/2447)(@PSeitz)\n    - Improve custom order deserialization [#2451](https://github.com/quickwit-oss/tantivy/pull/2451)(@PSeitz)\n    - Change AggregationLimits behavior [#2495](https://github.com/quickwit-oss/tantivy/pull/2495)(@PSeitz)\n    - lower contention on AggregationLimits [#2394](https://github.com/quickwit-oss/tantivy/pull/2394)(@PSeitz)\n    - fix postcard compatibility for top_hits, add postcard test [#2346](https://github.com/quickwit-oss/tantivy/pull/2346)(@PSeitz)\n    - reduce top hits memory consumption [#2426](https://github.com/quickwit-oss/tantivy/pull/2426)(@PSeitz)\n    - check unsupported parameters top_hits [#2351](https://github.com/quickwit-oss/tantivy/pull/2351)(@PSeitz)\n    - Change AggregationLimits to AggregationLimitsGuard [#2495](https://github.com/quickwit-oss/tantivy/pull/2495)(@PSeitz)\n- **Range Queries**\n    - Support fast field range queries on json fields [#2456](https://github.com/quickwit-oss/tantivy/pull/2456)(@PSeitz)\n    - Add support for str fast field range query [#2460](https://github.com/quickwit-oss/tantivy/pull/2460) [#2452](https://github.com/quickwit-oss/tantivy/pull/2452) [#2453](https://github.com/quickwit-oss/tantivy/pull/2453)(@PSeitz)\n    - modify fastfield range query heuristic [#2375](https://github.com/quickwit-oss/tantivy/pull/2375)(@trinity-1686a)\n    - add FastFieldRangeQuery for explicit range queries on fast field (for `RangeQuery` it is autodetected) [#2477](https://github.com/quickwit-oss/tantivy/pull/2477)(@PSeitz)\n\n- add format backwards-compatibility tests [#2485](https://github.com/quickwit-oss/tantivy/pull/2485)(@PSeitz)\n- add columnar format compatibility tests [#2433](https://github.com/quickwit-oss/tantivy/pull/2433)(@PSeitz)\n- Improved snippet ranges algorithm [#2474](https://github.com/quickwit-oss/tantivy/pull/2474)(@gezihuzi)\n- make find_field_with_default return json fields without path [#2476](https://github.com/quickwit-oss/tantivy/pull/2476)(@trinity-1686a)\n- feat(query): Make `BooleanQuery` support `minimum_number_should_match` [#2405](https://github.com/quickwit-oss/tantivy/pull/2405)(@LebranceBW)\n\n- **RegexPhraseQuery** \n`RegexPhraseQuery` supports phrase queries with regex. E.g. query \"b.* b.* wolf\" matches \"big bad wolf\". Slop is supported as well: \"b.* wolf\"~2 matches \"big bad wolf\" [#2516](https://github.com/quickwit-oss/tantivy/pull/2516)(@PSeitz)\n\n- **Optional Index in Multivalue Columnar Index** \nFor mostly empty multivalued indices there was a large overhead during creation when iterating all docids (merge case). \nThis is alleviated by placing an optional index in the multivalued index to mark documents that have values. \nThis will slightly increase space and access time. [#2439](https://github.com/quickwit-oss/tantivy/pull/2439)(@PSeitz)\n\n- **Store DateTime as nanoseconds in doc store** DateTime in the doc store was truncated to microseconds previously. This removes this truncation, while still keeping backwards compatibility. [#2486](https://github.com/quickwit-oss/tantivy/pull/2486)(@PSeitz)\n\n- **Performace/Memory**\n    - lift clauses in LogicalAst for optimized ast during execution [#2449](https://github.com/quickwit-oss/tantivy/pull/2449)(@PSeitz)\n    - Use Vec instead of BTreeMap to back OwnedValue object [#2364](https://github.com/quickwit-oss/tantivy/pull/2364)(@fulmicoton)\n    - Replace TantivyDocument with CompactDoc. CompactDoc is much smaller and provides similar performance. [#2402](https://github.com/quickwit-oss/tantivy/pull/2402)(@PSeitz)\n    - Recycling buffer in PrefixPhraseScorer [#2443](https://github.com/quickwit-oss/tantivy/pull/2443)(@fulmicoton)\n\n- **Json Type**\n    - JSON supports now all values on the root level. Previously an object was required. This enables support for flat mixed types. allow more JSON values, fix i64 special case [#2383](https://github.com/quickwit-oss/tantivy/pull/2383)(@PSeitz)\n    - add json path constructor to term [#2367](https://github.com/quickwit-oss/tantivy/pull/2367)(@PSeitz)\n\n- **QueryParser**\n    - fix de-escaping too much in query parser [#2427](https://github.com/quickwit-oss/tantivy/pull/2427)(@trinity-1686a)\n    - improve query parser [#2416](https://github.com/quickwit-oss/tantivy/pull/2416)(@trinity-1686a)\n    - Support field grouping `title:(return AND \"pink panther\")` [#2333](https://github.com/quickwit-oss/tantivy/pull/2333)(@trinity-1686a)\n\n- add access benchmark for columnar [#2432](https://github.com/quickwit-oss/tantivy/pull/2432)(@PSeitz)\n- extend indexwriter proptests [#2342](https://github.com/quickwit-oss/tantivy/pull/2342)(@PSeitz)\n- add bench & test for columnar merging [#2428](https://github.com/quickwit-oss/tantivy/pull/2428)(@PSeitz)\n- Change in Executor API [#2391](https://github.com/quickwit-oss/tantivy/pull/2391)(@fulmicoton)\n- Removed usage of num_cpus [#2387](https://github.com/quickwit-oss/tantivy/pull/2387)(@fulmicoton)\n- use bingang for agg and stacker benchmark [#2378](https://github.com/quickwit-oss/tantivy/pull/2378)[#2492](https://github.com/quickwit-oss/tantivy/pull/2492)(@PSeitz) \n- cleanup top level exports [#2382](https://github.com/quickwit-oss/tantivy/pull/2382)(@PSeitz)\n- make convert_to_fast_value_and_append_to_json_term pub [#2370](https://github.com/quickwit-oss/tantivy/pull/2370)(@PSeitz)\n- remove JsonTermWriter [#2238](https://github.com/quickwit-oss/tantivy/pull/2238)(@PSeitz)\n- validate sort by field type [#2336](https://github.com/quickwit-oss/tantivy/pull/2336)(@PSeitz)\n- Fix trait bound of StoreReader::iter [#2360](https://github.com/quickwit-oss/tantivy/pull/2360)(@adamreichold)\n- remove read_postings_no_deletes [#2526](https://github.com/quickwit-oss/tantivy/pull/2526)(@PSeitz)\n\nTantivy 0.22\n================================\n\nTantivy 0.22 will be able to read indices created with Tantivy 0.21.\n\n#### Bugfixes\n- Fix null byte handling in JSON paths (null bytes in json keys caused panic during indexing) [#2345](https://github.com/quickwit-oss/tantivy/pull/2345)(@PSeitz)\n- Fix bug that can cause `get_docids_for_value_range` to panic. [#2295](https://github.com/quickwit-oss/tantivy/pull/2295)(@fulmicoton)\n- Avoid 1 document indices by increase min memory to 15MB for indexing [#2176](https://github.com/quickwit-oss/tantivy/pull/2176)(@PSeitz)\n- Fix merge panic for JSON fields [#2284](https://github.com/quickwit-oss/tantivy/pull/2284)(@PSeitz)\n- Fix bug occurring when merging JSON object indexed with positions. [#2253](https://github.com/quickwit-oss/tantivy/pull/2253)(@fulmicoton)\n- Fix empty DateHistogram gap bug [#2183](https://github.com/quickwit-oss/tantivy/pull/2183)(@PSeitz)\n- Fix range query end check (fields with less than 1 value per doc are affected) [#2226](https://github.com/quickwit-oss/tantivy/pull/2226)(@PSeitz)\n- Handle exclusive out of bounds ranges on fastfield range queries [#2174](https://github.com/quickwit-oss/tantivy/pull/2174)(@PSeitz)\n\n#### Breaking API Changes\n- rename ReloadPolicy onCommit to onCommitWithDelay [#2235](https://github.com/quickwit-oss/tantivy/pull/2235)(@giovannicuccu)\n- Move exports from the root into modules [#2220](https://github.com/quickwit-oss/tantivy/pull/2220)(@PSeitz)\n- Accept field name instead of `Field` in FilterCollector [#2196](https://github.com/quickwit-oss/tantivy/pull/2196)(@PSeitz)\n- remove deprecated IntOptions and DateTime [#2353](https://github.com/quickwit-oss/tantivy/pull/2353)(@PSeitz)\n\n#### Features/Improvements\n- Tantivy documents as a trait: Index data directly without converting to tantivy types first [#2071](https://github.com/quickwit-oss/tantivy/pull/2071)(@ChillFish8)\n- encode some part of posting list as -1 instead of direct values (smaller inverted indices) [#2185](https://github.com/quickwit-oss/tantivy/pull/2185)(@trinity-1686a)\n- **Aggregation**\n  - Support to deserialize f64 from string [#2311](https://github.com/quickwit-oss/tantivy/pull/2311)(@PSeitz)\n  - Add a top_hits aggregator [#2198](https://github.com/quickwit-oss/tantivy/pull/2198)(@ditsuke)\n  - Support bool type in term aggregation [#2318](https://github.com/quickwit-oss/tantivy/pull/2318)(@PSeitz)\n  - Support ip addresses in term aggregation [#2319](https://github.com/quickwit-oss/tantivy/pull/2319)(@PSeitz)\n  - Support date type in term aggregation [#2172](https://github.com/quickwit-oss/tantivy/pull/2172)(@PSeitz)\n  - Support escaped dot when addressing field [#2250](https://github.com/quickwit-oss/tantivy/pull/2250)(@PSeitz)\n\n- Add ExistsQuery to check documents that have a value [#2160](https://github.com/quickwit-oss/tantivy/pull/2160)(@imotov)\n- Expose TopDocs::order_by_u64_field again [#2282](https://github.com/quickwit-oss/tantivy/pull/2282)(@ditsuke)\n\n- **Memory/Performance**\n  - Faster TopN: replace BinaryHeap with TopNComputer [#2186](https://github.com/quickwit-oss/tantivy/pull/2186)(@PSeitz)\n  - reduce number of allocations during indexing [#2257](https://github.com/quickwit-oss/tantivy/pull/2257)(@PSeitz)\n  - Less Memory while indexing: docid deltas while indexing [#2249](https://github.com/quickwit-oss/tantivy/pull/2249)(@PSeitz)\n  - Faster indexing: use term hashmap in fastfield [#2243](https://github.com/quickwit-oss/tantivy/pull/2243)(@PSeitz)\n  - term hashmap remove copy in is_empty, unused unordered_id [#2229](https://github.com/quickwit-oss/tantivy/pull/2229)(@PSeitz)\n  - add method to fetch block of first values in columnar [#2330](https://github.com/quickwit-oss/tantivy/pull/2330)(@PSeitz)\n  - Faster aggregations: add fast path for full columns in fetch_block [#2328](https://github.com/quickwit-oss/tantivy/pull/2328)(@PSeitz)\n  - Faster sstable loading: use fst for sstable index [#2268](https://github.com/quickwit-oss/tantivy/pull/2268)(@trinity-1686a)\n\n- **QueryParser**\n  - allow newline where we allow space in query parser [#2302](https://github.com/quickwit-oss/tantivy/pull/2302)(@trinity-1686a)\n  - allow some mixing of occur and bool in strict query parser [#2323](https://github.com/quickwit-oss/tantivy/pull/2323)(@trinity-1686a)\n  - handle * inside term in lenient query parser [#2228](https://github.com/quickwit-oss/tantivy/pull/2228)(@trinity-1686a)\n  - add support for exists query syntax in query parser [#2170](https://github.com/quickwit-oss/tantivy/pull/2170)(@trinity-1686a)\n- Add shared search executor [#2312](https://github.com/quickwit-oss/tantivy/pull/2312)(@MochiXu)\n- Truncate keys to u16::MAX in term hashmap [#2299](https://github.com/quickwit-oss/tantivy/pull/2299)(@PSeitz)\n- report if a term matched when warming up posting list [#2309](https://github.com/quickwit-oss/tantivy/pull/2309)(@trinity-1686a)\n- Support json fields in FuzzyTermQuery [#2173](https://github.com/quickwit-oss/tantivy/pull/2173)(@PingXia-at)\n- Read list of fields encoded in term dictionary for JSON fields [#2184](https://github.com/quickwit-oss/tantivy/pull/2184)(@PSeitz)\n- add collect_block to BoxableSegmentCollector [#2331](https://github.com/quickwit-oss/tantivy/pull/2331)(@PSeitz)\n- expose collect_block buffer size [#2326](https://github.com/quickwit-oss/tantivy/pull/2326)(@PSeitz)\n- Forward regex parser errors [#2288](https://github.com/quickwit-oss/tantivy/pull/2288)(@adamreichold)\n- Make FacetCounts defaultable and cloneable. [#2322](https://github.com/quickwit-oss/tantivy/pull/2322)(@adamreichold)\n- Derive Debug for SchemaBuilder [#2254](https://github.com/quickwit-oss/tantivy/pull/2254)(@GodTamIt)\n- add missing inlines to tantivy options [#2245](https://github.com/quickwit-oss/tantivy/pull/2245)(@PSeitz)\n\nTantivy 0.21.1\n================================\n#### Bugfixes\n- Range queries on fast fields with less values on that field than documents had an invalid end condition, leading to missing results. [#2226](https://github.com/quickwit-oss/tantivy/issues/2226)(@appaquet @PSeitz)\n- Increase the minimum memory budget from 3MB to 15MB to avoid single doc segments (API fix). [#2176](https://github.com/quickwit-oss/tantivy/issues/2176)(@PSeitz)\n\nTantivy 0.21\n================================\n#### Bugfixes\n- Fix track fast field memory consumption, which led to higher memory consumption than the budget allowed during indexing [#2148](https://github.com/quickwit-oss/tantivy/issues/2148)[#2147](https://github.com/quickwit-oss/tantivy/issues/2147)(@PSeitz)\n- Fix a regression from 0.20 where sort index by date wasn't working anymore [#2124](https://github.com/quickwit-oss/tantivy/issues/2124)(@PSeitz)\n- Fix getting the root facet on the `FacetCollector`. [#2086](https://github.com/quickwit-oss/tantivy/issues/2086)(@adamreichold)\n- Align numerical type priority order of columnar and query. [#2088](https://github.com/quickwit-oss/tantivy/issues/2088)(@fmassot)\n#### Breaking Changes\n- Remove support for Brotli and Snappy compression [#2123](https://github.com/quickwit-oss/tantivy/issues/2123)(@adamreichold)\n#### Features/Improvements\n- Implement lenient query parser [#2129](https://github.com/quickwit-oss/tantivy/pull/2129)(@trinity-1686a)\n- order_by_u64_field and order_by_fast_field allow sorting in ascending and descending order [#2111](https://github.com/quickwit-oss/tantivy/issues/2111)(@naveenann)\n- Allow dynamic filters in text analyzer builder [#2110](https://github.com/quickwit-oss/tantivy/issues/2110)(@fulmicoton @fmassot)\n- **Aggregation**\n  - Add missing parameter for term aggregation [#2149](https://github.com/quickwit-oss/tantivy/issues/2149)[#2103](https://github.com/quickwit-oss/tantivy/issues/2103)(@PSeitz)\n  - Add missing parameter for percentiles [#2157](https://github.com/quickwit-oss/tantivy/issues/2157)(@PSeitz)\n  - Add missing parameter for stats,min,max,count,sum,avg [#2151](https://github.com/quickwit-oss/tantivy/issues/2151)(@PSeitz)\n  - Improve aggregation deserialization error message [#2150](https://github.com/quickwit-oss/tantivy/issues/2150)(@PSeitz)\n  - Add validation for type Bytes to term_agg [#2077](https://github.com/quickwit-oss/tantivy/issues/2077)(@PSeitz)\n  - Alternative mixed field collection [#2135](https://github.com/quickwit-oss/tantivy/issues/2135)(@PSeitz)\n- Add missing query_terms impl for TermSetQuery. [#2120](https://github.com/quickwit-oss/tantivy/issues/2120)(@adamreichold)\n- Minor improvements to OwnedBytes [#2134](https://github.com/quickwit-oss/tantivy/issues/2134)(@adamreichold)\n- Remove allocations in split compound words [#2080](https://github.com/quickwit-oss/tantivy/issues/2080)(@PSeitz)\n- Ngram tokenizer now returns an error with invalid arguments [#2102](https://github.com/quickwit-oss/tantivy/issues/2102)(@fmassot)\n- Make TextAnalyzerBuilder public [#2097](https://github.com/quickwit-oss/tantivy/issues/2097)(@adamreichold)\n- Return an error when tokenizer is not found while indexing [#2093](https://github.com/quickwit-oss/tantivy/issues/2093)(@naveenann)\n- Delayed column opening during merge [#2132](https://github.com/quickwit-oss/tantivy/issues/2132)(@PSeitz)\n\nTantivy 0.20.2\n================================\n- Align numerical type priority order on the search side.  [#2088](https://github.com/quickwit-oss/tantivy/issues/2088) (@fmassot)\n- Fix is_child_of function not considering the root facet. [#2086](https://github.com/quickwit-oss/tantivy/issues/2086) (@adamreichhold)\n\nTantivy 0.20.1\n================================\n- Fix building on windows with mmap [#2070](https://github.com/quickwit-oss/tantivy/issues/2070) (@ChillFish8)\n\nTantivy 0.20\n================================\n#### Bugfixes\n- Fix phrase queries with slop (slop supports now transpositions, algorithm that carries slop so far for num terms > 2) [#2031](https://github.com/quickwit-oss/tantivy/issues/2031)[#2020](https://github.com/quickwit-oss/tantivy/issues/2020)(@PSeitz)\n- Handle error for exists on MMapDirectory [#1988](https://github.com/quickwit-oss/tantivy/issues/1988) (@PSeitz)\n- Aggregation\n  - Fix min doc_count empty merge bug [#2057](https://github.com/quickwit-oss/tantivy/issues/2057) (@PSeitz)\n  - Fix: Sort order for term aggregations (sort order on key was inverted) [#1858](https://github.com/quickwit-oss/tantivy/issues/1858) (@PSeitz)\n\n#### Features/Improvements\n- Add PhrasePrefixQuery [#1842](https://github.com/quickwit-oss/tantivy/issues/1842) (@trinity-1686a)\n- Add `coerce` option for text and numbers types (convert the value instead of returning an error during indexing) [#1904](https://github.com/quickwit-oss/tantivy/issues/1904) (@PSeitz)\n- Add regex tokenizer [#1759](https://github.com/quickwit-oss/tantivy/issues/1759)(@mkleen)\n- Move tokenizer API to separate crate. Having a separate crate with a stable API will allow us to use tokenizers with different tantivy versions. [#1767](https://github.com/quickwit-oss/tantivy/issues/1767) (@PSeitz)\n- **Columnar crate**: New fast field handling (@fulmicoton @PSeitz) [#1806](https://github.com/quickwit-oss/tantivy/issues/1806)[#1809](https://github.com/quickwit-oss/tantivy/issues/1809)\n  - Support for fast fields with optional values. Previously tantivy supported only single-valued and multi-value fast fields. The encoding of optional fast fields is now very compact.\n  - Fast field Support for JSON (schemaless fast fields). Support multiple types on the same column. [#1876](https://github.com/quickwit-oss/tantivy/issues/1876) (@fulmicoton)\n  - Unified access for fast fields over different cardinalities.\n  - Unified storage for typed and untyped fields.\n  - Move fastfield codecs into columnar. [#1782](https://github.com/quickwit-oss/tantivy/issues/1782) (@fulmicoton)\n  - Sparse dense index for optional values [#1716](https://github.com/quickwit-oss/tantivy/issues/1716) (@PSeitz)\n  - Switch to nanosecond precision in DateTime fastfield [#2016](https://github.com/quickwit-oss/tantivy/issues/2016) (@PSeitz)\n- **Aggregation**\n  - Add `date_histogram` aggregation (only `fixed_interval` for now) [#1900](https://github.com/quickwit-oss/tantivy/issues/1900) (@PSeitz)\n  - Add `percentiles` aggregations [#1984](https://github.com/quickwit-oss/tantivy/issues/1984) (@PSeitz)\n  - [**breaking**] Drop JSON support on intermediate agg result (we use postcard as format in `quickwit` to send intermediate results) [#1992](https://github.com/quickwit-oss/tantivy/issues/1992) (@PSeitz)\n  - Set memory limit in bytes for aggregations after which they abort (Previously there was only the bucket limit) [#1942](https://github.com/quickwit-oss/tantivy/issues/1942)[#1957](https://github.com/quickwit-oss/tantivy/issues/1957)(@PSeitz)\n  - Add support for u64,i64,f64 fields in term aggregation [#1883](https://github.com/quickwit-oss/tantivy/issues/1883) (@PSeitz)\n  - Allow histogram bounds to be passed as Rfc3339 [#2076](https://github.com/quickwit-oss/tantivy/issues/2076) (@PSeitz)\n  - Add count, min, max, and sum aggregations [#1794](https://github.com/quickwit-oss/tantivy/issues/1794) (@guilload)\n  - Switch to Aggregation without serde_untagged => better deserialization errors. [#2003](https://github.com/quickwit-oss/tantivy/issues/2003) (@PSeitz)\n  - Switch to ms in histogram for date type (ES compatibility) [#2045](https://github.com/quickwit-oss/tantivy/issues/2045) (@PSeitz)\n  - Reduce term aggregation memory consumption [#2013](https://github.com/quickwit-oss/tantivy/issues/2013) (@PSeitz)\n  - Reduce agg memory consumption: Replace generic aggregation collector (which has a high memory requirement per instance) in aggregation tree with optimized versions behind a trait.\n  - Split term collection count and sub_agg (Faster term agg with less memory consumption for cases without sub-aggs) [#1921](https://github.com/quickwit-oss/tantivy/issues/1921) (@PSeitz)\n  - Schemaless aggregations: In combination with stacker tantivy supports now schemaless aggregations via the JSON type.\n    - Add aggregation support for JSON type [#1888](https://github.com/quickwit-oss/tantivy/issues/1888) (@PSeitz)\n    - Mixed types support on JSON fields in aggs [#1971](https://github.com/quickwit-oss/tantivy/issues/1971) (@PSeitz)\n  - Perf: Fetch blocks of vals in aggregation for all cardinality [#1950](https://github.com/quickwit-oss/tantivy/issues/1950) (@PSeitz)\n  - Allow histogram bounds to be passed as Rfc3339 [#2076](https://github.com/quickwit-oss/tantivy/issues/2076) (@PSeitz)\n- `Searcher` with disabled scoring via `EnableScoring::Disabled` [#1780](https://github.com/quickwit-oss/tantivy/issues/1780) (@shikhar)\n- Enable tokenizer on json fields [#2053](https://github.com/quickwit-oss/tantivy/issues/2053) (@PSeitz)\n- Enforcing \"NOT\" and \"-\" queries consistency in UserInputAst [#1609](https://github.com/quickwit-oss/tantivy/issues/1609) (@bazhenov)\n- Faster indexing\n  - Refactor tokenization pipeline to use GATs [#1924](https://github.com/quickwit-oss/tantivy/issues/1924) (@trinity-1686a)\n  - Faster term hash map [#2058](https://github.com/quickwit-oss/tantivy/issues/2058)[#1940](https://github.com/quickwit-oss/tantivy/issues/1940) (@PSeitz)\n  - tokenizer-api: reduce Tokenizer allocation overhead [#2062](https://github.com/quickwit-oss/tantivy/issues/2062) (@PSeitz)\n  - Refactor vint [#2010](https://github.com/quickwit-oss/tantivy/issues/2010) (@PSeitz)\n- Faster search\n  - Work in batches of docs on the SegmentCollector (Only for cases without score for now) [#1937](https://github.com/quickwit-oss/tantivy/issues/1937) (@PSeitz)\n  - Faster fast field range queries using SIMD [#1954](https://github.com/quickwit-oss/tantivy/issues/1954) (@fulmicoton)\n  - Improve fast field range query performance [#1864](https://github.com/quickwit-oss/tantivy/issues/1864) (@PSeitz)\n- Make BM25 scoring more flexible [#1855](https://github.com/quickwit-oss/tantivy/issues/1855) (@alexcole)\n- Switch fs2 to fs4 as it is now unmaintained and does not support illumos [#1944](https://github.com/quickwit-oss/tantivy/issues/1944) (@Toasterson)\n- Made BooleanWeight and BoostWeight public [#1991](https://github.com/quickwit-oss/tantivy/issues/1991) (@fulmicoton)\n- Make index compatible with virtual drives on Windows [#1843](https://github.com/quickwit-oss/tantivy/issues/1843) (@gyk)\n- Add stop words for Hungarian language [#2069](https://github.com/quickwit-oss/tantivy/issues/2069) (@tnxbutno)\n- Auto downgrade index record option, instead of vint error [#1857](https://github.com/quickwit-oss/tantivy/issues/1857) (@PSeitz)\n- Enable range query on fast field for u64 compatible types [#1762](https://github.com/quickwit-oss/tantivy/issues/1762) (@PSeitz) [#1876]\n- sstable\n  - Isolating sstable and stacker in independent crates. [#1718](https://github.com/quickwit-oss/tantivy/issues/1718) (@fulmicoton)\n  - New sstable format [#1943](https://github.com/quickwit-oss/tantivy/issues/1943)[#1953](https://github.com/quickwit-oss/tantivy/issues/1953) (@trinity-1686a)\n  - Use DeltaReader directly to implement Dictionary::ord_to_term [#1928](https://github.com/quickwit-oss/tantivy/issues/1928) (@trinity-1686a)\n  - Use DeltaReader directly to implement Dictionary::term_ord [#1925](https://github.com/quickwit-oss/tantivy/issues/1925) (@trinity-1686a)\n- Add separate tokenizer manager for fast fields [#2019](https://github.com/quickwit-oss/tantivy/issues/2019) (@PSeitz)\n- Make construction of LevenshteinAutomatonBuilder for FuzzyTermQuery instances lazy. [#1756](https://github.com/quickwit-oss/tantivy/issues/1756) (@adamreichold)\n- Added support for madvise when opening an mmapped Index [#2036](https://github.com/quickwit-oss/tantivy/issues/2036) (@fulmicoton)\n- Rename `DatePrecision` to `DateTimePrecision` [#2051](https://github.com/quickwit-oss/tantivy/issues/2051) (@guilload)\n- Query Parser\n  - Quotation mark can now be used for phrase queries. [#2050](https://github.com/quickwit-oss/tantivy/issues/2050) (@fulmicoton)\n  - PhrasePrefixQuery is supported in the query parser via: `field:\"phrase ter\"*` [#2044](https://github.com/quickwit-oss/tantivy/issues/2044) (@adamreichold)\n- Docs\n  - Update examples for literate docs [#1880](https://github.com/quickwit-oss/tantivy/issues/1880) (@PSeitz)\n  - Add ip field example [#1775](https://github.com/quickwit-oss/tantivy/issues/1775) (@PSeitz)\n  - Fix doc store cache documentation [#1821](https://github.com/quickwit-oss/tantivy/issues/1821) (@PSeitz)\n  - Fix BooleanQuery document [#1999](https://github.com/quickwit-oss/tantivy/issues/1999) (@RT_Enzyme)\n  - Update comments in the faceted search example [#1737](https://github.com/quickwit-oss/tantivy/issues/1737) (@DawChihLiou)\n\n\nTantivy 0.19\n================================\n#### Bugfixes\n- Fix missing fieldnorms for u64, i64, f64, bool, bytes and date [#1620](https://github.com/quickwit-oss/tantivy/pull/1620) (@PSeitz)\n- Fix interpolation overflow in linear interpolation fastfield codec [#1480](https://github.com/quickwit-oss/tantivy/pull/1480) (@PSeitz @fulmicoton)\n\n#### Features/Improvements\n- Add support for `IN` in queryparser , e.g. `field: IN [val1 val2 val3]` [#1683](https://github.com/quickwit-oss/tantivy/pull/1683) (@trinity-1686a)\n- Skip score calculation, when no scoring is required [#1646](https://github.com/quickwit-oss/tantivy/pull/1646) (@PSeitz)\n- Limit fast fields to u32 (`get_val(u32)`) [#1644](https://github.com/quickwit-oss/tantivy/pull/1644) (@PSeitz)\n- The `DateTime` type has been updated to hold timestamps with microseconds precision.\n  `DateOptions` and `DatePrecision` have been added to configure Date fields. The precision is used to hint on fast values compression. Otherwise, seconds precision is used everywhere else (i.e terms, indexing) [#1396](https://github.com/quickwit-oss/tantivy/pull/1396) (@evanxg852000)\n- Add IP address field type [#1553](https://github.com/quickwit-oss/tantivy/pull/1553) (@PSeitz)\n- Add boolean field type [#1382](https://github.com/quickwit-oss/tantivy/pull/1382) (@boraarslan)\n- Remove Searcher pool and make `Searcher` cloneable. (@PSeitz)\n- Validate settings on create [#1570](https://github.com/quickwit-oss/tantivy/pull/1570) (@PSeitz)\n- Detect and apply gcd on fastfield codecs [#1418](https://github.com/quickwit-oss/tantivy/pull/1418) (@PSeitz)\n- Doc store\n  - use separate thread to compress block store [#1389](https://github.com/quickwit-oss/tantivy/pull/1389) [#1510](https://github.com/quickwit-oss/tantivy/pull/1510) (@PSeitz @fulmicoton)\n  - Expose doc store cache size [#1403](https://github.com/quickwit-oss/tantivy/pull/1403) (@PSeitz)\n  - Enable compression levels for doc store [#1378](https://github.com/quickwit-oss/tantivy/pull/1378) (@PSeitz)\n  - Make block size configurable [#1374](https://github.com/quickwit-oss/tantivy/pull/1374) (@kryesh)\n- Make `tantivy::TantivyError` cloneable [#1402](https://github.com/quickwit-oss/tantivy/pull/1402) (@PSeitz)\n- Add support for phrase slop in query language [#1393](https://github.com/quickwit-oss/tantivy/pull/1393) (@saroh)\n- Aggregation\n  - Add aggregation support for date type [#1693](https://github.com/quickwit-oss/tantivy/pull/1693)(@PSeitz)\n  - Add support for keyed parameter in range and histogram aggregations [#1424](https://github.com/quickwit-oss/tantivy/pull/1424) (@k-yomo)\n  - Add aggregation bucket limit [#1363](https://github.com/quickwit-oss/tantivy/pull/1363) (@PSeitz)\n- Faster indexing\n  - [#1610](https://github.com/quickwit-oss/tantivy/pull/1610) (@PSeitz)\n  - [#1594](https://github.com/quickwit-oss/tantivy/pull/1594) (@PSeitz)\n  - [#1582](https://github.com/quickwit-oss/tantivy/pull/1582) (@PSeitz)\n  - [#1611](https://github.com/quickwit-oss/tantivy/pull/1611) (@PSeitz)\n  - Added a pre-configured stop word filter for various language [#1666](https://github.com/quickwit-oss/tantivy/pull/1666) (@adamreichold)\n\nTantivy 0.18\n================================\n\n- For date values `chrono` has been replaced with `time` (@uklotzde) #1304 :\n  - The `time` crate is re-exported as `tantivy::time` instead of `tantivy::chrono`.\n  - The type alias `tantivy::DateTime` has been removed.\n  - `Value::Date` wraps `time::PrimitiveDateTime` without time zone information.\n  - Internally date/time values are stored as seconds since UNIX epoch in UTC.\n  - Converting a `time::OffsetDateTime` to `Value::Date` implicitly converts the value into UTC.\n    If this is not desired do the time zone conversion yourself and use `time::PrimitiveDateTime`\n    directly instead.\n- Add [histogram](https://github.com/quickwit-oss/tantivy/pull/1306) aggregation (@PSeitz)\n- Add support for fastfield on text fields (@PSeitz)\n- Add terms aggregation (@PSeitz)\n- Add support for zstd compression (@kryesh)\n\nTantivy 0.18.1\n================================\n- Hotfix: positions computation.  #1629 (@fmassot, @fulmicoton, @PSeitz)\n\nTantivy 0.17\n================================\n\n- LogMergePolicy now triggers merges if the ratio of deleted documents reaches a threshold (@shikhar @fulmicoton) [#115](https://github.com/quickwit-oss/tantivy/issues/115)\n- Adds a searcher Warmer API (@shikhar @fulmicoton)\n- Change to non-strict schema. Ignore fields in data which are not defined in schema. Previously this returned an error. #1211\n- Facets are necessarily indexed. Existing index with indexed facets should work out of the box. Index without facets that are marked with index: false should be broken (but they were already broken in a sense). (@fulmicoton) #1195 .\n- Bugfix that could in theory impact durability in theory on some filesystems [#1224](https://github.com/quickwit-oss/tantivy/issues/1224)\n- Schema now offers not indexing fieldnorms (@lpouget) [#922](https://github.com/quickwit-oss/tantivy/issues/922)\n- Reduce the number of fsync calls [#1225](https://github.com/quickwit-oss/tantivy/issues/1225)\n- Fix opening bytes index with dynamic codec (@PSeitz) [#1278](https://github.com/quickwit-oss/tantivy/issues/1278)\n- Added an aggregation collector for range, average and stats compatible with Elasticsearch. (@PSeitz)\n- Added a JSON schema type @fulmicoton [#1251](https://github.com/quickwit-oss/tantivy/issues/1251)\n- Added support for slop in phrase queries @halvorboe [#1068](https://github.com/quickwit-oss/tantivy/issues/1068)\n\nTantivy 0.16.2\n================================\n\n- Bugfix in FuzzyTermQuery. (transposition_cost_one was not doing anything)\n\nTantivy 0.16.1\n========================\n\n- Major Bugfix on multivalued fastfield.  #1151\n- Demux operation (@PSeitz)\n\nTantivy 0.16.0\n=========================\n\n- Bugfix in the filesum check. (@evanxg852000) #1127\n- Bugfix in positions when the index is sorted by a field. (@appaquet) #1125\n\nTantivy 0.15.3\n=========================\n\n- Major bugfix. Deleting documents was broken when the index was sorted by a field. (@appaquet, @fulmicoton) #1101\n\nTantivy 0.15.2\n========================\n\n- Major bugfix. DocStore still panics when a deleted doc is at the beginning of a block. (@appaquet) #1088\n\nTantivy 0.15.1\n=========================\n\n- Major bugfix. DocStore panics when first block is deleted. (@appaquet) #1077\n\nTantivy 0.15.0\n=========================\n\n- API Changes. Using Range instead of (start, end) in the API and internals (`FileSlice`, `OwnedBytes`, `Snippets`, ...)\n  This change is breaking but migration is trivial.\n- Added an Histogram collector. (@fulmicoton) #994\n- Added support for Option<TCollector>.  (@fulmicoton)\n- DocAddress is now a struct (@scampi) #987\n- Bugfix consistent tie break handling in facet's topk (@hardikpnsp) #357\n- Date field support for range queries (@rihardsk) #516\n- Added lz4-flex as the default compression scheme in tantivy (@PSeitz) #1009\n- Renamed a lot of symbols to avoid all uppercasing on acronyms, as per new clippy recommendation. For instance, RAMDirectory -> RamDirectory. (@fulmicoton)\n- Simplified positions index format (@fulmicoton) #1022\n- Moved bitpacking to bitpacker subcrate and add BlockedBitpacker, which bitpacks blocks of 128 elements (@PSeitz) #1030\n- Added support for more-like-this query in tantivy (@evanxg852000) #1011\n- Added support for sorting an index, e.g presorting documents in an index by a timestamp field. This can heavily improve performance for certain scenarios, by utilizing the sorted data (Top-n optimizations)(@PSeitz). #1026\n- Add iterator over documents in doc store (@PSeitz). #1044\n- Fix log merge policy (@PSeitz). #1043\n- Add detection to avoid small doc store blocks on merge (@PSeitz). #1054\n- Make doc store compression dynamic (@PSeitz). #1060\n- Switch to json for footer version handling (@PSeitz). #1060\n- Updated TermMerger implementation to rely on the union feature of the FST (@scampi) #469\n- Add boolean marking whether position is required in the query_terms API call (@fulmicoton). #1070\n\nTantivy 0.14.0\n=========================\n\n- Remove dependency to atomicwrites #833 .Implemented by @fulmicoton upon suggestion and research from @asafigan).\n- Migrated tantivy error from the now deprecated `failure` crate to `thiserror` #760. (@hirevo)\n- API Change. Accessing the typed value off a `Schema::Value` now returns an Option instead of panicking if the type does not match.\n- Large API Change in the Directory API. Tantivy used to assume that all files could be somehow memory mapped. After this change, Directory return a `FileSlice` that can be reduced and eventually read into an `OwnedBytes` object. Long and blocking io operation are still required by they do not span over the entire file.\n- Added support for Brotli compression in the DocStore. (@ppodolsky)\n- Added helper for building intersections and unions in BooleanQuery (@guilload)\n- Bugfix in `Query::explain`\n- Removed dependency on `notify` #924. Replaced with `FileWatcher` struct that polls meta file every 500ms in background thread. (@halvorboe @guilload)\n- Added `FilterCollector`, which wraps another collector and filters docs using a predicate over a fast field (@barrotsteindev)\n- Simplified the encoding of the skip reader struct. BlockWAND max tf is now encoded over a single byte. (@fulmicoton)\n- `FilterCollector` now supports all Fast Field value types (@barrotsteindev)\n- FastField are not all loaded when opening the segment reader. (@fulmicoton)\n- Added an API to merge segments, see `tantivy::merge_segments` #1005. (@evanxg852000)\n\nThis version breaks compatibility and requires users to reindex everything.\n\nTantivy 0.13.2\n===================\n\nBugfix. Acquiring a facet reader on a segment that does not contain any\ndoc with this facet returns `None`. (#896)\n\nTantivy 0.13.1\n===================\n\nMade `Query` and `Collector` `Send + Sync`.\nUpdated misc dependency versions.\n\nTantivy 0.13.0\n======================\n\nTantivy 0.13 introduce a change in the index format that will require\nyou to reindex your index (BlockWAND information are added in the skiplist).\nThe index size increase is minor as this information is only added for\nfull blocks.\nIf you have a massive index for which reindexing is not an option, please contact me\nso that we can discuss possible solutions.\n\n- Bugfix in `FuzzyTermQuery` not matching terms by prefix when it should (@Peachball)\n- Relaxed constraints on the custom/tweak score functions. At the segment level, they can be mut, and they are not required to be Sync + Send.\n- `MMapDirectory::open` does not return a `Result` anymore.\n- Change in the DocSet and Scorer API. (@fulmicoton).\nA freshly created DocSet point directly to their first doc. A sentinel value called TERMINATED marks the end of a DocSet.\n`.advance()` returns the new DocId. `Scorer::skip(target)` has been replaced by `Scorer::seek(target)` and returns the resulting DocId.\nAs a result, iterating through DocSet now looks as follows\n\n```rust\nlet mut doc = docset.doc();\nwhile doc != TERMINATED {\n   // ...\n   doc = docset.advance();\n}\n```\n\nThe change made it possible to greatly simplify a lot of the docset's code.\n\n- Misc internal optimization and introduction of the `Scorer::for_each_pruning` function. (@fulmicoton)\n- Added an offset option to the Top(.*)Collectors. (@robyoung)\n- Added Block WAND. Performance on TOP-K on term-unions should be greatly increased. (@fulmicoton, and special thanks\nto the PISA team for answering all my questions!)\n\nTantivy 0.12.0\n======================\n\n- Removing static dispatch in tokenizers for simplicity. (#762)\n- Added backward iteration for `TermDictionary` stream. (@halvorboe)\n- Fixed a performance issue when searching for the posting lists of a missing term (@audunhalland)\n- Added a configurable maximum number of docs (10M by default) for a segment to be considered for merge (@hntd187, landed by @halvorboe #713)\n- Important Bugfix #777, causing tantivy to retain memory mapping. (diagnosed by @poljar)\n- Added support for field boosting. (#547, @fulmicoton)\n\n## How to update?\n\nCrates relying on custom tokenizer, or registering tokenizer in the manager will require some\nminor changes. Check <https://github.com/quickwit-oss/tantivy/blob/main/examples/custom_tokenizer.rs>\nto check for some code sample.\n\nTantivy 0.11.3\n=======================\n\n- Fixed DateTime as a fast field (#735)\n\nTantivy 0.11.2\n=======================\n\n- The future returned by `IndexWriter::merge` does not borrow `self` mutably anymore (#732)\n- Exposing a constructor for `WatchHandle` (#731)\n\nTantivy 0.11.1\n=====================\n\n- Bug fix #729\n\nTantivy 0.11.0\n=====================\n\n- Added f64 field. Internally reuse u64 code the same way i64 does (@fdb-hiroshima)\n- Various bugfixes in the query parser.\n  - Better handling of hyphens in query parser. (#609)\n  - Better handling of whitespaces.\n- Closes #498 - add support for Elastic-style unbounded range queries for alphanumeric types eg. \"title:>hello\", \"weight:>=70.5\", \"height:<200\" (@petr-tik)\n- API change around `Box<BoxableTokenizer>`. See detail in #629\n- Avoid rebuilding Regex automaton whenever a regex query is reused. #639 (@brainlock)\n- Add footer with some metadata to index files. #605 (@fdb-hiroshima)\n- Add a method to check the compatibility of the footer in the index with the running version of tantivy (@petr-tik)\n- TopDocs collector: ensure stable sorting on equal score. #671 (@brainlock)\n- Added handling of pre-tokenized text fields (#642), which will enable users to\n  load tokens created outside tantivy. See usage in examples/pre_tokenized_text. (@kkoziara)\n- Fix crash when committing multiple times with deleted documents. #681 (@brainlock)\n\n## How to update?\n\n- The index format is changed. You are required to reindex your data to use tantivy 0.11.\n- `Box<dyn BoxableTokenizer>` has been replaced by a `BoxedTokenizer` struct.\n- Regex are now compiled when the `RegexQuery` instance is built. As a result, it can now return\nan error and handling the `Result` is required.\n- `tantivy::version()` now returns a `Version` object. This object implements `ToString()`\n\nTantivy 0.10.2\n=====================\n\n- Closes #656. Solving memory leak.\n\nTantivy 0.10.1\n=====================\n\n- Closes #544.  A few users experienced problems with the directory watching system.\nAvoid watching the mmap directory until someone effectively creates a reader that uses\nthis functionality.\n\nTantivy 0.10.0\n=====================\n\n*Tantivy 0.10.0 index format is compatible with the index format in 0.9.0.*\n\n- Added an API to easily tweak or entirely replace the\n default score. See `TopDocs::tweak_score`and `TopScore::custom_score` (@fulmicoton)\n- Added an ASCII folding filter (@drusellers)\n- Bugfix in `query.count` in presence of deletes (@fulmicoton)\n- Added `.explain(...)` in `Query` and `Weight` to (@fulmicoton)\n- Added an efficient way to `delete_all_documents` in `IndexWriter` (@petr-tik).\n  All segments are simply removed.\n\nMinor\n---------\n\n- Switched to Rust 2018 (@uvd)\n- Small simplification of the code.\nCalling .freq() or .doc() when .advance() has never been called\non segment postings should panic from now on.\n- Tokens exceeding `u16::max_value() - 4` chars are discarded silently instead of panicking.\n- Fast fields are now preloaded when the `SegmentReader` is created.\n- `IndexMeta` is now public.  (@hntd187)\n- `IndexWriter` `add_document`, `delete_term`. `IndexWriter` is `Sync`, making it possible to use it with a `Arc<RwLock<IndexWriter>>`. `add_document` and `delete_term` can\nonly require a read lock. (@fulmicoton)\n- Introducing `Opstamp` as an expressive type alias for `u64`. (@petr-tik)\n- Stamper now relies on `AtomicU64` on all platforms (@petr-tik)\n- Bugfix - Files get deleted slightly earlier\n- Compilation resources improved (@fdb-hiroshima)\n\n## How to update?\n\nYour program should be usable as is.\n\n### Fast fields\n\nFast fields used to be accessed directly from the `SegmentReader`.\nThe API changed, you are now required to acquire your fast field reader via the\n`segment_reader.fast_fields()`, and use one of the typed method:\n\n- `.u64()`, `.i64()` if your field is single-valued ;\n- `.u64s()`, `.i64s()` if your field is multi-valued ;\n- `.bytes()` if your field is bytes fast field.\n\nTantivy 0.9.0\n=====================\n\n*0.9.0 index format is not compatible with the\nprevious index format.*\n\n- MAJOR BUGFIX :\n  Some `Mmap` objects were being leaked, and would never get released. (@fulmicoton)\n- Removed most unsafe (@fulmicoton)\n- Indexer memory footprint improved. (VInt comp, inlining the first block. (@fulmicoton)\n- Stemming in other language possible (@pentlander)\n- Segments with no docs are deleted earlier (@barrotsteindev)\n- Added grouped add and delete operations.\n  They are guaranteed to happen together (i.e. they cannot be split by a commit).\n  In addition, adds are guaranteed to happen on the same segment. (@elbow-jason)\n- Removed `INT_STORED` and `INT_INDEXED`. It is now possible to use `STORED` and `INDEXED`\n  for int fields. (@fulmicoton)\n- Added DateTime field (@barrotsteindev)\n- Added IndexReader. By default, index is reloaded automatically upon new commits (@fulmicoton)\n- SIMD linear search within blocks (@fulmicoton)\n\n## How to update ?\n\ntantivy 0.9 brought some API breaking change.\nTo update from tantivy 0.8, you will need to go through the following steps.\n\n- `schema::INT_INDEXED` and `schema::INT_STORED`  should be replaced by `schema::INDEXED` and `schema::INT_STORED`.\n- The index now does not hold the pool of searcher anymore. You are required to create an intermediary object called\n`IndexReader` for this.\n\n    ```rust\n    // create the reader. You typically need to create 1 reader for the entire\n    // lifetime of you program.\n    let reader = index.reader()?;\n\n    // Acquire a searcher (previously `index.searcher()`) is now written:\n    let searcher = reader.searcher();\n\n    // With the default setting of the reader, you are not required to\n    // call `index.load_searchers()` anymore.\n    //\n    // The IndexReader will pick up that change automatically, regardless\n    // of whether the update was done in a different process or not.\n    // If this behavior is not wanted, you can create your reader with\n    // the `ReloadPolicy::Manual`, and manually decide when to reload the index\n    // by calling `reader.reload()?`.\n\n    ```\n\nTantivy 0.8.2\n=====================\n\nFixing build for x86_64 platforms. (#496)\nNo need to update from 0.8.1 if tantivy\nis building on your platform.\n\nTantivy 0.8.1\n=====================\n\nHotfix of #476.\n\nMerge was reflecting deletes before commit was passed.\nThanks @barrotsteindev  for reporting the bug.\n\nTantivy 0.8.0\n=====================\n\n*No change in the index format*\n\n- API Breaking change in the collector API. (@jwolfe, @fulmicoton)\n- Multithreaded search (@jwolfe, @fulmicoton)\n\nTantivy 0.7.1\n=====================\n\n*No change in the index format*\n\n- Bugfix: NGramTokenizer panics on non ascii chars\n- Added a space usage API\n\nTantivy 0.7\n=====================\n\n- Skip data for doc ids and positions (@fulmicoton),\n  greatly improving performance\n- Tantivy error now rely on the failure crate (@drusellers)\n- Added support for `AND`, `OR`, `NOT` syntax in addition to the `+`,`-` syntax\n- Added a snippet generator with highlight (@vigneshsarma, @fulmicoton)\n- Added a `TopFieldCollector` (@pentlander)\n\nTantivy 0.6.1\n=========================\n\n- Bugfix #324. GC removing was removing file that were still in useful\n- Added support for parsing AllQuery and RangeQuery via QueryParser\n  - AllQuery: `*`\n  - RangeQuery:\n    - Inclusive `field:[startIncl to endIncl]`\n    - Exclusive `field:{startExcl to endExcl}`\n    - Mixed `field:[startIncl to endExcl}` and vice versa\n    - Unbounded `field:[start to *]`, `field:[* to end]`\n\nTantivy 0.6\n==========================\n\nSpecial thanks to @drusellers and @jason-wolfe for their contributions\nto this release!\n\n- Removed C code. Tantivy is now pure Rust. (@fulmicoton)\n- BM25 (@fulmicoton)\n- Approximate field norms encoded over 1 byte. (@fulmicoton)\n- Compiles on stable rust (@fulmicoton)\n- Add &[u8] fastfield for associating arbitrary bytes to each document (@jason-wolfe) (#270)\n  - Completely uncompressed\n  - Internally: One u64 fast field for indexes, one fast field for the bytes themselves.\n- Add NGram token support (@drusellers)\n- Add Stopword Filter support (@drusellers)\n- Add a FuzzyTermQuery (@drusellers)\n- Add a RegexQuery (@drusellers)\n- Various performance improvements (@fulmicoton)_\n\nTantivy 0.5.2\n===========================\n\n- bugfix #274\n- bugfix #280\n- bugfix #289\n\nTantivy 0.5.1\n==========================\n\n- bugfix #254 : tantivy failed if no documents in a segment contained a specific field.\n\nTantivy 0.5\n==========================\n\n- Faceting\n- RangeQuery\n- Configurable tokenization pipeline\n- Bugfix in PhraseQuery\n- Various query optimisation\n- Allowing very large indexes\n  - 64 bits file address\n  - Smarter encoding of the `TermInfo` objects\n\nTantivy 0.4.3\n==========================\n\n- Bugfix race condition when deleting files. (#198)\n\nTantivy 0.4.2\n==========================\n\n- Prevent usage of AVX2 instructions (#201)\n\nTantivy 0.4.1\n==========================\n\n- Bugfix for non-indexed fields. (#199)\n\nTantivy 0.4.0\n==========================\n\n- Raise the limit of number of fields (previously 256 fields) (@fulmicoton)\n- Removed u32 fields. They are replaced by u64 and i64 fields (#65) (@fulmicoton)\n- Optimized skip in SegmentPostings (#130) (@lnicola)\n- Replacing rustc_serialize by serde. Kudos to  benchmark@KodrAus and @lnicola\n- Using error-chain (@KodrAus)\n- QueryParser: (@fulmicoton)\n  - Explicit error returned when searched for a term that is not indexed\n  - Searching for a int term via the query parser was broken `(age:1)`\n  - Searching for a non-indexed field returns an explicit Error\n  - Phrase query for non-tokenized field are not tokenized by the query parser.\n- Faster/Better indexing (@fulmicoton)\n  - using murmurhash2\n  - faster merging\n  - more memory efficient fast field writer (@lnicola )\n  - better handling of collisions\n  - lesser memory usage\n- Added API, most notably to iterate over ranges of terms (@fulmicoton)\n- Bugfix that was preventing to unmap segment files, on index drop (@fulmicoton)\n- Made the doc! macro public (@fulmicoton)\n- Added an alternative implementation of the streaming dictionary (@fulmicoton)\n\nTantivy 0.3.1\n==========================\n\n- Expose a method to trigger files garbage collection\n\nTantivy 0.3\n==========================\n\nSpecial thanks to @Kodraus @lnicola @Ameobea @manuel-woelker @celaus\nfor their contribution to this release.\n\nThanks also to everyone in tantivy gitter chat\nfor their advise and company :)\n\n<https://gitter.im/tantivy-search/tantivy>\n\nWarning:\n\nTantivy 0.3 is NOT backward compatible with tantivy 0.2\ncode and index format.\nYou should not expect backward compatibility before\ntantivy 1.0.\n\nNew Features\n------------\n\n- Delete. You can now delete documents from an index.\n- Support for windows (Thanks to @lnicola)\n\nVarious Bugfixes & small improvements\n----------------------------------------\n\n- Added CI for Windows (<https://ci.appveyor.com/project/fulmicoton/tantivy>)\nThanks to @KodrAus ! (#108)\n- Various dependy version update (Thanks to @Ameobea) #76\n- Fixed several race conditions in `Index.wait_merge_threads`\n- Fixed #72. Mmap were never released.\n- Fixed #80. Fast field used to take an amplitude of 32 bits after a merge. (Ouch!)\n- Fixed #92. u32 are now encoded using big endian in the fst\n  in order to make there enumeration consistent with\n  the natural ordering.\n- Building binary targets for tantivy-cli (Thanks to @KodrAus)\n- Misc invisible bug fixes, and code cleanup.\n- Use\n"
        },
        {
          "name": "CITATION.cff",
          "type": "blob",
          "size": 0.2783203125,
          "content": "cff-version: 1.2.0\nmessage: \"If you use this software, please cite it as below.\"\nauthors:\n  - alias: Quickwit Inc.\n    website: \"https://quickwit.io\"\ntitle: \"tantivy\"\nversion: 0.22.0\ndoi: 10.5281/zenodo.13942948\ndate-released: 2024-10-17\nurl: \"https://github.com/quickwit-oss/tantivy\"\n"
        },
        {
          "name": "Cargo.toml",
          "type": "blob",
          "size": 4.578125,
          "content": "[package]\nname = \"tantivy\"\nversion = \"0.23.0\"\nauthors = [\"Paul Masurel <paul.masurel@gmail.com>\"]\nlicense = \"MIT\"\ncategories = [\"database-implementations\", \"data-structures\"]\ndescription = \"\"\"Search engine library\"\"\"\ndocumentation = \"https://docs.rs/tantivy/\"\nhomepage = \"https://github.com/quickwit-oss/tantivy\"\nrepository = \"https://github.com/quickwit-oss/tantivy\"\nreadme = \"README.md\"\nkeywords = [\"search\", \"information\", \"retrieval\"]\nedition = \"2021\"\nrust-version = \"1.75\"\nexclude = [\"benches/*.json\", \"benches/*.txt\"]\n\n[dependencies]\noneshot = \"0.1.7\"\nbase64 = \"0.22.0\"\nbyteorder = \"1.4.3\"\ncrc32fast = \"1.3.2\"\nonce_cell = \"1.10.0\"\nregex = { version = \"1.5.5\", default-features = false, features = [\n    \"std\",\n    \"unicode\",\n] }\naho-corasick = \"1.0\"\ntantivy-fst = \"0.5\"\nmemmap2 = { version = \"0.9.0\", optional = true }\nlz4_flex = { version = \"0.11\", default-features = false, optional = true }\nzstd = { version = \"0.13\", optional = true, default-features = false }\ntempfile = { version = \"3.12.0\", optional = true }\nlog = \"0.4.16\"\nserde = { version = \"1.0.136\", features = [\"derive\"] }\nserde_json = \"1.0.79\"\nfs4 = { version = \"0.8.0\", optional = true }\nlevenshtein_automata = \"0.2.1\"\nuuid = { version = \"1.0.0\", features = [\"v4\", \"serde\"] }\ncrossbeam-channel = \"0.5.4\"\nrust-stemmers = \"1.2.0\"\ndowncast-rs = \"1.2.1\"\nbitpacking = { version = \"0.9.2\", default-features = false, features = [\n    \"bitpacker4x\",\n] }\ncensus = \"0.4.2\"\nrustc-hash = \"2.0.0\"\nthiserror = \"2.0.1\"\nhtmlescape = \"0.3.1\"\nfail = { version = \"0.5.0\", optional = true }\ntime = { version = \"0.3.35\", features = [\"serde-well-known\"] }\nsmallvec = \"1.8.0\"\nrayon = \"1.5.2\"\nlru = \"0.12.0\"\nfastdivide = \"0.4.0\"\nitertools = \"0.14.0\"\nmeasure_time = \"0.9.0\"\narc-swap = \"1.5.0\"\nbon = \"3.3.1\"\n\ncolumnar = { version = \"0.3\", path = \"./columnar\", package = \"tantivy-columnar\" }\nsstable = { version = \"0.3\", path = \"./sstable\", package = \"tantivy-sstable\", optional = true }\nstacker = { version = \"0.3\", path = \"./stacker\", package = \"tantivy-stacker\" }\nquery-grammar = { version = \"0.22.0\", path = \"./query-grammar\", package = \"tantivy-query-grammar\" }\ntantivy-bitpacker = { version = \"0.6\", path = \"./bitpacker\" }\ncommon = { version = \"0.7\", path = \"./common/\", package = \"tantivy-common\" }\ntokenizer-api = { version = \"0.3\", path = \"./tokenizer-api\", package = \"tantivy-tokenizer-api\" }\nsketches-ddsketch = { version = \"0.3.0\", features = [\"use_serde\"] }\nhyperloglogplus = { version = \"0.4.1\", features = [\"const-loop\"] }\nfutures-util = { version = \"0.3.28\", optional = true }\nfutures-channel = { version = \"0.3.28\", optional = true }\nfnv = \"1.0.7\"\n\n[target.'cfg(windows)'.dependencies]\nwinapi = \"0.3.9\"\n\n[dev-dependencies]\nbinggan = \"0.14.0\"\nrand = \"0.8.5\"\nmaplit = \"1.0.2\"\nmatches = \"0.1.9\"\npretty_assertions = \"1.2.1\"\nproptest = \"1.0.0\"\ntest-log = \"0.2.10\"\nfutures = \"0.3.21\"\npaste = \"1.0.11\"\nmore-asserts = \"0.3.1\"\nrand_distr = \"0.4.3\"\ntime = { version = \"0.3.10\", features = [\"serde-well-known\", \"macros\"] }\npostcard = { version = \"1.0.4\", features = [\n  \"use-std\",\n], default-features = false }\n\n[target.'cfg(not(windows))'.dev-dependencies]\ncriterion = { version = \"0.5\", default-features = false }\n\n[dev-dependencies.fail]\nversion = \"0.5.0\"\nfeatures = [\"failpoints\"]\n\n[profile.release]\nopt-level = 3\ndebug = false\ndebug-assertions = false\n\n[profile.bench]\nopt-level = 3\ndebug = true\ndebug-assertions = false\n\n[profile.test]\ndebug-assertions = true\noverflow-checks = true\n\n[features]\ndefault = [\"mmap\", \"stopwords\", \"lz4-compression\"]\nmmap = [\"fs4\", \"tempfile\", \"memmap2\"]\nstopwords = []\n\nlz4-compression = [\"lz4_flex\"]\nzstd-compression = [\"zstd\"]\n\nfailpoints = [\"fail\", \"fail/failpoints\"]\nunstable = []                            # useful for benches.\n\nquickwit = [\"sstable\", \"futures-util\", \"futures-channel\"]\n\n# Compares only the hash of a string when indexing data.\n# Increases indexing speed, but may lead to extremely rare missing terms, when there's a hash collision.\n# Uses 64bit ahash.\ncompare_hash_only = [\"stacker/compare_hash_only\"]\n\n[workspace]\nmembers = [\n    \"query-grammar\",\n    \"bitpacker\",\n    \"common\",\n    \"ownedbytes\",\n    \"stacker\",\n    \"sstable\",\n    \"tokenizer-api\",\n    \"columnar\",\n]\n\n# Following the \"fail\" crate best practises, we isolate\n# tests that define specific behavior in fail check points\n# in a different binary.\n#\n# We do that because, fail rely on a global definition of\n# failpoints behavior and hence, it is incompatible with\n# multithreading.\n[[test]]\nname = \"failpoints\"\npath = \"tests/failpoints/mod.rs\"\nrequired-features = [\"failpoints\"]\n\n[[bench]]\nname = \"analyzer\"\nharness = false\n\n[[bench]]\nname = \"index-bench\"\nharness = false\n\n[[bench]]\nname = \"agg_bench\"\nharness = false\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 1.0732421875,
          "content": "Copyright (c) 2018 by the project authors, as listed in the AUTHORS file. \n\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n"
        },
        {
          "name": "Makefile",
          "type": "blob",
          "size": 0.1005859375,
          "content": "test:\n\t@echo \"Run test only... No examples.\"\n\tcargo test --tests --lib\n\nfmt:\n\tcargo +nightly fmt --all\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 7.546875,
          "content": "[![Docs](https://docs.rs/tantivy/badge.svg)](https://docs.rs/crate/tantivy/)\n[![Build Status](https://github.com/quickwit-oss/tantivy/actions/workflows/test.yml/badge.svg)](https://github.com/quickwit-oss/tantivy/actions/workflows/test.yml)\n[![codecov](https://codecov.io/gh/quickwit-oss/tantivy/branch/main/graph/badge.svg)](https://codecov.io/gh/quickwit-oss/tantivy)\n[![Join the chat at https://discord.gg/MT27AG5EVE](https://shields.io/discord/908281611840282624?label=chat%20on%20discord)](https://discord.gg/MT27AG5EVE)\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n[![Crates.io](https://img.shields.io/crates/v/tantivy.svg)](https://crates.io/crates/tantivy)\n\n<img src=\"https://tantivy-search.github.io/logo/tantivy-logo.png\" alt=\"Tantivy, the fastest full-text search engine library written in Rust\" height=\"250\">\n\n## Fast full-text search engine library written in Rust\n\n**If you are looking for an alternative to Elasticsearch or Apache Solr, check out [Quickwit](https://github.com/quickwit-oss/quickwit), our distributed search engine built on top of Tantivy.**\n\nTantivy is closer to [Apache Lucene](https://lucene.apache.org/) than to [Elasticsearch](https://www.elastic.co/products/elasticsearch) or [Apache Solr](https://lucene.apache.org/solr/) in the sense it is not\nan off-the-shelf search engine server, but rather a crate that can be used to build such a search engine.\n\nTantivy is, in fact, strongly inspired by Lucene's design.\n\n## Benchmark\n\nThe following [benchmark](https://tantivy-search.github.io/bench/) breaks down the\nperformance for different types of queries/collections.\n\nYour mileage WILL vary depending on the nature of queries and their load.\n\n<img src=\"doc/assets/images/searchbenchmark.png\">\n\nDetails about the benchmark can be found at this [repository](https://github.com/quickwit-oss/search-benchmark-game).\n\n## Features\n\n- Full-text search\n- Configurable tokenizer (stemming available for 17 Latin languages) with third party support for Chinese ([tantivy-jieba](https://crates.io/crates/tantivy-jieba) and [cang-jie](https://crates.io/crates/cang-jie)), Japanese ([lindera](https://github.com/lindera-morphology/lindera-tantivy), [Vaporetto](https://crates.io/crates/vaporetto_tantivy), and [tantivy-tokenizer-tiny-segmenter](https://crates.io/crates/tantivy-tokenizer-tiny-segmenter)) and Korean ([lindera](https://github.com/lindera-morphology/lindera-tantivy) + [lindera-ko-dic-builder](https://github.com/lindera-morphology/lindera-ko-dic-builder))\n- Fast (check out the :racehorse: :sparkles: [benchmark](https://tantivy-search.github.io/bench/) :sparkles: :racehorse:)\n- Tiny startup time (<10ms), perfect for command-line tools\n- BM25 scoring (the same as Lucene)\n- Natural query language (e.g. `(michael AND jackson) OR \"king of pop\"`)\n- Phrase queries search (e.g. `\"michael jackson\"`)\n- Incremental indexing\n- Multithreaded indexing (indexing English Wikipedia takes < 3 minutes on my desktop)\n- Mmap directory\n- SIMD integer compression when the platform/CPU includes the SSE2 instruction set\n- Single valued and multivalued u64, i64, and f64 fast fields (equivalent of doc values in Lucene)\n- `&[u8]` fast fields\n- Text, i64, u64, f64, dates, ip, bool, and hierarchical facet fields\n- Compressed document store (LZ4, Zstd, None)\n- Range queries\n- Faceted search\n- Configurable indexing (optional term frequency and position indexing)\n- JSON Field\n- Aggregation Collector: histogram, range buckets, average, and stats metrics\n- LogMergePolicy with deletes\n- Searcher Warmer API\n- Cheesy logo with a horse\n\n### Non-features\n\nDistributed search is out of the scope of Tantivy, but if you are looking for this feature, check out [Quickwit](https://github.com/quickwit-oss/quickwit/).\n\n## Getting started\n\nTantivy works on stable Rust and supports Linux, macOS, and Windows.\n\n- [Tantivy's simple search example](https://tantivy-search.github.io/examples/basic_search.html)\n- [tantivy-cli and its tutorial](https://github.com/quickwit-oss/tantivy-cli) - `tantivy-cli` is an actual command-line interface that makes it easy for you to create a search engine,\nindex documents, and search via the CLI or a small server with a REST API.\nIt walks you through getting a Wikipedia search engine up and running in a few minutes.\n- [Reference doc for the last released version](https://docs.rs/tantivy/)\n\n## How can I support this project?\n\nThere are many ways to support this project.\n\n- Use Tantivy and tell us about your experience on [Discord](https://discord.gg/MT27AG5EVE) or by email (paul.masurel@gmail.com)\n- Report bugs\n- Write a blog post\n- Help with documentation by asking questions or submitting PRs\n- Contribute code (you can join [our Discord server](https://discord.gg/MT27AG5EVE))\n- Talk about Tantivy around you\n\n## Contributing code\n\nWe use the GitHub Pull Request workflow: reference a GitHub ticket and/or include a comprehensive commit message when opening a PR.\nFeel free to update CHANGELOG.md with your contribution.\n\n### Tokenizer\n\nWhen implementing a tokenizer for tantivy depend on the `tantivy-tokenizer-api` crate.\n\n### Clone and build locally\n\nTantivy compiles on stable Rust.\nTo check out and run tests, you can simply run:\n\n```bash\ngit clone https://github.com/quickwit-oss/tantivy.git\ncd tantivy\ncargo test\n```\n\n## Companies Using Tantivy\n\n<p align=\"left\">\n<img align=\"center\" src=\"doc/assets/images/etsy.png\" alt=\"Etsy\" height=\"25\" width=\"auto\" /> &nbsp;\n<img align=\"center\" src=\"doc/assets/images/paradedb.png\" alt=\"ParadeDB\" height=\"25\" width=\"auto\" /> &nbsp;\n<img align=\"center\" src=\"doc/assets/images/Nuclia.png#gh-light-mode-only\" alt=\"Nuclia\" height=\"25\" width=\"auto\" /> &nbsp;\n<img align=\"center\" src=\"doc/assets/images/humanfirst.png#gh-light-mode-only\" alt=\"Humanfirst.ai\" height=\"30\" width=\"auto\" />\n<img align=\"center\" src=\"doc/assets/images/element.io.svg#gh-light-mode-only\" alt=\"Element.io\" height=\"25\" width=\"auto\" />\n<img align=\"center\" src=\"doc/assets/images/nuclia-dark-theme.png#gh-dark-mode-only\" alt=\"Nuclia\" height=\"35\" width=\"auto\" /> &nbsp;\n<img align=\"center\" src=\"doc/assets/images/humanfirst.ai-dark-theme.png#gh-dark-mode-only\" alt=\"Humanfirst.ai\" height=\"25\" width=\"auto\" />&nbsp; &nbsp;\n<img align=\"center\" src=\"doc/assets/images/element-dark-theme.png#gh-dark-mode-only\" alt=\"Element.io\" height=\"25\" width=\"auto\" />\n</p>\n\n## FAQ\n\n### Can I use Tantivy in other languages?\n\n- Python → [tantivy-py](https://github.com/quickwit-oss/tantivy-py)\n- Ruby → [tantiny](https://github.com/baygeldin/tantiny)\n\nYou can also find other bindings on [GitHub](https://github.com/search?q=tantivy) but they may be less maintained.\n\n### What are some examples of Tantivy use?\n\n- [seshat](https://github.com/matrix-org/seshat/): A matrix message database/indexer\n- [tantiny](https://github.com/baygeldin/tantiny): Tiny full-text search for Ruby\n- [lnx](https://github.com/lnx-search/lnx): adaptable, typo tolerant search engine with a REST API\n- and [more](https://github.com/search?q=tantivy)!\n\n### On average, how much faster is Tantivy compared to Lucene?\n\n- According to our [search latency benchmark](https://tantivy-search.github.io/bench/), Tantivy is approximately 2x faster than Lucene.\n\n### Does tantivy support incremental indexing?\n\n- Yes.\n\n### How can I edit documents?\n\n- Data in tantivy is immutable. To edit a document, the document needs to be deleted and reindexed.\n\n### When will my documents be searchable during indexing?\n\n- Documents will be searchable after a `commit` is called on an `IndexWriter`. Existing `IndexReader`s will also need to be reloaded in order to reflect the changes. Finally, changes are only visible to newly acquired `Searcher`.\n"
        },
        {
          "name": "RELEASE.md",
          "type": "blob",
          "size": 0.6650390625,
          "content": "# Release a new Tantivy Version\n\n## Steps\n\n1. Identify new packages in workspace since last release\n2. Identify changed packages in workspace since last release\n3. Bump version in `Cargo.toml` and their dependents for all changed packages\n4. Update version of root `Cargo.toml`\n5. Publish version starting with leaf nodes\n6. Set git tag with new version\n\n\nIn conjucation with `cargo-release` Steps 1-4 (I'm not sure if the change detection works):\nSet new packages to version 0.0.0\n\nReplace prev-tag-name\n```bash\ncargo release --workspace --no-publish -v --prev-tag-name 0.19 --push-remote origin minor --no-tag --execute\n```\n\nno-tag or it will create tags for all the subpackages\n"
        },
        {
          "name": "TODO.txt",
          "type": "blob",
          "size": 0.5009765625,
          "content": "Make schema_builder API fluent.\nfix doc serialization and prevent compression problems\n\nu64 , etc. should return Result<Option> now that we support optional missing a column is really not an error\nremove fastfield codecs\nditch the first_or_default trick. if it is still useful, improve its implementation.\nrename FastFieldReaders::open to load\n\n\nremove fast field reader\n\nfind a way to unify the two DateTime.\nreadd type check in the filter wrapper\n\nadd unit test on columnar list columns.\n\nmake sure sort works\n\n"
        },
        {
          "name": "benches",
          "type": "tree",
          "content": null
        },
        {
          "name": "bitpacker",
          "type": "tree",
          "content": null
        },
        {
          "name": "cliff.toml",
          "type": "blob",
          "size": 2.98046875,
          "content": "# configuration file for git-cliff{ pattern = \"foo\", replace = \"bar\"}\n# see https://github.com/orhun/git-cliff#configuration-file\n\n[remote.github]\nowner = \"quickwit-oss\"\nrepo = \"tantivy\"\n\n[changelog]\n# changelog header\nheader = \"\"\"\n\"\"\"\n# template for the changelog body\n# https://tera.netlify.app/docs/#introduction\nbody = \"\"\"\n## What's Changed\n\n{%- if version %} in {{ version }}{%- endif -%}\n{% for commit in commits %}\n  {% if commit.github.pr_title -%}\n    {%- set commit_message = commit.github.pr_title -%}\n  {%- else -%}\n    {%- set commit_message = commit.message -%}\n  {%- endif -%}\n  - {{ commit_message | split(pat=\"\\n\") | first | trim }}\\\n    {% if commit.github.pr_number %} \\\n      [#{{ commit.github.pr_number }}]({{ self::remote_url() }}/pull/{{ commit.github.pr_number }}){% if commit.github.username %}(@{{ commit.github.username }}){%- endif -%} \\\n    {%- endif %}\n{%- endfor -%}\n\n{% if github.contributors | filter(attribute=\"is_first_time\", value=true) | length != 0 %}\n  {% raw %}\\n{% endraw -%}\n  ## New Contributors\n{%- endif %}\\\n{% for contributor in github.contributors | filter(attribute=\"is_first_time\", value=true) %}\n  * @{{ contributor.username }} made their first contribution\n    {%- if contributor.pr_number %} in \\\n      [#{{ contributor.pr_number }}]({{ self::remote_url() }}/pull/{{ contributor.pr_number }}) \\\n    {%- endif %}\n{%- endfor -%}\n\n{% if version %}\n    {% if previous.version %}\n      **Full Changelog**: {{ self::remote_url() }}/compare/{{ previous.version }}...{{ version }}\n    {% endif %}\n{% else -%}\n  {% raw %}\\n{% endraw %}\n{% endif %}\n\n{%- macro remote_url() -%}\n  https://github.com/{{ remote.github.owner }}/{{ remote.github.repo }}\n{%- endmacro -%}\n\"\"\"\n# remove the leading and trailing whitespace from the template\ntrim = true\n# changelog footer\nfooter = \"\"\"\n\"\"\"\n\npostprocessors = [\n]\n\n[git]\n# parse the commits based on https://www.conventionalcommits.org\n# This is required or commit.message contains the whole commit message and not just the title\nconventional_commits = false\n# filter out the commits that are not conventional\nfilter_unconventional = true\n# process each line of a commit as an individual commit\nsplit_commits = false\n# regex for preprocessing the commit messages\ncommit_preprocessors = [\n    { pattern = '\\((\\w+\\s)?#([0-9]+)\\)', replace = \"\"},\n]\n#link_parsers = [\n    #{ pattern = \"#(\\\\d+)\", href = \"https://github.com/quickwit-oss/tantivy/pulls/$1\"},\n#]\n# regex for parsing and grouping commits\n# protect breaking changes from being skipped due to matching a skipping commit_parser\nprotect_breaking_commits = false\n# filter out the commits that are not matched by commit parsers\nfilter_commits = false\n# glob pattern for matching git tags\ntag_pattern = \"v[0-9]*\"\n# regex for skipping tags\nskip_tags = \"v0.1.0-beta.1\"\n# regex for ignoring tags\nignore_tags = \"\"\n# sort the tags topologically\ntopo_order = false\n# sort the commits inside sections by oldest/newest order\nsort_commits = \"newest\"\n# limit the number of commits included in the changelog.\n# limit_commits = 42\n"
        },
        {
          "name": "columnar",
          "type": "tree",
          "content": null
        },
        {
          "name": "common",
          "type": "tree",
          "content": null
        },
        {
          "name": "doc",
          "type": "tree",
          "content": null
        },
        {
          "name": "examples",
          "type": "tree",
          "content": null
        },
        {
          "name": "ownedbytes",
          "type": "tree",
          "content": null
        },
        {
          "name": "query-grammar",
          "type": "tree",
          "content": null
        },
        {
          "name": "rustfmt.toml",
          "type": "blob",
          "size": 0.17578125,
          "content": "comment_width = 120\nformat_strings = true\ngroup_imports = \"StdExternalCrate\"\nimports_granularity = \"Module\"\nnormalize_comments = true\nwhere_single_line = true\nwrap_comments = true\n"
        },
        {
          "name": "src",
          "type": "tree",
          "content": null
        },
        {
          "name": "sstable",
          "type": "tree",
          "content": null
        },
        {
          "name": "stacker",
          "type": "tree",
          "content": null
        },
        {
          "name": "tests",
          "type": "tree",
          "content": null
        },
        {
          "name": "tokenizer-api",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}