{
  "metadata": {
    "timestamp": 1736709432882,
    "page": 43,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjUw",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "huggingface/candle",
      "stars": 16265,
      "defaultBranch": "main",
      "files": [
        {
          "name": ".cargo",
          "type": "tree",
          "content": null
        },
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.8857421875,
          "content": "# Generated by Cargo\n# will have compiled files and executables\ndebug/\ndata/\ndist/\ntarget/\n\n# Remove Cargo.lock from gitignore if creating an executable, leave it for libraries\n# More information here https://doc.rust-lang.org/cargo/guide/cargo-toml-vs-cargo-lock.html\nCargo.lock\n\n# editor config\n.helix\n.vscode\n\n# These are backup files generated by rustfmt\n**/*.rs.bk\n\n# MSVC Windows builds of rustc generate these, which store debugging information\n*.pdb\n\n*tokenizer*.json\n*.npz\n\nperf.data\nflamegraph.svg\n*.dylib\n*.so\n*.swp\n*.swo\ntrace-*.json\n\ncandle-wasm-examples/*/build\ncandle-wasm-examples/*/*.bin\ncandle-wasm-examples/*/*.jpeg\ncandle-wasm-examples/*/audios/*.wav\ncandle-wasm-examples/**/*.safetensors\ncandle-wasm-examples/**/*.gguf\ncandle-wasm-examples/*/package-lock.json\ncandle-wasm-examples/**/config*.json\n.DS_Store\n.idea/*\n__pycache__\nout.safetensors\nout.wav\nbria.mp3\nbria.safetensors\nbria.wav\n"
        },
        {
          "name": ".gitmodules",
          "type": "blob",
          "size": 0.1337890625,
          "content": "[submodule \"candle-examples/examples/flash-attn/cutlass\"]\n\tpath = candle-flash-attn/cutlass\n\turl = https://github.com/NVIDIA/cutlass.git\n"
        },
        {
          "name": ".pre-commit-config.yaml",
          "type": "blob",
          "size": 0.3349609375,
          "content": "repos:\n  - repo: https://github.com/Narsil/pre-commit-rust\n    rev: 2eed6366172ef2a5186e8785ec0e67243d7d73d0\n    hooks:\n      - id: fmt\n        name: \"Rust (fmt)\"\n      - id: clippy\n        name: \"Rust (clippy)\"\n        args:\n          [\n            \"--tests\",\n            \"--examples\",\n            \"--\",\n            \"-Dwarnings\",\n          ]\n"
        },
        {
          "name": ".vscode",
          "type": "tree",
          "content": null
        },
        {
          "name": "CHANGELOG.md",
          "type": "blob",
          "size": 3.9755859375,
          "content": "# Changelog\nThis documents the main changes to the `candle` crate.\n\n## v0.3.1 - Unreleased\n\n### Added\n\n### Modified\n\n## v0.3.0 - 2023-10-01\n\n### Added\n\n- Added the Mistral 7b v0.1 model\n  [983](https://github.com/huggingface/candle/pull/983).\n- Quantized version of the Mistral model\n  [1009](https://github.com/huggingface/candle/pull/1009).\n- Add the gelu-erf op and activation function\n  [969](https://github.com/huggingface/candle/pull/969).\n- Add the mixformer/phi-v1.5 model\n  [930](https://github.com/huggingface/candle/pull/930).\n- Add the sclice-scatter op\n  [927](https://github.com/huggingface/candle/pull/927).\n- Add the Wuerstchen diffusion model\n  [911](https://github.com/huggingface/candle/pull/911).\n\n### Modified\n\n- Support for simd128 intrinsics in some quantized vecdots\n  [982](https://github.com/huggingface/candle/pull/982).\n- Optimize the index-select cuda kernel\n  [976](https://github.com/huggingface/candle/pull/976).\n- Self-contained safetensor wrappers\n  [946](https://github.com/huggingface/candle/pull/946).\n\n## v0.2.2 - 2023-09-18\n\n### Added\n- Support for `top_p` sampling\n  [819](https://github.com/huggingface/candle/pull/819).\n- T5 model including decoding\n  [864](https://github.com/huggingface/candle/pull/864).\n- 1-d upsampling\n  [839](https://github.com/huggingface/candle/pull/839).\n\n### Modified\n- Bugfix for conv2d\n  [820](https://github.com/huggingface/candle/pull/820).\n- Support tensor based indexing using `.i`\n  [842](https://github.com/huggingface/candle/pull/842).\n\n## v0.2.1 - 2023-09-11\n\n### Added\n- Add some RNNs (GRU and LSTM) in `candle-nn`\n  [674](https://github.com/huggingface/candle/pull/674),\n  [688](https://github.com/huggingface/candle/pull/688).\n- gguf v2 support\n  [725](https://github.com/huggingface/candle/pull/725).\n- Quantized llama example in Python using the pyo3 api\n  [716](https://github.com/huggingface/candle/pull/716).\n- `candle-nn` layer for conv2d-transposed\n  [760](https://github.com/huggingface/candle/pull/760).\n- Add the Segment-Anything Model (SAM) as an example\n  [773](https://github.com/huggingface/candle/pull/773).\n- TinyViT backbone for the segment anything example\n  [787](https://github.com/huggingface/candle/pull/787).\n- Shape with holes support\n  [770](https://github.com/huggingface/candle/pull/770).\n\n### Modified\n- Dilations are now supported in conv-transpose2d.\n  [671](https://github.com/huggingface/candle/pull/671).\n- Interactive mode for the quantized model\n  [690](https://github.com/huggingface/candle/pull/690).\n- Faster softmax operation\n  [747](https://github.com/huggingface/candle/pull/747).\n- Faster convolution operations on CPU and CUDA via im2col\n  [802](https://github.com/huggingface/candle/pull/802).\n- Moving some models to a more central location\n  [796](https://github.com/huggingface/candle/pull/796).\n\n## v0.2.0 - 2023-08-30\n\n### Added\n- Add the powf op\n  [664](https://github.com/huggingface/candle/pull/664).\n- Stable Diffusion XL support\n  [647](https://github.com/huggingface/candle/pull/647).\n- Add the conv-transpose2d op\n  [635](https://github.com/huggingface/candle/pull/635).\n- Refactor the VarBuilder api\n  [627](https://github.com/huggingface/candle/pull/627).\n- Add some quantization command\n  [625](https://github.com/huggingface/candle/pull/625).\n- Support more quantized types, e.g. Q2K, Q4K, Q5K...\n  [586](https://github.com/huggingface/candle/pull/586).\n- Add pose estimation to the yolo example\n  [589](https://github.com/huggingface/candle/pull/589).\n- Api to write GGUF files\n  [585](https://github.com/huggingface/candle/pull/585).\n- Support more quantization types\n  [580](https://github.com/huggingface/candle/pull/580).\n- Add EfficientNet as an example Computer Vision model\n  [572](https://github.com/huggingface/candle/pull/572).\n- Add a group parameter to convolutions\n  [566](https://github.com/huggingface/candle/pull/566).\n- New dtype: int64\n  [563](https://github.com/huggingface/candle/pull/563).\n- Handling of the GGUF file format.\n  [559](https://github.com/huggingface/candle/pull/559).\n\n## v0.1.2 - 2023-08-21\n"
        },
        {
          "name": "Cargo.toml",
          "type": "blob",
          "size": 2.7763671875,
          "content": "[workspace]\nmembers = [\n    \"candle-core\",\n    \"candle-datasets\",\n    \"candle-examples\",\n    \"candle-book\",\n    \"candle-nn\",\n    \"candle-pyo3\",\n    \"candle-transformers\",\n    \"candle-wasm-examples/*\",\n    \"candle-wasm-tests\",\n    \"tensor-tools\",\n]\nexclude = [\n   \"candle-flash-attn\",\n   \"candle-kernels\",\n   \"candle-metal-kernels\",\n   \"candle-onnx\",\n]\nresolver = \"2\"\n\n[workspace.package]\nversion = \"0.8.2\"\nedition = \"2021\"\ndescription = \"Minimalist ML framework.\"\nrepository = \"https://github.com/huggingface/candle\"\nkeywords = [\"blas\", \"tensor\", \"machine-learning\"]\ncategories = [\"science\"]\nlicense = \"MIT OR Apache-2.0\"\n\n[workspace.dependencies]\nab_glyph = \"0.2.23\"\naccelerate-src = { version = \"0.3.2\" }\nanyhow = { version = \"1\", features = [\"backtrace\"] }\nbyteorder = \"1.4.3\"\ncandle = { path = \"./candle-core\", package = \"candle-core\", version = \"0.8.2\" }\ncandle-datasets = { path = \"./candle-datasets\", version = \"0.8.2\" }\ncandle-flash-attn = { path = \"./candle-flash-attn\", version = \"0.8.2\" }\ncandle-kernels = { path = \"./candle-kernels\", version = \"0.8.2\" }\ncandle-metal-kernels = { path = \"./candle-metal-kernels\", version = \"0.8.2\" }\ncandle-nn = { path = \"./candle-nn\", version = \"0.8.2\" }\ncandle-onnx = { path = \"./candle-onnx\", version = \"0.8.2\" }\ncandle-transformers = { path = \"./candle-transformers\", version = \"0.8.2\" }\nclap = { version = \"4.2.4\", features = [\"derive\"] }\ncriterion = { version = \"0.5.1\", default-features=false }\ncudarc = { version = \"0.13.0\", features = [\"std\", \"cublas\", \"cublaslt\", \"curand\", \"driver\", \"nvrtc\", \"f16\", \"cuda-version-from-build-system\", \"dynamic-linking\"], default-features=false }\nfancy-regex = \"0.13.0\"\ngemm = { version = \"0.17.0\", features = [\"wasm-simd128-enable\"] }\nhf-hub = \"0.4.1\"\nhalf = { version = \"2.3.1\", features = [\"num-traits\", \"use-intrinsics\", \"rand_distr\"] }\nhound = \"3.5.1\"\nimage = { version = \"0.25.2\", default-features = false, features = [\"jpeg\", \"png\"] }\nimageproc = { version = \"0.24.0\", default-features = false }\nintel-mkl-src = { version = \"0.8.1\", features = [\"mkl-static-lp64-iomp\"] }\nlibc = { version = \"0.2.147\" }\nlog = \"0.4\"\nmemmap2 = { version = \"0.9.3\", features = [\"stable_deref_trait\"] }\nnum_cpus = \"1.15.0\"\nnum-traits = \"0.2.15\"\nparquet = { version = \"51.0.0\" }\nrand = \"0.8.5\"\nrand_distr = \"0.4.3\"\nrayon = \"1.7.0\"\nsafetensors = \"0.4.1\"\nserde = { version = \"1.0.171\", features = [\"derive\"] }\nserde_plain = \"1.0.2\"\nserde_json = \"1.0.99\"\nthiserror = \"1\"\ntokenizers = { version = \"0.19.1\", default-features = false }\ntracing = \"0.1.37\"\ntracing-chrome = \"0.7.1\"\ntracing-subscriber = \"0.3.7\"\nug = \"0.0.2\"\nug-cuda = \"0.0.2\"\nug-metal = \"0.0.2\"\nyoke = { version = \"0.7.2\", features = [\"derive\"] }\nzip = { version = \"1.1.1\", default-features = false }\nmetal = { version = \"0.27.0\", features = [\"mps\"]}\n\n[profile.release-with-debug]\ninherits = \"release\"\ndebug = true\n"
        },
        {
          "name": "LICENSE-APACHE",
          "type": "blob",
          "size": 11.0908203125,
          "content": "                                 Apache License\n                           Version 2.0, January 2004\n                        http://www.apache.org/licenses/\n\n   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n\n   1. Definitions.\n\n      \"License\" shall mean the terms and conditions for use, reproduction,\n      and distribution as defined by Sections 1 through 9 of this document.\n\n      \"Licensor\" shall mean the copyright owner or entity authorized by\n      the copyright owner that is granting the License.\n\n      \"Legal Entity\" shall mean the union of the acting entity and all\n      other entities that control, are controlled by, or are under common\n      control with that entity. For the purposes of this definition,\n      \"control\" means (i) the power, direct or indirect, to cause the\n      direction or management of such entity, whether by contract or\n      otherwise, or (ii) ownership of fifty percent (50%) or more of the\n      outstanding shares, or (iii) beneficial ownership of such entity.\n\n      \"You\" (or \"Your\") shall mean an individual or Legal Entity\n      exercising permissions granted by this License.\n\n      \"Source\" form shall mean the preferred form for making modifications,\n      including but not limited to software source code, documentation\n      source, and configuration files.\n\n      \"Object\" form shall mean any form resulting from mechanical\n      transformation or translation of a Source form, including but\n      not limited to compiled object code, generated documentation,\n      and conversions to other media types.\n\n      \"Work\" shall mean the work of authorship, whether in Source or\n      Object form, made available under the License, as indicated by a\n      copyright notice that is included in or attached to the work\n      (an example is provided in the Appendix below).\n\n      \"Derivative Works\" shall mean any work, whether in Source or Object\n      form, that is based on (or derived from) the Work and for which the\n      editorial revisions, annotations, elaborations, or other modifications\n      represent, as a whole, an original work of authorship. For the purposes\n      of this License, Derivative Works shall not include works that remain\n      separable from, or merely link (or bind by name) to the interfaces of,\n      the Work and Derivative Works thereof.\n\n      \"Contribution\" shall mean any work of authorship, including\n      the original version of the Work and any modifications or additions\n      to that Work or Derivative Works thereof, that is intentionally\n      submitted to Licensor for inclusion in the Work by the copyright owner\n      or by an individual or Legal Entity authorized to submit on behalf of\n      the copyright owner. For the purposes of this definition, \"submitted\"\n      means any form of electronic, verbal, or written communication sent\n      to the Licensor or its representatives, including but not limited to\n      communication on electronic mailing lists, source code control systems,\n      and issue tracking systems that are managed by, or on behalf of, the\n      Licensor for the purpose of discussing and improving the Work, but\n      excluding communication that is conspicuously marked or otherwise\n      designated in writing by the copyright owner as \"Not a Contribution.\"\n\n      \"Contributor\" shall mean Licensor and any individual or Legal Entity\n      on behalf of whom a Contribution has been received by Licensor and\n      subsequently incorporated within the Work.\n\n   2. Grant of Copyright License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      copyright license to reproduce, prepare Derivative Works of,\n      publicly display, publicly perform, sublicense, and distribute the\n      Work and such Derivative Works in Source or Object form.\n\n   3. Grant of Patent License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      (except as stated in this section) patent license to make, have made,\n      use, offer to sell, sell, import, and otherwise transfer the Work,\n      where such license applies only to those patent claims licensable\n      by such Contributor that are necessarily infringed by their\n      Contribution(s) alone or by combination of their Contribution(s)\n      with the Work to which such Contribution(s) was submitted. If You\n      institute patent litigation against any entity (including a\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\n      or a Contribution incorporated within the Work constitutes direct\n      or contributory patent infringement, then any patent licenses\n      granted to You under this License for that Work shall terminate\n      as of the date such litigation is filed.\n\n   4. Redistribution. You may reproduce and distribute copies of the\n      Work or Derivative Works thereof in any medium, with or without\n      modifications, and in Source or Object form, provided that You\n      meet the following conditions:\n\n      (a) You must give any other recipients of the Work or\n          Derivative Works a copy of this License; and\n\n      (b) You must cause any modified files to carry prominent notices\n          stating that You changed the files; and\n\n      (c) You must retain, in the Source form of any Derivative Works\n          that You distribute, all copyright, patent, trademark, and\n          attribution notices from the Source form of the Work,\n          excluding those notices that do not pertain to any part of\n          the Derivative Works; and\n\n      (d) If the Work includes a \"NOTICE\" text file as part of its\n          distribution, then any Derivative Works that You distribute must\n          include a readable copy of the attribution notices contained\n          within such NOTICE file, excluding those notices that do not\n          pertain to any part of the Derivative Works, in at least one\n          of the following places: within a NOTICE text file distributed\n          as part of the Derivative Works; within the Source form or\n          documentation, if provided along with the Derivative Works; or,\n          within a display generated by the Derivative Works, if and\n          wherever such third-party notices normally appear. The contents\n          of the NOTICE file are for informational purposes only and\n          do not modify the License. You may add Your own attribution\n          notices within Derivative Works that You distribute, alongside\n          or as an addendum to the NOTICE text from the Work, provided\n          that such additional attribution notices cannot be construed\n          as modifying the License.\n\n      You may add Your own copyright statement to Your modifications and\n      may provide additional or different license terms and conditions\n      for use, reproduction, or distribution of Your modifications, or\n      for any such Derivative Works as a whole, provided Your use,\n      reproduction, and distribution of the Work otherwise complies with\n      the conditions stated in this License.\n\n   5. Submission of Contributions. Unless You explicitly state otherwise,\n      any Contribution intentionally submitted for inclusion in the Work\n      by You to the Licensor shall be under the terms and conditions of\n      this License, without any additional terms or conditions.\n      Notwithstanding the above, nothing herein shall supersede or modify\n      the terms of any separate license agreement you may have executed\n      with Licensor regarding such Contributions.\n\n   6. Trademarks. This License does not grant permission to use the trade\n      names, trademarks, service marks, or product names of the Licensor,\n      except as required for reasonable and customary use in describing the\n      origin of the Work and reproducing the content of the NOTICE file.\n\n   7. Disclaimer of Warranty. Unless required by applicable law or\n      agreed to in writing, Licensor provides the Work (and each\n      Contributor provides its Contributions) on an \"AS IS\" BASIS,\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n      implied, including, without limitation, any warranties or conditions\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n      PARTICULAR PURPOSE. You are solely responsible for determining the\n      appropriateness of using or redistributing the Work and assume any\n      risks associated with Your exercise of permissions under this License.\n\n   8. Limitation of Liability. In no event and under no legal theory,\n      whether in tort (including negligence), contract, or otherwise,\n      unless required by applicable law (such as deliberate and grossly\n      negligent acts) or agreed to in writing, shall any Contributor be\n      liable to You for damages, including any direct, indirect, special,\n      incidental, or consequential damages of any character arising as a\n      result of this License or out of the use or inability to use the\n      Work (including but not limited to damages for loss of goodwill,\n      work stoppage, computer failure or malfunction, or any and all\n      other commercial damages or losses), even if such Contributor\n      has been advised of the possibility of such damages.\n\n   9. Accepting Warranty or Additional Liability. While redistributing\n      the Work or Derivative Works thereof, You may choose to offer,\n      and charge a fee for, acceptance of support, warranty, indemnity,\n      or other liability obligations and/or rights consistent with this\n      License. However, in accepting such obligations, You may act only\n      on Your own behalf and on Your sole responsibility, not on behalf\n      of any other Contributor, and only if You agree to indemnify,\n      defend, and hold each Contributor harmless for any liability\n      incurred by, or claims asserted against, such Contributor by reason\n      of your accepting any such warranty or additional liability.\n\n   END OF TERMS AND CONDITIONS\n\n   APPENDIX: How to apply the Apache License to your work.\n\n      To apply the Apache License to your work, attach the following\n      boilerplate notice, with the fields enclosed by brackets \"[]\"\n      replaced with your own identifying information. (Don't include\n      the brackets!)  The text should be enclosed in the appropriate\n      comment syntax for the file format. We also recommend that a\n      file or class name and description of purpose be included on the\n      same \"printed page\" as the copyright notice for easier\n      identification within third-party archives.\n\n   Copyright [yyyy] [name of copyright owner]\n\n   Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.\n"
        },
        {
          "name": "LICENSE-MIT",
          "type": "blob",
          "size": 0.9990234375,
          "content": "Permission is hereby granted, free of charge, to any\nperson obtaining a copy of this software and associated\ndocumentation files (the \"Software\"), to deal in the\nSoftware without restriction, including without\nlimitation the rights to use, copy, modify, merge,\npublish, distribute, sublicense, and/or sell copies of\nthe Software, and to permit persons to whom the Software\nis furnished to do so, subject to the following\nconditions:\n\nThe above copyright notice and this permission notice\nshall be included in all copies or substantial portions\nof the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF\nANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED\nTO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A\nPARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT\nSHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY\nCLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION\nOF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR\nIN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER\nDEALINGS IN THE SOFTWARE.\n"
        },
        {
          "name": "Makefile",
          "type": "blob",
          "size": 0.2626953125,
          "content": ".PHONY: clean-ptx clean test\n\nclean-ptx:\n\tfind target -name \"*.ptx\" -type f -delete\n\techo \"\" > candle-kernels/src/lib.rs\n\ttouch candle-kernels/build.rs\n\ttouch candle-examples/build.rs\n\ttouch candle-flash-attn/build.rs\n\nclean:\n\tcargo clean\n\ntest:\n\tcargo test\n\nall: test\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 21.978515625,
          "content": "# candle\n[![discord server](https://dcbadge.vercel.app/api/server/hugging-face-879548962464493619)](https://discord.gg/hugging-face-879548962464493619)\n[![Latest version](https://img.shields.io/crates/v/candle-core.svg)](https://crates.io/crates/candle-core)\n[![Documentation](https://docs.rs/candle-core/badge.svg)](https://docs.rs/candle-core)\n[![License](https://img.shields.io/github/license/base-org/node?color=blue)](https://github.com/huggingface/candle/blob/main/LICENSE-MIT)\n[![License](https://img.shields.io/badge/license-Apache%202.0-blue?style=flat-square)](https://github.com/huggingface/candle/blob/main/LICENSE-APACHE)\n\nCandle is a minimalist ML framework for Rust with a focus on performance (including GPU support) \nand ease of use. Try our online demos: \n[whisper](https://huggingface.co/spaces/lmz/candle-whisper),\n[LLaMA2](https://huggingface.co/spaces/lmz/candle-llama2),\n[T5](https://huggingface.co/spaces/radames/Candle-T5-Generation-Wasm),\n[yolo](https://huggingface.co/spaces/lmz/candle-yolo),\n[Segment\nAnything](https://huggingface.co/spaces/radames/candle-segment-anything-wasm).\n\n## Get started\n\nMake sure that you have [`candle-core`](https://github.com/huggingface/candle/tree/main/candle-core) correctly installed as described in [**Installation**](https://huggingface.github.io/candle/guide/installation.html).\n\nLet's see how to run a simple matrix multiplication.\nWrite the following to your `myapp/src/main.rs` file:\n```rust\nuse candle_core::{Device, Tensor};\n\nfn main() -> Result<(), Box<dyn std::error::Error>> {\n    let device = Device::Cpu;\n\n    let a = Tensor::randn(0f32, 1., (2, 3), &device)?;\n    let b = Tensor::randn(0f32, 1., (3, 4), &device)?;\n\n    let c = a.matmul(&b)?;\n    println!(\"{c}\");\n    Ok(())\n}\n```\n\n`cargo run` should display a tensor of shape `Tensor[[2, 4], f32]`.\n\n\nHaving installed `candle` with Cuda support, simply define the `device` to be on GPU:\n\n```diff\n- let device = Device::Cpu;\n+ let device = Device::new_cuda(0)?;\n```\n\nFor more advanced examples, please have a look at the following section.\n\n## Check out our examples\n\nThese online demos run entirely in your browser:\n- [yolo](https://huggingface.co/spaces/lmz/candle-yolo): pose estimation and\n  object recognition.\n- [whisper](https://huggingface.co/spaces/lmz/candle-whisper): speech recognition.\n- [LLaMA2](https://huggingface.co/spaces/lmz/candle-llama2): text generation.\n- [T5](https://huggingface.co/spaces/radames/Candle-T5-Generation-Wasm): text generation.\n- [Phi-1.5, and Phi-2](https://huggingface.co/spaces/radames/Candle-Phi-1.5-Wasm): text generation.\n- [Segment Anything Model](https://huggingface.co/spaces/radames/candle-segment-anything-wasm): Image segmentation.\n- [BLIP](https://huggingface.co/spaces/radames/Candle-BLIP-Image-Captioning): image captioning.\n\nWe also provide a some command line based examples using state of the art models:\n\n- [LLaMA v1, v2, and v3](./candle-examples/examples/llama/): general LLM, includes\n  the SOLAR-10.7B variant.\n- [Falcon](./candle-examples/examples/falcon/): general LLM.\n- [Codegeex4](./candle-examples/examples/codegeex4-9b/): Code completion,code interpreter,web search,fuction calling,repository-level\n- [GLM4](./candle-examples/examples/glm4/): Open Multilingual Multimodal Chat LMs by THUDM\n- [Gemma v1 and v2](./candle-examples/examples/gemma/): 2b and 7b+/9b general LLMs from Google Deepmind.\n- [RecurrentGemma](./candle-examples/examples/recurrent-gemma/): 2b and 7b\n  Griffin based models from Google that mix attention with a RNN like state.\n- [Phi-1, Phi-1.5, Phi-2, and Phi-3](./candle-examples/examples/phi/): 1.3b,\n  2.7b, and 3.8b general LLMs with performance on par with 7b models.\n- [StableLM-3B-4E1T](./candle-examples/examples/stable-lm/): a 3b general LLM\n  pre-trained on 1T tokens of English and code datasets. Also supports\n  StableLM-2, a 1.6b LLM trained on 2T tokens, as well as the code variants.\n- [Mamba](./candle-examples/examples/mamba/): an inference only\n  implementation of the Mamba state space model.\n- [Mistral7b-v0.1](./candle-examples/examples/mistral/): a 7b general LLM with\n  better performance than all publicly available 13b models as of 2023-09-28.\n- [Mixtral8x7b-v0.1](./candle-examples/examples/mixtral/): a sparse mixture of\n  experts 8x7b general LLM with better performance than a Llama 2 70B model with\n  much faster inference.\n- [StarCoder](./candle-examples/examples/bigcode/) and\n  [StarCoder2](./candle-examples/examples/starcoder2/): LLM specialized to code generation.\n- [Qwen1.5](./candle-examples/examples/qwen/): Bilingual (English/Chinese) LLMs.\n- [RWKV v5 and v6](./candle-examples/examples/rwkv/): An RNN with transformer level LLM\n  performance.\n- [Replit-code-v1.5](./candle-examples/examples/replit-code/): a 3.3b LLM specialized for code completion.\n- [Yi-6B / Yi-34B](./candle-examples/examples/yi/): two bilingual\n  (English/Chinese) general LLMs with 6b and 34b parameters.\n- [Quantized LLaMA](./candle-examples/examples/quantized/): quantized version of\n  the LLaMA model using the same quantization techniques as\n  [llama.cpp](https://github.com/ggerganov/llama.cpp).\n\n<img src=\"https://github.com/huggingface/candle/raw/main/candle-examples/examples/quantized/assets/aoc.gif\" width=\"600\">\n  \n- [Stable Diffusion](./candle-examples/examples/stable-diffusion/): text to\n  image generative model, support for the 1.5, 2.1, SDXL 1.0 and Turbo versions.\n\n<img src=\"https://github.com/huggingface/candle/raw/main/candle-examples/examples/stable-diffusion/assets/stable-diffusion-xl.jpg\" width=\"200\">\n\n- [Wuerstchen](./candle-examples/examples/wuerstchen/): another text to\n  image generative model.\n\n<img src=\"https://github.com/huggingface/candle/raw/main/candle-examples/examples/wuerstchen/assets/cat.jpg\" width=\"200\">\n\n- [yolo-v3](./candle-examples/examples/yolo-v3/) and\n  [yolo-v8](./candle-examples/examples/yolo-v8/): object detection and pose\n  estimation models.\n\n<img src=\"https://github.com/huggingface/candle/raw/main/candle-examples/examples/yolo-v8/assets/bike.od.jpg\" width=\"200\"><img src=\"https://github.com/huggingface/candle/raw/main/candle-examples/examples/yolo-v8/assets/bike.pose.jpg\" width=\"200\">\n- [segment-anything](./candle-examples/examples/segment-anything/): image\n  segmentation model with prompt.\n\n<img src=\"https://github.com/huggingface/candle/raw/main/candle-examples/examples/segment-anything/assets/sam_merged.jpg\" width=\"200\">\n\n- [SegFormer](./candle-examples/examples/segformer/): transformer based semantic segmentation model.\n- [Whisper](./candle-examples/examples/whisper/): speech recognition model.\n- [EnCodec](./candle-examples/examples/encodec/): high-quality audio compression\n  model using residual vector quantization.\n- [MetaVoice](./candle-examples/examples/metavoice/): foundational model for\n  text-to-speech.\n- [Parler-TTS](./candle-examples/examples/parler-tts/): large text-to-speech\n  model.\n- [T5](./candle-examples/examples/t5), [Bert](./candle-examples/examples/bert/),\n  [JinaBert](./candle-examples/examples/jina-bert/) : useful for sentence embeddings.\n- [DINOv2](./candle-examples/examples/dinov2/): computer vision model trained\n  using self-supervision (can be used for imagenet classification, depth\n  evaluation, segmentation).\n- [VGG](./candle-examples/examples/vgg/),\n  [RepVGG](./candle-examples/examples/repvgg): computer vision models.\n- [BLIP](./candle-examples/examples/blip/): image to text model, can be used to\n  generate captions for an image.\n- [CLIP](./candle-examples/examples/clip/): multi-model vision and language\n  model.\n- [TrOCR](./candle-examples/examples/trocr/): a transformer OCR model, with\n  dedicated submodels for hand-writing and printed recognition.\n- [Marian-MT](./candle-examples/examples/marian-mt/): neural machine translation\n  model, generates the translated text from the input text.\n- [Moondream](./candle-examples/examples/moondream/): tiny computer-vision model \n  that can answer real-world questions about images.\n\nRun them using commands like:\n```\ncargo run --example quantized --release\n```\n\nIn order to use **CUDA** add `--features cuda` to the example command line. If\nyou have cuDNN installed, use `--features cudnn` for even more speedups.\n\nThere are also some wasm examples for whisper and\n[llama2.c](https://github.com/karpathy/llama2.c). You can either build them with\n`trunk` or try them online:\n[whisper](https://huggingface.co/spaces/lmz/candle-whisper),\n[llama2](https://huggingface.co/spaces/lmz/candle-llama2),\n[T5](https://huggingface.co/spaces/radames/Candle-T5-Generation-Wasm),\n[Phi-1.5, and Phi-2](https://huggingface.co/spaces/radames/Candle-Phi-1.5-Wasm),\n[Segment Anything Model](https://huggingface.co/spaces/radames/candle-segment-anything-wasm).\n\nFor LLaMA2, run the following command to retrieve the weight files and start a\ntest server:\n```bash\ncd candle-wasm-examples/llama2-c\nwget https://huggingface.co/spaces/lmz/candle-llama2/resolve/main/model.bin\nwget https://huggingface.co/spaces/lmz/candle-llama2/resolve/main/tokenizer.json\ntrunk serve --release --port 8081\n```\nAnd then head over to\n[http://localhost:8081/](http://localhost:8081/).\n\n<!--- ANCHOR: useful_libraries --->\n\n## Useful External Resources\n- [`candle-tutorial`](https://github.com/ToluClassics/candle-tutorial): A\n  very detailed tutorial showing how to convert a PyTorch model to Candle.\n- [`candle-lora`](https://github.com/EricLBuehler/candle-lora): Efficient and\n  ergonomic LoRA implementation for Candle. `candle-lora` has      \n  out-of-the-box LoRA support for many models from Candle, which can be found\n  [here](https://github.com/EricLBuehler/candle-lora/tree/master/candle-lora-transformers/examples).\n- [`optimisers`](https://github.com/KGrewal1/optimisers): A collection of optimisers\n  including SGD with momentum, AdaGrad, AdaDelta, AdaMax, NAdam, RAdam, and RMSprop.\n- [`candle-vllm`](https://github.com/EricLBuehler/candle-vllm): Efficient platform for inference and\n  serving local LLMs including an OpenAI compatible API server.\n- [`candle-ext`](https://github.com/mokeyish/candle-ext): An extension library to Candle that provides PyTorch functions not currently available in Candle.\n- [`candle-coursera-ml`](https://github.com/vishpat/candle-coursera-ml): Implementation of ML algorithms from Coursera's [Machine Learning Specialization](https://www.coursera.org/specializations/machine-learning-introduction) course.\n- [`kalosm`](https://github.com/floneum/floneum/tree/master/interfaces/kalosm): A multi-modal meta-framework in Rust for interfacing with local pre-trained models with support for controlled generation, custom samplers, in-memory vector databases, audio transcription, and more.\n- [`candle-sampling`](https://github.com/EricLBuehler/candle-sampling): Sampling techniques for Candle.\n- [`gpt-from-scratch-rs`](https://github.com/jeroenvlek/gpt-from-scratch-rs): A port of Andrej Karpathy's _Let's build GPT_ tutorial on YouTube showcasing the Candle API on a toy problem.\n- [`candle-einops`](https://github.com/tomsanbear/candle-einops): A pure rust implementation of the python [einops](https://github.com/arogozhnikov/einops) library.\n- [`atoma-infer`](https://github.com/atoma-network/atoma-infer): A Rust library for fast inference at scale, leveraging FlashAttention2 for efficient attention computation, PagedAttention for efficient KV-cache memory management, and multi-GPU support. It is OpenAI api compatible.\n- [`llms-from-scratch-rs`](https://github.com/nerdai/llms-from-scratch-rs): A comprehensive Rust translation of the code from Sebastian Raschka's Build an LLM from Scratch book.\n\nIf you have an addition to this list, please submit a pull request.\n\n<!--- ANCHOR_END: useful_libraries --->\n\n<!--- ANCHOR: features --->\n\n## Features\n\n- Simple syntax, looks and feels like PyTorch.\n    - Model training.\n    - Embed user-defined ops/kernels, such as [flash-attention v2](https://github.com/huggingface/candle/blob/89ba005962495f2bfbda286e185e9c3c7f5300a3/candle-flash-attn/src/lib.rs#L152).\n- Backends.\n    - Optimized CPU backend with optional MKL support for x86 and Accelerate for macs.\n    - CUDA backend for efficiently running on GPUs, multiple GPU distribution via NCCL.\n    - WASM support, run your models in a browser.\n- Included models.\n    - Language Models.\n        - LLaMA v1, v2, and v3 with variants such as SOLAR-10.7B.\n        - Falcon.\n        - StarCoder, StarCoder2.\n        - Phi 1, 1.5, 2, and 3.\n        - Mamba, Minimal Mamba\n        - Gemma v1 2b and 7b+, v2 2b and 9b.\n        - Mistral 7b v0.1.\n        - Mixtral 8x7b v0.1.\n        - StableLM-3B-4E1T, StableLM-2-1.6B, Stable-Code-3B.\n        - Replit-code-v1.5-3B.\n        - Bert.\n        - Yi-6B and Yi-34B.\n        - Qwen1.5, Qwen1.5 MoE.\n        - RWKV v5 and v6.\n    - Quantized LLMs.\n        - Llama 7b, 13b, 70b, as well as the chat and code variants.\n        - Mistral 7b, and 7b instruct.\n        - Mixtral 8x7b.\n        - Zephyr 7b a and b (Mistral-7b based).\n        - OpenChat 3.5 (Mistral-7b based).\n    - Text to text.\n        - T5 and its variants: FlanT5, UL2, MADLAD400 (translation), CoEdit (Grammar correction).\n        - Marian MT (Machine Translation).\n    - Text to image.\n        - Stable Diffusion v1.5, v2.1, XL v1.0.\n        - Wurstchen v2.\n    - Image to text.\n        - BLIP.\n        - TrOCR.\n    - Audio.\n        - Whisper, multi-lingual speech-to-text.\n        - EnCodec, audio compression model.\n        - MetaVoice-1B, text-to-speech model.\n        - Parler-TTS, text-to-speech model.\n    - Computer Vision Models.\n        - DINOv2, ConvMixer, EfficientNet, ResNet, ViT, VGG, RepVGG, ConvNeXT,\n          ConvNeXTv2, MobileOne, EfficientVit (MSRA), MobileNetv4, Hiera, FastViT.\n        - yolo-v3, yolo-v8.\n        - Segment-Anything Model (SAM).\n        - SegFormer.\n- File formats: load models from safetensors, npz, ggml, or PyTorch files.\n- Serverless (on CPU), small and fast deployments.\n- Quantization support using the llama.cpp quantized types.\n\n<!--- ANCHOR_END: features --->\n\n## How to use\n\n<!--- ANCHOR: cheatsheet --->\nCheatsheet:\n\n|            | Using PyTorch                            | Using Candle                                                     |\n|------------|------------------------------------------|------------------------------------------------------------------|\n| Creation   | `torch.Tensor([[1, 2], [3, 4]])`         | `Tensor::new(&[[1f32, 2.], [3., 4.]], &Device::Cpu)?`           |\n| Creation   | `torch.zeros((2, 2))`                    | `Tensor::zeros((2, 2), DType::F32, &Device::Cpu)?`               |\n| Indexing   | `tensor[:, :4]`                          | `tensor.i((.., ..4))?`                                           |\n| Operations | `tensor.view((2, 2))`                    | `tensor.reshape((2, 2))?`                                        |\n| Operations | `a.matmul(b)`                            | `a.matmul(&b)?`                                                  |\n| Arithmetic | `a + b`                                  | `&a + &b`                                                        |\n| Device     | `tensor.to(device=\"cuda\")`               | `tensor.to_device(&Device::new_cuda(0)?)?`                            |\n| Dtype      | `tensor.to(dtype=torch.float16)`         | `tensor.to_dtype(&DType::F16)?`                                  |\n| Saving     | `torch.save({\"A\": A}, \"model.bin\")`      | `candle::safetensors::save(&HashMap::from([(\"A\", A)]), \"model.safetensors\")?` |\n| Loading    | `weights = torch.load(\"model.bin\")`      | `candle::safetensors::load(\"model.safetensors\", &device)`        |\n\n<!--- ANCHOR_END: cheatsheet --->\n\n\n## Structure\n\n- [candle-core](./candle-core): Core ops, devices, and `Tensor` struct definition\n- [candle-nn](./candle-nn/): Tools to build real models\n- [candle-examples](./candle-examples/): Examples of using the library in realistic settings\n- [candle-kernels](./candle-kernels/): CUDA custom kernels\n- [candle-datasets](./candle-datasets/): Datasets and data loaders.\n- [candle-transformers](./candle-transformers): transformers-related utilities.\n- [candle-flash-attn](./candle-flash-attn): Flash attention v2 layer.\n- [candle-onnx](./candle-onnx/): ONNX model evaluation.\n\n## FAQ\n\n### Why should I use Candle?\n\nCandle's core goal is to *make serverless inference possible*. Full machine learning frameworks like PyTorch\nare very large, which makes creating instances on a cluster slow. Candle allows deployment of lightweight\nbinaries.\n\nSecondly, Candle lets you *remove Python* from production workloads. Python overhead can seriously hurt performance,\nand the [GIL](https://www.backblaze.com/blog/the-python-gil-past-present-and-future/) is a notorious source of headaches.\n\nFinally, Rust is cool! A lot of the HF ecosystem already has Rust crates, like [safetensors](https://github.com/huggingface/safetensors) and [tokenizers](https://github.com/huggingface/tokenizers).\n\n\n### Other ML frameworks\n\n- [dfdx](https://github.com/coreylowman/dfdx) is a formidable crate, with shapes being included\n  in types. This prevents a lot of headaches by getting the compiler to complain about shape mismatches right off the bat.\n  However, we found that some features still require nightly, and writing code can be a bit daunting for non rust experts.\n\n  We're leveraging and contributing to other core crates for the runtime so hopefully both crates can benefit from each\n  other.\n\n- [burn](https://github.com/burn-rs/burn) is a general crate that can leverage multiple backends so you can choose the best\n  engine for your workload.\n\n- [tch-rs](https://github.com/LaurentMazare/tch-rs.git) Bindings to the torch library in Rust. Extremely versatile, but they \n  bring in the entire torch library into the runtime. The main contributor of `tch-rs` is also involved in the development\n  of `candle`.\n\n### Common Errors\n\n#### Missing symbols when compiling with the mkl feature.\n\nIf you get some missing symbols when compiling binaries/tests using the mkl\nor accelerate features, e.g. for mkl you get:\n```\n  = note: /usr/bin/ld: (....o): in function `blas::sgemm':\n          .../blas-0.22.0/src/lib.rs:1944: undefined reference to `sgemm_' collect2: error: ld returned 1 exit status\n\n  = note: some `extern` functions couldn't be found; some native libraries may need to be installed or have their path specified\n  = note: use the `-l` flag to specify native libraries to link\n  = note: use the `cargo:rustc-link-lib` directive to specify the native libraries to link with Cargo\n```\nor for accelerate:\n```\nUndefined symbols for architecture arm64:\n            \"_dgemm_\", referenced from:\n                candle_core::accelerate::dgemm::h1b71a038552bcabe in libcandle_core...\n            \"_sgemm_\", referenced from:\n                candle_core::accelerate::sgemm::h2cf21c592cba3c47 in libcandle_core...\n          ld: symbol(s) not found for architecture arm64\n```\n\nThis is likely due to a missing linker flag that was needed to enable the mkl library. You\ncan try adding the following for mkl at the top of your binary:\n```rust\nextern crate intel_mkl_src;\n```\nor for accelerate:\n```rust\nextern crate accelerate_src;\n```\n\n#### Cannot run the LLaMA examples: access to source requires login credentials\n\n```\nError: request error: https://huggingface.co/meta-llama/Llama-2-7b-hf/resolve/main/tokenizer.json: status code 401\n```\n\nThis is likely because you're not permissioned for the LLaMA-v2 model. To fix\nthis, you have to register on the huggingface-hub, accept the [LLaMA-v2 model\nconditions](https://huggingface.co/meta-llama/Llama-2-7b-hf), and set up your\nauthentication token. See issue\n[#350](https://github.com/huggingface/candle/issues/350) for more details.\n\n#### Missing cute/cutlass headers when compiling flash-attn\n\n```\n  In file included from kernels/flash_fwd_launch_template.h:11:0,\n                   from kernels/flash_fwd_hdim224_fp16_sm80.cu:5:\n  kernels/flash_fwd_kernel.h:8:10: fatal error: cute/algorithm/copy.hpp: No such file or directory\n   #include <cute/algorithm/copy.hpp>\n            ^~~~~~~~~~~~~~~~~~~~~~~~~\n  compilation terminated.\n  Error: nvcc error while compiling:\n```\n[cutlass](https://github.com/NVIDIA/cutlass) is provided as a git submodule so you may want to run the following command to check it in properly.\n```bash\ngit submodule update --init\n```\n\n#### Compiling with flash-attention fails\n\n```\n/usr/include/c++/11/bits/std_function.h:530:146: error: parameter packs not expanded with ‘...’:\n```\n\nThis is a bug in gcc-11 triggered by the Cuda compiler. To fix this, install a different, supported gcc version - for example gcc-10, and specify the path to the compiler in the NVCC_CCBIN environment variable.\n```\nenv NVCC_CCBIN=/usr/lib/gcc/x86_64-linux-gnu/10 cargo ...\n```\n\n#### Linking error on windows when running rustdoc or mdbook tests\n\n```\nCouldn't compile the test.\n---- .\\candle-book\\src\\inference\\hub.md - Using_the_hub::Using_in_a_real_model_ (line 50) stdout ----\nerror: linking with `link.exe` failed: exit code: 1181\n//very long chain of linking\n = note: LINK : fatal error LNK1181: cannot open input file 'windows.0.48.5.lib'\n```\n\nMake sure you link all native libraries that might be located outside a project target, e.g., to run mdbook tests, you should run:\n\n```\nmdbook test candle-book -L .\\target\\debug\\deps\\ `\n-L native=$env:USERPROFILE\\.cargo\\registry\\src\\index.crates.io-6f17d22bba15001f\\windows_x86_64_msvc-0.42.2\\lib `\n-L native=$env:USERPROFILE\\.cargo\\registry\\src\\index.crates.io-6f17d22bba15001f\\windows_x86_64_msvc-0.48.5\\lib\n```\n\n#### Extremely slow model load time with WSL\n\nThis may be caused by the models being loaded from `/mnt/c`, more details on\n[stackoverflow](https://stackoverflow.com/questions/68972448/why-is-wsl-extremely-slow-when-compared-with-native-windows-npm-yarn-processing).\n\n#### Tracking down errors\n\nYou can set `RUST_BACKTRACE=1` to be provided with backtraces when a candle\nerror is generated.\n\n#### CudaRC error\n\nIf you encounter an error like this one `called `Result::unwrap()` on an `Err` value: LoadLibraryExW { source: Os { code: 126, kind: Uncategorized, message: \"The specified module could not be found.\" } }` on windows. To fix copy and rename these 3 files (make sure they are in path). The paths depend on your cuda version.\n`c:\\Windows\\System32\\nvcuda.dll` -> `cuda.dll`\n`c:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v12.4\\bin\\cublas64_12.dll` -> `cublas.dll`\n`c:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v12.4\\bin\\curand64_10.dll` -> `curand.dll`\n"
        },
        {
          "name": "candle-book",
          "type": "tree",
          "content": null
        },
        {
          "name": "candle-core",
          "type": "tree",
          "content": null
        },
        {
          "name": "candle-datasets",
          "type": "tree",
          "content": null
        },
        {
          "name": "candle-examples",
          "type": "tree",
          "content": null
        },
        {
          "name": "candle-flash-attn",
          "type": "tree",
          "content": null
        },
        {
          "name": "candle-kernels",
          "type": "tree",
          "content": null
        },
        {
          "name": "candle-metal-kernels",
          "type": "tree",
          "content": null
        },
        {
          "name": "candle-nn",
          "type": "tree",
          "content": null
        },
        {
          "name": "candle-onnx",
          "type": "tree",
          "content": null
        },
        {
          "name": "candle-pyo3",
          "type": "tree",
          "content": null
        },
        {
          "name": "candle-transformers",
          "type": "tree",
          "content": null
        },
        {
          "name": "candle-wasm-examples",
          "type": "tree",
          "content": null
        },
        {
          "name": "candle-wasm-tests",
          "type": "tree",
          "content": null
        },
        {
          "name": "tensor-tools",
          "type": "tree",
          "content": null
        },
        {
          "name": "test.onnx",
          "type": "blob",
          "size": 0.09375,
          "content": "\b\u0003\u0012\fbackend-test:J\n\u0012\n\u0001x\u0012\u0001y\u001a\u0004test\"\u0004Relu\u0012\nSingleReluZ\u0013\n\u0001x\u0012\u000e\n\f\b\u0001\u0012\b\n\u0002\b\u0001\n\u0002\b\u0002b\u0013\n\u0001y\u0012\u000e\n\f\b\u0001\u0012\b\n\u0002\b\u0001\n\u0002\b\u0002B\u0002\u0010\u0006"
        }
      ]
    }
  ]
}