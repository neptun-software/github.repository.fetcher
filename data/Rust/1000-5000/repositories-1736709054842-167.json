{
  "metadata": {
    "timestamp": 1736709054842,
    "page": 167,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjE3MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "TimelyDataflow/timely-dataflow",
      "stars": 3334,
      "defaultBranch": "master",
      "files": [
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.02734375,
          "content": "/target\n/.vscode\nCargo.lock\n"
        },
        {
          "name": "CHANGELOG.md",
          "type": "blob",
          "size": 13.005859375,
          "content": "# Changelog\n\nAll notable changes to this project will be documented in this file.\n\n## [0.16.0](https://github.com/TimelyDataflow/timely-dataflow/compare/timely-v0.15.1...timely-v0.16.0) - 2025-01-09\n\n### Other\n\n- Define loggers in terms of container builders ([#615](https://github.com/TimelyDataflow/timely-dataflow/pull/615))\n- Remove SizableContainer requirement from partition ([#612](https://github.com/TimelyDataflow/timely-dataflow/pull/612))\n\n## [0.15.1](https://github.com/TimelyDataflow/timely-dataflow/compare/timely-v0.15.0...timely-v0.15.1) - 2024-12-18\n\n### Other\n\n- Remove worker identifier from logging ([#533](https://github.com/TimelyDataflow/timely-dataflow/pull/533))\n- add `.partition()` for `StreamCore` (#610)\n- Update columnar ([#611](https://github.com/TimelyDataflow/timely-dataflow/pull/611))\n- Introduce columnar and derive extensively ([#608](https://github.com/TimelyDataflow/timely-dataflow/pull/608))\n\n## [0.15.0](https://github.com/TimelyDataflow/timely-dataflow/compare/timely-v0.14.1...timely-v0.15.0) - 2024-12-05\n\n### Other\n\n- Prefer byteorder in place of bincode ([#607](https://github.com/TimelyDataflow/timely-dataflow/pull/607))\n- Use help from columnar 0.1.1 ([#606](https://github.com/TimelyDataflow/timely-dataflow/pull/606))\n- Reorganize `Container` traits ([#605](https://github.com/TimelyDataflow/timely-dataflow/pull/605))\n- Robustify potential Bytes alignment\n- Correct bincode call to use and update reader\n- Demonstrate `columnar` stuff ([#586](https://github.com/TimelyDataflow/timely-dataflow/pull/586))\n- Allow containers to specify their own serialization ([#604](https://github.com/TimelyDataflow/timely-dataflow/pull/604))\n- Remove Container: Clone + 'static ([#540](https://github.com/TimelyDataflow/timely-dataflow/pull/540))\n- Apply various Clippy recommendations ([#603](https://github.com/TimelyDataflow/timely-dataflow/pull/603))\n- Several improvements around `Bytesable` and `Message`. ([#601](https://github.com/TimelyDataflow/timely-dataflow/pull/601))\n\n## [0.14.1](https://github.com/TimelyDataflow/timely-dataflow/compare/timely-v0.14.0...timely-v0.14.1) - 2024-11-12\n\n### Added\n\nThe type `timely::Message` is now publicly re-exported.\n\n### Other\n\n- Public Message type ([#599](https://github.com/TimelyDataflow/timely-dataflow/pull/599))\n\n## [0.14.0](https://github.com/TimelyDataflow/timely-dataflow/compare/timely-v0.13.0...timely-v0.14.0) - 2024-11-11\n\n### Added\n\nThe trait `communication::Bytesable`, for types that must be serialized into or from a `Bytes`, and stands in for \"timely appropriate serialization\".\nThe trait `communication::Exchangeable`, a composite trait bringing together the requirements on a type for it to be sent along a general purpose communication channel.\n\n### Removed\n\nThe communication `Message` and `RefOrMut` types have been removed.\nThe `RefOrMut` type wrapped either a `&T` or a `&mut T`, but with the removal of `abomonation` it is always a `&mut T`.\nThe `Message` type was used to indicate the serialization / deserialization behavior, and these opinions (e.g. \"use `bincode`\") have been migrated to the core `timely` crate.\n\n### Other\n\n- Move opinions about encoding from `communication` to `timely`. ([#597](https://github.com/TimelyDataflow/timely-dataflow/pull/597))\n- Rust updates, better doc testing ([#598](https://github.com/TimelyDataflow/timely-dataflow/pull/598))\n- Simplify communication `Message` type ([#596](https://github.com/TimelyDataflow/timely-dataflow/pull/596))\n\n## 0.13.0 - 2024-10-29\n\nChangelog bankruptcy declared.\n\n## 0.12.0\n\nThe `Timestamp` trait has a new method `minimim()` that replaces Timely's use of `Default::default()` for default capabilities. The most pressing reason for this is the use of signed integers for timestamps, where Timely would effectively prevent the use of negative numbers by providing the default value of zero for capabilities. This should not have reduced any functionality, but might provide surprising output for programs that use integer timestamps and do not first advance timestamps (the tidy `0` will be replaced with `_::min_value()`).\n\n### Added\n\nTimely configuration can now be done with the `worker::Config` type, which supports user-defined configuration options.\nThe `get` and `set` methods allow binding arbitrary strings to arbitrary types `T: Send + Sync + 'static`.\nFor example, differential dataflow uses this mechanism to configure its background compaction rates.\n\n### Removed\n\nRemoved all deprecated methods and traits.\n\nTimely no longer responds to the `DEFAULT_PROGRESS_MODE` environment variable. Instead, it uses the newly added `worker::Config`.\n\nRemoved the `sort` crate, whose sorting methods are interesting but not currently a core part of timely dataflow.\n\n### Changed\n\nThe default progress mode changed from \"eager\" to \"demand driven\", which causes progress updates to be accumulated for longer before transmission. The eager default had the potential to produce catastrophically large volumes of progress update messages, for the benefit of a reduced critical path latency. The demand-driven default removes the potential for catastrophic failure at the expense of an increase minimal latency. This should give a better default experience as one scales up the to large numbers of workers.\n\n## 0.11.0\n\nReduce the amount of log flushing, and increase the batching of log messages.\n\n## 0.10.0\n\n### Added\n\nA `Worker` now has a `step_or_park(Option<Duration>)` method, which instructs the worker to take a step and gives it permission to park the worker thread for at most the supplied timeout if there is no work to perform. A value of `None` implies no timeout (unboundedly parked) whereas a value of `Some(0)` should return immediately. The communication layers are implemented to awaken workers if they receive new communications, and workers should hand out copies of their `Thread` if they want other threads to wake them for other reasons (e.g. queues from threads external to timely).\n\nCommunication `WorkerGuards` expose their underlying join handles to allow the main thread or others to unpark worker threads that may be parked (for example, after pushing new data into a queue shared with the worker).\n\n## 0.9.0\n\n### Added\n\nThe `OperatorInfo` struct now contains the full address of the operator as a `Vec<usize>`.\n\n### Changed\n\nThe `source` operator requires a closure that accepts an `OperatorInfo` struct in addition to its initial capability. This brings it to parity with the other closure-based operators, and is required to provide address information to the operator.\n\nThe address associated with each operator, a `[usize]` used to start with the identifier of the worker hosting the operator, followed by the dataflow identifier and the path down the dataflow to the operator. The worker identifier has been removed.\n\nThe `Worker` and the `Subgraph` operator no longer schedules all of their child dataflows and scopes by default. Instead, they track \"active\" children and schedule only those. Operators become active by receiving a message, a progress update, or by explicit activation. Some operators, source as `source`, have no inputs and will require explicit activation to run more than once. Operators that yield before completing all of their work (good for you!) should explicitly re-activate themselves to ensure they are re-scheduled even if they receive no further messages or progress updates. Documentation examples for the `source` method demonstrate this.\n\nThe `dataflow_using` method has been generalized to support arbitrary dataflow names, loggers, and additional resources the dataflow should keep alive. Its name has been changed to `dataflow_core`.\n\nYou can now construct `feedback` operators with a `Default::default()` path summary, which has the ability to not increment timestamps. Instead of panicking, Timely's reachability module will inform you if a non-incrementing cycle is detected, at which point you should probably double check your code. It is not 100% known what the system will do in this case (e.g., the progress tracker may enter a non-terminating loop; this is on you, not us ;)).\n\n## 0.8.0\n\nThis release made several breaking modifications to the types associated with scopes, and in particular the generic parameters for the `Child<'a, G: ScopeParent, T: Timestamp>` type. Where previously the `T` parameter would be the *new coordinate* to add to `G`'s timestamp, it is now the *new timestamp* including `G`'s timestamp as well. This was done to support a broader class of timestamps to be used, beyond always requiring product combinations with new timestamps.\n\nBeneficial fallouts include our ability to remove `RootTimestamp`, as dataflows can now be timestamped by `usize` or other primitive timestamps. Yay!\n\n### Added\n\n- The communication crate now has a `bincode` feature flag which should swing serialization over to use serde's `Serialize` trait. While it seems to work the ergonomics are likely in flux, as the choice is crate-wide and doesn't allow you to pick and choose a la carte.\n\n- Timestamps may now implement a new `Refines` trait which allows one to describe one timestamp as a refinement of another. This is mainly used to describe which timestamps may be used for subscopes of an outer scope. The trait describes how to move between the timestamps (informally: \"adding a zero\" and \"removing the inner coordinate\") and how to summarize path summaries for the refining timestamp as those of the refined timestamp.\n\n### Changed\n\n- Many logging events have been rationalized. Operators and Channels should all have a worker-unique identifier that can be used to connect their metadata with events involving them. Previously this was a bit of a shambles.\n\n- The `Scope` method `scoped` now allows new scopes with non-`Product` timestamps. Instead, the new timestamp must implement `Refines<_>` of the parent timestamp. This is the case for `Product` timestamps, but each timestamp also refines itself (allowing logical regions w/o changing the timestamp), and other timestamp combinators (e.g. Lexicographic) can be used.\n\n- Root dataflow timestamps no longer need to be `Product<RootTimestamp,_>`. Instead, the `_` can be used as the timestamp.\n\n- The `loop_variable` operator now takes a timestamp summary for the timestamp of its scope, not just the timestamp extending its parent scope. The old behavior can be recovered with `Product::new(Default::default(), summary)`, but the change allows cycles in more general scopes and seemed worth it. The operator also no longer takes a `limit`, and if you need to impose a limit other than the summary returning `None` you should use the `branch_when` operator.\n\n### Removed\n\n- The `RootTimestamp` and `RootSummary` types have been excised. Where you previously used `Product<RootTimestamp,T>` you can now use `Product<(),T>`, or even better just `T`. The requirement of a worker's `dataflow()` method is that the timestamp type implement `Refines<()>`, which .. ideally would be true for all timestamps but we can't have a blanket implementation until specialization lands (I believe).\n\n- Several race conditions were \"removed\" from the communication library. These mostly involved rapid construction of dataflows (data received before a channel was constructed would be dropped) and clean shutdown (a timely computation could drop and fail to ack clean shutdown messages).\n\n## 0.7.0\n\n### Added\n- You can now construct your own vector of allocator builders and supply them directly to `timely::execute::execute_from`. Previously one was restricted to whatever a `Configuration` could provide for you. This should allow more pleasant construction of custom allocators, or custom construction of existing allocators.\n- Each timely worker now has a log registry, `worker.log_registry()`, from which you can register and acquire typed loggers for named log streams. This supports user-level logging, as well as user-configurable timely logging. Timely logging is under the name `\"timely\"`.\n\n### Changed\n- The `Root` type has been renamed `Worker` and is found in the `::worker` module. The methods of the `ScopeParent` trait are now in the `::worker::AsWorker` trait.\n- The communication `Allocate` trait's main method `allocate` now takes a worker-unique identifier to use for the channel. The allocator may or may not use the information (most often for logging), but they are allowed to be incorrect if one allocates two channels with the same identifier.\n- A `CapabilityRef<T>` now supports `retain_for(usize)` which indicates a specific output port the capability should be retain for use with. The `retain()` method still exists for now and is equivalent to `retain(0)`. This change also comes with the *inability* to use an arbitrary `Capability<T>` with any output; using a capability bound to the wrong output will result in a run-time error.\n- The `unary` and `binary` operators now provide `data` as a `RefOrMut`, which does not implement `DerefMut`. More information on how to port methods can be found [here](https://github.com/TimelyDataflow/timely-dataflow/pull/135#issuecomment-418355284).\n\n\n### Removed\n- The deprecated `Unary` and `Binary` operator extension traits have been removed in favor of the `Operator` trait that supports both of them, as well as their `_notify` variants.\n"
        },
        {
          "name": "CONTRIBUTING.md",
          "type": "blob",
          "size": 0.638671875,
          "content": "Thank you for your interest in contributing!\n\nHere is some legal stuff that will make you regret clicking on this link.\n\nBy submitting a pull request for this project, you are agreeing to license your contribution under the terms of the project's LICENSE file at the time of your submission (in case it changes or something). You are also certifying that you are in a position to make this agreement, in that you didn't nick your code from someone else, or some project with conflicting licensing requirements.\n\nIf you would like to put explicit copyright notices somewhere, please leave them in the repository's COPYRIGHT file rather than in each file.\n"
        },
        {
          "name": "COPYRIGHT",
          "type": "blob",
          "size": 0.22265625,
          "content": "Contributions by Andrea Lattuada <andreal@student.ethz.ch> are Copyright (c) 2016 Andrea Lattuada, ETH Zürich.\nContributions by Moritz Hoffmann <moritz.hoffmann@inf.ethz.ch> are Copyright (c) 2017 Moritz Hoffmann, ETH Zürich.\n"
        },
        {
          "name": "Cargo.toml",
          "type": "blob",
          "size": 0.3115234375,
          "content": "[workspace]\nmembers = [\n    \"bytes\",\n    \"communication\",\n    \"container\",\n    \"logging\",\n    \"timely\",\n]\n\nresolver = \"2\"\n\n[workspace.package]\nedition = \"2021\"\n\n[profile.release]\nopt-level = 3\ndebug = true\nrpath = false\n# Disable LTO because it causes erratic behavior in Rust 1.53\n#lto = true\ndebug-assertions = false\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 1.056640625,
          "content": "The MIT License (MIT)\n\nCopyright (c) 2014 Frank McSherry\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 14.39453125,
          "content": "# Timely Dataflow #\n\nTimely dataflow is a low-latency cyclic dataflow computational model, introduced in the paper [Naiad: a timely dataflow system](http://dl.acm.org/citation.cfm?id=2522738). This project is an extended and more modular implementation of timely dataflow in Rust.\n\nThis project is something akin to a distributed data-parallel compute engine, which scales the same program up from a single thread on your laptop to distributed execution across a cluster of computers. The main goals are expressive power and high performance. It is probably strictly more expressive and faster than whatever you are currently using, assuming you aren't yet using timely dataflow.\n\nBe sure to read the [documentation for timely dataflow](https://docs.rs/timely). It is a work in progress, but mostly improving. There is more [long-form text](https://timelydataflow.github.io/timely-dataflow/) in `mdbook` format with examples tested against the current builds. There is also a series of blog posts ([part 1](https://github.com/frankmcsherry/blog/blob/master/posts/2015-09-14.md), [part 2](https://github.com/frankmcsherry/blog/blob/master/posts/2015-09-18.md), [part 3](https://github.com/frankmcsherry/blog/blob/master/posts/2015-09-21.md)) introducing timely dataflow in a different way, though be warned that the examples there may need tweaks to build against the current code.\n\n# An example\n\nTo use timely dataflow, add the following to the dependencies section of your project's `Cargo.toml` file:\n\n```toml\n[dependencies]\ntimely=\"*\"\n```\n\nThis will bring in the [`timely` crate](https://crates.io/crates/timely) from [crates.io](http://crates.io), which should allow you to start writing timely dataflow programs like this one (also available in [timely/examples/simple.rs](https://github.com/timelydataflow/timely-dataflow/blob/master/timely/examples/simple.rs)):\n\n```rust\nuse timely::dataflow::operators::*;\n\nfn main() {\n    timely::example(|scope| {\n        (0..10).to_stream(scope)\n               .inspect(|x| println!(\"seen: {:?}\", x));\n    });\n}\n```\n\nYou can run this example from the root directory of the `timely-dataflow` repository by typing\n\n```text\n% cargo run --example simple\nRunning `target/debug/examples/simple`\nseen: 0\nseen: 1\nseen: 2\nseen: 3\nseen: 4\nseen: 5\nseen: 6\nseen: 7\nseen: 8\nseen: 9\n```\n\nThis is a very simple example (it's in the name), which only just suggests at how you might write dataflow programs.\n\n## Doing more things\n\nFor a more involved example, consider the very similar (but more explicit) [examples/hello.rs](https://github.com/timelydataflow/timely-dataflow/blob/master/timely/examples/hello.rs), which creates and drives the dataflow separately:\n\n```rust\nuse timely::dataflow::{InputHandle, ProbeHandle};\nuse timely::dataflow::operators::{Input, Exchange, Inspect, Probe};\n\nfn main() {\n    // initializes and runs a timely dataflow.\n    timely::execute_from_args(std::env::args(), |worker| {\n\n        let index = worker.index();\n        let mut input = InputHandle::new();\n        let mut probe = ProbeHandle::new();\n\n        // create a new input, exchange data, and inspect its output\n        worker.dataflow(|scope| {\n            scope.input_from(&mut input)\n                 .exchange(|x| *x)\n                 .inspect(move |x| println!(\"worker {}:\\thello {}\", index, x))\n                 .probe_with(&mut probe);\n        });\n\n        // introduce data and watch!\n        for round in 0..10 {\n            if index == 0 {\n                input.send(round);\n            }\n            input.advance_to(round + 1);\n            while probe.less_than(input.time()) {\n                worker.step();\n            }\n        }\n    }).unwrap();\n}\n```\n\nThis example does a fair bit more, to show off more of what timely can do for you.\n\nWe first build a dataflow graph creating an input stream (with `input_from`), whose output we `exchange` to drive records between workers (using the data itself to indicate which worker to route to). We `inspect` the data and print the worker index to indicate which worker received which data, and then `probe` the result so that each worker can see when all of a given round of data has been processed.\n\nWe then drive the computation by repeatedly introducing rounds of data, where the `round` itself is used as the data. In each round, each worker introduces the same data, and then repeatedly takes dataflow steps until the `probe` reveals that all workers have processed all work for that epoch, at which point the computation proceeds.\n\nWith two workers, the output looks like\n```text\n% cargo run --example hello -- -w2\nRunning `target/debug/examples/hello -w2`\nworker 0:   hello 0\nworker 1:   hello 1\nworker 0:   hello 2\nworker 1:   hello 3\nworker 0:   hello 4\nworker 1:   hello 5\nworker 0:   hello 6\nworker 1:   hello 7\nworker 0:   hello 8\nworker 1:   hello 9\n```\n\nNote that despite worker zero introducing the data `(0..10)`, each element is routed to a specific worker, as we intended.\n\n# Execution\n\nThe `hello.rs` program above will by default use a single worker thread. To use multiple threads in a process, use the `-w` or `--workers` options followed by the number of threads you would like to use. (note: the `simple.rs` program always uses one worker thread; it uses `timely::example` which ignores user-supplied input).\n\nTo use multiple processes, you will need to use the `-h` or `--hostfile` option to specify a text file whose lines are `hostname:port` entries corresponding to the locations you plan on spawning the processes. You will need to use the `-n` or `--processes` argument to indicate how many processes you will spawn (a prefix of the host file), and each process must use the `-p` or `--process` argument to indicate their index out of this number.\n\nSaid differently, you want a hostfile that looks like so,\n```text\n% cat hostfile.txt\nhost0:port\nhost1:port\nhost2:port\nhost3:port\n...\n```\nand then to launch the processes like so:\n```text\nhost0% cargo run -- -w 2 -h hostfile.txt -n 4 -p 0\nhost1% cargo run -- -w 2 -h hostfile.txt -n 4 -p 1\nhost2% cargo run -- -w 2 -h hostfile.txt -n 4 -p 2\nhost3% cargo run -- -w 2 -h hostfile.txt -n 4 -p 3\n```\nThe number of workers should be the same for each process.\n\n# The ecosystem\n\nTimely dataflow is intended to support multiple levels of abstraction, from the lowest level manual dataflow assembly, to higher level \"declarative\" abstractions.\n\nThere are currently a few options for writing timely dataflow programs. Ideally this set will expand with time, as interested people write their own layers (or build on those of others).\n\n* [**Timely dataflow**](https://docs.rs/timely/latest/timely/dataflow/operators/index.html): Timely dataflow includes several primitive operators, including standard operators like `map`, `filter`, and `concat`. It also includes more exotic operators for tasks like entering and exiting loops (`enter` and `leave`), as well as generic operators whose implementations can be supplied using closures (`unary` and `binary`).\n\n* [**Differential dataflow**](https://github.com/timelydataflow/differential-dataflow): A higher-level language built on timely dataflow, differential dataflow includes operators like `group`, `join`, and `iterate`. Its implementation is fully incrementalized, and the details are pretty cool (if mysterious).\n\nThere are also a few applications built on timely dataflow, including [a streaming worst-case optimal join implementation](https://github.com/frankmcsherry/dataflow_join) and a [PageRank](https://github.com/frankmcsherry/pagerank) implementation, both of which should provide helpful examples of writing timely dataflow programs.\n\n# Contributing\n\nIf you are interested in working with or helping out with timely dataflow, great!\n\nThere are a few classes of work that are helpful for us, and may be interesting for you. There are a few broad categories, and then an ever-shifting pile of issues of various complexity.\n\n* If you would like to write programs using timely dataflow, this is very interesting for us. Ideally timely dataflow is meant to be an ergonomic approach to a non-trivial class of dataflow computations. As people use it and report back on their experiences, we learn about the classes of bugs they find, the ergonomic pain points, and other things we didn't even imagine ahead of time. Learning about timely dataflow, trying to use it, and reporting back is helpful!\n\n* If you like writing little example programs or documentation tests, there are many places throughout timely dataflow where the examples are relatively sparse, or do not actually test the demonstrated functionality. These can often be easy to pick up, flesh out, and push without a large up-front obligation. It is probably also a great way to get one of us to explain something in detail to you, if that is what you are looking for.\n\n* If you like the idea of getting your hands dirty in timely dataflow, the [issue tracker](https://github.com/timelydataflow/timely-dataflow/issues) has a variety of issues that touch on different levels of the stack. For example:\n\n    * Timely currently [does more copies of data than it must](https://github.com/timelydataflow/timely-dataflow/issues/111), in the interest of appeasing Rust's ownership discipline most directly. Several of these copies could be elided with some more care in the resource management (for example, using shared regions of one `Vec<u8>` in the way that the [bytes crate](https://crates.io/crates/bytes) does). Not everything is obvious here, so there is the chance for a bit of design work too.\n\n    * We recently landed a bunch of logging changes, but there is still [a list of nice to have features](https://github.com/timelydataflow/timely-dataflow/issues/114) that haven't made it yet. If you are interested in teasing out how timely works in part by poking around at the infrastructure that records what it does, this could be a good fit! It has the added benefit that the logs are timely streams themselves, so you can even do some log processing on timely. Whoa...\n\n    * There is an open issue on [integrating Rust ownership idioms into timely dataflow](https://github.com/timelydataflow/timely-dataflow/issues/77). Right now, timely streams are of cloneable objects, and when a stream is re-used, items will be cloned. We could make that more explicit, and require calling a `.cloned()` method to get owned objects in the same way that iterators require it. At the same time, using a reference to a stream without taking ownership should get you the chance to look at the records that go past without taking ownership (and without requiring a clone, as is currently done). This is often plenty for exchange channels which may need to serialize the data and can't take much advantage of ownership anyhow.\n\n    * There is a bunch of interesting work in scheduling timely dataflow operators, where when given the chance to schedule many operators, we might think for a moment and realize that several of them have no work to do and can be skipped. Better, we might maintain the list of operators with anything to do, and do nothing for those without work to do.\n\nThere are also some larger themes of work, whose solutions are not immediately obvious and each with the potential to sort out various performance issues:\n\n## Rate-controlling output\n\nAt the moment, the implementations of `unary` and `binary` operators allow their closures to send un-bounded amounts of output. This can cause unwelcome resource exhaustion, and poor performance generally if the runtime needs to allocate lots of new memory to buffer data sent in bulk without being given a chance to digest it. It is commonly the case that when large amounts of data are produced, they are eventually reduced given the opportunity.\n\nWith the current interfaces there is not much to be done. One possible change would be to have the `input` and `notificator` objects ask for a closure from an input message or timestamp, respectively, to an output iterator. This gives the system the chance to play the iterator at the speed they feel is appropriate. As many operators produce data-parallel output (based on independent keys), it may not be that much of a burden to construct such iterators.\n\n## Buffer management\n\nThe timely communication layer currently discards most buffers it moves through exchange channels, because it doesn't have a sane way of rate controlling the output, nor a sane way to determine how many buffers should be cached. If either of these problems were fixed, it would make sense to recycle the buffers to avoid random allocations, especially for small batches. These changes have something like a 10%-20% performance impact in the `dataflow-join` triangle computation workload.\n\n## Support for non-serializable types\n\nThe communication layer is based on a type `Content<T>` which can be backed by typed or binary data. Consequently, it requires that the type it supports be serializable, because it needs to have logic for the case that the data is binary, even if this case is not used. It seems like the `Stream` type should be extendable to be parametric in the type of storage used for the data, so that we can express the fact that some types are not serializable and that this is ok.\n\n**NOTE**: Differential dataflow demonstrates how to do this at the user level in its `operators/arrange.rs`, if somewhat sketchily (with a wrapper that lies about the properties of the type it transports).\n\nThis would allow us to safely pass `Rc<T>` types around, as long as we use the `Pipeline` parallelization contract.\n\n## Coarse- vs fine-grained timestamps\n\nThe progress tracking machinery involves some non-trivial overhead per timestamp. This means that using very fine-grained timestamps, for example the nanosecond at which a record is processed, can swamp the progress tracking logic. By contrast, the logging infrastructure demotes nanoseconds to data, part of the logged payload, and approximates batches of events with the smallest timestamp in the batch. This is less accurate from a progress tracking point of view, but more performant. It may be possible to generalize this so that users can write programs without thinking about granularity of timestamp, and the system automatically coarsens when possible (essentially boxcar-ing times).\n\n**NOTE**: Differential dataflow demonstrates how to do this at the user level in its `collection.rs`. The lack of system support means that the user ends up indicating the granularity, which isn't horrible but could plausibly be improved. It may also be that leaving the user with control of the granularity leaves them with more control over the latency/throughput trade-off, which could be a good thing for the system to do.\n"
        },
        {
          "name": "bytes",
          "type": "tree",
          "content": null
        },
        {
          "name": "communication",
          "type": "tree",
          "content": null
        },
        {
          "name": "container",
          "type": "tree",
          "content": null
        },
        {
          "name": "logging",
          "type": "tree",
          "content": null
        },
        {
          "name": "mdbook",
          "type": "tree",
          "content": null
        },
        {
          "name": "release-plz.toml",
          "type": "blob",
          "size": 0.2607421875,
          "content": "[workspace]\n# disable the changelog for all packages\nchangelog_update = false\n\n[[package]]\nname = \"timely\"\n# enable the changelog for this package\nchangelog_update = true\n# set the path of the changelog to the root of the repository\nchangelog_path = \"./CHANGELOG.md\"\n"
        },
        {
          "name": "timely",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}