{
  "metadata": {
    "timestamp": 1736708988096,
    "page": 38,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjQw",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "LaurentMazare/tch-rs",
      "stars": 4490,
      "defaultBranch": "main",
      "files": [
        {
          "name": ".cargo",
          "type": "tree",
          "content": null
        },
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.1533203125,
          "content": "target/\n_build/\ndata/\n.idea/\ngen/.merlin\n**/*.rs.bk\n*.swp\n*.swo\nCargo.lock\n__pycache__\n*~\n.*~\n.vscode/\n\n*.ot\n*.safetensors\n*.so\n*.dylib\nllama-tokenizer.json\n"
        },
        {
          "name": "CHANGELOG.md",
          "type": "blob",
          "size": 4.615234375,
          "content": "# Changelog\nThis documents the main changes to the `tch` crate.\n\nThe format is based on [Keep a Changelog](https://keepachangelog.com/en/1.0.0/)\nand this project adheres to [Semantic Versioning](https://semver.org/spec/v2.0.0.html).\n\n## Unreleased\n### Changed\n\n## v0.18.1\n### Changed\n- PyTorch v2.5.1 support\n\n## v0.18.0\n### Changed\n- PyTorch v2.5 support\n\n## v0.17.0\n### Changed\n- PyTorch v2.4 support\n\n## v0.16.0\n### Changed\n- PyTorch v2.3 support\n\n## v0.15.0\n### Changed\n- PyTorch v2.2 support\n\n## v0.14.0\n### Changed\n- PyTorch v2.1 support\n  [803](https://github.com/LaurentMazare/tch-rs/pull/803).\n- Add a `pyo3-tch` crate for interacting with Python via PyO3\n  [730](https://github.com/LaurentMazare/tch-rs/pull/730).\n- Expose the cuda fuser enabled flag,\n  [728](https://github.com/LaurentMazare/tch-rs/pull/728).\n- Improved the safetensor error wrapping,\n  [720](https://github.com/LaurentMazare/tch-rs/pull/720).\n\n## v0.13.0 - 2023-05-18\n### Added\n- Support static linking in the build script,\n  [712](https://github.com/LaurentMazare/tch-rs/pull/712).\n- Make the libtorch download opt-in rather than a default behavior. The libtorch\n  library download can still be triggered by enabling the `download-libtorch`\n  feature, [707](https://github.com/LaurentMazare/tch-rs/pull/707).\n- Rename the `of_...` conversion functions to `from_...` so as to be closer to\n  the Rust best practices,\n  [706](https://github.com/LaurentMazare/tch-rs/pull/706). This is a breaking\n  change and will require modifying calls such as `of_slice` to be `from_slice`\n  instead.\n- Expose some functions so that Python extensions that operates on PyTorch\n  tensors can be written with `tch`,\n  [704](https://github.com/LaurentMazare/tch-rs/pull/704).\n- Rework the torch-sys build script making it easier to leverage a Python\n  PyTorch install as a source for libtorch,\n  [703](https://github.com/LaurentMazare/tch-rs/pull/703).\n\n## v0.12.0 - 2023-05-10\n### Changed\n- EfficientNet models have been reworked, pre-trained models used `safetensors`\n  weight by default, [679](https://github.com/LaurentMazare/tch-rs/pull/679).\n- None can be used for nullable scalar types,\n  [680](https://github.com/LaurentMazare/tch-rs/pull/680).\n- Automated conversion of list arguments: all the generated functions that take\n  as input a slice of int or float can now be used directly with int values or\n  fixed length arrays [682](https://github.com/LaurentMazare/tch-rs/pull/682).\n- Replace the `From<Tensor>` traits with some `TryFrom` versions,\n  [683](https://github.com/LaurentMazare/tch-rs/pull/683). This is a breaking\n  change, note that also the old version would flatten the tensor if needed to\n  reduce the number of dimensions, this has to be done explicitely with the new\n  version.\n\n## v0.11.0 - 2023-03-20\n### Added\n- Adapt to C++ PyTorch library (`libtorch`) version `v2.0.0`.\n### Changed\n- Update the `half` dependency to version `2`, [646](https://github.com/LaurentMazare/tch-rs/pull/646).\n\n## v0.10.3 - 2023-02-23\n### Added\n- Add some helper functions in a utils module to check the available devices and versions.\n\n## v0.10.2 - 2023-02-19\n### Added\n- Add the `eps` and `amsgrad` options to Adam, [600](https://github.com/LaurentMazare/tch-rs/pull/600).\n### Changed\n- Fix loading of `VarStore` when using `Mps` devices, [623](https://github.com/LaurentMazare/tch-rs/pull/623).\n- Use `ureq` instead of `curl` to reduce compile times, [620](https://github.com/LaurentMazare/tch-rs/pull/620).\n- Fix the handling of dicts in TorchScript, [597](https://github.com/LaurentMazare/tch-rs/issues/597).\n\n## v0.10.1 - 2022-12-12\n### Changed\n- Default `vs.load` to use the Python weight format when the file extension is `.pt` or `.bin`.\n\n## v0.10.0 - 2022-12-12\n### Added\n- Expose functions for setting manual seeds for CUDA devices, [#500](https://github.com/LaurentMazare/tch-rs/pull/500).\n- Expose functions for triggering manual sync, [#500](https://github.com/LaurentMazare/tch-rs/pull/500).\n- Add some functions to load Python weight files.\n### Changed\n- Extending the Kaiming initialization, [#573](https://github.com/LaurentMazare/tch-rs/pull/573).\n\n## v0.9.0 - 2022-11-05\n### Changed\n- Adapt to C++ PyTorch library (`libtorch`) version `v1.13.0`.\n\n## v0.8.0 - 2022-07-04\n### Changed\n- Adapt to C++ PyTorch library (`libtorch`) version `v1.12.0`.\n\n## v0.7.2 - 2022-05-16\n### Changed\n- Adapt to C++ PyTorch library (`libtorch`) version `v1.11.0`.\n\n## v0.6.1 - 2021-10-25\n### Changed\n- Adapt to C++ PyTorch library (`libtorch`) version `v1.10.0`.\n\n## v0.5.0 - 2021-06-25\n### Changed\n- Adapt to C++ PyTorch library (`libtorch`) version `v1.9.0`.\n\n## v0.4.1 - 2021-05-08\n### Changed\n- Adapt to C++ PyTorch library (`libtorch`) version `v1.8.1`.\n"
        },
        {
          "name": "Cargo.toml",
          "type": "blob",
          "size": 1.525390625,
          "content": "[package]\nname = \"tch\"\nversion = \"0.18.1\"\nauthors = [\"Laurent Mazare <lmazare@gmail.com>\"]\nedition = \"2021\"\nbuild = \"build.rs\"\n\ndescription = \"Rust wrappers for the PyTorch C++ api (libtorch).\"\nrepository = \"https://github.com/LaurentMazare/tch-rs\"\nkeywords = [\"pytorch\", \"deep-learning\", \"machine-learning\"]\ncategories = [\"science\"]\nlicense = \"MIT/Apache-2.0\"\nreadme = \"README.md\"\n\nexclude = [\n    \"examples/stable-diffusion/media/*\",\n]\n\n[dependencies]\nlazy_static = \"1.3.0\"\nlibc = \"0.2.0\"\nndarray = \"0.16.1\"\nrand = \"0.8\"\nthiserror = \"1\"\ntorch-sys = { version = \"0.18.1\", path = \"torch-sys\" }\nzip = \"0.6\"\nhalf = \"2\"\nsafetensors = \"0.3.0\"\n\ncpython = { version = \"0.7.1\", optional = true }\nregex = { version = \"1.6.0\", optional = true }\nimage = { version = \"0.24.5\", optional = true }\nclap = { version = \"4.2.4\", features = [\"derive\"], optional = true }\nserde_json = { version = \"1.0.96\", optional = true }\nmemmap2 = { version = \"0.6.1\", optional = true }\n\n[dev-dependencies]\nanyhow = \"^1.0.60\"\n\n[workspace]\nmembers = [\n  \"torch-sys\",\n  \"pyo3-tch\",\n  \"examples/python-extension\",\n]\n\n[features]\ndownload-libtorch = [\"torch-sys/download-libtorch\"]\npython-extension = [\"torch-sys/python-extension\"]\nrl-python = [\"cpython\"]\ndoc-only = [\"torch-sys/doc-only\"]\ncuda-tests = []\n\n[package.metadata.docs.rs]\nfeatures = [ \"doc-only\" ]\n\n[[example]]\nname = \"reinforcement-learning\"\nrequired-features = [\"rl-python\"]\n\n[[example]]\nname = \"stable-diffusion\"\nrequired-features = [\"regex\"]\n\n[[example]]\nname = \"llama\"\nrequired-features = [\"regex\", \"clap\", \"serde_json\", \"memmap2\"]\n"
        },
        {
          "name": "LICENSE-APACHE",
          "type": "blob",
          "size": 11.0908203125,
          "content": "                                 Apache License\n                           Version 2.0, January 2004\n                        http://www.apache.org/licenses/\n\n   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n\n   1. Definitions.\n\n      \"License\" shall mean the terms and conditions for use, reproduction,\n      and distribution as defined by Sections 1 through 9 of this document.\n\n      \"Licensor\" shall mean the copyright owner or entity authorized by\n      the copyright owner that is granting the License.\n\n      \"Legal Entity\" shall mean the union of the acting entity and all\n      other entities that control, are controlled by, or are under common\n      control with that entity. For the purposes of this definition,\n      \"control\" means (i) the power, direct or indirect, to cause the\n      direction or management of such entity, whether by contract or\n      otherwise, or (ii) ownership of fifty percent (50%) or more of the\n      outstanding shares, or (iii) beneficial ownership of such entity.\n\n      \"You\" (or \"Your\") shall mean an individual or Legal Entity\n      exercising permissions granted by this License.\n\n      \"Source\" form shall mean the preferred form for making modifications,\n      including but not limited to software source code, documentation\n      source, and configuration files.\n\n      \"Object\" form shall mean any form resulting from mechanical\n      transformation or translation of a Source form, including but\n      not limited to compiled object code, generated documentation,\n      and conversions to other media types.\n\n      \"Work\" shall mean the work of authorship, whether in Source or\n      Object form, made available under the License, as indicated by a\n      copyright notice that is included in or attached to the work\n      (an example is provided in the Appendix below).\n\n      \"Derivative Works\" shall mean any work, whether in Source or Object\n      form, that is based on (or derived from) the Work and for which the\n      editorial revisions, annotations, elaborations, or other modifications\n      represent, as a whole, an original work of authorship. For the purposes\n      of this License, Derivative Works shall not include works that remain\n      separable from, or merely link (or bind by name) to the interfaces of,\n      the Work and Derivative Works thereof.\n\n      \"Contribution\" shall mean any work of authorship, including\n      the original version of the Work and any modifications or additions\n      to that Work or Derivative Works thereof, that is intentionally\n      submitted to Licensor for inclusion in the Work by the copyright owner\n      or by an individual or Legal Entity authorized to submit on behalf of\n      the copyright owner. For the purposes of this definition, \"submitted\"\n      means any form of electronic, verbal, or written communication sent\n      to the Licensor or its representatives, including but not limited to\n      communication on electronic mailing lists, source code control systems,\n      and issue tracking systems that are managed by, or on behalf of, the\n      Licensor for the purpose of discussing and improving the Work, but\n      excluding communication that is conspicuously marked or otherwise\n      designated in writing by the copyright owner as \"Not a Contribution.\"\n\n      \"Contributor\" shall mean Licensor and any individual or Legal Entity\n      on behalf of whom a Contribution has been received by Licensor and\n      subsequently incorporated within the Work.\n\n   2. Grant of Copyright License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      copyright license to reproduce, prepare Derivative Works of,\n      publicly display, publicly perform, sublicense, and distribute the\n      Work and such Derivative Works in Source or Object form.\n\n   3. Grant of Patent License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      (except as stated in this section) patent license to make, have made,\n      use, offer to sell, sell, import, and otherwise transfer the Work,\n      where such license applies only to those patent claims licensable\n      by such Contributor that are necessarily infringed by their\n      Contribution(s) alone or by combination of their Contribution(s)\n      with the Work to which such Contribution(s) was submitted. If You\n      institute patent litigation against any entity (including a\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\n      or a Contribution incorporated within the Work constitutes direct\n      or contributory patent infringement, then any patent licenses\n      granted to You under this License for that Work shall terminate\n      as of the date such litigation is filed.\n\n   4. Redistribution. You may reproduce and distribute copies of the\n      Work or Derivative Works thereof in any medium, with or without\n      modifications, and in Source or Object form, provided that You\n      meet the following conditions:\n\n      (a) You must give any other recipients of the Work or\n          Derivative Works a copy of this License; and\n\n      (b) You must cause any modified files to carry prominent notices\n          stating that You changed the files; and\n\n      (c) You must retain, in the Source form of any Derivative Works\n          that You distribute, all copyright, patent, trademark, and\n          attribution notices from the Source form of the Work,\n          excluding those notices that do not pertain to any part of\n          the Derivative Works; and\n\n      (d) If the Work includes a \"NOTICE\" text file as part of its\n          distribution, then any Derivative Works that You distribute must\n          include a readable copy of the attribution notices contained\n          within such NOTICE file, excluding those notices that do not\n          pertain to any part of the Derivative Works, in at least one\n          of the following places: within a NOTICE text file distributed\n          as part of the Derivative Works; within the Source form or\n          documentation, if provided along with the Derivative Works; or,\n          within a display generated by the Derivative Works, if and\n          wherever such third-party notices normally appear. The contents\n          of the NOTICE file are for informational purposes only and\n          do not modify the License. You may add Your own attribution\n          notices within Derivative Works that You distribute, alongside\n          or as an addendum to the NOTICE text from the Work, provided\n          that such additional attribution notices cannot be construed\n          as modifying the License.\n\n      You may add Your own copyright statement to Your modifications and\n      may provide additional or different license terms and conditions\n      for use, reproduction, or distribution of Your modifications, or\n      for any such Derivative Works as a whole, provided Your use,\n      reproduction, and distribution of the Work otherwise complies with\n      the conditions stated in this License.\n\n   5. Submission of Contributions. Unless You explicitly state otherwise,\n      any Contribution intentionally submitted for inclusion in the Work\n      by You to the Licensor shall be under the terms and conditions of\n      this License, without any additional terms or conditions.\n      Notwithstanding the above, nothing herein shall supersede or modify\n      the terms of any separate license agreement you may have executed\n      with Licensor regarding such Contributions.\n\n   6. Trademarks. This License does not grant permission to use the trade\n      names, trademarks, service marks, or product names of the Licensor,\n      except as required for reasonable and customary use in describing the\n      origin of the Work and reproducing the content of the NOTICE file.\n\n   7. Disclaimer of Warranty. Unless required by applicable law or\n      agreed to in writing, Licensor provides the Work (and each\n      Contributor provides its Contributions) on an \"AS IS\" BASIS,\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n      implied, including, without limitation, any warranties or conditions\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n      PARTICULAR PURPOSE. You are solely responsible for determining the\n      appropriateness of using or redistributing the Work and assume any\n      risks associated with Your exercise of permissions under this License.\n\n   8. Limitation of Liability. In no event and under no legal theory,\n      whether in tort (including negligence), contract, or otherwise,\n      unless required by applicable law (such as deliberate and grossly\n      negligent acts) or agreed to in writing, shall any Contributor be\n      liable to You for damages, including any direct, indirect, special,\n      incidental, or consequential damages of any character arising as a\n      result of this License or out of the use or inability to use the\n      Work (including but not limited to damages for loss of goodwill,\n      work stoppage, computer failure or malfunction, or any and all\n      other commercial damages or losses), even if such Contributor\n      has been advised of the possibility of such damages.\n\n   9. Accepting Warranty or Additional Liability. While redistributing\n      the Work or Derivative Works thereof, You may choose to offer,\n      and charge a fee for, acceptance of support, warranty, indemnity,\n      or other liability obligations and/or rights consistent with this\n      License. However, in accepting such obligations, You may act only\n      on Your own behalf and on Your sole responsibility, not on behalf\n      of any other Contributor, and only if You agree to indemnify,\n      defend, and hold each Contributor harmless for any liability\n      incurred by, or claims asserted against, such Contributor by reason\n      of your accepting any such warranty or additional liability.\n\n   END OF TERMS AND CONDITIONS\n\n   APPENDIX: How to apply the Apache License to your work.\n\n      To apply the Apache License to your work, attach the following\n      boilerplate notice, with the fields enclosed by brackets \"[]\"\n      replaced with your own identifying information. (Don't include\n      the brackets!)  The text should be enclosed in the appropriate\n      comment syntax for the file format. We also recommend that a\n      file or class name and description of purpose be included on the\n      same \"printed page\" as the copyright notice for easier\n      identification within third-party archives.\n\n   Copyright [yyyy] [name of copyright owner]\n\n   Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.\n"
        },
        {
          "name": "LICENSE-MIT",
          "type": "blob",
          "size": 0.9990234375,
          "content": "Permission is hereby granted, free of charge, to any\nperson obtaining a copy of this software and associated\ndocumentation files (the \"Software\"), to deal in the\nSoftware without restriction, including without\nlimitation the rights to use, copy, modify, merge,\npublish, distribute, sublicense, and/or sell copies of\nthe Software, and to permit persons to whom the Software\nis furnished to do so, subject to the following\nconditions:\n\nThe above copyright notice and this permission notice\nshall be included in all copies or substantial portions\nof the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF\nANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED\nTO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A\nPARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT\nSHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY\nCLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION\nOF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR\nIN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER\nDEALINGS IN THE SOFTWARE.\n"
        },
        {
          "name": "Makefile",
          "type": "blob",
          "size": 0.2236328125,
          "content": "test: .FORCE\n\tcargo test\n\nclean: .FORCE\n\tcargo clean\n\ngen: .FORCE\n\tdune exec gen/gen.exe\n\trustfmt src/wrappers/tensor_fallible_generated.rs\n\trustfmt src/wrappers/tensor_generated.rs\n\trustfmt torch-sys/src/c_generated.rs\n\n.FORCE:\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 14.5888671875,
          "content": "# tch-rs\nRust bindings for the C++ api of PyTorch. The goal of the `tch` crate is to\nprovide some thin wrappers around the C++ PyTorch api (a.k.a. libtorch). It\naims at staying as close as possible to the original C++ api. More idiomatic\nrust bindings could then be developed on top of this. The\n[documentation](https://docs.rs/tch/) can be found on docs.rs.\n\n[![Build Status](https://github.com/LaurentMazare/tch-rs/workflows/Continuous%20integration/badge.svg)](https://github.com/LaurentMazare/tch-rs/actions)\n[![Latest version](https://img.shields.io/crates/v/tch.svg)](https://crates.io/crates/tch)\n[![Documentation](https://docs.rs/tch/badge.svg)](https://docs.rs/tch)\n[![Dependency Status](https://deps.rs/repo/github/LaurentMazare/tch-rs/status.svg)](https://deps.rs/repo/github/LaurentMazare/tch-rs)\n![License](https://img.shields.io/crates/l/tch.svg)\n[changelog](https://github.com/LaurentMazare/tch-rs/blob/main/CHANGELOG.md)\n\n\nThe code generation part for the C api on top of libtorch comes from\n[ocaml-torch](https://github.com/LaurentMazare/ocaml-torch).\n\n## Getting Started\n\nThis crate requires the C++ PyTorch library (libtorch) in version *v2.5.1* to be available on\nyour system. You can either:\n\n- Use the system-wide libtorch installation (default).\n- Install libtorch manually and let the build script know about it via the `LIBTORCH` environment variable.\n- Use a Python PyTorch install, to do this set `LIBTORCH_USE_PYTORCH=1`.\n- When a system-wide libtorch can't be found and `LIBTORCH` is not set, the\n  build script can download a pre-built binary version of libtorch by using\n  the `download-libtorch` feature. By default a CPU version is used. The\n  `TORCH_CUDA_VERSION` environment variable can be set to `cu117` in order to\n  get a pre-built binary using CUDA 11.7.\n\n### System-wide Libtorch\n\nOn linux platforms, the build script will look for a system-wide libtorch\nlibrary in `/usr/lib/libtorch.so`.\n\n### Python PyTorch Install\n\nIf the `LIBTORCH_USE_PYTORCH` environment variable is set, the active python\ninterpreter is called to retrieve information about the torch python package.\nThis version is then linked against.\n\n### Libtorch Manual Install\n\n- Get `libtorch` from the\n[PyTorch website download section](https://pytorch.org/get-started/locally/) and extract\nthe content of the zip file.\n- For Linux and macOS users, add the following to your `.bashrc` or equivalent, where `/path/to/libtorch`\nis the path to the directory that was created when unzipping the file.\n```bash\nexport LIBTORCH=/path/to/libtorch\n```\nThe header files location can also be specified separately from the shared library via\nthe following:\n```bash\n# LIBTORCH_INCLUDE must contain `include` directory.\nexport LIBTORCH_INCLUDE=/path/to/libtorch/\n# LIBTORCH_LIB must contain `lib` directory.\nexport LIBTORCH_LIB=/path/to/libtorch/\n```\n- For Windows users, assuming that `X:\\path\\to\\libtorch` is the unzipped libtorch directory.\n    - Navigate to Control Panel -> View advanced system settings -> Environment variables.\n    - Create the `LIBTORCH` variable and set it to `X:\\path\\to\\libtorch`.\n    - Append `X:\\path\\to\\libtorch\\lib` to the `Path` variable.\n\n  If you prefer to temporarily set environment variables, in PowerShell you can run\n```powershell\n$Env:LIBTORCH = \"X:\\path\\to\\libtorch\"\n$Env:Path += \";X:\\path\\to\\libtorch\\lib\"\n```\n- You should now be able to run some examples, e.g. `cargo run --example basics`.\n\n### Windows Specific Notes\n\nAs per [the pytorch docs](https://pytorch.org/cppdocs/installing.html) the Windows debug and release builds are not ABI-compatible. This could lead to some segfaults if the incorrect version of libtorch is used.\n\nIt is recommended to use the MSVC Rust toolchain (e.g. by installing `stable-x86_64-pc-windows-msvc` via rustup) rather than a MinGW based one as PyTorch has compatibilities issues with MinGW.\n\n### Static Linking\n\nWhen setting environment variable `LIBTORCH_STATIC=1`, `libtorch` is statically\nlinked rather than using the dynamic libraries. The pre-compiled artifacts don't\nseem to include `libtorch.a` by default so this would have to be compiled\nmanually, e.g. via the following:\n\n```bash\ngit clone -b v2.5.1 --recurse-submodule https://github.com/pytorch/pytorch.git pytorch-static --depth 1\ncd pytorch-static\nUSE_CUDA=OFF BUILD_SHARED_LIBS=OFF python setup.py build\n# export LIBTORCH to point at the build directory in pytorch-static.\n```\n\n## Examples\n\n### Basic Tensor Operations\n\nThis crate provides a tensor type which wraps PyTorch tensors. Here is a minimal\nexample of how to perform some tensor operations.\n\n```rust\nuse tch::Tensor;\n\nfn main() {\n    let t = Tensor::from_slice(&[3, 1, 4, 1, 5]);\n    let t = t * 2;\n    t.print();\n}\n```\n\n### Training a Model via Gradient Descent\n\nPyTorch provides automatic differentiation for most tensor operations\nit supports. This is commonly used to train models using gradient\ndescent. The optimization is performed over variables which are created\nvia a `nn::VarStore` by defining their shapes and initializations.\n\nIn the example below `my_module` uses two variables `x1` and `x2`\nwhich initial values are 0. The forward pass applied to tensor `xs`\nreturns `xs * x1 + exp(xs) * x2`.\n\nOnce the model has been generated, a `nn::Sgd` optimizer is created.\nThen on each step of the training loop:\n\n- The forward pass is applied to a mini-batch of data.\n- A loss is computed as the mean square error between the model output and the mini-batch ground truth.\n- Finally an optimization step is performed: gradients are computed and variables from the `VarStore` are modified accordingly.\n\n\n```rust\nuse tch::nn::{Module, OptimizerConfig};\nuse tch::{kind, nn, Device, Tensor};\n\nfn my_module(p: nn::Path, dim: i64) -> impl nn::Module {\n    let x1 = p.zeros(\"x1\", &[dim]);\n    let x2 = p.zeros(\"x2\", &[dim]);\n    nn::func(move |xs| xs * &x1 + xs.exp() * &x2)\n}\n\nfn gradient_descent() {\n    let vs = nn::VarStore::new(Device::Cpu);\n    let my_module = my_module(vs.root(), 7);\n    let mut opt = nn::Sgd::default().build(&vs, 1e-2).unwrap();\n    for _idx in 1..50 {\n        // Dummy mini-batches made of zeros.\n        let xs = Tensor::zeros(&[7], kind::FLOAT_CPU);\n        let ys = Tensor::zeros(&[7], kind::FLOAT_CPU);\n        let loss = (my_module.forward(&xs) - ys).pow_tensor_scalar(2).sum(kind::Kind::Float);\n        opt.backward_step(&loss);\n    }\n}\n```\n\n### Writing a Simple Neural Network\n\nThe `nn` api can be used to create neural network architectures, e.g. the following code defines\na simple model with one hidden layer and trains it on the MNIST dataset using the Adam optimizer.\n\n```rust\nuse anyhow::Result;\nuse tch::{nn, nn::Module, nn::OptimizerConfig, Device};\n\nconst IMAGE_DIM: i64 = 784;\nconst HIDDEN_NODES: i64 = 128;\nconst LABELS: i64 = 10;\n\nfn net(vs: &nn::Path) -> impl Module {\n    nn::seq()\n        .add(nn::linear(\n            vs / \"layer1\",\n            IMAGE_DIM,\n            HIDDEN_NODES,\n            Default::default(),\n        ))\n        .add_fn(|xs| xs.relu())\n        .add(nn::linear(vs, HIDDEN_NODES, LABELS, Default::default()))\n}\n\npub fn run() -> Result<()> {\n    let m = tch::vision::mnist::load_dir(\"data\")?;\n    let vs = nn::VarStore::new(Device::Cpu);\n    let net = net(&vs.root());\n    let mut opt = nn::Adam::default().build(&vs, 1e-3)?;\n    for epoch in 1..200 {\n        let loss = net\n            .forward(&m.train_images)\n            .cross_entropy_for_logits(&m.train_labels);\n        opt.backward_step(&loss);\n        let test_accuracy = net\n            .forward(&m.test_images)\n            .accuracy_for_logits(&m.test_labels);\n        println!(\n            \"epoch: {:4} train loss: {:8.5} test acc: {:5.2}%\",\n            epoch,\n            f64::from(&loss),\n            100. * f64::from(&test_accuracy),\n        );\n    }\n    Ok(())\n}\n```\n\nMore details on the training loop can be found in the\n[detailed tutorial](https://github.com/LaurentMazare/tch-rs/tree/master/examples/mnist).\n\n### Using some Pre-Trained Model\n\nThe [pretrained-models  example](https://github.com/LaurentMazare/tch-rs/tree/master/examples/pretrained-models/main.rs)\nillustrates how to use some pre-trained computer vision model on an image.\nThe weights - which have been extracted from the PyTorch implementation - can be\ndownloaded here [resnet18.ot](https://github.com/LaurentMazare/tch-rs/releases/download/mw/resnet18.ot)\nand here [resnet34.ot](https://github.com/LaurentMazare/tch-rs/releases/download/mw/resnet34.ot).\n\nThe example can then be run via the following command:\n```bash\ncargo run --example pretrained-models -- resnet18.ot tiger.jpg\n```\nThis should print the top 5 imagenet categories for the image. The code for this example is pretty simple.\n\n```rust\n    // First the image is loaded and resized to 224x224.\n    let image = imagenet::load_image_and_resize(image_file)?;\n\n    // A variable store is created to hold the model parameters.\n    let vs = tch::nn::VarStore::new(tch::Device::Cpu);\n\n    // Then the model is built on this variable store, and the weights are loaded.\n    let resnet18 = tch::vision::resnet::resnet18(vs.root(), imagenet::CLASS_COUNT);\n    vs.load(weight_file)?;\n\n    // Apply the forward pass of the model to get the logits and convert them\n    // to probabilities via a softmax.\n    let output = resnet18\n        .forward_t(&image.unsqueeze(0), /*train=*/ false)\n        .softmax(-1);\n\n    // Finally print the top 5 categories and their associated probabilities.\n    for (probability, class) in imagenet::top(&output, 5).iter() {\n        println!(\"{:50} {:5.2}%\", class, 100.0 * probability)\n    }\n```\n### Importing Pre-Trained Weights from PyTorch Using SafeTensors\n\n`safetensors` is a new simple format by HuggingFace for storing tensors. It does not rely on Python's `pickle` module, and therefore the tensors are not bound to the specific classes and the exact directory structure used when the model is saved. It is also zero-copy, which means that reading the file will require no more memory than the original file.\n\nFor more information on `safetensors`, please check out https://github.com/huggingface/safetensors\n\n#### Installing `safetensors`\n\nYou can install `safetensors` via the pip manager:\n\n```\npip install safetensors\n```\n\n#### Exporting weights in PyTorch\n\n```python\nimport torchvision\nfrom safetensors import torch as stt\n\nmodel = torchvision.models.resnet18(pretrained=True)\nstt.save_file(model.state_dict(), 'resnet18.safetensors')\n```\n\n*Note: the filename of the export must be named with  a `.safetensors` suffix for it to be properly decoded by `tch`.*\n\n#### Importing weights in `tch`\n\n```rust\nuse anyhow::Result;\nuse tch::{\n\tDevice,\n\tKind,\n\tnn::VarStore,\n\tvision::{\n\t\timagenet,\n\t\tresnet::resnet18,\n\t}\n};\n\nfn main() -> Result<()> {\n\t// Create the model and load the pre-trained weights\n\tlet mut vs = VarStore::new(Device::cuda_if_available());\n\tlet model = resnet18(&vs.root(), 1000);\n\tvs.load(\"resnet18.safetensors\")?;\n\t\n\t// Load the image file and resize it to the usual imagenet dimension of 224x224.\n\tlet image = imagenet::load_image_and_resize224(\"dog.jpg\")?\n\t\t.to_device(vs.device());\n\n\t// Apply the forward pass of the model to get the logits\n\tlet output = image\n\t\t.unsqueeze(0)\n\t\t.apply_t(&model, false)\n\t\t.softmax(-1, Kind::Float);\n\t\n\t// Print the top 5 categories for this image.\n    for (probability, class) in imagenet::top(&output, 5).iter() {\n        println!(\"{:50} {:5.2}%\", class, 100.0 * probability)\n    }\n    \n    Ok(())\n}\n```\n\nFurther examples include:\n* A simplified version of\n  [char-rnn](https://github.com/LaurentMazare/tch-rs/blob/master/examples/char-rnn)\n  illustrating character level language modeling using Recurrent Neural Networks.\n* [Neural style transfer](https://github.com/LaurentMazare/tch-rs/blob/master/examples/neural-style-transfer)\n  uses a pre-trained VGG-16 model to compose an image in the style of another image (pre-trained weights:\n  [vgg16.ot](https://github.com/LaurentMazare/tch-rs/releases/download/mw/vgg16.ot)).\n* Some [ResNet examples on CIFAR-10](https://github.com/LaurentMazare/tch-rs/tree/master/examples/cifar).\n* A [tutorial](https://github.com/LaurentMazare/tch-rs/tree/master/examples/jit)\n  showing how to deploy/run some Python trained models using\n  [TorchScript JIT](https://pytorch.org/docs/stable/jit.html).\n* Some [Reinforcement Learning](https://github.com/LaurentMazare/tch-rs/blob/master/examples/reinforcement-learning)\n  examples using the [OpenAI Gym](https://github.com/openai/gym) environment. This includes a policy gradient\n  example as well as an A2C implementation that can run on Atari games.\n* A [Transfer Learning Tutorial](https://github.com/LaurentMazare/tch-rs/blob/master/examples/transfer-learning)\n  shows how to finetune a pre-trained ResNet model on a very small dataset.\n* A [simplified version of GPT](https://github.com/LaurentMazare/tch-rs/blob/master/examples/min-gpt)\n  similar to minGPT.\n* A [Stable Diffusion](https://github.com/LaurentMazare/diffusers-rs)\n  implementation following the lines of hugginface's diffusers library.\n\nExternal material:\n* A [tutorial](http://vegapit.com/article/how-to-use-torch-in-rust-with-tch-rs) showing how to use Torch to compute option prices and greeks.\n* [tchrs-opencv-webcam-inference](https://github.com/metobom/tchrs-opencv-webcam-inference) uses `tch-rs` and `opencv` to run inference\n  on a webcam feed for some Python trained model based on mobilenet v3.\n\n## FAQ\n\n### What are the best practices for Python to Rust model translations?\n\nSee some details in [this thread](https://github.com/LaurentMazare/tch-rs/issues/549#issuecomment-1296840898).\n\n### How to get this to work on a M1/M2 mac?\n\nCheck this [issue](https://github.com/LaurentMazare/tch-rs/issues/488).\n\n### Compilation is slow, torch-sys seems to be rebuilt every time cargo gets run.\nSee this [issue](https://github.com/LaurentMazare/tch-rs/issues/596), this could\nbe caused by rust-analyzer not knowing about the proper environment variables\nlike `LIBTORCH` and `LD_LIBRARY_PATH`.\n\n### Using Rust/tch code from Python.\nIt is possible to call Rust/tch code from Python via PyO3,\n[tch-ext](https://github.com/LaurentMazare/tch-ext) provides an example of such\na Python extension.\n\n### Error loading shared libraries. \n\nIf you get an error about not finding some shared libraries when running the generated binaries\n(e.g. \n` error while loading shared libraries: libtorch_cpu.so: cannot open shared object file: No such file or directory`).\nYou can try adding the following to your `.bashrc` where `/path/to/libtorch` is the path to your\nlibtorch install.\n```\n# For Linux\nexport LD_LIBRARY_PATH=/path/to/libtorch/lib:$LD_LIBRARY_PATH\n# For macOS\nexport DYLD_LIBRARY_PATH=/path/to/libtorch/lib:$DYLD_LIBRARY_PATH\n```\n\n## License\n`tch-rs` is distributed under the terms of both the MIT license\nand the Apache license (version 2.0), at your option.\n\nSee [LICENSE-APACHE](LICENSE-APACHE), [LICENSE-MIT](LICENSE-MIT) for more\ndetails.\n"
        },
        {
          "name": "build.rs",
          "type": "blob",
          "size": 0.5537109375,
          "content": "fn main() {\n    let os = std::env::var(\"CARGO_CFG_TARGET_OS\").expect(\"Unable to get TARGET_OS\");\n    match os.as_str() {\n        \"linux\" | \"windows\" => {\n            if let Some(lib_path) = std::env::var_os(\"DEP_TCH_LIBTORCH_LIB\") {\n                println!(\"cargo:rustc-link-arg=-Wl,-rpath={}\", lib_path.to_string_lossy());\n            }\n            println!(\"cargo:rustc-link-arg=-Wl,--no-as-needed\");\n            println!(\"cargo:rustc-link-arg=-Wl,--copy-dt-needed-entries\");\n            println!(\"cargo:rustc-link-arg=-ltorch\");\n        }\n        _ => {}\n    }\n}\n"
        },
        {
          "name": "clippy.toml",
          "type": "blob",
          "size": 0.033203125,
          "content": "too-many-arguments-threshold = 20\n"
        },
        {
          "name": "dune-project",
          "type": "blob",
          "size": 0.015625,
          "content": "(lang dune 1.6)\n"
        },
        {
          "name": "examples",
          "type": "tree",
          "content": null
        },
        {
          "name": "gen",
          "type": "tree",
          "content": null
        },
        {
          "name": "pyo3-tch",
          "type": "tree",
          "content": null
        },
        {
          "name": "rustfmt.toml",
          "type": "blob",
          "size": 0.044921875,
          "content": "use_small_heuristics = \"Max\"\nedition = \"2021\"\n"
        },
        {
          "name": "src",
          "type": "tree",
          "content": null
        },
        {
          "name": "tests",
          "type": "tree",
          "content": null
        },
        {
          "name": "third_party",
          "type": "tree",
          "content": null
        },
        {
          "name": "torch-sys",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}