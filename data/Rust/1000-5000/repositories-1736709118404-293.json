{
  "metadata": {
    "timestamp": 1736709118404,
    "page": 293,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjMwMA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "TimelyDataflow/differential-dataflow",
      "stars": 2618,
      "defaultBranch": "master",
      "files": [
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.017578125,
          "content": "Cargo.lock\ntarget\n"
        },
        {
          "name": ".travis.yml",
          "type": "blob",
          "size": 0.580078125,
          "content": "language: rust\nsudo: required\nrust:\n- stable\nscript:\n- cargo build\n- cargo test\n- cargo bench\n- cargo doc\nafter_success: |\n  [ $TRAVIS_BRANCH = master ] &&\n  [ $TRAVIS_PULL_REQUEST = false ] &&\n  cargo install mdbook &&\n  (cd mdbook; mdbook build) &&\n  sudo pip install ghp-import &&\n  ghp-import -n mdbook/book &&\n  git push -fq https://${GH_TOKEN}@github.com/${TRAVIS_REPO_SLUG}.git gh-pages\nenv:\n  global:\n    secure: d8tbB97FfXlLwGfzal2h/J2H7GdeR0dHBznqPfb/VxNPHevLJPtkD1Tnsqcs0/9w4piqmbKDzVWPAUXCA5Pg1e3f3pBqmgi12SYvzcFIHI1LLLbjF144S9vNNXhXc0IQE0bnaadX24A1JgRmmJJRt28qZnPxcCNdSJfrqV95h9Q=\n"
        },
        {
          "name": "CHANGELOG.md",
          "type": "blob",
          "size": 2.8876953125,
          "content": "# Changelog\n\nAll notable changes to this project will be documented in this file.\n\nThe format is based on [Keep a Changelog](https://keepachangelog.com/en/1.0.0/),\nand this project adheres to [Semantic Versioning](https://semver.org/spec/v2.0.0.html).\n\n## [Unreleased]\n\n## [0.13.3](https://github.com/TimelyDataflow/differential-dataflow/compare/differential-dataflow-v0.13.2...differential-dataflow-v0.13.3) - 2025-01-09\n\n### Other\n\n- Incorporate breaking changes from Timely's logging update ([#558](https://github.com/TimelyDataflow/differential-dataflow/pull/558))\n- Derive columnar for log events ([#557](https://github.com/TimelyDataflow/differential-dataflow/pull/557))\n- Correct capacity logic\n- Demonstrate container input batching ([#556](https://github.com/TimelyDataflow/differential-dataflow/pull/556))\n- Work towards `Batcher` unification ([#553](https://github.com/TimelyDataflow/differential-dataflow/pull/553))\n\n## [0.13.2](https://github.com/TimelyDataflow/differential-dataflow/compare/differential-dataflow-v0.13.1...differential-dataflow-v0.13.2) - 2024-12-18\n\n### Other\n\n- Update to track timely changes ([#554](https://github.com/TimelyDataflow/differential-dataflow/pull/554))\n- Consolidation consolidation ([#552](https://github.com/TimelyDataflow/differential-dataflow/pull/552))\n- Pass description itself to builder ([#551](https://github.com/TimelyDataflow/differential-dataflow/pull/551))\n- Simplify ContainerChunker::push_into ([#549](https://github.com/TimelyDataflow/differential-dataflow/pull/549))\n- Remove time from MergeBatcher ([#550](https://github.com/TimelyDataflow/differential-dataflow/pull/550))\n- Changes to track timely master ([#542](https://github.com/TimelyDataflow/differential-dataflow/pull/542))\n- Remove (key, val) structure from merge batchers ([#548](https://github.com/TimelyDataflow/differential-dataflow/pull/548))\n- Merge batcher for flat container without key and value ([#547](https://github.com/TimelyDataflow/differential-dataflow/pull/547))\n- Move `Batcher::seal` to `Builder` ([#546](https://github.com/TimelyDataflow/differential-dataflow/pull/546))\n- Extract `Builder` from `Trace` ([#545](https://github.com/TimelyDataflow/differential-dataflow/pull/545))\n- Remove batcher from Trace ([#544](https://github.com/TimelyDataflow/differential-dataflow/pull/544))\n- Build against timely master ([#539](https://github.com/TimelyDataflow/differential-dataflow/pull/539))\n\n## [0.13.1](https://github.com/TimelyDataflow/differential-dataflow/compare/differential-dataflow-v0.13.0...differential-dataflow-v0.13.1) - 2024-11-11\n\n### Other\n\n- Changes to track timely's [#597](https://github.com/TimelyDataflow/differential-dataflow/pull/597) ([#538](https://github.com/TimelyDataflow/differential-dataflow/pull/538))\n\n## [0.13.0](https://github.com/TimelyDataflow/differential-dataflow/compare/differential-dataflow-v0.12.0...differential-dataflow-v0.13.0) - 2024-10-29\n\nChangelog bankruptcy declared.\n"
        },
        {
          "name": "CONTRIBUTING.md",
          "type": "blob",
          "size": 0.638671875,
          "content": "Thank you for your interest in contributing! \n\nHere is some legal stuff that will make you regret clicking on this link.\n\nBy submitting a pull request for this project, you are agreeing to license your contribution under the terms of the project's LICENSE file at the time of your submission (in case it changes or something). You are also certifying that you are in a position to make this agreement, in that you didn't nick your code from someone else, or some project with conflicting licensing requirements.\n\nIf you would like to put explicit copyright notices somewhere, please leave them in the repository's COPYRIGHT file rather than in each file."
        },
        {
          "name": "COPYRIGHT",
          "type": "blob",
          "size": 0.109375,
          "content": "Contributions by Andrea Lattuada <andreal@student.ethz.ch> are Copyright (c) 2016 Andrea Lattuada, ETH Zürich.\n"
        },
        {
          "name": "Cargo.toml",
          "type": "blob",
          "size": 1.39453125,
          "content": "[package]\n\nname = \"differential-dataflow\"\nversion = \"0.13.3\"\nauthors = [\"Frank McSherry <fmcsherry@me.com>\"]\n\ndescription = \"An incremental data-parallel dataflow platform\"\n\n# These URLs point to more information about the repository\ndocumentation = \"https://docs.rs/differential-dataflow\"\nhomepage = \"https://github.com/TimelyDataflow/differential-dataflow\"\nrepository = \"https://github.com/TimelyDataflow/differential-dataflow.git\"\nkeywords = [\"differential\", \"dataflow\"]\nlicense = \"MIT\"\nreadme = \"README.md\"\nedition=\"2021\"\n\n[workspace]\nmembers = [\n    \".\",\n    # \"advent_of_code_2017\",\n    \"dogsdogsdogs\",\n    \"experiments\",\n    \"interactive\",\n    \"server\",\n    \"server/dataflows/degr_dist\",\n    \"server/dataflows/neighborhood\",\n    \"server/dataflows/random_graph\",\n    \"server/dataflows/reachability\",\n    #\"tpchlike\",\n    \"doop\"\n]\n\n[dev-dependencies]\nbincode = \"1.3.1\"\nindexmap = \"2.1\"\nrand=\"0.4\"\nbyteorder=\"1\"\nitertools=\"^0.13\"\nserde_json = \"1.0\"\ngraph_map = \"0.1\"\nbytemuck = \"1.18.0\"\n\n[dependencies]\nserde = { version = \"1.0\", features = [\"derive\"] }\nfnv=\"1.0.2\"\ntimely = {workspace = true}\ncolumnar = \"0.2\"\n\n[workspace.dependencies]\ntimely = { version = \"0.16\", default-features = false }\n#timely = { path = \"../timely-dataflow/timely/\", default-features = false }\n\n[features]\ndefault = [\"timely/getopts\"]\n\n[profile.release]\nopt-level = 3\ndebug = true\nrpath = false\nlto = true\ndebug-assertions = false\ncodegen-units = 4\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 1.056640625,
          "content": "The MIT License (MIT)\n\nCopyright (c) 2015 Frank McSherry\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 21.43359375,
          "content": "# Differential Dataflow\nAn implementation of [differential dataflow](https://github.com/timelydataflow/differential-dataflow/blob/master/differentialdataflow.pdf) over [timely dataflow](https://github.com/timelydataflow/timely-dataflow) on [Rust](http://www.rust-lang.org).\n\n## Background\n\nDifferential dataflow is a data-parallel programming framework designed to efficiently process large volumes of data and to quickly respond to arbitrary changes in input collections. You can read more in the [differential dataflow mdbook](https://timelydataflow.github.io/differential-dataflow/) and in the [differential dataflow documentation](https://docs.rs/differential-dataflow/).\n\nDifferential dataflow programs are written as functional transformations of collections of data, using familiar operators like `map`, `filter`, `join`, and `reduce`. Differential dataflow also includes more exotic operators such as `iterate`, which repeatedly applies a differential dataflow fragment to a collection. The programs are compiled down to [timely dataflow](https://github.com/timelydataflow/timely-dataflow) computations.\n\nFor example, here is a differential dataflow fragment to compute the out-degree distribution of a directed graph (for each degree, the number of nodes with that many outgoing edges):\n\n```rust\nlet out_degr_dist =\nedges.map(|(src, _dst)| src)    // extract source\n     .count()                   // count occurrences of source\n     .map(|(_src, deg)| deg)    // extract degree\n     .count();                  // count occurrences of degree\n```\n\nAlternately, here is a fragment that computes the set of nodes reachable from a set `roots` of starting nodes:\n\n```rust\nlet reachable =\nroots.iterate(|reach|\n    edges.enter(&reach.scope())\n         .semijoin(reach)\n         .map(|(src, dst)| dst)\n         .concat(reach)\n         .distinct()\n)\n```\n\nOnce written, a differential dataflow responds to arbitrary changes to its initially empty input collections, reporting the corresponding changes to each of its output collections. Differential dataflow can react quickly because it only acts where changes in collections occur, and does no work elsewhere.\n\nIn the examples above, we can add to and remove from `edges`, dynamically altering the graph, and get immediate feedback on how the results change: if the degree distribution shifts we'll see the changes, and if nodes are now (or no longer) reachable we'll hear about that too. We could also add to and remove from `roots`, more fundamentally altering the reachability query itself.\n\nBe sure to check out the [differential dataflow documentation](https://docs.rs/differential-dataflow), which is continually improving.\n\n## An example: counting degrees in a graph.\n\nLet's check out that out-degree distribution computation, to get a sense for how differential dataflow actually works. This example is [examples/hello.rs](https://github.com/TimelyDataflow/differential-dataflow/blob/master/examples/hello.rs) in this repository, if you'd like to follow along.\n\nA graph is a collection of pairs `(Node, Node)`, and one standard analysis is to determine the number of times each `Node` occurs in the first position, its \"degree\". The number of nodes with each degree is a helpful graph statistic.\n\nTo determine the out-degree distribution, we create a new timely dataflow scope in which we describe our computation and how we plan to interact with it.\n\n```rust\n// create a degree counting differential dataflow\nlet (mut input, probe) = worker.dataflow(|scope| {\n\n    // create edge input, count a few ways.\n    let (input, edges) = scope.new_collection();\n\n    let out_degr_distr =\n    edges.map(|(src, _dst)| src)    // extract source\n         .count()                   // count occurrences of source\n         .map(|(_src, deg)| deg)    // extract degree\n         .count();                  // count occurrences of degree\n\n    // show us something about the collection, notice when done.\n    let probe =\n    out_degr_distr\n        .inspect(|x| println!(\"observed: {:?}\", x))\n        .probe();\n\n    (input, probe)\n});\n```\n\nThe `input` and `probe` we return are how we get data into the dataflow, and how we notice when some amount of computation is complete. These are timely dataflow idioms, and we won't get in to them in more detail here (check out [the timely dataflow repository](https://github.com/timelydataflow/timely-dataflow)).\n\nIf we feed this computation with some random graph data, say fifty random edges among ten nodes, we get output like\n\n    Echidnatron% cargo run --release --example hello -- 10 50 1 inspect\n        Finished release [optimized + debuginfo] target(s) in 0.05s\n        Running `target/release/examples/hello 10 50 1 inspect`\n    observed: ((3, 1), 0, 1)\n    observed: ((4, 2), 0, 1)\n    observed: ((5, 4), 0, 1)\n    observed: ((6, 2), 0, 1)\n    observed: ((7, 1), 0, 1)\n    round 0 finished after 772.464µs (loading)\n\nThis shows us the records that passed the `inspect` operator, revealing the contents of the collection: there are five distinct degrees, three through seven. The records have the form `((degree, count), time, delta)` where the `time` field says this is the first round of data, and the `delta` field tells us that each record is coming into existence. If the corresponding record were departing the collection, it would be a negative number.\n\nLet's update the input by removing one edge and adding a new random edge:\n\n    observed: ((2, 1), 1, 1)\n    observed: ((3, 1), 1, -1)\n    observed: ((7, 1), 1, -1)\n    observed: ((8, 1), 1, 1)\n    round 1 finished after 149.701µs\n\nWe see here some changes! Those degree three and seven nodes have been replaced by degree two and eight nodes; looks like one node lost an edge and gave it to the other!\n\nHow about a few more changes?\n\n    round 2 finished after 127.444µs\n    round 3 finished after 100.628µs\n    round 4 finished after 130.609µs\n    observed: ((5, 3), 5, 1)\n    observed: ((5, 4), 5, -1)\n    observed: ((6, 2), 5, -1)\n    observed: ((6, 3), 5, 1)\n    observed: ((7, 1), 5, 1)\n    observed: ((8, 1), 5, -1)\n    round 5 finished after 161.82µs\n\nWell a few weird things happen here. First, rounds 2, 3, and 4 don't print anything. Seriously? It turns out that the random changes we made didn't affect any of the degree counts, we moved edges between nodes, preserving degrees. It can happen.\n\nThe second weird thing is that in round 5, with only two edge changes we have six changes in the output! It turns out we can have up to eight. The degree eight gets turned back into a seven, and a five gets turned into a six. But: going from five to six *changes* the count for each, and each change requires two record differences. Eight and seven were more concise because their counts were only one, meaning just arrival and departure of records rather than changes.\n\n### Scaling up\n\nThe appealing thing about differential dataflow is that it only does work where changes occur, so even if there is a lot of data, if not much changes it can still go quite fast. Let's scale our 10 nodes and 50 edges up by a factor of one million:\n\n    Echidnatron% cargo run --release --example hello -- 10000000 50000000 1 inspect\n        Finished release [optimized + debuginfo] target(s) in 0.04s\n        Running `target/release/examples/hello 10000000 50000000 1 inspect`\n    observed: ((1, 336908), 0, 1)\n    observed: ((2, 843854), 0, 1)\n    observed: ((3, 1404462), 0, 1)\n    observed: ((4, 1751921), 0, 1)\n    observed: ((5, 1757099), 0, 1)\n    observed: ((6, 1459805), 0, 1)\n    observed: ((7, 1042894), 0, 1)\n    observed: ((8, 653178), 0, 1)\n    observed: ((9, 363983), 0, 1)\n    observed: ((10, 181423), 0, 1)\n    observed: ((11, 82478), 0, 1)\n    observed: ((12, 34407), 0, 1)\n    observed: ((13, 13216), 0, 1)\n    observed: ((14, 4842), 0, 1)\n    observed: ((15, 1561), 0, 1)\n    observed: ((16, 483), 0, 1)\n    observed: ((17, 143), 0, 1)\n    observed: ((18, 38), 0, 1)\n    observed: ((19, 8), 0, 1)\n    observed: ((20, 3), 0, 1)\n    observed: ((22, 1), 0, 1)\n    round 0 finished after 15.470465014s (loading)\n\nThere are a lot more distinct degrees here. I sorted them because it was too painful to look at the unsorted data. You would normally get to see the output unsorted, because they are just changes to values in a collection.\n\nLet's perform a single change again.\n\n    observed: ((5, 1757098), 1, 1)\n    observed: ((5, 1757099), 1, -1)\n    observed: ((6, 1459805), 1, -1)\n    observed: ((6, 1459807), 1, 1)\n    observed: ((7, 1042893), 1, 1)\n    observed: ((7, 1042894), 1, -1)\n    round 1 finished after 228.451µs\n\nAlthough the initial computation took about fifteen seconds, we get our changes in about 230 microseconds; that's about one hundred thousand times faster than re-running the computation. That's pretty nice. Actually, it is small enough that the time to print things to the screen is a bit expensive, so let's stop doing that.\n\nNow we can just watch as changes roll past and look at the times.\n\n    Echidnatron% cargo run --release --example hello -- 10000000 50000000 1 no_inspect\n        Finished release [optimized + debuginfo] target(s) in 0.04s\n        Running `target/release/examples/hello 10000000 50000000 1 no_inspect`\n    round 0 finished after 15.586969662s (loading)\n    round 1 finished after 1.070239ms\n    round 2 finished after 2.303187ms\n    round 3 finished after 208.45µs\n    round 4 finished after 163.224µs\n    round 5 finished after 118.792µs\n    ...\n\nNice. This is some hundreds of microseconds per update, which means maybe ten thousand updates per second. It's not a horrible number for my laptop, but it isn't the right answer yet.\n\n### Scaling .. \"along\"?\n\nDifferential dataflow is designed for throughput in addition to latency. We can increase the number of rounds of updates it works on concurrently, which can increase its effective throughput. This does not change the output of the computation, except that we see larger batches of output changes at once.\n\nNotice that those times above are a few hundred microseconds for each single update. If we work on ten rounds of updates at once, we get times that look like this:\n\n    Echidnatron% cargo run --release --example hello -- 10000000 50000000 10 no_inspect\n        Finished release [optimized + debuginfo] target(s) in 0.04s\n        Running `target/release/examples/hello 10000000 50000000 10 no_inspect`\n    round 0 finished after 15.556475008s (loading)\n    round 10 finished after 421.219µs\n    round 20 finished after 1.56369ms\n    round 30 finished after 338.54µs\n    round 40 finished after 351.843µs\n    round 50 finished after 339.608µs\n    ...\n\nThis is appealing in that rounds of ten aren't much more expensive than single updates, and we finish the first ten rounds in much less time than it takes to perform the first ten updates one at a time. Every round after that is just bonus time.\n\nAs we turn up the batching, performance improves. Here we work on one hundred rounds of updates at once:\n\n    Echidnatron% cargo run --release --example hello -- 10000000 50000000 100 no_inspect\n        Finished release [optimized + debuginfo] target(s) in 0.04s\n        Running `target/release/examples/hello 10000000 50000000 100 no_inspect`\n    round 0 finished after 15.528724145s (loading)\n    round 100 finished after 2.567577ms\n    round 200 finished after 1.861168ms\n    round 300 finished after 1.753794ms\n    round 400 finished after 1.528285ms\n    round 500 finished after 1.416605ms\n    ...\n\nWe are still improving, and continue to do so as we increase the batch sizes. When processing 100,000 updates at a time we take about half a second for each batch. This is less \"interactive\" but a higher throughput.\n\n    Echidnatron% cargo run --release --example hello -- 10000000 50000000 100000 no_inspect\n        Finished release [optimized + debuginfo] target(s) in 0.04s\n        Running `target/release/examples/hello 10000000 50000000 100000 no_inspect`\n    round 0 finished after 15.65053789s (loading)\n    round 100000 finished after 505.210924ms\n    round 200000 finished after 524.069497ms\n    round 300000 finished after 470.77752ms\n    round 400000 finished after 621.325393ms\n    round 500000 finished after 472.791742ms\n    ...\n\nThis averages to about five microseconds on average; a fair bit faster than the hundred microseconds for individual updates! And now that I think about it each update was actually two changes, wasn't it. Good for you, differential dataflow!\n\n### Scaling out\n\nDifferential dataflow is built on top of [timely dataflow](https://github.com/timelydataflow/timely-dataflow), a distributed data-parallel runtime. Timely dataflow scales out to multiple independent workers, increasing the capacity of the system (at the cost of some coordination that cuts into latency).\n\nIf we bring two workers to bear, our 10 million node, 50 million edge computation drops down from fifteen seconds to just over eight seconds.\n\n    Echidnatron% cargo run --release --example hello -- 10000000 50000000 1 no_inspect -w2\n        Finished release [optimized + debuginfo] target(s) in 0.04s\n        Running `target/release/examples/hello 10000000 50000000 1 no_inspect -w2`\n    round 0 finished after 8.065386177s (loading)\n    round 1 finished after 275.373µs\n    round 2 finished after 759.632µs\n    round 3 finished after 171.671µs\n    round 4 finished after 745.078µs\n    round 5 finished after 213.146µs\n    ...\n\nThat is a so-so reduction. You might notice that the times *increased* for the subsequent rounds. It turns out that multiple workers just get in each other's way when there isn't much work to do.\n\nFortunately, as we work on more and more rounds of updates at the same time, the benefit of multiple workers increases. Here are the numbers for ten rounds at a time:\n\n    Echidnatron% cargo run --release --example hello -- 10000000 50000000 10 no_inspect -w2\n        Finished release [optimized + debuginfo] target(s) in 0.04s\n        Running `target/release/examples/hello 10000000 50000000 10 no_inspect -w2`\n    round 0 finished after 8.083000954s (loading)\n    round 10 finished after 1.901946ms\n    round 20 finished after 3.092976ms\n    round 30 finished after 889.63µs\n    round 40 finished after 409.001µs\n    round 50 finished after 320.248µs\n    ...\n\nOne hundred rounds at a time:\n\n    Echidnatron% cargo run --release --example hello -- 10000000 50000000 100 no_inspect -w2\n        Finished release [optimized + debuginfo] target(s) in 0.04s\n        Running `target/release/examples/hello 10000000 50000000 100 no_inspect -w2`\n    round 0 finished after 8.121800831s (loading)\n    round 100 finished after 2.52821ms\n    round 200 finished after 3.119036ms\n    round 300 finished after 1.63147ms\n    round 400 finished after 1.008668ms\n    round 500 finished after 941.426µs\n    ...\n\nOne hundred thousand rounds at a time:\n\n    Echidnatron% cargo run --release --example hello -- 10000000 50000000 100000 no_inspect -w2\n        Finished release [optimized + debuginfo] target(s) in 0.04s\n        Running `target/release/examples/hello 10000000 50000000 100000 no_inspect -w2`\n    round 0 finished after 8.200755198s (loading)\n    round 100000 finished after 275.262419ms\n    round 200000 finished after 279.291957ms\n    round 300000 finished after 259.137138ms\n    round 400000 finished after 340.624124ms\n    round 500000 finished after 259.870938ms\n    ...\n\nThese last numbers were about half a second with one worker, and are decently improved with the second worker.\n\n### Going even faster\n\nThere are several performance optimizations in differential dataflow designed to make the underlying operators as close to what you would expect to write, when possible. Additionally, by building on timely dataflow, you can drop in your own implementations a la carte where you know best.\n\nFor example, we also know in this case that the underlying collections go through a *sequence* of changes, meaning their timestamps are totally ordered. In this case we can use a much simpler implementation, `count_total`. The reduces the update times substantially, for each batch size:\n\n    Echidnatron% cargo run --release --example hello -- 10000000 50000000 10 no_inspect -w2\n        Finished release [optimized + debuginfo] target(s) in 0.04s\n        Running `target/release/examples/hello 10000000 50000000 10 no_inspect -w2`\n    round 0 finished after 5.985084002s (loading)\n    round 10 finished after 1.802729ms\n    round 20 finished after 2.202838ms\n    round 30 finished after 192.902µs\n    round 40 finished after 198.342µs\n    round 50 finished after 187.725µs\n    ...\n\n    Echidnatron% cargo run --release --example hello -- 10000000 50000000 100 no_inspect -w2\n        Finished release [optimized + debuginfo] target(s) in 0.04s\n        Running `target/release/examples/hello 10000000 50000000 100 no_inspect -w2`\n    round 0 finished after 5.588270073s (loading)\n    round 100 finished after 3.114716ms\n    round 200 finished after 2.657691ms\n    round 300 finished after 890.972µs\n    round 400 finished after 448.537µs\n    round 500 finished after 384.565µs\n    ...\n\n    Echidnatron% cargo run --release --example hello -- 10000000 50000000 100000 no_inspect -w2\n        Finished release [optimized + debuginfo] target(s) in 0.04s\n        Running `target/release/examples/hello 10000000 50000000 100000 no_inspect -w2`\n    round 0 finished after 6.486550581s (loading)\n    round 100000 finished after 89.096615ms\n    round 200000 finished after 79.469464ms\n    round 300000 finished after 72.568018ms\n    round 400000 finished after 93.456272ms\n    round 500000 finished after 73.954886ms\n    ...\n\nThese times have now dropped quite a bit from where we started; we now absorb over one million rounds of updates per second, and produce correct (not just consistent) answers even while distributed across multiple workers.\n\n## A second example: k-core computation\n\nThe k-core of a graph is the largest subset of its edges so that all vertices with any incident edges have degree at least k. One way to find the k-core is to repeatedly delete all edges incident on vertices with degree less than k. Those edges going away might lower the degrees of other vertices, so we need to *iteratively* throwing away edges on vertices with degree less than k until we stop. Maybe we throw away all the edges, maybe we stop with some left over.\n\nHere is a direct implementation, in which we repeatedly take determine the set of active nodes (those with at least\n`k` edges point to or from them), and restrict the set `edges` to those with both `src` and `dst` present in `active`.\n\n```rust\nlet k = 5;\n\n// iteratively thin edges.\nedges.iterate(|inner| {\n\n    // determine the active vertices        /-- this is a lie --\\\n    let active = inner.flat_map(|(src,dst)| [src,dst].into_iter())\n                      .map(|node| (node, ()))\n                      .group(|_node, s, t| if s[0].1 > k { t.push(((), 1)); })\n                      .map(|(node,_)| node);\n\n    // keep edges between active vertices\n    edges.enter(&inner.scope())\n         .semijoin(active)\n         .map(|(src,dst)| (dst,src))\n         .semijoin(active)\n         .map(|(dst,src)| (src,dst))\n});\n```\n\nTo be totally clear, the syntax with `into_iter()` doesn't work, because Rust, and instead there is a more horrible syntax needed to get a non-heap allocated iterator over two elements. But, it works, and\n\n    Running `target/release/examples/degrees 10000000 50000000 1 5 kcore1`\n    Loading finished after 72204416910\n\nWell that is a thing. Who knows if 72 seconds is any good? (*ed:* it is worse than the numbers in the previous version of this readme).\n\nThe amazing thing, though is what happens next:\n\n    worker 0, round 1 finished after Duration { secs: 0, nanos: 567171 }\n    worker 0, round 2 finished after Duration { secs: 0, nanos: 449687 }\n    worker 0, round 3 finished after Duration { secs: 0, nanos: 467143 }\n    worker 0, round 4 finished after Duration { secs: 0, nanos: 480019 }\n    worker 0, round 5 finished after Duration { secs: 0, nanos: 404831 }\n\nWe are taking about half a millisecond to *update* the k-core computation. Each edge addition and deletion could cause other edges to drop out of or more confusingly *return* to the k-core, and differential dataflow is correctly updating all of that for you. And it is doing it in sub-millisecond timescales.\n\nIf we crank the batching up by one thousand, we improve the throughput a fair bit:\n\n    Running `target/release/examples/degrees 10000000 50000000 1000 5 kcore1`\n    Loading finished after Duration { secs: 73, nanos: 507094824 }\n    worker 0, round 1000 finished after Duration { secs: 0, nanos: 55649900 }\n    worker 0, round 2000 finished after Duration { secs: 0, nanos: 51793416 }\n    worker 0, round 3000 finished after Duration { secs: 0, nanos: 57733231 }\n    worker 0, round 4000 finished after Duration { secs: 0, nanos: 50438934 }\n    worker 0, round 5000 finished after Duration { secs: 0, nanos: 55020469 }\n\nEach batch is doing one thousand rounds of updates in just over 50 milliseconds, averaging out to about 50 microseconds for each update, and corresponding to roughly 20,000 distinct updates per second.\n\nI think this is all great, both that it works at all and that it even seems to work pretty well.\n\n## Roadmap\n\nThe [issue tracker](https://github.com/timelydataflow/differential-dataflow/issues) has several open issues relating to current performance defects or missing features. If you are interested in contributing, that would be great! If you have other questions, don't hesitate to get in touch.\n\n## Acknowledgements\n\nIn addition to contributions to this repository, differential dataflow is based on work at the now defunct Microsoft Research lab in Silicon Valley, and continued at the Systems Group of ETH Zürich. Numerous collaborators at each institution (among others) have contributed both ideas and implementations.\n"
        },
        {
          "name": "TODO.md",
          "type": "blob",
          "size": 1.1259765625,
          "content": "## Differential dataflow to-do list:\n\n1. Batch builders need an \"ordered\" option where the keys and vals are already sorted (cf group.output).\n2. Several trace implementations need to be fleshed out (e.g. `time` and `constant`).\n\t- Consider ConstantCollection type which can only be construct from known data, arranged to `constant`.\n3. Several trace implementations could benefit from a RHH `keys` field; prototype and test!\n\t- Probably wants a Uniform<T: Unsigned> struct for \"node identifiers\"; needs tweaks to `Data` trait.\n4. Lots of sorting, but no radix-sorting. Historically a big improvement.\n\t- Connects to U: Unsigned output of `hashed`; no point radix-sorting u32 keys as if u64s.\n5. Several operators need revision: distinct, threshold, cogroup.\n6. The `keys` trace implementation has had zero testing. Important!\n7. Progressive merging under-explored; trade-offs in rate of work? (yes, but worth?)\n\n8. High-resolution times aren't too far away. \n\t- Think up alternate Collection type with new data bits.\n\t- Uncomment `group` implementation and get to work.\n\n9. Join now has \"deferred work\"; check it out to see if it helps on large graphs."
        },
        {
          "name": "advent_of_code_2017",
          "type": "tree",
          "content": null
        },
        {
          "name": "benches",
          "type": "tree",
          "content": null
        },
        {
          "name": "differentialdataflow.pdf",
          "type": "blob",
          "size": 1773.3955078125,
          "content": null
        },
        {
          "name": "dogsdogsdogs",
          "type": "tree",
          "content": null
        },
        {
          "name": "doop",
          "type": "tree",
          "content": null
        },
        {
          "name": "examples",
          "type": "tree",
          "content": null
        },
        {
          "name": "experiments",
          "type": "tree",
          "content": null
        },
        {
          "name": "interactive",
          "type": "tree",
          "content": null
        },
        {
          "name": "mdbook",
          "type": "tree",
          "content": null
        },
        {
          "name": "osdi2018-reviews.md",
          "type": "blob",
          "size": 38.2861328125,
          "content": "Reviews, author response, and PC comments as available in the system. Review #86F is post-response, and none of the other reviews changed from their initial state.\n\n|             | Overall Merit | Novelty | Experimental Methodology | Writing Quality | Reviewer Expertise |\n|:------------|-------:|----:|-------:|-------:|-------:|\n| Review #86A |      2 |   2 |     3  |      2 |      2 |\n| Review #86B |      3 |   2 |     3  |      2 |      2 |\n| Review #86C |      2 |   2 |     3  |      2 |      2 |\n| Review #86D |      3 |   3 |     4  |      4 |      2 |\n| Review #86E |      3 |   2 |     4  |      2 |      2 |\n| Review #86F |      3 |   3 |     3  |      4 |      2 |\n\n## Review #86A\n\n### Paper summary\nThis paper describes a new differential dataflow system which supports shared state among operators. It proposes a novel “arrange” operator to maintain the shared indexed state to achieve high throughput and low latency. The system is motivated by the demand to support different types of big data workloads in a unified system. The evaluation shows that the proposed system can achieve comparable performance for workloads on the corresponding alternative specialized systems.\n\n### Strengths\nMoved the state-of-the-art of the differential dataflow with shared state support.\n\nThe system has many types of workloads built and evaluated.\n\n### Areas for improvement\nThe writing can be improved by describing the key techniques and challenges with running example.\n\n### Comments for author\nThis work looks like an extension of Naiad, from the design wise, to achieve higher generality and performance. The benefit of the system hinges on the design of “arrange” operator. However, this paper does not do a good job on elaborating why the design of “arrange” operator is challenging and why it cannot be easily achieved through a simple revision of Naiad. The answer might be straightforward if the reader is an expert of Naiad, but this assumption seems to be too strong. It could be very helpful to describe the “arrange” operator design and challenges with a running example. This weakness of the paper makes me feel the innovation is incremental.\n\nI think this work shares the similar motivation as Naiad. It is kind of disappointing that the evaluation does not has comparison with Naiad system. It also does not have experiments that clearly help understand how much benefit the optimizations in “arrange” operator design can bring.\n\nThis work seems to mainly advocate the high-level design principles of “arrange” operator. Wouldn’t be the design of data structure for the shared indexed state important and critical for the performance improvement? If so, it will be helpful to elaborate it.\n\nAlthough I buy the point that a unified system for different types of workloads is useful, I think it is only critical in the scenarios where there are needs for those workloads to cooperatively run. In another word, if two applications are commonly used in separate scenarios, it is more reasonable to use different specialized systems to support them, respectively. I would appreciate more if the paper can illustrate an example scenario where multiple types of workloads that the system targets to support need to run in a cooperative way.\n\n\n## Review #86B\n\n### Paper summary\nThe paper presents a design of adding indexes to timely differential dataflow systems.\n\n### Strengths\nThe work adds indexes to timely differential dataflow systems, which are very interesting streaming computing engines. If indexes can improve performance and simplify the writing of timely dataflow programs, that's a great win.\n\n### Areas for improvement\nExplain in plainer terms the specific problems of existing dataflow designs that this paper deals with. The current description of these problems is overreaching and confusing.\nClarify what properties you get from your design vs. you inherit from Naiad. For example, is \"generality\" something you largely inherit from Naiad?\nWhy is there no performance comparison with Naiad w/o indexes on the real workloads?\n\n### Comments for author\nI was excited to read this paper, because the differential data flow model is very interesting and worth exploring more. Unfortunately, this paper was very confusing to me. I couldn't grasp what new properties the indexes added, beyond potential performance improvements, which is indeed what indexes typically provide. You make many arguments about several other properties they supposedly add, but these arguments seem overreaching. For example, the \"generality\" property: is that something the indexes buy you, or is this a property your system inherits from the differential dataflow model? I understand that the indexes add a bit more uniformity between how one interfaces with historical data vs. streaming data, but beyond that, I don't see why the indexes add to the \"generality\" property of differential data flow systems. As another example, you argue that because the indexes improve performance, indexed differential dataflow systems obviate the need for specialized systems. This seems like an overreaching argument. Maybe the conclusion that your system is as fast as some specialized system designs is worth pointing out in your evaluation, but motivating your index with this argument -- that no one will need to specialize system designs now -- seems overreaching.\n\nIn my opinion, the question of how one integrates indexes into differential dataflow is interesting, even if the only motivation for it is improved performance. You could then reflect upon other benefits the indexes bring, perhaps toward the end of the paper.\n\nA few more comments I jotted down while reading the paper (some are redundant to the points I'm making above to summarize my overall concern with the paper):\n\nAbstract\n\n* \"general, scalable, responsive\" are these properties inherited from Naiad or newly added by indexes? Also, what does \"responsive\" mean here -- is it high availability, low latency, something else? Please use more standard or define what you mean.\nWould be good to give intuition of your technical contribution here: you add shared index support.\n\nIntro\n\n* You are motivating your contribution with many problems -- lots of data compute models, none of them \"general\" enough, often systems are specialized for good performance, etc. Yet, the contribution you bring -- adding indexes to dataflow graphs -- does not seem to resolve all of these problems, at least not completely. I understand that the design offers benefits along many of these dimensions, but I wouldn't consider it as the ultimate solution to this problem. Describing a more focused problem -- e.g., differential dataflow systems present substantial promise but they are not as efficient as they could be because they lack indexing capabilities -- and then irrefutably addressing that one challenge, seems more appropriate. You can then extend the discussion to the additional benefits the indexes buy you, e.g., with them, differential dataflows can support workloads.\n* \"nonetheless re-use common principles.\" Is this ever supported in the body of the paper? What are these commonly reused principles? Is that what you are leveraging for the design? This statement makes me think that there will be an analysis of the \"commonly reused principles\" in narrow-functionality, performant systems, and those commonly reused principles would then be leveraged as part of your general design. But I didn't find that point addressed anywhere in the paper.\n* \"K-Pg exposes and shares indexed state between operators\" - indexes of what and for what purpose? Why is it expected that indexes would help/increase performance in the case of streaming systems?\n* \"K-Pg's adherence to a specific data and computational model brings structure and meaning to its shared state...\" -- yes, but why doesn't that constrain applicability?\n* Paragraph starting with \"Several changes are required...\" is very abstract. I didn't get much out of it. Explaining your functionality more concretely would be very useful.\n* \"We evaluate K-Pg on benchmark tasks in relational analytics, graph processing, and datalog evaluation\" -- what about ML? I guess such workloads aren't supported, would be good to explicitly carve out.\n* \"K-Pg is our attempt to provide a computational framework that is general, scalable, and responsive.\" As in the abstract, this claim struck me as overreaching for what you are actually contributing over Naiad. Some of these properties are inherited from Naiad/differential dataflow, and perhaps boosted further by your design. But the specific contribution appears to be improving performance of differential dataflow systems with indexes.\n\nSection 2\n\n* \"preclude cycles along with timestamps do not strictly increase\" -- seems like a bit limitation for generality. It means essentially, no looping over the data (as is often needed for ML workloads for example). Is that what you are addressing with indexing from a generality perspective?\n* Section 2.3: It's good to have this diffing section, but it was still too abstract for me. Maybe you could continue the example in Figure 1 and show how it would be executed with indexes vs. without?\n\nSections 3 and 4\n\n* These two sections cover a lot of material and techniques fairly abstractly. It would be good to have one or a few running examples to illustrate how the indexes work in various contexts.\n* You argue that the implementation for some of the operators can be simplified with indexes. It was hard to keep track of where you simplified things vs. where they complicated them. Might be good to include side-by-side algorithms for some of the operators' implementations.\n\nSection 5\n\n* Why are there no evaluation results comparing Naiad (or other differential dataflow implementation without indexes) on realistic workloads? Are some of your configurations equivalent to differential dataflow w/o indexes? From what I understood, the value of indexes is only illustrated on microbenchmarks.\n* Figure 4: what's DBpaster (as used in label) vs. DBToaster (as used in caption)?\n* I appreciate the thoroughness of evaluating on multiple different application domains! It does give support to your \"generality\" claim. I would have liked to understand whether it is the indexes or the differential dataflow model, or a combination of the two that provide general support. My guess is that it's the latter, but I would have wanted to see that in the graph: no indexes in differential dataflow is worse than specialized systems, which are worse than differential dataflow with indexes.\n\nRelated work:\n\n* I liked the way you compared particularly with Stream processors. Very concrete and clear.\n\n\n## Review #86C\n\n### Paper summary\nK-Pg is a re-implementation of differential dataflow system. K-Pg introduces a new dataflow operator arrange to maintain shared index within a worker. Operating on pre-indexed input batches is more efficient. Evaluations on relational analytics, graph processing, and program analysis tasks show that the new dataflow engine achieves lower latency and higher throughput.\n### Strengths\nThe re-implementation of the differential dataflow system has achieved good performance gains.\n### Areas for improvement\nThis paper is not really self-contained and thus very difficult to understand. One must already know and believe in the design of differential dataflow in order to have any appreciation of the current re-implementation.\n### Comments for author\nThis paper is very hard to understand. There is no clear motivation and problem definition. The solution is also hard to decipher and appreciate. As a result, the paper might as well contain some good idea, but as a reviewer, I completely failed to grasp it.\n\nI do not really agree with the high-level motivation outlined in the paper's abstract and introduction. It is said that existing frameworks are limited in generality. This claim is too general (there is no mentioning of specific systems and there is no discussion of any concrete example that the specific existing systems fail to address). Later, Figure 1 shows a graph reachability example for the K-Pg system. However, this example would work perfectly well in Spark. I read the \"Different dataflow\" paper. There, the motivation is on performance rather than generality. I think the performance motivation is much more convincing.\n\nThe paper's discussion on differential dataflow is not sufficient for readers to understand the rest of the paper. I'm not sure what is the best way to summarize differential dataflow without too much text. Perhaps one could illustrate the workings of differential dataflow with Figure 1 and explain what are the \"difference streams\" being passed between the operators. How does each operator process these difference? etc. Crucially, it's important to explain what is being indexed of the data, why it is so important to maintain the index, and why sharing these indexes across operators within a worker is beneficial. In the paper's current form, I don't know such important questions are answered. As a result, it is very difficult to read Sec 3. I have to read the differential dataflow paper to get some idea what these indexes are, assuming the indexes in that paper mean the same thing as this paper.\n\nI would also appreciate the authors consolidate all the built-in operators and their semantics in a single Table and explicitly list it somewhere. Currently, the operators are sprinkled througout the paper. It's sometimes hard to tell whether a term in sans-serif font is a built-in operator or user-defined one.\n\nSince arrange is a crucial new operator supported by K-Pg, it would be good to see an example application using it. For example, it would be nice if Figure 1 is shown to use this arrange operator. Or, is this operator automatically added to the dataflow graph and hidden from the end user?\n\nI do not understand what the phrase \"downgrade the frontier capability\" means (Sec 3). Similarly, what is an \"opposing trace\" or an \"exchange edge\" (Sec 4.3.1)?\n\nIn Sec 5, last paragraph, \"K-Pg outperforms ... while supporting more general computation\". Which of these tasks being evaluated cannot be supported by Spark?\n\nFor the evaluation's experiment setup, do you run a worker on each CPU core for a maximum of 40 total workers in the single machine setup?\n\nMicrobenchmark. How often does the frontier advance? what is the effective (physical) batch size created in these microbenchmarks? In general, I am not sure what to make of these performance number, since I do not have a good idea what creating an index entails for the arrangement operator.\n\nWhat are query modifications like in interatctive queries in Sec 5.3?\n\nThe description of experimental results in Sec 5.4. feels very casual and not quite helpful. For example, it's said for 4 out of the 6 reference problems, K-Pg... was comparable to DeALS on one core but scaled less well with increasing cores. What happened to the rest of the 2 problems? Later, it is said that DeALS permits racy code? What kind of racy code? Does it compromise application correctness?\n\n## Review #86D\n\n### Paper summary\nK-PG is a distributed data processor that aims to support a generic set of workloads including graph processing, analytics, and Datalog. K-PG adopts the abstraction of differential data flows, and enables shared state that supports a high rate of logical updates. The evaluation shows that K-PG performs comparably to specialized data processors.\n### Strengths\nA generic data processing system enables programmers to write hybrid applications with varying workloads.\n\nA well-thought system design, synthesizing existing elements in an interesting new way.\n\nEvaluation results show that K-PG performs comparably to specialized data processors.\n\nThe paper is clear and well-written.\n\n### Areas for improvement\nThe building blocks have been previously designed and are not novel by themselves — yet the overall system design is new and the choice of the building blocks is interesting.\n\nI felt that the paper was missing a characterization of the limitations of K-PG.\n\n### Comments for author\nI have enjoyed reading this paper. While I believe in the power of specialization to achieve good performance, a system like K-PG that can provide comparable performance while enabling a set of workloads is appealing. Developers often write applications that need to exercise more than one type of computation, and such a system makes it easier for them.\n\nI have enjoyed the design of K-PG based on differential dataflows: the choice and rationale for the building blocks and their composition in the overall system is elegant. Even though each building block exists, as the authors acknowledge, the overall design is interesting.\n\nHowever, I felt that the paper portrays too much of a perfect view on K-PG with almost no limitations exposed, which makes it harder to believe. What are some limitations or shortcomings of K-PG? I have a hard time believing that K-PG will always perform comparably to the specialized solutions. In what situations will it or will it not perform comparably?\n\nThe abstract sounded too vague and generic, to the point that I was skeptical of the work at the end of the abstract.\n\nThe paper is mostly well written, with only a few issues such as:\n\n“effecient” -> “efficient”\ngrammar “eliminate the cost all subsequent uses of the arrangement.”\n\n## Review #86E\n\n### Paper summary\nThis paper re-implementation of differential dataflow based on the concepts of indexed shared state. Such state can be shared between operators in multiple dataflows. Shared states can reduce the memory consumption and communication. The modified operators can have higher throughput for computation. K-Pg is a general computing framework supporting a wide range of workloads. Evaluation shows that K-Pg can be as capable as specialized data processing systems in target domains.\n### Strengths\nK-Pg is a data-processing framework that is general, scalable and responsive. Also, the performance is comparable with and occasionally better than specialized existing systems on a variety of tasks.\n### Areas for improvement\nThe paper is very abstract for the readers who are not familiar with differential dataflow. It would be better to use examples to show the design and implementation of operators.\n\nIs there any comparison between the K-Pg and previous implementation of differential dataflow? Since K-Pg is complete re-implementation of differential dataflow, it would be better to show the performance improvement over existing implementation.\n\n### Comments for author\nThis paper is well motivated and organized. The system design and implementation are reasonable and quite different from the existing general framework of data processing systems. Despite the comments mentioned previously, here are a few more comments for the paper.\n\nThe paper talks about the design and implementation of K-Pg. However, it does not explicitly discuss the exact improvement that can brought by K-Pg. Despite the reduced memory consumption(evaluated) and network communication (not evaluated), it is not clear how K-Pg can reduce the computation. One is that the other workflow does not need to re-arrange again i.e. reduce the computation. The other is by using batch, the operator programming interfaces are changed. And the computation will be performed in a batched way. Are these the computation improvements? What other kinds improvements can be gotten? Some discussions can help.\n\nNot quite sure about the evaluation setup. It seems that a single machine is used. It is okay sometimes. However, it will be unclear about the effects of the current method working in the distributed environment. Will the efforts be erased by introducing network overhead? Some discussions can help.\n\nThere should be more explanations and results related to the evaluation in section 5.3. It would be better to provide some information about the implementation of the applications. No detailed performance results are provided. K-Pg's performance is supposed to be comparable to specialized systems, but current comparison is not enough. For example, K-Pg is faster than existing systems including BigDatalog, Myria, SociaLite and GraphX on graph workloads. But there are many state-of-art works which can be much faster, such as Gemini(OSDI 16).\n\nStill not clear about some details of indexed shared states. Section 3 can use more space to explain the organization of shared states. Since it is indexed and has multiple versions, what about the organization of data structures? Are the data indexed by time or data in <data, time, diff> triplets?\n\nLast paragraph Section 5.5 refers Figure 1, but it should be Table 1.\n\n## Review #86F\n\n### Paper summary\nThere are a variety of scalable data processing systems: databases, timely dataflow, graph processing. The paper introduces a core concept called an \"arrangement\" whach carries along a log-structured index of a node's output that's easily sharable by subsequent dataflow input nodes. Index data are reference with handles that signal when old index data (or entire indices) can be garbage collected because no subsequent graph nodes will later reference it.\n\nThis new structure is sufficient to reconstruct instances of many different types of specialized systems, and produce competetive performance.\n\n### Strengths\nAn ambitious Grand Unified Theory that seems to pull it off: it subsumes problems from a spectrum of other systems, and often performs better BECAUSE of its cleanliness.\n\n### Areas for improvement\nThe paper bites off such an enormous mouthful that nothing gets proper treatment. I wanted illustrations of all the the architectural ideas, and the evaluation elided way more than one would hope.\nThis is is the sort of synthesis that organizes and advances the field; I'm not surprised it's difficult to exhaustively make the case in 12 pages! The writing is very tight; the problem isn't presentation as much as the page limit.\n\nThe evaluation punts away distributed execution, excused because the improved constants allow K-pg to beat other systems that need multiple nodes. But it's a missed opportunity to show how it scales when data needs to be transported.\n\nThe evaluation references published numbers for baselines, rather than comparing apples-to-apples; in some places, the evaluation only reports a qualitative result, not the actual data for the reader to compare.\n\n### Comments for author\nI was REALLY excited about the promise of a grand unified theory. But I had a hard time understanding this paper, in part because of my inexpertise, and partly because there are a LOT of ideas crammed into the paper. The writing is good, but the writing task is very demanding. I'm also concerned the evaluation is a little too summarized.\n\np2 \"nothing prevents the sharing of state within a worker (even across\ndataflows), other than a discipline for doing so.\"\nI think you mean \"you definitely shouldn't do this sharing, because it'll be impossible to read about the semantics\".\n\np3 I think \"associative group\" and \"these times may be only partially ordered\" are somehow related, but couldn't quite make the connection. If that's true, spell it out?\n\np3 \"and would likely be new to other streaming systems\"\nThe literature is already published; is it new or not?\n\np3 \"as indexed batches\"\nI wondered \"do batches interfere with low latency?\", but you later point out that there's a knob to turn to adjust the latency/throughput tradeoff.\n\np3 \"given their in- put as indexed batches rather than streams of indepen-\ndent updates.\"\nWhy are batches easier to build for than independent updates? you can't exploit the batch, since the boundaries are arbitrary.\n\np3 \"and in all cases we view violations of any of these principles as\nproblematic.\"\nRather than assert they're problematic up here, better to explain at appropriate points in the text why each violation would be problematic.\n\np3 \"Principle 3: Bounded memory footprint.\"\nI think you mean \"memory proportional to the working set size, not the complete collection including quiescent records.\" Is that right?\n\np4 \"(i) a stream of shared indexed batches of updates and (ii) a shared,\ncom- pactly maintained index of all produced updates.\"\nShared across what scope?\n\np4 \"in that it can be understood ...  An independent timely dataflow\ncan ... correctly understand the collection history.\"\nWhat do you mean by understand?\n\np4 \"the number of distinct (data, time) pairs (the longest list\ncontains only distinct pairs, and all other lists accumulate\nto at most a constant multiple of its length).\"\nCan duplicate data be created anywhere along the pipeline, or only at the input?\n\np4 \"This decouples the logical update rate from the physical batching\"\nThis physical/logical decoupling remains slippery to me. Work an example?\n\np5 \"the arrange operator will con- tinue to produce indexed batches, but it\nwill not ...\"\nWhy wouldn't you want to also garbage collect the arrange operator as well?\n\np5 \"Consolidation.\"\nThin paragraph left me confused.\n\np5 \"Full historical information means that computations do not require\nspecial logic or modes to ac- commodate attaching to incomplete in-progress\nstreams\"\nSounds awesome but I still don't quite get it.\n\np6 \"A user can filter an ar- rangement, or first reduce the arrangement to\na stream of updates and then filter it.\"\nAaaugh, I still don't intuit the distinction between an arrangement and a reduced stream of updates\n\np6 \"Amortized work.\"\nIllustration here, too?\n\np6 \"and subtracts the accumulated output.\"\nConfused.\n\np7 \"without extensive re-invocation\"\nCostly? Because the next sentence would be relevant even if it were just a little reinvocation.\n\np7 \"At the tail of the body, the result is merged with the negation of the\ninitial input collection\"\nConfused.\n\np7 \"We have made two minor modifications. First, ar-\nrangements external to the iteration can be introduced (as\ncan un-arranged collections) with the enter operator,\nwhose implementation for arrangements only wraps cur-\nsors with logic that introduces a zero coordinate to the\ntimestamp; indices and batches remain shared.\"\nConfused. Maybe this section needs an illustration.\n\np7 \"K-Pg distributes across multiple machines, but our\nevaluation here is restricted to multiprocessors, which\nhave been sufficient to reproduce computation that re-\nquire more resources for less expressive frameworks.\"\nThat's awesome, but a missed opportunity for the evaluation. Better to also show the system scaling to a distributed implementation, and show that it can tackle bigger problems as a result.\n\np7 \"(with 64bit timestamp and signed difference).\"\nConfused.\n\np8, fig 3 and others: Sort the legends to match the dominant order of the curves in the figure, to make it easier for the reader to dereference.\n\np8 \"As the number of workers increases the latency-throughput trade-off\nswings in favor of latency.\"\nAt the expense of throughput? \"swings\" is a misleading metaphor\n\np8 \"Figure 4b reports the relative throughput increases ...\"\nRelative to DBToaster?\n\nFig 4 has \"query number\" on the X axis, which isn't an interpolatable quantity. Don't connect the points with lines, because it's not meaningful to infer the y-axis value for query #10.5.\n\np9 \"Worker Scaling\": Am I reading this correctly: Kp-g admits batching and parallelization opportunities that allow 10-100x improvements over state of the art? If not, clarify the graphs. If so, call that claim out explicitly, because it's kinda important.\n\np9 \"Our measurements indicate that K-Pg is consistently faster than systems\nlike BigDatalog, Myria, SociaLite,\"\nWhere's the data?\n\np10 \"The throughput greatly exceeds that observed in [21]\"\nBy what factor? Where's the data?\n\np10 \"was comparable to the best shared memory systems (DeALS) on one core\n(three out of five problems) but scaled less well with increasing cores\n(one out of five problems on 64 cores).\"\nWhere's the data plot?\n\np10 \"K-Pg prevents racy access and imposes synchronization be- tween rounds\nof derivation, which can limit performance but also allows it to\nefficiently update such computations when base facts are added or removed.\"\nI wish you could set up experiments that showed both sides of this story.\n\np11 \"This appears to be more a limitation of the query transformation\nrather than K-Pg.\"\nDo other systems do better? Not enough detail in this paragraph for me to follow the argument and evaluate it.\n\np11 \"substantial improvement, due in some part to our ability\nto re-use operators from an optimized system rather than\nimplement and optimize an entirely new system.\"\nCool!\n\nnits\n\np2 \"K-Pg is our attempt to provide a computational framework...\"\nSuggest: 'K-Pg is a computational framework...' That you wrote it down means you provided it, and that it's under submission means it's an attempt. :v) Just make your assertions without apology, and let the paper's content defend them.\n\np4 \"and eliminate the cost all subsequent uses of the arrange-\"\nmissing 'of'\n\np6 \"across otherwise independent workers,\"\ndelete otherwise? I'm not sure what it meant here.\n\np6 \"even if this violates the timestamp order.\"\ns/violates/disregards/ -- violates seems too strong here, since the behavior is preserving semantics.\n\np6 \"when the cur- sor keys match we perform work, and the keys do not match\"\ninsert \"when\" after \"and\"\n\np11 \"Figure 1\"\n-> \"Table 1\"\n\nReview by Jon Howell www.jonh.net/unblind ; intentionally unblinded.\n\n## Author response\n\n*ed: precedes Reveiw #86F*\n\nThank you for the helpful comments! We address three cross-cutting concerns, then answer specific questions.\n\n1.  Why are there no comparisons with Naiad?\n\n    For two reasons: (1) our high-rate experiments cannot run on Naiad because its progress tracking logic is quadratic in the number of concurrent timestamps; and (2) we wanted to decouple measurement of the benefits of shared indexed state from less interesting advantages K-Pg has over Naiad (Rust vs C#: compiled vs. JITed, GC vs non).\n\n    We have baselines of K-Pg configured as Naiad’s differential dataflow implementation would be, without shared indices. “No sharing” in Figures 5b, 5c, and Table 2 represents how we believe Naiad would perform. These improvements may carry over to Naiad, but Naiad’s overheads (above) may mask the benefits (or amplify them; we don’t know). Our experience suggests that K-Pg is substantially faster and uses less memory than Naiad, in addition to catering to classes of algorithms that are impossible to run efficiently on Naiad (viz., those requiring random access, shared state, and high-rate updates). The text should make this clearer, and we will label the baselines appropriately.\n\n2.  Are claims of generality justified?\n\n    We agree that the generality claim in the abstract and intro will benefit from more context.\n\n    This claim is rooted in the fact that neither Naiad, nor any other prior parallel data-processing system (plus RDBMs, GraphDBs, stream processors) natively supports all of (i) random access reads, (ii) random access writes, and (iii) iteration, without substantial overheads. Unfortunately, many non-trivial algorithms require all three. Consider bidirectional search, an iterative graph search algorithm. On K-Pg, the algorithm touches only an asymptotically vanishing fraction of graph edges, while other systems (e.g, Hadoop, Spark, Flink, Naiad) must perform at least one full edge scan; most perform several scans.\n\n    K-Pg can emulate the PRAM model using at most a logarithmic factor more compute. Other systems’ computational models (including Naiad’s, without sharing) don’t have this property. On big data, a quadratic-time implementation of a linear-time algorithm is fundamentally troubling! Hence, we take “generality” to refer to the breadth of algorithms that K-Pg can support with faithful performance.\n\n    We certainly didn’t mean to claim that K-Pg obviates all existing systems. K-Pg is not great on several domains; we’ll discuss these. For example, a specialized relational database (e.g., Vertica) and a batch graph processor (e.g., Gemini) win if no streaming updates are required (§5.3). For tasks with a known, limited scope, existing specialized tools may be preferable; for tasks that may benefit from algorithmic sophistication and flexibility, we believe that K-Pg constitutes a platform with relatively few efficiency-constraining limitations. If a workload needs only specific sophistication (e.g., tensor operators, temporal roll-ups), developing a new, specialized system may be best.\n\n    We have focused on problems based on discrete/combinatorial algorithms, and ones in which time matters. Machine learning applications haven’t been our focus, but are an interesting avenue for future work, since e.g., updating ML parameters requires iteration over shared, indexed state.\n\n3.  The paper is hard to read; can you better explain DD?\n\n    Sorry! The writing can be improved, and running examples should illustrate what’s going on. It’s challenging to fully explain differential dataflow as background for this paper, but we’ll try to improve the intuition. All reviewers ask for more examples, measurements, discussion; we think this is a great sign and will act on their suggestions.\n\n4. Questions answered\n\n    We are now at 558 words. Below, we answer specific questions from the reviews if you’re happy to keep reading beyond the 800 word limit.\n\n    [B] \"preclude cycles along with timestamps do not strictly increase\" -- seems like a bit limitation for generality. It means essentially, no looping over the data [...]\n\n    K-Pg can absolutely loop over data (indeed, this is crucial for iterative graph processing). Like Naiad, K-Pg uses timestamps that extend \"real time\" with additional coordinates.\n\n    [B] Section 2.3 is good to have, but still too abstract. How would the example in Figure 1 be executed with indexes vs. without?\n\n    Good idea! If you executed Fig1 without shared indices, you would need to re-scan and re-index the graph input, even if the reachability query touched only a dozen vertices. If the index already exists, the computation completes in milliseconds instead (cf. Figure 3f).\n\n    [C] Is the arrange operator automatically added and hidden from the end user?\n\n    The operator can be called explicitly by the user (cf. §3) and chained with e.g., join and group operators. If the user calls join or group on unarranged data (a stream), K-Pg invokes the arrange method for them, but does not share results.\n\n    [C] §5: \"K-Pg outperforms ... while supporting more general computation\". Which of these tasks [...] cannot be supported by Spark?\n\n    For Spark to execute computations that involve iteration or streaming, the Spark driver program needs to repeatedly deploy Spark DAG computations. Each time this happens, the RDD abstraction (which doesn’t support random access) requires a re-scan of each of the potentially large inputs. Frameworks in the Spark ecosystem (e.g., GraphX) use various techniques to achieve random-access-like behavior for specific tasks, but don’t support arbitrary iteration and random access reads and writes.\n\n    For example, Spark Streaming can only distinguish the effects of input records by launching them in independent batch computations. To process the sequence of 60M input records for TPC-H, or the 200k/s interleaved reads and writes for streaming graph processing (with consistent reads), Spark would need to launch a batch computation for each input record.\n\n    [C] How often does the frontier advance [in the microbenchmark]? what is the effective (physical) batch size created?\n\n    Open loop measurements run at the fastest rate they can manage, consuming and retiring data as quickly as possible. The reported measurements indicate the distributions of these latencies. The effective batch sizes result from multiplying the reported latencies with the offered load (which varies by experiment).\n\n    [C] What are query modifications like in interactive queries in Sec 5.3?\n\n    Changing the arguments to a query; when we add or remove a query node from e.g., the query set for 2-hop reachability.\n\n    [C] §5.4 sounds casual. What happened to the rest of the 2 problems? What kind of racy code does DeALS permit, and does it compromise correctness?\n\n    The two other measurements were 148s (DeALS) vs. 155s (K-Pg) and 1309s vs. 1471s. We had to cut the detailed measurements for space, but will improve the text. DeALS takes advantage of benign races; this explains why DeALS scales better than K-Pg on a multicore. DeALS finds correct results; however, by performing more synchronous updates, K-Pg retains enough information to incrementally update the computation (tricky with racy updates).\n\n    [E] Will the efforts be erased by introducing network overhead?\n\n    We don't expect so, but we haven’t evaluated it. RDMA latencies are substantially smaller than the latencies we currently see.\n\n    [E] Since [shared state] is indexed and has multiple versions, what about the organization of data structures? Are the data indexed by time or data in <data, time, diff> triplets?\n\n    The data are stored sorted by (data, time, diff), so first by data and then by time. Our incremental operators usually want random access to the history of a particular key (which is part of data), rather than a cross-cutting view of a collection at some time, so this indexing makes sense for our operator implementations.\n\n## PC Summary\n\nMost of us were enthusiastic about the premise of this paper, but nobody understood it well enough to confidently accept it. Our advice:\n\n(1) Writing. You have a challenging writing problem ahead of you. The writing is good, but the challenge is great.\n\n(2) Evaluation. The exciting claim of this paper is \"a generic framework that subsumes diverse specific data processing strategies, while retaining performance in each case.\" Here's the evaluation that would support that hypothesis really well:\n\nOur system subsumes 3 types of general framework: batch, stream, graph. (Feel free to add some of the specific frameworks too, with some allowance to not be quite so exhaustive in the matrix below.) Here are a few familiar (e.g. cited) workloads typically processed by each framework: {B1, B2, S1, S2, S3, G1, G2}. We run each workload against a state of the art batch system, a state of the art graph system, and a state of the art graph system, and K-Pg. Notice that every prior system does a terrible job on some workloads, because they're not good fits. (Let's drill down on each of the failures to understand how the existing design fails.) Notice that K-Pg is competitive (and sometimes better) on every workload. (Drill down on some cases to show how K-Pg resolves the failure mode of the existing systems.)\n\nSuch an evaluation would support the ambitious thesis. And in producing it, you're likely to discover clearer ways to clarify the explanation in the body of the paper.\n\nThe PC is excited about this direction, and we strongly encourage you to push farther on this work."
        },
        {
          "name": "osdi2018-submission.pdf",
          "type": "blob",
          "size": 534.0810546875,
          "content": null
        },
        {
          "name": "release-plz.toml",
          "type": "blob",
          "size": 0.4140625,
          "content": "[workspace]\n# disable the changelog for all packages\nchangelog_update = false\n# disable releasing all packages\nrelease = false\n\n[[package]]\nname = \"differential-dataflow\"\n# enable releasing package\nrelease = true\n# enable the changelog for this package\nchangelog_update = true\n\n[[package]]\nname = \"differential-dogs3\"\n# enable releasing package\nrelease = true\n# enable the changelog for this package\nchangelog_update = true\n"
        },
        {
          "name": "server",
          "type": "tree",
          "content": null
        },
        {
          "name": "sigmod2019-reviews.md",
          "type": "blob",
          "size": 6.8857421875,
          "content": "**Paper ID** 286\n\n**Paper Title** K-Pg: Shared State in Differential Dataflows\n\n**Track Name** Research Paper Second-Round\n\n## Reviewer #1\n\n1. **Overall Evaluation** Weak Reject\n2. **Reviewer's confidence** Some Familiarity\n3. **Originality** Medium\n4. **Importance** Medium\n5. **Summary of the contribution (in a few sentences)**\n\n    The paper described the design and implementation of K-pg a novel data analytics engine built on top of a timely data flow execution engine. Like prior work Naiad, K-pg is based on the differential data flows paradigm, but differs by introducing shared state in the form of indexes across all workers, temporally as well as across iterations. K-pg models updates to data as streams and computations as operations on updates or batches of updates. The key innovation in the system seems to be the concept of arrangements that takes in a set of updates and outputs shared indexes.\n\n6. **List 3 or more strong points, labelled S1, S2, S3, etc.**\n\n    S1: A general framework and implementation for large scale data analytics that subsumes relational, stream and iterative processing pipelines.\n\n    S2: The design based on differential data flows includes novel abstractions like the arrange operator.\n\n    S3: The system outperforms the best of breed systems for relational, streaming and iterative tasks.\n\n7. **List 3 or more weak points, labelled W1, W2, W3, etc.**\n\n    W1: The paper needs to be better written.\n\n    W2: It is not clear how the implementation of the system conforms to the principles laid out in section 3.3\n\n    W3: It is not clear what are all the ways arrangements are used, and which operators use arrangements.\n\n8. **Detailed evaluation. Number the paragraphs (D1, D2, D3, ...)**\n\n    D1: I think the paper would definitely benefit with a running example that is taken throughout the paper to explain the design of the system.\n\n    D2: While the data structures (or indexes) generated by arrangement operators are shared, some language suggests that when another data flow gets a trace handle, the arrangements are “copied over” during an import. Is that correct?\n\n    D3: What is the batch size in figure 4c? And what is w in 4b?\n\n    D4: The term relative throughput is not defined. Throughput is relative to what?\n\n9. **Candidate for a Revision? (Answer yes only if an acceptable revision is possible within one month.)**\n\n    Yes\n\n10. **Required changes for a revision, if applicable. Labelled R1, R2, R3, etc.**\n\n    The paper needs a significant rewrite. See weak points.\n\n## Reviewer #2\n\n1. **Overall Evaluation** Reject\n2. **Reviewer's confidence** No Familiarity\n3. **Originality** Medium\n4. **Importance** Medium\n5. **Summary of the contribution (in a few sentences)**\n\n    The paper presents K-Pg, a system that enhances data parallel operators with the ability to process shared indexes.\n\n6. **List 3 or more strong points, labelled S1, S2, S3, etc.**\n\n    S1. The implementation is modular\n\n    S2. The authors compare against specialised systems\n\n    S3. K-Pg can be used in diverse workflows\n\n7. **List 3 or more weak points, labelled W1, W2, W3, etc.**\n\n    W1. The paper is difficult to follow.\n\n    W2. The plots are not easy to read.\n\n    W3. Latency plots are often omitted.\n\n8. **Detailed evaluation. Number the paragraphs (D1, D2, D3, ...)**\n\n    The paper introduces shared indexes in order to efficiently implement data parallel workflows in various domains. The experimental evaluation shows that their approach achieves good results.\n\n    Timely dataflows are the central concept of the paper but they are not explained well in Section 3.1\n    In section 3.3 it is not clear why the principles are that important\n    Section 4 is difficult to follow\n    The plots are difficult to read\n\n    In general the paper is not well written and it seems that K-Pg is a small extension of Naiad\n\n9. **Candidate for a Revision? (Answer yes only if an acceptable revision is possible within one month.)**\n\n    No\n\n## Reviewer #3\n\n1. **Overall Evaluation** Reject\n2. **Reviewer's confidence** Expert\n3. **Originality** Low\n4. **Importance** Low\n5. **Summary of the contribution (in a few sentences)**\n\n    The authors propose a new system for doing analytics with recursive queries (online and offline). As part of this, they propose a novel operator for indexing and sharing stream data.\n\n6. **List 3 or more strong points, labelled S1, S2, S3, etc.**\n\n    1) A reasonable problem to work on\n    2) Indexing and sharing stream history has some novelty\n    3) Real implementation and experiments with TPCH\n\n7. **List 3 or more weak points, labelled W1, W2, W3, etc.**\n\n    1) Writeup need major improving\n    2) Missing key references\n    3) Comparisons in the paper with similar systems needs major improving, and probably rethinking.\n\n8. **Detailed evaluation. Number the paragraphs (D1, D2, D3, ...)**\n\n    First, much of the paper is incomprehensible, filled with vague unsupported claims. For instance, there is a whole paragraph after the first table in the intro that I found incomprehensible. I have no idea what is meant. Why the word 'holistic'?\n\n    Section 3.3 also made no sense to me. Is exchange supposed to be a shuffle? Nobody implements this as part of a count operator, including Naiad. If I understood what arrange is suppose to be, it keeps a stream history. Count operators don't typically do this. Why is it part of count? Maybe a clear example would help clear some of this up.\n\n    In terms of unsupported claims there are many cases of this throughout the paper. For instance, sticking with section 3.3, there is the statement (relative to Naiad) \"executes robustly despite the absence of frustrating systems knobs\". What knobs? Robust w.r.t. what?\n\n    While some aspects of the experiments are good (full TPCH, real implementation etc...) The authors didn't actually run the competition. Rather, in some cases (they don't say which), they just use numbers published in other papers. How do we know the queries were run the same way. Were they run warm? cold?\n\n    Also significant discussion is spent talking about performance improvements using batching and careful memory layout. The first streaming system do this, resulting in significantly higher performance, was Trill, which can also be used for offline analytics, and which I think the authors are unaware of. Much of the performance related effort is less novel than the authors imply.\n\n    Finally, there is a significant body of work on recursive streaming queries, of which Naiad is one (e.g. \"On-the-fly Progress Detection in Iterative Stream Queries\"). It is really unclear to me what the value is of the proposed system over these approaches. Perhaps a clear example would help? This paper needs a complete rewrite, and possibly significantly more research in the experimental section, which is why I didn't give it a revise.\n\n9. **Candidate for a Revision? (Answer yes only if an acceptable revision is possible within one month.)**\n\n    No\n\n## META-REVIEWER #1\n\n    Not submitted."
        },
        {
          "name": "sigmod2019-submission.pdf",
          "type": "blob",
          "size": 766.935546875,
          "content": null
        },
        {
          "name": "sosp2019-reviews.md",
          "type": "blob",
          "size": 33.9462890625,
          "content": "Reviews, author response, and PC comments as available in the system.\n\n|              | Overall Merit | Originality | Validation | Clarity | Reviewer Expertise |\n|:-------------|-------:|----:|-------:|-------:|-------:|\n| Review #314A |      2 |   2 |     3  |      3 |      3 |\n| Review #314B |      3 |   2 |     3  |      3 |      2 |\n| Review #314C |      3 |   2 |     3  |      2 |      2 |\n| Review #314D |      2 |   3 |     1  |      2 |      4 |\n| Review #314E |      4 |   3 |     3  |      2 |      2 |\n| Review #314F |      3 |   2 |     3  |      3 |      2 |\n\n## Review #314A\n\n### Paper summary\nThe paper proposes and implements a model for sharing computation in a distributed streaming processing system (e.g., Naiad, a successor of Dryad). The proposed system (named SysX for anonymity, I guess) stores data in batches. Each batch is indexed by \"data\" (e.g., some key identifying the data) and it returns the updates for that data for a particular time range. As the data flows through the computation graph, SysX keeps generating batches for new sets of time frontiers. Old batches are aggregated to represent larger intervals.\n\nSysX also introduces an \"arrange\" operator to query indexed time-sharded batches of data.\n\n### Comments for author\nThe reason for my less than enthusiastic score should not be taken as a message that your work is unimportant or poorly done. Quite the opposite: I got a sense of thoughtfulness and care that you put into building this system. I think that, in order to be ready for publication, your paper needs to do a better job of motivating itself and positioning itself with respect to related work:\n\n* Nectar has done a very similar thing. It caches intermediate results of the computation, but to index those results, Nectar uses a hash of data and code. Your system uses a key of data and receives back batches of update organized by time. It seems to me that Nectar can give equivalent results back. A batch is determined by its input records. If a Nectar user hashes a set of input records for a particular time frame and looks up the Nectar has by that key, they would get cached updates for that time frame for the data stream of interest. So does Nectar offer the same functionality as your system? Seems so from a birds-eye view... I could be missing something, but the paper did not do a good enough job convincing me (and perhaps other readers).\n\n* Being a devil's advocate, I could say: \"Nectar does the same thing as SysX, but using a much simpler API.\" Nectar is transparent to programmers. SysX requires reasoning about time intervals and using new operators, which seems awfully complicated. Which brings my second point:\n\n* Could you motivate with several concrete example when a programmer would need to reason about specific time intervals (e.g., query specific time intervals) and show more examples about how they can do this in SysX (vs. how they currently do this). I need more convincing that the API provided by SysX is called for and that it is not overly complicated to use.\n\n* Unlike Nectar, you don't keep track of the code version (or code hash, to be more precise). What if the code version has changed, and the update at time t is not the same as the update at time t1? It seems that operators in SysX are not opaque (unlike in Nectar). Is my understanding correct?\n\nOther comments:\n\nWhat’s your model security and isolation? Do you support users with different access capabilities for parts of the dataset? E.g., someone is allowed to access the aggregate, but not see individual records (as often happens with medical data, for example).\n\nWhat about data loss and recovery? What about “holes” in the data? E.g. if some part of the batch is missing for a particular range of timestamps, does this hold up the advancement of the entire batch?\n\n## Review #314B\n\n### Paper summary\nThis paper presents Shared Arrangements, a new abstraction for sharing indexes across streaming data flows. Sharing indexes across workloads is shown to benefit cases where multiple queries need to lookup or join data from a base input stream and this new abstraction is used to implement differential dataflow on top of timely dataflow. Experiments on streaming TPC-H and graph workload are presented.\n\n### Comments for author\nThanks for you submission to SOSP. The paper is well written and presents an interesting new abstraction for streaming dataflows. Some high level comments followed by detailed comments\n\n- The paper would benefit from presenting a motivation example early on that clearly explains what are the shortcomings from existing dataflow systems like timely or Naiad. Connecting this example to a real-world workload scenario would be better and provide the reader with more context on where such a solution might be useful.\n\n- The abstraction presented here is quite simple and I think that is a positive as this leads to some defined semantics on how multiple streaming data flows can share an index. However there are some lingering questions here that would be good to address: For example right now the assumption is that a single writer, multiple reader setup is good enough. Are there scenarios where this is insufficient?\n\n- The idea of partitioning all the operators by the same key is again very good. But could this lead to scenarios where say one of the dataflows is really large and is starving the other colocated operators because they are sharing the same index? And would there be a situation where there are too many dataflows that share an index and how would the system handle such a situation?\n\n- It would be good to present more details on the trace maintenance section of the paper. It looks to me that this is the key technical area for the paper but right now there are too few details on how the merge works and how work is split to ensure that there is work proportional to the batch size\n\nDetailed comments\n\n- Sec 3.1: A natural question to consider when discussing batching of logical times is whether that leads to increase in latency.\n- Sec 4.1: The description of the trace handle and how it is initialized is not very clear\n- Sec 4.2: It would be good to give an example to show why group and join only need access for times in advance of input frontiers\n- Sec 4.2: Is it desirable for the imported collection to produce consolidated batches? Are there scenarios where this isn't necessary?\n- 5.3.1: variations -> differences?\n- 5.3.1: It would be good to compare this proposal to Eddies http://db.cs.berkeley.edu/papers/sigmod00-eddy.pdf\n- 5.3.1: Can you comment on how garbage collection happens for the join operator? Does having futures delay that? -\n\n## Review #314C\n\n### Paper summary\nShared arrangement (SysX) is a stream processing system that can shared indices maintained at individual workers across multiple concurrent data-parallel streaming dataflows. Doing so avoids re-building indices for each new query and reduces query latency.\n\n### Comments for author\nThe paper argues that indices should be shared across multiple data streams. I agree with this sentiment. However, the paper's writing did not point out why sharing indices is a difficult problem. Both the interface and realization seem somewhat straightforward. It's very likely that I miss something, but it would be good if the paper can explicitly point out the challenges so I don't have to second guess them.\n\nThe evaluation is done only on a single computer, but the discussion makes it seem that SysX designed for a distributed setting. If you've already implemented the distributed functionalities as you claimed, why not evaluate them?\n\nIt would be helpful if the paper can use a more detailed and better example to motivate the technical challenges of implementing shared arrangements. Figure 1. is an example, but it does not illustrate the system's technical challenges. First, the interface is simple, just save the indices and load it from another data stream! Second, this query seems very simplistic for graph analytics. Why pick this specific query, and not a more realistic queries? e.g. counting triangles with double joins? If graph analytics do not present good examples, why not pick some query from TPC-H? Third, there is no discussion on the subtleties or behind-the-scenes stuff done for this example. Are the nodes/edges shared based on vertices? If so, how is the join performed (across the workers)? Perhaps there's no subtlety and the behind-the-scenes operations are straightforward? If so, then you need a better example.\n\nI cannot help but think the authors assume readers are very familar with timely dataflow (or the specific open source implementation of timely dataflow).\nFor example, in the last paragraph of Sec 3, the \"exchange\" operator is brought up abruptly for the first time without any explanation.\n\n## Review #314D\n\n### Paper summary\nThis paper proposes the abstraction of shared arrangements to allow multiple queries to share indexed views of maintained state in streaming dataflows. A shared arrangement is essentially a shared, multi-versioned index of updates maintained for a streaming dataflow, often sharded for data-parallel processing. In addition, the paper discusses the design of operators that are optimized to leverage shared arrangements. Shared arrangements have been designed and implemented on top of Differential Dataflow (which builds on the Timely Data flow model). Evaluations show end-to-end benefits of shared arrangements with standard relational analytics (TPC-H) and interactive graph query. Experiments have also been conducted to evaluate various aspects of design.\n\n### Comments for author\n\n+ Shared arrangements are a nice abstraction for cross-query sharing of sharded index in streaming dataflows.\n\n+ The overall design is elegantly laid out with clean abstractions and operator design.\n\n+ Shared arrangements can be highly effective, as shown in some of the evaluations.\n\n- The paper remains of theoretical interest, as it ignores a large number of hard problems that would arise in practice.\n\n- The evaluation is insufficient and too simplistic due to its choice of baseline and workload.\n\nSharing sharded, multiversioned index across queries of the same streaming dataflows sounds like a good idea, at least in theory. There are clear cases where maintaining such an index is hugely profitable. I like the principled approach the paper takes in defining the abstraction of shared arrangements and framing the design and implementation using operators. The resulting solution is elegant and easy to understand.\n\nBut in practice, there seem to be many challenges and hard design decisions to make. For example, when is building a sharded mulltiversioned index beneficial? Which set of queries should we group together for a shared arrangement? How do we balance the benefits of shared arrangements against performance interference, coordination between multiple queries, overall memory pressure, and the lack of failure isolation? What is the fault tolerance story of shared arrangements? None of those issues are addressed in the paper.\n\nShared arrangements are neatly presented in the context of Differential Dataflow and Timely Dataflow. The paper claims that the approach is general and can be applied to other streaming systems. It would be desirable to abstract out the properties that shared arrangements depend on from Differential DataFlow and Timely DataFlow (e.g., the notion of frontiers), to show how they can be done on other streaming systems.\n\nNo real streaming scenarios (e.g., with windowing operators) are used in the evaluation, which is surprising. The benchmarks used are relatively simple and do not expose any interesting complications. For example, in a real streaming scenario, where multiple queries (processing logic) can benefit from sharing an in-memory index. But there are often timing constraints and coordination cost to be considered. For example, different streaming queries might use different windowing operations, making it significantly more challenging for shared arrangements to be effective.\n\nFor interactive graph query, the case is even less convincing. Shared arrangements seem just to be an in-memory evolving graph representation. Maintaining such an in-memory structure seems to be the obvious and natural choice. No one really builds multiple in-memory graph representations for different queries on evolving graphs.\n\nFor TPC-H, it sounds like the main benefits comes from a better in-memory index. Would it be better to use STREAM (which supports synopses) as the baseline?\n\n## Review #314E\n\n### Paper summary\nExisting modern stream processing systems based on the dataflow paradigm support efficient incremental updates by maintaining indexes of operator state. This state and its index is private to each dataflow operator, removing issues of coordinating concurrent updates to the state, but leading to duplication of effort and wasted memory when multiple operators need to maintain the same indexed state. The authors introduce the \"shared arrangements\" abstraction to allow multiversioned indexed state to be shared by multiple operators. The key challenge is how to support this without excessive coordination overhead. Their solution builds on the frontiers provided by Timely Dataflow for coordination, and the collection traces from Differential Dataflow, to allow collection traces to be shared by multiple dataflow operators. Their new dataflow operator, arrange(), maintains the traces by constructing batches of updates when the Timely Dataflow frontier advances. Each batch contains updates between its lower and upper frontier times, indexed by a key derived from the update's data item. Other operators can share the indexed trace by importing it, which yields a trace handle used to access the trace. The trace handle also has a time frontier, which advances as the operator processes its inputs, and which determines the view of the shared trace that this operator can access. These per-trace-handle frontiers allow multiple operators to share access to the trace without additional coordination between trace updates and accesses. To take advantage of this, operators such as filter, join, and group, need to implemented in an \"arrangment-aware\" manner. The evaluation shows that the solution improves throughput, latency and memory utilization, meeting its goals.\n\n### Comments for author\nThe shared arrangements abstraction is a nice solution to the problem of wasted work and memory when multiple operators construct essentially the same indexed state for their own needs. The discussion of the background and related work in Section 2 does a really good job of showing how other data processing systems (including relational databases, batch processing and stream processing) deal with issues of sharing state, incorporating updates, and coordination. Similarly, the context in Section 3 shows how Timely Dataflow and Differential Dataflow provide the tools needed to build shared arrangements. Taking these building blocks and designing the new abstraction, including the arrange() operator, batched indexed updates in traces, trace handles to allow operators to proceed through the shared index at their own pace, and the compaction of trace batches requires innovation. The evaluation shows that the resulting system achieves its goals and improves performance in multiple dimensions.\n\nWhile the paper is well-written, I downgraded the \"Presentation and clarity\" score because some concepts did not come across clearly. At times, the full explanations appear long after they would have been useful. At other times, familiarity with concepts from prior work (such as time frontiers from Timely) is assumed. And in some places, the writing is just not clear. Examples follow:\n\n- Section 4 and 4.1 (and Figure 2) refer to the batches of updates that make up a collection trace. How the arrange() operator produces batches isn't revealed until 4.2 however. It would be helpful to move the first 2 sentences of the second paragraph of 4.2 (\"At a high level...\") to the end of the first paragaraph of 4.1 (i.e., right after mentioning that the advance operator and the advance of the Timely Dataflow input frontier).\n\n- Section 4.1 \"Each reader of a trace holds a trace handle, which acts as a cursor that can navigate the multiversioned index as of any time in advance of a frontier the trace handle holds.\" This sentence is a bit convoluted, but aside from that, I found it hard to grasp the idea that an operator would navigate the index at times that are logically ahead of its own time. Wouldn't that mean it can view updates that have not logically happened yet from its own time frontier? Section 4.3 helps a bit by saying that many operators only need access to accumulated input collections for time in advance of their input frontiers, but it isn't clear to me why that is true. Either a separate illustration of these frontiers with respect to a particular operator and its trace handle, or an enhanced version of Figure 2 that illustrates the time frontiers, along with a more detailed explanation, could help greatly.\n- Section 4 \"Logically, an arrangement is a pair of (i) a stream of shared indexed batches of updates and (ii) a shared, compactly maintained index of all updates.\" It is not clear at this point in the paper (or ever, really) what the distinction is between (i) and (ii), or when you would use each of them. After reading 4.3, I suspect it relates to the \"consolidated historical batches\" vs the \"newly minted batches.\"\n- Section 4.2, Consolidaton: \"... but the mathematics of compaction have not been reported previously.\" This submission does not remedy that - it contains none of the mathematics of compaction. It also doesn't really explain what it means for times to be indistinguishable, or how updates to the same data are coalesced.\n\nI think the evaluation does a good job exploring the benefits of shared arrangements in a variety of settings, and of explaining the results. This section has a few more typos than earlier parts of the paper, but I am sure they are easily fixed in proofreading. My comments here are very minor:\n\n- Section 3 ends by mentioning a tradeoff involving reduced granularity of parallelism in exchange for co-locating operators for shared processing. It says the tradeoff is \"nearly always worthwhile\" in the authors' experience. If this tradeoff is explored in the evaluation, I couldn't identify it. I would love to see some data backing up the statement from Section 3.\n- In Section 7.1.1, Memory footprint, you say wihtout shared arrangements memory varies between 60-120 GB. With the log-scale y-axis it is hard to say exactly what the range is, but this seems overstated. It doesn't get up to 120 GB and the 60 GB point is only seen early in the run. Most of the time is spent fluctuating around the 90 GB line. There is still much more memory consumed, and more variability than with shared arrangements, but you can make a more precise statement about it.\n- In Section 7.1.2, Update throughput, last sentence, \"We note ... further illustrating the benefits of parallel dataflow computation with shared arrangements.\" The orders of magnitude higher best throughput compared to Pacaci et al. also occurs for your SysX without shared arrangements.\n- In Section 7.3.1, Top-down (interactive) evaluation, \"... due to a known problem with the magic set transform.\" Can you cite something for this known problem?\n- In Section 7.3.1, Bottom-up (batch) evaluation, \"... and is comparable to the best shared-memory engine (DeALS). \"\n\n## Review #314F\n\n### Paper summary\nThis paper presents shared arrangements, a streaming dataflow mechanism that shares indexes between different queries. Classical streaming systems build indices separately on every query. Sharing indices among queries saves index update time as well as space. Whereas the existing work STREAM already shares indices across queries, shared arrangements provides a data-parallel multiversioned version of STREAM. The paper provides changes to the dataflow model, the arrange operator which builds the shared arrangements, and update operator implementations which take advantage of the shared indices. The evaluation shows that this approach reduces the interactive query latency by orders of magnitude as compared to approaches that cannot share indices.\n\n### Comments for author\nThe paper is well written and provides a useful improvement over the shared indices in STREAM, by providing data-parallel and multiversioned shared indices. The performance improvement is undoubtedly significant over the classical streaming approaches (which do not share indices across queries) and the evaluation does a convincing job of proving this point.\n\nMy main concern with this paper is that its contribution is incremental over STREAM and Naiad. I was also a bit disappointed to see that the introduction, which positions shared arragements, compares the system to primarily classical streaming approaches, which do not have indexing. I think it would have been more upfront to admit in the introduction that the idea of sharing indices across queries already existed in STREAM and then discuss the limitations with that and improvements your approach brings. Also, in the evaluation, a key demonstration I wanted to see is how shared arrangements improve over an approach as in STREAM, yet the system is primarily compared again with non-shared index approaches. There is a mention of a full paper that might contain something relevant to this, but I wanted to see this evaluation in the paper as a key aspect. I recommend the authors to update their positioning in the introduction and the evaluation.\n\nI didn’t get much insight from reading the design sections: what were the challenges in designing the protocol? What were the main insights? It would be great to distill these insights for the reader.\n\nI was wondering if there is any way to automatically infer optimal arrangements given the data and a list of queries? Right now, the programmer has to call the arrange function.\n\nThe language in the paper is generally polished, with only a few typos: “its has received” -> “it has received\"\n\n## Author Response\n\nThank you for your reviews! We clarify factual errors and cross-cutting concerns, then respond to other questions.\n\n### Policy questions with shared arrangements (SAs)\n[B,D,F] We agree that interesting questions come with a powerful new primitive like SAs! Our paper is primarily about this new primitive. In practice, policy questions are addressed at higher layers: e.g., an automated query optimizer decides when to use an SA, and how many operators to co-locate. Two industrial SysX users wrote query compilers on top of SysX, and anecdotally report that “[TOOL1] works hard to identify those sharing opportunities” and found that “every single [TOOL2] query [makes] use of shared base arrangements”. We’ll add this context.\n\n### Related work\n[A] Nectar identifies previously-computed collections to avoid whole-collection recomputation. It presents each collection as a sequence of records, rather than as a structured index. SysX’s harnesses the tremendous benefit from sharing index-structured views of these collections, rather than the sequences Nectar would present. These are complementary contributions: Nectar identifies what to share, and SysX how to share it. While one could imagine treating the contents of a collection as many tiny Nectar “collections”, Nectar wouldn’t provide the microseconds latencies and update throughput SysX targets.\n\n[D,F] STREAM lacks data-parallelism and doesn’t index batches: each operator remains tuple-at-a-time, and so doesn’t recoup shareable computation. A STREAM evaluation baseline would measure the benefits of data parallelism in SysX, as opposed to benefits brought by sharing. As all current scalable stream processors employ data-parallelism and none employ sharing, which SysX proposes to [re-]introduce, we measured the impact of sharing on a data-parallel system. We’ll frame the evaluation this way. (Additionally, the final STREAM release (February 2005) doesn't compile for us.)\n\n[B] Eddies [SIGMOD’00] dynamically reorder operators in a relational query plan for long-running one-shot queries on static data. SysX supports sharing of maintained indices among multiple incrementally-updated, not necessarily relational, standing queries.\n\nFault tolerance\n[A,D] SysX’s underlying timely dataflow model is fault-tolerant (see Naiad [SOSP’13], Falkirk Wheel [arXiv:1503.08877]). SAs potentially simplify this further, as their representation as LSMs of immutable batches is relatively easy to checkpoint and recover.\n\n### Evaluation\n[D] SysX supports windowed operators. The sequence of changes in §7.1.2 is a continuously sliding window, where random edges are overwritten in the same order they were introduced. SysX supports arbitrary retractions, making windows an idiom that drives one retraction pattern. It is absolutely correct that differently windowed collections are semantically different and cannot be trivially shared; we’ll clarify this.\n\n[D] SAs for graphs are indeed an in-memory evolving graph representation. Most academic graph processing projects and industrial graph databases don’t maintain sharded, multiversioned indices (exception: Tegra, mentioned in §2). E.g., the Neo4j advice is to set up read replicas for analytic queries, as it lacks multiversioned indices and queries would either block the write stream or produce inconsistent results.\n\n[C] The sharing of arrangements is purely intra-thread, and SysX’s inherits its distributed functionality from timely dataflow. We felt that a single-machine evaluation isolates the benefit of SAs without other confounding factors.\n\n### Extended responses to specific points\n\n> [D] It would be desirable to abstract out the properties that shared arrangements depend on from Differential DataFlow and Timely DataFlow (e.g., the notion of frontiers).\n\nSAs use timely dataflow’s logical timestamps for versioning, but this dependency isn’t fundamental: watermarks (e.g., in Flink) could also provide a notion of frontier. However, SysX’s performance relies on the ability to co-locate dataflows, which Flink does not currently support.\n\n> [F]: I didnʼt get much insight from reading the design sections: what were the challenges in designing the protocol? What were the main insights?\n\nThe primary challenges were in finding a mechanism for fine-grained shared, indexed state that supports high read and write throughput, low read and write latency, and a compact footprint. Our LSM-tree-style design provides a mechanism with sufficiently clear semantics to function in a concurrent environment, and integrates well with data-parallel processing. In particular, a crucial insight was that, in the context of data-parallel stream processing, the hierarchical merging of batches (similar to an LSM tree) allows for amortized processing, so that no single batch incurs a high latency, and that a trace consisting of indexed batches does not require fine-grained coordination with downstream operators.\n\n> [B]: [R]ight now the assumption is that a single writer, multiple reader setup is good enough. Are there scenarios where this is insufficient?\n\nFor performance, no: one main structural benefit of dataflow models is the pre-resolution of concurrency issues through sharding and logical timestamps. A single-writer per shard works quite well and additional workers offer better data-parallelism than having multiple writers to a particular shared arrangement.\n\nFor mutably sharing an arrangement across multiple subsequent operators, the complexity and overheads likely exceed the benefit. Multi-writer sharing introduces the need to coordinate (or multiversion) their writes, and makes for a much more involved data-structure closer to distributed shared or transactional memory. The high-performance DB community seems to be moving in this direction as well (thread based ownership, rather than locks).\n\n> [B]: Is it desirable for the imported collection to produce consolidated batches? Are there scenarios where this isn't necessary?\n\nYes, and yes. The imported collection should be maintained and presented in a compact representation, rather than as the full stream of historical events (which could be unboundedly large), and consolidated batches are a natural representation for that. Some restrictive streaming computations, like tumbling windows, do not require access to historical data and may not require consolidated batches, although they could still benefit from them if their first action is to join or reduce on the arranged key.\n\n> [B]: A natural question to consider when discussing batching of logical times is whether that leads to increase in latency.\n\nBatching in SysX occurs when there is more than a single input record enqueued for processing. In this context, batching increases the latency for the first record in the queue, but reduces latency by substantially more for the records at the tail end of the queue. QoS guarantees might motivate using smaller batches based on priority or arrival times to better service earlier records, but this isn’t something we’ve investigated.\n\n> [A]: [What] if some part of the batch is missing for a particular range of timestamps, does this hold up the advancement of the entire batch?\n\nYes. SysX has a strong commitment to computing the \"correct answer\", and only exposes complete results for a given timestamp. A higher layer could attempt to address this by re-ordering late data and communicating the changed semantics on to downstream consumers, but at this point there is an important policy decision to make (e.g. this could be fine for ad click monitoring, but not for security audits).\n\n> [B]: But could [partitioning all operators by the same key] lead to scenarios where say one of the dataflows is really large and is starving the other colocated operators because they are sharing the same index? And would there be a situation where there are too many dataflows that share an index and how would the system handle such a situation?\n\nThis is a trade-off we are delighted to have access to! One always retains the ability to not share the state, and we believe that a higher-level optimizer can automate reasonable approximations of this decision. Note, however, that in the Naiad/timely dataflow design with a fixed set of workers, computation will be co-located even if it does not share state (operators are implemented as fibers with cooperative multithreading, to avoid starvation). In the limit, one can start a new system instance to host additional queries if they overwhelm the provisioned system, but sharing delays the moment at which this needs to happen (and reduces the load on other systems, such as a Kafka source, which will need to feed fewer instances).\n\n> [A]: What’s your model security and isolation? [...] E.g., someone is allowed to access the aggregate, but not see individual records.\n\nThis authentication is the responsibility of a higher layer, but we view it as out of scope for the compute plane.\n\n> [B]: Can you comment on how garbage collection happens for the join operator? Does having futures delay that?\n\nNot exactly, no. Futures capture reference-counted references to the shared immutable batches they require, but they do not maintain trace handles themselves and so do not block trace maintenance and garbage collection. They do block the release of the memory backing the shared batches they hold until the future completes.\n\n> [C]: [Fig. 1] seems very simplistic for graph analytics. Why pick this specific query, and not a more realistic queries? e.g. counting triangles with double joins? If graph analytics do not present good examples, why not pick some query from TPC-H?\n\nWe picked a simple query to avoid overloading the reader early in the paper; this specific query is representative of TPC-H queries that performs two joins on primary keys. Counting triangles would make for a more complex example if done efficiently (SysX and its sharing supports worst-case optimal joins); doing it with two binary joins is a very inefficient way to go about the problem and we did not want to present an inefficient implementation.\n\n> [C]: Are the nodes/edges shared based on vertices? If so, how is the join performed (across the workers)? Perhaps there's no subtlety and the behind-the-scenes operations are straightforward?\n\nThe example is primarily intended to give a flavor of how the API for shared arrangements looks like, and relies on operator APIs similar to differential dataflow’s. In particular, the join operator joins tuples (x, y) and (x, z) into (x, y, z), using the first coordinate as its key. In the example, tuples represent edges, so x refers to a vertex. The system partitions the collections by this key, so exchanges (shuffles) happen between the map() calls and the subsequent join() calls. Hence, the large data stay put, but the records produced by the joins need to be shuffled.\n\n> [D]: For TPC-H, it sounds like the main benefits comes from a better in-memory index. Would it be better to use STREAM (which supports synopses) as the baseline?\n\nOur goal was to evaluate the benefits of shared state, as this is the feature currently absent from scalable data processors. STREAM doesn't support data parallelism or iterative computation or the more general operator vocabulary of SysX. If we were to compare to it, results would like be dominated by the benefits of these features, rather than the systems’ shared indexes.\n\n> [E]: In Section 7.3.1, Top-down (interactive) evaluation, \"... due to a known problem with the magic set transform.\" Can you cite something for this known problem?\n\nNot that we are aware, unfortunately. It is a negative result, and it may have been challenging to publish.\n\n## PC Response\n\nThe PC discussed this paper extensively. We enjoyed a number of things about the paper such as the new abstraction as well as the performance gain over a classical setting of no-sharing indices. At the same time, the PC members felt that the contribution was not sufficiently motivated. It would help to motivate the API and the contribution via concrete examples and demonstrate the shortcomings of existing dataflow systems on these and why the new API is called for. The PC members also felt that the choice of baseline in the evaluation was not convincing, as detailed in the reviews. While the rebuttal helped address a number of smaller points, it did not assuage these high level concerns of the reviewers."
        },
        {
          "name": "sosp2019-submission.pdf",
          "type": "blob",
          "size": 815.8388671875,
          "content": null
        },
        {
          "name": "src",
          "type": "tree",
          "content": null
        },
        {
          "name": "tests",
          "type": "tree",
          "content": null
        },
        {
          "name": "timely_master",
          "type": "tree",
          "content": null
        },
        {
          "name": "tpchlike",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}