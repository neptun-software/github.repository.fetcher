{
  "metadata": {
    "timestamp": 1736709110572,
    "page": 276,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjI4MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "guillaume-be/rust-bert",
      "stars": 2707,
      "defaultBranch": "main",
      "files": [
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.4375,
          "content": ".idea/*\n# Generated by Cargo\n# will have compiled files and executables\n/target/\n\n# Remove Cargo.lock from gitignore if creating an executable, leave it for libraries\n# More information here https://doc.rust-lang.org/cargo/guide/cargo-toml-vs-cargo-lock.html\nCargo.lock\n\n# These are backup files generated by rustfmt\n**/*.rs.bk\n\n\n#Added by cargo\n#\n#already existing elements are commented out\n\n/target\n#**/*.rs.bk\n/models/\n/.venv/\nconvert_model.log"
        },
        {
          "name": ".travis.yml",
          "type": "blob",
          "size": 0.75390625,
          "content": "language: rust\ndist: bionic\n\njobs:\n  include:\n    - before_script:\n      - rustup component add rustfmt\n      script:\n        - cargo fmt -- --check\n    - before_script:\n      - rustup component add clippy\n      script:\n        - cargo clippy --all-targets --all-features  -- -D warnings -A clippy::assign_op_pattern\n    - script:\n      - cargo build --verbose\n    - os:\n      - windows\n      script:\n        - cargo build --verbose\n    - script:\n      - sudo apt-get install python3-pip python3-setuptools\n      - pip3 install --upgrade pip\n      - pip3 install -r requirements.txt --progress-bar off\n      - python3 ./utils/download-dependencies_distilbert.py\n    - before_script:\n      - cargo run --example download_all_dependencies\n      script:\n        - cargo test\n"
        },
        {
          "name": "CHANGELOG.md",
          "type": "blob",
          "size": 27.126953125,
          "content": "# Changelog\n\nAll notable changes to this project will be documented in this file. The format is based\non [Keep a Changelog](https://keepachangelog.com/en/1.0.0/).\n\n## [Unreleased]\n\n## [0.23.0] - 2024-01-20\n\n## Changed\n\n- (BREAKING) Upgraded to `torch` 2.4 (via `tch` 0.17.0).\n\n## [0.22.0] - 2024-01-20\n\n## Added\n\n- Addition of `new_with_tokenizer` constructor for `SentenceEmbeddingsModel` allowing passing custom tokenizers for\n  sentence embeddings pipelines.\n- Support for [Tokenizers](https://github.com/huggingface/tokenizers) in pipelines, allowing loading `tokenizer.json`\n  and `special_token_map.json` tokenizer files.\n- (BREAKING) Most model configuration can now take an optional `kind` parameter to specify the model weight precision.\n  If not provided, will default to full precision on CPU, or the serialized weights precision otherwise.\n\n## Fixed\n\n- (BREAKING) Fixed the keyword extraction pipeline for n-gram sizes > 2. Add new configuration\n  option `tokenizer_forbidden_ngram_chars` to specify characters that should be excluded from n-grams (allows filtering\n  n-grams spanning multiple sentences).\n- Improved MPS device compatibility setting the `sparse_grad` flag to false for `gather` operations\n- Updated ONNX runtime backend version to 1.15.x\n- Issue with incorrect results for QA models with a tokenizer not using segment ids\n- Issue with GPT-J that was incorrectly tracking the gradients for the attention bias\n\n## Changed\n\n- (BREAKING) Upgraded to `torch` 2.1 (via `tch` 0.14.0).\n- (BREAKING) Text generation traits and pipelines (including conversation, summarization and translation) now return\n  a `Result` for improved error handling\n\n## [0.21.0] - 2023-06-03\n\n## Added\n\n- Addition of the [LongT5](https://arxiv.org/abs/2112.07916) model architecture and pretrained weights.\n- Addition of `add_tokens` and `add_extra_ids` interface methods to the `TokenizerOption`. Allow building most pipeline\n  with custom tokenizer via `new_with_tokenizer`.\n- Addition of `get_tokenizer` and `get_tokenizer_mut` methods to all pipelines allowing to get a (mutable) reference to\n  the pipeline tokenizer.\n- Addition of a `get_embedding_dim` method to get the dimension of the embeddings for sentence embeddings pipelines\n- `get_vocab_size`, `get_decoder_start_token_id` and `get_prefix_and_forced_bos_id`  for the `TokenizerOption` in\n  pipelines\n- Addition of the [GPT-J](https://www.eleuther.ai/artifacts/gpt-j) model architecture\n- Addition of the [NLLB](https://arxiv.org/abs/2207.04672) model architecture and pretrained weights\n- Addition of support for ONNX models (encoder, decoders, encoder-decoders) via the [ort](https://github.com/pykeio/ort)\n  onnxruntime bindings\n- Integration of ONNX models to the sequence classification, token classification, question answering, zero-shot\n  classification, text generation, summarization and translation pipelines\n\n## Changed\n\n- Bumped the tokenizers dependency from 7.x to 8.x, exposing additional options for special token mapping and adding the\n  NLLBTokenizer\n- (BREAKING) Simplified the generation traits (removal of LMHeadModel and elimination of unnecessary specification for\n  LanguageGenerator)\n- (BREAKING) Upgraded to `torch` 2.0 (via `tch` 0.13.0). The process to automatically download the dependencies have\n  changed, it must now be enabled via the `download-libtorch` feature flag.\n- Read the `decoder_start_token_id` from the provided configuration rather than using a hard-coded default value\n- (BREAKING) Changed the return type of the `LanguageGenerator` and pipelines functions `float`, `half`, `set_device`\n  to `Result<(), RustBertError>` as these become fallible for ONNX models\n- (BREAKING) Wrapped the model resources specification for the pipeline `Config` objects into an `Enum` to allow\n  handling both torch-based and ONNX models.\n  The `model_resources` field now needs to be wrapped in the corresponding enum variant,\n  e.g. `model_resources: ModelResources::TORCH(model_resource)` for Torch-based models\n- (BREAKING) Added the `forced_bos_token_id` and `forced_eos_token_id` fields to text generation models.\n  If these are not None, this will trigger a forced BOS/EOS token generation at the first of `max_length` positions (\n  aligns with the Pytorch Transformers library)\n- Project structure refactoring (torch-based models moved under common module). Non-breaking change via re-exports.\n\n## Fixed\n\n- MIN/MAX computation for float-like (was set to infinity instead of min/max)\n- Remove the (unused) pooler from the set of weights for BERT Masked LM architecture\n\n## [0.20.0] - 2023-01-21\n\n## Added\n\n- Addition of All-MiniLM-L6-V2 model weights\n- Addition of Keyword/Keyphrases extraction pipeline based on KeyBERT (https://github.com/MaartenGr/KeyBERT)\n- Addition of Masked Language Model pipeline, allowing to predict masked words.\n- Support for the CodeBERT language model with pretrained models for language detection and masked token prediction\n\n## Changed\n\n- Addition of type aliases for the controlled generation (`PrefixAllowedFunction`) and zero-shot\n  classification (`ZeroShotTemplate`).\n- (BREAKING) `merges_resource` now optional for all pipelines.\n- Allow mixing local and remote resources in pipelines.\n- Upgraded to `torch` 1.13 (via `tch` 0.9.0).\n- (BREAKING) Made the `max_length` argument for generation methods and pipelines optional.\n- (BREAKING) Changed return type of `ModelForSequenceClassification` and `ModelForTokenClassification`\n  to `Result<Self, RustBertError>` allowing error handling if no labels are provided in the configuration.\n\n## Fixed\n\n- Fixed configuration check for RoBERTa models for sentence classification.\n- Fixed a bug causing the input prompt to be truncated for text generation if the prompt length was longer\n  than `max_length`\n\n## [0.19.0] - 2022-07-24\n\n## Added\n\n- Support for sentence embeddings models and pipelines, based on [SentenceTransformers](https://www.sbert.net).\n\n## Changed\n\n- Upgraded to `torch` 1.12 (via `tch` 0.8.0)\n\n## Fixed\n\n- Allow empty slices or slices of empty prompts for text generation.\n\n## [0.18.0] - 2022-05-29\n\n## Added\n\n- Addition of the DeBERTa language model and support for question answering, sequence and token classification\n- Addition of the DeBERTa v2/v3 language model and support for question answering, sequence and token classification\n- Addition of a `new_with_tokenizer` method allowing building language model generator with a custom tokenizer (or\n  pairing a tokenizer that was not originally designed with the model, e.g. T5 tokenizer with GPT2 model).\n- (BREAKING) Addition of support for mT5 model, addition of new optional fields to T5Config\n- Addition of `token_scores` field when `output_scores` is set to `true` for generation, returning the score for each\n  token generated\n- Addition of `offsets` to entities generated in the `NER` pipeline\n\n## Changed\n\n- (BREAKING) Updated `Resources`, moving `RemoteResource` and associated download utilities/dependencies behind a\n  feature gate (enabled by default). Reworked the API for building and using resources.\n- Upgraded to `torch` 1.11 (via `tch` 0.7.2)\n- Simplified token classification pipeline and mode aggregation now deterministic (fall back to the highest score for\n  equally common labels)\n\n## Fixed\n\n- Fixed sinusoidal embeddings not being updated when loading a state dictionary (DistilBERT)\n\n## [0.17.0] - 2021-12-19\n\n## Changed\n\n- Updated to `tch` 0.6.1 (libtorch 1.10)\n- (BREAKING) Simplified the generics for multiple library traits taking as a rule `&[AsRef<str>]` or `&str` as inputs (\n  no longer accepts owned types `Vec` and `String`)\n\n## Added\n\n- (BREAKING) Support for `bad_word_ids` generation, allowing to ban a set of word ids for all model supporting text\n  generation\n- Support for half-precision mode for all models (reducing memory footprint). A model can be converted to half-precision\n  by calling the `half()` method on the `VarStore` is it currently stored in. Half-precision Torch kernels are not\n  available for CPU (limited to CUDA devices)\n- (BREAKING) Extension of the generation options that can be provided at runtime (after a model has been instantiated\n  with a `GenerateConfig`), allowing to update the generation options from one text generation to another with the same\n  model. This feature is implemented at the `LanguageGenerator` trait level, the high-level `TextGeneration` pipeline\n  API remains unchanged.\n- Addition of the FNet language model and support for sequence, token and multiple choice classification, question\n  answering\n- Addition of a full entities' prediction method supporting the IOBES scheme (merging entities token such\n  as <New> + <York> -> <New York>)\n\n## [0.16.0] - 2021-08-24\n\n## Added\n\n- (BREAKING) Support for `prefix_allowed_tokens_fn` argument for generation, allowing users to control the generation\n  via custom functions\n- (BREAKING) Support for `forced_bos_token_id` argument for generation, allowing users to force a given BOS token for\n  generation (useful for MBart/M2M-class models)\n- (BREAKING) Support for `output_scores` boolean argument for generation, allowing users to output the log-probability\n  scores of generated sequences. Updated the return type of low-level generate API to `GeneratedTextOutput`\n  and `GeneratedIndicesOutput` containing optional scores along with the generated output.\n- Addition of the MBart Language model and support for text generation / direct translation between 50 language\n- Addition of the M2M100 Language model and support for text generation / direct translation between 100 language\n\n## Changed\n\n- Updated GPT2 architecture to re-use embeddings for the output projection layer (resulting in smaller model weights\n  files and memory footprint)\n- Upgraded `tch` version to 0.5.0 (using `libtorch` 1.9.0)\n- Changed default value of `no_repeat_ngram_size` for text generation from 3 to 0, aligning\n  with [Python's Transformers](https://huggingface.co/transformers/main_classes/model.html?highlight=no_repeat_ngram_size#transformers.generation_utils.GenerationMixin.generate)\n- Added the possibility to handle long inputs for token classification tasks (exceeding the model maximum length) using\n  sliding windows over the input\n- (BREAKING) Generalized borrowing of Tensors as input for models\n- Aligned the optional `all_hidden_states` output for all models\n\n## Fixed\n\n- Updated T5 Decoder cross-attention to no longer use relative position bias (aligned\n  with [Python reference update](https://github.com/huggingface/transformers/pull/8518))\n- Removed hardcoded maximum length for sequence and token classification tasks, now using the model maximum position\n  embeddings instead\n\n## [0.15.1] - 2021-06-01\n\n### Fixed\n\n- Fixed conversation model panic for user inputs exceeding the maximum model length (1000 tokens)\n- Fixed translation model panic for user inputs exceeding the maximum number of position embeddings\n\n## [0.15.0] - 2021-05-16\n\n### Added\n\n- Addition of translation language pairs:\n    - English <-> Chinese (Simplified)\n    - English <-> Chinese (Traditional)\n    - English <-> Dutch\n    - English <-> Swedish\n    - English <-> Arabic\n    - English <-> Hebrew\n    - English <-> Hindi\n- Addition of a Part of Speech pipeline. This pipeline allows predicting the POS tag (e.g. Noun, Adjective, Verb) of\n  words in input sentences.\n- Addition of a lightweight English Part of Speech tagging pretrained MobileBERT model\n- Addition of the Pegasus language model and support for conditional generation\n- Addition of a model for Pegasus summarization pretrained on the CNN-DM dataset\n- Addition of the GPT-Neo language model and pretrained snapshots (125M, 1.3B and 2.7B parameters). Registration of\n  GPT-Neo as an option for `TextGenerationPipeline`.\n\n### Changed\n\n- (BREAKING) Changed `classif_dropout` in `BartConfig` to be an optional field. This affects dependencies\n  instantiating `BartConfig` from scratch, or using `classif_config` for custom model heads.\n- (BREAKING) Changed token classification pipelines to return a Vec<Vec<Token>> instead of a Vec<Token>. The token-level\n  predictions are now returned in separate vectors for each input sequence provided as an input (they were previously\n  returned in a flattened vector)\n- Simplification of the BART language model code base (also used for Marian and Pegasus language models)\n- (BREAKING) Updated to `tch 0.4.1` (based on `libtorch 1.8.1`)\n\n### Fixed\n\n- Fixed character indexing error for Question Answering pipeline answers\n\n### Removed\n\n- Dependency to `itertools` crate\n\n## [0.14.0] - 2021-02-22\n\n### Added\n\n- Addition of the Longformer language model, task-specific heads and registration in relevant pipelines\n\n### Changed\n\n- (BREAKING) Exposed additional settings for the Question Answering pipeline related to the maximum question, context\n  and answer length. This is not backward compatible if the question answering configuration was created without using\n  the `new` creator.\n- Simplified the Question answering pipeline to rely on the offsets calculated by the tokenizers instead of a manual\n  alignment. This results in moderate execution speed improvements for this pipeline.\n- Updated the padding strategy for the Question answering pipeline. While before all sequences were padded to a\n  fixed `max_length` (defaulting to 384), the padding is now done dynamically based on the length of the inputs. This\n  results in a significant speed improvement for this pipeline.\n\n### Fixed\n\n- Fixed a bug for Question Answering for models that were not based on Wordpiece tokenization (including BPE and unigram\n  based tokenizers). The issue was caused by the pre-tokenization step that was stripping the leading whitespace for all\n  tokens. The performance of these models for QA should improve significantly.\n\n## [0.13.0] - 2021-02-03\n\n### Added\n\n- Addition of the ProphetNet language model, task-specific heads and registration in relevant pipelines\n- (BREAKING) Implementation of [Diverse Beam Search](https://arxiv.org/abs/1610.02424). This allows the generation of\n  more diverse sequences within the number of beams. Addition of 2 new fields to the `GenerateConfig` that are\n  propagated through all text generation configs (e.g. `TranslationConfig`):\n    - `num_beam_groups` (`Option<i64>`), indicating the number of sub-beam groups. This must be a divisor of the number\n      of beams.\n    - `diversity_penalty` (`Option<f64>`), indicating by which amount to penalize common words between beam groups. This\n      will default to 5.5 if not provided. The impact of this diverse beam search is illustrated in the GPT2 integration\n      tests.\n\n### Changed\n\n- (BREAKING) Simplified the input and output of encoder/decoder models to avoid needing to take ownership of the\n  possibly cached encoder hidden state, offering a minor performance improvement for text generation tasks. The model\n  output field for encoder hidden states are now optional, and only returned if the encoder hidden states were not\n  provided for the given forward path. This may be a breaking change for low-level dependencies that manipulate directly\n  the encoder/decoder model outputs.\n- (BREAKING) Moved the language models implementation of the `PrivateLanguageGenerator` and `LanguageGenerator` traits (\n  needed to generate text) to the model modules, cleaning up the generation_utils module.\n- Updated download utilities crate, now leveraging Tokio 1.0 runtimes.\n\n### Fixed\n\n- Updated padding information and addition of position ids for batched GPT2 generation. Prior to this change, inputs\n  that required padding had a lower quality for the text generated.\n\n## [0.12.1] - 2021-01-04\n\n### Added\n\n- Addition of the MobileBERT language model, task-specific heads and registration in relevant pipelines\n\n### Changed\n\n- Made all model configurations `Clone`\n- Made several base modules of the BERT language model public, and added model output `Struct` for the new publicly\n  exposed, complex types\n\n## [0.12.0] - 2020-11-29\n\n### Added\n\n- Addition of the Reformer language model, task-specific heads and registration in relevant pipelines\n- Pre-trained models for DistilRoBERTa, used as a default for integration tests\n\n### Changed\n\n- Updated endpoint of the model resources reflecting changes to the Hugging Face's model hub\n- Early stopping turned by default on for translation and summarization\n\n## [0.11.0] - 2020-11-02\n\n### Added\n\n- Support for additional models for the conversational pipeline\n\n### Changed\n\n- Updated the version of Tokenizer crate with consistent visibility\n- (BREAKING) move of teh text generation pipeline to its owned pipeline. Shared generation utilities are moved\n  to `generation_utils`\n- All models, tokenizers and pipelines are now `Send`\n\n## [0.10.0] - 2020-10-04\n\n### Added\n\n- Benchmark scripts for all pipelines\n- Addition of the XLNet model and task-specific heads\n\n### Changed\n\n- (BREAKING) Changed the download method for resources now a method of the resource itself, and leveraging the\n  cached-path crate.\n- (BREAKING) Changed the return type of models to be output `Struct` instead of long tuples.\n- (BREAKING) Changed the naming of the model main modules from `modelname` to `model_modelname` to avoid confusion with\n  the top level module name\n- Extended the range of allowed types for pipelines input, allowing both owned `Vec` and slices, and both `String` and\n  sting slice.\n- Handling of all activations functions is mow made from a common module and `Struct`\n\n## [0.9.0] - 2020-09-06\n\n### Added\n\n- Zero-shot classification pipeline using a natural language inference model\n\n### Changed\n\n- (BREAKING) Updated version of tokenizers crate with added options for lower casing, accent stripping and prefix\n  addition\n- Updated BART classification model to allow running their `forward` method without being mutable.\n\n## [0.8.0] - 2020-08-25\n\n### Added\n\n- (BREAKING) Improved error handling via the addition of `RustBertError` and error propagation throughout the crate.\n\n### Changed\n\n- Updated version of tokenizers crate with improved error handling\n\n## [0.7.12] - 2020-08-12\n\n### Added\n\n- Addition of the reformer language model and its integration for language generation\n\n### Changed\n\n- Changed model resources endpoints to leverage updated Hugging Face's model hub\n- Updated the beam search processing to use vectorized operations\n\n## [0.7.11] - 2020-07-26\n\n### Changed\n\n- Generalization of the accepted input for several pipelines to accept both `Vec` and slices, and to accept\n  both `String` and `&str`\n\n## [0.7.10] - 2020-07-08\n\n### Added\n\n- Addition of the ALBERT language model and task-specific heads\n- Addition of German - English translation models\n- Addition of the T5 language model and integration in supported pipelines (translation and summarization)\n\n### Changed\n\n- Updated the modules throughout the crate to accept both owned and references to varstore paths.\n\n## [0.7.9] - 2020-06-28\n\n### Added\n\n- Addition of a multi-turn conversational pipeline based on DialoGPT.\n\n## [0.7.8] - 2020-06-23\n\n### Fixed\n\n- Code formatting using `rustfmt`\n\n## [0.7.7] - 2020-06-06\n\n### Changed\n\n- Removed the requirement for generation models to be mutable. Models are now all stateless, and no longer store an\n  internal cache (now provided as an input).\n- Updated BART model to take past layer states as an input instead of storing in internally.\n\n### Fixed\n\n- Fixed sequence classification model logits squeeze causing it to crash for batched inputs.\n\n## [0.7.6] - 2020-05-27\n\n### Added\n\n- Addition of translation between Russian and English\n\n### Fixed\n\n- Fixed a bug causing downloads to be incomplete, and removes the creation of a tokio runtime for the download of\n  resources.\n\n## [0.7.5] - 2020-05-25\n\n### Added\n\n- Addition of the Marian model, leveraging a shared language model implementation with the BART model.\n- Addition of translation capabilities. Supports translation between English and French, Spanish, Portuguese, Italian,\n  Catalan and German, and between German and French.\n\n## [0.7.4] - 2020-05-25\n\n### Added\n\n- Addition of multi-label classification capabilities for sequence classification via the `predict_mutilabel` function.\n\n## [0.7.3] - 2020-05-19\n\n### Added\n\n- Generalization of pipelines to allow leveraging multiple model architectures. Leveraging `Enum` unpacking,\n  introduces `ConfigOption`, `TokenizerOption` and pipeline-specific Options.\n- Addition of generic `SentenceClassificationModel` pipeline. The `SentimentModel` now leverages shared implementation\n  for sentence classification.\n- Addition of `TokenClassificationModel` pipeline. The `NERModel`now leverages shared implementation for token\n  classification.\n\n### Changed\n\n- Major rework of tokenization crate, alignment with updated API\n\n## [0.7.2] - 2020-05-03\n\n### Fixed\n\n- Minor bug fixes for tokenization\n\n## [0.7.1] - 2020-05-03\n\n### Added\n\n- Implementation of the Electra model (generator, discriminator, task-specific heads)\n- GPT2-medium and GPT2-large models\n\n## [0.7.0] - 2020-04-26\n\n### Added\n\n- Addition of Resources for handling file dependencies (e.g. vocabularies, model weights, configurations). Resources may\n  be `LocalResources` (pointing to a filesystem location) or `RemoteResources` (pointing to a remote endpoint). These\n  resources can be passed to a `download_resource` method that returns the location in the local filesystem for both\n  types of resources, downloading them if necessary.\n- Resources specifications for all existing architectures, pointing to model files hosted on Hugging Face's model hub.\n\n### Changed\n\n- (BREAKING) moved the resources' specification to the `GenerateConfig` for `GPT2Generator`.\n- (BREAKING) creation of pipeline configurations to contain the resources required to build the pipeline, used as an\n  input rather than paths to local files.\n- Updated the configuration for the number of target labels to use the `id2label` field instead of `num_labels` (\n  aligning with changes in standard configuration in the Transformers library). Removed `num_labels` from\n  configurations.\n- Made the `output_attentions`, `output_hidden_states` and `torchscript` fields for DistilBERT configuration optional\n- Fixed the device placement for sinusoidal embeddings for DistilBERT model.\n\n## [0.6.2] - 2020-04-07\n\n### Changed\n\n- Optimization of the BART model avoiding unnecessary tensor copies for cache manipulation and residual connections.\n- Optimization of DistilBERT model when embeddings are provided as an input\n\n## [0.6.1] - 2020-04-06\n\n### Changed\n\n- Minor optimizations to question answering and sentiment analysis pipelines\n- Addition of a cache reset for text generation routines\n- Implementation of cache reset for BART language model\n\n## [0.6.0] - 2020-04-05\n\n### Added\n\n- BART language model\n- Implementation of `LanguageModel` and `PrivateLanguageModel` for BART\n- Summarization capabilities\n- Tanh activation\n\n### Changed\n\n- (BREAKING) Moved the `LMHeadModel` Trait from GPT2 module to the pipelines module\n- Updated the `LMHeadModel` inputs to include `encoder_outputs` and `decoder_input_ids` to support causal language\n  model (e.g. BART)\n- (BREAKING) Added methods to the `PrivateLanguageGenerator` to support encoder-decoder models\n- (BREAKING) changed the type of `Generator` language model to require mutability (BART caching mechanism stores the\n  cache in the model requiring the entire model mutability - changed at a later point)\n- Optimization of the `get_banned_token` method\n\n### Fixed\n\n- Updated the device location of the token update when EOS is not allowed because the minimum sequence length was not\n  reached\n- No longer process a given beam hypothesis if it is marked as done\n- No longer add beams to a hypothesis if the rank is lower than the number of beams\n- Updated final beam update to skip completed hypotheses\n\n## [0.5.3] - 2020-03-27\n\n### Added\n\n- Documentation throughout the crate\n- Creation of a `GenerateConfig` configuration structure to hold generation options\n\n### Changed\n\n- Visibility of low-level utilities in the crate\n- Updated the generation options to be passed at the text generation model instantiation, rather than at every call to\n  the `generate` method\n- Updated visibility of generation routines into a public API and private lower level methods\n\n## [0.5.2] - 2020-03-17\n\n### Changed\n\n- Text generation now takes a `Option<Vec<&str>>` instead of a `Option<&str>`. Shorter sequences are left-padded\n  with `pad` if available, otherwise with `eos`.\n- Turned-off gradient calculations for generation process\n\n## [0.5.1] - 2020-03-16\n\n### Fixed\n\n- Beam search completion validation\n- Padding sequence for sentences shorter than the maximum length moved to correct device\n\n## [0.5.0] - 2020-03-16\n\n### Added\n\n- DistilGPT2 pretrained weights for GPT2\n- `LMHeadModel` trait for model supporting text generation, offering an interface between the model specific\n  input/output, and the generic set of inputs/outputs expected for model supporting text generation\n- Implementation of `LMHeadModel` for GPT2 and GPT\n- Text generation pipeline, supporting beam search, top-k/top-p decoding, repeated tokens banning, repetition and length\n  penalties as `LanguageGenerator` Trait\n- Implementation of `LanguageGenerator` for GPT and GPT2\n- Examples and tests for language generation\n\n### Fixed\n\n- Fixed concatenation dimension for GPT2 past\n\n## [0.4.5] - 2020-03-07\n\n### Changed\n\n- Updated input type for `QuestionAnsweringModel`'s `predict` to be `&[QaInput]` instead of a pair of question and\n  context strings. QuestionAnsweringModel now works with a list of inputs and returns a list of predictions, processing\n  inputs as batches.\n\n## [0.4.4] - 2020-03-01\n\n### Added\n\n- Swish and gelu_new activation functions\n- GPT2 language model\n- GPT language model\n\n## [0.4.3] - 2020-02-25\n\n### Added\n\n- Addition of a NER pipeline\n- Addition of a QuestionAnswering pipeline\n\n### Changed\n\n- Moved `SentimentClassifier` from DistilBERT module to the newly created pipelines\n- Changed precision of id to label mapping of BERT config from `i32` to `i64`\n- Simplified calculation of sinusoidal embeddings for DistilBERT\n\n## [0.4.1] - 2020-02-21\n\n### Added\n\n- Addition of RoBERTa language model\n- Addition of `BertEmbedding` trait for BERT-like models\n\n### Changed\n\n- Updated `BertEmbeddings` to implement the newly created `BertEmbedding` Trait\n- Updated `BertModel`'s embeddings to be of type `impl BertEmbedding` rather than specific embeddings, allowing to\n  re-use the BERT structure for other models, only replacing the embeddings layer.\n\n### Fixed\n\n- Fixed the variable path for BERT models with task-specific heads to allow loading a snapshot from models trained on\n  Transformers.\n\n## [0.4.0] - 2020-02-18\n\n### Added\n\n- BERT Model and examples\n- Addition of `DistilBertForTokenClassification` and `DistilBertForQuestionAnswering` model heads\n- Collection of activation functions (gelu, relu, mish)\n- Dropout module\n- Custom Linear layer, allowing a creation without bias\n- Config trait allowing to deserialize from `json` files\n\n### Changed\n\n- (BREAKING) Updated `DistilBertConfig` to use the newly created `Config` Trait\n\n## [0.3.1] - 2020-02-16\n\n### Added\n\n- Integration tests\n\n### Changed\n\n- Migrated from `rust_transformers` v0.2.0 (deprecated) to `rust_tokenizers v1.0.0\n\n## [0.3.0] - 2020-02-13\n\n### Added\n\n- Example for DistilBERT masked language modeling\n- Download utilities script for DistilBERT (base and SST2)\n\n### Changed\n\n- made `label2id`, `id2label`, `is_decoder`, `output_past` and `use_bfloat` configuration fields optional for\n  DistilBertConfig\n\n## [0.2.0] - 2020-02-11\n\n### Initial release\n\n- Tensor conversion tools from Pytorch to Libtorch format\n- DistilBERT model architecture\n- Ready-to-use `SentimentClassifier` using a DistilBERT model fine-tuned on SST2\n"
        },
        {
          "name": "CONTRIBUTING.md",
          "type": "blob",
          "size": 2.6787109375,
          "content": "# How to contribute to rust-bert?\n\nCode contributions to the library are very welcome, especially for areas of focus given below. \nHowever, please note that direct contributions to the `rust-bert` library are not the only way you can help the project.\nBuilding applications and supporting the applications built on top of the library, sharing\nthe word about them in the Rust and NLP community or simply a star go a long way in helping this project develop.\n\n## Code contributions areas of focus\n\nRust is a very efficient, safe and fast language when implemented in the right way.\nThe field of transformers in NLP and especially their implementation in Rust is a rather new development.\nConsidering this, the general execution speed of pipelines built using `rust-bert` is a priority for the project (along with the correctness of the results).\n\nContributions are therefore welcome on the following areas:\n- Improvement of execution performance at a module, model or pipeline level\n- Reduction of memory footprint for the models\n\nFor other areas of contributions, opening an issue to discuss the feature addition would be very welcome. \nAs this started out as a personal project, it would be great to coordinate as this may be something that is already in the implementation pipeline.\n\n## General contribution guidelines\n\n- Please try running the suite of integration tests locally before submitting a pull request. Most features are tested automatically in the Travis CI - but due to the large size of some models some tests cannot be run in the virtual machines provided.\n- The code should be formatted using `cargo +nightly fmt` to format both the code and the documentation\n- As much as possible, please try to adhere to the coding style of the crate. I am open to discuss non-idiomatic code.\n- When providing a performance improvement, please provide benchmarks to illustrate the performance gain, if possible with and without GPU support.\n- Please try to ensure that the documentation always reflects the actual state of the code.\n\n## Did you find a bug?\n\nThank you - identifying and sharing bugs is one of the best way to improve the overall quality of the crate!\nWhen submitting the bug as an issue, it would be very helpful if you could share the full stack trace of the error, and the input provided to reproduce the error.\nSince several models are non deterministic (generation pipelines are using random sampling to generate text), it would be very useful to manually turn off sampling (`do_sample: false` in the relevant configuration) and reproduce the error with a given input.\n\n\nThis guide was inspired by the original [Transformers guide to contributing](https://github.com/huggingface/transformers/blob/master/CONTRIBUTING.md)"
        },
        {
          "name": "Cargo.toml",
          "type": "blob",
          "size": 3.025390625,
          "content": "[package]\nname = \"rust-bert\"\nversion = \"0.23.0\"\nauthors = [\"Guillaume Becquin <guillaume.becquin@gmail.com>\"]\nedition = \"2018\"\ndescription = \"Ready-to-use NLP pipelines and language models\"\nrepository = \"https://github.com/guillaume-be/rust-bert\"\ndocumentation = \"https://docs.rs/rust-bert\"\nlicense = \"Apache-2.0\"\nreadme = \"README.md\"\nbuild = \"build.rs\"\nkeywords = [\n    \"nlp\",\n    \"deep-learning\",\n    \"machine-learning\",\n    \"transformers\",\n    \"translation\",\n]\n\n# See more keys and their definitions at https://doc.rust-lang.org/cargo/reference/manifest.html\n\n[lib]\nname = \"rust_bert\"\npath = \"src/lib.rs\"\ncrate-type = [\"lib\"]\n\n[[bin]]\nname = \"convert-tensor\"\npath = \"src/convert-tensor.rs\"\ndoc = false\n\n[[bench]]\nname = \"sst2_benchmark\"\nharness = false\n\n[[bench]]\nname = \"squad_benchmark\"\nharness = false\n\n[[bench]]\nname = \"summarization_benchmark\"\nharness = false\n\n[[bench]]\nname = \"translation_benchmark\"\nharness = false\n\n[[bench]]\nname = \"generation_benchmark\"\nharness = false\n\n[[bench]]\nname = \"tensor_operations_benchmark\"\nharness = false\n\n[[bench]]\nname = \"token_classification_benchmark\"\nharness = false\n\n[profile.bench]\nopt-level = 3\n\n[features]\ndefault = [\"remote\", \"default-tls\"]\ndoc-only = [\"tch/doc-only\"]\nall-tests = []\nremote = [\"cached-path\", \"dirs\", \"lazy_static\"]\ndownload-libtorch = [\"tch/download-libtorch\"]\nonnx = [\"ort\", \"ndarray\"]\nrustls-tls = [\"cached-path/rustls-tls\"]\ndefault-tls = [\"cached-path/default-tls\"]\nhf-tokenizers = [\"tokenizers\"]\n\n[package.metadata.docs.rs]\nfeatures = [\"doc-only\"]\n\n[dependencies]\nrust_tokenizers = \"8.1.1\"\ntch = { version = \"0.17.0\", features = [\"download-libtorch\"] }\nserde_json = \"1\"\nserde = { version = \"1\", features = [\"derive\"] }\nordered-float = \"4.2.0\"\nuuid = { version = \"1\", features = [\"v4\"] }\nthiserror = \"1\"\nhalf = \"2\"\nregex = \"1.10\"\n\ncached-path = { version = \"0.6\", default-features = false, optional = true }\ndirs = { version = \"5\", optional = true }\nlazy_static = { version = \"1\", optional = true }\nort = { version = \"1.16.3\", optional = true, default-features = false, features = [\n    \"half\",\n] }\nndarray = { version = \"0.15\", optional = true }\ntokenizers = { version = \"0.20\", optional = true, default-features = false, features = [\n    \"onig\",\n] }\n\n[dev-dependencies]\nanyhow = \"1\"\ncsv = \"1\"\ncriterion = \"0.5\"\ntokio = { version = \"1.35\", features = [\"sync\", \"rt-multi-thread\", \"macros\"] }\ntempfile = \"3\"\nitertools = \"0.13.0\"\ntracing-subscriber = { version = \"0.3\", default-features = false, features = [\n    \"env-filter\",\n    \"fmt\",\n] }\nort = { version = \"1.16.3\", features = [\"load-dynamic\"] }\n\n[[example]]\nname = \"onnx-masked-lm\"\nrequired-features = [\"onnx\"]\n\n[[example]]\nname = \"onnx-question-answering\"\nrequired-features = [\"onnx\"]\n\n[[example]]\nname = \"onnx-sequence-classification\"\nrequired-features = [\"onnx\"]\n\n[[example]]\nname = \"onnx-text-generation\"\nrequired-features = [\"onnx\"]\n\n[[example]]\nname = \"onnx-token-classification\"\nrequired-features = [\"onnx\"]\n\n[[example]]\nname = \"onnx-translation\"\nrequired-features = [\"onnx\"]\n\n[[example]]\nname = \"generation_gpt2_hf_tokenizers\"\nrequired-features = [\"hf-tokenizers\"]\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 11.0908203125,
          "content": "                                 Apache License\n                           Version 2.0, January 2004\n                        http://www.apache.org/licenses/\n\n   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n\n   1. Definitions.\n\n      \"License\" shall mean the terms and conditions for use, reproduction,\n      and distribution as defined by Sections 1 through 9 of this document.\n\n      \"Licensor\" shall mean the copyright owner or entity authorized by\n      the copyright owner that is granting the License.\n\n      \"Legal Entity\" shall mean the union of the acting entity and all\n      other entities that control, are controlled by, or are under common\n      control with that entity. For the purposes of this definition,\n      \"control\" means (i) the power, direct or indirect, to cause the\n      direction or management of such entity, whether by contract or\n      otherwise, or (ii) ownership of fifty percent (50%) or more of the\n      outstanding shares, or (iii) beneficial ownership of such entity.\n\n      \"You\" (or \"Your\") shall mean an individual or Legal Entity\n      exercising permissions granted by this License.\n\n      \"Source\" form shall mean the preferred form for making modifications,\n      including but not limited to software source code, documentation\n      source, and configuration files.\n\n      \"Object\" form shall mean any form resulting from mechanical\n      transformation or translation of a Source form, including but\n      not limited to compiled object code, generated documentation,\n      and conversions to other media types.\n\n      \"Work\" shall mean the work of authorship, whether in Source or\n      Object form, made available under the License, as indicated by a\n      copyright notice that is included in or attached to the work\n      (an example is provided in the Appendix below).\n\n      \"Derivative Works\" shall mean any work, whether in Source or Object\n      form, that is based on (or derived from) the Work and for which the\n      editorial revisions, annotations, elaborations, or other modifications\n      represent, as a whole, an original work of authorship. For the purposes\n      of this License, Derivative Works shall not include works that remain\n      separable from, or merely link (or bind by name) to the interfaces of,\n      the Work and Derivative Works thereof.\n\n      \"Contribution\" shall mean any work of authorship, including\n      the original version of the Work and any modifications or additions\n      to that Work or Derivative Works thereof, that is intentionally\n      submitted to Licensor for inclusion in the Work by the copyright owner\n      or by an individual or Legal Entity authorized to submit on behalf of\n      the copyright owner. For the purposes of this definition, \"submitted\"\n      means any form of electronic, verbal, or written communication sent\n      to the Licensor or its representatives, including but not limited to\n      communication on electronic mailing lists, source code control systems,\n      and issue tracking systems that are managed by, or on behalf of, the\n      Licensor for the purpose of discussing and improving the Work, but\n      excluding communication that is conspicuously marked or otherwise\n      designated in writing by the copyright owner as \"Not a Contribution.\"\n\n      \"Contributor\" shall mean Licensor and any individual or Legal Entity\n      on behalf of whom a Contribution has been received by Licensor and\n      subsequently incorporated within the Work.\n\n   2. Grant of Copyright License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      copyright license to reproduce, prepare Derivative Works of,\n      publicly display, publicly perform, sublicense, and distribute the\n      Work and such Derivative Works in Source or Object form.\n\n   3. Grant of Patent License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      (except as stated in this section) patent license to make, have made,\n      use, offer to sell, sell, import, and otherwise transfer the Work,\n      where such license applies only to those patent claims licensable\n      by such Contributor that are necessarily infringed by their\n      Contribution(s) alone or by combination of their Contribution(s)\n      with the Work to which such Contribution(s) was submitted. If You\n      institute patent litigation against any entity (including a\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\n      or a Contribution incorporated within the Work constitutes direct\n      or contributory patent infringement, then any patent licenses\n      granted to You under this License for that Work shall terminate\n      as of the date such litigation is filed.\n\n   4. Redistribution. You may reproduce and distribute copies of the\n      Work or Derivative Works thereof in any medium, with or without\n      modifications, and in Source or Object form, provided that You\n      meet the following conditions:\n\n      (a) You must give any other recipients of the Work or\n          Derivative Works a copy of this License; and\n\n      (b) You must cause any modified files to carry prominent notices\n          stating that You changed the files; and\n\n      (c) You must retain, in the Source form of any Derivative Works\n          that You distribute, all copyright, patent, trademark, and\n          attribution notices from the Source form of the Work,\n          excluding those notices that do not pertain to any part of\n          the Derivative Works; and\n\n      (d) If the Work includes a \"NOTICE\" text file as part of its\n          distribution, then any Derivative Works that You distribute must\n          include a readable copy of the attribution notices contained\n          within such NOTICE file, excluding those notices that do not\n          pertain to any part of the Derivative Works, in at least one\n          of the following places: within a NOTICE text file distributed\n          as part of the Derivative Works; within the Source form or\n          documentation, if provided along with the Derivative Works; or,\n          within a display generated by the Derivative Works, if and\n          wherever such third-party notices normally appear. The contents\n          of the NOTICE file are for informational purposes only and\n          do not modify the License. You may add Your own attribution\n          notices within Derivative Works that You distribute, alongside\n          or as an addendum to the NOTICE text from the Work, provided\n          that such additional attribution notices cannot be construed\n          as modifying the License.\n\n      You may add Your own copyright statement to Your modifications and\n      may provide additional or different license terms and conditions\n      for use, reproduction, or distribution of Your modifications, or\n      for any such Derivative Works as a whole, provided Your use,\n      reproduction, and distribution of the Work otherwise complies with\n      the conditions stated in this License.\n\n   5. Submission of Contributions. Unless You explicitly state otherwise,\n      any Contribution intentionally submitted for inclusion in the Work\n      by You to the Licensor shall be under the terms and conditions of\n      this License, without any additional terms or conditions.\n      Notwithstanding the above, nothing herein shall supersede or modify\n      the terms of any separate license agreement you may have executed\n      with Licensor regarding such Contributions.\n\n   6. Trademarks. This License does not grant permission to use the trade\n      names, trademarks, service marks, or product names of the Licensor,\n      except as required for reasonable and customary use in describing the\n      origin of the Work and reproducing the content of the NOTICE file.\n\n   7. Disclaimer of Warranty. Unless required by applicable law or\n      agreed to in writing, Licensor provides the Work (and each\n      Contributor provides its Contributions) on an \"AS IS\" BASIS,\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n      implied, including, without limitation, any warranties or conditions\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n      PARTICULAR PURPOSE. You are solely responsible for determining the\n      appropriateness of using or redistributing the Work and assume any\n      risks associated with Your exercise of permissions under this License.\n\n   8. Limitation of Liability. In no event and under no legal theory,\n      whether in tort (including negligence), contract, or otherwise,\n      unless required by applicable law (such as deliberate and grossly\n      negligent acts) or agreed to in writing, shall any Contributor be\n      liable to You for damages, including any direct, indirect, special,\n      incidental, or consequential damages of any character arising as a\n      result of this License or out of the use or inability to use the\n      Work (including but not limited to damages for loss of goodwill,\n      work stoppage, computer failure or malfunction, or any and all\n      other commercial damages or losses), even if such Contributor\n      has been advised of the possibility of such damages.\n\n   9. Accepting Warranty or Additional Liability. While redistributing\n      the Work or Derivative Works thereof, You may choose to offer,\n      and charge a fee for, acceptance of support, warranty, indemnity,\n      or other liability obligations and/or rights consistent with this\n      License. However, in accepting such obligations, You may act only\n      on Your own behalf and on Your sole responsibility, not on behalf\n      of any other Contributor, and only if You agree to indemnify,\n      defend, and hold each Contributor harmless for any liability\n      incurred by, or claims asserted against, such Contributor by reason\n      of your accepting any such warranty or additional liability.\n\n   END OF TERMS AND CONDITIONS\n\n   APPENDIX: How to apply the Apache License to your work.\n\n      To apply the Apache License to your work, attach the following\n      boilerplate notice, with the fields enclosed by brackets \"[]\"\n      replaced with your own identifying information. (Don't include\n      the brackets!)  The text should be enclosed in the appropriate\n      comment syntax for the file format. We also recommend that a\n      file or class name and description of purpose be included on the\n      same \"printed page\" as the copyright notice for easier\n      identification within third-party archives.\n\n   Copyright [yyyy] [name of copyright owner]\n\n   Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 30.548828125,
          "content": "# rust-bert\n\n[![Build Status](https://github.com/guillaume-be/rust-bert/workflows/Build/badge.svg?event=push)](https://github.com/guillaume-be/rust-bert/actions)\n[![Latest version](https://img.shields.io/crates/v/rust_bert.svg)](https://crates.io/crates/rust_bert)\n[![Documentation](https://docs.rs/rust-bert/badge.svg)](https://docs.rs/rust-bert)\n![License](https://img.shields.io/crates/l/rust_bert.svg)\n\nRust-native state-of-the-art Natural Language Processing models and pipelines.\nPort of Hugging Face's\n[Transformers library](https://github.com/huggingface/transformers), using\n[tch-rs](https://github.com/LaurentMazare/tch-rs) or\n[onnxruntime bindings](https://github.com/pykeio/ort) and pre-processing from\n[rust-tokenizers](https://github.com/guillaume-be/rust-tokenizers). Supports\nmulti-threaded tokenization and GPU inference. This repository exposes the model\nbase architecture, task-specific heads (see below) and\n[ready-to-use pipelines](#ready-to-use-pipelines). [Benchmarks](#benchmarks) are\navailable at the end of this document.\n\nGet started with tasks including question answering, named entity recognition,\ntranslation, summarization, text generation, conversational agents and more in\njust a few lines of code:\n\n```rust\n    let qa_model = QuestionAnsweringModel::new(Default::default ()) ?;\n\nlet question = String::from(\"Where does Amy live ?\");\nlet context = String::from(\"Amy lives in Amsterdam\");\n\nlet answers = qa_model.predict( & [QaInput { question, context }], 1, 32);\n```\n\nOutput:\n\n```\n[Answer { score: 0.9976, start: 13, end: 21, answer: \"Amsterdam\" }]\n```\n\nThe tasks currently supported include:\n\n- Translation\n- Summarization\n- Multi-turn dialogue\n- Zero-shot classification\n- Sentiment Analysis\n- Named Entity Recognition\n- Part of Speech tagging\n- Question-Answering\n- Language Generation\n- Masked Language Model\n- Sentence Embeddings\n- Keywords extraction\n\n<details>\n<summary> <b>Expand to display the supported models/tasks matrix </b> </summary>\n\n|              | **Sequence classification** | **Token classification** | **Question answering** | **Text Generation** | **Summarization** | **Translation** | **Masked LM** | **Sentence Embeddings** |\n|:------------:|:---------------------------:|:------------------------:|:----------------------:|:-------------------:|:-----------------:|:---------------:|:-------------:|:-----------------------:|\n|  DistilBERT  |                            |                         |                       |                     |                   |                 |              |                        |\n|  MobileBERT  |                            |                         |                       |                     |                   |                 |              |                         |\n|   DeBERTa    |                            |                         |                       |                     |                   |                 |              |                         |\n| DeBERTa (v2) |                            |                         |                       |                     |                   |                 |              |                         |\n|     FNet     |                            |                         |                       |                     |                   |                 |              |                         |\n|     BERT     |                            |                         |                       |                     |                   |                 |              |                        |\n|   RoBERTa    |                            |                         |                       |                     |                   |                 |              |                        |\n|     GPT      |                             |                          |                        |                    |                   |                 |               |                         |\n|     GPT2     |                             |                          |                        |                    |                   |                 |               |                         |\n|   GPT-Neo    |                             |                          |                        |                    |                   |                 |               |                         |\n|    GPT-J     |                             |                          |                        |                    |                   |                 |               |                         |\n|     BART     |                            |                          |                        |                    |                  |                 |               |                         |\n|    Marian    |                             |                          |                        |                     |                   |                |               |                         |\n|    MBart     |                            |                          |                        |                    |                   |                 |               |                         |\n|    M2M100    |                             |                          |                        |                    |                   |                 |               |                         |\n|     NLLB     |                             |                          |                        |                    |                   |                 |               |                         |\n|   Electra    |                             |                         |                        |                     |                   |                 |              |                         |\n|    ALBERT    |                            |                         |                       |                     |                   |                 |              |                        |\n|      T5      |                             |                          |                        |                    |                  |                |               |                        |\n|    LongT5    |                             |                          |                        |                    |                  |                 |               |                         |\n|    XLNet     |                            |                         |                       |                    |                   |                 |              |                         |\n|   Reformer   |                            |                          |                       |                    |                   |                 |              |                         |\n|  ProphetNet  |                             |                          |                        |                    |                  |                 |               |                         |\n|  Longformer  |                            |                         |                       |                     |                   |                 |              |                         |\n|   Pegasus    |                             |                          |                        |                     |                  |                 |               |                         |\n\n</details>\n\n## Getting started\n\nThis library relies on the [tch](https://github.com/LaurentMazare/tch-rs) crate\nfor bindings to the C++ Libtorch API. The libtorch library is required can be\ndownloaded either automatically or manually. The following provides a reference\non how to set-up your environment to use these bindings, please refer to the\n[tch](https://github.com/LaurentMazare/tch-rs) for detailed information or\nsupport.\n\nFurthermore, this library relies on a cache folder for downloading pre-trained\nmodels. This cache location defaults to `~/.cache/.rustbert`, but can be changed\nby setting the `RUSTBERT_CACHE` environment variable. Note that the language\nmodels used by this library are in the order of the 100s of MBs to GBs.\n\n### Manual installation (recommended)\n\n1. Download `libtorch` from https://pytorch.org/get-started/locally/. This\n   package requires `v2.4`: if this version is no longer available on the \"get\n   started\" page, the file should be accessible by modifying the target link,\n   for example\n   `https://download.pytorch.org/libtorch/cu124/libtorch-cxx11-abi-shared-with-deps-2.4.0%2Bcu124.zip`\n   for a Linux version with CUDA12. **NOTE:** When using `rust-bert` as\n   dependency from [crates.io](https://crates.io), please check the required\n   `LIBTORCH` on the published package\n   [readme](https://crates.io/crates/rust-bert) as it may differ from the\n   version documented here (applying to the current repository version).\n2. Extract the library to a location of your choice\n3. Set the following environment variables\n\n##### Linux:\n\n```bash\nexport LIBTORCH=/path/to/libtorch\nexport LD_LIBRARY_PATH=${LIBTORCH}/lib:$LD_LIBRARY_PATH\n```\n\n##### Windows\n\n```powershell\n$Env:LIBTORCH = \"X:\\path\\to\\libtorch\"\n$Env:Path += \";X:\\path\\to\\libtorch\\lib\"\n```\n\n#### macOS + Homebrew\n\n```bash\nbrew install pytorch jq\nexport LIBTORCH=$(brew --cellar pytorch)/$(brew info --json pytorch | jq -r '.[0].installed[0].version')\nexport LD_LIBRARY_PATH=${LIBTORCH}/lib:$LD_LIBRARY_PATH\n```\n\n### Automatic installation\n\nAlternatively, you can let the `build` script automatically download the\n`libtorch` library for you. The `download-libtorch` feature flag needs to be\nenabled. The CPU version of libtorch will be downloaded by default. To download\na CUDA version, please set the environment variable `TORCH_CUDA_VERSION` to\n`cu124`. Note that the libtorch library is large (order of several GBs for the\nCUDA-enabled version) and the first build may therefore take several minutes to\ncomplete.\n\n### Verifying installation\n\nVerify your installation (and linking with libtorch) by adding the `rust-bert`\ndependency to your `Cargo.toml` or by cloning the rust-bert source and running\nan example:\n\n```bash\ngit clone git@github.com:guillaume-be/rust-bert.git\ncd rust-bert\ncargo run --example sentence_embeddings\n```\n\n## ONNX Support (Optional)\n\nONNX support can be enabled via the optional `onnx` feature. This crate then\nleverages the [ort](https://github.com/pykeio/ort) crate with bindings to the\nonnxruntime C++ library. We refer the user to this page project for further\ninstallation instructions/support.\n\n1. Enable the optional `onnx` feature. The `rust-bert` crate does not include\n   any optional dependencies for `ort`, the end user should select the set of\n   features that would be adequate for pulling the required `onnxruntime` C++\n   library.\n2. The current recommended installation is to use dynamic linking by pointing to\n   an existing library location. Use the `load-dynamic` cargo feature for `ort`.\n3. set the `ORT_DYLIB_PATH` to point to the location of downloaded onnxruntime\n   library (`onnxruntime.dll`/`libonnxruntime.so`/`libonnxruntime.dylib`\n   depending on the operating system). These can be downloaded from the\n   [release page](https://github.com/microsoft/onnxruntime/releases) of the\n   onnxruntime project\n\nMost architectures (including encoders, decoders and encoder-decoders) are\nsupported. the library aims at keeping compatibility with models exported using\nthe [Optimum](https://github.com/huggingface/optimum) library. A detailed guide\non how to export a Transformer model to ONNX using Optimum is available at\nhttps://huggingface.co/docs/optimum/main/en/exporters/onnx/usage_guides/export_a_model\nThe resources used to create ONNX models are similar to those based on Pytorch,\nreplacing the pytorch by the ONNX model. Since ONNX models are less flexible\nthan their Pytorch counterparts in the handling of optional arguments, exporting\na decoder or encoder-decoder model to ONNX will usually result in multiple\nfiles. These files are expected (but not all are necessary) for use in this\nlibrary as per the table below:\n\n| Architecture                | Encoder file | Decoder without past file | Decoder with past file |\n|-----------------------------|--------------|---------------------------|------------------------|\n| Encoder (e.g. BERT)         | required     | not used                  | not used               |\n| Decoder (e.g. GPT2)         | not used     | required                  | optional               |\n| Encoder-decoder (e.g. BART) | required     | required                  | optional               |\n\nNote that the computational efficiency will drop when the `decoder with past`\nfile is optional but not provided since the model will not used cached past keys\nand values for the attention mechanism, leading to a high number of redundant\ncomputations. The Optimum library offers export options to ensure such a\n`decoder with past` model file is created. The base encoder and decoder model\narchitecture are available (and exposed for convenience) in the `encoder` and\n`decoder` modules, respectively.\n\nGeneration models (pure decoder or encoder/decoder architectures) are available\nin the `models` module. ost pipelines are available for ONNX model checkpoints,\nincluding sequence classification, zero-shot classification, token\nclassification (including named entity recognition and part-of-speech tagging),\nquestion answering, text generation, summarization and translation. These models\nuse the same configuration and tokenizer files as their Pytorch counterparts\nwhen used in a pipeline. Examples leveraging ONNX models are given in the\n`./examples` directory\n\n## Ready-to-use pipelines\n\nBased on Hugging Face's pipelines, ready to use end-to-end NLP pipelines are\navailable as part of this crate. The following capabilities are currently\navailable:\n\n**Disclaimer** The contributors of this repository are not responsible for any\ngeneration from the 3rd party utilization of the pretrained systems proposed\nherein.\n\n<details>\n<summary> <b>1. Question Answering</b> </summary>\n\nExtractive question answering from a given question and context. DistilBERT\nmodel fine-tuned on SQuAD (Stanford Question Answering Dataset)\n\n```rust\n    let qa_model = QuestionAnsweringModel::new(Default::default ()) ?;\n\nlet question = String::from(\"Where does Amy live ?\");\nlet context = String::from(\"Amy lives in Amsterdam\");\n\nlet answers = qa_model.predict( & [QaInput { question, context }], 1, 32);\n```\n\nOutput:\n\n```\n[Answer { score: 0.9976, start: 13, end: 21, answer: \"Amsterdam\" }]\n```\n\n</details>\n&nbsp;\n<details>\n<summary> <b>2. Translation </b> </summary>\n\nTranslation pipeline supporting a broad range of source and target languages.\nLeverages two main architectures for translation tasks:\n\n- Marian-based models, for specific source/target combinations\n- M2M100 models allowing for direct translation between 100 languages (at a\n  higher computational cost and lower performance for some selected languages)\n\nMarian-based pretrained models for the following language pairs are readily\navailable in the library - but the user can import any Pytorch-based model for\npredictions\n\n- English <-> French\n- English <-> Spanish\n- English <-> Portuguese\n- English <-> Italian\n- English <-> Catalan\n- English <-> German\n- English <-> Russian\n- English <-> Chinese\n- English <-> Dutch\n- English <-> Swedish\n- English <-> Arabic\n- English <-> Hebrew\n- English <-> Hindi\n- French <-> German\n\nFor languages not supported by the proposed pretrained Marian models, the user\ncan leverage a M2M100 model supporting direct translation between 100 languages\n(without intermediate English translation) The full list of supported languages\nis available in the\n[crate documentation](https://docs.rs/rust-bert/latest/rust_bert/pipelines/translation/enum.Language.html)\n\n```rust\nuse rust_bert::pipelines::translation::{Language, TranslationModelBuilder};\nfn main() -> anyhow::Result<()> {\n    let model = TranslationModelBuilder::new()\n        .with_source_languages(vec![Language::English])\n        .with_target_languages(vec![Language::Spanish, Language::French, Language::Italian])\n        .create_model()?;\n    let input_text = \"This is a sentence to be translated\";\n    let output = model.translate(&[input_text], None, Language::French)?;\n    for sentence in output {\n        println!(\"{}\", sentence);\n    }\n    Ok(())\n}\n```\n\nOutput:\n\n```\nIl s'agit d'une phrase  traduire\n```\n\n</details>\n&nbsp;\n<details>\n<summary> <b>3. Summarization </b> </summary>\n\nAbstractive summarization using a pretrained BART model.\n\n```rust\n    let summarization_model = SummarizationModel::new(Default::default ()) ?;\n\nlet input = [\"In findings published Tuesday in Cornell University's arXiv by a team of scientists \\\nfrom the University of Montreal and a separate report published Wednesday in Nature Astronomy by a team \\\nfrom University College London (UCL), the presence of water vapour was confirmed in the atmosphere of K2-18b, \\\na planet circling a star in the constellation Leo. This is the first such discovery in a planet in its star's \\\nhabitable zone  not too hot and not too cold for liquid water to exist. The Montreal team, led by Bjrn Benneke, \\\nused data from the NASA's Hubble telescope to assess changes in the light coming from K2-18b's star as the planet \\\npassed between it and Earth. They found that certain wavelengths of light, which are usually absorbed by water, \\\nweakened when the planet was in the way, indicating not only does K2-18b have an atmosphere, but the atmosphere \\\ncontains water in vapour form. The team from UCL then analyzed the Montreal team's data using their own software \\\nand confirmed their conclusion. This was not the first time scientists have found signs of water on an exoplanet, \\\nbut previous discoveries were made on planets with high temperatures or other pronounced differences from Earth. \\\n\\\"This is the first potentially habitable planet where the temperature is right and where we now know there is water,\\\" \\\nsaid UCL astronomer Angelos Tsiaras. \\\"It's the best candidate for habitability right now.\\\" \\\"It's a good sign\\\", \\\nsaid Ryan Cloutier of the HarvardSmithsonian Center for Astrophysics, who was not one of either study's authors. \\\n\\\"Overall,\\\" he continued, \\\"the presence of water in its atmosphere certainly improves the prospect of K2-18b being \\\na potentially habitable planet, but further observations will be required to say for sure. \\\"\nK2-18b was first identified in 2015 by the Kepler space telescope. It is about 110 light-years from Earth and larger \\\nbut less dense. Its star, a red dwarf, is cooler than the Sun, but the planet's orbit is much closer, such that a year \\\non K2-18b lasts 33 Earth days. According to The Guardian, astronomers were optimistic that NASA's James Webb space \\\ntelescope  scheduled for launch in 2021  and the European Space Agency's 2028 ARIEL program, could reveal more \\\nabout exoplanets like K2-18b.\"];\n\nlet output = summarization_model.summarize( & input);\n```\n\n(example from:\n[WikiNews](https://en.wikinews.org/wiki/Astronomers_find_water_vapour_in_atmosphere_of_exoplanet_K2-18b))\n\nOutput:\n\n```\n\"Scientists have found water vapour on K2-18b, a planet 110 light-years from Earth. \nThis is the first such discovery in a planet in its star's habitable zone. \nThe planet is not too hot and not too cold for liquid water to exist.\"\n```\n\n</details>\n&nbsp;\n<details>\n<summary> <b>4. Dialogue Model </b> </summary>\n\nConversation model based on Microsoft's\n[DialoGPT](https://github.com/microsoft/DialoGPT). This pipeline allows the\ngeneration of single or multi-turn conversations between a human and a model.\nThe DialoGPT's page states that\n\n> The human evaluation results indicate that the response generated from\n> DialoGPT is comparable to human response quality under a single-turn\n> conversation Turing test.\n> ([DialoGPT repository](https://github.com/microsoft/DialoGPT))\n\nThe model uses a `ConversationManager` to keep track of active conversations and\ngenerate responses to them.\n\n```rust\nuse rust_bert::pipelines::conversation::{ConversationModel, ConversationManager};\n\nlet conversation_model = ConversationModel::new(Default::default ());\nlet mut conversation_manager = ConversationManager::new();\n\nlet conversation_id = conversation_manager.create(\"Going to the movies tonight - any suggestions?\");\nlet output = conversation_model.generate_responses( & mut conversation_manager);\n```\n\nExample output:\n\n```\n\"The Big Lebowski.\"\n```\n\n</details>\n&nbsp;\n<details>\n<summary> <b>5. Natural Language Generation </b> </summary>\n\nGenerate language based on a prompt. GPT2 and GPT available as base models.\nInclude techniques such as beam search, top-k and nucleus sampling, temperature\nsetting and repetition penalty. Supports batch generation of sentences from\nseveral prompts. Sequences will be left-padded with the model's padding token if\npresent, the unknown token otherwise. This may impact the results, it is\nrecommended to submit prompts of similar length for best results\n\n```rust\n    let model = GPT2Generator::new(Default::default ()) ?;\n\nlet input_context_1 = \"The dog\";\nlet input_context_2 = \"The cat was\";\n\nlet generate_options = GenerateOptions {\nmax_length: 30,\n..Default::default ()\n};\n\nlet output = model.generate(Some( & [input_context_1, input_context_2]), generate_options);\n```\n\nExample output:\n\n```\n[\n    \"The dog's owners, however, did not want to be named. According to the lawsuit, the animal's owner, a 29-year\"\n    \"The dog has always been part of the family. \\\"He was always going to be my dog and he was always looking out for me\"\n    \"The dog has been able to stay in the home for more than three months now. \\\"It's a very good dog. She's\"\n    \"The cat was discovered earlier this month in the home of a relative of the deceased. The cat\\'s owner, who wished to remain anonymous,\"\n    \"The cat was pulled from the street by two-year-old Jazmine.\\\"I didn't know what to do,\\\" she said\"\n    \"The cat was attacked by two stray dogs and was taken to a hospital. Two other cats were also injured in the attack and are being treated.\"\n]\n```\n\n</details>\n&nbsp;\n<details>\n<summary> <b>6. Zero-shot classification </b> </summary>\n\nPerforms zero-shot classification on input sentences with provided labels using\na model fine-tuned for Natural Language Inference.\n\n```rust\n    let sequence_classification_model = ZeroShotClassificationModel::new(Default::default ()) ?;\n\nlet input_sentence = \"Who are you voting for in 2020?\";\nlet input_sequence_2 = \"The prime minister has announced a stimulus package which was widely criticized by the opposition.\";\nlet candidate_labels = & [\"politics\", \"public health\", \"economics\", \"sports\"];\n\nlet output = sequence_classification_model.predict_multilabel(\n& [input_sentence, input_sequence_2],\ncandidate_labels,\nNone,\n128,\n);\n```\n\nOutput:\n\n```\n[\n  [ Label { \"politics\", score: 0.972 }, Label { \"public health\", score: 0.032 }, Label {\"economics\", score: 0.006 }, Label {\"sports\", score: 0.004 } ],\n  [ Label { \"politics\", score: 0.975 }, Label { \"public health\", score: 0.0818 }, Label {\"economics\", score: 0.852 }, Label {\"sports\", score: 0.001 } ],\n]\n```\n\n</details>\n&nbsp;\n<details>\n<summary> <b>7. Sentiment analysis </b> </summary>\n\nPredicts the binary sentiment for a sentence. DistilBERT model fine-tuned on\nSST-2.\n\n```rust\n    let sentiment_classifier = SentimentModel::new(Default::default ()) ?;\n\nlet input = [\n\"Probably my all-time favorite movie, a story of selflessness, sacrifice and dedication to a noble cause, but it's not preachy or boring.\",\n\"This film tried to be too many things all at once: stinging political satire, Hollywood blockbuster, sappy romantic comedy, family values promo...\",\n\"If you like original gut wrenching laughter you will like this movie. If you are young or old then you will love this movie, hell even my mom liked it.\",\n];\n\nlet output = sentiment_classifier.predict( & input);\n```\n\n(Example courtesy of [IMDb](http://www.imdb.com))\n\nOutput:\n\n```\n[\n    Sentiment { polarity: Positive, score: 0.9981985493795946 },\n    Sentiment { polarity: Negative, score: 0.9927982091903687 },\n    Sentiment { polarity: Positive, score: 0.9997248985164333 }\n]\n```\n\n</details>\n&nbsp;\n<details>\n<summary> <b>8. Named Entity Recognition </b> </summary>\n\nExtracts entities (Person, Location, Organization, Miscellaneous) from text.\nBERT cased large model fine-tuned on CoNNL03, contributed by the\n[MDZ Digital Library team at the Bavarian State Library](https://github.com/dbmdz).\nModels are currently available for English, German, Spanish and Dutch.\n\n```rust\n    let ner_model = NERModel::new( default::default ()) ?;\n\nlet input = [\n\"My name is Amy. I live in Paris.\",\n\"Paris is a city in France.\"\n];\n\nlet output = ner_model.predict( & input);\n```\n\nOutput:\n\n```\n[\n  [\n    Entity { word: \"Amy\", score: 0.9986, label: \"I-PER\" }\n    Entity { word: \"Paris\", score: 0.9985, label: \"I-LOC\" }\n  ],\n  [\n    Entity { word: \"Paris\", score: 0.9988, label: \"I-LOC\" }\n    Entity { word: \"France\", score: 0.9993, label: \"I-LOC\" }\n  ]\n]\n```\n\n</details>\n&nbsp;\n<details>\n<summary> <b>9. Keywords/keyphrases extraction</b> </summary>\n\nExtract keywords and keyphrases extractions from input documents\n\n```rust\nfn main() -> anyhow::Result<()> {\n    let keyword_extraction_model = KeywordExtractionModel::new(Default::default())?;\n\n    let input = \"Rust is a multi-paradigm, general-purpose programming language. \\\n       Rust emphasizes performance, type safety, and concurrency. Rust enforces memory safetythat is, \\\n       that all references point to valid memorywithout requiring the use of a garbage collector or \\\n       reference counting present in other memory-safe languages. To simultaneously enforce \\\n       memory safety and prevent concurrent data races, Rust's borrow checker tracks the object lifetime \\\n       and variable scope of all references in a program during compilation. Rust is popular for \\\n       systems programming but also offers high-level features including functional programming constructs.\";\n\n    let output = keyword_extraction_model.predict(&[input])?;\n}\n```\n\nOutput:\n\n```\n\"rust\" - 0.50910604\n\"programming\" - 0.35731024\n\"concurrency\" - 0.33825397\n\"concurrent\" - 0.31229728\n\"program\" - 0.29115444\n```\n\n</details>\n&nbsp;\n<details>\n<summary> <b>10. Part of Speech tagging </b> </summary>\n\nExtracts Part of Speech tags (Noun, Verb, Adjective...) from text.\n\n```rust\n    let pos_model = POSModel::new( default::default ()) ?;\n\nlet input = [\"My name is Bob\"];\n\nlet output = pos_model.predict( & input);\n```\n\nOutput:\n\n```\n[\n    Entity { word: \"My\", score: 0.1560, label: \"PRP\" }\n    Entity { word: \"name\", score: 0.6565, label: \"NN\" }\n    Entity { word: \"is\", score: 0.3697, label: \"VBZ\" }\n    Entity { word: \"Bob\", score: 0.7460, label: \"NNP\" }\n]\n```\n\n</details>\n&nbsp;\n<details>\n<summary> <b>11. Sentence embeddings </b> </summary>\n\nGenerate sentence embeddings (vector representation). These can be used for\napplications including dense information retrieval.\n\n```rust\n    let model = SentenceEmbeddingsBuilder::remote(\nSentenceEmbeddingsModelType::AllMiniLmL12V2\n).create_model() ?;\n\nlet sentences = [\n\"this is an example sentence\",\n\"each sentence is converted\"\n];\n\nlet output = model.encode( & sentences) ?;\n```\n\nOutput:\n\n```\n[\n    [-0.000202666, 0.08148022, 0.03136178, 0.002920636 ...],\n    [0.064757116, 0.048519745, -0.01786038, -0.0479775 ...]\n]\n```\n\n</details>\n&nbsp;\n<details>\n<summary> <b>12. Masked Language Model </b> </summary>\n\nPredict masked words in input sentences.\n\n```rust\n    let model = MaskedLanguageModel::new(Default::default ()) ?;\n\nlet sentences = [\n\"Hello I am a <mask> student\",\n\"Paris is the <mask> of France. It is <mask> in Europe.\",\n];\n\nlet output = model.predict( & sentences);\n```\n\nOutput:\n\n```\n[\n    [MaskedToken { text: \"college\", id: 2267, score: 8.091}],\n    [\n        MaskedToken { text: \"capital\", id: 3007, score: 16.7249}, \n        MaskedToken { text: \"located\", id: 2284, score: 9.0452}\n    ]\n]\n```\n\n</details>\n\n## Benchmarks\n\nFor simple pipelines (sequence classification, tokens classification, question\nanswering) the performance between Python and Rust is expected to be comparable.\nThis is because the most expensive part of these pipeline is the language model\nitself, sharing a common implementation in the Torch backend. The\n[End-to-end NLP Pipelines in Rust](https://www.aclweb.org/anthology/2020.nlposs-1.4/)\nprovides a benchmarks section covering all pipelines.\n\nFor text generation tasks (summarization, translation, conversation, free text\ngeneration), significant benefits can be expected (up to 2 to 4 times faster\nprocessing depending on the input and application). The article\n[Accelerating text generation with Rust](https://guillaume-be.github.io/2020-11-21/generation_benchmarks)\nfocuses on these text generation applications and provides more details on the\nperformance comparison to Python.\n\n## Loading pretrained and custom model weights\n\nThe base model and task-specific heads are also available for users looking to\nexpose their own transformer based models. Examples on how to prepare the date\nusing a native tokenizers Rust library are available in `./examples` for BERT,\nDistilBERT, RoBERTa, GPT, GPT2 and BART. Note that when importing models from\nPytorch, the convention for parameters naming needs to be aligned with the Rust\nschema. Loading of the pre-trained weights will fail if any of the model\nparameters weights cannot be found in the weight files. If this quality check is\nto be skipped, an alternative method `load_partial` can be invoked from the\nvariables store.\n\nPretrained models are available on Hugging face's\n[model hub](https://huggingface.co/models?filter=rust) and can be loaded using\n`RemoteResources` defined in this library.\n\nA conversion utility script is included in `./utils` to convert Pytorch weights\nto a set of weights compatible with this library. This script requires Python\nand `torch` to be set-up, and can be used as follows:\n`python ./utils/convert_model.py path/to/pytorch_model.bin` where\n`path/to/pytorch_model.bin` is the location of the original Pytorch weights.\n\n```bash\npython3 -m venv .venv\nsource .venv/bin/activate\n\npip install -r requirements.txt\n\npython utils/convert_model.py path/to/pytorch_model.bin\n```\n\n## Citation\n\nIf you use `rust-bert` for your work, please cite\n[End-to-end NLP Pipelines in Rust](https://www.aclweb.org/anthology/2020.nlposs-1.4/):\n\n```bibtex\n@inproceedings{becquin-2020-end,\n    title = \"End-to-end {NLP} Pipelines in Rust\",\n    author = \"Becquin, Guillaume\",\n    booktitle = \"Proceedings of Second Workshop for NLP Open Source Software (NLP-OSS)\",\n    year = \"2020\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://www.aclweb.org/anthology/2020.nlposs-1.4\",\n    pages = \"20--25\",\n}\n```\n\n## Acknowledgements\n\nThank you to [Hugging Face](https://huggingface.co) for hosting a set of weights\ncompatible with this Rust library. The list of ready-to-use pretrained models is\nlisted at\n[https://huggingface.co/models?filter=rust](https://huggingface.co/models?filter=rust).\n"
        },
        {
          "name": "benches",
          "type": "tree",
          "content": null
        },
        {
          "name": "build.rs",
          "type": "blob",
          "size": 1.24609375,
          "content": "// Copyright 2023 Laurent Mazare\n// https://github.com/LaurentMazare/diffusers-rs/blob/main/build.rs\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//     http://www.apache.org/licenses/LICENSE-2.0\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\nfn main() {\n    let os = std::env::var(\"CARGO_CFG_TARGET_OS\").expect(\"Unable to get TARGET_OS\");\n    match os.as_str() {\n        \"linux\" | \"windows\" => {\n            if let Some(lib_path) = std::env::var_os(\"DEP_TCH_LIBTORCH_LIB\") {\n                println!(\n                    \"cargo:rustc-link-arg=-Wl,-rpath={}\",\n                    lib_path.to_string_lossy()\n                );\n            }\n            println!(\"cargo:rustc-link-arg=-Wl,--no-as-needed\");\n            println!(\"cargo:rustc-link-arg=-Wl,--copy-dt-needed-entries\");\n            println!(\"cargo:rustc-link-arg=-ltorch\");\n        }\n        _ => {}\n    }\n}\n"
        },
        {
          "name": "clippy.toml",
          "type": "blob",
          "size": 0.0322265625,
          "content": "too-many-arguments-threshold = 12"
        },
        {
          "name": "examples",
          "type": "tree",
          "content": null
        },
        {
          "name": "rustfmt.toml",
          "type": "blob",
          "size": 0.033203125,
          "content": "format_code_in_doc_comments = true"
        },
        {
          "name": "src",
          "type": "tree",
          "content": null
        },
        {
          "name": "tests",
          "type": "tree",
          "content": null
        },
        {
          "name": "utils",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}