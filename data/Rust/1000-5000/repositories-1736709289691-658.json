{
  "metadata": {
    "timestamp": 1736709289691,
    "page": 658,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjY2MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "josephg/diamond-types",
      "stars": 1623,
      "defaultBranch": "master",
      "files": [
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.1435546875,
          "content": "/target\nCargo.lock\nnode_modules\nvis/build\n.DS_Store\n/*.dt\nwiki/dist\nwiki/dist-client\npkg-web\npkg-node\njs/dist\nbundle*\ngenerated\n\ndb.dtjson\n.vscode\n"
        },
        {
          "name": ".vscode",
          "type": "tree",
          "content": null
        },
        {
          "name": "BINARY.md",
          "type": "blob",
          "size": 5.7529296875,
          "content": "# Diamond types binary encoding format\n\nThis document describes the diamond types binary file formats.\n\n> WARNING: This format is still flux. There may be some last minute breaking changes to the file format before DT hits 1.0.\n\nAs outlined in other documents, diamond types has 2 core data structures:\n\nAn **OpLog** is a log of all changes which have happened to a document. Essentially an oplog is a list of operations between two points in time. The list of operations isn't always strictly ordered, because sometimes operations are concurrent.\n\nA **Branch** is a copy of a document at some specific point in time. Essentially, a branch is simply a `(data, version)` tuple.\n\n## Oplog Encoding\n\nThe operation log can be thought of as a table, where each row in the table describes a single insert or delete operation. For each operation we store the following fields:\n\n- Parent version(s) (eg *[(mike, 2), (seph, 10)]*)\n- Resulting version (eg *(seph, 11)*)\n- Operation type (Currently just *Insert* or *Delete*)\n- Operation position. Ie, the position in the document where the insert or delete took place.\n- Operation contents (optional). For inserts and deletes, this names the item (character) which was inserted or deleted.\n\nNote the operation contents is optional.\n\n- Inserted contents are almost always desirable. But they aren't always needed - particularly in some minimal merging situations.\n- Deleted content is only needed in order to make it easier to rewind & replay history. Given the full history of a document, the deleted content can always be regenerated.\n\nThe operation log file format stores all of this information, as well as some additional metadata:\n\n- File format version (currently 0)\n- Starting version (for the whole file)\n- Starting content (Optional)\n- Resulting content (optional, Not implemented yet)\n- UserData (Optional)\n- CRC check (optional)\n\nEncoding the oplog is complex in order to store the oplog in a compact form. And we've achieved a very impressive result on that front, even though the cost is extra code complexity.\n\nMeasured in KB:\n\n| Implementation | Automerge-perf |\n| Raw JSON       | 16 060         |\n| JSON+gzip      | 904            |\n| JSON+Brotli    | 461            |\n| DT (full)      | 281            |\n| DT (patches)   | 23             |\n\n> TODO: Fill out the rest of this table with yjs & automerge implementations, and the other data sets.\n\nAt a high level, the file format looks like this:\n\n* Magic bytes (`DMNDTYPS`) (8 bytes)\n* Protocol version (Currently 0)\n* FileInfo chunk\n  * (**TODO**): File type\n  * UserData (optional)\n  * AgentNames (used below)\n* StartBranch (Ie, what the document looks like before the ops below)\n  * Frontier (Version / parents of the start of this file)\n  * Content (Optional)\n* Patches chunk (This contains the operations themselves)\n  * Inserted content\n  * Deleted content\n  * AgentAssignment (Version of each change)\n  * PositionalPatches (Type & position of each change)\n  * TimeDAG chunk (Parents of each change)\n* CRC\n\nThis file format is very much optimized for large files. Its not optimized for sending teeny tiny individual changes.\n\n\n### VarInts\n\nDT makes extensive use of the [Varint encoding](https://developers.google.com/protocol-buffers/docs/encoding#varints) from Google Protobufs. All integers in a DT file (unless otherwise specified) are encoded using the varint format.\n\n### Chunks\n\nA diamond types file is made up of a tree of chunks. Each chunk contains:\n\n- A chunk type (varint, see below)\n- The chunk's byte length, not including the chunk header (varint)\n- Bytes of data\n\nThe format has been designed to allow extra chunk types to be added over time. Unknown chunks should be ignored.\n\nRegistered chunk types are below:\n\n| Name | Code | Description |\n| ---- | ---- | ----------- |\n\n> TODO: Move into table above\n\n```rust\nenum ChunkType {\n    /// FileInfo contains optional UserData and AgentNames.\n    FileInfo = 1,\n    UserData = 2,\n    AgentNames = 3,\n\n    /// The StartBranch chunk describes the state of the document before included patches have been\n    /// applied.\n    StartBranch = 10,\n    Frontier = 12,\n    Content = 13,\n\n    Patches = 20,\n    Version = 21,\n    OpTypeAndPosition = 22,\n    Parents = 23,\n    InsertedContent = 24,\n    DeletedContent = 25,\n\n    CRC = 100,\n}\n```\n\n### Patch encoding\n\nThe DT patch encoding makes very heavy use of DT's RLE encoding tricks. Each chunk in the patch block contains one or more fields from the data set, run-length encoded. The data is formatted this way for compactness. For example:\n\n- Its quite common for all changes in the file to be from a single author, with sequential times. This allows the patch `Versions` field to essentially encode a single value.\n- Even complex histories often have quite simple time DAGs. The `Parents` field takes advantage of the fact almost all changes simply have the previous item as their parent, and essentially only encodes the parents of patches where this is not the case.\n- The patch `OpTypeAndPosition` field run-length encodes adjacent insert & delete operations. In a list with append-only (or prepend-only) edits, this will collapse to a single item! Real-world text editing traces are also compressed very efficiently with this, since users tend to type and delete in runs of characters.\n- The actual inserted & deleted content chunks are pulled out for a few reasons:\n  - They're optional\n  - Having the content itself separate makes it easier for code to adjust based on data type\n  - Content - particularly text content - compresses very well. Although compression hasn't been added to DT yet, I'm intending to add LZ4 compression to all content chunks. LZ4's fast compression seems to dramatically reduce file size with almost no cost to performance.\n\n\n## Branch Encoding\n\n> TODO\n\nBranches store the following fields:\n\n- Version\n- Type of content\n- Content itself\n\n"
        },
        {
          "name": "Cargo.toml",
          "type": "blob",
          "size": 2.794921875,
          "content": "[package]\nname = \"diamond-types\"\nversion = \"2.0.0\"\nedition = \"2021\"\nexclude = [\n    \".idea\", \".vscode\",\n    \"vis\", \"wiki\", \"js\",\n    \"benchmark_data\", \"test_data\",\n    \".github\"\n]\nlicense = \"ISC\"\ndescription = \"The world's fastest text CRDT\"\n\n# See more keys and their definitions at https://doc.rust-lang.org/cargo/reference/manifest.html\n\n[workspace]\nmembers = [\"crates/*\"]\n\n[dependencies]\nsmartstring = \"1.0.1\"\nstr_indices = \"0.4.3\"\nsmallvec = \"2.0.0-alpha.6\"\nlazy_static = \"1.4.0\"\n\n# Used by wasm module, CLI.\nserde = { version = \"1.0.183\", features = [\"derive\"], optional = true }\nrle = { version = \"0.2.0\", path = \"crates/rle\", features = [\"smallvec\"] }\n\n# Only used for generating testing data.\nserde_json = { version = \"1.0.104\", optional = true }\n\nbumpalo = { version = \"3.16.0\", features = [\"collections\"] }\n\n#jumprope = { path = \"../../../jumprope-rs\" }\n\n# Its tempting to disable default-features in jumprope because it means we don't need to hook in crypto random, which\n# saves some size in the wasm output size. But I think its better to default to having this feature enabled.\n#jumprope = { path = \"../jumprope-rs\", version = \"1.1.0\" }\njumprope = \"1.1.2\"\nhumansize = \"2.0.0\"\nnum_enum = \"0.7.2\"\n\n# crc32c might be faster, but it adds 10kb to the wasm bundle size. crc only adds 1kb.\n#crc32c = \"0.6\"\ncrc = \"3.0.0\"\nlz4_flex = { version = \"0.11.3\", optional = true }\n\n#bitvec = \"1.0.1\"\n\n# Needed for macos F_BARRIERFSYNC.\nlibc = \"0.2.139\"\n\nrand = { version = \"0.8.5\", features = [\"small_rng\"], optional = true }\n\n\n[dev-dependencies]\nrand = { version = \"0.8.5\", features = [\"small_rng\"] }\ncrdt-testdata = { path = \"crates/crdt-testdata\" }\ntrace-alloc = { path = \"crates/trace-alloc\" }\n\n# For OT fuzz data tests\n#json_minimal = \"0.1.3\"\n\n[features]\n#default = [\"lz4\", \"storage\", \"rand\"] # rand is only used in testing code, but there's no way to specify that.\ndefault = [\"lz4\", \"storage\"]\nmemusage = [\"trace-alloc/memusage\"]\nlz4 = [\"dep:lz4_flex\"]\nserde = [\"dep:serde\", \"smallvec/serde\", \"smartstring/serde\"]\ndot_export = []\nwchar_conversion = [\"jumprope/wchar_conversion\"]\nmerge_conflict_checks = []\nstorage = []\nexpose_benchmarking = [\"serde\", \"serde_json\"]\nstats = []\n\n# This is internal only for generating JSON testing data. To generate, run test suite with\n# rm *_tests.json; cargo test --features gen_test_data causalgraph::parents::tools -- --test-threads 1\ngen_test_data = [\"serde\", \"serde_json\", \"rand\"]\n\n[lib]\nbench = false\n\n[profile.release]\n#debug = true\nlto = true\ncodegen-units = 1\n#opt-level = \"s\"\npanic = \"abort\"\n\n[profile.release.package.dt-wasm]\nopt-level = 2\n#opt-level = \"s\"\n#debugging = true\n\n[profile.release.package.dt-cli]\nopt-level = \"s\"\n#lto = false\nstrip = true\n\n[profile.release.package.bench]\ncodegen-units = 1\n\n# Use with cargo build --profile profiling\n[profile.profiling]\ninherits = \"release\"\ndebug = true\n#opt-level = 0"
        },
        {
          "name": "INTERNALS.md",
          "type": "blob",
          "size": 9.486328125,
          "content": "# Diamond Types list type internals\n\nThis sounds weird, but a CRDT stores information about *space* and *time*.\n\nThe *spatial component* of a CRDT is its data - what does the document actually look like (at some moment in time)?\n\nThe *temporal component* of a CRDT is its history of changes. What happened, when? By whom? How did the document change from point in time A to B?\n\nDiamond types (like automerge) stores information about both the temporal and spatial dimensions of a document.\n\n\n# The 3 types of edits\n\nList CRDTs like diamond types use 3 different formats for interacting with edits:\n\n1. The *original change*. Original changes specify type, the position in the document and the *time* when the change happened. Eg: *Insert 'k' at position 12 after the merge of changes X and Y*. The original changes make up a DAG (directed acyclic graph).\n2. Merge algorithm specifics. This code uses a modified version of Yjs's merge logic. The algorithm creates a list of items in document order. Each item names some yjs-specific fields.\n3. The *resulting change* or *transformed change*. These changes superficially look like the original changes, but rather than being arranged in a DAG, these items are a simple list of changes. The changes can be applied in order to a document to recreate the document state.\n\nFor example, given these two concurrent original changes:\n\n- **ID 1:** Insert 'a' position 0, parents []\n- **ID 2:** Insert 'b' position 10, parents []\n\nWe can create a merge structure like this:\n\n1. Insert 'a' between *ROOT* and the original item at position 0.\n2. Original items from 0..10\n3. Insert 'b' between original item 10 and original item 11.\n\nWhile generating this structure, we can flatten the changes into a transformed list that looks like this:\n\n- **ID 1:** Insert 'a' position 0\n- **ID 2:** Insert 'b' position **11**\n\nOr equivilently like this:\n\n- **ID 2:** Insert 'b' position **10**\n- **ID 1:** Insert 'a' position 0\n\n(When concurrent changes happen, there is no canonical ordering - but every valid ordering will produce the same document state).\n\nThe transformed list can be applied to the document state to replay all the changes.\n\n\n## Causal graph (aka Time DAG)\n\nEach change has a *parents* field specifying the version of the document when the operation was created. We can use these versions to construct a [causal graph](https://en.wikipedia.org/wiki/Causal_graph) of changes.\n\nThe causal graph itself is stored and persisted by diamond types. We need this data to merge changes.\n\nLuckily, this data can be stored incredibly compactly thanks to the fact that concurrent operations are rare. The structure is stored in a run-length encoded list which only needs new entries for items which are not in an ordinary list.\n\n\n\n\n---\n\n# Old internals document\n\n> This was written for an earlier version of diamond types when I persisted the merge structure like yjs and automerge do. This has much worse performance when there are no concurrent changes, and a bigger file size. TODO: Bring this entirely up to date with the current DT version!\n\n## Space\n\nDiamond types stores a list of entries internally, one for each item in the document. This is stored in document-sorted order.\n\nConceptually, this is a list of Yjs entries. For a plain text document, it would look like this:\n\n```rust\nstruct YjsEntry {\n    id: Id,\n\n    // Note that after an entry is deleted, we still keep the entry!\n    value: Option<char>,\n\n    // Needed to order remote edits. Based on Yjs (with small changes)\n    origin_left: Id,\n    origin_right: Id,\n}\n\ntype DocData = Vec<YjsEntry>;\n```\n\nBut you won't find this structure in the codebase. The actual implementation of this structure is a little different because of a handful of optimizations operating in concert:\n\n- These entries are actually stored run-length encoded when possible. Adjacent, consecutive entries are compacted together.\n- Semantically we use attributed IDs: `(agent ID, seq)`. But internally all operations are locally linearized in time. So we only store a `u32` \"order number\" for each ID internally. These numbers are simply linearly increasing with each change seen at this peer. They're also local only - other peers will end up with different id-to-order mappings.\n- The values of each entry are pulled out into a separate data structure. (SoA instead of AoS). This makes it much easier to make the structure to be adapted to different kinds of data (text, rich text, lists of objects, etc).\n- These values are stored in a non-traditional b-tree, which I'm calling a [range tree](https://en.wikipedia.org/wiki/Range_tree) until someone convinces me not to.\n\nThe result of all this is that a diamond list stores the following information for a document:\n\n- Range tree of RLE entries\n- Index into the range tree, to map from item order to entry.\n- Bidirectional mapping from Order <-> `(agent, seq)` pairs\n\n\n## Time\n\nEach change which happens to a document is called an \"operation\" (or sometimes \"patch\"). Each operation has a unique ID and one or more *parents*.\n\nFor now, operations in diamond types are one of two types:\n\n- Insert some value at a position in the list\n- Mark a value in the list for deletion\n\nMore operation types will probably be added over time.\n\nEach operation 'consumes' the next ID in sequence from the user which authored that change. Eg, ('seph', 1), ('seph', 2), ('seph', 3).\n\nEach change also specifies a set of one or more parent IDs. This works the same way as commit parents in git, where an operation's parents are the IDs which came 'directly' before that operation. Changes end up forming a [DAG (Directed Acyclic Graph)](https://en.wikipedia.org/wiki/Directed_acyclic_graph). In the braid working group, we've taken to describing this structure as the \"time DAG\".\n\nThe first change has a special parent of \"ROOT\".\n\nSemantically we could describe the time DAG like this:\n\n```rust\nenum OpContent {\n    Insert(YjsEntry),\n    Delete(Id),\n}\n\nstruct Op {\n    id: Id,\n    content: OpContent,\n    parents: Vec<Id>\n}\n\ntype TimeDag = Set<Op>;\n```\n\nThis structure is very \"stringy\" in practice. Again, a series of optimizations allow this to be normally stored in a tiny amount of memory in practice.\n\nEach peer flattens all operations into a list based on *when* each operation was locally observed. The first operation a peer sees is item 0, then item 1, then 2, and so on. These item indexes are called \"order numbers\", and they're used everywhere internally. They are not shared between peers though - as peers may see the same operations in different orders.\n\nThis list always maintains partial order. An item with order X will always have parents with order lower than X.\n\n\n### Time formats\n\nThere's 3 ways to name a moment in time in diamond types:\n\n1. Using a full vector clock. This is a set of (id, seq) pairs for every agent which has ever contributed operations to the document. This structure will naturally be big (and grow over time) as more agent IDs make changes. But it can be interpreted by any peer at any time. This is useful for syncing peers which know nothing about one another - and may each have changes the other peer has never seen.\n2. A frontier set. If we consider the time DAG, there will always be a set of one or more items in the tree which have no children. We can transitively figure out the entire set of parents by following the parents' tree. When an operation is created, its parents set to be the document's frontier at the time that operation was created. (And in turn, that operation's ID will become the new frontier). This is much smaller than using the full vector clock (and it doesn't grow over time). It can be shared between peers - but if a peer is missing the latest changes, the frontier set will be incomprehensible.\n3. The \"next order\" number. This is a local only number naming the order which the next operation we see will be assigned. This is used by the OT bridge.\n\n\n# All together\n\nTaken together, the core document data structure (currently) looks something like this:\n\n```rust\npub struct ListCRDT {\n    // *** Space DAG stuff ***\n\n    /// The marker tree maps from order positions to btree entries, so we can map between orders and\n    /// document locations.\n    ///\n    /// This is the CRDT chum for the space DAG.\n    range_tree: RangeTree<YjsSpan>,\n\n    /// We need to be able to map each location to an item in the associated BST.\n    /// Note for inserts which insert a lot of contiguous characters, this will\n    /// contain a lot of repeated pointers. I'm trading off memory for simplicity\n    /// here - which might or might not be the right approach.\n    ///\n    /// This is a map from insert Order -> a pointer to the leaf node which contains that insert.\n    index: RleBTreeMap<Order, RangeTreeLeafPtr>,\n\n    /// This is used to map Order -> External CRDT locations.\n    client_with_order: RleVec<(Order, CRDTSpan)>,\n    /// This is used to map external CRDT locations -> Order numbers.\n    client_data: Vec<ClientData>,\n\n    /// The content of the document itself. This will become generic with time.\n    document_content: Ropey::Rope,\n\n    // *** Time DAG stuff ***\n\n    /// The set of txn orders with no children in the document. With a single writer this will\n    /// always just be the last order we've seen.\n    ///\n    /// Never empty. Starts pointing at the root order.\n    frontier: Vec<Order>,\n\n    /// Compact 'parents' for all operations\n    txns: RleVec<TxnSpan>,\n\n    /// Optimizations around deletes are a little complex. Essentially this\n    /// maps from delete operations -> which items each operation deleted.\n    deletes: RleVec<(Order, OrderSpan)>,\n    double_deletes: RleVec<(Order, DoubleDelete)>,\n}\n```"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 2.7763671875,
          "content": "# Diamond Types\n\n[**📦 Cargo package**](https://crates.io/crates/diamond-types)\n\n[**📖 Documentation on docs.rs**](https://docs.rs/diamond-types/latest/diamond_types/)\n\n[**🇳 NodeJS package on npm (via WASM)**](https://www.npmjs.com/package/diamond-types-node)\n\n[**🌐 Web browser package on npm (via WASM)**](https://www.npmjs.com/package/diamond-types-web)\n\nThis repository contains a high performance rust CRDT for text editing. This is a special data type which supports concurrent editing of lists or strings\n(text documents) by multiple users in a P2P network without needing a\ncentralized server.\n\nThis version of diamond types only supports plain text editing. Work is underway to add support for other JSON-style data types. See the `more_types` branch for details.\n\nThis project was initially created as a prototype to see how fast a well optimized CRDT could be made to go. The answer is really fast - faster than other similar libraries. This library is currently in the process of being expanded into a fast, feature rich CRDT in its own right.\n\nFor detail about how to *use* diamond types, see the [package level documentation at docs.rs](https://docs.rs/diamond-types/latest/diamond_types/).\n\nNote the package published to cargo is quite out of date, both in terms of API and performance.\n\nFor much more detail about how this library *works*, see:\n\n- The talk I gave on this library at a recent [braid user meetings](https://braid.org/meeting-14) or\n- [INTERNALS.md](INTERNALS.md) in this repository.\n- [This blog post on making diamond types 5000x faster than competing CRDT implementations](https://josephg.com/blog/crdts-go-brrr/)\n  - And since that blog post came out, performance has increased another 10-80x (!).\n\nAs well as being lightning fast, this library is also designed to be interoperable with positional updates. This allows simple peers to interact with the data structure via operational transform.\n\n\n## Internals\n\nEach client / device has a unique ID. Each character typed or deleted on\neach device is assigned an incrementing sequence number (starting at 0).\nEach character in the document can thus be uniquely identified by the\ntuple of `(client ID, sequence number)`. This allows any location in the\ndocument to be uniquely named.\n\nThe internal data structures are designed to optimize two main operations:\n\n- Text edit to CRDT operation (Eg, \"user A inserts at position 100\" -> \"user A\n  seq 1000 inserts at (B, 50)\")\n- CRDT operation to text edit (\"user A\n  seq 1000 inserts at (B, 50)\" -> \"insert at document position 100\")\n\nMuch more detail on the internal data structures used is in [INTERNALS.md](INTERNALS.md).\n\n\n# LICENSE\n\nThis code is published under the ISC license.\n\n\n# Acknowledgements\n\nThis work has been made possible by funding from the [Invisible College](https://invisible.college/)."
        },
        {
          "name": "bench.sh",
          "type": "blob",
          "size": 0.48046875,
          "content": "#!/usr/bin/env bash\nset -e\n#set -o xtrace\n\nstart_time=$(date +%s)   # Capture start time in seconds\n\ncargo build --release -p bench\n\nend_time=$(date +%s)     # Capture end time in seconds\n# Calculate duration\nduration=$((end_time - start_time))\n\n# Check if duration is less than 1 second\nif [ $duration -gt 1 ]; then\n  echo \"Waiting 5s for CPU to cool down\"\n  sleep 5\nfi\n\n#taskset 0x1 nice -10 cargo run --release -p bench -- --bench $@\ntaskset 0x1 nice -10 target/release/bench --bench \"$@\"\n"
        },
        {
          "name": "benchmark_data",
          "type": "tree",
          "content": null
        },
        {
          "name": "bucket_counts.js",
          "type": "blob",
          "size": 1.1318359375,
          "content": "const bucket_items_50 = [\n    0,\n    615,\n    205,\n    290,\n    180,\n    169,\n    132,\n    94,\n    82,\n    91,\n    79,\n    55,\n    43,\n    59,\n    61,\n    58,\n    44,\n    117,\n    115,\n    85,\n    73,\n    68,\n    92,\n    109,\n    105,\n    327,\n    244,\n    39,\n    10,\n    2,\n    0,\n    2,\n    1,\n    0,\n    0,\n    1,\n]\nconst bucket_items_10 = [\n    0,\n    7567,\n    1512,\n    1095,\n    2417,\n    3104,\n    2367,\n    137,\n    19,\n    13,\n    1,\n]\n\n\nconst size_counts = [\n    0,\n    5496,\n    24211,\n    7895,\n    899,\n    329,\n    258,\n    240,\n    232,\n    194,\n    204,\n    182,\n    158,\n    140,\n    146,\n    139,\n    134,\n    110,\n    104,\n    108,\n    81,\n    89,\n    81,\n    59,\n    77,\n    59,\n    60,\n    59,\n    58,\n    63,\n    40,\n    38,\n    38,\n    41,\n    40,\n    55,\n    36,\n    27,\n    29,\n    42,\n    27,\n    40,\n    34,\n    26,\n    25,\n    25,\n    22,\n    26,\n    28,\n    25,\n    614,\n]\n\nconst mean = nums => {\n  const num = nums.reduce((a, b, idx) => a+b, 0)\n  const sum = nums.reduce((a, b, idx) => a+b*idx, 0)\n  console.log('sum', sum, num)\n  console.log('mean', sum / num)\n}\n\nmean(size_counts)\nmean(bucket_items_50)\nmean(bucket_items_10)\n"
        },
        {
          "name": "build-swift.sh",
          "type": "blob",
          "size": 1.33203125,
          "content": "#!/bin/bash\n\nset -e\n\nTHISDIR=$(dirname $0)\ncd $THISDIR\n\n#FLAGS=\"\"\n#MODE=\"debug\"\nFLAGS=\"--release\"\nMODE=\"release\"\n\nexport SWIFT_BRIDGE_OUT_DIR=\"$(pwd)/crates/dt-swift/generated\"\nexport RUSTFLAGS=\"\"\n# Build the project for the desired platforms:\ncargo build $FLAGS --target x86_64-apple-darwin -p dt-swift\ncargo build $FLAGS --target aarch64-apple-darwin -p dt-swift\nmkdir -p ./target/universal-macos/\"$MODE\"\n\nlipo \\\n    ./target/aarch64-apple-darwin/\"$MODE\"/libdt_swift.a \\\n    ./target/x86_64-apple-darwin/\"$MODE\"/libdt_swift.a \\\n    -create -output ./target/universal-macos/\"$MODE\"/libdt_swift.a\n\ncargo build $FLAGS --target aarch64-apple-ios -p dt-swift\n#cargo build --target x86_64-apple-ios\ncargo build $FLAGS --target aarch64-apple-ios-sim -p dt-swift\nmkdir -p ./target/universal-ios/\"$MODE\"\n\n#lipo \\\n#    ./target/aarch64-apple-ios-sim/\"$MODE\"/libdt_swift.a \\\n#    -create -output ./target/universal-ios/\"$MODE\"/libdt_swift.a\n#    ./target/aarch64-apple-ios/\"$MODE\"/libdt_swift.a \\\n\nswift-bridge-cli create-package \\\n  --bridges-dir \"$SWIFT_BRIDGE_OUT_DIR\" \\\n  --out-dir target/dt-swift \\\n  --ios ./target/aarch64-apple-ios/\"$MODE\"/libdt_swift.a \\\n  --simulator ./target/aarch64-apple-ios-sim/\"$MODE\"/libdt_swift.a \\\n  --macos ./target/universal-macos/\"$MODE\"/libdt_swift.a \\\n  --name DiamondTypes\n\n\n#--simulator target/universal-ios/\"$MODE\"/libdt_swift.a \\\n"
        },
        {
          "name": "build_wasm.sh",
          "type": "blob",
          "size": 1.2646484375,
          "content": "set -e\n\nRUSTFLAGS=\"\"\n#cd crates/diamond-wasm\n\necho \"=== Before ===\"\nls -l pkg-web pkg-node || true\n#wasm-pack build --target nodejs\n#wasm-pack build --target bundler\n#wasm-pack build --target web --dev\n\nrm -rf pkg-*\n#wasm-pack build --target web --out-dir ../../pkg-web --out-name dt crates/dt-wasm --profiling\nwasm-pack build --target web --out-dir ../../pkg-web --out-name dt crates/dt-wasm\nwasm-pack build --target nodejs --out-dir ../../pkg-node --out-name dt crates/dt-wasm\n\n# sed -i '3i\\ \\ \"type\": \"module\",' pkg/package.json\n\n# Set version\n#sed -i.old 's/: \"0.1.0\"/: \"0.1.1\"/' pkg-*/package.json\n\n# Web code needs to have \"main\" defined since its an es6 module package\nsed -i.old 's/\"module\":/\"main\":/' pkg-web/package.json\nsed -i.old 's/\"name\": \"dt-wasm\"/\"name\": \"diamond-types-web\"/' pkg-web/package.json\nsed -i.old 's/\"name\": \"dt-wasm\"/\"name\": \"diamond-types-node\"/' pkg-node/package.json\nsed -i.old 's/\"files\": \\[/\"files\": \\[\\n    \"dt_bg.wasm.br\",/' pkg-web/package.json\n#perl -wlpi -e 'print \"  \\\"type\\\": \\\"module\\\",\" if $. == 2' pkg-web/package.json\n\nsed -i.old 's/\"0.1.0\"/\"1.0.2\"/' pkg-web/package.json\nsed -i.old 's/\"0.1.0\"/\"1.0.2\"/' pkg-node/package.json\n\nrm pkg-*/package.json.old\n\nbrotli -f pkg-web/*.wasm\n\necho \"=== After ===\"\nls -l pkg-web pkg-node\n\ncat pkg-web/package.json\n"
        },
        {
          "name": "crates",
          "type": "tree",
          "content": null
        },
        {
          "name": "examples",
          "type": "tree",
          "content": null
        },
        {
          "name": "js",
          "type": "tree",
          "content": null
        },
        {
          "name": "npm-pkg-isomorphic",
          "type": "tree",
          "content": null
        },
        {
          "name": "prev_oplog",
          "type": "tree",
          "content": null
        },
        {
          "name": "profile.sh",
          "type": "blob",
          "size": 0.2802734375,
          "content": "#!/usr/bin/env bash\nset -e\n\nRUSTFLAGS=\"-Cforce-frame-pointers=yes\" cargo build --profile profiling --example profile\nperf record -g -F 9999 --call-graph fp target/profiling/examples/profile\nperf script -F +pid > /tmp/test.perf\n\necho \"Perf data in perf.data and script in /tmp/test.perf\"\n"
        },
        {
          "name": "src",
          "type": "tree",
          "content": null
        },
        {
          "name": "test_data",
          "type": "tree",
          "content": null
        },
        {
          "name": "vis",
          "type": "tree",
          "content": null
        },
        {
          "name": "wiki",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}