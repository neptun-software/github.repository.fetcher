{
  "metadata": {
    "timestamp": 1736709505709,
    "page": 3,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjEw",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "Unstructured-IO/unstructured",
      "stars": 9757,
      "defaultBranch": "main",
      "files": [
        {
          "name": ".coveragerc",
          "type": "blob",
          "size": 0.1669921875,
          "content": "[run]\nomit =\n    unstructured/ingest/*\n    # TODO(yuming): please remove this line after adding tests for paddle\n    unstructured/partition/utils/ocr_models/paddle_ocr.py\n"
        },
        {
          "name": ".dockerignore",
          "type": "blob",
          "size": 0.0419921875,
          "content": ".git\n.vscode\n__pycache__\n*.pyc\n*.pyo\n*.bak\n"
        },
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 3.0576171875,
          "content": "# Byte-compiled / optimized / DLL files\n__pycache__/\n*.py[cod]\n*$py.class\n\n# C extensions\n*.so\n\n# Distribution / packaging\n.Python\nbuild/\ndevelop-eggs/\ndist/\ndownloads/\nfigures/\neggs/\n.eggs/\nlib/\nlib64/\nparts/\nsdist/\nvar/\nwheels/\npip-wheel-metadata/\nshare/python-wheels/\n*.egg-info/\nnltk_data/\n.installed.cfg\n*.egg\nMANIFEST\n\n# PyInstaller\n#  Usually these files are written by a python script from a template\n#  before PyInstaller builds the exe, so as to inject date/other infos into it.\n*.manifest\n*.spec\n\n# Installer logs\npip-log.txt\npip-delete-this-directory.txt\n\n# Pycharm\n.idea/\n\n# Unit test / coverage reports\nhtmlcov/\n.tox/\n.nox/\n.coverage\n.coverage.*\n.cache\nnosetests.xml\ncoverage.xml\n*.cover\n*.py,cover\n.hypothesis/\n.pytest_cache/\n\n# Translations\n*.mo\n*.pot\n\n# Django stuff:\n*.log\nlocal_settings.py\ndb.sqlite3\ndb.sqlite3-journal\n\n# Flask stuff:\ninstance/\n.webassets-cache\n\n# Scrapy stuff:\n.scrapy\n\n# Sphinx documentation\ndocs/_build/\n\n# PyBuilder\ntarget/\n\n# Jupyter Notebook\n.ipynb_checkpoints\nnbs/\n\n# IPython\nprofile_default/\nipython_config.py\n\n# pyenv\n.python-version\n\n# pipenv\n#   According to pypa/pipenv#598, it is recommended to include Pipfile.lock in version control.\n#   However, in case of collaboration, if having platform-specific dependencies or dependencies\n#   having no cross-platform support, pipenv may install dependencies that don't work, or not\n#   install all needed dependencies.\n#Pipfile.lock\n\n# PEP 582; used by e.g. github.com/David-OConnor/pyflow\n__pypackages__/\n\n# Celery stuff\ncelerybeat-schedule\ncelerybeat.pid\n\n# SageMath parsed files\n*.sage.py\n\n# Environments\n.env\n.venv\nenv/\nvenv/\nENV/\nenv.bak/\nvenv.bak/\n\n# Spyder project settings\n.spyderproject\n.spyproject\n\n# Rope project settings\n.ropeproject\n\n# mkdocs documentation\n/site\n\n# mypy\n.mypy_cache/\n.dmypy.json\ndmypy.json\n\n# Pyre type checker\n.pyre/\n\n# pyright (Python LSP/type-checker in VSCode) config\n/pyrightconfig.json\n\n# ingest outputs\n/structured-output\ntest_unstructured_ingest/workdir/\ntest_unstructured_ingest/delta-table-dest/\ntest_unstructured_ingest/skipped-files.txt\ntest_unstructured_ingest/chroma-dest/\n\n# suggested ingest mirror directory\n/mirror\n\n## https://github.com/github/gitignore/blob/main/Global/Emacs.gitignore (partial)\n\n*~\n\\#*\\#\n/.emacs.desktop\n/.emacs.desktop.lock\n*.elc\nauto-save-list\ntramp\n.\\#*\n\n## https://github.com/github/gitignore/blob/main/Global/VisualStudioCode.gitignore\n.vscode/*\n!.vscode/tasks.json\n!.vscode/launch.json\n!.vscode/extensions.json\n!.vscode/*.code-snippets\n\n# Local History for Visual Studio Code\n.history/\n\n# Built Visual Studio Code Extensions\n*.vsix\n\n## https://github.com/github/gitignore/blob/main/Global/Vim.gitignore\n# Swap\n[._]*.s[a-v][a-z]\n!*.svg  # comment out if you don't need vector files\n[._]*.sw[a-p]\n[._]s[a-rt-v][a-z]\n[._]ss[a-gi-z]\n[._]sw[a-p]\n\n# Session\nSession.vim\nSessionx.vim\n\n# Temporary\n.netrwhist\n# Auto-generated tag files\ntags\n# Persistent undo\n[._]*.un~\n\n*.DS_Store\n\n# Ruff cache\n.ruff_cache/\n\nunstructured-inference/sample-docs/*\n.ppm\nunstructured-inference/\n\nexample-docs/*_images\nexamples/**/output/\n\noutputdiff.txt\nmetricsdiff.txt\n\n# analysis\nannotated/"
        },
        {
          "name": ".grype.yaml",
          "type": "blob",
          "size": 0.041015625,
          "content": "ignore:\n  - vulnerability: CVE-2024-11053\n"
        },
        {
          "name": ".pre-commit-config.yaml",
          "type": "blob",
          "size": 0.7509765625,
          "content": "repos:\n  - repo: https://github.com/pre-commit/pre-commit-hooks\n    rev: \"v4.3.0\"\n    hooks:\n      - id: check-added-large-files\n      - id: check-toml\n      - id: check-yaml\n      - id: check-json\n      - id: check-xml\n      - id: end-of-file-fixer\n        exclude: \\.json$\n        files: \\.py$\n      - id: trailing-whitespace\n      - id: mixed-line-ending\n\n  - repo: https://github.com/psf/black\n    rev: 24.2.0\n    hooks:\n      - id: black\n        args: [\"--line-length=100\"]\n        language_version: python3\n\n  - repo: https://github.com/astral-sh/ruff-pre-commit\n    rev: v0.2.1\n    hooks:\n      - id: ruff\n        args:\n          [\"--fix\"]\n\n  - repo: https://github.com/pycqa/flake8\n    rev: 7.0.0\n    hooks:\n      - id: flake8\n        language_version: python3\n"
        },
        {
          "name": "CHANGELOG.md",
          "type": "blob",
          "size": 169.3603515625,
          "content": "## 0.16.13-dev0\n\n### Enhancements\n\n### Features\n\n### Fixes\n\n- **Fix NLTK Download** to use nltk assets in docker image\n- removed the ability to automatically download nltk package if missing\n  \n## 0.16.12\n\n### Enhancements\n\n- **Prepare auto-partitioning for pluggable partitioners**. Move toward a uniform partitioner call signature so a custom or override partitioner can be registered without code changes.\n- **Add NDJSON file type support.**\n\n### Features\n\n### Fixes\n\n- **Base image has been updated.**\n- **Upgrade ruff to latest.** Previously the ruff version was pinned to <0.5. Remove that pin and fix the handful of lint items that resulted.\n- **CSV with asserted XLS content-type is correctly identified as CSV.** Resolves a bug where a CSV file with an asserted content-type of `application/vnd.ms-excel` was incorrectly identified as an XLS file.\n- **Improve element-type mapping for Chinese text.** Fixes bug where Chinese text would produce large numbers of false-positive `Title` elements.\n- **Improve element-type mapping for HTML.** Fixes bug where certain non-title elements were classified as `Title`.\n\n## 0.16.11\n\n### Enhancements\n\n- **Enhance quote standardization tests** with additional Unicode scenarios\n- **Relax table segregation rule in chunking.** Previously a `Table` element was always segregated into its own pre-chunk such that the `Table` appeared alone in a chunk or was split into multiple `TableChunk` elements, but never combined with `Text`-subtype elements. Allow table elements to be combined with other elements in the same chunk when space allows.\n- **Compute chunk length based solely on `element.text`.** Previously `.metadata.text_as_html` was also considered and since it is always longer that the text (due to HTML tag overhead) it was the effective length criterion. Remove text-as-html from the length calculation such that text-length is the sole criterion for sizing a chunk.\n\n### Features\n\n### Fixes\n\n- Fix ipv4 regex to correctly include up to three digit octets.\n\n## 0.16.10\n\n### Enhancements\n\n### Features\n\n### Fixes\n\n- **Fix original file doctype detection** from cct converted file paths for metrics calculation.\n\n## 0.16.9\n\n### Enhancements\n\n### Features\n\n### Fixes\n\n- **Fix NLTK Download** to not download from unstructured S3 Bucket\n\n## 0.16.8\n\n### Enhancements\n- **Metrics: Weighted table average is optional**\n\n### Features\n\n### Fixes\n\n## 0.16.7\n\n### Enhancements\n- **Add image_alt_mode to partition_html** Adds an `image_alt_mode` parameter to `partition_html()` to control how alt text is extracted from images in HTML documents for `html_parser_version=v2` . The parameter can be set to `to_text` to extract alt text as text from `<img>` html tags\n\n### Features\n\n### Fixes\n\n\n## 0.16.6\n\n### Enhancements\n- **Every `<table>` tag is considered to be ontology.Table** Added special handling for tables in HTML partitioning (`html_parser_version=v2`. This change is made to improve the accuracy of table extraction from HTML documents.\n- **Every HTML has default ontology class assigned** When parsing HTML with `html_parser_version=v2` to ontology each defined HTML in the Ontology has assigned default ontology class. This way it is possible to assign ontology class instead of UncategorizedText when the HTML tag is predicted correctly without class assigned class\n- **Use (number of actual table) weighted average for table metrics** In evaluating table metrics the mean aggregation now uses the actual number of tables in a document to weight the metric scores\n\n### Features\n\n### Fixes\n- **ElementMetadata consolidation** Now `text_as_html` metadata is combined across all elements in CompositeElement when chunking HTML output\n\n## 0.16.5\n\n### Enhancements\n\n### Features\n\n### Fixes\n- **Fixes parsing HTML v2 parser** Now max recursion limit is set and value is correctly extracted from ontology element\n\n\n## 0.16.4\n\n### Enhancements\n\n* **`value` attribute in `<input/>` element is parsed to `OntologyElement.text` in ontology**\n* **`id` and `class` attributes removed from Table subtags in HTML partitioning**\n* **cleaned `to_html` and newly introduced `to_text` in `OntologyElement`**\n* **Elements created from V2 HTML are less granular** Added merging of adjacent text elements and inline html tags in the HTML partitioner to reduce the number of elements created from V2 HTML.\n\n### Features\n\n* **Add support for link extraction in pdf hi_res strategy.** The `partition_pdf()` function now supports link extraction when using the `hi_res` strategy, allowing users to extract hyperlinks from PDF documents more effectively.\n\n### Fixes\n\n\n## 0.16.3\n\n### Enhancements\n\n### Features\n\n### Fixes\n\n* **V2 elements without first parent ID can be parsed**\n* **Fix missing elements when layout element parsed in V2 ontology**\n* updated **unstructured-inference** to be **0.8.1** in requirements/extra-pdf-image.in\n\n\n## 0.16.2\n\n### Enhancements\n\n### Features\n\n* **Whitespace-invariant CCT distance metric.** CCT Levenshtein distance for strings is by default computed with standardized whitespaces.\n\n### Fixes\n\n* **Fixed retry config settings for partition_via_api function** If the SDK's default retry config is not set the retry config getter function does not fail anymore.\n\n## 0.16.1\n\n### Enhancements\n\n* **Bump `unstructured-inference` to 0.7.39** and upgrade other dependencies\n* **Round coordinates** Round coordinates when computing bounding box overlaps in `pdfminer_processing.py` to nearest machine precision. This can help reduce underterministic behavior from machine precision that affects which bounding boxes to combine.\n* **Request retry parameters in `partition_via_api` function.** Expose retry-mechanism related parameters in the `partition_via_api` function to allow users to configure the retry behavior of the API requests.\n\n### Features\n\n* **Parsing HTML to Unstructured Elements and back**\n\n### Fixes\n\n* **Remove unsupported chipper model**\n* **Rewrite of `partition.email` module and tests.** Use modern Python stdlib `email` module interface to parse email messages and attachments. This change shortens and simplifies the code, and makes it more robust and maintainable. Several historical problems were remedied in the process.\n* **Minify text_as_html from DOCX.** Previously `.metadata.text_as_html` for DOCX tables was \"bloated\" with whitespace and noise elements introduced by `tabulate` that produced over-chunking and lower \"semantic density\" of elements. Reduce HTML to minimum character count while preserving all text.\n* **Fall back to filename extension-based file-type detection for unidentified OLE files.** Resolves a problem where a DOC file that could not be detected as such by `filetype` was incorrectly identified as a MSG file.\n* **Minify text_as_html from XLSX.** Previously `.metadata.text_as_html` for DOCX tables was \"bloated\" with whitespace and noise elements introduced by `pandas` that produced over-chunking and lower \"semantic density\" of elements. Reduce HTML to minimum character count while preserving all text.\n* **Minify text_as_html from CSV.** Previously `.metadata.text_as_html` for CSV tables was \"bloated\" with whitespace and noise elements introduced by `pandas` that produced over-chunking and lower \"semantic density\" of elements. Reduce HTML to minimum character count while preserving all text.\n* **Minify text_as_html from PPTX.** Previously `.metadata.text_as_html` for PPTX tables was \"bloated\" with whitespace and noise elements introduced by `tabulate` that produced over-chunking and lower \"semantic density\" of elements. Reduce HTML to minimum character count while preserving all text and structure.\n\n## 0.16.0\n\n### Enhancements\n\n* **Remove ingest implementation.** The deprecated ingest functionality has been removed, as it is now maintained in the separate [unstructured-ingest](https://github.com/Unstructured-IO/unstructured-ingest) repository.\n  * Replace extras in `requirements/ingest` directory with a new `ingest.txt` extra for installing the `unstructured-ingest` library.\n  * Remove the `unstructured.ingest` submodule.\n  * Delete all shell scripts previously used for destination ingest tests.\n\n### Features\n\n### Fixes\n\n* **Add language parameter to `OCRAgentGoogleVision`.**  Introduces an optional language parameter in the `OCRAgentGoogleVision` constructor to serve as a language hint for `document_text_detection`. This ensures compatibility with the OCRAgent's `get_instance` method and resolves errors when parsing PDFs with Google Cloud Vision as the OCR agent.\n\n## 0.15.14\n\n### Enhancements\n\n### Features\n\n* **Add (but do not install) a new post-partitioning decorator to handle metadata added for all file-types, like `.filename`, `.filetype` and `.languages`.** This will be installed in a closely following PR to replace the four currently being used for this purpose.\n\n### Fixes\n\n* **Update Python SDK usage in `partition_via_api`.** Make a minor syntax change to ensure forward compatibility with the upcoming 0.26.0 Python SDK.\n* **Remove \"unused\" `date_from_file_object` parameter.** As part of simplifying partitioning parameter set, remove `date_from_file_object` parameter. A file object does not have a last-modified date attribute so can never give a useful value. When a file-object is used as the document source (such as in Unstructured API) the last-modified date must come from the `metadata_last_modified` argument.\n* **Fix occasional `KeyError` when mapping parent ids to hash ids.** Occasionally the input elements into `assign_and_map_hash_ids` can contain duplicated element instances, which lead to error when mapping parent id.\n* **Allow empty text files.** Fixes an issue where text files with only white space would fail to be partitioned.\n* **Remove double-decoration for CSV, DOC, ODT partitioners.** Refactor these partitioners to use the new `@apply_metadata()` decorator and only decorate the principal partitioner (CSV and DOCX in this case); remove decoration from delegating partitioners.\n* **Remove double-decoration for PPTX, TSV, XLSX, and XML partitioners.** Refactor these partitioners to use the new `@apply_metadata()` decorator and only decorate the principal partitioner; remove decoration from delegating partitioners.\n* **Remove double-decoration for HTML, EPUB, MD, ORG, RST, and RTF partitioners.** Refactor these partitioners to use the new `@apply_metadata()` decorator and only decorate the principal partitioner (HTML in this case); remove decoration from delegating partitioners.\n* **Remove obsolete min_partition/max_partition args from TXT and EML.** The legacy `min_partition` and `max_partition` parameters were an initial rough implementation of chunking but now interfere with chunking and are unused. Remove those parameters from `partition_text()` and `partition_email()`.\n* **Remove double-decoration on EML and MSG.** Refactor these partitioners to rely on the new `@apply_metadata()` decorator operating on partitioners they delegate to (TXT, HTML, and all others for attachments) and remove direct decoration from EML and MSG.\n* **Remove double-decoration for PPT.** Remove decorators from the delegating PPT partitioner.\n* **Quick-fix CI error in auto test-filetype.** Better fix to follow shortly.\n\n## 0.15.13\n\n### BREAKING CHANGES\n\n* **Remove dead experimental code.** Unused code in `file_utils.experimental` and `file_utils.metadata` was removed. These functions were never published in the documentation, but if a client dug these out and used them this removal could break client code.\n\n### Enhancements\n\n* **Improve `pdfminer` image cleanup process**. Optimized the removal of duplicated pdfminer images by performing the cleanup before merging elements, rather than after. This improvement reduces execution time and enhances overall processing speed of PDF documents.\n\n### Features\n\n### Fixes\n\n* **Fixes high memory overhead for intersection area computation** Using `numpy.float32` for coordinates and remove intermediate variables to reduce memory usage when computing intersection areas\n* **Fixes the `arm64` image build** `arm64` builds are now fixed and will be available against starting with the `0.15.13` release.\n\n## 0.15.12\n\n### Enhancements\n\n* **Improve `pdfminer` element processing** Implemented splitting of `pdfminer` elements (groups of text chunks) into smaller bounding boxes (text lines). This prevents loss of information from the object detection model and facilitates more effective removal of duplicated `pdfminer` text.\n\n### Features\n\n### Fixes\n\n* **Fixed table accuracy metric** Table accuracy was incorrectly using column content difference in calculating row accuracy.\n\n## 0.15.11\n\n### Enhancements\n\n* **Add deprecation warning to embed code**\n* **Remove ingest console script**\n\n## 0.15.10\n\n### Enhancements\n\n* **Enhance `pdfminer` element cleanup** Expand removal of `pdfminer` elements to include those inside all `non-pdfminer` elements, not just `tables`.\n* **Modified analysis drawing tools to dump to files and draw from dumps** If the parameter `analysis` of the `partition_pdf` function is set to `True`, the layout for Object Detection, Pdfminer Extraction, OCR and final layouts will be dumped as json files. The drawers now accept dict (dump) objects instead of internal classes instances.\n* **Vectorize pdfminer elements deduplication computation**. Use `numpy` operations to compute IOU and sub-region membership instead of using simply loop. This improves the speed of deduplicating elements for pages with a lot of elements.\n\n### Features\n\n### Fixes\n\n## 0.15.9\n\n### Enhancements\n\n### Features\n\n* **Add support for encoding parameter in partition_csv**\n\n### Fixes\n\n* **Check storage contents for OLE file type detection** Updates `detect_filetype` to check the content of OLE files to more reliable differentiate DOC, PPT, XLS, and MSG files. As part of this, the `\"msg\"` extra was removed because the `python-oxmsg` package is now a base dependency.\n* **Fix disk space leaks and Windows errors when accessing file.name on a NamedTemporaryFile** Uses of `NamedTemporaryFile(..., delete=False)` and/or uses of `file.name` of NamedTemporaryFiles have been replaced with TemporaryFileDirectory to avoid a known issue: https://docs.python.org/3/library/tempfile.html#tempfile.NamedTemporaryFile\n\n## 0.15.8\n\n### Enhancements\n\n* **Bump unstructured.paddleocr to 2.8.1.0.**\n\n### Features\n\n* **Add MixedbreadAI embedder** Adds MixedbreadAI embeddings to support embedding via Mixedbread AI.\n\n### Fixes\n\n* **Replace `pillow-heif` with `pi-heif`**. Replaces `pillow-heif` with `pi-heif` due to more permissive licensing on the wheel for `pi-heif`.\n* **Minify text_as_html from DOCX.** Previously `.metadata.text_as_html` for DOCX tables was \"bloated\" with whitespace and noise elements introduced by `tabulate` that produced over-chunking and lower \"semantic density\" of elements. Reduce HTML to minimum character count without preserving all text.\n* **Fall back to filename extension-based file-type detection for unidentified OLE files.** Resolves a problem where a DOC file that could not be detected as such by `filetype` was incorrectly identified as a MSG file.\n\n## 0.15.7\n\n### Enhancements\n\n### Features\n\n### Fixes\n\n* **Fix NLTK data download path to prevent nested directories**. Resolved an issue where a nested \"nltk_data\" directory was created within the parent \"nltk_data\" directory when it already existed. This fix prevents errors in checking for existing downloads and loading models from NLTK data.\n\n## 0.15.6\n\n### Enhancements\n\n### Features\n\n### Fixes\n\n* **Bump to NLTK 3.9.x** Bumps to the latest `nltk` version to resolve CVE.\n* **Update CI for `ingest-test-fixture-update-pr` to resolve NLTK model download errors.**\n* **Synchronized text and html on `TableChunk` splits.** When a `Table` element is divided during chunking to fit the chunking window, `TableChunk.text` corresponds exactly with the table text in `TableChunk.metadata.text_as_html`, `.text_as_html` is always parseable HTML, and the table is split on even row boundaries whenever possible.\n\n## 0.15.5\n\n### Enhancements\n\n### Features\n\n### Fixes\n\n* **Revert to using `unstructured.pytesseract` fork**. Due to the unavailability of some recent release versions of `pytesseract` on PyPI, the project now uses the `unstructured.pytesseract` fork to ensure stability and continued support.\n* **Bump `libreoffice` verson in image.** Bumps the `libreoffice` version to `25.2.5.2` to address CVEs.\n* **Downgrade NLTK dependency version for compatibility**. Due to the unavailability of `nltk==3.8.2` on PyPI, the NLTK dependency has been downgraded to `<3.8.2`. This change ensures continued functionality and compatibility.\n\n## 0.15.4\n\n### Enhancements\n\n### Features\n\n### Fixes\n\n* **Resolve an installation error with `pytesseract>=0.3.12` that occurred during `pip install unstructured[pdf]==0.15.3`.**\n\n## 0.15.3\n\n### Enhancements\n\n### Features\n\n### Fixes\n\n* **Remove the custom index URL from `extra-paddleocr.in` to resolve the error in the `setup.py` configuration.**\n\n## 0.15.2\n\n### Enhancements\n\n* **Improve directory handling when extracting image blocks**. The `figures` directory is no longer created when the `extract_image_block_to_payload` parameter is set to `True`.\n\n### Features\n\n* **Added per-class Object Detection metrics in the evaluation**. The metrics include average precision, precision, recall, and f1-score for each class in the dataset.\n\n### Fixes\n\n* **Updates NLTK data file for compatibility with `nltk>=3.8.2`**. The NLTK data file now container `punkt_tab`, making it possible to upgrade to `nltk>=3.8.2`. The `nltk==3.8.2` patches CVE-2024-39705.\n* **Renames Astra to Astra DB** Conforms with DataStax internal naming conventions.\n* **Accommodate single-column CSV files.** Resolves a limitation of `partition_csv()` where delimiter detection would fail on a single-column CSV file (which naturally has no delimeters).\n* **Accommodate `image/jpg` in PPTX as alias for `image/jpeg`.** Resolves problem partitioning PPTX files having an invalid `image/jpg` (should be `image/jpeg`) MIME-type in the `[Content_Types].xml` member of the PPTX Zip archive.\n* **Fixes an issue in Object Detection metrics** The issue was in preprocessing/validating the ground truth and predicted data for object detection metrics.\n* **Removes dependency on unstructured.pytesseract** Unstructured forked pytesseract while waiting for code to be upstreamed. Now that the new version has been released, this fork can be removed.\n\n## 0.15.1\n\n### Enhancements\n\n* **Improve `pdfminer` embedded `image` extraction to exclude text elements and produce more accurate bounding boxes.** This results in cleaner, more precise element extraction in `pdf` partitioning.\n\n### Features\n\n* **Update partition_eml and partition_msg to capture cc, bcc, and message_id fields** Cc, bcc, and message_id information is captured in element metadata for both msg and email partitioning and `Recipient` elements are generated for cc and bcc when `include_headers=True` for email partitioning.\n* **Mark ingest as deprecated** Begin sunset of ingest code in this repo as it's been moved to a dedicated repo.\n* **Add `pdf_hi_res_max_pages` argument for partitioning, which allows rejecting PDF files that exceed this page number limit, when the `high_res` strategy is chosen.** By default, it will allow parsing PDF files with an unlimited number of pages.\n\n### Fixes\n\n* **Update `HuggingFaceEmbeddingEncoder` to use `HuggingFaceEmbeddings` from `langchain_huggingface` package instead of the deprecated version from `langchain-community`.** This resolves the deprecation warning and ensures compatibility with future versions of langchain.\n* **Update `OpenAIEmbeddingEncoder` to use `OpenAIEmbeddings` from `langchain-openai` package instead of the deprecated version from `langchain-community`.** This resolves the deprecation warning and ensures compatibility with future versions of langchain.\n* **Update import of Pinecone exception** Adds compatibility for pinecone-client>=5.0.0\n* **File-type detection catches non-existent file-path.** `detect_filetype()` no longer silently falls back to detecting a file-type based on the extension when no file exists at the path provided. Instead `FileNotFoundError` is raised. This provides consistent user notification of a mis-typed path rather than an unpredictable exception from a file-type specific partitioner when the file cannot be opened.\n* **EML files specified as a file-path are detected correctly.** Resolved a bug where an EML file submitted to `partition()` as a file-path was identified as TXT and partitioned using `partition_text()`. EML files specified by path are now identified and processed correctly, including processing any attachments.\n* **A DOCX, PPTX, or XLSX file specified by path and ambiguously identified as MIME-type \"application/octet-stream\" is identified correctly.** Resolves a shortcoming where a file specified by path immediately fell back to filename-extension based identification when misidentified as \"application/octet-stream\", either by asserted content type or a mis-guess by libmagic. An MS Office file misidentified in this way is now correctly identified regardless of its filename and whether it is specified by path or file-like object.\n* **Textual content retrieved from a URL with gzip transport compression now partitions correctly.** Resolves a bug where a textual file-type (such as Markdown) retrieved by passing a URL to `partition()` would raise when `gzip` compression was used for transport by the server.\n* **A DOCX, PPTX, or XLSX content-type asserted on partition is confirmed or fixed.** Resolves a bug where calling `partition()` with a swapped MS-Office `content_type` would cause the file-type to be misidentified. A DOCX, PPTX, or XLSX MIME-type received by `partition()` is now checked for accuracy and corrected if the file is for a different MS-Office 2007+ type.\n* **DOC, PPT, XLS, and MSG files are now auto-detected correctly.** Resolves a bug where DOC, PPT, and XLS files were auto-detected as MSG files under certain circumstances.\n\n## 0.15.0\n\n### Enhancements\n\n* **Improve text clearing process in email partitioning.** Updated the email partitioner to remove both `=\\n` and `=\\r\\n` characters during the clearing process. Previously, only `=\\n` characters were removed.\n* **Bump unstructured.paddleocr to 2.8.0.1.**\n* **Refine HTML parser to accommodate block element nested in phrasing.** HTML parser no longer raises on a block element (e.g. `<p>`, `<div>`) nested inside a phrasing element (e.g. `<strong>` or `<cite>`). Instead it breaks the phrasing run (and therefore element) at the block-item start and begins a new phrasing run after the block-item. This is consistent with how the browser determines element boundaries in this situation.\n* **Install rewritten HTML parser to fix 12 existing bugs and provide headroom for refinement and growth.** A rewritten HTML parser resolves a collection of outstanding bugs with HTML partitioning and provides a firm foundation for further elaborating that important partitioner.\n* **CI check for dependency licenses** Adds a CI check to ensure dependencies are appropriately licensed.\n\n### Features\n\n* **Add support for specifying OCR language to `partition_pdf()`.** Extend language specification capability to `PaddleOCR` in addition to `TesseractOCR`. Users can now specify OCR languages for both OCR engines when using `partition_pdf()`.\n* **Add AstraDB source connector** Adds support for ingesting documents from AstraDB.\n\n### Fixes\n\n* **Remedy error on Windows when `nltk` binaries are downloaded.** Work around a quirk in the Windows implementation of `tempfile.NamedTemporaryFile` where accessing the temporary file by name raises `PermissionError`.\n* **Move Astra embedded_dimension to write config**\n\n## 0.14.10\n\n### Enhancements\n\n* **Update unstructured-client dependency** Change unstructured-client dependency pin back to greater than min version and updated tests that were failing given the update.\n* **`.doc` files are now supported in the `arm64` image.**. `libreoffice24` is added to the `arm64` image, meaning `.doc` files are now supported. We have follow on work planned to investigate adding `.ppt` support for `arm64` as well.\n* **Add table detection metrics: recall, precision and f1.**\n* **Remove unused _with_spans metrics.**\n\n### Features\n\n**Add Object Detection Metrics to CI** Add object detection metrics (average precision, precision, recall and f1-score) implementations.\n\n### Fixes\n\n* **Fix counting false negatives and false positives in table structure evaluation.**\n* **Fix Slack CI test** Change channel that Slack test is pointing to because previous test bot expired\n* **Remove NLTK download** Removes `nltk.download` in favor of downloading from an S3 bucket we host to mitigate CVE-2024-39705\n\n## 0.14.9\n\n### Enhancements\n\n* **Added visualization and OD model result dump for PDF** In PDF `hi_res` strategy the `analysis` parameter can be used to visualize the result of the OD model and dump the result to a file. Additionally, the visualization of bounding boxes of each layout source is rendered and saved for each page.\n* **`partition_docx()` distinguishes \"file not found\" from \"not a ZIP archive\" error.** `partition_docx()` now provides different error messages for \"file not found\" and \"file is not a ZIP archive (and therefore not a DOCX file)\". This aids diagnosis since these two conditions generally point in different directions as to the cause and fix.\n\n### Features\n\n### Fixes\n\n* **Fix a bug where multiple `soffice` processes could be attempted** Add a wait mechanism in `convert_office_doc` so that the function first checks if another `soffice` is running already: if yes wait till the other process finishes or till the wait timeout before spawning a subprocess to run `soffice`\n* **`partition()` now forwards `strategy` arg to `partition_docx()`, `partition_pptx()`, and their brokering partitioners for DOC, ODT, and PPT formats.** A `strategy` argument passed to `partition()` (or the default value \"auto\" assigned by `partition()`) is now forwarded to `partition_docx()`, `partition_pptx()`, and their brokering partitioners when those filetypes are detected.\n\n## 0.14.8\n\n### Enhancements\n\n* **Move arm64 image to wolfi-base** The `arm64` image now runs on `wolfi-base`. The `arm64` build for `wolfi-base` does not yet include `libreoffce`, and so `arm64` does not currently support processing `.doc`, `.ppt`, or `.xls` file. If you need to process those files on `arm64`, use the legacy `rockylinux` image.\n\n### Features\n\n### Fixes\n\n* **Bump unstructured-inference==0.7.36** Fix `ValueError` when converting cells to html.\n* **`partition()` now forwards `strategy` arg to `partition_docx()`, `partition_ppt()`, and `partition_pptx()`.** A `strategy` argument passed to `partition()` (or the default value \"auto\" assigned by `partition()`) is now forwarded to `partition_docx()`, `partition_ppt()`, and `partition_pptx()` when those filetypes are detected.\n* **Fix missing sensitive field markers** for embedders\n\n## 0.14.7\n\n### Enhancements\n\n* **Pull from `wolfi-base` image.** The amd64 image now pulls from the `unstructured` `wolfi-base` image to avoid duplication of dependency setup steps.\n* **Fix windows temp file.** Make the creation of a temp file in unstructured/partition/pdf_image/ocr.py windows compatible.\n\n### Features\n\n* **Expose conversion functions for tables** Adds public functions to convert tables from HTML to the Deckerd format and back\n* **Adds Kafka Source and Destination** New source and destination connector added to all CLI ingest commands to support reading from and writing to Kafka streams. Also supports Confluent Kafka.\n\n### Fixes\n\n* **Fix an error publishing docker images.** Update user in docker-smoke-test to reflect changes made by the amd64 image pull from the \"unstructured\" \"wolfi-base\" image.\n* **Fix a IndexError when partitioning a pdf with values for both `extract_image_block_types` and `starting_page_number`.\n\n## 0.14.6\n\n### Enhancements\n\n* **Bump unstructured-inference==0.7.35** Fix syntax for generated HTML tables.\n\n### Features\n\n* **tqdm ingest support** add optional flag to ingest flow to print out progress bar of each step in the process.\n\n### Fixes\n\n* **Remove deprecated `overwrite_schema` kwarg from Delta Table connector.** The `overwrite_schema` kwarg is deprecated in `deltalake>=0.18.0`. `schema_mode=` should be used now instead. `schema_mode=\"overwrite\"` is equivalent to `overwrite_schema=True` and `schema_mode=\"merge\"` is equivalent to `overwrite_schema=\"False\"`. `schema_mode` defaults to `None`. You can also now specify `engine`, which defaults to `\"pyarrow\"`. You need to specify `enginer=\"rust\"` to use `\"schema_mode\"`.\n* **Fix passing parameters to python-client** - Remove parsing list arguments to strings in passing arguments to python-client in Ingest workflow and `partition_via_api`\n* **table metric bug fix** get_element_level_alignment()now will find all the matched indices in predicted table data instead of only returning the first match in the case of multiple matches for the same gt string.\n* **fsspec connector path/permissions bug** V2 fsspec connectors were failing when defined relative filepaths had leading slash. This strips that slash to guarantee the relative path never has it.\n* **Dropbox connector internal file path bugs** Dropbox source connector currently raises exceptions when indexing files due to two issues: a path formatting idiosyncrasy of the Dropbox library and a divergence in the definition of the Dropbox libraries fs.info method, expecting a 'url' parameter rather than 'path'.\n* **update table metric evaluation to handle corrected HTML syntax for tables** This change is connected to the update in [unstructured-inference change](https://github.com/Unstructured-IO/unstructured-inference/pull/355) - fixes transforming HTML table to deckerd and internal cells format.\n\n## 0.14.5\n\n### Enhancements\n\n* **Filtering for tar extraction** Adds tar filtering to the compression module for connectors to avoid decompression malicious content in `.tar.gz` files. This was added to the Python `tarfile` lib in Python 3.12. The change only applies when using Python 3.12 and above.\n* **Use `python-oxmsg` for `partition_msg()`.** Outlook MSG emails are now partitioned using the `python-oxmsg` package which resolves some shortcomings of the prior MSG parser.\n\n### Features\n\n### Fixes\n\n* **8-bit string Outlook MSG files are parsed.** `partition_msg()` is now able to parse non-unicode Outlook MSG emails.\n* **Attachments to Outlook MSG files are extracted intact.** `partition_msg()` is now able to extract attachments without corruption.\n\n## 0.14.4\n\n### Enhancements\n\n* **Move logger error to debug level when PDFminer fails to extract text** which includes error message for Invalid dictionary construct.\n* **Add support for Pinecone serverless** Adds Pinecone serverless to the connector tests. Pinecone\n  serverless will work version versions >=0.14.2, but hadn't been tested until now.\n\n### Features\n\n- **Allow configuration of the Google Vision API endpoint** Add an environment variable to select the Google Vision API in the US or the EU.\n\n### Fixes\n\n* **Address the issue of unrecognized tables in `UnstructuredTableTransformerModel`** When a table is not recognized, the `element.metadata.text_as_html` attribute is set to an empty string.\n* **Remove root handlers in ingest logger**. Removes root handlers in ingest loggers to ensure secrets aren't accidentally exposed in Colab notebooks.\n* **Fix V2 S3 Destination Connector authentication** Fixes bugs with S3 Destination Connector where the connection config was neither registered nor properly deserialized.\n* **Clarified dependence on particular version of `python-docx`** Pinned `python-docx` version to ensure a particular method `unstructured` uses is included.\n* **Ingest preserves original file extension** Ingest V2 introduced a change that dropped the original extension for upgraded connectors. This reverts that change.\n\n## 0.14.3\n\n### Enhancements\n\n* **Move `category` field from Text class to Element class.**\n* **`partition_docx()` now supports pluggable picture sub-partitioners.** A subpartitioner that accepts a DOCX `Paragraph` and generates elements is now supported. This allows adding a custom sub-partitioner that extracts images and applies OCR or summarization for the image.\n* **Add VoyageAI embedder** Adds VoyageAI embeddings to support embedding via Voyage AI.\n\n### Features\n\n### Fixes\n\n* **Fix `partition_pdf()` to keep spaces in the text**. The control character `\\t` is now replaced with a space instead of being removed when merging inferred elements with embedded elements.\n* **Turn off XML resolve entities** Sets `resolve_entities=False` for XML parsing with `lxml`\n  to avoid text being dynamically injected into the XML document.\n* **Add backward compatibility for the deprecated pdf_infer_table_structure parameter**.\n* **Add the missing `form_extraction_skip_tables` argument to the `partition_pdf_or_image` call**.\n  to avoid text being dynamically injected into the XML document.\n* **Chromadb change from Add to Upsert using element_id to make idempotent**\n* **Diable `table_as_cells` output by default** to reduce overhead in partition; now `table_as_cells` is only produced when the env `EXTACT_TABLE_AS_CELLS` is `true`\n* **Reduce excessive logging** Change per page ocr info level logging into detail level trace logging\n* **Replace try block in `document_to_element_list` for handling HTMLDocument** Use `getattr(element, \"type\", \"\")` to get the `type` attribute of an element when it exists. This is more explicit way to handle the special case for HTML documents and prevents other types of attribute error from being silenced by the try block\n\n## 0.14.2\n\n### Enhancements\n\n* **Bump unstructured-inference==0.7.33**.\n\n### Features\n\n* **Add attribution to the `pinecone` connector**.\n\n### Fixes\n\n## 0.14.1\n\n### Enhancements\n\n* **Refactor code related to embedded text extraction**. The embedded text extraction code is moved from `unstructured-inference` to `unstructured`.\n\n### Features\n\n* **Large improvements to the ingest process:**\n  * Support for multiprocessing and async, with limits for both.\n  * Streamlined to process when mapping CLI invocations to the underlying code\n  * More granular steps introduced to give better control over process (i.e. dedicated step to uncompress files already in the local filesystem, new optional staging step before upload)\n  * Use the python client when calling the unstructured api for partitioning or chunking\n  * Saving the final content is now a dedicated destination connector (local) set as the default if none are provided. Avoids adding new files locally if uploading elsewhere.\n  * Leverage last modified date when deciding if new files should be downloaded and reprocessed.\n  * Add attribution to the `pinecone` connector\n  * **Add support for Python 3.12**. `unstructured` now works with Python 3.12!\n\n### Fixes\n\n## 0.14.0\n\n### BREAKING CHANGES\n\n* **Turn table extraction for PDFs and images off by default**. Reverting the default behavior for table extraction to \"off\" for PDFs and images. A number of users didn't realize we made the change and were impacted by slower processing times due to the extra model call for table extraction.\n\n### Enhancements\n\n* **Skip unnecessary element sorting in `partition_pdf()`**. Skip element sorting when determining whether embedded text can be extracted.\n* **Faster evaluation** Support for concurrent processing of documents during evaluation\n* **Add strategy parameter to `partition_docx()`.** Behavior of future enhancements may be sensitive the partitioning strategy. Add this parameter so `partition_docx()` is aware of the requested strategy.\n* **Add GLOBAL_WORKING_DIR and GLOBAL_WORKING_PROCESS_DIR** configuration parameteres to control temporary storage.\n\n### Features\n\n* **Add form extraction basics (document elements and placeholder code in partition)**. This is to lay the ground work for the future. Form extraction models are not currently available in the library. An attempt to use this functionality will end in a `NotImplementedError`.\n\n### Fixes\n\n* **Add missing starting_page_num param to partition_image**\n* **Make the filename and file params for partition_image and partition_pdf match the other partitioners**\n* **Fix include_slide_notes and include_page_breaks params in partition_ppt**\n* **Re-apply: skip accuracy calculation feature** Overwritten by mistake\n* **Fix type hint for paragraph_grouper param** `paragraph_grouper` can be set to `False`, but the type hint did not not reflect this previously.\n* **Remove links param from partition_pdf** `links` is extracted during partitioning and is not needed as a paramter in partition_pdf.\n* **Improve CSV delimeter detection.** `partition_csv()` would raise on CSV files with very long lines.\n* **Fix disk-space leak in `partition_doc()`.** Remove temporary file created but not removed when `file` argument is passed to `partition_doc()`.\n* **Fix possible `SyntaxError` or `SyntaxWarning` on regex patterns.** Change regex patterns to raw strings to avoid these warnings/errors in Python 3.11+.\n* **Fix disk-space leak in `partition_odt()`.** Remove temporary file created but not removed when `file` argument is passed to `partition_odt()`.\n* **AstraDB: option to prevent indexing metadata**\n* **Fix Missing py.typed**\n\n## 0.13.7\n\n### Enhancements\n\n* **Remove `page_number` metadata fields** for HTML partition until we have a better strategy to decide page counting.\n* **Extract OCRAgent.get_agent().** Generalize access to the configured OCRAgent instance beyond its use for PDFs.\n* **Add calculation of table related metrics which take into account colspans and rowspans**\n* **Evaluation: skip accuracy calculation** for files for which output and ground truth sizes differ greatly\n\n### Features\n\n* **add ability to get ratio of `cid` characters in embedded text extracted by `pdfminer`**.\n\n### Fixes\n\n* **`partition_docx()` handles short table rows.** The DOCX format allows a table row to start late and/or end early, meaning cells at the beginning or end of a row can be omitted. While there are legitimate uses for this capability, using it in practice is relatively rare. However, it can happen unintentionally when adjusting cell borders with the mouse. Accommodate this case and generate accurate `.text` and `.metadata.text_as_html` for these tables.\n* **Remedy macOS test failure not triggered by CI.** Generalize temp-file detection beyond hard-coded Linux-specific prefix.\n* **Remove unnecessary warning log for using default layout model.**\n* **Add chunking to partition_tsv** Even though partition_tsv() produces a single Table element, chunking is made available because the Table element is often larger than the desired chunk size and must be divided into smaller chunks.\n\n## 0.13.6\n\n### Enhancements\n\n### Features\n\n### Fixes\n\n- **ValueError: Invalid file (FileType.UNK) when parsing Content-Type header with charset directive** URL response Content-Type headers are now parsed according to RFC 9110.\n\n## 0.13.5\n\n### Enhancements\n\n### Features\n\n### Fixes\n\n* **KeyError raised when updating parent_id** In the past, combining `ListItem` elements could result in reusing the same memory location which then led to unexpected side effects when updating element IDs.\n* **Bump unstructured-inference==0.7.29**: table transformer predictions are now removed if confidence is below threshold\n\n## 0.13.4\n\n### Enhancements\n\n* **Unique and deterministic hash IDs for elements** Element IDs produced by any partitioning\n  function are now deterministic and unique at the document level by default. Before, hashes were\n  based only on text; however, they now also take into account the element's sequence number on a\n  page, the page's number in the document, and the document's file name.\n* **Enable remote chunking via unstructured-ingest** Chunking using unstructured-ingest was\n  previously limited to local chunking using the strategies `basic` and `by_title`. Remote chunking\n  options via the API are now accessible.\n* **Save table in cells format**. `UnstructuredTableTransformerModel` is able to return predicted table in cells format\n\n### Features\n\n* **Add a `PDF_ANNOTATION_THRESHOLD` environment variable to control the capture of embedded links in `partition_pdf()` for `fast` strategy**.\n* **Add integration with the Google Cloud Vision API**. Adds a third OCR provider, alongside Tesseract and Paddle: the Google Cloud Vision API.\n\n### Fixes\n\n* **Remove ElementMetadata.section field.**. This field was unused, not populated by any partitioners.\n\n## 0.13.3\n\n### Enhancements\n\n* **Remove duplicate image elements**. Remove image elements identified by PDFMiner that have similar bounding boxes and the same text.\n* **Add support for `start_index` in `html` links extraction**\n* **Add `strategy` arg value to `_PptxPartitionerOptions`.** This makes this paritioning option available for sub-partitioners to come that may optionally use inference or other expensive operations to improve the partitioning.\n* **Support pluggable sub-partitioner for PPTX Picture shapes.** Use a distinct sub-partitioner for partitioning PPTX Picture (image) shapes and allow the default picture sub-partitioner to be replaced at run-time by one of the user's choosing.\n* **Introduce `starting_page_number` parameter to partitioning functions** It applies to those partitioners which support `page_number` in element's metadata: PDF, TIFF, XLSX, DOC, DOCX, PPT, PPTX.\n* **Redesign the internal mechanism of assigning element IDs** This allows for further enhancements related to element IDs such as deterministic and document-unique hashes. The way partitioning functions operate hasn't changed, which means `unique_element_ids` continues to be `False` by default, utilizing text hashes.\n\n### Features\n\n### Fixes\n\n* **Add support for extracting text from tag tails in HTML**. This fix adds ability to generate separate elements using tag tails.\n* **Add support for extracting text from `<b>` tags in HTML** Now `partition_html()` can extract text from `<b>` tags inside container tags (like `<div>`, `<pre>`).\n* **Fix pip-compile make target** Missing base.in dependency missing from requirments make file added\n\n## 0.13.2\n\n### Enhancements\n\n### Features\n\n### Fixes\n\n* **Brings back missing word list files** that caused `partition` failures in 0.13.1.\n\n## 0.13.1\n\n### Enhancements\n\n* **Drop constraint on pydantic, supporting later versions** All dependencies has pydantic pinned at an old version. This explicit pin was removed, allowing the latest version to be pulled in when requirements are compiled.\n\n### Features\n\n* **Add a set of new `ElementType`s to extend future element types**\n\n### Fixes\n\n* **Fix `partition_html()` swallowing some paragraphs**. The `partition_html()` only considers elements with limited depth to avoid becoming the text representation of a giant div. This fix increases the limit value.\n* **Fix SFTP** Adds flag options to SFTP connector on whether to use ssh keys / agent, with flag values defaulting to False. This is to prevent looking for ssh files when using username and password. Currently, username and password are required, making that always the case.\n\n## 0.13.0\n\n### Enhancements\n\n* **Add `.metadata.is_continuation` to text-split chunks.** `.metadata.is_continuation=True` is added to second-and-later chunks formed by text-splitting an oversized `Table` element but not to their counterpart `Text` element splits. Add this indicator for `CompositeElement` to allow text-split continuation chunks to be identified for downstream processes that may wish to skip intentionally redundant metadata values in continuation chunks.\n* **Add `compound_structure_acc` metric to table eval.** Add a new property to `unstructured.metrics.table_eval.TableEvaluation`: `composite_structure_acc`, which is computed from the element level row and column index and content accuracy scores\n* **Add `.metadata.orig_elements` to chunks.** `.metadata.orig_elements: list[Element]` is added to chunks during the chunking process (when requested) to allow access to information from the elements each chunk was formed from. This is useful for example to recover metadata fields that cannot be consolidated to a single value for a chunk, like `page_number`, `coordinates`, and `image_base64`.\n* **Add `--include_orig_elements` option to Ingest CLI.** By default, when chunking, the original elements used to form each chunk are added to `chunk.metadata.orig_elements` for each chunk. * The `include_orig_elements` parameter allows the user to turn off this behavior to produce a smaller payload when they don't need this metadata.\n* **Add Google VertexAI embedder** Adds VertexAI embeddings to support embedding via Google Vertex AI.\n\n### Features\n\n* **Chunking populates `.metadata.orig_elements` for each chunk.** This behavior allows the text and metadata of the elements combined to make each chunk to be accessed. This can be important for example to recover metadata such as `.coordinates` that cannot be consolidated across elements and so is dropped from chunks. This option is controlled by the `include_orig_elements` parameter to `partition_*()` or to the chunking functions. This option defaults to `True` so original-elements are preserved by default. This behavior is not yet supported via the REST APIs or SDKs but will be in a closely subsequent PR to other `unstructured` repositories. The original elements will also not serialize or deserialize yet; this will also be added in a closely subsequent PR.\n* **Add Clarifai destination connector** Adds support for writing partitioned and chunked documents into Clarifai.\n\n### Fixes\n\n* **Fix `clean_pdfminer_inner_elements()` to remove only pdfminer (embedded) elements merged with inferred elements**. Previously, some embedded elements were removed even if they were not merged with inferred elements. Now, only embedded elements that are already merged with inferred elements are removed.\n* **Clarify IAM Role Requirement for GCS Platform Connectors**. The GCS Source Connector requires Storage Object Viewer and GCS Destination Connector requires Storage Object Creator IAM roles.\n* **Change table extraction defaults** Change table extraction defaults in favor of using `skip_infer_table_types` parameter and reflect these changes in documentation.\n* **Fix OneDrive dates with inconsistent formatting** Adds logic to conditionally support dates returned by office365 that may vary in date formatting or may be a datetime rather than a string. See previous fix for SharePoint\n* **Adds tracking for AstraDB** Adds tracking info so AstraDB can see what source called their api.\n* **Support AWS Bedrock Embeddings in ingest CLI** The configs required to instantiate the bedrock embedding class are now exposed in the api and the version of boto being used meets the minimum requirement to introduce the bedrock runtime required to hit the service.\n* **Change MongoDB redacting** Original redact secrets solution is causing issues in platform. This fix uses our standard logging redact solution.\n\n## 0.12.6\n\n### Enhancements\n\n* **Improve ability to capture embedded links in `partition_pdf()` for `fast` strategy** Previously, a threshold value that affects the capture of embedded links was set to a fixed value by default. This allows users to specify the threshold value for better capturing.\n* **Refactor `add_chunking_strategy` decorator to dispatch by name.** Add `chunk()` function to be used by the `add_chunking_strategy` decorator to dispatch chunking call based on a chunking-strategy name (that can be dynamic at runtime). This decouples chunking dispatch from only those chunkers known at \"compile\" time and enables runtime registration of custom chunkers.\n* **Redefine `table_level_acc` metric for table evaluation.** `table_level_acc` now is an average of individual predicted table's accuracy. A predicted table's accuracy is defined as the sequence matching ratio between itself and its corresponding ground truth table.\n\n### Features\n\n* **Added Unstructured Platform Documentation** The Unstructured Platform is currently in beta. The documentation provides how-to guides for setting up workflow automation, job scheduling, and configuring source and destination connectors.\n\n### Fixes\n\n* **Partitioning raises on file-like object with `.name` not a local file path.** When partitioning a file using the `file=` argument, and `file` is a file-like object (e.g. io.BytesIO) having a `.name` attribute, and the value of `file.name` is not a valid path to a file present on the local filesystem, `FileNotFoundError` is raised. This prevents use of the `file.name` attribute for downstream purposes to, for example, describe the source of a document retrieved from a network location via HTTP.\n* **Fix SharePoint dates with inconsistent formatting** Adds logic to conditionally support dates returned by office365 that may vary in date formatting or may be a datetime rather than a string.\n* **Include warnings** about the potential risk of installing a version of `pandoc` which does not support RTF files + instructions that will help resolve that issue.\n* **Incorporate the `install-pandoc` Makefile recipe** into relevant stages of CI workflow, ensuring it is a version that supports RTF input files.\n* **Fix Google Drive source key** Allow passing string for source connector key.\n* **Fix table structure evaluations calculations** Replaced special value `-1.0` with `np.nan` and corrected rows filtering of files metrics basing on that.\n* **Fix Sharepoint-with-permissions test** Ignore permissions metadata, update test.\n* **Fix table structure evaluations for edge case** Fixes the issue when the prediction does not contain any table - no longer errors in such case.\n\n## 0.12.5\n\n### Enhancements\n\n### Features\n\n* Add `date_from_file_object` parameter to partition. If True and if file is provided via `file` parameter it will cause partition to infer last modified date from `file`'s content. If False, last modified metadata will be `None`.\n* **Header and footer detection for fast strategy** `partition_pdf` with `fast` strategy now\n  detects elements that are in the top or bottom 5 percent of the page as headers and footers.\n* **Add parent_element to overlapping case output** Adds parent_element to the output for `identify_overlapping_or_nesting_case` and `catch_overlapping_and_nested_bboxes` functions.\n* **Add table structure evaluation** Adds a new function to evaluate the structure of a table and return a metric that represents the quality of the table structure. This function is used to evaluate the quality of the table structure and the table contents.\n* **Add AstraDB destination connector** Adds support for writing embedded documents into an AstraDB vector database.\n* **Add OctoAI embedder** Adds support for embeddings via OctoAI.\n\n### Fixes\n\n* **Fix passing list type parameters when calling unstructured API via `partition_via_api()`** Update `partition_via_api()` to convert all list type parameters to JSON formatted strings before calling the unstructured client SDK. This will support image block extraction via `partition_via_api()`.\n* **Fix `check_connection` in opensearch, databricks, postgres, azure connectors**\n* **Fix don't treat plain text files with double quotes as JSON** If a file can be deserialized as JSON but it deserializes as a string, treat it as plain text even though it's valid JSON.\n* **Fix `check_connection` in opensearch, databricks, postgres, azure connectors**\n* **Fix cluster of bugs in `partition_xlsx()` that dropped content.** Algorithm for detecting \"subtables\" within a worksheet dropped table elements for certain patterns of populated cells such as when a trailing single-cell row appeared in a contiguous block of populated cells.\n* **Improved documentation**. Fixed broken links and improved readability on `Key Concepts` page.\n* **Rename `OpenAiEmbeddingConfig` to `OpenAIEmbeddingConfig`.**\n* **Fix partition_json() doesn't chunk.** The `@add_chunking_strategy` decorator was missing from `partition_json()` such that pre-partitioned documents serialized to JSON did not chunk when a chunking-strategy was specified.\n\n## 0.12.4\n\n### Enhancements\n\n* **Apply New Version of `black` formatting** The `black` library recently introduced a new major version that introduces new formatting conventions. This change brings code in the `unstructured` repo into compliance with the new conventions.\n* **Move ingest imports to local scopes** Moved ingest dependencies into local scopes to be able to import ingest connector classes without the need of installing imported external dependencies. This allows lightweight use of the classes (not the instances. to use the instances as intended you'll still need the dependencies).\n* **Add support for `.p7s` files** `partition_email` can now process `.p7s` files. The signature for the signed message is extracted and added to metadata.\n* **Fallback to valid content types for emails** If the user selected content type does not exist on the email message, `partition_email` now falls back to anoter valid content type if it's available.\n\n### Features\n\n* **Add .heic file partitioning** .heic image files were previously unsupported and are now supported though partition_image()\n* **Add the ability to specify an alternate OCR** implementation by implementing an `OCRAgent` interface and specify it using `OCR_AGENT` environment variable.\n* **Add Vectara destination connector** Adds support for writing partitioned documents into a Vectara index.\n* **Add ability to detect text in .docx inline shapes** extensions of docx partition, extracts text from inline shapes and includes them in paragraph's text\n\n### Fixes\n\n* **Fix `partition_pdf()` not working when using chipper model with `file`**\n* **Handle common incorrect arguments for `languages` and `ocr_languages`** Users are regularly receiving errors on the API because they are defining `ocr_languages` or `languages` with additional quotationmarks, brackets, and similar mistakes. This update handles common incorrect arguments and raises an appropriate warning.\n* **Default `hi_res_model_name` now relies on `unstructured-inference`** When no explicit `hi_res_model_name` is passed into `partition` or `partition_pdf_or_image` the default model is picked by `unstructured-inference`'s settings or os env variable `UNSTRUCTURED_HI_RES_MODEL_NAME`; it now returns the same model name regardless of `infer_table_structure`'s value; this function will be deprecated in the future and the default model name will simply rely on `unstructured-inference` and will not consider os env in a future release.\n* **Fix remove Vectara requirements from setup.py - there are no dependencies**\n* **Add missing dependency files to package manifest**. Updates the file path for the ingest\n  dependencies and adds missing extra dependencies.\n* **Fix remove Vectara requirements from setup.py - there are no dependencies **\n* **Add title to Vectara upload - was not separated out from initial connector **\n* **Fix change OpenSearch port to fix potential conflict with Elasticsearch in ingest test **\n\n## 0.12.3\n\n### Enhancements\n\n* **Driver for MongoDB connector.** Adds a driver with `unstructured` version information to the\n  MongoDB connector.\n\n### Features\n\n* **Add Databricks Volumes destination connector** Databricks Volumes connector added to ingest CLI.  Users may now use `unstructured-ingest` to write partitioned data to a Databricks Volumes storage service.\n\n### Fixes\n\n* **Fix support for different Chipper versions and prevent running PDFMiner with Chipper**\n* **Treat YAML files as text.** Adds YAML MIME types to the file detection code and treats those\n  files as text.\n* **Fix FSSpec destination connectors check_connection.** FSSpec destination connectors did not use `check_connection`. There was an error when trying to `ls` destination directory - it may not exist at the moment of connector creation. Now `check_connection` calls `ls` on bucket root and this method is called on `initialize` of destination connector.\n* **Fix databricks-volumes extra location.** `setup.py` is currently pointing to the wrong location for the databricks-volumes extra requirements. This results in errors when trying to build the wheel for unstructured. This change updates to point to the correct path.\n* **Fix uploading None values to Chroma and Pinecone.** Removes keys with None values with Pinecone and Chroma destinations. Pins Pinecone dependency\n* **Update documentation.** (i) best practice for table extration by using 'skip_infer_table_types' param, instead of 'pdf_infer_table_structure', and (ii) fixed CSS, RST issues and typo in the documentation.\n* **Fix postgres storage of link_texts.** Formatting of link_texts was breaking metadata storage.\n\n## 0.12.2\n\n### Enhancements\n\n### Features\n\n### Fixes\n\n* **Fix index error in table processing.** Bumps the `unstructured-inference` version to address and\n  index error that occurs on some tables in the table transformer object.\n\n## 0.12.1\n\n### Enhancements\n\n* **Allow setting image block crop padding parameter** In certain circumstances, adjusting the image block crop padding can improve image block extraction by preventing extracted image blocks from being clipped.\n* **Add suport for bitmap images in `partition_image`** Adds support for `.bmp` files in\n  `partition`, `partition_image`, and `detect_filetype`.\n* **Keep all image elements when using \"hi_res\" strategy** Previously, `Image` elements with small chunks of text were ignored unless the image block extraction parameters (`extract_images_in_pdf` or `extract_image_block_types`) were specified. Now, all image elements are kept regardless of whether the image block extraction parameters are specified.\n* **Add filetype detection for `.wav` files.** Add filetpye detection for `.wav` files.\n* **Add \"basic\" chunking strategy.** Add baseline chunking strategy that includes all shared chunking behaviors without breaking chunks on section or page boundaries.\n* **Add overlap option for chunking.** Add option to overlap chunks. Intra-chunk and inter-chunk overlap are requested separately. Intra-chunk overlap is applied only to the second and later chunks formed by text-splitting an oversized chunk. Inter-chunk overlap may also be specified; this applies overlap between \"normal\" (not-oversized) chunks.\n* **Salesforce connector accepts private key path or value.** Salesforce parameter `private-key-file` has been renamed to `private-key`. Private key can be provided as path to file or file contents.\n* **Update documentation**: (i) added verbiage about the free API cap limit, (ii) added deprecation warning on ``Staging`` bricks in favor of ``Destination Connectors``, (iii) added warning and code examples to use the SaaS API Endpoints using CLI-vs-SDKs, (iv) fixed example pages formatting, (v) added deprecation on ``model_name`` in favor of ``hi_res_model_name``, (vi) added ``extract_images_in_pdf`` usage in ``partition_pdf`` section, (vii) reorganize and improve the documentation introduction section, and (viii) added PDF table extraction best practices.\n* **Add \"basic\" chunking to ingest CLI.** Add options to ingest CLI allowing access to the new \"basic\" chunking strategy and overlap options.\n* **Make Elasticsearch Destination connector arguments optional.** Elasticsearch Destination connector write settings are made optional and will rely on default values when not specified.\n* **Normalize Salesforce artifact names.** Introduced file naming pattern present in other connectors to Salesforce connector.\n* **Install Kapa AI chatbot.** Added Kapa.ai website widget on the documentation.\n\n### Features\n\n* **MongoDB Source Connector.** New source connector added to all CLI ingest commands to support downloading/partitioning files from MongoDB.\n* **Add OpenSearch source and destination connectors.** OpenSearch, a fork of Elasticsearch, is a popular storage solution for various functionality such as search, or providing intermediary caches within data pipelines. Feature: Added OpenSearch source connector to support downloading/partitioning files. Added OpenSearch destination connector to be able to ingest documents from any supported source, embed them and write the embeddings / documents into OpenSearch.\n\n### Fixes\n\n* **Fix GCS connector converting JSON to string with single quotes.** FSSpec serialization caused conversion of JSON token to string with single quotes. GCS requires token in form of dict so this format is now assured.\n* **Pin version of unstructured-client** Set minimum version of unstructured-client to avoid raising a TypeError when passing `api_key_auth` to `UnstructuredClient`\n* **Fix the serialization of the Pinecone destination connector.** Presence of the PineconeIndex object breaks serialization due to TypeError: cannot pickle '_thread.lock' object. This removes that object before serialization.\n* **Fix the serialization of the Elasticsearch destination connector.** Presence of the _client object breaks serialization due to TypeError: cannot pickle '_thread.lock' object. This removes that object before serialization.\n* **Fix the serialization of the Postgres destination connector.** Presence of the _client object breaks serialization due to TypeError: cannot pickle '_thread.lock' object. This removes that object before serialization.\n* **Fix documentation and sample code for Chroma.** Was pointing to wrong examples..\n* **Fix flatten_dict to be able to flatten tuples inside dicts** Update flatten_dict function to support flattening tuples inside dicts. This is necessary for objects like Coordinates, when the object is not written to the disk, therefore not being converted to a list before getting flattened (still being a tuple).\n* **Fix the serialization of the Chroma destination connector.** Presence of the ChromaCollection object breaks serialization due to TypeError: cannot pickle 'module' object. This removes that object before serialization.\n* **Fix fsspec connectors returning version as integer.** Connector data source versions should always be string values, however we were using the integer checksum value for the version for fsspec connectors. This casts that value to a string.\n\n## 0.12.0\n\n### Enhancements\n\n* **Drop support for python3.8** All dependencies are now built off of the minimum version of python being `3.10`\n\n## 0.11.9\n\n### Enhancements\n\n* **Rename kwargs related to extracting image blocks** Rename the kwargs related to extracting image blocks for consistency and API usage.\n\n### Features\n\n* **Add PostgreSQL/SQLite destination connector** PostgreSQL and SQLite connector added to ingest CLI.  Users may now use `unstructured-ingest` to write partitioned data to a PostgreSQL or SQLite database. And write embeddings to PostgreSQL pgvector database.\n\n### Fixes\n\n* **Handle users providing fully spelled out languages** Occasionally some users are defining the `languages` param as a fully spelled out language instead of a language code. This adds a dictionary for common languages so those small mistakes are caught and silently fixed.\n* **Fix unequal row-length in HTMLTable.text_as_html.** Fixes to other aspects of partition_html() in v0.11 allowed unequal cell-counts in table rows. Make the cells in each row correspond 1:1 with cells in the original table row. This fix also removes \"noise\" cells resulting from HTML-formatting whitespace and eliminates the \"column-shifting\" of cells that previously resulted from noise-cells.\n* **Fix MongoDB connector URI password redaction.** MongoDB documentation states that characters `$ : / ? # [ ] @` must be percent encoded. URIs with password containing such special character were not redacted.\n\n## 0.11.8\n\n### Enhancements\n\n* **Add SaaS API User Guide.** This documentation serves as a guide for Unstructured SaaS API users to register, receive an API key and URL, and manage your account and billing information.\n* **Add inter-chunk overlap capability.** Implement overlap between chunks. This applies to all chunks prior to any text-splitting of oversized chunks so is a distinct behavior; overlap at text-splits of oversized chunks is independent of inter-chunk overlap (distinct chunk boundaries) and can be requested separately. Note this capability is not yet available from the API but will shortly be made accessible using a new `overlap_all` kwarg on partition functions.\n\n### Features\n\n### Fixes\n\n## 0.11.7\n\n### Enhancements\n\n* **Add intra-chunk overlap capability.** Implement overlap for split-chunks where text-splitting is used to divide an oversized chunk into two or more chunks that fit in the chunking window. Note this capability is not yet available from the API but will shortly be made accessible using a new `overlap` kwarg on partition functions.\n* **Update encoders to leverage dataclasses** All encoders now follow a class approach which get annotated with the dataclass decorator. Similar to the connectors, it uses a nested dataclass for the configs required to configure a client as well as a field/property approach to cache the client. This makes sure any variable associated with the class exists as a dataclass field.\n\n### Features\n\n* **Add Qdrant destination connector.** Adds support for writing documents and embeddings into a Qdrant collection.\n* **Store base64 encoded image data in metadata fields.** Rather than saving to file, stores base64 encoded data of the image bytes and the mimetype for the image in metadata fields: `image_base64` and `image_mime_type` (if that is what the user specifies by some other param like `pdf_extract_to_payload`). This would allow the API to have parity with the library.\n\n### Fixes\n\n* **Fix table structure metric script** Update the call to table agent to now provide OCR tokens as required\n* **Fix element extraction not working when using \"auto\" strategy for pdf and image** If element extraction is specified, the \"auto\" strategy falls back to the \"hi_res\" strategy.\n* **Fix a bug passing a custom url to `partition_via_api`** Users that self host the api were not able to pass their custom url to `partition_via_api`.\n\n## 0.11.6\n\n### Enhancements\n\n* **Update the layout analysis script.** The previous script only supported annotating `final` elements. The updated script also supports annotating `inferred` and `extracted` elements.\n* **AWS Marketplace API documentation**: Added the user guide, including setting up VPC and CloudFormation, to deploy Unstructured API on AWS platform.\n* **Azure Marketplace API documentation**: Improved the user guide to deploy Azure Marketplace API by adding references to Azure documentation.\n* **Integration documentation**: Updated URLs for the `staging_for` bricks\n\n### Features\n\n* **Partition emails with base64-encoded text.** Automatically handles and decodes base64 encoded text in emails with content type `text/plain` and `text/html`.\n* **Add Chroma destination connector** Chroma database connector added to ingest CLI.  Users may now use `unstructured-ingest` to write partitioned/embedded data to a Chroma vector database.\n* **Add Elasticsearch destination connector.** Problem: After ingesting data from a source, users might want to move their data into a destination. Elasticsearch is a popular storage solution for various functionality such as search, or providing intermediary caches within data pipelines. Feature: Added Elasticsearch destination connector to be able to ingest documents from any supported source, embed them and write the embeddings / documents into Elasticsearch.\n\n### Fixes\n\n* **Enable --fields argument omission for elasticsearch connector** Solves two bugs where removing the optional parameter --fields broke the connector due to an integer processing error and using an elasticsearch config for a destination connector resulted in a serialization issue when optional parameter --fields was not provided.\n* **Add hi_res_model_name** Adds kwarg to relevant functions and add comments that model_name is to be deprecated.\n\n## 0.11.5\n\n### Enhancements\n\n### Features\n\n### Fixes\n\n* **Fix `partition_pdf()` and `partition_image()` importation issue.** Reorganize `pdf.py` and `image.py` modules to be consistent with other types of document import code.\n\n## 0.11.4\n\n### Enhancements\n\n* **Refactor image extraction code.** The image extraction code is moved from `unstructured-inference` to `unstructured`.\n* **Refactor pdfminer code.** The pdfminer code is moved from `unstructured-inference` to `unstructured`.\n* **Improve handling of auth data for fsspec connectors.** Leverage an extension of the dataclass paradigm to support a `sensitive` annotation for fields related to auth (i.e. passwords, tokens). Refactor all fsspec connectors to use explicit access configs rather than a generic dictionary.\n* **Add glob support for fsspec connectors** Similar to the glob support in the ingest local source connector, similar filters are now enabled on all fsspec based source connectors to limit files being partitioned.\n* Define a constant for the splitter \"+\" used in tesseract ocr languages.\n\n### Features\n\n* **Save tables in PDF's separately as images.** The \"table\" elements are saved as `table-<pageN>-<tableN>.jpg`. This filename is presented in the `image_path` metadata field for the Table element. The default would be to not do this.\n* **Add Weaviate destination connector** Weaviate connector added to ingest CLI.  Users may now use `unstructured-ingest` to write partitioned data from over 20 data sources (so far) to a Weaviate object collection.\n* **Sftp Source Connector.** New source connector added to support downloading/partitioning files from Sftp.\n\n### Fixes\n\n* **Fix pdf `hi_res` partitioning failure when pdfminer fails.** Implemented logic to fall back to the \"inferred_layout + OCR\" if pdfminer fails in the `hi_res` strategy.\n* **Fix a bug where image can be scaled too large for tesseract** Adds a limit to prevent auto-scaling an image beyond the maximum size `tesseract` can handle for ocr layout detection\n* **Update partition_csv to handle different delimiters** CSV files containing both non-comma delimiters and commas in the data were throwing an error in Pandas. `partition_csv` now identifies the correct delimiter before the file is processed.\n* **partition returning cid code in `hi_res`** occasionally pdfminer can fail to decode the text in an pdf file and return cid code as text. Now when this happens the text from OCR is used.\n\n## 0.11.2\n\n### Enhancements\n\n* **Updated Documentation**: (i) Added examples, and (ii) API Documentation, including Usage, SDKs, Azure Marketplace, and parameters and validation errors.\n\n### Features\n\n* * **Add Pinecone destination connector.** Problem: After ingesting data from a source, users might want to produce embeddings for their data and write these into a vector DB. Pinecone is an option among these vector databases. Feature: Added Pinecone destination connector to be able to ingest documents from any supported source, embed them and write the embeddings / documents into Pinecone.\n\n### Fixes\n\n* **Process chunking parameter names in ingest correctly** Solves a bug where chunking parameters weren't being processed and used by ingest cli by renaming faulty parameter names and prepends; adds relevant parameters to ingest pinecone test to verify that the parameters are functional.\n\n## 0.11.1\n\n### Enhancements\n\n* **Use `pikepdf` to repair invalid PDF structure** for PDFminer when we see error `PSSyntaxError` when PDFminer opens the document and creates the PDFminer pages object or processes a single PDF page.\n* **Batch Source Connector support** For instances where it is more optimal to read content from a source connector in batches, a new batch ingest doc is added which created multiple ingest docs after reading them in in batches per process.\n\n### Features\n\n* **Staging Brick for Coco Format** Staging brick which converts a list of Elements into Coco Format.\n* **Adds HubSpot connector** Adds connector to retrieve call, communications, emails, notes, products and tickets from HubSpot\n\n### Fixes\n\n* **Do not extract text of `<style>` tags in HTML.** `<style>` tags containing CSS in invalid positions previously contributed to element text. Do not consider text node of a `<style>` element as textual content.\n* **Fix DOCX merged table cell repeats cell text.** Only include text for a merged cell, not for each underlying cell spanned by the merge.\n* **Fix tables not extracted from DOCX header/footers.** Headers and footers in DOCX documents skip tables defined in the header and commonly used for layout/alignment purposes. Extract text from tables as a string and include in the `Header` and `Footer` document elements.\n* **Fix output filepath for fsspec-based source connectors.** Previously the base directory was being included in the output filepath unnecessarily.\n\n## 0.11.0\n\n### Enhancements\n\n* **Add a class for the strategy constants.** Add a class `PartitionStrategy` for the strategy constants and use the constants to replace strategy strings.\n* **Temporary Support for paddle language parameter.** User can specify default langage code for paddle with ENV `DEFAULT_PADDLE_LANG` before we have the language mapping for paddle.\n* **Improve DOCX page-break fidelity.** Improve page-break fidelity such that a paragraph containing a page-break is split into two elements, one containing the text before the page-break and the other the text after. Emit the PageBreak element between these two and assign the correct page-number (n and n+1 respectively) to the two textual elements.\n\n### Features\n\n* **Add ad-hoc fields to `ElementMetadata` instance.** End-users can now add their own metadata fields simply by assigning to an element-metadata attribute-name of their choice, like `element.metadata.coefficient = 0.58`. These fields will round-trip through JSON and can be accessed with dotted notation.\n* **MongoDB Destination Connector.** New destination connector added to all CLI ingest commands to support writing partitioned json output to mongodb.\n\n### Fixes\n\n* **Fix `TYPE_TO_TEXT_ELEMENT_MAP`.** Updated `Figure` mapping from `FigureCaption` to `Image`.\n* **Handle errors when extracting PDF text** Certain pdfs throw unexpected errors when being opened by `pdfminer`, causing `partition_pdf()` to fail. We expect to be able to partition smoothly using an alternative strategy if text extraction doesn't work.  Added exception handling to handle unexpected errors when extracting pdf text and to help determine pdf strategy.\n* **Fix `fast` strategy fall back to `ocr_only`** The `fast` strategy should not fall back to a more expensive strategy.\n* **Remove default user ./ssh folder** The default notebook user during image build would create the known_hosts file with incorrect ownership, this is legacy and no longer needed so it was removed.\n* **Include `languages` in metadata when partitioning `strategy=hi_res` or `fast`** User defined `languages` was previously used for text detection, but not included in the resulting element metadata for some strategies. `languages` will now be included in the metadata regardless of partition strategy for pdfs and images.\n* **Handle a case where Paddle returns a list item in ocr_data as None** In partition, while parsing PaddleOCR data, it was assumed that PaddleOCR does not return None for any list item in ocr_data. Removed the assumption by skipping the text region whenever this happens.\n* **Fix some pdfs returning `KeyError: 'N'`** Certain pdfs were throwing this error when being opened by pdfminer. Added a wrapper function for pdfminer that allows these documents to be partitioned.\n* **Fix mis-splits on `Table` chunks.** Remedies repeated appearance of full `.text_as_html` on metadata of each `TableChunk` split from a `Table` element too large to fit in the chunking window.\n* **Import tables_agent from inference** so that we don't have to initialize a global table agent in unstructured OCR again\n* **Fix empty table is identified as bulleted-table.** A table with no text content was mistakenly identified as a bulleted-table and processed by the wrong branch of the initial HTML partitioner.\n* **Fix partition_html() emits empty (no text) tables.** A table with cells nested below a `<thead>` or `<tfoot>` element was emitted as a table element having no text and unparseable HTML in `element.metadata.text_as_html`. Do not emit empty tables to the element stream.\n* **Fix HTML `element.metadata.text_as_html` contains spurious `<br>` elements in invalid locations.** The HTML generated for the `text_as_html` metadata for HTML tables contained `<br>` elements invalid locations like between `<table>` and `<tr>`. Change the HTML generator such that these do not appear.\n* **Fix HTML table cells enclosed in `<thead>` and `<tfoot>` elements are dropped.** HTML table cells nested in a `<thead>` or `<tfoot>` element were not detected and the text in those cells was omitted from the table element text and `.text_as_html`. Detect table rows regardless of the semantic tag they may be nested in.\n* **Remove whitespace padding from `.text_as_html`.** `tabulate` inserts padding spaces to achieve visual alignment of columns in HTML tables it generates. Add our own HTML generator to do this simple job and omit that padding as well as newlines (\"\\n\") used for human readability.\n* **Fix local connector with absolute input path** When passed an absolute filepath for the input document path, the local connector incorrectly writes the output file to the input file directory. This fixes such that the output in this case is written to `output-dir/input-filename.json`\n\n## 0.10.30\n\n### Enhancements\n\n* **Support nested DOCX tables.** In DOCX, like HTML, a table cell can itself contain a table. In this case, create nested HTML tables to reflect that structure and create a plain-text table with captures all the text in nested tables, formatting it as a reasonable facsimile of a table.\n* **Add connection check to ingest connectors** Each source and destination connector now support a `check_connection()` method which makes sure a valid connection can be established with the source/destination given any authentication credentials in a lightweight request.\n\n### Features\n\n* **Add functionality to do a second OCR on cropped table images.** Changes to the values for scaling ENVs affect entire page OCR output(OCR regression) so we now do a second OCR for tables.\n* **Adds ability to pass timeout for a request when partitioning via a `url`.** `partition` now accepts a new optional parameter `request_timeout` which if set will prevent any `requests.get` from hanging indefinitely and instead will raise a timeout error. This is useful when partitioning a url that may be slow to respond or may not respond at all.\n\n### Fixes\n\n* **Fix logic that determines pdf auto strategy.** Previously, `_determine_pdf_auto_strategy` returned `hi_res` strategy only if `infer_table_structure` was true. It now returns the `hi_res` strategy if either `infer_table_structure` or `extract_images_in_pdf` is true.\n* **Fix invalid coordinates when parsing tesseract ocr data.** Previously, when parsing tesseract ocr data, the ocr data had invalid bboxes if zoom was set to `0`. A logical check is now added to avoid such error.\n* **Fix ingest partition parameters not being passed to the api.** When using the --partition-by-api flag via unstructured-ingest, none of the partition arguments are forwarded, meaning that these options are disregarded. With this change, we now pass through all of the relevant partition arguments to the api. This allows a user to specify all of the same partition arguments they would locally and have them respected when specifying --partition-by-api.\n* **Support tables in section-less DOCX.** Generalize solution for MS Chat Transcripts exported as DOCX by including tables in the partitioned output when present.\n* **Support tables that contain only numbers when partitioning via `ocr_only`** Tables that contain only numbers are returned as floats in a pandas.DataFrame when the image is converted from `.image_to_data()`. An AttributeError was raised downstream when trying to `.strip()` the floats.\n* **Improve DOCX page-break detection.** DOCX page breaks are reliably indicated by `w:lastRenderedPageBreak` elements present in the document XML. Page breaks are NOT reliably indicated by \"hard\" page-breaks inserted by the author and when present are redundant to a `w:lastRenderedPageBreak` element so cause over-counting if used. Use rendered page-breaks only.\n\n## 0.10.29\n\n### Enhancements\n\n* **Adds include_header argument for partition_csv and partition_tsv** Now supports retaining header rows in CSV and TSV documents element partitioning.\n* **Add retry logic for all source connectors** All http calls being made by the ingest source connectors have been isolated and wrapped by the `SourceConnectionNetworkError` custom error, which triggers the retry logic, if enabled, in the ingest pipeline.\n* **Google Drive source connector supports credentials from memory** Originally, the connector expected a filepath to pull the credentials from when creating the client. This was expanded to support passing that information from memory as a dict if access to the file system might not be available.\n* **Add support for generic partition configs in ingest cli** Along with the explicit partition options supported by the cli, an `additional_partition_args` arg was added to allow users to pass in any other arguments that should be added when calling partition(). This helps keep any changes to the input parameters of the partition() exposed in the CLI.\n* **Map full output schema for table-based destination connectors** A full schema was introduced to map the type of all output content from the json partition output and mapped to a flattened table structure to leverage table-based destination connectors. The delta table destination connector was updated at the moment to take advantage of this.\n* **Incorporate multiple embedding model options into ingest, add diff test embeddings** Problem: Ingest pipeline already supported embedding functionality, however users might want to use different types of embedding providers. Enhancement: Extend ingest pipeline so that users can specify and embed via a particular embedding provider from a range of options. Also adds a diff test to compare output from an embedding module with the expected output\n\n### Features\n\n* **Allow setting table crop parameter** In certain circumstances, adjusting the table crop padding may improve table.\n\n### Fixes\n\n* **Fixes `partition_text` to prevent empty elements** Adds a check to filter out empty bullets.\n* **Handle empty string for `ocr_languages` with values for `languages`** Some API users ran into an issue with sending `languages` params because the API defaulted to also using an empty string for `ocr_languages`. This update handles situations where `languages` is defined and `ocr_languages` is an empty string.\n* **Fix PDF tried to loop through None** Previously the PDF annotation extraction tried to loop through `annots` that resolved out as None. A logical check added to avoid such error.\n* **Ingest session handler not being shared correctly** All ingest docs that leverage the session handler should only need to set it once per process. It was recreating it each time because the right values weren't being set nor available given how dataclasses work in python.\n* **Ingest download-only fix.** Previously the download only flag was being checked after the doc factory pipeline step, which occurs before the files are actually downloaded by the source node. This check was moved after the source node to allow for the files to be downloaded first before exiting the pipeline.\n* **Fix flaky chunk-metadata.** Prior implementation was sensitive to element order in the section resulting in metadata values sometimes being dropped. Also, not all metadata items can be consolidated across multiple elements (e.g. coordinates) and so are now dropped from consolidated metadata.\n* **Fix tesseract error `Estimating resolution as X`** leaded by invalid language parameters input. Proceed with defalut language `eng` when `lang.py` fails to find valid language code for tesseract, so that we don't pass an empty string to tesseract CLI and raise an exception in downstream.\n\n## 0.10.28\n\n### Enhancements\n\n* **Add table structure evaluation helpers** Adds functions to evaluate the similarity between predicted table structure and actual table structure.\n* **Use `yolox` by default for table extraction when partitioning pdf/image** `yolox` model provides higher recall of the table regions than the quantized version and it is now the default element detection model when `infer_table_structure=True` for partitioning pdf/image files\n* **Remove pdfminer elements from inside tables** Previously, when using `hi_res` some elements where extracted using pdfminer too, so we removed pdfminer from the tables pipeline to avoid duplicated elements.\n* **Fsspec downstream connectors** New destination connector added to ingest CLI, users may now use `unstructured-ingest` to write to any of the following:\n  * Azure\n  * Box\n  * Dropbox\n  * Google Cloud Service\n\n### Features\n\n* **Update `ocr_only` strategy in `partition_pdf()`** Adds the functionality to get accurate coordinate data when partitioning PDFs and Images with the `ocr_only` strategy.\n\n### Fixes\n\n* **Fixed SharePoint permissions for the fetching to be opt-in** Problem: Sharepoint permissions were trying to be fetched even when no reletad cli params were provided, and this gave an error due to values for those keys not existing. Fix: Updated getting keys to be with .get() method and changed the \"skip-check\" to check individual cli params rather than checking the existance of a config object.\n* **Fixes issue where tables from markdown documents were being treated as text** Problem: Tables from markdown documents were being treated as text, and not being extracted as tables. Solution: Enable the `tables` extension when instantiating the `python-markdown` object. Importance: This will allow users to extract structured data from tables in markdown documents.\n* **Fix wrong logger for paddle info** Replace the logger from unstructured-inference with the logger from unstructured for paddle_ocr.py module.\n* **Fix ingest pipeline to be able to use chunking and embedding together** Problem: When ingest pipeline was using chunking and embedding together, embedding outputs were empty and the outputs of chunking couldn't be re-read into memory and be forwarded to embeddings. Fix: Added CompositeElement type to TYPE_TO_TEXT_ELEMENT_MAP to be able to process CompositeElements with unstructured.staging.base.isd_to_elements\n* **Fix unnecessary mid-text chunk-splitting.** The \"pre-chunker\" did not consider separator blank-line (\"\\n\\n\") length when grouping elements for a single chunk. As a result, sections were frequently over-populated producing a over-sized chunk that required mid-text splitting.\n* **Fix frequent dissociation of title from chunk.** The sectioning algorithm included the title of the next section with the prior section whenever it would fit, frequently producing association of a section title with the prior section and dissociating it from its actual section. Fix this by performing combination of whole sections only.\n* **Fix PDF attempt to get dict value from string.** Fixes a rare edge case that prevented some PDF's from being partitioned. The `get_uris_from_annots` function tried to access the dictionary value of a string instance variable. Assign `None` to the annotation variable if the instance type is not dictionary to avoid the erroneous attempt.\n\n## 0.10.27\n\n### Enhancements\n\n* **Leverage dict to share content across ingest pipeline** To share the ingest doc content across steps in the ingest pipeline, this was updated to use a multiprocessing-safe dictionary so changes get persisted and each step has the option to modify the ingest docs in place.\n\n### Features\n\n### Fixes\n\n* **Removed `ebooklib` as a dependency** `ebooklib` is licensed under AGPL3, which is incompatible with the Apache 2.0 license. Thus it is being removed.\n* **Caching fixes in ingest pipeline** Previously, steps like the source node were not leveraging parameters such as `re_download` to dictate if files should be forced to redownload rather than use what might already exist locally.\n\n## 0.10.26\n\n### Enhancements\n\n* **Add text CCT CI evaluation workflow** Adds cct text extraction evaluation metrics to the current ingest workflow to measure the performance of each file extracted as well as aggregated-level performance.\n\n### Features\n\n* **Functionality to catch and classify overlapping/nested elements** Method to identify overlapping-bboxes cases within detected elements in a document. It returns two values: a boolean defining if there are overlapping elements present, and a list reporting them with relevant metadata. The output includes information about the `overlapping_elements`, `overlapping_case`, `overlapping_percentage`, `largest_ngram_percentage`, `overlap_percentage_total`, `max_area`, `min_area`, and `total_area`.\n* **Add Local connector source metadata** python's os module used to pull stats from local file when processing via the local connector and populates fields such as last modified time, created time.\n\n### Fixes\n\n* **Fixes elements partitioned from an image file missing certain metadata** Metadata for image files, like file type, was being handled differently from other file types. This caused a bug where other metadata, like the file name, was being missed. This change brought metadata handling for image files to be more in line with the handling for other file types so that file name and other metadata fields are being captured.\n* **Adds `typing-extensions` as an explicit dependency** This package is an implicit dependency, but the module is being imported directly in `unstructured.documents.elements` so the dependency should be explicit in case changes in other dependencies lead to `typing-extensions` being dropped as a dependency.\n* **Stop passing `extract_tables` to `unstructured-inference` since it is now supported in `unstructured` instead** Table extraction previously occurred in `unstructured-inference`, but that logic, except for the table model itself, is now a part of the `unstructured` library. Thus the parameter triggering table extraction is no longer passed to the `unstructured-inference` package. Also noted the table output regression for PDF files.\n* **Fix a bug in Table partitioning** Previously the `skip_infer_table_types` variable used in `partition` was not being passed down to specific file partitioners. Now you can utilize the `skip_infer_table_types` list variable when calling `partition` to specify the filetypes for which you want to skip table extraction, or the `infer_table_structure` boolean variable on the file specific partitioning function.\n* **Fix partition docx without sections** Some docx files, like those from teams output, do not contain sections and it would produce no results because the code assumes all components are in sections. Now if no sections is detected from a document we iterate through the paragraphs and return contents found in the paragraphs.\n* **Fix out-of-order sequencing of split chunks.** Fixes behavior where \"split\" chunks were inserted at the beginning of the chunk sequence. This would produce a chunk sequence like [5a, 5b, 3a, 3b, 1, 2, 4] when sections 3 and 5 exceeded `max_characters`.\n* **Deserialization of ingest docs fixed** When ingest docs are being deserialized as part of the ingest pipeline process (cli), there were certain fields that weren't getting persisted (metadata and date processed). The from_dict method was updated to take these into account and a unit test added to check.\n* **Map source cli command configs when destination set** Due to how the source connector is dynamically called when the destination connector is set via the CLI, the configs were being set incorrectoy, causing the source connector to break. The configs were fixed and updated to take into account Fsspec-specific connectors.\n\n## 0.10.25\n\n### Enhancements\n\n* **Duplicate CLI param check** Given that many of the options associated with the `Click` based cli ingest commands are added dynamically from a number of configs, a check was incorporated to make sure there were no duplicate entries to prevent new configs from overwriting already added options.\n* **Ingest CLI refactor for better code reuse** Much of the ingest cli code can be templated and was a copy-paste across files, adding potential risk. Code was refactored to use a base class which had much of the shared code templated.\n\n### Features\n\n* **Table OCR refactor** support Table OCR with pre-computed OCR data to ensure we only do one OCR for entrie document. User can specify\n  ocr agent tesseract/paddle in environment variable `OCR_AGENT` for OCRing the entire document.\n* **Adds accuracy function** The accuracy scoring was originally an option under `calculate_edit_distance`. For easy function call, it is now a wrapper around the original function that calls edit_distance and return as \"score\".\n* **Adds HuggingFaceEmbeddingEncoder** The HuggingFace Embedding Encoder uses a local embedding model as opposed to using an API.\n* **Add AWS bedrock embedding connector** `unstructured.embed.bedrock` now provides a connector to use AWS bedrock's `titan-embed-text` model to generate embeddings for elements. This features requires valid AWS bedrock setup and an internet connectionto run.\n\n### Fixes\n\n* **Import PDFResourceManager more directly** We were importing `PDFResourceManager` from `pdfminer.converter` which was causing an error for some users. We changed to import from the actual location of `PDFResourceManager`, which is `pdfminer.pdfinterp`.\n* **Fix language detection of elements with empty strings** This resolves a warning message that was raised by `langdetect` if the language was attempted to be detected on an empty string. Language detection is now skipped for empty strings.\n* **Fix chunks breaking on regex-metadata matches.** Fixes \"over-chunking\" when `regex_metadata` was used, where every element that contained a regex-match would start a new chunk.\n* **Fix regex-metadata match offsets not adjusted within chunk.** Fixes incorrect regex-metadata match start/stop offset in chunks where multiple elements are combined.\n* **Map source cli command configs when destination set** Due to how the source connector is dynamically called when the destination connector is set via the CLI, the configs were being set incorrectoy, causing the source connector to break. The configs were fixed and updated to take into account Fsspec-specific connectors.\n* **Fix metrics folder not discoverable** Fixes issue where unstructured/metrics folder is not discoverable on PyPI by adding an `__init__.py` file under the folder.\n* **Fix a bug when `parition_pdf` get `model_name=None`** In API usage the `model_name` value is `None` and the `cast` function in `partition_pdf` would return `None` and lead to attribution error. Now we use `str` function to explicit convert the content to string so it is garanteed to have `starts_with` and other string functions as attributes\n* **Fix html partition fail on tables without `tbody` tag** HTML tables may sometimes just contain headers without body (`tbody` tag)\n\n## 0.10.24\n\n### Enhancements\n\n* **Improve natural reading order** Some `OCR` elements with only spaces in the text have full-page width in the bounding box, which causes the `xycut` sorting to not work as expected. Now the logic to parse OCR results removes any elements with only spaces (more than one space).\n* **Ingest compression utilities and fsspec connector support** Generic utility code added to handle files that get pulled from a source connector that are either tar or zip compressed and uncompress them locally. This is then processed using a local source connector. Currently this functionality has been incorporated into the fsspec connector and all those inheriting from it (currently: Azure Blob Storage, Google Cloud Storage, S3, Box, and Dropbox).\n* **Ingest destination connectors support for writing raw list of elements** Along with the default write method used in the ingest pipeline to write the json content associated with the ingest docs, each destination connector can now also write a raw list of elements to the desired downstream location without having an ingest doc associated with it.\n\n### Features\n\n* **Adds element type percent match function** In order to evaluate the element type extracted, we add a function that calculates the matched percentage between two frequency dictionary.\n\n### Fixes\n\n* **Fix paddle model file not discoverable** Fixes issue where ocr_models/paddle_ocr.py file is not discoverable on PyPI by adding\n  an `__init__.py` file under the folder.\n* **Chipper v2 Fixes** Includes fix for a memory leak and rare last-element bbox fix. (unstructured-inference==0.7.7)\n* **Fix image resizing issue** Includes fix related to resizing images in the tables pipeline. (unstructured-inference==0.7.6)\n\n## 0.10.23\n\n### Enhancements\n\n* **Add functionality to limit precision when serializing to json** Precision for `points` is limited to 1 decimal point if coordinates[\"system\"] == \"PixelSpace\" (otherwise 2 decimal points?). Precision for `detection_class_prob` is limited to 5 decimal points.\n* **Fix csv file detection logic when mime-type is text/plain** Previously the logic to detect csv file type was considering only first row's comma count comparing with the header_row comma count and both the rows being same line the result was always true, Now the logic is changed to consider the comma's count for all the lines except first line and compare with header_row comma count.\n* **Improved inference speed for Chipper V2** API requests with 'hi_res_model_name=chipper' now have ~2-3x faster responses.\n\n### Features\n\n### Fixes\n\n* **Cleans up temporary files after conversion** Previously a file conversion utility was leaving temporary files behind on the filesystem without removing them when no longer needed. This fix helps prevent an accumulation of temporary files taking up excessive disk space.\n* **Fixes `under_non_alpha_ratio` dividing by zero** Although this function guarded against a specific cause of division by zero, there were edge cases slipping through like strings with only whitespace. This update more generally prevents the function from performing a division by zero.\n* **Fix languages default** Previously the default language was being set to English when elements didn't have text or if langdetect could not detect the language. It now defaults to None so there is not misleading information about the language detected.\n* **Fixes recursion limit error that was being raised when partitioning Excel documents of a certain size** Previously we used a recursive method to find subtables within an excel sheet. However this would run afoul of Python's recursion depth limit when there was a contiguous block of more than 1000 cells within a sheet. This function has been updated to use the NetworkX library which avoids Python recursion issues.\n\n## 0.10.22\n\n### Enhancements\n\n* **bump `unstructured-inference` to `0.7.3`** The updated version of `unstructured-inference` supports a new version of the Chipper model, as well as a cleaner schema for its output classes. Support is included for new inference features such as hierarchy and ordering.\n* **Expose skip_infer_table_types in ingest CLI.** For each connector a new `--skip-infer-table-types` parameter was added to map to the `skip_infer_table_types` partition argument. This gives more granular control to unstructured-ingest users, allowing them to specify the file types for which we should attempt table extraction.\n* **Add flag to ingest CLI to raise error if any single doc fails in pipeline** Currently if a single doc fails in the pipeline, the whole thing halts due to the error. This flag defaults to log an error but continue with the docs it can.\n* **Emit hyperlink metadata for DOCX file-type.** DOCX partitioner now adds `metadata.links`, `metadata.link_texts` and `metadata.link_urls` for elements that contain a hyperlink that points to an external resource. So-called \"jump\" links pointing to document internal locations (such as those found in a table-of-contents \"jumping\" to a chapter or section) are excluded.\n\n### Features\n\n* **Add `elements_to_text` as a staging helper function** In order to get a single clean text output from unstructured for metric calculations, automate the process of extracting text from elements using this function.\n* **Adds permissions(RBAC) data ingestion functionality for the Sharepoint connector.** Problem: Role based access control is an important component in many data storage systems. Users may need to pass permissions (RBAC) data to downstream systems when ingesting data. Feature: Added permissions data ingestion functionality to the Sharepoint connector.\n\n### Fixes\n\n* **Fixes PDF list parsing creating duplicate list items** Previously a bug in PDF list item parsing caused removal of other elements and duplication of the list item\n* **Fixes duplicated elements** Fixes issue where elements are duplicated when embeddings are generated. This will allow users to generate embeddings for their list of Elements without duplicating/breaking the orginal content.\n* **Fixes failure when flagging for embeddings through unstructured-ingest** Currently adding the embedding parameter to any connector results in a failure on the copy stage. This is resolves the issue by adding the IngestDoc to the context map in the embedding node's `run` method. This allows users to specify that connectors fetch embeddings without failure.\n* **Fix ingest pipeline reformat nodes not discoverable** Fixes issue where  reformat nodes raise ModuleNotFoundError on import. This was due to the directory was missing `__init__.py` in order to make it discoverable.\n* **Fix default language in ingest CLI** Previously the default was being set to english which injected potentially incorrect information to downstream language detection libraries. By setting the default to None allows those libraries to better detect what language the text is in the doc being processed.\n\n## 0.10.21\n\n* **Adds Scarf analytics**.\n\n## 0.10.20\n\n### Enhancements\n\n* **Add document level language detection functionality.** Adds the \"auto\" default for the languages param to all partitioners. The primary language present in the document is detected using the `langdetect` package. Additional param `detect_language_per_element` is also added for partitioners that return multiple elements. Defaults to `False`.\n* **Refactor OCR code** The OCR code for entire page is moved from unstructured-inference to unstructured. On top of continuing support for OCR language parameter, we also support two OCR processing modes, \"entire_page\" or \"individual_blocks\".\n* **Align to top left when shrinking bounding boxes for `xy-cut` sorting:** Update `shrink_bbox()` to keep top left rather than center.\n* **Add visualization script to annotate elements** This script is often used to analyze/visualize elements with coordinates (e.g. partition_pdf()).\n* **Adds data source properties to the Jira, Github and Gitlab connectors** These properties (date_created, date_modified, version, source_url, record_locator) are written to element metadata during ingest, mapping elements to information about the document source from which they derive. This functionality enables downstream applications to reveal source document applications, e.g. a link to a GDrive doc, Salesforce record, etc.\n* **Improve title detection in pptx documents** The default title textboxes on a pptx slide are now categorized as titles.\n* **Improve hierarchy detection in pptx documents** List items, and other slide text are properly nested under the slide title. This will enable better chunking of pptx documents.\n* **Refactor of the ingest cli workflow** The refactored approach uses a dynamically set pipeline with a snapshot along each step to save progress and accommodate continuation from a snapshot if an error occurs. This also allows the pipeline to dynamically assign any number of steps to modify the partitioned content before it gets written to a destination.\n* **Applies `max_characters=<n>` argument to all element types in `add_chunking_strategy` decorator** Previously this argument was only utilized in chunking Table elements and now applies to all partitioned elements if `add_chunking_strategy` decorator is utilized, further preparing the elements for downstream processing.\n* **Add common retry strategy utilities for unstructured-ingest** Dynamic retry strategy with exponential backoff added to Notion source connector.\n*\n\n### Features\n\n* **Adds `bag_of_words` and `percent_missing_text` functions** In order to count the word frequencies in two input texts and calculate the percentage of text missing relative to the source document.\n* **Adds `edit_distance` calculation metrics** In order to benchmark the cleaned, extracted text with unstructured, `edit_distance` (`Levenshtein distance`) is included.\n* **Adds detection_origin field to metadata** Problem: Currently isn't an easy way to find out how an element was created. With this change that information is added. Importance: With this information the developers and users are now able to know how an element was created to make decisions on how to use it. In order tu use this feature\n  setting UNSTRUCTURED_INCLUDE_DEBUG_METADATA=true is needed.\n* **Adds a function that calculates frequency of the element type and its depth** To capture the accuracy of element type extraction, this function counts the occurrences of each unique element type with its depth for use in element metrics.\n\n### Fixes\n\n* **Fix zero division error in annotation bbox size** This fixes the bug where we find annotation bboxes realted to an element that need to divide the intersection size between annotation bbox and element bbox by the size of the annotation bbox\n* **Fix prevent metadata module from importing dependencies from unnecessary modules** Problem: The `metadata` module had several top level imports that were only used in and applicable to code related to specific document types, while there were many general-purpose functions. As a result, general-purpose functions couldn't be used without unnecessary dependencies being installed. Fix: moved 3rd party dependency top level imports to inside the functions in which they are used and applied a decorator to check that the dependency is installed and emit a helpful error message if not.\n* **Fixes category_depth None value for Title elements** Problem: `Title` elements from `chipper` get `category_depth`= None even when `Headline` and/or `Subheadline` elements are present in the same page. Fix: all `Title` elements with `category_depth` = None should be set to have a depth of 0 instead iff there are `Headline` and/or `Subheadline` element-types present. Importance: `Title` elements should be equivalent html `H1` when nested headings are present; otherwise, `category_depth` metadata can result ambiguous within elements in a page.\n* **Tweak `xy-cut` ordering output to be more column friendly** This results in the order of elements more closely reflecting natural reading order which benefits downstream applications. While element ordering from `xy-cut` is usually mostly correct when ordering multi-column documents, sometimes elements from a RHS column will appear before elements in a LHS column. Fix: add swapped `xy-cut` ordering by sorting by X coordinate first and then Y coordinate.\n* **Fixes badly initialized Formula** Problem: YoloX contain new types of elements, when loading a document that contain formulas a new element of that class\n  should be generated, however the Formula class inherits from Element instead of Text. After this change the element is correctly created with the correct class\n  allowing the document to be loaded. Fix: Change parent class for Formula to Text. Importance: Crucial to be able to load documents that contain formulas.\n* **Fixes pdf uri error** An error was encountered when URI type of `GoToR` which refers to pdf resources outside of its own was detected since no condition catches such case. The code is fixing the issue by initialize URI before any condition check.\n\n## 0.10.19\n\n### Enhancements\n\n* **Adds XLSX document level language detection** Enhancing on top of language detection functionality in previous release, we now support language detection within `.xlsx` file type at Element level.\n* **bump `unstructured-inference` to `0.6.6`** The updated version of `unstructured-inference` makes table extraction in `hi_res` mode configurable to fine tune table extraction performance; it also improves element detection by adding a deduplication post processing step in the `hi_res` partitioning of pdfs and images.\n* **Detect text in HTML Heading Tags as Titles** This will increase the accuracy of hierarchies in HTML documents and provide more accurate element categorization. If text is in an HTML heading tag and is not a list item, address, or narrative text, categorize it as a title.\n* **Update python-based docs** Refactor docs to use the actual unstructured code rather than using the subprocess library to run the cli command itself.\n* **Adds Table support for the `add_chunking_strategy` decorator to partition functions.** In addition to combining elements under Title elements, user's can now specify the `max_characters=<n>` argument to chunk Table elements into TableChunk elements with `text` and `text_as_html` of length `<n>` characters. This means partitioned Table results are ready for use in downstream applications without any post processing.\n* **Expose endpoint url for s3 connectors** By allowing for the endpoint url to be explicitly overwritten, this allows for any non-AWS data providers supporting the s3 protocol to be supported (i.e. minio).\n\n### Features\n\n* **change default `hi_res` model for pdf/image partition to `yolox`** Now partitioning pdf/image using `hi_res` strategy utilizes `yolox_quantized` model isntead of `detectron2_onnx` model. This new default model has better recall for tables and produces more detailed categories for elements.\n* **XLSX can now reads subtables within one sheet** Problem: Many .xlsx files are not created to be read as one full table per sheet. There are subtables, text and header along with more informations to extract from each sheet. Feature: This `partition_xlsx` now can reads subtable(s) within one .xlsx sheet, along with extracting other title and narrative texts. Importance: This enhance the power of .xlsx reading to not only one table per sheet, allowing user to capture more data tables from the file, if exists.\n* **Update Documentation on Element Types and Metadata**: We have updated the documentation according to the latest element types and metadata. It includes the common and additional metadata provided by the Partitions and Connectors.\n\n### Fixes\n\n* **Fixes partition_pdf is_alnum reference bug** Problem: The `partition_pdf` when attempt to get bounding box from element experienced a reference before assignment error when the first object is not text extractable.  Fix: Switched to a flag when the condition is met. Importance: Crucial to be able to partition with pdf.\n* **Fix various cases of HTML text missing after partition**\n  Problem: Under certain circumstances, text immediately after some HTML tags will be misssing from partition result.\n  Fix: Updated code to deal with these cases.\n  Importance: This will ensure the correctness when partitioning HTML and Markdown documents.\n* **Fixes chunking when `detection_class_prob` appears in Element metadata** Problem: when `detection_class_prob` appears in Element metadata, Elements will only be combined by chunk_by_title if they have the same `detection_class_prob` value (which is rare). This is unlikely a case we ever need to support and most often results in no chunking. Fix: `detection_class_prob` is included in the chunking list of metadata keys excluded for similarity comparison. Importance: This change allows `chunk_by_title` to operate as intended for documents which include `detection_class_prob` metadata in their Elements.\n\n## 0.10.18\n\n### Enhancements\n\n* **Better detection of natural reading order in images and PDF's** The elements returned by partition better reflect natural reading order in some cases, particularly in complicated multi-column layouts, leading to better chunking and retrieval for downstream applications. Achieved by improving the `xy-cut` sorting to preprocess bboxes, shrinking all bounding boxes by 90% along x and y axes (still centered around the same center point), which allows projection lines to be drawn where not possible before if layout bboxes overlapped.\n* **Improves `partition_xml` to be faster and more memory efficient when partitioning large XML files** The new behavior is to partition iteratively to prevent loading the entire XML tree into memory at once in most use cases.\n* **Adds data source properties to SharePoint, Outlook, Onedrive, Reddit, Slack, DeltaTable connectors** These properties (date_created, date_modified, version, source_url, record_locator) are written to element metadata during ingest, mapping elements to information about the document source from which they derive. This functionality enables downstream applications to reveal source document applications, e.g. a link to a GDrive doc, Salesforce record, etc.\n* **Add functionality to save embedded images in PDF's separately as images** This allows users to save embedded images in PDF's separately as images, given some directory path. The saved image path is written to the metadata for the Image element. Downstream applications may benefit by providing users with image links from relevant \"hits.\"\n* **Azure Cognite Search destination connector** New Azure Cognitive Search destination connector added to ingest CLI.  Users may now use `unstructured-ingest` to write partitioned data from over 20 data sources (so far) to an Azure Cognitive Search index.\n* **Improves salesforce partitioning** Partitions Salesforce data as xlm instead of text for improved detail and flexibility. Partitions htmlbody instead of textbody for Salesforce emails. Importance: Allows all Salesforce fields to be ingested and gives Salesforce emails more detailed partitioning.\n* **Add document level language detection functionality.** Introduces the \"auto\" default for the languages param, which then detects the languages present in the document using the `langdetect` package. Adds the document languages as ISO 639-3 codes to the element metadata. Implemented only for the partition_text function to start.\n* **PPTX partitioner refactored in preparation for enhancement.** Behavior should be unchanged except that shapes enclosed in a group-shape are now included, as many levels deep as required (a group-shape can itself contain a group-shape).\n* **Embeddings support for the SharePoint SourceConnector via unstructured-ingest CLI** The SharePoint connector can now optionally create embeddings from the elements it pulls out during partition and upload those embeddings to Azure Cognitive Search index.\n* **Improves hierarchy from docx files by leveraging natural hierarchies built into docx documents**  Hierarchy can now be detected from an indentation level for list bullets/numbers and by style name (e.g. Heading 1, List Bullet 2, List Number).\n* **Chunking support for the SharePoint SourceConnector via unstructured-ingest CLI** The SharePoint connector can now optionally chunk the elements pulled out during partition via the chunking unstructured brick. This can be used as a stage before creating embeddings.\n\n### Features\n\n* **Adds `links` metadata in `partition_pdf` for `fast` strategy.** Problem: PDF files contain rich information and hyperlink that Unstructured did not captured earlier. Feature: `partition_pdf` now can capture embedded links within the file along with its associated text and page number. Importance: Providing depth in extracted elements give user a better understanding and richer context of documents. This also enables user to map to other elements within the document if the hyperlink is refered internally.\n* **Adds the embedding module to be able to embed Elements** Problem: Many NLP applications require the ability to represent parts of documents in a semantic way. Until now, Unstructured did not have text embedding ability within the core library. Feature: This embedding module is able to track embeddings related data with a class, embed a list of elements, and return an updated list of Elements with the *embeddings* property. The module is also able to embed query strings. Importance: Ability to embed documents or parts of documents will enable users to make use of these semantic representations in different NLP applications, such as search, retrieval, and retrieval augmented generation.\n\n### Fixes\n\n* **Fixes a metadata source serialization bug** Problem: In unstructured elements, when loading an elements json file from the disk, the data_source attribute is assumed to be an instance of DataSourceMetadata and the code acts based on that. However the loader did not satisfy the assumption, and loaded it as a dict instead, causing an error. Fix: Added necessary code block to initialize a DataSourceMetadata object, also refactored DataSourceMetadata.from_dict() method to remove redundant code. Importance: Crucial to be able to load elements (which have data_source fields) from json files.\n* **Fixes issue where unstructured-inference was not getting updated** Problem: unstructured-inference was not getting upgraded to the version to match unstructured release when doing a pip install.  Solution: using `pip install unstructured[all-docs]` it will now upgrade both unstructured and unstructured-inference. Importance: This will ensure that the inference library is always in sync with the unstructured library, otherwise users will be using outdated libraries which will likely lead to unintended behavior.\n* **Fixes SharePoint connector failures if any document has an unsupported filetype** Problem: Currently the entire connector ingest run fails if a single IngestDoc has an unsupported filetype. This is because a ValueError is raised in the IngestDoc's `__post_init__`. Fix: Adds a try/catch when the IngestConnector runs get_ingest_docs such that the error is logged but all processable documents->IngestDocs are still instantiated and returned. Importance: Allows users to ingest SharePoint content even when some files with unsupported filetypes exist there.\n* **Fixes Sharepoint connector server_path issue** Problem: Server path for the Sharepoint Ingest Doc was incorrectly formatted, causing issues while fetching pages from the remote source. Fix: changes formatting of remote file path before instantiating SharepointIngestDocs and appends a '/' while fetching pages from the remote source. Importance: Allows users to fetch pages from Sharepoint Sites.\n* **Fixes Sphinx errors.** Fixes errors when running Sphinx `make html` and installs library to suppress warnings.\n* **Fixes a metadata backwards compatibility error** Problem: When calling `partition_via_api`, the hosted api may return an element schema that's newer than the current `unstructured`. In this case, metadata fields were added which did not exist in the local `ElementMetadata` dataclass, and `__init__()` threw an error. Fix: remove nonexistent fields before instantiating in `ElementMetadata.from_json()`. Importance: Crucial to avoid breaking changes when adding fields.\n* **Fixes issue with Discord connector when a channel returns `None`** Problem: Getting the `jump_url` from a nonexistent Discord `channel` fails. Fix: property `jump_url` is now retrieved within the same context as the messages from the channel. Importance: Avoids cascading issues when the connector fails to fetch information about a Discord channel.\n* **Fixes occasionally SIGABTR when writing table with `deltalake` on Linux** Problem: occasionally on Linux ingest can throw a `SIGABTR` when writing `deltalake` table even though the table was written correctly. Fix: put the writing function into a `Process` to ensure its execution to the fullest extent before returning to the main process. Importance: Improves stability of connectors using `deltalake`\n* **Fixes badly initialized Formula** Problem: YoloX contain new types of elements, when loading a document that contain formulas a new element of that class should be generated, however the Formula class inherits from Element instead of Text. After this change the element is correctly created with the correct class allowing the document to be loaded. Fix: Change parent class for Formula to Text. Importance: Crucial to be able to load documents that contain formulas.\n\n## 0.10.16\n\n### Enhancements\n\n* **Adds data source properties to Airtable, Confluence, Discord, Elasticsearch, Google Drive, and Wikipedia connectors** These properties (date_created, date_modified, version, source_url, record_locator) are written to element metadata during ingest, mapping elements to information about the document source from which they derive. This functionality enables downstream applications to reveal source document applications, e.g. a link to a GDrive doc, Salesforce record, etc.\n* **DOCX partitioner refactored in preparation for enhancement.** Behavior should be unchanged except in multi-section documents containing different headers/footers for different sections. These will now emit all distinct headers and footers encountered instead of just those for the last section.\n* **Add a function to map between Tesseract and standard language codes.** This allows users to input language information to the `languages` param in any Tesseract-supported langcode or any ISO 639 standard language code.\n* **Add document level language detection functionality.** Introduces the \"auto\" default for the languages param, which then detects the languages present in the document using the `langdetect` package. Implemented only for the partition_text function to start.\n\n### Features\n\n### Fixes\n\n* ***Fixes an issue that caused a partition error for some PDF's.** Fixes GH Issue 1460 by bypassing a coordinate check if an element has invalid coordinates.\n\n## 0.10.15\n\n### Enhancements\n\n* **Support for better element categories from the next-generation image-to-text model (\"chipper\").** Previously, not all of the classifications from Chipper were being mapped to proper `unstructured` element categories so the consumer of the library would see many `UncategorizedText` elements. This fixes the issue, improving the granularity of the element categories outputs for better downstream processing and chunking. The mapping update is:\n  * \"Threading\": `NarrativeText`\n  * \"Form\": `NarrativeText`\n  * \"Field-Name\": `Title`\n  * \"Value\": `NarrativeText`\n  * \"Link\": `NarrativeText`\n  * \"Headline\": `Title` (with `category_depth=1`)\n  * \"Subheadline\": `Title` (with `category_depth=2`)\n  * \"Abstract\": `NarrativeText`\n* **Better ListItem grouping for PDF's (fast strategy).** The `partition_pdf` with `fast` strategy previously broke down some numbered list item lines as separate elements. This enhancement leverages the x,y coordinates and bbox sizes to help decide whether the following chunk of text is a continuation of the immediate previous detected ListItem element or not, and not detect it as its own non-ListItem element.\n* **Fall back to text-based classification for uncategorized Layout elements for Images and PDF's**. Improves element classification by running existing text-based rules on previously `UncategorizedText` elements.\n* **Adds table partitioning for Partitioning for many doc types including: .html, .epub., .md, .rst, .odt, and .msg.** At the core of this change is the .html partition functionality, which is leveraged by the other effected doc types. This impacts many scenarios where `Table` Elements are now propery extracted.\n* **Create and add `add_chunking_strategy` decorator to partition functions.** Previously, users were responsible for their own chunking after partitioning elements, often required for downstream applications. Now, individual elements may be combined into right-sized chunks where min and max character size may be specified if `chunking_strategy=by_title`. Relevant elements are grouped together for better downstream results. This enables users immediately use partitioned results effectively in downstream applications (e.g. RAG architecture apps) without any additional post-processing.\n* **Adds `languages` as an input parameter and marks `ocr_languages` kwarg for deprecation in pdf, image, and auto partitioning functions.** Previously, language information was only being used for Tesseract OCR for image-based documents and was in a Tesseract specific string format, but by refactoring into a list of standard language codes independent of Tesseract, the `unstructured` library will better support `languages` for other non-image pipelines and/or support for other OCR engines.\n* **Removes `UNSTRUCTURED_LANGUAGE` env var usage and replaces `language` with `languages` as an input parameter to unstructured-partition-text_type functions.** The previous parameter/input setup was not user-friendly or scalable to the variety of elements being processed. By refactoring the inputted language information into a list of standard language codes, we can support future applications of the element language such as detection, metadata, and multi-language elements. Now, to skip English specific checks, set the `languages` parameter to any non-English language(s).\n* **Adds `xlsx` and `xls` filetype extensions to the `skip_infer_table_types` default list in `partition`.** By adding these file types to the input parameter these files should not go through table extraction. Users can still specify if they would like to extract tables from these filetypes, but will have to set the `skip_infer_table_types` to exclude the desired filetype extension. This avoids mis-representing complex spreadsheets where there may be multiple sub-tables and other content.\n* **Better debug output related to sentence counting internals**. Clarify message when sentence is not counted toward sentence count because there aren't enough words, relevant for developers focused on `unstructured`s NLP internals.\n* **Faster ocr_only speed for partitioning PDF and images.** Use `unstructured_pytesseract.run_and_get_multiple_output` function to reduce the number of calls to `tesseract` by half when partitioning pdf or image with `tesseract`\n* **Adds data source properties to fsspec connectors** These properties (date_created, date_modified, version, source_url, record_locator) are written to element metadata during ingest, mapping elements to information about the document source from which they derive. This functionality enables downstream applications to reveal source document applications, e.g. a link to a GDrive doc, Salesforce record, etc.\n* **Add delta table destination connector** New delta table destination connector added to ingest CLI.  Users may now use `unstructured-ingest` to write partitioned data from over 20 data sources (so far) to a Delta Table.\n* **Rename to Source and Destination Connectors in the Documentation.** Maintain naming consistency between Connectors codebase and documentation with the first addition to a destination connector.\n* **Non-HTML text files now return unstructured-elements as opposed to HTML-elements.** Previously the text based files that went through `partition_html` would return HTML-elements but now we preserve the format from the input using `source_format` argument in the partition call.\n* **Adds `PaddleOCR` as an optional alternative to `Tesseract`** for OCR in processing of PDF or Image files, it is installable via the `makefile` command `install-paddleocr`. For experimental purposes only.\n* **Bump unstructured-inference** to 0.5.28. This version bump markedly improves the output of table data, rendered as `metadata.text_as_html` in an element. These changes include:\n  * add env variable `ENTIRE_PAGE_OCR` to specify using paddle or tesseract on entire page OCR\n  * table structure detection now pads the input image by 25 pixels in all 4 directions to improve its recall (0.5.27)\n  * support paddle with both cpu and gpu and assume it is pre-installed (0.5.26)\n  * fix a bug where `cells_to_html` doesn't handle cells spanning multiple rows properly (0.5.25)\n  * remove `cv2` preprocessing step before OCR step in table transformer (0.5.24)\n\n### Features\n\n* **Adds element metadata via `category_depth` with default value None**.\n  * This additional metadata is useful for vectordb/LLM, chunking strategies, and retrieval applications.\n* **Adds a naive hierarchy for elements via a `parent_id` on the element's metadata**\n  * Users will now have more metadata for implementing vectordb/LLM chunking strategies. For example, text elements could be queried by their preceding title element.\n  * Title elements created from HTML headings will properly nest\n\n### Fixes\n\n* **`add_pytesseract_bboxes_to_elements` no longer returns `nan` values**. The function logic is now broken into new methods\n  `_get_element_box` and `convert_multiple_coordinates_to_new_system`\n* **Selecting a different model wasn't being respected when calling `partition_image`.** Problem: `partition_pdf` allows for passing a `model_name` parameter. Given the similarity between the image and PDF pipelines, the expected behavior is that `partition_image` should support the same parameter, but `partition_image` was unintentionally not passing along its `kwargs`. This was corrected by adding the kwargs to the downstream call.\n* **Fixes a chunking issue via dropping the field \"coordinates\".** Problem: chunk_by_title function was chunking each element to its own individual chunk while it needed to group elements into a fewer number of chunks. We've discovered that this happens due to a metadata matching logic in chunk_by_title function, and discovered that elements with different metadata can't be put into the same chunk. At the same time, any element with \"coordinates\" essentially had different metadata than other elements, due each element locating in different places and having different coordinates. Fix: That is why we have included the key \"coordinates\" inside a list of excluded metadata keys, while doing this \"metadata_matches\" comparision. Importance: This change is crucial to be able to chunk by title for documents which include \"coordinates\" metadata in their elements.\n\n## 0.10.14\n\n### Enhancements\n\n* Update all connectors to use new downstream architecture\n  * New click type added to parse comma-delimited string inputs\n  * Some CLI options renamed\n\n### Features\n\n### Fixes\n\n## 0.10.13\n\n### Enhancements\n\n* Updated documentation: Added back support doc types for partitioning, more Python codes in the API page,  RAG definition, and use case.\n* Updated Hi-Res Metadata: PDFs and Images using Hi-Res strategy now have layout model class probabilities added ot metadata.\n* Updated the `_detect_filetype_from_octet_stream()` function to use libmagic to infer the content type of file when it is not a zip file.\n* Tesseract minor version bump to 5.3.2\n\n### Features\n\n* Add Jira Connector to be able to pull issues from a Jira organization\n* Add `clean_ligatures` function to expand ligatures in text\n\n### Fixes\n\n* `partition_html` breaks on `<br>` elements.\n* Ingest error handling to properly raise errors when wrapped\n* GH issue 1361: fixes a sortig error that prevented some PDF's from being parsed\n* Bump unstructured-inference\n  * Brings back embedded images in PDF's (0.5.23)\n\n## 0.10.12\n\n### Enhancements\n\n* Removed PIL pin as issue has been resolved upstream\n* Bump unstructured-inference\n  * Support for yolox_quantized layout detection model (0.5.20)\n* YoloX element types added\n\n### Features\n\n* Add Salesforce Connector to be able to pull Account, Case, Campaign, EmailMessage, Lead\n\n### Fixes\n\n* Bump unstructured-inference\n  * Avoid divide-by-zero errors swith `safe_division` (0.5.21)\n\n## 0.10.11\n\n### Enhancements\n\n* Bump unstructured-inference\n  * Combine entire-page OCR output with layout-detected elements, to ensure full coverage of the page (0.5.19)\n\n### Features\n\n* Add in ingest cli s3 writer\n\n### Fixes\n\n* Fix a bug where `xy-cut` sorting attemps to sort elements without valid coordinates; now xy cut sorting only works when **all** elements have valid coordinates\n\n## 0.10.10\n\n### Enhancements\n\n* Adds `text` as an input parameter to `partition_xml`.\n* `partition_xml` no longer runs through `partition_text`, avoiding incorrect splitting\n  on carriage returns in the XML. Since `partition_xml` no longer calls `partition_text`,\n  `min_partition` and `max_partition` are no longer supported in `partition_xml`.\n* Bump `unstructured-inference==0.5.18`, change non-default detectron2 classification threshold\n* Upgrade base image from rockylinux 8 to rockylinux 9\n* Serialize IngestDocs to JSON when passing to subprocesses\n\n### Features\n\n### Fixes\n\n- Fix a bug where mismatched `elements` and `bboxes` are passed into `add_pytesseract_bbox_to_elements`\n\n## 0.10.9\n\n### Enhancements\n\n* Fix `test_json` to handle only non-extra dependencies file types (plain-text)\n\n### Features\n\n* Adds `chunk_by_title` to break a document into sections based on the presence of `Title`\n  elements.\n* add new extraction function `extract_image_urls_from_html` to extract all img related URL from html text.\n\n### Fixes\n\n* Make cv2 dependency optional\n* Edit `add_pytesseract_bbox_to_elements`'s (`ocr_only` strategy) `metadata.coordinates.points` return type to `Tuple` for consistency.\n* Re-enable test-ingest-confluence-diff for ingest tests\n* Fix syntax for ingest test check number of files\n* Fix csv and tsv partitioners loosing the first line of the files when creating elements\n\n## 0.10.8\n\n### Enhancements\n\n* Release docker image that installs Python 3.10 rather than 3.8\n\n### Features\n\n### Fixes\n\n## 0.10.7\n\n### Enhancements\n\n### Features\n\n### Fixes\n\n* Remove overly aggressive ListItem chunking for images and PDF's which typically resulted in inchorent elements.\n\n## 0.10.6\n\n### Enhancements\n\n* Enable `partition_email` and `partition_msg` to detect if an email is PGP encryped. If\n  and email is PGP encryped, the functions will return an empy list of elements and\n  emit a warning about the encrypted content.\n* Add threaded Slack conversations into Slack connector output\n* Add functionality to sort elements using `xy-cut` sorting approach in `partition_pdf` for `hi_res` and `fast` strategies\n* Bump unstructured-inference\n  * Set OMP_THREAD_LIMIT to 1 if not set for better tesseract perf (0.5.17)\n\n### Features\n\n* Extract coordinates from PDFs and images when using OCR only strategy and add to metadata\n\n### Fixes\n\n* Update `partition_html` to respect the order of `<pre>` tags.\n* Fix bug in `partition_pdf_or_image` where two partitions were called if `strategy == \"ocr_only\"`.\n* Bump unstructured-inference\n  * Fix issue where temporary files were being left behind (0.5.16)\n* Adds deprecation warning for the `file_filename` kwarg to `partition`, `partition_via_api`,\n  and `partition_multiple_via_api`.\n* Fix documentation build workflow by pinning dependencies\n\n## 0.10.5\n\n### Enhancements\n\n* Create new CI Pipelines\n  - Checking text, xml, email, and html doc tests against the library installed without extras\n  - Checking each library extra against their respective tests\n* `partition` raises an error and tells the user to install the appropriate extra if a filetype\n  is detected that is missing dependencies.\n* Add custom errors to ingest\n* Bump `unstructured-ingest==0.5.15`\n  - Handle an uncaught TesseractError (0.5.15)\n  - Add TIFF test file and TIFF filetype to `test_from_image_file` in `test_layout` (0.5.14)\n* Use `entire_page` ocr mode for pdfs and images\n* Add notes on extra installs to docs\n* Adds ability to reuse connections per process in unstructured-ingest\n\n### Features\n\n* Add delta table connector\n\n### Fixes\n\n## 0.10.4\n\n* Pass ocr_mode in partition_pdf and set the default back to individual pages for now\n* Add diagrams and descriptions for ingest design in the ingest README\n\n### Features\n\n* Supports multipage TIFF image partitioning\n\n### Fixes\n\n## 0.10.2\n\n### Enhancements\n\n* Bump unstructured-inference==0.5.13:\n  - Fix extracted image elements being included in layout merge, addresses the issue\n    where an entire-page image in a PDF was not passed to the layout model when using hi_res.\n\n### Features\n\n### Fixes\n\n## 0.10.1\n\n### Enhancements\n\n* Bump unstructured-inference==0.5.12:\n  - fix to avoid trace for certain PDF's (0.5.12)\n  - better defaults for DPI for hi_res and  Chipper (0.5.11)\n  - implement full-page OCR (0.5.10)\n\n### Features\n\n### Fixes\n\n* Fix dead links in repository README (Quick Start > Install for local development, and Learn more > Batch Processing)\n* Update document dependencies to include tesseract-lang for additional language support (required for tests to pass)\n\n## 0.10.0\n\n### Enhancements\n\n* Add `include_header` kwarg to `partition_xlsx` and change default behavior to `True`\n* Update the `links` and `emphasized_texts` metadata fields\n\n### Features\n\n### Fixes\n\n## 0.9.3\n\n### Enhancements\n\n* Pinned dependency cleanup.\n* Update `partition_csv` to always use `soupparser_fromstring` to parse `html text`\n* Update `partition_tsv` to always use `soupparser_fromstring` to parse `html text`\n* Add `metadata.section` to capture epub table of contents data\n* Add `unique_element_ids` kwarg to partition functions. If `True`, will use a UUID\n  for element IDs instead of a SHA-256 hash.\n* Update `partition_xlsx` to always use `soupparser_fromstring` to parse `html text`\n* Add functionality to switch `html` text parser based on whether the `html` text contains emoji\n* Add functionality to check if a string contains any emoji characters\n* Add CI tests around Notion\n\n### Features\n\n* Add Airtable Connector to be able to pull views/tables/bases from an Airtable organization\n\n### Fixes\n\n* fix pdf partition of list items being detected as titles in OCR only mode\n* make notion module discoverable\n* fix emails with `Content-Distribution: inline` and `Content-Distribution: attachment` with no filename\n* Fix email attachment filenames which had `=` in the filename itself\n\n## 0.9.2\n\n### Enhancements\n\n* Update table extraction section in API documentation to sync with change in Prod API\n* Update Notion connector to extract to html\n* Added UUID option for `element_id`\n* Bump unstructured-inference==0.5.9:\n  - better caching of models\n  - another version of detectron2 available, though the default layout model is unchanged\n* Added UUID option for element_id\n* Added UUID option for element_id\n* CI improvements to run ingest tests in parallel\n\n### Features\n\n* Adds Sharepoint connector.\n\n### Fixes\n\n* Bump unstructured-inference==0.5.9:\n  - ignores Tesseract errors where no text is extracted for tiles that indeed, have no text\n\n## 0.9.1\n\n### Enhancements\n\n* Adds --partition-pdf-infer-table-structure to unstructured-ingest.\n* Enable `partition_html` to skip headers and footers with the `skip_headers_and_footers` flag.\n* Update `partition_doc` and `partition_docx` to track emphasized texts in the output\n* Adds post processing function `filter_element_types`\n* Set the default strategy for partitioning images to `hi_res`\n* Add page break parameter section in API documentation to sync with change in Prod API\n* Update `partition_html` to track emphasized texts in the output\n* Update `XMLDocument._read_xml` to create `<p>` tag element for the text enclosed in the `<pre>` tag\n* Add parameter `include_tail_text` to `_construct_text` to enable (skip) tail text inclusion\n* Add Notion connector\n\n### Features\n\n### Fixes\n\n* Remove unused `_partition_via_api` function\n* Fixed emoji bug in `partition_xlsx`.\n* Pass `file_filename` metadata when partitioning file object\n* Skip ingest test on missing Slack token\n* Add Dropbox variables to CI environments\n* Remove default encoding for ingest\n* Adds new element type `EmailAddress` for recognising email address in the  text\n* Simplifies `min_partition` logic; makes partitions falling below the `min_partition`\n  less likely.\n* Fix bug where ingest test check for number of files fails in smoke test\n* Fix unstructured-ingest entrypoint failure\n\n## 0.9.0\n\n### Enhancements\n\n* Dependencies are now split by document type, creating a slimmer base installation.\n\n## 0.8.8\n\n### Enhancements\n\n### Features\n\n### Fixes\n\n* Rename \"date\" field to \"last_modified\"\n* Adds Box connector\n\n### Fixes\n\n## 0.8.7\n\n### Enhancements\n\n* Put back useful function `split_by_paragraph`\n\n### Features\n\n### Fixes\n\n* Fix argument order in NLTK download step\n\n## 0.8.6\n\n### Enhancements\n\n### Features\n\n### Fixes\n\n* Remove debug print lines and non-functional code\n\n## 0.8.5\n\n### Enhancements\n\n* Add parameter `skip_infer_table_types` to enable (skip) table extraction for other doc types\n* Adds optional Unstructured API unit tests in CI\n* Tracks last modified date for all document types.\n* Add auto_paragraph_grouper to detect new-line and blank-line new paragraph for .txt files.\n* refactor the ingest cli to better support expanding supported connectors\n\n## 0.8.3\n\n### Enhancements\n\n### Features\n\n### Fixes\n\n* NLTK now only gets downloaded if necessary.\n* Handling for empty tables in Word Documents and PowerPoints.\n\n## 0.8.4\n\n### Enhancements\n\n* Additional tests and refactor of JSON detection.\n* Update functionality to retrieve image metadata from a page for `document_to_element_list`\n* Links are now tracked in `partition_html` output.\n* Set the file's current position to the beginning after reading the file in `convert_to_bytes`\n* Add `min_partition` kwarg to that combines elements below a specified threshold and modifies splitting of strings longer than max partition so words are not split.\n* set the file's current position to the beginning after reading the file in `convert_to_bytes`\n* Add slide notes to pptx\n* Add `--encoding` directive to ingest\n* Improve json detection by `detect_filetype`\n\n### Features\n\n* Adds Outlook connector\n* Add support for dpi parameter in inference library\n* Adds Onedrive connector.\n* Add Confluence connector for ingest cli to pull the body text from all documents from all spaces in a confluence domain.\n\n### Fixes\n\n* Fixes issue with email partitioning where From field was being assigned the To field value.\n* Use the `image_metadata` property of the `PageLayout` instance to get the page image info in the `document_to_element_list`\n* Add functionality to write images to computer storage temporarily instead of keeping them in memory for `ocr_only` strategy\n* Add functionality to convert a PDF in small chunks of pages at a time for `ocr_only` strategy\n* Adds `.txt`, `.text`, and `.tab` to list of extensions to check if file\n  has a `text/plain` MIME type.\n* Enables filters to be passed to `partition_doc` so it doesn't error with LibreOffice7.\n* Removed old error message that's superseded by `requires_dependencies`.\n* Removes using `hi_res` as the default strategy value for `partition_via_api` and `partition_multiple_via_api`\n\n## 0.8.1\n\n### Enhancements\n\n* Add support for Python 3.11\n\n### Features\n\n### Fixes\n\n* Fixed `auto` strategy detected scanned document as having extractable text and using `fast` strategy, resulting in no output.\n* Fix list detection in MS Word documents.\n* Don't instantiate an element with a coordinate system when there isn't a way to get its location data.\n\n## 0.8.0\n\n### Enhancements\n\n* Allow model used for hi res pdf partition strategy to be chosen when called.\n* Updated inference package\n\n### Features\n\n* Add `metadata_filename` parameter across all partition functions\n\n### Fixes\n\n* Update to ensure `convert_to_datafame` grabs all of the metadata fields.\n* Adjust encoding recognition threshold value in `detect_file_encoding`\n* Fix KeyError when `isd_to_elements` doesn't find a type\n* Fix `_output_filename` for local connector, allowing single files to be written correctly to the disk\n* Fix for cases where an invalid encoding is extracted from an email header.\n\n### BREAKING CHANGES\n\n* Information about an element's location is no longer returned as top-level attributes of an element. Instead, it is returned in the `coordinates` attribute of the element's metadata.\n\n## 0.7.12\n\n### Enhancements\n\n* Adds `include_metadata` kwarg to `partition_doc`, `partition_docx`, `partition_email`, `partition_epub`, `partition_json`, `partition_msg`, `partition_odt`, `partition_org`, `partition_pdf`, `partition_ppt`, `partition_pptx`, `partition_rst`, and `partition_rtf`\n\n### Features\n\n* Add Elasticsearch connector for ingest cli to pull specific fields from all documents in an index.\n* Adds Dropbox connector\n\n### Fixes\n\n* Fix tests that call unstructured-api by passing through an api-key\n* Fixed page breaks being given (incorrect) page numbers\n* Fix skipping download on ingest when a source document exists locally\n\n## 0.7.11\n\n### Enhancements\n\n* More deterministic element ordering when using `hi_res` PDF parsing strategy (from unstructured-inference bump to 0.5.4)\n* Make large model available (from unstructured-inference bump to 0.5.3)\n* Combine inferred elements with extracted elements (from unstructured-inference bump to 0.5.2)\n* `partition_email` and `partition_msg` will now process attachments if `process_attachments=True`\n  and a attachment partitioning functions is passed through with `attachment_partitioner=partition`.\n\n### Features\n\n### Fixes\n\n* Fix tests that call unstructured-api by passing through an api-key\n* Fixed page breaks being given (incorrect) page numbers\n* Fix skipping download on ingest when a source document exists locally\n\n## 0.7.10\n\n### Enhancements\n\n* Adds a `max_partition` parameter to `partition_text`, `partition_pdf`, `partition_email`,\n  `partition_msg` and `partition_xml` that sets a limit for the size of an individual\n  document elements. Defaults to `1500` for everything except `partition_xml`, which has\n  a default value of `None`.\n* DRY connector refactor\n\n### Features\n\n* `hi_res` model for pdfs and images is selectable via environment variable.\n\n### Fixes\n\n* CSV check now ignores escaped commas.\n* Fix for filetype exploration util when file content does not have a comma.\n* Adds negative lookahead to bullet pattern to avoid detecting plain text line\n  breaks like `-------` as list items.\n* Fix pre tag parsing for `partition_html`\n* Fix lookup error for annotated Arabic and Hebrew encodings\n\n## 0.7.9\n\n### Enhancements\n\n* Improvements to string check for leafs in `partition_xml`.\n* Adds --partition-ocr-languages to unstructured-ingest.\n\n### Features\n\n* Adds `partition_org` for processed Org Mode documents.\n\n### Fixes\n\n## 0.7.8\n\n### Enhancements\n\n### Features\n\n* Adds Google Cloud Service connector\n\n### Fixes\n\n* Updates the `parse_email` for `partition_eml` so that `unstructured-api` passes the smoke tests\n* `partition_email` now works if there is no message content\n* Updates the `\"fast\"` strategy for `partition_pdf` so that it's able to recursively\n* Adds recursive functionality to all fsspec connectors\n* Adds generic --recursive ingest flag\n\n## 0.7.7\n\n### Enhancements\n\n* Adds functionality to replace the `MIME` encodings for `eml` files with one of the common encodings if a `unicode` error occurs\n* Adds missed file-like object handling in `detect_file_encoding`\n* Adds functionality to extract charset info from `eml` files\n\n### Features\n\n* Added coordinate system class to track coordinate types and convert to different coordinate\n\n### Fixes\n\n* Adds an `html_assemble_articles` kwarg to `partition_html` to enable users to capture\n  control whether content outside of `<article>` tags is captured when\n  `<article>` tags are present.\n* Check for the `xml` attribute on `element` before looking for pagebreaks in `partition_docx`.\n\n## 0.7.6\n\n### Enhancements\n\n* Convert fast startegy to ocr_only for images\n* Adds support for page numbers in `.docx` and `.doc` when user or renderer\n  created page breaks are present.\n* Adds retry logic for the unstructured-ingest Biomed connector\n\n### Features\n\n* Provides users with the ability to extract additional metadata via regex.\n* Updates `partition_docx` to include headers and footers in the output.\n* Create `partition_tsv` and associated tests. Make additional changes to `detect_filetype`.\n\n### Fixes\n\n* Remove fake api key in test `partition_via_api` since we now require valid/empty api keys\n* Page number defaults to `None` instead of `1` when page number is not present in the metadata.\n  A page number of `None` indicates that page numbers are not being tracked for the document\n  or that page numbers do not apply to the element in question..\n* Fixes an issue with some pptx files. Assume pptx shapes are found in top left position of slide\n  in case the shape.top and shape.left attributes are `None`.\n\n## 0.7.5\n\n### Enhancements\n\n* Adds functionality to sort elements in `partition_pdf` for `fast` strategy\n* Adds ingest tests with `--fast` strategy on PDF documents\n* Adds --api-key to unstructured-ingest\n\n### Features\n\n* Adds `partition_rst` for processed ReStructured Text documents.\n\n### Fixes\n\n* Adds handling for emails that do not have a datetime to extract.\n* Adds pdf2image package as core requirement of unstructured (with no extras)\n\n## 0.7.4\n\n### Enhancements\n\n* Allows passing kwargs to request data field for `partition_via_api` and `partition_multiple_via_api`\n* Enable MIME type detection if libmagic is not available\n* Adds handling for empty files in `detect_filetype` and `partition`.\n\n### Features\n\n### Fixes\n\n* Reslove `grpcio` import issue on `weaviate.schema.validate_schema` for python 3.9 and 3.10\n* Remove building `detectron2` from source in Dockerfile\n\n## 0.7.3\n\n### Enhancements\n\n* Update IngestDoc abstractions and add data source metadata in ElementMetadata\n\n### Features\n\n### Fixes\n\n* Pass `strategy` parameter down from `partition` for `partition_image`\n* Filetype detection if a CSV has a `text/plain` MIME type\n* `convert_office_doc` no longers prints file conversion info messages to stdout.\n* `partition_via_api` reflects the actual filetype for the file processed in the API.\n\n## 0.7.2\n\n### Enhancements\n\n* Adds an optional encoding kwarg to `elements_to_json` and `elements_from_json`\n* Bump version of base image to use new stable version of tesseract\n\n### Features\n\n### Fixes\n\n* Update the `read_txt_file` utility function to keep using `spooled_to_bytes_io_if_needed` for xml\n* Add functionality to the `read_txt_file` utility function to handle file-like object from URL\n* Remove the unused parameter `encoding` from `partition_pdf`\n* Change auto.py to have a `None` default for encoding\n* Add functionality to try other common encodings for html and xml files if an error related to the encoding is raised and the user has not specified an encoding.\n* Adds benchmark test with test docs in example-docs\n* Re-enable test_upload_label_studio_data_with_sdk\n* File detection now detects code files as plain text\n* Adds `tabulate` explicitly to dependencies\n* Fixes an issue in `metadata.page_number` of pptx files\n* Adds showing help if no parameters passed\n\n## 0.7.1\n\n### Enhancements\n\n### Features\n\n* Add `stage_for_weaviate` to stage `unstructured` outputs for upload to Weaviate, along with\n  a helper function for defining a class to use in Weaviate schemas.\n* Builds from Unstructured base image, built off of Rocky Linux 8.7, this resolves almost all CVE's in the image.\n\n### Fixes\n\n## 0.7.0\n\n### Enhancements\n\n* Installing `detectron2` from source is no longer required when using the `local-inference` extra.\n* Updates `.pptx` parsing to include text in tables.\n\n### Features\n\n### Fixes\n\n* Fixes an issue in `_add_element_metadata` that caused all elements to have `page_number=1`\n  in the element metadata.\n* Adds `.log` as a file extension for TXT files.\n* Adds functionality to try other common encodings for email (`.eml`) files if an error related to the encoding is raised and the user has not specified an encoding.\n* Allow passed encoding to be used in the `replace_mime_encodings`\n* Fixes page metadata for `partition_html` when `include_metadata=False`\n* A `ValueError` now raises if `file_filename` is not specified when you use `partition_via_api`\n  with a file-like object.\n\n## 0.6.11\n\n### Enhancements\n\n* Supports epub tests since pandoc is updated in base image\n\n### Features\n\n### Fixes\n\n## 0.6.10\n\n### Enhancements\n\n* XLS support from auto partition\n\n### Features\n\n### Fixes\n\n## 0.6.9\n\n### Enhancements\n\n* fast strategy for pdf now keeps element bounding box data\n* setup.py refactor\n\n### Features\n\n### Fixes\n\n* Adds functionality to try other common encodings if an error related to the encoding is raised and the user has not specified an encoding.\n* Adds additional MIME types for CSV\n\n## 0.6.8\n\n### Enhancements\n\n### Features\n\n* Add `partition_csv` for CSV files.\n\n### Fixes\n\n## 0.6.7\n\n### Enhancements\n\n* Deprecate `--s3-url` in favor of `--remote-url` in CLI\n* Refactor out non-connector-specific config variables\n* Add `file_directory` to metadata\n* Add `page_name` to metadata. Currently used for the sheet name in XLSX documents.\n* Added a `--partition-strategy` parameter to unstructured-ingest so that users can specify\n  partition strategy in CLI. For example, `--partition-strategy fast`.\n* Added metadata for filetype.\n* Add Discord connector to pull messages from a list of channels\n* Refactor `unstructured/file-utils/filetype.py` to better utilise hashmap to return mime type.\n* Add local declaration of DOCX_MIME_TYPES and XLSX_MIME_TYPES for `test_filetype.py`.\n\n### Features\n\n* Add `partition_xml` for XML files.\n* Add `partition_xlsx` for Microsoft Excel documents.\n\n### Fixes\n\n* Supports `hml` filetype for partition as a variation of html filetype.\n* Makes `pytesseract` a function level import in `partition_pdf` so you can use the `\"fast\"`\n  or `\"hi_res\"` strategies if `pytesseract` is not installed. Also adds the\n  `required_dependencies` decorator for the `\"hi_res\"` and `\"ocr_only\"` strategies.\n* Fix to ensure `filename` is tracked in metadata for `docx` tables.\n\n## 0.6.6\n\n### Enhancements\n\n* Adds an `\"auto\"` strategy that chooses the partitioning strategy based on document\n  characteristics and function kwargs. This is the new default strategy for `partition_pdf`\n  and `partition_image`. Users can maintain existing behavior by explicitly setting\n  `strategy=\"hi_res\"`.\n* Added an additional trace logger for NLP debugging.\n* Add `get_date` method to `ElementMetadata` for converting the datestring to a `datetime` object.\n* Cleanup the `filename` attribute on `ElementMetadata` to remove the full filepath.\n\n### Features\n\n* Added table reading as html with URL parsing to `partition_docx` in docx\n* Added metadata field for text_as_html for docx files\n\n### Fixes\n\n* `fileutils/file_type` check json and eml decode ignore error\n* `partition_email` was updated to more flexibly handle deviations from the RFC-2822 standard.\n  The time in the metadata returns `None` if the time does not match RFC-2822 at all.\n* Include all metadata fields when converting to dataframe or CSV\n\n## 0.6.5\n\n### Enhancements\n\n* Added support for SpooledTemporaryFile file argument.\n\n### Features\n\n### Fixes\n\n## 0.6.4\n\n### Enhancements\n\n* Added an \"ocr_only\" strategy for `partition_pdf`. Refactored the strategy decision\n  logic into its own module.\n\n### Features\n\n### Fixes\n\n## 0.6.3\n\n### Enhancements\n\n* Add an \"ocr_only\" strategy for `partition_image`.\n\n### Features\n\n* Added `partition_multiple_via_api` for partitioning multiple documents in a single REST\n  API call.\n* Added `stage_for_baseplate` function to prepare outputs for ingestion into Baseplate.\n* Added `partition_odt` for processing Open Office documents.\n\n### Fixes\n\n* Updates the grouping logic in the `partition_pdf` fast strategy to group together text\n  in the same bounding box.\n\n## 0.6.2\n\n### Enhancements\n\n* Added logic to `partition_pdf` for detecting copy protected PDFs and falling back\n  to the hi res strategy when necessary.\n\n### Features\n\n* Add `partition_via_api` for partitioning documents through the hosted API.\n\n### Fixes\n\n* Fix how `exceeds_cap_ratio` handles empty (returns `True` instead of `False`)\n* Updates `detect_filetype` to properly detect JSONs when the MIME type is `text/plain`.\n\n## 0.6.1\n\n### Enhancements\n\n* Updated the table extraction parameter name to be more descriptive\n\n### Features\n\n### Fixes\n\n## 0.6.0\n\n### Enhancements\n\n* Adds an `ssl_verify` kwarg to `partition` and `partition_html` to enable turning off\n  SSL verification for HTTP requests. SSL verification is on by default.\n* Allows users to pass in ocr language to `partition_pdf` and `partition_image` through\n  the `ocr_language` kwarg. `ocr_language` corresponds to the code for the language pack\n  in Tesseract. You will need to install the relevant Tesseract language pack to use a\n  given language.\n\n### Features\n\n* Table extraction is now possible for pdfs from `partition` and `partition_pdf`.\n* Adds support for extracting attachments from `.msg` files\n\n### Fixes\n\n* Adds an `ssl_verify` kwarg to `partition` and `partition_html` to enable turning off\n  SSL verification for HTTP requests. SSL verification is on by default.\n\n## 0.5.13\n\n### Enhancements\n\n* Allow headers to be passed into `partition` when `url` is used.\n\n### Features\n\n* `bytes_string_to_string` cleaning brick for bytes string output.\n\n### Fixes\n\n* Fixed typo in call to `exactly_one` in `partition_json`\n* unstructured-documents encode xml string if document_tree is `None` in `_read_xml`.\n* Update to `_read_xml` so that Markdown files with embedded HTML process correctly.\n* Fallback to \"fast\" strategy only emits a warning if the user specifies the \"hi_res\" strategy.\n* unstructured-partition-text_type exceeds_cap_ratio fix returns and how capitalization ratios are calculated\n* `partition_pdf` and `partition_text` group broken paragraphs to avoid fragmented `NarrativeText` elements.\n* .json files resolved as \"application/json\" on centos7 (or other installs with older libmagic libs)\n\n## 0.5.12\n\n### Enhancements\n\n* Add OS mimetypes DB to docker image, mainly for unstructured-api compat.\n* Use the image registry as a cache when building Docker images.\n* Adds the ability for `partition_text` to group together broken paragraphs.\n* Added method to utils to allow date time format validation\n\n### Features\n\n* Add Slack connector to pull messages for a specific channel\n* Add --partition-by-api parameter to unstructured-ingest\n* Added `partition_rtf` for processing rich text files.\n* `partition` now accepts a `url` kwarg in addition to `file` and `filename`.\n\n### Fixes\n\n* Allow encoding to be passed into `replace_mime_encodings`.\n* unstructured-ingest connector-specific dependencies are imported on demand.\n* unstructured-ingest --flatten-metadata supported for local connector.\n* unstructured-ingest fix runtime error when using --metadata-include.\n\n## 0.5.11\n\n### Enhancements\n\n### Features\n\n### Fixes\n\n* Guard against null style attribute in docx document elements\n* Update HTML encoding to better support foreign language characters\n\n## 0.5.10\n\n### Enhancements\n\n* Updated inference package\n* Add sender, recipient, date, and subject to element metadata for emails\n\n### Features\n\n* Added `--download-only` parameter to `unstructured-ingest`\n\n### Fixes\n\n* FileNotFound error when filename is provided but file is not on disk\n\n## 0.5.9\n\n### Enhancements\n\n### Features\n\n### Fixes\n\n* Convert file to str in helper `split_by_paragraph` for `partition_text`\n\n## 0.5.8\n\n### Enhancements\n\n* Update `elements_to_json` to return string when filename is not specified\n* `elements_from_json` may take a string instead of a filename with the `text` kwarg\n* `detect_filetype` now does a final fallback to file extension.\n* Empty tags are now skipped during the depth check for HTML processing.\n\n### Features\n\n* Add local file system to `unstructured-ingest`\n* Add `--max-docs` parameter to `unstructured-ingest`\n* Added `partition_msg` for processing MSFT Outlook .msg files.\n\n### Fixes\n\n* `convert_file_to_text` now passes through the `source_format` and `target_format` kwargs.\n  Previously they were hard coded.\n* Partitioning functions that accept a `text` kwarg no longer raise an error if an empty\n  string is passed (and empty list of elements is returned instead).\n* `partition_json` no longer fails if the input is an empty list.\n* Fixed bug in `chunk_by_attention_window` that caused the last word in segments to be cut-off\n  in some cases.\n\n### BREAKING CHANGES\n\n* `stage_for_transformers` now returns a list of elements, making it consistent with other\n  staging bricks\n\n## 0.5.7\n\n### Enhancements\n\n* Refactored codebase using `exactly_one`\n* Adds ability to pass headers when passing a url in partition_html()\n* Added optional `content_type` and `file_filename` parameters to `partition()` to bypass file detection\n\n### Features\n\n* Add `--flatten-metadata` parameter to `unstructured-ingest`\n* Add `--fields-include` parameter to `unstructured-ingest`\n\n### Fixes\n\n## 0.5.6\n\n### Enhancements\n\n* `contains_english_word()`, used heavily in text processing, is 10x faster.\n\n### Features\n\n* Add `--metadata-include` and `--metadata-exclude` parameters to `unstructured-ingest`\n* Add `clean_non_ascii_chars` to remove non-ascii characters from unicode string\n\n### Fixes\n\n* Fix problem with PDF partition (duplicated test)\n\n## 0.5.4\n\n### Enhancements\n\n* Added Biomedical literature connector for ingest cli.\n* Add `FsspecConnector` to easily integrate any existing `fsspec` filesystem as a connector.\n* Rename `s3_connector.py` to `s3.py` for readability and consistency with the\n  rest of the connectors.\n* Now `S3Connector` relies on `s3fs` instead of on `boto3`, and it inherits\n  from `FsspecConnector`.\n* Adds an `UNSTRUCTURED_LANGUAGE_CHECKS` environment variable to control whether or not language\n  specific checks like vocabulary and POS tagging are applied. Set to `\"true\"` for higher\n  resolution partitioning and `\"false\"` for faster processing.\n* Improves `detect_filetype` warning to include filename when provided.\n* Adds a \"fast\" strategy for partitioning PDFs with PDFMiner. Also falls back to the \"fast\"\n  strategy if detectron2 is not available.\n* Start deprecation life cycle for `unstructured-ingest --s3-url` option, to be deprecated in\n  favor of `--remote-url`.\n\n### Features\n\n* Add `AzureBlobStorageConnector` based on its `fsspec` implementation inheriting\n  from `FsspecConnector`\n* Add `partition_epub` for partitioning e-books in EPUB3 format.\n\n### Fixes\n\n* Fixes processing for text files with `message/rfc822` MIME type.\n* Open xml files in read-only mode when reading contents to construct an XMLDocument.\n\n## 0.5.3\n\n### Enhancements\n\n* `auto.partition()` can now load Unstructured ISD json documents.\n* Simplify partitioning functions.\n* Improve logging for ingest CLI.\n\n### Features\n\n* Add `--wikipedia-auto-suggest` argument to the ingest CLI to disable automatic redirection\n  to pages with similar names.\n* Add setup script for Amazon Linux 2\n* Add optional `encoding` argument to the `partition_(text/email/html)` functions.\n* Added Google Drive connector for ingest cli.\n* Added Gitlab connector for ingest cli.\n\n### Fixes\n\n## 0.5.2\n\n### Enhancements\n\n* Fully move from printing to logging.\n* `unstructured-ingest` now uses a default `--download_dir` of `$HOME/.cache/unstructured/ingest`\n  rather than a \"tmp-ingest-\" dir in the working directory.\n\n### Features\n\n### Fixes\n\n* `setup_ubuntu.sh` no longer fails in some contexts by interpreting\n  `DEBIAN_FRONTEND=noninteractive` as a command\n* `unstructured-ingest` no longer re-downloads files when --preserve-downloads\n  is used without --download-dir.\n* Fixed an issue that was causing text to be skipped in some HTML documents.\n\n## 0.5.1\n\n### Enhancements\n\n### Features\n\n### Fixes\n\n* Fixes an error causing JavaScript to appear in the output of `partition_html` sometimes.\n* Fix several issues with the `requires_dependencies` decorator, including the error message\n  and how it was used, which had caused an error for `unstructured-ingest --github-url ...`.\n\n## 0.5.0\n\n### Enhancements\n\n* Add `requires_dependencies` Python decorator to check dependencies are installed before\n  instantiating a class or running a function\n\n### Features\n\n* Added Wikipedia connector for ingest cli.\n\n### Fixes\n\n* Fix `process_document` file cleaning on failure\n* Fixes an error introduced in the metadata tracking commit that caused `NarrativeText`\n  and `FigureCaption` elements to be represented as `Text` in HTML documents.\n\n## 0.4.16\n\n### Enhancements\n\n* Fallback to using file extensions for filetype detection if `libmagic` is not present\n\n### Features\n\n* Added setup script for Ubuntu\n* Added GitHub connector for ingest cli.\n* Added `partition_md` partitioner.\n* Added Reddit connector for ingest cli.\n\n### Fixes\n\n* Initializes connector properly in ingest.main::MainProcess\n* Restricts version of unstructured-inference to avoid multithreading issue\n\n## 0.4.15\n\n### Enhancements\n\n* Added `elements_to_json` and `elements_from_json` for easier serialization/deserialization\n* `convert_to_dict`, `dict_to_elements` and `convert_to_csv` are now aliases for functions\n  that use the ISD terminology.\n\n### Fixes\n\n* Update to ensure all elements are preserved during serialization/deserialization\n\n## 0.4.14\n\n* Automatically install `nltk` models in the `tokenize` module.\n\n## 0.4.13\n\n* Fixes unstructured-ingest cli.\n\n## 0.4.12\n\n* Adds console_entrypoint for unstructured-ingest, other structure/doc updates related to ingest.\n* Add `parser` parameter to `partition_html`.\n\n## 0.4.11\n\n* Adds `partition_doc` for partitioning Word documents in `.doc` format. Requires `libreoffice`.\n* Adds `partition_ppt` for partitioning PowerPoint documents in `.ppt` format. Requires `libreoffice`.\n\n## 0.4.10\n\n* Fixes `ElementMetadata` so that it's JSON serializable when the filename is a `Path` object.\n\n## 0.4.9\n\n* Added ingest modules and s3 connector, sample ingest script\n* Default to `url=None` for `partition_pdf` and `partition_image`\n* Add ability to skip English specific check by setting the `UNSTRUCTURED_LANGUAGE` env var to `\"\"`.\n* Document `Element` objects now track metadata\n\n## 0.4.8\n\n* Modified XML and HTML parsers not to load comments.\n\n## 0.4.7\n\n* Added the ability to pull an HTML document from a url in `partition_html`.\n* Added the the ability to get file summary info from lists of filenames and lists\n  of file contents.\n* Added optional page break to `partition` for `.pptx`, `.pdf`, images, and `.html` files.\n* Added `to_dict` method to document elements.\n* Include more unicode quotes in `replace_unicode_quotes`.\n\n## 0.4.6\n\n* Loosen the default cap threshold to `0.5`.\n* Add a `UNSTRUCTURED_NARRATIVE_TEXT_CAP_THRESHOLD` environment variable for controlling\n  the cap ratio threshold.\n* Unknown text elements are identified as `Text` for HTML and plain text documents.\n* `Body Text` styles no longer default to `NarrativeText` for Word documents. The style information\n  is insufficient to determine that the text is narrative.\n* Upper cased text is lower cased before checking for verbs. This helps avoid some missed verbs.\n* Adds an `Address` element for capturing elements that only contain an address.\n* Suppress the `UserWarning` when detectron is called.\n* Checks that titles and narrative test have at least one English word.\n* Checks that titles and narrative text are at least 50% alpha characters.\n* Restricts titles to a maximum word length. Adds a `UNSTRUCTURED_TITLE_MAX_WORD_LENGTH`\n  environment variable for controlling the max number of words in a title.\n* Updated `partition_pptx` to order the elements on the page\n\n## 0.4.4\n\n* Updated `partition_pdf` and `partition_image` to return `unstructured` `Element` objects\n* Fixed the healthcheck url path when partitioning images and PDFs via API\n* Adds an optional `coordinates` attribute to document objects\n* Adds `FigureCaption` and `CheckBox` document elements\n* Added ability to split lists detected in `LayoutElement` objects\n* Adds `partition_pptx` for partitioning PowerPoint documents\n* LayoutParser models now download from HugginfaceHub instead of DropBox\n* Fixed file type detection for XML and HTML files on Amazone Linux\n\n## 0.4.3\n\n* Adds `requests` as a base dependency\n* Fix in `exceeds_cap_ratio` so the function doesn't break with empty text\n* Fix bug in `_parse_received_data`.\n* Update `detect_filetype` to properly handle `.doc`, `.xls`, and `.ppt`.\n\n## 0.4.2\n\n* Added `partition_image` to process documents in an image format.\n* Fixed utf-8 encoding error in `partition_email` with attachments for `text/html`\n\n## 0.4.1\n\n* Added support for text files in the `partition` function\n* Pinned `opencv-python` for easier installation on Linux\n\n## 0.4.0\n\n* Added generic `partition` brick that detects the file type and routes a file to the appropriate\n  partitioning brick.\n* Added a file type detection module.\n* Updated `partition_html` and `partition_eml` to support file-like objects in 'rb' mode.\n* Cleaning brick for removing ordered bullets `clean_ordered_bullets`.\n* Extract brick method for ordered bullets `extract_ordered_bullets`.\n* Test for `clean_ordered_bullets`.\n* Test for `extract_ordered_bullets`.\n* Added `partition_docx` for pre-processing Word Documents.\n* Added new REGEX patterns to extract email header information\n* Added new functions to extract header information `parse_received_data` and `partition_header`\n* Added new function to parse plain text files `partition_text`\n* Added new cleaners functions `extract_ip_address`, `extract_ip_address_name`, `extract_mapi_id`, `extract_datetimetz`\n* Add new `Image` element and function to find embedded images `find_embedded_images`\n* Added `get_directory_file_info` for summarizing information about source documents\n\n## 0.3.5\n\n* Add support for local inference\n* Add new pattern to recognize plain text dash bullets\n* Add test for bullet patterns\n* Fix for `partition_html` that allows for processing `div` tags that have both text and child\n  elements\n* Add ability to extract document metadata from `.docx`, `.xlsx`, and `.jpg` files.\n* Helper functions for identifying and extracting phone numbers\n* Add new function `extract_attachment_info` that extracts and decodes the attachment\n  of an email.\n* Staging brick to convert a list of `Element`s to a `pandas` dataframe.\n* Add plain text functionality to `partition_email`\n\n## 0.3.4\n\n* Python-3.7 compat\n\n## 0.3.3\n\n* Removes BasicConfig from logger configuration\n* Adds the `partition_email` partitioning brick\n* Adds the `replace_mime_encodings` cleaning bricks\n* Small fix to HTML parsing related to processing list items with sub-tags\n* Add `EmailElement` data structure to store email documents\n\n## 0.3.2\n\n* Added `translate_text` brick for translating text between languages\n* Add an `apply` method to make it easier to apply cleaners to elements\n\n## 0.3.1\n\n* Added \\_\\_init.py\\_\\_ to `partition`\n\n## 0.3.0\n\n* Implement staging brick for Argilla. Converts lists of `Text` elements to `argilla` dataset classes.\n* Removing the local PDF parsing code and any dependencies and tests.\n* Reorganizes the staging bricks in the unstructured.partition module\n* Allow entities to be passed into the Datasaur staging brick\n* Added HTML escapes to the `replace_unicode_quotes` brick\n* Fix bad responses in partition_pdf to raise ValueError\n* Adds `partition_html` for partitioning HTML documents.\n\n## 0.2.6\n\n* Small change to how \\_read is placed within the inheritance structure since it doesn't really apply to pdf\n* Add partitioning brick for calling the document image analysis API\n\n## 0.2.5\n\n* Update python requirement to >=3.7\n\n## 0.2.4\n\n* Add alternative way of importing `Final` to support google colab\n\n## 0.2.3\n\n* Add cleaning bricks for removing prefixes and postfixes\n* Add cleaning bricks for extracting text before and after a pattern\n\n## 0.2.2\n\n* Add staging brick for Datasaur\n\n## 0.2.1\n\n* Added brick to convert an ISD dictionary to a list of elements\n* Update `PDFDocument` to use the `from_file` method\n* Added staging brick for CSV format for ISD (Initial Structured Data) format.\n* Added staging brick for separating text into attention window size chunks for `transformers`.\n* Added staging brick for LabelBox.\n* Added ability to upload LabelStudio predictions\n* Added utility function for JSONL reading and writing\n* Added staging brick for CSV format for Prodigy\n* Added staging brick for Prodigy\n* Added ability to upload LabelStudio annotations\n* Added text_field and id_field to stage_for_label_studio signature\n\n## 0.2.0\n\n* Initial release of unstructured\n"
        },
        {
          "name": "CODE_OF_CONDUCT.md",
          "type": "blob",
          "size": 5.1025390625,
          "content": "# Contributor Covenant Code of Conduct\n\n## Our Pledge\n\nWe as members, contributors, and leaders pledge to make participation in our\ncommunity a harassment-free experience for everyone, regardless of age, body\nsize, visible or invisible disability, ethnicity, sex characteristics, gender\nidentity and expression, level of experience, education, socio-economic status,\nnationality, personal appearance, race, religion, or sexual identity\nand orientation.\n\nWe pledge to act and interact in ways that contribute to an open, welcoming,\ndiverse, inclusive, and healthy community.\n\n## Our Standards\n\nExamples of behavior that contributes to a positive environment for our\ncommunity include:\n\n* Demonstrating empathy and kindness toward other people\n* Being respectful of differing opinions, viewpoints, and experiences\n* Giving and gracefully accepting constructive feedback\n* Accepting responsibility and apologizing to those affected by our mistakes,\n  and learning from the experience\n* Focusing on what is best not just for us as individuals, but for the\n  overall community\n\nExamples of unacceptable behavior include:\n\n* The use of sexualized language or imagery, and sexual attention or\n  advances of any kind\n* Trolling, insulting or derogatory comments, and personal or political attacks\n* Public or private harassment\n* Publishing others' private information, such as a physical or email\n  address, without their explicit permission\n* Other conduct which could reasonably be considered inappropriate in a\n  professional setting\n\n## Enforcement Responsibilities\n\nCommunity leaders are responsible for clarifying and enforcing our standards of\nacceptable behavior and will take appropriate and fair corrective action in\nresponse to any behavior that they deem inappropriate, threatening, offensive,\nor harmful.\n\nCommunity leaders have the right and responsibility to remove, edit, or reject\ncomments, commits, code, wiki edits, issues, and other contributions that are\nnot aligned to this Code of Conduct, and will communicate reasons for moderation\ndecisions when appropriate.\n\n## Scope\n\nThis Code of Conduct applies within all community spaces, and also applies when\nan individual is officially representing the community in public spaces.\nExamples of representing our community include using an official e-mail address,\nposting via an official social media account, or acting as an appointed\nrepresentative at an online or offline event.\n\n## Enforcement\n\nInstances of abusive, harassing, or otherwise unacceptable behavior may be\nreported to the community leaders responsible for enforcement at\nsupport@unstructured.io.\nAll complaints will be reviewed and investigated promptly and fairly.\n\nAll community leaders are obligated to respect the privacy and security of the\nreporter of any incident.\n\n## Enforcement Guidelines\n\nCommunity leaders will follow these Community Impact Guidelines in determining\nthe consequences for any action they deem in violation of this Code of Conduct:\n\n### 1. Correction\n\n**Community Impact**: Use of inappropriate language or other behavior deemed\nunprofessional or unwelcome in the community.\n\n**Consequence**: A private, written warning from community leaders, providing\nclarity around the nature of the violation and an explanation of why the\nbehavior was inappropriate. A public apology may be requested.\n\n### 2. Warning\n\n**Community Impact**: A violation through a single incident or series\nof actions.\n\n**Consequence**: A warning with consequences for continued behavior. No\ninteraction with the people involved, including unsolicited interaction with\nthose enforcing the Code of Conduct, for a specified period of time. This\nincludes avoiding interactions in community spaces as well as external channels\nlike social media. Violating these terms may lead to a temporary or\npermanent ban.\n\n### 3. Temporary Ban\n\n**Community Impact**: A serious violation of community standards, including\nsustained inappropriate behavior.\n\n**Consequence**: A temporary ban from any sort of interaction or public\ncommunication with the community for a specified period of time. No public or\nprivate interaction with the people involved, including unsolicited interaction\nwith those enforcing the Code of Conduct, is allowed during this period.\nViolating these terms may lead to a permanent ban.\n\n### 4. Permanent Ban\n\n**Community Impact**: Demonstrating a pattern of violation of community\nstandards, including sustained inappropriate behavior,  harassment of an\nindividual, or aggression toward or disparagement of classes of individuals.\n\n**Consequence**: A permanent ban from any sort of public interaction within\nthe community.\n\n## Attribution\n\nThis Code of Conduct is adapted from the [Contributor Covenant][homepage],\nversion 2.0, available at\nhttps://www.contributor-covenant.org/version/2/0/code_of_conduct.html.\n\nCommunity Impact Guidelines were inspired by [Mozilla's code of conduct\nenforcement ladder](https://github.com/mozilla/diversity).\n\n[homepage]: https://www.contributor-covenant.org\n\nFor answers to common questions about this code of conduct, see the FAQ at\nhttps://www.contributor-covenant.org/faq. Translations are available at\nhttps://www.contributor-covenant.org/translations.\n"
        },
        {
          "name": "CONTRIBUTING.md",
          "type": "blob",
          "size": 8.78515625,
          "content": "## Contributing to Unstructured\n\n[![Contributor Covenant](https://img.shields.io/badge/Contributor%20Covenant-2.1-4baaaa.svg)](code_of_conduct.md)\n\n👍🎉 First off, thank you for taking the time to contribute! 🎉👍\n\nThe following is a set of guidelines for contributing to the open source ecosystem of preprocessing pipeline APIs and supporting libraries hosted [here](https://github.com/Unstructured-IO).\n\nThis is meant to help the review process go smoothly, save the reviewer(s) time in catching common issues, and avoid submitting PRs that will be rejected by the CI.\n\nIn some cases it's convenient to put up a PR that's not ready for final review. This is fine (and under those circumstances it's not necessary to go through this checklist), but the PR should be put in draft mode so everyone knows it's not ready for review. \n\n### How to Contribute?\n\nIf you want to contribute, start working through the Unstructured codebase, navigate to the Github \"issues\" tab and start looking through interesting issues. If you are not sure of where to start, then start by trying one of the smaller/easier issues here i.e. issues with the \"good first issue\" label and then take a look at the issues with the \"contributions welcome\" label. These are issues that we believe are particularly well suited for outside contributions, often because we probably won't get to them right now. If you decide to start on an issue, leave a comment so that other people know that you're working on it. If you want to help out, but not alone, use the issue comment thread to coordinate.\n\n\n## Pull-Request Checklist\n\nThe following is a list of tasks to be completed before submitting a pull request for final review.\n\n### Before creating PR:\n\n1. Follow coding best practices\n    1. [ ] Make sure all new classes/functions/methods have docstrings.\n    1. [ ] Make sure all new functions/methods have type hints (optional for tests).\n    1. [ ] Make sure all new functions/methods have associated tests.\n    1. [ ] Update `CHANGELOG.md` and `__version__.py` if the core code has changed\n<br/><br/>\n1. Ensure environment is consistent\n    1. [ ] Update dependencies in `.in` files if needed (pay special attention to whether the current PR depends on changes to internal repos that are not packaged - if so the commit needs to be bumped).\n    1. [ ] If dependencies have changed, recompile dependencies with `make pip-compile`.\n    1. [ ] Make sure local virtual environment matches what CI will see - reinstall internal/external dependencies as needed.\\\n<sub>Follow the [virtualenv install instructions](https://github.com/Unstructured-IO/community#mac--homebrew) if you are unsure about working with virtual environments.\n<br/><br/>    \n1. Run tests and checks locally\n    1. [ ] Run tests locally with `make test`. Some repositories have supplemental tests with targets like `make test-integration` or `make test-sample-docs`. If applicable, run these as well. Try to make sure all tests are passing before submitting the PR, unless you are submitting in draft mode.\n    1. [ ] Run typing, linting, and formatting checks with `make check`. Some repositories have supplemental checks with targets like `make check-scripts` or `make check-notebooks`. If applicable, run these as well. Try to make sure all checks are passing before submitting the PR, unless you are submitting in draft mode.\n<br/><br/>    \n1. Ensure code is clean\n    1. [ ] Remove all debugging artifacts.\n    1. [ ] Remove commented out code. \n    1. [ ] For actual comments, note that our typical format is `# NOTE(<username>): <comment>`\n    1. [ ] Double check everything has been committed and pushed, recommended that local feature branch is clean.\n    \n### PR Guidelines:\n\n1. [ ] PR title should follow [conventional commit](https://www.conventionalcommits.org/en/v1.0.0/) standards.\n      \n1. [ ] PR description should give enough detail that the reviewer knows what they reviewing - sometimes a copy-paste of the added `CHANGELOG.md` items is enough, sometimes more detail is needed.\n\n1. [ ] If applicable, add a testing section to the PR description that recommends steps a reviewer can take to verify the changes, e.g. a snippet of code they can run locally.\n\n### License\n\nUnstructured open source projects are licensed under the [Apache 2.0 license](https://www.apache.org/licenses/LICENSE-2.0).\n\nInclude a license at the top of new `setup.py` files:\n\n- [Python license example](https://github.com/Unstructured-IO/unstructured/blob/main/setup.py)\n\n\n## Conventions\n\nFor pull requests, our convention is to squash and merge. For PR titles, we use [conventional commit](https://www.freecodecamp.org/news/how-to-write-better-git-commit-messages/#conventional-commits) messages. The format should look like \n\n- `<type>: <description>`.\n\nFor example, if the PR addresses a new feature, the PR title should look like: \n\n- `feat: Implements exciting new feature`. \n\nFor feature branches, the naming convention is:\n\n- `<username>/<description>`. \n\nFor the commit above, coming from the user called `contributor` the branch name would look like: \n\n- `contributor/exciting-new-feature`.\n\nHere is a list of some of the most common possible commit types:\n\n- `feat` – a new feature is introduced with the changes\n- `fix` – a bug fix has occurred\n- `chore` – changes that do not relate to a fix or feature and don't modify src or test files (for example updating dependencies)\n- `refactor` – refactored code that neither fixes a bug nor adds a feature\n- `docs` – updates to documentation such as a the README or other markdown files\n\n### Why should you write better commit messages?\n\nBy writing good commits, you are simply future-proofing yourself. You could save yourself and/or coworkers hours of digging around while troubleshooting by providing that helpful description 🙂. \n\nThe extra time it takes to write a thoughtful commit message as a letter to your potential future self is extremely worthwhile. On large scale projects, documentation is imperative for maintenance.\n\nCollaboration and communication are of utmost importance within engineering teams. The Git commit message is a prime example of this. I highly suggest setting up a convention for commit messages on your team if you do not already have one in place.\n\n\n## Code of Conduct\n\nIn the interest of fostering an open and welcoming environment, we as contributors and maintainers pledge to making participation in our project and our community a harassment-free experience for everyone, regardless of age, body size, disability, ethnicity, gender identity and expression, level of experience, nationality, personal appearance, race, religion, or sexual identity and orientation.\n\n### Enforcement\n\nPlease report unacceptable behavior to support@unstructured.io. All complaints will be reviewed and investigated and will result in a response that is deemed necessary and appropriate to the circumstances. The project team is obligated to maintain confidentiality with regard to the reporter of an incident. Further details of specific enforcement policies may be posted separately.\n\nProject maintainers who do not follow or enforce the Code of Conduct in good faith may face temporary or permanent repercussions as determined by other members of the project's leadership.\n\nThank you! 🤗\n\nThe Unstructured Team\n\n\n## Learn more\n\n| Section | Description |\n|-|-|\n| [Company Website](https://unstructured.io) | Unstructured.io product and company info |\n| [Documentation](https://docs.unstructured.io/) | Full API documentation |\n| [Working with Pull Requests](https://docs.github.com/en/pull-requests/collaborating-with-pull-requests/proposing-changes-to-your-work-with-pull-requests/about-pull-requests) | About pull requests |\n| [Code of Conduct](https://www.contributor-covenant.org/version/1/4/code-of-conduct/) | Contributor Covenant Code Of Conduct |\n| [Conventional Commits](https://www.freecodecamp.org/news/how-to-write-better-git-commit-messages/) | How to write better git commit messages |\n| [Conventional Commits](https://www.conventionalcommits.org/en/v1.0.0/) | Lightweight convention on top of commit messages |\n| [First Contributions](https://github.com/firstcontributions/first-contributions/blob/main/README.md) | Beginners' guide to make their first contribution! |\n\n\n## Contributing Guides\n\nIf you're stumped 😓, here are some good examples of contribution guidelines:\n\n- The GitHub Docs [contribution guidelines](https://github.com/github/docs/blob/main/CONTRIBUTING.md).\n- The Ruby on Rails [contribution guidelines](https://github.com/rails/rails/blob/main/CONTRIBUTING.md).\n- The Open Government [contribution guidelines](https://github.com/opengovernment/opengovernment/blob/master/CONTRIBUTING.md).\n- The MMOCR [contribution guidelines](https://mmocr.readthedocs.io/en/dev-1.x/notes/contribution_guide.html).\n- The HuggingFace [contribution guidelines](https://huggingface2.notion.site/Contribution-Guide-19411c29298644df8e9656af45a7686d).\n"
        },
        {
          "name": "Dockerfile",
          "type": "blob",
          "size": 1.2021484375,
          "content": "FROM quay.io/unstructured-io/base-images:wolfi-base-latest AS base\n\nARG PYTHON=python3.11\nARG PIP=pip3.11\n\nUSER root\n\nWORKDIR /app\n\nCOPY ./requirements requirements/\nCOPY unstructured unstructured\nCOPY test_unstructured test_unstructured\nCOPY example-docs example-docs\n\nRUN chown -R notebook-user:notebook-user /app && \\\n    apk add font-ubuntu git && \\\n    fc-cache -fv && \\\n    [ -e /usr/bin/python3 ] || ln -s /usr/bin/$PYTHON /usr/bin/python3\n\nUSER notebook-user\n\nENV NLTK_DATA=/home/notebook-user/nltk_data\n\n# Install Python dependencies and download required NLTK packages\nRUN find requirements/ -type f -name \"*.txt\" -exec $PIP install --no-cache-dir --user -r '{}' ';' && \\\n    mkdir -p ${NLTK_DATA} && \\\n    $PYTHON -m nltk.downloader -d ${NLTK_DATA} punkt_tab averaged_perceptron_tagger_eng && \\\n    $PYTHON -c \"from unstructured.partition.model_init import initialize; initialize()\" && \\\n    $PYTHON -c \"from unstructured_inference.models.tables import UnstructuredTableTransformerModel; model = UnstructuredTableTransformerModel(); model.initialize('microsoft/table-transformer-structure-recognition')\"\n\nENV PATH=\"${PATH}:/home/notebook-user/.local/bin\"\nENV TESSDATA_PREFIX=/usr/local/share/tessdata\n\nCMD [\"/bin/bash\"]\n"
        },
        {
          "name": "LICENSE.md",
          "type": "blob",
          "size": 11.09375,
          "content": "                                 Apache License\n                           Version 2.0, January 2004\n                        http://www.apache.org/licenses/\n\n   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n\n   1. Definitions.\n\n      \"License\" shall mean the terms and conditions for use, reproduction,\n      and distribution as defined by Sections 1 through 9 of this document.\n\n      \"Licensor\" shall mean the copyright owner or entity authorized by\n      the copyright owner that is granting the License.\n\n      \"Legal Entity\" shall mean the union of the acting entity and all\n      other entities that control, are controlled by, or are under common\n      control with that entity. For the purposes of this definition,\n      \"control\" means (i) the power, direct or indirect, to cause the\n      direction or management of such entity, whether by contract or\n      otherwise, or (ii) ownership of fifty percent (50%) or more of the\n      outstanding shares, or (iii) beneficial ownership of such entity.\n\n      \"You\" (or \"Your\") shall mean an individual or Legal Entity\n      exercising permissions granted by this License.\n\n      \"Source\" form shall mean the preferred form for making modifications,\n      including but not limited to software source code, documentation\n      source, and configuration files.\n\n      \"Object\" form shall mean any form resulting from mechanical\n      transformation or translation of a Source form, including but\n      not limited to compiled object code, generated documentation,\n      and conversions to other media types.\n\n      \"Work\" shall mean the work of authorship, whether in Source or\n      Object form, made available under the License, as indicated by a\n      copyright notice that is included in or attached to the work\n      (an example is provided in the Appendix below).\n\n      \"Derivative Works\" shall mean any work, whether in Source or Object\n      form, that is based on (or derived from) the Work and for which the\n      editorial revisions, annotations, elaborations, or other modifications\n      represent, as a whole, an original work of authorship. For the purposes\n      of this License, Derivative Works shall not include works that remain\n      separable from, or merely link (or bind by name) to the interfaces of,\n      the Work and Derivative Works thereof.\n\n      \"Contribution\" shall mean any work of authorship, including\n      the original version of the Work and any modifications or additions\n      to that Work or Derivative Works thereof, that is intentionally\n      submitted to Licensor for inclusion in the Work by the copyright owner\n      or by an individual or Legal Entity authorized to submit on behalf of\n      the copyright owner. For the purposes of this definition, \"submitted\"\n      means any form of electronic, verbal, or written communication sent\n      to the Licensor or its representatives, including but not limited to\n      communication on electronic mailing lists, source code control systems,\n      and issue tracking systems that are managed by, or on behalf of, the\n      Licensor for the purpose of discussing and improving the Work, but\n      excluding communication that is conspicuously marked or otherwise\n      designated in writing by the copyright owner as \"Not a Contribution.\"\n\n      \"Contributor\" shall mean Licensor and any individual or Legal Entity\n      on behalf of whom a Contribution has been received by Licensor and\n      subsequently incorporated within the Work.\n\n   2. Grant of Copyright License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      copyright license to reproduce, prepare Derivative Works of,\n      publicly display, publicly perform, sublicense, and distribute the\n      Work and such Derivative Works in Source or Object form.\n\n   3. Grant of Patent License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      (except as stated in this section) patent license to make, have made,\n      use, offer to sell, sell, import, and otherwise transfer the Work,\n      where such license applies only to those patent claims licensable\n      by such Contributor that are necessarily infringed by their\n      Contribution(s) alone or by combination of their Contribution(s)\n      with the Work to which such Contribution(s) was submitted. If You\n      institute patent litigation against any entity (including a\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\n      or a Contribution incorporated within the Work constitutes direct\n      or contributory patent infringement, then any patent licenses\n      granted to You under this License for that Work shall terminate\n      as of the date such litigation is filed.\n\n   4. Redistribution. You may reproduce and distribute copies of the\n      Work or Derivative Works thereof in any medium, with or without\n      modifications, and in Source or Object form, provided that You\n      meet the following conditions:\n\n      (a) You must give any other recipients of the Work or\n          Derivative Works a copy of this License; and\n\n      (b) You must cause any modified files to carry prominent notices\n          stating that You changed the files; and\n\n      (c) You must retain, in the Source form of any Derivative Works\n          that You distribute, all copyright, patent, trademark, and\n          attribution notices from the Source form of the Work,\n          excluding those notices that do not pertain to any part of\n          the Derivative Works; and\n\n      (d) If the Work includes a \"NOTICE\" text file as part of its\n          distribution, then any Derivative Works that You distribute must\n          include a readable copy of the attribution notices contained\n          within such NOTICE file, excluding those notices that do not\n          pertain to any part of the Derivative Works, in at least one\n          of the following places: within a NOTICE text file distributed\n          as part of the Derivative Works; within the Source form or\n          documentation, if provided along with the Derivative Works; or,\n          within a display generated by the Derivative Works, if and\n          wherever such third-party notices normally appear. The contents\n          of the NOTICE file are for informational purposes only and\n          do not modify the License. You may add Your own attribution\n          notices within Derivative Works that You distribute, alongside\n          or as an addendum to the NOTICE text from the Work, provided\n          that such additional attribution notices cannot be construed\n          as modifying the License.\n\n      You may add Your own copyright statement to Your modifications and\n      may provide additional or different license terms and conditions\n      for use, reproduction, or distribution of Your modifications, or\n      for any such Derivative Works as a whole, provided Your use,\n      reproduction, and distribution of the Work otherwise complies with\n      the conditions stated in this License.\n\n   5. Submission of Contributions. Unless You explicitly state otherwise,\n      any Contribution intentionally submitted for inclusion in the Work\n      by You to the Licensor shall be under the terms and conditions of\n      this License, without any additional terms or conditions.\n      Notwithstanding the above, nothing herein shall supersede or modify\n      the terms of any separate license agreement you may have executed\n      with Licensor regarding such Contributions.\n\n   6. Trademarks. This License does not grant permission to use the trade\n      names, trademarks, service marks, or product names of the Licensor,\n      except as required for reasonable and customary use in describing the\n      origin of the Work and reproducing the content of the NOTICE file.\n\n   7. Disclaimer of Warranty. Unless required by applicable law or\n      agreed to in writing, Licensor provides the Work (and each\n      Contributor provides its Contributions) on an \"AS IS\" BASIS,\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n      implied, including, without limitation, any warranties or conditions\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n      PARTICULAR PURPOSE. You are solely responsible for determining the\n      appropriateness of using or redistributing the Work and assume any\n      risks associated with Your exercise of permissions under this License.\n\n   8. Limitation of Liability. In no event and under no legal theory,\n      whether in tort (including negligence), contract, or otherwise,\n      unless required by applicable law (such as deliberate and grossly\n      negligent acts) or agreed to in writing, shall any Contributor be\n      liable to You for damages, including any direct, indirect, special,\n      incidental, or consequential damages of any character arising as a\n      result of this License or out of the use or inability to use the\n      Work (including but not limited to damages for loss of goodwill,\n      work stoppage, computer failure or malfunction, or any and all\n      other commercial damages or losses), even if such Contributor\n      has been advised of the possibility of such damages.\n\n   9. Accepting Warranty or Additional Liability. While redistributing\n      the Work or Derivative Works thereof, You may choose to offer,\n      and charge a fee for, acceptance of support, warranty, indemnity,\n      or other liability obligations and/or rights consistent with this\n      License. However, in accepting such obligations, You may act only\n      on Your own behalf and on Your sole responsibility, not on behalf\n      of any other Contributor, and only if You agree to indemnify,\n      defend, and hold each Contributor harmless for any liability\n      incurred by, or claims asserted against, such Contributor by reason\n      of your accepting any such warranty or additional liability.\n\n   END OF TERMS AND CONDITIONS\n\n   APPENDIX: How to apply the Apache License to your work.\n\n      To apply the Apache License to your work, attach the following\n      boilerplate notice, with the fields enclosed by brackets \"[]\"\n      replaced with your own identifying information. (Don't include\n      the brackets!)  The text should be enclosed in the appropriate\n      comment syntax for the file format. We also recommend that a\n      file or class name and description of purpose be included on the\n      same \"printed page\" as the copyright notice for easier\n      identification within third-party archives.\n\n   Copyright 2022 Unstructured Technologies, Inc\n\n   Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.\n"
        },
        {
          "name": "MANIFEST.in",
          "type": "blob",
          "size": 0.5185546875,
          "content": "# Base requirements and constraints\ninclude requirements/base.in\n\n# Unstructured library extras\ninclude requirements/extra-csv.in\ninclude requirements/extra-docx.in\ninclude requirements/extra-epub.in\ninclude requirements/extra-markdown.in\ninclude requirements/extra-msg.in\ninclude requirements/extra-odt.in\ninclude requirements/extra-paddleocr.in\ninclude requirements/extra-pandoc.in\ninclude requirements/extra-pdf-image.in\ninclude requirements/extra-pptx.in\ninclude requirements/extra-xlsx.in\ninclude requirements/huggingface.in\n\n"
        },
        {
          "name": "Makefile",
          "type": "blob",
          "size": 9.4365234375,
          "content": "PACKAGE_NAME := unstructured\nPIP_VERSION := 23.2.1\nCURRENT_DIR := $(shell pwd)\nARCH := $(shell uname -m)\nPYTHON ?= python3\n\n.PHONY: help\nhelp: Makefile\n\t@sed -n 's/^\\(## \\)\\([a-zA-Z]\\)/\\2/p' $<\n\n\n###########\n# Install #\n###########\n\n## install-base:            installs core requirements needed for text processing bricks\n.PHONY: install-base\ninstall-base: install-base-pip-packages install-nltk-models\n\n## install:                 installs all test, dev, and experimental requirements\n.PHONY: install\ninstall: install-base-pip-packages install-dev install-nltk-models install-test install-huggingface install-all-docs\n\n.PHONY: install-ci\ninstall-ci: install-base-pip-packages install-nltk-models install-huggingface install-all-docs install-test install-pandoc\n\n.PHONY: install-base-ci\ninstall-base-ci: install-base-pip-packages install-nltk-models install-test install-pandoc\n\n.PHONY: install-base-pip-packages\ninstall-base-pip-packages:\n\t${PYTHON} -m pip install pip==${PIP_VERSION}\n\t${PYTHON} -m pip install -r requirements/base.txt\n\n.PHONY: install-huggingface\ninstall-huggingface:\n\t${PYTHON} -m pip install pip==${PIP_VERSION}\n\t${PYTHON} -m pip install -r requirements/huggingface.txt\n\n.PHONY: install-nltk-models\ninstall-nltk-models:\n\t${PYTHON} -c \"from unstructured.nlp.tokenize import download_nltk_packages; download_nltk_packages()\"\n\n.PHONY: install-test\ninstall-test:\n\t${PYTHON} -m pip install -r requirements/test.txt\n\t# NOTE(yao) - CI seem to always install tesseract to test so it would make sense to also require\n\t# pytesseract installation into the virtual env for testing\n\t${PYTHON} -m pip install unstructured_pytesseract\n\t# ${PYTHON} -m pip install argilla==1.28.0 -c requirements/deps/constraints.txt\n\t# NOTE(robinson) - Installing weaviate-client separately here because the requests\n\t# version conflicts with label_studio_sdk\n\t${PYTHON} -m pip install weaviate-client -c requirements/deps/constraints.txt\n\n.PHONY: install-dev\ninstall-dev:\n\t${PYTHON} -m pip install -r requirements/dev.txt\n\n.PHONY: install-build\ninstall-build:\n\t${PYTHON} -m pip install -r requirements/build.txt\n\n.PHONY: install-csv\ninstall-csv:\n\t${PYTHON} -m pip install -r requirements/extra-csv.txt\n\n.PHONY: install-docx\ninstall-docx:\n\t${PYTHON} -m pip install -r requirements/extra-docx.txt\n\n.PHONY: install-epub\ninstall-epub:\n\t${PYTHON} -m pip install -r requirements/extra-epub.txt\n\n.PHONY: install-odt\ninstall-odt:\n\t${PYTHON} -m pip install -r requirements/extra-odt.txt\n\n.PHONY: install-pypandoc\ninstall-pypandoc:\n\t${PYTHON} -m pip install -r requirements/extra-pandoc.txt\n\n.PHONY: install-markdown\ninstall-markdown:\n\t${PYTHON} -m pip install -r requirements/extra-markdown.txt\n\n.PHONY: install-pdf-image\ninstall-pdf-image:\n\t${PYTHON} -m pip install -r requirements/extra-pdf-image.txt\n\n.PHONY: install-pptx\ninstall-pptx:\n\t${PYTHON} -m pip install -r requirements/extra-pptx.txt\n\n.PHONY: install-xlsx\ninstall-xlsx:\n\t${PYTHON} -m pip install -r requirements/extra-xlsx.txt\n\n.PHONY: install-all-docs\ninstall-all-docs: install-base install-csv install-docx install-epub install-odt install-pypandoc install-markdown install-pdf-image install-pptx install-xlsx\n\n.PHONY: install-ingest\ninstall-ingest:\n\tpython3 -m pip install -r requirements/ingest/ingest.txt\n\n## install-local-inference: installs requirements for local inference\n.PHONY: install-local-inference\ninstall-local-inference: install install-all-docs\n\n.PHONY: install-pandoc\ninstall-pandoc:\n\tARCH=${ARCH} ./scripts/install-pandoc.sh\n\n## pip-compile:             compiles all base/dev/test requirements\n.PHONY: pip-compile\npip-compile:\n\t@scripts/pip-compile.sh\n\n## install-project-local:   install unstructured into your local python environment\n.PHONY: install-project-local\ninstall-project-local: install\n\t# MAYBE TODO: fail if already exists?\n\t${PYTHON} -m pip install -e .\n\n## uninstall-project-local: uninstall unstructured from your local python environment\n.PHONY: uninstall-project-local\nuninstall-project-local:\n\t${PYTHON} -m pip uninstall ${PACKAGE_NAME}\n\n#################\n# Test and Lint #\n#################\n\nexport CI ?= false\nexport UNSTRUCTURED_INCLUDE_DEBUG_METADATA ?= false\n\n## test:                    runs all unittests\n.PHONY: test\ntest:\n\tPYTHONPATH=. CI=$(CI) \\\n\tUNSTRUCTURED_INCLUDE_DEBUG_METADATA=$(UNSTRUCTURED_INCLUDE_DEBUG_METADATA) ${PYTHON} -m pytest test_${PACKAGE_NAME} --cov=${PACKAGE_NAME} --cov-report term-missing --durations=40\n\n.PHONY: test-unstructured-api-unit\ntest-unstructured-api-unit:\n\tscripts/test-unstructured-api-unit.sh\n\n.PHONY: test-no-extras\ntest-no-extras:\n\tPYTHONPATH=. CI=$(CI) \\\n\t\tUNSTRUCTURED_INCLUDE_DEBUG_METADATA=$(UNSTRUCTURED_INCLUDE_DEBUG_METADATA) ${PYTHON} -m pytest \\\n\t\ttest_${PACKAGE_NAME}/partition/test_text.py \\\n\t\ttest_${PACKAGE_NAME}/partition/test_email.py \\\n\t\ttest_${PACKAGE_NAME}/partition/html/test_partition.py \\\n\t\ttest_${PACKAGE_NAME}/partition/test_xml.py\n\n.PHONY: test-extra-csv\ntest-extra-csv:\n\tPYTHONPATH=. CI=$(CI) ${PYTHON} -m pytest \\\n\t\ttest_unstructured/partition/test_csv.py \\\n\t\ttest_unstructured/partition/test_tsv.py\n\n.PHONY: test-extra-docx\ntest-extra-docx:\n\tPYTHONPATH=. CI=$(CI) ${PYTHON} -m pytest \\\n\t\ttest_unstructured/partition/test_doc.py \\\n\t\ttest_unstructured/partition/test_docx.py\n\n.PHONY: test-extra-epub\ntest-extra-epub:\n\tPYTHONPATH=. CI=$(CI) ${PYTHON} -m pytest test_unstructured/partition/test_epub.py\n\n.PHONY: test-extra-markdown\ntest-extra-markdown:\n\tPYTHONPATH=. CI=$(CI) ${PYTHON} -m pytest test_unstructured/partition/test_md.py\n\n.PHONY: test-extra-odt\ntest-extra-odt:\n\tPYTHONPATH=. CI=$(CI) ${PYTHON} -m pytest test_unstructured/partition/test_odt.py\n\n.PHONY: test-extra-pdf-image\ntest-extra-pdf-image:\n\tPYTHONPATH=. CI=$(CI) ${PYTHON} -m pytest test_unstructured/partition/pdf_image\n\n.PHONY: test-extra-pptx\ntest-extra-pptx:\n\tPYTHONPATH=. CI=$(CI) ${PYTHON} -m pytest \\\n\t\ttest_unstructured/partition/test_ppt.py \\\n\t\ttest_unstructured/partition/test_pptx.py\n\n.PHONY: test-extra-pypandoc\ntest-extra-pypandoc:\n\tPYTHONPATH=. CI=$(CI) ${PYTHON} -m pytest \\\n\t\ttest_unstructured/partition/test_org.py \\\n\t\ttest_unstructured/partition/test_rst.py \\\n\t\ttest_unstructured/partition/test_rtf.py\n\n.PHONY: test-extra-xlsx\ntest-extra-xlsx:\n\tPYTHONPATH=. CI=$(CI) ${PYTHON} -m pytest test_unstructured/partition/test_xlsx.py\n\n.PHONY: test-text-extraction-evaluate\ntest-text-extraction-evaluate:\n\tPYTHONPATH=. CI=$(CI) ${PYTHON} -m pytest test_unstructured/metrics/test_text_extraction.py\n\n## check:                   runs linters (includes tests)\n.PHONY: check\ncheck: check-ruff check-black check-flake8 check-version\n\n.PHONY: check-shfmt\ncheck-shfmt:\n\tshfmt -i 2 -d .\n\n.PHONY: check-black\ncheck-black:\n\t${PYTHON} -m black . --check --line-length=100\n\n.PHONY: check-flake8\ncheck-flake8:\n\t${PYTHON} -m flake8 .\n\n.PHONY: check-licenses\ncheck-licenses:\n\t@scripts/check-licenses.sh\n\n.PHONY: check-ruff\ncheck-ruff:\n    # -- ruff options are determined by pyproject.toml --\n\truff check .\n\n.PHONY: check-autoflake\ncheck-autoflake:\n\tautoflake --check-diff .\n\n## check-scripts:           run shellcheck\n.PHONY: check-scripts\ncheck-scripts:\n    # Fail if any of these files have warnings\n\tscripts/shellcheck.sh\n\n## check-version:           run check to ensure version in CHANGELOG.md matches version in package\n.PHONY: check-version\ncheck-version:\n    # Fail if syncing version would produce changes\n\tscripts/version-sync.sh -c \\\n\t\t-f \"unstructured/__version__.py\" semver\n\n## tidy:                    run black\n.PHONY: tidy\ntidy: tidy-python\n\n.PHONY: tidy_shell\ntidy-shell:\n\tshfmt -i 2 -l -w .\n\n.PHONY: tidy-python\ntidy-python:\n\truff . --fix-only || true\n\tautoflake --in-place .\n\tblack --line-length=100 .\n\n## version-sync:            update __version__.py with most recent version from CHANGELOG.md\n.PHONY: version-sync\nversion-sync:\n\tscripts/version-sync.sh \\\n\t\t-f \"unstructured/__version__.py\" semver\n\n.PHONY: check-coverage\ncheck-coverage:\n\t${PYTHON} -m coverage report --fail-under=90\n\n## check-deps:              check consistency of dependencies\n.PHONY: check-deps\ncheck-deps:\n\tscripts/consistent-deps.sh\n\n.PHONY: check-extras\ncheck-extras:\n\tscripts/check-extras.sh\n\n##########\n# Docker #\n##########\n\n# Docker targets are provided for convenience only and are not required in a standard development environment\n\nDOCKER_IMAGE ?= unstructured:dev\n\n.PHONY: docker-build\ndocker-build:\n\tPIP_VERSION=${PIP_VERSION} DOCKER_IMAGE_NAME=${DOCKER_IMAGE} ./scripts/docker-build.sh\n\n.PHONY: docker-start-bash\ndocker-start-bash:\n\tdocker run -ti --rm ${DOCKER_IMAGE}\n\n.PHONY: docker-start-dev\ndocker-start-dev:\n\tdocker run --rm \\\n\t-v ${CURRENT_DIR}:/mnt/local_unstructued \\\n\t-ti ${DOCKER_IMAGE}\n\n.PHONY: docker-test\ndocker-test:\n\tdocker run --rm \\\n\t-v ${CURRENT_DIR}/test_unstructured:/home/notebook-user/test_unstructured \\\n\t-v ${CURRENT_DIR}/test_unstructured_ingest:/home/notebook-user/test_unstructured_ingest \\\n\t$(if $(wildcard uns_test_env_file),--env-file uns_test_env_file,) \\\n\t$(DOCKER_IMAGE) \\\n\tbash -c \"CI=$(CI) \\\n\tUNSTRUCTURED_INCLUDE_DEBUG_METADATA=$(UNSTRUCTURED_INCLUDE_DEBUG_METADATA) \\\n\tpytest $(if $(TEST_FILE),$(TEST_FILE),test_unstructured)\"\n\n.PHONY: docker-smoke-test\ndocker-smoke-test:\n\tDOCKER_IMAGE=${DOCKER_IMAGE} ./scripts/docker-smoke-test.sh\n\n\n###########\n# Jupyter #\n###########\n\n.PHONY: docker-jupyter-notebook\ndocker-jupyter-notebook:\n\tdocker run -p 8888:8888 --mount type=bind,source=$(realpath .),target=/home --entrypoint jupyter-notebook -t --rm ${DOCKER_IMAGE} --allow-root --port 8888 --ip 0.0.0.0 --NotebookApp.token='' --NotebookApp.password=''\n\n\n.PHONY: run-jupyter\nrun-jupyter:\n\tPYTHONPATH=$(realpath .) JUPYTER_PATH=$(realpath .) jupyter-notebook --NotebookApp.token='' --NotebookApp.password=''\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 18.1044921875,
          "content": "<h3 align=\"center\">\n  <img\n    src=\"https://raw.githubusercontent.com/Unstructured-IO/unstructured/main/img/unstructured_logo.png\"\n    height=\"200\"\n  >\n</h3>\n\n<div align=\"center\">\n\n  <a href=\"https://github.com/Unstructured-IO/unstructured/blob/main/LICENSE.md\">![https://pypi.python.org/pypi/unstructured/](https://img.shields.io/pypi/l/unstructured.svg)</a>\n  <a href=\"https://pypi.python.org/pypi/unstructured/\">![https://pypi.python.org/pypi/unstructured/](https://img.shields.io/pypi/pyversions/unstructured.svg)</a>\n  <a href=\"https://GitHub.com/unstructured-io/unstructured/graphs/contributors\">![https://GitHub.com/unstructured-io/unstructured.js/graphs/contributors](https://img.shields.io/github/contributors/unstructured-io/unstructured)</a>\n  <a href=\"https://github.com/Unstructured-IO/unstructured/blob/main/CODE_OF_CONDUCT.md\">![code_of_conduct.md](https://img.shields.io/badge/Contributor%20Covenant-2.1-4baaaa.svg) </a>\n  <a href=\"https://GitHub.com/unstructured-io/unstructured/releases\">![https://GitHub.com/unstructured-io/unstructured.js/releases](https://img.shields.io/github/release/unstructured-io/unstructured)</a>\n  <a href=\"https://pypi.python.org/pypi/unstructured/\">![https://github.com/Naereen/badges/](https://badgen.net/badge/Open%20Source%20%3F/Yes%21/blue?icon=github)</a>\n  [![Downloads](https://static.pepy.tech/badge/unstructured)](https://pepy.tech/project/unstructured)\n  [![Downloads](https://static.pepy.tech/badge/unstructured/month)](https://pepy.tech/project/unstructured)\n  <a\n   href=\"https://www.phorm.ai/query?projectId=34efc517-2201-4376-af43-40c4b9da3dc5\">\n\t<img src=\"https://img.shields.io/badge/Phorm-Ask_AI-%23F2777A.svg?&logo=data:image/svg+xml;base64,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\" />\n   </a>\n\n</div>\n\n<div>\n  <p align=\"center\">\n  <a\n  href=\"https://short.unstructured.io/pzw05l7\">\n    <img src=\"https://img.shields.io/badge/JOIN US ON SLACK-4A154B?style=for-the-badge&logo=slack&logoColor=white\" />\n  </a>\n  <a href=\"https://www.linkedin.com/company/unstructuredio/\">\n    <img src=\"https://img.shields.io/badge/LinkedIn-0077B5?style=for-the-badge&logo=linkedin&logoColor=white\" />\n  </a>\n</div>\n\n<h2 align=\"center\">\n  <p>Open-Source Pre-Processing Tools for Unstructured Data</p>\n</h2>\n\nThe `unstructured` library provides open-source components for ingesting and pre-processing images and text documents, such as PDFs, HTML, Word docs, and [many more](https://docs.unstructured.io/open-source/core-functionality/partitioning). The use cases of `unstructured` revolve around streamlining and optimizing the data processing workflow for LLMs. `unstructured` modular functions and connectors form a cohesive system that simplifies data ingestion and pre-processing, making it adaptable to different platforms and efficient in transforming unstructured data into structured outputs.\n\n## Try the Unstructured Serverless API!\n\nLooking for better pre-processing performance and less setup?\nCheck out our new [Serverless API](https://unstructured.io/api-key-hosted)!\nThe Unstructured Serverless API is our most performant API yet, delivering a more responsive,\nproduction-grade solution to better support your business and LLM needs.\nHead to our [signup page](https://app.unstructured.io/) page to get started for\nfree.\n\n## :eight_pointed_black_star: Quick Start\n\nThere are several ways to use the `unstructured` library:\n* [Run the library in a container](https://github.com/Unstructured-IO/unstructured#run-the-library-in-a-container) or\n* Install the library\n    1. [Install from PyPI](https://github.com/Unstructured-IO/unstructured#installing-the-library)\n    2. [Install for local development](https://github.com/Unstructured-IO/unstructured#installation-instructions-for-local-development)\n* For installation with `conda` on Windows system, please refer to the [documentation](https://unstructured-io.github.io/unstructured/installing.html#installation-with-conda-on-windows)\n\n### Run the library in a container\n\nThe following instructions are intended to help you get up and running using Docker to interact with `unstructured`.\nSee [here](https://docs.docker.com/get-docker/) if you don't already have docker installed on your machine.\n\nNOTE: we build multi-platform images to support both x86_64 and Apple silicon hardware. `docker pull` should download the corresponding image for your architecture, but you can specify with `--platform` (e.g. `--platform linux/amd64`) if needed.\n\nWe build Docker images for all pushes to `main`. We tag each image with the corresponding short commit hash (e.g. `fbc7a69`) and the application version (e.g. `0.5.5-dev1`). We also tag the most recent image with `latest`. To leverage this, `docker pull` from our image repository.\n\n```bash\ndocker pull downloads.unstructured.io/unstructured-io/unstructured:latest\n```\n\nOnce pulled, you can create a container from this image and shell to it.\n\n```bash\n# create the container\ndocker run -dt --name unstructured downloads.unstructured.io/unstructured-io/unstructured:latest\n\n# this will drop you into a bash shell where the Docker image is running\ndocker exec -it unstructured bash\n```\n\nYou can also build your own Docker image. Note that the base image is `wolfi-base`, which is\nupdated regularly. If you are building the image locally, it is possible `docker-build` could\nfail due to upstream changes in `wolfi-base`.\n\nIf you only plan on parsing one type of data you can speed up building the image by commenting out some\nof the packages/requirements necessary for other data types. See Dockerfile to know which lines are necessary\nfor your use case.\n\n```bash\nmake docker-build\n\n# this will drop you into a bash shell where the Docker image is running\nmake docker-start-bash\n```\n\nOnce in the running container, you can try things directly in Python interpreter's interactive mode.\n```bash\n# this will drop you into a python console so you can run the below partition functions\npython3\n\n>>> from unstructured.partition.pdf import partition_pdf\n>>> elements = partition_pdf(filename=\"example-docs/layout-parser-paper-fast.pdf\")\n\n>>> from unstructured.partition.text import partition_text\n>>> elements = partition_text(filename=\"example-docs/fake-text.txt\")\n```\n\n### Installing the library\nUse the following instructions to get up and running with `unstructured` and test your\ninstallation.\n\n- Install the Python SDK to support all document types with `pip install \"unstructured[all-docs]\"`\n  - For plain text files, HTML, XML, JSON and Emails that do not require any extra dependencies, you can run `pip install unstructured`\n  - To process other doc types, you can install the extras required for those documents, such as `pip install \"unstructured[docx,pptx]\"`\n- Install the following system dependencies if they are not already available on your system.\n  Depending on what document types you're parsing, you may not need all of these.\n    - `libmagic-dev` (filetype detection)\n    - `poppler-utils` (images and PDFs)\n    - `tesseract-ocr` (images and PDFs, install `tesseract-lang` for additional language support)\n    - `libreoffice` (MS Office docs)\n    - `pandoc` (EPUBs, RTFs and Open Office docs). Please note that to handle RTF files, you need version `2.14.2` or newer. Running either `make install-pandoc` or `./scripts/install-pandoc.sh` will install the correct version for you.\n\n- For suggestions on how to install on the Windows and to learn about dependencies for other features, see the\n  installation documentation [here](https://unstructured-io.github.io/unstructured/installing.html).\n\nAt this point, you should be able to run the following code:\n\n```python\nfrom unstructured.partition.auto import partition\n\nelements = partition(filename=\"example-docs/eml/fake-email.eml\")\nprint(\"\\n\\n\".join([str(el) for el in elements]))\n```\n\n### Installation Instructions for Local Development\n\nThe following instructions are intended to help you get up and running with `unstructured`\nlocally if you are planning to contribute to the project.\n\n* Using `pyenv` to manage virtualenv's is recommended but not necessary\n\t* Mac install instructions. See [here](https://github.com/Unstructured-IO/community#mac--homebrew) for more detailed instructions.\n\t\t* `brew install pyenv-virtualenv`\n\t  * `pyenv install 3.10`\n  * Linux instructions are available [here](https://github.com/Unstructured-IO/community#linux).\n\n* Create a virtualenv to work in and activate it, e.g. for one named `unstructured`:\n\n\t`pyenv  virtualenv 3.10 unstructured` <br />\n\t`pyenv activate unstructured`\n\n* Run `make install`\n\n* Optional:\n  * To install models and dependencies for processing images and PDFs locally, run `make install-local-inference`.\n  * For processing image files, `tesseract` is required. See [here](https://tesseract-ocr.github.io/tessdoc/Installation.html) for installation instructions.\n  * For processing PDF files, `tesseract` and `poppler` are required. The [pdf2image docs](https://pdf2image.readthedocs.io/en/latest/installation.html) have instructions on installing `poppler` across various platforms.\n\nAdditionally, if you're planning to contribute to `unstructured`, we provide you an optional `pre-commit` configuration\nfile to ensure your code matches the formatting and linting standards used in `unstructured`.\nIf you'd prefer not to have code changes auto-tidied before every commit, you can use  `make check` to see\nwhether any linting or formatting changes should be applied, and `make tidy` to apply them.\n\nIf using the optional `pre-commit`, you'll just need to install the hooks with `pre-commit install` since the\n`pre-commit` package is installed as part of `make install` mentioned above. Finally, if you decided to use `pre-commit`\nyou can also uninstall the hooks with `pre-commit uninstall`.\n\nIn addition to develop in your local OS we also provide a helper to use docker providing a development environment:\n\n```bash\nmake docker-start-dev\n```\n\nThis starts a docker container with your local repo mounted to `/mnt/local_unstructured`. This docker image allows you to develop without worrying about your OS's compatibility with the repo and its dependencies.\n\n## :clap: Quick Tour\n\n### Documentation\nFor more comprehensive documentation, visit https://docs.unstructured.io . You can also learn\nmore about our other products on the documentation page, including our SaaS API.\n\nHere are a few pages from the [Open Source documentation page](https://docs.unstructured.io/open-source/introduction/overview)\nthat are helpful for new users to review:\n\n- [Quick Start](https://docs.unstructured.io/open-source/introduction/quick-start)\n- [Using the `unstructured` open source package](https://docs.unstructured.io/open-source/core-functionality/overview)\n- [Connectors](https://docs.unstructured.io/open-source/ingest/overview)\n- [Concepts](https://docs.unstructured.io/open-source/concepts/document-elements)\n- [Integrations](https://docs.unstructured.io/open-source/integrations)\n\n\n### PDF Document Parsing Example\nThe following examples show how to get started with the `unstructured` library. The easiest way to parse a document in unstructured is to use the `partition` function. If you use `partition` function, `unstructured` will detect the file type and route it to the appropriate file-specific partitioning function. If you are using the `partition` function, you may need to install additional dependencies per doc type.\nFor example, to install docx dependencies you need to run `pip install \"unstructured[docx]\"`.\nSee our  [installation guide](https://docs.unstructured.io/open-source/installation/full-installation) for more details.\n\n```python\nfrom unstructured.partition.auto import partition\n\nelements = partition(\"example-docs/layout-parser-paper.pdf\")\n```\n\nRun `print(\"\\n\\n\".join([str(el) for el in elements]))` to get a string representation of the\noutput, which looks like:\n\n```\n\nLayoutParser : A Uniﬁed Toolkit for Deep Learning Based Document Image Analysis\n\nZejiang Shen 1 ( (cid:0) ), Ruochen Zhang 2 , Melissa Dell 3 , Benjamin Charles Germain Lee 4 , Jacob Carlson 3 , and\nWeining Li 5\n\nAbstract. Recent advances in document image analysis (DIA) have been primarily driven by the application of neural\nnetworks. Ideally, research outcomes could be easily deployed in production and extended for further investigation.\nHowever, various factors like loosely organized codebases and sophisticated model conﬁgurations complicate the easy\nreuse of important innovations by a wide audience. Though there have been ongoing eﬀorts to improve reusability and\nsimplify deep learning (DL) model development in disciplines like natural language processing and computer vision, none\nof them are optimized for challenges in the domain of DIA. This represents a major gap in the existing toolkit, as DIA\nis central to academic research across a wide range of disciplines in the social sciences and humanities. This paper\nintroduces LayoutParser, an open-source library for streamlining the usage of DL in DIA research and applications.\nThe core LayoutParser library comes with a set of simple and intuitive interfaces for applying and customizing DL models\nfor layout detection, character recognition, and many other document processing tasks. To promote extensibility,\nLayoutParser also incorporates a community platform for sharing both pre-trained models and full document digitization\npipelines. We demonstrate that LayoutParser is helpful for both lightweight and large-scale digitization pipelines in\nreal-word use cases. The library is publicly available at https://layout-parser.github.io\n\nKeywords: Document Image Analysis · Deep Learning · Layout Analysis · Character Recognition · Open Source library ·\nToolkit.\n\nIntroduction\n\nDeep Learning(DL)-based approaches are the state-of-the-art for a wide range of document image analysis (DIA) tasks\nincluding document image classiﬁcation [11,\n```\n\nSee the [partitioning](https://docs.unstructured.io/open-source/core-functionality/partitioning)\nsection in our documentation for a full list of options and instructions on how to use\nfile-specific partitioning functions.\n\n## :guardsman: Security Policy\n\nSee our [security policy](https://github.com/Unstructured-IO/unstructured/security/policy) for\ninformation on how to report security vulnerabilities.\n\n## :bug: Reporting Bugs\n\nEncountered a bug? Please create a new [GitHub issue](https://github.com/Unstructured-IO/unstructured/issues/new/choose) and use our bug report template to describe the problem. To help us diagnose the issue, use the `python scripts/collect_env.py` command to gather your system's environment information and include it in your report. Your assistance helps us continuously improve our software - thank you!\n\n## :books: Learn more\n\n| Section | Description |\n|-|-|\n| [Company Website](https://unstructured.io) | Unstructured.io product and company info |\n| [Documentation](https://docs.unstructured.io/) | Full API documentation |\n| [Batch Processing](https://github.com/Unstructured-IO/unstructured-ingest) | Ingesting batches of documents through Unstructured |\n\n## :chart_with_upwards_trend: Analytics\n\nWe’ve partnered with Scarf (https://scarf.sh) to collect anonymized user statistics to understand which features our community is using and how to prioritize product decision-making in the future. To learn more about how we collect and use this data, please read our [Privacy Policy](https://unstructured.io/privacy-policy).\nTo opt out of this data collection, you can set the environment variable `SCARF_NO_ANALYTICS=true` before running any `unstructured` commands.\n"
        },
        {
          "name": "discord-test",
          "type": "tree",
          "content": null
        },
        {
          "name": "docker",
          "type": "tree",
          "content": null
        },
        {
          "name": "docs",
          "type": "tree",
          "content": null
        },
        {
          "name": "environment.yml",
          "type": "blob",
          "size": 0.2255859375,
          "content": "name: unstructured\n\nchannels:\n  - defaults\n  - anaconda\n  - conda-forge\n  - pytorch\n\ndependencies:\n  - python=3.10\n  - pytorch=2.1.2\n  - pywin32\n  - poppler\n  - torchvision\n  - pip\n  - pip:\n    - huggingface-hub\n    - layoutparser\n"
        },
        {
          "name": "example-docs",
          "type": "tree",
          "content": null
        },
        {
          "name": "img",
          "type": "tree",
          "content": null
        },
        {
          "name": "liccheck.ini",
          "type": "blob",
          "size": 2.013671875,
          "content": "# Authorized and unauthorized licenses in LOWER CASE\n[Licenses]\nauthorized_licenses:\n        ######################\n        # Permissive Licenses\n        ######################\n\n        # Apache-2.0\n        apache\n        apache 2.0\n        apache-2.0\n        apache software license\n        apache software\n        apache license v2.0\n        apache license 2.0\n        apache license, version 2.0\n\n        # BSD\n        bsd\n        new bsd\n        bsd license\n        new bsd license\n        simplified bsd\n        3-clause bsd\n        freebsd\n        bsd 3-clause\n\n        # MIT\n        mit\n        mit license\n\n        # ISC\n        isc license\n        isc license (iscl)\n\n        # The Unlicense\n        the unlicense (unlicense)\n\n        # HPND\n        historical permission notice and disclaimer (hpnd)\n\n        #########################\n        # Weak Copy Left Licenses\n        #########################\n\n        # MPL-2.0\n        mozilla public license 2.0 (mpl 2.0)\n\n        # LGPL\n        gnu lesser general public license v2 or later (lgplv2+)\n        gnu lgpl\n        lgpl with exceptions or zpl\n        gnu library or lesser general public license (lgpl)\n        gnu lesser general public license v3 (lgplv3)\n        gnu general public license v2 (gplv2)\n\n        # PSF-2.0\n        python software foundation\n        python software foundation license\n\n\nunauthorized_licenses:\n        ###########################\n        # Strong Copy Left Licenses\n        ###########################\n        gpl v3\n\n[Authorized Packages]\n# Apache-2.0 https://github.com/chroma-core/hnswlib#Apache-2.0-1-ov-file\nchroma-hnswlib: >=0.7.3\n# MIT https://github.com/facebookresearch/iopath?tab=MIT-1-ov-file#readme\niopath: >=0.1.10\n# BSD https://github.com/PDFium/PDFium?tab=BSD-3-Clause-1-ov-file#readme\npypdfium2: >=4.30.0\n# MIT https://github.com/voyage-ai/voyageai-python?tab=MIT-1-ov-file#readme\nvoyageai: >=0.2.3\n# OpenLDAP Public License, which is a permissive BSD style license\n# https://github.com/jnwatson/py-lmdb/?tab=License-1-ov-file#readme\nlmdb: >=1.5.1\n"
        },
        {
          "name": "pyproject.toml",
          "type": "blob",
          "size": 1.48046875,
          "content": "[tool.black]\nline-length = 100\n\n[tool.pyright]\npythonPlatform = \"Linux\"\npythonVersion = \"3.9\"\nreportUnnecessaryCast = true\nreportUnnecessaryTypeIgnoreComment = true\nstubPath = \"./typings\"\ntypeCheckingMode = \"strict\"\nverboseOutput = true\n\n[tool.ruff]\nline-length = 100\ntarget-version = \"py39\"\n\n[tool.ruff.lint]\nignore = [\n    \"COM812\",   # -- over aggressively insists on trailing commas where not desireable --\n    \"PT001\",    # -- wants empty parens on @pytest.fixture where not used (essentially always) --\n    \"PT011\",    # -- pytest.raises({exc}) too broad, use match param or more specific exception --\n    \"PT012\",    # -- pytest.raises() block should contain a single simple statement --\n    \"SIM117\",   # -- merge `with` statements for context managers that have same scope --\n]\nselect = [\n    \"C4\",       # -- flake8-comprehensions --\n    \"COM\",      # -- flake8-commas --\n    \"E\",        # -- pycodestyle errors --\n    \"F\",        # -- pyflakes --\n    \"I\",        # -- isort (imports) --\n    \"PLR0402\",  # -- Name compared with itself like `foo == foo` --\n    \"PT\",       # -- flake8-pytest-style --\n    \"SIM\",      # -- flake8-simplify --\n    \"UP015\",    # -- redundant `open()` mode parameter (like \"r\" is default) --\n    \"UP018\",    # -- Unnecessary {literal_type} call like `str(\"abc\")`. (rewrite as a literal) --\n    \"UP032\",    # -- Use f-string instead of `.format()` call --\n    \"UP034\",    # -- Avoid extraneous parentheses --\n    \"W\",        # -- Warnings, including invalid escape-sequence --\n]\n"
        },
        {
          "name": "requirements",
          "type": "tree",
          "content": null
        },
        {
          "name": "scripts",
          "type": "tree",
          "content": null
        },
        {
          "name": "setup.cfg",
          "type": "blob",
          "size": 0.513671875,
          "content": "[metadata]\nlicense_files = LICENSE.md\n\n[flake8]\nignore = E203,E704,W503\nmax-line-length = 100\nexclude =\n    .venv\n    unstructured-inference\nper-file-ignores =\n    *: T20\n\n[tool:pytest]\nfilterwarnings =\n    ignore::DeprecationWarning\npython_classes = Test Describe\npython_functions = test_ it_ they_ but_ and_\ntestpaths =\n    test_unstructured\n    test_unstructured_ingest\n\n[autoflake]\nexpand_star_imports=true\nignore_pass_statements=false\nrecursive=true\nquiet=true\nremove_all_unused_imports=true\nremove_unused_variables=true\n"
        },
        {
          "name": "setup.py",
          "type": "blob",
          "size": 4.48828125,
          "content": "\"\"\"\nsetup.py\n\nunstructured - pre-processing tools for unstructured data\n\nCopyright 2022 Unstructured Technologies, Inc.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n\"\"\"\n\nfrom typing import List, Optional, Union\n\nfrom setuptools import find_packages, setup\n\nfrom unstructured.__version__ import __version__\n\n\ndef load_requirements(file_list: Optional[Union[str, List[str]]] = None) -> List[str]:\n    if file_list is None:\n        file_list = [\"requirements/base.in\"]\n    if isinstance(file_list, str):\n        file_list = [file_list]\n    requirements: List[str] = []\n    for file in file_list:\n        with open(file, encoding=\"utf-8\") as f:\n            requirements.extend(f.readlines())\n    requirements = [\n        req for req in requirements if not req.startswith(\"#\") and not req.startswith(\"-\")\n    ]\n    return requirements\n\n\ncsv_reqs = load_requirements(\"requirements/extra-csv.in\")\ndoc_reqs = load_requirements(\"requirements/extra-docx.in\")\ndocx_reqs = load_requirements(\"requirements/extra-docx.in\")\nepub_reqs = load_requirements(\"requirements/extra-epub.in\")\nimage_reqs = load_requirements(\"requirements/extra-pdf-image.in\")\nmarkdown_reqs = load_requirements(\"requirements/extra-markdown.in\")\nodt_reqs = load_requirements(\"requirements/extra-odt.in\")\norg_reqs = load_requirements(\"requirements/extra-pandoc.in\")\npdf_reqs = load_requirements(\"requirements/extra-pdf-image.in\")\nppt_reqs = load_requirements(\"requirements/extra-pptx.in\")\npptx_reqs = load_requirements(\"requirements/extra-pptx.in\")\nrtf_reqs = load_requirements(\"requirements/extra-pandoc.in\")\nrst_reqs = load_requirements(\"requirements/extra-pandoc.in\")\ntsv_reqs = load_requirements(\"requirements/extra-csv.in\")\nxlsx_reqs = load_requirements(\"requirements/extra-xlsx.in\")\n\nall_doc_reqs = list(\n    set(\n        csv_reqs\n        + docx_reqs\n        + epub_reqs\n        + image_reqs\n        + markdown_reqs\n        + odt_reqs\n        + org_reqs\n        + pdf_reqs\n        + pptx_reqs\n        + rtf_reqs\n        + rst_reqs\n        + tsv_reqs\n        + xlsx_reqs,\n    ),\n)\n\n\nsetup(\n    name=\"unstructured\",\n    description=\"A library that prepares raw documents for downstream ML tasks.\",\n    long_description=open(\"README.md\", encoding=\"utf-8\").read(),  # noqa: SIM115\n    long_description_content_type=\"text/markdown\",\n    keywords=\"NLP PDF HTML CV XML parsing preprocessing\",\n    url=\"https://github.com/Unstructured-IO/unstructured\",\n    python_requires=\">=3.9.0,<3.13\",\n    classifiers=[\n        \"Development Status :: 4 - Beta\",\n        \"Intended Audience :: Developers\",\n        \"Intended Audience :: Education\",\n        \"Intended Audience :: Science/Research\",\n        \"License :: OSI Approved :: Apache Software License\",\n        \"Operating System :: OS Independent\",\n        \"Programming Language :: Python :: 3\",\n        \"Programming Language :: Python :: 3.9\",\n        \"Programming Language :: Python :: 3.10\",\n        \"Programming Language :: Python :: 3.11\",\n        \"Programming Language :: Python :: 3.12\",\n        \"Topic :: Scientific/Engineering :: Artificial Intelligence\",\n    ],\n    author=\"Unstructured Technologies\",\n    author_email=\"devops@unstructuredai.io\",\n    license=\"Apache-2.0\",\n    packages=find_packages(),\n    version=__version__,\n    install_requires=load_requirements(),\n    extras_require={\n        # Document specific extra requirements\n        \"all-docs\": all_doc_reqs,\n        \"csv\": csv_reqs,\n        \"doc\": doc_reqs,\n        \"docx\": docx_reqs,\n        \"epub\": epub_reqs,\n        \"image\": image_reqs,\n        \"md\": markdown_reqs,\n        \"odt\": odt_reqs,\n        \"org\": org_reqs,\n        \"pdf\": pdf_reqs,\n        \"ppt\": ppt_reqs,\n        \"pptx\": pptx_reqs,\n        \"rtf\": rtf_reqs,\n        \"rst\": rst_reqs,\n        \"tsv\": tsv_reqs,\n        \"xlsx\": xlsx_reqs,\n        # Legacy extra requirements\n        \"huggingface\": load_requirements(\"requirements/huggingface.in\"),\n        \"local-inference\": all_doc_reqs,\n        \"paddleocr\": load_requirements(\"requirements/extra-paddleocr.in\"),\n    },\n    package_dir={\"unstructured\": \"unstructured\"},\n    package_data={\"unstructured\": [\"nlp/*.txt\", \"py.typed\"]},\n)\n"
        },
        {
          "name": "test_unstructured",
          "type": "tree",
          "content": null
        },
        {
          "name": "test_unstructured_ingest",
          "type": "tree",
          "content": null
        },
        {
          "name": "typings",
          "type": "tree",
          "content": null
        },
        {
          "name": "unstructured",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}