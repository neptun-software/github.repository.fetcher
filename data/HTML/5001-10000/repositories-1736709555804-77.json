{
  "metadata": {
    "timestamp": 1736709555804,
    "page": 77,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjgw",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "MrNeRF/awesome-3D-gaussian-splatting",
      "stars": 6595,
      "defaultBranch": "main",
      "files": [
        {
          "name": ".gitattributes",
          "type": "blob",
          "size": 0.0419921875,
          "content": "assets filter=lfs diff=lfs merge=lfs -text\n"
        },
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.0244140625,
          "content": ".venv\n__pycache__/\n*.pyc\n"
        },
        {
          "name": "CONTRIBUTING.md",
          "type": "blob",
          "size": 1.7333984375,
          "content": "# Contributing Guide\n\nThank you for your interest in contributing to the Awesome 3D Gaussian Splatting repository! This document will guide you through the contribution process.\n\n## Adding Papers\n\nWe use a custom YAML editor to maintain the paper database. To add or edit papers:\n\n1. Clone the repository:\n```bash\ngit clone https://github.com/MrNeRF/awesome-3D-gaussian-splatting.git\ncd awesome-3D-gaussian-splatting\n```\n\n2. Install dependencies:\n```bash\npip install -r requirements.txt\n```\n\n3. Install Poppler (required for PDF processing):\n   - **Ubuntu/Debian:**\n     ```bash\n     sudo apt-get install poppler-utils\n     ```\n   - **macOS:**\n     ```bash\n     brew install poppler\n     ```\n   - **Windows:**\n     - Download and install from: https://github.com/oschwartz10612/poppler-windows/releases/\n     - Add the `bin` directory to your system PATH\n\n4. Run the YAML editor:\n```bash\npython src/yaml_editor.py\n```\n\n5. Use the editor to:\n   - Add new papers using the \"Add from arXiv\" button\n   - Edit existing entries\n   - Add tags, links, and other metadata\n   - Preview thumbnails\n\n6. The editor will automatically save changes to `awesome_3dgs_papers.yaml`\n\n## Adding Other Resources\n\nFor adding other resources (implementations, tools, tutorials, etc.):\n\n1. Fork the repository\n2. Create a new branch (`git checkout -b feature/new-resource`)\n3. Edit the README.md file\n4. Commit your changes (`git commit -m 'Add new resource'`)\n5. Push to your fork (`git push origin feature/new-resource`)\n6. Open a Pull Request\n\nPlease ensure your additions:\n- Are related to 3D Gaussian Splatting\n- Have working links\n- Are placed in the appropriate section\n- Follow the existing formatting\n\n---\n\nBy contributing to this repository, you agree to abide by its terms and conditions."
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 1.0390625,
          "content": "MIT License\n\nCopyright (c) 2023 janusch\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 4.6015625,
          "content": "# Awesome 3D Gaussian Splatting\n\n<div align=\"center\">\n  A curated collection of resources focused on 3D Gaussian Splatting (3DGS) and related technologies.\n  \n  [**Browse the Paper List**](https://mrnerf.github.io/awesome-3D-gaussian-splatting/) | [**Contribute**](CONTRIBUTING.md)\n</div>\n\n## Contents\n\n- [Papers & Documentation](#papers--documentation)\n- [Implementations](#implementations)\n- [Viewers & Game Engine Support](#viewers--game-engine-support)\n- [Tools & Utilities](#tools--utilities)\n- [Learning Resources](#learning-resources)\n- [Sponsors](#sponsors)\n\n## Papers & Documentation\n\n### Papers Database\nVisit our comprehensive, searchable database of 3D Gaussian Splatting papers:  \n[Papers Database](https://mrnerf.github.io/awesome-3D-gaussian-splatting/)\n\n### Courses\n- [MIT Inverse Rendering Lectures (Module 2)](https://www.scenerepresentations.org/courses/inverse-graphics-23/) - Academic deep dive into inverse rendering\n\n### Datasets\n- [NERDS 360 Multi-View dataset](https://zubair-irshad.github.io/projects/neo360.html) - High-quality outdoor scene dataset\n\n## Implementations\n\n### Official Reference\n- [Original Gaussian Splatting](https://github.com/graphdeco-inria/gaussian-splatting) - The reference implementation by the original authors\n\n### Community Implementations\n| Implementation | Language | License | Description |\n|----------------|----------|----------|-------------|\n| [Taichi 3D GS](https://github.com/wanmeihuali/taichi_3d_gaussian_splatting) | Taichi | Apache-2.0 | Taichi-based implementation |\n| [Nerfstudio gsplat](https://github.com/nerfstudio-project/gsplat) | Python/CUDA | Apache-2.0 | Integration with Nerfstudio |\n| [fast](https://github.com/MrNeRF/gaussian-splatting-cuda) | C++/CUDA | Inria/MPII | High-performance implementation |\n| [OpenSplat](https://github.com/pierotofy/OpenSplat) | C++/CPU/GPU | AGPL-3.0 | Cross-platform solution |\n| [Grendel](https://github.com/nyu-systems/Grendel-GS) | Python/CUDA | Apache-2.0 | Distributed computing focus |\n\n### Frameworks\n- [Pointrix](https://github.com/pointrix-project/pointrix) - Differentiable point-based rendering\n- [GauStudio](https://github.com/GAP-LAB-CUHK-SZ/gaustudio) - Unified framework with multiple implementations\n- [DriveStudio](https://github.com/ziyc/drivestudio) - Urban scene reconstruction framework\n\n## Viewers & Game Engine Support\n\n### Game Engines\n- [Unity Plugin](https://github.com/aras-p/UnityGaussianSplatting)\n- [Unreal Plugin](https://github.com/xverse-engine/XV3DGS-UEPlugin)\n- [PlayCanvas Integration](https://github.com/playcanvas/engine/tree/main/src/scene/gsplat)\n\n### Web Viewers\n**WebGL**\n- [Splat Viewer](https://github.com/antimatter15/splat)\n- [Gauzilla](https://github.com/BladeTransformerLLC/gauzilla)\n- [Interactive Viewer](https://github.com/kishimisu/Gaussian-Splatting-WebGL)\n\n**WebGPU**\n- [EPFL Viewer](https://github.com/cvlab-epfl/gaussian-splatting-web)\n- [WebGPU Splat](https://github.com/KeKsBoTer/web-splat)\n\n### Native Applications\n- [Blender Add-on](https://github.com/ReshotAI/gaussian-splatting-blender-addon)\n- [iOS Metal Viewer](https://github.com/laanlabs/metal-splats)\n- [OpenGL Viewer](https://github.com/limacv/GaussianSplattingViewer)\n- [VR Support (OpenXR)](https://github.com/hyperlogic/splatapult)\n\n## Tools & Utilities\n\n### Data Processing\n- [Kapture](https://github.com/naver/kapture) - Unified data format for visual localization\n- [3DGS Converter](https://github.com/francescofugazzi/3dgsconverter) - Format conversion tool\n- [SuperSplat](https://github.com/playcanvas/super-splat) - Browser-based cleanup tool\n- [Point Cloud Editor](https://github.com/JohannesKrueger/pointcloudeditor) - Web-based point cloud editing\n\n### Development Tools\n- [GSOPs for Houdini](https://github.com/david-rhodes/GSOPs) - Houdini integration tools\n- [camorph](https://github.com/Fraunhofer-IIS/camorph) - Camera parameter conversion\n\n## Learning Resources\n\n### Blog Posts\n- [3DGS Introduction](https://huggingface.co/blog/gaussian-splatting) - HuggingFace guide\n- [Implementation Details](https://github.com/kwea123/gaussian_splatting_notes) - Technical deep dive\n- [Mathematical Foundation](https://github.com/chiehwangs/3d-gaussian-theory) - Theory explanation\n- [Capture Guide](https://medium.com/@heyulei/capture-images-for-gaussian-splatting-81d081bbc826) - Image capture tutorial\n\n### Video Tutorials\n- [Getting Started (Windows)](https://youtu.be/UXtuigy_wYc)\n- [Unity Integration Guide](https://youtu.be/5_GaPYBHqOo)\n- [Two-Minute Explanation](https://youtu.be/HVv_IQKlafQ)\n- [Jupyter Tutorial](https://www.youtube.com/watch?v=OcvA7fmiZYM)\n\n\n## Sponsors\n\nA big thank you to our sponsors for their generous support:\n\n- [Yehe Liu](https://x.com/YeheLiu)"
        },
        {
          "name": "assets",
          "type": "tree",
          "content": null
        },
        {
          "name": "awesome_3dgs_papers.yaml",
          "type": "blob",
          "size": 718.5751953125,
          "content": "- id: meng2025zero1tog\n  title: 'Zero-1-to-G: Taming Pretrained 2D Diffusion Model for Direct 3D Generation'\n  authors: Xuyi Meng, Chen Wang, Jiahui Lei, Kostas Daniilidis, Jiatao Gu, Lingjie\n    Liu\n  year: '2025'\n  abstract: 'Recent advances in 2D image generation have achieved remarkable quality,largely\n    driven by the capacity of diffusion models and the availability of large-scale\n    datasets. However, direct 3D generation is still constrained by the scarcity and\n    lower fidelity of 3D datasets. In this paper, we introduce Zero-1-to-G, a novel\n    approach that addresses this problem by enabling direct single-view generation\n    on Gaussian splats using pretrained 2D diffusion models. Our key insight is that\n    Gaussian splats, a 3D representation, can be decomposed into multi-view images\n    encoding different attributes. This reframes the challenging task of direct 3D\n    generation within a 2D diffusion framework, allowing us to leverage the rich priors\n    of pretrained 2D diffusion models. To incorporate 3D awareness, we introduce cross-view\n    and cross-attribute attention layers, which capture complex correlations and enforce\n    3D consistency across generated splats. This makes Zero-1-to-G the first direct\n    image-to-3D generative model to effectively utilize pretrained 2D diffusion priors,\n    enabling efficient training and improved generalization to unseen objects. Extensive\n    experiments on both synthetic and in-the-wild datasets demonstrate superior performance\n    in 3D object generation, offering a new approach to high-quality 3D generation.\n\n    '\n  project_page: https://mengxuyigit.github.io/projects/zero-1-to-G/\n  paper: https://arxiv.org/pdf/2501.05427.pdf\n  code: null\n  video: null\n  tags:\n  - Diffusion\n  - Project\n  thumbnail: assets/thumbnails/meng2025zero1tog.jpg\n  publication_date: '2025-01-09T18:37:35+00:00'\n  date_source: arxiv\n- id: gerogiannis2025arc2avatar\n  title: 'Arc2Avatar: Generating Expressive 3D Avatars from a Single Image via ID\n    Guidance'\n  authors: Dimitrios Gerogiannis, Foivos Paraperas Papantoniou, Rolandos Alexandros\n    Potamias, Alexandros Lattas, Stefanos Zafeiriou\n  year: '2025'\n  abstract: 'Inspired by the effectiveness of 3D Gaussian Splatting (3DGS) in reconstructing\n    detailed 3D scenes within multi-view setups and the emergence of large 2D human\n    foundation models, we introduce Arc2Avatar, the first SDS-based method utilizing\n    a human face foundation model as guidance with just a single image as input. To\n    achieve that, we extend such a model for diverse-view human head generation by\n    fine-tuning on synthetic data and modifying its conditioning. Our avatars maintain\n    a dense correspondence with a human face mesh template, allowing blendshape-based\n    expression generation. This is achieved through a modified 3DGS approach, connectivity\n    regularizers, and a strategic initialization tailored for our task. Additionally,\n    we propose an optional efficient SDS-based correction step to refine the blendshape\n    expressions, enhancing realism and diversity. Experiments demonstrate that Arc2Avatar\n    achieves state-of-the-art realism and identity preservation, effectively addressing\n    color issues by allowing the use of very low guidance, enabled by our strong identity\n    prior and initialization strategy, without compromising detail.\n\n    '\n  project_page: null\n  paper: https://arxiv.org/pdf/2501.05379.pdf\n  code: null\n  video: null\n  tags:\n  - Avatar\n  - Diffusion\n  thumbnail: assets/thumbnails/gerogiannis2025arc2avatar.jpg\n  publication_date: '2025-01-09T17:04:33+00:00'\n  date_source: arxiv\n- id: tianci2025scaffoldslam\n  title: 'Scaffold-SLAM: Structured 3D Gaussians for Simultaneous Localization and\n    Photorealistic Mapping'\n  authors: Wen Tianci, Liu Zhiang, Lu Biao, Fang Yongchun\n  year: '2025'\n  abstract: '3D Gaussian Splatting (3DGS) has recently revolutionized novel view synthesis\n    in the Simultaneous Localization and Mapping (SLAM). However, existing SLAM methods\n    utilizing 3DGS have failed to provide high-quality novel view rendering for monocular,\n    stereo, and RGB-D cameras simultaneously. Notably, some methods perform well for\n    RGB-D cameras but suffer significant degradation in rendering quality for monocular\n    cameras. In this paper, we present Scaffold-SLAM, which delivers simultaneous\n    localization and high-quality photorealistic mapping across monocular, stereo,\n    and RGB-D cameras. We introduce two key innovations to achieve this state-of-the-art\n    visual quality. First, we propose Appearance-from-Motion embedding, enabling 3D\n    Gaussians to better model image appearance variations across different camera\n    poses. Second, we introduce a frequency regularization pyramid to guide the distribution\n    of Gaussians, allowing the model to effectively capture finer details in the scene.\n    Extensive experiments on monocular, stereo, and RGB-D datasets demonstrate that\n    Scaffold-SLAM significantly outperforms state-of-the-art methods in photorealistic\n    mapping quality, e.g., PSNR is 16.76% higher in the TUM RGB-D datasets for monocular\n    cameras.\n\n    '\n  project_page: null\n  paper: https://arxiv.org/pdf/2501.05242.pdf\n  code: null\n  video: null\n  tags:\n  - SLAM\n  thumbnail: assets/thumbnails/tianci2025scaffoldslam.jpg\n  publication_date: '2025-01-09T13:50:26+00:00'\n  date_source: arxiv\n- id: bond2025gaussianvideo\n  title: 'GaussianVideo: Efficient Video Representation via Hierarchical Gaussian\n    Splatting'\n  authors: Andrew Bond, Jui-Hsien Wang, Long Mai, Erkut Erdem, Aykut Erdem\n  year: '2025'\n  abstract: 'Efficient neural representations for dynamic video scenes are critical\n    for applications ranging from video compression to interactive simulations. Yet,\n    existing methods often face challenges related to high memory usage, lengthy training\n    times, and temporal consistency. To address these issues, we introduce a novel\n    neural video representation that combines 3D Gaussian splatting with continuous\n    camera motion modeling. By leveraging Neural ODEs, our approach learns smooth\n    camera trajectories while maintaining an explicit 3D scene representation through\n    Gaussians. Additionally, we introduce a spatiotemporal hierarchical learning strategy,\n    progressively refining spatial and temporal features to enhance reconstruction\n    quality and accelerate convergence. This memory-efficient approach achieves high-quality\n    rendering at impressive speeds. Experimental results show that our hierarchical\n    learning, combined with robust camera motion modeling, captures complex dynamic\n    scenes with strong temporal consistency, achieving state-of-the-art performance\n    across diverse video datasets in both high- and low-motion scenarios.\n\n    '\n  project_page: https://cyberiada.github.io/GaussianVideo/\n  paper: https://arxiv.org/pdf/2501.04782.pdf\n  code: null\n  video: null\n  tags:\n  - Project\n  - Video\n  thumbnail: assets/thumbnails/bond2025gaussianvideo.jpg\n  publication_date: '2025-01-08T19:01:12+00:00'\n  date_source: arxiv\n- id: kwak2025modecgs\n  title: 'MoDec-GS: Global-to-Local Motion Decomposition and Temporal Interval Adjustment\n    for Compact Dynamic 3D Gaussian Splatting'\n  authors: Sangwoon Kwak, Joonsoo Kim, Jun Young Jeong, Won-Sik Cheong, Jihyong Oh,\n    Munchurl Kim\n  year: '2025'\n  abstract: '3D Gaussian Splatting (3DGS) has made significant strides in scene representation\n    and neural rendering, with intense efforts focused on adapting it for dynamic\n    scenes. Despite delivering remarkable rendering quality and speed, existing methods\n    struggle with storage demands and representing complex real-world motions. To\n    tackle these issues, we propose MoDecGS, a memory-efficient Gaussian splatting\n    framework designed for reconstructing novel views in challenging scenarios with\n    complex motions. We introduce GlobaltoLocal Motion Decomposition (GLMD) to effectively\n    capture dynamic motions in a coarsetofine manner. This approach leverages Global\n    Canonical Scaffolds (Global CS) and Local Canonical Scaffolds (Local CS), extending\n    static Scaffold representation to dynamic video reconstruction. For Global CS,\n    we propose Global Anchor Deformation (GAD) to efficiently represent global dynamics\n    along complex motions, by directly deforming the implicit Scaffold attributes\n    which are anchor position, offset, and local context features. Next, we finely\n    adjust local motions via the Local Gaussian Deformation (LGD) of Local CS explicitly.\n    Additionally, we introduce Temporal Interval Adjustment (TIA) to automatically\n    control the temporal coverage of each Local CS during training, allowing MoDecGS\n    to find optimal interval assignments based on the specified number of temporal\n    segments. Extensive evaluations demonstrate that MoDecGS achieves an average 70%\n    reduction in model size over stateoftheart methods for dynamic 3D Gaussians from\n    realworld dynamic videos while maintaining or even improving rendering quality.\n\n    '\n  project_page: 'MoDec-GS: Global-to-Local Motion Decomposition and Temporal Interval\n    Adjustment for Compact Dynamic 3D Gaussian Splatting'\n  paper: https://arxiv.org/pdf/2501.03714.pdf\n  code: null\n  video: https://youtu.be/5L6gzc5-cw8?si=L6v6XLZFQrYK50iV\n  tags:\n  - Compression\n  - Dynamic\n  - Project\n  - Video\n  thumbnail: assets/thumbnails/kwak2025modecgs.jpg\n  publication_date: '2025-01-07T11:43:13+00:00'\n  date_source: arxiv\n- id: yu2025dehazegs\n  title: 'DehazeGS: Seeing Through Fog with 3D Gaussian Splatting'\n  authors: Jinze Yu, Yiqun Wang, Zhengda Lu, Jianwei Guo, Yong Li, Hongxing Qin, Xiaopeng\n    Zhang\n  year: '2025'\n  abstract: 'Current novel view synthesis tasks primarily rely on high-quality and\n    clear images. However, in foggy scenes, scattering and attenuation can significantly\n    degrade the reconstruction and rendering quality. Although NeRF-based dehazing\n    reconstruction algorithms have been developed, their use of deep fully connected\n    neural networks and per-ray sampling strategies leads to high computational costs.\n    Moreover, NeRF''s implicit representation struggles to recover fine details from\n    hazy scenes. In contrast, recent advancements in 3D Gaussian Splatting achieve\n    high-quality 3D scene reconstruction by explicitly modeling point clouds into\n    3D Gaussians. In this paper, we propose leveraging the explicit Gaussian representation\n    to explain the foggy image formation process through a physically accurate forward\n    rendering process. We introduce DehazeGS, a method capable of decomposing and\n    rendering a fog-free background from participating media using only muti-view\n    foggy images as input. We model the transmission within each Gaussian distribution\n    to simulate the formation of fog. During this process, we jointly learn the atmospheric\n    light and scattering coefficient while optimizing the Gaussian representation\n    of the hazy scene. In the inference stage, we eliminate the effects of scattering\n    and attenuation on the Gaussians and directly project them onto a 2D plane to\n    obtain a clear view. Experiments on both synthetic and real-world foggy datasets\n    demonstrate that DehazeGS achieves state-of-the-art performance in terms of both\n    rendering quality and computational efficiency.\n\n    '\n  project_page: null\n  paper: https://arxiv.org/pdf/2501.03659.pdf\n  code: null\n  video: null\n  tags:\n  - In the Wild\n  - Rendering\n  thumbnail: assets/thumbnails/yu2025dehazegs.jpg\n  publication_date: '2025-01-07T09:47:46+00:00'\n  date_source: arxiv\n- id: lee2025compression\n  title: Compression of 3D Gaussian Splatting with Optimized Feature Planes and Standard\n    Video Codecs\n  authors: Soonbin Lee, Fangwen Shu, Yago Sanchez, Thomas Schierl, Cornelius Hellge\n  year: '2025'\n  abstract: '3D Gaussian Splatting is a recognized method for 3D scene representation,\n    known for its high rendering quality and speed. However, its substantial data\n    requirements present challenges for practical applications. In this paper, we\n    introduce an efficient compression technique that significantly reduces storage\n    overhead by using compact representation. We propose a unified architecture that\n    combines point cloud data and feature planes through a progressive tri-plane structure.\n    Our method utilizes 2D feature planes, enabling continuous spatial representation.\n    To further optimize these representations, we incorporate entropy modeling in\n    the frequency domain, specifically designed for standard video codecs. We also\n    propose channel-wise bit allocation to achieve a better trade-off between bitrate\n    consumption and feature plane representation. Consequently, our model effectively\n    leverages spatial correlations within the feature planes to enhance rate-distortion\n    performance using standard, non-differentiable video codecs. Experimental results\n    demonstrate that our method outperforms existing methods in data compactness while\n    maintaining high rendering quality. Our project page is available at https://fraunhoferhhi.github.io/CodecGS\n\n    '\n  project_page: null\n  paper: https://arxiv.org/pdf/2501.03399.pdf\n  code: null\n  video: null\n  tags:\n  - Compression\n  thumbnail: assets/thumbnails/lee2025compression.jpg\n  publication_date: '2025-01-06T21:37:30+00:00'\n  date_source: arxiv\n- id: rajasegaran2025gaussian\n  title: Gaussian Masked Autoencoders\n  authors: Jathushan Rajasegaran, Xinlei Chen, Rulilong Li, Christoph Feichtenhofer,\n    Jitendra Malik, Shiry Ginosar\n  year: '2025'\n  abstract: 'This paper explores Masked Autoencoders (MAE) with Gaussian Splatting.\n    While reconstructive self-supervised learning frameworks such as MAE learns good\n    semantic abstractions, it is not trained for explicit spatial awareness. Our approach,\n    named Gaussian Masked Autoencoder, or GMAE, aims to learn semantic abstractions\n    and spatial understanding jointly. Like MAE, it reconstructs the image end-to-end\n    in the pixel space, but beyond MAE, it also introduces an intermediate, 3D Gaussian-based\n    representation and renders images via splatting. We show that GMAE can enable\n    various zero-shot learning capabilities of spatial understanding (e.g., figure-ground\n    segmentation, image layering, edge detection, etc.) while preserving the high-level\n    semantics of self-supervised representation quality from MAE. To our knowledge,\n    we are the first to employ Gaussian primitives in an image representation learning\n    framework beyond optimization-based single-scene reconstructions. We believe GMAE\n    will inspire further research in this direction and contribute to developing next-generation\n    techniques for modeling high-fidelity visual data. More details at https://brjathu.github.io/gmae\n\n    '\n  project_page: null\n  paper: https://arxiv.org/pdf/2501.03229.pdf\n  code: null\n  video: null\n  tags:\n  - Transformer\n  thumbnail: assets/thumbnails/rajasegaran2025gaussian.jpg\n  publication_date: '2025-01-06T18:59:57+00:00'\n  date_source: arxiv\n- id: nguyen2025pointmapconditioned\n  title: Pointmap-Conditioned Diffusion for Consistent Novel View Synthesis\n  authors: Thang-Anh-Quan Nguyen, Nathan Piasco, Luis Roldão, Moussab Bennehar, Dzmitry\n    Tsishkou, Laurent Caraffa, Jean-Philippe Tarel, Roland Brémond\n  year: '2025'\n  abstract: 'In this paper, we present PointmapDiffusion, a novel framework for single-image\n    novel view synthesis (NVS) that utilizes pre-trained 2D diffusion models. Our\n    method is the first to leverage pointmaps (i.e. rasterized 3D scene coordinates)\n    as a conditioning signal, capturing geometric prior from the reference images\n    to guide the diffusion process. By embedding reference attention blocks and a\n    ControlNet for pointmap features, our model balances between generative capability\n    and geometric consistency, enabling accurate view synthesis across varying viewpoints.\n    Extensive experiments on diverse real-world datasets demonstrate that PointmapDiffusion\n    achieves high-quality, multi-view consistent results with significantly fewer\n    trainable parameters compared to other baselines for single-image NVS tasks.\n\n    '\n  project_page: null\n  paper: https://arxiv.org/pdf/2501.02913.pdf\n  code: null\n  video: null\n  tags:\n  - Diffusion\n  thumbnail: assets/thumbnails/nguyen2025pointmapconditioned.jpg\n  publication_date: '2025-01-06T10:48:31+00:00'\n  date_source: arxiv\n- id: bian2025gsdit\n  title: 'GS-DiT: Advancing Video Generation with Pseudo 4D Gaussian Fields through\n    Efficient Dense 3D Point Tracking'\n  authors: Weikang Bian, Zhaoyang Huang, Xiaoyu Shi, Yijin Li, Fu-Yun Wang, Hongsheng\n    Li\n  year: '2025'\n  abstract: '4D video control is essential in video generation as it enables the use\n    of sophisticated lens techniques, such as multi-camera shooting and dolly zoom,\n    which are currently unsupported by existing methods. Training a video Diffusion\n    Transformer (DiT) directly to control 4D content requires expensive multi-view\n    videos. Inspired by Monocular Dynamic novel View Synthesis (MDVS) that optimizes\n    a 4D representation and renders videos according to different 4D elements, such\n    as camera pose and object motion editing, we bring pseudo 4D Gaussian fields to\n    video generation. Specifically, we propose a novel framework that constructs a\n    pseudo 4D Gaussian field with dense 3D point tracking and renders the Gaussian\n    field for all video frames. Then we finetune a pretrained DiT to generate videos\n    following the guidance of the rendered video, dubbed as GS-DiT. To boost the training\n    of the GS-DiT, we also propose an efficient Dense 3D Point Tracking (D3D-PT) method\n    for the pseudo 4D Gaussian field construction. Our D3D-PT outperforms SpatialTracker,\n    the state-of-the-art sparse 3D point tracking method, in accuracy and accelerates\n    the inference speed by two orders of magnitude. During the inference stage, GS-DiT\n    can generate videos with the same dynamic content while adhering to different\n    camera parameters, addressing a significant limitation of current video generation\n    models. GS-DiT demonstrates strong generalization capabilities and extends the\n    4D controllability of Gaussian splatting to video generation beyond just camera\n    poses. It supports advanced cinematic effects through the manipulation of the\n    Gaussian field and camera intrinsics, making it a powerful tool for creative video\n    production. Demos are available at https://wkbian.github.io/Projects/GS-DiT/.\n\n    '\n  project_page: null\n  paper: https://arxiv.org/pdf/2501.02690.pdf\n  code: null\n  video: null\n  tags:\n  - Year 2025\n  thumbnail: assets/thumbnails/bian2025gsdit.jpg\n  publication_date: '2025-01-05T23:55:33+00:00'\n  date_source: arxiv\n- id: cong2025videolifter\n  title: 'VideoLifter: Lifting Videos to 3D with Fast Hierarchical Stereo Alignment'\n  authors: Wenyan Cong, Kevin Wang, Jiahui Lei, Colton Stearns, Yuanhao Cai, Dilin\n    Wang, Rakesh Ranjan, Matt Feiszli, Leonidas Guibas, Zhangyang Wang, Weiyao Wang,\n    Zhiwen Fan\n  year: '2025'\n  abstract: 'Efficiently reconstructing accurate 3D models from monocular video is\n    a key challenge in computer vision, critical for advancing applications in virtual\n    reality, robotics, and scene understanding. Existing approaches typically require\n    pre-computed camera parameters and frame-by-frame reconstruction pipelines, which\n    are prone to error accumulation and entail significant computational overhead.\n    To address these limitations, we introduce VideoLifter, a novel framework that\n    leverages geometric priors from a learnable model to incrementally optimize a\n    globally sparse to dense 3D representation directly from video sequences. VideoLifter\n    segments the video sequence into local windows, where it matches and registers\n    frames, constructs consistent fragments, and aligns them hierarchically to produce\n    a unified 3D model. By tracking and propagating sparse point correspondences across\n    frames and fragments, VideoLifter incrementally refines camera poses and 3D structure,\n    minimizing reprojection error for improved accuracy and robustness. This approach\n    significantly accelerates the reconstruction process, reducing training time by\n    over 82% while surpassing current state-of-the-art methods in visual fidelity\n    and computational efficiency.\n\n    '\n  project_page: https://videolifter.github.io/\n  paper: https://arxiv.org/pdf/2501.01949.pdf\n  code: null\n  video: null\n  tags:\n  - Acceleration\n  - Diffusion\n  - Project\n  thumbnail: assets/thumbnails/cong2025videolifter.jpg\n  publication_date: '2025-01-03T18:52:36+00:00'\n  date_source: arxiv\n- id: huang2025enerverse\n  title: 'EnerVerse: Envisioning Embodied Future Space for Robotics Manipulation'\n  authors: Siyuan Huang, Liliang Chen, Pengfei Zhou, Shengcong Chen, Zhengkai Jiang,\n    Yue Hu, Peng Gao, Hongsheng Li, Maoqing Yao, Guanghui Ren\n  year: '2025'\n  abstract: 'We introduce EnerVerse, a comprehensive framework for embodied future\n    space generation specifically designed for robotic manipulation tasks. EnerVerse\n    seamlessly integrates convolutional and bidirectional attention mechanisms for\n    inner-chunk space modeling, ensuring low-level consistency and continuity. Recognizing\n    the inherent redundancy in video data, we propose a sparse memory context combined\n    with a chunkwise unidirectional generative paradigm to enable the generation of\n    infinitely long sequences. To further augment robotic capabilities, we introduce\n    the Free Anchor View (FAV) space, which provides flexible perspectives to enhance\n    observation and analysis. The FAV space mitigates motion modeling ambiguity, removes\n    physical constraints in confined environments, and significantly improves the\n    robot''s generalization and adaptability across various tasks and settings. To\n    address the prohibitive costs and labor intensity of acquiring multi-camera observations,\n    we present a data engine pipeline that integrates a generative model with 4D Gaussian\n    Splatting (4DGS). This pipeline leverages the generative model''s robust generalization\n    capabilities and the spatial constraints provided by 4DGS, enabling an iterative\n    enhancement of data quality and diversity, thus creating a data flywheel effect\n    that effectively narrows the sim-to-real gap. Finally, our experiments demonstrate\n    that the embodied future space generation prior substantially enhances policy\n    predictive capabilities, resulting in improved overall performance, particularly\n    in long-range robotic manipulation tasks.\n\n    '\n  project_page: https://sites.google.com/view/enerverse\n  paper: https://arxiv.org/pdf/2501.01895.pdf\n  code: null\n  video: null\n  tags:\n  - Dynamic\n  - Project\n  - Robotics\n  thumbnail: assets/thumbnails/huang2025enerverse.jpg\n  publication_date: '2025-01-03T17:00:33+00:00'\n- id: longhini2024clothsplatting\n  title: 'Cloth-Splatting: 3D Cloth State Estimation from RGB Supervision'\n  authors: Alberta Longhini, Marcel Büsching, Bardienus Pieter Duisterhof, Jens Lundell,\n    Jeffrey Ichnowski, Mårten Björkman, Danica Kragic\n  year: '2024'\n  abstract: Recently, 3D Gaussian Splatting (3DGS) has revolutionized radiance field\n    reconstruction, manifesting efficient and high-fidelity novel view synthesis.\n    However, accurately We introduce Cloth-Splatting, a method for estimating 3D states\n    of cloth from RGB images through a prediction-update framework. Cloth-Splatting\n    leverages an action-conditioned dynamics model for predicting future states and\n    uses 3D Gaussian Splatting to update the predicted states. Our key insight is\n    that coupling a 3D mesh-based representation with Gaussian Splatting allows us\n    to define a differentiable map between the cloth's state space and the image space.\n    This enables the use of gradient-based optimization techniques to refine inaccurate\n    state estimates using only RGB supervision. Our experiments demonstrate that Cloth-Splatting\n    not only improves state estimation accuracy over current baselines but also reduces\n    convergence time by ~85%.\n  project_page: https://kth-rpl.github.io/cloth-splatting/\n  paper: https://arxiv.org/pdf/2501.01715.pdf\n  code: https://github.com/KTH-RPL/cloth-splatting\n  video: null\n  tags:\n  - Code\n  - Meshing\n  - Project\n  - Rendering\n  thumbnail: assets/thumbnails/longhini2024clothsplatting.jpg\n  publication_date: '2025-01-03T09:17:30+00:00'\n  date_source: arxiv\n- id: zhang2025crossviewgs\n  title: 'CrossView-GS: Cross-view Gaussian Splatting For Large-scale Scene Reconstruction'\n  authors: Chenhao Zhang, Yuanping Cao, Lei Zhang\n  year: '2025'\n  abstract: 3D Gaussian Splatting (3DGS) has emerged as a prominent method for scene\n    representation and reconstruction, leveraging densely distributed Gaussian primitives\n    to enable real-time rendering of high-resolution images. While existing 3DGS methods\n    perform well in scenes with minor view variation, large view changes in cross-view\n    scenes pose optimization challenges for these methods. To address these issues,\n    we propose a novel cross-view Gaussian Splatting method for large-scale scene\n    reconstruction, based on dual-branch fusion. Our method independently reconstructs\n    models from aerial and ground views as two independent branches to establish the\n    baselines of Gaussian distribution, providing reliable priors for cross-view reconstruction\n    during both initialization and densification. Specifically, a gradient-aware regularization\n    strategy is introduced to mitigate smoothing issues caused by significant view\n    disparities. Additionally, a unique Gaussian supplementation strategy is utilized\n    to incorporate complementary information of dual-branch into the cross-view model.\n    Extensive experiments on benchmark datasets demonstrate that our method achieves\n    superior performance in novel view synthesis compared to state-of-the-art methods.\n  project_page: null\n  paper: https://arxiv.org/pdf/2501.01695.pdf\n  code: null\n  video: null\n  tags:\n  - Large-Scale\n  - Optimization\n  thumbnail: assets/thumbnails/zhang2025crossviewgs.jpg\n  publication_date: '2025-01-03T08:24:59+00:00'\n- id: wang2025pgsag\n  title: 'PG-SAG: Parallel Gaussian Splatting for Fine-Grained Large-Scale Urban Buildings\n    Reconstruction via Semantic-Aware Grouping'\n  authors: Tengfei Wang, Xin Wang, Yongmao Hou, Yiwei Xu, Wendi Zhang, Zongqian Zhan\n  year: '2025'\n  abstract: 3D Gaussian Splatting (3DGS) has emerged as a transformative method in\n    the field of real-time novel synthesis. Based on 3DGS, recent advancements cope\n    with large-scale scenes via spatial-based partition strategy to reduce video memory\n    and optimization time costs. In this work, we introduce a parallel Gaussian splatting\n    method, termed PG-SAG, which fully exploits semantic cues for both partitioning\n    and Gaussian kernel optimization, enabling fine-grained building surface reconstruction\n    of large-scale urban areas without downsampling the original image resolution.\n    First, the Cross-modal model - Language Segment Anything is leveraged to segment\n    building masks. Then, the segmented building regions is grouped into sub-regions\n    according to the visibility check across registered images. The Gaussian kernels\n    for these sub-regions are optimized in parallel with masked pixels. In addition,\n    the normal loss is re-formulated for the detected edges of masks to alleviate\n    the ambiguities in normal vectors on edges. Finally, to improve the optimization\n    of 3D Gaussians, we introduce a gradient-constrained balance-load loss that accounts\n    for the complexity of the corresponding scenes, effectively minimizing the thread\n    waiting time in the pixel-parallel rendering stage as well as the reconstruction\n    lost. Extensive experiments are tested on various urban datasets, the results\n    demonstrated the superior performance of our PG-SAG on building surface reconstruction,\n    compared to several state-of-the-art 3DGS-based methods.\n  project_page: null\n  paper: https://arxiv.org/pdf/2501.01677.pdf\n  code: null\n  video: null\n  tags:\n  - Large-Scale\n  - Meshing\n  - Optimization\n  thumbnail: assets/thumbnails/wang2025pgsag.jpg\n  publication_date: '2025-01-03T07:40:16+00:00'\n- id: gao2025easysplat\n  title: 'EasySplat: View-Adaptive Learning makes 3D Gaussian Splatting Easy'\n  authors: Ao Gao, Luosong Guo, Tao Chen, Zhao Wang, Ying Tai, Jian Yang, Zhenyu Zhang\n  year: '2025'\n  abstract: 3D Gaussian Splatting (3DGS) techniques have achieved satisfactory 3D\n    scene representation. Despite their impressive performance, they confront challenges\n    due to the limitation of structure-from-motion (SfM) methods on acquiring accurate\n    scene initialization, or the inefficiency of densification strategy. In this paper,\n    we introduce a novel framework EasySplat to achieve high-quality 3DGS modeling.\n    Instead of using SfM for scene initialization, we employ a novel method to release\n    the power of large-scale pointmap approaches. Specifically, we propose an efficient\n    grouping strategy based on view similarity, and use robust pointmap priors to\n    obtain high-quality point clouds and camera poses for 3D scene initialization.\n    After obtaining a reliable scene structure, we propose a novel densification approach\n    that adaptively splits Gaussian primitives based on the average shape of neighboring\n    Gaussian ellipsoids, utilizing KNN scheme. In this way, the proposed method tackles\n    the limitation on initialization and optimization, leading to an efficient and\n    accurate 3DGS modeling. Extensive experiments demonstrate that EasySplat outperforms\n    the current state-of-the-art (SOTA) in handling novel view synthesis.\n  project_page: null\n  paper: https://arxiv.org/pdf/2501.01003.pdf\n  code: null\n  video: null\n  tags:\n  - 3ster-based\n  - Acceleration\n  - Densification\n  - Rendering\n  thumbnail: assets/thumbnails/gao2025easysplat.jpg\n  publication_date: '2025-01-02T01:56:58+00:00'\n- id: yang2024storm\n  title: 'STORM: Spatio-Temporal Reconstruction Model for Large-Scale Outdoor Scenes'\n  authors: Jiawei Yang, Jiahui Huang, Yuxiao Chen, Yan Wang, Boyi Li, Yurong You,\n    Apoorva Sharma, Maximilian Igl, Peter Karkus, Danfei Xu, Boris Ivanovic, Yue Wang,\n    Marco Pavone\n  year: '2024'\n  abstract: We present STORM, a spatio-temporal reconstruction model designed for\n    reconstructing dynamic outdoor scenes from sparse observations. Existing dynamic\n    reconstruction methods often rely on per-scene optimization, dense observations\n    across space and time, and strong motion supervision, resulting in lengthy optimization\n    times, limited generalization to novel views or scenes, and degenerated quality\n    caused by noisy pseudo-labels for dynamics. To address these challenges, STORM\n    leverages a data-driven Transformer architecture that directly infers dynamic\n    3D scene representations--parameterized by 3D Gaussians and their velocities--in\n    a single forward pass. Our key design is to aggregate 3D Gaussians from all frames\n    using self-supervised scene flows, transforming them to the target timestep to\n    enable complete (i.e., \"amodal\") reconstructions from arbitrary viewpoints at\n    any moment in time. As an emergent property, STORM automatically captures dynamic\n    instances and generates high-quality masks using only reconstruction losses. Extensive\n    experiments on public datasets show that STORM achieves precise dynamic scene\n    reconstruction, surpassing state-of-the-art per-scene optimization methods (+4.3\n    to 6.6 PSNR) and existing feed-forward approaches (+2.1 to 4.7 PSNR) in dynamic\n    regions. STORM reconstructs large-scale outdoor scenes in 200ms, supports real-time\n    rendering, and outperforms competitors in scene flow estimation, improving 3D\n    EPE by 0.422m and Acc5 by 28.02%. Beyond reconstruction, we showcase four additional\n    applications of our model, illustrating the potential of self-supervised learning\n    for broader dynamic scene understanding.\n  project_page: null\n  paper: https://arxiv.org/pdf/2501.00602.pdf\n  code: null\n  video: https://jiawei-yang.github.io/STORM/\n  tags:\n  - Autonomous Driving\n  - Dynamic\n  - Large-Scale\n  - Video\n  thumbnail: assets/thumbnails/yang2024storm.jpg\n  publication_date: '2024-12-31T18:59:58+00:00'\n- id: mao2024dreamdrive\n  title: 'DreamDrive: Generative 4D Scene Modeling from Street View Images'\n  authors: Jiageng Mao, Boyi Li, Boris Ivanovic, Yuxiao Chen, Yan Wang, Yurong You,\n    Chaowei Xiao, Danfei Xu, Marco Pavone, Yue Wang\n  year: '2024'\n  abstract: Synthesizing photo-realistic visual observations from an ego vehicle's\n    driving trajectory is a critical step towards scalable training of self-driving\n    models. Reconstruction-based methods create 3D scenes from driving logs and synthesize\n    geometry-consistent driving videos through neural rendering, but their dependence\n    on costly object annotations limits their ability to generalize to in-the-wild\n    driving scenarios. On the other hand, generative models can synthesize action-conditioned\n    driving videos in a more generalizable way but often struggle with maintaining\n    3D visual consistency. In this paper, we present DreamDrive, a 4D spatial-temporal\n    scene generation approach that combines the merits of generation and reconstruction,\n    to synthesize generalizable 4D driving scenes and dynamic driving videos with\n    3D consistency. Specifically, we leverage the generative power of video diffusion\n    models to synthesize a sequence of visual references and further elevate them\n    to 4D with a novel hybrid Gaussian representation. Given a driving trajectory,\n    we then render 3D-consistent driving videos via Gaussian splatting. The use of\n    generative priors allows our method to produce high-quality 4D scenes from in-the-wild\n    driving data, while neural rendering ensures 3D-consistent video generation from\n    the 4D scenes. Extensive experiments on nuScenes and street view images demonstrate\n    that DreamDrive can generate controllable and generalizable 4D driving scenes,\n    synthesize novel views of driving videos with high fidelity and 3D consistency,\n    decompose static and dynamic elements in a self-supervised manner, and enhance\n    perception and planning tasks for autonomous driving.\n  project_page: null\n  paper: https://arxiv.org/pdf/2501.00601.pdf\n  code: null\n  video: null\n  tags:\n  - Autonomous Driving\n  - Dynamic\n  - Feed-Forward\n  thumbnail: assets/thumbnails/mao2024dreamdrive.jpg\n  publication_date: '2024-12-31T18:59:57+00:00'\n- id: wang2024sgsplatting\n  title: 'SG-Splatting: Accelerating 3D Gaussian Splatting with Spherical Gaussians'\n  authors: Yiwen Wang, Siyuan Chen, Ran Yi\n  year: '2024'\n  abstract: '3D Gaussian Splatting is emerging as a state-of-the-art technique in\n    novel view synthesis, recognized for its impressive balance between visual quality,\n    speed, and rendering efficiency. However, reliance on third-degree spherical harmonics\n    for color representation introduces significant storage demands and computational\n    overhead, resulting in a large memory footprint and slower rendering speed. We\n    introduce SG-Splatting with Spherical Gaussians based color representation, a\n    novel approach to enhance rendering speed and quality in novel view synthesis.\n    Our method first represents view-dependent color using Spherical Gaussians, instead\n    of three degree spherical harmonics, which largely reduces the number of parameters\n    used for color representation, and significantly accelerates the rendering process.\n    We then develop an efficient strategy for organizing multiple Spherical Gaussians,\n    optimizing their arrangement to achieve a balanced and accurate scene representation.\n    To further improve rendering quality, we propose a mixed representation that combines\n    Spherical Gaussians with low-degree spherical harmonics, capturing both high-\n    and low-frequency color information effectively. SG-Splatting also has plug-and-play\n    capability, allowing it to be easily integrated into existing systems. This approach\n    improves computational efficiency and overall visual fidelity, making it a practical\n    solution for real-time applications.\n\n    '\n  project_page: null\n  paper: https://arxiv.org/pdf/2501.00342.pdf\n  code: null\n  video: null\n  tags:\n  - Acceleration\n  thumbnail: assets/thumbnails/wang2024sgsplatting.jpg\n  publication_date: '2024-12-31T08:31:52+00:00'\n- id: cha2024perse\n  title: 'PERSE: Personalized 3D Generative Avatars from A Single Portrait'\n  authors: Hyunsoo Cha, Inhee Lee, Hanbyul Joo\n  year: '2024'\n  abstract: We present PERSE, a method for building an animatable personalized generative\n    avatar from a reference portrait. Our avatar model enables facial attribute editing\n    in a continuous and disentangled latent space to control each facial attribute,\n    while preserving the individual's identity. To achieve this, our method begins\n    by synthesizing large-scale synthetic 2D video datasets, where each video contains\n    consistent changes in the facial expression and viewpoint, combined with a variation\n    in a specific facial attribute from the original input. We propose a novel pipeline\n    to produce high-quality, photorealistic 2D videos with facial attribute editing.\n    Leveraging this synthetic attribute dataset, we present a personalized avatar\n    creation method based on the 3D Gaussian Splatting, learning a continuous and\n    disentangled latent space for intuitive facial attribute manipulation. To enforce\n    smooth transitions in this latent space, we introduce a latent space regularization\n    technique by using interpolated 2D faces as supervision. Compared to previous\n    approaches, we demonstrate that PERSE generates high-quality avatars with interpolated\n    attributes while preserving identity of reference person.\n  project_page: https://hyunsoocha.github.io/perse/\n  paper: https://arxiv.org/pdf/2412.21206v1.pdf\n  code: null\n  video: https://youtu.be/zX881Zx03o4\n  tags:\n  - Avatar\n  - GAN\n  - Project\n  - Video\n  thumbnail: assets/thumbnails/cha2024perse.jpg\n  publication_date: '2024-12-30T18:59:58+00:00'\n- id: yang20244d\n  title: '4D Gaussian Splatting: Modeling Dynamic Scenes with Native 4D Primitives'\n  authors: Zeyu Yang, Zijie Pan, Xiatian Zhu, Li Zhang, Yu-Gang Jiang, Philip H. S.\n    Torr\n  year: '2024'\n  abstract: Dynamic 3D scene representation and novel view synthesis from captured\n    videos are crucial for enabling immersive experiences required by AR/VR and metaverse\n    applications. However, this task is challenging due to the complexity of unconstrained\n    real-world scenes and their temporal dynamics. In this paper, we frame dynamic\n    scenes as a spatio-temporal 4D volume learning problem, offering a native explicit\n    reformulation with minimal assumptions about motion, which serves as a versatile\n    dynamic scene learning framework. Specifically, we represent a target dynamic\n    scene using a collection of 4D Gaussian primitives with explicit geometry and\n    appearance features, dubbed as 4D Gaussian splatting (4DGS). This approach can\n    capture relevant information in space and time by fitting the underlying spatio-temporal\n    volume. Modeling the spacetime as a whole with 4D Gaussians parameterized by anisotropic\n    ellipses that can rotate arbitrarily in space and time, our model can naturally\n    learn view-dependent and time-evolved appearance with 4D spherindrical harmonics.\n    Notably, our 4DGS model is the first solution that supports real-time rendering\n    of high-resolution, photorealistic novel views for complex dynamic scenes. To\n    enhance efficiency, we derive several compact variants that effectively reduce\n    memory footprint and mitigate the risk of overfitting. Extensive experiments validate\n    the superiority of 4DGS in terms of visual quality and efficiency across a range\n    of dynamic scene-related tasks (e.g., novel view synthesis, 4D generation, scene\n    understanding) and scenarios (e.g., single object, indoor scenes, driving environments,\n    synthetic and real data).\n  project_page: null\n  paper: https://arxiv.org/pdf/2412.20720v1.pdf\n  code: null\n  video: null\n  tags:\n  - Compression\n  - Dynamic\n  - Large-Scale\n  thumbnail: assets/thumbnails/yang20244d.jpg\n  publication_date: '2024-12-30T05:30:26+00:00'\n- id: liu2024maskgaussian\n  title: 'MaskGaussian: Adaptive 3D Gaussian Representation from Probabilistic Masks'\n  authors: Yifei Liu, Zhihang Zhong, Yifan Zhan, Sheng Xu, Xiao Sun\n  year: '2024'\n  abstract: 'While 3D Gaussian Splatting (3DGS) has demonstrated remarkable performance\n    in novel view synthesis and real-time rendering, the high memory consumption due\n    to the use of millions of Gaussians limits its practicality. To mitigate this\n    issue, improvements have been made by pruning unnecessary Gaussians, either through\n    a hand-crafted criterion or by using learned masks. However, these methods deterministically\n    remove Gaussians based on a snapshot of the pruning moment, leading to sub-optimized\n    reconstruction performance from a long-term perspective. To address this issue,\n    we introduce MaskGaussian, which models Gaussians as probabilistic entities rather\n    than permanently removing them, and utilize them according to their probability\n    of existence. To achieve this, we propose a masked-rasterization technique that\n    enables unused yet probabilistically existing Gaussians to receive gradients,\n    allowing for dynamic assessment of their contribution to the evolving scene and\n    adjustment of their probability of existence. Hence, the importance of Gaussians\n    iteratively changes and the pruned Gaussians are selected diversely. Extensive\n    experiments demonstrate the superiority of the proposed method in achieving better\n    rendering quality with fewer Gaussians than previous pruning methods, pruning\n    over 60% of Gaussians on average with only a 0.02 PSNR decline. Our code can be\n    found at: https://github.com/kaikai23/MaskGaussian\n\n    '\n  project_page: null\n  paper: https://arxiv.org/pdf/2412.20522.pdf\n  code: https://github.com/kaikai23/MaskGaussian\n  video: null\n  tags:\n  - Code\n  - Compression\n  thumbnail: assets/thumbnails/liu2024maskgaussian.jpg\n  publication_date: '2024-12-29T17:12:16+00:00'\n  date_source: arxiv\n- id: xu2024das3r\n  title: 'DAS3R: Dynamics-Aware Gaussian Splatting for Static Scene Reconstruction'\n  authors: Kai Xu, Tze Ho Elden Tse, Jizong Peng, Angela Yao\n  year: '2024'\n  abstract: 'We propose a novel framework for scene decomposition and static background\n    reconstruction from everyday videos. By integrating the trained motion masks and\n    modeling the static scene as Gaussian splats with dynamics-aware optimization,\n    our method achieves more accurate background reconstruction results than previous\n    works. Our proposed method is termed DAS3R, an abbreviation for Dynamics-Aware\n    Gaussian Splatting for Static Scene Reconstruction. Compared to existing methods,\n    DAS3R is more robust in complex motion scenarios, capable of handling videos where\n    dynamic objects occupy a significant portion of the scene, and does not require\n    camera pose inputs or point cloud data from SLAM-based methods. We compared DAS3R\n    against recent distractor-free approaches on the DAVIS and Sintel datasets; DAS3R\n    demonstrates enhanced performance and robustness with a margin of more than 2\n    dB in PSNR. The project''s webpage can be accessed via \\url{https://kai422.github.io/DAS3R/}\n\n    '\n  project_page: https://kai422.github.io/DAS3R/\n  paper: https://arxiv.org/pdf/2412.19584.pdf\n  code: https://github.com/kai422/das3r\n  video: https://kai422.github.io/DAS3R/assets/davis.gif\n  tags:\n  - Code\n  - Project\n  - Video\n  thumbnail: assets/thumbnails/xu2024das3r.jpg\n  publication_date: '2024-12-27T10:59:46+00:00'\n  date_source: arxiv\n- id: cai2024dust\n  title: 'Dust to Tower: Coarse-to-Fine Photo-Realistic Scene Reconstruction from\n    Sparse Uncalibrated Images'\n  authors: Xudong Cai, Yongcai Wang, Zhaoxin Fan, Deng Haoran, Shuo Wang, Wanting\n    Li, Deying Li, Lun Luo, Minhang Wang, Jintao Xu\n  year: '2024'\n  abstract: Photo-realistic scene reconstruction from sparse-view, uncalibrated images\n    is highly required in practice. Although some successes have been made, existing\n    methods are either Sparse-View but require accurate camera parameters (i.e., intrinsic\n    and extrinsic), or SfM-free but need densely captured images. To combine the advantages\n    of both methods while addressing their respective weaknesses, we propose Dust\n    to Tower (D2T), an accurate and efficient coarse-to-fine framework to optimize\n    3DGS and image poses simultaneously from sparse and uncalibrated images. Our key\n    idea is to first construct a coarse model efficiently and subsequently refine\n    it using warped and inpainted images at novel viewpoints. To do this, we first\n    introduce a Coarse Construction Module (CCM) which exploits a fast Multi-View\n    Stereo model to initialize a 3D Gaussian Splatting (3DGS) and recover initial\n    camera poses. To refine the 3D model at novel viewpoints, we propose a Confidence\n    Aware Depth Alignment (CADA) module to refine the coarse depth maps by aligning\n    their confident parts with estimated depths by a Mono-depth model. Then, a Warped\n    Image-Guided Inpainting (WIGI) module is proposed to warp the training images\n    to novel viewpoints by the refined depth maps, and inpainting is applied to fulfill\n    the ``holes\" in the warped images caused by view-direction changes, providing\n    high-quality supervision to further optimize the 3D model and the camera poses.\n    Extensive experiments and ablation studies demonstrate the validity of D2T and\n    its design choices, achieving state-of-the-art performance in both tasks of novel\n    view synthesis and pose estimation while keeping high efficiency. Codes will be\n    publicly available.\n  project_page: null\n  paper: https://arxiv.org/pdf/2412.19518.pdf\n  code: null\n  video: null\n  tags:\n  - Inpainting\n  - Poses\n  - Sparse\n  thumbnail: assets/thumbnails/cai2024dust.jpg\n  publication_date: '2024-12-27T08:19:34+00:00'\n- id: yao2024reflective\n  title: Reflective Gaussian Splatting\n  authors: Yuxuan Yao, Zixuan Zeng, Chun Gu, Xiatian Zhu, Li Zhang\n  year: '2024'\n  abstract: 'Novel view synthesis has experienced significant advancements owing to\n    increasingly capable NeRF- and 3DGS-based methods. However, reflective object\n    reconstruction remains challenging, lacking a proper solution to achieve real-time,\n    high-quality rendering while accommodating inter-reflection. To fill this gap,\n    we introduce a Reflective Gaussian splatting (\\textbf{Ref-Gaussian}) framework\n    characterized with two components: (I) {\\em Physically based deferred rendering}\n    that empowers the rendering equation with pixel-level material properties via\n    formulating split-sum approximation; (II) {\\em Gaussian-grounded inter-reflection}\n    that realizes the desired inter-reflection function within a Gaussian splatting\n    paradigm for the first time. To enhance geometry modeling, we further introduce\n    material-aware normal propagation and an initial per-Gaussian shading stage, along\n    with 2D Gaussian primitives. Extensive experiments on standard datasets demonstrate\n    that Ref-Gaussian surpasses existing approaches in terms of quantitative metrics,\n    visual quality, and compute efficiency. Further, we show that our method serves\n    as a unified solution for both reflective and non-reflective scenes, going beyond\n    the previous alternatives focusing on only reflective scenes. Also, we illustrate\n    that Ref-Gaussian supports more applications such as relighting and editing.\n\n    '\n  project_page: https://fudan-zvg.github.io/ref-gaussian/\n  paper: https://arxiv.org/pdf/2412.19282.pdf\n  code: null\n  video: null\n  tags:\n  - Meshing\n  - Project\n  - Ray Tracing\n  - Relight\n  thumbnail: assets/thumbnails/yao2024reflective.jpg\n  publication_date: '2024-12-26T16:58:35+00:00'\n- id: qian2024weathergs\n  title: 'WeatherGS: 3D Scene Reconstruction in Adverse Weather Conditions via Gaussian\n    Splatting'\n  authors: Chenghao Qian, Yuhu Guo, Wenjing Li, Gustav Markkula\n  year: '2024'\n  abstract: 3D Gaussian Splatting (3DGS) has gained significant attention for 3D scene\n    reconstruction, but still suffers from complex outdoor environments, especially\n    under adverse weather. This is because 3DGS treats the artifacts caused by adverse\n    weather as part of the scene and will directly reconstruct them, largely reducing\n    the clarity of the reconstructed scene. To address this challenge, we propose\n    WeatherGS, a 3DGS-based framework for reconstructing clear scenes from multi-view\n    images under different weather conditions. Specifically, we explicitly categorize\n    the multi-weather artifacts into the dense particles and lens occlusions that\n    have very different characters, in which the former are caused by snowflakes and\n    raindrops in the air, and the latter are raised by the precipitation on the camera\n    lens. In light of this, we propose a dense-to-sparse preprocess strategy, which\n    sequentially removes the dense particles by an Atmospheric Effect Filter (AEF)\n    and then extracts the relatively sparse occlusion masks with a Lens Effect Detector\n    (LED). Finally, we train a set of 3D Gaussians by the processed images and generated\n    masks for excluding occluded areas, and accurately recover the underlying clear\n    scene by Gaussian splatting. We conduct a diverse and challenging benchmark to\n    facilitate the evaluation of 3D reconstruction under complex weather scenarios.\n    Extensive experiments on this benchmark demonstrate that our WeatherGS consistently\n    produces high-quality, clean scenes across various weather scenarios, outperforming\n    existing state-of-the-art methods.\n  project_page: null\n  paper: https://arxiv.org/pdf/2412.18862.pdf\n  code: https://github.com/Jumponthemoon/WeatherGS\n  video: null\n  tags:\n  - Code\n  - In the Wild\n  thumbnail: assets/thumbnails/qian2024weathergs.jpg\n  publication_date: '2024-12-25T10:16:57+00:00'\n- id: lyu2024facelift\n  title: 'FaceLift: Single Image to 3D Head with View Generation and GS-LRM'\n  authors: Weijie Lyu, Yi Zhou, Ming-Hsuan Yang, Zhixin Shu\n  year: '2024'\n  abstract: We present FaceLift, a feed-forward approach for rapid, high-quality,\n    360-degree head reconstruction from a single image. Our pipeline begins by employing\n    a multi-view latent diffusion model that generates consistent side and back views\n    of the head from a single facial input. These generated views then serve as input\n    to a GS-LRM reconstructor, which produces a comprehensive 3D representation using\n    Gaussian splats. To train our system, we develop a dataset of multi-view renderings\n    using synthetic 3D human head as-sets. The diffusion-based multi-view generator\n    is trained exclusively on synthetic head images, while the GS-LRM reconstructor\n    undergoes initial training on Objaverse followed by fine-tuning on synthetic head\n    data. FaceLift excels at preserving identity and maintaining view consistency\n    across views. Despite being trained solely on synthetic data, FaceLift demonstrates\n    remarkable generalization to real-world images. Through extensive qualitative\n    and quantitative evaluations, we show that FaceLift outperforms state-of-the-art\n    methods in 3D head reconstruction, highlighting its practical applicability and\n    robust performance on real-world images. In addition to single image reconstruction,\n    FaceLift supports video inputs for 4D novel view synthesis and seamlessly integrates\n    with 2D reanimation techniques to enable 3D facial animation.\n  project_page: https://www.wlyu.me/FaceLift/\n  paper: https://arxiv.org/pdf/2412.17812.pdf\n  code: null\n  video: https://huggingface.co/wlyu/FaceLift/resolve/main/videos/website_video.mp4\n  tags:\n  - Avatar\n  - Feed-Forward\n  - Project\n  - Video\n  thumbnail: assets/thumbnails/lyu2024facelift.jpg\n  publication_date: '2024-12-23T18:59:49+00:00'\n- id: shao2024gausim\n  title: 'GauSim: Registering Elastic Objects into Digital World by Gaussian Simulator'\n  authors: Yidi Shao, Mu Huang, Chen Change Loy, Bo Dai\n  year: '2024'\n  abstract: In this work, we introduce GauSim, a novel neural network-based simulator\n    designed to capture the dynamic behaviors of real-world elastic objects represented\n    through Gaussian kernels. Unlike traditional methods that treat kernels as particles\n    within particle-based simulations, we leverage continuum mechanics, modeling each\n    kernel as a continuous piece of matter to account for realistic deformations without\n    idealized assumptions. To improve computational efficiency and fidelity, we employ\n    a hierarchical structure that organizes kernels into Center of Mass Systems (CMS)\n    with explicit formulations, enabling a coarse-to-fine simulation approach. This\n    structure significantly reduces computational overhead while preserving detailed\n    dynamics. In addition, GauSim incorporates explicit physics constraints, such\n    as mass and momentum conservation, ensuring interpretable results and robust,\n    physically plausible simulations. To validate our approach, we present a new dataset,\n    READY, containing multi-view videos of real-world elastic deformations. Experimental\n    results demonstrate that GauSim achieves superior performance compared to existing\n    physics-driven baselines, offering a practical and accurate solution for simulating\n    complex dynamic behaviors. Code and model will be released.\n  project_page: https://www.mmlab-ntu.com/project/gausim/index.html\n  paper: https://arxiv.org/pdf/2412.17804.pdf\n  code: null\n  video: null\n  tags:\n  - Dynamic\n  - Physics\n  - Project\n  thumbnail: assets/thumbnails/shao2024gausim.jpg\n  publication_date: '2024-12-23T18:58:17+00:00'\n- id: jin2024activegs\n  title: 'ActiveGS: Active Scene Reconstruction using Gaussian Splatting'\n  authors: Liren Jin, Xingguang Zhong, Yue Pan, Jens Behley, Cyrill Stachniss, Marija\n    Popović\n  year: '2024'\n  abstract: 'Robotics applications often rely on scene reconstructions to enable downstream\n    tasks. In this work, we tackle the challenge of actively building an accurate\n    map of an unknown scene using an on-board RGB-D camera. We propose a hybrid map\n    representation that combines a Gaussian splatting map with a coarse voxel map,\n    leveraging the strengths of both representations: the high-fidelity scene reconstruction\n    capabilities of Gaussian splatting and the spatial modelling strengths of the\n    voxel map. The core of our framework is an effective confidence modelling technique\n    for the Gaussian splatting map to identify under-reconstructed areas, while utilising\n    spatial information from the voxel map to target unexplored areas and assist in\n    collision-free path planning. By actively collecting scene information in under-reconstructed\n    and unexplored areas for map updates, our approach achieves superior Gaussian\n    splatting reconstruction results compared to state-of-the-art approaches. Additionally,\n    we demonstrate the applicability of our active scene reconstruction framework\n    in the real world using an unmanned aerial vehicle.\n\n    '\n  project_page: null\n  paper: https://arxiv.org/pdf/2412.17769.pdf\n  code: null\n  video: null\n  tags:\n  - Meshing\n  - Robotics\n  - SLAM\n  thumbnail: assets/thumbnails/jin2024activegs.jpg\n  publication_date: '2024-12-23T18:29:03+00:00'\n  date_source: arxiv\n- id: gao2024cosurfgscollaborative\n  title: CoSurfGS:Collaborative 3D Surface Gaussian Splatting with Distributed Learning\n    for Large Scene Reconstruction\n  authors: Yuanyuan Gao, Yalun Dai, Hao Li, Weicai Ye, Junyi Chen, Danpeng Chen, Dingwen\n    Zhang, Tong He, Guofeng Zhang, Junwei Han\n  year: '2024'\n  abstract: 3D Gaussian Splatting (3DGS) has demonstrated impressive performance in\n    scene reconstruction. However, most existing GS-based surface reconstruction methods\n    focus on 3D objects or limited scenes. Directly applying these methods to large-scale\n    scene reconstruction will pose challenges such as high memory costs, excessive\n    time consumption, and lack of geometric detail, which makes it difficult to implement\n    in practical applications. To address these issues, we propose a multi-agent collaborative\n    fast 3DGS surface reconstruction framework based on distributed learning for large-scale\n    surface reconstruction. Specifically, we develop local model compression (LMC)\n    and model aggregation schemes (MAS) to achieve high-quality surface representation\n    of large scenes while reducing GPU memory consumption. Extensive experiments on\n    Urban3d, MegaNeRF, and BlendedMVS demonstrate that our proposed method can achieve\n    fast and scalable high-fidelity surface reconstruction and photorealistic rendering.\n  project_page: https://gyy456.github.io/CoSurfGS/\n  paper: https://arxiv.org/pdf/2412.17612.pdf\n  code: null\n  video: null\n  tags:\n  - Distributed\n  - Large-Scale\n  - Meshing\n  - Project\n  thumbnail: assets/thumbnails/gao2024cosurfgscollaborative.jpg\n  publication_date: '2024-12-23T14:31:15+00:00'\n- id: gui2024balanced\n  title: 'Balanced 3DGS: Gaussian-wise Parallelism Rendering with Fine-Grained Tiling'\n  authors: Hao Gui, Lin Hu, Rui Chen, Mingxiao Huang, Yuxin Yin, Jin Yang, Yong Wu\n  year: '2024'\n  abstract: '3D Gaussian Splatting (3DGS) is increasingly attracting attention in\n    both academia and industry owing to its superior visual quality and rendering\n    speed. However, training a 3DGS model remains a time-intensive task, especially\n    in load imbalance scenarios where workload diversity among pixels and Gaussian\n    spheres causes poor renderCUDA kernel performance. We introduce Balanced 3DGS,\n    a Gaussian-wise parallelism rendering with fine-grained tiling approach in 3DGS\n    training process, perfectly solving load-imbalance issues. First, we innovatively\n    introduce the inter-block dynamic workload distribution technique to map workloads\n    to Streaming Multiprocessor(SM) resources within a single GPU dynamically, which\n    constitutes the foundation of load balancing. Second, we are the first to propose\n    the Gaussian-wise parallel rendering technique to significantly reduce workload\n    divergence inside a warp, which serves as a critical component in addressing load\n    imbalance. Based on the above two methods, we further creatively put forward the\n    fine-grained combined load balancing technique to uniformly distribute workload\n    across all SMs, which boosts the forward renderCUDA kernel performance by up to\n    7.52x. Besides, we present a self-adaptive render kernel selection strategy during\n    the 3DGS training process based on different load-balance situations, which effectively\n    improves training efficiency.\n\n    '\n  project_page: null\n  paper: https://arxiv.org/pdf/2412.17378.pdf\n  code: null\n  video: null\n  tags:\n  - Acceleration\n  thumbnail: assets/thumbnails/gui2024balanced.jpg\n  publication_date: '2024-12-23T08:26:30+00:00'\n- id: jambon2024interactive\n  title: Interactive Scene Authoring with Specialized Generative Primitives\n  authors: Clément Jambon, Changwoon Choi, Dongsu Zhang, Olga Sorkine-Hornung, Young\n    Min Kim\n  year: '2024'\n  abstract: 'Generating high-quality 3D digital assets often requires expert knowledge\n    of complex design tools. We introduce Specialized Generative Primitives, a generative\n    framework that allows non-expert users to author high-quality 3D scenes in a seamless,\n    lightweight, and controllable manner. Each primitive is an efficient generative\n    model that captures the distribution of a single exemplar from the real world.\n    With our framework, users capture a video of an environment, which we turn into\n    a high-quality and explicit appearance model thanks to 3D Gaussian Splatting.\n    Users then select regions of interest guided by semantically-aware features. To\n    create a generative primitive, we adapt Generative Cellular Automata to single-exemplar\n    training and controllable generation. We decouple the generative task from the\n    appearance model by operating on sparse voxels and we recover a high-quality output\n    with a subsequent sparse patch consistency step. Each primitive can be trained\n    within 10 minutes and used to author new scenes interactively in a fully compositional\n    manner. We showcase interactive sessions where various primitives are extracted\n    from real-world scenes and controlled to create 3D assets and scenes in a few\n    minutes. We also demonstrate additional capabilities of our primitives: handling\n    various 3D representations to control generation, transferring appearances, and\n    editing geometries.\n\n    '\n  project_page: null\n  paper: https://arxiv.org/pdf/2412.16253.pdf\n  code: null\n  video: null\n  tags:\n  - Editing\n  - World Generation\n  thumbnail: assets/thumbnails/jambon2024interactive.jpg\n  publication_date: '2024-12-20T04:39:50+00:00'\n- id: shen2024solidgs\n  title: 'SolidGS: Consolidating Gaussian Surfel Splatting for Sparse-View Surface\n    Reconstruction'\n  authors: Zhuowen Shen, Yuan Liu, Zhang Chen, Zhong Li, Jiepeng Wang, Yongqing Liang,\n    Zhengming Yu, Jingdong Zhang, Yi Xu, Scott Schaefer, Xin Li, Wenping Wang\n  year: '2024'\n  abstract: 'Gaussian splatting has achieved impressive improvements for both novel-view\n    synthesis and surface reconstruction from multi-view images. However, current\n    methods still struggle to reconstruct high-quality surfaces from only sparse view\n    input images using Gaussian splatting. In this paper, we propose a novel method\n    called SolidGS to address this problem. We observed that the reconstructed geometry\n    can be severely inconsistent across multi-views, due to the property of Gaussian\n    function in geometry rendering. This motivates us to consolidate all Gaussians\n    by adopting a more solid kernel function, which effectively improves the surface\n    reconstruction quality. With the additional help of geometrical regularization\n    and monocular normal estimation, our method achieves superior performance on the\n    sparse view surface reconstruction than all the Gaussian splatting methods and\n    neural field methods on the widely used DTU, Tanks-and-Temples, and LLFF datasets.\n\n    '\n  project_page: https://mickshen7558.github.io/projects/SolidGS/\n  paper: https://arxiv.org/pdf/2412.15400.pdf\n  code: null\n  video: null\n  tags:\n  - Meshing\n  - Project\n  - Sparse\n  thumbnail: assets/thumbnails/shen2024solidgs.jpg\n  publication_date: '2024-12-19T21:04:43+00:00'\n  date_source: arxiv\n- id: xie2024envgs\n  title: 'EnvGS: Modeling View-Dependent Appearance with Environment Gaussian'\n  authors: Tao Xie, Xi Chen, Zhen Xu, Yiman Xie, Yudong Jin, Yujun Shen, Sida Peng,\n    Hujun Bao, Xiaowei Zhou\n  year: '2024'\n  abstract: 'Reconstructing complex reflections in real-world scenes from 2D images\n    is essential for achieving photorealistic novel view synthesis. Existing methods\n    that utilize environment maps to model reflections from distant lighting often\n    struggle with high-frequency reflection details and fail to account for near-field\n    reflections. In this work, we introduce EnvGS, a novel approach that employs a\n    set of Gaussian primitives as an explicit 3D representation for capturing reflections\n    of environments. These environment Gaussian primitives are incorporated with base\n    Gaussian primitives to model the appearance of the whole scene. To efficiently\n    render these environment Gaussian primitives, we developed a ray-tracing-based\n    renderer that leverages the GPU''s RT core for fast rendering. This allows us\n    to jointly optimize our model for high-quality reconstruction while maintaining\n    real-time rendering speeds. Results from multiple real-world and synthetic datasets\n    demonstrate that our method produces significantly more detailed reflections,\n    achieving the best rendering quality in real-time novel view synthesis.\n\n    '\n  project_page: https://zju3dv.github.io/envgs/\n  paper: https://arxiv.org/pdf/2412.15215.pdf\n  code: null\n  video: https://raw.githubusercontent.com/xbillowy/assets/refs/heads/main/envgs.github.io.assets/teaser.mp4\n  tags:\n  - Project\n  - Ray Tracing\n  - Rendering\n  - Video\n  thumbnail: assets/thumbnails/xie2024envgs.jpg\n  publication_date: '2024-12-19T18:59:57+00:00'\n  date_source: arxiv\n- id: saito2024squeezeme\n  title: 'SqueezeMe: Efficient Gaussian Avatars for VR'\n  authors: Shunsuke Saito, Stanislav Pidhorskyi, Igor Santesteban, Forrest Iandola,\n    Divam Gupta, Anuj Pahuja, Nemanja Bartolovic, Frank Yu, Emanuel Garbin, Tomas\n    Simon\n  year: '2024'\n  abstract: \"Gaussian Splatting has enabled real-time 3D human avatars with unprecedented\\\n    \\ levels of visual quality. While previous methods require a desktop GPU for real-time\\\n    \\ inference of a single avatar, we aim to squeeze multiple Gaussian avatars onto\\\n    \\ a portable virtual reality headset with real-time drivable inference. We begin\\\n    \\ by training a previous work, Animatable Gaussians, on a high quality dataset\\\n    \\ captured with 512 cameras. The Gaussians are animated by controlling base set\\\n    \\ of Gaussians with linear blend skinning (LBS) motion and then further adjusting\\\n    \\ the Gaussians with a neural network decoder to correct their appearance. When\\\n    \\ deploying the model on a Meta Quest 3 VR headset, we find two major computational\\\n    \\ bottlenecks: the decoder and the rendering. To accelerate the decoder, we train\\\n    \\ the Gaussians in UV-space instead of pixel-space, and we distill the decoder\\\n    \\ to a single neural network layer. Further, we discover that neighborhoods of\\\n    \\ Gaussians can share a single corrective from the decoder, which provides an\\\n    \\ additional speedup. To accelerate the rendering, we develop a custom pipeline\\\n    \\ in Vulkan that runs on the mobile GPU. Putting it all together, we run 3 Gaussian\\\n    \\ avatars concurrently at 72 FPS on a VR headset. \\n\"\n  project_page: https://forresti.github.io/squeezeme.\n  paper: https://arxiv.org/pdf/2412.15171.pdf\n  code: null\n  video: null\n  tags:\n  - Avatar\n  - Dynamic\n  - Project\n  thumbnail: assets/thumbnails/saito2024squeezeme.jpg\n  publication_date: '2024-12-19T18:46:55+00:00'\n  date_source: arxiv\n- id: lu2024turbogs\n  title: 'Turbo-GS: Accelerating 3D Gaussian Fitting for High-Quality Radiance Fields'\n  authors: Tao Lu, Ankit Dhiman, R Srinath, Emre Arslan, Angela Xing, Yuanbo Xiangli,\n    R Venkatesh Babu, Srinath Sridhar\n  year: '2024'\n  abstract: 'Novel-view synthesis is an important problem in computer vision with\n    applications in 3D reconstruction, mixed reality, and robotics. Recent methods\n    like 3D Gaussian Splatting (3DGS) have become the preferred method for this task,\n    providing high-quality novel views in real time. However, the training time of\n    a 3DGS model is slow, often taking 30 minutes for a scene with 200 views. In contrast,\n    our goal is to reduce the optimization time by training for fewer steps while\n    maintaining high rendering quality. Specifically, we combine the guidance from\n    both the position error and the appearance error to achieve a more effective densification.\n    To balance the rate between adding new Gaussians and fitting old Gaussians, we\n    develop a convergence-aware budget control mechanism. Moreover, to make the densification\n    process more reliable, we selectively add new Gaussians from mostly visited regions.\n    With these designs, we reduce the Gaussian optimization steps to one-third of\n    the previous approach while achieving a comparable or even better novel view rendering\n    quality. To further facilitate the rapid fitting of 4K resolution images, we introduce\n    a dilation-based rendering technique. Our method, Turbo-GS, speeds up optimization\n    for typical scenes and scales well to high-resolution (4K) scenarios on standard\n    datasets. Through extensive experiments, we show that our method is significantly\n    faster in optimization than other methods while retaining quality. Project page:\n    https://ivl.cs.brown.edu/research/turbo-gs.\n\n    '\n  project_page: https://ivl.cs.brown.edu/research/turbo-gs\n  paper: https://arxiv.org/pdf/2412.13547v1.pdf\n  code: null\n  video: null\n  tags:\n  - Acceleration\n  - Densification\n  - Project\n  thumbnail: assets/thumbnails/lu2024turbogs.jpg\n  publication_date: '2024-12-18T06:46:40+00:00'\n  date_source: arxiv\n- id: jiang2024gausstr\n  title: 'GaussTR: Foundation Model-Aligned Gaussian Transformer for Self-Supervised\n    3D Spatial Understanding'\n  authors: Haoyi Jiang, Liu Liu, Tianheng Cheng, Xinjie Wang, Tianwei Lin, Zhizhong\n    Su, Wenyu Liu, Xinggang Wang\n  year: '2024'\n  abstract: '3D Semantic Occupancy Prediction is fundamental for spatial understanding\n    as it provides a comprehensive semantic cognition of surrounding environments.\n    However, prevalent approaches primarily rely on extensive labeled data and computationally\n    intensive voxel-based modeling, restricting the scalability and generalizability\n    of 3D representation learning. In this paper, we introduce GaussTR, a novel Gaussian\n    Transformer that leverages alignment with foundation models to advance self-supervised\n    3D spatial understanding. GaussTR adopts a Transformer architecture to predict\n    sparse sets of 3D Gaussians that represent scenes in a feed-forward manner. Through\n    aligning rendered Gaussian features with diverse knowledge from pre-trained foundation\n    models, GaussTR facilitates the learning of versatile 3D representations and enables\n    open-vocabulary occupancy prediction without explicit annotations. Empirical evaluations\n    on the Occ3D-nuScenes dataset showcase GaussTR''s state-of-the-art zero-shot performance,\n    achieving 11.70 mIoU while reducing training duration by approximately 50%. These\n    experimental results highlight the significant potential of GaussTR for scalable\n    and holistic 3D spatial understanding, with promising implications for autonomous\n    driving and embodied agents. Code is available at https://github.com/hustvl/GaussTR.\n\n    '\n  project_page: https://hustvl.github.io/GaussTR/\n  paper: https://arxiv.org/pdf/2412.13193.pdf\n  code: https://github.com/hustvl/GaussTR\n  video: null\n  tags:\n  - Code\n  - Project\n  thumbnail: assets/thumbnails/jiang2024gausstr.jpg\n  publication_date: '2024-12-17T18:59:46+00:00'\n  date_source: arxiv\n- id: sun2024realtime\n  title: Real-time Free-view Human Rendering from Sparse-view RGB Videos using Double\n    Unprojected Textures\n  authors: Guoxing Sun, Rishabh Dabral, Heming Zhu, Pascal Fua, Christian Theobalt,\n    Marc Habermann\n  year: '2024'\n  abstract: 'Real-time free-view human rendering from sparse-view RGB inputs is a\n    challenging task due to the sensor scarcity and the tight time budget. To ensure\n    efficiency, recent methods leverage 2D CNNs operating in texture space to learn\n    rendering primitives. However, they either jointly learn geometry and appearance,\n    or completely ignore sparse image information for geometry estimation, significantly\n    harming visual quality and robustness to unseen body poses. To address these issues,\n    we present Double Unprojected Textures, which at the core disentangles coarse\n    geometric deformation estimation from appearance synthesis, enabling robust and\n    photorealistic 4K rendering in real-time. Specifically, we first introduce a novel\n    image-conditioned template deformation network, which estimates the coarse deformation\n    of the human template from a first unprojected texture. This updated geometry\n    is then used to apply a second and more accurate texture unprojection. The resulting\n    texture map has fewer artifacts and better alignment with input views, which benefits\n    our learning of finer-level geometry and appearance represented by Gaussian splats.\n    We validate the effectiveness and efficiency of the proposed method in quantitative\n    and qualitative experiments, which significantly surpasses other state-of-the-art\n    methods.\n\n    '\n  project_page: https://vcai.mpi-inf.mpg.de/projects/DUT/\n  paper: https://arxiv.org/pdf/2412.13183v1.pdf\n  code: null\n  video: https://vcai.mpi-inf.mpg.de/projects/DUT/videos/main_video.mp4\n  tags:\n  - Avatar\n  - Project\n  - Sparse\n  - Texturing\n  - Video\n  thumbnail: assets/thumbnails/sun2024realtime.jpg\n  publication_date: '2024-12-17T18:57:38+00:00'\n  date_source: arxiv\n- id: weiss2024gaussian\n  title: 'Gaussian Billboards: Expressive 2D Gaussian Splatting with Textures'\n  authors: Sebastian Weiss, Derek Bradley\n  year: '2024'\n  abstract: 'Gaussian Splatting has recently emerged as the go-to representation for\n    reconstructing and rendering 3D scenes. The transition from 3D to 2D Gaussian\n    primitives has further improved multi-view consistency and surface reconstruction\n    accuracy. In this work we highlight the similarity between 2D Gaussian Splatting\n    (2DGS) and billboards from traditional computer graphics. Both use flat semi-transparent\n    2D geometry that is positioned, oriented and scaled in 3D space. However 2DGS\n    uses a solid color per splat and an opacity modulated by a Gaussian distribution,\n    where billboards are more expressive, modulating the color with a uv-parameterized\n    texture. We propose to unify these concepts by presenting Gaussian Billboards,\n    a modification of 2DGS to add spatially-varying color achieved using per-splat\n    texture interpolation. The result is a mixture of the two representations, which\n    benefits from both the robust scene optimization power of 2DGS and the expressiveness\n    of texture mapping. We show that our method can improve the sharpness and quality\n    of the scene representation in a wide range of qualitative and quantitative evaluations\n    compared to the original 2DGS implementation.\n\n    '\n  project_page: null\n  paper: https://arxiv.org/pdf/2412.12734v1.pdf\n  code: null\n  video: null\n  tags:\n  - 2DGS\n  - Texturing\n  thumbnail: assets/thumbnails/weiss2024gaussian.jpg\n  publication_date: '2024-12-17T09:57:04+00:00'\n  date_source: arxiv\n- id: murai2024mast3rslam\n  title: 'MASt3R-SLAM: Real-Time Dense SLAM with 3D Reconstruction Priors'\n  authors: Riku Murai, Eric Dexheimer, Andrew J. Davison\n  year: '2024'\n  abstract: 'We present a real-time monocular dense SLAM system designed bottom-up\n    from MASt3R, a two-view 3D reconstruction and matching prior. Equipped with this\n    strong prior, our system is robust on in-the-wild video sequences despite making\n    no assumption on a fixed or parametric camera model beyond a unique camera centre.\n    We introduce efficient methods for pointmap matching, camera tracking and local\n    fusion, graph construction and loop closure, and second-order global optimisation.\n    With known calibration, a simple modification to the system achieves state-of-the-art\n    performance across various benchmarks. Altogether, we propose a plug-and-play\n    monocular SLAM system capable of producing globally-consistent poses and dense\n    geometry while operating at 15 FPS.\n\n    '\n  project_page: null\n  paper: https://arxiv.org/pdf/2412.12392.pdf\n  code: null\n  video: https://www.youtube.com/watch?v=wozt71NBFTQ\n  tags:\n  - 3ster-based\n  - SLAM\n  - Video\n  thumbnail: assets/thumbnails/murai2024mast3rslam.jpg\n  publication_date: '2024-12-16T23:00:05+00:00'\n  date_source: arxiv\n- id: zhang2024pansplat\n  title: 'PanSplat: 4K Panorama Synthesis with Feed-Forward Gaussian Splatting'\n  authors: Cheng Zhang, Haofei Xu, Qianyi Wu, Camilo Cruz Gambardella, Dinh Phung,\n    Jianfei Cai\n  year: '2024'\n  abstract: 'With the advent of portable 360{\\deg} cameras, panorama has gained significant\n    attention in applications like virtual reality (VR), virtual tours, robotics,\n    and autonomous driving. As a result, wide-baseline panorama view synthesis has\n    emerged as a vital task, where high resolution, fast inference, and memory efficiency\n    are essential. Nevertheless, existing methods are typically constrained to lower\n    resolutions (512 $\\times$ 1024) due to demanding memory and computational requirements.\n    In this paper, we present PanSplat, a generalizable, feed-forward approach that\n    efficiently supports resolution up to 4K (2048 $\\times$ 4096). Our approach features\n    a tailored spherical 3D Gaussian pyramid with a Fibonacci lattice arrangement,\n    enhancing image quality while reducing information redundancy. To accommodate\n    the demands of high resolution, we propose a pipeline that integrates a hierarchical\n    spherical cost volume and Gaussian heads with local operations, enabling two-step\n    deferred backpropagation for memory-efficient training on a single A100 GPU. Experiments\n    demonstrate that PanSplat achieves state-of-the-art results with superior efficiency\n    and image quality across both synthetic and real-world datasets. Code will be\n    available at \\url{https://github.com/chengzhag/PanSplat}.\n\n    '\n  project_page: https://chengzhag.github.io/publication/pansplat/\n  paper: https://arxiv.org/pdf/2412.12096v1.pdf\n  code: null\n  video: https://youtu.be/R3qIzL77ZSc\n  tags:\n  - 360 degree\n  - Feed-Forward\n  - Project\n  - Video\n  - World Generation\n  thumbnail: assets/thumbnails/zhang2024pansplat.jpg\n  publication_date: '2024-12-16T18:59:45+00:00'\n  date_source: arxiv\n- id: taubner2024cap4d\n  title: 'CAP4D: Creating Animatable 4D Portrait Avatars with Morphable Multi-View\n    Diffusion Models'\n  authors: Felix Taubner, Ruihang Zhang, Mathieu Tuli, David B. Lindell\n  year: '2024'\n  abstract: 'Reconstructing photorealistic and dynamic portrait avatars from images\n    is essential to many applications including advertising, visual effects, and virtual\n    reality. Depending on the application, avatar reconstruction involves different\n    capture setups and constraints − for example, visual effects studios use camera\n    arrays to capture hundreds of reference images, while content creators may seek\n    to animate a single portrait image downloaded from the internet. As such, there\n    is a large and heterogeneous ecosystem of methods for avatar reconstruction. Techniques\n    based on multi-view stereo or neural rendering achieve the highest quality results,\n    but require hundreds of reference images. Recent generative models produce convincing\n    avatars from a single reference image, but visual fidelity yet lags behind multi-view\n    techniques. Here, we present CAP4D: an approach that uses a morphable multi-view\n    diffusion model to reconstruct photoreal 4D (dynamic 3D) portrait avatars from\n    any number of reference images (i.e., one to 100) and animate and render them\n    in real time. Our approach demonstrates state-of-the-art performance for single-,\n    few-, and multi-image 4D portrait avatar reconstruction, and takes steps to bridge\n    the gap in visual fidelity between single-image and multi-view reconstruction\n    techniques.'\n  project_page: https://felixtaubner.github.io/cap4d/\n  paper: https://arxiv.org/pdf/2412.12093\n  code: null\n  video: null\n  tags:\n  - Avatar\n  - Project\n  thumbnail: assets/thumbnails/taubner2024cap4d.jpg\n  publication_date: '2024-12-16T18:58:51+00:00'\n- id: liang2024wonderland\n  title: 'Wonderland: Navigating 3D Scenes from a Single Image'\n  authors: Hanwen Liang, Junli Cao, Vidit Goel, Guocheng Qian, Sergei Korolev, Demetri\n    Terzopoulos, Konstantinos N. Plataniotis, Sergey Tulyakov, Jian Ren\n  year: '2024'\n  abstract: 'This paper addresses a challenging question: How can we efficiently create\n    high-quality, wide-scope 3D scenes from a single arbitrary image? Existing methods\n    face several constraints, such as requiring multi-view data, time-consuming per-scene\n    optimization, low visual quality in backgrounds, and distorted reconstructions\n    in unseen areas. We propose a novel pipeline to overcome these limitations. Specifically,\n    we introduce a large-scale reconstruction model that uses latents from a video\n    diffusion model to predict 3D Gaussian Splattings for the scenes in a feed-forward\n    manner. The video diffusion model is designed to create videos precisely following\n    specified camera trajectories, allowing it to generate compressed video latents\n    that contain multi-view information while maintaining 3D consistency. We train\n    the 3D reconstruction model to operate on the video latent space with a progressive\n    training strategy, enabling the efficient generation of high-quality, wide-scope,\n    and generic 3D scenes. Extensive evaluations across various datasets demonstrate\n    that our model significantly outperforms existing methods for single-view 3D scene\n    generation, particularly with out-of-domain images. For the first time, we demonstrate\n    that a 3D reconstruction model can be effectively built upon the latent space\n    of a diffusion model to realize efficient 3D scene generation.\n\n    '\n  project_page: https://snap-research.github.io/wonderland/\n  paper: https://arxiv.org/pdf/2412.12091v1.pdf\n  code: null\n  video: null\n  tags:\n  - Feed-Forward\n  - Project\n  - Sparse\n  - World Generation\n  thumbnail: assets/thumbnails/liang2024wonderland.jpg\n  publication_date: '2024-12-16T18:58:17+00:00'\n  date_source: arxiv\n- id: huang2024deformable\n  title: Deformable Radial Kernel Splatting\n  authors: Yi-Hua Huang, Ming-Xian Lin, Yang-Tian Sun, Ziyi Yang, Xiaoyang Lyu, Yan-Pei\n    Cao, Xiaojuan Qi\n  year: '2024'\n  abstract: 'Recently, Gaussian splatting has emerged as a robust technique for representing\n    3D scenes, enabling real-time rasterization and high-fidelity rendering. However,\n    Gaussians'' inherent radial symmetry and smoothness constraints limit their ability\n    to represent complex shapes, often requiring thousands of primitives to approximate\n    detailed geometry. We introduce Deformable Radial Kernel (DRK), which extends\n    Gaussian splatting into a more general and flexible framework. Through learnable\n    radial bases with adjustable angles and scales, DRK efficiently models diverse\n    shape primitives while enabling precise control over edge sharpness and boundary\n    curvature. iven DRK''s planar nature, we further develop accurate ray-primitive\n    intersection computation for depth sorting and introduce efficient kernel culling\n    strategies for improved rasterization efficiency. Extensive experiments demonstrate\n    that DRK outperforms existing methods in both representation efficiency and rendering\n    quality, achieving state-of-the-art performance while dramatically reducing primitive\n    count.\n\n    '\n  project_page: https://yihua7.github.io/DRK-web/\n  paper: https://arxiv.org/pdf/2412.11752v1.pdf\n  code: null\n  video: null\n  tags:\n  - Optimization\n  - Project\n  - Rendering\n  thumbnail: assets/thumbnails/huang2024deformable.jpg\n  publication_date: '2024-12-16T13:11:02+00:00'\n  date_source: arxiv\n- id: liang2024supergseg\n  title: 'SuperGSeg: Open-Vocabulary 3D Segmentation with Structured Super-Gaussians'\n  authors: Siyun Liang, Sen Wang, Kunyi Li, Michael Niemeyer, Stefano Gasperini, Nassir\n    Navab, Federico Tombari\n  year: '2024'\n  abstract: '3D Gaussian Splatting has recently gained traction for its efficient\n    training and real-time rendering. While the vanilla Gaussian Splatting representation\n    is mainly designed for view synthesis, more recent works investigated how to extend\n    it with scene understanding and language features. However, existing methods lack\n    a detailed comprehension of scenes, limiting their ability to segment and interpret\n    complex structures. To this end, We introduce SuperGSeg, a novel approach that\n    fosters cohesive, context-aware scene representation by disentangling segmentation\n    and language field distillation. SuperGSeg first employs neural Gaussians to learn\n    instance and hierarchical segmentation features from multi-view images with the\n    aid of off-the-shelf 2D masks. These features are then leveraged to create a sparse\n    set of what we call Super-Gaussians. Super-Gaussians facilitate the distillation\n    of 2D language features into 3D space. Through Super-Gaussians, our method enables\n    high-dimensional language feature rendering without extreme increases in GPU memory.\n    Extensive experiments demonstrate that SuperGSeg outperforms prior works on both\n    open-vocabulary object localization and semantic segmentation tasks.\n\n    '\n  project_page: https://supergseg.github.io/\n  paper: https://arxiv.org/pdf/2412.10231.pdf\n  code: null\n  video: null\n  tags:\n  - Language Embedding\n  - Project\n  - Segmentation\n  thumbnail: assets/thumbnails/liang2024supergseg.jpg\n  publication_date: '2024-12-13T16:01:19+00:00'\n  date_source: arxiv\n- id: tang2024gaf\n  title: 'GAF: Gaussian Avatar Reconstruction from Monocular Videos via Multi-view\n    Diffusion'\n  authors: Jiapeng Tang, Davide Davoli, Tobias Kirschstein, Liam Schoneveld, Matthias\n    Nießner\n  year: '2024'\n  abstract: We propose a novel approach for reconstructing animatable 3D Gaussian\n    avatars from monocular videos captured by commodity devices like smartphones.\n    Photorealistic 3D head avatar reconstruction from such recordings is challenging\n    due to limited observations, which leaves unobserved regions under-constrained\n    and can lead to artifacts in novel views. To address this problem, we introduce\n    a multi-view head diffusion model, leveraging its priors to fill in missing regions\n    and ensure view consistency in Gaussian splatting renderings. To enable precise\n    viewpoint control, we use normal maps rendered from FLAME-based head reconstruction,\n    which provides pixel-aligned inductive biases. We also condition the diffusion\n    model on VAE features extracted from the input image to preserve details of facial\n    identity and appearance. For Gaussian avatar reconstruction, we distill multi-view\n    diffusion priors by using iteratively denoised images as pseudo-ground truths,\n    effectively mitigating over-saturation issues. To further improve photorealism,\n    we apply latent upsampling to refine the denoised latent before decoding it into\n    an image. We evaluate our method on the NeRSemble dataset, showing that GAF outperforms\n    the previous state-of-the-art methods in novel view synthesis and novel expression\n    animation. Furthermore, we demonstrate higher-fidelity avatar reconstructions\n    from monocular videos captured on commodity devices.\n  project_page: https://tangjiapeng.github.io/projects/GAF/\n  paper: https://arxiv.org/pdf/2412.10209\n  code: null\n  video: https://www.youtube.com/embed/QuIYTljvhygE\n  tags:\n  - Avatar\n  - Project\n  - Video\n  thumbnail: assets/thumbnails/tang2024gaf.jpg\n  publication_date: '2024-12-13T15:31:22+00:00'\n- id: park2024splinegs\n  title: 'SplineGS: Robust Motion-Adaptive Spline for Real-Time Dynamic 3D Gaussians\n    from Monocular Video'\n  authors: Jongmin Park, Minh-Quan Viet Bui, Juan Luis Gonzalez Bello, Jaeho Moon,\n    Jihyong Oh, Munchurl Kim\n  year: '2024'\n  abstract: 'Synthesizing novel views from in-the-wild monocular videos is challenging\n    due to scene dynamics and the lack of multi-view cues. To address this, we propose\n    SplineGS, a COLMAP-free dynamic 3D Gaussian Splatting (3DGS) framework for high-quality\n    reconstruction and fast rendering from monocular videos. At its core is a novel\n    Motion-Adaptive Spline (MAS) method, which represents continuous dynamic 3D Gaussian\n    trajectories using cubic Hermite splines with a small number of control points.\n    For MAS, we introduce a Motion-Adaptive Control points Pruning (MACP) method to\n    model the deformation of each dynamic 3D Gaussian across varying motions, progressively\n    pruning control points while maintaining dynamic modeling integrity. Additionally,\n    we present a joint optimization strategy for camera parameter estimation and 3D\n    Gaussian attributes, leveraging photometric and geometric consistency. This eliminates\n    the need for Structure-from-Motion preprocessing and enhances SplineGS''s robustness\n    in real-world conditions. Experiments show that SplineGS significantly outperforms\n    state-of-the-art methods in novel view synthesis quality for dynamic scenes from\n    monocular videos, achieving thousands times faster rendering speed.\n\n    '\n  project_page: https://kaist-viclab.github.io/splinegs-site/\n  paper: https://arxiv.org/pdf/2412.09982v1.pdf\n  code: null\n  video: null\n  tags:\n  - Dynamic\n  - Monocular\n  - Project\n  thumbnail: assets/thumbnails/park2024splinegs.jpg\n  publication_date: '2024-12-13T09:09:14+00:00'\n  date_source: arxiv\n- id: xu2024representing\n  title: Representing Long Volumetric Video with Temporal Gaussian Hierarchy\n  authors: Zhen Xu, Yinghao Xu, Zhiyuan Yu, Sida Peng, Jiaming Sun, Hujun Bao, Xiaowei\n    Zhou\n  year: '2024'\n  abstract: 'This paper aims to address the challenge of reconstructing long volumetric\n    videos from multi-view RGB videos. Recent dynamic view synthesis methods leverage\n    powerful 4D representations, like feature grids or point cloud sequences, to achieve\n    high-quality rendering results. However, they are typically limited to short (1~2s)\n    video clips and often suffer from large memory footprints when dealing with longer\n    videos. To solve this issue, we propose a novel 4D representation, named Temporal\n    Gaussian Hierarchy, to compactly model long volumetric videos. Our key observation\n    is that there are generally various degrees of temporal redundancy in dynamic\n    scenes, which consist of areas changing at different speeds. Motivated by this,\n    our approach builds a multi-level hierarchy of 4D Gaussian primitives, where each\n    level separately describes scene regions with different degrees of content change,\n    and adaptively shares Gaussian primitives to represent unchanged scene content\n    over different temporal segments, thus effectively reducing the number of Gaussian\n    primitives. In addition, the tree-like structure of the Gaussian hierarchy allows\n    us to efficiently represent the scene at a particular moment with a subset of\n    Gaussian primitives, leading to nearly constant GPU memory usage during the training\n    or rendering regardless of the video length. Extensive experimental results demonstrate\n    the superiority of our method over alternative methods in terms of training cost,\n    rendering speed, and storage usage. To our knowledge, this work is the first approach\n    capable of efficiently handling minutes of volumetric video data while maintaining\n    state-of-the-art rendering quality. Our project page is available at: https://zju3dv.github.io/longvolcap.\n\n    '\n  project_page: https://zju3dv.github.io/longvolcap/\n  paper: https://arxiv.org/pdf/2412.09608.pdf\n  code: null\n  video: https://www.youtube.com/watch?v=y7e0YRNNmXw&feature=youtu.be\n  tags:\n  - Acceleration\n  - Dynamic\n  - Project\n  - Video\n  thumbnail: assets/thumbnails/xu2024representing.jpg\n  publication_date: '2024-12-12T18:59:34+00:00'\n- id: chen2024feat2gs\n  title: 'Feat2GS: Probing Visual Foundation Models with Gaussian Splatting'\n  authors: Yue Chen, Xingyu Chen, Anpei Chen, Gerard Pons-Moll, Yuliang Xiu\n  year: '2024'\n  abstract: 'Given that visual foundation models (VFMs) are trained on extensive datasets\n    but often limited to 2D images, a natural question arises: how well do they understand\n    the 3D world? With the differences in architecture and training protocols (i.e.,\n    objectives, proxy tasks), a unified framework to fairly and comprehensively probe\n    their 3D awareness is urgently needed. Existing works on 3D probing suggest single-view\n    2.5D estimation (e.g., depth and normal) or two-view sparse 2D correspondence\n    (e.g., matching and tracking). Unfortunately, these tasks ignore texture awareness,\n    and require 3D data as ground-truth, which limits the scale and diversity of their\n    evaluation set. To address these issues, we introduce Feat2GS, which readout 3D\n    Gaussians attributes from VFM features extracted from unposed images. This allows\n    us to probe 3D awareness for geometry and texture via novel view synthesis, without\n    requiring 3D data. Additionally, the disentanglement of 3DGS parameters - geometry\n    ($\\boldsymbol{x}, \\alpha, \\Sigma$) and texture ($\\boldsymbol{c}$) - enables separate\n    analysis of texture and geometry awareness. Under Feat2GS, we conduct extensive\n    experiments to probe the 3D awareness of several VFMs, and investigate the ingredients\n    that lead to a 3D aware VFM. Building on these findings, we develop several variants\n    that achieve state-of-the-art across diverse datasets. This makes Feat2GS useful\n    for probing VFMs, and as a simple-yet-effective baseline for novel-view synthesis.\n\n    '\n  project_page: https://fanegg.github.io/Feat2GS/\n  paper: https://arxiv.org/pdf/2412.09606.pdf\n  code: null\n  video: https://youtu.be/4fT5lzcAJqo\n  tags:\n  - Project\n  - Rendering\n  - Video\n  - World Generation\n  thumbnail: assets/thumbnails/chen2024feat2gs.jpg\n  publication_date: '2024-12-12T18:59:28+00:00'\n  date_source: arxiv\n- id: li2024simavatar\n  title: 'SimAvatar: Simulation-Ready Avatars with Layered Hair and Clothing'\n  authors: Xueting Li, Ye Yuan, Shalini De Mello, Gilles Daviet, Jonathan Leaf, Miles\n    Macklin, Jan Kautz, Umar Iqbal\n  year: '2024'\n  abstract: 'We introduce SimAvatar, a framework designed to generate simulation-ready\n    clothed 3D human avatars from a text prompt. Current text-driven human avatar\n    generation methods either model hair, clothing, and the human body using a unified\n    geometry or produce hair and garments that are not easily adaptable for simulation\n    within existing simulation pipelines. The primary challenge lies in representing\n    the hair and garment geometry in a way that allows leveraging established prior\n    knowledge from foundational image diffusion models (e.g., Stable Diffusion) while\n    being simulation-ready using either physics or neural simulators. To address this\n    task, we propose a two-stage framework that combines the flexibility of 3D Gaussians\n    with simulation-ready hair strands and garment meshes. Specifically, we first\n    employ three text-conditioned 3D generative models to generate garment mesh, body\n    shape and hair strands from the given text prompt. To leverage prior knowledge\n    from foundational diffusion models, we attach 3D Gaussians to the body mesh, garment\n    mesh, as well as hair strands and learn the avatar appearance through optimization.\n    To drive the avatar given a pose sequence, we first apply physics simulators onto\n    the garment meshes and hair strands. We then transfer the motion onto 3D Gaussians\n    through carefully designed mechanisms for each body part. As a result, our synthesized\n    avatars have vivid texture and realistic dynamic motion. To the best of our knowledge,\n    our method is the first to produce highly realistic, fully simulation-ready 3D\n    avatars, surpassing the capabilities of current approaches.\n\n    '\n  project_page: https://nvlabs.github.io/SimAvatar/\n  paper: https://arxiv.org/pdf/2412.09545.pdf\n  code: null\n  video: https://www.youtube.com/watch?v=qEwBY7LBW2Y\n  tags:\n  - Avatar\n  - Diffusion\n  - Language Embedding\n  - Project\n  - Video\n  thumbnail: assets/thumbnails/li2024simavatar.jpg\n  publication_date: '2024-12-12T18:35:26+00:00'\n  date_source: arxiv\n- id: liu2024slam3r\n  title: 'SLAM3R: Real-Time Dense Scene Reconstruction from Monocular RGB Videos'\n  authors: Yuzheng Liu, Siyan Dong, Shuzhe Wang, Yanchao Yang, Qingnan Fan, Baoquan\n    Chen\n  year: '2024'\n  abstract: 'In this paper, we introduce SLAM3R, a novel and effective monocular RGB\n    SLAM system for real-time and high-quality dense 3D reconstruction. SLAM3R provides\n    an end-to-end solution by seamlessly integrating local 3D reconstruction and global\n    coordinate registration through feed-forward neural networks. Given an input video,\n    the system first converts it into overlapping clips using a sliding window mechanism.\n    Unlike traditional pose optimization-based methods, SLAM3R directly regresses\n    3D pointmaps from RGB images in each window and progressively aligns and deforms\n    these local pointmaps to create a globally consistent scene reconstruction - all\n    without explicitly solving any camera parameters. Experiments across datasets\n    consistently show that SLAM3R achieves state-of-the-art reconstruction accuracy\n    and completeness while maintaining real-time performance at 20+ FPS. Code and\n    weights at: https://github.com/PKU-VCL-3DV/SLAM3R.\n\n    '\n  project_page: null\n  paper: https://arxiv.org/pdf/2412.09401.pdf\n  code: https://github.com/PKU-VCL-3DV/SLAM3R\n  video: null\n  tags:\n  - 3ster-based\n  - Code\n  - SLAM\n  thumbnail: assets/thumbnails/liu2024slam3r.jpg\n  publication_date: '2024-12-12T16:08:03+00:00'\n  date_source: arxiv\n- id: gomel2024diffusionbased\n  title: Diffusion-Based Attention Warping for Consistent 3D Scene Editing\n  authors: Eyal Gomel, Lior Wolf\n  year: '2024'\n  abstract: 'We present a novel method for 3D scene editing using diffusion models,\n    designed to ensure view consistency and realism across perspectives. Our approach\n    leverages attention features extracted from a single reference image to define\n    the intended edits. These features are warped across multiple views by aligning\n    them with scene geometry derived from Gaussian splatting depth estimates. Injecting\n    these warped features into other viewpoints enables coherent propagation of edits,\n    achieving high fidelity and spatial alignment in 3D space. Extensive evaluations\n    demonstrate the effectiveness of our method in generating versatile edits of 3D\n    scenes, significantly advancing the capabilities of scene manipulation compared\n    to the existing methods. Project page: \\url{https://attention-warp.github.io}\n\n    '\n  project_page: https://attention-warp.github.io/\n  paper: https://arxiv.org/pdf/2412.07984.pdf\n  code: null\n  video: null\n  tags:\n  - Diffusion\n  - Project\n  - Style Transfer\n  thumbnail: assets/thumbnails/gomel2024diffusionbased.jpg\n  publication_date: '2024-12-10T23:57:18+00:00'\n  date_source: arxiv\n- id: saunders2024gasp\n  title: 'GASP: Gaussian Avatars with Synthetic Priors'\n  authors: Jack Saunders, Charlie Hewitt, Yanan Jian, Marek Kowalski, Tadas Baltrušaitis,\n    Yiye Chen, Darren Cosker, Virginia Estellers, Nicholas Gyde, Vinay Namboodiri,\n    Benjamin Lundell\n  year: '2024'\n  abstract: 'Gaussian Splatting has changed the game for real-time photo-realistic\n    rendering. One of the most popular applications of Gaussian Splatting is to create\n    animatable avatars, known as Gaussian Avatars. Recent works have pushed the boundaries\n    of quality and rendering efficiency but suffer from two main limitations. Either\n    they require expensive multi-camera rigs to produce avatars with free-view rendering,\n    or they can be trained with a single camera but only rendered at high quality\n    from this fixed viewpoint. An ideal model would be trained using a short monocular\n    video or image from available hardware, such as a webcam, and rendered from any\n    view. To this end, we propose GASP: Gaussian Avatars with Synthetic Priors. To\n    overcome the limitations of existing datasets, we exploit the pixel-perfect nature\n    of synthetic data to train a Gaussian Avatar prior. By fitting this prior model\n    to a single photo or video and fine-tuning it, we get a high-quality Gaussian\n    Avatar, which supports 360-degree rendering. Our prior is only required for fitting,\n    not inference, enabling real-time application. Through our method, we obtain high-quality,\n    animatable Avatars from limited data which can be animated and rendered at 70fps\n    on commercial hardware.'\n  project_page: https://microsoft.github.io/GASP/\n  paper: https://arxiv.org/pdf/2412.07739\n  code: null\n  video: https://www.youtube.com/watch?v=3oWB7-UJUYE\n  tags:\n  - Avatar\n  - Project\n  - Video\n  thumbnail: assets/thumbnails/saunders2024gasp.jpg\n  publication_date: '2024-12-10T18:36:21+00:00'\n- id: wang2024faster\n  title: Faster and Better 3D Splatting via Group Training\n  authors: Chengbo Wang, Guozheng Ma, Yifei Xue, Yizhen Lao\n  year: '2024'\n  abstract: '3D Gaussian Splatting (3DGS) has emerged as a powerful technique for\n    novel view synthesis, demonstrating remarkable capability in high-fidelity scene\n    reconstruction through its Gaussian primitive representations. However, the computational\n    overhead induced by the massive number of primitives poses a significant bottleneck\n    to training efficiency. To overcome this challenge, we propose Group Training,\n    a simple yet effective strategy that organizes Gaussian primitives into manageable\n    groups, optimizing training efficiency and improving rendering quality. This approach\n    shows universal compatibility with existing 3DGS frameworks, including vanilla\n    3DGS and Mip-Splatting, consistently achieving accelerated training while maintaining\n    superior synthesis quality. Extensive experiments reveal that our straightforward\n    Group Training strategy achieves up to 30% faster convergence and improved rendering\n    quality across diverse scenarios.\n\n    '\n  project_page: null\n  paper: https://arxiv.org/pdf/2412.07608.pdf\n  code: null\n  video: null\n  tags:\n  - Acceleration\n  - Densification\n  - Optimization\n  thumbnail: assets/thumbnails/wang2024faster.jpg\n  publication_date: '2024-12-10T15:47:17+00:00'\n  date_source: arxiv\n- id: li2024recap\n  title: 'ReCap: Better Gaussian Relighting with Cross-Environment Captures'\n  authors: Jingzhi Li, Zongwei Wu, Eduard Zamfir, Radu Timofte\n  year: '2024'\n  abstract: 'Accurate 3D objects relighting in diverse unseen environments is crucial\n    for realistic virtual object placement. Due to the albedo-lighting ambiguity,\n    existing methods often fall short in producing faithful relights. Without proper\n    constraints, observed training views can be explained by numerous combinations\n    of lighting and material attributes, lacking physical correspondence with the\n    actual environment maps used for relighting. In this work, we present ReCap, treating\n    cross-environment captures as multi-task target to provide the missing supervision\n    that cuts through the entanglement. Specifically, ReCap jointly optimizes multiple\n    lighting representations that share a common set of material attributes. This\n    naturally harmonizes a coherent set of lighting representations around the mutual\n    material attributes, exploiting commonalities and differences across varied object\n    appearances. Such coherence enables physically sound lighting reconstruction and\n    robust material estimation - both essential for accurate relighting. Together\n    with a streamlined shading function and effective post-processing, ReCap outperforms\n    the leading competitor by 3.4 dB in PSNR on an expanded relighting benchmark.\n\n    '\n  project_page: https://jingzhi.github.io/ReCap/\n  paper: https://arxiv.org/pdf/2412.07534.pdf\n  code: null\n  video: null\n  tags:\n  - Project\n  - Relight\n  thumbnail: assets/thumbnails/li2024recap.jpg\n  publication_date: '2024-12-10T14:15:32+00:00'\n  date_source: arxiv\n- id: tang2024mvdust3r\n  title: 'MV-DUSt3R+: Single-Stage Scene Reconstruction from Sparse Views In 2 Seconds'\n  authors: Zhenggang Tang, Yuchen Fan, Dilin Wang, Hongyu Xu, Rakesh Ranjan, Alexander\n    Schwing, Zhicheng Yan\n  year: '2024'\n  abstract: 'Recent sparse multi-view scene reconstruction advances like DUSt3R and\n    MASt3R no longer require camera calibration and camera pose estimation. However,\n    they only process a pair of views at a time to infer pixel-aligned pointmaps.\n    When dealing with more than two views, a combinatorial number of error prone pairwise\n    reconstructions are usually followed by an expensive global optimization, which\n    often fails to rectify the pairwise reconstruction errors. To handle more views,\n    reduce errors, and improve inference time, we propose the fast single-stage feed-forward\n    network MV-DUSt3R. At its core are multi-view decoder blocks which exchange information\n    across any number of views while considering one reference view. To make our method\n    robust to reference view selection, we further propose MV-DUSt3R+, which employs\n    cross-reference-view blocks to fuse information across different reference view\n    choices. To further enable novel view synthesis, we extend both by adding and\n    jointly training Gaussian splatting heads. Experiments on multi-view stereo reconstruction,\n    multi-view pose estimation, and novel view synthesis confirm that our methods\n    improve significantly upon prior art. Code will be released.\n\n    '\n  project_page: https://mv-dust3rp.github.io/\n  paper: https://arxiv.org/pdf/2412.06974.pdf\n  code: https://github.com/facebookresearch/mvdust3r\n  video: https://youtu.be/LBvnuKQ8Rso\n  tags:\n  - 3ster-based\n  - Code\n  - Project\n  - Sparse\n  - Video\n  thumbnail: assets/thumbnails/tang2024mvdust3r.jpg\n  publication_date: '2024-12-09T20:34:55+00:00'\n  date_source: arxiv\n- id: guédon2024matcha\n  title: 'MAtCha Gaussians: Atlas of Charts for High-Quality Geometry and Photorealism\n    From Sparse Views'\n  authors: Antoine Guédon, Tomoki Ichikawa, Kohei Yamashita, Ko Nishino\n  year: '2024'\n  abstract: 'We present a novel appearance model that simultaneously realizes explicit\n    high-quality 3D surface mesh recovery and photorealistic novel view synthesis\n    from sparse view samples. Our key idea is to model the underlying scene geometry\n    Mesh as an Atlas of Charts which we render with 2D Gaussian surfels (MAtCha Gaussians).\n    MAtCha distills high-frequency scene surface details from an off-the-shelf monocular\n    depth estimator and refines it through Gaussian surfel rendering. The Gaussian\n    surfels are attached to the charts on the fly, satisfying photorealism of neural\n    volumetric rendering and crisp geometry of a mesh model, i.e., two seemingly contradicting\n    goals in a single model. At the core of MAtCha lies a novel neural deformation\n    model and a structure loss that preserve the fine surface details distilled from\n    learned monocular depths while addressing their fundamental scale ambiguities.\n    Results of extensive experimental validation demonstrate MAtCha''s state-of-the-art\n    quality of surface reconstruction and photorealism on-par with top contenders\n    but with dramatic reduction in the number of input views and computational time.\n    We believe MAtCha will serve as a foundational tool for any visual application\n    in vision, graphics, and robotics that require explicit geometry in addition to\n    photorealism. Our project page is the following: https://anttwo.github.io/matcha/\n\n    '\n  project_page: https://anttwo.github.io/matcha/\n  paper: https://arxiv.org/pdf/2412.06767.pdf\n  code: null\n  video: null\n  tags:\n  - Meshing\n  - Project\n  - Sparse\n  thumbnail: assets/thumbnails/guédon2024matcha.jpg\n  publication_date: '2024-12-09T18:55:23+00:00'\n  date_source: arxiv\n- id: qiu2024advancing\n  title: 'Advancing Extended Reality with 3D Gaussian Splatting: Innovations and Prospects'\n  authors: Shi Qiu, Binzhu Xie, Qixuan Liu, Pheng-Ann Heng\n  year: '2024'\n  abstract: 3D Gaussian Splatting (3DGS) has attracted significant attention for its\n    potential to revolutionize 3D representation, rendering, and interaction. Despite\n    the rapid growth of 3DGS research, its direct application to Extended Reality\n    (XR) remains underexplored. Although many studies recognize the potential of 3DGS\n    for XR, few have explicitly focused on or demonstrated its effectiveness within\n    XR environments. In this paper, we aim to synthesize innovations in 3DGS that\n    show specific potential for advancing XR research and development. We conduct\n    a comprehensive review of publicly available 3DGS papers, with a focus on those\n    referencing XR-related concepts. Additionally, we perform an in-depth analysis\n    of innovations explicitly relevant to XR and propose a taxonomy to highlight their\n    significance. Building on these insights, we propose several prospective XR research\n    areas where 3DGS can make promising contributions, yet remain rarely touched.\n    By investigating the intersection of 3DGS and XR, this paper provides a roadmap\n    to push the boundaries of XR using cutting-edge 3DGS techniques.\n  project_page: null\n  paper: https://arxiv.org/pdf/2412.06257\n  code: null\n  video: null\n  tags:\n  - Review\n  thumbnail: assets/thumbnails/qiu2024advancing.jpg\n  publication_date: '2024-12-09T07:14:58+00:00'\n- id: joanna2024occams\n  title: 'Occam''s LGS: A Simple Approach for Language Gaussian Splatting'\n  authors: Jiahuan (Joanna) Cheng, Jan-Nico Zaech, Luc Van Gool, Danda Pani Paudel\n  year: '2024'\n  abstract: 'Gaussian Splatting is a widely adopted approach for 3D scene representation\n    that offers efficient, high-quality 3D reconstruction and rendering. A major reason\n    for the success of 3DGS is its simplicity of representing a scene with a set of\n    Gaussians, which makes it easy to interpret and adapt. To enhance scene understanding\n    beyond the visual representation, approaches have been developed that extend 3D\n    Gaussian Splatting with semantic vision-language features, especially allowing\n    for open-set tasks. In this setting, the language features of 3D Gaussian Splatting\n    are often aggregated from multiple 2D views. Existing works address this aggregation\n    problem using cumbersome techniques that lead to high computational cost and training\n    time. In this work, we show that the sophisticated techniques for language-grounded\n    3D Gaussian Splatting are simply unnecessary. Instead, we apply Occam''s razor\n    to the task at hand and perform weighted multi-view feature aggregation using\n    the weights derived from the standard rendering process, followed by a simple\n    heuristic-based noisy Gaussian filtration. Doing so offers us state-of-the-art\n    results with a speed-up of two orders of magnitude. We showcase our results in\n    two commonly used benchmark datasets: LERF and 3D-OVS. Our simple approach allows\n    us to perform reasoning directly in the language features, without any compression\n    whatsoever. Such modeling in turn offers easy scene manipulation, unlike the existing\n    methods -- which we illustrate using an application of object insertion in the\n    scene. Furthermore, we provide a thorough discussion regarding the significance\n    of our contributions within the context of the current literature.'\n  project_page: https://insait-institute.github.io/OccamLGS/\n  paper: https://arxiv.org/pdf/2412.01807\n  code: null\n  video: null\n  tags:\n  - Acceleration\n  - Language Embedding\n  - Project\n  - Segmentation\n  thumbnail: assets/thumbnails/joanna2024occams.jpg\n  publication_date: '2024-12-02T18:50:37+00:00'\n- id: li2024dynsup\n  title: 'DynSUP: Dynamic Gaussian Splatting from An Unposed Image Pair'\n  authors: Weihang Li, Weirong Chen, Shenhan Qian, Jiajie Chen, Daniel Cremers, Haoang\n    Li\n  year: '2024'\n  abstract: Recent advances in 3D Gaussian Splatting have shown promising results.\n    Existing methods typically assume static scenes and/or multiple images with prior\n    poses. Dynamics, sparse views, and unknown poses significantly increase the problem\n    complexity due to insufficient geometric constraints. To overcome this challenge,\n    we propose a method that can use only two images without prior poses to fit Gaussians\n    in dynamic environments. To achieve this, we introduce two technical contributions.\n    First, we propose an object-level two-view bundle adjustment. This strategy decomposes\n    dynamic scenes into piece-wise rigid components, and jointly estimates the camera\n    pose and motions of dynamic objects. Second, we design an SE(3) field-driven Gaussian\n    training method. It enables fine-grained motion modeling through learnable per-Gaussian\n    transformations. Our method leads to high-fidelity novel view synthesis of dynamic\n    scenes while accurately preserving temporal consistency and object motion. Experiments\n    on both synthetic and real-world datasets demonstrate that our method significantly\n    outperforms state-of-the-art approaches designed for the cases of static environments,\n    multiple images, and/or known poses. Our project page is available at https://colin-de.github.io/DynSUP/.\n  project_page: https://colin-de.github.io/DynSUP/\n  paper: https://arxiv.org/pdf/2412.00851\n  code: null\n  video: null\n  tags:\n  - Dynamic\n  - Poses\n  - Project\n  thumbnail: assets/thumbnails/li2024dynsup.jpg\n  publication_date: '2024-12-01T15:25:33+00:00'\n- id: pryadilshchikov2024t3dgs\n  title: 'T-3DGS: Removing Transient Objects for 3D Scene Reconstruction'\n  authors: Vadim Pryadilshchikov, Alexander Markin, Artem Komarichev, Ruslan Rakhimov,\n    Peter Wonka, Evgeny Burnaev\n  year: '2024'\n  abstract: We propose a novel framework to remove transient objects from input videos\n    for 3D scene reconstruction using Gaussian Splatting. Our framework consists of\n    the following steps. In the first step, we propose an unsupervised training strategy\n    for a classification network to distinguish between transient objects and static\n    scene parts based on their different training behavior inside the 3D Gaussian\n    Splatting reconstruction. In the second step, we improve the boundary quality\n    and stability of the detected transients by combining our results from the first\n    step with an off-the-shelf segmentation method. We also propose a simple and effective\n    strategy to track objects in the input video forward and backward in time. Our\n    results show an improvement over the current state of the art in existing sparsely\n    captured datasets and significant improvements in a newly proposed densely captured\n    (video) dataset.\n  project_page: https://transient-3dgs.github.io/\n  paper: https://arxiv.org/pdf/2412.00155\n  code: https://github.com/Vadim200116/T-3DGS\n  video: null\n  tags:\n  - Code\n  - Project\n  - Rendering\n  thumbnail: assets/thumbnails/pryadilshchikov2024t3dgs.jpg\n  publication_date: '2024-11-29T07:45:24+00:00'\n- id: wang2024gaussurf\n  title: 'GausSurf: Geometry-Guided 3D Gaussian Splatting for Surface Reconstruction'\n  authors: Jiepeng Wang, Yuan Liu, Peng Wang, Cheng Lin, Junhui Hou, Xin Li, Taku\n    Komura, Wenping Wang\n  year: '2024'\n  abstract: '3D Gaussian Splatting has achieved impressive performance in novel view\n    synthesis with real-time rendering capabilities. However, reconstructing high-quality\n    surfaces with fine details using 3D Gaussians remains a challenging task. In this\n    work, we introduce GausSurf, a novel approach to high-quality surface reconstruction\n    by employing geometry guidance from multi-view consistency in texture-rich areas\n    and normal priors in texture-less areas of a scene. We observe that a scene can\n    be mainly divided into two primary regions: 1) texture-rich and 2) texture-less\n    areas. To enforce multi-view consistency at texture-rich areas, we enhance the\n    reconstruction quality by incorporating a traditional patch-match based Multi-View\n    Stereo (MVS) approach to guide the geometry optimization in an iterative scheme.\n    This scheme allows for mutual reinforcement between the optimization of Gaussians\n    and patch-match refinement, which significantly improves the reconstruction results\n    and accelerates the training process. Meanwhile, for the texture-less areas, we\n    leverage normal priors from a pre-trained normal estimation model to guide optimization.\n    Extensive experiments on the DTU and Tanks and Temples datasets demonstrate that\n    our method surpasses state-of-the-art methods in terms of reconstruction quality\n    and computation time.'\n  project_page: https://jiepengwang.github.io/GausSurf/\n  paper: https://arxiv.org/pdf/2411.19454\n  code: null\n  video: null\n  tags:\n  - Meshing\n  - Project\n  thumbnail: assets/thumbnails/wang2024gaussurf.jpg\n  publication_date: '2024-11-29T03:54:54+00:00'\n- id: li2024sadg\n  title: 'SADG: Segment Any Dynamic Gaussians Without Object Trackers'\n  authors: Yun-Jin Li, Mariia Gladkova, Yan Xia, Daniel Cremers\n  year: '2024'\n  abstract: Understanding dynamic 3D scenes is fundamental for various applications,\n    including extended reality (XR) and autonomous driving. Effectively integrating\n    semantic information into 3D reconstruction enables holistic representation that\n    opens opportunities for immersive and interactive applications. We introduce SADG,\n    Segment Any Dynamic Gaussian Without Object Trackers, a novel approach that combines\n    dynamic Gaussian Splatting representation and semantic information without reliance\n    on object IDs. In contrast to existing works, we do not rely on supervision based\n    on object identities to enable consistent segmentation of dynamic 3D objects.\n    To this end, we propose to learn semantically-aware features by leveraging masks\n    generated from the Segment Anything Model (SAM) and utilizing our novel contrastive\n    learning objective based on hard pixel mining. The learned Gaussian features can\n    be effectively clustered without further post-processing. This enables fast computation\n    for further object-level editing, such as object removal, composition, and style\n    transfer by manipulating the Gaussians in the scene. We further extend several\n    dynamic novel-view datasets with segmentation benchmarks to enable testing of\n    learned feature fields from unseen viewpoints. We evaluate SADG on proposed benchmarks\n    and demonstrate the superior performance of our approach in segmenting objects\n    within dynamic scenes along with its effectiveness for further downstream editing\n    tasks.\n  project_page: https://yunjinli.github.io/project-sadg/\n  paper: https://arxiv.org/pdf/2411.19290\n  code: null\n  video: https://yunjinli.github.io/project-sadg/static/videos/split-cookie-2x4x.mp4\n  tags:\n  - Dynamic\n  - Project\n  - Segmentation\n  - Video\n  thumbnail: assets/thumbnails/li2024sadg.jpg\n  publication_date: '2024-11-28T17:47:48+00:00'\n- id: ren2024agsmesh\n  title: 'AGS-Mesh: Adaptive Gaussian Splatting and Meshing with Geometric Priors\n    for Indoor Room Reconstruction Using Smartphones'\n  authors: Xuqian Ren, Matias Turkulainen, Jiepeng Wang, Otto Seiskari, Iaroslav Melekhov,\n    Juho Kannala, Esa Rahtu\n  year: '2024'\n  abstract: Geometric priors are often used to enhance 3D reconstruction. With many\n    smartphones featuring low-resolution depth sensors and the prevalence of off-the-shelf\n    monocular geometry estimators, incorporating geometric priors as regularization\n    signals has become common in 3D vision tasks. However, the accuracy of depth estimates\n    from mobile devices is typically poor for highly detailed geometry, and monocular\n    estimators often suffer from poor multi-view consistency and precision. In this\n    work, we propose an approach for joint surface depth and normal refinement of\n    Gaussian Splatting methods for accurate 3D reconstruction of indoor scenes. We\n    develop supervision strategies that adaptively filters low-quality depth and normal\n    estimates by comparing the consistency of the priors during optimization. We mitigate\n    regularization in regions where prior estimates have high uncertainty or ambiguities.\n    Our filtering strategy and optimization design demonstrate significant improvements\n    in both mesh estimation and novel-view synthesis for both 3D and 2D Gaussian Splatting-based\n    methods on challenging indoor room datasets. Furthermore, we explore the use of\n    alternative meshing strategies for finer geometry extraction. We develop a scale-aware\n    meshing strategy inspired by TSDF and octree-based isosurface extraction, which\n    recovers finer details from Gaussian models compared to other commonly used open-source\n    meshing tools. Our code is released in https://xuqianren.github.io/ags_mesh_website/.\n  project_page: https://xuqianren.github.io/ags_mesh_website/\n  paper: https://arxiv.org/pdf/2411.19271\n  code: https://github.com/XuqianRen/AGS_Mesh\n  video: null\n  tags:\n  - Code\n  - Meshing\n  - Project\n  thumbnail: assets/thumbnails/ren2024agsmesh.jpg\n  publication_date: '2024-11-28T17:04:32+00:00'\n- id: wimmer2024gaussianstolife\n  title: 'Gaussians-to-Life: Text-Driven Animation of 3D Gaussian Splatting Scenes'\n  authors: Thomas Wimmer, Michael Oechsle, Michael Niemeyer, Federico Tombari\n  year: '2024'\n  abstract: 'State-of-the-art novel view synthesis methods achieve impressive results\n    for multi-view captures of static 3D scenes. However, the reconstructed scenes\n    still lack \"liveliness,\" a key component for creating engaging 3D experiences.\n    Recently, novel video diffusion models generate realistic videos with complex\n    motion and enable animations of 2D images, however they cannot naively be used\n    to animate 3D scenes as they lack multi-view consistency. To breathe life into\n    the static world, we propose Gaussians2Life, a method for animating parts of high-quality\n    3D scenes in a Gaussian Splatting representation. Our key idea is to leverage\n    powerful video diffusion models as the generative component of our model and to\n    combine these with a robust technique to lift 2D videos into meaningful 3D motion.\n    We find that, in contrast to prior work, this enables realistic animations of\n    complex, pre-existing 3D scenes and further enables the animation of a large variety\n    of object classes, while related work is mostly focused on prior-based character\n    animation, or single 3D objects. Our model enables the creation of consistent,\n    immersive 3D experiences for arbitrary scenes.\n\n    '\n  project_page: https://wimmerth.github.io/gaussians2life.html\n  paper: https://arxiv.org/pdf/2411.19233.pdf\n  code: https://github.com/wimmerth/gaussians2life\n  video: null\n  tags:\n  - Code\n  - Diffusion\n  - Dynamic\n  - Project\n  thumbnail: assets/thumbnails/wimmer2024gaussianstolife.jpg\n  publication_date: '2024-11-28T16:01:58+00:00'\n  date_source: arxiv\n- id: xu2024supergaussians\n  title: 'SuperGaussians: Enhancing Gaussian Splatting Using Primitives with Spatially\n    Varying Colors'\n  authors: Rui Xu, Wenyue Chen, Jiepeng Wang, Yuan Liu, Peng Wang, Lin Gao, Shiqing\n    Xin, Taku Komura, Xin Li, Wenping Wang\n  year: '2024'\n  abstract: Gaussian Splattings demonstrate impressive results in multi-view reconstruction\n    based on Gaussian explicit representations. However, the current Gaussian primitives\n    only have a single view-dependent color and an opacity to represent the appearance\n    and geometry of the scene, resulting in a non-compact representation. In this\n    paper, we introduce a new method called SuperGaussians that utilizes spatially\n    varying colors and opacity in a single Gaussian primitive to improve its representation\n    ability. We have implemented bilinear interpolation, movable kernels, and even\n    tiny neural networks as spatially varying functions. Quantitative and qualitative\n    experimental results demonstrate that all three functions outperform the baseline,\n    with the best movable kernels achieving superior novel view synthesis performance\n    on multiple datasets, highlighting the strong potential of spatially varying functions.\n  project_page: https://ruixu.me/html/SuperGaussians/index.html\n  paper: https://arxiv.org/pdf/2411.18966\n  code: https://github.com/Xrvitd/SuperGaussians\n  video: https://www.youtube.com/watch?v=K0liSjZGnIY&feature=youtu.be\n  tags:\n  - Code\n  - Project\n  - Rendering\n  - Video\n  thumbnail: assets/thumbnails/xu2024supergaussians.jpg\n  publication_date: '2024-11-28T07:36:22+00:00'\n- id: hess2024splatad\n  title: 'SplatAD: Real-Time Lidar and Camera Rendering with 3D Gaussian Splatting\n    for Autonomous Driving'\n  authors: Georg Hess, Carl Lindström, Maryam Fatemi, Christoffer Petersson, Lennart\n    Svensson\n  year: '2024'\n  abstract: Ensuring the safety of autonomous robots, such as self-driving vehicles,\n    requires extensive testing across diverse driving scenarios. Simulation is a key\n    ingredient for conducting such testing in a cost-effective and scalable way. Neural\n    rendering methods have gained popularity, as they can build simulation environments\n    from collected logs in a data-driven manner. However, existing neural radiance\n    field (NeRF) methods for sensor-realistic rendering of camera and lidar data suffer\n    from low rendering speeds, limiting their applicability for large-scale testing.\n    While 3D Gaussian Splatting (3DGS) enables real-time rendering, current methods\n    are limited to camera data and are unable to render lidar data essential for autonomous\n    driving. To address these limitations, we propose SplatAD, the first 3DGS-based\n    method for realistic, real-time rendering of dynamic scenes for both camera and\n    lidar data. SplatAD accurately models key sensor-specific phenomena such as rolling\n    shutter effects, lidar intensity, and lidar ray dropouts, using purpose-built\n    algorithms to optimize rendering efficiency. Evaluation across three autonomous\n    driving datasets demonstrates that SplatAD achieves state-of-the-art rendering\n    quality with up to +2 PSNR for NVS and +3 PSNR for reconstruction while increasing\n    rendering speed over NeRF-based methods by an order of magnitude.\n  project_page: https://research.zenseact.com/publications/splatad/\n  paper: https://arxiv.org/pdf/2411.16816\n  code: null\n  video: null\n  tags:\n  - Autonomous Driving\n  - Project\n  thumbnail: assets/thumbnails/hess2024splatad.jpg\n  publication_date: '2024-11-25T16:18:22+00:00'\n- id: zhang2024quadratic\n  title: Quadratic Gaussian Splatting for Efficient and Detailed Surface Reconstruction\n  authors: Ziyu Zhang, Binbin Huang, Hanqing Jiang, Liyang Zhou, Xiaojun Xiang, Shunhan\n    Shen\n  year: '2024'\n  abstract: 'Recently, 3D Gaussian Splatting (3DGS) has attracted attention for its\n    superior rendering quality and speed over Neural Radiance Fields (NeRF). To address\n    3DGS''s limitations in surface representation, 2D Gaussian Splatting (2DGS) introduced\n    disks as scene primitives to model and reconstruct geometries from multi-view\n    images, offering view-consistent geometry. However, the disk''s first-order linear\n    approximation often leads to over-smoothed results. We propose Quadratic Gaussian\n    Splatting (QGS), a novel method that replaces disks with quadric surfaces, enhancing\n    geometric fitting, whose code will be open-sourced. QGS defines Gaussian distributions\n    in non-Euclidean space, allowing primitives to capture more complex textures.\n    As a second-order surface approximation, QGS also renders spatial curvature to\n    guide the normal consistency term, to effectively reduce over-smoothing. Moreover,\n    QGS is a generalized version of 2DGS that achieves more accurate and detailed\n    reconstructions, as verified by experiments on DTU and TNT, demonstrating its\n    effectiveness in surpassing current state-of-the-art methods in geometry reconstruction.\n    Our code willbe released as open source.\n\n    '\n  project_page: https://quadraticgs.github.io/QGS/\n  paper: https://arxiv.org/pdf/2411.16392.pdf\n  code: https://github.com/QuadraticGS/QGS\n  video: https://quadraticgs.github.io/QGS/assets/QGS_web.mp4\n  tags:\n  - Code\n  - Meshing\n  - Project\n  - Video\n  thumbnail: assets/thumbnails/zhang2024quadratic.jpg\n  publication_date: '2024-11-25T13:55:00+00:00'\n  date_source: arxiv\n- id: blark2024splatsdf\n  title: 'SplatSDF: Boosting Neural Implicit SDF via Gaussian Splatting Fusion'\n  authors: Runfa Blark Li, Keito Suzuki, Bang Du, Ki Myung Brian Lee, Nikolay Atanasov,\n    Truong Nguyen\n  year: '2024'\n  abstract: A signed distance function (SDF) is a useful representation for continuous-space\n    geometry and many related operations, including rendering, collision checking,\n    and mesh generation. Hence, reconstructing SDF from image observations accurately\n    and efficiently is a fundamental problem. Recently, neural implicit SDF (SDF-NeRF)\n    techniques, trained using volumetric rendering, have gained a lot of attention.\n    Compared to earlier truncated SDF (TSDF) fusion algorithms that rely on depth\n    maps and voxelize continuous space, SDF-NeRF enables continuous-space SDF reconstruction\n    with better geometric and photometric accuracy. However, the accuracy and convergence\n    speed of scene-level SDF reconstruction require further improvements for many\n    applications. With the advent of 3D Gaussian Splatting (3DGS) as an explicit representation\n    with excellent rendering quality and speed, several works have focused on improving\n    SDF-NeRF by introducing consistency losses on depth and surface normals between\n    3DGS and SDF-NeRF. However, loss-level connections alone lead to incremental improvements.\n    We propose a novel neural implicit SDF called “SplatSDF” to fuse 3DGS and SDF-NeRF\n    at an architecture level with significant boosts to geometric and photometric\n    accuracy and convergence speed. Our SplatSDF relies on 3DGS as input only during\n    training, and keeps the same complexity and efficiency as the original SDF-NeRF\n    during inference. Our method outperforms state-of-the-art SDF-NeRF models on geometric\n    and photometric evaluation by the time of submission.\n  project_page: https://blarklee.github.io/splatsdf/\n  paper: https://arxiv.org/pdf/2411.15468.pdf\n  code: null\n  video: null\n  tags:\n  - Meshing\n  - Project\n  thumbnail: assets/thumbnails/blark2024splatsdf.jpg\n  publication_date: '2024-11-23T06:35:19+00:00'\n- id: kong2024dgsslam\n  title: 'DGS-SLAM: Gaussian Splatting SLAM in Dynamic Environment'\n  authors: Mangyu Kong, Jaewon Lee, Seongwon Lee, Euntai Kim\n  year: '2024'\n  abstract: We introduce Dynamic Gaussian Splatting SLAM (DGS-SLAM), the first dynamic\n    SLAM framework built on the foundation of Gaussian Splatting. While recent advancements\n    in dense SLAM have leveraged Gaussian Splatting to enhance scene representation,\n    most approaches assume a static environment, making them vulnerable to photometric\n    and geometric inconsistencies caused by dynamic objects. To address these challenges,\n    we integrate Gaussian Splatting SLAM with a robust filtering process to handle\n    dynamic objects throughout the entire pipeline, including Gaussian insertion and\n    keyframe selection. Within this framework, to further improve the accuracy of\n    dynamic object removal, we introduce a robust mask generation method that enforces\n    photometric consistency across keyframes, reducing noise from inaccurate segmentation\n    and artifacts such as shadows. Additionally, we propose the loop-aware window\n    selection mechanism, which utilizes unique keyframe IDs of 3D Gaussians to detect\n    loops between the current and past frames, facilitating joint optimization of\n    the current camera poses and the Gaussian map. DGS-SLAM achieves state-of-the-art\n    performance in both camera tracking and novel view synthesis on various dynamic\n    SLAM benchmarks, proving its effectiveness in handling real-world dynamic scenes.\n  project_page: null\n  paper: https://arxiv.org/pdf/2411.10722\n  code: null\n  video: https://youtu.be/Mq3qZTTcN3E?feature=shared\n  tags:\n  - SLAM\n  - Video\n  thumbnail: assets/thumbnails/kong2024dgsslam.jpg\n  publication_date: '2024-11-16T07:02:46+00:00'\n- id: svitov2024billboard\n  title: 'BillBoard Splatting (BBSplat): Learnable Textured Primitives for Novel View\n    Synthesis'\n  authors: David Svitov, Pietro Morerio, Lourdes Agapito, Alessio Del Bue\n  year: '2024'\n  abstract: 'We present billboard Splatting (BBSplat) - a novel approach for 3D scene\n    representation based on textured geometric primitives. BBSplat represents the\n    scene as a set of optimizable textured planar primitives with learnable RGB textures\n    and alpha-maps to control their shape. BBSplat primitives can be used in any Gaussian\n    Splatting pipeline as drop-in replacements for Gaussians. Our method''s qualitative\n    and quantitative improvements over 3D and 2D Gaussians are most noticeable when\n    fewer primitives are used, when BBSplat achieves over 1200 FPS. Our novel regularization\n    term encourages textures to have a sparser structure, unlocking an efficient compression\n    that leads to a reduction in storage space of the model. Our experiments show\n    the efficiency of BBSplat on standard datasets of real indoor and outdoor scenes\n    such as Tanks&Temples, DTU, and Mip-NeRF-360. We demonstrate improvements on PSNR,\n    SSIM, and LPIPS metrics compared to the state-of-the-art, especially for the case\n    when fewer primitives are used, which, on the other hand, leads to up to 2 times\n    inference speed improvement for the same rendering quality.\n\n    '\n  project_page: https://david-svitov.github.io/BBSplat_project_page/\n  paper: https://arxiv.org/pdf/2411.08508.pdf\n  code: https://github.com/david-svitov/BBSplat\n  video: https://youtu.be/uRM7WFo5vVg\n  tags:\n  - Code\n  - Optimization\n  - Project\n  - Texturing\n  - Video\n  thumbnail: assets/thumbnails/svitov2024billboard.jpg\n  publication_date: '2024-11-13T10:43:39+00:00'\n  date_source: arxiv\n- id: wang2024mbaslam\n  title: 'MBA-SLAM: Motion Blur Aware Dense Visual SLAM with Radiance Fields Representation'\n  authors: Peng Wang, Lingzhe Zhao, Yin Zhang, Shiyu Zhao, Peidong Liu\n  year: '2024'\n  abstract: Emerging 3D scene representations, such as Neural Radiance Fields (NeRF)\n    and 3D Gaussian Splatting (3DGS), have demonstrated their effectiveness in Simultaneous\n    Localization and Mapping (SLAM) for photo-realistic rendering, particularly when\n    using high-quality video sequences as input. However, existing methods struggle\n    with motion-blurred frames, which are common in real-world scenarios like low-light\n    or long-exposure conditions. This often results in a significant reduction in\n    both camera localization accuracy and map reconstruction quality. To address this\n    challenge, we propose a dense visual SLAM pipeline (i.e. MBA-SLAM) to handle severe\n    motion-blurred inputs. Our approach integrates an efficient motion blur-aware\n    tracker with either neural radiance fields or Gaussian Splatting based mapper.\n    By accurately modeling the physical image formation process of motion-blurred\n    images, our method simultaneously learns 3D scene representation and estimates\n    the cameras' local trajectory during exposure time, enabling proactive compensation\n    for motion blur caused by camera movement. In our experiments, we demonstrate\n    that MBA-SLAM surpasses previous state-of-the-art methods in both camera localization\n    and map reconstruction, showcasing superior performance across a range of datasets,\n    including synthetic and real datasets featuring sharp images as well as those\n    affected by motion blur, highlighting the versatility and robustness of our approach.\n  project_page: https://wangpeng000.github.io/MBA-SLAM/\n  paper: https://arxiv.org/pdf/2411.08279\n  code: null\n  video: null\n  tags:\n  - Project\n  - SLAM\n  thumbnail: assets/thumbnails/wang2024mbaslam.jpg\n  publication_date: '2024-11-13T01:38:06+00:00'\n- id: lu20243dgscd\n  title: '3DGS-CD: 3D Gaussian Splatting-based Change Detection for Physical Object\n    Rearrangement'\n  authors: Ziqi Lu, Jianbo Ye, John Leonard\n  year: '2024'\n  abstract: 'We present 3DGS-CD, the first 3D Gaussian Splatting (3DGS)-based method\n    for detecting physical object rearrangements in 3D scenes. Our approach estimates\n    3D object-level changes by comparing two sets of unaligned images taken at different\n    times. Leveraging 3DGS''s novel view rendering and EfficientSAM''s zero-shot segmentation\n    capabilities, we detect 2D object-level changes, which are then associated and\n    fused across views to estimate 3D changes. Our method can detect changes in cluttered\n    environments using sparse post-change images within as little as 18s, using as\n    few as a single new image. It does not rely on depth input, user instructions,\n    object classes, or object models -- An object is recognized simply if it has been\n    re-arranged. Our approach is evaluated on both public and self-collected real-world\n    datasets, achieving up to 14% higher accuracy and three orders of magnitude faster\n    performance compared to the state-of-the-art radiance-field-based change detection\n    method. This significant performance boost enables a broad range of downstream\n    applications, where we highlight three key use cases: object reconstruction, robot\n    workspace reset, and 3DGS model update. Our code and data will be made available\n    at https://github.com/520xyxyzq/3DGS-CD.'\n  project_page: null\n  paper: https://arxiv.org/pdf/2411.03706\n  code: null\n  video: null\n  tags:\n  - Robotics\n  thumbnail: assets/thumbnails/lu20243dgscd.jpg\n  publication_date: '2024-11-06T07:08:41+00:00'\n- id: liu2024citygaussianv2\n  title: 'CityGaussianV2: Efficient and Geometrically Accurate Reconstruction for\n    Large-Scale Scenes'\n  authors: Yang Liu, Chuanchen Luo, Zhongkai Mao, Junran Peng, Zhaoxiang Zhang\n  year: '2024'\n  abstract: Recently, 3D Gaussian Splatting (3DGS) has revolutionized radiance field\n    reconstruction, manifesting efficient and high-fidelity novel view synthesis.\n    However, accurately representing surfaces, especially in large and complex scenarios,\n    remains a significant challenge due to the unstructured nature of 3DGS. In this\n    paper, we present CityGaussianV2, a novel approach for large-scale scene reconstruction\n    that addresses critical challenges related to geometric accuracy and efficiency.\n    Building on the favorable generalization capabilities of 2D Gaussian Splatting\n    (2DGS), we address its convergence and scalability issues. Specifically, we implement\n    a decomposed-gradient-based densification and depth regression technique to eliminate\n    blurry artifacts and accelerate convergence. To scale up, we introduce an elongation\n    filter that mitigates Gaussian count explosion caused by 2DGS degeneration. Furthermore,\n    we optimize the CityGaussian pipeline for parallel training, achieving up to 10x\n    compression, at least 25% savings in training time, and a 50% decrease in memory\n    usage. We also established standard geometry benchmarks under large-scale scenes.\n    Experimental results demonstrate that our method strikes a promising balance between\n    visual quality, geometric accuracy, as well as storage and training costs.\n  project_page: https://dekuliutesla.github.io/CityGaussianV2/\n  paper: https://arxiv.org/pdf/2411.00771\n  code: https://github.com/DekuLiuTesla/CityGaussian\n  video: null\n  tags:\n  - Code\n  - Large-Scale\n  - Meshing\n  - Project\n  thumbnail: assets/thumbnails/liu2024citygaussianv2.jpg\n  publication_date: '2024-11-01T17:59:31+00:00'\n- id: zubair2024neural\n  title: 'Neural Fields in Robotics: A Survey'\n  authors: Muhammad Zubair Irshad, Mauro Comi, Yen-Chen Lin, Nick Heppert, Abhinav\n    Valada, Zsolt Kira, Rares Ambrus, Johnathan Trembley\n  year: '2024'\n  abstract: 'Neural Fields have emerged as a transformative approach for 3D scene\n    representation in computer vision and robotics, enabling accurate inference of\n    geometry, 3D semantics, and dynamics from posed 2D data. Leveraging differentiable\n    rendering, Neural Fields encompass both continuous implicit and explicit neural\n    representations enabling high-fidelity 3D reconstruction, integration of multi-modal\n    sensor data, and generation of novel viewpoints. This survey explores their applications\n    in robotics, emphasizing their potential to enhance perception, planning, and\n    control. Their compactness, memory efficiency, and differentiability, along with\n    seamless integration with foundation and generative models, make them ideal for\n    real-time applications, improving robot adaptability and decision-making. This\n    paper provides a thorough review of Neural Fields in robotics, categorizing applications\n    across various domains and evaluating their strengths and limitations, based on\n    over 200 papers. First, we present four key Neural Fields frameworks: Occupancy\n    Networks, Signed Distance Fields, Neural Radiance Fields, and Gaussian Splatting.\n    Second, we detail Neural Fields'' applications in five major robotics domains:\n    pose estimation, manipulation, navigation, physics, and autonomous driving, highlighting\n    key works and discussing takeaways and open challenges. Finally, we outline the\n    current limitations of Neural Fields in robotics and propose promising directions\n    for future research.'\n  project_page: null\n  paper: https://arxiv.org/pdf/2410.20220\n  code: null\n  video: null\n  tags:\n  - Review\n  - Robotics\n  thumbnail: assets/thumbnails/zubair2024neural.jpg\n  publication_date: '2024-10-26T16:26:41+00:00'\n- id: fan2024large\n  title: 'Large Spatial Model: End-to-end Unposed Images to Semantic 3D'\n  authors: Zhiwen Fan, Jian Zhang, Wenyan Cong, Peihao Wang, Renjie Li, Kairun Wen,\n    Shijie Zhou, Achuta Kadambi, Zhangyang Wang, Danfei Xu, Boris Ivanovic, Marco\n    Pavone, Yue Wang\n  year: '2024'\n  abstract: 'Reconstructing and understanding 3D structures from a limited number\n    of images is a well-established problem in computer vision. Traditional methods\n    usually break this task into multiple subtasks, each requiring complex transformations\n    between different data representations. For instance, dense reconstruction through\n    Structure-from-Motion (SfM) involves converting images into key points, optimizing\n    camera parameters, and estimating structures. Afterward, accurate sparse reconstructions\n    are required for further dense modeling, which is subsequently fed into task-specific\n    neural networks. This multi-step process results in considerable processing time\n    and increased engineering complexity. In this work, we present the Large Spatial\n    Model (LSM), which processes unposed RGB images directly into semantic radiance\n    fields. LSM simultaneously estimates geometry, appearance, and semantics in a\n    single feed-forward operation, and it can generate versatile label maps by interacting\n    with language at novel viewpoints. Leveraging a Transformer-based architecture,\n    LSM integrates global geometry through pixel-aligned point maps. To enhance spatial\n    attribute regression, we incorporate local context aggregation with multi-scale\n    fusion, improving the accuracy of fine local details. To tackle the scarcity of\n    labeled 3D semantic data and enable natural language-driven scene manipulation,\n    we incorporate a pre-trained 2D language-based segmentation model into a 3D-consistent\n    semantic feature field. An efficient decoder then parameterizes a set of semantic\n    anisotropic Gaussians, facilitating supervised end-to-end learning. Extensive\n    experiments across various tasks show that LSM unifies multiple 3D vision tasks\n    directly from unposed images, achieving real-time semantic 3D reconstruction for\n    the first time.\n\n    '\n  project_page: https://largespatialmodel.github.io/\n  paper: https://arxiv.org/pdf/2410.18956.pdf\n  code: https://github.com/NVlabs/LSM\n  video: https://largespatialmodel.github.io/static/videos/LSM_demo1.mp4\n  tags:\n  - Code\n  - Feed-Forward\n  - Poses\n  - Project\n  - Segmentation\n  - Video\n  thumbnail: assets/thumbnails/fan2024large.jpg\n  publication_date: '2024-10-24T17:54:42+00:00'\n  date_source: arxiv\n- id: lee2024fully\n  title: Fully Explicit Dynamic Gaussian Splatting\n  authors: Junoh Lee, Changyeon Won, HyunJun Jung, Inhwan Bae, Hae-Gon Jeon\n  year: '2024'\n  abstract: 3D Gaussian Splatting has shown fast and high-quality rendering results\n    in static scenes by leveraging dense 3D prior and explicit representations. Unfortunately,\n    the benefits of the prior and representation do not involve novel view synthesis\n    for dynamic motions. Ironically, this is because the main barrier is the reliance\n    on them, which requires increasing training and rendering times to account for\n    dynamic motions. In this paper, we design a \\Edited{Explicit 4D Gaussian Splatting(Ex4DGS)}.\n    Our key idea is to firstly separate static and dynamic Gaussians during training,\n    and to explicitly sample positions and rotations of the dynamic Gaussians at sparse\n    timestamps. The sampled positions and rotations are then interpolated to represent\n    both spatially and temporally continuous motions of objects in dynamic scenes\n    as well as reducing computational cost. Additionally, we introduce a progressive\n    training scheme and a point-backtracking technique that improves Ex4DGS's convergence.\n    We initially train Ex4DGS using short timestamps and progressively extend timestamps,\n    which makes it work well with a few point clouds. The point-backtracking is used\n    to quantify the cumulative error of each Gaussian over time, enabling the detection\n    and removal of erroneous Gaussians in dynamic scenes. Comprehensive experiments\n    on various scenes demonstrate the state-of-the-art rendering quality from our\n    method, achieving fast rendering of 62 fps on a single 2080Ti GPU.\n  project_page: https://leejunoh.com/Ex4DGS/\n  paper: https://arxiv.org/pdf/2410.15629\n  code: https://github.com/juno181/Ex4DGS\n  video: null\n  tags:\n  - Code\n  - Dynamic\n  - Project\n  thumbnail: assets/thumbnails/lee2024fully.jpg\n  publication_date: '2024-10-21T04:25:43+00:00'\n- id: hahlbohm2024efficient\n  title: Efficient Perspective-Correct 3D Gaussian Splatting Using Hybrid Transparency\n  authors: Florian Hahlbohm, Fabian Friederichs, Tim Weyrich, Linus Franke, Moritz\n    Kappel, Susana Castillo, Marc Stamminger, Martin Eisemann, Marcus Magnor\n  year: '2024'\n  abstract: '3D Gaussian Splats (3DGS) have proven a versatile rendering primitive,\n    both for inverse rendering as well as real-time exploration of scenes. In these\n    applications, coherence across camera frames and multiple views is crucial, be\n    it for robust convergence of a scene reconstruction or for artifact-free fly-throughs.\n    Recent work started mitigating artifacts that break multi-view coherence, including\n    popping artifacts due to inconsistent transparency sorting and perspective-correct\n    outlines of (2D) splats. At the same time, real-time requirements forced such\n    implementations to accept compromises in how transparency of large assemblies\n    of 3D Gaussians is resolved, in turn breaking coherence in other ways. In our\n    work, we aim at achieving maximum coherence, by rendering fully perspective-correct\n    3D Gaussians while using a high-quality approximation of accurate blending, hybrid\n    transparency, on a per-pixel level, in order to retain real-time frame rates.\n    Our fast and perspectively accurate approach for evaluation of 3D Gaussians does\n    not require matrix inversions, thereby ensuring numerical stability and eliminating\n    the need for special handling of degenerate splats, and the hybrid transparency\n    formulation for blending maintains similar quality as fully resolved per-pixel\n    transparencies at a fraction of the rendering costs. We further show that each\n    of these two components can be independently integrated into Gaussian splatting\n    systems. In combination, they achieve up to 2$\\times$ higher frame rates, 2$\\times$\n    faster optimization, and equal or better image quality with fewer rendering artifacts\n    compared to traditional 3DGS on common benchmarks.\n\n    '\n  project_page: https://fhahlbohm.github.io/htgs/\n  paper: https://arxiv.org/pdf/2410.08129.pdf\n  code: null\n  video: https://fhahlbohm.github.io/htgs/assets/htgs_twitter_teaser.mp4\n  tags:\n  - Perspective-correct\n  - Project\n  - Video\n  thumbnail: assets/thumbnails/hahlbohm2024efficient.jpg\n  publication_date: '2024-10-10T17:14:16+00:00'\n  date_source: arxiv\n- id: chu2024generalizable\n  title: Generalizable and Animatable Gaussian Head Avatar\n  authors: Xuangeng Chu, Tatsuya Harada\n  year: '2024'\n  abstract: In this paper, we propose Generalizable and Animatable Gaussian head Avatar\n    (GAGAvatar) for one-shot animatable head avatar reconstruction. Existing methods\n    rely on neural radiance fields, leading to heavy rendering consumption and low\n    reenactment speeds. To address these limitations, we generate the parameters of\n    3D Gaussians from a single image in a single forward pass. The key innovation\n    of our work is the proposed dual-lifting method, which produces high-fidelity\n    3D Gaussians that capture identity and facial details. Additionally, we leverage\n    global image features and the 3D morphable model to construct 3D Gaussians for\n    controlling expressions. After training, our model can reconstruct unseen identities\n    without specific optimizations and perform reenactment rendering at real-time\n    speeds. Experiments show that our method exhibits superior performance compared\n    to previous methods in terms of reconstruction quality and expression accuracy.\n    We believe our method can establish new benchmarks for future research and advance\n    applications of digital avatars.\n  project_page: https://xg-chu.site/project_gagavatar/\n  paper: https://arxiv.org/pdf/2410.07971\n  code: https://github.com/xg-chu/GAGAvatar\n  video: https://youtu.be/9244ZgOl4Xk\n  tags:\n  - Avatar\n  - Code\n  - Project\n  - Video\n  thumbnail: assets/thumbnails/chu2024generalizable.jpg\n  publication_date: '2024-10-10T14:29:00+00:00'\n- id: zhu2024motiongs\n  title: 'MotionGS: Exploring Explicit Motion Guidance for Deformable 3D Gaussian\n    Splatting'\n  authors: Ruijie Zhu*, Yanzhe Liang*, Hanzhi Chang, Jiacheng Deng, Jiahao Lu, Wenfei\n    Yang, Tianzhu Zhang, Yongdong Zhang\n  year: '2024'\n  abstract: Dynamic scene reconstruction is a long-term challenge in the field of\n    3D vision. Recently, the emergence of 3D Gaussian Splatting has provided new insights\n    into this problem. Although subsequent efforts rapidly extend static 3D Gaussian\n    to dynamic scenes, they often lack explicit constraints on object motion, leading\n    to optimization difficulties and performance degradation. To address the above\n    issues, we propose a novel deformable 3D Gaussian splatting framework called MotionGS,\n    which explores explicit motion priors to guide the deformation of 3D Gaussians.\n    Specifically, we first introduce an optical flow decoupling module that decouples\n    optical flow into camera flow and motion flow, corresponding to camera movement\n    and object motion respectively. Then the motion flow can effectively constrain\n    the deformation of 3D Gaussians, thus simulating the motion of dynamic objects.\n    Additionally, a camera pose refinement module is proposed to alternately optimize\n    3D Gaussians and camera poses, mitigating the impact of inaccurate camera poses.\n    Extensive experiments in the monocular dynamic scenes validate that MotionGS surpasses\n    state-of-the-art methods and exhibits significant superiority in both qualitative\n    and quantitative results.\n  project_page: https://ruijiezhu94.github.io/MotionGS_page\n  paper: https://arxiv.org/pdf/2410.07707\n  code: null\n  video: https://youtu.be/25DgViuuKFI\n  tags:\n  - Dynamic\n  - Project\n  - Video\n  thumbnail: assets/thumbnails/zhu2024motiongs.jpg\n  publication_date: '2024-10-10T08:19:47+00:00'\n- id: tang2024hisplat\n  title: 'HiSplat: Hierarchical 3D Gaussian Splatting for Generalizable Sparse-View\n    Reconstruction'\n  authors: Shengji Tang, Weicai Ye, Peng Ye, Weihao Lin, Yang Zhou, Tao Chen, Wanli\n    Ouyang\n  year: '2024'\n  abstract: 'Reconstructing 3D scenes from multiple viewpoints is a fundamental task\n    in stereo vision. Recently, advances in generalizable 3D Gaussian Splatting have\n    enabled high-quality novel view synthesis for unseen scenes from sparse input\n    views by feed-forward predicting per-pixel Gaussian parameters without extra optimization.\n    However, existing methods typically generate single-scale 3D Gaussians, which\n    lack representation of both large-scale structure and texture details, resulting\n    in mislocation and artefacts. In this paper, we propose a novel framework, HiSplat,\n    which introduces a hierarchical manner in generalizable 3D Gaussian Splatting\n    to construct hierarchical 3D Gaussians via a coarse-to-fine strategy. Specifically,\n    HiSplat generates large coarse-grained Gaussians to capture large-scale structures,\n    followed by fine-grained Gaussians to enhance delicate texture details. To promote\n    inter-scale interactions, we propose an Error Aware Module for Gaussian compensation\n    and a Modulating Fusion Module for Gaussian repair. Our method achieves joint\n    optimization of hierarchical representations, allowing for novel view synthesis\n    using only two-view reference images. Comprehensive experiments on various datasets\n    demonstrate that HiSplat significantly enhances reconstruction quality and cross-dataset\n    generalization compared to prior single-scale methods. The corresponding ablation\n    study and analysis of different-scale 3D Gaussians reveal the mechanism behind\n    the effectiveness. Project website: https://open3dvlab.github.io/HiSplat/\n\n    '\n  project_page: https://open3dvlab.github.io/HiSplat/\n  paper: https://arxiv.org/pdf/2410.06245.pdf\n  code: https://github.com/Open3DVLab/HiSplat\n  video: null\n  tags:\n  - Code\n  - Feed-Forward\n  - Project\n  - Sparse\n  thumbnail: assets/thumbnails/tang2024hisplat.jpg\n  publication_date: '2024-10-08T17:59:32+00:00'\n  date_source: arxiv\n- id: liu2024splatraj\n  title: 'SplaTraj: Camera Trajectory Generation with Semantic Gaussian Splatting'\n  authors: Xinyi Liu, Tianyi Zhang, Matthew Johnson-Roberson, Weiming Zhi\n  year: '2024'\n  abstract: Many recent developments for robots to represent environments have focused\n    on photorealistic reconstructions. This paper particularly focuses on generating\n    sequences of images from the photorealistic Gaussian Splatting models, that match\n    instructions that are given by user-inputted language. We contribute a novel framework,\n    SplaTraj, which formulates the generation of images within photorealistic environment\n    representations as a continuous-time trajectory optimization problem. Costs are\n    designed so that a camera following the trajectory poses will smoothly traverse\n    through the environment and render the specified spatial information in a photogenic\n    manner. This is achieved by querying a photorealistic representation with language\n    embedding to isolate regions that correspond to the user-specified inputs. These\n    regions are then projected to the camera's view as it moves over time and a cost\n    is constructed. We can then apply gradient-based optimization and differentiate\n    through the rendering to optimize the trajectory for the defined cost. The resulting\n    trajectory moves to photogenically view each of the specified objects. We empirically\n    evaluate our approach on a suite of environments and instructions, and demonstrate\n    the quality of generated image sequences.\n  project_page: null\n  paper: https://arxiv.org/pdf/2410.06014\n  code: null\n  video: https://youtu.be/PUXNBfpeZkg\n  tags:\n  - Language Embedding\n  - Rendering\n  - Video\n  thumbnail: assets/thumbnails/liu2024splatraj.jpg\n  publication_date: '2024-10-08T13:16:49+00:00'\n- id: zhang2024monst3r\n  title: 'MonST3R: A Simple Approach for Estimating Geometry in the Presence of Motion'\n  authors: Junyi Zhang, Charles Herrmann, Junhwa Hur, Varun Jampani, Trevor Darrell,\n    Forrester Cole, Deqing Sun, Ming-Hsuan Yang\n  year: '2024'\n  abstract: 'Estimating geometry from dynamic scenes, where objects move and deform\n    over time, remains a core challenge in computer vision. Current approaches often\n    rely on multi-stage pipelines or global optimizations that decompose the problem\n    into subtasks, like depth and flow, leading to complex systems prone to errors.\n    In this paper, we present Motion DUSt3R (MonST3R), a novel geometry-first approach\n    that directly estimates per-timestep geometry from dynamic scenes. Our key insight\n    is that by simply estimating a pointmap for each timestep, we can effectively\n    adapt DUST3R''s representation, previously only used for static scenes, to dynamic\n    scenes. However, this approach presents a significant challenge: the scarcity\n    of suitable training data, namely dynamic, posed videos with depth labels. Despite\n    this, we show that by posing the problem as a fine-tuning task, identifying several\n    suitable datasets, and strategically training the model on this limited data,\n    we can surprisingly enable the model to handle dynamics, even without an explicit\n    motion representation. Based on this, we introduce new optimizations for several\n    downstream video-specific tasks and demonstrate strong performance on video depth\n    and camera pose estimation, outperforming prior work in terms of robustness and\n    efficiency. Moreover, MonST3R shows promising results for primarily feed-forward\n    4D reconstruction.\n\n    '\n  project_page: https://monst3r-project.github.io/\n  paper: https://arxiv.org/pdf/2410.03825.pdf\n  code: https://github.com/Junyi42/monst3r\n  video: https://monst3r-project.github.io/files/teaser_vid_v2_lowres.mp4\n  tags:\n  - 3ster-based\n  - Code\n  - Dynamic\n  - Project\n  - Sparse\n  - Video\n  thumbnail: assets/thumbnails/zhang2024monst3r.jpg\n  publication_date: '2024-10-04T18:00:07+00:00'\n  date_source: arxiv\n- id: cao20243dgsdet\n  title: '3DGS-DET: Empower 3D Gaussian Splatting with Boundary Guidance and Box-Focused\n    Sampling for 3D Object Detection'\n  authors: Yang Cao, Yuanliang Jv, Dan Xu\n  year: '2024'\n  abstract: 'Neural Radiance Fields (NeRF) are widely used for novel-view synthesis\n    and have been adapted for 3D Object Detection (3DOD), offering a promising approach\n    to 3D object detection through view-synthesis representation. However, NeRF faces\n    inherent limitations: (i) It has limited representational capacity for 3DOD due\n    to its implicit nature, and (ii) it suffers from slow rendering speeds. Recently,\n    3D Gaussian Splatting (3DGS) has emerged as an explicit 3D representation that\n    addresses these limitations with faster rendering capabilities. Inspired by these\n    advantages, this paper introduces 3DGS into 3DOD for the first time, identifying\n    two main challenges: (i) Ambiguous spatial distribution of Gaussian blobs – 3DGS\n    primarily relies on 2D pixel-level supervision, resulting in unclear 3D spatial\n    distribution of Gaussian blobs and poor differentiation between objects and background,\n    which hinders 3DOD; (ii) Excessive background blobs – 2D images often include\n    numerous background pixels, leading to densely reconstructed 3DGS with many noisy\n    Gaussian blobs representing the background, negatively affecting detection. To\n    tackle the challenge (i), we leverage the fact that 3DGS reconstruction is derived\n    from 2D images, and propose an elegant and efficient solution by incorporating\n    2D Boundary Guidance to significantly enhance the spatial distribution of Gaussian\n    blobs, resulting in clearer differentiation between objects and their background\n    (see Fig. 1). To address the challenge (ii), we propose a Box-Focused Sampling\n    strategy using 2D boxes to generate object probability distribution in 3D spaces,\n    allowing effective probabilistic sampling in 3D to retain more object blobs and\n    reduce noisy background blobs. Benefiting from the proposed Boundary Guidance\n    and Box-Focused Sampling, our final method, 3DGS-DET, achieves significant improvements\n    (+5.6 on mAP@0.25, +3.7 on mAP@0.5) over our basic pipeline version, without introducing\n    any additional learnable parameters. Furthermore, 3DGS-DET significantly outperforms\n    the state-of-the-art NeRF-based method, NeRF-Det, achieving improvements of +6.6\n    on mAP@0.25 and +8.1 on mAP@0.5 for the ScanNet dataset, and impressive +31.5\n    on mAP@0.25 for the ARKITScenes dataset. Codes and models are publicly available\n    at: https://github.com/yangcaoai/3DGS-DET.'\n  project_page: null\n  paper: https://arxiv.org/pdf/2410.01647\n  code: null\n  video: null\n  tags:\n  - Object Detection\n  thumbnail: assets/thumbnails/cao20243dgsdet.jpg\n  publication_date: '2024-10-02T15:15:52+00:00'\n- id: duisterhof2024mast3rsfm\n  title: 'MASt3R-SfM: a Fully-Integrated Solution for Unconstrained Structure-from-Motion'\n  authors: Bardienus Duisterhof, Lojze Zust, Philippe Weinzaepfel, Vincent Leroy,\n    Yohann Cabon, Jerome Revaud\n  year: '2024'\n  abstract: 'Structure-from-Motion (SfM), a task aiming at jointly recovering camera\n    poses and 3D geometry of a scene given a set of images, remains a hard problem\n    with still many open challenges despite decades of significant progress. The traditional\n    solution for SfM consists of a complex pipeline of minimal solvers which tends\n    to propagate errors and fails when images do not sufficiently overlap, have too\n    little motion, etc. Recent methods have attempted to revisit this paradigm, but\n    we empirically show that they fall short of fixing these core issues. In this\n    paper, we propose instead to build upon a recently released foundation model for\n    3D vision that can robustly produce local 3D reconstructions and accurate matches.\n    We introduce a low-memory approach to accurately align these local reconstructions\n    in a global coordinate system. We further show that such foundation models can\n    serve as efficient image retrievers without any overhead, reducing the overall\n    complexity from quadratic to linear. Overall, our novel SfM pipeline is simple,\n    scalable, fast and truly unconstrained, i.e. it can handle any collection of images,\n    ordered or not. Extensive experiments on multiple benchmarks show that our method\n    provides steady performance across diverse settings, especially outperforming\n    existing methods in small- and medium-scale settings.\n\n    '\n  project_page: null\n  paper: https://arxiv.org/pdf/2409.19152.pdf\n  code: https://github.com/naver/mast3r\n  video: null\n  tags:\n  - Code\n  thumbnail: assets/thumbnails/duisterhof2024mast3rsfm.jpg\n  publication_date: '2024-09-27T21:29:58+00:00'\n  date_source: arxiv\n- id: yu2024languageembedded\n  title: 'Language-Embedded Gaussian Splats (LEGS): Incrementally Building Room-Scale\n    Representations with a Mobile Robot'\n  authors: Justin Yu, Kush Hari, Kishore Srinivas, Karim El-Refai, Adam Rashid, Chung\n    Min Kim, Justin Kerr, Richard Cheng, Muhammad Zubair Irshad, Ashwin Balakrishna,\n    Thomas Kollar, Ken Goldberg\n  year: '2024'\n  abstract: 'Building semantic 3D maps is valuable for searching for objects of interest\n    in offices, warehouses, stores, and homes. We present a mapping system that incrementally\n    builds a Language-Embedded Gaussian Splat (LEGS): a detailed 3D scene representation\n    that encodes both appearance and semantics in a unified representation. LEGS is\n    trained online as a robot traverses its environment to enable localization of\n    open-vocabulary object queries. We evaluate LEGS on 4 room-scale scenes where\n    we query for objects in the scene to assess how LEGS can capture semantic meaning.\n    We compare LEGS to LERF and find that while both systems have comparable object\n    query success rates, LEGS trains over 3.5x faster than LERF. Results suggest that\n    a multi-camera setup and incremental bundle adjustment can boost visual reconstruction\n    quality in constrained robot trajectories, and suggest LEGS can localize open-vocabulary\n    and long-tail object queries with up to 66% accuracy.'\n  project_page: https://berkeleyautomation.github.io/LEGS/\n  paper: https://arxiv.org/pdf/2409.18108\n  code: https://github.com/BerkeleyAutomation/L3GS\n  video: null\n  tags:\n  - Code\n  - Language Embedding\n  - Project\n  - Robotics\n  - Segmentation\n  thumbnail: assets/thumbnails/yu2024languageembedded.jpg\n  publication_date: '2024-09-26T17:51:31+00:00'\n- id: wang2024v3\n  title: 'V^3: Viewing Volumetric Videos on Mobiles via Streamable 2D Dynamic Gaussians'\n  authors: Penghao Wang, Zhirui Zhang, Liao Wang, Kaixin Yao, Siyuan Xie, Jingyi Yu,\n    Minye Wu, Lan Xu\n  year: '2024'\n  abstract: Experiencing high-fidelity volumetric video as seamlessly as 2D videos\n    is a long-held dream. However, current dynamic 3DGS methods, despite their high\n    rendering quality, face challenges in streaming on mobile devices due to computational\n    and bandwidth constraints. In this paper, we introduce V^3 (Viewing Volumetric\n    Videos), a novel approach that enables high-quality mobile rendering through the\n    streaming of dynamic Gaussians. Our key innovation is to view dynamic 3DGS as\n    2D videos, facilitating the use of hardware video codecs. Additionally, we propose\n    a two-stage training strategy to reduce storage requirements with rapid training\n    speed. The first stage employs hash encoding and shallow MLP to learn motion,\n    then reduces the number of Gaussians through pruning to meet the streaming requirements,\n    while the second stage fine tunes other Gaussian attributes using residual entropy\n    loss and temporal loss to improve temporal continuity. This strategy, which disentangles\n    motion and appearance, maintains high rendering quality with compact storage requirements.\n    Meanwhile, we designed a multi-platform player to decode and render 2D Gaussian\n    videos. Extensive experiments demonstrate the effectiveness of V^3, outperforming\n    other methods by enabling high-quality rendering and streaming on common devices,\n    which is unseen before. As the first to stream dynamic Gaussians on mobile devices,\n    our companion player offers users an unprecedented volumetric video experience,\n    including smooth scrolling and instant sharing. Our project page with source code\n    is available at this https URL.\n  project_page: https://authoritywang.github.io/v3/\n  paper: https://arxiv.org/pdf/2409.13648\n  code: https://github.com/AuthorityWang/VideoGS\n  video: https://youtu.be/Z5La9AporRU?si=iJ-m_mvUSxQN4Bwm\n  tags:\n  - Avatar\n  - Code\n  - Project\n  - Video\n  thumbnail: assets/thumbnails/wang2024v3.jpg\n  publication_date: '2024-09-20T16:54:27+00:00'\n- id: chelani2024edgegaussians\n  title: EdgeGaussians -- 3D Edge Mapping via Gaussian Splatting\n  authors: Kunal Chelani, Assia Benbihi, Torsten Sattler, Fredrik Kahl\n  year: '2024'\n  abstract: 'With their meaningful geometry and their omnipresence in the 3D world,\n    edges are extremely useful primitives in computer vision. 3D edges comprise of\n    lines and curves, and methods to reconstruct them use either multi-view images\n    or point clouds as input. State-of-the-art image-based methods first learn a 3D\n    edge point cloud then fit 3D edges to it. The edge point cloud is obtained by\n    learning a 3D neural implicit edge field from which the 3D edge points are sampled\n    on a specific level set (0 or 1). However, such methods present two important\n    drawbacks: i) it is not realistic to sample points on exact level sets due to\n    float imprecision and training inaccuracies. Instead, they are sampled within\n    a range of levels so the points do not lie accurately on the 3D edges and require\n    further processing. ii) Such implicit representations are computationally expensive\n    and require long training times. In this paper, we address these two limitations\n    and propose a 3D edge mapping that is simpler, more efficient, and preserves accuracy.\n    Our method learns explicitly the 3D edge points and their edge direction hence\n    bypassing the need for point sampling. It casts a 3D edge point as the center\n    of a 3D Gaussian and the edge direction as the principal axis of the Gaussian.\n    Such a representation has the advantage of being not only geometrically meaningful\n    but also compatible with the efficient training optimization defined in Gaussian\n    Splatting. Results show that the proposed method produces edges as accurate and\n    complete as the state-of-the-art while being an order of magnitude faster.\n\n    '\n  project_page: null\n  paper: https://arxiv.org/pdf/2409.12886.pdf\n  code: https://github.com/kunalchelani/EdgeGaussians\n  video: null\n  tags:\n  - Code\n  - Rendering\n  thumbnail: assets/thumbnails/chelani2024edgegaussians.jpg\n  publication_date: '2024-09-19T16:28:45+00:00'\n  date_source: arxiv\n- id: joseph2024gradientdriven\n  title: Gradient-Driven 3D Segmentation and Affordance Transfer in Gaussian Splatting\n    Using 2D Masks\n  authors: Joji Joseph, Bharadwaj Amrutur, Shalabh Bhatnagar\n  year: '2024'\n  abstract: In this paper, we introduce a novel voting-based method that extends 2D\n    segmentation models to 3D Gaussian splats. Our approach leverages masked gradients,\n    where gradients are filtered by input 2D masks, and these gradients are used as\n    votes to achieve accurate segmentation. As a byproduct, we found that inference-time\n    gradients can also be used to prune Gaussians, resulting in up to 21% compression.\n    Additionally, we explore few-shot affordance transfer, allowing annotations from\n    2D images to be effectively transferred onto 3D Gaussian splats. The robust yet\n    straightforward mathematical formulation underlying this approach makes it a highly\n    effective tool for numerous downstream applications, such as augmented reality\n    (AR), object editing, and robotics.\n  project_page: https://jojijoseph.github.io/3dgs-segmentation/\n  paper: https://arxiv.org/pdf/2409.11681\n  code: https://github.com/JojiJoseph/3dgs-gradient-segmentation\n  video: null\n  tags:\n  - Code\n  - Project\n  - Segmentation\n  thumbnail: assets/thumbnails/joseph2024gradientdriven.jpg\n  publication_date: '2024-09-18T03:45:44+00:00'\n- id: mihajlovic2024splatfields\n  title: 'SplatFields: Neural Gaussian Splats for Sparse 3D and 4D Reconstruction'\n  authors: Marko Mihajlovic, Sergey Prokudin, Siyu Tang, Robert Maier, Federica Bogo,\n    Tony Tung, Edmond Boyer\n  year: '2024'\n  abstract: Digitizing 3D static scenes and 4D dynamic events from multi-view images\n    has long been a challenge in computer vision and graphics. Recently, 3D Gaussian\n    Splatting (3DGS) has emerged as a practical and scalable reconstruction method,\n    gaining popularity due to its impressive reconstruction quality, real-time rendering\n    capabilities, and compatibility with widely used visualization tools. However,\n    the method requires a substantial number of input views to achieve high-quality\n    scene reconstruction, introducing a significant practical bottleneck. This challenge\n    is especially severe in capturing dynamic scenes, where deploying an extensive\n    camera array can be prohibitively costly. In this work, we identify the lack of\n    spatial autocorrelation of splat features as one of the factors contributing to\n    the suboptimal performance of the 3DGS technique in sparse reconstruction settings.\n    To address the issue, we propose an optimization strategy that effectively regularizes\n    splat features by modeling them as the outputs of a corresponding implicit neural\n    field. This results in a consistent enhancement of reconstruction quality across\n    various scenarios. Our approach effectively handles static and dynamic cases,\n    as demonstrated by extensive testing across different setups and scene complexities.\n  project_page: https://markomih.github.io/SplatFields/\n  paper: https://arxiv.org/pdf/2409.11211.pdf\n  code: https://github.com/markomih/SplatFields\n  video: null\n  tags:\n  - Code\n  - Project\n  - Sparse\n  thumbnail: assets/thumbnails/mihajlovic2024splatfields.jpg\n  publication_date: '2024-09-17T14:04:20+00:00'\n- id: meng2024beings\n  title: 'BEINGS: Bayesian Embodied Image-goal Navigation with Gaussian Splatting'\n  authors: Wugang Meng, Tianfu Wu, Huan Yin, Fumin Zhang\n  year: '2024'\n  abstract: Image-goal navigation enables a robot to reach the location where a target\n    image was captured, using visual cues for guidance. However, current methods either\n    rely heavily on data and computationally expensive learning-based approaches or\n    lack efficiency in complex environments due to insufficient exploration strategies.\n    To address these limitations, we propose Bayesian Embodied Image-goal Navigation\n    Using Gaussian Splatting, a novel method that formulates ImageNav as an optimal\n    control problem within a model predictive control framework. BEINGS leverages\n    3D Gaussian Splatting as a scene prior to predict future observations, enabling\n    efficient, real-time navigation decisions grounded in the robot’s sensory experiences.\n    By integrating Bayesian updates, our method dynamically refines the robot's strategy\n    without requiring extensive prior experience or data. Our algorithm is validated\n    through extensive simulations and physical experiments, showcasing its potential\n    for embodied robot systems in visually complex scenarios.\n  project_page: https://www.mwg.ink/BEINGS-web\n  paper: https://arxiv.org/pdf/2409.10216\n  code: https://github.com/guaMass/BEINGS\n  video: null\n  tags:\n  - Autonomous Driving\n  - Code\n  - Project\n  - Robotics\n  thumbnail: assets/thumbnails/meng2024beings.jpg\n  publication_date: '2024-09-16T12:07:02+00:00'\n- id: li2024pshuman\n  title: 'PSHuman: Photorealistic Single-view Human Reconstruction using Cross-Scale\n    Diffusion'\n  authors: Peng Li, Wangguandong Zheng, Yuan Liu, Tao Yu, Yangguang Li, Xingqun Qi,\n    Mengfei Li, Xiaowei Chi, Siyu Xia, Wei Xue, Wenhan Luo, Qifeng Liu, Yike Guo\n  year: '2024'\n  abstract: 'Detailed and photorealistic 3D human modeling is essential for various\n    applications and has seen tremendous progress. However, full-body reconstruction\n    from a monocular RGB image remains challenging due to the ill-posed nature of\n    the problem and sophisticated clothing topology with self-occlusions. In this\n    paper, we propose PSHuman, a novel framework that explicitly reconstructs human\n    meshes utilizing priors from the multiview diffusion model. It is found that directly\n    applying multiview diffusion on single-view human images leads to severe geometric\n    distortions, especially on generated faces. To address it, we propose a cross-scale\n    diffusion that models the joint probability distribution of global full-body shape\n    and local facial characteristics, enabling detailed and identity-preserved novel-view\n    generation without any geometric distortion. Moreover, to enhance cross-view body\n    shape consistency of varied human poses, we condition the generative model on\n    parametric models like SMPL-X, which provide body priors and prevent unnatural\n    views inconsistent with human anatomy. Leveraging the generated multi-view normal\n    and color images, we present SMPLX-initialized explicit human carving to recover\n    realistic textured human meshes efficiently. Extensive experimental results and\n    quantitative evaluations on CAPE and THuman2.1 datasets demonstrate PSHumans superiority\n    in geometry details, texture fidelity, and generalization capability.\n\n    '\n  project_page: https://penghtyx.github.io/PSHuman/\n  paper: https://arxiv.org/pdf/2409.10141.pdf\n  code: https://github.com/pengHTYX/PSHuman\n  video: null\n  tags:\n  - Avatar\n  - Code\n  - Diffusion\n  - Meshing\n  - Project\n  thumbnail: assets/thumbnails/li2024pshuman.jpg\n  publication_date: '2024-09-16T10:13:06+00:00'\n  date_source: arxiv\n- id: jiang2024dualgs\n  title: 'DualGS: Robust Dual Gaussian Splatting for Immersive Human-centric Volumetric\n    Videos'\n  authors: Yuheng Jiang, Zhehao Shen, Yu Hong, Chengcheng Guo, Yize Wu, Yingliang\n    Zhang, Jingyi Yu, Lan Xu\n  year: '2024'\n  abstract: Volumetric video represents a transformative advancement in visual media,\n    enabling users to freely navigate immersive virtual experiences and narrowing\n    the gap between digital and real worlds. However, the need for extensive manual\n    intervention to stabilize mesh sequences and the generation of excessively large\n    assets in existing workflows impedes broader adoption. In this paper, we present\n    a novel Gaussian-based approach, dubbed \\textit{DualGS}, for real-time and high-fidelity\n    playback of complex human performance with excellent compression ratios. Our key\n    idea in DualGS is to separately represent motion and appearance using the corresponding\n    skin and joint Gaussians. Such an explicit disentanglement can significantly reduce\n    motion redundancy and enhance temporal coherence. We begin by initializing the\n    DualGS and anchoring skin Gaussians to joint Gaussians at the first frame. Subsequently,\n    we employ a coarse-to-fine training strategy for frame-by-frame human performance\n    modeling. It includes a coarse alignment phase for overall motion prediction as\n    well as a fine-grained optimization for robust tracking and high-fidelity rendering.\n    To integrate volumetric video seamlessly into VR environments, we efficiently\n    compress motion using entropy encoding and appearance using codec compression\n    coupled with a persistent codebook. Our approach achieves a compression ratio\n    of up to 120 times, only requiring approximately 350KB of storage per frame. We\n    demonstrate the efficacy of our representation through photo-realistic, free-view\n    experiences on VR headsets, enabling users to immersively watch musicians in performance\n    and feel the rhythm of the notes at the performers' fingertips.\n  project_page: https://nowheretrix.github.io/DualGS/\n  paper: https://arxiv.org/pdf/2409.08353\n  code: null\n  video: https://www.youtube.com/watch?v=vwDE8xr78Bg\n  tags:\n  - Avatar\n  - Project\n  - Video\n  thumbnail: assets/thumbnails/jiang2024dualgs.jpg\n  publication_date: '2024-09-12T18:33:13+00:00'\n- id: liao2024fisheyegs\n  title: 'Fisheye-GS: Lightweight and Extensible Gaussian Splatting Module for Fisheye\n    Cameras'\n  authors: Zimu Liao, Siyan Chen, Rong Fu, Yi Wang, Zhongling Su, Hao Luo, Li Ma,\n    Linning Xu, Bo Dai, Hengjie Li, Zhilin Pei, Xingcheng Zhang\n  year: '2024'\n  abstract: 'Recently, 3D Gaussian Splatting (3DGS) has garnered attention for its\n    high fidelity and real-time rendering. However, adapting 3DGS to different camera\n    models, particularly fisheye lenses, poses challenges due to the unique 3D to\n    2D projection calculation. Additionally, there are inefficiencies in the tile-based\n    splatting, especially for the extreme curvature and wide field of view of fisheye\n    lenses, which are crucial for its broader real-life applications. To tackle these\n    challenges, we introduce Fisheye-GS.This innovative method recalculates the projection\n    transformation and its gradients for fisheye cameras. Our approach can be seamlessly\n    integrated as a module into other efficient 3D rendering methods, emphasizing\n    its extensibility, lightweight nature, and modular design. Since we only modified\n    the projection component, it can also be easily adapted for use with different\n    camera models. Compared to methods that train after undistortion, our approach\n    demonstrates a clear improvement in visual quality.\n\n    '\n  project_page: null\n  paper: https://arxiv.org/pdf/2409.04751.pdf\n  code: https://github.com/zmliao/Fisheye-GS\n  video: null\n  tags:\n  - Code\n  - Rendering\n  thumbnail: assets/thumbnails/liao2024fisheyegs.jpg\n  publication_date: '2024-09-07T07:53:40+00:00'\n  date_source: arxiv\n- id: chen2024omnire\n  title: 'OmniRe: Omni Urban Scene Reconstruction'\n  authors: Ziyu Chen, Jiawei Yang, Jiahui Huang, Riccardo de Lutio, Janick Martinez\n    Esturo, Boris Ivanovic, Or Litany, Zan Gojcic, Sanja Fidler, Marco Pavone, Li\n    Song, Yue Wang\n  year: '2024'\n  abstract: We introduce OmniRe, a holistic approach for efficiently reconstructing\n    high-fidelity dynamic urban scenes from on-device logs. Recent methods for modeling\n    driving sequences using neural radiance fields or Gaussian Splatting have demonstrated\n    the potential of reconstructing challenging dynamic scenes, but often overlook\n    pedestrians and other non-vehicle dynamic actors, hindering a complete pipeline\n    for dynamic urban scene reconstruction. To that end, we propose a comprehensive\n    3DGS framework for driving scenes, named OmniRe, that allows for accurate, full-length\n    reconstruction of diverse dynamic objects in a driving log. OmniRe builds dynamic\n    neural scene graphs based on Gaussian representations and constructs multiple\n    local canonical spaces that model various dynamic actors, including vehicles,\n    pedestrians, and cyclists, among many others. This capability is unmatched by\n    existing methods. OmniRe allows us to holistically reconstruct different objects\n    present in the scene, subsequently enabling the simulation of reconstructed scenarios\n    with all actors participating in real-time (~60Hz). Extensive evaluations on the\n    Waymo dataset show that our approach outperforms prior state-of-the-art methods\n    quantitatively and qualitatively by a large margin. We believe our work fills\n    a critical gap in driving reconstruction.\n  project_page: https://ziyc.github.io/omnire/\n  paper: https://arxiv.org/pdf/2408.16760\n  code: https://github.com/ziyc/drivestudio\n  video: null\n  tags:\n  - Autonomous Driving\n  - Code\n  - Project\n  thumbnail: assets/thumbnails/chen2024omnire.jpg\n  publication_date: '2024-08-29T17:56:33+00:00'\n- id: wang20243d\n  title: 3D Reconstruction with Spatial Memory\n  authors: Hengyi Wang, Lourdes Agapito\n  year: '2024'\n  abstract: 'We present Spann3R, a novel approach for dense 3D reconstruction from\n    ordered or unordered image collections. Built on the DUSt3R paradigm, Spann3R\n    uses a transformer-based architecture to directly regress pointmaps from images\n    without any prior knowledge of the scene or camera parameters. Unlike DUSt3R,\n    which predicts per image-pair pointmaps each expressed in its local coordinate\n    frame, Spann3R can predict per-image pointmaps expressed in a global coordinate\n    system, thus eliminating the need for optimization-based global alignment. The\n    key idea of Spann3R is to manage an external spatial memory that learns to keep\n    track of all previous relevant 3D information. Spann3R then queries this spatial\n    memory to predict the 3D structure of the next frame in a global coordinate system.\n    Taking advantage of DUSt3R''s pre-trained weights, and further fine-tuning on\n    a subset of datasets, Spann3R shows competitive performance and generalization\n    ability on various unseen datasets and can process ordered image collections in\n    real time. Project page: \\url{https://hengyiwang.github.io/projects/spanner}\n\n    '\n  project_page: https://hengyiwang.github.io/projects/spanner\n  paper: https://arxiv.org/pdf/2408.16061.pdf\n  code: https://github.com/HengyiWang/spann3r\n  video: https://hengyiwang.github.io/projects/spanner/videos/spanner_intro.mp4\n  tags:\n  - 3ster-based\n  - Code\n  - Project\n  - SLAM\n  - Video\n  thumbnail: assets/thumbnails/wang20243d.jpg\n  publication_date: '2024-08-28T18:01:00+00:00'\n  date_source: arxiv\n- id: shi2024lapisgs\n  title: 'LapisGS: Layered Progressive 3D Gaussian Splatting for Adaptive Streaming'\n  authors: Yuang Shi, Simone Gasparini, Géraldine Morin, Wei Tsang Ooi,\n  year: '2024'\n  abstract: The rise of Extended Reality (XR) requires efficient streaming of 3D online\n    worlds, challenging current 3DGS representations to adapt to bandwidth-constrained\n    environments. We propose LapisGS, a layered 3DGS that supports adaptive streaming\n    and progressive rendering. Our method constructs a layered structure for cumulative\n    representation, incorporates dynamic opacity optimization to maintain visual fidelity,\n    and utilizes occupancy maps to efficiently manage Gaussian splats. This proposed\n    model offers a progressive representation supporting a continuous rendering quality\n    adapted for bandwidth-aware streaming. Extensive experiments validate the effectiveness\n    of our approach in balancing visual fidelity with the compactness of the model,\n    with up to 50.71% improvement in SSIM, 286.53% improvement in LPIPS, and 318.41%\n    reduction in model size, and shows its potential for bandwidth-adapted 3D streaming\n    and rendering applications.\n  project_page: https://yuang-ian.github.io/lapisgs/\n  paper: https://arxiv.org/pdf/2408.14823\n  code: null\n  video: null\n  tags:\n  - Compression\n  - Project\n  thumbnail: assets/thumbnails/shi2024lapisgs.jpg\n  publication_date: '2024-08-27T07:06:49+00:00'\n- id: smart2024splatt3r\n  title: 'Splatt3R: Zero-shot Gaussian Splatting from Uncalibrated Image Pairs'\n  authors: Brandon Smart, Chuanxia Zheng, Iro Laina, Victor Adrian Prisacariu\n  year: '2024'\n  abstract: 'In this paper, we introduce Splatt3R, a pose-free, feed-forward method\n    for in-the-wild 3D reconstruction and novel view synthesis from stereo pairs.\n    Given uncalibrated natural images, Splatt3R can predict 3D Gaussian Splats without\n    requiring any camera parameters or depth information. For generalizability, we\n    build Splatt3R upon a ``foundation'''' 3D geometry reconstruction method, MASt3R,\n    by extending it to deal with both 3D structure and appearance. Specifically, unlike\n    the original MASt3R which reconstructs only 3D point clouds, we predict the additional\n    Gaussian attributes required to construct a Gaussian primitive for each point.\n    Hence, unlike other novel view synthesis methods, Splatt3R is first trained by\n    optimizing the 3D point cloud''s geometry loss, and then a novel view synthesis\n    objective. By doing this, we avoid the local minima present in training 3D Gaussian\n    Splats from stereo views. We also propose a novel loss masking strategy that we\n    empirically find is critical for strong performance on extrapolated viewpoints.\n    We train Splatt3R on the ScanNet++ dataset and demonstrate excellent generalisation\n    to uncalibrated, in-the-wild images. Splatt3R can reconstruct scenes at 4FPS at\n    512 x 512 resolution, and the resultant splats can be rendered in real-time.\n\n    '\n  project_page: https://splatt3r.active.vision/\n  paper: https://arxiv.org/pdf/2408.13912.pdf\n  code: https://github.com/btsmart/splatt3r\n  video: null\n  tags:\n  - 3ster-based\n  - Code\n  - Project\n  - Rendering\n  thumbnail: assets/thumbnails/smart2024splatt3r.jpg\n  publication_date: '2024-08-25T18:27:20+00:00'\n  date_source: arxiv\n- id: zhang202425\n  title: '''25] 10. TranSplat: Generalizable 3D Gaussian Splatting from Sparse Multi-View\n    Images with Transformers'\n  authors: Chuanrui Zhang, Yingshuang Zou, Zhuoling Li, Minmin Yi, Haoqian Wang\n  year: '2024'\n  abstract: Compared with previous 3D reconstruction methods like Nerf, recent Generalizable\n    3D Gaussian Splatting (G-3DGS) methods demonstrate impressive efficiency even\n    in the sparse-view setting. However, the promising reconstruction performance\n    of existing G-3DGS methods relies heavily on accurate multi-view feature matching,\n    which is quite challenging. Especially for the scenes that have many non-overlapping\n    areas between various views and contain numerous similar regions, the matching\n    performance of existing methods is poor and the reconstruction precision is limited.\n    To address this problem, we develop a strategy that utilizes a predicted depth\n    confidence map to guide accurate local feature matching. In addition, we propose\n    to utilize the knowledge of existing monocular depth estimation models as prior\n    to boost the depth estimation precision in non-overlapping areas between views.\n    Combining the proposed strategies, we present a novel G-3DGS method named TranSplat,\n    which obtains the best performance on both the RealEstate10K and ACID benchmarks\n    while maintaining competitive speed and presenting strong cross-dataset generalization\n    ability.\n  project_page: https://xingyoujun.github.io/transplat/\n  paper: https://arxiv.org/pdf/2408.13770\n  code: https://github.com/xingyoujun/transplat\n  video: null\n  tags:\n  - Code\n  - Feed-Forward\n  - Project\n  - Sparse\n  - Transformer\n  thumbnail: assets/thumbnails/zhang202425.jpg\n  publication_date: '2024-08-25T08:37:57+00:00'\n- id: liu2024gsloc\n  title: 'GSLoc: Efficient Camera Pose Refinement via 3D Gaussian Splatting'\n  authors: Changkun Liu, Shuai Chen, Yash Bhalgat, Siyan Hu, Ming Cheng, Zirui Wang,\n    Victor Adrian Prisacariu, Tristan Braud\n  year: '2024'\n  abstract: We leverage 3D Gaussian Splatting (3DGS) as a scene representation and\n    propose a novel test-time camera pose refinement framework, GSLoc. This framework\n    enhances the localization accuracy of state-of-the-art absolute pose regression\n    and scene coordinate regression methods. The 3DGS model renders high-quality synthetic\n    images and depth maps to facilitate the establishment of 2D-3D correspondences.\n    GSLoc obviates the need for training feature extractors or descriptors by operating\n    directly on RGB images, utilizing the 3D foundation model, MASt3R, for precise\n    2D matching. To improve the robustness of our model in challenging outdoor environments,\n    we incorporate an exposure-adaptive module within the 3DGS framework. Consequently,\n    GSLoc enables efficient one-shot pose refinement given a single RGB query and\n    a coarse initial pose estimation. Our proposed approach surpasses leading NeRF-based\n    optimization methods in both accuracy and runtime across indoor and outdoor visual\n    localization benchmarks, achieving new state-of-the-art accuracy on two indoor\n    datasets.\n  project_page: https://gsloc.active.vision/\n  paper: https://arxiv.org/pdf/2408.11085\n  code: null\n  video: null\n  tags:\n  - Poses\n  - Project\n  thumbnail: assets/thumbnails/liu2024gsloc.jpg\n  publication_date: '2024-08-20T17:58:23+00:00'\n- id: zhu2024loopsplat\n  title: 'LoopSplat: Loop Closure by Registering 3D Gaussian Splats'\n  authors: Liyuan Zhu, Yue Li, Erik Sandström, Shengyu Huang, Konrad Schindler, Iro\n    Armeni\n  year: '2024'\n  abstract: Simultaneous Localization and Mapping (SLAM) based on 3D Gaussian Splats\n    (3DGS) has recently shown promise towards more accurate, dense 3D scene maps.\n    However, existing 3DGS-based methods fail to address the global consistency of\n    the scene via loop closure and/or global bundle adjustment. To this end, we propose\n    LoopSplat, which takes RGB-D images as input and performs dense mapping with 3DGS\n    submaps and frame-to-model tracking. LoopSplat triggers loop closure online and\n    computes relative loop edge constraints between submaps directly via 3DGS registration,\n    leading to improvements in efficiency and accuracy over traditional global-to-local\n    point cloud registration. It uses a robust pose graph optimization formulation\n    and rigidly aligns the submaps to achieve global consistency. Evaluation on the\n    synthetic Replica and real-world TUM-RGBD, ScanNet, and ScanNet++ datasets demonstrates\n    competitive or superior tracking, mapping, and rendering compared to existing\n    methods for dense RGB-D SLAM.\n  project_page: https://loopsplat.github.io/\n  paper: https://arxiv.org/pdf/2408.10154\n  code: https://github.com/GradientSpaces/LoopSplat\n  video: null\n  tags:\n  - Code\n  - Project\n  - SLAM\n  thumbnail: assets/thumbnails/zhu2024loopsplat.jpg\n  publication_date: '2024-08-19T17:04:18+00:00'\n- id: lee2024rethinking\n  title: Rethinking Open-Vocabulary Segmentation of Radiance Fields in 3D Space\n  authors: Hyunjee Lee*, Youngsik Yun*, Jeongmin Bae, Seoha Kim, Youngjung Uh\n  year: '2024'\n  abstract: 'Understanding the 3D semantics of a scene is a fundamental problem for\n    various scenarios such as embodied agents. While NeRFs and 3DGS excel at novel-view\n    synthesis, previous methods for understanding their semantics have been limited\n    to incomplete 3D understanding: their segmentation results are 2D masks and their\n    supervision is anchored at 2D pixels. This paper revisits the problem set to pursue\n    a better 3D understanding of a scene modeled by NeRFs and 3DGS as follows. 1)\n    We directly supervise the 3D points to train the language embedding field. It\n    achieves state-of-the-art accuracy without relying on multi-scale language embeddings.\n    2) We transfer the pre-trained language field to 3DGS, achieving the first real-time\n    rendering speed without sacrificing training time or accuracy. 3) We introduce\n    a 3D querying and evaluation protocol for assessing the reconstructed geometry\n    and semantics together. Code, checkpoints, and annotations will be available online.'\n  project_page: https://hyunji12.github.io/Open3DRF/\n  paper: https://arxiv.org/pdf/2408.07416\n  code: https://github.com/hyunji12/Open3DRF\n  video: null\n  tags:\n  - Code\n  - Language Embedding\n  - Project\n  - Segmentation\n  thumbnail: assets/thumbnails/lee2024rethinking.jpg\n  publication_date: '2024-08-14T09:50:02+00:00'\n- id: wildersmith2024radiance\n  title: Radiance Fields for Robotic Teleoperation\n  authors: Maximum Wilder-Smith, Vaishakh Patil, Marco Hutter\n  year: '2024'\n  abstract: 'Radiance field methods such as Neural Radiance Fields (NeRFs) or 3D Gaussian\n    Splatting (3DGS), have revolutionized graphics and novel view synthesis. Their\n    ability to synthesize new viewpoints with photo-realistic quality, as well as\n    capture complex volumetric and specular scenes, makes them an ideal visualization\n    for robotic teleoperation setups. Direct camera teleoperation provides high-fidelity\n    operation at the cost of maneuverability, while reconstruction-based approaches\n    offer controllable scenes with lower fidelity. With this in mind, we propose replacing\n    the traditional reconstruction-visualization components of the robotic teleoperation\n    pipeline with online Radiance Fields, offering highly maneuverable scenes with\n    photorealistic quality. As such, there are three main contributions to state of\n    the art: (1) online training of Radiance Fields using live data from multiple\n    cameras, (2) support for a variety of radiance methods including NeRF and 3DGS,\n    (3) visualization suite for these methods including a virtual reality scene. To\n    enable seamless integration with existing setups, these components were tested\n    with multiple robots in multiple configurations and were displayed using traditional\n    tools as well as the VR headset. The results across methods and robots were compared\n    quantitatively to a baseline of mesh reconstruction, and a user study was conducted\n    to compare the different visualization methods.'\n  project_page: https://leggedrobotics.github.io/rffr.github.io/\n  paper: https://arxiv.org/pdf/2407.20194\n  code: https://github.com/leggedrobotics/radiance_field_ros\n  video: null\n  tags:\n  - Code\n  - Misc\n  - Project\n  - Robotics\n  thumbnail: assets/thumbnails/wildersmith2024radiance.jpg\n  publication_date: '2024-07-29T17:20:55+00:00'\n- id: moenne-loccoz20243d\n  title: '3D Gaussian Ray Tracing: Fast Tracing of Particle Scenes'\n  authors: Nicolas Moenne-Loccoz, Ashkan Mirzaei, Or Perel, Riccardo de Lutio, Janick\n    Martinez Esturo, Gavriel State, Sanja Fidler, Nicholas Sharp, Zan Gojcic\n  year: '2024'\n  abstract: 'Particle-based representations of radiance fields such as 3D Gaussian\n    Splatting have found great success for reconstructing and re-rendering of complex\n    scenes. Most existing methods render particles via rasterization, projecting them\n    to screen space tiles for processing in a sorted order. This work instead considers\n    ray tracing the particles, building a bounding volume hierarchy and casting a\n    ray for each pixel using high-performance GPU ray tracing hardware. To efficiently\n    handle large numbers of semi-transparent particles, we describe a specialized\n    rendering algorithm which encapsulates particles with bounding meshes to leverage\n    fast ray-triangle intersections, and shades batches of intersections in depth-order.\n    The benefits of ray tracing are well-known in computer graphics: processing incoherent\n    rays for secondary lighting effects such as shadows and reflections, rendering\n    from highly-distorted cameras common in robotics, stochastically sampling rays,\n    and more. With our renderer, this flexibility comes at little cost compared to\n    rasterization. Experiments demonstrate the speed and accuracy of our approach,\n    as well as several applications in computer graphics and vision. We further propose\n    related improvements to the basic Gaussian representation, including a simple\n    use of generalized kernel functions which significantly reduces particle hit counts.\n\n    '\n  project_page: https://gaussiantracer.github.io/\n  paper: https://arxiv.org/pdf/2407.07090.pdf\n  code: null\n  video: https://gaussiantracer.github.io/res/3dgrt_supplementary_video.mp4\n  tags:\n  - Project\n  - Ray Tracing\n  - Video\n  thumbnail: assets/thumbnails/moenne-loccoz20243d.jpg\n  publication_date: '2024-07-09T17:59:30+00:00'\n  date_source: arxiv\n- id: ji2024segment\n  title: Segment Any 4D Gaussians\n  authors: Shengxiang Ji, Guanjun Wu, Jiemin Fang, Jiazhong Cen, Taoran Yi, Wenyu\n    Liu, Qi Tian, Xinggang Wang\n  year: '2024'\n  abstract: 'Modeling, understanding, and reconstructing the real world are crucial\n    in XR/VR. Recently, 3D Gaussian Splatting (3D-GS) methods have shown remarkable\n    success in modeling and understanding 3D scenes. Similarly, various 4D representations\n    have demonstrated the ability to capture the dynamics of the 4D world. However,\n    there is a dearth of research focusing on segmentation within 4D representations.\n    In this paper, we propose Segment Any 4D Gaussians (SA4D), one of the first frameworks\n    to segment anything in the 4D digital world based on 4D Gaussians. In SA4D, an\n    efficient temporal identity feature field is introduced to handle Gaussian drifting,\n    with the potential to learn precise identity features from noisy and sparse input.\n    Additionally, a 4D segmentation refinement process is proposed to remove artifacts.\n    Our SA4D achieves precise, high-quality segmentation within seconds in 4D Gaussians\n    and shows the ability to remove, recolor, compose, and render high-quality anything\n    masks. More demos are available at: https://jsxzs.github.io/sa4d/.'\n  project_page: https://jsxzs.github.io/sa4d/\n  paper: https://arxiv.org/pdf/2407.04504\n  code: null\n  video: null\n  tags:\n  - Project\n  - Segmentation\n  thumbnail: assets/thumbnails/ji2024segment.jpg\n  publication_date: '2024-07-05T13:44:15+00:00'\n- id: lee2024gscore\n  title: 'GSCore: Efficient Radiance Field Rendering via Architectural Support for\n    3D Gaussian Splatting'\n  authors: Junseo Lee, Seokwon Lee, Jungi Lee, Junyong Park, Jaewoong Sim\n  year: '2024'\n  abstract: This paper presents GSCore, a hardware acceleration unit that efficiently\n    executes the rendering pipeline of 3D Gaussian Splatting with algorithmic optimizations.\n    GSCore builds on the observations from an in-depth analysis of Gaussian-based\n    radiance field rendering to enhance computational efficiency and bring the technique\n    to wide adoption. In doing so, we present several optimization techniques, Gaussian\n    shape-aware intersection test, hierarchical sorting, and subtile skipping, all\n    of which are synergistically integrated with GSCore. We implement the hardware\n    design of GSCore, synthesize it using a commercial 28nm technology, and evaluate\n    the performance across a range of synthetic and real-world scenes with varying\n    image resolutions. Our evaluation results show that GSCore achieves a 15.86× speedup\n    on average over the mobile consumer GPU with a substantially smaller area and\n    lower energy consumption.\n  project_page: null\n  paper: https://jaewoong.org/pubs/asplos24-gscore.pdf\n  code: null\n  video: https://youtu.be/TByYGw837IU?si=7zBe0yqpsJUoVbIV\n  tags:\n  - Acceleration\n  - Rendering\n  thumbnail: assets/thumbnails/lee2024gscore.jpg\n  publication_date: '2024-07-01T00:00:00'\n  date_source: estimated\n- id: kim2024optimizing\n  title: Optimizing Dynamic NeRF and 3DGS with No Video Synchronization\n  authors: Seoha Kim*, Jeongmin Bae*, Youngsik Yun, HyunSeung Son, Hahyun Lee, Gun\n    Bang, Youngjung Uh\n  year: '2024'\n  abstract: Recent advancements in 4D scene reconstruction using dynamic NeRF and\n    3DGS have demonstrated the ability to represent dynamic scenes from multi-view\n    videos. However, they fail to reconstruct the dynamic scenes and struggle to fit\n    even the training views in unsynchronized settings. It happens because they employ\n    a single latent embedding for a frame, while the multi-view images at the same\n    frame were actually captured at different moments. To address this limitation,\n    we introduce time offsets for individual unsynchronized videos and jointly optimize\n    the offsets with the field. By design, our method is applicable for various baselines,\n    even regardless of the types of radiance fields. We conduct experiments on the\n    common Plenoptic Video Dataset and a newly built Unsynchronized Dynamic Blender\n    Dataset to verify the performance of our method.\n  project_page: null\n  paper: https://openreview.net/pdf?id=RQutkn4V9I\n  code: null\n  video: null\n  tags:\n  - Dynamic\n  thumbnail: assets/thumbnails/kim2024optimizing.jpg\n  publication_date: '2024-07-01T00:00:00'\n  date_source: estimated\n- id: zhang2024egogaussian\n  title: 'EgoGaussian: Dynamic Scene Understanding from Egocentric Video with 3D Gaussian\n    Splatting'\n  authors: Daiwei Zhang, Gengyan Li, Jiajie Li, Mickaël Bressieux, Otmar Hilliges,\n    Marc Pollefeys, Luc Van Gool, Xi Wang\n  year: '2024'\n  abstract: Human activities are inherently complex, often involving numerous object\n    interactions. To better understand these activities, it is crucial to model their\n    interactions with the environment captured through dynamic changes. The recent\n    availability of affordable head-mounted cameras and egocentric data offers a more\n    accessible and efficient means to understand human-object interactions in 3D environments.\n    However, most existing methods for human activity modeling neglect the dynamic\n    interactions with objects, resulting in only static representations. The few existing\n    solutions often require inputs from multiple sources, including multi-camera setups,\n    depth-sensing cameras, or kinesthetic sensors. To this end, we introduce EgoGaussian,\n    the first method capable of simultaneously reconstructing 3D scenes and dynamically\n    tracking 3D object motion from RGB egocentric input alone. We leverage the uniquely\n    discrete nature of Gaussian Splatting and segment dynamic interactions from the\n    background, with both having explicit representations. Our approach employs a\n    clip-level online learning pipeline that leverages the dynamic nature of human\n    activities, allowing us to reconstruct the temporal evolution of the scene in\n    chronological order and track rigid object motion. EgoGaussian shows significant\n    improvements in terms of both dynamic object and background reconstruction quality\n    compared to the state-of-the-art. We also qualitatively demonstrate the high quality\n    of the reconstructed models.\n  project_page: https://zdwww.github.io/egogs.github.io/\n  paper: https://arxiv.org/pdf/2406.19811\n  code: https://github.com/zdwww/EgoGaussian\n  video: https://youtu.be/nsZrmM7CJB0?si=IJnfWH_Vf_UW2JoF\n  tags:\n  - Code\n  - Dynamic\n  - Project\n  - Video\n  thumbnail: assets/thumbnails/zhang2024egogaussian.jpg\n  publication_date: '2024-06-28T10:39:36+00:00'\n- id: zhao2024on\n  title: On Scaling Up 3D Gaussian Splatting Training\n  authors: Hexu Zhao, Haoyang Weng, Daohan Lu, Ang Li, Jinyang Li, Aurojit Panda,\n    Saining Xie\n  year: '2024'\n  abstract: '3D Gaussian Splatting (3DGS) is increasingly popular for 3D reconstruction\n    due to its superior visual quality and rendering speed. However, 3DGS training\n    currently occurs on a single GPU, limiting its ability to handle high-resolution\n    and large-scale 3D reconstruction tasks due to memory constraints. We introduce\n    Grendel, a distributed system designed to partition 3DGS parameters and parallelize\n    computation across multiple GPUs. As each Gaussian affects a small, dynamic subset\n    of rendered pixels, Grendel employs sparse all-to-all communication to transfer\n    the necessary Gaussians to pixel partitions and performs dynamic load balancing.\n    Unlike existing 3DGS systems that train using one camera view image at a time,\n    Grendel supports batched training with multiple views. We explore various optimization\n    hyperparameter scaling strategies and find that a simple sqrt(batch size) scaling\n    rule is highly effective. Evaluations using large-scale, high-resolution scenes\n    show that Grendel enhances rendering quality by scaling up 3DGS parameters across\n    multiple GPUs. On the Rubble dataset, we achieve a test PSNR of 27.28 by distributing\n    40.4 million Gaussians across 16 GPUs, compared to a PSNR of 26.28 using 11.2\n    million Gaussians on a single GPU. Grendel is an open-source project available\n    at: https://github.com/nyu-systems/Grendel-GS'\n  project_page: https://daohanlu.github.io/scaling-up-3dgs/\n  paper: https://arxiv.org/pdf/2406.18533\n  code: https://github.com/nyu-systems/Grendel-GS\n  video: https://youtu.be/WaYfY3GTs6U\n  tags:\n  - Code\n  - Distributed\n  - Large-Scale\n  - Project\n  - Video\n  thumbnail: assets/thumbnails/zhao2024on.jpg\n  publication_date: '2024-06-26T17:59:28+00:00'\n- id: papantonakis2024reducing\n  title: Reducing the Memory Footprint of 3D Gaussian Splatting\n  authors: Panagiotis Papantonakis, Georgios Kopanas, Bernhard Kerbl, Alexandre Lanvin,\n    George Drettakis\n  year: '2024'\n  abstract: '3D Gaussian splatting provides excellent visual quality for novel view\n    synthesis, with fast training and realtime rendering; unfortunately, the memory\n    requirements of this method for storing and transmission are unreasonably high.\n    We first analyze the reasons for this, identifying three main areas where storage\n    can be reduced: the number of 3D Gaussian primitives used to represent a scene,\n    the number of coefficients for the spherical harmonics used to represent directional\n    radiance, and the precision required to store Gaussian primitive attributes. We\n    present a solution to each of these issues. First, we propose an efficient, resolutionaware\n    primitive pruning approach, reducing the primitive count by half. Second, we introduce\n    an adaptive adjustment method to choose the number of coefficients used to represent\n    directional radiance for each Gaussian primitive, and finally a codebook-based\n    quantization method, together with a half-float representation for further memory\n    reduction. Taken together, these three components result in a ×27 reduction in\n    overall size on disk on the standard datasets we tested, along with a x1.7 speedup\n    in rendering speed. We demonstrate our method on standard datasets and show how\n    our solution results in significantly reduced download times when using the method\n    on a mobile device (see Fig. 1).'\n  project_page: https://repo-sam.inria.fr/fungraph/reduced_3dgs/\n  paper: https://arxiv.org/pdf/2406.17074.pdf\n  code: https://repo-sam.inria.fr/fungraph/reduced_3dgs\n  video: https://www.youtube.com/watch?v=EnKE-d7eMds&t=48s\n  tags:\n  - Code\n  - Compression\n  - Project\n  - Video\n  thumbnail: assets/thumbnails/papantonakis2024reducing.jpg\n  publication_date: '2024-06-24T19:01:44+00:00'\n  date_source: arxiv\n- id: mallick2024taming\n  title: 'Taming 3DGS: High-Quality Radiance Fields with Limited Resources'\n  authors: Saswat Subhajyoti Mallick, Rahul Goel, Bernhard Kerbl, Francisco Vicente\n    Carrasco, Markus Steinberger, Fernando De La Torre\n  year: '2024'\n  abstract: '3D Gaussian Splatting (3DGS) has transformed novel-view synthesis with\n    its fast, interpretable, and high-fidelity rendering. However, its resource requirements\n    limit its usability. Especially on constrained devices, training performance degrades\n    quickly and often cannot complete due to excessive memory consumption of the model.\n    The method converges with an indefinite number of Gaussians -- many of them redundant\n    -- making rendering unnecessarily slow and preventing its usage in downstream\n    tasks that expect fixed-size inputs. To address these issues, we tackle the challenges\n    of training and rendering 3DGS models on a budget. We use a guided, purely constructive\n    densification process that steers densification toward Gaussians that raise the\n    reconstruction quality. Model size continuously increases in a controlled manner\n    towards an exact budget, using score-based densification of Gaussians with training-time\n    priors that measure their contribution. We further address training speed obstacles:\n    following a careful analysis of 3DGS'' original pipeline, we derive faster, numerically\n    equivalent solutions for gradient computation and attribute updates, including\n    an alternative parallelization for efficient backpropagation. We also propose\n    quality-preserving approximations where suitable to reduce training time even\n    further. Taken together, these enhancements yield a robust, scalable solution\n    with reduced training times, lower compute and memory requirements, and high quality.\n    Our evaluation shows that in a budgeted setting, we obtain competitive quality\n    metrics with 3DGS while achieving a 4--5x reduction in both model size and training\n    time. With more generous budgets, our measured quality surpasses theirs. These\n    advances open the door for novel-view synthesis in constrained environments, e.g.,\n    mobile devices.\n\n    '\n  project_page: https://humansensinglab.github.io/taming-3dgs/\n  paper: https://arxiv.org/pdf/2406.15643.pdf\n  code: https://github.com/humansensinglab/taming-3dgs\n  video: https://youtu.be/iiUXqYmezbY\n  tags:\n  - Acceleration\n  - Code\n  - Densification\n  - Project\n  - Video\n  thumbnail: assets/thumbnails/mallick2024taming.jpg\n  publication_date: '2024-06-21T20:44:23+00:00'\n- id: pan2023humansplat\n  title: 'HumanSplat: Generalizable Single-Image Human Gaussian Splatting with Structure\n    Priors'\n  authors: Panwang Pan, Zhuo Su, Chenguo Lin, Zhen Fan, Yongjie Zhang, Zeming Li,\n    Tingting Shen, Yadong Mu, Yebin Liu\n  year: '2023'\n  abstract: 'Despite recent advancements in high-fidelity human reconstruction techniques,\n    the requirements for densely captured images or time-consuming per-instance optimization\n    significantly hinder their applications in broader scenarios. To tackle these\n    issues, we present HumanSplat that predicts the 3D Gaussian Splatting properties\n    of any human from a single input image in a generalizable manner. In particular,\n    HumanSplat comprises a 2D multi-view diffusion model and a latent reconstruction\n    transformer with human structure priors that adeptly integrate geometric priors\n    and semantic features within a unified framework. A hierarchical loss that incorporates\n    human semantic information is further designed to achieve high-fidelity texture\n    modeling and better constrain the estimated multiple views. Comprehensive experiments\n    on standard benchmarks and in-the-wild images demonstrate that HumanSplat surpasses\n    existing state-of-the-art methods in achieving photorealistic novel-view synthesis.\n    Project page: https://humansplat.github.io/.'\n  project_page: https://humansplat.github.io/\n  paper: https://arxiv.org/pdf/2406.12459\n  code: https://github.com/humansplat/humansplat\n  video: null\n  tags:\n  - Avatar\n  - Code\n  - Project\n  thumbnail: assets/thumbnails/pan2023humansplat.jpg\n  publication_date: '2024-06-18T10:05:33+00:00'\n- id: kerbl2024a\n  title: A Hierarchical 3D Gaussian Representation for Real-Time Rendering of Very\n    Large Datasets\n  authors: Bernhard Kerbl, Andreas Meuleman, Georgios Kopanas, Michael Wimmer,  Alexandre\n    Lanvin, George Drettakis\n  year: '2024'\n  abstract: Novel view synthesis has seen major advances in recent years, with 3D\n    Gaussian splatting offering an excellent level of visual quality, fast training\n    and real-time rendering. However, the resources needed for training and rendering\n    inevitably limit the size of the captured scenes that can be represented with\n    good visual quality. We introduce a hierarchy of 3D Gaussians that preserves visual\n    quality for very large scenes, while offering an efficient Level-of-Detail (LOD)\n    solution for efficient rendering of distant content with effective level selection\n    and smooth transitions between levels. We introduce a divide-and-conquer approach\n    that allows us to train very large scenes in independent chunks. We consolidate\n    the chunks into a hierarchy that can be optimized to further improve visual quality\n    of Gaussians merged into intermediate nodes. Very large captures typically have\n    sparse coverage of the scene, presenting many challenges to the original 3D Gaussian\n    splatting training method; we adapt and regularize training to account for these\n    issues. We present a complete solution, that enables real-time rendering of very\n    large scenes and can adapt to available resources thanks to our LOD method. We\n    show results for captured scenes with up to tens of thousands of images with a\n    simple and affordable rig, covering trajectories of up to several kilometers and\n    lasting up to one hour.\n  project_page: https://repo-sam.inria.fr/fungraph/hierarchical-3d-gaussians/\n  paper: https://arxiv.org/pdf/2406.12080.pdf\n  code: https://github.com/graphdeco-inria/hierarchical-3d-gaussians\n  video: https://repo-sam.inria.fr/fungraph/hierarchical-3d-gaussians/content/videos/small_city.mp4\n  tags:\n  - Code\n  - Large-Scale\n  - Project\n  - Video\n  thumbnail: assets/thumbnails/kerbl2024a.jpg\n  publication_date: '2024-06-17T20:40:18+00:00'\n  date_source: arxiv\n- id: hyung2024effective\n  title: Effective Rank Analysis and Regularization for Enhanced 3D Gaussian Splatting\n  authors: Junha Hyung, Susung Hong, Sungwon Hwang, Jaeseong Lee, Jaegul Choo, Jin-Hwa\n    Kim\n  year: '2024'\n  abstract: 3D reconstruction from multi-view images is one of the fundamental challenges\n    in computer vision and graphics. Recently, 3D Gaussian Splatting (3DGS) has emerged\n    as a promising technique capable of real-time rendering with high-quality 3D reconstruction.\n    This method utilizes 3D Gaussian representation and tile-based splatting techniques,\n    bypassing the expensive neural field querying. Despite its potential, 3DGS encounters\n    challenges, including needle-like artifacts, suboptimal geometries, and inaccurate\n    normals, due to the Gaussians converging into anisotropic Gaussians with one dominant\n    variance. We propose using effective rank analysis to examine the shape statistics\n    of 3D Gaussian primitives, and identify the Gaussians indeed converge into needle-like\n    shapes with the effective rank 1. To address this, we introduce effective rank\n    as a regularization, which constrains the structure of the Gaussians. Our new\n    regularization method enhances normal and geometry reconstruction while reducing\n    needle-like artifacts. The approach can be integrated as an add-on module to other\n    3DGS variants, improving their quality without compromising visual fidelity.\n  project_page: https://junhahyung.github.io/erankgs.github.io/\n  paper: https://arxiv.org/pdf/2406.11672\n  code: null\n  video: null\n  tags:\n  - Densification\n  - Meshing\n  - Project\n  thumbnail: assets/thumbnails/hyung2024effective.jpg\n  publication_date: '2024-06-17T15:51:59+00:00'\n- id: t20243dgszip\n  title: '3DGS.zip: A survey on 3D Gaussian Splatting Compression Methods'\n  authors: Milena T. Bagdasarian, Paul Knoll, Florian Barthel, Anna Hilsmann, Peter\n    Eisert, Wieland Morgenstern\n  year: '2024'\n  abstract: We present a work-in-progress survey on 3D Gaussian Splatting compression\n    methods, focusing on their statistical performance across various benchmarks.\n    This survey aims to facilitate comparability by summarizing key statistics of\n    different compression approaches in a tabulated format. The datasets evaluated\n    include TanksAndTemples, MipNeRF360, DeepBlending, and SyntheticNeRF. For each\n    method, we report the Peak Signal-to-Noise Ratio (PSNR), Structural Similarity\n    Index (SSIM), Learned Perceptual Image Patch Similarity (LPIPS), and the resultant\n    size in megabytes (MB), as provided by the respective authors. This is an ongoing,\n    open project, and we invite contributions from the research community as GitHub\n    issues or pull requests. Please visit https://w-m.github.io/3dgs-compression-survey/\n    for more information and a sortable version of the table.\n  project_page: https://w-m.github.io/3dgs-compression-survey\n  paper: https://arxiv.org/pdf/2407.09510\n  code: null\n  video: null\n  tags:\n  - Compression\n  - Project\n  thumbnail: assets/thumbnails/t20243dgszip.jpg\n  publication_date: '2024-06-17T11:43:38+00:00'\n- id: leroy2024grounding\n  title: Grounding Image Matching in 3D with MASt3R\n  authors: Vincent Leroy, Yohann Cabon, Jérôme Revaud\n  year: '2024'\n  abstract: 'Image Matching is a core component of all best-performing algorithms\n    and pipelines in 3D vision. Yet despite matching being fundamentally a 3D problem,\n    intrinsically linked to camera pose and scene geometry, it is typically treated\n    as a 2D problem. This makes sense as the goal of matching is to establish correspondences\n    between 2D pixel fields, but also seems like a potentially hazardous choice. In\n    this work, we take a different stance and propose to cast matching as a 3D task\n    with DUSt3R, a recent and powerful 3D reconstruction framework based on Transformers.\n    Based on pointmaps regression, this method displayed impressive robustness in\n    matching views with extreme viewpoint changes, yet with limited accuracy. We aim\n    here to improve the matching capabilities of such an approach while preserving\n    its robustness. We thus propose to augment the DUSt3R network with a new head\n    that outputs dense local features, trained with an additional matching loss. We\n    further address the issue of quadratic complexity of dense matching, which becomes\n    prohibitively slow for downstream applications if not carefully treated. We introduce\n    a fast reciprocal matching scheme that not only accelerates matching by orders\n    of magnitude, but also comes with theoretical guarantees and, lastly, yields improved\n    results. Extensive experiments show that our approach, coined MASt3R, significantly\n    outperforms the state of the art on multiple matching tasks. In particular, it\n    beats the best published methods by 30% (absolute improvement) in VCRE AUC on\n    the extremely challenging Map-free localization dataset.\n\n    '\n  project_page: https://europe.naverlabs.com/blog/mast3r-matching-and-stereo-3d-reconstruction/\n  paper: https://arxiv.org/pdf/2406.09756.pdf\n  code: https://github.com/naver/mast3r\n  video: null\n  tags:\n  - Code\n  - Project\n  thumbnail: assets/thumbnails/leroy2024grounding.jpg\n  publication_date: '2024-06-14T06:46:30+00:00'\n  date_source: arxiv\n- id: kirschstein2024gghead\n  title: 'GGHead: Fast and Generalizable 3D Gaussian Heads'\n  authors: Tobias Kirschstein, Simon Giebenhain, Jiapeng Tang, Markos Georgopoulos,\n    Matthias Nießner\n  year: '2024'\n  abstract: 'Learning 3D head priors from large 2D image collections is an important\n    step towards high-quality 3D-aware human modeling. A core requirement is an efficient\n    architecture that scales well to large-scale datasets and large image resolutions.\n    Unfortunately, existing 3D GANs struggle to scale to generate samples at high\n    resolutions due to their relatively slow train and render speeds, and typically\n    have to rely on 2D superresolution networks at the expense of global 3D consistency.\n    To address these challenges, we propose Generative Gaussian Heads (GGHead), which\n    adopts the recent 3D Gaussian Splatting representation within a 3D GAN framework.\n    To generate a 3D representation, we employ a powerful 2D CNN generator to predict\n    Gaussian attributes in the UV space of a template head mesh. This way, GGHead\n    exploits the regularity of the template''s UV layout, substantially facilitating\n    the challenging task of predicting an unstructured set of 3D Gaussians. We further\n    improve the geometric fidelity of the generated 3D representations with a novel\n    total variation loss on rendered UV coordinates. Intuitively, this regularization\n    encourages that neighboring rendered pixels should stem from neighboring Gaussians\n    in the template''s UV space. Taken together, our pipeline can efficiently generate\n    3D heads trained only from single-view 2D image observations. Our proposed framework\n    matches the quality of existing 3D head GANs on FFHQ while being both substantially\n    faster and fully 3D consistent. As a result, we demonstrate real-time generation\n    and rendering of high-quality 3D-consistent heads at $1024^2$ resolution for the\n    first time. Project Website: https://tobias-kirschstein.github.io/gghead\n\n    '\n  project_page: https://tobias-kirschstein.github.io/gghead/\n  paper: https://arxiv.org/pdf/2406.09377.pdf\n  code: https://github.com/tobias-kirschstein/gghead\n  video: https://youtu.be/M5vq3DoZ7RI\n  tags:\n  - Code\n  - GAN\n  - Project\n  - Video\n  thumbnail: assets/thumbnails/kirschstein2024gghead.jpg\n  publication_date: '2024-06-13T17:54:38+00:00'\n  date_source: arxiv\n- id: jaganathan2024iceg\n  title: 'ICE-G: Image Conditional Editing of 3D Gaussian Splats'\n  authors: Vishnu Jaganathan, Hannah Huang, Muhammad Zubair Irshad, Varun Jampani,\n    Amit Raj, Zsolt Kira\n  year: '2024'\n  abstract: Recently many techniques have emerged to create high quality 3D assets\n    and scenes. When it comes to editing of these objects, however, existing approaches\n    are either slow, compromise on quality, or do not provide enough customization.\n    We introduce a novel approach to quickly edit a 3D model from a single reference\n    view. Our technique first segments the edit image, and then matches semantically\n    corresponding regions across chosen segmented dataset views using DINO features.\n    A color or texture change from a particular region of the edit image can then\n    be applied to other views automatically in a semantically sensible manner. These\n    edited views act as an updated dataset to further train and re-style the 3D scene.\n    The end-result is therefore an edited 3D model. Our framework enables a wide variety\n    of editing tasks such as manual local edits, correspondence based style transfer\n    from any example image, and a combination of different styles from multiple example\n    images. We use Gaussian Splats as our primary 3D representation due to their speed\n    and ease of local editing, but our technique works for other methods such as NeRFs\n    as well. We show through multiple examples that our method produces higher quality\n    results while offering fine grained control of editing.\n  project_page: https://ice-gaussian.github.io/\n  paper: https://arxiv.org/pdf/2406.08488\n  code: null\n  video: https://youtu.be/dDsCwRXixp8?si=415s7-dEpM7-FPMq\n  tags:\n  - Editing\n  - Project\n  - Video\n  thumbnail: assets/thumbnails/jaganathan2024iceg.jpg\n  publication_date: '2024-06-12T17:59:52+00:00'\n- id: fan2024trim\n  title: Trim 3D Gaussian Splatting for Accurate Geometry Representation\n  authors: Lue Fan, Yuxue Yang, Minxing Li, Hongsheng Li, Zhaoxiang Zhang\n  year: '2024'\n  abstract: In this paper, we introduce Trim 3D Gaussian Splatting (TrimGS) to reconstruct\n    accurate 3D geometry from images. Previous arts for geometry reconstruction from\n    3D Gaussians mainly focus on exploring strong geometry regularization. Instead,\n    from a fresh perspective, we propose to obtain accurate 3D geometry of a scene\n    by Gaussian trimming, which selectively removes the inaccurate geometry while\n    preserving accurate structures. To achieve this, we analyze the contributions\n    of individual 3D Gaussians and propose a contribution-based trimming strategy\n    to remove the redundant or inaccurate Gaussians. Furthermore, our experimental\n    and theoretical analyses reveal that a relatively small Gaussian scale is a non-negligible\n    factor in representing and optimizing the intricate details. Therefore the proposed\n    TrimGS maintains relatively small Gaussian scales. In addition, TrimGS is also\n    compatible with the effective geometry regularization strategies in previous arts.\n    When combined with the original 3DGS and the state-of-the-art 2DGS, TrimGS consistently\n    yields more accurate geometry and higher perceptual quality.\n  project_page: https://trimgs.github.io/\n  paper: https://arxiv.org/pdf/2406.07499\n  code: https://github.com/YuxueYang1204/TrimGS\n  video: null\n  tags:\n  - 2DGS\n  - Code\n  - Densification\n  - Project\n  thumbnail: assets/thumbnails/fan2024trim.jpg\n  publication_date: '2024-06-11T17:34:46+00:00'\n- id: yu20244real\n  title: '4Real: Towards Photorealistic 4D Scene Generation via Video Diffusion Models'\n  authors: Heng Yu, Chaoyang Wang, Peiye Zhuang, Willi Menapace, Aliaksandr Siarohin,\n    Junli Cao, Laszlo A Jeni, Sergey Tulyakov, Hsin-Ying Lee\n  year: '2024'\n  abstract: Existing dynamic scene generation methods mostly rely on distilling knowledge\n    from pre-trained 3D generative models, which are typically fine-tuned on synthetic\n    object datasets. As a result, the generated scenes are often object-centric and\n    lack photorealism. To address these limitations, we introduce a novel pipeline\n    designed for photorealistic text-to-4D scene generation, discarding the dependency\n    on multi-view generative models and instead fully utilizing video generative models\n    trained on diverse real-world datasets. Our method begins by generating a reference\n    video using the video generation model. We then learn the canonical 3D representation\n    of the video using a freeze-time video, delicately generated from the reference\n    video. To handle inconsistencies in the freeze-time video, we jointly learn a\n    per-frame deformation to model these imperfections. We then learn the temporal\n    deformation based on the canonical representation to capture dynamic interactions\n    in the reference video. The pipeline facilitates the generation of dynamic scenes\n    with enhanced photorealism and structural integrity, viewable from multiple perspectives,\n    thereby setting a new standard in 4D scene generation.\n  project_page: https://snap-research.github.io/4Real/\n  paper: https://arxiv.org/pdf/2406.07472.pdf\n  code: null\n  video: null\n  tags:\n  - Diffusion\n  - Project\n  thumbnail: assets/thumbnails/yu20244real.jpg\n  publication_date: '2024-06-11T17:19:26+00:00'\n- id: jin2024lighting\n  title: 'Lighting Every Darkness with 3DGS: Fast Training and Real-Time Rendering\n    for HDR View Synthesis'\n  authors: Xin Jin, Pengyi Jiao, Zheng-Peng Duan, Xingchao Yang, Chun-Le Guo, Bo Ren,\n    Chongyi Li\n  year: '2024'\n  abstract: 'Volumetric rendering based methods, like NeRF, excel in HDR view synthesis\n    from RAWimages, especially for nighttime scenes. While, they suffer from long\n    training times and cannot perform real-time rendering due to dense sampling requirements.\n    The advent of 3D Gaussian Splatting (3DGS) enables real-time rendering and faster\n    training. However, implementing RAW image-based view synthesis directly using\n    3DGS is challenging due to its inherent drawbacks: 1) in nighttime scenes, extremely\n    low SNR leads to poor structure-from-motion (SfM) estimation in distant views;\n    2) the limited representation capacity of spherical harmonics (SH) function is\n    unsuitable for RAW linear color space; and 3) inaccurate scene structure hampers\n    downstream tasks such as refocusing. To address these issues, we propose LE3D\n    (Lighting Every darkness with 3DGS). Our method proposes Cone Scatter Initialization\n    to enrich the estimation of SfM, and replaces SH with a Color MLP to represent\n    the RAW linear color space. Additionally, we introduce depth distortion and near-far\n    regularizations to improve the accuracy of scene structure for downstream tasks.\n    These designs enable LE3D to perform real-time novel view synthesis, HDR rendering,\n    refocusing, and tone-mapping changes. Compared to previous volumetric rendering\n    based methods, LE3D reduces training time to 1% and improves rendering speed by\n    up to 4,000 times for 2K resolution images in terms of FPS. Code and viewer can\n    be found in https://github.com/Srameo/LE3D .\n\n    '\n  project_page: https://srameo.github.io/projects/le3d/intro.htmla\n  paper: https://arxiv.org/pdf/2406.06216.pdf\n  code: https://github.com/Srameo/LE3D\n  video: https://srameo.github.io/projects/le3d/assets/demo_video_interactive_viewer_compressed.mp4\n  tags:\n  - Code\n  - Project\n  - Rendering\n  - Video\n  thumbnail: assets/thumbnails/jin2024lighting.jpg\n  publication_date: '2024-06-10T12:33:08+00:00'\n  date_source: arxiv\n- id: hyun2024adversarial\n  title: Adversarial Generation of Hierarchical Gaussians for 3d Generative Model\n  authors: Sangeek Hyun, Jae-Pil Heo\n  year: '2024'\n  abstract: Most advances in 3D Generative Adversarial Networks (3D GANs) largely\n    depend on ray casting-based volume rendering, which incurs demanding rendering\n    costs. One promising alternative is rasterization-based 3D Gaussian Splatting\n    (3D-GS), providing a much faster rendering speed and explicit 3D representation.\n    In this paper, we exploit Gaussian as a 3D representation for 3D GANs by leveraging\n    its efficient and explicit characteristics. However, in an adversarial framework,\n    we observe that a naïve generator architecture suffers from training instability\n    and lacks the capability to adjust the scale of Gaussians. This leads to model\n    divergence and visual artifacts due to the absence of proper guidance for initialized\n    positions of Gaussians and densification to manage their scales adaptively. To\n    address these issues, we introduce a generator architecture with a hierarchical\n    multi-scale Gaussian representation that effectively regularizes the position\n    and scale of generated Gaussians. Specifically, we design a hierarchy of Gaussians\n    where finer-level Gaussians are parameterized by their coarser-level counterparts;\n    the position of finer-level Gaussians would be located near their coarser-level\n    counterparts, and the scale would monotonically decrease as the level becomes\n    finer, modeling both coarse and fine details of the 3D scene. Experimental results\n    demonstrate that ours achieves a significantly faster rendering speed (x100) compared\n    to state-of-the-art 3D consistent GANs with comparable 3D generation capability.\n  project_page: https://hse1032.github.io/gsgan\n  paper: https://arxiv.org/pdf/2406.02968\n  code: https://github.com/hse1032/GSGAN\n  video: null\n  tags:\n  - Code\n  - GAN\n  - Project\n  thumbnail: assets/thumbnails/hyun2024adversarial.jpg\n  publication_date: '2024-06-05T05:52:20+00:00'\n- id: zhang2024gs2mesh\n  title: ' RaDe-GS: Rasterizing Depth in Gaussian Splatting '\n  authors: Baowen Zhang, Chuan Fang, Rakesh Shrestha, Yixun Liang, Xiaoxiao Long,\n    Ping Tan\n  year: '2024'\n  abstract: Gaussian Splatting (GS) has proven to be highly effective in novel view\n    synthesis, achieving high-quality and real-time rendering. However, its potential\n    for reconstructing detailed 3D shapes has not been fully explored. Existing methods\n    often suffer from limited shape accuracy due to the discrete and unstructured\n    nature of Gaussian splats, which complicates the shape extraction. While recent\n    techniques like 2D GS have attempted to improve shape reconstruction, they often\n    reformulate the Gaussian primitives in ways that reduce both rendering quality\n    and computational efficiency. To address these problems, our work introduces a\n    rasterized approach to render the depth maps and surface normal maps of general\n    3D Gaussian splats. Our method not only significantly enhances shape reconstruction\n    accuracy but also maintains the computational efficiency intrinsic to Gaussian\n    Splatting. Our approach achieves a Chamfer distance error comparable to NeuraLangelo[Li\n    et al. 2023] on the DTU dataset and similar training and rendering time as traditional\n    Gaussian Splatting on the Tanks & Temples dataset. Our method is a significant\n    advancement in Gaussian Splatting and can be directly integrated into existing\n    Gaussian Splatting-based methods.\n  project_page: https://baowenz.github.io/radegs/\n  paper: https://arxiv.org/pdf/2406.01467\n  code: https://github.com/BaowenZ/RaDe-GS\n  video: null\n  tags:\n  - Code\n  - Meshing\n  - Project\n  thumbnail: assets/thumbnails/zhang2024gs2mesh.jpg\n  publication_date: '2024-06-03T15:56:58+00:00'\n- id: liu2024modgs\n  title: 'MoDGS: Dynamic Gaussian Splatting from Causually-captured Monocular Videos'\n  authors: Qingming Liu*, Yuan Liu*, Jiepeng Wang, Xianqiang Lv,Peng Wang, Wenping\n    Wang, Junhui Hou†,\n  year: '2024'\n  abstract: In this paper, we propose MoDGS, a new pipeline to render novel-view images\n    in dynamic scenes using only casually captured monocular videos. Previous monocular\n    dynamic NeRF or Gaussian Splatting methods strongly rely on the rapid movement\n    of input cameras to construct multiview consistency but fail to reconstruct dynamic\n    scenes on casually captured input videos whose cameras are static or move slowly.\n    To address this challenging task, MoDGS adopts recent single-view depth estimation\n    methods to guide the learning of the dynamic scene. Then, a novel 3D-aware initialization\n    method is proposed to learn a reasonable deformation field and a new robust depth\n    loss is proposed to guide the learning of dynamic scene geometry. Comprehensive\n    experiments demonstrate that MoDGS is able to render high-quality novel view images\n    of dynamic scenes from just a casually captured monocular video, which outperforms\n    baseline methods by a significant margin.\n  project_page: https://modgs.github.io/\n  paper: https://arxiv.org/pdf/2406.00434\n  code: null\n  video: null\n  tags:\n  - Dynamic\n  - Monocular\n  - Project\n  thumbnail: assets/thumbnails/liu2024modgs.jpg\n  publication_date: '2024-06-01T13:20:46+00:00'\n- id: labe2024dgd\n  title: 'DGD: Dynamic 3D Gaussians Distillation'\n  authors: Isaac Labe*, Noam Issachar*, Itai Lang, Sagie Benaim\n  year: '2024'\n  abstract: We tackle the task of learning dynamic 3D semantic radiance fields given\n    a single monocular video as input. Our learned semantic radiance field captures\n    per-point semantics as well as color and geometric properties for a dynamic 3D\n    scene, enabling the generation of novel views and their corresponding semantics.\n    This enables the segmentation and tracking of a diverse set of 3D semantic entities,\n    specified using a simple and intuitive interface that includes a user click or\n    a text prompt. To this end, we present DGD, a unified 3D representation for both\n    the appearance and semantics of a dynamic 3D scene, building upon the recently\n    proposed dynamic 3D Gaussians representation. Our representation is optimized\n    over time with both color and semantic information. Key to our method is the joint\n    optimization of the appearance and semantic attributes, which jointly affect the\n    geometric properties of the scene. We evaluate our approach in its ability to\n    enable dense semantic 3D object tracking and demonstrate high-quality results\n    that are fast to render, for a diverse set of scenes.\n  project_page: https://isaaclabe.github.io/DGD-Website/\n  paper: https://arxiv.org/pdf/2405.19321\n  code: https://github.com/Isaaclabe/DGD-Dynamic-3D-Gaussians-Distillation\n  video: https://www.youtube.com/watch?v=GzX2GJn9OKs\n  tags:\n  - Code\n  - Dynamic\n  - Project\n  - Video\n  thumbnail: assets/thumbnails/labe2024dgd.jpg\n  publication_date: '2024-05-29T17:52:22+00:00'\n- id: paul2024spsup2sup360\n  title: 'Sp<sup>2</sup>360: Sparse-view 360 Scene Reconstruction using Cascaded 2D\n    Diffusion Priors'\n  authors: Soumava Paul, Christopher Wewer, Bernt Schiele, Jan Eric Lenssen\n  year: '2024'\n  abstract: We aim to tackle sparse-view reconstruction of a 360 3D scene using priors\n    from latent diffusion models (LDM). The sparse-view setting is ill-posed and underconstrained,\n    especially for scenes where the camera rotates 360 degrees around a point, as\n    no visual information is available beyond some frontal views focused on the central\n    object(s) of interest. In this work, we show that pretrained 2D diffusion models\n    can strongly improve the reconstruction of a scene with low-cost fine-tuning.\n    Specifically, we present SparseSplat360 (Sp2360), a method that employs a cascade\n    of in-painting and artifact removal models to fill in missing details and clean\n    novel views. Due to superior training and rendering speeds, we use an explicit\n    scene representation in the form of 3D Gaussians over NeRF-based implicit representations.\n    We propose an iterative update strategy to fuse generated pseudo novel views with\n    existing 3D Gaussians fitted to the initial sparse inputs. As a result, we obtain\n    a multi-view consistent scene representation with details coherent with the observed\n    inputs. Our evaluation on the challenging Mip-NeRF360 dataset shows that our proposed\n    2D to 3D distillation algorithm considerably improves the performance of a regularized\n    version of 3DGS adapted to a sparse-view setting and outperforms existing sparse-view\n    reconstruction methods in 360 scene reconstruction. Qualitatively, our method\n    generates entire 360 scenes from as few as 9 input views, with a high degree of\n    foreground and background detail.\n  project_page: null\n  paper: https://arxiv.org/pdf/2405.16517\n  code: https://github.com/mvp18/sp2-360\n  video: null\n  tags:\n  - Code\n  - Sparse\n  thumbnail: assets/thumbnails/paul2024spsup2sup360.jpg\n  publication_date: '2024-05-26T11:01:39+00:00'\n- id: dalal2024gaussian\n  title: 'Gaussian Splatting: 3D Reconstruction and Novel View Synthesis, a Review'\n  authors: Anurag Dalal, Daniel Hagen, Kjell G. Robbersmyr, Kristian Muri Knausgård\n  year: '2024'\n  abstract: Image-based 3D reconstruction is a challenging task that involves inferring\n    the 3D shape of an object or scene from a set of input images. Learning-based\n    methods have gained attention for their ability to directly estimate 3D shapes.\n    This review paper focuses on state-of-the-art techniques for 3D reconstruction,\n    including the generation of novel, unseen views. An overview of recent developments\n    in the Gaussian Splatting method is provided, covering input types, model structures,\n    output representations, and training strategies. Unresolved challenges and future\n    directions are also discussed. Given the rapid progress in this domain and the\n    numerous opportunities for enhancing 3D reconstruction methods, a comprehensive\n    examination of algorithms appears essential. Consequently, this study offers a\n    thorough overview of the latest advancements in Gaussian Splatting.\n  project_page: null\n  paper: https://arxiv.org/pdf/2405.03417\n  code: null\n  video: null\n  tags:\n  - Review\n  thumbnail: assets/thumbnails/dalal2024gaussian.jpg\n  publication_date: '2024-05-06T12:32:38+00:00'\n- id: chu2024dreamscene4d\n  title: 'DreamScene4D: Dynamic Multi-Object Scene Generation from Monocular Videos'\n  authors: Wen-Hsuan Chu, Lei Ke, Katerina Fragkiadaki\n  year: '2024'\n  abstract: Existing VLMs can track in-the-wild 2D video objects while current generative\n    models provide powerful visual priors for synthesizing novel views for the highly\n    under-constrained 2D-to-3D object lifting. Building upon this exciting progress,\n    we present DreamScene4D, the first approach that can generate three-dimensional\n    dynamic scenes of multiple objects from monocular in-the-wild videos with large\n    object motion across occlusions and novel viewpoints. Our key insight is to design\n    a \"decompose-then-recompose\" scheme to factorize both the whole video scene and\n    each object's 3D motion. We first decompose the video scene by using open-vocabulary\n    mask trackers and an adapted image diffusion model to segment, track, and amodally\n    complete the objects and background in the video. Each object track is mapped\n    to a set of 3D Gaussians that deform and move in space and time. We also factorize\n    the observed motion into multiple components to handle fast motion. The camera\n    motion can be inferred by re-rendering the background to match the video frames.\n    For the object motion, we first model the object-centric deformation of the objects\n    by leveraging rendering losses and multi-view generative priors in an object-centric\n    frame, then optimize object-centric to world-frame transformations by comparing\n    the rendered outputs against the perceived pixel and optical flow. Finally, we\n    recompose the background and objects and optimize for relative object scales using\n    monocular depth prediction guidance. We show extensive results on the challenging\n    DAVIS, Kubric, and self-captured videos, detail some limitations, and provide\n    future directions. Besides 4D scene generation, our results show that DreamScene4D\n    enables accurate 2D point motion tracking by projecting the inferred 3D trajectories\n    to 2D, while never explicitly trained to do so.\n  project_page: https://dreamscene4d.github.io/\n  paper: https://arxiv.org/pdf/2405.02280\n  code: https://github.com/dreamscene4d/dreamscene4d\n  video: null\n  tags:\n  - Code\n  - Dynamic\n  - Monocular\n  - Project\n  thumbnail: assets/thumbnails/chu2024dreamscene4d.jpg\n  publication_date: '2024-05-03T17:55:34+00:00'\n- id: peng2024rtgslam\n  title: 'RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting'\n  authors: Zhexi Peng, Tianjia Shao, Yong Liu, Jingke Zhou, Yin Yang, Jingdong Wang,\n    Kun Zhou\n  year: '2024'\n  abstract: 'We present Real-time Gaussian SLAM (RTG-SLAM), a real-time 3D reconstruction\n    system with an RGBD camera for large-scale environments using Gaussian splatting.\n    The system features a compact Gaussian representation and a highly efficient on-the-fly\n    Gaussian optimization scheme. We force each Gaussian to be either opaque or nearly\n    transparent, with the opaque ones fitting the surface and dominant colors, and\n    transparent ones fitting residual colors. By rendering depth in a different way\n    from color rendering, we let a single opaque Gaussian well fit a local surface\n    region without the need of multiple overlapping Gaussians, hence largely reducing\n    the memory and computation cost. For on-the-fly Gaussian optimization, we explicitly\n    add Gaussians for three types of pixels per frame: newly observed, with large\n    color errors, and with large depth errors. We also categorize all Gaussians into\n    stable and unstable ones, where the stable Gaussians are expected to well fit\n    previously observed RGBD images and otherwise unstable. We only optimize the unstable\n    Gaussians and only render the pixels occupied by unstable Gaussians. In this way,\n    both the number of Gaussians to be optimized and pixels to be rendered are largely\n    reduced, and the optimization can be done in real time. We show real-time reconstructions\n    of a variety of large scenes. Compared with the state-of-the-art NeRF-based RGBD\n    SLAM, our system achieves comparable high-quality reconstruction but with around\n    twice the speed and half the memory cost, and shows superior performance in the\n    realism of novel view synthesis and camera tracking accuracy.'\n  project_page: https://gapszju.github.io/RTG-SLAM/\n  paper: https://arxiv.org/pdf/2404.19706\n  code: https://github.com/MisEty/RTG-SLAM\n  video: null\n  tags:\n  - Code\n  - Project\n  - SLAM\n  thumbnail: assets/thumbnails/peng2024rtgslam.jpg\n  publication_date: '2024-04-30T16:54:59+00:00'\n- id: lee2024guess\n  title: 'Guess The Unseen: Dynamic 3D Scene Reconstruction from Partial 2D Glimpses'\n  authors: Inhee Lee, Byungjun Kim, Hanbyul Joo\n  year: '2024'\n  abstract: In this paper, we present a method to reconstruct the world and multiple\n    dynamic humans in 3D from a monocular video input. As a key idea, we represent\n    both the world and multiple humans via the recently emerging 3D Gaussian Splatting\n    (3D-GS) representation, enabling to conveniently and efficiently compose and render\n    them together. In particular, we address the scenarios with severely limited and\n    sparse observations in 3D human reconstruction, a common challenge encountered\n    in the real world. To tackle this challenge, we introduce a novel approach to\n    optimize the 3D-GS representation in a canonical space by fusing the sparse cues\n    in the common space, where we leverage a pre-trained 2D diffusion model to synthesize\n    unseen views while keeping the consistency with the observed 2D appearances. We\n    demonstrate our method can reconstruct high-quality animatable 3D humans in various\n    challenging examples, in the presence of occlusion, image crops, few-shot, and\n    extremely sparse observations. After reconstruction, our method is capable of\n    not only rendering the scene in any novel views at arbitrary time instances, but\n    also editing the 3D scene by removing individual humans or applying different\n    motions for each human. Through various experiments, we demonstrate the quality\n    and efficiency of our methods over alternative existing approaches.\n  project_page: https://snuvclab.github.io/gtu/\n  paper: https://arxiv.org/pdf/2404.14410\n  code: https://github.com/snuvclab/gtu/\n  video: https://youtu.be/l9c_rd4hmFI\n  tags:\n  - Avatar\n  - Code\n  - Project\n  - Video\n  thumbnail: assets/thumbnails/lee2024guess.jpg\n  publication_date: '2024-04-22T17:59:50+00:00'\n- id: c2024contrastive\n  title: 'Contrastive Gaussian Clustering: Weakly Supervised 3D Scene Segmentation'\n  authors: Myrna C. Silva, Mahtab Dahaghin, Matteo Toso, Alessio Del Bue\n  year: '2024'\n  abstract: We introduce Contrastive Gaussian Clustering, a novel approach capable\n    of provide segmentation masks from any viewpoint and of enabling 3D segmentation\n    of the scene. Recent works in novel-view synthesis have shown how to model the\n    appearance of a scene via a cloud of 3D Gaussians, and how to generate accurate\n    images from a given viewpoint by projecting on it the Gaussians before α blending\n    their color. Following this example, we train a model to include also a segmentation\n    feature vector for each Gaussian. These can then be used for 3D scene segmentation,\n    by clustering Gaussians according to their feature vectors; and to generate 2D\n    segmentation masks, by projecting the Gaussians on a plane and α blending over\n    their segmentation features. Using a combination of contrastive learning and spatial\n    regularization, our method can be trained on inconsistent 2D segmentation masks,\n    and still learn to generate segmentation masks consistent across all views. Moreover,\n    the resulting model is extremely accurate, improving the IoU accuracy of the predicted\n    masks by +8% over the state of the art. Code and trained models will be released\n    upon acceptance.\n  project_page: null\n  paper: https://arxiv.org/pdf/2404.12784\n  code: null\n  video: null\n  tags:\n  - Segmentation\n  thumbnail: assets/thumbnails/c2024contrastive.jpg\n  publication_date: '2024-04-19T10:47:53+00:00'\n- id: liu2024infusion\n  title: 'InFusion: Inpainting 3D Gaussians via Learning Depth Completion from Diffusion\n    Prior'\n  authors: Zhiheng Liu, Hao Ouyang, Qiuyu Wang, Ka Leong Cheng, Jie Xiao, Kai Zhu,\n    Nan Xue, Yu Liu, Yujun Shen, Yang Cao\n  year: '2024'\n  abstract: 3D Gaussians have recently emerged as an efficient representation for\n    novel view synthesis. This work studies its editability with a particular focus\n    on the inpainting task, which aims to supplement an incomplete set of 3D Gaussians\n    with additional points for visually harmonious rendering. Compared to 2D inpainting,\n    the crux of inpainting 3D Gaussians is to figure out the rendering-relevant properties\n    of the introduced points, whose optimization largely benefits from their initial\n    3D positions. To this end, we propose to guide the point initialization with an\n    image-conditioned depth completion model, which learns to directly restore the\n    depth map based on the observed image. Such a design allows our model to fill\n    in depth values at an aligned scale with the original depth, and also to harness\n    strong generalizability from largescale diffusion prior. Thanks to the more accurate\n    depth completion, our approach, dubbed InFusion, surpasses existing alternatives\n    with sufficiently better fidelity and efficiency under various complex scenarios.\n    We further demonstrate the effectiveness of InFusion with several practical applications,\n    such as inpainting with user-specific texture or with novel object insertion.\n  project_page: https://johanan528.github.io/Infusion/\n  paper: https://arxiv.org/pdf/2404.11613\n  code: https://github.com/ali-vilab/infusion\n  video: null\n  tags:\n  - Code\n  - Editing\n  - Inpainting\n  - Project\n  thumbnail: assets/thumbnails/liu2024infusion.jpg\n  publication_date: '2024-04-17T17:59:53+00:00'\n- id: oh2024deblurgs\n  title: 'DeblurGS: Gaussian Splatting for Camera Motion Blur'\n  authors: Jeongtaek Oh, Jaeyoung Chung, Dongwoo Lee, Kyoung Mu Lee\n  year: '2024'\n  abstract: Although significant progress has been made in reconstructing sharp 3D\n    scenes from motion-blurred images, a transition to realworld applications remains\n    challenging. The primary obstacle stems from the severe blur which leads to inaccuracies\n    in the acquisition of initial camera poses through Structure-from-Motion, a critical\n    aspect often overlooked by previous approaches. To address this challenge, we\n    propose DeblurGS, a method to optimize sharp 3D Gaussian Splatting from motion-blurred\n    images, even with the noisy camera pose initialization. We restore a fine-grained\n    sharp scene by leveraging the remarkable reconstruction capability of 3D Gaussian\n    Splatting. Our approach estimates the 6-Degree-of-Freedom camera motion for each\n    blurry observation and synthesizes corresponding blurry renderings for the optimization\n    process. Furthermore, we propose Gaussian Densification Annealing strategy to\n    prevent the generation of inaccurate Gaussians at erroneous locations during the\n    early training stages when camera motion is still imprecise. Comprehensive experiments\n    demonstrate that our DeblurGS achieves state-of-the-art performance in deblurring\n    and novel view synthesis for real-world and synthetic benchmark datasets, as well\n    as field-captured blurry smartphone videos.\n  project_page: null\n  paper: https://arxiv.org/pdf/2404.11358\n  code: null\n  video: null\n  tags:\n  - Deblurring\n  - Rendering\n  thumbnail: assets/thumbnails/oh2024deblurgs.jpg\n  publication_date: '2024-04-17T13:14:52+00:00'\n- id: barthel2024gaussian\n  title: Gaussian Splatting Decoder for 3D‑aware Generative Adversarial Networks\n  authors: Florian Barthel, Arian Beckmann, Wieland Morgenstern, Anna Hilsmann, Peter\n    Eisert\n  year: '2024'\n  abstract: 'NeRF-based 3D-aware Generative Adversarial Networks like EG3D or GIRAFFE\n    have shown very high rendering quality under large representational variety. However,\n    rendering with Neural Radiance Fields poses several challenges for most 3D applications:\n    First, the significant computational demands of NeRF rendering preclude its use\n    on low-power devices, such as mobiles and VR/AR headsets. Second, implicit representations\n    based on neural networks are difficult to incorporate into explicit 3D scenes,\n    such as VR environments or video games. 3D Gaussian Splatting (3DGS) overcomes\n    these limitations by providing an explicit 3D representation that can be rendered\n    efficiently at high frame rates. In this work, we present a novel approach that\n    combines the high rendering quality of NeRF-based 3D-aware Generative Adversarial\n    Networks with the flexibility and computational advantages of 3DGS. By training\n    a decoder that maps implicit NeRF representations to explicit 3D Gaussian Splatting\n    attributes, we can integrate the representational diversity and quality of 3D\n    GANs into the ecosystem of 3D Gaussian Splatting for the first time. Additionally,\n    our approach allows for a high resolution GAN inversion and real-time GAN editing\n    with 3D Gaussian Splatting scenes.'\n  project_page: https://florian-barthel.github.io/gaussian_decoder/index.html\n  paper: https://arxiv.org/pdf/2404.10625\n  code: https://github.com/fraunhoferhhi/gaussian_gan_decoder\n  video: https://florian-barthel.github.io/gaussian_decoder/videos/latent.mp4\n  tags:\n  - Avatar\n  - Code\n  - Project\n  - Video\n  thumbnail: assets/thumbnails/barthel2024gaussian.jpg\n  publication_date: '2024-04-16T14:48:40+00:00'\n- id: cui2024letsgo\n  title: 'LetsGo: Large-Scale Garage Modeling and Rendering via LiDAR-Assisted Gaussian\n    Primitives'\n  authors: Jiadi Cui, Junming Cao, Fuqiang Zhao, Zhipeng He, Yifan Chen, Yuhui Zhong,\n    Lan Xu, Yujiao Shi, Yingliang Zhang, Jingyi Yu\n  year: '2024'\n  abstract: 'Large garages are ubiquitous yet intricate scenes that present unique\n    challenges due to their monotonous colors, repetitive patterns, reflective surfaces,\n    and transparent vehicle glass. Conventional Structure from Motion (SfM) methods\n    for camera pose estimation and 3D reconstruction often fail in these environments\n    due to poor correspondence construction. To address these challenges, we introduce\n    LetsGo, a LiDAR-assisted Gaussian splatting framework for large-scale garage modeling\n    and rendering. We develop a handheld scanner, Polar, equipped with IMU, LiDAR,\n    and a fisheye camera, to facilitate accurate data acquisition. Using this Polar\n    device, we present the GarageWorld dataset, consisting of eight expansive garage\n    scenes with diverse geometric structures, which will be made publicly available\n    for further research. Our approach demonstrates that LiDAR point clouds collected\n    by the Polar device significantly enhance a suite of 3D Gaussian splatting algorithms\n    for garage scene modeling and rendering. We introduce a novel depth regularizer\n    that effectively eliminates floating artifacts in rendered images. Additionally,\n    we propose a multi-resolution 3D Gaussian representation designed for Level-of-Detail\n    (LOD) rendering. This includes adapted scaling factors for individual levels and\n    a random-resolution-level training scheme to optimize the Gaussians across different\n    resolutions. This representation enables efficient rendering of large-scale garage\n    scenes on lightweight devices via a web-based renderer. Experimental results on\n    our GarageWorld dataset, as well as on ScanNet++ and KITTI-360, demonstrate the\n    superiority of our method in terms of rendering quality and resource efficiency.\n\n    '\n  project_page: https://zhaofuq.github.io/LetsGo/\n  paper: https://arxiv.org/pdf/2404.09748.pdf\n  code: https://github.com/zhaofuq/LOD-3DGS\n  video: https://youtu.be/fs42UBKvGRw?si=v1D0kPj1-QEzpMSR\n  tags:\n  - Code\n  - Large-Scale\n  - Lidar\n  - Project\n  - Video\n  thumbnail: assets/thumbnails/cui2024letsgo.jpg\n  publication_date: '2024-04-15T12:50:44+00:00'\n  date_source: arxiv\n- id: kheradmand20243d\n  title: 3D Gaussian Splatting as Markov Chain Monte Carlo\n  authors: Shakiba Kheradmand, Daniel Rebain, Gopal Sharma, Weiwei Sun, Jeff Tseng,\n    Hossam Isack, Abhishek Kar, Andrea Tagliasacchi, Kwang Moo Yi\n  year: '2024'\n  abstract: 'While 3D Gaussian Splatting has recently become popular for neural rendering,\n    current methods rely on carefully engineered cloning and splitting strategies\n    for placing Gaussians, which can lead to poor-quality renderings, and reliance\n    on a good initialization. In this work, we rethink the set of 3D Gaussians as\n    a random sample drawn from an underlying probability distribution describing the\n    physical representation of the scene-in other words, Markov Chain Monte Carlo\n    (MCMC) samples. Under this view, we show that the 3D Gaussian updates can be converted\n    as Stochastic Gradient Langevin Dynamics (SGLD) updates by simply introducing\n    noise. We then rewrite the densification and pruning strategies in 3D Gaussian\n    Splatting as simply a deterministic state transition of MCMC samples, removing\n    these heuristics from the framework. To do so, we revise the ''cloning'' of Gaussians\n    into a relocalization scheme that approximately preserves sample probability.\n    To encourage efficient use of Gaussians, we introduce a regularizer that promotes\n    the removal of unused Gaussians. On various standard evaluation scenes, we show\n    that our method provides improved rendering quality, easy control over the number\n    of Gaussians, and robustness to initialization.\n\n    '\n  project_page: https://ubc-vision.github.io/3dgs-mcmc/\n  paper: https://arxiv.org/pdf/2404.09591.pdf\n  code: https://github.com/ubc-vision/3dgs-mcmc\n  video: null\n  tags:\n  - Code\n  - Densification\n  - Project\n  thumbnail: assets/thumbnails/kheradmand20243d.jpg\n  publication_date: '2024-04-15T09:01:47+00:00'\n  date_source: arxiv\n- id: li2024loopgaussian\n  title: 'LoopGaussian: Creating 3D Cinemagraph with Multi-view Images via Eulerian\n    Motion Field'\n  authors: Jiyang Li, Lechao Cheng, Zhangye Wang, Tingting Mu, Jingxuan He\n  year: '2024'\n  abstract: Cinemagraph is a unique form of visual media that combines elements of\n    still photography and subtle motion to create a captivating experience. However,\n    the majority of videos generated by recent works lack depth information and are\n    confined to the constraints of 2D image space. In this paper, inspired by significant\n    progress in the field of novel view synthesis (NVS) achieved by 3D Gaussian Splatting\n    (3D-GS), we propose LoopGaussian to elevate cinemagraph from 2D image space to\n    3D space using 3D Gaussian modeling. To achieve this, we first employ the 3D-GS\n    method to reconstruct 3D Gaussian point clouds from multi-view images of static\n    scenes,incorporating shape regularization terms to prevent blurring or artifacts\n    caused by object deformation. We then adopt an autoencoder tailored for 3D Gaussian\n    to project it into feature space. To maintain the local continuity of the scene,\n    we devise SuperGaussian for clustering based on the acquired features. By calculating\n    the similarity between clusters and employing a two-stage estimation method, we\n    derive an Eulerian motion field to describe velocities across the entire scene.\n    The 3D Gaussian points then move within the estimated Eulerian motion field. Through\n    bidirectional animation techniques, we ultimately generate a 3D Cinemagraph that\n    exhibits natural and seamlessly loopable dynamics. Experiment results validate\n    the effectiveness of our approach, demonstrating high-quality and visually appealing\n    scene generation.\n  project_page: https://pokerlishao.github.io/LoopGaussian/\n  paper: https://arxiv.org/pdf/2404.08966\n  code: https://github.com/Pokerlishao/LoopGaussian\n  video: null\n  tags:\n  - Code\n  - Physics\n  - Project\n  - Rendering\n  thumbnail: assets/thumbnails/li2024loopgaussian.jpg\n  publication_date: '2024-04-13T11:07:53+00:00'\n- id: ye2024occgaussian\n  title: 'OccGaussian: 3D Gaussian Splatting for Occluded Human Rendering'\n  authors: Jingrui Ye, Zongkai Zhang, Yujiao Jiang, Qingmin Liao, Wenming Yang, Zongqing\n    Lu\n  year: '2024'\n  abstract: Rendering dynamic 3D human from monocular videos is crucial for various\n    applications such as virtual reality and digital entertainment. Most methods assume\n    the people is in an unobstructed scene, while various objects may cause the occlusion\n    of body parts in real-life scenarios. Previous method utilizing NeRF for surface\n    rendering to recover the occluded areas, but it requiring more than one day to\n    train and several seconds to render, failing to meet the requirements of real-time\n    interactive applications. To address these issues, we propose OccGaussian based\n    on 3D Gaussian Splatting, which can be trained within 6 minutes and produces high-quality\n    human renderings up to 160 FPS with occluded input. OccGaussian initializes 3D\n    Gaussian distributions in the canonical space, and we perform occlusion feature\n    query at occluded regions, the aggregated pixel-align feature is extracted to\n    compensate for the missing information. Then we use Gaussian Feature MLP to further\n    process the feature along with the occlusion-aware loss functions to better perceive\n    the occluded area. Extensive experiments both in simulated and real-world occlusions,\n    demonstrate that our method achieves comparable or even superior performance compared\n    to the state-of-the-art method. And we improving training and inference speeds\n    by 250x and 800x, respectively.\n  project_page: https://wenj.github.io/GoMAvatar/\n  paper: https://arxiv.org/pdf/2404.07991\n  code: https://github.com/wenj/GoMAvatar\n  video: null\n  tags:\n  - Avatar\n  - Code\n  - Project\n  thumbnail: assets/thumbnails/ye2024occgaussian.jpg\n  publication_date: '2024-04-11T17:59:57+00:00'\n- id: wen2024gomavatar\n  title: 'GoMAvatar: Efficient Animatable Human Modeling from Monocular Video Using\n    Gaussians-on-Mesh'\n  authors: Jing Wen, Xiaoming Zhao, Zhongzheng Ren, Alexander G. Schwing, Shenlong\n    Wang\n  year: '2024'\n  abstract: We introduce GoMAvatar, a novel approach for real-time, memory-efficient,\n    high-quality animatable human modeling. GoMAvatar takes as input a single monocular\n    video to create a digital avatar capable of re-articulation in new poses and real-time\n    rendering from novel viewpoints, while seamlessly integrating with rasterization-based\n    graphics pipelines. Central to our method is the Gaussians-on-Mesh representation,\n    a hybrid 3D model combining rendering quality and speed of Gaussian splatting\n    with geometry modeling and compatibility of deformable meshes. We assess GoMAvatar\n    on ZJU-MoCap data and various YouTube videos. GoMAvatar matches or surpasses current\n    monocular human modeling algorithms in rendering quality and significantly outperforms\n    them in computational efficiency (43 FPS) while being memory-efficient (3.63 MB\n    per subject).\n  project_page: https://wenj.github.io/GoMAvatar/\n  paper: https://arxiv.org/pdf/2404.07991\n  code: https://github.com/wenj/GoMAvatar\n  video: null\n  tags:\n  - Avatar\n  - Code\n  - Monocular\n  - Project\n  thumbnail: assets/thumbnails/wen2024gomavatar.jpg\n  publication_date: '2024-04-11T17:59:57+00:00'\n- id: lyu2024gaga\n  title: 'Gaga: Group Any Gaussians via 3D-aware Memory Bank'\n  authors: Weijie Lyu, Xueting Li, Abhijit Kundu, Yi-Hsuan Tsai, Ming-Hsuan Yang\n  year: '2024'\n  abstract: We introduce Gaga, a framework that reconstructs and segments open-world\n    3D scenes by leveraging inconsistent 2D masks predicted by zero-shot segmentation\n    models. Contrasted to prior 3D scene segmentation approaches that heavily rely\n    on video object tracking, Gaga utilizes spatial information and effectively associates\n    object masks across diverse camera poses. By eliminating the assumption of continuous\n    view changes in training images, Gaga demonstrates robustness to variations in\n    camera poses, particularly beneficial for sparsely sampled images, ensuring precise\n    mask label consistency. Furthermore, Gaga accommodates 2D segmentation masks from\n    diverse sources and demonstrates robust performance with different open-world\n    zero-shot segmentation models, significantly enhancing its versatility. Extensive\n    qualitative and quantitative evaluations demonstrate that Gaga performs favorably\n    against state-of-the-art methods, emphasizing its potential for real-world applications\n    such as scene understanding and manipulation.\n  project_page: https://www.gaga.gallery/\n  paper: https://arxiv.org/pdf/2404.07977.pdf\n  code: https://github.com/weijielyu/Gaga\n  video: https://www.youtube.com/watch?v=rqs5BuVFOok\n  tags:\n  - Code\n  - Editing\n  - Project\n  - Segmentation\n  - Video\n  thumbnail: assets/thumbnails/lyu2024gaga.jpg\n  publication_date: '2024-04-11T17:57:19+00:00'\n- id: shriram2024realmdreamer\n  title: 'RealmDreamer: Text-Driven 3D Scene Generation with Inpainting and Depth\n    Diffusion'\n  authors: Jaidev Shriram, Alex Trevithick, Lingjie Liu, Ravi Ramamoorthi\n  year: '2024'\n  abstract: We introduce RealmDreamer, a technique for generation of general forward-facing\n    3D scenes from text descriptions. Our technique optimizes a 3D Gaussian Splatting\n    representation to match complex text prompts. We initialize these splats by utilizing\n    the state-of-the-art text-to-image generators, lifting their samples into 3D,\n    and computing the occlusion volume. We then optimize this representation across\n    multiple views as a 3D inpainting task with image-conditional diffusion models.\n    To learn correct geometric structure, we incorporate a depth diffusion model by\n    conditioning on the samples from the inpainting model, giving rich geometric structure.\n    Finally, we finetune the model using sharpened samples from image generators.\n    Notably, our technique does not require video or multi-view data and can synthesize\n    a variety of high-quality 3D scenes in different styles, consisting of multiple\n    objects. Its generality additionally allows 3D synthesis from a single image\n  project_page: https://realmdreamer.github.io/\n  paper: https://arxiv.org/pdf/2404.07199\n  code: null\n  video: null\n  tags:\n  - Diffusion\n  - Project\n  thumbnail: assets/thumbnails/shriram2024realmdreamer.jpg\n  publication_date: '2024-04-10T17:57:41+00:00'\n- id: lang2024gaussianlic\n  title: 'Gaussian-LIC: Photo-realistic LiDAR-Inertial-Camera SLAM with 3D Gaussian\n    Splatting'\n  authors: Xiaolei Lang, Laijian Li, Hang Zhang, Feng Xiong, Mu Xu, Yong Liu, Xingxing\n    Zuo, Jiajun Lv\n  year: '2024'\n  abstract: We present a real-time LiDAR-Inertial-Camera SLAM system with 3D Gaussian\n    Splatting as the mapping backend. Leveraging robust pose estimates from our LiDAR-Inertial-Camera\n    odometry, Coco-LIC, an incremental photo-realistic mapping system is proposed\n    in this paper. We initialize 3D Gaussians from colorized LiDAR points and optimize\n    them using differentiable rendering powered by 3D Gaussian Splatting. Meticulously\n    designed strategies are employed to incrementally expand the Gaussian map and\n    adaptively control its density, ensuring high-quality mapping with real-time capability.\n    Experiments conducted in diverse scenarios demonstrate the superior performance\n    of our method compared to existing radiance-field-based SLAM systems.\n  project_page: https://xingxingzuo.github.io/gaussian_lic/\n  paper: https://arxiv.org/pdf/2404.06926\n  code: null\n  video: null\n  tags:\n  - Project\n  - SLAM\n  thumbnail: assets/thumbnails/lang2024gaussianlic.jpg\n  publication_date: '2024-04-10T11:24:34+00:00'\n- id: zhou2024dreamscene360\n  title: 'DreamScene360: Unconstrained Text-to-3D Scene Generation with Panoramic\n    Gaussian Splatting'\n  authors: Shijie Zhou, Zhiwen Fan, Dejia Xu, Haoran Chang, Pradyumna Chari, Tejas\n    Bharadwaj, Suya You, Zhangyang Wang, Achuta Kadambi\n  year: '2024'\n  abstract: The increasing demand for virtual reality applications has highlighted\n    the significance of crafting immersive 3D assets. We present a text-to-3D 360∘\n    scene generation pipeline that facilitates the creation of comprehensive 360∘\n    scenes for in-the-wild environments in a matter of minutes. Our approach utilizes\n    the generative power of a 2D diffusion model and prompt self-refinement to create\n    a high-quality and globally coherent panoramic image. This image acts as a preliminary\n    \"flat\" (2D) scene representation. Subsequently, it is lifted into 3D Gaussians,\n    employing splatting techniques to enable real-time exploration. To produce consistent\n    3D geometry, our pipeline constructs a spatially coherent structure by aligning\n    the 2D monocular depth into a globally optimized point cloud. This point cloud\n    serves as the initial state for the centroids of 3D Gaussians. In order to address\n    invisible issues inherent in single-view inputs, we impose semantic and geometric\n    constraints on both synthesized and input camera views as regularizations. These\n    guide the optimization of Gaussians, aiding in the reconstruction of unseen regions.\n    In summary, our method offers a globally consistent 3D scene within a 360∘ perspective,\n    providing an enhanced immersive experience over existing techniques.\n  project_page: https://dreamscene360.github.io/\n  paper: https://arxiv.org/pdf/2404.06903\n  code: https://github.com/ShijieZhou-UCLA/DreamScene360\n  video: https://www.youtube.com/embed/6rMIQfe7b24?si=cm7cZ-T9r5na7YFD\n  tags:\n  - Code\n  - Diffusion\n  - Project\n  - Video\n  thumbnail: assets/thumbnails/zhou2024dreamscene360.jpg\n  publication_date: '2024-04-10T10:46:59+00:00'\n- id: kruse2024splatpose\n  title: 'SplatPose & Detect: Pose-Agnostic 3D Anomaly Detection'\n  authors: Mathis Kruse, Marco Rudolph, Dominik Woiwode, Bodo Rosenhahn\n  year: '2024'\n  abstract: Detecting anomalies in images has become a well-explored problem in both\n    academia and industry. State-of-the-art algorithms are able to detect defects\n    in increasingly difficult settings and data modalities. However, most current\n    methods are not suited to address 3D objects captured from differing poses. While\n    solutions using Neural Radiance Fields (NeRFs) have been proposed, they suffer\n    from excessive computation requirements, which hinder real-world usability. For\n    this reason, we propose the novel 3D Gaussian splatting-based framework SplatPose\n    which, given multi-view images of a 3D object, accurately estimates the pose of\n    unseen views in a differentiable manner, and detects anomalies in them. We achieve\n    state-of-the-art results in both training and inference speed, and detection performance,\n    even when using less training data than competing methods. We thoroughly evaluate\n    our framework using the recently proposed Pose-agnostic Anomaly Detection benchmark\n    and its multi-pose anomaly detection (MAD) data set.\n  project_page: null\n  paper: https://arxiv.org/pdf/2404.06832\n  code: https://github.com/m-kruse98/SplatPose\n  video: null\n  tags:\n  - Code\n  - Misc\n  - Poses\n  thumbnail: assets/thumbnails/kruse2024splatpose.jpg\n  publication_date: '2024-04-10T08:48:09+00:00'\n- id: huang2024zeroshot\n  title: Zero-shot Point Cloud Completion Via 2D Priors\n  authors: Tianxin Huang, Zhiwen Yan, Yuyang Zhao, Gim Hee Lee\n  year: '2024'\n  abstract: 3D point cloud completion is designed to recover complete shapes from\n    partially observed point clouds. Conventional completion methods typically depend\n    on extensive point cloud data for training %, with their effectiveness often constrained\n    to object categories similar to those seen during training. In contrast, we propose\n    a zero-shot framework aimed at completing partially observed point clouds across\n    any unseen categories. Leveraging point rendering via Gaussian Splatting, we develop\n    techniques of Point Cloud Colorization and Zero-shot Fractal Completion that utilize\n    2D priors from pre-trained diffusion models to infer missing regions. Experimental\n    results on both synthetic and real-world scanned point clouds demonstrate that\n    our approach outperforms existing methods in completing a variety of objects without\n    any requirement for specific training data.\n  project_page: null\n  paper: https://arxiv.org/pdf/2404.06814\n  code: null\n  video: null\n  tags:\n  - Diffusion\n  - Point Cloud\n  thumbnail: assets/thumbnails/huang2024zeroshot.jpg\n  publication_date: '2024-04-10T08:02:17+00:00'\n- id: dai2024spikenvs\n  title: 'SpikeNVS: Enhancing Novel View Synthesis from Blurry Images via Spike Camera'\n  authors: Gaole Dai, Zhenyu Wang, Qinwen Xu, Ming Lu, Wen Chen, Boxin Shi, Shanghang\n    Zhang, Tiejun Huang\n  year: '2024'\n  abstract: One of the most critical factors in achieving sharp Novel View Synthesis\n    (NVS) using neural field methods like Neural Radiance Fields (NeRF) and 3D Gaussian\n    Splatting (3DGS) is the quality of the training images. However, Conventional\n    RGB cameras are susceptible to motion blur. In contrast, neuromorphic cameras\n    like event and spike cameras inherently capture more comprehensive temporal information,\n    which can provide a sharp representation of the scene as additional training data.\n    Recent methods have explored the integration of event cameras to improve the quality\n    of NVS. The event-RGB approaches have some limitations, such as high training\n    costs and the inability to work effectively in the background. Instead, our study\n    introduces a new method that uses the spike camera to overcome these limitations.\n    By considering texture reconstruction from spike streams as ground truth, we design\n    the Texture from Spike (TfS) loss. Since the spike camera relies on temporal integration\n    instead of temporal differentiation used by event cameras, our proposed TfS loss\n    maintains manageable training costs. It handles foreground objects with backgrounds\n    simultaneously. We also provide a real-world dataset captured with our spike-RGB\n    camera system to facilitate future research endeavors. We conduct extensive experiments\n    using synthetic and real-world datasets to demonstrate that our design can enhance\n    novel view synthesis across NeRF and 3DGS.\n  project_page: null\n  paper: https://arxiv.org/pdf/2404.06710\n  code: null\n  video: null\n  tags:\n  - Deblurring\n  - Misc\n  thumbnail: assets/thumbnails/dai2024spikenvs.jpg\n  publication_date: '2024-04-10T03:31:32+00:00'\n- id: wang2024endtoend\n  title: End-to-End Rate-Distortion Optimized 3D Gaussian Representation\n  authors: Henan Wang, Hanxin Zhu, Tianyu He, Runsen Feng, Jiajun Deng, Jiang Bian,\n    Zhibo Chen\n  year: '2024'\n  abstract: '3D Gaussian Splatting (3DGS) has become an emerging technique with remarkable\n    potential in 3D representation and image rendering. However, the substantial storage\n    overhead of 3DGS significantly impedes its practical applications. In this work,\n    we formulate the compact 3D Gaussian learning as an end-to-end Rate-Distortion\n    Optimization (RDO) problem and propose RDO-Gaussian that can achieve flexible\n    and continuous rate control. RDO-Gaussian addresses two main issues that exist\n    in current schemes: 1) Different from prior endeavors that minimize the rate under\n    the fixed distortion, we introduce dynamic pruning and entropy-constrained vector\n    quantization (ECVQ) that optimize the rate and distortion at the same time. 2)\n    Previous works treat the colors of each Gaussian equally, while we model the colors\n    of different regions and materials with learnable numbers of parameters. We verify\n    our method on both real and synthetic scenes, showcasing that RDO-Gaussian greatly\n    reduces the size of 3D Gaussian over 40×, and surpasses existing methods in rate-distortion\n    performance.'\n  project_page: https://rdogaussian.github.io/\n  paper: https://arxiv.org/pdf/2406.01597.pdf\n  code: https://github.com/USTC-IMCL/RDO-Gaussian\n  video: null\n  tags:\n  - Code\n  - Compression\n  - Project\n  thumbnail: assets/thumbnails/wang2024endtoend.jpg\n  publication_date: '2024-04-09T14:37:54+00:00'\n- id: lu20243d\n  title: 3D Geometry-aware Deformable Gaussian Splatting for Dynamic View Synthesis\n  authors: Zhicheng Lu, Xiang Guo, Le Hui, Tianrui Chen, Min Yang, Xiao Tang, Feng\n    Zhu, Yuchao Dai\n  year: '2024'\n  abstract: In this paper, we propose a 3D geometry-aware deformable Gaussian Splatting\n    method for dynamic view synthesis. Existing neural radiance fields (NeRF) based\n    solutions learn the deformation in an implicit manner, which cannot incorporate\n    3D scene geometry. Therefore, the learned deformation is not necessarily geometrically\n    coherent, which results in unsatisfactory dynamic view synthesis and 3D dynamic\n    reconstruction. Recently, 3D Gaussian Splatting provides a new representation\n    of the 3D scene, building upon which the 3D geometry could be exploited in learning\n    the complex 3D deformation. Specifically, the scenes are represented as a collection\n    of 3D Gaussian, where each 3D Gaussian is optimized to move and rotate over time\n    to model the deformation. To enforce the 3D scene geometry constraint during deformation,\n    we explicitly extract 3D geometry features and integrate them in learning the\n    3D deformation. In this way, our solution achieves 3D geometry-aware deformation\n    modeling, which enables improved dynamic view synthesis and 3D dynamic reconstruction.\n    Extensive experimental results on both synthetic and real datasets prove the superiority\n    of our solution, which achieves new state-of-the-art performance.\n  project_page: https://npucvr.github.io/GaGS/\n  paper: https://arxiv.org/pdf/2404.06270\n  code: null\n  video: null\n  tags:\n  - Dynamic\n  - Project\n  thumbnail: assets/thumbnails/lu20243d.jpg\n  publication_date: '2024-04-09T12:47:30+00:00'\n- id: bonilla2024gaussian\n  title: 'Gaussian Pancakes: Geometrically-Regularized 3D Gaussian Splatting for Realistic\n    Endoscopic Reconstruction'\n  authors: Sierra Bonilla, Shuai Zhang, Dimitrios Psychogyios, Danail Stoyanov, Francisco\n    Vasconcelos, Sophia Bano\n  year: '2024'\n  abstract: Within colorectal cancer diagnostics, conventional colonoscopy techniques\n    face critical limitations, including a limited field of view and a lack of depth\n    information, which can impede the detection of precancerous lesions. Current methods\n    struggle to provide comprehensive and accurate 3D reconstructions of the colonic\n    surface which can help minimize the missing regions and reinspection for pre-cancerous\n    polyps. Addressing this, we introduce 'Gaussian Pancakes', a method that leverages\n    3D Gaussian Splatting (3D GS) combined with a Recurrent Neural Network-based Simultaneous\n    Localization and Mapping (RNNSLAM) system. By introducing geometric and depth\n    regularization into the 3D GS framework, our approach ensures more accurate alignment\n    of Gaussians with the colon surface, resulting in smoother 3D reconstructions\n    with novel viewing of detailed textures and structures. Evaluations across three\n    diverse datasets show that Gaussian Pancakes enhances novel view synthesis quality,\n    surpassing current leading methods with a 18% boost in PSNR and a 16% improvement\n    in SSIM. It also delivers over 100X faster rendering and more than 10X shorter\n    training times, making it a practical tool for real-time applications. Hence,\n    this holds promise for achieving clinical translation for better detection and\n    diagnosis of colorectal cancer.\n  project_page: https://papers.miccai.org/miccai-2024/349-Paper2298.html\n  paper: https://arxiv.org/pdf/2404.06128\n  code: https://github.com/smbonilla/GaussianPancakes\n  video: null\n  tags:\n  - Code\n  - Medicine\n  - Project\n  thumbnail: assets/thumbnails/bonilla2024gaussian.jpg\n  publication_date: '2024-04-09T08:51:44+00:00'\n- id: rota2024revising\n  title: Revising Densification in Gaussian Splatting\n  authors: Samuel Rota Bulò, Lorenzo Porzi, Peter Kontschieder\n  year: '2024'\n  abstract: In this paper, we address the limitations of Adaptive Density Control\n    (ADC) in 3D Gaussian Splatting (3DGS), a scene representation method achieving\n    high-quality, photorealistic results for novel view synthesis. ADC has been introduced\n    for automatic 3D point primitive management, controlling densification and pruning,\n    however, with certain limitations in the densification logic. Our main contribution\n    is a more principled, pixel-error driven formulation for density control in 3DGS,\n    leveraging an auxiliary, per-pixel error function as the criterion for densification.\n    We further introduce a mechanism to control the total number of primitives generated\n    per scene and correct a bias in the current opacity handling strategy of ADC during\n    cloning operations. Our approach leads to consistent quality improvements across\n    a variety of benchmark scenes, without sacrificing the method's efficiency.\n  project_page: null\n  paper: https://arxiv.org/pdf/2404.06109\n  code: null\n  video: null\n  tags:\n  - Densification\n  thumbnail: assets/thumbnails/rota2024revising.jpg\n  publication_date: '2024-04-09T08:20:37+00:00'\n- id: yang2024hash3d\n  title: 'Hash3D: Training-free Acceleration for 3D Generation'\n  authors: Xingyi Yang, Xinchao Wang\n  year: '2024'\n  abstract: The evolution of 3D generative modeling has been notably propelled by\n    the adoption of 2D diffusion models. Despite this progress, the cumbersome optimization\n    process per se presents a critical hurdle to efficiency. In this paper, we introduce\n    Hash3D, a universal acceleration for 3D generation without model training. Central\n    to Hash3D is the insight that feature-map redundancy is prevalent in images rendered\n    from camera positions and diffusion time-steps in close proximity. By effectively\n    hashing and reusing these feature maps across neighboring timesteps and camera\n    angles, Hash3D substantially prevents redundant calculations, thus accelerating\n    the diffusion model's inference in 3D generation tasks. We achieve this through\n    an adaptive grid-based hashing. Surprisingly, this feature-sharing mechanism not\n    only speed up the generation but also enhances the smoothness and view consistency\n    of the synthesized 3D objects. Our experiments covering 5 text-to-3D and 3 image-to-3D\n    models, demonstrate Hash3D's versatility to speed up optimization, enhancing efficiency\n    by 1.3 to 4 times. Additionally, Hash3D's integration with 3D Gaussian splatting\n    largely speeds up 3D model creation, reducing text-to-3D processing to about 10\n    minutes and image-to-3D conversion to roughly 30 seconds.\n  project_page: https://adamdad.github.io/hash3D/\n  paper: https://arxiv.org/pdf/2404.06091\n  code: https://github.com/Adamdad/hash3D\n  video: null\n  tags:\n  - Acceleration\n  - Code\n  - Diffusion\n  - Project\n  thumbnail: assets/thumbnails/yang2024hash3d.jpg\n  publication_date: '2024-04-09T07:49:30+00:00'\n- id: zhang2024stylizedgs\n  title: 'StylizedGS: Controllable Stylization for 3D Gaussian Splatting'\n  authors: Dingxi Zhang, Zhuoxun Chen, Yu-Jie Yuan, Fang-Lue Zhang, Zhenliang He,\n    Shiguang Shan, Lin Gao\n  year: '2024'\n  abstract: With the rapid development of XR, 3D generation and editing are becoming\n    more and more important, among which, stylization is an important tool of 3D appearance\n    editing. It can achieve consistent 3D artistic stylization given a single reference\n    style image and thus is a user-friendly editing way. However, recent NeRF-based\n    3D stylization methods face efficiency issues that affect the actual user experience\n    and the implicit nature limits its ability to transfer the geometric pattern styles.\n    Additionally, the ability for artists to exert flexible control over stylized\n    scenes is considered highly desirable, fostering an environment conducive to creative\n    exploration. In this paper, we introduce StylizedGS, a 3D neural style transfer\n    framework with adaptable control over perceptual factors based on 3D Gaussian\n    Splatting (3DGS) representation. The 3DGS brings the benefits of high efficiency.\n    We propose a GS filter to eliminate floaters in the reconstruction which affects\n    the stylization effects before stylization. Then the nearest neighbor-based style\n    loss is introduced to achieve stylization by fine-tuning the geometry and color\n    parameters of 3DGS, while a depth preservation loss with other regularizations\n    is proposed to prevent the tampering of geometry content. Moreover, facilitated\n    by specially designed losses, StylizedGS enables users to control color, stylized\n    scale and regions during the stylization to possess customized capabilities. Our\n    method can attain high-quality stylization results characterized by faithful brushstrokes\n    and geometric consistency with flexible controls. Extensive experiments across\n    various scenes and styles demonstrate the effectiveness and efficiency of our\n    method concerning both stylization quality and inference FPS.\n  project_page: null\n  paper: https://arxiv.org/pdf/2404.05220\n  code: null\n  video: null\n  tags:\n  - Rendering\n  - Style Transfer\n  thumbnail: assets/thumbnails/zhang2024stylizedgs.jpg\n  publication_date: '2024-04-08T06:32:11+00:00'\n- id: wu2024dualcamera\n  title: Dual-Camera Smooth Zoom on Mobile Phones\n  authors: Renlong Wu, Zhilu Zhang, Yu Yang, Wangmeng Zuo\n  year: '2024'\n  abstract: When zooming between dual cameras on a mobile, noticeable jumps in geometric\n    content and image color occur in the preview, inevitably affecting the user's\n    zoom experience. In this work, we introduce a new task, ie, dual-camera smooth\n    zoom (DCSZ) to achieve a smooth zoom preview. The frame interpolation (FI) technique\n    is a potential solution but struggles with ground-truth collection. To address\n    the issue, we suggest a data factory solution where continuous virtual cameras\n    are assembled to generate DCSZ data by rendering reconstructed 3D models of the\n    scene. In particular, we propose a novel dual-camera smooth zoom Gaussian Splatting\n    (ZoomGS), where a camera-specific encoding is introduced to construct a specific\n    3D model for each virtual camera. With the proposed data factory, we construct\n    a synthetic dataset for DCSZ, and we utilize it to fine-tune FI models. In addition,\n    we collect real-world dual-zoom images without ground-truth for evaluation. Extensive\n    experiments are conducted with multiple FI methods. The results show that the\n    fine-tuned FI models achieve a significant performance improvement over the original\n    ones on DCSZ task.\n  project_page: https://dualcamerasmoothzoom.github.io/\n  paper: https://arxiv.org/pdf/2404.04908\n  code: https://github.com/ZcsrenlongZ/ZoomGS\n  video: null\n  tags:\n  - Code\n  - Misc\n  - Project\n  - Rendering\n  thumbnail: assets/thumbnails/wu2024dualcamera.jpg\n  publication_date: '2024-04-07T10:28:01+00:00'\n- id: darmon2024robust\n  title: Robust Gaussian Splatting\n  authors: François Darmon, Lorenzo Porzi, Samuel Rota-Bulò, Peter Kontschieder\n  year: '2024'\n  abstract: In this paper, we address common error sources for 3D Gaussian Splatting\n    (3DGS) including blur, imperfect camera poses, and color inconsistencies, with\n    the goal of improving its robustness for practical applications like reconstructions\n    from handheld phone captures. Our main contribution involves modeling motion blur\n    as a Gaussian distribution over camera poses, allowing us to address both camera\n    pose refinement and motion blur correction in a unified way. Additionally, we\n    propose mechanisms for defocus blur compensation and for addressing color in-consistencies\n    caused by ambient light, shadows, or due to camera-related factors like varying\n    white balancing settings. Our proposed solutions integrate in a seamless way with\n    the 3DGS formulation while maintaining its benefits in terms of training efficiency\n    and rendering speed. We experimentally validate our contributions on relevant\n    benchmark datasets including Scannet++ and Deblur-NeRF, obtaining state-of-the-art\n    results and thus consistent improvements over relevant baselines.\n  project_page: null\n  paper: https://arxiv.org/pdf/2404.04211\n  code: null\n  video: null\n  tags:\n  - Deblurring\n  - Rendering\n  thumbnail: assets/thumbnails/darmon2024robust.jpg\n  publication_date: '2024-04-05T16:42:16+00:00'\n- id: wu2024mmgaussian\n  title: 'MM-Gaussian: 3D Gaussian-based Multi-modal Fusion for Localization and Reconstruction\n    in Unbounded Scenes'\n  authors: Chenyang Wu, Yifan Duan, Xinran Zhang, Yu Sheng, Jianmin Ji, Yanyong Zhang\n  year: '2024'\n  abstract: Localization and mapping are critical tasks for various applications such\n    as autonomous vehicles and robotics. The challenges posed by outdoor environments\n    present particular complexities due to their unbounded characteristics. In this\n    work, we present MM-Gaussian, a LiDAR-camera multi-modal fusion system for localization\n    and mapping in unbounded scenes. Our approach is inspired by the recently developed\n    3D Gaussians, which demonstrate remarkable capabilities in achieving high rendering\n    quality and fast rendering speed. Specifically, our system fully utilizes the\n    geometric structure information provided by solid-state LiDAR to address the problem\n    of inaccurate depth encountered when relying solely on visual solutions in unbounded,\n    outdoor scenarios. Additionally, we utilize 3D Gaussian point clouds, with the\n    assistance of pixel-level gradient descent, to fully exploit the color information\n    in photos, thereby achieving realistic rendering effects. To further bolster the\n    robustness of our system, we designed a relocalization module, which assists in\n    returning to the correct trajectory in the event of a localization failure. Experiments\n    conducted in multiple scenarios demonstrate the effectiveness of our method.\n  project_page: null\n  paper: https://arxiv.org/pdf/2404.04026\n  code: null\n  video: null\n  tags:\n  - Poses\n  thumbnail: assets/thumbnails/wu2024mmgaussian.jpg\n  publication_date: '2024-04-05T11:14:19+00:00'\n- id: wu2024sc4d\n  title: 'SC4D: Sparse-Controlled Video-to-4D Generation and Motion Transfer'\n  authors: Zijie Wu, Chaohui Yu, Yanqin Jiang, Chenjie Cao, Fan Wang, Xiang Bai\n  year: '2024'\n  abstract: Recent advances in 2D/3D generative models enable the generation of dynamic\n    3D objects from a single-view video. Existing approaches utilize score distillation\n    sampling to form the dynamic scene as dynamic NeRF or dense 3D Gaussians. However,\n    these methods struggle to strike a balance among reference view alignment, spatio-temporal\n    consistency, and motion fidelity under single-view conditions due to the implicit\n    nature of NeRF or the intricate dense Gaussian motion prediction. To address these\n    issues, this paper proposes an efficient, sparse-controlled video-to-4D framework\n    named SC4D, that decouples motion and appearance to achieve superior video-to-4D\n    generation. Moreover, we introduce Adaptive Gaussian (AG) initialization and Gaussian\n    Alignment (GA) loss to mitigate shape degeneration issue, ensuring the fidelity\n    of the learned motion and shape. Comprehensive experimental results demonstrate\n    that our method surpasses existing methods in both quality and efficiency. In\n    addition, facilitated by the disentangled modeling of motion and appearance of\n    SC4D, we devise a novel application that seamlessly transfers the learned motion\n    onto a diverse array of 4D entities according to textual descriptions.\n  project_page: https://sc4d.github.io/\n  paper: https://arxiv.org/pdf/2404.03736\n  code: https://github.com/JarrentWu1031/SC4D\n  video: https://youtu.be/SkpTEuX4B5c?si=yvrF_iRHnMQR9TD0\n  tags:\n  - Code\n  - Diffusion\n  - Project\n  - Video\n  thumbnail: assets/thumbnails/wu2024sc4d.jpg\n  publication_date: '2024-04-04T18:05:18+00:00'\n- id: bae2024pergaussian\n  title: Per-Gaussian Embedding-Based Deformation for Deformable 3D Gaussian Splatting\n  authors: Jeongmin Bae*, Seoha Kim*, Youngsik Yun, Hahyun Lee, Gun Bang, Youngjung\n    Uh\n  year: '2024'\n  abstract: As 3D Gaussian Splatting (3DGS) provides fast and high-quality novel view\n    synthesis, it is a natural extension to deform a canonical 3DGS to multiple frames\n    for representing a dynamic scene. However, previous works fail to accurately reconstruct\n    complex dynamic scenes. We attribute the failure to the design of the deformation\n    field, which is built as a coordinate-based function. This approach is problematic\n    because 3DGS is a mixture of multiple fields centered at the Gaussians, not just\n    a single coordinate-based framework. To resolve this problem, we define the deformation\n    as a function of per-Gaussian embeddings and temporal embeddings. Moreover, we\n    decompose deformations as coarse and fine deformations to model slow and fast\n    movements, respectively. Also, we introduce a local smoothness regularization\n    for per-Gaussian embedding to improve the details in dynamic regions.\n  project_page: https://jeongminb.github.io/e-d3dgs/\n  paper: https://arxiv.org/pdf/2404.03613\n  code: https://github.com/JeongminB/E-D3DGS\n  video: null\n  tags:\n  - Code\n  - Dynamic\n  - Project\n  thumbnail: assets/thumbnails/bae2024pergaussian.jpg\n  publication_date: '2024-04-04T17:34:41+00:00'\n- id: li2024dreamscene\n  title: 'DreamScene: 3D Gaussian-based Text-to-3D Scene Generation via Formation\n    Pattern Sampling'\n  authors: Haoran Li, Haolin Shi, Wenli Zhang, Wenjun Wu, Yong Liao, Lin Wang, Lik\n    Hang Lee, Pengyuan Zhou\n  year: '2024'\n  abstract: Text-to-3D scene generation holds immense potential for the gaming, film,\n    and architecture sectors, increasingly capturing the attention of both academic\n    and industry circles. Despite significant progress, current methods still struggle\n    with maintaining high quality, consistency, and editing flexibility. In this paper,\n    we propose DreamScene, a 3D Gaussian-based novel text-to-3D scene generation framework\n    that leverages Formation Pattern Sampling (FPS) for core structuring, augmented\n    with a strategic camera sampling and supported by holistic object-environment\n    integration to overcome these hurdles. FPS, guided by the formation patterns of\n    3D objects, employs multi-timesteps sampling to quickly form semantically rich,\n    high-quality representations, uses 3D Gaussian filtering for optimization stability,\n    and leverages reconstruction techniques to generate plausible textures. The camera\n    sampling strategy incorporates a progressive three-stage approach, specifically\n    designed for both indoor and outdoor settings, to effectively ensure scene-wide\n    3D consistency. DreamScene enhances scene editing flexibility by combining objects\n    and environments, enabling targeted adjustments. Extensive experiments showcase\n    DreamScene's superiority over current state-of-the-art techniques, heralding its\n    wide-ranging potential for diverse applications.\n  project_page: https://dreamscene-project.github.io/\n  paper: https://arxiv.org/pdf/2404.03575.pdf\n  code: https://github.com/DreamScene-Project/DreamScene\n  video: null\n  tags:\n  - Code\n  - Diffusion\n  - Project\n  thumbnail: assets/thumbnails/li2024dreamscene.jpg\n  publication_date: '2024-04-04T16:38:57+00:00'\n- id: meng2024omnigs\n  title: 'OmniGS: Omnidirectional Gaussian Splatting for Fast Radiance Field Reconstruction\n    using Omnidirectional Images'\n  authors: Jiarui Meng, Haijie Li, Yanmin Wu, Qiankun Gao, Shuzhou Yang, Jian Zhang,\n    Siwei Ma\n  year: '2024'\n  abstract: Photorealistic reconstruction relying on 3D Gaussian Splatting has shown\n    promising potential in robotics. However, the current 3D Gaussian Splatting system\n    only supports radiance field reconstruction using undistorted perspective images.\n    In this paper, we present OmniGS, a novel omnidirectional Gaussian splatting system,\n    to take advantage of omnidirectional images for fast radiance field reconstruction.\n    Specifically, we conduct a theoretical analysis of spherical camera model derivatives\n    in 3D Gaussian Splatting. According to the derivatives, we then implement a new\n    GPU-accelerated omnidirectional rasterizer that directly splats 3D Gaussians onto\n    the equirectangular screen space for omnidirectional image rendering. As a result,\n    we realize differentiable optimization of the radiance field without the requirement\n    of cube-map rectification or tangent-plane approximation. Extensive experiments\n    conducted in egocentric and roaming scenarios demonstrate that our method achieves\n    state-of-the-art reconstruction quality and high rendering speed using omnidirectional\n    images. To benefit the research community, the code will be made publicly available\n    once the paper is published.\n  project_page: https://liquorleaf.github.io/research/OmniGS/\n  paper: https://arxiv.org/pdf/2404.03202\n  code: null\n  video: null\n  tags:\n  - 360 degree\n  - Project\n  - Rendering\n  thumbnail: assets/thumbnails/meng2024omnigs.jpg\n  publication_date: '2024-04-04T05:10:26+00:00'\n- id: nikolakakis2024gaspct\n  title: 'GaSpCT: Gaussian Splatting for Novel CT Projection View Synthesis'\n  authors: Emmanouil Nikolakakis, Utkarsh Gupta, Jonathan Vengosh, Justin Bui, Razvan\n    Marinescu\n  year: '2024'\n  abstract: 'We present GaSpCT, a novel view synthesis and 3D scene representation\n    method used to generate novel projection views for Computer Tomography (CT) scans.\n    We adapt the Gaussian Splatting framework to enable novel view synthesis in CT\n    based on limited sets of 2D image projections and without the need for Structure\n    from Motion (SfM) methodologies. Therefore, we reduce the total scanning duration\n    and the amount of radiation dose the patient receives during the scan. We adapted\n    the loss function to our use-case by encouraging a stronger background and foreground\n    distinction using two sparsity promoting regularizers: a beta loss and a total\n    variation (TV) loss. Finally, we initialize the Gaussian locations across the\n    3D space using a uniform prior distribution of where the brain''s positioning\n    would be expected to be within the field of view. We evaluate the performance\n    of our model using brain CT scans from the Parkinson''s Progression Markers Initiative\n    (PPMI) dataset and demonstrate that the rendered novel views closely match the\n    original projection views of the simulated scan, and have better performance than\n    other implicit 3D scene representations methodologies. Furthermore, we empirically\n    observe reduced training time compared to neural network based image synthesis\n    for sparse-view CT image reconstruction. Finally, the memory requirements of the\n    Gaussian Splatting representations are reduced by 17% compared to the equivalent\n    voxel grid image representations.'\n  project_page: null\n  paper: https://arxiv.org/pdf/2404.03126\n  code: null\n  video: null\n  tags:\n  - Medicine\n  thumbnail: assets/thumbnails/nikolakakis2024gaspct.jpg\n  publication_date: '2024-04-04T00:28:50+00:00'\n- id: zhao2024tclcgs\n  title: 'TCLC-GS: Tightly Coupled LiDAR-Camera Gaussian Splatting for Surrounding\n    Autonomous Driving Scenes'\n  authors: Cheng Zhao, Su Sun, Ruoyu Wang, Yuliang Guo, Jun-Jun Wan, Zhou Huang, Xinyu\n    Huang, Yingjie Victor Chen, Liu Ren\n  year: '2024'\n  abstract: Most 3D Gaussian Splatting (3D-GS) based methods for urban scenes initialize\n    3D Gaussians directly with 3D LiDAR points, which not only underutilizes LiDAR\n    data capabilities but also overlooks the potential advantages of fusing LiDAR\n    with camera data. In this paper, we design a novel tightly coupled LiDAR-Camera\n    Gaussian Splatting (TCLC-GS) to fully leverage the combined strengths of both\n    LiDAR and camera sensors, enabling rapid, high-quality 3D reconstruction and novel\n    view RGB/depth synthesis. TCLC-GS designs a hybrid explicit (colorized 3D mesh)\n    and implicit (hierarchical octree feature) 3D representation derived from LiDAR-camera\n    data, to enrich the properties of 3D Gaussians for splatting. 3D Gaussian's properties\n    are not only initialized in alignment with the 3D mesh which provides more completed\n    3D shape and color information, but are also endowed with broader contextual information\n    through retrieved octree implicit features. During the Gaussian Splatting optimization\n    process, the 3D mesh offers dense depth information as supervision, which enhances\n    the training process by learning of a robust geometry. Comprehensive evaluations\n    conducted on the Waymo Open Dataset and nuScenes Dataset validate our method's\n    state-of-the-art (SOTA) performance. Utilizing a single NVIDIA RTX 3090 Ti, our\n    method demonstrates fast training and achieves real-time RGB and depth rendering\n    at 90 FPS in resolution of 1920x1280 (Waymo), and 120 FPS in resolution of 1600x900\n    (nuScenes) in urban scenarios.\n  project_page: null\n  paper: https://arxiv.org/pdf/2404.02410.pdf\n  code: null\n  video: https://www.youtube.com/watch?v=CEo6mZ6FGg0\n  tags:\n  - Autonomous Driving\n  - Lidar\n  - Video\n  thumbnail: assets/thumbnails/zhao2024tclcgs.jpg\n  publication_date: '2024-04-03T02:26:15+00:00'\n- id: wolf2024gs2mesh\n  title: 'GS2Mesh: Surface Reconstruction from Gaussian Splatting via Novel Stereo\n    Views'\n  authors: Yaniv Wolf, Amit Bracha, Ron Kimmel\n  year: '2024'\n  abstract: Recently, 3D Gaussian Splatting (3DGS) has emerged as an efficient approach\n    for accurately representing scenes. However, despite its superior novel view synthesis\n    capabilities, extracting the geometry of the scene directly from the Gaussian\n    properties remains a challenge, as those are optimized based on a photometric\n    loss. While some concurrent models have tried adding geometric constraints during\n    the Gaussian optimization process, they still produce noisy, unrealistic surfaces.\n    We propose a novel approach for bridging the gap between the noisy 3DGS representation\n    and the smooth 3D mesh representation, by injecting real-world knowledge into\n    the depth extraction process. Instead of extracting the geometry of the scene\n    directly from the Gaussian properties, we instead extract the geometry through\n    a pre-trained stereo-matching model. We render stereo-aligned pairs of images\n    corresponding to the original training poses, feed the pairs into a stereo model\n    to get a depth profile, and finally fuse all of the profiles together to get a\n    single mesh. The resulting reconstruction is smoother, more accurate and shows\n    more intricate details compared to other methods for surface reconstruction from\n    Gaussian Splatting, while only requiring a small overhead on top of the fairly\n    short 3DGS optimization process. We performed extensive testing of the proposed\n    method on in-the-wild scenes, obtained using a smartphone, showcasing its superior\n    reconstruction abilities. Additionally, we tested the method on the Tanks and\n    Temples and DTU benchmarks, achieving state-of-the-art results.\n  project_page: https://gs2mesh.github.io//\n  paper: https://arxiv.org/pdf/2404.01810\n  code: https://github.com/yanivw12/gs2mesh\n  video: https://youtu.be/cjtmLDD8YZk\n  tags:\n  - 2DGS\n  - Code\n  - Meshing\n  - Project\n  - Stereo\n  - Video\n  thumbnail: assets/thumbnails/wolf2024gs2mesh.jpg\n  publication_date: '2024-04-02T10:13:18+00:00'\n- id: qiu2024feature\n  title: 'Feature Splatting: Language-Driven Physics-Based Scene Synthesis and Editing'\n  authors: Ri-Zhao Qiu, Ge Yang, Weijia Zeng, Xiaolong Wang\n  year: '2024'\n  abstract: Scene representations using 3D Gaussian primitives have produced excellent\n    results in modeling the appearance of static and dynamic 3D scenes. Many graphics\n    applications, however, demand the ability to manipulate both the appearance and\n    the physical properties of objects. We introduce Feature Splatting, an approach\n    that unifies physics-based dynamic scene synthesis with rich semantics from vision\n    language foundation models that are grounded by natural language. Our first contribution\n    is a way to distill high-quality, object-centric vision-language features into\n    3D Gaussians, that enables semi-automatic scene decomposition using text queries.\n    Our second contribution is a way to synthesize physics-based dynamics from an\n    otherwise static scene using a particle-based simulator, in which material properties\n    are assigned automatically via text queries. We ablate key techniques used in\n    this pipeline, to illustrate the challenge and opportunities in using feature-carrying\n    3D Gaussians as a unified format for appearance, geometry, material properties\n    and semantics grounded on natural language.\n  project_page: https://feature-splatting.github.io/\n  paper: https://arxiv.org/pdf/2404.01223\n  code: https://github.com/vuer-ai/feature_splatting\n  video: https://feature-splatting.github.io/resources/teaser_overview.mp4\n  tags:\n  - Code\n  - Editing\n  - Language Embedding\n  - Physics\n  - Project\n  - Segmentation\n  - Video\n  thumbnail: assets/thumbnails/qiu2024feature.jpg\n  publication_date: '2024-04-01T16:31:04+00:00'\n- id: meng2024mirror3dgs\n  title: 'Mirror-3DGS: Incorporating Mirror Reflections into 3D Gaussian Splatting'\n  authors: Jiarui Meng, Haijie Li, Yanmin Wu, Qiankun Gao, Shuzhou Yang, Jian Zhang,\n    Siwei Ma\n  year: '2024'\n  abstract: 3D Gaussian Splatting (3DGS) has marked a significant breakthrough in\n    the realm of 3D scene reconstruction and novel view synthesis. However, 3DGS,\n    much like its predecessor Neural Radiance Fields (NeRF), struggles to accurately\n    model physical reflections, particularly in mirrors that are ubiquitous in real-world\n    scenes. This oversight mistakenly perceives reflections as separate entities that\n    physically exist, resulting in inaccurate reconstructions and inconsistent reflective\n    properties across varied viewpoints. To address this pivotal challenge, we introduce\n    Mirror-3DGS, an innovative rendering framework devised to master the intricacies\n    of mirror geometries and reflections, paving the way for the generation of realistically\n    depicted mirror reflections. By ingeniously incorporating mirror attributes into\n    the 3DGS and leveraging the principle of plane mirror imaging, Mirror-3DGS crafts\n    a mirrored viewpoint to observe from behind the mirror, enriching the realism\n    of scene renderings. Extensive assessments, spanning both synthetic and real-world\n    scenes, showcase our method's ability to render novel views with enhanced fidelity\n    in real-time, surpassing the state-of-the-art Mirror-NeRF specifically within\n    the challenging mirror regions.\n  project_page: https://mirror-gaussian.github.io/\n  paper: https://arxiv.org/pdf/2404.01168.pdf\n  code: null\n  video: null\n  tags:\n  - Project\n  - Rendering\n  thumbnail: assets/thumbnails/meng2024mirror3dgs.jpg\n  publication_date: '2024-04-01T15:16:33+00:00'\n- id: liu2024citygaussian\n  title: 'CityGaussian: Real-time High-quality Large-Scale Scene Rendering with Gaussians'\n  authors: Yang Liu, He Guan, Chuanchen Luo, Lue Fan, Naiyan Wang, Junran Peng, Zhaoxiang\n    Zhang\n  year: '2024'\n  abstract: The advancement of real-time 3D scene reconstruction and novel view synthesis\n    has been significantly propelled by 3D Gaussian Splatting (3DGS). However, effectively\n    training large-scale 3DGS and rendering it in real-time across various scales\n    remains challenging. This paper introduces CityGaussian (CityGS), which employs\n    a novel divide-and-conquer training approach and Level-of-Detail (LoD) strategy\n    for efficient large-scale 3DGS training and rendering. Specifically, the global\n    scene prior and adaptive training data selection enables efficient training and\n    seamless fusion. Based on fused Gaussian primitives, we generate different detail\n    levels through compression, and realize fast rendering across various scales through\n    the proposed block-wise detail levels selection and aggregation strategy. Extensive\n    experimental results on large-scale scenes demonstrate that our approach attains\n    state-of-theart rendering quality, enabling consistent real-time rendering of\n    largescale scenes across vastly different scales.\n  project_page: https://dekuliutesla.github.io/citygs/\n  paper: https://arxiv.org/pdf/2404.01133\n  code: https://github.com/DekuLiuTesla/CityGaussian\n  video: null\n  tags:\n  - Code\n  - Large-Scale\n  - Project\n  thumbnail: assets/thumbnails/liu2024citygaussian.jpg\n  publication_date: '2024-04-01T14:24:40+00:00'\n- id: shao2024haha\n  title: 'HAHA: Highly Articulated Gaussian Human Avatars with Textured Mesh Prior'\n  authors: Zhijing Shao, Zhaolong Wang, Zhuang Li, Duotun Wang, Xiangru Lin, Yu Zhang,\n    Mingming Fan, Zeyu Wang\n  year: '2024'\n  abstract: 'We present HAHA - a novel approach for animatable human avatar generation\n    from monocular input videos. The proposed method relies on learning the trade-off\n    between the use of Gaussian splatting and a textured mesh for efficient and high\n    fidelity rendering. We demonstrate its efficiency to animate and render full-body\n    human avatars controlled via the SMPL-X parametric model. Our model learns to\n    apply Gaussian splatting only in areas of the SMPL-X mesh where it is necessary,\n    like hair and out-of-mesh clothing. This results in a minimal number of Gaussians\n    being used to represent the full avatar, and reduced rendering artifacts. This\n    allows us to handle the animation of small body parts such as fingers that are\n    traditionally disregarded. We demonstrate the effectiveness of our approach on\n    two open datasets: SnapshotPeople and X-Humans. Our method demonstrates on par\n    reconstruction quality to the state-of-the-art on SnapshotPeople, while using\n    less than a third of Gaussians. HAHA outperforms previous state-of-the-art on\n    novel poses from X-Humans both quantitatively and qualitatively.'\n  project_page: null\n  paper: https://arxiv.org/pdf/2404.01053\n  code: https://github.com/david-svitov/HAHA\n  video: null\n  tags:\n  - Avatar\n  - Code\n  thumbnail: assets/thumbnails/shao2024haha.jpg\n  publication_date: '2024-04-01T11:23:38+00:00'\n- id: c2024mm3dgs\n  title: 'MM3DGS SLAM: Multi-modal 3D Gaussian Splatting for SLAM Using Vision, Depth,\n    and Inertial Measurements'\n  authors: Lisong C. Sun, Neel P. Bhatt, Jonathan C. Liu, Zhiwen Fan, Zhangyang Wang,\n    Todd E. Humphreys, Ufuk Topcu\n  year: '2024'\n  abstract: Simultaneous localization and mapping is essential for position tracking\n    and scene understanding. 3D Gaussian-based map representations enable photorealistic\n    reconstruction and real-time rendering of scenes using multiple posed cameras.\n    We show for the first time that using 3D Gaussians for map representation with\n    unposed camera images and inertial measurements can enable accurate SLAM. Our\n    method, MM3DGS, addresses the limitations of prior neural radiance field-based\n    representations by enabling faster rendering, scale awareness, and improved trajectory\n    tracking. Our framework enables keyframe-based mapping and tracking utilizing\n    loss functions that incorporate relative pose transformations from pre-integrated\n    inertial measurements, depth estimates, and measures of photometric rendering\n    quality. We also release a multi-modal dataset, UT-MM, collected from a mobile\n    robot equipped with a camera and an inertial measurement unit. Experimental evaluation\n    on several scenes from the dataset shows that MM3DGS achieves 3x improvement in\n    tracking and 5% improvement in photometric rendering quality compared to the current\n    3DGS SLAM state-of-the-art, while allowing real-time rendering of a high-resolution\n    dense 3D map.\n  project_page: https://vita-group.github.io/MM3DGS-SLAM/\n  paper: https://arxiv.org/pdf/2404.00923\n  code: https://github.com/VITA-Group/MM3DGS-SLAM\n  video: https://www.youtube.com/watch?v=drf6UxehChE&feature=youtu.be\n  tags:\n  - Code\n  - Project\n  - SLAM\n  - Video\n  thumbnail: assets/thumbnails/c2024mm3dgs.jpg\n  publication_date: '2024-04-01T04:57:41+00:00'\n- id: comi20243dgsr\n  title: '3DGSR: Implicit Surface Reconstruction with 3D Gaussian Splatting'\n  authors: Mauro Comi, Alessio Tonioni, Max Yang, Jonathan Tremblay, Valts Blukis,\n    Yijiong Lin, Nathan F. Lepora, Laurence Aitchison\n  year: '2024'\n  abstract: In this paper, we present an implicit surface reconstruction method with\n    3D Gaussian Splatting (3DGS), namely 3DGSR, that allows for accurate 3D reconstruction\n    with intricate details while inheriting the high efficiency and rendering quality\n    of 3DGS. The key insight is incorporating an implicit signed distance field (SDF)\n    within 3D Gaussians to enable them to be aligned and jointly optimized. First,\n    we introduce a differentiable SDF-to-opacity transformation function that converts\n    SDF values into corresponding Gaussians' opacities. This function connects the\n    SDF and 3D Gaussians, allowing for unified optimization and enforcing surface\n    constraints on the 3D Gaussians. During learning, optimizing the 3D Gaussians\n    provides supervisory signals for SDF learning, enabling the reconstruction of\n    intricate details. However, this only provides sparse supervisory signals to the\n    SDF at locations occupied by Gaussians, which is insufficient for learning a continuous\n    SDF. Then, to address this limitation, we incorporate volumetric rendering and\n    align the rendered geometric attributes (depth, normal) with those derived from\n    3D Gaussians. This consistency regularization introduces supervisory signals to\n    locations not covered by discrete 3D Gaussians, effectively eliminating redundant\n    surfaces outside the Gaussian sampling range. Our extensive experimental results\n    demonstrate that our 3DGSR method enables high-quality 3D surface reconstruction\n    while preserving the efficiency and rendering quality of 3DGS. Besides, our method\n    competes favorably with leading surface reconstruction techniques while offering\n    a more efficient learning process and much better rendering qualities.\n  project_page: null\n  paper: https://arxiv.org/pdf/2404.00409.pdf\n  code: null\n  video: null\n  tags:\n  - Meshing\n  - Rendering\n  thumbnail: assets/thumbnails/comi20243dgsr.jpg\n  publication_date: '2024-03-30T16:35:38+00:00'\n- id: fan2024instantsplat\n  title: 'InstantSplat: Unbounded Sparse-view Pose-free Gaussian Splatting in 40 Seconds'\n  authors: Zhiwen Fan, Wenyan Cong, Kairun Wen, Kevin Wang, Jian Zhang, Xinghao Ding,\n    Danfei Xu, Boris Ivanovic, Marco Pavone, Georgios Pavlakos, Zhangyang Wang, Yue\n    Wang\n  year: '2024'\n  abstract: While novel view synthesis (NVS) has made substantial progress in 3D computer\n    vision, it typically requires an initial estimation of camera intrinsics and extrinsics\n    from dense viewpoints. This pre-processing is usually conducted via a Structure-from-Motion\n    (SfM) pipeline, a procedure that can be slow and unreliable, particularly in sparse-view\n    scenarios with insufficient matched features for accurate reconstruction. In this\n    work, we integrate the strengths of point-based representations (e.g., 3D Gaussian\n    Splatting, 3D-GS) with end-to-end dense stereo models (DUSt3R) to tackle the complex\n    yet unresolved issues in NVS under unconstrained settings, which encompasses pose-free\n    and sparse view challenges. Our framework, InstantSplat, unifies dense stereo\n    priors with 3D-GS to build 3D Gaussians of large-scale scenes from sparseview\n    & pose-free images in less than 1 minute. Specifically, InstantSplat comprises\n    a Coarse Geometric Initialization (CGI) module that swiftly establishes a preliminary\n    scene structure and camera parameters across all training views, utilizing globally-aligned\n    3D point maps derived from a pre-trained dense stereo pipeline. This is followed\n    by the Fast 3D-Gaussian Optimization (F-3DGO) module, which jointly optimizes\n    the 3D Gaussian attributes and the initialized poses with pose regularization.\n    Experiments conducted on the large-scale outdoor Tanks & Temples datasets demonstrate\n    that InstantSplat significantly improves SSIM (by 32%) while concurrently reducing\n    Absolute Trajectory Error (ATE) by 80%. These establish InstantSplat as a viable\n    solution for scenarios involving posefree and sparse-view conditions.\n  project_page: https://instantsplat.github.io/\n  paper: https://arxiv.org/pdf/2403.20309.pdf\n  code: https://github.com/NVlabs/InstantSplat\n  video: https://www.youtube.com/live/JdfrG89iPOA?si=JhoiMxrjVIh91Ws1\n  tags:\n  - Code\n  - Project\n  - Sparse\n  - Video\n  thumbnail: assets/thumbnails/fan2024instantsplat.jpg\n  publication_date: '2024-03-29T17:29:58+00:00'\n- id: comi2024snapit\n  title: 'Snap-it, Tap-it, Splat-it: Tactile-Informed 3D Gaussian Splatting for Reconstructing\n    Challenging Surfaces'\n  authors: Mauro Comi, Alessio Tonioni, Max Yang, Jonathan Tremblay, Valts Blukis,\n    Yijiong Lin, Nathan F. Lepora, Laurence Aitchison\n  year: '2024'\n  abstract: Touch and vision go hand in hand, mutually enhancing our ability to understand\n    the world. From a research perspective, the problem of mixing touch and vision\n    is underexplored and presents interesting challenges. To this end, we propose\n    Tactile-Informed 3DGS, a novel approach that incorporates touch data (local depth\n    maps) with multi-view vision data to achieve surface reconstruction and novel\n    view synthesis. Our method optimises 3D Gaussian primitives to accurately model\n    the object's geometry at points of contact. By creating a framework that decreases\n    the transmittance at touch locations, we achieve a refined surface reconstruction,\n    ensuring a uniformly smooth depth map. Touch is particularly useful when considering\n    non-Lambertian objects (e.g. shiny or reflective surfaces) since contemporary\n    methods tend to fail to reconstruct with fidelity specular highlights. By combining\n    vision and tactile sensing, we achieve more accurate geometry reconstructions\n    with fewer images than prior methods. We conduct evaluation on objects with glossy\n    and reflective surfaces and demonstrate the effectiveness of our approach, offering\n    significant improvements in reconstruction quality.\n  project_page: https://maxyang27896.github.io/publication/gaussian_splat/\n  paper: https://arxiv.org/pdf/2403.20275\n  code: null\n  video: null\n  tags:\n  - Project\n  - Rendering\n  thumbnail: assets/thumbnails/comi2024snapit.jpg\n  publication_date: '2024-03-29T16:30:17+00:00'\n- id: wu2024hgsmapping\n  title: 'HGS-Mapping: Online Dense Mapping Using Hybrid Gaussian Representation in\n    Urban Scenes'\n  authors: Ke Wu, Kaizhao Zhang, Zhiwei Zhang, Shanshuai Yuan, Muer Tie, Julong Wei,\n    Zijun Xu, Jieru Zhao, Zhongxue Gan, Wenchao Ding\n  year: '2024'\n  abstract: Online dense mapping of urban scenes forms a fundamental cornerstone for\n    scene understanding and navigation of autonomous vehicles. Recent advancements\n    in mapping methods are mainly based on NeRF, whose rendering speed is too slow\n    to meet online requirements. 3D Gaussian Splatting (3DGS), with its rendering\n    speed hundreds of times faster than NeRF, holds greater potential in online dense\n    mapping. However, integrating 3DGS into a street-view dense mapping framework\n    still faces two challenges, including incomplete reconstruction due to the absence\n    of geometric information beyond the LiDAR coverage area and extensive computation\n    for reconstruction in large urban scenes. To this end, we propose HGS-Mapping,\n    an online dense mapping framework in unbounded large-scale scenes. To attain complete\n    construction, our framework introduces Hybrid Gaussian Representation, which models\n    different parts of the entire scene using Gaussians with distinct properties.\n    Furthermore, we employ a hybrid Gaussian initialization mechanism and an adaptive\n    update method to achieve high-fidelity and rapid reconstruction. To the best of\n    our knowledge, we are the first to integrate Gaussian representation into online\n    dense mapping of urban scenes. Our approach achieves SOTA reconstruction accuracy\n    while only employing 66% number of Gaussians, leading to 20% faster reconstruction\n    speed.\n  project_page: null\n  paper: https://arxiv.org/pdf/2403.20159.pdf\n  code: null\n  video: null\n  tags:\n  - Large-Scale\n  thumbnail: assets/thumbnails/wu2024hgsmapping.jpg\n  publication_date: '2024-03-29T13:16:05+00:00'\n- id: yu2024sgd\n  title: 'SGD: Street View Synthesis with Gaussian Splatting and Diffusion Prior'\n  authors: Zhongrui Yu, Haoran Wang, Jinze Yang, Hanzhang Wang, Zeke Xie, Yunfeng\n    Cai, Jiale Cao, Zhong Ji, Mingming Sun\n  year: '2024'\n  abstract: Novel View Synthesis (NVS) for street scenes play a critical role in the\n    autonomous driving simulation. The current mainstream technique to achieve it\n    is neural rendering, such as Neural Radiance Fields (NeRF) and 3D Gaussian Splatting\n    (3DGS). Although thrilling progress has been made, when handling street scenes,\n    current methods struggle to maintain rendering quality at the viewpoint that deviates\n    significantly from the training viewpoints. This issue stems from the sparse training\n    views captured by a fixed camera on a moving vehicle. To tackle this problem,\n    we propose a novel approach that enhances the capacity of 3DGS by leveraging prior\n    from a Diffusion Model along with complementary multi-modal data. Specifically,\n    we first fine-tune a Diffusion Model by adding images from adjacent frames as\n    condition, meanwhile exploiting depth data from LiDAR point clouds to supply additional\n    spatial information. Then we apply the Diffusion Model to regularize the 3DGS\n    at unseen views during training. Experimental results validate the effectiveness\n    of our method compared with current state-of-the-art models, and demonstrate its\n    advance in rendering images from broader views.\n  project_page: null\n  paper: https://arxiv.org/pdf/2403.20079.pdf\n  code: null\n  video: null\n  tags:\n  - Diffusion\n  - Lidar\n  - Sparse\n  thumbnail: assets/thumbnails/yu2024sgd.jpg\n  publication_date: '2024-03-29T09:20:29+00:00'\n- id: li2024hogaussian\n  title: 'HO-Gaussian: Hybrid Optimization of 3D Gaussian Splatting for Urban Scenes'\n  authors: Zhuopeng Li, Yilin Zhang, Chenming Wu, Jianke Zhu, Liangjun Zhang\n  year: '2024'\n  abstract: The rapid growth of 3D Gaussian Splatting (3DGS) has revolutionized neural\n    rendering, enabling real-time production of high-quality renderings. However,\n    the previous 3DGS-based methods have limitations in urban scenes due to reliance\n    on initial Structure-from-Motion(SfM) points and difficulties in rendering distant,\n    sky and low-texture areas. To overcome these challenges, we propose a hybrid optimization\n    method named HO-Gaussian, which combines a grid-based volume with the 3DGS pipeline.\n    HO-Gaussian eliminates the dependency on SfM point initialization, allowing for\n    rendering of urban scenes, and incorporates the Point Densitification to enhance\n    rendering quality in problematic regions during training. Furthermore, we introduce\n    Gaussian Direction Encoding as an alternative for spherical harmonics in the rendering\n    pipeline, which enables view-dependent color representation. To account for multi-camera\n    systems, we introduce neural warping to enhance object consistency across different\n    cameras. Experimental results on widely used autonomous driving datasets demonstrate\n    that HO-Gaussian achieves photo-realistic rendering in real-time on multi-camera\n    urban datasets.\n  project_page: null\n  paper: https://arxiv.org/pdf/2403.20032.pdf\n  code: null\n  video: null\n  tags:\n  - Densification\n  - Misc\n  thumbnail: assets/thumbnails/li2024hogaussian.jpg\n  publication_date: '2024-03-29T07:58:21+00:00'\n- id: zhang2024gaussiancube\n  title: 'GaussianCube: A Structured and Explicit Radiance Representation for 3D Generative\n    Modeling'\n  authors: Bowen Zhang, Yiji Cheng, Jiaolong Yang, Chunyu Wang, Feng Zhao, Yansong\n    Tang, Dong Chen, Baining Guo\n  year: '2024'\n  abstract: We introduce a radiance representation that is both structured and fully\n    explicit and thus greatly facilitates 3D generative modeling. Existing radiance\n    representations either require an implicit feature decoder, which significantly\n    degrades the modeling power of the representation, or are spatially unstructured,\n    making them difficult to integrate with mainstream 3D diffusion methods. We derive\n    GaussianCube by first using a novel densification-constrained Gaussian fitting\n    algorithm, which yields high-accuracy fitting using a fixed number of free Gaussians,\n    and then rearranging these Gaussians into a predefined voxel grid via Optimal\n    Transport. Since GaussianCube is a structured grid representation, it allows us\n    to use standard 3D U-Net as our backbone in diffusion modeling without elaborate\n    designs. More importantly, the high-accuracy fitting of the Gaussians allows us\n    to achieve a high-quality representation with orders of magnitude fewer parameters\n    than previous structured representations for comparable quality, ranging from\n    one to two orders of magnitude. The compactness of GaussianCube greatly eases\n    the difficulty of 3D generative modeling. Extensive experiments conducted on unconditional\n    and class-conditioned object generation, digital avatar creation, and text-to-3D\n    synthesis all show that our model achieves state-of-the-art generation results\n    both qualitatively and quantitatively, underscoring the potential of GaussianCube\n    as a highly accurate and versatile radiance representation for 3D generative modeling.\n  project_page: https://gaussiancube.github.io/\n  paper: https://arxiv.org/pdf/2403.19655.pdf\n  code: https://github.com/GaussianCube/GaussianCube\n  video: https://youtu.be/3uo4Oud4cxI\n  tags:\n  - Code\n  - Diffusion\n  - Project\n  - Video\n  thumbnail: assets/thumbnails/zhang2024gaussiancube.jpg\n  publication_date: '2024-03-28T17:59:50+00:00'\n- id: song2024sags\n  title: 'SA-GS: Scale-Adaptive Gaussian Splatting for Training-Free Anti-Aliasing'\n  authors: Xiaowei Song, Jv Zheng, Shiran Yuan, Huan-ang Gao, Jingwei Zhao, Xiang\n    He, Weihao Gu, Hao Zhao\n  year: '2024'\n  abstract: In this paper, we present a Scale-adaptive method for Anti-aliasing Gaussian\n    Splatting (SA-GS). While the state-of-the-art method Mip-Splatting needs modifying\n    the training procedure of Gaussian splatting, our method functions at test-time\n    and is training-free. Specifically, SA-GS can be applied to any pretrained Gaussian\n    splatting field as a plugin to significantly improve the field's anti-alising\n    performance. The core technique is to apply 2D scale-adaptive filters to each\n    Gaussian during test time. As pointed out by Mip-Splatting, observing Gaussians\n    at different frequencies leads to mismatches between the Gaussian scales during\n    training and testing. Mip-Splatting resolves this issue using 3D smoothing and\n    2D Mip filters, which are unfortunately not aware of testing frequency. In this\n    work, we show that a 2D scale-adaptive filter that is informed of testing frequency\n    can effectively match the Gaussian scale, thus making the Gaussian primitive distribution\n    remain consistent across different testing frequencies. When scale inconsistency\n    is eliminated, sampling rates smaller than the scene frequency result in conventional\n    jaggedness, and we propose to integrate the projected 2D Gaussian within each\n    pixel during testing. This integration is actually a limiting case of super-sampling,\n    which significantly improves anti-aliasing performance over vanilla Gaussian Splatting.\n    Through extensive experiments using various settings and both bounded and unbounded\n    scenes, we show SA-GS performs comparably with or better than Mip-Splatting. Note\n    that super-sampling and integration are only effective when our scale-adaptive\n    filtering is activated.\n  project_page: https://kevinsong729.github.io/project-pages/SA-GS/\n  paper: https://arxiv.org/pdf/2403.19615\n  code: https://github.com/zsy1987/SA-GS/\n  video: null\n  tags:\n  - Antialiasing\n  - Code\n  - Project\n  - Rendering\n  thumbnail: assets/thumbnails/song2024sags.jpg\n  publication_date: '2024-03-28T17:32:58+00:00'\n- id: zhang2024togs\n  title: 'TOGS: Gaussian Splatting with Temporal Opacity Offset for Real-Time 4D DSA\n    Rendering'\n  authors: Shuai Zhang, Huangxuan Zhao, Zhenghong Zhou, Guanjun Wu, Chuansheng Zheng,\n    Xinggang Wang, Wenyu Liu\n  year: '2024'\n  abstract: Four-dimensional Digital Subtraction Angiography (4D DSA) is a medical\n    imaging technique that provides a series of 2D images captured at different stages\n    and angles during the process of contrast agent filling blood vessels. It plays\n    a significant role in the diagnosis of cerebrovascular diseases. Improving the\n    rendering quality and speed under sparse sampling is important for observing the\n    status and location of lesions. The current methods exhibit inadequate rendering\n    quality in sparse views and suffer from slow rendering speed. To overcome these\n    limitations, we propose TOGS, a Gaussian splatting method with opacity offset\n    over time, which can effectively improve the rendering quality and speed of 4D\n    DSA. We introduce an opacity offset table for each Gaussian to model the temporal\n    variations in the radiance of the contrast agent. By interpolating the opacity\n    offset table, the opacity variation of the Gaussian at different time points can\n    be determined. This enables us to render the 2D DSA image at that specific moment.\n    Additionally, we introduced a Smooth loss term in the loss function to mitigate\n    overfitting issues that may arise in the model when dealing with sparse view scenarios.\n    During the training phase, we randomly prune Gaussians, thereby reducing the storage\n    overhead of the model. The experimental results demonstrate that compared to previous\n    methods, this model achieves state-of-the-art reconstruction quality under the\n    same number of training views. Additionally, it enables real-time rendering while\n    maintaining low storage overhead.\n  project_page: null\n  paper: https://arxiv.org/pdf/2403.19586\n  code: https://github.com/hustvl/TOGS\n  video: null\n  tags:\n  - Code\n  - Medicine\n  thumbnail: assets/thumbnails/zhang2024togs.jpg\n  publication_date: '2024-03-28T17:08:58+00:00'\n- id: paliwal2024coherentgs\n  title: 'CoherentGS: Sparse Novel View Synthesis with Coherent 3D Gaussians'\n  authors: Avinash Paliwal, Wei Ye, Jinhui Xiong, Dmytro Kotovenko, Rakesh Ranjan,\n    Vikas Chandra, Nima Khademi Kalantari\n  year: '2024'\n  abstract: The field of 3D reconstruction from images has rapidly evolved in the\n    past few years, first with the introduction of Neural Radiance Field (NeRF) and\n    more recently with 3D Gaussian Splatting (3DGS). The latter provides a significant\n    edge over NeRF in terms of the training and inference speed, as well as the reconstruction\n    quality. Although 3DGS works well for dense input images, the unstructured point-cloud\n    like representation quickly overfits to the more challenging setup of extremely\n    sparse input images (e.g., 3 images), creating a representation that appears as\n    a jumble of needles from novel views. To address this issue, we propose regularized\n    optimization and depth-based initialization. Our key idea is to introduce a structured\n    Gaussian representation that can be controlled in 2D image space. We then constraint\n    the Gaussians, in particular their position, and prevent them from moving independently\n    during optimization. Specifically, we introduce single and multiview constraints\n    through an implicit convolutional decoder and a total variation loss, respectively.\n    With the coherency introduced to the Gaussians, we further constrain the optimization\n    through a flow-based loss function. To support our regularized optimization, we\n    propose an approach to initialize the Gaussians using monocular depth estimates\n    at each input view. We demonstrate significant improvements compared to the state-of-the-art\n    sparse-view NeRF-based approaches on a variety of scenes.\n  project_page: https://people.engr.tamu.edu/nimak/Papers/CoherentGS/index.html\n  paper: https://arxiv.org/pdf/2403.19495\n  code: https://github.com/avinashpaliwal/CoherentGS\n  video: https://www.youtube.com/watch?v=qiWdD3tOHKM\n  tags:\n  - Code\n  - Project\n  - Sparse\n  - Video\n  thumbnail: assets/thumbnails/paliwal2024coherentgs.jpg\n  publication_date: '2024-03-28T15:27:13+00:00'\n- id: shen2024gamba\n  title: 'Gamba: Marry Gaussian Splatting with Mamba for single view 3D reconstruction'\n  authors: Qiuhong Shen, Xuanyu Yi, Zike Wu, Pan Zhou, Hanwang Zhang, Shuicheng Yan,\n    Xinchao Wang\n  year: '2024'\n  abstract: 'We tackle the challenge of efficiently reconstructing a 3D asset from\n    a single image with growing demands for automated 3D content creation pipelines.\n    Previous methods primarily rely on Score Distillation Sampling (SDS) and Neural\n    Radiance Fields (NeRF). Despite their significant success, these approaches encounter\n    practical limitations due to lengthy optimization and considerable memory usage.\n    In this report, we introduce Gamba, an end-to-end amortized 3D reconstruction\n    model from single-view images, emphasizing two main insights: (1) 3D representation:\n    leveraging a large number of 3D Gaussians for an efficient 3D Gaussian splatting\n    process; (2) Backbone design: introducing a Mamba-based sequential network that\n    facilitates context-dependent reasoning and linear scalability with the sequence\n    (token) length, accommodating a substantial number of Gaussians. Gamba incorporates\n    significant advancements in data preprocessing, regularization design, and training\n    methodologies. We assessed Gamba against existing optimization-based and feed-forward\n    3D generation approaches using the real-world scanned OmniObject3D dataset. Here,\n    Gamba demonstrates competitive generation capabilities, both qualitatively and\n    quantitatively, while achieving remarkable speed, approximately 0.6 second on\n    a single NVIDIA A100 GPU.'\n  project_page: https://florinshen.github.io/gamba-project/\n  paper: https://arxiv.org/pdf/2403.18795\n  code: https://github.com/SkyworkAI/Gamba\n  video: null\n  tags:\n  - Code\n  - Feed-Forward\n  - Project\n  - Sparse\n  thumbnail: assets/thumbnails/shen2024gamba.jpg\n  publication_date: '2024-03-27T17:40:14+00:00'\n- id: shao2024splatface\n  title: 'SplatFace: Gaussian Splat Face Reconstruction Leveraging an Optimizable\n    Surface'\n  authors: Zhijing Shao, Zhaolong Wang, Zhuang Li, Duotun Wang, Xiangru Lin, Yu Zhang,\n    Mingming Fan, Zeyu Wang\n  year: '2024'\n  abstract: We present SplatFace, a novel Gaussian splatting framework designed for\n    3D human face reconstruction without reliance on accurate pre-determined geometry.\n    Our method is designed to simultaneously deliver both high-quality novel view\n    rendering and accurate 3D mesh reconstructions. We incorporate a generic 3D Morphable\n    Model (3DMM) to provide a surface geometric structure, making it possible to reconstruct\n    faces with a limited set of input images. We introduce a joint optimization strategy\n    that refines both the Gaussians and the morphable surface through a synergistic\n    non-rigid alignment process. A novel distance metric, splat-to-surface, is proposed\n    to improve alignment by considering both the Gaussian position and covariance.\n    The surface information is also utilized to incorporate a world-space densification\n    process, resulting in superior reconstruction quality. Our experimental analysis\n    demonstrates that the proposed method is competitive with both other Gaussian\n    splatting techniques in novel view synthesis and other 3D reconstruction methods\n    in producing 3D face meshes with high geometric precision.\n  project_page: null\n  paper: https://arxiv.org/pdf/2403.18784\n  code: null\n  video: null\n  tags:\n  - Avatar\n  thumbnail: assets/thumbnails/shao2024splatface.jpg\n  publication_date: '2024-03-27T17:32:04+00:00'\n- id: savant2024modeling\n  title: Modeling uncertainty for Gaussian Splatting\n  authors: Luca Savant, Diego Valsesia, Enrico Magli\n  year: '2024'\n  abstract: 'We present Stochastic Gaussian Splatting (SGS): the first framework for\n    uncertainty estimation using Gaussian Splatting (GS). GS recently advanced the\n    novel-view synthesis field by achieving impressive reconstruction quality at a\n    fraction of the computational cost of Neural Radiance Fields (NeRF). However,\n    contrary to the latter, it still lacks the ability to provide information about\n    the confidence associated with their outputs. To address this limitation, in this\n    paper, we introduce a Variational Inference-based approach that seamlessly integrates\n    uncertainty prediction into the common rendering pipeline of GS. Additionally,\n    we introduce the Area Under Sparsification Error (AUSE) as a new term in the loss\n    function, enabling optimization of uncertainty estimation alongside image reconstruction.\n    Experimental results on the LLFF dataset demonstrate that our method outperforms\n    existing approaches in terms of both image rendering quality and uncertainty estimation\n    accuracy. Overall, our framework equips practitioners with valuable insights into\n    the reliability of synthesized views, facilitating safer decision-making in real-world\n    applications.'\n  project_page: null\n  paper: https://arxiv.org/pdf/2403.18476\n  code: null\n  video: null\n  tags:\n  - Uncertainty\n  thumbnail: assets/thumbnails/savant2024modeling.jpg\n  publication_date: '2024-03-27T11:45:08+00:00'\n- id: gu2024egolifter\n  title: 'EgoLifter: Open-world 3D Segmentation for Egocentric Perception'\n  authors: Qiao Gu, Zhaoyang Lv, Duncan Frost, Simon Green, Julian Straub, Chris Sweeney\n  year: '2024'\n  abstract: In this paper we present EgoLifter, a novel system that can automatically\n    segment scenes captured from egocentric sensors into a complete decomposition\n    of individual 3D objects. The system is specifically designed for egocentric data\n    where scenes contain hundreds of objects captured from natural (non-scanning)\n    motion. EgoLifter adopts 3D Gaussians as the underlying representation of 3D scenes\n    and objects and uses segmentation masks from the Segment Anything Model (SAM)\n    as weak supervision to learn flexible and promptable definitions of object instances\n    free of any specific object taxonomy. To handle the challenge of dynamic objects\n    in ego-centric videos, we design a transient prediction module that learns to\n    filter out dynamic objects in the 3D reconstruction. The result is a fully automatic\n    pipeline that is able to reconstruct 3D object instances as collections of 3D\n    Gaussians that collectively compose the entire scene. We created a new benchmark\n    on the Aria Digital Twin dataset that quantitatively demonstrates its state-of-the-art\n    performance in open-world 3D segmentation from natural egocentric input. We run\n    EgoLifter on various egocentric activity datasets which shows the promise of the\n    method for 3D egocentric perception at scale.\n  project_page: https://egolifter.github.io/\n  paper: https://arxiv.org/pdf/2403.18118.pdf\n  code: https://github.com/facebookresearch/egolifter\n  video: https://www.youtube.com/watch?v=dWuZyeiOXyM&feature=youtu.be\n  tags:\n  - Code\n  - Project\n  - Segmentation\n  - Video\n  thumbnail: assets/thumbnails/gu2024egolifter.jpg\n  publication_date: '2024-03-26T21:48:27+00:00'\n- id: ren2024octreegs\n  title: 'Octree-GS: Towards Consistent Real-time Rendering with LOD-Structured 3D\n    Gaussians'\n  authors: Kerui Ren, Lihan Jiang, Tao Lu, Mulin Yu, Linning Xu, Zhangkai Ni, Bo Dai\n  year: '2024'\n  abstract: The recent 3D Gaussian splatting (3D-GS) has shown remarkable rendering\n    fidelity and efficiency compared to NeRF-based neural scene representations. While\n    demonstrating the potential for real-time rendering, 3D-GS encounters rendering\n    bottlenecks in large scenes with complex details due to an excessive number of\n    Gaussian primitives located within the viewing frustum. This limitation is particularly\n    noticeable in zoom-out views and can lead to inconsistent rendering speeds in\n    scenes with varying details. Moreover, it often struggles to capture the corresponding\n    level of details at different scales with its heuristic density control operation.\n    Inspired by the Level-of-Detail (LOD) techniques, we introduce Octree-GS, featuring\n    an LOD-structured 3D Gaussian approach supporting level-of-detail decomposition\n    for scene representation that contributes to the final rendering results. Our\n    model dynamically selects the appropriate level from the set of multi-resolution\n    anchor points, ensuring consistent rendering performance with adaptive LOD adjustments\n    while maintaining high-fidelity rendering results.\n  project_page: https://city-super.github.io/octree-gs/\n  paper: https://arxiv.org/pdf/2403.17898\n  code: https://github.com/city-super/Octree-GS\n  video: null\n  tags:\n  - Code\n  - Large-Scale\n  - Project\n  - Rendering\n  thumbnail: assets/thumbnails/ren2024octreegs.jpg\n  publication_date: '2024-03-26T17:39:36+00:00'\n- id: huang20242d\n  title: 2D Gaussian Splatting for Geometrically Accurate Radiance Fields\n  authors: Binbin Huang, Zehao Yu, Anpei Chen, Andreas Geiger, Shenghua Gao\n  year: '2024'\n  abstract: 3D Gaussian Splatting (3DGS) has recently revolutionized radiance field\n    reconstruction, achieving high quality novel view synthesis and fast rendering\n    speed without baking. However, 3DGS fails to accurately represent surfaces due\n    to the multi-view inconsistent nature of 3D Gaussians. We present 2D Gaussian\n    Splatting (2DGS), a novel approach to model and reconstruct geometrically accurate\n    radiance fields from multi-view images. Our key idea is to collapse the 3D volume\n    into a set of 2D oriented planar Gaussian disks. Unlike 3D Gaussians, 2D Gaussians\n    provide view-consistent geometry while modeling surfaces intrinsically. To accurately\n    recover thin surfaces and achieve stable optimization, we introduce a perspective-accurate\n    2D splatting process utilizing ray-splat intersection and rasterization. Additionally,\n    we incorporate depth distortion and normal consistency terms to further enhance\n    the quality of the reconstructions. We demonstrate that our differentiable renderer\n    allows for noise-free and detailed geometry reconstruction while maintaining competitive\n    appearance quality, fast training speed, and real-time rendering.\n  project_page: https://surfsplatting.github.io/\n  paper: https://arxiv.org/pdf/2403.17888\n  code: https://github.com/hbb1/2d-gaussian-splatting\n  video: https://www.youtube.com/watch?v=oaHCtB6yiKU\n  tags:\n  - 2DGS\n  - Code\n  - Meshing\n  - Project\n  - Video\n  thumbnail: assets/thumbnails/huang20242d.jpg\n  publication_date: '2024-03-26T17:21:24+00:00'\n- id: turkulainen2024dnsplatter\n  title: 'DN-Splatter: Depth and Normal Priors for Gaussian Splatting and Meshing'\n  authors: Matias Turkulainen, Xuqian Ren, Iaroslav Melekhov, Otto Seiskari, Esa Rahtu,\n    Juho Kannala\n  year: '2024'\n  abstract: 3D Gaussian splatting, a novel differentiable rendering technique, has\n    achieved state-of-the-art novel view synthesis results with high rendering speeds\n    and relatively low training times. However, its performance on scenes commonly\n    seen in indoor datasets is poor due to the lack of geometric constraints during\n    optimization. We extend 3D Gaussian splatting with depth and normal cues to tackle\n    challenging indoor datasets and showcase techniques for efficient mesh extraction,\n    an important downstream application. Specifically, we regularize the optimization\n    procedure with depth information, enforce local smoothness of nearby Gaussians,\n    and use the geometry of the 3D Gaussians supervised by normal cues to achieve\n    better alignment with the true scene geometry. We improve depth estimation and\n    novel view synthesis results over baselines and show how this simple yet effective\n    regularization technique can be used to directly extract meshes from the Gaussian\n    representation yielding more physically accurate reconstructions on indoor scenes.\n  project_page: https://maturk.github.io/dn-splatter/\n  paper: https://arxiv.org/pdf/2403.17822\n  code: https://github.com/maturk/dn-splatter\n  video: null\n  tags:\n  - Code\n  - Meshing\n  - Project\n  thumbnail: assets/thumbnails/turkulainen2024dnsplatter.jpg\n  publication_date: '2024-03-26T16:00:31+00:00'\n- id: lin2024dreampolisher\n  title: 'DreamPolisher: Towards High-Quality Text-to-3D Generation via Geometric\n    Diffusion'\n  authors: Yuanze Lin, Ronald Clark, Philip Torr\n  year: '2024'\n  abstract: We present DreamPolisher, a novel Gaussian Splatting based method with\n    geometric guidance, tailored to learn cross-view consistency and intricate detail\n    from textual descriptions. While recent progress on text-to-3D generation methods\n    have been promising, prevailing methods often fail to ensure view-consistency\n    and textural richness. This problem becomes particularly noticeable for methods\n    that work with text input alone. To address this, we propose a two-stage Gaussian\n    Splatting based approach that enforces geometric consistency among views. Initially,\n    a coarse 3D generation undergoes refinement via geometric optimization. Subsequently,\n    we use a ControlNet driven refiner coupled with the geometric consistency term\n    to improve both texture fidelity and overall consistency of the generated 3D asset.\n    Empirical evaluations across diverse textual prompts spanning various object categories\n    demonstrate the efficacy of DreamPolisher in generating consistent and realistic\n    3D objects, aligning closely with the semantics of the textual instructions.\n  project_page: https://yuanze-lin.me/DreamPolisher_page/\n  paper: https://arxiv.org/pdf/2403.17237\n  code: https://github.com/yuanze-lin/DreamPolisher\n  video: https://youtu.be/YJkFMIV2OyQ\n  tags:\n  - Code\n  - Diffusion\n  - Project\n  - Video\n  thumbnail: assets/thumbnails/lin2024dreampolisher.jpg\n  publication_date: '2024-03-25T22:34:05+00:00'\n- id: xu2024comp4d\n  title: 'Comp4D: LLM-Guided Compositional 4D Scene Generation'\n  authors: Dejia Xu, Hanwen Liang, Neel P. Bhatt, Hezhen Hu, Hanxue Liang, Konstantinos\n    N. Plataniotis, Zhangyang Wang\n  year: '2024'\n  abstract: Recent advancements in diffusion models for 2D and 3D content creation\n    have sparked a surge of interest in generating 4D content. However, the scarcity\n    of 3D scene datasets constrains current methodologies to primarily object-centric\n    generation. To overcome this limitation, we present Comp4D, a novel framework\n    for Compositional 4D Generation. Unlike conventional methods that generate a singular\n    4D representation of the entire scene, Comp4D innovatively constructs each 4D\n    object within the scene separately. Utilizing Large Language Models (LLMs), the\n    framework begins by decomposing an input text prompt into distinct entities and\n    maps out their trajectories. It then constructs the compositional 4D scene by\n    accurately positioning these objects along their designated paths. To refine the\n    scene, our method employs a compositional score distillation technique guided\n    by the pre-defined trajectories, utilizing pre-trained diffusion models across\n    text-to-image, text-to-video, and text-to-3D domains. Extensive experiments demonstrate\n    our outstanding 4D content creation capability compared to prior arts, showcasing\n    superior visual quality, motion fidelity, and enhanced object interactions.\n  project_page: https://vita-group.github.io/Comp4D/\n  paper: https://arxiv.org/pdf/2403.16993.pdf\n  code: https://github.com/VITA-Group/Comp4D\n  video: https://youtu.be/9q8SV1Xf_Xw\n  tags:\n  - Code\n  - Diffusion\n  - Project\n  - Video\n  thumbnail: assets/thumbnails/xu2024comp4d.jpg\n  publication_date: '2024-03-25T17:55:52+00:00'\n- id: yu2024gsdf\n  title: 'GSDF: 3DGS Meets SDF for Improved Rendering and Reconstruction'\n  authors: Mulin Yu, Tao Lu, Linning Xu, Lihan Jiang, Yuanbo Xiangli, Bo Dai\n  year: '2024'\n  abstract: Presenting a 3D scene from multiview images remains a core and long-standing\n    challenge in computer vision and computer graphics. Two main requirements lie\n    in rendering and reconstruction. Notably, SOTA rendering quality is usually achieved\n    with neural volumetric rendering techniques, which rely on aggregated point/primitive-wise\n    color and neglect the underlying scene geometry. Learning of neural implicit surfaces\n    is sparked from the success of neural rendering. Current works either constrain\n    the distribution of density fields or the shape of primitives, resulting in degraded\n    rendering quality and flaws on the learned scene surfaces. The efficacy of such\n    methods is limited by the inherent constraints of the chosen neural representation,\n    which struggles to capture fine surface details, especially for larger, more intricate\n    scenes. To address these issues, we introduce GSDF, a novel dual-branch architecture\n    that combines the benefits of a flexible and efficient 3D Gaussian Splatting (3DGS)\n    representation with neural Signed Distance Fields (SDF). The core idea is to leverage\n    and enhance the strengths of each branch while alleviating their limitation through\n    mutual guidance and joint supervision. We show on diverse scenes that our design\n    unlocks the potential for more accurate and detailed surface reconstructions,\n    and at the meantime benefits 3DGS rendering with structures that are more aligned\n    with the underlying geometry.\n  project_page: https://city-super.github.io/GSDF/\n  paper: https://arxiv.org/pdf/2403.16964\n  code: https://github.com/city-super/GSDF\n  video: null\n  tags:\n  - Code\n  - Meshing\n  - Project\n  - Rendering\n  thumbnail: assets/thumbnails/yu2024gsdf.jpg\n  publication_date: '2024-03-25T17:22:11+00:00'\n- id: wewer2024latentsplat\n  title: 'latentSplat: Autoencoding Variational Gaussians for Fast Generalizable 3D\n    Reconstruction'\n  authors: Christopher Wewer, Kevin Raj, Eddy Ilg, Bernt Schiele, Jan Eric Lenssen\n  year: '2024'\n  abstract: We present latentSplat, a method to predict semantic Gaussians in a 3D\n    latent space that can be splatted and decoded by a light-weight generative 2D\n    architecture. Existing methods for generalizable 3D reconstruction either do not\n    enable fast inference of high resolution novel views due to slow volume rendering,\n    or are limited to interpolation of close input views, even in simpler settings\n    with a single central object, where 360-degree generalization is possible. In\n    this work, we combine a regression-based approach with a generative model, moving\n    towards both of these capabilities within the same method, trained purely on readily\n    available real video data. The core of our method are variational 3D Gaussians,\n    a representation that efficiently encodes varying uncertainty within a latent\n    space consisting of 3D feature Gaussians. From these Gaussians, specific instances\n    can be sampled and rendered via efficient Gaussian splatting and a fast, generative\n    decoder network. We show that latentSplat outperforms previous works in reconstruction\n    quality and generalization, while being fast and scalable to high-resolution data.\n  project_page: https://geometric-rl.mpi-inf.mpg.de/latentsplat/\n  paper: https://arxiv.org/pdf/2403.16292.pdf\n  code: https://github.com/Chrixtar/latentsplat\n  video: null\n  tags:\n  - Feed-Forward\n  - Sparse\n  thumbnail: assets/thumbnails/wewer2024latentsplat.jpg\n  publication_date: '2024-03-24T20:48:36+00:00'\n- id: hu2024cgslam\n  title: 'CG-SLAM: Efficient Dense RGB-D SLAM in a Consistent Uncertainty-aware 3D\n    Gaussian Field'\n  authors: Jiarui Hu, Xianhao Chen, Boyin Feng, Guanglin Li, Liangjing Yang, Hujun\n    Bao, Guofeng Zhang, Zhaopeng Cui\n  year: '2024'\n  abstract: Recently neural radiance fields (NeRF) have been widely exploited as 3D\n    representations for dense simultaneous localization and mapping (SLAM). Despite\n    their notable successes in surface modeling and novel view synthesis, existing\n    NeRF-based methods are hindered by their computationally intensive and time-consuming\n    volume rendering pipeline. This paper presents an efficient dense RGB-D SLAM system,\n    i.e., CG-SLAM, based on a novel uncertainty-aware 3D Gaussian field with high\n    consistency and geometric stability. Through an in-depth analysis of Gaussian\n    Splatting, we propose several techniques to construct a consistent and stable\n    3D Gaussian field suitable for tracking and mapping. Additionally, a novel depth\n    uncertainty model is proposed to ensure the selection of valuable Gaussian primitives\n    during optimization, thereby improving tracking efficiency and accuracy. Experiments\n    on various datasets demonstrate that CG-SLAM achieves superior tracking and mapping\n    performance with a notable tracking speed of up to 15 Hz. We will make our source\n    code publicly available.\n  project_page: https://zju3dv.github.io/cg-slam/\n  paper: https://arxiv.org/pdf/2403.16095\n  code: null\n  video: https://raw.githubusercontent.com/hjr37/open_access_assets/main/cg-slam/video/cg-slam-show.mp4\n  tags:\n  - Project\n  - SLAM\n  - Video\n  thumbnail: assets/thumbnails/hu2024cgslam.jpg\n  publication_date: '2024-03-24T11:19:59+00:00'\n- id: zhang2024gaussian\n  title: 'Gaussian in the Wild: 3D Gaussian Splatting for Unconstrained Image Collections'\n  authors: Dongbin Zhang, Chuming Wang, Weitao Wang, Peihao Li, Minghan Qin, Haoqian\n    Wang\n  year: '2024'\n  abstract: Novel view synthesis from unconstrained in-the-wild images remains a meaningful\n    but challenging task. The photometric variation and transient occluders in those\n    unconstrained images make it difficult to reconstruct the original scene accurately.\n    Previous approaches tackle the problem by introducing a global appearance feature\n    in Neural Radiance Fields (NeRF). However, in the real world, the unique appearance\n    of each tiny point in a scene is determined by its independent intrinsic material\n    attributes and the varying environmental impacts it receives. Inspired by this\n    fact, we propose Gaussian in the wild (GS-W), a method that uses 3D Gaussian points\n    to reconstruct the scene and introduces separated intrinsic and dynamic appearance\n    feature for each point, capturing the unchanged scene appearance along with dynamic\n    variation like illumination and weather. Additionally, an adaptive sampling strategy\n    is presented to allow each Gaussian point to focus on the local and detailed information\n    more effectively. We also reduce the impact of transient occluders using a 2D\n    visibility map. More experiments have demonstrated better reconstruction quality\n    and details of GS-W compared to previous methods, with a 1000× increase in rendering\n    speed.\n  project_page: https://eastbeanzhang.github.io/GS-W/\n  paper: https://arxiv.org/pdf/2403.15704\n  code: https://github.com/EastbeanZhang/Gaussian-Wild\n  video: https://www.youtube.com/watch?v=BNIX-OmIzgo\n  tags:\n  - Code\n  - In the Wild\n  - Project\n  - Rendering\n  - Video\n  thumbnail: assets/thumbnails/zhang2024gaussian.jpg\n  publication_date: '2024-03-23T03:55:41+00:00'\n- id: guo2024semantic\n  title: 'Semantic Gaussians: Open-Vocabulary Scene Understanding with 3D Gaussian\n    Splatting'\n  authors: Jun Guo, Xiaojian Ma, Yue Fan, Huaping Liu, Qing Li\n  year: '2024'\n  abstract: 'Open-vocabulary 3D scene understanding presents a significant challenge\n    in computer vision, withwide-ranging applications in embodied agents and augmented\n    reality systems. Previous approaches haveadopted Neural Radiance Fields (NeRFs)\n    to analyze 3D scenes. In this paper, we introduce SemanticGaussians, a novel open-vocabulary\n    scene understanding approach based on 3D Gaussian Splatting. Our keyidea is distilling\n    pre-trained 2D semantics into 3D Gaussians. We design a versatile projection approachthat\n    maps various 2Dsemantic features from pre-trained image encoders into a novel\n    semantic component of 3D Gaussians, withoutthe additional training required by\n    NeRFs. We further build a 3D semantic network that directly predictsthe semantic\n    component from raw 3D Gaussians for fast inference. We explore several applications\n    ofSemantic Gaussians: semantic segmentation on ScanNet-20, where our approach\n    attains a 4.2% mIoU and 4.0%mAcc improvement over prior open-vocabulary scene\n    understanding counterparts; object part segmentation,sceneediting, and spatial-temporal\n    segmentation with better qualitative results over 2D and 3D baselines,highlighting\n    its versatility and effectiveness on supporting diverse downstream tasks.'\n  project_page: https://semantic-gaussians.github.io/\n  paper: https://arxiv.org/pdf/2403.15624\n  code: https://github.com/sharinka0715/semantic-gaussians\n  video: null\n  tags:\n  - Code\n  - Language Embedding\n  - Project\n  - Segmentation\n  thumbnail: assets/thumbnails/guo2024semantic.jpg\n  publication_date: '2024-03-22T21:28:19+00:00'\n- id: zhang2024pixelgs\n  title: 'Pixel-GS: Density Control with Pixel-aware Gradient for 3D Gaussian Splatting'\n  authors: Zheng Zhang, Wenbo Hu, Yixing Lao, Tong He, Hengshuang Zhao\n  year: '2024'\n  abstract: 3D Gaussian Splatting (3DGS) has demonstrated impressive novel view synthesis\n    results while advancing real-time rendering performance. However, it relies heavily\n    on the quality of the initial point cloud, resulting in blurring and needle-like\n    artifacts in areas with insufficient initializing points. This is mainly attributed\n    to the point cloud growth condition in 3DGS that only considers the average gradient\n    magnitude of points from observable views, thereby failing to grow for large Gaussians\n    that are observable for many viewpoints while many of them are only covered in\n    the boundaries. To this end, we propose a novel method, named Pixel-GS, to take\n    into account the number of pixels covered by the Gaussian in each view during\n    the computation of the growth condition. We regard the covered pixel numbers as\n    the weights to dynamically average the gradients from different views, such that\n    the growth of large Gaussians can be prompted. As a result, points within the\n    areas with insufficient initializing points can be grown more effectively, leading\n    to a more accurate and detailed reconstruction. In addition, we propose a simple\n    yet effective strategy to scale the gradient field according to the distance to\n    the camera, to suppress the growth of floaters near the camera. Extensive experiments\n    both qualitatively and quantitatively demonstrate that our method achieves state-of-the-art\n    rendering quality while maintaining real-time rendering speed, on the challenging\n    Mip-NeRF 360 and Tanks & Temples datasets.\n  project_page: https://pixelgs.github.io/\n  paper: https://arxiv.org/pdf/2403.15530.pdf\n  code: https://github.com/zhengzhang01/Pixel-GS\n  video: null\n  tags:\n  - Code\n  - Densification\n  - Project\n  - Rendering\n  thumbnail: assets/thumbnails/zhang2024pixelgs.jpg\n  publication_date: '2024-03-22T17:59:21+00:00'\n- id: wang2024endogslam\n  title: 'EndoGSLAM: Real-Time Dense Reconstruction and Tracking in Endoscopic Surgeries\n    using Gaussian Splatting'\n  authors: Kailing Wang, Chen Yang, Yuehao Wang, Sikuang Li, Yan Wang, Qi Dou, Xiaokang\n    Yang, Wei Shen\n  year: '2024'\n  abstract: Precise camera tracking, high-fidelity 3D tissue reconstruction, and real-time\n    online visualization are critical for intrabody medical imaging devices such as\n    endoscopes and capsule robots. However, existing SLAM (Simultaneous Localization\n    and Mapping) methods often struggle to achieve both complete high-quality surgical\n    field reconstruction and efficient computation, restricting their intraoperative\n    applications among endoscopic surgeries. In this paper, we introduce EndoGSLAM,\n    an efficient SLAM approach for endoscopic surgeries, which integrates streamlined\n    Gaussian representation and differentiable rasterization to facilitate over 100\n    fps rendering speed during online camera tracking and tissue reconstructing. Extensive\n    experiments show that EndoGSLAM achieves a better trade-off between intraoperative\n    availability and reconstruction quality than traditional or neural SLAM approaches,\n    showing tremendous potential for endoscopic surgeries\n  project_page: https://endogslam.loping151.com/\n  paper: https://arxiv.org/pdf/2403.15124.pdf\n  code: https://github.com/endogslam/EndoGSLAM\n  video: https://loping151.github.io/endogslam/static/video/3_1.mp4\n  tags:\n  - Code\n  - Medicine\n  - Project\n  - SLAM\n  - Video\n  thumbnail: assets/thumbnails/wang2024endogslam.jpg\n  publication_date: '2024-03-22T11:27:43+00:00'\n- id: zeng2024stag4d\n  title: 'STAG4D: Spatial-Temporal Anchored Generative 4D Gaussians'\n  authors: Yifei Zeng, Yanqin Jiang, Siyu Zhu, Yuanxun Lu, Youtian Lin, Hao Zhu, Weiming\n    Hu, Xun Cao, Yao Yao\n  year: '2024'\n  abstract: Recent progress in pre-trained diffusion models and 3D generation have\n    spurred interest in 4D content creation. However, achieving high-fidelity 4D generation\n    with spatial-temporal consistency remains a challenge. In this work, we propose\n    STAG4D, a novel framework that combines pre-trained diffusion models with dynamic\n    3D Gaussian splatting for high-fidelity 4D generation. Drawing inspiration from\n    3D generation techniques, we utilize a multi-view diffusion model to initialize\n    multi-view images anchoring on the input video frames, where the video can be\n    either real-world captured or generated by a video diffusion model. To ensure\n    the temporal consistency of the multi-view sequence initialization, we introduce\n    a simple yet effective fusion strategy to leverage the first frame as a temporal\n    anchor in the self-attention computation. With the almost consistent multi-view\n    sequences, we then apply the score distillation sampling to optimize the 4D Gaussian\n    point cloud. The 4D Gaussian spatting is specially crafted for the generation\n    task, where an adaptive densification strategy is proposed to mitigate the unstable\n    Gaussian gradient for robust optimization. Notably, the proposed pipeline does\n    not require any pre-training or fine-tuning of diffusion networks, offering a\n    more accessible and practical solution for the 4D generation task. Extensive experiments\n    demonstrate that our method outperforms prior 4D generation works in rendering\n    quality, spatial-temporal consistency, and generation robustness, setting a new\n    state-of-the-art for 4D generation from diverse inputs, including text, image,\n    and video.\n  project_page: https://nju-3dv.github.io/projects/STAG4D/\n  paper: https://arxiv.org/pdf/2403.14939\n  code: https://github.com/zeng-yifei/STAG4D\n  video: https://www.youtube.com/watch?v=YJkFMIV2OyQ\n  tags:\n  - Code\n  - Diffusion\n  - Project\n  - Video\n  thumbnail: assets/thumbnails/zeng2024stag4d.jpg\n  publication_date: '2024-03-22T04:16:33+00:00'\n- id: chen2024mvsplat\n  title: 'MVSplat: Efficient 3D Gaussian Splatting from Sparse Multi-View Images'\n  authors: Yuedong Chen, Haofei Xu, Chuanxia Zheng, Bohan Zhuang, Marc Pollefeys,\n    Andreas Geiger, Tat-Jen Cham, Jianfei Cai\n  year: '2024'\n  abstract: We propose MVSplat, an efficient feed-forward 3D Gaussian Splatting model\n    learned from sparse multi-view images. To accurately localize the Gaussian centers,\n    we propose to build a cost volume representation via plane sweeping in the 3D\n    space, where the cross-view feature similarities stored in the cost volume can\n    provide valuable geometry cues to the estimation of depth. We learn the Gaussian\n    primitives' opacities, covariances, and spherical harmonics coefficients jointly\n    with the Gaussian centers while only relying on photometric supervision. We demonstrate\n    the importance of the cost volume representation in learning feed-forward Gaussian\n    Splatting models via extensive experimental evaluations. On the large-scale RealEstate10K\n    and ACID benchmarks, our model achieves state-of-the-art performance with the\n    fastest feed-forward inference speed (22 fps). Compared to the latest state-of-the-art\n    method pixelSplat, our model uses 10× fewer parameters and infers more than 2×\n    faster while providing higher appearance and geometry quality as well as better\n    cross-dataset generalization.\n  project_page: https://donydchen.github.io/mvsplat/\n  paper: https://arxiv.org/pdf/2403.14627\n  code: https://github.com/donydchen/mvsplat\n  video: null\n  tags:\n  - Code\n  - Feed-Forward\n  - Project\n  - Sparse\n  thumbnail: assets/thumbnails/chen2024mvsplat.jpg\n  publication_date: '2024-03-21T17:59:58+00:00'\n- id: xu2024grm\n  title: 'GRM: Large Gaussian Reconstruction Model for Efficient 3D Reconstruction\n    and Generation'\n  authors: Yinghao Xu, Zifan Shi, Wang Yifan, Hansheng Chen, Ceyuan Yang, Sida Peng,\n    Yujun Shen, Gordon Wetzstein\n  year: '2024'\n  abstract: We introduce GRM, a large-scale reconstructor capable of recovering a\n    3D asset from sparse-view images in around 0.1s. GRM is a feed-forward transformer-based\n    model that efficiently incorporates multi-view information to translate the input\n    pixels into pixel-aligned Gaussians, which are unprojected to create a set of\n    densely distributed 3D Gaussians representing a scene. Together, our transformer\n    architecture and the use of 3D Gaussians unlock a scalable and efficient reconstruction\n    framework. Extensive experimental results demonstrate the superiority of our method\n    over alternatives regarding both reconstruction quality and efficiency. We also\n    showcase the potential of GRM in generative tasks, i.e., text-to-3D and image-to-3D,\n    by integrating it with existing multi-view diffusion models.\n  project_page: https://justimyhxu.github.io/projects/grm/\n  paper: https://arxiv.org/pdf/2403.14621.pdf\n  code: null\n  video: null\n  tags:\n  - Project\n  - Sparse\n  thumbnail: assets/thumbnails/xu2024grm.jpg\n  publication_date: '2024-03-21T17:59:34+00:00'\n- id: guédon2024gaussian\n  title: 'Gaussian Frosting: Editable Complex Radiance Fields with Real-Time Rendering'\n  authors: Antoine Guédon, Vincent Lepetit\n  year: '2024'\n  abstract: We propose Gaussian Frosting, a novel mesh-based representation for high-quality\n    rendering and editing of complex 3D effects in real-time. Our approach builds\n    on the recent 3D Gaussian Splatting framework, which optimizes a set of 3D Gaussians\n    to approximate a radiance field from images. We propose first extracting a base\n    mesh from Gaussians during optimization, then building and refining an adaptive\n    layer of Gaussians with a variable thickness around the mesh to better capture\n    the fine details and volumetric effects near the surface, such as hair or grass.\n    We call this layer Gaussian Frosting, as it resembles a coating of frosting on\n    a cake. The fuzzier the material, the thicker the frosting. We also introduce\n    a parameterization of the Gaussians to enforce them to stay inside the frosting\n    layer and automatically adjust their parameters when deforming, rescaling, editing\n    or animating the mesh. Our representation allows for efficient rendering using\n    Gaussian splatting, as well as editing and animation by modifying the base mesh.\n    We demonstrate the effectiveness of our method on various synthetic and real scenes,\n    and show that it outperforms existing surface-based approaches. We will release\n    our code and a web-based viewer as additional contributions.\n  project_page: https://anttwo.github.io/frosting/\n  paper: https://arxiv.org/pdf/2403.14554\n  code: https://github.com/Anttwo/Frosting\n  video: https://youtu.be/h7LeWq8sG78\n  tags:\n  - Code\n  - Dynamic\n  - Editing\n  - Meshing\n  - Project\n  - Video\n  thumbnail: assets/thumbnails/guédon2024gaussian.jpg\n  publication_date: '2024-03-21T16:53:03+00:00'\n- id: chen2024hac\n  title: 'HAC: Hash-grid Assisted Context for 3D Gaussian Splatting Compression'\n  authors: Yihang Chen, Qianyi Wu, Weiyao Lin, Mehrtash Harandi, Jianfei Cai\n  year: '2024'\n  abstract: '3D Gaussian Splatting (3DGS) has emerged as a promising framework for\n    novel view synthesis, boasting rapid rendering speed with high fidelity. However,\n    the substantial Gaussians and their associated attributes necessitate effective\n    compression techniques. Nevertheless, the sparse and unorganized nature of the\n    point cloud of Gaussians (or anchors in our paper) presents challenges for compression.\n    To address this, we make use of the relations between the unorganized anchors\n    and the structured hash grid, leveraging their mutual information for context\n    modeling, and propose a Hash-grid Assisted Context (HAC) framework for highly\n    compact 3DGS representation. Our approach introduces a binary hash grid to establish\n    continuous spatial consistencies, allowing us to unveil the inherent spatial relations\n    of anchors through a carefully designed context model. To facilitate entropy coding,\n    we utilize Gaussian distributions to accurately estimate the probability of each\n    quantized attribute, where an adaptive quantization module is proposed to enable\n    high-precision quantization of these attributes for improved fidelity restoration.\n    Additionally, we incorporate an adaptive masking strategy to eliminate invalid\n    Gaussians and anchors. Importantly, our work is the pioneer to explore context-based\n    compression for 3DGS representation, resulting in a remarkable size reduction\n    of over $75\\times$ compared to vanilla 3DGS, while simultaneously improving fidelity,\n    and achieving over $11\\times$ size reduction over SOTA 3DGS compression approach\n    Scaffold-GS.\n\n    '\n  project_page: https://yihangchen-ee.github.io/project_hac/\n  paper: https://arxiv.org/pdf/2403.14530.pdf\n  code: https://github.com/YihangChen-ee/HAC\n  video: null\n  tags:\n  - Code\n  - Compression\n  - Project\n  thumbnail: assets/thumbnails/chen2024hac.jpg\n  publication_date: '2024-03-21T16:28:58+00:00'\n- id: kim2024synctweedies\n  title: 'SyncTweedies: A General Generative Framework Based on Synchronized Diffusions'\n  authors: Jaihoon Kim, Juil Koo, Kyeongmin Yeo, Minhyuk Sung\n  year: '2024'\n  abstract: 'We introduce a general framework for generating diverse visual content,\n    including ambiguous images, panorama images, mesh textures, and Gaussian splat\n    textures, by synchronizing multiple diffusion processes. We present exhaustive\n    investigation into all possible scenarios for synchronizing multiple diffusion\n    processes through a canonical space and analyze their characteristics across applications.\n    In doing so, we reveal a previously unexplored case: averaging the outputs of\n    Tweedie''s formula while conducting denoising in multiple instance spaces. This\n    case also provides the best quality with the widest applicability to downstream\n    tasks. We name this case SyncTweedies. In our experiments generating visual content\n    aforementioned, we demonstrate the superior quality of generation by SyncTweedies\n    compared to other synchronization methods, optimization-based and iterative-update-based\n    methods.'\n  project_page: https://synctweedies.github.io/\n  paper: https://arxiv.org/pdf/2403.14370\n  code: https://github.com/KAIST-Visual-AI-Group/SyncTweedies\n  video: null\n  tags:\n  - Code\n  - Diffusion\n  - Project\n  thumbnail: assets/thumbnails/kim2024synctweedies.jpg\n  publication_date: '2024-03-21T12:57:30+00:00'\n- id: fang2024minisplatting\n  title: 'Mini-Splatting: Representing Scenes with a Constrained Number of Gaussians'\n  authors: Guangchi Fang, Bing Wang\n  year: '2024'\n  abstract: In this study, we explore the challenge of efficiently representing scenes\n    with a constrained number of Gaussians. Our analysis shifts from traditional graphics\n    and 2D computer vision to the perspective of point clouds, highlighting the inefficient\n    spatial distribution of Gaussian representation as a key limitation in model performance.\n    To address this, we introduce strategies for densification including blur split\n    and depth reinitialization, and simplification through Gaussian binarization and\n    sampling. These techniques reorganize the spatial positions of the Gaussians,\n    resulting in significant improvements across various datasets and benchmarks in\n    terms of rendering quality, resource consumption, and storage compression. Our\n    proposed Mini-Splatting method integrates seamlessly with the original rasterization\n    pipeline, providing a strong baseline for future research in Gaussian-Splatting-based\n    works.\n  project_page: null\n  paper: https://arxiv.org/pdf/2403.14166.pdf\n  code: https://github.com/fatPeter/mini-splatting\n  video: null\n  tags:\n  - Code\n  - Densification\n  - Rendering\n  thumbnail: assets/thumbnails/fang2024minisplatting.jpg\n  publication_date: '2024-03-21T06:34:46+00:00'\n- id: niemeyer2024radsplat\n  title: 'RadSplat: Radiance Field-Informed Gaussian Splatting for Robust Real-Time\n    Rendering with 900+ FPS'\n  authors: Michael Niemeyer, Fabian Manhardt, Marie-Julie Rakotosaona, Michael Oechsle,\n    Daniel Duckworth, Rama Gosula, Keisuke Tateno, John Bates, Dominik Kaeser, Federico\n    Tombari\n  year: '2024'\n  abstract: Recent advances in view synthesis and real-time rendering have achieved\n    photorealistic quality at impressive rendering speeds. While Radiance Field-based\n    methods achieve state-of-the-art quality in challenging scenarios such as in-the-wild\n    captures and large-scale scenes, they often suffer from excessively high compute\n    requirements linked to volumetric rendering. Gaussian Splatting-based methods,\n    on the other hand, rely on rasterization and naturally achieve real-time rendering\n    but suffer from brittle optimization heuristics that underperform on more challenging\n    scenes. In this work, we present RadSplat, a lightweight method for robust real-time\n    rendering of complex scenes. Our main contributions are threefold. First, we use\n    radiance fields as a prior and supervision signal for optimizing point-based scene\n    representations, leading to improved quality and more robust optimization. Next,\n    we develop a novel pruning technique reducing the overall point count while maintaining\n    high quality, leading to smaller and more compact scene representations with faster\n    inference speeds. Finally, we propose a novel test-time filtering approach that\n    further accelerates rendering and allows to scale to larger, house-sized scenes.\n    We find that our method enables state-of-the-art synthesis of complex captures\n    at 900+ FPS.\n  project_page: https://m-niemeyer.github.io/radsplat/\n  paper: https://arxiv.org/pdf/2403.13806.pdf\n  code: null\n  video: null\n  tags:\n  - Densification\n  - Misc\n  - Project\n  - Rendering\n  thumbnail: assets/thumbnails/niemeyer2024radsplat.jpg\n  publication_date: '2024-03-20T17:59:55+00:00'\n- id: seiskari2024gaussian\n  title: 'Gaussian Splatting on the Move: Blur and Rolling Shutter Compensation for\n    Natural Camera Motion'\n  authors: Otto Seiskari, Jerry Ylilammi, Valtteri Kaatrasalo, Pekka Rantalankila,\n    Matias Turkulainen, Juho Kannala, Esa Rahtu, Arno Solin\n  year: '2024'\n  abstract: High-quality scene reconstruction and novel view synthesis based on Gaussian\n    Splatting (3DGS) typically require steady, high-quality photographs, often impractical\n    to capture with handheld cameras. We present a method that adapts to camera motion\n    and allows high-quality scene reconstruction with handheld video data suffering\n    from motion blur and rolling shutter distortion. Our approach is based on detailed\n    modelling of the physical image formation process and utilizes velocities estimated\n    using visual-inertial odometry (VIO). Camera poses are considered non-static during\n    the exposure time of a single image frame and camera poses are further optimized\n    in the reconstruction process. We formulate a differentiable rendering pipeline\n    that leverages screen space approximation to efficiently incorporate rolling-shutter\n    and motion blur effects into the 3DGS framework. Our results with both synthetic\n    and real data demonstrate superior performance in mitigating camera motion over\n    existing methods, thereby advancing 3DGS in naturalistic settings.\n  project_page: https://spectacularai.github.io/3dgs-deblur/\n  paper: https://arxiv.org/pdf/2403.13327.pdf\n  code: https://github.com/SpectacularAI/3dgs-deblur\n  video: https://www.youtube.com/watch?v=GwhNerXMLd4&feature=youtu.be\n  tags:\n  - Code\n  - Deblurring\n  - Project\n  - Rendering\n  - Video\n  thumbnail: assets/thumbnails/seiskari2024gaussian.jpg\n  publication_date: '2024-03-20T06:19:41+00:00'\n- id: he2024gvgen\n  title: 'GVGEN: Text-to-3D Generation with Volumetric Representation'\n  authors: Xianglong He, Junyi Chen, Sida Peng, Di Huang, Yangguang Li, Xiaoshui Huang,\n    Chun Yuan, Wanli Ouyang, Tong He\n  year: '2024'\n  abstract: In recent years, 3D Gaussian splatting has emerged as a powerful technique\n    for 3D reconstruction and generation, known for its fast and high-quality rendering\n    capabilities. To address these shortcomings, this paper introduces a novel diffusion-based\n    framework, GVGEN, designed to efficiently generate 3D Gaussian representations\n    from text input. We propose two innovative techniques:(1) Structured Volumetric\n    Representation. We first arrange disorganized 3D Gaussian points as a structured\n    form GaussianVolume. This transformation allows the capture of intricate texture\n    details within a volume composed of a fixed number of Gaussians. To better optimize\n    the representation of these details, we propose a unique pruning and densifying\n    method named the Candidate Pool Strategy, enhancing detail fidelity through selective\n    optimization. (2) Coarse-to-fine Generation Pipeline. To simplify the generation\n    of GaussianVolume and empower the model to generate instances with detailed 3D\n    geometry, we propose a coarse-to-fine pipeline. It initially constructs a basic\n    geometric structure, followed by the prediction of complete Gaussian attributes.\n    Our framework, GVGEN, demonstrates superior performance in qualitative and quantitative\n    assessments compared to existing 3D generation methods. Simultaneously, it maintains\n    a fast generation speed (∼7 seconds), effectively striking a balance between quality\n    and efficiency.\n  project_page: https://gvgen.github.io/\n  paper: https://arxiv.org/pdf/2403.12957\n  code: https://github.com/GVGEN/GVGEN\n  video: null\n  tags:\n  - Code\n  - Diffusion\n  - Project\n  thumbnail: assets/thumbnails/he2024gvgen.jpg\n  publication_date: '2024-03-19T17:57:52+00:00'\n- id: zhou2023hugs\n  title: 'HUGS: Holistic Urban 3D Scene Understanding via Gaussian Splatting'\n  authors: Hongyu Zhou, Jiahao Shao, Lu Xu, Dongfeng Bai, Weichao Qiu, Bingbing Liu,\n    Yue Wang, Andreas Geiger, Yiyi Liao\n  year: '2023'\n  abstract: Holistic understanding of urban scenes based on RGB images is a challenging\n    yet important problem. It encompasses understanding both the geometry and appearance\n    to enable novel view synthesis, parsing semantic labels, and tracking moving objects.\n    Despite considerable progress, existing approaches often focus on specific aspects\n    of this task and require additional inputs such as LiDAR scans or manually annotated\n    3D bounding boxes. In this paper, we introduce a novel pipeline that utilizes\n    3D Gaussian Splatting for holistic urban scene understanding. Our main idea involves\n    the joint optimization of geometry, appearance, semantics, and motion using a\n    combination of static and dynamic 3D Gaussians, where moving object poses are\n    regularized via physical constraints. Our approach offers the ability to render\n    new viewpoints in real-time, yielding 2D and 3D semantic information with high\n    accuracy, and reconstruct dynamic scenes, even in scenarios where 3D bounding\n    box detection are highly noisy. Experimental results on KITTI, KITTI-360, and\n    Virtual KITTI 2 demonstrate the effectiveness of our approach.\n  project_page: https://xdimlab.github.io/hugs_website/\n  paper: https://arxiv.org/pdf/2403.12722.pdf\n  code: https://github.com/hyzhou404/HUGS\n  video: null\n  tags:\n  - Autonomous Driving\n  - Code\n  - Project\n  thumbnail: assets/thumbnails/zhou2023hugs.jpg\n  publication_date: '2024-03-19T13:39:05+00:00'\n- id: ha2024rgbd\n  title: RGBD GS-ICP SLAM\n  authors: Seongbo Ha, Jiung Yeon, Hyeonwoo Yu\n  year: '2024'\n  abstract: Simultaneous Localization and Mapping (SLAM) with dense representation\n    plays a key role in robotics, Virtual Reality (VR), and Augmented Reality (AR)\n    applications. Recent advancements in dense representation SLAM have highlighted\n    the potential of leveraging neural scene representation and 3D Gaussian representation\n    for high-fidelity spatial representation. In this paper, we propose a novel dense\n    representation SLAM approach with a fusion of Generalized Iterative Closest Point\n    (G-ICP) and 3D Gaussian Splatting (3DGS). In contrast to existing methods, we\n    utilize a single Gaussian map for both tracking and mapping, resulting in mutual\n    benefits. Through the exchange of covariances between tracking and mapping processes\n    with scale alignment techniques, we minimize redundant computations and achieve\n    an efficient system. Additionally, we enhance tracking accuracy and mapping quality\n    through our keyframe selection methods. Experimental results demonstrate the effectiveness\n    of our approach, showing an incredibly fast speed up to 107 FPS (for the entire\n    system) and superior quality of the reconstructed map.\n  project_page: null\n  paper: https://arxiv.org/pdf/2403.12550.pdf\n  code: https://github.com/Lab-of-AI-and-Robotics/GS_ICP_SLAM\n  video: https://www.youtube.com/watch?v=e-bHh_uMMxE\n  tags:\n  - Code\n  - SLAM\n  - Video\n  thumbnail: assets/thumbnails/ha2024rgbd.jpg\n  publication_date: '2024-03-19T08:49:48+00:00'\n- id: sun2024highfidelity\n  title: High-Fidelity SLAM Using Gaussian Splatting with Rendering-Guided Densification\n    and Regularized Optimization\n  authors: Shuo Sun, Malcolm Mielle, Achim J. Lilienthal, Martin Magnusson\n  year: '2024'\n  abstract: We propose a dense RGBD SLAM system based on 3D Gaussian Splatting that\n    provides metrically accurate pose tracking and visually realistic reconstruction.\n    To this end, we first propose a Gaussian densification strategy based on the rendering\n    loss to map unobserved areas and refine reobserved areas. Second, we introduce\n    extra regularization parameters to alleviate the forgetting problem in the continuous\n    mapping problem, where parameters tend to overfit the latest frame and result\n    in decreasing rendering quality for previous frames. Both mapping and tracking\n    are performed with Gaussian parameters by minimizing re-rendering loss in a differentiable\n    way. Compared to recent neural and concurrently developed gaussian splatting RGBD\n    SLAM baselines, our method achieves state-of-the-art results on the synthetic\n    dataset Replica and competitive results on the real-world dataset TUM.\n  project_page: null\n  paper: https://arxiv.org/pdf/2403.12535.pdf\n  code: null\n  video: null\n  tags:\n  - SLAM\n  thumbnail: assets/thumbnails/sun2024highfidelity.jpg\n  publication_date: '2024-03-19T08:19:53+00:00'\n- id: gao2024gaussianflow\n  title: 'GaussianFlow: Splatting Gaussian Dynamics for 4D Content Creation'\n  authors: Quankai Gao, Qiangeng Xu, Zhe Cao, Ben Mildenhall, Wenchao Ma, Le Chen,\n    Danhang Tang, Ulrich Neumann\n  year: '2024'\n  abstract: Creating 4D fields of Gaussian Splatting from images or videos is a challenging\n    task due to its under-constrained nature. While the optimization can draw photometric\n    reference from the input videos or be regulated by generative models, directly\n    supervising Gaussian motions remains underexplored. In this paper, we introduce\n    a novel concept, Gaussian flow, which connects the dynamics of 3D Gaussians and\n    pixel velocities between consecutive frames. The Gaussian flow can be efficiently\n    obtained by splatting Gaussian dynamics into the image space. This differentiable\n    process enables direct dynamic supervision from optical flow. Our method significantly\n    benefits 4D dynamic content generation and 4D novel view synthesis with Gaussian\n    Splatting, especially for contents with rich motions that are hard to be handled\n    by existing methods. The common color drifting issue that happens in 4D generation\n    is also resolved with improved Guassian dynamics. Superior visual quality on extensive\n    experiments demonstrates our method's effectiveness. Quantitative and qualitative\n    evaluations show that our method achieves state-of-the-art results on both tasks\n    of 4D generation and 4D novel view synthesis.\n  project_page: https://zerg-overmind.github.io/GaussianFlow.github.io/\n  paper: https://arxiv.org/pdf/2403.12365\n  code: null\n  video: https://www.youtube.com/watch?v=0qRcjTw7-YU\n  tags:\n  - Dynamic\n  - Project\n  - Video\n  thumbnail: assets/thumbnails/gao2024gaussianflow.jpg\n  publication_date: '2024-03-19T02:22:21+00:00'\n- id: wang2024reinforcement\n  title: Reinforcement Learning with Generalizable Gaussian Splatting\n  authors: Jiaxu Wang, Qiang Zhang, Jingkai Sun, Jiahang Cao, Yecheng Shao, Renjing\n    Xu\n  year: '2024'\n  abstract: An excellent representation is crucial for reinforcement learning (RL)\n    performance, especially in vision-based reinforcement learning tasks. The quality\n    of the environment representation directly influences the achievement of the learning\n    task. Previous vision-based RL typically uses explicit or implicit ways to represent\n    environments, such as images, points, voxels, and neural radiance fields. However,\n    these representations contain several drawbacks. They cannot either describe complex\n    local geometries or generalize well to unseen scenes, or require precise foreground\n    masks. Moreover, these implicit neural representations are akin to a ``black box\",\n    significantly hindering interpretability. 3D Gaussian Splatting (3DGS), with its\n    explicit scene representation and differentiable rendering nature, is considered\n    a revolutionary change for reconstruction and representation methods. In this\n    paper, we propose a novel Generalizable Gaussian Splatting framework to be the\n    representation of RL tasks, called GSRL. Through validation in the RoboMimic environment,\n    our method achieves better results than other baselines in multiple tasks, improving\n    the performance by 10%, 44%, and 15% compared with baselines on the hardest task.\n    This work is the first attempt to leverage generalizable 3DGS as a representation\n    for RL.\n  project_page: null\n  paper: https://arxiv.org/pdf/2404.07950\n  code: null\n  video: null\n  tags:\n  - Misc\n  - Robotics\n  thumbnail: assets/thumbnails/wang2024reinforcement.jpg\n  publication_date: '2024-03-18T16:50:23+00:00'\n- id: wang2024viewconsistent\n  title: View-Consistent 3D Editing with Gaussian Splatting\n  authors: Yuxuan Wang, Xuanyu Yi, Zike Wu, Na Zhao, Long Chen, Hanwang Zhang\n  year: '2024'\n  abstract: 'The advent of 3D Gaussian Splatting (3DGS) has revolutionized 3D editing,\n    offering efficient, high-fidelity rendering and enabling precise local manipulations.\n    Currently, diffusion-based 2D editing models are harnessed to modify multi-view\n    rendered images, which then guide the editing of 3DGS models. However, this approach\n    faces a critical issue of multi-view inconsistency, where the guidance images\n    exhibit significant discrepancies across views, leading to mode collapse and visual\n    artifacts of 3DGS. To this end, we introduce View-consistent Editing (VcEdit),\n    a novel framework that seamlessly incorporates 3DGS into image editing processes,\n    ensuring multi-view consistency in edited guidance images and effectively mitigating\n    mode collapse issues. VcEdit employs two innovative consistency modules: the Cross-attention\n    Consistency Module and the Editing Consistency Module, both designed to reduce\n    inconsistencies in edited images. By incorporating these consistency modules into\n    an iterative pattern, VcEdit proficiently resolves the issue of multi-view inconsistency,\n    facilitating high-quality 3DGS editing across a diverse range of scenes.'\n  project_page: https://vcedit.github.io/\n  paper: https://arxiv.org/pdf/2403.11868.pdf\n  code: null\n  video: null\n  tags:\n  - Editing\n  - Project\n  thumbnail: assets/thumbnails/wang2024viewconsistent.jpg\n  publication_date: '2024-03-18T15:22:09+00:00'\n- id: zhao2024badgaussians\n  title: 'BAD-Gaussians: Bundle Adjusted Deblur Gaussian Splatting'\n  authors: Lingzhe Zhao, Peng Wang, Peidong Liu\n  year: '2024'\n  abstract: While neural rendering has demonstrated impressive capabilities in 3D\n    scene reconstruction and novel view synthesis, it heavily relies on high-quality\n    sharp images and accurate camera poses. Numerous approaches have been proposed\n    to train Neural Radiance Fields (NeRF) with motion-blurred images, commonly encountered\n    in real-world scenarios such as low-light or long-exposure conditions. However,\n    the implicit representation of NeRF struggles to accurately recover intricate\n    details from severely motion-blurred images and cannot achieve real-time rendering.\n    In contrast, recent advancements in 3D Gaussian Splatting achieve high-quality\n    3D scene reconstruction and real-time rendering by explicitly optimizing point\n    clouds into 3D Gaussians. In this paper, we introduce a novel approach, named\n    BAD-Gaussians (Bundle Adjusted Deblur Gaussian Splatting), which leverages explicit\n    Gaussian representation and handles severe motion-blurred images with inaccurate\n    camera poses to achieve high-quality scene reconstruction. Our method models the\n    physical image formation process of motion-blurred images and jointly learns the\n    parameters of Gaussians while recovering camera motion trajectories during exposure\n    time. In our experiments, we demonstrate that BAD-Gaussians not only achieves\n    superior rendering quality compared to previous state-of-the-art deblur neural\n    rendering methods on both synthetic and real datasets but also enables real-time\n    rendering capabilities.\n  project_page: https://lingzhezhao.github.io/BAD-Gaussians/\n  paper: https://arxiv.org/pdf/2403.11831.pdf\n  code: https://github.com/WU-CVGL/BAD-Gaussians/\n  video: null\n  tags:\n  - Code\n  - Deblurring\n  - Project\n  - Rendering\n  thumbnail: assets/thumbnails/zhao2024badgaussians.jpg\n  publication_date: '2024-03-18T14:43:04+00:00'\n- id: ji2024nedsslam\n  title: 'NEDS-SLAM: A Novel Neural Explicit Dense Semantic SLAM Framework using 3D\n    Gaussian Splatting'\n  authors: Yiming Ji, Yang Liu, Guanghu Xie, Boyu Ma, Zongwu Xie\n  year: '2024'\n  abstract: We propose NEDS-SLAM, an Explicit Dense semantic SLAM system based on\n    3D Gaussian representation, that enables robust 3D semantic mapping, accurate\n    camera tracking, and high-quality rendering in real-time. In the system, we propose\n    a Spatially Consistent Feature Fusion model to reduce the effect of erroneous\n    estimates from pre-trained segmentation head on semantic reconstruction, achieving\n    robust 3D semantic Gaussian mapping. Additionally, we employ a lightweight encoder-decoder\n    to compress the high-dimensional semantic features into a compact 3D Gaussian\n    representation, mitigating the burden of excessive memory consumption. Furthermore,\n    we leverage the advantage of 3D Gaussian splatting, which enables efficient and\n    differentiable novel view rendering, and propose a Virtual Camera View Pruning\n    method to eliminate outlier GS points, thereby effectively enhancing the quality\n    of scene representations. Our NEDS-SLAM method demonstrates competitive performance\n    over existing dense semantic SLAM methods in terms of mapping and tracking accuracy\n    on Replica and ScanNet datasets, while also showing excellent capabilities in\n    3D dense semantic mapping.\n  project_page: null\n  paper: https://arxiv.org/pdf/2403.11679.pdf\n  code: null\n  video: null\n  tags:\n  - SLAM\n  thumbnail: assets/thumbnails/ji2024nedsslam.jpg\n  publication_date: '2024-03-18T11:31:03+00:00'\n- id: lei2024gaussnav\n  title: 'GaussNav: Gaussian Splatting for Visual Navigation'\n  authors: Xiaohan Lei, Min Wang, Wengang Zhou, Houqiang Li\n  year: '2024'\n  abstract: In embodied vision, Instance ImageGoal Navigation (IIN) requires an agent\n    to locate a specific object depicted in a goal image within an unexplored environment.\n    The primary difficulty of IIN stems from the necessity of recognizing the target\n    object across varying viewpoints and rejecting potential distractors. Existing\n    map-based navigation methods largely adopt the representation form of Bird's Eye\n    View (BEV) maps, which, however, lack the representation of detailed textures\n    in a scene. To address the above issues, we propose a new Gaussian Splatting Navigation\n    (abbreviated as GaussNav) framework for IIN task, which constructs a novel map\n    representation based on 3D Gaussian Splatting (3DGS). The proposed framework enables\n    the agent to not only memorize the geometry and semantic information of the scene,\n    but also retain the textural features of objects. Our GaussNav framework demonstrates\n    a significant leap in performance, evidenced by an increase in Success weighted\n    by Path Length (SPL) from 0.252 to 0.578 on the challenging Habitat-Matterport\n    3D (HM3D) dataset.\n  project_page: https://xiaohanlei.github.io/projects/GaussNav/\n  paper: https://arxiv.org/pdf/2403.11625.pdf\n  code: https://github.com/XiaohanLei/GaussNav\n  video: null\n  tags:\n  - Autonomous Driving\n  - Code\n  - Project\n  - Robotics\n  thumbnail: assets/thumbnails/lei2024gaussnav.jpg\n  publication_date: '2024-03-18T09:56:48+00:00'\n- id: herau20243dgscalib\n  title: '3DGS-Calib: 3D Gaussian Splatting for Multimodal SpatioTemporal Calibration'\n  authors: Quentin Herau, Moussab Bennehar, Arthur Moreau, Nathan Piasco, Luis Roldao,\n    Dzmitry Tsishkou, Cyrille Migniot, Pascal Vasseur, Cédric Demonceaux\n  year: '2024'\n  abstract: Reliable multimodal sensor fusion algorithms re- quire accurate spatiotemporal\n    calibration. Recently, targetless calibration techniques based on implicit neural\n    representations have proven to provide precise and robust results. Nevertheless,\n    such methods are inherently slow to train given the high compu- tational overhead\n    caused by the large number of sampled points required for volume rendering. With\n    the recent introduction of 3D Gaussian Splatting as a faster alternative to implicit\n    representation methods, we propose to leverage this new ren- dering approach to\n    achieve faster multi-sensor calibration. We introduce 3DGS-Calib, a new calibration\n    method that relies on the speed and rendering accuracy of 3D Gaussian Splatting\n    to achieve multimodal spatiotemporal calibration that is accurate, robust, and\n    with a substantial speed-up compared to methods relying on implicit neural representations.\n    We demonstrate the superiority of our proposal with experimental results on sequences\n    from KITTI-360, a widely used driving dataset.\n  project_page: https://qherau.github.io/3DGS-Calib/\n  paper: https://arxiv.org/pdf/2403.11577\n  code: null\n  video: https://www.youtube.com/watch?v=9_RE_4xkRcM\n  tags:\n  - Misc\n  - Project\n  - Video\n  thumbnail: assets/thumbnails/herau20243dgscalib.jpg\n  publication_date: '2024-03-18T08:53:03+00:00'\n- id: suzuki2024fed3dgs\n  title: 'Fed3DGS: Scalable 3D Gaussian Splatting with Federated Learning'\n  authors: Teppei Suzuki\n  year: '2024'\n  abstract: In this work, we present Fed3DGS, a scalable 3D reconstruction framework\n    based on 3D Gaussian splatting (3DGS) with federated learning. Existing city-scale\n    reconstruction methods typically adopt a centralized approach, which gathers all\n    data in a central server and reconstructs scenes. The approach hampers scalability\n    because it places a heavy load on the server and demands extensive data storage\n    when reconstructing scenes on a scale beyond city-scale. In pursuit of a more\n    scalable 3D reconstruction, we propose a federated learning framework with 3DGS,\n    which is a decentralized framework and can potentially use distributed computational\n    resources across millions of clients. We tailor a distillation-based model update\n    scheme for 3DGS and introduce appearance modeling for handling non-IID data in\n    the scenario of 3D reconstruction with federated learning. We simulate our method\n    on several large-scale benchmarks, and our method demonstrates rendered image\n    quality comparable to centralized approaches. In addition, we also simulate our\n    method with data collected in different seasons, demonstrating that our framework\n    can reflect changes in the scenes and our appearance modeling captures changes\n    due to seasonal variations.\n  project_page: null\n  paper: https://arxiv.org/pdf/2403.11460\n  code: https://github.com/DensoITLab/Fed3DGS\n  video: null\n  tags:\n  - Distributed\n  - Large-Scale\n  thumbnail: assets/thumbnails/suzuki2024fed3dgs.jpg\n  publication_date: '2024-03-18T04:26:18+00:00'\n- id: xiao2024bridging\n  title: Bridging 3D Gaussian and Mesh for Freeview Video Rendering\n  authors: Yuting Xiao, Xuan Wang, Jiafei Li, Hongrui Cai, Yanbo Fan, Nan Xue, Minghui\n    Yang, Yujun Shen, Shenghua Gao\n  year: '2024'\n  abstract: This is only a preview version of GauMesh. Recently, primitive-based rendering\n    has been proven to achieve convincing results in solving the problem of modeling\n    and rendering the 3D dynamic scene from 2D images. Despite this, in the context\n    of novel view synthesis, each type of primitive has its inherent defects in terms\n    of representation ability. It is difficult to exploit the mesh to depict the fuzzy\n    geometry. Meanwhile, the point-based splatting (e.g. the 3D Gaussian Splatting)\n    method usually produces artifacts or blurry pixels in the area with smooth geometry\n    and sharp textures. As a result, it is difficult, even not impossible, to represent\n    the complex and dynamic scene with a single type of primitive. To this end, we\n    propose a novel approach, GauMesh, to bridge the 3D Gaussian and Mesh for modeling\n    and rendering the dynamic scenes. Given a sequence of tracked mesh as initialization,\n    our goal is to simultaneously optimize the mesh geometry, color texture, opacity\n    maps, a set of 3D Gaussians, and the deformation field. At a specific time, we\n    perform α-blending on the RGB and opacity values based on the merged and re-ordered\n    z-buffers from mesh and 3D Gaussian rasterizations. This produces the final rendering,\n    which is supervised by the ground-truth image. Experiments demonstrate that our\n    approach adapts the appropriate type of primitives to represent the different\n    parts of the dynamic scene and outperforms all the baseline methods in both quantitative\n    and qualitative comparisons without losing render speed.\n  project_page: null\n  paper: https://arxiv.org/pdf/2403.11453\n  code: null\n  video: null\n  tags:\n  - Avatar\n  - Dynamic\n  - Meshing\n  thumbnail: assets/thumbnails/xiao2024bridging.jpg\n  publication_date: '2024-03-18T04:01:26+00:00'\n- id: guo2024motionaware\n  title: null\n  authors: Zhiyang Guo, Wengang Zhou, Li Li, Min Wang, Houqiang Li\n  year: '2024'\n  abstract: 3D Gaussian Splatting (3DGS) has become an emerging tool for dynamic scene\n    reconstruction. However, existing methods focus mainly on extending static 3DGS\n    into a time-variant representation, while overlooking the rich motion information\n    carried by 2D observations, thus suffering from performance degradation and model\n    redundancy. To address the above problem, we propose a novel motion-aware enhancement\n    framework for dynamic scene reconstruction, which mines useful motion cues from\n    optical flow to improve different paradigms of dynamic 3DGS. Specifically, we\n    first establish a correspondence between 3D Gaussian movements and pixel-level\n    flow. Then a novel flow augmentation method is introduced with additional insights\n    into uncertainty and loss collaboration. Moreover, for the prevalent deformation-based\n    paradigm that presents a harder optimization problem, a transient-aware deformation\n    auxiliary module is proposed. We conduct extensive experiments on both multi-view\n    and monocular scenes to verify the merits of our work. Compared with the baselines,\n    our method shows significant superiority in both rendering quality and efficiency.\n  project_page: null\n  paper: https://arxiv.org/pdf/2403.11447\n  code: null\n  video: null\n  tags:\n  - Dynamic\n  thumbnail: assets/thumbnails/guo2024motionaware.jpg\n  publication_date: '2024-03-18T03:46:26+00:00'\n- id: zhang2024bags\n  title: 'BAGS: Building Animatable Gaussian Splatting from a Monocular Video with\n    Diffusion Priors'\n  authors: Tingyang Zhang, Qingzhe Gao, Weiyu Li, Libin Liu, Baoquan Chen\n  year: '2024'\n  abstract: Animatable 3D reconstruction has significant applications across various\n    fields, primarily relying on artists' handcraft creation. Recently, some studies\n    have successfully constructed animatable 3D models from monocular videos. However,\n    these approaches require sufficient view coverage of the object within the input\n    video and typically necessitate significant time and computational costs for training\n    and rendering. This limitation restricts the practical applications. In this work,\n    we propose a method to build animatable 3D Gaussian Splatting from monocular video\n    with diffusion priors. The 3D Gaussian representations significantly accelerate\n    the training and rendering process, and the diffusion priors allow the method\n    to learn 3D models with limited viewpoints. We also present the rigid regularization\n    to enhance the utilization of the priors. We perform an extensive evaluation across\n    various real-world videos, demonstrating its superior performance compared to\n    the current state-of-the-art methods.\n  project_page: null\n  paper: https://arxiv.org/pdf/2403.11427\n  code: https://github.com/Michaelszj/bags\n  video: null\n  tags:\n  - Code\n  - Diffusion\n  - Dynamic\n  - Monocular\n  thumbnail: assets/thumbnails/zhang2024bags.jpg\n  publication_date: '2024-03-18T02:44:46+00:00'\n- id: liu2024beyond\n  title: 'Beyond Uncertainty: Risk-Aware Active View Acquisition for Safe Robot Navigation\n    and 3D Scene Understanding with FisherRF'\n  authors: Guangyi Liu, Wen Jiang, Boshu Lei, Vivek Pandey, Kostas Daniilidis, Nader\n    Motee\n  year: '2024'\n  abstract: 'This work proposes a novel approach to bolster both the robot''s risk\n    assessment and safety measures while deepening its understanding of 3D scenes,\n    which is achieved by leveraging Radiance Field (RF) models and 3D Gaussian Splatting.\n    To further enhance these capabilities, we incorporate additional sampled views\n    from the environment with the RF model. One of our key contributions is the introduction\n    of Risk-aware Environment Masking (RaEM), which prioritizes crucial information\n    by selecting the next-best-view that maximizes the expected information gain.\n    This targeted approach aims to minimize uncertainties surrounding the robot''s\n    path and enhance the safety of its navigation. Our method offers a dual benefit:\n    improved robot safety and increased efficiency in risk-aware 3D scene reconstruction\n    and understanding. Extensive experiments in real-world scenarios demonstrate the\n    effectiveness of our proposed approach, highlighting its potential to establish\n    a robust and safety-focused framework for active robot exploration and 3D scene\n    understanding.'\n  project_page: null\n  paper: https://arxiv.org/pdf/2403.11396\n  code: null\n  video: null\n  tags:\n  - Autonomous Driving\n  - Robotics\n  - Uncertainty\n  thumbnail: assets/thumbnails/liu2024beyond.jpg\n  publication_date: '2024-03-18T01:08:18+00:00'\n- id: jiang20243dgsreloc\n  title: '3DGS-ReLoc: 3D Gaussian Splatting for Map Representation and Visual ReLocalization'\n  authors: Peng Jiang, Gaurav Pandey, Srikanth Saripalli\n  year: '2024'\n  abstract: This paper presents a novel system designed for 3D mapping and visual\n    relocalization using 3D Gaussian Splatting. Our proposed method uses LiDAR and\n    camera data to create accurate and visually plausible representations of the environment.\n    By leveraging LiDAR data to initiate the training of the 3D Gaussian Splatting\n    map, our system constructs maps that are both detailed and geometrically accurate.\n    To mitigate excessive GPU memory usage and facilitate rapid spatial queries, we\n    employ a combination of a 2D voxel map and a KD-tree. This preparation makes our\n    method well-suited for visual localization tasks, enabling efficient identification\n    of correspondences between the query image and the rendered image from the Gaussian\n    Splatting map via normalized cross-correlation (NCC). Additionally, we refine\n    the camera pose of the query image using feature-based matching and the Perspective-n-Point\n    (PnP) technique. The effectiveness, adaptability, and precision of our system\n    are demonstrated through extensive evaluation on the KITTI360 dataset.\n  project_page: null\n  paper: https://arxiv.org/pdf/2403.11367\n  code: null\n  video: null\n  tags:\n  - Poses\n  thumbnail: assets/thumbnails/jiang20243dgsreloc.jpg\n  publication_date: '2024-03-17T23:06:12+00:00'\n- id: tarun2024creating\n  title: Creating Seamless 3D Maps Using Radiance Fields\n  authors: Sai Tarun Sathyan, Thomas B. Kinsman\n  year: '2024'\n  abstract: It is desirable to create 3D object models and 3D maps from 2D input images\n    for applications such as navigation, virtual tourism, and urban planning. The\n    traditional methods of creating 3D maps, (such as photogrammetry), require a large\n    number of images and odometry. Additionally, traditional methods have difficulty\n    with reflective surfaces and specular reflections; windows and chrome in the scene\n    can be problematic. Google Road View is a familiar application, which uses traditional\n    methods to fuse a collection of 2D input images into the illusion of a 3D map.\n    However, Google Road View does not create an actual 3D object model, only a collection\n    of views. The objective of this work is to create an actual 3D object model using\n    updated techniques. Neural Radiance Fields (NeRF[1]) has emerged as a potential\n    solution, offering the capability to produce more precise and intricate 3D maps.\n    Gaussian Splatting[4] is another contemporary technique. This investigation compares\n    Neural Radiance Fields to Gaussian Splatting, and describes some of their inner\n    workings. Our primary contribution is a method for improving the results of the\n    3D reconstructed models. Our results indicate that Gaussian Splatting was superior\n    to the NeRF technique.\n  project_page: null\n  paper: https://arxiv.org/pdf/2403.11364.pdf\n  code: null\n  video: null\n  tags:\n  - Misc\n  thumbnail: assets/thumbnails/tarun2024creating.jpg\n  publication_date: '2024-03-17T22:49:07+00:00'\n- id: li2024geogaussian\n  title: 'GeoGaussian: Geometry-aware Gaussian Splatting for Scene Rendering'\n  authors: Yanyan Li, Chenyu Lyu, Yan Di, Guangyao Zhai, Gim Hee Lee, Federico Tombari\n  year: '2024'\n  abstract: During the Gaussian Splatting optimization process, the scene's geometry\n    can gradually deteriorate if its structure is not deliberately preserved, especially\n    in non-textured regions such as walls, ceilings, and furniture surfaces. This\n    degradation significantly affects the rendering quality of novel views that deviate\n    significantly from the viewpoints in the training data. To mitigate this issue,\n    we propose a novel approach called GeoGaussian. Based on the smoothly connected\n    areas observed from point clouds, this method introduces a novel pipeline to initialize\n    thin Gaussians aligned with the surfaces, where the characteristic can be transferred\n    to new generations through a carefully designed densification strategy. Finally,\n    the pipeline ensures that the scene's geometry and texture are maintained through\n    constrained optimization processes with explicit geometry constraints. Benefiting\n    from the proposed architecture, the generative ability of 3D Gaussians is enhanced,\n    especially in structured regions. Our proposed pipeline achieves state-of-the-art\n    performance in novel view synthesis and geometric reconstruction, as evaluated\n    qualitatively and quantitatively on public datasets.\n  project_page: https://yanyan-li.github.io/project/gs/geogaussian\n  paper: https://arxiv.org/pdf/2403.11324.pdf\n  code: https://github.com/yanyan-li/GeoGaussian\n  video: null\n  tags:\n  - Code\n  - Densification\n  - Project\n  - Rendering\n  thumbnail: assets/thumbnails/li2024geogaussian.jpg\n  publication_date: '2024-03-17T20:06:41+00:00'\n- id: jiang2024brightdreamer\n  title: 'BrightDreamer: Generic 3D Gaussian Generative Framework for Fast Text-to-3D\n    Synthesis'\n  authors: Lutao Jiang, Lin Wang\n  year: '2024'\n  abstract: Text-to-3D synthesis has recently seen intriguing advances by combining\n    the text-to-image models with 3D representation methods, e.g., Gaussian Splatting\n    (GS), via Score Distillation Sampling (SDS). However, a hurdle of existing methods\n    is the low efficiency, per-prompt optimization for a single 3D object. Therefore,\n    it is imperative for a paradigm shift from per-prompt optimization to one-stage\n    generation for any unseen text prompts, which yet remains challenging. A hurdle\n    is how to directly generate a set of millions of 3D Gaussians to represent a 3D\n    object. This paper presents BrightDreamer, an end-to-end single-stage approach\n    that can achieve generalizable and fast (77 ms) text-to-3D generation. Our key\n    idea is to formulate the generation process as estimating the 3D deformation from\n    an anchor shape with predefined positions. For this, we first propose a Text-guided\n    Shape Deformation (TSD) network to predict the deformed shape and its new positions,\n    used as the centers (one attribute) of 3D Gaussians. To estimate the other four\n    attributes (i.e., scaling, rotation, opacity, and SH coefficient), we then design\n    a novel Text-guided Triplane Generator (TTG) to generate a triplane representation\n    for a 3D object. The center of each Gaussian enables us to transform the triplane\n    feature into the four attributes. The generated 3D Gaussians can be finally rendered\n    at 705 frames per second. Extensive experiments demonstrate the superiority of\n    our method over existing methods. Also, BrightDreamer possesses a strong semantic\n    understanding capability even for complex text prompts.\n  project_page: https://vlislab22.github.io/BrightDreamer/\n  paper: https://arxiv.org/pdf/2403.11273\n  code: https://github.com/lutao2021/BrightDreamer\n  video: https://vlislab22.github.io/BrightDreamer/videos/gui_demo.mp4\n  tags:\n  - Code\n  - Diffusion\n  - Project\n  - Video\n  thumbnail: assets/thumbnails/jiang2024brightdreamer.jpg\n  publication_date: '2024-03-17T17:04:45+00:00'\n- id: deng2024compact\n  title: Compact 3D Gaussian Splatting For Dense Visual SLAM\n  authors: Tianchen Deng, Yaohui Chen, Leyan Zhang, Jianfei Yang, Shenghai Yuan, Danwei\n    Wang, Weidong Chen\n  year: '2024'\n  abstract: Recent work has shown that 3D Gaussian-based SLAM enables high-quality\n    reconstruction, accurate pose estimation, and real-time rendering of scenes. However,\n    these approaches are built on a tremendous number of redundant 3D Gaussian ellipsoids,\n    leading to high memory and storage costs, and slow training speed. To address\n    the limitation, we propose a compact 3D Gaussian Splatting SLAM system that reduces\n    the number and the parameter size of Gaussian ellipsoids. A sliding window-based\n    masking strategy is first proposed to reduce the redundant ellipsoids. Then we\n    observe that the covariance matrix (geometry) of most 3D Gaussian ellipsoids are\n    extremely similar, which motivates a novel geometry codebook to compress 3D Gaussian\n    geometric attributes, i.e., the parameters. Robust and accurate pose estimation\n    is achieved by a global bundle adjustment method with reprojection loss. Extensive\n    experiments demonstrate that our method achieves faster training and rendering\n    speed while maintaining the state-of-the-art (SOTA) quality of the scene representation.\n  project_page: null\n  paper: https://arxiv.org/pdf/2403.11247.pdf\n  code: null\n  video: null\n  tags:\n  - SLAM\n  thumbnail: assets/thumbnails/deng2024compact.jpg\n  publication_date: '2024-03-17T15:41:35+00:00'\n- id: wu2024recent\n  title: Recent Advances in 3D Gaussian Splatting\n  authors: Tong Wu, Yu-Jie Yuan, Ling-Xiao Zhang, Jie Yang, Yan-Pei Cao, Ling-Qi Yan,\n    Lin Gao\n  year: '2024'\n  abstract: The emergence of 3D Gaussian Splatting (3DGS) has greatly accelerated\n    the rendering speed of novel view synthesis. Unlike neural implicit representations\n    like Neural Radiance Fields (NeRF) that represent a 3D scene with position and\n    viewpoint-conditioned neural networks, 3D Gaussian Splatting utilizes a set of\n    Gaussian ellipsoids to model the scene so that efficient rendering can be accomplished\n    by rasterizing Gaussian ellipsoids into images. Apart from the fast rendering\n    speed, the explicit representation of 3D Gaussian Splatting facilitates editing\n    tasks like dynamic reconstruction, geometry editing, and physical simulation.\n    Considering the rapid change and growing number of works in this field, we present\n    a literature review of recent 3D Gaussian Splatting methods, which can be roughly\n    classified into 3D reconstruction, 3D editing, and other downstream applications\n    by functionality. Traditional point-based rendering methods and the rendering\n    formulation of 3D Gaussian Splatting are also illustrated for a better understanding\n    of this technique. This survey aims to help beginners get into this field quickly\n    and provide experienced researchers with a comprehensive overview, which can stimulate\n    the future development of the 3D Gaussian Splatting representation.\n  project_page: null\n  paper: https://arxiv.org/pdf/2403.11134\n  code: null\n  video: null\n  tags:\n  - Review\n  thumbnail: assets/thumbnails/wu2024recent.jpg\n  publication_date: '2024-03-17T07:57:08+00:00'\n- id: liang2024analyticsplatting\n  title: 'Analytic-Splatting: Anti-Aliased 3D Gaussian Splatting via Analytic Integration'\n  authors: Zhihao Liang, Qi Zhang, Wenbo Hu, Ying Feng, Lei Zhu, Kui Jia\n  year: '2024'\n  abstract: The 3D Gaussian Splatting (3DGS) gained its popularity recently by combining\n    the advantages of both primitive-based and volumetric 3D representations, resulting\n    in improved quality and efficiency for 3D scene rendering. However, 3DGS is not\n    alias-free, and its rendering at varying resolutions could produce severe blurring\n    or jaggies. This is because 3DGS treats each pixel as an isolated, single point\n    rather than as an area, causing insensitivity to changes in the footprints of\n    pixels. Consequently, this discrete sampling scheme inevitably results in aliasing,\n    owing to the restricted sampling bandwidth. In this paper, we derive an analytical\n    solution to address this issue. More specifically, we use a conditioned logistic\n    function as the analytic approximation of the cumulative distribution function\n    (CDF) in a one-dimensional Gaussian signal and calculate the Gaussian integral\n    by subtracting the CDFs. We then introduce this approximation in the two-dimensional\n    pixel shading, and present Analytic-Splatting, which analytically approximates\n    the Gaussian integral within the 2D-pixel window area to better capture the intensity\n    response of each pixel. Moreover, we use the approximated response of the pixel\n    window integral area to participate in the transmittance calculation of volume\n    rendering, making Analytic-Splatting sensitive to the changes in pixel footprint\n    at different resolutions. Experiments on various datasets validate that our approach\n    has better anti-aliasing capability that gives more details and better fidelity.\n  project_page: https://lzhnb.github.io/project-pages/analytic-splatting/\n  paper: https://arxiv.org/pdf/2403.11056.pdf\n  code: https://github.com/lzhnb/Analytic-Splatting\n  video: null\n  tags:\n  - Antialiasing\n  - Code\n  - Project\n  - Rendering\n  thumbnail: assets/thumbnails/liang2024analyticsplatting.jpg\n  publication_date: '2024-03-17T02:06:03+00:00'\n- id: zhang2024darkgs\n  title: 'DarkGS: Learning Neural Illumination and 3D Gaussians Relighting for Robotic\n    Exploration in the Dark'\n  authors: Tianyi Zhang, Kaining Huang, Weiming Zhi, Matthew Johnson-Roberson\n  year: '2024'\n  abstract: Humans have the remarkable ability to construct consistent mental models\n    of an environment, even under limited or varying levels of illumination. We wish\n    to endow robots with this same capability. In this paper, we tackle the challenge\n    of constructing a photorealistic scene representation under poorly illuminated\n    conditions and with a moving light source. We approach the task of modeling illumination\n    as a learning problem, and utilize the developed illumination model to aid in\n    scene reconstruction. We introduce an innovative framework that uses a data-driven\n    approach, Neural Light Simulators (NeLiS), to model and calibrate the camera-light\n    system. Furthermore, we present DarkGS, a method that applies NeLiS to create\n    a relightable 3D Gaussian scene model capable of real-time, photorealistic rendering\n    from novel viewpoints. We show the applicability and robustness of our proposed\n    simulator and system in a variety of real-world environments.\n  project_page: null\n  paper: https://arxiv.org/pdf/2403.10814\n  code: https://github.com/tyz1030/darkgs\n  video: https://www.linkedin.com/posts/tianyi-zhang-396b0a186_darkgs-building-3d-gaussians-with-a-torch-activity-7197672371393019905-iY2-?utm_source=share&utm_medium=member_desktop\n  tags:\n  - Code\n  - Misc\n  - Robotics\n  - Video\n  thumbnail: assets/thumbnails/zhang2024darkgs.jpg\n  publication_date: '2024-03-16T05:21:42+00:00'\n- id: cai2024gspose\n  title: 'GS-Pose: Cascaded Framework for Generalizable Segmentation-based 6D Object\n    Pose Estimation'\n  authors: Dingding Cai, Janne Heikkilä, Esa Rahtu\n  year: '2024'\n  abstract: This paper introduces GS-Pose, an end-to-end framework for locating and\n    estimating the 6D pose of objects. GS-Pose begins with a set of posed RGB images\n    of a previously unseen object and builds three distinct representations stored\n    in a database. At inference, GS-Pose operates sequentially by locating the object\n    in the input image, estimating its initial 6D pose using a retrieval approach,\n    and refining the pose with a render-and-compare method. The key insight is the\n    application of the appropriate object representation at each stage of the process.\n    In particular, for the refinement step, we utilize 3D Gaussian splatting, a novel\n    differentiable rendering technique that offers high rendering speed and relatively\n    low optimization time. Off-the-shelf toolchains and commodity hardware, such as\n    mobile phones, can be used to capture new objects to be added to the database.\n    Extensive evaluations on the LINEMOD and OnePose-LowTexture datasets demonstrate\n    excellent performance, establishing the new state-of-the-art.\n  project_page: https://dingdingcai.github.io/gs-pose/\n  paper: https://arxiv.org/pdf/2403.10683\n  code: https://github.com/dingdingcai/GSPose\n  video: https://youtu.be/SnJazusDLM8\n  tags:\n  - Poses\n  thumbnail: assets/thumbnails/cai2024gspose.jpg\n  publication_date: '2024-03-15T21:06:14+00:00'\n- id: dahmani2024swag\n  title: 'SWAG: Splatting in the Wild images with Appearance-conditioned Gaussians'\n  authors: Hiba Dahmani, Moussab Bennehar, Nathan Piasco, Luis Roldao, Dzmitry Tsishkou\n  year: '2024'\n  abstract: Implicit neural representation methods have shown impressive advancements\n    in learning 3D scenes from unstructured in-the-wild photo collections but are\n    still limited by the large computational cost of volumetric rendering. More recently,\n    3D Gaussian Splatting emerged as a much faster alternative with superior rendering\n    quality and training efficiency, especially for small-scale and object-centric\n    scenarios. Nevertheless, this technique suffers from poor performance on unstructured\n    in-the-wild data. To tackle this, we extend over 3D Gaussian Splatting to handle\n    unstructured image collections. We achieve this by modeling appearance to seize\n    photometric variations in the rendered images. Additionally, we introduce a new\n    mechanism to train transient Gaussians to handle the presence of scene occluders\n    in an unsupervised manner. Experiments on diverse photo collection scenes and\n    multi-pass acquisition of outdoor landmarks show the effectiveness of our method\n    over prior works achieving state-of-the-art results with improved efficiency.\n  project_page: null\n  paper: https://arxiv.org/pdf/2403.10427.pdf\n  code: null\n  video: null\n  tags:\n  - In the Wild\n  - Rendering\n  thumbnail: assets/thumbnails/dahmani2024swag.jpg\n  publication_date: '2024-03-15T16:00:04+00:00'\n- id: feng2024fdgaussian\n  title: 'FDGaussian: Fast Gaussian Splatting from Single Image via Geometric-aware\n    Diffusion Model'\n  authors: Qijun Feng, Zhen Xing, Zuxuan Wu, Yu-Gang Jiang\n  year: '2024'\n  abstract: Reconstructing detailed 3D objects from single-view images remains a challenging\n    task due to the limited information available. In this paper, we introduce FDGaussian,\n    a novel two-stage framework for single-image 3D reconstruction. Recent methods\n    typically utilize pre-trained 2D diffusion models to generate plausible novel\n    views from the input image, yet they encounter issues with either multi-view inconsistency\n    or lack of geometric fidelity. To overcome these challenges, we propose an orthogonal\n    plane decomposition mechanism to extract 3D geometric features from the 2D input,\n    enabling the generation of consistent multi-view images. Moreover, we further\n    accelerate the state-of-the-art Gaussian Splatting incorporating epipolar attention\n    to fuse images from different viewpoints. We demonstrate that FDGaussian generates\n    images with high consistency across different views and reconstructs high-quality\n    3D objects, both qualitatively and quantitatively.\n  project_page: null\n  paper: https://arxiv.org/pdf/2403.10242.pdf\n  code: null\n  video: null\n  tags:\n  - Diffusion\n  thumbnail: assets/thumbnails/feng2024fdgaussian.jpg\n  publication_date: '2024-03-15T12:24:36+00:00'\n- id: li2024ggrt\n  title: 'GGRt: Towards Generalizable 3D Gaussians without Pose Priors in Real-Time'\n  authors: Hao Li, Yuanyuan Gao, Dingwen Zhang, Chenming Wu, Yalun Dai, Chen Zhao,\n    Haocheng Feng, Errui Ding, Jingdong Wang, Junwei Han\n  year: '2024'\n  abstract: This paper presents GGRt, a novel approach to generalizable novel view\n    synthesis that alleviates the need for real camera poses, complexity in processing\n    high-resolution images, and lengthy optimization processes, thus facilitating\n    stronger applicability of 3D Gaussian Splatting (3D-GS) in real-world scenarios.\n    Specifically, we design a novel joint learning framework that consists of an Iterative\n    Pose Optimization Network (IPO-Net) and a Generalizable 3D-Gaussians (G-3DG) model.\n    With the joint learning mechanism, the proposed framework can inherently estimate\n    robust relative pose information from the image observations and thus primarily\n    alleviate the requirement of real camera poses. Moreover, we implement a deferred\n    back-propagation mechanism that enables high-resolution training and inference,\n    overcoming the resolution constraints of previous methods. To enhance the speed\n    and efficiency, we further introduce a progressive Gaussian cache module that\n    dynamically adjusts during training and inference. As the first pose-free generalizable\n    3D-GS framework, GGRt achieves inference at ≥ 5 FPS and real-time rendering at\n    ≥ 100 FPS. Through extensive experimentation, we demonstrate that our method outperforms\n    existing NeRF-based pose-free techniques in terms of inference speed and effectiveness.\n    It can also approach the real pose-based 3D-GS methods. Our contributions provide\n    a significant leap forward for the integration of computer vision and computer\n    graphics into practical applications, offering state-of-the-art results on LLFF,\n    KITTI, and Waymo Open datasets and enabling real-time rendering for immersive\n    experiences.\n  project_page: https://3d-aigc.github.io/GGRt/\n  paper: https://arxiv.org/pdf/2403.10147\n  code: https://github.com/lifuguan/GGRt_official\n  video: null\n  tags:\n  - Code\n  - Poses\n  - Project\n  thumbnail: assets/thumbnails/li2024ggrt.jpg\n  publication_date: '2024-03-15T09:47:35+00:00'\n- id: xu2024texturegs\n  title: 'Texture-GS: Disentangling the Geometry and Texture for 3D Gaussian Splatting\n    Editing'\n  authors: Tian-Xing Xu, Wenbo Hu, Yu-Kun Lai, Ying Shan, Song-Hai Zhang\n  year: '2024'\n  abstract: 3D Gaussian splatting, emerging as a groundbreaking approach, has drawn\n    increasing attention for its capabilities of high-fidelity reconstruction and\n    real-time rendering. However, it couples the appearance and geometry of the scene\n    within the Gaussian attributes, which hinders the flexibility of editing operations,\n    such as texture swapping. To address this issue, we propose a novel approach,\n    namely Texture-GS, to disentangle the appearance from the geometry by representing\n    it as a 2D texture mapped onto the 3D surface, thereby facilitating appearance\n    editing. Technically, the disentanglement is achieved by our proposed texture\n    mapping module, which consists of a UV mapping MLP to learn the UV coordinates\n    for the 3D Gaussian centers, a local Taylor expansion of the MLP to efficiently\n    approximate the UV coordinates for the ray-Gaussian intersections, and a learnable\n    texture to capture the fine-grained appearance. Extensive experiments on the DTU\n    dataset demonstrate that our method not only facilitates high-fidelity appearance\n    editing but also achieves real-time rendering on consumer-level devices, e.g.\n    a single RTX 2080 Ti GPU.\n  project_page: https://slothfulxtx.github.io/TexGS/\n  paper: https://arxiv.org/pdf/2403.10050\n  code: https://github.com/slothfulxtx/Texture-GS\n  video: https://www.youtube.com/watch?v=mn_vi4a_fu4\n  tags:\n  - Code\n  - Editing\n  - Project\n  - Texturing\n  - Video\n  thumbnail: assets/thumbnails/xu2024texturegs.jpg\n  publication_date: '2024-03-15T06:42:55+00:00'\n- id: li2024controllable\n  title: Controllable Text-to-3D Generation via Surface-Aligned Gaussian Splatting\n  authors: Zhiqi Li, Yiming Chen, Lingzhe Zhao, Peidong Liu\n  year: '2024'\n  abstract: While text-to-3D and image-to-3D generation tasks have received considerable\n    attention, one important but under-explored field between them is controllable\n    text-to-3D generation, which we mainly focus on in this work. To address this\n    task, 1) we introduce ControlNet (MVControl), a novel neural network architecture\n    designed to enhance existing pre-trained multi-view diffusion models by integrating\n    additional input conditions, such as edge, depth, normal, and scribble maps. Our\n    innovation lies in the introduction of a conditioning module that controls the\n    base diffusion model using both local and global embeddings, which are computed\n    from the input condition images and camera poses. Once trained, MVControl is able\n    to offer 3D diffusion guidance for optimization-based 3D generation. And, 2) we\n    propose an efficient multi-stage 3D generation pipeline that leverages the benefits\n    of recent large reconstruction models and score distillation algorithm. Building\n    upon our MVControl architecture, we employ a unique hybrid diffusion guidance\n    method to direct the optimization process. In pursuit of efficiency, we adopt\n    3D Gaussians as our representation instead of the commonly used implicit representations.\n    We also pioneer the use of SuGaR, a hybrid representation that binds Gaussians\n    to mesh triangle faces. This approach alleviates the issue of poor geometry in\n    3D Gaussians and enables the direct sculpting of fine-grained geometry on the\n    mesh. Extensive experiments demonstrate that our method achieves robust generalization\n    and enables the controllable generation of high-quality 3D content.\n  project_page: https://lizhiqi49.github.io/MVControl/\n  paper: https://arxiv.org/pdf/2403.09981.pdf\n  code: https://github.com/WU-CVGL/MVControl-threestudio\n  video: null\n  tags:\n  - Code\n  - Diffusion\n  - Project\n  thumbnail: assets/thumbnails/li2024controllable.jpg\n  publication_date: '2024-03-15T02:57:20+00:00'\n  date_source: arxiv\n- id: yu2024densoft\n  title: 'Den-SOFT: Dense Space-Oriented Light Field DataseT for 6-DOF Immersive Experience'\n  authors: Xiaohang Yu, Zhengxian Yang, Shi Pan, Yuqi Han, Haoxiang Wang, Jun Zhang,\n    Shi Yan, Borong Lin, Lei Yang, Tao Yu, Lu Fang\n  year: '2024'\n  abstract: We have built a custom mobile multi-camera large-space dense light field\n    capture system, which provides a series of high-quality and sufficiently dense\n    light field images for various scenarios. Our aim is to contribute to the development\n    of popular 3D scene reconstruction algorithms such as IBRnet, NeRF, and 3D Gaussian\n    splitting. More importantly, the collected dataset, which is much denser than\n    existing datasets, may also inspire space-oriented light field reconstruction,\n    which is potentially different from object-centric 3D reconstruction, for immersive\n    VR/AR experiences. We utilized a total of 40 GoPro 10 cameras, capturing images\n    of 5k resolution. The number of photos captured for each scene is no less than\n    1000, and the average density (view number within a unit sphere) is 134.68. It\n    is also worth noting that our system is capable of efficiently capturing large\n    outdoor scenes. Addressing the current lack of large-space and dense light field\n    datasets, we made efforts to include elements such as sky, reflections, lights\n    and shadows that are of interest to researchers in the field of 3D reconstruction\n    during the data capture process. Finally, we validated the effectiveness of our\n    provided dataset on three popular algorithms and also integrated the reconstructed\n    3DGS results into the Unity engine, demonstrating the potential of utilizing our\n    datasets to enhance the realism of virtual reality (VR) and create feasible interactive\n    spaces.\n  project_page: https://metaverse-ai-lab-thu.github.io/Den-SOFT/\n  paper: https://arxiv.org/pdf/2403.09973.pdf\n  code: null\n  video: null\n  tags:\n  - Project\n  - Virtual Reality\n  thumbnail: assets/thumbnails/yu2024densoft.jpg\n  publication_date: '2024-03-15T02:39:44+00:00'\n- id: swann2024touchgs\n  title: 'Touch-GS: Visual-Tactile Supervised 3D Gaussian Splatting'\n  authors: Aiden Swann, Matthew Strong, Won Kyung Do, Gadiel Sznaier Camps, Mac Schwager,\n    Monroe Kennedy III\n  year: '2024'\n  abstract: In this work, we propose a novel method to supervise 3D Gaussian Splatting\n    (3DGS) scenes using optical tactile sensors. Optical tactile sensors have become\n    widespread in their use in robotics for manipulation and object representation;\n    however, raw optical tactile sensor data is unsuitable to directly supervise a\n    3DGS scene. Our representation leverages a Gaussian Process Implicit Surface to\n    implicitly represent the object, combining many touches into a unified representation\n    with uncertainty. We merge this model with a monocular depth estimation network,\n    which is aligned in a two stage process, coarsely aligning with a depth camera\n    and then finely adjusting to match our touch data. For every training image, our\n    method produces a corresponding fused depth and uncertainty map. Utilizing this\n    additional information, we propose a new loss function, variance weighted depth\n    supervised loss, for training the 3DGS scene model. We leverage the DenseTact\n    optical tactile sensor and RealSense RGB-D camera to show that combining touch\n    and vision in this manner leads to quantitatively and qualitatively better results\n    than vision or touch alone in a few-view scene syntheses on opaque as well as\n    on reflective and transparent objects.\n  project_page: https://armlabstanford.github.io/touch-gs\n  paper: https://arxiv.org/pdf/2403.09875.pdf\n  code: https://github.com/armlabstanford/Touch-GS\n  video: https://www.youtube.com/watch?v=FqejaTEt7aU\n  tags:\n  - Code\n  - Project\n  - Rendering\n  - Robotics\n  - Sparse\n  - Video\n  thumbnail: assets/thumbnails/swann2024touchgs.jpg\n  publication_date: '2024-03-14T21:09:59+00:00'\n- id: zheng2024gaussiangrasper\n  title: 'GaussianGrasper: 3D Language Gaussian Splatting for Open-vocabulary Robotic\n    Grasping'\n  authors: Yuhang Zheng, Xiangyu Chen, Yupeng Zheng, Songen Gu, Runyi Yang, Bu Jin,\n    Pengfei Li, Chengliang Zhong, Zengmao Wang, Lina Liu, Chao Yang, Dawei Wang, Zhen\n    Chen, Xiaoxiao Long, Meiqing Wang\n  year: '2024'\n  abstract: Constructing a 3D scene capable of accommodating open-ended language queries,\n    is a pivotal pursuit, particularly within the domain of robotics. Such technology\n    facilitates robots in executing object manipulations based on human language directives.\n    To tackle this challenge, some research efforts have been dedicated to the development\n    of language-embedded implicit fields. However, implicit fields (e.g. NeRF) encounter\n    limitations due to the necessity of processing a large number of input views for\n    reconstruction, coupled with their inherent inefficiencies in inference. Thus,\n    we present the GaussianGrasper, which utilizes 3D Gaussian Splatting to explicitly\n    represent the scene as a collection of Gaussian primitives. Our approach takes\n    a limited set of RGB-D views and employs a tile-based splatting technique to create\n    a feature field. In particular, we propose an Efficient Feature Distillation (EFD)\n    module that employs contrastive learning to efficiently and accurately distill\n    language embeddings derived from foundational models. With the reconstructed geometry\n    of the Gaussian field, our method enables the pre-trained grasping model to generate\n    collision-free grasp pose candidates. Furthermore, we propose a normal-guided\n    grasp module to select the best grasp pose. Through comprehensive real-world experiments,\n    we demonstrate that GaussianGrasper enables robots to accurately query and grasp\n    objects with language instructions, providing a new solution for language-guided\n    manipulation tasks.\n  project_page: https://mrsecant.github.io/GaussianGrasper/\n  paper: https://arxiv.org/pdf/2403.09637\n  code: https://github.com/MrSecant/GaussianGrasper\n  video: null\n  tags:\n  - Code\n  - Language Embedding\n  - Project\n  - Robotics\n  - Segmentation\n  thumbnail: assets/thumbnails/zheng2024gaussiangrasper.jpg\n  publication_date: '2024-03-14T17:59:46+00:00'\n- id: zhong2024reconstruction\n  title: Reconstruction and Simulation of Elastic Objects with Spring-Mass 3D Gaussians\n  authors: Licheng Zhong, Hong-Xing Yu, Jiajun Wu, Yunzhu Li\n  year: '2024'\n  abstract: Reconstructing and simulating elastic objects from visual observations\n    is crucial for applications in computer vision and robotics. Existing methods,\n    such as 3D Gaussians, provide modeling for 3D appearance and geometry but lack\n    the ability to simulate physical properties or optimize parameters for heterogeneous\n    objects. We propose Spring-Gaus, a novel framework that integrates 3D Gaussians\n    with physics-based simulation for reconstructing and simulating elastic objects\n    from multi-view videos. Our method utilizes a 3D Spring-Mass model, enabling the\n    optimization of physical parameters at the individual point level while decoupling\n    the learning of physics and appearance. This approach achieves great sample efficiency,\n    enhances generalization, and reduces sensitivity to the distribution of simulation\n    particles. We evaluate Spring-Gaus on both synthetic and real-world datasets,\n    demonstrating accurate reconstruction and simulation of elastic objects. This\n    includes future prediction and simulation under varying initial states and environmental\n    parameters.\n  project_page: https://zlicheng.com/spring_gaus/\n  paper: https://arxiv.org/pdf/2403.09434\n  code: https://github.com/Colmar-zlicheng/Spring-Gaus\n  video: null\n  tags:\n  - Code\n  - Dynamic\n  - Physics\n  - Project\n  thumbnail: assets/thumbnails/zhong2024reconstruction.jpg\n  publication_date: '2024-03-14T14:25:10+00:00'\n- id: jung2024raings\n  title: 'RAIN-GS: Relaxing Accurate Initialization Constraint for 3D Gaussian Splatting'\n  authors: Jaewoo Jung, Jisang Han, Honggyu An, Jiwon Kang, Seonghoon Park, Seungryong\n    Kim\n  year: '2024'\n  abstract: 3D Gaussian splatting (3DGS) has recently demonstrated impressive capabilities\n    in real-time novel view synthesis and 3D reconstruction. However, 3DGS heavily\n    depends on the accurate initialization derived from Structure-from-Motion (SfM)\n    methods. When trained with randomly initialized point clouds, 3DGS often fails\n    to maintain its ability to produce high-quality images, undergoing large performance\n    drops of 4-5 dB in PSNR in general. Through extensive analysis of SfM initialization\n    in the frequency domain and analysis of a 1D regression task with multiple 1D\n    Gaussians, we propose a novel optimization strategy dubbed RAIN-GS (Relaxing Accurate\n    INitialization Constraint for 3D Gaussian Splatting) that successfully trains\n    3D Gaussians from randomly initialized point clouds. We show the effectiveness\n    of our strategy through quantitative and qualitative comparisons on standard datasets,\n    largely improving the performance in all settings.\n  project_page: https://ku-cvlab.github.io/RAIN-GS/\n  paper: https://arxiv.org/pdf/2403.09413\n  code: https://github.com/cvlab-kaist/RAIN-GS\n  video: null\n  tags:\n  - Code\n  - Densification\n  - Project\n  - Rendering\n  thumbnail: assets/thumbnails/jung2024raings.jpg\n  publication_date: '2024-03-14T14:04:21+00:00'\n- id: di2024hyper3dgtextto3d\n  title: Hyper-3DG:Text-to-3D Gaussian Generation via Hypergraph\n  authors: Donglin Di, Jiahui Yang, Chaofan Luo, Zhou Xue, Wei Chen, Xun Yang, Yue\n    Gao\n  year: '2024'\n  abstract: Text-to-3D generation represents an exciting field that has seen rapid\n    advancements, facilitating the transformation of textual descriptions into detailed\n    3D models. However, current progress often neglects the intricate high-order correlation\n    of geometry and texture within 3D objects, leading to challenges such as over-smoothness,\n    over-saturation and the Janus problem. In this work, we propose a method named\n    ``3D Gaussian Generation via Hypergraph (Hyper-3DG)'', designed to capture the\n    sophisticated high-order correlations present within 3D objects. Our framework\n    is anchored by a well-established mainflow and an essential module, named ``Geometry\n    and Texture Hypergraph Refiner (HGRefiner)''. This module not only refines the\n    representation of 3D Gaussians but also accelerates the update process of these\n    3D Gaussians by conducting the Patch-3DGS Hypergraph Learning on both explicit\n    attributes and latent visual features. Our framework allows for the production\n    of finely generated 3D objects within a cohesive optimization, effectively circumventing\n    degradation. Extensive experimentation has shown that our proposed method significantly\n    enhances the quality of 3D generation while incurring no additional computational\n    overhead for the underlying framework.\n  project_page: null\n  paper: https://arxiv.org/pdf/2403.09236.pdf\n  code: null\n  video: null\n  tags:\n  - Diffusion\n  thumbnail: assets/thumbnails/di2024hyper3dgtextto3d.jpg\n  publication_date: '2024-03-14T09:59:55+00:00'\n- id: feng2024a\n  title: A New Split Algorithm for 3D Gaussian Splatting\n  authors: Qiyuan Feng, Gengchen Cao, Haoxiang Chen, Tai-Jiang Mu, Ralph R. Martin,\n    Shi-Min Hu\n  year: '2024'\n  abstract: 3D Gaussian splatting models, as a novel explicit 3D representation, have\n    been applied in many domains recently, such as explicit geometric editing and\n    geometry generation. Progress has been rapid. However, due to their mixed scales\n    and cluttered shapes, 3D Gaussian splatting models can produce a blurred or needle-like\n    effect near the surface. At the same time, 3D Gaussian splatting models tend to\n    flatten large untextured regions, yielding a very sparse point cloud. These problems\n    are caused by the non-uniform nature of 3D Gaussian splatting models, so in this\n    paper, we propose a new 3D Gaussian splitting algorithm, which can produce a more\n    uniform and surface-bounded 3D Gaussian splatting model. Our algorithm splits\n    an N-dimensional Gaussian into two N-dimensional Gaussians. It ensures consistency\n    of mathematical characteristics and similarity of appearance, allowing resulting\n    3D Gaussian splatting models to be more uniform and a better fit to the underlying\n    surface, and thus more suitable for explicit editing, point cloud extraction and\n    other tasks. Meanwhile, our 3D Gaussian splitting approach has a very simple closed-form\n    solution, making it readily applicable to any 3D Gaussian model.\n  project_page: null\n  paper: https://arxiv.org/pdf/2403.09143\n  code: null\n  video: null\n  tags:\n  - Densification\n  thumbnail: assets/thumbnails/feng2024a.jpg\n  publication_date: '2024-03-14T07:42:12+00:00'\n- id: wu2024gaussctrl\n  title: 'GaussCtrl: Multi-View Consistent Text-Driven 3D Gaussian Splatting Editing'\n  authors: Jing Wu, Jia-Wang Bian, Xinghui Li, Guangrun Wang, Ian Reid, Philip Torr,\n    Victor Adrian Prisacariu\n  year: '2024'\n  abstract: 'We propose GaussCtrl, a text-driven method to edit a 3D scene reconstructed\n    by the 3D Gaussian Splatting (3DGS). Our method first renders a collection of\n    images by using the 3DGS and edits them by using a pre-trained 2D diffusion model\n    (ControlNet) based on the input prompt, which is then used to optimise the 3D\n    model. Our key contribution is multi-view consistent editing, which enables editing\n    all images together instead of iteratively editing one image while updating the\n    3D model as in previous works. It leads to faster editing as well as higher visual\n    quality. This is achieved by the two terms: (a) depth-conditioned editing that\n    enforces geometric consistency across multi-view images by leveraging naturally\n    consistent depth maps. (b) attention-based latent code alignment that unifies\n    the appearance of edited images by conditioning their editing to several reference\n    views through self and cross-view attention between images'' latent representations.\n    Experiments demonstrate that our method achieves faster editing and better visual\n    results than previous state-of-the-art methods.'\n  project_page: https://gaussctrl.active.vision/\n  paper: https://arxiv.org/pdf/2403.08733.pdf\n  code: https://github.com/ActiveVisionLab/gaussctrl\n  video: https://gaussctrl.active.vision/assets/teaser_vid.mp4\n  tags:\n  - Code\n  - Editing\n  - Project\n  - Video\n  thumbnail: assets/thumbnails/wu2024gaussctrl.jpg\n  publication_date: '2024-03-13T17:35:28+00:00'\n- id: zhang2024gaussianimage\n  title: 'GaussianImage: 1000 FPS Image Representation and Compression by 2D Gaussian\n    Splatting'\n  authors: Xinjie Zhang, Xingtong Ge, Tongda Xu, Dailan He, Yan Wang, Hongwei Qin,\n    Guo Lu, Jing Geng, Jun Zhang\n  year: '2024'\n  abstract: Implicit neural representations (INRs) recently achieved great success\n    in image representation and compression, offering high visual quality and fast\n    rendering speeds with 10-1000 FPS, assuming sufficient GPU resources are available.\n    However, this requirement often hinders their use on low-end devices with limited\n    memory. In response, we propose a groundbreaking paradigm of image representation\n    and compression by 2D Gaussian Splatting, named GaussianImage. We first introduce\n    2D Gaussian to represent the image, where each Gaussian has 8 parameters including\n    position, covariance and color. Subsequently, we unveil a novel rendering algorithm\n    based on accumulated summation. Remarkably, our method with a minimum of 3× lower\n    GPU memory usage and 5× faster fitting time not only rivals INRs (e.g., WIRE,\n    I-NGP) in representation performance, but also delivers a faster rendering speed\n    of 1500-2000 FPS regardless of parameter size. Furthermore, we integrate existing\n    vector quantization technique to build an image codec. Experimental results demonstrate\n    that our codec attains rate-distortion performance comparable to compression-based\n    INRs such as COIN and COIN++, while facilitating decoding speeds of approximately\n    1000 FPS. Additionally, preliminary proof of concept shows that our codec surpasses\n    COIN and COIN++ in performance when using partial bits-back coding.\n  project_page: https://xingtongge.github.io/GaussianImage-page/\n  paper: https://arxiv.org/pdf/2403.08551\n  code: https://github.com/Xinjie-Q/GaussianImage\n  video: null\n  tags:\n  - 2DGS\n  - Code\n  - Compression\n  - Project\n  thumbnail: assets/thumbnails/zhang2024gaussianimage.jpg\n  publication_date: '2024-03-13T14:02:54+00:00'\n- id: saroha2024gaussian\n  title: Gaussian Splatting in Style\n  authors: Abhishek Saroha, Mariia Gladkova, Cecilia Curreli, Tarun Yenamandra, Daniel\n    Cremers\n  year: '2024'\n  abstract: Scene stylization extends the work of neural style transfer to three spatial\n    dimensions. A vital challenge in this problem is to maintain the uniformity of\n    the stylized appearance across a multi-view setting. A vast majority of the previous\n    works achieve this by optimizing the scene with a specific style image. In contrast,\n    we propose a novel architecture trained on a collection of style images, that\n    at test time produces high quality stylized novel views. Our work builds up on\n    the framework of 3D Gaussian splatting. For a given scene, we take the pretrained\n    Gaussians and process them using a multi resolution hash grid and a tiny MLP to\n    obtain the conditional stylised views. The explicit nature of 3D Gaussians give\n    us inherent advantages over NeRF-based methods including geometric consistency,\n    along with having a fast training and rendering regime. This enables our method\n    to be useful for vast practical use cases such as in augmented or virtual reality\n    applications. Through our experiments, we show our methods achieve state-of-the-art\n    performance with superior visual quality on various indoor and outdoor real-world\n    data.\n  project_page: null\n  paper: https://arxiv.org/pdf/2403.08498\n  code: null\n  video: null\n  tags:\n  - Style Transfer\n  thumbnail: assets/thumbnails/saroha2024gaussian.jpg\n  publication_date: '2024-03-13T13:06:31+00:00'\n- id: lu2024manigaussian\n  title: 'ManiGaussian: Dynamic Gaussian Splatting for Multi-task Robotic Manipulation'\n  authors: Guanxing Lu, Shiyi Zhang, Ziwei Wang, Changliu Liu, Jiwen Lu, Yansong Tang\n  year: '2024'\n  abstract: Performing language-conditioned robotic manipulation tasks in unstructured\n    environments is highly demanded for general intelligent robots. Conventional robotic\n    manipulation methods usually learn semantic representation of the observation\n    for action prediction, which ignores the scene-level spatiotemporal dynamics for\n    human goal completion. In this paper, we propose a dynamic Gaussian Splatting\n    method named ManiGaussian for multi-task robotic manipulation, which mines scene\n    dynamics via future scene reconstruction. Specifically, we first formulate the\n    dynamic Gaussian Splatting framework that infers the semantics propagation in\n    the Gaussian embedding space, where the semantic representation is leveraged to\n    predict the optimal robot action. Then, we build a Gaussian world model to parameterize\n    the distribution in our dynamic Gaussian Splatting framework, which provides informative\n    supervision in the interactive environment via future scene reconstruction. We\n    evaluate our ManiGaussian on 10 RLBench tasks with 166 variations, and the results\n    demonstrate our framework can outperform the state-of-the-art methods by 13.1%\n    in average success rate.\n  project_page: https://guanxinglu.github.io/ManiGaussian/\n  paper: https://arxiv.org/pdf/2403.08321.pdf\n  code: https://github.com/GuanxingLu/ManiGaussian\n  video: null\n  tags:\n  - Code\n  - Dynamic\n  - Project\n  - Robotics\n  thumbnail: assets/thumbnails/lu2024manigaussian.jpg\n  publication_date: '2024-03-13T08:06:41+00:00'\n- id: liu2024stylegaussian\n  title: 'StyleGaussian: Instant 3D Style Transfer with Gaussian Splatting'\n  authors: Kunhao Liu, Fangneng Zhan, Muyu Xu, Christian Theobalt, Ling Shao, Shijian\n    Lu\n  year: '2024'\n  abstract: 'We introduce StyleGaussian, a novel 3D style transfer technique that\n    allows instant transfer of any image''s style to a 3D scene at 10 frames per second\n    (fps). Leveraging 3D Gaussian Splatting (3DGS), StyleGaussian achieves style transfer\n    without compromising its real-time rendering ability and multi-view consistency.\n    It achieves instant style transfer with three steps: embedding, transfer, and\n    decoding. Initially, 2D VGG scene features are embedded into reconstructed 3D\n    Gaussians. Next, the embedded features are transformed according to a reference\n    style image. Finally, the transformed features are decoded into the stylized RGB.\n    StyleGaussian has two novel designs. The first is an efficient feature rendering\n    strategy that first renders low-dimensional features and then maps them into high-dimensional\n    features while embedding VGG features. It cuts the memory consumption significantly\n    and enables 3DGS to render the high-dimensional memory-intensive features. The\n    second is a K-nearest-neighbor-based 3D CNN. Working as the decoder for the stylized\n    features, it eliminates the 2D CNN operations that compromise strict multi-view\n    consistency. Extensive experiments show that StyleGaussian achieves instant 3D\n    stylization with superior stylization quality while preserving real-time rendering\n    and strict multi-view consistency.'\n  project_page: https://kunhao-liu.github.io/StyleGaussian/\n  paper: https://arxiv.org/pdf/2403.07807.pdf\n  code: https://github.com/Kunhao-Liu/StyleGaussian\n  video: null\n  tags:\n  - Code\n  - Project\n  - Style Transfer\n  thumbnail: assets/thumbnails/liu2024stylegaussian.jpg\n  publication_date: '2024-03-12T16:44:52+00:00'\n- id: zhu2024semgaussslam\n  title: 'SemGauss-SLAM: Dense Semantic Gaussian Splatting SLAM'\n  authors: Siting Zhu, Renjie Qin, Guangming Wang, Jiuming Liu, Hesheng Wang\n  year: '2024'\n  abstract: We propose SemGauss-SLAM, the first semantic SLAM system utilizing 3D\n    Gaussian representation, that enables accurate 3D semantic mapping, robust camera\n    tracking, and high-quality rendering in real-time. In this system, we incorporate\n    semantic feature embedding into 3D Gaussian representation, which effectively\n    encodes semantic information within the spatial layout of the environment for\n    precise semantic scene representation. Furthermore, we propose feature-level loss\n    for updating 3D Gaussian representation, enabling higher-level guidance for 3D\n    Gaussian optimization. In addition, to reduce cumulative drift and improve reconstruction\n    accuracy, we introduce semantic-informed bundle adjustment leveraging semantic\n    associations for joint optimization of 3D Gaussian representation and camera poses,\n    leading to more robust tracking and consistent mapping. Our SemGauss-SLAM method\n    demonstrates superior performance over existing dense semantic SLAM methods in\n    terms of mapping and tracking accuracy on Replica and ScanNet datasets, while\n    also showing excellent capabilities in novel-view semantic synthesis and 3D semantic\n    mapping.\n  project_page: null\n  paper: https://arxiv.org/pdf/2403.07494.pdf\n  code: null\n  video: null\n  tags:\n  - SLAM\n  thumbnail: assets/thumbnails/zhu2024semgaussslam.jpg\n  publication_date: '2024-03-12T10:33:26+00:00'\n- id: li2024dngaussian\n  title: 'DNGaussian: Optimizing Sparse-View 3D Gaussian Radiance Fields with Global-Local\n    Depth Normalization'\n  authors: Jiahe Li, Jiawei Zhang, Xiao Bai, Jin Zheng, Xin Ning, Jun Zhou, Lin Gu\n  year: '2024'\n  abstract: Radiance fields have demonstrated impressive performance in synthesizing\n    novel views from sparse input views, yet prevailing methods suffer from high training\n    costs and slow inference speed. This paper introduces DNGaussian, a depth-regularized\n    framework based on 3D Gaussian radiance fields, offering real-time and high-quality\n    few-shot novel view synthesis at low costs. Our motivation stems from the highly\n    efficient representation and surprising quality of the recent 3D Gaussian Splatting,\n    despite it will encounter a geometry degradation when input views decrease. In\n    the Gaussian radiance fields, we find this degradation in scene geometry primarily\n    lined to the positioning of Gaussian primitives and can be mitigated by depth\n    constraint. Consequently, we propose a Hard and Soft Depth Regularization to restore\n    accurate scene geometry under coarse monocular depth supervision while maintaining\n    a fine-grained color appearance. To further refine detailed geometry reshaping,\n    we introduce Global-Local Depth Normalization, enhancing the focus on small local\n    depth changes. Extensive experiments on LLFF, DTU, and Blender datasets demonstrate\n    that DNGaussian outperforms state-of-the-art methods, achieving comparable or\n    better results with significantly reduced memory cost, a 25× reduction in training\n    time, and over 3000× faster rendering speed.\n  project_page: https://fictionarry.github.io/DNGaussian/\n  paper: https://arxiv.org/pdf/2403.06912.pdf\n  code: https://github.com/Fictionarry/DNGaussian\n  video: https://www.youtube.com/watch?v=WKXCFNJHZ4o\n  tags:\n  - Code\n  - Project\n  - Sparse\n  - Video\n  thumbnail: assets/thumbnails/li2024dngaussian.jpg\n  publication_date: '2024-03-11T17:02:11+00:00'\n- id: zhang2024fregs\n  title: 'FreGS: 3D Gaussian Splatting with Progressive Frequency Regularization'\n  authors: Jiahui Zhang, Fangneng Zhan, Muyu Xu, Shijian Lu, Eric Xing\n  year: '2024'\n  abstract: 3D Gaussian splatting has achieved very impressive performance in real-time\n    novel view synthesis. However, it often suffers from over-reconstruction during\n    Gaussian densification where high-variance image regions are covered by a few\n    large Gaussians only, leading to blur and artifacts in the rendered images. We\n    design a progressive frequency regularization (FreGS) technique to tackle the\n    over-reconstruction issue within the frequency space. Specifically, FreGS performs\n    coarse-to-fine Gaussian densification by exploiting low-to-high frequency components\n    that can be easily extracted with low-pass and high-pass filters in the Fourier\n    space. By minimizing the discrepancy between the frequency spectrum of the rendered\n    image and the corresponding ground truth, it achieves high-quality Gaussian densification\n    and alleviates the over-reconstruction of Gaussian splatting effectively. Experiments\n    over multiple widely adopted benchmarks (e.g., Mip-NeRF360, Tanks-and-Temples\n    and Deep Blending) show that FreGS achieves superior novel view synthesis and\n    outperforms the state-of-the-art consistently.\n  project_page: null\n  paper: https://arxiv.org/pdf/2403.06908.pdf\n  code: null\n  video: null\n  tags:\n  - Densification\n  thumbnail: assets/thumbnails/zhang2024fregs.jpg\n  publication_date: '2024-03-11T17:00:27+00:00'\n- id: palandra2024gsedit\n  title: 'GSEdit: Efficient Text-Guided Editing of 3D Objects via Gaussian Splatting'\n  authors: Francesco Palandra, Andrea Sanchietti, Daniele Baieri, Emanuele Rodolà\n  year: '2024'\n  abstract: We present GSEdit, a pipeline for text-guided 3D object editing based\n    on Gaussian Splatting models. Our method enables the editing of the style and\n    appearance of 3D objects without altering their main details, all in a matter\n    of minutes on consumer hardware. We tackle the problem by leveraging Gaussian\n    splatting to represent 3D scenes, and we optimize the model while progressively\n    varying the image supervision by means of a pretrained image-based diffusion model.\n    The input object may be given as a 3D triangular mesh, or directly provided as\n    Gaussians from a generative model such as DreamGaussian. GSEdit ensures consistency\n    across different viewpoints, maintaining the integrity of the original object's\n    information. Compared to previously proposed methods relying on NeRF-like MLP\n    models, GSEdit stands out for its efficiency, making 3D editing tasks much faster.\n    Our editing process is refined via the application of the SDS loss, ensuring that\n    our edits are both precise and accurate. Our comprehensive evaluation demonstrates\n    that GSEdit effectively alters object shape and appearance following the given\n    textual instructions while preserving their coherence and detail.\n  project_page: null\n  paper: https://arxiv.org/pdf/2403.05154.pdf\n  code: null\n  video: null\n  tags:\n  - Editing\n  thumbnail: assets/thumbnails/palandra2024gsedit.jpg\n  publication_date: '2024-03-08T08:42:23+00:00'\n- id: shao2024splattingavatar\n  title: 'SplattingAvatar: Realistic Real-Time Human Avatars with Mesh-Embedded Gaussian\n    Splatting'\n  authors: Zhijing Shao, Zhaolong Wang, Zhuang Li, Duotun Wang, Xiangru Lin, Yu Zhang,\n    Mingming Fan, Zeyu Wang\n  year: '2024'\n  abstract: We present SplattingAvatar, a hybrid 3D representation of photorealistic\n    human avatars with Gaussian Splatting embedded on a triangle mesh, which renders\n    over 300 FPS on a modern GPU and 30 FPS on a mobile device. We disentangle the\n    motion and appearance of a virtual human with explicit mesh geometry and implicit\n    appearance modeling with Gaussian Splatting. The Gaussians are defined by barycentric\n    coordinates and displacement on a triangle mesh as Phong surfaces. We extend lifted\n    optimization to simultaneously optimize the parameters of the Gaussians while\n    walking on the triangle mesh. SplattingAvatar is a hybrid representation of virtual\n    humans where the mesh represents low-frequency motion and surface deformation,\n    while the Gaussians take over the high-frequency geometry and detailed appearance.\n    Unlike existing deformation methods that rely on an MLP-based linear blend skinning\n    (LBS) field for motion, we control the rotation and translation of the Gaussians\n    directly by mesh, which empowers its compatibility with various animation techniques,\n    e.g., skeletal animation, blend shapes, and mesh editing. Trainable from monocular\n    videos for both full-body and head avatars, SplattingAvatar shows state-of-the-art\n    rendering quality across multiple datasets.\n  project_page: https://initialneil.github.io/SplattingAvatar\n  paper: https://arxiv.org/pdf/2403.05087.pdf\n  code: https://github.com/initialneil/SplattingAvatar\n  video: https://www.youtube.com/watch?v=IzC-fLvdntA\n  tags:\n  - Avatar\n  - Code\n  - Project\n  - Video\n  thumbnail: assets/thumbnails/shao2024splattingavatar.jpg\n  publication_date: '2024-03-08T06:28:09+00:00'\n- id: peng2024bags\n  title: 'BAGS: Blur Agnostic Gaussian Splatting through Multi-Scale Kernel Modeling'\n  authors: Cheng Peng, Yutao Tang, Yifan Zhou, Nengyu Wang, Xijun Liu, Deming Li,\n    Rama Chellappa\n  year: '2024'\n  abstract: Recent efforts in using 3D Gaussians for scene reconstruction and novel\n    view synthesis can achieve impressive results on curated benchmarks; however,\n    images captured in real life are often blurry. In this work, we analyze the robustness\n    of Gaussian-Splatting-based methods against various image blur, such as motion\n    blur, defocus blur, downscaling blur, \\etc. Under these degradations, Gaussian-Splatting-based\n    methods tend to overfit and produce worse results than Neural-Radiance-Field-based\n    methods. To address this issue, we propose Blur Agnostic Gaussian Splatting (BAGS).\n    BAGS introduces additional 2D modeling capacities such that a 3D-consistent and\n    high quality scene can be reconstructed despite image-wise blur. Specifically,\n    we model blur by estimating per-pixel convolution kernels from a Blur Proposal\n    Network (BPN). BPN is designed to consider spatial, color, and depth variations\n    of the scene to maximize modeling capacity. Additionally, BPN also proposes a\n    quality-assessing mask, which indicates regions where blur occur. Finally, we\n    introduce a coarse-to-fine kernel optimization scheme; this optimization scheme\n    is fast and avoids sub-optimal solutions due to a sparse point cloud initialization,\n    which often occurs when we apply Structure-from-Motion on blurry images. We demonstrate\n    that BAGS achieves photorealistic renderings under various challenging blur conditions\n    and imaging geometry, while significantly improving upon existing approaches.\n  project_page: https://nwang43jhu.github.io/BAGS/\n  paper: https://arxiv.org/pdf/2403.04926.pdf\n  code: https://github.com/snldmt/BAGS\n  video: null\n  tags:\n  - Code\n  - Deblurring\n  - Project\n  thumbnail: assets/thumbnails/peng2024bags.jpg\n  publication_date: '2024-03-07T22:21:08+00:00'\n- id: cai2024radiative\n  title: Radiative Gaussian Splatting for Efficient X-ray Novel View Synthesis\n  authors: TYuanhao Cai, Yixun Liang, Jiahao Wang, Angtian Wang, Yulun Zhang, Xiaokang\n    Yang, Zongwei Zhou, Alan Yuille\n  year: '2024'\n  abstract: X-ray is widely applied for transmission imaging due to its stronger penetration\n    than natural light. When rendering novel view X-ray projections, existing methods\n    mainly based on NeRF suffer from long training time and slow inference speed.\n    In this paper, we propose a 3D Gaussian splatting-based framework, namely X-Gaussian,\n    for X-ray novel view synthesis. Firstly, we redesign a radiative Gaussian point\n    cloud model inspired by the isotropic nature of X-ray imaging. Our model excludes\n    the influence of view direction when learning to predict the radiation intensity\n    of 3D points. Based on this model, we develop a Differentiable Radiative Rasterization\n    (DRR) with CUDA implementation. Secondly, we customize an Angle-pose Cuboid Uniform\n    Initialization (ACUI) strategy that directly uses the parameters of the X-ray\n    scanner to compute the camera information and then uniformly samples point positions\n    within a cuboid enclosing the scanned object. Experiments show that our X-Gaussian\n    outperforms state-of-the-art methods by 6.5 dB while enjoying less than 15% training\n    time and over 73x inference speed. The application on sparse-view CT reconstruction\n    also reveals the practical values of our method.\n  project_page: null\n  paper: https://arxiv.org/pdf/2403.04116.pdf\n  code: null\n  video: null\n  tags:\n  - Medicine\n  thumbnail: assets/thumbnails/cai2024radiative.jpg\n  publication_date: '2024-03-07T00:12:08+00:00'\n- id: chen2024splatnav\n  title: 'Splat-Nav: Safe Real-Time Robot Navigation in Gaussian Splatting Maps'\n  authors: Timothy Chen, Ola Shorinwa, Weijia Zeng, Joseph Bruno, Philip Dames, Mac\n    Schwager\n  year: '2024'\n  abstract: We present Splat-Nav, a navigation pipeline that consists of a real-time\n    safe planning module and a robust state estimation module designed to operate\n    in the Gaussian Splatting (GSplat) environment representation, a popular emerging\n    3D scene representation from computer vision. We formulate rigorous collision\n    constraints that can be computed quickly to build a guaranteed-safe polytope corridor\n    through the map. We then optimize a B-spline trajectory through this corridor.\n    We also develop a real-time, robust state estimation module by interpreting the\n    GSplat representation as a point cloud. The module enables the robot to localize\n    its global pose with zero prior knowledge from RGB-D images using point cloud\n    alignment, and then track its own pose as it moves through the scene from RGB\n    images using image-to-point cloud localization. We also incorporate semantics\n    into the GSplat in order to obtain better images for localization. All of these\n    modules operate mainly on CPU, freeing up GPU resources for tasks like real-time\n    scene reconstruction. We demonstrate the safety and robustness of our pipeline\n    in both simulation and hardware, where we show re-planning at 5 Hz and pose estimation\n    at 20 Hz, an order of magnitude faster than Neural Radiance Field (NeRF)-based\n    navigation methods, thereby enabling real-time navigation.\n  project_page: null\n  paper: https://arxiv.org/pdf/2403.02751.pdf\n  code: null\n  video: null\n  tags:\n  - Robotics\n  thumbnail: assets/thumbnails/chen2024splatnav.jpg\n  publication_date: '2024-03-05T08:10:11+00:00'\n- id: sun20233dgstream\n  title: '3DGStream: On-the-Fly Training of 3D Gaussians for Efficient Streaming of\n    Photo-Realistic Free-Viewpoint Videos'\n  authors: Jiakai Sun, Han Jiao, Guangyuan Li, Zhanjie Zhang, Lei Zhao, Wei Xing\n  year: '2023'\n  abstract: Constructing photo-realistic Free-Viewpoint Videos (FVVs) of dynamic scenes\n    from multi-view videos remains a challenging endeavor. Despite the remarkable\n    advancements achieved by current neural rendering techniques, these methods generally\n    require complete video sequences for offline training and are not capable of real-time\n    rendering. To address these constraints, we introduce 3DGStream, a method designed\n    for efficient FVV streaming of real-world dynamic scenes. Our method achieves\n    fast on-the-fly per-frame reconstruction within 12 seconds and real-time rendering\n    at 200 FPS. Specificallggy, we utilize 3D Gaussians (3DGs) to represent the scene.\n    Instead of the naïve approach of directly optimizing 3DGs per-frame, we employ\n    a compact Neural Transformation Cache (NTC) to model the translations and rotations\n    of 3DGs, markedly reducing the training time and storage required for each FVV\n    frame. Furthermore, we propose an adaptive 3DG addition strategy to handle emerging\n    objects in dynamic scenes. Experiments demonstrate that 3DGStream achieves competitive\n    performance in terms of rendering speed, image quality, training time, and model\n    storage when compared with state-of-the-art methods.\n  project_page: https://sjojok.github.io/3dgstream/\n  paper: https://arxiv.org/pdf/2403.01444.pdf\n  code: https://github.com/SJoJoK/3DGStream\n  video: null\n  tags:\n  - Code\n  - Dynamic\n  - Project\n  thumbnail: assets/thumbnails/sun20233dgstream.jpg\n  publication_date: '2024-03-03T08:42:40+00:00'\n- id: lin2024vastgaussian\n  title: 'VastGaussian: Vast 3D Gaussians for Large Scene Reconstruction'\n  authors: Jiaqi Lin, Zhihao Li, Xiao Tang, Jianzhuang Liu, Shiyong Liu, Jiayue Liu,\n    Yangdi Lu, Xiaofei Wu, Songcen Xu, Youliang Yan, Wenming Yang\n  year: '2024'\n  abstract: Existing NeRF-based methods for large scene reconstruction often have\n    limitations in visual quality and rendering speed. While the recent 3D Gaussian\n    Splatting works well on small-scale and object-centric scenes, scaling it up to\n    large scenes poses challenges due to limited video memory, long optimization time,\n    and noticeable appearance variations. To address these challenges, we present\n    VastGaussian, the first method for high-quality reconstruction and real-time rendering\n    on large scenes based on 3D Gaussian Splatting. We propose a progressive partitioning\n    strategy to divide a large scene into multiple cells, where the training cameras\n    and point cloud are properly distributed with an airspace-aware visibility criterion.\n    These cells are merged into a complete scene after parallel optimization. We also\n    introduce decoupled appearance modeling into the optimization process to reduce\n    appearance variations in the rendered images. Our approach outperforms existing\n    NeRF-based methods and achieves state-of-the-art results on multiple large scene\n    datasets, enabling fast optimization and high-fidelity real-time rendering.\n  project_page: https://vastgaussian.github.io/\n  paper: https://arxiv.org/pdf/2402.17427.pdf\n  code: https://github.com/kangpeilun/VastGaussian\n  video: null\n  tags:\n  - Large-Scale\n  thumbnail: assets/thumbnails/lin2024vastgaussian.jpg\n  publication_date: '2024-02-27T11:40:50+00:00'\n- id: liu2024gva\n  title: 'GVA: Reconstructing Vivid 3D Gaussian Avatars from Monocular Videos'\n  authors: Xinqi Liu, Chenming Wu, Jialun Liu, Xing Liu, Jinbo Wu, Chen Zhao, Haocheng\n    Feng, Errui Ding, Jingdong Wang\n  year: '2024'\n  abstract: In this paper, we present a novel method that facilitates the creation\n    of vivid 3D Gaussian avatars from monocular video inputs (GVA). Our innovation\n    lies in addressing the intricate challenges of delivering high-fidelity human\n    body reconstructions and aligning 3D Gaussians with human skin surfaces accurately.\n    The key contributions of this paper are twofold. Firstly, we introduce a pose\n    refinement technique to improve hand and foot pose accuracy by aligning normal\n    maps and silhouettes. Precise pose is crucial for correct shape and appearance\n    reconstruction. Secondly, we address the problems of unbalanced aggregation and\n    initialization bias that previously diminished the quality of 3D Gaussian avatars,\n    through a novel surface-guided re-initialization method that ensures accurate\n    alignment of 3D Gaussian points with avatar surfaces. Experimental results demonstrate\n    that our proposed method achieves high-fidelity and vivid 3D Gaussian avatar reconstruction.\n    Extensive experimental analyses validate the performance qualitatively and quantitatively,\n    demonstrating that it achieves state-of-the-art performance in photo-realistic\n    novel view synthesis while offering fine-grained control over the human body and\n    hand pose.\n  project_page: https://3d-aigc.github.io/GEA/\n  paper: https://arxiv.org/pdf/2402.16607.pdf\n  code: null\n  video: null\n  tags:\n  - Avatar\n  - Project\n  thumbnail: assets/thumbnails/liu2024gva.jpg\n  publication_date: '2024-02-26T14:40:15+00:00'\n- id: yang2024specgaussian\n  title: 'Spec-Gaussian: Anisotropic View-Dependent Appearance for 3D Gaussian Splatting'\n  authors: Ziyi Yang, Xinyu Gao, Yangtian Sun, Yihua Huang, Xiaoyang Lyu, Wen Zhou,\n    Shaohui Jiao, Xiaojuan Qi, Xiaogang Jin\n  year: '2024'\n  abstract: The recent advancements in 3D Gaussian splatting (3D-GS) have not only\n    facilitated real-time rendering through modern GPU rasterization pipelines but\n    have also attained state-of-the-art rendering quality. Nevertheless, despite its\n    exceptional rendering quality and performance on standard datasets, 3D-GS frequently\n    encounters difficulties in accurately modeling specular and anisotropic components.\n    This issue stems from the limited ability of spherical harmonics (SH) to represent\n    high-frequency information. To overcome this challenge, we introduce Spec-Gaussian,\n    an approach that utilizes an anisotropic spherical Gaussian (ASG) appearance field\n    instead of SH for modeling the view-dependent appearance of each 3D Gaussian.\n    Additionally, we have developed a coarse-to-fine training strategy to improve\n    learning efficiency and eliminate floaters caused by overfitting in real-world\n    scenes. Our experimental results demonstrate that our method surpasses existing\n    approaches in terms of rendering quality. Thanks to ASG, we have significantly\n    improved the ability of 3D-GS to model scenes with specular and anisotropic components\n    without increasing the number of 3D Gaussians. This improvement extends the applicability\n    of 3D GS to handle intricate scenarios with specular and anisotropic surfaces.\n  project_page: https://ingra14m.github.io/Spec-Gaussian-website/\n  paper: https://arxiv.org/pdf/2402.15870.pdf\n  code: https://github.com/ingra14m/Specular-Gaussians\n  video: null\n  tags:\n  - Code\n  - Project\n  - Rendering\n  thumbnail: assets/thumbnails/yang2024specgaussian.jpg\n  publication_date: '2024-02-24T17:22:15+00:00'\n- id: cheng2024gaussianpro\n  title: 'GaussianPro: 3D Gaussian Splatting with Progressive Propagation'\n  authors: Kai Cheng, Xiaoxiao Long, Kaizhi Yang, Yao Yao, Wei Yin, Yuexin Ma, Wenping\n    Wang, Xuejin Chen\n  year: '2024'\n  abstract: The advent of 3D Gaussian Splatting (3DGS) has recently brought about\n    a revolution in the field of neural rendering, facilitating high-quality renderings\n    at real-time speed. However, 3DGS heavily depends on the initialized point cloud\n    produced by Structure-from-Motion (SfM) techniques. When tackling with large-scale\n    scenes that unavoidably contain texture-less surfaces, the SfM techniques always\n    fail to produce enough points in these surfaces and cannot provide good initialization\n    for 3DGS. As a result, 3DGS suffers from difficult optimization and low-quality\n    renderings. In this paper, inspired by classical multi-view stereo (MVS) techniques,\n    we propose GaussianPro, a novel method that applies a progressive propagation\n    strategy to guide the densification of the 3D Gaussians. Compared to the simple\n    split and clone strategies used in 3DGS, our method leverages the priors of the\n    existing reconstructed geometries of the scene and patch matching techniques to\n    produce new Gaussians with accurate positions and orientations. Experiments on\n    both large-scale and small-scale scenes validate the effectiveness of our method,\n    where our method significantly surpasses 3DGS on the Waymo dataset, exhibiting\n    an improvement of 1.15dB in terms of PSNR.\n  project_page: https://kcheng1021.github.io/gaussianpro.github.io/\n  paper: https://arxiv.org/pdf/2402.14650.pdf\n  code: https://github.com/kcheng1021/GaussianPro\n  video: null\n  tags:\n  - Code\n  - Densification\n  - Project\n  thumbnail: assets/thumbnails/cheng2024gaussianpro.jpg\n  publication_date: '2024-02-22T16:00:20+00:00'\n- id: tosi2024how\n  title: 'How NeRFs and 3D Gaussian Splatting are Reshaping SLAM: a Survey'\n  authors: Fabio Tosi, Youmin Zhang, Ziren Gong, Erik Sandström, Stefano Mattoccia,\n    Martin R. Oswald, Matteo Poggi\n  year: '2024'\n  abstract: Over the past two decades, research in the field of Simultaneous Localization\n    and Mapping (SLAM) has undergone a significant evolution, highlighting its critical\n    role in enabling autonomous exploration of unknown environments. This evolution\n    ranges from hand-crafted methods, through the era of deep learning, to more recent\n    developments focused on Neural Radiance Fields (NeRFs) and 3D Gaussian Splatting\n    (3DGS) representations. Recognizing the growing body of research and the absence\n    of a comprehensive survey on the topic, this paper aims to provide the first comprehensive\n    overview of SLAM progress through the lens of the latest advancements in radiance\n    fields. It sheds light on the background, evolutionary path, inherent strengths\n    and limitations, and serves as a fundamental reference to highlight the dynamic\n    progress and specific challenges.\n  project_page: null\n  paper: https://arxiv.org/pdf/2402.13255.pdf\n  code: null\n  video: null\n  tags:\n  - Review\n  thumbnail: assets/thumbnails/tosi2024how.jpg\n  publication_date: '2024-02-20T18:59:57+00:00'\n- id: luo2024gaussianhair\n  title: 'GaussianHair: Hair Modeling and Rendering with Light-aware Gaussians'\n  authors: Haimin Luo, Min Ouyang, Zijun Zhao, Suyi Jiang, Longwen Zhang, Qixuan Zhang,\n    Wei Yang, Lan Xu, Jingyi Yu\n  year: '2024'\n  abstract: Hairstyle reflects culture and ethnicity at first glance. In the digital\n    era, various realistic human hairstyles are also critical to high-fidelity digital\n    human assets for beauty and inclusivity. Yet, realistic hair modeling and real-time\n    rendering for animation is a formidable challenge due to its sheer number of strands,\n    complicated structures of geometry, and sophisticated interaction with light.\n    This paper presents GaussianHair, a novel explicit hair representation. It enables\n    comprehensive modeling of hair geometry and appearance from images, fostering\n    innovative illumination effects and dynamic animation capabilities. At the heart\n    of GaussianHair is the novel concept of representing each hair strand as a sequence\n    of connected cylindrical 3D Gaussian primitives. This approach not only retains\n    the hair's geometric structure and appearance but also allows for efficient rasterization\n    onto a 2D image plane, facilitating differentiable volumetric rendering. We further\n    enhance this model with the \"GaussianHair Scattering Model\", adept at recreating\n    the slender structure of hair strands and accurately capturing their local diffuse\n    color in uniform lighting. Through extensive experiments, we substantiate that\n    GaussianHair achieves breakthroughs in both geometric and appearance fidelity,\n    transcending the limitations encountered in state-of-the-art methods for hair\n    reconstruction. Beyond representation, GaussianHair extends to support editing,\n    relighting, and dynamic rendering of hair, offering seamless integration with\n    conventional CG pipeline workflows. Complementing these advancements, we have\n    compiled an extensive dataset of real human hair, each with meticulously detailed\n    strand geometry, to propel further research in this field.\n  project_page: null\n  paper: https://arxiv.org/pdf/2402.10483.pdf\n  code: null\n  video: null\n  tags:\n  - Avatar\n  thumbnail: assets/thumbnails/luo2024gaussianhair.jpg\n  publication_date: '2024-02-16T07:13:24+00:00'\n- id: yang2024gaussianobject\n  title: 'GaussianObject: Just Taking Four Images to Get A High-Quality 3D Object\n    with Gaussian Splatting'\n  authors: Chen Yang, Sikuang Li, Jiemin Fang, Ruofan Liang, Lingxi Xie, Xiaopeng\n    Zhang, Wei Shen, Qi Tian\n  year: '2024'\n  abstract: 'Reconstructing and rendering 3D objects from highly sparse views is of\n    critical importance for promoting applications of 3D vision techniques and improving\n    user experience. However, images from sparse views only contain very limited 3D\n    information, leading to two significant challenges: 1) Difficulty in building\n    multi-view consistency as images for matching are too few; 2) Partially omitted\n    or highly compressed object information as view coverage is insufficient. To tackle\n    these challenges, we propose GaussianObject, a framework to represent and render\n    the 3D object with Gaussian splatting, that achieves high rendering quality with\n    only 4 input images. We first introduce techniques of visual hull and floater\n    elimination which explicitly inject structure priors into the initial optimization\n    process for helping build multi-view consistency, yielding a coarse 3D Gaussian\n    representation. Then we construct a Gaussian repair model based on diffusion models\n    to supplement the omitted object information, where Gaussians are further refined.\n    We design a self-generating strategy to obtain image pairs for training the repair\n    model. Our GaussianObject is evaluated on several challenging datasets, including\n    MipNeRF360, OmniObject3D, and OpenIllumination, achieving strong reconstruction\n    results from only 4 views and significantly outperforming previous state-of-the-art\n    methods.'\n  project_page: https://gaussianobject.github.io/\n  paper: https://arxiv.org/pdf/2402.10259.pdf\n  code: https://github.com/GaussianObject/GaussianObject\n  video: https://youtu.be/ozoI0tmW3r0?si=KcaHtvVnrexqaf58\n  tags:\n  - Code\n  - Diffusion\n  - Project\n  - Video\n  thumbnail: assets/thumbnails/yang2024gaussianobject.jpg\n  publication_date: '2024-02-15T18:42:33+00:00'\n- id: hamdi2024ges\n  title: 'GES: Generalized Exponential Splatting for Efficient Radiance Field Rendering'\n  authors: Abdullah Hamdi, Luke Melas-Kyriazi, Guocheng Qian, Jinjie Mai, Ruoshi Liu,\n    Carl Vondrick, Bernard Ghanem, Andrea Vedaldi\n  year: '2024'\n  abstract: Advancements in 3D Gaussian Splatting have significantly accelerated 3D\n    reconstruction and generation. However, it may require a large number of Gaussians,\n    which creates a substantial memory footprint. This paper introduces GES (Generalized\n    Exponential Splatting), a novel representation that employs Generalized Exponential\n    Function (GEF) to model 3D scenes, requiring far fewer particles to represent\n    a scene and thus significantly outperforming Gaussian Splatting methods in efficiency\n    with a plug-and-play replacement ability for Gaussian-based utilities. GES is\n    validated theoretically and empirically in both principled 1D setup and realistic\n    3D scenes. It is shown to represent signals with sharp edges more accurately,\n    which are typically challenging for Gaussians due to their inherent low-pass characteristics.\n    Our empirical analysis demonstrates that GEF outperforms Gaussians in fitting\n    natural-occurring signals (e.g. squares, triangles, and parabolic signals), thereby\n    reducing the need for extensive splitting operations that increase the memory\n    footprint of Gaussian Splatting. With the aid of a frequency-modulated loss, GES\n    achieves competitive performance in novel-view synthesis benchmarks while requiring\n    less than half the memory storage of Gaussian Splatting and increasing the rendering\n    speed by up to 39%.\n  project_page: https://abdullahamdi.com/ges/\n  paper: https://arxiv.org/pdf/2402.10128.pdf\n  code: https://github.com/ajhamdi/ges-splatting\n  video: https://youtu.be/edSvNy3roV8?si=VGncH7op1OfqkEtx\n  tags:\n  - Code\n  - Project\n  - Rendering\n  - Video\n  thumbnail: assets/thumbnails/hamdi2024ges.jpg\n  publication_date: '2024-02-15T17:32:50+00:00'\n- id: melaskyriazi2024im3d\n  title: ' IM-3D: Iterative Multiview Diffusion and Reconstruction for High-Quality\n    3D Generation'\n  authors: Luke Melas-Kyriazi, Iro Laina, Christian Rupprecht, Natalia Neverova, Andrea\n    Vedaldi, Oran Gafni, Filippos Kokkinos\n  year: '2024'\n  abstract: Most text-to-3D generators build upon off-the-shelf text-to-image models\n    trained on billions of images. They use variants of Score Distillation Sampling\n    (SDS), which is slow, somewhat unstable, and prone to artifacts. A mitigation\n    is to fine-tune the 2D generator to be multi-view aware, which can help distillation\n    or can be combined with reconstruction networks to output 3D objects directly.\n    In this paper, we further explore the design space of text-to-3D models. We significantly\n    improve multi-view generation by considering video instead of image generators.\n    Combined with a 3D reconstruction algorithm which, by using Gaussian splatting,\n    can optimize a robust image-based loss, we directly produce high-quality 3D outputs\n    from the generated views. Our new method, IM-3D, reduces the number of evaluations\n    of the 2D generator network 10-100x, resulting in a much more efficient pipeline,\n    better quality, fewer geometric inconsistencies, and higher yield of usable 3D\n    assets.\n  project_page: https://lukemelas.github.io/IM-3D/\n  paper: https://arxiv.org/pdf/2402.08682.pdf\n  code: null\n  video: null\n  tags:\n  - Diffusion\n  - Project\n  thumbnail: assets/thumbnails/melaskyriazi2024im3d.jpg\n  publication_date: '2024-02-13T18:59:51+00:00'\n- id: zhou2024gala3d\n  title: 'GALA3D: Towards Text-to-3D Complex Scene Generation via Layout-guided Generative\n    Gaussian Splatting'\n  authors: Xiaoyu Zhou, Xingjian Ran, Yajiao Xiong, Jinlin He, Zhiwei Lin, Yongtao\n    Wang, Deqing Sun, Ming-Hsuan Yang\n  year: '2024'\n  abstract: We present GALA3D, generative 3D GAussians with LAyout-guided control,\n    for effective compositional text-to-3D generation. We first utilize large language\n    models (LLMs) to generate the initial layout and introduce a layout-guided 3D\n    Gaussian representation for 3D content generation with adaptive geometric constraints.\n    We then propose an object-scene compositional optimization mechanism with conditioned\n    diffusion to collaboratively generate realistic 3D scenes with consistent geometry,\n    texture, scale, and accurate interactions among multiple objects while simultaneously\n    adjusting the coarse layout priors extracted from the LLMs to align with the generated\n    scene. Experiments show that GALA3D is a user-friendly, end-to-end framework for\n    state-of-the-art scene-level 3D content generation and controllable editing while\n    ensuring the high fidelity of object-level entities within the scene.\n  project_page: https://gala3d.github.io/\n  paper: https://arxiv.org/pdf/2402.07207.pdf\n  code: null\n  video: null\n  tags:\n  - Diffusion\n  - Project\n  thumbnail: assets/thumbnails/zhou2024gala3d.jpg\n  publication_date: '2024-02-11T13:40:08+00:00'\n- id: fei20243d\n  title: '3D Gaussian as a New Vision Era: A Survey'\n  authors: Ben Fei, Jingyi Xu, Rui Zhang, Qingyuan Zhou, Weidong Yang, Ying He\n  year: '2024'\n  abstract: 3D Gaussian Splatting (3D-GS) has emerged as a significant advancement\n    in the field of Computer Graphics, offering explicit scene representation and\n    novel view synthesis without the reliance on neural networks, such as Neural Radiance\n    Fields (NeRF). This technique has found diverse applications in areas such as\n    robotics, urban mapping, autonomous navigation, and virtual reality/augmented\n    reality, just name a few. Given the growing popularity and expanding research\n    in 3D Gaussian Splatting, this paper presents a comprehensive survey of relevant\n    papers from the past year. We organize the survey into taxonomies based on characteristics\n    and applications, providing an introduction to the theoretical underpinnings of\n    3D Gaussian Splatting. Our goal through this survey is to acquaint new researchers\n    with 3D Gaussian Splatting, serve as a valuable reference for seminal works in\n    the field, and inspire future research directions, as discussed in our concluding\n    section.\n  project_page: null\n  paper: https://arxiv.org/pdf/2402.07181.pdf\n  code: null\n  video: null\n  tags:\n  - Review\n  thumbnail: assets/thumbnails/fei20243d.jpg\n  publication_date: '2024-02-11T12:33:08+00:00'\n- id: stanishevskii2024implicitdeepfake\n  title: 'ImplicitDeepfake: Plausible Face-Swapping through Implicit Deepfake Generation\n    using NeRF and Gaussian Splatting'\n  authors: Georgii Stanishevskii, Jakub Steczkiewicz, Tomasz Szczepanik, Sławomir\n    Tadeja, Jacek Tabor, Przemysław Spurek\n  year: '2024'\n  abstract: Numerous emerging deep-learning techniques have had a substantial impact\n    on computer graphics. Among the most promising breakthroughs are the recent rise\n    of Neural Radiance Fields (NeRFs) and Gaussian Splatting (GS). NeRFs encode the\n    object's shape and color in neural network weights using a handful of images with\n    known camera positions to generate novel views. In contrast, GS provides accelerated\n    training and inference without a decrease in rendering quality by encoding the\n    object's characteristics in a collection of Gaussian distributions. These two\n    techniques have found many use cases in spatial computing and other domains. On\n    the other hand, the emergence of deepfake methods has sparked considerable controversy.\n    Such techniques can have a form of artificial intelligence-generated videos that\n    closely mimic authentic footage. Using generative models, they can modify facial\n    features, enabling the creation of altered identities or facial expressions that\n    exhibit a remarkably realistic appearance to a real person. Despite these controversies,\n    deepfake can offer a next-generation solution for avatar creation and gaming when\n    of desirable quality. To that end, we show how to combine all these emerging technologies\n    to obtain a more plausible outcome. Our ImplicitDeepfake1 uses the classical deepfake\n    algorithm to modify all training images separately and then train NeRF and GS\n    on modified faces. Such relatively simple strategies can produce plausible 3D\n    deepfake-based avatars.\n  project_page: null\n  paper: https://arxiv.org/pdf/2402.06390.pdf\n  code: null\n  video: null\n  tags:\n  - Avatar\n  thumbnail: assets/thumbnails/stanishevskii2024implicitdeepfake.jpg\n  publication_date: '2024-02-09T13:11:57+00:00'\n- id: zhou2024headstudio\n  title: 'HeadStudio: Text to Animatable Head Avatars with 3D Gaussian Splatting'\n  authors: Zhenglin Zhou, Fan Ma, Hehe Fan, Yi Yang\n  year: '2024'\n  abstract: 'Creating digital avatars from textual prompts has long been a desirable\n    yet challenging task. Despite the promising outcomes obtained through 2D diffusion\n    priors in recent works, current methods face challenges in achieving high-quality\n    and animated avatars effectively. In this paper, we present HeadStudio, a novel\n    framework that utilizes 3D Gaussian splatting to generate realistic and animated\n    avatars from text prompts. Our method drives 3D Gaussians semantically to create\n    a flexible and achievable appearance through the intermediate FLAME representation.\n    Specifically, we incorporate the FLAME into both 3D representation and score distillation:\n    1) FLAME-based 3D Gaussian splatting, driving 3D Gaussian points by rigging each\n    point to a FLAME mesh. 2) FLAME-based score distillation sampling, utilizing FLAME-based\n    fine-grained control signal to guide score distillation from the text prompt.\n    Extensive experiments demonstrate the efficacy of HeadStudio in generating animatable\n    avatars from textual prompts, exhibiting visually appealing appearances. The avatars\n    are capable of rendering high-quality real-time (≥40 fps) novel views at a resolution\n    of 1024. They can be smoothly controlled by real-world speech and video. We hope\n    that HeadStudio can advance digital avatar creation and that the present method\n    can widely be applied across various domains.'\n  project_page: https://zhenglinzhou.github.io/HeadStudio-ProjectPage/\n  paper: https://arxiv.org/pdf/2402.06149.pdf\n  code: https://github.com/ZhenglinZhou/HeadStudio/\n  video: https://zhenglinzhou.github.io/HeadStudio-ProjectPage/videos/demo_arxiv.mp4\n  tags:\n  - Avatar\n  - Code\n  - Project\n  - Video\n  thumbnail: assets/thumbnails/zhou2024headstudio.jpg\n  publication_date: '2024-02-09T02:58:37+00:00'\n- id: yang2024large\n  title: Large Multi-View Gaussian Model for High-Resolution 3D Content Creation\n  authors: Chen Yang, Sikuang Li, Jiemin Fang, Ruofan Liang, Lingxi Xie, Xiaopeng\n    Zhang, Wei Shen, Qi Tian\n  year: '2024'\n  abstract: '3D content creation has achieved significant progress in terms of both\n    quality and speed. Although current feed-forward models can produce 3D objects\n    in seconds, their resolution is constrained by the intensive computation required\n    during training. In this paper, we introduce Large Multi-View Gaussian Model (LGM),\n    a novel framework designed to generate high-resolution 3D models from text prompts\n    or single-view images. Our key insights are two-fold: 1) 3D Representation: We\n    propose multi-view Gaussian features as an efficient yet powerful representation,\n    which can then be fused together for differentiable rendering. 2) 3D Backbone:\n    We present an asymmetric U-Net as a high-throughput backbone operating on multi-view\n    images, which can be produced from text or single-view image input by leveraging\n    multi-view diffusion models. Extensive experiments demonstrate the high fidelity\n    and efficiency of our approach. Notably, we maintain the fast speed to generate\n    3D objects within 5 seconds while boosting the training resolution to 512, thereby\n    achieving high-resolution 3D content generation.'\n  project_page: https://me.kiui.moe/lgm/\n  paper: https://arxiv.org/pdf/2402.05054.pdf\n  code: https://github.com/3DTopia/LGM\n  video: null\n  tags:\n  - Code\n  - Diffusion\n  - Project\n  thumbnail: assets/thumbnails/yang2024large.jpg\n  publication_date: '2024-02-07T17:57:03+00:00'\n- id: gao2024meshbased\n  title: Mesh-based Gaussian Splatting for Real-time Large-scale Deformation\n  authors: Lin Gao, Jie Yang, Bo-Tao Zhang, Jia-Mu Sun, Yu-Jie Yuan, Hongbo Fu, Yu-Kun\n    Lai\n  year: '2024'\n  abstract: 'Neural implicit representations, including Neural Distance Fields and\n    Neural Radiance Fields, have demonstrated significant capabilities for reconstructing\n    surfaces with complicated geometry and topology, and generating novel views of\n    a scene. Nevertheless, it is challenging for users to directly deform or manipulate\n    these implicit representations with large deformations in the real-time fashion.\n    Gaussian Splatting(GS) has recently become a promising method with explicit geometry\n    for representing static scenes and facilitating high-quality and real-time synthesis\n    of novel views. However,it cannot be easily deformed due to the use of discrete\n    Gaussians and lack of explicit topology. To address this, we develop a novel GS-based\n    method that enables interactive deformation. Our key idea is to design an innovative\n    mesh-based GS representation, which is integrated into Gaussian learning and manipulation.\n    3D Gaussians are defined over an explicit mesh, and they are bound with each other:\n    the rendering of 3D Gaussians guides the mesh face split for adaptive refinement,\n    and the mesh face split directs the splitting of 3D Gaussians. Moreover, the explicit\n    mesh constraints help regularize the Gaussian distribution, suppressing poor-quality\n    Gaussians(e.g. misaligned Gaussians,long-narrow shaped Gaussians), thus enhancing\n    visual quality and avoiding artifacts during deformation. Based on this representation,\n    we further introduce a large-scale Gaussian deformation technique to enable deformable\n    GS, which alters the parameters of 3D Gaussians according to the manipulation\n    of the associated mesh. Our method benefits from existing mesh deformation datasets\n    for more realistic data-driven Gaussian deformation. Extensive experiments show\n    that our approach achieves high-quality reconstruction and effective deformation,\n    while maintaining the promising rendering results at a high frame rate(65 FPS\n    on average).'\n  project_page: null\n  paper: https://arxiv.org/pdf/2402.04796.pdf\n  code: null\n  video: null\n  tags:\n  - Dynamic\n  - Meshing\n  thumbnail: assets/thumbnails/gao2024meshbased.jpg\n  publication_date: '2024-02-07T12:36:54+00:00'\n- id: rivero2024rig3dgs\n  title: 'Rig3DGS: Creating Controllable Portraits from Casual Monocular Videos'\n  authors: Alfredo Rivero, ShahRukh Athar, Zhixin Shu, Dimitris Samaras\n  year: '2024'\n  abstract: Creating controllable 3D human portraits from casual smartphone videos\n    is highly desirable due to their immense value in AR/VR applications. The recent\n    development of 3D Gaussian Splatting (3DGS) has shown improvements in rendering\n    quality and training efficiency. However, it still remains a challenge to accurately\n    model and disentangle head movements and facial expressions from a single-view\n    capture to achieve high-quality renderings. In this paper, we introduce Rig3DGS\n    to address this challenge. We represent the entire scene, including the dynamic\n    subject, using a set of 3D Gaussians in a canonical space. Using a set of control\n    signals, such as head pose and expressions, we transform them to the 3D space\n    with learned deformations to generate the desired rendering. Our key innovation\n    is a carefully designed deformation method which is guided by a learnable prior\n    derived from a 3D morphable model. This approach is highly efficient in training\n    and effective in controlling facial expressions, head positions, and view synthesis\n    across various captures. We demonstrate the effectiveness of our learned deformation\n    through extensive quantitative and qualitative experiments.\n  project_page: https://shahrukhathar.github.io/2024/02/05/Rig3DGS.html\n  paper: https://arxiv.org/pdf/2402.03723.pdf\n  code: null\n  video: null\n  tags:\n  - Avatar\n  - Project\n  thumbnail: assets/thumbnails/rivero2024rig3dgs.jpg\n  publication_date: '2024-02-06T05:40:53+00:00'\n- id: duan20244d\n  title: '4D Gaussian Splatting: Towards Efficient Novel View Synthesis for Dynamic\n    Scenes'\n  authors: Yuanxing Duan, Fangyin Wei, Qiyu Dai, Yuhang He, Wenzheng Chen, Baoquan\n    Chen\n  year: '2024'\n  abstract: We consider the problem of novel view synthesis (NVS) for dynamic scenes.\n    Recent neural approaches have accomplished exceptional NVS results for static\n    3D scenes, but extensions to 4D time-varying scenes remain non-trivial. Prior\n    efforts often encode dynamics by learning a canonical space plus implicit or explicit\n    deformation fields, which struggle in challenging scenarios like sudden movements\n    or capturing high-fidelity renderings. In this paper, we introduce 4D Gaussian\n    Splatting (4DGS), a novel method that represents dynamic scenes with anisotropic\n    4D XYZT Gaussians, inspired by the success of 3D Gaussian Splatting in static\n    scenes. We model dynamics at each timestamp by temporally slicing the 4D Gaussians,\n    which naturally compose dynamic 3D Gaussians and can be seamlessly projected into\n    images. As an explicit spatial-temporal representation, 4DGS demonstrates powerful\n    capabilities for modeling complicated dynamics and fine details, especially for\n    scenes with abrupt motions. We further implement our temporal slicing and splatting\n    techniques in a highly optimized CUDA acceleration framework, achieving real-time\n    inference rendering speeds of up to 277 FPS on an RTX 3090 GPU and 583 FPS on\n    an RTX 4090 GPU. Rigorous evaluations on scenes with diverse motions showcase\n    the superior efficiency and effectiveness of 4DGS, which consistently outperforms\n    existing methods both quantitatively and qualitatively.\n  project_page: https://weify627.github.io/4drotorgs/\n  paper: https://arxiv.org/pdf/2402.03307.pdf\n  code: https://github.com/weify627/4D-Rotor-Gaussians\n  video: https://youtu.be/18zPuhQ3sSs\n  tags:\n  - Code\n  - Dynamic\n  - Project\n  - Video\n  thumbnail: assets/thumbnails/duan20244d.jpg\n  publication_date: '2024-02-05T18:59:04+00:00'\n- id: li2024sgsslam\n  title: 'SGS-SLAM: Semantic Gaussian Splatting For Neural Dense SLAM'\n  authors: Mingrui Li, Shuhong Liu, Heng Zhou\n  year: '2024'\n  abstract: Semantic understanding plays a crucial role in Dense Simultaneous Localization\n    and Mapping (SLAM), facilitating comprehensive scene interpretation. Recent advancements\n    that integrate Gaussian Splatting into SLAM systems have demonstrated its effectiveness\n    in generating high-quality renderings through the use of explicit 3D Gaussian\n    representations. Building on this progress, we propose SGS-SLAM, the first semantic\n    dense visual SLAM system grounded in 3D Gaussians, which provides precise 3D semantic\n    segmentation alongside high-fidelity reconstructions. Specifically, we propose\n    to employ multi-channel optimization during the mapping process, integrating appearance,\n    geometric, and semantic constraints with key-frame optimization to enhance reconstruction\n    quality. Extensive experiments demonstrate that SGS-SLAM delivers state-of-the-art\n    performance in camera pose estimation, map reconstruction, and semantic segmentation,\n    outperforming existing methods meanwhile preserving real-time rendering ability.\n  project_page: null\n  paper: https://arxiv.org/pdf/2402.03246.pdf\n  code: https://github.com/ShuhongLL/SGS-SLAM\n  video: https://www.youtube.com/watch?v=y83yw1E-oUo\n  tags:\n  - Code\n  - SLAM\n  - Video\n  thumbnail: assets/thumbnails/li2024sgsslam.jpg\n  publication_date: '2024-02-05T18:03:53+00:00'\n- id: waczyńska2024games\n  title: 'GaMeS: Mesh-Based Adapting and Modification of Gaussian Splatting'\n  authors: Joanna Waczyńska, Piotr Borycki, Sławomir Tadeja, Jacek Tabor, Przemysław\n    Spurek\n  year: '2024'\n  abstract: In recent years, a range of neural network-based methods for image rendering\n    have been introduced. For instance, widely-researched neural radiance fields (NeRF)\n    rely on a neural network to represent 3D scenes, allowing for realistic view synthesis\n    from a small number of 2D images. However, most NeRF models are constrained by\n    long training and inference times. In comparison, Gaussian Splatting (GS) is a\n    novel, state-of-theart technique for rendering points in a 3D scene by approximating\n    their contribution to image pixels through Gaussian distributions, warranting\n    fast training and swift, real-time rendering. A drawback of GS is the absence\n    of a well-defined approach for its conditioning due to the necessity to condition\n    several hundred thousand Gaussian components. To solve this, we introduce Gaussian\n    Mesh Splatting (GaMeS) model, a hybrid of mesh and a Gaussian distribution, that\n    pin all Gaussians splats on the object surface (mesh). The unique contribution\n    of our methods is defining Gaussian splats solely based on their location on the\n    mesh, allowing for automatic adjustments in position, scale, and rotation during\n    animation. As a result, we obtain high-quality renders in the real-time generation\n    of high-quality views. Furthermore, we demonstrate that in the absence of a predefined\n    mesh, it is possible to fine-tune the initial mesh during the learning process.\n  project_page: null\n  paper: https://arxiv.org/pdf/2402.01459.pdf\n  code: https://github.com/waczjoan/gaussian-mesh-splatting\n  video: null\n  tags:\n  - Code\n  - Dynamic\n  - Meshing\n  thumbnail: assets/thumbnails/waczyńska2024games.jpg\n  publication_date: '2024-02-02T14:50:23+00:00'\n- id: huang2024360gs\n  title: '360-GS: Layout-guided Panoramic Gaussian Splatting For Indoor Roaming'\n  authors: Letian Huang, Jiayang Bai, Jie Guo, Yanwen Guo\n  year: '2024'\n  abstract: 3D Gaussian Splatting (3D-GS) has recently attracted great attention with\n    real-time and photo-realistic renderings. This technique typically takes perspective\n    images as input and optimizes a set of 3D elliptical Gaussians by splatting them\n    onto the image planes, resulting in 2D Gaussians. However, applying 3D-GS to panoramic\n    inputs presents challenges in effectively modeling the projection onto the spherical\n    surface of 360∘ images using 2D Gaussians. In practical applications, input panoramas\n    are often sparse, leading to unreliable initialization of 3D Gaussians and subsequent\n    degradation of 3D-GS quality. In addition, due to the under-constrained geometry\n    of texture-less planes (e.g., walls and floors), 3D-GS struggles to model these\n    flat regions with elliptical Gaussians, resulting in significant floaters in novel\n    views. To address these issues, we propose 360-GS, a novel 360∘ Gaussian splatting\n    for a limited set of panoramic inputs. Instead of splatting 3D Gaussians directly\n    onto the spherical surface, 360-GS projects them onto the tangent plane of the\n    unit sphere and then maps them to the spherical projections. This adaptation enables\n    the representation of the projection using Gaussians. We guide the optimization\n    of 360-GS by exploiting layout priors within panoramas, which are simple to obtain\n    and contain strong structural information about the indoor scene. Our experimental\n    results demonstrate that 360-GS allows panoramic rendering and outperforms state-of-the-art\n    methods with fewer artifacts in novel view synthesis, thus providing immersive\n    roaming in indoor scenarios.\n  project_page: null\n  paper: https://arxiv.org/pdf/2402.00763.pdf\n  code: https://github.com/LeoDarcy/360GS\n  video: null\n  tags:\n  - 360 degree\n  - Code\n  - Rendering\n  thumbnail: assets/thumbnails/huang2024360gs.jpg\n  publication_date: '2024-02-01T16:52:21+00:00'\n- id: huang2024optimal\n  title: 'On the Error Analysis of 3D Gaussian Splatting\n\n    and an Optimal Projection Strategy\n\n    '\n  authors: Letian Huang, Jiayang Bai, Jie Guo, Yanwen Guo\n  year: '2024'\n  abstract: 3D Gaussian Splatting has garnered extensive attention and application\n    in real-time neural rendering. Concurrently, concerns have been raised about the\n    limitations of this technology in aspects such as point cloud storage, performance,\n    and robustness in sparse viewpoints, leading to various improvements. However,\n    there has been a notable lack of attention to the fundamental problem of projection\n    errors introduced by the local affine approximation inherent in the splatting\n    itself, and the consequential impact of these errors on the quality of photo-realistic\n    rendering. This paper addresses the projection error function of 3D Gaussian Splatting,\n    commencing with the residual error from the first-order Taylor expansion of the\n    projection function. The analysis establishes a correlation between the error\n    and the Gaussian mean position. Subsequently, leveraging function optimization\n    theory, this paper analyzes the function's minima to provide an optimal projection\n    strategy for Gaussian Splatting referred to Optimal Gaussian Splatting, which\n    can accommodate a variety of camera models. Experimental validation further confirms\n    that this projection methodology reduces artifacts, resulting in a more convincingly\n    realistic rendering.\n  project_page: https://letianhuang.github.io/op43dgs/\n  paper: https://arxiv.org/pdf/2402.00752.pdf\n  code: https://github.com/LetianHuang/op43dgs\n  video: null\n  tags:\n  - Code\n  - Perspective-correct\n  - Project\n  - Rendering\n  thumbnail: assets/thumbnails/huang2024optimal.jpg\n  publication_date: '2024-02-01T16:43:58+00:00'\n- id: radl2024stopthepop\n  title: 'StopThePop: Sorted Gaussian Splatting for View-Consistent Real-time Rendering'\n  authors: Lukas Radl, Michael Steiner, Mathias Parger, Alexander Weinrauch, Bernhard\n    Kerbl, Markus Steinberger\n  year: '2024'\n  abstract: Gaussian Splatting has emerged as a prominent model for constructing 3D\n    representations from images across diverse domains. However, the efficiency of\n    the 3D Gaussian Splatting rendering pipeline relies on several simplifications.\n    Notably, reducing Gaussian to 2D splats with a single view-space depth introduces\n    popping and blending artifacts during view rotation. Addressing this issue requires\n    accurate per-pixel depth computation, yet a full per-pixel sort proves excessively\n    costly compared to a global sort operation. In this paper, we present a novel\n    hierarchical rasterization approach that systematically resorts and culls splats\n    with minimal processing overhead. Our software rasterizer effectively eliminates\n    popping artifacts and view inconsistencies, as demonstrated through both quantitative\n    and qualitative measurements. Simultaneously, our method mitigates the potential\n    for cheating view-dependent effects with popping, ensuring a more authentic representation.\n    Despite the elimination of cheating, our approach achieves comparable quantitative\n    results for test images, while increasing the consistency for novel view synthesis\n    in motion. Due to its design, our hierarchical approach is only 4% slower on average\n    than the original Gaussian Splatting. Notably, enforcing consistency enables a\n    reduction in the number of Gaussians by approximately half with nearly identical\n    quality and view-consistency. Consequently, rendering performance is nearly doubled,\n    making our approach 1.6x faster than the original Gaussian Splatting, with a 50%\n    reduction in memory requirements.\n  project_page: https://r4dl.github.io/StopThePop/\n  paper: https://arxiv.org/pdf/2402.00525.pdf\n  code: https://github.com/r4dl/StopThePop\n  video: https://youtu.be/EmcXtHYhigk\n  tags:\n  - Code\n  - Perspective-correct\n  - Project\n  - Rendering\n  - Video\n  thumbnail: assets/thumbnails/radl2024stopthepop.jpg\n  publication_date: '2024-02-01T11:46:44+00:00'\n- id: hu2024segment\n  title: Segment Anything in 3D Gaussians\n  authors: Xu Hu, Yuxi Wang, Lue Fan, Junsong Fan, Junran Peng, Zhen Lei, Qing Li,\n    Zhaoxiang Zhang\n  year: '2024'\n  abstract: 3D Gauss"
        },
        {
          "name": "editor.py",
          "type": "blob",
          "size": 0.0703125,
          "content": "from src.yaml_editor import main\n\nif __name__ == '__main__':\n    main()\n"
        },
        {
          "name": "index.html",
          "type": "blob",
          "size": 1186.2392578125,
          "content": "<!DOCTYPE HTML>\n<html lang=\"en\">\n<head>\n    <meta charset=\"UTF-8\">\n    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1\">\n    <title>Awesome 3D Gaussian Splatting Paper List</title>\n\n    <!-- Preconnects -->\n    <link rel=\"preconnect\" href=\"https://cdnjs.cloudflare.com\">\n    <link rel=\"preconnect\" href=\"https://raw.githubusercontent.com\">\n\n    <!-- External CSS -->\n    <link rel=\"stylesheet\" href=\"https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css\">\n    \n    <!-- Site CSS -->\n    <style>\n        /* Root Variables */\n:root {\n    --primary-color: #1772d0;\n    --hover-color: #f09228;\n    --bg-color: #ffffff;\n    --card-bg: #ffffff;\n    --border-color: #e5e7eb;\n    --text-color: #1f2937;\n}\n\n/* Base Styles */\nbody {\n    font-family: -apple-system, BlinkMacSystemFont, \"Segoe UI\", Roboto, \"Helvetica Neue\", Arial, sans-serif;\n    margin: 0;\n    padding: 20px;\n    background-color: var(--bg-color);\n    color: var(--text-color);\n    line-height: 1.5;\n}\n\n.container {\n    max-width: 1200px;\n    margin: 0 auto;\n    padding: 0 20px;\n    position: relative;\n}\n\nh1 {\n    text-align: center;\n    font-size: 2.5rem;\n    margin-bottom: 2rem;\n    color: var(--text-color);\n}\n\n/* Donation Box */\n.donate-box {\n    background-color: #f8fafc;\n    border: 1px solid var(--border-color);\n    border-radius: 0.5rem;\n    padding: 1.5rem;\n    margin-bottom: 2rem;\n    text-align: center;\n}\n\n.donate-box h3 {\n    margin-top: 0;\n    color: var(--primary-color);\n    font-size: 1.25rem;\n    margin-bottom: 0.5rem;\n}\n\n.donate-box p {\n    margin: 0.5rem 0 1rem;\n    color: #4b5563;\n}\n\n.bitcoin-info {\n    display: flex;\n    align-items: center;\n    justify-content: center;\n    gap: 1rem;\n    flex-wrap: wrap;\n    margin-top: 1rem;\n}\n\n.bitcoin-label {\n    font-weight: 600;\n    color: #4b5563;\n}\n\n.bitcoin-address {\n    background: #fff;\n    padding: 0.5rem 1rem;\n    border-radius: 0.25rem;\n    border: 1px solid var(--border-color);\n    font-family: monospace;\n    font-size: 0.9rem;\n}\n\n.copy-button, .sponsor-button {\n    background-color: var(--primary-color);\n    color: white;\n    border: none;\n    border-radius: 0.25rem;\n    padding: 0.5rem 1rem;\n    cursor: pointer;\n    display: inline-flex;\n    align-items: center;\n    gap: 0.5rem;\n    transition: background-color 0.2s;\n    text-decoration: none;\n    font-size: 0.9rem;\n}\n\n.copy-button:hover, .sponsor-button:hover {\n    background-color: var(--hover-color);\n}\n\n/* Filter Info */\n.filter-info {\n    background-color: #f8fafc;\n    border: 1px solid var(--border-color);\n    border-radius: 0.5rem;\n    padding: 1rem 1.5rem;\n    margin-bottom: 2rem;\n}\n\n.filter-info h3 {\n    margin-top: 0;\n    color: var(--primary-color);\n    font-size: 1.1rem;\n}\n\n.filter-info p {\n    margin: 0.5rem 0;\n    color: #4b5563;\n}\n\n/* Search and Filters */\n.filters {\n    display: flex;\n    flex-wrap: wrap;\n    gap: 1rem;\n    margin-bottom: 2rem;\n    align-items: center;\n    justify-content: space-between;\n}\n\n.search-wrapper {\n    position: relative;\n    flex: 1;\n    min-width: 200px;\n    max-width: calc(100% - 20px);\n}\n\n.search-box {\n    width: 100%;\n    padding: 0.75rem 1rem;\n    padding-right: 2.5rem;\n    border: 1px solid var(--border-color);\n    border-radius: 0.5rem;\n    font-size: 1rem;\n    box-sizing: border-box;\n}\n\n.clear-search-btn {\n    position: absolute;\n    right: 10px;\n    top: 50%;\n    transform: translateY(-50%);\n    background: none;\n    border: none;\n    cursor: pointer;\n    color: #9ca3af;\n    font-size: 1.2rem;\n    padding: 0.25rem;\n    z-index: 1;\n    line-height: 1;\n}\n\n.clear-search-btn:hover {\n    color: #dc2626;\n}\n\n.filter-select {\n    padding: 0.75rem 1rem;\n    border: 1px solid var(--border-color);\n    border-radius: 0.5rem;\n    background-color: white;\n}\n\n/* Tag Filters */\n.tag-filters {\n    display: flex;\n    flex-wrap: wrap;\n    gap: 0.5rem;\n    margin-bottom: 2rem;\n}\n\n.tag-filter {\n    background: #f3f4f6;\n    border: none;\n    padding: 0.5rem 1rem;\n    border-radius: 0.5rem;\n    cursor: pointer;\n    font-size: 0.9rem;\n    transition: all 0.2s;\n    color: var(--text-color);\n}\n\n.tag-filter:hover {\n    background: #e5e7eb;\n}\n\n.tag-filter.include {\n    background: var(--primary-color);\n    color: white;\n}\n\n.tag-filter.exclude {\n    background: #dc2626;\n    color: white;\n}\n\n/* Floating Navigation */\n.floating-nav {\n    position: fixed;\n    bottom: 100px;\n    right: 2rem;\n    display: flex;\n    flex-direction: column;\n    gap: 1rem;\n    z-index: 1000;\n}\n\n.floating-nav button {\n    width: 48px;\n    height: 48px;\n    border-radius: 50%;\n    background: rgba(255, 255, 255, 0.9);\n    border: 1px solid var(--border-color);\n    cursor: pointer;\n    display: flex;\n    align-items: center;\n    justify-content: center;\n    box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);\n    transition: all 0.2s;\n}\n\n.floating-nav button:hover {\n    background: white;\n    transform: translateY(-2px);\n    box-shadow: 0 4px 12px rgba(0, 0, 0, 0.15);\n}\n\n.scroll-progress {\n    width: 48px;\n    height: 48px;\n    border-radius: 50%;\n    background: rgba(255, 255, 255, 0.9);\n    border: 1px solid var(--border-color);\n    display: flex;\n    align-items: center;\n    justify-content: center;\n    font-size: 0.875rem;\n    font-weight: 500;\n    box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);\n}\n\n/* Selection Preview Bar */\n.selection-preview {\n    position: sticky;\n    top: 0;\n    left: 0;\n    right: 0;\n    width: 100%;\n    background: white;\n    border-bottom: 1px solid var(--border-color);\n    box-shadow: 0 2px 4px rgba(0, 0, 0, 0.1);\n    z-index: 1000;\n    display: none;\n    margin-bottom: 2rem;\n}\n\n.selection-mode .selection-preview {\n    display: block;\n}\n\n/* Filter Status Bar */\n.filter-status {\n    position: sticky;\n    top: 0;\n    left: 0;\n    right: 0;\n    width: 100%;\n    background: white;\n    border-bottom: 1px solid var(--border-color);\n    box-shadow: 0 2px 4px rgba(0, 0, 0, 0.1);\n    z-index: 999;\n    margin-bottom: 2rem;\n}\n\n/* Preview Header (shared by both bars) */\n.preview-header {\n    display: flex;\n    align-items: center;\n    justify-content: space-between;\n    padding: 0.75rem 1rem;\n    background: #f8fafc;\n    border-bottom: 1px solid var(--border-color);\n}\n\n.preview-header-left {\n    font-weight: 600;\n    color: var(--text-color);\n    display: flex;\n    align-items: center;\n    gap: 1rem;\n}\n\n.preview-header-right {\n    display: flex;\n    align-items: center;\n    gap: 0.75rem;\n}\n\n.selection-counter,\n.filter-counter {\n    color: #6b7280;\n    font-size: 0.9rem;\n}\n\n.preview-container {\n    padding: 0.75rem 1rem;\n    display: flex;\n    flex-wrap: wrap;\n    gap: 0.5rem;\n}\n\n/* Preview Items (shared style for selection and filter tags) */\n.preview-item,\n.filter-tag {\n    flex: 0 0 auto;\n    display: flex;\n    justify-content: space-between;\n    align-items: center;\n    padding: 0.75rem;\n    border: 1px solid var(--border-color);\n    border-radius: 0.5rem;\n    background: white;\n    max-width: 300px;\n    margin-bottom: 0.5rem;\n}\n\n.preview-content,\n.filter-tag-content {\n    flex: 1;\n    min-width: 0;\n    cursor: pointer;\n}\n\n.preview-title,\n.filter-tag-title {\n    font-weight: 600;\n    margin-bottom: 0.25rem;\n    font-size: 0.9rem;\n    color: var(--text-color);\n    overflow: hidden;\n    text-overflow: ellipsis;\n    white-space: nowrap;\n}\n\n.preview-authors,\n.filter-tag-info {\n    font-size: 0.8rem;\n    color: #4b5563;\n    overflow: hidden;\n    text-overflow: ellipsis;\n    white-space: nowrap;\n}\n\n/* Filter Tag Variations */\n.filter-tag.search {\n    background: #e6f3ff;\n    border-color: #1a73e8;\n}\n\n.filter-tag.year {\n    background: #e6f7e6;\n    border-color: #1e8e1e;\n}\n\n.filter-tag.tag {\n    background: #f3e6ff;\n    border-color: #8e1eb8;\n}\n\n/* Remove Button (shared) */\n.preview-remove {\n    background: none;\n    border: none;\n    cursor: pointer;\n    padding: 2px;\n    opacity: 0.7;\n    transition: opacity 0.2s;\n    display: inline-flex;\n    align-items: center;\n    justify-content: center;\n}\n\n.preview-remove:hover {\n    opacity: 1;\n}\n\n/* Control Buttons */\n.control-button {\n    padding: 0.5rem 1rem;\n    border-radius: 0.5rem;\n    border: none;\n    cursor: pointer;\n    font-size: 0.9rem;\n    display: inline-flex;\n    align-items: center;\n    gap: 0.5rem;\n    transition: all 0.2s;\n}\n\n.control-button.primary {\n    background-color: var(--primary-color);\n    color: white;\n}\n\n.control-button.secondary {\n    background-color: #f3f4f6;\n    color: var(--text-color);\n}\n\n.control-button:hover {\n    opacity: 0.9;\n    transform: translateY(-1px);\n}\n\n/* Selection Mode Adjustments */\n.selection-mode .floating-nav {\n    bottom: 180px;\n}\n\n.selection-mode .filter-status {\n    top: 84px; /* Height of selection preview */\n}\n\n/* Dark Mode Support */\n@media (prefers-color-scheme: dark) {\n    .floating-nav button,\n    .scroll-progress {\n        background: rgba(30, 41, 59, 0.9);\n        border-color: rgba(255, 255, 255, 0.1);\n        color: rgba(255, 255, 255, 0.9);\n    }\n\n    .floating-nav button:hover {\n        background: rgb(30, 41, 59);\n    }\n}\n\n/* Responsive Design */\n@media (max-width: 768px) {\n    .container {\n        padding: 0 1rem;\n    }\n\n    .filters {\n        flex-direction: column;\n        align-items: stretch;\n    }\n\n    .search-wrapper {\n        width: 100%;\n    }\n\n    .preview-header {\n        flex-direction: column;\n        gap: 0.5rem;\n    }\n\n    .preview-container {\n        flex-direction: column;\n    }\n\n    .preview-item,\n    .filter-tag {\n        max-width: none;\n        width: 100%;\n    }\n\n    .floating-nav {\n        bottom: 80px;\n        right: 1rem;\n    }\n\n    .floating-nav button,\n    .scroll-progress {\n        width: 40px;\n        height: 40px;\n    }\n}\n\n/* Print Styles */\n@media print {\n    .floating-nav,\n    .filter-status,\n    .selection-preview {\n        display: none !important;\n    }\n}\n/* Selection Controls */\n.selection-mode-toggle {\n    position: fixed;\n    bottom: 2rem;\n    right: 2rem;\n    background: var(--primary-color);\n    color: white;\n    padding: 1rem;\n    border-radius: 50%;\n    box-shadow: 0 2px 8px rgba(0, 0, 0, 0.2);\n    cursor: pointer;\n    border: none;\n    z-index: 1000;\n    width: 60px;\n    height: 60px;\n    display: flex;\n    align-items: center;\n    justify-content: center;\n    transition: transform 0.2s;\n}\n\n.selection-mode-toggle:hover {\n    transform: scale(1.1);\n}\n\n.selection-mode-toggle .tooltip {\n    position: absolute;\n    bottom: 100%;\n    right: 0;\n    background: rgba(0, 0, 0, 0.8);\n    color: white;\n    padding: 0.5rem 1rem;\n    border-radius: 0.25rem;\n    font-size: 0.875rem;\n    white-space: nowrap;\n    margin-bottom: 0.5rem;\n    opacity: 0;\n    visibility: hidden;\n    transition: opacity 0.2s, visibility 0.2s;\n}\n\n.selection-mode-toggle:hover .tooltip {\n    opacity: 1;\n    visibility: visible;\n}\n\n.selection-mode .selection-mode-toggle .tooltip {\n    content: \"Exit Selection Mode\";\n}\n\n.selection-checkbox {\n    display: none;\n    position: absolute;\n    top: 1rem;\n    right: 1rem;\n    width: 2rem;\n    height: 2rem;\n    z-index: 2;\n    cursor: pointer;\n    opacity: 1;\n    appearance: none;\n    -webkit-appearance: none;\n    background: white;\n    border: 2px solid #e5e7eb;\n    border-radius: 50%;\n    transition: all 0.2s ease;\n}\n\n.selection-checkbox:checked {\n    background: #10b981;\n    border-color: #10b981;\n}\n\n.selection-checkbox:checked::before {\n    content: '✓';\n    position: absolute;\n    top: 50%;\n    left: 50%;\n    transform: translate(-50%, -50%);\n    color: white;\n    font-size: 1.2rem;\n}\n\n.selection-mode .selection-checkbox {\n    display: block !important;\n    pointer-events: auto !important;\n}\n\n/* Paper Cards */\n.papers-grid {\n    display: grid;\n    gap: 2rem;\n}\n\n.paper-card {\n    background: var(--card-bg);\n    border: 1px solid var(--border-color);\n    border-radius: 0.75rem;\n    padding: 2rem;\n    box-shadow: 0 1px 3px rgba(0, 0, 0, 0.1);\n    transition: transform 0.2s ease, box-shadow 0.2s ease;\n    display: flex;\n    gap: 1.5rem;\n    position: relative;\n}\n\n.paper-card:hover {\n    transform: translateY(-2px);\n    box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);\n}\n\n.paper-card.selected {\n    border: 2px solid var(--primary-color);\n    box-shadow: 0 0 0 1px var(--primary-color);\n}\n\n.paper-number {\n    position: absolute;\n    top: -1rem;\n    left: -1rem;\n    background-color: var(--primary-color);\n    color: white;\n    width: 2rem;\n    height: 2rem;\n    border-radius: 50%;\n    display: flex;\n    align-items: center;\n    justify-content: center;\n    font-weight: bold;\n    z-index: 1;\n}\n\n.paper-thumbnail {\n    flex: 0 0 240px;\n    height: 200px;\n    border-radius: 0.5rem;\n    overflow: hidden;\n    border: 1px solid var(--border-color);\n    background-color: #f3f4f6;\n    position: relative;\n    display: flex;\n    align-items: center;\n    justify-content: center;\n}\n\n.paper-thumbnail img {\n    width: 100%;\n    height: 100%;\n    object-fit: cover;\n    transition: transform 0.2s;\n}\n\n.paper-thumbnail img:hover {\n    transform: scale(1.05);\n}\n\n.paper-content {\n    flex: 1;\n    min-width: 0;\n}\n\n.paper-title {\n    font-size: 1.25rem;\n    font-weight: 600;\n    margin: 0 0 1rem 0;\n    color: var(--text-color);\n}\n\n.paper-authors {\n    color: #4b5563;\n    margin-bottom: 1rem;\n}\n\n.paper-tags {\n    display: flex;\n    flex-wrap: wrap;\n    gap: 0.5rem;\n    margin-bottom: 1rem;\n}\n\n.paper-tag {\n    background: #f3f4f6;\n    padding: 0.25rem 0.75rem;\n    border-radius: 0.5rem;\n    font-size: 0.85rem;\n    color: #4b5563;\n}\n\n.paper-links {\n    display: flex;\n    flex-wrap: wrap;\n    gap: 0.5rem;\n    margin-top: 1rem;\n    align-items: center;\n}\n\n.paper-link, .abstract-toggle {\n    color: var(--primary-color);\n    text-decoration: none;\n    display: inline-flex;\n    align-items: center;\n    gap: 0.5rem;\n    padding: 0.5rem 1rem;\n    background: #f3f4f6;\n    border: none;\n    border-radius: 0.5rem;\n    transition: all 0.2s;\n    font-size: 0.9rem;\n    cursor: pointer;\n    white-space: nowrap;\n}\n\n.paper-link:hover, .abstract-toggle:hover {\n    background: #e5e7eb;\n    color: var(--hover-color);\n}\n\n.paper-abstract {\n    margin-top: 1rem;\n    display: none;\n    background: #f9fafb;\n    padding: 1rem;\n    border-radius: 0.5rem;\n    color: #4b5563;\n    line-height: 1.6;\n    border: 1px solid var(--border-color);\n}\n\n.paper-abstract.show {\n    display: block;\n}\n\n.paper-row {\n    display: none;\n}\n\n.paper-row.visible {\n    display: block;\n}\n\n/* Selection Preview Bar */\n.selection-preview {\n    position: sticky;\n    top: 0;\n    left: 0;\n    right: 0;\n    width: 100%;\n    background: white;\n    border-bottom: 1px solid var(--border-color);\n    border-radius: 0;\n    box-shadow: 0 2px 4px rgba(0, 0, 0, 0.1);\n    z-index: 1000;\n    display: none;\n    flex-direction: column;\n    margin-bottom: 2rem;\n}\n\n.selection-mode .selection-preview {\n    display: flex;\n}\n\n.preview-header {\n    display: flex;\n    align-items: center;\n    justify-content: space-between;\n    padding: 0.75rem 1rem;\n    border-bottom: 1px solid var(--border-color);\n    background: #f8fafc;\n}\n\n.preview-header-left {\n    font-weight: 600;\n    color: var(--text-color);\n    display: flex;\n    align-items: center;\n    gap: 1rem;\n}\n\n.preview-header-right {\n    display: flex;\n    align-items: center;\n    gap: 0.75rem;\n}\n\n.selection-counter {\n    color: #6b7280;\n    font-size: 0.9rem;\n}\n\n.preview-container {\n    padding: 1rem;\n    overflow-x: auto;\n    display: flex;\n    gap: 1rem;\n    flex-wrap: nowrap;\n    max-height: 200px;\n    scrollbar-width: thin;\n}\n\n.preview-container::-webkit-scrollbar {\n    height: 8px;\n}\n\n.preview-container::-webkit-scrollbar-track {\n    background: #f1f1f1;\n    border-radius: 4px;\n}\n\n.preview-container::-webkit-scrollbar-thumb {\n    background: #888;\n    border-radius: 4px;\n}\n\n.preview-item {\n    flex: 0 0 300px;\n    display: flex;\n    justify-content: space-between;\n    align-items: flex-start;\n    padding: 0.75rem;\n    border: 1px solid var(--border-color);\n    border-radius: 0.5rem;\n    background: white;\n    max-width: 300px;\n    margin-bottom: 0.5rem;\n}\n\n.preview-content {\n    flex: 1;\n    min-width: 0;\n    cursor: pointer;\n}\n\n.preview-title {\n    font-weight: 600;\n    margin-bottom: 0.25rem;\n    font-size: 0.9rem;\n    color: var(--text-color);\n    overflow: hidden;\n    text-overflow: ellipsis;\n    white-space: nowrap;\n}\n\n.preview-authors {\n    font-size: 0.8rem;\n    color: #4b5563;\n    overflow: hidden;\n    text-overflow: ellipsis;\n    white-space: nowrap;\n}\n\n.preview-remove {\n    background: none;\n    border: none;\n    color: #6b7280;\n    cursor: pointer;\n    width: 24px;\n    height: 24px;\n    border-radius: 50%;\n    display: flex;\n    align-items: center;\n    justify-content: center;\n    transition: all 0.2s ease;\n    margin-left: 8px;\n    flex-shrink: 0;\n}\n\n.preview-remove:hover {\n    background-color: #fee2e2;\n    color: #dc2626;\n}\n\n/* Share Modal */\n.share-modal {\n    display: none;\n    position: fixed;\n    top: 0;\n    left: 0;\n    width: 100%;\n    height: 100%;\n    background-color: rgba(0, 0, 0, 0.5);\n    z-index: 1000;\n    justify-content: center;\n    align-items: center;\n}\n\n.share-modal.show {\n    display: flex;\n}\n\n.share-modal-content {\n    background-color: white;\n    padding: 2rem;\n    border-radius: 0.75rem;\n    max-width: 600px;\n    width: 90%;\n    position: relative;\n}\n\n.share-modal-header {\n    margin-bottom: 1.5rem;\n}\n\n.share-modal-header h2 {\n    margin: 0;\n    font-size: 1.5rem;\n    color: var(--text-color);\n}\n\n.share-url-container {\n    display: flex;\n    gap: 1rem;\n    margin-bottom: 1.5rem;\n}\n\n.share-url-input {\n    flex: 1;\n    padding: 0.75rem 1rem;\n    border: 1px solid var(--border-color);\n    border-radius: 0.5rem;\n    font-size: 0.9rem;\n}\n\n.share-modal-close {\n    position: absolute;\n    top: 1rem;\n    right: 1rem;\n    background: none;\n    border: none;\n    font-size: 1.5rem;\n    cursor: pointer;\n    color: #6b7280;\n}\n\n/* Control Buttons */\n.control-button {\n    padding: 0.5rem 1rem;\n    border-radius: 0.5rem;\n    border: none;\n    cursor: pointer;\n    font-size: 0.9rem;\n    display: inline-flex;\n    align-items: center;\n    gap: 0.5rem;\n    transition: all 0.2s;\n}\n\n.control-button.primary {\n    background-color: var(--primary-color);\n    color: white;\n}\n\n.control-button.secondary {\n    background-color: #f3f4f6;\n    color: var(--text-color);\n}\n\n.control-button:hover {\n    opacity: 0.9;\n    transform: translateY(-1px);\n}\n\n/* Mobile Responsiveness */\n@media (max-width: 768px) {\n    .preview-item {\n        flex: 0 0 250px;\n    }\n    \n    .preview-container {\n        max-height: 150px;\n    }\n    \n    .paper-card {\n        flex-direction: column;\n    }\n\n    .paper-thumbnail {\n        width: 100%;\n        max-height: 200px;\n        flex: none;\n        aspect-ratio: 1.2/1;\n    }\n\n    .selection-controls {\n        flex-direction: column;\n        align-items: stretch;\n    }\n\n    .paper-links {\n        flex-direction: column;\n    }\n}\n@media (max-width: 1024px) {\n    .container {\n        padding: 0 1rem;\n    }\n\n    .selection-preview {\n        display: none !important;\n    }\n}\n\n@media (max-width: 768px) {\n    .filters {\n        flex-direction: column;\n        align-items: stretch;\n    }\n\n    .search-wrapper {\n        width: 100%;\n    }\n\n    .paper-card {\n        flex-direction: column;\n    }\n\n    .paper-thumbnail {\n        width: 100%;\n        height: 200px;\n        flex: none;\n        aspect-ratio: 1.2/1;\n    }\n\n    .bitcoin-info {\n        flex-direction: column;\n    }\n\n    .selection-controls {\n        flex-direction: column;\n        align-items: stretch;\n    }\n\n    .share-modal-content {\n        padding: 1rem;\n        width: 95%;\n    }\n\n    .share-url-container {\n        flex-direction: column;\n    }\n\n    .paper-links {\n        flex-direction: column;\n    }\n}\n\n@media (max-width: 480px) {\n    body {\n        padding: 10px;\n    }\n\n    h1 {\n        font-size: 1.8rem;\n    }\n\n    .paper-card {\n        padding: 1rem;\n    }\n\n    .tag-filters {\n        gap: 0.25rem;\n    }\n\n    .tag-filter {\n        font-size: 0.8rem;\n        padding: 0.25rem 0.5rem;\n    }\n}\n    </style>\n\n    <!-- Lazy load -->\n    <script src=\"https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/17.8.3/lazyload.min.js\"></script>\n</head>\n<body>\n    <div class=\"container\">\n        <!-- Floating Navigation -->\n        <div class=\"floating-nav\">\n            <button onclick=\"scrollToTop()\" aria-label=\"Scroll to top\">\n                <i class=\"fas fa-arrow-up\"></i>\n            </button>\n            <button onclick=\"scrollToBottom()\" aria-label=\"Scroll to bottom\">\n                <i class=\"fas fa-arrow-down\"></i>\n            </button>\n            <div class=\"scroll-progress\">0%</div>\n        </div>\n\n        <!-- Selection Mode Toggle Button -->\n        <button class=\"selection-mode-toggle\" onclick=\"toggleSelectionMode()\">\n            <i class=\"fas fa-list-check\"></i>\n            <span class=\"tooltip\">Enter Selection Mode</span>\n        </button>\n\n        <!-- Selection Preview Bar -->\n        <div class=\"selection-preview\">\n            <div class=\"preview-header\">\n                <div class=\"preview-header-left\">\n                    <span>Selected Papers</span>\n                    <span class=\"selection-counter\">0 papers selected</span>\n                </div>\n                <div class=\"preview-header-right\">\n                    <button class=\"control-button secondary\" onclick=\"clearSelection()\">\n                        <i class=\"fas fa-trash\"></i> Clear\n                    </button>\n                    <button class=\"control-button primary\" onclick=\"showShareModal()\">\n                        <i class=\"fas fa-share\"></i> Share\n                    </button>\n                </div>\n            </div>\n            <div class=\"preview-container\" id=\"selectionPreview\"></div>\n        </div>\n\n        <!-- Filter Status Bar -->\n        <div class=\"filter-status\">\n            <div class=\"preview-header\">\n                <div class=\"preview-header-left\">\n                    <span>Active Filters</span>\n                    <span class=\"filter-counter\">Showing <span id=\"visibleCount\">0</span> of <span id=\"totalCount\">0</span> papers</span>\n                </div>\n                <div class=\"preview-header-right\">\n                    <button class=\"control-button secondary\" onclick=\"clearAllFilters()\">\n                        <i class=\"fas fa-trash\"></i> Clear All\n                    </button>\n                </div>\n            </div>\n            <div class=\"preview-container\" id=\"activeFilters\">\n                <!-- Active filters will be inserted here -->\n            </div>\n        </div>\n\n        <h1>MrNeRF's Awesome-3D-Gaussian-Splatting-Paper-List</h1>\n\n        <!-- Donation Box -->\n        <div class=\"donate-box\">\n            <h3>Support This Project</h3>\n            <p>If you find this resource helpful, consider supporting its development and maintenance.</p>\n            <div class=\"bitcoin-info\">\n                <span class=\"bitcoin-label\">Bitcoin:</span>\n                <code class=\"bitcoin-address\">bc1qz7z4c2cn46t7rkgsh7mr8tw9ssgctepzxrtqfw</code>\n                <button class=\"copy-button\" onclick=\"copyBitcoinAddress()\">\n                    <i class=\"fas fa-copy\"></i> Copy\n                </button>\n                <a href=\"https://github.com/sponsors/MrNeRF\" class=\"sponsor-button\" target=\"_blank\" rel=\"noopener\">\n                    <i class=\"fas fa-heart\"></i> Sponsor\n                </a>\n            </div>\n        </div>\n        <div class=\"preview-header-right\">\n                <button class=\"control-button secondary show-selected\" onclick=\"toggleSelectedOnly()\">\n                    <i class=\"fas fa-filter\"></i> Show Selected Only\n                </button>\n                <button class=\"control-button secondary\" onclick=\"clearSelection()\">\n                    <i class=\"fas fa-trash\"></i> Clear\n                </button>\n                <button class=\"control-button primary\" onclick=\"showShareModal()\">\n                    <i class=\"fas fa-share\"></i> Share\n                </button>\n            </div>\n\n        <!-- Filter Info -->\n        <div class=\"filter-info\">\n            <h3>Filter Options</h3>\n            <p><strong>Search:</strong> Enter paper title or author names, then click <i class=\"fas fa-times\"></i> to clear.</p>\n            <p><strong>Year:</strong> Filter by publication year</p>\n            <p><strong>Tags:</strong> Click once to include (blue), twice to exclude (red), third time to remove filter</p>\n            <p><strong>Selection:</strong> Use selection mode to pick and share specific papers</p>\n        </div>\n\n        <!-- Share Modal -->\n        <div class=\"share-modal\" id=\"shareModal\">\n            <div class=\"share-modal-content\">\n                <button class=\"share-modal-close\" onclick=\"hideShareModal()\">&times;</button>\n                <div class=\"share-modal-header\">\n                    <h2>Share Selected Papers</h2>\n                </div>\n                <div class=\"share-url-container\">\n                    <input type=\"text\" class=\"share-url-input\" id=\"shareUrl\" readonly>\n                    <button class=\"control-button primary\" onclick=\"copyShareLink()\">\n                        <i class=\"fas fa-copy\"></i> Copy Link\n                    </button>\n                </div>\n            </div>\n        </div>\n\n        <!-- Filters Bar -->\n        <div class=\"filters\">\n            <div class=\"search-wrapper\">\n                <input type=\"text\" id=\"searchInput\" class=\"search-box\" placeholder=\"Search papers by title or authors...\">\n                <button class=\"clear-search-btn\" onclick=\"clearSearch()\" title=\"Clear search\">\n                    <i class=\"fas fa-times\"></i>\n                </button>\n            </div>\n            <select id=\"yearFilter\" class=\"filter-select\">\n                <option value=\"all\">All Years</option>\n                <option value=\"2025\">2025</option>\n<option value=\"2024\">2024</option>\n<option value=\"2023\">2023</option>\n            </select>\n        </div>\n\n        <!-- Tag Filters -->\n        <div class=\"tag-filters\" id=\"tagFilters\">\n            <div class=\"tag-filter\" data-tag=\"2DGS\">2DGS</div>\n<div class=\"tag-filter\" data-tag=\"360 degree\">360 degree</div>\n<div class=\"tag-filter\" data-tag=\"3ster-based\">3ster-based</div>\n<div class=\"tag-filter\" data-tag=\"Acceleration\">Acceleration</div>\n<div class=\"tag-filter\" data-tag=\"Antialiasing\">Antialiasing</div>\n<div class=\"tag-filter\" data-tag=\"Autonomous Driving\">Autonomous Driving</div>\n<div class=\"tag-filter\" data-tag=\"Avatar\">Avatar</div>\n<div class=\"tag-filter\" data-tag=\"Classic Work\">Classic Work</div>\n<div class=\"tag-filter\" data-tag=\"Code\">Code</div>\n<div class=\"tag-filter\" data-tag=\"Compression\">Compression</div>\n<div class=\"tag-filter\" data-tag=\"Deblurring\">Deblurring</div>\n<div class=\"tag-filter\" data-tag=\"Densification\">Densification</div>\n<div class=\"tag-filter\" data-tag=\"Diffusion\">Diffusion</div>\n<div class=\"tag-filter\" data-tag=\"Distributed\">Distributed</div>\n<div class=\"tag-filter\" data-tag=\"Dynamic\">Dynamic</div>\n<div class=\"tag-filter\" data-tag=\"Editing\">Editing</div>\n<div class=\"tag-filter\" data-tag=\"Feed-Forward\">Feed-Forward</div>\n<div class=\"tag-filter\" data-tag=\"GAN\">GAN</div>\n<div class=\"tag-filter\" data-tag=\"In the Wild\">In the Wild</div>\n<div class=\"tag-filter\" data-tag=\"Inpainting\">Inpainting</div>\n<div class=\"tag-filter\" data-tag=\"Language Embedding\">Language Embedding</div>\n<div class=\"tag-filter\" data-tag=\"Large-Scale\">Large-Scale</div>\n<div class=\"tag-filter\" data-tag=\"Lidar\">Lidar</div>\n<div class=\"tag-filter\" data-tag=\"Medicine\">Medicine</div>\n<div class=\"tag-filter\" data-tag=\"Meshing\">Meshing</div>\n<div class=\"tag-filter\" data-tag=\"Misc\">Misc</div>\n<div class=\"tag-filter\" data-tag=\"Monocular\">Monocular</div>\n<div class=\"tag-filter\" data-tag=\"Object Detection\">Object Detection</div>\n<div class=\"tag-filter\" data-tag=\"Optimization\">Optimization</div>\n<div class=\"tag-filter\" data-tag=\"Perspective-correct\">Perspective-correct</div>\n<div class=\"tag-filter\" data-tag=\"Physics\">Physics</div>\n<div class=\"tag-filter\" data-tag=\"Point Cloud\">Point Cloud</div>\n<div class=\"tag-filter\" data-tag=\"Poses\">Poses</div>\n<div class=\"tag-filter\" data-tag=\"Project\">Project</div>\n<div class=\"tag-filter\" data-tag=\"Ray Tracing\">Ray Tracing</div>\n<div class=\"tag-filter\" data-tag=\"Relight\">Relight</div>\n<div class=\"tag-filter\" data-tag=\"Rendering\">Rendering</div>\n<div class=\"tag-filter\" data-tag=\"Review\">Review</div>\n<div class=\"tag-filter\" data-tag=\"Robotics\">Robotics</div>\n<div class=\"tag-filter\" data-tag=\"SLAM\">SLAM</div>\n<div class=\"tag-filter\" data-tag=\"Segmentation\">Segmentation</div>\n<div class=\"tag-filter\" data-tag=\"Sparse\">Sparse</div>\n<div class=\"tag-filter\" data-tag=\"Stereo\">Stereo</div>\n<div class=\"tag-filter\" data-tag=\"Style Transfer\">Style Transfer</div>\n<div class=\"tag-filter\" data-tag=\"Texturing\">Texturing</div>\n<div class=\"tag-filter\" data-tag=\"Transformer\">Transformer</div>\n<div class=\"tag-filter\" data-tag=\"Uncertainty\">Uncertainty</div>\n<div class=\"tag-filter\" data-tag=\"Video\">Video</div>\n<div class=\"tag-filter\" data-tag=\"Virtual Reality\">Virtual Reality</div>\n<div class=\"tag-filter\" data-tag=\"World Generation\">World Generation</div>\n        </div>\n\n        <!-- Paper Cards -->\n        <div class=\"papers-grid\">\n            <div class=\"paper-row\" data-id=\"meng2025zero1tog\" data-title=\"Zero-1-to-G: Taming Pretrained 2D Diffusion Model for Direct 3D Generation\" data-authors=\"Xuyi Meng, Chen Wang, Jiahui Lei, Kostas Daniilidis, Jiatao Gu, Lingjie Liu\" data-year=\"2025\" data-tags='[\"Diffusion\", \"Project\"]'>\n  <div class=\"paper-card\">\n    <input type=\"checkbox\" class=\"selection-checkbox\" onclick=\"handleCheckboxClick(event, 'meng2025zero1tog', this)\">\n    <div class=\"paper-number\"></div>\n    <div class=\"paper-thumbnail\">\n      <img data-src=\"assets/thumbnails/meng2025zero1tog.jpg\" data-fallback=\"None\" alt=\"Paper thumbnail for Zero-1-to-G: Taming Pretrained 2D Diffusion Model for Direct 3D Generation\" class=\"lazy\" loading=\"lazy\"/>\n    </div>\n    <div class=\"paper-content\">\n      <h2 class=\"paper-title\">Zero-1-to-G: Taming Pretrained 2D Diffusion Model for Direct 3D Generation <span class=\"paper-year\">(2025)</span></h2>\n      <p class=\"paper-authors\">Xuyi Meng, Chen Wang, Jiahui Lei, Kostas Daniilidis, Jiatao Gu, Lingjie Liu</p>\n      <div class=\"paper-tags\"><span class=\"paper-tag\">Diffusion</span>\n<span class=\"paper-tag\">Project</span></div>\n      <div class=\"paper-links\"><a href=\"https://arxiv.org/pdf/2501.05427.pdf\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">📄 Paper</a>\n<a href=\"https://mengxuyigit.github.io/projects/zero-1-to-G/\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">🌐 Project</a>\n<button class=\"abstract-toggle\" onclick=\"toggleAbstract(this)\">📖 Show Abstract</button>\n<div class=\"paper-abstract\">Recent advances in 2D image generation have achieved remarkable quality,largely driven by the capacity of diffusion models and the availability of large-scale datasets. However, direct 3D generation is still constrained by the scarcity and lower fidelity of 3D datasets. In this paper, we introduce Zero-1-to-G, a novel approach that addresses this problem by enabling direct single-view generation on Gaussian splats using pretrained 2D diffusion models. Our key insight is that Gaussian splats, a 3D representation, can be decomposed into multi-view images encoding different attributes. This reframes the challenging task of direct 3D generation within a 2D diffusion framework, allowing us to leverage the rich priors of pretrained 2D diffusion models. To incorporate 3D awareness, we introduce cross-view and cross-attribute attention layers, which capture complex correlations and enforce 3D consistency across generated splats. This makes Zero-1-to-G the first direct image-to-3D generative model to effectively utilize pretrained 2D diffusion priors, enabling efficient training and improved generalization to unseen objects. Extensive experiments on both synthetic and in-the-wild datasets demonstrate superior performance in 3D object generation, offering a new approach to high-quality 3D generation.\n</div></div>\n    </div>\n  </div>\n</div>\n<div class=\"paper-row\" data-id=\"gerogiannis2025arc2avatar\" data-title=\"Arc2Avatar: Generating Expressive 3D Avatars from a Single Image via ID Guidance\" data-authors=\"Dimitrios Gerogiannis, Foivos Paraperas Papantoniou, Rolandos Alexandros Potamias, Alexandros Lattas, Stefanos Zafeiriou\" data-year=\"2025\" data-tags='[\"Avatar\", \"Diffusion\"]'>\n  <div class=\"paper-card\">\n    <input type=\"checkbox\" class=\"selection-checkbox\" onclick=\"handleCheckboxClick(event, 'gerogiannis2025arc2avatar', this)\">\n    <div class=\"paper-number\"></div>\n    <div class=\"paper-thumbnail\">\n      <img data-src=\"assets/thumbnails/gerogiannis2025arc2avatar.jpg\" data-fallback=\"None\" alt=\"Paper thumbnail for Arc2Avatar: Generating Expressive 3D Avatars from a Single Image via ID Guidance\" class=\"lazy\" loading=\"lazy\"/>\n    </div>\n    <div class=\"paper-content\">\n      <h2 class=\"paper-title\">Arc2Avatar: Generating Expressive 3D Avatars from a Single Image via ID Guidance <span class=\"paper-year\">(2025)</span></h2>\n      <p class=\"paper-authors\">Dimitrios Gerogiannis, Foivos Paraperas Papantoniou, Rolandos Alexandros Potamias, Alexandros Lattas, Stefanos Zafeiriou</p>\n      <div class=\"paper-tags\"><span class=\"paper-tag\">Avatar</span>\n<span class=\"paper-tag\">Diffusion</span></div>\n      <div class=\"paper-links\"><a href=\"https://arxiv.org/pdf/2501.05379.pdf\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">📄 Paper</a>\n<button class=\"abstract-toggle\" onclick=\"toggleAbstract(this)\">📖 Show Abstract</button>\n<div class=\"paper-abstract\">Inspired by the effectiveness of 3D Gaussian Splatting (3DGS) in reconstructing detailed 3D scenes within multi-view setups and the emergence of large 2D human foundation models, we introduce Arc2Avatar, the first SDS-based method utilizing a human face foundation model as guidance with just a single image as input. To achieve that, we extend such a model for diverse-view human head generation by fine-tuning on synthetic data and modifying its conditioning. Our avatars maintain a dense correspondence with a human face mesh template, allowing blendshape-based expression generation. This is achieved through a modified 3DGS approach, connectivity regularizers, and a strategic initialization tailored for our task. Additionally, we propose an optional efficient SDS-based correction step to refine the blendshape expressions, enhancing realism and diversity. Experiments demonstrate that Arc2Avatar achieves state-of-the-art realism and identity preservation, effectively addressing color issues by allowing the use of very low guidance, enabled by our strong identity prior and initialization strategy, without compromising detail.\n</div></div>\n    </div>\n  </div>\n</div>\n<div class=\"paper-row\" data-id=\"tianci2025scaffoldslam\" data-title=\"Scaffold-SLAM: Structured 3D Gaussians for Simultaneous Localization and Photorealistic Mapping\" data-authors=\"Wen Tianci, Liu Zhiang, Lu Biao, Fang Yongchun\" data-year=\"2025\" data-tags='[\"SLAM\"]'>\n  <div class=\"paper-card\">\n    <input type=\"checkbox\" class=\"selection-checkbox\" onclick=\"handleCheckboxClick(event, 'tianci2025scaffoldslam', this)\">\n    <div class=\"paper-number\"></div>\n    <div class=\"paper-thumbnail\">\n      <img data-src=\"assets/thumbnails/tianci2025scaffoldslam.jpg\" data-fallback=\"None\" alt=\"Paper thumbnail for Scaffold-SLAM: Structured 3D Gaussians for Simultaneous Localization and Photorealistic Mapping\" class=\"lazy\" loading=\"lazy\"/>\n    </div>\n    <div class=\"paper-content\">\n      <h2 class=\"paper-title\">Scaffold-SLAM: Structured 3D Gaussians for Simultaneous Localization and Photorealistic Mapping <span class=\"paper-year\">(2025)</span></h2>\n      <p class=\"paper-authors\">Wen Tianci, Liu Zhiang, Lu Biao, Fang Yongchun</p>\n      <div class=\"paper-tags\"><span class=\"paper-tag\">SLAM</span></div>\n      <div class=\"paper-links\"><a href=\"https://arxiv.org/pdf/2501.05242.pdf\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">📄 Paper</a>\n<button class=\"abstract-toggle\" onclick=\"toggleAbstract(this)\">📖 Show Abstract</button>\n<div class=\"paper-abstract\">3D Gaussian Splatting (3DGS) has recently revolutionized novel view synthesis in the Simultaneous Localization and Mapping (SLAM). However, existing SLAM methods utilizing 3DGS have failed to provide high-quality novel view rendering for monocular, stereo, and RGB-D cameras simultaneously. Notably, some methods perform well for RGB-D cameras but suffer significant degradation in rendering quality for monocular cameras. In this paper, we present Scaffold-SLAM, which delivers simultaneous localization and high-quality photorealistic mapping across monocular, stereo, and RGB-D cameras. We introduce two key innovations to achieve this state-of-the-art visual quality. First, we propose Appearance-from-Motion embedding, enabling 3D Gaussians to better model image appearance variations across different camera poses. Second, we introduce a frequency regularization pyramid to guide the distribution of Gaussians, allowing the model to effectively capture finer details in the scene. Extensive experiments on monocular, stereo, and RGB-D datasets demonstrate that Scaffold-SLAM significantly outperforms state-of-the-art methods in photorealistic mapping quality, e.g., PSNR is 16.76% higher in the TUM RGB-D datasets for monocular cameras.\n</div></div>\n    </div>\n  </div>\n</div>\n<div class=\"paper-row\" data-id=\"bond2025gaussianvideo\" data-title=\"GaussianVideo: Efficient Video Representation via Hierarchical Gaussian Splatting\" data-authors=\"Andrew Bond, Jui-Hsien Wang, Long Mai, Erkut Erdem, Aykut Erdem\" data-year=\"2025\" data-tags='[\"Project\", \"Video\"]'>\n  <div class=\"paper-card\">\n    <input type=\"checkbox\" class=\"selection-checkbox\" onclick=\"handleCheckboxClick(event, 'bond2025gaussianvideo', this)\">\n    <div class=\"paper-number\"></div>\n    <div class=\"paper-thumbnail\">\n      <img data-src=\"assets/thumbnails/bond2025gaussianvideo.jpg\" data-fallback=\"None\" alt=\"Paper thumbnail for GaussianVideo: Efficient Video Representation via Hierarchical Gaussian Splatting\" class=\"lazy\" loading=\"lazy\"/>\n    </div>\n    <div class=\"paper-content\">\n      <h2 class=\"paper-title\">GaussianVideo: Efficient Video Representation via Hierarchical Gaussian Splatting <span class=\"paper-year\">(2025)</span></h2>\n      <p class=\"paper-authors\">Andrew Bond, Jui-Hsien Wang, Long Mai, Erkut Erdem, Aykut Erdem</p>\n      <div class=\"paper-tags\"><span class=\"paper-tag\">Project</span>\n<span class=\"paper-tag\">Video</span></div>\n      <div class=\"paper-links\"><a href=\"https://arxiv.org/pdf/2501.04782.pdf\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">📄 Paper</a>\n<a href=\"https://cyberiada.github.io/GaussianVideo/\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">🌐 Project</a>\n<button class=\"abstract-toggle\" onclick=\"toggleAbstract(this)\">📖 Show Abstract</button>\n<div class=\"paper-abstract\">Efficient neural representations for dynamic video scenes are critical for applications ranging from video compression to interactive simulations. Yet, existing methods often face challenges related to high memory usage, lengthy training times, and temporal consistency. To address these issues, we introduce a novel neural video representation that combines 3D Gaussian splatting with continuous camera motion modeling. By leveraging Neural ODEs, our approach learns smooth camera trajectories while maintaining an explicit 3D scene representation through Gaussians. Additionally, we introduce a spatiotemporal hierarchical learning strategy, progressively refining spatial and temporal features to enhance reconstruction quality and accelerate convergence. This memory-efficient approach achieves high-quality rendering at impressive speeds. Experimental results show that our hierarchical learning, combined with robust camera motion modeling, captures complex dynamic scenes with strong temporal consistency, achieving state-of-the-art performance across diverse video datasets in both high- and low-motion scenarios.\n</div></div>\n    </div>\n  </div>\n</div>\n<div class=\"paper-row\" data-id=\"kwak2025modecgs\" data-title=\"MoDec-GS: Global-to-Local Motion Decomposition and Temporal Interval Adjustment for Compact Dynamic 3D Gaussian Splatting\" data-authors=\"Sangwoon Kwak, Joonsoo Kim, Jun Young Jeong, Won-Sik Cheong, Jihyong Oh, Munchurl Kim\" data-year=\"2025\" data-tags='[\"Compression\", \"Dynamic\", \"Project\", \"Video\"]'>\n  <div class=\"paper-card\">\n    <input type=\"checkbox\" class=\"selection-checkbox\" onclick=\"handleCheckboxClick(event, 'kwak2025modecgs', this)\">\n    <div class=\"paper-number\"></div>\n    <div class=\"paper-thumbnail\">\n      <img data-src=\"assets/thumbnails/kwak2025modecgs.jpg\" data-fallback=\"None\" alt=\"Paper thumbnail for MoDec-GS: Global-to-Local Motion Decomposition and Temporal Interval Adjustment for Compact Dynamic 3D Gaussian Splatting\" class=\"lazy\" loading=\"lazy\"/>\n    </div>\n    <div class=\"paper-content\">\n      <h2 class=\"paper-title\">MoDec-GS: Global-to-Local Motion Decomposition and Temporal Interval Adjustment for Compact Dynamic 3D Gaussian Splatting <span class=\"paper-year\">(2025)</span></h2>\n      <p class=\"paper-authors\">Sangwoon Kwak, Joonsoo Kim, Jun Young Jeong, Won-Sik Cheong, Jihyong Oh, Munchurl Kim</p>\n      <div class=\"paper-tags\"><span class=\"paper-tag\">Compression</span>\n<span class=\"paper-tag\">Dynamic</span>\n<span class=\"paper-tag\">Project</span>\n<span class=\"paper-tag\">Video</span></div>\n      <div class=\"paper-links\"><a href=\"https://arxiv.org/pdf/2501.03714.pdf\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">📄 Paper</a>\n<a href=\"MoDec-GS: Global-to-Local Motion Decomposition and Temporal Interval Adjustment for Compact Dynamic 3D Gaussian Splatting\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">🌐 Project</a>\n<a href=\"https://youtu.be/5L6gzc5-cw8?si=L6v6XLZFQrYK50iV\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">🎥 Video</a>\n<button class=\"abstract-toggle\" onclick=\"toggleAbstract(this)\">📖 Show Abstract</button>\n<div class=\"paper-abstract\">3D Gaussian Splatting (3DGS) has made significant strides in scene representation and neural rendering, with intense efforts focused on adapting it for dynamic scenes. Despite delivering remarkable rendering quality and speed, existing methods struggle with storage demands and representing complex real-world motions. To tackle these issues, we propose MoDecGS, a memory-efficient Gaussian splatting framework designed for reconstructing novel views in challenging scenarios with complex motions. We introduce GlobaltoLocal Motion Decomposition (GLMD) to effectively capture dynamic motions in a coarsetofine manner. This approach leverages Global Canonical Scaffolds (Global CS) and Local Canonical Scaffolds (Local CS), extending static Scaffold representation to dynamic video reconstruction. For Global CS, we propose Global Anchor Deformation (GAD) to efficiently represent global dynamics along complex motions, by directly deforming the implicit Scaffold attributes which are anchor position, offset, and local context features. Next, we finely adjust local motions via the Local Gaussian Deformation (LGD) of Local CS explicitly. Additionally, we introduce Temporal Interval Adjustment (TIA) to automatically control the temporal coverage of each Local CS during training, allowing MoDecGS to find optimal interval assignments based on the specified number of temporal segments. Extensive evaluations demonstrate that MoDecGS achieves an average 70% reduction in model size over stateoftheart methods for dynamic 3D Gaussians from realworld dynamic videos while maintaining or even improving rendering quality.\n</div></div>\n    </div>\n  </div>\n</div>\n<div class=\"paper-row\" data-id=\"yu2025dehazegs\" data-title=\"DehazeGS: Seeing Through Fog with 3D Gaussian Splatting\" data-authors=\"Jinze Yu, Yiqun Wang, Zhengda Lu, Jianwei Guo, Yong Li, Hongxing Qin, Xiaopeng Zhang\" data-year=\"2025\" data-tags='[\"In the Wild\", \"Rendering\"]'>\n  <div class=\"paper-card\">\n    <input type=\"checkbox\" class=\"selection-checkbox\" onclick=\"handleCheckboxClick(event, 'yu2025dehazegs', this)\">\n    <div class=\"paper-number\"></div>\n    <div class=\"paper-thumbnail\">\n      <img data-src=\"assets/thumbnails/yu2025dehazegs.jpg\" data-fallback=\"None\" alt=\"Paper thumbnail for DehazeGS: Seeing Through Fog with 3D Gaussian Splatting\" class=\"lazy\" loading=\"lazy\"/>\n    </div>\n    <div class=\"paper-content\">\n      <h2 class=\"paper-title\">DehazeGS: Seeing Through Fog with 3D Gaussian Splatting <span class=\"paper-year\">(2025)</span></h2>\n      <p class=\"paper-authors\">Jinze Yu, Yiqun Wang, Zhengda Lu, Jianwei Guo, Yong Li, Hongxing Qin, Xiaopeng Zhang</p>\n      <div class=\"paper-tags\"><span class=\"paper-tag\">In the Wild</span>\n<span class=\"paper-tag\">Rendering</span></div>\n      <div class=\"paper-links\"><a href=\"https://arxiv.org/pdf/2501.03659.pdf\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">📄 Paper</a>\n<button class=\"abstract-toggle\" onclick=\"toggleAbstract(this)\">📖 Show Abstract</button>\n<div class=\"paper-abstract\">Current novel view synthesis tasks primarily rely on high-quality and clear images. However, in foggy scenes, scattering and attenuation can significantly degrade the reconstruction and rendering quality. Although NeRF-based dehazing reconstruction algorithms have been developed, their use of deep fully connected neural networks and per-ray sampling strategies leads to high computational costs. Moreover, NeRF's implicit representation struggles to recover fine details from hazy scenes. In contrast, recent advancements in 3D Gaussian Splatting achieve high-quality 3D scene reconstruction by explicitly modeling point clouds into 3D Gaussians. In this paper, we propose leveraging the explicit Gaussian representation to explain the foggy image formation process through a physically accurate forward rendering process. We introduce DehazeGS, a method capable of decomposing and rendering a fog-free background from participating media using only muti-view foggy images as input. We model the transmission within each Gaussian distribution to simulate the formation of fog. During this process, we jointly learn the atmospheric light and scattering coefficient while optimizing the Gaussian representation of the hazy scene. In the inference stage, we eliminate the effects of scattering and attenuation on the Gaussians and directly project them onto a 2D plane to obtain a clear view. Experiments on both synthetic and real-world foggy datasets demonstrate that DehazeGS achieves state-of-the-art performance in terms of both rendering quality and computational efficiency.\n</div></div>\n    </div>\n  </div>\n</div>\n<div class=\"paper-row\" data-id=\"lee2025compression\" data-title=\"Compression of 3D Gaussian Splatting with Optimized Feature Planes and Standard Video Codecs\" data-authors=\"Soonbin Lee, Fangwen Shu, Yago Sanchez, Thomas Schierl, Cornelius Hellge\" data-year=\"2025\" data-tags='[\"Compression\"]'>\n  <div class=\"paper-card\">\n    <input type=\"checkbox\" class=\"selection-checkbox\" onclick=\"handleCheckboxClick(event, 'lee2025compression', this)\">\n    <div class=\"paper-number\"></div>\n    <div class=\"paper-thumbnail\">\n      <img data-src=\"assets/thumbnails/lee2025compression.jpg\" data-fallback=\"None\" alt=\"Paper thumbnail for Compression of 3D Gaussian Splatting with Optimized Feature Planes and Standard Video Codecs\" class=\"lazy\" loading=\"lazy\"/>\n    </div>\n    <div class=\"paper-content\">\n      <h2 class=\"paper-title\">Compression of 3D Gaussian Splatting with Optimized Feature Planes and Standard Video Codecs <span class=\"paper-year\">(2025)</span></h2>\n      <p class=\"paper-authors\">Soonbin Lee, Fangwen Shu, Yago Sanchez, Thomas Schierl, Cornelius Hellge</p>\n      <div class=\"paper-tags\"><span class=\"paper-tag\">Compression</span></div>\n      <div class=\"paper-links\"><a href=\"https://arxiv.org/pdf/2501.03399.pdf\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">📄 Paper</a>\n<button class=\"abstract-toggle\" onclick=\"toggleAbstract(this)\">📖 Show Abstract</button>\n<div class=\"paper-abstract\">3D Gaussian Splatting is a recognized method for 3D scene representation, known for its high rendering quality and speed. However, its substantial data requirements present challenges for practical applications. In this paper, we introduce an efficient compression technique that significantly reduces storage overhead by using compact representation. We propose a unified architecture that combines point cloud data and feature planes through a progressive tri-plane structure. Our method utilizes 2D feature planes, enabling continuous spatial representation. To further optimize these representations, we incorporate entropy modeling in the frequency domain, specifically designed for standard video codecs. We also propose channel-wise bit allocation to achieve a better trade-off between bitrate consumption and feature plane representation. Consequently, our model effectively leverages spatial correlations within the feature planes to enhance rate-distortion performance using standard, non-differentiable video codecs. Experimental results demonstrate that our method outperforms existing methods in data compactness while maintaining high rendering quality. Our project page is available at https://fraunhoferhhi.github.io/CodecGS\n</div></div>\n    </div>\n  </div>\n</div>\n<div class=\"paper-row\" data-id=\"rajasegaran2025gaussian\" data-title=\"Gaussian Masked Autoencoders\" data-authors=\"Jathushan Rajasegaran, Xinlei Chen, Rulilong Li, Christoph Feichtenhofer, Jitendra Malik, Shiry Ginosar\" data-year=\"2025\" data-tags='[\"Transformer\"]'>\n  <div class=\"paper-card\">\n    <input type=\"checkbox\" class=\"selection-checkbox\" onclick=\"handleCheckboxClick(event, 'rajasegaran2025gaussian', this)\">\n    <div class=\"paper-number\"></div>\n    <div class=\"paper-thumbnail\">\n      <img data-src=\"assets/thumbnails/rajasegaran2025gaussian.jpg\" data-fallback=\"None\" alt=\"Paper thumbnail for Gaussian Masked Autoencoders\" class=\"lazy\" loading=\"lazy\"/>\n    </div>\n    <div class=\"paper-content\">\n      <h2 class=\"paper-title\">Gaussian Masked Autoencoders <span class=\"paper-year\">(2025)</span></h2>\n      <p class=\"paper-authors\">Jathushan Rajasegaran, Xinlei Chen, Rulilong Li, Christoph Feichtenhofer, Jitendra Malik, Shiry Ginosar</p>\n      <div class=\"paper-tags\"><span class=\"paper-tag\">Transformer</span></div>\n      <div class=\"paper-links\"><a href=\"https://arxiv.org/pdf/2501.03229.pdf\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">📄 Paper</a>\n<button class=\"abstract-toggle\" onclick=\"toggleAbstract(this)\">📖 Show Abstract</button>\n<div class=\"paper-abstract\">This paper explores Masked Autoencoders (MAE) with Gaussian Splatting. While reconstructive self-supervised learning frameworks such as MAE learns good semantic abstractions, it is not trained for explicit spatial awareness. Our approach, named Gaussian Masked Autoencoder, or GMAE, aims to learn semantic abstractions and spatial understanding jointly. Like MAE, it reconstructs the image end-to-end in the pixel space, but beyond MAE, it also introduces an intermediate, 3D Gaussian-based representation and renders images via splatting. We show that GMAE can enable various zero-shot learning capabilities of spatial understanding (e.g., figure-ground segmentation, image layering, edge detection, etc.) while preserving the high-level semantics of self-supervised representation quality from MAE. To our knowledge, we are the first to employ Gaussian primitives in an image representation learning framework beyond optimization-based single-scene reconstructions. We believe GMAE will inspire further research in this direction and contribute to developing next-generation techniques for modeling high-fidelity visual data. More details at https://brjathu.github.io/gmae\n</div></div>\n    </div>\n  </div>\n</div>\n<div class=\"paper-row\" data-id=\"nguyen2025pointmapconditioned\" data-title=\"Pointmap-Conditioned Diffusion for Consistent Novel View Synthesis\" data-authors=\"Thang-Anh-Quan Nguyen, Nathan Piasco, Luis Roldão, Moussab Bennehar, Dzmitry Tsishkou, Laurent Caraffa, Jean-Philippe Tarel, Roland Brémond\" data-year=\"2025\" data-tags='[\"Diffusion\"]'>\n  <div class=\"paper-card\">\n    <input type=\"checkbox\" class=\"selection-checkbox\" onclick=\"handleCheckboxClick(event, 'nguyen2025pointmapconditioned', this)\">\n    <div class=\"paper-number\"></div>\n    <div class=\"paper-thumbnail\">\n      <img data-src=\"assets/thumbnails/nguyen2025pointmapconditioned.jpg\" data-fallback=\"None\" alt=\"Paper thumbnail for Pointmap-Conditioned Diffusion for Consistent Novel View Synthesis\" class=\"lazy\" loading=\"lazy\"/>\n    </div>\n    <div class=\"paper-content\">\n      <h2 class=\"paper-title\">Pointmap-Conditioned Diffusion for Consistent Novel View Synthesis <span class=\"paper-year\">(2025)</span></h2>\n      <p class=\"paper-authors\">Thang-Anh-Quan Nguyen, Nathan Piasco, Luis Roldão, Moussab Bennehar, Dzmitry Tsishkou, Laurent Caraffa, Jean-Philippe Tarel, Roland Brémond</p>\n      <div class=\"paper-tags\"><span class=\"paper-tag\">Diffusion</span></div>\n      <div class=\"paper-links\"><a href=\"https://arxiv.org/pdf/2501.02913.pdf\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">📄 Paper</a>\n<button class=\"abstract-toggle\" onclick=\"toggleAbstract(this)\">📖 Show Abstract</button>\n<div class=\"paper-abstract\">In this paper, we present PointmapDiffusion, a novel framework for single-image novel view synthesis (NVS) that utilizes pre-trained 2D diffusion models. Our method is the first to leverage pointmaps (i.e. rasterized 3D scene coordinates) as a conditioning signal, capturing geometric prior from the reference images to guide the diffusion process. By embedding reference attention blocks and a ControlNet for pointmap features, our model balances between generative capability and geometric consistency, enabling accurate view synthesis across varying viewpoints. Extensive experiments on diverse real-world datasets demonstrate that PointmapDiffusion achieves high-quality, multi-view consistent results with significantly fewer trainable parameters compared to other baselines for single-image NVS tasks.\n</div></div>\n    </div>\n  </div>\n</div>\n<div class=\"paper-row\" data-id=\"bian2025gsdit\" data-title=\"GS-DiT: Advancing Video Generation with Pseudo 4D Gaussian Fields through Efficient Dense 3D Point Tracking\" data-authors=\"Weikang Bian, Zhaoyang Huang, Xiaoyu Shi, Yijin Li, Fu-Yun Wang, Hongsheng Li\" data-year=\"2025\" data-tags='[\"Year 2025\"]'>\n  <div class=\"paper-card\">\n    <input type=\"checkbox\" class=\"selection-checkbox\" onclick=\"handleCheckboxClick(event, 'bian2025gsdit', this)\">\n    <div class=\"paper-number\"></div>\n    <div class=\"paper-thumbnail\">\n      <img data-src=\"assets/thumbnails/bian2025gsdit.jpg\" data-fallback=\"None\" alt=\"Paper thumbnail for GS-DiT: Advancing Video Generation with Pseudo 4D Gaussian Fields through Efficient Dense 3D Point Tracking\" class=\"lazy\" loading=\"lazy\"/>\n    </div>\n    <div class=\"paper-content\">\n      <h2 class=\"paper-title\">GS-DiT: Advancing Video Generation with Pseudo 4D Gaussian Fields through Efficient Dense 3D Point Tracking <span class=\"paper-year\">(2025)</span></h2>\n      <p class=\"paper-authors\">Weikang Bian, Zhaoyang Huang, Xiaoyu Shi, Yijin Li, Fu-Yun Wang, Hongsheng Li</p>\n      <div class=\"paper-tags\"></div>\n      <div class=\"paper-links\"><a href=\"https://arxiv.org/pdf/2501.02690.pdf\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">📄 Paper</a>\n<button class=\"abstract-toggle\" onclick=\"toggleAbstract(this)\">📖 Show Abstract</button>\n<div class=\"paper-abstract\">4D video control is essential in video generation as it enables the use of sophisticated lens techniques, such as multi-camera shooting and dolly zoom, which are currently unsupported by existing methods. Training a video Diffusion Transformer (DiT) directly to control 4D content requires expensive multi-view videos. Inspired by Monocular Dynamic novel View Synthesis (MDVS) that optimizes a 4D representation and renders videos according to different 4D elements, such as camera pose and object motion editing, we bring pseudo 4D Gaussian fields to video generation. Specifically, we propose a novel framework that constructs a pseudo 4D Gaussian field with dense 3D point tracking and renders the Gaussian field for all video frames. Then we finetune a pretrained DiT to generate videos following the guidance of the rendered video, dubbed as GS-DiT. To boost the training of the GS-DiT, we also propose an efficient Dense 3D Point Tracking (D3D-PT) method for the pseudo 4D Gaussian field construction. Our D3D-PT outperforms SpatialTracker, the state-of-the-art sparse 3D point tracking method, in accuracy and accelerates the inference speed by two orders of magnitude. During the inference stage, GS-DiT can generate videos with the same dynamic content while adhering to different camera parameters, addressing a significant limitation of current video generation models. GS-DiT demonstrates strong generalization capabilities and extends the 4D controllability of Gaussian splatting to video generation beyond just camera poses. It supports advanced cinematic effects through the manipulation of the Gaussian field and camera intrinsics, making it a powerful tool for creative video production. Demos are available at https://wkbian.github.io/Projects/GS-DiT/.\n</div></div>\n    </div>\n  </div>\n</div>\n<div class=\"paper-row\" data-id=\"cong2025videolifter\" data-title=\"VideoLifter: Lifting Videos to 3D with Fast Hierarchical Stereo Alignment\" data-authors=\"Wenyan Cong, Kevin Wang, Jiahui Lei, Colton Stearns, Yuanhao Cai, Dilin Wang, Rakesh Ranjan, Matt Feiszli, Leonidas Guibas, Zhangyang Wang, Weiyao Wang, Zhiwen Fan\" data-year=\"2025\" data-tags='[\"Acceleration\", \"Diffusion\", \"Project\"]'>\n  <div class=\"paper-card\">\n    <input type=\"checkbox\" class=\"selection-checkbox\" onclick=\"handleCheckboxClick(event, 'cong2025videolifter', this)\">\n    <div class=\"paper-number\"></div>\n    <div class=\"paper-thumbnail\">\n      <img data-src=\"assets/thumbnails/cong2025videolifter.jpg\" data-fallback=\"None\" alt=\"Paper thumbnail for VideoLifter: Lifting Videos to 3D with Fast Hierarchical Stereo Alignment\" class=\"lazy\" loading=\"lazy\"/>\n    </div>\n    <div class=\"paper-content\">\n      <h2 class=\"paper-title\">VideoLifter: Lifting Videos to 3D with Fast Hierarchical Stereo Alignment <span class=\"paper-year\">(2025)</span></h2>\n      <p class=\"paper-authors\">Wenyan Cong, Kevin Wang, Jiahui Lei, Colton Stearns, Yuanhao Cai, Dilin Wang, Rakesh Ranjan, Matt Feiszli, Leonidas Guibas, Zhangyang Wang, Weiyao Wang, Zhiwen Fan</p>\n      <div class=\"paper-tags\"><span class=\"paper-tag\">Acceleration</span>\n<span class=\"paper-tag\">Diffusion</span>\n<span class=\"paper-tag\">Project</span></div>\n      <div class=\"paper-links\"><a href=\"https://arxiv.org/pdf/2501.01949.pdf\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">📄 Paper</a>\n<a href=\"https://videolifter.github.io/\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">🌐 Project</a>\n<button class=\"abstract-toggle\" onclick=\"toggleAbstract(this)\">📖 Show Abstract</button>\n<div class=\"paper-abstract\">Efficiently reconstructing accurate 3D models from monocular video is a key challenge in computer vision, critical for advancing applications in virtual reality, robotics, and scene understanding. Existing approaches typically require pre-computed camera parameters and frame-by-frame reconstruction pipelines, which are prone to error accumulation and entail significant computational overhead. To address these limitations, we introduce VideoLifter, a novel framework that leverages geometric priors from a learnable model to incrementally optimize a globally sparse to dense 3D representation directly from video sequences. VideoLifter segments the video sequence into local windows, where it matches and registers frames, constructs consistent fragments, and aligns them hierarchically to produce a unified 3D model. By tracking and propagating sparse point correspondences across frames and fragments, VideoLifter incrementally refines camera poses and 3D structure, minimizing reprojection error for improved accuracy and robustness. This approach significantly accelerates the reconstruction process, reducing training time by over 82% while surpassing current state-of-the-art methods in visual fidelity and computational efficiency.\n</div></div>\n    </div>\n  </div>\n</div>\n<div class=\"paper-row\" data-id=\"huang2025enerverse\" data-title=\"EnerVerse: Envisioning Embodied Future Space for Robotics Manipulation\" data-authors=\"Siyuan Huang, Liliang Chen, Pengfei Zhou, Shengcong Chen, Zhengkai Jiang, Yue Hu, Peng Gao, Hongsheng Li, Maoqing Yao, Guanghui Ren\" data-year=\"2025\" data-tags='[\"Dynamic\", \"Project\", \"Robotics\"]'>\n  <div class=\"paper-card\">\n    <input type=\"checkbox\" class=\"selection-checkbox\" onclick=\"handleCheckboxClick(event, 'huang2025enerverse', this)\">\n    <div class=\"paper-number\"></div>\n    <div class=\"paper-thumbnail\">\n      <img data-src=\"assets/thumbnails/huang2025enerverse.jpg\" data-fallback=\"None\" alt=\"Paper thumbnail for EnerVerse: Envisioning Embodied Future Space for Robotics Manipulation\" class=\"lazy\" loading=\"lazy\"/>\n    </div>\n    <div class=\"paper-content\">\n      <h2 class=\"paper-title\">EnerVerse: Envisioning Embodied Future Space for Robotics Manipulation <span class=\"paper-year\">(2025)</span></h2>\n      <p class=\"paper-authors\">Siyuan Huang, Liliang Chen, Pengfei Zhou, Shengcong Chen, Zhengkai Jiang, Yue Hu, Peng Gao, Hongsheng Li, Maoqing Yao, Guanghui Ren</p>\n      <div class=\"paper-tags\"><span class=\"paper-tag\">Dynamic</span>\n<span class=\"paper-tag\">Project</span>\n<span class=\"paper-tag\">Robotics</span></div>\n      <div class=\"paper-links\"><a href=\"https://arxiv.org/pdf/2501.01895.pdf\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">📄 Paper</a>\n<a href=\"https://sites.google.com/view/enerverse\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">🌐 Project</a>\n<button class=\"abstract-toggle\" onclick=\"toggleAbstract(this)\">📖 Show Abstract</button>\n<div class=\"paper-abstract\">We introduce EnerVerse, a comprehensive framework for embodied future space generation specifically designed for robotic manipulation tasks. EnerVerse seamlessly integrates convolutional and bidirectional attention mechanisms for inner-chunk space modeling, ensuring low-level consistency and continuity. Recognizing the inherent redundancy in video data, we propose a sparse memory context combined with a chunkwise unidirectional generative paradigm to enable the generation of infinitely long sequences. To further augment robotic capabilities, we introduce the Free Anchor View (FAV) space, which provides flexible perspectives to enhance observation and analysis. The FAV space mitigates motion modeling ambiguity, removes physical constraints in confined environments, and significantly improves the robot's generalization and adaptability across various tasks and settings. To address the prohibitive costs and labor intensity of acquiring multi-camera observations, we present a data engine pipeline that integrates a generative model with 4D Gaussian Splatting (4DGS). This pipeline leverages the generative model's robust generalization capabilities and the spatial constraints provided by 4DGS, enabling an iterative enhancement of data quality and diversity, thus creating a data flywheel effect that effectively narrows the sim-to-real gap. Finally, our experiments demonstrate that the embodied future space generation prior substantially enhances policy predictive capabilities, resulting in improved overall performance, particularly in long-range robotic manipulation tasks.\n</div></div>\n    </div>\n  </div>\n</div>\n<div class=\"paper-row\" data-id=\"longhini2024clothsplatting\" data-title=\"Cloth-Splatting: 3D Cloth State Estimation from RGB Supervision\" data-authors=\"Alberta Longhini, Marcel Büsching, Bardienus Pieter Duisterhof, Jens Lundell, Jeffrey Ichnowski, Mårten Björkman, Danica Kragic\" data-year=\"2024\" data-tags='[\"Code\", \"Meshing\", \"Project\", \"Rendering\"]'>\n  <div class=\"paper-card\">\n    <input type=\"checkbox\" class=\"selection-checkbox\" onclick=\"handleCheckboxClick(event, 'longhini2024clothsplatting', this)\">\n    <div class=\"paper-number\"></div>\n    <div class=\"paper-thumbnail\">\n      <img data-src=\"assets/thumbnails/longhini2024clothsplatting.jpg\" data-fallback=\"None\" alt=\"Paper thumbnail for Cloth-Splatting: 3D Cloth State Estimation from RGB Supervision\" class=\"lazy\" loading=\"lazy\"/>\n    </div>\n    <div class=\"paper-content\">\n      <h2 class=\"paper-title\">Cloth-Splatting: 3D Cloth State Estimation from RGB Supervision <span class=\"paper-year\">(2024)</span></h2>\n      <p class=\"paper-authors\">Alberta Longhini, Marcel Büsching, Bardienus Pieter Duisterhof, Jens Lundell, Jeffrey Ichnowski, Mårten Björkman, Danica Kragic</p>\n      <div class=\"paper-tags\"><span class=\"paper-tag\">Code</span>\n<span class=\"paper-tag\">Meshing</span>\n<span class=\"paper-tag\">Project</span>\n<span class=\"paper-tag\">Rendering</span></div>\n      <div class=\"paper-links\"><a href=\"https://arxiv.org/pdf/2501.01715.pdf\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">📄 Paper</a>\n<a href=\"https://kth-rpl.github.io/cloth-splatting/\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">🌐 Project</a>\n<a href=\"https://github.com/KTH-RPL/cloth-splatting\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">💻 Code</a>\n<button class=\"abstract-toggle\" onclick=\"toggleAbstract(this)\">📖 Show Abstract</button>\n<div class=\"paper-abstract\">Recently, 3D Gaussian Splatting (3DGS) has revolutionized radiance field reconstruction, manifesting efficient and high-fidelity novel view synthesis. However, accurately We introduce Cloth-Splatting, a method for estimating 3D states of cloth from RGB images through a prediction-update framework. Cloth-Splatting leverages an action-conditioned dynamics model for predicting future states and uses 3D Gaussian Splatting to update the predicted states. Our key insight is that coupling a 3D mesh-based representation with Gaussian Splatting allows us to define a differentiable map between the cloth's state space and the image space. This enables the use of gradient-based optimization techniques to refine inaccurate state estimates using only RGB supervision. Our experiments demonstrate that Cloth-Splatting not only improves state estimation accuracy over current baselines but also reduces convergence time by ~85%.</div></div>\n    </div>\n  </div>\n</div>\n<div class=\"paper-row\" data-id=\"zhang2025crossviewgs\" data-title=\"CrossView-GS: Cross-view Gaussian Splatting For Large-scale Scene Reconstruction\" data-authors=\"Chenhao Zhang, Yuanping Cao, Lei Zhang\" data-year=\"2025\" data-tags='[\"Large-Scale\", \"Optimization\"]'>\n  <div class=\"paper-card\">\n    <input type=\"checkbox\" class=\"selection-checkbox\" onclick=\"handleCheckboxClick(event, 'zhang2025crossviewgs', this)\">\n    <div class=\"paper-number\"></div>\n    <div class=\"paper-thumbnail\">\n      <img data-src=\"assets/thumbnails/zhang2025crossviewgs.jpg\" data-fallback=\"None\" alt=\"Paper thumbnail for CrossView-GS: Cross-view Gaussian Splatting For Large-scale Scene Reconstruction\" class=\"lazy\" loading=\"lazy\"/>\n    </div>\n    <div class=\"paper-content\">\n      <h2 class=\"paper-title\">CrossView-GS: Cross-view Gaussian Splatting For Large-scale Scene Reconstruction <span class=\"paper-year\">(2025)</span></h2>\n      <p class=\"paper-authors\">Chenhao Zhang, Yuanping Cao, Lei Zhang</p>\n      <div class=\"paper-tags\"><span class=\"paper-tag\">Large-Scale</span>\n<span class=\"paper-tag\">Optimization</span></div>\n      <div class=\"paper-links\"><a href=\"https://arxiv.org/pdf/2501.01695.pdf\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">📄 Paper</a>\n<button class=\"abstract-toggle\" onclick=\"toggleAbstract(this)\">📖 Show Abstract</button>\n<div class=\"paper-abstract\">3D Gaussian Splatting (3DGS) has emerged as a prominent method for scene representation and reconstruction, leveraging densely distributed Gaussian primitives to enable real-time rendering of high-resolution images. While existing 3DGS methods perform well in scenes with minor view variation, large view changes in cross-view scenes pose optimization challenges for these methods. To address these issues, we propose a novel cross-view Gaussian Splatting method for large-scale scene reconstruction, based on dual-branch fusion. Our method independently reconstructs models from aerial and ground views as two independent branches to establish the baselines of Gaussian distribution, providing reliable priors for cross-view reconstruction during both initialization and densification. Specifically, a gradient-aware regularization strategy is introduced to mitigate smoothing issues caused by significant view disparities. Additionally, a unique Gaussian supplementation strategy is utilized to incorporate complementary information of dual-branch into the cross-view model. Extensive experiments on benchmark datasets demonstrate that our method achieves superior performance in novel view synthesis compared to state-of-the-art methods.</div></div>\n    </div>\n  </div>\n</div>\n<div class=\"paper-row\" data-id=\"wang2025pgsag\" data-title=\"PG-SAG: Parallel Gaussian Splatting for Fine-Grained Large-Scale Urban Buildings Reconstruction via Semantic-Aware Grouping\" data-authors=\"Tengfei Wang, Xin Wang, Yongmao Hou, Yiwei Xu, Wendi Zhang, Zongqian Zhan\" data-year=\"2025\" data-tags='[\"Large-Scale\", \"Meshing\", \"Optimization\"]'>\n  <div class=\"paper-card\">\n    <input type=\"checkbox\" class=\"selection-checkbox\" onclick=\"handleCheckboxClick(event, 'wang2025pgsag', this)\">\n    <div class=\"paper-number\"></div>\n    <div class=\"paper-thumbnail\">\n      <img data-src=\"assets/thumbnails/wang2025pgsag.jpg\" data-fallback=\"None\" alt=\"Paper thumbnail for PG-SAG: Parallel Gaussian Splatting for Fine-Grained Large-Scale Urban Buildings Reconstruction via Semantic-Aware Grouping\" class=\"lazy\" loading=\"lazy\"/>\n    </div>\n    <div class=\"paper-content\">\n      <h2 class=\"paper-title\">PG-SAG: Parallel Gaussian Splatting for Fine-Grained Large-Scale Urban Buildings Reconstruction via Semantic-Aware Grouping <span class=\"paper-year\">(2025)</span></h2>\n      <p class=\"paper-authors\">Tengfei Wang, Xin Wang, Yongmao Hou, Yiwei Xu, Wendi Zhang, Zongqian Zhan</p>\n      <div class=\"paper-tags\"><span class=\"paper-tag\">Large-Scale</span>\n<span class=\"paper-tag\">Meshing</span>\n<span class=\"paper-tag\">Optimization</span></div>\n      <div class=\"paper-links\"><a href=\"https://arxiv.org/pdf/2501.01677.pdf\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">📄 Paper</a>\n<button class=\"abstract-toggle\" onclick=\"toggleAbstract(this)\">📖 Show Abstract</button>\n<div class=\"paper-abstract\">3D Gaussian Splatting (3DGS) has emerged as a transformative method in the field of real-time novel synthesis. Based on 3DGS, recent advancements cope with large-scale scenes via spatial-based partition strategy to reduce video memory and optimization time costs. In this work, we introduce a parallel Gaussian splatting method, termed PG-SAG, which fully exploits semantic cues for both partitioning and Gaussian kernel optimization, enabling fine-grained building surface reconstruction of large-scale urban areas without downsampling the original image resolution. First, the Cross-modal model - Language Segment Anything is leveraged to segment building masks. Then, the segmented building regions is grouped into sub-regions according to the visibility check across registered images. The Gaussian kernels for these sub-regions are optimized in parallel with masked pixels. In addition, the normal loss is re-formulated for the detected edges of masks to alleviate the ambiguities in normal vectors on edges. Finally, to improve the optimization of 3D Gaussians, we introduce a gradient-constrained balance-load loss that accounts for the complexity of the corresponding scenes, effectively minimizing the thread waiting time in the pixel-parallel rendering stage as well as the reconstruction lost. Extensive experiments are tested on various urban datasets, the results demonstrated the superior performance of our PG-SAG on building surface reconstruction, compared to several state-of-the-art 3DGS-based methods.</div></div>\n    </div>\n  </div>\n</div>\n<div class=\"paper-row\" data-id=\"gao2025easysplat\" data-title=\"EasySplat: View-Adaptive Learning makes 3D Gaussian Splatting Easy\" data-authors=\"Ao Gao, Luosong Guo, Tao Chen, Zhao Wang, Ying Tai, Jian Yang, Zhenyu Zhang\" data-year=\"2025\" data-tags='[\"3ster-based\", \"Acceleration\", \"Densification\", \"Rendering\"]'>\n  <div class=\"paper-card\">\n    <input type=\"checkbox\" class=\"selection-checkbox\" onclick=\"handleCheckboxClick(event, 'gao2025easysplat', this)\">\n    <div class=\"paper-number\"></div>\n    <div class=\"paper-thumbnail\">\n      <img data-src=\"assets/thumbnails/gao2025easysplat.jpg\" data-fallback=\"None\" alt=\"Paper thumbnail for EasySplat: View-Adaptive Learning makes 3D Gaussian Splatting Easy\" class=\"lazy\" loading=\"lazy\"/>\n    </div>\n    <div class=\"paper-content\">\n      <h2 class=\"paper-title\">EasySplat: View-Adaptive Learning makes 3D Gaussian Splatting Easy <span class=\"paper-year\">(2025)</span></h2>\n      <p class=\"paper-authors\">Ao Gao, Luosong Guo, Tao Chen, Zhao Wang, Ying Tai, Jian Yang, Zhenyu Zhang</p>\n      <div class=\"paper-tags\"><span class=\"paper-tag\">3ster-based</span>\n<span class=\"paper-tag\">Acceleration</span>\n<span class=\"paper-tag\">Densification</span>\n<span class=\"paper-tag\">Rendering</span></div>\n      <div class=\"paper-links\"><a href=\"https://arxiv.org/pdf/2501.01003.pdf\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">📄 Paper</a>\n<button class=\"abstract-toggle\" onclick=\"toggleAbstract(this)\">📖 Show Abstract</button>\n<div class=\"paper-abstract\">3D Gaussian Splatting (3DGS) techniques have achieved satisfactory 3D scene representation. Despite their impressive performance, they confront challenges due to the limitation of structure-from-motion (SfM) methods on acquiring accurate scene initialization, or the inefficiency of densification strategy. In this paper, we introduce a novel framework EasySplat to achieve high-quality 3DGS modeling. Instead of using SfM for scene initialization, we employ a novel method to release the power of large-scale pointmap approaches. Specifically, we propose an efficient grouping strategy based on view similarity, and use robust pointmap priors to obtain high-quality point clouds and camera poses for 3D scene initialization. After obtaining a reliable scene structure, we propose a novel densification approach that adaptively splits Gaussian primitives based on the average shape of neighboring Gaussian ellipsoids, utilizing KNN scheme. In this way, the proposed method tackles the limitation on initialization and optimization, leading to an efficient and accurate 3DGS modeling. Extensive experiments demonstrate that EasySplat outperforms the current state-of-the-art (SOTA) in handling novel view synthesis.</div></div>\n    </div>\n  </div>\n</div>\n<div class=\"paper-row\" data-id=\"yang2024storm\" data-title=\"STORM: Spatio-Temporal Reconstruction Model for Large-Scale Outdoor Scenes\" data-authors=\"Jiawei Yang, Jiahui Huang, Yuxiao Chen, Yan Wang, Boyi Li, Yurong You, Apoorva Sharma, Maximilian Igl, Peter Karkus, Danfei Xu, Boris Ivanovic, Yue Wang, Marco Pavone\" data-year=\"2024\" data-tags='[\"Autonomous Driving\", \"Dynamic\", \"Large-Scale\", \"Video\"]'>\n  <div class=\"paper-card\">\n    <input type=\"checkbox\" class=\"selection-checkbox\" onclick=\"handleCheckboxClick(event, 'yang2024storm', this)\">\n    <div class=\"paper-number\"></div>\n    <div class=\"paper-thumbnail\">\n      <img data-src=\"assets/thumbnails/yang2024storm.jpg\" data-fallback=\"None\" alt=\"Paper thumbnail for STORM: Spatio-Temporal Reconstruction Model for Large-Scale Outdoor Scenes\" class=\"lazy\" loading=\"lazy\"/>\n    </div>\n    <div class=\"paper-content\">\n      <h2 class=\"paper-title\">STORM: Spatio-Temporal Reconstruction Model for Large-Scale Outdoor Scenes <span class=\"paper-year\">(2024)</span></h2>\n      <p class=\"paper-authors\">Jiawei Yang, Jiahui Huang, Yuxiao Chen, Yan Wang, Boyi Li, Yurong You, Apoorva Sharma, Maximilian Igl, Peter Karkus, Danfei Xu, Boris Ivanovic, Yue Wang, Marco Pavone</p>\n      <div class=\"paper-tags\"><span class=\"paper-tag\">Autonomous Driving</span>\n<span class=\"paper-tag\">Dynamic</span>\n<span class=\"paper-tag\">Large-Scale</span>\n<span class=\"paper-tag\">Video</span></div>\n      <div class=\"paper-links\"><a href=\"https://arxiv.org/pdf/2501.00602.pdf\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">📄 Paper</a>\n<a href=\"https://jiawei-yang.github.io/STORM/\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">🎥 Video</a>\n<button class=\"abstract-toggle\" onclick=\"toggleAbstract(this)\">📖 Show Abstract</button>\n<div class=\"paper-abstract\">We present STORM, a spatio-temporal reconstruction model designed for reconstructing dynamic outdoor scenes from sparse observations. Existing dynamic reconstruction methods often rely on per-scene optimization, dense observations across space and time, and strong motion supervision, resulting in lengthy optimization times, limited generalization to novel views or scenes, and degenerated quality caused by noisy pseudo-labels for dynamics. To address these challenges, STORM leverages a data-driven Transformer architecture that directly infers dynamic 3D scene representations--parameterized by 3D Gaussians and their velocities--in a single forward pass. Our key design is to aggregate 3D Gaussians from all frames using self-supervised scene flows, transforming them to the target timestep to enable complete (i.e., \"amodal\") reconstructions from arbitrary viewpoints at any moment in time. As an emergent property, STORM automatically captures dynamic instances and generates high-quality masks using only reconstruction losses. Extensive experiments on public datasets show that STORM achieves precise dynamic scene reconstruction, surpassing state-of-the-art per-scene optimization methods (+4.3 to 6.6 PSNR) and existing feed-forward approaches (+2.1 to 4.7 PSNR) in dynamic regions. STORM reconstructs large-scale outdoor scenes in 200ms, supports real-time rendering, and outperforms competitors in scene flow estimation, improving 3D EPE by 0.422m and Acc5 by 28.02%. Beyond reconstruction, we showcase four additional applications of our model, illustrating the potential of self-supervised learning for broader dynamic scene understanding.</div></div>\n    </div>\n  </div>\n</div>\n<div class=\"paper-row\" data-id=\"mao2024dreamdrive\" data-title=\"DreamDrive: Generative 4D Scene Modeling from Street View Images\" data-authors=\"Jiageng Mao, Boyi Li, Boris Ivanovic, Yuxiao Chen, Yan Wang, Yurong You, Chaowei Xiao, Danfei Xu, Marco Pavone, Yue Wang\" data-year=\"2024\" data-tags='[\"Autonomous Driving\", \"Dynamic\", \"Feed-Forward\"]'>\n  <div class=\"paper-card\">\n    <input type=\"checkbox\" class=\"selection-checkbox\" onclick=\"handleCheckboxClick(event, 'mao2024dreamdrive', this)\">\n    <div class=\"paper-number\"></div>\n    <div class=\"paper-thumbnail\">\n      <img data-src=\"assets/thumbnails/mao2024dreamdrive.jpg\" data-fallback=\"None\" alt=\"Paper thumbnail for DreamDrive: Generative 4D Scene Modeling from Street View Images\" class=\"lazy\" loading=\"lazy\"/>\n    </div>\n    <div class=\"paper-content\">\n      <h2 class=\"paper-title\">DreamDrive: Generative 4D Scene Modeling from Street View Images <span class=\"paper-year\">(2024)</span></h2>\n      <p class=\"paper-authors\">Jiageng Mao, Boyi Li, Boris Ivanovic, Yuxiao Chen, Yan Wang, Yurong You, Chaowei Xiao, Danfei Xu, Marco Pavone, Yue Wang</p>\n      <div class=\"paper-tags\"><span class=\"paper-tag\">Autonomous Driving</span>\n<span class=\"paper-tag\">Dynamic</span>\n<span class=\"paper-tag\">Feed-Forward</span></div>\n      <div class=\"paper-links\"><a href=\"https://arxiv.org/pdf/2501.00601.pdf\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">📄 Paper</a>\n<button class=\"abstract-toggle\" onclick=\"toggleAbstract(this)\">📖 Show Abstract</button>\n<div class=\"paper-abstract\">Synthesizing photo-realistic visual observations from an ego vehicle's driving trajectory is a critical step towards scalable training of self-driving models. Reconstruction-based methods create 3D scenes from driving logs and synthesize geometry-consistent driving videos through neural rendering, but their dependence on costly object annotations limits their ability to generalize to in-the-wild driving scenarios. On the other hand, generative models can synthesize action-conditioned driving videos in a more generalizable way but often struggle with maintaining 3D visual consistency. In this paper, we present DreamDrive, a 4D spatial-temporal scene generation approach that combines the merits of generation and reconstruction, to synthesize generalizable 4D driving scenes and dynamic driving videos with 3D consistency. Specifically, we leverage the generative power of video diffusion models to synthesize a sequence of visual references and further elevate them to 4D with a novel hybrid Gaussian representation. Given a driving trajectory, we then render 3D-consistent driving videos via Gaussian splatting. The use of generative priors allows our method to produce high-quality 4D scenes from in-the-wild driving data, while neural rendering ensures 3D-consistent video generation from the 4D scenes. Extensive experiments on nuScenes and street view images demonstrate that DreamDrive can generate controllable and generalizable 4D driving scenes, synthesize novel views of driving videos with high fidelity and 3D consistency, decompose static and dynamic elements in a self-supervised manner, and enhance perception and planning tasks for autonomous driving.</div></div>\n    </div>\n  </div>\n</div>\n<div class=\"paper-row\" data-id=\"wang2024sgsplatting\" data-title=\"SG-Splatting: Accelerating 3D Gaussian Splatting with Spherical Gaussians\" data-authors=\"Yiwen Wang, Siyuan Chen, Ran Yi\" data-year=\"2024\" data-tags='[\"Acceleration\"]'>\n  <div class=\"paper-card\">\n    <input type=\"checkbox\" class=\"selection-checkbox\" onclick=\"handleCheckboxClick(event, 'wang2024sgsplatting', this)\">\n    <div class=\"paper-number\"></div>\n    <div class=\"paper-thumbnail\">\n      <img data-src=\"assets/thumbnails/wang2024sgsplatting.jpg\" data-fallback=\"None\" alt=\"Paper thumbnail for SG-Splatting: Accelerating 3D Gaussian Splatting with Spherical Gaussians\" class=\"lazy\" loading=\"lazy\"/>\n    </div>\n    <div class=\"paper-content\">\n      <h2 class=\"paper-title\">SG-Splatting: Accelerating 3D Gaussian Splatting with Spherical Gaussians <span class=\"paper-year\">(2024)</span></h2>\n      <p class=\"paper-authors\">Yiwen Wang, Siyuan Chen, Ran Yi</p>\n      <div class=\"paper-tags\"><span class=\"paper-tag\">Acceleration</span></div>\n      <div class=\"paper-links\"><a href=\"https://arxiv.org/pdf/2501.00342.pdf\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">📄 Paper</a>\n<button class=\"abstract-toggle\" onclick=\"toggleAbstract(this)\">📖 Show Abstract</button>\n<div class=\"paper-abstract\">3D Gaussian Splatting is emerging as a state-of-the-art technique in novel view synthesis, recognized for its impressive balance between visual quality, speed, and rendering efficiency. However, reliance on third-degree spherical harmonics for color representation introduces significant storage demands and computational overhead, resulting in a large memory footprint and slower rendering speed. We introduce SG-Splatting with Spherical Gaussians based color representation, a novel approach to enhance rendering speed and quality in novel view synthesis. Our method first represents view-dependent color using Spherical Gaussians, instead of three degree spherical harmonics, which largely reduces the number of parameters used for color representation, and significantly accelerates the rendering process. We then develop an efficient strategy for organizing multiple Spherical Gaussians, optimizing their arrangement to achieve a balanced and accurate scene representation. To further improve rendering quality, we propose a mixed representation that combines Spherical Gaussians with low-degree spherical harmonics, capturing both high- and low-frequency color information effectively. SG-Splatting also has plug-and-play capability, allowing it to be easily integrated into existing systems. This approach improves computational efficiency and overall visual fidelity, making it a practical solution for real-time applications.\n</div></div>\n    </div>\n  </div>\n</div>\n<div class=\"paper-row\" data-id=\"cha2024perse\" data-title=\"PERSE: Personalized 3D Generative Avatars from A Single Portrait\" data-authors=\"Hyunsoo Cha, Inhee Lee, Hanbyul Joo\" data-year=\"2024\" data-tags='[\"Avatar\", \"GAN\", \"Project\", \"Video\"]'>\n  <div class=\"paper-card\">\n    <input type=\"checkbox\" class=\"selection-checkbox\" onclick=\"handleCheckboxClick(event, 'cha2024perse', this)\">\n    <div class=\"paper-number\"></div>\n    <div class=\"paper-thumbnail\">\n      <img data-src=\"assets/thumbnails/cha2024perse.jpg\" data-fallback=\"None\" alt=\"Paper thumbnail for PERSE: Personalized 3D Generative Avatars from A Single Portrait\" class=\"lazy\" loading=\"lazy\"/>\n    </div>\n    <div class=\"paper-content\">\n      <h2 class=\"paper-title\">PERSE: Personalized 3D Generative Avatars from A Single Portrait <span class=\"paper-year\">(2024)</span></h2>\n      <p class=\"paper-authors\">Hyunsoo Cha, Inhee Lee, Hanbyul Joo</p>\n      <div class=\"paper-tags\"><span class=\"paper-tag\">Avatar</span>\n<span class=\"paper-tag\">GAN</span>\n<span class=\"paper-tag\">Project</span>\n<span class=\"paper-tag\">Video</span></div>\n      <div class=\"paper-links\"><a href=\"https://arxiv.org/pdf/2412.21206v1.pdf\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">📄 Paper</a>\n<a href=\"https://hyunsoocha.github.io/perse/\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">🌐 Project</a>\n<a href=\"https://youtu.be/zX881Zx03o4\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">🎥 Video</a>\n<button class=\"abstract-toggle\" onclick=\"toggleAbstract(this)\">📖 Show Abstract</button>\n<div class=\"paper-abstract\">We present PERSE, a method for building an animatable personalized generative avatar from a reference portrait. Our avatar model enables facial attribute editing in a continuous and disentangled latent space to control each facial attribute, while preserving the individual's identity. To achieve this, our method begins by synthesizing large-scale synthetic 2D video datasets, where each video contains consistent changes in the facial expression and viewpoint, combined with a variation in a specific facial attribute from the original input. We propose a novel pipeline to produce high-quality, photorealistic 2D videos with facial attribute editing. Leveraging this synthetic attribute dataset, we present a personalized avatar creation method based on the 3D Gaussian Splatting, learning a continuous and disentangled latent space for intuitive facial attribute manipulation. To enforce smooth transitions in this latent space, we introduce a latent space regularization technique by using interpolated 2D faces as supervision. Compared to previous approaches, we demonstrate that PERSE generates high-quality avatars with interpolated attributes while preserving identity of reference person.</div></div>\n    </div>\n  </div>\n</div>\n<div class=\"paper-row\" data-id=\"yang20244d\" data-title=\"4D Gaussian Splatting: Modeling Dynamic Scenes with Native 4D Primitives\" data-authors=\"Zeyu Yang, Zijie Pan, Xiatian Zhu, Li Zhang, Yu-Gang Jiang, Philip H. S. Torr\" data-year=\"2024\" data-tags='[\"Compression\", \"Dynamic\", \"Large-Scale\"]'>\n  <div class=\"paper-card\">\n    <input type=\"checkbox\" class=\"selection-checkbox\" onclick=\"handleCheckboxClick(event, 'yang20244d', this)\">\n    <div class=\"paper-number\"></div>\n    <div class=\"paper-thumbnail\">\n      <img data-src=\"assets/thumbnails/yang20244d.jpg\" data-fallback=\"None\" alt=\"Paper thumbnail for 4D Gaussian Splatting: Modeling Dynamic Scenes with Native 4D Primitives\" class=\"lazy\" loading=\"lazy\"/>\n    </div>\n    <div class=\"paper-content\">\n      <h2 class=\"paper-title\">4D Gaussian Splatting: Modeling Dynamic Scenes with Native 4D Primitives <span class=\"paper-year\">(2024)</span></h2>\n      <p class=\"paper-authors\">Zeyu Yang, Zijie Pan, Xiatian Zhu, Li Zhang, Yu-Gang Jiang, Philip H. S. Torr</p>\n      <div class=\"paper-tags\"><span class=\"paper-tag\">Compression</span>\n<span class=\"paper-tag\">Dynamic</span>\n<span class=\"paper-tag\">Large-Scale</span></div>\n      <div class=\"paper-links\"><a href=\"https://arxiv.org/pdf/2412.20720v1.pdf\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">📄 Paper</a>\n<button class=\"abstract-toggle\" onclick=\"toggleAbstract(this)\">📖 Show Abstract</button>\n<div class=\"paper-abstract\">Dynamic 3D scene representation and novel view synthesis from captured videos are crucial for enabling immersive experiences required by AR/VR and metaverse applications. However, this task is challenging due to the complexity of unconstrained real-world scenes and their temporal dynamics. In this paper, we frame dynamic scenes as a spatio-temporal 4D volume learning problem, offering a native explicit reformulation with minimal assumptions about motion, which serves as a versatile dynamic scene learning framework. Specifically, we represent a target dynamic scene using a collection of 4D Gaussian primitives with explicit geometry and appearance features, dubbed as 4D Gaussian splatting (4DGS). This approach can capture relevant information in space and time by fitting the underlying spatio-temporal volume. Modeling the spacetime as a whole with 4D Gaussians parameterized by anisotropic ellipses that can rotate arbitrarily in space and time, our model can naturally learn view-dependent and time-evolved appearance with 4D spherindrical harmonics. Notably, our 4DGS model is the first solution that supports real-time rendering of high-resolution, photorealistic novel views for complex dynamic scenes. To enhance efficiency, we derive several compact variants that effectively reduce memory footprint and mitigate the risk of overfitting. Extensive experiments validate the superiority of 4DGS in terms of visual quality and efficiency across a range of dynamic scene-related tasks (e.g., novel view synthesis, 4D generation, scene understanding) and scenarios (e.g., single object, indoor scenes, driving environments, synthetic and real data).</div></div>\n    </div>\n  </div>\n</div>\n<div class=\"paper-row\" data-id=\"liu2024maskgaussian\" data-title=\"MaskGaussian: Adaptive 3D Gaussian Representation from Probabilistic Masks\" data-authors=\"Yifei Liu, Zhihang Zhong, Yifan Zhan, Sheng Xu, Xiao Sun\" data-year=\"2024\" data-tags='[\"Code\", \"Compression\"]'>\n  <div class=\"paper-card\">\n    <input type=\"checkbox\" class=\"selection-checkbox\" onclick=\"handleCheckboxClick(event, 'liu2024maskgaussian', this)\">\n    <div class=\"paper-number\"></div>\n    <div class=\"paper-thumbnail\">\n      <img data-src=\"assets/thumbnails/liu2024maskgaussian.jpg\" data-fallback=\"None\" alt=\"Paper thumbnail for MaskGaussian: Adaptive 3D Gaussian Representation from Probabilistic Masks\" class=\"lazy\" loading=\"lazy\"/>\n    </div>\n    <div class=\"paper-content\">\n      <h2 class=\"paper-title\">MaskGaussian: Adaptive 3D Gaussian Representation from Probabilistic Masks <span class=\"paper-year\">(2024)</span></h2>\n      <p class=\"paper-authors\">Yifei Liu, Zhihang Zhong, Yifan Zhan, Sheng Xu, Xiao Sun</p>\n      <div class=\"paper-tags\"><span class=\"paper-tag\">Code</span>\n<span class=\"paper-tag\">Compression</span></div>\n      <div class=\"paper-links\"><a href=\"https://arxiv.org/pdf/2412.20522.pdf\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">📄 Paper</a>\n<a href=\"https://github.com/kaikai23/MaskGaussian\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">💻 Code</a>\n<button class=\"abstract-toggle\" onclick=\"toggleAbstract(this)\">📖 Show Abstract</button>\n<div class=\"paper-abstract\">While 3D Gaussian Splatting (3DGS) has demonstrated remarkable performance in novel view synthesis and real-time rendering, the high memory consumption due to the use of millions of Gaussians limits its practicality. To mitigate this issue, improvements have been made by pruning unnecessary Gaussians, either through a hand-crafted criterion or by using learned masks. However, these methods deterministically remove Gaussians based on a snapshot of the pruning moment, leading to sub-optimized reconstruction performance from a long-term perspective. To address this issue, we introduce MaskGaussian, which models Gaussians as probabilistic entities rather than permanently removing them, and utilize them according to their probability of existence. To achieve this, we propose a masked-rasterization technique that enables unused yet probabilistically existing Gaussians to receive gradients, allowing for dynamic assessment of their contribution to the evolving scene and adjustment of their probability of existence. Hence, the importance of Gaussians iteratively changes and the pruned Gaussians are selected diversely. Extensive experiments demonstrate the superiority of the proposed method in achieving better rendering quality with fewer Gaussians than previous pruning methods, pruning over 60% of Gaussians on average with only a 0.02 PSNR decline. Our code can be found at: https://github.com/kaikai23/MaskGaussian\n</div></div>\n    </div>\n  </div>\n</div>\n<div class=\"paper-row\" data-id=\"xu2024das3r\" data-title=\"DAS3R: Dynamics-Aware Gaussian Splatting for Static Scene Reconstruction\" data-authors=\"Kai Xu, Tze Ho Elden Tse, Jizong Peng, Angela Yao\" data-year=\"2024\" data-tags='[\"Code\", \"Project\", \"Video\"]'>\n  <div class=\"paper-card\">\n    <input type=\"checkbox\" class=\"selection-checkbox\" onclick=\"handleCheckboxClick(event, 'xu2024das3r', this)\">\n    <div class=\"paper-number\"></div>\n    <div class=\"paper-thumbnail\">\n      <img data-src=\"assets/thumbnails/xu2024das3r.jpg\" data-fallback=\"None\" alt=\"Paper thumbnail for DAS3R: Dynamics-Aware Gaussian Splatting for Static Scene Reconstruction\" class=\"lazy\" loading=\"lazy\"/>\n    </div>\n    <div class=\"paper-content\">\n      <h2 class=\"paper-title\">DAS3R: Dynamics-Aware Gaussian Splatting for Static Scene Reconstruction <span class=\"paper-year\">(2024)</span></h2>\n      <p class=\"paper-authors\">Kai Xu, Tze Ho Elden Tse, Jizong Peng, Angela Yao</p>\n      <div class=\"paper-tags\"><span class=\"paper-tag\">Code</span>\n<span class=\"paper-tag\">Project</span>\n<span class=\"paper-tag\">Video</span></div>\n      <div class=\"paper-links\"><a href=\"https://arxiv.org/pdf/2412.19584.pdf\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">📄 Paper</a>\n<a href=\"https://kai422.github.io/DAS3R/\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">🌐 Project</a>\n<a href=\"https://github.com/kai422/das3r\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">💻 Code</a>\n<a href=\"https://kai422.github.io/DAS3R/assets/davis.gif\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">🎥 Video</a>\n<button class=\"abstract-toggle\" onclick=\"toggleAbstract(this)\">📖 Show Abstract</button>\n<div class=\"paper-abstract\">We propose a novel framework for scene decomposition and static background reconstruction from everyday videos. By integrating the trained motion masks and modeling the static scene as Gaussian splats with dynamics-aware optimization, our method achieves more accurate background reconstruction results than previous works. Our proposed method is termed DAS3R, an abbreviation for Dynamics-Aware Gaussian Splatting for Static Scene Reconstruction. Compared to existing methods, DAS3R is more robust in complex motion scenarios, capable of handling videos where dynamic objects occupy a significant portion of the scene, and does not require camera pose inputs or point cloud data from SLAM-based methods. We compared DAS3R against recent distractor-free approaches on the DAVIS and Sintel datasets; DAS3R demonstrates enhanced performance and robustness with a margin of more than 2 dB in PSNR. The project's webpage can be accessed via \\url{https://kai422.github.io/DAS3R/}\n</div></div>\n    </div>\n  </div>\n</div>\n<div class=\"paper-row\" data-id=\"cai2024dust\" data-title=\"Dust to Tower: Coarse-to-Fine Photo-Realistic Scene Reconstruction from Sparse Uncalibrated Images\" data-authors=\"Xudong Cai, Yongcai Wang, Zhaoxin Fan, Deng Haoran, Shuo Wang, Wanting Li, Deying Li, Lun Luo, Minhang Wang, Jintao Xu\" data-year=\"2024\" data-tags='[\"Inpainting\", \"Poses\", \"Sparse\"]'>\n  <div class=\"paper-card\">\n    <input type=\"checkbox\" class=\"selection-checkbox\" onclick=\"handleCheckboxClick(event, 'cai2024dust', this)\">\n    <div class=\"paper-number\"></div>\n    <div class=\"paper-thumbnail\">\n      <img data-src=\"assets/thumbnails/cai2024dust.jpg\" data-fallback=\"None\" alt=\"Paper thumbnail for Dust to Tower: Coarse-to-Fine Photo-Realistic Scene Reconstruction from Sparse Uncalibrated Images\" class=\"lazy\" loading=\"lazy\"/>\n    </div>\n    <div class=\"paper-content\">\n      <h2 class=\"paper-title\">Dust to Tower: Coarse-to-Fine Photo-Realistic Scene Reconstruction from Sparse Uncalibrated Images <span class=\"paper-year\">(2024)</span></h2>\n      <p class=\"paper-authors\">Xudong Cai, Yongcai Wang, Zhaoxin Fan, Deng Haoran, Shuo Wang, Wanting Li, Deying Li, Lun Luo, Minhang Wang, Jintao Xu</p>\n      <div class=\"paper-tags\"><span class=\"paper-tag\">Inpainting</span>\n<span class=\"paper-tag\">Poses</span>\n<span class=\"paper-tag\">Sparse</span></div>\n      <div class=\"paper-links\"><a href=\"https://arxiv.org/pdf/2412.19518.pdf\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">📄 Paper</a>\n<button class=\"abstract-toggle\" onclick=\"toggleAbstract(this)\">📖 Show Abstract</button>\n<div class=\"paper-abstract\">Photo-realistic scene reconstruction from sparse-view, uncalibrated images is highly required in practice. Although some successes have been made, existing methods are either Sparse-View but require accurate camera parameters (i.e., intrinsic and extrinsic), or SfM-free but need densely captured images. To combine the advantages of both methods while addressing their respective weaknesses, we propose Dust to Tower (D2T), an accurate and efficient coarse-to-fine framework to optimize 3DGS and image poses simultaneously from sparse and uncalibrated images. Our key idea is to first construct a coarse model efficiently and subsequently refine it using warped and inpainted images at novel viewpoints. To do this, we first introduce a Coarse Construction Module (CCM) which exploits a fast Multi-View Stereo model to initialize a 3D Gaussian Splatting (3DGS) and recover initial camera poses. To refine the 3D model at novel viewpoints, we propose a Confidence Aware Depth Alignment (CADA) module to refine the coarse depth maps by aligning their confident parts with estimated depths by a Mono-depth model. Then, a Warped Image-Guided Inpainting (WIGI) module is proposed to warp the training images to novel viewpoints by the refined depth maps, and inpainting is applied to fulfill the ``holes\" in the warped images caused by view-direction changes, providing high-quality supervision to further optimize the 3D model and the camera poses. Extensive experiments and ablation studies demonstrate the validity of D2T and its design choices, achieving state-of-the-art performance in both tasks of novel view synthesis and pose estimation while keeping high efficiency. Codes will be publicly available.</div></div>\n    </div>\n  </div>\n</div>\n<div class=\"paper-row\" data-id=\"yao2024reflective\" data-title=\"Reflective Gaussian Splatting\" data-authors=\"Yuxuan Yao, Zixuan Zeng, Chun Gu, Xiatian Zhu, Li Zhang\" data-year=\"2024\" data-tags='[\"Meshing\", \"Project\", \"Ray Tracing\", \"Relight\"]'>\n  <div class=\"paper-card\">\n    <input type=\"checkbox\" class=\"selection-checkbox\" onclick=\"handleCheckboxClick(event, 'yao2024reflective', this)\">\n    <div class=\"paper-number\"></div>\n    <div class=\"paper-thumbnail\">\n      <img data-src=\"assets/thumbnails/yao2024reflective.jpg\" data-fallback=\"None\" alt=\"Paper thumbnail for Reflective Gaussian Splatting\" class=\"lazy\" loading=\"lazy\"/>\n    </div>\n    <div class=\"paper-content\">\n      <h2 class=\"paper-title\">Reflective Gaussian Splatting <span class=\"paper-year\">(2024)</span></h2>\n      <p class=\"paper-authors\">Yuxuan Yao, Zixuan Zeng, Chun Gu, Xiatian Zhu, Li Zhang</p>\n      <div class=\"paper-tags\"><span class=\"paper-tag\">Meshing</span>\n<span class=\"paper-tag\">Project</span>\n<span class=\"paper-tag\">Ray Tracing</span>\n<span class=\"paper-tag\">Relight</span></div>\n      <div class=\"paper-links\"><a href=\"https://arxiv.org/pdf/2412.19282.pdf\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">📄 Paper</a>\n<a href=\"https://fudan-zvg.github.io/ref-gaussian/\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">🌐 Project</a>\n<button class=\"abstract-toggle\" onclick=\"toggleAbstract(this)\">📖 Show Abstract</button>\n<div class=\"paper-abstract\">Novel view synthesis has experienced significant advancements owing to increasingly capable NeRF- and 3DGS-based methods. However, reflective object reconstruction remains challenging, lacking a proper solution to achieve real-time, high-quality rendering while accommodating inter-reflection. To fill this gap, we introduce a Reflective Gaussian splatting (\\textbf{Ref-Gaussian}) framework characterized with two components: (I) {\\em Physically based deferred rendering} that empowers the rendering equation with pixel-level material properties via formulating split-sum approximation; (II) {\\em Gaussian-grounded inter-reflection} that realizes the desired inter-reflection function within a Gaussian splatting paradigm for the first time. To enhance geometry modeling, we further introduce material-aware normal propagation and an initial per-Gaussian shading stage, along with 2D Gaussian primitives. Extensive experiments on standard datasets demonstrate that Ref-Gaussian surpasses existing approaches in terms of quantitative metrics, visual quality, and compute efficiency. Further, we show that our method serves as a unified solution for both reflective and non-reflective scenes, going beyond the previous alternatives focusing on only reflective scenes. Also, we illustrate that Ref-Gaussian supports more applications such as relighting and editing.\n</div></div>\n    </div>\n  </div>\n</div>\n<div class=\"paper-row\" data-id=\"qian2024weathergs\" data-title=\"WeatherGS: 3D Scene Reconstruction in Adverse Weather Conditions via Gaussian Splatting\" data-authors=\"Chenghao Qian, Yuhu Guo, Wenjing Li, Gustav Markkula\" data-year=\"2024\" data-tags='[\"Code\", \"In the Wild\"]'>\n  <div class=\"paper-card\">\n    <input type=\"checkbox\" class=\"selection-checkbox\" onclick=\"handleCheckboxClick(event, 'qian2024weathergs', this)\">\n    <div class=\"paper-number\"></div>\n    <div class=\"paper-thumbnail\">\n      <img data-src=\"assets/thumbnails/qian2024weathergs.jpg\" data-fallback=\"None\" alt=\"Paper thumbnail for WeatherGS: 3D Scene Reconstruction in Adverse Weather Conditions via Gaussian Splatting\" class=\"lazy\" loading=\"lazy\"/>\n    </div>\n    <div class=\"paper-content\">\n      <h2 class=\"paper-title\">WeatherGS: 3D Scene Reconstruction in Adverse Weather Conditions via Gaussian Splatting <span class=\"paper-year\">(2024)</span></h2>\n      <p class=\"paper-authors\">Chenghao Qian, Yuhu Guo, Wenjing Li, Gustav Markkula</p>\n      <div class=\"paper-tags\"><span class=\"paper-tag\">Code</span>\n<span class=\"paper-tag\">In the Wild</span></div>\n      <div class=\"paper-links\"><a href=\"https://arxiv.org/pdf/2412.18862.pdf\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">📄 Paper</a>\n<a href=\"https://github.com/Jumponthemoon/WeatherGS\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">💻 Code</a>\n<button class=\"abstract-toggle\" onclick=\"toggleAbstract(this)\">📖 Show Abstract</button>\n<div class=\"paper-abstract\">3D Gaussian Splatting (3DGS) has gained significant attention for 3D scene reconstruction, but still suffers from complex outdoor environments, especially under adverse weather. This is because 3DGS treats the artifacts caused by adverse weather as part of the scene and will directly reconstruct them, largely reducing the clarity of the reconstructed scene. To address this challenge, we propose WeatherGS, a 3DGS-based framework for reconstructing clear scenes from multi-view images under different weather conditions. Specifically, we explicitly categorize the multi-weather artifacts into the dense particles and lens occlusions that have very different characters, in which the former are caused by snowflakes and raindrops in the air, and the latter are raised by the precipitation on the camera lens. In light of this, we propose a dense-to-sparse preprocess strategy, which sequentially removes the dense particles by an Atmospheric Effect Filter (AEF) and then extracts the relatively sparse occlusion masks with a Lens Effect Detector (LED). Finally, we train a set of 3D Gaussians by the processed images and generated masks for excluding occluded areas, and accurately recover the underlying clear scene by Gaussian splatting. We conduct a diverse and challenging benchmark to facilitate the evaluation of 3D reconstruction under complex weather scenarios. Extensive experiments on this benchmark demonstrate that our WeatherGS consistently produces high-quality, clean scenes across various weather scenarios, outperforming existing state-of-the-art methods.</div></div>\n    </div>\n  </div>\n</div>\n<div class=\"paper-row\" data-id=\"lyu2024facelift\" data-title=\"FaceLift: Single Image to 3D Head with View Generation and GS-LRM\" data-authors=\"Weijie Lyu, Yi Zhou, Ming-Hsuan Yang, Zhixin Shu\" data-year=\"2024\" data-tags='[\"Avatar\", \"Feed-Forward\", \"Project\", \"Video\"]'>\n  <div class=\"paper-card\">\n    <input type=\"checkbox\" class=\"selection-checkbox\" onclick=\"handleCheckboxClick(event, 'lyu2024facelift', this)\">\n    <div class=\"paper-number\"></div>\n    <div class=\"paper-thumbnail\">\n      <img data-src=\"assets/thumbnails/lyu2024facelift.jpg\" data-fallback=\"None\" alt=\"Paper thumbnail for FaceLift: Single Image to 3D Head with View Generation and GS-LRM\" class=\"lazy\" loading=\"lazy\"/>\n    </div>\n    <div class=\"paper-content\">\n      <h2 class=\"paper-title\">FaceLift: Single Image to 3D Head with View Generation and GS-LRM <span class=\"paper-year\">(2024)</span></h2>\n      <p class=\"paper-authors\">Weijie Lyu, Yi Zhou, Ming-Hsuan Yang, Zhixin Shu</p>\n      <div class=\"paper-tags\"><span class=\"paper-tag\">Avatar</span>\n<span class=\"paper-tag\">Feed-Forward</span>\n<span class=\"paper-tag\">Project</span>\n<span class=\"paper-tag\">Video</span></div>\n      <div class=\"paper-links\"><a href=\"https://arxiv.org/pdf/2412.17812.pdf\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">📄 Paper</a>\n<a href=\"https://www.wlyu.me/FaceLift/\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">🌐 Project</a>\n<a href=\"https://huggingface.co/wlyu/FaceLift/resolve/main/videos/website_video.mp4\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">🎥 Video</a>\n<button class=\"abstract-toggle\" onclick=\"toggleAbstract(this)\">📖 Show Abstract</button>\n<div class=\"paper-abstract\">We present FaceLift, a feed-forward approach for rapid, high-quality, 360-degree head reconstruction from a single image. Our pipeline begins by employing a multi-view latent diffusion model that generates consistent side and back views of the head from a single facial input. These generated views then serve as input to a GS-LRM reconstructor, which produces a comprehensive 3D representation using Gaussian splats. To train our system, we develop a dataset of multi-view renderings using synthetic 3D human head as-sets. The diffusion-based multi-view generator is trained exclusively on synthetic head images, while the GS-LRM reconstructor undergoes initial training on Objaverse followed by fine-tuning on synthetic head data. FaceLift excels at preserving identity and maintaining view consistency across views. Despite being trained solely on synthetic data, FaceLift demonstrates remarkable generalization to real-world images. Through extensive qualitative and quantitative evaluations, we show that FaceLift outperforms state-of-the-art methods in 3D head reconstruction, highlighting its practical applicability and robust performance on real-world images. In addition to single image reconstruction, FaceLift supports video inputs for 4D novel view synthesis and seamlessly integrates with 2D reanimation techniques to enable 3D facial animation.</div></div>\n    </div>\n  </div>\n</div>\n<div class=\"paper-row\" data-id=\"shao2024gausim\" data-title=\"GauSim: Registering Elastic Objects into Digital World by Gaussian Simulator\" data-authors=\"Yidi Shao, Mu Huang, Chen Change Loy, Bo Dai\" data-year=\"2024\" data-tags='[\"Dynamic\", \"Physics\", \"Project\"]'>\n  <div class=\"paper-card\">\n    <input type=\"checkbox\" class=\"selection-checkbox\" onclick=\"handleCheckboxClick(event, 'shao2024gausim', this)\">\n    <div class=\"paper-number\"></div>\n    <div class=\"paper-thumbnail\">\n      <img data-src=\"assets/thumbnails/shao2024gausim.jpg\" data-fallback=\"None\" alt=\"Paper thumbnail for GauSim: Registering Elastic Objects into Digital World by Gaussian Simulator\" class=\"lazy\" loading=\"lazy\"/>\n    </div>\n    <div class=\"paper-content\">\n      <h2 class=\"paper-title\">GauSim: Registering Elastic Objects into Digital World by Gaussian Simulator <span class=\"paper-year\">(2024)</span></h2>\n      <p class=\"paper-authors\">Yidi Shao, Mu Huang, Chen Change Loy, Bo Dai</p>\n      <div class=\"paper-tags\"><span class=\"paper-tag\">Dynamic</span>\n<span class=\"paper-tag\">Physics</span>\n<span class=\"paper-tag\">Project</span></div>\n      <div class=\"paper-links\"><a href=\"https://arxiv.org/pdf/2412.17804.pdf\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">📄 Paper</a>\n<a href=\"https://www.mmlab-ntu.com/project/gausim/index.html\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">🌐 Project</a>\n<button class=\"abstract-toggle\" onclick=\"toggleAbstract(this)\">📖 Show Abstract</button>\n<div class=\"paper-abstract\">In this work, we introduce GauSim, a novel neural network-based simulator designed to capture the dynamic behaviors of real-world elastic objects represented through Gaussian kernels. Unlike traditional methods that treat kernels as particles within particle-based simulations, we leverage continuum mechanics, modeling each kernel as a continuous piece of matter to account for realistic deformations without idealized assumptions. To improve computational efficiency and fidelity, we employ a hierarchical structure that organizes kernels into Center of Mass Systems (CMS) with explicit formulations, enabling a coarse-to-fine simulation approach. This structure significantly reduces computational overhead while preserving detailed dynamics. In addition, GauSim incorporates explicit physics constraints, such as mass and momentum conservation, ensuring interpretable results and robust, physically plausible simulations. To validate our approach, we present a new dataset, READY, containing multi-view videos of real-world elastic deformations. Experimental results demonstrate that GauSim achieves superior performance compared to existing physics-driven baselines, offering a practical and accurate solution for simulating complex dynamic behaviors. Code and model will be released.</div></div>\n    </div>\n  </div>\n</div>\n<div class=\"paper-row\" data-id=\"jin2024activegs\" data-title=\"ActiveGS: Active Scene Reconstruction using Gaussian Splatting\" data-authors=\"Liren Jin, Xingguang Zhong, Yue Pan, Jens Behley, Cyrill Stachniss, Marija Popović\" data-year=\"2024\" data-tags='[\"Meshing\", \"Robotics\", \"SLAM\"]'>\n  <div class=\"paper-card\">\n    <input type=\"checkbox\" class=\"selection-checkbox\" onclick=\"handleCheckboxClick(event, 'jin2024activegs', this)\">\n    <div class=\"paper-number\"></div>\n    <div class=\"paper-thumbnail\">\n      <img data-src=\"assets/thumbnails/jin2024activegs.jpg\" data-fallback=\"None\" alt=\"Paper thumbnail for ActiveGS: Active Scene Reconstruction using Gaussian Splatting\" class=\"lazy\" loading=\"lazy\"/>\n    </div>\n    <div class=\"paper-content\">\n      <h2 class=\"paper-title\">ActiveGS: Active Scene Reconstruction using Gaussian Splatting <span class=\"paper-year\">(2024)</span></h2>\n      <p class=\"paper-authors\">Liren Jin, Xingguang Zhong, Yue Pan, Jens Behley, Cyrill Stachniss, Marija Popović</p>\n      <div class=\"paper-tags\"><span class=\"paper-tag\">Meshing</span>\n<span class=\"paper-tag\">Robotics</span>\n<span class=\"paper-tag\">SLAM</span></div>\n      <div class=\"paper-links\"><a href=\"https://arxiv.org/pdf/2412.17769.pdf\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">📄 Paper</a>\n<button class=\"abstract-toggle\" onclick=\"toggleAbstract(this)\">📖 Show Abstract</button>\n<div class=\"paper-abstract\">Robotics applications often rely on scene reconstructions to enable downstream tasks. In this work, we tackle the challenge of actively building an accurate map of an unknown scene using an on-board RGB-D camera. We propose a hybrid map representation that combines a Gaussian splatting map with a coarse voxel map, leveraging the strengths of both representations: the high-fidelity scene reconstruction capabilities of Gaussian splatting and the spatial modelling strengths of the voxel map. The core of our framework is an effective confidence modelling technique for the Gaussian splatting map to identify under-reconstructed areas, while utilising spatial information from the voxel map to target unexplored areas and assist in collision-free path planning. By actively collecting scene information in under-reconstructed and unexplored areas for map updates, our approach achieves superior Gaussian splatting reconstruction results compared to state-of-the-art approaches. Additionally, we demonstrate the applicability of our active scene reconstruction framework in the real world using an unmanned aerial vehicle.\n</div></div>\n    </div>\n  </div>\n</div>\n<div class=\"paper-row\" data-id=\"gao2024cosurfgscollaborative\" data-title=\"CoSurfGS:Collaborative 3D Surface Gaussian Splatting with Distributed Learning for Large Scene Reconstruction\" data-authors=\"Yuanyuan Gao, Yalun Dai, Hao Li, Weicai Ye, Junyi Chen, Danpeng Chen, Dingwen Zhang, Tong He, Guofeng Zhang, Junwei Han\" data-year=\"2024\" data-tags='[\"Distributed\", \"Large-Scale\", \"Meshing\", \"Project\"]'>\n  <div class=\"paper-card\">\n    <input type=\"checkbox\" class=\"selection-checkbox\" onclick=\"handleCheckboxClick(event, 'gao2024cosurfgscollaborative', this)\">\n    <div class=\"paper-number\"></div>\n    <div class=\"paper-thumbnail\">\n      <img data-src=\"assets/thumbnails/gao2024cosurfgscollaborative.jpg\" data-fallback=\"None\" alt=\"Paper thumbnail for CoSurfGS:Collaborative 3D Surface Gaussian Splatting with Distributed Learning for Large Scene Reconstruction\" class=\"lazy\" loading=\"lazy\"/>\n    </div>\n    <div class=\"paper-content\">\n      <h2 class=\"paper-title\">CoSurfGS:Collaborative 3D Surface Gaussian Splatting with Distributed Learning for Large Scene Reconstruction <span class=\"paper-year\">(2024)</span></h2>\n      <p class=\"paper-authors\">Yuanyuan Gao, Yalun Dai, Hao Li, Weicai Ye, Junyi Chen, Danpeng Chen, Dingwen Zhang, Tong He, Guofeng Zhang, Junwei Han</p>\n      <div class=\"paper-tags\"><span class=\"paper-tag\">Distributed</span>\n<span class=\"paper-tag\">Large-Scale</span>\n<span class=\"paper-tag\">Meshing</span>\n<span class=\"paper-tag\">Project</span></div>\n      <div class=\"paper-links\"><a href=\"https://arxiv.org/pdf/2412.17612.pdf\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">📄 Paper</a>\n<a href=\"https://gyy456.github.io/CoSurfGS/\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">🌐 Project</a>\n<button class=\"abstract-toggle\" onclick=\"toggleAbstract(this)\">📖 Show Abstract</button>\n<div class=\"paper-abstract\">3D Gaussian Splatting (3DGS) has demonstrated impressive performance in scene reconstruction. However, most existing GS-based surface reconstruction methods focus on 3D objects or limited scenes. Directly applying these methods to large-scale scene reconstruction will pose challenges such as high memory costs, excessive time consumption, and lack of geometric detail, which makes it difficult to implement in practical applications. To address these issues, we propose a multi-agent collaborative fast 3DGS surface reconstruction framework based on distributed learning for large-scale surface reconstruction. Specifically, we develop local model compression (LMC) and model aggregation schemes (MAS) to achieve high-quality surface representation of large scenes while reducing GPU memory consumption. Extensive experiments on Urban3d, MegaNeRF, and BlendedMVS demonstrate that our proposed method can achieve fast and scalable high-fidelity surface reconstruction and photorealistic rendering.</div></div>\n    </div>\n  </div>\n</div>\n<div class=\"paper-row\" data-id=\"gui2024balanced\" data-title=\"Balanced 3DGS: Gaussian-wise Parallelism Rendering with Fine-Grained Tiling\" data-authors=\"Hao Gui, Lin Hu, Rui Chen, Mingxiao Huang, Yuxin Yin, Jin Yang, Yong Wu\" data-year=\"2024\" data-tags='[\"Acceleration\"]'>\n  <div class=\"paper-card\">\n    <input type=\"checkbox\" class=\"selection-checkbox\" onclick=\"handleCheckboxClick(event, 'gui2024balanced', this)\">\n    <div class=\"paper-number\"></div>\n    <div class=\"paper-thumbnail\">\n      <img data-src=\"assets/thumbnails/gui2024balanced.jpg\" data-fallback=\"None\" alt=\"Paper thumbnail for Balanced 3DGS: Gaussian-wise Parallelism Rendering with Fine-Grained Tiling\" class=\"lazy\" loading=\"lazy\"/>\n    </div>\n    <div class=\"paper-content\">\n      <h2 class=\"paper-title\">Balanced 3DGS: Gaussian-wise Parallelism Rendering with Fine-Grained Tiling <span class=\"paper-year\">(2024)</span></h2>\n      <p class=\"paper-authors\">Hao Gui, Lin Hu, Rui Chen, Mingxiao Huang, Yuxin Yin, Jin Yang, Yong Wu</p>\n      <div class=\"paper-tags\"><span class=\"paper-tag\">Acceleration</span></div>\n      <div class=\"paper-links\"><a href=\"https://arxiv.org/pdf/2412.17378.pdf\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">📄 Paper</a>\n<button class=\"abstract-toggle\" onclick=\"toggleAbstract(this)\">📖 Show Abstract</button>\n<div class=\"paper-abstract\">3D Gaussian Splatting (3DGS) is increasingly attracting attention in both academia and industry owing to its superior visual quality and rendering speed. However, training a 3DGS model remains a time-intensive task, especially in load imbalance scenarios where workload diversity among pixels and Gaussian spheres causes poor renderCUDA kernel performance. We introduce Balanced 3DGS, a Gaussian-wise parallelism rendering with fine-grained tiling approach in 3DGS training process, perfectly solving load-imbalance issues. First, we innovatively introduce the inter-block dynamic workload distribution technique to map workloads to Streaming Multiprocessor(SM) resources within a single GPU dynamically, which constitutes the foundation of load balancing. Second, we are the first to propose the Gaussian-wise parallel rendering technique to significantly reduce workload divergence inside a warp, which serves as a critical component in addressing load imbalance. Based on the above two methods, we further creatively put forward the fine-grained combined load balancing technique to uniformly distribute workload across all SMs, which boosts the forward renderCUDA kernel performance by up to 7.52x. Besides, we present a self-adaptive render kernel selection strategy during the 3DGS training process based on different load-balance situations, which effectively improves training efficiency.\n</div></div>\n    </div>\n  </div>\n</div>\n<div class=\"paper-row\" data-id=\"jambon2024interactive\" data-title=\"Interactive Scene Authoring with Specialized Generative Primitives\" data-authors=\"Clément Jambon, Changwoon Choi, Dongsu Zhang, Olga Sorkine-Hornung, Young Min Kim\" data-year=\"2024\" data-tags='[\"Editing\", \"World Generation\"]'>\n  <div class=\"paper-card\">\n    <input type=\"checkbox\" class=\"selection-checkbox\" onclick=\"handleCheckboxClick(event, 'jambon2024interactive', this)\">\n    <div class=\"paper-number\"></div>\n    <div class=\"paper-thumbnail\">\n      <img data-src=\"assets/thumbnails/jambon2024interactive.jpg\" data-fallback=\"None\" alt=\"Paper thumbnail for Interactive Scene Authoring with Specialized Generative Primitives\" class=\"lazy\" loading=\"lazy\"/>\n    </div>\n    <div class=\"paper-content\">\n      <h2 class=\"paper-title\">Interactive Scene Authoring with Specialized Generative Primitives <span class=\"paper-year\">(2024)</span></h2>\n      <p class=\"paper-authors\">Clément Jambon, Changwoon Choi, Dongsu Zhang, Olga Sorkine-Hornung, Young Min Kim</p>\n      <div class=\"paper-tags\"><span class=\"paper-tag\">Editing</span>\n<span class=\"paper-tag\">World Generation</span></div>\n      <div class=\"paper-links\"><a href=\"https://arxiv.org/pdf/2412.16253.pdf\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">📄 Paper</a>\n<button class=\"abstract-toggle\" onclick=\"toggleAbstract(this)\">📖 Show Abstract</button>\n<div class=\"paper-abstract\">Generating high-quality 3D digital assets often requires expert knowledge of complex design tools. We introduce Specialized Generative Primitives, a generative framework that allows non-expert users to author high-quality 3D scenes in a seamless, lightweight, and controllable manner. Each primitive is an efficient generative model that captures the distribution of a single exemplar from the real world. With our framework, users capture a video of an environment, which we turn into a high-quality and explicit appearance model thanks to 3D Gaussian Splatting. Users then select regions of interest guided by semantically-aware features. To create a generative primitive, we adapt Generative Cellular Automata to single-exemplar training and controllable generation. We decouple the generative task from the appearance model by operating on sparse voxels and we recover a high-quality output with a subsequent sparse patch consistency step. Each primitive can be trained within 10 minutes and used to author new scenes interactively in a fully compositional manner. We showcase interactive sessions where various primitives are extracted from real-world scenes and controlled to create 3D assets and scenes in a few minutes. We also demonstrate additional capabilities of our primitives: handling various 3D representations to control generation, transferring appearances, and editing geometries.\n</div></div>\n    </div>\n  </div>\n</div>\n<div class=\"paper-row\" data-id=\"shen2024solidgs\" data-title=\"SolidGS: Consolidating Gaussian Surfel Splatting for Sparse-View Surface Reconstruction\" data-authors=\"Zhuowen Shen, Yuan Liu, Zhang Chen, Zhong Li, Jiepeng Wang, Yongqing Liang, Zhengming Yu, Jingdong Zhang, Yi Xu, Scott Schaefer, Xin Li, Wenping Wang\" data-year=\"2024\" data-tags='[\"Meshing\", \"Project\", \"Sparse\"]'>\n  <div class=\"paper-card\">\n    <input type=\"checkbox\" class=\"selection-checkbox\" onclick=\"handleCheckboxClick(event, 'shen2024solidgs', this)\">\n    <div class=\"paper-number\"></div>\n    <div class=\"paper-thumbnail\">\n      <img data-src=\"assets/thumbnails/shen2024solidgs.jpg\" data-fallback=\"None\" alt=\"Paper thumbnail for SolidGS: Consolidating Gaussian Surfel Splatting for Sparse-View Surface Reconstruction\" class=\"lazy\" loading=\"lazy\"/>\n    </div>\n    <div class=\"paper-content\">\n      <h2 class=\"paper-title\">SolidGS: Consolidating Gaussian Surfel Splatting for Sparse-View Surface Reconstruction <span class=\"paper-year\">(2024)</span></h2>\n      <p class=\"paper-authors\">Zhuowen Shen, Yuan Liu, Zhang Chen, Zhong Li, Jiepeng Wang, Yongqing Liang, Zhengming Yu, Jingdong Zhang, Yi Xu, Scott Schaefer, Xin Li, Wenping Wang</p>\n      <div class=\"paper-tags\"><span class=\"paper-tag\">Meshing</span>\n<span class=\"paper-tag\">Project</span>\n<span class=\"paper-tag\">Sparse</span></div>\n      <div class=\"paper-links\"><a href=\"https://arxiv.org/pdf/2412.15400.pdf\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">📄 Paper</a>\n<a href=\"https://mickshen7558.github.io/projects/SolidGS/\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">🌐 Project</a>\n<button class=\"abstract-toggle\" onclick=\"toggleAbstract(this)\">📖 Show Abstract</button>\n<div class=\"paper-abstract\">Gaussian splatting has achieved impressive improvements for both novel-view synthesis and surface reconstruction from multi-view images. However, current methods still struggle to reconstruct high-quality surfaces from only sparse view input images using Gaussian splatting. In this paper, we propose a novel method called SolidGS to address this problem. We observed that the reconstructed geometry can be severely inconsistent across multi-views, due to the property of Gaussian function in geometry rendering. This motivates us to consolidate all Gaussians by adopting a more solid kernel function, which effectively improves the surface reconstruction quality. With the additional help of geometrical regularization and monocular normal estimation, our method achieves superior performance on the sparse view surface reconstruction than all the Gaussian splatting methods and neural field methods on the widely used DTU, Tanks-and-Temples, and LLFF datasets.\n</div></div>\n    </div>\n  </div>\n</div>\n<div class=\"paper-row\" data-id=\"xie2024envgs\" data-title=\"EnvGS: Modeling View-Dependent Appearance with Environment Gaussian\" data-authors=\"Tao Xie, Xi Chen, Zhen Xu, Yiman Xie, Yudong Jin, Yujun Shen, Sida Peng, Hujun Bao, Xiaowei Zhou\" data-year=\"2024\" data-tags='[\"Project\", \"Ray Tracing\", \"Rendering\", \"Video\"]'>\n  <div class=\"paper-card\">\n    <input type=\"checkbox\" class=\"selection-checkbox\" onclick=\"handleCheckboxClick(event, 'xie2024envgs', this)\">\n    <div class=\"paper-number\"></div>\n    <div class=\"paper-thumbnail\">\n      <img data-src=\"assets/thumbnails/xie2024envgs.jpg\" data-fallback=\"None\" alt=\"Paper thumbnail for EnvGS: Modeling View-Dependent Appearance with Environment Gaussian\" class=\"lazy\" loading=\"lazy\"/>\n    </div>\n    <div class=\"paper-content\">\n      <h2 class=\"paper-title\">EnvGS: Modeling View-Dependent Appearance with Environment Gaussian <span class=\"paper-year\">(2024)</span></h2>\n      <p class=\"paper-authors\">Tao Xie, Xi Chen, Zhen Xu, Yiman Xie, Yudong Jin, Yujun Shen, Sida Peng, Hujun Bao, Xiaowei Zhou</p>\n      <div class=\"paper-tags\"><span class=\"paper-tag\">Project</span>\n<span class=\"paper-tag\">Ray Tracing</span>\n<span class=\"paper-tag\">Rendering</span>\n<span class=\"paper-tag\">Video</span></div>\n      <div class=\"paper-links\"><a href=\"https://arxiv.org/pdf/2412.15215.pdf\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">📄 Paper</a>\n<a href=\"https://zju3dv.github.io/envgs/\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">🌐 Project</a>\n<a href=\"https://raw.githubusercontent.com/xbillowy/assets/refs/heads/main/envgs.github.io.assets/teaser.mp4\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">🎥 Video</a>\n<button class=\"abstract-toggle\" onclick=\"toggleAbstract(this)\">📖 Show Abstract</button>\n<div class=\"paper-abstract\">Reconstructing complex reflections in real-world scenes from 2D images is essential for achieving photorealistic novel view synthesis. Existing methods that utilize environment maps to model reflections from distant lighting often struggle with high-frequency reflection details and fail to account for near-field reflections. In this work, we introduce EnvGS, a novel approach that employs a set of Gaussian primitives as an explicit 3D representation for capturing reflections of environments. These environment Gaussian primitives are incorporated with base Gaussian primitives to model the appearance of the whole scene. To efficiently render these environment Gaussian primitives, we developed a ray-tracing-based renderer that leverages the GPU's RT core for fast rendering. This allows us to jointly optimize our model for high-quality reconstruction while maintaining real-time rendering speeds. Results from multiple real-world and synthetic datasets demonstrate that our method produces significantly more detailed reflections, achieving the best rendering quality in real-time novel view synthesis.\n</div></div>\n    </div>\n  </div>\n</div>\n<div class=\"paper-row\" data-id=\"saito2024squeezeme\" data-title=\"SqueezeMe: Efficient Gaussian Avatars for VR\" data-authors=\"Shunsuke Saito, Stanislav Pidhorskyi, Igor Santesteban, Forrest Iandola, Divam Gupta, Anuj Pahuja, Nemanja Bartolovic, Frank Yu, Emanuel Garbin, Tomas Simon\" data-year=\"2024\" data-tags='[\"Avatar\", \"Dynamic\", \"Project\"]'>\n  <div class=\"paper-card\">\n    <input type=\"checkbox\" class=\"selection-checkbox\" onclick=\"handleCheckboxClick(event, 'saito2024squeezeme', this)\">\n    <div class=\"paper-number\"></div>\n    <div class=\"paper-thumbnail\">\n      <img data-src=\"assets/thumbnails/saito2024squeezeme.jpg\" data-fallback=\"None\" alt=\"Paper thumbnail for SqueezeMe: Efficient Gaussian Avatars for VR\" class=\"lazy\" loading=\"lazy\"/>\n    </div>\n    <div class=\"paper-content\">\n      <h2 class=\"paper-title\">SqueezeMe: Efficient Gaussian Avatars for VR <span class=\"paper-year\">(2024)</span></h2>\n      <p class=\"paper-authors\">Shunsuke Saito, Stanislav Pidhorskyi, Igor Santesteban, Forrest Iandola, Divam Gupta, Anuj Pahuja, Nemanja Bartolovic, Frank Yu, Emanuel Garbin, Tomas Simon</p>\n      <div class=\"paper-tags\"><span class=\"paper-tag\">Avatar</span>\n<span class=\"paper-tag\">Dynamic</span>\n<span class=\"paper-tag\">Project</span></div>\n      <div class=\"paper-links\"><a href=\"https://arxiv.org/pdf/2412.15171.pdf\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">📄 Paper</a>\n<a href=\"https://forresti.github.io/squeezeme.\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">🌐 Project</a>\n<button class=\"abstract-toggle\" onclick=\"toggleAbstract(this)\">📖 Show Abstract</button>\n<div class=\"paper-abstract\">Gaussian Splatting has enabled real-time 3D human avatars with unprecedented levels of visual quality. While previous methods require a desktop GPU for real-time inference of a single avatar, we aim to squeeze multiple Gaussian avatars onto a portable virtual reality headset with real-time drivable inference. We begin by training a previous work, Animatable Gaussians, on a high quality dataset captured with 512 cameras. The Gaussians are animated by controlling base set of Gaussians with linear blend skinning (LBS) motion and then further adjusting the Gaussians with a neural network decoder to correct their appearance. When deploying the model on a Meta Quest 3 VR headset, we find two major computational bottlenecks: the decoder and the rendering. To accelerate the decoder, we train the Gaussians in UV-space instead of pixel-space, and we distill the decoder to a single neural network layer. Further, we discover that neighborhoods of Gaussians can share a single corrective from the decoder, which provides an additional speedup. To accelerate the rendering, we develop a custom pipeline in Vulkan that runs on the mobile GPU. Putting it all together, we run 3 Gaussian avatars concurrently at 72 FPS on a VR headset. \n</div></div>\n    </div>\n  </div>\n</div>\n<div class=\"paper-row\" data-id=\"lu2024turbogs\" data-title=\"Turbo-GS: Accelerating 3D Gaussian Fitting for High-Quality Radiance Fields\" data-authors=\"Tao Lu, Ankit Dhiman, R Srinath, Emre Arslan, Angela Xing, Yuanbo Xiangli, R Venkatesh Babu, Srinath Sridhar\" data-year=\"2024\" data-tags='[\"Acceleration\", \"Densification\", \"Project\"]'>\n  <div class=\"paper-card\">\n    <input type=\"checkbox\" class=\"selection-checkbox\" onclick=\"handleCheckboxClick(event, 'lu2024turbogs', this)\">\n    <div class=\"paper-number\"></div>\n    <div class=\"paper-thumbnail\">\n      <img data-src=\"assets/thumbnails/lu2024turbogs.jpg\" data-fallback=\"None\" alt=\"Paper thumbnail for Turbo-GS: Accelerating 3D Gaussian Fitting for High-Quality Radiance Fields\" class=\"lazy\" loading=\"lazy\"/>\n    </div>\n    <div class=\"paper-content\">\n      <h2 class=\"paper-title\">Turbo-GS: Accelerating 3D Gaussian Fitting for High-Quality Radiance Fields <span class=\"paper-year\">(2024)</span></h2>\n      <p class=\"paper-authors\">Tao Lu, Ankit Dhiman, R Srinath, Emre Arslan, Angela Xing, Yuanbo Xiangli, R Venkatesh Babu, Srinath Sridhar</p>\n      <div class=\"paper-tags\"><span class=\"paper-tag\">Acceleration</span>\n<span class=\"paper-tag\">Densification</span>\n<span class=\"paper-tag\">Project</span></div>\n      <div class=\"paper-links\"><a href=\"https://arxiv.org/pdf/2412.13547v1.pdf\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">📄 Paper</a>\n<a href=\"https://ivl.cs.brown.edu/research/turbo-gs\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">🌐 Project</a>\n<button class=\"abstract-toggle\" onclick=\"toggleAbstract(this)\">📖 Show Abstract</button>\n<div class=\"paper-abstract\">Novel-view synthesis is an important problem in computer vision with applications in 3D reconstruction, mixed reality, and robotics. Recent methods like 3D Gaussian Splatting (3DGS) have become the preferred method for this task, providing high-quality novel views in real time. However, the training time of a 3DGS model is slow, often taking 30 minutes for a scene with 200 views. In contrast, our goal is to reduce the optimization time by training for fewer steps while maintaining high rendering quality. Specifically, we combine the guidance from both the position error and the appearance error to achieve a more effective densification. To balance the rate between adding new Gaussians and fitting old Gaussians, we develop a convergence-aware budget control mechanism. Moreover, to make the densification process more reliable, we selectively add new Gaussians from mostly visited regions. With these designs, we reduce the Gaussian optimization steps to one-third of the previous approach while achieving a comparable or even better novel view rendering quality. To further facilitate the rapid fitting of 4K resolution images, we introduce a dilation-based rendering technique. Our method, Turbo-GS, speeds up optimization for typical scenes and scales well to high-resolution (4K) scenarios on standard datasets. Through extensive experiments, we show that our method is significantly faster in optimization than other methods while retaining quality. Project page: https://ivl.cs.brown.edu/research/turbo-gs.\n</div></div>\n    </div>\n  </div>\n</div>\n<div class=\"paper-row\" data-id=\"jiang2024gausstr\" data-title=\"GaussTR: Foundation Model-Aligned Gaussian Transformer for Self-Supervised 3D Spatial Understanding\" data-authors=\"Haoyi Jiang, Liu Liu, Tianheng Cheng, Xinjie Wang, Tianwei Lin, Zhizhong Su, Wenyu Liu, Xinggang Wang\" data-year=\"2024\" data-tags='[\"Code\", \"Project\"]'>\n  <div class=\"paper-card\">\n    <input type=\"checkbox\" class=\"selection-checkbox\" onclick=\"handleCheckboxClick(event, 'jiang2024gausstr', this)\">\n    <div class=\"paper-number\"></div>\n    <div class=\"paper-thumbnail\">\n      <img data-src=\"assets/thumbnails/jiang2024gausstr.jpg\" data-fallback=\"None\" alt=\"Paper thumbnail for GaussTR: Foundation Model-Aligned Gaussian Transformer for Self-Supervised 3D Spatial Understanding\" class=\"lazy\" loading=\"lazy\"/>\n    </div>\n    <div class=\"paper-content\">\n      <h2 class=\"paper-title\">GaussTR: Foundation Model-Aligned Gaussian Transformer for Self-Supervised 3D Spatial Understanding <span class=\"paper-year\">(2024)</span></h2>\n      <p class=\"paper-authors\">Haoyi Jiang, Liu Liu, Tianheng Cheng, Xinjie Wang, Tianwei Lin, Zhizhong Su, Wenyu Liu, Xinggang Wang</p>\n      <div class=\"paper-tags\"><span class=\"paper-tag\">Code</span>\n<span class=\"paper-tag\">Project</span></div>\n      <div class=\"paper-links\"><a href=\"https://arxiv.org/pdf/2412.13193.pdf\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">📄 Paper</a>\n<a href=\"https://hustvl.github.io/GaussTR/\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">🌐 Project</a>\n<a href=\"https://github.com/hustvl/GaussTR\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">💻 Code</a>\n<button class=\"abstract-toggle\" onclick=\"toggleAbstract(this)\">📖 Show Abstract</button>\n<div class=\"paper-abstract\">3D Semantic Occupancy Prediction is fundamental for spatial understanding as it provides a comprehensive semantic cognition of surrounding environments. However, prevalent approaches primarily rely on extensive labeled data and computationally intensive voxel-based modeling, restricting the scalability and generalizability of 3D representation learning. In this paper, we introduce GaussTR, a novel Gaussian Transformer that leverages alignment with foundation models to advance self-supervised 3D spatial understanding. GaussTR adopts a Transformer architecture to predict sparse sets of 3D Gaussians that represent scenes in a feed-forward manner. Through aligning rendered Gaussian features with diverse knowledge from pre-trained foundation models, GaussTR facilitates the learning of versatile 3D representations and enables open-vocabulary occupancy prediction without explicit annotations. Empirical evaluations on the Occ3D-nuScenes dataset showcase GaussTR's state-of-the-art zero-shot performance, achieving 11.70 mIoU while reducing training duration by approximately 50%. These experimental results highlight the significant potential of GaussTR for scalable and holistic 3D spatial understanding, with promising implications for autonomous driving and embodied agents. Code is available at https://github.com/hustvl/GaussTR.\n</div></div>\n    </div>\n  </div>\n</div>\n<div class=\"paper-row\" data-id=\"sun2024realtime\" data-title=\"Real-time Free-view Human Rendering from Sparse-view RGB Videos using Double Unprojected Textures\" data-authors=\"Guoxing Sun, Rishabh Dabral, Heming Zhu, Pascal Fua, Christian Theobalt, Marc Habermann\" data-year=\"2024\" data-tags='[\"Avatar\", \"Project\", \"Sparse\", \"Texturing\", \"Video\"]'>\n  <div class=\"paper-card\">\n    <input type=\"checkbox\" class=\"selection-checkbox\" onclick=\"handleCheckboxClick(event, 'sun2024realtime', this)\">\n    <div class=\"paper-number\"></div>\n    <div class=\"paper-thumbnail\">\n      <img data-src=\"assets/thumbnails/sun2024realtime.jpg\" data-fallback=\"None\" alt=\"Paper thumbnail for Real-time Free-view Human Rendering from Sparse-view RGB Videos using Double Unprojected Textures\" class=\"lazy\" loading=\"lazy\"/>\n    </div>\n    <div class=\"paper-content\">\n      <h2 class=\"paper-title\">Real-time Free-view Human Rendering from Sparse-view RGB Videos using Double Unprojected Textures <span class=\"paper-year\">(2024)</span></h2>\n      <p class=\"paper-authors\">Guoxing Sun, Rishabh Dabral, Heming Zhu, Pascal Fua, Christian Theobalt, Marc Habermann</p>\n      <div class=\"paper-tags\"><span class=\"paper-tag\">Avatar</span>\n<span class=\"paper-tag\">Project</span>\n<span class=\"paper-tag\">Sparse</span>\n<span class=\"paper-tag\">Texturing</span>\n<span class=\"paper-tag\">Video</span></div>\n      <div class=\"paper-links\"><a href=\"https://arxiv.org/pdf/2412.13183v1.pdf\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">📄 Paper</a>\n<a href=\"https://vcai.mpi-inf.mpg.de/projects/DUT/\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">🌐 Project</a>\n<a href=\"https://vcai.mpi-inf.mpg.de/projects/DUT/videos/main_video.mp4\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">🎥 Video</a>\n<button class=\"abstract-toggle\" onclick=\"toggleAbstract(this)\">📖 Show Abstract</button>\n<div class=\"paper-abstract\">Real-time free-view human rendering from sparse-view RGB inputs is a challenging task due to the sensor scarcity and the tight time budget. To ensure efficiency, recent methods leverage 2D CNNs operating in texture space to learn rendering primitives. However, they either jointly learn geometry and appearance, or completely ignore sparse image information for geometry estimation, significantly harming visual quality and robustness to unseen body poses. To address these issues, we present Double Unprojected Textures, which at the core disentangles coarse geometric deformation estimation from appearance synthesis, enabling robust and photorealistic 4K rendering in real-time. Specifically, we first introduce a novel image-conditioned template deformation network, which estimates the coarse deformation of the human template from a first unprojected texture. This updated geometry is then used to apply a second and more accurate texture unprojection. The resulting texture map has fewer artifacts and better alignment with input views, which benefits our learning of finer-level geometry and appearance represented by Gaussian splats. We validate the effectiveness and efficiency of the proposed method in quantitative and qualitative experiments, which significantly surpasses other state-of-the-art methods.\n</div></div>\n    </div>\n  </div>\n</div>\n<div class=\"paper-row\" data-id=\"weiss2024gaussian\" data-title=\"Gaussian Billboards: Expressive 2D Gaussian Splatting with Textures\" data-authors=\"Sebastian Weiss, Derek Bradley\" data-year=\"2024\" data-tags='[\"2DGS\", \"Texturing\"]'>\n  <div class=\"paper-card\">\n    <input type=\"checkbox\" class=\"selection-checkbox\" onclick=\"handleCheckboxClick(event, 'weiss2024gaussian', this)\">\n    <div class=\"paper-number\"></div>\n    <div class=\"paper-thumbnail\">\n      <img data-src=\"assets/thumbnails/weiss2024gaussian.jpg\" data-fallback=\"None\" alt=\"Paper thumbnail for Gaussian Billboards: Expressive 2D Gaussian Splatting with Textures\" class=\"lazy\" loading=\"lazy\"/>\n    </div>\n    <div class=\"paper-content\">\n      <h2 class=\"paper-title\">Gaussian Billboards: Expressive 2D Gaussian Splatting with Textures <span class=\"paper-year\">(2024)</span></h2>\n      <p class=\"paper-authors\">Sebastian Weiss, Derek Bradley</p>\n      <div class=\"paper-tags\"><span class=\"paper-tag\">2DGS</span>\n<span class=\"paper-tag\">Texturing</span></div>\n      <div class=\"paper-links\"><a href=\"https://arxiv.org/pdf/2412.12734v1.pdf\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">📄 Paper</a>\n<button class=\"abstract-toggle\" onclick=\"toggleAbstract(this)\">📖 Show Abstract</button>\n<div class=\"paper-abstract\">Gaussian Splatting has recently emerged as the go-to representation for reconstructing and rendering 3D scenes. The transition from 3D to 2D Gaussian primitives has further improved multi-view consistency and surface reconstruction accuracy. In this work we highlight the similarity between 2D Gaussian Splatting (2DGS) and billboards from traditional computer graphics. Both use flat semi-transparent 2D geometry that is positioned, oriented and scaled in 3D space. However 2DGS uses a solid color per splat and an opacity modulated by a Gaussian distribution, where billboards are more expressive, modulating the color with a uv-parameterized texture. We propose to unify these concepts by presenting Gaussian Billboards, a modification of 2DGS to add spatially-varying color achieved using per-splat texture interpolation. The result is a mixture of the two representations, which benefits from both the robust scene optimization power of 2DGS and the expressiveness of texture mapping. We show that our method can improve the sharpness and quality of the scene representation in a wide range of qualitative and quantitative evaluations compared to the original 2DGS implementation.\n</div></div>\n    </div>\n  </div>\n</div>\n<div class=\"paper-row\" data-id=\"murai2024mast3rslam\" data-title=\"MASt3R-SLAM: Real-Time Dense SLAM with 3D Reconstruction Priors\" data-authors=\"Riku Murai, Eric Dexheimer, Andrew J. Davison\" data-year=\"2024\" data-tags='[\"3ster-based\", \"SLAM\", \"Video\"]'>\n  <div class=\"paper-card\">\n    <input type=\"checkbox\" class=\"selection-checkbox\" onclick=\"handleCheckboxClick(event, 'murai2024mast3rslam', this)\">\n    <div class=\"paper-number\"></div>\n    <div class=\"paper-thumbnail\">\n      <img data-src=\"assets/thumbnails/murai2024mast3rslam.jpg\" data-fallback=\"None\" alt=\"Paper thumbnail for MASt3R-SLAM: Real-Time Dense SLAM with 3D Reconstruction Priors\" class=\"lazy\" loading=\"lazy\"/>\n    </div>\n    <div class=\"paper-content\">\n      <h2 class=\"paper-title\">MASt3R-SLAM: Real-Time Dense SLAM with 3D Reconstruction Priors <span class=\"paper-year\">(2024)</span></h2>\n      <p class=\"paper-authors\">Riku Murai, Eric Dexheimer, Andrew J. Davison</p>\n      <div class=\"paper-tags\"><span class=\"paper-tag\">3ster-based</span>\n<span class=\"paper-tag\">SLAM</span>\n<span class=\"paper-tag\">Video</span></div>\n      <div class=\"paper-links\"><a href=\"https://arxiv.org/pdf/2412.12392.pdf\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">📄 Paper</a>\n<a href=\"https://www.youtube.com/watch?v=wozt71NBFTQ\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">🎥 Video</a>\n<button class=\"abstract-toggle\" onclick=\"toggleAbstract(this)\">📖 Show Abstract</button>\n<div class=\"paper-abstract\">We present a real-time monocular dense SLAM system designed bottom-up from MASt3R, a two-view 3D reconstruction and matching prior. Equipped with this strong prior, our system is robust on in-the-wild video sequences despite making no assumption on a fixed or parametric camera model beyond a unique camera centre. We introduce efficient methods for pointmap matching, camera tracking and local fusion, graph construction and loop closure, and second-order global optimisation. With known calibration, a simple modification to the system achieves state-of-the-art performance across various benchmarks. Altogether, we propose a plug-and-play monocular SLAM system capable of producing globally-consistent poses and dense geometry while operating at 15 FPS.\n</div></div>\n    </div>\n  </div>\n</div>\n<div class=\"paper-row\" data-id=\"zhang2024pansplat\" data-title=\"PanSplat: 4K Panorama Synthesis with Feed-Forward Gaussian Splatting\" data-authors=\"Cheng Zhang, Haofei Xu, Qianyi Wu, Camilo Cruz Gambardella, Dinh Phung, Jianfei Cai\" data-year=\"2024\" data-tags='[\"360 degree\", \"Feed-Forward\", \"Project\", \"Video\", \"World Generation\"]'>\n  <div class=\"paper-card\">\n    <input type=\"checkbox\" class=\"selection-checkbox\" onclick=\"handleCheckboxClick(event, 'zhang2024pansplat', this)\">\n    <div class=\"paper-number\"></div>\n    <div class=\"paper-thumbnail\">\n      <img data-src=\"assets/thumbnails/zhang2024pansplat.jpg\" data-fallback=\"None\" alt=\"Paper thumbnail for PanSplat: 4K Panorama Synthesis with Feed-Forward Gaussian Splatting\" class=\"lazy\" loading=\"lazy\"/>\n    </div>\n    <div class=\"paper-content\">\n      <h2 class=\"paper-title\">PanSplat: 4K Panorama Synthesis with Feed-Forward Gaussian Splatting <span class=\"paper-year\">(2024)</span></h2>\n      <p class=\"paper-authors\">Cheng Zhang, Haofei Xu, Qianyi Wu, Camilo Cruz Gambardella, Dinh Phung, Jianfei Cai</p>\n      <div class=\"paper-tags\"><span class=\"paper-tag\">360 degree</span>\n<span class=\"paper-tag\">Feed-Forward</span>\n<span class=\"paper-tag\">Project</span>\n<span class=\"paper-tag\">Video</span>\n<span class=\"paper-tag\">World Generation</span></div>\n      <div class=\"paper-links\"><a href=\"https://arxiv.org/pdf/2412.12096v1.pdf\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">📄 Paper</a>\n<a href=\"https://chengzhag.github.io/publication/pansplat/\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">🌐 Project</a>\n<a href=\"https://youtu.be/R3qIzL77ZSc\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">🎥 Video</a>\n<button class=\"abstract-toggle\" onclick=\"toggleAbstract(this)\">📖 Show Abstract</button>\n<div class=\"paper-abstract\">With the advent of portable 360{\\deg} cameras, panorama has gained significant attention in applications like virtual reality (VR), virtual tours, robotics, and autonomous driving. As a result, wide-baseline panorama view synthesis has emerged as a vital task, where high resolution, fast inference, and memory efficiency are essential. Nevertheless, existing methods are typically constrained to lower resolutions (512 $\\times$ 1024) due to demanding memory and computational requirements. In this paper, we present PanSplat, a generalizable, feed-forward approach that efficiently supports resolution up to 4K (2048 $\\times$ 4096). Our approach features a tailored spherical 3D Gaussian pyramid with a Fibonacci lattice arrangement, enhancing image quality while reducing information redundancy. To accommodate the demands of high resolution, we propose a pipeline that integrates a hierarchical spherical cost volume and Gaussian heads with local operations, enabling two-step deferred backpropagation for memory-efficient training on a single A100 GPU. Experiments demonstrate that PanSplat achieves state-of-the-art results with superior efficiency and image quality across both synthetic and real-world datasets. Code will be available at \\url{https://github.com/chengzhag/PanSplat}.\n</div></div>\n    </div>\n  </div>\n</div>\n<div class=\"paper-row\" data-id=\"taubner2024cap4d\" data-title=\"CAP4D: Creating Animatable 4D Portrait Avatars with Morphable Multi-View Diffusion Models\" data-authors=\"Felix Taubner, Ruihang Zhang, Mathieu Tuli, David B. Lindell\" data-year=\"2024\" data-tags='[\"Avatar\", \"Project\"]'>\n  <div class=\"paper-card\">\n    <input type=\"checkbox\" class=\"selection-checkbox\" onclick=\"handleCheckboxClick(event, 'taubner2024cap4d', this)\">\n    <div class=\"paper-number\"></div>\n    <div class=\"paper-thumbnail\">\n      <img data-src=\"assets/thumbnails/taubner2024cap4d.jpg\" data-fallback=\"None\" alt=\"Paper thumbnail for CAP4D: Creating Animatable 4D Portrait Avatars with Morphable Multi-View Diffusion Models\" class=\"lazy\" loading=\"lazy\"/>\n    </div>\n    <div class=\"paper-content\">\n      <h2 class=\"paper-title\">CAP4D: Creating Animatable 4D Portrait Avatars with Morphable Multi-View Diffusion Models <span class=\"paper-year\">(2024)</span></h2>\n      <p class=\"paper-authors\">Felix Taubner, Ruihang Zhang, Mathieu Tuli, David B. Lindell</p>\n      <div class=\"paper-tags\"><span class=\"paper-tag\">Avatar</span>\n<span class=\"paper-tag\">Project</span></div>\n      <div class=\"paper-links\"><a href=\"https://arxiv.org/pdf/2412.12093\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">📄 Paper</a>\n<a href=\"https://felixtaubner.github.io/cap4d/\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">🌐 Project</a>\n<button class=\"abstract-toggle\" onclick=\"toggleAbstract(this)\">📖 Show Abstract</button>\n<div class=\"paper-abstract\">Reconstructing photorealistic and dynamic portrait avatars from images is essential to many applications including advertising, visual effects, and virtual reality. Depending on the application, avatar reconstruction involves different capture setups and constraints − for example, visual effects studios use camera arrays to capture hundreds of reference images, while content creators may seek to animate a single portrait image downloaded from the internet. As such, there is a large and heterogeneous ecosystem of methods for avatar reconstruction. Techniques based on multi-view stereo or neural rendering achieve the highest quality results, but require hundreds of reference images. Recent generative models produce convincing avatars from a single reference image, but visual fidelity yet lags behind multi-view techniques. Here, we present CAP4D: an approach that uses a morphable multi-view diffusion model to reconstruct photoreal 4D (dynamic 3D) portrait avatars from any number of reference images (i.e., one to 100) and animate and render them in real time. Our approach demonstrates state-of-the-art performance for single-, few-, and multi-image 4D portrait avatar reconstruction, and takes steps to bridge the gap in visual fidelity between single-image and multi-view reconstruction techniques.</div></div>\n    </div>\n  </div>\n</div>\n<div class=\"paper-row\" data-id=\"liang2024wonderland\" data-title=\"Wonderland: Navigating 3D Scenes from a Single Image\" data-authors=\"Hanwen Liang, Junli Cao, Vidit Goel, Guocheng Qian, Sergei Korolev, Demetri Terzopoulos, Konstantinos N. Plataniotis, Sergey Tulyakov, Jian Ren\" data-year=\"2024\" data-tags='[\"Feed-Forward\", \"Project\", \"Sparse\", \"World Generation\"]'>\n  <div class=\"paper-card\">\n    <input type=\"checkbox\" class=\"selection-checkbox\" onclick=\"handleCheckboxClick(event, 'liang2024wonderland', this)\">\n    <div class=\"paper-number\"></div>\n    <div class=\"paper-thumbnail\">\n      <img data-src=\"assets/thumbnails/liang2024wonderland.jpg\" data-fallback=\"None\" alt=\"Paper thumbnail for Wonderland: Navigating 3D Scenes from a Single Image\" class=\"lazy\" loading=\"lazy\"/>\n    </div>\n    <div class=\"paper-content\">\n      <h2 class=\"paper-title\">Wonderland: Navigating 3D Scenes from a Single Image <span class=\"paper-year\">(2024)</span></h2>\n      <p class=\"paper-authors\">Hanwen Liang, Junli Cao, Vidit Goel, Guocheng Qian, Sergei Korolev, Demetri Terzopoulos, Konstantinos N. Plataniotis, Sergey Tulyakov, Jian Ren</p>\n      <div class=\"paper-tags\"><span class=\"paper-tag\">Feed-Forward</span>\n<span class=\"paper-tag\">Project</span>\n<span class=\"paper-tag\">Sparse</span>\n<span class=\"paper-tag\">World Generation</span></div>\n      <div class=\"paper-links\"><a href=\"https://arxiv.org/pdf/2412.12091v1.pdf\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">📄 Paper</a>\n<a href=\"https://snap-research.github.io/wonderland/\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">🌐 Project</a>\n<button class=\"abstract-toggle\" onclick=\"toggleAbstract(this)\">📖 Show Abstract</button>\n<div class=\"paper-abstract\">This paper addresses a challenging question: How can we efficiently create high-quality, wide-scope 3D scenes from a single arbitrary image? Existing methods face several constraints, such as requiring multi-view data, time-consuming per-scene optimization, low visual quality in backgrounds, and distorted reconstructions in unseen areas. We propose a novel pipeline to overcome these limitations. Specifically, we introduce a large-scale reconstruction model that uses latents from a video diffusion model to predict 3D Gaussian Splattings for the scenes in a feed-forward manner. The video diffusion model is designed to create videos precisely following specified camera trajectories, allowing it to generate compressed video latents that contain multi-view information while maintaining 3D consistency. We train the 3D reconstruction model to operate on the video latent space with a progressive training strategy, enabling the efficient generation of high-quality, wide-scope, and generic 3D scenes. Extensive evaluations across various datasets demonstrate that our model significantly outperforms existing methods for single-view 3D scene generation, particularly with out-of-domain images. For the first time, we demonstrate that a 3D reconstruction model can be effectively built upon the latent space of a diffusion model to realize efficient 3D scene generation.\n</div></div>\n    </div>\n  </div>\n</div>\n<div class=\"paper-row\" data-id=\"huang2024deformable\" data-title=\"Deformable Radial Kernel Splatting\" data-authors=\"Yi-Hua Huang, Ming-Xian Lin, Yang-Tian Sun, Ziyi Yang, Xiaoyang Lyu, Yan-Pei Cao, Xiaojuan Qi\" data-year=\"2024\" data-tags='[\"Optimization\", \"Project\", \"Rendering\"]'>\n  <div class=\"paper-card\">\n    <input type=\"checkbox\" class=\"selection-checkbox\" onclick=\"handleCheckboxClick(event, 'huang2024deformable', this)\">\n    <div class=\"paper-number\"></div>\n    <div class=\"paper-thumbnail\">\n      <img data-src=\"assets/thumbnails/huang2024deformable.jpg\" data-fallback=\"None\" alt=\"Paper thumbnail for Deformable Radial Kernel Splatting\" class=\"lazy\" loading=\"lazy\"/>\n    </div>\n    <div class=\"paper-content\">\n      <h2 class=\"paper-title\">Deformable Radial Kernel Splatting <span class=\"paper-year\">(2024)</span></h2>\n      <p class=\"paper-authors\">Yi-Hua Huang, Ming-Xian Lin, Yang-Tian Sun, Ziyi Yang, Xiaoyang Lyu, Yan-Pei Cao, Xiaojuan Qi</p>\n      <div class=\"paper-tags\"><span class=\"paper-tag\">Optimization</span>\n<span class=\"paper-tag\">Project</span>\n<span class=\"paper-tag\">Rendering</span></div>\n      <div class=\"paper-links\"><a href=\"https://arxiv.org/pdf/2412.11752v1.pdf\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">📄 Paper</a>\n<a href=\"https://yihua7.github.io/DRK-web/\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">🌐 Project</a>\n<button class=\"abstract-toggle\" onclick=\"toggleAbstract(this)\">📖 Show Abstract</button>\n<div class=\"paper-abstract\">Recently, Gaussian splatting has emerged as a robust technique for representing 3D scenes, enabling real-time rasterization and high-fidelity rendering. However, Gaussians' inherent radial symmetry and smoothness constraints limit their ability to represent complex shapes, often requiring thousands of primitives to approximate detailed geometry. We introduce Deformable Radial Kernel (DRK), which extends Gaussian splatting into a more general and flexible framework. Through learnable radial bases with adjustable angles and scales, DRK efficiently models diverse shape primitives while enabling precise control over edge sharpness and boundary curvature. iven DRK's planar nature, we further develop accurate ray-primitive intersection computation for depth sorting and introduce efficient kernel culling strategies for improved rasterization efficiency. Extensive experiments demonstrate that DRK outperforms existing methods in both representation efficiency and rendering quality, achieving state-of-the-art performance while dramatically reducing primitive count.\n</div></div>\n    </div>\n  </div>\n</div>\n<div class=\"paper-row\" data-id=\"liang2024supergseg\" data-title=\"SuperGSeg: Open-Vocabulary 3D Segmentation with Structured Super-Gaussians\" data-authors=\"Siyun Liang, Sen Wang, Kunyi Li, Michael Niemeyer, Stefano Gasperini, Nassir Navab, Federico Tombari\" data-year=\"2024\" data-tags='[\"Language Embedding\", \"Project\", \"Segmentation\"]'>\n  <div class=\"paper-card\">\n    <input type=\"checkbox\" class=\"selection-checkbox\" onclick=\"handleCheckboxClick(event, 'liang2024supergseg', this)\">\n    <div class=\"paper-number\"></div>\n    <div class=\"paper-thumbnail\">\n      <img data-src=\"assets/thumbnails/liang2024supergseg.jpg\" data-fallback=\"None\" alt=\"Paper thumbnail for SuperGSeg: Open-Vocabulary 3D Segmentation with Structured Super-Gaussians\" class=\"lazy\" loading=\"lazy\"/>\n    </div>\n    <div class=\"paper-content\">\n      <h2 class=\"paper-title\">SuperGSeg: Open-Vocabulary 3D Segmentation with Structured Super-Gaussians <span class=\"paper-year\">(2024)</span></h2>\n      <p class=\"paper-authors\">Siyun Liang, Sen Wang, Kunyi Li, Michael Niemeyer, Stefano Gasperini, Nassir Navab, Federico Tombari</p>\n      <div class=\"paper-tags\"><span class=\"paper-tag\">Language Embedding</span>\n<span class=\"paper-tag\">Project</span>\n<span class=\"paper-tag\">Segmentation</span></div>\n      <div class=\"paper-links\"><a href=\"https://arxiv.org/pdf/2412.10231.pdf\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">📄 Paper</a>\n<a href=\"https://supergseg.github.io/\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">🌐 Project</a>\n<button class=\"abstract-toggle\" onclick=\"toggleAbstract(this)\">📖 Show Abstract</button>\n<div class=\"paper-abstract\">3D Gaussian Splatting has recently gained traction for its efficient training and real-time rendering. While the vanilla Gaussian Splatting representation is mainly designed for view synthesis, more recent works investigated how to extend it with scene understanding and language features. However, existing methods lack a detailed comprehension of scenes, limiting their ability to segment and interpret complex structures. To this end, We introduce SuperGSeg, a novel approach that fosters cohesive, context-aware scene representation by disentangling segmentation and language field distillation. SuperGSeg first employs neural Gaussians to learn instance and hierarchical segmentation features from multi-view images with the aid of off-the-shelf 2D masks. These features are then leveraged to create a sparse set of what we call Super-Gaussians. Super-Gaussians facilitate the distillation of 2D language features into 3D space. Through Super-Gaussians, our method enables high-dimensional language feature rendering without extreme increases in GPU memory. Extensive experiments demonstrate that SuperGSeg outperforms prior works on both open-vocabulary object localization and semantic segmentation tasks.\n</div></div>\n    </div>\n  </div>\n</div>\n<div class=\"paper-row\" data-id=\"tang2024gaf\" data-title=\"GAF: Gaussian Avatar Reconstruction from Monocular Videos via Multi-view Diffusion\" data-authors=\"Jiapeng Tang, Davide Davoli, Tobias Kirschstein, Liam Schoneveld, Matthias Nießner\" data-year=\"2024\" data-tags='[\"Avatar\", \"Project\", \"Video\"]'>\n  <div class=\"paper-card\">\n    <input type=\"checkbox\" class=\"selection-checkbox\" onclick=\"handleCheckboxClick(event, 'tang2024gaf', this)\">\n    <div class=\"paper-number\"></div>\n    <div class=\"paper-thumbnail\">\n      <img data-src=\"assets/thumbnails/tang2024gaf.jpg\" data-fallback=\"None\" alt=\"Paper thumbnail for GAF: Gaussian Avatar Reconstruction from Monocular Videos via Multi-view Diffusion\" class=\"lazy\" loading=\"lazy\"/>\n    </div>\n    <div class=\"paper-content\">\n      <h2 class=\"paper-title\">GAF: Gaussian Avatar Reconstruction from Monocular Videos via Multi-view Diffusion <span class=\"paper-year\">(2024)</span></h2>\n      <p class=\"paper-authors\">Jiapeng Tang, Davide Davoli, Tobias Kirschstein, Liam Schoneveld, Matthias Nießner</p>\n      <div class=\"paper-tags\"><span class=\"paper-tag\">Avatar</span>\n<span class=\"paper-tag\">Project</span>\n<span class=\"paper-tag\">Video</span></div>\n      <div class=\"paper-links\"><a href=\"https://arxiv.org/pdf/2412.10209\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">📄 Paper</a>\n<a href=\"https://tangjiapeng.github.io/projects/GAF/\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">🌐 Project</a>\n<a href=\"https://www.youtube.com/embed/QuIYTljvhygE\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">🎥 Video</a>\n<button class=\"abstract-toggle\" onclick=\"toggleAbstract(this)\">📖 Show Abstract</button>\n<div class=\"paper-abstract\">We propose a novel approach for reconstructing animatable 3D Gaussian avatars from monocular videos captured by commodity devices like smartphones. Photorealistic 3D head avatar reconstruction from such recordings is challenging due to limited observations, which leaves unobserved regions under-constrained and can lead to artifacts in novel views. To address this problem, we introduce a multi-view head diffusion model, leveraging its priors to fill in missing regions and ensure view consistency in Gaussian splatting renderings. To enable precise viewpoint control, we use normal maps rendered from FLAME-based head reconstruction, which provides pixel-aligned inductive biases. We also condition the diffusion model on VAE features extracted from the input image to preserve details of facial identity and appearance. For Gaussian avatar reconstruction, we distill multi-view diffusion priors by using iteratively denoised images as pseudo-ground truths, effectively mitigating over-saturation issues. To further improve photorealism, we apply latent upsampling to refine the denoised latent before decoding it into an image. We evaluate our method on the NeRSemble dataset, showing that GAF outperforms the previous state-of-the-art methods in novel view synthesis and novel expression animation. Furthermore, we demonstrate higher-fidelity avatar reconstructions from monocular videos captured on commodity devices.</div></div>\n    </div>\n  </div>\n</div>\n<div class=\"paper-row\" data-id=\"park2024splinegs\" data-title=\"SplineGS: Robust Motion-Adaptive Spline for Real-Time Dynamic 3D Gaussians from Monocular Video\" data-authors=\"Jongmin Park, Minh-Quan Viet Bui, Juan Luis Gonzalez Bello, Jaeho Moon, Jihyong Oh, Munchurl Kim\" data-year=\"2024\" data-tags='[\"Dynamic\", \"Monocular\", \"Project\"]'>\n  <div class=\"paper-card\">\n    <input type=\"checkbox\" class=\"selection-checkbox\" onclick=\"handleCheckboxClick(event, 'park2024splinegs', this)\">\n    <div class=\"paper-number\"></div>\n    <div class=\"paper-thumbnail\">\n      <img data-src=\"assets/thumbnails/park2024splinegs.jpg\" data-fallback=\"None\" alt=\"Paper thumbnail for SplineGS: Robust Motion-Adaptive Spline for Real-Time Dynamic 3D Gaussians from Monocular Video\" class=\"lazy\" loading=\"lazy\"/>\n    </div>\n    <div class=\"paper-content\">\n      <h2 class=\"paper-title\">SplineGS: Robust Motion-Adaptive Spline for Real-Time Dynamic 3D Gaussians from Monocular Video <span class=\"paper-year\">(2024)</span></h2>\n      <p class=\"paper-authors\">Jongmin Park, Minh-Quan Viet Bui, Juan Luis Gonzalez Bello, Jaeho Moon, Jihyong Oh, Munchurl Kim</p>\n      <div class=\"paper-tags\"><span class=\"paper-tag\">Dynamic</span>\n<span class=\"paper-tag\">Monocular</span>\n<span class=\"paper-tag\">Project</span></div>\n      <div class=\"paper-links\"><a href=\"https://arxiv.org/pdf/2412.09982v1.pdf\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">📄 Paper</a>\n<a href=\"https://kaist-viclab.github.io/splinegs-site/\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">🌐 Project</a>\n<button class=\"abstract-toggle\" onclick=\"toggleAbstract(this)\">📖 Show Abstract</button>\n<div class=\"paper-abstract\">Synthesizing novel views from in-the-wild monocular videos is challenging due to scene dynamics and the lack of multi-view cues. To address this, we propose SplineGS, a COLMAP-free dynamic 3D Gaussian Splatting (3DGS) framework for high-quality reconstruction and fast rendering from monocular videos. At its core is a novel Motion-Adaptive Spline (MAS) method, which represents continuous dynamic 3D Gaussian trajectories using cubic Hermite splines with a small number of control points. For MAS, we introduce a Motion-Adaptive Control points Pruning (MACP) method to model the deformation of each dynamic 3D Gaussian across varying motions, progressively pruning control points while maintaining dynamic modeling integrity. Additionally, we present a joint optimization strategy for camera parameter estimation and 3D Gaussian attributes, leveraging photometric and geometric consistency. This eliminates the need for Structure-from-Motion preprocessing and enhances SplineGS's robustness in real-world conditions. Experiments show that SplineGS significantly outperforms state-of-the-art methods in novel view synthesis quality for dynamic scenes from monocular videos, achieving thousands times faster rendering speed.\n</div></div>\n    </div>\n  </div>\n</div>\n<div class=\"paper-row\" data-id=\"xu2024representing\" data-title=\"Representing Long Volumetric Video with Temporal Gaussian Hierarchy\" data-authors=\"Zhen Xu, Yinghao Xu, Zhiyuan Yu, Sida Peng, Jiaming Sun, Hujun Bao, Xiaowei Zhou\" data-year=\"2024\" data-tags='[\"Acceleration\", \"Dynamic\", \"Project\", \"Video\"]'>\n  <div class=\"paper-card\">\n    <input type=\"checkbox\" class=\"selection-checkbox\" onclick=\"handleCheckboxClick(event, 'xu2024representing', this)\">\n    <div class=\"paper-number\"></div>\n    <div class=\"paper-thumbnail\">\n      <img data-src=\"assets/thumbnails/xu2024representing.jpg\" data-fallback=\"None\" alt=\"Paper thumbnail for Representing Long Volumetric Video with Temporal Gaussian Hierarchy\" class=\"lazy\" loading=\"lazy\"/>\n    </div>\n    <div class=\"paper-content\">\n      <h2 class=\"paper-title\">Representing Long Volumetric Video with Temporal Gaussian Hierarchy <span class=\"paper-year\">(2024)</span></h2>\n      <p class=\"paper-authors\">Zhen Xu, Yinghao Xu, Zhiyuan Yu, Sida Peng, Jiaming Sun, Hujun Bao, Xiaowei Zhou</p>\n      <div class=\"paper-tags\"><span class=\"paper-tag\">Acceleration</span>\n<span class=\"paper-tag\">Dynamic</span>\n<span class=\"paper-tag\">Project</span>\n<span class=\"paper-tag\">Video</span></div>\n      <div class=\"paper-links\"><a href=\"https://arxiv.org/pdf/2412.09608.pdf\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">📄 Paper</a>\n<a href=\"https://zju3dv.github.io/longvolcap/\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">🌐 Project</a>\n<a href=\"https://www.youtube.com/watch?v=y7e0YRNNmXw&feature=youtu.be\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">🎥 Video</a>\n<button class=\"abstract-toggle\" onclick=\"toggleAbstract(this)\">📖 Show Abstract</button>\n<div class=\"paper-abstract\">This paper aims to address the challenge of reconstructing long volumetric videos from multi-view RGB videos. Recent dynamic view synthesis methods leverage powerful 4D representations, like feature grids or point cloud sequences, to achieve high-quality rendering results. However, they are typically limited to short (1~2s) video clips and often suffer from large memory footprints when dealing with longer videos. To solve this issue, we propose a novel 4D representation, named Temporal Gaussian Hierarchy, to compactly model long volumetric videos. Our key observation is that there are generally various degrees of temporal redundancy in dynamic scenes, which consist of areas changing at different speeds. Motivated by this, our approach builds a multi-level hierarchy of 4D Gaussian primitives, where each level separately describes scene regions with different degrees of content change, and adaptively shares Gaussian primitives to represent unchanged scene content over different temporal segments, thus effectively reducing the number of Gaussian primitives. In addition, the tree-like structure of the Gaussian hierarchy allows us to efficiently represent the scene at a particular moment with a subset of Gaussian primitives, leading to nearly constant GPU memory usage during the training or rendering regardless of the video length. Extensive experimental results demonstrate the superiority of our method over alternative methods in terms of training cost, rendering speed, and storage usage. To our knowledge, this work is the first approach capable of efficiently handling minutes of volumetric video data while maintaining state-of-the-art rendering quality. Our project page is available at: https://zju3dv.github.io/longvolcap.\n</div></div>\n    </div>\n  </div>\n</div>\n<div class=\"paper-row\" data-id=\"chen2024feat2gs\" data-title=\"Feat2GS: Probing Visual Foundation Models with Gaussian Splatting\" data-authors=\"Yue Chen, Xingyu Chen, Anpei Chen, Gerard Pons-Moll, Yuliang Xiu\" data-year=\"2024\" data-tags='[\"Project\", \"Rendering\", \"Video\", \"World Generation\"]'>\n  <div class=\"paper-card\">\n    <input type=\"checkbox\" class=\"selection-checkbox\" onclick=\"handleCheckboxClick(event, 'chen2024feat2gs', this)\">\n    <div class=\"paper-number\"></div>\n    <div class=\"paper-thumbnail\">\n      <img data-src=\"assets/thumbnails/chen2024feat2gs.jpg\" data-fallback=\"None\" alt=\"Paper thumbnail for Feat2GS: Probing Visual Foundation Models with Gaussian Splatting\" class=\"lazy\" loading=\"lazy\"/>\n    </div>\n    <div class=\"paper-content\">\n      <h2 class=\"paper-title\">Feat2GS: Probing Visual Foundation Models with Gaussian Splatting <span class=\"paper-year\">(2024)</span></h2>\n      <p class=\"paper-authors\">Yue Chen, Xingyu Chen, Anpei Chen, Gerard Pons-Moll, Yuliang Xiu</p>\n      <div class=\"paper-tags\"><span class=\"paper-tag\">Project</span>\n<span class=\"paper-tag\">Rendering</span>\n<span class=\"paper-tag\">Video</span>\n<span class=\"paper-tag\">World Generation</span></div>\n      <div class=\"paper-links\"><a href=\"https://arxiv.org/pdf/2412.09606.pdf\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">📄 Paper</a>\n<a href=\"https://fanegg.github.io/Feat2GS/\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">🌐 Project</a>\n<a href=\"https://youtu.be/4fT5lzcAJqo\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">🎥 Video</a>\n<button class=\"abstract-toggle\" onclick=\"toggleAbstract(this)\">📖 Show Abstract</button>\n<div class=\"paper-abstract\">Given that visual foundation models (VFMs) are trained on extensive datasets but often limited to 2D images, a natural question arises: how well do they understand the 3D world? With the differences in architecture and training protocols (i.e., objectives, proxy tasks), a unified framework to fairly and comprehensively probe their 3D awareness is urgently needed. Existing works on 3D probing suggest single-view 2.5D estimation (e.g., depth and normal) or two-view sparse 2D correspondence (e.g., matching and tracking). Unfortunately, these tasks ignore texture awareness, and require 3D data as ground-truth, which limits the scale and diversity of their evaluation set. To address these issues, we introduce Feat2GS, which readout 3D Gaussians attributes from VFM features extracted from unposed images. This allows us to probe 3D awareness for geometry and texture via novel view synthesis, without requiring 3D data. Additionally, the disentanglement of 3DGS parameters - geometry ($\\boldsymbol{x}, \\alpha, \\Sigma$) and texture ($\\boldsymbol{c}$) - enables separate analysis of texture and geometry awareness. Under Feat2GS, we conduct extensive experiments to probe the 3D awareness of several VFMs, and investigate the ingredients that lead to a 3D aware VFM. Building on these findings, we develop several variants that achieve state-of-the-art across diverse datasets. This makes Feat2GS useful for probing VFMs, and as a simple-yet-effective baseline for novel-view synthesis.\n</div></div>\n    </div>\n  </div>\n</div>\n<div class=\"paper-row\" data-id=\"li2024simavatar\" data-title=\"SimAvatar: Simulation-Ready Avatars with Layered Hair and Clothing\" data-authors=\"Xueting Li, Ye Yuan, Shalini De Mello, Gilles Daviet, Jonathan Leaf, Miles Macklin, Jan Kautz, Umar Iqbal\" data-year=\"2024\" data-tags='[\"Avatar\", \"Diffusion\", \"Language Embedding\", \"Project\", \"Video\"]'>\n  <div class=\"paper-card\">\n    <input type=\"checkbox\" class=\"selection-checkbox\" onclick=\"handleCheckboxClick(event, 'li2024simavatar', this)\">\n    <div class=\"paper-number\"></div>\n    <div class=\"paper-thumbnail\">\n      <img data-src=\"assets/thumbnails/li2024simavatar.jpg\" data-fallback=\"None\" alt=\"Paper thumbnail for SimAvatar: Simulation-Ready Avatars with Layered Hair and Clothing\" class=\"lazy\" loading=\"lazy\"/>\n    </div>\n    <div class=\"paper-content\">\n      <h2 class=\"paper-title\">SimAvatar: Simulation-Ready Avatars with Layered Hair and Clothing <span class=\"paper-year\">(2024)</span></h2>\n      <p class=\"paper-authors\">Xueting Li, Ye Yuan, Shalini De Mello, Gilles Daviet, Jonathan Leaf, Miles Macklin, Jan Kautz, Umar Iqbal</p>\n      <div class=\"paper-tags\"><span class=\"paper-tag\">Avatar</span>\n<span class=\"paper-tag\">Diffusion</span>\n<span class=\"paper-tag\">Language Embedding</span>\n<span class=\"paper-tag\">Project</span>\n<span class=\"paper-tag\">Video</span></div>\n      <div class=\"paper-links\"><a href=\"https://arxiv.org/pdf/2412.09545.pdf\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">📄 Paper</a>\n<a href=\"https://nvlabs.github.io/SimAvatar/\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">🌐 Project</a>\n<a href=\"https://www.youtube.com/watch?v=qEwBY7LBW2Y\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">🎥 Video</a>\n<button class=\"abstract-toggle\" onclick=\"toggleAbstract(this)\">📖 Show Abstract</button>\n<div class=\"paper-abstract\">We introduce SimAvatar, a framework designed to generate simulation-ready clothed 3D human avatars from a text prompt. Current text-driven human avatar generation methods either model hair, clothing, and the human body using a unified geometry or produce hair and garments that are not easily adaptable for simulation within existing simulation pipelines. The primary challenge lies in representing the hair and garment geometry in a way that allows leveraging established prior knowledge from foundational image diffusion models (e.g., Stable Diffusion) while being simulation-ready using either physics or neural simulators. To address this task, we propose a two-stage framework that combines the flexibility of 3D Gaussians with simulation-ready hair strands and garment meshes. Specifically, we first employ three text-conditioned 3D generative models to generate garment mesh, body shape and hair strands from the given text prompt. To leverage prior knowledge from foundational diffusion models, we attach 3D Gaussians to the body mesh, garment mesh, as well as hair strands and learn the avatar appearance through optimization. To drive the avatar given a pose sequence, we first apply physics simulators onto the garment meshes and hair strands. We then transfer the motion onto 3D Gaussians through carefully designed mechanisms for each body part. As a result, our synthesized avatars have vivid texture and realistic dynamic motion. To the best of our knowledge, our method is the first to produce highly realistic, fully simulation-ready 3D avatars, surpassing the capabilities of current approaches.\n</div></div>\n    </div>\n  </div>\n</div>\n<div class=\"paper-row\" data-id=\"liu2024slam3r\" data-title=\"SLAM3R: Real-Time Dense Scene Reconstruction from Monocular RGB Videos\" data-authors=\"Yuzheng Liu, Siyan Dong, Shuzhe Wang, Yanchao Yang, Qingnan Fan, Baoquan Chen\" data-year=\"2024\" data-tags='[\"3ster-based\", \"Code\", \"SLAM\"]'>\n  <div class=\"paper-card\">\n    <input type=\"checkbox\" class=\"selection-checkbox\" onclick=\"handleCheckboxClick(event, 'liu2024slam3r', this)\">\n    <div class=\"paper-number\"></div>\n    <div class=\"paper-thumbnail\">\n      <img data-src=\"assets/thumbnails/liu2024slam3r.jpg\" data-fallback=\"None\" alt=\"Paper thumbnail for SLAM3R: Real-Time Dense Scene Reconstruction from Monocular RGB Videos\" class=\"lazy\" loading=\"lazy\"/>\n    </div>\n    <div class=\"paper-content\">\n      <h2 class=\"paper-title\">SLAM3R: Real-Time Dense Scene Reconstruction from Monocular RGB Videos <span class=\"paper-year\">(2024)</span></h2>\n      <p class=\"paper-authors\">Yuzheng Liu, Siyan Dong, Shuzhe Wang, Yanchao Yang, Qingnan Fan, Baoquan Chen</p>\n      <div class=\"paper-tags\"><span class=\"paper-tag\">3ster-based</span>\n<span class=\"paper-tag\">Code</span>\n<span class=\"paper-tag\">SLAM</span></div>\n      <div class=\"paper-links\"><a href=\"https://arxiv.org/pdf/2412.09401.pdf\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">📄 Paper</a>\n<a href=\"https://github.com/PKU-VCL-3DV/SLAM3R\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">💻 Code</a>\n<button class=\"abstract-toggle\" onclick=\"toggleAbstract(this)\">📖 Show Abstract</button>\n<div class=\"paper-abstract\">In this paper, we introduce SLAM3R, a novel and effective monocular RGB SLAM system for real-time and high-quality dense 3D reconstruction. SLAM3R provides an end-to-end solution by seamlessly integrating local 3D reconstruction and global coordinate registration through feed-forward neural networks. Given an input video, the system first converts it into overlapping clips using a sliding window mechanism. Unlike traditional pose optimization-based methods, SLAM3R directly regresses 3D pointmaps from RGB images in each window and progressively aligns and deforms these local pointmaps to create a globally consistent scene reconstruction - all without explicitly solving any camera parameters. Experiments across datasets consistently show that SLAM3R achieves state-of-the-art reconstruction accuracy and completeness while maintaining real-time performance at 20+ FPS. Code and weights at: https://github.com/PKU-VCL-3DV/SLAM3R.\n</div></div>\n    </div>\n  </div>\n</div>\n<div class=\"paper-row\" data-id=\"gomel2024diffusionbased\" data-title=\"Diffusion-Based Attention Warping for Consistent 3D Scene Editing\" data-authors=\"Eyal Gomel, Lior Wolf\" data-year=\"2024\" data-tags='[\"Diffusion\", \"Project\", \"Style Transfer\"]'>\n  <div class=\"paper-card\">\n    <input type=\"checkbox\" class=\"selection-checkbox\" onclick=\"handleCheckboxClick(event, 'gomel2024diffusionbased', this)\">\n    <div class=\"paper-number\"></div>\n    <div class=\"paper-thumbnail\">\n      <img data-src=\"assets/thumbnails/gomel2024diffusionbased.jpg\" data-fallback=\"None\" alt=\"Paper thumbnail for Diffusion-Based Attention Warping for Consistent 3D Scene Editing\" class=\"lazy\" loading=\"lazy\"/>\n    </div>\n    <div class=\"paper-content\">\n      <h2 class=\"paper-title\">Diffusion-Based Attention Warping for Consistent 3D Scene Editing <span class=\"paper-year\">(2024)</span></h2>\n      <p class=\"paper-authors\">Eyal Gomel, Lior Wolf</p>\n      <div class=\"paper-tags\"><span class=\"paper-tag\">Diffusion</span>\n<span class=\"paper-tag\">Project</span>\n<span class=\"paper-tag\">Style Transfer</span></div>\n      <div class=\"paper-links\"><a href=\"https://arxiv.org/pdf/2412.07984.pdf\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">📄 Paper</a>\n<a href=\"https://attention-warp.github.io/\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">🌐 Project</a>\n<button class=\"abstract-toggle\" onclick=\"toggleAbstract(this)\">📖 Show Abstract</button>\n<div class=\"paper-abstract\">We present a novel method for 3D scene editing using diffusion models, designed to ensure view consistency and realism across perspectives. Our approach leverages attention features extracted from a single reference image to define the intended edits. These features are warped across multiple views by aligning them with scene geometry derived from Gaussian splatting depth estimates. Injecting these warped features into other viewpoints enables coherent propagation of edits, achieving high fidelity and spatial alignment in 3D space. Extensive evaluations demonstrate the effectiveness of our method in generating versatile edits of 3D scenes, significantly advancing the capabilities of scene manipulation compared to the existing methods. Project page: \\url{https://attention-warp.github.io}\n</div></div>\n    </div>\n  </div>\n</div>\n<div class=\"paper-row\" data-id=\"saunders2024gasp\" data-title=\"GASP: Gaussian Avatars with Synthetic Priors\" data-authors=\"Jack Saunders, Charlie Hewitt, Yanan Jian, Marek Kowalski, Tadas Baltrušaitis, Yiye Chen, Darren Cosker, Virginia Estellers, Nicholas Gyde, Vinay Namboodiri, Benjamin Lundell\" data-year=\"2024\" data-tags='[\"Avatar\", \"Project\", \"Video\"]'>\n  <div class=\"paper-card\">\n    <input type=\"checkbox\" class=\"selection-checkbox\" onclick=\"handleCheckboxClick(event, 'saunders2024gasp', this)\">\n    <div class=\"paper-number\"></div>\n    <div class=\"paper-thumbnail\">\n      <img data-src=\"assets/thumbnails/saunders2024gasp.jpg\" data-fallback=\"None\" alt=\"Paper thumbnail for GASP: Gaussian Avatars with Synthetic Priors\" class=\"lazy\" loading=\"lazy\"/>\n    </div>\n    <div class=\"paper-content\">\n      <h2 class=\"paper-title\">GASP: Gaussian Avatars with Synthetic Priors <span class=\"paper-year\">(2024)</span></h2>\n      <p class=\"paper-authors\">Jack Saunders, Charlie Hewitt, Yanan Jian, Marek Kowalski, Tadas Baltrušaitis, Yiye Chen, Darren Cosker, Virginia Estellers, Nicholas Gyde, Vinay Namboodiri, Benjamin Lundell</p>\n      <div class=\"paper-tags\"><span class=\"paper-tag\">Avatar</span>\n<span class=\"paper-tag\">Project</span>\n<span class=\"paper-tag\">Video</span></div>\n      <div class=\"paper-links\"><a href=\"https://arxiv.org/pdf/2412.07739\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">📄 Paper</a>\n<a href=\"https://microsoft.github.io/GASP/\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">🌐 Project</a>\n<a href=\"https://www.youtube.com/watch?v=3oWB7-UJUYE\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">🎥 Video</a>\n<button class=\"abstract-toggle\" onclick=\"toggleAbstract(this)\">📖 Show Abstract</button>\n<div class=\"paper-abstract\">Gaussian Splatting has changed the game for real-time photo-realistic rendering. One of the most popular applications of Gaussian Splatting is to create animatable avatars, known as Gaussian Avatars. Recent works have pushed the boundaries of quality and rendering efficiency but suffer from two main limitations. Either they require expensive multi-camera rigs to produce avatars with free-view rendering, or they can be trained with a single camera but only rendered at high quality from this fixed viewpoint. An ideal model would be trained using a short monocular video or image from available hardware, such as a webcam, and rendered from any view. To this end, we propose GASP: Gaussian Avatars with Synthetic Priors. To overcome the limitations of existing datasets, we exploit the pixel-perfect nature of synthetic data to train a Gaussian Avatar prior. By fitting this prior model to a single photo or video and fine-tuning it, we get a high-quality Gaussian Avatar, which supports 360-degree rendering. Our prior is only required for fitting, not inference, enabling real-time application. Through our method, we obtain high-quality, animatable Avatars from limited data which can be animated and rendered at 70fps on commercial hardware.</div></div>\n    </div>\n  </div>\n</div>\n<div class=\"paper-row\" data-id=\"wang2024faster\" data-title=\"Faster and Better 3D Splatting via Group Training\" data-authors=\"Chengbo Wang, Guozheng Ma, Yifei Xue, Yizhen Lao\" data-year=\"2024\" data-tags='[\"Acceleration\", \"Densification\", \"Optimization\"]'>\n  <div class=\"paper-card\">\n    <input type=\"checkbox\" class=\"selection-checkbox\" onclick=\"handleCheckboxClick(event, 'wang2024faster', this)\">\n    <div class=\"paper-number\"></div>\n    <div class=\"paper-thumbnail\">\n      <img data-src=\"assets/thumbnails/wang2024faster.jpg\" data-fallback=\"None\" alt=\"Paper thumbnail for Faster and Better 3D Splatting via Group Training\" class=\"lazy\" loading=\"lazy\"/>\n    </div>\n    <div class=\"paper-content\">\n      <h2 class=\"paper-title\">Faster and Better 3D Splatting via Group Training <span class=\"paper-year\">(2024)</span></h2>\n      <p class=\"paper-authors\">Chengbo Wang, Guozheng Ma, Yifei Xue, Yizhen Lao</p>\n      <div class=\"paper-tags\"><span class=\"paper-tag\">Acceleration</span>\n<span class=\"paper-tag\">Densification</span>\n<span class=\"paper-tag\">Optimization</span></div>\n      <div class=\"paper-links\"><a href=\"https://arxiv.org/pdf/2412.07608.pdf\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">📄 Paper</a>\n<button class=\"abstract-toggle\" onclick=\"toggleAbstract(this)\">📖 Show Abstract</button>\n<div class=\"paper-abstract\">3D Gaussian Splatting (3DGS) has emerged as a powerful technique for novel view synthesis, demonstrating remarkable capability in high-fidelity scene reconstruction through its Gaussian primitive representations. However, the computational overhead induced by the massive number of primitives poses a significant bottleneck to training efficiency. To overcome this challenge, we propose Group Training, a simple yet effective strategy that organizes Gaussian primitives into manageable groups, optimizing training efficiency and improving rendering quality. This approach shows universal compatibility with existing 3DGS frameworks, including vanilla 3DGS and Mip-Splatting, consistently achieving accelerated training while maintaining superior synthesis quality. Extensive experiments reveal that our straightforward Group Training strategy achieves up to 30% faster convergence and improved rendering quality across diverse scenarios.\n</div></div>\n    </div>\n  </div>\n</div>\n<div class=\"paper-row\" data-id=\"li2024recap\" data-title=\"ReCap: Better Gaussian Relighting with Cross-Environment Captures\" data-authors=\"Jingzhi Li, Zongwei Wu, Eduard Zamfir, Radu Timofte\" data-year=\"2024\" data-tags='[\"Project\", \"Relight\"]'>\n  <div class=\"paper-card\">\n    <input type=\"checkbox\" class=\"selection-checkbox\" onclick=\"handleCheckboxClick(event, 'li2024recap', this)\">\n    <div class=\"paper-number\"></div>\n    <div class=\"paper-thumbnail\">\n      <img data-src=\"assets/thumbnails/li2024recap.jpg\" data-fallback=\"None\" alt=\"Paper thumbnail for ReCap: Better Gaussian Relighting with Cross-Environment Captures\" class=\"lazy\" loading=\"lazy\"/>\n    </div>\n    <div class=\"paper-content\">\n      <h2 class=\"paper-title\">ReCap: Better Gaussian Relighting with Cross-Environment Captures <span class=\"paper-year\">(2024)</span></h2>\n      <p class=\"paper-authors\">Jingzhi Li, Zongwei Wu, Eduard Zamfir, Radu Timofte</p>\n      <div class=\"paper-tags\"><span class=\"paper-tag\">Project</span>\n<span class=\"paper-tag\">Relight</span></div>\n      <div class=\"paper-links\"><a href=\"https://arxiv.org/pdf/2412.07534.pdf\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">📄 Paper</a>\n<a href=\"https://jingzhi.github.io/ReCap/\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">🌐 Project</a>\n<button class=\"abstract-toggle\" onclick=\"toggleAbstract(this)\">📖 Show Abstract</button>\n<div class=\"paper-abstract\">Accurate 3D objects relighting in diverse unseen environments is crucial for realistic virtual object placement. Due to the albedo-lighting ambiguity, existing methods often fall short in producing faithful relights. Without proper constraints, observed training views can be explained by numerous combinations of lighting and material attributes, lacking physical correspondence with the actual environment maps used for relighting. In this work, we present ReCap, treating cross-environment captures as multi-task target to provide the missing supervision that cuts through the entanglement. Specifically, ReCap jointly optimizes multiple lighting representations that share a common set of material attributes. This naturally harmonizes a coherent set of lighting representations around the mutual material attributes, exploiting commonalities and differences across varied object appearances. Such coherence enables physically sound lighting reconstruction and robust material estimation - both essential for accurate relighting. Together with a streamlined shading function and effective post-processing, ReCap outperforms the leading competitor by 3.4 dB in PSNR on an expanded relighting benchmark.\n</div></div>\n    </div>\n  </div>\n</div>\n<div class=\"paper-row\" data-id=\"tang2024mvdust3r\" data-title=\"MV-DUSt3R+: Single-Stage Scene Reconstruction from Sparse Views In 2 Seconds\" data-authors=\"Zhenggang Tang, Yuchen Fan, Dilin Wang, Hongyu Xu, Rakesh Ranjan, Alexander Schwing, Zhicheng Yan\" data-year=\"2024\" data-tags='[\"3ster-based\", \"Code\", \"Project\", \"Sparse\", \"Video\"]'>\n  <div class=\"paper-card\">\n    <input type=\"checkbox\" class=\"selection-checkbox\" onclick=\"handleCheckboxClick(event, 'tang2024mvdust3r', this)\">\n    <div class=\"paper-number\"></div>\n    <div class=\"paper-thumbnail\">\n      <img data-src=\"assets/thumbnails/tang2024mvdust3r.jpg\" data-fallback=\"None\" alt=\"Paper thumbnail for MV-DUSt3R+: Single-Stage Scene Reconstruction from Sparse Views In 2 Seconds\" class=\"lazy\" loading=\"lazy\"/>\n    </div>\n    <div class=\"paper-content\">\n      <h2 class=\"paper-title\">MV-DUSt3R+: Single-Stage Scene Reconstruction from Sparse Views In 2 Seconds <span class=\"paper-year\">(2024)</span></h2>\n      <p class=\"paper-authors\">Zhenggang Tang, Yuchen Fan, Dilin Wang, Hongyu Xu, Rakesh Ranjan, Alexander Schwing, Zhicheng Yan</p>\n      <div class=\"paper-tags\"><span class=\"paper-tag\">3ster-based</span>\n<span class=\"paper-tag\">Code</span>\n<span class=\"paper-tag\">Project</span>\n<span class=\"paper-tag\">Sparse</span>\n<span class=\"paper-tag\">Video</span></div>\n      <div class=\"paper-links\"><a href=\"https://arxiv.org/pdf/2412.06974.pdf\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">📄 Paper</a>\n<a href=\"https://mv-dust3rp.github.io/\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">🌐 Project</a>\n<a href=\"https://github.com/facebookresearch/mvdust3r\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">💻 Code</a>\n<a href=\"https://youtu.be/LBvnuKQ8Rso\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">🎥 Video</a>\n<button class=\"abstract-toggle\" onclick=\"toggleAbstract(this)\">📖 Show Abstract</button>\n<div class=\"paper-abstract\">Recent sparse multi-view scene reconstruction advances like DUSt3R and MASt3R no longer require camera calibration and camera pose estimation. However, they only process a pair of views at a time to infer pixel-aligned pointmaps. When dealing with more than two views, a combinatorial number of error prone pairwise reconstructions are usually followed by an expensive global optimization, which often fails to rectify the pairwise reconstruction errors. To handle more views, reduce errors, and improve inference time, we propose the fast single-stage feed-forward network MV-DUSt3R. At its core are multi-view decoder blocks which exchange information across any number of views while considering one reference view. To make our method robust to reference view selection, we further propose MV-DUSt3R+, which employs cross-reference-view blocks to fuse information across different reference view choices. To further enable novel view synthesis, we extend both by adding and jointly training Gaussian splatting heads. Experiments on multi-view stereo reconstruction, multi-view pose estimation, and novel view synthesis confirm that our methods improve significantly upon prior art. Code will be released.\n</div></div>\n    </div>\n  </div>\n</div>\n<div class=\"paper-row\" data-id=\"guédon2024matcha\" data-title=\"MAtCha Gaussians: Atlas of Charts for High-Quality Geometry and Photorealism From Sparse Views\" data-authors=\"Antoine Guédon, Tomoki Ichikawa, Kohei Yamashita, Ko Nishino\" data-year=\"2024\" data-tags='[\"Meshing\", \"Project\", \"Sparse\"]'>\n  <div class=\"paper-card\">\n    <input type=\"checkbox\" class=\"selection-checkbox\" onclick=\"handleCheckboxClick(event, 'guédon2024matcha', this)\">\n    <div class=\"paper-number\"></div>\n    <div class=\"paper-thumbnail\">\n      <img data-src=\"assets/thumbnails/guédon2024matcha.jpg\" data-fallback=\"None\" alt=\"Paper thumbnail for MAtCha Gaussians: Atlas of Charts for High-Quality Geometry and Photorealism From Sparse Views\" class=\"lazy\" loading=\"lazy\"/>\n    </div>\n    <div class=\"paper-content\">\n      <h2 class=\"paper-title\">MAtCha Gaussians: Atlas of Charts for High-Quality Geometry and Photorealism From Sparse Views <span class=\"paper-year\">(2024)</span></h2>\n      <p class=\"paper-authors\">Antoine Guédon, Tomoki Ichikawa, Kohei Yamashita, Ko Nishino</p>\n      <div class=\"paper-tags\"><span class=\"paper-tag\">Meshing</span>\n<span class=\"paper-tag\">Project</span>\n<span class=\"paper-tag\">Sparse</span></div>\n      <div class=\"paper-links\"><a href=\"https://arxiv.org/pdf/2412.06767.pdf\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">📄 Paper</a>\n<a href=\"https://anttwo.github.io/matcha/\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">🌐 Project</a>\n<button class=\"abstract-toggle\" onclick=\"toggleAbstract(this)\">📖 Show Abstract</button>\n<div class=\"paper-abstract\">We present a novel appearance model that simultaneously realizes explicit high-quality 3D surface mesh recovery and photorealistic novel view synthesis from sparse view samples. Our key idea is to model the underlying scene geometry Mesh as an Atlas of Charts which we render with 2D Gaussian surfels (MAtCha Gaussians). MAtCha distills high-frequency scene surface details from an off-the-shelf monocular depth estimator and refines it through Gaussian surfel rendering. The Gaussian surfels are attached to the charts on the fly, satisfying photorealism of neural volumetric rendering and crisp geometry of a mesh model, i.e., two seemingly contradicting goals in a single model. At the core of MAtCha lies a novel neural deformation model and a structure loss that preserve the fine surface details distilled from learned monocular depths while addressing their fundamental scale ambiguities. Results of extensive experimental validation demonstrate MAtCha's state-of-the-art quality of surface reconstruction and photorealism on-par with top contenders but with dramatic reduction in the number of input views and computational time. We believe MAtCha will serve as a foundational tool for any visual application in vision, graphics, and robotics that require explicit geometry in addition to photorealism. Our project page is the following: https://anttwo.github.io/matcha/\n</div></div>\n    </div>\n  </div>\n</div>\n<div class=\"paper-row\" data-id=\"qiu2024advancing\" data-title=\"Advancing Extended Reality with 3D Gaussian Splatting: Innovations and Prospects\" data-authors=\"Shi Qiu, Binzhu Xie, Qixuan Liu, Pheng-Ann Heng\" data-year=\"2024\" data-tags='[\"Review\"]'>\n  <div class=\"paper-card\">\n    <input type=\"checkbox\" class=\"selection-checkbox\" onclick=\"handleCheckboxClick(event, 'qiu2024advancing', this)\">\n    <div class=\"paper-number\"></div>\n    <div class=\"paper-thumbnail\">\n      <img data-src=\"assets/thumbnails/qiu2024advancing.jpg\" data-fallback=\"None\" alt=\"Paper thumbnail for Advancing Extended Reality with 3D Gaussian Splatting: Innovations and Prospects\" class=\"lazy\" loading=\"lazy\"/>\n    </div>\n    <div class=\"paper-content\">\n      <h2 class=\"paper-title\">Advancing Extended Reality with 3D Gaussian Splatting: Innovations and Prospects <span class=\"paper-year\">(2024)</span></h2>\n      <p class=\"paper-authors\">Shi Qiu, Binzhu Xie, Qixuan Liu, Pheng-Ann Heng</p>\n      <div class=\"paper-tags\"><span class=\"paper-tag\">Review</span></div>\n      <div class=\"paper-links\"><a href=\"https://arxiv.org/pdf/2412.06257\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">📄 Paper</a>\n<button class=\"abstract-toggle\" onclick=\"toggleAbstract(this)\">📖 Show Abstract</button>\n<div class=\"paper-abstract\">3D Gaussian Splatting (3DGS) has attracted significant attention for its potential to revolutionize 3D representation, rendering, and interaction. Despite the rapid growth of 3DGS research, its direct application to Extended Reality (XR) remains underexplored. Although many studies recognize the potential of 3DGS for XR, few have explicitly focused on or demonstrated its effectiveness within XR environments. In this paper, we aim to synthesize innovations in 3DGS that show specific potential for advancing XR research and development. We conduct a comprehensive review of publicly available 3DGS papers, with a focus on those referencing XR-related concepts. Additionally, we perform an in-depth analysis of innovations explicitly relevant to XR and propose a taxonomy to highlight their significance. Building on these insights, we propose several prospective XR research areas where 3DGS can make promising contributions, yet remain rarely touched. By investigating the intersection of 3DGS and XR, this paper provides a roadmap to push the boundaries of XR using cutting-edge 3DGS techniques.</div></div>\n    </div>\n  </div>\n</div>\n<div class=\"paper-row\" data-id=\"joanna2024occams\" data-title=\"Occam's LGS: A Simple Approach for Language Gaussian Splatting\" data-authors=\"Jiahuan (Joanna) Cheng, Jan-Nico Zaech, Luc Van Gool, Danda Pani Paudel\" data-year=\"2024\" data-tags='[\"Acceleration\", \"Language Embedding\", \"Project\", \"Segmentation\"]'>\n  <div class=\"paper-card\">\n    <input type=\"checkbox\" class=\"selection-checkbox\" onclick=\"handleCheckboxClick(event, 'joanna2024occams', this)\">\n    <div class=\"paper-number\"></div>\n    <div class=\"paper-thumbnail\">\n      <img data-src=\"assets/thumbnails/joanna2024occams.jpg\" data-fallback=\"None\" alt=\"Paper thumbnail for Occam's LGS: A Simple Approach for Language Gaussian Splatting\" class=\"lazy\" loading=\"lazy\"/>\n    </div>\n    <div class=\"paper-content\">\n      <h2 class=\"paper-title\">Occam's LGS: A Simple Approach for Language Gaussian Splatting <span class=\"paper-year\">(2024)</span></h2>\n      <p class=\"paper-authors\">Jiahuan (Joanna) Cheng, Jan-Nico Zaech, Luc Van Gool, Danda Pani Paudel</p>\n      <div class=\"paper-tags\"><span class=\"paper-tag\">Acceleration</span>\n<span class=\"paper-tag\">Language Embedding</span>\n<span class=\"paper-tag\">Project</span>\n<span class=\"paper-tag\">Segmentation</span></div>\n      <div class=\"paper-links\"><a href=\"https://arxiv.org/pdf/2412.01807\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">📄 Paper</a>\n<a href=\"https://insait-institute.github.io/OccamLGS/\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">🌐 Project</a>\n<button class=\"abstract-toggle\" onclick=\"toggleAbstract(this)\">📖 Show Abstract</button>\n<div class=\"paper-abstract\">Gaussian Splatting is a widely adopted approach for 3D scene representation that offers efficient, high-quality 3D reconstruction and rendering. A major reason for the success of 3DGS is its simplicity of representing a scene with a set of Gaussians, which makes it easy to interpret and adapt. To enhance scene understanding beyond the visual representation, approaches have been developed that extend 3D Gaussian Splatting with semantic vision-language features, especially allowing for open-set tasks. In this setting, the language features of 3D Gaussian Splatting are often aggregated from multiple 2D views. Existing works address this aggregation problem using cumbersome techniques that lead to high computational cost and training time. In this work, we show that the sophisticated techniques for language-grounded 3D Gaussian Splatting are simply unnecessary. Instead, we apply Occam's razor to the task at hand and perform weighted multi-view feature aggregation using the weights derived from the standard rendering process, followed by a simple heuristic-based noisy Gaussian filtration. Doing so offers us state-of-the-art results with a speed-up of two orders of magnitude. We showcase our results in two commonly used benchmark datasets: LERF and 3D-OVS. Our simple approach allows us to perform reasoning directly in the language features, without any compression whatsoever. Such modeling in turn offers easy scene manipulation, unlike the existing methods -- which we illustrate using an application of object insertion in the scene. Furthermore, we provide a thorough discussion regarding the significance of our contributions within the context of the current literature.</div></div>\n    </div>\n  </div>\n</div>\n<div class=\"paper-row\" data-id=\"li2024dynsup\" data-title=\"DynSUP: Dynamic Gaussian Splatting from An Unposed Image Pair\" data-authors=\"Weihang Li, Weirong Chen, Shenhan Qian, Jiajie Chen, Daniel Cremers, Haoang Li\" data-year=\"2024\" data-tags='[\"Dynamic\", \"Poses\", \"Project\"]'>\n  <div class=\"paper-card\">\n    <input type=\"checkbox\" class=\"selection-checkbox\" onclick=\"handleCheckboxClick(event, 'li2024dynsup', this)\">\n    <div class=\"paper-number\"></div>\n    <div class=\"paper-thumbnail\">\n      <img data-src=\"assets/thumbnails/li2024dynsup.jpg\" data-fallback=\"None\" alt=\"Paper thumbnail for DynSUP: Dynamic Gaussian Splatting from An Unposed Image Pair\" class=\"lazy\" loading=\"lazy\"/>\n    </div>\n    <div class=\"paper-content\">\n      <h2 class=\"paper-title\">DynSUP: Dynamic Gaussian Splatting from An Unposed Image Pair <span class=\"paper-year\">(2024)</span></h2>\n      <p class=\"paper-authors\">Weihang Li, Weirong Chen, Shenhan Qian, Jiajie Chen, Daniel Cremers, Haoang Li</p>\n      <div class=\"paper-tags\"><span class=\"paper-tag\">Dynamic</span>\n<span class=\"paper-tag\">Poses</span>\n<span class=\"paper-tag\">Project</span></div>\n      <div class=\"paper-links\"><a href=\"https://arxiv.org/pdf/2412.00851\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">📄 Paper</a>\n<a href=\"https://colin-de.github.io/DynSUP/\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">🌐 Project</a>\n<button class=\"abstract-toggle\" onclick=\"toggleAbstract(this)\">📖 Show Abstract</button>\n<div class=\"paper-abstract\">Recent advances in 3D Gaussian Splatting have shown promising results. Existing methods typically assume static scenes and/or multiple images with prior poses. Dynamics, sparse views, and unknown poses significantly increase the problem complexity due to insufficient geometric constraints. To overcome this challenge, we propose a method that can use only two images without prior poses to fit Gaussians in dynamic environments. To achieve this, we introduce two technical contributions. First, we propose an object-level two-view bundle adjustment. This strategy decomposes dynamic scenes into piece-wise rigid components, and jointly estimates the camera pose and motions of dynamic objects. Second, we design an SE(3) field-driven Gaussian training method. It enables fine-grained motion modeling through learnable per-Gaussian transformations. Our method leads to high-fidelity novel view synthesis of dynamic scenes while accurately preserving temporal consistency and object motion. Experiments on both synthetic and real-world datasets demonstrate that our method significantly outperforms state-of-the-art approaches designed for the cases of static environments, multiple images, and/or known poses. Our project page is available at https://colin-de.github.io/DynSUP/.</div></div>\n    </div>\n  </div>\n</div>\n<div class=\"paper-row\" data-id=\"pryadilshchikov2024t3dgs\" data-title=\"T-3DGS: Removing Transient Objects for 3D Scene Reconstruction\" data-authors=\"Vadim Pryadilshchikov, Alexander Markin, Artem Komarichev, Ruslan Rakhimov, Peter Wonka, Evgeny Burnaev\" data-year=\"2024\" data-tags='[\"Code\", \"Project\", \"Rendering\"]'>\n  <div class=\"paper-card\">\n    <input type=\"checkbox\" class=\"selection-checkbox\" onclick=\"handleCheckboxClick(event, 'pryadilshchikov2024t3dgs', this)\">\n    <div class=\"paper-number\"></div>\n    <div class=\"paper-thumbnail\">\n      <img data-src=\"assets/thumbnails/pryadilshchikov2024t3dgs.jpg\" data-fallback=\"None\" alt=\"Paper thumbnail for T-3DGS: Removing Transient Objects for 3D Scene Reconstruction\" class=\"lazy\" loading=\"lazy\"/>\n    </div>\n    <div class=\"paper-content\">\n      <h2 class=\"paper-title\">T-3DGS: Removing Transient Objects for 3D Scene Reconstruction <span class=\"paper-year\">(2024)</span></h2>\n      <p class=\"paper-authors\">Vadim Pryadilshchikov, Alexander Markin, Artem Komarichev, Ruslan Rakhimov, Peter Wonka, Evgeny Burnaev</p>\n      <div class=\"paper-tags\"><span class=\"paper-tag\">Code</span>\n<span class=\"paper-tag\">Project</span>\n<span class=\"paper-tag\">Rendering</span></div>\n      <div class=\"paper-links\"><a href=\"https://arxiv.org/pdf/2412.00155\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">📄 Paper</a>\n<a href=\"https://transient-3dgs.github.io/\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">🌐 Project</a>\n<a href=\"https://github.com/Vadim200116/T-3DGS\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">💻 Code</a>\n<button class=\"abstract-toggle\" onclick=\"toggleAbstract(this)\">📖 Show Abstract</button>\n<div class=\"paper-abstract\">We propose a novel framework to remove transient objects from input videos for 3D scene reconstruction using Gaussian Splatting. Our framework consists of the following steps. In the first step, we propose an unsupervised training strategy for a classification network to distinguish between transient objects and static scene parts based on their different training behavior inside the 3D Gaussian Splatting reconstruction. In the second step, we improve the boundary quality and stability of the detected transients by combining our results from the first step with an off-the-shelf segmentation method. We also propose a simple and effective strategy to track objects in the input video forward and backward in time. Our results show an improvement over the current state of the art in existing sparsely captured datasets and significant improvements in a newly proposed densely captured (video) dataset.</div></div>\n    </div>\n  </div>\n</div>\n<div class=\"paper-row\" data-id=\"wang2024gaussurf\" data-title=\"GausSurf: Geometry-Guided 3D Gaussian Splatting for Surface Reconstruction\" data-authors=\"Jiepeng Wang, Yuan Liu, Peng Wang, Cheng Lin, Junhui Hou, Xin Li, Taku Komura, Wenping Wang\" data-year=\"2024\" data-tags='[\"Meshing\", \"Project\"]'>\n  <div class=\"paper-card\">\n    <input type=\"checkbox\" class=\"selection-checkbox\" onclick=\"handleCheckboxClick(event, 'wang2024gaussurf', this)\">\n    <div class=\"paper-number\"></div>\n    <div class=\"paper-thumbnail\">\n      <img data-src=\"assets/thumbnails/wang2024gaussurf.jpg\" data-fallback=\"None\" alt=\"Paper thumbnail for GausSurf: Geometry-Guided 3D Gaussian Splatting for Surface Reconstruction\" class=\"lazy\" loading=\"lazy\"/>\n    </div>\n    <div class=\"paper-content\">\n      <h2 class=\"paper-title\">GausSurf: Geometry-Guided 3D Gaussian Splatting for Surface Reconstruction <span class=\"paper-year\">(2024)</span></h2>\n      <p class=\"paper-authors\">Jiepeng Wang, Yuan Liu, Peng Wang, Cheng Lin, Junhui Hou, Xin Li, Taku Komura, Wenping Wang</p>\n      <div class=\"paper-tags\"><span class=\"paper-tag\">Meshing</span>\n<span class=\"paper-tag\">Project</span></div>\n      <div class=\"paper-links\"><a href=\"https://arxiv.org/pdf/2411.19454\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">📄 Paper</a>\n<a href=\"https://jiepengwang.github.io/GausSurf/\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">🌐 Project</a>\n<button class=\"abstract-toggle\" onclick=\"toggleAbstract(this)\">📖 Show Abstract</button>\n<div class=\"paper-abstract\">3D Gaussian Splatting has achieved impressive performance in novel view synthesis with real-time rendering capabilities. However, reconstructing high-quality surfaces with fine details using 3D Gaussians remains a challenging task. In this work, we introduce GausSurf, a novel approach to high-quality surface reconstruction by employing geometry guidance from multi-view consistency in texture-rich areas and normal priors in texture-less areas of a scene. We observe that a scene can be mainly divided into two primary regions: 1) texture-rich and 2) texture-less areas. To enforce multi-view consistency at texture-rich areas, we enhance the reconstruction quality by incorporating a traditional patch-match based Multi-View Stereo (MVS) approach to guide the geometry optimization in an iterative scheme. This scheme allows for mutual reinforcement between the optimization of Gaussians and patch-match refinement, which significantly improves the reconstruction results and accelerates the training process. Meanwhile, for the texture-less areas, we leverage normal priors from a pre-trained normal estimation model to guide optimization. Extensive experiments on the DTU and Tanks and Temples datasets demonstrate that our method surpasses state-of-the-art methods in terms of reconstruction quality and computation time.</div></div>\n    </div>\n  </div>\n</div>\n<div class=\"paper-row\" data-id=\"li2024sadg\" data-title=\"SADG: Segment Any Dynamic Gaussians Without Object Trackers\" data-authors=\"Yun-Jin Li, Mariia Gladkova, Yan Xia, Daniel Cremers\" data-year=\"2024\" data-tags='[\"Dynamic\", \"Project\", \"Segmentation\", \"Video\"]'>\n  <div class=\"paper-card\">\n    <input type=\"checkbox\" class=\"selection-checkbox\" onclick=\"handleCheckboxClick(event, 'li2024sadg', this)\">\n    <div class=\"paper-number\"></div>\n    <div class=\"paper-thumbnail\">\n      <img data-src=\"assets/thumbnails/li2024sadg.jpg\" data-fallback=\"None\" alt=\"Paper thumbnail for SADG: Segment Any Dynamic Gaussians Without Object Trackers\" class=\"lazy\" loading=\"lazy\"/>\n    </div>\n    <div class=\"paper-content\">\n      <h2 class=\"paper-title\">SADG: Segment Any Dynamic Gaussians Without Object Trackers <span class=\"paper-year\">(2024)</span></h2>\n      <p class=\"paper-authors\">Yun-Jin Li, Mariia Gladkova, Yan Xia, Daniel Cremers</p>\n      <div class=\"paper-tags\"><span class=\"paper-tag\">Dynamic</span>\n<span class=\"paper-tag\">Project</span>\n<span class=\"paper-tag\">Segmentation</span>\n<span class=\"paper-tag\">Video</span></div>\n      <div class=\"paper-links\"><a href=\"https://arxiv.org/pdf/2411.19290\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">📄 Paper</a>\n<a href=\"https://yunjinli.github.io/project-sadg/\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">🌐 Project</a>\n<a href=\"https://yunjinli.github.io/project-sadg/static/videos/split-cookie-2x4x.mp4\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">🎥 Video</a>\n<button class=\"abstract-toggle\" onclick=\"toggleAbstract(this)\">📖 Show Abstract</button>\n<div class=\"paper-abstract\">Understanding dynamic 3D scenes is fundamental for various applications, including extended reality (XR) and autonomous driving. Effectively integrating semantic information into 3D reconstruction enables holistic representation that opens opportunities for immersive and interactive applications. We introduce SADG, Segment Any Dynamic Gaussian Without Object Trackers, a novel approach that combines dynamic Gaussian Splatting representation and semantic information without reliance on object IDs. In contrast to existing works, we do not rely on supervision based on object identities to enable consistent segmentation of dynamic 3D objects. To this end, we propose to learn semantically-aware features by leveraging masks generated from the Segment Anything Model (SAM) and utilizing our novel contrastive learning objective based on hard pixel mining. The learned Gaussian features can be effectively clustered without further post-processing. This enables fast computation for further object-level editing, such as object removal, composition, and style transfer by manipulating the Gaussians in the scene. We further extend several dynamic novel-view datasets with segmentation benchmarks to enable testing of learned feature fields from unseen viewpoints. We evaluate SADG on proposed benchmarks and demonstrate the superior performance of our approach in segmenting objects within dynamic scenes along with its effectiveness for further downstream editing tasks.</div></div>\n    </div>\n  </div>\n</div>\n<div class=\"paper-row\" data-id=\"ren2024agsmesh\" data-title=\"AGS-Mesh: Adaptive Gaussian Splatting and Meshing with Geometric Priors for Indoor Room Reconstruction Using Smartphones\" data-authors=\"Xuqian Ren, Matias Turkulainen, Jiepeng Wang, Otto Seiskari, Iaroslav Melekhov, Juho Kannala, Esa Rahtu\" data-year=\"2024\" data-tags='[\"Code\", \"Meshing\", \"Project\"]'>\n  <div class=\"paper-card\">\n    <input type=\"checkbox\" class=\"selection-checkbox\" onclick=\"handleCheckboxClick(event, 'ren2024agsmesh', this)\">\n    <div class=\"paper-number\"></div>\n    <div class=\"paper-thumbnail\">\n      <img data-src=\"assets/thumbnails/ren2024agsmesh.jpg\" data-fallback=\"None\" alt=\"Paper thumbnail for AGS-Mesh: Adaptive Gaussian Splatting and Meshing with Geometric Priors for Indoor Room Reconstruction Using Smartphones\" class=\"lazy\" loading=\"lazy\"/>\n    </div>\n    <div class=\"paper-content\">\n      <h2 class=\"paper-title\">AGS-Mesh: Adaptive Gaussian Splatting and Meshing with Geometric Priors for Indoor Room Reconstruction Using Smartphones <span class=\"paper-year\">(2024)</span></h2>\n      <p class=\"paper-authors\">Xuqian Ren, Matias Turkulainen, Jiepeng Wang, Otto Seiskari, Iaroslav Melekhov, Juho Kannala, Esa Rahtu</p>\n      <div class=\"paper-tags\"><span class=\"paper-tag\">Code</span>\n<span class=\"paper-tag\">Meshing</span>\n<span class=\"paper-tag\">Project</span></div>\n      <div class=\"paper-links\"><a href=\"https://arxiv.org/pdf/2411.19271\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">📄 Paper</a>\n<a href=\"https://xuqianren.github.io/ags_mesh_website/\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">🌐 Project</a>\n<a href=\"https://github.com/XuqianRen/AGS_Mesh\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">💻 Code</a>\n<button class=\"abstract-toggle\" onclick=\"toggleAbstract(this)\">📖 Show Abstract</button>\n<div class=\"paper-abstract\">Geometric priors are often used to enhance 3D reconstruction. With many smartphones featuring low-resolution depth sensors and the prevalence of off-the-shelf monocular geometry estimators, incorporating geometric priors as regularization signals has become common in 3D vision tasks. However, the accuracy of depth estimates from mobile devices is typically poor for highly detailed geometry, and monocular estimators often suffer from poor multi-view consistency and precision. In this work, we propose an approach for joint surface depth and normal refinement of Gaussian Splatting methods for accurate 3D reconstruction of indoor scenes. We develop supervision strategies that adaptively filters low-quality depth and normal estimates by comparing the consistency of the priors during optimization. We mitigate regularization in regions where prior estimates have high uncertainty or ambiguities. Our filtering strategy and optimization design demonstrate significant improvements in both mesh estimation and novel-view synthesis for both 3D and 2D Gaussian Splatting-based methods on challenging indoor room datasets. Furthermore, we explore the use of alternative meshing strategies for finer geometry extraction. We develop a scale-aware meshing strategy inspired by TSDF and octree-based isosurface extraction, which recovers finer details from Gaussian models compared to other commonly used open-source meshing tools. Our code is released in https://xuqianren.github.io/ags_mesh_website/.</div></div>\n    </div>\n  </div>\n</div>\n<div class=\"paper-row\" data-id=\"wimmer2024gaussianstolife\" data-title=\"Gaussians-to-Life: Text-Driven Animation of 3D Gaussian Splatting Scenes\" data-authors=\"Thomas Wimmer, Michael Oechsle, Michael Niemeyer, Federico Tombari\" data-year=\"2024\" data-tags='[\"Code\", \"Diffusion\", \"Dynamic\", \"Project\"]'>\n  <div class=\"paper-card\">\n    <input type=\"checkbox\" class=\"selection-checkbox\" onclick=\"handleCheckboxClick(event, 'wimmer2024gaussianstolife', this)\">\n    <div class=\"paper-number\"></div>\n    <div class=\"paper-thumbnail\">\n      <img data-src=\"assets/thumbnails/wimmer2024gaussianstolife.jpg\" data-fallback=\"None\" alt=\"Paper thumbnail for Gaussians-to-Life: Text-Driven Animation of 3D Gaussian Splatting Scenes\" class=\"lazy\" loading=\"lazy\"/>\n    </div>\n    <div class=\"paper-content\">\n      <h2 class=\"paper-title\">Gaussians-to-Life: Text-Driven Animation of 3D Gaussian Splatting Scenes <span class=\"paper-year\">(2024)</span></h2>\n      <p class=\"paper-authors\">Thomas Wimmer, Michael Oechsle, Michael Niemeyer, Federico Tombari</p>\n      <div class=\"paper-tags\"><span class=\"paper-tag\">Code</span>\n<span class=\"paper-tag\">Diffusion</span>\n<span class=\"paper-tag\">Dynamic</span>\n<span class=\"paper-tag\">Project</span></div>\n      <div class=\"paper-links\"><a href=\"https://arxiv.org/pdf/2411.19233.pdf\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">📄 Paper</a>\n<a href=\"https://wimmerth.github.io/gaussians2life.html\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">🌐 Project</a>\n<a href=\"https://github.com/wimmerth/gaussians2life\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">💻 Code</a>\n<button class=\"abstract-toggle\" onclick=\"toggleAbstract(this)\">📖 Show Abstract</button>\n<div class=\"paper-abstract\">State-of-the-art novel view synthesis methods achieve impressive results for multi-view captures of static 3D scenes. However, the reconstructed scenes still lack \"liveliness,\" a key component for creating engaging 3D experiences. Recently, novel video diffusion models generate realistic videos with complex motion and enable animations of 2D images, however they cannot naively be used to animate 3D scenes as they lack multi-view consistency. To breathe life into the static world, we propose Gaussians2Life, a method for animating parts of high-quality 3D scenes in a Gaussian Splatting representation. Our key idea is to leverage powerful video diffusion models as the generative component of our model and to combine these with a robust technique to lift 2D videos into meaningful 3D motion. We find that, in contrast to prior work, this enables realistic animations of complex, pre-existing 3D scenes and further enables the animation of a large variety of object classes, while related work is mostly focused on prior-based character animation, or single 3D objects. Our model enables the creation of consistent, immersive 3D experiences for arbitrary scenes.\n</div></div>\n    </div>\n  </div>\n</div>\n<div class=\"paper-row\" data-id=\"xu2024supergaussians\" data-title=\"SuperGaussians: Enhancing Gaussian Splatting Using Primitives with Spatially Varying Colors\" data-authors=\"Rui Xu, Wenyue Chen, Jiepeng Wang, Yuan Liu, Peng Wang, Lin Gao, Shiqing Xin, Taku Komura, Xin Li, Wenping Wang\" data-year=\"2024\" data-tags='[\"Code\", \"Project\", \"Rendering\", \"Video\"]'>\n  <div class=\"paper-card\">\n    <input type=\"checkbox\" class=\"selection-checkbox\" onclick=\"handleCheckboxClick(event, 'xu2024supergaussians', this)\">\n    <div class=\"paper-number\"></div>\n    <div class=\"paper-thumbnail\">\n      <img data-src=\"assets/thumbnails/xu2024supergaussians.jpg\" data-fallback=\"None\" alt=\"Paper thumbnail for SuperGaussians: Enhancing Gaussian Splatting Using Primitives with Spatially Varying Colors\" class=\"lazy\" loading=\"lazy\"/>\n    </div>\n    <div class=\"paper-content\">\n      <h2 class=\"paper-title\">SuperGaussians: Enhancing Gaussian Splatting Using Primitives with Spatially Varying Colors <span class=\"paper-year\">(2024)</span></h2>\n      <p class=\"paper-authors\">Rui Xu, Wenyue Chen, Jiepeng Wang, Yuan Liu, Peng Wang, Lin Gao, Shiqing Xin, Taku Komura, Xin Li, Wenping Wang</p>\n      <div class=\"paper-tags\"><span class=\"paper-tag\">Code</span>\n<span class=\"paper-tag\">Project</span>\n<span class=\"paper-tag\">Rendering</span>\n<span class=\"paper-tag\">Video</span></div>\n      <div class=\"paper-links\"><a href=\"https://arxiv.org/pdf/2411.18966\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">📄 Paper</a>\n<a href=\"https://ruixu.me/html/SuperGaussians/index.html\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">🌐 Project</a>\n<a href=\"https://github.com/Xrvitd/SuperGaussians\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">💻 Code</a>\n<a href=\"https://www.youtube.com/watch?v=K0liSjZGnIY&feature=youtu.be\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">🎥 Video</a>\n<button class=\"abstract-toggle\" onclick=\"toggleAbstract(this)\">📖 Show Abstract</button>\n<div class=\"paper-abstract\">Gaussian Splattings demonstrate impressive results in multi-view reconstruction based on Gaussian explicit representations. However, the current Gaussian primitives only have a single view-dependent color and an opacity to represent the appearance and geometry of the scene, resulting in a non-compact representation. In this paper, we introduce a new method called SuperGaussians that utilizes spatially varying colors and opacity in a single Gaussian primitive to improve its representation ability. We have implemented bilinear interpolation, movable kernels, and even tiny neural networks as spatially varying functions. Quantitative and qualitative experimental results demonstrate that all three functions outperform the baseline, with the best movable kernels achieving superior novel view synthesis performance on multiple datasets, highlighting the strong potential of spatially varying functions.</div></div>\n    </div>\n  </div>\n</div>\n<div class=\"paper-row\" data-id=\"hess2024splatad\" data-title=\"SplatAD: Real-Time Lidar and Camera Rendering with 3D Gaussian Splatting for Autonomous Driving\" data-authors=\"Georg Hess, Carl Lindström, Maryam Fatemi, Christoffer Petersson, Lennart Svensson\" data-year=\"2024\" data-tags='[\"Autonomous Driving\", \"Project\"]'>\n  <div class=\"paper-card\">\n    <input type=\"checkbox\" class=\"selection-checkbox\" onclick=\"handleCheckboxClick(event, 'hess2024splatad', this)\">\n    <div class=\"paper-number\"></div>\n    <div class=\"paper-thumbnail\">\n      <img data-src=\"assets/thumbnails/hess2024splatad.jpg\" data-fallback=\"None\" alt=\"Paper thumbnail for SplatAD: Real-Time Lidar and Camera Rendering with 3D Gaussian Splatting for Autonomous Driving\" class=\"lazy\" loading=\"lazy\"/>\n    </div>\n    <div class=\"paper-content\">\n      <h2 class=\"paper-title\">SplatAD: Real-Time Lidar and Camera Rendering with 3D Gaussian Splatting for Autonomous Driving <span class=\"paper-year\">(2024)</span></h2>\n      <p class=\"paper-authors\">Georg Hess, Carl Lindström, Maryam Fatemi, Christoffer Petersson, Lennart Svensson</p>\n      <div class=\"paper-tags\"><span class=\"paper-tag\">Autonomous Driving</span>\n<span class=\"paper-tag\">Project</span></div>\n      <div class=\"paper-links\"><a href=\"https://arxiv.org/pdf/2411.16816\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">📄 Paper</a>\n<a href=\"https://research.zenseact.com/publications/splatad/\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">🌐 Project</a>\n<button class=\"abstract-toggle\" onclick=\"toggleAbstract(this)\">📖 Show Abstract</button>\n<div class=\"paper-abstract\">Ensuring the safety of autonomous robots, such as self-driving vehicles, requires extensive testing across diverse driving scenarios. Simulation is a key ingredient for conducting such testing in a cost-effective and scalable way. Neural rendering methods have gained popularity, as they can build simulation environments from collected logs in a data-driven manner. However, existing neural radiance field (NeRF) methods for sensor-realistic rendering of camera and lidar data suffer from low rendering speeds, limiting their applicability for large-scale testing. While 3D Gaussian Splatting (3DGS) enables real-time rendering, current methods are limited to camera data and are unable to render lidar data essential for autonomous driving. To address these limitations, we propose SplatAD, the first 3DGS-based method for realistic, real-time rendering of dynamic scenes for both camera and lidar data. SplatAD accurately models key sensor-specific phenomena such as rolling shutter effects, lidar intensity, and lidar ray dropouts, using purpose-built algorithms to optimize rendering efficiency. Evaluation across three autonomous driving datasets demonstrates that SplatAD achieves state-of-the-art rendering quality with up to +2 PSNR for NVS and +3 PSNR for reconstruction while increasing rendering speed over NeRF-based methods by an order of magnitude.</div></div>\n    </div>\n  </div>\n</div>\n<div class=\"paper-row\" data-id=\"zhang2024quadratic\" data-title=\"Quadratic Gaussian Splatting for Efficient and Detailed Surface Reconstruction\" data-authors=\"Ziyu Zhang, Binbin Huang, Hanqing Jiang, Liyang Zhou, Xiaojun Xiang, Shunhan Shen\" data-year=\"2024\" data-tags='[\"Code\", \"Meshing\", \"Project\", \"Video\"]'>\n  <div class=\"paper-card\">\n    <input type=\"checkbox\" class=\"selection-checkbox\" onclick=\"handleCheckboxClick(event, 'zhang2024quadratic', this)\">\n    <div class=\"paper-number\"></div>\n    <div class=\"paper-thumbnail\">\n      <img data-src=\"assets/thumbnails/zhang2024quadratic.jpg\" data-fallback=\"None\" alt=\"Paper thumbnail for Quadratic Gaussian Splatting for Efficient and Detailed Surface Reconstruction\" class=\"lazy\" loading=\"lazy\"/>\n    </div>\n    <div class=\"paper-content\">\n      <h2 class=\"paper-title\">Quadratic Gaussian Splatting for Efficient and Detailed Surface Reconstruction <span class=\"paper-year\">(2024)</span></h2>\n      <p class=\"paper-authors\">Ziyu Zhang, Binbin Huang, Hanqing Jiang, Liyang Zhou, Xiaojun Xiang, Shunhan Shen</p>\n      <div class=\"paper-tags\"><span class=\"paper-tag\">Code</span>\n<span class=\"paper-tag\">Meshing</span>\n<span class=\"paper-tag\">Project</span>\n<span class=\"paper-tag\">Video</span></div>\n      <div class=\"paper-links\"><a href=\"https://arxiv.org/pdf/2411.16392.pdf\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">📄 Paper</a>\n<a href=\"https://quadraticgs.github.io/QGS/\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">🌐 Project</a>\n<a href=\"https://github.com/QuadraticGS/QGS\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">💻 Code</a>\n<a href=\"https://quadraticgs.github.io/QGS/assets/QGS_web.mp4\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">🎥 Video</a>\n<button class=\"abstract-toggle\" onclick=\"toggleAbstract(this)\">📖 Show Abstract</button>\n<div class=\"paper-abstract\">Recently, 3D Gaussian Splatting (3DGS) has attracted attention for its superior rendering quality and speed over Neural Radiance Fields (NeRF). To address 3DGS's limitations in surface representation, 2D Gaussian Splatting (2DGS) introduced disks as scene primitives to model and reconstruct geometries from multi-view images, offering view-consistent geometry. However, the disk's first-order linear approximation often leads to over-smoothed results. We propose Quadratic Gaussian Splatting (QGS), a novel method that replaces disks with quadric surfaces, enhancing geometric fitting, whose code will be open-sourced. QGS defines Gaussian distributions in non-Euclidean space, allowing primitives to capture more complex textures. As a second-order surface approximation, QGS also renders spatial curvature to guide the normal consistency term, to effectively reduce over-smoothing. Moreover, QGS is a generalized version of 2DGS that achieves more accurate and detailed reconstructions, as verified by experiments on DTU and TNT, demonstrating its effectiveness in surpassing current state-of-the-art methods in geometry reconstruction. Our code willbe released as open source.\n</div></div>\n    </div>\n  </div>\n</div>\n<div class=\"paper-row\" data-id=\"blark2024splatsdf\" data-title=\"SplatSDF: Boosting Neural Implicit SDF via Gaussian Splatting Fusion\" data-authors=\"Runfa Blark Li, Keito Suzuki, Bang Du, Ki Myung Brian Lee, Nikolay Atanasov, Truong Nguyen\" data-year=\"2024\" data-tags='[\"Meshing\", \"Project\"]'>\n  <div class=\"paper-card\">\n    <input type=\"checkbox\" class=\"selection-checkbox\" onclick=\"handleCheckboxClick(event, 'blark2024splatsdf', this)\">\n    <div class=\"paper-number\"></div>\n    <div class=\"paper-thumbnail\">\n      <img data-src=\"assets/thumbnails/blark2024splatsdf.jpg\" data-fallback=\"None\" alt=\"Paper thumbnail for SplatSDF: Boosting Neural Implicit SDF via Gaussian Splatting Fusion\" class=\"lazy\" loading=\"lazy\"/>\n    </div>\n    <div class=\"paper-content\">\n      <h2 class=\"paper-title\">SplatSDF: Boosting Neural Implicit SDF via Gaussian Splatting Fusion <span class=\"paper-year\">(2024)</span></h2>\n      <p class=\"paper-authors\">Runfa Blark Li, Keito Suzuki, Bang Du, Ki Myung Brian Lee, Nikolay Atanasov, Truong Nguyen</p>\n      <div class=\"paper-tags\"><span class=\"paper-tag\">Meshing</span>\n<span class=\"paper-tag\">Project</span></div>\n      <div class=\"paper-links\"><a href=\"https://arxiv.org/pdf/2411.15468.pdf\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">📄 Paper</a>\n<a href=\"https://blarklee.github.io/splatsdf/\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">🌐 Project</a>\n<button class=\"abstract-toggle\" onclick=\"toggleAbstract(this)\">📖 Show Abstract</button>\n<div class=\"paper-abstract\">A signed distance function (SDF) is a useful representation for continuous-space geometry and many related operations, including rendering, collision checking, and mesh generation. Hence, reconstructing SDF from image observations accurately and efficiently is a fundamental problem. Recently, neural implicit SDF (SDF-NeRF) techniques, trained using volumetric rendering, have gained a lot of attention. Compared to earlier truncated SDF (TSDF) fusion algorithms that rely on depth maps and voxelize continuous space, SDF-NeRF enables continuous-space SDF reconstruction with better geometric and photometric accuracy. However, the accuracy and convergence speed of scene-level SDF reconstruction require further improvements for many applications. With the advent of 3D Gaussian Splatting (3DGS) as an explicit representation with excellent rendering quality and speed, several works have focused on improving SDF-NeRF by introducing consistency losses on depth and surface normals between 3DGS and SDF-NeRF. However, loss-level connections alone lead to incremental improvements. We propose a novel neural implicit SDF called “SplatSDF” to fuse 3DGS and SDF-NeRF at an architecture level with significant boosts to geometric and photometric accuracy and convergence speed. Our SplatSDF relies on 3DGS as input only during training, and keeps the same complexity and efficiency as the original SDF-NeRF during inference. Our method outperforms state-of-the-art SDF-NeRF models on geometric and photometric evaluation by the time of submission.</div></div>\n    </div>\n  </div>\n</div>\n<div class=\"paper-row\" data-id=\"kong2024dgsslam\" data-title=\"DGS-SLAM: Gaussian Splatting SLAM in Dynamic Environment\" data-authors=\"Mangyu Kong, Jaewon Lee, Seongwon Lee, Euntai Kim\" data-year=\"2024\" data-tags='[\"SLAM\", \"Video\"]'>\n  <div class=\"paper-card\">\n    <input type=\"checkbox\" class=\"selection-checkbox\" onclick=\"handleCheckboxClick(event, 'kong2024dgsslam', this)\">\n    <div class=\"paper-number\"></div>\n    <div class=\"paper-thumbnail\">\n      <img data-src=\"assets/thumbnails/kong2024dgsslam.jpg\" data-fallback=\"None\" alt=\"Paper thumbnail for DGS-SLAM: Gaussian Splatting SLAM in Dynamic Environment\" class=\"lazy\" loading=\"lazy\"/>\n    </div>\n    <div class=\"paper-content\">\n      <h2 class=\"paper-title\">DGS-SLAM: Gaussian Splatting SLAM in Dynamic Environment <span class=\"paper-year\">(2024)</span></h2>\n      <p class=\"paper-authors\">Mangyu Kong, Jaewon Lee, Seongwon Lee, Euntai Kim</p>\n      <div class=\"paper-tags\"><span class=\"paper-tag\">SLAM</span>\n<span class=\"paper-tag\">Video</span></div>\n      <div class=\"paper-links\"><a href=\"https://arxiv.org/pdf/2411.10722\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">📄 Paper</a>\n<a href=\"https://youtu.be/Mq3qZTTcN3E?feature=shared\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">🎥 Video</a>\n<button class=\"abstract-toggle\" onclick=\"toggleAbstract(this)\">📖 Show Abstract</button>\n<div class=\"paper-abstract\">We introduce Dynamic Gaussian Splatting SLAM (DGS-SLAM), the first dynamic SLAM framework built on the foundation of Gaussian Splatting. While recent advancements in dense SLAM have leveraged Gaussian Splatting to enhance scene representation, most approaches assume a static environment, making them vulnerable to photometric and geometric inconsistencies caused by dynamic objects. To address these challenges, we integrate Gaussian Splatting SLAM with a robust filtering process to handle dynamic objects throughout the entire pipeline, including Gaussian insertion and keyframe selection. Within this framework, to further improve the accuracy of dynamic object removal, we introduce a robust mask generation method that enforces photometric consistency across keyframes, reducing noise from inaccurate segmentation and artifacts such as shadows. Additionally, we propose the loop-aware window selection mechanism, which utilizes unique keyframe IDs of 3D Gaussians to detect loops between the current and past frames, facilitating joint optimization of the current camera poses and the Gaussian map. DGS-SLAM achieves state-of-the-art performance in both camera tracking and novel view synthesis on various dynamic SLAM benchmarks, proving its effectiveness in handling real-world dynamic scenes.</div></div>\n    </div>\n  </div>\n</div>\n<div class=\"paper-row\" data-id=\"svitov2024billboard\" data-title=\"BillBoard Splatting (BBSplat): Learnable Textured Primitives for Novel View Synthesis\" data-authors=\"David Svitov, Pietro Morerio, Lourdes Agapito, Alessio Del Bue\" data-year=\"2024\" data-tags='[\"Code\", \"Optimization\", \"Project\", \"Texturing\", \"Video\"]'>\n  <div class=\"paper-card\">\n    <input type=\"checkbox\" class=\"selection-checkbox\" onclick=\"handleCheckboxClick(event, 'svitov2024billboard', this)\">\n    <div class=\"paper-number\"></div>\n    <div class=\"paper-thumbnail\">\n      <img data-src=\"assets/thumbnails/svitov2024billboard.jpg\" data-fallback=\"None\" alt=\"Paper thumbnail for BillBoard Splatting (BBSplat): Learnable Textured Primitives for Novel View Synthesis\" class=\"lazy\" loading=\"lazy\"/>\n    </div>\n    <div class=\"paper-content\">\n      <h2 class=\"paper-title\">BillBoard Splatting (BBSplat): Learnable Textured Primitives for Novel View Synthesis <span class=\"paper-year\">(2024)</span></h2>\n      <p class=\"paper-authors\">David Svitov, Pietro Morerio, Lourdes Agapito, Alessio Del Bue</p>\n      <div class=\"paper-tags\"><span class=\"paper-tag\">Code</span>\n<span class=\"paper-tag\">Optimization</span>\n<span class=\"paper-tag\">Project</span>\n<span class=\"paper-tag\">Texturing</span>\n<span class=\"paper-tag\">Video</span></div>\n      <div class=\"paper-links\"><a href=\"https://arxiv.org/pdf/2411.08508.pdf\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">📄 Paper</a>\n<a href=\"https://david-svitov.github.io/BBSplat_project_page/\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">🌐 Project</a>\n<a href=\"https://github.com/david-svitov/BBSplat\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">💻 Code</a>\n<a href=\"https://youtu.be/uRM7WFo5vVg\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">🎥 Video</a>\n<button class=\"abstract-toggle\" onclick=\"toggleAbstract(this)\">📖 Show Abstract</button>\n<div class=\"paper-abstract\">We present billboard Splatting (BBSplat) - a novel approach for 3D scene representation based on textured geometric primitives. BBSplat represents the scene as a set of optimizable textured planar primitives with learnable RGB textures and alpha-maps to control their shape. BBSplat primitives can be used in any Gaussian Splatting pipeline as drop-in replacements for Gaussians. Our method's qualitative and quantitative improvements over 3D and 2D Gaussians are most noticeable when fewer primitives are used, when BBSplat achieves over 1200 FPS. Our novel regularization term encourages textures to have a sparser structure, unlocking an efficient compression that leads to a reduction in storage space of the model. Our experiments show the efficiency of BBSplat on standard datasets of real indoor and outdoor scenes such as Tanks&Temples, DTU, and Mip-NeRF-360. We demonstrate improvements on PSNR, SSIM, and LPIPS metrics compared to the state-of-the-art, especially for the case when fewer primitives are used, which, on the other hand, leads to up to 2 times inference speed improvement for the same rendering quality.\n</div></div>\n    </div>\n  </div>\n</div>\n<div class=\"paper-row\" data-id=\"wang2024mbaslam\" data-title=\"MBA-SLAM: Motion Blur Aware Dense Visual SLAM with Radiance Fields Representation\" data-authors=\"Peng Wang, Lingzhe Zhao, Yin Zhang, Shiyu Zhao, Peidong Liu\" data-year=\"2024\" data-tags='[\"Project\", \"SLAM\"]'>\n  <div class=\"paper-card\">\n    <input type=\"checkbox\" class=\"selection-checkbox\" onclick=\"handleCheckboxClick(event, 'wang2024mbaslam', this)\">\n    <div class=\"paper-number\"></div>\n    <div class=\"paper-thumbnail\">\n      <img data-src=\"assets/thumbnails/wang2024mbaslam.jpg\" data-fallback=\"None\" alt=\"Paper thumbnail for MBA-SLAM: Motion Blur Aware Dense Visual SLAM with Radiance Fields Representation\" class=\"lazy\" loading=\"lazy\"/>\n    </div>\n    <div class=\"paper-content\">\n      <h2 class=\"paper-title\">MBA-SLAM: Motion Blur Aware Dense Visual SLAM with Radiance Fields Representation <span class=\"paper-year\">(2024)</span></h2>\n      <p class=\"paper-authors\">Peng Wang, Lingzhe Zhao, Yin Zhang, Shiyu Zhao, Peidong Liu</p>\n      <div class=\"paper-tags\"><span class=\"paper-tag\">Project</span>\n<span class=\"paper-tag\">SLAM</span></div>\n      <div class=\"paper-links\"><a href=\"https://arxiv.org/pdf/2411.08279\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">📄 Paper</a>\n<a href=\"https://wangpeng000.github.io/MBA-SLAM/\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">🌐 Project</a>\n<button class=\"abstract-toggle\" onclick=\"toggleAbstract(this)\">📖 Show Abstract</button>\n<div class=\"paper-abstract\">Emerging 3D scene representations, such as Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS), have demonstrated their effectiveness in Simultaneous Localization and Mapping (SLAM) for photo-realistic rendering, particularly when using high-quality video sequences as input. However, existing methods struggle with motion-blurred frames, which are common in real-world scenarios like low-light or long-exposure conditions. This often results in a significant reduction in both camera localization accuracy and map reconstruction quality. To address this challenge, we propose a dense visual SLAM pipeline (i.e. MBA-SLAM) to handle severe motion-blurred inputs. Our approach integrates an efficient motion blur-aware tracker with either neural radiance fields or Gaussian Splatting based mapper. By accurately modeling the physical image formation process of motion-blurred images, our method simultaneously learns 3D scene representation and estimates the cameras' local trajectory during exposure time, enabling proactive compensation for motion blur caused by camera movement. In our experiments, we demonstrate that MBA-SLAM surpasses previous state-of-the-art methods in both camera localization and map reconstruction, showcasing superior performance across a range of datasets, including synthetic and real datasets featuring sharp images as well as those affected by motion blur, highlighting the versatility and robustness of our approach.</div></div>\n    </div>\n  </div>\n</div>\n<div class=\"paper-row\" data-id=\"lu20243dgscd\" data-title=\"3DGS-CD: 3D Gaussian Splatting-based Change Detection for Physical Object Rearrangement\" data-authors=\"Ziqi Lu, Jianbo Ye, John Leonard\" data-year=\"2024\" data-tags='[\"Robotics\"]'>\n  <div class=\"paper-card\">\n    <input type=\"checkbox\" class=\"selection-checkbox\" onclick=\"handleCheckboxClick(event, 'lu20243dgscd', this)\">\n    <div class=\"paper-number\"></div>\n    <div class=\"paper-thumbnail\">\n      <img data-src=\"assets/thumbnails/lu20243dgscd.jpg\" data-fallback=\"None\" alt=\"Paper thumbnail for 3DGS-CD: 3D Gaussian Splatting-based Change Detection for Physical Object Rearrangement\" class=\"lazy\" loading=\"lazy\"/>\n    </div>\n    <div class=\"paper-content\">\n      <h2 class=\"paper-title\">3DGS-CD: 3D Gaussian Splatting-based Change Detection for Physical Object Rearrangement <span class=\"paper-year\">(2024)</span></h2>\n      <p class=\"paper-authors\">Ziqi Lu, Jianbo Ye, John Leonard</p>\n      <div class=\"paper-tags\"><span class=\"paper-tag\">Robotics</span></div>\n      <div class=\"paper-links\"><a href=\"https://arxiv.org/pdf/2411.03706\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">📄 Paper</a>\n<button class=\"abstract-toggle\" onclick=\"toggleAbstract(this)\">📖 Show Abstract</button>\n<div class=\"paper-abstract\">We present 3DGS-CD, the first 3D Gaussian Splatting (3DGS)-based method for detecting physical object rearrangements in 3D scenes. Our approach estimates 3D object-level changes by comparing two sets of unaligned images taken at different times. Leveraging 3DGS's novel view rendering and EfficientSAM's zero-shot segmentation capabilities, we detect 2D object-level changes, which are then associated and fused across views to estimate 3D changes. Our method can detect changes in cluttered environments using sparse post-change images within as little as 18s, using as few as a single new image. It does not rely on depth input, user instructions, object classes, or object models -- An object is recognized simply if it has been re-arranged. Our approach is evaluated on both public and self-collected real-world datasets, achieving up to 14% higher accuracy and three orders of magnitude faster performance compared to the state-of-the-art radiance-field-based change detection method. This significant performance boost enables a broad range of downstream applications, where we highlight three key use cases: object reconstruction, robot workspace reset, and 3DGS model update. Our code and data will be made available at https://github.com/520xyxyzq/3DGS-CD.</div></div>\n    </div>\n  </div>\n</div>\n<div class=\"paper-row\" data-id=\"liu2024citygaussianv2\" data-title=\"CityGaussianV2: Efficient and Geometrically Accurate Reconstruction for Large-Scale Scenes\" data-authors=\"Yang Liu, Chuanchen Luo, Zhongkai Mao, Junran Peng, Zhaoxiang Zhang\" data-year=\"2024\" data-tags='[\"Code\", \"Large-Scale\", \"Meshing\", \"Project\"]'>\n  <div class=\"paper-card\">\n    <input type=\"checkbox\" class=\"selection-checkbox\" onclick=\"handleCheckboxClick(event, 'liu2024citygaussianv2', this)\">\n    <div class=\"paper-number\"></div>\n    <div class=\"paper-thumbnail\">\n      <img data-src=\"assets/thumbnails/liu2024citygaussianv2.jpg\" data-fallback=\"None\" alt=\"Paper thumbnail for CityGaussianV2: Efficient and Geometrically Accurate Reconstruction for Large-Scale Scenes\" class=\"lazy\" loading=\"lazy\"/>\n    </div>\n    <div class=\"paper-content\">\n      <h2 class=\"paper-title\">CityGaussianV2: Efficient and Geometrically Accurate Reconstruction for Large-Scale Scenes <span class=\"paper-year\">(2024)</span></h2>\n      <p class=\"paper-authors\">Yang Liu, Chuanchen Luo, Zhongkai Mao, Junran Peng, Zhaoxiang Zhang</p>\n      <div class=\"paper-tags\"><span class=\"paper-tag\">Code</span>\n<span class=\"paper-tag\">Large-Scale</span>\n<span class=\"paper-tag\">Meshing</span>\n<span class=\"paper-tag\">Project</span></div>\n      <div class=\"paper-links\"><a href=\"https://arxiv.org/pdf/2411.00771\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">📄 Paper</a>\n<a href=\"https://dekuliutesla.github.io/CityGaussianV2/\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">🌐 Project</a>\n<a href=\"https://github.com/DekuLiuTesla/CityGaussian\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">💻 Code</a>\n<button class=\"abstract-toggle\" onclick=\"toggleAbstract(this)\">📖 Show Abstract</button>\n<div class=\"paper-abstract\">Recently, 3D Gaussian Splatting (3DGS) has revolutionized radiance field reconstruction, manifesting efficient and high-fidelity novel view synthesis. However, accurately representing surfaces, especially in large and complex scenarios, remains a significant challenge due to the unstructured nature of 3DGS. In this paper, we present CityGaussianV2, a novel approach for large-scale scene reconstruction that addresses critical challenges related to geometric accuracy and efficiency. Building on the favorable generalization capabilities of 2D Gaussian Splatting (2DGS), we address its convergence and scalability issues. Specifically, we implement a decomposed-gradient-based densification and depth regression technique to eliminate blurry artifacts and accelerate convergence. To scale up, we introduce an elongation filter that mitigates Gaussian count explosion caused by 2DGS degeneration. Furthermore, we optimize the CityGaussian pipeline for parallel training, achieving up to 10x compression, at least 25% savings in training time, and a 50% decrease in memory usage. We also established standard geometry benchmarks under large-scale scenes. Experimental results demonstrate that our method strikes a promising balance between visual quality, geometric accuracy, as well as storage and training costs.</div></div>\n    </div>\n  </div>\n</div>\n<div class=\"paper-row\" data-id=\"zubair2024neural\" data-title=\"Neural Fields in Robotics: A Survey\" data-authors=\"Muhammad Zubair Irshad, Mauro Comi, Yen-Chen Lin, Nick Heppert, Abhinav Valada, Zsolt Kira, Rares Ambrus, Johnathan Trembley\" data-year=\"2024\" data-tags='[\"Review\", \"Robotics\"]'>\n  <div class=\"paper-card\">\n    <input type=\"checkbox\" class=\"selection-checkbox\" onclick=\"handleCheckboxClick(event, 'zubair2024neural', this)\">\n    <div class=\"paper-number\"></div>\n    <div class=\"paper-thumbnail\">\n      <img data-src=\"assets/thumbnails/zubair2024neural.jpg\" data-fallback=\"None\" alt=\"Paper thumbnail for Neural Fields in Robotics: A Survey\" class=\"lazy\" loading=\"lazy\"/>\n    </div>\n    <div class=\"paper-content\">\n      <h2 class=\"paper-title\">Neural Fields in Robotics: A Survey <span class=\"paper-year\">(2024)</span></h2>\n      <p class=\"paper-authors\">Muhammad Zubair Irshad, Mauro Comi, Yen-Chen Lin, Nick Heppert, Abhinav Valada, Zsolt Kira, Rares Ambrus, Johnathan Trembley</p>\n      <div class=\"paper-tags\"><span class=\"paper-tag\">Review</span>\n<span class=\"paper-tag\">Robotics</span></div>\n      <div class=\"paper-links\"><a href=\"https://arxiv.org/pdf/2410.20220\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">📄 Paper</a>\n<button class=\"abstract-toggle\" onclick=\"toggleAbstract(this)\">📖 Show Abstract</button>\n<div class=\"paper-abstract\">Neural Fields have emerged as a transformative approach for 3D scene representation in computer vision and robotics, enabling accurate inference of geometry, 3D semantics, and dynamics from posed 2D data. Leveraging differentiable rendering, Neural Fields encompass both continuous implicit and explicit neural representations enabling high-fidelity 3D reconstruction, integration of multi-modal sensor data, and generation of novel viewpoints. This survey explores their applications in robotics, emphasizing their potential to enhance perception, planning, and control. Their compactness, memory efficiency, and differentiability, along with seamless integration with foundation and generative models, make them ideal for real-time applications, improving robot adaptability and decision-making. This paper provides a thorough review of Neural Fields in robotics, categorizing applications across various domains and evaluating their strengths and limitations, based on over 200 papers. First, we present four key Neural Fields frameworks: Occupancy Networks, Signed Distance Fields, Neural Radiance Fields, and Gaussian Splatting. Second, we detail Neural Fields' applications in five major robotics domains: pose estimation, manipulation, navigation, physics, and autonomous driving, highlighting key works and discussing takeaways and open challenges. Finally, we outline the current limitations of Neural Fields in robotics and propose promising directions for future research.</div></div>\n    </div>\n  </div>\n</div>\n<div class=\"paper-row\" data-id=\"fan2024large\" data-title=\"Large Spatial Model: End-to-end Unposed Images to Semantic 3D\" data-authors=\"Zhiwen Fan, Jian Zhang, Wenyan Cong, Peihao Wang, Renjie Li, Kairun Wen, Shijie Zhou, Achuta Kadambi, Zhangyang Wang, Danfei Xu, Boris Ivanovic, Marco Pavone, Yue Wang\" data-year=\"2024\" data-tags='[\"Code\", \"Feed-Forward\", \"Poses\", \"Project\", \"Segmentation\", \"Video\"]'>\n  <div class=\"paper-card\">\n    <input type=\"checkbox\" class=\"selection-checkbox\" onclick=\"handleCheckboxClick(event, 'fan2024large', this)\">\n    <div class=\"paper-number\"></div>\n    <div class=\"paper-thumbnail\">\n      <img data-src=\"assets/thumbnails/fan2024large.jpg\" data-fallback=\"None\" alt=\"Paper thumbnail for Large Spatial Model: End-to-end Unposed Images to Semantic 3D\" class=\"lazy\" loading=\"lazy\"/>\n    </div>\n    <div class=\"paper-content\">\n      <h2 class=\"paper-title\">Large Spatial Model: End-to-end Unposed Images to Semantic 3D <span class=\"paper-year\">(2024)</span></h2>\n      <p class=\"paper-authors\">Zhiwen Fan, Jian Zhang, Wenyan Cong, Peihao Wang, Renjie Li, Kairun Wen, Shijie Zhou, Achuta Kadambi, Zhangyang Wang, Danfei Xu, Boris Ivanovic, Marco Pavone, Yue Wang</p>\n      <div class=\"paper-tags\"><span class=\"paper-tag\">Code</span>\n<span class=\"paper-tag\">Feed-Forward</span>\n<span class=\"paper-tag\">Poses</span>\n<span class=\"paper-tag\">Project</span>\n<span class=\"paper-tag\">Segmentation</span>\n<span class=\"paper-tag\">Video</span></div>\n      <div class=\"paper-links\"><a href=\"https://arxiv.org/pdf/2410.18956.pdf\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">📄 Paper</a>\n<a href=\"https://largespatialmodel.github.io/\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">🌐 Project</a>\n<a href=\"https://github.com/NVlabs/LSM\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">💻 Code</a>\n<a href=\"https://largespatialmodel.github.io/static/videos/LSM_demo1.mp4\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">🎥 Video</a>\n<button class=\"abstract-toggle\" onclick=\"toggleAbstract(this)\">📖 Show Abstract</button>\n<div class=\"paper-abstract\">Reconstructing and understanding 3D structures from a limited number of images is a well-established problem in computer vision. Traditional methods usually break this task into multiple subtasks, each requiring complex transformations between different data representations. For instance, dense reconstruction through Structure-from-Motion (SfM) involves converting images into key points, optimizing camera parameters, and estimating structures. Afterward, accurate sparse reconstructions are required for further dense modeling, which is subsequently fed into task-specific neural networks. This multi-step process results in considerable processing time and increased engineering complexity. In this work, we present the Large Spatial Model (LSM), which processes unposed RGB images directly into semantic radiance fields. LSM simultaneously estimates geometry, appearance, and semantics in a single feed-forward operation, and it can generate versatile label maps by interacting with language at novel viewpoints. Leveraging a Transformer-based architecture, LSM integrates global geometry through pixel-aligned point maps. To enhance spatial attribute regression, we incorporate local context aggregation with multi-scale fusion, improving the accuracy of fine local details. To tackle the scarcity of labeled 3D semantic data and enable natural language-driven scene manipulation, we incorporate a pre-trained 2D language-based segmentation model into a 3D-consistent semantic feature field. An efficient decoder then parameterizes a set of semantic anisotropic Gaussians, facilitating supervised end-to-end learning. Extensive experiments across various tasks show that LSM unifies multiple 3D vision tasks directly from unposed images, achieving real-time semantic 3D reconstruction for the first time.\n</div></div>\n    </div>\n  </div>\n</div>\n<div class=\"paper-row\" data-id=\"lee2024fully\" data-title=\"Fully Explicit Dynamic Gaussian Splatting\" data-authors=\"Junoh Lee, Changyeon Won, HyunJun Jung, Inhwan Bae, Hae-Gon Jeon\" data-year=\"2024\" data-tags='[\"Code\", \"Dynamic\", \"Project\"]'>\n  <div class=\"paper-card\">\n    <input type=\"checkbox\" class=\"selection-checkbox\" onclick=\"handleCheckboxClick(event, 'lee2024fully', this)\">\n    <div class=\"paper-number\"></div>\n    <div class=\"paper-thumbnail\">\n      <img data-src=\"assets/thumbnails/lee2024fully.jpg\" data-fallback=\"None\" alt=\"Paper thumbnail for Fully Explicit Dynamic Gaussian Splatting\" class=\"lazy\" loading=\"lazy\"/>\n    </div>\n    <div class=\"paper-content\">\n      <h2 class=\"paper-title\">Fully Explicit Dynamic Gaussian Splatting <span class=\"paper-year\">(2024)</span></h2>\n      <p class=\"paper-authors\">Junoh Lee, Changyeon Won, HyunJun Jung, Inhwan Bae, Hae-Gon Jeon</p>\n      <div class=\"paper-tags\"><span class=\"paper-tag\">Code</span>\n<span class=\"paper-tag\">Dynamic</span>\n<span class=\"paper-tag\">Project</span></div>\n      <div class=\"paper-links\"><a href=\"https://arxiv.org/pdf/2410.15629\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">📄 Paper</a>\n<a href=\"https://leejunoh.com/Ex4DGS/\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">🌐 Project</a>\n<a href=\"https://github.com/juno181/Ex4DGS\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">💻 Code</a>\n<button class=\"abstract-toggle\" onclick=\"toggleAbstract(this)\">📖 Show Abstract</button>\n<div class=\"paper-abstract\">3D Gaussian Splatting has shown fast and high-quality rendering results in static scenes by leveraging dense 3D prior and explicit representations. Unfortunately, the benefits of the prior and representation do not involve novel view synthesis for dynamic motions. Ironically, this is because the main barrier is the reliance on them, which requires increasing training and rendering times to account for dynamic motions. In this paper, we design a \\Edited{Explicit 4D Gaussian Splatting(Ex4DGS)}. Our key idea is to firstly separate static and dynamic Gaussians during training, and to explicitly sample positions and rotations of the dynamic Gaussians at sparse timestamps. The sampled positions and rotations are then interpolated to represent both spatially and temporally continuous motions of objects in dynamic scenes as well as reducing computational cost. Additionally, we introduce a progressive training scheme and a point-backtracking technique that improves Ex4DGS's convergence. We initially train Ex4DGS using short timestamps and progressively extend timestamps, which makes it work well with a few point clouds. The point-backtracking is used to quantify the cumulative error of each Gaussian over time, enabling the detection and removal of erroneous Gaussians in dynamic scenes. Comprehensive experiments on various scenes demonstrate the state-of-the-art rendering quality from our method, achieving fast rendering of 62 fps on a single 2080Ti GPU.</div></div>\n    </div>\n  </div>\n</div>\n<div class=\"paper-row\" data-id=\"hahlbohm2024efficient\" data-title=\"Efficient Perspective-Correct 3D Gaussian Splatting Using Hybrid Transparency\" data-authors=\"Florian Hahlbohm, Fabian Friederichs, Tim Weyrich, Linus Franke, Moritz Kappel, Susana Castillo, Marc Stamminger, Martin Eisemann, Marcus Magnor\" data-year=\"2024\" data-tags='[\"Perspective-correct\", \"Project\", \"Video\"]'>\n  <div class=\"paper-card\">\n    <input type=\"checkbox\" class=\"selection-checkbox\" onclick=\"handleCheckboxClick(event, 'hahlbohm2024efficient', this)\">\n    <div class=\"paper-number\"></div>\n    <div class=\"paper-thumbnail\">\n      <img data-src=\"assets/thumbnails/hahlbohm2024efficient.jpg\" data-fallback=\"None\" alt=\"Paper thumbnail for Efficient Perspective-Correct 3D Gaussian Splatting Using Hybrid Transparency\" class=\"lazy\" loading=\"lazy\"/>\n    </div>\n    <div class=\"paper-content\">\n      <h2 class=\"paper-title\">Efficient Perspective-Correct 3D Gaussian Splatting Using Hybrid Transparency <span class=\"paper-year\">(2024)</span></h2>\n      <p class=\"paper-authors\">Florian Hahlbohm, Fabian Friederichs, Tim Weyrich, Linus Franke, Moritz Kappel, Susana Castillo, Marc Stamminger, Martin Eisemann, Marcus Magnor</p>\n      <div class=\"paper-tags\"><span class=\"paper-tag\">Perspective-correct</span>\n<span class=\"paper-tag\">Project</span>\n<span class=\"paper-tag\">Video</span></div>\n      <div class=\"paper-links\"><a href=\"https://arxiv.org/pdf/2410.08129.pdf\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">📄 Paper</a>\n<a href=\"https://fhahlbohm.github.io/htgs/\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">🌐 Project</a>\n<a href=\"https://fhahlbohm.github.io/htgs/assets/htgs_twitter_teaser.mp4\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">🎥 Video</a>\n<button class=\"abstract-toggle\" onclick=\"toggleAbstract(this)\">📖 Show Abstract</button>\n<div class=\"paper-abstract\">3D Gaussian Splats (3DGS) have proven a versatile rendering primitive, both for inverse rendering as well as real-time exploration of scenes. In these applications, coherence across camera frames and multiple views is crucial, be it for robust convergence of a scene reconstruction or for artifact-free fly-throughs. Recent work started mitigating artifacts that break multi-view coherence, including popping artifacts due to inconsistent transparency sorting and perspective-correct outlines of (2D) splats. At the same time, real-time requirements forced such implementations to accept compromises in how transparency of large assemblies of 3D Gaussians is resolved, in turn breaking coherence in other ways. In our work, we aim at achieving maximum coherence, by rendering fully perspective-correct 3D Gaussians while using a high-quality approximation of accurate blending, hybrid transparency, on a per-pixel level, in order to retain real-time frame rates. Our fast and perspectively accurate approach for evaluation of 3D Gaussians does not require matrix inversions, thereby ensuring numerical stability and eliminating the need for special handling of degenerate splats, and the hybrid transparency formulation for blending maintains similar quality as fully resolved per-pixel transparencies at a fraction of the rendering costs. We further show that each of these two components can be independently integrated into Gaussian splatting systems. In combination, they achieve up to 2$\\times$ higher frame rates, 2$\\times$ faster optimization, and equal or better image quality with fewer rendering artifacts compared to traditional 3DGS on common benchmarks.\n</div></div>\n    </div>\n  </div>\n</div>\n<div class=\"paper-row\" data-id=\"chu2024generalizable\" data-title=\"Generalizable and Animatable Gaussian Head Avatar\" data-authors=\"Xuangeng Chu, Tatsuya Harada\" data-year=\"2024\" data-tags='[\"Avatar\", \"Code\", \"Project\", \"Video\"]'>\n  <div class=\"paper-card\">\n    <input type=\"checkbox\" class=\"selection-checkbox\" onclick=\"handleCheckboxClick(event, 'chu2024generalizable', this)\">\n    <div class=\"paper-number\"></div>\n    <div class=\"paper-thumbnail\">\n      <img data-src=\"assets/thumbnails/chu2024generalizable.jpg\" data-fallback=\"None\" alt=\"Paper thumbnail for Generalizable and Animatable Gaussian Head Avatar\" class=\"lazy\" loading=\"lazy\"/>\n    </div>\n    <div class=\"paper-content\">\n      <h2 class=\"paper-title\">Generalizable and Animatable Gaussian Head Avatar <span class=\"paper-year\">(2024)</span></h2>\n      <p class=\"paper-authors\">Xuangeng Chu, Tatsuya Harada</p>\n      <div class=\"paper-tags\"><span class=\"paper-tag\">Avatar</span>\n<span class=\"paper-tag\">Code</span>\n<span class=\"paper-tag\">Project</span>\n<span class=\"paper-tag\">Video</span></div>\n      <div class=\"paper-links\"><a href=\"https://arxiv.org/pdf/2410.07971\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">📄 Paper</a>\n<a href=\"https://xg-chu.site/project_gagavatar/\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">🌐 Project</a>\n<a href=\"https://github.com/xg-chu/GAGAvatar\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">💻 Code</a>\n<a href=\"https://youtu.be/9244ZgOl4Xk\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">🎥 Video</a>\n<button class=\"abstract-toggle\" onclick=\"toggleAbstract(this)\">📖 Show Abstract</button>\n<div class=\"paper-abstract\">In this paper, we propose Generalizable and Animatable Gaussian head Avatar (GAGAvatar) for one-shot animatable head avatar reconstruction. Existing methods rely on neural radiance fields, leading to heavy rendering consumption and low reenactment speeds. To address these limitations, we generate the parameters of 3D Gaussians from a single image in a single forward pass. The key innovation of our work is the proposed dual-lifting method, which produces high-fidelity 3D Gaussians that capture identity and facial details. Additionally, we leverage global image features and the 3D morphable model to construct 3D Gaussians for controlling expressions. After training, our model can reconstruct unseen identities without specific optimizations and perform reenactment rendering at real-time speeds. Experiments show that our method exhibits superior performance compared to previous methods in terms of reconstruction quality and expression accuracy. We believe our method can establish new benchmarks for future research and advance applications of digital avatars.</div></div>\n    </div>\n  </div>\n</div>\n<div class=\"paper-row\" data-id=\"zhu2024motiongs\" data-title=\"MotionGS: Exploring Explicit Motion Guidance for Deformable 3D Gaussian Splatting\" data-authors=\"Ruijie Zhu*, Yanzhe Liang*, Hanzhi Chang, Jiacheng Deng, Jiahao Lu, Wenfei Yang, Tianzhu Zhang, Yongdong Zhang\" data-year=\"2024\" data-tags='[\"Dynamic\", \"Project\", \"Video\"]'>\n  <div class=\"paper-card\">\n    <input type=\"checkbox\" class=\"selection-checkbox\" onclick=\"handleCheckboxClick(event, 'zhu2024motiongs', this)\">\n    <div class=\"paper-number\"></div>\n    <div class=\"paper-thumbnail\">\n      <img data-src=\"assets/thumbnails/zhu2024motiongs.jpg\" data-fallback=\"None\" alt=\"Paper thumbnail for MotionGS: Exploring Explicit Motion Guidance for Deformable 3D Gaussian Splatting\" class=\"lazy\" loading=\"lazy\"/>\n    </div>\n    <div class=\"paper-content\">\n      <h2 class=\"paper-title\">MotionGS: Exploring Explicit Motion Guidance for Deformable 3D Gaussian Splatting <span class=\"paper-year\">(2024)</span></h2>\n      <p class=\"paper-authors\">Ruijie Zhu*, Yanzhe Liang*, Hanzhi Chang, Jiacheng Deng, Jiahao Lu, Wenfei Yang, Tianzhu Zhang, Yongdong Zhang</p>\n      <div class=\"paper-tags\"><span class=\"paper-tag\">Dynamic</span>\n<span class=\"paper-tag\">Project</span>\n<span class=\"paper-tag\">Video</span></div>\n      <div class=\"paper-links\"><a href=\"https://arxiv.org/pdf/2410.07707\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">📄 Paper</a>\n<a href=\"https://ruijiezhu94.github.io/MotionGS_page\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">🌐 Project</a>\n<a href=\"https://youtu.be/25DgViuuKFI\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">🎥 Video</a>\n<button class=\"abstract-toggle\" onclick=\"toggleAbstract(this)\">📖 Show Abstract</button>\n<div class=\"paper-abstract\">Dynamic scene reconstruction is a long-term challenge in the field of 3D vision. Recently, the emergence of 3D Gaussian Splatting has provided new insights into this problem. Although subsequent efforts rapidly extend static 3D Gaussian to dynamic scenes, they often lack explicit constraints on object motion, leading to optimization difficulties and performance degradation. To address the above issues, we propose a novel deformable 3D Gaussian splatting framework called MotionGS, which explores explicit motion priors to guide the deformation of 3D Gaussians. Specifically, we first introduce an optical flow decoupling module that decouples optical flow into camera flow and motion flow, corresponding to camera movement and object motion respectively. Then the motion flow can effectively constrain the deformation of 3D Gaussians, thus simulating the motion of dynamic objects. Additionally, a camera pose refinement module is proposed to alternately optimize 3D Gaussians and camera poses, mitigating the impact of inaccurate camera poses. Extensive experiments in the monocular dynamic scenes validate that MotionGS surpasses state-of-the-art methods and exhibits significant superiority in both qualitative and quantitative results.</div></div>\n    </div>\n  </div>\n</div>\n<div class=\"paper-row\" data-id=\"tang2024hisplat\" data-title=\"HiSplat: Hierarchical 3D Gaussian Splatting for Generalizable Sparse-View Reconstruction\" data-authors=\"Shengji Tang, Weicai Ye, Peng Ye, Weihao Lin, Yang Zhou, Tao Chen, Wanli Ouyang\" data-year=\"2024\" data-tags='[\"Code\", \"Feed-Forward\", \"Project\", \"Sparse\"]'>\n  <div class=\"paper-card\">\n    <input type=\"checkbox\" class=\"selection-checkbox\" onclick=\"handleCheckboxClick(event, 'tang2024hisplat', this)\">\n    <div class=\"paper-number\"></div>\n    <div class=\"paper-thumbnail\">\n      <img data-src=\"assets/thumbnails/tang2024hisplat.jpg\" data-fallback=\"None\" alt=\"Paper thumbnail for HiSplat: Hierarchical 3D Gaussian Splatting for Generalizable Sparse-View Reconstruction\" class=\"lazy\" loading=\"lazy\"/>\n    </div>\n    <div class=\"paper-content\">\n      <h2 class=\"paper-title\">HiSplat: Hierarchical 3D Gaussian Splatting for Generalizable Sparse-View Reconstruction <span class=\"paper-year\">(2024)</span></h2>\n      <p class=\"paper-authors\">Shengji Tang, Weicai Ye, Peng Ye, Weihao Lin, Yang Zhou, Tao Chen, Wanli Ouyang</p>\n      <div class=\"paper-tags\"><span class=\"paper-tag\">Code</span>\n<span class=\"paper-tag\">Feed-Forward</span>\n<span class=\"paper-tag\">Project</span>\n<span class=\"paper-tag\">Sparse</span></div>\n      <div class=\"paper-links\"><a href=\"https://arxiv.org/pdf/2410.06245.pdf\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">📄 Paper</a>\n<a href=\"https://open3dvlab.github.io/HiSplat/\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">🌐 Project</a>\n<a href=\"https://github.com/Open3DVLab/HiSplat\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">💻 Code</a>\n<button class=\"abstract-toggle\" onclick=\"toggleAbstract(this)\">📖 Show Abstract</button>\n<div class=\"paper-abstract\">Reconstructing 3D scenes from multiple viewpoints is a fundamental task in stereo vision. Recently, advances in generalizable 3D Gaussian Splatting have enabled high-quality novel view synthesis for unseen scenes from sparse input views by feed-forward predicting per-pixel Gaussian parameters without extra optimization. However, existing methods typically generate single-scale 3D Gaussians, which lack representation of both large-scale structure and texture details, resulting in mislocation and artefacts. In this paper, we propose a novel framework, HiSplat, which introduces a hierarchical manner in generalizable 3D Gaussian Splatting to construct hierarchical 3D Gaussians via a coarse-to-fine strategy. Specifically, HiSplat generates large coarse-grained Gaussians to capture large-scale structures, followed by fine-grained Gaussians to enhance delicate texture details. To promote inter-scale interactions, we propose an Error Aware Module for Gaussian compensation and a Modulating Fusion Module for Gaussian repair. Our method achieves joint optimization of hierarchical representations, allowing for novel view synthesis using only two-view reference images. Comprehensive experiments on various datasets demonstrate that HiSplat significantly enhances reconstruction quality and cross-dataset generalization compared to prior single-scale methods. The corresponding ablation study and analysis of different-scale 3D Gaussians reveal the mechanism behind the effectiveness. Project website: https://open3dvlab.github.io/HiSplat/\n</div></div>\n    </div>\n  </div>\n</div>\n<div class=\"paper-row\" data-id=\"liu2024splatraj\" data-title=\"SplaTraj: Camera Trajectory Generation with Semantic Gaussian Splatting\" data-authors=\"Xinyi Liu, Tianyi Zhang, Matthew Johnson-Roberson, Weiming Zhi\" data-year=\"2024\" data-tags='[\"Language Embedding\", \"Rendering\", \"Video\"]'>\n  <div class=\"paper-card\">\n    <input type=\"checkbox\" class=\"selection-checkbox\" onclick=\"handleCheckboxClick(event, 'liu2024splatraj', this)\">\n    <div class=\"paper-number\"></div>\n    <div class=\"paper-thumbnail\">\n      <img data-src=\"assets/thumbnails/liu2024splatraj.jpg\" data-fallback=\"None\" alt=\"Paper thumbnail for SplaTraj: Camera Trajectory Generation with Semantic Gaussian Splatting\" class=\"lazy\" loading=\"lazy\"/>\n    </div>\n    <div class=\"paper-content\">\n      <h2 class=\"paper-title\">SplaTraj: Camera Trajectory Generation with Semantic Gaussian Splatting <span class=\"paper-year\">(2024)</span></h2>\n      <p class=\"paper-authors\">Xinyi Liu, Tianyi Zhang, Matthew Johnson-Roberson, Weiming Zhi</p>\n      <div class=\"paper-tags\"><span class=\"paper-tag\">Language Embedding</span>\n<span class=\"paper-tag\">Rendering</span>\n<span class=\"paper-tag\">Video</span></div>\n      <div class=\"paper-links\"><a href=\"https://arxiv.org/pdf/2410.06014\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">📄 Paper</a>\n<a href=\"https://youtu.be/PUXNBfpeZkg\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">🎥 Video</a>\n<button class=\"abstract-toggle\" onclick=\"toggleAbstract(this)\">📖 Show Abstract</button>\n<div class=\"paper-abstract\">Many recent developments for robots to represent environments have focused on photorealistic reconstructions. This paper particularly focuses on generating sequences of images from the photorealistic Gaussian Splatting models, that match instructions that are given by user-inputted language. We contribute a novel framework, SplaTraj, which formulates the generation of images within photorealistic environment representations as a continuous-time trajectory optimization problem. Costs are designed so that a camera following the trajectory poses will smoothly traverse through the environment and render the specified spatial information in a photogenic manner. This is achieved by querying a photorealistic representation with language embedding to isolate regions that correspond to the user-specified inputs. These regions are then projected to the camera's view as it moves over time and a cost is constructed. We can then apply gradient-based optimization and differentiate through the rendering to optimize the trajectory for the defined cost. The resulting trajectory moves to photogenically view each of the specified objects. We empirically evaluate our approach on a suite of environments and instructions, and demonstrate the quality of generated image sequences.</div></div>\n    </div>\n  </div>\n</div>\n<div class=\"paper-row\" data-id=\"zhang2024monst3r\" data-title=\"MonST3R: A Simple Approach for Estimating Geometry in the Presence of Motion\" data-authors=\"Junyi Zhang, Charles Herrmann, Junhwa Hur, Varun Jampani, Trevor Darrell, Forrester Cole, Deqing Sun, Ming-Hsuan Yang\" data-year=\"2024\" data-tags='[\"3ster-based\", \"Code\", \"Dynamic\", \"Project\", \"Sparse\", \"Video\"]'>\n  <div class=\"paper-card\">\n    <input type=\"checkbox\" class=\"selection-checkbox\" onclick=\"handleCheckboxClick(event, 'zhang2024monst3r', this)\">\n    <div class=\"paper-number\"></div>\n    <div class=\"paper-thumbnail\">\n      <img data-src=\"assets/thumbnails/zhang2024monst3r.jpg\" data-fallback=\"None\" alt=\"Paper thumbnail for MonST3R: A Simple Approach for Estimating Geometry in the Presence of Motion\" class=\"lazy\" loading=\"lazy\"/>\n    </div>\n    <div class=\"paper-content\">\n      <h2 class=\"paper-title\">MonST3R: A Simple Approach for Estimating Geometry in the Presence of Motion <span class=\"paper-year\">(2024)</span></h2>\n      <p class=\"paper-authors\">Junyi Zhang, Charles Herrmann, Junhwa Hur, Varun Jampani, Trevor Darrell, Forrester Cole, Deqing Sun, Ming-Hsuan Yang</p>\n      <div class=\"paper-tags\"><span class=\"paper-tag\">3ster-based</span>\n<span class=\"paper-tag\">Code</span>\n<span class=\"paper-tag\">Dynamic</span>\n<span class=\"paper-tag\">Project</span>\n<span class=\"paper-tag\">Sparse</span>\n<span class=\"paper-tag\">Video</span></div>\n      <div class=\"paper-links\"><a href=\"https://arxiv.org/pdf/2410.03825.pdf\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">📄 Paper</a>\n<a href=\"https://monst3r-project.github.io/\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">🌐 Project</a>\n<a href=\"https://github.com/Junyi42/monst3r\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">💻 Code</a>\n<a href=\"https://monst3r-project.github.io/files/teaser_vid_v2_lowres.mp4\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">🎥 Video</a>\n<button class=\"abstract-toggle\" onclick=\"toggleAbstract(this)\">📖 Show Abstract</button>\n<div class=\"paper-abstract\">Estimating geometry from dynamic scenes, where objects move and deform over time, remains a core challenge in computer vision. Current approaches often rely on multi-stage pipelines or global optimizations that decompose the problem into subtasks, like depth and flow, leading to complex systems prone to errors. In this paper, we present Motion DUSt3R (MonST3R), a novel geometry-first approach that directly estimates per-timestep geometry from dynamic scenes. Our key insight is that by simply estimating a pointmap for each timestep, we can effectively adapt DUST3R's representation, previously only used for static scenes, to dynamic scenes. However, this approach presents a significant challenge: the scarcity of suitable training data, namely dynamic, posed videos with depth labels. Despite this, we show that by posing the problem as a fine-tuning task, identifying several suitable datasets, and strategically training the model on this limited data, we can surprisingly enable the model to handle dynamics, even without an explicit motion representation. Based on this, we introduce new optimizations for several downstream video-specific tasks and demonstrate strong performance on video depth and camera pose estimation, outperforming prior work in terms of robustness and efficiency. Moreover, MonST3R shows promising results for primarily feed-forward 4D reconstruction.\n</div></div>\n    </div>\n  </div>\n</div>\n<div class=\"paper-row\" data-id=\"cao20243dgsdet\" data-title=\"3DGS-DET: Empower 3D Gaussian Splatting with Boundary Guidance and Box-Focused Sampling for 3D Object Detection\" data-authors=\"Yang Cao, Yuanliang Jv, Dan Xu\" data-year=\"2024\" data-tags='[\"Object Detection\"]'>\n  <div class=\"paper-card\">\n    <input type=\"checkbox\" class=\"selection-checkbox\" onclick=\"handleCheckboxClick(event, 'cao20243dgsdet', this)\">\n    <div class=\"paper-number\"></div>\n    <div class=\"paper-thumbnail\">\n      <img data-src=\"assets/thumbnails/cao20243dgsdet.jpg\" data-fallback=\"None\" alt=\"Paper thumbnail for 3DGS-DET: Empower 3D Gaussian Splatting with Boundary Guidance and Box-Focused Sampling for 3D Object Detection\" class=\"lazy\" loading=\"lazy\"/>\n    </div>\n    <div class=\"paper-content\">\n      <h2 class=\"paper-title\">3DGS-DET: Empower 3D Gaussian Splatting with Boundary Guidance and Box-Focused Sampling for 3D Object Detection <span class=\"paper-year\">(2024)</span></h2>\n      <p class=\"paper-authors\">Yang Cao, Yuanliang Jv, Dan Xu</p>\n      <div class=\"paper-tags\"><span class=\"paper-tag\">Object Detection</span></div>\n      <div class=\"paper-links\"><a href=\"https://arxiv.org/pdf/2410.01647\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">📄 Paper</a>\n<button class=\"abstract-toggle\" onclick=\"toggleAbstract(this)\">📖 Show Abstract</button>\n<div class=\"paper-abstract\">Neural Radiance Fields (NeRF) are widely used for novel-view synthesis and have been adapted for 3D Object Detection (3DOD), offering a promising approach to 3D object detection through view-synthesis representation. However, NeRF faces inherent limitations: (i) It has limited representational capacity for 3DOD due to its implicit nature, and (ii) it suffers from slow rendering speeds. Recently, 3D Gaussian Splatting (3DGS) has emerged as an explicit 3D representation that addresses these limitations with faster rendering capabilities. Inspired by these advantages, this paper introduces 3DGS into 3DOD for the first time, identifying two main challenges: (i) Ambiguous spatial distribution of Gaussian blobs – 3DGS primarily relies on 2D pixel-level supervision, resulting in unclear 3D spatial distribution of Gaussian blobs and poor differentiation between objects and background, which hinders 3DOD; (ii) Excessive background blobs – 2D images often include numerous background pixels, leading to densely reconstructed 3DGS with many noisy Gaussian blobs representing the background, negatively affecting detection. To tackle the challenge (i), we leverage the fact that 3DGS reconstruction is derived from 2D images, and propose an elegant and efficient solution by incorporating 2D Boundary Guidance to significantly enhance the spatial distribution of Gaussian blobs, resulting in clearer differentiation between objects and their background (see Fig. 1). To address the challenge (ii), we propose a Box-Focused Sampling strategy using 2D boxes to generate object probability distribution in 3D spaces, allowing effective probabilistic sampling in 3D to retain more object blobs and reduce noisy background blobs. Benefiting from the proposed Boundary Guidance and Box-Focused Sampling, our final method, 3DGS-DET, achieves significant improvements (+5.6 on mAP@0.25, +3.7 on mAP@0.5) over our basic pipeline version, without introducing any additional learnable parameters. Furthermore, 3DGS-DET significantly outperforms the state-of-the-art NeRF-based method, NeRF-Det, achieving improvements of +6.6 on mAP@0.25 and +8.1 on mAP@0.5 for the ScanNet dataset, and impressive +31.5 on mAP@0.25 for the ARKITScenes dataset. Codes and models are publicly available at: https://github.com/yangcaoai/3DGS-DET.</div></div>\n    </div>\n  </div>\n</div>\n<div class=\"paper-row\" data-id=\"duisterhof2024mast3rsfm\" data-title=\"MASt3R-SfM: a Fully-Integrated Solution for Unconstrained Structure-from-Motion\" data-authors=\"Bardienus Duisterhof, Lojze Zust, Philippe Weinzaepfel, Vincent Leroy, Yohann Cabon, Jerome Revaud\" data-year=\"2024\" data-tags='[\"Code\"]'>\n  <div class=\"paper-card\">\n    <input type=\"checkbox\" class=\"selection-checkbox\" onclick=\"handleCheckboxClick(event, 'duisterhof2024mast3rsfm', this)\">\n    <div class=\"paper-number\"></div>\n    <div class=\"paper-thumbnail\">\n      <img data-src=\"assets/thumbnails/duisterhof2024mast3rsfm.jpg\" data-fallback=\"None\" alt=\"Paper thumbnail for MASt3R-SfM: a Fully-Integrated Solution for Unconstrained Structure-from-Motion\" class=\"lazy\" loading=\"lazy\"/>\n    </div>\n    <div class=\"paper-content\">\n      <h2 class=\"paper-title\">MASt3R-SfM: a Fully-Integrated Solution for Unconstrained Structure-from-Motion <span class=\"paper-year\">(2024)</span></h2>\n      <p class=\"paper-authors\">Bardienus Duisterhof, Lojze Zust, Philippe Weinzaepfel, Vincent Leroy, Yohann Cabon, Jerome Revaud</p>\n      <div class=\"paper-tags\"><span class=\"paper-tag\">Code</span></div>\n      <div class=\"paper-links\"><a href=\"https://arxiv.org/pdf/2409.19152.pdf\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">📄 Paper</a>\n<a href=\"https://github.com/naver/mast3r\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">💻 Code</a>\n<button class=\"abstract-toggle\" onclick=\"toggleAbstract(this)\">📖 Show Abstract</button>\n<div class=\"paper-abstract\">Structure-from-Motion (SfM), a task aiming at jointly recovering camera poses and 3D geometry of a scene given a set of images, remains a hard problem with still many open challenges despite decades of significant progress. The traditional solution for SfM consists of a complex pipeline of minimal solvers which tends to propagate errors and fails when images do not sufficiently overlap, have too little motion, etc. Recent methods have attempted to revisit this paradigm, but we empirically show that they fall short of fixing these core issues. In this paper, we propose instead to build upon a recently released foundation model for 3D vision that can robustly produce local 3D reconstructions and accurate matches. We introduce a low-memory approach to accurately align these local reconstructions in a global coordinate system. We further show that such foundation models can serve as efficient image retrievers without any overhead, reducing the overall complexity from quadratic to linear. Overall, our novel SfM pipeline is simple, scalable, fast and truly unconstrained, i.e. it can handle any collection of images, ordered or not. Extensive experiments on multiple benchmarks show that our method provides steady performance across diverse settings, especially outperforming existing methods in small- and medium-scale settings.\n</div></div>\n    </div>\n  </div>\n</div>\n<div class=\"paper-row\" data-id=\"yu2024languageembedded\" data-title=\"Language-Embedded Gaussian Splats (LEGS): Incrementally Building Room-Scale Representations with a Mobile Robot\" data-authors=\"Justin Yu, Kush Hari, Kishore Srinivas, Karim El-Refai, Adam Rashid, Chung Min Kim, Justin Kerr, Richard Cheng, Muhammad Zubair Irshad, Ashwin Balakrishna, Thomas Kollar, Ken Goldberg\" data-year=\"2024\" data-tags='[\"Code\", \"Language Embedding\", \"Project\", \"Robotics\", \"Segmentation\"]'>\n  <div class=\"paper-card\">\n    <input type=\"checkbox\" class=\"selection-checkbox\" onclick=\"handleCheckboxClick(event, 'yu2024languageembedded', this)\">\n    <div class=\"paper-number\"></div>\n    <div class=\"paper-thumbnail\">\n      <img data-src=\"assets/thumbnails/yu2024languageembedded.jpg\" data-fallback=\"None\" alt=\"Paper thumbnail for Language-Embedded Gaussian Splats (LEGS): Incrementally Building Room-Scale Representations with a Mobile Robot\" class=\"lazy\" loading=\"lazy\"/>\n    </div>\n    <div class=\"paper-content\">\n      <h2 class=\"paper-title\">Language-Embedded Gaussian Splats (LEGS): Incrementally Building Room-Scale Representations with a Mobile Robot <span class=\"paper-year\">(2024)</span></h2>\n      <p class=\"paper-authors\">Justin Yu, Kush Hari, Kishore Srinivas, Karim El-Refai, Adam Rashid, Chung Min Kim, Justin Kerr, Richard Cheng, Muhammad Zubair Irshad, Ashwin Balakrishna, Thomas Kollar, Ken Goldberg</p>\n      <div class=\"paper-tags\"><span class=\"paper-tag\">Code</span>\n<span class=\"paper-tag\">Language Embedding</span>\n<span class=\"paper-tag\">Project</span>\n<span class=\"paper-tag\">Robotics</span>\n<span class=\"paper-tag\">Segmentation</span></div>\n      <div class=\"paper-links\"><a href=\"https://arxiv.org/pdf/2409.18108\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">📄 Paper</a>\n<a href=\"https://berkeleyautomation.github.io/LEGS/\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">🌐 Project</a>\n<a href=\"https://github.com/BerkeleyAutomation/L3GS\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">💻 Code</a>\n<button class=\"abstract-toggle\" onclick=\"toggleAbstract(this)\">📖 Show Abstract</button>\n<div class=\"paper-abstract\">Building semantic 3D maps is valuable for searching for objects of interest in offices, warehouses, stores, and homes. We present a mapping system that incrementally builds a Language-Embedded Gaussian Splat (LEGS): a detailed 3D scene representation that encodes both appearance and semantics in a unified representation. LEGS is trained online as a robot traverses its environment to enable localization of open-vocabulary object queries. We evaluate LEGS on 4 room-scale scenes where we query for objects in the scene to assess how LEGS can capture semantic meaning. We compare LEGS to LERF and find that while both systems have comparable object query success rates, LEGS trains over 3.5x faster than LERF. Results suggest that a multi-camera setup and incremental bundle adjustment can boost visual reconstruction quality in constrained robot trajectories, and suggest LEGS can localize open-vocabulary and long-tail object queries with up to 66% accuracy.</div></div>\n    </div>\n  </div>\n</div>\n<div class=\"paper-row\" data-id=\"wang2024v3\" data-title=\"V^3: Viewing Volumetric Videos on Mobiles via Streamable 2D Dynamic Gaussians\" data-authors=\"Penghao Wang, Zhirui Zhang, Liao Wang, Kaixin Yao, Siyuan Xie, Jingyi Yu, Minye Wu, Lan Xu\" data-year=\"2024\" data-tags='[\"Avatar\", \"Code\", \"Project\", \"Video\"]'>\n  <div class=\"paper-card\">\n    <input type=\"checkbox\" class=\"selection-checkbox\" onclick=\"handleCheckboxClick(event, 'wang2024v3', this)\">\n    <div class=\"paper-number\"></div>\n    <div class=\"paper-thumbnail\">\n      <img data-src=\"assets/thumbnails/wang2024v3.jpg\" data-fallback=\"None\" alt=\"Paper thumbnail for V^3: Viewing Volumetric Videos on Mobiles via Streamable 2D Dynamic Gaussians\" class=\"lazy\" loading=\"lazy\"/>\n    </div>\n    <div class=\"paper-content\">\n      <h2 class=\"paper-title\">V^3: Viewing Volumetric Videos on Mobiles via Streamable 2D Dynamic Gaussians <span class=\"paper-year\">(2024)</span></h2>\n      <p class=\"paper-authors\">Penghao Wang, Zhirui Zhang, Liao Wang, Kaixin Yao, Siyuan Xie, Jingyi Yu, Minye Wu, Lan Xu</p>\n      <div class=\"paper-tags\"><span class=\"paper-tag\">Avatar</span>\n<span class=\"paper-tag\">Code</span>\n<span class=\"paper-tag\">Project</span>\n<span class=\"paper-tag\">Video</span></div>\n      <div class=\"paper-links\"><a href=\"https://arxiv.org/pdf/2409.13648\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">📄 Paper</a>\n<a href=\"https://authoritywang.github.io/v3/\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">🌐 Project</a>\n<a href=\"https://github.com/AuthorityWang/VideoGS\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">💻 Code</a>\n<a href=\"https://youtu.be/Z5La9AporRU?si=iJ-m_mvUSxQN4Bwm\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">🎥 Video</a>\n<button class=\"abstract-toggle\" onclick=\"toggleAbstract(this)\">📖 Show Abstract</button>\n<div class=\"paper-abstract\">Experiencing high-fidelity volumetric video as seamlessly as 2D videos is a long-held dream. However, current dynamic 3DGS methods, despite their high rendering quality, face challenges in streaming on mobile devices due to computational and bandwidth constraints. In this paper, we introduce V^3 (Viewing Volumetric Videos), a novel approach that enables high-quality mobile rendering through the streaming of dynamic Gaussians. Our key innovation is to view dynamic 3DGS as 2D videos, facilitating the use of hardware video codecs. Additionally, we propose a two-stage training strategy to reduce storage requirements with rapid training speed. The first stage employs hash encoding and shallow MLP to learn motion, then reduces the number of Gaussians through pruning to meet the streaming requirements, while the second stage fine tunes other Gaussian attributes using residual entropy loss and temporal loss to improve temporal continuity. This strategy, which disentangles motion and appearance, maintains high rendering quality with compact storage requirements. Meanwhile, we designed a multi-platform player to decode and render 2D Gaussian videos. Extensive experiments demonstrate the effectiveness of V^3, outperforming other methods by enabling high-quality rendering and streaming on common devices, which is unseen before. As the first to stream dynamic Gaussians on mobile devices, our companion player offers users an unprecedented volumetric video experience, including smooth scrolling and instant sharing. Our project page with source code is available at this https URL.</div></div>\n    </div>\n  </div>\n</div>\n<div class=\"paper-row\" data-id=\"chelani2024edgegaussians\" data-title=\"EdgeGaussians -- 3D Edge Mapping via Gaussian Splatting\" data-authors=\"Kunal Chelani, Assia Benbihi, Torsten Sattler, Fredrik Kahl\" data-year=\"2024\" data-tags='[\"Code\", \"Rendering\"]'>\n  <div class=\"paper-card\">\n    <input type=\"checkbox\" class=\"selection-checkbox\" onclick=\"handleCheckboxClick(event, 'chelani2024edgegaussians', this)\">\n    <div class=\"paper-number\"></div>\n    <div class=\"paper-thumbnail\">\n      <img data-src=\"assets/thumbnails/chelani2024edgegaussians.jpg\" data-fallback=\"None\" alt=\"Paper thumbnail for EdgeGaussians -- 3D Edge Mapping via Gaussian Splatting\" class=\"lazy\" loading=\"lazy\"/>\n    </div>\n    <div class=\"paper-content\">\n      <h2 class=\"paper-title\">EdgeGaussians -- 3D Edge Mapping via Gaussian Splatting <span class=\"paper-year\">(2024)</span></h2>\n      <p class=\"paper-authors\">Kunal Chelani, Assia Benbihi, Torsten Sattler, Fredrik Kahl</p>\n      <div class=\"paper-tags\"><span class=\"paper-tag\">Code</span>\n<span class=\"paper-tag\">Rendering</span></div>\n      <div class=\"paper-links\"><a href=\"https://arxiv.org/pdf/2409.12886.pdf\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">📄 Paper</a>\n<a href=\"https://github.com/kunalchelani/EdgeGaussians\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">💻 Code</a>\n<button class=\"abstract-toggle\" onclick=\"toggleAbstract(this)\">📖 Show Abstract</button>\n<div class=\"paper-abstract\">With their meaningful geometry and their omnipresence in the 3D world, edges are extremely useful primitives in computer vision. 3D edges comprise of lines and curves, and methods to reconstruct them use either multi-view images or point clouds as input. State-of-the-art image-based methods first learn a 3D edge point cloud then fit 3D edges to it. The edge point cloud is obtained by learning a 3D neural implicit edge field from which the 3D edge points are sampled on a specific level set (0 or 1). However, such methods present two important drawbacks: i) it is not realistic to sample points on exact level sets due to float imprecision and training inaccuracies. Instead, they are sampled within a range of levels so the points do not lie accurately on the 3D edges and require further processing. ii) Such implicit representations are computationally expensive and require long training times. In this paper, we address these two limitations and propose a 3D edge mapping that is simpler, more efficient, and preserves accuracy. Our method learns explicitly the 3D edge points and their edge direction hence bypassing the need for point sampling. It casts a 3D edge point as the center of a 3D Gaussian and the edge direction as the principal axis of the Gaussian. Such a representation has the advantage of being not only geometrically meaningful but also compatible with the efficient training optimization defined in Gaussian Splatting. Results show that the proposed method produces edges as accurate and complete as the state-of-the-art while being an order of magnitude faster.\n</div></div>\n    </div>\n  </div>\n</div>\n<div class=\"paper-row\" data-id=\"joseph2024gradientdriven\" data-title=\"Gradient-Driven 3D Segmentation and Affordance Transfer in Gaussian Splatting Using 2D Masks\" data-authors=\"Joji Joseph, Bharadwaj Amrutur, Shalabh Bhatnagar\" data-year=\"2024\" data-tags='[\"Code\", \"Project\", \"Segmentation\"]'>\n  <div class=\"paper-card\">\n    <input type=\"checkbox\" class=\"selection-checkbox\" onclick=\"handleCheckboxClick(event, 'joseph2024gradientdriven', this)\">\n    <div class=\"paper-number\"></div>\n    <div class=\"paper-thumbnail\">\n      <img data-src=\"assets/thumbnails/joseph2024gradientdriven.jpg\" data-fallback=\"None\" alt=\"Paper thumbnail for Gradient-Driven 3D Segmentation and Affordance Transfer in Gaussian Splatting Using 2D Masks\" class=\"lazy\" loading=\"lazy\"/>\n    </div>\n    <div class=\"paper-content\">\n      <h2 class=\"paper-title\">Gradient-Driven 3D Segmentation and Affordance Transfer in Gaussian Splatting Using 2D Masks <span class=\"paper-year\">(2024)</span></h2>\n      <p class=\"paper-authors\">Joji Joseph, Bharadwaj Amrutur, Shalabh Bhatnagar</p>\n      <div class=\"paper-tags\"><span class=\"paper-tag\">Code</span>\n<span class=\"paper-tag\">Project</span>\n<span class=\"paper-tag\">Segmentation</span></div>\n      <div class=\"paper-links\"><a href=\"https://arxiv.org/pdf/2409.11681\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">📄 Paper</a>\n<a href=\"https://jojijoseph.github.io/3dgs-segmentation/\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">🌐 Project</a>\n<a href=\"https://github.com/JojiJoseph/3dgs-gradient-segmentation\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">💻 Code</a>\n<button class=\"abstract-toggle\" onclick=\"toggleAbstract(this)\">📖 Show Abstract</button>\n<div class=\"paper-abstract\">In this paper, we introduce a novel voting-based method that extends 2D segmentation models to 3D Gaussian splats. Our approach leverages masked gradients, where gradients are filtered by input 2D masks, and these gradients are used as votes to achieve accurate segmentation. As a byproduct, we found that inference-time gradients can also be used to prune Gaussians, resulting in up to 21% compression. Additionally, we explore few-shot affordance transfer, allowing annotations from 2D images to be effectively transferred onto 3D Gaussian splats. The robust yet straightforward mathematical formulation underlying this approach makes it a highly effective tool for numerous downstream applications, such as augmented reality (AR), object editing, and robotics.</div></div>\n    </div>\n  </div>\n</div>\n<div class=\"paper-row\" data-id=\"mihajlovic2024splatfields\" data-title=\"SplatFields: Neural Gaussian Splats for Sparse 3D and 4D Reconstruction\" data-authors=\"Marko Mihajlovic, Sergey Prokudin, Siyu Tang, Robert Maier, Federica Bogo, Tony Tung, Edmond Boyer\" data-year=\"2024\" data-tags='[\"Code\", \"Project\", \"Sparse\"]'>\n  <div class=\"paper-card\">\n    <input type=\"checkbox\" class=\"selection-checkbox\" onclick=\"handleCheckboxClick(event, 'mihajlovic2024splatfields', this)\">\n    <div class=\"paper-number\"></div>\n    <div class=\"paper-thumbnail\">\n      <img data-src=\"assets/thumbnails/mihajlovic2024splatfields.jpg\" data-fallback=\"None\" alt=\"Paper thumbnail for SplatFields: Neural Gaussian Splats for Sparse 3D and 4D Reconstruction\" class=\"lazy\" loading=\"lazy\"/>\n    </div>\n    <div class=\"paper-content\">\n      <h2 class=\"paper-title\">SplatFields: Neural Gaussian Splats for Sparse 3D and 4D Reconstruction <span class=\"paper-year\">(2024)</span></h2>\n      <p class=\"paper-authors\">Marko Mihajlovic, Sergey Prokudin, Siyu Tang, Robert Maier, Federica Bogo, Tony Tung, Edmond Boyer</p>\n      <div class=\"paper-tags\"><span class=\"paper-tag\">Code</span>\n<span class=\"paper-tag\">Project</span>\n<span class=\"paper-tag\">Sparse</span></div>\n      <div class=\"paper-links\"><a href=\"https://arxiv.org/pdf/2409.11211.pdf\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">📄 Paper</a>\n<a href=\"https://markomih.github.io/SplatFields/\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">🌐 Project</a>\n<a href=\"https://github.com/markomih/SplatFields\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">💻 Code</a>\n<button class=\"abstract-toggle\" onclick=\"toggleAbstract(this)\">📖 Show Abstract</button>\n<div class=\"paper-abstract\">Digitizing 3D static scenes and 4D dynamic events from multi-view images has long been a challenge in computer vision and graphics. Recently, 3D Gaussian Splatting (3DGS) has emerged as a practical and scalable reconstruction method, gaining popularity due to its impressive reconstruction quality, real-time rendering capabilities, and compatibility with widely used visualization tools. However, the method requires a substantial number of input views to achieve high-quality scene reconstruction, introducing a significant practical bottleneck. This challenge is especially severe in capturing dynamic scenes, where deploying an extensive camera array can be prohibitively costly. In this work, we identify the lack of spatial autocorrelation of splat features as one of the factors contributing to the suboptimal performance of the 3DGS technique in sparse reconstruction settings. To address the issue, we propose an optimization strategy that effectively regularizes splat features by modeling them as the outputs of a corresponding implicit neural field. This results in a consistent enhancement of reconstruction quality across various scenarios. Our approach effectively handles static and dynamic cases, as demonstrated by extensive testing across different setups and scene complexities.</div></div>\n    </div>\n  </div>\n</div>\n<div class=\"paper-row\" data-id=\"meng2024beings\" data-title=\"BEINGS: Bayesian Embodied Image-goal Navigation with Gaussian Splatting\" data-authors=\"Wugang Meng, Tianfu Wu, Huan Yin, Fumin Zhang\" data-year=\"2024\" data-tags='[\"Autonomous Driving\", \"Code\", \"Project\", \"Robotics\"]'>\n  <div class=\"paper-card\">\n    <input type=\"checkbox\" class=\"selection-checkbox\" onclick=\"handleCheckboxClick(event, 'meng2024beings', this)\">\n    <div class=\"paper-number\"></div>\n    <div class=\"paper-thumbnail\">\n      <img data-src=\"assets/thumbnails/meng2024beings.jpg\" data-fallback=\"None\" alt=\"Paper thumbnail for BEINGS: Bayesian Embodied Image-goal Navigation with Gaussian Splatting\" class=\"lazy\" loading=\"lazy\"/>\n    </div>\n    <div class=\"paper-content\">\n      <h2 class=\"paper-title\">BEINGS: Bayesian Embodied Image-goal Navigation with Gaussian Splatting <span class=\"paper-year\">(2024)</span></h2>\n      <p class=\"paper-authors\">Wugang Meng, Tianfu Wu, Huan Yin, Fumin Zhang</p>\n      <div class=\"paper-tags\"><span class=\"paper-tag\">Autonomous Driving</span>\n<span class=\"paper-tag\">Code</span>\n<span class=\"paper-tag\">Project</span>\n<span class=\"paper-tag\">Robotics</span></div>\n      <div class=\"paper-links\"><a href=\"https://arxiv.org/pdf/2409.10216\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">📄 Paper</a>\n<a href=\"https://www.mwg.ink/BEINGS-web\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">🌐 Project</a>\n<a href=\"https://github.com/guaMass/BEINGS\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">💻 Code</a>\n<button class=\"abstract-toggle\" onclick=\"toggleAbstract(this)\">📖 Show Abstract</button>\n<div class=\"paper-abstract\">Image-goal navigation enables a robot to reach the location where a target image was captured, using visual cues for guidance. However, current methods either rely heavily on data and computationally expensive learning-based approaches or lack efficiency in complex environments due to insufficient exploration strategies. To address these limitations, we propose Bayesian Embodied Image-goal Navigation Using Gaussian Splatting, a novel method that formulates ImageNav as an optimal control problem within a model predictive control framework. BEINGS leverages 3D Gaussian Splatting as a scene prior to predict future observations, enabling efficient, real-time navigation decisions grounded in the robot’s sensory experiences. By integrating Bayesian updates, our method dynamically refines the robot's strategy without requiring extensive prior experience or data. Our algorithm is validated through extensive simulations and physical experiments, showcasing its potential for embodied robot systems in visually complex scenarios.</div></div>\n    </div>\n  </div>\n</div>\n<div class=\"paper-row\" data-id=\"li2024pshuman\" data-title=\"PSHuman: Photorealistic Single-view Human Reconstruction using Cross-Scale Diffusion\" data-authors=\"Peng Li, Wangguandong Zheng, Yuan Liu, Tao Yu, Yangguang Li, Xingqun Qi, Mengfei Li, Xiaowei Chi, Siyu Xia, Wei Xue, Wenhan Luo, Qifeng Liu, Yike Guo\" data-year=\"2024\" data-tags='[\"Avatar\", \"Code\", \"Diffusion\", \"Meshing\", \"Project\"]'>\n  <div class=\"paper-card\">\n    <input type=\"checkbox\" class=\"selection-checkbox\" onclick=\"handleCheckboxClick(event, 'li2024pshuman', this)\">\n    <div class=\"paper-number\"></div>\n    <div class=\"paper-thumbnail\">\n      <img data-src=\"assets/thumbnails/li2024pshuman.jpg\" data-fallback=\"None\" alt=\"Paper thumbnail for PSHuman: Photorealistic Single-view Human Reconstruction using Cross-Scale Diffusion\" class=\"lazy\" loading=\"lazy\"/>\n    </div>\n    <div class=\"paper-content\">\n      <h2 class=\"paper-title\">PSHuman: Photorealistic Single-view Human Reconstruction using Cross-Scale Diffusion <span class=\"paper-year\">(2024)</span></h2>\n      <p class=\"paper-authors\">Peng Li, Wangguandong Zheng, Yuan Liu, Tao Yu, Yangguang Li, Xingqun Qi, Mengfei Li, Xiaowei Chi, Siyu Xia, Wei Xue, Wenhan Luo, Qifeng Liu, Yike Guo</p>\n      <div class=\"paper-tags\"><span class=\"paper-tag\">Avatar</span>\n<span class=\"paper-tag\">Code</span>\n<span class=\"paper-tag\">Diffusion</span>\n<span class=\"paper-tag\">Meshing</span>\n<span class=\"paper-tag\">Project</span></div>\n      <div class=\"paper-links\"><a href=\"https://arxiv.org/pdf/2409.10141.pdf\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">📄 Paper</a>\n<a href=\"https://penghtyx.github.io/PSHuman/\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">🌐 Project</a>\n<a href=\"https://github.com/pengHTYX/PSHuman\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">💻 Code</a>\n<button class=\"abstract-toggle\" onclick=\"toggleAbstract(this)\">📖 Show Abstract</button>\n<div class=\"paper-abstract\">Detailed and photorealistic 3D human modeling is essential for various applications and has seen tremendous progress. However, full-body reconstruction from a monocular RGB image remains challenging due to the ill-posed nature of the problem and sophisticated clothing topology with self-occlusions. In this paper, we propose PSHuman, a novel framework that explicitly reconstructs human meshes utilizing priors from the multiview diffusion model. It is found that directly applying multiview diffusion on single-view human images leads to severe geometric distortions, especially on generated faces. To address it, we propose a cross-scale diffusion that models the joint probability distribution of global full-body shape and local facial characteristics, enabling detailed and identity-preserved novel-view generation without any geometric distortion. Moreover, to enhance cross-view body shape consistency of varied human poses, we condition the generative model on parametric models like SMPL-X, which provide body priors and prevent unnatural views inconsistent with human anatomy. Leveraging the generated multi-view normal and color images, we present SMPLX-initialized explicit human carving to recover realistic textured human meshes efficiently. Extensive experimental results and quantitative evaluations on CAPE and THuman2.1 datasets demonstrate PSHumans superiority in geometry details, texture fidelity, and generalization capability.\n</div></div>\n    </div>\n  </div>\n</div>\n<div class=\"paper-row\" data-id=\"jiang2024dualgs\" data-title=\"DualGS: Robust Dual Gaussian Splatting for Immersive Human-centric Volumetric Videos\" data-authors=\"Yuheng Jiang, Zhehao Shen, Yu Hong, Chengcheng Guo, Yize Wu, Yingliang Zhang, Jingyi Yu, Lan Xu\" data-year=\"2024\" data-tags='[\"Avatar\", \"Project\", \"Video\"]'>\n  <div class=\"paper-card\">\n    <input type=\"checkbox\" class=\"selection-checkbox\" onclick=\"handleCheckboxClick(event, 'jiang2024dualgs', this)\">\n    <div class=\"paper-number\"></div>\n    <div class=\"paper-thumbnail\">\n      <img data-src=\"assets/thumbnails/jiang2024dualgs.jpg\" data-fallback=\"None\" alt=\"Paper thumbnail for DualGS: Robust Dual Gaussian Splatting for Immersive Human-centric Volumetric Videos\" class=\"lazy\" loading=\"lazy\"/>\n    </div>\n    <div class=\"paper-content\">\n      <h2 class=\"paper-title\">DualGS: Robust Dual Gaussian Splatting for Immersive Human-centric Volumetric Videos <span class=\"paper-year\">(2024)</span></h2>\n      <p class=\"paper-authors\">Yuheng Jiang, Zhehao Shen, Yu Hong, Chengcheng Guo, Yize Wu, Yingliang Zhang, Jingyi Yu, Lan Xu</p>\n      <div class=\"paper-tags\"><span class=\"paper-tag\">Avatar</span>\n<span class=\"paper-tag\">Project</span>\n<span class=\"paper-tag\">Video</span></div>\n      <div class=\"paper-links\"><a href=\"https://arxiv.org/pdf/2409.08353\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">📄 Paper</a>\n<a href=\"https://nowheretrix.github.io/DualGS/\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">🌐 Project</a>\n<a href=\"https://www.youtube.com/watch?v=vwDE8xr78Bg\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">🎥 Video</a>\n<button class=\"abstract-toggle\" onclick=\"toggleAbstract(this)\">📖 Show Abstract</button>\n<div class=\"paper-abstract\">Volumetric video represents a transformative advancement in visual media, enabling users to freely navigate immersive virtual experiences and narrowing the gap between digital and real worlds. However, the need for extensive manual intervention to stabilize mesh sequences and the generation of excessively large assets in existing workflows impedes broader adoption. In this paper, we present a novel Gaussian-based approach, dubbed \\textit{DualGS}, for real-time and high-fidelity playback of complex human performance with excellent compression ratios. Our key idea in DualGS is to separately represent motion and appearance using the corresponding skin and joint Gaussians. Such an explicit disentanglement can significantly reduce motion redundancy and enhance temporal coherence. We begin by initializing the DualGS and anchoring skin Gaussians to joint Gaussians at the first frame. Subsequently, we employ a coarse-to-fine training strategy for frame-by-frame human performance modeling. It includes a coarse alignment phase for overall motion prediction as well as a fine-grained optimization for robust tracking and high-fidelity rendering. To integrate volumetric video seamlessly into VR environments, we efficiently compress motion using entropy encoding and appearance using codec compression coupled with a persistent codebook. Our approach achieves a compression ratio of up to 120 times, only requiring approximately 350KB of storage per frame. We demonstrate the efficacy of our representation through photo-realistic, free-view experiences on VR headsets, enabling users to immersively watch musicians in performance and feel the rhythm of the notes at the performers' fingertips.</div></div>\n    </div>\n  </div>\n</div>\n<div class=\"paper-row\" data-id=\"liao2024fisheyegs\" data-title=\"Fisheye-GS: Lightweight and Extensible Gaussian Splatting Module for Fisheye Cameras\" data-authors=\"Zimu Liao, Siyan Chen, Rong Fu, Yi Wang, Zhongling Su, Hao Luo, Li Ma, Linning Xu, Bo Dai, Hengjie Li, Zhilin Pei, Xingcheng Zhang\" data-year=\"2024\" data-tags='[\"Code\", \"Rendering\"]'>\n  <div class=\"paper-card\">\n    <input type=\"checkbox\" class=\"selection-checkbox\" onclick=\"handleCheckboxClick(event, 'liao2024fisheyegs', this)\">\n    <div class=\"paper-number\"></div>\n    <div class=\"paper-thumbnail\">\n      <img data-src=\"assets/thumbnails/liao2024fisheyegs.jpg\" data-fallback=\"None\" alt=\"Paper thumbnail for Fisheye-GS: Lightweight and Extensible Gaussian Splatting Module for Fisheye Cameras\" class=\"lazy\" loading=\"lazy\"/>\n    </div>\n    <div class=\"paper-content\">\n      <h2 class=\"paper-title\">Fisheye-GS: Lightweight and Extensible Gaussian Splatting Module for Fisheye Cameras <span class=\"paper-year\">(2024)</span></h2>\n      <p class=\"paper-authors\">Zimu Liao, Siyan Chen, Rong Fu, Yi Wang, Zhongling Su, Hao Luo, Li Ma, Linning Xu, Bo Dai, Hengjie Li, Zhilin Pei, Xingcheng Zhang</p>\n      <div class=\"paper-tags\"><span class=\"paper-tag\">Code</span>\n<span class=\"paper-tag\">Rendering</span></div>\n      <div class=\"paper-links\"><a href=\"https://arxiv.org/pdf/2409.04751.pdf\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">📄 Paper</a>\n<a href=\"https://github.com/zmliao/Fisheye-GS\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">💻 Code</a>\n<button class=\"abstract-toggle\" onclick=\"toggleAbstract(this)\">📖 Show Abstract</button>\n<div class=\"paper-abstract\">Recently, 3D Gaussian Splatting (3DGS) has garnered attention for its high fidelity and real-time rendering. However, adapting 3DGS to different camera models, particularly fisheye lenses, poses challenges due to the unique 3D to 2D projection calculation. Additionally, there are inefficiencies in the tile-based splatting, especially for the extreme curvature and wide field of view of fisheye lenses, which are crucial for its broader real-life applications. To tackle these challenges, we introduce Fisheye-GS.This innovative method recalculates the projection transformation and its gradients for fisheye cameras. Our approach can be seamlessly integrated as a module into other efficient 3D rendering methods, emphasizing its extensibility, lightweight nature, and modular design. Since we only modified the projection component, it can also be easily adapted for use with different camera models. Compared to methods that train after undistortion, our approach demonstrates a clear improvement in visual quality.\n</div></div>\n    </div>\n  </div>\n</div>\n<div class=\"paper-row\" data-id=\"chen2024omnire\" data-title=\"OmniRe: Omni Urban Scene Reconstruction\" data-authors=\"Ziyu Chen, Jiawei Yang, Jiahui Huang, Riccardo de Lutio, Janick Martinez Esturo, Boris Ivanovic, Or Litany, Zan Gojcic, Sanja Fidler, Marco Pavone, Li Song, Yue Wang\" data-year=\"2024\" data-tags='[\"Autonomous Driving\", \"Code\", \"Project\"]'>\n  <div class=\"paper-card\">\n    <input type=\"checkbox\" class=\"selection-checkbox\" onclick=\"handleCheckboxClick(event, 'chen2024omnire', this)\">\n    <div class=\"paper-number\"></div>\n    <div class=\"paper-thumbnail\">\n      <img data-src=\"assets/thumbnails/chen2024omnire.jpg\" data-fallback=\"None\" alt=\"Paper thumbnail for OmniRe: Omni Urban Scene Reconstruction\" class=\"lazy\" loading=\"lazy\"/>\n    </div>\n    <div class=\"paper-content\">\n      <h2 class=\"paper-title\">OmniRe: Omni Urban Scene Reconstruction <span class=\"paper-year\">(2024)</span></h2>\n      <p class=\"paper-authors\">Ziyu Chen, Jiawei Yang, Jiahui Huang, Riccardo de Lutio, Janick Martinez Esturo, Boris Ivanovic, Or Litany, Zan Gojcic, Sanja Fidler, Marco Pavone, Li Song, Yue Wang</p>\n      <div class=\"paper-tags\"><span class=\"paper-tag\">Autonomous Driving</span>\n<span class=\"paper-tag\">Code</span>\n<span class=\"paper-tag\">Project</span></div>\n      <div class=\"paper-links\"><a href=\"https://arxiv.org/pdf/2408.16760\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">📄 Paper</a>\n<a href=\"https://ziyc.github.io/omnire/\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">🌐 Project</a>\n<a href=\"https://github.com/ziyc/drivestudio\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">💻 Code</a>\n<button class=\"abstract-toggle\" onclick=\"toggleAbstract(this)\">📖 Show Abstract</button>\n<div class=\"paper-abstract\">We introduce OmniRe, a holistic approach for efficiently reconstructing high-fidelity dynamic urban scenes from on-device logs. Recent methods for modeling driving sequences using neural radiance fields or Gaussian Splatting have demonstrated the potential of reconstructing challenging dynamic scenes, but often overlook pedestrians and other non-vehicle dynamic actors, hindering a complete pipeline for dynamic urban scene reconstruction. To that end, we propose a comprehensive 3DGS framework for driving scenes, named OmniRe, that allows for accurate, full-length reconstruction of diverse dynamic objects in a driving log. OmniRe builds dynamic neural scene graphs based on Gaussian representations and constructs multiple local canonical spaces that model various dynamic actors, including vehicles, pedestrians, and cyclists, among many others. This capability is unmatched by existing methods. OmniRe allows us to holistically reconstruct different objects present in the scene, subsequently enabling the simulation of reconstructed scenarios with all actors participating in real-time (~60Hz). Extensive evaluations on the Waymo dataset show that our approach outperforms prior state-of-the-art methods quantitatively and qualitatively by a large margin. We believe our work fills a critical gap in driving reconstruction.</div></div>\n    </div>\n  </div>\n</div>\n<div class=\"paper-row\" data-id=\"wang20243d\" data-title=\"3D Reconstruction with Spatial Memory\" data-authors=\"Hengyi Wang, Lourdes Agapito\" data-year=\"2024\" data-tags='[\"3ster-based\", \"Code\", \"Project\", \"SLAM\", \"Video\"]'>\n  <div class=\"paper-card\">\n    <input type=\"checkbox\" class=\"selection-checkbox\" onclick=\"handleCheckboxClick(event, 'wang20243d', this)\">\n    <div class=\"paper-number\"></div>\n    <div class=\"paper-thumbnail\">\n      <img data-src=\"assets/thumbnails/wang20243d.jpg\" data-fallback=\"None\" alt=\"Paper thumbnail for 3D Reconstruction with Spatial Memory\" class=\"lazy\" loading=\"lazy\"/>\n    </div>\n    <div class=\"paper-content\">\n      <h2 class=\"paper-title\">3D Reconstruction with Spatial Memory <span class=\"paper-year\">(2024)</span></h2>\n      <p class=\"paper-authors\">Hengyi Wang, Lourdes Agapito</p>\n      <div class=\"paper-tags\"><span class=\"paper-tag\">3ster-based</span>\n<span class=\"paper-tag\">Code</span>\n<span class=\"paper-tag\">Project</span>\n<span class=\"paper-tag\">SLAM</span>\n<span class=\"paper-tag\">Video</span></div>\n      <div class=\"paper-links\"><a href=\"https://arxiv.org/pdf/2408.16061.pdf\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">📄 Paper</a>\n<a href=\"https://hengyiwang.github.io/projects/spanner\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">🌐 Project</a>\n<a href=\"https://github.com/HengyiWang/spann3r\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">💻 Code</a>\n<a href=\"https://hengyiwang.github.io/projects/spanner/videos/spanner_intro.mp4\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">🎥 Video</a>\n<button class=\"abstract-toggle\" onclick=\"toggleAbstract(this)\">📖 Show Abstract</button>\n<div class=\"paper-abstract\">We present Spann3R, a novel approach for dense 3D reconstruction from ordered or unordered image collections. Built on the DUSt3R paradigm, Spann3R uses a transformer-based architecture to directly regress pointmaps from images without any prior knowledge of the scene or camera parameters. Unlike DUSt3R, which predicts per image-pair pointmaps each expressed in its local coordinate frame, Spann3R can predict per-image pointmaps expressed in a global coordinate system, thus eliminating the need for optimization-based global alignment. The key idea of Spann3R is to manage an external spatial memory that learns to keep track of all previous relevant 3D information. Spann3R then queries this spatial memory to predict the 3D structure of the next frame in a global coordinate system. Taking advantage of DUSt3R's pre-trained weights, and further fine-tuning on a subset of datasets, Spann3R shows competitive performance and generalization ability on various unseen datasets and can process ordered image collections in real time. Project page: \\url{https://hengyiwang.github.io/projects/spanner}\n</div></div>\n    </div>\n  </div>\n</div>\n<div class=\"paper-row\" data-id=\"shi2024lapisgs\" data-title=\"LapisGS: Layered Progressive 3D Gaussian Splatting for Adaptive Streaming\" data-authors=\"Yuang Shi, Simone Gasparini, Géraldine Morin, Wei Tsang Ooi,\" data-year=\"2024\" data-tags='[\"Compression\", \"Project\"]'>\n  <div class=\"paper-card\">\n    <input type=\"checkbox\" class=\"selection-checkbox\" onclick=\"handleCheckboxClick(event, 'shi2024lapisgs', this)\">\n    <div class=\"paper-number\"></div>\n    <div class=\"paper-thumbnail\">\n      <img data-src=\"assets/thumbnails/shi2024lapisgs.jpg\" data-fallback=\"None\" alt=\"Paper thumbnail for LapisGS: Layered Progressive 3D Gaussian Splatting for Adaptive Streaming\" class=\"lazy\" loading=\"lazy\"/>\n    </div>\n    <div class=\"paper-content\">\n      <h2 class=\"paper-title\">LapisGS: Layered Progressive 3D Gaussian Splatting for Adaptive Streaming <span class=\"paper-year\">(2024)</span></h2>\n      <p class=\"paper-authors\">Yuang Shi, Simone Gasparini, Géraldine Morin, Wei Tsang Ooi,</p>\n      <div class=\"paper-tags\"><span class=\"paper-tag\">Compression</span>\n<span class=\"paper-tag\">Project</span></div>\n      <div class=\"paper-links\"><a href=\"https://arxiv.org/pdf/2408.14823\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">📄 Paper</a>\n<a href=\"https://yuang-ian.github.io/lapisgs/\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">🌐 Project</a>\n<button class=\"abstract-toggle\" onclick=\"toggleAbstract(this)\">📖 Show Abstract</button>\n<div class=\"paper-abstract\">The rise of Extended Reality (XR) requires efficient streaming of 3D online worlds, challenging current 3DGS representations to adapt to bandwidth-constrained environments. We propose LapisGS, a layered 3DGS that supports adaptive streaming and progressive rendering. Our method constructs a layered structure for cumulative representation, incorporates dynamic opacity optimization to maintain visual fidelity, and utilizes occupancy maps to efficiently manage Gaussian splats. This proposed model offers a progressive representation supporting a continuous rendering quality adapted for bandwidth-aware streaming. Extensive experiments validate the effectiveness of our approach in balancing visual fidelity with the compactness of the model, with up to 50.71% improvement in SSIM, 286.53% improvement in LPIPS, and 318.41% reduction in model size, and shows its potential for bandwidth-adapted 3D streaming and rendering applications.</div></div>\n    </div>\n  </div>\n</div>\n<div class=\"paper-row\" data-id=\"smart2024splatt3r\" data-title=\"Splatt3R: Zero-shot Gaussian Splatting from Uncalibrated Image Pairs\" data-authors=\"Brandon Smart, Chuanxia Zheng, Iro Laina, Victor Adrian Prisacariu\" data-year=\"2024\" data-tags='[\"3ster-based\", \"Code\", \"Project\", \"Rendering\"]'>\n  <div class=\"paper-card\">\n    <input type=\"checkbox\" class=\"selection-checkbox\" onclick=\"handleCheckboxClick(event, 'smart2024splatt3r', this)\">\n    <div class=\"paper-number\"></div>\n    <div class=\"paper-thumbnail\">\n      <img data-src=\"assets/thumbnails/smart2024splatt3r.jpg\" data-fallback=\"None\" alt=\"Paper thumbnail for Splatt3R: Zero-shot Gaussian Splatting from Uncalibrated Image Pairs\" class=\"lazy\" loading=\"lazy\"/>\n    </div>\n    <div class=\"paper-content\">\n      <h2 class=\"paper-title\">Splatt3R: Zero-shot Gaussian Splatting from Uncalibrated Image Pairs <span class=\"paper-year\">(2024)</span></h2>\n      <p class=\"paper-authors\">Brandon Smart, Chuanxia Zheng, Iro Laina, Victor Adrian Prisacariu</p>\n      <div class=\"paper-tags\"><span class=\"paper-tag\">3ster-based</span>\n<span class=\"paper-tag\">Code</span>\n<span class=\"paper-tag\">Project</span>\n<span class=\"paper-tag\">Rendering</span></div>\n      <div class=\"paper-links\"><a href=\"https://arxiv.org/pdf/2408.13912.pdf\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">📄 Paper</a>\n<a href=\"https://splatt3r.active.vision/\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">🌐 Project</a>\n<a href=\"https://github.com/btsmart/splatt3r\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">💻 Code</a>\n<button class=\"abstract-toggle\" onclick=\"toggleAbstract(this)\">📖 Show Abstract</button>\n<div class=\"paper-abstract\">In this paper, we introduce Splatt3R, a pose-free, feed-forward method for in-the-wild 3D reconstruction and novel view synthesis from stereo pairs. Given uncalibrated natural images, Splatt3R can predict 3D Gaussian Splats without requiring any camera parameters or depth information. For generalizability, we build Splatt3R upon a ``foundation'' 3D geometry reconstruction method, MASt3R, by extending it to deal with both 3D structure and appearance. Specifically, unlike the original MASt3R which reconstructs only 3D point clouds, we predict the additional Gaussian attributes required to construct a Gaussian primitive for each point. Hence, unlike other novel view synthesis methods, Splatt3R is first trained by optimizing the 3D point cloud's geometry loss, and then a novel view synthesis objective. By doing this, we avoid the local minima present in training 3D Gaussian Splats from stereo views. We also propose a novel loss masking strategy that we empirically find is critical for strong performance on extrapolated viewpoints. We train Splatt3R on the ScanNet++ dataset and demonstrate excellent generalisation to uncalibrated, in-the-wild images. Splatt3R can reconstruct scenes at 4FPS at 512 x 512 resolution, and the resultant splats can be rendered in real-time.\n</div></div>\n    </div>\n  </div>\n</div>\n<div class=\"paper-row\" data-id=\"zhang202425\" data-title=\"'25] 10. TranSplat: Generalizable 3D Gaussian Splatting from Sparse Multi-View Images with Transformers\" data-authors=\"Chuanrui Zhang, Yingshuang Zou, Zhuoling Li, Minmin Yi, Haoqian Wang\" data-year=\"2024\" data-tags='[\"Code\", \"Feed-Forward\", \"Project\", \"Sparse\", \"Transformer\"]'>\n  <div class=\"paper-card\">\n    <input type=\"checkbox\" class=\"selection-checkbox\" onclick=\"handleCheckboxClick(event, 'zhang202425', this)\">\n    <div class=\"paper-number\"></div>\n    <div class=\"paper-thumbnail\">\n      <img data-src=\"assets/thumbnails/zhang202425.jpg\" data-fallback=\"None\" alt=\"Paper thumbnail for '25] 10. TranSplat: Generalizable 3D Gaussian Splatting from Sparse Multi-View Images with Transformers\" class=\"lazy\" loading=\"lazy\"/>\n    </div>\n    <div class=\"paper-content\">\n      <h2 class=\"paper-title\">'25] 10. TranSplat: Generalizable 3D Gaussian Splatting from Sparse Multi-View Images with Transformers <span class=\"paper-year\">(2024)</span></h2>\n      <p class=\"paper-authors\">Chuanrui Zhang, Yingshuang Zou, Zhuoling Li, Minmin Yi, Haoqian Wang</p>\n      <div class=\"paper-tags\"><span class=\"paper-tag\">Code</span>\n<span class=\"paper-tag\">Feed-Forward</span>\n<span class=\"paper-tag\">Project</span>\n<span class=\"paper-tag\">Sparse</span>\n<span class=\"paper-tag\">Transformer</span></div>\n      <div class=\"paper-links\"><a href=\"https://arxiv.org/pdf/2408.13770\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">📄 Paper</a>\n<a href=\"https://xingyoujun.github.io/transplat/\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">🌐 Project</a>\n<a href=\"https://github.com/xingyoujun/transplat\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">💻 Code</a>\n<button class=\"abstract-toggle\" onclick=\"toggleAbstract(this)\">📖 Show Abstract</button>\n<div class=\"paper-abstract\">Compared with previous 3D reconstruction methods like Nerf, recent Generalizable 3D Gaussian Splatting (G-3DGS) methods demonstrate impressive efficiency even in the sparse-view setting. However, the promising reconstruction performance of existing G-3DGS methods relies heavily on accurate multi-view feature matching, which is quite challenging. Especially for the scenes that have many non-overlapping areas between various views and contain numerous similar regions, the matching performance of existing methods is poor and the reconstruction precision is limited. To address this problem, we develop a strategy that utilizes a predicted depth confidence map to guide accurate local feature matching. In addition, we propose to utilize the knowledge of existing monocular depth estimation models as prior to boost the depth estimation precision in non-overlapping areas between views. Combining the proposed strategies, we present a novel G-3DGS method named TranSplat, which obtains the best performance on both the RealEstate10K and ACID benchmarks while maintaining competitive speed and presenting strong cross-dataset generalization ability.</div></div>\n    </div>\n  </div>\n</div>\n<div class=\"paper-row\" data-id=\"liu2024gsloc\" data-title=\"GSLoc: Efficient Camera Pose Refinement via 3D Gaussian Splatting\" data-authors=\"Changkun Liu, Shuai Chen, Yash Bhalgat, Siyan Hu, Ming Cheng, Zirui Wang, Victor Adrian Prisacariu, Tristan Braud\" data-year=\"2024\" data-tags='[\"Poses\", \"Project\"]'>\n  <div class=\"paper-card\">\n    <input type=\"checkbox\" class=\"selection-checkbox\" onclick=\"handleCheckboxClick(event, 'liu2024gsloc', this)\">\n    <div class=\"paper-number\"></div>\n    <div class=\"paper-thumbnail\">\n      <img data-src=\"assets/thumbnails/liu2024gsloc.jpg\" data-fallback=\"None\" alt=\"Paper thumbnail for GSLoc: Efficient Camera Pose Refinement via 3D Gaussian Splatting\" class=\"lazy\" loading=\"lazy\"/>\n    </div>\n    <div class=\"paper-content\">\n      <h2 class=\"paper-title\">GSLoc: Efficient Camera Pose Refinement via 3D Gaussian Splatting <span class=\"paper-year\">(2024)</span></h2>\n      <p class=\"paper-authors\">Changkun Liu, Shuai Chen, Yash Bhalgat, Siyan Hu, Ming Cheng, Zirui Wang, Victor Adrian Prisacariu, Tristan Braud</p>\n      <div class=\"paper-tags\"><span class=\"paper-tag\">Poses</span>\n<span class=\"paper-tag\">Project</span></div>\n      <div class=\"paper-links\"><a href=\"https://arxiv.org/pdf/2408.11085\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">📄 Paper</a>\n<a href=\"https://gsloc.active.vision/\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">🌐 Project</a>\n<button class=\"abstract-toggle\" onclick=\"toggleAbstract(this)\">📖 Show Abstract</button>\n<div class=\"paper-abstract\">We leverage 3D Gaussian Splatting (3DGS) as a scene representation and propose a novel test-time camera pose refinement framework, GSLoc. This framework enhances the localization accuracy of state-of-the-art absolute pose regression and scene coordinate regression methods. The 3DGS model renders high-quality synthetic images and depth maps to facilitate the establishment of 2D-3D correspondences. GSLoc obviates the need for training feature extractors or descriptors by operating directly on RGB images, utilizing the 3D foundation model, MASt3R, for precise 2D matching. To improve the robustness of our model in challenging outdoor environments, we incorporate an exposure-adaptive module within the 3DGS framework. Consequently, GSLoc enables efficient one-shot pose refinement given a single RGB query and a coarse initial pose estimation. Our proposed approach surpasses leading NeRF-based optimization methods in both accuracy and runtime across indoor and outdoor visual localization benchmarks, achieving new state-of-the-art accuracy on two indoor datasets.</div></div>\n    </div>\n  </div>\n</div>\n<div class=\"paper-row\" data-id=\"zhu2024loopsplat\" data-title=\"LoopSplat: Loop Closure by Registering 3D Gaussian Splats\" data-authors=\"Liyuan Zhu, Yue Li, Erik Sandström, Shengyu Huang, Konrad Schindler, Iro Armeni\" data-year=\"2024\" data-tags='[\"Code\", \"Project\", \"SLAM\"]'>\n  <div class=\"paper-card\">\n    <input type=\"checkbox\" class=\"selection-checkbox\" onclick=\"handleCheckboxClick(event, 'zhu2024loopsplat', this)\">\n    <div class=\"paper-number\"></div>\n    <div class=\"paper-thumbnail\">\n      <img data-src=\"assets/thumbnails/zhu2024loopsplat.jpg\" data-fallback=\"None\" alt=\"Paper thumbnail for LoopSplat: Loop Closure by Registering 3D Gaussian Splats\" class=\"lazy\" loading=\"lazy\"/>\n    </div>\n    <div class=\"paper-content\">\n      <h2 class=\"paper-title\">LoopSplat: Loop Closure by Registering 3D Gaussian Splats <span class=\"paper-year\">(2024)</span></h2>\n      <p class=\"paper-authors\">Liyuan Zhu, Yue Li, Erik Sandström, Shengyu Huang, Konrad Schindler, Iro Armeni</p>\n      <div class=\"paper-tags\"><span class=\"paper-tag\">Code</span>\n<span class=\"paper-tag\">Project</span>\n<span class=\"paper-tag\">SLAM</span></div>\n      <div class=\"paper-links\"><a href=\"https://arxiv.org/pdf/2408.10154\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">📄 Paper</a>\n<a href=\"https://loopsplat.github.io/\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">🌐 Project</a>\n<a href=\"https://github.com/GradientSpaces/LoopSplat\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">💻 Code</a>\n<button class=\"abstract-toggle\" onclick=\"toggleAbstract(this)\">📖 Show Abstract</button>\n<div class=\"paper-abstract\">Simultaneous Localization and Mapping (SLAM) based on 3D Gaussian Splats (3DGS) has recently shown promise towards more accurate, dense 3D scene maps. However, existing 3DGS-based methods fail to address the global consistency of the scene via loop closure and/or global bundle adjustment. To this end, we propose LoopSplat, which takes RGB-D images as input and performs dense mapping with 3DGS submaps and frame-to-model tracking. LoopSplat triggers loop closure online and computes relative loop edge constraints between submaps directly via 3DGS registration, leading to improvements in efficiency and accuracy over traditional global-to-local point cloud registration. It uses a robust pose graph optimization formulation and rigidly aligns the submaps to achieve global consistency. Evaluation on the synthetic Replica and real-world TUM-RGBD, ScanNet, and ScanNet++ datasets demonstrates competitive or superior tracking, mapping, and rendering compared to existing methods for dense RGB-D SLAM.</div></div>\n    </div>\n  </div>\n</div>\n<div class=\"paper-row\" data-id=\"lee2024rethinking\" data-title=\"Rethinking Open-Vocabulary Segmentation of Radiance Fields in 3D Space\" data-authors=\"Hyunjee Lee*, Youngsik Yun*, Jeongmin Bae, Seoha Kim, Youngjung Uh\" data-year=\"2024\" data-tags='[\"Code\", \"Language Embedding\", \"Project\", \"Segmentation\"]'>\n  <div class=\"paper-card\">\n    <input type=\"checkbox\" class=\"selection-checkbox\" onclick=\"handleCheckboxClick(event, 'lee2024rethinking', this)\">\n    <div class=\"paper-number\"></div>\n    <div class=\"paper-thumbnail\">\n      <img data-src=\"assets/thumbnails/lee2024rethinking.jpg\" data-fallback=\"None\" alt=\"Paper thumbnail for Rethinking Open-Vocabulary Segmentation of Radiance Fields in 3D Space\" class=\"lazy\" loading=\"lazy\"/>\n    </div>\n    <div class=\"paper-content\">\n      <h2 class=\"paper-title\">Rethinking Open-Vocabulary Segmentation of Radiance Fields in 3D Space <span class=\"paper-year\">(2024)</span></h2>\n      <p class=\"paper-authors\">Hyunjee Lee*, Youngsik Yun*, Jeongmin Bae, Seoha Kim, Youngjung Uh</p>\n      <div class=\"paper-tags\"><span class=\"paper-tag\">Code</span>\n<span class=\"paper-tag\">Language Embedding</span>\n<span class=\"paper-tag\">Project</span>\n<span class=\"paper-tag\">Segmentation</span></div>\n      <div class=\"paper-links\"><a href=\"https://arxiv.org/pdf/2408.07416\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">📄 Paper</a>\n<a href=\"https://hyunji12.github.io/Open3DRF/\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">🌐 Project</a>\n<a href=\"https://github.com/hyunji12/Open3DRF\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">💻 Code</a>\n<button class=\"abstract-toggle\" onclick=\"toggleAbstract(this)\">📖 Show Abstract</button>\n<div class=\"paper-abstract\">Understanding the 3D semantics of a scene is a fundamental problem for various scenarios such as embodied agents. While NeRFs and 3DGS excel at novel-view synthesis, previous methods for understanding their semantics have been limited to incomplete 3D understanding: their segmentation results are 2D masks and their supervision is anchored at 2D pixels. This paper revisits the problem set to pursue a better 3D understanding of a scene modeled by NeRFs and 3DGS as follows. 1) We directly supervise the 3D points to train the language embedding field. It achieves state-of-the-art accuracy without relying on multi-scale language embeddings. 2) We transfer the pre-trained language field to 3DGS, achieving the first real-time rendering speed without sacrificing training time or accuracy. 3) We introduce a 3D querying and evaluation protocol for assessing the reconstructed geometry and semantics together. Code, checkpoints, and annotations will be available online.</div></div>\n    </div>\n  </div>\n</div>\n<div class=\"paper-row\" data-id=\"wildersmith2024radiance\" data-title=\"Radiance Fields for Robotic Teleoperation\" data-authors=\"Maximum Wilder-Smith, Vaishakh Patil, Marco Hutter\" data-year=\"2024\" data-tags='[\"Code\", \"Misc\", \"Project\", \"Robotics\"]'>\n  <div class=\"paper-card\">\n    <input type=\"checkbox\" class=\"selection-checkbox\" onclick=\"handleCheckboxClick(event, 'wildersmith2024radiance', this)\">\n    <div class=\"paper-number\"></div>\n    <div class=\"paper-thumbnail\">\n      <img data-src=\"assets/thumbnails/wildersmith2024radiance.jpg\" data-fallback=\"None\" alt=\"Paper thumbnail for Radiance Fields for Robotic Teleoperation\" class=\"lazy\" loading=\"lazy\"/>\n    </div>\n    <div class=\"paper-content\">\n      <h2 class=\"paper-title\">Radiance Fields for Robotic Teleoperation <span class=\"paper-year\">(2024)</span></h2>\n      <p class=\"paper-authors\">Maximum Wilder-Smith, Vaishakh Patil, Marco Hutter</p>\n      <div class=\"paper-tags\"><span class=\"paper-tag\">Code</span>\n<span class=\"paper-tag\">Misc</span>\n<span class=\"paper-tag\">Project</span>\n<span class=\"paper-tag\">Robotics</span></div>\n      <div class=\"paper-links\"><a href=\"https://arxiv.org/pdf/2407.20194\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">📄 Paper</a>\n<a href=\"https://leggedrobotics.github.io/rffr.github.io/\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">🌐 Project</a>\n<a href=\"https://github.com/leggedrobotics/radiance_field_ros\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">💻 Code</a>\n<button class=\"abstract-toggle\" onclick=\"toggleAbstract(this)\">📖 Show Abstract</button>\n<div class=\"paper-abstract\">Radiance field methods such as Neural Radiance Fields (NeRFs) or 3D Gaussian Splatting (3DGS), have revolutionized graphics and novel view synthesis. Their ability to synthesize new viewpoints with photo-realistic quality, as well as capture complex volumetric and specular scenes, makes them an ideal visualization for robotic teleoperation setups. Direct camera teleoperation provides high-fidelity operation at the cost of maneuverability, while reconstruction-based approaches offer controllable scenes with lower fidelity. With this in mind, we propose replacing the traditional reconstruction-visualization components of the robotic teleoperation pipeline with online Radiance Fields, offering highly maneuverable scenes with photorealistic quality. As such, there are three main contributions to state of the art: (1) online training of Radiance Fields using live data from multiple cameras, (2) support for a variety of radiance methods including NeRF and 3DGS, (3) visualization suite for these methods including a virtual reality scene. To enable seamless integration with existing setups, these components were tested with multiple robots in multiple configurations and were displayed using traditional tools as well as the VR headset. The results across methods and robots were compared quantitatively to a baseline of mesh reconstruction, and a user study was conducted to compare the different visualization methods.</div></div>\n    </div>\n  </div>\n</div>\n<div class=\"paper-row\" data-id=\"moenne-loccoz20243d\" data-title=\"3D Gaussian Ray Tracing: Fast Tracing of Particle Scenes\" data-authors=\"Nicolas Moenne-Loccoz, Ashkan Mirzaei, Or Perel, Riccardo de Lutio, Janick Martinez Esturo, Gavriel State, Sanja Fidler, Nicholas Sharp, Zan Gojcic\" data-year=\"2024\" data-tags='[\"Project\", \"Ray Tracing\", \"Video\"]'>\n  <div class=\"paper-card\">\n    <input type=\"checkbox\" class=\"selection-checkbox\" onclick=\"handleCheckboxClick(event, 'moenne-loccoz20243d', this)\">\n    <div class=\"paper-number\"></div>\n    <div class=\"paper-thumbnail\">\n      <img data-src=\"assets/thumbnails/moenne-loccoz20243d.jpg\" data-fallback=\"None\" alt=\"Paper thumbnail for 3D Gaussian Ray Tracing: Fast Tracing of Particle Scenes\" class=\"lazy\" loading=\"lazy\"/>\n    </div>\n    <div class=\"paper-content\">\n      <h2 class=\"paper-title\">3D Gaussian Ray Tracing: Fast Tracing of Particle Scenes <span class=\"paper-year\">(2024)</span></h2>\n      <p class=\"paper-authors\">Nicolas Moenne-Loccoz, Ashkan Mirzaei, Or Perel, Riccardo de Lutio, Janick Martinez Esturo, Gavriel State, Sanja Fidler, Nicholas Sharp, Zan Gojcic</p>\n      <div class=\"paper-tags\"><span class=\"paper-tag\">Project</span>\n<span class=\"paper-tag\">Ray Tracing</span>\n<span class=\"paper-tag\">Video</span></div>\n      <div class=\"paper-links\"><a href=\"https://arxiv.org/pdf/2407.07090.pdf\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">📄 Paper</a>\n<a href=\"https://gaussiantracer.github.io/\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">🌐 Project</a>\n<a href=\"https://gaussiantracer.github.io/res/3dgrt_supplementary_video.mp4\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">🎥 Video</a>\n<button class=\"abstract-toggle\" onclick=\"toggleAbstract(this)\">📖 Show Abstract</button>\n<div class=\"paper-abstract\">Particle-based representations of radiance fields such as 3D Gaussian Splatting have found great success for reconstructing and re-rendering of complex scenes. Most existing methods render particles via rasterization, projecting them to screen space tiles for processing in a sorted order. This work instead considers ray tracing the particles, building a bounding volume hierarchy and casting a ray for each pixel using high-performance GPU ray tracing hardware. To efficiently handle large numbers of semi-transparent particles, we describe a specialized rendering algorithm which encapsulates particles with bounding meshes to leverage fast ray-triangle intersections, and shades batches of intersections in depth-order. The benefits of ray tracing are well-known in computer graphics: processing incoherent rays for secondary lighting effects such as shadows and reflections, rendering from highly-distorted cameras common in robotics, stochastically sampling rays, and more. With our renderer, this flexibility comes at little cost compared to rasterization. Experiments demonstrate the speed and accuracy of our approach, as well as several applications in computer graphics and vision. We further propose related improvements to the basic Gaussian representation, including a simple use of generalized kernel functions which significantly reduces particle hit counts.\n</div></div>\n    </div>\n  </div>\n</div>\n<div class=\"paper-row\" data-id=\"ji2024segment\" data-title=\"Segment Any 4D Gaussians\" data-authors=\"Shengxiang Ji, Guanjun Wu, Jiemin Fang, Jiazhong Cen, Taoran Yi, Wenyu Liu, Qi Tian, Xinggang Wang\" data-year=\"2024\" data-tags='[\"Project\", \"Segmentation\"]'>\n  <div class=\"paper-card\">\n    <input type=\"checkbox\" class=\"selection-checkbox\" onclick=\"handleCheckboxClick(event, 'ji2024segment', this)\">\n    <div class=\"paper-number\"></div>\n    <div class=\"paper-thumbnail\">\n      <img data-src=\"assets/thumbnails/ji2024segment.jpg\" data-fallback=\"None\" alt=\"Paper thumbnail for Segment Any 4D Gaussians\" class=\"lazy\" loading=\"lazy\"/>\n    </div>\n    <div class=\"paper-content\">\n      <h2 class=\"paper-title\">Segment Any 4D Gaussians <span class=\"paper-year\">(2024)</span></h2>\n      <p class=\"paper-authors\">Shengxiang Ji, Guanjun Wu, Jiemin Fang, Jiazhong Cen, Taoran Yi, Wenyu Liu, Qi Tian, Xinggang Wang</p>\n      <div class=\"paper-tags\"><span class=\"paper-tag\">Project</span>\n<span class=\"paper-tag\">Segmentation</span></div>\n      <div class=\"paper-links\"><a href=\"https://arxiv.org/pdf/2407.04504\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">📄 Paper</a>\n<a href=\"https://jsxzs.github.io/sa4d/\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">🌐 Project</a>\n<button class=\"abstract-toggle\" onclick=\"toggleAbstract(this)\">📖 Show Abstract</button>\n<div class=\"paper-abstract\">Modeling, understanding, and reconstructing the real world are crucial in XR/VR. Recently, 3D Gaussian Splatting (3D-GS) methods have shown remarkable success in modeling and understanding 3D scenes. Similarly, various 4D representations have demonstrated the ability to capture the dynamics of the 4D world. However, there is a dearth of research focusing on segmentation within 4D representations. In this paper, we propose Segment Any 4D Gaussians (SA4D), one of the first frameworks to segment anything in the 4D digital world based on 4D Gaussians. In SA4D, an efficient temporal identity feature field is introduced to handle Gaussian drifting, with the potential to learn precise identity features from noisy and sparse input. Additionally, a 4D segmentation refinement process is proposed to remove artifacts. Our SA4D achieves precise, high-quality segmentation within seconds in 4D Gaussians and shows the ability to remove, recolor, compose, and render high-quality anything masks. More demos are available at: https://jsxzs.github.io/sa4d/.</div></div>\n    </div>\n  </div>\n</div>\n<div class=\"paper-row\" data-id=\"lee2024gscore\" data-title=\"GSCore: Efficient Radiance Field Rendering via Architectural Support for 3D Gaussian Splatting\" data-authors=\"Junseo Lee, Seokwon Lee, Jungi Lee, Junyong Park, Jaewoong Sim\" data-year=\"2024\" data-tags='[\"Acceleration\", \"Rendering\"]'>\n  <div class=\"paper-card\">\n    <input type=\"checkbox\" class=\"selection-checkbox\" onclick=\"handleCheckboxClick(event, 'lee2024gscore', this)\">\n    <div class=\"paper-number\"></div>\n    <div class=\"paper-thumbnail\">\n      <img data-src=\"assets/thumbnails/lee2024gscore.jpg\" data-fallback=\"None\" alt=\"Paper thumbnail for GSCore: Efficient Radiance Field Rendering via Architectural Support for 3D Gaussian Splatting\" class=\"lazy\" loading=\"lazy\"/>\n    </div>\n    <div class=\"paper-content\">\n      <h2 class=\"paper-title\">GSCore: Efficient Radiance Field Rendering via Architectural Support for 3D Gaussian Splatting <span class=\"paper-year\">(2024)</span></h2>\n      <p class=\"paper-authors\">Junseo Lee, Seokwon Lee, Jungi Lee, Junyong Park, Jaewoong Sim</p>\n      <div class=\"paper-tags\"><span class=\"paper-tag\">Acceleration</span>\n<span class=\"paper-tag\">Rendering</span></div>\n      <div class=\"paper-links\"><a href=\"https://jaewoong.org/pubs/asplos24-gscore.pdf\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">📄 Paper</a>\n<a href=\"https://youtu.be/TByYGw837IU?si=7zBe0yqpsJUoVbIV\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">🎥 Video</a>\n<button class=\"abstract-toggle\" onclick=\"toggleAbstract(this)\">📖 Show Abstract</button>\n<div class=\"paper-abstract\">This paper presents GSCore, a hardware acceleration unit that efficiently executes the rendering pipeline of 3D Gaussian Splatting with algorithmic optimizations. GSCore builds on the observations from an in-depth analysis of Gaussian-based radiance field rendering to enhance computational efficiency and bring the technique to wide adoption. In doing so, we present several optimization techniques, Gaussian shape-aware intersection test, hierarchical sorting, and subtile skipping, all of which are synergistically integrated with GSCore. We implement the hardware design of GSCore, synthesize it using a commercial 28nm technology, and evaluate the performance across a range of synthetic and real-world scenes with varying image resolutions. Our evaluation results show that GSCore achieves a 15.86× speedup on average over the mobile consumer GPU with a substantially smaller area and lower energy consumption.</div></div>\n    </div>\n  </div>\n</div>\n<div class=\"paper-row\" data-id=\"kim2024optimizing\" data-title=\"Optimizing Dynamic NeRF and 3DGS with No Video Synchronization\" data-authors=\"Seoha Kim*, Jeongmin Bae*, Youngsik Yun, HyunSeung Son, Hahyun Lee, Gun Bang, Youngjung Uh\" data-year=\"2024\" data-tags='[\"Dynamic\"]'>\n  <div class=\"paper-card\">\n    <input type=\"checkbox\" class=\"selection-checkbox\" onclick=\"handleCheckboxClick(event, 'kim2024optimizing', this)\">\n    <div class=\"paper-number\"></div>\n    <div class=\"paper-thumbnail\">\n      <img data-src=\"assets/thumbnails/kim2024optimizing.jpg\" data-fallback=\"None\" alt=\"Paper thumbnail for Optimizing Dynamic NeRF and 3DGS with No Video Synchronization\" class=\"lazy\" loading=\"lazy\"/>\n    </div>\n    <div class=\"paper-content\">\n      <h2 class=\"paper-title\">Optimizing Dynamic NeRF and 3DGS with No Video Synchronization <span class=\"paper-year\">(2024)</span></h2>\n      <p class=\"paper-authors\">Seoha Kim*, Jeongmin Bae*, Youngsik Yun, HyunSeung Son, Hahyun Lee, Gun Bang, Youngjung Uh</p>\n      <div class=\"paper-tags\"><span class=\"paper-tag\">Dynamic</span></div>\n      <div class=\"paper-links\"><a href=\"https://openreview.net/pdf?id=RQutkn4V9I\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">📄 Paper</a>\n<button class=\"abstract-toggle\" onclick=\"toggleAbstract(this)\">📖 Show Abstract</button>\n<div class=\"paper-abstract\">Recent advancements in 4D scene reconstruction using dynamic NeRF and 3DGS have demonstrated the ability to represent dynamic scenes from multi-view videos. However, they fail to reconstruct the dynamic scenes and struggle to fit even the training views in unsynchronized settings. It happens because they employ a single latent embedding for a frame, while the multi-view images at the same frame were actually captured at different moments. To address this limitation, we introduce time offsets for individual unsynchronized videos and jointly optimize the offsets with the field. By design, our method is applicable for various baselines, even regardless of the types of radiance fields. We conduct experiments on the common Plenoptic Video Dataset and a newly built Unsynchronized Dynamic Blender Dataset to verify the performance of our method.</div></div>\n    </div>\n  </div>\n</div>\n<div class=\"paper-row\" data-id=\"zhang2024egogaussian\" data-title=\"EgoGaussian: Dynamic Scene Understanding from Egocentric Video with 3D Gaussian Splatting\" data-authors=\"Daiwei Zhang, Gengyan Li, Jiajie Li, Mickaël Bressieux, Otmar Hilliges, Marc Pollefeys, Luc Van Gool, Xi Wang\" data-year=\"2024\" data-tags='[\"Code\", \"Dynamic\", \"Project\", \"Video\"]'>\n  <div class=\"paper-card\">\n    <input type=\"checkbox\" class=\"selection-checkbox\" onclick=\"handleCheckboxClick(event, 'zhang2024egogaussian', this)\">\n    <div class=\"paper-number\"></div>\n    <div class=\"paper-thumbnail\">\n      <img data-src=\"assets/thumbnails/zhang2024egogaussian.jpg\" data-fallback=\"None\" alt=\"Paper thumbnail for EgoGaussian: Dynamic Scene Understanding from Egocentric Video with 3D Gaussian Splatting\" class=\"lazy\" loading=\"lazy\"/>\n    </div>\n    <div class=\"paper-content\">\n      <h2 class=\"paper-title\">EgoGaussian: Dynamic Scene Understanding from Egocentric Video with 3D Gaussian Splatting <span class=\"paper-year\">(2024)</span></h2>\n      <p class=\"paper-authors\">Daiwei Zhang, Gengyan Li, Jiajie Li, Mickaël Bressieux, Otmar Hilliges, Marc Pollefeys, Luc Van Gool, Xi Wang</p>\n      <div class=\"paper-tags\"><span class=\"paper-tag\">Code</span>\n<span class=\"paper-tag\">Dynamic</span>\n<span class=\"paper-tag\">Project</span>\n<span class=\"paper-tag\">Video</span></div>\n      <div class=\"paper-links\"><a href=\"https://arxiv.org/pdf/2406.19811\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">📄 Paper</a>\n<a href=\"https://zdwww.github.io/egogs.github.io/\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">🌐 Project</a>\n<a href=\"https://github.com/zdwww/EgoGaussian\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">💻 Code</a>\n<a href=\"https://youtu.be/nsZrmM7CJB0?si=IJnfWH_Vf_UW2JoF\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">🎥 Video</a>\n<button class=\"abstract-toggle\" onclick=\"toggleAbstract(this)\">📖 Show Abstract</button>\n<div class=\"paper-abstract\">Human activities are inherently complex, often involving numerous object interactions. To better understand these activities, it is crucial to model their interactions with the environment captured through dynamic changes. The recent availability of affordable head-mounted cameras and egocentric data offers a more accessible and efficient means to understand human-object interactions in 3D environments. However, most existing methods for human activity modeling neglect the dynamic interactions with objects, resulting in only static representations. The few existing solutions often require inputs from multiple sources, including multi-camera setups, depth-sensing cameras, or kinesthetic sensors. To this end, we introduce EgoGaussian, the first method capable of simultaneously reconstructing 3D scenes and dynamically tracking 3D object motion from RGB egocentric input alone. We leverage the uniquely discrete nature of Gaussian Splatting and segment dynamic interactions from the background, with both having explicit representations. Our approach employs a clip-level online learning pipeline that leverages the dynamic nature of human activities, allowing us to reconstruct the temporal evolution of the scene in chronological order and track rigid object motion. EgoGaussian shows significant improvements in terms of both dynamic object and background reconstruction quality compared to the state-of-the-art. We also qualitatively demonstrate the high quality of the reconstructed models.</div></div>\n    </div>\n  </div>\n</div>\n<div class=\"paper-row\" data-id=\"zhao2024on\" data-title=\"On Scaling Up 3D Gaussian Splatting Training\" data-authors=\"Hexu Zhao, Haoyang Weng, Daohan Lu, Ang Li, Jinyang Li, Aurojit Panda, Saining Xie\" data-year=\"2024\" data-tags='[\"Code\", \"Distributed\", \"Large-Scale\", \"Project\", \"Video\"]'>\n  <div class=\"paper-card\">\n    <input type=\"checkbox\" class=\"selection-checkbox\" onclick=\"handleCheckboxClick(event, 'zhao2024on', this)\">\n    <div class=\"paper-number\"></div>\n    <div class=\"paper-thumbnail\">\n      <img data-src=\"assets/thumbnails/zhao2024on.jpg\" data-fallback=\"None\" alt=\"Paper thumbnail for On Scaling Up 3D Gaussian Splatting Training\" class=\"lazy\" loading=\"lazy\"/>\n    </div>\n    <div class=\"paper-content\">\n      <h2 class=\"paper-title\">On Scaling Up 3D Gaussian Splatting Training <span class=\"paper-year\">(2024)</span></h2>\n      <p class=\"paper-authors\">Hexu Zhao, Haoyang Weng, Daohan Lu, Ang Li, Jinyang Li, Aurojit Panda, Saining Xie</p>\n      <div class=\"paper-tags\"><span class=\"paper-tag\">Code</span>\n<span class=\"paper-tag\">Distributed</span>\n<span class=\"paper-tag\">Large-Scale</span>\n<span class=\"paper-tag\">Project</span>\n<span class=\"paper-tag\">Video</span></div>\n      <div class=\"paper-links\"><a href=\"https://arxiv.org/pdf/2406.18533\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">📄 Paper</a>\n<a href=\"https://daohanlu.github.io/scaling-up-3dgs/\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">🌐 Project</a>\n<a href=\"https://github.com/nyu-systems/Grendel-GS\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">💻 Code</a>\n<a href=\"https://youtu.be/WaYfY3GTs6U\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">🎥 Video</a>\n<button class=\"abstract-toggle\" onclick=\"toggleAbstract(this)\">📖 Show Abstract</button>\n<div class=\"paper-abstract\">3D Gaussian Splatting (3DGS) is increasingly popular for 3D reconstruction due to its superior visual quality and rendering speed. However, 3DGS training currently occurs on a single GPU, limiting its ability to handle high-resolution and large-scale 3D reconstruction tasks due to memory constraints. We introduce Grendel, a distributed system designed to partition 3DGS parameters and parallelize computation across multiple GPUs. As each Gaussian affects a small, dynamic subset of rendered pixels, Grendel employs sparse all-to-all communication to transfer the necessary Gaussians to pixel partitions and performs dynamic load balancing. Unlike existing 3DGS systems that train using one camera view image at a time, Grendel supports batched training with multiple views. We explore various optimization hyperparameter scaling strategies and find that a simple sqrt(batch size) scaling rule is highly effective. Evaluations using large-scale, high-resolution scenes show that Grendel enhances rendering quality by scaling up 3DGS parameters across multiple GPUs. On the Rubble dataset, we achieve a test PSNR of 27.28 by distributing 40.4 million Gaussians across 16 GPUs, compared to a PSNR of 26.28 using 11.2 million Gaussians on a single GPU. Grendel is an open-source project available at: https://github.com/nyu-systems/Grendel-GS</div></div>\n    </div>\n  </div>\n</div>\n<div class=\"paper-row\" data-id=\"papantonakis2024reducing\" data-title=\"Reducing the Memory Footprint of 3D Gaussian Splatting\" data-authors=\"Panagiotis Papantonakis, Georgios Kopanas, Bernhard Kerbl, Alexandre Lanvin, George Drettakis\" data-year=\"2024\" data-tags='[\"Code\", \"Compression\", \"Project\", \"Video\"]'>\n  <div class=\"paper-card\">\n    <input type=\"checkbox\" class=\"selection-checkbox\" onclick=\"handleCheckboxClick(event, 'papantonakis2024reducing', this)\">\n    <div class=\"paper-number\"></div>\n    <div class=\"paper-thumbnail\">\n      <img data-src=\"assets/thumbnails/papantonakis2024reducing.jpg\" data-fallback=\"None\" alt=\"Paper thumbnail for Reducing the Memory Footprint of 3D Gaussian Splatting\" class=\"lazy\" loading=\"lazy\"/>\n    </div>\n    <div class=\"paper-content\">\n      <h2 class=\"paper-title\">Reducing the Memory Footprint of 3D Gaussian Splatting <span class=\"paper-year\">(2024)</span></h2>\n      <p class=\"paper-authors\">Panagiotis Papantonakis, Georgios Kopanas, Bernhard Kerbl, Alexandre Lanvin, George Drettakis</p>\n      <div class=\"paper-tags\"><span class=\"paper-tag\">Code</span>\n<span class=\"paper-tag\">Compression</span>\n<span class=\"paper-tag\">Project</span>\n<span class=\"paper-tag\">Video</span></div>\n      <div class=\"paper-links\"><a href=\"https://arxiv.org/pdf/2406.17074.pdf\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">📄 Paper</a>\n<a href=\"https://repo-sam.inria.fr/fungraph/reduced_3dgs/\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">🌐 Project</a>\n<a href=\"https://repo-sam.inria.fr/fungraph/reduced_3dgs\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">💻 Code</a>\n<a href=\"https://www.youtube.com/watch?v=EnKE-d7eMds&t=48s\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">🎥 Video</a>\n<button class=\"abstract-toggle\" onclick=\"toggleAbstract(this)\">📖 Show Abstract</button>\n<div class=\"paper-abstract\">3D Gaussian splatting provides excellent visual quality for novel view synthesis, with fast training and realtime rendering; unfortunately, the memory requirements of this method for storing and transmission are unreasonably high. We first analyze the reasons for this, identifying three main areas where storage can be reduced: the number of 3D Gaussian primitives used to represent a scene, the number of coefficients for the spherical harmonics used to represent directional radiance, and the precision required to store Gaussian primitive attributes. We present a solution to each of these issues. First, we propose an efficient, resolutionaware primitive pruning approach, reducing the primitive count by half. Second, we introduce an adaptive adjustment method to choose the number of coefficients used to represent directional radiance for each Gaussian primitive, and finally a codebook-based quantization method, together with a half-float representation for further memory reduction. Taken together, these three components result in a ×27 reduction in overall size on disk on the standard datasets we tested, along with a x1.7 speedup in rendering speed. We demonstrate our method on standard datasets and show how our solution results in significantly reduced download times when using the method on a mobile device (see Fig. 1).</div></div>\n    </div>\n  </div>\n</div>\n<div class=\"paper-row\" data-id=\"mallick2024taming\" data-title=\"Taming 3DGS: High-Quality Radiance Fields with Limited Resources\" data-authors=\"Saswat Subhajyoti Mallick, Rahul Goel, Bernhard Kerbl, Francisco Vicente Carrasco, Markus Steinberger, Fernando De La Torre\" data-year=\"2024\" data-tags='[\"Acceleration\", \"Code\", \"Densification\", \"Project\", \"Video\"]'>\n  <div class=\"paper-card\">\n    <input type=\"checkbox\" class=\"selection-checkbox\" onclick=\"handleCheckboxClick(event, 'mallick2024taming', this)\">\n    <div class=\"paper-number\"></div>\n    <div class=\"paper-thumbnail\">\n      <img data-src=\"assets/thumbnails/mallick2024taming.jpg\" data-fallback=\"None\" alt=\"Paper thumbnail for Taming 3DGS: High-Quality Radiance Fields with Limited Resources\" class=\"lazy\" loading=\"lazy\"/>\n    </div>\n    <div class=\"paper-content\">\n      <h2 class=\"paper-title\">Taming 3DGS: High-Quality Radiance Fields with Limited Resources <span class=\"paper-year\">(2024)</span></h2>\n      <p class=\"paper-authors\">Saswat Subhajyoti Mallick, Rahul Goel, Bernhard Kerbl, Francisco Vicente Carrasco, Markus Steinberger, Fernando De La Torre</p>\n      <div class=\"paper-tags\"><span class=\"paper-tag\">Acceleration</span>\n<span class=\"paper-tag\">Code</span>\n<span class=\"paper-tag\">Densification</span>\n<span class=\"paper-tag\">Project</span>\n<span class=\"paper-tag\">Video</span></div>\n      <div class=\"paper-links\"><a href=\"https://arxiv.org/pdf/2406.15643.pdf\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">📄 Paper</a>\n<a href=\"https://humansensinglab.github.io/taming-3dgs/\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">🌐 Project</a>\n<a href=\"https://github.com/humansensinglab/taming-3dgs\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">💻 Code</a>\n<a href=\"https://youtu.be/iiUXqYmezbY\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">🎥 Video</a>\n<button class=\"abstract-toggle\" onclick=\"toggleAbstract(this)\">📖 Show Abstract</button>\n<div class=\"paper-abstract\">3D Gaussian Splatting (3DGS) has transformed novel-view synthesis with its fast, interpretable, and high-fidelity rendering. However, its resource requirements limit its usability. Especially on constrained devices, training performance degrades quickly and often cannot complete due to excessive memory consumption of the model. The method converges with an indefinite number of Gaussians -- many of them redundant -- making rendering unnecessarily slow and preventing its usage in downstream tasks that expect fixed-size inputs. To address these issues, we tackle the challenges of training and rendering 3DGS models on a budget. We use a guided, purely constructive densification process that steers densification toward Gaussians that raise the reconstruction quality. Model size continuously increases in a controlled manner towards an exact budget, using score-based densification of Gaussians with training-time priors that measure their contribution. We further address training speed obstacles: following a careful analysis of 3DGS' original pipeline, we derive faster, numerically equivalent solutions for gradient computation and attribute updates, including an alternative parallelization for efficient backpropagation. We also propose quality-preserving approximations where suitable to reduce training time even further. Taken together, these enhancements yield a robust, scalable solution with reduced training times, lower compute and memory requirements, and high quality. Our evaluation shows that in a budgeted setting, we obtain competitive quality metrics with 3DGS while achieving a 4--5x reduction in both model size and training time. With more generous budgets, our measured quality surpasses theirs. These advances open the door for novel-view synthesis in constrained environments, e.g., mobile devices.\n</div></div>\n    </div>\n  </div>\n</div>\n<div class=\"paper-row\" data-id=\"pan2023humansplat\" data-title=\"HumanSplat: Generalizable Single-Image Human Gaussian Splatting with Structure Priors\" data-authors=\"Panwang Pan, Zhuo Su, Chenguo Lin, Zhen Fan, Yongjie Zhang, Zeming Li, Tingting Shen, Yadong Mu, Yebin Liu\" data-year=\"2023\" data-tags='[\"Avatar\", \"Code\", \"Project\"]'>\n  <div class=\"paper-card\">\n    <input type=\"checkbox\" class=\"selection-checkbox\" onclick=\"handleCheckboxClick(event, 'pan2023humansplat', this)\">\n    <div class=\"paper-number\"></div>\n    <div class=\"paper-thumbnail\">\n      <img data-src=\"assets/thumbnails/pan2023humansplat.jpg\" data-fallback=\"None\" alt=\"Paper thumbnail for HumanSplat: Generalizable Single-Image Human Gaussian Splatting with Structure Priors\" class=\"lazy\" loading=\"lazy\"/>\n    </div>\n    <div class=\"paper-content\">\n      <h2 class=\"paper-title\">HumanSplat: Generalizable Single-Image Human Gaussian Splatting with Structure Priors <span class=\"paper-year\">(2023)</span></h2>\n      <p class=\"paper-authors\">Panwang Pan, Zhuo Su, Chenguo Lin, Zhen Fan, Yongjie Zhang, Zeming Li, Tingting Shen, Yadong Mu, Yebin Liu</p>\n      <div class=\"paper-tags\"><span class=\"paper-tag\">Avatar</span>\n<span class=\"paper-tag\">Code</span>\n<span class=\"paper-tag\">Project</span></div>\n      <div class=\"paper-links\"><a href=\"https://arxiv.org/pdf/2406.12459\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">📄 Paper</a>\n<a href=\"https://humansplat.github.io/\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">🌐 Project</a>\n<a href=\"https://github.com/humansplat/humansplat\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">💻 Code</a>\n<button class=\"abstract-toggle\" onclick=\"toggleAbstract(this)\">📖 Show Abstract</button>\n<div class=\"paper-abstract\">Despite recent advancements in high-fidelity human reconstruction techniques, the requirements for densely captured images or time-consuming per-instance optimization significantly hinder their applications in broader scenarios. To tackle these issues, we present HumanSplat that predicts the 3D Gaussian Splatting properties of any human from a single input image in a generalizable manner. In particular, HumanSplat comprises a 2D multi-view diffusion model and a latent reconstruction transformer with human structure priors that adeptly integrate geometric priors and semantic features within a unified framework. A hierarchical loss that incorporates human semantic information is further designed to achieve high-fidelity texture modeling and better constrain the estimated multiple views. Comprehensive experiments on standard benchmarks and in-the-wild images demonstrate that HumanSplat surpasses existing state-of-the-art methods in achieving photorealistic novel-view synthesis. Project page: https://humansplat.github.io/.</div></div>\n    </div>\n  </div>\n</div>\n<div class=\"paper-row\" data-id=\"kerbl2024a\" data-title=\"A Hierarchical 3D Gaussian Representation for Real-Time Rendering of Very Large Datasets\" data-authors=\"Bernhard Kerbl, Andreas Meuleman, Georgios Kopanas, Michael Wimmer,  Alexandre Lanvin, George Drettakis\" data-year=\"2024\" data-tags='[\"Code\", \"Large-Scale\", \"Project\", \"Video\"]'>\n  <div class=\"paper-card\">\n    <input type=\"checkbox\" class=\"selection-checkbox\" onclick=\"handleCheckboxClick(event, 'kerbl2024a', this)\">\n    <div class=\"paper-number\"></div>\n    <div class=\"paper-thumbnail\">\n      <img data-src=\"assets/thumbnails/kerbl2024a.jpg\" data-fallback=\"None\" alt=\"Paper thumbnail for A Hierarchical 3D Gaussian Representation for Real-Time Rendering of Very Large Datasets\" class=\"lazy\" loading=\"lazy\"/>\n    </div>\n    <div class=\"paper-content\">\n      <h2 class=\"paper-title\">A Hierarchical 3D Gaussian Representation for Real-Time Rendering of Very Large Datasets <span class=\"paper-year\">(2024)</span></h2>\n      <p class=\"paper-authors\">Bernhard Kerbl, Andreas Meuleman, Georgios Kopanas, Michael Wimmer,  Alexandre Lanvin, George Drettakis</p>\n      <div class=\"paper-tags\"><span class=\"paper-tag\">Code</span>\n<span class=\"paper-tag\">Large-Scale</span>\n<span class=\"paper-tag\">Project</span>\n<span class=\"paper-tag\">Video</span></div>\n      <div class=\"paper-links\"><a href=\"https://arxiv.org/pdf/2406.12080.pdf\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">📄 Paper</a>\n<a href=\"https://repo-sam.inria.fr/fungraph/hierarchical-3d-gaussians/\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">🌐 Project</a>\n<a href=\"https://github.com/graphdeco-inria/hierarchical-3d-gaussians\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">💻 Code</a>\n<a href=\"https://repo-sam.inria.fr/fungraph/hierarchical-3d-gaussians/content/videos/small_city.mp4\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">🎥 Video</a>\n<button class=\"abstract-toggle\" onclick=\"toggleAbstract(this)\">📖 Show Abstract</button>\n<div class=\"paper-abstract\">Novel view synthesis has seen major advances in recent years, with 3D Gaussian splatting offering an excellent level of visual quality, fast training and real-time rendering. However, the resources needed for training and rendering inevitably limit the size of the captured scenes that can be represented with good visual quality. We introduce a hierarchy of 3D Gaussians that preserves visual quality for very large scenes, while offering an efficient Level-of-Detail (LOD) solution for efficient rendering of distant content with effective level selection and smooth transitions between levels. We introduce a divide-and-conquer approach that allows us to train very large scenes in independent chunks. We consolidate the chunks into a hierarchy that can be optimized to further improve visual quality of Gaussians merged into intermediate nodes. Very large captures typically have sparse coverage of the scene, presenting many challenges to the original 3D Gaussian splatting training method; we adapt and regularize training to account for these issues. We present a complete solution, that enables real-time rendering of very large scenes and can adapt to available resources thanks to our LOD method. We show results for captured scenes with up to tens of thousands of images with a simple and affordable rig, covering trajectories of up to several kilometers and lasting up to one hour.</div></div>\n    </div>\n  </div>\n</div>\n<div class=\"paper-row\" data-id=\"hyung2024effective\" data-title=\"Effective Rank Analysis and Regularization for Enhanced 3D Gaussian Splatting\" data-authors=\"Junha Hyung, Susung Hong, Sungwon Hwang, Jaeseong Lee, Jaegul Choo, Jin-Hwa Kim\" data-year=\"2024\" data-tags='[\"Densification\", \"Meshing\", \"Project\"]'>\n  <div class=\"paper-card\">\n    <input type=\"checkbox\" class=\"selection-checkbox\" onclick=\"handleCheckboxClick(event, 'hyung2024effective', this)\">\n    <div class=\"paper-number\"></div>\n    <div class=\"paper-thumbnail\">\n      <img data-src=\"assets/thumbnails/hyung2024effective.jpg\" data-fallback=\"None\" alt=\"Paper thumbnail for Effective Rank Analysis and Regularization for Enhanced 3D Gaussian Splatting\" class=\"lazy\" loading=\"lazy\"/>\n    </div>\n    <div class=\"paper-content\">\n      <h2 class=\"paper-title\">Effective Rank Analysis and Regularization for Enhanced 3D Gaussian Splatting <span class=\"paper-year\">(2024)</span></h2>\n      <p class=\"paper-authors\">Junha Hyung, Susung Hong, Sungwon Hwang, Jaeseong Lee, Jaegul Choo, Jin-Hwa Kim</p>\n      <div class=\"paper-tags\"><span class=\"paper-tag\">Densification</span>\n<span class=\"paper-tag\">Meshing</span>\n<span class=\"paper-tag\">Project</span></div>\n      <div class=\"paper-links\"><a href=\"https://arxiv.org/pdf/2406.11672\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">📄 Paper</a>\n<a href=\"https://junhahyung.github.io/erankgs.github.io/\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">🌐 Project</a>\n<button class=\"abstract-toggle\" onclick=\"toggleAbstract(this)\">📖 Show Abstract</button>\n<div class=\"paper-abstract\">3D reconstruction from multi-view images is one of the fundamental challenges in computer vision and graphics. Recently, 3D Gaussian Splatting (3DGS) has emerged as a promising technique capable of real-time rendering with high-quality 3D reconstruction. This method utilizes 3D Gaussian representation and tile-based splatting techniques, bypassing the expensive neural field querying. Despite its potential, 3DGS encounters challenges, including needle-like artifacts, suboptimal geometries, and inaccurate normals, due to the Gaussians converging into anisotropic Gaussians with one dominant variance. We propose using effective rank analysis to examine the shape statistics of 3D Gaussian primitives, and identify the Gaussians indeed converge into needle-like shapes with the effective rank 1. To address this, we introduce effective rank as a regularization, which constrains the structure of the Gaussians. Our new regularization method enhances normal and geometry reconstruction while reducing needle-like artifacts. The approach can be integrated as an add-on module to other 3DGS variants, improving their quality without compromising visual fidelity.</div></div>\n    </div>\n  </div>\n</div>\n<div class=\"paper-row\" data-id=\"t20243dgszip\" data-title=\"3DGS.zip: A survey on 3D Gaussian Splatting Compression Methods\" data-authors=\"Milena T. Bagdasarian, Paul Knoll, Florian Barthel, Anna Hilsmann, Peter Eisert, Wieland Morgenstern\" data-year=\"2024\" data-tags='[\"Compression\", \"Project\"]'>\n  <div class=\"paper-card\">\n    <input type=\"checkbox\" class=\"selection-checkbox\" onclick=\"handleCheckboxClick(event, 't20243dgszip', this)\">\n    <div class=\"paper-number\"></div>\n    <div class=\"paper-thumbnail\">\n      <img data-src=\"assets/thumbnails/t20243dgszip.jpg\" data-fallback=\"None\" alt=\"Paper thumbnail for 3DGS.zip: A survey on 3D Gaussian Splatting Compression Methods\" class=\"lazy\" loading=\"lazy\"/>\n    </div>\n    <div class=\"paper-content\">\n      <h2 class=\"paper-title\">3DGS.zip: A survey on 3D Gaussian Splatting Compression Methods <span class=\"paper-year\">(2024)</span></h2>\n      <p class=\"paper-authors\">Milena T. Bagdasarian, Paul Knoll, Florian Barthel, Anna Hilsmann, Peter Eisert, Wieland Morgenstern</p>\n      <div class=\"paper-tags\"><span class=\"paper-tag\">Compression</span>\n<span class=\"paper-tag\">Project</span></div>\n      <div class=\"paper-links\"><a href=\"https://arxiv.org/pdf/2407.09510\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">📄 Paper</a>\n<a href=\"https://w-m.github.io/3dgs-compression-survey\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">🌐 Project</a>\n<button class=\"abstract-toggle\" onclick=\"toggleAbstract(this)\">📖 Show Abstract</button>\n<div class=\"paper-abstract\">We present a work-in-progress survey on 3D Gaussian Splatting compression methods, focusing on their statistical performance across various benchmarks. This survey aims to facilitate comparability by summarizing key statistics of different compression approaches in a tabulated format. The datasets evaluated include TanksAndTemples, MipNeRF360, DeepBlending, and SyntheticNeRF. For each method, we report the Peak Signal-to-Noise Ratio (PSNR), Structural Similarity Index (SSIM), Learned Perceptual Image Patch Similarity (LPIPS), and the resultant size in megabytes (MB), as provided by the respective authors. This is an ongoing, open project, and we invite contributions from the research community as GitHub issues or pull requests. Please visit https://w-m.github.io/3dgs-compression-survey/ for more information and a sortable version of the table.</div></div>\n    </div>\n  </div>\n</div>\n<div class=\"paper-row\" data-id=\"leroy2024grounding\" data-title=\"Grounding Image Matching in 3D with MASt3R\" data-authors=\"Vincent Leroy, Yohann Cabon, Jérôme Revaud\" data-year=\"2024\" data-tags='[\"Code\", \"Project\"]'>\n  <div class=\"paper-card\">\n    <input type=\"checkbox\" class=\"selection-checkbox\" onclick=\"handleCheckboxClick(event, 'leroy2024grounding', this)\">\n    <div class=\"paper-number\"></div>\n    <div class=\"paper-thumbnail\">\n      <img data-src=\"assets/thumbnails/leroy2024grounding.jpg\" data-fallback=\"None\" alt=\"Paper thumbnail for Grounding Image Matching in 3D with MASt3R\" class=\"lazy\" loading=\"lazy\"/>\n    </div>\n    <div class=\"paper-content\">\n      <h2 class=\"paper-title\">Grounding Image Matching in 3D with MASt3R <span class=\"paper-year\">(2024)</span></h2>\n      <p class=\"paper-authors\">Vincent Leroy, Yohann Cabon, Jérôme Revaud</p>\n      <div class=\"paper-tags\"><span class=\"paper-tag\">Code</span>\n<span class=\"paper-tag\">Project</span></div>\n      <div class=\"paper-links\"><a href=\"https://arxiv.org/pdf/2406.09756.pdf\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">📄 Paper</a>\n<a href=\"https://europe.naverlabs.com/blog/mast3r-matching-and-stereo-3d-reconstruction/\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">🌐 Project</a>\n<a href=\"https://github.com/naver/mast3r\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">💻 Code</a>\n<button class=\"abstract-toggle\" onclick=\"toggleAbstract(this)\">📖 Show Abstract</button>\n<div class=\"paper-abstract\">Image Matching is a core component of all best-performing algorithms and pipelines in 3D vision. Yet despite matching being fundamentally a 3D problem, intrinsically linked to camera pose and scene geometry, it is typically treated as a 2D problem. This makes sense as the goal of matching is to establish correspondences between 2D pixel fields, but also seems like a potentially hazardous choice. In this work, we take a different stance and propose to cast matching as a 3D task with DUSt3R, a recent and powerful 3D reconstruction framework based on Transformers. Based on pointmaps regression, this method displayed impressive robustness in matching views with extreme viewpoint changes, yet with limited accuracy. We aim here to improve the matching capabilities of such an approach while preserving its robustness. We thus propose to augment the DUSt3R network with a new head that outputs dense local features, trained with an additional matching loss. We further address the issue of quadratic complexity of dense matching, which becomes prohibitively slow for downstream applications if not carefully treated. We introduce a fast reciprocal matching scheme that not only accelerates matching by orders of magnitude, but also comes with theoretical guarantees and, lastly, yields improved results. Extensive experiments show that our approach, coined MASt3R, significantly outperforms the state of the art on multiple matching tasks. In particular, it beats the best published methods by 30% (absolute improvement) in VCRE AUC on the extremely challenging Map-free localization dataset.\n</div></div>\n    </div>\n  </div>\n</div>\n<div class=\"paper-row\" data-id=\"kirschstein2024gghead\" data-title=\"GGHead: Fast and Generalizable 3D Gaussian Heads\" data-authors=\"Tobias Kirschstein, Simon Giebenhain, Jiapeng Tang, Markos Georgopoulos, Matthias Nießner\" data-year=\"2024\" data-tags='[\"Code\", \"GAN\", \"Project\", \"Video\"]'>\n  <div class=\"paper-card\">\n    <input type=\"checkbox\" class=\"selection-checkbox\" onclick=\"handleCheckboxClick(event, 'kirschstein2024gghead', this)\">\n    <div class=\"paper-number\"></div>\n    <div class=\"paper-thumbnail\">\n      <img data-src=\"assets/thumbnails/kirschstein2024gghead.jpg\" data-fallback=\"None\" alt=\"Paper thumbnail for GGHead: Fast and Generalizable 3D Gaussian Heads\" class=\"lazy\" loading=\"lazy\"/>\n    </div>\n    <div class=\"paper-content\">\n      <h2 class=\"paper-title\">GGHead: Fast and Generalizable 3D Gaussian Heads <span class=\"paper-year\">(2024)</span></h2>\n      <p class=\"paper-authors\">Tobias Kirschstein, Simon Giebenhain, Jiapeng Tang, Markos Georgopoulos, Matthias Nießner</p>\n      <div class=\"paper-tags\"><span class=\"paper-tag\">Code</span>\n<span class=\"paper-tag\">GAN</span>\n<span class=\"paper-tag\">Project</span>\n<span class=\"paper-tag\">Video</span></div>\n      <div class=\"paper-links\"><a href=\"https://arxiv.org/pdf/2406.09377.pdf\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">📄 Paper</a>\n<a href=\"https://tobias-kirschstein.github.io/gghead/\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">🌐 Project</a>\n<a href=\"https://github.com/tobias-kirschstein/gghead\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">💻 Code</a>\n<a href=\"https://youtu.be/M5vq3DoZ7RI\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">🎥 Video</a>\n<button class=\"abstract-toggle\" onclick=\"toggleAbstract(this)\">📖 Show Abstract</button>\n<div class=\"paper-abstract\">Learning 3D head priors from large 2D image collections is an important step towards high-quality 3D-aware human modeling. A core requirement is an efficient architecture that scales well to large-scale datasets and large image resolutions. Unfortunately, existing 3D GANs struggle to scale to generate samples at high resolutions due to their relatively slow train and render speeds, and typically have to rely on 2D superresolution networks at the expense of global 3D consistency. To address these challenges, we propose Generative Gaussian Heads (GGHead), which adopts the recent 3D Gaussian Splatting representation within a 3D GAN framework. To generate a 3D representation, we employ a powerful 2D CNN generator to predict Gaussian attributes in the UV space of a template head mesh. This way, GGHead exploits the regularity of the template's UV layout, substantially facilitating the challenging task of predicting an unstructured set of 3D Gaussians. We further improve the geometric fidelity of the generated 3D representations with a novel total variation loss on rendered UV coordinates. Intuitively, this regularization encourages that neighboring rendered pixels should stem from neighboring Gaussians in the template's UV space. Taken together, our pipeline can efficiently generate 3D heads trained only from single-view 2D image observations. Our proposed framework matches the quality of existing 3D head GANs on FFHQ while being both substantially faster and fully 3D consistent. As a result, we demonstrate real-time generation and rendering of high-quality 3D-consistent heads at $1024^2$ resolution for the first time. Project Website: https://tobias-kirschstein.github.io/gghead\n</div></div>\n    </div>\n  </div>\n</div>\n<div class=\"paper-row\" data-id=\"jaganathan2024iceg\" data-title=\"ICE-G: Image Conditional Editing of 3D Gaussian Splats\" data-authors=\"Vishnu Jaganathan, Hannah Huang, Muhammad Zubair Irshad, Varun Jampani, Amit Raj, Zsolt Kira\" data-year=\"2024\" data-tags='[\"Editing\", \"Project\", \"Video\"]'>\n  <div class=\"paper-card\">\n    <input type=\"checkbox\" class=\"selection-checkbox\" onclick=\"handleCheckboxClick(event, 'jaganathan2024iceg', this)\">\n    <div class=\"paper-number\"></div>\n    <div class=\"paper-thumbnail\">\n      <img data-src=\"assets/thumbnails/jaganathan2024iceg.jpg\" data-fallback=\"None\" alt=\"Paper thumbnail for ICE-G: Image Conditional Editing of 3D Gaussian Splats\" class=\"lazy\" loading=\"lazy\"/>\n    </div>\n    <div class=\"paper-content\">\n      <h2 class=\"paper-title\">ICE-G: Image Conditional Editing of 3D Gaussian Splats <span class=\"paper-year\">(2024)</span></h2>\n      <p class=\"paper-authors\">Vishnu Jaganathan, Hannah Huang, Muhammad Zubair Irshad, Varun Jampani, Amit Raj, Zsolt Kira</p>\n      <div class=\"paper-tags\"><span class=\"paper-tag\">Editing</span>\n<span class=\"paper-tag\">Project</span>\n<span class=\"paper-tag\">Video</span></div>\n      <div class=\"paper-links\"><a href=\"https://arxiv.org/pdf/2406.08488\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">📄 Paper</a>\n<a href=\"https://ice-gaussian.github.io/\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">🌐 Project</a>\n<a href=\"https://youtu.be/dDsCwRXixp8?si=415s7-dEpM7-FPMq\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">🎥 Video</a>\n<button class=\"abstract-toggle\" onclick=\"toggleAbstract(this)\">📖 Show Abstract</button>\n<div class=\"paper-abstract\">Recently many techniques have emerged to create high quality 3D assets and scenes. When it comes to editing of these objects, however, existing approaches are either slow, compromise on quality, or do not provide enough customization. We introduce a novel approach to quickly edit a 3D model from a single reference view. Our technique first segments the edit image, and then matches semantically corresponding regions across chosen segmented dataset views using DINO features. A color or texture change from a particular region of the edit image can then be applied to other views automatically in a semantically sensible manner. These edited views act as an updated dataset to further train and re-style the 3D scene. The end-result is therefore an edited 3D model. Our framework enables a wide variety of editing tasks such as manual local edits, correspondence based style transfer from any example image, and a combination of different styles from multiple example images. We use Gaussian Splats as our primary 3D representation due to their speed and ease of local editing, but our technique works for other methods such as NeRFs as well. We show through multiple examples that our method produces higher quality results while offering fine grained control of editing.</div></div>\n    </div>\n  </div>\n</div>\n<div class=\"paper-row\" data-id=\"fan2024trim\" data-title=\"Trim 3D Gaussian Splatting for Accurate Geometry Representation\" data-authors=\"Lue Fan, Yuxue Yang, Minxing Li, Hongsheng Li, Zhaoxiang Zhang\" data-year=\"2024\" data-tags='[\"2DGS\", \"Code\", \"Densification\", \"Project\"]'>\n  <div class=\"paper-card\">\n    <input type=\"checkbox\" class=\"selection-checkbox\" onclick=\"handleCheckboxClick(event, 'fan2024trim', this)\">\n    <div class=\"paper-number\"></div>\n    <div class=\"paper-thumbnail\">\n      <img data-src=\"assets/thumbnails/fan2024trim.jpg\" data-fallback=\"None\" alt=\"Paper thumbnail for Trim 3D Gaussian Splatting for Accurate Geometry Representation\" class=\"lazy\" loading=\"lazy\"/>\n    </div>\n    <div class=\"paper-content\">\n      <h2 class=\"paper-title\">Trim 3D Gaussian Splatting for Accurate Geometry Representation <span class=\"paper-year\">(2024)</span></h2>\n      <p class=\"paper-authors\">Lue Fan, Yuxue Yang, Minxing Li, Hongsheng Li, Zhaoxiang Zhang</p>\n      <div class=\"paper-tags\"><span class=\"paper-tag\">2DGS</span>\n<span class=\"paper-tag\">Code</span>\n<span class=\"paper-tag\">Densification</span>\n<span class=\"paper-tag\">Project</span></div>\n      <div class=\"paper-links\"><a href=\"https://arxiv.org/pdf/2406.07499\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">📄 Paper</a>\n<a href=\"https://trimgs.github.io/\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">🌐 Project</a>\n<a href=\"https://github.com/YuxueYang1204/TrimGS\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">💻 Code</a>\n<button class=\"abstract-toggle\" onclick=\"toggleAbstract(this)\">📖 Show Abstract</button>\n<div class=\"paper-abstract\">In this paper, we introduce Trim 3D Gaussian Splatting (TrimGS) to reconstruct accurate 3D geometry from images. Previous arts for geometry reconstruction from 3D Gaussians mainly focus on exploring strong geometry regularization. Instead, from a fresh perspective, we propose to obtain accurate 3D geometry of a scene by Gaussian trimming, which selectively removes the inaccurate geometry while preserving accurate structures. To achieve this, we analyze the contributions of individual 3D Gaussians and propose a contribution-based trimming strategy to remove the redundant or inaccurate Gaussians. Furthermore, our experimental and theoretical analyses reveal that a relatively small Gaussian scale is a non-negligible factor in representing and optimizing the intricate details. Therefore the proposed TrimGS maintains relatively small Gaussian scales. In addition, TrimGS is also compatible with the effective geometry regularization strategies in previous arts. When combined with the original 3DGS and the state-of-the-art 2DGS, TrimGS consistently yields more accurate geometry and higher perceptual quality.</div></div>\n    </div>\n  </div>\n</div>\n<div class=\"paper-row\" data-id=\"yu20244real\" data-title=\"4Real: Towards Photorealistic 4D Scene Generation via Video Diffusion Models\" data-authors=\"Heng Yu, Chaoyang Wang, Peiye Zhuang, Willi Menapace, Aliaksandr Siarohin, Junli Cao, Laszlo A Jeni, Sergey Tulyakov, Hsin-Ying Lee\" data-year=\"2024\" data-tags='[\"Diffusion\", \"Project\"]'>\n  <div class=\"paper-card\">\n    <input type=\"checkbox\" class=\"selection-checkbox\" onclick=\"handleCheckboxClick(event, 'yu20244real', this)\">\n    <div class=\"paper-number\"></div>\n    <div class=\"paper-thumbnail\">\n      <img data-src=\"assets/thumbnails/yu20244real.jpg\" data-fallback=\"None\" alt=\"Paper thumbnail for 4Real: Towards Photorealistic 4D Scene Generation via Video Diffusion Models\" class=\"lazy\" loading=\"lazy\"/>\n    </div>\n    <div class=\"paper-content\">\n      <h2 class=\"paper-title\">4Real: Towards Photorealistic 4D Scene Generation via Video Diffusion Models <span class=\"paper-year\">(2024)</span></h2>\n      <p class=\"paper-authors\">Heng Yu, Chaoyang Wang, Peiye Zhuang, Willi Menapace, Aliaksandr Siarohin, Junli Cao, Laszlo A Jeni, Sergey Tulyakov, Hsin-Ying Lee</p>\n      <div class=\"paper-tags\"><span class=\"paper-tag\">Diffusion</span>\n<span class=\"paper-tag\">Project</span></div>\n      <div class=\"paper-links\"><a href=\"https://arxiv.org/pdf/2406.07472.pdf\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">📄 Paper</a>\n<a href=\"https://snap-research.github.io/4Real/\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">🌐 Project</a>\n<button class=\"abstract-toggle\" onclick=\"toggleAbstract(this)\">📖 Show Abstract</button>\n<div class=\"paper-abstract\">Existing dynamic scene generation methods mostly rely on distilling knowledge from pre-trained 3D generative models, which are typically fine-tuned on synthetic object datasets. As a result, the generated scenes are often object-centric and lack photorealism. To address these limitations, we introduce a novel pipeline designed for photorealistic text-to-4D scene generation, discarding the dependency on multi-view generative models and instead fully utilizing video generative models trained on diverse real-world datasets. Our method begins by generating a reference video using the video generation model. We then learn the canonical 3D representation of the video using a freeze-time video, delicately generated from the reference video. To handle inconsistencies in the freeze-time video, we jointly learn a per-frame deformation to model these imperfections. We then learn the temporal deformation based on the canonical representation to capture dynamic interactions in the reference video. The pipeline facilitates the generation of dynamic scenes with enhanced photorealism and structural integrity, viewable from multiple perspectives, thereby setting a new standard in 4D scene generation.</div></div>\n    </div>\n  </div>\n</div>\n<div class=\"paper-row\" data-id=\"jin2024lighting\" data-title=\"Lighting Every Darkness with 3DGS: Fast Training and Real-Time Rendering for HDR View Synthesis\" data-authors=\"Xin Jin, Pengyi Jiao, Zheng-Peng Duan, Xingchao Yang, Chun-Le Guo, Bo Ren, Chongyi Li\" data-year=\"2024\" data-tags='[\"Code\", \"Project\", \"Rendering\", \"Video\"]'>\n  <div class=\"paper-card\">\n    <input type=\"checkbox\" class=\"selection-checkbox\" onclick=\"handleCheckboxClick(event, 'jin2024lighting', this)\">\n    <div class=\"paper-number\"></div>\n    <div class=\"paper-thumbnail\">\n      <img data-src=\"assets/thumbnails/jin2024lighting.jpg\" data-fallback=\"None\" alt=\"Paper thumbnail for Lighting Every Darkness with 3DGS: Fast Training and Real-Time Rendering for HDR View Synthesis\" class=\"lazy\" loading=\"lazy\"/>\n    </div>\n    <div class=\"paper-content\">\n      <h2 class=\"paper-title\">Lighting Every Darkness with 3DGS: Fast Training and Real-Time Rendering for HDR View Synthesis <span class=\"paper-year\">(2024)</span></h2>\n      <p class=\"paper-authors\">Xin Jin, Pengyi Jiao, Zheng-Peng Duan, Xingchao Yang, Chun-Le Guo, Bo Ren, Chongyi Li</p>\n      <div class=\"paper-tags\"><span class=\"paper-tag\">Code</span>\n<span class=\"paper-tag\">Project</span>\n<span class=\"paper-tag\">Rendering</span>\n<span class=\"paper-tag\">Video</span></div>\n      <div class=\"paper-links\"><a href=\"https://arxiv.org/pdf/2406.06216.pdf\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">📄 Paper</a>\n<a href=\"https://srameo.github.io/projects/le3d/intro.htmla\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">🌐 Project</a>\n<a href=\"https://github.com/Srameo/LE3D\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">💻 Code</a>\n<a href=\"https://srameo.github.io/projects/le3d/assets/demo_video_interactive_viewer_compressed.mp4\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">🎥 Video</a>\n<button class=\"abstract-toggle\" onclick=\"toggleAbstract(this)\">📖 Show Abstract</button>\n<div class=\"paper-abstract\">Volumetric rendering based methods, like NeRF, excel in HDR view synthesis from RAWimages, especially for nighttime scenes. While, they suffer from long training times and cannot perform real-time rendering due to dense sampling requirements. The advent of 3D Gaussian Splatting (3DGS) enables real-time rendering and faster training. However, implementing RAW image-based view synthesis directly using 3DGS is challenging due to its inherent drawbacks: 1) in nighttime scenes, extremely low SNR leads to poor structure-from-motion (SfM) estimation in distant views; 2) the limited representation capacity of spherical harmonics (SH) function is unsuitable for RAW linear color space; and 3) inaccurate scene structure hampers downstream tasks such as refocusing. To address these issues, we propose LE3D (Lighting Every darkness with 3DGS). Our method proposes Cone Scatter Initialization to enrich the estimation of SfM, and replaces SH with a Color MLP to represent the RAW linear color space. Additionally, we introduce depth distortion and near-far regularizations to improve the accuracy of scene structure for downstream tasks. These designs enable LE3D to perform real-time novel view synthesis, HDR rendering, refocusing, and tone-mapping changes. Compared to previous volumetric rendering based methods, LE3D reduces training time to 1% and improves rendering speed by up to 4,000 times for 2K resolution images in terms of FPS. Code and viewer can be found in https://github.com/Srameo/LE3D .\n</div></div>\n    </div>\n  </div>\n</div>\n<div class=\"paper-row\" data-id=\"hyun2024adversarial\" data-title=\"Adversarial Generation of Hierarchical Gaussians for 3d Generative Model\" data-authors=\"Sangeek Hyun, Jae-Pil Heo\" data-year=\"2024\" data-tags='[\"Code\", \"GAN\", \"Project\"]'>\n  <div class=\"paper-card\">\n    <input type=\"checkbox\" class=\"selection-checkbox\" onclick=\"handleCheckboxClick(event, 'hyun2024adversarial', this)\">\n    <div class=\"paper-number\"></div>\n    <div class=\"paper-thumbnail\">\n      <img data-src=\"assets/thumbnails/hyun2024adversarial.jpg\" data-fallback=\"None\" alt=\"Paper thumbnail for Adversarial Generation of Hierarchical Gaussians for 3d Generative Model\" class=\"lazy\" loading=\"lazy\"/>\n    </div>\n    <div class=\"paper-content\">\n      <h2 class=\"paper-title\">Adversarial Generation of Hierarchical Gaussians for 3d Generative Model <span class=\"paper-year\">(2024)</span></h2>\n      <p class=\"paper-authors\">Sangeek Hyun, Jae-Pil Heo</p>\n      <div class=\"paper-tags\"><span class=\"paper-tag\">Code</span>\n<span class=\"paper-tag\">GAN</span>\n<span class=\"paper-tag\">Project</span></div>\n      <div class=\"paper-links\"><a href=\"https://arxiv.org/pdf/2406.02968\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">📄 Paper</a>\n<a href=\"https://hse1032.github.io/gsgan\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">🌐 Project</a>\n<a href=\"https://github.com/hse1032/GSGAN\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">💻 Code</a>\n<button class=\"abstract-toggle\" onclick=\"toggleAbstract(this)\">📖 Show Abstract</button>\n<div class=\"paper-abstract\">Most advances in 3D Generative Adversarial Networks (3D GANs) largely depend on ray casting-based volume rendering, which incurs demanding rendering costs. One promising alternative is rasterization-based 3D Gaussian Splatting (3D-GS), providing a much faster rendering speed and explicit 3D representation. In this paper, we exploit Gaussian as a 3D representation for 3D GANs by leveraging its efficient and explicit characteristics. However, in an adversarial framework, we observe that a naïve generator architecture suffers from training instability and lacks the capability to adjust the scale of Gaussians. This leads to model divergence and visual artifacts due to the absence of proper guidance for initialized positions of Gaussians and densification to manage their scales adaptively. To address these issues, we introduce a generator architecture with a hierarchical multi-scale Gaussian representation that effectively regularizes the position and scale of generated Gaussians. Specifically, we design a hierarchy of Gaussians where finer-level Gaussians are parameterized by their coarser-level counterparts; the position of finer-level Gaussians would be located near their coarser-level counterparts, and the scale would monotonically decrease as the level becomes finer, modeling both coarse and fine details of the 3D scene. Experimental results demonstrate that ours achieves a significantly faster rendering speed (x100) compared to state-of-the-art 3D consistent GANs with comparable 3D generation capability.</div></div>\n    </div>\n  </div>\n</div>\n<div class=\"paper-row\" data-id=\"zhang2024gs2mesh\" data-title=\" RaDe-GS: Rasterizing Depth in Gaussian Splatting \" data-authors=\"Baowen Zhang, Chuan Fang, Rakesh Shrestha, Yixun Liang, Xiaoxiao Long, Ping Tan\" data-year=\"2024\" data-tags='[\"Code\", \"Meshing\", \"Project\"]'>\n  <div class=\"paper-card\">\n    <input type=\"checkbox\" class=\"selection-checkbox\" onclick=\"handleCheckboxClick(event, 'zhang2024gs2mesh', this)\">\n    <div class=\"paper-number\"></div>\n    <div class=\"paper-thumbnail\">\n      <img data-src=\"assets/thumbnails/zhang2024gs2mesh.jpg\" data-fallback=\"None\" alt=\"Paper thumbnail for  RaDe-GS: Rasterizing Depth in Gaussian Splatting \" class=\"lazy\" loading=\"lazy\"/>\n    </div>\n    <div class=\"paper-content\">\n      <h2 class=\"paper-title\"> RaDe-GS: Rasterizing Depth in Gaussian Splatting  <span class=\"paper-year\">(2024)</span></h2>\n      <p class=\"paper-authors\">Baowen Zhang, Chuan Fang, Rakesh Shrestha, Yixun Liang, Xiaoxiao Long, Ping Tan</p>\n      <div class=\"paper-tags\"><span class=\"paper-tag\">Code</span>\n<span class=\"paper-tag\">Meshing</span>\n<span class=\"paper-tag\">Project</span></div>\n      <div class=\"paper-links\"><a href=\"https://arxiv.org/pdf/2406.01467\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">📄 Paper</a>\n<a href=\"https://baowenz.github.io/radegs/\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">🌐 Project</a>\n<a href=\"https://github.com/BaowenZ/RaDe-GS\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">💻 Code</a>\n<button class=\"abstract-toggle\" onclick=\"toggleAbstract(this)\">📖 Show Abstract</button>\n<div class=\"paper-abstract\">Gaussian Splatting (GS) has proven to be highly effective in novel view synthesis, achieving high-quality and real-time rendering. However, its potential for reconstructing detailed 3D shapes has not been fully explored. Existing methods often suffer from limited shape accuracy due to the discrete and unstructured nature of Gaussian splats, which complicates the shape extraction. While recent techniques like 2D GS have attempted to improve shape reconstruction, they often reformulate the Gaussian primitives in ways that reduce both rendering quality and computational efficiency. To address these problems, our work introduces a rasterized approach to render the depth maps and surface normal maps of general 3D Gaussian splats. Our method not only significantly enhances shape reconstruction accuracy but also maintains the computational efficiency intrinsic to Gaussian Splatting. Our approach achieves a Chamfer distance error comparable to NeuraLangelo[Li et al. 2023] on the DTU dataset and similar training and rendering time as traditional Gaussian Splatting on the Tanks & Temples dataset. Our method is a significant advancement in Gaussian Splatting and can be directly integrated into existing Gaussian Splatting-based methods.</div></div>\n    </div>\n  </div>\n</div>\n<div class=\"paper-row\" data-id=\"liu2024modgs\" data-title=\"MoDGS: Dynamic Gaussian Splatting from Causually-captured Monocular Videos\" data-authors=\"Qingming Liu*, Yuan Liu*, Jiepeng Wang, Xianqiang Lv,Peng Wang, Wenping Wang, Junhui Hou†,\" data-year=\"2024\" data-tags='[\"Dynamic\", \"Monocular\", \"Project\"]'>\n  <div class=\"paper-card\">\n    <input type=\"checkbox\" class=\"selection-checkbox\" onclick=\"handleCheckboxClick(event, 'liu2024modgs', this)\">\n    <div class=\"paper-number\"></div>\n    <div class=\"paper-thumbnail\">\n      <img data-src=\"assets/thumbnails/liu2024modgs.jpg\" data-fallback=\"None\" alt=\"Paper thumbnail for MoDGS: Dynamic Gaussian Splatting from Causually-captured Monocular Videos\" class=\"lazy\" loading=\"lazy\"/>\n    </div>\n    <div class=\"paper-content\">\n      <h2 class=\"paper-title\">MoDGS: Dynamic Gaussian Splatting from Causually-captured Monocular Videos <span class=\"paper-year\">(2024)</span></h2>\n      <p class=\"paper-authors\">Qingming Liu*, Yuan Liu*, Jiepeng Wang, Xianqiang Lv,Peng Wang, Wenping Wang, Junhui Hou†,</p>\n      <div class=\"paper-tags\"><span class=\"paper-tag\">Dynamic</span>\n<span class=\"paper-tag\">Monocular</span>\n<span class=\"paper-tag\">Project</span></div>\n      <div class=\"paper-links\"><a href=\"https://arxiv.org/pdf/2406.00434\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">📄 Paper</a>\n<a href=\"https://modgs.github.io/\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">🌐 Project</a>\n<button class=\"abstract-toggle\" onclick=\"toggleAbstract(this)\">📖 Show Abstract</button>\n<div class=\"paper-abstract\">In this paper, we propose MoDGS, a new pipeline to render novel-view images in dynamic scenes using only casually captured monocular videos. Previous monocular dynamic NeRF or Gaussian Splatting methods strongly rely on the rapid movement of input cameras to construct multiview consistency but fail to reconstruct dynamic scenes on casually captured input videos whose cameras are static or move slowly. To address this challenging task, MoDGS adopts recent single-view depth estimation methods to guide the learning of the dynamic scene. Then, a novel 3D-aware initialization method is proposed to learn a reasonable deformation field and a new robust depth loss is proposed to guide the learning of dynamic scene geometry. Comprehensive experiments demonstrate that MoDGS is able to render high-quality novel view images of dynamic scenes from just a casually captured monocular video, which outperforms baseline methods by a significant margin.</div></div>\n    </div>\n  </div>\n</div>\n<div class=\"paper-row\" data-id=\"labe2024dgd\" data-title=\"DGD: Dynamic 3D Gaussians Distillation\" data-authors=\"Isaac Labe*, Noam Issachar*, Itai Lang, Sagie Benaim\" data-year=\"2024\" data-tags='[\"Code\", \"Dynamic\", \"Project\", \"Video\"]'>\n  <div class=\"paper-card\">\n    <input type=\"checkbox\" class=\"selection-checkbox\" onclick=\"handleCheckboxClick(event, 'labe2024dgd', this)\">\n    <div class=\"paper-number\"></div>\n    <div class=\"paper-thumbnail\">\n      <img data-src=\"assets/thumbnails/labe2024dgd.jpg\" data-fallback=\"None\" alt=\"Paper thumbnail for DGD: Dynamic 3D Gaussians Distillation\" class=\"lazy\" loading=\"lazy\"/>\n    </div>\n    <div class=\"paper-content\">\n      <h2 class=\"paper-title\">DGD: Dynamic 3D Gaussians Distillation <span class=\"paper-year\">(2024)</span></h2>\n      <p class=\"paper-authors\">Isaac Labe*, Noam Issachar*, Itai Lang, Sagie Benaim</p>\n      <div class=\"paper-tags\"><span class=\"paper-tag\">Code</span>\n<span class=\"paper-tag\">Dynamic</span>\n<span class=\"paper-tag\">Project</span>\n<span class=\"paper-tag\">Video</span></div>\n      <div class=\"paper-links\"><a href=\"https://arxiv.org/pdf/2405.19321\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">📄 Paper</a>\n<a href=\"https://isaaclabe.github.io/DGD-Website/\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">🌐 Project</a>\n<a href=\"https://github.com/Isaaclabe/DGD-Dynamic-3D-Gaussians-Distillation\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">💻 Code</a>\n<a href=\"https://www.youtube.com/watch?v=GzX2GJn9OKs\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">🎥 Video</a>\n<button class=\"abstract-toggle\" onclick=\"toggleAbstract(this)\">📖 Show Abstract</button>\n<div class=\"paper-abstract\">We tackle the task of learning dynamic 3D semantic radiance fields given a single monocular video as input. Our learned semantic radiance field captures per-point semantics as well as color and geometric properties for a dynamic 3D scene, enabling the generation of novel views and their corresponding semantics. This enables the segmentation and tracking of a diverse set of 3D semantic entities, specified using a simple and intuitive interface that includes a user click or a text prompt. To this end, we present DGD, a unified 3D representation for both the appearance and semantics of a dynamic 3D scene, building upon the recently proposed dynamic 3D Gaussians representation. Our representation is optimized over time with both color and semantic information. Key to our method is the joint optimization of the appearance and semantic attributes, which jointly affect the geometric properties of the scene. We evaluate our approach in its ability to enable dense semantic 3D object tracking and demonstrate high-quality results that are fast to render, for a diverse set of scenes.</div></div>\n    </div>\n  </div>\n</div>\n<div class=\"paper-row\" data-id=\"paul2024spsup2sup360\" data-title=\"Sp<sup>2</sup>360: Sparse-view 360 Scene Reconstruction using Cascaded 2D Diffusion Priors\" data-authors=\"Soumava Paul, Christopher Wewer, Bernt Schiele, Jan Eric Lenssen\" data-year=\"2024\" data-tags='[\"Code\", \"Sparse\"]'>\n  <div class=\"paper-card\">\n    <input type=\"checkbox\" class=\"selection-checkbox\" onclick=\"handleCheckboxClick(event, 'paul2024spsup2sup360', this)\">\n    <div class=\"paper-number\"></div>\n    <div class=\"paper-thumbnail\">\n      <img data-src=\"assets/thumbnails/paul2024spsup2sup360.jpg\" data-fallback=\"None\" alt=\"Paper thumbnail for Sp<sup>2</sup>360: Sparse-view 360 Scene Reconstruction using Cascaded 2D Diffusion Priors\" class=\"lazy\" loading=\"lazy\"/>\n    </div>\n    <div class=\"paper-content\">\n      <h2 class=\"paper-title\">Sp<sup>2</sup>360: Sparse-view 360 Scene Reconstruction using Cascaded 2D Diffusion Priors <span class=\"paper-year\">(2024)</span></h2>\n      <p class=\"paper-authors\">Soumava Paul, Christopher Wewer, Bernt Schiele, Jan Eric Lenssen</p>\n      <div class=\"paper-tags\"><span class=\"paper-tag\">Code</span>\n<span class=\"paper-tag\">Sparse</span></div>\n      <div class=\"paper-links\"><a href=\"https://arxiv.org/pdf/2405.16517\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">📄 Paper</a>\n<a href=\"https://github.com/mvp18/sp2-360\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">💻 Code</a>\n<button class=\"abstract-toggle\" onclick=\"toggleAbstract(this)\">📖 Show Abstract</button>\n<div class=\"paper-abstract\">We aim to tackle sparse-view reconstruction of a 360 3D scene using priors from latent diffusion models (LDM). The sparse-view setting is ill-posed and underconstrained, especially for scenes where the camera rotates 360 degrees around a point, as no visual information is available beyond some frontal views focused on the central object(s) of interest. In this work, we show that pretrained 2D diffusion models can strongly improve the reconstruction of a scene with low-cost fine-tuning. Specifically, we present SparseSplat360 (Sp2360), a method that employs a cascade of in-painting and artifact removal models to fill in missing details and clean novel views. Due to superior training and rendering speeds, we use an explicit scene representation in the form of 3D Gaussians over NeRF-based implicit representations. We propose an iterative update strategy to fuse generated pseudo novel views with existing 3D Gaussians fitted to the initial sparse inputs. As a result, we obtain a multi-view consistent scene representation with details coherent with the observed inputs. Our evaluation on the challenging Mip-NeRF360 dataset shows that our proposed 2D to 3D distillation algorithm considerably improves the performance of a regularized version of 3DGS adapted to a sparse-view setting and outperforms existing sparse-view reconstruction methods in 360 scene reconstruction. Qualitatively, our method generates entire 360 scenes from as few as 9 input views, with a high degree of foreground and background detail.</div></div>\n    </div>\n  </div>\n</div>\n<div class=\"paper-row\" data-id=\"dalal2024gaussian\" data-title=\"Gaussian Splatting: 3D Reconstruction and Novel View Synthesis, a Review\" data-authors=\"Anurag Dalal, Daniel Hagen, Kjell G. Robbersmyr, Kristian Muri Knausgård\" data-year=\"2024\" data-tags='[\"Review\"]'>\n  <div class=\"paper-card\">\n    <input type=\"checkbox\" class=\"selection-checkbox\" onclick=\"handleCheckboxClick(event, 'dalal2024gaussian', this)\">\n    <div class=\"paper-number\"></div>\n    <div class=\"paper-thumbnail\">\n      <img data-src=\"assets/thumbnails/dalal2024gaussian.jpg\" data-fallback=\"None\" alt=\"Paper thumbnail for Gaussian Splatting: 3D Reconstruction and Novel View Synthesis, a Review\" class=\"lazy\" loading=\"lazy\"/>\n    </div>\n    <div class=\"paper-content\">\n      <h2 class=\"paper-title\">Gaussian Splatting: 3D Reconstruction and Novel View Synthesis, a Review <span class=\"paper-year\">(2024)</span></h2>\n      <p class=\"paper-authors\">Anurag Dalal, Daniel Hagen, Kjell G. Robbersmyr, Kristian Muri Knausgård</p>\n      <div class=\"paper-tags\"><span class=\"paper-tag\">Review</span></div>\n      <div class=\"paper-links\"><a href=\"https://arxiv.org/pdf/2405.03417\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">📄 Paper</a>\n<button class=\"abstract-toggle\" onclick=\"toggleAbstract(this)\">📖 Show Abstract</button>\n<div class=\"paper-abstract\">Image-based 3D reconstruction is a challenging task that involves inferring the 3D shape of an object or scene from a set of input images. Learning-based methods have gained attention for their ability to directly estimate 3D shapes. This review paper focuses on state-of-the-art techniques for 3D reconstruction, including the generation of novel, unseen views. An overview of recent developments in the Gaussian Splatting method is provided, covering input types, model structures, output representations, and training strategies. Unresolved challenges and future directions are also discussed. Given the rapid progress in this domain and the numerous opportunities for enhancing 3D reconstruction methods, a comprehensive examination of algorithms appears essential. Consequently, this study offers a thorough overview of the latest advancements in Gaussian Splatting.</div></div>\n    </div>\n  </div>\n</div>\n<div class=\"paper-row\" data-id=\"chu2024dreamscene4d\" data-title=\"DreamScene4D: Dynamic Multi-Object Scene Generation from Monocular Videos\" data-authors=\"Wen-Hsuan Chu, Lei Ke, Katerina Fragkiadaki\" data-year=\"2024\" data-tags='[\"Code\", \"Dynamic\", \"Monocular\", \"Project\"]'>\n  <div class=\"paper-card\">\n    <input type=\"checkbox\" class=\"selection-checkbox\" onclick=\"handleCheckboxClick(event, 'chu2024dreamscene4d', this)\">\n    <div class=\"paper-number\"></div>\n    <div class=\"paper-thumbnail\">\n      <img data-src=\"assets/thumbnails/chu2024dreamscene4d.jpg\" data-fallback=\"None\" alt=\"Paper thumbnail for DreamScene4D: Dynamic Multi-Object Scene Generation from Monocular Videos\" class=\"lazy\" loading=\"lazy\"/>\n    </div>\n    <div class=\"paper-content\">\n      <h2 class=\"paper-title\">DreamScene4D: Dynamic Multi-Object Scene Generation from Monocular Videos <span class=\"paper-year\">(2024)</span></h2>\n      <p class=\"paper-authors\">Wen-Hsuan Chu, Lei Ke, Katerina Fragkiadaki</p>\n      <div class=\"paper-tags\"><span class=\"paper-tag\">Code</span>\n<span class=\"paper-tag\">Dynamic</span>\n<span class=\"paper-tag\">Monocular</span>\n<span class=\"paper-tag\">Project</span></div>\n      <div class=\"paper-links\"><a href=\"https://arxiv.org/pdf/2405.02280\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">📄 Paper</a>\n<a href=\"https://dreamscene4d.github.io/\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">🌐 Project</a>\n<a href=\"https://github.com/dreamscene4d/dreamscene4d\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">💻 Code</a>\n<button class=\"abstract-toggle\" onclick=\"toggleAbstract(this)\">📖 Show Abstract</button>\n<div class=\"paper-abstract\">Existing VLMs can track in-the-wild 2D video objects while current generative models provide powerful visual priors for synthesizing novel views for the highly under-constrained 2D-to-3D object lifting. Building upon this exciting progress, we present DreamScene4D, the first approach that can generate three-dimensional dynamic scenes of multiple objects from monocular in-the-wild videos with large object motion across occlusions and novel viewpoints. Our key insight is to design a \"decompose-then-recompose\" scheme to factorize both the whole video scene and each object's 3D motion. We first decompose the video scene by using open-vocabulary mask trackers and an adapted image diffusion model to segment, track, and amodally complete the objects and background in the video. Each object track is mapped to a set of 3D Gaussians that deform and move in space and time. We also factorize the observed motion into multiple components to handle fast motion. The camera motion can be inferred by re-rendering the background to match the video frames. For the object motion, we first model the object-centric deformation of the objects by leveraging rendering losses and multi-view generative priors in an object-centric frame, then optimize object-centric to world-frame transformations by comparing the rendered outputs against the perceived pixel and optical flow. Finally, we recompose the background and objects and optimize for relative object scales using monocular depth prediction guidance. We show extensive results on the challenging DAVIS, Kubric, and self-captured videos, detail some limitations, and provide future directions. Besides 4D scene generation, our results show that DreamScene4D enables accurate 2D point motion tracking by projecting the inferred 3D trajectories to 2D, while never explicitly trained to do so.</div></div>\n    </div>\n  </div>\n</div>\n<div class=\"paper-row\" data-id=\"peng2024rtgslam\" data-title=\"RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting\" data-authors=\"Zhexi Peng, Tianjia Shao, Yong Liu, Jingke Zhou, Yin Yang, Jingdong Wang, Kun Zhou\" data-year=\"2024\" data-tags='[\"Code\", \"Project\", \"SLAM\"]'>\n  <div class=\"paper-card\">\n    <input type=\"checkbox\" class=\"selection-checkbox\" onclick=\"handleCheckboxClick(event, 'peng2024rtgslam', this)\">\n    <div class=\"paper-number\"></div>\n    <div class=\"paper-thumbnail\">\n      <img data-src=\"assets/thumbnails/peng2024rtgslam.jpg\" data-fallback=\"None\" alt=\"Paper thumbnail for RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting\" class=\"lazy\" loading=\"lazy\"/>\n    </div>\n    <div class=\"paper-content\">\n      <h2 class=\"paper-title\">RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting <span class=\"paper-year\">(2024)</span></h2>\n      <p class=\"paper-authors\">Zhexi Peng, Tianjia Shao, Yong Liu, Jingke Zhou, Yin Yang, Jingdong Wang, Kun Zhou</p>\n      <div class=\"paper-tags\"><span class=\"paper-tag\">Code</span>\n<span class=\"paper-tag\">Project</span>\n<span class=\"paper-tag\">SLAM</span></div>\n      <div class=\"paper-links\"><a href=\"https://arxiv.org/pdf/2404.19706\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">📄 Paper</a>\n<a href=\"https://gapszju.github.io/RTG-SLAM/\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">🌐 Project</a>\n<a href=\"https://github.com/MisEty/RTG-SLAM\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">💻 Code</a>\n<button class=\"abstract-toggle\" onclick=\"toggleAbstract(this)\">📖 Show Abstract</button>\n<div class=\"paper-abstract\">We present Real-time Gaussian SLAM (RTG-SLAM), a real-time 3D reconstruction system with an RGBD camera for large-scale environments using Gaussian splatting. The system features a compact Gaussian representation and a highly efficient on-the-fly Gaussian optimization scheme. We force each Gaussian to be either opaque or nearly transparent, with the opaque ones fitting the surface and dominant colors, and transparent ones fitting residual colors. By rendering depth in a different way from color rendering, we let a single opaque Gaussian well fit a local surface region without the need of multiple overlapping Gaussians, hence largely reducing the memory and computation cost. For on-the-fly Gaussian optimization, we explicitly add Gaussians for three types of pixels per frame: newly observed, with large color errors, and with large depth errors. We also categorize all Gaussians into stable and unstable ones, where the stable Gaussians are expected to well fit previously observed RGBD images and otherwise unstable. We only optimize the unstable Gaussians and only render the pixels occupied by unstable Gaussians. In this way, both the number of Gaussians to be optimized and pixels to be rendered are largely reduced, and the optimization can be done in real time. We show real-time reconstructions of a variety of large scenes. Compared with the state-of-the-art NeRF-based RGBD SLAM, our system achieves comparable high-quality reconstruction but with around twice the speed and half the memory cost, and shows superior performance in the realism of novel view synthesis and camera tracking accuracy.</div></div>\n    </div>\n  </div>\n</div>\n<div class=\"paper-row\" data-id=\"lee2024guess\" data-title=\"Guess The Unseen: Dynamic 3D Scene Reconstruction from Partial 2D Glimpses\" data-authors=\"Inhee Lee, Byungjun Kim, Hanbyul Joo\" data-year=\"2024\" data-tags='[\"Avatar\", \"Code\", \"Project\", \"Video\"]'>\n  <div class=\"paper-card\">\n    <input type=\"checkbox\" class=\"selection-checkbox\" onclick=\"handleCheckboxClick(event, 'lee2024guess', this)\">\n    <div class=\"paper-number\"></div>\n    <div class=\"paper-thumbnail\">\n      <img data-src=\"assets/thumbnails/lee2024guess.jpg\" data-fallback=\"None\" alt=\"Paper thumbnail for Guess The Unseen: Dynamic 3D Scene Reconstruction from Partial 2D Glimpses\" class=\"lazy\" loading=\"lazy\"/>\n    </div>\n    <div class=\"paper-content\">\n      <h2 class=\"paper-title\">Guess The Unseen: Dynamic 3D Scene Reconstruction from Partial 2D Glimpses <span class=\"paper-year\">(2024)</span></h2>\n      <p class=\"paper-authors\">Inhee Lee, Byungjun Kim, Hanbyul Joo</p>\n      <div class=\"paper-tags\"><span class=\"paper-tag\">Avatar</span>\n<span class=\"paper-tag\">Code</span>\n<span class=\"paper-tag\">Project</span>\n<span class=\"paper-tag\">Video</span></div>\n      <div class=\"paper-links\"><a href=\"https://arxiv.org/pdf/2404.14410\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">📄 Paper</a>\n<a href=\"https://snuvclab.github.io/gtu/\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">🌐 Project</a>\n<a href=\"https://github.com/snuvclab/gtu/\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">💻 Code</a>\n<a href=\"https://youtu.be/l9c_rd4hmFI\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">🎥 Video</a>\n<button class=\"abstract-toggle\" onclick=\"toggleAbstract(this)\">📖 Show Abstract</button>\n<div class=\"paper-abstract\">In this paper, we present a method to reconstruct the world and multiple dynamic humans in 3D from a monocular video input. As a key idea, we represent both the world and multiple humans via the recently emerging 3D Gaussian Splatting (3D-GS) representation, enabling to conveniently and efficiently compose and render them together. In particular, we address the scenarios with severely limited and sparse observations in 3D human reconstruction, a common challenge encountered in the real world. To tackle this challenge, we introduce a novel approach to optimize the 3D-GS representation in a canonical space by fusing the sparse cues in the common space, where we leverage a pre-trained 2D diffusion model to synthesize unseen views while keeping the consistency with the observed 2D appearances. We demonstrate our method can reconstruct high-quality animatable 3D humans in various challenging examples, in the presence of occlusion, image crops, few-shot, and extremely sparse observations. After reconstruction, our method is capable of not only rendering the scene in any novel views at arbitrary time instances, but also editing the 3D scene by removing individual humans or applying different motions for each human. Through various experiments, we demonstrate the quality and efficiency of our methods over alternative existing approaches.</div></div>\n    </div>\n  </div>\n</div>\n<div class=\"paper-row\" data-id=\"c2024contrastive\" data-title=\"Contrastive Gaussian Clustering: Weakly Supervised 3D Scene Segmentation\" data-authors=\"Myrna C. Silva, Mahtab Dahaghin, Matteo Toso, Alessio Del Bue\" data-year=\"2024\" data-tags='[\"Segmentation\"]'>\n  <div class=\"paper-card\">\n    <input type=\"checkbox\" class=\"selection-checkbox\" onclick=\"handleCheckboxClick(event, 'c2024contrastive', this)\">\n    <div class=\"paper-number\"></div>\n    <div class=\"paper-thumbnail\">\n      <img data-src=\"assets/thumbnails/c2024contrastive.jpg\" data-fallback=\"None\" alt=\"Paper thumbnail for Contrastive Gaussian Clustering: Weakly Supervised 3D Scene Segmentation\" class=\"lazy\" loading=\"lazy\"/>\n    </div>\n    <div class=\"paper-content\">\n      <h2 class=\"paper-title\">Contrastive Gaussian Clustering: Weakly Supervised 3D Scene Segmentation <span class=\"paper-year\">(2024)</span></h2>\n      <p class=\"paper-authors\">Myrna C. Silva, Mahtab Dahaghin, Matteo Toso, Alessio Del Bue</p>\n      <div class=\"paper-tags\"><span class=\"paper-tag\">Segmentation</span></div>\n      <div class=\"paper-links\"><a href=\"https://arxiv.org/pdf/2404.12784\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">📄 Paper</a>\n<button class=\"abstract-toggle\" onclick=\"toggleAbstract(this)\">📖 Show Abstract</button>\n<div class=\"paper-abstract\">We introduce Contrastive Gaussian Clustering, a novel approach capable of provide segmentation masks from any viewpoint and of enabling 3D segmentation of the scene. Recent works in novel-view synthesis have shown how to model the appearance of a scene via a cloud of 3D Gaussians, and how to generate accurate images from a given viewpoint by projecting on it the Gaussians before α blending their color. Following this example, we train a model to include also a segmentation feature vector for each Gaussian. These can then be used for 3D scene segmentation, by clustering Gaussians according to their feature vectors; and to generate 2D segmentation masks, by projecting the Gaussians on a plane and α blending over their segmentation features. Using a combination of contrastive learning and spatial regularization, our method can be trained on inconsistent 2D segmentation masks, and still learn to generate segmentation masks consistent across all views. Moreover, the resulting model is extremely accurate, improving the IoU accuracy of the predicted masks by +8% over the state of the art. Code and trained models will be released upon acceptance.</div></div>\n    </div>\n  </div>\n</div>\n<div class=\"paper-row\" data-id=\"liu2024infusion\" data-title=\"InFusion: Inpainting 3D Gaussians via Learning Depth Completion from Diffusion Prior\" data-authors=\"Zhiheng Liu, Hao Ouyang, Qiuyu Wang, Ka Leong Cheng, Jie Xiao, Kai Zhu, Nan Xue, Yu Liu, Yujun Shen, Yang Cao\" data-year=\"2024\" data-tags='[\"Code\", \"Editing\", \"Inpainting\", \"Project\"]'>\n  <div class=\"paper-card\">\n    <input type=\"checkbox\" class=\"selection-checkbox\" onclick=\"handleCheckboxClick(event, 'liu2024infusion', this)\">\n    <div class=\"paper-number\"></div>\n    <div class=\"paper-thumbnail\">\n      <img data-src=\"assets/thumbnails/liu2024infusion.jpg\" data-fallback=\"None\" alt=\"Paper thumbnail for InFusion: Inpainting 3D Gaussians via Learning Depth Completion from Diffusion Prior\" class=\"lazy\" loading=\"lazy\"/>\n    </div>\n    <div class=\"paper-content\">\n      <h2 class=\"paper-title\">InFusion: Inpainting 3D Gaussians via Learning Depth Completion from Diffusion Prior <span class=\"paper-year\">(2024)</span></h2>\n      <p class=\"paper-authors\">Zhiheng Liu, Hao Ouyang, Qiuyu Wang, Ka Leong Cheng, Jie Xiao, Kai Zhu, Nan Xue, Yu Liu, Yujun Shen, Yang Cao</p>\n      <div class=\"paper-tags\"><span class=\"paper-tag\">Code</span>\n<span class=\"paper-tag\">Editing</span>\n<span class=\"paper-tag\">Inpainting</span>\n<span class=\"paper-tag\">Project</span></div>\n      <div class=\"paper-links\"><a href=\"https://arxiv.org/pdf/2404.11613\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">📄 Paper</a>\n<a href=\"https://johanan528.github.io/Infusion/\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">🌐 Project</a>\n<a href=\"https://github.com/ali-vilab/infusion\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">💻 Code</a>\n<button class=\"abstract-toggle\" onclick=\"toggleAbstract(this)\">📖 Show Abstract</button>\n<div class=\"paper-abstract\">3D Gaussians have recently emerged as an efficient representation for novel view synthesis. This work studies its editability with a particular focus on the inpainting task, which aims to supplement an incomplete set of 3D Gaussians with additional points for visually harmonious rendering. Compared to 2D inpainting, the crux of inpainting 3D Gaussians is to figure out the rendering-relevant properties of the introduced points, whose optimization largely benefits from their initial 3D positions. To this end, we propose to guide the point initialization with an image-conditioned depth completion model, which learns to directly restore the depth map based on the observed image. Such a design allows our model to fill in depth values at an aligned scale with the original depth, and also to harness strong generalizability from largescale diffusion prior. Thanks to the more accurate depth completion, our approach, dubbed InFusion, surpasses existing alternatives with sufficiently better fidelity and efficiency under various complex scenarios. We further demonstrate the effectiveness of InFusion with several practical applications, such as inpainting with user-specific texture or with novel object insertion.</div></div>\n    </div>\n  </div>\n</div>\n<div class=\"paper-row\" data-id=\"oh2024deblurgs\" data-title=\"DeblurGS: Gaussian Splatting for Camera Motion Blur\" data-authors=\"Jeongtaek Oh, Jaeyoung Chung, Dongwoo Lee, Kyoung Mu Lee\" data-year=\"2024\" data-tags='[\"Deblurring\", \"Rendering\"]'>\n  <div class=\"paper-card\">\n    <input type=\"checkbox\" class=\"selection-checkbox\" onclick=\"handleCheckboxClick(event, 'oh2024deblurgs', this)\">\n    <div class=\"paper-number\"></div>\n    <div class=\"paper-thumbnail\">\n      <img data-src=\"assets/thumbnails/oh2024deblurgs.jpg\" data-fallback=\"None\" alt=\"Paper thumbnail for DeblurGS: Gaussian Splatting for Camera Motion Blur\" class=\"lazy\" loading=\"lazy\"/>\n    </div>\n    <div class=\"paper-content\">\n      <h2 class=\"paper-title\">DeblurGS: Gaussian Splatting for Camera Motion Blur <span class=\"paper-year\">(2024)</span></h2>\n      <p class=\"paper-authors\">Jeongtaek Oh, Jaeyoung Chung, Dongwoo Lee, Kyoung Mu Lee</p>\n      <div class=\"paper-tags\"><span class=\"paper-tag\">Deblurring</span>\n<span class=\"paper-tag\">Rendering</span></div>\n      <div class=\"paper-links\"><a href=\"https://arxiv.org/pdf/2404.11358\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">📄 Paper</a>\n<button class=\"abstract-toggle\" onclick=\"toggleAbstract(this)\">📖 Show Abstract</button>\n<div class=\"paper-abstract\">Although significant progress has been made in reconstructing sharp 3D scenes from motion-blurred images, a transition to realworld applications remains challenging. The primary obstacle stems from the severe blur which leads to inaccuracies in the acquisition of initial camera poses through Structure-from-Motion, a critical aspect often overlooked by previous approaches. To address this challenge, we propose DeblurGS, a method to optimize sharp 3D Gaussian Splatting from motion-blurred images, even with the noisy camera pose initialization. We restore a fine-grained sharp scene by leveraging the remarkable reconstruction capability of 3D Gaussian Splatting. Our approach estimates the 6-Degree-of-Freedom camera motion for each blurry observation and synthesizes corresponding blurry renderings for the optimization process. Furthermore, we propose Gaussian Densification Annealing strategy to prevent the generation of inaccurate Gaussians at erroneous locations during the early training stages when camera motion is still imprecise. Comprehensive experiments demonstrate that our DeblurGS achieves state-of-the-art performance in deblurring and novel view synthesis for real-world and synthetic benchmark datasets, as well as field-captured blurry smartphone videos.</div></div>\n    </div>\n  </div>\n</div>\n<div class=\"paper-row\" data-id=\"barthel2024gaussian\" data-title=\"Gaussian Splatting Decoder for 3D‑aware Generative Adversarial Networks\" data-authors=\"Florian Barthel, Arian Beckmann, Wieland Morgenstern, Anna Hilsmann, Peter Eisert\" data-year=\"2024\" data-tags='[\"Avatar\", \"Code\", \"Project\", \"Video\"]'>\n  <div class=\"paper-card\">\n    <input type=\"checkbox\" class=\"selection-checkbox\" onclick=\"handleCheckboxClick(event, 'barthel2024gaussian', this)\">\n    <div class=\"paper-number\"></div>\n    <div class=\"paper-thumbnail\">\n      <img data-src=\"assets/thumbnails/barthel2024gaussian.jpg\" data-fallback=\"None\" alt=\"Paper thumbnail for Gaussian Splatting Decoder for 3D‑aware Generative Adversarial Networks\" class=\"lazy\" loading=\"lazy\"/>\n    </div>\n    <div class=\"paper-content\">\n      <h2 class=\"paper-title\">Gaussian Splatting Decoder for 3D‑aware Generative Adversarial Networks <span class=\"paper-year\">(2024)</span></h2>\n      <p class=\"paper-authors\">Florian Barthel, Arian Beckmann, Wieland Morgenstern, Anna Hilsmann, Peter Eisert</p>\n      <div class=\"paper-tags\"><span class=\"paper-tag\">Avatar</span>\n<span class=\"paper-tag\">Code</span>\n<span class=\"paper-tag\">Project</span>\n<span class=\"paper-tag\">Video</span></div>\n      <div class=\"paper-links\"><a href=\"https://arxiv.org/pdf/2404.10625\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">📄 Paper</a>\n<a href=\"https://florian-barthel.github.io/gaussian_decoder/index.html\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">🌐 Project</a>\n<a href=\"https://github.com/fraunhoferhhi/gaussian_gan_decoder\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">💻 Code</a>\n<a href=\"https://florian-barthel.github.io/gaussian_decoder/videos/latent.mp4\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">🎥 Video</a>\n<button class=\"abstract-toggle\" onclick=\"toggleAbstract(this)\">📖 Show Abstract</button>\n<div class=\"paper-abstract\">NeRF-based 3D-aware Generative Adversarial Networks like EG3D or GIRAFFE have shown very high rendering quality under large representational variety. However, rendering with Neural Radiance Fields poses several challenges for most 3D applications: First, the significant computational demands of NeRF rendering preclude its use on low-power devices, such as mobiles and VR/AR headsets. Second, implicit representations based on neural networks are difficult to incorporate into explicit 3D scenes, such as VR environments or video games. 3D Gaussian Splatting (3DGS) overcomes these limitations by providing an explicit 3D representation that can be rendered efficiently at high frame rates. In this work, we present a novel approach that combines the high rendering quality of NeRF-based 3D-aware Generative Adversarial Networks with the flexibility and computational advantages of 3DGS. By training a decoder that maps implicit NeRF representations to explicit 3D Gaussian Splatting attributes, we can integrate the representational diversity and quality of 3D GANs into the ecosystem of 3D Gaussian Splatting for the first time. Additionally, our approach allows for a high resolution GAN inversion and real-time GAN editing with 3D Gaussian Splatting scenes.</div></div>\n    </div>\n  </div>\n</div>\n<div class=\"paper-row\" data-id=\"cui2024letsgo\" data-title=\"LetsGo: Large-Scale Garage Modeling and Rendering via LiDAR-Assisted Gaussian Primitives\" data-authors=\"Jiadi Cui, Junming Cao, Fuqiang Zhao, Zhipeng He, Yifan Chen, Yuhui Zhong, Lan Xu, Yujiao Shi, Yingliang Zhang, Jingyi Yu\" data-year=\"2024\" data-tags='[\"Code\", \"Large-Scale\", \"Lidar\", \"Project\", \"Video\"]'>\n  <div class=\"paper-card\">\n    <input type=\"checkbox\" class=\"selection-checkbox\" onclick=\"handleCheckboxClick(event, 'cui2024letsgo', this)\">\n    <div class=\"paper-number\"></div>\n    <div class=\"paper-thumbnail\">\n      <img data-src=\"assets/thumbnails/cui2024letsgo.jpg\" data-fallback=\"None\" alt=\"Paper thumbnail for LetsGo: Large-Scale Garage Modeling and Rendering via LiDAR-Assisted Gaussian Primitives\" class=\"lazy\" loading=\"lazy\"/>\n    </div>\n    <div class=\"paper-content\">\n      <h2 class=\"paper-title\">LetsGo: Large-Scale Garage Modeling and Rendering via LiDAR-Assisted Gaussian Primitives <span class=\"paper-year\">(2024)</span></h2>\n      <p class=\"paper-authors\">Jiadi Cui, Junming Cao, Fuqiang Zhao, Zhipeng He, Yifan Chen, Yuhui Zhong, Lan Xu, Yujiao Shi, Yingliang Zhang, Jingyi Yu</p>\n      <div class=\"paper-tags\"><span class=\"paper-tag\">Code</span>\n<span class=\"paper-tag\">Large-Scale</span>\n<span class=\"paper-tag\">Lidar</span>\n<span class=\"paper-tag\">Project</span>\n<span class=\"paper-tag\">Video</span></div>\n      <div class=\"paper-links\"><a href=\"https://arxiv.org/pdf/2404.09748.pdf\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">📄 Paper</a>\n<a href=\"https://zhaofuq.github.io/LetsGo/\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">🌐 Project</a>\n<a href=\"https://github.com/zhaofuq/LOD-3DGS\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">💻 Code</a>\n<a href=\"https://youtu.be/fs42UBKvGRw?si=v1D0kPj1-QEzpMSR\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">🎥 Video</a>\n<button class=\"abstract-toggle\" onclick=\"toggleAbstract(this)\">📖 Show Abstract</button>\n<div class=\"paper-abstract\">Large garages are ubiquitous yet intricate scenes that present unique challenges due to their monotonous colors, repetitive patterns, reflective surfaces, and transparent vehicle glass. Conventional Structure from Motion (SfM) methods for camera pose estimation and 3D reconstruction often fail in these environments due to poor correspondence construction. To address these challenges, we introduce LetsGo, a LiDAR-assisted Gaussian splatting framework for large-scale garage modeling and rendering. We develop a handheld scanner, Polar, equipped with IMU, LiDAR, and a fisheye camera, to facilitate accurate data acquisition. Using this Polar device, we present the GarageWorld dataset, consisting of eight expansive garage scenes with diverse geometric structures, which will be made publicly available for further research. Our approach demonstrates that LiDAR point clouds collected by the Polar device significantly enhance a suite of 3D Gaussian splatting algorithms for garage scene modeling and rendering. We introduce a novel depth regularizer that effectively eliminates floating artifacts in rendered images. Additionally, we propose a multi-resolution 3D Gaussian representation designed for Level-of-Detail (LOD) rendering. This includes adapted scaling factors for individual levels and a random-resolution-level training scheme to optimize the Gaussians across different resolutions. This representation enables efficient rendering of large-scale garage scenes on lightweight devices via a web-based renderer. Experimental results on our GarageWorld dataset, as well as on ScanNet++ and KITTI-360, demonstrate the superiority of our method in terms of rendering quality and resource efficiency.\n</div></div>\n    </div>\n  </div>\n</div>\n<div class=\"paper-row\" data-id=\"kheradmand20243d\" data-title=\"3D Gaussian Splatting as Markov Chain Monte Carlo\" data-authors=\"Shakiba Kheradmand, Daniel Rebain, Gopal Sharma, Weiwei Sun, Jeff Tseng, Hossam Isack, Abhishek Kar, Andrea Tagliasacchi, Kwang Moo Yi\" data-year=\"2024\" data-tags='[\"Code\", \"Densification\", \"Project\"]'>\n  <div class=\"paper-card\">\n    <input type=\"checkbox\" class=\"selection-checkbox\" onclick=\"handleCheckboxClick(event, 'kheradmand20243d', this)\">\n    <div class=\"paper-number\"></div>\n    <div class=\"paper-thumbnail\">\n      <img data-src=\"assets/thumbnails/kheradmand20243d.jpg\" data-fallback=\"None\" alt=\"Paper thumbnail for 3D Gaussian Splatting as Markov Chain Monte Carlo\" class=\"lazy\" loading=\"lazy\"/>\n    </div>\n    <div class=\"paper-content\">\n      <h2 class=\"paper-title\">3D Gaussian Splatting as Markov Chain Monte Carlo <span class=\"paper-year\">(2024)</span></h2>\n      <p class=\"paper-authors\">Shakiba Kheradmand, Daniel Rebain, Gopal Sharma, Weiwei Sun, Jeff Tseng, Hossam Isack, Abhishek Kar, Andrea Tagliasacchi, Kwang Moo Yi</p>\n      <div class=\"paper-tags\"><span class=\"paper-tag\">Code</span>\n<span class=\"paper-tag\">Densification</span>\n<span class=\"paper-tag\">Project</span></div>\n      <div class=\"paper-links\"><a href=\"https://arxiv.org/pdf/2404.09591.pdf\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">📄 Paper</a>\n<a href=\"https://ubc-vision.github.io/3dgs-mcmc/\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">🌐 Project</a>\n<a href=\"https://github.com/ubc-vision/3dgs-mcmc\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">💻 Code</a>\n<button class=\"abstract-toggle\" onclick=\"toggleAbstract(this)\">📖 Show Abstract</button>\n<div class=\"paper-abstract\">While 3D Gaussian Splatting has recently become popular for neural rendering, current methods rely on carefully engineered cloning and splitting strategies for placing Gaussians, which can lead to poor-quality renderings, and reliance on a good initialization. In this work, we rethink the set of 3D Gaussians as a random sample drawn from an underlying probability distribution describing the physical representation of the scene-in other words, Markov Chain Monte Carlo (MCMC) samples. Under this view, we show that the 3D Gaussian updates can be converted as Stochastic Gradient Langevin Dynamics (SGLD) updates by simply introducing noise. We then rewrite the densification and pruning strategies in 3D Gaussian Splatting as simply a deterministic state transition of MCMC samples, removing these heuristics from the framework. To do so, we revise the 'cloning' of Gaussians into a relocalization scheme that approximately preserves sample probability. To encourage efficient use of Gaussians, we introduce a regularizer that promotes the removal of unused Gaussians. On various standard evaluation scenes, we show that our method provides improved rendering quality, easy control over the number of Gaussians, and robustness to initialization.\n</div></div>\n    </div>\n  </div>\n</div>\n<div class=\"paper-row\" data-id=\"li2024loopgaussian\" data-title=\"LoopGaussian: Creating 3D Cinemagraph with Multi-view Images via Eulerian Motion Field\" data-authors=\"Jiyang Li, Lechao Cheng, Zhangye Wang, Tingting Mu, Jingxuan He\" data-year=\"2024\" data-tags='[\"Code\", \"Physics\", \"Project\", \"Rendering\"]'>\n  <div class=\"paper-card\">\n    <input type=\"checkbox\" class=\"selection-checkbox\" onclick=\"handleCheckboxClick(event, 'li2024loopgaussian', this)\">\n    <div class=\"paper-number\"></div>\n    <div class=\"paper-thumbnail\">\n      <img data-src=\"assets/thumbnails/li2024loopgaussian.jpg\" data-fallback=\"None\" alt=\"Paper thumbnail for LoopGaussian: Creating 3D Cinemagraph with Multi-view Images via Eulerian Motion Field\" class=\"lazy\" loading=\"lazy\"/>\n    </div>\n    <div class=\"paper-content\">\n      <h2 class=\"paper-title\">LoopGaussian: Creating 3D Cinemagraph with Multi-view Images via Eulerian Motion Field <span class=\"paper-year\">(2024)</span></h2>\n      <p class=\"paper-authors\">Jiyang Li, Lechao Cheng, Zhangye Wang, Tingting Mu, Jingxuan He</p>\n      <div class=\"paper-tags\"><span class=\"paper-tag\">Code</span>\n<span class=\"paper-tag\">Physics</span>\n<span class=\"paper-tag\">Project</span>\n<span class=\"paper-tag\">Rendering</span></div>\n      <div class=\"paper-links\"><a href=\"https://arxiv.org/pdf/2404.08966\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">📄 Paper</a>\n<a href=\"https://pokerlishao.github.io/LoopGaussian/\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">🌐 Project</a>\n<a href=\"https://github.com/Pokerlishao/LoopGaussian\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">💻 Code</a>\n<button class=\"abstract-toggle\" onclick=\"toggleAbstract(this)\">📖 Show Abstract</button>\n<div class=\"paper-abstract\">Cinemagraph is a unique form of visual media that combines elements of still photography and subtle motion to create a captivating experience. However, the majority of videos generated by recent works lack depth information and are confined to the constraints of 2D image space. In this paper, inspired by significant progress in the field of novel view synthesis (NVS) achieved by 3D Gaussian Splatting (3D-GS), we propose LoopGaussian to elevate cinemagraph from 2D image space to 3D space using 3D Gaussian modeling. To achieve this, we first employ the 3D-GS method to reconstruct 3D Gaussian point clouds from multi-view images of static scenes,incorporating shape regularization terms to prevent blurring or artifacts caused by object deformation. We then adopt an autoencoder tailored for 3D Gaussian to project it into feature space. To maintain the local continuity of the scene, we devise SuperGaussian for clustering based on the acquired features. By calculating the similarity between clusters and employing a two-stage estimation method, we derive an Eulerian motion field to describe velocities across the entire scene. The 3D Gaussian points then move within the estimated Eulerian motion field. Through bidirectional animation techniques, we ultimately generate a 3D Cinemagraph that exhibits natural and seamlessly loopable dynamics. Experiment results validate the effectiveness of our approach, demonstrating high-quality and visually appealing scene generation.</div></div>\n    </div>\n  </div>\n</div>\n<div class=\"paper-row\" data-id=\"ye2024occgaussian\" data-title=\"OccGaussian: 3D Gaussian Splatting for Occluded Human Rendering\" data-authors=\"Jingrui Ye, Zongkai Zhang, Yujiao Jiang, Qingmin Liao, Wenming Yang, Zongqing Lu\" data-year=\"2024\" data-tags='[\"Avatar\", \"Code\", \"Project\"]'>\n  <div class=\"paper-card\">\n    <input type=\"checkbox\" class=\"selection-checkbox\" onclick=\"handleCheckboxClick(event, 'ye2024occgaussian', this)\">\n    <div class=\"paper-number\"></div>\n    <div class=\"paper-thumbnail\">\n      <img data-src=\"assets/thumbnails/ye2024occgaussian.jpg\" data-fallback=\"None\" alt=\"Paper thumbnail for OccGaussian: 3D Gaussian Splatting for Occluded Human Rendering\" class=\"lazy\" loading=\"lazy\"/>\n    </div>\n    <div class=\"paper-content\">\n      <h2 class=\"paper-title\">OccGaussian: 3D Gaussian Splatting for Occluded Human Rendering <span class=\"paper-year\">(2024)</span></h2>\n      <p class=\"paper-authors\">Jingrui Ye, Zongkai Zhang, Yujiao Jiang, Qingmin Liao, Wenming Yang, Zongqing Lu</p>\n      <div class=\"paper-tags\"><span class=\"paper-tag\">Avatar</span>\n<span class=\"paper-tag\">Code</span>\n<span class=\"paper-tag\">Project</span></div>\n      <div class=\"paper-links\"><a href=\"https://arxiv.org/pdf/2404.07991\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">📄 Paper</a>\n<a href=\"https://wenj.github.io/GoMAvatar/\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">🌐 Project</a>\n<a href=\"https://github.com/wenj/GoMAvatar\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">💻 Code</a>\n<button class=\"abstract-toggle\" onclick=\"toggleAbstract(this)\">📖 Show Abstract</button>\n<div class=\"paper-abstract\">Rendering dynamic 3D human from monocular videos is crucial for various applications such as virtual reality and digital entertainment. Most methods assume the people is in an unobstructed scene, while various objects may cause the occlusion of body parts in real-life scenarios. Previous method utilizing NeRF for surface rendering to recover the occluded areas, but it requiring more than one day to train and several seconds to render, failing to meet the requirements of real-time interactive applications. To address these issues, we propose OccGaussian based on 3D Gaussian Splatting, which can be trained within 6 minutes and produces high-quality human renderings up to 160 FPS with occluded input. OccGaussian initializes 3D Gaussian distributions in the canonical space, and we perform occlusion feature query at occluded regions, the aggregated pixel-align feature is extracted to compensate for the missing information. Then we use Gaussian Feature MLP to further process the feature along with the occlusion-aware loss functions to better perceive the occluded area. Extensive experiments both in simulated and real-world occlusions, demonstrate that our method achieves comparable or even superior performance compared to the state-of-the-art method. And we improving training and inference speeds by 250x and 800x, respectively.</div></div>\n    </div>\n  </div>\n</div>\n<div class=\"paper-row\" data-id=\"wen2024gomavatar\" data-title=\"GoMAvatar: Efficient Animatable Human Modeling from Monocular Video Using Gaussians-on-Mesh\" data-authors=\"Jing Wen, Xiaoming Zhao, Zhongzheng Ren, Alexander G. Schwing, Shenlong Wang\" data-year=\"2024\" data-tags='[\"Avatar\", \"Code\", \"Monocular\", \"Project\"]'>\n  <div class=\"paper-card\">\n    <input type=\"checkbox\" class=\"selection-checkbox\" onclick=\"handleCheckboxClick(event, 'wen2024gomavatar', this)\">\n    <div class=\"paper-number\"></div>\n    <div class=\"paper-thumbnail\">\n      <img data-src=\"assets/thumbnails/wen2024gomavatar.jpg\" data-fallback=\"None\" alt=\"Paper thumbnail for GoMAvatar: Efficient Animatable Human Modeling from Monocular Video Using Gaussians-on-Mesh\" class=\"lazy\" loading=\"lazy\"/>\n    </div>\n    <div class=\"paper-content\">\n      <h2 class=\"paper-title\">GoMAvatar: Efficient Animatable Human Modeling from Monocular Video Using Gaussians-on-Mesh <span class=\"paper-year\">(2024)</span></h2>\n      <p class=\"paper-authors\">Jing Wen, Xiaoming Zhao, Zhongzheng Ren, Alexander G. Schwing, Shenlong Wang</p>\n      <div class=\"paper-tags\"><span class=\"paper-tag\">Avatar</span>\n<span class=\"paper-tag\">Code</span>\n<span class=\"paper-tag\">Monocular</span>\n<span class=\"paper-tag\">Project</span></div>\n      <div class=\"paper-links\"><a href=\"https://arxiv.org/pdf/2404.07991\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">📄 Paper</a>\n<a href=\"https://wenj.github.io/GoMAvatar/\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">🌐 Project</a>\n<a href=\"https://github.com/wenj/GoMAvatar\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">💻 Code</a>\n<button class=\"abstract-toggle\" onclick=\"toggleAbstract(this)\">📖 Show Abstract</button>\n<div class=\"paper-abstract\">We introduce GoMAvatar, a novel approach for real-time, memory-efficient, high-quality animatable human modeling. GoMAvatar takes as input a single monocular video to create a digital avatar capable of re-articulation in new poses and real-time rendering from novel viewpoints, while seamlessly integrating with rasterization-based graphics pipelines. Central to our method is the Gaussians-on-Mesh representation, a hybrid 3D model combining rendering quality and speed of Gaussian splatting with geometry modeling and compatibility of deformable meshes. We assess GoMAvatar on ZJU-MoCap data and various YouTube videos. GoMAvatar matches or surpasses current monocular human modeling algorithms in rendering quality and significantly outperforms them in computational efficiency (43 FPS) while being memory-efficient (3.63 MB per subject).</div></div>\n    </div>\n  </div>\n</div>\n<div class=\"paper-row\" data-id=\"lyu2024gaga\" data-title=\"Gaga: Group Any Gaussians via 3D-aware Memory Bank\" data-authors=\"Weijie Lyu, Xueting Li, Abhijit Kundu, Yi-Hsuan Tsai, Ming-Hsuan Yang\" data-year=\"2024\" data-tags='[\"Code\", \"Editing\", \"Project\", \"Segmentation\", \"Video\"]'>\n  <div class=\"paper-card\">\n    <input type=\"checkbox\" class=\"selection-checkbox\" onclick=\"handleCheckboxClick(event, 'lyu2024gaga', this)\">\n    <div class=\"paper-number\"></div>\n    <div class=\"paper-thumbnail\">\n      <img data-src=\"assets/thumbnails/lyu2024gaga.jpg\" data-fallback=\"None\" alt=\"Paper thumbnail for Gaga: Group Any Gaussians via 3D-aware Memory Bank\" class=\"lazy\" loading=\"lazy\"/>\n    </div>\n    <div class=\"paper-content\">\n      <h2 class=\"paper-title\">Gaga: Group Any Gaussians via 3D-aware Memory Bank <span class=\"paper-year\">(2024)</span></h2>\n      <p class=\"paper-authors\">Weijie Lyu, Xueting Li, Abhijit Kundu, Yi-Hsuan Tsai, Ming-Hsuan Yang</p>\n      <div class=\"paper-tags\"><span class=\"paper-tag\">Code</span>\n<span class=\"paper-tag\">Editing</span>\n<span class=\"paper-tag\">Project</span>\n<span class=\"paper-tag\">Segmentation</span>\n<span class=\"paper-tag\">Video</span></div>\n      <div class=\"paper-links\"><a href=\"https://arxiv.org/pdf/2404.07977.pdf\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">📄 Paper</a>\n<a href=\"https://www.gaga.gallery/\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">🌐 Project</a>\n<a href=\"https://github.com/weijielyu/Gaga\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">💻 Code</a>\n<a href=\"https://www.youtube.com/watch?v=rqs5BuVFOok\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">🎥 Video</a>\n<button class=\"abstract-toggle\" onclick=\"toggleAbstract(this)\">📖 Show Abstract</button>\n<div class=\"paper-abstract\">We introduce Gaga, a framework that reconstructs and segments open-world 3D scenes by leveraging inconsistent 2D masks predicted by zero-shot segmentation models. Contrasted to prior 3D scene segmentation approaches that heavily rely on video object tracking, Gaga utilizes spatial information and effectively associates object masks across diverse camera poses. By eliminating the assumption of continuous view changes in training images, Gaga demonstrates robustness to variations in camera poses, particularly beneficial for sparsely sampled images, ensuring precise mask label consistency. Furthermore, Gaga accommodates 2D segmentation masks from diverse sources and demonstrates robust performance with different open-world zero-shot segmentation models, significantly enhancing its versatility. Extensive qualitative and quantitative evaluations demonstrate that Gaga performs favorably against state-of-the-art methods, emphasizing its potential for real-world applications such as scene understanding and manipulation.</div></div>\n    </div>\n  </div>\n</div>\n<div class=\"paper-row\" data-id=\"shriram2024realmdreamer\" data-title=\"RealmDreamer: Text-Driven 3D Scene Generation with Inpainting and Depth Diffusion\" data-authors=\"Jaidev Shriram, Alex Trevithick, Lingjie Liu, Ravi Ramamoorthi\" data-year=\"2024\" data-tags='[\"Diffusion\", \"Project\"]'>\n  <div class=\"paper-card\">\n    <input type=\"checkbox\" class=\"selection-checkbox\" onclick=\"handleCheckboxClick(event, 'shriram2024realmdreamer', this)\">\n    <div class=\"paper-number\"></div>\n    <div class=\"paper-thumbnail\">\n      <img data-src=\"assets/thumbnails/shriram2024realmdreamer.jpg\" data-fallback=\"None\" alt=\"Paper thumbnail for RealmDreamer: Text-Driven 3D Scene Generation with Inpainting and Depth Diffusion\" class=\"lazy\" loading=\"lazy\"/>\n    </div>\n    <div class=\"paper-content\">\n      <h2 class=\"paper-title\">RealmDreamer: Text-Driven 3D Scene Generation with Inpainting and Depth Diffusion <span class=\"paper-year\">(2024)</span></h2>\n      <p class=\"paper-authors\">Jaidev Shriram, Alex Trevithick, Lingjie Liu, Ravi Ramamoorthi</p>\n      <div class=\"paper-tags\"><span class=\"paper-tag\">Diffusion</span>\n<span class=\"paper-tag\">Project</span></div>\n      <div class=\"paper-links\"><a href=\"https://arxiv.org/pdf/2404.07199\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">📄 Paper</a>\n<a href=\"https://realmdreamer.github.io/\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">🌐 Project</a>\n<button class=\"abstract-toggle\" onclick=\"toggleAbstract(this)\">📖 Show Abstract</button>\n<div class=\"paper-abstract\">We introduce RealmDreamer, a technique for generation of general forward-facing 3D scenes from text descriptions. Our technique optimizes a 3D Gaussian Splatting representation to match complex text prompts. We initialize these splats by utilizing the state-of-the-art text-to-image generators, lifting their samples into 3D, and computing the occlusion volume. We then optimize this representation across multiple views as a 3D inpainting task with image-conditional diffusion models. To learn correct geometric structure, we incorporate a depth diffusion model by conditioning on the samples from the inpainting model, giving rich geometric structure. Finally, we finetune the model using sharpened samples from image generators. Notably, our technique does not require video or multi-view data and can synthesize a variety of high-quality 3D scenes in different styles, consisting of multiple objects. Its generality additionally allows 3D synthesis from a single image</div></div>\n    </div>\n  </div>\n</div>\n<div class=\"paper-row\" data-id=\"lang2024gaussianlic\" data-title=\"Gaussian-LIC: Photo-realistic LiDAR-Inertial-Camera SLAM with 3D Gaussian Splatting\" data-authors=\"Xiaolei Lang, Laijian Li, Hang Zhang, Feng Xiong, Mu Xu, Yong Liu, Xingxing Zuo, Jiajun Lv\" data-year=\"2024\" data-tags='[\"Project\", \"SLAM\"]'>\n  <div class=\"paper-card\">\n    <input type=\"checkbox\" class=\"selection-checkbox\" onclick=\"handleCheckboxClick(event, 'lang2024gaussianlic', this)\">\n    <div class=\"paper-number\"></div>\n    <div class=\"paper-thumbnail\">\n      <img data-src=\"assets/thumbnails/lang2024gaussianlic.jpg\" data-fallback=\"None\" alt=\"Paper thumbnail for Gaussian-LIC: Photo-realistic LiDAR-Inertial-Camera SLAM with 3D Gaussian Splatting\" class=\"lazy\" loading=\"lazy\"/>\n    </div>\n    <div class=\"paper-content\">\n      <h2 class=\"paper-title\">Gaussian-LIC: Photo-realistic LiDAR-Inertial-Camera SLAM with 3D Gaussian Splatting <span class=\"paper-year\">(2024)</span></h2>\n      <p class=\"paper-authors\">Xiaolei Lang, Laijian Li, Hang Zhang, Feng Xiong, Mu Xu, Yong Liu, Xingxing Zuo, Jiajun Lv</p>\n      <div class=\"paper-tags\"><span class=\"paper-tag\">Project</span>\n<span class=\"paper-tag\">SLAM</span></div>\n      <div class=\"paper-links\"><a href=\"https://arxiv.org/pdf/2404.06926\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">📄 Paper</a>\n<a href=\"https://xingxingzuo.github.io/gaussian_lic/\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">🌐 Project</a>\n<button class=\"abstract-toggle\" onclick=\"toggleAbstract(this)\">📖 Show Abstract</button>\n<div class=\"paper-abstract\">We present a real-time LiDAR-Inertial-Camera SLAM system with 3D Gaussian Splatting as the mapping backend. Leveraging robust pose estimates from our LiDAR-Inertial-Camera odometry, Coco-LIC, an incremental photo-realistic mapping system is proposed in this paper. We initialize 3D Gaussians from colorized LiDAR points and optimize them using differentiable rendering powered by 3D Gaussian Splatting. Meticulously designed strategies are employed to incrementally expand the Gaussian map and adaptively control its density, ensuring high-quality mapping with real-time capability. Experiments conducted in diverse scenarios demonstrate the superior performance of our method compared to existing radiance-field-based SLAM systems.</div></div>\n    </div>\n  </div>\n</div>\n<div class=\"paper-row\" data-id=\"zhou2024dreamscene360\" data-title=\"DreamScene360: Unconstrained Text-to-3D Scene Generation with Panoramic Gaussian Splatting\" data-authors=\"Shijie Zhou, Zhiwen Fan, Dejia Xu, Haoran Chang, Pradyumna Chari, Tejas Bharadwaj, Suya You, Zhangyang Wang, Achuta Kadambi\" data-year=\"2024\" data-tags='[\"Code\", \"Diffusion\", \"Project\", \"Video\"]'>\n  <div class=\"paper-card\">\n    <input type=\"checkbox\" class=\"selection-checkbox\" onclick=\"handleCheckboxClick(event, 'zhou2024dreamscene360', this)\">\n    <div class=\"paper-number\"></div>\n    <div class=\"paper-thumbnail\">\n      <img data-src=\"assets/thumbnails/zhou2024dreamscene360.jpg\" data-fallback=\"None\" alt=\"Paper thumbnail for DreamScene360: Unconstrained Text-to-3D Scene Generation with Panoramic Gaussian Splatting\" class=\"lazy\" loading=\"lazy\"/>\n    </div>\n    <div class=\"paper-content\">\n      <h2 class=\"paper-title\">DreamScene360: Unconstrained Text-to-3D Scene Generation with Panoramic Gaussian Splatting <span class=\"paper-year\">(2024)</span></h2>\n      <p class=\"paper-authors\">Shijie Zhou, Zhiwen Fan, Dejia Xu, Haoran Chang, Pradyumna Chari, Tejas Bharadwaj, Suya You, Zhangyang Wang, Achuta Kadambi</p>\n      <div class=\"paper-tags\"><span class=\"paper-tag\">Code</span>\n<span class=\"paper-tag\">Diffusion</span>\n<span class=\"paper-tag\">Project</span>\n<span class=\"paper-tag\">Video</span></div>\n      <div class=\"paper-links\"><a href=\"https://arxiv.org/pdf/2404.06903\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">📄 Paper</a>\n<a href=\"https://dreamscene360.github.io/\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">🌐 Project</a>\n<a href=\"https://github.com/ShijieZhou-UCLA/DreamScene360\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">💻 Code</a>\n<a href=\"https://www.youtube.com/embed/6rMIQfe7b24?si=cm7cZ-T9r5na7YFD\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">🎥 Video</a>\n<button class=\"abstract-toggle\" onclick=\"toggleAbstract(this)\">📖 Show Abstract</button>\n<div class=\"paper-abstract\">The increasing demand for virtual reality applications has highlighted the significance of crafting immersive 3D assets. We present a text-to-3D 360∘ scene generation pipeline that facilitates the creation of comprehensive 360∘ scenes for in-the-wild environments in a matter of minutes. Our approach utilizes the generative power of a 2D diffusion model and prompt self-refinement to create a high-quality and globally coherent panoramic image. This image acts as a preliminary \"flat\" (2D) scene representation. Subsequently, it is lifted into 3D Gaussians, employing splatting techniques to enable real-time exploration. To produce consistent 3D geometry, our pipeline constructs a spatially coherent structure by aligning the 2D monocular depth into a globally optimized point cloud. This point cloud serves as the initial state for the centroids of 3D Gaussians. In order to address invisible issues inherent in single-view inputs, we impose semantic and geometric constraints on both synthesized and input camera views as regularizations. These guide the optimization of Gaussians, aiding in the reconstruction of unseen regions. In summary, our method offers a globally consistent 3D scene within a 360∘ perspective, providing an enhanced immersive experience over existing techniques.</div></div>\n    </div>\n  </div>\n</div>\n<div class=\"paper-row\" data-id=\"kruse2024splatpose\" data-title=\"SplatPose & Detect: Pose-Agnostic 3D Anomaly Detection\" data-authors=\"Mathis Kruse, Marco Rudolph, Dominik Woiwode, Bodo Rosenhahn\" data-year=\"2024\" data-tags='[\"Code\", \"Misc\", \"Poses\"]'>\n  <div class=\"paper-card\">\n    <input type=\"checkbox\" class=\"selection-checkbox\" onclick=\"handleCheckboxClick(event, 'kruse2024splatpose', this)\">\n    <div class=\"paper-number\"></div>\n    <div class=\"paper-thumbnail\">\n      <img data-src=\"assets/thumbnails/kruse2024splatpose.jpg\" data-fallback=\"None\" alt=\"Paper thumbnail for SplatPose & Detect: Pose-Agnostic 3D Anomaly Detection\" class=\"lazy\" loading=\"lazy\"/>\n    </div>\n    <div class=\"paper-content\">\n      <h2 class=\"paper-title\">SplatPose & Detect: Pose-Agnostic 3D Anomaly Detection <span class=\"paper-year\">(2024)</span></h2>\n      <p class=\"paper-authors\">Mathis Kruse, Marco Rudolph, Dominik Woiwode, Bodo Rosenhahn</p>\n      <div class=\"paper-tags\"><span class=\"paper-tag\">Code</span>\n<span class=\"paper-tag\">Misc</span>\n<span class=\"paper-tag\">Poses</span></div>\n      <div class=\"paper-links\"><a href=\"https://arxiv.org/pdf/2404.06832\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">📄 Paper</a>\n<a href=\"https://github.com/m-kruse98/SplatPose\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">💻 Code</a>\n<button class=\"abstract-toggle\" onclick=\"toggleAbstract(this)\">📖 Show Abstract</button>\n<div class=\"paper-abstract\">Detecting anomalies in images has become a well-explored problem in both academia and industry. State-of-the-art algorithms are able to detect defects in increasingly difficult settings and data modalities. However, most current methods are not suited to address 3D objects captured from differing poses. While solutions using Neural Radiance Fields (NeRFs) have been proposed, they suffer from excessive computation requirements, which hinder real-world usability. For this reason, we propose the novel 3D Gaussian splatting-based framework SplatPose which, given multi-view images of a 3D object, accurately estimates the pose of unseen views in a differentiable manner, and detects anomalies in them. We achieve state-of-the-art results in both training and inference speed, and detection performance, even when using less training data than competing methods. We thoroughly evaluate our framework using the recently proposed Pose-agnostic Anomaly Detection benchmark and its multi-pose anomaly detection (MAD) data set.</div></div>\n    </div>\n  </div>\n</div>\n<div class=\"paper-row\" data-id=\"huang2024zeroshot\" data-title=\"Zero-shot Point Cloud Completion Via 2D Priors\" data-authors=\"Tianxin Huang, Zhiwen Yan, Yuyang Zhao, Gim Hee Lee\" data-year=\"2024\" data-tags='[\"Diffusion\", \"Point Cloud\"]'>\n  <div class=\"paper-card\">\n    <input type=\"checkbox\" class=\"selection-checkbox\" onclick=\"handleCheckboxClick(event, 'huang2024zeroshot', this)\">\n    <div class=\"paper-number\"></div>\n    <div class=\"paper-thumbnail\">\n      <img data-src=\"assets/thumbnails/huang2024zeroshot.jpg\" data-fallback=\"None\" alt=\"Paper thumbnail for Zero-shot Point Cloud Completion Via 2D Priors\" class=\"lazy\" loading=\"lazy\"/>\n    </div>\n    <div class=\"paper-content\">\n      <h2 class=\"paper-title\">Zero-shot Point Cloud Completion Via 2D Priors <span class=\"paper-year\">(2024)</span></h2>\n      <p class=\"paper-authors\">Tianxin Huang, Zhiwen Yan, Yuyang Zhao, Gim Hee Lee</p>\n      <div class=\"paper-tags\"><span class=\"paper-tag\">Diffusion</span>\n<span class=\"paper-tag\">Point Cloud</span></div>\n      <div class=\"paper-links\"><a href=\"https://arxiv.org/pdf/2404.06814\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">📄 Paper</a>\n<button class=\"abstract-toggle\" onclick=\"toggleAbstract(this)\">📖 Show Abstract</button>\n<div class=\"paper-abstract\">3D point cloud completion is designed to recover complete shapes from partially observed point clouds. Conventional completion methods typically depend on extensive point cloud data for training %, with their effectiveness often constrained to object categories similar to those seen during training. In contrast, we propose a zero-shot framework aimed at completing partially observed point clouds across any unseen categories. Leveraging point rendering via Gaussian Splatting, we develop techniques of Point Cloud Colorization and Zero-shot Fractal Completion that utilize 2D priors from pre-trained diffusion models to infer missing regions. Experimental results on both synthetic and real-world scanned point clouds demonstrate that our approach outperforms existing methods in completing a variety of objects without any requirement for specific training data.</div></div>\n    </div>\n  </div>\n</div>\n<div class=\"paper-row\" data-id=\"dai2024spikenvs\" data-title=\"SpikeNVS: Enhancing Novel View Synthesis from Blurry Images via Spike Camera\" data-authors=\"Gaole Dai, Zhenyu Wang, Qinwen Xu, Ming Lu, Wen Chen, Boxin Shi, Shanghang Zhang, Tiejun Huang\" data-year=\"2024\" data-tags='[\"Deblurring\", \"Misc\"]'>\n  <div class=\"paper-card\">\n    <input type=\"checkbox\" class=\"selection-checkbox\" onclick=\"handleCheckboxClick(event, 'dai2024spikenvs', this)\">\n    <div class=\"paper-number\"></div>\n    <div class=\"paper-thumbnail\">\n      <img data-src=\"assets/thumbnails/dai2024spikenvs.jpg\" data-fallback=\"None\" alt=\"Paper thumbnail for SpikeNVS: Enhancing Novel View Synthesis from Blurry Images via Spike Camera\" class=\"lazy\" loading=\"lazy\"/>\n    </div>\n    <div class=\"paper-content\">\n      <h2 class=\"paper-title\">SpikeNVS: Enhancing Novel View Synthesis from Blurry Images via Spike Camera <span class=\"paper-year\">(2024)</span></h2>\n      <p class=\"paper-authors\">Gaole Dai, Zhenyu Wang, Qinwen Xu, Ming Lu, Wen Chen, Boxin Shi, Shanghang Zhang, Tiejun Huang</p>\n      <div class=\"paper-tags\"><span class=\"paper-tag\">Deblurring</span>\n<span class=\"paper-tag\">Misc</span></div>\n      <div class=\"paper-links\"><a href=\"https://arxiv.org/pdf/2404.06710\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">📄 Paper</a>\n<button class=\"abstract-toggle\" onclick=\"toggleAbstract(this)\">📖 Show Abstract</button>\n<div class=\"paper-abstract\">One of the most critical factors in achieving sharp Novel View Synthesis (NVS) using neural field methods like Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS) is the quality of the training images. However, Conventional RGB cameras are susceptible to motion blur. In contrast, neuromorphic cameras like event and spike cameras inherently capture more comprehensive temporal information, which can provide a sharp representation of the scene as additional training data. Recent methods have explored the integration of event cameras to improve the quality of NVS. The event-RGB approaches have some limitations, such as high training costs and the inability to work effectively in the background. Instead, our study introduces a new method that uses the spike camera to overcome these limitations. By considering texture reconstruction from spike streams as ground truth, we design the Texture from Spike (TfS) loss. Since the spike camera relies on temporal integration instead of temporal differentiation used by event cameras, our proposed TfS loss maintains manageable training costs. It handles foreground objects with backgrounds simultaneously. We also provide a real-world dataset captured with our spike-RGB camera system to facilitate future research endeavors. We conduct extensive experiments using synthetic and real-world datasets to demonstrate that our design can enhance novel view synthesis across NeRF and 3DGS.</div></div>\n    </div>\n  </div>\n</div>\n<div class=\"paper-row\" data-id=\"wang2024endtoend\" data-title=\"End-to-End Rate-Distortion Optimized 3D Gaussian Representation\" data-authors=\"Henan Wang, Hanxin Zhu, Tianyu He, Runsen Feng, Jiajun Deng, Jiang Bian, Zhibo Chen\" data-year=\"2024\" data-tags='[\"Code\", \"Compression\", \"Project\"]'>\n  <div class=\"paper-card\">\n    <input type=\"checkbox\" class=\"selection-checkbox\" onclick=\"handleCheckboxClick(event, 'wang2024endtoend', this)\">\n    <div class=\"paper-number\"></div>\n    <div class=\"paper-thumbnail\">\n      <img data-src=\"assets/thumbnails/wang2024endtoend.jpg\" data-fallback=\"None\" alt=\"Paper thumbnail for End-to-End Rate-Distortion Optimized 3D Gaussian Representation\" class=\"lazy\" loading=\"lazy\"/>\n    </div>\n    <div class=\"paper-content\">\n      <h2 class=\"paper-title\">End-to-End Rate-Distortion Optimized 3D Gaussian Representation <span class=\"paper-year\">(2024)</span></h2>\n      <p class=\"paper-authors\">Henan Wang, Hanxin Zhu, Tianyu He, Runsen Feng, Jiajun Deng, Jiang Bian, Zhibo Chen</p>\n      <div class=\"paper-tags\"><span class=\"paper-tag\">Code</span>\n<span class=\"paper-tag\">Compression</span>\n<span class=\"paper-tag\">Project</span></div>\n      <div class=\"paper-links\"><a href=\"https://arxiv.org/pdf/2406.01597.pdf\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">📄 Paper</a>\n<a href=\"https://rdogaussian.github.io/\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">🌐 Project</a>\n<a href=\"https://github.com/USTC-IMCL/RDO-Gaussian\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">💻 Code</a>\n<button class=\"abstract-toggle\" onclick=\"toggleAbstract(this)\">📖 Show Abstract</button>\n<div class=\"paper-abstract\">3D Gaussian Splatting (3DGS) has become an emerging technique with remarkable potential in 3D representation and image rendering. However, the substantial storage overhead of 3DGS significantly impedes its practical applications. In this work, we formulate the compact 3D Gaussian learning as an end-to-end Rate-Distortion Optimization (RDO) problem and propose RDO-Gaussian that can achieve flexible and continuous rate control. RDO-Gaussian addresses two main issues that exist in current schemes: 1) Different from prior endeavors that minimize the rate under the fixed distortion, we introduce dynamic pruning and entropy-constrained vector quantization (ECVQ) that optimize the rate and distortion at the same time. 2) Previous works treat the colors of each Gaussian equally, while we model the colors of different regions and materials with learnable numbers of parameters. We verify our method on both real and synthetic scenes, showcasing that RDO-Gaussian greatly reduces the size of 3D Gaussian over 40×, and surpasses existing methods in rate-distortion performance.</div></div>\n    </div>\n  </div>\n</div>\n<div class=\"paper-row\" data-id=\"lu20243d\" data-title=\"3D Geometry-aware Deformable Gaussian Splatting for Dynamic View Synthesis\" data-authors=\"Zhicheng Lu, Xiang Guo, Le Hui, Tianrui Chen, Min Yang, Xiao Tang, Feng Zhu, Yuchao Dai\" data-year=\"2024\" data-tags='[\"Dynamic\", \"Project\"]'>\n  <div class=\"paper-card\">\n    <input type=\"checkbox\" class=\"selection-checkbox\" onclick=\"handleCheckboxClick(event, 'lu20243d', this)\">\n    <div class=\"paper-number\"></div>\n    <div class=\"paper-thumbnail\">\n      <img data-src=\"assets/thumbnails/lu20243d.jpg\" data-fallback=\"None\" alt=\"Paper thumbnail for 3D Geometry-aware Deformable Gaussian Splatting for Dynamic View Synthesis\" class=\"lazy\" loading=\"lazy\"/>\n    </div>\n    <div class=\"paper-content\">\n      <h2 class=\"paper-title\">3D Geometry-aware Deformable Gaussian Splatting for Dynamic View Synthesis <span class=\"paper-year\">(2024)</span></h2>\n      <p class=\"paper-authors\">Zhicheng Lu, Xiang Guo, Le Hui, Tianrui Chen, Min Yang, Xiao Tang, Feng Zhu, Yuchao Dai</p>\n      <div class=\"paper-tags\"><span class=\"paper-tag\">Dynamic</span>\n<span class=\"paper-tag\">Project</span></div>\n      <div class=\"paper-links\"><a href=\"https://arxiv.org/pdf/2404.06270\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">📄 Paper</a>\n<a href=\"https://npucvr.github.io/GaGS/\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">🌐 Project</a>\n<button class=\"abstract-toggle\" onclick=\"toggleAbstract(this)\">📖 Show Abstract</button>\n<div class=\"paper-abstract\">In this paper, we propose a 3D geometry-aware deformable Gaussian Splatting method for dynamic view synthesis. Existing neural radiance fields (NeRF) based solutions learn the deformation in an implicit manner, which cannot incorporate 3D scene geometry. Therefore, the learned deformation is not necessarily geometrically coherent, which results in unsatisfactory dynamic view synthesis and 3D dynamic reconstruction. Recently, 3D Gaussian Splatting provides a new representation of the 3D scene, building upon which the 3D geometry could be exploited in learning the complex 3D deformation. Specifically, the scenes are represented as a collection of 3D Gaussian, where each 3D Gaussian is optimized to move and rotate over time to model the deformation. To enforce the 3D scene geometry constraint during deformation, we explicitly extract 3D geometry features and integrate them in learning the 3D deformation. In this way, our solution achieves 3D geometry-aware deformation modeling, which enables improved dynamic view synthesis and 3D dynamic reconstruction. Extensive experimental results on both synthetic and real datasets prove the superiority of our solution, which achieves new state-of-the-art performance.</div></div>\n    </div>\n  </div>\n</div>\n<div class=\"paper-row\" data-id=\"bonilla2024gaussian\" data-title=\"Gaussian Pancakes: Geometrically-Regularized 3D Gaussian Splatting for Realistic Endoscopic Reconstruction\" data-authors=\"Sierra Bonilla, Shuai Zhang, Dimitrios Psychogyios, Danail Stoyanov, Francisco Vasconcelos, Sophia Bano\" data-year=\"2024\" data-tags='[\"Code\", \"Medicine\", \"Project\"]'>\n  <div class=\"paper-card\">\n    <input type=\"checkbox\" class=\"selection-checkbox\" onclick=\"handleCheckboxClick(event, 'bonilla2024gaussian', this)\">\n    <div class=\"paper-number\"></div>\n    <div class=\"paper-thumbnail\">\n      <img data-src=\"assets/thumbnails/bonilla2024gaussian.jpg\" data-fallback=\"None\" alt=\"Paper thumbnail for Gaussian Pancakes: Geometrically-Regularized 3D Gaussian Splatting for Realistic Endoscopic Reconstruction\" class=\"lazy\" loading=\"lazy\"/>\n    </div>\n    <div class=\"paper-content\">\n      <h2 class=\"paper-title\">Gaussian Pancakes: Geometrically-Regularized 3D Gaussian Splatting for Realistic Endoscopic Reconstruction <span class=\"paper-year\">(2024)</span></h2>\n      <p class=\"paper-authors\">Sierra Bonilla, Shuai Zhang, Dimitrios Psychogyios, Danail Stoyanov, Francisco Vasconcelos, Sophia Bano</p>\n      <div class=\"paper-tags\"><span class=\"paper-tag\">Code</span>\n<span class=\"paper-tag\">Medicine</span>\n<span class=\"paper-tag\">Project</span></div>\n      <div class=\"paper-links\"><a href=\"https://arxiv.org/pdf/2404.06128\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">📄 Paper</a>\n<a href=\"https://papers.miccai.org/miccai-2024/349-Paper2298.html\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">🌐 Project</a>\n<a href=\"https://github.com/smbonilla/GaussianPancakes\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">💻 Code</a>\n<button class=\"abstract-toggle\" onclick=\"toggleAbstract(this)\">📖 Show Abstract</button>\n<div class=\"paper-abstract\">Within colorectal cancer diagnostics, conventional colonoscopy techniques face critical limitations, including a limited field of view and a lack of depth information, which can impede the detection of precancerous lesions. Current methods struggle to provide comprehensive and accurate 3D reconstructions of the colonic surface which can help minimize the missing regions and reinspection for pre-cancerous polyps. Addressing this, we introduce 'Gaussian Pancakes', a method that leverages 3D Gaussian Splatting (3D GS) combined with a Recurrent Neural Network-based Simultaneous Localization and Mapping (RNNSLAM) system. By introducing geometric and depth regularization into the 3D GS framework, our approach ensures more accurate alignment of Gaussians with the colon surface, resulting in smoother 3D reconstructions with novel viewing of detailed textures and structures. Evaluations across three diverse datasets show that Gaussian Pancakes enhances novel view synthesis quality, surpassing current leading methods with a 18% boost in PSNR and a 16% improvement in SSIM. It also delivers over 100X faster rendering and more than 10X shorter training times, making it a practical tool for real-time applications. Hence, this holds promise for achieving clinical translation for better detection and diagnosis of colorectal cancer.</div></div>\n    </div>\n  </div>\n</div>\n<div class=\"paper-row\" data-id=\"rota2024revising\" data-title=\"Revising Densification in Gaussian Splatting\" data-authors=\"Samuel Rota Bulò, Lorenzo Porzi, Peter Kontschieder\" data-year=\"2024\" data-tags='[\"Densification\"]'>\n  <div class=\"paper-card\">\n    <input type=\"checkbox\" class=\"selection-checkbox\" onclick=\"handleCheckboxClick(event, 'rota2024revising', this)\">\n    <div class=\"paper-number\"></div>\n    <div class=\"paper-thumbnail\">\n      <img data-src=\"assets/thumbnails/rota2024revising.jpg\" data-fallback=\"None\" alt=\"Paper thumbnail for Revising Densification in Gaussian Splatting\" class=\"lazy\" loading=\"lazy\"/>\n    </div>\n    <div class=\"paper-content\">\n      <h2 class=\"paper-title\">Revising Densification in Gaussian Splatting <span class=\"paper-year\">(2024)</span></h2>\n      <p class=\"paper-authors\">Samuel Rota Bulò, Lorenzo Porzi, Peter Kontschieder</p>\n      <div class=\"paper-tags\"><span class=\"paper-tag\">Densification</span></div>\n      <div class=\"paper-links\"><a href=\"https://arxiv.org/pdf/2404.06109\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">📄 Paper</a>\n<button class=\"abstract-toggle\" onclick=\"toggleAbstract(this)\">📖 Show Abstract</button>\n<div class=\"paper-abstract\">In this paper, we address the limitations of Adaptive Density Control (ADC) in 3D Gaussian Splatting (3DGS), a scene representation method achieving high-quality, photorealistic results for novel view synthesis. ADC has been introduced for automatic 3D point primitive management, controlling densification and pruning, however, with certain limitations in the densification logic. Our main contribution is a more principled, pixel-error driven formulation for density control in 3DGS, leveraging an auxiliary, per-pixel error function as the criterion for densification. We further introduce a mechanism to control the total number of primitives generated per scene and correct a bias in the current opacity handling strategy of ADC during cloning operations. Our approach leads to consistent quality improvements across a variety of benchmark scenes, without sacrificing the method's efficiency.</div></div>\n    </div>\n  </div>\n</div>\n<div class=\"paper-row\" data-id=\"yang2024hash3d\" data-title=\"Hash3D: Training-free Acceleration for 3D Generation\" data-authors=\"Xingyi Yang, Xinchao Wang\" data-year=\"2024\" data-tags='[\"Acceleration\", \"Code\", \"Diffusion\", \"Project\"]'>\n  <div class=\"paper-card\">\n    <input type=\"checkbox\" class=\"selection-checkbox\" onclick=\"handleCheckboxClick(event, 'yang2024hash3d', this)\">\n    <div class=\"paper-number\"></div>\n    <div class=\"paper-thumbnail\">\n      <img data-src=\"assets/thumbnails/yang2024hash3d.jpg\" data-fallback=\"None\" alt=\"Paper thumbnail for Hash3D: Training-free Acceleration for 3D Generation\" class=\"lazy\" loading=\"lazy\"/>\n    </div>\n    <div class=\"paper-content\">\n      <h2 class=\"paper-title\">Hash3D: Training-free Acceleration for 3D Generation <span class=\"paper-year\">(2024)</span></h2>\n      <p class=\"paper-authors\">Xingyi Yang, Xinchao Wang</p>\n      <div class=\"paper-tags\"><span class=\"paper-tag\">Acceleration</span>\n<span class=\"paper-tag\">Code</span>\n<span class=\"paper-tag\">Diffusion</span>\n<span class=\"paper-tag\">Project</span></div>\n      <div class=\"paper-links\"><a href=\"https://arxiv.org/pdf/2404.06091\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">📄 Paper</a>\n<a href=\"https://adamdad.github.io/hash3D/\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">🌐 Project</a>\n<a href=\"https://github.com/Adamdad/hash3D\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">💻 Code</a>\n<button class=\"abstract-toggle\" onclick=\"toggleAbstract(this)\">📖 Show Abstract</button>\n<div class=\"paper-abstract\">The evolution of 3D generative modeling has been notably propelled by the adoption of 2D diffusion models. Despite this progress, the cumbersome optimization process per se presents a critical hurdle to efficiency. In this paper, we introduce Hash3D, a universal acceleration for 3D generation without model training. Central to Hash3D is the insight that feature-map redundancy is prevalent in images rendered from camera positions and diffusion time-steps in close proximity. By effectively hashing and reusing these feature maps across neighboring timesteps and camera angles, Hash3D substantially prevents redundant calculations, thus accelerating the diffusion model's inference in 3D generation tasks. We achieve this through an adaptive grid-based hashing. Surprisingly, this feature-sharing mechanism not only speed up the generation but also enhances the smoothness and view consistency of the synthesized 3D objects. Our experiments covering 5 text-to-3D and 3 image-to-3D models, demonstrate Hash3D's versatility to speed up optimization, enhancing efficiency by 1.3 to 4 times. Additionally, Hash3D's integration with 3D Gaussian splatting largely speeds up 3D model creation, reducing text-to-3D processing to about 10 minutes and image-to-3D conversion to roughly 30 seconds.</div></div>\n    </div>\n  </div>\n</div>\n<div class=\"paper-row\" data-id=\"zhang2024stylizedgs\" data-title=\"StylizedGS: Controllable Stylization for 3D Gaussian Splatting\" data-authors=\"Dingxi Zhang, Zhuoxun Chen, Yu-Jie Yuan, Fang-Lue Zhang, Zhenliang He, Shiguang Shan, Lin Gao\" data-year=\"2024\" data-tags='[\"Rendering\", \"Style Transfer\"]'>\n  <div class=\"paper-card\">\n    <input type=\"checkbox\" class=\"selection-checkbox\" onclick=\"handleCheckboxClick(event, 'zhang2024stylizedgs', this)\">\n    <div class=\"paper-number\"></div>\n    <div class=\"paper-thumbnail\">\n      <img data-src=\"assets/thumbnails/zhang2024stylizedgs.jpg\" data-fallback=\"None\" alt=\"Paper thumbnail for StylizedGS: Controllable Stylization for 3D Gaussian Splatting\" class=\"lazy\" loading=\"lazy\"/>\n    </div>\n    <div class=\"paper-content\">\n      <h2 class=\"paper-title\">StylizedGS: Controllable Stylization for 3D Gaussian Splatting <span class=\"paper-year\">(2024)</span></h2>\n      <p class=\"paper-authors\">Dingxi Zhang, Zhuoxun Chen, Yu-Jie Yuan, Fang-Lue Zhang, Zhenliang He, Shiguang Shan, Lin Gao</p>\n      <div class=\"paper-tags\"><span class=\"paper-tag\">Rendering</span>\n<span class=\"paper-tag\">Style Transfer</span></div>\n      <div class=\"paper-links\"><a href=\"https://arxiv.org/pdf/2404.05220\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">📄 Paper</a>\n<button class=\"abstract-toggle\" onclick=\"toggleAbstract(this)\">📖 Show Abstract</button>\n<div class=\"paper-abstract\">With the rapid development of XR, 3D generation and editing are becoming more and more important, among which, stylization is an important tool of 3D appearance editing. It can achieve consistent 3D artistic stylization given a single reference style image and thus is a user-friendly editing way. However, recent NeRF-based 3D stylization methods face efficiency issues that affect the actual user experience and the implicit nature limits its ability to transfer the geometric pattern styles. Additionally, the ability for artists to exert flexible control over stylized scenes is considered highly desirable, fostering an environment conducive to creative exploration. In this paper, we introduce StylizedGS, a 3D neural style transfer framework with adaptable control over perceptual factors based on 3D Gaussian Splatting (3DGS) representation. The 3DGS brings the benefits of high efficiency. We propose a GS filter to eliminate floaters in the reconstruction which affects the stylization effects before stylization. Then the nearest neighbor-based style loss is introduced to achieve stylization by fine-tuning the geometry and color parameters of 3DGS, while a depth preservation loss with other regularizations is proposed to prevent the tampering of geometry content. Moreover, facilitated by specially designed losses, StylizedGS enables users to control color, stylized scale and regions during the stylization to possess customized capabilities. Our method can attain high-quality stylization results characterized by faithful brushstrokes and geometric consistency with flexible controls. Extensive experiments across various scenes and styles demonstrate the effectiveness and efficiency of our method concerning both stylization quality and inference FPS.</div></div>\n    </div>\n  </div>\n</div>\n<div class=\"paper-row\" data-id=\"wu2024dualcamera\" data-title=\"Dual-Camera Smooth Zoom on Mobile Phones\" data-authors=\"Renlong Wu, Zhilu Zhang, Yu Yang, Wangmeng Zuo\" data-year=\"2024\" data-tags='[\"Code\", \"Misc\", \"Project\", \"Rendering\"]'>\n  <div class=\"paper-card\">\n    <input type=\"checkbox\" class=\"selection-checkbox\" onclick=\"handleCheckboxClick(event, 'wu2024dualcamera', this)\">\n    <div class=\"paper-number\"></div>\n    <div class=\"paper-thumbnail\">\n      <img data-src=\"assets/thumbnails/wu2024dualcamera.jpg\" data-fallback=\"None\" alt=\"Paper thumbnail for Dual-Camera Smooth Zoom on Mobile Phones\" class=\"lazy\" loading=\"lazy\"/>\n    </div>\n    <div class=\"paper-content\">\n      <h2 class=\"paper-title\">Dual-Camera Smooth Zoom on Mobile Phones <span class=\"paper-year\">(2024)</span></h2>\n      <p class=\"paper-authors\">Renlong Wu, Zhilu Zhang, Yu Yang, Wangmeng Zuo</p>\n      <div class=\"paper-tags\"><span class=\"paper-tag\">Code</span>\n<span class=\"paper-tag\">Misc</span>\n<span class=\"paper-tag\">Project</span>\n<span class=\"paper-tag\">Rendering</span></div>\n      <div class=\"paper-links\"><a href=\"https://arxiv.org/pdf/2404.04908\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">📄 Paper</a>\n<a href=\"https://dualcamerasmoothzoom.github.io/\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">🌐 Project</a>\n<a href=\"https://github.com/ZcsrenlongZ/ZoomGS\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">💻 Code</a>\n<button class=\"abstract-toggle\" onclick=\"toggleAbstract(this)\">📖 Show Abstract</button>\n<div class=\"paper-abstract\">When zooming between dual cameras on a mobile, noticeable jumps in geometric content and image color occur in the preview, inevitably affecting the user's zoom experience. In this work, we introduce a new task, ie, dual-camera smooth zoom (DCSZ) to achieve a smooth zoom preview. The frame interpolation (FI) technique is a potential solution but struggles with ground-truth collection. To address the issue, we suggest a data factory solution where continuous virtual cameras are assembled to generate DCSZ data by rendering reconstructed 3D models of the scene. In particular, we propose a novel dual-camera smooth zoom Gaussian Splatting (ZoomGS), where a camera-specific encoding is introduced to construct a specific 3D model for each virtual camera. With the proposed data factory, we construct a synthetic dataset for DCSZ, and we utilize it to fine-tune FI models. In addition, we collect real-world dual-zoom images without ground-truth for evaluation. Extensive experiments are conducted with multiple FI methods. The results show that the fine-tuned FI models achieve a significant performance improvement over the original ones on DCSZ task.</div></div>\n    </div>\n  </div>\n</div>\n<div class=\"paper-row\" data-id=\"darmon2024robust\" data-title=\"Robust Gaussian Splatting\" data-authors=\"François Darmon, Lorenzo Porzi, Samuel Rota-Bulò, Peter Kontschieder\" data-year=\"2024\" data-tags='[\"Deblurring\", \"Rendering\"]'>\n  <div class=\"paper-card\">\n    <input type=\"checkbox\" class=\"selection-checkbox\" onclick=\"handleCheckboxClick(event, 'darmon2024robust', this)\">\n    <div class=\"paper-number\"></div>\n    <div class=\"paper-thumbnail\">\n      <img data-src=\"assets/thumbnails/darmon2024robust.jpg\" data-fallback=\"None\" alt=\"Paper thumbnail for Robust Gaussian Splatting\" class=\"lazy\" loading=\"lazy\"/>\n    </div>\n    <div class=\"paper-content\">\n      <h2 class=\"paper-title\">Robust Gaussian Splatting <span class=\"paper-year\">(2024)</span></h2>\n      <p class=\"paper-authors\">François Darmon, Lorenzo Porzi, Samuel Rota-Bulò, Peter Kontschieder</p>\n      <div class=\"paper-tags\"><span class=\"paper-tag\">Deblurring</span>\n<span class=\"paper-tag\">Rendering</span></div>\n      <div class=\"paper-links\"><a href=\"https://arxiv.org/pdf/2404.04211\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">📄 Paper</a>\n<button class=\"abstract-toggle\" onclick=\"toggleAbstract(this)\">📖 Show Abstract</button>\n<div class=\"paper-abstract\">In this paper, we address common error sources for 3D Gaussian Splatting (3DGS) including blur, imperfect camera poses, and color inconsistencies, with the goal of improving its robustness for practical applications like reconstructions from handheld phone captures. Our main contribution involves modeling motion blur as a Gaussian distribution over camera poses, allowing us to address both camera pose refinement and motion blur correction in a unified way. Additionally, we propose mechanisms for defocus blur compensation and for addressing color in-consistencies caused by ambient light, shadows, or due to camera-related factors like varying white balancing settings. Our proposed solutions integrate in a seamless way with the 3DGS formulation while maintaining its benefits in terms of training efficiency and rendering speed. We experimentally validate our contributions on relevant benchmark datasets including Scannet++ and Deblur-NeRF, obtaining state-of-the-art results and thus consistent improvements over relevant baselines.</div></div>\n    </div>\n  </div>\n</div>\n<div class=\"paper-row\" data-id=\"wu2024mmgaussian\" data-title=\"MM-Gaussian: 3D Gaussian-based Multi-modal Fusion for Localization and Reconstruction in Unbounded Scenes\" data-authors=\"Chenyang Wu, Yifan Duan, Xinran Zhang, Yu Sheng, Jianmin Ji, Yanyong Zhang\" data-year=\"2024\" data-tags='[\"Poses\"]'>\n  <div class=\"paper-card\">\n    <input type=\"checkbox\" class=\"selection-checkbox\" onclick=\"handleCheckboxClick(event, 'wu2024mmgaussian', this)\">\n    <div class=\"paper-number\"></div>\n    <div class=\"paper-thumbnail\">\n      <img data-src=\"assets/thumbnails/wu2024mmgaussian.jpg\" data-fallback=\"None\" alt=\"Paper thumbnail for MM-Gaussian: 3D Gaussian-based Multi-modal Fusion for Localization and Reconstruction in Unbounded Scenes\" class=\"lazy\" loading=\"lazy\"/>\n    </div>\n    <div class=\"paper-content\">\n      <h2 class=\"paper-title\">MM-Gaussian: 3D Gaussian-based Multi-modal Fusion for Localization and Reconstruction in Unbounded Scenes <span class=\"paper-year\">(2024)</span></h2>\n      <p class=\"paper-authors\">Chenyang Wu, Yifan Duan, Xinran Zhang, Yu Sheng, Jianmin Ji, Yanyong Zhang</p>\n      <div class=\"paper-tags\"><span class=\"paper-tag\">Poses</span></div>\n      <div class=\"paper-links\"><a href=\"https://arxiv.org/pdf/2404.04026\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">📄 Paper</a>\n<button class=\"abstract-toggle\" onclick=\"toggleAbstract(this)\">📖 Show Abstract</button>\n<div class=\"paper-abstract\">Localization and mapping are critical tasks for various applications such as autonomous vehicles and robotics. The challenges posed by outdoor environments present particular complexities due to their unbounded characteristics. In this work, we present MM-Gaussian, a LiDAR-camera multi-modal fusion system for localization and mapping in unbounded scenes. Our approach is inspired by the recently developed 3D Gaussians, which demonstrate remarkable capabilities in achieving high rendering quality and fast rendering speed. Specifically, our system fully utilizes the geometric structure information provided by solid-state LiDAR to address the problem of inaccurate depth encountered when relying solely on visual solutions in unbounded, outdoor scenarios. Additionally, we utilize 3D Gaussian point clouds, with the assistance of pixel-level gradient descent, to fully exploit the color information in photos, thereby achieving realistic rendering effects. To further bolster the robustness of our system, we designed a relocalization module, which assists in returning to the correct trajectory in the event of a localization failure. Experiments conducted in multiple scenarios demonstrate the effectiveness of our method.</div></div>\n    </div>\n  </div>\n</div>\n<div class=\"paper-row\" data-id=\"wu2024sc4d\" data-title=\"SC4D: Sparse-Controlled Video-to-4D Generation and Motion Transfer\" data-authors=\"Zijie Wu, Chaohui Yu, Yanqin Jiang, Chenjie Cao, Fan Wang, Xiang Bai\" data-year=\"2024\" data-tags='[\"Code\", \"Diffusion\", \"Project\", \"Video\"]'>\n  <div class=\"paper-card\">\n    <input type=\"checkbox\" class=\"selection-checkbox\" onclick=\"handleCheckboxClick(event, 'wu2024sc4d', this)\">\n    <div class=\"paper-number\"></div>\n    <div class=\"paper-thumbnail\">\n      <img data-src=\"assets/thumbnails/wu2024sc4d.jpg\" data-fallback=\"None\" alt=\"Paper thumbnail for SC4D: Sparse-Controlled Video-to-4D Generation and Motion Transfer\" class=\"lazy\" loading=\"lazy\"/>\n    </div>\n    <div class=\"paper-content\">\n      <h2 class=\"paper-title\">SC4D: Sparse-Controlled Video-to-4D Generation and Motion Transfer <span class=\"paper-year\">(2024)</span></h2>\n      <p class=\"paper-authors\">Zijie Wu, Chaohui Yu, Yanqin Jiang, Chenjie Cao, Fan Wang, Xiang Bai</p>\n      <div class=\"paper-tags\"><span class=\"paper-tag\">Code</span>\n<span class=\"paper-tag\">Diffusion</span>\n<span class=\"paper-tag\">Project</span>\n<span class=\"paper-tag\">Video</span></div>\n      <div class=\"paper-links\"><a href=\"https://arxiv.org/pdf/2404.03736\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">📄 Paper</a>\n<a href=\"https://sc4d.github.io/\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">🌐 Project</a>\n<a href=\"https://github.com/JarrentWu1031/SC4D\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">💻 Code</a>\n<a href=\"https://youtu.be/SkpTEuX4B5c?si=yvrF_iRHnMQR9TD0\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">🎥 Video</a>\n<button class=\"abstract-toggle\" onclick=\"toggleAbstract(this)\">📖 Show Abstract</button>\n<div class=\"paper-abstract\">Recent advances in 2D/3D generative models enable the generation of dynamic 3D objects from a single-view video. Existing approaches utilize score distillation sampling to form the dynamic scene as dynamic NeRF or dense 3D Gaussians. However, these methods struggle to strike a balance among reference view alignment, spatio-temporal consistency, and motion fidelity under single-view conditions due to the implicit nature of NeRF or the intricate dense Gaussian motion prediction. To address these issues, this paper proposes an efficient, sparse-controlled video-to-4D framework named SC4D, that decouples motion and appearance to achieve superior video-to-4D generation. Moreover, we introduce Adaptive Gaussian (AG) initialization and Gaussian Alignment (GA) loss to mitigate shape degeneration issue, ensuring the fidelity of the learned motion and shape. Comprehensive experimental results demonstrate that our method surpasses existing methods in both quality and efficiency. In addition, facilitated by the disentangled modeling of motion and appearance of SC4D, we devise a novel application that seamlessly transfers the learned motion onto a diverse array of 4D entities according to textual descriptions.</div></div>\n    </div>\n  </div>\n</div>\n<div class=\"paper-row\" data-id=\"bae2024pergaussian\" data-title=\"Per-Gaussian Embedding-Based Deformation for Deformable 3D Gaussian Splatting\" data-authors=\"Jeongmin Bae*, Seoha Kim*, Youngsik Yun, Hahyun Lee, Gun Bang, Youngjung Uh\" data-year=\"2024\" data-tags='[\"Code\", \"Dynamic\", \"Project\"]'>\n  <div class=\"paper-card\">\n    <input type=\"checkbox\" class=\"selection-checkbox\" onclick=\"handleCheckboxClick(event, 'bae2024pergaussian', this)\">\n    <div class=\"paper-number\"></div>\n    <div class=\"paper-thumbnail\">\n      <img data-src=\"assets/thumbnails/bae2024pergaussian.jpg\" data-fallback=\"None\" alt=\"Paper thumbnail for Per-Gaussian Embedding-Based Deformation for Deformable 3D Gaussian Splatting\" class=\"lazy\" loading=\"lazy\"/>\n    </div>\n    <div class=\"paper-content\">\n      <h2 class=\"paper-title\">Per-Gaussian Embedding-Based Deformation for Deformable 3D Gaussian Splatting <span class=\"paper-year\">(2024)</span></h2>\n      <p class=\"paper-authors\">Jeongmin Bae*, Seoha Kim*, Youngsik Yun, Hahyun Lee, Gun Bang, Youngjung Uh</p>\n      <div class=\"paper-tags\"><span class=\"paper-tag\">Code</span>\n<span class=\"paper-tag\">Dynamic</span>\n<span class=\"paper-tag\">Project</span></div>\n      <div class=\"paper-links\"><a href=\"https://arxiv.org/pdf/2404.03613\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">📄 Paper</a>\n<a href=\"https://jeongminb.github.io/e-d3dgs/\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">🌐 Project</a>\n<a href=\"https://github.com/JeongminB/E-D3DGS\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">💻 Code</a>\n<button class=\"abstract-toggle\" onclick=\"toggleAbstract(this)\">📖 Show Abstract</button>\n<div class=\"paper-abstract\">As 3D Gaussian Splatting (3DGS) provides fast and high-quality novel view synthesis, it is a natural extension to deform a canonical 3DGS to multiple frames for representing a dynamic scene. However, previous works fail to accurately reconstruct complex dynamic scenes. We attribute the failure to the design of the deformation field, which is built as a coordinate-based function. This approach is problematic because 3DGS is a mixture of multiple fields centered at the Gaussians, not just a single coordinate-based framework. To resolve this problem, we define the deformation as a function of per-Gaussian embeddings and temporal embeddings. Moreover, we decompose deformations as coarse and fine deformations to model slow and fast movements, respectively. Also, we introduce a local smoothness regularization for per-Gaussian embedding to improve the details in dynamic regions.</div></div>\n    </div>\n  </div>\n</div>\n<div class=\"paper-row\" data-id=\"li2024dreamscene\" data-title=\"DreamScene: 3D Gaussian-based Text-to-3D Scene Generation via Formation Pattern Sampling\" data-authors=\"Haoran Li, Haolin Shi, Wenli Zhang, Wenjun Wu, Yong Liao, Lin Wang, Lik Hang Lee, Pengyuan Zhou\" data-year=\"2024\" data-tags='[\"Code\", \"Diffusion\", \"Project\"]'>\n  <div class=\"paper-card\">\n    <input type=\"checkbox\" class=\"selection-checkbox\" onclick=\"handleCheckboxClick(event, 'li2024dreamscene', this)\">\n    <div class=\"paper-number\"></div>\n    <div class=\"paper-thumbnail\">\n      <img data-src=\"assets/thumbnails/li2024dreamscene.jpg\" data-fallback=\"None\" alt=\"Paper thumbnail for DreamScene: 3D Gaussian-based Text-to-3D Scene Generation via Formation Pattern Sampling\" class=\"lazy\" loading=\"lazy\"/>\n    </div>\n    <div class=\"paper-content\">\n      <h2 class=\"paper-title\">DreamScene: 3D Gaussian-based Text-to-3D Scene Generation via Formation Pattern Sampling <span class=\"paper-year\">(2024)</span></h2>\n      <p class=\"paper-authors\">Haoran Li, Haolin Shi, Wenli Zhang, Wenjun Wu, Yong Liao, Lin Wang, Lik Hang Lee, Pengyuan Zhou</p>\n      <div class=\"paper-tags\"><span class=\"paper-tag\">Code</span>\n<span class=\"paper-tag\">Diffusion</span>\n<span class=\"paper-tag\">Project</span></div>\n      <div class=\"paper-links\"><a href=\"https://arxiv.org/pdf/2404.03575.pdf\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">📄 Paper</a>\n<a href=\"https://dreamscene-project.github.io/\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">🌐 Project</a>\n<a href=\"https://github.com/DreamScene-Project/DreamScene\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">💻 Code</a>\n<button class=\"abstract-toggle\" onclick=\"toggleAbstract(this)\">📖 Show Abstract</button>\n<div class=\"paper-abstract\">Text-to-3D scene generation holds immense potential for the gaming, film, and architecture sectors, increasingly capturing the attention of both academic and industry circles. Despite significant progress, current methods still struggle with maintaining high quality, consistency, and editing flexibility. In this paper, we propose DreamScene, a 3D Gaussian-based novel text-to-3D scene generation framework that leverages Formation Pattern Sampling (FPS) for core structuring, augmented with a strategic camera sampling and supported by holistic object-environment integration to overcome these hurdles. FPS, guided by the formation patterns of 3D objects, employs multi-timesteps sampling to quickly form semantically rich, high-quality representations, uses 3D Gaussian filtering for optimization stability, and leverages reconstruction techniques to generate plausible textures. The camera sampling strategy incorporates a progressive three-stage approach, specifically designed for both indoor and outdoor settings, to effectively ensure scene-wide 3D consistency. DreamScene enhances scene editing flexibility by combining objects and environments, enabling targeted adjustments. Extensive experiments showcase DreamScene's superiority over current state-of-the-art techniques, heralding its wide-ranging potential for diverse applications.</div></div>\n    </div>\n  </div>\n</div>\n<div class=\"paper-row\" data-id=\"meng2024omnigs\" data-title=\"OmniGS: Omnidirectional Gaussian Splatting for Fast Radiance Field Reconstruction using Omnidirectional Images\" data-authors=\"Jiarui Meng, Haijie Li, Yanmin Wu, Qiankun Gao, Shuzhou Yang, Jian Zhang, Siwei Ma\" data-year=\"2024\" data-tags='[\"360 degree\", \"Project\", \"Rendering\"]'>\n  <div class=\"paper-card\">\n    <input type=\"checkbox\" class=\"selection-checkbox\" onclick=\"handleCheckboxClick(event, 'meng2024omnigs', this)\">\n    <div class=\"paper-number\"></div>\n    <div class=\"paper-thumbnail\">\n      <img data-src=\"assets/thumbnails/meng2024omnigs.jpg\" data-fallback=\"None\" alt=\"Paper thumbnail for OmniGS: Omnidirectional Gaussian Splatting for Fast Radiance Field Reconstruction using Omnidirectional Images\" class=\"lazy\" loading=\"lazy\"/>\n    </div>\n    <div class=\"paper-content\">\n      <h2 class=\"paper-title\">OmniGS: Omnidirectional Gaussian Splatting for Fast Radiance Field Reconstruction using Omnidirectional Images <span class=\"paper-year\">(2024)</span></h2>\n      <p class=\"paper-authors\">Jiarui Meng, Haijie Li, Yanmin Wu, Qiankun Gao, Shuzhou Yang, Jian Zhang, Siwei Ma</p>\n      <div class=\"paper-tags\"><span class=\"paper-tag\">360 degree</span>\n<span class=\"paper-tag\">Project</span>\n<span class=\"paper-tag\">Rendering</span></div>\n      <div class=\"paper-links\"><a href=\"https://arxiv.org/pdf/2404.03202\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">📄 Paper</a>\n<a href=\"https://liquorleaf.github.io/research/OmniGS/\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">🌐 Project</a>\n<button class=\"abstract-toggle\" onclick=\"toggleAbstract(this)\">📖 Show Abstract</button>\n<div class=\"paper-abstract\">Photorealistic reconstruction relying on 3D Gaussian Splatting has shown promising potential in robotics. However, the current 3D Gaussian Splatting system only supports radiance field reconstruction using undistorted perspective images. In this paper, we present OmniGS, a novel omnidirectional Gaussian splatting system, to take advantage of omnidirectional images for fast radiance field reconstruction. Specifically, we conduct a theoretical analysis of spherical camera model derivatives in 3D Gaussian Splatting. According to the derivatives, we then implement a new GPU-accelerated omnidirectional rasterizer that directly splats 3D Gaussians onto the equirectangular screen space for omnidirectional image rendering. As a result, we realize differentiable optimization of the radiance field without the requirement of cube-map rectification or tangent-plane approximation. Extensive experiments conducted in egocentric and roaming scenarios demonstrate that our method achieves state-of-the-art reconstruction quality and high rendering speed using omnidirectional images. To benefit the research community, the code will be made publicly available once the paper is published.</div></div>\n    </div>\n  </div>\n</div>\n<div class=\"paper-row\" data-id=\"nikolakakis2024gaspct\" data-title=\"GaSpCT: Gaussian Splatting for Novel CT Projection View Synthesis\" data-authors=\"Emmanouil Nikolakakis, Utkarsh Gupta, Jonathan Vengosh, Justin Bui, Razvan Marinescu\" data-year=\"2024\" data-tags='[\"Medicine\"]'>\n  <div class=\"paper-card\">\n    <input type=\"checkbox\" class=\"selection-checkbox\" onclick=\"handleCheckboxClick(event, 'nikolakakis2024gaspct', this)\">\n    <div class=\"paper-number\"></div>\n    <div class=\"paper-thumbnail\">\n      <img data-src=\"assets/thumbnails/nikolakakis2024gaspct.jpg\" data-fallback=\"None\" alt=\"Paper thumbnail for GaSpCT: Gaussian Splatting for Novel CT Projection View Synthesis\" class=\"lazy\" loading=\"lazy\"/>\n    </div>\n    <div class=\"paper-content\">\n      <h2 class=\"paper-title\">GaSpCT: Gaussian Splatting for Novel CT Projection View Synthesis <span class=\"paper-year\">(2024)</span></h2>\n      <p class=\"paper-authors\">Emmanouil Nikolakakis, Utkarsh Gupta, Jonathan Vengosh, Justin Bui, Razvan Marinescu</p>\n      <div class=\"paper-tags\"><span class=\"paper-tag\">Medicine</span></div>\n      <div class=\"paper-links\"><a href=\"https://arxiv.org/pdf/2404.03126\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">📄 Paper</a>\n<button class=\"abstract-toggle\" onclick=\"toggleAbstract(this)\">📖 Show Abstract</button>\n<div class=\"paper-abstract\">We present GaSpCT, a novel view synthesis and 3D scene representation method used to generate novel projection views for Computer Tomography (CT) scans. We adapt the Gaussian Splatting framework to enable novel view synthesis in CT based on limited sets of 2D image projections and without the need for Structure from Motion (SfM) methodologies. Therefore, we reduce the total scanning duration and the amount of radiation dose the patient receives during the scan. We adapted the loss function to our use-case by encouraging a stronger background and foreground distinction using two sparsity promoting regularizers: a beta loss and a total variation (TV) loss. Finally, we initialize the Gaussian locations across the 3D space using a uniform prior distribution of where the brain's positioning would be expected to be within the field of view. We evaluate the performance of our model using brain CT scans from the Parkinson's Progression Markers Initiative (PPMI) dataset and demonstrate that the rendered novel views closely match the original projection views of the simulated scan, and have better performance than other implicit 3D scene representations methodologies. Furthermore, we empirically observe reduced training time compared to neural network based image synthesis for sparse-view CT image reconstruction. Finally, the memory requirements of the Gaussian Splatting representations are reduced by 17% compared to the equivalent voxel grid image representations.</div></div>\n    </div>\n  </div>\n</div>\n<div class=\"paper-row\" data-id=\"zhao2024tclcgs\" data-title=\"TCLC-GS: Tightly Coupled LiDAR-Camera Gaussian Splatting for Surrounding Autonomous Driving Scenes\" data-authors=\"Cheng Zhao, Su Sun, Ruoyu Wang, Yuliang Guo, Jun-Jun Wan, Zhou Huang, Xinyu Huang, Yingjie Victor Chen, Liu Ren\" data-year=\"2024\" data-tags='[\"Autonomous Driving\", \"Lidar\", \"Video\"]'>\n  <div class=\"paper-card\">\n    <input type=\"checkbox\" class=\"selection-checkbox\" onclick=\"handleCheckboxClick(event, 'zhao2024tclcgs', this)\">\n    <div class=\"paper-number\"></div>\n    <div class=\"paper-thumbnail\">\n      <img data-src=\"assets/thumbnails/zhao2024tclcgs.jpg\" data-fallback=\"None\" alt=\"Paper thumbnail for TCLC-GS: Tightly Coupled LiDAR-Camera Gaussian Splatting for Surrounding Autonomous Driving Scenes\" class=\"lazy\" loading=\"lazy\"/>\n    </div>\n    <div class=\"paper-content\">\n      <h2 class=\"paper-title\">TCLC-GS: Tightly Coupled LiDAR-Camera Gaussian Splatting for Surrounding Autonomous Driving Scenes <span class=\"paper-year\">(2024)</span></h2>\n      <p class=\"paper-authors\">Cheng Zhao, Su Sun, Ruoyu Wang, Yuliang Guo, Jun-Jun Wan, Zhou Huang, Xinyu Huang, Yingjie Victor Chen, Liu Ren</p>\n      <div class=\"paper-tags\"><span class=\"paper-tag\">Autonomous Driving</span>\n<span class=\"paper-tag\">Lidar</span>\n<span class=\"paper-tag\">Video</span></div>\n      <div class=\"paper-links\"><a href=\"https://arxiv.org/pdf/2404.02410.pdf\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">📄 Paper</a>\n<a href=\"https://www.youtube.com/watch?v=CEo6mZ6FGg0\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">🎥 Video</a>\n<button class=\"abstract-toggle\" onclick=\"toggleAbstract(this)\">📖 Show Abstract</button>\n<div class=\"paper-abstract\">Most 3D Gaussian Splatting (3D-GS) based methods for urban scenes initialize 3D Gaussians directly with 3D LiDAR points, which not only underutilizes LiDAR data capabilities but also overlooks the potential advantages of fusing LiDAR with camera data. In this paper, we design a novel tightly coupled LiDAR-Camera Gaussian Splatting (TCLC-GS) to fully leverage the combined strengths of both LiDAR and camera sensors, enabling rapid, high-quality 3D reconstruction and novel view RGB/depth synthesis. TCLC-GS designs a hybrid explicit (colorized 3D mesh) and implicit (hierarchical octree feature) 3D representation derived from LiDAR-camera data, to enrich the properties of 3D Gaussians for splatting. 3D Gaussian's properties are not only initialized in alignment with the 3D mesh which provides more completed 3D shape and color information, but are also endowed with broader contextual information through retrieved octree implicit features. During the Gaussian Splatting optimization process, the 3D mesh offers dense depth information as supervision, which enhances the training process by learning of a robust geometry. Comprehensive evaluations conducted on the Waymo Open Dataset and nuScenes Dataset validate our method's state-of-the-art (SOTA) performance. Utilizing a single NVIDIA RTX 3090 Ti, our method demonstrates fast training and achieves real-time RGB and depth rendering at 90 FPS in resolution of 1920x1280 (Waymo), and 120 FPS in resolution of 1600x900 (nuScenes) in urban scenarios.</div></div>\n    </div>\n  </div>\n</div>\n<div class=\"paper-row\" data-id=\"wolf2024gs2mesh\" data-title=\"GS2Mesh: Surface Reconstruction from Gaussian Splatting via Novel Stereo Views\" data-authors=\"Yaniv Wolf, Amit Bracha, Ron Kimmel\" data-year=\"2024\" data-tags='[\"2DGS\", \"Code\", \"Meshing\", \"Project\", \"Stereo\", \"Video\"]'>\n  <div class=\"paper-card\">\n    <input type=\"checkbox\" class=\"selection-checkbox\" onclick=\"handleCheckboxClick(event, 'wolf2024gs2mesh', this)\">\n    <div class=\"paper-number\"></div>\n    <div class=\"paper-thumbnail\">\n      <img data-src=\"assets/thumbnails/wolf2024gs2mesh.jpg\" data-fallback=\"None\" alt=\"Paper thumbnail for GS2Mesh: Surface Reconstruction from Gaussian Splatting via Novel Stereo Views\" class=\"lazy\" loading=\"lazy\"/>\n    </div>\n    <div class=\"paper-content\">\n      <h2 class=\"paper-title\">GS2Mesh: Surface Reconstruction from Gaussian Splatting via Novel Stereo Views <span class=\"paper-year\">(2024)</span></h2>\n      <p class=\"paper-authors\">Yaniv Wolf, Amit Bracha, Ron Kimmel</p>\n      <div class=\"paper-tags\"><span class=\"paper-tag\">2DGS</span>\n<span class=\"paper-tag\">Code</span>\n<span class=\"paper-tag\">Meshing</span>\n<span class=\"paper-tag\">Project</span>\n<span class=\"paper-tag\">Stereo</span>\n<span class=\"paper-tag\">Video</span></div>\n      <div class=\"paper-links\"><a href=\"https://arxiv.org/pdf/2404.01810\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">📄 Paper</a>\n<a href=\"https://gs2mesh.github.io//\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">🌐 Project</a>\n<a href=\"https://github.com/yanivw12/gs2mesh\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">💻 Code</a>\n<a href=\"https://youtu.be/cjtmLDD8YZk\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">🎥 Video</a>\n<button class=\"abstract-toggle\" onclick=\"toggleAbstract(this)\">📖 Show Abstract</button>\n<div class=\"paper-abstract\">Recently, 3D Gaussian Splatting (3DGS) has emerged as an efficient approach for accurately representing scenes. However, despite its superior novel view synthesis capabilities, extracting the geometry of the scene directly from the Gaussian properties remains a challenge, as those are optimized based on a photometric loss. While some concurrent models have tried adding geometric constraints during the Gaussian optimization process, they still produce noisy, unrealistic surfaces. We propose a novel approach for bridging the gap between the noisy 3DGS representation and the smooth 3D mesh representation, by injecting real-world knowledge into the depth extraction process. Instead of extracting the geometry of the scene directly from the Gaussian properties, we instead extract the geometry through a pre-trained stereo-matching model. We render stereo-aligned pairs of images corresponding to the original training poses, feed the pairs into a stereo model to get a depth profile, and finally fuse all of the profiles together to get a single mesh. The resulting reconstruction is smoother, more accurate and shows more intricate details compared to other methods for surface reconstruction from Gaussian Splatting, while only requiring a small overhead on top of the fairly short 3DGS optimization process. We performed extensive testing of the proposed method on in-the-wild scenes, obtained using a smartphone, showcasing its superior reconstruction abilities. Additionally, we tested the method on the Tanks and Temples and DTU benchmarks, achieving state-of-the-art results.</div></div>\n    </div>\n  </div>\n</div>\n<div class=\"paper-row\" data-id=\"qiu2024feature\" data-title=\"Feature Splatting: Language-Driven Physics-Based Scene Synthesis and Editing\" data-authors=\"Ri-Zhao Qiu, Ge Yang, Weijia Zeng, Xiaolong Wang\" data-year=\"2024\" data-tags='[\"Code\", \"Editing\", \"Language Embedding\", \"Physics\", \"Project\", \"Segmentation\", \"Video\"]'>\n  <div class=\"paper-card\">\n    <input type=\"checkbox\" class=\"selection-checkbox\" onclick=\"handleCheckboxClick(event, 'qiu2024feature', this)\">\n    <div class=\"paper-number\"></div>\n    <div class=\"paper-thumbnail\">\n      <img data-src=\"assets/thumbnails/qiu2024feature.jpg\" data-fallback=\"None\" alt=\"Paper thumbnail for Feature Splatting: Language-Driven Physics-Based Scene Synthesis and Editing\" class=\"lazy\" loading=\"lazy\"/>\n    </div>\n    <div class=\"paper-content\">\n      <h2 class=\"paper-title\">Feature Splatting: Language-Driven Physics-Based Scene Synthesis and Editing <span class=\"paper-year\">(2024)</span></h2>\n      <p class=\"paper-authors\">Ri-Zhao Qiu, Ge Yang, Weijia Zeng, Xiaolong Wang</p>\n      <div class=\"paper-tags\"><span class=\"paper-tag\">Code</span>\n<span class=\"paper-tag\">Editing</span>\n<span class=\"paper-tag\">Language Embedding</span>\n<span class=\"paper-tag\">Physics</span>\n<span class=\"paper-tag\">Project</span>\n<span class=\"paper-tag\">Segmentation</span>\n<span class=\"paper-tag\">Video</span></div>\n      <div class=\"paper-links\"><a href=\"https://arxiv.org/pdf/2404.01223\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">📄 Paper</a>\n<a href=\"https://feature-splatting.github.io/\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">🌐 Project</a>\n<a href=\"https://github.com/vuer-ai/feature_splatting\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">💻 Code</a>\n<a href=\"https://feature-splatting.github.io/resources/teaser_overview.mp4\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">🎥 Video</a>\n<button class=\"abstract-toggle\" onclick=\"toggleAbstract(this)\">📖 Show Abstract</button>\n<div class=\"paper-abstract\">Scene representations using 3D Gaussian primitives have produced excellent results in modeling the appearance of static and dynamic 3D scenes. Many graphics applications, however, demand the ability to manipulate both the appearance and the physical properties of objects. We introduce Feature Splatting, an approach that unifies physics-based dynamic scene synthesis with rich semantics from vision language foundation models that are grounded by natural language. Our first contribution is a way to distill high-quality, object-centric vision-language features into 3D Gaussians, that enables semi-automatic scene decomposition using text queries. Our second contribution is a way to synthesize physics-based dynamics from an otherwise static scene using a particle-based simulator, in which material properties are assigned automatically via text queries. We ablate key techniques used in this pipeline, to illustrate the challenge and opportunities in using feature-carrying 3D Gaussians as a unified format for appearance, geometry, material properties and semantics grounded on natural language.</div></div>\n    </div>\n  </div>\n</div>\n<div class=\"paper-row\" data-id=\"meng2024mirror3dgs\" data-title=\"Mirror-3DGS: Incorporating Mirror Reflections into 3D Gaussian Splatting\" data-authors=\"Jiarui Meng, Haijie Li, Yanmin Wu, Qiankun Gao, Shuzhou Yang, Jian Zhang, Siwei Ma\" data-year=\"2024\" data-tags='[\"Project\", \"Rendering\"]'>\n  <div class=\"paper-card\">\n    <input type=\"checkbox\" class=\"selection-checkbox\" onclick=\"handleCheckboxClick(event, 'meng2024mirror3dgs', this)\">\n    <div class=\"paper-number\"></div>\n    <div class=\"paper-thumbnail\">\n      <img data-src=\"assets/thumbnails/meng2024mirror3dgs.jpg\" data-fallback=\"None\" alt=\"Paper thumbnail for Mirror-3DGS: Incorporating Mirror Reflections into 3D Gaussian Splatting\" class=\"lazy\" loading=\"lazy\"/>\n    </div>\n    <div class=\"paper-content\">\n      <h2 class=\"paper-title\">Mirror-3DGS: Incorporating Mirror Reflections into 3D Gaussian Splatting <span class=\"paper-year\">(2024)</span></h2>\n      <p class=\"paper-authors\">Jiarui Meng, Haijie Li, Yanmin Wu, Qiankun Gao, Shuzhou Yang, Jian Zhang, Siwei Ma</p>\n      <div class=\"paper-tags\"><span class=\"paper-tag\">Project</span>\n<span class=\"paper-tag\">Rendering</span></div>\n      <div class=\"paper-links\"><a href=\"https://arxiv.org/pdf/2404.01168.pdf\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">📄 Paper</a>\n<a href=\"https://mirror-gaussian.github.io/\" class=\"paper-link\" target=\"_blank\" rel=\"noopener\">🌐 Project</a>\n<button class=\"abstract-toggle\" onclick=\"toggleAbstract(this)\">📖 Show Abstract</button>\n<div class=\"paper-abstract\">3D Gaussian Splatting (3DGS) has marked a significant breakthrough in the realm of 3D scene reconstruction and novel view synthesis. However, 3DGS, much like its predecessor Neural Radiance Fields (NeRF), struggles to accurately model physical reflections, particularly in mirrors that are ubiquitous in real-world scenes. This oversight mistakenly perceives reflections as separate entities that physically exist, resulting in inaccurate reco"
        },
        {
          "name": "requirements.txt",
          "type": "blob",
          "size": 0.376953125,
          "content": "arxiv==2.1.3\ncertifi==2024.12.14\ncffi==1.17.1\ncharset-normalizer==3.4.1\ncryptography==44.0.0\nDeprecated==1.2.15\nfeedparser==6.0.11\nidna==3.10\npdf2image==1.17.0\npillow==11.1.0\npycparser==2.22\nPyGithub==2.5.0\nPyJWT==2.10.1\nPyNaCl==1.5.0\nPyQt6==6.8.0\nPyQt6-Qt6==6.8.1\nPyQt6_sip==13.9.1\nPyYAML==6.0.2\nrequests==2.32.3\nsgmllib3k==1.0.0\ntyping_extensions==4.12.2\nurllib3==2.3.0\nwrapt==1.17.0\n"
        },
        {
          "name": "src",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}