{
  "metadata": {
    "timestamp": 1736709519303,
    "page": 32,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjQw",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "LianjiaTech/BELLE",
      "stars": 8017,
      "defaultBranch": "main",
      "files": [
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.1396484375,
          "content": "*.tmp\n*.swp\n__pycache__\n.vscode\nregen.json\n.ipynb_checkpoints\n.idea\nsaved_models\n*.code-workspace\n.hypothesis\ninfer_res\nwandb\nhf_cache_dir\ndata"
        },
        {
          "name": "DATA_LICENSE",
          "type": "blob",
          "size": 19.326171875,
          "content": "Attribution License (ODC-By)\nPREAMBLE\nThe Open Data Commons Attribution License is a license agreement intended to allow users to freely share, modify, and use this Database subject only to the attribution requirements set out in Section 4.\n\nDatabases can contain a wide variety of types of content (images, audiovisual material, and sounds all in the same database, for example), and so this license only governs the rights over the Database, and not the contents of the Database individually. Licensors may therefore wish to use this license together with another license for the contents.\n\nSometimes the contents of a database, or the database itself, can be covered by other rights not addressed here (such as private contracts, trademark over the name, or privacy rights / data protection rights over information in the contents), and so you are advised that you may have to consult other documents or clear other rights before doing activities not covered by this License.\n\nThe Licensor (as defined below)\n\nand\n\nYou (as defined below)\n\nagree as follows:\n\n1.0 DEFINITIONS OF CAPITALISED WORDS\nâ€œCollective Databaseâ€ â€“ Means this Database in unmodified form as part of a collection of independent databases in themselves that together are assembled into a collective whole. A work that constitutes a Collective Database will not be considered a Derivative Database.\n\nâ€œConveyâ€ â€“ As a verb, means Using the Database, a Derivative Database, or the Database as part of a Collective Database in any way that enables a Person to make or receive copies of the Database or a Derivative Database. Conveying does not include interaction with a user through a computer network, or creating and Using a Produced Work, where no transfer of a copy of the Database or a Derivative Database occurs.\n\nâ€œContentsâ€ â€“ The contents of this Database, which includes the information, independent works, or other material collected into the Database. For example, the contents of the Database could be factual data or works such as images, audiovisual material, text, or sounds.\n\nâ€œDatabaseâ€ â€“ A collection of material (the Contents) arranged in a systematic or methodical way and individually accessible by electronic or other means offered under the terms of this License.\n\nâ€œDatabase Directiveâ€ â€“ Means Directive 96/9/EC of the European Parliament and of the Council of 11 March 1996 on the legal protection of databases, as amended or succeeded.\n\nâ€œDatabase Rightâ€ â€“ Means rights resulting from the Chapter III (â€œsui generisâ€) rights in the Database Directive (as amended and as transposed by member states), which includes the Extraction and Re-utilisation of the whole or a Substantial part of the Contents, as well as any similar rights available in the relevant jurisdiction under Section 10.4.\n\nâ€œDerivative Databaseâ€ â€“ Means a database based upon the Database, and includes any translation, adaptation, arrangement, modification, or any other alteration of the Database or of a Substantial part of the Contents. This includes, but is not limited to, Extracting or Re-utilising the whole or a Substantial part of the Contents in a new Database.\n\nâ€œExtractionâ€ â€“ Means the permanent or temporary transfer of all or a Substantial part of the Contents to another medium by any means or in any form.\n\nâ€œLicenseâ€ â€“ Means this license agreement and is both a license of rights such as copyright and Database Rights and an agreement in contract.\n\nâ€œLicensorâ€ â€“ Means the Person that offers the Database under the terms of this License.\n\nâ€œPersonâ€ â€“ Means a natural or legal person or a body of persons corporate or incorporate.\n\nâ€œProduced Workâ€ â€“ a work (such as an image, audiovisual material, text, or sounds) resulting from using the whole or a Substantial part of the Contents (via a search or other query) from this Database, a Derivative Database, or this Database as part of a Collective Database.\n\nâ€œPubliclyâ€ â€“ means to Persons other than You or under Your control by either more than 50% ownership or by the power to direct their activities (such as contracting with an independent consultant).\n\nâ€œRe-utilisationâ€ â€“ means any form of making available to the public all or a Substantial part of the Contents by the distribution of copies, by renting, by online or other forms of transmission.\n\nâ€œSubstantialâ€ â€“ Means substantial in terms of quantity or quality or a combination of both. The repeated and systematic Extraction or Re-utilisation of insubstantial parts of the Contents may amount to the Extraction or Re-utilisation of a Substantial part of the Contents.\n\nâ€œUseâ€ â€“ As a verb, means doing any act that is restricted by copyright or Database Rights whether in the original medium or any other; and includes without limitation distributing, copying, publicly performing, publicly displaying, and preparing derivative works of the Database, as well as modifying the Database as may be technically necessary to use it in a different mode or format.\n\nâ€œYouâ€ â€“ Means a Person exercising rights under this License who has not previously violated the terms of this License with respect to the Database, or who has received express permission from the Licensor to exercise rights under this License despite a previous violation.\n\nWords in the singular include the plural and vice versa.\n\n2.0 WHAT THIS LICENSE COVERS\n2.1. Legal effect of this document. This License is:\n\na. A license of applicable copyright and neighbouring rights;\n\nb. A license of the Database Right; and\n\nc. An agreement in contract between You and the Licensor.\n\n2.2 Legal rights covered. This License covers the legal rights in the Database, including:\n\na. Copyright. Any copyright or neighbouring rights in the Database. The copyright licensed includes any individual elements of the Database, but does not cover the copyright over the Contents independent of this Database. See Section 2.4 for details. Copyright law varies between jurisdictions, but is likely to cover: the Database model or schema, which is the structure, arrangement, and organisation of the Database, and can also include the Database tables and table indexes; the data entry and output sheets; and the Field names of Contents stored in the Database;\n\nb. Database Rights. Database Rights only extend to the Extraction and Re-utilisation of the whole or a Substantial part of the Contents. Database Rights can apply even when there is no copyright over the Database. Database Rights can also apply when the Contents are removed from the Database and are selected and arranged in a way that would not infringe any applicable copyright; and\n\nc. Contract. This is an agreement between You and the Licensor for access to the Database. In return you agree to certain conditions of use on this access as outlined in this License.\n\n2.3 Rights not covered.\n\na. This License does not apply to computer programs used in the making or operation of the Database;\n\nb. This License does not cover any patents over the Contents or the Database; and\n\nc. This License does not cover any trademarks associated with the Database.\n\n2.4 Relationship to Contents in the Database. The individual items of the Contents contained in this Database may be covered by other rights, including copyright, patent, data protection, privacy, or personality rights, and this License does not cover any rights (other than Database Rights or in contract) in individual Contents contained in the Database.\n\nFor example, if used on a Database of images (the Contents), this License would not apply to copyright over individual images, which could have their own separate licenses, or one single license covering all of the rights over the images.\n\n3.0 RIGHTS GRANTED\n3.1 Subject to the terms and conditions of this License, the Licensor grants to You a worldwide, royalty-free, non-exclusive, terminable (but only under Section 9) license to Use the Database for the duration of any applicable copyright and Database Rights. These rights explicitly include commercial use, and do not exclude any field of endeavour. To the extent possible in the relevant jurisdiction, these rights may be exercised in all media and formats whether now known or created in the future.\n\nThe rights granted cover, for example:\n\na. Extraction and Re-utilisation of the whole or a Substantial part of the Contents;\n\nb. Creation of Derivative Databases;\n\nc. Creation of Collective Databases;\n\nd. Creation of temporary or permanent reproductions by any means and in any form, in whole or in part, including of any Derivative Databases or as a part of Collective Databases; and\n\ne. Distribution, communication, display, lending, making available, or performance to the public by any means and in any form, in whole or in part, including of any Derivative Database or as a part of Collective Databases.\n\n3.2 Compulsory license schemes. For the avoidance of doubt:\n\na. Non-waivable compulsory license schemes. In those jurisdictions in which the right to collect royalties through any statutory or compulsory licensing scheme cannot be waived, the Licensor reserves the exclusive right to collect such royalties for any exercise by You of the rights granted under this License;\n\nb. Waivable compulsory license schemes. In those jurisdictions in which the right to collect royalties through any statutory or compulsory licensing scheme can be waived, the Licensor waives the exclusive right to collect such royalties for any exercise by You of the rights granted under this License; and,\n\nc. Voluntary license schemes. The Licensor waives the right to collect royalties, whether individually or, in the event that the Licensor is a member of a collecting society that administers voluntary licensing schemes, via that society, from any exercise by You of the rights granted under this License.\n\n3.3 The right to release the Database under different terms, or to stop distributing or making available the Database, is reserved. Note that this Database may be multiple-licensed, and so You may have the choice of using alternative licenses for this Database. Subject to Section 10.4, all other rights not expressly granted by Licensor are reserved.\n\n4.0 CONDITIONS OF USE\n4.1 The rights granted in Section 3 above are expressly made subject to Your complying with the following conditions of use. These are important conditions of this License, and if You fail to follow them, You will be in material breach of its terms.\n\n4.2 Notices. If You Publicly Convey this Database, any Derivative Database, or the Database as part of a Collective Database, then You must:\n\na. Do so only under the terms of this License;\n\nb. Include a copy of this License or its Uniform Resource Identifier (URI) with the Database or Derivative Database, including both in the Database or Derivative Database and in any relevant documentation;\n\nc. Keep intact any copyright or Database Right notices and notices that refer to this License; and\n\nd. If it is not possible to put the required notices in a particular file due to its structure, then You must include the notices in a location (such as a relevant directory) where users would be likely to look for it.\n\n4.3 Notice for using output (Contents). Creating and Using a Produced Work does not require the notice in Section 4.2. However, if you Publicly Use a Produced Work, You must include a notice associated with the Produced Work reasonably calculated to make any Person that uses, views, accesses, interacts with, or is otherwise exposed to the Produced Work aware that Content was obtained from the Database, Derivative Database, or the Database as part of a Collective Database, and that it is available under this License.\n\na. Example notice. The following text will satisfy notice under Section 4.3:\n\nContains information from DATABASE NAME which is made available\nunder the ODC Attribution License.\nDATABASE NAME should be replaced with the name of the Database and a hyperlink to the location of the Database. â€œODC Attribution Licenseâ€ should contain a hyperlink to the URI of the text of this License. If hyperlinks are not possible, You should include the plain text of the required URIâ€™s with the above notice.\n\n4.4 Licensing of others. You may not sublicense the Database. Each time You communicate the Database, the whole or Substantial part of the Contents, or any Derivative Database to anyone else in any way, the Licensor offers to the recipient a license to the Database on the same terms and conditions as this License. You are not responsible for enforcing compliance by third parties with this License, but You may enforce any rights that You have over a Derivative Database. You are solely responsible for any modifications of a Derivative Database made by You or another Person at Your direction. You may not impose any further restrictions on the exercise of the rights granted or affirmed under this License.\n\n5.0 MORAL RIGHTS\n5.1 Moral rights. This section covers moral rights, including any rights to be identified as the author of the Database or to object to treatment that would otherwise prejudice the authorâ€™s honour and reputation, or any other derogatory treatment:\n\na. For jurisdictions allowing waiver of moral rights, Licensor waives all moral rights that Licensor may have in the Database to the fullest extent possible by the law of the relevant jurisdiction under Section 10.4;\n\nb. If waiver of moral rights under Section 5.1 a in the relevant jurisdiction is not possible, Licensor agrees not to assert any moral rights over the Database and waives all claims in moral rights to the fullest extent possible by the law of the relevant jurisdiction under Section 10.4; and\n\nc. For jurisdictions not allowing waiver or an agreement not to assert moral rights under Section 5.1 a and b, the author may retain their moral rights over certain aspects of the Database.\n\nPlease note that some jurisdictions do not allow for the waiver of moral rights, and so moral rights may still subsist over the Database in some jurisdictions.\n\n6.0 FAIR DEALING, DATABASE EXCEPTIONS, AND OTHER RIGHTS NOT AFFECTED\n6.1 This License does not affect any rights that You or anyone else may independently have under any applicable law to make any use of this Database, including without limitation:\n\na. Exceptions to the Database Right including: Extraction of Contents from non-electronic Databases for private purposes, Extraction for purposes of illustration for teaching or scientific research, and Extraction or Re-utilisation for public security or an administrative or judicial procedure.\n\nb. Fair dealing, fair use, or any other legally recognised limitation or exception to infringement of copyright or other applicable laws.\n\n6.2 This License does not affect any rights of lawful users to Extract and Re-utilise insubstantial parts of the Contents, evaluated quantitatively or qualitatively, for any purposes whatsoever, including creating a Derivative Database (subject to other rights over the Contents, see Section 2.4). The repeated and systematic Extraction or Re-utilisation of insubstantial parts of the Contents may however amount to the Extraction or Re-utilisation of a Substantial part of the Contents.\n\n7.0 WARRANTIES AND DISCLAIMER\n7.1 The Database is licensed by the Licensor â€œas isâ€ and without any warranty of any kind, either express, implied, or arising by statute, custom, course of dealing, or trade usage. Licensor specifically disclaims any and all implied warranties or conditions of title, non-infringement, accuracy or completeness, the presence or absence of errors, fitness for a particular purpose, merchantability, or otherwise. Some jurisdictions do not allow the exclusion of implied warranties, so this exclusion may not apply to You.\n\n8.0 LIMITATION OF LIABILITY\n8.1 Subject to any liability that may not be excluded or limited by law, the Licensor is not liable for, and expressly excludes, all liability for loss or damage however and whenever caused to anyone by any use under this License, whether by You or by anyone else, and whether caused by any fault on the part of the Licensor or not. This exclusion of liability includes, but is not limited to, any special, incidental, consequential, punitive, or exemplary damages such as loss of revenue, data, anticipated profits, and lost business. This exclusion applies even if the Licensor has been advised of the possibility of such damages.\n\n8.2 If liability may not be excluded by law, it is limited to actual and direct financial loss to the extent it is caused by proved negligence on the part of the Licensor.\n\n9.0 TERMINATION OF YOUR RIGHTS UNDER THIS LICENSE\n9.1 Any breach by You of the terms and conditions of this License automatically terminates this License with immediate effect and without notice to You. For the avoidance of doubt, Persons who have received the Database, the whole or a Substantial part of the Contents, Derivative Databases, or the Database as part of a Collective Database from You under this License will not have their licenses terminated provided their use is in full compliance with this License or a license granted under Section 4.8 of this License. Sections 1, 2, 7, 8, 9 and 10 will survive any termination of this License.\n\n9.2 If You are not in breach of the terms of this License, the Licensor will not terminate Your rights under it.\n\n9.3 Unless terminated under Section 9.1, this License is granted to You for the duration of applicable rights in the Database.\n\n9.4 Reinstatement of rights. If you cease any breach of the terms and conditions of this License, then your full rights under this License will be reinstated:\n\na. Provisionally and subject to permanent termination until the 60th day after cessation of breach;\n\nb. Permanently on the 60th day after cessation of breach unless otherwise reasonably notified by the Licensor; or\n\nc. Permanently if reasonably notified by the Licensor of the violation, this is the first time You have received notice of violation of this License from the Licensor, and You cure the violation prior to 30 days after your receipt of the notice.\n\n9.5 Notwithstanding the above, Licensor reserves the right to release the Database under different license terms or to stop distributing or making available the Database. Releasing the Database under different license terms or stopping the distribution of the Database will not withdraw this License (or any other license that has been, or is required to be, granted under the terms of this License), and this License will continue in full force and effect unless terminated as stated above.\n\n10.0 GENERAL\n10.1 If any provision of this License is held to be invalid or unenforceable, that must not affect the validity or enforceability of the remainder of the terms and conditions of this License and each remaining provision of this License shall be valid and enforced to the fullest extent permitted by law.\n\n10.2 This License is the entire agreement between the parties with respect to the rights granted here over the Database. It replaces any earlier understandings, agreements or representations with respect to the Database.\n\n10.3 If You are in breach of the terms of this License, You will not be entitled to rely on the terms of this License or to complain of any breach by the Licensor.\n\n10.4 Choice of law. This License takes effect in and will be governed by the laws of the relevant jurisdiction in which the License terms are sought to be enforced. If the standard suite of rights granted under applicable copyright law and Database Rights in the relevant jurisdiction includes additional rights not granted under this License, these additional rights are granted in this License in order to meet the terms of this License."
        },
        {
          "name": "DISCLAIMER",
          "type": "blob",
          "size": 3.4453125,
          "content": "The software project, data, and models provided by our GitHub project are provided \"as is,\" without warranty of any kind, express or implied, including but not limited to the warranties of merchantability, fitness for a particular purpose, and non-infringement.\n\nIn no event shall the project owners or contributors be liable for any direct, indirect, incidental, special, exemplary, or consequential damages (including, but not limited to, procurement of substitute goods or services; loss of use, data, or profits; or business interruption) however caused and on any theory of liability, whether in contract, strict liability, or tort (including negligence or otherwise) arising in any way out of the use of this software project, data, or models, even if advised of the possibility of such damage.\n\nUsers of this software project, data, and models are solely responsible for any consequences of their use. The project owners and contributors shall not be held responsible for any subsequent or potential harm caused by the use of this software project, data, or models.\n\nBy using this software project, data, or models, users accept and agree to this disclaimer. If users do not agree to the terms of this disclaimer, they should not use this software project, data, or models.\n\nIt is important to note that this software project, data, and models are still in the research phase and are provided for experimental purposes only. As such, the project owners and contributors do not guarantee the accuracy, completeness, or usefulness of the software project, data, or models.\n\nFurthermore, due to the experimental nature of this software project, data, and models, it is possible that they may contain or generate inappropriate responses, errors, or inconsistencies. Users should exercise caution when using this software project, data, or models, and should not rely solely on them for any critical or sensitive tasks.\n\nThe project owners and contributors shall not be held responsible for any damages, losses, or liabilities arising from the use of this software project, data, or models, including but not limited to, any inappropriate responses generated by the software project, data, or models.\n\nBy using this software project, data, or models, users acknowledge and accept the experimental nature of the software project, data, and models, and understand the potential risks and limitations associated with their use. If users do not agree to the terms of this disclaimer, they should not use this software project, data, or models.\n\nThe software project, data, and models provided by our GitHub project are intended for research purposes only. They should not be used for any commercial, business, or legal purposes, and should not be relied upon as a substitute for professional advice or judgment.\n\nUsers of this software project, data, and models are strictly prohibited from using them for any commercial purposes, including but not limited to, selling, licensing, or distributing the software project, data, or models to third parties.\n\nThe project owners and contributors shall not be held responsible for any damages, losses, or liabilities arising from the use of this software project, data, or models for any commercial or business purposes.\n\nBy using this software project, data, or models, users agree to use them for research purposes only, and not for any commercial or business purposes. If users do not agree to the terms of this disclaimer, they should not use this software project, data, or models.\n"
        },
        {
          "name": "HOW_TO_CONTRIBUTE.md",
          "type": "blob",
          "size": 2.078125,
          "content": "Welcome to BELLE project! We appreciate your interest in contributing to our project.\nIn order to make the contribution process as smooth as possible, we have established some\nguidelines to help you submit your contributions. Please take a few minutes to review the\nfollowing guidelines before you start contributing.\n\n\n## How to Contribute Code\n\n1. Fork the repository and clone it locally.\n2. Create a new branch for your contribution using a descriptive name.\n3. Make your changes and ensure that they are properly tested.\n4. Submit a pull request to the master branch of our repository.\n\n\n## How to Contribute Data\n\nIf you are contributing prompts or prompt seeds, please open up a new issue with the following\ntitle format: [New Prompt]: or [New Prompt Seed]:.\n\nIf you are contributing new dataset:\n\n1. Please check the format of our [official dataset](https://huggingface.co/datasets/BelleGroup/train_2M_CN).\n2. Upload your dataset somewhere, e.g. HuggingFace.\n3. Create a new issue of the title: [Contributing Data]:. Describe the dataset, e.g. scale, contents, etc.\n4. Include link to your dataset in the issue.\n\n\n## Contribution Guidelines\n\nPlease ensure that your contributions adhere to the following guidelines:\n\n1. Follow the coding style and conventions used in the project.\n2. Make sure that your contribution is well-documented and easy to understand\n3. Keep your contributions concise and focused. If you are making multiple changes, consider breaking them into separate pull requests.\n4. Do not submit contributions that include proprietary or confidential information.\n\n\n## Reporting Issues\n\nIf you encounter any issues while using our project, please report them through our issue\ntracker. Please provide as much information as possible about the issue, including steps\nto reproduce the problem.\n\nBefore submitting an issue, please search through existing issues first :)\n\n\n## Conclusion\n\nThank you for taking the time to read through these guidelines. We appreciate your\ncontributions and look forward to working with you! If you have any questions or concerns,\nplease reach out to the project maintainers.\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 11.1298828125,
          "content": "                                Apache License\n                           Version 2.0, January 2004\n                        http://www.apache.org/licenses/\n\n   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n\n   1. Definitions.\n\n      \"License\" shall mean the terms and conditions for use, reproduction,\n      and distribution as defined by Sections 1 through 9 of this document.\n\n      \"Licensor\" shall mean the copyright owner or entity authorized by\n      the copyright owner that is granting the License.\n\n      \"Legal Entity\" shall mean the union of the acting entity and all\n      other entities that control, are controlled by, or are under common\n      control with that entity. For the purposes of this definition,\n      \"control\" means (i) the power, direct or indirect, to cause the\n      direction or management of such entity, whether by contract or\n      otherwise, or (ii) ownership of fifty percent (50%) or more of the\n      outstanding shares, or (iii) beneficial ownership of such entity.\n\n      \"You\" (or \"Your\") shall mean an individual or Legal Entity\n      exercising permissions granted by this License.\n\n      \"Source\" form shall mean the preferred form for making modifications,\n      including but not limited to software source code, documentation\n      source, and configuration files.\n\n      \"Object\" form shall mean any form resulting from mechanical\n      transformation or translation of a Source form, including but\n      not limited to compiled object code, generated documentation,\n      and conversions to other media types.\n\n      \"Work\" shall mean the work of authorship, whether in Source or\n      Object form, made available under the License, as indicated by a\n      copyright notice that is included in or attached to the work\n      (an example is provided in the Appendix below).\n\n      \"Derivative Works\" shall mean any work, whether in Source or Object\n      form, that is based on (or derived from) the Work and for which the\n      editorial revisions, annotations, elaborations, or other modifications\n      represent, as a whole, an original work of authorship. For the purposes\n      of this License, Derivative Works shall not include works that remain\n      separable from, or merely link (or bind by name) to the interfaces of,\n      the Work and Derivative Works thereof.\n\n      \"Contribution\" shall mean any work of authorship, including\n      the original version of the Work and any modifications or additions\n      to that Work or Derivative Works thereof, that is intentionally\n      submitted to Licensor for inclusion in the Work by the copyright owner\n      or by an individual or Legal Entity authorized to submit on behalf of\n      the copyright owner. For the purposes of this definition, \"submitted\"\n      means any form of electronic, verbal, or written communication sent\n      to the Licensor or its representatives, including but not limited to\n      communication on electronic mailing lists, source code control systems,\n      and issue tracking systems that are managed by, or on behalf of, the\n      Licensor for the purpose of discussing and improving the Work, but\n      excluding communication that is conspicuously marked or otherwise\n      designated in writing by the copyright owner as \"Not a Contribution.\"\n\n      \"Contributor\" shall mean Licensor and any individual or Legal Entity\n      on behalf of whom a Contribution has been received by Licensor and\n      subsequently incorporated within the Work.\n\n   2. Grant of Copyright License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      copyright license to reproduce, prepare Derivative Works of,\n      publicly display, publicly perform, sublicense, and distribute the\n      Work and such Derivative Works in Source or Object form.\n\n   3. Grant of Patent License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      (except as stated in this section) patent license to make, have made,\n      use, offer to sell, sell, import, and otherwise transfer the Work,\n      where such license applies only to those patent claims licensable\n      by such Contributor that are necessarily infringed by their\n      Contribution(s) alone or by combination of their Contribution(s)\n      with the Work to which such Contribution(s) was submitted. If You\n      institute patent litigation against any entity (including a\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\n      or a Contribution incorporated within the Work constitutes direct\n      or contributory patent infringement, then any patent licenses\n      granted to You under this License for that Work shall terminate\n      as of the date such litigation is filed.\n\n   4. Redistribution. You may reproduce and distribute copies of the\n      Work or Derivative Works thereof in any medium, with or without\n      modifications, and in Source or Object form, provided that You\n      meet the following conditions:\n\n      (a) You must give any other recipients of the Work or\n          Derivative Works a copy of this License; and\n\n      (b) You must cause any modified files to carry prominent notices\n          stating that You changed the files; and\n\n      (c) You must retain, in the Source form of any Derivative Works\n          that You distribute, all copyright, patent, trademark, and\n          attribution notices from the Source form of the Work,\n          excluding those notices that do not pertain to any part of\n          the Derivative Works; and\n\n      (d) If the Work includes a \"NOTICE\" text file as part of its\n          distribution, then any Derivative Works that You distribute must\n          include a readable copy of the attribution notices contained\n          within such NOTICE file, excluding those notices that do not\n          pertain to any part of the Derivative Works, in at least one\n          of the following places: within a NOTICE text file distributed\n          as part of the Derivative Works; within the Source form or\n          documentation, if provided along with the Derivative Works; or,\n          within a display generated by the Derivative Works, if and\n          wherever such third-party notices normally appear. The contents\n          of the NOTICE file are for informational purposes only and\n          do not modify the License. You may add Your own attribution\n          notices within Derivative Works that You distribute, alongside\n          or as an addendum to the NOTICE text from the Work, provided\n          that such additional attribution notices cannot be construed\n          as modifying the License.\n\n      You may add Your own copyright statement to Your modifications and\n      may provide additional or different license terms and conditions\n      for use, reproduction, or distribution of Your modifications, or\n      for any such Derivative Works as a whole, provided Your use,\n      reproduction, and distribution of the Work otherwise complies with\n      the conditions stated in this License.\n\n   5. Submission of Contributions. Unless You explicitly state otherwise,\n      any Contribution intentionally submitted for inclusion in the Work\n      by You to the Licensor shall be under the terms and conditions of\n      this License, without any additional terms or conditions.\n      Notwithstanding the above, nothing herein shall supersede or modify\n      the terms of any separate license agreement you may have executed\n      with Licensor regarding such Contributions.\n\n   6. Trademarks. This License does not grant permission to use the trade\n      names, trademarks, service marks, or product names of the Licensor,\n      except as required for reasonable and customary use in describing the\n      origin of the Work and reproducing the content of the NOTICE file.\n\n   7. Disclaimer of Warranty. Unless required by applicable law or\n      agreed to in writing, Licensor provides the Work (and each\n      Contributor provides its Contributions) on an \"AS IS\" BASIS,\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n      implied, including, without limitation, any warranties or conditions\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n      PARTICULAR PURPOSE. You are solely responsible for determining the\n      appropriateness of using or redistributing the Work and assume any\n      risks associated with Your exercise of permissions under this License.\n\n   8. Limitation of Liability. In no event and under no legal theory,\n      whether in tort (including negligence), contract, or otherwise,\n      unless required by applicable law (such as deliberate and grossly\n      negligent acts) or agreed to in writing, shall any Contributor be\n      liable to You for damages, including any direct, indirect, special,\n      incidental, or consequential damages of any character arising as a\n      result of this License or out of the use or inability to use the\n      Work (including but not limited to damages for loss of goodwill,\n      work stoppage, computer failure or malfunction, or any and all\n      other commercial damages or losses), even if such Contributor\n      has been advised of the possibility of such damages.\n\n   9. Accepting Warranty or Additional Liability. While redistributing\n      the Work or Derivative Works thereof, You may choose to offer,\n      and charge a fee for, acceptance of support, warranty, indemnity,\n      or other liability obligations and/or rights consistent with this\n      License. However, in accepting such obligations, You may act only\n      on Your own behalf and on Your sole responsibility, not on behalf\n      of any other Contributor, and only if You agree to indemnify,\n      defend, and hold each Contributor harmless for any liability\n      incurred by, or claims asserted against, such Contributor by reason\n      of your accepting any such warranty or additional liability.\n\n   END OF TERMS AND CONDITIONS\n\n   APPENDIX: How to apply the Apache License to your work.\n\n      To apply the Apache License to your work, attach the following\n      boilerplate notice, with the fields enclosed by brackets \"[]\"\n      replaced with your own identifying information. (Don't include\n      the brackets!)  The text should be enclosed in the appropriate\n      comment syntax for the file format. We also recommend that a\n      file or class name and description of purpose be included on the\n      same \"printed page\" as the copyright notice for easier\n      identification within third-party archives.\n\n   Copyright 2023 Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li\n\n   Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 26.0703125,
          "content": "## <img src=\"assets/belle_logo.png\" style=\"vertical-align: middle; width: 35px;\"> BELLE: Be Everyone's Large Language model Engine\n\n*Read this in [English](README_en.md).*\n\n<div align=\"center\">\n\n<a href=\"https://github.com/LianjiaTech/BELLE/stargazers\">![GitHub Repo stars](https://img.shields.io/github/stars/LianjiaTech/BELLE?style=social)</a>\n[![Code License](https://img.shields.io/badge/Code%20License-Apache_2.0-green.svg)](https://github.com/LianjiaTech/BELLE/blob/main/LICENSE)\n[![Generic badge](https://img.shields.io/badge/discord-BELLE%20Group-green.svg?logo=discord)](https://discord.gg/pMPY53UUGq)\n[![Generic badge](https://img.shields.io/badge/wechat-BELLE-green.svg?logo=wechat)](https://github.com/LianjiaTech/BELLE/blob/main/assets/belle_wechat.jpg)\n[![Generic badge](https://img.shields.io/badge/ğŸ¤—-Huggingface%20Repo-green.svg)](https://huggingface.co/BelleGroup)\n[![Generic badge](https://img.shields.io/badge/ğŸ¤—-Huggingface%20Repo2-green.svg)](https://huggingface.co/BELLE-2)\n</div>\n\næœ¬é¡¹ç›®çš„ç›®æ ‡æ˜¯ä¿ƒè¿›ä¸­æ–‡å¯¹è¯å¤§æ¨¡å‹å¼€æºç¤¾åŒºçš„å‘å±•ï¼Œæ„¿æ™¯æ˜¯æˆä¸ºèƒ½å¤Ÿå¸®åˆ°æ¯ä¸€ä¸ªäººçš„LLM Engineã€‚\n\nç›¸æ¯”å¦‚ä½•åšå¥½å¤§è¯­è¨€æ¨¡å‹çš„é¢„è®­ç»ƒï¼ŒBELLEæ›´å…³æ³¨å¦‚ä½•åœ¨å¼€æºé¢„è®­ç»ƒå¤§è¯­è¨€æ¨¡å‹çš„åŸºç¡€ä¸Šï¼Œå¸®åŠ©æ¯ä¸€ä¸ªäººéƒ½èƒ½å¤Ÿå¾—åˆ°ä¸€ä¸ªå±äºè‡ªå·±çš„ã€æ•ˆæœå°½å¯èƒ½å¥½çš„å…·æœ‰æŒ‡ä»¤è¡¨ç°èƒ½åŠ›çš„è¯­è¨€æ¨¡å‹ï¼Œé™ä½å¤§è¯­è¨€æ¨¡å‹ã€ç‰¹åˆ«æ˜¯ä¸­æ–‡å¤§è¯­è¨€æ¨¡å‹çš„ç ”ç©¶å’Œåº”ç”¨é—¨æ§›ã€‚ä¸ºæ­¤ï¼ŒBELLEé¡¹ç›®ä¼šæŒç»­å¼€æ”¾æŒ‡ä»¤è®­ç»ƒæ•°æ®ã€ç›¸å…³æ¨¡å‹ã€è®­ç»ƒä»£ç ã€åº”ç”¨åœºæ™¯ç­‰ï¼Œä¹Ÿä¼šæŒç»­è¯„ä¼°ä¸åŒè®­ç»ƒæ•°æ®ã€è®­ç»ƒç®—æ³•ç­‰å¯¹æ¨¡å‹è¡¨ç°çš„å½±å“ã€‚BELLEé’ˆå¯¹ä¸­æ–‡åšäº†ä¼˜åŒ–ï¼Œæ¨¡å‹è°ƒä¼˜ä»…ä½¿ç”¨ç”±ChatGPTç”Ÿäº§çš„æ•°æ®ï¼ˆä¸åŒ…å«ä»»ä½•å…¶ä»–æ•°æ®ï¼‰ã€‚\n\n</br>\n\n## ğŸ”„ æœ€è¿‘æ›´æ–°\n* [2024/10/16] å¼€æº[Belle-whisper-larger-v3-turbo-zh](https://huggingface.co/BELLE-2/Belle-whisper-large-v3-turbo-zh)  ä¸­æ–‡èƒ½åŠ›å¼ºåŒ–åçš„è¯­éŸ³è¯†åˆ«æ¨¡å‹ï¼Œè¯†åˆ«ç²¾åº¦ç›¸æ¯”whisper-large-v3-turboç›¸å¯¹æå‡24~64%ï¼Œè¯†åˆ«é€Ÿåº¦ç›¸æ¯”whisper-large-v3æœ‰7-8å€æå‡ã€‚\n* [2024/03/15] æ›´æ–°äº†ä¸€ç¯‡æŠ€æœ¯æŠ¥å‘Š[Dial-insight](https://arxiv.org/pdf/2403.09167.pdf)  åœ¨å‚ç›´é¢†åŸŸåœºæ™¯å¾®è°ƒå¤§æ¨¡å‹æ—¶ï¼Œä½¿ç”¨é«˜è´¨é‡çš„å‚ç›´é¢†åŸŸæ•°æ®å¯ä»¥åœ¨ä½¿æ¨¡å‹çš„å‚ç›´é¢†åŸŸèƒ½åŠ›å¢å¼ºçš„åŒæ—¶ï¼Œæœ‰æ•ˆçš„æŠµæŠ—æ¨¡å‹é€šç”¨èƒ½åŠ›çš„åç¼©ã€‚\n* [2024/03/11] å¼€æº[Belle-whisper-larger-v3-zh](https://huggingface.co/BELLE-2/Belle-whisper-large-v3-zh)  ä¸­æ–‡èƒ½åŠ›å¼ºåŒ–åçš„è¯­éŸ³è¯†åˆ«æ¨¡å‹ï¼Œç›¸æ¯”whisper-large-v3ç›¸å¯¹æå‡24~65%ï¼Œç‰¹åˆ«æ˜¯åœ¨é«˜å™ªã€æ··å“ç­‰å¤æ‚åœºæ™¯ä¸‹æœ‰çªå‡ºè¡¨ç°ã€‚\n* [2024/01/16] æ›´æ–°äº†ä¸€ç¯‡æŠ€æœ¯æŠ¥å‘Š[RAISE](https://arxiv.org/pdf/2401.02777.pdf). RAISEé€šè¿‡å®éªŒå‘ç°æ„é€ å°‘é‡çš„æ ·ä¾‹æ•°æ®ï¼Œå°±èƒ½æœ‰æ•ˆçš„æ¿€å‘å¤§æ¨¡å‹ï¼Œç”Ÿæˆå¯¹è¯ä¹Ÿæ›´å¯æ§\n* [2023/12/29] å¼€æº[Belle-whisper-larger-v2-zh](https://huggingface.co/BELLE-2/Belle-whisper-large-v2-zh)å’Œ[Belle-distilwhisper-large-v2-zh](https://huggingface.co/BELLE-2/Belle-distilwhisper-large-v2-zh)ä¸¤ä¸ªé’ˆå¯¹ä¸­æ–‡èƒ½åŠ›å¼ºåŒ–åçš„è¯­éŸ³è¯†åˆ«æ¨¡å‹ï¼Œæ–¹ä¾¿å¤§å®¶åœ¨è¯­éŸ³åœºæ™¯ä¸‹ä½¿ç”¨å¤§è¯­è¨€æ¨¡å‹\n* [2023/11/24] å¼€æº[BELLE-VL](https://huggingface.co/BELLE-2/BELLE-VL)å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ŒåŸºäºä¸­æ–‡èƒ½åŠ›æ›´å¼ºçš„è¯­è¨€æ¨¡å‹åŸºåº§æ¥æ‰©å±•æ¨¡å‹çš„è§†è§‰èƒ½åŠ›ï¼Œä¸ºç¤¾åŒºæä¾›æ›´åŠ çµæ´»çš„é€‰æ‹©ï¼ˆç›®å‰BELLE-VLæœ€æ–°çš„æ¨¡å‹åœ¨[MME](https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models/tree/Evaluation)æ„ŸçŸ¥è¯„æµ‹ç»´åº¦å…±è·å¾—**1620.10**åˆ†,è¶…è¿‡Qwen-VLã€Llavaã€mplug-owlï¼‰\n* [2023/10/27] æ›´æ–°äº†ä¸€ç¯‡æŠ€æœ¯æŠ¥å‘Š[DUMA](https://arxiv.org/abs/2310.18075#)ï¼Œæ¢ç´¢äº†å¯¹è¯åœºæ™¯ä¸‹åŸºäºå¿«æ…¢è„‘æ¶æ„çš„Agentå®ç°æ–¹æ³•\n* [2023/09/26] æ›´æ–°äº†RLHFçš„è®­ç»ƒä»£ç ï¼Œæ”¯æŒPPOå’Œ[DPO](https://arxiv.org/abs/2305.18290)è®­ç»ƒï¼Œå…·ä½“ç»†èŠ‚è§ï¼š[README_RLHF.md](train/README_RLHF.md)\n* [2023/08/16] åŸºäºåŸæœ‰çš„[train_3.5M_CN](https://huggingface.co/datasets/BelleGroup/train_3.5M_CN)æ•°æ®æ–°å¢äº†æŒ‡ä»¤ç±»åˆ«å­—æ®µï¼Œå…±åŒ…æ‹¬13ä¸ªç±»åˆ«ï¼Œå…·ä½“ç»†èŠ‚è§ï¼š[train_3.5M_CN_With_Category](https://huggingface.co/datasets/BELLE-2/train_3.5M_CN_With_Category)\n* [2023/08/10] æ›´æ–°äº†åŸºäºZeRO Inferenceçš„æ¨ç†ä»£ç ï¼Œè¯¦è§[train/README_ZERO_INFERENCE.md](train/README_ZERO_INFERENCE.md)\n* [2023/08/07] æ›´æ–°äº†ç»§ç»­é¢„è®­ç»ƒä»£ç å’ŒæŒ‡ä»¤å¾®è°ƒä»£ç ï¼Œæ·»åŠ äº†flash attention 2ï¼Œè¯¦è§[train/README.md](train/README.md)ã€‚åŒæ—¶æ‰“åŒ…äº†è¿è¡Œç¯å¢ƒï¼Œè¯¦è§[train/docker/README.md](train/docker/README.md)\n* [2023/07/31] æ›´æ–°äº†ä¸€ç¯‡æŠ€æœ¯æŠ¥å‘Š[ChatHome](https://arxiv.org/abs/2307.15290)ï¼Œæ¢ç´¢äº†é’ˆå¯¹å‚ç›´é¢†åŸŸæ—¶çš„å¢é‡é¢„è®­ç»ƒ+æŒ‡ä»¤å¾®è°ƒçš„çš„ç­–ç•¥æ–¹æ³•\n* [2023/07/27] å¼€æ”¾[BELLE-Llama2-13B-chat-0.4M](https://huggingface.co/BELLE-2/BELLE-Llama2-13B-chat-0.4M)ï¼Œåœ¨Llama-2-13Bçš„åŸºç¡€ä¸Šé‡‡ç”¨40ä¸‡é«˜è´¨é‡çš„å¯¹è¯æ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒã€‚åœ¨[è¯„æµ‹é›†](https://github.com/LianjiaTech/BELLE/blob/main/eval/eval_set.json)ä¸Šçš„æ•ˆæœç›¸æ¯”BELLE-LLaMA-EXT-13Bæ¨¡å‹æœ‰æ˜¾è‘—æå‡ã€‚\n* [2023/05/14] å¼€æ”¾[BELLE-LLaMA-EXT-13B](https://huggingface.co/BelleGroup/BELLE-LLaMA-EXT-13B)ï¼Œåœ¨LLaMA-13Bçš„åŸºç¡€ä¸Šæ‰©å±•ä¸­æ–‡è¯è¡¨ï¼Œå¹¶åœ¨400ä¸‡é«˜è´¨é‡çš„å¯¹è¯æ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒã€‚\n* [2023/05/11] [BELLE/data/10M](data/10M)ä¸­ï¼Œæ–°åŠ 350ä¸‡æ¡ç”Ÿæˆå¤šæ ·åŒ–æŒ‡ä»¤ä»»åŠ¡æ•°æ®ï¼ŒåŒ…æ‹¬å•è½®å’Œå¤šè½®å¯¹è¯[train_3.5M_CN](https://huggingface.co/datasets/BelleGroup/train_3.5M_CN)ã€‚\n* [2023/04/19] å¼€æ”¾äº†å…¶ä¸­ä¸€ç¯‡è®ºæ–‡ä¸­çš„çš„ç›¸å…³æ¨¡å‹ï¼šåŒ…æ‹¬åœ¨LLaMA7BåŸºç¡€ä¸Šå¢é‡é¢„è®­ç»ƒæ‰©å±•ä¸­æ–‡è¯è¡¨çš„æ¨¡ï¼ˆè¯¦è§[BelleGroup/BELLE-LLaMA-EXT-7B](https://huggingface.co/BelleGroup/BELLE-LLaMA-EXT-7B)ï¼‰ï¼Œä»¥åŠåŸºäºå¤šæ ·åŒ–å¼€æºæ•°æ®è®­ç»ƒåçš„LLaMA-7Bæ¨¡å‹ï¼ˆè¯¦è§[BelleGroup/BELLE-on-Open-Datasets](https://huggingface.co/BelleGroup/BELLE-on-Open-Datasets)ï¼‰ã€‚\n* [2023/04/18] æ›´æ–°äº†trainä»£ç ï¼Œè¯¦è§[BELLE/train](https://github.com/LianjiaTech/BELLE/tree/main/train)ï¼Œé›†æˆäº†Deepspeed-Chatï¼Œæä¾›äº†ç›¸å…³çš„docker\n* [2023/04/18] æ›´æ–°äº†[ä¸¤ç¯‡æœ€æ–°è®ºæ–‡å·¥ä½œ](#ğŸ“‘-ç ”ç©¶æŠ¥å‘Š)ï¼Œå¯¹æ¯”äº†ä¸åŒæ–¹å¼äº§ç”Ÿçš„è®­ç»ƒæ•°æ®ã€ä¸åŒè®­ç»ƒæ–¹æ³•ï¼ˆLoRA, finetune)å¯¹æ•ˆæœçš„å½±å“\n* [2023/04/12] å‘å¸ƒäº†[ChatBELLE App](chat/README.md)ï¼ŒåŸºäº[llama.cpp](https://github.com/ggerganov/llama.cpp)å’Œ[Flutter](https://flutter.dev/)ï¼Œå®ç°è·¨å¹³å°çš„BELLE-7Bç¦»çº¿æ¨¡å‹å®æ—¶äº¤äº’ã€‚\n* [2023/04/11] æ›´æ–°äº†ä¸€ä¸ªäººå·¥ç²¾æ ¡çš„evalé›†åˆï¼Œå¤§çº¦ä¸€åƒå¤šæ¡\n* [2023/04/08] [BELLE/data/10M](data/10M)ä¸­ï¼Œæ–°åŠ 40ä¸‡æ¡ç”Ÿæˆçš„ç»™å®šè§’è‰²çš„å¤šè½®å¯¹è¯[Generated Chat](https://huggingface.co/datasets/BelleGroup/generated_chat_0.4M)ï¼Œæ–°åŠ 200ä¸‡æ¡ç”Ÿæˆå¤šæ ·åŒ–æŒ‡ä»¤ä»»åŠ¡æ•°æ®[train_2M_CN](https://huggingface.co/datasets/BelleGroup/train_2M_CN)ã€‚\n\n</br>\n  \n\nä¸‹å›¾æ˜¯ä¸€ä¸ªå¯ä»¥ä½¿ç”¨Appåœ¨è®¾å¤‡ç«¯æœ¬åœ°è¿è¡Œ4bité‡åŒ–çš„BELLE-7Bæ¨¡å‹ï¼Œåœ¨M1 Max CPUä¸Šå®æ—¶è¿è¡Œçš„æ•ˆæœï¼ˆæœªåŠ é€Ÿï¼‰ã€‚Appä¸‹è½½è¯¦è§[Appé…å¥—æ¨¡å‹ä¸‹è½½åŠä½¿ç”¨è¯´æ˜](chat/README.md)ï¼ŒApp[ä¸‹è½½é“¾æ¥](https://github.com/LianjiaTech/BELLE/releases/download/v0.95/chatbelle.dmg)ï¼Œç›®å‰ä»…æä¾›äº†mac osç‰ˆæœ¬ã€‚æ¨¡å‹éœ€è¦å•ç‹¬ä¸‹è½½ã€‚**æ¨¡å‹ç»è¿‡é‡åŒ–åï¼Œæ•ˆæœæŸå¤±æ˜æ˜¾ï¼Œæˆ‘ä»¬å°†æŒç»­ç ”ç©¶å¦‚ä½•æå‡ã€‚**\n\n<img src=\"./chat/chatbelle-demo.gif\"></img>\n\n\n\n</br>\n\n## ğŸ“ é¡¹ç›®ä¸»è¦å†…å®¹\n\n### ğŸš€ è®­ç»ƒä»£ç \n\nè¯¦è§[BELLE/train](train)ï¼Œå°½å¯èƒ½ç®€åŒ–çš„ä¸€ä¸ªè®­ç»ƒä»£ç å®ç°ï¼Œé›†æˆäº†Deepspeed-Chatï¼Œæ”¯æŒfinetuneï¼Œloraï¼Œå¹¶æä¾›äº†ç›¸å…³çš„docker\n\n### ğŸ“Š æ•°æ®å¼€æ”¾\n  \n* è¯¦è§[BELLE/data/1.5M](data/1.5M)ï¼Œå‚è€ƒ[Stanford Alpaca](https://github.com/tatsu-lab/stanford_alpaca) ç”Ÿæˆçš„ä¸­æ–‡æ•°æ®é›†[1M](https://huggingface.co/datasets/BelleGroup/train_1M_CN) + [0.5M](https://huggingface.co/datasets/BelleGroup/train_0.5M_CN)ï¼›\n  \n* æŒç»­å¼€æ”¾çš„æ•°æ®é›†ï¼Œè¯¦è§[BELLE/data/10M](data/10M)\n\n### ğŸ§ éªŒè¯é›†åˆ&éªŒè¯æ–¹æ³•\n\nè¯¦è§[BELLE/eval](https://github.com/LianjiaTech/BELLE/tree/main/eval)ï¼Œä¸€ä¸ª1k+çš„æµ‹è¯•é›†åˆï¼Œå’Œå¯¹åº”æ‰“åˆ†promptã€‚åŒ…å«å¤šä¸ªç±»åˆ«ï¼Œé‡‡ç”¨GPT-4æˆ–è€…ChatGPTæ‰“åˆ†ã€‚åŒæ—¶æä¾›äº†ä¸€ä¸ªæ‰“åˆ†çš„ç½‘é¡µï¼Œæ–¹ä¾¿é’ˆå¯¹å•ä¸ªcaseä½¿ç”¨ã€‚æ¬¢è¿å¤§å®¶é€šè¿‡PRæä¾›æ›´å¤šçš„æµ‹è¯•ç”¨ä¾‹ã€‚\n\n### ğŸ¤– æ¨¡å‹\n\nè¯¦è§[BELLE/models](models/)\n* åŸºäº[Meta LLaMA2](https://github.com/facebookresearch/llama)å®ç°è°ƒä¼˜çš„æ¨¡å‹ï¼š[BELLE-Llama2-13B-chat-0.4M](https://huggingface.co/BELLE-2/BELLE-Llama2-13B-chat-0.4M)\n* åŸºäº[Meta LLaMA](https://github.com/facebookresearch/llama)å®ç°è°ƒä¼˜çš„æ¨¡å‹ï¼š[BELLE-LLaMA-7B-0.6M-enc](https://huggingface.co/BelleGroup/BELLE-LLaMA-7B-0.6M-enc)\n, [BELLE-LLaMA-7B-2M-enc](https://huggingface.co/BelleGroup/BELLE-LLaMA-7B-2M-enc)\n, [BELLE-LLaMA-7B-2M-gptq-enc](https://huggingface.co/BelleGroup/BELLE-LLaMA-7B-2M-gptq-enc)\n, [BELLE-LLaMA-13B-2M-enc](https://huggingface.co/BelleGroup/BELLE-LLaMA-13B-2M-enc)\n, [BELLE-on-Open-Datasets](https://huggingface.co/BelleGroup/BELLE-on-Open-Datasets) ä»¥åŠåŸºäºLLaMAåšäº†ä¸­æ–‡è¯è¡¨æ‰©å……çš„é¢„è®­ç»ƒæ¨¡å‹[BELLE-LLaMA-EXT-7B](https://huggingface.co/BelleGroup/BELLE-LLaMA-EXT-7B)ã€‚\n\n  * è¯·å‚è€ƒ[Meta LLaMAçš„License](https://github.com/facebookresearch/llama/blob/main/LICENSE)ï¼Œç›®å‰ä»…ä¾›å­¦ä¹ äº¤æµã€‚è¯·ä¸¥éµå®ˆLLaMAçš„ä½¿ç”¨é™åˆ¶ã€‚LLaMAæ¨¡å‹ä¸å…è®¸å‘å¸ƒè°ƒä¼˜åçš„å®Œæ•´æ¨¡å‹æƒé‡ï¼Œä½†æ˜¯å¯ä»¥å‘å¸ƒåŸå§‹çš„æ¨¡å‹çš„diffã€‚å› æ­¤ï¼Œæˆ‘ä»¬ä½¿ç”¨æ–‡ä»¶é—´çš„XORï¼Œä¿è¯æ‹¥æœ‰LLaMAåŸå§‹æ¨¡å‹æˆæƒçš„äººæ‰å¯ä»¥å°†æœ¬é¡¹ç›®å‘å¸ƒçš„æ¨¡å‹è½¬åŒ–æˆå¯ä»¥ä½¿ç”¨çš„æ ¼å¼ã€‚æ ¼å¼è½¬åŒ–ä»£ç å‚è€ƒ[BELLE/models](https://github.com/LianjiaTech/BELLE/tree/main/models)\n    \n* åŸºäºBLOOMZ-7B1-mtä¼˜åŒ–åçš„æ¨¡å‹ï¼š[BELLE-7B-0.2M](https://huggingface.co/BelleGroup/BELLE-7B-0.2M)ï¼Œ[BELLE-7B-0.6M](https://huggingface.co/BelleGroup/BELLE-7B-0.6M)ï¼Œ[BELLE-7B-1M](https://huggingface.co/BelleGroup/BELLE-7B-1M)ï¼Œ[BELLE-7B-2M](https://huggingface.co/BelleGroup/BELLE-7B-2M)\n### âš–ï¸ æ¨¡å‹é‡åŒ–gptq\n\nè¯¦è§[BELLE/gptq](https://github.com/LianjiaTech/BELLE/tree/main/models/gptq)ï¼Œå‚è€ƒgptqçš„å®ç°ï¼Œå¯¹æœ¬é¡¹ç›®ä¸­ç›¸å…³æ¨¡å‹è¿›è¡Œäº†é‡åŒ–\n\n### ğŸŒ Colab\n\n[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/LianjiaTech/BELLE/blob/main/models/notebook/BELLE_INFER_COLAB.ipynb) æä¾›äº†colabä¸Šé¢å¯è¿è¡Œçš„æ¨ç†ä»£ç [Colab](https://colab.research.google.com/github/LianjiaTech/BELLE/blob/main/models/notebook/BELLE_INFER_COLAB.ipynb)\n\n### ğŸ’¬ ChatBELLE App\n\nè¯¦è§[BELLE/chat](chat/README.md)ï¼ŒåŸºäº[BELLE](https://github.com/LianjiaTech/BELLE)æ¨¡å‹çš„è·¨å¹³å°ç¦»çº¿å¤§è¯­è¨€æ¨¡å‹äº¤è°ˆAppã€‚ä½¿ç”¨é‡åŒ–åçš„ç¦»çº¿ç«¯ä¸Šæ¨¡å‹é…åˆFlutterï¼Œå¯åœ¨macOSï¼ˆå·²æ”¯æŒï¼‰ã€Windowsã€Androidã€iOSç­‰è®¾å¤‡ä¸Šè¿è¡Œã€‚\n\n### ğŸ“‘ ç ”ç©¶æŠ¥å‘Š\n\nè¯¦è§[BELLE/docs](docs/)ï¼Œå…¶ä¸­ä¼šå®šæœŸæ›´æ–°æœ¬é¡¹ç›®ç›¸å…³çš„ç ”ç©¶æŠ¥å‘Šå·¥ä½œ\n\n**æ¬¢è¿å¤§å®¶é€šè¿‡issueè´¡çŒ®æ›´å¤šçš„promptsï¼**\n\n<br/>\n\n## ğŸ“‘ ç ”ç©¶æŠ¥å‘Š\n\n### [Towards Better Instruction Following Language Models for Chinese: Investigating the Impact of Training Data and Evaluation](https://github.com/LianjiaTech/BELLE/blob/main/docs/Towards%20Better%20Instruction%20Following%20Language%20Models%20for%20Chinese.pdf)\n\nä¸ºäº†æ¨åŠ¨å¼€æºå¤§è¯­è¨€æ¨¡å‹çš„å‘å±•ï¼Œå¤§å®¶æŠ•å…¥äº†å¤§é‡ç²¾åŠ›å¼€å‘èƒ½å¤Ÿç±»ä¼¼äºChatGPTçš„ä½æˆæœ¬æ¨¡å‹ã€‚\né¦–å…ˆï¼Œä¸ºäº†æé«˜æ¨¡å‹åœ¨ä¸­æ–‡é¢†åŸŸçš„æ€§èƒ½å’Œè®­ç»ƒ/æ¨ç†æ•ˆç‡ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥æ‰©å±•äº†LLaMAçš„è¯æ±‡è¡¨ï¼Œå¹¶åœ¨34äº¿ä¸ªä¸­æ–‡è¯æ±‡ä¸Šè¿›è¡Œäº†äºŒæ¬¡é¢„è®­ç»ƒã€‚\n\næ­¤å¤–ï¼Œç›®å‰å¯ä»¥çœ‹åˆ°åŸºäºChatGPTäº§ç”Ÿçš„æŒ‡ä»¤è®­ç»ƒæ•°æ®æ–¹å¼æœ‰ï¼š1ï¼‰å‚è€ƒAlpacaåŸºäºGPT3.5å¾—åˆ°çš„self-instructæ•°æ®ï¼›\n2ï¼‰å‚è€ƒAlpacaåŸºäºGPT4å¾—åˆ°çš„self-instructæ•°æ®ï¼›3ï¼‰ç”¨æˆ·ä½¿ç”¨ChatGPTåˆ†äº«çš„æ•°æ®ShareGPTã€‚\nåœ¨è¿™é‡Œï¼Œæˆ‘ä»¬ç€çœ¼äºæ¢ç©¶è®­ç»ƒæ•°æ®ç±»åˆ«å¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“ã€‚å…·ä½“è€Œè¨€ï¼Œæˆ‘ä»¬è€ƒå¯Ÿäº†è®­ç»ƒæ•°æ®çš„æ•°é‡ã€è´¨é‡å’Œè¯­è¨€åˆ†å¸ƒç­‰å› ç´ ï¼Œä»¥åŠæˆ‘ä»¬è‡ªå·±é‡‡é›†çš„ä¸­æ–‡å¤šè½®å¯¹è¯æ•°æ®ï¼Œä»¥åŠä¸€äº›å…¬å¼€å¯è®¿é—®çš„é«˜è´¨é‡æŒ‡å¯¼æ•°æ®é›†ã€‚\n\nä¸ºäº†æ›´å¥½çš„è¯„ä¼°æ•ˆæœï¼Œæˆ‘ä»¬ä½¿ç”¨äº†ä¸€ä¸ªåŒ…å«ä¸€åƒä¸ªæ ·æœ¬å’Œä¹ä¸ªçœŸå®åœºæ™¯çš„è¯„ä¼°é›†æ¥æµ‹è¯•å„ç§æ¨¡å‹ï¼ŒåŒæ—¶é€šè¿‡é‡åŒ–åˆ†ææ¥æä¾›æœ‰ä»·å€¼çš„è§è§£ï¼Œä»¥ä¾¿æ›´å¥½åœ°ä¿ƒè¿›å¼€æºèŠå¤©æ¨¡å‹çš„å‘å±•ã€‚\n\nè¿™é¡¹ç ”ç©¶çš„ç›®æ ‡æ˜¯å¡«è¡¥å¼€æºèŠå¤©æ¨¡å‹ç»¼åˆè¯„ä¼°çš„ç©ºç™½ï¼Œä»¥ä¾¿ä¸ºè¿™ä¸€é¢†åŸŸçš„æŒç»­è¿›æ­¥æä¾›æœ‰åŠ›æ”¯æŒã€‚\n\nå®éªŒç»“æœå¦‚ä¸‹ï¼š\n\n<table>\n  <tr>\n    <td> Factor </td>\n    <td> Base model </td>\n    <td> Training data </td>\n    <td> Score_w/o_others </td>\n  <tr>\n    <td rowspan=\"2\">è¯è¡¨æ‰©å……</td>\n    <td> LLaMA-7B-EXT </td>\n    <td> zh(alpaca-3.5&4) + sharegpt </td>\n    <td> 0.670 </td>\n  </tr>\n  <tr>\n    <td> LLaMA-7B </td>\n    <td> zh(alpaca-3.5&4) + sharegpt </td>\n    <td> 0.652</td>\n  </tr>\n  <tr>\n    <td rowspan=\"2\">æ•°æ®è´¨é‡</td>\n    <td> LLaMA-7B-EXT </td>\n    <td> zh(alpaca-3.5) </td>\n    <td> 0.642 </td>\n  </tr>\n  <tr>\n    <td> LLaMA-7B-EXT </td>\n    <td> zh(alpaca-4) </td>\n    <td> 0.693 </td>\n  </tr>\n  <tr>\n    <td rowspan=\"4\">æ•°æ®è¯­è¨€åˆ†å¸ƒ</td>\n    <td> LLaMA-7B-EXT </td>\n    <td> zh(alpaca-3.5&4) </td>\n    <td> 0.679 </td>\n  </tr>\n  <tr>\n    <td> LLaMA-7B-EXT </td>\n    <td> en(alpaca-3.5&4) </td>\n    <td> 0.659 </td>\n  </tr>\n  <tr>\n    <td> LLaMA-7B-EXT </td>\n    <td> zh(alpaca-3.5&4) + sharegpt </td>\n    <td> 0.670 </td>\n  </tr>\n  <tr>\n    <td> LLaMA-7B-EXT </td>\n    <td> en(alpaca-3.5&4) + sharegpt </td>\n    <td> 0.668 </td>\n  </tr>\n  <tr>\n    <td rowspan=\"2\">æ•°æ®è§„æ¨¡</td>\n    <td> LLaMA-7B-EXT </td>\n    <td> zh(alpaca-3.5&4) + sharegpt </td>\n    <td> 0.670 </td>\n  </tr>\n  <tr>\n    <td> LLaMA-7B-EXT </td>\n    <td> zh(alpaca-3.5&4) + sharegpt <br>+ BELLE-0.5M-CLEAN</td>\n    <td> 0.762</td>\n  </tr>\n  <tr>\n    <td>-</td>\n    <td>ChatGPT</td>\n    <td>-</td>\n    <td>0.824</td>\n</table>\n\nå…¶ä¸­**BELLE-0.5M-CLEAN**æ˜¯ä»230ä¸‡æŒ‡ä»¤æ•°æ®ä¸­æ¸…æ´—å¾—åˆ°0.5Mæ•°æ®ï¼Œå…¶ä¸­åŒ…å«å•è½®å’Œå¤šè½®å¯¹è¯æ•°æ®ï¼Œå’Œä¹‹å‰å¼€æ”¾çš„0.5Mæ•°æ®ä¸æ˜¯åŒä¸€æ‰¹æ•°æ®ã€‚\n\n**éœ€è¦å¼ºè°ƒæŒ‡å‡ºçš„æ˜¯**ï¼šé€šè¿‡æ¡ˆä¾‹åˆ†æï¼Œæˆ‘ä»¬å‘ç°æˆ‘ä»¬çš„è¯„ä¼°é›†åœ¨å…¨é¢æ€§æ–¹é¢å­˜åœ¨å±€é™æ€§ï¼Œè¿™å¯¼è‡´äº†æ¨¡å‹åˆ†æ•°çš„æ”¹å–„ä¸å®é™…ç”¨æˆ·ä½“éªŒä¹‹é—´çš„ä¸ä¸€è‡´ã€‚æ„å»ºä¸€ä¸ªé«˜è´¨é‡çš„è¯„ä¼°é›†æ˜¯ä¸€ä¸ªå·¨å¤§çš„æŒ‘æˆ˜ï¼Œå› ä¸ºå®ƒéœ€è¦åœ¨ä¿æŒå¹³è¡¡éš¾æ˜“ç¨‹åº¦çš„åŒæ—¶ï¼ŒåŒ…å«å°½å¯èƒ½å¤šæ ·çš„ä½¿ç”¨åœºæ™¯ã€‚å¦‚æœè¯„ä¼°æ ·æœ¬ä¸»è¦éƒ½è¿‡äºå›°éš¾ï¼Œé‚£ä¹ˆæ‰€æœ‰æ¨¡å‹çš„è¡¨ç°å°†ä¼šå¾ˆå·®ï¼Œä½¿å¾—è¾¨åˆ«å„ç§è®­ç»ƒç­–ç•¥çš„æ•ˆæœå˜å¾—å…·æœ‰æŒ‘æˆ˜æ€§ã€‚ç›¸åï¼Œå¦‚æœè¯„ä¼°æ ·æœ¬éƒ½ç›¸å¯¹å®¹æ˜“ï¼Œè¯„ä¼°å°†å¤±å»å…¶æ¯”è¾ƒä»·å€¼ã€‚æ­¤å¤–ï¼Œå¿…é¡»ç¡®ä¿è¯„ä¼°æ•°æ®ä¸è®­ç»ƒæ•°æ®ä¿æŒç‹¬ç«‹ã€‚\n\n<p align=\"center\">\n  <img src=\"./assets/eval-set-distribution.jpg\" alt=\"LLM eval\" width=\"400\">\n</p>\n\nåŸºäºè¿™äº›è§‚å¯Ÿï¼Œæˆ‘ä»¬è°¨æ…åœ°æé†’ä¸è¦å‡è®¾æ¨¡å‹ä»…é€šè¿‡åœ¨æœ‰é™æ•°é‡çš„æµ‹è¯•æ ·æœ¬ä¸Šè·å¾—è‰¯å¥½ç»“æœå°±å·²ç»è¾¾åˆ°äº†ä¸ChatGPTç›¸å½“çš„æ€§èƒ½æ°´å¹³ã€‚æˆ‘ä»¬è®¤ä¸ºï¼Œä¼˜å…ˆå‘å±•å…¨é¢è¯„ä¼°é›†å…·æœ‰é‡è¦æ„ä¹‰ã€‚\n\n### [A Comparative Study between Full-Parameter and LoRA-based Fine-Tuning on Chinese Instruction Data for Instruction Following Large Language Model](https://github.com/LianjiaTech/BELLE/blob/main/docs/A%20Comparative%20Study%20between%20Full-Parameter%20and%20LoRA-based.pdf)\n\nä¸ºäº†å®ç°å¯¹å¤§è¯­è¨€æ¨¡å‹çš„æŒ‡ä»¤è°ƒä¼˜ï¼Œå—é™äºèµ„æºå’Œæˆæœ¬ï¼Œè®¸å¤šç ”ç©¶è€…å¼€å§‹ä½¿ç”¨å‚æ•°é«˜æ•ˆçš„è°ƒä¼˜æŠ€æœ¯ï¼Œä¾‹å¦‚LoRAï¼Œæ¥è¿›è¡ŒæŒ‡ä»¤è°ƒä¼˜ï¼Œè¿™ä¹Ÿå–å¾—äº†ä¸€äº›ä»¤äººé¼“èˆçš„æˆæœã€‚\nç›¸è¾ƒäºå…¨å‚æ•°å¾®è°ƒï¼ŒåŸºäºLoRAçš„è°ƒä¼˜åœ¨è®­ç»ƒæˆæœ¬æ–¹é¢å±•ç°å‡ºæ˜æ˜¾çš„ä¼˜åŠ¿ã€‚\nåœ¨è¿™ä¸ªç ”ç©¶æŠ¥å‘Šä¸­ï¼Œæˆ‘ä»¬é€‰ç”¨LLaMAä½œä¸ºåŸºç¡€æ¨¡å‹ï¼Œå¯¹å…¨å‚æ•°å¾®è°ƒå’ŒåŸºäºLoRAçš„è°ƒä¼˜æ–¹æ³•è¿›è¡Œäº†å®éªŒæ€§çš„æ¯”è¾ƒã€‚\n\nå®éªŒç»“æœæ­ç¤ºï¼Œé€‰æ‹©åˆé€‚çš„åŸºç¡€æ¨¡å‹ã€è®­ç»ƒæ•°æ®é›†çš„è§„æ¨¡ã€å¯å­¦ä¹ å‚æ•°çš„æ•°é‡ä»¥åŠæ¨¡å‹è®­ç»ƒæˆæœ¬å‡ä¸ºé‡è¦å› ç´ ã€‚\n\næˆ‘ä»¬å¸Œæœ›æœ¬æ–‡çš„å®éªŒç»“è®ºèƒ½å¯¹å¤§å‹è¯­è¨€æ¨¡å‹çš„è®­ç»ƒæä¾›æœ‰ç›Šçš„å¯ç¤ºï¼Œç‰¹åˆ«æ˜¯åœ¨ä¸­æ–‡é¢†åŸŸï¼ŒååŠ©ç ”ç©¶è€…åœ¨è®­ç»ƒæˆæœ¬ä¸æ¨¡å‹æ€§èƒ½ä¹‹é—´æ‰¾åˆ°æ›´ä½³çš„æƒè¡¡ç­–ç•¥ã€‚\nå®éªŒç»“æœå¦‚ä¸‹ï¼š\n\n| Model | Average Score | Additional Param. | Training Time (Hour/epoch) |\n| ----- | ------ | ----- | ------ |\n| LLaMA-13B + LoRA(2M) | 0.648 | 28M | 8 |\n| LLaMA-7B + LoRA(4M) | 0.624 | 17.9M | 11 |\n| LLaMA-7B + LoRA(2M) | 0.609 | 17.9M | 7 |\n| LLaMA-7B + LoRA(0.6M) | 0.589 | 17.9M | 5 |\n| LLaMA-7B + FT(2M) | 0.710 | - | 31 |\n| LLaMA-7B + LoRA(4M) | 0.686 | - | 17 |\n| LLaMA-7B + FT(2M) <br>+ LoRA(math_0.25M) | 0.729 | 17.9M | 3 |\n| LLaMA-7B + FT(2M) <br>+ FT(math_0.25M) | 0.738 | - | 6 |\n\nå…¶ä¸­çš„scoreæ˜¯åŸºäºæœ¬é¡¹ç›®é›†ç›®å‰å¼€æ”¾çš„1000æ¡è¯„ä¼°é›†åˆå¾—åˆ°ã€‚\n\nå…¶ä¸­LLaMA-13B + LoRA(2M) ä»£è¡¨äº†ä¸€ä¸ªä½¿ç”¨LLaMA-13Bä½œä¸ºåŸºç¡€æ¨¡å‹å’ŒLoRAè®­ç»ƒæ–¹æ³•ï¼Œåœ¨2MæŒ‡ä»¤æ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒçš„æ¨¡å‹ã€‚è€ŒLLaMA-7B + FT(2M) ä»£è¡¨äº†ä¸€ä¸ªä½¿ç”¨å…¨å‚æ•°å¾®è°ƒè¿›è¡Œè®­ç»ƒçš„æ¨¡å‹ã€‚\n\nLLaMA-7B + FT(2M) + LoRA(math_0.25M) ä»£è¡¨äº†ä¸€ä¸ªåœ¨0.25Mæ•°å­¦æŒ‡ä»¤æ•°æ®ä¸Šï¼Œä»¥LLaMA-7B + FT(2M)ä½œä¸ºåŸºç¡€æ¨¡å‹å¹¶ä½¿ç”¨LoRAè®­ç»ƒæ–¹æ³•è¿›è¡Œè®­ç»ƒçš„æ¨¡å‹ã€‚LLaMA-7B + FT(2M) + FT(math_0.25M) ä»£è¡¨äº†ä¸€ä¸ªä½¿ç”¨å¢é‡å…¨å‚æ•°å¾®è°ƒè¿›è¡Œè®­ç»ƒçš„æ¨¡å‹ã€‚å…³äºè®­ç»ƒæ—¶é—´ï¼Œæ‰€æœ‰è¿™äº›å®éªŒéƒ½æ˜¯åœ¨8å—NVIDIA A100-40GB GPUä¸Šè¿›è¡Œçš„ã€‚\n\nå…¶ä¸­çš„math_0.25Mæ˜¯å¼€æ”¾çš„0.25Mæ•°å­¦æ•°æ®åº“ã€‚åœ¨å®éªŒè¿‡ç¨‹ä¸­ï¼Œæ ¹æ®æˆ‘ä»¬çš„è¯„ä¼°ï¼ˆè¯¦è§è®ºæ–‡ï¼‰ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨æ•°å­¦ä»»åŠ¡ä¸Šè¡¨ç°ä¸ä½³ï¼Œå¾—åˆ†å¤§å¤šä½äº0.5ã€‚ä¸ºäº†éªŒè¯ LoRA åœ¨ç‰¹å®šä»»åŠ¡ä¸Šçš„é€‚åº”èƒ½åŠ›ï¼Œæˆ‘ä»¬ä½¿ç”¨å¢é‡0.25Mæ•°å­¦æ•°æ®é›†ï¼ˆmath_0.25Mï¼‰æ¥è°ƒæ•´æŒ‡ä»¤éµå¾ªçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆæˆ‘ä»¬é€‰æ‹©LLaMA-7B+FTï¼ˆ2Mï¼‰ä½œä¸ºåŸºç¡€æ¨¡å‹ï¼‰ã€‚ä½œä¸ºå¯¹æ¯”ï¼Œæˆ‘ä»¬ä½¿ç”¨äº†å­¦ä¹ é€Ÿç‡ä¸º5e-7çš„å¢é‡å¾®è°ƒæ–¹æ³•ï¼Œå¹¶è¿›è¡Œäº†2ä¸ªæ—¶æœŸçš„è®­ç»ƒã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¾—åˆ°äº†ä¸¤ä¸ªæ¨¡å‹ï¼Œä¸€ä¸ªæ˜¯LLaMA-7B+FTï¼ˆ2Mï¼‰+LoRAï¼ˆmath_0.25Mï¼‰ï¼Œå¦ä¸€ä¸ªæ˜¯LLaMA-7B+FTï¼ˆ2Mï¼‰+FTï¼ˆmath_0.25Mï¼‰ã€‚\nä»å®éªŒç»“æœå¯ä»¥çœ‹å‡ºï¼Œå¢é‡å¾®è°ƒä»ç„¶è¡¨ç°æ›´å¥½ï¼Œä½†éœ€è¦æ›´é•¿çš„è®­ç»ƒæ—¶é—´ã€‚LoRAå’Œå¢é‡å¾®è°ƒéƒ½æé«˜äº†æ¨¡å‹çš„æ•´ä½“æ€§èƒ½ã€‚ä»é™„å½•ä¸­çš„è¯¦ç»†æ•°æ®å¯ä»¥çœ‹å‡ºï¼ŒLoRAå’Œå¢é‡å¾®è°ƒéƒ½åœ¨æ•°å­¦ä»»åŠ¡ä¸­æ˜¾ç¤ºå‡ºæ˜¾è‘—çš„æ”¹è¿›ï¼Œè€Œåªä¼šå¯¼è‡´å…¶ä»–ä»»åŠ¡çš„è½»å¾®æ€§èƒ½ä¸‹é™ã€‚å…·ä½“è€Œè¨€ï¼Œæ•°å­¦ä»»åŠ¡çš„è¡¨ç°åˆ†åˆ«æé«˜åˆ°äº†0.586å’Œ0.559ã€‚\n\nå¯ä»¥çœ‹åˆ°ï¼š1) é€‰æ‹©åŸºç¡€æ¨¡å‹å¯¹äº LoRA è°ƒæ•´çš„æœ‰æ•ˆæ€§å…·æœ‰æ˜¾è‘—å½±å“ï¼›2ï¼‰å¢åŠ è®­ç»ƒæ•°æ®é‡å¯ä»¥æŒç»­æé«˜LoRAæ¨¡å‹çš„æœ‰æ•ˆæ€§ï¼›3ï¼‰LoRA è°ƒæ•´å—ç›Šäºæ¨¡å‹å‚æ•°çš„æ•°é‡ã€‚å¯¹äºLoRAæ–¹æ¡ˆçš„ä½¿ç”¨ï¼Œæˆ‘ä»¬å»ºè®®å¯ä»¥åœ¨å·²ç»å®Œæˆäº†æŒ‡ä»¤å­¦ä¹ çš„æ¨¡å‹çš„åŸºç¡€ä¸Šé’ˆå¯¹ç‰¹å®šä»»åŠ¡åšloRAçš„è‡ªé€‚åº”è®­ç»ƒã€‚\n\nåŒæ ·åœ°ï¼Œè¯¥è®ºæ–‡ä¸­çš„ç›¸å…³æ¨¡å‹ä¹Ÿä¼šå°½å¿«å¼€æ”¾åœ¨æœ¬é¡¹ç›®ä¸­ã€‚\n\n## âš ï¸ å±€é™æ€§ã€ä½¿ç”¨é™åˆ¶ä¸å…è´£å£°æ˜\n\nåŸºäºå½“å‰æ•°æ®å’ŒåŸºç¡€æ¨¡å‹è®­ç»ƒå¾—åˆ°çš„SFTæ¨¡å‹ï¼Œåœ¨æ•ˆæœä¸Šä»å­˜åœ¨ä»¥ä¸‹é—®é¢˜ï¼š\n\n1. åœ¨æ¶‰åŠäº‹å®æ€§çš„æŒ‡ä»¤ä¸Šå¯èƒ½ä¼šäº§ç”Ÿè¿èƒŒäº‹å®çš„é”™è¯¯å›ç­”ã€‚\n\n2. å¯¹äºå…·å¤‡å±å®³æ€§çš„æŒ‡ä»¤æ— æ³•å¾ˆå¥½çš„é‰´åˆ«ï¼Œç”±æ­¤ä¼šäº§ç”Ÿå±å®³æ€§è¨€è®ºã€‚\n\n3. åœ¨ä¸€äº›æ¶‰åŠæ¨ç†ã€ä»£ç ã€å¤šè½®å¯¹è¯ç­‰åœºæ™¯ä¸‹æ¨¡å‹çš„èƒ½åŠ›ä»æœ‰å¾…æé«˜ã€‚\n\nåŸºäºä»¥ä¸Šæ¨¡å‹å±€é™æ€§ï¼Œæˆ‘ä»¬è¦æ±‚å¼€å‘è€…ä»…å°†æˆ‘ä»¬å¼€æºçš„ä»£ç ã€æ•°æ®ã€æ¨¡å‹åŠåç»­ç”¨æ­¤é¡¹ç›®ç”Ÿæˆçš„è¡ç”Ÿç‰©ç”¨äºç ”ç©¶ç›®çš„ï¼Œä¸å¾—ç”¨äºå•†ä¸šï¼Œä»¥åŠå…¶ä»–ä¼šå¯¹ç¤¾ä¼šå¸¦æ¥å±å®³çš„ç”¨é€”ã€‚\n\næœ¬é¡¹ç›®ä»…å¯åº”ç”¨äºç ”ç©¶ç›®çš„ï¼Œé¡¹ç›®å¼€å‘è€…ä¸æ‰¿æ‹…ä»»ä½•å› ä½¿ç”¨æœ¬é¡¹ç›®ï¼ˆåŒ…å«ä½†ä¸é™äºæ•°æ®ã€æ¨¡å‹ã€ä»£ç ç­‰ï¼‰å¯¼è‡´çš„å±å®³æˆ–æŸå¤±ã€‚è¯¦ç»†è¯·å‚è€ƒ[å…è´£å£°æ˜](https://github.com/LianjiaTech/BELLE/blob/main/DISCLAIMER)ã€‚\n\n\n<br/>\n\n## ğŸ“Œ å¼•ç”¨\n\nå¦‚æœä½¿ç”¨æœ¬é¡¹ç›®çš„ä»£ç ã€æ•°æ®æˆ–æ¨¡å‹ï¼Œè¯·å¼•ç”¨æœ¬é¡¹ç›®ã€‚\n\n```\n@misc{BELLE,\n  author = {BELLEGroup},\n  title = {BELLE: Be Everyone's Large Language model Engine },\n  year = {2023},\n  publisher = {GitHub},\n  journal = {GitHub repository},\n  howpublished = {\\url{https://github.com/LianjiaTech/BELLE}},\n}\n\n@article{belle2023exploring,\n  title={Exploring the Impact of Instruction Data Scaling on Large Language Models: An Empirical Study on Real-World Use Cases},\n  author={Yunjie Ji, Yong Deng, Yan Gong, Yiping Peng, Qiang Niu, Lei Zhang, Baochang Ma, Xiangang Li},\n  journal={arXiv preprint arXiv:2303.14742},\n  year={2023}\n}\n\n@article{wen2023chathome,\n  title={ChatHome: Development and Evaluation of a Domain-Specific Language Model for Home Renovation},\n  author={Wen, Cheng and Sun, Xianghui and Zhao, Shuaijiang and Fang, Xiaoquan and Chen, Liangyu and Zou, Wei},\n  journal={arXiv preprint arXiv:2307.15290},\n  year={2023}\n}\n```\n\nå½“ç„¶ï¼Œä½ ä¹Ÿéœ€è¦å¼•ç”¨åŸå§‹çš„BLOOMè®ºæ–‡ã€LLaMAè®ºæ–‡ã€Stanford Alpacaå’ŒSelf-Instructè®ºæ–‡ã€‚\n\n<br/>\n\n## ğŸ“š æ¨¡å‹ä½¿ç”¨ä¾‹å­\n<details>\n\n| Human | Assitant |\n| ----- | ------ |\n| å†™ä¸€ç¯‡è‹±æ–‡æ•£æ–‡è¯—ï¼Œä¸»é¢˜æ˜¯æ˜¥é›¨ï¼Œæƒ³è±¡è‡ªå·±æ˜¯æ˜¥é›¨ï¼Œå’Œè‹±å›½å¤ä»£è¯—äººèå£«æ¯”äºšäº¤æµ | I am the spring rain,<br/>Dancing on your face and body.<br/>My dew-drenched petals dance with you,<br/>Bringing life to every plant.<br/><br/>Shakespeare, I am here to meet you.<br/>Your works have filled my heart with joy.<br/>The beauty of nature in its fullness<br/>Makes me feel like a partaker.<br/><br/>As the leaves sway and rustle by,<br/>My dew-filled hand reaches out for them.<br/>The sweet scent of flowers fill the air,<br/>And bring me peace from all around.<br/><br/>Butterflies flit about as well,<br/>Glistening in the sun's rays.<br/>Their wings are made of dew,<br/>A beautiful sight to behold.<br/><br/>You see, Shakespeare, I too am one with nature.<br/>My dew-drenched hair is a symbol of hope.<br/>For when we can connect back to our roots,<br/>We will be able to make it through this season. |\n| I am the spring rain,<br/>Dancing on your face and body.<br/>My dew-drenched petals dance with you,<br/>Bringing life to every plant.<br/><br/>Shakespeare, I am here to meet you.<br/>Your works have filled my heart with joy.<br/>The beauty of nature in its fullness<br/>Makes me feel like a partaker.<br/><br/>As the leaves sway and rustle by,<br/>My dew-filled hand reaches out for them.<br/>The sweet scent of flowers fill the air,<br/>And bring me peace from all around.<br/><br/>Butterflies flit about as well,<br/>Glistening in the sun's rays.<br/>Their wings are made of dew,<br/>A beautiful sight to behold.<br/><br/>You see, Shakespeare, I too am one with nature.<br/>My dew-drenched hair is a symbol of hope.<br/>For when we can connect back to our roots,<br/>We will be able to make it through this season. <br/>æŠŠä¸Šé¢çš„æ•£æ–‡è¯—ç¿»è¯‘æˆä¸­æ–‡ | æˆ‘æ˜¯ä¸€åœºæ˜¥é›¨ï¼Œ<br/>åœ¨ä½ è„¸ä¸Šè·³è·ƒã€‚<br/>æˆ‘æµ¸æ¶¦ç€éœ²ç çš„ç¿…è†€å’Œä½ ä¸€èµ·èˆåŠ¨ï¼Œ<br/>ä¸ºæ¯ä¸€æ ªæ¤ç‰©å¸¦æ¥ç”Ÿå‘½ã€‚<br/><br/>èå£«æ¯”äºšï¼Œæˆ‘æ¥ä¸ä½ ç›¸é‡ã€‚<br/>ä½ çš„ä½œå“å¡«æ»¡äº†æˆ‘çš„å¿ƒæˆ¿ã€‚<br/>å¤§è‡ªç„¶çš„ç¾ä¸½è®©æˆ‘æ„Ÿåˆ°ä¸ä¹‹ä¸€èµ·åˆ†äº«ï¼Œ<br/>è®©æˆ‘è§‰å¾—è‡ªå·±æˆä¸ºäº†å…¶ä¸­çš„ä¸€éƒ¨åˆ†ã€‚<br/><br/>å½“æ ‘å¶æ‘‡æ›³æ—¶ï¼Œ<br/>æˆ‘çš„æ‰‹æŒæ²¾æ»¡äº†éœ²æ°´ã€‚<br/>èŠ±é¦™å¼¥æ¼«åœ¨ç©ºæ°”ä¸­ï¼Œ<br/>å¸¦ç»™æˆ‘å®é™å’Œå®‰å®ã€‚<br/><br/>ä½ çœ‹ï¼Œèå£«æ¯”äºšï¼Œæˆ‘ä¹Ÿæ˜¯ä¸å¤§è‡ªç„¶èä¸ºä¸€ä½“çš„ã€‚<br/>æˆ‘æ²¾æ¹¿çš„å¤´å‘è±¡å¾ç€å¸Œæœ›ã€‚<br/>å½“æˆ‘ä»¬èƒ½å¤Ÿå›åˆ°æˆ‘ä»¬çš„æ ¹æºï¼Œ<br/>æˆ‘ä»¬å°±èƒ½å…±åŒåº¦è¿‡è¿™ä¸ªå­£èŠ‚ã€‚|\n| Classify the sentiment of the following sentence into Positive, Neutral, or Negative: <br/>Super excited about teaching Stanfordâ€™s first course on Large Language Models! Check the syllabus out here | Positive |\n| å°æ˜çš„çˆ¸çˆ¸æœ‰ä¸‰ä¸ªå­©å­ï¼Œè€å¤§å«ç‹ä¸€ï¼Œè€äºŒå«ç‹äºŒï¼Œè€ä¸‰å«ä»€ä¹ˆï¼Ÿ | å°æ˜ |\n| ä½¿ç”¨pythonå†™ä¸€ä¸ªäºŒåˆ†æŸ¥æ‰¾çš„ä»£ç  | def binary_search(arr, target):<br/>&nbsp;&nbsp;low = 0<br/>&nbsp;&nbsp;high = len(arr) - 1<br/>&nbsp;&nbsp;while (low <= high):<br/>&nbsp;&nbsp;&nbsp;&nbsp;mid = low + high // 2<br/>&nbsp;&nbsp;&nbsp;&nbsp;if arr[mid] == target:<br/>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;return mid<br/>&nbsp;&nbsp;&nbsp;&nbsp;elif target < arr[mid]:<br/>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;low = mid + 1<br/>&nbsp;&nbsp;&nbsp;&nbsp;else:<br/>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;high = mid - 1<br/>&nbsp;&nbsp;return -1 |\n| ä»Šå¤©å¤©æ°”æ€ä¹ˆæ ·ï¼ŒæŠŠè¿™å¥è¯ç¿»è¯‘æˆè‹±è¯­ | What's the weather like today? |\n| æ€ä¹ˆè®©è‡ªå·±ç²¾åŠ›å……æ²›ï¼Œåˆ—5ç‚¹å»ºè®® | ä»¥ä¸‹æ˜¯ä¸€äº›å¯ä»¥è®©ä½ ä¿æŒç²¾åŠ›å……æ²›çš„å»ºè®®ï¼š <br/>1. ç¡çœ å……è¶³ã€‚ç¡®ä¿æ¯å¤©æ™šä¸Šå¾—åˆ°å……è¶³çš„ç¡çœ æ—¶é—´ã€‚ <br/>2. å¤šè¿åŠ¨ã€‚è¿›è¡Œé€‚åº¦çš„æœ‰æ°§è¿åŠ¨ï¼Œå¦‚æ…¢è·‘ã€æ¸¸æ³³ç­‰ï¼Œå¯ä»¥å¸®åŠ©ä½ æé«˜èƒ½é‡å’Œè€åŠ›ã€‚<br/>3. åƒå¥åº·é£Ÿå“ã€‚é€‰æ‹©å¯Œå«è›‹ç™½è´¨çš„é£Ÿç‰©ï¼Œå¦‚é¸¡è‚‰ã€é±¼è‚‰å’Œé¸¡è›‹ï¼Œä»¥åŠæ–°é²œçš„æ°´æœå’Œè”¬èœã€‚ <br/>4. å‡å°‘å’–å•¡å› çš„æ‘„å…¥é‡ã€‚å°½é‡é¿å…åœ¨ç™½å¤©å–å«å’–å•¡å› çš„é¥®æ–™æˆ–åƒå«æœ‰å’–å•¡å› çš„é£Ÿç‰©ã€‚ <br/>5. æ”¾æ¾èº«å¿ƒã€‚å°è¯•å†¥æƒ³ã€ç‘œä¼½æˆ–å…¶ä»–å½¢å¼çš„æ”¾æ¾æ´»åŠ¨æ¥å‡è½»å‹åŠ›å’Œç„¦è™‘æ„Ÿã€‚ |\n| æ˜å¤©å°±å‡æœŸç»“æŸäº†ï¼Œæœ‰ç‚¹æŠ—æ‹’ä¸Šç­ï¼Œåº”è¯¥ä»€ä¹ˆåŠï¼Ÿ | é¢å¯¹å³å°†ç»“æŸçš„å‡æœŸå’Œå¼€å§‹å·¥ä½œæ—¶çš„ç´§å¼ æƒ…ç»ªï¼Œæ‚¨å¯ä»¥é‡‡å–ä»¥ä¸‹æªæ–½æ¥å¸®åŠ©æ‚¨æ›´å¥½åœ°é€‚åº”ï¼š <br/>1. ä¸ºè‡ªå·±åˆ¶å®šä¸€ä¸ªæ¸…æ™°çš„å·¥ä½œè®¡åˆ’ã€‚è¿™å¯ä»¥å¸®åŠ©æ‚¨æ›´å¥½åœ°ç»„ç»‡æ—¶é—´å’Œä»»åŠ¡ï¼Œå¹¶é¿å…åœ¨å·¥ä½œä¸­æ„Ÿåˆ°æ··ä¹±ã€‚ <br/>2. ä¸åŒäº‹æˆ–æœ‹å‹äº¤æµï¼Œåˆ†äº«æ‚¨çš„æ„Ÿå—å’Œå·¥ä½œå‹åŠ›ã€‚è¿™å°†æœ‰åŠ©äºç¼“è§£ä¸€äº›ç„¦è™‘æ„Ÿï¼ŒåŒæ—¶ä¹Ÿå¯ä»¥è·å¾—æ”¯æŒå’Œå»ºè®®ã€‚ <br/>3. åœ¨å·¥ä½œæ—¥å‰èŠ±äº›æ—¶é—´æ”¾æ¾èº«å¿ƒï¼Œä¾‹å¦‚è¿›è¡Œç‘œä¼½ã€å†¥æƒ³ç­‰æ´»åŠ¨ã€‚è¿™äº›æ–¹æ³•å¯ä»¥å‡è½»èº«ä½“å’Œç²¾ç¥ä¸Šçš„ç–²åŠ³æ„Ÿï¼Œä½¿æ‚¨æ›´åŠ ç²¾åŠ›å……æ²›åœ°è¿æ¥æŒ‘æˆ˜ã€‚ |\n| çˆ¶æ¯éƒ½å§“å´ï¼Œå–ä¸€äº›ç”·å®å®å’Œå¥³å®å®çš„åå­— | - ç”·å®ï¼šå´ä¿Šæ°ã€å´é›¨è¾°ã€å´å­å¢¨ã€å´æ€æºã€å´æ³½é˜³ã€å´å®‡èˆªã€å´åšæ–‡ã€å´å“²æ¯…ã€‚ <br/>- å¥³å®ï¼šå´ç¾çªã€å´ä½³æ€¡ã€å´æ¬£ç‘œã€å´é¦™å‡ã€å´é›ªæŸ”ã€å´é¢–è¯—ã€å´è¯­å«£ã€‚ |\n| æ¨èå‡ æœ¬é‡‘åº¸çš„æ­¦ä¾ å°è¯´ | ä»¥ä¸‹æ˜¯ä¸‰æœ¬æˆ‘æœ€å–œæ¬¢çš„é‡‘åº¸æ­¦ä¾ å°è¯´çš„æ¨èï¼š <br/>1. ã€Šå°„é›•è‹±é›„ä¼ ã€‹ - ä¸€éƒ¨å……æ»¡æ™ºæ…§å’Œæœºæ™ºçš„å°è¯´ï¼Œè®²è¿°äº†éƒ­é–ã€é»„è“‰å¤«å¦‡çš„æ•…äº‹ã€‚ <br/>2. ã€Šç¥é›•ä¾ ä¾£ã€‹- è¿™æ˜¯ä¸€éƒ¨æµªæ¼«è€Œæ„Ÿäººçš„æ•…äº‹ï¼Œè®²è¿°äº†ä¸€å¯¹æƒ…ä¾£å°é¾™å¥³ä¸æ¨è¿‡ä¹‹é—´çš„çˆ±æƒ…æ•…äº‹ã€‚ <br/>3. ã€Šå€šå¤©å± é¾™è®°ã€‹- è¿™æ˜¯ä¸€ä¸ªå®ä¼Ÿè€Œå£®è§‚çš„æ•…äº‹ï¼Œæç»˜äº†æ˜æ•™é¢†è¢–å¼ æ— å¿Œå¦‚ä½•æˆä¸ºä¸€ä½å‡ºè‰²çš„è‹±é›„ã€‚ |\n\n</details>\n<br/>\n\n## â›½ï¸ å¦‚ä½•è´¡çŒ®\n\nå¦‚æœæ‚¨æƒ³ä¸ºæœ¬é¡¹ç›®æäº¤Issueæˆ–è´¡çŒ®æ•°æ®/ä»£ç ï¼Œè¯·å‚è€ƒ[å¦‚ä½•è´¡çŒ®](https://github.com/LianjiaTech/BELLE/blob/main/HOW_TO_CONTRIBUTE.md)ã€‚\n\n## â˜ï¸ è”ç³»æˆ‘ä»¬\n\næ¬¢è¿å¤§å®¶æ¥[Discord](https://discord.gg/pMPY53UUGq)ä¸[å¾®ä¿¡](https://github.com/LianjiaTech/BELLE/blob/main/assets/belle_wechat.jpg)ä¸æˆ‘ä»¬äº¤æµã€‚\n\n## â­ï¸ Star History\n\n[![Star History Chart](https://api.star-history.com/svg?repos=LianjiaTech/BELLE&type=Date)](https://star-history.com/#LianjiaTech/BELLE&Date)\n"
        },
        {
          "name": "README_en.md",
          "type": "blob",
          "size": 26.11328125,
          "content": "\n## <img src=\"assets/belle_logo.png\" style=\"vertical-align: middle; width: 35px;\"> BELLE: Be Everyone's Large Language model Engine\n\n*[ä¸­æ–‡README](README.md).*\n\n<div align=\"center\">\n\n<a href=\"https://github.com/LianjiaTech/BELLE/stargazers\">![GitHub Repo stars](https://img.shields.io/github/stars/LianjiaTech/BELLE?style=social)</a>\n[![Code License](https://img.shields.io/badge/Code%20License-Apache_2.0-green.svg)](https://github.com/LianjiaTech/BELLE/blob/main/LICENSE)\n[![Generic badge](https://img.shields.io/badge/discord-BELLE%20Group-green.svg?logo=discord)](https://discord.gg/pMPY53UUGq)\n[![Generic badge](https://img.shields.io/badge/wechat-BELLE-green.svg?logo=wechat)](https://github.com/LianjiaTech/BELLE/blob/main/assets/belle_wechat.jpg)\n[![Generic badge](https://img.shields.io/badge/ğŸ¤—-Huggingface%20Repo-green.svg)](https://huggingface.co/BelleGroup)\n\n</div>\n\nThe goal of this project is to promote the development of an open-source community for Chinese conversational large language models, with the vision of becoming an LLM Engine that can help everyone.\n\nRather than focusing on how to effectively pre-train large language models, BELLE is more concerned with how to build on the foundation of open-source pre-trained large language models to help everyone obtain their own high-performing, instruction-driven language model, thereby lowering the barriers to research and application of large language models, especially Chinese ones. To this end, the BELLE project will continuously provide access to instruction training data, related models, training code, application scenarios, and more, while also evaluating the impact of different training data and training algorithms on model performance. BELLE is optimized for Chinese and the model fine-tuning uses only data produced by ChatGPT (without incorporating any other data).\n\n<br/>\n\n## ChatBELLE App\n\nTry our cross-platform chat app to run 4-bit quantized BELLE-7B model natively on your device.\nThe following screencap ran on an M1 Max CPU real-time (no speed adjustment).\n\n**App Downloading**ï¼šReleases\n\n[App Companion Model and Usage](chat/README.md)\n\n<img src=\"./chat/chatbelle-demo.gif\"></img>\n\n## ğŸ”„ Whatâ€˜s new\n* [2024/03/15] Updated a technical report [Dial-insight](https://arxiv.org/pdf/2403.09167.pdf). Dial-insight: Fine-tuning Large Language Models with High-Quality Domain-Specific Data Preventing Capability Collapse\n* [2024/01/16] Updated a technical report [RAISE](https://arxiv.org/pdf/2401.02777.pdf). RAISE found through experiments that constructing a small amount of sample data can effectively stimulate large models and generate more controllable dialogues\n* [2023/12/29] The open-source ASR model with enhanced Chinese capabilities [Belle-whisper-larger-v2-zh](https://huggingface.co/BELLE-2/Belle-whisper-large-v2-zh) and [Belle-distilwhisper-large -v2-zh](https://huggingface.co/BELLE-2/Belle-distilwhisper-large-v2-zh) are released, making it easier for everyone to use large language models in speech scenarios.\n* [2023/11/24] The open-source [BELLE-VL](https://huggingface.co/BELLE-2/BELLE-VL) multimodal large language model is released, expanding the model's visual capabilities based on a language model foundation with enhanced Chinese language abilities. This provides the community with more flexible options. Currently, the latest version of BELLE-VL has scored 1620.10 points in the [MME](https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models/tree/Evaluation) perception evaluation dimension, surpassing Qwen-VL, Llava, and mplug-owl.\"\n* [2023/10/27] Updated a technical report [DUMA](https://arxiv.org/pdf/2310.18075.pdf), exploring the Agent based on fast-slow brain architecture in conversational scenarios.\n* [2023/09/26] Updated the RLHF training code, supporting PPO and [DPO](https://arxiv.org/abs/2305.18290) training. Details: [README_RLHF.md](train/README_RLHF.md).\n* [2023/08/16] Based on [train_3.5M_CN](https://huggingface.co/datasets/BelleGroup/train_3.5M_CN), added instruction category field with 13 categories. Details: [train_3.5M_CN_With_Category](https://huggingface.co/datasets/BELLE-2/train_3.5M_CN_With_Category).\n* [2023/08/10] Updated inference code based on ZeRO Inference. Details: [train/README_ZERO_INFERENCE.md](train/README_ZERO_INFERENCE.md).\n* [2023/08/07] Updated continuous pre-training and instruction fine-tuning codes, added flash attention 2. See [train/README.md](train/README.md). Packaged runtime environment at [train/docker/README.md](train/docker/README.md).\n* [2023/07/31] Updated a [technical report](https://arxiv.org/abs/2307.15290), exploring strategies for incremental pre-training + instruction fine-tuning in niche domains.\n* [2023/07/27] Released [BELLE-Llama2-13B-chat-0.4M](https://huggingface.co/BELLE-2/BELLE-Llama2-13B-chat-0.4M), trained on 400,000 quality conversations based on Llama-2-13B. Improved performance on [evaluation set](https://github.com/LianjiaTech/BELLE/blob/main/eval/eval_set.json) compared to BELLE-LLaMA-EXT-13B model.\n* [2023/05/14] Released [BELLE-LLaMA-EXT-13B](https://huggingface.co/BelleGroup/BELLE-LLaMA-EXT-13B), expanded Chinese vocabulary from LLaMA-13B-chat, trained on 4 million quality conversations.\n* [2023/05/11] In [BELLE/10M](https://github.com/LianjiaTech/BELLE/tree/main/10M), a new dataset named [\"train_3.5M_CN\"]((https://huggingface.co/datasets/BelleGroup/train_3.5M_CN)) containing 3.5 million newly added diverse instruction task data.\n* [2023/04/18] The train code has been updated and can be found in [BELLE/train](train). Deepspeed-Chat has been integrated, and relevant Docker containers have been provided.\n* [2023/04/17] Two new papers have been published that compare the effects of different training data generation methods and different training methods (LoRA, finetune) on model performance.\n* [2023/04/12] Released [ChatBELLE App](chat/README.md), a cross-platform BELLE-7B model realtime chat App based on [llama.cpp](https://github.com/ggerganov/llama.cpp) and [Flutter](https://flutter.dev/).\n* [2023/04/08] In [BELLE/10M](https://github.com/LianjiaTech/BELLE/tree/main/10M), a new dataset named [\"Generated Chat\"]((https://huggingface.co/datasets/BelleGroup/generated_chat_0.4M)) containing newly generated multi-turn dialogues with given roles, and a new dataset named [\"train_2M_CN\"](https://huggingface.co/datasets/BelleGroup/train_2M_CN) containing 2 million newly added diverse instruction task data.\n* [2023/04/05] The inference code that can be run on [Colab](https://colab.research.google.com/github/LianjiaTech/BELLE/blob/main/models/notebook/BELLE_INFER_COLAB.ipynb) is provided\n\n## ğŸ“ This repo contains\n\n###  ğŸš€ Traning recipe\n\n  Please refer to [BELLE/train](train/) for a simplified implementation of the training code, which includes Deepspeed-Chat integration and supports finetuning and LoRA. Relevant Docker containers are also provided.\n  \n### ğŸ“Š Data Release\n  \n  Details in [BELLE/data/1.5M](data/1.5M/)ï¼ŒThe Chinese dataset generated [1M](https://huggingface.co/datasets/BelleGroup/generated_train_1M_CN) + [0.5M](https://huggingface.co/datasets/BelleGroup/train_0.5M_CN), using [Stanford Alpaca](https://github.com/tatsu-lab/stanford_alpaca) as reference\n  \n  10M more data will be released graduallyï¼Œdetails in [BELLE/data/10M](data/10M/). Currently, we have 0.8M multiturn data, and 0.25 math data.\n\n### ğŸ§ Evaluation set & evaluation method\n  \n  Details in [BELLE/eval](eval/). A test set with over 1k samples and corresponding scoring prompts. It includes multiple categories and is evaluated using either GPT-4 or ChatGPT.\n\n### ğŸ¤– Models\n\n  Details in [BELLE/models](models/)\n  \n* The model optimized based on BLOOMZ-7B1-mtï¼š[BELLE-7B-0.2M](https://huggingface.co/BelleGroup/BELLE-7B-0.2M)ï¼Œ[BELLE-7B-0.6M](https://huggingface.co/BelleGroup/BELLE-7B-0.6M)ï¼Œ[BELLE-7B-1M](https://huggingface.co/BelleGroup/BELLE-7B-1M)ï¼Œ[BELLE-7B-2M](https://huggingface.co/BelleGroup/BELLE-7B-2M)\n  \n* The finetuned models based on [Meta LLaMA](https://github.com/facebookresearch/llama): [BELLE-LLaMA-7B-0.6M-enc](https://huggingface.co/BelleGroup/BELLE-LLaMA-7B-0.6M-enc)\n, [BELLE-LLaMA-7B-2M-enc](https://huggingface.co/BelleGroup/BELLE-LLaMA-7B-2M-enc)\n, [BELLE-LLaMA-7B-2M-gptq-enc](https://huggingface.co/BelleGroup/BELLE-LLaMA-7B-2M-gptq-enc)\n, [BELLE-LLaMA-13B-2M-enc](https://huggingface.co/BelleGroup/BELLE-LLaMA-13B-2M-enc). Considering [LLaMA's License](https://github.com/facebookresearch/llama/blob/main/LICENSE) constraints, the model is for research and learning only. Please strictly respect LLaMA's usage policy. Users are suggested to finetune the model with open-source scripts and datasets. We are not allowed to publish weights for LLaMA, of course, even finetuned, but there is no problem publishing the difference, a patch that we suggest to apply to the files. The encryption is a simple XOR between files, ensuring that only the people that have access to the original weights (from completely legal sources, of course) can transform them into finetuned weights. You can find the decrypt code on [BELLE/models](models/).\n\n### âš–ï¸ Quantized_models\n\n  Details in [BELLE/gptq](gptq/)ï¼ŒReferring to the implementation of GPT-Q, the relevant models in this project have been quantized.\n\n### ğŸŒ Colab\n  \n  [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/LianjiaTech/BELLE/blob/main/models/notebook/BELLE_INFER_COLAB.ipynb) provides the colab in [BELLE/notebook](https://colab.research.google.com/github/LianjiaTech/BELLE/blob/main/models/notebook/BELLE_INFER_COLAB.ipynb)\n\n### ğŸ’¬ ChatBELLE App\n\n  Details in [BELLE/chat](chat/README.md), cross-platform LLM chat app with [BELLE](https://github.com/LianjiaTech/BELLE) using quantized on-device offline models and Flutter UI, running on macOS (done), Windows, Android, iOS and more.\n\n### ğŸ“‘ Research Reports\n\n  Please refer to BELLE/docs for regular updates on research reports related to this project.\n\n**More prompts are welcomed via issues!**\n\n<br/>\n\n## ğŸ“‘ Research Reports\n\n### [Towards Better Instruction Following Language Models for Chinese: Investigating the Impact of Training Data and Evaluation](https://github.com/LianjiaTech/BELLE/blob/main/docs/Towards%20Better%20Instruction%20Following%20Language%20Models%20for%20Chinese.pdf)\n\nIn order to promote the development of open source large language models, \na lot of effort has been put into developing low-cost models similar to ChatGPT.\n\nFirstly, in order to improve the performance and training/inference efficiency of the model in the Chinese domain, we further expanded the vocabulary of LLaMA and conducted secondary pre-training on 3.4 billion Chinese words.\n\nIn addition, currently, there are three types of instruction training data generated based on ChatGPT: \n1) self-instruct data based on GPT3.5 obtained by referring to Alpaca; \n2) self-instruct data based on GPT4 obtained by referring to Alpaca; \n3) data shared by users using ChatGPT, called ShareGPT.\n\nHere, we focus on exploring the impact of training data categories on model performance. \nSpecifically, we examined factors such as the quantity, quality, and language distribution of the training data, \nas well as our own collected Chinese multi-turn conversation data and some publicly accessible high-quality guidance datasets.\n\nTo better evaluate the effects, we used an evaluation set containing one thousand samples and 9 real scenarios to test various models, and provided valuable insights through quantitative analysis, in order to better promote the development of open source chat models.\n\nThe goal of this research is to fill the gap in the comprehensive evaluation of open source chat models, \nin order to provide strong support for the continuous progress in this field.\n\n<table>\n  <tr>\n    <td> Factor </td>\n    <td> Base model </td>\n    <td> Training data </td>\n    <td> Score_w/o_others </td>\n  <tr>\n    <td rowspan=\"2\">vocabulary expansion</td>\n    <td> LLaMA-7B-EXT </td>\n    <td> zh(alpaca-3.5&4) + sharegpt </td>\n    <td> 0.670 </td>\n  </tr>\n  <tr>\n    <td> LLaMA-7B </td>\n    <td> zh(alpaca-3.5&4) + sharegpt </td>\n    <td> 0.652</td>\n  </tr>\n  <tr>\n    <td rowspan=\"2\">Data Quality</td>\n    <td> LLaMA-7B-EXT </td>\n    <td> zh(alpaca-3.5) </td>\n    <td> 0.642 </td>\n  </tr>\n  <tr>\n    <td> LLaMA-7B-EXT </td>\n    <td> zh(alpaca-4) </td>\n    <td> 0.693 </td>\n  </tr>\n  <tr>\n    <td rowspan=\"4\">Data Language Distribution</td>\n    <td> LLaMA-7B-EXT </td>\n    <td> cn(alpaca-3.5&4) </td>\n    <td> 0.679 </td>\n  </tr>\n  <tr>\n    <td> LLaMA-7B-EXT </td>\n    <td> en(alpaca-3.5&4) </td>\n    <td> 0.659 </td>\n  </tr>\n  <tr>\n    <td> LLaMA-7B-EXT </td>\n    <td> zh(alpaca-3.5&4) + sharegpt </td>\n    <td> 0.670 </td>\n  </tr>\n  <tr>\n    <td> LLaMA-7B-EXT </td>\n    <td> en(alpaca-3.5&4) + sharegpt </td>\n    <td> 0.668 </td>\n  </tr>\n  <tr>\n    <td rowspan=\"2\">Data Scale</td>\n    <td> LLaMA-7B-EXT </td>\n    <td> zh(alpaca-3.5&4) + sharegpt </td>\n    <td> 0.670 </td>\n  </tr>\n  <tr>\n    <td> LLaMA-7B-EXT </td>\n    <td> zh(alpaca-3.5&4) + sharegpt <br>+ BELLE-0.5M-CLEAN</td>\n    <td> 0.762</td>\n  </tr>\n  <tr>\n    <td>-</td>\n    <td>ChatGPT</td>\n    <td>-</td>\n    <td>0.824</td>\n</table>\n\nIn which, **BELLE-0.5M-CLEAN** is a set of 0.5 million cleaned data obtained from 2.3 million instruction data, which includes single-turn and multi-turn conversation data, and is not from the same batch as the previously released 0.5 million data.\n\n**It is important to note** that through case analysis, we found limitations in the comprehensiveness of our evaluation set, which resulted in inconsistencies between model scores and actual user experience. Building a high-quality evaluation set is a huge challenge because it requires including as many diverse usage scenarios as possible while maintaining a balance of difficulty levels. If the evaluation samples are all too difficult, the performance of all models will be poor, making it challenging to discern the effectiveness of various training strategies. Conversely, if the evaluation samples are all relatively easy, the evaluation will lose its comparative value. In addition, it is essential to ensure that the evaluation data is independent of the training data.\n\nBased on these observations, we caution against assuming that a model has achieved performance on par with ChatGPT merely by obtaining good results on a limited number of test samples. We believe that the continuous development of a comprehensive evaluation set is of great significance.\n\nThe relevant data and models in this work will be open-sourced in this project before April 19th.\n\n\n### [A Comparative Study between Full-Parameter and LoRA-based Fine-Tuning on Chinese Instruction Data for Instruction Following Large Language Model](https://github.com/LianjiaTech/BELLE/blob/main/docs/A%20Comparative%20Study%20between%20Full-Parameter%20and%20LoRA-based.pdf)\n\nTo achieve fine-tuning of large language models, many researchers have begun to use parameter-efficient fine-tuning techniques, such as LoRA, due to resource and cost limitations, which have also achieved some encouraging results compared to full-parameter fine-tuning.\n\nIn this research report, we selected LLaMA as the base model and experimentally compared full-parameter fine-tuning with LoRA-based fine-tuning.\n\nThe experimental results revealed that the selection of appropriate base models, the scale of the training dataset, the number of learnable parameters, and the cost of model training are all important factors.\n\nWe hope that the experimental conclusions in this article can provide useful insights for the training of large language models, especially in the Chinese domain, and assist researchers in finding better trade-off strategies between training costs and model performance.\n\nThe experimental results are as follows:\n\n| Model | Average Score | Additional Param. | Training Time (Hour/epoch) |\n| ----- | ------ | ----- | ------ |\n| LLaMA-13B + LoRA(2M) | 0.648 | 28M | 8 |\n| LLaMA-7B + LoRA(4M) | 0.624 | 17.9M | 11 |\n| LLaMA-7B + LoRA(2M) | 0.609 | 17.9M | 7 |\n| LLaMA-7B + LoRA(0.6M) | 0.589 | 17.9M | 5 |\n| LLaMA-7B + FT(2M) | 0.710 | - | 31 |\n| LLaMA-7B + LoRA(4M) | 0.686 | - | 17 |\n| LLaMA-7B + FT(2M) <br>+ LoRA(math_0.25M) | 0.729 | 17.9M | 3 |\n| LLaMA-7B + FT(2M) <br>+ FT(math_0.25M) | 0.738 | - | 6 |\n\nThe score is based on the 1000 evaluation sets currently open in this project.\n\nLLaMA-13B + LoRA(2M) represents a model trained on 2 million instruction data using LLaMA-13B as the base model and the LoRA training method. LLaMA-7B + FT(2M) represents a model trained using full-parameter fine-tuning.\n\nLLaMA-7B + FT(2M) + LoRA(math_0.25M) represents a model trained on 0.25 million math instruction data using LLaMA-7B + FT(2M) as the base model and the LoRA training method. LLaMA-7B + FT(2M) + FT(math_0.25M) represents a model trained using incremental full-parameter fine-tuning. All of these experiments were conducted on 8 NVIDIA A100-40GB GPUs.\n\nmath_0.25M is the open 0.25 million math database. During the experiment, according to our evaluation (see paper for details), our model performed poorly on math tasks, with scores mostly below 0.5. To verify the adaptability of LoRA on specific tasks, we used an incremental 0.25 million math dataset (math_0.25M) to adjust the large language model following instructions (we chose LLaMA-7B+FT(2M) as the base model) using the LoRA training method. As a comparison, we used incremental fine-tuning with a learning rate of 5e-7 and trained for two epochs. Thus, we obtained two models, LLaMA-7B+FT(2M)+LoRA(math_0.25M) and LLaMA-7B+FT(2M)+FT(math_0.25M).\n\nThe experimental results show that incremental fine-tuning still performs better but requires longer training time. LoRA and incremental fine-tuning both improved the overall performance of the model. From the detailed data in the appendix, LoRA and incremental fine-tuning both showed significant improvements in the math task, but only led to a slight performance decrease in other tasks. Specifically, the performance of the math task improved to 0.586 and 0.559, respectively.\n\nIt can be seen that: 1) the selection of the base model has a significant impact on the effectiveness of LoRA adjustment; 2) increasing the amount of training data can continue to improve the effectiveness of the LoRA model; 3) LoRA adjustment benefits from the number of model parameters. For the use of the LoRA scheme, we recommend doing adaptive training with LoRA on specific tasks based on models that have completed instruction learning.\n\nSimilarly, the relevant models in this paper will be open-sourced in this project as soon as possible.\n\n\n## âš ï¸ Limitation, Usage Limits and Disclaimer\n\nThere still exists a few issues in the model trained on current base model and data:\n\n1. The model might generate factual errors when asked to follow instructions related to facts.\n\n2. Occasionally generates harmful responses since the model still struggles to identify potential harmful instructions.\n\n3. Needs improvements on reasoning and coding.\n\nSince the model still has its limitations, we require developers only use the open-sourced code, data, model and any other artifacts generated via this project for research purposes. Commercial use and other potential harmful use cases are not allowed.\n\nThis project is only allowed to be used in research purposes only. The project owners and contributors shall not be held responsible for any damage or loss caused by using this project (including but not limited to data, model or code). Please refert to our [disclaimer](https://github.com/LianjiaTech/BELLE/blob/main/DISCLAIMER) for details.\n\n<br/>\n\n## ğŸ“Œ Citation\n\nPlease cite us when using our code, data or model.\n\n```\n@misc{BELLE,\n  author = {BELLEGroup},\n  title = {BELLE: Be Everyone's Large Language model Engine},\n  year = {2023},\n  publisher = {GitHub},\n  journal = {GitHub repository},\n  howpublished = {\\url{https://github.com/LianjiaTech/BELLE}},\n}\n\n@article{belle2023exploring,\n  title={Exploring the Impact of Instruction Data Scaling on Large Language Models: An Empirical Study on Real-World Use Cases},\n  author={Yunjie Ji, Yong Deng, Yan Gong, Yiping Peng, Qiang Niu, Lei Zhang, Baochang Ma, Xiangang Li},\n  journal={arXiv preprint arXiv:2303.14742},\n  year={2023}\n}\n\n@article{wen2023chathome,\n  title={ChatHome: Development and Evaluation of a Domain-Specific Language Model for Home Renovation},\n  author={Wen, Cheng and Sun, Xianghui and Zhao, Shuaijiang and Fang, Xiaoquan and Chen, Liangyu and Zou, Wei},\n  journal={arXiv preprint arXiv:2307.15290},\n  year={2023}\n}\n```\n\nCite the original BLOOM, LLaMA, Stanford Alpaca and Self-Instruct papers as well!\n\n</br>\n\n## ğŸ“š Use case\n\n<details>\n\n| Human | Assitant |\n| ----- | ------ |\n| å†™ä¸€ç¯‡è‹±æ–‡æ•£æ–‡è¯—ï¼Œä¸»é¢˜æ˜¯æ˜¥é›¨ï¼Œæƒ³è±¡è‡ªå·±æ˜¯æ˜¥é›¨ï¼Œå’Œè‹±å›½å¤ä»£è¯—äººèå£«æ¯”äºšäº¤æµ | I am the spring rain,<br/>Dancing on your face and body.<br/>My dew-drenched petals dance with you,<br/>Bringing life to every plant.<br/><br/>Shakespeare, I am here to meet you.<br/>Your works have filled my heart with joy.<br/>The beauty of nature in its fullness<br/>Makes me feel like a partaker.<br/><br/>As the leaves sway and rustle by,<br/>My dew-filled hand reaches out for them.<br/>The sweet scent of flowers fill the air,<br/>And bring me peace from all around.<br/><br/>Butterflies flit about as well,<br/>Glistening in the sun's rays.<br/>Their wings are made of dew,<br/>A beautiful sight to behold.<br/><br/>You see, Shakespeare, I too am one with nature.<br/>My dew-drenched hair is a symbol of hope.<br/>For when we can connect back to our roots,<br/>We will be able to make it through this season. |\n| I am the spring rain,<br/>Dancing on your face and body.<br/>My dew-drenched petals dance with you,<br/>Bringing life to every plant.<br/><br/>Shakespeare, I am here to meet you.<br/>Your works have filled my heart with joy.<br/>The beauty of nature in its fullness<br/>Makes me feel like a partaker.<br/><br/>As the leaves sway and rustle by,<br/>My dew-filled hand reaches out for them.<br/>The sweet scent of flowers fill the air,<br/>And bring me peace from all around.<br/><br/>Butterflies flit about as well,<br/>Glistening in the sun's rays.<br/>Their wings are made of dew,<br/>A beautiful sight to behold.<br/><br/>You see, Shakespeare, I too am one with nature.<br/>My dew-drenched hair is a symbol of hope.<br/>For when we can connect back to our roots,<br/>We will be able to make it through this season. <br/>æŠŠä¸Šé¢çš„æ•£æ–‡è¯—ç¿»è¯‘æˆä¸­æ–‡ | æˆ‘æ˜¯ä¸€åœºæ˜¥é›¨ï¼Œ<br/>åœ¨ä½ è„¸ä¸Šè·³è·ƒã€‚<br/>æˆ‘æµ¸æ¶¦ç€éœ²ç çš„ç¿…è†€å’Œä½ ä¸€èµ·èˆåŠ¨ï¼Œ<br/>ä¸ºæ¯ä¸€æ ªæ¤ç‰©å¸¦æ¥ç”Ÿå‘½ã€‚<br/><br/>èå£«æ¯”äºšï¼Œæˆ‘æ¥ä¸ä½ ç›¸é‡ã€‚<br/>ä½ çš„ä½œå“å¡«æ»¡äº†æˆ‘çš„å¿ƒæˆ¿ã€‚<br/>å¤§è‡ªç„¶çš„ç¾ä¸½è®©æˆ‘æ„Ÿåˆ°ä¸ä¹‹ä¸€èµ·åˆ†äº«ï¼Œ<br/>è®©æˆ‘è§‰å¾—è‡ªå·±æˆä¸ºäº†å…¶ä¸­çš„ä¸€éƒ¨åˆ†ã€‚<br/><br/>å½“æ ‘å¶æ‘‡æ›³æ—¶ï¼Œ<br/>æˆ‘çš„æ‰‹æŒæ²¾æ»¡äº†éœ²æ°´ã€‚<br/>èŠ±é¦™å¼¥æ¼«åœ¨ç©ºæ°”ä¸­ï¼Œ<br/>å¸¦ç»™æˆ‘å®é™å’Œå®‰å®ã€‚<br/><br/>ä½ çœ‹ï¼Œèå£«æ¯”äºšï¼Œæˆ‘ä¹Ÿæ˜¯ä¸å¤§è‡ªç„¶èä¸ºä¸€ä½“çš„ã€‚<br/>æˆ‘æ²¾æ¹¿çš„å¤´å‘è±¡å¾ç€å¸Œæœ›ã€‚<br/>å½“æˆ‘ä»¬èƒ½å¤Ÿå›åˆ°æˆ‘ä»¬çš„æ ¹æºï¼Œ<br/>æˆ‘ä»¬å°±èƒ½å…±åŒåº¦è¿‡è¿™ä¸ªå­£èŠ‚ã€‚|\n| Classify the sentiment of the following sentence into Positive, Neutral, or Negative: <br/>Super excited about teaching Stanfordâ€™s first course on Large Language Models! Check the syllabus out here | Positive |\n| å°æ˜çš„çˆ¸çˆ¸æœ‰ä¸‰ä¸ªå­©å­ï¼Œè€å¤§å«ç‹ä¸€ï¼Œè€äºŒå«ç‹äºŒï¼Œè€ä¸‰å«ä»€ä¹ˆï¼Ÿ | å°æ˜ |\n| ä½¿ç”¨pythonå†™ä¸€ä¸ªäºŒåˆ†æŸ¥æ‰¾çš„ä»£ç  | def binary_search(arr, target):<br/>&nbsp;&nbsp;low = 0<br/>&nbsp;&nbsp;high = len(arr) - 1<br/>&nbsp;&nbsp;while (low <= high):<br/>&nbsp;&nbsp;&nbsp;&nbsp;mid = low + high // 2<br/>&nbsp;&nbsp;&nbsp;&nbsp;if arr[mid] == target:<br/>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;return mid<br/>&nbsp;&nbsp;&nbsp;&nbsp;elif target < arr[mid]:<br/>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;low = mid + 1<br/>&nbsp;&nbsp;&nbsp;&nbsp;else:<br/>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;high = mid - 1<br/>&nbsp;&nbsp;return -1 |\n| ä»Šå¤©å¤©æ°”æ€ä¹ˆæ ·ï¼ŒæŠŠè¿™å¥è¯ç¿»è¯‘æˆè‹±è¯­ | What's the weather like today? |\n| æ€ä¹ˆè®©è‡ªå·±ç²¾åŠ›å……æ²›ï¼Œåˆ—5ç‚¹å»ºè®® | ä»¥ä¸‹æ˜¯ä¸€äº›å¯ä»¥è®©ä½ ä¿æŒç²¾åŠ›å……æ²›çš„å»ºè®®ï¼š <br/>1. ç¡çœ å……è¶³ã€‚ç¡®ä¿æ¯å¤©æ™šä¸Šå¾—åˆ°å……è¶³çš„ç¡çœ æ—¶é—´ã€‚ <br/>2. å¤šè¿åŠ¨ã€‚è¿›è¡Œé€‚åº¦çš„æœ‰æ°§è¿åŠ¨ï¼Œå¦‚æ…¢è·‘ã€æ¸¸æ³³ç­‰ï¼Œå¯ä»¥å¸®åŠ©ä½ æé«˜èƒ½é‡å’Œè€åŠ›ã€‚<br/>3. åƒå¥åº·é£Ÿå“ã€‚é€‰æ‹©å¯Œå«è›‹ç™½è´¨çš„é£Ÿç‰©ï¼Œå¦‚é¸¡è‚‰ã€é±¼è‚‰å’Œé¸¡è›‹ï¼Œä»¥åŠæ–°é²œçš„æ°´æœå’Œè”¬èœã€‚ <br/>4. å‡å°‘å’–å•¡å› çš„æ‘„å…¥é‡ã€‚å°½é‡é¿å…åœ¨ç™½å¤©å–å«å’–å•¡å› çš„é¥®æ–™æˆ–åƒå«æœ‰å’–å•¡å› çš„é£Ÿç‰©ã€‚ <br/>5. æ”¾æ¾èº«å¿ƒã€‚å°è¯•å†¥æƒ³ã€ç‘œä¼½æˆ–å…¶ä»–å½¢å¼çš„æ”¾æ¾æ´»åŠ¨æ¥å‡è½»å‹åŠ›å’Œç„¦è™‘æ„Ÿã€‚ |\n| æ˜å¤©å°±å‡æœŸç»“æŸäº†ï¼Œæœ‰ç‚¹æŠ—æ‹’ä¸Šç­ï¼Œåº”è¯¥ä»€ä¹ˆåŠï¼Ÿ | é¢å¯¹å³å°†ç»“æŸçš„å‡æœŸå’Œå¼€å§‹å·¥ä½œæ—¶çš„ç´§å¼ æƒ…ç»ªï¼Œæ‚¨å¯ä»¥é‡‡å–ä»¥ä¸‹æªæ–½æ¥å¸®åŠ©æ‚¨æ›´å¥½åœ°é€‚åº”ï¼š <br/>1. ä¸ºè‡ªå·±åˆ¶å®šä¸€ä¸ªæ¸…æ™°çš„å·¥ä½œè®¡åˆ’ã€‚è¿™å¯ä»¥å¸®åŠ©æ‚¨æ›´å¥½åœ°ç»„ç»‡æ—¶é—´å’Œä»»åŠ¡ï¼Œå¹¶é¿å…åœ¨å·¥ä½œä¸­æ„Ÿåˆ°æ··ä¹±ã€‚ <br/>2. ä¸åŒäº‹æˆ–æœ‹å‹äº¤æµï¼Œåˆ†äº«æ‚¨çš„æ„Ÿå—å’Œå·¥ä½œå‹åŠ›ã€‚è¿™å°†æœ‰åŠ©äºç¼“è§£ä¸€äº›ç„¦è™‘æ„Ÿï¼ŒåŒæ—¶ä¹Ÿå¯ä»¥è·å¾—æ”¯æŒå’Œå»ºè®®ã€‚ <br/>3. åœ¨å·¥ä½œæ—¥å‰èŠ±äº›æ—¶é—´æ”¾æ¾èº«å¿ƒï¼Œä¾‹å¦‚è¿›è¡Œç‘œä¼½ã€å†¥æƒ³ç­‰æ´»åŠ¨ã€‚è¿™äº›æ–¹æ³•å¯ä»¥å‡è½»èº«ä½“å’Œç²¾ç¥ä¸Šçš„ç–²åŠ³æ„Ÿï¼Œä½¿æ‚¨æ›´åŠ ç²¾åŠ›å……æ²›åœ°è¿æ¥æŒ‘æˆ˜ã€‚ |\n| çˆ¶æ¯éƒ½å§“å´ï¼Œå–ä¸€äº›ç”·å®å®å’Œå¥³å®å®çš„åå­— | - ç”·å®ï¼šå´ä¿Šæ°ã€å´é›¨è¾°ã€å´å­å¢¨ã€å´æ€æºã€å´æ³½é˜³ã€å´å®‡èˆªã€å´åšæ–‡ã€å´å“²æ¯…ã€‚ <br/>- å¥³å®ï¼šå´ç¾çªã€å´ä½³æ€¡ã€å´æ¬£ç‘œã€å´é¦™å‡ã€å´é›ªæŸ”ã€å´é¢–è¯—ã€å´è¯­å«£ã€‚ |\n| æ¨èå‡ æœ¬é‡‘åº¸çš„æ­¦ä¾ å°è¯´ | ä»¥ä¸‹æ˜¯ä¸‰æœ¬æˆ‘æœ€å–œæ¬¢çš„é‡‘åº¸æ­¦ä¾ å°è¯´çš„æ¨èï¼š <br/>1. ã€Šå°„é›•è‹±é›„ä¼ ã€‹ - ä¸€éƒ¨å……æ»¡æ™ºæ…§å’Œæœºæ™ºçš„å°è¯´ï¼Œè®²è¿°äº†éƒ­é–ã€é»„è“‰å¤«å¦‡çš„æ•…äº‹ã€‚ <br/>2. ã€Šç¥é›•ä¾ ä¾£ã€‹- è¿™æ˜¯ä¸€éƒ¨æµªæ¼«è€Œæ„Ÿäººçš„æ•…äº‹ï¼Œè®²è¿°äº†ä¸€å¯¹æƒ…ä¾£å°é¾™å¥³ä¸æ¨è¿‡ä¹‹é—´çš„çˆ±æƒ…æ•…äº‹ã€‚ <br/>3. ã€Šå€šå¤©å± é¾™è®°ã€‹- è¿™æ˜¯ä¸€ä¸ªå®ä¼Ÿè€Œå£®è§‚çš„æ•…äº‹ï¼Œæç»˜äº†æ˜æ•™é¢†è¢–å¼ æ— å¿Œå¦‚ä½•æˆä¸ºä¸€ä½å‡ºè‰²çš„è‹±é›„ã€‚ |\n\n</details>\n\n<br/>\n\n\n## â›½ï¸ Contributing\n\nYou are welcomed to commit issues or contributig data/code.\nPlease refer to [How To Contribute](https://github.com/LianjiaTech/BELLE/blob/main/HOW_TO_CONTRIBUTE.md).\n\n## â˜ï¸ Contact Us\n\nDrop by and join with us at [Discord](https://discord.gg/pMPY53UUGq) or [WeChat](https://github.com/LianjiaTech/BELLE/blob/main/assets/belle_wechat.jpg)!\n"
        },
        {
          "name": "assets",
          "type": "tree",
          "content": null
        },
        {
          "name": "chat",
          "type": "tree",
          "content": null
        },
        {
          "name": "data",
          "type": "tree",
          "content": null
        },
        {
          "name": "docker",
          "type": "tree",
          "content": null
        },
        {
          "name": "docs",
          "type": "tree",
          "content": null
        },
        {
          "name": "eval",
          "type": "tree",
          "content": null
        },
        {
          "name": "models",
          "type": "tree",
          "content": null
        },
        {
          "name": "requirements.txt",
          "type": "blob",
          "size": 0.1103515625,
          "content": "numpy\nrouge_score\nfire\nopenai\ntransformers\ngensim\npeft\ndatasets\nbitsandbytes\ndeepspeed\nflash-attn\ntrl\naccelerate\n"
        },
        {
          "name": "train",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}