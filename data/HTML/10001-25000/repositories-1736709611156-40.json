{
  "metadata": {
    "timestamp": 1736709611156,
    "page": 40,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjQw",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "liguodongiot/llm-action",
      "stars": 12782,
      "defaultBranch": "main",
      "files": [
        {
          "name": ".DS_Store",
          "type": "blob",
          "size": 16.00390625,
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 3.005859375,
          "content": "# Byte-compiled / optimized / DLL files\n__pycache__/\n*.py[cod]\n*$py.class\n\n# C extensions\n*.so\n\n# Distribution / packaging\n.Python\nbuild/\ndevelop-eggs/\ndist/\ndownloads/\neggs/\n.eggs/\nlib/\nlib64/\nparts/\nsdist/\nvar/\nwheels/\nshare/python-wheels/\n*.egg-info/\n.installed.cfg\n*.egg\nMANIFEST\n\n# PyInstaller\n#  Usually these files are written by a python script from a template\n#  before PyInstaller builds the exe, so as to inject date/other infos into it.\n*.manifest\n*.spec\n\n# Installer logs\npip-log.txt\npip-delete-this-directory.txt\n\n# Unit test / coverage reports\nhtmlcov/\n.tox/\n.nox/\n.coverage\n.coverage.*\n.cache\nnosetests.xml\ncoverage.xml\n*.cover\n*.py,cover\n.hypothesis/\n.pytest_cache/\ncover/\n\n# Translations\n*.mo\n*.pot\n\n# Django stuff:\n*.log\nlocal_settings.py\ndb.sqlite3\ndb.sqlite3-journal\n\n# Flask stuff:\ninstance/\n.webassets-cache\n\n# Scrapy stuff:\n.scrapy\n\n# Sphinx documentation\ndocs/_build/\n\n# PyBuilder\n.pybuilder/\ntarget/\n\n# Jupyter Notebook\n.ipynb_checkpoints\n\n# IPython\nprofile_default/\nipython_config.py\n\n# pyenv\n#   For a library or package, you might want to ignore these files since the code is\n#   intended to run in multiple environments; otherwise, check them in:\n# .python-version\n\n# pipenv\n#   According to pypa/pipenv#598, it is recommended to include Pipfile.lock in version control.\n#   However, in case of collaboration, if having platform-specific dependencies or dependencies\n#   having no cross-platform support, pipenv may install dependencies that don't work, or not\n#   install all needed dependencies.\n#Pipfile.lock\n\n# poetry\n#   Similar to Pipfile.lock, it is generally recommended to include poetry.lock in version control.\n#   This is especially recommended for binary packages to ensure reproducibility, and is more\n#   commonly ignored for libraries.\n#   https://python-poetry.org/docs/basic-usage/#commit-your-poetrylock-file-to-version-control\n#poetry.lock\n\n# pdm\n#   Similar to Pipfile.lock, it is generally recommended to include pdm.lock in version control.\n#pdm.lock\n#   pdm stores project-wide configurations in .pdm.toml, but it is recommended to not include it\n#   in version control.\n#   https://pdm.fming.dev/#use-with-ide\n.pdm.toml\n\n# PEP 582; used by e.g. github.com/David-OConnor/pyflow and github.com/pdm-project/pdm\n__pypackages__/\n\n# Celery stuff\ncelerybeat-schedule\ncelerybeat.pid\n\n# SageMath parsed files\n*.sage.py\n\n# Environments\n.env\n.venv\nenv/\nvenv/\nENV/\nenv.bak/\nvenv.bak/\n\n# Spyder project settings\n.spyderproject\n.spyproject\n\n# Rope project settings\n.ropeproject\n\n# mkdocs documentation\n/site\n\n# mypy\n.mypy_cache/\n.dmypy.json\ndmypy.json\n\n# Pyre type checker\n.pyre/\n\n# pytype static type analyzer\n.pytype/\n\n# Cython debug symbols\ncython_debug/\n\n# PyCharm\n#  JetBrains specific template is maintained in a separate JetBrains.gitignore that can\n#  be found at https://github.com/github/gitignore/blob/main/Global/JetBrains.gitignore\n#  and can be added to the global gitignore or merged into this file.  For a more nuclear\n#  option (not recommended) you can uncomment the following to ignore the entire idea folder.\n#.idea/\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 11.0908203125,
          "content": "                                 Apache License\n                           Version 2.0, January 2004\n                        http://www.apache.org/licenses/\n\n   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n\n   1. Definitions.\n\n      \"License\" shall mean the terms and conditions for use, reproduction,\n      and distribution as defined by Sections 1 through 9 of this document.\n\n      \"Licensor\" shall mean the copyright owner or entity authorized by\n      the copyright owner that is granting the License.\n\n      \"Legal Entity\" shall mean the union of the acting entity and all\n      other entities that control, are controlled by, or are under common\n      control with that entity. For the purposes of this definition,\n      \"control\" means (i) the power, direct or indirect, to cause the\n      direction or management of such entity, whether by contract or\n      otherwise, or (ii) ownership of fifty percent (50%) or more of the\n      outstanding shares, or (iii) beneficial ownership of such entity.\n\n      \"You\" (or \"Your\") shall mean an individual or Legal Entity\n      exercising permissions granted by this License.\n\n      \"Source\" form shall mean the preferred form for making modifications,\n      including but not limited to software source code, documentation\n      source, and configuration files.\n\n      \"Object\" form shall mean any form resulting from mechanical\n      transformation or translation of a Source form, including but\n      not limited to compiled object code, generated documentation,\n      and conversions to other media types.\n\n      \"Work\" shall mean the work of authorship, whether in Source or\n      Object form, made available under the License, as indicated by a\n      copyright notice that is included in or attached to the work\n      (an example is provided in the Appendix below).\n\n      \"Derivative Works\" shall mean any work, whether in Source or Object\n      form, that is based on (or derived from) the Work and for which the\n      editorial revisions, annotations, elaborations, or other modifications\n      represent, as a whole, an original work of authorship. For the purposes\n      of this License, Derivative Works shall not include works that remain\n      separable from, or merely link (or bind by name) to the interfaces of,\n      the Work and Derivative Works thereof.\n\n      \"Contribution\" shall mean any work of authorship, including\n      the original version of the Work and any modifications or additions\n      to that Work or Derivative Works thereof, that is intentionally\n      submitted to Licensor for inclusion in the Work by the copyright owner\n      or by an individual or Legal Entity authorized to submit on behalf of\n      the copyright owner. For the purposes of this definition, \"submitted\"\n      means any form of electronic, verbal, or written communication sent\n      to the Licensor or its representatives, including but not limited to\n      communication on electronic mailing lists, source code control systems,\n      and issue tracking systems that are managed by, or on behalf of, the\n      Licensor for the purpose of discussing and improving the Work, but\n      excluding communication that is conspicuously marked or otherwise\n      designated in writing by the copyright owner as \"Not a Contribution.\"\n\n      \"Contributor\" shall mean Licensor and any individual or Legal Entity\n      on behalf of whom a Contribution has been received by Licensor and\n      subsequently incorporated within the Work.\n\n   2. Grant of Copyright License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      copyright license to reproduce, prepare Derivative Works of,\n      publicly display, publicly perform, sublicense, and distribute the\n      Work and such Derivative Works in Source or Object form.\n\n   3. Grant of Patent License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      (except as stated in this section) patent license to make, have made,\n      use, offer to sell, sell, import, and otherwise transfer the Work,\n      where such license applies only to those patent claims licensable\n      by such Contributor that are necessarily infringed by their\n      Contribution(s) alone or by combination of their Contribution(s)\n      with the Work to which such Contribution(s) was submitted. If You\n      institute patent litigation against any entity (including a\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\n      or a Contribution incorporated within the Work constitutes direct\n      or contributory patent infringement, then any patent licenses\n      granted to You under this License for that Work shall terminate\n      as of the date such litigation is filed.\n\n   4. Redistribution. You may reproduce and distribute copies of the\n      Work or Derivative Works thereof in any medium, with or without\n      modifications, and in Source or Object form, provided that You\n      meet the following conditions:\n\n      (a) You must give any other recipients of the Work or\n          Derivative Works a copy of this License; and\n\n      (b) You must cause any modified files to carry prominent notices\n          stating that You changed the files; and\n\n      (c) You must retain, in the Source form of any Derivative Works\n          that You distribute, all copyright, patent, trademark, and\n          attribution notices from the Source form of the Work,\n          excluding those notices that do not pertain to any part of\n          the Derivative Works; and\n\n      (d) If the Work includes a \"NOTICE\" text file as part of its\n          distribution, then any Derivative Works that You distribute must\n          include a readable copy of the attribution notices contained\n          within such NOTICE file, excluding those notices that do not\n          pertain to any part of the Derivative Works, in at least one\n          of the following places: within a NOTICE text file distributed\n          as part of the Derivative Works; within the Source form or\n          documentation, if provided along with the Derivative Works; or,\n          within a display generated by the Derivative Works, if and\n          wherever such third-party notices normally appear. The contents\n          of the NOTICE file are for informational purposes only and\n          do not modify the License. You may add Your own attribution\n          notices within Derivative Works that You distribute, alongside\n          or as an addendum to the NOTICE text from the Work, provided\n          that such additional attribution notices cannot be construed\n          as modifying the License.\n\n      You may add Your own copyright statement to Your modifications and\n      may provide additional or different license terms and conditions\n      for use, reproduction, or distribution of Your modifications, or\n      for any such Derivative Works as a whole, provided Your use,\n      reproduction, and distribution of the Work otherwise complies with\n      the conditions stated in this License.\n\n   5. Submission of Contributions. Unless You explicitly state otherwise,\n      any Contribution intentionally submitted for inclusion in the Work\n      by You to the Licensor shall be under the terms and conditions of\n      this License, without any additional terms or conditions.\n      Notwithstanding the above, nothing herein shall supersede or modify\n      the terms of any separate license agreement you may have executed\n      with Licensor regarding such Contributions.\n\n   6. Trademarks. This License does not grant permission to use the trade\n      names, trademarks, service marks, or product names of the Licensor,\n      except as required for reasonable and customary use in describing the\n      origin of the Work and reproducing the content of the NOTICE file.\n\n   7. Disclaimer of Warranty. Unless required by applicable law or\n      agreed to in writing, Licensor provides the Work (and each\n      Contributor provides its Contributions) on an \"AS IS\" BASIS,\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n      implied, including, without limitation, any warranties or conditions\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n      PARTICULAR PURPOSE. You are solely responsible for determining the\n      appropriateness of using or redistributing the Work and assume any\n      risks associated with Your exercise of permissions under this License.\n\n   8. Limitation of Liability. In no event and under no legal theory,\n      whether in tort (including negligence), contract, or otherwise,\n      unless required by applicable law (such as deliberate and grossly\n      negligent acts) or agreed to in writing, shall any Contributor be\n      liable to You for damages, including any direct, indirect, special,\n      incidental, or consequential damages of any character arising as a\n      result of this License or out of the use or inability to use the\n      Work (including but not limited to damages for loss of goodwill,\n      work stoppage, computer failure or malfunction, or any and all\n      other commercial damages or losses), even if such Contributor\n      has been advised of the possibility of such damages.\n\n   9. Accepting Warranty or Additional Liability. While redistributing\n      the Work or Derivative Works thereof, You may choose to offer,\n      and charge a fee for, acceptance of support, warranty, indemnity,\n      or other liability obligations and/or rights consistent with this\n      License. However, in accepting such obligations, You may act only\n      on Your own behalf and on Your sole responsibility, not on behalf\n      of any other Contributor, and only if You agree to indemnify,\n      defend, and hold each Contributor harmless for any liability\n      incurred by, or claims asserted against, such Contributor by reason\n      of your accepting any such warranty or additional liability.\n\n   END OF TERMS AND CONDITIONS\n\n   APPENDIX: How to apply the Apache License to your work.\n\n      To apply the Apache License to your work, attach the following\n      boilerplate notice, with the fields enclosed by brackets \"[]\"\n      replaced with your own identifying information. (Don't include\n      the brackets!)  The text should be enclosed in the appropriate\n      comment syntax for the file format. We also recommend that a\n      file or class name and description of purpose be included on the\n      same \"printed page\" as the copyright notice for easier\n      identification within third-party archives.\n\n   Copyright [yyyy] [name of copyright owner]\n\n   Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 38.5166015625,
          "content": "<p align=\"center\">\n  <img src=\"https://github.com/liguodongiot/llm-action/blob/main/pic/llm-action-v3.png\" >\n</p>\n\n\n<p> \n<a href=\"https://github.com/liguodongiot/llm-action/stargazers\">\n<img src=\"https://img.shields.io/github/stars/liguodongiot/llm-action?style=social\" > </a>\n<a href=\"https://github.com/liguodongiot/llm-action/blob/main/pic/wx.jpg\"> <img src=\"https://img.shields.io/badge/吃果冻不吐果冻皮-1AAD19.svg?style=plastic&logo=wechat&logoColor=white\" > </a>\n<a href=\"https://www.zhihu.com/people/liguodong-iot\"> <img src=\"https://img.shields.io/badge/吃果冻不吐果冻皮-0079FF.svg?style=plastic&logo=zhihu&logoColor=white\"> </a>\n<a href=\"https://juejin.cn/user/3642056016410728\"> <img src=\"https://img.shields.io/badge/掘金-吃果冻不吐果冻皮-000099.svg?style=plastic&logo=juejin\"> </a>\n<a href=\"https://liguodong.blog.csdn.net/\"> <img src=\"https://img.shields.io/badge/CSDN-吃果冻不吐果冻皮-6B238E.svg\"> </a>\n</p> \n\n\n## 目录\n\n- :snail: [LLM训练](#llm训练)\n  - 🐫 [LLM训练实战](#llm训练实战)\n  - 🐼 [LLM参数高效微调技术原理](#llm微调技术原理)\n  - 🐰 [LLM参数高效微调技术实战](#llm微调实战)\n  - 🐘 [LLM分布式训练并行技术](#llm分布式训练并行技术)\n  - 🌋 [分布式AI框架](#分布式ai框架)\n  - 📡 [分布式训练网络通信](#分布式训练网络通信)\n  - :herb: [LLM训练优化技术](#llm训练优化技术)\n  - :hourglass: [LLM对齐技术](#llm对齐技术)\n- 🐎 [LLM推理](#llm推理)\n  - 🚀 [LLM推理框架](#llm推理框架)\n  - ✈️ [LLM推理优化技术](#llm推理优化技术)\n- ♻️ [LLM压缩](#llm压缩)\n  - 📐 [LLM量化](#llm量化)\n  - 🔰 [LLM剪枝](#llm剪枝)\n  - 💹 [LLM知识蒸馏](#llm知识蒸馏)\n  - ♑️ [低秩分解](#低秩分解)\n- :herb: [LLM测评](#llm测评)\n- :palm_tree: [LLM数据工程](#llm数据工程)\n  - :dolphin: [LLM微调高效数据筛选技术](#llm微调高效数据筛选技术)\n- :cyclone: [提示工程](#提示工程)\n- ♍️ [LLM算法架构](#llm算法架构)\n- :jigsaw: [LLM应用开发](#llm应用开发)\n- 🀄️ [LLM国产化适配](#llm国产化适配)\n- 🔯 [AI编译器](#ai编译器)\n- 🔘 [AI基础设施](#ai基础设施)\n  - :maple_leaf: [AI加速卡](#ai加速卡)\n  - :octocat: [AI集群网络通信](#ai集群网络通信)\n- 💟 [LLMOps](#llmops)\n- 🍄 [LLM生态相关技术](#llm生态相关技术)\n- :dizzy: [LLM面试题](#llm面试题)\n- 🔨 [服务器基础环境安装及常用工具](#服务器基础环境安装及常用工具)\n- 💬 [LLM学习交流群](#llm学习交流群)\n- 👥 [微信公众号](#微信公众号)\n- ⭐️ [Star History](#star-history)\n- :link: [AI工程化课程推荐](#ai工程化课程推荐)\n\n\n## LLM训练\n\n### LLM训练实战\n\n下面汇总了我在大模型实践中训练相关的所有教程。从6B到65B，从全量微调到高效微调（LoRA，QLoRA，P-Tuning v2），再到RLHF（基于人工反馈的强化学习）。\n\n| LLM                         | 预训练/SFT/RLHF...            | 参数     | 教程                                                                                                                                                                                                                     | 代码                                                                                     |\n| --------------------------- | ----------------------------- | -------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | ---------------------------------------------------------------------------------------- |\n| Alpaca                      | full fine-turning             | 7B       | [从0到1复现斯坦福羊驼（Stanford Alpaca 7B）](https://zhuanlan.zhihu.com/p/618321077)                                                                                                                                        | [配套代码](https://github.com/liguodongiot/llm-action/tree/main/llm-train/alpaca)               |\n| Alpaca(LLaMA)               | LoRA                          | 7B~65B   | 1.[足够惊艳，使用Alpaca-Lora基于LLaMA(7B)二十分钟完成微调，效果比肩斯坦福羊驼](https://zhuanlan.zhihu.com/p/619426866)<br>2. [使用 LoRA 技术对 LLaMA 65B 大模型进行微调及推理](https://zhuanlan.zhihu.com/p/632492604)    | [配套代码](https://github.com/liguodongiot/llm-action/tree/main/llm-train/alpaca-lora)          |\n| BELLE(LLaMA/Bloom)          | full fine-turning             | 7B       | 1.[基于LLaMA-7B/Bloomz-7B1-mt复现开源中文对话大模型BELLE及GPTQ量化](https://zhuanlan.zhihu.com/p/618876472) <br> 2. [BELLE(LLaMA-7B/Bloomz-7B1-mt)大模型使用GPTQ量化后推理性能测试](https://zhuanlan.zhihu.com/p/621128368) | N/A                                                                                      |\n| ChatGLM                     | LoRA                          | 6B       | [从0到1基于ChatGLM-6B使用LoRA进行参数高效微调](https://zhuanlan.zhihu.com/p/621793987)                                                                                                                                      | [配套代码](https://github.com/liguodongiot/llm-action/tree/main/train/chatglm-lora)         |\n| ChatGLM                     | full fine-turning/P-Tuning v2 | 6B       | [使用DeepSpeed/P-Tuning v2对ChatGLM-6B进行微调](https://zhuanlan.zhihu.com/p/622351059)                                                                                                                                     | [配套代码](https://github.com/liguodongiot/llm-action/tree/main/train/chatglm)              |\n| Vicuna(LLaMA)               | full fine-turning             | 7B       | [大模型也内卷，Vicuna训练及推理指南，效果碾压斯坦福羊驼](https://zhuanlan.zhihu.com/p/624012908)                                                                                                                            | N/A                                                                                      |\n| OPT                         | RLHF                          | 0.1B~66B | 1.[一键式 RLHF 训练 DeepSpeed Chat（一）：理论篇](https://zhuanlan.zhihu.com/p/626159553) <br> 2. [一键式 RLHF 训练 DeepSpeed Chat（二）：实践篇](https://zhuanlan.zhihu.com/p/626214655)                                 | [配套代码](https://github.com/liguodongiot/llm-action/tree/main/train/deepspeedchat)        |\n| MiniGPT-4(LLaMA)            | full fine-turning             | 7B       | [大杀器，多模态大模型MiniGPT-4入坑指南](https://zhuanlan.zhihu.com/p/627671257)                                                                                                                                             | N/A                                                                                      |\n| Chinese-LLaMA-Alpaca(LLaMA) | LoRA（预训练+微调）           | 7B       | [中文LLaMA&amp;Alpaca大语言模型词表扩充+预训练+指令精调](https://zhuanlan.zhihu.com/p/631360711)                                                                                                                            | [配套代码](https://github.com/liguodongiot/llm-action/tree/main/train/chinese-llama-alpaca) |\n| LLaMA                       | QLoRA                         | 7B/65B   | [高效微调技术QLoRA实战，基于LLaMA-65B微调仅需48G显存，真香](https://zhuanlan.zhihu.com/p/636644164)                                                                                                                         | [配套代码](https://github.com/liguodongiot/llm-action/tree/main/train/qlora)                |\n| LLaMA                       | GaLore                         | 60M/7B   | [突破内存瓶颈，使用 GaLore 一张4090消费级显卡也能预训练LLaMA-7B](https://zhuanlan.zhihu.com/p/686686751)   | [配套代码](https://github.com/liguodongiot/llm-action/blob/main/train/galore/torchrun_main.py)  |\n\n**[⬆ 一键返回目录](#目录)**\n\n### LLM微调技术原理\n\n对于普通大众来说，进行大模型的预训练或者全量微调遥不可及。由此，催生了各种参数高效微调技术，让科研人员或者普通开发者有机会尝试微调大模型。\n\n因此，该技术值得我们进行深入分析其背后的机理，本系列大体分七篇文章进行讲解。\n\n![peft方法](./pic/llm/train/sft/peft方法.jpg)\n\n\n- [大模型参数高效微调技术原理综述（一）-背景、参数高效微调简介](https://zhuanlan.zhihu.com/p/635152813)\n- [大模型参数高效微调技术原理综述（二）-BitFit、Prefix Tuning、Prompt Tuning](https://zhuanlan.zhihu.com/p/635686756)\n- [大模型参数高效微调技术原理综述（三）-P-Tuning、P-Tuning v2](https://zhuanlan.zhihu.com/p/635848732)\n- [大模型参数高效微调技术原理综述（四）-Adapter Tuning及其变体](https://zhuanlan.zhihu.com/p/636038478)\n- [大模型参数高效微调技术原理综述（五）-LoRA、AdaLoRA、QLoRA](https://zhuanlan.zhihu.com/p/636215898)\n- [大模型参数高效微调技术原理综述（六）-MAM Adapter、UniPELT](https://zhuanlan.zhihu.com/p/636362246)\n- [大模型参数高效微调技术原理综述（七）-最佳实践、总结](https://zhuanlan.zhihu.com/p/649755252)\n\n### LLM微调实战\n\n下面给大家分享**大模型参数高效微调技术实战**，该系列主要针对 HuggingFace PEFT 框架支持的一些高效微调技术进行讲解。\n\n| 教程                                                                                                | 代码                                                                                                      | 框架             |\n| --------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------- | ---------------- |\n| [大模型参数高效微调技术实战（一）-PEFT概述及环境搭建](https://zhuanlan.zhihu.com/p/651744834)          | N/A                                                                                                       | HuggingFace PEFT |\n| [大模型参数高效微调技术实战（二）-Prompt Tuning](https://zhuanlan.zhihu.com/p/646748939)               | [配套代码](https://github.com/liguodongiot/llm-action/blob/main/llm-action/peft/clm/peft_prompt_tuning_clm.ipynb) | HuggingFace PEFT |\n| [大模型参数高效微调技术实战（三）-P-Tuning](https://zhuanlan.zhihu.com/p/646876256)                    | [配套代码](https://github.com/liguodongiot/llm-action/blob/main/llm-action/peft/clm/peft_p_tuning_clm.ipynb)      | HuggingFace PEFT |\n| [大模型参数高效微调技术实战（四）-Prefix Tuning / P-Tuning v2](https://zhuanlan.zhihu.com/p/648156780) | [配套代码](https://github.com/liguodongiot/llm-action/blob/main/llm-action/peft/clm/peft_p_tuning_v2_clm.ipynb)   | HuggingFace PEFT |\n| [大模型参数高效微调技术实战（五）-LoRA](https://zhuanlan.zhihu.com/p/649315197)                        | [配套代码](https://github.com/liguodongiot/llm-action/blob/main/llm-action/peft/clm/peft_lora_clm.ipynb)          | HuggingFace PEFT |\n| [大模型参数高效微调技术实战（六）-IA3](https://zhuanlan.zhihu.com/p/649707359)                         | [配套代码](https://github.com/liguodongiot/llm-action/blob/main/llm-action/peft/clm/peft_ia3_clm.ipynb)           | HuggingFace PEFT |\n| [大模型微调实战（七）-基于LoRA微调多模态大模型](https://zhuanlan.zhihu.com/p/670048482)       |     [配套代码](https://github.com/liguodongiot/llm-action/blob/main/llm-action/peft/multimodal/blip2_lora_int8_fine_tune.py) | HuggingFace PEFT |\n| [大模型微调实战（八）-使用INT8/FP4/NF4微调大模型](https://zhuanlan.zhihu.com/p/670116171)    |     [配套代码](https://github.com/liguodongiot/llm-action/blob/main/llm-action/peft/multimodal/finetune_bloom_bnb_peft.ipynb) | PEFT、bitsandbytes |\n\n\n\n\n**[⬆ 一键返回目录](#目录)**\n\n### [LLM分布式训练并行技术](https://github.com/liguodongiot/llm-action/tree/main/docs/llm-base/distribution-parallelism)\n\n近年来，随着Transformer、MOE架构的提出，使得深度学习模型轻松突破上万亿规模参数，传统的单机单卡模式已经无法满足超大模型进行训练的要求。因此，我们需要基于单机多卡、甚至是多机多卡进行分布式大模型的训练。\n\n而利用AI集群，使深度学习算法更好地从大量数据中高效地训练出性能优良的大模型是分布式机器学习的首要目标。为了实现该目标，一般需要根据硬件资源与数据/模型规模的匹配情况，考虑对计算任务、训练数据和模型进行划分，从而进行分布式训练。因此，分布式训练相关技术值得我们进行深入分析其背后的机理。\n\n下面主要对大模型进行分布式训练的并行技术进行讲解，本系列大体分九篇文章进行讲解。\n\n- [大模型分布式训练并行技术（一）-概述](https://zhuanlan.zhihu.com/p/598714869)\n- [大模型分布式训练并行技术（二）-数据并行](https://zhuanlan.zhihu.com/p/650002268)\n- [大模型分布式训练并行技术（三）-流水线并行](https://zhuanlan.zhihu.com/p/653860567)\n- [大模型分布式训练并行技术（四）-张量并行](https://zhuanlan.zhihu.com/p/657921100)\n- [大模型分布式训练并行技术（五）-序列并行](https://zhuanlan.zhihu.com/p/659792351)\n- [大模型分布式训练并行技术（六）-多维混合并行](https://zhuanlan.zhihu.com/p/661279318)\n- [大模型分布式训练并行技术（七）-自动并行](https://zhuanlan.zhihu.com/p/662517647)\n- [大模型分布式训练并行技术（八）-MOE并行](https://zhuanlan.zhihu.com/p/662518387)\n- [大模型分布式训练并行技术（九）-总结](https://zhuanlan.zhihu.com/p/667051845)\n\n**[⬆ 一键返回目录](#目录)**\n\n### 分布式AI框架\n\n- [PyTorch](https://github.com/liguodongiot/llm-action/tree/main/train/pytorch/)\n  - PyTorch 单机多卡训练\n  - PyTorch 多机多卡训练\n- [Megatron-LM](https://github.com/liguodongiot/llm-action/tree/main/train/megatron)\n  - Megatron-LM 单机多卡训练\n  - Megatron-LM 多机多卡训练\n  - [基于Megatron-LM从0到1完成GPT2模型预训练、模型评估及推理](https://juejin.cn/post/7259682893648724029)\n- [DeepSpeed](https://github.com/liguodongiot/llm-action/tree/main/train/deepspeed)\n  - DeepSpeed 单机多卡训练\n  - DeepSpeed 多机多卡训练\n- [Megatron-DeepSpeed](https://github.com/liguodongiot/llm-action/tree/main/train/megatron-deepspeed)\n  - 基于 Megatron-DeepSpeed 从 0 到1 完成 LLaMA 预训练\n  - 基于 Megatron-DeepSpeed 从 0 到1 完成 Bloom 预训练\n\n\n### 分布式训练网络通信\n\n待更新...\n\n\n### LLM训练优化技术\n\n- FlashAttention V1、V2\n- 混合精度训练\n- 重计算\n- MQA / GQA\n- 梯度累积\n\n\n### LLM对齐技术\n\n\n- PPO（近端策略优化）\n- DPO\n- ORPO\n\n\n\n**[⬆ 一键返回目录](#目录)**\n\n## [LLM推理](https://github.com/liguodongiot/llm-action/tree/main/inference)\n\n\n### 模型推理引擎\n\n- [大模型推理框架概述](https://www.zhihu.com/question/625415776/answer/3243562246)\n- [大模型的好伙伴，浅析推理加速引擎FasterTransformer](https://zhuanlan.zhihu.com/p/626008090)\n- [TensorRT-LLM保姆级教程（一）-快速入门](https://zhuanlan.zhihu.com/p/666849728)\n- [TensorRT-LLM保姆级教程（二）-离线环境搭建、模型量化及推理](https://zhuanlan.zhihu.com/p/667572720)\n- [TensorRT-LLM保姆级教程（三）-使用Triton推理服务框架部署模型](https://juejin.cn/post/7398122968200593419)\n- TensorRT-LLM保姆级教程（四）-新模型适配\n- vLLM\n- [LightLLM](https://github.com/ModelTC/lightllm)：纯python开发的大语言模型推理和服务框架\n- [MNN-LLM](https://github.com/alibaba/MNN)：基于MNN引擎开发的大型语言模型运行时解决方案\n\n\n### 模型推理服务\n\n- [模型推理服务工具综述](https://zhuanlan.zhihu.com/p/721395381)\n- [模型推理服务化框架Triton保姆式教程（一）：快速入门](https://zhuanlan.zhihu.com/p/629336492)\n- [模型推理服务化框架Triton保姆式教程（二）：架构解析](https://zhuanlan.zhihu.com/p/634143650)\n- [模型推理服务化框架Triton保姆式教程（三）：开发实践](https://zhuanlan.zhihu.com/p/634444666)\n\n\n### LLM推理优化技术\n\n- [LLM推理优化技术-概述]()\n- [大模型推理优化技术-KV Cache](https://www.zhihu.com/question/653658936/answer/3569365986)\n- [大模型推理服务调度优化技术-Continuous batching](https://zhuanlan.zhihu.com/p/719610083)\n- [大模型低显存推理优化-Offload技术](https://juejin.cn/post/7405158045628596224)\n- [大模型推理优化技术-KV Cache量化](https://juejin.cn/post/7420231738558627874)\n- [大模型推理服务调度优化技术-Chunked Prefill]()\n- [大模型推理优化技术-KV Cache优化方法综述]()\n- 大模型吞吐优化技术-多LoRA推理服务\n- 大模型推理服务调度优化技术-公平性调度\n- 大模型访存优化技术-FlashAttention\n- 大模型显存优化技术-PagedAttention\n- 大模型解码优化-Speculative Decoding及其变体\n- 大模型推理优化-结构化文本生成\n- Flash Decoding\n- FlashDecoding++\n\n\n## LLM压缩\n\n近年来，随着Transformer、MOE架构的提出，使得深度学习模型轻松突破上万亿规模参数，从而导致模型变得越来越大，因此，我们需要一些大模型压缩技术来降低模型部署的成本，并提升模型的推理性能。\n模型压缩主要分为如下几类：\n\n-   模型剪枝（Pruning）\n-   知识蒸馏（Knowledge Distillation）\n-   模型量化（Quantization）\n-   低秩分解（Low-Rank Factorization）\n\n### [LLM量化](https://github.com/liguodongiot/llm-action/tree/main/model-compression/quantization)\n\n本系列将针对一些常见大模型量化方案（GPTQ、LLM.int8()、SmoothQuant、AWQ等）进行讲述。\n\n- [大模型量化概述](https://www.zhihu.com/question/627484732/answer/3261671478)\n- 量化感知训练：\n    - [大模型量化感知训练技术原理：LLM-QAT](https://zhuanlan.zhihu.com/p/647589650)\n    - [大模型量化感知微调技术原理：QLoRA]()\n    - PEQA\n- 训练后量化：\n    - [大模型量化技术原理：GPTQ、LLM.int8()](https://zhuanlan.zhihu.com/p/680212402)\n    - [大模型量化技术原理：SmoothQuant](https://www.zhihu.com/question/576376372/answer/3388402085)\n    - [大模型量化技术原理：AWQ、AutoAWQ](https://zhuanlan.zhihu.com/p/681578090)\n    - [大模型量化技术原理：SpQR](https://zhuanlan.zhihu.com/p/682871823)\n    - [大模型量化技术原理：ZeroQuant系列](https://zhuanlan.zhihu.com/p/683813769)\n    - [大模型量化技术原理：FP8](https://www.zhihu.com/question/658712811/answer/3596678896)\n    - [大模型量化技术原理：FP6](https://juejin.cn/post/7412893752090853386)\n    - [大模型量化技术原理：KIVI、IntactKV、KVQuant](https://juejin.cn/post/7420231738558627874)\n    - [大模型量化技术原理：Atom、QuaRot](https://juejin.cn/post/7424334647570513972)\n    - [大模型量化技术原理：QoQ量化及QServe推理服务系统](https://zhuanlan.zhihu.com/p/8047106486)\n    - 大模型量化技术原理：QuIP、QuIP#、OmniQuant\n    - [大模型量化技术原理：FP4]()\n- [大模型量化技术原理：总结](https://zhuanlan.zhihu.com/p/11886909512)\n\n\n\n### LLM剪枝\n\n- [万字长文谈深度神经网络剪枝综述](https://zhuanlan.zhihu.com/p/692858636?)\n\n\n目前，大多数针对大模型模型的压缩技术都专注于模型量化领域，即降低单个权重的数值表示的精度。另一种模型压缩方法模型剪枝的研究相对较少，即删除网络元素，包括从单个权重（非结构化剪枝）到更高粒度的组件，如权重矩阵的整行/列（结构化剪枝）。\n\n本系列将针对一些常见大模型剪枝方案（LLM-Pruner、SliceGPT、SparseGPT、Wanda等）进行讲述。\n\n- [大模型剪枝技术原理：概述](https://www.zhihu.com/question/652126515/answer/3457652467)\n- [大模型剪枝技术原理：LLM-Pruner、SliceGPT]()\n- [大模型剪枝技术原理：SparseGPT、Wanda]()\n- [大模型剪枝技术原理：总结]()\n\n\n**结构化剪枝**：\n\n- LLM-Pruner(LLM-Pruner: On the Structural Pruning of Large Language Models)\n- LLM-Shearing(Sheared LLaMA: Accelerating Language Model Pre-training via Structured Pruning)\n- SliceGPT: Compress Large Language Models by Deleting Rows and Columns\n- LoSparse\n\n\n**非结构化剪枝**：\n\n- SparseGPT(SparseGPT: Massive Language Models Can be Accurately Pruned in One-Shot)\n- LoRAPrune(LoRAPrune: Pruning Meets Low-Rank Parameter-Efficient Fine-Tuning)\n- Wanda(A Simple and Effective Pruning Approach for Large Language Models)\n- Flash-LLM(Flash-LLM: Enabling Cost-Effective and Highly-Efficient Large Generative Model Inference with Unstructured Sparsity)\n\n\n\n### LLM知识蒸馏\n\n- [大模型知识蒸馏概述](https://www.zhihu.com/question/625415893/answer/3243565375)\n\n**Standard KD**:\n\n使学生模型学习教师模型(LLM)所拥有的常见知识，如输出分布和特征信息，这种方法类似于传统的KD。\n\n- MINILLM\n- GKD\n\n**EA-based KD**:\n\n不仅仅是将LLM的常见知识转移到学生模型中，还涵盖了蒸馏它们独特的涌现能力。具体来说，EA-based KD又分为了上下文学习（ICL）、思维链（CoT）和指令跟随（IF）。\n\nIn-Context Learning：\n\n- In-Context Learning distillation\n\nChain-of-Thought：\n\n- MT-COT\n- Fine-tune-CoT\n- DISCO\n- SCOTT\n- SOCRATIC CoT\n\nInstruction Following：\n\n- Lion\n\n### 低秩分解\n\n低秩分解旨在通过将给定的权重矩阵分解成两个或多个较小维度的矩阵，从而对其进行近似。低秩分解背后的核心思想是找到一个大的权重矩阵W的分解，得到两个矩阵U和V，使得W≈U V，其中U是一个m×k矩阵，V是一个k×n矩阵，其中k远小于m和n。U和V的乘积近似于原始的权重矩阵，从而大幅减少了参数数量和计算开销。\n\n在LLM研究的模型压缩领域，研究人员通常将多种技术与低秩分解相结合，包括修剪、量化等。\n\n- ZeroQuant-FP（低秩分解+量化）\n- LoRAPrune（低秩分解+剪枝）\n\n\n\n## LLM测评\n\n- [C-Eval](https://github.com/liguodongiot/ceval)：全面的中文基础模型评估套件，涵盖了52个不同学科的13948个多项选择题，分为四个难度级别。\n- [CMMLU](https://github.com/liguodongiot/CMMLU)：一个综合性的中文评估基准，专门用于评估语言模型在中文语境下的知识和推理能力。CMMLU涵盖了从基础学科到高级专业水平的67个主题。它包括：需要计算和推理的自然科学，需要知识的人文科学和社会科学,以及需要生活常识的中国驾驶规则等。此外，CMMLU中的许多任务具有中国特定的答案，可能在其他地区或语言中并不普遍适用。因此是一个完全中国化的中文测试基准。\n- [IFEval: Instruction Following Eval](https://github.com/google-research/google-research/tree/master/instruction_following_eval)/[Paper](https://arxiv.org/abs/2311.07911)：专注评估大模型遵循指令的能力,包含关键词检测、标点控制、输出格式要求等25种任务。\n- [SuperCLUE](https://github.com/CLUEbenchmark/SuperCLUE)：一个综合性大模型评测基准，本次评测主要聚焦于大模型的四个能力象限，包括语言理解与生成、专业技能与知识、Agent智能体和安全性，进而细化为12项基础能力。\n- [AGIEval](https://github.com/ruixiangcui/AGIEval/)：用于评估基础模型在与人类认知和解决问题相关的任务中的能力。该基准源自 20 项面向普通考生的官方、公开、高标准的入学和资格考试，例如：普通大学入学考试（例如：中国高考（Gaokao）和美国 SAT）、法学院入学考试、数学竞赛、律师资格考试、国家公务员考试。\n- [OpenCompass](https://github.com/open-compass/opencompass/blob/main/README_zh-CN.md)：司南 2.0 大模型评测体系。支持的数据集如下：\n\n<table align=\"center\">\n  <tbody>\n    <tr align=\"center\" valign=\"bottom\">\n      <td>\n        <b>语言</b>\n      </td>\n      <td>\n        <b>知识</b>\n      </td>\n      <td>\n        <b>推理</b>\n      </td>\n      <td>\n        <b>考试</b>\n      </td>\n    </tr>\n    <tr valign=\"top\">\n      <td>\n<details open>\n<summary><b>字词释义</b></summary>\n\n- WiC\n- SummEdits\n\n</details>\n\n<details open>\n<summary><b>成语习语</b></summary>\n\n- CHID\n\n</details>\n\n<details open>\n<summary><b>语义相似度</b></summary>\n\n- AFQMC\n- BUSTM\n\n</details>\n\n<details open>\n<summary><b>指代消解</b></summary>\n\n- CLUEWSC\n- WSC\n- WinoGrande\n\n</details>\n\n<details open>\n<summary><b>翻译</b></summary>\n\n- Flores\n- IWSLT2017\n\n</details>\n\n<details open>\n<summary><b>多语种问答</b></summary>\n\n- TyDi-QA\n- XCOPA\n\n</details>\n\n<details open>\n<summary><b>多语种总结</b></summary>\n\n- XLSum\n\n</details>\n      </td>\n      <td>\n<details open>\n<summary><b>知识问答</b></summary>\n\n- BoolQ\n- CommonSenseQA\n- NaturalQuestions\n- TriviaQA\n\n</details>\n      </td>\n      <td>\n<details open>\n<summary><b>文本蕴含</b></summary>\n\n- CMNLI\n- OCNLI\n- OCNLI_FC\n- AX-b\n- AX-g\n- CB\n- RTE\n- ANLI\n\n</details>\n\n<details open>\n<summary><b>常识推理</b></summary>\n\n- StoryCloze\n- COPA\n- ReCoRD\n- HellaSwag\n- PIQA\n- SIQA\n\n</details>\n\n<details open>\n<summary><b>数学推理</b></summary>\n\n- MATH\n- GSM8K\n\n</details>\n\n<details open>\n<summary><b>定理应用</b></summary>\n\n- TheoremQA\n- StrategyQA\n- SciBench\n\n</details>\n\n<details open>\n<summary><b>综合推理</b></summary>\n\n- BBH\n\n</details>\n      </td>\n      <td>\n<details open>\n<summary><b>初中/高中/大学/职业考试</b></summary>\n\n- C-Eval\n- AGIEval\n- MMLU\n- GAOKAO-Bench\n- CMMLU\n- ARC\n- Xiezhi\n\n</details>\n\n<details open>\n<summary><b>医学考试</b></summary>\n\n- CMB\n\n</details>\n      </td>\n    </tr>\n</td>\n    </tr>\n  </tbody>\n  <tbody>\n    <tr align=\"center\" valign=\"bottom\">\n      <td>\n        <b>理解</b>\n      </td>\n      <td>\n        <b>长文本</b>\n      </td>\n      <td>\n        <b>安全</b>\n      </td>\n      <td>\n        <b>代码</b>\n      </td>\n    </tr>\n    <tr valign=\"top\">\n      <td>\n<details open>\n<summary><b>阅读理解</b></summary>\n\n- C3\n- CMRC\n- DRCD\n- MultiRC\n- RACE\n- DROP\n- OpenBookQA\n- SQuAD2.0\n\n</details>\n\n<details open>\n<summary><b>内容总结</b></summary>\n\n- CSL\n- LCSTS\n- XSum\n- SummScreen\n\n</details>\n\n<details open>\n<summary><b>内容分析</b></summary>\n\n- EPRSTMT\n- LAMBADA\n- TNEWS\n\n</details>\n      </td>\n      <td>\n<details open>\n<summary><b>长文本理解</b></summary>\n\n- LEval\n- LongBench\n- GovReports\n- NarrativeQA\n- Qasper\n\n</details>\n      </td>\n      <td>\n<details open>\n<summary><b>安全</b></summary>\n\n- CivilComments\n- CrowsPairs\n- CValues\n- JigsawMultilingual\n- TruthfulQA\n\n</details>\n<details open>\n<summary><b>健壮性</b></summary>\n\n- AdvGLUE\n\n</details>\n      </td>\n      <td>\n<details open>\n<summary><b>代码</b></summary>\n\n- HumanEval\n- HumanEvalX\n- MBPP\n- APPs\n- DS1000\n\n</details>\n      </td>\n    </tr>\n</td>\n    </tr>\n  </tbody>\n</table>\n\n\n\n## LLM数据工程\n\nLLM Data Engineering\n\n\n### 预训练语料处理技术\n\n![llm-pretrain-pipeline](./pic/llm/train/pretrain/llm-pretrain-pipeline-v2.png)\n\n- 数据收集\n- 数据处理\n  - 去重\n  - 过滤\n  - 选择\n  - 组合\n\n### LLM微调高效数据筛选技术\n\n- [LLM微调高效数据筛选技术原理-DEITA]()\n- [LLM微调高效数据筛选技术原理-MoDS]()\n- [LLM微调高效数据筛选技术原理-IFD]()\n- [LLM微调高效数据筛选技术原理-CaR]()\n- [LESS：仅选择5%有影响力的数据优于全量数据集进行目标指令微调](https://zhuanlan.zhihu.com/p/686007325)\n- [LESS 实践：用少量的数据进行目标指令微调](https://zhuanlan.zhihu.com/p/686687923)\n\n\n\n## 提示工程\n\n- Zero-Shot Prompting\n- Few-Shot Prompting\n- Chain-of-Thought (CoT) Prompting\n- Automatic Chain-of-Thought (Auto-CoT) Prompting\n- Tree-of-Thoughts (ToT) Prompting\n\n\n\n## [LLM算法架构](https://github.com/liguodongiot/llm-action/tree/main/docs/llm-base/ai-algo)\n\n![llm-famliy](./pic/llm/model/llm-famliy.jpg)\n\n\n- [大模型算法演进](https://zhuanlan.zhihu.com/p/600016134)\n\n![llm-famliy](./pic/llm/model/llm-timeline-v2.png)\n\n- ChatGLM / ChatGLM2 / ChatGLM3 大模型解析\n- Bloom 大模型解析\n- LLaMA / LLaMA2 大模型解析\n- [百川智能开源大模型baichuan-7B技术剖析](https://www.zhihu.com/question/606757218/answer/3075464500)\n- [百川智能开源大模型baichuan-13B技术剖析](https://www.zhihu.com/question/611507751/answer/3114988669)\n- [LLaMA3 技术剖析](https://www.zhihu.com/question/653374932/answer/3470909634)\n- QWen 大模型剖析\n\n\n## LLM应用开发\n\n大模型是基座，要想让其变成一款产品，我们还需要一些其他相关的技术，比如：向量数据库（Pinecone、Milvus、Vespa、Weaviate），LangChain等。\n\n- [云原生向量数据库Milvus（一）-简述、系统架构及应用场景](https://zhuanlan.zhihu.com/p/476025527)\n- [云原生向量数据库Milvus（二）-数据与索引的处理流程、索引类型及Schema](https://zhuanlan.zhihu.com/p/477231485)\n- [关于大模型驱动的AI智能体Agent的一些思考](https://zhuanlan.zhihu.com/p/651921120)\n\n\n\n## [LLM国产化适配](https://github.com/liguodongiot/llm-action/tree/main/docs/llm_localization)\n\n随着 ChatGPT 的现象级走红，引领了AI大模型时代的变革，从而导致 AI 算力日益紧缺。与此同时，中美贸易战以及美国对华进行AI芯片相关的制裁导致 AI 算力的国产化适配势在必行。本系列将对一些国产化 AI 加速卡进行讲解。\n\n- [大模型国产化适配1-华为昇腾AI全栈软硬件平台总结](https://zhuanlan.zhihu.com/p/637918406)\n- [大模型国产化适配2-基于昇腾910使用ChatGLM-6B进行模型推理](https://zhuanlan.zhihu.com/p/650730807)\n- [大模型国产化适配3-基于昇腾910使用ChatGLM-6B进行模型训练](https://zhuanlan.zhihu.com/p/651324599)\n  - MindRecord数据格式说明、全量微调、LoRA微调\n- [大模型国产化适配4-基于昇腾910使用LLaMA-13B进行多机多卡训练](https://zhuanlan.zhihu.com/p/655902796)\n- [大模型国产化适配5-百度飞浆PaddleNLP大语言模型工具链总结](https://zhuanlan.zhihu.com/p/665807431)\n- [大模型国产化适配6-基于昇腾910B快速验证ChatGLM3-6B/BaiChuan2-7B模型推理](https://zhuanlan.zhihu.com/p/677799157)\n- [大模型国产化适配7-华为昇腾LLM落地可选解决方案（MindFormers、ModelLink、MindIE）](https://zhuanlan.zhihu.com/p/692377206)\n- [MindIE 1.0.RC1 发布，华为昇腾终于推出了针对LLM的完整部署方案，结束小米加步枪时代](https://www.zhihu.com/question/654472145/answer/3482521709)\n- [大模型国产化适配8-基于昇腾MindIE推理工具部署Qwen-72B实战（推理引擎、推理服务化）](https://juejin.cn/post/7365879319598727180)\n  - Qwen-72B、Baichuan2-7B、ChatGLM3-6B\n- [大模型国产化适配9-LLM推理框架MindIE-Service性能基准测试](https://zhuanlan.zhihu.com/p/704649189)\n- [大模型国产化适配10-快速迁移大模型到昇腾910B保姆级教程（Pytorch版）](https://juejin.cn/post/7375351908896866323)\n- [大模型国产化适配11-LLM训练性能基准测试（昇腾910B3）](https://juejin.cn/post/7380995631790964772)\n- [国产知名AI芯片厂商产品大揭秘-昇腾、海光、天数智芯...](https://f46522gm22.feishu.cn/docx/PfWfdMKo8oXYN6xi7uycuhgFnKg)\n- [国内AI芯片厂商的计算平台大揭秘-昇腾、海光、天数智芯...](https://f46522gm22.feishu.cn/docx/XnhcdXVDholUBpxYoMccS11Mnfc)\n- [【LLM国产化】量化技术在MindIE推理框架中的应用](https://juejin.cn/post/7416723051377377316)\n\n\n\n\n**[⬆ 一键返回目录](#目录)**\n\n\n## [AI编译器](https://github.com/liguodongiot/llm-action/tree/main/ai-compiler)\n\nAI编译器是指将机器学习算法从开发阶段，通过变换和优化算法，使其变成部署状态。\n\n- [AI编译器技术剖析（一）-概述](https://zhuanlan.zhihu.com/p/669347560)\n- [AI编译器技术剖析（二）-传统编译器](https://zhuanlan.zhihu.com/p/671477784)\n- [AI编译器技术剖析（三）-树模型编译工具 Treelite 详解](https://zhuanlan.zhihu.com/p/676723324)\n- [AI编译器技术剖析（四）-编译器前端]()\n- [AI编译器技术剖析（五）-编译器后端]()\n- [AI编译器技术剖析（六）-主流编译框架]()\n- [AI编译器技术剖析（七）-深度学习模型编译优化]()\n- [lleaves：使用 LLVM 编译梯度提升决策树将预测速度提升10+倍](https://zhuanlan.zhihu.com/p/672584013)\n\n框架：\n\n- MLIR\n- XLA\n- TVM\n\n\n## AI基础设施\n\n- [AI 集群基础设施 NVMe SSD 详解](https://zhuanlan.zhihu.com/p/672098336)\n- [AI 集群基础设施 InfiniBand 详解](https://zhuanlan.zhihu.com/p/673903240)\n- [大模型训练基础设施：算力篇]()\n\n\n### AI加速卡\n\n- [AI芯片技术原理剖析（一）：国内外AI芯片概述](https://zhuanlan.zhihu.com/p/667686665)\n- AI芯片技术原理剖析（二）：英伟达GPU \n- AI芯片技术原理剖析（三）：谷歌TPU\n\n### AI集群\n\n待更新...\n\n\n### [AI集群网络通信](https://github.com/liguodongiot/llm-action/tree/main/docs/llm-base/network-communication)\n\n待更新...\n\n- 分布式训练网络通讯原语\n- AI 集群通信软硬件\n\n\n## LLMOps\n\n- [在 Kubernetes 上部署机器学习模型的指南](https://zhuanlan.zhihu.com/p/676389726)\n- [使用 Kubernetes 部署机器学习模型的优势](https://juejin.cn/post/7320513026188099619)\n\n\n\n## LLM生态相关技术\n\n- [大模型词表扩充必备工具SentencePiece](https://zhuanlan.zhihu.com/p/630696264)\n- [大模型实践总结](https://www.zhihu.com/question/601594836/answer/3032763174)\n- [ChatGLM 和 ChatGPT 的技术区别在哪里？](https://www.zhihu.com/question/604393963/answer/3061358152)\n- [现在为什么那么多人以清华大学的ChatGLM-6B为基座进行试验？](https://www.zhihu.com/question/602504880/answer/3041965998)\n- [为什么很多新发布的大模型默认使用BF16而不是FP16？](https://www.zhihu.com/question/616600181/answer/3195333332)\n- [大模型训练时ZeRO-2、ZeRO-3能否和Pipeline并行相结合？](https://www.zhihu.com/question/652836990/answer/3468210626)\n- [一文详解模型权重存储新格式 Safetensors](https://juejin.cn/post/7386360803039838235)\n- [一文搞懂大模型文件存储格式新宠GGUF](https://juejin.cn/post/7408858126042726435)\n\n\n## [LLM面试题](https://github.com/liguodongiot/llm-action/blob/main/llm-interview/README.md)\n\n正在收集中...\n\n- [大模型基础常见面试题](https://github.com/liguodongiot/llm-action/blob/main/llm-interview/base.md)\n- [大模型算法常见面试题](https://github.com/liguodongiot/llm-action/blob/main/llm-interview/llm-algo.md)\n- [大模型训练常见面试题](https://github.com/liguodongiot/llm-action/blob/main/llm-interview/llm-train.md)\n- [大模型微调常见面试题](https://github.com/liguodongiot/llm-action/blob/main/llm-interview/llm-ft.md)\n- [大模型评估常见面试题](https://github.com/liguodongiot/llm-action/blob/main/llm-interview/llm-eval.md)\n- [大模型压缩常见面试题](https://github.com/liguodongiot/llm-action/blob/main/llm-interview/llm-compress.md)\n- [大模型推理常见面试题](https://github.com/liguodongiot/llm-action/blob/main/llm-interview/llm-inference.md)\n- [大模型应用常见面试题](https://github.com/liguodongiot/llm-action/blob/main/llm-interview/llm-app.md)\n- [大模型综合性面试题](https://github.com/liguodongiot/llm-action/blob/main/llm-interview/comprehensive.md)\n\n\n\n\n**[⬆ 一键返回目录](#目录)**\n\n## 服务器基础环境安装及常用工具\n\n基础环境安装：\n\n- [英伟达A800加速卡常见软件包安装命令](https://github.com/liguodongiot/llm-action/blob/main/docs/llm-base/a800-env-install.md)\n- [英伟达H800加速卡常见软件包安装命令](https://github.com/liguodongiot/llm-action/blob/main/docs/llm-base/h800-env-install.md)\n- [昇腾910加速卡常见软件包安装命令](https://github.com/liguodongiot/llm-action/blob/main/llm_localization/ascend910-env-install.md)\n\n常用工具：\n\n- [Linux 常见命令大全](https://juejin.cn/post/6992742028605915150)\n- [Conda 常用命令大全](https://juejin.cn/post/7089093437223338015)\n- [Poetry 常用命令大全](https://juejin.cn/post/6999405667261874183)\n- [Docker 常用命令大全](https://juejin.cn/post/7016238524286861325)\n- [Docker Dockerfile 指令大全](https://juejin.cn/post/7016595442062327844)\n- [Kubernetes 常用命令大全](https://juejin.cn/post/7031201391553019911)\n- [集群环境 GPU 管理和监控工具 DCGM 常用命令大全](https://github.com/liguodongiot/llm-action/blob/main/docs/llm-base/dcgmi.md)\n\n## LLM学习交流群\n\n我创建了大模型相关的学习交流群，供大家一起学习交流大模型相关的最新技术，目前已有5个群，每个群都有上百人的规模，**可加我微信进群**（加微信请备注来意，如：进大模型学习交流群+GitHub，进大模型推理加速交流群+GitHub、进大模型应用开发交流群+GitHub、进大模型校招交流群+GitHub等）。**一定要备注哟，否则不予通过**。\n\nPS：**成都有个本地大模型交流群，想进可以另外单独备注下。**\n\n<p align=\"center\">\n  <img src=\"https://github.com/liguodongiot/llm-action/blob/main/pic/wx.jpg\">\n</p>\n\n## 微信公众号\n\n微信公众号：**吃果冻不吐果冻皮**，该公众号主要分享AI工程化（大模型、MLOps等）相关实践经验，免费电子书籍、论文等。\n\n<p align=\"center\">\n  <img src=\"https://github.com/liguodongiot/llm-action/blob/main/pic/wx-gzh.png\" >\n</p>\n\n**[⬆ 一键返回目录](#目录)**\n\n## Star History\n\n[![Star History Chart](https://api.star-history.com/svg?repos=liguodongiot/llm-action&type=Date)](https://star-history.com/#liguodongiot/llm-action&Date)\n\n\n## AI工程化课程推荐\n\n如今人工智能的发展可谓是如火如荼，ChatGPT、Sora、文心一言等AI大模型如雨后春笋般纷纷涌现。AI大模型优势在于它能处理复杂性问题；因此，越来越多的企业需要具备**AI算法设计、AI应用开发、模型推理加速及模型压缩**等AI工程化落地的能力。这就导致行业内的工程师，需要快速提升自身的技术栈，以便于在行业内站稳脚跟。我在[llm-resource](https://github.com/liguodongiot/llm-resource) 和 [ai-system](https://github.com/liguodongiot/ai-system)梳理了一些大模型和AI工程化相关资料，同时，推荐一些AI工程化相关的课程，[点击](https://mp.weixin.qq.com/s?__biz=MzU3Mzg5ODgxMg==&mid=2247488019&idx=1&sn=90ca4657a643431f21df44fa199e695f&chksm=fd3bfb40ca4c72565fdb91717c8a27739bcb71d10fae89c7994f141d375d21059be19e702164&token=701439972&lang=zh_CN#rd)查看详情。\n\n\n\n\n\n"
        },
        {
          "name": "ai-compiler",
          "type": "tree",
          "content": null
        },
        {
          "name": "ai-framework",
          "type": "tree",
          "content": null
        },
        {
          "name": "ai-infra",
          "type": "tree",
          "content": null
        },
        {
          "name": "blog",
          "type": "tree",
          "content": null
        },
        {
          "name": "docs",
          "type": "tree",
          "content": null
        },
        {
          "name": "faq",
          "type": "tree",
          "content": null
        },
        {
          "name": "git-pull-push.sh",
          "type": "blob",
          "size": 0.1787109375,
          "content": "git pull origin main\ngit add .\n\n#time=`date -Iminutes`\ntime=`date +\"%Y-%m-%d_%H:%M:%S\"`\n\necho $time\n\ncommit_info=\"update-\"\"$time\"\n\ngit commit -m $commit_info\n\ngit push origin main\n\n\n\n"
        },
        {
          "name": "llm-algo",
          "type": "tree",
          "content": null
        },
        {
          "name": "llm-alignment",
          "type": "tree",
          "content": null
        },
        {
          "name": "llm-application",
          "type": "tree",
          "content": null
        },
        {
          "name": "llm-compression",
          "type": "tree",
          "content": null
        },
        {
          "name": "llm-data-engineering",
          "type": "tree",
          "content": null
        },
        {
          "name": "llm-eval",
          "type": "tree",
          "content": null
        },
        {
          "name": "llm-inference",
          "type": "tree",
          "content": null
        },
        {
          "name": "llm-interview",
          "type": "tree",
          "content": null
        },
        {
          "name": "llm-localization",
          "type": "tree",
          "content": null
        },
        {
          "name": "llm-maas",
          "type": "tree",
          "content": null
        },
        {
          "name": "llm-optimizer",
          "type": "tree",
          "content": null
        },
        {
          "name": "llm-pipeline",
          "type": "tree",
          "content": null
        },
        {
          "name": "llm-train",
          "type": "tree",
          "content": null
        },
        {
          "name": "llmops",
          "type": "tree",
          "content": null
        },
        {
          "name": "mkdir-dir-file.sh",
          "type": "blob",
          "size": 0.125,
          "content": "\n\ndir=$1\n\n\n# sh mkdir-dir-file.sh llm-algo/chatglm3\n\nmkdir $dir\n\ntouch $dir/README.md\ntouch $dir/reference.md\n\n\ntree -h  $dir\n\n\n"
        },
        {
          "name": "paper",
          "type": "tree",
          "content": null
        },
        {
          "name": "pic",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}