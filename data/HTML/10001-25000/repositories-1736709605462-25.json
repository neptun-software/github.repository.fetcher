{
  "metadata": {
    "timestamp": 1736709605462,
    "page": 25,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjMw",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "emilwallner/Screenshot-to-code",
      "stars": 16449,
      "defaultBranch": "master",
      "files": [
        {
          "name": "Bootstrap",
          "type": "tree",
          "content": null
        },
        {
          "name": "HTML",
          "type": "tree",
          "content": null
        },
        {
          "name": "Hello_world",
          "type": "tree",
          "content": null
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 1.2529296875,
          "content": "MIT License\n\nCopyright (c) 2017 Emil Wallner\n\nThe files in the folder compiler and the dataset are licensed \nunder Tony Beltramelli's Apache License 2.0: https://github.com/tonybeltramelli/pix2code/blob/master/LICENSE\n\nAll other files are licensed under MIT.\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 4.5087890625,
          "content": "<img src=\"/README_images/screenshot-to-code.svg?raw=true\" width=\"800px\">\n\n---\n\n**A detailed tutorial covering the code in this repository:** [Turning design mockups into code with deep learning](https://emilwallner.medium.com/how-you-can-train-an-ai-to-convert-your-design-mockups-into-html-and-css-cc7afd82fed4).\n\n**Plug:** ðŸ‘‰ Check out my 60-page guide, [No ML Degree](https://www.emilwallner.com/p/no-ml-degree), on how to land a machine learning job without a degree.\n\nThe neural network is built in three iterations. Starting with a Hello World version, followed by the main neural network layers, and ending by training it to generalize. \n\nThe models are based on Tony Beltramelli's [pix2code](https://github.com/tonybeltramelli/pix2code), and inspired by Airbnb's [sketching interfaces](https://airbnb.design/sketching-interfaces/), and Harvard's [im2markup](https://github.com/harvardnlp/im2markup).\n\n**Note:** only the Bootstrap version can generalize on new design mock-ups. It uses 16 domain-specific tokens which are translated into HTML/CSS. It has a 97% accuracy. The best model uses a GRU instead of an LSTM. This version can be trained on a few GPUs. The raw HTML version has potential to generalize, but is still unproven and requires a significant amount of GPUs to train. The current model is also trained on a homogeneous and small dataset, thus it's hard to tell how well it behaves on more complex layouts.\n\nDataset: https://github.com/tonybeltramelli/pix2code/tree/master/datasets\n\nA quick overview of the process: \n\n### 1) Give a design image to the trained neural network\n\n![Insert image](https://i.imgur.com/LDmoLLV.png)\n\n### 2) The neural network converts the image into HTML markup \n\n<img src=\"/README_images/html_display.gif?raw=true\" width=\"800px\">\n\n### 3) Rendered output\n\n![Screenshot](https://i.imgur.com/tEAfyZ8.png)\n\n\n## Installation\n\n### FloydHub\n\n[![Run on FloydHub](https://static.floydhub.com/button/button.svg)](https://floydhub.com/run?template=https://github.com/floydhub/pix2code-template)\n\nClick this button to open a [Workspace](https://blog.floydhub.com/workspaces/) on [FloydHub](https://www.floydhub.com/?utm_medium=readme&utm_source=pix2code&utm_campaign=aug_2018) where you will find the same environment and dataset used for the *Bootstrap version*. You can also find the trained models for testing.\n\n### Local\n``` bash\npip install keras tensorflow pillow h5py jupyter\n```\n```\ngit clone https://github.com/emilwallner/Screenshot-to-code.git\ncd Screenshot-to-code/\njupyter notebook\n```\nGo do the desired notebook, files that end with '.ipynb'. To run the model, go to the menu then click on Cell > Run all\n\nThe final version, the Bootstrap version, is prepared with a small set to test run the model. If you want to try it with all the data, you need to download the data here: https://www.floydhub.com/emilwallner/datasets/imagetocode, and specify the correct ```dir_name```.\n\n## Folder structure\n\n``` bash\n  |  |-Bootstrap                           #The Bootstrap version\n  |  |  |-compiler                         #A compiler to turn the tokens to HTML/CSS (by pix2code)\n  |  |  |-resources\t\t\t\t\t\t\t\t\t\t\t\n  |  |  |  |-eval_light                    #10 test images and markup\n  |  |-Hello_world                         #The Hello World version\n  |  |-HTML                                #The HTML version\n  |  |  |-Resources_for_index_file         #CSS,images and scripts to test index.html file\n  |  |  |-html                             #HTML files to train it on\n  |  |  |-images                           #Screenshots for training\n  |-readme_images                          #Images for the readme page\n```\n\n\n## Hello World\n<p align=\"center\"><img src=\"/README_images/Hello_world_model.png?raw=true\" width=\"400px\"></p>\n\n\n## HTML\n<p align=\"center\"><img src=\"/README_images/HTML_model.png?raw=true\" width=\"400px\"></p>\n\n\n## Bootstrap\n<p align=\"center\"><img src=\"/README_images/Bootstrap_model.png?raw=true\" width=\"400px\"></p>\n\n## Model weights\n- [Bootstrap](https://www.floydhub.com/emilwallner/datasets/imagetocode) (The pre-trained model uses GRUs instead of LSTMs)\n- [HTML](https://www.floydhub.com/emilwallner/datasets/html_models)\n\n## Acknowledgments\n- Thanks to IBM for donating computing power through their PowerAI platform\n- The code is largely influenced by Tony Beltramelli's pix2code paper. [Code](https://github.com/tonybeltramelli/pix2code) [Paper](https://arxiv.org/abs/1705.07962)\n- The structure and some of the functions are from Jason Brownlee's [excellent tutorial](https://machinelearningmastery.com/develop-a-caption-generation-model-in-keras/)\n"
        },
        {
          "name": "README_images",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}