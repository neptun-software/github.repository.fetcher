{
  "metadata": {
    "timestamp": 1736557589116,
    "page": 643,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjY3MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "karpathy/LLM101n",
      "stars": 30949,
      "defaultBranch": "master",
      "files": [
        {
          "name": "README.md",
          "type": "blob",
          "size": 2.34,
          "content": "# LLM101n: Let's build a Storyteller\n\n---\n\n**!!! NOTE: this course does not yet exist. It is current being developed by [Eureka Labs](https://eurekalabs.ai). Until it is ready I am archiving this repo !!!**\n\n---\n\n![LLM101n header image](llm101n.jpg)\n\n>  What I cannot create, I do not understand. -Richard Feynman\n\nIn this course we will build a Storyteller AI Large Language Model (LLM). Hand in hand, you'll be able to create, refine and illustrate little [stories](https://huggingface.co/datasets/roneneldan/TinyStories) with the AI. We are going to build everything end-to-end from basics to a functioning web app similar to ChatGPT, from scratch in Python, C and CUDA, and with minimal computer science prerequisites. By the end you should have a relatively deep understanding of AI, LLMs, and deep learning more generally.\n\n**Syllabus**\n\n- Chapter 01 **Bigram Language Model** (language modeling)\n- Chapter 02 **Micrograd** (machine learning, backpropagation)\n- Chapter 03 **N-gram model** (multi-layer perceptron, matmul, gelu)\n- Chapter 04 **Attention** (attention, softmax, positional encoder)\n- Chapter 05 **Transformer** (transformer, residual, layernorm, GPT-2)\n- Chapter 06 **Tokenization** (minBPE, byte pair encoding)\n- Chapter 07 **Optimization** (initialization, optimization, AdamW)\n- Chapter 08 **Need for Speed I: Device** (device, CPU, GPU, ...)\n- Chapter 09 **Need for Speed II: Precision** (mixed precision training, fp16, bf16, fp8, ...)\n- Chapter 10 **Need for Speed III: Distributed** (distributed optimization, DDP, ZeRO)\n- Chapter 11 **Datasets** (datasets, data loading, synthetic data generation)\n- Chapter 12 **Inference I: kv-cache** (kv-cache)\n- Chapter 13 **Inference II: Quantization** (quantization)\n- Chapter 14 **Finetuning I: SFT** (supervised finetuning SFT, PEFT, LoRA, chat)\n- Chapter 15 **Finetuning II: RL** (reinforcement learning, RLHF, PPO, DPO)\n- Chapter 16 **Deployment** (API, web app)\n- Chapter 17 **Multimodal** (VQVAE, diffusion transformer)\n\n**Appendix**\n\nFurther topics to work into the progression above:\n\n- Programming languages: Assembly, C, Python\n- Data types: Integer, Float, String (ASCII, Unicode, UTF-8)\n- Tensor: shapes, views, strides, contiguous, ...\n- Deep Learning frameworks: PyTorch, JAX\n- Neural Net Architecture: GPT (1,2,3,4), Llama (RoPE, RMSNorm, GQA), MoE, ...\n- Multimodal: Images, Audio, Video, VQVAE, VQGAN, diffusion\n"
        },
        {
          "name": "llm101n.jpg",
          "type": "blob",
          "size": 275.57,
          "content": null
        }
      ]
    }
  ]
}