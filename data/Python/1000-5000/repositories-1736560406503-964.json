{
  "metadata": {
    "timestamp": 1736560406503,
    "page": 964,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjk3MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "hyunwoongko/transformer",
      "stars": 3231,
      "defaultBranch": "master",
      "files": [
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.060546875,
          "content": "## dataset ignore\ndata/\nvenv/\n.data/\n*.pt\n__pycache__\nresult/\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 12.67578125,
          "content": "### WARNING\nThis code was written in 2019, and I was not very familiar with transformer model in that time.\nSo don't trust this code too much. Currently I am not managing this code well, so please open pull requests if you find bugs in the code and want to fix.\n\n# Transformer\nMy own implementation Transformer model (Attention is All You Need - Google Brain, 2017)\n<br><br>\n![model](image/model.png)\n<br><br>\n\n## 1. Implementations\n\n### 1.1 Positional Encoding\n\n![model](image/positional_encoding.jpg)\n   \n    \n```python\nclass PositionalEncoding(nn.Module):\n    \"\"\"\n    compute sinusoid encoding.\n    \"\"\"\n    def __init__(self, d_model, max_len, device):\n        \"\"\"\n        constructor of sinusoid encoding class\n\n        :param d_model: dimension of model\n        :param max_len: max sequence length\n        :param device: hardware device setting\n        \"\"\"\n        super(PositionalEncoding, self).__init__()\n\n        # same size with input matrix (for adding with input matrix)\n        self.encoding = torch.zeros(max_len, d_model, device=device)\n        self.encoding.requires_grad = False  # we don't need to compute gradient\n\n        pos = torch.arange(0, max_len, device=device)\n        pos = pos.float().unsqueeze(dim=1)\n        # 1D => 2D unsqueeze to represent word's position\n\n        _2i = torch.arange(0, d_model, step=2, device=device).float()\n        # 'i' means index of d_model (e.g. embedding size = 50, 'i' = [0,50])\n        # \"step=2\" means 'i' multiplied with two (same with 2 * i)\n\n        self.encoding[:, 0::2] = torch.sin(pos / (10000 ** (_2i / d_model)))\n        self.encoding[:, 1::2] = torch.cos(pos / (10000 ** (_2i / d_model)))\n        # compute positional encoding to consider positional information of words\n\n    def forward(self, x):\n        # self.encoding\n        # [max_len = 512, d_model = 512]\n\n        batch_size, seq_len = x.size()\n        # [batch_size = 128, seq_len = 30]\n\n        return self.encoding[:seq_len, :]\n        # [seq_len = 30, d_model = 512]\n        # it will add with tok_emb : [128, 30, 512]         \n```\n<br><br>\n\n### 1.2 Multi-Head Attention\n\n\n![model](image/multi_head_attention.jpg)\n\n```python\nclass MultiHeadAttention(nn.Module):\n\n    def __init__(self, d_model, n_head):\n        super(MultiHeadAttention, self).__init__()\n        self.n_head = n_head\n        self.attention = ScaleDotProductAttention()\n        self.w_q = nn.Linear(d_model, d_model)\n        self.w_k = nn.Linear(d_model, d_model)\n        self.w_v = nn.Linear(d_model, d_model)\n        self.w_concat = nn.Linear(d_model, d_model)\n\n    def forward(self, q, k, v, mask=None):\n        # 1. dot product with weight matrices\n        q, k, v = self.w_q(q), self.w_k(k), self.w_v(v)\n\n        # 2. split tensor by number of heads\n        q, k, v = self.split(q), self.split(k), self.split(v)\n\n        # 3. do scale dot product to compute similarity\n        out, attention = self.attention(q, k, v, mask=mask)\n        \n        # 4. concat and pass to linear layer\n        out = self.concat(out)\n        out = self.w_concat(out)\n\n        # 5. visualize attention map\n        # TODO : we should implement visualization\n\n        return out\n\n    def split(self, tensor):\n        \"\"\"\n        split tensor by number of head\n\n        :param tensor: [batch_size, length, d_model]\n        :return: [batch_size, head, length, d_tensor]\n        \"\"\"\n        batch_size, length, d_model = tensor.size()\n\n        d_tensor = d_model // self.n_head\n        tensor = tensor.view(batch_size, length, self.n_head, d_tensor).transpose(1, 2)\n        # it is similar with group convolution (split by number of heads)\n\n        return tensor\n\n    def concat(self, tensor):\n        \"\"\"\n        inverse function of self.split(tensor : torch.Tensor)\n\n        :param tensor: [batch_size, head, length, d_tensor]\n        :return: [batch_size, length, d_model]\n        \"\"\"\n        batch_size, head, length, d_tensor = tensor.size()\n        d_model = head * d_tensor\n\n        tensor = tensor.transpose(1, 2).contiguous().view(batch_size, length, d_model)\n        return tensor\n```\n<br><br>\n\n### 1.3 Scale Dot Product Attention\n\n![model](image/scale_dot_product_attention.jpg)\n\n```python\nclass ScaleDotProductAttention(nn.Module):\n    \"\"\"\n    compute scale dot product attention\n\n    Query : given sentence that we focused on (decoder)\n    Key : every sentence to check relationship with Qeury(encoder)\n    Value : every sentence same with Key (encoder)\n    \"\"\"\n\n    def __init__(self):\n        super(ScaleDotProductAttention, self).__init__()\n        self.softmax = nn.Softmax(dim=-1)\n\n    def forward(self, q, k, v, mask=None, e=1e-12):\n        # input is 4 dimension tensor\n        # [batch_size, head, length, d_tensor]\n        batch_size, head, length, d_tensor = k.size()\n\n        # 1. dot product Query with Key^T to compute similarity\n        k_t = k.transpose(2, 3)  # transpose\n        score = (q @ k_t) / math.sqrt(d_tensor)  # scaled dot product\n\n        # 2. apply masking (opt)\n        if mask is not None:\n            score = score.masked_fill(mask == 0, -10000)\n\n        # 3. pass them softmax to make [0, 1] range\n        score = self.softmax(score)\n\n        # 4. multiply with Value\n        v = score @ v\n\n        return v, score\n```\n<br><br>\n\n### 1.4 Layer Norm\n\n![model](image/layer_norm.jpg)\n    \n```python\nclass LayerNorm(nn.Module):\n    def __init__(self, d_model, eps=1e-12):\n        super(LayerNorm, self).__init__()\n        self.gamma = nn.Parameter(torch.ones(d_model))\n        self.beta = nn.Parameter(torch.zeros(d_model))\n        self.eps = eps\n\n    def forward(self, x):\n        mean = x.mean(-1, keepdim=True)\n        var = x.var(-1, unbiased=False, keepdim=True)\n        # '-1' means last dimension. \n\n        out = (x - mean) / torch.sqrt(var + self.eps)\n        out = self.gamma * out + self.beta\n        return out\n\n```\n<br><br>\n\n### 1.5 Positionwise Feed Forward\n\n![model](image/positionwise_feed_forward.jpg)\n    \n```python\n\nclass PositionwiseFeedForward(nn.Module):\n\n    def __init__(self, d_model, hidden, drop_prob=0.1):\n        super(PositionwiseFeedForward, self).__init__()\n        self.linear1 = nn.Linear(d_model, hidden)\n        self.linear2 = nn.Linear(hidden, d_model)\n        self.relu = nn.ReLU()\n        self.dropout = nn.Dropout(p=drop_prob)\n\n    def forward(self, x):\n        x = self.linear1(x)\n        x = self.relu(x)\n        x = self.dropout(x)\n        x = self.linear2(x)\n        return x\n```\n<br><br>\n\n### 1.6 Encoder & Decoder Structure\n\n![model](image/enc_dec.jpg)\n    \n```python\nclass EncoderLayer(nn.Module):\n\n    def __init__(self, d_model, ffn_hidden, n_head, drop_prob):\n        super(EncoderLayer, self).__init__()\n        self.attention = MultiHeadAttention(d_model=d_model, n_head=n_head)\n        self.norm1 = LayerNorm(d_model=d_model)\n        self.dropout1 = nn.Dropout(p=drop_prob)\n\n        self.ffn = PositionwiseFeedForward(d_model=d_model, hidden=ffn_hidden, drop_prob=drop_prob)\n        self.norm2 = LayerNorm(d_model=d_model)\n        self.dropout2 = nn.Dropout(p=drop_prob)\n\n    def forward(self, x, src_mask):\n        # 1. compute self attention\n        _x = x\n        x = self.attention(q=x, k=x, v=x, mask=src_mask)\n        \n        # 2. add and norm\n        x = self.dropout1(x)\n        x = self.norm1(x + _x)\n        \n        # 3. positionwise feed forward network\n        _x = x\n        x = self.ffn(x)\n      \n        # 4. add and norm\n        x = self.dropout2(x)\n        x = self.norm2(x + _x)\n        return x\n```\n<br>\n\n```python\nclass Encoder(nn.Module):\n\n    def __init__(self, enc_voc_size, max_len, d_model, ffn_hidden, n_head, n_layers, drop_prob, device):\n        super().__init__()\n        self.emb = TransformerEmbedding(d_model=d_model,\n                                        max_len=max_len,\n                                        vocab_size=enc_voc_size,\n                                        drop_prob=drop_prob,\n                                        device=device)\n\n        self.layers = nn.ModuleList([EncoderLayer(d_model=d_model,\n                                                  ffn_hidden=ffn_hidden,\n                                                  n_head=n_head,\n                                                  drop_prob=drop_prob)\n                                     for _ in range(n_layers)])\n\n    def forward(self, x, src_mask):\n        x = self.emb(x)\n\n        for layer in self.layers:\n            x = layer(x, src_mask)\n\n        return x\n```\n<br>\n\n```python\nclass DecoderLayer(nn.Module):\n\n    def __init__(self, d_model, ffn_hidden, n_head, drop_prob):\n        super(DecoderLayer, self).__init__()\n        self.self_attention = MultiHeadAttention(d_model=d_model, n_head=n_head)\n        self.norm1 = LayerNorm(d_model=d_model)\n        self.dropout1 = nn.Dropout(p=drop_prob)\n\n        self.enc_dec_attention = MultiHeadAttention(d_model=d_model, n_head=n_head)\n        self.norm2 = LayerNorm(d_model=d_model)\n        self.dropout2 = nn.Dropout(p=drop_prob)\n\n        self.ffn = PositionwiseFeedForward(d_model=d_model, hidden=ffn_hidden, drop_prob=drop_prob)\n        self.norm3 = LayerNorm(d_model=d_model)\n        self.dropout3 = nn.Dropout(p=drop_prob)\n\n    def forward(self, dec, enc, trg_mask, src_mask):    \n        # 1. compute self attention\n        _x = dec\n        x = self.self_attention(q=dec, k=dec, v=dec, mask=trg_mask)\n        \n        # 2. add and norm\n        x = self.dropout1(x)\n        x = self.norm1(x + _x)\n\n        if enc is not None:\n            # 3. compute encoder - decoder attention\n            _x = x\n            x = self.enc_dec_attention(q=x, k=enc, v=enc, mask=src_mask)\n            \n            # 4. add and norm\n            x = self.dropout2(x)\n            x = self.norm2(x + _x)\n\n        # 5. positionwise feed forward network\n        _x = x\n        x = self.ffn(x)\n        \n        # 6. add and norm\n        x = self.dropout3(x)\n        x = self.norm3(x + _x)\n        return x\n```\n<br>\n\n```python        \nclass Decoder(nn.Module):\n    def __init__(self, dec_voc_size, max_len, d_model, ffn_hidden, n_head, n_layers, drop_prob, device):\n        super().__init__()\n        self.emb = TransformerEmbedding(d_model=d_model,\n                                        drop_prob=drop_prob,\n                                        max_len=max_len,\n                                        vocab_size=dec_voc_size,\n                                        device=device)\n\n        self.layers = nn.ModuleList([DecoderLayer(d_model=d_model,\n                                                  ffn_hidden=ffn_hidden,\n                                                  n_head=n_head,\n                                                  drop_prob=drop_prob)\n                                     for _ in range(n_layers)])\n\n        self.linear = nn.Linear(d_model, dec_voc_size)\n\n    def forward(self, trg, src, trg_mask, src_mask):\n        trg = self.emb(trg)\n\n        for layer in self.layers:\n            trg = layer(trg, src, trg_mask, src_mask)\n\n        # pass to LM head\n        output = self.linear(trg)\n        return output\n```\n<br><br>\n\n## 2. Experiments\n\nI use Multi30K Dataset to train and evaluate model <br>\nYou can check detail of dataset [here](https://arxiv.org/abs/1605.00459) <br>\nI follow original paper's parameter settings. (below) <br>\n\n![conf](image/transformer-model-size.jpg)\n### 2.1 Model Specification\n\n* total parameters = 55,207,087\n* model size = 215.7MB\n* lr scheduling : ReduceLROnPlateau\n\n#### 2.1.1 configuration\n\n* batch_size = 128\n* max_len = 256\n* d_model = 512\n* n_layers = 6\n* n_heads = 8\n* ffn_hidden = 2048\n* drop_prob = 0.1\n* init_lr = 0.1\n* factor = 0.9\n* patience = 10\n* warmup = 100\n* adam_eps = 5e-9\n* epoch = 1000\n* clip = 1\n* weight_decay = 5e-4\n<br><br>\n\n### 2.2 Training Result\n\n![image](saved/transformer-base/train_result.jpg)\n* Minimum Training Loss = 2.852672759656864\n* Minimum Validation Loss = 3.2048025131225586 \n<br><br>\n\n| Model | Dataset | BLEU Score |\n|:---:|:---:|:---:|\n| Original Paper's | WMT14 EN-DE | 25.8 |\n| My Implementation | Multi30K EN-DE | 26.4 |\n\n<br><br>\n\n\n## 3. Reference\n- [Attention is All You Need, 2017 - Google](https://arxiv.org/abs/1706.03762)\n- [The Illustrated Transformer - Jay Alammar](http://jalammar.github.io/illustrated-transformer/)\n- [Data & Optimization Code Reference - Bentrevett](https://github.com/bentrevett/pytorch-seq2seq/)\n\n<br><br>\n\n## 4. Licence\n    Copyright 2019 Hyunwoong Ko.\n    \n    Licensed under the Apache License, Version 2.0 (the \"License\");\n    you may not use this file except in compliance with the License.\n    You may obtain a copy of the License at\n    \n    http://www.apache.org/licenses/LICENSE-2.0\n    \n    Unless required by applicable law or agreed to in writing, software\n    distributed under the License is distributed on an \"AS IS\" BASIS,\n    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n    See the License for the specific language governing permissions and\n    limitations under the License.\n"
        },
        {
          "name": "conf.py",
          "type": "blob",
          "size": 0.4794921875,
          "content": "\"\"\"\n@author : Hyunwoong\n@when : 2019-10-22\n@homepage : https://github.com/gusdnd852\n\"\"\"\nimport torch\n\n# GPU device setting\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\n# model parameter setting\nbatch_size = 128\nmax_len = 256\nd_model = 512\nn_layers = 6\nn_heads = 8\nffn_hidden = 2048\ndrop_prob = 0.1\n\n# optimizer parameter setting\ninit_lr = 1e-5\nfactor = 0.9\nadam_eps = 5e-9\npatience = 10\nwarmup = 100\nepoch = 1000\nclip = 1.0\nweight_decay = 5e-4\ninf = float('inf')\n"
        },
        {
          "name": "data.py",
          "type": "blob",
          "size": 0.951171875,
          "content": "\"\"\"\n@author : Hyunwoong\n@when : 2019-10-29\n@homepage : https://github.com/gusdnd852\n\"\"\"\nfrom conf import *\nfrom util.data_loader import DataLoader\nfrom util.tokenizer import Tokenizer\n\ntokenizer = Tokenizer()\nloader = DataLoader(ext=('.en', '.de'),\n                    tokenize_en=tokenizer.tokenize_en,\n                    tokenize_de=tokenizer.tokenize_de,\n                    init_token='<sos>',\n                    eos_token='<eos>')\n\ntrain, valid, test = loader.make_dataset()\nloader.build_vocab(train_data=train, min_freq=2)\ntrain_iter, valid_iter, test_iter = loader.make_iter(train, valid, test,\n                                                     batch_size=batch_size,\n                                                     device=device)\n\nsrc_pad_idx = loader.source.vocab.stoi['<pad>']\ntrg_pad_idx = loader.target.vocab.stoi['<pad>']\ntrg_sos_idx = loader.target.vocab.stoi['<sos>']\n\nenc_voc_size = len(loader.source.vocab)\ndec_voc_size = len(loader.target.vocab)\n"
        },
        {
          "name": "graph.py",
          "type": "blob",
          "size": 0.94921875,
          "content": "\"\"\"\n@author : Hyunwoong\n@when : 2019-12-18\n@homepage : https://github.com/gusdnd852\n\"\"\"\n\nimport matplotlib.pyplot as plt\nimport re\n\n\ndef read(name):\n    f = open(name, 'r')\n    file = f.read()\n    file = re.sub('\\\\[', '', file)\n    file = re.sub('\\\\]', '', file)\n    f.close()\n\n    return [float(i) for idx, i in enumerate(file.split(','))]\n\n\ndef draw(mode):\n    if mode == 'loss':\n        train = read('./result/train_loss.txt')\n        test = read('./result/test_loss.txt')\n        plt.plot(train, 'r', label='train')\n        plt.plot(test, 'b', label='validation')\n        plt.legend(loc='lower left')\n\n\n    elif mode == 'bleu':\n        bleu = read('./result/bleu.txt')\n        plt.plot(bleu, 'b', label='bleu score')\n        plt.legend(loc='lower right')\n\n    plt.xlabel('epoch')\n    plt.ylabel(mode)\n    plt.title('training result')\n    plt.grid(True, which='both', axis='both')\n    plt.show()\n\n\nif __name__ == '__main__':\n    draw(mode='loss')\n    draw(mode='bleu')\n"
        },
        {
          "name": "image",
          "type": "tree",
          "content": null
        },
        {
          "name": "models",
          "type": "tree",
          "content": null
        },
        {
          "name": "paper",
          "type": "tree",
          "content": null
        },
        {
          "name": "saved",
          "type": "tree",
          "content": null
        },
        {
          "name": "train.py",
          "type": "blob",
          "size": 4.791015625,
          "content": "\"\"\"\n@author : Hyunwoong\n@when : 2019-10-22\n@homepage : https://github.com/gusdnd852\n\"\"\"\nimport math\nimport time\n\nfrom torch import nn, optim\nfrom torch.optim import Adam\n\nfrom data import *\nfrom models.model.transformer import Transformer\nfrom util.bleu import idx_to_word, get_bleu\nfrom util.epoch_timer import epoch_time\n\n\ndef count_parameters(model):\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n\n\ndef initialize_weights(m):\n    if hasattr(m, 'weight') and m.weight.dim() > 1:\n        nn.init.kaiming_uniform(m.weight.data)\n\n\nmodel = Transformer(src_pad_idx=src_pad_idx,\n                    trg_pad_idx=trg_pad_idx,\n                    trg_sos_idx=trg_sos_idx,\n                    d_model=d_model,\n                    enc_voc_size=enc_voc_size,\n                    dec_voc_size=dec_voc_size,\n                    max_len=max_len,\n                    ffn_hidden=ffn_hidden,\n                    n_head=n_heads,\n                    n_layers=n_layers,\n                    drop_prob=drop_prob,\n                    device=device).to(device)\n\nprint(f'The model has {count_parameters(model):,} trainable parameters')\nmodel.apply(initialize_weights)\noptimizer = Adam(params=model.parameters(),\n                 lr=init_lr,\n                 weight_decay=weight_decay,\n                 eps=adam_eps)\n\nscheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer,\n                                                 verbose=True,\n                                                 factor=factor,\n                                                 patience=patience)\n\ncriterion = nn.CrossEntropyLoss(ignore_index=src_pad_idx)\n\n\ndef train(model, iterator, optimizer, criterion, clip):\n    model.train()\n    epoch_loss = 0\n    for i, batch in enumerate(iterator):\n        src = batch.src\n        trg = batch.trg\n\n        optimizer.zero_grad()\n        output = model(src, trg[:, :-1])\n        output_reshape = output.contiguous().view(-1, output.shape[-1])\n        trg = trg[:, 1:].contiguous().view(-1)\n\n        loss = criterion(output_reshape, trg)\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n        optimizer.step()\n\n        epoch_loss += loss.item()\n        print('step :', round((i / len(iterator)) * 100, 2), '% , loss :', loss.item())\n\n    return epoch_loss / len(iterator)\n\n\ndef evaluate(model, iterator, criterion):\n    model.eval()\n    epoch_loss = 0\n    batch_bleu = []\n    with torch.no_grad():\n        for i, batch in enumerate(iterator):\n            src = batch.src\n            trg = batch.trg\n            output = model(src, trg[:, :-1])\n            output_reshape = output.contiguous().view(-1, output.shape[-1])\n            trg = trg[:, 1:].contiguous().view(-1)\n\n            loss = criterion(output_reshape, trg)\n            epoch_loss += loss.item()\n\n            total_bleu = []\n            for j in range(batch_size):\n                try:\n                    trg_words = idx_to_word(batch.trg[j], loader.target.vocab)\n                    output_words = output[j].max(dim=1)[1]\n                    output_words = idx_to_word(output_words, loader.target.vocab)\n                    bleu = get_bleu(hypotheses=output_words.split(), reference=trg_words.split())\n                    total_bleu.append(bleu)\n                except:\n                    pass\n\n            total_bleu = sum(total_bleu) / len(total_bleu)\n            batch_bleu.append(total_bleu)\n\n    batch_bleu = sum(batch_bleu) / len(batch_bleu)\n    return epoch_loss / len(iterator), batch_bleu\n\n\ndef run(total_epoch, best_loss):\n    train_losses, test_losses, bleus = [], [], []\n    for step in range(total_epoch):\n        start_time = time.time()\n        train_loss = train(model, train_iter, optimizer, criterion, clip)\n        valid_loss, bleu = evaluate(model, valid_iter, criterion)\n        end_time = time.time()\n\n        if step > warmup:\n            scheduler.step(valid_loss)\n\n        train_losses.append(train_loss)\n        test_losses.append(valid_loss)\n        bleus.append(bleu)\n        epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n\n        if valid_loss < best_loss:\n            best_loss = valid_loss\n            torch.save(model.state_dict(), 'saved/model-{0}.pt'.format(valid_loss))\n\n        f = open('result/train_loss.txt', 'w')\n        f.write(str(train_losses))\n        f.close()\n\n        f = open('result/bleu.txt', 'w')\n        f.write(str(bleus))\n        f.close()\n\n        f = open('result/test_loss.txt', 'w')\n        f.write(str(test_losses))\n        f.close()\n\n        print(f'Epoch: {step + 1} | Time: {epoch_mins}m {epoch_secs}s')\n        print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n        print(f'\\tVal Loss: {valid_loss:.3f} |  Val PPL: {math.exp(valid_loss):7.3f}')\n        print(f'\\tBLEU Score: {bleu:.3f}')\n\n\nif __name__ == '__main__':\n    run(total_epoch=epoch, best_loss=inf)\n"
        },
        {
          "name": "util",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}