{
  "metadata": {
    "timestamp": 1736560411350,
    "page": 970,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjk4MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "facebookresearch/fairscale",
      "stars": 3226,
      "defaultBranch": "main",
      "files": [
        {
          "name": ".circleci",
          "type": "tree",
          "content": null
        },
        {
          "name": ".coveragerc",
          "type": "blob",
          "size": 0.0478515625,
          "content": "[run]\nomit =\n    docs/*\n    tests/*\n    setup.py\n"
        },
        {
          "name": ".editorconfig",
          "type": "blob",
          "size": 0.1865234375,
          "content": "root = true\n\n[*.py]\ncharset = utf-8\ntrim_trailing_whitespace = true\nend_of_line = lf\ninsert_final_newline = true\nindent_style = space\nindent_size = 4\n\n[*.md]\ntrim_trailing_whitespace = false\n"
        },
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.357421875,
          "content": "# Editor\n*~\n*.swp\n\n# IDEs\n.idea/\n\n# Testing\n*.pyc\n*.pyo\n.mypy_cache/\n*.egg-info/\n.testmondata\n\n# Build and release\nbuild/\ndist/\n.eggs/\n\n# Pytest verbose output\ntest-results/\n\n# Coverage reports\n.coverage\n.coverage.*\n./coverage.xml\n\n# Environments\n.env\n.venv\n.vscode\nenv/\nvenv/\nENV/\nenv.bak/\nvenv.bak/\n.vscode/*\n*.DS_Store\n\n# Data generated by tests\ncached_datasets/\n"
        },
        {
          "name": ".pre-commit-config.yaml",
          "type": "blob",
          "size": 1.3115234375,
          "content": "# Copyright (c) Facebook, Inc. and its affiliates. All rights reserved.\n#\n# This source code is licensed under the BSD license found in the\n# LICENSE file in the root directory of this source tree.\n\n# If you change the versions below, please make sure they are in-sync\n# with requirements-dev.txt\n\nexclude: 'build|stubs'\n\ndefault_language_version:\n    python: python3\n\nrepos:\n-   repo: https://github.com/pre-commit/pre-commit-hooks\n    rev: v4.0.1\n    hooks:\n    -   id: trailing-whitespace\n    -   id: check-ast\n    -   id: check-merge-conflict\n    -   id: check-added-large-files\n        args: ['--maxkb=500']\n    -   id: end-of-file-fixer\n\n-   repo: https://github.com/ambv/black\n    rev: 22.3.0\n    hooks:\n    - id: black\n\n-   repo: https://github.com/PyCQA/flake8\n    rev: 4.0.1\n    hooks:\n    -   id: flake8\n        args: [--show-source, --statistics]\n\n-   repo: https://github.com/pycqa/isort\n    rev: 5.10.1\n    hooks:\n    -   id: isort\n        exclude: README.md\n        additional_dependencies: [toml]\n\n-   repo: https://github.com/pre-commit/mirrors-mypy\n    rev: 'v0.910'\n    hooks:\n    -   id: mypy\n        args: [--no-strict-optional, --ignore-missing-imports, --scripts-are-modules, --pretty]\n        # See requirements-dev.txt for the reason for a fixed version of numpy here.\n        additional_dependencies: [numpy==1.21.5]\n"
        },
        {
          "name": ".readthedocs.yaml",
          "type": "blob",
          "size": 0.693359375,
          "content": "# .readthedocs.yaml\n# Read the Docs configuration file\n# See https://docs.readthedocs.io/en/stable/config-file/v2.html for details\n\n# Required\nversion: 2\n\n# We need python > 3.8 due to a dependency on numpy.\nbuild:\n  os: ubuntu-20.04\n  tools:\n    python: \"3.9\"\n    # You can also specify other tool versions:\n    # nodejs: \"16\"\n    # rust: \"1.55\"\n    # golang: \"1.17\"\n\n# Build documentation in the docs/ directory with Sphinx\nsphinx:\n   configuration: docs/source/conf.py\n\n# If using Sphinx, optionally build your docs in additional formats such as PDF\n# formats:\n#    - pdf\n\n# Optionally declare the Python requirements required to build your docs\npython:\n   install:\n   - requirements: docs/requirements.txt\n"
        },
        {
          "name": "CHANGELOG.md",
          "type": "blob",
          "size": 22.7275390625,
          "content": "# Changelog\nAll notable changes to this project will be documented in this file.\n\nThe format is based on [Keep a Changelog](https://keepachangelog.com/en/1.0.0/),\nand this project adheres to [Semantic Versioning](https://semver.org/spec/v2.0.0.html).\n\n\n## [0.4.11] - TBD\n\n- cleaned up some old issues and fixed a few bug in FSDP\n- removing SSD offload to simplify the FSDP code\n\n## [0.4.8]/[0.4.9]/[0.4.10]\n\n### Added\n\n- FSDP: Add pickle/unpickle support for SsdTensorHandle (and derived classes),\n  verified that FSDP models w/ ssd_offload enabled can correctly call model.state_dict()\n  and model.load_state_dict(...) and thus successfully checkpoint and recover parameters\n  stored as SsdFlatParameters. [#964]\n- FSDP Ssd Offload: [#974]\n   * add __setattr__ function for SsdTensorHandle/SsdFlatParameter to capture when .data is overridden\n     and perform necessary checks/updates before letting the tensor metadata be updated.\n   * There was a bug in pytorch core that caused re-assigning TensorSubclass .data to\n     disable the __torch_dispatch__ mechanism and effectively revert back to a normal\n     tensor. Since ssd_offload feature uses __torch_dispatch__ extensively and FSDP overrides\n     .data, ssd_offload now can only be used if pytorch version is 1.12.0 or later\n     (currently pytorch-nightly release). pytest tests are disabled and trying to import\n     ssd_offload or use FSDP with ssd_offload enabled will raise ImportError Exceptions.\n   * Enhance storage_state ON_CPU into ON_CPU_CLEAN and ON_CPU_DIRTY. ON_CPU_CLEAN\n     indicates that the .tensor value and the value stored on disk are identical, and no\n     writes are needed when calling .to_file(). ON_CPU_DIRTY indicates .tensor does not\n     match value stored on disk, and a write is necessary. This is disabled in FSDP as\n     it does not currently flush variables to disk as soon as optimizer modifies values.\n   * Fix detection in SsdTensorHandle __torch_dispatch__ when it is used in an in-place\n     operator, or as an output, then calling mark_dirty().\n   * Fix grad_fn on SsdFlatParameterViews so that both ssd_flat_param.view.grad_fn points\n     to a split and view on ssd_flat_parameter so that when X.backwards() is called. ssd_flat_param.grad\n     is properly updated in the backward pass.\n   * Add unit tests to verify grad_fn and backward hooks are called appropriately on\n     SsdFlatParameters/SsdFlatParameterViews\n   * Implement SsdFlatParameter.from_tensor() direct_to_file option. This allows creating\n     and SsdFlatParameter and directly writing it to disk, rather than creating a new tensor\n     first. This prevents another copy of tensors to be instantiated and can save memory when\n     creating very large SsdFlatParameters.\n   * Add SsdFlatParameterViewProperty for overriding a parameter in an nn.Module\n     similar to pytorch core's parameterization code path. This allows SsdFlatParameterViews\n     to be updated as SsdFlatParameter.data is overridden by replacing Layer.weight with a\n     property whose getter returns ssd_flat_param.views[view_id].\n   * Added example SsdFlatParameterViewParameterization on how the existing parameterization\n     method could be used instead of SsdFlatParameterViewProperty. But this method will result\n     in memory inefficiencies due to parameterization always keeping a copy of the original\n     parameter internally.\n\n### Fixed\n- fixed some bugs in FSDP related to supporting data2vec EMA modules.\n\n\n[0.4.6] - 2022-03-08\n\n### Added\n- CosFace's LMCL is added to MEVO. This is a loss function that is suitable\n  for large number of prediction target classes. It added normalization,\n  class separation margins and feature vector scaling to the standard\n  output projection + cross-entropy loss. MEVO supported this with its\n  memory saving techniques so that peak GPU memory is much reduced. [#916]\n- FSDP: Added skip_params_check_for_root flag to default_auto_wrap_policy which,\n  if set, wraps the root module regardless of how many unwrapped params there were\n  left after children were wrapped. [#930]\n- FSDP: Add support for saving optimizer state when using expert replicas with FSDP.\n- OSS: Add a new arg \"forced_broadcast_object\" to OSS __init__ to apply \"_broadcast_object\"\n  for rebuilding the sharded optimizer. [#937]\n- FSDP: Add an arg disable_reshard_on_root for FSDP __init__ [#878]\n\n### Fixed\n- FSDP: fixed handling of internal states with state_dict and load_state_dict\n  function so that they don't change lazy init state if training hasn't started. [#922]\n- FSDP: added support of optimizer state handling when some of the parameters are\n  not used. An example is that in a model with a EMA copy that doesn't get trained\n  but still wants to be sharded. [#922]\n\n## [0.4.5] - 2022-01-14\n\n### Added\n- Layer-wise Gradient Scaling [new feature][experimental] Layer-wise gradient\nscaling helps overcomes gradient overflow issues. When used in conjunction with\nmixed precision, it enables training larger models and makes the training\nprocess more stable, especially in deep networks [#879]\n- FSDP: Added process_group_reduce_scatter parameter to allow users to pass in the process group that is used for reduce scatter operation. [#897]\n- FSDP: Added state_dict_on_rank_0_only flag allow user choose to return full state dict on rank 0 and return empty dict non-rank 0 to prevent OOM [#844]\n\n### Changed\n- FSDP: Enabled reduce_scatter operation to overlap with all_gather stream and computation stream in backward propagation.\nThis change increases FSDP's throughput. To roll back this change, please pass ProcessGroupName.default to the process_group_reduce_scatter API parameter in FSDP [#897]\n\n### Fixed\n- FSDP: Fixed the issue that the padding size of the input tensor of the reduce scatter is not equal to the reduce scatter process group size # [#907]\n\n## [0.4.4] - 2021-12-21\n\n### Fixed\n- Inf/nan check on all gradient tensors in ShardedGradScaler [#890]\n- Allow user to supress warning on trainable_parameter in sharded_ddp.py [#886]\n- KeyError in certain FSDP wrapping scenarios [#881]\n\n### Added\n- Support eval mode in MEVO [#884]\n- A new benchmark for the MOE model [#866]\n\n### Changed\n- Fixed a corner case of FSDP init order and losing one of the flags [#880]\n- FSDP: Adding basic training support for SSD Offload, it now only supports flattened parameters. Renamed OffloadConfig.ssd_filepath_dir to more generic OffloadConfig.dir. SSD Offload remains an experimental feature. [#887]\n\n## [0.4.3] - 2021-11-18\n\n### Added\n- Sharded Grad Scaler works with cpu offload in mixed and full precision. [#831]\n- API for specifying SSD offload for params with FSDP. You can use a OffloadConfig to specify the type of offload\n  and the file path for storing params on SSD. Note: This is an experimental feature. [#855]\n\n### Changed\n- MEVO: fixed eval and checkpointing code paths [#851]\n- Cleanup: Moving forward we would be testing all of our code with Python 3.9.7, CUDA 11.2 and the following three versions of PyTorch [#847]:\n  - the most recent stable version\n  - the most recent LTS version\n  - a recent nightly build\n\n## [0.4.2] - 2021-11-08\n### Fixed\n- FSDP: Fixed an pre-backward hook bug for certain type of models and FSDP config. [#833]\n\n### Added\n- FSDP: Add support for SSD offload for eval workloads. This is a new experimental feature and should be\n        used with caution.\n- LayerwiseMemoryTracker[feature][experimental]: This is a new experimental tool to help track, visualize and suggest fix for memory issues occurring during the forward/backward pass of your models. [#808]\n- FSDP: limited support of shared weights between FSDP wrappers. This allows large parameter\n          and gradient memory to be sharded despite being needed from different layers due to\n          weight sharing. [#836]\n- OffloadModel: Fix node names to enable correct sharding in auto_shard.py [#830]\n- OSS: Relaxed speed and memory constraints on OSS golden data due to regression when we bumped up the\n       PyTorch version to 1.9. [#828] [#825]\n- Chore: Update PyTorch version that we run benchmarks with. [#823]\n- Chore: Update PyTorch version that we run test with. [#809]\n- OffloadModel: Extend auto_shard.py to allow dealing with conditionals automatically when tracing with\n                torch.fx. This will work for most cases except when the conditional is part of the root instance. [#817]\n- [MEVO]: a custom layer to help big vocab trainings. Experimental. Docs is still TBD. [#840]\n- SlowMoDistributedDataParallel[feature][experimental] - This is a distributed training wrapper which should be useful on clusters with slow network interconnects (eg Ethernet). This improves on performance as compared to Distributed Data Parallel in such clusters. [#378]\n\n## [0.4.1] - 2021-09-17\n### Fixed\n- FSDP: We don't attach post backward hooks for params that don't require grad. However in the hook\n        triggered after the post backward hook, we assert on the POST_BACKWARD state which can only\n        be set in the post backward hook. Modified the assert to account for the fact that the root\n        FSDP module can have child modules with params that require grad and it can contain params\n        that don't require grad and hence can fail the previous assert. [#761]\n- FSDP: Fixed a bug when multiple backward pass is called within an iteration, parameters' sharding\n        state might be incorrect. [#775]\n- activation checkpoint: Ensure outputs of checkpointed modules only require grad if either\n                         the input requires grad or if the parameters require grad. [#787]\n\n- OSS: fix the broadcast_fp16 option, broken after a refactor, this flag was doing nothing (bugfix).[#795]\n- OSS: update default device when refreshing the params, meaning that moving the model to GPU after\n       the OSS wrap will not trigger warnings and slow the jobs (ease of use). [#786]\n\n### Added\n- FSDP: Added support for returning the original names of parameters when `named_parameters` is called on\n        the module. To retrieve the orginal names of the parameters along with the params, you need to\n        call `named_parameters` under the `summon_full_params` context when using flattened params or original\n        params. If you are using original params (i.e flatten_params=False), calling `named_parameters` outside\n        of the `summon_full_params` context will still return the original param names along with the local shards. [#755]\n- FSDP: Ensure gradient reduction accumulates into the unsharded gradient tensor\n        within a backwards pass. This matters when an FSDP module is called\n        multiple times within a forward pass, and reduction is not deferred\n        using activation checkpoint forward counters, bucketing or some other\n        mechanism. [#784]\n- activation checkpoint: Added a context manager to disable checkpoint in case the same wrapped module\n                         needs to be checkpointed and not checkpointed in different parts of\n                         the module forward pass. [#772]\n- FSDP: Added a toggle with an environment variable ENABLE_NCCL_BASE_COLLECTIVES=[0,1] to allow users\n        enable/disable using new nccl base collecectives. By default, using new nccl base collectives\n        is enabled. [#801]\n\n## [0.4.0] - 2021-07-31\n### Fixed\n- FSDP: fixed final backward callback in certain activation checkpointed cases. Before this fix,\n        if a model is activation checkpointed in a certain way, the final backward\n        callback can fire incorrectly. That's due to autograd and reentrant backward\n        graphs. With this fix, the final callback is always registered on the outer\n        most root FSDP instance (i.e. the outer most backward graph), which result\n        in reliably firing. This makes FSDP much more robust with respect to different\n        models and activation checkpoints. [#753]\n\n### Added\n- FSDP: support gradient accumulation without the `no_sync` context. This is useful\n        in training with smaller number of GPU with same overall batch size as large\n        number of GPUs. Compared with the `no_sync` context, this mode consumes less\n        GPU memory but uses more networking bandwidth. [#752]\n\n\n## [0.3.9] - 2021-07-26\n### Fixed\n- FSDP: fixed metadata saving and shard consolidation for MoE cases. When a model has\n        shared parameters or mixture of expert layers, the handling of state dict\n        metadata was broken. This release fixes that. [#746]\n- OSS: fixed the buckets which would stay in fp16 if `broadcast fp16` was required [#751]\n\n### Added\n- FSDP: better performance; use `_allgather_base` and `_reduce_scatter_base` when they are\n        available from pytorch nightly version (will be in 1.10 releases) [#729]\n- FSDP: prepared FSDP internals for supporting multiple groups of flatten parameters (to support more general optimization) [#746]\n\n## [0.3.8] - 2021-07-12\n### Fixed\n- checkpointing: Use dummy tensor to ensure backward pass is called. [#701]\n- checkpointing: Ensure internal fwd counter is not incremented in eval mode. [#709]\n- checkpointing: Use non-blocking CPU transfer to improve perf. [#719]\n- FSDP: Fixed bug where buffers returned in `state_dict()` could still be half precision when `mixed_precision` is set to `True`. [#705]\n- FSDP: Ensure requires_grad of FlatParameter is consistent with requires_grad of the original parameters. [#721]\n- doc: Thoroughly improved the doc for FSDP. [#711]\n- cleanup: Remove examples/ doc from the repo. [#712]\n- cleanup: Future proof storage size test. [#735]\n- cleanup: Migrate away from legacy torchtext iterators. [#713]\n- chore: Updated torch 1.9 to release version. [#717]\n\n### Added\n- FSDP: supporting multiple flatten parameter groups [#708] [#711]\n- chore: Add the latest numpy version to requirements-test.txt to prevent mypy errors on certain PR commits [#732]\n\n## [0.3.7] - 2021-05-17\n### Fixed\n- setup.py: hide CUDA extensions behind `BUILD_CUDA_EXTENSIONS` envvar [#634]\n- checkpointing: rename and move the `checkpoint_activations` wrapper [#654]\n- FSDP: fix `local_state_dict` potentially called child class's `state_dict` [#574]\n- FSDP: fix extra process groups being created by default. Old behavior can cause excessive GPU memory usage [#678] [#681]\n- FSDP: fix forward pass not overlapping compute and allgather [#671]\n- FSDP: improved frozen weight support [#657]\n- FSDP: workaround AMP autocast cache issue with `clear_autocast_cache` flag [#650]\n- FSDP: Rename API arg `cpu_offload` to `move_params_to_cpu` to better reflect functionality. We will deprecate `cpu_offload` in an upcoming release [#676]\n- MoE: several fixes [#666] [#667] [#668]\n- SDP: re-expose the module property [#647]\n- wrap: support wrapping based on `wrapper_config` [#685]\n\n### Added\n- FSDP: added `force_input_to_fp32` flag for SyncBatchNorm [#659]\n- FSDP: better memory usage for reduce bucket [#633]\n- FSDP: added `local_metadata_dict` to save sharding relating information [#683]\n- FSDP: added `consolidate_shard_weights` to reconstruct the consolidated (non-sharded) model weights from saved sharded weights and metadata on the disk [#683]\n- Experimental SyncBatchNorm [#662] [#680]\n\n## [0.3.6] - 2021-04-26\n### Added\n- FSDP: Consolidate cpu\\_adam optimizer state dict ([#607](https://github.com/facebookresearch/fairscale/pull/607))\n\n### Fixed\n- FSDP: handle model with multiple forward pass and checkpoint ([#621](https://github.com/facebookresearch/fairscale/pull/621))\n- FSDP & SDP: check before calling `_specify_ddp_gpu_num` ([#626](https://github.com/facebookresearch/fairscale/pull/626))\n- FSDP: relax checking root condition ([#620](https://github.com/facebookresearch/fairscale/pull/620))\n- SDP: removing an assert which does not seem always accurate ([#625](https://github.com/facebookresearch/fairscale/pull/625))\n- FSDP: changing FSDP init to by pass pg validation ([#619](https://github.com/facebookresearch/fairscale/pull/619))\n- OSS: to 100% coverage ([#618](https://github.com/facebookresearch/fairscale/pull/618))\n\n## [0.3.5] - 2021-04-19\n### Added\n- [offload] Add API, tutorial and smaller doc string changes. ([#576](https://github.com/facebookresearch/fairscale/pull/576))\n\n### Fixed\n- FSDP: fixing training with freezing weights ([#614](https://github.com/facebookresearch/fairscale/pull/614))\n- SDP: privatizing all the things ([#611](https://github.com/facebookresearch/fairscale/pull/611))\n- FSDP: Make `_get_default_cuda_device` more robust to modules without params ([#606](https://github.com/facebookresearch/fairscale/pull/606))\n- OffloadModel: Add prev codepath of using OffloadModel without activation checkpointing ([#608](https://github.com/facebookresearch/fairscale/pull/608))\n\n## [0.3.4] - 2021-04-13\n### Added\n- FSDP: Add no broadcast optim state option ([#560](https://github.com/facebookresearch/fairscale/pull/560))\n\n### Fixed\n- ShardedDDP: Properly handle .eval() mode ([#587](https://github.com/facebookresearch/fairscale/pull/587))\n- ShardedDDP: Handle model being moved back to CPU prior to state consolidation ([#573](https://github.com/facebookresearch/fairscale/pull/573))\n- FSDP: much faster state consolidation ([#595](https://github.com/facebookresearch/fairscale/pull/595))\n- FSDP: Add gradient pre-dedivide to prevent overflow with large world sizes ([#565](https://github.com/facebookresearch/fairscale/pull/565))\n- Offload: (experimental) Fix activation offloading to CPU ([#588]((https://github.com/facebookresearch/fairscale/pull/588) )\n\n## [0.3.3] - 2021-04-1\n### Added\n- FSDP: changed `auto_wrap_bn` utility function so that single FSDP group is optional ([#556](https://github.com/facebookresearch/fairscale/pull/556))\n- FSDP: optimizer state load/save ([#537](https://github.com/facebookresearch/fairscale/pull/537))\n- FSDP: fix weight init when using apply() ([#543](https://github.com/facebookresearch/fairscale/pull/543))\n- Multiprocess Pipe: retired old implementation\n- Experimental: xpipe\n\n### Fixed\n- ShardedDDP deferred init ([#558](https://github.com/facebookresearch/fairscale/pull/558))\n\n## [0.3.2] - 2021-03-18\n### Added\n- Experimental: Add spectrain support ([#372](https://github.com/facebookresearch/fairscale/issues/372))\n- FSDP: enabled pytorch SyncBN (no asserting) ([#527](https://github.com/facebookresearch/fairscale/issues/527))\n- FSDP: added `auto_wrap_bn` utility function ([#531](https://github.com/facebookresearch/fairscale/pull/531))\n\n### Fixed\n- OSS: fix a compatibily problem with lightning wrt optimizer state dict ([#510](https://github.com/facebookresearch/fairscale/issues/510))\n- FSDP: fixed a bug when part of autograd graph is traversed multiple times in mixed precision mode ([#513](https://github.com/facebookresearch/fairscale/pull/513))\n\n## [0.3.1] - 2021-03-09\n### Added\n- FSDP docs ([#455](https://github.com/facebookresearch/fairscale/issues/455))\n- `enable_wrap` and `auto_wrap` APIs ([#446](https://github.com/facebookresearch/fairscale/issues/446))\n- Added experimental.nn.OffloadModel API for training large models on a single GPU.([#432](https://github.com/facebookresearch/fairscale/issues/432))\n\n### Fixed\n- OSS: fix a broken state dict when using non contiguous param groups\n- Several SDP fixes around performance and corner cases\n- Many FSDP fixes\n- AdaScale & SDP/FSDP test added but not officially supported\n\n## [0.3.0] - 2021-02-22\n### Added\n- FullyShardedDataParallel (FSDP) ([#413](https://github.com/facebookresearch/fairscale/issues/413))\n- ShardedDDP fp16 grad reduction option ([#402](https://github.com/facebookresearch/fairscale/issues/402))\n- Expose experimental algorithms within the pip package ([#410](https://github.com/facebookresearch/fairscale/pull/410))\n\n### Fixed\n- Catch corner case when the model is too small with respect to the world size, and shards are empty ([#406](https://github.com/facebookresearch/fairscale/pull/406))\n- Memory leak in `checkpoint_wrapper` ([#412](https://github.com/facebookresearch/fairscale/pull/412))\n\n## [0.1.7] - 2021-02-19\n### Fixed\n- ShardedDDP and OSS handle model trainability changes during training ([#369](https://github.com/facebookresearch/fairscale/issues/369))\n- ShardedDDP state dict load/save bug ([#386](https://github.com/facebookresearch/fairscale/issues/386))\n- ShardedDDP handle train/eval modes ([#393](https://github.com/facebookresearch/fairscale/issues/393))\n- AdaScale handling custom scaling factors ([#401](https://github.com/facebookresearch/fairscale/issues/401))\n\n### Added\n- ShardedDDP manual reduce option for checkpointing ([#389](https://github.com/facebookresearch/fairscale/issues/389))\n\n## [0.1.6] - 2021-02-10\n### Added\n- Checkpointing model wrapper (#376)\n- Faster OSS, flatbuffers (#371)\n- Small speedup in OSS clipgradnorm (#363)\n\n### Fixed\n- Bug in ShardedDDP with 0.1.5 depending the init (KeyError / OSS)\n- Much refactoring in Pipe (#357, #358, #360, #362, #370, #373)\n- Better pip integration / resident pytorch (#375)\n\n## [0.1.5] - 2021-02-03\n### Added\n- Pytorch compatibility for OSS checkpoints (#310)\n- Elastic checkpoints for OSS, world size can vary in between save and loads (#310)\n- Tensor views for OSS bucketing, reduced CPU use (#300)\n- Bucket calls in ShardedDDP, for faster inter node communications (#327)\n- FlattenParamWrapper, which flattens module parameters into a single tensor seamlessly (#317)\n- AMPnet experimental support (#304)\n\n### Fixed\n- ShardedDDP properly handles device changes via `.to()` (#353)\n- Add a new interface for AdaScale, AdaScaleWrapper, which makes it compatible with OSS (#347)\n\n\n## [0.1.4] - 2021-01-07\n### Fixed\n- Missing cu files in the pip package\n\n\n## [0.1.3] - 2021-01-04\n### Fixed\n- Release numbering within python and from pypi\n\n## [0.1.2] - 2021-01-04\n### Added\n- AdaScale:\n  . Added gradient accumulation feature (#202)\n  . Added support of `torch.lr_scheduler` (#229)\n  . Added support for `add_param_groups` (#266)\n  . Added support for `scale != world_size` (#266)\n\n### Fixed\n- AdaScale: smoothing factor value fixed when using gradient accumulation (#235)\n- Pipe: documentation on balancing functions (#243)\n- ShardedDDP: handle typical NLP models\n- ShardedDDP: better partitioning when finetuning\n\n\n## [0.1.1] - 2020-12-01\n### Fixed\n- make sure pip package includes header files (#221)\n\n## [0.1.0] - 2020-12-01\n### Added\n- ShardedDataParallel with autoreduce (#157)\n- cpu support for Pipe (#188)\n- ShardedOptim: Distributed Grad Scaler (for torch AMP)  (#182)\n- OSS-aware clip grads, bridge sharded states (#167)\n- oss: add `rank_local_state_dict` staticmethod (#174)\n- support for PyTorch 1.7.0 (#171)\n- Add implementation of AdaScale (#139)\n\n### Fixed\n- pip package install (#196, #200)\n\n## [0.0.3] - 2020-10-14\n### Added\n- multi-process pipe\n\n### Fixed\n- multiple OSS fixes\n- MegaTron+OSS DDP fix\n\n## [0.0.2] - 2020-08-28\n### Added\n- add ddp that works with oss with `reduce()` not `all_reduce()` (#19)\n- support for PyTorch v1.6\n- add mixed precision Adam (#40)\n- Adam optimizer state scaling (#44)\n\n### Fixed\n- properly restore a sharded optim state (#39)\n- OSS restore state to proper device (#46)\n- optim/oss: support optimizers with additional step kwargs (#53)\n- optim/oss: fix state cast (#56)\n- fix eval for `oss_ddp` (#55)\n- optim/oss: work correctly with LRScheduler (#58)\n\n## [0.0.1] - 2020-07-31\n- Initial release.\n"
        },
        {
          "name": "CODE_OF_CONDUCT.md",
          "type": "blob",
          "size": 3.1806640625,
          "content": "# Open Source Code of Conduct\n\n## Our Pledge\n\nIn the interest of fostering an open and welcoming environment, we as contributors and maintainers pledge to make participation in our project and our community a harassment-free experience for everyone, regardless of age, body size, disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, religion, or sexual identity and orientation.\n\n## Our Standards\n\nExamples of behavior that contributes to creating a positive environment include:\n\n* Using welcoming and inclusive language\n* Being respectful of differing viewpoints and experiences\n* Gracefully accepting constructive criticism\n* Focusing on what is best for the community\n* Showing empathy towards other community members\n\nExamples of unacceptable behavior by participants include:\n\n* The use of sexualized language or imagery and unwelcome sexual attention or advances\n* Trolling, insulting/derogatory comments, and personal or political attacks\n* Public or private harassment\n* Publishing others’ private information, such as a physical or electronic address, without explicit permission\n* Other conduct which could reasonably be considered inappropriate in a professional setting\n\n## Our Responsibilities\n\nProject maintainers are responsible for clarifying the standards of acceptable behavior and are expected to take appropriate and fair corrective action in response to any instances of unacceptable behavior.\n\nProject maintainers have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, or to ban temporarily or permanently any contributor for other behaviors that they deem inappropriate, threatening, offensive, or harmful.\n\n## Scope\n\nThis Code of Conduct applies within all project spaces, and it also applies when an individual is representing the project or its community in public spaces. Examples of representing a project or community include using an official project e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event. Representation of a project may be further defined and clarified by project maintainers.\n\n## Enforcement\n\nInstances of abusive, harassing, or otherwise unacceptable behavior may be reported by contacting the project team at opensource-conduct@fb.com. All complaints will be reviewed and investigated and will result in a response that is deemed necessary and appropriate to the circumstances. The project team is obligated to maintain confidentiality with regard to the reporter of an incident. Further details of specific enforcement policies may be posted separately.\n\nProject maintainers who do not follow or enforce the Code of Conduct in good faith may face temporary or permanent repercussions as determined by other members of the project’s leadership.\n\n## Attribution\n\nThis Code of Conduct is adapted from the [Contributor Covenant][homepage], version 1.4,\navailable at https://www.contributor-covenant.org/version/1/4/code-of-conduct.html\n\n[homepage]: https://www.contributor-covenant.org\n"
        },
        {
          "name": "CONTRIBUTING.md",
          "type": "blob",
          "size": 4.984375,
          "content": "# Contributing to FairScale\n\nWe want to make contributing to this project as easy and transparent as\npossible.\n\n## Our Development Process\n\nMinor changes and improvements will be released on an ongoing basis. Larger\nchanges (e.g., changesets implementing a new paper) will be released on a\nmore periodic basis.\n\n## Pull Requests\n\nWe actively welcome your pull requests.\n\n1. Fork the repo and create your branch from `main`.\n2. If you've added code that should be tested, add tests.\n3. If you've changed APIs, update the documentation.\n4. Ensure the test suite passes.\n5. Make sure your code passes static analysis (see below).\n6. If you haven't already, complete the Contributor License Agreement (\"CLA\").\n\n## Contributor License Agreement (\"CLA\")\n\nIn order to accept your pull request, we need you to submit a CLA. You only need\nto do this once to work on any of Facebook's open source projects.\n\nComplete your CLA here: <https://code.facebook.com/cla>\n\n## Issues\n\nWe use GitHub issues to track public bugs. Please ensure your description is\nclear and has sufficient instructions to be able to reproduce the issue.\n\nFacebook has a [bounty program](https://www.facebook.com/whitehat/) for the safe\ndisclosure of security bugs. In those cases, please go through the process\noutlined on that page and do not file a public issue.\n\n## Environment setup\n\n```\n~$ python3 -m venv venv\n~$ source venv2/bin/activate\n(venv2) ~$ cd git/fairscale/\n(venv2) ~/git/fairscale $ pip3 install -r requirements-dev.txt\n```\n\n## Coding Style\n\n* We follow the [PEP8](https://www.python.org/dev/peps/pep-0008/) style guide.\n* In your editor, install the [editorconfig](https://editorconfig.org/) extension\n  which should ensure that you are following the same standards as us.\n* Please read the [editorconfig](.editorconfig) file to understand the exact coding style preferences.\n* Please place Python code related to models in fairscale/nn. Place Python code related to optimizers\n  in fairscale/optim. Place C++ extensions in fairscale/clib.\n* Please put `__all__:List[str] = []` in new `__init__.py` files for consistent importing behavior\n  and less development overhead in maintaining an importing list.\n* Please setup pre-commit before opening up your PR.\n\n### Pre-Commit (Recommended)\n\nWe use pre-commit to maintain the coding style. Pre-Commit checks are run via Github Actions on every\ncommit. To install all the relevant libraries and run the pre-commit tests locally, execute the following\ncommands:\n\n```\npip install -r requirements-dev.txt\npre-commit install\n```\n\nAfter the above, your `git commit` command will automatically trigger pre-commit checks.\n\n### Running static code analysis manually (Deprecated)\n\nNote that, trailing spaces are not checked by the manual commands below, but they are checked by the\npre-commit hooks we use above.\n\n```\nblack .\nisort .\nflake8\nmypy --ignore-missing-imports --scripts-are-modules --pretty .\n```\n\n## Testing\n\nFairScale code is tested on Python 3.9.7, CUDA 11.2 and the following three PyTorch versions:\n- the latest stable version\n- the latest LTS version\n- a recent nightly release\n\nSee the [README](https://github.com/facebookresearch/fairscale/blob/main/README.md#testing) for the exact version numbers.\n\n### Unit tests\n\n```\npytest\n# single test\npython -m pytest tests/nn/data_parallel/test_oss_ddp.py::test_on_cpu\n```\n\n### Check test coverage\n\n```\npython -m pytest --cov-report term --cov=fairscale/nn/data_parallel \\\n   tests/nn/data_parallel/test_oss_ddp.py::test_on_cpu\n```\n\n### CircleCI status\n\nFrom your PR page, you can expand on the CircleCI results. For GPU test, you should see\nwhat CI has run, like:\n\n```\n...\n----- generated xml file: /home/circleci/fairscale/test-results/junit.xml ------\n================== 217 passed, 2 xfailed in 218.74s (0:03:38) ==================\nCircleCI received exit code 0\n```\n\nThe number of passed and failed should give you an idea on whether your local\ntest was the same or not.\n\n## Commit Guidelines\n\nWe follow the same guidelines as AngularJS. Each commit message consists of a **header**,\na **body** and a **footer**.  The header has a special format that includes a **type**,\nand a **subject**:\n\n```\n[<type>] <subject>\n<BLANK LINE>\n<body>\n<BLANK LINE>\n<footer>\n```\n\nAny line of the commit message cannot be longer 100 characters! This allows the message to be easier\nto read on github as well as in various git tools.\n\n### Type\n\nMust be one of the following:\n\n* **feat**: A new feature\n* **fix**: A bug fix\n* **cleanup**: Changes that do not affect the meaning of the code (white-space, formatting, missing\n  semi-colons, dead code removal etc.)\n* **refactor**: A code change that neither fixes a bug or adds a feature\n* **perf**: A code change that improves performance\n* **test**: Adding missing tests or fixing them\n* **chore**: Changes to the build process or auxiliary tools and libraries such as documentation\ngeneration\n* **docs**: Documentation only changes\n\n## License\n\nBy contributing to fairscale, you agree that your contributions will be licensed\nunder the LICENSE file in the root directory of this source tree.\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 1.6982421875,
          "content": "From fairscale:\n\nCopyright (c) Facebook, Inc. and its affiliates\n\n===\n\nFrom torchgpipe (fairscale/nn/pipe):\n\nCopyright 2019 Kakao Brain\n\nAll contributions by Facebook:\nCopyright (c) Facebook, Inc. and its affiliates\n\n===\n\nRedistribution and use in source and binary forms, with or without\nmodification, are permitted provided that the following conditions are met:\n\n1. Redistributions of source code must retain the above copyright\n   notice, this list of conditions and the following disclaimer.\n\n2. Redistributions in binary form must reproduce the above copyright\n   notice, this list of conditions and the following disclaimer in the\n   documentation and/or other materials provided with the distribution.\n\n3. Neither the names of Facebook, Deepmind Technologies, NYU, NEC Laboratories America\n   and IDIAP Research Institute nor the names of its contributors may be\n   used to endorse or promote products derived from this software without\n   specific prior written permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\nAND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\nIMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\nARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE\nLIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\nCONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\nSUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\nINTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\nCONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\nARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\nPOSSIBILITY OF SUCH DAMAGE.\n"
        },
        {
          "name": "MANIFEST.in",
          "type": "blob",
          "size": 0.0771484375,
          "content": "include LICENSE\ninclude requirements.txt\nrecursive-include fairscale *.h *.cuh\n"
        },
        {
          "name": "NOTICE",
          "type": "blob",
          "size": 26.650390625,
          "content": "===============================================================================\ntorchgpipe's Apache License 2.0\n===============================================================================\n\n                                 Apache License\n                           Version 2.0, January 2004\n                        http://www.apache.org/licenses/\n\n   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n\n   1. Definitions.\n\n      \"License\" shall mean the terms and conditions for use, reproduction,\n      and distribution as defined by Sections 1 through 9 of this document.\n\n      \"Licensor\" shall mean the copyright owner or entity authorized by\n      the copyright owner that is granting the License.\n\n      \"Legal Entity\" shall mean the union of the acting entity and all\n      other entities that control, are controlled by, or are under common\n      control with that entity. For the purposes of this definition,\n      \"control\" means (i) the power, direct or indirect, to cause the\n      direction or management of such entity, whether by contract or\n      otherwise, or (ii) ownership of fifty percent (50%) or more of the\n      outstanding shares, or (iii) beneficial ownership of such entity.\n\n      \"You\" (or \"Your\") shall mean an individual or Legal Entity\n      exercising permissions granted by this License.\n\n      \"Source\" form shall mean the preferred form for making modifications,\n      including but not limited to software source code, documentation\n      source, and configuration files.\n\n      \"Object\" form shall mean any form resulting from mechanical\n      transformation or translation of a Source form, including but\n      not limited to compiled object code, generated documentation,\n      and conversions to other media types.\n\n      \"Work\" shall mean the work of authorship, whether in Source or\n      Object form, made available under the License, as indicated by a\n      copyright notice that is included in or attached to the work\n      (an example is provided in the Appendix below).\n\n      \"Derivative Works\" shall mean any work, whether in Source or Object\n      form, that is based on (or derived from) the Work and for which the\n      editorial revisions, annotations, elaborations, or other modifications\n      represent, as a whole, an original work of authorship. For the purposes\n      of this License, Derivative Works shall not include works that remain\n      separable from, or merely link (or bind by name) to the interfaces of,\n      the Work and Derivative Works thereof.\n\n      \"Contribution\" shall mean any work of authorship, including\n      the original version of the Work and any modifications or additions\n      to that Work or Derivative Works thereof, that is intentionally\n      submitted to Licensor for inclusion in the Work by the copyright owner\n      or by an individual or Legal Entity authorized to submit on behalf of\n      the copyright owner. For the purposes of this definition, \"submitted\"\n      means any form of electronic, verbal, or written communication sent\n      to the Licensor or its representatives, including but not limited to\n      communication on electronic mailing lists, source code control systems,\n      and issue tracking systems that are managed by, or on behalf of, the\n      Licensor for the purpose of discussing and improving the Work, but\n      excluding communication that is conspicuously marked or otherwise\n      designated in writing by the copyright owner as \"Not a Contribution.\"\n\n      \"Contributor\" shall mean Licensor and any individual or Legal Entity\n      on behalf of whom a Contribution has been received by Licensor and\n      subsequently incorporated within the Work.\n\n   2. Grant of Copyright License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      copyright license to reproduce, prepare Derivative Works of,\n      publicly display, publicly perform, sublicense, and distribute the\n      Work and such Derivative Works in Source or Object form.\n\n   3. Grant of Patent License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      (except as stated in this section) patent license to make, have made,\n      use, offer to sell, sell, import, and otherwise transfer the Work,\n      where such license applies only to those patent claims licensable\n      by such Contributor that are necessarily infringed by their\n      Contribution(s) alone or by combination of their Contribution(s)\n      with the Work to which such Contribution(s) was submitted. If You\n      institute patent litigation against any entity (including a\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\n      or a Contribution incorporated within the Work constitutes direct\n      or contributory patent infringement, then any patent licenses\n      granted to You under this License for that Work shall terminate\n      as of the date such litigation is filed.\n\n   4. Redistribution. You may reproduce and distribute copies of the\n      Work or Derivative Works thereof in any medium, with or without\n      modifications, and in Source or Object form, provided that You\n      meet the following conditions:\n\n      (a) You must give any other recipients of the Work or\n          Derivative Works a copy of this License; and\n\n      (b) You must cause any modified files to carry prominent notices\n          stating that You changed the files; and\n\n      (c) You must retain, in the Source form of any Derivative Works\n          that You distribute, all copyright, patent, trademark, and\n          attribution notices from the Source form of the Work,\n          excluding those notices that do not pertain to any part of\n          the Derivative Works; and\n\n      (d) If the Work includes a \"NOTICE\" text file as part of its\n          distribution, then any Derivative Works that You distribute must\n          include a readable copy of the attribution notices contained\n          within such NOTICE file, excluding those notices that do not\n          pertain to any part of the Derivative Works, in at least one\n          of the following places: within a NOTICE text file distributed\n          as part of the Derivative Works; within the Source form or\n          documentation, if provided along with the Derivative Works; or,\n          within a display generated by the Derivative Works, if and\n          wherever such third-party notices normally appear. The contents\n          of the NOTICE file are for informational purposes only and\n          do not modify the License. You may add Your own attribution\n          notices within Derivative Works that You distribute, alongside\n          or as an addendum to the NOTICE text from the Work, provided\n          that such additional attribution notices cannot be construed\n          as modifying the License.\n\n      You may add Your own copyright statement to Your modifications and\n      may provide additional or different license terms and conditions\n      for use, reproduction, or distribution of Your modifications, or\n      for any such Derivative Works as a whole, provided Your use,\n      reproduction, and distribution of the Work otherwise complies with\n      the conditions stated in this License.\n\n   5. Submission of Contributions. Unless You explicitly state otherwise,\n      any Contribution intentionally submitted for inclusion in the Work\n      by You to the Licensor shall be under the terms and conditions of\n      this License, without any additional terms or conditions.\n      Notwithstanding the above, nothing herein shall supersede or modify\n      the terms of any separate license agreement you may have executed\n      with Licensor regarding such Contributions.\n\n   6. Trademarks. This License does not grant permission to use the trade\n      names, trademarks, service marks, or product names of the Licensor,\n      except as required for reasonable and customary use in describing the\n      origin of the Work and reproducing the content of the NOTICE file.\n\n   7. Disclaimer of Warranty. Unless required by applicable law or\n      agreed to in writing, Licensor provides the Work (and each\n      Contributor provides its Contributions) on an \"AS IS\" BASIS,\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n      implied, including, without limitation, any warranties or conditions\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n      PARTICULAR PURPOSE. You are solely responsible for determining the\n      appropriateness of using or redistributing the Work and assume any\n      risks associated with Your exercise of permissions under this License.\n\n   8. Limitation of Liability. In no event and under no legal theory,\n      whether in tort (including negligence), contract, or otherwise,\n      unless required by applicable law (such as deliberate and grossly\n      negligent acts) or agreed to in writing, shall any Contributor be\n      liable to You for damages, including any direct, indirect, special,\n      incidental, or consequential damages of any character arising as a\n      result of this License or out of the use or inability to use the\n      Work (including but not limited to damages for loss of goodwill,\n      work stoppage, computer failure or malfunction, or any and all\n      other commercial damages or losses), even if such Contributor\n      has been advised of the possibility of such damages.\n\n   9. Accepting Warranty or Additional Liability. While redistributing\n      the Work or Derivative Works thereof, You may choose to offer,\n      and charge a fee for, acceptance of support, warranty, indemnity,\n      or other liability obligations and/or rights consistent with this\n      License. However, in accepting such obligations, You may act only\n      on Your own behalf and on Your sole responsibility, not on behalf\n      of any other Contributor, and only if You agree to indemnify,\n      defend, and hold each Contributor harmless for any liability\n      incurred by, or claims asserted against, such Contributor by reason\n      of your accepting any such warranty or additional liability.\n\n   END OF TERMS AND CONDITIONS\n\n   APPENDIX: How to apply the Apache License to your work.\n\n      To apply the Apache License to your work, attach the following\n      boilerplate notice, with the fields enclosed by brackets \"[]\"\n      replaced with your own identifying information. (Don't include\n      the brackets!)  The text should be enclosed in the appropriate\n      comment syntax for the file format. We also recommend that a\n      file or class name and description of purpose be included on the\n      same \"printed page\" as the copyright notice for easier\n      identification within third-party archives.\n\n   Copyright 2019 Kakao Brain\n\n   Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.\n\n===============================================================================\nLicense from NVIDIA Megatron\n===============================================================================\n\nThe following applies to all files unless otherwise noted:\n\n# Copyright (c) 2020, NVIDIA CORPORATION. All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions\n# are met:\n#  * Redistributions of source code must retain the above copyright\n#    notice, this list of conditions and the following disclaimer.\n#  * Redistributions in binary form must reproduce the above copyright\n#    notice, this list of conditions and the following disclaimer in the\n#    documentation and/or other materials provided with the distribution.\n#  * Neither the name of NVIDIA CORPORATION nor the names of its\n#    contributors may be used to endorse or promote products derived\n#    from this software without specific prior written permission.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY\n# EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR\n# PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY\n# OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n# (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n# OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\n--\n\nThis repository also contains code from Hugging Face Inc., Google Research,\nand Facebook (from their Fairseq project). Files from these\norganizations have notices at the top of each file. Below are licenses\nused in those files, as indicated.\n\n\n------------- LICENSE FOR huggingface and Google Research code  --------------\n\n\n                                 Apache License\n                           Version 2.0, January 2004\n                        http://www.apache.org/licenses/\n\n   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n\n   1. Definitions.\n\n      \"License\" shall mean the terms and conditions for use, reproduction,\n      and distribution as defined by Sections 1 through 9 of this document.\n\n      \"Licensor\" shall mean the copyright owner or entity authorized by\n      the copyright owner that is granting the License.\n\n      \"Legal Entity\" shall mean the union of the acting entity and all\n      other entities that control, are controlled by, or are under common\n      control with that entity. For the purposes of this definition,\n      \"control\" means (i) the power, direct or indirect, to cause the\n      direction or management of such entity, whether by contract or\n      otherwise, or (ii) ownership of fifty percent (50%) or more of the\n      outstanding shares, or (iii) beneficial ownership of such entity.\n\n      \"You\" (or \"Your\") shall mean an individual or Legal Entity\n      exercising permissions granted by this License.\n\n      \"Source\" form shall mean the preferred form for making modifications,\n      including but not limited to software source code, documentation\n      source, and configuration files.\n\n      \"Object\" form shall mean any form resulting from mechanical\n      transformation or translation of a Source form, including but\n      not limited to compiled object code, generated documentation,\n      and conversions to other media types.\n\n      \"Work\" shall mean the work of authorship, whether in Source or\n      Object form, made available under the License, as indicated by a\n      copyright notice that is included in or attached to the work\n      (an example is provided in the Appendix below).\n\n      \"Derivative Works\" shall mean any work, whether in Source or Object\n      form, that is based on (or derived from) the Work and for which the\n      editorial revisions, annotations, elaborations, or other modifications\n      represent, as a whole, an original work of authorship. For the purposes\n      of this License, Derivative Works shall not include works that remain\n      separable from, or merely link (or bind by name) to the interfaces of,\n      the Work and Derivative Works thereof.\n\n      \"Contribution\" shall mean any work of authorship, including\n      the original version of the Work and any modifications or additions\n      to that Work or Derivative Works thereof, that is intentionally\n      submitted to Licensor for inclusion in the Work by the copyright owner\n      or by an individual or Legal Entity authorized to submit on behalf of\n      the copyright owner. For the purposes of this definition, \"submitted\"\n      means any form of electronic, verbal, or written communication sent\n      to the Licensor or its representatives, including but not limited to\n      communication on electronic mailing lists, source code control systems,\n      and issue tracking systems that are managed by, or on behalf of, the\n      Licensor for the purpose of discussing and improving the Work, but\n      excluding communication that is conspicuously marked or otherwise\n      designated in writing by the copyright owner as \"Not a Contribution.\"\n\n      \"Contributor\" shall mean Licensor and any individual or Legal Entity\n      on behalf of whom a Contribution has been received by Licensor and\n      subsequently incorporated within the Work.\n\n   2. Grant of Copyright License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      copyright license to reproduce, prepare Derivative Works of,\n      publicly display, publicly perform, sublicense, and distribute the\n      Work and such Derivative Works in Source or Object form.\n\n   3. Grant of Patent License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      (except as stated in this section) patent license to make, have made,\n      use, offer to sell, sell, import, and otherwise transfer the Work,\n      where such license applies only to those patent claims licensable\n      by such Contributor that are necessarily infringed by their\n      Contribution(s) alone or by combination of their Contribution(s)\n      with the Work to which such Contribution(s) was submitted. If You\n      institute patent litigation against any entity (including a\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\n      or a Contribution incorporated within the Work constitutes direct\n      or contributory patent infringement, then any patent licenses\n      granted to You under this License for that Work shall terminate\n      as of the date such litigation is filed.\n\n   4. Redistribution. You may reproduce and distribute copies of the\n      Work or Derivative Works thereof in any medium, with or without\n      modifications, and in Source or Object form, provided that You\n      meet the following conditions:\n\n      (a) You must give any other recipients of the Work or\n          Derivative Works a copy of this License; and\n\n      (b) You must cause any modified files to carry prominent notices\n          stating that You changed the files; and\n\n      (c) You must retain, in the Source form of any Derivative Works\n          that You distribute, all copyright, patent, trademark, and\n          attribution notices from the Source form of the Work,\n          excluding those notices that do not pertain to any part of\n          the Derivative Works; and\n\n      (d) If the Work includes a \"NOTICE\" text file as part of its\n          distribution, then any Derivative Works that You distribute must\n          include a readable copy of the attribution notices contained\n          within such NOTICE file, excluding those notices that do not\n          pertain to any part of the Derivative Works, in at least one\n          of the following places: within a NOTICE text file distributed\n          as part of the Derivative Works; within the Source form or\n          documentation, if provided along with the Derivative Works; or,\n          within a display generated by the Derivative Works, if and\n          wherever such third-party notices normally appear. The contents\n          of the NOTICE file are for informational purposes only and\n          do not modify the License. You may add Your own attribution\n          notices within Derivative Works that You distribute, alongside\n          or as an addendum to the NOTICE text from the Work, provided\n          that such additional attribution notices cannot be construed\n          as modifying the License.\n\n      You may add Your own copyright statement to Your modifications and\n      may provide additional or different license terms and conditions\n      for use, reproduction, or distribution of Your modifications, or\n      for any such Derivative Works as a whole, provided Your use,\n      reproduction, and distribution of the Work otherwise complies with\n      the conditions stated in this License.\n\n   5. Submission of Contributions. Unless You explicitly state otherwise,\n      any Contribution intentionally submitted for inclusion in the Work\n      by You to the Licensor shall be under the terms and conditions of\n      this License, without any additional terms or conditions.\n      Notwithstanding the above, nothing herein shall supersede or modify\n      the terms of any separate license agreement you may have executed\n      with Licensor regarding such Contributions.\n\n   6. Trademarks. This License does not grant permission to use the trade\n      names, trademarks, service marks, or product names of the Licensor,\n      except as required for reasonable and customary use in describing the\n      origin of the Work and reproducing the content of the NOTICE file.\n\n   7. Disclaimer of Warranty. Unless required by applicable law or\n      agreed to in writing, Licensor provides the Work (and each\n      Contributor provides its Contributions) on an \"AS IS\" BASIS,\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n      implied, including, without limitation, any warranties or conditions\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n      PARTICULAR PURPOSE. You are solely responsible for determining the\n      appropriateness of using or redistributing the Work and assume any\n      risks associated with Your exercise of permissions under this License.\n\n   8. Limitation of Liability. In no event and under no legal theory,\n      whether in tort (including negligence), contract, or otherwise,\n      unless required by applicable law (such as deliberate and grossly\n      negligent acts) or agreed to in writing, shall any Contributor be\n      liable to You for damages, including any direct, indirect, special,\n      incidental, or consequential damages of any character arising as a\n      result of this License or out of the use or inability to use the\n      Work (including but not limited to damages for loss of goodwill,\n      work stoppage, computer failure or malfunction, or any and all\n      other commercial damages or losses), even if such Contributor\n      has been advised of the possibility of such damages.\n\n   9. Accepting Warranty or Additional Liability. While redistributing\n      the Work or Derivative Works thereof, You may choose to offer,\n      and charge a fee for, acceptance of support, warranty, indemnity,\n      or other liability obligations and/or rights consistent with this\n      License. However, in accepting such obligations, You may act only\n      on Your own behalf and on Your sole responsibility, not on behalf\n      of any other Contributor, and only if You agree to indemnify,\n      defend, and hold each Contributor harmless for any liability\n      incurred by, or claims asserted against, such Contributor by reason\n      of your accepting any such warranty or additional liability.\n\n   END OF TERMS AND CONDITIONS\n\n   APPENDIX: How to apply the Apache License to your work.\n\n      To apply the Apache License to your work, attach the following\n      boilerplate notice, with the fields enclosed by brackets \"[]\"\n      replaced with your own identifying information. (Don't include\n      the brackets!)  The text should be enclosed in the appropriate\n      comment syntax for the file format. We also recommend that a\n      file or class name and description of purpose be included on the\n      same \"printed page\" as the copyright notice for easier\n      identification within third-party archives.\n\n   Copyright [yyyy] [name of copyright owner]\n\n   Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.\n\n------------- LICENSE FOR Facebook Fairseq code --------------\n\nMIT License\n\nCopyright (c) Facebook, Inc. and its affiliates.\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n\n------------- LICENSE FOR FlattenParamsWrapper --------------\n\nMIT License\n\nCopyright (c) 2018 Tongzhou Wang\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 4.638671875,
          "content": "![FairScale Logo](./docs/source/_static/img/fairscale-logo.png)\n\n[![Support Ukraine](https://img.shields.io/badge/Support-Ukraine-FFD500?style=flat&labelColor=005BBB)](https://opensource.facebook.com/support-ukraine)\n![PyPI](https://img.shields.io/pypi/v/fairscale)\n[![Documentation Status](https://readthedocs.org/projects/fairscale/badge/?version=latest)](https://fairscale.readthedocs.io/en/latest/?badge=latest)\n[![CircleCI](https://circleci.com/gh/facebookresearch/fairscale.svg?style=shield)](https://app.circleci.com/pipelines/github/facebookresearch/fairscale/) ![PyPI - License](https://img.shields.io/pypi/l/fairscale) [![Downloads](https://pepy.tech/badge/fairscale)](https://pepy.tech/project/fairscale) [![PRs Welcome](https://img.shields.io/badge/PRs-welcome-brightgreen.svg)](https://github.com/facebookresearch/fairscale/blob/main/CONTRIBUTING.md)\n--------------------------------------------------------------------------------\n\n## Description\nFairScale is a PyTorch extension library for high performance and large scale training.\nThis library extends basic PyTorch capabilities while adding new SOTA scaling techniques.\nFairScale makes available the latest distributed training techniques in the form of composable\nmodules and easy to use APIs. These APIs are a fundamental part of a researcher's toolbox as\nthey attempt to scale models with limited resources.\n\nFairScale was designed with the following values in mind:\n\n* **Usability** -  Users should be able to understand and use FairScale APIs with minimum cognitive overload.\n\n* **Modularity** - Users should be able to combine multiple FairScale APIs as part of their training loop seamlessly.\n\n* **Performance** - FairScale APIs provide the best performance in terms of scaling and efficiency.\n\n## Watch Introductory Video\n\n[![Explain Like I’m 5: FairScale](https://img.youtube.com/vi/oDt7ebOwWIc/0.jpg)](https://www.youtube.com/watch?v=oDt7ebOwWIc)\n\n## Installation\n\nTo install FairScale, please see the following [instructions](https://github.com/facebookresearch/fairscale/blob/main/docs/source/installation_instructions.rst).\nYou should be able to install a package with pip or conda, or build directly from source.\n\n## Getting Started\nThe full [documentation](https://fairscale.readthedocs.io/) contains instructions for getting started, deep dives and tutorials about the various FairScale APIs.\n\n## FSDP\n\nFullyShardedDataParallel (FSDP) is the recommended method for scaling to large NN models.\nThis library has been [upstreamed to PyTorch](https://pytorch.org/blog/introducing-pytorch-fully-sharded-data-parallel-api/).\nThe version of FSDP here is for historical references as well as for experimenting with\nnew and crazy ideas in research of scaling techniques. Please see the following blog\nfor [how to use FairScale FSDP and how does it work](https://engineering.fb.com/2021/07/15/open-source/fsdp/).\n\n## Testing\n\nWe use circleci to test FairScale with the following PyTorch versions (with CUDA 11.2):\n* the latest stable release (e.g. 1.10.0)\n* the latest LTS release (e.g. 1.8.1)\n* a recent nightly release (e.g. 1.11.0.dev20211101+cu111)\n\nPlease create an [issue](https://github.com/facebookresearch/fairscale/issues) if you are having trouble with installation.\n\n## Contributors\n\nWe welcome contributions! Please see the [CONTRIBUTING](CONTRIBUTING.md) instructions for how you can contribute to FairScale.\n\n## License\n\nFairScale is licensed under the [BSD-3-Clause License](LICENSE).\n\nfairscale.nn.pipe is forked from [torchgpipe](https://github.com/kakaobrain/torchgpipe), Copyright 2019, Kakao Brain, licensed under [Apache License](http://www.apache.org/licenses/LICENSE-2.0).\n\nfairscale.nn.model_parallel is forked from [Megatron-LM](https://github.com/NVIDIA/Megatron-LM), Copyright 2020, NVIDIA CORPORATION, licensed under [Apache License](http://www.apache.org/licenses/LICENSE-2.0).\n\nfairscale.optim.adascale is forked from [AdaptDL](https://github.com/petuum/adaptdl), Copyright 2020, Petuum, Inc., licensed under [Apache License](http://www.apache.org/licenses/LICENSE-2.0).\n\nfairscale.nn.misc.flatten_params_wrapper is forked from [PyTorch-Reparam-Module](https://github.com/SsnL/PyTorch-Reparam-Module), Copyright 2018, Tongzhou Wang, licensed under [MIT License](https://github.com/SsnL/PyTorch-Reparam-Module/blob/master/LICENSE).\n\n\n## Citing FairScale\n\nIf you use FairScale in your publication, please cite it by using the following BibTeX entry.\n\n```BibTeX\n@Misc{FairScale2021,\n  author =       {{FairScale authors}},\n  title =        {FairScale:  A general purpose modular PyTorch library for high performance and large scale training},\n  howpublished = {\\url{https://github.com/facebookresearch/fairscale}},\n  year =         {2021}\n}\n```\n"
        },
        {
          "name": "RELEASE.md",
          "type": "blob",
          "size": 1.4609375,
          "content": "## Steps to do a release\n\n### New Approach\n- Go to the [fairscale release workflow](https://github.com/facebookresearch/fairscale/actions/workflows/release.yml) in Github actions.\n- In the __Run Workflow__ dropdown, select the branch from which you wish to release. The default value is __main__ and should be used in almost all cases.\n- In adherence to [Semantic Versioning]((https://semver.org/spec/v2.0.0.html)) enter one of the following three values for _Release Type_:\n  - _patch_\n  - _minor_\n  - _major_\n- Click __Run Workflow__.\n- Verify [fairscale/version.py](https://github.com/facebookresearch/fairscale/blob/main/fairscale/version.py) has been updated.\n- Verify a new [PyPI package](https://pypi.org/project/fairscale/) has been published.\n- Verify a new [Github release](https://github.com/facebookresearch/fairscale/releases) has been created.\n\n---\n### Old Approach\n\n- Update the CHANGELOG.md\n- Update \"what's new\" in README.md\n- If needed, update the PyTorch versions in README.md in the Testing section.\n- Update `fairscale/__init__.py` and `docs/source/conf.py` for the new version number\n- git commit the change with title like \"[chore] 0.3.1 release\"\n- make a tag, like `git tag v0.3.1`\n- git push --tags origin [your/branch]\n- `python3 setup.py sdist` to build a new package (will be in dist/)\n- `python3 -m twine upload --repository pypi dist/*` to upload to pypi\n- visit [this page](https://github.com/facebookresearch/fairscale/tags) and create the newly\n  tagged release.\n"
        },
        {
          "name": "benchmarks",
          "type": "tree",
          "content": null
        },
        {
          "name": "codecov.yml",
          "type": "blob",
          "size": 0.509765625,
          "content": "# Copyright (c) Facebook, Inc. and its affiliates. All rights reserved.\n#\n# This source code is licensed under the BSD license found in the\n# LICENSE file in the root directory of this source tree.\n\ncodecov:\n  require_ci_to_pass: yes\ncoverage:\n  status:\n    project:\n      default:\n        target: 94%\n        threshold: 0.1%\nparsers:\n  gcov:\n    branch_detection:\n      conditional: yes\n      loop: yes\n      method: no\n      macro: no\ncomment:\n  layout: \"reach,diff,flags,tree\"\n  behavior: default\n  require_changes: no\n"
        },
        {
          "name": "docs",
          "type": "tree",
          "content": null
        },
        {
          "name": "fairscale",
          "type": "tree",
          "content": null
        },
        {
          "name": "pyproject.toml",
          "type": "blob",
          "size": 0.4814453125,
          "content": "[build-system]\nrequires = [\n    \"setuptools >= 40.6.2\",\n    \"wheel >= 0.30.0\"\n]\nbuild-backend = \"setuptools.build_meta:__legacy__\"\n\n[tool.black]\nline-length = 120\nexclude = '''\n/(\n    \\.git\n  | \\.mypy_cache\n  | \\.pytest_cache\n  | build\n  | docs\n  | stubs\n)/\n'''\n\n[tool.isort]\nline_length = 120\nmulti_line_output = 3\ninclude_trailing_comma = true\nforce_grid_wrap = 0\nuse_parentheses = true\nskip_glob = [\"build/*\", \"stubs/*\"]\n# Don't split \"import\" and \"from\".\nforce_sort_within_sections = true\n"
        },
        {
          "name": "release_utils.py",
          "type": "blob",
          "size": 2.3583984375,
          "content": "import argparse\nimport re\nfrom typing import Tuple\n\nfrom setup import find_version\n\n\ndef get_next_version(release_type) -> Tuple[Tuple[int, int, int], str, str]:\n    current_ver = find_version(\"fairscale/version.py\")\n    version_list = [int(x) for x in current_ver.strip(\"'\").split(\".\")]\n    major, minor, patch = version_list[0], version_list[1], version_list[2]\n    if release_type == \"patch\":\n        patch += 1\n    elif release_type == \"minor\":\n        minor += 1\n        patch = 0\n    elif release_type == \"major\":\n        major += 1\n        minor = patch = 0\n    else:\n        raise ValueError(\"Incorrect release type specified. Acceptable types are major, minor and patch.\")\n\n    new_version_tuple = (major, minor, patch)\n    new_version_str = \".\".join([str(x) for x in new_version_tuple])\n    new_tag_str = \"v\" + new_version_str\n    return new_version_tuple, new_version_str, new_tag_str\n\n\ndef update_version(new_version_tuple) -> None:\n    \"\"\"\n    given the current version, update the version to the\n    next version depending on the type of release.\n    \"\"\"\n\n    with open(\"fairscale/version.py\", \"r\") as reader:\n        current_version_data = reader.read()\n\n    # for line in current_version_data:\n    version_match = re.search(r\"^__version_tuple__ \", current_version_data)\n\n    if version_match:\n        new_version_data = \"__version_tuple__ = %s\\n\" % str(new_version_tuple)\n        current_version_data = current_version_data.replace(version_match.string, new_version_data)\n\n        with open(\"fairscale/version.py\", \"w\") as writer:\n            writer.write(current_version_data)\n    else:\n        raise RuntimeError(\"__version_tuple__ not found in version.py\")\n\n\ndef main(args):\n    if args.release_type in [\"major\", \"minor\", \"patch\"]:\n        new_version_tuple, new_version, new_tag = get_next_version(args.release_type)\n    else:\n        raise ValueError(\"Incorrect release type specified\")\n\n    if args.update_version:\n        update_version(new_version_tuple)\n\n    print(new_version, new_tag)\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser(description=\"Versioning utils\")\n    parser.add_argument(\"--release-type\", type=str, required=True, help=\"type of release = major/minor/patch\")\n    parser.add_argument(\n        \"--update-version\", action=\"store_true\", required=False, help=\"updates the version in fairscale/version.py\"\n    )\n\n    args = parser.parse_args()\n    main(args)\n"
        },
        {
          "name": "requirements-benchmarks.txt",
          "type": "blob",
          "size": 0.146484375,
          "content": "# Bring in everything that tests depends on.\n-r requirements-dev.txt\n\n# Benchmark dependencies.\ntorchtext == 0.6.0\ntorchvision >= 0.6.0\ntimm == 0.3.4\n"
        },
        {
          "name": "requirements-dev.txt",
          "type": "blob",
          "size": 1.0791015625,
          "content": "# Core deps.\n-r requirements.txt\n\n# Tools for static checking.\n#   - flake8-annotations is needed to avoid F811 error with overload\n#     function typing with mypy.\n#   - if you change versions below, please make sure it is in-sync with\n#     .pre-commit-config.yaml for pre-commit.\nblack == 22.3.0\nflake8 == 4.0.1\nflake8-annotations == 2.7.0\nisort == 5.10.1\nmypy == 0.910\npre-commit >= 2.15.0\n\n# Tools for unit tests & coverage.\npytest == 7.0.0\npytest-cov == 3.0.0\npytest-timeout == 2.1.0\nremote-pdb >= 2.1.0\nparameterized >= 0.8.1\n\n# Tools for testing docs\ndocutils == 0.17\n\n# For torch.cuda.list_gpu_processes()\npynvml == 8.0.4\n\n# For mypy typing. It is important to have a fixed version. Otherwise, you\n# may run into mypy errors out differently for different versions.\nnumpy == 1.22.0\n\n# For layerwise gradient scaler\nscikit-learn == 1.1.3\n\n# For weigit. These are actually user requirements, not developer requirements.\n# However, due to the experimental nature of weigit, we don't expose to the\n# general users of fairscale yet. We check for them in weigit's init code.\npygit2==1.11.1\npgzip==0.3.1\n"
        },
        {
          "name": "requirements.txt",
          "type": "blob",
          "size": 0.34765625,
          "content": "# FairScale should only depends on torch, not things higher level than torch.\n# Note1: setup.py automatically reads this file to setup install-time dependencies.\n# Note2: we use >= in this file but == in requirements-dev.txt for determinism\n#        in testing.\n# Note3: update docs/requirements.txt if you change this file.\ntorch >= 1.8.0\nnumpy >= 1.22.0\n"
        },
        {
          "name": "setup.cfg",
          "type": "blob",
          "size": 2.08203125,
          "content": "# -----------------------------------------------------------------------------\n# pytest\n# -----------------------------------------------------------------------------\n\n[tool:pytest]\ntestpaths = tests\naddopts = --verbose\njunit_family = xunit2\n\n[aliases]\ntest = pytest\n\n# -----------------------------------------------------------------------------\n# coverage\n# -----------------------------------------------------------------------------\n\n[coverage:report]\n# Coverage couldn't detect backward functions because they are called by C++.\n# Append \"# pragma: no cover\" to the definition lines to ignore them.\n# https://www.janfreyberg.com/blog/2019-04-01-testing-pytorch-functions/\nexclude_lines = pragma: no cover\n\n# -----------------------------------------------------------------------------\n# flake8\n# -----------------------------------------------------------------------------\n\n[flake8]\nselect = B,C,E,F,P,T4,W,B9\nmax-line-length = 120\n# C408 ignored because we like the dict keyword argument syntax\n# E501 is not flexible enough, we're using B950 instead\nignore =\n    E203,E305,E402,E501,E721,E741,F403,F405,F821,F841,F999,W503,W504,C408,E302,W291,E303,\nper-file-ignores = __init__.py: F401\nexclude = build,*.pyi,.git\n\n# -----------------------------------------------------------------------------\n# mypy\n# -----------------------------------------------------------------------------\n\n# Docs for mypy config: https://mypy.readthedocs.io/en/latest/config_file.html\n[mypy]\nmypy_path = ./stubs/\nfollow_imports = normal\nplugins = numpy.typing.mypy_plugin\n\n# This project must be strictly typed.\n[mypy-fairscale.*]\ncheck_untyped_defs = true\ndisallow_untyped_defs = true\ndisallow_untyped_calls = true\ndisallow_untyped_decorators = true\ndisallow_incomplete_defs = true\nwarn_unused_ignores = true\n\n[mypy-fairscale.experimental.nn.distributed_pipeline.trace]\nignore_errors = True\n\n[mypy-fairscale.experimental.nn.auto_shard]\nignore_errors = True\n\n[mypy-benchmarks.*]\nignore_errors = True\n\n# Ignore missing imports from untyped third-party libraries.\n[mypy-torch.*,torchvision.*,setuptools.*,pytest.*]\nignore_missing_imports = true\n"
        },
        {
          "name": "setup.py",
          "type": "blob",
          "size": 3.076171875,
          "content": "#!/usr/bin/env python3\n\n# Copyright (c) Facebook, Inc. and its affiliates. All rights reserved.\n#\n# This source code is licensed under the BSD license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport os\nimport re\n\nimport setuptools\n\nthis_dir = os.path.dirname(os.path.abspath(__file__))\n\n\ndef fetch_requirements():\n    with open(\"requirements.txt\") as f:\n        reqs = f.read().strip().split(\"\\n\")\n    return reqs\n\n\n# https://packaging.python.org/guides/single-sourcing-package-version/\ndef find_version(version_file_path) -> str:\n    with open(version_file_path) as version_file:\n        version_match = re.search(r\"^__version_tuple__ = (.*)\", version_file.read(), re.M)\n        if version_match:\n            ver_tup = eval(version_match.group(1))\n            ver_str = \".\".join([str(x) for x in ver_tup])\n            return ver_str\n        raise RuntimeError(\"Unable to find version tuple.\")\n\n\nextensions = []\ncmdclass = {}\nsetup_requires = []\n\nif os.getenv(\"BUILD_CUDA_EXTENSIONS\", \"0\") == \"1\":\n    from torch.utils.cpp_extension import BuildExtension, CUDAExtension\n\n    setup_requires = [\"ninja\"]\n    extensions.extend(\n        [\n            CUDAExtension(\n                name=\"fairscale.fused_adam_cuda\",\n                include_dirs=[os.path.join(this_dir, \"fairscale/clib/fused_adam_cuda\")],\n                sources=[\n                    \"fairscale/clib/fused_adam_cuda/fused_adam_cuda.cpp\",\n                    \"fairscale/clib/fused_adam_cuda/fused_adam_cuda_kernel.cu\",\n                ],\n                extra_compile_args={\"cxx\": [\"-O3\"], \"nvcc\": [\"-O3\", \"--use_fast_math\"]},\n            )\n        ]\n    )\n\n    cmdclass[\"build_ext\"] = BuildExtension\n\n\nif __name__ == \"__main__\":\n    setuptools.setup(\n        name=\"fairscale\",\n        description=\"FairScale: A PyTorch library for large-scale and high-performance training.\",\n        version=find_version(\"fairscale/version.py\"),\n        setup_requires=setup_requires,\n        install_requires=fetch_requirements(),\n        include_package_data=True,\n        packages=setuptools.find_packages(include=[\"fairscale*\"]),  # Only include code within fairscale.\n        ext_modules=extensions,\n        cmdclass=cmdclass,\n        python_requires=\">=3.8\",\n        author=\"Foundational AI Research @ Meta AI\",\n        author_email=\"todo@meta.com\",\n        long_description=(\n            \"FairScale is a PyTorch extension library for high performance and \"\n            \"large scale training on one or multiple machines/nodes. This library \"\n            \"extends basic PyTorch capabilities while adding new experimental ones.\"\n        ),\n        long_description_content_type=\"text/markdown\",\n        entry_points={\"console_scripts\": [\"wgit = fairscale.experimental.wgit.__main__:main\"]},\n        classifiers=[\n            \"Programming Language :: Python :: 3.8\",\n            \"Programming Language :: Python :: 3.9\",\n            \"Programming Language :: Python :: 3.10\",\n            \"License :: OSI Approved :: BSD License\",\n            \"Topic :: Scientific/Engineering :: Artificial Intelligence\",\n            \"Operating System :: OS Independent\",\n        ],\n    )\n"
        },
        {
          "name": "stubs",
          "type": "tree",
          "content": null
        },
        {
          "name": "tests",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}