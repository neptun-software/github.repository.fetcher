{
  "metadata": {
    "timestamp": 1736559674544,
    "page": 344,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjM1MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "NVlabs/stylegan2-ada-pytorch",
      "stars": 4161,
      "defaultBranch": "main",
      "files": [
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.0205078125,
          "content": "__pycache__/\n.cache/\n"
        },
        {
          "name": "Dockerfile",
          "type": "blob",
          "size": 0.8759765625,
          "content": "# Copyright (c) 2021, NVIDIA CORPORATION.  All rights reserved.\n#\n# NVIDIA CORPORATION and its licensors retain all intellectual property\n# and proprietary rights in and to this software, related documentation\n# and any modifications thereto.  Any use, reproduction, disclosure or\n# distribution of this software and related documentation without an express\n# license agreement from NVIDIA CORPORATION is strictly prohibited.\n\nFROM nvcr.io/nvidia/pytorch:20.12-py3\n\nENV PYTHONDONTWRITEBYTECODE 1\nENV PYTHONUNBUFFERED 1\n\nRUN pip install imageio-ffmpeg==0.4.3 pyspng==0.1.0\n\nWORKDIR /workspace\n\n# Unset TORCH_CUDA_ARCH_LIST and exec.  This makes pytorch run-time\n# extension builds significantly faster as we only compile for the\n# currently active GPU configuration.\nRUN (printf '#!/bin/bash\\nunset TORCH_CUDA_ARCH_LIST\\nexec \\\"$@\\\"\\n' >> /entry.sh) && chmod a+x /entry.sh\nENTRYPOINT [\"/entry.sh\"]\n"
        },
        {
          "name": "LICENSE.txt",
          "type": "blob",
          "size": 4.3173828125,
          "content": "Copyright (c) 2021, NVIDIA Corporation. All rights reserved.\n\n\nNVIDIA Source Code License for StyleGAN2 with Adaptive Discriminator Augmentation (ADA)\n\n\n=======================================================================\n\n1. Definitions\n\n\"Licensor\" means any person or entity that distributes its Work.\n\n\"Software\" means the original work of authorship made available under\nthis License.\n\n\"Work\" means the Software and any additions to or derivative works of\nthe Software that are made available under this License.\n\nThe terms \"reproduce,\" \"reproduction,\" \"derivative works,\" and\n\"distribution\" have the meaning as provided under U.S. copyright law;\nprovided, however, that for the purposes of this License, derivative\nworks shall not include works that remain separable from, or merely\nlink (or bind by name) to the interfaces of, the Work.\n\nWorks, including the Software, are \"made available\" under this License\nby including in or with the Work either (a) a copyright notice\nreferencing the applicability of this License to the Work, or (b) a\ncopy of this License.\n\n2. License Grants\n\n    2.1 Copyright Grant. Subject to the terms and conditions of this\n    License, each Licensor grants to you a perpetual, worldwide,\n    non-exclusive, royalty-free, copyright license to reproduce,\n    prepare derivative works of, publicly display, publicly perform,\n    sublicense and distribute its Work and any resulting derivative\n    works in any form.\n\n3. Limitations\n\n    3.1 Redistribution. You may reproduce or distribute the Work only\n    if (a) you do so under this License, (b) you include a complete\n    copy of this License with your distribution, and (c) you retain\n    without modification any copyright, patent, trademark, or\n    attribution notices that are present in the Work.\n\n    3.2 Derivative Works. You may specify that additional or different\n    terms apply to the use, reproduction, and distribution of your\n    derivative works of the Work (\"Your Terms\") only if (a) Your Terms\n    provide that the use limitation in Section 3.3 applies to your\n    derivative works, and (b) you identify the specific derivative\n    works that are subject to Your Terms. Notwithstanding Your Terms,\n    this License (including the redistribution requirements in Section\n    3.1) will continue to apply to the Work itself.\n\n    3.3 Use Limitation. The Work and any derivative works thereof only\n    may be used or intended for use non-commercially. Notwithstanding\n    the foregoing, NVIDIA and its affiliates may use the Work and any\n    derivative works commercially. As used herein, \"non-commercially\"\n    means for research or evaluation purposes only.\n\n    3.4 Patent Claims. If you bring or threaten to bring a patent claim\n    against any Licensor (including any claim, cross-claim or\n    counterclaim in a lawsuit) to enforce any patents that you allege\n    are infringed by any Work, then your rights under this License from\n    such Licensor (including the grant in Section 2.1) will terminate\n    immediately.\n\n    3.5 Trademarks. This License does not grant any rights to use any\n    Licensor’s or its affiliates’ names, logos, or trademarks, except\n    as necessary to reproduce the notices described in this License.\n\n    3.6 Termination. If you violate any term of this License, then your\n    rights under this License (including the grant in Section 2.1) will\n    terminate immediately.\n\n4. Disclaimer of Warranty.\n\nTHE WORK IS PROVIDED \"AS IS\" WITHOUT WARRANTIES OR CONDITIONS OF ANY\nKIND, EITHER EXPRESS OR IMPLIED, INCLUDING WARRANTIES OR CONDITIONS OF\nMERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, TITLE OR\nNON-INFRINGEMENT. YOU BEAR THE RISK OF UNDERTAKING ANY ACTIVITIES UNDER\nTHIS LICENSE.\n\n5. Limitation of Liability.\n\nEXCEPT AS PROHIBITED BY APPLICABLE LAW, IN NO EVENT AND UNDER NO LEGAL\nTHEORY, WHETHER IN TORT (INCLUDING NEGLIGENCE), CONTRACT, OR OTHERWISE\nSHALL ANY LICENSOR BE LIABLE TO YOU FOR DAMAGES, INCLUDING ANY DIRECT,\nINDIRECT, SPECIAL, INCIDENTAL, OR CONSEQUENTIAL DAMAGES ARISING OUT OF\nOR RELATED TO THIS LICENSE, THE USE OR INABILITY TO USE THE WORK\n(INCLUDING BUT NOT LIMITED TO LOSS OF GOODWILL, BUSINESS INTERRUPTION,\nLOST PROFITS OR DATA, COMPUTER FAILURE OR MALFUNCTION, OR ANY OTHER\nCOMMERCIAL DAMAGES OR LOSSES), EVEN IF THE LICENSOR HAS BEEN ADVISED OF\nTHE POSSIBILITY OF SUCH DAMAGES.\n\n=======================================================================\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 25.1533203125,
          "content": "## StyleGAN2-ADA &mdash; Official PyTorch implementation\n\n![Teaser image](./docs/stylegan2-ada-teaser-1024x252.png)\n\n**Training Generative Adversarial Networks with Limited Data**<br>\nTero Karras, Miika Aittala, Janne Hellsten, Samuli Laine, Jaakko Lehtinen, Timo Aila<br>\nhttps://arxiv.org/abs/2006.06676<br>\n\nAbstract: *Training generative adversarial networks (GAN) using too little data typically leads to discriminator overfitting, causing training to diverge. We propose an adaptive discriminator augmentation mechanism that significantly stabilizes training in limited data regimes. The approach does not require changes to loss functions or network architectures, and is applicable both when training from scratch and when fine-tuning an existing GAN on another dataset. We demonstrate, on several datasets, that good results are now possible using only a few thousand training images, often matching StyleGAN2 results with an order of magnitude fewer images. We expect this to open up new application domains for GANs. We also find that the widely used CIFAR-10 is, in fact, a limited data benchmark, and improve the record FID from 5.59 to 2.42.*\n\nFor business inquiries, please visit our website and submit the form: [NVIDIA Research Licensing](https://www.nvidia.com/en-us/research/inquiries/)\n\n## Release notes\n\nThis repository is a faithful reimplementation of [StyleGAN2-ADA](https://github.com/NVlabs/stylegan2-ada/) in PyTorch, focusing on correctness, performance, and compatibility.\n\n**Correctness**\n* Full support for all primary training configurations.\n* Extensive verification of image quality, training curves, and quality metrics against the TensorFlow version.\n* Results are expected to match in all cases, excluding the effects of pseudo-random numbers and floating-point arithmetic.\n\n**Performance**\n* Training is typically 5%&ndash;30% faster compared to the TensorFlow version on NVIDIA Tesla V100 GPUs.\n* Inference is up to 35% faster in high resolutions, but it may be slightly slower in low resolutions.\n* GPU memory usage is comparable to the TensorFlow version.\n* Faster startup time when training new networks (<50s), and also when using pre-trained networks (<4s).\n* New command line options for tweaking the training performance.\n\n**Compatibility**\n* Compatible with old network pickles created using the TensorFlow version.\n* New ZIP/PNG based dataset format for maximal interoperability with existing 3rd party tools.\n* TFRecords datasets are no longer supported &mdash; they need to be converted to the new format.\n* New JSON-based format for logs, metrics, and training curves.\n* Training curves are also exported in the old TFEvents format if TensorBoard is installed.\n* Command line syntax is mostly unchanged, with a few exceptions (e.g., `dataset_tool.py`).\n* Comparison methods are not supported (`--cmethod`, `--dcap`, `--cfg=cifarbaseline`, `--aug=adarv`)\n* **Truncation is now disabled by default.**\n\n## Data repository\n\n| Path | Description\n| :--- | :----------\n| [stylegan2-ada-pytorch](https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/) | Main directory hosted on Amazon S3\n| &ensp;&ensp;&boxvr;&nbsp; [ada-paper.pdf](https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/ada-paper.pdf) | Paper PDF\n| &ensp;&ensp;&boxvr;&nbsp; [images](https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/images/) | Curated example images produced using the pre-trained models\n| &ensp;&ensp;&boxvr;&nbsp; [videos](https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/videos/) | Curated example interpolation videos\n| &ensp;&ensp;&boxur;&nbsp; [pretrained](https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/) | Pre-trained models\n| &ensp;&ensp;&ensp;&ensp;&boxvr;&nbsp; ffhq.pkl | FFHQ at 1024x1024, trained using original StyleGAN2\n| &ensp;&ensp;&ensp;&ensp;&boxvr;&nbsp; metfaces.pkl | MetFaces at 1024x1024, transfer learning from FFHQ using ADA\n| &ensp;&ensp;&ensp;&ensp;&boxvr;&nbsp; afhqcat.pkl | AFHQ Cat at 512x512, trained from scratch using ADA\n| &ensp;&ensp;&ensp;&ensp;&boxvr;&nbsp; afhqdog.pkl | AFHQ Dog at 512x512, trained from scratch using ADA\n| &ensp;&ensp;&ensp;&ensp;&boxvr;&nbsp; afhqwild.pkl | AFHQ Wild at 512x512, trained from scratch using ADA\n| &ensp;&ensp;&ensp;&ensp;&boxvr;&nbsp; cifar10.pkl | Class-conditional CIFAR-10 at 32x32\n| &ensp;&ensp;&ensp;&ensp;&boxvr;&nbsp; brecahad.pkl | BreCaHAD at 512x512, trained from scratch using ADA\n| &ensp;&ensp;&ensp;&ensp;&boxvr;&nbsp; [paper-fig7c-training-set-sweeps](https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/paper-fig7c-training-set-sweeps/) | Models used in Fig.7c (sweep over training set size)\n| &ensp;&ensp;&ensp;&ensp;&boxvr;&nbsp; [paper-fig11a-small-datasets](https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/paper-fig11a-small-datasets/) | Models used in Fig.11a (small datasets & transfer learning)\n| &ensp;&ensp;&ensp;&ensp;&boxvr;&nbsp; [paper-fig11b-cifar10](https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/paper-fig11b-cifar10/) | Models used in Fig.11b (CIFAR-10)\n| &ensp;&ensp;&ensp;&ensp;&boxvr;&nbsp; [transfer-learning-source-nets](https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/transfer-learning-source-nets/) | Models used as starting point for transfer learning\n| &ensp;&ensp;&ensp;&ensp;&boxur;&nbsp; [metrics](https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/metrics/) | Feature detectors used by the quality metrics\n\n## Requirements\n\n* Linux and Windows are supported, but we recommend Linux for performance and compatibility reasons.\n* 1&ndash;8 high-end NVIDIA GPUs with at least 12 GB of memory. We have done all testing and development using NVIDIA DGX-1 with 8 Tesla V100 GPUs.\n* 64-bit Python 3.7 and PyTorch 1.7.1. See [https://pytorch.org/](https://pytorch.org/) for PyTorch install instructions.\n* CUDA toolkit 11.0 or later.  Use at least version 11.1 if running on RTX 3090.  (Why is a separate CUDA toolkit installation required?  See comments in [#2](https://github.com/NVlabs/stylegan2-ada-pytorch/issues/2#issuecomment-779457121).)\n* Python libraries: `pip install click requests tqdm pyspng ninja imageio-ffmpeg==0.4.3`.  We use the Anaconda3 2020.11 distribution which installs most of these by default.\n* Docker users: use the [provided Dockerfile](./Dockerfile) to build an image with the required library dependencies.\n\nThe code relies heavily on custom PyTorch extensions that are compiled on the fly using NVCC. On Windows, the compilation requires Microsoft Visual Studio. We recommend installing [Visual Studio Community Edition](https://visualstudio.microsoft.com/vs/) and adding it into `PATH` using `\"C:\\Program Files (x86)\\Microsoft Visual Studio\\<VERSION>\\Community\\VC\\Auxiliary\\Build\\vcvars64.bat\"`.\n\n## Getting started\n\nPre-trained networks are stored as `*.pkl` files that can be referenced using local filenames or URLs:\n\n```.bash\n# Generate curated MetFaces images without truncation (Fig.10 left)\npython generate.py --outdir=out --trunc=1 --seeds=85,265,297,849 \\\n    --network=https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/metfaces.pkl\n\n# Generate uncurated MetFaces images with truncation (Fig.12 upper left)\npython generate.py --outdir=out --trunc=0.7 --seeds=600-605 \\\n    --network=https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/metfaces.pkl\n\n# Generate class conditional CIFAR-10 images (Fig.17 left, Car)\npython generate.py --outdir=out --seeds=0-35 --class=1 \\\n    --network=https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/cifar10.pkl\n\n# Style mixing example\npython style_mixing.py --outdir=out --rows=85,100,75,458,1500 --cols=55,821,1789,293 \\\n    --network=https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/metfaces.pkl\n```\n\nOutputs from the above commands are placed under `out/*.png`, controlled by `--outdir`. Downloaded network pickles are cached under `$HOME/.cache/dnnlib`, which can be overridden by setting the `DNNLIB_CACHE_DIR` environment variable. The default PyTorch extension build directory is `$HOME/.cache/torch_extensions`, which can be overridden by setting `TORCH_EXTENSIONS_DIR`.\n\n**Docker**: You can run the above curated image example using Docker as follows:\n\n```.bash\ndocker build --tag sg2ada:latest .\n./docker_run.sh python3 generate.py --outdir=out --trunc=1 --seeds=85,265,297,849 \\\n    --network=https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/metfaces.pkl\n```\n\nNote: The Docker image requires NVIDIA driver release `r455.23` or later.\n\n**Legacy networks**: The above commands can load most of the network pickles created using the previous TensorFlow versions of StyleGAN2 and StyleGAN2-ADA. However, for future compatibility, we recommend converting such legacy pickles into the new format used by the PyTorch version:\n\n```.bash\npython legacy.py \\\n    --source=https://nvlabs-fi-cdn.nvidia.com/stylegan2/networks/stylegan2-cat-config-f.pkl \\\n    --dest=stylegan2-cat-config-f.pkl\n```\n\n## Projecting images to latent space\n\nTo find the matching latent vector for a given image file, run:\n\n```.bash\npython projector.py --outdir=out --target=~/mytargetimg.png \\\n    --network=https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/ffhq.pkl\n```\n\nFor optimal results, the target image should be cropped and aligned similar to the [FFHQ dataset](https://github.com/NVlabs/ffhq-dataset). The above command saves the projection target `out/target.png`, result `out/proj.png`, latent vector `out/projected_w.npz`, and progression video `out/proj.mp4`. You can render the resulting latent vector by specifying `--projected_w` for `generate.py`:\n\n```.bash\npython generate.py --outdir=out --projected_w=out/projected_w.npz \\\n    --network=https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/ffhq.pkl\n```\n\n## Using networks from Python\n\nYou can use pre-trained networks in your own Python code as follows:\n\n```.python\nwith open('ffhq.pkl', 'rb') as f:\n    G = pickle.load(f)['G_ema'].cuda()  # torch.nn.Module\nz = torch.randn([1, G.z_dim]).cuda()    # latent codes\nc = None                                # class labels (not used in this example)\nimg = G(z, c)                           # NCHW, float32, dynamic range [-1, +1]\n```\n\nThe above code requires `torch_utils` and `dnnlib` to be accessible via `PYTHONPATH`. It does not need source code for the networks themselves &mdash; their class definitions are loaded from the pickle via `torch_utils.persistence`.\n\nThe pickle contains three networks. `'G'` and `'D'` are instantaneous snapshots taken during training, and `'G_ema'` represents a moving average of the generator weights over several training steps. The networks are regular instances of `torch.nn.Module`, with all of their parameters and buffers placed on the CPU at import and gradient computation disabled by default.\n\nThe generator consists of two submodules, `G.mapping` and `G.synthesis`, that can be executed separately. They also support various additional options:\n\n```.python\nw = G.mapping(z, c, truncation_psi=0.5, truncation_cutoff=8)\nimg = G.synthesis(w, noise_mode='const', force_fp32=True)\n```\n\nPlease refer to [`generate.py`](./generate.py), [`style_mixing.py`](./style_mixing.py), and [`projector.py`](./projector.py) for further examples.\n\n## Preparing datasets\n\nDatasets are stored as uncompressed ZIP archives containing uncompressed PNG files and a metadata file `dataset.json` for labels.\n\nCustom datasets can be created from a folder containing images; see [`python dataset_tool.py --help`](./docs/dataset-tool-help.txt) for more information. Alternatively, the folder can also be used directly as a dataset, without running it through `dataset_tool.py` first, but doing so may lead to suboptimal performance.\n\nLegacy TFRecords datasets are not supported &mdash; see below for instructions on how to convert them.\n\n**FFHQ**:\n\nStep 1: Download the [Flickr-Faces-HQ dataset](https://github.com/NVlabs/ffhq-dataset) as TFRecords.\n\nStep 2: Extract images from TFRecords using `dataset_tool.py` from the [TensorFlow version of StyleGAN2-ADA](https://github.com/NVlabs/stylegan2-ada/):\n\n```.bash\n# Using dataset_tool.py from TensorFlow version at\n# https://github.com/NVlabs/stylegan2-ada/\npython ../stylegan2-ada/dataset_tool.py unpack \\\n    --tfrecord_dir=~/ffhq-dataset/tfrecords/ffhq --output_dir=/tmp/ffhq-unpacked\n```\n\nStep 3: Create ZIP archive using `dataset_tool.py` from this repository:\n\n```.bash\n# Original 1024x1024 resolution.\npython dataset_tool.py --source=/tmp/ffhq-unpacked --dest=~/datasets/ffhq.zip\n\n# Scaled down 256x256 resolution.\n#\n# Note: --resize-filter=box is required to reproduce FID scores shown in the\n# paper.  If you don't need to match exactly, it's better to leave this out\n# and default to Lanczos.  See https://github.com/NVlabs/stylegan2-ada-pytorch/issues/283#issuecomment-1731217782\npython dataset_tool.py --source=/tmp/ffhq-unpacked --dest=~/datasets/ffhq256x256.zip \\\n    --width=256 --height=256 --resize-filter=box\n```\n\n**MetFaces**: Download the [MetFaces dataset](https://github.com/NVlabs/metfaces-dataset) and create ZIP archive:\n\n```.bash\npython dataset_tool.py --source=~/downloads/metfaces/images --dest=~/datasets/metfaces.zip\n```\n\n**AFHQ**: Download the [AFHQ dataset](https://github.com/clovaai/stargan-v2/blob/master/README.md#animal-faces-hq-dataset-afhq) and create ZIP archive:\n\n```.bash\npython dataset_tool.py --source=~/downloads/afhq/train/cat --dest=~/datasets/afhqcat.zip\npython dataset_tool.py --source=~/downloads/afhq/train/dog --dest=~/datasets/afhqdog.zip\npython dataset_tool.py --source=~/downloads/afhq/train/wild --dest=~/datasets/afhqwild.zip\n```\n\n**CIFAR-10**: Download the [CIFAR-10 python version](https://www.cs.toronto.edu/~kriz/cifar.html) and convert to ZIP archive:\n\n```.bash\npython dataset_tool.py --source=~/downloads/cifar-10-python.tar.gz --dest=~/datasets/cifar10.zip\n```\n\n**LSUN**: Download the desired categories from the [LSUN project page](https://www.yf.io/p/lsun/) and convert to ZIP archive:\n\n```.bash\npython dataset_tool.py --source=~/downloads/lsun/raw/cat_lmdb --dest=~/datasets/lsuncat200k.zip \\\n    --transform=center-crop --width=256 --height=256 --max_images=200000\n\npython dataset_tool.py --source=~/downloads/lsun/raw/car_lmdb --dest=~/datasets/lsuncar200k.zip \\\n    --transform=center-crop-wide --width=512 --height=384 --max_images=200000\n```\n\n**BreCaHAD**:\n\nStep 1: Download the [BreCaHAD dataset](https://figshare.com/articles/BreCaHAD_A_Dataset_for_Breast_Cancer_Histopathological_Annotation_and_Diagnosis/7379186).\n\nStep 2: Extract 512x512 resolution crops using `dataset_tool.py` from the [TensorFlow version of StyleGAN2-ADA](https://github.com/NVlabs/stylegan2-ada/):\n\n```.bash\n# Using dataset_tool.py from TensorFlow version at\n# https://github.com/NVlabs/stylegan2-ada/\npython dataset_tool.py extract_brecahad_crops --cropsize=512 \\\n    --output_dir=/tmp/brecahad-crops --brecahad_dir=~/downloads/brecahad/images\n```\n\nStep 3: Create ZIP archive using `dataset_tool.py` from this repository:\n\n```.bash\npython dataset_tool.py --source=/tmp/brecahad-crops --dest=~/datasets/brecahad.zip\n```\n\n## Training new networks\n\nIn its most basic form, training new networks boils down to:\n\n```.bash\npython train.py --outdir=~/training-runs --data=~/mydataset.zip --gpus=1 --dry-run\npython train.py --outdir=~/training-runs --data=~/mydataset.zip --gpus=1\n```\n\nThe first command is optional; it validates the arguments, prints out the training configuration, and exits. The second command kicks off the actual training.\n\nIn this example, the results are saved to a newly created directory `~/training-runs/<ID>-mydataset-auto1`, controlled by `--outdir`. The training exports network pickles (`network-snapshot-<INT>.pkl`) and example images (`fakes<INT>.png`) at regular intervals (controlled by `--snap`). For each pickle, it also evaluates FID (controlled by `--metrics`) and logs the resulting scores in `metric-fid50k_full.jsonl` (as well as TFEvents if TensorBoard is installed).\n\nThe name of the output directory reflects the training configuration. For example, `00000-mydataset-auto1` indicates that the *base configuration* was `auto1`, meaning that the hyperparameters were selected automatically for training on one GPU. The base configuration is controlled by `--cfg`:\n\n| Base config           | Description\n| :-------------------- | :----------\n| `auto`&nbsp;(default) | Automatically select reasonable defaults based on resolution and GPU count. Serves as a good starting point for new datasets but does not necessarily lead to optimal results.\n| `stylegan2`           | Reproduce results for StyleGAN2 config F at 1024x1024 using 1, 2, 4, or 8 GPUs.\n| `paper256`            | Reproduce results for FFHQ and LSUN Cat at 256x256 using 1, 2, 4, or 8 GPUs.\n| `paper512`            | Reproduce results for BreCaHAD and AFHQ at 512x512 using 1, 2, 4, or 8 GPUs.\n| `paper1024`           | Reproduce results for MetFaces at 1024x1024 using 1, 2, 4, or 8 GPUs.\n| `cifar`               | Reproduce results for CIFAR-10 (tuned configuration) using 1 or 2 GPUs.\n\nThe training configuration can be further customized with additional command line options:\n\n* `--aug=noaug` disables ADA.\n* `--cond=1` enables class-conditional training (requires a dataset with labels).\n* `--mirror=1` amplifies the dataset with x-flips. Often beneficial, even with ADA.\n* `--resume=ffhq1024 --snap=10` performs transfer learning from FFHQ trained at 1024x1024.\n* `--resume=~/training-runs/<NAME>/network-snapshot-<INT>.pkl` resumes a previous training run.\n* `--gamma=10` overrides R1 gamma. We recommend trying a couple of different values for each new dataset.\n* `--aug=ada --target=0.7` adjusts ADA target value (default: 0.6).\n* `--augpipe=blit` enables pixel blitting but disables all other augmentations.\n* `--augpipe=bgcfnc` enables all available augmentations (blit, geom, color, filter, noise, cutout).\n\nPlease refer to [`python train.py --help`](./docs/train-help.txt) for the full list.\n\n## Expected training time\n\nThe total training time depends heavily on resolution, number of GPUs, dataset, desired quality, and hyperparameters. The following table lists expected wallclock times to reach different points in the training, measured in thousands of real images shown to the discriminator (\"kimg\"):\n\n| Resolution | GPUs | 1000 kimg | 25000 kimg | sec/kimg          | GPU mem | CPU mem\n| :--------: | :--: | :-------: | :--------: | :---------------: | :-----: | :-----:\n| 128x128    | 1    | 4h 05m    | 4d 06h     | 12.8&ndash;13.7   | 7.2 GB  | 3.9 GB\n| 128x128    | 2    | 2h 06m    | 2d 04h     | 6.5&ndash;6.8     | 7.4 GB  | 7.9 GB\n| 128x128    | 4    | 1h 20m    | 1d 09h     | 4.1&ndash;4.6     | 4.2 GB  | 16.3 GB\n| 128x128    | 8    | 1h 13m    | 1d 06h     | 3.9&ndash;4.9     | 2.6 GB  | 31.9 GB\n| 256x256    | 1    | 6h 36m    | 6d 21h     | 21.6&ndash;24.2   | 5.0 GB  | 4.5 GB\n| 256x256    | 2    | 3h 27m    | 3d 14h     | 11.2&ndash;11.8   | 5.2 GB  | 9.0 GB\n| 256x256    | 4    | 1h 45m    | 1d 20h     | 5.6&ndash;5.9     | 5.2 GB  | 17.8 GB\n| 256x256    | 8    | 1h 24m    | 1d 11h     | 4.4&ndash;5.5     | 3.2 GB  | 34.7 GB\n| 512x512    | 1    | 21h 03m   | 21d 22h    | 72.5&ndash;74.9   | 7.6 GB  | 5.0 GB\n| 512x512    | 2    | 10h 59m   | 11d 10h    | 37.7&ndash;40.0   | 7.8 GB  | 9.8 GB\n| 512x512    | 4    | 5h 29m    | 5d 17h     | 18.7&ndash;19.1   | 7.9 GB  | 17.7 GB\n| 512x512    | 8    | 2h 48m    | 2d 22h     | 9.5&ndash;9.7     | 7.8 GB  | 38.2 GB\n| 1024x1024  | 1    | 1d 20h    | 46d 03h    | 154.3&ndash;161.6 | 8.1 GB  | 5.3 GB\n| 1024x1024  | 2    | 23h 09m   | 24d 02h    | 80.6&ndash;86.2   | 8.6 GB  | 11.9 GB\n| 1024x1024  | 4    | 11h 36m   | 12d 02h    | 40.1&ndash;40.8   | 8.4 GB  | 21.9 GB\n| 1024x1024  | 8    | 5h 54m    | 6d 03h     | 20.2&ndash;20.6   | 8.3 GB  | 44.7 GB\n\nThe above measurements were done using NVIDIA Tesla V100 GPUs with default settings (`--cfg=auto --aug=ada --metrics=fid50k_full`). \"sec/kimg\" shows the expected range of variation in raw training performance, as reported in `log.txt`. \"GPU mem\" and \"CPU mem\" show the highest observed memory consumption, excluding the peak at the beginning caused by `torch.backends.cudnn.benchmark`.\n\nIn typical cases, 25000 kimg or more is needed to reach convergence, but the results are already quite reasonable around 5000 kimg. 1000 kimg is often enough for transfer learning, which tends to converge significantly faster. The following figure shows example convergence curves for different datasets as a function of wallclock time, using the same settings as above:\n\n![Training curves](./docs/stylegan2-ada-training-curves.png)\n\nNote: `--cfg=auto` serves as a reasonable first guess for the hyperparameters but it does not necessarily lead to optimal results for a given dataset. For example, `--cfg=stylegan2` yields considerably better FID  for FFHQ-140k at 1024x1024 than illustrated above. We recommend trying out at least a few different values of `--gamma` for each new dataset.\n\n## Quality metrics\n\nBy default, `train.py` automatically computes FID for each network pickle exported during training. We recommend inspecting `metric-fid50k_full.jsonl` (or TensorBoard) at regular intervals to monitor the training progress. When desired, the automatic computation can be disabled with `--metrics=none` to speed up the training slightly (3%&ndash;9%).\n\nAdditional quality metrics can also be computed after the training:\n\n```.bash\n# Previous training run: look up options automatically, save result to JSONL file.\npython calc_metrics.py --metrics=pr50k3_full \\\n    --network=~/training-runs/00000-ffhq10k-res64-auto1/network-snapshot-000000.pkl\n\n# Pre-trained network pickle: specify dataset explicitly, print result to stdout.\npython calc_metrics.py --metrics=fid50k_full --data=~/datasets/ffhq.zip --mirror=1 \\\n    --network=https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/ffhq.pkl\n```\n\nThe first example looks up the training configuration and performs the same operation as if `--metrics=pr50k3_full` had been specified during training. The second example downloads a pre-trained network pickle, in which case the values of `--mirror` and `--data` must be specified explicitly.\n\nNote that many of the metrics have a significant one-off cost when calculating them for the first time for a new dataset (up to 30min). Also note that the evaluation is done using a different random seed each time, so the results will vary if the same metric is computed multiple times.\n\nWe employ the following metrics in the ADA paper. Execution time and GPU memory usage is reported for one NVIDIA Tesla V100 GPU at 1024x1024 resolution:\n\n| Metric        | Time   | GPU mem | Description |\n| :-----        | :----: | :-----: | :---------- |\n| `fid50k_full` | 13 min | 1.8 GB  | Fr&eacute;chet inception distance<sup>[1]</sup> against the full dataset\n| `kid50k_full` | 13 min | 1.8 GB  | Kernel inception distance<sup>[2]</sup> against the full dataset\n| `pr50k3_full` | 13 min | 4.1 GB  | Precision and recall<sup>[3]</sup> againt the full dataset\n| `is50k`       | 13 min | 1.8 GB  | Inception score<sup>[4]</sup> for CIFAR-10\n\nIn addition, the following metrics from the [StyleGAN](https://github.com/NVlabs/stylegan) and [StyleGAN2](https://github.com/NVlabs/stylegan2) papers are also supported:\n\n| Metric        | Time   | GPU mem | Description |\n| :------------ | :----: | :-----: | :---------- |\n| `fid50k`      | 13 min | 1.8 GB  | Fr&eacute;chet inception distance against 50k real images\n| `kid50k`      | 13 min | 1.8 GB  | Kernel inception distance against 50k real images\n| `pr50k3`      | 13 min | 4.1 GB  | Precision and recall against 50k real images\n| `ppl2_wend`   | 36 min | 2.4 GB  | Perceptual path length<sup>[5]</sup> in W, endpoints, full image\n| `ppl_zfull`   | 36 min | 2.4 GB  | Perceptual path length in Z, full paths, cropped image\n| `ppl_wfull`   | 36 min | 2.4 GB  | Perceptual path length in W, full paths, cropped image\n| `ppl_zend`    | 36 min | 2.4 GB  | Perceptual path length in Z, endpoints, cropped image\n| `ppl_wend`    | 36 min | 2.4 GB  | Perceptual path length in W, endpoints, cropped image\n\nReferences:\n1. [GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium](https://arxiv.org/abs/1706.08500), Heusel et al. 2017\n2. [Demystifying MMD GANs](https://arxiv.org/abs/1801.01401), Bi&nacute;kowski et al. 2018\n3. [Improved Precision and Recall Metric for Assessing Generative Models](https://arxiv.org/abs/1904.06991), Kynk&auml;&auml;nniemi et al. 2019\n4. [Improved Techniques for Training GANs](https://arxiv.org/abs/1606.03498), Salimans et al. 2016\n5. [A Style-Based Generator Architecture for Generative Adversarial Networks](https://arxiv.org/abs/1812.04948), Karras et al. 2018\n\n## License\n\nCopyright &copy; 2021, NVIDIA Corporation. All rights reserved.\n\nThis work is made available under the [Nvidia Source Code License](https://nvlabs.github.io/stylegan2-ada-pytorch/license.html).\n\n## Citation\n\n```\n@inproceedings{Karras2020ada,\n  title     = {Training Generative Adversarial Networks with Limited Data},\n  author    = {Tero Karras and Miika Aittala and Janne Hellsten and Samuli Laine and Jaakko Lehtinen and Timo Aila},\n  booktitle = {Proc. NeurIPS},\n  year      = {2020}\n}\n```\n\n## Development\n\nThis is a research reference implementation and is treated as a one-time code drop. As such, we do not accept outside code contributions in the form of pull requests.\n\n## Acknowledgements\n\nWe thank David Luebke for helpful comments; Tero Kuosmanen and Sabu Nadarajan for their support with compute infrastructure; and Edgar Sch&ouml;nfeld for guidance on setting up unconditional BigGAN.\n"
        },
        {
          "name": "calc_metrics.py",
          "type": "blob",
          "size": 8.140625,
          "content": "# Copyright (c) 2021, NVIDIA CORPORATION.  All rights reserved.\n#\n# NVIDIA CORPORATION and its licensors retain all intellectual property\n# and proprietary rights in and to this software, related documentation\n# and any modifications thereto.  Any use, reproduction, disclosure or\n# distribution of this software and related documentation without an express\n# license agreement from NVIDIA CORPORATION is strictly prohibited.\n\n\"\"\"Calculate quality metrics for previous training run or pretrained network pickle.\"\"\"\n\nimport os\nimport click\nimport json\nimport tempfile\nimport copy\nimport torch\nimport dnnlib\n\nimport legacy\nfrom metrics import metric_main\nfrom metrics import metric_utils\nfrom torch_utils import training_stats\nfrom torch_utils import custom_ops\nfrom torch_utils import misc\n\n#----------------------------------------------------------------------------\n\ndef subprocess_fn(rank, args, temp_dir):\n    dnnlib.util.Logger(should_flush=True)\n\n    # Init torch.distributed.\n    if args.num_gpus > 1:\n        init_file = os.path.abspath(os.path.join(temp_dir, '.torch_distributed_init'))\n        if os.name == 'nt':\n            init_method = 'file:///' + init_file.replace('\\\\', '/')\n            torch.distributed.init_process_group(backend='gloo', init_method=init_method, rank=rank, world_size=args.num_gpus)\n        else:\n            init_method = f'file://{init_file}'\n            torch.distributed.init_process_group(backend='nccl', init_method=init_method, rank=rank, world_size=args.num_gpus)\n\n    # Init torch_utils.\n    sync_device = torch.device('cuda', rank) if args.num_gpus > 1 else None\n    training_stats.init_multiprocessing(rank=rank, sync_device=sync_device)\n    if rank != 0 or not args.verbose:\n        custom_ops.verbosity = 'none'\n\n    # Print network summary.\n    device = torch.device('cuda', rank)\n    torch.backends.cudnn.benchmark = True\n    torch.backends.cuda.matmul.allow_tf32 = False\n    torch.backends.cudnn.allow_tf32 = False\n    G = copy.deepcopy(args.G).eval().requires_grad_(False).to(device)\n    if rank == 0 and args.verbose:\n        z = torch.empty([1, G.z_dim], device=device)\n        c = torch.empty([1, G.c_dim], device=device)\n        misc.print_module_summary(G, [z, c])\n\n    # Calculate each metric.\n    for metric in args.metrics:\n        if rank == 0 and args.verbose:\n            print(f'Calculating {metric}...')\n        progress = metric_utils.ProgressMonitor(verbose=args.verbose)\n        result_dict = metric_main.calc_metric(metric=metric, G=G, dataset_kwargs=args.dataset_kwargs,\n            num_gpus=args.num_gpus, rank=rank, device=device, progress=progress)\n        if rank == 0:\n            metric_main.report_metric(result_dict, run_dir=args.run_dir, snapshot_pkl=args.network_pkl)\n        if rank == 0 and args.verbose:\n            print()\n\n    # Done.\n    if rank == 0 and args.verbose:\n        print('Exiting...')\n\n#----------------------------------------------------------------------------\n\nclass CommaSeparatedList(click.ParamType):\n    name = 'list'\n\n    def convert(self, value, param, ctx):\n        _ = param, ctx\n        if value is None or value.lower() == 'none' or value == '':\n            return []\n        return value.split(',')\n\n#----------------------------------------------------------------------------\n\n@click.command()\n@click.pass_context\n@click.option('network_pkl', '--network', help='Network pickle filename or URL', metavar='PATH', required=True)\n@click.option('--metrics', help='Comma-separated list or \"none\"', type=CommaSeparatedList(), default='fid50k_full', show_default=True)\n@click.option('--data', help='Dataset to evaluate metrics against (directory or zip) [default: same as training data]', metavar='PATH')\n@click.option('--mirror', help='Whether the dataset was augmented with x-flips during training [default: look up]', type=bool, metavar='BOOL')\n@click.option('--gpus', help='Number of GPUs to use', type=int, default=1, metavar='INT', show_default=True)\n@click.option('--verbose', help='Print optional information', type=bool, default=True, metavar='BOOL', show_default=True)\n\ndef calc_metrics(ctx, network_pkl, metrics, data, mirror, gpus, verbose):\n    \"\"\"Calculate quality metrics for previous training run or pretrained network pickle.\n\n    Examples:\n\n    \\b\n    # Previous training run: look up options automatically, save result to JSONL file.\n    python calc_metrics.py --metrics=pr50k3_full \\\\\n        --network=~/training-runs/00000-ffhq10k-res64-auto1/network-snapshot-000000.pkl\n\n    \\b\n    # Pre-trained network pickle: specify dataset explicitly, print result to stdout.\n    python calc_metrics.py --metrics=fid50k_full --data=~/datasets/ffhq.zip --mirror=1 \\\\\n        --network=https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/ffhq.pkl\n\n    Available metrics:\n\n    \\b\n      ADA paper:\n        fid50k_full  Frechet inception distance against the full dataset.\n        kid50k_full  Kernel inception distance against the full dataset.\n        pr50k3_full  Precision and recall againt the full dataset.\n        is50k        Inception score for CIFAR-10.\n\n    \\b\n      StyleGAN and StyleGAN2 papers:\n        fid50k       Frechet inception distance against 50k real images.\n        kid50k       Kernel inception distance against 50k real images.\n        pr50k3       Precision and recall against 50k real images.\n        ppl2_wend    Perceptual path length in W at path endpoints against full image.\n        ppl_zfull    Perceptual path length in Z for full paths against cropped image.\n        ppl_wfull    Perceptual path length in W for full paths against cropped image.\n        ppl_zend     Perceptual path length in Z at path endpoints against cropped image.\n        ppl_wend     Perceptual path length in W at path endpoints against cropped image.\n    \"\"\"\n    dnnlib.util.Logger(should_flush=True)\n\n    # Validate arguments.\n    args = dnnlib.EasyDict(metrics=metrics, num_gpus=gpus, network_pkl=network_pkl, verbose=verbose)\n    if not all(metric_main.is_valid_metric(metric) for metric in args.metrics):\n        ctx.fail('\\n'.join(['--metrics can only contain the following values:'] + metric_main.list_valid_metrics()))\n    if not args.num_gpus >= 1:\n        ctx.fail('--gpus must be at least 1')\n\n    # Load network.\n    if not dnnlib.util.is_url(network_pkl, allow_file_urls=True) and not os.path.isfile(network_pkl):\n        ctx.fail('--network must point to a file or URL')\n    if args.verbose:\n        print(f'Loading network from \"{network_pkl}\"...')\n    with dnnlib.util.open_url(network_pkl, verbose=args.verbose) as f:\n        network_dict = legacy.load_network_pkl(f)\n        args.G = network_dict['G_ema'] # subclass of torch.nn.Module\n\n    # Initialize dataset options.\n    if data is not None:\n        args.dataset_kwargs = dnnlib.EasyDict(class_name='training.dataset.ImageFolderDataset', path=data)\n    elif network_dict['training_set_kwargs'] is not None:\n        args.dataset_kwargs = dnnlib.EasyDict(network_dict['training_set_kwargs'])\n    else:\n        ctx.fail('Could not look up dataset options; please specify --data')\n\n    # Finalize dataset options.\n    args.dataset_kwargs.resolution = args.G.img_resolution\n    args.dataset_kwargs.use_labels = (args.G.c_dim != 0)\n    if mirror is not None:\n        args.dataset_kwargs.xflip = mirror\n\n    # Print dataset options.\n    if args.verbose:\n        print('Dataset options:')\n        print(json.dumps(args.dataset_kwargs, indent=2))\n\n    # Locate run dir.\n    args.run_dir = None\n    if os.path.isfile(network_pkl):\n        pkl_dir = os.path.dirname(network_pkl)\n        if os.path.isfile(os.path.join(pkl_dir, 'training_options.json')):\n            args.run_dir = pkl_dir\n\n    # Launch processes.\n    if args.verbose:\n        print('Launching processes...')\n    torch.multiprocessing.set_start_method('spawn')\n    with tempfile.TemporaryDirectory() as temp_dir:\n        if args.num_gpus == 1:\n            subprocess_fn(rank=0, args=args, temp_dir=temp_dir)\n        else:\n            torch.multiprocessing.spawn(fn=subprocess_fn, args=(args, temp_dir), nprocs=args.num_gpus)\n\n#----------------------------------------------------------------------------\n\nif __name__ == \"__main__\":\n    calc_metrics() # pylint: disable=no-value-for-parameter\n\n#----------------------------------------------------------------------------\n"
        },
        {
          "name": "dataset_tool.py",
          "type": "blob",
          "size": 17.45703125,
          "content": "# Copyright (c) 2021, NVIDIA CORPORATION.  All rights reserved.\n#\n# NVIDIA CORPORATION and its licensors retain all intellectual property\n# and proprietary rights in and to this software, related documentation\n# and any modifications thereto.  Any use, reproduction, disclosure or\n# distribution of this software and related documentation without an express\n# license agreement from NVIDIA CORPORATION is strictly prohibited.\n\nimport functools\nimport io\nimport json\nimport os\nimport pickle\nimport sys\nimport tarfile\nimport gzip\nimport zipfile\nfrom pathlib import Path\nfrom typing import Callable, Optional, Tuple, Union\n\nimport click\nimport numpy as np\nimport PIL.Image\nfrom tqdm import tqdm\n\n#----------------------------------------------------------------------------\n\ndef error(msg):\n    print('Error: ' + msg)\n    sys.exit(1)\n\n#----------------------------------------------------------------------------\n\ndef maybe_min(a: int, b: Optional[int]) -> int:\n    if b is not None:\n        return min(a, b)\n    return a\n\n#----------------------------------------------------------------------------\n\ndef file_ext(name: Union[str, Path]) -> str:\n    return str(name).split('.')[-1]\n\n#----------------------------------------------------------------------------\n\ndef is_image_ext(fname: Union[str, Path]) -> bool:\n    ext = file_ext(fname).lower()\n    return f'.{ext}' in PIL.Image.EXTENSION # type: ignore\n\n#----------------------------------------------------------------------------\n\ndef open_image_folder(source_dir, *, max_images: Optional[int]):\n    input_images = [str(f) for f in sorted(Path(source_dir).rglob('*')) if is_image_ext(f) and os.path.isfile(f)]\n\n    # Load labels.\n    labels = {}\n    meta_fname = os.path.join(source_dir, 'dataset.json')\n    if os.path.isfile(meta_fname):\n        with open(meta_fname, 'r') as file:\n            labels = json.load(file)['labels']\n            if labels is not None:\n                labels = { x[0]: x[1] for x in labels }\n            else:\n                labels = {}\n\n    max_idx = maybe_min(len(input_images), max_images)\n\n    def iterate_images():\n        for idx, fname in enumerate(input_images):\n            arch_fname = os.path.relpath(fname, source_dir)\n            arch_fname = arch_fname.replace('\\\\', '/')\n            img = np.array(PIL.Image.open(fname))\n            yield dict(img=img, label=labels.get(arch_fname))\n            if idx >= max_idx-1:\n                break\n    return max_idx, iterate_images()\n\n#----------------------------------------------------------------------------\n\ndef open_image_zip(source, *, max_images: Optional[int]):\n    with zipfile.ZipFile(source, mode='r') as z:\n        input_images = [str(f) for f in sorted(z.namelist()) if is_image_ext(f)]\n\n        # Load labels.\n        labels = {}\n        if 'dataset.json' in z.namelist():\n            with z.open('dataset.json', 'r') as file:\n                labels = json.load(file)['labels']\n                if labels is not None:\n                    labels = { x[0]: x[1] for x in labels }\n                else:\n                    labels = {}\n\n    max_idx = maybe_min(len(input_images), max_images)\n\n    def iterate_images():\n        with zipfile.ZipFile(source, mode='r') as z:\n            for idx, fname in enumerate(input_images):\n                with z.open(fname, 'r') as file:\n                    img = PIL.Image.open(file) # type: ignore\n                    img = np.array(img)\n                yield dict(img=img, label=labels.get(fname))\n                if idx >= max_idx-1:\n                    break\n    return max_idx, iterate_images()\n\n#----------------------------------------------------------------------------\n\ndef open_lmdb(lmdb_dir: str, *, max_images: Optional[int]):\n    import cv2  # pip install opencv-python\n    import lmdb  # pip install lmdb # pylint: disable=import-error\n\n    with lmdb.open(lmdb_dir, readonly=True, lock=False).begin(write=False) as txn:\n        max_idx = maybe_min(txn.stat()['entries'], max_images)\n\n    def iterate_images():\n        with lmdb.open(lmdb_dir, readonly=True, lock=False).begin(write=False) as txn:\n            for idx, (_key, value) in enumerate(txn.cursor()):\n                try:\n                    try:\n                        img = cv2.imdecode(np.frombuffer(value, dtype=np.uint8), 1)\n                        if img is None:\n                            raise IOError('cv2.imdecode failed')\n                        img = img[:, :, ::-1] # BGR => RGB\n                    except IOError:\n                        img = np.array(PIL.Image.open(io.BytesIO(value)))\n                    yield dict(img=img, label=None)\n                    if idx >= max_idx-1:\n                        break\n                except:\n                    print(sys.exc_info()[1])\n\n    return max_idx, iterate_images()\n\n#----------------------------------------------------------------------------\n\ndef open_cifar10(tarball: str, *, max_images: Optional[int]):\n    images = []\n    labels = []\n\n    with tarfile.open(tarball, 'r:gz') as tar:\n        for batch in range(1, 6):\n            member = tar.getmember(f'cifar-10-batches-py/data_batch_{batch}')\n            with tar.extractfile(member) as file:\n                data = pickle.load(file, encoding='latin1')\n            images.append(data['data'].reshape(-1, 3, 32, 32))\n            labels.append(data['labels'])\n\n    images = np.concatenate(images)\n    labels = np.concatenate(labels)\n    images = images.transpose([0, 2, 3, 1]) # NCHW -> NHWC\n    assert images.shape == (50000, 32, 32, 3) and images.dtype == np.uint8\n    assert labels.shape == (50000,) and labels.dtype in [np.int32, np.int64]\n    assert np.min(images) == 0 and np.max(images) == 255\n    assert np.min(labels) == 0 and np.max(labels) == 9\n\n    max_idx = maybe_min(len(images), max_images)\n\n    def iterate_images():\n        for idx, img in enumerate(images):\n            yield dict(img=img, label=int(labels[idx]))\n            if idx >= max_idx-1:\n                break\n\n    return max_idx, iterate_images()\n\n#----------------------------------------------------------------------------\n\ndef open_mnist(images_gz: str, *, max_images: Optional[int]):\n    labels_gz = images_gz.replace('-images-idx3-ubyte.gz', '-labels-idx1-ubyte.gz')\n    assert labels_gz != images_gz\n    images = []\n    labels = []\n\n    with gzip.open(images_gz, 'rb') as f:\n        images = np.frombuffer(f.read(), np.uint8, offset=16)\n    with gzip.open(labels_gz, 'rb') as f:\n        labels = np.frombuffer(f.read(), np.uint8, offset=8)\n\n    images = images.reshape(-1, 28, 28)\n    images = np.pad(images, [(0,0), (2,2), (2,2)], 'constant', constant_values=0)\n    assert images.shape == (60000, 32, 32) and images.dtype == np.uint8\n    assert labels.shape == (60000,) and labels.dtype == np.uint8\n    assert np.min(images) == 0 and np.max(images) == 255\n    assert np.min(labels) == 0 and np.max(labels) == 9\n\n    max_idx = maybe_min(len(images), max_images)\n\n    def iterate_images():\n        for idx, img in enumerate(images):\n            yield dict(img=img, label=int(labels[idx]))\n            if idx >= max_idx-1:\n                break\n\n    return max_idx, iterate_images()\n\n#----------------------------------------------------------------------------\n\ndef make_transform(\n    transform: Optional[str],\n    output_width: Optional[int],\n    output_height: Optional[int],\n    resize_filter: str\n) -> Callable[[np.ndarray], Optional[np.ndarray]]:\n    resample = { 'box': PIL.Image.BOX, 'lanczos': PIL.Image.LANCZOS }[resize_filter]\n    def scale(width, height, img):\n        w = img.shape[1]\n        h = img.shape[0]\n        if width == w and height == h:\n            return img\n        img = PIL.Image.fromarray(img)\n        ww = width if width is not None else w\n        hh = height if height is not None else h\n        img = img.resize((ww, hh), resample)\n        return np.array(img)\n\n    def center_crop(width, height, img):\n        crop = np.min(img.shape[:2])\n        img = img[(img.shape[0] - crop) // 2 : (img.shape[0] + crop) // 2, (img.shape[1] - crop) // 2 : (img.shape[1] + crop) // 2]\n        img = PIL.Image.fromarray(img, 'RGB')\n        img = img.resize((width, height), resample)\n        return np.array(img)\n\n    def center_crop_wide(width, height, img):\n        ch = int(np.round(width * img.shape[0] / img.shape[1]))\n        if img.shape[1] < width or ch < height:\n            return None\n\n        img = img[(img.shape[0] - ch) // 2 : (img.shape[0] + ch) // 2]\n        img = PIL.Image.fromarray(img, 'RGB')\n        img = img.resize((width, height), resample)\n        img = np.array(img)\n\n        canvas = np.zeros([width, width, 3], dtype=np.uint8)\n        canvas[(width - height) // 2 : (width + height) // 2, :] = img\n        return canvas\n\n    if transform is None:\n        return functools.partial(scale, output_width, output_height)\n    if transform == 'center-crop':\n        if (output_width is None) or (output_height is None):\n            error ('must specify --width and --height when using ' + transform + 'transform')\n        return functools.partial(center_crop, output_width, output_height)\n    if transform == 'center-crop-wide':\n        if (output_width is None) or (output_height is None):\n            error ('must specify --width and --height when using ' + transform + ' transform')\n        return functools.partial(center_crop_wide, output_width, output_height)\n    assert False, 'unknown transform'\n\n#----------------------------------------------------------------------------\n\ndef open_dataset(source, *, max_images: Optional[int]):\n    if os.path.isdir(source):\n        if source.rstrip('/').endswith('_lmdb'):\n            return open_lmdb(source, max_images=max_images)\n        else:\n            return open_image_folder(source, max_images=max_images)\n    elif os.path.isfile(source):\n        if os.path.basename(source) == 'cifar-10-python.tar.gz':\n            return open_cifar10(source, max_images=max_images)\n        elif os.path.basename(source) == 'train-images-idx3-ubyte.gz':\n            return open_mnist(source, max_images=max_images)\n        elif file_ext(source) == 'zip':\n            return open_image_zip(source, max_images=max_images)\n        else:\n            assert False, 'unknown archive type'\n    else:\n        error(f'Missing input file or directory: {source}')\n\n#----------------------------------------------------------------------------\n\ndef open_dest(dest: str) -> Tuple[str, Callable[[str, Union[bytes, str]], None], Callable[[], None]]:\n    dest_ext = file_ext(dest)\n\n    if dest_ext == 'zip':\n        if os.path.dirname(dest) != '':\n            os.makedirs(os.path.dirname(dest), exist_ok=True)\n        zf = zipfile.ZipFile(file=dest, mode='w', compression=zipfile.ZIP_STORED)\n        def zip_write_bytes(fname: str, data: Union[bytes, str]):\n            zf.writestr(fname, data)\n        return '', zip_write_bytes, zf.close\n    else:\n        # If the output folder already exists, check that is is\n        # empty.\n        #\n        # Note: creating the output directory is not strictly\n        # necessary as folder_write_bytes() also mkdirs, but it's better\n        # to give an error message earlier in case the dest folder\n        # somehow cannot be created.\n        if os.path.isdir(dest) and len(os.listdir(dest)) != 0:\n            error('--dest folder must be empty')\n        os.makedirs(dest, exist_ok=True)\n\n        def folder_write_bytes(fname: str, data: Union[bytes, str]):\n            os.makedirs(os.path.dirname(fname), exist_ok=True)\n            with open(fname, 'wb') as fout:\n                if isinstance(data, str):\n                    data = data.encode('utf8')\n                fout.write(data)\n        return dest, folder_write_bytes, lambda: None\n\n#----------------------------------------------------------------------------\n\n@click.command()\n@click.pass_context\n@click.option('--source', help='Directory or archive name for input dataset', required=True, metavar='PATH')\n@click.option('--dest', help='Output directory or archive name for output dataset', required=True, metavar='PATH')\n@click.option('--max-images', help='Output only up to `max-images` images', type=int, default=None)\n@click.option('--resize-filter', help='Filter to use when resizing images for output resolution', type=click.Choice(['box', 'lanczos']), default='lanczos', show_default=True)\n@click.option('--transform', help='Input crop/resize mode', type=click.Choice(['center-crop', 'center-crop-wide']))\n@click.option('--width', help='Output width', type=int)\n@click.option('--height', help='Output height', type=int)\ndef convert_dataset(\n    ctx: click.Context,\n    source: str,\n    dest: str,\n    max_images: Optional[int],\n    transform: Optional[str],\n    resize_filter: str,\n    width: Optional[int],\n    height: Optional[int]\n):\n    \"\"\"Convert an image dataset into a dataset archive usable with StyleGAN2 ADA PyTorch.\n\n    The input dataset format is guessed from the --source argument:\n\n    \\b\n    --source *_lmdb/                    Load LSUN dataset\n    --source cifar-10-python.tar.gz     Load CIFAR-10 dataset\n    --source train-images-idx3-ubyte.gz Load MNIST dataset\n    --source path/                      Recursively load all images from path/\n    --source dataset.zip                Recursively load all images from dataset.zip\n\n    Specifying the output format and path:\n\n    \\b\n    --dest /path/to/dir                 Save output files under /path/to/dir\n    --dest /path/to/dataset.zip         Save output files into /path/to/dataset.zip\n\n    The output dataset format can be either an image folder or an uncompressed zip archive.\n    Zip archives makes it easier to move datasets around file servers and clusters, and may\n    offer better training performance on network file systems.\n\n    Images within the dataset archive will be stored as uncompressed PNG.\n    Uncompresed PNGs can be efficiently decoded in the training loop.\n\n    Class labels are stored in a file called 'dataset.json' that is stored at the\n    dataset root folder.  This file has the following structure:\n\n    \\b\n    {\n        \"labels\": [\n            [\"00000/img00000000.png\",6],\n            [\"00000/img00000001.png\",9],\n            ... repeated for every image in the datase\n            [\"00049/img00049999.png\",1]\n        ]\n    }\n\n    If the 'dataset.json' file cannot be found, the dataset is interpreted as\n    not containing class labels.\n\n    Image scale/crop and resolution requirements:\n\n    Output images must be square-shaped and they must all have the same power-of-two\n    dimensions.\n\n    To scale arbitrary input image size to a specific width and height, use the\n    --width and --height options.  Output resolution will be either the original\n    input resolution (if --width/--height was not specified) or the one specified with\n    --width/height.\n\n    Use the --transform=center-crop or --transform=center-crop-wide options to apply a\n    center crop transform on the input image.  These options should be used with the\n    --width and --height options.  For example:\n\n    \\b\n    python dataset_tool.py --source LSUN/raw/cat_lmdb --dest /tmp/lsun_cat \\\\\n        --transform=center-crop-wide --width 512 --height=384\n    \"\"\"\n\n    PIL.Image.init() # type: ignore\n\n    if dest == '':\n        ctx.fail('--dest output filename or directory must not be an empty string')\n\n    num_files, input_iter = open_dataset(source, max_images=max_images)\n    archive_root_dir, save_bytes, close_dest = open_dest(dest)\n\n    transform_image = make_transform(transform, width, height, resize_filter)\n\n    dataset_attrs = None\n\n    labels = []\n    for idx, image in tqdm(enumerate(input_iter), total=num_files):\n        idx_str = f'{idx:08d}'\n        archive_fname = f'{idx_str[:5]}/img{idx_str}.png'\n\n        # Apply crop and resize.\n        img = transform_image(image['img'])\n\n        # Transform may drop images.\n        if img is None:\n            continue\n\n        # Error check to require uniform image attributes across\n        # the whole dataset.\n        channels = img.shape[2] if img.ndim == 3 else 1\n        cur_image_attrs = {\n            'width': img.shape[1],\n            'height': img.shape[0],\n            'channels': channels\n        }\n        if dataset_attrs is None:\n            dataset_attrs = cur_image_attrs\n            width = dataset_attrs['width']\n            height = dataset_attrs['height']\n            if width != height:\n                error(f'Image dimensions after scale and crop are required to be square.  Got {width}x{height}')\n            if dataset_attrs['channels'] not in [1, 3]:\n                error('Input images must be stored as RGB or grayscale')\n            if width != 2 ** int(np.floor(np.log2(width))):\n                error('Image width/height after scale and crop are required to be power-of-two')\n        elif dataset_attrs != cur_image_attrs:\n            err = [f'  dataset {k}/cur image {k}: {dataset_attrs[k]}/{cur_image_attrs[k]}' for k in dataset_attrs.keys()]\n            error(f'Image {archive_fname} attributes must be equal across all images of the dataset.  Got:\\n' + '\\n'.join(err))\n\n        # Save the image as an uncompressed PNG.\n        img = PIL.Image.fromarray(img, { 1: 'L', 3: 'RGB' }[channels])\n        image_bits = io.BytesIO()\n        img.save(image_bits, format='png', compress_level=0, optimize=False)\n        save_bytes(os.path.join(archive_root_dir, archive_fname), image_bits.getbuffer())\n        labels.append([archive_fname, image['label']] if image['label'] is not None else None)\n\n    metadata = {\n        'labels': labels if all(x is not None for x in labels) else None\n    }\n    save_bytes(os.path.join(archive_root_dir, 'dataset.json'), json.dumps(metadata))\n    close_dest()\n\n#----------------------------------------------------------------------------\n\nif __name__ == \"__main__\":\n    convert_dataset() # pylint: disable=no-value-for-parameter\n"
        },
        {
          "name": "dnnlib",
          "type": "tree",
          "content": null
        },
        {
          "name": "docker_run.sh",
          "type": "blob",
          "size": 1.16796875,
          "content": "#!/bin/bash\n\n# Copyright (c) 2021, NVIDIA CORPORATION.  All rights reserved.\n#\n# NVIDIA CORPORATION and its licensors retain all intellectual property\n# and proprietary rights in and to this software, related documentation\n# and any modifications thereto.  Any use, reproduction, disclosure or\n# distribution of this software and related documentation without an express\n# license agreement from NVIDIA CORPORATION is strictly prohibited.\n\nset -e\n\n# Wrapper script for setting up `docker run` to properly\n# cache downloaded files, custom extension builds and\n# mount the source directory into the container and make it\n# run as non-root user.\n#\n# Use it like:\n#\n# ./docker_run.sh python generate.py --help\n#\n# To override the default `stylegan2ada:latest` image, run:\n#\n# IMAGE=my_image:v1.0 ./docker_run.sh python generate.py --help\n#\n\nrest=$@\n\nIMAGE=\"${IMAGE:-sg2ada:latest}\"\n\nCONTAINER_ID=$(docker inspect --format=\"{{.Id}}\" ${IMAGE} 2> /dev/null)\nif [[ \"${CONTAINER_ID}\" ]]; then\n    docker run --shm-size=2g --gpus all -it --rm -v `pwd`:/scratch --user $(id -u):$(id -g) \\\n        --workdir=/scratch -e HOME=/scratch $IMAGE $@\nelse\n    echo \"Unknown container image: ${IMAGE}\"\n    exit 1\nfi\n"
        },
        {
          "name": "docs",
          "type": "tree",
          "content": null
        },
        {
          "name": "generate.py",
          "type": "blob",
          "size": 5.212890625,
          "content": "# Copyright (c) 2021, NVIDIA CORPORATION.  All rights reserved.\n#\n# NVIDIA CORPORATION and its licensors retain all intellectual property\n# and proprietary rights in and to this software, related documentation\n# and any modifications thereto.  Any use, reproduction, disclosure or\n# distribution of this software and related documentation without an express\n# license agreement from NVIDIA CORPORATION is strictly prohibited.\n\n\"\"\"Generate images using pretrained network pickle.\"\"\"\n\nimport os\nimport re\nfrom typing import List, Optional\n\nimport click\nimport dnnlib\nimport numpy as np\nimport PIL.Image\nimport torch\n\nimport legacy\n\n#----------------------------------------------------------------------------\n\ndef num_range(s: str) -> List[int]:\n    '''Accept either a comma separated list of numbers 'a,b,c' or a range 'a-c' and return as a list of ints.'''\n\n    range_re = re.compile(r'^(\\d+)-(\\d+)$')\n    m = range_re.match(s)\n    if m:\n        return list(range(int(m.group(1)), int(m.group(2))+1))\n    vals = s.split(',')\n    return [int(x) for x in vals]\n\n#----------------------------------------------------------------------------\n\n@click.command()\n@click.pass_context\n@click.option('--network', 'network_pkl', help='Network pickle filename', required=True)\n@click.option('--seeds', type=num_range, help='List of random seeds')\n@click.option('--trunc', 'truncation_psi', type=float, help='Truncation psi', default=1, show_default=True)\n@click.option('--class', 'class_idx', type=int, help='Class label (unconditional if not specified)')\n@click.option('--noise-mode', help='Noise mode', type=click.Choice(['const', 'random', 'none']), default='const', show_default=True)\n@click.option('--projected-w', help='Projection result file', type=str, metavar='FILE')\n@click.option('--outdir', help='Where to save the output images', type=str, required=True, metavar='DIR')\ndef generate_images(\n    ctx: click.Context,\n    network_pkl: str,\n    seeds: Optional[List[int]],\n    truncation_psi: float,\n    noise_mode: str,\n    outdir: str,\n    class_idx: Optional[int],\n    projected_w: Optional[str]\n):\n    \"\"\"Generate images using pretrained network pickle.\n\n    Examples:\n\n    \\b\n    # Generate curated MetFaces images without truncation (Fig.10 left)\n    python generate.py --outdir=out --trunc=1 --seeds=85,265,297,849 \\\\\n        --network=https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/metfaces.pkl\n\n    \\b\n    # Generate uncurated MetFaces images with truncation (Fig.12 upper left)\n    python generate.py --outdir=out --trunc=0.7 --seeds=600-605 \\\\\n        --network=https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/metfaces.pkl\n\n    \\b\n    # Generate class conditional CIFAR-10 images (Fig.17 left, Car)\n    python generate.py --outdir=out --seeds=0-35 --class=1 \\\\\n        --network=https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/cifar10.pkl\n\n    \\b\n    # Render an image from projected W\n    python generate.py --outdir=out --projected_w=projected_w.npz \\\\\n        --network=https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/metfaces.pkl\n    \"\"\"\n\n    print('Loading networks from \"%s\"...' % network_pkl)\n    device = torch.device('cuda')\n    with dnnlib.util.open_url(network_pkl) as f:\n        G = legacy.load_network_pkl(f)['G_ema'].to(device) # type: ignore\n\n    os.makedirs(outdir, exist_ok=True)\n\n    # Synthesize the result of a W projection.\n    if projected_w is not None:\n        if seeds is not None:\n            print ('warn: --seeds is ignored when using --projected-w')\n        print(f'Generating images from projected W \"{projected_w}\"')\n        ws = np.load(projected_w)['w']\n        ws = torch.tensor(ws, device=device) # pylint: disable=not-callable\n        assert ws.shape[1:] == (G.num_ws, G.w_dim)\n        for idx, w in enumerate(ws):\n            img = G.synthesis(w.unsqueeze(0), noise_mode=noise_mode)\n            img = (img.permute(0, 2, 3, 1) * 127.5 + 128).clamp(0, 255).to(torch.uint8)\n            img = PIL.Image.fromarray(img[0].cpu().numpy(), 'RGB').save(f'{outdir}/proj{idx:02d}.png')\n        return\n\n    if seeds is None:\n        ctx.fail('--seeds option is required when not using --projected-w')\n\n    # Labels.\n    label = torch.zeros([1, G.c_dim], device=device)\n    if G.c_dim != 0:\n        if class_idx is None:\n            ctx.fail('Must specify class label with --class when using a conditional network')\n        label[:, class_idx] = 1\n    else:\n        if class_idx is not None:\n            print ('warn: --class=lbl ignored when running on an unconditional network')\n\n    # Generate images.\n    for seed_idx, seed in enumerate(seeds):\n        print('Generating image for seed %d (%d/%d) ...' % (seed, seed_idx, len(seeds)))\n        z = torch.from_numpy(np.random.RandomState(seed).randn(1, G.z_dim)).to(device)\n        img = G(z, label, truncation_psi=truncation_psi, noise_mode=noise_mode)\n        img = (img.permute(0, 2, 3, 1) * 127.5 + 128).clamp(0, 255).to(torch.uint8)\n        PIL.Image.fromarray(img[0].cpu().numpy(), 'RGB').save(f'{outdir}/seed{seed:04d}.png')\n\n\n#----------------------------------------------------------------------------\n\nif __name__ == \"__main__\":\n    generate_images() # pylint: disable=no-value-for-parameter\n\n#----------------------------------------------------------------------------\n"
        },
        {
          "name": "legacy.py",
          "type": "blob",
          "size": 16.1171875,
          "content": "﻿# Copyright (c) 2021, NVIDIA CORPORATION.  All rights reserved.\n#\n# NVIDIA CORPORATION and its licensors retain all intellectual property\n# and proprietary rights in and to this software, related documentation\n# and any modifications thereto.  Any use, reproduction, disclosure or\n# distribution of this software and related documentation without an express\n# license agreement from NVIDIA CORPORATION is strictly prohibited.\n\nimport click\nimport pickle\nimport re\nimport copy\nimport numpy as np\nimport torch\nimport dnnlib\nfrom torch_utils import misc\n\n#----------------------------------------------------------------------------\n\ndef load_network_pkl(f, force_fp16=False):\n    data = _LegacyUnpickler(f).load()\n\n    # Legacy TensorFlow pickle => convert.\n    if isinstance(data, tuple) and len(data) == 3 and all(isinstance(net, _TFNetworkStub) for net in data):\n        tf_G, tf_D, tf_Gs = data\n        G = convert_tf_generator(tf_G)\n        D = convert_tf_discriminator(tf_D)\n        G_ema = convert_tf_generator(tf_Gs)\n        data = dict(G=G, D=D, G_ema=G_ema)\n\n    # Add missing fields.\n    if 'training_set_kwargs' not in data:\n        data['training_set_kwargs'] = None\n    if 'augment_pipe' not in data:\n        data['augment_pipe'] = None\n\n    # Validate contents.\n    assert isinstance(data['G'], torch.nn.Module)\n    assert isinstance(data['D'], torch.nn.Module)\n    assert isinstance(data['G_ema'], torch.nn.Module)\n    assert isinstance(data['training_set_kwargs'], (dict, type(None)))\n    assert isinstance(data['augment_pipe'], (torch.nn.Module, type(None)))\n\n    # Force FP16.\n    if force_fp16:\n        for key in ['G', 'D', 'G_ema']:\n            old = data[key]\n            kwargs = copy.deepcopy(old.init_kwargs)\n            if key.startswith('G'):\n                kwargs.synthesis_kwargs = dnnlib.EasyDict(kwargs.get('synthesis_kwargs', {}))\n                kwargs.synthesis_kwargs.num_fp16_res = 4\n                kwargs.synthesis_kwargs.conv_clamp = 256\n            if key.startswith('D'):\n                kwargs.num_fp16_res = 4\n                kwargs.conv_clamp = 256\n            if kwargs != old.init_kwargs:\n                new = type(old)(**kwargs).eval().requires_grad_(False)\n                misc.copy_params_and_buffers(old, new, require_all=True)\n                data[key] = new\n    return data\n\n#----------------------------------------------------------------------------\n\nclass _TFNetworkStub(dnnlib.EasyDict):\n    pass\n\nclass _LegacyUnpickler(pickle.Unpickler):\n    def find_class(self, module, name):\n        if module == 'dnnlib.tflib.network' and name == 'Network':\n            return _TFNetworkStub\n        return super().find_class(module, name)\n\n#----------------------------------------------------------------------------\n\ndef _collect_tf_params(tf_net):\n    # pylint: disable=protected-access\n    tf_params = dict()\n    def recurse(prefix, tf_net):\n        for name, value in tf_net.variables:\n            tf_params[prefix + name] = value\n        for name, comp in tf_net.components.items():\n            recurse(prefix + name + '/', comp)\n    recurse('', tf_net)\n    return tf_params\n\n#----------------------------------------------------------------------------\n\ndef _populate_module_params(module, *patterns):\n    for name, tensor in misc.named_params_and_buffers(module):\n        found = False\n        value = None\n        for pattern, value_fn in zip(patterns[0::2], patterns[1::2]):\n            match = re.fullmatch(pattern, name)\n            if match:\n                found = True\n                if value_fn is not None:\n                    value = value_fn(*match.groups())\n                break\n        try:\n            assert found\n            if value is not None:\n                tensor.copy_(torch.from_numpy(np.array(value)))\n        except:\n            print(name, list(tensor.shape))\n            raise\n\n#----------------------------------------------------------------------------\n\ndef convert_tf_generator(tf_G):\n    if tf_G.version < 4:\n        raise ValueError('TensorFlow pickle version too low')\n\n    # Collect kwargs.\n    tf_kwargs = tf_G.static_kwargs\n    known_kwargs = set()\n    def kwarg(tf_name, default=None, none=None):\n        known_kwargs.add(tf_name)\n        val = tf_kwargs.get(tf_name, default)\n        return val if val is not None else none\n\n    # Convert kwargs.\n    kwargs = dnnlib.EasyDict(\n        z_dim                   = kwarg('latent_size',          512),\n        c_dim                   = kwarg('label_size',           0),\n        w_dim                   = kwarg('dlatent_size',         512),\n        img_resolution          = kwarg('resolution',           1024),\n        img_channels            = kwarg('num_channels',         3),\n        mapping_kwargs = dnnlib.EasyDict(\n            num_layers          = kwarg('mapping_layers',       8),\n            embed_features      = kwarg('label_fmaps',          None),\n            layer_features      = kwarg('mapping_fmaps',        None),\n            activation          = kwarg('mapping_nonlinearity', 'lrelu'),\n            lr_multiplier       = kwarg('mapping_lrmul',        0.01),\n            w_avg_beta          = kwarg('w_avg_beta',           0.995,  none=1),\n        ),\n        synthesis_kwargs = dnnlib.EasyDict(\n            channel_base        = kwarg('fmap_base',            16384) * 2,\n            channel_max         = kwarg('fmap_max',             512),\n            num_fp16_res        = kwarg('num_fp16_res',         0),\n            conv_clamp          = kwarg('conv_clamp',           None),\n            architecture        = kwarg('architecture',         'skip'),\n            resample_filter     = kwarg('resample_kernel',      [1,3,3,1]),\n            use_noise           = kwarg('use_noise',            True),\n            activation          = kwarg('nonlinearity',         'lrelu'),\n        ),\n    )\n\n    # Check for unknown kwargs.\n    kwarg('truncation_psi')\n    kwarg('truncation_cutoff')\n    kwarg('style_mixing_prob')\n    kwarg('structure')\n    unknown_kwargs = list(set(tf_kwargs.keys()) - known_kwargs)\n    if len(unknown_kwargs) > 0:\n        raise ValueError('Unknown TensorFlow kwarg', unknown_kwargs[0])\n\n    # Collect params.\n    tf_params = _collect_tf_params(tf_G)\n    for name, value in list(tf_params.items()):\n        match = re.fullmatch(r'ToRGB_lod(\\d+)/(.*)', name)\n        if match:\n            r = kwargs.img_resolution // (2 ** int(match.group(1)))\n            tf_params[f'{r}x{r}/ToRGB/{match.group(2)}'] = value\n            kwargs.synthesis.kwargs.architecture = 'orig'\n    #for name, value in tf_params.items(): print(f'{name:<50s}{list(value.shape)}')\n\n    # Convert params.\n    from training import networks\n    G = networks.Generator(**kwargs).eval().requires_grad_(False)\n    # pylint: disable=unnecessary-lambda\n    _populate_module_params(G,\n        r'mapping\\.w_avg',                                  lambda:     tf_params[f'dlatent_avg'],\n        r'mapping\\.embed\\.weight',                          lambda:     tf_params[f'mapping/LabelEmbed/weight'].transpose(),\n        r'mapping\\.embed\\.bias',                            lambda:     tf_params[f'mapping/LabelEmbed/bias'],\n        r'mapping\\.fc(\\d+)\\.weight',                        lambda i:   tf_params[f'mapping/Dense{i}/weight'].transpose(),\n        r'mapping\\.fc(\\d+)\\.bias',                          lambda i:   tf_params[f'mapping/Dense{i}/bias'],\n        r'synthesis\\.b4\\.const',                            lambda:     tf_params[f'synthesis/4x4/Const/const'][0],\n        r'synthesis\\.b4\\.conv1\\.weight',                    lambda:     tf_params[f'synthesis/4x4/Conv/weight'].transpose(3, 2, 0, 1),\n        r'synthesis\\.b4\\.conv1\\.bias',                      lambda:     tf_params[f'synthesis/4x4/Conv/bias'],\n        r'synthesis\\.b4\\.conv1\\.noise_const',               lambda:     tf_params[f'synthesis/noise0'][0, 0],\n        r'synthesis\\.b4\\.conv1\\.noise_strength',            lambda:     tf_params[f'synthesis/4x4/Conv/noise_strength'],\n        r'synthesis\\.b4\\.conv1\\.affine\\.weight',            lambda:     tf_params[f'synthesis/4x4/Conv/mod_weight'].transpose(),\n        r'synthesis\\.b4\\.conv1\\.affine\\.bias',              lambda:     tf_params[f'synthesis/4x4/Conv/mod_bias'] + 1,\n        r'synthesis\\.b(\\d+)\\.conv0\\.weight',                lambda r:   tf_params[f'synthesis/{r}x{r}/Conv0_up/weight'][::-1, ::-1].transpose(3, 2, 0, 1),\n        r'synthesis\\.b(\\d+)\\.conv0\\.bias',                  lambda r:   tf_params[f'synthesis/{r}x{r}/Conv0_up/bias'],\n        r'synthesis\\.b(\\d+)\\.conv0\\.noise_const',           lambda r:   tf_params[f'synthesis/noise{int(np.log2(int(r)))*2-5}'][0, 0],\n        r'synthesis\\.b(\\d+)\\.conv0\\.noise_strength',        lambda r:   tf_params[f'synthesis/{r}x{r}/Conv0_up/noise_strength'],\n        r'synthesis\\.b(\\d+)\\.conv0\\.affine\\.weight',        lambda r:   tf_params[f'synthesis/{r}x{r}/Conv0_up/mod_weight'].transpose(),\n        r'synthesis\\.b(\\d+)\\.conv0\\.affine\\.bias',          lambda r:   tf_params[f'synthesis/{r}x{r}/Conv0_up/mod_bias'] + 1,\n        r'synthesis\\.b(\\d+)\\.conv1\\.weight',                lambda r:   tf_params[f'synthesis/{r}x{r}/Conv1/weight'].transpose(3, 2, 0, 1),\n        r'synthesis\\.b(\\d+)\\.conv1\\.bias',                  lambda r:   tf_params[f'synthesis/{r}x{r}/Conv1/bias'],\n        r'synthesis\\.b(\\d+)\\.conv1\\.noise_const',           lambda r:   tf_params[f'synthesis/noise{int(np.log2(int(r)))*2-4}'][0, 0],\n        r'synthesis\\.b(\\d+)\\.conv1\\.noise_strength',        lambda r:   tf_params[f'synthesis/{r}x{r}/Conv1/noise_strength'],\n        r'synthesis\\.b(\\d+)\\.conv1\\.affine\\.weight',        lambda r:   tf_params[f'synthesis/{r}x{r}/Conv1/mod_weight'].transpose(),\n        r'synthesis\\.b(\\d+)\\.conv1\\.affine\\.bias',          lambda r:   tf_params[f'synthesis/{r}x{r}/Conv1/mod_bias'] + 1,\n        r'synthesis\\.b(\\d+)\\.torgb\\.weight',                lambda r:   tf_params[f'synthesis/{r}x{r}/ToRGB/weight'].transpose(3, 2, 0, 1),\n        r'synthesis\\.b(\\d+)\\.torgb\\.bias',                  lambda r:   tf_params[f'synthesis/{r}x{r}/ToRGB/bias'],\n        r'synthesis\\.b(\\d+)\\.torgb\\.affine\\.weight',        lambda r:   tf_params[f'synthesis/{r}x{r}/ToRGB/mod_weight'].transpose(),\n        r'synthesis\\.b(\\d+)\\.torgb\\.affine\\.bias',          lambda r:   tf_params[f'synthesis/{r}x{r}/ToRGB/mod_bias'] + 1,\n        r'synthesis\\.b(\\d+)\\.skip\\.weight',                 lambda r:   tf_params[f'synthesis/{r}x{r}/Skip/weight'][::-1, ::-1].transpose(3, 2, 0, 1),\n        r'.*\\.resample_filter',                             None,\n    )\n    return G\n\n#----------------------------------------------------------------------------\n\ndef convert_tf_discriminator(tf_D):\n    if tf_D.version < 4:\n        raise ValueError('TensorFlow pickle version too low')\n\n    # Collect kwargs.\n    tf_kwargs = tf_D.static_kwargs\n    known_kwargs = set()\n    def kwarg(tf_name, default=None):\n        known_kwargs.add(tf_name)\n        return tf_kwargs.get(tf_name, default)\n\n    # Convert kwargs.\n    kwargs = dnnlib.EasyDict(\n        c_dim                   = kwarg('label_size',           0),\n        img_resolution          = kwarg('resolution',           1024),\n        img_channels            = kwarg('num_channels',         3),\n        architecture            = kwarg('architecture',         'resnet'),\n        channel_base            = kwarg('fmap_base',            16384) * 2,\n        channel_max             = kwarg('fmap_max',             512),\n        num_fp16_res            = kwarg('num_fp16_res',         0),\n        conv_clamp              = kwarg('conv_clamp',           None),\n        cmap_dim                = kwarg('mapping_fmaps',        None),\n        block_kwargs = dnnlib.EasyDict(\n            activation          = kwarg('nonlinearity',         'lrelu'),\n            resample_filter     = kwarg('resample_kernel',      [1,3,3,1]),\n            freeze_layers       = kwarg('freeze_layers',        0),\n        ),\n        mapping_kwargs = dnnlib.EasyDict(\n            num_layers          = kwarg('mapping_layers',       0),\n            embed_features      = kwarg('mapping_fmaps',        None),\n            layer_features      = kwarg('mapping_fmaps',        None),\n            activation          = kwarg('nonlinearity',         'lrelu'),\n            lr_multiplier       = kwarg('mapping_lrmul',        0.1),\n        ),\n        epilogue_kwargs = dnnlib.EasyDict(\n            mbstd_group_size    = kwarg('mbstd_group_size',     None),\n            mbstd_num_channels  = kwarg('mbstd_num_features',   1),\n            activation          = kwarg('nonlinearity',         'lrelu'),\n        ),\n    )\n\n    # Check for unknown kwargs.\n    kwarg('structure')\n    unknown_kwargs = list(set(tf_kwargs.keys()) - known_kwargs)\n    if len(unknown_kwargs) > 0:\n        raise ValueError('Unknown TensorFlow kwarg', unknown_kwargs[0])\n\n    # Collect params.\n    tf_params = _collect_tf_params(tf_D)\n    for name, value in list(tf_params.items()):\n        match = re.fullmatch(r'FromRGB_lod(\\d+)/(.*)', name)\n        if match:\n            r = kwargs.img_resolution // (2 ** int(match.group(1)))\n            tf_params[f'{r}x{r}/FromRGB/{match.group(2)}'] = value\n            kwargs.architecture = 'orig'\n    #for name, value in tf_params.items(): print(f'{name:<50s}{list(value.shape)}')\n\n    # Convert params.\n    from training import networks\n    D = networks.Discriminator(**kwargs).eval().requires_grad_(False)\n    # pylint: disable=unnecessary-lambda\n    _populate_module_params(D,\n        r'b(\\d+)\\.fromrgb\\.weight',     lambda r:       tf_params[f'{r}x{r}/FromRGB/weight'].transpose(3, 2, 0, 1),\n        r'b(\\d+)\\.fromrgb\\.bias',       lambda r:       tf_params[f'{r}x{r}/FromRGB/bias'],\n        r'b(\\d+)\\.conv(\\d+)\\.weight',   lambda r, i:    tf_params[f'{r}x{r}/Conv{i}{[\"\",\"_down\"][int(i)]}/weight'].transpose(3, 2, 0, 1),\n        r'b(\\d+)\\.conv(\\d+)\\.bias',     lambda r, i:    tf_params[f'{r}x{r}/Conv{i}{[\"\",\"_down\"][int(i)]}/bias'],\n        r'b(\\d+)\\.skip\\.weight',        lambda r:       tf_params[f'{r}x{r}/Skip/weight'].transpose(3, 2, 0, 1),\n        r'mapping\\.embed\\.weight',      lambda:         tf_params[f'LabelEmbed/weight'].transpose(),\n        r'mapping\\.embed\\.bias',        lambda:         tf_params[f'LabelEmbed/bias'],\n        r'mapping\\.fc(\\d+)\\.weight',    lambda i:       tf_params[f'Mapping{i}/weight'].transpose(),\n        r'mapping\\.fc(\\d+)\\.bias',      lambda i:       tf_params[f'Mapping{i}/bias'],\n        r'b4\\.conv\\.weight',            lambda:         tf_params[f'4x4/Conv/weight'].transpose(3, 2, 0, 1),\n        r'b4\\.conv\\.bias',              lambda:         tf_params[f'4x4/Conv/bias'],\n        r'b4\\.fc\\.weight',              lambda:         tf_params[f'4x4/Dense0/weight'].transpose(),\n        r'b4\\.fc\\.bias',                lambda:         tf_params[f'4x4/Dense0/bias'],\n        r'b4\\.out\\.weight',             lambda:         tf_params[f'Output/weight'].transpose(),\n        r'b4\\.out\\.bias',               lambda:         tf_params[f'Output/bias'],\n        r'.*\\.resample_filter',         None,\n    )\n    return D\n\n#----------------------------------------------------------------------------\n\n@click.command()\n@click.option('--source', help='Input pickle', required=True, metavar='PATH')\n@click.option('--dest', help='Output pickle', required=True, metavar='PATH')\n@click.option('--force-fp16', help='Force the networks to use FP16', type=bool, default=False, metavar='BOOL', show_default=True)\ndef convert_network_pickle(source, dest, force_fp16):\n    \"\"\"Convert legacy network pickle into the native PyTorch format.\n\n    The tool is able to load the main network configurations exported using the TensorFlow version of StyleGAN2 or StyleGAN2-ADA.\n    It does not support e.g. StyleGAN2-ADA comparison methods, StyleGAN2 configs A-D, or StyleGAN1 networks.\n\n    Example:\n\n    \\b\n    python legacy.py \\\\\n        --source=https://nvlabs-fi-cdn.nvidia.com/stylegan2/networks/stylegan2-cat-config-f.pkl \\\\\n        --dest=stylegan2-cat-config-f.pkl\n    \"\"\"\n    print(f'Loading \"{source}\"...')\n    with dnnlib.util.open_url(source) as f:\n        data = load_network_pkl(f, force_fp16=force_fp16)\n    print(f'Saving \"{dest}\"...')\n    with open(dest, 'wb') as f:\n        pickle.dump(data, f)\n    print('Done.')\n\n#----------------------------------------------------------------------------\n\nif __name__ == \"__main__\":\n    convert_network_pickle() # pylint: disable=no-value-for-parameter\n\n#----------------------------------------------------------------------------\n"
        },
        {
          "name": "metrics",
          "type": "tree",
          "content": null
        },
        {
          "name": "projector.py",
          "type": "blob",
          "size": 8.779296875,
          "content": "# Copyright (c) 2021, NVIDIA CORPORATION.  All rights reserved.\n#\n# NVIDIA CORPORATION and its licensors retain all intellectual property\n# and proprietary rights in and to this software, related documentation\n# and any modifications thereto.  Any use, reproduction, disclosure or\n# distribution of this software and related documentation without an express\n# license agreement from NVIDIA CORPORATION is strictly prohibited.\n\n\"\"\"Project given image to the latent space of pretrained network pickle.\"\"\"\n\nimport copy\nimport os\nfrom time import perf_counter\n\nimport click\nimport imageio\nimport numpy as np\nimport PIL.Image\nimport torch\nimport torch.nn.functional as F\n\nimport dnnlib\nimport legacy\n\ndef project(\n    G,\n    target: torch.Tensor, # [C,H,W] and dynamic range [0,255], W & H must match G output resolution\n    *,\n    num_steps                  = 1000,\n    w_avg_samples              = 10000,\n    initial_learning_rate      = 0.1,\n    initial_noise_factor       = 0.05,\n    lr_rampdown_length         = 0.25,\n    lr_rampup_length           = 0.05,\n    noise_ramp_length          = 0.75,\n    regularize_noise_weight    = 1e5,\n    verbose                    = False,\n    device: torch.device\n):\n    assert target.shape == (G.img_channels, G.img_resolution, G.img_resolution)\n\n    def logprint(*args):\n        if verbose:\n            print(*args)\n\n    G = copy.deepcopy(G).eval().requires_grad_(False).to(device) # type: ignore\n\n    # Compute w stats.\n    logprint(f'Computing W midpoint and stddev using {w_avg_samples} samples...')\n    z_samples = np.random.RandomState(123).randn(w_avg_samples, G.z_dim)\n    w_samples = G.mapping(torch.from_numpy(z_samples).to(device), None)  # [N, L, C]\n    w_samples = w_samples[:, :1, :].cpu().numpy().astype(np.float32)       # [N, 1, C]\n    w_avg = np.mean(w_samples, axis=0, keepdims=True)      # [1, 1, C]\n    w_std = (np.sum((w_samples - w_avg) ** 2) / w_avg_samples) ** 0.5\n\n    # Setup noise inputs.\n    noise_bufs = { name: buf for (name, buf) in G.synthesis.named_buffers() if 'noise_const' in name }\n\n    # Load VGG16 feature detector.\n    url = 'https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/metrics/vgg16.pt'\n    with dnnlib.util.open_url(url) as f:\n        vgg16 = torch.jit.load(f).eval().to(device)\n\n    # Features for target image.\n    target_images = target.unsqueeze(0).to(device).to(torch.float32)\n    if target_images.shape[2] > 256:\n        target_images = F.interpolate(target_images, size=(256, 256), mode='area')\n    target_features = vgg16(target_images, resize_images=False, return_lpips=True)\n\n    w_opt = torch.tensor(w_avg, dtype=torch.float32, device=device, requires_grad=True) # pylint: disable=not-callable\n    w_out = torch.zeros([num_steps] + list(w_opt.shape[1:]), dtype=torch.float32, device=device)\n    optimizer = torch.optim.Adam([w_opt] + list(noise_bufs.values()), betas=(0.9, 0.999), lr=initial_learning_rate)\n\n    # Init noise.\n    for buf in noise_bufs.values():\n        buf[:] = torch.randn_like(buf)\n        buf.requires_grad = True\n\n    for step in range(num_steps):\n        # Learning rate schedule.\n        t = step / num_steps\n        w_noise_scale = w_std * initial_noise_factor * max(0.0, 1.0 - t / noise_ramp_length) ** 2\n        lr_ramp = min(1.0, (1.0 - t) / lr_rampdown_length)\n        lr_ramp = 0.5 - 0.5 * np.cos(lr_ramp * np.pi)\n        lr_ramp = lr_ramp * min(1.0, t / lr_rampup_length)\n        lr = initial_learning_rate * lr_ramp\n        for param_group in optimizer.param_groups:\n            param_group['lr'] = lr\n\n        # Synth images from opt_w.\n        w_noise = torch.randn_like(w_opt) * w_noise_scale\n        ws = (w_opt + w_noise).repeat([1, G.mapping.num_ws, 1])\n        synth_images = G.synthesis(ws, noise_mode='const')\n\n        # Downsample image to 256x256 if it's larger than that. VGG was built for 224x224 images.\n        synth_images = (synth_images + 1) * (255/2)\n        if synth_images.shape[2] > 256:\n            synth_images = F.interpolate(synth_images, size=(256, 256), mode='area')\n\n        # Features for synth images.\n        synth_features = vgg16(synth_images, resize_images=False, return_lpips=True)\n        dist = (target_features - synth_features).square().sum()\n\n        # Noise regularization.\n        reg_loss = 0.0\n        for v in noise_bufs.values():\n            noise = v[None,None,:,:] # must be [1,1,H,W] for F.avg_pool2d()\n            while True:\n                reg_loss += (noise*torch.roll(noise, shifts=1, dims=3)).mean()**2\n                reg_loss += (noise*torch.roll(noise, shifts=1, dims=2)).mean()**2\n                if noise.shape[2] <= 8:\n                    break\n                noise = F.avg_pool2d(noise, kernel_size=2)\n        loss = dist + reg_loss * regularize_noise_weight\n\n        # Step\n        optimizer.zero_grad(set_to_none=True)\n        loss.backward()\n        optimizer.step()\n        logprint(f'step {step+1:>4d}/{num_steps}: dist {dist:<4.2f} loss {float(loss):<5.2f}')\n\n        # Save projected W for each optimization step.\n        w_out[step] = w_opt.detach()[0]\n\n        # Normalize noise.\n        with torch.no_grad():\n            for buf in noise_bufs.values():\n                buf -= buf.mean()\n                buf *= buf.square().mean().rsqrt()\n\n    return w_out.repeat([1, G.mapping.num_ws, 1])\n\n#----------------------------------------------------------------------------\n\n@click.command()\n@click.option('--network', 'network_pkl', help='Network pickle filename', required=True)\n@click.option('--target', 'target_fname', help='Target image file to project to', required=True, metavar='FILE')\n@click.option('--num-steps',              help='Number of optimization steps', type=int, default=1000, show_default=True)\n@click.option('--seed',                   help='Random seed', type=int, default=303, show_default=True)\n@click.option('--save-video',             help='Save an mp4 video of optimization progress', type=bool, default=True, show_default=True)\n@click.option('--outdir',                 help='Where to save the output images', required=True, metavar='DIR')\ndef run_projection(\n    network_pkl: str,\n    target_fname: str,\n    outdir: str,\n    save_video: bool,\n    seed: int,\n    num_steps: int\n):\n    \"\"\"Project given image to the latent space of pretrained network pickle.\n\n    Examples:\n\n    \\b\n    python projector.py --outdir=out --target=~/mytargetimg.png \\\\\n        --network=https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/ffhq.pkl\n    \"\"\"\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n\n    # Load networks.\n    print('Loading networks from \"%s\"...' % network_pkl)\n    device = torch.device('cuda')\n    with dnnlib.util.open_url(network_pkl) as fp:\n        G = legacy.load_network_pkl(fp)['G_ema'].requires_grad_(False).to(device) # type: ignore\n\n    # Load target image.\n    target_pil = PIL.Image.open(target_fname).convert('RGB')\n    w, h = target_pil.size\n    s = min(w, h)\n    target_pil = target_pil.crop(((w - s) // 2, (h - s) // 2, (w + s) // 2, (h + s) // 2))\n    target_pil = target_pil.resize((G.img_resolution, G.img_resolution), PIL.Image.LANCZOS)\n    target_uint8 = np.array(target_pil, dtype=np.uint8)\n\n    # Optimize projection.\n    start_time = perf_counter()\n    projected_w_steps = project(\n        G,\n        target=torch.tensor(target_uint8.transpose([2, 0, 1]), device=device), # pylint: disable=not-callable\n        num_steps=num_steps,\n        device=device,\n        verbose=True\n    )\n    print (f'Elapsed: {(perf_counter()-start_time):.1f} s')\n\n    # Render debug output: optional video and projected image and W vector.\n    os.makedirs(outdir, exist_ok=True)\n    if save_video:\n        video = imageio.get_writer(f'{outdir}/proj.mp4', mode='I', fps=10, codec='libx264', bitrate='16M')\n        print (f'Saving optimization progress video \"{outdir}/proj.mp4\"')\n        for projected_w in projected_w_steps:\n            synth_image = G.synthesis(projected_w.unsqueeze(0), noise_mode='const')\n            synth_image = (synth_image + 1) * (255/2)\n            synth_image = synth_image.permute(0, 2, 3, 1).clamp(0, 255).to(torch.uint8)[0].cpu().numpy()\n            video.append_data(np.concatenate([target_uint8, synth_image], axis=1))\n        video.close()\n\n    # Save final projected frame and W vector.\n    target_pil.save(f'{outdir}/target.png')\n    projected_w = projected_w_steps[-1]\n    synth_image = G.synthesis(projected_w.unsqueeze(0), noise_mode='const')\n    synth_image = (synth_image + 1) * (255/2)\n    synth_image = synth_image.permute(0, 2, 3, 1).clamp(0, 255).to(torch.uint8)[0].cpu().numpy()\n    PIL.Image.fromarray(synth_image, 'RGB').save(f'{outdir}/proj.png')\n    np.savez(f'{outdir}/projected_w.npz', w=projected_w.unsqueeze(0).cpu().numpy())\n\n#----------------------------------------------------------------------------\n\nif __name__ == \"__main__\":\n    run_projection() # pylint: disable=no-value-for-parameter\n\n#----------------------------------------------------------------------------\n"
        },
        {
          "name": "style_mixing.py",
          "type": "blob",
          "size": 4.7763671875,
          "content": "# Copyright (c) 2021, NVIDIA CORPORATION.  All rights reserved.\n#\n# NVIDIA CORPORATION and its licensors retain all intellectual property\n# and proprietary rights in and to this software, related documentation\n# and any modifications thereto.  Any use, reproduction, disclosure or\n# distribution of this software and related documentation without an express\n# license agreement from NVIDIA CORPORATION is strictly prohibited.\n\n\"\"\"Generate style mixing image matrix using pretrained network pickle.\"\"\"\n\nimport os\nimport re\nfrom typing import List\n\nimport click\nimport dnnlib\nimport numpy as np\nimport PIL.Image\nimport torch\n\nimport legacy\n\n#----------------------------------------------------------------------------\n\ndef num_range(s: str) -> List[int]:\n    '''Accept either a comma separated list of numbers 'a,b,c' or a range 'a-c' and return as a list of ints.'''\n\n    range_re = re.compile(r'^(\\d+)-(\\d+)$')\n    m = range_re.match(s)\n    if m:\n        return list(range(int(m.group(1)), int(m.group(2))+1))\n    vals = s.split(',')\n    return [int(x) for x in vals]\n\n#----------------------------------------------------------------------------\n\n@click.command()\n@click.option('--network', 'network_pkl', help='Network pickle filename', required=True)\n@click.option('--rows', 'row_seeds', type=num_range, help='Random seeds to use for image rows', required=True)\n@click.option('--cols', 'col_seeds', type=num_range, help='Random seeds to use for image columns', required=True)\n@click.option('--styles', 'col_styles', type=num_range, help='Style layer range', default='0-6', show_default=True)\n@click.option('--trunc', 'truncation_psi', type=float, help='Truncation psi', default=1, show_default=True)\n@click.option('--noise-mode', help='Noise mode', type=click.Choice(['const', 'random', 'none']), default='const', show_default=True)\n@click.option('--outdir', type=str, required=True)\ndef generate_style_mix(\n    network_pkl: str,\n    row_seeds: List[int],\n    col_seeds: List[int],\n    col_styles: List[int],\n    truncation_psi: float,\n    noise_mode: str,\n    outdir: str\n):\n    \"\"\"Generate images using pretrained network pickle.\n\n    Examples:\n\n    \\b\n    python style_mixing.py --outdir=out --rows=85,100,75,458,1500 --cols=55,821,1789,293 \\\\\n        --network=https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/metfaces.pkl\n    \"\"\"\n    print('Loading networks from \"%s\"...' % network_pkl)\n    device = torch.device('cuda')\n    with dnnlib.util.open_url(network_pkl) as f:\n        G = legacy.load_network_pkl(f)['G_ema'].to(device) # type: ignore\n\n    os.makedirs(outdir, exist_ok=True)\n\n    print('Generating W vectors...')\n    all_seeds = list(set(row_seeds + col_seeds))\n    all_z = np.stack([np.random.RandomState(seed).randn(G.z_dim) for seed in all_seeds])\n    all_w = G.mapping(torch.from_numpy(all_z).to(device), None)\n    w_avg = G.mapping.w_avg\n    all_w = w_avg + (all_w - w_avg) * truncation_psi\n    w_dict = {seed: w for seed, w in zip(all_seeds, list(all_w))}\n\n    print('Generating images...')\n    all_images = G.synthesis(all_w, noise_mode=noise_mode)\n    all_images = (all_images.permute(0, 2, 3, 1) * 127.5 + 128).clamp(0, 255).to(torch.uint8).cpu().numpy()\n    image_dict = {(seed, seed): image for seed, image in zip(all_seeds, list(all_images))}\n\n    print('Generating style-mixed images...')\n    for row_seed in row_seeds:\n        for col_seed in col_seeds:\n            w = w_dict[row_seed].clone()\n            w[col_styles] = w_dict[col_seed][col_styles]\n            image = G.synthesis(w[np.newaxis], noise_mode=noise_mode)\n            image = (image.permute(0, 2, 3, 1) * 127.5 + 128).clamp(0, 255).to(torch.uint8)\n            image_dict[(row_seed, col_seed)] = image[0].cpu().numpy()\n\n    print('Saving images...')\n    os.makedirs(outdir, exist_ok=True)\n    for (row_seed, col_seed), image in image_dict.items():\n        PIL.Image.fromarray(image, 'RGB').save(f'{outdir}/{row_seed}-{col_seed}.png')\n\n    print('Saving image grid...')\n    W = G.img_resolution\n    H = G.img_resolution\n    canvas = PIL.Image.new('RGB', (W * (len(col_seeds) + 1), H * (len(row_seeds) + 1)), 'black')\n    for row_idx, row_seed in enumerate([0] + row_seeds):\n        for col_idx, col_seed in enumerate([0] + col_seeds):\n            if row_idx == 0 and col_idx == 0:\n                continue\n            key = (row_seed, col_seed)\n            if row_idx == 0:\n                key = (col_seed, col_seed)\n            if col_idx == 0:\n                key = (row_seed, row_seed)\n            canvas.paste(PIL.Image.fromarray(image_dict[key], 'RGB'), (W * col_idx, H * row_idx))\n    canvas.save(f'{outdir}/grid.png')\n\n\n#----------------------------------------------------------------------------\n\nif __name__ == \"__main__\":\n    generate_style_mix() # pylint: disable=no-value-for-parameter\n\n#----------------------------------------------------------------------------\n"
        },
        {
          "name": "torch_utils",
          "type": "tree",
          "content": null
        },
        {
          "name": "train.py",
          "type": "blob",
          "size": 23.5029296875,
          "content": "# Copyright (c) 2021, NVIDIA CORPORATION.  All rights reserved.\n#\n# NVIDIA CORPORATION and its licensors retain all intellectual property\n# and proprietary rights in and to this software, related documentation\n# and any modifications thereto.  Any use, reproduction, disclosure or\n# distribution of this software and related documentation without an express\n# license agreement from NVIDIA CORPORATION is strictly prohibited.\n\n\"\"\"Train a GAN using the techniques described in the paper\n\"Training Generative Adversarial Networks with Limited Data\".\"\"\"\n\nimport os\nimport click\nimport re\nimport json\nimport tempfile\nimport torch\nimport dnnlib\n\nfrom training import training_loop\nfrom metrics import metric_main\nfrom torch_utils import training_stats\nfrom torch_utils import custom_ops\n\n#----------------------------------------------------------------------------\n\nclass UserError(Exception):\n    pass\n\n#----------------------------------------------------------------------------\n\ndef setup_training_loop_kwargs(\n    # General options (not included in desc).\n    gpus       = None, # Number of GPUs: <int>, default = 1 gpu\n    snap       = None, # Snapshot interval: <int>, default = 50 ticks\n    metrics    = None, # List of metric names: [], ['fid50k_full'] (default), ...\n    seed       = None, # Random seed: <int>, default = 0\n\n    # Dataset.\n    data       = None, # Training dataset (required): <path>\n    cond       = None, # Train conditional model based on dataset labels: <bool>, default = False\n    subset     = None, # Train with only N images: <int>, default = all\n    mirror     = None, # Augment dataset with x-flips: <bool>, default = False\n\n    # Base config.\n    cfg        = None, # Base config: 'auto' (default), 'stylegan2', 'paper256', 'paper512', 'paper1024', 'cifar'\n    gamma      = None, # Override R1 gamma: <float>\n    kimg       = None, # Override training duration: <int>\n    batch      = None, # Override batch size: <int>\n\n    # Discriminator augmentation.\n    aug        = None, # Augmentation mode: 'ada' (default), 'noaug', 'fixed'\n    p          = None, # Specify p for 'fixed' (required): <float>\n    target     = None, # Override ADA target for 'ada': <float>, default = depends on aug\n    augpipe    = None, # Augmentation pipeline: 'blit', 'geom', 'color', 'filter', 'noise', 'cutout', 'bg', 'bgc' (default), ..., 'bgcfnc'\n\n    # Transfer learning.\n    resume     = None, # Load previous network: 'noresume' (default), 'ffhq256', 'ffhq512', 'ffhq1024', 'celebahq256', 'lsundog256', <file>, <url>\n    freezed    = None, # Freeze-D: <int>, default = 0 discriminator layers\n\n    # Performance options (not included in desc).\n    fp32       = None, # Disable mixed-precision training: <bool>, default = False\n    nhwc       = None, # Use NHWC memory format with FP16: <bool>, default = False\n    allow_tf32 = None, # Allow PyTorch to use TF32 for matmul and convolutions: <bool>, default = False\n    nobench    = None, # Disable cuDNN benchmarking: <bool>, default = False\n    workers    = None, # Override number of DataLoader workers: <int>, default = 3\n):\n    args = dnnlib.EasyDict()\n\n    # ------------------------------------------\n    # General options: gpus, snap, metrics, seed\n    # ------------------------------------------\n\n    if gpus is None:\n        gpus = 1\n    assert isinstance(gpus, int)\n    if not (gpus >= 1 and gpus & (gpus - 1) == 0):\n        raise UserError('--gpus must be a power of two')\n    args.num_gpus = gpus\n\n    if snap is None:\n        snap = 50\n    assert isinstance(snap, int)\n    if snap < 1:\n        raise UserError('--snap must be at least 1')\n    args.image_snapshot_ticks = snap\n    args.network_snapshot_ticks = snap\n\n    if metrics is None:\n        metrics = ['fid50k_full']\n    assert isinstance(metrics, list)\n    if not all(metric_main.is_valid_metric(metric) for metric in metrics):\n        raise UserError('\\n'.join(['--metrics can only contain the following values:'] + metric_main.list_valid_metrics()))\n    args.metrics = metrics\n\n    if seed is None:\n        seed = 0\n    assert isinstance(seed, int)\n    args.random_seed = seed\n\n    # -----------------------------------\n    # Dataset: data, cond, subset, mirror\n    # -----------------------------------\n\n    assert data is not None\n    assert isinstance(data, str)\n    args.training_set_kwargs = dnnlib.EasyDict(class_name='training.dataset.ImageFolderDataset', path=data, use_labels=True, max_size=None, xflip=False)\n    args.data_loader_kwargs = dnnlib.EasyDict(pin_memory=True, num_workers=3, prefetch_factor=2)\n    try:\n        training_set = dnnlib.util.construct_class_by_name(**args.training_set_kwargs) # subclass of training.dataset.Dataset\n        args.training_set_kwargs.resolution = training_set.resolution # be explicit about resolution\n        args.training_set_kwargs.use_labels = training_set.has_labels # be explicit about labels\n        args.training_set_kwargs.max_size = len(training_set) # be explicit about dataset size\n        desc = training_set.name\n        del training_set # conserve memory\n    except IOError as err:\n        raise UserError(f'--data: {err}')\n\n    if cond is None:\n        cond = False\n    assert isinstance(cond, bool)\n    if cond:\n        if not args.training_set_kwargs.use_labels:\n            raise UserError('--cond=True requires labels specified in dataset.json')\n        desc += '-cond'\n    else:\n        args.training_set_kwargs.use_labels = False\n\n    if subset is not None:\n        assert isinstance(subset, int)\n        if not 1 <= subset <= args.training_set_kwargs.max_size:\n            raise UserError(f'--subset must be between 1 and {args.training_set_kwargs.max_size}')\n        desc += f'-subset{subset}'\n        if subset < args.training_set_kwargs.max_size:\n            args.training_set_kwargs.max_size = subset\n            args.training_set_kwargs.random_seed = args.random_seed\n\n    if mirror is None:\n        mirror = False\n    assert isinstance(mirror, bool)\n    if mirror:\n        desc += '-mirror'\n        args.training_set_kwargs.xflip = True\n\n    # ------------------------------------\n    # Base config: cfg, gamma, kimg, batch\n    # ------------------------------------\n\n    if cfg is None:\n        cfg = 'auto'\n    assert isinstance(cfg, str)\n    desc += f'-{cfg}'\n\n    cfg_specs = {\n        'auto':      dict(ref_gpus=-1, kimg=25000,  mb=-1, mbstd=-1, fmaps=-1,  lrate=-1,     gamma=-1,   ema=-1,  ramp=0.05, map=2), # Populated dynamically based on resolution and GPU count.\n        'stylegan2': dict(ref_gpus=8,  kimg=25000,  mb=32, mbstd=4,  fmaps=1,   lrate=0.002,  gamma=10,   ema=10,  ramp=None, map=8), # Uses mixed-precision, unlike the original StyleGAN2.\n        'paper256':  dict(ref_gpus=8,  kimg=25000,  mb=64, mbstd=8,  fmaps=0.5, lrate=0.0025, gamma=1,    ema=20,  ramp=None, map=8),\n        'paper512':  dict(ref_gpus=8,  kimg=25000,  mb=64, mbstd=8,  fmaps=1,   lrate=0.0025, gamma=0.5,  ema=20,  ramp=None, map=8),\n        'paper1024': dict(ref_gpus=8,  kimg=25000,  mb=32, mbstd=4,  fmaps=1,   lrate=0.002,  gamma=2,    ema=10,  ramp=None, map=8),\n        'cifar':     dict(ref_gpus=2,  kimg=100000, mb=64, mbstd=32, fmaps=1,   lrate=0.0025, gamma=0.01, ema=500, ramp=0.05, map=2),\n    }\n\n    assert cfg in cfg_specs\n    spec = dnnlib.EasyDict(cfg_specs[cfg])\n    if cfg == 'auto':\n        desc += f'{gpus:d}'\n        spec.ref_gpus = gpus\n        res = args.training_set_kwargs.resolution\n        spec.mb = max(min(gpus * min(4096 // res, 32), 64), gpus) # keep gpu memory consumption at bay\n        spec.mbstd = min(spec.mb // gpus, 4) # other hyperparams behave more predictably if mbstd group size remains fixed\n        spec.fmaps = 1 if res >= 512 else 0.5\n        spec.lrate = 0.002 if res >= 1024 else 0.0025\n        spec.gamma = 0.0002 * (res ** 2) / spec.mb # heuristic formula\n        spec.ema = spec.mb * 10 / 32\n\n    args.G_kwargs = dnnlib.EasyDict(class_name='training.networks.Generator', z_dim=512, w_dim=512, mapping_kwargs=dnnlib.EasyDict(), synthesis_kwargs=dnnlib.EasyDict())\n    args.D_kwargs = dnnlib.EasyDict(class_name='training.networks.Discriminator', block_kwargs=dnnlib.EasyDict(), mapping_kwargs=dnnlib.EasyDict(), epilogue_kwargs=dnnlib.EasyDict())\n    args.G_kwargs.synthesis_kwargs.channel_base = args.D_kwargs.channel_base = int(spec.fmaps * 32768)\n    args.G_kwargs.synthesis_kwargs.channel_max = args.D_kwargs.channel_max = 512\n    args.G_kwargs.mapping_kwargs.num_layers = spec.map\n    args.G_kwargs.synthesis_kwargs.num_fp16_res = args.D_kwargs.num_fp16_res = 4 # enable mixed-precision training\n    args.G_kwargs.synthesis_kwargs.conv_clamp = args.D_kwargs.conv_clamp = 256 # clamp activations to avoid float16 overflow\n    args.D_kwargs.epilogue_kwargs.mbstd_group_size = spec.mbstd\n\n    args.G_opt_kwargs = dnnlib.EasyDict(class_name='torch.optim.Adam', lr=spec.lrate, betas=[0,0.99], eps=1e-8)\n    args.D_opt_kwargs = dnnlib.EasyDict(class_name='torch.optim.Adam', lr=spec.lrate, betas=[0,0.99], eps=1e-8)\n    args.loss_kwargs = dnnlib.EasyDict(class_name='training.loss.StyleGAN2Loss', r1_gamma=spec.gamma)\n\n    args.total_kimg = spec.kimg\n    args.batch_size = spec.mb\n    args.batch_gpu = spec.mb // spec.ref_gpus\n    args.ema_kimg = spec.ema\n    args.ema_rampup = spec.ramp\n\n    if cfg == 'cifar':\n        args.loss_kwargs.pl_weight = 0 # disable path length regularization\n        args.loss_kwargs.style_mixing_prob = 0 # disable style mixing\n        args.D_kwargs.architecture = 'orig' # disable residual skip connections\n\n    if gamma is not None:\n        assert isinstance(gamma, float)\n        if not gamma >= 0:\n            raise UserError('--gamma must be non-negative')\n        desc += f'-gamma{gamma:g}'\n        args.loss_kwargs.r1_gamma = gamma\n\n    if kimg is not None:\n        assert isinstance(kimg, int)\n        if not kimg >= 1:\n            raise UserError('--kimg must be at least 1')\n        desc += f'-kimg{kimg:d}'\n        args.total_kimg = kimg\n\n    if batch is not None:\n        assert isinstance(batch, int)\n        if not (batch >= 1 and batch % gpus == 0):\n            raise UserError('--batch must be at least 1 and divisible by --gpus')\n        desc += f'-batch{batch}'\n        args.batch_size = batch\n        args.batch_gpu = batch // gpus\n\n    # ---------------------------------------------------\n    # Discriminator augmentation: aug, p, target, augpipe\n    # ---------------------------------------------------\n\n    if aug is None:\n        aug = 'ada'\n    else:\n        assert isinstance(aug, str)\n        desc += f'-{aug}'\n\n    if aug == 'ada':\n        args.ada_target = 0.6\n\n    elif aug == 'noaug':\n        pass\n\n    elif aug == 'fixed':\n        if p is None:\n            raise UserError(f'--aug={aug} requires specifying --p')\n\n    else:\n        raise UserError(f'--aug={aug} not supported')\n\n    if p is not None:\n        assert isinstance(p, float)\n        if aug != 'fixed':\n            raise UserError('--p can only be specified with --aug=fixed')\n        if not 0 <= p <= 1:\n            raise UserError('--p must be between 0 and 1')\n        desc += f'-p{p:g}'\n        args.augment_p = p\n\n    if target is not None:\n        assert isinstance(target, float)\n        if aug != 'ada':\n            raise UserError('--target can only be specified with --aug=ada')\n        if not 0 <= target <= 1:\n            raise UserError('--target must be between 0 and 1')\n        desc += f'-target{target:g}'\n        args.ada_target = target\n\n    assert augpipe is None or isinstance(augpipe, str)\n    if augpipe is None:\n        augpipe = 'bgc'\n    else:\n        if aug == 'noaug':\n            raise UserError('--augpipe cannot be specified with --aug=noaug')\n        desc += f'-{augpipe}'\n\n    augpipe_specs = {\n        'blit':   dict(xflip=1, rotate90=1, xint=1),\n        'geom':   dict(scale=1, rotate=1, aniso=1, xfrac=1),\n        'color':  dict(brightness=1, contrast=1, lumaflip=1, hue=1, saturation=1),\n        'filter': dict(imgfilter=1),\n        'noise':  dict(noise=1),\n        'cutout': dict(cutout=1),\n        'bg':     dict(xflip=1, rotate90=1, xint=1, scale=1, rotate=1, aniso=1, xfrac=1),\n        'bgc':    dict(xflip=1, rotate90=1, xint=1, scale=1, rotate=1, aniso=1, xfrac=1, brightness=1, contrast=1, lumaflip=1, hue=1, saturation=1),\n        'bgcf':   dict(xflip=1, rotate90=1, xint=1, scale=1, rotate=1, aniso=1, xfrac=1, brightness=1, contrast=1, lumaflip=1, hue=1, saturation=1, imgfilter=1),\n        'bgcfn':  dict(xflip=1, rotate90=1, xint=1, scale=1, rotate=1, aniso=1, xfrac=1, brightness=1, contrast=1, lumaflip=1, hue=1, saturation=1, imgfilter=1, noise=1),\n        'bgcfnc': dict(xflip=1, rotate90=1, xint=1, scale=1, rotate=1, aniso=1, xfrac=1, brightness=1, contrast=1, lumaflip=1, hue=1, saturation=1, imgfilter=1, noise=1, cutout=1),\n    }\n\n    assert augpipe in augpipe_specs\n    if aug != 'noaug':\n        args.augment_kwargs = dnnlib.EasyDict(class_name='training.augment.AugmentPipe', **augpipe_specs[augpipe])\n\n    # ----------------------------------\n    # Transfer learning: resume, freezed\n    # ----------------------------------\n\n    resume_specs = {\n        'ffhq256':     'https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/transfer-learning-source-nets/ffhq-res256-mirror-paper256-noaug.pkl',\n        'ffhq512':     'https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/transfer-learning-source-nets/ffhq-res512-mirror-stylegan2-noaug.pkl',\n        'ffhq1024':    'https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/transfer-learning-source-nets/ffhq-res1024-mirror-stylegan2-noaug.pkl',\n        'celebahq256': 'https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/transfer-learning-source-nets/celebahq-res256-mirror-paper256-kimg100000-ada-target0.5.pkl',\n        'lsundog256':  'https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/transfer-learning-source-nets/lsundog-res256-paper256-kimg100000-noaug.pkl',\n    }\n\n    assert resume is None or isinstance(resume, str)\n    if resume is None:\n        resume = 'noresume'\n    elif resume == 'noresume':\n        desc += '-noresume'\n    elif resume in resume_specs:\n        desc += f'-resume{resume}'\n        args.resume_pkl = resume_specs[resume] # predefined url\n    else:\n        desc += '-resumecustom'\n        args.resume_pkl = resume # custom path or url\n\n    if resume != 'noresume':\n        args.ada_kimg = 100 # make ADA react faster at the beginning\n        args.ema_rampup = None # disable EMA rampup\n\n    if freezed is not None:\n        assert isinstance(freezed, int)\n        if not freezed >= 0:\n            raise UserError('--freezed must be non-negative')\n        desc += f'-freezed{freezed:d}'\n        args.D_kwargs.block_kwargs.freeze_layers = freezed\n\n    # -------------------------------------------------\n    # Performance options: fp32, nhwc, nobench, workers\n    # -------------------------------------------------\n\n    if fp32 is None:\n        fp32 = False\n    assert isinstance(fp32, bool)\n    if fp32:\n        args.G_kwargs.synthesis_kwargs.num_fp16_res = args.D_kwargs.num_fp16_res = 0\n        args.G_kwargs.synthesis_kwargs.conv_clamp = args.D_kwargs.conv_clamp = None\n\n    if nhwc is None:\n        nhwc = False\n    assert isinstance(nhwc, bool)\n    if nhwc:\n        args.G_kwargs.synthesis_kwargs.fp16_channels_last = args.D_kwargs.block_kwargs.fp16_channels_last = True\n\n    if nobench is None:\n        nobench = False\n    assert isinstance(nobench, bool)\n    if nobench:\n        args.cudnn_benchmark = False\n\n    if allow_tf32 is None:\n        allow_tf32 = False\n    assert isinstance(allow_tf32, bool)\n    if allow_tf32:\n        args.allow_tf32 = True\n\n    if workers is not None:\n        assert isinstance(workers, int)\n        if not workers >= 1:\n            raise UserError('--workers must be at least 1')\n        args.data_loader_kwargs.num_workers = workers\n\n    return desc, args\n\n#----------------------------------------------------------------------------\n\ndef subprocess_fn(rank, args, temp_dir):\n    dnnlib.util.Logger(file_name=os.path.join(args.run_dir, 'log.txt'), file_mode='a', should_flush=True)\n\n    # Init torch.distributed.\n    if args.num_gpus > 1:\n        init_file = os.path.abspath(os.path.join(temp_dir, '.torch_distributed_init'))\n        if os.name == 'nt':\n            init_method = 'file:///' + init_file.replace('\\\\', '/')\n            torch.distributed.init_process_group(backend='gloo', init_method=init_method, rank=rank, world_size=args.num_gpus)\n        else:\n            init_method = f'file://{init_file}'\n            torch.distributed.init_process_group(backend='nccl', init_method=init_method, rank=rank, world_size=args.num_gpus)\n\n    # Init torch_utils.\n    sync_device = torch.device('cuda', rank) if args.num_gpus > 1 else None\n    training_stats.init_multiprocessing(rank=rank, sync_device=sync_device)\n    if rank != 0:\n        custom_ops.verbosity = 'none'\n\n    # Execute training loop.\n    training_loop.training_loop(rank=rank, **args)\n\n#----------------------------------------------------------------------------\n\nclass CommaSeparatedList(click.ParamType):\n    name = 'list'\n\n    def convert(self, value, param, ctx):\n        _ = param, ctx\n        if value is None or value.lower() == 'none' or value == '':\n            return []\n        return value.split(',')\n\n#----------------------------------------------------------------------------\n\n@click.command()\n@click.pass_context\n\n# General options.\n@click.option('--outdir', help='Where to save the results', required=True, metavar='DIR')\n@click.option('--gpus', help='Number of GPUs to use [default: 1]', type=int, metavar='INT')\n@click.option('--snap', help='Snapshot interval [default: 50 ticks]', type=int, metavar='INT')\n@click.option('--metrics', help='Comma-separated list or \"none\" [default: fid50k_full]', type=CommaSeparatedList())\n@click.option('--seed', help='Random seed [default: 0]', type=int, metavar='INT')\n@click.option('-n', '--dry-run', help='Print training options and exit', is_flag=True)\n\n# Dataset.\n@click.option('--data', help='Training data (directory or zip)', metavar='PATH', required=True)\n@click.option('--cond', help='Train conditional model based on dataset labels [default: false]', type=bool, metavar='BOOL')\n@click.option('--subset', help='Train with only N images [default: all]', type=int, metavar='INT')\n@click.option('--mirror', help='Enable dataset x-flips [default: false]', type=bool, metavar='BOOL')\n\n# Base config.\n@click.option('--cfg', help='Base config [default: auto]', type=click.Choice(['auto', 'stylegan2', 'paper256', 'paper512', 'paper1024', 'cifar']))\n@click.option('--gamma', help='Override R1 gamma', type=float)\n@click.option('--kimg', help='Override training duration', type=int, metavar='INT')\n@click.option('--batch', help='Override batch size', type=int, metavar='INT')\n\n# Discriminator augmentation.\n@click.option('--aug', help='Augmentation mode [default: ada]', type=click.Choice(['noaug', 'ada', 'fixed']))\n@click.option('--p', help='Augmentation probability for --aug=fixed', type=float)\n@click.option('--target', help='ADA target value for --aug=ada', type=float)\n@click.option('--augpipe', help='Augmentation pipeline [default: bgc]', type=click.Choice(['blit', 'geom', 'color', 'filter', 'noise', 'cutout', 'bg', 'bgc', 'bgcf', 'bgcfn', 'bgcfnc']))\n\n# Transfer learning.\n@click.option('--resume', help='Resume training [default: noresume]', metavar='PKL')\n@click.option('--freezed', help='Freeze-D [default: 0 layers]', type=int, metavar='INT')\n\n# Performance options.\n@click.option('--fp32', help='Disable mixed-precision training', type=bool, metavar='BOOL')\n@click.option('--nhwc', help='Use NHWC memory format with FP16', type=bool, metavar='BOOL')\n@click.option('--nobench', help='Disable cuDNN benchmarking', type=bool, metavar='BOOL')\n@click.option('--allow-tf32', help='Allow PyTorch to use TF32 internally', type=bool, metavar='BOOL')\n@click.option('--workers', help='Override number of DataLoader workers', type=int, metavar='INT')\n\ndef main(ctx, outdir, dry_run, **config_kwargs):\n    \"\"\"Train a GAN using the techniques described in the paper\n    \"Training Generative Adversarial Networks with Limited Data\".\n\n    Examples:\n\n    \\b\n    # Train with custom dataset using 1 GPU.\n    python train.py --outdir=~/training-runs --data=~/mydataset.zip --gpus=1\n\n    \\b\n    # Train class-conditional CIFAR-10 using 2 GPUs.\n    python train.py --outdir=~/training-runs --data=~/datasets/cifar10.zip \\\\\n        --gpus=2 --cfg=cifar --cond=1\n\n    \\b\n    # Transfer learn MetFaces from FFHQ using 4 GPUs.\n    python train.py --outdir=~/training-runs --data=~/datasets/metfaces.zip \\\\\n        --gpus=4 --cfg=paper1024 --mirror=1 --resume=ffhq1024 --snap=10\n\n    \\b\n    # Reproduce original StyleGAN2 config F.\n    python train.py --outdir=~/training-runs --data=~/datasets/ffhq.zip \\\\\n        --gpus=8 --cfg=stylegan2 --mirror=1 --aug=noaug\n\n    \\b\n    Base configs (--cfg):\n      auto       Automatically select reasonable defaults based on resolution\n                 and GPU count. Good starting point for new datasets.\n      stylegan2  Reproduce results for StyleGAN2 config F at 1024x1024.\n      paper256   Reproduce results for FFHQ and LSUN Cat at 256x256.\n      paper512   Reproduce results for BreCaHAD and AFHQ at 512x512.\n      paper1024  Reproduce results for MetFaces at 1024x1024.\n      cifar      Reproduce results for CIFAR-10 at 32x32.\n\n    \\b\n    Transfer learning source networks (--resume):\n      ffhq256        FFHQ trained at 256x256 resolution.\n      ffhq512        FFHQ trained at 512x512 resolution.\n      ffhq1024       FFHQ trained at 1024x1024 resolution.\n      celebahq256    CelebA-HQ trained at 256x256 resolution.\n      lsundog256     LSUN Dog trained at 256x256 resolution.\n      <PATH or URL>  Custom network pickle.\n    \"\"\"\n    dnnlib.util.Logger(should_flush=True)\n\n    # Setup training options.\n    try:\n        run_desc, args = setup_training_loop_kwargs(**config_kwargs)\n    except UserError as err:\n        ctx.fail(err)\n\n    # Pick output directory.\n    prev_run_dirs = []\n    if os.path.isdir(outdir):\n        prev_run_dirs = [x for x in os.listdir(outdir) if os.path.isdir(os.path.join(outdir, x))]\n    prev_run_ids = [re.match(r'^\\d+', x) for x in prev_run_dirs]\n    prev_run_ids = [int(x.group()) for x in prev_run_ids if x is not None]\n    cur_run_id = max(prev_run_ids, default=-1) + 1\n    args.run_dir = os.path.join(outdir, f'{cur_run_id:05d}-{run_desc}')\n    assert not os.path.exists(args.run_dir)\n\n    # Print options.\n    print()\n    print('Training options:')\n    print(json.dumps(args, indent=2))\n    print()\n    print(f'Output directory:   {args.run_dir}')\n    print(f'Training data:      {args.training_set_kwargs.path}')\n    print(f'Training duration:  {args.total_kimg} kimg')\n    print(f'Number of GPUs:     {args.num_gpus}')\n    print(f'Number of images:   {args.training_set_kwargs.max_size}')\n    print(f'Image resolution:   {args.training_set_kwargs.resolution}')\n    print(f'Conditional model:  {args.training_set_kwargs.use_labels}')\n    print(f'Dataset x-flips:    {args.training_set_kwargs.xflip}')\n    print()\n\n    # Dry run?\n    if dry_run:\n        print('Dry run; exiting.')\n        return\n\n    # Create output directory.\n    print('Creating output directory...')\n    os.makedirs(args.run_dir)\n    with open(os.path.join(args.run_dir, 'training_options.json'), 'wt') as f:\n        json.dump(args, f, indent=2)\n\n    # Launch processes.\n    print('Launching processes...')\n    torch.multiprocessing.set_start_method('spawn')\n    with tempfile.TemporaryDirectory() as temp_dir:\n        if args.num_gpus == 1:\n            subprocess_fn(rank=0, args=args, temp_dir=temp_dir)\n        else:\n            torch.multiprocessing.spawn(fn=subprocess_fn, args=(args, temp_dir), nprocs=args.num_gpus)\n\n#----------------------------------------------------------------------------\n\nif __name__ == \"__main__\":\n    main() # pylint: disable=no-value-for-parameter\n\n#----------------------------------------------------------------------------\n"
        },
        {
          "name": "training",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}