{
  "metadata": {
    "timestamp": 1736559831355,
    "page": 572,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjU4MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "eth-sri/lmql",
      "stars": 3766,
      "defaultBranch": "main",
      "files": [
        {
          "name": ".dockerignore",
          "type": "blob",
          "size": 2.8115234375,
          "content": "api.env\n**/node_modules/*\n\n*.tokens\n\ndist\n*__pycache__*\n**/*_compiled.py\nsrc/lmql.egg-info\nsrc/lmql.egg-info/*\ndebugger_state.pkl\napi.env\n**/api.env\n.env\nweb-deploy\nweb/index.html\n**/node_modules/*\n**/.DS_Store\n\n\n# Created by https://www.toptal.com/developers/gitignore/api/python,jupyternotebooks\n# Edit at https://www.toptal.com/developers/gitignore?templates=python,jupyternotebooks\n\n### JupyterNotebooks ###\n# gitignore template for Jupyter Notebooks\n# website: http://jupyter.org/\n\n.ipynb_checkpoints\n*/.ipynb_checkpoints/*\n\n# IPython\nprofile_default/\nipython_config.py\n\n# Remove previous ipynb_checkpoints\n#   git rm -r .ipynb_checkpoints/\n\n### Python ###\n# Byte-compiled / optimized / DLL files\n__pycache__/\n*.py[cod]\n*$py.class\n\n# C extensions\n*.so\n\n# Distribution / packaging\n.Python\nbuild/\ndevelop-eggs/\ndist/\ndownloads/\neggs/\n.eggs/\nlib/\nlib64/\nparts/\nsdist/\nvar/\nwheels/\nshare/python-wheels/\n*.egg-info/\n.installed.cfg\n*.egg\nMANIFEST\n\n# PyInstaller\n#  Usually these files are written by a python script from a template\n#  before PyInstaller builds the exe, so as to inject date/other infos into it.\n*.manifest\n*.spec\n\n# Installer logs\npip-log.txt\npip-delete-this-directory.txt\n\n# Unit test / coverage reports\nhtmlcov/\n.tox/\n.nox/\n.coverage\n.coverage.*\n.cache\nnosetests.xml\ncoverage.xml\n*.cover\n*.py,cover\n.hypothesis/\n.pytest_cache/\ncover/\n\n# Translations\n*.mo\n*.pot\n\n# Django stuff:\n*.log\nlocal_settings.py\ndb.sqlite3\ndb.sqlite3-journal\n\n# Flask stuff:\ninstance/\n.webassets-cache\n\n# Scrapy stuff:\n.scrapy\n\n# Sphinx documentation\ndocs/_build/\n\n# PyBuilder\n.pybuilder/\ntarget/\n\n# Jupyter Notebook\n\n# IPython\n\n# pyenv\n#   For a library or package, you might want to ignore these files since the code is\n#   intended to run in multiple environments; otherwise, check them in:\n# .python-version\n\n# pipenv\n#   According to pypa/pipenv#598, it is recommended to include Pipfile.lock in version control.\n#   However, in case of collaboration, if having platform-specific dependencies or dependencies\n#   having no cross-platform support, pipenv may install dependencies that don't work, or not\n#   install all needed dependencies.\n#Pipfile.lock\n\n# PEP 582; used by e.g. github.com/David-OConnor/pyflow\n__pypackages__/\n\n# Celery stuff\ncelerybeat-schedule\ncelerybeat.pid\n\n# SageMath parsed files\n*.sage.py\n\n# Environments\n.env\n.venv\nenv/\nvenv/\nENV/\nenv.bak/\nvenv.bak/\n\n# Spyder project settings\n.spyderproject\n.spyproject\n\n# Rope project settings\n.ropeproject\n\n# mkdocs documentation\n/site\n\n# mypy\n.mypy_cache/\n.dmypy.json\ndmypy.json\n\n# Pyre type checker\n.pyre/\n\n# pytype static type analyzer\n.pytype/\n\n# Cython debug symbols\ncython_debug/\n\n# End of https://www.toptal.com/developers/gitignore/api/python,jupyternotebooks\n\nwip-snippets\n.vscode\n.lmql-algorithms-cache\n\n*.tokens\n\nscripts/Dockerfile*\nscripts/Dockerfile.serve\nscripts/lmql-serve-docker.py\ntransformers-cache\nweb/\n"
        },
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 2.7177734375,
          "content": "dist\n*__pycache__*\n**/*_compiled.py\nsrc/lmql.egg-info\nsrc/lmql.egg-info/*\ndebugger_state.pkl\napi.env\n**/api.env\n.env\nweb-deploy\nweb/index.html\n**/node_modules/*\n**/.DS_Store\n\n# Nix build output\nresult\n\n# Created by https://www.toptal.com/developers/gitignore/api/python,jupyternotebooks\n# Edit at https://www.toptal.com/developers/gitignore?templates=python,jupyternotebooks\n\n### JupyterNotebooks ###\n# gitignore template for Jupyter Notebooks\n# website: http://jupyter.org/\n\n.ipynb_checkpoints\n*/.ipynb_checkpoints/*\n\n# IPython\nprofile_default/\nipython_config.py\n\n# Remove previous ipynb_checkpoints\n#   git rm -r .ipynb_checkpoints/\n\n### Python ###\n# Byte-compiled / optimized / DLL files\n__pycache__/\n*.py[cod]\n*$py.class\n\n# C extensions\n*.so\n\n# Distribution / packaging\n.Python\nbuild/\ndevelop-eggs/\ndist/\ndownloads/\neggs/\n.eggs/\nlib64/\nparts/\nsdist/\nvar/\nwheels/\nshare/python-wheels/\n*.egg-info/\n.installed.cfg\n*.egg\nMANIFEST\n\n# PyInstaller\n#  Usually these files are written by a python script from a template\n#  before PyInstaller builds the exe, so as to inject date/other infos into it.\n*.manifest\n*.spec\n\n# Installer logs\npip-log.txt\npip-delete-this-directory.txt\n\n# Unit test / coverage reports\nhtmlcov/\n.tox/\n.nox/\n.coverage\n.coverage.*\n.cache\nnosetests.xml\ncoverage.xml\n*.cover\n*.py,cover\n.hypothesis/\n.pytest_cache/\ncover/\n\n# Translations\n*.mo\n*.pot\n\n# Django stuff:\n*.log\nlocal_settings.py\ndb.sqlite3\ndb.sqlite3-journal\n\n# Flask stuff:\ninstance/\n.webassets-cache\n\n# Scrapy stuff:\n.scrapy\n\n# Sphinx documentation\ndocs/_build/\n\n# PyBuilder\n.pybuilder/\ntarget/\n\n# Jupyter Notebook\n\n# IPython\n\n# pyenv\n#   For a library or package, you might want to ignore these files since the code is\n#   intended to run in multiple environments; otherwise, check them in:\n# .python-version\n\n# pipenv\n#   According to pypa/pipenv#598, it is recommended to include Pipfile.lock in version control.\n#   However, in case of collaboration, if having platform-specific dependencies or dependencies\n#   having no cross-platform support, pipenv may install dependencies that don't work, or not\n#   install all needed dependencies.\n#Pipfile.lock\n\n# PEP 582; used by e.g. github.com/David-OConnor/pyflow\n__pypackages__/\n\n# Celery stuff\ncelerybeat-schedule\ncelerybeat.pid\n\n# SageMath parsed files\n*.sage.py\n\n# Environments\n.env\n.venv\nenv/\nvenv/\nENV/\nenv.bak/\nvenv.bak/\n\n# Spyder project settings\n.spyderproject\n.spyproject\n\n# Rope project settings\n.ropeproject\n\n# mkdocs documentation\n/site\n\n# mypy\n.mypy_cache/\n.dmypy.json\ndmypy.json\n\n# Pyre type checker\n.pyre/\n\n# pytype static type analyzer\n.pytype/\n\n# Cython debug symbols\ncython_debug/\n\n# End of https://www.toptal.com/developers/gitignore/api/python,jupyternotebooks\n\nwip-snippets\n.vscode\n.lmql-algorithms-cache\n\n*.tokens\ntransformers-cache\n"
        },
        {
          "name": ".readthedocs.yaml",
          "type": "blob",
          "size": 0.564453125,
          "content": "# .readthedocs.yaml\n# Read the Docs configuration file\n# See https://docs.readthedocs.io/en/stable/config-file/v2.html for details\n\n# Required\nversion: 2\n\n# Set the version of Python and other tools you might need\nbuild:\n  os: ubuntu-22.04\n  tools:\n    python: \"3.11\"\n\n# Build documentation in the docs/ directory with Sphinx\nsphinx:\n  configuration: docs/source/conf.py\n\n# We recommend specifying your dependencies to enable reproducible builds:\n# https://docs.readthedocs.io/en/stable/guides/reproducible-builds.html\npython:\n  install:\n  - requirements: docs/requirements.txt\n"
        },
        {
          "name": "CONTRIBUTING.md",
          "type": "blob",
          "size": 7.65625,
          "content": "# Contributing\n\nThank you so much for your interest in contributing to LMQL, we are very happy to have you here! We \nactively encourage any form of contribution, and are convinced that LMQL should be a community-driven\nproject.\n\n## Pull Request Process\n\n1. **Reach Out** If you are planning to implement a new feature (not just a bugfix), please open an issue first or come and talk to the team in our [community Discord](https://discord.gg/7eJP4fcyNT) (#dev channel). In general we are very open to new and \n   experimental ideas, but we want to make sure that the feature is in line with the overall goals of the project and that it is not already being worked on.\n2. **Test Well** Please make sure that your code is well-tested. For this also see the 'Testing' section below.\n3. **Import Defensively** Please make sure your code uses defensive imports, i.e. imports that do not fail if a dependency is not installed. For example, if you are using the `transformers` library, you should use `try/except` blocks to catch the `ModuleNotFoundError` that is raised if the library is not installed. This is  important because we want to make sure that LMQL can be installed without all dependencies, and that the user is only required to install the dependencies that are actually needed for their use case.\n4. **Document Your Changes** If your contributions or feature requires new forms of configuration or syntax, please make sure to also provide a documentation chapter for it. You can find the MarkDown-based documentation in the `docs/` directory. A script for building and previewing the documentation is provided in `scripts/serve-all.py` (also serves the website and browser playground).\n\n## Testing\n\n**General Testing** For general testing, please make sure that your code is compatible with multiple backends, e.g. if possible try to test with `transformers` models, OpenAI models and `llama.cpp`. LMQL is designed as a vendor-agnostic language, which means all features should be available across all backends. If you do not have the hardware to test changes with multiple backends, please make sure to ask a team member to run the tests for you, before merging your pull request.\n\n**Dependency Changes/Updates** After adding new dependencies, `scripts/flake.d/poetry.lock` needs to be updated. This can be done by running `(cd scripts/flake.d && exec poetry lock --no-update)` (if you run Nix, `nix develop .#minimal` will put you in a shell with the `poetry` command available, even if the `poetry.lock`, `pyproject.cfg`, and other related files are currently broken). If you are able, please also check that the Nix build works after making dependency changes; if you're not in a position to do this, please feel encouraged to request a hand on Discord.\n\n**Running Test Suites** The repository contains a number of test suites in the `src/lmql/tests/` directory. To run all \ntests simply run `python src/lmql/tests/all.py`. Note that for some tests you need to configure an\nOpenAI API key according to the instructions in [documentation](https://lmql.ai/docs/models/openai.html).\nWe are working to remove the external dependency on the OpenAI API, but for now it is still required\nfor some tests. If you cannot get an API key, you can ask one of the core maintainers to run the\ntests for your, once your pull request is ready.\n\n**Adding Tests** You are also invited to add new tests in the form of a new `test_*.py` file in the `src/lmql/tests/` \ndirectory. For an example of how to write tests, please see the e.g. `https://github.com/eth-sri/lmql/blob/main/src/lmql/tests/test_functions.py`.\nAs demonstrated by this file, also try to implement your tests using `lmql.model(\"random\", seed=<SEED>)` to make sure\nyour test code can be run without actually using an LLM or external API, and that it can be re-run\ndeterministically.\n\n**[Optional] Web Build Testing:** If you are working on a feature that is available in the web playground of LMQL (e.g. also works with API-based (OpenAI) models only), you can also test the web build with `scripts/serve-all.py`, by typing `bb` for browser build into the script's prompt. This will build and serve the WebAssembly/Pyodide version of LMQL on `http://localhost:8081/playground/`. Please see `scripts/deploy-web.sh` for the build process and requirements of the web playground, which may require you to install additional dependencies. \n\n## Licensing\n\nBy contributing to LMQL you agree to license your contributions under the terms of\nthe [Apache License 2.0](https://www.apache.org/licenses/LICENSE-2.0) as included in the [LICENSE](./LICENSE) file.\n\n## Code of Conduct\n\n### Our Pledge\n\nIn the interest of fostering an open and welcoming environment, we as\ncontributors and maintainers pledge to making participation in our project and\nour community a harassment-free experience for everyone, regardless of age, body\nsize, disability, ethnicity, gender identity and expression, level of experience,\nnationality, personal appearance, race, religion, or sexual identity and\norientation.\n\n### Our Standards\n\nExamples of behavior that contributes to creating a positive environment\ninclude:\n\n* Using welcoming and inclusive language\n* Being respectful of differing viewpoints and experiences\n* Gracefully accepting constructive criticism\n* Focusing on what is best for the community\n* Showing empathy towards other community members\n\nExamples of unacceptable behavior by participants include:\n\n* The use of sexualized language or imagery and unwelcome sexual attention or\nadvances\n* Trolling, insulting/derogatory comments, and personal or political attacks\n* Public or private harassment\n* Publishing others' private information, such as a physical or electronic\n  address, without explicit permission\n* Other conduct which could reasonably be considered inappropriate in a\n  professional setting\n\n### Our Responsibilities\n\nProject maintainers are responsible for clarifying the standards of acceptable\nbehavior and are expected to take appropriate and fair corrective action in\nresponse to any instances of unacceptable behavior.\n\nProject maintainers have the right and responsibility to remove, edit, or\nreject comments, commits, code, wiki edits, issues, and other contributions\nthat are not aligned to this Code of Conduct, or to ban temporarily or\npermanently any contributor for other behaviors that they deem inappropriate,\nthreatening, offensive, or harmful.\n\n### Scope\n\nThis Code of Conduct applies both within project spaces and in public spaces\nwhen an individual is representing the project or its community. Examples of\nrepresenting a project or community include using an official project e-mail\naddress, posting via an official social media account, or acting as an appointed\nrepresentative at an online or offline event. Representation of a project may be\nfurther defined and clarified by project maintainers.\n\n### Enforcement\n\nInstances of abusive, harassing, or otherwise unacceptable behavior may be\nreported by contacting the project team at [hello@lmql.ai](mailto:hello@lmql.ai). All\ncomplaints will be reviewed and investigated and will result in a response that\nis deemed necessary and appropriate to the circumstances. The project team is\nobligated to maintain confidentiality with regard to the reporter of an incident.\nFurther details of specific enforcement policies may be posted separately.\n\nProject maintainers who do not follow or enforce the Code of Conduct in good\nfaith may face temporary or permanent repercussions as determined by other\nmembers of the project's leadership.\n\n### Attribution\n\nThis Code of Conduct is adapted from the [Contributor Covenant][homepage], version 1.4,\navailable at [http://contributor-covenant.org/version/1/4][version]\n\n[homepage]: http://contributor-covenant.org\n[version]: http://contributor-covenant.org/version/1/4/\n"
        },
        {
          "name": "Dockerfile.tests",
          "type": "blob",
          "size": 0.9736328125,
          "content": "FROM python:3.11-bullseye\n\n# install lmql with llama.cpp dependencies\nWORKDIR /lmql\n\n# download test model weights\nRUN wget https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGUF/resolve/main/llama-2-7b-chat.Q2_K.gguf?download=true -O /lmql/llama-2-7b-chat.Q2_K.gguf\n\n# install the torch cpu version\nRUN pip install torch --index-url https://download.pytorch.org/whl/cpu\n\nCOPY setup.cfg /lmql/setup.cfg\nCOPY setup.py /lmql/setup.py\nRUN mkdir /lmql/src\n\nRUN pip install -e \".[llama,hf,hf-accel,tests]\"\nRUN pip install -e \".[hf]\"\nRUN pip install -e \".[hf,hf-accel,tests]\"\nRUN pip install langchain\n\nCOPY src /lmql/src\n\n# python install sshleifer/tiny-gpt2 via transformers\nRUN python -c \"from transformers import AutoTokenizer, AutoModelForCausalLM; AutoTokenizer.from_pretrained('sshleifer/tiny-gpt2'); AutoModelForCausalLM.from_pretrained('sshleifer/tiny-gpt2'); AutoTokenizer.from_pretrained('gpt2'); AutoModelForCausalLM.from_pretrained('gpt2')\"\n\nCMD [\"python\", \"src/lmql/tests/all.py\"]\nCMD \"bash\""
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 22.6083984375,
          "content": "Copyright 2022- The LMQL Language Team. All rights reserved.\n\n                                 Apache License\n                           Version 2.0, January 2004\n                        http://www.apache.org/licenses/\n\n   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n\n   1. Definitions.\n\n      \"License\" shall mean the terms and conditions for use, reproduction,\n      and distribution as defined by Sections 1 through 9 of this document.\n\n      \"Licensor\" shall mean the copyright owner or entity authorized by\n      the copyright owner that is granting the License.\n\n      \"Legal Entity\" shall mean the union of the acting entity and all\n      other entities that control, are controlled by, or are under common\n      control with that entity. For the purposes of this definition,\n      \"control\" means (i) the power, direct or indirect, to cause the\n      direction or management of such entity, whether by contract or\n      otherwise, or (ii) ownership of fifty percent (50%) or more of the\n      outstanding shares, or (iii) beneficial ownership of such entity.\n\n      \"You\" (or \"Your\") shall mean an individual or Legal Entity\n      exercising permissions granted by this License.\n\n      \"Source\" form shall mean the preferred form for making modifications,\n      including but not limited to software source code, documentation\n      source, and configuration files.\n\n      \"Object\" form shall mean any form resulting from mechanical\n      transformation or translation of a Source form, including but\n      not limited to compiled object code, generated documentation,\n      and conversions to other media types.\n\n      \"Work\" shall mean the work of authorship, whether in Source or\n      Object form, made available under the License, as indicated by a\n      copyright notice that is included in or attached to the work\n      (an example is provided in the Appendix below).\n\n      \"Derivative Works\" shall mean any work, whether in Source or Object\n      form, that is based on (or derived from) the Work and for which the\n      editorial revisions, annotations, elaborations, or other modifications\n      represent, as a whole, an original work of authorship. For the purposes\n      of this License, Derivative Works shall not include works that remain\n      separable from, or merely link (or bind by name) to the interfaces of,\n      the Work and Derivative Works thereof.\n\n      \"Contribution\" shall mean any work of authorship, including\n      the original version of the Work and any modifications or additions\n      to that Work or Derivative Works thereof, that is intentionally\n      submitted to Licensor for inclusion in the Work by the copyright owner\n      or by an individual or Legal Entity authorized to submit on behalf of\n      the copyright owner. For the purposes of this definition, \"submitted\"\n      means any form of electronic, verbal, or written communication sent\n      to the Licensor or its representatives, including but not limited to\n      communication on electronic mailing lists, source code control systems,\n      and issue tracking systems that are managed by, or on behalf of, the\n      Licensor for the purpose of discussing and improving the Work, but\n      excluding communication that is conspicuously marked or otherwise\n      designated in writing by the copyright owner as \"Not a Contribution.\"\n\n      \"Contributor\" shall mean Licensor and any individual or Legal Entity\n      on behalf of whom a Contribution has been received by Licensor and\n      subsequently incorporated within the Work.\n\n   2. Grant of Copyright License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      copyright license to reproduce, prepare Derivative Works of,\n      publicly display, publicly perform, sublicense, and distribute the\n      Work and such Derivative Works in Source or Object form.\n\n   3. Grant of Patent License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      (except as stated in this section) patent license to make, have made,\n      use, offer to sell, sell, import, and otherwise transfer the Work,\n      where such license applies only to those patent claims licensable\n      by such Contributor that are necessarily infringed by their\n      Contribution(s) alone or by combination of their Contribution(s)\n      with the Work to which such Contribution(s) was submitted. If You\n      institute patent litigation against any entity (including a\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\n      or a Contribution incorporated within the Work constitutes direct\n      or contributory patent infringement, then any patent licenses\n      granted to You under this License for that Work shall terminate\n      as of the date such litigation is filed.\n\n   4. Redistribution. You may reproduce and distribute copies of the\n      Work or Derivative Works thereof in any medium, with or without\n      modifications, and in Source or Object form, provided that You\n      meet the following conditions:\n\n      (a) You must give any other recipients of the Work or\n          Derivative Works a copy of this License; and\n\n      (b) You must cause any modified files to carry prominent notices\n          stating that You changed the files; and\n\n      (c) You must retain, in the Source form of any Derivative Works\n          that You distribute, all copyright, patent, trademark, and\n          attribution notices from the Source form of the Work,\n          excluding those notices that do not pertain to any part of\n          the Derivative Works; and\n\n      (d) If the Work includes a \"NOTICE\" text file as part of its\n          distribution, then any Derivative Works that You distribute must\n          include a readable copy of the attribution notices contained\n          within such NOTICE file, excluding those notices that do not\n          pertain to any part of the Derivative Works, in at least one\n          of the following places: within a NOTICE text file distributed\n          as part of the Derivative Works; within the Source form or\n          documentation, if provided along with the Derivative Works; or,\n          within a display generated by the Derivative Works, if and\n          wherever such third-party notices normally appear. The contents\n          of the NOTICE file are for informational purposes only and\n          do not modify the License. You may add Your own attribution\n          notices within Derivative Works that You distribute, alongside\n          or as an addendum to the NOTICE text from the Work, provided\n          that such additional attribution notices cannot be construed\n          as modifying the License.\n\n      You may add Your own copyright statement to Your modifications and\n      may provide additional or different license terms and conditions\n      for use, reproduction, or distribution of Your modifications, or\n      for any such Derivative Works as a whole, provided Your use,\n      reproduction, and distribution of the Work otherwise complies with\n      the conditions stated in this License.\n\n   5. Submission of Contributions. Unless You explicitly state otherwise,\n      any Contribution intentionally submitted for inclusion in the Work\n      by You to the Licensor shall be under the terms and conditions of\n      this License, without any additional terms or conditions.\n      Notwithstanding the above, nothing herein shall supersede or modify\n      the terms of any separate license agreement you may have executed\n      with Licensor regarding such Contributions.\n\n   6. Trademarks. This License does not grant permission to use the trade\n      names, trademarks, service marks, or product names of the Licensor,\n      except as required for reasonable and customary use in describing the\n      origin of the Work and reproducing the content of the NOTICE file.\n\n   7. Disclaimer of Warranty. Unless required by applicable law or\n      agreed to in writing, Licensor provides the Work (and each\n      Contributor provides its Contributions) on an \"AS IS\" BASIS,\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n      implied, including, without limitation, any warranties or conditions\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n      PARTICULAR PURPOSE. You are solely responsible for determining the\n      appropriateness of using or redistributing the Work and assume any\n      risks associated with Your exercise of permissions under this License.\n\n   8. Limitation of Liability. In no event and under no legal theory,\n      whether in tort (including negligence), contract, or otherwise,\n      unless required by applicable law (such as deliberate and grossly\n      negligent acts) or agreed to in writing, shall any Contributor be\n      liable to You for damages, including any direct, indirect, special,\n      incidental, or consequential damages of any character arising as a\n      result of this License or out of the use or inability to use the\n      Work (including but not limited to damages for loss of goodwill,\n      work stoppage, computer failure or malfunction, or any and all\n      other commercial damages or losses), even if such Contributor\n      has been advised of the possibility of such damages.\n\n   9. Accepting Warranty or Additional Liability. While redistributing\n      the Work or Derivative Works thereof, You may choose to offer,\n      and charge a fee for, acceptance of support, warranty, indemnity,\n      or other liability obligations and/or rights consistent with this\n      License. However, in accepting such obligations, You may act only\n      on Your own behalf and on Your sole responsibility, not on behalf\n      of any other Contributor, and only if You agree to indemnify,\n      defend, and hold each Contributor harmless for any liability\n      incurred by, or claims asserted against, such Contributor by reason\n      of your accepting any such warranty or additional liability.\n\n   END OF TERMS AND CONDITIONS\n\n   APPENDIX: How to apply the Apache License to your work.\n\n      To apply the Apache License to your work, attach the following\n      boilerplate notice, with the fields enclosed by brackets \"[]\"\n      replaced with your own identifying information. (Don't include\n      the brackets!)  The text should be enclosed in the appropriate\n      comment syntax for the file format. We also recommend that a\n      file or class name and description of purpose be included on the\n      same \"printed page\" as the copyright notice for easier\n      identification within third-party archives.\n\n   Copyright [yyyy] [name of copyright owner]\n\n   Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.\n================================================================================\n\nThe project also includes one file adapted from the huggingface/tranformers project:\n\n- async_generation_utils.py\n\nThese files are adapted from their original project and redistributed according to the following license agreement:\n\nCopyright 2018- The Hugging Face team. All rights reserved.\n\n                                 Apache License\n                           Version 2.0, January 2004\n                        http://www.apache.org/licenses/\n\n   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n\n   1. Definitions.\n\n      \"License\" shall mean the terms and conditions for use, reproduction,\n      and distribution as defined by Sections 1 through 9 of this document.\n\n      \"Licensor\" shall mean the copyright owner or entity authorized by\n      the copyright owner that is granting the License.\n\n      \"Legal Entity\" shall mean the union of the acting entity and all\n      other entities that control, are controlled by, or are under common\n      control with that entity. For the purposes of this definition,\n      \"control\" means (i) the power, direct or indirect, to cause the\n      direction or management of such entity, whether by contract or\n      otherwise, or (ii) ownership of fifty percent (50%) or more of the\n      outstanding shares, or (iii) beneficial ownership of such entity.\n\n      \"You\" (or \"Your\") shall mean an individual or Legal Entity\n      exercising permissions granted by this License.\n\n      \"Source\" form shall mean the preferred form for making modifications,\n      including but not limited to software source code, documentation\n      source, and configuration files.\n\n      \"Object\" form shall mean any form resulting from mechanical\n      transformation or translation of a Source form, including but\n      not limited to compiled object code, generated documentation,\n      and conversions to other media types.\n\n      \"Work\" shall mean the work of authorship, whether in Source or\n      Object form, made available under the License, as indicated by a\n      copyright notice that is included in or attached to the work\n      (an example is provided in the Appendix below).\n\n      \"Derivative Works\" shall mean any work, whether in Source or Object\n      form, that is based on (or derived from) the Work and for which the\n      editorial revisions, annotations, elaborations, or other modifications\n      represent, as a whole, an original work of authorship. For the purposes\n      of this License, Derivative Works shall not include works that remain\n      separable from, or merely link (or bind by name) to the interfaces of,\n      the Work and Derivative Works thereof.\n\n      \"Contribution\" shall mean any work of authorship, including\n      the original version of the Work and any modifications or additions\n      to that Work or Derivative Works thereof, that is intentionally\n      submitted to Licensor for inclusion in the Work by the copyright owner\n      or by an individual or Legal Entity authorized to submit on behalf of\n      the copyright owner. For the purposes of this definition, \"submitted\"\n      means any form of electronic, verbal, or written communication sent\n      to the Licensor or its representatives, including but not limited to\n      communication on electronic mailing lists, source code control systems,\n      and issue tracking systems that are managed by, or on behalf of, the\n      Licensor for the purpose of discussing and improving the Work, but\n      excluding communication that is conspicuously marked or otherwise\n      designated in writing by the copyright owner as \"Not a Contribution.\"\n\n      \"Contributor\" shall mean Licensor and any individual or Legal Entity\n      on behalf of whom a Contribution has been received by Licensor and\n      subsequently incorporated within the Work.\n\n   2. Grant of Copyright License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      copyright license to reproduce, prepare Derivative Works of,\n      publicly display, publicly perform, sublicense, and distribute the\n      Work and such Derivative Works in Source or Object form.\n\n   3. Grant of Patent License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      (except as stated in this section) patent license to make, have made,\n      use, offer to sell, sell, import, and otherwise transfer the Work,\n      where such license applies only to those patent claims licensable\n      by such Contributor that are necessarily infringed by their\n      Contribution(s) alone or by combination of their Contribution(s)\n      with the Work to which such Contribution(s) was submitted. If You\n      institute patent litigation against any entity (including a\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\n      or a Contribution incorporated within the Work constitutes direct\n      or contributory patent infringement, then any patent licenses\n      granted to You under this License for that Work shall terminate\n      as of the date such litigation is filed.\n\n   4. Redistribution. You may reproduce and distribute copies of the\n      Work or Derivative Works thereof in any medium, with or without\n      modifications, and in Source or Object form, provided that You\n      meet the following conditions:\n\n      (a) You must give any other recipients of the Work or\n          Derivative Works a copy of this License; and\n\n      (b) You must cause any modified files to carry prominent notices\n          stating that You changed the files; and\n\n      (c) You must retain, in the Source form of any Derivative Works\n          that You distribute, all copyright, patent, trademark, and\n          attribution notices from the Source form of the Work,\n          excluding those notices that do not pertain to any part of\n          the Derivative Works; and\n\n      (d) If the Work includes a \"NOTICE\" text file as part of its\n          distribution, then any Derivative Works that You distribute must\n          include a readable copy of the attribution notices contained\n          within such NOTICE file, excluding those notices that do not\n          pertain to any part of the Derivative Works, in at least one\n          of the following places: within a NOTICE text file distributed\n          as part of the Derivative Works; within the Source form or\n          documentation, if provided along with the Derivative Works; or,\n          within a display generated by the Derivative Works, if and\n          wherever such third-party notices normally appear. The contents\n          of the NOTICE file are for informational purposes only and\n          do not modify the License. You may add Your own attribution\n          notices within Derivative Works that You distribute, alongside\n          or as an addendum to the NOTICE text from the Work, provided\n          that such additional attribution notices cannot be construed\n          as modifying the License.\n\n      You may add Your own copyright statement to Your modifications and\n      may provide additional or different license terms and conditions\n      for use, reproduction, or distribution of Your modifications, or\n      for any such Derivative Works as a whole, provided Your use,\n      reproduction, and distribution of the Work otherwise complies with\n      the conditions stated in this License.\n\n   5. Submission of Contributions. Unless You explicitly state otherwise,\n      any Contribution intentionally submitted for inclusion in the Work\n      by You to the Licensor shall be under the terms and conditions of\n      this License, without any additional terms or conditions.\n      Notwithstanding the above, nothing herein shall supersede or modify\n      the terms of any separate license agreement you may have executed\n      with Licensor regarding such Contributions.\n\n   6. Trademarks. This License does not grant permission to use the trade\n      names, trademarks, service marks, or product names of the Licensor,\n      except as required for reasonable and customary use in describing the\n      origin of the Work and reproducing the content of the NOTICE file.\n\n   7. Disclaimer of Warranty. Unless required by applicable law or\n      agreed to in writing, Licensor provides the Work (and each\n      Contributor provides its Contributions) on an \"AS IS\" BASIS,\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n      implied, including, without limitation, any warranties or conditions\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n      PARTICULAR PURPOSE. You are solely responsible for determining the\n      appropriateness of using or redistributing the Work and assume any\n      risks associated with Your exercise of permissions under this License.\n\n   8. Limitation of Liability. In no event and under no legal theory,\n      whether in tort (including negligence), contract, or otherwise,\n      unless required by applicable law (such as deliberate and grossly\n      negligent acts) or agreed to in writing, shall any Contributor be\n      liable to You for damages, including any direct, indirect, special,\n      incidental, or consequential damages of any character arising as a\n      result of this License or out of the use or inability to use the\n      Work (including but not limited to damages for loss of goodwill,\n      work stoppage, computer failure or malfunction, or any and all\n      other commercial damages or losses), even if such Contributor\n      has been advised of the possibility of such damages.\n\n   9. Accepting Warranty or Additional Liability. While redistributing\n      the Work or Derivative Works thereof, You may choose to offer,\n      and charge a fee for, acceptance of support, warranty, indemnity,\n      or other liability obligations and/or rights consistent with this\n      License. However, in accepting such obligations, You may act only\n      on Your own behalf and on Your sole responsibility, not on behalf\n      of any other Contributor, and only if You agree to indemnify,\n      defend, and hold each Contributor harmless for any liability\n      incurred by, or claims asserted against, such Contributor by reason\n      of your accepting any such warranty or additional liability.\n\n   END OF TERMS AND CONDITIONS\n\n   APPENDIX: How to apply the Apache License to your work.\n\n      To apply the Apache License to your work, attach the following\n      boilerplate notice, with the fields enclosed by brackets \"[]\"\n      replaced with your own identifying information. (Don't include\n      the brackets!)  The text should be enclosed in the appropriate\n      comment syntax for the file format. We also recommend that a\n      file or class name and description of purpose be included on the\n      same \"printed page\" as the copyright notice for easier\n      identification within third-party archives.\n\n   Copyright [yyyy] [name of copyright owner]\n\n   Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.\n"
        },
        {
          "name": "MANIFEST.in",
          "type": "blob",
          "size": 0.375,
          "content": "include src/lmql/ui *\nrecursive-include src/lmql/ui/playground *\nrecursive-exclude src/lmql/ui/playground/build *\nrecursive-exclude src/lmql/ui/playground/node_modules *\nrecursive-include src/lmql/lib/chat/assets *\nrecursive-include src/lmql/ui/live *\nrecursive-exclude src/lmql/ui/live/node_modules *\nrecursive-exclude src/lmql/ui/live *.tokens\nrecursive-exclude src/lmql/ui/vscode *"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 9.8349609375,
          "content": "<div align=\"center\">\n  <a href=\"https://lmql.ai\">\n    <img src=\"https://raw.githubusercontent.com/eth-sri/lmql/web/lmql.svg\" alt=\"Logo\" width=\"80\" height=\"80\">\n  </a>\n\n  <h3 align=\"center\">LMQL</h3>\n\n  <p align=\"center\">\n    A programming language for large language models.\n    <br />\n    <a href=\"https://lmql.ai/docs\"><strong>Documentation Â»</strong></a>\n    <br />\n    <br />\n    <a href=\"https://lmql.ai\">Explore Examples</a>\n    Â·\n    <a href=\"https://lmql.ai/playground\">Playground IDE</a>\n    Â·\n    <a href=\"https://github.com/eth-sri/lmql/issues\">Report Bug</a>\n    <br/>\n    <br/>\n    <a href=\"https://discord.gg/7eJP4fcyNT\"><img src=\"https://img.shields.io/discord/1091288833997410414?style=plastic&logo=discord&color=blueviolet&logoColor=white\" height=18/></a>\n    <a href=\"https://badge.fury.io/py/Lmql\"><img src=\"https://badge.fury.io/py/Lmql.svg?cacheSeconds=3600\" alt=\"PyPI version\" height=18></a>\n  </p>\n</div>\n\n\nLMQL is a programming language for large language models (LLMs) based on a *superset of Python*. LMQL offers a novel way of interweaving traditional programming with the ability to call LLMs in your code. It goes beyond traditional templating languages by integrating LLM interaction natively at the level of your program code. \n\n> <img width=\"200pt\" alt=\"image\" src=\"https://github.com/eth-sri/lmql/assets/17903049/bfa7ecf5-847b-47d0-9160-f8596aa47749\"> <br/>Help us shape the next major version of LMQL by filling out the LMQL developer survey: https://forms.gle/pGvAicNpUhS1rAkK9\n\n## Explore LMQL\n\nAn LMQL program reads like standard Python, but top-level strings are interpreted as query strings: They are passed to an LLM, where template variables like `[GREETINGS]` are automatically completed by the model:\n\n```python\n\"Greet LMQL:[GREETINGS]\\n\" where stops_at(GREETINGS, \".\") and not \"\\n\" in GREETINGS\n\nif \"Hi there\" in GREETINGS:\n    \"Can you reformulate your greeting in the speech of \\\n     victorian-era English: [VIC_GREETINGS]\\n\" where stops_at(VIC_GREETINGS, \".\")\n\n\"Analyse what part of this response makes it typically victorian:\\n\"\n\nfor i in range(4):\n    \"-[THOUGHT]\\n\" where stops_at(THOUGHT, \".\")\n\n\"To summarize:[SUMMARY]\"\n```\n\nProgram Output:\n<div align=\"center\">\n  <img src=\"https://github.com/eth-sri/lmql/assets/17903049/243176f1-dfd4-4129-a59e-ca3dee068295\"/>\n  <br/>\n</div>\n\nLMQL allows you to express programs that contain both, traditional algorithmic logic, and LLM calls. \nAt any point during execution, you can prompt an LLM on program variables in combination with standard natural language prompting, to leverage model reasoning capabilities in the context of your program.\n\nTo better control LLM behavior, you can use the `where` keyword to specify constraints and data types of the generated text. This enables guidance of the model's reasoning process, and constraining of intermediate outputs using an [expressive constraint language](https://lmql.ai/docs/language/constraints.html).\n\nBeyond this linear form of scripting, LMQL also supports a number of decoding algorithms to execute your program, such as `argmax`, `sample` or even advanced branching decoders like [beam search and `best_k`](https://lmql.ai/docs/language/decoding.html). \n\nLearn more about LMQL by exploring thne **[Example Showcase](https://lmql.ai)**, by running your own programs in our **[browser-based Playground IDE](https://lmql.ai/playground)** or by reading the **[documentation](https://lmql.ai/docs)**.\n\n## Feature Overview\n\nLMQL is designed to make working with language models like OpenAI and ðŸ¤— Transformers more efficient and powerful through its advanced functionality, including multi-variable templates, conditional distributions, constraints, datatypes and control flow.\n\n- [X] **Python Syntax**: Write your queries using [familiar Python syntax](https://lmql.ai/docs/language/overview.html), fully integrated with your Python environment (classes, variable captures, etc.)\n- [X] **Rich Control-Flow**: LMQL offers full Python support, enabling powerful [control flow and logic](https://lmql.ai/docs/language/scripted-prompting.html) in your prompting logic.\n- [X] **Advanced Decoding**: Take advantage of advanced decoding techniques like [beam search, best_k, and more](https://lmql.ai/docs/language/decoding.html).\n- [X] **Powerful Constraints Via Logit Masking**: Apply [constraints to model output](https://lmql.ai/docs/language/constraints.html), e.g. to specify token length, character-level constraints, datatype and stopping phrases to get more control of model behavior.\n- [X] **Optimizing Runtime:** LMQL leverages speculative execution to enable faster inference, constraint short-circuiting, more efficient token use and [tree-based caching](https://lmql.ai/blog/release-0.0.6.html).\n- [X] **Sync and Async API**: Execute hundreds of queries in parallel with LMQL's [asynchronous API](https://lmql.ai/docs/lib/python.html), which enables cross-query batching.\n- [X] **Multi-Model Support**: Seamlessly use LMQL with [OpenAI API, Azure OpenAI, and ðŸ¤— Transformers models](https://lmql.ai/docs/models/).\n- [X] **Extensive Applications**: Use LMQL to implement advanced applications like [schema-safe JSON decoding](https://github.com/microsoft/guidance#guaranteeing-valid-syntax-json-example-notebook), [algorithmic prompting](https://twitter.com/lbeurerkellner/status/1648076868807950337), [interactive chat interfaces](https://twitter.com/lmqllang/status/1645776209702182917), and [inline tool use](https://lmql.ai/#kv).\n- [X] **Library Integration**: Easily employ LMQL in your existing stack leveraging [LangChain](https://lmql.ai/docs/lib/integrations/langchain.html) or [LlamaIndex](https://lmql.ai/docs/lib/integrations/llama_index.html).\n- [X] **Flexible Tooling**: Enjoy an interactive development experience with [LMQL's Interactive Playground IDE](https://lmql.ai/playground), and [Visual Studio Code Extension](https://marketplace.visualstudio.com/items?itemName=lmql-team.lmql).\n- [X] **Output Streaming**: Stream model output easily via [WebSocket, REST endpoint, or Server-Sent Event streaming](https://github.com/eth-sri/lmql/blob/main/src/lmql/output/).\n\n## Getting Started\n\nTo install the latest version of LMQL run the following command with Python ==3.10 installed.\n\n```\npip install lmql\n```\n\n**Local GPU Support:** If you want to run models on a local GPU, make sure to install LMQL in an environment with a GPU-enabled installation of PyTorch >= 1.11 (cf. https://pytorch.org/get-started/locally/) and install via `pip install lmql[hf]`.\n\n## Running LMQL Programs\n\nAfter installation, you can launch the LMQL playground IDE with the following command:\n\n```\nlmql playground\n```\n\n> Using the LMQL playground requires an installation of Node.js. If you are in a conda-managed environment you can install node.js via `conda install nodejs=14.20 -c conda-forge`. Otherwise, please see the official Node.js website https://nodejs.org/en/download/ for instructions how to install it on your system.\n\nThis launches a browser-based playground IDE, including a showcase of many exemplary LMQL programs. If the IDE does not launch automatically, go to `http://localhost:3000`.\n\nAlternatively, `lmql run` can be used to execute local `.lmql` files. Note that when using local HuggingFace Transformers models in the Playground IDE or via `lmql run`, you have to first launch an instance of the LMQL Inference API for the corresponding model via the command `lmql serve-model`.\n\n### Configuring OpenAI API Credentials\n\nIf you want to use OpenAI models, you have to configure your API credentials. To do so you can either define the `OPENAI_API_KEY` environment variable or create a file `api.env` in the active working directory, with the following contents:\n\n```\nopenai-org: <org identifier>\nopenai-secret: <api secret>\n```\n\nFor system-wide configuration, you can also create an `api.env` file at `$HOME/.lmql/api.env` or at the project root of your LMQL distribution (e.g. `src/` in a development copy).\n\nAlternatively, you can use LMQL-specific env variables `LMQL_OPENAI_SECRET` and `LMQL_OPENAI_ORG`.\n\n## Installing the Latest Development Version\n\nTo install the latest (bleeding-edge) version of LMQL, you can also run the following command:\n\n```\npip install git+https://github.com/eth-sri/lmql\n```\n\nThis will install the `lmql` package directly from the `main` branch of this repository. We do not continously test the `main` version, so it may be less stable than the latest PyPI release.\n\n# Contributing\n\nLMQL is a community-centric project. If you are interested in contributing to LMQL, please see the [contributing guidelines](./CONTRIBUTING.md) for more information, and reach out to us via [Discord](https://discord.gg/7eJP4fcyNT). We are looking forward to your contributions!\n\n## Setting Up a Development Environment\n\nTo setup a `conda` environment for local LMQL development with GPU support, run the following commands:\n\n```\n# prepare conda environment\nconda env create -f scripts/conda/requirements.yml -n lmql\nconda activate lmql\n\n# registers the `lmql` command in the current shell\nsource scripts/activate-dev.sh\n```\n\n> **Operating System**: The GPU-enabled version of LMQL was tested to work on Ubuntu 22.04 with CUDA 12.0 and Windows 10 via WSL2 and CUDA 11.7. The no-GPU version (see below) was tested to work on Ubuntu 22.04 and macOS 13.2 Ventura or Windows 10 via WSL2.\n\n### Development without GPU\n\nThis section outlines how to setup an LMQL development environment without local GPU support. Note that LMQL without local GPU support only supports the use of API-integrated models like `openai/text-davinci-003`. Please see the OpenAI API documentation (https://platform.openai.com/docs/models/gpt-3-5) to learn more about the set of available models.\n\nTo setup a `conda` environment for LMQL with no GPU support, run the following commands:\n\n```\n# prepare conda environment\nconda env create -f scripts/conda/requirements-no-gpu.yml -n lmql-no-gpu\nconda activate lmql-no-gpu\n\n# registers the `lmql` command in the current shell\nsource scripts/activate-dev.sh\n```\n"
        },
        {
          "name": "api.py",
          "type": "blob",
          "size": 0.373046875,
          "content": "import lmql\n\nm = lmql.model(\"openai/gpt-3.5-turbo-instruct\")\n\n# simple generation\nm.generate_sync(\"Hello\", max_tokens=10)\n# Hello, I am a 23 year old female.\n\n# sequence scoring\nm.score_sync(\"Hello\", [\"World\", \"Apples\", \"Oranges\"])\n# lmql.ScoringResult(model='openai/gpt-3.5-turbo-instruct')\n# -World: -3.9417848587036133\n# -Apples: -15.26676321029663\n# -Oranges: -16.22640037536621"
        },
        {
          "name": "docs",
          "type": "tree",
          "content": null
        },
        {
          "name": "flake.lock",
          "type": "blob",
          "size": 5.3486328125,
          "content": "{\n  \"nodes\": {\n    \"flake-utils\": {\n      \"inputs\": {\n        \"systems\": \"systems\"\n      },\n      \"locked\": {\n        \"lastModified\": 1689068808,\n        \"narHash\": \"sha256-6ixXo3wt24N/melDWjq70UuHQLxGV8jZvooRanIHXw0=\",\n        \"owner\": \"numtide\",\n        \"repo\": \"flake-utils\",\n        \"rev\": \"919d646de7be200f3bf08cb76ae1f09402b6f9b4\",\n        \"type\": \"github\"\n      },\n      \"original\": {\n        \"owner\": \"numtide\",\n        \"repo\": \"flake-utils\",\n        \"type\": \"github\"\n      }\n    },\n    \"flake-utils_2\": {\n      \"inputs\": {\n        \"systems\": \"systems_2\"\n      },\n      \"locked\": {\n        \"lastModified\": 1685518550,\n        \"narHash\": \"sha256-o2d0KcvaXzTrPRIo0kOLV0/QXHhDQ5DTi+OxcjO8xqY=\",\n        \"owner\": \"numtide\",\n        \"repo\": \"flake-utils\",\n        \"rev\": \"a1720a10a6cfe8234c0e93907ffe81be440f4cef\",\n        \"type\": \"github\"\n      },\n      \"original\": {\n        \"owner\": \"numtide\",\n        \"repo\": \"flake-utils\",\n        \"type\": \"github\"\n      }\n    },\n    \"flake-utils_3\": {\n      \"inputs\": {\n        \"systems\": \"systems_3\"\n      },\n      \"locked\": {\n        \"lastModified\": 1689068808,\n        \"narHash\": \"sha256-6ixXo3wt24N/melDWjq70UuHQLxGV8jZvooRanIHXw0=\",\n        \"owner\": \"numtide\",\n        \"repo\": \"flake-utils\",\n        \"rev\": \"919d646de7be200f3bf08cb76ae1f09402b6f9b4\",\n        \"type\": \"github\"\n      },\n      \"original\": {\n        \"owner\": \"numtide\",\n        \"repo\": \"flake-utils\",\n        \"type\": \"github\"\n      }\n    },\n    \"llamaDotCpp\": {\n      \"inputs\": {\n        \"flake-utils\": \"flake-utils_2\",\n        \"nixpkgs\": \"nixpkgs\"\n      },\n      \"locked\": {\n        \"lastModified\": 1692316484,\n        \"narHash\": \"sha256-Ed73hOi1m56gFJ4aGt/3wUXXccTQ70ocpgifoQBzddU=\",\n        \"owner\": \"ggerganov\",\n        \"repo\": \"llama.cpp\",\n        \"rev\": \"604b8bdfa6320bbcb018eebcc1252dfede603c6b\",\n        \"type\": \"github\"\n      },\n      \"original\": {\n        \"owner\": \"ggerganov\",\n        \"repo\": \"llama.cpp\",\n        \"type\": \"github\"\n      }\n    },\n    \"nix-github-actions\": {\n      \"inputs\": {\n        \"nixpkgs\": [\n          \"poetry2nix\",\n          \"nixpkgs\"\n        ]\n      },\n      \"locked\": {\n        \"lastModified\": 1688870561,\n        \"narHash\": \"sha256-4UYkifnPEw1nAzqqPOTL2MvWtm3sNGw1UTYTalkTcGY=\",\n        \"owner\": \"nix-community\",\n        \"repo\": \"nix-github-actions\",\n        \"rev\": \"165b1650b753316aa7f1787f3005a8d2da0f5301\",\n        \"type\": \"github\"\n      },\n      \"original\": {\n        \"owner\": \"nix-community\",\n        \"repo\": \"nix-github-actions\",\n        \"type\": \"github\"\n      }\n    },\n    \"nixpkgs\": {\n      \"locked\": {\n        \"lastModified\": 1685931219,\n        \"narHash\": \"sha256-8EWeOZ6LKQfgAjB/USffUSELPRjw88A+xTcXnOUvO5M=\",\n        \"owner\": \"NixOS\",\n        \"repo\": \"nixpkgs\",\n        \"rev\": \"7409480d5c8584a1a83c422530419efe4afb0d19\",\n        \"type\": \"github\"\n      },\n      \"original\": {\n        \"owner\": \"NixOS\",\n        \"ref\": \"nixos-unstable\",\n        \"repo\": \"nixpkgs\",\n        \"type\": \"github\"\n      }\n    },\n    \"nixpkgs_2\": {\n      \"locked\": {\n        \"lastModified\": 1691937172,\n        \"narHash\": \"sha256-6bvVr4fBRydOigdFG6Eueq3nYhpWwO+C5LgwBiqvYcg=\",\n        \"owner\": \"NixOS\",\n        \"repo\": \"nixpkgs\",\n        \"rev\": \"b66113cc63f010781cf27ed8bfac63d347fd79a8\",\n        \"type\": \"github\"\n      },\n      \"original\": {\n        \"owner\": \"NixOS\",\n        \"ref\": \"release-23.05\",\n        \"repo\": \"nixpkgs\",\n        \"type\": \"github\"\n      }\n    },\n    \"poetry2nix\": {\n      \"inputs\": {\n        \"flake-utils\": \"flake-utils_3\",\n        \"nix-github-actions\": \"nix-github-actions\",\n        \"nixpkgs\": [\n          \"nixpkgs\"\n        ]\n      },\n      \"locked\": {\n        \"lastModified\": 1693051011,\n        \"narHash\": \"sha256-HNbuVCS/Fnl1YZOjBk9/MlIem+wM8fvIzTH0CVQrLSQ=\",\n        \"owner\": \"nix-community\",\n        \"repo\": \"poetry2nix\",\n        \"rev\": \"5b3a5151cf212021ff8d424f215fb030e4ff2837\",\n        \"type\": \"github\"\n      },\n      \"original\": {\n        \"owner\": \"nix-community\",\n        \"repo\": \"poetry2nix\",\n        \"type\": \"github\"\n      }\n    },\n    \"root\": {\n      \"inputs\": {\n        \"flake-utils\": \"flake-utils\",\n        \"llamaDotCpp\": \"llamaDotCpp\",\n        \"nixpkgs\": \"nixpkgs_2\",\n        \"poetry2nix\": \"poetry2nix\"\n      }\n    },\n    \"systems\": {\n      \"locked\": {\n        \"lastModified\": 1681028828,\n        \"narHash\": \"sha256-Vy1rq5AaRuLzOxct8nz4T6wlgyUR7zLU309k9mBC768=\",\n        \"owner\": \"nix-systems\",\n        \"repo\": \"default\",\n        \"rev\": \"da67096a3b9bf56a91d16901293e51ba5b49a27e\",\n        \"type\": \"github\"\n      },\n      \"original\": {\n        \"owner\": \"nix-systems\",\n        \"repo\": \"default\",\n        \"type\": \"github\"\n      }\n    },\n    \"systems_2\": {\n      \"locked\": {\n        \"lastModified\": 1681028828,\n        \"narHash\": \"sha256-Vy1rq5AaRuLzOxct8nz4T6wlgyUR7zLU309k9mBC768=\",\n        \"owner\": \"nix-systems\",\n        \"repo\": \"default\",\n        \"rev\": \"da67096a3b9bf56a91d16901293e51ba5b49a27e\",\n        \"type\": \"github\"\n      },\n      \"original\": {\n        \"owner\": \"nix-systems\",\n        \"repo\": \"default\",\n        \"type\": \"github\"\n      }\n    },\n    \"systems_3\": {\n      \"locked\": {\n        \"lastModified\": 1681028828,\n        \"narHash\": \"sha256-Vy1rq5AaRuLzOxct8nz4T6wlgyUR7zLU309k9mBC768=\",\n        \"owner\": \"nix-systems\",\n        \"repo\": \"default\",\n        \"rev\": \"da67096a3b9bf56a91d16901293e51ba5b49a27e\",\n        \"type\": \"github\"\n      },\n      \"original\": {\n        \"owner\": \"nix-systems\",\n        \"repo\": \"default\",\n        \"type\": \"github\"\n      }\n    }\n  },\n  \"root\": \"root\",\n  \"version\": 7\n}\n"
        },
        {
          "name": "flake.nix",
          "type": "blob",
          "size": 7.56640625,
          "content": "{\n  inputs = {\n    nixpkgs.url = \"github:NixOS/nixpkgs/release-23.05\";\n    poetry2nix.url = \"github:nix-community/poetry2nix\";\n    poetry2nix.inputs.nixpkgs.follows = \"nixpkgs\";\n    llamaDotCpp.url = \"github:ggerganov/llama.cpp\";\n    flake-utils.url = \"github:numtide/flake-utils\";\n  };\n  outputs = { self, nixpkgs, flake-utils, llamaDotCpp, poetry2nix }: let\n    llamaDotCppFlake = llamaDotCpp;\n    poetry2nixFlake = poetry2nix;\n    nonSystemSpecificOutputs = {\n      overlays = {\n        noSentencePieceCustomMallocOnDarwin = (final: prev: {\n          sentencepiece = if prev.stdenv.isDarwin then prev.sentencepiece.override { withGPerfTools = false; } else prev.sentencepiece;\n        });\n      };\n    };\n  in nonSystemSpecificOutputs // flake-utils.lib.eachSystem [ \"aarch64-darwin\" \"x86_64-linux\" ] (system: let\n    version =\n      if self.sourceInfo ? \"rev\"\n      then \"${self.sourceInfo.lastModifiedDate}-${self.sourceInfo.shortRev}\"\n      else \"dirty\";\n    pkgs = import nixpkgs {\n      inherit system;\n      config.allowUnfree = true; # needed for CUDA on Linux\n      overlays = [\n        nonSystemSpecificOutputs.overlays.noSentencePieceCustomMallocOnDarwin\n        poetry2nixFlake.overlay\n      ];\n    };\n    inherit (pkgs) lib;\n    llamaDotCppPkg = llamaDotCppFlake.packages.${system}.default;\n    mkPoetryEnv = {llamaDotCppPkg ? null, wantHf ? false, wantHfAccel ? false, wantHfGptq ? false, wantReplicate ? false}:\n      let\n        wantLlama = llamaDotCppPkg != null;\n      in pkgs.poetry2nix.mkPoetryEnv {\n        python = pkgs.python310;\n        projectDir = \"${self}/scripts/flake.d\";\n        overrides = import ./scripts/flake.d/overrides.nix {\n          inherit (pkgs) poetry2nix;\n          inherit llamaDotCppPkg;\n        };\n        editablePackageSources = {\n          lmql = self;\n        };\n        # huggingface tokenizers used for llama.cpp, replicate\n        extras =\n          lib.optionals wantLlama [ \"llama\" ] ++\n          lib.optionals (wantHf || wantHfAccel || wantHfGptq || wantLlama || wantReplicate) [ \"hf\" ] ++\n          lib.optionals wantHfAccel [ \"hf-accel\" ] ++\n          lib.optionals wantHfGptq [ \"hf-gptq\" ] ++\n          lib.optionals wantReplicate [ \"replicate\" ];\n      };\n\n    poetryEnvBasic = mkPoetryEnv { };\n    poetryEnvHf = mkPoetryEnv { wantHf = true; wantHfAccel = true; wantHfGptq = ! pkgs.stdenv.isDarwin; };\n    poetryEnvLlamaCpp = mkPoetryEnv { inherit llamaDotCppPkg; };\n    poetryEnvReplicate = mkPoetryEnv { wantReplicate = true; };\n    poetryEnvAll = mkPoetryEnv { inherit llamaDotCppPkg; wantHf = true; wantHfAccel = true; wantHfGptq = ! pkgs.stdenv.isDarwin; wantReplicate = true; };\n\n    mkLmtpServerApp = {llamaDotCppPkg ? null, ...} @ opts: {\n      type = \"app\";\n      program = \"${pkgs.writeShellScript \"run-lmtp-server\" ''\n        set -a\n        ${if llamaDotCppPkg != null then ''\n          PATH=${llamaDotCppPkg}/bin:$PATH\n        '' else \"\"}\n        PYTHONPATH=${self}/src\n        exec ${mkPoetryEnv opts}/bin/python -m lmql.cli serve-model \"$@\"\n      ''}\";\n    };\n    playgroundStaticContent = pkgs.mkYarnPackage rec {\n      pname = \"web\";\n      inherit version;\n      src = ./src/lmql/ui/playground;\n      yarnLock = \"${src}/yarn.lock\";\n      yarnNix = \"${src}/yarn.nix\";\n      packageJSON = \"${src}/package.json\";\n      dontStrip = true;\n\n      patchPhase  = ''\n        find . -type d -name browser-build -exec rm -rf -- {} +\n      '';\n\n      DISABLE_ESLINT_PLUGIN = \"true\";\n\n      buildPhase = ''\n        HOME=$(mktemp -d) yarn --offline build\n      '';\n\n      distPhase = ''\n        shopt -s extglob\n        mv \"$out\"/libexec/web/deps/web/build \"$out/content\"\n        rm -rf -- \"$out\"/!(content)\n      '';\n    };\n    mkPlaygroundPkg = poetryEnv:\n      pkgs.mkYarnPackage rec {\n        pname = \"lmql-playground-live\";\n        inherit version;\n        src = ./src/lmql/ui/live;\n        yarnLock = \"${src}/yarn.lock\";\n        yarnNix = \"${src}/yarn.nix\";\n        packageJSON = \"${src}/package.json\";\n        dontStrip = true;\n\n        buildPhase = ''\n          true\n        '';\n\n        distPhase = ''\n          # We need a Python interpreter with all the dependencies\n          mkdir -p -- $out/bin $out/libexec\n          ln -s ${poetryEnv}/bin/python \"$out/bin/python\"\n          ln -s ${pkgs.nodejs}/bin/node \"$out/bin/node\"\n\n          ln -s ${pkgs.writeShellScript \"lmql-live-run\" ''\n            bindir=''${BASH_SOURCE%/*}\n            : addr=''${addr:=127.0.0.1} port=''${port:=3000}\n            cd \"$bindir/../libexec/liveserve/deps/liveserve\" || exit\n            export PATH=$bindir:$PATH\n            export PYTHONPATH=${self}/src:$PYTHONPATH\n            export NODE_PATH=$out/libexec/node_modules\n            export PORT=$port\n            export content_dir=${playgroundStaticContent}/content\n            exec \"$bindir/node\" \"live.js\"\n          ''} \"$out/bin/run\"\n        '';\n\n        meta.mainProgram = \"run\";\n      }; in {\n    legacyPackages = pkgs;\n    apps = rec {\n      lmtp-server = lmtp-server-all;\n      lmtp-server-basic = mkLmtpServerApp { };\n      lmtp-server-hf = mkLmtpServerApp { wantHf = true; };\n      lmtp-server-replicate = mkLmtpServerApp { wantReplicate = true; };\n      lmtp-server-llamaCpp = mkLmtpServerApp { inherit llamaDotCppPkg; };\n      lmtp-server-all = mkLmtpServerApp { inherit llamaDotCppPkg; wantHf = true; wantHfAccel = true; wantHfGptq = ! pkgs.stdenv.isDarwin; wantReplicate = true; };\n    };\n    packages = rec {\n      # If someone just says they want to \"run LMQL\", let's give them the friendly interface.\n      default = playground;\n\n      llamaDotCpp = llamaDotCppPkg;\n\n      python-basic = poetryEnvBasic;\n      python-hf = poetryEnvHf;\n      python-llamaCpp = poetryEnvLlamaCpp;\n      python-replicate = poetryEnvReplicate;\n      python-all = poetryEnvAll;\n\n      playground = playground-all;\n      playground-basic = mkPlaygroundPkg poetryEnvBasic;\n      playground-hf = mkPlaygroundPkg poetryEnvHf;\n      playground-llamaCpp = mkPlaygroundPkg poetryEnvLlamaCpp;\n      playground-replicate = mkPlaygroundPkg poetryEnvReplicate;\n      playground-all = mkPlaygroundPkg poetryEnvAll;\n\n      lmql-docs = pkgs.runCommand \"lmql-docs\" {\n        python = pkgs.python310.withPackages (p: [p.myst-parser p.pydata-sphinx-theme p.sphinx-book-theme p.nbsphinx]);\n        docSource = ./docs/source;\n      } ''\n        PATH=${pkgs.pandoc}/bin:$PATH ${poetryEnvBasic}/bin/sphinx-build \"$docSource\" \"$out\"\n      '';\n    };\n    devShells = let\n      jsDevPackages = [\n        pkgs.yarn\n        pkgs.yarn2nix\n      ];\n      pythonDevPackages = [\n        pkgs.pandoc # not python-specific, but used in building docs\n        pkgs.poetry\n        pkgs.poetry2nix.cli\n        (pkgs.python310.withPackages (p: [p.poetry-core]))\n      ];\n      runtimePackages = [\n        pkgs.nodejs\n      ];\n    in let lmqlDevShell = poetryEnv: extraBuildInputs: poetryEnv.env.overrideAttrs (oldAttrs: {\n        shellHook = ''\n          PS1='[lmql] '\"$PS1\"\n          lmql() { python -m lmql.cli \"$@\"; }\n        '';\n        PYTHONPATH = \"${builtins.toString ./src}\";\n        buildInputs = jsDevPackages ++ pythonDevPackages ++ runtimePackages ++ extraBuildInputs;\n      });\n    in rec {\n      default = lmql-all;\n      # python interpreter able to import lmql successfully\n      lmql-basic = lmqlDevShell poetryEnvBasic [];\n      lmql-hf = lmqlDevShell poetryEnvHf [];\n      lmql-llamaCpp = lmqlDevShell poetryEnvLlamaCpp [llamaDotCppPkg];\n      lmql-replicate = lmqlDevShell poetryEnvReplicate [];\n      lmql-all = lmqlDevShell poetryEnvAll [llamaDotCppPkg];\n      # tools to run poetry and yarn2nix, and nothing else\n      minimal = pkgs.mkShell {\n        name = \"minimal-dev-shell\";\n        buildInputs = jsDevPackages ++ pythonDevPackages;\n      };\n    };\n  });\n}\n"
        },
        {
          "name": "scripts",
          "type": "tree",
          "content": null
        },
        {
          "name": "setup.cfg",
          "type": "blob",
          "size": 1.220703125,
          "content": "[metadata]\nname = lmql \nversion = 0.999999\nauthor = Luca Beurer-Kellner, Marc Fischer, Martin Vechev\nauthor_email = hello@lmql.ai\ndescription = A query language for language models.\nlong_description = file: README.md\nlong_description_content_type = text/markdown\nurl = https://lmql.ai\nproject_urls =\n    Docs = https://lmql.ai/docs\nclassifiers =\n    Programming Language :: Python :: 3\n    Operating System :: OS Independent\n\n[options]\npackages = find:\npackage_dir =\n    = src\ninclude_package_data = True\npython_requires = >=3.10\n\n# When updating this list, regenerate poetry.lock by running ''poetry lock --no-update'' in scripts/flake.d\ninstall_requires =\n    aiohttp <4.0.0\n    astunparse ==1.6.3\n    openai\n    termcolor\n    numpy\n    tiktoken\n\n[options.extras_require]\n# When updating version requirements for optional dependencies, also update flake.d/pyproject.toml\nhf =\n    transformers >=4.32.0\nhf-accel =\n    accelerate\nhf-gptq =\n    optimum\n    auto-gptq\nllama =\n    llama-cpp-python\nreplicate =\n    aiohttp-sse-client\n    transformers >=4.32.0\ntests =\n    pytest\n    pytest-asyncio\n\n[options.packages.find]\nwhere = src\nexclude = \n    **/node_modules/*\n    **/vscode/**/*\n\n[options.entry_points]\nconsole_scripts =\n    lmql = lmql.cli:main\n"
        },
        {
          "name": "setup.py",
          "type": "blob",
          "size": 0.0361328125,
          "content": "from setuptools import setup\n\nsetup()"
        },
        {
          "name": "src",
          "type": "tree",
          "content": null
        },
        {
          "name": "yarn.lock",
          "type": "blob",
          "size": 0.083984375,
          "content": "# THIS IS AN AUTOGENERATED FILE. DO NOT EDIT THIS FILE DIRECTLY.\n# yarn lockfile v1\n\n\n"
        }
      ]
    }
  ]
}