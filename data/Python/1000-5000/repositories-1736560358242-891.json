{
  "metadata": {
    "timestamp": 1736560358242,
    "page": 891,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjkwMA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "hitsz-ids/synthetic-data-generator",
      "stars": 3315,
      "defaultBranch": "main",
      "files": [
        {
          "name": ".all-contributorsrc",
          "type": "blob",
          "size": 1.876953125,
          "content": "{\n  \"projectName\": \"synthetic-data-generator\",\n  \"projectOwner\": \"hitsz-ids\",\n  \"files\": [\n    \"README.md\",\n    \"README_ZH_CN.md\"\n  ],\n  \"commitType\": \"docs\",\n  \"commitConvention\": \"angular\",\n  \"contributorsPerLine\": 7,\n  \"contributors\": [\n    {\n      \"login\": \"Wh1isper\",\n      \"name\": \"Zhongsheng Ji\",\n      \"avatar_url\": \"https://avatars.githubusercontent.com/u/43375501?v=4\",\n      \"profile\": \"https://wh1isper.github.io/\",\n      \"contributions\": [\n        \"code\"\n      ]\n    },\n    {\n      \"login\": \"MooooCat\",\n      \"name\": \"MoooCat\",\n      \"avatar_url\": \"https://avatars.githubusercontent.com/u/141886018?v=4\",\n      \"profile\": \"https://github.com/MooooCat\",\n      \"contributions\": [\n        \"code\"\n      ]\n    },\n    {\n      \"login\": \"joeyscave\",\n      \"name\": \"YUAN KAIWEN\",\n      \"avatar_url\": \"https://avatars.githubusercontent.com/u/72662648?v=4\",\n      \"profile\": \"https://github.com/joeyscave\",\n      \"contributions\": [\n        \"code\"\n      ]\n    },\n    {\n      \"login\": \"sjh120\",\n      \"name\": \"sjh120\",\n      \"avatar_url\": \"https://avatars.githubusercontent.com/u/86507761?v=4\",\n      \"profile\": \"https://github.com/sjh120\",\n      \"contributions\": [\n        \"code\"\n      ]\n    },\n    {\n      \"login\": \"Z712023\",\n      \"name\": \"Z712023\",\n      \"avatar_url\": \"https://avatars.githubusercontent.com/u/132286135?v=4\",\n      \"profile\": \"https://github.com/Z712023\",\n      \"contributions\": [\n        \"code\"\n      ]\n    },\n    {\n\n      \"login\": \"Femi-lawal\",\n      \"name\": \"Oluwafemi Lawal\",\n      \"avatar_url\": \"https://avatars.githubusercontent.com/u/33192240?v=4\",\n      \"profile\": \"http://femilawal.com\",\n      \"contributions\": [\n        \"code\"\n       ]\n      },\n      {\n      \"login\": \"iokk3732\",\n      \"name\": \"iokk3732\",\n      \"avatar_url\": \"https://avatars.githubusercontent.com/u/141700052?v=4\",\n      \"profile\": \"https://github.com/iokk3732\",\n      \"contributions\": [\n        \"code\"\n      ]\n    }\n  ]\n}\n"
        },
        {
          "name": ".coveragerc",
          "type": "blob",
          "size": 0.060546875,
          "content": "[run]\nomit =\n    */tests/*\n    */sdgx/models/components/sdv_*\n"
        },
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 4.2685546875,
          "content": "# Created by https://www.toptal.com/developers/gitignore/api/macos,emacs,python\n# Edit at https://www.toptal.com/developers/gitignore?templates=macos,emacs,python\n\n*.log\n*.csv\n\n### Emacs ###\n# -*- mode: gitignore; -*-\n*~\n\\#*\\#\n/.emacs.desktop\n/.emacs.desktop.lock\n*.elc\nauto-save-list\ntramp\n.\\#*\n\n# Org-mode\n.org-id-locations\n*_archive\n\n# flymake-mode\n*_flymake.*\n\n# eshell files\n/eshell/history\n/eshell/lastdir\n\n# elpa packages\n/elpa/\n\n# reftex files\n*.rel\n\n# AUCTeX auto folder\n/auto/\n\n# cask packages\n.cask/\ndist/\n\n# Flycheck\nflycheck_*.el\n\n# server auth directory\n/server/\n\n# projectiles files\n.projectile\n\n# directory configuration\n.dir-locals.el\n\n# network security\n/network-security.data\n\n\n### macOS ###\n# General\n.DS_Store\n.AppleDouble\n.LSOverride\n\n# Icon must end with two \\r\nIcon\n\n\n# Thumbnails\n._*\n\n# Files that might appear in the root of a volume\n.DocumentRevisions-V100\n.fseventsd\n.Spotlight-V100\n.TemporaryItems\n.Trashes\n.VolumeIcon.icns\n.com.apple.timemachine.donotpresent\n\n# Directories potentially created on remote AFP share\n.AppleDB\n.AppleDesktop\nNetwork Trash Folder\nTemporary Items\n.apdisk\n\n### macOS Patch ###\n# iCloud generated files\n*.icloud\n\n### Python ###\n# Byte-compiled / optimized / DLL files\n__pycache__/\n*.py[cod]\n*$py.class\n\n# C extensions\n*.so\n\n# Distribution / packaging\n.Python\nbuild/\ndevelop-eggs/\ndownloads/\neggs/\n.eggs/\nlib/\nlib64/\nparts/\nsdist/\nvar/\nwheels/\nshare/python-wheels/\n*.egg-info/\n.installed.cfg\n*.egg\nMANIFEST\n\n# PyInstaller\n#  Usually these files are written by a python script from a template\n#  before PyInstaller builds the exe, so as to inject date/other infos into it.\n*.manifest\n*.spec\n\n# Installer logs\npip-log.txt\npip-delete-this-directory.txt\n\n# Unit test / coverage reports\nhtmlcov/\n.tox/\n.nox/\n.coverage\n.coverage.*\n.cache\nnosetests.xml\ncoverage.xml\n*.cover\n*.py,cover\n.hypothesis/\n.pytest_cache/\ncover/\n\n# Translations\n*.mo\n*.pot\n\n# Django stuff:\n*.log\nlocal_settings.py\ndb.sqlite3\ndb.sqlite3-journal\n\n# Flask stuff:\ninstance/\n.webassets-cache\n\n# Scrapy stuff:\n.scrapy\n\n# Sphinx documentation\ndocs/_build/\n\n# PyBuilder\n.pybuilder/\ntarget/\n\n# Jupyter Notebook\n.ipynb_checkpoints\n\n# IPython\nprofile_default/\nipython_config.py\n\n# pyenv\n#   For a library or package, you might want to ignore these files since the code is\n#   intended to run in multiple environments; otherwise, check them in:\n# .python-version\n\n# pipenv\n#   According to pypa/pipenv#598, it is recommended to include Pipfile.lock in version control.\n#   However, in case of collaboration, if having platform-specific dependencies or dependencies\n#   having no cross-platform support, pipenv may install dependencies that don't work, or not\n#   install all needed dependencies.\n#Pipfile.lock\n\n# poetry\n#   Similar to Pipfile.lock, it is generally recommended to include poetry.lock in version control.\n#   This is especially recommended for binary packages to ensure reproducibility, and is more\n#   commonly ignored for libraries.\n#   https://python-poetry.org/docs/basic-usage/#commit-your-poetrylock-file-to-version-control\n#poetry.lock\n\n# pdm\n#   Similar to Pipfile.lock, it is generally recommended to include pdm.lock in version control.\n#pdm.lock\n#   pdm stores project-wide configurations in .pdm.toml, but it is recommended to not include it\n#   in version control.\n#   https://pdm.fming.dev/#use-with-ide\n.pdm.toml\n\n# PEP 582; used by e.g. github.com/David-OConnor/pyflow and github.com/pdm-project/pdm\n__pypackages__/\n\n# Celery stuff\ncelerybeat-schedule\ncelerybeat.pid\n\n# SageMath parsed files\n*.sage.py\n\n# Environments\n.env\n.venv\nenv/\nvenv/\nENV/\nenv.bak/\nvenv.bak/\n\n# Spyder project settings\n.spyderproject\n.spyproject\n\n# Rope project settings\n.ropeproject\n\n# mkdocs documentation\n/site\n\n# mypy\n.mypy_cache/\n.dmypy.json\ndmypy.json\n\n# Pyre type checker\n.pyre/\n\n# pytype static type analyzer\n.pytype/\n\n# Cython debug symbols\ncython_debug/\n\n# PyCharm\n#  JetBrains specific template is maintained in a separate JetBrains.gitignore that can\n#  be found at https://github.com/github/gitignore/blob/main/Global/JetBrains.gitignore\n#  and can be added to the global gitignore or merged into this file.  For a more nuclear\n#  option (not recommended) you can uncomment the following to ignore the entire idea folder.\n.idea/\n.run/\n\n# End of https://www.toptal.com/developers/gitignore/api/macos,emacs,python\n\n*.log\n\n.sdgx_cache\n.ndarry_cache\n"
        },
        {
          "name": ".pre-commit-config.yaml",
          "type": "blob",
          "size": 1.4052734375,
          "content": "repos:\n  - repo: https://github.com/pre-commit/pre-commit-hooks\n    rev: v5.0.0\n    hooks:\n      - id: end-of-file-fixer\n      - id: check-case-conflict\n      - id: check-executables-have-shebangs\n      - id: requirements-txt-fixer\n      - id: check-added-large-files\n      - id: check-case-conflict\n      - id: check-toml\n      - id: check-yaml\n      - id: debug-statements\n      - id: forbid-new-submodules\n      - id: trailing-whitespace\n\n  - repo: https://github.com/psf/black\n    rev: 24.10.0\n    hooks:\n      - id: black\n        args: [\"--line-length\", \"100\"]\n\n  - repo: https://github.com/PyCQA/isort\n    rev: 5.13.2\n    hooks:\n      - id: isort\n        files: \\.py$\n        args: [--profile=black]\n\n  - repo: https://github.com/executablebooks/mdformat\n    rev: 0.7.21\n    hooks:\n      - id: mdformat\n        additional_dependencies:\n          [mdformat-gfm, mdformat-frontmatter, mdformat-footnote]\n\n  - repo: https://github.com/pre-commit/mirrors-prettier\n    rev: \"v4.0.0-alpha.8\"\n    hooks:\n      - id: prettier\n        entry: env PRETTIER_LEGACY_CLI=1 prettier\n        types_or: [yaml, html, json]\n\n  - repo: https://github.com/pre-commit/pygrep-hooks\n    rev: \"v1.10.0\"\n    hooks:\n      - id: rst-backticks\n      - id: rst-directive-colons\n      - id: rst-inline-touching-normal\n\n  - repo: https://github.com/pre-commit/mirrors-mypy\n    rev: \"v1.14.0\"\n    hooks:\n      - id: mypy\n        files: sdgx\n        stages: [manual]\n"
        },
        {
          "name": ".prettierignore",
          "type": "blob",
          "size": 0.0107421875,
          "content": "**/*.ipynb\n"
        },
        {
          "name": ".readthedocs.yaml",
          "type": "blob",
          "size": 0.4375,
          "content": "# .readthedocs.yaml\n# Read the Docs configuration file\n# See https://docs.readthedocs.io/en/stable/config-file/v2.html for details\n\n# Required\nversion: 2\n\n# Set the OS, Python version and other tools you might need\nbuild:\n  os: ubuntu-22.04\n  tools:\n    python: \"3.10\"\n\nsphinx:\n  configuration: docs/source/conf.py\n\npython:\n  install:\n    # install itself with pip install .\n    - method: pip\n      path: .\n      extra_requirements:\n        - docs\n"
        },
        {
          "name": "CONTRIBUTING.md",
          "type": "blob",
          "size": 16.0810546875,
          "content": "# Overview\n\n## Technical\n\nThe following is a list of technologies involved in this project.\n\n| Technology      | Category           | Purpose                                                                                                                                                                                                                                                                                        |\n| --------------- | ------------------ | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n| PyTorch         | Deep Learning      | Mainstream deep learning framework providing dynamic computation graphs and automatic differentiation. Used for: - Building and training generative models like VAE - GPU-accelerated model training - Implementing custom neural network layers and loss functions - Model saving and loading |\n| NumPy           | Deep Learning      | Fundamental scientific computing library with version constraint for stability. Used for: - Efficient multi-dimensional array operations - Data preprocessing and feature engineering - Numerical computation and statistical analysis - Data interchange with other scientific libraries      |\n| SciPy           | Deep Learning      | Advanced scientific computing toolkit built on NumPy. Used for: - Advanced statistical analysis and hypothesis testing - Probability distribution calculation and random number generation - Optimization algorithms - Sparse matrix operations and linear algebra computations                |\n| Pandas          | Deep Learning      | Powerful data analysis and manipulation library. Used for: - Structured data I/O - Data cleaning and preprocessing - Complex data transformation and aggregation - Time series data handling                                                                                                   |\n| scikit-learn    | Deep Learning      | Machine learning algorithm toolkit. Used for: - Data preprocessing and feature scaling - Model evaluation and cross-validation - Feature selection and dimensionality reduction - ML model benchmarking                                                                                        |\n| Faker           | Data Generation    | Multi-language fake data generation library. Used for: - Test dataset generation - System testing with mock data - Example data generation - Custom data generation rule support                                                                                                               |\n| Matplotlib      | Data Evaluation    | Comprehensive plotting library. Used for: - Training process visualization - Data distribution and statistical plotting - Model evaluation visualization - Report and documentation graphics                                                                                                   |\n| table-evaluator | Data Evaluation    | Specialized tabular data evaluation tool. Used for: - Statistical comparison between real and synthetic data - Data generation quality assessment - Data quality reporting - Distribution comparison visualization                                                                             |\n| PyArrow         | Data Processing    | High-performance data processing library. Used for: - Fast I/O for large-scale data - Memory-efficient data processing - Integration with big data tools - Columnar data format handling                                                                                                       |\n| Pydantic        | Data Processing    | Data validation and settings management framework. Used for: - Type-safe configuration loading - API data validation - Model parameter validation - Data schema definition and verification                                                                                                    |\n| loguru          | Logging            | Modern logging utility. Used for: - Training process logging - Error tracking and debugging - Performance monitoring - Structured log output                                                                                                                                                   |\n| cloudpickle     | Data Processing    | Enhanced Python object serialization tool. Used for: - Model serialization and deserialization - Complex Python object persistence - Data transfer in distributed computing - Intermediate result caching                                                                                      |\n| pluggy          | Plugin System      | Python plugin framework. Used for: - Implementing extensible architecture - Managing model and processor plugins - Supporting custom component integration - Implementing modular design                                                                                                       |\n| joblib          | Parallel Computing | Parallel computing support library. Used for: - Data processing parallelization - CPU-intensive task optimization - Result caching - Parallel model training                                                                                                                                   |\n| Click           | CLI Tools          | Command-line interface framework. Used for: - Building CLI tools - Parameter parsing and validation - Subcommand management - User interaction interface                                                                                                                                       |\n\n## Core Process Diagram\n\n```mermaid\nsequenceDiagram\n    participant User\n    participant DataConnector\n    participant DataLoader\n    participant Metadata\n    participant Synthesizer\n    participant DataProcessor\n    participant Model\n    participant Evaluator\n\n    User->>DataConnector: create_connector()\n    DataConnector-->>DataLoader: connector\n    User->>DataLoader: load_data()\n    DataLoader->>Metadata: from_dataloader()\n\n    User->>Synthesizer: fit(metadata)\n    Synthesizer->>DataProcessor: convert(data)\n    DataProcessor-->>Synthesizer: processed_data\n    Synthesizer->>Model: fit(metadata, processed_data)\n    Model-->>Synthesizer: trained_model\n\n    User->>Synthesizer: sample(n_samples)\n    Synthesizer->>Model: generate()\n    Model-->>Synthesizer: synthetic_data\n    Synthesizer->>DataProcessor: reverse_convert(synthetic_data)\n    DataProcessor-->>Synthesizer: restored_data\n    Synthesizer-->>User: restored_data\n\n    User->>Evaluator: evaluate(real_data, restored_data)\n    Evaluator-->>User: evaluation_results\n```\n\n## 4+1 architectural view\n\n### Logical view\n\n```mermaid\ngraph TB\n    subgraph Infrastructure[\"Infrastructure Layer\"]\n        direction LR\n        Arrow[\"Apache Arrow<br/>Data Processing Engine\"]\n        PyTorch[\"PyTorch<br/>Deep Learning Framework\"]\n        Sklearn[\"Sklearn<br/>Machine Learning Framework\"]\n    end\n\n    subgraph Core[\"SDG Core Library\"]\n        direction TB\n\n        subgraph DataEngine[\"Data Engine Layer\"]\n            DataLoader[\"DataLoader<br/>Data Loader\"]\n            Metadata[\"Metadata<br/>Metadata Management\"]\n            ProcessedData[\"ProcessedData<br/>Unified Data Format\"]\n        end\n\n        subgraph Processing[\"Processing Layer\"]\n            Inspector[\"Inspector<br/>Data Inspector\"]\n            Processor[\"Processor<br/>Data Processor\"]\n            Transformer[\"Transformer<br/>Feature Transformer\"]\n            Formatter[\"Formatter<br/>Format Converter\"]\n        end\n\n        Synthesizer[\"Synthesizer<br/>Data Synthesizer\"]\n\n        subgraph Models[\"Model Layer\"]\n            Traditional[\"Traditional Models\"]\n            LLM[\"LLM Models\"]\n\n            Traditional --GAN Network--> CTGAN\n            Traditional --Gaussian Copula--> GaussianCopula\n\n            LLM --No Data Generation--> Synthesis[\"Data Synthesis\"]\n        end\n\n        Evaluator[\"Evaluator<br/>Data Evaluator\"]\n\n        %% Data Engine Internal Relations\n        DataLoader --Load Raw Data--> ProcessedData\n        DataLoader --Extract--> Metadata\n        Metadata --Guide Processing--> ProcessedData\n\n        %% Processing Layer and Data Engine Relations\n        ProcessedData --Input--> Inspector\n        Inspector --Inspection Results--> Processor\n        Processor --Processed Data--> Transformer\n        Transformer --Transformed Features--> Formatter\n\n        %% Synthesizer Relations\n        Metadata --Metadata Config--> Synthesizer\n        Formatter --Normalized Data--> Synthesizer\n        Synthesizer --Control--> Models\n        Models --Generated Data--> Synthesizer\n\n        %% Evaluator Relations\n        ProcessedData --Raw Data--> Evaluator\n        Synthesizer --Synthetic Data--> Evaluator\n    end\n\n    subgraph PluginSystem[\"Plugin System Layer\"]\n        direction LR\n        ConnectorManager[\"ConnectorManager<br/>Data Source Management\"]\n        ProcessorManager[\"ProcessorManager<br/>Processor Management\"]\n        InspectorManager[\"InspectorManager<br/>Inspector Management\"]\n        ModelManager[\"ModelManager<br/>Model Management\"]\n        EvaluatorManager[\"EvaluatorManager<br/>Evaluator Management\"]\n    end\n\n    %% Infrastructure and Core Dependencies\n    Arrow -.->|\"Provide Column Storage and Computation\"| DataLoader\n    PyTorch -.->|\"Provide Deep Learning Training\"| CTGAN\n    Sklearn -.->|\"Provide Probability Distribution Fitting\"| GaussianCopula\n\n    %% Plugin System and Core Relations\n    DataLoader --Register--> ConnectorManager\n    Processor --Register--> ProcessorManager\n    Inspector --Register--> InspectorManager\n    Models --Register--> ModelManager\n    Evaluator --Register--> EvaluatorManager\n```\n\n### Process view\n\n```mermaid\nflowchart TB\n    subgraph SDGProcess[SDG Main Process]\n        direction TB\n        CLI[CLI Interface] --> Synthesizer\n\n        subgraph DataAccess[Data Access Layer]\n            DataConnector[Data Connector]\n            DataLoader[Data Loader]\n            Metadata[Metadata]\n            ProcessedData[Processed Data]\n            ConnectorManager[Connector Manager]\n        end\n\n        subgraph DataProcessing[Data Processing Layer]\n            Inspector[Inspector]\n            Processor[Data Processor]\n            Transformer[Data Transformer]\n            Formatter[Data Formatter]\n            Sampler[Data Sampler]\n            ProcessorManager[Processor Manager]\n        end\n\n        subgraph ModelLayer[Model Layer]\n            ModelManager[Model Manager]\n            CTGAN[CTGAN Model]\n            LLM[LLM Model]\n        end\n\n        Synthesizer[Synthesizer]\n        Evaluator[Evaluator]\n    end\n```\n\n### Development view\n\n```mermaid\ngraph TB\n    subgraph SDGPackages[\"SDG Package Structure\"]\n        direction TB\n\n        subgraph Core[\"sdgx\"]\n            DataModels[\"data_models<br/>(Metadata and Data Processing)\"]\n            Models[\"models<br/>(Synthesis Model Implementation)\"]\n            DataConnectors[\"data_connectors<br/>(Data Source Connection)\"]\n            CLI[\"cli<br/>(Command Line Interface)\"]\n            Utils[\"utils<br/>(Utility Functions)\"]\n            Types[\"types<br/>(Type Definitions)\"]\n            Exceptions[\"exceptions<br/>(Exception Definitions)\"]\n        end\n\n        subgraph Dependencies[\"Core Dependencies\"]\n            PyTorch[\"torch>=2<br/>(Deep Learning)\"]\n            Arrow[\"pyarrow<br/>(Data Processing)\"]\n            Sklearn[\"scikit-learn<br/>(Machine Learning)\"]\n            Pluggy[\"pluggy<br/>(Plugin System)\"]\n            Pandas[\"pandas<br/>(Data Analysis)\"]\n            OpenAI[\"openai>=1.10.0<br/>(LLM Interface)\"]\n        end\n    end\n\n    %% Core Package Dependencies\n    DataModels --Uses--> Types\n    DataModels --Uses--> Exceptions\n    Models --Uses--> DataModels\n    DataConnectors --Uses--> DataModels\n    CLI --Uses--> Models\n    CLI --Uses--> DataConnectors\n\n    %% Infrastructure Dependencies\n    Models --Deep Learning--> PyTorch\n    Models --LLM--> OpenAI\n    DataConnectors --Data Processing--> Arrow\n    DataConnectors --Data Analysis--> Pandas\n    Models --Machine Learning--> Sklearn\n```\n\n### Physical view\n\n```mermaid\ngraph TB\n    subgraph ExternalServices[External Service Nodes]\n        OpenAI[(\"OpenAI API Service<br/>>= 1.10.0\")]\n    end\n\n    subgraph DistributionNodes[Distribution Registry Nodes]\n        PyPI[(\"PyPI Registry<br/>sdgx package\")]\n        Docker[(\"Docker Registry<br/>idsteam/sdgx\")]\n    end\n\n    subgraph ComputeNode[Deployment Node]\n        subgraph Container[Docker Container]\n            SDGX1[\"SDGX Service\"]\n        end\n\n        subgraph PythonRuntime[Python Environment]\n            SDGX2[\"SDGX Package\"]\n            PyTorch[\"PyTorch >= 2.0\"]\n            Arrow[\"Apache Arrow\"]\n        end\n    end\n\n    %% Deployment Relations\n    PyPI -->|\"pip install\"| PythonRuntime\n    Docker -->|\"docker pull\"| Container\n    OpenAI -->|\"API\"| SDGX1\n    OpenAI -->|\"API\"| SDGX2\n\n    %% Runtime Dependencies\n    SDGX2 --> PyTorch\n    SDGX2 --> Arrow\n```\n\n### Scenarios\n\n```mermaid\nsequenceDiagram\n    participant User\n    participant DataConnector\n    participant Metadata\n    participant Model\n    participant Evaluator\n\n    %% Scenario 1: Data-Driven Synthesis\n    rect rgb(200, 220, 240)\n        Note over User,Evaluator: Scenario 1: Data-Driven Synthesis\n        User->>DataConnector: Load Raw Data\n        DataConnector->>Metadata: Auto-detect Metadata\n        Metadata->>Model: Configure Model\n        Model->>Model: Train and Generate\n        Model->>Evaluator: Evaluate Synthetic Data\n        Evaluator->>User: Return Evaluation Results\n    end\n\n    %% Scenario 2: LLM-Based No-Data Synthesis\n    rect rgb(220, 240, 200)\n        Note over User,Evaluator: Scenario 2: LLM-Based No-Data Synthesis\n        User->>Metadata: Define Metadata\n        Metadata->>Model: Configure LLM Model\n        Model->>Model: Generate Synthetic Data\n        Model->>User: Return Generated Results\n    end\n```\n\n# Development Guide\n\n## Clone project\n\n```sh\ngit clone https://github.com/hitsz-ids/synthetic-data-generator.git\n```\n\n## Create python env\n\nThis is based on [miniconda](https://docs.anaconda.com/miniconda/).\n\n```sh\nconda create -n sdg python=3.11\nconda activate sdg\n```\n\n## Code Style and Lint\n\nWe use [black](https://github.com/psf/black) as the code formatter, the best way to use it is to install the pre-commit hook, it will automatically format the code before each commit\n\nInstall pre-commit before commit\n\n```bash\npip install pre-commit\npre-commit install\n```\n\nPre-commit will automatically format the code before each commit, It can also be executed manually on all files\n\n```bash\npre-commit run --all-files\n```\n\nComment style follows [Google Python Style Guide](https://google.github.io/styleguide/pyguide.html#38-comments-and-docstrings).\n\n## Install Locally\n\n```bash\npip install -e '.[test,docs]'\n```\n\n## Unit tests\n\nWe use pytest to write unit tests, and use pytest-cov to generate coverage reports\n\n```bash\npytest -vv --cov-config=.coveragerc --cov=sdgx/ tests\n```\n\nRun unit-test before PR, **ensure that new features are covered by unit tests**\n\n## Build Docs\n\nInstall docs dependencies\n\n```bash\npip install -e .[docs]\n```\n\nBuild docs\n\n```bash\ncd docs && make html\n```\n\nUse [start-docs-host.sh](dev-tools/start-docs-host.sh) to deploy a local http server to view the docs\n\n```bash\ncd ./dev-tools && ./start-docs-host.sh\n```\n\nAccess `http://localhost:8910` for docs.\n\n## Started with the features\n\nAfter understanding all the content mentioned in the overview of this chapter, we recommend starting with the sdg functionality. You can explore everything under the tests/ package, using the LLM Chat Tool (such as cursor) to add test and tested classes, thereby gaining insight into the detailed functionalities you want to understand.\n\nHere we provide a system-role prompt for the LLM Chat Tool to help you formulate good questions.\n\n```sh\nPlease provide a detailed explanation of the logic and implementation of the following Python class. In addition to reviewing the code for this class, you also need to look at the code for the subject class being tested:\n1. Describe the overall functionality and purpose of this class, including the subject being tested, its functions, and basic usage logic.\n2. Analyze each method in the class, explaining its functionality and parameters.\n3. Explain any important algorithms or design patterns used in the class.\n```\n"
        },
        {
          "name": "CONTRIBUTING_ZH_CN.md",
          "type": "blob",
          "size": 14.1240234375,
          "content": "# 概览\n\n## 技术框架\n\n以下是此项目涉及的技术列表。\n\n|      技术       |   类别   |                                                                       目的                                                                       |\n| :-------------: | :------: | :----------------------------------------------------------------------------------------------------------------------------------------------: |\n|     PyTorch     | 深度学习 | 主流深度学习框架，提供动态计算图和自动微分。用于： - 构建和训练生成模型如VAE - GPU加速模型训练 - 实现自定义神经网络层和损失函数 - 模型保存和加载 |\n|      NumPy      | 深度学习 |      基础科学计算库，版本限制用于保持稳定性。用于： - 高效多维数组运算 - 数据预处理和特征工程 - 数值计算和统计分析 - 与其他科学库的数据交换      |\n|      SciPy      | 深度学习 |        基于NumPy的高级科学计算工具包。用于： - 高级统计分析和假设检验 - 概率分布计算和随机数生成 - 优化算法 - 稀疏矩阵运算和线性代数计算         |\n|     Pandas      | 深度学习 |                  强大的数据分析和操作库。用于： - 结构化数据输入输出 - 数据清洗和预处理 - 复杂数据转换和聚合 - 时间序列数据处理                  |\n|  scikit-learn   | 深度学习 |                  机器学习算法工具包。用于： - 数据预处理和特征缩放 - 模型评估和交叉验证 - 特征选择和降维 - 机器学习模型基准测试                  |\n|      Faker      | 数据生成 |                    多语言虚拟数据生成库。用于： - 测试数据集生成 - 系统测试的模拟数据 - 示例数据生成 - 自定义数据生成规则支持                    |\n|   Matplotlib    | 数据评估 |                            综合绘图库。用于： - 训练过程可视化 - 数据分布和统计绘图 - 模型评估可视化 - 报告和文档图形                            |\n| table-evaluator | 数据评估 |                 专门的表格数据评估工具。用于： - 真实数据与合成数据的统计比较 - 数据生成质量评估 - 数据质量报告 - 分布比较可视化                 |\n|     PyArrow     | 数据处理 |                 高性能数据处理库。用于： - 大规模数据的快速输入输出 - 内存高效的数据处理 - 与大数据工具的集成 - 列式数据格式处理                 |\n|    Pydantic     | 数据处理 |                      数据验证和设置管理框架。用于： - 类型安全的配置加载 - API数据验证 - 模型参数验证 - 数据模式定义和验证                       |\n|     loguru      | 日志记录 |                               现代日志工具。用于： - 训练过程日志记录 - 错误跟踪和调试 - 性能监控 - 结构化日志输出                               |\n|   cloudpickle   | 数据处理 |             增强的Python对象序列化工具。用于： - 模型序列化和反序列化 - 复杂Python对象持久化 - 分布式计算中的数据传输 - 中间结果缓存             |\n|     pluggy      | 插件系统 |                       Python插件框架。用于： - 实现可扩展架构 - 管理模型和处理器插件 - 支持自定义组件集成 - 实现模块化设计                       |\n|     joblib      | 并行计算 |                              并行计算支持库。用于： - 数据处理并行化 - CPU密集型任务优化 - 结果缓存 - 并行模型训练                               |\n|      Click      | CLI工具  |                                命令行界面框架。用于： - 构建CLI工具 - 参数解析和验证 - 子命令管理 - 用户交互界面                                 |\n\n## 核心流程图\n\n```mermaid\nsequenceDiagram\n    participant 用户\n    participant 数据连接器\n    participant 数据加载器\n    participant 元数据\n    participant 合成器\n    participant 数据处理器\n    participant 模型\n    participant 评估器\n\n    用户->>数据连接器: 创建连接器()\n    数据连接器-->>数据加载器: 连接器\n    用户->>数据加载器: 加载数据()\n    数据加载器->>元数据: 从数据加载器获取()\n\n    用户->>合成器: 拟合(元数据)\n    合成器->>数据处理器: 转换(数据)\n    数据处理器-->>合成器: 处理后数据\n    合成器->>模型: 拟合(元数据, 处理后数据)\n    模型-->>合成器: 训练后模型\n\n    用户->>合成器: 采样(样本数量)\n    合成器->>模型: 生成()\n    模型-->>合成器: 合成数据\n    合成器->>数据处理器: 反向转换(合成数据)\n    数据处理器-->>合成器: 还原数据\n    合成器-->>用户: 还原数据\n\n    用户->>评估器: 评估(真实数据, 还原数据)\n    评估器-->>用户: 评估结果\n```\n\n## 4+1 架构视图\n\n### 逻辑视图\n\n```mermaid\ngraph TB\n    subgraph 基础设施层[\"基础设施层\"]\n        direction LR\n        Arrow[\"Apache Arrow<br/>数据处理引擎\"]\n        PyTorch[\"PyTorch<br/>深度学习框架\"]\n        Sklearn[\"Sklearn<br/>机器学习框架\"]\n    end\n\n    subgraph 核心[\"SDG核心库\"]\n        direction TB\n\n        subgraph 数据引擎[\"数据引擎层\"]\n            数据加载器[\"数据加载器<br/>数据加载\"]\n            元数据[\"元数据<br/>元数据管理\"]\n            处理后数据[\"处理后数据<br/>统一数据格式\"]\n        end\n\n        subgraph 处理层[\"处理层\"]\n            检查器[\"检查器<br/>数据检查\"]\n            处理器[\"处理器<br/>数据处理\"]\n            转换器[\"转换器<br/>特征转换\"]\n            格式化器[\"格式化器<br/>格式转换\"]\n        end\n\n        合成器[\"合成器<br/>数据合成\"]\n\n        subgraph 模型层[\"模型层\"]\n            传统模型[\"传统模型\"]\n            大语言模型[\"大语言模型\"]\n\n            传统模型 --GAN网络--> CTGAN\n            传统模型 --高斯Copula--> 高斯Copula\n\n            大语言模型 --无数据生成--> 合成[\"数据合成\"]\n        end\n\n        评估器[\"评估器<br/>数据评估\"]\n\n        %% 数据引擎内部关系\n        数据加载器 --加载原始数据--> 处理后数据\n        数据加载器 --提取--> 元数据\n        元数据 --指导处理--> 处理后数据\n\n        %% 处理层和数据引擎关系\n        处理后数据 --输入--> 检查器\n        检查器 --检查结果--> 处理器\n        处理器 --处理后数据--> 转换器\n        转换器 --转换后特征--> 格式化器\n\n        %% 合成器关系\n        元数据 --元数据配置--> 合成器\n        格式化器 --标准化数据--> 合成器\n        合成器 --控制--> 模型层\n        模型层 --生成数据--> 合成器\n\n        %% 评估器关系\n        处理后数据 --原始数据--> 评估器\n        合成器 --合成数据--> 评估器\n    end\n\n    subgraph 插件系统层[\"插件系统层\"]\n        direction LR\n        连接器管理器[\"连接器管理器<br/>数据源管理\"]\n        处理器管理器[\"处理器管理器<br/>处理器管理\"]\n        检查器管理器[\"检查器管理器<br/>检查器管理\"]\n        模型管理器[\"模型管理器<br/>模型管理\"]\n        评估器管理器[\"评估器管理器<br/>评估器管理\"]\n    end\n\n    %% 基础设施和核心依赖关系\n    Arrow -.->|\"提供列存储和计算\"| 数据加载器\n    PyTorch -.->|\"提供深度学习训练\"| CTGAN\n    Sklearn -.->|\"提供概率分布拟合\"| 高斯Copula\n\n    %% 插件系统和核心关系\n    数据加载器 --注册--> 连接器管理器\n    处理器 --注册--> 处理器管理器\n    检查器 --注册--> 检查器管理器\n    模型层 --注册--> 模型管理器\n    评估器 --注册--> 评估器管理器\n```\n\n### 进程视图\n\n```mermaid\nflowchart TB\n    subgraph SDG主流程[SDG主流程]\n        direction TB\n        命令行界面[命令行界面] --> 合成器\n\n        subgraph 数据访问[数据访问层]\n            数据连接器[数据连接器]\n            数据加载器[数据加载器]\n            元数据[元数据]\n            处理后数据[处理后数据]\n            连接器管理器[连接器管理器]\n        end\n\n        subgraph 数据处理[数据处理层]\n            检查器[检查器]\n            处理器[数据处理器]\n            转换器[数据转换器]\n            格式化器[数据格式化器]\n            采样器[数据采样器]\n            处理器管理器[处理器管理器]\n        end\n\n        subgraph 模型层[模型层]\n            模型管理器[模型管理器]\n            CTGAN模型[CTGAN模型]\n            大语言模型[大语言模型]\n        end\n\n        合成器[合成器]\n        评估器[评估器]\n    end\n```\n\n### 开发视图\n\n```mermaid\ngraph TB\n    subgraph SDG包结构[\"SDG包结构\"]\n        direction TB\n\n        subgraph 核心[\"sdgx\"]\n            数据模型[\"data_models<br/>(元数据和数据处理)\"]\n            模型[\"models<br/>(合成模型实现)\"]\n            数据连接器[\"data_connectors<br/>(数据源连接)\"]\n            命令行[\"cli<br/>(命令行接口)\"]\n            工具[\"utils<br/>(工具函数)\"]\n            类型[\"types<br/>(类型定义)\"]\n            异常[\"exceptions<br/>(异常定义)\"]\n        end\n\n        subgraph 依赖[\"核心依赖\"]\n            PyTorch[\"torch>=2<br/>(深度学习)\"]\n            Arrow[\"pyarrow<br/>(数据处理)\"]\n            Sklearn[\"scikit-learn<br/>(机器学习)\"]\n            Pluggy[\"pluggy<br/>(插件系统)\"]\n            Pandas[\"pandas<br/>(数据分析)\"]\n            OpenAI[\"openai>=1.10.0<br/>(大语言模型接口)\"]\n        end\n    end\n\n    %% 核心包依赖关系\n    数据模型 --使用--> 类型\n    数据模型 --使用--> 异常\n    模型 --使用--> 数据模型\n    数据连接器 --使用--> 数据模型\n    命令行 --使用--> 模型\n    命令行 --使用--> 数据连接器\n\n    %% 基础设施依赖关系\n    模型 --深度学习--> PyTorch\n    模型 --大语言模型--> OpenAI\n    数据连接器 --数据处理--> Arrow\n    数据连接器 --数据分析--> Pandas\n    模型 --机器学习--> Sklearn\n```\n\n### 物理视图\n\n```mermaid\ngraph TB\n    subgraph 外部服务节点[外部服务节点]\n        OpenAI[(\"OpenAI API服务<br/>>= 1.10.0\")]\n    end\n\n    subgraph 分发注册节点[分发注册节点]\n        PyPI[(\"PyPI仓库<br/>sdgx包\")]\n        Docker[(\"Docker仓库<br/>idsteam/sdgx\")]\n    end\n\n    subgraph 计算节点[部署节点]\n        subgraph 容器[Docker容器]\n            SDGX1[\"SDGX服务\"]\n        end\n\n        subgraph Python环境[Python运行环境]\n            SDGX2[\"SDGX包\"]\n            PyTorch[\"PyTorch >= 2.0\"]\n            Arrow[\"Apache Arrow\"]\n        end\n    end\n\n    %% 部署关系\n    PyPI -->|\"pip安装\"| Python环境\n    Docker -->|\"docker拉取\"| 容器\n    OpenAI -->|\"API\"| SDGX1\n    OpenAI -->|\"API\"| SDGX2\n\n    %% 运行时依赖\n    SDGX2 --> PyTorch\n    SDGX2 --> Arrow\n```\n\n### 场景视图\n\n```mermaid\nsequenceDiagram\n    participant 用户\n    participant 数据连接器\n    participant 元数据\n    participant 模型\n    participant 评估器\n\n    %% 场景1: 数据驱动合成\n    rect rgb(200, 220, 240)\n        Note over 用户,评估器: 场景1: 数据驱动合成\n        用户->>数据连接器: 加载原始数据\n        数据连接器->>元数据: 自动检测元数据\n        元数据->>模型: 配置模型\n        模型->>模型: 训练和生成\n        模型->>评估器: 评估合成数据\n        评估器->>用户: 返回评估结果\n    end\n\n    %% 场景2: 基于大语言模型的无数据合成\n    rect rgb(220, 240, 200)\n        Note over 用户,评估器: 场景2: 基于大语言模型的无数据合成\n        用户->>元数据: 定义元数据\n        元数据->>模型: 配置大语言模型\n        模型->>模型: 生成合成数据\n        模型->>用户: 返回生成结果\n    end\n```\n\n# 开发指南\n\n## 克隆项目\n\n```sh\ngit clone https://github.com/hitsz-ids/synthetic-data-generator.git\n```\n\n## 创建 Python 环境\n\n基于[miniconda](https://docs.anaconda.com/miniconda/) 创建环境。\n\n```sh\nconda create -n sdg python=3.11\nconda activate sdg\n```\n\n## 代码风格和检查\n\n我们使用 [black](https://github.com/psf/black) 作为代码格式化工具，最佳使用方式是安装 pre-commit 钩子，它会在每次提交前自动格式化代码。\n\n在提交前安装 pre-commit\n\n```bash\npip install pre-commit\npre-commit install\n```\n\nPre-commit 会在每次提交前自动格式化代码，也可以手动对所有文件执行格式化\n\n```bash\npre-commit run --all-files\n```\n\n注释风格遵循 [Google Python风格指南](https://google.github.io/styleguide/pyguide.html#38-comments-and-docstrings)。\n\n## 本地安装\n\n```sh\npip install -e '.[test,docs]'\n```\n\n## 单元测试\n\n我们使用 pytest 编写单元测试，使用 pytest-cov 生成覆盖率报告\n\n```bash\npytest -vv --cov-config=.coveragerc --cov=sdgx/ tests\n```\n\n在提交PR前请运行单元测试，**确保新功能已被单元测试覆盖**\n\n注意，测试过程中会从 GitHub 下载测试数据，因此最好**开启全局代理**，以保证网络连通性\n\n## 构建文档\n\n安装文档依赖\n\n```bash\npip install -e .[docs]\n```\n\n构建文档\n\n```bash\ncd docs && make html\n```\n\n使用 [start-docs-host.sh](dev-tools/start-docs-host.sh) 部署本地HTTP服务器来查看文档\n\n```sh\ncd ./dev-tools && ./start-docs-host.sh\n```\n\n访问 `http://localhost:8910` 查看文档。\n\n## 开始了解功能\n\n在理解本章概述中提到的所有内容后，我们建议从 SDG 功能开始进行下一步了解。你可以探索 `tests/` 包下的所有内容，使用大语言模型聊天工具（如 cursor ）来添加测试和被测试类，从而深入了解你想要理解的详细功能。\n\n这里我们为大语言模型聊天工具提供一个系统角色提示，以帮助你提出好的问题。\n\n```sh\n请详细解释以下 Python 类的逻辑和实现。除了查看该类的代码外，你还需要查看被测试主体类的代码：\n\n1. 描述该类的整体功能和目的，包括被测试的主体、其功能和基本使用逻辑。\n2. 分析类中的每个方法，解释其功能和参数。\n3. 解释类中使用的任何重要算法或设计模式。\n```\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 11.0732421875,
          "content": "                                 Apache License\n                           Version 2.0, January 2004\n                        http://www.apache.org/licenses/\n\n   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n\n   1. Definitions.\n\n      \"License\" shall mean the terms and conditions for use, reproduction,\n      and distribution as defined by Sections 1 through 9 of this document.\n\n      \"Licensor\" shall mean the copyright owner or entity authorized by\n      the copyright owner that is granting the License.\n\n      \"Legal Entity\" shall mean the union of the acting entity and all\n      other entities that control, are controlled by, or are under common\n      control with that entity. For the purposes of this definition,\n      \"control\" means (i) the power, direct or indirect, to cause the\n      direction or management of such entity, whether by contract or\n      otherwise, or (ii) ownership of fifty percent (50%) or more of the\n      outstanding shares, or (iii) beneficial ownership of such entity.\n\n      \"You\" (or \"Your\") shall mean an individual or Legal Entity\n      exercising permissions granted by this License.\n\n      \"Source\" form shall mean the preferred form for making modifications,\n      including but not limited to software source code, documentation\n      source, and configuration files.\n\n      \"Object\" form shall mean any form resulting from mechanical\n      transformation or translation of a Source form, including but\n      not limited to compiled object code, generated documentation,\n      and conversions to other media types.\n\n      \"Work\" shall mean the work of authorship, whether in Source or\n      Object form, made available under the License, as indicated by a\n      copyright notice that is included in or attached to the work\n      (an example is provided in the Appendix below).\n\n      \"Derivative Works\" shall mean any work, whether in Source or Object\n      form, that is based on (or derived from) the Work and for which the\n      editorial revisions, annotations, elaborations, or other modifications\n      represent, as a whole, an original work of authorship. For the purposes\n      of this License, Derivative Works shall not include works that remain\n      separable from, or merely link (or bind by name) to the interfaces of,\n      the Work and Derivative Works thereof.\n\n      \"Contribution\" shall mean any work of authorship, including\n      the original version of the Work and any modifications or additions\n      to that Work or Derivative Works thereof, that is intentionally\n      submitted to Licensor for inclusion in the Work by the copyright owner\n      or by an individual or Legal Entity authorized to submit on behalf of\n      the copyright owner. For the purposes of this definition, \"submitted\"\n      means any form of electronic, verbal, or written communication sent\n      to the Licensor or its representatives, including but not limited to\n      communication on electronic mailing lists, source code control systems,\n      and issue tracking systems that are managed by, or on behalf of, the\n      Licensor for the purpose of discussing and improving the Work, but\n      excluding communication that is conspicuously marked or otherwise\n      designated in writing by the copyright owner as \"Not a Contribution.\"\n\n      \"Contributor\" shall mean Licensor and any individual or Legal Entity\n      on behalf of whom a Contribution has been received by Licensor and\n      subsequently incorporated within the Work.\n\n   2. Grant of Copyright License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      copyright license to reproduce, prepare Derivative Works of,\n      publicly display, publicly perform, sublicense, and distribute the\n      Work and such Derivative Works in Source or Object form.\n\n   3. Grant of Patent License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      (except as stated in this section) patent license to make, have made,\n      use, offer to sell, sell, import, and otherwise transfer the Work,\n      where such license applies only to those patent claims licensable\n      by such Contributor that are necessarily infringed by their\n      Contribution(s) alone or by combination of their Contribution(s)\n      with the Work to which such Contribution(s) was submitted. If You\n      institute patent litigation against any entity (including a\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\n      or a Contribution incorporated within the Work constitutes direct\n      or contributory patent infringement, then any patent licenses\n      granted to You under this License for that Work shall terminate\n      as of the date such litigation is filed.\n\n   4. Redistribution. You may reproduce and distribute copies of the\n      Work or Derivative Works thereof in any medium, with or without\n      modifications, and in Source or Object form, provided that You\n      meet the following conditions:\n\n      (a) You must give any other recipients of the Work or\n          Derivative Works a copy of this License; and\n\n      (b) You must cause any modified files to carry prominent notices\n          stating that You changed the files; and\n\n      (c) You must retain, in the Source form of any Derivative Works\n          that You distribute, all copyright, patent, trademark, and\n          attribution notices from the Source form of the Work,\n          excluding those notices that do not pertain to any part of\n          the Derivative Works; and\n\n      (d) If the Work includes a \"NOTICE\" text file as part of its\n          distribution, then any Derivative Works that You distribute must\n          include a readable copy of the attribution notices contained\n          within such NOTICE file, excluding those notices that do not\n          pertain to any part of the Derivative Works, in at least one\n          of the following places: within a NOTICE text file distributed\n          as part of the Derivative Works; within the Source form or\n          documentation, if provided along with the Derivative Works; or,\n          within a display generated by the Derivative Works, if and\n          wherever such third-party notices normally appear. The contents\n          of the NOTICE file are for informational purposes only and\n          do not modify the License. You may add Your own attribution\n          notices within Derivative Works that You distribute, alongside\n          or as an addendum to the NOTICE text from the Work, provided\n          that such additional attribution notices cannot be construed\n          as modifying the License.\n\n      You may add Your own copyright statement to Your modifications and\n      may provide additional or different license terms and conditions\n      for use, reproduction, or distribution of Your modifications, or\n      for any such Derivative Works as a whole, provided Your use,\n      reproduction, and distribution of the Work otherwise complies with\n      the conditions stated in this License.\n\n   5. Submission of Contributions. Unless You explicitly state otherwise,\n      any Contribution intentionally submitted for inclusion in the Work\n      by You to the Licensor shall be under the terms and conditions of\n      this License, without any additional terms or conditions.\n      Notwithstanding the above, nothing herein shall supersede or modify\n      the terms of any separate license agreement you may have executed\n      with Licensor regarding such Contributions.\n\n   6. Trademarks. This License does not grant permission to use the trade\n      names, trademarks, service marks, or product names of the Licensor,\n      except as required for reasonable and customary use in describing the\n      origin of the Work and reproducing the content of the NOTICE file.\n\n   7. Disclaimer of Warranty. Unless required by applicable law or\n      agreed to in writing, Licensor provides the Work (and each\n      Contributor provides its Contributions) on an \"AS IS\" BASIS,\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n      implied, including, without limitation, any warranties or conditions\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n      PARTICULAR PURPOSE. You are solely responsible for determining the\n      appropriateness of using or redistributing the Work and assume any\n      risks associated with Your exercise of permissions under this License.\n\n   8. Limitation of Liability. In no event and under no legal theory,\n      whether in tort (including negligence), contract, or otherwise,\n      unless required by applicable law (such as deliberate and grossly\n      negligent acts) or agreed to in writing, shall any Contributor be\n      liable to You for damages, including any direct, indirect, special,\n      incidental, or consequential damages of any character arising as a\n      result of this License or out of the use or inability to use the\n      Work (including but not limited to damages for loss of goodwill,\n      work stoppage, computer failure or malfunction, or any and all\n      other commercial damages or losses), even if such Contributor\n      has been advised of the possibility of such damages.\n\n   9. Accepting Warranty or Additional Liability. While redistributing\n      the Work or Derivative Works thereof, You may choose to offer,\n      and charge a fee for, acceptance of support, warranty, indemnity,\n      or other liability obligations and/or rights consistent with this\n      License. However, in accepting such obligations, You may act only\n      on Your own behalf and on Your sole responsibility, not on behalf\n      of any other Contributor, and only if You agree to indemnify,\n      defend, and hold each Contributor harmless for any liability\n      incurred by, or claims asserted against, such Contributor by reason\n      of your accepting any such warranty or additional liability.\n\n   END OF TERMS AND CONDITIONS\n\n   APPENDIX: How to apply the Apache License to your work.\n\n      To apply the Apache License to your work, attach the following\n      boilerplate notice, with the fields enclosed by brackets \"[]\"\n      replaced with your own identifying information. (Don't include\n      the brackets!)  The text should be enclosed in the appropriate\n      comment syntax for the file format. We also recommend that a\n      file or class name and description of purpose be included on the\n      same \"printed page\" as the copyright notice for easier\n      identification within third-party archives.\n\n   Copyright 2024 hitsz-ids\n\n   Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.\n"
        },
        {
          "name": "NOTICE",
          "type": "blob",
          "size": 0.228515625,
          "content": "sdgx/models/components/sdv_* is released under MIT License:\n- sdv_copulas: https://github.com/sdv-dev/copulas/tree/v0.7.0\n- sdv_ctgan: https://github.com/sdv-dev/CTGAN/tree/v0.6.0\n- sdv_rdt: https://github.com/sdv-dev/rdt/tree/v1.2.1\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 13.4287109375,
          "content": "<div align=\"center\">\n  <img src=\"assets/sdg_logo.png\" width=\"400\" >\n</div>\n<div align=\"center\">\n<p align=\"center\">\n\n<p align=\"center\">\n<a href=\"https://github.com/hitsz-ids/synthetic-data-generator/actions\"><img alt=\"Actions Status\" src=\"https://github.com/hitsz-ids/synthetic-data-generator/actions/workflows/ci-test-python-package.yml/badge.svg\"></a>\n<a href='https://synthetic-data-generator.readthedocs.io/en/latest/?badge=latest'><img src='https://readthedocs.org/projects/synthetic-data-generator/badge/?version=latest' alt='Documentation Status' /></a>\n<a href=\"https://results.pre-commit.ci/latest/github/hitsz-ids/synthetic-data-generator/main\"><img alt=\"pre-commit.ci status\" src=\"https://results.pre-commit.ci/badge/github/hitsz-ids/synthetic-data-generator/main.svg\"></a>\n<a href=\"https://github.com/hitsz-ids/synthetic-data-generator/blob/main/LICENSE\"><img alt=\"LICENSE\" src=\"https://img.shields.io/github/license/hitsz-ids/synthetic-data-generator\"></a>\n<a href=\"https://github.com/hitsz-ids/synthetic-data-generator/releases/\"><img alt=\"Releases\" src=\"https://img.shields.io/github/v/release/hitsz-ids/synthetic-data-generator\"></a>\n<a href=\"https://github.com/hitsz-ids/synthetic-data-generator/releases/\"><img alt=\"Pre Releases\" src=\"https://img.shields.io/github/v/release/hitsz-ids/synthetic-data-generator?include_prereleases&label=pre-release&logo=github\"></a>\n<a href=\"https://github.com/hitsz-ids/synthetic-data-generator\"><img alt=\"Last Commit\" src=\"https://img.shields.io/github/last-commit/hitsz-ids/synthetic-data-generator\"></a>\n<a href=\"https://github.com/hitsz-ids/synthetic-data-generator\"><img alt=\"Python version\" src=\"https://img.shields.io/pypi/pyversions/sdgx\"></a>\n<a href=\"https://github.com/hitsz-ids/synthetic-data-generator/contributors\"><img alt=\"contributors\" src=\"https://img.shields.io/github/all-contributors/hitsz-ids/synthetic-data-generator?color=ee8449&style=flat-square\"></a>\n<a href=\"https://join.slack.com/t/hitsz-ids/shared_invite/zt-2395mt6x2-dwf0j_423QkAgGvlNA5E1g\"><img alt=\"slack\" src=\"https://img.shields.io/badge/slack-join%20chat-ff69b4.svg?style=flat-square\"></a>\n</p>\n\n# 🚀 Synthetic Data Generator\n\n<p style=\"font-size: small;\">Switch Language:\n    <a href=\"https://github.com/hitsz-ids/synthetic-data-generator/blob/main/README_ZH_CN.md\" target=\"_blank\">简体中文</a> &nbsp;| &nbsp;\n    Latest <a href=\"https://synthetic-data-generator.readthedocs.io/en/latest/\" target=\"value\">API Docs</a> &nbsp;| &nbsp;\n    <a href=\"ROADMAP.md\" target=\"value\">Roadmap</a> &nbsp;| &nbsp;\n    Join <a href=\"assets/live_QR_code.jpg\" target=\"value\">Wechat Group</a>\n</p>\n\n<p style=\"font-size: small;\">\n    Colab Examples:&nbsp;\n    <a href=\"https://colab.research.google.com/drive/1VFnP59q3eoVtMJ1PvcYjmuXtx9N8C7o0?usp=sharing\" target=\"value\"> LLM: Data Synthesis</a>\n    &nbsp;| &nbsp;\n    <a href=\"https://colab.research.google.com/drive/1_chuTVZECpj5fklj-RAp7ZVrew8weLW_?usp=sharing\" target=\"value\"> LLM: Off-Table Inference</a>\n    &nbsp;| &nbsp;\n    <a href=\"https://colab.research.google.com/drive/1cMB336jN3kb-m_pr1aJjshnNep_6bhsf?usp=sharing\" target=\"value\"> Billion-Level-Data supported CTGAN</a>\n</p>\n\n</p>\n</div>\n\nThe Synthetic Data Generator (SDG) is a specialized framework designed to generate high-quality structured tabular data.\n\nSynthetic data does not contain any sensitive information, yet it retains the essential characteristics of the original data, making it exempt from privacy regulations such as GDPR and ADPPA.\n\nHigh-quality synthetic data can be safely utilized across various domains including data sharing, model training and debugging, system development and testing, etc.\n\nWe are excited to have you here and look forward to your contributions, get started with the project through this [Contributing Overview Guide](CONTRIBUTING.md)!\n\n## 💥News\n\nOur current key achievements and timelines are as follows:\n\n🔥 Nov 21, 2024: 1) Model Integration - We've integrated the `GaussianCopula` model into our Data Processor System. Check out the code example in this [PR](https://github.com/hitsz-ids/synthetic-data-generator/pull/241); 2) Synthetic Quality - We implemented automatic detection of data column relationships and allowed for relationship specification, improved the quality of synthetic data([Code Example](https://synthetic-data-generator.readthedocs.io/en/latest/user_guides/single_table_column_combinations.html)); 3) Performance Enhancement - We significantly reduced the memory usage of GaussianCopula when handling discrete data, enabling training on thousands of categorical data entries with a `2C4G` setup!\n\n🔥 May 30, 2024: The Data Processor module was officially merged. This module will: 1) help SDG convert the format of some data columns (such as Datetime columns) before feeded into the model (so as to avoid being treated as discrete types), and reversely convert the model-generated data into the original format; 2) perform more customized pre-processing and post-processing on various data types; 3) easily deal with problems such as null values ​​in the original data; 4) support the plug-in system.\n\n🔥 Feb 20, 2024: a single-table data synthesis model based on LLM is included, view colab example: <a href=\"https://colab.research.google.com/drive/1VFnP59q3eoVtMJ1PvcYjmuXtx9N8C7o0?usp=sharing\" target=\"value\"> LLM: Data Synthesis</a> and <a href=\"https://colab.research.google.com/drive/1_chuTVZECpj5fklj-RAp7ZVrew8weLW_?usp=sharing\" target=\"value\"> LLM: Off-table Feature Inference</a>.\n\n🔧 Feb 7, 2024: We improved `sdgx.data_models.metadata` to support metadata information describing for single tables and multiple tables, support multiple data types, support automatic data type inference. view colab example: <a href=\"https://colab.research.google.com/drive/1b4ZTpgSYjOt7ekp1Wj8CxDknbOHEwA7s?usp=sharing\" target=\"value\">SDG Single-Table Metadata</a>。\n\n🔶 Dec 20, 2023: v0.1.0 released, a CTGAN model that supports billions of data processing capabilities is included, view our <a href=\"https://github.com/hitsz-ids/synthetic-data-generator/tree/main/benchmarks#results\" target=\"value\"> benchmark against SDV</a>, where SDG achieved less memory consumption and avoided crashing during training. For specific use, view colab example: <a href=\"https://colab.research.google.com/drive/1cMB336jN3kb-m_pr1aJjshnNep_6bhsf?usp=sharing\" target=\"value\"> Billion-Level-Data supported CTGAN</a>.\n\n🔆 Aug 10, 2023: First line of SDG code committed.\n\n## 🎉 LLM-integrated synthetic data generation\n\nFor a long time, LLM has been used to understand and generate various types of data. In fact, LLM also has certain capabilities in tabular data generation. Also, it has some abilities that cannot be achieved by traditional (based on GAN methods or statistical methods) .\n\nOur `sdgx.models.LLM.single_table.gpt.SingleTableGPTModel` implements two new features:\n\n### Synthetic data generation without Data\n\nNo training data is required, synthetic data can be generated based on metadata data, view in our <a href=\"https://colab.research.google.com/drive/1VFnP59q3eoVtMJ1PvcYjmuXtx9N8C7o0?usp=sharing\" target=\"value\"> colab example</a>.\n\n![Synthetic data generation without Data](assets/LLM_Case_1.gif)\n\n### Off-Table feature inference\n\nInfer new column data based on the existing data in the table and the knowledge mastered by LLM, view in our <a href=\"https://colab.research.google.com/drive/1_chuTVZECpj5fklj-RAp7ZVrew8weLW_?usp=sharing\" target=\"value\"> colab example</a>.\n\n![Off-Table feature inference](assets/LLM_Case_2.gif)\n\n## 💫 Why SDG ?\n\n- Technological advancements:\n  - Supports a wide range of statistical data synthesis algorithms, LLM-based synthetic data generation model is also integrated;\n  - Optimized for big data, effectively reducing memory consumption;\n  - Continuously tracking the latest advances in academia and industry, and introducing support for excellent algorithms and models in a timely manner.\n- Privacy enhancements:\n  - SDG supports differential privacy, anonymization and other methods to enhance the security of synthetic data.\n- Easy to extend:\n  - Supports expansion of models, data processing, data connectors, etc. in the form of plug-in packages.\n\n## 🌀 Quick Start\n\n### Pre-build image\n\nYou can use pre-built images to quickly experience the latest features.\n\n```bash\ndocker pull idsteam/sdgx:latest\n```\n\n### Install from PyPi\n\n```bash\npip install sdgx\n```\n\n### Local Install (Recommended)\n\nUse SDG by installing it through the source code.\n\n```bash\ngit clone git@github.com:hitsz-ids/synthetic-data-generator.git\npip install .\n# Or install from git\npip install git+https://github.com/hitsz-ids/synthetic-data-generator.git\n```\n\n### Quick Demo of Single Table Data Generation and Metric\n\n#### Demo code\n\n```python\nfrom sdgx.data_connectors.csv_connector import CsvConnector\nfrom sdgx.models.ml.single_table.ctgan import CTGANSynthesizerModel\nfrom sdgx.synthesizer import Synthesizer\nfrom sdgx.utils import download_demo_data\n\n# This will download demo data to ./dataset\ndataset_csv = download_demo_data()\n\n# Create data connector for csv file\ndata_connector = CsvConnector(path=dataset_csv)\n\n# Initialize synthesizer, use CTGAN model\nsynthesizer = Synthesizer(\n    model=CTGANSynthesizerModel(epochs=1),  # For quick demo\n    data_connector=data_connector,\n)\n\n# Fit the model\nsynthesizer.fit()\n\n# Sample\nsampled_data = synthesizer.sample(1000)\nprint(sampled_data)\n```\n\n#### Comparison\n\nReal data are as follows：\n\n```python\n>>> data_connector.read()\n       age         workclass  fnlwgt  education  ...  capitalloss hoursperweek native-country  class\n0        2         State-gov   77516  Bachelors  ...            0            2  United-States  <=50K\n1        3  Self-emp-not-inc   83311  Bachelors  ...            0            0  United-States  <=50K\n2        2           Private  215646    HS-grad  ...            0            2  United-States  <=50K\n3        3           Private  234721       11th  ...            0            2  United-States  <=50K\n4        1           Private  338409  Bachelors  ...            0            2           Cuba  <=50K\n...    ...               ...     ...        ...  ...          ...          ...            ...    ...\n48837    2           Private  215419  Bachelors  ...            0            2  United-States  <=50K\n48838    4               NaN  321403    HS-grad  ...            0            2  United-States  <=50K\n48839    2           Private  374983  Bachelors  ...            0            3  United-States  <=50K\n48840    2           Private   83891  Bachelors  ...            0            2  United-States  <=50K\n48841    1      Self-emp-inc  182148  Bachelors  ...            0            3  United-States   >50K\n\n[48842 rows x 15 columns]\n\n```\n\nSynthetic data are as follows：\n\n```python\n>>> sampled_data\n     age workclass  fnlwgt     education  ...  capitalloss hoursperweek native-country  class\n0      1       NaN   28219  Some-college  ...            0            2    Puerto-Rico  <=50K\n1      2   Private  250166       HS-grad  ...            0            2  United-States   >50K\n2      2   Private   50304       HS-grad  ...            0            2  United-States  <=50K\n3      4   Private   89318     Bachelors  ...            0            2    Puerto-Rico   >50K\n4      1   Private  172149     Bachelors  ...            0            3  United-States  <=50K\n..   ...       ...     ...           ...  ...          ...          ...            ...    ...\n995    2       NaN  208938     Bachelors  ...            0            1  United-States  <=50K\n996    2   Private  166416     Bachelors  ...            2            2  United-States  <=50K\n997    2       NaN  336022       HS-grad  ...            0            1  United-States  <=50K\n998    3   Private  198051       Masters  ...            0            2  United-States   >50K\n999    1       NaN   41973       HS-grad  ...            0            2  United-States  <=50K\n\n[1000 rows x 15 columns]\n```\n\n## 👩‍🎓 Related Work\n\n- CTGAN：[Modeling Tabular Data using Conditional GAN](https://proceedings.neurips.cc/paper/2019/hash/254ed7d2de3b23ab10936522dd547b78-Abstract.html)\n- C3-TGAN: [C3-TGAN- Controllable Tabular Data Synthesis with Explicit Correlations and Property Constraints](https://www.researchgate.net/publication/374652636_C3-TGAN-_Controllable_Tabular_Data_Synthesis_with_Explicit_Correlations_and_Property_Constraints)\n- TVAE：[Modeling Tabular Data using Conditional GAN](https://proceedings.neurips.cc/paper/2019/hash/254ed7d2de3b23ab10936522dd547b78-Abstract.html)\n- table-GAN：[Data Synthesis based on Generative Adversarial Networks](https://arxiv.org/pdf/1806.03384.pdf)\n- CTAB-GAN:[CTAB-GAN: Effective Table Data Synthesizing](https://proceedings.mlr.press/v157/zhao21a/zhao21a.pdf)\n- OCT-GAN: [OCT-GAN: Neural ODE-based Conditional Tabular GANs](https://arxiv.org/pdf/2105.14969.pdf)\n\n## 🤝 Join Community\n\nThe SDG project was initiated by **Institute of Data Security, Harbin Institute of Technology**. If you are interested in out project, welcome to join our community. We welcome organizations, teams, and individuals who share our commitment to data protection and security through open source:\n\n- Read [CONTRIBUTING](./CONTRIBUTING.md) before draft a pull request.\n- Submit an issue by viewing [View Good First Issue](https://github.com/hitsz-ids/synthetic-data-generator/labels/good%20first%20issue) or submit a Pull Request.\n- Join our Wechat Group through QR code.\n\n<div align=\"left\">\n  <img src=\"assets/live_QR_code.jpg\" width=\"200\" >\n</div>\n\n## 📄 License\n\nThe SDG open source project uses Apache-2.0 license, please refer to the [LICENSE](https://github.com/hitsz-ids/synthetic-data-generator/blob/main/LICENSE).\n"
        },
        {
          "name": "README_ZH_CN.md",
          "type": "blob",
          "size": 12.9599609375,
          "content": "<div align=\"center\">\n  <img src=\"assets/sdg_logo.png\" width=\"400\" >\n</div>\n\n<div align=\"center\">\n<p align=\"center\">\n\n<p align=\"center\">\n<a href=\"https://github.com/hitsz-ids/synthetic-data-generator/actions\"><img alt=\"Actions Status\" src=\"https://github.com/hitsz-ids/synthetic-data-generator/actions/workflows/ci-test-python-package.yml/badge.svg\"></a>\n<a href='https://synthetic-data-generator.readthedocs.io/en/latest/?badge=latest'><img src='https://readthedocs.org/projects/synthetic-data-generator/badge/?version=latest' alt='Documentation Status' /></a>\n<a href=\"https://results.pre-commit.ci/latest/github/hitsz-ids/synthetic-data-generator/main\"><img alt=\"pre-commit.ci status\" src=\"https://results.pre-commit.ci/badge/github/hitsz-ids/synthetic-data-generator/main.svg\"></a>\n<a href=\"https://github.com/hitsz-ids/synthetic-data-generator/blob/main/LICENSE\"><img alt=\"LICENSE\" src=\"https://img.shields.io/github/license/hitsz-ids/synthetic-data-generator\"></a>\n<a href=\"https://github.com/hitsz-ids/synthetic-data-generator/releases/\"><img alt=\"Releases\" src=\"https://img.shields.io/github/v/release/hitsz-ids/synthetic-data-generator\"></a>\n<a href=\"https://github.com/hitsz-ids/synthetic-data-generator/releases/\"><img alt=\"Pre Releases\" src=\"https://img.shields.io/github/v/release/hitsz-ids/synthetic-data-generator?include_prereleases&label=pre-release&logo=github\"></a>\n<a href=\"https://github.com/hitsz-ids/synthetic-data-generator\"><img alt=\"Last Commit\" src=\"https://img.shields.io/github/last-commit/hitsz-ids/synthetic-data-generator\"></a>\n<a href=\"https://github.com/hitsz-ids/synthetic-data-generator\"><img alt=\"Python version\" src=\"https://img.shields.io/pypi/pyversions/sdgx\"></a>\n<a href=\"https://github.com/hitsz-ids/synthetic-data-generator/contributors\"><img alt=\"contributors\" src=\"https://img.shields.io/github/all-contributors/hitsz-ids/synthetic-data-generator?color=ee8449&style=flat-square\"></a>\n<a href=\"https://join.slack.com/t/hitsz-ids/shared_invite/zt-2395mt6x2-dwf0j_423QkAgGvlNA5E1g\"><img alt=\"slack\" src=\"https://img.shields.io/badge/slack-join%20chat-ff69b4.svg?style=flat-square\"></a>\n</p>\n\n# 🚀 合成数据生成器 -- 快速生成高质量合成数据！\n\n<p style=\"font-size: small;\">切换语言:\n    <a href=\"https://github.com/hitsz-ids/synthetic-data-generator/blob/main/README.md\" target=\"_blank\">English</a> &nbsp;| &nbsp;最新\n    <a href=\"https://synthetic-data-generator.readthedocs.io/en/latest/\" target=\"value\">API文档</a>&nbsp;| &nbsp;\n     <a href=\"ROADMAP_ZH_CN.md\" target=\"value\">项目发展规划</a> &nbsp;| &nbsp;\n    加入 <a href=\"assets/live_QR_code.jpg\" target=\"value\">Wechat群组</a>\n  </p>\n\n<p style=\"font-size: small;\">\n    查看 Colab 例子:&nbsp;\n    <a href=\"https://colab.research.google.com/drive/1VFnP59q3eoVtMJ1PvcYjmuXtx9N8C7o0?usp=sharing\" target=\"value\"> 使用LLM仿真数据</a>\n    &nbsp;| &nbsp;\n    <a href=\"https://colab.research.google.com/drive/1_chuTVZECpj5fklj-RAp7ZVrew8weLW_?usp=sharing\" target=\"value\"> 借助LLM进行表外特征推断</a>\n    &nbsp;| &nbsp;\n    <a href=\"https://colab.research.google.com/drive/1cMB336jN3kb-m_pr1aJjshnNep_6bhsf?usp=sharing\" target=\"value\">支持十亿级数据的CTGAN</a>\n</p>\n\n</p>\n</div>\n\n合成数据生成器（Synthetic Data Generator，SDG）是一个专注于快速生成高质量的结构化表格数据的数据组件。\n\n合成数据（Synthetic Data）不包含任何敏感信息，但它保留了原始数据的基本特性，使其免于GDPR和ADPPA等隐私法规的约束，消除实际应用中的隐私泄露风险。\n\n高质量的合成数据可以安全、多样化地在各种领域中使用，包括数据共享、模型训练和调试、系统开发和测试等应用。\n\n我们很期待您的贡献，请通过这个[贡献概述指南](CONTRIBUTING_ZH_CN.md)开始项目！\n\n## 💥 相关信息\n\n我们的里程碑和时间节点如下所示：\n\n🔥 2024年11月21日：1) 模型集成 - 现在我们集成 GaussianCopula 模型到我们的 Data Processor 体系，可以查看此 [PR](https://github.com/hitsz-ids/synthetic-data-generator/pull/241) 的代码实例; 2) 合成质量增强 - 我们做了数据列关系的自动检测，同时也提供数据列的关系指定，进一步提高合成数据保真度质量（[代码实例](https://synthetic-data-generator.readthedocs.io/en/latest/user_guides/single_table_column_combinations.html)）; 3) 性能增强 - 我们大大降低 GaussianCopula 处理离散数据的内存占用，使其能在 2C4G 的配置下完成万级别的离散列数据训练！\n\n🔥 2024年5月30日：Data Processor 模块被正式合并，该模块可以：1）可以帮助 SDG 将部分数据列（如 Datetime 类型的列）在送入模型前进行格式转换，从而避免被当作离散类型处理，对模型生成数据反向转换成原有格式；2）对各种数据类型进行更加定制化的预处理和后处理；3）轻松应对原始数据中的空值等问题；4）支持插件系统。\n\n🔥 2024年2月20日：基于LLM的单表数据合成模型已包含，查看Colab示例：<a href=\"https://colab.research.google.com/drive/1VFnP59q3eoVtMJ1PvcYjmuXtx9N8C7o0?usp=sharing\" target=\"value\">LLM：数据合成</a> 和 <a href=\"https://colab.research.google.com/drive/1_chuTVZECpj5fklj-RAp7ZVrew8weLW_?usp=sharing\" target=\"value\">LLM：表外特征推断</a>。\n\n🔧 2024年2月7日：SDG团队完善了 `sdgx.data_models.metadata`，支持描述单表、多表的元数据信息，支持多种数据类型，支持数据类型的自动推断能力，查看Colab示例：<a href=\"https://colab.research.google.com/drive/1b4ZTpgSYjOt7ekp1Wj8CxDknbOHEwA7s?usp=sharing\" target=\"value\">SDG Single-Table Metadata</a>。\n\n🔶 2023年12月20日：v0.1.0发布，包含支持数十亿数据处理能力的CTGAN模型，查看我们的<a href=\"https://github.com/hitsz-ids/synthetic-data-generator/tree/ main/benchmarks#results\" target=\"value\"> 针对 SDV 的基准</a>，SDG 实现了更少的内存消耗并避免了训练期间的崩溃（Out of Memory），具体使用，请查看我们的Colab示例：<a href=\"https://colab.research.google.com/drive/1cMB336jN3kb-m_pr1aJjshnNep_6bhsf?usp=sharing\" target=\"value\">支持十亿级数据的CTGAN</a>。\n\n🔆 2023年8月10日：第一行SDG代码提交。\n\n## 🎉 借助LLM进行合成数据生成\n\n长期以来，LLM一直被用来理解和生成各种类型的数据。 事实上，LLM在表格数据生成方面也有较强的性能。 且LLM还具有一些传统（基于GAN方法或统计方法）无法实现的能力。\n\n我们的 `sdgx.models.LLM.single_table.gpt.SingleTableGPTModel` 实现了两个新功能：\n\n### 无原始记录的数据合成功能\n\n无需原始训练数据，可以根据元数据生成合成数据，查看 <a href=\"https://colab.research.google.com/drive/1VFnP59q3eoVtMJ1PvcYjmuXtx9N8C7o0?usp=sharing\" target=\"value\"> Colab 例子</a>。\n\n![Synthetic data generation without Data](assets/LLM_Case_1.gif)\n\n### 表外特征推断功能\n\n根据表中已有的数据以及LLM掌握的知识推断表外特征，即新的列数据，查看 <a href=\"https://colab.research.google.com/drive/1_chuTVZECpj5fklj-RAp7ZVrew8weLW_?usp=sharing\" target=\"value\"> Colab 例子</a>。\n\n![Off-Table feature inference](assets/LLM_Case_2.gif)\n\n## 💫 Why SDG ?\n\n- 无限进步：\n  - 支持多种统计学数据合成算法，支持基于LLM的仿真数据生成方法；\n  - 为大数据场景优化，有效减少内存消耗；\n  - 持续跟踪学术界和工业界的最新进展，及时引入支持优秀算法和模型。\n- 隐私增强：\n  - 提供中文敏感数据自动识别能力，包括姓名、身份证号、人名等17种常见敏感字段；\n  - 支持差分隐私、匿名化等方法，加强合成数据安全性。\n- 易扩展：\n  - 支持以插件包的形式拓展模型、数据处理、数据连接器等功能。\n\n## 🌀 快速开始\n\n### 预构建镜像\n\n您可以使用预构建的镜像来快速体验最新功能。\n\n```bash\ndocker pull idsteam/sdgx:latest\n```\n\n### 从Pypi安装\n\n```bash\npip install sdgx\n```\n\n### 从本地安装\n\n您可以通过源码进行安装的方式使用SDG。\n\n```bash\ngit clone git@github.com:hitsz-ids/synthetic-data-generator.git\npip install .\n# 或者直接从git安装\npip install git+https://github.com/hitsz-ids/synthetic-data-generator.git\n```\n\n### 单表数据快速合成示例\n\n#### 演示代码\n\n```python\nfrom sdgx.data_connectors.csv_connector import CsvConnector\nfrom sdgx.models.ml.single_table.ctgan import CTGANSynthesizerModel\nfrom sdgx.synthesizer import Synthesizer\nfrom sdgx.utils import download_demo_data\n\n# This will download demo data to ./dataset\ndataset_csv = download_demo_data()\n\n# Create data connector for csv file\ndata_connector = CsvConnector(path=dataset_csv)\n\n# Initialize synthesizer, use CTGAN model\nsynthesizer = Synthesizer(\n    model=CTGANSynthesizerModel(epochs=1),  # For quick demo\n    data_connector=data_connector,\n)\n\n# Fit the model\nsynthesizer.fit()\n\n# Sample\nsampled_data = synthesizer.sample(1000)\nprint(sampled_data)\n```\n\n#### 对比\n\n真实数据：\n\n```python\n>>> data_connector.read()\n       age         workclass  fnlwgt  education  ...  capitalloss hoursperweek native-country  class\n0        2         State-gov   77516  Bachelors  ...            0            2  United-States  <=50K\n1        3  Self-emp-not-inc   83311  Bachelors  ...            0            0  United-States  <=50K\n2        2           Private  215646    HS-grad  ...            0            2  United-States  <=50K\n3        3           Private  234721       11th  ...            0            2  United-States  <=50K\n4        1           Private  338409  Bachelors  ...            0            2           Cuba  <=50K\n...    ...               ...     ...        ...  ...          ...          ...            ...    ...\n48837    2           Private  215419  Bachelors  ...            0            2  United-States  <=50K\n48838    4               NaN  321403    HS-grad  ...            0            2  United-States  <=50K\n48839    2           Private  374983  Bachelors  ...            0            3  United-States  <=50K\n48840    2           Private   83891  Bachelors  ...            0            2  United-States  <=50K\n48841    1      Self-emp-inc  182148  Bachelors  ...            0            3  United-States   >50K\n\n[48842 rows x 15 columns]\n\n```\n\n仿真数据：\n\n```python\n>>> sampled_data\n     age workclass  fnlwgt     education  ...  capitalloss hoursperweek native-country  class\n0      1       NaN   28219  Some-college  ...            0            2    Puerto-Rico  <=50K\n1      2   Private  250166       HS-grad  ...            0            2  United-States   >50K\n2      2   Private   50304       HS-grad  ...            0            2  United-States  <=50K\n3      4   Private   89318     Bachelors  ...            0            2    Puerto-Rico   >50K\n4      1   Private  172149     Bachelors  ...            0            3  United-States  <=50K\n..   ...       ...     ...           ...  ...          ...          ...            ...    ...\n995    2       NaN  208938     Bachelors  ...            0            1  United-States  <=50K\n996    2   Private  166416     Bachelors  ...            2            2  United-States  <=50K\n997    2       NaN  336022       HS-grad  ...            0            1  United-States  <=50K\n998    3   Private  198051       Masters  ...            0            2  United-States   >50K\n999    1       NaN   41973       HS-grad  ...            0            2  United-States  <=50K\n\n[1000 rows x 15 columns]\n```\n\n## 👩‍🎓 相关工作\n\n### 论文\n\n- CTGAN：[Modeling Tabular Data using Conditional GAN](https://proceedings.neurips.cc/paper/2019/hash/254ed7d2de3b23ab10936522dd547b78-Abstract.html)\n- C3-TGAN: [C3-TGAN- Controllable Tabular Data Synthesis with Explicit Correlations and Property Constraints](https://www.researchgate.net/publication/374652636_C3-TGAN-_Controllable_Tabular_Data_Synthesis_with_Explicit_Correlations_and_Property_Constraints)\n- TVAE：[Modeling Tabular Data using Conditional GAN](https://proceedings.neurips.cc/paper/2019/hash/254ed7d2de3b23ab10936522dd547b78-Abstract.html)\n- table-GAN：[Data Synthesis based on Generative Adversarial Networks](https://arxiv.org/pdf/1806.03384.pdf)\n- CTAB-GAN:[CTAB-GAN: Effective Table Data Synthesizing](https://proceedings.mlr.press/v157/zhao21a/zhao21a.pdf)\n- OCT-GAN: [OCT-GAN: Neural ODE-based Conditional Tabular GANs](https://arxiv.org/pdf/2105.14969.pdf)\n\n## 🤝 如何贡献\n\nSDG开源项目由**哈尔滨工业大学（深圳）数据安全研究院**发起，若您对SDG项目感兴趣并愿意一起完善它，欢迎加入我们的开源社区：\n\n- 非常欢迎你的加入！[查看 Good First Issue](https://github.com/hitsz-ids/synthetic-data-generator/labels/good%20first%20issue) 或者提交一个 Pull Request。\n- 开发环境配置请参考[开发者文档](./CONTRIBUTING.md)\n- 加入微信群：\n\n<div align=\"left\">\n  <img src=\"assets/live_QR_code.jpg\" width=\"400\" >\n</div>\n\n## 📄 许可证\n\nSDG开源项目使用 Apache-2.0 license，有关协议请参考[LICENSE](https://github.com/hitsz-ids/synthetic-data-generator/blob/main/LICENSE)。\n"
        },
        {
          "name": "ROADMAP.md",
          "type": "blob",
          "size": 1.40234375,
          "content": "# 📝 TODO\n\n- [ ] **Feature**| Support more single-table data synthetic models\n  - [ ] OCT-GAN Model\n  - [ ] CTAB-GAN Model\n  - [ ] Table-GAN Model\n\n# 🚧 Doing\n\n- [ ] **Feature**| Support multi-table data synthesis\n- [ ] **Feature**| Support more single-table data synthetic models - TVAE Model\n\n# ✅ Done\n\n- [x] **Enhance**| Hundred Thousand Level Discrete Data supported Gaussian Copula\n- [x] **Docs**| Improved CONTRIBUTING documentation with 4+1 view and overview diagram\n- [x] **Enhance**| Automatic identification and handling of fixed relationships\n- [x] **Feature**| Support for single-table data synthesis based on CTGAN\n- [x] **Feature**| Support for single-table data synthesis based on GaussianCopula\n- [x] **Feature**| Support for single-table data synthesis without real data based on LLM\n- [x] **Enhance**| Billion-Level-Data supported CTGAN\n- [x] **Enhance**| Numeric Data Inspection and introduce positive/negative filtering\n- [x] **Enhance**| Add NonValueTransformer reverse conversion with NAN_VALUE replacement\n- [x] **Enhance**| Add ConstInspector and ConstValueTransformer for handling constant columns\n- [x] **Enhance**| Data handling with empty column inspector and transformer\n- [x] **Enhance**| Add ChnPiiGenerator and enhance models\n\n# 🌟 Longterm\n\n- [ ] **Feature**| Establishing an evaluation system for privacy metric\n- [ ] **Feature**| Establishing an evaluation system for synthesis quality metric\n"
        },
        {
          "name": "ROADMAP_ZH_CN.md",
          "type": "blob",
          "size": 1.1728515625,
          "content": "# 📝 待处理\n\n- [ ] **功能**| 支持多表数据合成\n- [ ] **功能**| 支持更多单表数据合成模型\n  - OCT-GAN 模型\n  - CTAB-GAN 模型\n  - Table-GAN 模型\n\n# 🚧 进行中\n\n- [ ] **功能**| 支持多表数据合成\n- [ ] **功能**| 支持更多单表数据合成模型 - TVAE 模型\n\n# ✅ 已完成\n\n- [x] **增强**| 自动识别和处理固定关系\n- [x] **文档**| 改进贡献者导读文档，添加4+1视图和概览图\n- [x] **增强**| 支持十万级离散数据的高斯 Copula 数据合成\n- [x] **功能**| 支持基于CTGAN的单表数据合成\n- [x] **功能**| 支持基于LLM的无真实数据单表数据合成\n- [x] **增强**| 支持十亿级数据的CTGAN\n- [x] **增强**| 数值数据检查并引入正/负过滤\n- [x] **增强**| 添加NonValueTransformer的反向转换及NAN_VALUE替换\n- [x] **增强**| 添加ConstInspector和ConstValueTransformer以处理常量列\n- [x] **增强**| 通过空列检查器和转换器进行数据处理\n- [x] **增强**| 添加ChnPiiGenerator并增强模型处理中文敏感信息\n\n# 🌟 长期计划\n\n- [ ] **功能**| 建立隐私指标的评估系统\n- [ ] **功能**| 建立合成质量指标的评估系统\n"
        },
        {
          "name": "assets",
          "type": "tree",
          "content": null
        },
        {
          "name": "benchmarks",
          "type": "tree",
          "content": null
        },
        {
          "name": "dev-tools",
          "type": "tree",
          "content": null
        },
        {
          "name": "docker",
          "type": "tree",
          "content": null
        },
        {
          "name": "docs",
          "type": "tree",
          "content": null
        },
        {
          "name": "example",
          "type": "tree",
          "content": null
        },
        {
          "name": "pyproject.toml",
          "type": "blob",
          "size": 1.4228515625,
          "content": "[build-system]\nrequires = [\"hatchling\"]\nbuild-backend = \"hatchling.build\"\n\n[project]\nname = \"sdgx\"\ndescription = \"synthetic-data-generator\"\nkeywords = [\"synthetic data\", \"hitsz-ids\"]\nrequires-python = \">=3.9\"\n\ndependencies = [\n    \"pandas\",\n    \"numpy<2\",\n    \"scipy\",\n    \"pyyaml\",\n    'scikit-learn>=0.24,<2',\n    'Faker>=10',\n    \"matplotlib\",\n    \"torch>=2\",\n    \"table-evaluator\",\n    \"click\",\n    \"pluggy\",\n    \"loguru\",\n    \"pyarrow\",\n    \"pydantic>=2\",\n    \"cloudpickle\",\n    \"importlib_metadata\",\n    \"openai>=1.10.0\",\n    \"python-dotenv\",\n    \"joblib>=1.4.0\"\n]\ndynamic = [\"version\"]\nclassifiers = [\n    \"Programming Language :: Python :: 3\",\n    'Programming Language :: Python :: 3.8',\n    'Programming Language :: Python :: 3.9',\n    'Programming Language :: Python :: 3.10',\n    'Programming Language :: Python :: 3.11',\n    'Programming Language :: Python :: 3.12',\n]\n\n[project.optional-dependencies]\ntest = [\"pytest\", \"pytest-cov\", \"coverage<7\", \"email_validator\"]\ndocs = [\n    \"Sphinx\",\n    \"pydata-sphinx-theme\",\n    \"sphinx-click\",\n    \"autodoc_pydantic>=2\",\n]\n\n[project.scripts]\nsdgx = \"sdgx.cli.main:cli\"\n\n[[project.authors]]\nname = \"hitsz-ids\"\n\n[project.readme]\nfile = \"README.md\"\ncontent-type = \"text/markdown\"\n\n[project.license]\ntext = \"Apache Software License 2.0\"\n\n[project.urls]\nSource = \"https://github.com/hitsz-ids/synthetic-data-generator\"\n\n[tool.check-manifest]\nignore = [\".*\"]\n\n[tool.hatch.version]\npath = \"sdgx/__init__.py\"\n"
        },
        {
          "name": "sdgx",
          "type": "tree",
          "content": null
        },
        {
          "name": "sweep.yaml",
          "type": "blob",
          "size": 0.80078125,
          "content": "# Sweep AI turns bug fixes & feature requests into code changes (https://sweep.dev)\n# For details on our config file, check out our docs at https://docs.sweep.dev\n\n# If you use this be sure to frequently sync your default branch(main, master) to dev.\nbranch: \"main\"\n# By default Sweep will read the logs and outputs from your existing Github Actions. To disable this, set this to false.\ngha_enabled: True\n# This is the description of your project. It will be used by sweep when creating PRs. You can tell Sweep what's unique about your project, what frameworks you use, or anything else you want.\n# Here's an example: sweepai/sweep is a python project. The main api endpoints are in sweepai/api.py. Write code that adheres to PEP8.\ndescription: \"\"\n# Default Values: https://github.com/sweepai/sweep/blob/main/sweep.yaml\n"
        },
        {
          "name": "tests",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}