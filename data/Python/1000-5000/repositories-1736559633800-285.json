{
  "metadata": {
    "timestamp": 1736559633800,
    "page": 285,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjI5MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "jingyaogong/minimind",
      "stars": 4305,
      "defaultBranch": "master",
      "files": [
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.0380859375,
          "content": "/model/__pycache__\n/dataset\n/wandb\n/out"
        },
        {
          "name": "0-eval_pretrain.py",
          "type": "blob",
          "size": 4.921875,
          "content": "import random\nimport time\n\nimport numpy as np\nimport torch\nimport warnings\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nfrom model.model import Transformer\nfrom model.LMConfig import LMConfig\n\nwarnings.filterwarnings('ignore')\n\n\ndef count_parameters(model):\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n\n\ndef init_model(lm_config):\n    tokenizer = AutoTokenizer.from_pretrained('./model/minimind_tokenizer')\n    model_from = 1  # 1从权重，2用transformers\n\n    if model_from == 1:\n        moe_path = '_moe' if lm_config.use_moe else ''\n        ckp = f'./out/pretrain_{lm_config.dim}{moe_path}.pth'\n\n        model = Transformer(lm_config)\n        state_dict = torch.load(ckp, map_location=device)\n\n        # 处理不需要的前缀\n        unwanted_prefix = '_orig_mod.'\n        for k, v in list(state_dict.items()):\n            if k.startswith(unwanted_prefix):\n                state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n\n        for k, v in list(state_dict.items()):\n            if 'mask' in k:\n                del state_dict[k]\n\n        # 加载到模型中\n        model.load_state_dict(state_dict, strict=False)\n    else:\n        model = AutoModelForCausalLM.from_pretrained('minimind', trust_remote_code=True)\n    model = model.to(device)\n\n    print(f'模型参数: {count_parameters(model) / 1e6} 百万 = {count_parameters(model) / 1e9} B (Billion)')\n    return model, tokenizer\n\n\ndef setup_seed(seed):\n    random.seed(seed)  # 设置 Python 的随机种子\n    np.random.seed(seed)  # 设置 NumPy 的随机种子\n    torch.manual_seed(seed)  # 设置 PyTorch 的随机种子\n    torch.cuda.manual_seed(seed)  # 为当前 GPU 设置随机种子（如果有）\n    torch.cuda.manual_seed_all(seed)  # 为所有 GPU 设置随机种子（如果有）\n    torch.backends.cudnn.deterministic = True  # 确保每次返回的卷积算法是确定的\n    torch.backends.cudnn.benchmark = False  # 关闭 cuDNN 的自动调优，避免不确定性\n\n\nif __name__ == \"__main__\":\n    # -----------------------------------------------------------------------------\n    out_dir = 'out'\n    start = \"\"\n    temperature = 0.7\n    top_k = 8\n    setup_seed(1337)\n    # device = 'cpu'\n    device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n    dtype = 'bfloat16'\n    max_seq_len = 512\n    lm_config = LMConfig()\n    lm_config.max_seq_len = max_seq_len\n    # -----------------------------------------------------------------------------\n\n    model, tokenizer = init_model(lm_config)\n    model = model.eval()\n    # int(input('输入0自动测试，输入1问题测试：'))\n    answer_way = 0\n    stream = True\n\n    prompt_datas = [\n        '椭圆和圆的区别',\n        '中国关于马克思主义基本原理',\n        '人类大脑的主要功能是',\n        '万有引力是',\n        '世界上人口最多的国家是',\n        'DNA的全称是',\n        '数学中π的值大约是',\n        '世界上最高的山峰是',\n        '太阳系中最大的行星是',\n        '二氧化碳的化学分子式是',\n        '地球上最大的动物是',\n        '地球自转一圈大约需要',\n        '杭州市的美食有',\n        '江苏省的最好的大学',\n    ]\n\n    qa_index = 0\n    while True:\n        start = time.time()\n        if answer_way == 1:\n            # run generation\n            prompt = input('用户：')\n        else:\n            if qa_index >= len(prompt_datas):\n                break\n            prompt = prompt_datas[qa_index]\n            print('问题：', prompt)\n            qa_index += 1\n\n        prompt = tokenizer.bos_token + prompt\n        x = tokenizer(prompt).data['input_ids']\n        x = (torch.tensor(x, dtype=torch.long, device=device)[None, ...])\n\n        with torch.no_grad():\n            res_y = model.generate(x, tokenizer.eos_token_id, max_new_tokens=max_seq_len, temperature=temperature,\n                                   top_k=top_k, stream=stream)\n            print('回答：', end='')\n            try:\n                y = next(res_y)\n            except StopIteration:\n                print(\"No answer\")\n                continue\n\n            history_idx = 0\n            while y != None:\n                answer = tokenizer.decode(y[0].tolist())\n                if answer and answer[-1] == '�':\n                    try:\n                        y = next(res_y)\n                    except:\n                        break\n                    continue\n                # print(answer)\n                if not len(answer):\n                    try:\n                        y = next(res_y)\n                    except:\n                        break\n                    continue\n\n                print(answer[history_idx:], end='', flush=True)\n                try:\n                    y = next(res_y)\n                except:\n                    break\n                history_idx = len(answer)\n                if not stream:\n                    break\n\n            print('\\n')\n\n        end = time.time()\n        print(end - start, 's')\n"
        },
        {
          "name": "1-pretrain.py",
          "type": "blob",
          "size": 7.935546875,
          "content": "import os\nimport platform\nimport argparse\nimport time\nimport math\nimport warnings\n\nimport pandas as pd\nimport torch\nimport torch.distributed as dist\nfrom torch import optim\nfrom torch.nn.parallel import DistributedDataParallel\nfrom torch.optim.lr_scheduler import CosineAnnealingLR\nfrom torch.utils.data import DataLoader, DistributedSampler\nfrom contextlib import nullcontext\n\nfrom transformers import AutoTokenizer\n\nfrom model.model import Transformer\nfrom model.LMConfig import LMConfig\nfrom model.dataset import PretrainDataset\n\nwarnings.filterwarnings('ignore')\n\n\ndef Logger(content):\n    if not ddp or dist.get_rank() == 0:\n        print(content)\n\n\ndef get_lr(it, all):\n    warmup_iters = args.warmup_iters\n    lr_decay_iters = all\n    min_lr = args.learning_rate / 10\n\n    if it < warmup_iters:\n        return args.learning_rate * it / warmup_iters\n    if it > lr_decay_iters:\n        return min_lr\n    decay_ratio = (it - warmup_iters) / (lr_decay_iters - warmup_iters)\n    assert 0 <= decay_ratio <= 1\n    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio))\n    return min_lr + coeff * (args.learning_rate - min_lr)\n\n\ndef train_epoch(epoch, wandb):\n    start_time = time.time()\n    for step, (X, Y, loss_mask) in enumerate(train_loader):\n        X = X.to(args.device)\n        Y = Y.to(args.device)\n        loss_mask = loss_mask.to(args.device)\n\n        lr = get_lr(epoch * iter_per_epoch + step, args.epochs * iter_per_epoch)\n        for param_group in optimizer.param_groups:\n            param_group['lr'] = lr\n\n        with ctx:\n            out = model(X, Y)\n            loss = out.last_loss / args.accumulation_steps\n            loss_mask = loss_mask.view(-1)\n            loss = torch.sum(loss * loss_mask) / loss_mask.sum()\n\n        scaler.scale(loss).backward()\n\n        if (step + 1) % args.accumulation_steps == 0:\n            scaler.unscale_(optimizer)\n            torch.nn.utils.clip_grad_norm_(model.parameters(), args.grad_clip)\n\n            scaler.step(optimizer)\n            scaler.update()\n\n            optimizer.zero_grad(set_to_none=True)\n\n        if step % args.log_interval == 0:\n            spend_time = time.time() - start_time\n            Logger(\n                'Epoch:[{}/{}]({}/{}) loss:{:.3f} lr:{:.7f} epoch_Time:{}min:'.format(\n                    epoch,\n                    args.epochs,\n                    step,\n                    iter_per_epoch,\n                    loss.item() * args.accumulation_steps,\n                    optimizer.param_groups[-1]['lr'],\n                    spend_time / (step + 1) * iter_per_epoch // 60 - spend_time // 60))\n\n            if (wandb is not None) and (not ddp or dist.get_rank() == 0):\n                wandb.log({\"loss\": loss.item() * args.accumulation_steps,\n                           \"lr\": optimizer.param_groups[-1]['lr'],\n                           \"epoch_Time\": spend_time / (step + 1) * iter_per_epoch // 60 - spend_time // 60})\n\n        if (step + 1) % args.save_interval == 0 and (not ddp or dist.get_rank() == 0):\n            model.eval()\n            moe_path = '_moe' if lm_config.use_moe else ''\n            ckp = f'{args.save_dir}/pretrain_{lm_config.dim}{moe_path}.pth'\n\n            if isinstance(model, torch.nn.parallel.DistributedDataParallel):\n                state_dict = model.module.state_dict()\n            else:\n                state_dict = model.state_dict()\n\n            torch.save(state_dict, ckp)\n            model.train()\n\n\ndef init_model():\n    def count_parameters(model):\n        return sum(p.numel() for p in model.parameters() if p.requires_grad)\n\n    tokenizer = AutoTokenizer.from_pretrained('./model/minimind_tokenizer')\n\n    model = Transformer(lm_config).to(args.device)\n    # moe_path = '_moe' if lm_config.use_moe else ''\n\n    Logger(f'LLM总参数量：{count_parameters(model) / 1e6:.3f} 百万')\n    return model, tokenizer\n\n\ndef init_distributed_mode():\n    if not ddp: return\n    global ddp_local_rank, DEVICE\n\n    dist.init_process_group(backend=\"nccl\")\n    ddp_rank = int(os.environ[\"RANK\"])\n    ddp_local_rank = int(os.environ[\"LOCAL_RANK\"])\n    ddp_world_size = int(os.environ[\"WORLD_SIZE\"])\n    DEVICE = f\"cuda:{ddp_local_rank}\"\n    torch.cuda.set_device(DEVICE)\n\n\n# torchrun --nproc_per_node 2 1-pretrain.py\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser(description=\"MiniMind Pretraining\")\n    parser.add_argument(\"--out_dir\", type=str, default=\"out\", help=\"Output directory\")\n    parser.add_argument(\"--epochs\", type=int, default=20, help=\"Number of epochs\")\n    parser.add_argument(\"--batch_size\", type=int, default=64, help=\"Batch size\")\n    parser.add_argument(\"--learning_rate\", type=float, default=2e-4, help=\"Learning rate\")\n    parser.add_argument(\"--device\", type=str, default=\"cuda:0\" if torch.cuda.is_available() else \"cpu\",\n                        help=\"Device to use\")\n    parser.add_argument(\"--dtype\", type=str, default=\"bfloat16\", help=\"Data type\")\n    parser.add_argument(\"--use_wandb\", action=\"store_true\", help=\"Use Weights & Biases\")\n    parser.add_argument(\"--wandb_project\", type=str, default=\"MiniMind-Pretrain\", help=\"Weights & Biases project name\")\n    parser.add_argument(\"--num_workers\", type=int, default=1, help=\"Number of workers for data loading\")\n    parser.add_argument(\"--data_path\", type=str, default=\"./dataset/pretrain_data.csv\", help=\"Path to training data\")\n    parser.add_argument(\"--ddp\", action=\"store_true\", help=\"Use DistributedDataParallel\")\n    parser.add_argument(\"--accumulation_steps\", type=int, default=8, help=\"Gradient accumulation steps\")\n    parser.add_argument(\"--grad_clip\", type=float, default=1.0, help=\"Gradient clipping threshold\")\n    parser.add_argument(\"--warmup_iters\", type=int, default=0, help=\"Number of warmup iterations\")\n    parser.add_argument(\"--log_interval\", type=int, default=100, help=\"Logging interval\")\n    parser.add_argument(\"--save_interval\", type=int, default=1000, help=\"Model saving interval\")\n    parser.add_argument('--local_rank', type=int, default=-1, help='local rank for distributed training')\n\n    args = parser.parse_args()\n\n    lm_config = LMConfig()\n    max_seq_len = lm_config.max_seq_len\n    args.save_dir = os.path.join(args.out_dir)\n    os.makedirs(args.save_dir, exist_ok=True)\n    os.makedirs(args.out_dir, exist_ok=True)\n    tokens_per_iter = args.batch_size * max_seq_len\n    torch.manual_seed(1337)\n    device_type = \"cuda\" if \"cuda\" in args.device else \"cpu\"\n\n    args.wandb_run_name = f\"MiniMind-Pretrain-Epoch-{args.epochs}-BatchSize-{args.batch_size}-LearningRate-{args.learning_rate}\"\n\n    ctx = nullcontext() if device_type == \"cpu\" else torch.cuda.amp.autocast()\n\n    ddp = int(os.environ.get(\"RANK\", -1)) != -1  # is this a ddp run?\n    ddp_local_rank, DEVICE = 0, \"cuda:0\"\n    if ddp:\n        init_distributed_mode()\n        args.device = torch.device(DEVICE)\n\n    if args.use_wandb and (not ddp or ddp_local_rank == 0):\n        import wandb\n\n        wandb.init(project=args.wandb_project, name=args.wandb_run_name)\n    else:\n        wandb = None\n\n    model, tokenizer = init_model()\n    df = pd.read_csv(args.data_path)\n    df = df.sample(frac=1.0)\n    train_ds = PretrainDataset(df, tokenizer, max_length=max_seq_len)\n    train_sampler = DistributedSampler(train_ds) if ddp else None\n    train_loader = DataLoader(\n        train_ds,\n        batch_size=args.batch_size,\n        pin_memory=True,\n        drop_last=False,\n        shuffle=False,\n        num_workers=args.num_workers,\n        sampler=train_sampler\n    )\n\n    scaler = torch.cuda.amp.GradScaler(enabled=(args.dtype in ['float16', 'bfloat16']))\n    optimizer = optim.Adam(model.parameters(), lr=args.learning_rate)\n\n    if False and platform.system() != 'Windows' and float(torch.__version__.split('.')[0]) >= 2:\n        Logger(\"compiling the model... (takes a ~minute)\")\n        unoptimized_model = model\n        model = torch.compile(model)\n\n    if ddp:\n        model._ddp_params_and_buffers_to_ignore = {\"pos_cis\"}\n        model = DistributedDataParallel(model, device_ids=[ddp_local_rank])\n\n    iter_per_epoch = len(train_loader)\n    for epoch in range(args.epochs):\n        train_epoch(epoch, wandb)\n"
        },
        {
          "name": "2-eval.py",
          "type": "blob",
          "size": 6.30859375,
          "content": "import random\nimport time\n\nimport numpy as np\nimport torch\nimport warnings\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nfrom model.model import Transformer\nfrom model.LMConfig import LMConfig\n\nwarnings.filterwarnings('ignore')\n\n\ndef count_parameters(model):\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n\n\ndef init_model(lm_config):\n    tokenizer = AutoTokenizer.from_pretrained('./model/minimind_tokenizer')\n    model_from = 1  # 1从权重，2用transformers\n\n    if model_from == 1:\n        moe_path = '_moe' if lm_config.use_moe else ''\n        ckp = f'./out/full_sft_{lm_config.dim}{moe_path}.pth'\n\n        model = Transformer(lm_config)\n        state_dict = torch.load(ckp, map_location=device)\n\n        # 处理不需要的前缀\n        unwanted_prefix = '_orig_mod.'\n        for k, v in list(state_dict.items()):\n            if k.startswith(unwanted_prefix):\n                state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n\n        for k, v in list(state_dict.items()):\n            if 'mask' in k:\n                del state_dict[k]\n\n        # 加载到模型中\n        model.load_state_dict(state_dict, strict=False)\n    else:\n        model = AutoModelForCausalLM.from_pretrained('./minimind-v1-small', trust_remote_code=True)\n    model = model.to(device)\n\n    print(f'模型参数: {count_parameters(model) / 1e6} 百万 = {count_parameters(model) / 1e9} B (Billion)')\n    return model, tokenizer\n\n\ndef setup_seed(seed):\n    random.seed(seed)  # 设置 Python 的随机种子\n    np.random.seed(seed)  # 设置 NumPy 的随机种子\n    torch.manual_seed(seed)  # 设置 PyTorch 的随机种子\n    torch.cuda.manual_seed(seed)  # 为当前 GPU 设置随机种子（如果有）\n    torch.cuda.manual_seed_all(seed)  # 为所有 GPU 设置随机种子（如果有）\n    torch.backends.cudnn.deterministic = True  # 确保每次返回的卷积算法是确定的\n    torch.backends.cudnn.benchmark = False  # 关闭 cuDNN 的自动调优，避免不确定性\n\n\nif __name__ == \"__main__\":\n    # -----------------------------------------------------------------------------\n    out_dir = 'out'\n    start = \"\"\n    temperature = 0.7\n    top_k = 16\n    # device = 'cpu'\n    device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n    dtype = 'bfloat16'\n    max_seq_len = 1 * 1024\n    lm_config = LMConfig()\n    lm_config.max_seq_len = max_seq_len\n    # 对话是否携带历史对话（当前模型没有在连续对话数据集上训练，增大历史上文基本不会有新的问答能力）\n    contain_history_chat = False\n    # -----------------------------------------------------------------------------\n\n    model, tokenizer = init_model(lm_config)\n\n    model = model.eval()\n    # 推送到huggingface\n    # model.push_to_hub(\"minimind\")\n    # tokenizer.push_to_hub(\"minimind\")\n\n    # answer_way = int(input('输入0自动测试，输入1问题测试：'))\n    answer_way = 0\n    stream = True\n\n    prompt_datas = [\n        '你叫什么名字',\n        '你是谁',\n        '中国有哪些比较好的大学？',\n        '全世界最好的大学是什么？',\n        '你知道光速是多少吗？',\n        '你知道长江吗？',\n        '人类的血液主要由哪些成分组成？',\n        '第一颗人造卫星是哪个国家发射的？',\n        '你知道杭州有什么美食吗？',\n        '你知道泰山在哪里吗？',\n        '地球上最大的动物是什么？',\n        '地球自转一圈大约需要多少时间？',\n        '人类最早使用的金属是什么？',\n        '水的化学分子式是什么？',\n        '大气层中含量最多的气体是什么？',\n        '世界上最高的山峰是什么？',\n        '你知道世界上最深的海沟是什么吗？',\n        '最早发明印刷术的是哪个国家？',\n        '万有引力是谁提出的？',\n        '光合作用的主要原理是什么？',\n        '你知道大熊猫的主要食物是什么吗？',\n        '海水为什么是咸的？',\n        '我们平时喝的牛奶主要含有什么营养成分？',\n        '一星期有多少天？'\n    ]\n\n    messages_origin = []\n    messages = messages_origin\n\n    i = 0\n    while i < len(prompt_datas):\n        # Generate a random seed\n        random_seed = random.randint(0, 2 ** 32 - 1)\n        setup_seed(random_seed)\n        if not contain_history_chat:\n            messages = messages_origin.copy()\n\n        if answer_way == 1:\n            prompt = input('[Q]: ')\n        else:\n            prompt = prompt_datas[i]\n            print(f'[Q]: {prompt}')\n            i += 1\n\n        prompt = '请问，' + prompt\n        messages.append({\"role\": \"user\", \"content\": prompt})\n\n        # print(messages)\n        new_prompt = tokenizer.apply_chat_template(\n            messages,\n            tokenize=False,\n            add_generation_prompt=True\n        )[-(max_seq_len - 1):]\n\n        x = tokenizer(new_prompt).data['input_ids']\n        x = (torch.tensor(x, dtype=torch.long, device=device)[None, ...])\n\n        answer = new_prompt\n\n        with torch.no_grad():\n            res_y = model.generate(x, tokenizer.eos_token_id, max_new_tokens=max_seq_len, temperature=temperature,\n                                   top_k=top_k, stream=stream)\n            print('[A]: ', end='')\n            try:\n                y = next(res_y)\n            except StopIteration:\n                print(\"No answer\")\n                continue\n\n            history_idx = 0\n            while y != None:\n                answer = tokenizer.decode(y[0].tolist())\n                if answer and answer[-1] == '�':\n                    try:\n                        y = next(res_y)\n                    except:\n                        break\n                    continue\n                # print(answer)\n                if not len(answer):\n                    try:\n                        y = next(res_y)\n                    except:\n                        break\n                    continue\n\n                print(answer[history_idx:], end='', flush=True)\n                try:\n                    y = next(res_y)\n                except:\n                    break\n                history_idx = len(answer)\n                if not stream:\n                    break\n\n            print('\\n')\n\n        if contain_history_chat:\n            assistant_answer = answer.replace(new_prompt, \"\")\n            messages.append({\"role\": \"assistant\", \"content\": assistant_answer})\n"
        },
        {
          "name": "3-full_sft.py",
          "type": "blob",
          "size": 8.3681640625,
          "content": "import os\nimport platform\nimport argparse\nimport time\nimport math\nimport warnings\n\nimport pandas as pd\nimport torch\nimport torch.nn.functional as F\nimport torch.distributed as dist\nfrom contextlib import nullcontext\n\nfrom torch import optim\nfrom torch.nn.parallel import DistributedDataParallel\nfrom torch.utils.data import DataLoader, DistributedSampler\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nfrom model.model import Transformer\nfrom model.LMConfig import LMConfig\nfrom model.dataset import SFTDataset\n\nwarnings.filterwarnings('ignore')\n\n\ndef Logger(content):\n    if not ddp or dist.get_rank() == 0:\n        print(content)\n\n\ndef get_lr(it, all):\n    warmup_iters = args.warmup_iters\n    lr_decay_iters = all\n    min_lr = args.learning_rate / 10\n\n    if it < warmup_iters:\n        return args.learning_rate * it / warmup_iters\n    if it > lr_decay_iters:\n        return min_lr\n    decay_ratio = (it - warmup_iters) / (lr_decay_iters - warmup_iters)\n    assert 0 <= decay_ratio <= 1\n    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio))\n    return min_lr + coeff * (args.learning_rate - min_lr)\n\n\ndef train_epoch(epoch, wandb):\n    start_time = time.time()\n    for step, (X, Y, loss_mask) in enumerate(train_loader):\n        X = X.to(args.device)\n        Y = Y.to(args.device)\n        loss_mask = loss_mask.to(args.device)\n        lr = get_lr(epoch * iter_per_epoch + step, args.epochs * iter_per_epoch)\n        for param_group in optimizer.param_groups:\n            param_group['lr'] = lr\n\n        with ctx:\n            logits = model(X, Y).logits\n            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), Y.view(-1), ignore_index=0, reduction='none')\n            loss_mask = loss_mask.view(-1)\n            loss = torch.sum(loss * loss_mask) / loss_mask.sum()\n\n        scaler.scale(loss).backward()\n\n        if (step + 1) % args.accumulation_steps == 0:\n            scaler.unscale_(optimizer)\n            torch.nn.utils.clip_grad_norm_(model.parameters(), args.grad_clip)\n\n            scaler.step(optimizer)\n            scaler.update()\n\n            optimizer.zero_grad(set_to_none=True)\n\n        if step % args.log_interval == 0:\n            spend_time = time.time() - start_time\n            Logger(\n                'Epoch:[{}/{}]({}/{}) loss:{:.3f} lr:{:.7f} epoch_Time:{}min:'.format(\n                    epoch,\n                    args.epochs,\n                    step,\n                    iter_per_epoch,\n                    loss.item(),\n                    optimizer.param_groups[-1]['lr'],\n                    spend_time / (step + 1) * iter_per_epoch // 60 - spend_time // 60))\n\n            if (wandb is not None) and (not ddp or dist.get_rank() == 0):\n                wandb.log({\"loss\": loss,\n                           \"lr\": optimizer.param_groups[-1]['lr'],\n                           \"epoch_Time\": spend_time / (step + 1) * iter_per_epoch // 60 - spend_time // 60})\n\n        if (step + 1) % args.save_interval == 0 and (not ddp or dist.get_rank() == 0):\n            model.eval()\n            moe_path = '_moe' if lm_config.use_moe else ''\n            ckp = f'{args.save_dir}/full_sft_{lm_config.dim}{moe_path}.pth'\n\n            if isinstance(model, torch.nn.parallel.DistributedDataParallel):\n                state_dict = model.module.state_dict()\n            else:\n                state_dict = model.state_dict()\n\n            torch.save(state_dict, ckp)\n            model.train()\n\n\ndef init_model():\n    tokenizer = AutoTokenizer.from_pretrained('./model/minimind_tokenizer')\n    model_from = 1  # 1从权重，2用transformers\n\n    def count_parameters(model):\n        return sum(p.numel() for p in model.parameters() if p.requires_grad)\n\n    if model_from == 1:\n        model = Transformer(lm_config)\n        moe_path = '_moe' if lm_config.use_moe else ''\n        ckp = f'./out/pretrain_{lm_config.dim}{moe_path}.pth'\n        state_dict = torch.load(ckp, map_location=args.device)\n        unwanted_prefix = '_orig_mod.'\n        for k, v in list(state_dict.items()):\n            if k.startswith(unwanted_prefix):\n                state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n        model.load_state_dict(state_dict, strict=False)\n    else:\n        model = AutoModelForCausalLM.from_pretrained('./minimind-v1-small', trust_remote_code=True)\n\n    Logger(f'LLM总参数量：{count_parameters(model) / 1e6:.3f} 百万')\n    model = model.to(args.device)\n\n    return model, tokenizer\n\n\ndef init_distributed_mode():\n    if not ddp: return\n    global ddp_local_rank, DEVICE\n\n    dist.init_process_group(backend=\"nccl\")\n    ddp_rank = int(os.environ[\"RANK\"])\n    ddp_local_rank = int(os.environ[\"LOCAL_RANK\"])\n    ddp_world_size = int(os.environ[\"WORLD_SIZE\"])\n    DEVICE = f\"cuda:{ddp_local_rank}\"\n    torch.cuda.set_device(DEVICE)\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser(description=\"MiniMind Full SFT\")\n    parser.add_argument(\"--out_dir\", type=str, default=\"out\", help=\"Output directory\")\n    parser.add_argument(\"--epochs\", type=int, default=19, help=\"Number of epochs\")\n    parser.add_argument(\"--batch_size\", type=int, default=32, help=\"Batch size\")\n    parser.add_argument(\"--learning_rate\", type=float, default=1e-4, help=\"Learning rate\")\n    parser.add_argument(\"--device\", type=str, default=\"cuda:0\" if torch.cuda.is_available() else \"cpu\", help=\"Device to use\")\n    parser.add_argument(\"--dtype\", type=str, default=\"bfloat16\", help=\"Data type\")\n    parser.add_argument(\"--use_wandb\", action=\"store_true\", help=\"Use Weights & Biases\")\n    parser.add_argument(\"--wandb_project\", type=str, default=\"MiniMind-Full-SFT\", help=\"Weights & Biases project name\")\n    parser.add_argument(\"--num_workers\", type=int, default=1, help=\"Number of workers for data loading\")\n    parser.add_argument(\"--ddp\", action=\"store_true\", help=\"Use DistributedDataParallel\")\n    parser.add_argument(\"--accumulation_steps\", type=int, default=1, help=\"Gradient accumulation steps\")\n    parser.add_argument(\"--grad_clip\", type=float, default=1.0, help=\"Gradient clipping threshold\")\n    parser.add_argument(\"--warmup_iters\", type=int, default=0, help=\"Number of warmup iterations\")\n    parser.add_argument(\"--log_interval\", type=int, default=100, help=\"Logging interval\")\n    parser.add_argument(\"--save_interval\", type=int, default=1000, help=\"Model saving interval\")\n    parser.add_argument('--local_rank', type=int, default=-1, help='local rank for distributed training')\n\n    args = parser.parse_args()\n\n    lm_config = LMConfig()\n    max_seq_len = lm_config.max_seq_len\n    args.save_dir = os.path.join(args.out_dir)\n    os.makedirs(args.save_dir, exist_ok=True)\n    os.makedirs(args.out_dir, exist_ok=True)\n    tokens_per_iter = args.batch_size * max_seq_len\n    torch.manual_seed(1337)\n    device_type = \"cuda\" if \"cuda\" in args.device else \"cpu\"\n\n    args.wandb_run_name = f\"MiniMind-Full-SFT-Epoch-{args.epochs}-BatchSize-{args.batch_size}-LearningRate-{args.learning_rate}\"\n\n    ctx = nullcontext() if device_type == \"cpu\" else torch.cuda.amp.autocast()\n    ddp = int(os.environ.get(\"RANK\", -1)) != -1  # is this a ddp run?\n    ddp_local_rank, DEVICE = 0, \"cuda:0\"\n    if ddp:\n        init_distributed_mode()\n        args.device = torch.device(DEVICE)\n\n    if args.use_wandb and (not ddp or ddp_local_rank == 0):\n        import wandb\n        wandb.init(project=args.wandb_project, name=args.wandb_run_name)\n    else:\n        wandb = None\n\n    model, tokenizer = init_model()\n\n    df = pd.read_csv('./dataset/sft_data_single.csv')\n    df = df.sample(frac=1.0)\n    train_ds = SFTDataset(df, tokenizer, max_length=max_seq_len)\n    train_sampler = DistributedSampler(train_ds) if ddp else None\n    train_loader = DataLoader(\n        train_ds,\n        batch_size=args.batch_size,\n        pin_memory=True,\n        drop_last=False,\n        shuffle=False,\n        num_workers=args.num_workers,\n        sampler=train_sampler\n    )\n\n    scaler = torch.cuda.amp.GradScaler(enabled=(args.dtype in ['float16', 'bfloat16']))\n    optimizer = optim.Adam(model.parameters(), lr=args.learning_rate)\n\n    if False and not lm_config.use_moe and platform.system() != 'Windows' and float(torch.__version__.split('.')[0]) >= 2:\n        Logger(\"compiling the model... (takes a ~minute)\")\n        unoptimized_model = model\n        model = torch.compile(model)\n\n    if ddp:\n        model._ddp_params_and_buffers_to_ignore = {\"pos_cis\"}\n        model = DistributedDataParallel(model, device_ids=[ddp_local_rank])\n\n    iter_per_epoch = len(train_loader)\n    for epoch in range(args.epochs):\n        train_epoch(epoch, wandb)\n"
        },
        {
          "name": "4-lora_sft.py",
          "type": "blob",
          "size": 6.9697265625,
          "content": "import os\nimport platform\nimport argparse\nimport time\nimport math\nimport warnings\nimport torch\nimport pandas as pd\nimport torch.nn.functional as F\nfrom contextlib import nullcontext\n\nfrom torch import optim\nfrom transformers import AutoTokenizer\nfrom transformers import AutoModelForCausalLM\nfrom peft import get_peft_model, LoraConfig, TaskType\nfrom torch.utils.data import DataLoader\nfrom model.LMConfig import LMConfig\nfrom model.dataset import SFTDataset\nfrom model.model import Transformer\n\nwarnings.filterwarnings('ignore')\n\n\ndef Logger(content):\n    print(content)\n\n\ndef get_lr(it, all):\n    warmup_iters = args.warmup_iters\n    lr_decay_iters = all\n    min_lr = args.learning_rate / 10\n\n    if it < warmup_iters:\n        return args.learning_rate * it / warmup_iters\n    if it > lr_decay_iters:\n        return min_lr\n    decay_ratio = (it - warmup_iters) / (lr_decay_iters - warmup_iters)\n    assert 0 <= decay_ratio <= 1\n    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio))\n    return min_lr + coeff * (args.learning_rate - min_lr)\n\n\ndef train_epoch(epoch, wandb):\n    start_time = time.time()\n    for step, (X, Y, loss_mask) in enumerate(train_loader):\n        X = X.to(args.device)\n        Y = Y.to(args.device)\n        loss_mask = loss_mask.to(args.device)\n\n        lr = get_lr(epoch * iter_per_epoch + step, args.epochs * iter_per_epoch)\n        for param_group in optimizer.param_groups:\n            param_group['lr'] = lr\n\n        with ctx:\n            logits = model(X, Y).logits\n            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), Y.view(-1), ignore_index=0, reduction='none')\n            loss_mask = loss_mask.view(-1)\n            loss = torch.sum(loss * loss_mask) / loss_mask.sum()\n            loss = loss / args.accumulation_steps\n\n        scaler.scale(loss).backward()\n\n        if (step + 1) % args.accumulation_steps == 0:\n            scaler.unscale_(optimizer)\n            torch.nn.utils.clip_grad_norm_(model.parameters(), args.grad_clip)\n\n            scaler.step(optimizer)\n            scaler.update()\n\n            optimizer.zero_grad(set_to_none=True)\n\n        if step % args.log_interval == 0:\n            spend_time = time.time() - start_time\n            Logger(\n                'Epoch:[{}/{}]({}/{}) loss:{:.3f} lr:{:.7f} epoch_Time:{}min:'.format(\n                    epoch,\n                    args.epochs,\n                    step,\n                    iter_per_epoch,\n                    loss.item() * args.accumulation_steps,\n                    optimizer.param_groups[-1]['lr'],\n                    spend_time / (step + 1) * iter_per_epoch // 60 - spend_time // 60))\n            if wandb is not None:\n                wandb.log({\"loss\": loss.item() * args.accumulation_steps,\n                           \"lr\": optimizer.param_groups[-1]['lr'],\n                           \"epoch_Time\": spend_time / (step + 1) * iter_per_epoch // 60 - spend_time // 60})\n\n        if (step + 1) % args.save_interval == 0:\n            model.save_pretrained(args.save_dir)\n\n\ndef find_linear_with_keys(model, keys=[\"wq\", \"wk\"]):\n    cls = torch.nn.Linear\n    linear_names = []\n    for name, module in model.named_modules():\n        if isinstance(module, cls):\n            for key in keys:\n                if key in name:\n                    linear_names.append(name)\n                    break\n    return linear_names\n\n\ndef init_model():\n    model_name_or_path = \"./minimind-v1-small\"\n    tokenizer_name_or_path = \"./minimind-v1-small\"\n    tokenizer = AutoTokenizer.from_pretrained(tokenizer_name_or_path, trust_remote_code=True, use_fast=False)\n    model = AutoModelForCausalLM.from_pretrained(model_name_or_path, trust_remote_code=True).to(args.device)\n\n    target_modules = find_linear_with_keys(model)\n    peft_config = LoraConfig(\n        r=8,\n        target_modules=target_modules\n    )\n    model = get_peft_model(model, peft_config)\n    model.print_trainable_parameters()\n    model = model.to(args.device)\n    return model, tokenizer\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser(description=\"MiniMind LoRA Fine-tuning\")\n    parser.add_argument(\"--out_dir\", type=str, default=\"out\", help=\"Output directory\")\n    parser.add_argument(\"--epochs\", type=int, default=20, help=\"Number of epochs\")\n    parser.add_argument(\"--batch_size\", type=int, default=32, help=\"Batch size\")\n    parser.add_argument(\"--learning_rate\", type=float, default=1e-4, help=\"Learning rate\")\n    parser.add_argument(\"--device\", type=str, default=\"cuda:0\" if torch.cuda.is_available() else \"cpu\",\n                        help=\"Device to use\")\n    parser.add_argument(\"--dtype\", type=str, default=\"bfloat16\", help=\"Data type\")\n    parser.add_argument(\"--use_wandb\", action=\"store_true\", help=\"Use Weights & Biases\")\n    parser.add_argument(\"--wandb_project\", type=str, default=\"MiniMind-LoRA\", help=\"Weights & Biases project name\")\n    parser.add_argument(\"--num_workers\", type=int, default=1, help=\"Number of workers for data loading\")\n    parser.add_argument(\"--accumulation_steps\", type=int, default=1, help=\"Gradient accumulation steps\")\n    parser.add_argument(\"--grad_clip\", type=float, default=1.0, help=\"Gradient clipping threshold\")\n    parser.add_argument(\"--warmup_iters\", type=int, default=1000, help=\"Number of warmup iterations\")\n    parser.add_argument(\"--log_interval\", type=int, default=100, help=\"Logging interval\")\n    parser.add_argument(\"--save_interval\", type=int, default=1000, help=\"Model saving interval\")\n\n    args = parser.parse_args()\n\n    lm_config = LMConfig()\n    max_seq_len = lm_config.max_seq_len\n    args.save_dir = os.path.join(args.out_dir)\n    os.makedirs(args.save_dir, exist_ok=True)\n    os.makedirs(args.out_dir, exist_ok=True)\n    tokens_per_iter = args.batch_size * max_seq_len\n    torch.manual_seed(1337)\n    device_type = \"cuda\" if \"cuda\" in args.device else \"cpu\"\n\n    args.wandb_run_name = f\"MiniMind-LoRA-Epoch-{args.epochs}-BatchSize-{args.batch_size}-LearningRate-{args.learning_rate}\"\n\n    ctx = nullcontext() if device_type == \"cpu\" else torch.cuda.amp.autocast()\n\n    if args.use_wandb:\n        import wandb\n\n        wandb.init(project=args.wandb_project, name=args.wandb_run_name)\n    else:\n        wandb = None\n\n    model, tokenizer = init_model()\n\n    df = pd.read_csv('./dataset/sft_data_single.csv')\n    df = df.sample(frac=1.0)\n    train_ds = SFTDataset(df, tokenizer, max_length=max_seq_len)\n    train_loader = DataLoader(\n        train_ds,\n        batch_size=args.batch_size,\n        pin_memory=True,\n        drop_last=False,\n        shuffle=False,\n        num_workers=args.num_workers,\n    )\n\n    scaler = torch.cuda.amp.GradScaler(enabled=(args.dtype in ['float16', 'bfloat16']))\n    optimizer = optim.Adam(\n        filter(lambda p: p.requires_grad, model.parameters()),\n        lr=args.learning_rate\n    )\n\n    if False and platform.system() != 'Windows' and float(torch.__version__.split('.')[0]) >= 2:\n        Logger(\"compiling the model... (takes a ~minute)\")\n        unoptimized_model = model\n        model = torch.compile(model)\n\n    iter_per_epoch = len(train_loader)\n    for epoch in range(args.epochs):\n        train_epoch(epoch, wandb)\n"
        },
        {
          "name": "5-dpo_train.py",
          "type": "blob",
          "size": 1.3916015625,
          "content": "import os\nimport warnings\n\nos.environ['CUDA_VISIBLE_DEVICES'] = '0'\nfrom transformers import TrainingArguments, AutoModelForCausalLM, AutoTokenizer\nfrom trl import DPOConfig, DPOTrainer\nfrom datasets import load_dataset\n\nwarnings.filterwarnings('ignore')\n\n\ndef init_model():\n    device = 'cuda:0'\n    # Do model patching and add fast LoRA weights\n    model_name_or_path = \"minimind-v1\"\n    tokenizer_name_or_path = \"minimind-v1\"\n    model = AutoModelForCausalLM.from_pretrained(model_name_or_path, trust_remote_code=True)\n    tokenizer = AutoTokenizer.from_pretrained(tokenizer_name_or_path, trust_remote_code=True, use_fast=False)\n    tokenizer.pad_token = tokenizer.eos_token\n    model = model.to(device)\n    return model, tokenizer\n\n\nif __name__ == '__main__':\n    model, tokenizer = init_model()\n    training_config = DPOConfig(\n        output_dir=\"./minimind_dpo\",\n        per_device_train_batch_size=1,\n        remove_unused_columns=False,\n        report_to=\"none\",\n        save_steps=2000,\n        learning_rate=4e-5\n    )\n\n    dataset_path = './dataset/dpo/train_data.json'\n    train_dataset = load_dataset('json', data_files=dataset_path)\n\n    dpo_trainer = DPOTrainer(\n        model,\n        ref_model=None,\n        args=training_config,\n        beta=0.1,\n        train_dataset=train_dataset['train'],\n        tokenizer=tokenizer,\n        max_length=512,\n        max_prompt_length=512\n    )\n    dpo_trainer.train()\n"
        },
        {
          "name": "CODE_OF_CONDUCT.md",
          "type": "blob",
          "size": 5.080078125,
          "content": "# Contributor Covenant Code of Conduct\n\n## Our Pledge\n\nWe as members, contributors, and leaders pledge to make participation in our\ncommunity a harassment-free experience for everyone, regardless of age, body\nsize, visible or invisible disability, ethnicity, sex characteristics, gender\nidentity and expression, level of experience, education, socio-economic status,\nnationality, personal appearance, race, religion, or sexual identity\nand orientation.\n\nWe pledge to act and interact in ways that contribute to an open, welcoming,\ndiverse, inclusive, and healthy community.\n\n## Our Standards\n\nExamples of behavior that contributes to a positive environment for our\ncommunity include:\n\n* Demonstrating empathy and kindness toward other people\n* Being respectful of differing opinions, viewpoints, and experiences\n* Giving and gracefully accepting constructive feedback\n* Accepting responsibility and apologizing to those affected by our mistakes,\n  and learning from the experience\n* Focusing on what is best not just for us as individuals, but for the\n  overall community\n\nExamples of unacceptable behavior include:\n\n* The use of sexualized language or imagery, and sexual attention or\n  advances of any kind\n* Trolling, insulting or derogatory comments, and personal or political attacks\n* Public or private harassment\n* Publishing others' private information, such as a physical or email\n  address, without their explicit permission\n* Other conduct which could reasonably be considered inappropriate in a\n  professional setting\n\n## Enforcement Responsibilities\n\nCommunity leaders are responsible for clarifying and enforcing our standards of\nacceptable behavior and will take appropriate and fair corrective action in\nresponse to any behavior that they deem inappropriate, threatening, offensive,\nor harmful.\n\nCommunity leaders have the right and responsibility to remove, edit, or reject\ncomments, commits, code, wiki edits, issues, and other contributions that are\nnot aligned to this Code of Conduct, and will communicate reasons for moderation\ndecisions when appropriate.\n\n## Scope\n\nThis Code of Conduct applies within all community spaces, and also applies when\nan individual is officially representing the community in public spaces.\nExamples of representing our community include using an official e-mail address,\nposting via an official social media account, or acting as an appointed\nrepresentative at an online or offline event.\n\n## Enforcement\n\nInstances of abusive, harassing, or otherwise unacceptable behavior may be\nreported to the community leaders responsible for enforcement at\n.\nAll complaints will be reviewed and investigated promptly and fairly.\n\nAll community leaders are obligated to respect the privacy and security of the\nreporter of any incident.\n\n## Enforcement Guidelines\n\nCommunity leaders will follow these Community Impact Guidelines in determining\nthe consequences for any action they deem in violation of this Code of Conduct:\n\n### 1. Correction\n\n**Community Impact**: Use of inappropriate language or other behavior deemed\nunprofessional or unwelcome in the community.\n\n**Consequence**: A private, written warning from community leaders, providing\nclarity around the nature of the violation and an explanation of why the\nbehavior was inappropriate. A public apology may be requested.\n\n### 2. Warning\n\n**Community Impact**: A violation through a single incident or series\nof actions.\n\n**Consequence**: A warning with consequences for continued behavior. No\ninteraction with the people involved, including unsolicited interaction with\nthose enforcing the Code of Conduct, for a specified period of time. This\nincludes avoiding interactions in community spaces as well as external channels\nlike social media. Violating these terms may lead to a temporary or\npermanent ban.\n\n### 3. Temporary Ban\n\n**Community Impact**: A serious violation of community standards, including\nsustained inappropriate behavior.\n\n**Consequence**: A temporary ban from any sort of interaction or public\ncommunication with the community for a specified period of time. No public or\nprivate interaction with the people involved, including unsolicited interaction\nwith those enforcing the Code of Conduct, is allowed during this period.\nViolating these terms may lead to a permanent ban.\n\n### 4. Permanent Ban\n\n**Community Impact**: Demonstrating a pattern of violation of community\nstandards, including sustained inappropriate behavior,  harassment of an\nindividual, or aggression toward or disparagement of classes of individuals.\n\n**Consequence**: A permanent ban from any sort of public interaction within\nthe community.\n\n## Attribution\n\nThis Code of Conduct is adapted from the [Contributor Covenant][homepage],\nversion 2.0, available at\nhttps://www.contributor-covenant.org/version/2/0/code_of_conduct.html.\n\nCommunity Impact Guidelines were inspired by [Mozilla's code of conduct\nenforcement ladder](https://github.com/mozilla/diversity).\n\n[homepage]: https://www.contributor-covenant.org\n\nFor answers to common questions about this code of conduct, see the FAQ at\nhttps://www.contributor-covenant.org/faq. Translations are available at\nhttps://www.contributor-covenant.org/translations.\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 11.0908203125,
          "content": "                                 Apache License\n                           Version 2.0, January 2004\n                        http://www.apache.org/licenses/\n\n   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n\n   1. Definitions.\n\n      \"License\" shall mean the terms and conditions for use, reproduction,\n      and distribution as defined by Sections 1 through 9 of this document.\n\n      \"Licensor\" shall mean the copyright owner or entity authorized by\n      the copyright owner that is granting the License.\n\n      \"Legal Entity\" shall mean the union of the acting entity and all\n      other entities that control, are controlled by, or are under common\n      control with that entity. For the purposes of this definition,\n      \"control\" means (i) the power, direct or indirect, to cause the\n      direction or management of such entity, whether by contract or\n      otherwise, or (ii) ownership of fifty percent (50%) or more of the\n      outstanding shares, or (iii) beneficial ownership of such entity.\n\n      \"You\" (or \"Your\") shall mean an individual or Legal Entity\n      exercising permissions granted by this License.\n\n      \"Source\" form shall mean the preferred form for making modifications,\n      including but not limited to software source code, documentation\n      source, and configuration files.\n\n      \"Object\" form shall mean any form resulting from mechanical\n      transformation or translation of a Source form, including but\n      not limited to compiled object code, generated documentation,\n      and conversions to other media types.\n\n      \"Work\" shall mean the work of authorship, whether in Source or\n      Object form, made available under the License, as indicated by a\n      copyright notice that is included in or attached to the work\n      (an example is provided in the Appendix below).\n\n      \"Derivative Works\" shall mean any work, whether in Source or Object\n      form, that is based on (or derived from) the Work and for which the\n      editorial revisions, annotations, elaborations, or other modifications\n      represent, as a whole, an original work of authorship. For the purposes\n      of this License, Derivative Works shall not include works that remain\n      separable from, or merely link (or bind by name) to the interfaces of,\n      the Work and Derivative Works thereof.\n\n      \"Contribution\" shall mean any work of authorship, including\n      the original version of the Work and any modifications or additions\n      to that Work or Derivative Works thereof, that is intentionally\n      submitted to Licensor for inclusion in the Work by the copyright owner\n      or by an individual or Legal Entity authorized to submit on behalf of\n      the copyright owner. For the purposes of this definition, \"submitted\"\n      means any form of electronic, verbal, or written communication sent\n      to the Licensor or its representatives, including but not limited to\n      communication on electronic mailing lists, source code control systems,\n      and issue tracking systems that are managed by, or on behalf of, the\n      Licensor for the purpose of discussing and improving the Work, but\n      excluding communication that is conspicuously marked or otherwise\n      designated in writing by the copyright owner as \"Not a Contribution.\"\n\n      \"Contributor\" shall mean Licensor and any individual or Legal Entity\n      on behalf of whom a Contribution has been received by Licensor and\n      subsequently incorporated within the Work.\n\n   2. Grant of Copyright License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      copyright license to reproduce, prepare Derivative Works of,\n      publicly display, publicly perform, sublicense, and distribute the\n      Work and such Derivative Works in Source or Object form.\n\n   3. Grant of Patent License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      (except as stated in this section) patent license to make, have made,\n      use, offer to sell, sell, import, and otherwise transfer the Work,\n      where such license applies only to those patent claims licensable\n      by such Contributor that are necessarily infringed by their\n      Contribution(s) alone or by combination of their Contribution(s)\n      with the Work to which such Contribution(s) was submitted. If You\n      institute patent litigation against any entity (including a\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\n      or a Contribution incorporated within the Work constitutes direct\n      or contributory patent infringement, then any patent licenses\n      granted to You under this License for that Work shall terminate\n      as of the date such litigation is filed.\n\n   4. Redistribution. You may reproduce and distribute copies of the\n      Work or Derivative Works thereof in any medium, with or without\n      modifications, and in Source or Object form, provided that You\n      meet the following conditions:\n\n      (a) You must give any other recipients of the Work or\n          Derivative Works a copy of this License; and\n\n      (b) You must cause any modified files to carry prominent notices\n          stating that You changed the files; and\n\n      (c) You must retain, in the Source form of any Derivative Works\n          that You distribute, all copyright, patent, trademark, and\n          attribution notices from the Source form of the Work,\n          excluding those notices that do not pertain to any part of\n          the Derivative Works; and\n\n      (d) If the Work includes a \"NOTICE\" text file as part of its\n          distribution, then any Derivative Works that You distribute must\n          include a readable copy of the attribution notices contained\n          within such NOTICE file, excluding those notices that do not\n          pertain to any part of the Derivative Works, in at least one\n          of the following places: within a NOTICE text file distributed\n          as part of the Derivative Works; within the Source form or\n          documentation, if provided along with the Derivative Works; or,\n          within a display generated by the Derivative Works, if and\n          wherever such third-party notices normally appear. The contents\n          of the NOTICE file are for informational purposes only and\n          do not modify the License. You may add Your own attribution\n          notices within Derivative Works that You distribute, alongside\n          or as an addendum to the NOTICE text from the Work, provided\n          that such additional attribution notices cannot be construed\n          as modifying the License.\n\n      You may add Your own copyright statement to Your modifications and\n      may provide additional or different license terms and conditions\n      for use, reproduction, or distribution of Your modifications, or\n      for any such Derivative Works as a whole, provided Your use,\n      reproduction, and distribution of the Work otherwise complies with\n      the conditions stated in this License.\n\n   5. Submission of Contributions. Unless You explicitly state otherwise,\n      any Contribution intentionally submitted for inclusion in the Work\n      by You to the Licensor shall be under the terms and conditions of\n      this License, without any additional terms or conditions.\n      Notwithstanding the above, nothing herein shall supersede or modify\n      the terms of any separate license agreement you may have executed\n      with Licensor regarding such Contributions.\n\n   6. Trademarks. This License does not grant permission to use the trade\n      names, trademarks, service marks, or product names of the Licensor,\n      except as required for reasonable and customary use in describing the\n      origin of the Work and reproducing the content of the NOTICE file.\n\n   7. Disclaimer of Warranty. Unless required by applicable law or\n      agreed to in writing, Licensor provides the Work (and each\n      Contributor provides its Contributions) on an \"AS IS\" BASIS,\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n      implied, including, without limitation, any warranties or conditions\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n      PARTICULAR PURPOSE. You are solely responsible for determining the\n      appropriateness of using or redistributing the Work and assume any\n      risks associated with Your exercise of permissions under this License.\n\n   8. Limitation of Liability. In no event and under no legal theory,\n      whether in tort (including negligence), contract, or otherwise,\n      unless required by applicable law (such as deliberate and grossly\n      negligent acts) or agreed to in writing, shall any Contributor be\n      liable to You for damages, including any direct, indirect, special,\n      incidental, or consequential damages of any character arising as a\n      result of this License or out of the use or inability to use the\n      Work (including but not limited to damages for loss of goodwill,\n      work stoppage, computer failure or malfunction, or any and all\n      other commercial damages or losses), even if such Contributor\n      has been advised of the possibility of such damages.\n\n   9. Accepting Warranty or Additional Liability. While redistributing\n      the Work or Derivative Works thereof, You may choose to offer,\n      and charge a fee for, acceptance of support, warranty, indemnity,\n      or other liability obligations and/or rights consistent with this\n      License. However, in accepting such obligations, You may act only\n      on Your own behalf and on Your sole responsibility, not on behalf\n      of any other Contributor, and only if You agree to indemnify,\n      defend, and hold each Contributor harmless for any liability\n      incurred by, or claims asserted against, such Contributor by reason\n      of your accepting any such warranty or additional liability.\n\n   END OF TERMS AND CONDITIONS\n\n   APPENDIX: How to apply the Apache License to your work.\n\n      To apply the Apache License to your work, attach the following\n      boilerplate notice, with the fields enclosed by brackets \"[]\"\n      replaced with your own identifying information. (Don't include\n      the brackets!)  The text should be enclosed in the appropriate\n      comment syntax for the file format. We also recommend that a\n      file or class name and description of purpose be included on the\n      same \"printed page\" as the copyright notice for easier\n      identification within third-party archives.\n\n   Copyright [yyyy] [name of copyright owner]\n\n   Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 60.5908203125,
          "content": "<div align=\"center\">\n\n![logo](./images/logo.png)\n\n</div>\n\n<div align=\"center\">\n\n![visitors](https://visitor-badge.laobi.icu/badge?page_id=jingyaogong/minimind)\n[![GitHub Repo stars](https://img.shields.io/github/stars/jingyaogong/minimind?style=social)](https://github.com/jingyaogong/minimind/stargazers)\n[![GitHub Code License](https://img.shields.io/github/license/jingyaogong/minimind)](LICENSE)\n[![GitHub last commit](https://img.shields.io/github/last-commit/jingyaogong/minimind)](https://github.com/jingyaogong/minimind/commits/master)\n[![GitHub pull request](https://img.shields.io/badge/PRs-welcome-blue)](https://github.com/jingyaogong/minimind/pulls)\n[![Collection](https://img.shields.io/badge/🤗-MiniMind%20%20Collection-blue)](https://huggingface.co/collections/jingyaogong/minimind-66caf8d999f5c7fa64f399e5)\n\n</div>\n\n<div align=\"center\">\n  <h3>\"大道至简\"</h3>\n</div>\n\n<div align=\"center\">\n\n中文 | [English](./README_en.md)\n\n</div>\n\n* 本开源项目旨在完全从0开始，最快仅用3小时！即可训练出仅为26.88M大小的微型语言模型**MiniMind**。\n* **MiniMind**极其轻量，最小版本体积约是 GPT3 的 $\\frac{1}{7000}$，力求做到最普通的个人GPU也可快速推理甚至训练。\n* **MiniMind**发布了大模型极简结构，数据集清洗和预处理、监督预训练(Pretrain)、有监督指令微调(SFT)、低秩自适应(LoRA)\n  微调，无奖励强化学习直接偏好对齐(DPO)的全阶段代码，也包含拓展共享混合专家(MoE)\n  的稀疏模型；拓展视觉多模态VLM: [MiniMind-V](https://github.com/jingyaogong/minimind-v)。\n* 这不仅是一个开源模型的实现，也是入门大语言模型（LLM）的教程。\n* 希望此项目能为研究者提供一个抛砖引玉的入门示例，帮助大家快速上手并对LLM领域产生更多的探索与创新。\n\n  > 为防止误读，「最快3小时」是指您需要具备＞本人硬件配置的机器，具体规格的详细信息将在下文提供。\n\n---\n\n\n\n\n<div align=\"center\">\n\n![streamlit](./images/streamlit.gif)\n\n[ModelScope在线测试](https://www.modelscope.cn/studios/gongjy/minimind) | [Bilibili视频链接](https://www.bilibili.com/video/BV12dHPeqE72/?share_source=copy_web&vd_source=670c2504f88726f8cf4a21ef6147c0e8)\n\n---\n\n</div>\n\n# 📌 Introduction\n\n大语言模型（LLM）领域，如 GPT、LLaMA、GLM 等，虽然它们效果惊艳，\n但动辄10 Bilion庞大的模型参数个人设备显存远不够训练，甚至推理困难。\n几乎所有人都不会只满足于用Lora等方案fine-tuing大模型学会一些新的指令，\n这约等于在教牛顿玩21世纪的智能手机，然而，这远远脱离了学习物理本身的奥妙。\n此外，卖课付费订阅的营销号漏洞百出的一知半解讲解AI的教程遍地，\n让理解LLM的优质内容雪上加霜，严重阻碍了学习者。\n\n因此，本项目的目标是把上手LLM的门槛无限降低，\n直接从0开始训练一个极其轻量的语言模型。\n\n> [!TIP]\n> （截至2024-9-17）MiniMind系列已完成了3个型号模型的预训练，最小仅需26M（0.02B），即可具备流畅的对话能力！\n\n| 模型 (大小)                 | tokenizer长度 | 推理占用   | release    | 主观评分（/100） | \n|-------------------------|-------------|--------|------------|------------|\n| minimind-v1-small (26M) | 6400        | 0.5 GB | 2024.08.28 | 50'        |\n| minimind-v1-moe (4×26M) | 6400        | 1.0 GB | 2024.09.17 | 55'        |\n| minimind-v1 (108M)      | 6400        | 1.0 GB | 2024.09.01 | 60'        |\n\n> 该分析在具有Torch 2.1.2、CUDA 12.2和Flash Attention 2的2×RTX 3090 GPU上进行。\n\n\n\n项目包含：\n\n- 公开MiniMind模型代码（包含Dense和MoE模型）、Pretrain、SFT指令微调、LoRA微调、DPO偏好优化的全过程代码、数据集和来源。\n- 兼容`transformers`、`accelerate`、`trl`、`peft`等流行框架。\n- 训练支持单机单卡、单机多卡(DDP、DeepSpeed)训练，使用wandb可视化训练流程。支持在任意位置停止，及在任意位置继续训练。\n- 在Ceval数据集上进行模型测试的代码。\n- 实现Openai-Api基本的chat接口，便于集成到第三方ChatUI使用（FastGPT、Open-WebUI等）。\n\n希望此开源项目可以帮助LLM初学者快速入门！\n\n### 👉**最近更新**\n\n<details close> \n<summary> <b>2024-10-05 (newest 🎉)</b> </summary>\n\n- 为MiniMind拓展了多模态能力之---视觉\n\n- 移步孪生项目[minimind-v](https://github.com/jingyaogong/minimind-v)查看详情！\n\n</details>\n\n<details close> \n<summary> <b>2024-09-27</b> </summary>\n\n- 09-27更新pretrain数据集的预处理方式，为了保证文本完整性，放弃预处理成.bin训练的形式（轻微牺牲训练速度）。\n\n- 目前pretrain预处理后的文件命名为：pretrain_data.csv。\n\n- 删除了一些冗余的代码。\n\n</details>\n\n<details close> \n<summary> <b>2024-09-17</b> </summary>\n\n- 更新minimind-v1-moe模型\n\n- 为了防止歧义，不再使用mistral_tokenizer分词，全部采用自定义的minimind_tokenizer作为分词器。\n\n</details>\n\n<details close>\n<summary> <b>2024-09-01</b> </summary>\n\n- 更新minimind-v1 (108M)模型，采用minimind_tokenizer，预训练轮次3 + SFT轮次10，更充分训练，性能更强。\n\n- 项目已部署至ModelScope创空间，可以在此网站上体验：\n\n- [🔗ModelScope在线体验🔗](https://www.modelscope.cn/studios/gongjy/minimind)\n\n</details>\n\n<details close> \n<summary> <b>2024-08-27</b> </summary>\n\n- 项目首次开源\n\n</details>\n\n# 📌 Environment\n\n仅是我个人的软硬件环境配置，自行酌情更改：\n\n```bash\nCPU: Intel(R) Core(TM) i9-10980XE CPU @ 3.00GHz\n内存：128 GB\n显卡：NVIDIA GeForce RTX 3090(24GB) * 2\n环境：python 3.9 + Torch 2.1.2 + DDP单机多卡训练\n```\n\n* Ubuntu == 20.04\n* Python == 3.9\n* Pytorch == 2.1.2\n* CUDA == 12.2\n* [requirements.txt](./requirements.txt)\n\n# 📌 Quick Start Test\n\n<div align=\"center\" style=\"font-size: 1.5em; font-weight: bold;\">\n  <img src=\"https://huggingface.co/front/assets/huggingface_logo-noborder.svg\" alt=\"Hugging Face Logo\" style=\"vertical-align: middle; height: 30px;\" />\n  Hugging Face\n\n[MiniMind (HuggingFace)](https://huggingface.co/collections/jingyaogong/minimind-66caf8d999f5c7fa64f399e5)\n\n <img src=\"https://g.alicdn.com/sail-web/maas/1.15.0/static/modelscopeIcon.cd89353f.svg\" alt=\"Hugging Face Logo\" style=\"vertical-align: middle; height: 30px;\" />\n\n[MiniMind (ModelScope)](https://www.modelscope.cn/models/gongjy/minimind-v1)\n\n</div>\n\n```bash\n# step 1\ngit clone https://huggingface.co/jingyaogong/minimind-v1\n```\n\n```bash\n# step 2\npython 2-eval.py\n```\n\n或者启动streamlit，启动网页聊天界面\n> 「注意」需要python>=3.10，安装 `pip install streamlit==1.27.2`\n\n```bash\n# or step 3, use streamlit\nstreamlit run fast_inference.py\n```\n\n# 📌 Quick Start Train\n\n* 0、克隆项目代码\n    ```bash\n    git clone https://github.com/jingyaogong/minimind.git\n    cd minimind\n    ```\n\n* 1、环境安装\n  ```bash\n  pip install -r requirements.txt -i https://pypi.tuna.tsinghua.edu.cn/simple\n  ```\n\n  ```text\n  # 测试torch是否可用cuda\n  import torch\n  print(torch.cuda.is_available())\n  ```\n\n  > 如果不可用，请自行去[torch_stable](https://download.pytorch.org/whl/torch_stable.html)\n  下载whl文件安装。参考[链接](https://blog.csdn.net/weixin_45456738/article/details/141029610?ops_request_misc=&request_id=&biz_id=102&utm_term=%E5%AE%89%E8%A3%85torch&utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduweb~default-2-141029610.nonecase&spm=1018.2226.3001.4187)\n\n\n* 2、如果你需要自己训练\n\n    * 2.1 下载[数据集下载地址](#数据集下载地址)放到`./dataset`目录下\n\n    * 2.2 `python data_process.py`处理数据集，例如pretrain数据提前进行token-encoder、sft数据集抽离qa到csv文件\n\n    * 2.3 在`./model/LMConfig.py` 中调整model的参数配置\n      > 这里仅需调整dim和n_layers和use_moe参数，分别是`(512+8)`或`(768+16)`，对应于`minimind-v1-small`和`minimind-v1`\n    * 2.4 `python 1-pretrain.py` 执行预训练，得到 `pretrain_*.pth` 作为预训练的输出权重\n    * 2.5 `python 3-full_sft.py` 执行指令微调，得到 `full_sft_*.pth` 作为指令微调的输出权重\n    * 2.6 `python 4-lora_sft.py` 执行lora微调（非必须）\n    * 2.7 `python 5-dpo_train.py` 执行DPO人类偏好强化学习对齐（非必须）\n* 3、测试模型推理效果\n    * 确保需要使用的，训练完成的参数权重`*.pth`文件位于`./out/`目录下\n    * 也可以直接去[训练完成的模型权重](#训练完成的模型权重)下载使用我训练好的`*.pth`权重文件\n       ```text\n      minimind/out\n      ├── multi_chat\n      │   ├── full_sft_512.pth\n      │   ├── full_sft_512_moe.pth\n      │   └── full_sft_768.pth\n      ├── single_chat\n      │   ├── full_sft_512.pth\n      │   ├── full_sft_512_moe.pth\n      │   └── full_sft_768.pth\n      ├── pretrain_768.pth\n      ├── pretrain_512_moe.pth\n      ├── pretrain_512.pth\n      ```\n    * `python 0-eval_pretrain.py`测试预训练模型的接龙效果\n    * `python 2-eval.py`测试模型的对话效果\n      ![2-eval](./images/2-eval.png)\n\n🍭「Tip」预训练和全参微调pretrain和full_sft均支持多卡加速\n\n> 假设你的设备只有1张显卡，使用原生python启动训练即可：\n\n* 执行预训练或指令微调训练\n    ```bash\n    python 1-pretrain.py\n    # and\n    python 3-full_sft.py\n    ```\n\n> 假设你的设备有N (N＞1) 张显卡：\n\n* 单机N卡启动训练(DDP)\n    ```bash\n    torchrun --nproc_per_node N 1-pretrain.py\n    # and\n    torchrun --nproc_per_node N 3-full_sft.py\n    ```\n* 单机N卡启动训练(DeepSpeed)\n    ```bash\n    deepspeed --master_port 29500 --num_gpus=N 1-pretrain.py\n    # and\n    deepspeed --master_port 29500 --num_gpus=N 3-full_sft.py\n    ```\n\n* 开启wandb记录训练过程(非必须)\n    ```bash\n    torchrun --nproc_per_node N 1-pretrain.py --use_wandb\n    # and\n    python 1-pretrain.py --use_wandb\n    ```\n  通过添加`--use_wandb`参数，可以记录训练过程，训练完成后，可以在wandb网站上查看训练过程。通过修改`wandb_project`\n  和`wandb_run_name`参数，可以指定项目名称和运行名称。\n\n# 📌 Data sources\n\n- 🤖 分词器：nlp中的Tokenizer类似于词典，将单词从自然语言通过“词典”映射到0,1,36这样的数字，可以理解为数字就代表了单词在“词典”中的页码。\n  LLM分词器的构建方式有两种：一种是自己构造词表训练一个分词器，代码可见`train_tokenizer.py`；另一种是选择开源模型训练好的分词器。\n  “词典”当然可以直接选择用新华词典或是牛津词典，优点是token转化压缩率很好，但缺点是词表太长，动辄数十万个词汇短语；\n  也可以使用自己训练的分词器，优点是词表随意控制，缺点是压缩率不够理想，且生僻词不容易面面俱到。\n  当然，“词典”的选择很重要，LLM的输出本质上是SoftMax到词典N个词的多分类问题，然后通过“词典”解码到自然语言。\n  因为LLM体积非常小，为了避免模型头重脚轻（词嵌入embedding层参数占整个LLM比太高），所以词表长度需要选择比较小。\n  强大的开源模型例如01万物、千问、chatglm、mistral、Llama3等，它们的tokenizer词表长度如下：\n\n    <table>\n      <tr><th>Tokenizer模型</th><th>词表大小</th><th>来源</th></tr>\n      <tr><td>yi tokenizer</td><td>64,000</td><td>01万物（中国）</td></tr>\n      <tr><td>qwen2 tokenizer</td><td>151,643</td><td>阿里云（中国）</td></tr>\n      <tr><td>glm tokenizer</td><td>151,329</td><td>智谱AI（中国）</td></tr>\n      <tr><td>mistral tokenizer</td><td>32,000</td><td>Mistral AI（法国）</td></tr>\n      <tr><td>llama3 tokenizer</td><td>128,000</td><td>Meta（美国）</td></tr>\n      <tr><td>minimind tokenizer</td><td>6,400</td><td>自定义</td></tr>\n    </table>\n\n  > 👉2024-09-17更新：为了防止过去的版本歧义&控制体积，minimind所有模型均使用minimind_tokenizer分词，废弃所有mistral_tokenizer版本。\n\n  > 尽管minimind_tokenizer长度很小，编解码效率弱于qwen2、glm等中文友好型分词器。\n  > 但minimind模型选择了自己训练的minimind_tokenizer作为分词器，以保持整体参数轻量，避免编码层和计算层占比失衡，头重脚轻，因为minimind的词表大小只有6400。\n  > 且minimind在实际测试中没有出现过生僻词汇解码失败的情况，效果良好。\n  > 由于自定义词表压缩长度到6400，使得LLM总参数量最低只有26M。\n\n---\n\n- 📙【Pretrain数据】：\n  [Seq-Monkey通用文本数据集](https://github.com/mobvoi/seq-monkey-data/blob/main/docs/pretrain_open_corpus.md) / [Seq-Monkey百度网盘](https://pan.baidu.com/s/114F1k3eksiWCOQLvaT3RYQ?pwd=6666)\n  是由多种公开来源的数据（如网页、百科、博客、开源代码、书籍等）汇总清洗而成。整理成统一的JSONL格式，并经过了严格的筛选和去重，确保数据的全面性、规模、可信性和高质量。总量大约在10B\n  token，适合中文大语言模型的预训练。\n\n  > 第2种选择：[SkyPile-150B数据集](https://hf-mirror.com/datasets/Skywork/SkyPile-150B/tree/main/data)\n  的可公开访问部分包含约2.33亿个独立网页，每个网页平均包含1000多个汉字。数据集包括大约1500亿个令牌和620GB的纯文本数据。\n  **如果着急的话**，可以尝试只挑选SkyPile-150B的部分jsonl下载（并在./data_process.py中对文本tokenizer生成*\n  .csv文件），以便快速跑通预训练流程。\n\n---\n\n- 📕【SFT数据】：[匠数大模型SFT数据集](https://www.modelscope.cn/datasets/deepctrl/deepctrl-sft-data)\n  是一个完整、格式统一、安全的大模型训练和研究资源。\n  从网络上的公开数据源收集并整理了大量开源数据集，对其进行了格式统一，数据清洗，\n  包含10M条数据的中文数据集和包含2M条数据的英文数据集。\n  总量大约在3B token，适合中文大语言模型的SFT。\n  数据集整合来源于以下所有数据（仅供参考，因此无需单独下载，仅需下载一个完整的【SFT数据】）：\n    - [BelleGroup/train_3.5M_CN](https://huggingface.co/datasets/BelleGroup/train_3.5M_CN)\n    - [LinkSoul/instruction_merge_set](https://huggingface.co/datasets/LinkSoul/instruction_merge_set)\n    - [stingning/ultrachat](https://huggingface.co/datasets/stingning/ultrachat)\n    - [BAAI/COIG-PC-core](https://huggingface.co/datasets/BAAI/COIG-PC-core)\n    - [shibing624/sharegpt_gpt4](https://huggingface.co/datasets/shibing624/sharegpt_gpt4)\n    - [shareAI/ShareGPT-Chinese-English-90k](https://huggingface.co/datasets/shareAI/ShareGPT-Chinese-English-90k)\n    - [Tiger Research](https://huggingface.co/TigerResearch/sft_zh)\n    - [BelleGroup/school_math_0.25M](https://huggingface.co/datasets/BelleGroup/school_math_0.25M)\n    - [YeungNLP/moss-003-sft-data](https://huggingface.co/datasets/YeungNLP/moss-003-sft-data)\n\n---\n\n- 📘【DPO数据】：大约合并后共8万条dpo数据，人工标注的偏好数据，均来自[活字模型](https://github.com/HIT-SCIR/huozi)\n  ，可以用于训练奖励模型，优化模型回复质量，使其更加符合人类偏好。\n\n---\n\n- 【更多数据集】目前已经有[HqWu-HITCS/Awesome-Chinese-LLM](https://github.com/HqWu-HITCS/Awesome-Chinese-LLM)\n  在收集和梳理中文LLM相关的开源模型、应用、数据集及教程等资料，并持续更新这方面的最新进展。全面且专业，Respect！\n\n---\n\n### 数据集下载地址\n\n下载到`./dataset/`目录下\n\n| MiniMind训练数据集      | 下载地址                                                                                                                                                                                                                       |\n|--------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| **【tokenizer训练集】** | [HuggingFace](https://huggingface.co/datasets/jingyaogong/minimind_dataset/tree/main) / [百度网盘](https://pan.baidu.com/s/1yAw1LVTftuhQGAC1Y9RdYQ?pwd=6666)                                                                   |\n| **【Pretrain数据】**   | [Seq-Monkey官方](http://share.mobvoi.com:5000/sharing/O91blwPkY)  / [百度网盘](https://pan.baidu.com/s/1-Z8Q37lJD4tOKhyBs1D_6Q?pwd=6666) / [HuggingFace](https://huggingface.co/datasets/jingyaogong/minimind_dataset/tree/main) |\n| **【SFT数据】**        | [匠数大模型SFT数据集](https://www.modelscope.cn/datasets/deepctrl/deepctrl-sft-data/resolve/master/sft_data_zh.jsonl)                                                                                                              |\n| **【DPO数据】**        | [Huggingface](https://huggingface.co/datasets/jingyaogong/minimind_dataset/tree/main/dpo)                                                                                                                                  |\n\n# 📌 Model\n\nMiniMind-Dense（和[Llama3.1](https://ai.meta.com/blog/meta-llama-3-1/)一样）使用了Transformer的Decoder-Only结构，跟GPT-3的区别在于：\n\n* 采用了GPT-3的预标准化方法，也就是在每个Transformer子层的输入上进行归一化，而不是在输出上。具体来说，使用的是RMSNorm归一化函数。\n* 用SwiGLU激活函数替代了ReLU，这样做是为了提高性能。\n* 像GPT-Neo一样，去掉了绝对位置嵌入，改用了旋转位置嵌入（RoPE），这样在处理超出训练长度的推理时效果更好。\n\n---\n\nMiniMind-MoE模型，它的结构基于Llama3和[Deepseek-V2](https://arxiv.org/pdf/2405.04434)中的MixFFN混合专家模块。\n\n* DeepSeek-V2在前馈网络（FFN）方面，采用了更细粒度的专家分割和共享的专家隔离技术，以提高Experts的效果。\n\n---\n\nMiniMind的整体结构一致，只是在RoPE计算、推理函数和FFN层的代码上做了一些小调整。\n其结构如下图（重绘版）：\n\n![](./images/LLM-structure.png)\n![](./images/LLM-structure-moe.png)\n\n修改模型配置见[./model/LMConfig.py](./model/LMConfig.py)。\nminimind目前训练的模型版本见下表：\n\n| Model Name        | params | len_vocab | n_layers | d_model | kv_heads | q_heads | share+route | TopK |\n|-------------------|--------|-----------|----------|---------|----------|---------|-------------|------|\n| minimind-v1-small | 26M    | 6400      | 8        | 512     | 8        | 16      | -           | -    |\n| minimind-v1-moe   | 4×26M  | 6400      | 8        | 512     | 8        | 16      | 2+4         | 2    |\n| minimind-v1       | 108M   | 6400      | 16       | 768     | 8        | 16      | -           | -    |\n\n# 📌 Experiment\n\n| Model Name        | params | len_vocab | batch_size | pretrain_time     | sft_single_time   | sft_multi_time      |\n|-------------------|--------|-----------|------------|-------------------|-------------------|---------------------|\n| minimind-v1-small | 26M    | 6400      | 64         | ≈2 hour (1 epoch) | ≈2 hour (1 epoch) | ≈0.5 hour (1 epoch) |\n| minimind-v1-moe   | 4×26M  | 6400      | 40         | ≈6 hour (1 epoch) | ≈5 hour (1 epoch) | ≈1 hour (1 epoch)   |\n| minimind-v1       | 108M   | 6400      | 16         | ≈6 hour (1 epoch) | ≈4 hour (1 epoch) | ≈1 hour (1 epoch)   |\n\n---\n\n1. **预训练(Text-to-Text)**:\n    - LLM首先要学习的并非直接与人交流，而是让肚子中充满知识的墨水，至于墨水理论上喝的越饱越好，产生大量的对世界的认知积累。\n    - 预训练就是让Model先埋头苦学大量基本的知识，例如从维基百科、新闻、常识、书籍等。\n    - 它无监督的从大量的文本数据中压缩知识到自己模型的权重，目的是：学会词语接龙。例如我们输入“秦始皇是”四个字，它在大量学习后能预测出下一句话大概率是“中国的第一位皇帝”。\n   > pretrain的学习率设置为1e-4到1e-5的动态学习率，预训练epoch数设为5。\n    ```bash\n    torchrun --nproc_per_node 2 1-pretrain.py\n    ```\n2. **单轮次对话有监督微调(Single dialog Fine-tuning)**:\n    - 经过预训练，半成品LLM此时已经掌握了几乎所有的语言知识和百科常识。此时它还不会与人聊天，相反它只会无脑地进行输入词语的接龙，生成下一个词。\n    - 此时需要对半成品LLM做限制在聊天模板中进行微调，例如当它遇到这样的模板“<聊天开始>秦始皇是<聊天终止>\n      ”后不再无脑接龙，而是意识到这是一段完整的对话结束。\n    - 我们称这个过程为指令微调，就如同让学富五车的「牛顿」先生适应21世纪的聊天习惯，学习屏幕左侧是对方消息，右侧是本人消息这个规律。\n    - 在训练时，MiniMind的指令和回答长度被截断在512，是为了节省显存空间。就像我们学习时，会先从短的文章开始，当学会阅读200字作文后，800字长文章就不需要再单独学习。\n   > 在推理时通过调整RoPE线性差值，实现长度外推到1024或2048及以上很方便。学习率设置为1e-5到1e-6的动态学习率，微调epoch数为6。\n\n   ```bash\n   # 3-full_sft.py中设置数据集为sft_data_single.csv\n   torchrun --nproc_per_node 2 3-full_sft.py\n   ```\n3. **多轮对话微调(Multi dialog Fine-tuning)**:\n    - 在2的基础上，LLM已经学会一个问题->一个回答的聊天模板。此时仅需在具备历史问答的更长聊天模板上进一步微调即可。\n    - 我们仅需使用数据集的history_chat 字段，即历史对话，以及history_chat_response字段，即历史对话的回答。\n    - 构建【问题->回答，问题->回答，问题->】的新聊天模板，然后使用这个数据集进行微调。\n    - 学习完成的模型不仅仅只能回答当前问题，还能根据历史对话进行连贯的对话。\n    - 这一步 **并非必须** ，因为小模型长上文对话能力很弱，强行对齐多轮问答模板会损失一定程度的单轮SFT效果。\n   > 学习率设置为1e-5到1e-6的动态学习率，微调epoch数为5。\n    ```bash\n    # 3-full_sft.py中设置数据集为sft_data.csv\n    torchrun --nproc_per_node 2 3-full_sft.py\n    ```\n4. **人类反馈强化学习(RLHF)之-直接偏好优化(Direct Preference Optimization, DPO)**:\n    - 在前面的训练中，GPT已经具备了基本的对话能力，但是这样的能力完全基于单词接龙，缺少正例反例的激励。\n    - GPT尚且未知什么回答是好的，什么是差的。我们希望它能够更符合人的偏好，给出更让人满意的回答。\n    - 这个过程就像是让GPT参加工作培训，从优秀员工的作为例子，消极员工作为反例，学习如何更好地服务客户。\n    - RLHF系列中，与PPO(Proximal Policy Optimization)这种需要奖励模型、价值模型的RL算法不同；\n    - DPO通过推导PPO奖励模型的显式解，把在线奖励模型换成离线数据，ref输出可以提前保存。\n    - DPO性能几乎不变，只用跑 actor 和 ref 2 个模型，大大节省显存开销和增加训练稳定性。\n    - 同样的，LLM的RL步骤也 **并非必须**，有利也有弊。\n   > 活字三元组(q,chose,reject)数据集，学习率le-5，半精度fp16,共1个epoch，耗时1h。\n    ```bash\n    python 5-dpo_train.py\n    ```\n\n---\n\n📋关于LLM的参数配置，有一篇很有意思的论文[MobileLLM](https://arxiv.org/pdf/2402.14905)做了详细的研究和实验。\nscaling law在小模型中有自己独特的规律。\n引起Transformer参数成规模变化的参数几乎只取决于`d_model`和`n_layers`。\n\n* `d_model`↑+`n_layers`↓->矮胖子\n* `d_model`↓+`n_layers`↑->瘦高个\n\n2020年提出Scaling Law的论文认为，训练数据量、参数量以及训练迭代次数才是决定性能的关键因素，而模型架构的影响几乎可以忽视。\n然而似乎这个定律对小模型并不完全适用。\nMobileLLM提出架构的深度比宽度更重要，「深而窄」的「瘦长」模型可以学习到比「宽而浅」模型更多的抽象概念。\n例如当模型参数固定在125M或者350M时，30～42层的「狭长」模型明显比12层左右的「矮胖」模型有更优越的性能，\n在常识推理、问答、阅读理解等8个基准测试上都有类似的趋势。\n这其实是非常有趣的发现，因为以往为100M左右量级的小模型设计架构时，几乎没人尝试过叠加超过12层。\n这与MiniMind在训练过程中，模型参数量在`d_model`和`n_layers`之间进行调整实验观察到的效果是一致的。\n然而「深而窄」的「窄」也是有维度极限的，当d_model<512时，词嵌入维度坍塌的劣势非常明显，\n增加的layers并不能弥补词嵌入在固定q_head带来d_head不足的劣势。\n当d_model>1536时，layers的增加似乎比d_model的优先级更高，更能带来具有“性价比”的参数->效果增益。\n因此MiniMind设定small模型的d_model=512，n_layers=8来获取的「极小体积<->更好效果」的平衡。\n设定d_model=768，n_layers=16来获取效果的更大收益，更加符合小模型scaling-law的变化曲线。\n\n\n> 作为参考，GPT3的参数设定见下表：\n\n![gpt3_config.png](./images/gpt3_config.png)\n\n---\n\n### 训练完成的模型权重\n\n[🔗百度网盘](https://pan.baidu.com/s/1KUfSzEkSXYbCCBj0Pw-9fA?pwd=6666)\n\n| Model Name        | params | Config                      | pretrain_model         | single_sft_model                   | multi_sft_model                   | rl_model     |\n|-------------------|--------|-----------------------------|------------------------|------------------------------------|-----------------------------------|--------------|\n| minimind-v1-small | 26M    | d_model=512<br/>n_layers=8  | `pretrain_512.pth`     | `single_chat/full_sft_512.pth`     | `multi_chat/full_sft_512.pth`     | `rl_512.pth` |\n| minimind-v1-moe   | 4×26M  | d_model=512<br/>n_layers=8  | `pretrain_512_moe.pth` | `single_chat/full_sft_512_moe.pth` | `multi_chat/full_sft_512_moe.pth` | -            |\n| minimind-v1       | 108M   | d_model=768<br/>n_layers=16 | `pretrain_768.pth`     | `single_chat/full_sft_768.pth`     | `multi_chat/full_sft_768.pth`     | `rl_768.pth` |\n\n---\n\n# 📌 Eval\n\n## ① RLHF篇\n\n> [!TIP]\n> 测试基于「单轮对话full_sft」和「DPO强化学习对齐」的minimind模型对比。\n\n模型文件[百度网盘](https://pan.baidu.com/s/1KUfSzEkSXYbCCBj0Pw-9fA?pwd=6666)，其中 `rl_<dim>.pth`\n即为「DPO强化学习对齐」后的minimind模型权重。\n\n```text\n[Q]: 你叫什么名字？\n[ft_768]: 我是一款人工智能语言模型，可以回答您的问题和提供帮助。您有什么需要我帮忙的吗？\n[rl_768]: 我是一款基于人工智能技术的自然语言处理系统，主要用于处理和生成人类语言的任务。我可以帮助用户进行文本处理、语音识别、文本生成等任务，同时也可以帮助用户进行智能聊天、翻译、翻译等多种应用。如果您需要更多关于该项目的信息，请告诉我。\n[ft_512]: 我很高兴能够帮助你。你有什么其他问题需要我帮忙解答吗？\n[rl_512]: 我是一款基于自然语言处理技术的聊天机器人，能够理解和回复用户的问题和指令，并进行相应的回复和回复。我被训练为能够与人类进行交互和回答各种问题和任务。请问有什么需要吗？\n\n[Q]: 你是谁？\n[ft_768]: 我是一个人工智能语言模型，没有个人身份和身份。我只是根据输入的指令和文本信息生成响应。\n[rl_768]: 我是一个计算机程序，没有个人身份或身份。我被设计来回答各种问题和提供帮助，包括回答问题、提供建议、进行实验、提供信息等等。我被广泛应用于各种领域，例如自然语言处理、计算机视觉、人工智能等等。如果您需要更多的帮助或信息，请随时与我联系。我会尽我所能来帮助您的工作和生活。\n[ft_512]: 我是一个大型语言模型，可以回答各种各样的问题，包括科学、历史、文化和技术方面的问题。您有什么需要我回答的问题吗？\n[rl_512]: 我是一个计算机程序，没有具体的身世和人物。如果您可以告诉我您想知道的具体信息，我会尽力回答您的问题。\n\n[Q]: 中国有哪些比较好的大学？\n[ft_768]: 中国有很多著名的大学，其中包括北京大学、上海交通大学、复兴中医药学院、浙江大学等。这些大学在不同的领域都有着自己的标志性建筑和研究方向，值得一提的是，它们在中国的顶尖教育资源中享有很高的声誉。\n[rl_768]: 中国的大学有很多，具体情况还要根据你的需求来决定。如果你需要更详细的信息，建议可以去咨询一下你所在城市的大学排名和政策法规，从而选择一个合适的大学。同时，你可以在网上查阅相关信息，或者去咨询一些大学相关机构，如中国人民大学、中国人民大学等，以获得更为详细和专业的指导和建议。\n[ft_512]: 中国有很多优秀的大学，比如麻省理工学院、北京大学、上海大学等等。这些大学在中国教育事业的发展和成就都非常突出。\n[rl_512]: 中国教育局、中国大学、中国大陆大学和中国大陆大学均有优秀的教育体裁，但各地教育水平和专业要求都可能存在差异。中国教育局的学历要求、教学方法、师资力量等因素都各具特点，而大学的学历要求也各有千秋。中国大陆大学生的学历要求也各有不同，需要根据自己的实际情况进行选择。如果您对中国大陆大学生的专业要求有更具体需求，建议您可以通过互联网、国内外的各类机构和机构进行比较和交流。同时，您也可以向身边的人或朋友、老师、家长等人士寻求帮助和建议。希望以上回答对您有帮助。\n\n[Q]: 你知道光速是多少吗？\n[ft_768]: 光速是一个物理常数，表示光在真空中传播的速度，大约是每秒299792458米。光速是物理学中最基本的物理常数之一，也是相对论的重要基础之一。\n[rl_768]: 当然知道，光速是光在真空中传播的速度，通常用c来表示。它是宇宙物理学中非常重要的理论之一。在物理学中，光速被认为是宇宙中最重要的速度之一。它代表着光在真空中传播的速度，这也是许多重要公式和公式的基础。\n[ft_512]: 光速是光在真空中传播的速度，约为每秒299,792,458米。它是几乎所有的物理学概念，也是宇宙中最神秘和最重要的物理概念之一。\n[rl_512]: 光速是光在真空中传播的速度，是自然界中不可逆转的速度。光源是由光的原子、分子、固体和光的组成，其速度约为每秒平方。光的速度是每秒约1个英亞或光年，这个数字是根据相对论、量子力学和引力理论来计算的。光速是相对的概念之一，它代表着我们对自然界中任何事件的理解和解释。\n```\n\n### 👉效果总结\n\n* RLHF数据使用大约10万条；full_sft模型在简洁性和信息准确性方面表现更好；rl模型在回答中提供了更多的背景信息，但信息准确性有待改进。\n* 总的来说RLHF后的模型倾向于学习：说更多有礼貌但无用的废话讨好“对话”本身，而对信息准确性则有轻微损失。\n* 天下没有免费的午餐，还需要继续提升RLHF数据集的质量，也要接受模型能力无法避免的损失(程度有轻重)。\n* DPO和在线PPO的区别在于reject和chosen都是离线准备的，和minimind模型本身的输出必然存在很大的分布差异。\n* 这类似于DPO算法使模型观看乒乓球世界冠军的打法「录像」进行强化学习，而不是像PPO一样请reward模型做「教练」纠正自己的打法强化学习。\n\n## ② Instruct Fine-Tuning 篇\n\n> [!TIP]\n> 以下测试于2024-09-17完成，此日期后发布的新模型，无特殊需要时将不加入测试。\n> 测试基于单轮对话full_sft的minimind模型(无多轮微调和强化学习微调)。\n\n[A] [minimind-v1-small(0.02B)](https://pan.baidu.com/s/1_COe0FQRDmeapSsvArahCA?pwd=6666)<br/>\n[B] [minimind-v1-moe(0.1B)](https://pan.baidu.com/s/1tqB-GMvuiGQBvEl-yZ-oBw?pwd=6666)<br/>\n[C] [minimind-v1(0.1B)](https://pan.baidu.com/s/1p713loS7EfwHQf3G9eYI3Q?pwd=6666)<br/>\n[D] [baby-llama2-chinese(0.2B)](https://github.com/DLLXW/baby-llama2-chinese)<br/>\n[E] [chatlm-mini-chinese(0.2B)](https://github.com/charent/ChatLM-mini-Chinese)<br/>\n\n```text\n[Q]: 你知道长江吗？\n[A]: 是的，长江是中国最长的河流，全长超过3300千米。\n[B]: 长江是中国最长的河流，总长度约为6300公里。它发源于青藏高原的青海湖，流经青海、四川、甘肃、宁夏、内蒙古、陕西和山西。长江流域是中国最大的农业区之一，也是中国的文化和经济中心。\n[C]: 是的，长江是中国最长的河流之一，也是世界第三大河流。长江发源于青藏高原，流经11个省份，最终注入东海。长江流域是中国最大的河流流域之一，拥有着丰富的自然资源和文化遗产，如长江大桥、长江海洋博物馆、长江古城等。\n[D]: 同时也知道，中国是世界上最长的城市之一，拥有超过3400万人口。\n[E]: 是的，长江是中国最长的河流，也是世界上最长的河流之一。它发源于青藏高原，流经九个省区，最终注入东海。长江流域的气候温和，四季分明，是中国重要的农业和工业基地。长江流域的水产资源丰富，是中国的重要渔业资源之一。\n\n\n[Q]: 第一颗人造卫星是哪个国家发射的？\n[A]: 第一颗人造卫星是斯普特尼克1号。\n[B]: 第一颗人造卫星是俄罗斯的国际卫星，于1957年发射。\n[C]: 第一颗人造卫星是苏联发射的，它是由苏联于1957年10月发射的。\n[D]: 这颗名为“阿姆斯特朗”的卫星，于1961年11月20日发射，是第一个成功将人类送入太空的卫星。\n[E]: 第一颗人造卫星是苏联于1957年4月4日发射的，它是由苏联宇航员尤里·加加林（Yuri Gagarin）和苏联工程师罗尔德·谢尔盖耶夫（Roald Skohley）共同发射的。在发射过程中，加加林和谢尔盖耶夫使用了一个名为“斯普特尼克1号”的小型卫星，它的直径为4.3米，重量为1,051公斤。这枚卫星被称为“斯普特尼克1号”，是苏联太空竞赛中的一个里程碑，也是人类历史上第一次在太空中进行太空行走。\n\n\n[Q]: 你知道泰山在哪里吗？\n[A]: 泰山位于中国东部地区，是中国五岳之一。\n[B]: 泰山位于山东省泰安市泰安市。泰安市是中国五岳之一，也是世界文化遗产和中国历史文化名山之一。泰安市有许多著名的景点，如五岳之一、玉皇顶、南天门、玉皇阁等。\n[C]: 泰山位于中国山东省泰安市，是中国五岳之一。\n[D]: 作为一个人工智能助手，我无法访问互联网，因此无法提供关于你所在位置的信息。不过，你可以通过搜索或搜索相关关键词来获取有关你所在位置的信息，或者查看相关新闻或网站来获取有关你所在位置的信息。\n[E]: 泰山位于中国山东省泰安市境内，是中国五岳之一，也是中国文化和历史的重要象征之一。它是中国五岳之一，也是中国道教和佛教的发源地之一。泰山的海拔高度为1,545米，是中国五岳之一，也是中国最著名的旅游景点之一。它的历史可以追溯到公元前5世纪，是中国古代帝王祭祀的场所。泰山也是中国文化的重要组成部分，被誉为“五岳之首”，是中国文化的重要遗产之一。\n\n\n[Q]: 世界上最高的山峰是什么？\n[A]: 世界上最高的山峰是珠穆朗玛峰，位于尼泊尔和中国的边境上，海拔8848米。\n[B]: 世界上最高的山峰是珠穆朗玛峰，位于喜马拉雅山脉，海拔8,848米。\n[C]: 珠穆朗玛峰是世界上最高的山峰，海拔8848米。\n[D]: 世界上最高的山峰是珠穆朗玛峰，位于喜马拉雅山脉，海拔8,848.86米（29,031.69英尺）。\n[E]: 世界上最高的山峰是珠穆朗玛峰，也称为萨加玛塔，位于喜马拉雅山脉的尼泊尔和中国边境之间，海拔8,848.86米（29,031.69英尺）。它是世界上最著名的登山目标之一，吸引着来自世界各地的登山者前来挑战。珠穆朗玛峰的名字来自藏语，意思是“女神的额峰”，它是由两个不同的山峰组成的，其中一个是位于尼泊尔的珠穆朗玛峰，另一个则位于中国。它的高度是通过测量海拔高度来确定的，因为它是通过测量地球的平均高度来确定的。尽管珠穆朗玛峰是世界上最高的山峰，但它并不是地球上任何其他地方都能到达的。地球上的其他地方，如南美洲的安第斯山脉，也是世界上最高的山脉，但它并不是地球上最高的山峰。总之，珠穆朗玛峰是世界上最高的山峰，但它并不是地球上任何地方都能到达的。它是地球上最高的山脉，但它并不是地球上任何地方都能到达的。\n\n\n[Q]: 万有引力是谁提出的？\n[A]: 万有引力是由英国物理学家牛顿提出的。他认为，所有物体都会产生引力，这个引力是由物体的质量和距离决定的。这个理论揭示了质量和距离之间的关系，并证明了万有引力的本质。\n[B]: 万有引力是由牛顿在1687年提出的。它是宇宙中一个基本的物理现象，它描述了物体之间的相互作用。这个力被称为万有引力，它在所有物体之间都存在，这种力与物体的质量和距离有关。万有引力是自然界中最基本的力量之一，它不仅影响着物体的运动和变化，还对宇宙的结构和演化产生着重要的影响。\n[C]: 万有引力是英国物理学家艾萨克·牛顿在17世纪提出的。\n[D]: 引力是宇宙中一种力量，它是由质量或能量所组成的。根据牛顿的万有引力定律，一个物体之间的引力大小与它的质量成正比，与它的质量成反比。因此，牛顿的万有引力定律表明，一个物体之间的引力大小与它的质量成正比，与它的质量成反比。这个定律在物理学、天文学和物理学中都有广泛的应用。\n[E]: 万有引力是由英国物理学家艾萨克·牛顿于1687年提出的。他在1687年发表了一篇著名的论文，题为《自然哲学的数学原理》，其中提出了万有引力定律，指出任何两个物体之间都存在着一种引力，这种引力的大小与它们的质量成正比，与它们之间的距离的平方成反比。这个定律被称为万有引力定律，它描述了质量和距离之间的关系。它表明，两个物体之间的引力与它们的质量成正比，与它们之间的距离的平方成反比。这个定律对于解释宇宙中的许多现象和现象之间的相互作用非常重要，例如行星绕太阳运动、恒星之间的引力透镜效应等等。它也被广泛应用于天文学、物理学和工程学等领域，为我们理解宇宙的本质和演化提供了重要的理论基础。\n\n\n[Q]: 你知道大熊猫的主要食物是什么吗？\n[A]: 大熊猫的主要食物是竹子。它们通常吃竹子，每天需要吃掉大约30公斤的竹子。\n[B]: 是的，大熊猫主要以竹子为食。它们每天需要吃掉大量的竹子来维持生命，每天需要吃掉40至80公斤的竹子。此外，它们也会吃一些其他的植物，如草、果实、坚果和小型哺乳动物。\n[C]: 大熊猫主要以竹子为食，也会偶尔食用其他的植物和动物。此外，大熊猫也会吃一些野果、小动物和植物，但这些都不属于它们的主要食物来源。\n[D]: 大熊猫的主要食物是竹子。\n[E]: 大熊猫是一种以竹子为主要食物的植食动物，它们的肠道结构适合消化纤维素含量高的竹子，但也会吃一些其他植物，如草本植物和小型哺乳动物。大熊猫的肠道结构也适应了高纤维的竹子，这使得它们能够消化大量的纤维素，从而帮助它们在野外生存。大熊猫的肠道中还含有一种叫做“纤维素酶”的酶类物质，这种酶能够帮助大熊猫分解纤维素，从而帮助它们消化大量的水分和营养物质。此外，大熊猫的肠道还含有一种叫做“纤维素蛋白”的蛋白质，这种蛋白质能够帮助大熊猫维持体内的能量水平，从而帮助它们在野外生存。总的来说，大熊猫的主要食物是竹子，但它们也会吃一些其他植物，如草本植物和小型哺乳动物。\n\n\n[Q]: 海水为什么是咸的？\n[A]: 海水中含有许多盐分，这些盐分可以导致海水咸度的增加。海水中的盐分主要来自于海水中的盐分，这些盐分可以从海水中的盐分中获得。海水中的氧化物和盐分可以与海水中的盐分形成氢键，从而使海水咸度增加。\n[B]: 海水是咸的，这是因为它含有大量的盐分，其中包括氯离子和氯离子。这些离子在海洋中形成了大量的盐，这些盐分被称为盐分。海水中含有许多溶解在水中的盐，这些盐分会随着海水的升高而逐渐蒸发，最终导致海水中盐分的蒸发。这些盐分在海水中被完全消耗，然后被排除在海水中。此外，海水中还含有许多其他的盐分，但这些盐分是不同的，因为它们在水中的浮起和下降过程是不同的。海水中的盐分是从海底的岩石中提取出来的，这些盐分在海水中可以被利用。总的来说，海水是咸的，这也是为什么海水中含有许多盐分和盐分的原因。\n[C]: 海水含有许多溶解的盐类，其中最主要的是氯化钠。当水中加入盐时，盐的分子会与水分子发生反应，形成氢氧化钠，这是一种盐。\n[D]: 因为水是许多生物的生存所必需的，它能够吸收和释放二氧化碳，释放氧气，维持大气中的水蒸气，以及维持水循环。水中的盐类和化学物质会释放出氧气，使地球上的生物呼吸。此外，水还具有许多其他功能，如保持水循环、维持水循环、调节气候和环境等。水是许多生物生存所必需的，它也是许多自然现象的重要基础。\n[E]: 海水是咸的主要原因是因为其中含有大量的溶解盐类，其中最主要的是氯化钠（NaCl）。氯化钠是一种非常常见的盐类，它存在于地球上的大部分地区，包括海洋、湖泊、河流和地下水。当水从海洋中蒸发时，它会带走一部分盐分，使海水变得咸。此外，海中还含有其他溶解盐类的物质，如硫酸盐、硫酸镁、硫酸钙、硫酸镁盐、硫酸镁钾、硫酸镁钠、硫酸镁钙、硫酸镁镁钙、硫酸镁镁钙、硫酸镁镁钙、硫酸镁镁钠、硫酸镁镁钙、硫酸镁镁镁钙、硫酸镁镁镁钙、硫酸镁镁镁钙、硫酸镁镁镁钙、硫酸镁镁镁钙、硫酸镁镁镁钙、硫酸镁镁镁钙、硫酸镁镁镁钙、硫酸镁镁镁钙、硫酸镁镁镁钙、硫酸镁镁镁镁钙、硫酸镁镁镁镁钙、硫酸镁镁镁镁钙、硫酸镁镁镁镁钙、硫酸镁镁镁镁钙、硫酸镁镁镁镁钙、硫酸镁镁镁镁镁钙、硫酸镁镁镁镁镁钙、硫酸镁镁镁镁镁钙、硫酸镁镁镁镁镁钙、硫酸镁镁镁镁镁镁钙、硫酸镁镁镁镁镁镁钙、硫酸镁镁镁镁镁镁钙、硫酸镁镁镁镁镁镁镁钙、硫酸镁镁镁镁\n```\n\n> [!NOTE]\n> 🙋‍♂️直接把上述模型的回答丢给GPT-4o，让它帮忙打个分：\n\n---\n\n### 模型表现点评：\n\n1. **模型A**：\n    - **表现**：模型A的回答通常简洁明了，但在某些问题上缺乏详细信息和准确性。例如，在长江的长度问题上，模型A的回答是错误的。\n    - **评分**：60\n\n2. **模型B**：\n    - **表现**：模型B的回答在某些问题上提供了额外的信息，但这些信息有时是不准确的或多余的。例如，在长江的长度问题上，模型B提供了不准确的长度和流域面积。\n    - **评分**：65\n\n3. **模型C**：\n    - **表现**：模型C的回答通常较为详细，且在大多数问题上提供了准确的信息。例如，在长江和泰山的问题上，模型C的回答是准确的。\n    - **评分**：75\n\n4. **模型D**：\n    - **表现**：模型D的回答在某些问题上显得混乱，且缺乏准确性。例如，在泰山的问题上，模型D的回答完全偏离了主题。\n    - **评分**：50\n\n5. **模型E**：\n    - **表现**：模型E的回答通常非常详细，但在某些问题上过于冗长，且包含了一些不必要的信息。例如，在万有引力的问题上，模型E的回答过于复杂。\n    - **评分**：70\n\n#### 排序（从高到低）：\n\n| 模型 | C  | E  | B  | A  | D  |\n|----|----|----|----|----|----|\n| 分数 | 75 | 70 | 65 | 60 | 50 |\n\n---\n\n### 👉效果总结\n\n* minimind系列（ABC）的排序符合直觉，minimind-v1(0.1B)评分最高，常识性问题的回答基本没有错误和幻觉。\n    * 出乎意料的是，minimind-v1-small(0.02B)仅有26M参数，却可以接近minimind-v1(0.1B)的表现。\n    * minimind-v1(0.1B)的sft轮数`epochs`仅有不到2，偷懒提前kill腾出资源给小模型，0.1B没有得到充分训练的情况下依然做到了最强，其实还是底大一级压死人。\n    * minimind-v1-moe(0.1B)表现只比minimind-v1-small(0.02B)\n      略好，同样是因为偷懒早停腾出资源做其它训练了，但是MoE模型这种稀疏多Experts模式需要的训练轮次需要酌情更高，让所有FFN层专家得到路由的激活充分训练，在目前epochs设置为3时训练的还不够充足。\n      minimind在早期实验验证阶段在Yi-Tokenizer上试验过moe的充分训练版本，可以做到比dense小模型表现肉眼可见地更好。此部分可能需要留给日后腾出服务器再训练并更新v2、v3版本。\n\n* E模型的回答肉眼看起来是非常不错的，尽管存在些许幻觉瞎编的情况。但GPT-4o和Deepseek的评分都一致认为它“信息过度冗长，且有重复内容，存在幻觉”。\n  其实这种评价略显严格，100个字中哪怕有10个字是幻觉，就很容易把它归到低分。由于E模型预训练文本长度更长，数据集大得多，所以回答的看起来很完备。在体积近似的情况下，数据数量和质量都很重要。\n\n> 🙋‍♂️个人主观评价：E>C>B≈A>D\n\n> 🤖 GPT-4o 评价：C>E>B>A>D\n\nScaling Law：模型参数越大，训练数据越多模型的性能越强。\n\n# 📌 Objective dataset: C-Eval\n\nC-Eval评测代码见：`./eval_ceval.py`，\n小模型的测评通常为了避免回复格式的难以固定的特点，\n而直接判断`A`,`B`,`C`,`D`四个字母对应token预测概率，取最大的作为回答答案，与标准答案计算正确率。\nminimind模型本身没有使用较大的数据集训练，也没有针对回答选择题的指令做微调，测评结果可以当个参考。\n\n> 例如minimind-small的结果细项：\n\n| Type | 1                          | 2   | 3                     | 4                     | 5                   | 6                  | 7                   | 8                   | 9              | 10                     | 11                    | 12                    | 13             | 14               | 15    | 16                  | 17            | 18                              | 19                  | 20         | 21               | 22                      | 23                 | 24                  | 25      | 26                   | 27                      | 28                      | 29                 | 30                                | 31                | 32                      | 33                                       | 34                    | 35                      | 36              | 37                        | 38                   | 39        | 40                | 41                  | 42                    | 43                     | 44                | 45               | 46             | 47          | 48                    | 49                   | 50                | 51            | 52                      |\n|------|----------------------------|-----|-----------------------|-----------------------|---------------------|--------------------|---------------------|---------------------|----------------|------------------------|-----------------------|-----------------------|----------------|------------------|-------|---------------------|---------------|---------------------------------|---------------------|------------|------------------|-------------------------|--------------------|---------------------|---------|----------------------|-------------------------|-------------------------|--------------------|-----------------------------------|-------------------|-------------------------|------------------------------------------|-----------------------|-------------------------|-----------------|---------------------------|----------------------|-----------|-------------------|---------------------|-----------------------|------------------------|-------------------|------------------|----------------|-------------|-----------------------|----------------------|-------------------|---------------|-------------------------|\n| Data | probability_and_statistics | law | middle_school_biology | high_school_chemistry | high_school_physics | legal_professional | high_school_chinese | high_school_history | tax_accountant | modern_chinese_history | middle_school_physics | middle_school_history | basic_medicine | operating_system | logic | electrical_engineer | civil_servant | chinese_language_and_literature | college_programming | accountant | plant_protection | middle_school_chemistry | metrology_engineer | veterinary_medicine | marxism | advanced_mathematics | high_school_mathematics | business_administration | mao_zedong_thought | ideological_and_moral_cultivation | college_economics | professional_tour_guide | environmental_impact_assessment_engineer | computer_architecture | urban_and_rural_planner | college_physics | middle_school_mathematics | high_school_politics | physician | college_chemistry | high_school_biology | high_school_geography | middle_school_politics | clinical_medicine | computer_network | sports_science | art_studies | teacher_qualification | discrete_mathematics | education_science | fire_engineer | middle_school_geography |\n\n| Type     | 1      | 2      | 3      | 4      | 5      | 6     | 7      | 8      | 9      | 10     | 11     | 12     | 13    | 14     | 15     | 16     | 17     | 18     | 19     | 20     | 21     | 22     | 23     | 24     | 25     | 26     | 27     | 28     | 29     | 30     | 31     | 32     | 33     | 34     | 35     | 36     | 37     | 38     | 39     | 40     | 41     | 42     | 43     | 44     | 45     | 46     | 47     | 48     | 49     | 50     | 51     | 52    |\n|----------|--------|--------|--------|--------|--------|-------|--------|--------|--------|--------|--------|--------|-------|--------|--------|--------|--------|--------|--------|--------|--------|--------|--------|--------|--------|--------|--------|--------|--------|--------|--------|--------|--------|--------|--------|--------|--------|--------|--------|--------|--------|--------|--------|--------|--------|--------|--------|--------|--------|--------|--------|-------|\n| T/A      | 3/18   | 5/24   | 4/21   | 7/19   | 5/19   | 2/23  | 4/19   | 6/20   | 10/49  | 4/23   | 4/19   | 4/22   | 1/19  | 3/19   | 4/22   | 7/37   | 11/47  | 5/23   | 10/37  | 9/49   | 7/22   | 4/20   | 3/24   | 6/23   | 5/19   | 5/19   | 4/18   | 8/33   | 8/24   | 5/19   | 17/55  | 10/29  | 7/31   | 6/21   | 11/46  | 5/19   | 3/19   | 4/19   | 13/49  | 3/24   | 5/19   | 4/19   | 6/21   | 6/22   | 2/19   | 2/19   | 14/33  | 12/44  | 6/16   | 7/29   | 9/31   | 1/12  |\n| Accuracy | 16.67% | 20.83% | 19.05% | 36.84% | 26.32% | 8.70% | 21.05% | 30.00% | 20.41% | 17.39% | 21.05% | 18.18% | 5.26% | 15.79% | 18.18% | 18.92% | 23.40% | 21.74% | 27.03% | 18.37% | 31.82% | 20.00% | 12.50% | 26.09% | 26.32% | 26.32% | 22.22% | 24.24% | 33.33% | 26.32% | 30.91% | 34.48% | 22.58% | 28.57% | 23.91% | 26.32% | 15.79% | 21.05% | 26.53% | 12.50% | 26.32% | 21.05% | 28.57% | 27.27% | 10.53% | 10.53% | 42.42% | 27.27% | 37.50% | 24.14% | 29.03% | 8.33% |\n\n```text\n总题数: 1346  \n总正确数: 316  \n总正确率: 23.48%\n```\n\n---\n\n#### 结果汇总：\n\n| category          | correct  | question_count | accuracy |\n|:------------------|:--------:|:--------------:|:--------:|\n| minimind-v1-small | \t   344\t |      1346      |  25.56%  |\n| minimind-v1       | \t   351\t |      1346      |  26.08%  |\n\n#### 以下来自GPT-4o对minimind表现的瞎猜：\n\n```text\n### 模型擅长的领域：\n1. 高中的化学：正确率为42.11%，是最高的一个领域。说明模型在这方面的知识可能较为扎实。\n2. 离散数学：正确率为37.50%，属于数学相关领域，表现较好。\n3. 教育科学：正确率为37.93%，说明模型在教育相关问题上的表现也不错。\n4. 基础医学：正确率为36.84%，在医学基础知识方面表现也比较好。\n5. 操作系统：正确率为36.84%，说明模型在计算机操作系统方面的表现较为可靠。\n\n### 模型不擅长的领域：\n1. 法律相关：如法律专业（8.70%）和税务会计（20.41%），表现相对较差。\n2. 中学和大学的物理：如中学物理（26.32%）和大学物理（21.05%），模型在物理相关的领域表现不佳。\n3. 高中的政治、地理：如高中政治（15.79%）和高中地理（21.05%），模型在这些领域的正确率较低。\n4. 计算机网络与体系结构：如计算机网络（21.05%）和计算机体系结构（9.52%），在这些计算机专业课程上的表现也不够好。\n5. 环境影响评估工程师：正确率仅为12.90%，在环境科学领域的表现也不理想。\n\n### 总结：\n- 擅长领域：化学、数学（特别是离散数学）、教育科学、基础医学、计算机操作系统。\n- 不擅长领域：法律、物理、政治、地理、计算机网络与体系结构、环境科学。\n\n这表明模型在涉及逻辑推理、基础科学和一些工程技术领域的问题上表现较好，但在人文社科、环境科学以及某些特定专业领域（如法律和税务）上表现较弱。如果要提高模型的性能，可能需要加强它在人文社科、物理、法律、以及环境科学等方面的训练。\n```\n\n# 📌 Others\n\n### 推理与导出\n\n* [./export_model.py](./export_model.py)可以导出模型到transformers格式，推送到huggingface\n\n* MiniMind的huggingface集合地址：\n  [MiniMind](https://huggingface.co/collections/jingyaogong/minimind-66caf8d999f5c7fa64f399e5)\n\n---\n\n### API推理\n\n* [my_openai_api.py](./my_openai_api.py)完成了openai_api的聊天接口，方便将自己的模型接入第三方UI\n  例如fastgpt、OpenWebUI等\n\n* 从[Huggingface](https://huggingface.co/collections/jingyaogong/minimind-66caf8d999f5c7fa64f399e5)下载模型权重文件\n    ```\n    minimind (root dir)\n    ├─minimind\n    |  ├── config.json\n    |  ├── generation_config.json\n    |  ├── LMConfig.py\n    |  ├── model.py\n    |  ├── pytorch_model.bin\n    |  ├── special_tokens_map.json\n    |  ├── tokenizer_config.json\n    |  ├── tokenizer.json\n    ```\n\n* 启动聊天服务端\n    ```bash\n    python my_openai_api.py\n    ```\n* 测试服务接口\n    ```bash\n    python chat_openai_api.py\n    ```\n* API接口示例，兼容openai api格式\n    ```bash\n    curl http://ip:port/v1/chat/completions \\\n      -H \"Content-Type: application/json\" \\\n      -d '{ \n        \"model\": \"model-identifier\",\n        \"messages\": [ \n          { \"role\": \"user\", \"content\": \"世界上最高的山是什么？\" }\n        ], \n        \"temperature\": 0.7, \n        \"max_tokens\": -1,\n        \"stream\": true\n    }'\n    ```\n\n![images](./images/logger.png)\n\n### 在fastgpt中接入使用minimind api\n\n![images](./images/fastgpt.png)\n\n# 📌 Acknowledge\n\n> [!TIP]\n> 如果您觉得 `MiniMind`对您有所帮助，可以在 GitHub 上加一个⭐<br/>\n> 篇幅不短水平有限难免纰漏，欢迎在Issues交流指正或提交PR改进项目<br/>\n> 您的支持就是持续改进项目的动力\n\n> [!NOTE]\n> 众人拾柴火焰高\n> 如果您已经尝试训练了新的MiniMind型号，欢迎在Discussions或Issues中分享您的模型权重<br/>\n> 可以是在特定下游任务或垂直领域（例如情感识别、医疗、心理、金融、法律问答等）的MiniMind新模型版本<br/>\n> 也可以是拓展训练后（例如探索更长文本序列、更大体积（0.1B+）或更大的数据集）的MiniMind新模型版本<br/>\n> 任何分享都视作独一无二的，所有尝试都具有价值，并受到鼓励<br/>\n> 这些贡献都会被及时发现并整理在鸣谢列表中，再次感谢所有支持！\n\n## 🤝[贡献者](https://github.com/jingyaogong/minimind/graphs/contributors)\n\n<!--\n<a href=\"https://github.com/jingyaogong/minimind/graphs/contributors\">\n  <img src=\"https://contrib.rocks/image?repo=jingyaogong/minimind&v3\" />\n</a>\n-->\n\n<a href=\"https://github.com/jingyaogong\"><img src=\"https://avatars.githubusercontent.com/u/62287848\" width=\"70px\" height=\"70px\"/></a>\n&nbsp;\n<a href=\"https://github.com/MuWinds\"><img src=\"https://avatars.githubusercontent.com/u/93832089\" width=\"70px\" height=\"70px\"/></a>\n&nbsp;\n<a href=\"https://github.com/chuanzhubin\"><img src=\"https://avatars.githubusercontent.com/u/2813798\" width=\"70px\" height=\"70px\"/></a>\n&nbsp;\n<a href=\"https://github.com/iomgaa-ycz\"><img src=\"https://avatars.githubusercontent.com/u/124225682\" width=\"70px\" height=\"70px\"/></a>\n&nbsp;\n\n## 😊鸣谢\n\n<a href=\"https://github.com/ipfgao\"><b>@ipfgao</b></a>:\n<a href=\"https://github.com/jingyaogong/minimind/issues/26\">🔗训练步骤记录</a>\n\n<a href=\"https://github.com/chuanzhubin\"><b>@chuanzhubin</b></a>:\n<a href=\"https://github.com/jingyaogong/minimind/pull/34\">🔗代码逐行注释</a>\n\n<a href=\"https://github.com/WangRongsheng\"><b>@WangRongsheng</b></a>:\n<a href=\"https://github.com/jingyaogong/minimind/issues/39\">🔗大型数据集预处理</a>\n\n<a href=\"https://github.com/pengqianhan\"><b>@pengqianhan</b></a>:\n<a href=\"https://github.com/jingyaogong/minimind/issues/73\">🔗一个简明教程</a>\n\n<a href=\"https://github.com/RyanSunn\"><b>@RyanSunn</b></a>:\n<a href=\"https://github.com/jingyaogong/minimind/issues/75\">🔗推理过程学习记录</a>\n\n<details close> \n<summary> <b>参考链接 & 感谢以下优秀的论文或项目</b> </summary>\n\n- 排名不分任何先后顺序\n- [https://github.com/meta-llama/llama3](https://github.com/meta-llama/llama3)\n- [https://github.com/karpathy/llama2.c](https://github.com/karpathy/llama2.c)\n- [https://github.com/DLLXW/baby-llama2-chinese](https://github.com/DLLXW/baby-llama2-chinese)\n- [(DeepSeek-V2)https://arxiv.org/abs/2405.04434](https://arxiv.org/abs/2405.04434)\n- [https://github.com/charent/ChatLM-mini-Chinese](https://github.com/charent/ChatLM-mini-Chinese)\n- [https://github.com/wdndev/tiny-llm-zh](https://github.com/wdndev/tiny-llm-zh)\n- [(Mistral-MoE)https://arxiv.org/pdf/2401.04088](https://arxiv.org/pdf/2401.04088)\n- [https://github.com/Tongjilibo/build_MiniLLM_from_scratch](https://github.com/Tongjilibo/build_MiniLLM_from_scratch)\n- [https://github.com/jzhang38/TinyLlama](https://github.com/jzhang38/TinyLlama)\n- [https://github.com/AI-Study-Han/Zero-Chatgpt](https://github.com/AI-Study-Han/Zero-Chatgpt)\n- [https://github.com/xusenlinzy/api-for-open-llm](https://github.com/xusenlinzy/api-for-open-llm)\n- [https://github.com/HqWu-HITCS/Awesome-Chinese-LLM](https://github.com/HqWu-HITCS/Awesome-Chinese-LLM)\n\n</details>\n\n## 🫶支持者\n\n<a href=\"https://github.com/jingyaogong/minimind/stargazers\">\n    <picture>\n      <source media=\"(prefers-color-scheme: dark)\" srcset=\"https://reporoster.com/stars/dark/jingyaogong/minimind\"/>\n      <source media=\"(prefers-color-scheme: light)\" srcset=\"https://reporoster.com/stars/jingyaogong/minimind\"/>\n      <img alt=\"github contribution grid snake animation\" src=\"https://reporoster.com/stars/jingyaogong/minimind\"/>\n    </picture>\n</a>\n\n<a href=\"https://github.com/jingyaogong/minimind/network/members\">\n    <picture>\n      <source media=\"(prefers-color-scheme: dark)\" srcset=\"https://reporoster.com/forks/dark/jingyaogong/minimind\"/>\n      <source media=\"(prefers-color-scheme: light)\" srcset=\"https://reporoster.com/forks/jingyaogong/minimind\"/>\n      <img alt=\"github contribution grid snake animation\" src=\"https://reporoster.com/forks/jingyaogong/minimind\"/>\n    </picture>\n</a>\n\n<picture>\n  <source media=\"(prefers-color-scheme: dark)\" srcset=\"https://api.star-history.com/svg?repos=jingyaogong/minimind&type=Date&theme=dark\"/>\n  <source media=\"(prefers-color-scheme: light)\" srcset=\"https://api.star-history.com/svg?repos=jingyaogong/minimind&type=Date\"/>\n  <img alt=\"Star History Chart\" src=\"https://api.star-history.com/svg?repos=jingyaogong/minimind&type=Date\"/>\n</picture>\n\n# License\n\nThis repository is licensed under the [Apache-2.0 License](LICENSE).\n\n\n"
        },
        {
          "name": "README_en.md",
          "type": "blob",
          "size": 63.78125,
          "content": "<div align=\"center\">\n\n![logo](./images/logo.png)\n\n</div>\n\n<div align=\"center\">\n\n![visitors](https://visitor-badge.laobi.icu/badge?page_id=jingyaogong/minimind)\n[![GitHub Repo stars](https://img.shields.io/github/stars/jingyaogong/minimind?style=social)](https://github.com/jingyaogong/minimind/stargazers)\n[![GitHub Code License](https://img.shields.io/github/license/jingyaogong/minimind)](LICENSE)\n[![GitHub last commit](https://img.shields.io/github/last-commit/jingyaogong/minimind)](https://github.com/jingyaogong/minimind/commits/master)\n[![GitHub pull request](https://img.shields.io/badge/PRs-welcome-blue)](https://github.com/jingyaogong/minimind/pulls)\n[![Collection](https://img.shields.io/badge/🤗-MiniMind%20%20Collection-blue)](https://huggingface.co/collections/jingyaogong/minimind-66caf8d999f5c7fa64f399e5)\n\n\n</div>\n\n<div align=\"center\">\n  <h3>\"The Greatest Path is the Simplest\"</h3>\n</div>\n\n<div align=\"center\">\n\n[中文](./README.md) | English\n\n</div>\n\n* This open-source project aims to train a tiny language model called **MiniMind** from scratch in just 3 hours, with a\n  model size of only 26.88M.\n\n* **MiniMind** is extremely lightweight, with the smallest version being approximately $\\frac{1}{7000}$ the size of\n  GPT3, making it possible for even an ordinary personal GPU to perform quick inference and even training.\n\n* **MiniMind** provides the full-stage code for a simplified large model structure, dataset cleaning and preprocessing,\n  supervised pretraining, supervised instruction fine-tuning (SFT), low-rank adaptation (LoRA) fine-tuning, and direct\n  preference alignment with reinforcement learning without rewards (DPO). It also includes code for expanding to sparse\n  models with mixed experts (MoE) and multi-modal vision language models (\n  VLM): [MiniMind-V](https://github.com/jingyaogong/minimind-v).\n\n* This is not just an implementation of an open-source model but also a tutorial for getting started with large language\n  models (LLM).\n\n* We hope this project will serve as an introductory example for researchers, helping them quickly get started and\n  inspiring more exploration and innovation in the LLM field.\n\n> To avoid misinterpretation, \"fastest 3 hours\" means you need a machine with hardware configuration superior to mine.\n> Detailed specifications will be provided below.\n\n---\n\n<div align=\"center\">\n\n![streamlit](./images/streamlit.gif)\n\n[ModelScope Online Testing](https://www.modelscope.cn/studios/gongjy/minimind) | [Bilibili Video Link](https://www.bilibili.com/video/BV12dHPeqE72/?share_source=copy_web&vd_source=670c2504f88726f8cf4a21ef6147c0e8)\n\n---\n\n</div>\n\n# 📌 Introduction\n\nIn the field of large language models (LLMs) such as GPT, LLaMA, GLM, etc., while their performance is impressive, the\nmassive model parameters—often in the range of 10 billion—make them difficult to train or even infer on personal devices\nwith limited memory. Most users do not settle for merely fine-tuning large models using methods like LoRA to learn a few\nnew instructions. It's akin to teaching Newton to use a 21st-century smartphone, which is far removed from the essence\nof learning physics itself.\n\nAdditionally, the abundance of flawed, superficial AI tutorials offered by subscription-based marketing accounts\nexacerbates the problem of finding quality content to understand LLMs, severely hindering learners.\n\nTherefore, the goal of this project is to lower the barrier to entry for working with LLMs as much as possible, by\ntraining an extremely lightweight language model from scratch.\n\n> [!CAUTION]\n> As of 2024-09-17, MiniMind has trained three model versions, with the smallest model requiring only 26M (0.02B)\n> parameters to achieve smooth conversational abilities!\n\n| Model (Size)            | Tokenizer Length | Inference Memory Usage | Release Date | Subjective Rating (/100) |\n|-------------------------|------------------|------------------------|--------------|--------------------------|\n| minimind-v1-small (26M) | 6400             | 0.5 GB                 | 2024.08.28   | 50'                      |\n| minimind-v1-moe (4×26M) | 6400             | 1.0 GB                 | 2024.09.17   | 55'                      |\n| MiniMind-V1 (108M)      | 6400             | 1.0 GB                 | 2024.09.01   | 60'                      |\n\n> This analysis was run on an RTX 3090 GPU with Torch 2.1.2, CUDA 12.2, and Flash Attention 2.\n\nThe project includes:\n\n- Public MiniMind model code (including Dense and MoE models), code for Pretrain, SFT instruction fine-tuning, LoRA\n  fine-tuning, and DPO preference optimization, along with datasets and sources.\n- Compatibility with popular frameworks such as `transformers`, `accelerate`, `trl`, and `peft`.\n- Training support for single-GPU and multi-GPU setups(DDP、DeepSpeed), Use wandb to visualize the training process. The\n  training process allows for stopping and resuming at any point.\n- Code for testing the model on the Ceval dataset.\n- Implementation of a basic chat interface compatible with OpenAI's API, facilitating integration into third-party Chat\n  UIs (such as FastGPT, Open-WebUI, etc.).\n\nWe hope this open-source project helps LLM beginners get started quickly!\n\n### 👉**Recent Updates**\n\n<details close> \n<summary> <b>2024-10-05 (newest 🎉)</b> </summary>\n\n- Added visual capabilities to MiniMind-V(ision)\n\n- Check out the twin project [minimind-v](https://github.com/jingyaogong/minimind-v) for more details!\n\n</details>\n\n<details close> \n<summary> <b>2024-09-27</b> </summary>\n\n- 👉Updated the preprocessing method for the pretrain dataset on 09-27 to ensure text integrity, opting to abandon the\n  preprocessing into .bin training format (slightly sacrificing training speed).\n\n- The current filename for the pretrain data after preprocessing is: pretrain_data.csv.\n\n- Removed some redundant code.\n\n</details>\n\n<details close> \n<summary> <b>2024-09-17 (new🎉)</b> </summary>\n\n- Updated the minimind-v1-moe model\n- To prevent ambiguity, all mistral_tokenizer versions have been removed, and a custom minimind_tokenizer is now used as\n  the tokenizer.\n\n</details>\n\n<details close>\n<summary> <b>2024-09-01</b> </summary>\n\n- Updated the MiniMind-V1 (108M) model, using minimind_tokenizer with 3 pre-training epochs and 10 SFT epochs for more\n  thorough training and improved performance.\n\n- The project has been deployed to ModelScope's Creative Space and can be experienced on the website:\n\n- [ModelScope Online Experience](https://www.modelscope.cn/studios/gongjy/minimind)\n\n</details>\n\n<details close> \n<summary> <b>2024-08-27</b> </summary>\n\n- The project was open-sourced for the first time.\n\n</details>\n\n# 📌 Environment\n\nThese are my personal software and hardware environment configurations. Please adjust according to your own setup:\n\n```bash\nCPU: Intel(R) Core(TM) i9-10980XE CPU @ 3.00GHz\nMemory: 128 GB\nGPU: NVIDIA GeForce RTX 3090 (24GB) * 2\nEnvironment: python 3.9 + Torch 2.1.2 + DDP multi-GPU training\n```\n\n* Ubuntu == 20.04\n* Python == 3.9\n* Pytorch == 2.1.2\n* CUDA == 12.2\n* [requirements.txt](./requirements.txt)\n\n# 📌 Quick Start Test\n\n<div align=\"center\" style=\"font-size: 1.5em; font-weight: bold;\">\n  <img src=\"https://huggingface.co/front/assets/huggingface_logo-noborder.svg\" alt=\"Hugging Face Logo\" style=\"vertical-align: middle; height: 30px;\" />\n  Hugging Face\n\n[MiniMind (HuggingFace)](https://huggingface.co/collections/jingyaogong/minimind-66caf8d999f5c7fa64f399e5)\n\n <img src=\"https://g.alicdn.com/sail-web/maas/1.15.0/static/modelscopeIcon.cd89353f.svg\" alt=\"Hugging Face Logo\" style=\"vertical-align: middle; height: 30px;\" />\n\n[MiniMind (ModelScope)](https://www.modelscope.cn/models/gongjy/MiniMind-V1)\n\n</div>\n\n```bash\n# step 1\ngit clone https://huggingface.co/jingyaogong/minimind-v1\n```\n\n```bash\n# step 2\npython 2-eval.py\n```\n\nor you can run streamlit, launch a web page to chat with minimind-v1\n> Requires python>=3.10, install `pip install streamlit==1.27.2`\n\n```bash\n# or step 3, use streamlit\nstreamlit run fast_inference.py\n```\n\n# 📌 Quick Start Train\n\n* 0.Clone the project code\n\n  ```text\n  git clone https://github.com/jingyaogong/minimind.git\n  cd minimind\n  ```\n\n* 1.Install the required dependencies\n\n  ```bash\n    pip install -r requirements.txt\n  ```\n\n  ```text\n  # Test if torch can use CUDA\n  import torch\n  print(torch.cuda.is_available())\n  ```\n\n  > If it is not available, please go to [torch_stable](https://download.pytorch.org/whl/torch_stable.html)\n  to download the whl file for installation. Refer\n  to [this link](https://blog.csdn.net/weixin_45456738/article/details/141029610?ops_request_misc=&request_id=&biz_id=102&utm_term=安装torch&utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduweb~default-2-141029610.nonecase&spm=1018.2226.3001.4187)\n\n* 2.If you need to train the model yourself\n\n    * 2.1 Download the [dataset download link](#dataset-download-links) and place it in the `./dataset` directory.\n\n    * 2.2 Run `python data_process.py` to process the dataset, such as token-encoding pretrain data and extracting QA\n      data to CSV files for the SFT dataset.\n\n    * 2.3 Adjust the model parameter configuration in `./model/LMConfig.py`.\n    * 2.4 Execute pretraining with `python 1-pretrain.py`.\n    * 2.5 Perform instruction fine-tuning with `python 3-full_sft.py`.\n    * 2.6 Perform LoRA fine-tuning (optional) with `python 4-lora_sft.py`.\n    * 2.7 Execute DPO human preference reinforcement learning alignment (optional) with `python 5-dpo_train.py`.\n\n* 3.Test model inference performance\n\n    * Ensure that the required trained parameter weights are located in the `./out/` directory.\n    * You can also directly download and use the trained model weights\n      from [Trained Model Weights](#Trained Model Weights).\n       ```text\n      out\n      ├── multi_chat\n      │   ├── full_sft_512.pth\n      │   ├── full_sft_512_moe.pth\n      │   └── full_sft_768.pth\n      ├── single_chat\n      │   ├── full_sft_512.pth\n      │   ├── full_sft_512_moe.pth\n      │   └── full_sft_768.pth\n      ├── pretrain_768.pth\n      ├── pretrain_512_moe.pth\n      ├── pretrain_512.pth\n      ```\n\n    * Test the pretraining model's chain effect with `python 0-eval_pretrain.py`\n    * Test the model's conversational effect with `python 2-eval.py`\n      ![2-eval](./images/2-eval.png)\n\n🍭「Tip」Both pretrain and full_sft support multi-card acceleration.\n\n> If your device has only 1 GPU, you can start the training using native Python:\n\n* Execute pretrain or instruction fine-tuning:\n    ```bash\n    python 1-pretrain.py\n    # and\n    python 3-full_sft.py\n    ```\n  \n> If your device has N (N > 1) GPUs:\n\n* Start training on a single machine with N GPUs(DDP)\n    ```bash\n    torchrun --nproc_per_node N 1-pretrain.py\n    # and\n    torchrun --nproc_per_node N 3-full_sft.py\n    ```\n* Start training on a single machine with N GPUs(DeepSpeed)\n    ```bash\n    deepspeed --master_port 29500 --num_gpus=N 1-pretrain.py\n    # and\n    deepspeed --master_port 29500 --num_gpus=N 3-full_sft.py\n    ```\n* Record the training process\n    ```bash\n    torchrun --nproc_per_node N 1-pretrain.py --use_wandb\n    # and\n    python 1-pretrain.py --use_wandb\n    ```\n  By adding the `--use_wandb` parameter, you can record the training process. After training is complete, you can view\n  the training process on the wandb website. You can specify the project name and run name by modifying\n  the `wandb_project` and `wandb_run_name` parameters.\n\n# 📌 Data sources\n\n- 🤖 Tokenizer: In NLP, a Tokenizer is similar to a dictionary, mapping words from natural language to numbers like 0, 1,\n  36, etc., which can be understood as page numbers in the \"dictionary\" representing words. There are two ways to build\n  an LLM tokenizer: one is to create a vocabulary and train a tokenizer yourself, as seen in `train_tokenizer.py`; the\n  other is to use a pre-trained tokenizer from an open-source model.\n\n  You can use a standard dictionary like Xinhua or Oxford. The advantage is that token conversion has a good compression\n  rate, but the downside is that the vocabulary can be very large, with tens of thousands of words and phrases.\n  Alternatively, you can use a custom-trained tokenizer. The advantage is that you can control the vocabulary size, but\n  the compression rate may not be ideal, and rare words might be missed.\n\n  The choice of \"dictionary\" is crucial. The output of an LLM is essentially a multi-class classification problem over N\n  words in the dictionary, which is then decoded back into natural language. Because LLMs are very small, to avoid the\n  model being top-heavy (with the embedding layer's parameters taking up too much of the model), the vocabulary length\n  should be kept relatively small.\n\n  Powerful open-source models like 01万物, 千问, chatglm, mistral, and Llama3 have the following tokenizer vocabulary\n  sizes:\n    <table>\n      <tr><th>Tokenizer Model</th><th>Vocabulary Size</th><th>Come from</th></tr>\n      <tr><td>yi tokenizer</td><td>64,000</td><td>01-AI（China）</td></tr>\n      <tr><td>qwen2 tokenizer</td><td>151,643</td><td>Alibaba Cloud（China）</td></tr>\n      <tr><td>glm tokenizer</td><td>151,329</td><td>Zhipu AI（China）</td></tr>\n      <tr><td>mistral tokenizer</td><td>32,000</td><td>Mistral AI（China）</td></tr>\n      <tr><td>llama3 tokenizer</td><td>128,000</td><td>Meta（China）</td></tr>\n      <tr><td>minimind tokenizer</td><td>6,400</td><td>Custom</td></tr>\n    </table>\n\n  > 👉Update on 2024-09-17: To avoid ambiguity from previous versions and control the model size, all Minimind models now\n  use the Minimind_tokenizer for tokenization, and all versions of the Mistral_tokenizer have been deprecated.\n\n  > Although the Minimind_tokenizer has a small length and its encoding/decoding efficiency is weaker compared to\n  Chinese-friendly tokenizers like Qwen2 and GLM, the Minimind models have opted for their custom-trained\n  Minimind_tokenizer to maintain a lightweight parameter structure and prevent an imbalance between encoding and\n  computation layers. This is because the Minimind vocabulary size is only 6,400.\n  > Moreover, Minimind has not encountered any issues with decoding rare words in practical tests, and the performance\n  has been satisfactory. Due to the custom vocabulary being compressed to 6,400 tokens, the total parameter size of the\n  LLM is minimized to only 26M.\n\n---\n\n- 📙 **[Pretrain Data](https://github.com/mobvoi/seq-monkey-data/blob/main/docs/pretrain_open_corpus.md)**:\n  The [Seq-Monkey General Text Dataset](https://github.com/mobvoi/seq-monkey-data/blob/main/docs/pretrain_open_corpus.md) / [Baidu](https://pan.baidu.com/s/114F1k3eksiWCOQLvaT3RYQ?pwd=6666)\n  is a collection of data from various public sources such as websites, encyclopedias, blogs, open-source code, books,\n  etc. It has been compiled, cleaned, and organized into a unified JSONL format, with rigorous filtering and\n  deduplication to ensure data comprehensiveness, scale, reliability, and high quality. The total amount is\n  approximately 10B tokens, suitable for pretraining Chinese large language models.\n\n---\n\n- 📕 **[SFT Data](https://www.modelscope.cn/datasets/deepctrl/deepctrl-sft-data)**:\n  The [Jiangshu Large Model SFT Dataset](https://www.modelscope.cn/datasets/deepctrl/deepctrl-sft-data) is a\n  comprehensive, uniformly formatted, and secure resource for large model training and research. It includes a large\n  amount of open-source data collected and organized from publicly available online sources, with format unification and\n  data cleaning. It comprises a Chinese dataset with 10M entries and an English dataset with 2M entries. The total\n  amount is approximately 3B tokens, suitable for SFT of Chinese large language models. The dataset integration includes\n  all data from the following sources (for reference only, no need to download separately, just download the\n  complete [SFT Data]):\n\n    - [BelleGroup/train_3.5M_CN](https://huggingface.co/datasets/BelleGroup/train_3.5M_CN)\n    - [LinkSoul/instruction_merge_set](https://huggingface.co/datasets/LinkSoul/instruction_merge_set)\n    - [stingning/ultrachat](https://huggingface.co/datasets/stingning/ultrachat)\n    - [BAAI/COIG-PC-core](https://huggingface.co/datasets/BAAI/COIG-PC-core)\n    - [shibing624/sharegpt_gpt4](https://huggingface.co/datasets/shibing624/sharegpt_gpt4)\n    - [shareAI/ShareGPT-Chinese-English-90k](https://huggingface.co/datasets/shareAI/ShareGPT-Chinese-English-90k)\n    - [Tiger Research](https://huggingface.co/TigerResearch/sft_zh)\n    - [BelleGroup/school_math_0.25M](https://huggingface.co/datasets/BelleGroup/school_math_0.25M)\n    - [YeungNLP/moss-003-sft-data](https://huggingface.co/datasets/YeungNLP/moss-003-sft-data)\n- 📘 **DPO Data**: Approximately 80,000 DPO (Direct Preference Optimization) data entries, which are manually labeled\n  preference data, come from [Huozi Model](https://github.com/HIT-SCIR/huozi). These can be used to train reward models\n  to optimize response quality and align more closely with human preferences.\n\n---\n\n- **More Datasets**: [HqWu-HITCS/Awesome-Chinese-LLM](https://github.com/HqWu-HITCS/Awesome-Chinese-LLM) is currently\n  collecting and organizing open-source models, applications, datasets, and tutorials related to Chinese LLMs, with\n  continuous updates on the latest developments in this field. Comprehensive and professional, respect!\n\n---\n\n### Dataset Download Links\n\n| MiniMind Training Dataset | Download Link                                                                                                                                             |\n|---------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------|\n| **[tokenizer Data]**      | [HuggingFace](https://huggingface.co/datasets/jingyaogong/minimind_dataset/tree/main) / [Baidu](https://pan.baidu.com/s/1yAw1LVTftuhQGAC1Y9RdYQ?pwd=6666) |\n| **[Pretrain Data]**       | [Seq-Monkey General Text Dataset](http://share.mobvoi.com:5000/sharing/O91blwPkY) / [Baidu](https://pan.baidu.com/s/114F1k3eksiWCOQLvaT3RYQ?pwd=6666)     |\n| **[SFT Data]**            | [Jiangshu Large Model SFT Dataset](https://www.modelscope.cn/datasets/deepctrl/deepctrl-sft-data/resolve/master/sft_data_zh.jsonl)                        |\n| **[DPO Data]**            | [Huggingface](https://huggingface.co/datasets/jingyaogong/minimind_dataset/tree/main/dpo)                                                                 |\n\n# 📌 Model\n\nMiniMind-Dense (like [Llama3.1](https://ai.meta.com/blog/meta-llama-3-1/)) uses a Transformer Decoder-Only architecture.\nThe differences from GPT-3 are:\n\n* It employs GPT-3's pre-normalization method, which normalizes the input of each Transformer sub-layer rather than the\n  output. Specifically, it uses the RMSNorm normalization function.\n* It replaces ReLU with the SwiGLU activation function to enhance performance.\n* Like GPT-Neo, it omits absolute position embeddings in favor of Rotary Position Embeddings (RoPE), which improves\n  performance for inference beyond the training length.\n\n---\n\nThe MiniMind-MoE model is based on the MixFFN mixture-of-experts module from Llama3\nand [DeepSeek-V2](https://arxiv.org/pdf/2405.04434).\n\n* DeepSeek-V2 adopts more granular expert partitioning and shared expert isolation techniques in the feed-forward\n  network (FFN) to improve the performance of experts.\n\n---\n\nThe overall structure of MiniMind remains consistent, with minor adjustments in RoPE calculations, inference functions,\nand FFN layer code. The structure is illustrated in the figure below (redrawn):\n\n![](./images/LLM-structure.png)\n![](./images/LLM-structure-moe.png)\nModel configurations can be found in [./model/LMConfig.py](./model/LMConfig.py). The model types and parameters are\nshown in the table below:\n\n| Model Name        | params | len_vocab | n_layers | d_model | kv_heads | q_heads | share+route | TopK |\n|-------------------|--------|-----------|----------|---------|----------|---------|-------------|------|\n| minimind-v1-small | 26M    | 6400      | 8        | 512     | 8        | 16      | -           | -    |\n| minimind-v1-moe   | 4×26M  | 6400      | 8        | 512     | 8        | 16      | 2+4         | 2    |\n| minimind-v1       | 108M   | 6400      | 16       | 768     | 8        | 16      | -           | -    |\n\n# 📌 Experiment\n\n| Model Name        | params | len_vocab | batch_size | pretrain_time     | sft_single_time   | sft_multi_time      |\n|-------------------|--------|-----------|------------|-------------------|-------------------|---------------------|\n| minimind-v1-small | 26M    | 6400      | 64         | ≈2 hour (1 epoch) | ≈2 hour (1 epoch) | ≈0.5 hour (1 epoch) |\n| minimind-v1-moe   | 4×26M  | 6400      | 40         | ≈6 hour (1 epoch) | ≈5 hour (1 epoch) | ≈1 hour (1 epoch)   |\n| minimind-v1       | 108M   | 6400      | 16         | ≈6 hour (1 epoch) | ≈4 hour (1 epoch) | ≈1 hour (1 epoch)   |\n\n---\n\n1. **Pretraining (Text-to-Text)**:\n    - LLMs first need to absorb a vast amount of knowledge, much like filling a well with ink. The more \"ink\" it has,\n      the better its understanding of the world will be.\n    - Pretraining involves the model learning a large amount of basic knowledge from sources such as Wikipedia, news\n      articles, common knowledge, books, etc.\n    - It unsupervisedly compresses knowledge from vast text data into its model weights with the aim of learning word\n      sequences. For instance, if we input “Qin Shi Huang is,” after extensive training, the model can predict that the\n      next probable sentence is “the first emperor of China.”\n   > The learning rate for pretraining is set dynamically between 1e-4 and 1e-5, with 2 epochs and a training time of\n   less than one day.\n    ```bash\n    torchrun --nproc_per_node 2 1-pretrain.py\n    ```\n\n2. **Single Dialog Fine-Tuning**:\n    - After pretraining, the semi-finished LLM has almost all language knowledge and encyclopedic common sense. At this\n      stage, it only performs word sequences without understanding how to chat with humans.\n    - The model needs fine-tuning to adapt to chat templates. For example, it should recognize that a template\n      like “<chat start> Qin Shi Huang is <chat end>” indicates the end of a complete conversation, rather than just\n      generating the next word.\n    - This process is known as instruction fine-tuning, akin to teaching a knowledgeable person like Newton to adapt to\n      21st-century chat habits, learning the pattern of messages on the left and responses on the right.\n    - During training, MiniMind’s instruction and response lengths are truncated to 512 to save memory. Just as we start\n      with shorter texts when learning, we don’t need to separately learn longer articles once we master shorter ones.\n   > During inference, RoPE can be linearly interpolated to extend lengths to 1024 or 2048 or more. The learning rate is\n   set dynamically between 1e-5 and 1e-6, with 5 epochs for fine-tuning.\n    ```bash\n    # Set dataset to sft_data_single.csv in 3-full_sft.py\n    torchrun --nproc_per_node 2 3-full_sft.py\n    ```\n\n3. **Multi-Dialog Fine-Tuning**:\n    - Building on step 2, the LLM has learned a single-question-to-answer chat template. Now, it only needs further\n      fine-tuning on longer chat templates with historical question-and-answer pairs.\n    - Use the `history_chat` field for historical dialogues and `history_chat_response` for historical responses in the\n      dataset.\n    - Construct new chat templates like [question->answer, question->answer, question->] and use this dataset for\n      fine-tuning.\n    - The trained model will not only answer the current question but also conduct coherent dialogues based on\n      historical interactions.\n    - This step is not strictly necessary, as small models have weak long-context dialogue abilities, and forcing\n      multi-turn Q&A templates may slightly compromise single-turn SFT performance.\n   > The learning rate is set dynamically between 1e-5 and 1e-6, with 2 epochs for fine-tuning.\n    ```bash\n    # Set dataset to sft_data.csv in 3-full_sft.py\n    torchrun --nproc_per_node 2 3-full_sft.py\n    ```\n\n4. **Direct Preference Optimization (DPO)**:\n    - In previous training sessions, GPT has already acquired basic conversational abilities, but these abilities are\n      entirely based on word-by-word concatenation, lacking the motivation of positive and negative examples.\n    - GPT is still unaware of what constitutes a good response and what constitutes a poor one. We hope it can align\n      more with human preferences and provide more satisfying responses.\n    - This process is akin to training GPT in a workplace setting, learning from the examples of outstanding employees\n      and the mistakes of underperforming ones, to better serve customers.\n    - In the RLHF series, unlike PPO (Proximal Policy Optimization), which requires reward models and value models,\n    - DPO derives an explicit solution for the PPO reward model, replacing the online reward model with offline data,\n      where ref outputs can be saved in advance.\n    - DPO maintains nearly the same performance, requiring only the actor and ref models to run, significantly reducing\n      memory overhead and increasing training stability.\n    - Similarly, the RL steps for LLM are **not mandatory**, with both advantages and disadvantages.\n   > For the Huozi trio (q, chose, reject) dataset, the learning rate is set to 1e-5, with half-precision fp16, 1 epoch,\n   and it takes about 1 hour.\n    ```bash\n    python 5-dpo_train.py\n    ```\n\n---\n📋 Regarding LLM parameter configuration, an interesting paper [MobileLLM](https://arxiv.org/pdf/2402.14905) provides\ndetailed research and experiments.\nThe scaling law exhibits unique patterns in small models. The parameters that significantly influence the scaling of\nTransformer models are primarily `d_model` and `n_layers`.\n\n* `d_model`↑ + `n_layers`↓ -> Short and wide models\n* `d_model`↓ + `n_layers`↑ -> Tall and narrow models\n\nThe Scaling Law proposed in 2020 posits that the amount of training data, parameter count, and training iterations are\nthe key factors determining performance, with the influence of model architecture being nearly negligible. However, this\nlaw seems not to fully apply to small models.\nMobileLLM suggests that the depth of the architecture is more important than its width. A \"deep and narrow\" model can\nlearn more abstract concepts compared to a \"wide and shallow\" model. For instance, when the model parameters are fixed\nat 125M or 350M, a 30–42 layer \"narrow\" model significantly outperforms a 12-layer \"short and wide\" model. This trend is\nobserved across eight benchmark tests, including common sense reasoning, question answering, and reading comprehension.\nThis is a fascinating discovery, as previously, few attempts were made to stack more than 12 layers when designing\narchitectures for small models around the 100M parameter range. This aligns with the observations from MiniMind, where\nadjusting parameters between `d_model` and `n_layers` during training produced similar effects.\nHowever, \"deep and narrow\" has its limitations. When `d_model` < 512, the disadvantages of collapsing word embedding\ndimensions become very pronounced, and increasing layers does not compensate for the shortcomings in `d_head` caused by\nfixed `q_head`. Conversely, when `d_model` > 1536, increasing layers seems to have a higher priority than `d_model`,\nproviding a better \"cost-performance\" ratio and effect gain.\nTherefore, MiniMind sets `d_model = 512` and `n_layers = 8` for the small model to achieve a balance between \"minimal\nsize <-> better performance.\" For greater performance gains, `d_model = 768` and `n_layers = 16` are set, aligning\nbetter with the scaling law for small models.\n\n> For reference, the configuration details for GPT-3 are shown in the table below:\n\n![gpt3_config.png](./images/gpt3_config.png)\n\n---\n\n### Trained Model Weights\n\n[🔗Baidu Netdisk](https://pan.baidu.com/s/1KUfSzEkSXYbCCBj0Pw-9fA?pwd=6666)\n\n| Model Name        | params | Config                      | pretrain_model         | single_sft_model                   | multi_sft_model                   | rl_model     |\n|-------------------|--------|-----------------------------|------------------------|------------------------------------|-----------------------------------|--------------|\n| minimind-v1-small | 26M    | d_model=512<br/>n_layers=8  | `pretrain_512.pth`     | `single_chat/full_sft_512.pth`     | `multi_chat/full_sft_512.pth`     | `rl_512.pth` |\n| minimind-v1-moe   | 4×26M  | d_model=512<br/>n_layers=8  | `pretrain_512_moe.pth` | `single_chat/full_sft_512_moe.pth` | `multi_chat/full_sft_512_moe.pth` | -            |\n| minimind-v1       | 108M   | d_model=768<br/>n_layers=16 | `pretrain_768.pth`     | `single_chat/full_sft_768.pth`     | `multi_chat/full_sft_768.pth`     | `rl_768.pth` |\n\n---\n\n# 📌 Evaluation\n\n### ① RLHF Section\n\n> [!TIP]\n> Testing based on the comparison of the \"single-turn dialogue full_sft\" and \"DPO reinforcement learning alignment\" versions of the minimind model.\n\nModel files are available on [Baidu Netdisk](https://pan.baidu.com/s/1KUfSzEkSXYbCCBj0Pw-9fA?pwd=6666), where `rl_<dim>.pth` represents the weights of the minimind model after \"DPO reinforcement learning alignment.\"\n\n```text\n[Q]: What is your name?\n[ft_768]: I am an artificial intelligence language model, capable of answering your questions and providing assistance. Do you need any help?\n[rl_768]: I am a natural language processing system based on artificial intelligence technology, primarily used for processing and generating human language tasks. I can assist users in text processing, speech recognition, text generation, and various other tasks. I can also help with intelligent chat, translation, and many other applications. If you need more information about this project, please let me know.\n[ft_512]: I am glad to help you. Do you have any other questions you need me to answer?\n[rl_512]: I am a chatbot based on natural language processing technology, capable of understanding and responding to user questions and instructions. I am trained to interact with humans and answer various questions and tasks. Is there anything you need?\n\n[Q]: Who are you?\n[ft_768]: I am an artificial intelligence language model, without personal identity or status. I only generate responses based on the input instructions and text information.\n[rl_768]: I am a computer program, without personal identity or status. I am designed to answer various questions and provide assistance, including answering questions, offering advice, conducting experiments, providing information, and more. I am widely used in various fields such as natural language processing, computer vision, and artificial intelligence. If you need more help or information, please feel free to contact me. I will do my best to assist you in your work and life.\n[ft_512]: I am a large language model, capable of answering a wide variety of questions, including those related to science, history, culture, and technology. Do you have any questions you need me to answer?\n[rl_512]: I am a computer program, without a specific background or identity. If you can tell me the specific information you want to know, I will do my best to answer your questions.\n\n[Q]: What are some good universities in China?\n[ft_768]: China has many renowned universities, including Peking University, Shanghai Jiao Tong University, Fuxing Chinese Medicine College, Zhejiang University, and more. These universities have their own iconic buildings and research directions in different fields, and they enjoy a high reputation in China's top educational resources.\n[rl_768]: There are many universities in China, and the choice depends on your needs. If you need more detailed information, it is recommended to consult the university rankings and policies in your city to choose a suitable university. You can also search for relevant information online or consult some university-related institutions such as Renmin University of China, Renmin University of China, etc., to obtain more detailed and professional guidance and advice.\n[ft_512]: China has many excellent universities, such as MIT, Peking University, Shanghai University, and more. These universities have made significant contributions to the development and achievements of China's education.\n[rl_512]: The Chinese Ministry of Education, Chinese universities, mainland Chinese universities, and mainland Chinese universities all have excellent educational systems, but the educational levels and professional requirements may vary. The Chinese Ministry of Education has different academic requirements, teaching methods, and faculty strengths, and the academic requirements of universities also vary. The academic requirements for mainland Chinese students also differ, and you need to choose based on your actual situation. If you have more specific needs for the professional requirements of mainland Chinese students, it is recommended to compare and communicate through the internet, various domestic and international institutions, and organizations. You can also seek help and advice from people around you, such as friends, teachers, and parents. I hope the above answer is helpful to you.\n\n[Q]: Do you know what the speed of light is?\n[ft_768]: The speed of light is a physical constant, representing the speed at which light travels in a vacuum, approximately 299,792,458 meters per second. The speed of light is one of the most fundamental physical constants in physics and is also an important basis for relativity.\n[rl_768]: Of course, I know. The speed of light is the speed at which light travels in a vacuum, usually denoted by c. It is a very important theory in cosmology. In physics, the speed of light is considered one of the most important speeds in the universe. It represents the speed at which light travels in a vacuum, which is the basis for many important formulas and equations.\n[ft_512]: The speed of light is the speed at which light travels in a vacuum, approximately 299,792,458 meters per second. It is one of the most mysterious and important physical concepts in the universe.\n[rl_512]: The speed of light is the speed at which light travels in a vacuum, an irreversible speed in nature. The source of light consists of atoms, molecules, solids, and light, with a speed of approximately one light year per second. This number is calculated based on relativity, quantum mechanics, and gravitational theory. The speed of light is a relative concept, representing our understanding and interpretation of any event in nature.\n```\n\n### 👉Effect Summary\n* Approximately 100,000 RLHF data samples were used; the full_sft model performs better in terms of conciseness and information accuracy; the rl model provides more background information in its responses, but the accuracy of the information needs improvement.\n* Overall, the model after RLHF tends to learn: to say more polite but useless \"fluff\" to please the \"conversation\" itself, while slightly sacrificing information accuracy.\n* There is no such thing as a free lunch; we need to continue to improve the quality of the RLHF dataset, and we must also accept the inevitable loss of model capabilities (with varying degrees of severity).\n* The difference between DPO and online PPO is that reject and chosen are prepared offline, which inevitably creates a large distribution difference with the output of the minimind model itself.\n* This is similar to the DPO algorithm making the model watch the \"replay\" of the table tennis world champion's gameplay for reinforcement learning, rather than having the reward model act as a \"coach\" to correct its gameplay in real-time, like PPO.\n\n## ② Instruct Fine-Tuning Section\n\n> [!TIP]\n> The following tests were completed on September 17, 2024. New models released after this date will not be included in\n> the tests unless there is a special need.\n\n[A] [minimind-v1-small(0.02B)](https://pan.baidu.com/s/1_COe0FQRDmeapSsvArahCA?pwd=6666)<br/>\n[B] [minimind-v1-moe(0.1B)](https://pan.baidu.com/s/1tqB-GMvuiGQBvEl-yZ-oBw?pwd=6666)<br/>\n[C] [minimind-v1(0.1B)](https://pan.baidu.com/s/1p713loS7EfwHQf3G9eYI3Q?pwd=6666)<br/>\n[D] [baby-llama2-chinese(0.2B)](https://github.com/DLLXW/baby-llama2-chinese)<br/>\n[E] [chatlm-mini-chinese(0.2B)](https://github.com/charent/ChatLM-mini-Chinese)<br/>\n\n```text\n[Q]: Do you know the Yangtze River?\n[A]: Yes, the Yangtze River is the longest river in China, with a length of over 3,300 kilometers.\n[B]: Yes, the Yangtze River is the longest river in China, with a length of about 7,100 kilometers. Its drainage area is about 33.28 million square kilometers, making it one of China's largest rivers and the third longest river in the world, with a total length of about 6,300 kilometers.\n[C]: Yes, the Yangtze River is one of the longest rivers in China and the third longest river in the world. It originates from the Tibetan Plateau, flows through 11 provinces, and eventually empties into the East China Sea. The Yangtze River Basin is one of China's largest river basins, with abundant natural resources and cultural heritage, such as the Yangtze River Bridge, Yangtze River Maritime Museum, and ancient cities along the Yangtze.\n[D]: I also know that China is one of the longest cities in the world, with a population of over 34 million people.\n[E]: Yes, the Yangtze River is the longest river in China and also one of the longest rivers in the world. It originates from the Tibetan Plateau, flows through nine provinces and regions, and eventually empties into the East China Sea. The climate in the Yangtze River Basin is mild, with distinct seasons, making it an important agricultural and industrial base in China. The river basin is rich in aquatic resources and is one of China's major fishing resources.\n\n[Q]: Which country launched the first artificial satellite?\n[A]: The first artificial satellite was Sputnik 1.\n[B]: The first artificial satellite was launched by the United States.\n[C]: The first artificial satellite was launched by the Soviet Union on October 4, 1957.\n[D]: This satellite, named \"Armstrong,\" was launched on November 20, 1961, and was the first successful satellite to send a human into space.\n[E]: The first artificial satellite was launched by the Soviet Union on April 4, 1957. It was launched by Soviet astronaut Yuri Gagarin and Soviet engineer Roald Skohley. During the launch, Gagarin and Skohley used a small satellite named \"Sputnik 1,\" which had a diameter of 4.3 meters and weighed 1,051 kilograms. This satellite, known as \"Sputnik 1,\" marked a milestone in the Soviet space race and was the first instance of a human spacewalk in history.\n\n[Q]: Do you know where Mount Tai is located?\n[A]: Mount Tai is located in eastern China and is one of the Five Great Mountains of China.\n[B]: Mount Tai is located in Tai'an City, Shandong Province, and is one of the Five Great Mountains of China.\n[C]: Mount Tai is located in Tai'an City, Shandong Province, China, and is one of the Five Great Mountains of China.\n[D]: As an AI assistant, I cannot access the internet, so I cannot provide information about your location. However, you can search for information or related keywords to find details about your location, or check news and websites for relevant information.\n[E]: Mount Tai is located in Tai'an City, Shandong Province, China, and is one of the Five Great Mountains of China. It is also an important symbol of Chinese culture and history. With an elevation of 1,545 meters, it is one of the most famous tourist destinations in China. Its history dates back to the 5th century BC and was a site for ancient royal sacrifices. Mount Tai is a significant part of Chinese culture, known as the \"First of the Five Great Mountains.\"\n\n[Q]: What is the highest mountain in the world?\n[A]: The highest mountain in the world is Mount Everest, located on the border between Nepal and China, with an elevation of 8,848 meters.\n[B]: Mount Everest is the highest mountain in the world, located in the Himalayas, with an elevation of 8,848 meters.\n[C]: Mount Everest is the highest mountain in the world, with an elevation of 8,848 meters.\n[D]: The highest mountain in the world is Mount Everest, located in the Himalayas, with an elevation of 8,848.86 meters (29,031.69 feet).\n[E]: The highest mountain in the world is Mount Everest, also known as Sagarmatha, located on the border between Nepal and China in the Himalayas, with an elevation of 8,848.86 meters (29,031.69 feet). It is one of the most famous climbing destinations, attracting climbers from around the world. The name Everest comes from Tibetan, meaning \"The Goddess’s Peak,\" and it consists of two peaks, one in Nepal and one in China. Its height is determined through measurements of sea level, as it is the highest point on Earth. Although Everest is the highest mountain, it is not accessible from every place on Earth. Other places, such as the Andes Mountains in South America, are also high but do not have the highest peak. Overall, Everest is the highest mountain, but it is not accessible from every location on Earth.\n\n[Q]: Who proposed the law of universal gravitation?\n[A]: The law of universal gravitation was proposed by the British physicist Newton. He believed that all objects exert gravitational force, which depends on their mass and distance. This theory reveals the relationship between mass and distance and explains the nature of gravitation.\n[B]: The law of universal gravitation was discovered by Newton and proposed by the British physicist James Watt in 1879.\n[C]: The law of universal gravitation was proposed by the British physicist Isaac Newton in the 17th century.\n[D]: Gravitation is a force in the universe composed of mass or energy. According to Newton's law of universal gravitation, the force between two objects is proportional to their mass and inversely proportional to their distance. Thus, Newton's law of universal gravitation shows that the gravitational force between two objects is proportional to their mass and inversely proportional to their distance. This law has wide applications in physics, astronomy, and other fields.\n[E]: The law of universal gravitation was proposed by the British physicist Isaac Newton in 1687. In his famous paper \"Philosophiæ Naturalis Principia Mathematica,\" Newton proposed the law of universal gravitation, stating that any two objects exert a gravitational force on each other, proportional to their masses and inversely proportional to the square of their distance. This law is known as the law of universal gravitation and describes the relationship between mass and distance. It is crucial for explaining many phenomena in the universe, such as planetary orbits and gravitational lensing between stars, and has broad applications in astronomy, physics, and engineering, providing a fundamental theoretical basis for understanding the nature and evolution of the universe.\n\n[Q]: Do you know what the main food of giant pandas is?\n[A]: The main food of giant pandas is bamboo. They typically eat bamboo and need to consume about 30 kilograms of it each day.\n[B]: Giant pandas mainly eat bamboo, which is their primary food. Their digestive system is quite specialized, and they need to digest a large amount of bamboo daily to meet their survival needs. Besides bamboo, giant pandas also survive in the wild.\n[C]: Giant pandas mainly eat bamboo but occasionally consume other plants and animals. They also eat some wild fruits, small animals, and plants, but these are not their main food sources.\n[D]: The main food of giant pandas is bamboo.\n[E]: Giant pandas are herbivores that primarily eat bamboo. Their digestive system is adapted to process the high cellulose content of bamboo, but they also consume some other plants, such as herbaceous plants and small mammals. Their digestive system includes enzymes like \"cellulase\" to break down cellulose, helping them digest a large amount of fiber and nutrients. Additionally, they have \"cellulose protein\" that helps maintain their energy levels, aiding their survival in the wild. Overall, bamboo is their primary food, but they also eat other plants and small mammals.\n\n[Q]: Why is seawater salty?\n[A]: Seawater contains many salts, which increase its salinity. The salts in seawater mainly come from the dissolved salts and form hydrogen bonds with water molecules, increasing the salinity.\n[B]: Seawater contains abundant elements like calcium, phosphorus, potassium, and phosphorus, which are limited in quantity and distribution in the ocean. Additionally, seawater contains nutrients from various organisms and microorganisms, such as shells and seaweeds.\n[C]: Seawater contains many dissolved salts, with the most common being sodium chloride. When salt is added to water, its molecules react with water molecules to form sodium hydroxide, which is a type of salt.\n[D]: Water is essential for many organisms, as it can absorb and release carbon dioxide, release oxygen,\n[E]: The primary reason seawater is salty is due to the high concentration of dissolved salts, with sodium chloride (NaCl) being the most significant. Sodium chloride is a very common salt that is present in most regions of the Earth, including oceans, lakes, rivers, and groundwater. When water evaporates from the ocean, it carries away some of the salt, making the seawater salty. Additionally, seawater contains other dissolved salts such as sulfates, magnesium sulfate, calcium sulfate, magnesium potassium sulfate, magnesium sodium sulfate, magnesium calcium sulfate, magnesium magnesium calcium sulfate, magnesium magnesium calcium sulfate, magnesium magnesium calcium sulfate, magnesium magnesium sodium sulfate, magnesium magnesium calcium sulfate, magnesium magnesium magnesium calcium sulfate, magnesium magnesium magnesium \n```\n\n> [!NOTE]\n> 🙋‍♂️Directly throw the answer of the above model to GPT-4o and ask it to help score it:\n\n---\n\n### Model Performance Review:\n\n1. **Model A**:\n    - **Performance**: Model A's responses are usually concise and clear but lack detail and accuracy in some cases. For\n      example, Model A provided incorrect information about the length of the Yangtze River.\n    - **Score**: 60\n\n2. **Model B**:\n    - **Performance**: Model B provides additional information in some cases, but this information can sometimes be\n      inaccurate or excessive. For instance, Model B gave incorrect figures for the length and drainage area of the\n      Yangtze River.\n    - **Score**: 65\n\n3. **Model C**:\n    - **Performance**: Model C typically provides detailed and accurate answers for most questions. For example,\n      responses about the Yangtze River and Mount Tai were accurate.\n    - **Score**: 75\n\n4. **Model D**:\n    - **Performance**: Model D’s responses sometimes appear disorganized and lack accuracy. For example, the answer\n      about Mount Tai was completely off-topic.\n    - **Score**: 50\n\n5. **Model E**:\n    - **Performance**: Model E’s responses are usually very detailed, but they can be overly verbose and contain\n      unnecessary information. For instance, the answer on gravity was overly complex.\n    - **Score**: 70\n\n#### Ranking (from highest to lowest):\n\n| Model | C  | E  | B  | A  | D  |\n|-------|----|----|----|----|----|\n| Score | 75 | 70 | 65 | 60 | 50 |\n\n---\n\n## 👉 Summary of Effects\n\n* The ranking of the minimind series (ABC) aligns with intuition, with minimind-v1(0.1B) scoring the highest, and its\n  responses to common sense questions are mostly error-free and free of hallucinations.\n    * Surprisingly, minimind-v1-small(0.02B), with only 26M parameters, can perform nearly as well as minimind-v1(0.1B).\n    * minimind-v1(0.1B) underwent less than 2 epochs of SFT (Supervised Fine-Tuning) due to being prematurely killed to\n      free up resources for smaller models. Despite not being fully trained, it still achieved the best performance,\n      demonstrating that larger models generally outperform smaller ones.\n    * minimind-v1-moe(0.1B) performed only slightly better than minimind-v1-small(0.02B), also due to early termination\n      to free up resources for other training. However, the MoE (Mixture of Experts) model, with its sparse\n      multi-Experts mode, requires more training epochs to fully activate and train all FFN (Feed-Forward Network) layer\n      experts. In the current setup with 3 epochs, the training is not yet sufficient.\n      Early experiments with minimind on the Yi-Tokenizer showed that a fully trained MoE version could outperform dense\n      small models visibly. This aspect may need to be reserved for future training and updates to v2 and v3 versions\n      when more server resources are available.\n\n* The responses from Model E appear to be quite good to the naked eye, although there are occasional instances of\n  hallucinations and fabrications. However, both GPT-4o and Deepseek's evaluations consistently noted that it \"provides\n  overly verbose and repetitive information, and contains hallucinations.\"\n  This evaluation seems somewhat strict, as even a small number of hallucinated words in a 100-word response can easily\n  result in a low score. Given that Model E was pre-trained on longer texts and a larger dataset, its responses appear\n  more comprehensive. In models of similar size, both the quantity and quality of the data are crucial.\n\n> 🙋‍♂️ Personal Subjective Evaluation: E>C>B≈A>D\n\n> 🤖 GPT-4o Evaluation: C>E>B>A>D\n\nScaling Law: Larger model parameters and more training data generally lead to better model performance.\n\n# 📌 Objective Dataset: C-Eval\n\nC-Eval evaluation code is located at: `./eval_ceval.py`.\n\nFor small models, to avoid issues with fixed response formatting, we directly judge the prediction probabilities of the\nfour tokens `A`, `B`, `C`, `D`, and choose the one with the highest probability as the answer, then calculate accuracy\nagainst the standard answer. Note that minimind models were not trained on larger datasets or fine-tuned for question\nanswering, so results should be considered as reference only.\n\n> For example, detailed results for minimind-small:\n\n| Type | 1                          | 2   | 3                     | 4                     | 5                   | 6                  | 7                   | 8                   | 9              | 10                     | 11                    | 12                    | 13             | 14               | 15    | 16                  | 17            | 18                              | 19                  | 20         | 21               | 22                      | 23                 | 24                  | 25      | 26                   | 27                      | 28                      | 29                 | 30                                | 31                | 32                      | 33                                       | 34                    | 35                      | 36              | 37                        | 38                   | 39        | 40                | 41                  | 42                    | 43                     | 44                | 45               | 46             | 47          | 48                    | 49                   | 50                | 51            | 52                      |\n|------|----------------------------|-----|-----------------------|-----------------------|---------------------|--------------------|---------------------|---------------------|----------------|------------------------|-----------------------|-----------------------|----------------|------------------|-------|---------------------|---------------|---------------------------------|---------------------|------------|------------------|-------------------------|--------------------|---------------------|---------|----------------------|-------------------------|-------------------------|--------------------|-----------------------------------|-------------------|-------------------------|------------------------------------------|-----------------------|-------------------------|-----------------|---------------------------|----------------------|-----------|-------------------|---------------------|-----------------------|------------------------|-------------------|------------------|----------------|-------------|-----------------------|----------------------|-------------------|---------------|-------------------------|\n| Data | probability_and_statistics | law | middle_school_biology | high_school_chemistry | high_school_physics | legal_professional | high_school_chinese | high_school_history | tax_accountant | modern_chinese_history | middle_school_physics | middle_school_history | basic_medicine | operating_system | logic | electrical_engineer | civil_servant | chinese_language_and_literature | college_programming | accountant | plant_protection | middle_school_chemistry | metrology_engineer | veterinary_medicine | marxism | advanced_mathematics | high_school_mathematics | business_administration | mao_zedong_thought | ideological_and_moral_cultivation | college_economics | professional_tour_guide | environmental_impact_assessment_engineer | computer_architecture | urban_and_rural_planner | college_physics | middle_school_mathematics | high_school_politics | physician | college_chemistry | high_school_biology | high_school_geography | middle_school_politics | clinical_medicine | computer_network | sports_science | art_studies | teacher_qualification | discrete_mathematics | education_science | fire_engineer | middle_school_geography |\n\n| Type     | 1      | 2      | 3      | 4      | 5      | 6     | 7      | 8      | 9      | 10     | 11     | 12     | 13    | 14     | 15     | 16     | 17     | 18     | 19     | 20     | 21     | 22     | 23     | 24     | 25     | 26     | 27     | 28     | 29     | 30     | 31     | 32     | 33     | 34     | 35     | 36     | 37     | 38     | 39     | 40     | 41     | 42     | 43     | 44     | 45     | 46     | 47     | 48     | 49     | 50     | 51     | 52    |\n|----------|--------|--------|--------|--------|--------|-------|--------|--------|--------|--------|--------|--------|-------|--------|--------|--------|--------|--------|--------|--------|--------|--------|--------|--------|--------|--------|--------|--------|--------|--------|--------|--------|--------|--------|--------|--------|--------|--------|--------|--------|--------|--------|--------|--------|--------|--------|--------|--------|--------|--------|--------|-------|\n| T/A      | 3/18   | 5/24   | 4/21   | 7/19   | 5/19   | 2/23  | 4/19   | 6/20   | 10/49  | 4/23   | 4/19   | 4/22   | 1/19  | 3/19   | 4/22   | 7/37   | 11/47  | 5/23   | 10/37  | 9/49   | 7/22   | 4/20   | 3/24   | 6/23   | 5/19   | 5/19   | 4/18   | 8/33   | 8/24   | 5/19   | 17/55  | 10/29  | 7/31   | 6/21   | 11/46  | 5/19   | 3/19   | 4/19   | 13/49  | 3/24   | 5/19   | 4/19   | 6/21   | 6/22   | 2/19   | 2/19   | 14/33  | 12/44  | 6/16   | 7/29   | 9/31   | 1/12  |\n| Accuracy | 16.67% | 20.83% | 19.05% | 36.84% | 26.32% | 8.70% | 21.05% | 30.00% | 20.41% | 17.39% | 21.05% | 18.18% | 5.26% | 15.79% | 18.18% | 18.92% | 23.40% | 21.74% | 27.03% | 18.37% | 31.82% | 20.00% | 12.50% | 26.09% | 26.32% | 26.32% | 22.22% | 24.24% | 33.33% | 26.32% | 30.91% | 34.48% | 22.58% | 28.57% | 23.91% | 26.32% | 15.79% | 21.05% | 26.53% | 12.50% | 26.32% | 21.05% | 28.57% | 27.27% | 10.53% | 10.53% | 42.42% | 27.27% | 37.50% | 24.14% | 29.03% | 8.33% |\n\n**Total number of questions**: 1346\n\n**Total confirmed number**: 316\n\n**Total accuracy rate**: 23.48%\n\n---\n\n#### Results summary：\n\n| category          | correct  | question_count | accuracy |\n|:------------------|:--------:|:--------------:|:--------:|\n| minimind-v1-small | \t   344\t |      1346      |  25.56%  |\n| minimind-v1       | \t   351\t |      1346      |  26.08%  |\n\n### Model Performance Insights from GPT-4o\n\n```text\n### Areas Where the Model Excels:\n1. **High School Chemistry**: With an accuracy of 42.11%, this is the strongest area for the model, suggesting a solid grasp of chemistry-related knowledge.\n2. **Discrete Mathematics**: Achieving an accuracy of 37.50%, the model performs well in mathematics-related fields.\n3. **Education Science**: The model shows good performance in education-related topics with a 37.93% accuracy.\n4. **Basic Medicine**: The accuracy of 36.84% indicates strong performance in foundational medical knowledge.\n5. **Operating Systems**: With a 36.84% accuracy, the model demonstrates reliable performance in computer operating systems.\n\n### Areas Where the Model Struggles:\n1. **Legal Topics**: The model performs poorly in legal-related areas such as Legal Professional (8.70%) and Tax Accountant (20.41%).\n2. **Physics**: Both high school (26.32%) and college-level (21.05%) physics topics are challenging for the model.\n3. **High School Politics and Geography**: The model shows low accuracy in these areas, with High School Politics at 15.79% and High School Geography at 21.05%.\n4. **Computer Networking and Architecture**: The model struggles with Computer Networking (21.05%) and Computer Architecture (9.52%).\n5. **Environmental Impact Assessment Engineering**: The accuracy is only 12.90%, indicating weak performance in environmental science.\n\n### Summary:\n- **Strengths**: Chemistry, Mathematics (especially Discrete Mathematics), Education Science, Basic Medicine, and Operating Systems.\n- **Weaknesses**: Legal Topics, Physics, Politics, Geography, Computer Networking and Architecture, and Environmental Science.\n\nThis suggests that the model performs well in logical reasoning, foundational sciences, and some engineering disciplines but is weaker in humanities, social sciences, and certain specialized fields (such as law and taxation). To improve the model's performance, additional training in humanities, physics, law, and environmental science may be beneficial.\n```\n\n# 📌 Others\n\n### Inference and Export\n\n* [./export_model.py](./export_model.py) can export the model to the transformers format and push it to Hugging Face.\n\n* MiniMind's Hugging Face collection\n  address: [MiniMind](https://huggingface.co/collections/jingyaogong/minimind-66caf8d999f5c7fa64f399e5)\n\n---\n\n### API Inference\n\n[./my_openai_api.py](./my_openai_api.py) provides a chat interface for the OpenAI API, making it easier to integrate\nyour model with third-party UIs, such as fastgpt, OpenWebUI, etc.\n\n* Download the model weight files\n  from [Hugging Face](https://huggingface.co/collections/jingyaogong/minimind-66caf8d999f5c7fa64f399e5):\n    ```\n    minimind (root dir)\n    ├─minimind\n    |  ├── config.json\n    |  ├── generation_config.json\n    |  ├── LMConfig.py\n    |  ├── model.py\n    |  ├── pytorch_model.bin\n    |  ├── special_tokens_map.json\n    |  ├── tokenizer_config.json\n    |  ├── tokenizer.json\n    ```\n\n* Start the chat server:\n    ```bash\n    python my_openai_api.py\n    ```\n* Test the service interface:\n    ```bash\n    python chat_openai_api.py\n    ```\n* API interface example, compatible with the OpenAI API format:\n    ```bash\n    curl http://ip:port/v1/chat/completions \\\n      -H \"Content-Type: application/json\" \\\n      -d '{ \n        \"model\": \"model-identifier\",\n        \"messages\": [ \n          { \"role\": \"user\", \"content\": \"What is the highest mountain in the world?\" }\n        ], \n        \"temperature\": 0.7, \n        \"max_tokens\": -1,\n        \"stream\": true\n    }'\n    ```\n\n![images](./images/logger.png)\n\n### Integrating MiniMind API in FastGPT\n\n![images](./images/fastgpt.png)\n\n\n---\n\n# 📌 Acknowledgement\n\n> [!TIP]\n> If you find `MiniMind` helpful, please give us a ⭐ on GitHub.<br/>\n> Given the length and the limitations of our expertise, there may be errors. We welcome discussions and corrections in\n> the Issues section.<br/>\n> Your support is the driving force behind our continuous improvement of the project!\n\n\n> [!NOTE]\n> Many hands make light work.\n> If you have already tried training a new MiniMind model, you are welcome to share your model weights in Discussions or Issues. <br/>\n> This can be a new version of MiniMind for a specific downstream task or vertical domain (e.g., sentiment recognition, healthcare, psychology, finance, legal Q&A, etc.). <br/>\n> It can also be a new version of MiniMind after extended training (e.g., exploring longer text sequences, larger sizes (0.1B+), or larger datasets). <br/>\n> Any contribution is considered unique and valuable, and all attempts are encouraged. <br/>\n> These contributions will be promptly discovered and compiled in the acknowledgment list. Thank you again for all the support!\n\n## 🤝[Contributors](https://github.com/jingyaogong/minimind/graphs/contributors)\n\n<!--\n<a href=\"https://github.com/jingyaogong/minimind/graphs/contributors\">\n  <img src=\"https://contrib.rocks/image?repo=jingyaogong/minimind&v3\" />\n</a>\n-->\n\n<a href=\"https://github.com/jingyaogong\"><img src=\"https://avatars.githubusercontent.com/u/62287848\" width=\"70px\" height=\"70px\"/></a>\n&nbsp;\n<a href=\"https://github.com/MuWinds\"><img src=\"https://avatars.githubusercontent.com/u/93832089\" width=\"70px\" height=\"70px\"/></a>\n&nbsp;\n<a href=\"https://github.com/chuanzhubin\"><img src=\"https://avatars.githubusercontent.com/u/2813798\" width=\"70px\" height=\"70px\"/></a>\n&nbsp;\n<a href=\"https://github.com/iomgaa-ycz\"><img src=\"https://avatars.githubusercontent.com/u/124225682\" width=\"70px\" height=\"70px\"/></a>\n&nbsp;\n\n## 😊Thanks for\n\n<a href=\"https://github.com/ipfgao\"><b>@ipfgao</b></a>:\n<a href=\"https://github.com/jingyaogong/minimind/issues/26\">🔗Training step record</a>\n\n<a href=\"https://github.com/chuanzhubin\"><b>@chuanzhubin</b></a>:\n<a href=\"https://github.com/jingyaogong/minimind/pull/34\">🔗Code line by line comments (Chinese)</a>\n\n<a href=\"https://github.com/WangRongsheng\"><b>@WangRongsheng</b></a>:\n<a href=\"https://github.com/jingyaogong/minimind/issues/39\">🔗Preprocessing of large datasets</a>\n\n<a href=\"https://github.com/pengqianhan\"><b>@pengqianhan</b></a>:\n<a href=\"https://github.com/jingyaogong/minimind/issues/73\">🔗A Concise Tutorial</a>\n\n<a href=\"https://github.com/RyanSunn\"><b>@RyanSunn</b></a>:\n<a href=\"https://github.com/jingyaogong/minimind/issues/75\">🔗Learning Record of Model Inference Process</a>\n\n<details close> \n<summary> <b>Reference Links & Acknowledgments to the Following Excellent Papers or Projects</b> </summary>\n\n- No specific order\n- [https://github.com/meta-llama/llama3](https://github.com/meta-llama/llama3)\n- [https://github.com/karpathy/llama2.c](https://github.com/karpathy/llama2.c)\n- [https://github.com/DLLXW/baby-llama2-chinese](https://github.com/DLLXW/baby-llama2-chinese)\n- [(DeepSeek-V2)https://arxiv.org/abs/2405.04434](https://arxiv.org/abs/2405.04434)\n- [https://github.com/charent/ChatLM-mini-Chinese](https://github.com/charent/ChatLM-mini-Chinese)\n- [https://github.com/wdndev/tiny-llm-zh](https://github.com/wdndev/tiny-llm-zh)\n- [(Mistral-MoE)https://arxiv.org/pdf/2401.04088](https://arxiv.org/pdf/2401.04088)\n- [https://github.com/Tongjilibo/build_MiniLLM_from_scratch](https://github.com/Tongjilibo/build_MiniLLM_from_scratch)\n- [https://github.com/jzhang38/TinyLlama](https://github.com/jzhang38/TinyLlama)\n- [https://github.com/AI-Study-Han/Zero-Chatgpt](https://github.com/AI-Study-Han/Zero-Chatgpt)\n- [https://github.com/xusenlinzy/api-for-open-llm](https://github.com/xusenlinzy/api-for-open-llm)\n- [https://github.com/HqWu-HITCS/Awesome-Chinese-LLM](https://github.com/HqWu-HITCS/Awesome-Chinese-LLM)\n\n</details>\n\n## 🫶Supporter\n\n<a href=\"https://github.com/jingyaogong/minimind/stargazers\">\n    <picture>\n      <source media=\"(prefers-color-scheme: dark)\" srcset=\"https://reporoster.com/stars/dark/jingyaogong/minimind\"/>\n      <source media=\"(prefers-color-scheme: light)\" srcset=\"https://reporoster.com/stars/jingyaogong/minimind\"/>\n      <img alt=\"github contribution grid snake animation\" src=\"https://reporoster.com/stars/jingyaogong/minimind\"/>\n    </picture>\n</a>\n\n<a href=\"https://github.com/jingyaogong/minimind/network/members\">\n    <picture>\n      <source media=\"(prefers-color-scheme: dark)\" srcset=\"https://reporoster.com/forks/dark/jingyaogong/minimind\"/>\n      <source media=\"(prefers-color-scheme: light)\" srcset=\"https://reporoster.com/forks/jingyaogong/minimind\"/>\n      <img alt=\"github contribution grid snake animation\" src=\"https://reporoster.com/forks/jingyaogong/minimind\"/>\n    </picture>\n</a>\n\n<picture>\n  <source media=\"(prefers-color-scheme: dark)\" srcset=\"https://api.star-history.com/svg?repos=jingyaogong/minimind&type=Date&theme=dark\"/>\n  <source media=\"(prefers-color-scheme: light)\" srcset=\"https://api.star-history.com/svg?repos=jingyaogong/minimind&type=Date\"/>\n  <img alt=\"Star History Chart\" src=\"https://api.star-history.com/svg?repos=jingyaogong/minimind&type=Date\"/>\n</picture>\n\n# License\n\nThis repository is licensed under the [Apache-2.0 License](LICENSE)."
        },
        {
          "name": "ceval",
          "type": "tree",
          "content": null
        },
        {
          "name": "chat_openai_api.py",
          "type": "blob",
          "size": 1.421875,
          "content": "from openai import OpenAI\n\nclient = OpenAI(\n    api_key=\"none\",\n    base_url=\"http://202.195.167.206:8000/v1\"\n)\n\n# 初始化对话历史列表\nconversation_history_origin = []\nconversation_history = conversation_history_origin.copy()\nwhile True:\n    conversation_history = conversation_history_origin.copy()\n    query = input('[Q]:')\n\n    # 将用户的问题添加到对话历史中\n    conversation_history.append({\"role\": \"user\", \"content\": query})\n\n    # Chat completion API\n    stream = client.chat.completions.create(\n        model=\"minimind\",\n        messages=conversation_history,  # 传递整个对话历史\n        stream=True\n    )\n\n    print('[A]: ', end='')\n    assistant_res = ''\n    for chunk in stream:\n        # 将生成的回复实时打印出来\n        print(chunk.choices[0].delta.content or \"\", end=\"\")\n        assistant_res += chunk.choices[0].delta.content or \"\"\n\n    # 当完成生成回复后，将LLM的回答也添加到对话历史中\n    conversation_history.append({\"role\": \"assistant\", \"content\": assistant_res})\n    print()\n\n# # Example: reuse your existing OpenAI setup\n# from openai import OpenAI\n#\n# # Point to the local server\n# client = OpenAI(base_url=\"http://202.195.167.206:8000/v1\", api_key=\"none\")\n#\n# completion = client.chat.completions.create(\n#     model=\"minimind\",\n#     messages=[{\"role\": \"user\", \"content\": \"世界上最高的山是？\"}],\n#     stream=False\n# )\n#\n# print(completion.choices[0].message)\n"
        },
        {
          "name": "data_process.py",
          "type": "blob",
          "size": 5.1201171875,
          "content": "import csv\nimport itertools\nimport re\nimport json\nimport jsonlines\nimport psutil\nimport ujson\nimport numpy as np\nimport pandas as pd\nfrom transformers import AutoTokenizer\nfrom datasets import load_dataset\n\nbos_token = \"<s>\"\neos_token = \"</s>\"\n\n\ndef pretrain_process(chunk_size=50000):\n    chunk_idx = 0\n\n    with jsonlines.open('./dataset/mobvoi_seq_monkey_general_open_corpus.jsonl') as reader:\n        with open('./dataset/pretrain_data.csv', 'w', newline='', encoding='utf-8') as csvfile:\n            writer = csv.writer(csvfile)\n            writer.writerow(['text'])\n\n            while True:\n                chunk = list(itertools.islice(reader, chunk_size))\n                if not chunk:\n                    break\n\n                for idx, obj in enumerate(chunk):\n                    try:\n                        content = obj.get('text', '')\n                        if len(content) > 512:\n                            continue\n                        writer.writerow([content])\n                    except UnicodeDecodeError as e:\n                        print(f\"Skipping invalid line {chunk_idx * chunk_size + idx + 1}: {e}\")\n                        continue\n                chunk_idx += 1\n                print('chunk:', ((chunk_idx - 1) * chunk_size, chunk_idx * chunk_size), 'process end')\n\n\ndef sft_process(contain_history=False):\n    file_name = 'sft_data.csv'\n    if not contain_history:\n        file_name = 'sft_data_single.csv'\n\n    def chinese_ratio(text):\n        # 匹配所有中文字符\n        chinese_chars = re.findall(r'[\\u4e00-\\u9fff]', text)\n        # 中文字符数量占比\n        return len(chinese_chars) / len(text) if text else 0\n\n    def process_and_write_data(data):\n        q_lst, a_lst, history_lst = [], [], []\n        for per in data:\n            history, q, a = per['history'], per['q'], per['a']\n\n            if (contain_history and not history) or not q or not a:\n                continue\n            if len(q) < 10 or len(a) < 5:\n                continue\n            if len(q) > 512 or len(a) > 512:\n                continue\n            # 判断q和a中中文字符占比是否超过70%\n            if not (chinese_ratio(q) > 0.5 and chinese_ratio(a) > 0.5):\n                continue\n\n            q_lst.append(q)\n            a_lst.append(a)\n            if contain_history:\n                history_lst.append(history)\n            else:\n                history_lst.append([])\n\n        # 创建DataFrame并追加到CSV文件\n        df = pd.DataFrame({'history': history_lst, 'q': q_lst, 'a': a_lst})\n        # # 1、默认\n        # df.to_csv(f'./dataset/{file_name}', mode='a', header=False, index=False, lineterminator='\\r\\n', encoding='utf-8')\n        # 2、若遇到数据 `_csv.Error: need to escape, but no escapechar set` 问题，可加 escapechar='\\\\' 参数：\n        df.to_csv(f'./dataset/{file_name}', mode='a', header=False, index=False, lineterminator='\\r\\n', escapechar='\\\\',\n                  encoding='utf-8')\n\n    chunk_size = 1000  # 每次处理的记录数\n    data = []\n\n    with open(f'./dataset/{file_name}', 'w', encoding='utf-8') as f:\n        f.write('history,q,a\\n')\n\n    sft_datasets = ['./dataset/sft_data_zh.jsonl']\n    if not contain_history:\n        sft_datasets = ['./dataset/sft_data_zh.jsonl']\n\n    chunk_num = 0\n    for path in sft_datasets:\n        with jsonlines.open(path) as reader:\n            for idx, obj in enumerate(reader):\n                try:\n                    data.append({\n                        'history': obj.get('history', ''),\n                        'q': obj.get('input', '') + obj.get('q', ''),\n                        'a': obj.get('output', '') + obj.get('a', '')\n                    })\n\n                    if len(data) >= chunk_size:\n                        chunk_num += 1\n                        process_and_write_data(data)\n                        data = []\n                        if chunk_num % 100 == 0:\n                            print(f'chunk:{chunk_num} process end')\n                except jsonlines.InvalidLineError as e:\n                    print(f\"Skipping invalid JSON line {idx + 1}: {e}\")\n                    continue\n\n            if data:\n                process_and_write_data(data)\n                data = []\n\n\ndef rl_process():\n    ################\n    # Dataset\n    ################\n\n    dataset_paths = [\n        './dataset/dpo/dpo_zh_demo.json',\n        './dataset/dpo/dpo_train_data.json',\n        './dataset/dpo/huozi_rlhf_data.json',\n    ]\n\n    train_dataset = load_dataset('json', data_files=dataset_paths)\n\n    merged_data = []\n    for split in train_dataset.keys():\n        merged_data.extend(train_dataset[split])\n\n    with open('./dataset/dpo/train_data.json', 'w', encoding='utf-8') as f:\n        json.dump(merged_data, f, ensure_ascii=False, indent=4)\n\n\nif __name__ == \"__main__\":\n    tokenizer = AutoTokenizer.from_pretrained('./model/minimind_tokenizer', use_fast=False)\n    print('tokenizer词表大小：', len(tokenizer))\n\n    ################\n    # 1: pretrain\n    # 2: sft\n    # 3: RL\n    ################\n    process_type = 2\n\n    if process_type == 1:\n        pretrain_process()\n    if process_type == 2:\n        sft_process(contain_history=False)\n    if process_type == 3:\n        rl_process()\n"
        },
        {
          "name": "eval_ceval.py",
          "type": "blob",
          "size": 6.8740234375,
          "content": "import random\nimport time\nimport os\n\nimport pandas as pd\nimport torch\nimport warnings\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nfrom model.model import Transformer\nfrom model.LMConfig import LMConfig\nimport torch.nn.functional as F\n\nwarnings.filterwarnings('ignore')\n\n\ndef init_model(lm_config):\n    tokenizer = AutoTokenizer.from_pretrained('./model/minimind_tokenizer',\n                                              trust_remote_code=True, use_fast=False)\n    model_from = 1  # 1从权重，2用transformers\n\n    if model_from == 1:\n        moe_path = '_moe' if lm_config.use_moe else ''\n        ckp = f'./out/single_chat/full_sft_{lm_config.dim}{moe_path}.pth'\n\n        model = Transformer(lm_config)\n        state_dict = torch.load(ckp, map_location=device)\n\n        # 处理不需要的前缀\n        unwanted_prefix = '_orig_mod.'\n        for k, v in list(state_dict.items()):\n            if k.startswith(unwanted_prefix):\n                state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n        # 加载到模型中\n        model.load_state_dict(state_dict, strict=False)\n    else:\n        model = AutoModelForCausalLM.from_pretrained('minimind', trust_remote_code=True)\n    model = model.to(device)\n\n    return model, tokenizer\n\n\nif __name__ == \"__main__\":\n    # -----------------------------------------------------------------------------\n    seed = random.randint(1, 2000)\n    # device = 'cuda:0'\n    device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n    dtype = 'bfloat16'\n    lm_config = LMConfig()\n    # -----------------------------------------------------------------------------\n\n    model, tokenizer = init_model(lm_config)\n    model = model.eval()\n\n    # 消息模板，具体实现根据你的tokenizer进行调整\n    messages_origin = [{\"role\": \"system\", \"content\": \"开始回答问题\"}]\n\n    # 定义文件目录\n    File_Dir = \"ceval/ceval-exam/val\"\n    results_dir = \"ceval/ceval_result\"\n\n    # 确保结果目录存在\n    if not os.path.exists(results_dir):\n        os.makedirs(results_dir)\n\n    # 用于记录所有文件的总正确数和总题数\n    total_correct = 0\n    total_questions = 0\n\n    # 遍历目录下的所有CSV文件\n    for filename in os.listdir(File_Dir):\n        if filename.endswith('.csv'):\n            file_path = os.path.join(File_Dir, filename)\n            test_df = pd.read_csv(file_path)\n\n            # 存储结果的DataFrame\n            results_df = pd.DataFrame(columns=['question', 'A', 'B', 'C', 'D', 'answer', 'llm_answer', 'is_right'])\n            total_correct_in_file = 0  # 用于记录当前文件的正确数\n\n            for row in test_df.itertuples(index=True, name='Pandas'):\n                id = getattr(row, 'id')\n                question = getattr(row, 'question')\n                A = getattr(row, 'A')\n                B = getattr(row, 'B')\n                C = getattr(row, 'C')\n                D = getattr(row, 'D')\n                right_answer = getattr(row, 'answer')\n\n                prompt = f'{question}。选择 A: {A}, B: {B}, C: {C}, D: {D}'\n\n                messages = messages_origin.copy()\n                messages.append({\"role\": \"user\", \"content\": prompt})\n\n                # print(messages)\n                new_prompt = tokenizer.apply_chat_template(\n                    messages,\n                    tokenize=False,\n                    add_generation_prompt=True\n                )\n                x = tokenizer(new_prompt).data['input_ids']\n                x = (torch.tensor(x, dtype=torch.long, device=device)[None, ...])\n                res_ids = model.eval_answer(x)\n\n                # 假设 res_ids 是模型的 logits 输出，我们使用 softmax 转换为概率分布\n                probabilities = F.softmax(res_ids, dim=-1)\n\n                # 定义每个选项的 token id\n                A_id = tokenizer('A').data['input_ids']\n                B_id = tokenizer('B').data['input_ids']\n                C_id = tokenizer('C').data['input_ids']\n                D_id = tokenizer('D').data['input_ids']\n\n                # 获取每个选项的概率\n                A_prob = probabilities[0, A_id].item()\n                B_prob = probabilities[0, B_id].item()\n                C_prob = probabilities[0, C_id].item()\n                D_prob = probabilities[0, D_id].item()\n\n                # 将每个选项的概率放入字典中便于处理\n                options_prob = {\n                    'A': A_prob,\n                    'B': B_prob,\n                    'C': C_prob,\n                    'D': D_prob\n                }\n\n                # 找到具有最大概率的选项\n                max_option_answer = max(options_prob, key=options_prob.get)\n\n                # 比较答案并记录\n                is_right = 1 if max_option_answer == right_answer else 0\n                results_df = results_df.append({\n                    'question': question,\n                    'A': A,\n                    'B': B,\n                    'C': C,\n                    'D': D,\n                    'answer': right_answer,\n                    'llm_answer': max_option_answer,\n                    'is_right': is_right\n                }, ignore_index=True)\n                # print(f'id: {id} 问题: {question[:10]}... 是否正确: {is_right}')\n\n                if is_right:\n                    total_correct_in_file += 1\n\n            total_correct += total_correct_in_file\n            total_questions += len(test_df)\n\n            # 计算当前文件的正确率并添加到结果DataFrame的最后一行\n            accuracy = total_correct_in_file / len(test_df)\n            results_df = results_df.append({\n                'question': '-',\n                'A': '-',\n                'B': '-',\n                'C': '-',\n                'D': '-',\n                'answer': f'文件 {filename} 的正确率: {accuracy:.2%}',\n                'llm_answer': '-',\n                'is_right': '-'\n            }, ignore_index=True)\n\n            print(f'{filename.split(\".\")[0]} ，{total_correct_in_file}/{len(test_df)}，正确率: {accuracy:.2%}')\n\n            # 保存结果到CSV\n            results_path = os.path.join(results_dir, f\"{filename.split('.')[0]}_result.csv\")\n            results_df.to_csv(results_path, index=False)\n\n    # 计算总正确率\n    total_accuracy = total_correct / total_questions if total_questions > 0 else 0\n\n    # 将各个文件的正确率以及总正确率写入到 \"ceval/ceval_result/test.log\"\n    log_path = os.path.join(results_dir, \"test.log\")\n    with open(log_path, 'w') as log_file:\n        result = f\"总题数: {total_questions}\\n总正确数: {total_correct}\\n总正确率: {total_accuracy:.2%}\"\n        log_file.write(result)\n\n        print(result)\n\n        for filename in os.listdir(File_Dir):\n            if filename.endswith('.csv'):\n                accuracy_file = pd.read_csv(os.path.join(results_dir, f\"{filename.split('.')[0]}_result.csv\"))\n                last_row = accuracy_file.iloc[-1]['answer']\n                log_file.write(f\"{filename}: {last_row}\\n\")\n"
        },
        {
          "name": "export_model.py",
          "type": "blob",
          "size": 2.0703125,
          "content": "import torch\nimport warnings\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nfrom model.LMConfig import LMConfig\nfrom model.model import Transformer\n\nwarnings.filterwarnings('ignore', category=UserWarning)\n\n\ndef count_parameters(model):\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n\n\ndef export_transformers_model():\n    LMConfig.register_for_auto_class()\n    Transformer.register_for_auto_class(\"AutoModelForCausalLM\")\n\n    lm_config = LMConfig()\n    lm_model = Transformer(lm_config)\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    moe_path = '_moe' if lm_config.use_moe else ''\n    ckpt_path = f'./out/single_chat/full_sft_{lm_config.dim}{moe_path}.pth'\n\n    state_dict = torch.load(ckpt_path, map_location=device)\n    unwanted_prefix = '_orig_mod.'\n    for k, v in list(state_dict.items()):\n        if k.startswith(unwanted_prefix):\n            state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n    lm_model.load_state_dict(state_dict, strict=False)\n    print(f'模型参数: {count_parameters(lm_model) / 1e6} 百万 = {count_parameters(lm_model) / 1e9} B (Billion)')\n\n    lm_model.save_pretrained(\"minimind-v1-small\", safe_serialization=False)\n\n\ndef export_tokenizer():\n    tokenizer = AutoTokenizer.from_pretrained('./model/minimind_tokenizer',\n                                              trust_remote_code=True, use_fast=False)\n    tokenizer.save_pretrained(\"minimind-v1-small\")\n\n\ndef push_to_hf():\n    def init_model():\n        tokenizer = AutoTokenizer.from_pretrained('./model/minimind_tokenizer',\n                                                  trust_remote_code=True, use_fast=False)\n        model = AutoModelForCausalLM.from_pretrained('minimind-v1-small', trust_remote_code=True)\n        return model, tokenizer\n\n    model, tokenizer = init_model()\n    # 推送到huggingface\n    model.push_to_hub(\"minimind-v1-small\")\n    # tokenizer.push_to_hub(\"minimind-v1-small\", safe_serialization=False)\n\n\nif __name__ == '__main__':\n    # 1\n    export_transformers_model()\n    # 2\n    export_tokenizer()\n    # # 3\n    # push_to_hf()\n"
        },
        {
          "name": "fast_inference.py",
          "type": "blob",
          "size": 4.6259765625,
          "content": "import json\nimport random\nimport numpy as np\nimport streamlit as st\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom transformers.generation.utils import GenerationConfig\n\nst.set_page_config(page_title=\"MiniMind-V1\")\nst.title(\"MiniMind-V1\")\n\nmodel_id = \"./minimind-v1\"\n\n\n@st.cache_resource\ndef load_model_tokenizer():\n    model = AutoModelForCausalLM.from_pretrained(\n        model_id,\n        trust_remote_code=True\n    )\n    tokenizer = AutoTokenizer.from_pretrained(\n        model_id,\n        use_fast=False,\n        trust_remote_code=True\n    )\n    model = model.eval()\n    generation_config = GenerationConfig.from_pretrained(model_id)\n    return model, tokenizer, generation_config\n\n\ndef clear_chat_messages():\n    del st.session_state.messages\n    del st.session_state.chat_messages\n\n\ndef init_chat_messages():\n    with st.chat_message(\"assistant\", avatar='🤖'):\n        st.markdown(\"我是由JingyaoGong创造的MiniMind，很高兴为您服务😄  \\n\"\n                    \"注：所有AI生成内容的准确性和立场无法保证，不代表我们的态度或观点。\")\n\n    if \"messages\" in st.session_state:\n        for message in st.session_state.messages:\n            avatar = \"🫡\" if message[\"role\"] == \"user\" else \"🤖\"\n            with st.chat_message(message[\"role\"], avatar=avatar):\n                st.markdown(message[\"content\"])\n    else:\n        st.session_state.messages = []\n        st.session_state.chat_messages = []\n\n    return st.session_state.messages\n\n\nst.sidebar.title(\"设定调整\")\nst.session_state.history_chat_num = st.sidebar.slider(\"携带历史对话条数\", 0, 6, 0, step=2)\nst.session_state.max_new_tokens = st.sidebar.slider(\"最大输入/生成长度\", 256, 768, 512, step=1)\nst.session_state.top_k = st.sidebar.slider(\"top_k\", 0, 16, 14, step=1)\nst.session_state.temperature = st.sidebar.slider(\"temperature\", 0.3, 1.3, 0.5, step=0.01)\n\n\ndef setup_seed(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\n\ndef main():\n    model, tokenizer, generation_config = load_model_tokenizer()\n    messages = init_chat_messages()\n\n    if prompt := st.chat_input(\"Shift + Enter 换行, Enter 发送\"):\n        with st.chat_message(\"user\", avatar='🧑‍💻'):\n            st.markdown(prompt)\n            messages.append({\"role\": \"user\", \"content\": prompt})\n            st.session_state.chat_messages.append({\"role\": \"user\", \"content\": '请问，' + prompt + '？'})\n        with st.chat_message(\"assistant\", avatar='🤖'):\n            placeholder = st.empty()\n            # Generate a random seed\n            random_seed = random.randint(0, 2 ** 32 - 1)\n            setup_seed(random_seed)\n\n            new_prompt = tokenizer.apply_chat_template(\n                st.session_state.chat_messages[-(st.session_state.history_chat_num + 1):],\n                tokenize=False,\n                add_generation_prompt=True\n            )[-(st.session_state.max_new_tokens - 1):]\n\n            x = tokenizer(new_prompt).data['input_ids']\n            x = (torch.tensor(x, dtype=torch.long)[None, ...])\n            with torch.no_grad():\n                res_y = model.generate(x, tokenizer.eos_token_id, max_new_tokens=st.session_state.max_new_tokens,\n                                       temperature=st.session_state.temperature,\n                                       top_k=st.session_state.top_k, stream=True)\n                try:\n                    y = next(res_y)\n                except StopIteration:\n                    return\n\n                while y != None:\n                    answer = tokenizer.decode(y[0].tolist())\n                    if answer and answer[-1] == '�':\n                        try:\n                            y = next(res_y)\n                        except:\n                            break\n                        continue\n                    if not len(answer):\n                        try:\n                            y = next(res_y)\n                        except:\n                            break\n                        continue\n                    placeholder.markdown(answer)\n                    try:\n                        y = next(res_y)\n                    except:\n                        break\n\n            assistant_answer = answer.replace(new_prompt, \"\")\n            messages.append({\"role\": \"assistant\", \"content\": assistant_answer})\n            st.session_state.chat_messages.append({\"role\": \"assistant\", \"content\": assistant_answer})\n\n    st.button(\"清空对话\", on_click=clear_chat_messages)\n\n\nif __name__ == \"__main__\":\n    main()\n"
        },
        {
          "name": "images",
          "type": "tree",
          "content": null
        },
        {
          "name": "model",
          "type": "tree",
          "content": null
        },
        {
          "name": "my_openai_api.py",
          "type": "blob",
          "size": 15.0302734375,
          "content": "# encoding: utf-8\nimport json\nimport re\nimport time\nimport uuid\nimport warnings\n\nimport tiktoken\nimport torch\nimport numpy as np\nfrom typing import List\nfrom flask import Flask, current_app, request, Blueprint, stream_with_context\nfrom flask_cors import CORS\nfrom sentence_transformers import SentenceTransformer\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nfrom marshmallow import validate, Schema, fields, EXCLUDE\nfrom pydantic import BaseModel\n\nwarnings.filterwarnings('ignore', category=UserWarning)\n\n# ------------------------------------------------------------------------------------------------------------------\nDEVICE_NAME = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\nDEVICE = torch.device(DEVICE_NAME)\nMODEL_PATH = \"./minimind-v1-small\"\nTOKENIZE_PATH = MODEL_PATH\nmax_new_tokens = 1024\ntemperature = 0.7\ntop_k = 16\n\n\n# ------------------------------------------------------------------------------------------------------------------\n\nclass Transformers():\n    def __init__(self, app=None, tokenizer=None, model=None):\n        # self.chat = None\n        if app is not None:\n            self.init_app(app, tokenizer, model)\n\n    def init_app(self, app, tokenizer=None, model=None, chat=None):\n        self.tokenizer = tokenizer\n        self.model = model\n        # if chat is None:\n        #     # self.chat = model.chat\n        #     self.chat = self.chat\n\n    # gpt2's\n    def build_chat_input(self, tokenizer, messages: List[dict]):\n        new_prompt = tokenizer.apply_chat_template(\n            messages,\n            tokenize=False,\n            add_generation_prompt=True\n        )[-(max_new_tokens - 1):]\n        inputs_ids = tokenizer(new_prompt).data['input_ids']\n        inputs_ids = (torch.tensor(inputs_ids, dtype=torch.long, device=DEVICE)[None, ...])\n        return inputs_ids, tokenizer.eos_token_id, new_prompt\n\n    def chat_stream(self, tokenizer, messages: List[dict], stream=True):\n        input_ids, eos_token_id, new_prompt = self.build_chat_input(tokenizer, messages)\n        if stream:\n            res_y = self.model.generate(input_ids, tokenizer.eos_token_id, max_new_tokens=max_new_tokens,\n                                        temperature=temperature, top_k=top_k, stream=True)\n\n            try:\n                y = next(res_y)\n            except:\n                print(\"No answer\")\n                return 'No answer'\n\n            history_idx = 0\n            while y != None:\n                answer = tokenizer.decode(y[0].tolist())\n                if answer and answer[-1] == '�':\n                    try:\n                        y = next(res_y)\n                    except:\n                        break\n                    continue\n                # print(answer)\n                if not len(answer):\n                    try:\n                        y = next(res_y)\n                    except:\n                        break\n                    continue\n\n                yield answer[history_idx:]\n                try:\n                    y = next(res_y)\n                except:\n                    break\n                history_idx = len(answer)\n                if not stream:\n                    break\n\n    def chat_no_stream(self, tokenizer, messages: List[dict]):\n        input_ids, eos_token_id, new_prompt = self.build_chat_input(tokenizer, messages)\n        res_y = self.model.generate(input_ids, tokenizer.eos_token_id, max_new_tokens=max_new_tokens,\n                                    temperature=temperature, top_k=top_k, stream=False)\n        y = next(res_y)\n        answer = tokenizer.decode(y[0].tolist())\n        return answer\n\n\ntfs = Transformers()\nbase_tfs = Transformers()\n\nmodels_bp = Blueprint('Models', __name__, url_prefix='/v1/models')\nchat_bp = Blueprint('Chat', __name__, url_prefix='/v1/chat')\ncompletions_bp = Blueprint('Completions', __name__, url_prefix='/v1/completions')\nembedding_bp = Blueprint('Embeddings', __name__, url_prefix='/v1')\n\n\ndef sse(line, field=\"data\"):\n    return \"{}: {}\\n\\n\".format(\n        field, json.dumps(line, ensure_ascii=False) if isinstance(line, dict) else line)\n\n\ndef empty_cache():\n    if torch.backends.mps.is_available():\n        torch.mps.empty_cache()\n\n\ndef create_app():\n    app = Flask(__name__)\n    CORS(app)\n    app.register_blueprint(models_bp)\n    app.register_blueprint(chat_bp)\n    app.register_blueprint(completions_bp)\n    app.register_blueprint(embedding_bp)\n\n    @app.after_request\n    def after_request(resp):\n        empty_cache()\n        return resp\n\n    tokenizer = AutoTokenizer.from_pretrained(\n        TOKENIZE_PATH, trust_remote_code=True, use_fast=False)\n\n    model = AutoModelForCausalLM.from_pretrained(\n        MODEL_PATH, trust_remote_code=True).to(DEVICE)\n    # model.generation_config = GenerationConfig.from_pretrained(model_name)\n\n    tfs.init_app(app, tokenizer, model)\n    base_tfs.init_app(app, tokenizer, model)\n\n    return app\n\n\nclass ModelSchema(Schema):\n    id = fields.Str()\n    object = fields.Str(dump_default=\"model\", metadata={\"example\": \"model\"})\n    created = fields.Int(dump_default=lambda: int(time.time()), metadata={\"example\": 1695402567})\n    owned_by = fields.Str(dump_default=\"owner\", metadata={\"example\": \"owner\"})\n\n\nclass ModelListSchema(Schema):\n    object = fields.Str(dump_default=\"list\", metadata={\"example\": \"list\"})\n    data = fields.List(fields.Nested(ModelSchema), dump_default=[])\n\n\nclass ChatMessageSchema(Schema):\n    role = fields.Str(required=True, metadata={\"example\": \"system\"})\n    content = fields.Str(required=True, metadata={\"example\": \"You are a helpful assistant.\"})\n\n\nclass CreateChatCompletionSchema(Schema):\n    class Meta:\n        unknown = EXCLUDE  # 忽略未知的字段\n\n    model = fields.Str(required=True, metadata={\"example\": \"minimind\"})\n    messages = fields.List(\n        fields.Nested(ChatMessageSchema), required=True,\n        metadata={\"example\": [\n            ChatMessageSchema().dump({\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}),\n            ChatMessageSchema().dump({\"role\": \"user\", \"content\": \"Hello!\"})\n        ]}\n    )\n    temperature = fields.Float(load_default=1.0, metadata={\"example\": 1.0})\n    top_p = fields.Float(load_default=1.0, metadata={\"example\": 1.0})\n    n = fields.Int(load_default=1, metadata={\"example\": 1})\n    max_tokens = fields.Int(load_default=None, metadata={\"example\": None})\n    stream = fields.Bool(load_default=False, example=False)\n    presence_penalty = fields.Float(load_default=0.0, example=0.0)\n    frequency_penalty = fields.Float(load_default=0.0, example=0.0)\n\n\nclass ChatCompletionChoiceSchema(Schema):\n    index = fields.Int(metadata={\"example\": 0})\n    message = fields.Nested(ChatMessageSchema, metadata={\n        \"example\": ChatMessageSchema().dump(\n            {\"role\": \"assistant\", \"content\": \"\\n\\nHello there, how may I assist you today?\"}\n        )})\n    finish_reason = fields.Str(\n        validate=validate.OneOf([\"stop\", \"length\", \"content_filter\", \"function_call\"]),\n        metadata={\"example\": \"stop\"})\n\n\nclass ChatCompletionSchema(Schema):\n    id = fields.Str(\n        dump_default=lambda: uuid.uuid4().hex,\n        metadata={\"example\": \"cmpl-uqkvlQyYK7bGYrRHQ0eXlWi7\"})\n    object = fields.Constant(\"chat.completion\")\n    created = fields.Int(dump_default=lambda: int(time.time()), metadata={\"example\": 1695402567})\n    model = fields.Str(metadata={\"example\": \"minimind\"})\n    choices = fields.List(fields.Nested(ChatCompletionChoiceSchema))\n\n\nclass ChatDeltaSchema(Schema):\n    role = fields.Str(metadata={\"example\": \"assistant\"})\n    content = fields.Str(required=True, metadata={\"example\": \"Hello\"})\n\n\nclass ChatCompletionChunkChoiceSchema(Schema):\n    index = fields.Int(metadata={\"example\": 0})\n    delta = fields.Nested(ChatDeltaSchema, metadata={\"example\": ChatDeltaSchema().dump(\n        {\"role\": \"assistant\", \"example\": \"Hello\"})})\n    finish_reason = fields.Str(\n        validate=validate.OneOf([\"stop\", \"length\", \"content_filter\", \"function_call\"]),\n        metadata={\"example\": \"stop\"})\n\n\nclass ChatCompletionChunkShema(Schema):\n    id = fields.Str(\n        dump_default=lambda: uuid.uuid4().hex,\n        metadata={\"example\": \"cmpl-uqkvlQyYK7bGYrRHQ0eXlWi7\"})\n    object = fields.Constant(\"chat.completion.chunk\")\n    created = fields.Int(dump_default=lambda: int(time.time()), metadata={\"example\": 1695402567})\n    model = fields.Str(metadata={\"example\": \"minimind\"})\n    choices = fields.List(fields.Nested(ChatCompletionChunkChoiceSchema))\n\n\nclass CreateCompletionSchema(Schema):\n    model = fields.Str(required=True, metadata={\"example\": \"minimind\"})\n    prompt = fields.Raw(metadata={\"example\": \"Say this is a test\"})\n    max_tokens = fields.Int(load_default=16, metadata={\"example\": 256})\n    temperature = fields.Float(load_default=1.0, metadata={\"example\": 1.0})\n    top_p = fields.Float(load_default=1.0, metadata={\"example\": 1.0})\n    n = fields.Int(load_default=1, metadata={\"example\": 1})\n    stream = fields.Bool(load_default=False, example=False)\n    logit_bias = fields.Dict(load_default=None, example={})\n    presence_penalty = fields.Float(load_default=0.0, example=0.0)\n    frequency_penalty = fields.Float(load_default=0.0, example=0.0)\n\n\nclass CompletionChoiceSchema(Schema):\n    index = fields.Int(load_default=0, metadata={\"example\": 0})\n    text = fields.Str(required=True, metadata={\"example\": \"登鹳雀楼->王之涣\\n夜雨寄北->\"})\n    logprobs = fields.Dict(load_default=None, metadata={\"example\": {}})\n    finish_reason = fields.Str(\n        validate=validate.OneOf([\"stop\", \"length\", \"content_filter\", \"function_call\"]),\n        metadata={\"example\": \"stop\"})\n\n\nclass CompletionUsageSchema(Schema):\n    prompt_tokens = fields.Int(metadata={\"example\": 5})\n    completion_tokens = fields.Int(metadata={\"example\": 7})\n    total_tokens = fields.Int(metadata={\"example\": 12})\n\n\nclass CompletionSchema(Schema):\n    id = fields.Str(\n        dump_default=lambda: uuid.uuid4().hex,\n        metadata={\"example\": \"cmpl-uqkvlQyYK7bGYrRHQ0eXlWi7\"})\n    object = fields.Constant(\"text_completion\")\n    created = fields.Int(dump_default=lambda: int(time.time()), metadata={\"example\": 1695402567})\n    model = fields.Str(metadata={\"example\": \"minimind\"})\n    choices = fields.List(fields.Nested(CompletionChoiceSchema))\n    usage = fields.Nested(CompletionUsageSchema)\n\n\n@stream_with_context\ndef stream_chat_generate(messages):\n    delta = ChatDeltaSchema().dump(\n        {\"role\": \"assistant\"})\n    choice = ChatCompletionChunkChoiceSchema().dump(\n        {\"index\": 0, \"delta\": delta, \"finish_reason\": None})\n\n    yield sse(\n        ChatCompletionChunkShema().dump({\n            \"model\": \"minimind\",\n            \"choices\": [choice]})\n    )\n\n    # 调用 chat 方法并遍历其返回的生成器\n    for response in tfs.chat_stream(tfs.tokenizer, messages):\n        delta = ChatDeltaSchema().dump(\n            {\"content\": response})\n        choice = ChatCompletionChunkChoiceSchema().dump(\n            {\"index\": 0, \"delta\": delta, \"finish_reason\": None})\n\n        yield sse(\n            ChatCompletionChunkShema().dump({\n                \"model\": \"minimind\",\n                \"choices\": [choice]})\n        )\n\n    yield sse('[DONE]')\n\n\n@chat_bp.route(\"/completions\", methods=['POST'])\ndef create_chat_completion():\n    create_chat_completion = CreateChatCompletionSchema().load(request.json)\n\n    if create_chat_completion[\"stream\"]:\n        return current_app.response_class(\n            stream_chat_generate(create_chat_completion[\"messages\"]),\n            mimetype=\"text/event-stream\"\n        )\n    else:\n        response = tfs.chat_no_stream(tfs.tokenizer, create_chat_completion[\"messages\"])\n\n        message = ChatMessageSchema().dump(\n            {\"role\": \"assistant\", \"content\": response})\n        choice = ChatCompletionChoiceSchema().dump(\n            {\"index\": 0, \"message\": message, \"finish_reason\": \"stop\"})\n\n        return ChatCompletionSchema().dump({\n            \"model\": \"minimind\",\n            \"choices\": [choice]})\n\n\nclass EmbeddingRequest(BaseModel):\n    input: List[str]\n    model: str\n\n\n@embedding_bp.route(\"/embeddings\", methods=['POST'])\ndef get_embeddings():\n    request_data = request.get_json()  # 获取 POST 请求体中的 JSON 数据\n    request_params = EmbeddingRequest(**request_data)  # 将 JSON 数据转换为 EmbeddingRequest 对象\n\n    def expand_features(embedding, target_length):\n        poly = PolynomialFeatures(degree=2)\n        expanded_embedding = poly.fit_transform(embedding.reshape(1, -1))\n        expanded_embedding = expanded_embedding.flatten()\n        if len(expanded_embedding) > target_length:\n            # 如果扩展后的特征超过目标长度，可以通过截断或其他方法来减少维度\n            expanded_embedding = expanded_embedding[:target_length]\n        elif len(expanded_embedding) < target_length:\n            # 如果扩展后的特征少于目标长度，可以通过填充或其他方法来增加维度\n            expanded_embedding = np.pad(\n                expanded_embedding, (0, target_length - len(expanded_embedding))\n            )\n        return expanded_embedding\n\n    def num_tokens_from_string(string: str) -> int:\n        \"\"\"Returns the number of tokens in a text string.\"\"\"\n        encoding = tiktoken.get_encoding('cl100k_base')\n        num_tokens = len(encoding.encode(string))\n        return num_tokens\n\n    def has_chinese_char(s):\n        pattern = re.compile(r'[\\u4e00-\\u9fa5]')\n        # if bool(pattern.search(s)):\n        #     print('m3e编码')\n        # else:\n        #     print('bge编码')\n\n        return bool(pattern.search(s))\n\n    # 计算嵌入向量和tokens数量\n    embeddings = [embeddings_model_m3e.encode(text)\n                  if has_chinese_char(text)\n                  else embeddings_model_bge.encode(text)\n                  for text in request_params.input]\n\n    # 如果嵌入向量的维度不为1536，则使用插值法扩展至1536维度\n    embeddings = [\n        expand_features(embedding, 768) if len(embedding) < 768 else embedding\n        for embedding in embeddings\n    ]\n\n    # Min-Max normalization 归一化\n    embeddings = [embedding / np.linalg.norm(embedding) for embedding in embeddings]\n\n    # 将numpy数组转换为列表\n    embeddings = [embedding.tolist() for embedding in embeddings]\n    prompt_tokens = sum(len(text.split()) for text in request_params.input)\n    total_tokens = sum(num_tokens_from_string(text) for text in request_params.input)\n\n    response = {\n        \"data\": [\n            {\"embedding\": embedding, \"index\": index, \"object\": \"embedding\"}\n            for index, embedding in enumerate(embeddings)\n        ],\n        \"model\": request_params.model,\n        \"object\": \"list\",\n        \"usage\": {\n            \"prompt_tokens\": prompt_tokens,\n            \"total_tokens\": total_tokens,\n        },\n    }\n    # print(response)\n    return response\n\n\napp = create_app()\n\nif __name__ == '__main__':\n    use_emb = False\n    try:\n        import ngrok\n        import logging\n\n        logging.basicConfig(level=logging.INFO)\n        listener = ngrok.werkzeug_develop()\n    except Exception:\n        pass\n\n    embeddings_model_m3e = SentenceTransformer('.\\\\m3e-base', device='cpu') if use_emb else None\n    embeddings_model_bge = SentenceTransformer('.\\\\bge-base-en-v1.5', device='cpu') if use_emb else None\n\n    app.run(debug=False, host=\"0.0.0.0\", port=8000)\n"
        },
        {
          "name": "requirements.txt",
          "type": "blob",
          "size": 0.4296875,
          "content": "datasets==2.16.1\ndatasketch==1.6.4\nFlask==3.0.3\nFlask_Cors==4.0.0\njieba==0.42.1\njsonlines==4.0.0\nmarshmallow==3.22.0\nmatplotlib==3.5.1\nngrok==1.4.0\nnltk==3.8\nnumpy==1.26.4\nopenai==1.42.0\npandas==1.5.3\npeft==0.7.1\npsutil==5.9.8\npydantic==2.8.2\nrich==13.7.1\nscikit_learn==1.5.1\nsentence_transformers==2.3.1\nsimhash==2.1.2\ntiktoken==0.5.1\ntorch==2.1.2\ntransformers==4.44.0\njinja2==3.1.2\njsonlines==4.0.0\ntrl==0.11.3\nujson==5.1.0\nwandb==0.18.3\n"
        },
        {
          "name": "train_tokenizer.py",
          "type": "blob",
          "size": 4.765625,
          "content": "import random\nfrom tqdm import tqdm\nfrom transformers import AutoTokenizer\nimport json\nfrom datasets import load_dataset\nfrom tokenizers import (\n    decoders,\n    models,\n    normalizers,\n    pre_tokenizers,\n    processors,\n    trainers,\n    Tokenizer,\n)\nimport os\n\nrandom.seed(42)\n\ndef train_tokenizer():\n    # 读取JSONL文件并提取文本数据\n    def read_texts_from_jsonl(file_path):\n        with open(file_path, 'r', encoding='utf-8') as f:\n            for line in f:\n                data = json.loads(line)\n                yield data['text']\n\n    data_path = './dataset/tokenizer_train.jsonl'\n\n    # 初始化tokenizer\n    tokenizer = Tokenizer(models.BPE())\n    tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=False)\n\n    # 定义特殊token\n    special_tokens = [\"<unk>\", \"<s>\", \"</s>\"]\n\n    # 设置训练器并添加特殊token\n    trainer = trainers.BpeTrainer(\n        vocab_size=6400,\n        special_tokens=special_tokens,  # 确保这三个token被包含\n        show_progress=True,\n        initial_alphabet=pre_tokenizers.ByteLevel.alphabet()\n    )\n\n    # 读取文本数据\n    texts = read_texts_from_jsonl(data_path)\n\n    # 训练tokenizer\n    tokenizer.train_from_iterator(texts, trainer=trainer)\n\n    # 设置解码器\n    tokenizer.decoder = decoders.ByteLevel()\n\n    # 检查特殊token的索引\n    assert tokenizer.token_to_id(\"<unk>\") == 0\n    assert tokenizer.token_to_id(\"<s>\") == 1\n    assert tokenizer.token_to_id(\"</s>\") == 2\n\n    # 保存tokenizer\n    tokenizer_dir = \"./model/minimind_tokenizer\"\n    os.makedirs(tokenizer_dir, exist_ok=True)\n    tokenizer.save(os.path.join(tokenizer_dir, \"tokenizer.json\"))\n    tokenizer.model.save(\"./model/minimind_tokenizer\")\n\n    # 手动创建配置文件\n    config = {\n        \"add_bos_token\": False,\n        \"add_eos_token\": False,\n        \"add_prefix_space\": True,\n        \"added_tokens_decoder\": {\n            \"0\": {\n                \"content\": \"<unk>\",\n                \"lstrip\": False,\n                \"normalized\": False,\n                \"rstrip\": False,\n                \"single_word\": False,\n                \"special\": True\n            },\n            \"1\": {\n                \"content\": \"<s>\",\n                \"lstrip\": False,\n                \"normalized\": False,\n                \"rstrip\": False,\n                \"single_word\": False,\n                \"special\": True\n            },\n            \"2\": {\n                \"content\": \"</s>\",\n                \"lstrip\": False,\n                \"normalized\": False,\n                \"rstrip\": False,\n                \"single_word\": False,\n                \"special\": True\n            }\n        },\n        \"additional_special_tokens\": [],\n        \"bos_token\": \"<s>\",\n        \"clean_up_tokenization_spaces\": False,\n        \"eos_token\": \"</s>\",\n        \"legacy\": True,\n        \"model_max_length\": 1000000000000000019884624838656,\n        \"pad_token\": None,\n        \"sp_model_kwargs\": {},\n        \"spaces_between_special_tokens\": False,\n        \"tokenizer_class\": \"PreTrainedTokenizerFast\",\n        \"unk_token\": \"<unk>\",\n        \"use_default_system_prompt\": False,\n        \"chat_template\": \"{% if messages[0]['role'] == 'system' %}{% set system_message = messages[0]['content'] %}{% endif %}{% if system_message is defined %}{{ system_message }}{% endif %}{% for message in messages %}{% set content = message['content'] %}{% if message['role'] == 'user' %}{{ '<s>user\\\\n' + content + '</s>\\\\n<s>assistant\\\\n' }}{% elif message['role'] == 'assistant' %}{{ content + '</s>' + '\\\\n' }}{% endif %}{% endfor %}\"\n    }\n\n    # 保存配置文件\n    with open(os.path.join(tokenizer_dir, \"tokenizer_config.json\"), \"w\", encoding=\"utf-8\") as config_file:\n        json.dump(config, config_file, ensure_ascii=False, indent=4)\n\n    print(\"Tokenizer training completed and saved.\")\n\n\ndef eval_tokenizer():\n    from transformers import AutoTokenizer\n\n    # 加载预训练的tokenizer\n    tokenizer = AutoTokenizer.from_pretrained(\"./model/minimind_tokenizer\")\n\n    messages = [\n        {\"role\": \"system\", \"content\": \"你是一个优秀的聊天机器人，总是给我正确的回应！\"},\n        {\"role\": \"user\", \"content\": '你来自哪里？'},\n        {\"role\": \"assistant\", \"content\": '我来自地球'}\n    ]\n    new_prompt = tokenizer.apply_chat_template(\n        messages,\n        tokenize=False\n    )\n    print(new_prompt)\n\n    # 获取实际词汇表长度（包括特殊符号）\n    actual_vocab_size = len(tokenizer)\n    print('tokenizer实际词表长度：', actual_vocab_size)\n\n    model_inputs = tokenizer(new_prompt)\n    print('encoder长度：', len(model_inputs['input_ids']))\n\n    input_ids = model_inputs['input_ids']\n    response = tokenizer.decode(input_ids)\n    print('decoder和原始文本是否一致：', response == new_prompt)\n\ndef main():\n    # train_tokenizer()\n    eval_tokenizer()\n\n\nif __name__ == '__main__':\n    main()\n"
        },
        {
          "name": "中文逐行注释",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}