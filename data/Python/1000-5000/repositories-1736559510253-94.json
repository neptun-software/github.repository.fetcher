{
  "metadata": {
    "timestamp": 1736559510253,
    "page": 94,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjEwMA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "macanv/BERT-BiLSTM-CRF-NER",
      "stars": 4755,
      "defaultBranch": "master",
      "files": [
        {
          "name": "README.md",
          "type": "blob",
          "size": 17.3173828125,
          "content": "# BERT-BiLSTM-CRF-NER\nTensorflow solution of NER task Using BiLSTM-CRF model with Google BERT Fine-tuning\n\n使用谷歌的BERT模型在BLSTM-CRF模型上进行预训练用于中文命名实体识别的Tensorflow代码'\n\n中文文档请查看https://blog.csdn.net/macanv/article/details/85684284  如果对您有帮助，麻烦点个star,谢谢~~  \n\nWelcome to star this repository!\n\nThe Chinese training data($PATH/NERdata/) come from:https://github.com/zjy-ucas/ChineseNER \n  \nThe CoNLL-2003 data($PATH/NERdata/ori/) come from:https://github.com/kyzhouhzau/BERT-NER \n  \nThe evaluation codes come from:https://github.com/guillaumegenthial/tf_metrics/blob/master/tf_metrics/__init__.py  \n\n\nTry to implement NER work based on google's BERT code and BiLSTM-CRF network!\nThis project may be more close to process Chinese data. but other language only need Modify a small amount of code.\n\nTHIS PROJECT ONLY SUPPORT Python3.  \n###################################################################\n## Download project and install  \nYou can install this project by:  \n```\npip install bert-base==0.0.9 -i https://pypi.python.org/simple\n```\n\nOR\n```angular2html\ngit clone https://github.com/macanv/BERT-BiLSTM-CRF-NER\ncd BERT-BiLSTM-CRF-NER/\npython3 setup.py install\n```\n\nif you do not want to install, you just need clone this project and reference the file of <run.py> to train the model or start the service. \n\n## UPDATE:\n- 2020.2.6 add simple flask ner service code\n- 2019.2.25 Fix some bug for ner service\n- 2019.2.19: add text classification service\n-  fix Missing loss error\n- add label_list params in train process, so you can using -label_list xxx to special labels in training process.  \n  \n    \n## Train model:\nYou can use -help to view the relevant parameters of the training named entity recognition model, where data_dir, bert_config_file, output_dir, init_checkpoint, vocab_file must be specified.\n```angular2html\nbert-base-ner-train -help\n```\n![](./pictures/ner_help.png)  \n  \n\ntrain/dev/test dataset is like this:\n```\n海 O\n钓 O\n比 O\n赛 O\n地 O\n点 O\n在 O\n厦 B-LOC\n门 I-LOC\n与 O\n金 B-LOC\n门 I-LOC\n之 O\n间 O\n的 O\n海 O\n域 O\n。 O\n```\nThe first one of each line is a token, the second is token's label, and the line is divided by a blank line. The maximum length of each sentence is [max_seq_length] params.  \nYou can get training data from above two git repos  \nYou can training ner model by running below command:  \n```angular2html\nbert-base-ner-train \\\n    -data_dir {your dataset dir}\\\n    -output_dir {training output dir}\\\n    -init_checkpoint {Google BERT model dir}\\\n    -bert_config_file {bert_config.json under the Google BERT model dir} \\\n    -vocab_file {vocab.txt under the Google BERT model dir}\n```\nlike my init_checkpoint: \n```\ninit_checkpoint = F:\\chinese_L-12_H-768_A-12\\bert_model.ckpt\n```\nyou can special labels using -label_list params, the project get labels from training data.  \n```angular2html\n# using , split\n-labels 'B-LOC, I-LOC ...'\nOR save label in a file like labels.txt, one line one label\n-labels labels.txt\n```    \n\nAfter training model, the NER model will be saved in {output_dir} which you special above cmd line.  \n##### My Training environment：Tesla P40 24G mem  \n\n## As Service\nMany server and client code comes from excellent open source projects: [bert as service of hanxiao](https://github.com/hanxiao/bert-as-service) If my code violates any license agreement, please let me know and I will correct it the first time.\n~~and NER server/client service code can be applied to other tasks with simple modifications, such as text categorization, which I will provide later.~~\nthis project private Named Entity Recognition and Text Classification server service.\nWelcome to submit your request or share your model, if you want to share it on Github or my work.  \n\nYou can use -help to view the relevant parameters of the NER as Service:\nwhich model_dir, bert_model_dir is need\n```\nbert-base-serving-start -help\n```\n![](./pictures/server_help.png)\n\nand than you can using below cmd start ner service:\n```angular2html\nbert-base-serving-start \\\n    -model_dir C:\\workspace\\python\\BERT_Base\\output\\ner2 \\\n    -bert_model_dir F:\\chinese_L-12_H-768_A-12\n    -model_pb_dir C:\\workspace\\python\\BERT_Base\\model_pb_dir\n    -mode NER\n```\nor text classification service:\n```angular2html\nbert-base-serving-start \\\n    -model_dir C:\\workspace\\python\\BERT_Base\\output\\ner2 \\\n    -bert_model_dir F:\\chinese_L-12_H-768_A-12\n    -model_pb_dir C:\\workspace\\python\\BERT_Base\\model_pb_dir\n    -mode CLASS\n    -max_seq_len 202\n```\n\nas you see:   \nmode: If mode is NER/CLASS, then the service identified by the Named Entity Recognition/Text Classification will be started. If it is BERT, it will be the same as the [bert as service] project.  \nbert_model_dir: bert_model_dir is a BERT model, you can download from https://github.com/google-research/bert\nner_model_dir: your ner model checkpoint dir\nmodel_pb_dir: model freeze save dir, after run optimize func, there will contains like ner_model.pb binary file  \n>You can download my ner model from：https://pan.baidu.com/s/1m9VcueQ5gF-TJc00sFD88w, ex_code: guqq\n> Or text classification model from: https://pan.baidu.com/s/1oFPsOUh1n5AM2HjDIo2XCw, ex_code: bbu8   \nSet ner_mode.pb/classification_model.pb to model_pb_dir, and set other file to model_dir(Different models need to be stored separately, you can set ner models label_list.pkl and label2id.pkl to model_dir/ner/ and set text classification file to model_dir/text_classification) , Text classification model can classify 12 categories of Chinese data： '游戏', '娱乐', '财经', '时政', '股票', '教育', '社会', '体育', '家居', '时尚', '房产', '彩票'  \n\nYou can see below service starting info:\n![](./pictures/service_1.png)\n![](./pictures/service_2.png)\n\n\nyou can using below code test client:  \n#### 1. NER Client\n```angular2html\nimport time\nfrom bert_base.client import BertClient\n\nwith BertClient(show_server_config=False, check_version=False, check_length=False, mode='NER') as bc:\n    start_t = time.perf_counter()\n    str = '1月24日，新华社对外发布了中央对雄安新区的指导意见，洋洋洒洒1.2万多字，17次提到北京，4次提到天津，信息量很大，其实也回答了人们关心的很多问题。'\n    rst = bc.encode([str, str])\n    print('rst:', rst)\n    print(time.perf_counter() - start_t)\n```\nyou can see this after run the above code:\n![](./pictures/server_ner_rst.png)\nIf you want to customize the word segmentation method, you only need to make the following simple changes on the client side code.\n\n```angular2html\nrst = bc.encode([list(str), list(str)], is_tokenized=True)\n```  \n\n#### 2. Text Classification Client\n```angular2html\nwith BertClient(show_server_config=False, check_version=False, check_length=False, mode='CLASS') as bc:\n    start_t = time.perf_counter()\n    str1 = '北京时间2月17日凌晨，第69届柏林国际电影节公布主竞赛单元获奖名单，王景春、咏梅凭借王小帅执导的中国影片《地久天长》连夺最佳男女演员双银熊大奖，这是中国演员首次包揽柏林电影节最佳男女演员奖，为华语影片刷新纪录。与此同时，由青年导演王丽娜执导的影片《第一次的别离》也荣获了本届柏林电影节新生代单元国际评审团最佳影片，可以说，在经历数个获奖小年之后，中国电影在柏林影展再次迎来了高光时刻。'\n    str2 = '受粤港澳大湾区规划纲要提振，港股周二高开，恒指开盘上涨近百点，涨幅0.33%，报28440.49点，相关概念股亦集体上涨，电子元件、新能源车、保险、基建概念多数上涨。粤泰股份、珠江实业、深天地A等10余股涨停；中兴通讯、丘钛科技、舜宇光学分别高开1.4%、4.3%、1.6%。比亚迪电子、比亚迪股份、光宇国际分别高开1.7%、1.2%、1%。越秀交通基建涨近2%，粤海投资、碧桂园等多股涨超1%。其他方面，日本软银集团股价上涨超0.4%，推动日经225和东证指数齐齐高开，但随后均回吐涨幅转跌东证指数跌0.2%，日经225指数跌0.11%，报21258.4点。受芯片制造商SK海力士股价下跌1.34％拖累，韩国综指下跌0.34％至2203.9点。澳大利亚ASX 200指数早盘上涨0.39％至6089.8点，大多数行业板块均现涨势。在保健品品牌澳佳宝下调下半财年的销售预期后，其股价暴跌超过23％。澳佳宝CEO亨弗里（Richard Henfrey）认为，公司下半年的利润可能会低于上半年，主要是受到销售额疲弱的影响。同时，亚市早盘澳洲联储公布了2月会议纪要，政策委员将继续谨慎评估经济增长前景，因前景充满不确定性的影响，稳定当前的利率水平比贸然调整利率更为合适，而且当前利率水平将有利于趋向通胀目标及改善就业，当前劳动力市场数据表现强势于其他经济数据。另一方面，经济增长前景亦令消费者消费意愿下滑，如果房价出现下滑，消费可能会进一步疲弱。在澳洲联储公布会议纪要后，澳元兑美元下跌近30点，报0.7120 。美元指数在昨日触及96.65附近的低点之后反弹至96.904。日元兑美元报110.56，接近上一交易日的低点。'\n    str3 = '新京报快讯 据国家市场监管总局消息，针对媒体报道水饺等猪肉制品检出非洲猪瘟病毒核酸阳性问题，市场监管总局、农业农村部已要求企业立即追溯猪肉原料来源并对猪肉制品进行了处置。两部门已派出联合督查组调查核实相关情况，要求猪肉制品生产企业进一步加强对猪肉原料的管控，落实检验检疫票证查验规定，完善非洲猪瘟检测和复核制度，防止染疫猪肉原料进入食品加工环节。市场监管总局、农业农村部等部门要求各地全面落实防控责任，强化防控措施，规范信息报告和发布，对不按要求履行防控责任的企业，一旦发现将严厉查处。专家认为，非洲猪瘟不是人畜共患病，虽然对猪有致命危险，但对人没有任何危害，属于只传猪不传人型病毒，不会影响食品安全。开展猪肉制品病毒核酸检测，可为防控溯源工作提供线索。'\n    rst = bc.encode([str1, str2, str3])\n    print('rst:', rst)\n    print('time used:{}'.format(time.perf_counter() - start_t))\n```\nyou can see this after run the above code:\n![](./pictures/text_class_rst.png)\n\nNote that it can not start NER service and Text Classification service together. but you can using twice command line start ner service and text classification with different port.  \n\n### Flask server service\nsometimes, multi thread deep learning model service may not use C/S service, you can useing simple http service replace that, like using flask.\nnow you can reference code:bert_base/server/simple_flask_http_service.py，building your simple http server service\n\n## License\nMIT.  \n\n# The following tutorial is an old version and will be removed in the future.\n\n## How to train\n#### 1. Download BERT chinese model :  \n ```\n wget https://storage.googleapis.com/bert_models/2018_11_03/chinese_L-12_H-768_A-12.zip  \n ```\n#### 2. create output dir\ncreate output path in project path:\n```angular2html\nmkdir output\n```\n#### 3. Train model\n\n##### first method \n```\n  python3 bert_lstm_ner.py   \\\n                  --task_name=\"NER\"  \\ \n                  --do_train=True   \\\n                  --do_eval=True   \\\n                  --do_predict=True\n                  --data_dir=NERdata   \\\n                  --vocab_file=checkpoint/vocab.txt  \\ \n                  --bert_config_file=checkpoint/bert_config.json \\  \n                  --init_checkpoint=checkpoint/bert_model.ckpt   \\\n                  --max_seq_length=128   \\\n                  --train_batch_size=32   \\\n                  --learning_rate=2e-5   \\\n                  --num_train_epochs=3.0   \\\n                  --output_dir=./output/result_dir/ \n ```       \n ##### OR replace the BERT path and project path in bert_lstm_ner.py\n ```\n if os.name == 'nt': #windows path config\n    bert_path = '{your BERT model path}'\n    root_path = '{project path}'\nelse: # linux path config\n    bert_path = '{your BERT model path}'\n    root_path = '{project path}'\n ```\n Than Run:\n ```angular2html\npython3 bert_lstm_ner.py\n```\n\n### USING BLSTM-CRF OR ONLY CRF FOR DECODE!\nJust alter bert_lstm_ner.py line of 450, the params of the function of add_blstm_crf_layer: crf_only=True or False  \n\nONLY CRF output layer:\n```\n    blstm_crf = BLSTM_CRF(embedded_chars=embedding, hidden_unit=FLAGS.lstm_size, cell_type=FLAGS.cell, num_layers=FLAGS.num_layers,\n                          dropout_rate=FLAGS.droupout_rate, initializers=initializers, num_labels=num_labels,\n                          seq_length=max_seq_length, labels=labels, lengths=lengths, is_training=is_training)\n    rst = blstm_crf.add_blstm_crf_layer(crf_only=True)\n```\n  \n  \nBiLSTM with CRF output layer\n```\n    blstm_crf = BLSTM_CRF(embedded_chars=embedding, hidden_unit=FLAGS.lstm_size, cell_type=FLAGS.cell, num_layers=FLAGS.num_layers,\n                          dropout_rate=FLAGS.droupout_rate, initializers=initializers, num_labels=num_labels,\n                          seq_length=max_seq_length, labels=labels, lengths=lengths, is_training=is_training)\n    rst = blstm_crf.add_blstm_crf_layer(crf_only=False)\n```\n\n## Result:\nall params using default\n#### In dev data set:\n![](./pictures/picture1.png)\n\n#### In test data set\n![](./pictures/picture2.png)\n\n#### entity leval result:\nlast two result are label level result, the entitly level result in code of line 796-798,this result will be output in predict process.\nshow my entity level result :\n![](./pictures/03E18A6A9C16082CF22A9E8837F7E35F.png)\n> my model can download from baidu cloud:  \n>链接：https://pan.baidu.com/s/1GfDFleCcTv5393ufBYdgqQ 提取码：4cus  \nNOTE: My model is trained by crf_only params\n\n## ONLINE PREDICT\nIf model is train finished, just run\n```angular2html\npython3 terminal_predict.py\n```\n![](./pictures/predict.png)\n \n ## Using NER as Service\n\n#### Service \nUsing NER as Service is simple, you just need to run the python script below in the project root path:\n```angular2html\npython3 runs.py \\ \n    -mode NER\n    -bert_model_dir /home/macan/ml/data/chinese_L-12_H-768_A-12 \\\n    -ner_model_dir /home/macan/ml/data/bert_ner \\\n    -model_pd_dir /home/macan/ml/workspace/BERT_Base/output/predict_optimizer \\\n    -num_worker 8\n```\n\n  \nYou can download my ner model from：https://pan.baidu.com/s/1m9VcueQ5gF-TJc00sFD88w, ex_code: guqq  \nSet ner_mode.pb to model_pd_dir, and set other file to ner_model_dir and than run last cmd  \n![](./pictures/service_1.png)\n![](./pictures/service_2.png)\n\n\n#### Client\nThe client using methods can reference client_test.py script\n```angular2html\nimport time\nfrom client.client import BertClient\n\nner_model_dir = 'C:\\workspace\\python\\BERT_Base\\output\\predict_ner'\nwith BertClient( ner_model_dir=ner_model_dir, show_server_config=False, check_version=False, check_length=False, mode='NER') as bc:\n    start_t = time.perf_counter()\n    str = '1月24日，新华社对外发布了中央对雄安新区的指导意见，洋洋洒洒1.2万多字，17次提到北京，4次提到天津，信息量很大，其实也回答了人们关心的很多问题。'\n    rst = bc.encode([str])\n    print('rst:', rst)\n    print(time.perf_counter() - start_t)\n```\nNOTE: input format you can sometime reference bert as service project.    \nWelcome to provide more client language code like java or others.  \n ## Using yourself data to train\n if you want to use yourself data to train ner model,you just modify  the get_labes func.\n ```angular2html\ndef get_labels(self):\n        return [\"O\", \"B-PER\", \"I-PER\", \"B-ORG\", \"I-ORG\", \"B-LOC\", \"I-LOC\", \"X\", \"[CLS]\", \"[SEP]\"]\n```\nNOTE: \"X\", “[CLS]”, “[SEP]” These three are necessary, you just replace your data label to this return list.  \nOr you can use last code lets the program automatically get the label from training data\n```angular2html\ndef get_labels(self):\n        # 通过读取train文件获取标签的方法会出现一定的风险。\n        if os.path.exists(os.path.join(FLAGS.output_dir, 'label_list.pkl')):\n            with codecs.open(os.path.join(FLAGS.output_dir, 'label_list.pkl'), 'rb') as rf:\n                self.labels = pickle.load(rf)\n        else:\n            if len(self.labels) > 0:\n                self.labels = self.labels.union(set([\"X\", \"[CLS]\", \"[SEP]\"]))\n                with codecs.open(os.path.join(FLAGS.output_dir, 'label_list.pkl'), 'wb') as rf:\n                    pickle.dump(self.labels, rf)\n            else:\n                self.labels = [\"O\", 'B-TIM', 'I-TIM', \"B-PER\", \"I-PER\", \"B-ORG\", \"I-ORG\", \"B-LOC\", \"I-LOC\", \"X\", \"[CLS]\", \"[SEP]\"]\n        return self.labels\n\n```\n\n\n## NEW UPDATE\n2019.1.30 Support pip install and command line control  \n\n2019.1.30 Add Service/Client for NER process  \n\n2019.1.9: Add code to remove the adam related parameters in the model, and reduce the size of the model file from 1.3GB to 400MB.  \n  \n2019.1.3: Add online predict code  \n\n\n\n## reference: \n+ The evaluation codes come from:https://github.com/guillaumegenthial/tf_metrics/blob/master/tf_metrics/__init__.py\n\n+ [https://github.com/google-research/bert](https://github.com/google-research/bert)\n      \n+ [https://github.com/kyzhouhzau/BERT-NER](https://github.com/kyzhouhzau/BERT-NER)\n\n+ [https://github.com/zjy-ucas/ChineseNER](https://github.com/zjy-ucas/ChineseNER)\n\n+ [https://github.com/hanxiao/bert-as-service](https://github.com/hanxiao/bert-as-service)\n> Any problem please open issue OR email me(ma_cancan@163.com)\n"
        },
        {
          "name": "bert_base",
          "type": "tree",
          "content": null
        },
        {
          "name": "build.sh",
          "type": "blob",
          "size": 0.0615234375,
          "content": "python setup.py sdist bdist_wheel\npython -m twine upload dist/*"
        },
        {
          "name": "client_test.py",
          "type": "blob",
          "size": 5.08984375,
          "content": "# -*- coding: utf-8 -*-\n\n\"\"\"\n\n @Time    : 2019/1/29 14:32\n @Author  : MaCan (ma_cancan@163.com)\n @File    : client_test.py\n\"\"\"\nimport time\nfrom bert_base.client import BertClient\n\n\ndef ner_test():\n    with BertClient(show_server_config=False, check_version=False, check_length=False, mode='NER') as bc:\n        start_t = time.perf_counter()\n        str1 = '1月24日，新华社对外发布了中央对雄安新区的指导意见，洋洋洒洒1.2万多字，17次提到北京，4次提到天津，信息量很大，其实也回答了人们关心的很多问题。'\n        # rst = bc.encode([list(str1)], is_tokenized=True)\n        # str1 = list(str1)\n        rst = bc.encode([str1], is_tokenized=True)\n        print('rst:', rst)\n        print(len(rst[0]))\n        print(time.perf_counter() - start_t)\n\n\ndef ner_cu_seg():\n    \"\"\"\n    自定义分字\n    :return:\n    \"\"\"\n    with BertClient(show_server_config=False, check_version=False, check_length=False, mode='NER') as bc:\n        start_t = time.perf_counter()\n        str1 = '1月24日，新华社对外发布了中央对雄安新区的指导意见，洋洋洒洒1.2万多字，17次提到北京，4次提到天津，信息量很大，其实也回答了人们关心的很多问题。'\n        rst = bc.encode([list(str1)], is_tokenized=True)\n        print('rst:', rst)\n        print(len(rst[0]))\n        print(time.perf_counter() - start_t)\n\n\ndef class_test():\n    with BertClient(show_server_config=False, check_version=False, check_length=False, mode='CLASS') as bc:\n        start_t = time.perf_counter()\n        str = '北京时间2月17日凌晨，第69届柏林国际电影节公布主竞赛单元获奖名单，王景春、咏梅凭借王小帅执导的中国影片《地久天长》连夺最佳男女演员双银熊大奖，这是中国演员首次包揽柏林电影节最佳男女演员奖，为华语影片刷新纪录。与此同时，由青年导演王丽娜执导的影片《第一次的别离》也荣获了本届柏林电影节新生代单元国际评审团最佳影片，可以说，在经历数个获奖小年之后，中国电影在柏林影展再次迎来了高光时刻。'\n        str2 = '受粤港澳大湾区规划纲要提振，港股周二高开，恒指开盘上涨近百点，涨幅0.33%，报28440.49点，相关概念股亦集体上涨，电子元件、新能源车、保险、基建概念多数上涨。粤泰股份、珠江实业、深天地A等10余股涨停；中兴通讯、丘钛科技、舜宇光学分别高开1.4%、4.3%、1.6%。比亚迪电子、比亚迪股份、光宇国际分别高开1.7%、1.2%、1%。越秀交通基建涨近2%，粤海投资、碧桂园等多股涨超1%。其他方面，日本软银集团股价上涨超0.4%，推动日经225和东证指数齐齐高开，但随后均回吐涨幅转跌东证指数跌0.2%，日经225指数跌0.11%，报21258.4点。受芯片制造商SK海力士股价下跌1.34％拖累，韩国综指下跌0.34％至2203.9点。澳大利亚ASX 200指数早盘上涨0.39％至6089.8点，大多数行业板块均现涨势。在保健品品牌澳佳宝下调下半财年的销售预期后，其股价暴跌超过23％。澳佳宝CEO亨弗里（Richard Henfrey）认为，公司下半年的利润可能会低于上半年，主要是受到销售额疲弱的影响。同时，亚市早盘澳洲联储公布了2月会议纪要，政策委员将继续谨慎评估经济增长前景，因前景充满不确定性的影响，稳定当前的利率水平比贸然调整利率更为合适，而且当前利率水平将有利于趋向通胀目标及改善就业，当前劳动力市场数据表现强势于其他经济数据。另一方面，经济增长前景亦令消费者消费意愿下滑，如果房价出现下滑，消费可能会进一步疲弱。在澳洲联储公布会议纪要后，澳元兑美元下跌近30点，报0.7120 。美元指数在昨日触及96.65附近的低点之后反弹至96.904。日元兑美元报110.56，接近上一交易日的低点。'\n        str3 = '新京报快讯 据国家市场监管总局消息，针对媒体报道水饺等猪肉制品检出非洲猪瘟病毒核酸阳性问题，市场监管总局、农业农村部已要求企业立即追溯猪肉原料来源并对猪肉制品进行了处置。两部门已派出联合督查组调查核实相关情况，要求猪肉制品生产企业进一步加强对猪肉原料的管控，落实检验检疫票证查验规定，完善非洲猪瘟检测和复核制度，防止染疫猪肉原料进入食品加工环节。市场监管总局、农业农村部等部门要求各地全面落实防控责任，强化防控措施，规范信息报告和发布，对不按要求履行防控责任的企业，一旦发现将严厉查处。专家认为，非洲猪瘟不是人畜共患病，虽然对猪有致命危险，但对人没有任何危害，属于只传猪不传人型病毒，不会影响食品安全。开展猪肉制品病毒核酸检测，可为防控溯源工作提供线索。'\n        rst = bc.encode([str, str2, str3])\n        print('rst:', rst)\n        print('time used:{}'.format(time.perf_counter() - start_t))\n\n\nif __name__ == '__main__':\n    # class_test()\n    ner_test()\n    ner_cu_seg()"
        },
        {
          "name": "data_process.py",
          "type": "blob",
          "size": 3.1142578125,
          "content": "# encoding=utf-8\n\n\"\"\"\n用于语料库的处理\n1. 全部处理成小于max_seq_length的序列，这样可以避免解码出现不合法的数据或者在最后算结果的时候出现out of range 的错误。\n\n@Author: Macan\n\"\"\"\n\n\nimport os\nimport codecs\nimport argparse\n\ndef load_file(file_path):\n    if not os.path.exists(file_path):\n        return None\n    with codecs.open(file_path, 'r', encoding='utf-8') as fd:\n        for line in fd:\n            yield line\n\n\ndef _cut(sentence):\n    new_sentence = []\n    sen = []\n    for i in sentence:\n        if i.split(' ')[0] in ['。', '！', '？'] and len(sen) != 0:\n            sen.append(i)\n            new_sentence.append(sen)\n            sen = []\n            continue\n        sen.append(i)\n    if len(new_sentence) == 1: #娄底那种一句话超过max_seq_length的且没有句号的，用,分割，再长的不考虑了。。。\n        new_sentence = []\n        sen = []\n        for i in sentence:\n            if i.split(' ')[0] in ['，'] and len(sen) != 0:\n                sen.append(i)\n                new_sentence.append(sen)\n                sen = []\n                continue\n            sen.append(i)\n    return new_sentence\n\n\ndef cut_sentence(file, max_seq_length):\n    \"\"\"\n    句子截断\n    :param file: \n    :param max_seq_length: \n    :return: \n    \"\"\"\n    context = []\n    sentence = []\n    cnt = 0\n    for line in load_file(file):\n        line = line.strip()\n        if line == '' and len(sentence) != 0:\n            # 判断这一句是否超过最大长度\n            if len(sentence) > max_seq_length:\n                sentence = _cut(sentence)\n                context.extend(sentence)\n            else:\n                context.append(sentence)\n            sentence = []\n            continue\n        cnt += 1\n        sentence.append(line)\n    print('token cnt:{}'.format(cnt))\n    return context\n\ndef write_to_file(file, context):\n    # 首先将源文件改名为新文件名，避免覆盖\n    os.rename(file, '{}.bak'.format(file))\n    with codecs.open(file, 'w', encoding='utf-8') as fd:\n        for sen in context:\n            for token in sen:\n                fd.write(token + '\\n')\n            fd.write('\\n')\n\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser(description='data pre process')\n    parser.add_argument('--train_data', type=str, default='./NERdata/train.txt')\n    parser.add_argument('--dev_data', type=str, default='./NERdata/dev.txt')\n    parser.add_argument('--test_data', type=str, default='./NERdata/test.txt')\n    parser.add_argument('--max_seq_length', type=int, default=126)\n    args = parser.parse_args()\n\n    print('cut train data to max sequence length:{}'.format(args.max_seq_length))\n    context = cut_sentence(args.train_data, args.max_seq_length)\n    write_to_file(args.train_data, context)\n\n    print('cut dev data to max sequence length:{}'.format(args.max_seq_length))\n    context = cut_sentence(args.dev_data, args.max_seq_length)\n    write_to_file(args.dev_data, context)\n\n    print('cut test data to max sequence length:{}'.format(args.max_seq_length))\n    context = cut_sentence(args.test_data, args.max_seq_length)\n    write_to_file(args.test_data, context)"
        },
        {
          "name": "pictures",
          "type": "tree",
          "content": null
        },
        {
          "name": "requirement.txt",
          "type": "blob",
          "size": 0.337890625,
          "content": "# client-side requirements, pretty light-weight right?\n# tensorflow >= 1.12.0\n# tensorflow-gpu >= 1.12.0  # GPU version of TensorFlow.\nGPUtil >= 1.3.0  # no need if you dont have GPU\npyzmq >= 17.1.0  # python zmq\nflask # no need if you do not need http\nflask_compress # no need if you do not need http\nflask_json # no need if you do not need http"
        },
        {
          "name": "run.py",
          "type": "blob",
          "size": 1.1767578125,
          "content": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n\"\"\"\n运行 BERT NER Server\n#@Time    : 2019/1/26 21:00\n# @Author  : MaCan (ma_cancan@163.com)\n# @File    : run.py\n\"\"\"\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\n\ndef start_server():\n    from bert_base.server import BertServer\n    from bert_base.server.helper import get_run_args\n\n    args = get_run_args()\n    print(args)\n    server = BertServer(args)\n    server.start()\n    server.join()\n\n\ndef train_ner():\n    import os\n    from bert_base.train.train_helper import get_args_parser\n    from bert_base.train.bert_lstm_ner import train\n\n    args = get_args_parser()\n    if True:\n        import sys\n        param_str = '\\n'.join(['%20s = %s' % (k, v) for k, v in sorted(vars(args).items())])\n        print('usage: %s\\n%20s   %s\\n%s\\n%s\\n' % (' '.join(sys.argv), 'ARG', 'VALUE', '_' * 50, param_str))\n    print(args)\n    os.environ['CUDA_VISIBLE_DEVICES'] = args.device_map\n    train(args=args)\n\n\nif __name__ == '__main__':\n    \"\"\"\n    如果想训练，那么直接 指定参数跑，如果想启动服务，那么注释掉train,打开server即可\n    \"\"\"\n    train_ner()\n    #start_server()"
        },
        {
          "name": "setup.py",
          "type": "blob",
          "size": 1.833984375,
          "content": "# encoding =utf-8\n\nfrom os import path\nimport codecs\nfrom setuptools import setup, find_packages\n\n# setup metainfo\n# libinfo_py = 'bert_lstm_ner.py'\n# libinfo_content = open(libinfo_py, 'r', encoding='utf-8').readlines()\n# version_line = [l.strip() for l in libinfo_content if l.startswith('__version__')][0]\n# # exec(version_line)  # produce __version__\n# __version__ = version_line.split('=')[1].replace(' ', '')\n# print(__version__)\nsetup(\n    name='bert_base',\n    version='0.0.9',\n    description='Use Google\\'s BERT for Chinese natural language processing tasks such as named entity recognition and provide server services',\n    url='https://github.com/macanv/BERT-BiLSTM-CRF-NER',\n    long_description=open('README.md', 'r', encoding='utf-8').read(),\n    long_description_content_type='text/markdown',\n    author='Ma Can',\n    author_email='ma_cancan@163.com',\n    license='MIT',\n    packages=find_packages(),\n    zip_safe=False,\n    install_requires=[\n        'numpy',\n        'six',\n        'pyzmq>=16.0.0',\n        'GPUtil>=1.3.0',\n        'termcolor>=1.1',\n    ],\n    extras_require={\n        'cpu': ['tensorflow>=1.10.0'],\n        'gpu': ['tensorflow-gpu>=1.10.0'],\n        'http': ['flask', 'flask-compress', 'flask-cors', 'flask-json']\n    },\n    classifiers=(\n        'Programming Language :: Python :: 3.6',\n        'License :: OSI Approved :: MIT License',\n        'Operating System :: OS Independent',\n        #'Topic :: Scientific/Engineering :: Artificial Intelligence :: Natural Language Processing :: Named Entity Recognition',\n    ),\n    entry_points={\n        'console_scripts': ['bert-base-serving-start=bert_base.runs:start_server',\n                            'bert-base-ner-train=bert_base.runs:train_ner'],\n    },\n    keywords='bert nlp ner NER named entity recognition bilstm crf tensorflow machine learning sentence encoding embedding serving',\n)\n"
        },
        {
          "name": "terminal_predict.py",
          "type": "blob",
          "size": 11.23828125,
          "content": "# encoding=utf-8\n\n\"\"\"\n基于命令行的在线预测方法\n@Author: Macan (ma_cancan@163.com) \n\"\"\"\n\nimport tensorflow as tf\nimport numpy as np\nimport codecs\nimport pickle\nimport os\nfrom datetime import datetime\n\nfrom bert_base.train.models import create_model, InputFeatures\nfrom bert_base.bert import tokenization, modeling\nfrom bert_base.train.train_helper import get_args_parser\nargs = get_args_parser()\n\nmodel_dir = r'C:\\Users\\C\\Documents\\Tencent Files\\389631699\\FileRecv\\semi_corpus_people_2014'\nbert_dir = 'F:\\chinese_L-12_H-768_A-12'\n\nis_training=False\nuse_one_hot_embeddings=False\nbatch_size=1\n\ngpu_config = tf.ConfigProto()\ngpu_config.gpu_options.allow_growth = True\nsess=tf.Session(config=gpu_config)\nmodel=None\n\nglobal graph\ninput_ids_p, input_mask_p, label_ids_p, segment_ids_p = None, None, None, None\n\n\nprint('checkpoint path:{}'.format(os.path.join(model_dir, \"checkpoint\")))\nif not os.path.exists(os.path.join(model_dir, \"checkpoint\")):\n    raise Exception(\"failed to get checkpoint. going to return \")\n\n# 加载label->id的词典\nwith codecs.open(os.path.join(model_dir, 'label2id.pkl'), 'rb') as rf:\n    label2id = pickle.load(rf)\n    id2label = {value: key for key, value in label2id.items()}\n\nwith codecs.open(os.path.join(model_dir, 'label_list.pkl'), 'rb') as rf:\n    label_list = pickle.load(rf)\nnum_labels = len(label_list) + 1\n\ngraph = tf.get_default_graph()\nwith graph.as_default():\n    print(\"going to restore checkpoint\")\n    #sess.run(tf.global_variables_initializer())\n    input_ids_p = tf.placeholder(tf.int32, [batch_size, args.max_seq_length], name=\"input_ids\")\n    input_mask_p = tf.placeholder(tf.int32, [batch_size, args.max_seq_length], name=\"input_mask\")\n\n    bert_config = modeling.BertConfig.from_json_file(os.path.join(bert_dir, 'bert_config.json'))\n    (total_loss, logits, trans, pred_ids) = create_model(\n        bert_config=bert_config, is_training=False, input_ids=input_ids_p, input_mask=input_mask_p, segment_ids=None,\n        labels=None, num_labels=num_labels, use_one_hot_embeddings=False, dropout_rate=1.0)\n\n    saver = tf.train.Saver()\n    saver.restore(sess, tf.train.latest_checkpoint(model_dir))\n\n\ntokenizer = tokenization.FullTokenizer(\n        vocab_file=os.path.join(bert_dir, 'vocab.txt'), do_lower_case=args.do_lower_case)\n\n\ndef predict_online():\n    \"\"\"\n    do online prediction. each time make prediction for one instance.\n    you can change to a batch if you want.\n\n    :param line: a list. element is: [dummy_label,text_a,text_b]\n    :return:\n    \"\"\"\n    def convert(line):\n        feature = convert_single_example(0, line, label_list, args.max_seq_length, tokenizer, 'p')\n        input_ids = np.reshape([feature.input_ids],(batch_size, args.max_seq_length))\n        input_mask = np.reshape([feature.input_mask],(batch_size, args.max_seq_length))\n        segment_ids = np.reshape([feature.segment_ids],(batch_size, args.max_seq_length))\n        label_ids =np.reshape([feature.label_ids],(batch_size, args.max_seq_length))\n        return input_ids, input_mask, segment_ids, label_ids\n\n    global graph\n    with graph.as_default():\n        print(id2label)\n        while True:\n            print('input the test sentence:')\n            sentence = str(input())\n            start = datetime.now()\n            if len(sentence) < 2:\n                print(sentence)\n                continue\n            sentence = tokenizer.tokenize(sentence)\n            # print('your input is:{}'.format(sentence))\n            input_ids, input_mask, segment_ids, label_ids = convert(sentence)\n\n            feed_dict = {input_ids_p: input_ids,\n                         input_mask_p: input_mask}\n            # run session get current feed_dict result\n            pred_ids_result = sess.run([pred_ids], feed_dict)\n            pred_label_result = convert_id_to_label(pred_ids_result, id2label)\n            print(pred_label_result)\n            #todo: 组合策略\n            result = strage_combined_link_org_loc(sentence, pred_label_result[0])\n            print('time used: {} sec'.format((datetime.now() - start).total_seconds()))\n\ndef convert_id_to_label(pred_ids_result, idx2label):\n    \"\"\"\n    将id形式的结果转化为真实序列结果\n    :param pred_ids_result:\n    :param idx2label:\n    :return:\n    \"\"\"\n    result = []\n    for row in range(batch_size):\n        curr_seq = []\n        for ids in pred_ids_result[row][0]:\n            if ids == 0:\n                break\n            curr_label = idx2label[ids]\n            if curr_label in ['[CLS]', '[SEP]']:\n                continue\n            curr_seq.append(curr_label)\n        result.append(curr_seq)\n    return result\n\n\n\ndef strage_combined_link_org_loc(tokens, tags):\n    \"\"\"\n    组合策略\n    :param pred_label_result:\n    :param types:\n    :return:\n    \"\"\"\n    def print_output(data, type):\n        line = []\n        line.append(type)\n        for i in data:\n            line.append(i.word)\n        print(', '.join(line))\n\n    params = None\n    eval = Result(params)\n    if len(tokens) > len(tags):\n        tokens = tokens[:len(tags)]\n    person, loc, org = eval.get_result(tokens, tags)\n    print_output(loc, 'LOC')\n    print_output(person, 'PER')\n    print_output(org, 'ORG')\n\n\ndef convert_single_example(ex_index, example, label_list, max_seq_length, tokenizer, mode):\n    \"\"\"\n    将一个样本进行分析，然后将字转化为id, 标签转化为id,然后结构化到InputFeatures对象中\n    :param ex_index: index\n    :param example: 一个样本\n    :param label_list: 标签列表\n    :param max_seq_length:\n    :param tokenizer:\n    :param mode:\n    :return:\n    \"\"\"\n    label_map = {}\n    # 1表示从1开始对label进行index化\n    for (i, label) in enumerate(label_list, 1):\n        label_map[label] = i\n    # 保存label->index 的map\n    if not os.path.exists(os.path.join(model_dir, 'label2id.pkl')):\n        with codecs.open(os.path.join(model_dir, 'label2id.pkl'), 'wb') as w:\n            pickle.dump(label_map, w)\n\n    tokens = example\n    # tokens = tokenizer.tokenize(example.text)\n    # 序列截断\n    if len(tokens) >= max_seq_length - 1:\n        tokens = tokens[0:(max_seq_length - 2)]  # -2 的原因是因为序列需要加一个句首和句尾标志\n    ntokens = []\n    segment_ids = []\n    label_ids = []\n    ntokens.append(\"[CLS]\")  # 句子开始设置CLS 标志\n    segment_ids.append(0)\n    # append(\"O\") or append(\"[CLS]\") not sure!\n    label_ids.append(label_map[\"[CLS]\"])  # O OR CLS 没有任何影响，不过我觉得O 会减少标签个数,不过拒收和句尾使用不同的标志来标注，使用LCS 也没毛病\n    for i, token in enumerate(tokens):\n        ntokens.append(token)\n        segment_ids.append(0)\n        label_ids.append(0)\n    ntokens.append(\"[SEP]\")  # 句尾添加[SEP] 标志\n    segment_ids.append(0)\n    # append(\"O\") or append(\"[SEP]\") not sure!\n    label_ids.append(label_map[\"[SEP]\"])\n    input_ids = tokenizer.convert_tokens_to_ids(ntokens)  # 将序列中的字(ntokens)转化为ID形式\n    input_mask = [1] * len(input_ids)\n\n    # padding, 使用\n    while len(input_ids) < max_seq_length:\n        input_ids.append(0)\n        input_mask.append(0)\n        segment_ids.append(0)\n        # we don't concerned about it!\n        label_ids.append(0)\n        ntokens.append(\"**NULL**\")\n        # label_mask.append(0)\n    # print(len(input_ids))\n    assert len(input_ids) == max_seq_length\n    assert len(input_mask) == max_seq_length\n    assert len(segment_ids) == max_seq_length\n    assert len(label_ids) == max_seq_length\n    # assert len(label_mask) == max_seq_length\n\n    # 结构化为一个类\n    feature = InputFeatures(\n        input_ids=input_ids,\n        input_mask=input_mask,\n        segment_ids=segment_ids,\n        label_ids=label_ids,\n        # label_mask = label_mask\n    )\n    return feature\n\n\nclass Pair(object):\n    def __init__(self, word, start, end, type, merge=False):\n        self.__word = word\n        self.__start = start\n        self.__end = end\n        self.__merge = merge\n        self.__types = type\n\n    @property\n    def start(self):\n        return self.__start\n    @property\n    def end(self):\n        return self.__end\n    @property\n    def merge(self):\n        return self.__merge\n    @property\n    def word(self):\n        return self.__word\n\n    @property\n    def types(self):\n        return self.__types\n    @word.setter\n    def word(self, word):\n        self.__word = word\n    @start.setter\n    def start(self, start):\n        self.__start = start\n    @end.setter\n    def end(self, end):\n        self.__end = end\n    @merge.setter\n    def merge(self, merge):\n        self.__merge = merge\n\n    @types.setter\n    def types(self, type):\n        self.__types = type\n\n    def __str__(self) -> str:\n        line = []\n        line.append('entity:{}'.format(self.__word))\n        line.append('start:{}'.format(self.__start))\n        line.append('end:{}'.format(self.__end))\n        line.append('merge:{}'.format(self.__merge))\n        line.append('types:{}'.format(self.__types))\n        return '\\t'.join(line)\n\n\nclass Result(object):\n    def __init__(self, config):\n        self.config = config\n        self.person = []\n        self.loc = []\n        self.org = []\n        self.others = []\n    def get_result(self, tokens, tags, config=None):\n        # 先获取标注结果\n        self.result_to_json(tokens, tags)\n        return self.person, self.loc, self.org\n\n    def result_to_json(self, string, tags):\n        \"\"\"\n        将模型标注序列和输入序列结合 转化为结果\n        :param string: 输入序列\n        :param tags: 标注结果\n        :return:\n        \"\"\"\n        item = {\"entities\": []}\n        entity_name = \"\"\n        entity_start = 0\n        idx = 0\n        last_tag = ''\n\n        for char, tag in zip(string, tags):\n            if tag[0] == \"S\":\n                self.append(char, idx, idx+1, tag[2:])\n                item[\"entities\"].append({\"word\": char, \"start\": idx, \"end\": idx+1, \"type\":tag[2:]})\n            elif tag[0] == \"B\":\n                if entity_name != '':\n                    self.append(entity_name, entity_start, idx, last_tag[2:])\n                    item[\"entities\"].append({\"word\": entity_name, \"start\": entity_start, \"end\": idx, \"type\": last_tag[2:]})\n                    entity_name = \"\"\n                entity_name += char\n                entity_start = idx\n            elif tag[0] == \"I\":\n                entity_name += char\n            elif tag[0] == \"O\":\n                if entity_name != '':\n                    self.append(entity_name, entity_start, idx, last_tag[2:])\n                    item[\"entities\"].append({\"word\": entity_name, \"start\": entity_start, \"end\": idx, \"type\": last_tag[2:]})\n                    entity_name = \"\"\n            else:\n                entity_name = \"\"\n                entity_start = idx\n            idx += 1\n            last_tag = tag\n        if entity_name != '':\n            self.append(entity_name, entity_start, idx, last_tag[2:])\n            item[\"entities\"].append({\"word\": entity_name, \"start\": entity_start, \"end\": idx, \"type\": last_tag[2:]})\n        return item\n\n    def append(self, word, start, end, tag):\n        if tag == 'LOC':\n            self.loc.append(Pair(word, start, end, 'LOC'))\n        elif tag == 'PER':\n            self.person.append(Pair(word, start, end, 'PER'))\n        elif tag == 'ORG':\n            self.org.append(Pair(word, start, end, 'ORG'))\n        else:\n            self.others.append(Pair(word, start, end, tag))\n\n\nif __name__ == \"__main__\":\n    predict_online()\n\n"
        },
        {
          "name": "thu_classification.py",
          "type": "blob",
          "size": 25.18359375,
          "content": "# encoding=utf-8\n\n\"\"\"\n基于清华大学语料库的中文文本分类\nAuthor:MaCan\n\"\"\"\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport collections\nimport os\nimport codecs\nimport pickle\nimport numpy as np\nimport tensorflow as tf\n\nimport sys\n# sys.path.append('..')\n# from bert_base.server.helper import get_logger\nfrom bert_base.bert import modeling\nfrom bert_base.bert import optimization\nfrom bert_base.bert import tokenization\n\nos.environ['CUDA_VISIBLE_DEVICES'] = '2'\n\nif os.name == 'nt':\n    bert_path = 'C:\\迅雷下载\\chinese_L-12_H-768_A-12'\n    root_path = r'C:\\workspace\\python\\BERT_Base'\nelse:\n    bert_path = '/home/macan/ml/data/chinese_L-12_H-768_A-12'\n    root_path = '/home/macan/ml/workspace/BERT_Base2'\n\nflags = tf.flags\nFLAGS = flags.FLAGS\n\n## Required parameters\nflags.DEFINE_string(\"data_dir\",  os.path.join(os.path.join(root_path, 'data'), 'classification'),\n                    \"The input data dir. Should contain the .tsv files (or other data files) for the task.\")\n\nflags.DEFINE_string(\n    \"bert_config_file\", os.path.join(bert_path, 'bert_config.json'),\n    \"The config json file corresponding to the pre-trained BERT model. \"\n    \"This specifies the model architecture.\")\n\nflags.DEFINE_string(\"task_name\", None, \"The name of the task to train.\")\n\nflags.DEFINE_string(\"vocab_file\", os.path.join(bert_path, 'vocab.txt'),\n                    \"The vocabulary file that the BERT model was trained on.\")\n\nflags.DEFINE_string(\n    \"output_dir\", os.path.join(os.path.join(root_path, 'output'), 'classification'),\n    \"The output directory where the model checkpoints will be written.\")\n\n## Other parameters\n\nflags.DEFINE_string(\n    \"init_checkpoint\", os.path.join(bert_path, 'bert_model.ckpt'),\n    \"Initial checkpoint (usually from a pre-trained BERT model).\")\n\nflags.DEFINE_bool(\n    \"do_lower_case\", True,\n    \"Whether to lower case the input text. Should be True for uncased \"\n    \"models and False for cased models.\")\n\nflags.DEFINE_integer(\n    \"max_seq_length\", 202,\n    \"The maximum total input sequence length after WordPiece tokenization. \"\n    \"Sequences longer than this will be truncated, and sequences shorter \"\n    \"than this will be padded.\")\n\nflags.DEFINE_bool('clean', True, 'remove the files which created by last training')\n\nflags.DEFINE_bool(\"do_train\", True, \"Whether to run training.\")\n\nflags.DEFINE_bool(\"do_eval\", True, \"Whether to run eval on the dev set.\")\n\nflags.DEFINE_bool(\n    \"do_predict\", True,\n    \"Whether to run the model in inference mode on the test set.\")\n\nflags.DEFINE_integer(\"train_batch_size\", 32, \"Total batch size for training.\")\n\nflags.DEFINE_integer(\"eval_batch_size\", 8, \"Total batch size for eval.\")\n\nflags.DEFINE_integer(\"predict_batch_size\", 8, \"Total batch size for predict.\")\n\nflags.DEFINE_float(\"learning_rate\", 5e-5, \"The initial learning rate for Adam.\")\n\nflags.DEFINE_float('dropout_keep_prob', 0.5, 'dropout probability')\nflags.DEFINE_float(\"num_train_epochs\", 5.0,\n                   \"Total number of training epochs to perform.\")\n\nflags.DEFINE_float(\n    \"warmup_proportion\", 0.1,\n    \"Proportion of training to perform linear learning rate warmup for. \"\n    \"E.g., 0.1 = 10% of training.\")\n\nflags.DEFINE_integer(\"save_checkpoints_steps\",500,\n                     \"How often to save the model checkpoint.\")\n\nflags.DEFINE_integer(\"iterations_per_loop\", 1000,\n                     \"How many steps to make in each estimator call.\")\n\nflags.DEFINE_integer('save_summary_steps', 500, 'summary steps')\n\n# logger = get_logger(os.path.join(FLAGS.output_dir, 'c.log'))\nimport logging\nlogging.basicConfig(level = logging.INFO,format = '%(asctime)s - %(name)s - %(levelname)s - %(message)s')\nlogger = logging.getLogger(__name__)\n\nclass RestoreHook(tf.train.SessionRunHook):\n    def __init__(self, init_fn):\n        self.init_fn = init_fn\n\n    def after_create_session(self, session, coord=None):\n        if session.run(tf.train.get_or_create_global_step()) == 0:\n            self.init_fn(session)\n\nclass InputExample(object):\n    \"\"\"A single training/test example for simple sequence classification.\"\"\"\n\n    def __init__(self, guid, text_a, text_b=None, label=None):\n        \"\"\"Constructs a InputExample.\n        Args:\n          guid: Unique id for the example.\n          text_a: string. The untokenized text of the first sequence. For single\n            sequence tasks, only this sequence must be specified.\n          text_b: (Optional) string. The untokenized text of the second sequence.\n            Only must be specified for sequence pair tasks.\n          label: (Optional) string. The label of the example. This should be\n            specified for train and dev examples, but not for test examples.\n        \"\"\"\n        self.guid = guid\n        self.text_a = text_a\n        self.text_b = text_b\n        self.label = label\n\n\nclass InputFeatures(object):\n    \"\"\"A single set of features of data.\"\"\"\n\n    def __init__(self, input_ids, input_mask, segment_ids, label_id):\n        self.input_ids = input_ids\n        self.input_mask = input_mask\n        self.segment_ids = segment_ids\n        self.label_id = label_id\n\n\nclass DataProcessor(object):\n    \"\"\"Base class for data converters for sequence classification data sets.\"\"\"\n\n    def get_train_examples(self, data_dir):\n        \"\"\"Gets a collection of `InputExample`s for the train set.\"\"\"\n        raise NotImplementedError()\n\n    def get_dev_examples(self, data_dir):\n        \"\"\"Gets a collection of `InputExample`s for the dev set.\"\"\"\n        raise NotImplementedError()\n\n    def get_test_examples(self, data_dir):\n        \"\"\"Gets a collection of `InputExample`s for prediction.\"\"\"\n        raise NotImplementedError()\n\n    def get_labels(self):\n        \"\"\"Gets the list of labels for this data set.\"\"\"\n        raise NotImplementedError()\n\n    @classmethod\n    def _read_tsv(cls, input_file, quotechar=None):\n        \"\"\"Reads a tab separated value file.\"\"\"\n        with codecs.open(input_file, \"r\", encoding='utf-8') as f:\n            lines = []\n            for line in f:\n                line = line.strip()\n                if line == '':\n                    continue\n                line = line.split('__\\t')\n                if len(line) == 2:\n                    line[0] = line[0].replace('__', '')\n                    lines.append(line)\n        return lines\n\n\nclass ThuProcessor(DataProcessor):\n    \"\"\"Processor for the Thu data set.\"\"\"\n\n    def __init__(self):\n        self.labels = set()\n\n    def get_train_examples(self, data_dir):\n        \"\"\"See base class.\"\"\"\n        return self._create_examples(\n            self._read_tsv(os.path.join(data_dir, \"train.txt\")), 'train')\n\n    def get_dev_examples(self, data_dir):\n       return self._create_examples(\n            self._read_tsv(os.path.join(data_dir, \"dev.txt\")), 'dev')\n\n    def get_test_examples(self, data_dir):\n        return self._create_examples(\n            self._read_tsv(os.path.join(data_dir, \"test.txt\")), 'test')\n\n    def get_labels(self):\n        \"\"\"在读取数据的时候，自动获取类别个数\"\"\"\n        if not os.path.exists(os.path.join(FLAGS.output_dir, 'label_list.pkl')):\n            with codecs.open(os.path.join(FLAGS.output_dir, 'label_list.pkl'), 'wb') as fd:\n                pickle.dump(self.labels, fd)\n        else:\n            with codecs.open(os.path.join(FLAGS.output_dir, 'label_list.pkl'), 'rb') as fd:\n                labels = pickle.load(fd)\n            if len(labels) > len(self.labels):\n                self.labels = labels\n        return list(self.labels)\n\n    def _create_examples(self, lines, set_type):\n        examples = []\n        np.random.shuffle(lines)\n        for i, line in enumerate(lines):\n            guid = '%s-%s' %(set_type, i)\n            # if set_type == 'test':\n            #     text_a = tokenization.convert_to_unicode(line[1])\n            #     label = '0'\n            # else:\n            #     text_a = tokenization.convert_to_unicode(line[1])\n            #     label = tokenization.convert_to_unicode(line[0])\n            #     self.labels.add(label)\n            text_a = tokenization.convert_to_unicode(line[1])\n            label = tokenization.convert_to_unicode(line[0])\n            self.labels.add(label)\n            examples.append(\n                InputExample(guid=guid, text_a=text_a, label=label, text_b=None)\n            )\n        return examples\n\ndef conver_single_example(ex_index, example, label_list, max_seq_length, tokenizer, mode):\n    \"\"\"\n    将一个训练样本转化为InputFeature，其中进行字符seg并且index化,和label的index转化\n    :param ex_index:\n    :param example:\n    :param label_list:\n    :param max_seq_length:\n    :param tokenizer:\n    :return:\n    \"\"\"\n    # 1. 构建label->id的映射\n    label_map = {}\n    if os.path.exists(os.path.join(FLAGS.output_dir, 'label2id.pkl')):\n        with codecs.open(os.path.join(FLAGS.output_dir, 'label2id.pkl'), 'rb') as fd:\n            label_map = pickle.load(fd)\n    else:\n        for i, label in enumerate(label_list):\n            label_map[label] = i\n        with codecs.open(os.path.join(FLAGS.output_dir, 'label2id.pkl'), 'wb') as fd:\n            pickle.dump(label_map, fd)\n    # 不考虑seq pair 分类的情况\n    tokens_a = tokenizer.tokenize(example.text_a)\n\n    # 截断，因为有句首和句尾的标识符\n    if len(tokens_a) > max_seq_length - 2:\n        tokens_a = tokens_a[0:(max_seq_length-2)]\n\n    tokens = []\n    segment_ids = []\n    tokens.append('[CLS]')\n    segment_ids.append(0)\n    for token in tokens_a:\n        tokens.append(token)\n        segment_ids.append(0)\n    tokens.append('[SEP]')\n    segment_ids.append(0)\n    #将字符转化为id形式\n    input_ids = tokenizer.convert_tokens_to_ids(tokens)\n\n    input_mask = [1]*len(input_ids)\n    #补全到max_seg_length\n    while len(input_ids) < max_seq_length:\n        input_ids.append(0)\n        segment_ids.append(0)\n        input_mask.append(0)\n    if example.label is None:\n        label_id = -1\n    else:\n        label_id = label_map[example.label]\n    if ex_index < 2 and mode in ['train', 'dev']:\n        logger.info(\"*** Example ***\")\n        logger.info(\"guid: %s\" % (example.guid))\n        logger.info(\"tokens: %s\" % \" \".join(\n            [tokenization.printable_text(x) for x in tokens]))\n        logger.info(\"input_ids: %s\" % \" \".join([str(x) for x in input_ids]))\n        logger.info(\"input_mask: %s\" % \" \".join([str(x) for x in input_mask]))\n        logger.info(\"segment_ids: %s\" % \" \".join([str(x) for x in segment_ids]))\n        logger.info(\"label: %s (id = %d)\" % (example.label, label_id))\n\n    feature = InputFeatures(input_ids=input_ids, input_mask=input_mask, segment_ids=segment_ids, label_id=label_id)\n    return feature\n\ndef file_based_convert_examples_to_features(examples, label_list, max_seq_length, tokenizer, output_file, mode):\n    \"\"\"\n    将训练文件转化特征后，存储为tf_record格式，用于模型的读取\n    :param examples:\n    :param label_list:\n    :param max_seq_length:\n    :param tokenizer:\n    :param output_file:\n    :return:\n    \"\"\"\n    writer = tf.python_io.TFRecordWriter(path=output_file)\n    # 将每一个样本转化为idx特征，封装到map中后进行序列化存储为record\n    for ex_index, example in enumerate(examples):\n        if ex_index % 10000 == 0:\n            logger.info('Writing example %d of %d' %(ex_index, len(examples)))\n        feature = conver_single_example(ex_index, example, label_list, max_seq_length, tokenizer, mode)\n        # 将输入数据转化为64位int 的list，这是必须的\n        def create_int_feature(values):\n            f = tf.train.Feature(int64_list=tf.train.Int64List(value=list(values)))\n            return f\n        features = collections.OrderedDict()\n        features['input_ids'] = create_int_feature(feature.input_ids)\n        features['input_mask'] = create_int_feature(feature.input_mask)\n        features['segment_ids'] = create_int_feature(feature.segment_ids)\n        features['label_ids'] = create_int_feature([feature.label_id])\n        # 转化为Example 协议内存块\n        tf_example = tf.train.Example(features=tf.train.Features(feature=features))\n        writer.write(tf_example.SerializeToString())\n\ndef file_based_input_fn_builder(input_file, seq_length, num_label, is_training, drop_remainder):\n    \"\"\"\n\n    :param input_file:\n    :param seq_length:\n    :param is_training:\n    :param drop_remainder: 是否丢弃较小的batch\n    :return:\n    \"\"\"\n    name_to_features = {\n      \"input_ids\": tf.FixedLenFeature([seq_length], tf.int64),\n      \"input_mask\": tf.FixedLenFeature([seq_length], tf.int64),\n      \"segment_ids\": tf.FixedLenFeature([seq_length], tf.int64),\n      \"label_ids\": tf.FixedLenFeature([], tf.int64),\n    }\n\n    def _decode_record(record, name_to_feature):\n        # 解析一个record中的数据\n        example = tf.parse_single_example(record, name_to_feature)\n        for name in list(example.keys()):\n            t = example[name]\n            if t.dtype == tf.int64:\n                t = tf.to_int32(t)\n            example[name] = t\n        return example\n\n    def input_fn(params):\n        \"\"\"\n        模型输入函数\n        :param params:\n        :return:\n        \"\"\"\n        batch_size = params['batch_size']\n        d = tf.data.TFRecordDataset(input_file)\n        if is_training:\n            d = d.repeat()\n            d = d.shuffle(buffer_size=200)\n\n        # tf.data.experimental.map_and_batch will be deprecated, the replace methods like bellow\n        d = d.apply(tf.contrib.data.map_and_batch(\n            lambda record: _decode_record(record, name_to_features),\n            batch_size=batch_size,\n            drop_remainder=drop_remainder\n        ))\n        # d = d.apply(lambda record: _decode_record(record, name_to_features))\n        # d = d.batch(batch_size=batch_size, drop_remainder=drop_remainder)\n        return d\n    return input_fn\n\n\ndef create_model(bert_config, is_training, input_ids, input_mask, segment_ids, labels, num_labels):\n    \"\"\"\n\n    :param bert_config:\n    :param is_training:\n    :param input_ids:\n    :param input_mask:\n    :param segment_ids:\n    :param labels:\n    :param num_labels:\n    :param use_one_hot_embedding:\n    :return:\n    \"\"\"\n    # 通过传入的训练数据，进行representation\n    model = modeling.BertModel(\n        config=bert_config,\n        is_training=is_training,\n        input_ids=input_ids,\n        input_mask=input_mask,\n        token_type_ids=segment_ids,\n    )\n\n    embedding_layer = model.get_sequence_output()\n    output_layer = model.get_pooled_output()\n    hidden_size = output_layer.shape[-1].value\n\n    # model = CNN_Classification(embedding_chars=embedding_layer,\n    #                                labels=labels,\n    #                                num_tags=num_labels,\n    #                                sequence_length=FLAGS.max_seq_length,\n    #                                embedding_dims=embedding_layer.shape[-1].value,\n    #                                vocab_size=0,\n    #                                filter_sizes=[3, 4, 5],\n    #                                num_filters=3,\n    #                                dropout_keep_prob=FLAGS.dropout_keep_prob,\n    #                                l2_reg_lambda=0.001)\n    # loss, predictions, probabilities = model.add_cnn_layer()\n\n    output_weights = tf.get_variable(\n        \"output_weights\", [num_labels, hidden_size],\n        initializer=tf.truncated_normal_initializer(stddev=0.02))\n\n    output_bias = tf.get_variable(\n        \"output_bias\", [num_labels], initializer=tf.zeros_initializer())\n\n    with tf.variable_scope(\"loss\"):\n        if is_training:\n            # I.e., 0.1 dropout\n            output_layer = tf.nn.dropout(output_layer, keep_prob=0.9)\n\n        logits = tf.matmul(output_layer, output_weights, transpose_b=True)\n        logits = tf.nn.bias_add(logits, output_bias)\n        probabilities = tf.nn.softmax(logits, axis=-1)\n        log_probs = tf.nn.log_softmax(logits, axis=-1)\n\n        one_hot_labels = tf.one_hot(labels, depth=num_labels, dtype=tf.float32)\n\n        per_example_loss = -tf.reduce_sum(one_hot_labels * log_probs, axis=-1)\n        loss = tf.reduce_mean(per_example_loss)\n    return (loss, per_example_loss, logits, probabilities)\n\ndef model_fn_builder(bert_config, num_labels, init_checkpoint, learning_rate, num_train_steps,\n                     num_warmup_steps):\n    \"\"\"\n\n    :param bert_config:\n    :param num_labels:\n    :param init_checkpoint:\n    :param learning_rate:\n    :param num_train_steps:\n    :param num_warmup_steps:\n    :param use_one_hot_embeddings:\n    :return:\n    \"\"\"\n    def model_fn(features, labels, mode, params):\n        logger.info(\"*** Features ***\")\n        for name in sorted(features.keys()):\n            logger.info(\"  name = %s, shape = %s\" % (name, features[name].shape))\n\n        input_ids = features[\"input_ids\"]\n        input_mask = features[\"input_mask\"]\n        segment_ids = features[\"segment_ids\"]\n        label_ids = features[\"label_ids\"]\n\n        is_training = (mode == tf.estimator.ModeKeys.TRAIN)\n\n        (total_loss, per_example_loss, logits, probabilities) = create_model(\n            bert_config, is_training, input_ids, input_mask, segment_ids, label_ids,\n            num_labels)\n\n        # resort variable from checkpoint file to init current graph\n        tvars = tf.trainable_variables()\n        initialized_variable_names = {}\n        init_fn = None\n        if init_checkpoint:\n            (assignment_map, initialized_variable_names) = \\\n                modeling.get_assignment_map_from_checkpoint(tvars, init_checkpoint)\n            tf.train.init_from_checkpoint(init_checkpoint, assignment_map)\n            #variables_to_restore = tf.contrib.framework.get_model_variables()\n            #init_fn = tf.contrib.framework.\\\n               # assign_from_checkpoint_fn(init_checkpoint,\n                #                          variables_to_restore,\n                 #                         ignore_missing_vars=True)\n\n        # 打印变量名称\n        logger.info(\"**** Trainable Variables ****\")\n        for var in tvars:\n            init_string = \"\"\n            if var.name in initialized_variable_names:\n                init_string = \", *INIT_FROM_CKPT*\"\n            logger.info(\"  name = %s, shape = %s%s\", var.name, var.shape,\n                            init_string)\n\n\n\n        output_spec = None\n        if mode == tf.estimator.ModeKeys.TRAIN:\n            train_op = optimization.create_optimizer(\n                total_loss, learning_rate, num_train_steps, num_warmup_steps, use_tpu=False)\n            output_spec = tf.estimator.EstimatorSpec(\n                mode=mode,\n                loss=total_loss,\n                train_op=train_op\n            )\n            #    training_hooks=[RestoreHook(init_fn)])\n        elif mode == tf.estimator.ModeKeys.EVAL:\n            def metric_fn(per_example_loss, label_ids, logits):\n                predictions = tf.argmax(logits, axis=-1, output_type=tf.int32)\n                accuracy = tf.metrics.accuracy(label_ids, predictions)\n                loss = tf.metrics.mean(per_example_loss)\n                return {\n                    \"eval_accuracy\": accuracy,\n                    \"eval_loss\": loss,\n                }\n            eval_metrics = metric_fn(per_example_loss, label_ids, logits)\n            output_spec = tf.estimator.EstimatorSpec(\n                mode=mode,\n                loss=total_loss,\n                eval_metric_ops=eval_metrics\n                #evaluation_hooks=[RestoreHook(init_fn)]\n                )\n        else:\n            output_spec = tf.estimator.EstimatorSpec(\n                mode=mode, predictions=probabilities)\n        return output_spec\n    return model_fn\n\ndef main(_):\n    if not FLAGS.do_train and not FLAGS.do_eval and not FLAGS.do_predict:\n        raise ValueError(\n            \"At least one of `do_train`, `do_eval` or `do_predict' must be True.\")\n\n    bert_config = modeling.BertConfig.from_json_file(FLAGS.bert_config_file)\n\n    if FLAGS.max_seq_length > bert_config.max_position_embeddings:\n        raise ValueError(\n            \"Cannot use sequence length %d because the BERT model \"\n            \"was only trained up to sequence length %d\" %\n            (FLAGS.max_seq_length, bert_config.max_position_embeddings))\n\n    if not os.path.exists(FLAGS.output_dir):\n        os.makedirs(FLAGS.output_dir)\n\n    processor = ThuProcessor()\n    #定义分词器\n    tokenizer = tokenization.FullTokenizer(\n        vocab_file=FLAGS.vocab_file, do_lower_case=FLAGS.do_lower_case)\n    # estimator 运行参数\n    run_config = tf.estimator.RunConfig(\n        model_dir=FLAGS.output_dir,\n        save_summary_steps=FLAGS.save_summary_steps,\n        save_checkpoints_steps=FLAGS.save_checkpoints_steps,\n        keep_checkpoint_max=5,\n        log_step_count_steps=500,\n        session_config=tf.ConfigProto(log_device_placement=True)\n        #session_config=tf.ConfigProto(log_device_placement=True,\n        #                               device_count={'GPU': 1}))\n    )\n\n    train_examples = None\n    num_train_steps = None\n    num_warmup_steps = None\n    if FLAGS.do_train:\n        train_examples = processor.get_train_examples(FLAGS.data_dir)\n        num_train_steps = int(\n            len(train_examples) / FLAGS.train_batch_size * FLAGS.num_train_epochs)\n        num_warmup_steps = int(num_train_steps * FLAGS.warmup_proportion)\n\n    # get_labels() must be called after get_train_examoles or other examples\n    label_list = processor.get_labels()\n    logger.info('************ label_list=', ' '.join(label_list))\n    model_fn = model_fn_builder(\n        bert_config=bert_config,\n        num_labels=len(label_list),\n        init_checkpoint=FLAGS.init_checkpoint,\n        learning_rate=FLAGS.learning_rate,\n        num_train_steps=num_train_steps,\n        num_warmup_steps=num_warmup_steps)\n\n    # params是一个dict 里面的key是model_fn 里面用到的参数名称，value是对应的数据\n    params = {\n        'batch_size': FLAGS.train_batch_size,\n    }\n\n    estimator = tf.estimator.Estimator(\n        model_fn=model_fn,\n        config=run_config,\n        params=params,\n    )\n\n    if FLAGS.do_train:\n        train_file = os.path.join(FLAGS.output_dir, \"train.tf_record\")\n        file_based_convert_examples_to_features(\n            train_examples, label_list, FLAGS.max_seq_length, tokenizer, train_file, 'train')\n        logger.info(\"***** Running training *****\")\n        logger.info(\"  Num examples = %d\", len(train_examples))\n        logger.info(\"  Batch size = %d\", FLAGS.train_batch_size)\n        logger.info(\"  Num steps = %d\", num_train_steps)\n        train_input_fn = file_based_input_fn_builder(\n            input_file=train_file,\n            seq_length=FLAGS.max_seq_length,\n            num_label=len(label_list),\n            is_training=True,\n            drop_remainder=True)\n        estimator.train(input_fn=train_input_fn, max_steps=num_train_steps)\n\n    if FLAGS.do_eval:\n        eval_examples = processor.get_dev_examples(FLAGS.data_dir)\n        eval_file = os.path.join(FLAGS.output_dir, \"eval.tf_record\")\n        file_based_convert_examples_to_features(\n            eval_examples, label_list, FLAGS.max_seq_length, tokenizer, eval_file, 'eval')\n        logger.info(\"***** Running evaluation *****\")\n        logger.info(\"  Num examples = %d\", len(eval_examples))\n        logger.info(\"  Batch size = %d\", FLAGS.eval_batch_size)\n\n        eval_input_fn = file_based_input_fn_builder(\n            input_file=eval_file,\n            seq_length=FLAGS.max_seq_length,\n            num_label=len(label_list),\n            is_training=False,\n            drop_remainder=False)\n\n        result = estimator.evaluate(input_fn=eval_input_fn)\n\n        output_eval_file = os.path.join(FLAGS.output_dir, \"eval_results.txt\")\n        with tf.gfile.GFile(output_eval_file, \"w\") as writer:\n            logger.info(\"***** Eval results *****\")\n            for key in sorted(result.keys()):\n                logger.info(\"  %s = %s\", key, str(result[key]))\n                writer.write(\"%s = %s\\n\" % (key, str(result[key])))\n\n    if FLAGS.do_predict:\n        predict_examples = processor.get_test_examples(FLAGS.data_dir)\n        predict_file = os.path.join(FLAGS.output_dir, \"predict.tf_record\")\n        file_based_convert_examples_to_features(predict_examples, label_list,\n                                                FLAGS.max_seq_length, tokenizer,\n                                                predict_file, 'test')\n\n        logger.info(\"***** Running prediction*****\")\n        logger.info(\"  Num examples = %d\", len(predict_examples))\n        logger.info(\"  Batch size = %d\", FLAGS.predict_batch_size)\n\n        predict_input_fn = file_based_input_fn_builder(\n            input_file=predict_file,\n            seq_length=FLAGS.max_seq_length,\n            num_label=len(label_list),\n            is_training=False,\n            drop_remainder=False)\n\n        result = estimator.predict(input_fn=predict_input_fn)\n\n        output_predict_file = os.path.join(FLAGS.output_dir, \"test_results.txt\")\n        with tf.gfile.GFile(output_predict_file, \"w\") as writer:\n            logger.info(\"***** Predict results *****\")\n            for prediction in result:\n                output_line = \"\\t\".join(\n                    str(class_probability) for class_probability in prediction) + \"\\n\"\n                writer.write(output_line)\n\n\ndef load_data():\n    processer = ThuProcessor()\n    example = processer.get_train_examples(FLAGS.data_dir)\n    print()\n\n\nif __name__ == \"__main__\":\n    # flags.mark_flag_as_required(\"data_dir\")\n    # flags.mark_flag_as_required(\"vocab_file\")\n    # flags.mark_flag_as_required(\"bert_config_file\")\n    # flags.mark_flag_as_required(\"output_dir\")\n    tf.app.run()\n    # load_data()\n\n\n\n\n\n"
        }
      ]
    }
  ]
}