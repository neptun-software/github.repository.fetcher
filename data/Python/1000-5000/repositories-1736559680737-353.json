{
  "metadata": {
    "timestamp": 1736559680737,
    "page": 353,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjM2MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "QwenLM/Qwen2-VL",
      "stars": 4137,
      "defaultBranch": "main",
      "files": [
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.0908203125,
          "content": "# python generated files\n__pycache__/\n*.py[oc]\nbuild/\ndist/\nwheels/\n*.egg-info\n\n# venv\n.venv\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 11.0908203125,
          "content": "                                 Apache License\n                           Version 2.0, January 2004\n                        http://www.apache.org/licenses/\n\n   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n\n   1. Definitions.\n\n      \"License\" shall mean the terms and conditions for use, reproduction,\n      and distribution as defined by Sections 1 through 9 of this document.\n\n      \"Licensor\" shall mean the copyright owner or entity authorized by\n      the copyright owner that is granting the License.\n\n      \"Legal Entity\" shall mean the union of the acting entity and all\n      other entities that control, are controlled by, or are under common\n      control with that entity. For the purposes of this definition,\n      \"control\" means (i) the power, direct or indirect, to cause the\n      direction or management of such entity, whether by contract or\n      otherwise, or (ii) ownership of fifty percent (50%) or more of the\n      outstanding shares, or (iii) beneficial ownership of such entity.\n\n      \"You\" (or \"Your\") shall mean an individual or Legal Entity\n      exercising permissions granted by this License.\n\n      \"Source\" form shall mean the preferred form for making modifications,\n      including but not limited to software source code, documentation\n      source, and configuration files.\n\n      \"Object\" form shall mean any form resulting from mechanical\n      transformation or translation of a Source form, including but\n      not limited to compiled object code, generated documentation,\n      and conversions to other media types.\n\n      \"Work\" shall mean the work of authorship, whether in Source or\n      Object form, made available under the License, as indicated by a\n      copyright notice that is included in or attached to the work\n      (an example is provided in the Appendix below).\n\n      \"Derivative Works\" shall mean any work, whether in Source or Object\n      form, that is based on (or derived from) the Work and for which the\n      editorial revisions, annotations, elaborations, or other modifications\n      represent, as a whole, an original work of authorship. For the purposes\n      of this License, Derivative Works shall not include works that remain\n      separable from, or merely link (or bind by name) to the interfaces of,\n      the Work and Derivative Works thereof.\n\n      \"Contribution\" shall mean any work of authorship, including\n      the original version of the Work and any modifications or additions\n      to that Work or Derivative Works thereof, that is intentionally\n      submitted to Licensor for inclusion in the Work by the copyright owner\n      or by an individual or Legal Entity authorized to submit on behalf of\n      the copyright owner. For the purposes of this definition, \"submitted\"\n      means any form of electronic, verbal, or written communication sent\n      to the Licensor or its representatives, including but not limited to\n      communication on electronic mailing lists, source code control systems,\n      and issue tracking systems that are managed by, or on behalf of, the\n      Licensor for the purpose of discussing and improving the Work, but\n      excluding communication that is conspicuously marked or otherwise\n      designated in writing by the copyright owner as \"Not a Contribution.\"\n\n      \"Contributor\" shall mean Licensor and any individual or Legal Entity\n      on behalf of whom a Contribution has been received by Licensor and\n      subsequently incorporated within the Work.\n\n   2. Grant of Copyright License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      copyright license to reproduce, prepare Derivative Works of,\n      publicly display, publicly perform, sublicense, and distribute the\n      Work and such Derivative Works in Source or Object form.\n\n   3. Grant of Patent License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      (except as stated in this section) patent license to make, have made,\n      use, offer to sell, sell, import, and otherwise transfer the Work,\n      where such license applies only to those patent claims licensable\n      by such Contributor that are necessarily infringed by their\n      Contribution(s) alone or by combination of their Contribution(s)\n      with the Work to which such Contribution(s) was submitted. If You\n      institute patent litigation against any entity (including a\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\n      or a Contribution incorporated within the Work constitutes direct\n      or contributory patent infringement, then any patent licenses\n      granted to You under this License for that Work shall terminate\n      as of the date such litigation is filed.\n\n   4. Redistribution. You may reproduce and distribute copies of the\n      Work or Derivative Works thereof in any medium, with or without\n      modifications, and in Source or Object form, provided that You\n      meet the following conditions:\n\n      (a) You must give any other recipients of the Work or\n          Derivative Works a copy of this License; and\n\n      (b) You must cause any modified files to carry prominent notices\n          stating that You changed the files; and\n\n      (c) You must retain, in the Source form of any Derivative Works\n          that You distribute, all copyright, patent, trademark, and\n          attribution notices from the Source form of the Work,\n          excluding those notices that do not pertain to any part of\n          the Derivative Works; and\n\n      (d) If the Work includes a \"NOTICE\" text file as part of its\n          distribution, then any Derivative Works that You distribute must\n          include a readable copy of the attribution notices contained\n          within such NOTICE file, excluding those notices that do not\n          pertain to any part of the Derivative Works, in at least one\n          of the following places: within a NOTICE text file distributed\n          as part of the Derivative Works; within the Source form or\n          documentation, if provided along with the Derivative Works; or,\n          within a display generated by the Derivative Works, if and\n          wherever such third-party notices normally appear. The contents\n          of the NOTICE file are for informational purposes only and\n          do not modify the License. You may add Your own attribution\n          notices within Derivative Works that You distribute, alongside\n          or as an addendum to the NOTICE text from the Work, provided\n          that such additional attribution notices cannot be construed\n          as modifying the License.\n\n      You may add Your own copyright statement to Your modifications and\n      may provide additional or different license terms and conditions\n      for use, reproduction, or distribution of Your modifications, or\n      for any such Derivative Works as a whole, provided Your use,\n      reproduction, and distribution of the Work otherwise complies with\n      the conditions stated in this License.\n\n   5. Submission of Contributions. Unless You explicitly state otherwise,\n      any Contribution intentionally submitted for inclusion in the Work\n      by You to the Licensor shall be under the terms and conditions of\n      this License, without any additional terms or conditions.\n      Notwithstanding the above, nothing herein shall supersede or modify\n      the terms of any separate license agreement you may have executed\n      with Licensor regarding such Contributions.\n\n   6. Trademarks. This License does not grant permission to use the trade\n      names, trademarks, service marks, or product names of the Licensor,\n      except as required for reasonable and customary use in describing the\n      origin of the Work and reproducing the content of the NOTICE file.\n\n   7. Disclaimer of Warranty. Unless required by applicable law or\n      agreed to in writing, Licensor provides the Work (and each\n      Contributor provides its Contributions) on an \"AS IS\" BASIS,\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n      implied, including, without limitation, any warranties or conditions\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n      PARTICULAR PURPOSE. You are solely responsible for determining the\n      appropriateness of using or redistributing the Work and assume any\n      risks associated with Your exercise of permissions under this License.\n\n   8. Limitation of Liability. In no event and under no legal theory,\n      whether in tort (including negligence), contract, or otherwise,\n      unless required by applicable law (such as deliberate and grossly\n      negligent acts) or agreed to in writing, shall any Contributor be\n      liable to You for damages, including any direct, indirect, special,\n      incidental, or consequential damages of any character arising as a\n      result of this License or out of the use or inability to use the\n      Work (including but not limited to damages for loss of goodwill,\n      work stoppage, computer failure or malfunction, or any and all\n      other commercial damages or losses), even if such Contributor\n      has been advised of the possibility of such damages.\n\n   9. Accepting Warranty or Additional Liability. While redistributing\n      the Work or Derivative Works thereof, You may choose to offer,\n      and charge a fee for, acceptance of support, warranty, indemnity,\n      or other liability obligations and/or rights consistent with this\n      License. However, in accepting such obligations, You may act only\n      on Your own behalf and on Your sole responsibility, not on behalf\n      of any other Contributor, and only if You agree to indemnify,\n      defend, and hold each Contributor harmless for any liability\n      incurred by, or claims asserted against, such Contributor by reason\n      of your accepting any such warranty or additional liability.\n\n   END OF TERMS AND CONDITIONS\n\n   APPENDIX: How to apply the Apache License to your work.\n\n      To apply the Apache License to your work, attach the following\n      boilerplate notice, with the fields enclosed by brackets \"[]\"\n      replaced with your own identifying information. (Don't include\n      the brackets!)  The text should be enclosed in the appropriate\n      comment syntax for the file format. We also recommend that a\n      file or class name and description of purpose be included on the\n      same \"printed page\" as the copyright notice for easier\n      identification within third-party archives.\n\n   Copyright [yyyy] [name of copyright owner]\n\n   Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 63.3916015625,
          "content": "# Qwen2-VL\n\n\n<p align=\"center\">\n    <img src=\"https://qianwen-res.oss-accelerate-overseas.aliyuncs.com/Qwen2-VL/qwen2VL_logo.png\" width=\"400\"/>\n<p>\n\n<p align=\"center\">\n        ü§ó <a href=\"https://huggingface.co/collections/Qwen/qwen2-vl-66cee7455501d7126940800d\">Hugging Face</a>&nbsp&nbsp | &nbsp&nbspü§ñ <a href=\"https://modelscope.cn/organization/qwen\">ModelScope</a>&nbsp&nbsp | &nbsp&nbsp üìë <a href=\"https://qwenlm.github.io/blog/qwen2-vl/\">Blog</a> &nbsp&nbsp| &nbsp&nbsp üìë <a href=\"https://arxiv.org/pdf/2409.12191\">Paper</a> &nbsp&nbsp  </a>\n<br>\nüñ•Ô∏è <a href=\"https://huggingface.co/spaces/Qwen/Qwen2-VL\">Demo</a>&nbsp&nbsp | &nbsp&nbspüí¨ <a href=\"https://github.com/QwenLM/Qwen/blob/main/assets/wechat.png\">WeChat (ÂæÆ‰ø°)</a>&nbsp&nbsp | &nbsp&nbspü´® <a href=\"https://discord.gg/CV4E9rpNSD\">Discord</a>&nbsp&nbsp | &nbsp&nbsp<a href=\"https://help.aliyun.com/zh/model-studio/developer-reference/qwen-vl-api\"> üìë API</a>&nbsp&nbsp | &nbsp&nbspüñ•Ô∏è <a href=\"https://gallery.pai-ml.com/#/preview/deepLearning/cv/qwen2-vl\">PAI-DSW</a>&nbsp&nbsp\n</p>\n\n\n\n\n## Introduction\n\nAfter a year's relentless efforts, today we are thrilled to release **Qwen2-VL**! Qwen2-VL is the latest version of the vision language models in the Qwen model families. \n\n#### Key Enhancements:\n\n* **SoTA understanding of images of various resolution & ratio**: Qwen2-VL achieves state-of-the-art performance on visual understanding benchmarks, including MathVista, DocVQA, RealWorldQA, MTVQA, etc.\n\n* **Understanding videos of 20min+**: with the online streaming capabilities, Qwen2-VL can understand videos over 20 minutes by high-quality video-based question answering, dialog, content creation, etc.\n\n* **Agent that can operate your mobiles, robots, etc.**: with the abilities of complex reasoning and decision making, Qwen2-VL can be integrated with devices like mobile phones, robots, etc., for automatic operation based on visual environment and text instructions.\n\n* **Multilingual Support**: to serve global users, besides English and Chinese, Qwen2-VL now supports the understanding of texts in different languages inside images, including most European languages, Japanese, Korean, Arabic, Vietnamese, etc.\n\n#### Model Architecture Updates:\n\n* **Naive Dynamic Resolution**: Unlike before, Qwen2-VL can handle arbitrary image resolutions, mapping them into a dynamic number of visual tokens, offering a more human-like visual processing experience.\n<p align=\"center\">\n    <img src=\"https://qianwen-res.oss-accelerate-overseas.aliyuncs.com/Qwen2-VL/qwen2_vl_framework.jpg\" width=\"80%\"/>\n<p>\n\n* **Multimodal Rotary Position Embedding (M-ROPE)**: Decomposes positional embedding into parts to capture 1D textual, 2D visual, and 3D video positional information, enhancing its multimodal processing capabilities.\n\n<p align=\"center\">\n    <img src=\"http://qianwen-res.oss-accelerate-overseas.aliyuncs.com/Qwen2-VL/mrope.png\" width=\"80%\"/>\n<p>\n\n\nWe have open-sourced Qwen2-VL models, including Qwen2-VL-2B and Qwen2-VL-7B under the Apache 2.0 license, as well as Qwen2-VL-72B under the Qwen license. These models are now integrated with Hugging Face Transformers, vLLM, and other third-party frameworks. We hope you enjoy using them!\n\n\n## News\n* 2024.09.19: The instruction-tuned [Qwen2-VL-72B model](https://huggingface.co/Qwen/Qwen2-VL-72B-Instruct) and its quantized version [[AWQ](https://huggingface.co/Qwen/Qwen2-VL-72B-Instruct-AWQ), [GPTQ-Int4](https://huggingface.co/Qwen/Qwen2-VL-72B-Instruct-GPTQ-Int4), [GPTQ-Int8](https://huggingface.co/Qwen/Qwen2-VL-72B-Instruct-GPTQ-Int8)] are now available. We have also released the [Qwen2-VL paper](https://arxiv.org/pdf/2409.12191) simultaneously.\n* 2024.08.30: We have released the [Qwen2-VL series](\"https://huggingface.co/collections/Qwen/qwen2-vl-66cee7455501d7126940800d). The 2B and 7B models are now available, and the 72B model for opensource is coming soon. For more details, please check our [blog](https://qwenlm.github.io/blog/qwen2-vl/)!\n\n\n## Performance\n### Image Benchmarks\n\n| Benchmark | Previous SoTA<br><sup>(Open-source LVLM)<sup> | Claude-3.5 Sonnet | GPT-4o | **Qwen2-VL-72B**<br><sup>([ü§ó](https://huggingface.co/Qwen/Qwen2-VL-72B-Instruct) [ü§ñ](https://modelscope.cn/models/qwen/Qwen2-VL-72B-Instruct) |**Qwen2-VL-7B**<br><sup>([ü§ó](https://huggingface.co/Qwen/Qwen2-VL-7B-Instruct) [ü§ñ](https://modelscope.cn/models/qwen/Qwen2-VL-7B-Instruct)) |**Qwen2-VL-2B**<br><sup>([ü§ó](https://huggingface.co/Qwen/Qwen2-VL-2B-Instruct)[ü§ñ](https://modelscope.cn/models/qwen/Qwen2-VL-2B-Instruct)) \n| :--- | :---: | :---: | :---: | :---: |:---: |:---: |\n| MMMU<sub>val</sub>  | 58.3 | 68.3 | **69.1** | 64.5 | 54.1|41.1\n| MMMU-Pro | 46.9 | 51.5 | **51.9** | 46.2 | 30.5 | 21.2 \n| DocVQA<sub>test</sub>  | 94.1 | 95.2 | 92.8 | **96.5** | 94.5| 90.1\n| InfoVQA<sub>test</sub>  | 82.0 | - | - | **84.5** | 76.5|65.5\n| ChartQA<sub>test</sub>  | 88.4 | **90.8** | 85.7 | 88.3 |83.0| 73.5\n| TextVQA<sub>val</sub>  | 84.4 | - | - | **85.5** |84.3|79.7\n| OCRBench | 852 | 788 | 736 | **877** |845| 794\n| MTVQA | 17.3 | 25.7 | 27.8 | **30.9** |25.6| 18.1\n| VCR<sub>en easy</sub>  | 84.67 | 63.85 | 91.55 | **91.93** | 89.70| 81.45\n| VCR<sub>zh easy</sub>  | 22.09 | 1.0| 14.87 | **65.37** | 59.94| 46.16\n| RealWorldQA | 72.2 | 60.1 | 75.4 | **77.8** | 70.1| 62.9\n| MME<sub>sum</sub>   | 2414.7 | 1920.0 | 2328.7 | **2482.7** | 2326.8 | 1872.0\n| MMBench-EN<sub>test</sub>  | **86.5** | 79.7 | 83.4 | **86.5** | 83.0 | 74.9\n| MMBench-CN<sub>test</sub>  | 86.3 | 80.7 | 82.1 | **86.6** | 80.5| 73.5\n| MMBench-V1.1<sub>test</sub>  | 85.5 | 78.5 | 82.2 | **85.9** |80.7| 72.2\n| MMT-Bench<sub>test</sub> | 63.4 | - | 65.5 | **71.7** |63.7| 54.5\n| MMStar | 67.1 | 62.2 | 63.9 | **68.3** |60.7| 48.0\n| MMVet<sub>GPT-4-Turbo</sub>  | 65.7 | 66.0 | 69.1 | **74.0** |62.0| 49.5\n| HallBench<sub>avg</sub>  | 55.2 | 49.9 | 55.0 | **58.1** | 50.6 | 41.7\n| MathVista<sub>testmini</sub>  | 67.5 | 67.7 | 63.8 | **70.5** |58.2| 43.0\n| MathVision  | 16.97 | - | **30.4** | 25.9 | 16.3| 12.4\n\n### Video Benchmarks\n\n| Benchmark |  Previous SoTA<br><sup>(Open-source LVLM)<sup> | Gemini 1.5-Pro | GPT-4o | **Qwen2-VL-72B**<br><sup>([ü§ó](https://huggingface.co/Qwen/Qwen2-VL-72B-Instruct) [ü§ñ](https://modelscope.cn/models/qwen/Qwen2-VL-72B-Instruct)) |**Qwen2-VL-7B**<br><sup>([ü§ó](https://huggingface.co/Qwen/Qwen2-VL-7B-Instruct) [ü§ñ](https://modelscope.cn/models/qwen/Qwen2-VL-7B-Instruct)) |**Qwen2-VL-2B**<br><sup>([ü§ó](https://huggingface.co/Qwen/Qwen2-VL-2B-Instruct)[ü§ñ](https://modelscope.cn/models/qwen/Qwen2-VL-2B-Instruct)) \n| :--- | :---: | :---: | :---: | :---: | :---: | :---: |\n| MVBench | 69.6 | - | - | **73.6** | 67.0| 63.2 \n| PerceptionTest<sub>test</sub> |  66.9 | - | - | **68.0** | 62.3 |53.9\n| EgoSchema<sub>test</sub>  | 62.0 | 63.2 | 72.2 | **77.9** | 66.7 |54.9\n| Video-MME<br><sub>(wo/w subs)</sub>  | 66.3/69.6  | **75.0**/**81.3** | 71.9/77.2 | 71.2/77.8 | 63.3/69.0 |55.6/60.4\n\n### Agent Benchmarks\n|     |Benchmark | Metric | Previous SoTA | GPT-4o | **Qwen2-VL-72B** |\n| :-- | :-- | :--: | :--: | :--: | :--: |\n|   General  | FnCall<sup>[1]</sup> | TM | - | 90.2 | **93.1** |\n|     |  | EM | - | 50.0 | **53.2** |\n|   Game  | Number Line | SR | 89.4<sup>[2]</sup> | 91.5 | **100.0** |\n|     | BlackJack | SR | 40.2<sup>[2]</sup> | 34.5 | **42.6** |\n|     | EZPoint | SR | 50.0<sup>[2]</sup> | 85.5 | **100.0** |\n|     | Point24 | SR | 2.6<sup>[2]</sup> | 3.0 | **4.5** |\n| Android | AITZ  | TM | 83.0<sup>[3]</sup> | 70.0 | **89.6** |\n|     |  | EM | 47.7<sup>[3]</sup> | 35.3 | **72.1** |\n| AI2THOR | ALFRED<sub>valid-unseen</sub> | SR | 67.7<sup>[4]</sup> | - | **67.8** |\n|     |  | GC | 75.3<sup>[4]</sup> | - | **75.8** | \n|  VLN   | R2R<sub>valid-unseen</sub>  | SR | **79.0** | 43.7<sup>[5]</sup> | 51.7 | \n|     | REVERIE<sub>valid-unseen</sub> | SR | **61.0** | 31.6<sup>[5]</sup> | 31.0 | \n\nSR, GC, TM and EM are short for success rate, goal-condition success, type match and exact match. ALFRED is supported by SAM<sup>[6]</sup>.\n1. Self-Curated Function Call Benchmark by Qwen Team\n2. Fine-Tuning Large Vision-Language Models as Decision-Making Agents via Reinforcement Learning\n3. Android in the Zoo: Chain-of-Action-Thought for GUI Agents\n4. ThinkBot: Embodied Instruction Following with Thought Chain Reasoning\n5. MapGPT: Map-Guided Prompting with Adaptive Path Planning for Vision-and-Language Navigation\n6. Segment Anything.\n\n### Multilingual Benchmarks\n\n<table style=\"width:75%; text-align:center;\">\n    <tr>\n        <th>Models</th>\n        <td>AR </td>\n        <td>DE </td>\n        <td>FR </td>\n        <td>IT </td>\n        <td>JA </td>\n        <td>KO </td>\n        <td>RU </td>\n        <td>TH </td>\n        <td>VI </td>\n        <td>AVG</td>\n    </tr>\n    <tr>\n        <th align=\"left\">Qwen2-VL-72B</th>\n        <td>20.7 </td>\n        <td>36.5 </td>\n        <td>44.1 </td>\n        <td>42.8 </td>\n        <td>21.6 </td>\n        <td>37.4 </td>\n        <td>15.6 </td>\n        <td>17.7 </td>\n        <td>41.6 </td>\n        <td><b>30.9</b></td>\n    </tr>\n    <tr>\n        <th align=\"left\">GPT-4o</th>\n        <td>20.2 </td>\n        <td>34.2 </td>\n        <td>41.2 </td>\n        <td>32.7 </td>\n        <td>20.0 </td>\n        <td>33.9 </td>\n        <td>11.5 </td>\n        <td>22.5 </td>\n        <td>34.2 </td>\n        <td>27.8</td>\n    </tr>\n        <tr>\n        <th align=\"left\">Claude3 Opus</th>\n        <td>15.1 </td>\n        <td>33.4 </td>\n        <td>40.6 </td>\n        <td>34.4 </td>\n        <td>19.4 </td>\n        <td>27.2 </td>\n        <td>13.0 </td>\n        <td>19.5 </td>\n        <td>29.1 </td>\n        <td>25.7 </td>\n    </tr>\n    <tr>\n        <th align=\"left\">Gemini Ultra</th>\n        <td>14.7 </td>\n        <td>32.3 </td>\n        <td>40.0 </td>\n        <td>31.8 </td>\n        <td>12.3 </td>\n        <td>17.2 </td>\n        <td>11.8 </td>\n        <td>20.3 </td>\n        <td>28.6 </td>\n        <td>23.2</td>\n    </tr>\n</table>\n\nThese results are evaluated on the benchmark of [MTVQA](https://github.com/bytedance/MTVQA/tree/main).\n\n## Quickstart\n\nBelow, we provide simple examples to show how to use Qwen2-VL with ü§ñ ModelScope and ü§ó Transformers.\n\nThe code of Qwen2-VL has been in the latest Hugging face transformers and we advise you to build from source with command:\n```\npip install git+https://github.com/huggingface/transformers@21fac7abba2a37fae86106f87fcf9974fd1e3830 accelerate\n```\nor you might encounter the following error:\n```\nKeyError: 'qwen2_vl'\n```\n\n- ‚ö†Ô∏è**NOTE**: Current latest version of `transformers` have [a bug](https://github.com/huggingface/transformers/issues/33401) when loading Qwen2-VL config, so you need to install a specific version of transformers as above.\n\nWe offer a toolkit to help you handle various types of visual input more conveniently, as if you were using an API. This includes base64, URLs, and interleaved images and videos. You can install it using the following command:\n\n```bash\n# It's highly recommanded to use `[decord]` feature for faster video loading.\npip install qwen-vl-utils[decord]\n```\n\nIf you are not using Linux, you might not be able to install `decord` from PyPI. In that case, you can use `pip install qwen-vl-utils` which will fall back to using torchvision for video processing. However, you can still [install decord from source](https://github.com/dmlc/decord?tab=readme-ov-file#install-from-source) to get decord used when loading video.\n\n### Using ü§ó  Transformers to Chat\n\nHere we show a code snippet to show you how to use the chat model with `transformers` and `qwen_vl_utils`:\n\n```python\nfrom transformers import Qwen2VLForConditionalGeneration, AutoTokenizer, AutoProcessor\nfrom qwen_vl_utils import process_vision_info\n\n# default: Load the model on the available device(s)\nmodel = Qwen2VLForConditionalGeneration.from_pretrained(\n    \"Qwen/Qwen2-VL-7B-Instruct\", torch_dtype=\"auto\", device_map=\"auto\"\n)\n\n# We recommend enabling flash_attention_2 for better acceleration and memory saving, especially in multi-image and video scenarios.\n# model = Qwen2VLForConditionalGeneration.from_pretrained(\n#     \"Qwen/Qwen2-VL-7B-Instruct\",\n#     torch_dtype=torch.bfloat16,\n#     attn_implementation=\"flash_attention_2\",\n#     device_map=\"auto\",\n# )\n\n# default processer\nprocessor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-VL-7B-Instruct\")\n\n# The default range for the number of visual tokens per image in the model is 4-16384.\n# You can set min_pixels and max_pixels according to your needs, such as a token range of 256-1280, to balance performance and cost.\n# min_pixels = 256*28*28\n# max_pixels = 1280*28*28\n# processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-VL-7B-Instruct\", min_pixels=min_pixels, max_pixels=max_pixels)\n\nmessages = [\n    {\n        \"role\": \"user\",\n        \"content\": [\n            {\n                \"type\": \"image\",\n                \"image\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg\",\n            },\n            {\"type\": \"text\", \"text\": \"Describe this image.\"},\n        ],\n    }\n]\n\n# Preparation for inference\ntext = processor.apply_chat_template(\n    messages, tokenize=False, add_generation_prompt=True\n)\nimage_inputs, video_inputs = process_vision_info(messages)\ninputs = processor(\n    text=[text],\n    images=image_inputs,\n    videos=video_inputs,\n    padding=True,\n    return_tensors=\"pt\",\n)\ninputs = inputs.to(\"cuda\")\n\n# Inference: Generation of the output\ngenerated_ids = model.generate(**inputs, max_new_tokens=128)\ngenerated_ids_trimmed = [\n    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n]\noutput_text = processor.batch_decode(\n    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n)\nprint(output_text)\n```\n<details>\n<summary>Multi image inference</summary>\n\n```python\n# Messages containing multiple images and a text query\nmessages = [\n    {\n        \"role\": \"user\",\n        \"content\": [\n            {\"type\": \"image\", \"image\": \"file:///path/to/image1.jpg\"},\n            {\"type\": \"image\", \"image\": \"file:///path/to/image2.jpg\"},\n            {\"type\": \"text\", \"text\": \"Identify the similarities between these images.\"},\n        ],\n    }\n]\n\n# Preparation for inference\ntext = processor.apply_chat_template(\n    messages, tokenize=False, add_generation_prompt=True\n)\nimage_inputs, video_inputs = process_vision_info(messages)\ninputs = processor(\n    text=[text],\n    images=image_inputs,\n    videos=video_inputs,\n    padding=True,\n    return_tensors=\"pt\",\n)\ninputs = inputs.to(\"cuda\")\n\n# Inference\ngenerated_ids = model.generate(**inputs, max_new_tokens=128)\ngenerated_ids_trimmed = [\n    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n]\noutput_text = processor.batch_decode(\n    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n)\nprint(output_text)\n```\n</details>\n\n<details>\n<summary>Video inference</summary>\n\n```python\n# Messages containing a images list as a video and a text query\nmessages = [\n    {\n        \"role\": \"user\",\n        \"content\": [\n            {\n                \"type\": \"video\",\n                \"video\": [\n                    \"file:///path/to/frame1.jpg\",\n                    \"file:///path/to/frame2.jpg\",\n                    \"file:///path/to/frame3.jpg\",\n                    \"file:///path/to/frame4.jpg\",\n                ],\n            },\n            {\"type\": \"text\", \"text\": \"Describe this video.\"},\n        ],\n    }\n]\n\n# Messages containing a local video path and a text query\nmessages = [\n    {\n        \"role\": \"user\",\n        \"content\": [\n            {\n                \"type\": \"video\",\n                \"video\": \"file:///path/to/video1.mp4\",\n                \"max_pixels\": 360 * 420,\n                \"fps\": 1.0,\n            },\n            {\"type\": \"text\", \"text\": \"Describe this video.\"},\n        ],\n    }\n]\n\n# Messages containing a video url and a text query\nmessages = [\n    {\n        \"role\": \"user\",\n        \"content\": [\n            {\n                \"type\": \"video\",\n                \"video\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-VL/space_woaudio.mp4\",\n            },\n            {\"type\": \"text\", \"text\": \"Describe this video.\"},\n        ],\n    }\n]\n\n# Preparation for inference\ntext = processor.apply_chat_template(\n    messages, tokenize=False, add_generation_prompt=True\n)\nimage_inputs, video_inputs = process_vision_info(messages)\ninputs = processor(\n    text=[text],\n    images=image_inputs,\n    videos=video_inputs,\n    padding=True,\n    return_tensors=\"pt\",\n)\ninputs = inputs.to(\"cuda\")\n\n# Inference\ngenerated_ids = model.generate(**inputs, max_new_tokens=128)\ngenerated_ids_trimmed = [\n    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n]\noutput_text = processor.batch_decode(\n    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n)\nprint(output_text)\n```\n\nVideo URL compatibility largely depends on the third-party library version. The details are in the table below. change the backend by `FORCE_QWENVL_VIDEO_READER=torchvision` or `FORCE_QWENVL_VIDEO_READER=decord` if you prefer not to use the default one.\n\n| Backend     | HTTP | HTTPS |\n|-------------|------|-------|\n| torchvision >= 0.19.0 | ‚úÖ  | ‚úÖ   |\n| torchvision < 0.19.0  | ‚ùå  | ‚ùå   |\n| decord      | ‚úÖ  | ‚ùå   |\n</details>\n\n<details>\n<summary>Batch inference</summary>\n\n```python\n# Sample messages for batch inference\nmessages1 = [\n    {\n        \"role\": \"user\",\n        \"content\": [\n            {\"type\": \"image\", \"image\": \"file:///path/to/image1.jpg\"},\n            {\"type\": \"image\", \"image\": \"file:///path/to/image2.jpg\"},\n            {\"type\": \"text\", \"text\": \"What are the common elements in these pictures?\"},\n        ],\n    }\n]\nmessages2 = [\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n    {\"role\": \"user\", \"content\": \"Who are you?\"},\n]\n# Combine messages for batch processing\nmessages = [messages1, messages2]\n\n# Preparation for batch inference\ntexts = [\n    processor.apply_chat_template(msg, tokenize=False, add_generation_prompt=True)\n    for msg in messages\n]\nimage_inputs, video_inputs = process_vision_info(messages)\ninputs = processor(\n    text=texts,\n    images=image_inputs,\n    videos=video_inputs,\n    padding=True,\n    return_tensors=\"pt\",\n)\ninputs = inputs.to(\"cuda\")\n\n# Batch Inference\ngenerated_ids = model.generate(**inputs, max_new_tokens=128)\ngenerated_ids_trimmed = [\n    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n]\noutput_texts = processor.batch_decode(\n    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n)\nprint(output_texts)\n```\n</details>\n\n### ü§ñ ModelScope\nWe strongly advise users especially those in mainland China to use ModelScope. `snapshot_download` can help you solve issues concerning downloading checkpoints.\n\n### More Usage Tips\n\nFor input images, we support local files, base64, and URLs. For videos, we currently only support local files.\n\n```python\n# You can directly insert a local file path, a URL, or a base64-encoded image into the position where you want in the text.\n## Local file path\nmessages = [\n    {\n        \"role\": \"user\",\n        \"content\": [\n            {\"type\": \"image\", \"image\": \"file:///path/to/your/image.jpg\"},\n            {\"type\": \"text\", \"text\": \"Describe this image.\"},\n        ],\n    }\n]\n## Image URL\nmessages = [\n    {\n        \"role\": \"user\",\n        \"content\": [\n            {\"type\": \"image\", \"image\": \"http://path/to/your/image.jpg\"},\n            {\"type\": \"text\", \"text\": \"Describe this image.\"},\n        ],\n    }\n]\n## Base64 encoded image\nmessages = [\n    {\n        \"role\": \"user\",\n        \"content\": [\n            {\"type\": \"image\", \"image\": \"data:image;base64,/9j/...\"},\n            {\"type\": \"text\", \"text\": \"Describe this image.\"},\n        ],\n    }\n]\n```\n#### Image Resolution for performance boost\n\nThe model supports a wide range of resolution inputs. By default, it uses the native resolution for input, but higher resolutions can enhance performance at the cost of more computation. Users can set the minimum and maximum number of pixels to achieve an optimal configuration for their needs, such as a token count range of 256-1280, to balance speed and memory usage.\n\n```python\nmin_pixels = 256 * 28 * 28\nmax_pixels = 1280 * 28 * 28\nprocessor = AutoProcessor.from_pretrained(\n    \"Qwen/Qwen2-VL-7B-Instruct\", min_pixels=min_pixels, max_pixels=max_pixels\n)\n```\n\nBesides, We provide two methods for fine-grained control over the image size input to the model:\n\n1. Specify exact dimensions: Directly set `resized_height` and `resized_width`. These values will be rounded to the nearest multiple of 28.\n\n2. Define min_pixels and max_pixels: Images will be resized to maintain their aspect ratio within the range of min_pixels and max_pixels.\n\n```python\n# resized_height and resized_width\nmessages = [\n    {\n        \"role\": \"user\",\n        \"content\": [\n            {\n                \"type\": \"image\",\n                \"image\": \"file:///path/to/your/image.jpg\",\n                \"resized_height\": 280,\n                \"resized_width\": 420,\n            },\n            {\"type\": \"text\", \"text\": \"Describe this image.\"},\n        ],\n    }\n]\n# min_pixels and max_pixels\nmessages = [\n    {\n        \"role\": \"user\",\n        \"content\": [\n            {\n                \"type\": \"image\",\n                \"image\": \"file:///path/to/your/image.jpg\",\n                \"min_pixels\": 50176,\n                \"max_pixels\": 50176,\n            },\n            {\"type\": \"text\", \"text\": \"Describe this image.\"},\n        ],\n    }\n]\n```\n\n#### Add ids for Multiple Image Inputs\nBy default, images and video content are directly included in the conversation. When handling multiple images, it's helpful to add labels to the images and videos for better reference. Users can control this behavior with the following settings:\n<details>\n<summary>Add vision ids</summary>\n\n```python\nconversation = [\n    {\n        \"role\": \"user\",\n        \"content\": [{\"type\": \"image\"}, {\"type\": \"text\", \"text\": \"Hello, how are you?\"}],\n    },\n    {\n        \"role\": \"assistant\",\n        \"content\": \"I'm doing well, thank you for asking. How can I assist you today?\",\n    },\n    {\n        \"role\": \"user\",\n        \"content\": [\n            {\"type\": \"text\", \"text\": \"Can you describe these images and video?\"},\n            {\"type\": \"image\"},\n            {\"type\": \"image\"},\n            {\"type\": \"video\"},\n            {\"type\": \"text\", \"text\": \"These are from my vacation.\"},\n        ],\n    },\n    {\n        \"role\": \"assistant\",\n        \"content\": \"I'd be happy to describe the images and video for you. Could you please provide more context about your vacation?\",\n    },\n    {\n        \"role\": \"user\",\n        \"content\": \"It was a trip to the mountains. Can you see the details in the images and video?\",\n    },\n]\n\n# default:\nprompt_without_id = processor.apply_chat_template(\n    conversation, add_generation_prompt=True\n)\n# Excepted output: '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n<|im_start|>user\\n<|vision_start|><|image_pad|><|vision_end|>Hello, how are you?<|im_end|>\\n<|im_start|>assistant\\nI'm doing well, thank you for asking. How can I assist you today?<|im_end|>\\n<|im_start|>user\\nCan you describe these images and video?<|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|><|vision_start|><|video_pad|><|vision_end|>These are from my vacation.<|im_end|>\\n<|im_start|>assistant\\nI'd be happy to describe the images and video for you. Could you please provide more context about your vacation?<|im_end|>\\n<|im_start|>user\\nIt was a trip to the mountains. Can you see the details in the images and video?<|im_end|>\\n<|im_start|>assistant\\n'\n\n\n# add ids\nprompt_with_id = processor.apply_chat_template(\n    conversation, add_generation_prompt=True, add_vision_id=True\n)\n# Excepted output: '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n<|im_start|>user\\nPicture 1: <|vision_start|><|image_pad|><|vision_end|>Hello, how are you?<|im_end|>\\n<|im_start|>assistant\\nI'm doing well, thank you for asking. How can I assist you today?<|im_end|>\\n<|im_start|>user\\nCan you describe these images and video?Picture 2: <|vision_start|><|image_pad|><|vision_end|>Picture 3: <|vision_start|><|image_pad|><|vision_end|>Video 1: <|vision_start|><|video_pad|><|vision_end|>These are from my vacation.<|im_end|>\\n<|im_start|>assistant\\nI'd be happy to describe the images and video for you. Could you please provide more context about your vacation?<|im_end|>\\n<|im_start|>user\\nIt was a trip to the mountains. Can you see the details in the images and video?<|im_end|>\\n<|im_start|>assistant\\n'\n```\n</details>\n\n#### Flash-Attention 2 to speed up generation\n\nFirst, make sure to install the latest version of Flash Attention 2:\n\n```bash\npip install -U flash-attn --no-build-isolation\n```\n\nAlso, you should have a hardware that is compatible with Flash-Attention 2. Read more about it in the official documentation of the [flash attention repository](https://github.com/Dao-AILab/flash-attention). FlashAttention-2 can only be used when a model is loaded in `torch.float16` or `torch.bfloat16`.\n\nTo load and run a model using Flash Attention-2, simply add `attn_implementation=\"flash_attention_2\"` when loading the model as follows:\n\n```python\nfrom transformers import Qwen2VLForConditionalGeneration\n\nmodel = Qwen2VLForConditionalGeneration.from_pretrained(\n    \"Qwen/Qwen2-VL-7B-Instruct\", \n    torch_dtype=torch.bfloat16, \n    attn_implementation=\"flash_attention_2\",\n)\n```\n\n\n### Try Qwen2-VL-72B with API!\n\nTo explore Qwen2-VL-72B, a more fascinating multimodal model, we encourage you to test our cutting-edge API service. Let's start the exciting journey right now!\n\n#### Installation\n```bash\npip install dashscope\n```\n\n#### Examples\n```python\nimport dashscope\n\n\ndashscope.api_key = \"your_api_key\"\n\nmessages = [{\n    'role': 'user',\n    'content': [\n        {\n            'image': \"https://dashscope.oss-cn-beijing.aliyuncs.com/images/dog_and_girl.jpeg\"\n        },\n        {\n            'text': 'What are in the image?'\n        },\n    ]\n}]\n# The model name 'qwen-vl-max-0809' is the identity of 'Qwen2-VL-72B'.\nresponse = dashscope.MultiModalConversation.call(model='qwen-vl-max-0809', messages=messages)\nprint(response)\n```\n\nFor more usage, please refer to the tutorial at [aliyun](https://help.aliyun.com/zh/model-studio/developer-reference/qwen-vl-api).\n\n## Quantization\n\nFor quantized models, we offer two types of quantization: AWQ and GPQ([ü§ó](https://huggingface.co/collections/Qwen/qwen2-vl-66cee7455501d7126940800d)[ü§ñ](https://modelscope.cn/organization/qwen)).\n\n### AWQ\nOne of our recommendations is the usage of [AWQ](https://arxiv.org/abs/2306.00978) with [AutoAWQ](https://github.com/casper-hansen/AutoAWQ). AWQ refers to Activation-aware Weight Quantization, a hardware-friendly approach for LLM low-bit weight-only quantization. AutoAWQ is an easy-to-use package for 4-bit quantized models.\n#### Usage of AWQ Quantized Models with Transformers\nNow, Transformers has officially supported AutoAWQ, which means that you can directly use the quantized model with Transformers. The following is a very simple code snippet showing how to run `Qwen2-VL-7B-Instruct-AWQ` with the quantized model:\n```python\nfrom transformers import Qwen2VLForConditionalGeneration, AutoTokenizer, AutoProcessor\nfrom qwen_vl_utils import process_vision_info\n\n# We recommend enabling flash_attention_2 for better acceleration and memory saving, especially in multi-image and video scenarios.\n# model = Qwen2VLForConditionalGeneration.from_pretrained(\n#     \"Qwen/Qwen2-VL-7B-Instruct-AWQ\",\n#     torch_dtype=\"auto\",\n#     attn_implementation=\"flash_attention_2\",\n#     device_map=\"auto\",\n# )\n\n# default: Load the model on the available device(s)\nmodel = Qwen2VLForConditionalGeneration.from_pretrained(\n    \"Qwen/Qwen2-VL-7B-Instruct-AWQ\", torch_dtype=\"auto\", device_map=\"auto\"\n)\n\n# The default range for the number of visual tokens per image in the model is 4-16384. You can set min_pixels and max_pixels according to your needs, such as a token count range of 256-1280, to balance speed and memory usage.\nmin_pixels = 256 * 28 * 28\nmax_pixels = 1280 * 28 * 28\nprocessor = AutoProcessor.from_pretrained(\n    \"Qwen/Qwen2-VL-7B-Instruct-AWQ\", min_pixels=min_pixels, max_pixels=max_pixels\n)\n\nmessages = [\n    {\n        \"role\": \"user\",\n        \"content\": [\n            {\n                \"type\": \"image\",\n                \"image\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg\",\n            },\n            {\"type\": \"text\", \"text\": \"Describe this image.\"},\n        ],\n    }\n]\n\n# Preparation for inference\ntext = processor.apply_chat_template(\n    messages, tokenize=False, add_generation_prompt=True\n)\nimage_inputs, video_inputs = process_vision_info(messages)\ninputs = processor(\n    text=[text],\n    images=image_inputs,\n    videos=video_inputs,\n    padding=True,\n    return_tensors=\"pt\",\n)\n\n# Inference: Generation of the output\ngenerated_ids = model.generate(**inputs, max_new_tokens=128)\ngenerated_ids_trimmed = [\n    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n]\noutput_text = processor.batch_decode(\n    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n)\nprint(output_text)\n```\n#### Quantize Your Own Model with AutoAWQ\nIf you want to quantize your own model to AWQ quantized models, we advise you to use AutoAWQ. It is suggested installing the forked version of the package by installing from source code:\n\n\n```bash\ngit clone https://github.com/kq-chen/AutoAWQ.git\ncd AutoAWQ\npip install numpy gekko pandas\npip install -e .\n```\n\nSuppose you have finetuned a model based on `Qwen2-VL-7B`. To build your own AWQ quantized model, you need to use the training data for calibration. Below, we provide a simple demonstration for you to run:\n\n```python\nfrom transformers import Qwen2VLProcessor\nfrom awq.models.qwen2vl import Qwen2VLAWQForConditionalGeneration\n\n# Specify paths and hyperparameters for quantization\nmodel_path = \"your_model_path\"\nquant_path = \"your_quantized_model_path\"\nquant_config = {\"zero_point\": True, \"q_group_size\": 128, \"w_bit\": 4, \"version\": \"GEMM\"}\n\n# Load your processor and model with AutoAWQ\nprocessor = Qwen2VLProcessor.from_pretrained(model_path)\n# We recommend enabling flash_attention_2 for better acceleration and memory saving\n# model = Qwen2VLAWQForConditionalGeneration.from_pretrained(\n#     model_path, model_type=\"qwen2_vl\", use_cache=False, attn_implementation=\"flash_attention_2\"\n# )\nmodel = Qwen2VLAWQForConditionalGeneration.from_pretrained(\n    model_path, model_type=\"qwen2_vl\", use_cache=False\n)\n```\nThen you need to prepare your data for calibration. What you need to do is just put samples into a list, each of which is a typical chat message as shown below. you can specify `text` and `image` in `content` field, For example:\n```python\ndataset = [\n    # message 0\n    [\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n        {\"role\": \"user\", \"content\": \"Tell me who you are.\"},\n        {\"role\": \"assistant\", \"content\": \"I am a large language model named Qwen...\"},\n    ],\n    # message 1\n    [\n        {\n            \"role\": \"user\",\n            \"content\": [\n                {\"type\": \"image\", \"image\": \"file:///path/to/your/image.jpg\"},\n                {\"type\": \"text\", \"text\": \"Output all text in the image\"},\n            ],\n        },\n        {\"role\": \"assistant\", \"content\": \"The text in the image is balabala...\"},\n    ],\n    # other messages...\n    ...,\n]\n```\nhere, we use a caption dataset **only for demonstration**. You should replace it with your own sft dataset.\n\n```python\ndef prepare_dataset(n_sample: int = 8) -> list[list[dict]]:\n    from datasets import load_dataset\n\n    dataset = load_dataset(\n        \"laion/220k-GPT4Vision-captions-from-LIVIS\", split=f\"train[:{n_sample}]\"\n    )\n    return [\n        [\n            {\n                \"role\": \"user\",\n                \"content\": [\n                    {\"type\": \"image\", \"image\": sample[\"url\"]},\n                    {\"type\": \"text\", \"text\": \"generate a caption for this image\"},\n                ],\n            },\n            {\"role\": \"assistant\", \"content\": sample[\"caption\"]},\n        ]\n        for sample in dataset\n    ]\n\n\ndataset = prepare_dataset()\n```\n\nThen process the dataset into tensors:\n```python\nfrom qwen_vl_utils import process_vision_info\n\ntext = processor.apply_chat_template(\n    dataset, tokenize=False, add_generation_prompt=True\n)\nimage_inputs, video_inputs = process_vision_info(dataset)\ninputs = processor(\n    text=text,\n    images=image_inputs,\n    videos=video_inputs,\n    padding=True,\n    return_tensors=\"pt\",\n)\n```\n\nThen just run the calibration process by one line of code:\n```python\nmodel.quantize(calib_data=inputs, quant_config=quant_config)\n```\nFinally, save the quantized model:\n```python\nmodel.model.config.use_cache = model.model.generation_config.use_cache = True\nmodel.save_quantized(quant_path, safetensors=True, shard_size=\"4GB\")\nprocessor.save_pretrained(quant_path)\n```\nThen you can obtain your own AWQ quantized model for deployment. Enjoy!\n### GPTQ\n#### Usage of GPTQ Models with Transformers\nNow, Transformers has officially supported AutoGPTQ, which means that you can directly use the quantized model with Transformers. The following is a very simple code snippet showing how to run `Qwen2-VL-7B-Instruct-GPTQ-Int4` with the quantized model:\n```python\nfrom transformers import Qwen2VLForConditionalGeneration, AutoTokenizer, AutoProcessor\nfrom qwen_vl_utils import process_vision_info\n\n# We recommend enabling flash_attention_2 for better acceleration and memory saving, especially in multi-image and video scenarios.\n# model = Qwen2VLForConditionalGeneration.from_pretrained(\n#     \"Qwen/Qwen2-VL-7B-Instruct-GPTQ-Int4\",\n#     torch_dtype=torch.bfloat16,\n#     attn_implementation=\"flash_attention_2\",\n#     device_map=\"auto\",\n# )\n\n# default: Load the model on the available device(s)\nmodel = Qwen2VLForConditionalGeneration.from_pretrained(\n    \"Qwen/Qwen2-VL-7B-Instruct-GPTQ-Int4\", torch_dtype=\"auto\", device_map=\"auto\"\n)\n\n# The default range for the number of visual tokens per image in the model is 4-16384. You can set min_pixels and max_pixels according to your needs, such as a token count range of 256-1280, to balance speed and memory usage.\nmin_pixels = 256 * 28 * 28\nmax_pixels = 1280 * 28 * 28\nprocessor = AutoProcessor.from_pretrained(\n    \"Qwen/Qwen2-VL-7B-Instruct-GPTQ-Int4\", min_pixels=min_pixels, max_pixels=max_pixels\n)\n\nmessages = [\n    {\n        \"role\": \"user\",\n        \"content\": [\n            {\n                \"type\": \"image\",\n                \"image\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg\",\n            },\n            {\"type\": \"text\", \"text\": \"Describe this image.\"},\n        ],\n    }\n]\n\n# Preparation for inference\ntext = processor.apply_chat_template(\n    messages, tokenize=False, add_generation_prompt=True\n)\nimage_inputs, video_inputs = process_vision_info(messages)\ninputs = processor(\n    text=[text],\n    images=image_inputs,\n    videos=video_inputs,\n    padding=True,\n    return_tensors=\"pt\",\n)\n\n# Inference: Generation of the output\ngenerated_ids = model.generate(**inputs, max_new_tokens=128)\ngenerated_ids_trimmed = [\n    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n]\noutput_text = processor.batch_decode(\n    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n)\nprint(output_text)\n```\n#### Quantize Your Own Model with AutoGPTQ\nIf you want to quantize your own model to GPTQ quantized models, we advise you to use AutoGPTQ. It is suggested installing the forked version of the package by installing from source code:\n\n```bash\ngit clone https://github.com/kq-chen/AutoGPTQ.git\ncd AutoGPTQ\npip install numpy gekko pandas\npip install -vvv --no-build-isolation -e .\n```\nSuppose you have finetuned a model based on `Qwen2-VL-7B`. To build your own GPTQ quantized model, you need to use the training data for calibration. Below, we provide a simple demonstration for you to run:\n```python\nfrom transformers import Qwen2VLProcessor\nfrom auto_gptq import BaseQuantizeConfig\nfrom auto_gptq.modeling import Qwen2VLGPTQForConditionalGeneration\n\n# Specify paths and hyperparameters for quantization\nmodel_path = \"your_model_path\"\nquant_path = \"your_quantized_model_path\"\nquantize_config = BaseQuantizeConfig(\n    bits=8,  # 4 or 8\n    group_size=128,\n    damp_percent=0.1,\n    desc_act=False,  # set to False can significantly speed up inference but the perplexity may slightly bad\n    static_groups=False,\n    sym=True,\n    true_sequential=True,\n)\n# Load your processor and model with AutoGPTQ\nprocessor = Qwen2VLProcessor.from_pretrained(model_path)\n# We recommend enabling flash_attention_2 for better acceleration and memory saving\n# model = Qwen2VLGPTQForConditionalGeneration.from_pretrained(model_path, quantize_config, attn_implementation=\"flash_attention_2\")\nmodel = Qwen2VLGPTQForConditionalGeneration.from_pretrained(model_path, quantize_config)\n```\nThen you need to prepare your data for calibration. What you need to do is just put samples into a list, each of which is a typical chat message as shown below. you can specify `text` and `image` in `content` field, For example:\n```python\ndataset = [\n    # message 0\n    [\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n        {\"role\": \"user\", \"content\": \"Tell me who you are.\"},\n        {\"role\": \"assistant\", \"content\": \"I am a large language model named Qwen...\"},\n    ],\n    # message 1\n    [\n        {\n            \"role\": \"user\",\n            \"content\": [\n                {\"type\": \"image\", \"image\": \"file:///path/to/your/image.jpg\"},\n                {\"type\": \"text\", \"text\": \"Output all text in the image\"},\n            ],\n        },\n        {\"role\": \"assistant\", \"content\": \"The text in the image is balabala...\"},\n    ],\n    # other messages...\n    ...,\n]\n```\nHere, we use a caption dataset **only for demonstration**. You should replace it with your own sft dataset.\n```python\ndef prepare_dataset(n_sample: int = 20) -> list[list[dict]]:\n    from datasets import load_dataset\n\n    dataset = load_dataset(\n        \"laion/220k-GPT4Vision-captions-from-LIVIS\", split=f\"train[:{n_sample}]\"\n    )\n    return [\n        [\n            {\n                \"role\": \"user\",\n                \"content\": [\n                    {\"type\": \"image\", \"image\": sample[\"url\"]},\n                    {\"type\": \"text\", \"text\": \"generate a caption for this image\"},\n                ],\n            },\n            {\"role\": \"assistant\", \"content\": sample[\"caption\"]},\n        ]\n        for sample in dataset\n    ]\n\n\ndataset = prepare_dataset()\n```\n\nThen process the dataset into tensors:\n```python\nfrom qwen_vl_utils import process_vision_info\n\n\ndef batched(iterable, n: int):\n    # batched('ABCDEFG', 3) ‚Üí ABC DEF G\n    assert n >= 1, \"batch size must be at least one\"\n    from itertools import islice\n\n    iterator = iter(iterable)\n    while batch := tuple(islice(iterator, n)):\n        yield batch\n\n\nbatch_size = 1\ncalib_data = []\nfor batch in batched(dataset, batch_size):\n    text = processor.apply_chat_template(\n        batch, tokenize=False, add_generation_prompt=True\n    )\n    image_inputs, video_inputs = process_vision_info(batch)\n    inputs = processor(\n        text=text,\n        images=image_inputs,\n        videos=video_inputs,\n        padding=True,\n        return_tensors=\"pt\",\n    )\n    calib_data.append(inputs)\n```\nThen just run the calibration process by one line of code:\n```python\nmodel.quantize(dataset, cache_examples_on_gpu=False)\n```\nFinally, save the quantized model:\n```python\nmodel.save_quantized(quant_path, use_safetensors=True)\nprocessor.save_pretrained(quant_path)\n```\nThen you can obtain your own GPTQ quantized model for deployment. Enjoy!\n### Benchmark\n#### Performance of Quantized Models\nThis section reports the generation performance of quantized models (including GPTQ and AWQ) of the Qwen2-VL series. Specifically, we report:\n\n- MMMU_VAL (Accuracy)\n- DocVQA_VAL (Accuracy)\n- MMBench_DEV_EN (Accuracy)\n- MathVista_MINI (Accuracy)\n\nWe use [VLMEvalkit](https://github.com/open-compass/VLMEvalKit) to evaluate all models.\n\n| Model Size | Quantization | MMMU | DocVQA | MMBench | MathVista  |\n| --- | --- | --- | --- | --- | --- |\n| Qwen2-VL-72B-Instruct | BF16<br><sup>([ü§ó](https://huggingface.co/Qwen/Qwen2-VL-72B-Instruct)[ü§ñ](https://modelscope.cn/models/qwen/Qwen2-VL-72B-Instruct)) | 65.44 | 95.79 | 86.94 | 70.19 |\n|  | GPTQ-Int8<br><sup>([ü§ó](https://huggingface.co/Qwen/Qwen2-VL-72B-Instruct-GPTQ-Int8)[ü§ñ](https://modelscope.cn/models/qwen/Qwen2-VL-72B-Instruct-GPTQ-Int8)) | 64.56 | 95.84 | 87.03 | 68.90 |\n|  | GPTQ-Int4<br><sup>([ü§ó](https://huggingface.co/Qwen/Qwen2-VL-72B-Instruct-GPTQ-Int4)[ü§ñ](https://modelscope.cn/models/qwen/Qwen2-VL-72B-Instruct-GPTQ-Int4)) | 64.00 | 95.70 | 86.68 | 69.20 |\n|  | AWQ<br><sup>([ü§ó](https://huggingface.co/Qwen/Qwen2-VL-72B-Instruct-AWQ)[ü§ñ](https://modelscope.cn/models/qwen/Qwen2-VL-72B-Instruct-AWQ)) | 64.22 | 95.72 | 86.43 | 68.40 |\n| Qwen2-VL-7B-Instruct | BF16<br><sup>([ü§ó](https://huggingface.co/Qwen/Qwen2-VL-7B-Instruct)[ü§ñ](https://modelscope.cn/models/qwen/Qwen2-VL-7B-Instruct)) | 53.77 | 93.89 | 81.78 | 58.20 |\n|  | GPTQ-Int8<br><sup>([ü§ó](https://huggingface.co/Qwen/Qwen2-VL-7B-Instruct-GPTQ-Int8)[ü§ñ](https://modelscope.cn/models/qwen/Qwen2-VL-7B-Instruct-GPTQ-Int8)) | 53.00 | 93.94 | 82.38 | 57.90 |\n|  | GPTQ-Int4<br><sup>([ü§ó](https://huggingface.co/Qwen/Qwen2-VL-7B-Instruct-GPTQ-Int4)[ü§ñ](https://modelscope.cn/models/qwen/Qwen2-VL-7B-Instruct-GPTQ-Int4)) | 52.55 | 93.16 | 81.27 | 60.30 |\n|  | AWQ<br><sup>([ü§ó](https://huggingface.co/Qwen/Qwen2-VL-7B-Instruct-AWQ)[ü§ñ](https://modelscope.cn/models/qwen/Qwen2-VL-7B-Instruct-AWQ)) | 53.66 | 93.10 | 81.61 | 56.80 |\n| Qwen2-VL-2B-Instruct | BF16<br><sup>([ü§ó](https://huggingface.co/Qwen/Qwen2-VL-2B-Instruct)[ü§ñ](https://modelscope.cn/models/qwen/Qwen2-VL-2B-Instruct)) | 41.88 | 88.34 | 72.07 | 44.40 |\n|  | GPTQ-Int8<br><sup>([ü§ó](https://huggingface.co/Qwen/Qwen2-VL-2B-Instruct-GPTQ-Int8)[ü§ñ](https://modelscope.cn/models/qwen/Qwen2-VL-2B-Instruct-GPTQ-Int8)) | 41.55 |  88.28 | 71.99 | 44.60 |\n|  | GPTQ-Int4<br><sup>([ü§ó](https://huggingface.co/Qwen/Qwen2-VL-2B-Instruct-GPTQ-Int4)[ü§ñ](https://modelscope.cn/models/qwen/Qwen2-VL-2B-Instruct-GPTQ-Int4)) | 39.22 | 87.21 | 70.87 | 41.69 |\n|  | AWQ<br><sup>([ü§ó](https://huggingface.co/Qwen/Qwen2-VL-2B-Instruct-AWQ)[ü§ñ](https://modelscope.cn/models/qwen/Qwen2-VL-2B-Instruct-AWQ)) | 41.33 | 86.96 | 71.64 | 39.90 |\n\n\n\n\n\n\n#### Speed Benchmark\nThis section reports the speed performance of bf16 models, quantized models (including GPTQ-Int4, GPTQ-Int8 and AWQ) of the Qwen2-VL series. Specifically, we report the inference speed (tokens/s) as well as memory footprint (GB) under the conditions of different context lengths.\n\nThe environment of the evaluation with huggingface transformers is:\n\n- NVIDIA A100 80GB\n- CUDA 11.8\n- Pytorch 2.2.1+cu118\n- Flash Attention 2.6.1\n- Transformers 4.38.2\n- AutoGPTQ 0.6.0+cu118\n- AutoAWQ 0.2.5+cu118 (autoawq_kernels 0.0.6+cu118)\n\nNote:\n\n- We use the batch size of 1 and the least number of GPUs as possible for the evalution.\n- We test the speed and memory of generating 2048 tokens with the input lengths of 1, 6144, 14336, 30720, 63488, and 129024 tokens.\n- 72B (transformers)\n\n| Model | Input Length | Quantization | GPU Num | Speed(tokens/s) | GPU Memory(GB) |\n| --- | --- | --- | --- | --- | --- |\n| Qwen2-VL-72B-Instruct | 1 | BF16 | 2 | 8.90 | 138.74 |\n|  |  | GPTQ-Int8 | 2 | 9.53 | 75.173 |\n|  |  | GPTQ-Int4 | 1 | 11.04 | 42.46 |\n|  |  | AWQ | 1 | 12.00 | 41.98 |\n|  | 6144 | BF16 | 2 | 6.53 | 148.66 |\n|  |  | GPTQ-Int8 | 2 | 6.97 | 85.09 |\n|  |  | GPTQ-Int4 | 1 | 7.62 | 49.05 |\n|  |  | AWQ | 1 | 8.33 | 48.58 |\n|  | 14336 | BF16 | 3 | 4.39 | 165.92 |\n|  |  | GPTQ-Int8 | 2 | 5.04 | 99.31 |\n|  |  | GPTQ-Int4 | 1 | 5.39 | 58.76 |\n|  |  | AWQ | 1 | 5.72 | 58.29 |\n|  | 30720 | BF16 | 4 | 2.93 | 204.33 |\n|  |  | GPTQ-Int8 | 2 | 3.16 | 127.77 |\n|  |  | GPTQ-Int4 | 2 | 3.27 | 85.13 |\n|  |  | AWQ | 2 | 3.39 | 94.65 |\n\n- 7B (transformers)\n\n| Model | Input Length | Quantization | GPU Num | Speed(tokens/s) | GPU Memory(GB) |\n| --- | --- | --- | --- | --- | --- |\n| Qwen2-VL-7B-Instruct | 1 | BF16 | 1 | 39.02 | 16.07 |\n|  |  | GPTQ-Int8 | 1 | 31.60 | 10.11 |\n|  |  | GPTQ-Int4 | 1 | 42.76 | 7.20 |\n|  |  | AWQ | 1 | 32.08 | 7.07 |\n|  | 6144 | BF16 | 1 | 38.75 | 21.56 |\n|  |  | GPTQ-Int8 | 1 | 31.31 | 15.61 |\n|  |  | GPTQ-Int4 | 1 | 39.75 | 12.69 |\n|  |  | AWQ | 1 | 32.66 | 12.56 |\n|  | 14336 | BF16 | 1 | 30.65 | 29.07 |\n|  |  | GPTQ-Int8 | 1 | 27.96 | 23.11 |\n|  |  | GPTQ-Int4 | 1 | 29.72 | 20.20 |\n|  |  | AWQ | 1 | 31.42 | 20.07 |\n|  | 30720 | BF16 | 1 | 19.53 | 44.08 |\n|  |  | GPTQ-Int8 | 1 | 18.37 | 38.13 |\n|  |  | GPTQ-Int4 | 1 | 19.15 | 35.22 |\n|  |  | AWQ | 1 | 19.95 | 35.08 |\n\n\n- 2B (transformers)\n\n| Model | Input Length | Quantization | GPU Num | Speed(tokens/s) | GPU Memory(GB) |\n| --- | --- | --- | --- | --- | --- |\n| Qwen2-VL-2B-Instruct | 1 | BF16 | 1 | 35.29 | 4.68 |\n|  |  | GPTQ-Int8 | 1 | 28.59 | 3.55 |\n|  |  | GPTQ-Int4 | 1 | 39.76 | 2.91 |\n|  |  | AWQ | 1 | 29.89 | 2.88 |\n|  | 6144 | BF16 | 1 | 36.58 | 10.01 |\n|  |  | GPTQ-Int8 | 1 | 29.53  | 8.87 |\n|  |  | GPTQ-Int4 | 1 | 39.27 | 8.21 |\n|  |  | AWQ | 1 | 33.42 | 8.18 |\n|  | 14336 | BF16 | 1 | 36.31 | 17.20 |\n|  |  | GPTQ-Int8 | 1 | 31.03 | 16.07 |\n|  |  | GPTQ-Int4 | 1 | 39.89 | 15.40 |\n|  |  | AWQ | 1 | 32.28 | 15.40 |\n|  | 30720 | BF16 | 1 | 32.53 | 31.64 |\n|  |  | GPTQ-Int8 | 1 | 27.76 | 30.51 |\n|  |  | GPTQ-Int4 | 1 | 30.73 | 29.84 |\n|  |  | AWQ | 1 | 31.55 | 29.84 |\n\n\n\n## Deployment\n\nWe recommend using vLLM for fast Qwen2-VL deployment and inference. You need to use `vllm>=0.6.1` to enable Qwen2-VL support. You can also use our [official docker image](#-docker).\n\n### Installation\n```bash\npip install git+https://github.com/huggingface/transformers@21fac7abba2a37fae86106f87fcf9974fd1e3830\npip install accelerate\npip install qwen-vl-utils\n# Change to your CUDA version\nCUDA_VERSION=cu121\npip install 'vllm==0.6.1' --extra-index-url https://download.pytorch.org/whl/${CUDA_VERSION}\n\n```\n### Start an OpenAI API Service\n\nRun the command below to start an OpenAI-compatible API service:\n\n```bash\npython -m vllm.entrypoints.openai.api_server --served-model-name Qwen2-VL-7B-Instruct --model Qwen/Qwen2-VL-7B-Instruct\n```\n\nThen you can use the chat API as below (via curl or Python API):\n\n```bash\ncurl http://localhost:8000/v1/chat/completions \\\n    -H \"Content-Type: application/json\" \\\n    -d '{\n    \"model\": \"Qwen2-VL-7B-Instruct\",\n    \"messages\": [\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n    {\"role\": \"user\", \"content\": [\n        {\"type\": \"image_url\", \"image_url\": {\"url\": \"https://modelscope.oss-cn-beijing.aliyuncs.com/resource/qwen.png\"}},\n        {\"type\": \"text\", \"text\": \"What is the text in the illustrate?\"}\n    ]}\n    ]\n    }'\n```\n\n```python\nfrom openai import OpenAI\n\n# Set OpenAI's API key and API base to use vLLM's API server.\nopenai_api_key = \"EMPTY\"\nopenai_api_base = \"http://localhost:8000/v1\"\n\nclient = OpenAI(\n    api_key=openai_api_key,\n    base_url=openai_api_base,\n)\n\nchat_response = client.chat.completions.create(\n    model=\"Qwen2-VL-7B-Instruct\",\n    messages=[\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n        {\n            \"role\": \"user\",\n            \"content\": [\n                {\n                    \"type\": \"image_url\",\n                    \"image_url\": {\n                        \"url\": \"https://modelscope.oss-cn-beijing.aliyuncs.com/resource/qwen.png\"\n                    },\n                },\n                {\"type\": \"text\", \"text\": \"What is the text in the illustrate?\"},\n            ],\n        },\n    ],\n)\nprint(\"Chat response:\", chat_response)\n```\n\nYou can also upload base64-encoded local images (see [OpenAI API protocol document](https://platform.openai.com/docs/guides/vision/uploading-base-64-encoded-images) for more details):\n```python\nimport base64\nfrom openai import OpenAI\n# Set OpenAI's API key and API base to use vLLM's API server.\nopenai_api_key = \"EMPTY\"\nopenai_api_base = \"http://localhost:8000/v1\"\nclient = OpenAI(\n    api_key=openai_api_key,\n    base_url=openai_api_base,\n)\nimage_path = \"/path/to/local/image.png\"\nwith open(image_path, \"rb\") as f:\n    encoded_image = base64.b64encode(f.read())\nencoded_image_text = encoded_image.decode(\"utf-8\")\nbase64_qwen = f\"data:image;base64,{encoded_image_text}\"\nchat_response = client.chat.completions.create(\n    model=\"Qwen2-7B-Instruct\",\n    messages=[\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n        {\n            \"role\": \"user\",\n            \"content\": [\n                {\n                    \"type\": \"image_url\",\n                    \"image_url\": {\n                        \"url\": base64_qwen\n                    },\n                },\n                {\"type\": \"text\", \"text\": \"What is the text in the illustrate?\"},\n            ],\n        },\n    ],\n)\nprint(\"Chat response:\", chat_response)\n```\n\n### Notes\n\n- ‚ö†Ô∏è**NOTE**: Now `vllm.entrypoints.openai.api_server` does not support set `min_pixels` and `max_pixels` in messages (we are working hard on supporting this feature). If you want to limit the resolution, you can set them in model's `preprocessor_config.json`:\n\n```json\n{\n  \"min_pixels\": 50176,\n  \"max_pixels\": 1003520,\n  ...\n}\n```\n\n- ‚ö†Ô∏è**NOTE**: Now `vllm.entrypoints.openai.api_server` does not support video input yet. We are actively developing on it.\n- ‚ö†Ô∏è**NOTE**: If you want to pass multiple images in a single prompt, you need to pass `--limit-mm-per-prompt image=<N>` argument (`N` is max number of images in each prompt) when launching `vllm.entrypoints.openai.api_server`.\n### Inference Locally\n\nYou can also use vLLM to inference Qwen2-VL locally:\n\n```python\nfrom transformers import AutoProcessor\nfrom vllm import LLM, SamplingParams\nfrom qwen_vl_utils import process_vision_info\n\nMODEL_PATH = \"Qwen/Qwen2-VL-7B-Instruct\"\n\nllm = LLM(\n    model=MODEL_PATH,\n    limit_mm_per_prompt={\"image\": 10, \"video\": 10},\n)\n\nsampling_params = SamplingParams(\n    temperature=0.1,\n    top_p=0.001,\n    repetition_penalty=1.05,\n    max_tokens=256,\n    stop_token_ids=[],\n)\n\nmessages = [\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n    {\n        \"role\": \"user\",\n        \"content\": [\n            {\n                \"type\": \"image\",\n                \"image\": \"https://modelscope.oss-cn-beijing.aliyuncs.com/resource/qwen.png\",\n                \"min_pixels\": 224 * 224,\n                \"max_pixels\": 1280 * 28 * 28,\n            },\n            {\"type\": \"text\", \"text\": \"What is the text in the illustrate?\"},\n        ],\n    },\n]\n# For video input, you can pass following values instead:\n# \"type\": \"video\",\n# \"video\": \"<video URL>\",\n\nprocessor = AutoProcessor.from_pretrained(MODEL_PATH)\nprompt = processor.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True,\n)\nimage_inputs, video_inputs = process_vision_info(messages)\n\nmm_data = {}\nif image_inputs is not None:\n    mm_data[\"image\"] = image_inputs\nif video_inputs is not None:\n    mm_data[\"video\"] = video_inputs\n\nllm_inputs = {\n    \"prompt\": prompt,\n    \"multi_modal_data\": mm_data,\n}\n\noutputs = llm.generate([llm_inputs], sampling_params=sampling_params)\ngenerated_text = outputs[0].outputs[0].text\n\nprint(generated_text)\n```\n\n\n## Training\n#### LLaMA-Factory\n\nHere we provide a script for supervised finetuning Qwen2-VL with\n`LLaMA-Factory <https://github.com/hiyouga/LLaMA-Factory>`. This\nscript for supervised finetuning (SFT) has the following features:\n\n-  Support multi-images input;\n\n-  Support single-GPU and multi-GPU training;\n\n-  Support full-parameter tuning, LoRA.\n\nIn the following, we introduce more details about the usage of the\nscript.\n\n#### Installation\n\nBefore you start, make sure you have installed the following packages:\n\n1. Follow the instructions of\n   `LLaMA-Factory <https://github.com/hiyouga/LLaMA-Factory>`, and build\n   the environment.\n2. Install these packages (Optional):\n\n```\npip install deepspeed\npip install flash-attn --no-build-isolation\n```\n\n3. If you want to use\n   `FlashAttention-2 <https://github.com/Dao-AILab/flash-attention>`,\n   make sure your CUDA is 11.6 and above.\n\n#### Data Preparation\n\nLLaMA-Factory provides several training datasets in ``data`` folder, you\ncan use it directly. If you are using a custom dataset, please prepare\nyour dataset as follows.\n\n1. Organize your data in a **json** file and put your data in ``data``\n   folder. LLaMA-Factory supports multimodal dataset in ``sharegpt``\n   format.\n\n-  The dataset in ``sharegpt`` format should follow the below format:\n\n```json\n[\n  {\n    \"messages\": [\n      {\n        \"content\": \"<image>Who are they?\",\n        \"role\": \"user\"\n      },\n      {\n        \"content\": \"They're Kane and Gretzka from Bayern Munich.\",\n        \"role\": \"assistant\"\n      },\n      {\n        \"content\": \"What are they doing?<image>\",\n        \"role\": \"user\"\n      },\n      {\n        \"content\": \"They are celebrating on the soccer field.\",\n        \"role\": \"assistant\"\n      }\n    ],\n    \"images\": [\n      \"mllm_demo_data/1.jpg\",\n      \"mllm_demo_data/1.jpg\"\n    ]\n  },\n]\n```\n\n1. Provide your dataset definition in ``data/dataset_info.json`` in the\n   following format .\n\n-  For ``sharegpt`` format dataset, the columns in ``dataset_info.json``\n   should be:\n\n```json\n   \"dataset_name\": {\n       \"file_name\": \"dataset_name.json\",\n       \"formatting\": \"sharegpt\",\n       \"columns\": {\n          \"messages\": \"messages\",\n          \"images\": \"images\"\n        },\n      \"tags\": {\n         \"role_tag\": \"role\",\n         \"content_tag\": \"content\",\n         \"user_tag\": \"user\",\n         \"assistant_tag\": \"assistant\"\n        }\n   }\n```\n\n#### Training\n\nLora SFT examples:\n```\nllamafactory-cli train examples/train_lora/qwen2vl_lora_sft.yaml\nllamafactory-cli export examples/merge_lora/qwen2vl_lora_sft.yaml\n```\n\nLoRA DPO/ORPO/SimPO examples: (using [RLHF-V Dataset](https://huggingface.co/datasets/llamafactory/RLHF-V))\n```\nllamafactory-cli train examples/train_lora/qwen2vl_lora_dpo.yaml\n```\n\nFull SFT examples:\n```\nllamafactory-cli train examples/train_full/qwen2vl_full_sft.yaml\n```\n\nInference examples:\n```\nllamafactory-cli webchat examples/inference/qwen2_vl.yaml\nllamafactory-cli api examples/inference/qwen2_vl.yaml\n```\n\nExecute the following training command:\n\n```bash\nDISTRIBUTED_ARGS=\"\n    --nproc_per_node $NPROC_PER_NODE \\\n    --nnodes $NNODES \\\n    --node_rank $NODE_RANK \\\n    --master_addr $MASTER_ADDR \\\n    --master_port $MASTER_PORT\n    \"\n\ntorchrun $DISTRIBUTED_ARGS src/train.py \\\n    --deepspeed $DS_CONFIG_PATH \\\n    --stage sft \\\n    --do_train \\\n    --model_name_or_path Qwen/Qwen2-VL-7B-Instruct \\\n    --dataset mllm_demo \\\n    --template qwen2_vl \\\n    --finetuning_type lora \\\n    --output_dir $OUTPUT_PATH \\\n    --overwrite_cache \\\n    --overwrite_output_dir \\\n    --warmup_steps 100 \\\n    --weight_decay 0.1 \\\n    --per_device_train_batch_size 2 \\\n    --gradient_accumulation_steps 4 \\\n    --ddp_timeout 9000 \\\n    --learning_rate 5e-6 \\\n    --lr_scheduler_type cosine \\\n    --logging_steps 1 \\\n    --cutoff_len 4096 \\\n    --save_steps 1000 \\\n    --plot_loss \\\n    --num_train_epochs 3 \\\n    --bf16 \n```\n\nand enjoy the training process. To make changes to your training, you\ncan modify the arguments in the training command to adjust the\nhyperparameters. One argument to note is ``cutoff_len``, which is the\nmaximum length of the training data. Control this parameter to avoid OOM\nerror.\n\n## Function Calling\n\nQwen2-VL supports Function Calling (aka. Tool Calling or Tool Use). For details on how to use this capability, please refer to the Qwen-Agent project for [the function calling example](https://github.com/QwenLM/Qwen-Agent/blob/main/examples/qwen2vl_function_calling.py) and [the agent example](https://github.com/QwenLM/Qwen-Agent/blob/main/examples/qwen2vl_assistant_tooluse.py). \n### Simple Use Case\n```python\n# pip install qwen_agent\nfrom typing import List, Union\nfrom datetime import datetime\nfrom qwen_agent.agents import FnCallAgent\nfrom qwen_agent.gui import WebUI\nfrom qwen_agent.tools.base import BaseToolWithFileAccess, register_tool\n\n@register_tool(\"get_date\")\nclass GetDate(BaseToolWithFileAccess):\n    description = \"call this tool to get the current date\"\n    parameters = [\n        {\n            \"name\": \"lang\",\n            \"type\": \"string\",\n            \"description\": \"one of ['en', 'zh'], default is en\",\n            \"required\": False\n        },\n    ]\n\n    def call(self, params: Union[str, dict], files: List[str] = None, **kwargs) -> str:\n        super().call(params=params, files=files)\n        params = self._verify_json_format_args(params)\n        lang = \"zh\" if \"zh\" in params[\"lang\"] else \"en\"\n        now = datetime.now()\n        result = now.strftime(\"%Y-%m-%d %H:%M:%S\") + \"\\n\"\n        weekday = now.weekday()\n        if lang == \"zh\":\n            days_chinese = [\"‰∏Ä\", \"‰∫å\", \"‰∏â\", \"Âõõ\", \"‰∫î\", \"ÂÖ≠\", \"Êó•\"]\n            result += \"‰ªäÂ§©ÊòØÊòüÊúü\" + days_chinese[weekday]\n        else:\n            days = [\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\", \"Sunday\"]\n            result += \"Today is \" + days[weekday]\n        return result\n\n\ndef init_agent_service():\n    llm_cfg_vl = {\n        # Using Qwen2-VL deployed at any openai-compatible service such as vLLM:\n        \"model_type\": \"qwenvl_oai\",\n        \"model\": \"Qwen/Qwen2-VL-7B-Instruct\",\n        \"model_server\": \"http://localhost:8000/v1\",  # api_base\n        \"api_key\": 'EMPTY\",\n    }\n    tools = [\n        \"get_date\",\n        \"code_interpreter\",\n    ]  # code_interpreter is a built-in tool in Qwen-Agent\n    bot = FnCallAgent(\n        llm=llm_cfg_vl,\n        name=\"Qwen2-VL\",\n        description=\"function calling\",\n        function_list=tools,\n    )\n    return bot\n\ndef app_gui():\n    # Define the agent\n    bot = init_agent_service()\n    WebUI(bot).run()\n\n# Launch gradio app\napp_gui()\n```\n\n\n## Demo\n### Web UI Example\n\nIn this section, we provide instructions for users to build a web-based user interface (UI) demo. This UI demo allows users to interact with a predefined model or application through a web browser. Follow the steps below to get started.\n\n#### Installation\n\nBefore you begin, ensure that you have the required dependencies installed on your system. You can install them by running the following command:\n\n```bash\npip install -r requirements_web_demo.txt\n```\n\n#### Running the Demo with FlashAttention-2\n\nOnce the required packages are installed, you can launch the web demo using the following command. This command will start a web server and provide you with a link to access the UI in your web browser.\n\n**Recommended**: For enhanced performance and efficiency, especially in multi-image and video processing scenarios, we strongly recommend using [FlashAttention-2](https://github.com/Dao-AILab/flash-attention). FlashAttention-2 provides significant improvements in memory usage and speed, making it ideal for handling large-scale models and data processing.\n\nTo enable FlashAttention-2, use the following command:\n\n```bash\npython web_demo_mm.py --flash-attn2\n```\n\nThis will load the model with FlashAttention-2 enabled.\n\n**Default Usage**: If you prefer to run the demo without FlashAttention-2 or if you do not specify the `--flash-attn2` option, the demo will load the model using the standard attention implementation:\n\n```bash\npython web_demo_mm.py\n```\n\nAfter running the command, you‚Äôll see a link generated in the terminal similar to this:\n\n```\nRunning on local: http://127.0.0.1:7860/\n```\n\nCopy this link and paste it into your browser to access the web UI, where you can interact with the model by inputting text, uploading images, or using any other provided functionalities.\n\n##### Running the Streaming Video Chat Demo\nAn experimental streaming video chat demo is also available in the ``web_demo_streaming`` directory.\n\nTo run the streaming video chat demo, use the following command:\n\n```bash\ncd web_demo_streaming/\npython app.py --flash-attn2\n```\n\nIf you prefer to run the demo without FlashAttention-2, use the following command:\n```bash\ncd web_demo_streaming/\npython app.py\n```\n\nThis demo supports webcam/screen capture as its video input source. To support screen capture video input, we use code snippet from the following hugginface space: [gstaff/gradio-screen-recorder](https://huggingface.co/spaces/gstaff/gradio-screen-recorder/tree/main).\n\n#### Selecting Different Models (Qwen2-VL Series Only)\n\nThe demo is configured by default to use the `Qwen/Qwen2-VL-7B-Instruct` model, which is part of the Qwen2-VL series and is well-suited for various vision-language tasks. However, if you want to use a different model within the Qwen2-VL series, you can simply update the `DEFAULT_CKPT_PATH` variable in the script:\n\n1. **Locate the `DEFAULT_CKPT_PATH` Variable**:\n   Inside `web_demo_mm.py`, find the `DEFAULT_CKPT_PATH` variable that defines the model checkpoint path. It should look like this:\n\n   ```python\n   DEFAULT_CKPT_PATH = 'Qwen/Qwen2-VL-7B-Instruct'\n   ```\n\n2. **Replace with a Different Qwen2-VL Model Path**:\n   Modify `DEFAULT_CKPT_PATH` to point to another checkpoint path within the Qwen2-VL series. For example:\n\n   ```python\n   DEFAULT_CKPT_PATH = 'Qwen/Qwen2-VL-2B-Instruct'  # Example for a different model in the series\n   ```\n\n3. **Save and Re-run**:\n   After modifying the path, save the script and then re-run the demo using the instructions provided in the `Running the Demo` section above.\n\n**Note:** This `DEFAULT_CKPT_PATH` only supports models from the Qwen2-VL series. If you're using a model outside of the Qwen2-VL series, additional changes to the codebase may be necessary.\n\n\n#### Customization\n\nFurther customization of the web demo, including UI layout, interactions, and additional functionalities like handling specialized input, can be done by modifying the `web_demo_mm.py` script. This flexibility allows you to tailor the web interface to better fit specific tasks or workflows.\n\n\n## Limitations\n\nWhile Qwen2-VL are applicable to a wide range of visual tasks, it is equally important to understand its limitations. Here are some known restrictions:\n\n1. Lack of Audio Support: The current model does **not comprehend audio information** within videos.\n2. Data timeliness: Our image dataset is **updated until June 2023**, and information subsequent to this date may not be covered.\n3. Constraints in Individuals and Intellectual Property (IP): The model's capacity to recognize specific individuals or IPs is limited, potentially failing to comprehensively cover all well-known personalities or brands.\n4. Limited Capacity for Complex Instruction: When faced with intricate multi-step instructions, the model's understanding and execution capabilities require enhancement.\n5. Insufficient Counting Accuracy: Particularly in complex scenes, the accuracy of object counting is not high, necessitating further improvements.\n6. Weak Spatial Reasoning Skills: Especially in 3D spaces, the model's inference of object positional relationships is inadequate, making it difficult to precisely judge the relative positions of objects.\n\nThese limitations serve as ongoing directions for model optimization and improvement, and we are committed to continually enhancing the model's performance and scope of application.\n\n\n## üê≥ Docker\n\nTo simplify the deploy process, we provide docker images with pre-build environments: [qwenllm/qwenvl](https://hub.docker.com/r/qwenllm/qwenvl). You only need to install the driver and download model files to launch demos.\n\n```bash\ndocker run --gpus all --ipc=host --network=host --rm --name qwen2 -it qwenllm/qwenvl:2-cu121 bash\n```\n\n## Citation\n\nIf you find our paper and code useful in your research, please consider giving a star :star: and citation :pencil: :)\n\n\n\n\n```BibTeX\n@article{Qwen2VL,\n  title={Qwen2-VL: Enhancing Vision-Language Model's Perception of the World at Any Resolution},\n  author={Wang, Peng and Bai, Shuai and Tan, Sinan and Wang, Shijie and Fan, Zhihao and Bai, Jinze and Chen, Keqin and Liu, Xuejing and Wang, Jialin and Ge, Wenbin and Fan, Yang and Dang, Kai and Du, Mengfei and Ren, Xuancheng and Men, Rui and Liu, Dayiheng and Zhou, Chang and Zhou, Jingren and Lin, Junyang},\n  journal={arXiv preprint arXiv:2409.12191},\n  year={2024}\n}\n\n@article{Qwen-VL,\n  title={Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond},\n  author={Bai, Jinze and Bai, Shuai and Yang, Shusheng and Wang, Shijie and Tan, Sinan and Wang, Peng and Lin, Junyang and Zhou, Chang and Zhou, Jingren},\n  journal={arXiv preprint arXiv:2308.12966},\n  year={2023}\n}\n```\n\n<br>\n"
        },
        {
          "name": "docker",
          "type": "tree",
          "content": null
        },
        {
          "name": "qwen-vl-utils",
          "type": "tree",
          "content": null
        },
        {
          "name": "requirements_web_demo.txt",
          "type": "blob",
          "size": 0.302734375,
          "content": "# Core dependencies\ngradio==4.42.0\ngradio_client==1.3.0\nqwen-vl-utils==0.0.2\ntransformers-stream-generator==0.0.4\ntorch==2.4.0\ntorchvision==0.19.0\ngit+https://github.com/huggingface/transformers.git\naccelerate\nav\n\n# Optional dependency\n# Uncomment the following line if you need flash-attn\n# flash-attn==2.6.1\n"
        },
        {
          "name": "web_demo_mm.py",
          "type": "blob",
          "size": 11.1015625,
          "content": "# Copyright (c) Alibaba Cloud.\n#\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport copy\nimport re\nfrom argparse import ArgumentParser\nfrom threading import Thread\n\nimport gradio as gr\nimport torch\nfrom qwen_vl_utils import process_vision_info\nfrom transformers import AutoProcessor, Qwen2VLForConditionalGeneration, TextIteratorStreamer\n\nDEFAULT_CKPT_PATH = 'Qwen/Qwen2-VL-7B-Instruct'\n\n\ndef _get_args():\n    parser = ArgumentParser()\n\n    parser.add_argument('-c',\n                        '--checkpoint-path',\n                        type=str,\n                        default=DEFAULT_CKPT_PATH,\n                        help='Checkpoint name or path, default to %(default)r')\n    parser.add_argument('--cpu-only', action='store_true', help='Run demo with CPU only')\n\n    parser.add_argument('--flash-attn2',\n                        action='store_true',\n                        default=False,\n                        help='Enable flash_attention_2 when loading the model.')\n    parser.add_argument('--share',\n                        action='store_true',\n                        default=False,\n                        help='Create a publicly shareable link for the interface.')\n    parser.add_argument('--inbrowser',\n                        action='store_true',\n                        default=False,\n                        help='Automatically launch the interface in a new tab on the default browser.')\n    parser.add_argument('--server-port', type=int, default=7860, help='Demo server port.')\n    parser.add_argument('--server-name', type=str, default='127.0.0.1', help='Demo server name.')\n\n    args = parser.parse_args()\n    return args\n\n\ndef _load_model_processor(args):\n    if args.cpu_only:\n        device_map = 'cpu'\n    else:\n        device_map = 'auto'\n\n    # Check if flash-attn2 flag is enabled and load model accordingly\n    if args.flash_attn2:\n        model = Qwen2VLForConditionalGeneration.from_pretrained(args.checkpoint_path,\n                                                                torch_dtype='auto',\n                                                                attn_implementation='flash_attention_2',\n                                                                device_map=device_map)\n    else:\n        model = Qwen2VLForConditionalGeneration.from_pretrained(args.checkpoint_path, device_map=device_map)\n\n    processor = AutoProcessor.from_pretrained(args.checkpoint_path)\n    return model, processor\n\n\ndef _parse_text(text):\n    lines = text.split('\\n')\n    lines = [line for line in lines if line != '']\n    count = 0\n    for i, line in enumerate(lines):\n        if '```' in line:\n            count += 1\n            items = line.split('`')\n            if count % 2 == 1:\n                lines[i] = f'<pre><code class=\"language-{items[-1]}\">'\n            else:\n                lines[i] = '<br></code></pre>'\n        else:\n            if i > 0:\n                if count % 2 == 1:\n                    line = line.replace('`', r'\\`')\n                    line = line.replace('<', '&lt;')\n                    line = line.replace('>', '&gt;')\n                    line = line.replace(' ', '&nbsp;')\n                    line = line.replace('*', '&ast;')\n                    line = line.replace('_', '&lowbar;')\n                    line = line.replace('-', '&#45;')\n                    line = line.replace('.', '&#46;')\n                    line = line.replace('!', '&#33;')\n                    line = line.replace('(', '&#40;')\n                    line = line.replace(')', '&#41;')\n                    line = line.replace('$', '&#36;')\n                lines[i] = '<br>' + line\n    text = ''.join(lines)\n    return text\n\n\ndef _remove_image_special(text):\n    text = text.replace('<ref>', '').replace('</ref>', '')\n    return re.sub(r'<box>.*?(</box>|$)', '', text)\n\n\ndef _is_video_file(filename):\n    video_extensions = ['.mp4', '.avi', '.mkv', '.mov', '.wmv', '.flv', '.webm', '.mpeg']\n    return any(filename.lower().endswith(ext) for ext in video_extensions)\n\n\ndef _gc():\n    import gc\n    gc.collect()\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n\n\ndef _transform_messages(original_messages):\n    transformed_messages = []\n    for message in original_messages:\n        new_content = []\n        for item in message['content']:\n            if 'image' in item:\n                new_item = {'type': 'image', 'image': item['image']}\n            elif 'text' in item:\n                new_item = {'type': 'text', 'text': item['text']}\n            elif 'video' in item:\n                new_item = {'type': 'video', 'video': item['video']}\n            else:\n                continue\n            new_content.append(new_item)\n\n        new_message = {'role': message['role'], 'content': new_content}\n        transformed_messages.append(new_message)\n\n    return transformed_messages\n\n\ndef _launch_demo(args, model, processor):\n\n    def call_local_model(model, processor, messages):\n\n        messages = _transform_messages(messages)\n\n        text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n        image_inputs, video_inputs = process_vision_info(messages)\n        inputs = processor(text=[text], images=image_inputs, videos=video_inputs, padding=True, return_tensors='pt')\n        inputs = inputs.to(model.device)\n\n        tokenizer = processor.tokenizer\n        streamer = TextIteratorStreamer(tokenizer, timeout=20.0, skip_prompt=True, skip_special_tokens=True)\n\n        gen_kwargs = {'max_new_tokens': 512, 'streamer': streamer, **inputs}\n\n        thread = Thread(target=model.generate, kwargs=gen_kwargs)\n        thread.start()\n\n        generated_text = ''\n        for new_text in streamer:\n            generated_text += new_text\n            yield generated_text\n\n    def create_predict_fn():\n\n        def predict(_chatbot, task_history):\n            nonlocal model, processor\n            chat_query = _chatbot[-1][0]\n            query = task_history[-1][0]\n            if len(chat_query) == 0:\n                _chatbot.pop()\n                task_history.pop()\n                return _chatbot\n            print('User: ' + _parse_text(query))\n            history_cp = copy.deepcopy(task_history)\n            full_response = ''\n            messages = []\n            content = []\n            for q, a in history_cp:\n                if isinstance(q, (tuple, list)):\n                    if _is_video_file(q[0]):\n                        content.append({'video': f'file://{q[0]}'})\n                    else:\n                        content.append({'image': f'file://{q[0]}'})\n                else:\n                    content.append({'text': q})\n                    messages.append({'role': 'user', 'content': content})\n                    messages.append({'role': 'assistant', 'content': [{'text': a}]})\n                    content = []\n            messages.pop()\n\n            for response in call_local_model(model, processor, messages):\n                _chatbot[-1] = (_parse_text(chat_query), _remove_image_special(_parse_text(response)))\n\n                yield _chatbot\n                full_response = _parse_text(response)\n\n            task_history[-1] = (query, full_response)\n            print('Qwen-VL-Chat: ' + _parse_text(full_response))\n            yield _chatbot\n\n        return predict\n\n    def create_regenerate_fn():\n\n        def regenerate(_chatbot, task_history):\n            nonlocal model, processor\n            if not task_history:\n                return _chatbot\n            item = task_history[-1]\n            if item[1] is None:\n                return _chatbot\n            task_history[-1] = (item[0], None)\n            chatbot_item = _chatbot.pop(-1)\n            if chatbot_item[0] is None:\n                _chatbot[-1] = (_chatbot[-1][0], None)\n            else:\n                _chatbot.append((chatbot_item[0], None))\n            _chatbot_gen = predict(_chatbot, task_history)\n            for _chatbot in _chatbot_gen:\n                yield _chatbot\n\n        return regenerate\n\n    predict = create_predict_fn()\n    regenerate = create_regenerate_fn()\n\n    def add_text(history, task_history, text):\n        task_text = text\n        history = history if history is not None else []\n        task_history = task_history if task_history is not None else []\n        history = history + [(_parse_text(text), None)]\n        task_history = task_history + [(task_text, None)]\n        return history, task_history, ''\n\n    def add_file(history, task_history, file):\n        history = history if history is not None else []\n        task_history = task_history if task_history is not None else []\n        history = history + [((file.name,), None)]\n        task_history = task_history + [((file.name,), None)]\n        return history, task_history\n\n    def reset_user_input():\n        return gr.update(value='')\n\n    def reset_state(_chatbot, task_history):\n        task_history.clear()\n        _chatbot.clear()\n        _gc()\n        return []\n\n    with gr.Blocks() as demo:\n        gr.Markdown(\"\"\"\\\n<p align=\"center\"><img src=\"https://modelscope.oss-cn-beijing.aliyuncs.com/resource/qwen.png\" style=\"height: 80px\"/><p>\"\"\"\n                   )\n        gr.Markdown(\"\"\"<center><font size=8>Qwen2-VL</center>\"\"\")\n        gr.Markdown(\"\"\"\\\n<center><font size=3>This WebUI is based on Qwen2-VL, developed by Alibaba Cloud.</center>\"\"\")\n        gr.Markdown(\"\"\"<center><font size=3>Êú¨WebUIÂü∫‰∫éQwen2-VL„ÄÇ</center>\"\"\")\n\n        chatbot = gr.Chatbot(label='Qwen2-VL', elem_classes='control-height', height=500)\n        query = gr.Textbox(lines=2, label='Input')\n        task_history = gr.State([])\n\n        with gr.Row():\n            addfile_btn = gr.UploadButton('üìÅ Upload (‰∏ä‰º†Êñá‰ª∂)', file_types=['image', 'video'])\n            submit_btn = gr.Button('üöÄ Submit (ÂèëÈÄÅ)')\n            regen_btn = gr.Button('ü§îÔ∏è Regenerate (ÈáçËØï)')\n            empty_bin = gr.Button('üßπ Clear History (Ê∏ÖÈô§ÂéÜÂè≤)')\n\n        submit_btn.click(add_text, [chatbot, task_history, query],\n                         [chatbot, task_history]).then(predict, [chatbot, task_history], [chatbot], show_progress=True)\n        submit_btn.click(reset_user_input, [], [query])\n        empty_bin.click(reset_state, [chatbot, task_history], [chatbot], show_progress=True)\n        regen_btn.click(regenerate, [chatbot, task_history], [chatbot], show_progress=True)\n        addfile_btn.upload(add_file, [chatbot, task_history, addfile_btn], [chatbot, task_history], show_progress=True)\n\n        gr.Markdown(\"\"\"\\\n<font size=2>Note: This demo is governed by the original license of Qwen2-VL. \\\nWe strongly advise users not to knowingly generate or allow others to knowingly generate harmful content, \\\nincluding hate speech, violence, pornography, deception, etc. \\\n(Ê≥®ÔºöÊú¨ÊºîÁ§∫ÂèóQwen2-VLÁöÑËÆ∏ÂèØÂçèËÆÆÈôêÂà∂„ÄÇÊàë‰ª¨Âº∫ÁÉàÂª∫ËÆÆÔºåÁî®Êà∑‰∏çÂ∫î‰º†Êí≠Âèä‰∏çÂ∫îÂÖÅËÆ∏‰ªñ‰∫∫‰º†Êí≠‰ª•‰∏ãÂÜÖÂÆπÔºå\\\nÂåÖÊã¨‰ΩÜ‰∏çÈôê‰∫é‰ªáÊÅ®Ë®ÄËÆ∫„ÄÅÊö¥Âäõ„ÄÅËâ≤ÊÉÖ„ÄÅÊ¨∫ËØàÁõ∏ÂÖ≥ÁöÑÊúâÂÆ≥‰ø°ÊÅØ„ÄÇ)\"\"\")\n\n    demo.queue().launch(\n        share=args.share,\n        inbrowser=args.inbrowser,\n        server_port=args.server_port,\n        server_name=args.server_name,\n    )\n\n\ndef main():\n    args = _get_args()\n    model, processor = _load_model_processor(args)\n    _launch_demo(args, model, processor)\n\n\nif __name__ == '__main__':\n    main()\n"
        },
        {
          "name": "web_demo_streaming",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}