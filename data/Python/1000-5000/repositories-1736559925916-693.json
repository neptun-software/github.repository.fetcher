{
  "metadata": {
    "timestamp": 1736559925916,
    "page": 693,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjcwMA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "agiresearch/AIOS",
      "stars": 3562,
      "defaultBranch": "main",
      "files": [
        {
          "name": ".dockerignore",
          "type": "blob",
          "size": 0.0458984375,
          "content": "__pycache__\nvenv\nenv\n.github\ndocs\nscripts\ntests"
        },
        {
          "name": ".env.example",
          "type": "blob",
          "size": 0.087890625,
          "content": "OPENAI_API_KEY=''\nGEMINI_API_KEY=''\nANTHROPIC_API_KEY=''\nGROQ_API_KEY=''\nHF_AUTH_TOKENS=''"
        },
        {
          "name": ".gitattributes",
          "type": "blob",
          "size": 0.1162109375,
          "content": "*.css linguist-vendored\n*.html linguist-vendored\n*.js linguist-vendored\n*.ts linguist-vendored\n*.tsx linguist-vendored\n"
        },
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 2.7119140625,
          "content": "logs\n.idea/\n.DS_Store\n.ipynb_checkpoints\n__pycache__\n.idea\ncontext_restoration\n.pytest_cache\n.env\ntemp/\npyopenagi/environments/\ndeplogs.txt\nweb/deplogs.txt\n\n# IPython\nprofile_default/\nipython_config.py\n\n# Remove previous ipynb_checkpoints\n#   git rm -r .ipynb_checkpoints/\n\n### Python ###\n# Byte-compiled / optimized / DLL files\n*.py[cod]\n*$py.class\n\n# C extensions\n*.so\n\n# Distribution / packaging\n.Python\nbuild/\ndevelop-eggs/\ndist/\ndownloads\neggs\n.eggs/\n# lib\nlib64\nbin\nparts/\nsdist\nvar/\nwheels\nshare/python-wheels/\n*.egg-info/\n.installed.cfg\n*.egg\nMANIFEST\n\n# PyInstaller\n#  Usually these files are written by a python script from a template\n#  before PyInstaller builds the exe, so as to inject date/other infos into it.\n*.manifest\n*.spec\n\n# Installer logs\npip-log.txt\npip-delete-this-directory.txt\n\n# Unit test / coverage reports\nhtmlcov/\n.tox/\n.nox/\n.coverage\n.coverage.*\n.cache\nnosetests.xml\ncoverage.xml\n*.cover\n*.py,cover\n.hypothesis/\n.pytest_cache/\ncover/\n\n# Translations\n*.mo\n*.pot\n\n# Django stuff:\n*.log\n\nlocal_settings.py\ndb.sqlite3\ndb.sqlite3-journal\n\n# Flask stuff:\ninstance/\n.webassets-cache\n\n# Scrapy stuff:\n.scrapy\n\n# Sphinx documentation\ndocs/_build/\n\n# PyBuilder\n.pybuilder/\n\ntarget\nbuild\noutput\n\n# Jupyter Notebook\n\n# IPython\n\n# pyenv\n#   For a library or package, you might want to ignore these files since the code is\n#   intended to run in multiple environments; otherwise, check them in:\n# .python-version\n\n# pipenv\n#   According to pypa/pipenv#598, it is recommended to include Pipfile.lock in version control.\n#   However, in case of collaboration, if having platform-specific dependencies or dependencies\n#   having no cross-platform support, pipenv may install dependencies that don't work, or not\n#   install all needed dependencies.\n#Pipfile.lock\n\n# PEP 582; used by e.g. github.com/David-OConnor/pyflow\n__pypackages__\n\n# Celery stuff\ncelerybeat-schedule\ncelerybeat.pid\n\n# SageMath parsed files\n*.sage.py\n\n# Environments\n.env\n.venv\nenv/\nvenv/\nENV/\nenv.bak/\nvenv.bak/\n\n# Spyder project settings\n.spyderproject\n.spyproject\n\n# Rope project settings\n.ropeproject\n\n# mkdocs documentation\n/site\n\n# mypy\n.mypy_cache/\n.dmypy.json\ndmypy.json\n\n# Pyre type checker\n.pyre/\n\n# pytype static type analyzer\n.pytype/\n\n# Cython debug symbols\ncython_debug/\n\n# End of https://www.toptal.com/developers/gitignore/api/python,jupyternotebooks\n/pyopenagi/environments/\n\n\n# dependencies\nnode_modules\n/.pnp\n.pnp.js\n.yarn/install-state.gz\n\n# testing\n/coverage\n\n# next.js\n/.next/\n.next/\n/out/\n\n# production\n/build\n# misc\n.DS_Store\n*.pem\n\n# debug\nnpm-debug.log*\nyarn-debug.log*\nyarn-error.log*\n\n# local env files\n.env*.local\n\n# vercel\n.vercel\n\n# typescript\n*.tsbuildinfo\nnext-env.d.ts\n\n# metagpt\nmetagpt\nworkspace\n\ncache\n\n# Ignore configuration files\naios/config/config.yaml"
        },
        {
          "name": ".pre-commit-config.yaml",
          "type": "blob",
          "size": 0.3359375,
          "content": "repos:\n-   repo: https://github.com/pre-commit/pre-commit-hooks\n    rev: v3.2.0\n    hooks:\n    -   id: trailing-whitespace\n    -   id: end-of-file-fixer\n    -   id: check-yaml\n    -   id: check-added-large-files\n\n- repo: https://github.com/astral-sh/ruff-pre-commit\n  # Ruff version.\n  rev: v0.5.0\n  hooks:\n    # Run the linter.\n    - id: ruff\n"
        },
        {
          "name": "Dockerfile",
          "type": "blob",
          "size": 0.3154296875,
          "content": "FROM python:3.11.8-slim-bullseye\n\nENV PYTHONUNBUFFERED True\n\nENV APP_HOME /app\nWORKDIR $APP_HOME\nCOPY . ./\n\nENV PORT 8000\n\nRUN pip install --no-cache-dir -r requirements.txt\n\n# As an example here we're running the web service with one worker on uvicorn.\nCMD exec uvicorn server:app --host 0.0.0.0 --port ${PORT} --workers 1"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 1.0439453125,
          "content": "MIT License\n\nCopyright (c) 2024 AGI Research\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 17.9619140625,
          "content": "# AIOS: AI Agent Operating System\n\n<a href='https://arxiv.org/abs/2403.16971'><img src='https://img.shields.io/badge/Paper-PDF-red'></a>\n<a href='https://arxiv.org/abs/2312.03815'><img src='https://img.shields.io/badge/Paper-PDF-blue'></a>\n<a href='https://docs.aios.foundation/'><img src='https://img.shields.io/badge/Documentation-AIOS-green'></a>\n[![Code License](https://img.shields.io/badge/Code%20License-MIT-orange.svg)](https://github.com/agiresearch/AIOS/blob/main/LICENSE)\n<a href='https://discord.gg/B2HFxEgTJX'><img src='https://img.shields.io/badge/Community-Discord-8A2BE2'></a>\n[![Gurubase](https://img.shields.io/badge/Gurubase-Ask%20AIOS%20Guru-006BFF)](https://gurubase.io/g/aios)\n\n<a href=\"https://trendshift.io/repositories/8908\" target=\"_blank\"><img src=\"https://trendshift.io/api/badge/repositories/8908\" alt=\"agiresearch%2FAIOS | Trendshift\" style=\"width: 250px; height: 55px;\" width=\"250\" height=\"55\"/></a>\n\nAIOS is the AI Agent Operating System, which embeds large language model (LLM) into the operating system and facilitates the development and deployment of LLM-based AI Agents. AIOS is designed to address problems (e.g., scheduling, context switch, memory management, storage management, tool management, Agent SDK management, etc.) during the development and deployment of LLM-based agents, towards a better AIOS-Agent ecosystem for agent developers and agent users. AIOS includes the AIOS Kernel (this [AIOS](https://github.com/agiresearch/AIOS) repository) and the AIOS SDK (the [Cerebrum](https://github.com/agiresearch/Cerebrum) repository). AIOS supports both Web UI and Terminal UI.\n\n## üè† Architecture of AIOS\n### Overview\n<p align=\"center\">\n<img src=\"docs/assets/aios-figs/architecture.jpg\">\n</p>\n\nThe AIOS system is comprised of two key components: the AIOS kernel and the AIOS SDK.\nThe AIOS kernel acts as an abstraction layer over the operating system kernel, managing various resources that agents require, such as LLM, memory, storage and tool. \nThe AIOS SDK is designed for agent users and developers, enabling them to build and run agent applications by interacting with the AIOS kernel.\nAIOS kernel is the current repository and AIOS SDK can be found at [here](https://github.com/agiresearch/Cerebrum)\n\n### Modules and Connections\nBelow shows how agents utilize AIOS SDK to interact with AIOS kernel and how AIOS kernel receives agent queries and leverage the chain of syscalls that are scheduled and dispatched to run in different modules. \n<p align=\"center\">\n<img src=\"docs/assets/aios-figs/details.png\">\n</p>\n\n## üì∞ News\n- **[2024-11-30]** üî• AIOS v0.2: Disentangled AIOS Kernel (this [AIOS](https://github.com/agiresearch/AIOS) repository) and AIOS SDK (The [Cerebrum](https://github.com/agiresearch/Cerebrum) repository), Remote Kernel for agent users. \n- **[2024-09-01]** üî• AIOS supports multiple agent creation frameworks (e.g., ReAct, Reflexion, OpenAGI, AutoGen, Open Interpreter, MetaGPT). Agents created by these frameworks can onboard AIOS. Onboarding guidelines can be found at the [Doc](https://docs.aios.foundation/aios-docs/aios-agent/how-to-develop-agents).\n- **[2024-07-10]** üìñ AIOS documentation is up, which can be found at [Website](https://docs.aios.foundation/).\n- **[2024-06-20]** üî• Function calling for open-sourced LLMs (native huggingface, vLLM, ollama) is supported.\n- **[2024-05-20]** üöÄ More agents with ChatGPT-based tool calling are added (i.e., MathAgent, RecAgent, TravelAgent, AcademicAgent and CreationAgent), their profiles and workflows can be found in [OpenAGI](https://github.com/agiresearch/OpenAGI).\n- **[2024-05-13]** üõ†Ô∏è Local models (diffusion models) as tools from HuggingFace are integrated.\n- **[2024-05-01]** üõ†Ô∏è The agent creation in AIOS is refactored, which can be found in our [OpenAGI](https://github.com/agiresearch/OpenAGI) package.\n- **[2024-04-05]** üõ†Ô∏è AIOS currently supports external tool callings (google search, wolframalpha, rapid API, etc).\n- **[2024-04-02]** ü§ù AIOS [Discord Community](https://discord.gg/B2HFxEgTJX) is up. Welcome to join the community for discussions, brainstorming, development, or just random chats! For how to contribute to AIOS, please see [CONTRIBUTE](https://github.com/agiresearch/AIOS/blob/main/docs/CONTRIBUTE.md).\n- **[2024-03-25]** ‚úàÔ∏è Our paper [AIOS: LLM Agent Operating System](https://arxiv.org/abs/2403.16971) is released!\n- **[2023-12-06]** üìã After several months of working, our perspective paper [LLM as OS, Agents as Apps: Envisioning AIOS, Agents and the AIOS-Agent Ecosystem](https://arxiv.org/abs/2312.03815) is officially released.\n\n## Different deployment modes of AIOS\nHere are some key notations that are required to know before introducing the different modes of AIOS. \n- **AHM (Agent Hub Machine)**: Central server that hosts the agent marketplace/repository where users can publish, download, and share agents. Acts as the distribution center for all agent-related resources.\n- **AUM (Agent UI Machine)**: Client machine that provides user interface for interacting with agents. Can be any device from mobile phones to desktops that supports agent visualization and control.\n- **ADM (Agent Development Machine)**: Development environment where agent developers write, debug and test their agents. Requires proper development tools and libraries.\n- **ARM (Agent Running Machine)**: Execution environment where agents actually run and perform tasks. Needs adequate computational resources for agent operations.\n\nThe following parts introduce different modes of deploying AIOS. **Currently, AIOS already supports Mode 1 and Mode 2, other modes with new features are still ongoing.**\n\n### Mode 1 (Local Kernel Mode)\n\n<p align=\"center\">\n<img src=\"docs/assets/aios-figs/stage1.png\" width=300>\n</p>\n\n- Features:\n  - For agent users: They can download agents from agent hub from Machine B and run agents on Machine A. \n  - For agent developers: They can develop and test agents in Machine A and can upload agents to agent hub on Machine B.\n\n### Mode 2 (Remote Kernel Mode)\n\n<p align=\"center\">\n<img src=\"docs/assets/aios-figs/stage2.png\" width=300>\n</p>\n\n- Features: \n  - Remote use of agents: Agent users / developers can use agents on Machine B, which is different from the development and running machine (Machine A).  \n  - Benefit users who would like to use agents on resource-restricted machine (e.g., mobile device or edge device)\n\n### Mode 2.5 (Remote Kernel Dev Mode)\n\n<p align=\"center\">\n<img src=\"docs/assets/aios-figs/stage2.5.png\" width=300>\n</p>\n\n- Features:\n  - Remote development of agents: Agent developers can develop their agents on Machine B while running and testing their agents in Machine A. Benefit developers who would like to develop agents on resource-restricted machine (e.g., mobile device or edge device)\n- Critical technique:\n  - Packaging and agent transmission on different machines for distributed agent development and testing\n\n### Mode 3 (Personal Remote Kernel Mode)\n\n<p align=\"center\">\n<img src=\"docs/assets/aios-figs/stage3.png\" width=800>\n</p>\n\n- Ongoing Features:\n  - Each user/developer can have their personal AIOS with long-term persistent data as long as they have registered account in the AIOS ecosystem\n  - Their personal data can be synced to different machines with the same account\n\n- Critical techniques:\n  - User account registration and verification mechanism\n  - Persistent personal data storage for each user's AIOS\n  - Synchronization for different AIOS instances on different devices within the same account\n  - Data privacy mechanism\n\n### Mode 4 (Personal Remote Virtual Kernel Mode)\n\n<p align=\"center\">\n<img src=\"docs/assets/aios-figs/stage4.png\" width=800>\n</p>\n\n- Ongoing Features:\n  - Different user/developer‚Äôs personal AIOS kernels can co-exist in the same physical machine through virtualization\n- Critical techniques:\n  - Virtualization of different AIOS kernel instances in the same machine\n  - Scheduling and resource allocation mechanism for different virtual machines located in the same machine\n\n\n## ‚úàÔ∏è Getting Started\nPlease see our ongoing [documentation](https://docs.aios.foundation/) for more information.\n- [Installation](https://docs.aios.foundation/aios-docs/getting-started/installation)\n- [Quickstart](https://docs.aios.foundation/aios-docs/getting-started/quickstart)\n- [WebUI Quickstart](https://docs.aios.foundation/aios-docs/getting-started/webui-quickstart)\n\n### Installation\n#### Requirements\n##### Python\n- Supported versions: **Python 3.10 - 3.11**\n\n#### Set Up API Keys\nYou need API keys for services like OpenAI, Anthropic, Groq and HuggingFace. The simplest way to configure them is to edit the aios/config/config.yaml.\n\n> [!TIP]\n> It is important to mention that, we stronglyrecommend using the `aios/config/config.yaml` file to set up your API keys. This method is straightforward and helps avoid potential sychronization issues with environment variables. \n\nA simple example to set up your API keys in `aios/config/config.yaml` is shown below:\n```yaml\nopenai: \"your-openai-key\"\ngemini: \"your-gemini-key\"\ngroq: \"your-groq-key\"\nanthropic: \"your-anthropic-key\"\nhuggingface:\n  auth_token: \"your-huggingface-token\"\n  home: \"optional-path\"\n```\n\nTo obtain these API keys:\n1. OpenAI API: Visit https://platform.openai.com/api-keys\n2. Google Gemini API: Visit https://makersuite.google.com/app/apikey\n3. Groq API: Visit https://console.groq.com/keys\n4. HuggingFace Token: Visit https://huggingface.co/settings/tokens\n5. Anthropic API: Visit https://console.anthropic.com/keys\n\n**Use ollama Models:** If you would like to use ollama, you need to download ollama from from https://ollama.com/.\nThen pull the available models you would like to use from https://ollama.com/library\n```bash\nollama pull llama3:8b # use llama3:8b for example\n```\nThen you need to start the ollama server either from ollama app\nor using the following command in the terminal\n```bash\nollama serve\n```\n> [!TIP]\n> ollama can support both CPU-only and GPU environment, details of how to use ollama can be found at [here](https://github.com/ollama/ollama)\n\n\n**Use Huggingface Models:** Some of the huggingface models require authentification, if you want to use all of\nthe models you need to set up  your authentification token in https://huggingface.co/settings/tokens\nand set up it as an environment variable using the following command\n\nBy default, huggingface will download the models in the `~/.cache` directory.\nIf you want to designate the download directory, you can set up the home path in the `aios/config/config.yaml` file.\n\nIf you want to speed up the inference of huggingface models, you can use vLLM as the backend.\n\n> [!NOTE]\n>\n> It is important to note that vLLM currently only supports linux and GPU-enabled environment. So if you do not have the environment, you need to choose other options.\n\nConsidering that vLLM itself does not support passing designated GPU ids, you need to either\nsetup the environment variable,\n\n```bash\nexport CUDA_VISIBLE_DEVICES=\"0\" # replace with your designated gpu ids\n```\n\n##### Detailed Setup Instructions\nFor detailed instructions on setting up API keys and configuration files, see [Environment Variables Configuration](https://app.gitbook.com/o/6h6b4xbBVMu2pFXdNM0D/s/5h7XvlMFgKMtRboLGG1i/~/diff/~/changes/73/getting-started/environment-variables-configuration).\n\nAlternatively, you can set them as environment variables directly:\n\n- `aios env list`: Show current environment variables, or show available API keys if no variables are set\n- `aios env set`: Show current environment variables, or show available API keys if no variables are set\n- `aios refresh`: Refresh AIOS configuration.\n  Reloads the configuration from aios/config/config.yaml.\n  Reinitializes all components without restarting the server.\n  The server must be running.\n\nWhen no environment variables are set, the following API keys will be shown:\n- `OPENAI_API_KEY`: OpenAI API key for accessing OpenAI services\n- `GEMINI_API_KEY`: Google Gemini API key for accessing Google's Gemini services\n- `GROQ_API_KEY`: Groq API key for accessing Groq services\n- `HF_AUTH_TOKEN`: HuggingFace authentication token for accessing models\n- `HF_HOME`: Optional path to store HuggingFace models\n\n#### Installation from source\nGit clone AIOS kernel\n```bash\ngit clone https://github.com/agiresearch/AIOS.git\ncd AIOS && git checkout v0.2.0.beta\n```\nCreate venv environment (recommended)\n```bash\npython3.x -m venv venv # Only support for Python 3.10 and 3.11\nsource venv/bin/activate\n```\nor create conda environment\n```bash\nconda create -n venv python=3.x  # Only support for Python 3.10 and 3.11\nconda activate venv\n```\n\nIf you have GPU environments, you can install the dependencies using\n```bash\npip install -r requirements-cuda.txt\n```\nor else you can install the dependencies using\n```bash\npip install -r requirements.txt\n```\n\n**Note**: The machine where the AIOS kernel (AIOS) is installed must also have the AIOS SDK (Cerebrum) installed. Installing AIOS kernel will install the AIOS SDK automatically by default. If you are using the Local Kernel mode, i.e., you are running AIOS and agents on the same machine, then simply install both AIOS and Cerebrum on that machine. If you are using Remote Kernel mode, i.e., running AIOS on Machine 1 and running agents on Machine 2 and the agents remotely interact with the kernel, then you need to install both AIOS kernel and AIOS SDK on Machine 1, and install the AIOS SDK alone on Machine 2. Please follow the guidelines at [Cerebrum](https://github.com/agiresearch/Cerebrum) regarding how to install the SDK.\n\n### Quickstart\n\n#### Launch AIOS\nAfter you setup your keys or environment parameters, then you can follow the instructions below to start.\n\nRun:\n\n```\nbash runtime/launch_kernel.sh\n```\n\nOr if you need to explicity set the Python version by running `python3.10`, `python3.11`, `python3`, etc. run the command below:\n\n```\npython3.x -m uvicorn runtime.kernel:app --host 0.0.0.0\n```\n\nYou can also force the kernel to run in the background with:\n```\npython3.x -m uvicorn runtime.kernel:app --host 0.0.0.0 & 2>&1 > MYLOGFILE.txt\n```\n\nAnd you can run it even after the shell closes by typing `nohup` before the entire command.\n\nThen you can start the client provided by the AIOS SDK either in the terminal or in the WebUI. The instructions can be found at [here](https://github.com/agiresearch/Cerebrum)\n\n### Supported Agent Frameworks\n- [OpenAGI](https://github.com/agiresearch/openagi)\n- [AutoGen](https://github.com/microsoft/autogen)\n- [Open-Interpreter](https://github.com/OpenInterpreter/open-interpreter)\n- [MetaGPT](https://github.com/geekan/MetaGPT?tab=readme-ov-file)\n\n### Supported LLM Cores\n| Provider üè¢ | Model Name ü§ñ | Open Source üîì | Model String ‚å®Ô∏è | Backend ‚öôÔ∏è | Required API Key |\n|:------------|:-------------|:---------------|:---------------|:---------------|:----------------|\n| Anthropic | Claude 3.5 Sonnet | ‚ùå | claude-3-5-sonnet-20241022 |anthropic | ANTHROPIC_API_KEY |\n| Anthropic | Claude 3.5 Haiku | ‚ùå | claude-3-5-haiku-20241022 |anthropic | ANTHROPIC_API_KEY |\n| Anthropic | Claude 3 Opus | ‚ùå | claude-3-opus-20240229 |anthropic | ANTHROPIC_API_KEY |\n| Anthropic | Claude 3 Sonnet | ‚ùå | claude-3-sonnet-20240229 |anthropic | ANTHROPIC_API_KEY |\n| Anthropic | Claude 3 Haiku | ‚ùå | claude-3-haiku-20240307 |anthropic | ANTHROPIC_API_KEY |\n| OpenAI | GPT-4 | ‚ùå | gpt-4 |openai| OPENAI_API_KEY |\n| OpenAI | GPT-4 Turbo | ‚ùå | gpt-4-turbo |openai| OPENAI_API_KEY |\n| OpenAI | GPT-4o | ‚ùå | gpt-4o |openai| OPENAI_API_KEY |\n| OpenAI | GPT-4o mini | ‚ùå | gpt-4o-mini |openai| OPENAI_API_KEY |\n| OpenAI | GPT-3.5 Turbo | ‚ùå | gpt-3.5-turbo |openai| OPENAI_API_KEY |\n| Google | Gemini 1.5 Flash | ‚ùå | gemini-1.5-flash |google| GEMINI_API_KEY |\n| Google | Gemini 1.5 Flash-8B | ‚ùå | gemini-1.5-flash-8b |google| GEMINI_API_KEY |\n| Google | Gemini 1.5 Pro | ‚ùå | gemini-1.5-pro |google| GEMINI_API_KEY |\n| Google | Gemini 1.0 Pro | ‚ùå | gemini-1.0-pro |google| GEMINI_API_KEY |\n| Groq | Llama 3.2 90B Vision | ‚úÖ | llama-3.2-90b-vision-preview |groq| GROQ_API_KEY |\n| Groq | Llama 3.2 11B Vision | ‚úÖ | llama-3.2-11b-vision-preview |groq| GROQ_API_KEY |\n| Groq | Llama 3.1 70B | ‚úÖ | llama-3.1-70b-versatile |groq| GROQ_API_KEY |\n| Groq | Llama Guard 3 8B | ‚úÖ | llama-guard-3-8b |groq| GROQ_API_KEY |\n| Groq | Llama 3 70B | ‚úÖ | llama3-70b-8192 |groq| GROQ_API_KEY |\n| Groq | Llama 3 8B | ‚úÖ | llama3-8b-8192 |groq| GROQ_API_KEY |\n| Groq | Mixtral 8x7B | ‚úÖ | mixtral-8x7b-32768 |groq| GROQ_API_KEY |\n| Groq | Gemma 7B | ‚úÖ | gemma-7b-it |groq| GROQ_API_KEY |\n| Groq | Gemma 2B | ‚úÖ | gemma2-9b-it |groq| GROQ_API_KEY |\n| Groq | Llama3 Groq 70B | ‚úÖ | llama3-groq-70b-8192-tool-use-preview |groq| GROQ_API_KEY |\n| Groq | Llama3 Groq 8B | ‚úÖ | llama3-groq-8b-8192-tool-use-preview |groq| GROQ_API_KEY |\n| ollama | [All Models](https://ollama.com/search) | ‚úÖ | model-name |ollama| - |\n| vLLM | [All Models](https://docs.vllm.ai/en/latest/) | ‚úÖ | model-name |vllm| - |\n| HuggingFace | [All Models](https://huggingface.co/models/) | ‚úÖ | model-name |huggingface| HF_HOME |\n\n## üñãÔ∏è References\n```\n@article{mei2024aios,\n  title={AIOS: LLM Agent Operating System},\n  author={Mei, Kai and Li, Zelong and Xu, Shuyuan and Ye, Ruosong and Ge, Yingqiang and Zhang, Yongfeng}\n  journal={arXiv:2403.16971},\n  year={2024}\n}\n@article{ge2023llm,\n  title={LLM as OS, Agents as Apps: Envisioning AIOS, Agents and the AIOS-Agent Ecosystem},\n  author={Ge, Yingqiang and Ren, Yujie and Hua, Wenyue and Xu, Shuyuan and Tan, Juntao and Zhang, Yongfeng},\n  journal={arXiv:2312.03815},\n  year={2023}\n}\n```\n\n## üöÄ Contributions\nFor how to contribute, see [CONTRIBUTE](https://github.com/agiresearch/AIOS/blob/main/docs/CONTRIBUTE.md). If you would like to contribute to the codebase, [issues](https://github.com/agiresearch/AIOS/issues) or [pull requests](https://github.com/agiresearch/AIOS/pulls) are always welcome!\n\n## üåç AIOS Contributors\n[![AIOS contributors](https://contrib.rocks/image?repo=agiresearch/AIOS&max=300)](https://github.com/agiresearch/AIOS/graphs/contributors)\n\n\n## ü§ù Discord Channel\nIf you would like to join the community, ask questions, chat with fellows, learn about or propose new features, and participate in future developments, join our [Discord Community](https://discord.gg/B2HFxEgTJX)!\n"
        },
        {
          "name": "aios",
          "type": "tree",
          "content": null
        },
        {
          "name": "docs",
          "type": "tree",
          "content": null
        },
        {
          "name": "install",
          "type": "tree",
          "content": null
        },
        {
          "name": "requirements-cuda.txt",
          "type": "blob",
          "size": 0.0234375,
          "content": "-r requirements.txt\nvllm"
        },
        {
          "name": "requirements.txt",
          "type": "blob",
          "size": 0.2138671875,
          "content": "litellm\npydantic==2.7.0\nclick==8.1.7\nfastapi\nuvicorn\ntransformers\naccelerate\npython-dotenv\nchromadb\nllama_index==0.10.19\nllama_index_core==0.10.19\nnumpy==1.24.3\nsoundfile\ngit+https://github.com/agiresearch/Cerebrum.git\n"
        },
        {
          "name": "runtime",
          "type": "tree",
          "content": null
        },
        {
          "name": "scripts",
          "type": "tree",
          "content": null
        },
        {
          "name": "tests",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}