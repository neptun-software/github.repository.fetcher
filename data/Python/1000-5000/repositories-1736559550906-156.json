{
  "metadata": {
    "timestamp": 1736559550906,
    "page": 156,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjE2MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "AutoGPTQ/AutoGPTQ",
      "stars": 4613,
      "defaultBranch": "main",
      "files": [
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 3.005859375,
          "content": "# Byte-compiled / optimized / DLL files\n__pycache__/\n*.py[cod]\n*$py.class\n\n# C extensions\n*.so\n\n# Distribution / packaging\n.Python\nbuild/\ndevelop-eggs/\ndist/\ndownloads/\neggs/\n.eggs/\nlib/\nlib64/\nparts/\nsdist/\nvar/\nwheels/\nshare/python-wheels/\n*.egg-info/\n.installed.cfg\n*.egg\nMANIFEST\n\n# PyInstaller\n#  Usually these files are written by a python script from a template\n#  before PyInstaller builds the exe, so as to inject date/other infos into it.\n*.manifest\n*.spec\n\n# Installer logs\npip-log.txt\npip-delete-this-directory.txt\n\n# Unit test / coverage reports\nhtmlcov/\n.tox/\n.nox/\n.coverage\n.coverage.*\n.cache\nnosetests.xml\ncoverage.xml\n*.cover\n*.py,cover\n.hypothesis/\n.pytest_cache/\ncover/\n\n# Translations\n*.mo\n*.pot\n\n# Django stuff:\n*.log\nlocal_settings.py\ndb.sqlite3\ndb.sqlite3-journal\n\n# Flask stuff:\ninstance/\n.webassets-cache\n\n# Scrapy stuff:\n.scrapy\n\n# Sphinx documentation\ndocs/_build/\n\n# PyBuilder\n.pybuilder/\ntarget/\n\n# Jupyter Notebook\n.ipynb_checkpoints\n\n# IPython\nprofile_default/\nipython_config.py\n\n# pyenv\n#   For a library or package, you might want to ignore these files since the code is\n#   intended to run in multiple environments; otherwise, check them in:\n# .python-version\n\n# pipenv\n#   According to pypa/pipenv#598, it is recommended to include Pipfile.lock in version control.\n#   However, in case of collaboration, if having platform-specific dependencies or dependencies\n#   having no cross-platform support, pipenv may install dependencies that don't work, or not\n#   install all needed dependencies.\n#Pipfile.lock\n\n# poetry\n#   Similar to Pipfile.lock, it is generally recommended to include poetry.lock in version control.\n#   This is especially recommended for binary packages to ensure reproducibility, and is more\n#   commonly ignored for libraries.\n#   https://python-poetry.org/docs/basic-usage/#commit-your-poetrylock-file-to-version-control\n#poetry.lock\n\n# pdm\n#   Similar to Pipfile.lock, it is generally recommended to include pdm.lock in version control.\n#pdm.lock\n#   pdm stores project-wide configurations in .pdm.toml, but it is recommended to not include it\n#   in version control.\n#   https://pdm.fming.dev/#use-with-ide\n.pdm.toml\n\n# PEP 582; used by e.g. github.com/David-OConnor/pyflow and github.com/pdm-project/pdm\n__pypackages__/\n\n# Celery stuff\ncelerybeat-schedule\ncelerybeat.pid\n\n# SageMath parsed files\n*.sage.py\n\n# Environments\n.env\n.venv\nenv/\nvenv/\nENV/\nenv.bak/\nvenv.bak/\n\n# Spyder project settings\n.spyderproject\n.spyproject\n\n# Rope project settings\n.ropeproject\n\n# mkdocs documentation\n/site\n\n# mypy\n.mypy_cache/\n.dmypy.json\ndmypy.json\n\n# Pyre type checker\n.pyre/\n\n# pytype static type analyzer\n.pytype/\n\n# Cython debug symbols\ncython_debug/\n\n# PyCharm\n#  JetBrains specific template is maintained in a separate JetBrains.gitignore that can\n#  be found at https://github.com/github/gitignore/blob/main/Global/JetBrains.gitignore\n#  and can be added to the global gitignore or merged into this file.  For a more nuclear\n#  option (not recommended) you can uncomment the following to ignore the entire idea folder.\n#.idea/\n"
        },
        {
          "name": "Dockerfile",
          "type": "blob",
          "size": 0.7783203125,
          "content": "# Build with: `docker build -f Dockerfile -t autogptq .`\n# Run with: `docker run --gpus all --rm -it autogptq:latest /bin/bash`\n\nFROM nvcr.io/nvidia/cuda:12.1.0-runtime-ubuntu22.04\n\nRUN apt update && \\\n    apt install -y wget git && \\\n    apt clean && \\\n    rm -rf /var/lib/apt/lists/* /tmp/* /var/tmp/*\n\nENV PATH=\"/root/miniconda3/bin:${PATH}\"\nARG PATH=\"/root/miniconda3/bin:${PATH}\"\n\nRUN wget \\\n    https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh \\\n    && mkdir .conda \\\n    && bash Miniconda3-latest-Linux-x86_64.sh -b \\\n    && rm -f Miniconda3-latest-Linux-x86_64.sh\n\nRUN conda init bash\n\nRUN pip install --upgrade pip\nRUN pip install --upgrade numpy torch setuptools wheel\n\nRUN git clone https://github.com/AutoGPTQ/AutoGPTQ.git\nWORKDIR /AutoGPTQ\n\nRUN pip install -vvv ."
        },
        {
          "name": "Dockerfile_amd",
          "type": "blob",
          "size": 1.1279296875,
          "content": "# Build with: `docker build -f Dockerfile_amd -t autogptq-rocm .`\n# Run with: `docker run --rm -it --shm-size=150G --device /dev/kfd --device /dev/dri --net host --group-add=video --ipc=host --cap-add=SYS_PTRACE --security-opt seccomp=unconfined autogptq-rocm:latest /bin/bash`\n\nFROM rocm/dev-ubuntu-22.04:5.7\n\nRUN apt update && \\\n    apt install -y wget \\\n    git \\\n    rocsparse-dev \\\n    hipsparse-dev \\\n    rocthrust-dev \\\n    rocblas-dev \\\n    hipblas-dev && \\\n    apt clean && \\\n    rm -rf /var/lib/apt/lists/* /tmp/* /var/tmp/*\n\nENV PATH=\"/root/miniconda3/bin:${PATH}\"\nARG PATH=\"/root/miniconda3/bin:${PATH}\"\n\nRUN wget \\\n    https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh \\\n    && mkdir .conda \\\n    && bash Miniconda3-latest-Linux-x86_64.sh -b \\\n    && rm -f Miniconda3-latest-Linux-x86_64.sh\n\nRUN conda init bash\n\nRUN pip install --upgrade pip\nRUN pip install --upgrade numpy setuptools wheel ninja packaging\nRUN pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/rocm5.7\n\nRUN git clone https://github.com/AutoGPTQ/AutoGPTQ.git\nWORKDIR /AutoGPTQ\n\nRUN ROCM_VERSION=\"5.7\" pip install -vvv ."
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 1.0498046875,
          "content": "MIT License\n\nCopyright (c) 2023 æ½˜å…¶å¨(William)\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n"
        },
        {
          "name": "MANIFEST.in",
          "type": "blob",
          "size": 0.2060546875,
          "content": "global-include autogptq_extension/**/*.cuh\nglobal-include autogptq_extension/**/*.h\nglobal-include autogptq_extension/**/*.cpp\nglobal-include autogptq_extension/**/*.cu\nglobal-include autogptq_extension/**/*.py\n"
        },
        {
          "name": "Makefile",
          "type": "blob",
          "size": 0.0517578125,
          "content": "style:\n\truff auto_gptq examples tests setup.py --fix\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 17.185546875,
          "content": "<h1 align=\"center\">AutoGPTQ</h1>\n<h4 align=\"center\">\n    <p>\n       ğŸš¨ AutoGPTQ development has stopped. Please switch to <a href=\"https://github.com/ModelCloud/GPTQModel\">GPTQModel</a> as drop-in replacement. ğŸš¨\n    </p>\n</h4>\n<p align=\"center\">An easy-to-use LLM quantization package with user-friendly APIs, based on GPTQ algorithm (weight-only quantization).</p>\n<p align=\"center\">\n    <a href=\"https://github.com/PanQiWei/AutoGPTQ/releases\">\n        <img alt=\"GitHub release\" src=\"https://img.shields.io/github/release/PanQiWei/AutoGPTQ.svg\">\n    </a>\n    <a href=\"https://pypi.org/project/auto-gptq/\">\n        <img alt=\"PyPI - Downloads\" src=\"https://img.shields.io/pypi/dd/auto-gptq\">\n    </a>\n</p>\n\n\n## News or Update\n\n- 2024-02-15 - (News) - AutoGPTQ 0.7.0 is released, with [Marlin](https://github.com/IST-DASLab/marlin) int4*fp16 matrix multiplication kernel support, with the argument `use_marlin=True` when loading models.\n- 2023-08-23 - (News) - ğŸ¤— Transformers, optimum and peft have integrated `auto-gptq`, so now running and training GPTQ models can be more available to everyone! See [this blog](https://huggingface.co/blog/gptq-integration) and it's resources for more details!\n\n*For more histories please turn to [here](docs/NEWS_OR_UPDATE.md)*\n\n## Performance Comparison\n\n### Inference Speed\n> The result is generated using [this script](examples/benchmark/generation_speed.py), batch size of input is 1, decode strategy is beam search and enforce the model to generate 512 tokens, speed metric is tokens/s (the larger, the better).\n>\n> The quantized model is loaded using the setup that can gain the fastest inference speed.\n\n| model         | GPU           | num_beams | fp16  | gptq-int4 |\n|---------------|---------------|-----------|-------|-----------|\n| llama-7b      | 1xA100-40G    | 1         | 18.87 | 25.53     |\n| llama-7b      | 1xA100-40G    | 4         | 68.79 | 91.30     |\n| moss-moon 16b | 1xA100-40G    | 1         | 12.48 | 15.25     |\n| moss-moon 16b | 1xA100-40G    | 4         | OOM   | 42.67     |\n| moss-moon 16b | 2xA100-40G    | 1         | 06.83 | 06.78     |\n| moss-moon 16b | 2xA100-40G    | 4         | 13.10 | 10.80     |\n| gpt-j 6b      | 1xRTX3060-12G | 1         | OOM   | 29.55     |\n| gpt-j 6b      | 1xRTX3060-12G | 4         | OOM   | 47.36     |\n\n\n### Perplexity\nFor perplexity comparison, you can turn to [here](https://github.com/qwopqwop200/GPTQ-for-LLaMa#result) and [here](https://github.com/qwopqwop200/GPTQ-for-LLaMa#gptq-vs-bitsandbytes)\n\n## Installation\n\nAutoGPTQ is available on Linux and Windows only. You can install the latest stable release of AutoGPTQ from pip with pre-built wheels:\n\n| Platform version | Installation                                                                                      | Built against PyTorch |\n|-------------------|---------------------------------------------------------------------------------------------------|-----------------------|\n| CUDA 11.8         | `pip install auto-gptq --no-build-isolation --extra-index-url https://huggingface.github.io/autogptq-index/whl/cu118/`   | 2.2.1+cu118           |\n| CUDA 12.1         | `pip install auto-gptq --no-build-isolation`                                                                            | 2.2.1+cu121           |\n| ROCm 5.7          | `pip install auto-gptq --no-build-isolation --extra-index-url https://huggingface.github.io/autogptq-index/whl/rocm573/` | 2.2.1+rocm5.7\n\nAutoGPTQ can be installed with the Triton dependency with `pip install auto-gptq[triton] --no-build-isolation` in order to be able to use the Triton backend (currently only supports linux, no 3-bits quantization).\n\nFor older AutoGPTQ, please refer to [the previous releases installation table](docs/INSTALLATION.md).\n\nOn NVIDIA systems, AutoGPTQ does not support [Maxwell or lower](https://qiita.com/uyuni/items/733a93b975b524f89f46) GPUs.\n\n### Install from source\n\nClone the source code:\n```bash\ngit clone https://github.com/PanQiWei/AutoGPTQ.git && cd AutoGPTQ\n```\n\nA few packages are required in order to build from source: `pip install numpy gekko pandas`.\n\nThen, install locally from source:\n```bash\npip install -vvv --no-build-isolation -e .\n```\nYou can set `BUILD_CUDA_EXT=0` to disable pytorch extension building, but this is **strongly discouraged** as AutoGPTQ then falls back on a slow python implementation.\n\nAs a last resort, if the above command fails, you can try `python setup.py install`.\n\n#### On ROCm systems\n\nTo install from source for AMD GPUs supporting ROCm, please specify the `ROCM_VERSION` environment variable. Example:\n\n```bash\nROCM_VERSION=5.6 pip install -vvv --no-build-isolation -e .\n```\n\nThe compilation can be speeded up by specifying the `PYTORCH_ROCM_ARCH` variable ([reference](https://github.com/pytorch/pytorch/blob/7b73b1e8a73a1777ebe8d2cd4487eb13da55b3ba/setup.py#L132)) in order to build for a single target device, for example `gfx90a` for MI200 series devices.\n\nFor ROCm systems, the packages `rocsparse-dev`, `hipsparse-dev`, `rocthrust-dev`, `rocblas-dev` and `hipblas-dev` are required to build.\n\n#### On IntelÂ® GaudiÂ® 2 systems\n\n>Notice: make sure you're in commit 65c2e15 or later\n\nTo install from source for Intel Gaudi 2 HPUs, set the `BUILD_CUDA_EXT=0` environment variable to disable building the CUDA PyTorch extension. Example:\n\n```bash\nBUILD_CUDA_EXT=0 pip install -vvv --no-build-isolation -e .\n```\n\n>Notice that Intel Gaudi 2 uses an optimized kernel upon inference, and requires `BUILD_CUDA_EXT=0` on non-CUDA machines.\n\n## Quick Tour\n\n### Quantization and Inference\n> warning: this is just a showcase of the usage of basic apis in AutoGPTQ, which uses only one sample to quantize a much small model, quality of quantized model using such little samples may not good.\n\nBelow is an example for the simplest use of `auto_gptq` to quantize a model and inference after quantization:\n```python\nfrom transformers import AutoTokenizer, TextGenerationPipeline\nfrom auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig\nimport logging\n\nlogging.basicConfig(\n    format=\"%(asctime)s %(levelname)s [%(name)s] %(message)s\", level=logging.INFO, datefmt=\"%Y-%m-%d %H:%M:%S\"\n)\n\npretrained_model_dir = \"facebook/opt-125m\"\nquantized_model_dir = \"opt-125m-4bit\"\n\ntokenizer = AutoTokenizer.from_pretrained(pretrained_model_dir, use_fast=True)\nexamples = [\n    tokenizer(\n        \"auto-gptq is an easy-to-use model quantization library with user-friendly apis, based on GPTQ algorithm.\"\n    )\n]\n\nquantize_config = BaseQuantizeConfig(\n    bits=4,  # quantize model to 4-bit\n    group_size=128,  # it is recommended to set the value to 128\n    desc_act=False,  # set to False can significantly speed up inference but the perplexity may slightly bad\n)\n\n# load un-quantized model, by default, the model will always be loaded into CPU memory\nmodel = AutoGPTQForCausalLM.from_pretrained(pretrained_model_dir, quantize_config)\n\n# quantize model, the examples should be list of dict whose keys can only be \"input_ids\" and \"attention_mask\"\nmodel.quantize(examples)\n\n# save quantized model\nmodel.save_quantized(quantized_model_dir)\n\n# save quantized model using safetensors\nmodel.save_quantized(quantized_model_dir, use_safetensors=True)\n\n# push quantized model to Hugging Face Hub.\n# to use use_auth_token=True, Login first via huggingface-cli login.\n# or pass explcit token with: use_auth_token=\"hf_xxxxxxx\"\n# (uncomment the following three lines to enable this feature)\n# repo_id = f\"YourUserName/{quantized_model_dir}\"\n# commit_message = f\"AutoGPTQ model for {pretrained_model_dir}: {quantize_config.bits}bits, gr{quantize_config.group_size}, desc_act={quantize_config.desc_act}\"\n# model.push_to_hub(repo_id, commit_message=commit_message, use_auth_token=True)\n\n# alternatively you can save and push at the same time\n# (uncomment the following three lines to enable this feature)\n# repo_id = f\"YourUserName/{quantized_model_dir}\"\n# commit_message = f\"AutoGPTQ model for {pretrained_model_dir}: {quantize_config.bits}bits, gr{quantize_config.group_size}, desc_act={quantize_config.desc_act}\"\n# model.push_to_hub(repo_id, save_dir=quantized_model_dir, use_safetensors=True, commit_message=commit_message, use_auth_token=True)\n\n# load quantized model to the first GPU\nmodel = AutoGPTQForCausalLM.from_quantized(quantized_model_dir, device=\"cuda:0\")\n\n# download quantized model from Hugging Face Hub and load to the first GPU\n# model = AutoGPTQForCausalLM.from_quantized(repo_id, device=\"cuda:0\", use_safetensors=True, use_triton=False)\n\n# inference with model.generate\nprint(tokenizer.decode(model.generate(**tokenizer(\"auto_gptq is\", return_tensors=\"pt\").to(model.device))[0]))\n\n# or you can also use pipeline\npipeline = TextGenerationPipeline(model=model, tokenizer=tokenizer)\nprint(pipeline(\"auto-gptq is\")[0][\"generated_text\"])\n```\n\nFor more advanced features of model quantization, please reference to [this script](examples/quantization/quant_with_alpaca.py)\n\n### Customize Model\n<details>\n\n<summary>Below is an example to extend `auto_gptq` to support `OPT` model, as you will see, it's very easy:</summary>\n\n```python\nfrom auto_gptq.modeling import BaseGPTQForCausalLM\n\n\nclass OPTGPTQForCausalLM(BaseGPTQForCausalLM):\n    # chained attribute name of transformer layer block\n    layers_block_name = \"model.decoder.layers\"\n    # chained attribute names of other nn modules that in the same level as the transformer layer block\n    outside_layer_modules = [\n        \"model.decoder.embed_tokens\", \"model.decoder.embed_positions\", \"model.decoder.project_out\",\n        \"model.decoder.project_in\", \"model.decoder.final_layer_norm\"\n    ]\n    # chained attribute names of linear layers in transformer layer module\n    # normally, there are four sub lists, for each one the modules in it can be seen as one operation,\n    # and the order should be the order when they are truly executed, in this case (and usually in most cases),\n    # they are: attention q_k_v projection, attention output projection, MLP project input, MLP project output\n    inside_layer_modules = [\n        [\"self_attn.k_proj\", \"self_attn.v_proj\", \"self_attn.q_proj\"],\n        [\"self_attn.out_proj\"],\n        [\"fc1\"],\n        [\"fc2\"]\n    ]\n```\nAfter this, you can use `OPTGPTQForCausalLM.from_pretrained` and other methods as shown in Basic.\n\n</details>\n\n### Evaluation on Downstream Tasks\nYou can use tasks defined in `auto_gptq.eval_tasks` to evaluate model's performance on specific down-stream task before and after quantization.\n\nThe predefined tasks support all causal-language-models implemented in [ğŸ¤— transformers](https://github.com/huggingface/transformers) and in this project.\n\n<details>\n\n<summary>Below is an example to evaluate `EleutherAI/gpt-j-6b` on sequence-classification task using `cardiffnlp/tweet_sentiment_multilingual` dataset:</summary>\n\n```python\nfrom functools import partial\n\nimport datasets\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig\n\nfrom auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig\nfrom auto_gptq.eval_tasks import SequenceClassificationTask\n\n\nMODEL = \"EleutherAI/gpt-j-6b\"\nDATASET = \"cardiffnlp/tweet_sentiment_multilingual\"\nTEMPLATE = \"Question:What's the sentiment of the given text? Choices are {labels}.\\nText: {text}\\nAnswer:\"\nID2LABEL = {\n    0: \"negative\",\n    1: \"neutral\",\n    2: \"positive\"\n}\nLABELS = list(ID2LABEL.values())\n\n\ndef ds_refactor_fn(samples):\n    text_data = samples[\"text\"]\n    label_data = samples[\"label\"]\n\n    new_samples = {\"prompt\": [], \"label\": []}\n    for text, label in zip(text_data, label_data):\n        prompt = TEMPLATE.format(labels=LABELS, text=text)\n        new_samples[\"prompt\"].append(prompt)\n        new_samples[\"label\"].append(ID2LABEL[label])\n\n    return new_samples\n\n\n#  model = AutoModelForCausalLM.from_pretrained(MODEL).eval().half().to(\"cuda:0\")\nmodel = AutoGPTQForCausalLM.from_pretrained(MODEL, BaseQuantizeConfig())\ntokenizer = AutoTokenizer.from_pretrained(MODEL)\n\ntask = SequenceClassificationTask(\n        model=model,\n        tokenizer=tokenizer,\n        classes=LABELS,\n        data_name_or_path=DATASET,\n        prompt_col_name=\"prompt\",\n        label_col_name=\"label\",\n        **{\n            \"num_samples\": 1000,  # how many samples will be sampled to evaluation\n            \"sample_max_len\": 1024,  # max tokens for each sample\n            \"block_max_len\": 2048,  # max tokens for each data block\n            # function to load dataset, one must only accept data_name_or_path as input\n            # and return datasets.Dataset\n            \"load_fn\": partial(datasets.load_dataset, name=\"english\"),\n            # function to preprocess dataset, which is used for datasets.Dataset.map,\n            # must return Dict[str, list] with only two keys: [prompt_col_name, label_col_name]\n            \"preprocess_fn\": ds_refactor_fn,\n            # truncate label when sample's length exceed sample_max_len\n            \"truncate_prompt\": False\n        }\n    )\n\n# note that max_new_tokens will be automatically specified internally based on given classes\nprint(task.run())\n\n# self-consistency\nprint(\n    task.run(\n        generation_config=GenerationConfig(\n            num_beams=3,\n            num_return_sequences=3,\n            do_sample=True\n        )\n    )\n)\n```\n\n</details>\n\n## Learn More\n[tutorials](docs/tutorial) provide step-by-step guidance to integrate `auto_gptq` with your own project and some best practice principles.\n\n[examples](examples/README.md) provide plenty of example scripts to use `auto_gptq` in different ways.\n\n## Supported Models\n\n> you can use `model.config.model_type` to compare with the table below to check whether the model you use is supported by `auto_gptq`.\n>\n> for example, model_type of `WizardLM`, `vicuna` and `gpt4all` are all `llama`, hence they are all supported by `auto_gptq`.\n\n| model type                         | quantization | inference | peft-lora | peft-ada-lora | peft-adaption_prompt                                                                            |\n|------------------------------------|--------------|-----------|-----------|---------------|-------------------------------------------------------------------------------------------------|\n| bloom                              | âœ…            | âœ…         | âœ…         | âœ…             |                                                                                                 |\n| gpt2                               | âœ…            | âœ…         | âœ…         | âœ…             |                                                                                                 |\n| gpt_neox                           | âœ…            | âœ…         | âœ…         | âœ…             | âœ…[requires this peft branch](https://github.com/PanQiWei/peft/tree/multi_modal_adaption_prompt) |\n| gptj                               | âœ…            | âœ…         | âœ…         | âœ…             | âœ…[requires this peft branch](https://github.com/PanQiWei/peft/tree/multi_modal_adaption_prompt) |\n| llama                              | âœ…            | âœ…         | âœ…         | âœ…             | âœ…                                                                                               |\n| moss                               | âœ…            | âœ…         | âœ…         | âœ…             | âœ…[requires this peft branch](https://github.com/PanQiWei/peft/tree/multi_modal_adaption_prompt) |\n| opt                                | âœ…            | âœ…         | âœ…         | âœ…             |                                                                                                 |\n| gpt_bigcode                        | âœ…            | âœ…         | âœ…         | âœ…             |                                                                                                 |\n| codegen                            | âœ…            | âœ…         | âœ…         | âœ…             |                                                                                                 |\n| falcon(RefinedWebModel/RefinedWeb) | âœ…            | âœ…         | âœ…         | âœ…             |                                                                                                 |\n\n## Supported Evaluation Tasks\nCurrently, `auto_gptq` supports: `LanguageModelingTask`, `SequenceClassificationTask` and `TextSummarizationTask`; more Tasks will come soon!\n\n## Running tests\n\nTests can be run with:\n\n```\npytest tests/ -s\n```\n\n## FAQ\n\n### Which kernel is used by default?\n\nAutoGPTQ defaults to using exllamav2 int4*fp16 kernel for matrix multiplication.\n\n### How to use Marlin kernel?\n\nMarlin is an optimized int4 * fp16 kernel was recently proposed at https://github.com/IST-DASLab/marlin. This is integrated in AutoGPTQ when loading a model with `use_marlin=True`. This kernel is available only on devices with compute capability 8.0 or 8.6 (Ampere GPUs).\n\n## Acknowledgement\n- Special thanks **Elias Frantar**, **Saleh Ashkboos**, **Torsten Hoefler** and **Dan Alistarh** for proposing **GPTQ** algorithm and open source the [code](https://github.com/IST-DASLab/gptq), and for releasing [Marlin kernel](https://github.com/IST-DASLab/marlin) for mixed precision computation.\n- Special thanks **qwopqwop200**, for code in this project that relevant to quantization are mainly referenced from [GPTQ-for-LLaMa](https://github.com/qwopqwop200/GPTQ-for-LLaMa/tree/cuda).\n- Special thanks to **turboderp**, for releasing [Exllama](https://github.com/turboderp/exllama) and [Exllama v2](https://github.com/turboderp/exllamav2) libraries with efficient mixed precision kernels.\n"
        },
        {
          "name": "README_zh.md",
          "type": "blob",
          "size": 18.5341796875,
          "content": "<h1 align=\"center\">AutoGPTQ</h1>\n<p align=\"center\">ä¸€ä¸ªåŸºäº GPTQ ç®—æ³•ï¼Œç®€å•æ˜“ç”¨ä¸”æ‹¥æœ‰ç”¨æˆ·å‹å¥½å‹æ¥å£çš„å¤§è¯­è¨€æ¨¡å‹é‡åŒ–å·¥å…·åŒ…ã€‚</p>\n<p align=\"center\">\n    <a href=\"https://github.com/PanQiWei/AutoGPTQ/releases\">\n        <img alt=\"GitHub release\" src=\"https://img.shields.io/github/release/PanQiWei/AutoGPTQ.svg\">\n    </a>\n    <a href=\"https://pypi.org/project/auto-gptq/\">\n        <img alt=\"PyPI - Downloads\" src=\"https://img.shields.io/pypi/dd/auto-gptq\">\n    </a>\n</p>\n<h4 align=\"center\">\n    <p>\n        <a href=\"https://github.com/PanQiWei/AutoGPTQ/blob/main/README.md\">English</a> |\n        <b>ä¸­æ–‡</b>\n    </p>\n</h4>\n\nNote: The English README is likely to be more up to date.\n\n## é€šå‘ v1.0.0 ä¹‹è·¯\n\nå—¨ï¼Œç¤¾åŒºçš„ä¼™ä¼´ä»¬ï¼Œå¥½ä¹…ä¸è§ï¼å¾ˆæŠ±æ­‰è¿™æ®µæ—¶é—´ç”±äºä¸ªäººåŸå› ï¼Œæˆ‘æ²¡èƒ½ä»¥è¾ƒé«˜çš„é¢‘ç‡æ¥æ›´æ–°è¿™ä¸ªé¡¹ç›®ã€‚è¿‡å»å‡ å‘¨å¯¹æˆ‘çš„èŒä¸šç”Ÿæ¶¯è§„åˆ’è€Œè¨€æ„ä¹‰é‡å¤§ã€‚åœ¨ä¸ä¹…å‰ï¼Œæˆ‘æ­£å¼å‘Šåˆ«äº†æ¯•ä¸šåä¾¿åŠ å…¥ä¸¤å¹´ä¹‹ä¹…çš„åˆ›ä¸šå›¢é˜Ÿï¼Œéå¸¸æ„Ÿè°¢å›¢é˜Ÿçš„é¢†å¯¼å’ŒåŒäº‹ä»¬ç»™äºˆæˆ‘çš„ä¿¡ä»»ä¸æŒ‡å¯¼ï¼Œè®©æˆ‘èƒ½å¤Ÿåœ¨ä¸¤å¹´æ—¶é—´é‡Œé£é€Ÿåœ°æˆé•¿ï¼›åŒæ—¶ä¹Ÿååˆ†æ„Ÿæ¿€å›¢é˜Ÿå…è®¸æˆ‘è‡ª AutoGPTQ é¡¹ç›®åˆ›ç«‹ä»¥æ¥ä¸€ç›´æ— å¿ä½¿ç”¨å†…éƒ¨çš„ A100 GPU æœåŠ¡å™¨é›†ç¾¤ä»¥å®Œæˆå„é¡¹å®éªŒä¸æ€§èƒ½æµ‹è¯„ã€‚ï¼ˆå½“ç„¶ä»Šåæ˜¯æ— æ³•ç»§ç»­ä½¿ç”¨äº†ï¼Œå› æ­¤**è‹¥æœ‰æ–°çš„ç¡¬ä»¶èµåŠ©æˆ‘å°†æ„Ÿæ¿€ä¸å°½**ï¼ï¼‰è¿‡å»çš„ä¸¤å¹´é‡Œï¼Œæˆ‘åœ¨è¿™ä¸ªå›¢é˜Ÿä¸­æ‹…ä»»ç®—æ³•å·¥ç¨‹å¸ˆçš„è§’è‰²ï¼Œè´Ÿè´£åŸºäºå¤§è¯­è¨€æ¨¡å‹çš„å¯¹è¯ç³»ç»Ÿæ¶æ„è®¾è®¡ä¸å¼€å‘ï¼Œæˆ‘ä»¬æ›¾æˆåŠŸæ¨å‡ºä¸€æ¬¾åä¸º gemsouls çš„äº§å“ï¼Œä½†ä¸å¹¸çš„æ˜¯å®ƒå·²ç»åœæ­¢è¿è¥ã€‚è€Œç°åœ¨ï¼Œè¿™ä¸ªå›¢é˜Ÿå³å°†æ¨å‡ºä¸€æ¬¾åä¸º [modelize](https://www.beta.modelize.ai/) çš„æ–°äº§å“ï¼Œ**è¿™æ˜¯ä¸€ä¸ªå¤§æ¨¡å‹åŸç”Ÿçš„ AI æ™ºèƒ½ä½“å¹³å°ï¼Œç”¨æˆ·å¯ä»¥ä½¿ç”¨å¤šä¸ª AI æ™ºèƒ½ä½“æ­å»ºä¸€ä¸ªé«˜åº¦è‡ªåŠ¨åŒ–çš„å›¢é˜Ÿï¼Œè®©å®ƒä»¬åœ¨å·¥ä½œæµä¸­ç›¸äº’åˆä½œï¼Œé«˜æ•ˆå®Œæˆå¤æ‚çš„é¡¹ç›®ã€‚**\n\nè¯å½’æ­£é¢˜ï¼Œæˆ‘éå¸¸å…´å¥‹åœ°çœ‹åˆ°ï¼Œåœ¨è¿‡å»å‡ ä¸ªæœˆçš„æ—¶é—´é‡Œï¼Œé’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹æ¨ç†æ€§èƒ½ä¼˜åŒ–çš„ç ”ç©¶å–å¾—äº†å·¨å¤§çš„è¿›å±•ï¼Œå¦‚ä»Šæˆ‘ä»¬ä¸ä»…èƒ½å¤Ÿåœ¨é«˜ç«¯æ˜¾å¡ä¸Šå®Œæˆå¤§è¯­è¨€æ¨¡å‹çš„æ¨ç†ï¼Œç”šè‡³åœ¨ CPU å’Œè¾¹ç¼˜è®¾å¤‡ä¸Šéƒ½å¯ä»¥è½»æ¾è¿è¡Œå¤§è¯­è¨€æ¨¡å‹ã€‚ä¸€ç³»åˆ—çš„æŠ€æœ¯è¿›æ­¥ï¼Œè®©æˆ‘åŒæ ·è¿«ä¸åŠå¾…åœ°åœ¨å¼€æºç¤¾åŒºä¸Šåšå‡ºæ›´å¤šçš„è´¡çŒ®ï¼Œå› æ­¤ï¼Œé¦–å…ˆï¼Œæˆ‘å°†ç”¨çº¦å››å‘¨çš„æ—¶é—´å°† AutoGPTQ è¿­ä»£è‡³ v1.0.0 æ­£å¼ç‰ˆæœ¬ï¼Œåœ¨æ­¤æœŸé—´ï¼Œä¹Ÿä¼šæœ‰ 2~3 ä¸ªå°ç‰ˆæœ¬å‘å¸ƒä»¥è®©ç”¨æˆ·èƒ½å¤ŸåŠæ—¶ä½“éªŒæ€§èƒ½ä¼˜åŒ–å’Œæ–°ç‰¹æ€§ã€‚åœ¨æˆ‘çš„æ„¿æ™¯é‡Œï¼Œ**åˆ° v1.0.0 ç‰ˆæœ¬æ­£å¼å‘å¸ƒæ—¶ï¼ŒAutoGPTQ å°†èƒ½å¤Ÿä½œä¸ºä¸€ä¸ªçµæ´»å¯æ‹“å±•çš„ã€æ”¯æŒæ‰€æœ‰ GPTQ-like æ–¹æ³•çš„é‡åŒ–åç«¯ï¼Œè‡ªåŠ¨åœ°å®Œæˆå„ç§åŸºäº Pytorch ç¼–å†™çš„å¤§è¯­è¨€æ¨¡å‹çš„é‡åŒ–å·¥ä½œ**ã€‚æˆ‘åœ¨[è¿™é‡Œ](https://github.com/PanQiWei/AutoGPTQ/issues/348)è¯¦ç»†ä»‹ç»äº†å¼€å‘è®¡åˆ’ï¼Œæ¬¢è¿ç§»æ­¥è‡³æ­¤è¿›è¡Œè®¨è®ºå¹¶ç»™å‡ºä½ ä»¬çš„å»ºè®®ï¼\n\n## æ–°é—»æˆ–æ›´æ–°\n\n- 2023-08-23 - (æ–°é—») - ğŸ¤— Transformersã€optimum å’Œ peft å®Œæˆäº†å¯¹ `auto-gptq` çš„é›†æˆï¼Œç°åœ¨ä½¿ç”¨ GPTQ æ¨¡å‹è¿›è¡Œæ¨ç†å’Œè®­ç»ƒå°†å˜å¾—æ›´å®¹æ˜“ï¼é˜…è¯» [è¿™ç¯‡åšå®¢](https://huggingface.co/blog/gptq-integration) å’Œç›¸å…³èµ„æºä»¥äº†è§£æ›´å¤šç»†èŠ‚ï¼\n- 2023-08-21 - (æ–°é—») - é€šä¹‰åƒé—®å›¢é˜Ÿå‘å¸ƒäº†åŸºäº `auto-gptq` çš„ Qwen-7B 4bit é‡åŒ–ç‰ˆæœ¬æ¨¡å‹ï¼Œå¹¶æä¾›äº†[è¯¦å°½çš„æµ‹è¯„ç»“æœ](https://huggingface.co/Qwen/Qwen-7B-Chat-Int4#%E9%87%8F%E5%8C%96-quantization)\n- 2023-08-06 - (æ›´æ–°) - æ”¯æŒ exllama çš„ q4 CUDA ç®—å­ä½¿å¾— int4 é‡åŒ–æ¨¡å‹èƒ½å¤Ÿè·å¾—è‡³å°‘1.3å€çš„æ¨ç†é€Ÿåº¦æå‡.\n- 2023-08-04 - (æ›´æ–°) - æ”¯æŒ RoCm ä½¿å¾— AMD GPU çš„ç”¨æˆ·èƒ½å¤Ÿä½¿ç”¨ auto-gptq çš„ CUDA æ‹“å±•.\n- 2023-07-26 - (æ›´æ–°) - ä¸€ä¸ªä¼˜é›…çš„ [PPL æµ‹è¯„è„šæœ¬](examples/benchmark/perplexity.py)ä»¥è·å¾—å¯ä»¥ä¸è¯¸å¦‚ `llama.cpp` ç­‰ä»£ç åº“è¿›è¡Œå…¬å¹³æ¯”è¾ƒçš„ç»“æœã€‚\n- 2023-06-05 - (æ›´æ–°) - é›†æˆ ğŸ¤— peft æ¥ä½¿ç”¨ gptq é‡åŒ–è¿‡çš„æ¨¡å‹è®­ç»ƒé€‚åº”å±‚ï¼Œæ”¯æŒ LoRAï¼ŒAdaLoRAï¼ŒAdaptionPrompt ç­‰ã€‚\n- 2023-05-30 - (æ›´æ–°) - æ”¯æŒä» ğŸ¤— Hub ä¸‹è½½é‡åŒ–å¥½çš„æ¨¡å‹æˆ–ä¸Šæ¬¡é‡åŒ–å¥½çš„æ¨¡å‹åˆ° ğŸ¤— Hubã€‚\n\n*è·å–æ›´å¤šçš„å†å²ä¿¡æ¯ï¼Œè¯·è½¬è‡³[è¿™é‡Œ](docs/NEWS_OR_UPDATE.md)*\n\n## æ€§èƒ½å¯¹æ¯”\n\n### æ¨ç†é€Ÿåº¦\n> ä»¥ä¸‹ç»“æœé€šè¿‡[è¿™ä¸ªè„šæœ¬](examples/benchmark/generation_speed.py)ç”Ÿæˆï¼Œæ–‡æœ¬è¾“å…¥çš„ batch size ä¸º1ï¼Œè§£ç ç­–ç•¥ä¸º beam search å¹¶ä¸”å¼ºåˆ¶æ¨¡å‹ç”Ÿæˆ512ä¸ª tokenï¼Œé€Ÿåº¦çš„è®¡é‡å•ä½ä¸º tokens/sï¼ˆè¶Šå¤§è¶Šå¥½ï¼‰ã€‚\n> \n> é‡åŒ–æ¨¡å‹é€šè¿‡èƒ½å¤Ÿæœ€å¤§åŒ–æ¨ç†é€Ÿåº¦çš„æ–¹å¼åŠ è½½ã€‚\n\n| model         | GPU           | num_beams | fp16  | gptq-int4 |\n|---------------|---------------|-----------|-------|-----------|\n| llama-7b      | 1xA100-40G    | 1         | 18.87 | 25.53     |\n| llama-7b      | 1xA100-40G    | 4         | 68.79 | 91.30     |\n| moss-moon 16b | 1xA100-40G    | 1         | 12.48 | 15.25     |\n| moss-moon 16b | 1xA100-40G    | 4         | OOM   | 42.67     |\n| moss-moon 16b | 2xA100-40G    | 1         | 06.83 | 06.78     |\n| moss-moon 16b | 2xA100-40G    | 4         | 13.10 | 10.80     |\n| gpt-j 6b      | 1xRTX3060-12G | 1         | OOM   | 29.55     |\n| gpt-j 6b      | 1xRTX3060-12G | 4         | OOM   | 47.36     |\n\n\n### å›°æƒ‘åº¦ï¼ˆPPLï¼‰\nå¯¹äºå›°æƒ‘åº¦çš„å¯¹æ¯”ï¼Œ ä½ å¯ä»¥å‚è€ƒ [è¿™é‡Œ](https://github.com/qwopqwop200/GPTQ-for-LLaMa#result) å’Œ [è¿™é‡Œ](https://github.com/qwopqwop200/GPTQ-for-LLaMa#gptq-vs-bitsandbytes)\n\n## å®‰è£…\n\n### å¿«é€Ÿå®‰è£…\nä½ å¯ä»¥é€šè¿‡ pip æ¥å®‰è£…ä¸ PyTorch 2.0.1 ç›¸å…¼å®¹çš„æœ€æ–°ç¨³å®šç‰ˆæœ¬çš„ AutoGPTQ çš„é¢„æ„å»ºè½®å­æ–‡ä»¶ï¼š\n\n* å¯¹äº CUDA 11.7ï¼š `pip install auto-gptq --extra-index-url https://huggingface.github.io/autogptq-index/whl/cu117/`\n* å¯¹äº CUDA 11.8ï¼š `pip install auto-gptq --extra-index-url https://huggingface.github.io/autogptq-index/whl/cu118/`\n* å¯¹äº RoCm 5.4.2ï¼š `pip install auto-gptq --extra-index-url https://huggingface.github.io/autogptq-index/whl/rocm542/`\n\n**è­¦å‘Šï¼š** é¢„æ„å»ºçš„è½®å­æ–‡ä»¶ä¸ä¸€å®šåœ¨ PyTorch çš„ nightly ç‰ˆæœ¬ä¸Šæœ‰æ•ˆã€‚å¦‚æœè¦ä½¿ç”¨ PyTorch çš„ nightly ç‰ˆæœ¬ï¼Œè¯·ä»æºç å®‰è£… AutoGPTQã€‚\n\n#### å–æ¶ˆ cuda æ‹“å±•çš„å®‰è£…\né»˜è®¤æƒ…å†µä¸‹ï¼Œåœ¨ `torch` å’Œ `cuda` å·²ç»äºä½ çš„æœºå™¨ä¸Šè¢«å®‰è£…æ—¶ï¼Œcuda æ‹“å±•å°†è¢«è‡ªåŠ¨å®‰è£…ï¼Œå¦‚æœä½ ä¸æƒ³è¦è¿™äº›æ‹“å±•çš„è¯ï¼Œé‡‡ç”¨ä»¥ä¸‹å®‰è£…å‘½ä»¤ï¼š\n```shell\nBUILD_CUDA_EXT=0 pip install auto-gptq\n```\nåŒæ—¶ä¸ºç¡®ä¿è¯¥æ‹“å±•â€”â€”`autogptq_cuda` ä¸å†å­˜åœ¨äºä½ çš„è™šæ‹Ÿç¯å¢ƒï¼Œæ‰§è¡Œä»¥ä¸‹å‘½ä»¤ï¼š\n```shell\npip uninstall autogptq_cuda -y\n```\n\n#### æ”¯æŒä½¿ç”¨ triton åŠ é€Ÿ\nè‹¥æƒ³ä½¿ç”¨ `triton` åŠ é€Ÿæ¨¡å‹æ¨ç†ï¼Œä½¿ç”¨ä»¥ä¸‹å‘½ä»¤ï¼š\n> è­¦å‘Šï¼šç›®å‰ triton ä»…æ”¯æŒ linux æ“ä½œç³»ç»Ÿï¼›å½“ä½¿ç”¨ triton æ—¶ 3-bit æ•°å€¼ç±»å‹çš„é‡åŒ–å°†ä¸è¢«æ”¯æŒ\n\n```shell\npip install auto-gptq[triton]\n```\n\n### ä»æºç å®‰è£…\n<details>\n<summary>ç‚¹å‡»ä»¥æŸ¥çœ‹è¯¦æƒ…</summary>\n\nå…‹éš†æºç :\n```shell\ngit clone https://github.com/PanQiWei/AutoGPTQ.git && cd AutoGPTQ\n```\nç„¶åï¼Œä»é¡¹ç›®ç›®å½•å®‰è£…:\n```shell\npip install .\n```\næ­£å¦‚åœ¨å¿«é€Ÿå®‰è£…ä¸€èŠ‚ï¼Œä½ å¯ä»¥ä½¿ç”¨ `BUILD_CUDA_EXT=0` æ¥å–æ¶ˆæ„å»º cuda æ‹“å±•ã€‚\n\nå¦‚æœä½ æƒ³è¦ä½¿ç”¨ triton åŠ é€Ÿä¸”å…¶èƒ½å¤Ÿè¢«ä½ çš„æ“ä½œç³»ç»Ÿæ‰€æ”¯æŒï¼Œè¯·ä½¿ç”¨ `.[triton]`ã€‚\n\nå¯¹åº” AMD GPUsï¼Œä¸ºäº†ä»æºç å®‰è£…ä»¥æ”¯æŒ RoCmï¼Œè¯·è®¾ç½® `ROCM_VERSION` ç¯å¢ƒå˜é‡ã€‚åŒæ—¶é€šè¿‡è®¾ç½® `PYTORCH_ROCM_ARCH` ([reference](https://github.com/pytorch/pytorch/blob/7b73b1e8a73a1777ebe8d2cd4487eb13da55b3ba/setup.py#L132)) å¯æå‡ç¼–è¯‘é€Ÿåº¦ï¼Œä¾‹å¦‚ï¼šå¯¹äº MI200 ç³»åˆ—è®¾å¤‡ï¼Œè¯¥å˜é‡å¯è®¾ä¸º `gfx90a`ã€‚ä¾‹å­ï¼š\n\n```\nROCM_VERSION=5.6 pip install .\n```\n\nå¯¹äº RoCm ç³»ç»Ÿï¼Œåœ¨ä»æºç å®‰è£…æ—¶é¢å¤–éœ€è¦æå‰å®‰è£…ä»¥ä¸‹åŒ…ï¼š`rocsparse-dev`, `hipsparse-dev`, `rocthrust-dev`, `rocblas-dev` and `hipblas-dev`ã€‚\n\n</details>\n\n## å¿«é€Ÿå¼€å§‹\n\n### é‡åŒ–å’Œæ¨ç†\n> è­¦å‘Šï¼šè¿™é‡Œä»…æ˜¯å¯¹ AutoGPTQ ä¸­åŸºæœ¬æ¥å£çš„ç”¨æ³•å±•ç¤ºï¼Œåªä½¿ç”¨äº†ä¸€æ¡æ–‡æœ¬æ¥é‡åŒ–ä¸€ä¸ªç‰¹åˆ«å°çš„æ¨¡å‹ï¼Œå› æ­¤å…¶ç»“æœçš„è¡¨ç°å¯èƒ½ä¸å¦‚åœ¨å¤§æ¨¡å‹ä¸Šæ‰§è¡Œé‡åŒ–åé¢„æœŸçš„é‚£æ ·å¥½ã€‚\n\nä»¥ä¸‹å±•ç¤ºäº†ä½¿ç”¨ `auto_gptq` è¿›è¡Œé‡åŒ–å’Œæ¨ç†çš„æœ€ç®€å•ç”¨æ³•ï¼š\n```python\nfrom transformers import AutoTokenizer, TextGenerationPipeline\nfrom auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig\n\n\npretrained_model_dir = \"facebook/opt-125m\"\nquantized_model_dir = \"opt-125m-4bit\"\n\n\ntokenizer = AutoTokenizer.from_pretrained(pretrained_model_dir, use_fast=True)\nexamples = [\n    tokenizer(\n        \"auto-gptq is an easy-to-use model quantization library with user-friendly apis, based on GPTQ algorithm.\"\n    )\n]\n\nquantize_config = BaseQuantizeConfig(\n    bits=4,  # å°†æ¨¡å‹é‡åŒ–ä¸º 4-bit æ•°å€¼ç±»å‹\n    group_size=128,  # ä¸€èˆ¬æ¨èå°†æ­¤å‚æ•°çš„å€¼è®¾ç½®ä¸º 128\n    desc_act=False,  # è®¾ä¸º False å¯ä»¥æ˜¾è‘—æå‡æ¨ç†é€Ÿåº¦ï¼Œä½†æ˜¯ ppl å¯èƒ½ä¼šè½»å¾®åœ°å˜å·®\n)\n\n# åŠ è½½æœªé‡åŒ–çš„æ¨¡å‹ï¼Œé»˜è®¤æƒ…å†µä¸‹ï¼Œæ¨¡å‹æ€»æ˜¯ä¼šè¢«åŠ è½½åˆ° CPU å†…å­˜ä¸­\nmodel = AutoGPTQForCausalLM.from_pretrained(pretrained_model_dir, quantize_config)\n\n# é‡åŒ–æ¨¡å‹, æ ·æœ¬çš„æ•°æ®ç±»å‹åº”è¯¥ä¸º List[Dict]ï¼Œå…¶ä¸­å­—å…¸çš„é”®æœ‰ä¸”ä»…æœ‰ input_ids å’Œ attention_mask\nmodel.quantize(examples)\n\n# ä¿å­˜é‡åŒ–å¥½çš„æ¨¡å‹\nmodel.save_quantized(quantized_model_dir)\n\n# ä½¿ç”¨ safetensors ä¿å­˜é‡åŒ–å¥½çš„æ¨¡å‹\nmodel.save_quantized(quantized_model_dir, use_safetensors=True)\n\n# å°†é‡åŒ–å¥½çš„æ¨¡å‹ç›´æ¥ä¸Šä¼ è‡³ Hugging Face Hub \n# å½“ä½¿ç”¨ use_auth_token=True æ—¶, ç¡®ä¿ä½ å·²ç»é¦–å…ˆä½¿ç”¨ huggingface-cli login è¿›è¡Œäº†ç™»å½•\n# æˆ–è€…å¯ä»¥ä½¿ç”¨ use_auth_token=\"hf_xxxxxxx\" æ¥æ˜¾å¼åœ°æ·»åŠ è´¦æˆ·è®¤è¯ token\n# ï¼ˆå–æ¶ˆä¸‹é¢ä¸‰è¡Œä»£ç çš„æ³¨é‡Šæ¥ä½¿ç”¨è¯¥åŠŸèƒ½ï¼‰\n# repo_id = f\"YourUserName/{quantized_model_dir}\"\n# commit_message = f\"AutoGPTQ model for {pretrained_model_dir}: {quantize_config.bits}bits, gr{quantize_config.group_size}, desc_act={quantize_config.desc_act}\"\n# model.push_to_hub(repo_id, commit_message=commit_message, use_auth_token=True)\n\n# æˆ–è€…ä½ ä¹Ÿå¯ä»¥åŒæ—¶å°†é‡åŒ–å¥½çš„æ¨¡å‹ä¿å­˜åˆ°æœ¬åœ°å¹¶ä¸Šä¼ è‡³ Hugging Face Hub\n# ï¼ˆå–æ¶ˆä¸‹é¢ä¸‰è¡Œä»£ç çš„æ³¨é‡Šæ¥ä½¿ç”¨è¯¥åŠŸèƒ½ï¼‰\n# repo_id = f\"YourUserName/{quantized_model_dir}\"\n# commit_message = f\"AutoGPTQ model for {pretrained_model_dir}: {quantize_config.bits}bits, gr{quantize_config.group_size}, desc_act={quantize_config.desc_act}\"\n# model.push_to_hub(repo_id, save_dir=quantized_model_dir, use_safetensors=True, commit_message=commit_message, use_auth_token=True)\n\n# åŠ è½½é‡åŒ–å¥½çš„æ¨¡å‹åˆ°èƒ½è¢«è¯†åˆ«åˆ°çš„ç¬¬ä¸€å—æ˜¾å¡ä¸­\nmodel = AutoGPTQForCausalLM.from_quantized(quantized_model_dir, device=\"cuda:0\")\n\n# ä» Hugging Face Hub ä¸‹è½½é‡åŒ–å¥½çš„æ¨¡å‹å¹¶åŠ è½½åˆ°èƒ½è¢«è¯†åˆ«åˆ°çš„ç¬¬ä¸€å—æ˜¾å¡ä¸­\n# model = AutoGPTQForCausalLM.from_quantized(repo_id, device=\"cuda:0\", use_safetensors=True, use_triton=False)\n\n# ä½¿ç”¨ model.generate æ‰§è¡Œæ¨ç†\nprint(tokenizer.decode(model.generate(**tokenizer(\"auto_gptq is\", return_tensors=\"pt\").to(model.device))[0]))\n\n# æˆ–è€…ä½¿ç”¨ TextGenerationPipeline\npipeline = TextGenerationPipeline(model=model, tokenizer=tokenizer)\nprint(pipeline(\"auto-gptq is\")[0][\"generated_text\"])\n```\n\nå‚è€ƒ [æ­¤æ ·ä¾‹è„šæœ¬](examples/quantization/quant_with_alpaca.py) ä»¥äº†è§£è¿›é˜¶çš„ç”¨æ³•ã€‚\n\n### è‡ªå®šä¹‰æ¨¡å‹\n\n<details>\n\n<summary>ä»¥ä¸‹å±•ç¤ºäº†å¦‚ä½•æ‹“å±• `auto_gptq` ä»¥æ”¯æŒ `OPT` æ¨¡å‹ï¼Œå¦‚ä½ æ‰€è§ï¼Œè¿™éå¸¸ç®€å•ï¼š</summary>\n\n```python\nfrom auto_gptq.modeling import BaseGPTQForCausalLM\n\n\nclass OPTGPTQForCausalLM(BaseGPTQForCausalLM):\n    # chained attribute name of transformer layer block\n    layers_block_name = \"model.decoder.layers\"\n    # chained attribute names of other nn modules that in the same level as the transformer layer block\n    outside_layer_modules = [\n        \"model.decoder.embed_tokens\", \"model.decoder.embed_positions\", \"model.decoder.project_out\",\n        \"model.decoder.project_in\", \"model.decoder.final_layer_norm\"\n    ]\n    # chained attribute names of linear layers in transformer layer module\n    # normally, there are four sub lists, for each one the modules in it can be seen as one operation, \n    # and the order should be the order when they are truly executed, in this case (and usually in most cases), \n    # they are: attention q_k_v projection, attention output projection, MLP project input, MLP project output\n    inside_layer_modules = [\n        [\"self_attn.k_proj\", \"self_attn.v_proj\", \"self_attn.q_proj\"],\n        [\"self_attn.out_proj\"],\n        [\"fc1\"],\n        [\"fc2\"]\n    ]\n```\nç„¶å, ä½ å°±å¯ä»¥åƒåœ¨åŸºæœ¬ç”¨æ³•ä¸€èŠ‚ä¸­å±•ç¤ºçš„é‚£æ ·ä½¿ç”¨ `OPTGPTQForCausalLM.from_pretrained` å’Œå…¶ä»–æ–¹æ³•ã€‚\n\n</details>\n\n\n### åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸Šæ‰§è¡Œè¯„ä¼°\nä½ å¯ä»¥ä½¿ç”¨åœ¨ `auto_gptq.eval_tasks` ä¸­å®šä¹‰çš„ä»»åŠ¡æ¥è¯„ä¼°é‡åŒ–å‰åçš„æ¨¡å‹åœ¨æŸä¸ªç‰¹å®šä¸‹æ¸¸ä»»åŠ¡ä¸Šçš„è¡¨ç°ã€‚\n\nè¿™äº›é¢„å®šä¹‰çš„æ¨¡å‹æ”¯æŒæ‰€æœ‰åœ¨ [ğŸ¤— transformers](https://github.com/huggingface/transformers)å’Œæœ¬é¡¹ç›®ä¸­è¢«å®ç°äº†çš„ causal-language-modelsã€‚\n\n<details>\n\n<summary>ä»¥ä¸‹æ˜¯ä½¿ç”¨ `cardiffnlp/tweet_sentiment_multilingual` æ•°æ®é›†åœ¨åºåˆ—åˆ†ç±»ï¼ˆæ–‡æœ¬åˆ†ç±»ï¼‰ä»»åŠ¡ä¸Šè¯„ä¼° `EleutherAI/gpt-j-6b` æ¨¡å‹çš„ç¤ºä¾‹:</summary>\n\n```python\nfrom functools import partial\n\nimport datasets\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig\n\nfrom auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig\nfrom auto_gptq.eval_tasks import SequenceClassificationTask\n\n\nMODEL = \"EleutherAI/gpt-j-6b\"\nDATASET = \"cardiffnlp/tweet_sentiment_multilingual\"\nTEMPLATE = \"Question:What's the sentiment of the given text? Choices are {labels}.\\nText: {text}\\nAnswer:\"\nID2LABEL = {\n    0: \"negative\",\n    1: \"neutral\",\n    2: \"positive\"\n}\nLABELS = list(ID2LABEL.values())\n\n\ndef ds_refactor_fn(samples):\n    text_data = samples[\"text\"]\n    label_data = samples[\"label\"]\n\n    new_samples = {\"prompt\": [], \"label\": []}\n    for text, label in zip(text_data, label_data):\n        prompt = TEMPLATE.format(labels=LABELS, text=text)\n        new_samples[\"prompt\"].append(prompt)\n        new_samples[\"label\"].append(ID2LABEL[label])\n\n    return new_samples\n\n\n#  model = AutoModelForCausalLM.from_pretrained(MODEL).eval().half().to(\"cuda:0\")\nmodel = AutoGPTQForCausalLM.from_pretrained(MODEL, BaseQuantizeConfig())\ntokenizer = AutoTokenizer.from_pretrained(MODEL)\n\ntask = SequenceClassificationTask(\n        model=model,\n        tokenizer=tokenizer,\n        classes=LABELS,\n        data_name_or_path=DATASET,\n        prompt_col_name=\"prompt\",\n        label_col_name=\"label\",\n        **{\n            \"num_samples\": 1000,  # how many samples will be sampled to evaluation\n            \"sample_max_len\": 1024,  # max tokens for each sample\n            \"block_max_len\": 2048,  # max tokens for each data block\n            # function to load dataset, one must only accept data_name_or_path as input \n            # and return datasets.Dataset\n            \"load_fn\": partial(datasets.load_dataset, name=\"english\"),  \n            # function to preprocess dataset, which is used for datasets.Dataset.map, \n            # must return Dict[str, list] with only two keys: [prompt_col_name, label_col_name]\n            \"preprocess_fn\": ds_refactor_fn,  \n            # truncate label when sample's length exceed sample_max_len\n            \"truncate_prompt\": False  \n        }\n    )\n\n# note that max_new_tokens will be automatically specified internally based on given classes\nprint(task.run())\n\n# self-consistency\nprint(\n    task.run(\n        generation_config=GenerationConfig(\n            num_beams=3,\n            num_return_sequences=3,\n            do_sample=True\n        )\n    )\n)\n```\n\n</details>\n\n## äº†è§£æ›´å¤š\n[æ•™ç¨‹](docs/tutorial) æä¾›äº†å°† `auto_gptq` é›†æˆåˆ°ä½ çš„é¡¹ç›®ä¸­çš„æ‰‹æŠŠæ‰‹æŒ‡å¯¼å’Œæœ€ä½³å®è·µå‡†åˆ™ã€‚\n\n[ç¤ºä¾‹](examples/README.md) æä¾›äº†å¤§é‡ç¤ºä¾‹è„šæœ¬ä»¥å°† `auto_gptq` ç”¨äºä¸åŒé¢†åŸŸã€‚\n\n## æ”¯æŒçš„æ¨¡å‹\n\n> ä½ å¯ä»¥ä½¿ç”¨ `model.config.model_type` æ¥å¯¹ç…§ä¸‹è¡¨ä»¥æ£€æŸ¥ä½ æ­£åœ¨ä½¿ç”¨çš„ä¸€ä¸ªæ¨¡å‹æ˜¯å¦è¢« `auto_gptq` æ‰€æ”¯æŒã€‚\n> \n> æ¯”å¦‚ï¼Œ `WizardLM`ï¼Œ`vicuna` å’Œ `gpt4all` æ¨¡å‹çš„ `model_type` çš†ä¸º `llama`ï¼Œ å› æ­¤è¿™äº›æ¨¡å‹çš†è¢« `auto_gptq` æ‰€æ”¯æŒã€‚\n\n| model type                         | quantization | inference | peft-lora | peft-ada-lora | peft-adaption_prompt                                                              |\n|------------------------------------|--------------|-----------|-----------|---------------|-----------------------------------------------------------------------------------|\n| bloom                              | âœ…            | âœ…         | âœ…         | âœ…             |                                                                                   |\n| gpt2                               | âœ…            | âœ…         | âœ…         | âœ…             |                                                                                   |\n| gpt_neox                           | âœ…            | âœ…         | âœ…         | âœ…             | âœ…[è¦æ±‚è¯¥åˆ†æ”¯çš„ peft](https://github.com/PanQiWei/peft/tree/multi_modal_adaption_prompt) |\n| gptj                               | âœ…            | âœ…         | âœ…         | âœ…             | âœ…[è¦æ±‚è¯¥åˆ†æ”¯çš„ peft](https://github.com/PanQiWei/peft/tree/multi_modal_adaption_prompt) |\n| llama                              | âœ…            | âœ…         | âœ…         | âœ…             | âœ…                                                                                 |\n| moss                               | âœ…            | âœ…         | âœ…         | âœ…             | âœ…[è¦æ±‚è¯¥åˆ†æ”¯çš„ peft](https://github.com/PanQiWei/peft/tree/multi_modal_adaption_prompt) |\n| opt                                | âœ…            | âœ…         | âœ…         | âœ…             |                                                                                   |\n| gpt_bigcode                        | âœ…            | âœ…         | âœ…         | âœ…             |                                                                                   |\n| codegen                            | âœ…            | âœ…         | âœ…         | âœ…             |                                                                                   |\n| falcon(RefinedWebModel/RefinedWeb) | âœ…            | âœ…         | âœ…         | âœ…             |                                                                                   |\n\n## æ”¯æŒçš„è¯„ä¼°ä»»åŠ¡\nç›®å‰ï¼Œ `auto_gptq` æ”¯æŒä»¥ä¸‹è¯„ä¼°ä»»åŠ¡ï¼š `LanguageModelingTask`, `SequenceClassificationTask` å’Œ `TextSummarizationTask`ï¼›æ›´å¤šçš„è¯„ä¼°ä»»åŠ¡å³å°†åˆ°æ¥ï¼\n\n## è‡´è°¢\n- ç‰¹åˆ«æ„Ÿè°¢ **Elias Frantar**ï¼Œ **Saleh Ashkboos**ï¼Œ **Torsten Hoefler** å’Œ **Dan Alistarh** æå‡º **GPTQ** ç®—æ³•å¹¶å¼€æº[ä»£ç ](https://github.com/IST-DASLab/gptq)ã€‚\n- ç‰¹åˆ«æ„Ÿè°¢ **qwopqwop200**ï¼Œ æœ¬é¡¹ç›®ä¸­æ¶‰åŠåˆ°æ¨¡å‹é‡åŒ–çš„ä»£ç ä¸»è¦å‚è€ƒè‡ª [GPTQ-for-LLaMa](https://github.com/qwopqwop200/GPTQ-for-LLaMa/tree/cuda)ã€‚\n\n[![Star History Chart](https://api.star-history.com/svg?repos=PanQiwei/AutoGPTQ&type=Date)](https://star-history.com/#PanQiWei/AutoGPTQ&Date)\n"
        },
        {
          "name": "auto_gptq",
          "type": "tree",
          "content": null
        },
        {
          "name": "autogptq_extension",
          "type": "tree",
          "content": null
        },
        {
          "name": "docs",
          "type": "tree",
          "content": null
        },
        {
          "name": "examples",
          "type": "tree",
          "content": null
        },
        {
          "name": "ruff.toml",
          "type": "blob",
          "size": 0.6396484375,
          "content": "# Never enforce `E501` (line length violations).\nignore = [\"C901\", \"E501\", \"E741\", \"W605\"]\nselect = [\"C\", \"E\", \"F\", \"I\", \"W\"]\nline-length = 119\n\n# Ignore import violations in all `__init__.py` files.\n[per-file-ignores]\n\"__init__.py\" = [\"E402\", \"F401\", \"F403\", \"F811\"]\n\n[isort]\nlines-after-imports = 2\nknown-first-party = [\"auto_gptq\"]\n\n[format]\n# Like Black, use double quotes for strings.\nquote-style = \"double\"\n\n# Like Black, indent with spaces, rather than tabs.\nindent-style = \"space\"\n\n# Like Black, respect magic trailing commas.\nskip-magic-trailing-comma = false\n\n# Like Black, automatically detect the appropriate line ending.\nline-ending = \"auto\"\n"
        },
        {
          "name": "setup.py",
          "type": "blob",
          "size": 10.4072265625,
          "content": "import os\nimport platform\nimport subprocess\nimport sys\nfrom pathlib import Path\n\nfrom setuptools import find_packages, setup\n\n\nos.environ[\"CC\"] = \"g++\"\nos.environ[\"CXX\"] = \"g++\"\n\ncommon_setup_kwargs = {\n    \"version\": \"0.8.0.dev0\",\n    \"name\": \"auto_gptq\",\n    \"author\": \"PanQiWei\",\n    \"description\": \"An easy-to-use LLMs quantization package with user-friendly apis, based on GPTQ algorithm.\",\n    \"long_description\": (Path(__file__).parent / \"README.md\").read_text(encoding=\"UTF-8\"),\n    \"long_description_content_type\": \"text/markdown\",\n    \"url\": \"https://github.com/PanQiWei/AutoGPTQ\",\n    \"keywords\": [\"gptq\", \"quantization\", \"large-language-models\", \"transformers\"],\n    \"platforms\": [\"windows\", \"linux\"],\n    \"classifiers\": [\n        \"Environment :: GPU :: NVIDIA CUDA :: 11.7\",\n        \"Environment :: GPU :: NVIDIA CUDA :: 11.8\",\n        \"Environment :: GPU :: NVIDIA CUDA :: 12\",\n        \"License :: OSI Approved :: MIT License\",\n        \"Natural Language :: Chinese (Simplified)\",\n        \"Natural Language :: English\",\n        \"Programming Language :: Python :: 3.8\",\n        \"Programming Language :: Python :: 3.9\",\n        \"Programming Language :: Python :: 3.10\",\n        \"Programming Language :: Python :: 3.11\",\n        \"Programming Language :: C++\",\n    ]\n}\n\n\nPYPI_RELEASE = os.environ.get('PYPI_RELEASE', None)\nBUILD_CUDA_EXT = int(os.environ.get('BUILD_CUDA_EXT', '1')) == 1\nDISABLE_QIGEN = int(os.environ.get('DISABLE_QIGEN', '1')) == 1\nCOMPILE_MARLIN = int(os.environ.get('COMPILE_MARLIN', '1')) == 1\nUNSUPPORTED_COMPUTE_CAPABILITIES = ['3.5', '3.7', '5.0', '5.2', '5.3']\n\ndef detect_local_sm_architectures():\n    \"\"\"\n    Detect compute capabilities of one machine's GPUs as PyTorch does.\n\n    Copied from https://github.com/pytorch/pytorch/blob/v2.2.2/torch/utils/cpp_extension.py#L1962-L1976\n    \"\"\"\n    arch_list = []\n\n    for i in range(torch.cuda.device_count()):\n        capability = torch.cuda.get_device_capability(i)\n        supported_sm = [int(arch.split('_')[1])\n                        for arch in torch.cuda.get_arch_list() if 'sm_' in arch]\n        max_supported_sm = max((sm // 10, sm % 10) for sm in supported_sm)\n        # Capability of the device may be higher than what's supported by the user's\n        # NVCC, causing compilation error. User's NVCC is expected to match the one\n        # used to build pytorch, so we use the maximum supported capability of pytorch\n        # to clamp the capability.\n        capability = min(max_supported_sm, capability)\n        arch = f'{capability[0]}.{capability[1]}'\n        if arch not in arch_list:\n            arch_list.append(arch)\n\n    arch_list = sorted(arch_list)\n    arch_list[-1] += '+PTX'\n    return arch_list\n\n\nif BUILD_CUDA_EXT:\n    try:\n        import torch\n    except Exception as e:\n        print(f\"Building PyTorch CUDA extension requires PyTorch being installed, please install PyTorch first: {e}.\\n NOTE: This issue may be raised due to pip build isolation system (ignoring local packages). Please use `--no-build-isolation` when installing with pip, and refer to https://github.com/AutoGPTQ/AutoGPTQ/pull/620 for more details.\")\n        sys.exit(1)\n\n    CUDA_VERSION = None\n    ROCM_VERSION = os.environ.get('ROCM_VERSION', None)\n    if ROCM_VERSION and not torch.version.hip:\n        print(\n            f\"Trying to compile auto-gptq for ROCm, but PyTorch {torch.__version__} \"\n            \"is installed without ROCm support.\"\n        )\n        sys.exit(1)\n\n    if not ROCM_VERSION:\n        default_cuda_version = torch.version.cuda\n        CUDA_VERSION = \"\".join(os.environ.get(\"CUDA_VERSION\", default_cuda_version).split(\".\"))\n\n    if ROCM_VERSION:\n        common_setup_kwargs['version'] += f\"+rocm{ROCM_VERSION}\"\n    else:\n        if not CUDA_VERSION:\n            print(\n                f\"Trying to compile auto-gptq for CUDA, but Pytorch {torch.__version__} \"\n                \"is installed without CUDA support.\"\n            )\n            sys.exit(1)\n\n        torch_cuda_arch_list = os.environ.get(\"TORCH_CUDA_ARCH_LIST\", None)\n        if torch_cuda_arch_list is not None:\n            torch_cuda_arch_list = torch_cuda_arch_list.replace(' ', ';')\n            archs = torch_cuda_arch_list.split(';')\n\n            requested_but_unsupported_archs = {arch for arch in archs if arch in UNSUPPORTED_COMPUTE_CAPABILITIES }\n            if len(requested_but_unsupported_archs) > 0:\n                raise ValueError(f\"Trying to compile AutoGPTQ for CUDA compute capabilities {torch_cuda_arch_list}, but AutoGPTQ does not support the compute capabilities {requested_but_unsupported_archs} (AutoGPTQ requires Pascal or higher). Please fix your environment variable TORCH_CUDA_ARCH_LIST (Reference: https://github.com/pytorch/pytorch/blob/v2.2.2/setup.py#L135-L139).\")\n        else:\n            local_arch_list = detect_local_sm_architectures()\n            local_but_unsupported_archs = {arch for arch in local_arch_list if arch in UNSUPPORTED_COMPUTE_CAPABILITIES}\n            if len(local_but_unsupported_archs) > 0:\n                raise ValueError(f\"PyTorch detected the compute capabilities {local_arch_list} for the NVIDIA GPUs on the current machine, but AutoGPTQ can not be built for compute capabilities {local_but_unsupported_archs} (AutoGPTQ requires Pascal or higher). Please set the environment variable TORCH_CUDA_ARCH_LIST (Reference: https://github.com/pytorch/pytorch/blob/v2.2.2/setup.py#L135-L139) with your necessary architectures.\")\n\n        # For the PyPI release, the version is simply x.x.x to comply with PEP 440.\n        if not PYPI_RELEASE:\n            common_setup_kwargs['version'] += f\"+cu{CUDA_VERSION}\"\n\nrequirements = [\n    \"accelerate>=0.26.0\",\n    \"datasets\",\n    \"sentencepiece\",\n    \"numpy\",\n    \"rouge\",\n    \"gekko\",\n    \"torch>=1.13.0\",\n    \"safetensors\",\n    \"transformers>=4.31.0\",\n    \"peft>=0.5.0\",\n    \"tqdm\",\n    \"threadpoolctl\",\n]\n\nextras_require = {\n    \"triton\": [\"triton==2.0.0\"],\n    \"test\": [\"pytest\", \"parameterized\"],\n    \"quality\": [\"ruff==0.1.5\"],\n}\n\ninclude_dirs = [\"autogptq_cuda\"]\n\nadditional_setup_kwargs = {}\nif BUILD_CUDA_EXT:\n    from torch.utils import cpp_extension\n\n    if platform.system() != \"Windows\" and platform.machine() != \"aarch64\" and not DISABLE_QIGEN:\n        print(\"Generating qigen kernels...\")\n        cores_info = subprocess.run(\"cat /proc/cpuinfo | grep cores | head -1\", shell=True, check=True, text=True, stdout=subprocess.PIPE).stdout.split(\" \")\n        if (len(cores_info) == 3 and cores_info[1].startswith(\"cores\")) or (len(cores_info) == 2):\n            p = int(cores_info[-1])\n        else:\n            p = os.cpu_count()\n        try:\n            subprocess.check_output([\"python\", \"./autogptq_extension/qigen/generate.py\", \"--module\", \"--search\", \"--p\", str(p)])\n        except subprocess.CalledProcessError:\n            raise Exception(\"Generating QiGen kernels failed with the error shown above.\")\n\n    if not ROCM_VERSION:\n        from distutils.sysconfig import get_python_lib\n        conda_cuda_include_dir = os.path.join(get_python_lib(), \"nvidia/cuda_runtime/include\")\n\n        print(\"conda_cuda_include_dir\", conda_cuda_include_dir)\n        if os.path.isdir(conda_cuda_include_dir):\n            include_dirs.append(conda_cuda_include_dir)\n            print(f\"appending conda cuda include dir {conda_cuda_include_dir}\")\n    extensions = [\n        cpp_extension.CUDAExtension(\n            \"autogptq_cuda_64\",\n            [\n                \"autogptq_extension/cuda_64/autogptq_cuda_64.cpp\",\n                \"autogptq_extension/cuda_64/autogptq_cuda_kernel_64.cu\"\n            ]\n        ),\n        cpp_extension.CUDAExtension(\n            \"autogptq_cuda_256\",\n            [\n                \"autogptq_extension/cuda_256/autogptq_cuda_256.cpp\",\n                \"autogptq_extension/cuda_256/autogptq_cuda_kernel_256.cu\"\n            ]\n        )\n    ]\n\n    if platform.system() != \"Windows\":\n        if platform.machine() != \"aarch64\" and not DISABLE_QIGEN:\n            extensions.append(\n                cpp_extension.CppExtension(\n                    \"cQIGen\",\n                    [\n                        'autogptq_extension/qigen/backend.cpp'\n                    ],\n                    extra_compile_args = [\"-O3\", \"-mavx\", \"-mavx2\", \"-mfma\", \"-march=native\", \"-ffast-math\", \"-ftree-vectorize\", \"-faligned-new\", \"-std=c++17\", \"-fopenmp\", \"-fno-signaling-nans\", \"-fno-trapping-math\"]\n                )\n            )\n\n        # Marlin is not ROCm-compatible, CUDA only\n        if not ROCM_VERSION and COMPILE_MARLIN:\n            extensions.append(\n                cpp_extension.CUDAExtension(\n                    'autogptq_marlin_cuda',\n                    [\n                        'autogptq_extension/marlin/marlin_cuda.cpp',\n                        'autogptq_extension/marlin/marlin_cuda_kernel.cu',\n                        'autogptq_extension/marlin/marlin_repack.cu'\n                    ]\n                )\n            )\n\n    if os.name == \"nt\":\n        # On Windows, fix an error LNK2001: unresolved external symbol cublasHgemm bug in the compilation\n        cuda_path = os.environ.get(\"CUDA_PATH\", None)\n        if cuda_path is None:\n            raise ValueError(\"The environment variable CUDA_PATH must be set to the path to the CUDA install when installing from source on Windows systems.\")\n        extra_link_args = [\"-L\", f\"{cuda_path}/lib/x64/cublas.lib\"]\n    else:\n        extra_link_args = []\n\n    extensions.append(\n        cpp_extension.CUDAExtension(\n            \"exllama_kernels\",\n            [\n                \"autogptq_extension/exllama/exllama_ext.cpp\",\n                \"autogptq_extension/exllama/cuda_buffers.cu\",\n                \"autogptq_extension/exllama/cuda_func/column_remap.cu\",\n                \"autogptq_extension/exllama/cuda_func/q4_matmul.cu\",\n                \"autogptq_extension/exllama/cuda_func/q4_matrix.cu\"\n            ],\n            extra_link_args=extra_link_args\n        )\n    )\n    extensions.append(\n        cpp_extension.CUDAExtension(\n            \"exllamav2_kernels\",\n            [\n                \"autogptq_extension/exllamav2/ext.cpp\",\n                \"autogptq_extension/exllamav2/cuda/q_matrix.cu\",\n                \"autogptq_extension/exllamav2/cuda/q_gemm.cu\",\n            ],\n            extra_link_args=extra_link_args\n        )\n    )\n\n    additional_setup_kwargs = {\n        \"ext_modules\": extensions,\n        \"cmdclass\": {'build_ext': cpp_extension.BuildExtension}\n    }\ncommon_setup_kwargs.update(additional_setup_kwargs)\nsetup(\n    packages=find_packages(),\n    install_requires=requirements,\n    extras_require=extras_require,\n    include_dirs=include_dirs,\n    python_requires=\">=3.8.0\",\n    **common_setup_kwargs\n)\n"
        },
        {
          "name": "tests",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}