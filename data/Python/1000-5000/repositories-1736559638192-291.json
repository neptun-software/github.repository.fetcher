{
  "metadata": {
    "timestamp": 1736559638192,
    "page": 291,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjMwMA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "lipku/LiveTalking",
      "stars": 4284,
      "defaultBranch": "main",
      "files": [
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.1845703125,
          "content": "__pycache__/\nbuild/\n*.egg-info/\n*.so\n*.mp4\n\ntmp*\ntrial*/\n\ndata\ndata_utils/face_tracking/3DMM/*\ndata_utils/face_parsing/79999_iter.pth\n\npretrained\n*.mp4\n.DS_Store\nworkspace/log_ngp.txt\n.idea"
        },
        {
          "name": "Dockerfile",
          "type": "blob",
          "size": 1.7392578125,
          "content": "# Copyright (c) 2020-2022, NVIDIA CORPORATION.  All rights reserved.\n#\n# NVIDIA CORPORATION and its licensors retain all intellectual property\n# and proprietary rights in and to this software, related documentation\n# and any modifications thereto.  Any use, reproduction, disclosure or\n# distribution of this software and related documentation without an express\n# license agreement from NVIDIA CORPORATION is strictly prohibited.\n\nARG BASE_IMAGE=nvcr.io/nvidia/cuda:11.6.1-cudnn8-devel-ubuntu20.04\nFROM $BASE_IMAGE\n\nRUN apt-get update -yq --fix-missing \\\n && DEBIAN_FRONTEND=noninteractive apt-get install -yq --no-install-recommends \\\n    pkg-config \\\n    wget \\\n    cmake \\\n    curl \\\n    git \\\n    vim\n\n#ENV PYTHONDONTWRITEBYTECODE=1\n#ENV PYTHONUNBUFFERED=1\n\n# nvidia-container-runtime\n#ENV NVIDIA_VISIBLE_DEVICES all\n#ENV NVIDIA_DRIVER_CAPABILITIES compute,utility,graphics\n\nRUN wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh\nRUN sh Miniconda3-latest-Linux-x86_64.sh -b -u -p ~/miniconda3\nRUN ~/miniconda3/bin/conda init\nRUN source ~/.bashrc\nRUN conda create -n nerfstream python=3.10\nRUN conda activate nerfstream\n\nRUN pip config set global.index-url https://mirrors.aliyun.com/pypi/simple/\n# install depend\nRUN conda install pytorch==1.12.1 torchvision==0.13.1 cudatoolkit=11.3 -c pytorch\nCopy requirements.txt ./\nRUN pip install -r requirements.txt\n\n# additional libraries\nRUN pip install \"git+https://github.com/facebookresearch/pytorch3d.git\"\nRUN pip install tensorflow-gpu==2.8.0\n\nRUN pip uninstall protobuf\nRUN pip install protobuf==3.20.1\n\nRUN conda install ffmpeg\nCopy ../python_rtmpstream /python_rtmpstream\nWORKDIR /python_rtmpstream/python\nRUN pip install .\n\nCopy ../nerfstream /nerfstream\nWORKDIR /nerfstream\nCMD [\"python3\", \"app.py\"]\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 11.078125,
          "content": "                                 Apache License\n                           Version 2.0, January 2004\n                        http://www.apache.org/licenses/\n\n   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n\n   1. Definitions.\n\n      \"License\" shall mean the terms and conditions for use, reproduction,\n      and distribution as defined by Sections 1 through 9 of this document.\n\n      \"Licensor\" shall mean the copyright owner or entity authorized by\n      the copyright owner that is granting the License.\n\n      \"Legal Entity\" shall mean the union of the acting entity and all\n      other entities that control, are controlled by, or are under common\n      control with that entity. For the purposes of this definition,\n      \"control\" means (i) the power, direct or indirect, to cause the\n      direction or management of such entity, whether by contract or\n      otherwise, or (ii) ownership of fifty percent (50%) or more of the\n      outstanding shares, or (iii) beneficial ownership of such entity.\n\n      \"You\" (or \"Your\") shall mean an individual or Legal Entity\n      exercising permissions granted by this License.\n\n      \"Source\" form shall mean the preferred form for making modifications,\n      including but not limited to software source code, documentation\n      source, and configuration files.\n\n      \"Object\" form shall mean any form resulting from mechanical\n      transformation or translation of a Source form, including but\n      not limited to compiled object code, generated documentation,\n      and conversions to other media types.\n\n      \"Work\" shall mean the work of authorship, whether in Source or\n      Object form, made available under the License, as indicated by a\n      copyright notice that is included in or attached to the work\n      (an example is provided in the Appendix below).\n\n      \"Derivative Works\" shall mean any work, whether in Source or Object\n      form, that is based on (or derived from) the Work and for which the\n      editorial revisions, annotations, elaborations, or other modifications\n      represent, as a whole, an original work of authorship. For the purposes\n      of this License, Derivative Works shall not include works that remain\n      separable from, or merely link (or bind by name) to the interfaces of,\n      the Work and Derivative Works thereof.\n\n      \"Contribution\" shall mean any work of authorship, including\n      the original version of the Work and any modifications or additions\n      to that Work or Derivative Works thereof, that is intentionally\n      submitted to Licensor for inclusion in the Work by the copyright owner\n      or by an individual or Legal Entity authorized to submit on behalf of\n      the copyright owner. For the purposes of this definition, \"submitted\"\n      means any form of electronic, verbal, or written communication sent\n      to the Licensor or its representatives, including but not limited to\n      communication on electronic mailing lists, source code control systems,\n      and issue tracking systems that are managed by, or on behalf of, the\n      Licensor for the purpose of discussing and improving the Work, but\n      excluding communication that is conspicuously marked or otherwise\n      designated in writing by the copyright owner as \"Not a Contribution.\"\n\n      \"Contributor\" shall mean Licensor and any individual or Legal Entity\n      on behalf of whom a Contribution has been received by Licensor and\n      subsequently incorporated within the Work.\n\n   2. Grant of Copyright License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      copyright license to reproduce, prepare Derivative Works of,\n      publicly display, publicly perform, sublicense, and distribute the\n      Work and such Derivative Works in Source or Object form.\n\n   3. Grant of Patent License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      (except as stated in this section) patent license to make, have made,\n      use, offer to sell, sell, import, and otherwise transfer the Work,\n      where such license applies only to those patent claims licensable\n      by such Contributor that are necessarily infringed by their\n      Contribution(s) alone or by combination of their Contribution(s)\n      with the Work to which such Contribution(s) was submitted. If You\n      institute patent litigation against any entity (including a\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\n      or a Contribution incorporated within the Work constitutes direct\n      or contributory patent infringement, then any patent licenses\n      granted to You under this License for that Work shall terminate\n      as of the date such litigation is filed.\n\n   4. Redistribution. You may reproduce and distribute copies of the\n      Work or Derivative Works thereof in any medium, with or without\n      modifications, and in Source or Object form, provided that You\n      meet the following conditions:\n\n      (a) You must give any other recipients of the Work or\n          Derivative Works a copy of this License; and\n\n      (b) You must cause any modified files to carry prominent notices\n          stating that You changed the files; and\n\n      (c) You must retain, in the Source form of any Derivative Works\n          that You distribute, all copyright, patent, trademark, and\n          attribution notices from the Source form of the Work,\n          excluding those notices that do not pertain to any part of\n          the Derivative Works; and\n\n      (d) If the Work includes a \"NOTICE\" text file as part of its\n          distribution, then any Derivative Works that You distribute must\n          include a readable copy of the attribution notices contained\n          within such NOTICE file, excluding those notices that do not\n          pertain to any part of the Derivative Works, in at least one\n          of the following places: within a NOTICE text file distributed\n          as part of the Derivative Works; within the Source form or\n          documentation, if provided along with the Derivative Works; or,\n          within a display generated by the Derivative Works, if and\n          wherever such third-party notices normally appear. The contents\n          of the NOTICE file are for informational purposes only and\n          do not modify the License. You may add Your own attribution\n          notices within Derivative Works that You distribute, alongside\n          or as an addendum to the NOTICE text from the Work, provided\n          that such additional attribution notices cannot be construed\n          as modifying the License.\n\n      You may add Your own copyright statement to Your modifications and\n      may provide additional or different license terms and conditions\n      for use, reproduction, or distribution of Your modifications, or\n      for any such Derivative Works as a whole, provided Your use,\n      reproduction, and distribution of the Work otherwise complies with\n      the conditions stated in this License.\n\n   5. Submission of Contributions. Unless You explicitly state otherwise,\n      any Contribution intentionally submitted for inclusion in the Work\n      by You to the Licensor shall be under the terms and conditions of\n      this License, without any additional terms or conditions.\n      Notwithstanding the above, nothing herein shall supersede or modify\n      the terms of any separate license agreement you may have executed\n      with Licensor regarding such Contributions.\n\n   6. Trademarks. This License does not grant permission to use the trade\n      names, trademarks, service marks, or product names of the Licensor,\n      except as required for reasonable and customary use in describing the\n      origin of the Work and reproducing the content of the NOTICE file.\n\n   7. Disclaimer of Warranty. Unless required by applicable law or\n      agreed to in writing, Licensor provides the Work (and each\n      Contributor provides its Contributions) on an \"AS IS\" BASIS,\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n      implied, including, without limitation, any warranties or conditions\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n      PARTICULAR PURPOSE. You are solely responsible for determining the\n      appropriateness of using or redistributing the Work and assume any\n      risks associated with Your exercise of permissions under this License.\n\n   8. Limitation of Liability. In no event and under no legal theory,\n      whether in tort (including negligence), contract, or otherwise,\n      unless required by applicable law (such as deliberate and grossly\n      negligent acts) or agreed to in writing, shall any Contributor be\n      liable to You for damages, including any direct, indirect, special,\n      incidental, or consequential damages of any character arising as a\n      result of this License or out of the use or inability to use the\n      Work (including but not limited to damages for loss of goodwill,\n      work stoppage, computer failure or malfunction, or any and all\n      other commercial damages or losses), even if such Contributor\n      has been advised of the possibility of such damages.\n\n   9. Accepting Warranty or Additional Liability. While redistributing\n      the Work or Derivative Works thereof, You may choose to offer,\n      and charge a fee for, acceptance of support, warranty, indemnity,\n      or other liability obligations and/or rights consistent with this\n      License. However, in accepting such obligations, You may act only\n      on Your own behalf and on Your sole responsibility, not on behalf\n      of any other Contributor, and only if You agree to indemnify,\n      defend, and hold each Contributor harmless for any liability\n      incurred by, or claims asserted against, such Contributor by reason\n      of your accepting any such warranty or additional liability.\n\n   END OF TERMS AND CONDITIONS\n\n   APPENDIX: How to apply the Apache License to your work.\n\n      To apply the Apache License to your work, attach the following\n      boilerplate notice, with the fields enclosed by brackets \"[]\"\n      replaced with your own identifying information. (Don't include\n      the brackets!)  The text should be enclosed in the appropriate\n      comment syntax for the file format. We also recommend that a\n      file or class name and description of purpose be included on the\n      same \"printed page\" as the copyright notice for easier\n      identification within third-party archives.\n\n   Copyright [livetalking@lipku]\n\n   Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 4.0791015625,
          "content": "Real time interactive streaming digital human， realize audio video synchronous dialogue. It can basically achieve commercial effects.  \n实时交互流式数字人，实现音视频同步对话。基本可以达到商用效果\n\n[ernerf效果](https://www.bilibili.com/video/BV1PM4m1y7Q2/)  [musetalk效果](https://www.bilibili.com/video/BV1gm421N7vQ/)  [wav2lip效果](https://www.bilibili.com/video/BV1Bw4m1e74P/)\n\n## 为避免与3d数字人混淆，原项目metahuman-stream改名为livetalking，原有链接地址继续可用\n\n## News\n- 2024.12.8 完善多并发，显存不随并发数增加\n- 2024.12.21 添加wav2lip、musetalk模型预热，解决第一次推理卡顿问题。感谢@heimaojinzhangyz\n- 2024.12.28 添加数字人模型Ultralight-Digital-Human。 感谢@lijihua2017\n\n## Features\n1. 支持多种数字人模型: ernerf、musetalk、wav2lip、Ultralight-Digital-Human\n2. 支持声音克隆\n3. 支持数字人说话被打断\n4. 支持全身视频拼接\n5. 支持rtmp和webrtc\n6. 支持视频编排：不说话时播放自定义视频\n7. 支持多并发\n\n## 1. Installation\n\nTested on Ubuntu 20.04, Python3.10, Pytorch 1.12 and CUDA 11.3\n\n### 1.1 Install dependency\n\n```bash\nconda create -n nerfstream python=3.10\nconda activate nerfstream\n#如果cuda版本不为11.3(运行nvidia-smi确认版本)，根据<https://pytorch.org/get-started/previous-versions/>安装对应版本的pytorch \nconda install pytorch==1.12.1 torchvision==0.13.1 cudatoolkit=11.3 -c pytorch\npip install -r requirements.txt\n#如果不训练ernerf模型，不需要安装下面的库\npip install \"git+https://github.com/facebookresearch/pytorch3d.git\"\npip install tensorflow-gpu==2.8.0\npip install --upgrade \"protobuf<=3.20.1\"\n``` \n安装常见问题[FAQ](https://livetalking-doc.readthedocs.io/en/latest/faq.html)  \nlinux cuda环境搭建可以参考这篇文章 https://zhuanlan.zhihu.com/p/674972886\n\n\n## 2. Quick Start\n默认采用ernerf模型，webrtc推流到srs  \n### 2.1 运行srs\n```bash\nexport CANDIDATE='<服务器外网ip>'  #如果srs与浏览器访问在同一层级内网，不需要执行这步\ndocker run --rm --env CANDIDATE=$CANDIDATE \\\n  -p 1935:1935 -p 8080:8080 -p 1985:1985 -p 8000:8000/udp \\\n  registry.cn-hangzhou.aliyuncs.com/ossrs/srs:5 \\\n  objs/srs -c conf/rtc.conf\n```\n备注：<font color=red>服务端需要开放端口 tcp:8000,8010,1985; udp:8000</font>\n\n### 2.2 启动数字人：\n\n```python\npython app.py\n```\n\n如果访问不了huggingface，在运行前\n```\nexport HF_ENDPOINT=https://hf-mirror.com\n```\n\n用浏览器打开http://serverip:8010/rtcpushapi.html, 在文本框输入任意文字，提交。数字人播报该段文字  \n\n\n## 3. More Usage\n使用说明: <https://livetalking-doc.readthedocs.io/>\n  \n## 4. Docker Run  \n不需要前面的安装，直接运行。\n```\ndocker run --gpus all -it --network=host --rm registry.cn-beijing.aliyuncs.com/codewithgpu2/lipku-metahuman-stream:vjo1Y6NJ3N\n```\n代码在/root/metahuman-stream，先git pull拉一下最新代码，然后执行命令同第2、3步 \n\n提供如下镜像\n- autodl镜像: <https://www.codewithgpu.com/i/lipku/metahuman-stream/base>   \n[autodl教程](https://livetalking-doc.readthedocs.io/en/latest/autodl/README.html)\n- ucloud镜像: <https://www.compshare.cn/images-detail?ImageID=compshareImage-16ktl2kxwjef&ImageType=Community&referral_code=3XW3852OBmnD089hMMrtuU&ytag=lipku_github>  \n可以开放任意端口，不需要另外部署srs服务.  \n[ucloud教程](https://livetalking-doc.readthedocs.io/en/latest/ucloud/ucloud.html) \n\n\n## 5. TODO\n- [x] 添加chatgpt实现数字人对话\n- [x] 声音克隆\n- [x] 数字人静音时用一段视频代替\n- [x] MuseTalk\n- [x] Wav2Lip\n- [x] Ultralight-Digital-Human\n\n---\n如果本项目对你有帮助，帮忙点个star。也欢迎感兴趣的朋友一起来完善该项目.\n* 知识星球: https://t.zsxq.com/7NMyO 沉淀高质量常见问题、最佳实践经验、问题解答  \n* 微信公众号：数字人技术  \n![](https://mmbiz.qpic.cn/sz_mmbiz_jpg/l3ZibgueFiaeyfaiaLZGuMGQXnhLWxibpJUS2gfs8Dje6JuMY8zu2tVyU9n8Zx1yaNncvKHBMibX0ocehoITy5qQEZg/640?wxfrom=12&tp=wxpic&usePicPrefetch=1&wx_fmt=jpeg&amp;from=appmsg)  \n\n"
        },
        {
          "name": "app.py",
          "type": "blob",
          "size": 23.470703125,
          "content": "###############################################################################\n#  Copyright (C) 2024 LiveTalking@lipku https://github.com/lipku/LiveTalking\n#  email: lipku@foxmail.com\n# \n#  Licensed under the Apache License, Version 2.0 (the \"License\");\n#  you may not use this file except in compliance with the License.\n#  You may obtain a copy of the License at\n#  \n#       http://www.apache.org/licenses/LICENSE-2.0\n# \n#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an \"AS IS\" BASIS,\n#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#  See the License for the specific language governing permissions and\n#  limitations under the License.\n###############################################################################\n\n# server.py\nfrom flask import Flask, render_template,send_from_directory,request, jsonify\nfrom flask_sockets import Sockets\nimport base64\nimport time\nimport json\n#import gevent\n#from gevent import pywsgi\n#from geventwebsocket.handler import WebSocketHandler\nimport os\nimport re\nimport numpy as np\nfrom threading import Thread,Event\n#import multiprocessing\nimport torch.multiprocessing as mp\n\nfrom aiohttp import web\nimport aiohttp\nimport aiohttp_cors\nfrom aiortc import RTCPeerConnection, RTCSessionDescription\nfrom aiortc.rtcrtpsender import RTCRtpSender\nfrom webrtc import HumanPlayer\n\nimport argparse\nimport random\n\nimport shutil\nimport asyncio\nimport torch\n\n\napp = Flask(__name__)\n#sockets = Sockets(app)\nnerfreals = {}\nopt = None\nmodel = None\navatar = None\n\n\n# def llm_response(message):\n#     from llm.LLM import LLM\n#     # llm = LLM().init_model('Gemini', model_path= 'gemini-pro',api_key='Your API Key', proxy_url=None)\n#     # llm = LLM().init_model('ChatGPT', model_path= 'gpt-3.5-turbo',api_key='Your API Key')\n#     llm = LLM().init_model('VllmGPT', model_path= 'THUDM/chatglm3-6b')\n#     response = llm.chat(message)\n#     print(response)\n#     return response\n\ndef llm_response(message,nerfreal):\n    start = time.perf_counter()\n    from openai import OpenAI\n    client = OpenAI(\n        # 如果您没有配置环境变量，请在此处用您的API Key进行替换\n        api_key=os.getenv(\"DASHSCOPE_API_KEY\"),\n        # 填写DashScope SDK的base_url\n        base_url=\"https://dashscope.aliyuncs.com/compatible-mode/v1\",\n    )\n    end = time.perf_counter()\n    print(f\"llm Time init: {end-start}s\")\n    completion = client.chat.completions.create(\n        model=\"qwen-plus\",\n        messages=[{'role': 'system', 'content': 'You are a helpful assistant.'},\n                  {'role': 'user', 'content': message}],\n        stream=True,\n        # 通过以下设置，在流式输出的最后一行展示token使用信息\n        stream_options={\"include_usage\": True}\n    )\n    result=\"\"\n    first = True\n    for chunk in completion:\n        if len(chunk.choices)>0:\n            #print(chunk.choices[0].delta.content)\n            if first:\n                end = time.perf_counter()\n                print(f\"llm Time to first chunk: {end-start}s\")\n                first = False\n            msg = chunk.choices[0].delta.content\n            lastpos=0\n            #msglist = re.split('[,.!;:，。！?]',msg)\n            for i, char in enumerate(msg):\n                if char in \",.!;:，。！？：；\" :\n                    result = result+msg[lastpos:i+1]\n                    lastpos = i+1\n                    if len(result)>10:\n                        print(result)\n                        nerfreal.put_msg_txt(result)\n                        result=\"\"\n            result = result+msg[lastpos:]\n    end = time.perf_counter()\n    print(f\"llm Time to last chunk: {end-start}s\")\n    nerfreal.put_msg_txt(result)            \n\n#####webrtc###############################\npcs = set()\n\ndef randN(N):\n    '''生成长度为 N的随机数 '''\n    min = pow(10, N - 1)\n    max = pow(10, N)\n    return random.randint(min, max - 1)\n\ndef build_nerfreal(sessionid):\n    opt.sessionid=sessionid\n    if opt.model == 'wav2lip':\n        from lipreal import LipReal\n        nerfreal = LipReal(opt,model,avatar)\n    elif opt.model == 'musetalk':\n        from musereal import MuseReal\n        nerfreal = MuseReal(opt,model,avatar)\n    elif opt.model == 'ernerf':\n        from nerfreal import NeRFReal\n        nerfreal = NeRFReal(opt,model,avatar)\n    elif opt.model == 'ultralight':\n        from lightreal import LightReal\n        nerfreal = LightReal(opt,model,avatar)\n    return nerfreal\n\n#@app.route('/offer', methods=['POST'])\nasync def offer(request):\n    params = await request.json()\n    offer = RTCSessionDescription(sdp=params[\"sdp\"], type=params[\"type\"])\n\n    if len(nerfreals) >= opt.max_session:\n        print('reach max session')\n        return -1\n    sessionid = randN(6) #len(nerfreals)\n    print('sessionid=',sessionid)\n    nerfreals[sessionid] = None\n    nerfreal = await asyncio.get_event_loop().run_in_executor(None, build_nerfreal,sessionid)\n    nerfreals[sessionid] = nerfreal\n    \n    pc = RTCPeerConnection()\n    pcs.add(pc)\n\n    @pc.on(\"connectionstatechange\")\n    async def on_connectionstatechange():\n        print(\"Connection state is %s\" % pc.connectionState)\n        if pc.connectionState == \"failed\":\n            await pc.close()\n            pcs.discard(pc)\n            del nerfreals[sessionid]\n        if pc.connectionState == \"closed\":\n            pcs.discard(pc)\n            del nerfreals[sessionid]\n\n    player = HumanPlayer(nerfreals[sessionid])\n    audio_sender = pc.addTrack(player.audio)\n    video_sender = pc.addTrack(player.video)\n    capabilities = RTCRtpSender.getCapabilities(\"video\")\n    preferences = list(filter(lambda x: x.name == \"H264\", capabilities.codecs))\n    preferences += list(filter(lambda x: x.name == \"VP8\", capabilities.codecs))\n    preferences += list(filter(lambda x: x.name == \"rtx\", capabilities.codecs))\n    transceiver = pc.getTransceivers()[1]\n    transceiver.setCodecPreferences(preferences)\n\n    await pc.setRemoteDescription(offer)\n\n    answer = await pc.createAnswer()\n    await pc.setLocalDescription(answer)\n\n    #return jsonify({\"sdp\": pc.localDescription.sdp, \"type\": pc.localDescription.type})\n\n    return web.Response(\n        content_type=\"application/json\",\n        text=json.dumps(\n            {\"sdp\": pc.localDescription.sdp, \"type\": pc.localDescription.type, \"sessionid\":sessionid}\n        ),\n    )\n\nasync def human(request):\n    params = await request.json()\n\n    sessionid = params.get('sessionid',0)\n    if params.get('interrupt'):\n        nerfreals[sessionid].flush_talk()\n\n    if params['type']=='echo':\n        nerfreals[sessionid].put_msg_txt(params['text'])\n    elif params['type']=='chat':\n        res=await asyncio.get_event_loop().run_in_executor(None, llm_response, params['text'],nerfreals[sessionid])                         \n        #nerfreals[sessionid].put_msg_txt(res)\n\n    return web.Response(\n        content_type=\"application/json\",\n        text=json.dumps(\n            {\"code\": 0, \"data\":\"ok\"}\n        ),\n    )\n\nasync def humanaudio(request):\n    try:\n        form= await request.post()\n        sessionid = int(form.get('sessionid',0))\n        fileobj = form[\"file\"]\n        filename=fileobj.filename\n        filebytes=fileobj.file.read()\n        nerfreals[sessionid].put_audio_file(filebytes)\n\n        return web.Response(\n            content_type=\"application/json\",\n            text=json.dumps(\n                {\"code\": 0, \"msg\":\"ok\"}\n            ),\n        )\n    except Exception as e:\n        return web.Response(\n            content_type=\"application/json\",\n            text=json.dumps(\n                {\"code\": -1, \"msg\":\"err\",\"data\": \"\"+e.args[0]+\"\"}\n            ),\n        )\n\nasync def set_audiotype(request):\n    params = await request.json()\n\n    sessionid = params.get('sessionid',0)    \n    nerfreals[sessionid].set_curr_state(params['audiotype'],params['reinit'])\n\n    return web.Response(\n        content_type=\"application/json\",\n        text=json.dumps(\n            {\"code\": 0, \"data\":\"ok\"}\n        ),\n    )\n\nasync def record(request):\n    params = await request.json()\n\n    sessionid = params.get('sessionid',0)\n    if params['type']=='start_record':\n        # nerfreals[sessionid].put_msg_txt(params['text'])\n        nerfreals[sessionid].start_recording()\n    elif params['type']=='end_record':\n        nerfreals[sessionid].stop_recording()\n    return web.Response(\n        content_type=\"application/json\",\n        text=json.dumps(\n            {\"code\": 0, \"data\":\"ok\"}\n        ),\n    )\n\nasync def is_speaking(request):\n    params = await request.json()\n\n    sessionid = params.get('sessionid',0)\n    return web.Response(\n        content_type=\"application/json\",\n        text=json.dumps(\n            {\"code\": 0, \"data\": nerfreals[sessionid].is_speaking()}\n        ),\n    )\n\n\nasync def on_shutdown(app):\n    # close peer connections\n    coros = [pc.close() for pc in pcs]\n    await asyncio.gather(*coros)\n    pcs.clear()\n\nasync def post(url,data):\n    try:\n        async with aiohttp.ClientSession() as session:\n            async with session.post(url,data=data) as response:\n                return await response.text()\n    except aiohttp.ClientError as e:\n        print(f'Error: {e}')\n\nasync def run(push_url,sessionid):\n    nerfreal = await asyncio.get_event_loop().run_in_executor(None, build_nerfreal,sessionid)\n    nerfreals[sessionid] = nerfreal\n\n    pc = RTCPeerConnection()\n    pcs.add(pc)\n\n    @pc.on(\"connectionstatechange\")\n    async def on_connectionstatechange():\n        print(\"Connection state is %s\" % pc.connectionState)\n        if pc.connectionState == \"failed\":\n            await pc.close()\n            pcs.discard(pc)\n\n    player = HumanPlayer(nerfreals[sessionid])\n    audio_sender = pc.addTrack(player.audio)\n    video_sender = pc.addTrack(player.video)\n\n    await pc.setLocalDescription(await pc.createOffer())\n    answer = await post(push_url,pc.localDescription.sdp)\n    await pc.setRemoteDescription(RTCSessionDescription(sdp=answer,type='answer'))\n##########################################\n# os.environ['MKL_SERVICE_FORCE_INTEL'] = '1'\n# os.environ['MULTIPROCESSING_METHOD'] = 'forkserver'                                                    \nif __name__ == '__main__':\n    mp.set_start_method('spawn')\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--pose', type=str, default=\"data/data_kf.json\", help=\"transforms.json, pose source\")\n    parser.add_argument('--au', type=str, default=\"data/au.csv\", help=\"eye blink area\")\n    parser.add_argument('--torso_imgs', type=str, default=\"\", help=\"torso images path\")\n\n    parser.add_argument('-O', action='store_true', help=\"equals --fp16 --cuda_ray --exp_eye\")\n\n    parser.add_argument('--data_range', type=int, nargs='*', default=[0, -1], help=\"data range to use\")\n    parser.add_argument('--workspace', type=str, default='data/video')\n    parser.add_argument('--seed', type=int, default=0)\n\n    ### training options\n    parser.add_argument('--ckpt', type=str, default='data/pretrained/ngp_kf.pth')\n   \n    parser.add_argument('--num_rays', type=int, default=4096 * 16, help=\"num rays sampled per image for each training step\")\n    parser.add_argument('--cuda_ray', action='store_true', help=\"use CUDA raymarching instead of pytorch\")\n    parser.add_argument('--max_steps', type=int, default=16, help=\"max num steps sampled per ray (only valid when using --cuda_ray)\")\n    parser.add_argument('--num_steps', type=int, default=16, help=\"num steps sampled per ray (only valid when NOT using --cuda_ray)\")\n    parser.add_argument('--upsample_steps', type=int, default=0, help=\"num steps up-sampled per ray (only valid when NOT using --cuda_ray)\")\n    parser.add_argument('--update_extra_interval', type=int, default=16, help=\"iter interval to update extra status (only valid when using --cuda_ray)\")\n    parser.add_argument('--max_ray_batch', type=int, default=4096, help=\"batch size of rays at inference to avoid OOM (only valid when NOT using --cuda_ray)\")\n\n    ### loss set\n    parser.add_argument('--warmup_step', type=int, default=10000, help=\"warm up steps\")\n    parser.add_argument('--amb_aud_loss', type=int, default=1, help=\"use ambient aud loss\")\n    parser.add_argument('--amb_eye_loss', type=int, default=1, help=\"use ambient eye loss\")\n    parser.add_argument('--unc_loss', type=int, default=1, help=\"use uncertainty loss\")\n    parser.add_argument('--lambda_amb', type=float, default=1e-4, help=\"lambda for ambient loss\")\n\n    ### network backbone options\n    parser.add_argument('--fp16', action='store_true', help=\"use amp mixed precision training\")\n    \n    parser.add_argument('--bg_img', type=str, default='white', help=\"background image\")\n    parser.add_argument('--fbg', action='store_true', help=\"frame-wise bg\")\n    parser.add_argument('--exp_eye', action='store_true', help=\"explicitly control the eyes\")\n    parser.add_argument('--fix_eye', type=float, default=-1, help=\"fixed eye area, negative to disable, set to 0-0.3 for a reasonable eye\")\n    parser.add_argument('--smooth_eye', action='store_true', help=\"smooth the eye area sequence\")\n\n    parser.add_argument('--torso_shrink', type=float, default=0.8, help=\"shrink bg coords to allow more flexibility in deform\")\n\n    ### dataset options\n    parser.add_argument('--color_space', type=str, default='srgb', help=\"Color space, supports (linear, srgb)\")\n    parser.add_argument('--preload', type=int, default=0, help=\"0 means load data from disk on-the-fly, 1 means preload to CPU, 2 means GPU.\")\n    # (the default value is for the fox dataset)\n    parser.add_argument('--bound', type=float, default=1, help=\"assume the scene is bounded in box[-bound, bound]^3, if > 1, will invoke adaptive ray marching.\")\n    parser.add_argument('--scale', type=float, default=4, help=\"scale camera location into box[-bound, bound]^3\")\n    parser.add_argument('--offset', type=float, nargs='*', default=[0, 0, 0], help=\"offset of camera location\")\n    parser.add_argument('--dt_gamma', type=float, default=1/256, help=\"dt_gamma (>=0) for adaptive ray marching. set to 0 to disable, >0 to accelerate rendering (but usually with worse quality)\")\n    parser.add_argument('--min_near', type=float, default=0.05, help=\"minimum near distance for camera\")\n    parser.add_argument('--density_thresh', type=float, default=10, help=\"threshold for density grid to be occupied (sigma)\")\n    parser.add_argument('--density_thresh_torso', type=float, default=0.01, help=\"threshold for density grid to be occupied (alpha)\")\n    parser.add_argument('--patch_size', type=int, default=1, help=\"[experimental] render patches in training, so as to apply LPIPS loss. 1 means disabled, use [64, 32, 16] to enable\")\n\n    parser.add_argument('--init_lips', action='store_true', help=\"init lips region\")\n    parser.add_argument('--finetune_lips', action='store_true', help=\"use LPIPS and landmarks to fine tune lips region\")\n    parser.add_argument('--smooth_lips', action='store_true', help=\"smooth the enc_a in a exponential decay way...\")\n\n    parser.add_argument('--torso', action='store_true', help=\"fix head and train torso\")\n    parser.add_argument('--head_ckpt', type=str, default='', help=\"head model\")\n\n    ### GUI options\n    parser.add_argument('--gui', action='store_true', help=\"start a GUI\")\n    parser.add_argument('--W', type=int, default=450, help=\"GUI width\")\n    parser.add_argument('--H', type=int, default=450, help=\"GUI height\")\n    parser.add_argument('--radius', type=float, default=3.35, help=\"default GUI camera radius from center\")\n    parser.add_argument('--fovy', type=float, default=21.24, help=\"default GUI camera fovy\")\n    parser.add_argument('--max_spp', type=int, default=1, help=\"GUI rendering max sample per pixel\")\n\n    ### else\n    parser.add_argument('--att', type=int, default=2, help=\"audio attention mode (0 = turn off, 1 = left-direction, 2 = bi-direction)\")\n    parser.add_argument('--aud', type=str, default='', help=\"audio source (empty will load the default, else should be a path to a npy file)\")\n    parser.add_argument('--emb', action='store_true', help=\"use audio class + embedding instead of logits\")\n\n    parser.add_argument('--ind_dim', type=int, default=4, help=\"individual code dim, 0 to turn off\")\n    parser.add_argument('--ind_num', type=int, default=10000, help=\"number of individual codes, should be larger than training dataset size\")\n\n    parser.add_argument('--ind_dim_torso', type=int, default=8, help=\"individual code dim, 0 to turn off\")\n\n    parser.add_argument('--amb_dim', type=int, default=2, help=\"ambient dimension\")\n    parser.add_argument('--part', action='store_true', help=\"use partial training data (1/10)\")\n    parser.add_argument('--part2', action='store_true', help=\"use partial training data (first 15s)\")\n\n    parser.add_argument('--train_camera', action='store_true', help=\"optimize camera pose\")\n    parser.add_argument('--smooth_path', action='store_true', help=\"brute-force smooth camera pose trajectory with a window size\")\n    parser.add_argument('--smooth_path_window', type=int, default=7, help=\"smoothing window size\")\n\n    # asr\n    parser.add_argument('--asr', action='store_true', help=\"load asr for real-time app\")\n    parser.add_argument('--asr_wav', type=str, default='', help=\"load the wav and use as input\")\n    parser.add_argument('--asr_play', action='store_true', help=\"play out the audio\")\n\n    #parser.add_argument('--asr_model', type=str, default='deepspeech')\n    parser.add_argument('--asr_model', type=str, default='cpierse/wav2vec2-large-xlsr-53-esperanto') #\n    # parser.add_argument('--asr_model', type=str, default='facebook/wav2vec2-large-960h-lv60-self')\n    # parser.add_argument('--asr_model', type=str, default='facebook/hubert-large-ls960-ft')\n\n    parser.add_argument('--asr_save_feats', action='store_true')\n    # audio FPS\n    parser.add_argument('--fps', type=int, default=50)\n    # sliding window left-middle-right length (unit: 20ms)\n    parser.add_argument('-l', type=int, default=10)\n    parser.add_argument('-m', type=int, default=8)\n    parser.add_argument('-r', type=int, default=10)\n\n    parser.add_argument('--fullbody', action='store_true', help=\"fullbody human\")\n    parser.add_argument('--fullbody_img', type=str, default='data/fullbody/img')\n    parser.add_argument('--fullbody_width', type=int, default=580)\n    parser.add_argument('--fullbody_height', type=int, default=1080)\n    parser.add_argument('--fullbody_offset_x', type=int, default=0)\n    parser.add_argument('--fullbody_offset_y', type=int, default=0)\n\n    #musetalk opt\n    parser.add_argument('--avatar_id', type=str, default='avator_1')\n    parser.add_argument('--bbox_shift', type=int, default=5)\n    parser.add_argument('--batch_size', type=int, default=16)\n\n    # parser.add_argument('--customvideo', action='store_true', help=\"custom video\")\n    # parser.add_argument('--customvideo_img', type=str, default='data/customvideo/img')\n    # parser.add_argument('--customvideo_imgnum', type=int, default=1)\n\n    parser.add_argument('--customvideo_config', type=str, default='')\n\n    parser.add_argument('--tts', type=str, default='edgetts') #xtts gpt-sovits cosyvoice\n    parser.add_argument('--REF_FILE', type=str, default=None)\n    parser.add_argument('--REF_TEXT', type=str, default=None)\n    parser.add_argument('--TTS_SERVER', type=str, default='http://127.0.0.1:9880') # http://localhost:9000\n    # parser.add_argument('--CHARACTER', type=str, default='test')\n    # parser.add_argument('--EMOTION', type=str, default='default')\n\n    parser.add_argument('--model', type=str, default='ernerf') #musetalk wav2lip\n\n    parser.add_argument('--transport', type=str, default='rtcpush') #rtmp webrtc rtcpush\n    parser.add_argument('--push_url', type=str, default='http://localhost:1985/rtc/v1/whip/?app=live&stream=livestream') #rtmp://localhost/live/livestream\n\n    parser.add_argument('--max_session', type=int, default=1)  #multi session count\n    parser.add_argument('--listenport', type=int, default=8010)\n\n    opt = parser.parse_args()\n    #app.config.from_object(opt)\n    #print(app.config)\n    opt.customopt = []\n    if opt.customvideo_config!='':\n        with open(opt.customvideo_config,'r') as file:\n            opt.customopt = json.load(file)\n\n    if opt.model == 'ernerf':       \n        from nerfreal import NeRFReal,load_model,load_avatar\n        model = load_model(opt)\n        avatar = load_avatar(opt) \n        \n        # we still need test_loader to provide audio features for testing.\n        # for k in range(opt.max_session):\n        #     opt.sessionid=k\n        #     nerfreal = NeRFReal(opt, trainer, test_loader,audio_processor,audio_model)\n        #     nerfreals.append(nerfreal)\n    elif opt.model == 'musetalk':\n        from musereal import MuseReal,load_model,load_avatar,warm_up\n        print(opt)\n        model = load_model()\n        avatar = load_avatar(opt.avatar_id) \n        warm_up(opt.batch_size,model)      \n        # for k in range(opt.max_session):\n        #     opt.sessionid=k\n        #     nerfreal = MuseReal(opt,audio_processor,vae, unet, pe,timesteps)\n        #     nerfreals.append(nerfreal)\n    elif opt.model == 'wav2lip':\n        from lipreal import LipReal,load_model,load_avatar,warm_up\n        print(opt)\n        model = load_model(\"./models/wav2lip.pth\")\n        avatar = load_avatar(opt.avatar_id)\n        warm_up(opt.batch_size,model,96)\n        # for k in range(opt.max_session):\n        #     opt.sessionid=k\n        #     nerfreal = LipReal(opt,model)\n        #     nerfreals.append(nerfreal)\n    elif opt.model == 'ultralight':\n        from lightreal import LightReal,load_model,load_avatar,warm_up\n        print(opt)\n        model = load_model(opt)\n        avatar = load_avatar(opt.avatar_id)\n        warm_up(opt.batch_size,avatar,160)\n\n    if opt.transport=='rtmp':\n        thread_quit = Event()\n        nerfreals[0] = build_nerfreal(0)\n        rendthrd = Thread(target=nerfreals[0].render,args=(thread_quit,))\n        rendthrd.start()\n\n    #############################################################################\n    appasync = web.Application()\n    appasync.on_shutdown.append(on_shutdown)\n    appasync.router.add_post(\"/offer\", offer)\n    appasync.router.add_post(\"/human\", human)\n    appasync.router.add_post(\"/humanaudio\", humanaudio)\n    appasync.router.add_post(\"/set_audiotype\", set_audiotype)\n    appasync.router.add_post(\"/record\", record)\n    appasync.router.add_post(\"/is_speaking\", is_speaking)\n    appasync.router.add_static('/',path='web')\n\n    # Configure default CORS settings.\n    cors = aiohttp_cors.setup(appasync, defaults={\n            \"*\": aiohttp_cors.ResourceOptions(\n                allow_credentials=True,\n                expose_headers=\"*\",\n                allow_headers=\"*\",\n            )\n        })\n    # Configure CORS on all routes.\n    for route in list(appasync.router.routes()):\n        cors.add(route)\n\n    pagename='webrtcapi.html'\n    if opt.transport=='rtmp':\n        pagename='echoapi.html'\n    elif opt.transport=='rtcpush':\n        pagename='rtcpushapi.html'\n    print('start http server; http://<serverip>:'+str(opt.listenport)+'/'+pagename)\n    def run_server(runner):\n        loop = asyncio.new_event_loop()\n        asyncio.set_event_loop(loop)\n        loop.run_until_complete(runner.setup())\n        site = web.TCPSite(runner, '0.0.0.0', opt.listenport)\n        loop.run_until_complete(site.start())\n        if opt.transport=='rtcpush':\n            for k in range(opt.max_session):\n                push_url = opt.push_url\n                if k!=0:\n                    push_url = opt.push_url+str(k)\n                loop.run_until_complete(run(push_url,k))\n        loop.run_forever()    \n    #Thread(target=run_server, args=(web.AppRunner(appasync),)).start()\n    run_server(web.AppRunner(appasync))\n\n    #app.on_shutdown.append(on_shutdown)\n    #app.router.add_post(\"/offer\", offer)\n\n    # print('start websocket server')\n    # server = pywsgi.WSGIServer(('0.0.0.0', 8000), app, handler_class=WebSocketHandler)\n    # server.serve_forever()\n    \n    \n"
        },
        {
          "name": "assets",
          "type": "tree",
          "content": null
        },
        {
          "name": "baseasr.py",
          "type": "blob",
          "size": 2.814453125,
          "content": "###############################################################################\n#  Copyright (C) 2024 LiveTalking@lipku https://github.com/lipku/LiveTalking\n#  email: lipku@foxmail.com\n# \n#  Licensed under the Apache License, Version 2.0 (the \"License\");\n#  you may not use this file except in compliance with the License.\n#  You may obtain a copy of the License at\n#  \n#       http://www.apache.org/licenses/LICENSE-2.0\n# \n#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an \"AS IS\" BASIS,\n#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#  See the License for the specific language governing permissions and\n#  limitations under the License.\n###############################################################################\n\nimport time\nimport numpy as np\n\nimport queue\nfrom queue import Queue\nimport torch.multiprocessing as mp\n\n\nclass BaseASR:\n    def __init__(self, opt, parent=None):\n        self.opt = opt\n        self.parent = parent\n\n        self.fps = opt.fps # 20 ms per frame\n        self.sample_rate = 16000\n        self.chunk = self.sample_rate // self.fps # 320 samples per chunk (20ms * 16000 / 1000)\n        self.queue = Queue()\n        self.output_queue = mp.Queue()\n\n        self.batch_size = opt.batch_size\n\n        self.frames = []\n        self.stride_left_size = opt.l\n        self.stride_right_size = opt.r\n        #self.context_size = 10\n        self.feat_queue = mp.Queue(2)\n\n        #self.warm_up()\n\n    def flush_talk(self):\n        self.queue.queue.clear()\n\n    def put_audio_frame(self,audio_chunk): #16khz 20ms pcm\n        self.queue.put(audio_chunk)\n\n    def get_audio_frame(self):        \n        try:\n            frame = self.queue.get(block=True,timeout=0.01)\n            type = 0\n            #print(f'[INFO] get frame {frame.shape}')\n        except queue.Empty:\n            if self.parent and self.parent.curr_state>1: #播放自定义音频\n                frame = self.parent.get_audio_stream(self.parent.curr_state)\n                type = self.parent.curr_state\n            else:\n                frame = np.zeros(self.chunk, dtype=np.float32)\n                type = 1\n\n        return frame,type \n\n    def is_audio_frame_empty(self)->bool:\n        return self.queue.empty()\n\n    def get_audio_out(self):  #get origin audio pcm to nerf\n        return self.output_queue.get()\n    \n    def warm_up(self):\n        for _ in range(self.stride_left_size + self.stride_right_size):\n            audio_frame,type=self.get_audio_frame()\n            self.frames.append(audio_frame)\n            self.output_queue.put((audio_frame,type))\n        for _ in range(self.stride_left_size):\n            self.output_queue.get()\n\n    def run_step(self):\n        pass\n\n    def get_next_feat(self,block,timeout):        \n        return self.feat_queue.get(block,timeout)"
        },
        {
          "name": "basereal.py",
          "type": "blob",
          "size": 9.9755859375,
          "content": "###############################################################################\n#  Copyright (C) 2024 LiveTalking@lipku https://github.com/lipku/LiveTalking\n#  email: lipku@foxmail.com\n# \n#  Licensed under the Apache License, Version 2.0 (the \"License\");\n#  you may not use this file except in compliance with the License.\n#  You may obtain a copy of the License at\n#  \n#       http://www.apache.org/licenses/LICENSE-2.0\n# \n#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an \"AS IS\" BASIS,\n#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#  See the License for the specific language governing permissions and\n#  limitations under the License.\n###############################################################################\n\nimport math\nimport torch\nimport numpy as np\n\nimport subprocess\nimport os\nimport time\nimport cv2\nimport glob\nimport resampy\n\nimport queue\nfrom queue import Queue\nfrom threading import Thread, Event\nfrom io import BytesIO\nimport soundfile as sf\n\nimport av\nfrom fractions import Fraction\n\nfrom ttsreal import EdgeTTS,VoitsTTS,XTTS,CosyVoiceTTS\n\nfrom tqdm import tqdm\ndef read_imgs(img_list):\n    frames = []\n    print('reading images...')\n    for img_path in tqdm(img_list):\n        frame = cv2.imread(img_path)\n        frames.append(frame)\n    return frames\n\nclass BaseReal:\n    def __init__(self, opt):\n        self.opt = opt\n        self.sample_rate = 16000\n        self.chunk = self.sample_rate // opt.fps # 320 samples per chunk (20ms * 16000 / 1000)\n        self.sessionid = self.opt.sessionid\n\n        if opt.tts == \"edgetts\":\n            self.tts = EdgeTTS(opt,self)\n        elif opt.tts == \"gpt-sovits\":\n            self.tts = VoitsTTS(opt,self)\n        elif opt.tts == \"xtts\":\n            self.tts = XTTS(opt,self)\n        elif opt.tts == \"cosyvoice\":\n            self.tts = CosyVoiceTTS(opt,self)\n        \n        self.speaking = False\n\n        self.recording = False\n        self._record_video_pipe = None\n        self._record_audio_pipe = None\n        self.width = self.height = 0\n\n        self.curr_state=0\n        self.custom_img_cycle = {}\n        self.custom_audio_cycle = {}\n        self.custom_audio_index = {}\n        self.custom_index = {}\n        self.custom_opt = {}\n        self.__loadcustom()\n\n    def put_msg_txt(self,msg):\n        self.tts.put_msg_txt(msg)\n    \n    def put_audio_frame(self,audio_chunk): #16khz 20ms pcm\n        self.asr.put_audio_frame(audio_chunk)\n\n    def put_audio_file(self,filebyte): \n        input_stream = BytesIO(filebyte)\n        stream = self.__create_bytes_stream(input_stream)\n        streamlen = stream.shape[0]\n        idx=0\n        while streamlen >= self.chunk:  #and self.state==State.RUNNING\n            self.put_audio_frame(stream[idx:idx+self.chunk])\n            streamlen -= self.chunk\n            idx += self.chunk\n    \n    def __create_bytes_stream(self,byte_stream):\n        #byte_stream=BytesIO(buffer)\n        stream, sample_rate = sf.read(byte_stream) # [T*sample_rate,] float64\n        print(f'[INFO]put audio stream {sample_rate}: {stream.shape}')\n        stream = stream.astype(np.float32)\n\n        if stream.ndim > 1:\n            print(f'[WARN] audio has {stream.shape[1]} channels, only use the first.')\n            stream = stream[:, 0]\n    \n        if sample_rate != self.sample_rate and stream.shape[0]>0:\n            print(f'[WARN] audio sample rate is {sample_rate}, resampling into {self.sample_rate}.')\n            stream = resampy.resample(x=stream, sr_orig=sample_rate, sr_new=self.sample_rate)\n\n        return stream\n\n    def flush_talk(self):\n        self.tts.flush_talk()\n        self.asr.flush_talk()\n\n    def is_speaking(self)->bool:\n        return self.speaking\n    \n    def __loadcustom(self):\n        for item in self.opt.customopt:\n            print(item)\n            input_img_list = glob.glob(os.path.join(item['imgpath'], '*.[jpJP][pnPN]*[gG]'))\n            input_img_list = sorted(input_img_list, key=lambda x: int(os.path.splitext(os.path.basename(x))[0]))\n            self.custom_img_cycle[item['audiotype']] = read_imgs(input_img_list)\n            self.custom_audio_cycle[item['audiotype']], sample_rate = sf.read(item['audiopath'], dtype='float32')\n            self.custom_audio_index[item['audiotype']] = 0\n            self.custom_index[item['audiotype']] = 0\n            self.custom_opt[item['audiotype']] = item\n\n    def init_customindex(self):\n        self.curr_state=0\n        for key in self.custom_audio_index:\n            self.custom_audio_index[key]=0\n        for key in self.custom_index:\n            self.custom_index[key]=0\n\n    def start_recording(self):\n        \"\"\"开始录制视频\"\"\"\n        if self.recording:\n            return\n\n        command = ['ffmpeg',\n                    '-y', '-an',\n                    '-f', 'rawvideo',\n                    '-vcodec','rawvideo',\n                    '-pix_fmt', 'bgr24', #像素格式\n                    '-s', \"{}x{}\".format(self.width, self.height),\n                    '-r', str(25),\n                    '-i', '-',\n                    '-pix_fmt', 'yuv420p', \n                    '-vcodec', \"h264\",\n                    #'-f' , 'flv',                  \n                    f'temp{self.opt.sessionid}.mp4']\n        self._record_video_pipe = subprocess.Popen(command, shell=False, stdin=subprocess.PIPE)\n\n        acommand = ['ffmpeg',\n                    '-y', '-vn',\n                    '-f', 's16le',\n                    #'-acodec','pcm_s16le',\n                    '-ac', '1',\n                    '-ar', '16000',\n                    '-i', '-',\n                    '-acodec', 'aac',\n                    #'-f' , 'wav',                  \n                    f'temp{self.opt.sessionid}.aac']\n        self._record_audio_pipe = subprocess.Popen(acommand, shell=False, stdin=subprocess.PIPE)\n\n        self.recording = True\n        # self.recordq_video.queue.clear()\n        # self.recordq_audio.queue.clear()\n        # self.container = av.open(path, mode=\"w\")\n    \n        # process_thread = Thread(target=self.record_frame, args=())\n        # process_thread.start()\n    \n    def record_video_data(self,image):\n        if self.width == 0:\n            print(\"image.shape:\",image.shape)\n            self.height,self.width,_ = image.shape\n        if self.recording:\n            self._record_video_pipe.stdin.write(image.tostring())\n\n    def record_audio_data(self,frame):\n        if self.recording:\n            self._record_audio_pipe.stdin.write(frame.tostring())\n    \n    # def record_frame(self): \n    #     videostream = self.container.add_stream(\"libx264\", rate=25)\n    #     videostream.codec_context.time_base = Fraction(1, 25)\n    #     audiostream = self.container.add_stream(\"aac\")\n    #     audiostream.codec_context.time_base = Fraction(1, 16000)\n    #     init = True\n    #     framenum = 0       \n    #     while self.recording:\n    #         try:\n    #             videoframe = self.recordq_video.get(block=True, timeout=1)\n    #             videoframe.pts = framenum #int(round(framenum*0.04 / videostream.codec_context.time_base))\n    #             videoframe.dts = videoframe.pts\n    #             if init:\n    #                 videostream.width = videoframe.width\n    #                 videostream.height = videoframe.height\n    #                 init = False\n    #             for packet in videostream.encode(videoframe):\n    #                 self.container.mux(packet)\n    #             for k in range(2):\n    #                 audioframe = self.recordq_audio.get(block=True, timeout=1)\n    #                 audioframe.pts = int(round((framenum*2+k)*0.02 / audiostream.codec_context.time_base))\n    #                 audioframe.dts = audioframe.pts\n    #                 for packet in audiostream.encode(audioframe):\n    #                     self.container.mux(packet)\n    #             framenum += 1\n    #         except queue.Empty:\n    #             print('record queue empty,')\n    #             continue\n    #         except Exception as e:\n    #             print(e)\n    #             #break\n    #     for packet in videostream.encode(None):\n    #         self.container.mux(packet)\n    #     for packet in audiostream.encode(None):\n    #         self.container.mux(packet)\n    #     self.container.close()\n    #     self.recordq_video.queue.clear()\n    #     self.recordq_audio.queue.clear()\n    #     print('record thread stop')\n\t\t\n    def stop_recording(self):\n        \"\"\"停止录制视频\"\"\"\n        if not self.recording:\n            return\n        self.recording = False \n        self._record_video_pipe.stdin.close()  #wait() \n        self._record_video_pipe.wait()\n        self._record_audio_pipe.stdin.close()\n        self._record_audio_pipe.wait()\n        cmd_combine_audio = f\"ffmpeg -y -i temp{self.opt.sessionid}.aac -i temp{self.opt.sessionid}.mp4 -c:v copy -c:a copy data/record.mp4\"\n        os.system(cmd_combine_audio) \n        #os.remove(output_path)\n\n    def mirror_index(self,size, index):\n        #size = len(self.coord_list_cycle)\n        turn = index // size\n        res = index % size\n        if turn % 2 == 0:\n            return res\n        else:\n            return size - res - 1 \n    \n    def get_audio_stream(self,audiotype):\n        idx = self.custom_audio_index[audiotype]\n        stream = self.custom_audio_cycle[audiotype][idx:idx+self.chunk]\n        self.custom_audio_index[audiotype] += self.chunk\n        if self.custom_audio_index[audiotype]>=self.custom_audio_cycle[audiotype].shape[0]:\n            self.curr_state = 1  #当前视频不循环播放，切换到静音状态\n        return stream\n    \n    def set_curr_state(self,audiotype, reinit):\n        print('set_curr_state:',audiotype)\n        self.curr_state = audiotype\n        if reinit:\n            self.custom_audio_index[audiotype] = 0\n            self.custom_index[audiotype] = 0\n    \n    # def process_custom(self,audiotype:int,idx:int):\n    #     if self.curr_state!=audiotype: #从推理切到口播\n    #         if idx in self.switch_pos:  #在卡点位置可以切换\n    #             self.curr_state=audiotype\n    #             self.custom_index=0\n    #     else:\n    #         self.custom_index+=1"
        },
        {
          "name": "data",
          "type": "tree",
          "content": null
        },
        {
          "name": "ernerf",
          "type": "tree",
          "content": null
        },
        {
          "name": "lightasr.py",
          "type": "blob",
          "size": 1.1845703125,
          "content": "import time\r\nimport torch\r\nimport numpy as np\r\nfrom baseasr import BaseASR\r\n\r\n\r\nclass LightASR(BaseASR):\r\n    def __init__(self, opt, parent, audio_processor):\r\n        super().__init__(opt, parent)\r\n        self.audio_processor = audio_processor\r\n        self.stride_left_size = 32\r\n        self.stride_right_size = 32\r\n\r\n\r\n    def run_step(self):\r\n        start_time = time.time()\r\n        \r\n        for _ in range(self.batch_size * 2):\r\n            audio_frame, type_ = self.get_audio_frame()\r\n            self.frames.append(audio_frame)\r\n            self.output_queue.put((audio_frame, type_))\r\n        \r\n        if len(self.frames) <= self.stride_left_size + self.stride_right_size:\r\n            return\r\n        \r\n        inputs = np.concatenate(self.frames)  # [N * chunk]\r\n\r\n        mel = self.audio_processor.get_hubert_from_16k_speech(inputs)\r\n        mel_chunks=self.audio_processor.feature2chunks(feature_array=mel,fps=self.fps/2,batch_size=self.batch_size,start=self.stride_left_size/2)\r\n\r\n        self.feat_queue.put(mel_chunks)\r\n        self.frames = self.frames[-(self.stride_left_size + self.stride_right_size):]\r\n        #print(f\"Processing audio costs {(time.time() - start_time) * 1000}ms\")\r\n\r\n"
        },
        {
          "name": "lightreal.py",
          "type": "blob",
          "size": 12.875,
          "content": "###############################################################################\n#  Copyright (C) 2024 LiveTalking@lipku https://github.com/lipku/LiveTalking\n#  email: lipku@foxmail.com\n# \n#  Licensed under the Apache License, Version 2.0 (the \"License\");\n#  you may not use this file except in compliance with the License.\n#  You may obtain a copy of the License at\n#  \n#       http://www.apache.org/licenses/LICENSE-2.0\n# \n#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an \"AS IS\" BASIS,\n#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#  See the License for the specific language governing permissions and\n#  limitations under the License.\n###############################################################################\n\nimport math\nimport torch\nimport numpy as np\n\n#from .utils import *\nimport os\nimport time\nimport cv2\nimport glob\nimport pickle\nimport copy\n\nimport queue\nfrom queue import Queue\nfrom threading import Thread, Event\nimport torch.multiprocessing as mp\n\n\nfrom lightasr import LightASR\nimport asyncio\nfrom av import AudioFrame, VideoFrame\nfrom basereal import BaseReal\n\n#from imgcache import ImgCache\n\nfrom tqdm import tqdm\n\n#new\nimport os\nimport cv2\nimport torch\nimport numpy as np\nimport torch.nn as nn\nfrom torch import optim\nfrom tqdm import tqdm\nfrom transformers import Wav2Vec2Processor, HubertModel\nfrom torch.utils.data import DataLoader\nfrom ultralight.unet import Model\nfrom ultralight.audio2feature import Audio2Feature\n\n\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nprint('Using {} for inference.'.format(device))\n\n\ndef load_model(opt):\n    audio_processor = Audio2Feature()\n    return audio_processor\n\ndef load_avatar(avatar_id):\n    avatar_path = f\"./data/avatars/{avatar_id}\"\n    full_imgs_path = f\"{avatar_path}/full_imgs\" \n    face_imgs_path = f\"{avatar_path}/face_imgs\" \n    coords_path = f\"{avatar_path}/coords.pkl\" \n    \n    model = Model(6, 'hubert').to(device)  # 假设Model是你自定义的类\n    model.load_state_dict(torch.load(f\"{avatar_path}/ultralight.pth\"))\n    \n    with open(coords_path, 'rb') as f:\n        coord_list_cycle = pickle.load(f)\n    input_img_list = glob.glob(os.path.join(full_imgs_path, '*.[jpJP][pnPN]*[gG]'))\n    input_img_list = sorted(input_img_list, key=lambda x: int(os.path.splitext(os.path.basename(x))[0]))\n    frame_list_cycle = read_imgs(input_img_list)\n    #self.imagecache = ImgCache(len(self.coord_list_cycle),self.full_imgs_path,1000)\n    input_face_list = glob.glob(os.path.join(face_imgs_path, '*.[jpJP][pnPN]*[gG]'))\n    input_face_list = sorted(input_face_list, key=lambda x: int(os.path.splitext(os.path.basename(x))[0]))\n    face_list_cycle = read_imgs(input_face_list)\n\n    return model.eval(),frame_list_cycle,face_list_cycle,coord_list_cycle\n\n\n@torch.no_grad()\ndef warm_up(batch_size,avatar,modelres):\n    print('warmup model...')\n    model,_,_,_ = avatar\n    img_batch = torch.ones(batch_size, 6, modelres, modelres).to(device)\n    mel_batch = torch.ones(batch_size, 32, 32, 32).to(device)\n    model(img_batch, mel_batch)\n\ndef read_imgs(img_list):\n    frames = []\n    print('reading images...')\n    for img_path in tqdm(img_list):\n        frame = cv2.imread(img_path)\n        frames.append(frame)\n    return frames\n\ndef get_audio_features(features, index):\n    left = index - 8\n    right = index + 8\n    pad_left = 0\n    pad_right = 0\n    if left < 0:\n        pad_left = -left\n        left = 0\n    if right > features.shape[0]:\n        pad_right = right - features.shape[0]\n        right = features.shape[0]\n    auds = torch.from_numpy(features[left:right])\n    if pad_left > 0:\n        auds = torch.cat([torch.zeros_like(auds[:pad_left]), auds], dim=0)\n    if pad_right > 0:\n        auds = torch.cat([auds, torch.zeros_like(auds[:pad_right])], dim=0) # [8, 16]\n    return auds\n\n\ndef read_lms(lms_list):\n    land_marks = []\n    print('reading lms...')\n    for lms_path in tqdm(lms_list):\n        file_landmarks = []  # Store landmarks for this file\n        with open(lms_path, \"r\") as f:\n            lines = f.read().splitlines()\n            for line in lines:\n                arr = list(filter(None, line.split(\" \")))\n                if arr:\n                    arr = np.array(arr, dtype=np.float32)\n                    file_landmarks.append(arr)\n        land_marks.append(file_landmarks)  # Add the file's landmarks to the overall list\n    return land_marks\n\ndef __mirror_index(size, index):\n    #size = len(self.coord_list_cycle)\n    turn = index // size\n    res = index % size\n    if turn % 2 == 0:\n        return res\n    else:\n        return size - res - 1 \n\n\ndef inference(quit_event, batch_size, face_list_cycle, audio_feat_queue, audio_out_queue, res_frame_queue, model):\n    length = len(face_list_cycle)\n    index = 0\n    count = 0\n    counttime = 0\n    print('start inference')\n\n    while not quit_event.is_set():\n        starttime=time.perf_counter()\n        try:\n            mel_batch = audio_feat_queue.get(block=True, timeout=1)\n        except queue.Empty:\n            continue\n        is_all_silence=True\n        audio_frames = []\n        for _ in range(batch_size*2):\n            frame,type_ = audio_out_queue.get()\n            audio_frames.append((frame,type_))\n            if type_==0:\n                is_all_silence=False\n        if is_all_silence:\n            for i in range(batch_size):\n                res_frame_queue.put((None,__mirror_index(length,index),audio_frames[i*2:i*2+2]))\n                index = index + 1\n        else:\n            t = time.perf_counter()\n            img_batch = []\n\n            for i in range(batch_size):\n                idx = __mirror_index(length, index + i)\n                #face = face_list_cycle[idx]\n                crop_img = face_list_cycle[idx] #face[ymin:ymax, xmin:xmax]\n#                h, w = crop_img.shape[:2]\n                #crop_img = cv2.resize(crop_img, (168, 168), cv2.INTER_AREA)\n                #crop_img_ori = crop_img.copy()\n                img_real_ex = crop_img[4:164, 4:164].copy()\n                img_real_ex_ori = img_real_ex.copy()\n                img_masked = cv2.rectangle(img_real_ex_ori,(5,5,150,145),(0,0,0),-1)\n    \n                img_masked = img_masked.transpose(2,0,1).astype(np.float32)\n                img_real_ex = img_real_ex.transpose(2,0,1).astype(np.float32)\n    \n                img_real_ex_T = torch.from_numpy(img_real_ex / 255.0)\n                img_masked_T = torch.from_numpy(img_masked / 255.0)\n                img_concat_T = torch.cat([img_real_ex_T, img_masked_T], axis=0)[None]\n                img_batch.append(img_concat_T)\n\n            reshaped_mel_batch = [arr.reshape(32, 32, 32) for arr in mel_batch]\n            mel_batch = torch.stack([torch.from_numpy(arr) for arr in reshaped_mel_batch])\n            img_batch = torch.stack(img_batch).squeeze(1)\n\n\n            with torch.no_grad():\n                pred = model(img_batch.cuda(),mel_batch.cuda())\n            pred = pred.cpu().numpy().transpose(0, 2, 3, 1) * 255.\n\n            counttime += (time.perf_counter() - t)\n            count += batch_size\n            if count >= 100:\n                print(f\"------actual avg infer fps:{count / counttime:.4f}\")\n                count = 0\n                counttime = 0\n            for i,res_frame in enumerate(pred):\n                #self.__pushmedia(res_frame,loop,audio_track,video_track)\n                res_frame_queue.put((res_frame,__mirror_index(length,index),audio_frames[i*2:i*2+2]))\n                index = index + 1\n\n#            for i, pred_frame in enumerate(pred):\n#                pred_frame_uint8 = np.array(pred_frame, dtype=np.uint8)\n#                res_frame_queue.put((pred_frame_uint8, __mirror_index(length, index), audio_frames[i * 2:i * 2 + 2]))\n#                index = (index + 1) % length\n\n        #print('total batch time:', time.perf_counter() - starttime)\n\n    print('lightreal inference processor stop')\n\n\nclass LightReal(BaseReal):\n    @torch.no_grad()\n    def __init__(self, opt, model, avatar):\n        super().__init__(opt)\n        #self.opt = opt # shared with the trainer's opt to support in-place modification of rendering parameters.\n        self.W = opt.W\n        self.H = opt.H\n\n        self.fps = opt.fps # 20 ms per frame\n        \n        self.batch_size = opt.batch_size\n        self.idx = 0\n        self.res_frame_queue = Queue(self.batch_size*2)  #mp.Queue\n        #self.__loadavatar()\n        audio_processor = model\n        self.model,self.frame_list_cycle,self.face_list_cycle,self.coord_list_cycle = avatar\n\n        self.asr = LightASR(opt,self,audio_processor)\n        self.asr.warm_up()\n        #self.__warm_up()\n        \n        self.render_event = mp.Event()\n    \n    def __del__(self):\n        print(f'lightreal({self.sessionid}) delete')\n\n   \n    def process_frames(self,quit_event,loop=None,audio_track=None,video_track=None):\n        \n        while not quit_event.is_set():\n            try:\n                res_frame,idx,audio_frames = self.res_frame_queue.get(block=True, timeout=1)\n            except queue.Empty:\n                continue\n            if audio_frames[0][1]!=0 and audio_frames[1][1]!=0: #全为静音数据，只需要取fullimg\n                self.speaking = False\n                audiotype = audio_frames[0][1]\n                if self.custom_index.get(audiotype) is not None: #有自定义视频\n                    mirindex = self.mirror_index(len(self.custom_img_cycle[audiotype]),self.custom_index[audiotype])\n                    combine_frame = self.custom_img_cycle[audiotype][mirindex]\n                    self.custom_index[audiotype] += 1\n                    # if not self.custom_opt[audiotype].loop and self.custom_index[audiotype]>=len(self.custom_img_cycle[audiotype]):\n                    #     self.curr_state = 1  #当前视频不循环播放，切换到静音状态\n                else:\n                    combine_frame = self.frame_list_cycle[idx]\n                    #combine_frame = self.imagecache.get_img(idx)\n            else:\n                self.speaking = True\n                bbox = self.coord_list_cycle[idx]\n                combine_frame = copy.deepcopy(self.frame_list_cycle[idx])\n                x1, y1, x2, y2 = bbox\n\n                crop_img = self.face_list_cycle[idx]\n                crop_img_ori = crop_img.copy()\n                #res_frame = np.array(res_frame, dtype=np.uint8)\n                try:\n                    crop_img_ori[4:164, 4:164] = res_frame.astype(np.uint8)\n                    crop_img_ori = cv2.resize(crop_img_ori, (x2-x1,y2-y1))\n                except:\n                    continue\n                combine_frame[y1:y2, x1:x2] = crop_img_ori\n                #print('blending time:',time.perf_counter()-t)\n\n            new_frame = VideoFrame.from_ndarray(combine_frame, format=\"bgr24\")\n            asyncio.run_coroutine_threadsafe(video_track._queue.put(new_frame), loop)\n            self.record_video_data(combine_frame)\n\n            for audio_frame in audio_frames:\n                frame,type_ = audio_frame\n                frame = (frame * 32767).astype(np.int16)\n                new_frame = AudioFrame(format='s16', layout='mono', samples=frame.shape[0])\n                new_frame.planes[0].update(frame.tobytes())\n                new_frame.sample_rate=16000\n                # if audio_track._queue.qsize()>10:\n                #     time.sleep(0.1)\n                asyncio.run_coroutine_threadsafe(audio_track._queue.put(new_frame), loop)\n                self.record_audio_data(frame)\n        print('lightreal process_frames thread stop') \n            \n    def render(self,quit_event,loop=None,audio_track=None,video_track=None):\n        #if self.opt.asr:\n        #     self.asr.warm_up()\n\n        self.tts.render(quit_event)\n        self.init_customindex()\n        process_thread = Thread(target=self.process_frames, args=(quit_event,loop,audio_track,video_track))\n        process_thread.start()\n        Thread(target=inference, args=(quit_event,self.batch_size,self.face_list_cycle,self.asr.feat_queue,self.asr.output_queue,self.res_frame_queue,\n                                           self.model,)).start()  #mp.Process\n        \n\n        #self.render_event.set() #start infer process render\n        count=0\n        totaltime=0\n        _starttime=time.perf_counter()\n        #_totalframe=0\n        while not quit_event.is_set(): \n            # update texture every frame\n            # audio stream thread...\n            t = time.perf_counter()\n            self.asr.run_step()\n\n            # if video_track._queue.qsize()>=2*self.opt.batch_size:\n            #     print('sleep qsize=',video_track._queue.qsize())\n            #     time.sleep(0.04*video_track._queue.qsize()*0.8)\n            if video_track._queue.qsize()>=5:\n                print('sleep qsize=',video_track._queue.qsize())\n                time.sleep(0.04*video_track._queue.qsize()*0.8)\n                \n            # delay = _starttime+_totalframe*0.04-time.perf_counter() #40ms\n            # if delay > 0:\n            #     time.sleep(delay)\n        #self.render_event.clear() #end infer process render\n        print('lightreal thread stop')\n            \n\n"
        },
        {
          "name": "lipasr.py",
          "type": "blob",
          "size": 2.5322265625,
          "content": "###############################################################################\n#  Copyright (C) 2024 LiveTalking@lipku https://github.com/lipku/LiveTalking\n#  email: lipku@foxmail.com\n# \n#  Licensed under the Apache License, Version 2.0 (the \"License\");\n#  you may not use this file except in compliance with the License.\n#  You may obtain a copy of the License at\n#  \n#       http://www.apache.org/licenses/LICENSE-2.0\n# \n#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an \"AS IS\" BASIS,\n#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#  See the License for the specific language governing permissions and\n#  limitations under the License.\n###############################################################################\n\nimport time\nimport torch\nimport numpy as np\n\nimport queue\nfrom queue import Queue\n#import multiprocessing as mp\n\nfrom baseasr import BaseASR\nfrom wav2lip import audio\n\nclass LipASR(BaseASR):\n\n    def run_step(self):\n        ############################################## extract audio feature ##############################################\n        # get a frame of audio\n        for _ in range(self.batch_size*2):\n            frame,type = self.get_audio_frame()\n            self.frames.append(frame)\n            # put to output\n            self.output_queue.put((frame,type))\n        # context not enough, do not run network.\n        if len(self.frames) <= self.stride_left_size + self.stride_right_size:\n            return\n        \n        inputs = np.concatenate(self.frames) # [N * chunk]\n        mel = audio.melspectrogram(inputs)\n        #print(mel.shape[0],mel.shape,len(mel[0]),len(self.frames))\n        # cut off stride\n        left = max(0, self.stride_left_size*80/50)\n        right = min(len(mel[0]), len(mel[0]) - self.stride_right_size*80/50)\n        mel_idx_multiplier = 80.*2/self.fps \n        mel_step_size = 16\n        i = 0\n        mel_chunks = []\n        while i < (len(self.frames)-self.stride_left_size-self.stride_right_size)/2:\n            start_idx = int(left + i * mel_idx_multiplier)\n            #print(start_idx)\n            if start_idx + mel_step_size > len(mel[0]):\n                mel_chunks.append(mel[:, len(mel[0]) - mel_step_size:])\n            else:\n                mel_chunks.append(mel[:, start_idx : start_idx + mel_step_size])\n            i += 1\n        self.feat_queue.put(mel_chunks)\n        \n        # discard the old part to save memory\n        self.frames = self.frames[-(self.stride_left_size + self.stride_right_size):]\n"
        },
        {
          "name": "lipreal.py",
          "type": "blob",
          "size": 11.6484375,
          "content": "###############################################################################\r\n#  Copyright (C) 2024 LiveTalking@lipku https://github.com/lipku/LiveTalking\r\n#  email: lipku@foxmail.com\r\n# \r\n#  Licensed under the Apache License, Version 2.0 (the \"License\");\r\n#  you may not use this file except in compliance with the License.\r\n#  You may obtain a copy of the License at\r\n#  \r\n#       http://www.apache.org/licenses/LICENSE-2.0\r\n# \r\n#  Unless required by applicable law or agreed to in writing, software\r\n#  distributed under the License is distributed on an \"AS IS\" BASIS,\r\n#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n#  See the License for the specific language governing permissions and\r\n#  limitations under the License.\r\n###############################################################################\r\n\r\nimport math\r\nimport torch\r\nimport numpy as np\r\n\r\n#from .utils import *\r\nimport os\r\nimport time\r\nimport cv2\r\nimport glob\r\nimport pickle\r\nimport copy\r\n\r\nimport queue\r\nfrom queue import Queue\r\nfrom threading import Thread, Event\r\nimport torch.multiprocessing as mp\r\n\r\n\r\nfrom lipasr import LipASR\r\nimport asyncio\r\nfrom av import AudioFrame, VideoFrame\r\nfrom wav2lip.models import Wav2Lip\r\nfrom basereal import BaseReal\r\n\r\n#from imgcache import ImgCache\r\n\r\nfrom tqdm import tqdm\r\n\r\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\r\nprint('Using {} for inference.'.format(device))\r\n\r\ndef _load(checkpoint_path):\r\n\tif device == 'cuda':\r\n\t\tcheckpoint = torch.load(checkpoint_path) #,weights_only=True\r\n\telse:\r\n\t\tcheckpoint = torch.load(checkpoint_path,\r\n\t\t\t\t\t\t\t\tmap_location=lambda storage, loc: storage)\r\n\treturn checkpoint\r\n\r\ndef load_model(path):\r\n\tmodel = Wav2Lip()\r\n\tprint(\"Load checkpoint from: {}\".format(path))\r\n\tcheckpoint = _load(path)\r\n\ts = checkpoint[\"state_dict\"]\r\n\tnew_s = {}\r\n\tfor k, v in s.items():\r\n\t\tnew_s[k.replace('module.', '')] = v\r\n\tmodel.load_state_dict(new_s)\r\n\r\n\tmodel = model.to(device)\r\n\treturn model.eval()\r\n\r\ndef load_avatar(avatar_id):\r\n    avatar_path = f\"./data/avatars/{avatar_id}\"\r\n    full_imgs_path = f\"{avatar_path}/full_imgs\" \r\n    face_imgs_path = f\"{avatar_path}/face_imgs\" \r\n    coords_path = f\"{avatar_path}/coords.pkl\"\r\n    \r\n    with open(coords_path, 'rb') as f:\r\n        coord_list_cycle = pickle.load(f)\r\n    input_img_list = glob.glob(os.path.join(full_imgs_path, '*.[jpJP][pnPN]*[gG]'))\r\n    input_img_list = sorted(input_img_list, key=lambda x: int(os.path.splitext(os.path.basename(x))[0]))\r\n    frame_list_cycle = read_imgs(input_img_list)\r\n    #self.imagecache = ImgCache(len(self.coord_list_cycle),self.full_imgs_path,1000)\r\n    input_face_list = glob.glob(os.path.join(face_imgs_path, '*.[jpJP][pnPN]*[gG]'))\r\n    input_face_list = sorted(input_face_list, key=lambda x: int(os.path.splitext(os.path.basename(x))[0]))\r\n    face_list_cycle = read_imgs(input_face_list)\r\n\r\n    return frame_list_cycle,face_list_cycle,coord_list_cycle\r\n\r\n@torch.no_grad()\r\ndef warm_up(batch_size,model,modelres):\r\n    # 预热函数\r\n    print('warmup model...')\r\n    img_batch = torch.ones(batch_size, 6, modelres, modelres).to(device)\r\n    mel_batch = torch.ones(batch_size, 1, 80, 16).to(device)\r\n    model(mel_batch, img_batch)\r\n\r\ndef read_imgs(img_list):\r\n    frames = []\r\n    print('reading images...')\r\n    for img_path in tqdm(img_list):\r\n        frame = cv2.imread(img_path)\r\n        frames.append(frame)\r\n    return frames\r\n\r\ndef __mirror_index(size, index):\r\n    #size = len(self.coord_list_cycle)\r\n    turn = index // size\r\n    res = index % size\r\n    if turn % 2 == 0:\r\n        return res\r\n    else:\r\n        return size - res - 1 \r\n\r\ndef inference(quit_event,batch_size,face_list_cycle,audio_feat_queue,audio_out_queue,res_frame_queue,model):\r\n    \r\n    #model = load_model(\"./models/wav2lip.pth\")\r\n    # input_face_list = glob.glob(os.path.join(face_imgs_path, '*.[jpJP][pnPN]*[gG]'))\r\n    # input_face_list = sorted(input_face_list, key=lambda x: int(os.path.splitext(os.path.basename(x))[0]))\r\n    # face_list_cycle = read_imgs(input_face_list)\r\n    \r\n    #input_latent_list_cycle = torch.load(latents_out_path)\r\n    length = len(face_list_cycle)\r\n    index = 0\r\n    count=0\r\n    counttime=0\r\n    print('start inference')\r\n    while not quit_event.is_set():\r\n        starttime=time.perf_counter()\r\n        mel_batch = []\r\n        try:\r\n            mel_batch = audio_feat_queue.get(block=True, timeout=1)\r\n        except queue.Empty:\r\n            continue\r\n            \r\n        is_all_silence=True\r\n        audio_frames = []\r\n        for _ in range(batch_size*2):\r\n            frame,type = audio_out_queue.get()\r\n            audio_frames.append((frame,type))\r\n            if type==0:\r\n                is_all_silence=False\r\n\r\n        if is_all_silence:\r\n            for i in range(batch_size):\r\n                res_frame_queue.put((None,__mirror_index(length,index),audio_frames[i*2:i*2+2]))\r\n                index = index + 1\r\n        else:\r\n            # print('infer=======')\r\n            t=time.perf_counter()\r\n            img_batch = []\r\n            for i in range(batch_size):\r\n                idx = __mirror_index(length,index+i)\r\n                face = face_list_cycle[idx]\r\n                img_batch.append(face)\r\n            img_batch, mel_batch = np.asarray(img_batch), np.asarray(mel_batch)\r\n\r\n            img_masked = img_batch.copy()\r\n            img_masked[:, face.shape[0]//2:] = 0\r\n\r\n            img_batch = np.concatenate((img_masked, img_batch), axis=3) / 255.\r\n            mel_batch = np.reshape(mel_batch, [len(mel_batch), mel_batch.shape[1], mel_batch.shape[2], 1])\r\n            \r\n            img_batch = torch.FloatTensor(np.transpose(img_batch, (0, 3, 1, 2))).to(device)\r\n            mel_batch = torch.FloatTensor(np.transpose(mel_batch, (0, 3, 1, 2))).to(device)\r\n\r\n            with torch.no_grad():\r\n                pred = model(mel_batch, img_batch)\r\n            pred = pred.cpu().numpy().transpose(0, 2, 3, 1) * 255.\r\n\r\n            counttime += (time.perf_counter() - t)\r\n            count += batch_size\r\n            #_totalframe += 1\r\n            if count>=100:\r\n                print(f\"------actual avg infer fps:{count/counttime:.4f}\")\r\n                count=0\r\n                counttime=0\r\n            for i,res_frame in enumerate(pred):\r\n                #self.__pushmedia(res_frame,loop,audio_track,video_track)\r\n                res_frame_queue.put((res_frame,__mirror_index(length,index),audio_frames[i*2:i*2+2]))\r\n                index = index + 1\r\n            #print('total batch time:',time.perf_counter()-starttime)            \r\n    print('lipreal inference processor stop')\r\n\r\nclass LipReal(BaseReal):\r\n    @torch.no_grad()\r\n    def __init__(self, opt, model, avatar):\r\n        super().__init__(opt)\r\n        #self.opt = opt # shared with the trainer's opt to support in-place modification of rendering parameters.\r\n        self.W = opt.W\r\n        self.H = opt.H\r\n\r\n        self.fps = opt.fps # 20 ms per frame\r\n        \r\n        self.batch_size = opt.batch_size\r\n        self.idx = 0\r\n        self.res_frame_queue = Queue(self.batch_size*2)  #mp.Queue\r\n        #self.__loadavatar()\r\n        self.model = model\r\n        self.frame_list_cycle,self.face_list_cycle,self.coord_list_cycle = avatar\r\n\r\n        self.asr = LipASR(opt,self)\r\n        self.asr.warm_up()\r\n        \r\n        self.render_event = mp.Event()\r\n    \r\n    def __del__(self):\r\n        print(f'lipreal({self.sessionid}) delete')\r\n\r\n   \r\n    def process_frames(self,quit_event,loop=None,audio_track=None,video_track=None):\r\n        \r\n        while not quit_event.is_set():\r\n            try:\r\n                res_frame,idx,audio_frames = self.res_frame_queue.get(block=True, timeout=1)\r\n            except queue.Empty:\r\n                continue\r\n            if audio_frames[0][1]!=0 and audio_frames[1][1]!=0: #全为静音数据，只需要取fullimg\r\n                self.speaking = False\r\n                audiotype = audio_frames[0][1]\r\n                if self.custom_index.get(audiotype) is not None: #有自定义视频\r\n                    mirindex = self.mirror_index(len(self.custom_img_cycle[audiotype]),self.custom_index[audiotype])\r\n                    combine_frame = self.custom_img_cycle[audiotype][mirindex]\r\n                    self.custom_index[audiotype] += 1\r\n                    # if not self.custom_opt[audiotype].loop and self.custom_index[audiotype]>=len(self.custom_img_cycle[audiotype]):\r\n                    #     self.curr_state = 1  #当前视频不循环播放，切换到静音状态\r\n                else:\r\n                    combine_frame = self.frame_list_cycle[idx]\r\n                    #combine_frame = self.imagecache.get_img(idx)\r\n            else:\r\n                self.speaking = True\r\n                bbox = self.coord_list_cycle[idx]\r\n                combine_frame = copy.deepcopy(self.frame_list_cycle[idx])\r\n                #combine_frame = copy.deepcopy(self.imagecache.get_img(idx))\r\n                y1, y2, x1, x2 = bbox\r\n                try:\r\n                    res_frame = cv2.resize(res_frame.astype(np.uint8),(x2-x1,y2-y1))\r\n                except:\r\n                    continue\r\n                #combine_frame = get_image(ori_frame,res_frame,bbox)\r\n                #t=time.perf_counter()\r\n                combine_frame[y1:y2, x1:x2] = res_frame\r\n                #print('blending time:',time.perf_counter()-t)\r\n\r\n            image = combine_frame #(outputs['image'] * 255).astype(np.uint8)\r\n            new_frame = VideoFrame.from_ndarray(image, format=\"bgr24\")\r\n            asyncio.run_coroutine_threadsafe(video_track._queue.put(new_frame), loop)\r\n            self.record_video_data(image)\r\n\r\n            for audio_frame in audio_frames:\r\n                frame,type = audio_frame\r\n                frame = (frame * 32767).astype(np.int16)\r\n                new_frame = AudioFrame(format='s16', layout='mono', samples=frame.shape[0])\r\n                new_frame.planes[0].update(frame.tobytes())\r\n                new_frame.sample_rate=16000\r\n                # if audio_track._queue.qsize()>10:\r\n                #     time.sleep(0.1)\r\n                asyncio.run_coroutine_threadsafe(audio_track._queue.put(new_frame), loop)\r\n                self.record_audio_data(frame)\r\n        print('lipreal process_frames thread stop') \r\n            \r\n    def render(self,quit_event,loop=None,audio_track=None,video_track=None):\r\n        #if self.opt.asr:\r\n        #     self.asr.warm_up()\r\n\r\n        self.tts.render(quit_event)\r\n        self.init_customindex()\r\n        process_thread = Thread(target=self.process_frames, args=(quit_event,loop,audio_track,video_track))\r\n        process_thread.start()\r\n\r\n        Thread(target=inference, args=(quit_event,self.batch_size,self.face_list_cycle,\r\n                                           self.asr.feat_queue,self.asr.output_queue,self.res_frame_queue,\r\n                                           self.model,)).start()  #mp.Process\r\n\r\n        #self.render_event.set() #start infer process render\r\n        count=0\r\n        totaltime=0\r\n        _starttime=time.perf_counter()\r\n        #_totalframe=0\r\n        while not quit_event.is_set(): \r\n            # update texture every frame\r\n            # audio stream thread...\r\n            t = time.perf_counter()\r\n            self.asr.run_step()\r\n\r\n            # if video_track._queue.qsize()>=2*self.opt.batch_size:\r\n            #     print('sleep qsize=',video_track._queue.qsize())\r\n            #     time.sleep(0.04*video_track._queue.qsize()*0.8)\r\n            if video_track._queue.qsize()>=5:\r\n                print('sleep qsize=',video_track._queue.qsize())\r\n                time.sleep(0.04*video_track._queue.qsize()*0.8)\r\n                \r\n            # delay = _starttime+_totalframe*0.04-time.perf_counter() #40ms\r\n            # if delay > 0:\r\n            #     time.sleep(delay)\r\n        #self.render_event.clear() #end infer process render\r\n        print('lipreal thread stop')\r\n            "
        },
        {
          "name": "llm",
          "type": "tree",
          "content": null
        },
        {
          "name": "models",
          "type": "tree",
          "content": null
        },
        {
          "name": "museasr.py",
          "type": "blob",
          "size": 2.58203125,
          "content": "###############################################################################\n#  Copyright (C) 2024 LiveTalking@lipku https://github.com/lipku/LiveTalking\n#  email: lipku@foxmail.com\n# \n#  Licensed under the Apache License, Version 2.0 (the \"License\");\n#  you may not use this file except in compliance with the License.\n#  You may obtain a copy of the License at\n#  \n#       http://www.apache.org/licenses/LICENSE-2.0\n# \n#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an \"AS IS\" BASIS,\n#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#  See the License for the specific language governing permissions and\n#  limitations under the License.\n###############################################################################\n\nimport time\nimport numpy as np\n\nimport queue\nfrom queue import Queue\n#import multiprocessing as mp\nfrom baseasr import BaseASR\nfrom musetalk.whisper.audio2feature import Audio2Feature\n\nclass MuseASR(BaseASR):\n    def __init__(self, opt, parent,audio_processor:Audio2Feature):\n        super().__init__(opt,parent)\n        self.audio_processor = audio_processor\n\n    def run_step(self):\n        ############################################## extract audio feature ##############################################\n        start_time = time.time()\n        for _ in range(self.batch_size*2):\n            audio_frame,type=self.get_audio_frame()\n            self.frames.append(audio_frame)\n            self.output_queue.put((audio_frame,type))\n        \n        if len(self.frames) <= self.stride_left_size + self.stride_right_size:\n            return\n        \n        inputs = np.concatenate(self.frames) # [N * chunk]\n        whisper_feature = self.audio_processor.audio2feat(inputs)\n        # for feature in whisper_feature:\n        #     self.audio_feats.append(feature)        \n        #print(f\"processing audio costs {(time.time() - start_time) * 1000}ms, inputs shape:{inputs.shape} whisper_feature len:{len(whisper_feature)}\")\n        whisper_chunks = self.audio_processor.feature2chunks(feature_array=whisper_feature,fps=self.fps/2,batch_size=self.batch_size,start=self.stride_left_size/2 )\n        #print(f\"whisper_chunks len:{len(whisper_chunks)},self.audio_feats len:{len(self.audio_feats)},self.output_queue len:{self.output_queue.qsize()}\")\n        #self.audio_feats = self.audio_feats[-(self.stride_left_size + self.stride_right_size):]\n        self.feat_queue.put(whisper_chunks)\n        # discard the old part to save memory\n        self.frames = self.frames[-(self.stride_left_size + self.stride_right_size):]\n"
        },
        {
          "name": "musereal.py",
          "type": "blob",
          "size": 15.4638671875,
          "content": "###############################################################################\r\n#  Copyright (C) 2024 LiveTalking@lipku https://github.com/lipku/LiveTalking\r\n#  email: lipku@foxmail.com\r\n# \r\n#  Licensed under the Apache License, Version 2.0 (the \"License\");\r\n#  you may not use this file except in compliance with the License.\r\n#  You may obtain a copy of the License at\r\n#  \r\n#       http://www.apache.org/licenses/LICENSE-2.0\r\n# \r\n#  Unless required by applicable law or agreed to in writing, software\r\n#  distributed under the License is distributed on an \"AS IS\" BASIS,\r\n#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n#  See the License for the specific language governing permissions and\r\n#  limitations under the License.\r\n###############################################################################\r\n\r\nimport math\r\nimport torch\r\nimport numpy as np\r\n\r\n#from .utils import *\r\nimport subprocess\r\nimport os\r\nimport time\r\nimport torch.nn.functional as F\r\nimport cv2\r\nimport glob\r\nimport pickle\r\nimport copy\r\n\r\nimport queue\r\nfrom queue import Queue\r\nfrom threading import Thread, Event\r\nimport torch.multiprocessing as mp\r\n\r\nfrom musetalk.utils.utils import get_file_type,get_video_fps,datagen\r\n#from musetalk.utils.preprocessing import get_landmark_and_bbox,read_imgs,coord_placeholder\r\nfrom musetalk.utils.blending import get_image,get_image_prepare_material,get_image_blending\r\nfrom musetalk.utils.utils import load_all_model,load_diffusion_model,load_audio_model\r\nfrom musetalk.whisper.audio2feature import Audio2Feature\r\n\r\nfrom museasr import MuseASR\r\nimport asyncio\r\nfrom av import AudioFrame, VideoFrame\r\nfrom basereal import BaseReal\r\n\r\nfrom tqdm import tqdm\r\n\r\ndef load_model():\r\n    # load model weights\r\n    audio_processor,vae, unet, pe = load_all_model()\r\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\r\n    timesteps = torch.tensor([0], device=device)\r\n    pe = pe.half()\r\n    vae.vae = vae.vae.half()\r\n    #vae.vae.share_memory()\r\n    unet.model = unet.model.half()\r\n    #unet.model.share_memory()\r\n    return vae, unet, pe, timesteps, audio_processor\r\n\r\ndef load_avatar(avatar_id):\r\n    #self.video_path = '' #video_path\r\n    #self.bbox_shift = opt.bbox_shift\r\n    avatar_path = f\"./data/avatars/{avatar_id}\"\r\n    full_imgs_path = f\"{avatar_path}/full_imgs\" \r\n    coords_path = f\"{avatar_path}/coords.pkl\"\r\n    latents_out_path= f\"{avatar_path}/latents.pt\"\r\n    video_out_path = f\"{avatar_path}/vid_output/\"\r\n    mask_out_path =f\"{avatar_path}/mask\"\r\n    mask_coords_path =f\"{avatar_path}/mask_coords.pkl\"\r\n    avatar_info_path = f\"{avatar_path}/avator_info.json\"\r\n    # self.avatar_info = {\r\n    #     \"avatar_id\":self.avatar_id,\r\n    #     \"video_path\":self.video_path,\r\n    #     \"bbox_shift\":self.bbox_shift   \r\n    # }\r\n\r\n    input_latent_list_cycle = torch.load(latents_out_path)  #,weights_only=True\r\n    with open(coords_path, 'rb') as f:\r\n        coord_list_cycle = pickle.load(f)\r\n    input_img_list = glob.glob(os.path.join(full_imgs_path, '*.[jpJP][pnPN]*[gG]'))\r\n    input_img_list = sorted(input_img_list, key=lambda x: int(os.path.splitext(os.path.basename(x))[0]))\r\n    frame_list_cycle = read_imgs(input_img_list)\r\n    with open(mask_coords_path, 'rb') as f:\r\n        mask_coords_list_cycle = pickle.load(f)\r\n    input_mask_list = glob.glob(os.path.join(mask_out_path, '*.[jpJP][pnPN]*[gG]'))\r\n    input_mask_list = sorted(input_mask_list, key=lambda x: int(os.path.splitext(os.path.basename(x))[0]))\r\n    mask_list_cycle = read_imgs(input_mask_list)\r\n    return frame_list_cycle,mask_list_cycle,coord_list_cycle,mask_coords_list_cycle,input_latent_list_cycle\r\n\r\n@torch.no_grad()\r\ndef warm_up(batch_size,model):\r\n    # 预热函数\r\n    print('warmup model...')\r\n    vae, unet, pe, timesteps, audio_processor = model\r\n    #batch_size = 16\r\n    #timesteps = torch.tensor([0], device=unet.device)\r\n    whisper_batch = np.ones((batch_size, 50, 384), dtype=np.uint8)\r\n    latent_batch = torch.ones(batch_size, 8, 32, 32).to(unet.device)\r\n\r\n    audio_feature_batch = torch.from_numpy(whisper_batch)\r\n    audio_feature_batch = audio_feature_batch.to(device=unet.device, dtype=unet.model.dtype)\r\n    audio_feature_batch = pe(audio_feature_batch)\r\n    latent_batch = latent_batch.to(dtype=unet.model.dtype)\r\n    pred_latents = unet.model(latent_batch,\r\n                              timesteps,\r\n                              encoder_hidden_states=audio_feature_batch).sample\r\n    vae.decode_latents(pred_latents)\r\n\r\ndef read_imgs(img_list):\r\n    frames = []\r\n    print('reading images...')\r\n    for img_path in tqdm(img_list):\r\n        frame = cv2.imread(img_path)\r\n        frames.append(frame)\r\n    return frames\r\n\r\ndef __mirror_index(size, index):\r\n    #size = len(self.coord_list_cycle)\r\n    turn = index // size\r\n    res = index % size\r\n    if turn % 2 == 0:\r\n        return res\r\n    else:\r\n        return size - res - 1 \r\n\r\n@torch.no_grad()\r\ndef inference(render_event,batch_size,input_latent_list_cycle,audio_feat_queue,audio_out_queue,res_frame_queue,\r\n              vae, unet, pe,timesteps): #vae, unet, pe,timesteps\r\n    \r\n    # vae, unet, pe = load_diffusion_model()\r\n    # device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\r\n    # timesteps = torch.tensor([0], device=device)\r\n    # pe = pe.half()\r\n    # vae.vae = vae.vae.half()\r\n    # unet.model = unet.model.half()\r\n    \r\n    length = len(input_latent_list_cycle)\r\n    index = 0\r\n    count=0\r\n    counttime=0\r\n    print('start inference')\r\n    while render_event.is_set():\r\n        starttime=time.perf_counter()\r\n        try:\r\n            whisper_chunks = audio_feat_queue.get(block=True, timeout=1)\r\n        except queue.Empty:\r\n            continue\r\n        is_all_silence=True\r\n        audio_frames = []\r\n        for _ in range(batch_size*2):\r\n            frame,type = audio_out_queue.get()\r\n            audio_frames.append((frame,type))\r\n            if type==0:\r\n                is_all_silence=False\r\n        if is_all_silence:\r\n            for i in range(batch_size):\r\n                res_frame_queue.put((None,__mirror_index(length,index),audio_frames[i*2:i*2+2]))\r\n                index = index + 1\r\n        else:\r\n            # print('infer=======')\r\n            t=time.perf_counter()\r\n            whisper_batch = np.stack(whisper_chunks)\r\n            latent_batch = []\r\n            for i in range(batch_size):\r\n                idx = __mirror_index(length,index+i)\r\n                latent = input_latent_list_cycle[idx]\r\n                latent_batch.append(latent)\r\n            latent_batch = torch.cat(latent_batch, dim=0)\r\n            \r\n            # for i, (whisper_batch,latent_batch) in enumerate(gen):\r\n            audio_feature_batch = torch.from_numpy(whisper_batch)\r\n            audio_feature_batch = audio_feature_batch.to(device=unet.device,\r\n                                                            dtype=unet.model.dtype)\r\n            audio_feature_batch = pe(audio_feature_batch)\r\n            latent_batch = latent_batch.to(dtype=unet.model.dtype)\r\n            # print('prepare time:',time.perf_counter()-t)\r\n            # t=time.perf_counter()\r\n\r\n            pred_latents = unet.model(latent_batch, \r\n                                        timesteps, \r\n                                        encoder_hidden_states=audio_feature_batch).sample\r\n            # print('unet time:',time.perf_counter()-t)\r\n            # t=time.perf_counter()\r\n            recon = vae.decode_latents(pred_latents)\r\n            # infer_inqueue.put((whisper_batch,latent_batch,sessionid))\r\n            # recon,outsessionid = infer_outqueue.get()\r\n            # if outsessionid != sessionid:\r\n            #     print('outsessionid:',outsessionid,' mysessionid:',sessionid)\r\n\r\n            # print('vae time:',time.perf_counter()-t)\r\n            #print('diffusion len=',len(recon))\r\n            counttime += (time.perf_counter() - t)\r\n            count += batch_size\r\n            #_totalframe += 1\r\n            if count>=100:\r\n                print(f\"------actual avg infer fps:{count/counttime:.4f}\")\r\n                count=0\r\n                counttime=0\r\n            for i,res_frame in enumerate(recon):\r\n                #self.__pushmedia(res_frame,loop,audio_track,video_track)\r\n                res_frame_queue.put((res_frame,__mirror_index(length,index),audio_frames[i*2:i*2+2]))\r\n                index = index + 1\r\n            #print('total batch time:',time.perf_counter()-starttime)            \r\n    print('musereal inference processor stop')\r\n\r\nclass MuseReal(BaseReal):\r\n    @torch.no_grad()\r\n    def __init__(self, opt, model, avatar):\r\n        super().__init__(opt)\r\n        #self.opt = opt # shared with the trainer's opt to support in-place modification of rendering parameters.\r\n        self.W = opt.W\r\n        self.H = opt.H\r\n\r\n        self.fps = opt.fps # 20 ms per frame\r\n\r\n        self.batch_size = opt.batch_size\r\n        self.idx = 0\r\n        self.res_frame_queue = mp.Queue(self.batch_size*2)\r\n\r\n        self.vae, self.unet, self.pe, self.timesteps, self.audio_processor = model\r\n        self.frame_list_cycle,self.mask_list_cycle,self.coord_list_cycle,self.mask_coords_list_cycle, self.input_latent_list_cycle = avatar\r\n        #self.__loadavatar()\r\n\r\n        self.asr = MuseASR(opt,self,self.audio_processor)\r\n        self.asr.warm_up()\r\n        \r\n        self.render_event = mp.Event()\r\n\r\n    def __del__(self):\r\n        print(f'musereal({self.sessionid}) delete')\r\n    \r\n\r\n    def __mirror_index(self, index):\r\n        size = len(self.coord_list_cycle)\r\n        turn = index // size\r\n        res = index % size\r\n        if turn % 2 == 0:\r\n            return res\r\n        else:\r\n            return size - res - 1  \r\n\r\n    def __warm_up(self): \r\n        self.asr.run_step()\r\n        whisper_chunks = self.asr.get_next_feat()\r\n        whisper_batch = np.stack(whisper_chunks)\r\n        latent_batch = []\r\n        for i in range(self.batch_size):\r\n            idx = self.__mirror_index(self.idx+i)\r\n            latent = self.input_latent_list_cycle[idx]\r\n            latent_batch.append(latent)\r\n        latent_batch = torch.cat(latent_batch, dim=0)\r\n        print('infer=======')\r\n        # for i, (whisper_batch,latent_batch) in enumerate(gen):\r\n        audio_feature_batch = torch.from_numpy(whisper_batch)\r\n        audio_feature_batch = audio_feature_batch.to(device=self.unet.device,\r\n                                                        dtype=self.unet.model.dtype)\r\n        audio_feature_batch = self.pe(audio_feature_batch)\r\n        latent_batch = latent_batch.to(dtype=self.unet.model.dtype)\r\n\r\n        pred_latents = self.unet.model(latent_batch, \r\n                                    self.timesteps, \r\n                                    encoder_hidden_states=audio_feature_batch).sample\r\n        recon = self.vae.decode_latents(pred_latents)\r\n      \r\n\r\n    def process_frames(self,quit_event,loop=None,audio_track=None,video_track=None):\r\n        \r\n        while not quit_event.is_set():\r\n            try:\r\n                res_frame,idx,audio_frames = self.res_frame_queue.get(block=True, timeout=1)\r\n            except queue.Empty:\r\n                continue\r\n            if audio_frames[0][1]!=0 and audio_frames[1][1]!=0: #全为静音数据，只需要取fullimg\r\n                self.speaking = False\r\n                audiotype = audio_frames[0][1]\r\n                if self.custom_index.get(audiotype) is not None: #有自定义视频\r\n                    mirindex = self.mirror_index(len(self.custom_img_cycle[audiotype]),self.custom_index[audiotype])\r\n                    combine_frame = self.custom_img_cycle[audiotype][mirindex]\r\n                    self.custom_index[audiotype] += 1\r\n                    # if not self.custom_opt[audiotype].loop and self.custom_index[audiotype]>=len(self.custom_img_cycle[audiotype]):\r\n                    #     self.curr_state = 1  #当前视频不循环播放，切换到静音状态\r\n                else:\r\n                    combine_frame = self.frame_list_cycle[idx]\r\n            else:\r\n                self.speaking = True\r\n                bbox = self.coord_list_cycle[idx]\r\n                ori_frame = copy.deepcopy(self.frame_list_cycle[idx])\r\n                x1, y1, x2, y2 = bbox\r\n                try:\r\n                    res_frame = cv2.resize(res_frame.astype(np.uint8),(x2-x1,y2-y1))\r\n                except:\r\n                    continue\r\n                mask = self.mask_list_cycle[idx]\r\n                mask_crop_box = self.mask_coords_list_cycle[idx]\r\n                #combine_frame = get_image(ori_frame,res_frame,bbox)\r\n                #t=time.perf_counter()\r\n                combine_frame = get_image_blending(ori_frame,res_frame,bbox,mask,mask_crop_box)\r\n                #print('blending time:',time.perf_counter()-t)\r\n\r\n            image = combine_frame #(outputs['image'] * 255).astype(np.uint8)\r\n            new_frame = VideoFrame.from_ndarray(image, format=\"bgr24\")\r\n            asyncio.run_coroutine_threadsafe(video_track._queue.put(new_frame), loop)\r\n            self.record_video_data(image)\r\n            #self.recordq_video.put(new_frame)  \r\n\r\n            for audio_frame in audio_frames:\r\n                frame,type = audio_frame\r\n                frame = (frame * 32767).astype(np.int16)\r\n                new_frame = AudioFrame(format='s16', layout='mono', samples=frame.shape[0])\r\n                new_frame.planes[0].update(frame.tobytes())\r\n                new_frame.sample_rate=16000\r\n                # if audio_track._queue.qsize()>10:\r\n                #     time.sleep(0.1)\r\n                asyncio.run_coroutine_threadsafe(audio_track._queue.put(new_frame), loop)\r\n                self.record_audio_data(frame)\r\n                #self.recordq_audio.put(new_frame)\r\n        print('musereal process_frames thread stop') \r\n            \r\n    def render(self,quit_event,loop=None,audio_track=None,video_track=None):\r\n        #if self.opt.asr:\r\n        #     self.asr.warm_up()\r\n\r\n        self.tts.render(quit_event)\r\n        self.init_customindex()\r\n        process_thread = Thread(target=self.process_frames, args=(quit_event,loop,audio_track,video_track))\r\n        process_thread.start()\r\n\r\n        self.render_event.set() #start infer process render\r\n        Thread(target=inference, args=(self.render_event,self.batch_size,self.input_latent_list_cycle,\r\n                                           self.asr.feat_queue,self.asr.output_queue,self.res_frame_queue,\r\n                                           self.vae, self.unet, self.pe,self.timesteps)).start() #mp.Process\r\n        count=0\r\n        totaltime=0\r\n        _starttime=time.perf_counter()\r\n        #_totalframe=0\r\n        while not quit_event.is_set(): #todo\r\n            # update texture every frame\r\n            # audio stream thread...\r\n            t = time.perf_counter()\r\n            self.asr.run_step()\r\n            #self.test_step(loop,audio_track,video_track)\r\n            # totaltime += (time.perf_counter() - t)\r\n            # count += self.opt.batch_size\r\n            # if count>=100:\r\n            #     print(f\"------actual avg infer fps:{count/totaltime:.4f}\")\r\n            #     count=0\r\n            #     totaltime=0\r\n            if video_track._queue.qsize()>=1.5*self.opt.batch_size:\r\n                print('sleep qsize=',video_track._queue.qsize())\r\n                time.sleep(0.04*video_track._queue.qsize()*0.8)\r\n            # if video_track._queue.qsize()>=5:\r\n            #     print('sleep qsize=',video_track._queue.qsize())\r\n            #     time.sleep(0.04*video_track._queue.qsize()*0.8)\r\n                \r\n            # delay = _starttime+_totalframe*0.04-time.perf_counter() #40ms\r\n            # if delay > 0:\r\n            #     time.sleep(delay)\r\n        self.render_event.clear() #end infer process render\r\n        print('musereal thread stop')\r\n            \r\n"
        },
        {
          "name": "musetalk",
          "type": "tree",
          "content": null
        },
        {
          "name": "nerfasr.py",
          "type": "blob",
          "size": 14.392578125,
          "content": "###############################################################################\r\n#  Copyright (C) 2024 LiveTalking@lipku https://github.com/lipku/LiveTalking\r\n#  email: lipku@foxmail.com\r\n# \r\n#  Licensed under the Apache License, Version 2.0 (the \"License\");\r\n#  you may not use this file except in compliance with the License.\r\n#  You may obtain a copy of the License at\r\n#  \r\n#       http://www.apache.org/licenses/LICENSE-2.0\r\n# \r\n#  Unless required by applicable law or agreed to in writing, software\r\n#  distributed under the License is distributed on an \"AS IS\" BASIS,\r\n#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n#  See the License for the specific language governing permissions and\r\n#  limitations under the License.\r\n###############################################################################\r\n\r\nimport time\r\nimport numpy as np\r\nimport torch\r\nimport torch.nn.functional as F\r\n\r\nimport queue\r\nfrom queue import Queue\r\n#from collections import deque\r\n\r\nfrom baseasr import BaseASR\r\n\r\nclass NerfASR(BaseASR):\r\n    def __init__(self, opt, parent, audio_processor,audio_model):\r\n        super().__init__(opt,parent)\r\n\r\n        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\r\n        if 'esperanto' in self.opt.asr_model:\r\n            self.audio_dim = 44\r\n        elif 'deepspeech' in self.opt.asr_model:\r\n            self.audio_dim = 29\r\n        elif 'hubert' in self.opt.asr_model:\r\n            self.audio_dim = 1024\r\n        else:\r\n            self.audio_dim = 32\r\n\r\n        # prepare context cache\r\n        # each segment is (stride_left + ctx + stride_right) * 20ms, latency should be (ctx + stride_right) * 20ms\r\n        self.context_size = opt.m\r\n        self.stride_left_size = opt.l\r\n        self.stride_right_size = opt.r\r\n\r\n        # pad left frames\r\n        if self.stride_left_size > 0:\r\n            self.frames.extend([np.zeros(self.chunk, dtype=np.float32)] * self.stride_left_size)\r\n\r\n        # create wav2vec model\r\n        # print(f'[INFO] loading ASR model {self.opt.asr_model}...')\r\n        # if 'hubert' in self.opt.asr_model:\r\n        #     self.processor = Wav2Vec2Processor.from_pretrained(opt.asr_model)\r\n        #     self.model = HubertModel.from_pretrained(opt.asr_model).to(self.device) \r\n        # else:   \r\n        #     self.processor = AutoProcessor.from_pretrained(opt.asr_model)\r\n        #     self.model = AutoModelForCTC.from_pretrained(opt.asr_model).to(self.device)\r\n        self.processor = audio_processor\r\n        self.model = audio_model\r\n\r\n        # the extracted features \r\n        # use a loop queue to efficiently record endless features: [f--t---][-------][-------]\r\n        self.feat_buffer_size = 4\r\n        self.feat_buffer_idx = 0\r\n        self.feat_queue = torch.zeros(self.feat_buffer_size * self.context_size, self.audio_dim, dtype=torch.float32, device=self.device)\r\n\r\n        # TODO: hard coded 16 and 8 window size...\r\n        self.front = self.feat_buffer_size * self.context_size - 8 # fake padding\r\n        self.tail = 8\r\n        # attention window...\r\n        self.att_feats = [torch.zeros(self.audio_dim, 16, dtype=torch.float32, device=self.device)] * 4 # 4 zero padding...\r\n\r\n        # warm up steps needed: mid + right + window_size + attention_size\r\n        self.warm_up_steps = self.context_size + self.stride_left_size + self.stride_right_size #+ self.stride_left_size   #+ 8 + 2 * 3\r\n\r\n    def get_audio_frame(self):         \r\n        try:\r\n            frame = self.queue.get(block=False)\r\n            type = 0\r\n            #print(f'[INFO] get frame {frame.shape}')\r\n        except queue.Empty:\r\n            if self.parent and self.parent.curr_state>1: #播放自定义音频\r\n                frame = self.parent.get_audio_stream(self.parent.curr_state)\r\n                type = self.parent.curr_state\r\n            else:\r\n                frame = np.zeros(self.chunk, dtype=np.float32)\r\n                type = 1\r\n\r\n        return frame,type\r\n\r\n    def get_next_feat(self): #get audio embedding to nerf\r\n        # return a [1/8, 16] window, for the next input to nerf side.\r\n        if self.opt.att>0:\r\n            while len(self.att_feats) < 8:\r\n                # [------f+++t-----]\r\n                if self.front < self.tail:\r\n                    feat = self.feat_queue[self.front:self.tail]\r\n                # [++t-----------f+]\r\n                else:\r\n                    feat = torch.cat([self.feat_queue[self.front:], self.feat_queue[:self.tail]], dim=0)\r\n\r\n                self.front = (self.front + 2) % self.feat_queue.shape[0]\r\n                self.tail = (self.tail + 2) % self.feat_queue.shape[0]\r\n\r\n                # print(self.front, self.tail, feat.shape)\r\n\r\n                self.att_feats.append(feat.permute(1, 0))\r\n            \r\n            att_feat = torch.stack(self.att_feats, dim=0) # [8, 44, 16]\r\n\r\n            # discard old\r\n            self.att_feats = self.att_feats[1:]\r\n        else:\r\n            # [------f+++t-----]\r\n            if self.front < self.tail:\r\n                feat = self.feat_queue[self.front:self.tail]\r\n            # [++t-----------f+]\r\n            else:\r\n                feat = torch.cat([self.feat_queue[self.front:], self.feat_queue[:self.tail]], dim=0)\r\n\r\n            self.front = (self.front + 2) % self.feat_queue.shape[0]\r\n            self.tail = (self.tail + 2) % self.feat_queue.shape[0]\r\n\r\n            att_feat = feat.permute(1, 0).unsqueeze(0)\r\n\r\n\r\n        return att_feat\r\n\r\n    def run_step(self):\r\n\r\n        # get a frame of audio\r\n        frame,type = self.get_audio_frame()\r\n        self.frames.append(frame)\r\n        # put to output\r\n        self.output_queue.put((frame,type))\r\n        # context not enough, do not run network.\r\n        if len(self.frames) < self.stride_left_size + self.context_size + self.stride_right_size:\r\n            return\r\n        \r\n        inputs = np.concatenate(self.frames) # [N * chunk]\r\n\r\n        # discard the old part to save memory\r\n        self.frames = self.frames[-(self.stride_left_size + self.stride_right_size):]\r\n\r\n        #print(f'[INFO] frame_to_text... ')\r\n        #t = time.time()\r\n        logits, labels, text = self.__frame_to_text(inputs)\r\n        #print(f'-------wav2vec time:{time.time()-t:.4f}s')\r\n        feats = logits # better lips-sync than labels\r\n\r\n        # record the feats efficiently.. (no concat, constant memory)\r\n        start = self.feat_buffer_idx * self.context_size\r\n        end = start + feats.shape[0]\r\n        self.feat_queue[start:end] = feats\r\n        self.feat_buffer_idx = (self.feat_buffer_idx + 1) % self.feat_buffer_size\r\n\r\n        # very naive, just concat the text output.\r\n        #if text != '':\r\n        #    self.text = self.text + ' ' + text\r\n\r\n        # will only run once at ternimation\r\n        # if self.terminated:\r\n        #     self.text += '\\n[END]'\r\n        #     print(self.text)\r\n        #     if self.opt.asr_save_feats:\r\n        #         print(f'[INFO] save all feats for training purpose... ')\r\n        #         feats = torch.cat(self.all_feats, dim=0) # [N, C]\r\n        #         # print('[INFO] before unfold', feats.shape)\r\n        #         window_size = 16\r\n        #         padding = window_size // 2\r\n        #         feats = feats.view(-1, self.audio_dim).permute(1, 0).contiguous() # [C, M]\r\n        #         feats = feats.view(1, self.audio_dim, -1, 1) # [1, C, M, 1]\r\n        #         unfold_feats = F.unfold(feats, kernel_size=(window_size, 1), padding=(padding, 0), stride=(2, 1)) # [1, C * window_size, M / 2 + 1]\r\n        #         unfold_feats = unfold_feats.view(self.audio_dim, window_size, -1).permute(2, 1, 0).contiguous() # [C, window_size, M / 2 + 1] --> [M / 2 + 1, window_size, C]\r\n        #         # print('[INFO] after unfold', unfold_feats.shape)\r\n        #         # save to a npy file\r\n        #         if 'esperanto' in self.opt.asr_model:\r\n        #             output_path = self.opt.asr_wav.replace('.wav', '_eo.npy')\r\n        #         else:\r\n        #             output_path = self.opt.asr_wav.replace('.wav', '.npy')\r\n        #         np.save(output_path, unfold_feats.cpu().numpy())\r\n        #         print(f\"[INFO] saved logits to {output_path}\")\r\n    \r\n\r\n        \r\n    def __frame_to_text(self, frame):\r\n        # frame: [N * 320], N = (context_size + 2 * stride_size)\r\n        \r\n        inputs = self.processor(frame, sampling_rate=self.sample_rate, return_tensors=\"pt\", padding=True)\r\n        \r\n        with torch.no_grad():\r\n            result = self.model(inputs.input_values.to(self.device))\r\n            if 'hubert' in self.opt.asr_model:\r\n                logits = result.last_hidden_state # [B=1, T=pts//320, hid=1024]\r\n            else:\r\n                logits = result.logits # [1, N - 1, 32]\r\n        #print('logits.shape:',logits.shape)\r\n        \r\n        # cut off stride\r\n        left = max(0, self.stride_left_size)\r\n        right = min(logits.shape[1], logits.shape[1] - self.stride_right_size + 1) # +1 to make sure output is the same length as input.\r\n\r\n        # do not cut right if terminated.\r\n        # if self.terminated:\r\n        #     right = logits.shape[1]\r\n\r\n        logits = logits[:, left:right]\r\n\r\n        # print(frame.shape, inputs.input_values.shape, logits.shape)\r\n    \r\n        #predicted_ids = torch.argmax(logits, dim=-1)\r\n        #transcription = self.processor.batch_decode(predicted_ids)[0].lower()\r\n\r\n        \r\n        # for esperanto\r\n        # labels = np.array(['ŭ', '»', 'c', 'ĵ', 'ñ', '”', '„', '“', 'ǔ', 'o', 'ĝ', 'm', 'k', 'd', 'a', 'ŝ', 'z', 'i', '«', '—', '‘', 'ĥ', 'f', 'y', 'h', 'j', '|', 'r', 'u', 'ĉ', 's', '–', 'ﬁ', 'l', 'p', '’', 'g', 'v', 't', 'b', 'n', 'e', '[UNK]', '[PAD]'])\r\n\r\n        # labels = np.array([' ', ' ', ' ', '-', '|', 'E', 'T', 'A', 'O', 'N', 'I', 'H', 'S', 'R', 'D', 'L', 'U', 'M', 'W', 'C', 'F', 'G', 'Y', 'P', 'B', 'V', 'K', \"'\", 'X', 'J', 'Q', 'Z'])\r\n        # print(''.join(labels[predicted_ids[0].detach().cpu().long().numpy()]))\r\n        # print(predicted_ids[0])\r\n        # print(transcription)\r\n\r\n        return logits[0], None,None #predicted_ids[0], transcription # [N,]\r\n    \r\n\r\n    def warm_up(self):       \r\n        print(f'[INFO] warm up ASR live model, expected latency = {self.warm_up_steps / self.fps:.6f}s')\r\n        t = time.time()\r\n        #for _ in range(self.stride_left_size):\r\n        #    self.frames.append(np.zeros(self.chunk, dtype=np.float32))\r\n        for _ in range(self.warm_up_steps):\r\n            self.run_step()\r\n        #if torch.cuda.is_available():\r\n        #    torch.cuda.synchronize()\r\n        t = time.time() - t\r\n        print(f'[INFO] warm-up done, actual latency = {t:.6f}s')\r\n\r\n        #self.clear_queue()\r\n\r\n    #####not used function#####################################\r\n    '''\r\n    def __init_queue(self):\r\n        self.frames = []\r\n        self.queue.queue.clear()\r\n        self.output_queue.queue.clear()\r\n        self.front = self.feat_buffer_size * self.context_size - 8 # fake padding\r\n        self.tail = 8\r\n        # attention window...\r\n        self.att_feats = [torch.zeros(self.audio_dim, 16, dtype=torch.float32, device=self.device)] * 4\r\n        \r\n    def run(self):\r\n\r\n        self.listen()\r\n\r\n        while not self.terminated:\r\n            self.run_step()\r\n\r\n    def clear_queue(self):\r\n        # clear the queue, to reduce potential latency...\r\n        print(f'[INFO] clear queue')\r\n        if self.mode == 'live':\r\n            self.queue.queue.clear()\r\n        if self.play:\r\n            self.output_queue.queue.clear()\r\n\r\n    def listen(self):\r\n        # start\r\n        if self.mode == 'live' and not self.listening:\r\n            print(f'[INFO] starting read frame thread...')\r\n            self.process_read_frame.start()\r\n            self.listening = True\r\n        \r\n        if self.play and not self.playing:\r\n            print(f'[INFO] starting play frame thread...')\r\n            self.process_play_frame.start()\r\n            self.playing = True\r\n\r\n    def stop(self):\r\n\r\n        self.exit_event.set()\r\n\r\n        if self.play:\r\n            self.output_stream.stop_stream()\r\n            self.output_stream.close()\r\n            if self.playing:\r\n                self.process_play_frame.join()\r\n                self.playing = False\r\n\r\n        if self.mode == 'live':\r\n            #self.input_stream.stop_stream() todo\r\n            self.input_stream.close()\r\n            if self.listening:\r\n                self.process_read_frame.join()\r\n                self.listening = False\r\n\r\n\r\n    def __enter__(self):\r\n        return self\r\n\r\n    def __exit__(self, exc_type, exc_value, traceback):\r\n        \r\n        self.stop()\r\n\r\n        if self.mode == 'live':\r\n            # live mode: also print the result text.        \r\n            self.text += '\\n[END]'\r\n            print(self.text)\r\n\r\ndef _read_frame(stream, exit_event, queue, chunk):\r\n    while True:\r\n        if exit_event.is_set():\r\n            print(f'[INFO] read frame thread ends')\r\n            break\r\n        frame = stream.read(chunk, exception_on_overflow=False)\r\n        frame = np.frombuffer(frame, dtype=np.int16).astype(np.float32) / 32767 # [chunk]\r\n        queue.put(frame)\r\n\r\ndef _play_frame(stream, exit_event, queue, chunk):\r\n\r\n    while True:\r\n        if exit_event.is_set():\r\n            print(f'[INFO] play frame thread ends')\r\n            break\r\n        frame = queue.get()\r\n        frame = (frame * 32767).astype(np.int16).tobytes()\r\n        stream.write(frame, chunk)\r\n     #########################################################\r\n\r\nif __name__ == '__main__':\r\n    import argparse\r\n\r\n    parser = argparse.ArgumentParser()\r\n    parser.add_argument('--wav', type=str, default='')\r\n    parser.add_argument('--play', action='store_true', help=\"play out the audio\")\r\n    \r\n    # parser.add_argument('--model', type=str, default='cpierse/wav2vec2-large-xlsr-53-esperanto')\r\n    # parser.add_argument('--model', type=str, default='facebook/wav2vec2-large-960h-lv60-self')\r\n    parser.add_argument('--model', type=str, default='facebook/hubert-large-ls960-ft')\r\n\r\n    parser.add_argument('--save_feats', action='store_true')\r\n    # audio FPS\r\n    parser.add_argument('--fps', type=int, default=50)\r\n    # sliding window left-middle-right length.\r\n    parser.add_argument('-l', type=int, default=10)\r\n    parser.add_argument('-m', type=int, default=50)\r\n    parser.add_argument('-r', type=int, default=10)\r\n    \r\n    opt = parser.parse_args()\r\n\r\n    # fix\r\n    opt.asr_wav = opt.wav\r\n    opt.asr_play = opt.play\r\n    opt.asr_model = opt.model\r\n    opt.asr_save_feats = opt.save_feats\r\n\r\n    if 'deepspeech' in opt.asr_model:\r\n        raise ValueError(\"DeepSpeech features should not use this code to extract...\")\r\n\r\n    with ASR(opt) as asr:\r\n        asr.run()\r\n'''"
        },
        {
          "name": "nerfreal.py",
          "type": "blob",
          "size": 15.44140625,
          "content": "###############################################################################\r\n#  Copyright (C) 2024 LiveTalking@lipku https://github.com/lipku/LiveTalking\r\n#  email: lipku@foxmail.com\r\n# \r\n#  Licensed under the Apache License, Version 2.0 (the \"License\");\r\n#  you may not use this file except in compliance with the License.\r\n#  You may obtain a copy of the License at\r\n#  \r\n#       http://www.apache.org/licenses/LICENSE-2.0\r\n# \r\n#  Unless required by applicable law or agreed to in writing, software\r\n#  distributed under the License is distributed on an \"AS IS\" BASIS,\r\n#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n#  See the License for the specific language governing permissions and\r\n#  limitations under the License.\r\n###############################################################################\r\n\r\nimport math\r\nimport torch\r\nimport numpy as np\r\n\r\n#from .utils import *\r\nimport os\r\nimport time\r\nimport torch.nn.functional as F\r\nimport cv2\r\nimport glob\r\n\r\nfrom nerfasr import NerfASR\r\n\r\nimport asyncio\r\nfrom av import AudioFrame, VideoFrame\r\nfrom basereal import BaseReal\r\n\r\n#from imgcache import ImgCache\r\nfrom ernerf.nerf_triplane.provider import NeRFDataset_Test\r\nfrom ernerf.nerf_triplane.utils import *\r\nfrom ernerf.nerf_triplane.network import NeRFNetwork\r\nfrom transformers import AutoModelForCTC, AutoProcessor, Wav2Vec2Processor, HubertModel\r\n\r\nfrom tqdm import tqdm\r\ndef read_imgs(img_list):\r\n    frames = []\r\n    print('reading images...')\r\n    for img_path in tqdm(img_list):\r\n        frame = cv2.imread(img_path)\r\n        frames.append(frame)\r\n    return frames\r\n\r\ndef load_model(opt):\r\n    # assert test mode\r\n    opt.test = True\r\n    opt.test_train = False\r\n    #opt.train_camera =True\r\n    # explicit smoothing\r\n    opt.smooth_path = True\r\n    opt.smooth_lips = True\r\n\r\n    assert opt.pose != '', 'Must provide a pose source'\r\n\r\n    # if opt.O:\r\n    opt.fp16 = True\r\n    opt.cuda_ray = True\r\n    opt.exp_eye = True\r\n    opt.smooth_eye = True\r\n\r\n    if opt.torso_imgs=='': #no img,use model output\r\n        opt.torso = True\r\n\r\n    # assert opt.cuda_ray, \"Only support CUDA ray mode.\"\r\n    opt.asr = True\r\n\r\n    if opt.patch_size > 1:\r\n        # assert opt.patch_size > 16, \"patch_size should > 16 to run LPIPS loss.\"\r\n        assert opt.num_rays % (opt.patch_size ** 2) == 0, \"patch_size ** 2 should be dividable by num_rays.\"\r\n    seed_everything(opt.seed)\r\n    print(opt)\r\n\r\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\r\n    model = NeRFNetwork(opt)\r\n\r\n    criterion = torch.nn.MSELoss(reduction='none')\r\n    metrics = [] # use no metric in GUI for faster initialization...\r\n    print(model)\r\n    trainer = Trainer('ngp', opt, model, device=device, workspace=opt.workspace, criterion=criterion, fp16=opt.fp16, metrics=metrics, use_checkpoint=opt.ckpt)\r\n\r\n    test_loader = NeRFDataset_Test(opt, device=device).dataloader()\r\n    model.aud_features = test_loader._data.auds\r\n    model.eye_areas = test_loader._data.eye_area\r\n\r\n    print(f'[INFO] loading ASR model {opt.asr_model}...')\r\n    if 'hubert' in opt.asr_model:\r\n        audio_processor = Wav2Vec2Processor.from_pretrained(opt.asr_model)\r\n        audio_model = HubertModel.from_pretrained(opt.asr_model).to(device) \r\n    else:   \r\n        audio_processor = AutoProcessor.from_pretrained(opt.asr_model)\r\n        audio_model = AutoModelForCTC.from_pretrained(opt.asr_model).to(device)\r\n    return trainer,test_loader,audio_processor,audio_model\r\n\r\ndef load_avatar(opt):\r\n    fullbody_list_cycle = None\r\n    if opt.fullbody:\r\n        input_img_list = glob.glob(os.path.join(opt.fullbody_img, '*.[jpJP][pnPN]*[gG]'))\r\n        input_img_list = sorted(input_img_list, key=lambda x: int(os.path.splitext(os.path.basename(x))[0]))\r\n        #print('input_img_list:',input_img_list)\r\n        fullbody_list_cycle = read_imgs(input_img_list) #[:frame_total_num]\r\n        #self.imagecache = ImgCache(frame_total_num,self.opt.fullbody_img,1000)\r\n    return fullbody_list_cycle\r\n\r\nclass NeRFReal(BaseReal):\r\n    def __init__(self, opt, model,avatar, debug=True):\r\n        super().__init__(opt)\r\n        #self.opt = opt # shared with the trainer's opt to support in-place modification of rendering parameters.\r\n        self.W = opt.W\r\n        self.H = opt.H\r\n\r\n        #self.trainer = trainer\r\n        #self.data_loader = data_loader\r\n        self.trainer, self.data_loader, audio_processor,audio_model = model\r\n\r\n        # use dataloader's bg\r\n        #bg_img = data_loader._data.bg_img #.view(1, -1, 3)\r\n        #if self.H != bg_img.shape[0] or self.W != bg_img.shape[1]:\r\n        #    bg_img = F.interpolate(bg_img.permute(2, 0, 1).unsqueeze(0).contiguous(), (self.H, self.W), mode='bilinear').squeeze(0).permute(1, 2, 0).contiguous()\r\n        #self.bg_color = bg_img.view(1, -1, 3)\r\n\r\n        # audio features (from dataloader, only used in non-playing mode)\r\n        #self.audio_features = data_loader._data.auds # [N, 29, 16]\r\n        #self.audio_idx = 0\r\n\r\n        #self.frame_total_num = data_loader._data.end_index\r\n        #print(\"frame_total_num:\",self.frame_total_num)\r\n\r\n        # control eye\r\n        #self.eye_area = None if not self.opt.exp_eye else data_loader._data.eye_area.mean().item()\r\n\r\n        # playing seq from dataloader, or pause.\r\n        self.loader = iter(self.data_loader)\r\n        frame_total_num = self.data_loader._data.end_index\r\n        self.fullbody_list_cycle = avatar\r\n        \r\n\r\n        #self.render_buffer = np.zeros((self.W, self.H, 3), dtype=np.float32)\r\n        #self.need_update = True # camera moved, should reset accumulation\r\n        #self.spp = 1 # sample per pixel\r\n        #self.mode = 'image' # choose from ['image', 'depth']\r\n\r\n        #self.dynamic_resolution = False # assert False!\r\n        #self.downscale = 1\r\n        #self.train_steps = 16\r\n\r\n        #self.ind_index = 0\r\n        #self.ind_num = trainer.model.individual_codes.shape[0]\r\n\r\n        #self.customimg_index = 0\r\n\r\n        # build asr\r\n        self.asr = NerfASR(opt,self,audio_processor,audio_model)\r\n        self.asr.warm_up()\r\n        \r\n        '''\r\n        video_path = 'video_stream'\r\n        if not os.path.exists(video_path):\r\n            os.mkfifo(video_path, mode=0o777)\r\n        audio_path = 'audio_stream'\r\n        if not os.path.exists(audio_path):\r\n            os.mkfifo(audio_path, mode=0o777)\r\n        width=450\r\n        height=450\r\n        command = ['ffmpeg',\r\n                    '-y', #'-an',\r\n                    #'-re',\r\n                    '-f', 'rawvideo',\r\n                    '-vcodec','rawvideo',\r\n                    '-pix_fmt', 'rgb24', #像素格式\r\n                    '-s', \"{}x{}\".format(width, height),\r\n                    '-r', str(fps),\r\n                    '-i', video_path, \r\n                    '-f', 's16le',\r\n                    '-acodec','pcm_s16le',\r\n                    '-ac', '1',\r\n                    '-ar', '16000',\r\n                    '-i', audio_path,\r\n                    #'-fflags', '+genpts',\r\n                    '-map', '0:v',\r\n                    '-map', '1:a',\r\n                    #'-copyts', \r\n                    '-acodec', 'aac',\r\n                    '-pix_fmt', 'yuv420p', #'-vcodec', \"h264\",\r\n                    #\"-rtmp_buffer\", \"100\", \r\n                    '-f' , 'flv',                  \r\n                    push_url]\r\n        self.pipe = subprocess.Popen(command, shell=False) #, stdin=subprocess.PIPE)\r\n        self.fifo_video = open(video_path, 'wb')\r\n        self.fifo_audio = open(audio_path, 'wb')\r\n        #self.test_step()\r\n        '''\r\n\r\n    def __del__(self):\r\n        print(f'nerfreal({self.sessionid}) delete')    \r\n\r\n    def __enter__(self):\r\n        return self\r\n\r\n    def __exit__(self, exc_type, exc_value, traceback):\r\n        if self.opt.asr:\r\n            self.asr.stop()  \r\n    \r\n\r\n    # def mirror_index(self, index):\r\n    #     size = self.opt.customvideo_imgnum\r\n    #     turn = index // size\r\n    #     res = index % size\r\n    #     if turn % 2 == 0:\r\n    #         return res\r\n    #     else:\r\n    #         return size - res - 1   \r\n\r\n    def test_step(self,loop=None,audio_track=None,video_track=None):\r\n        \r\n        #starter, ender = torch.cuda.Event(enable_timing=True), torch.cuda.Event(enable_timing=True)\r\n        #starter.record()\r\n\r\n        try:\r\n            data = next(self.loader)\r\n        except StopIteration:\r\n            self.loader = iter(self.data_loader)\r\n            data = next(self.loader)\r\n        \r\n        if self.opt.asr:\r\n            # use the live audio stream\r\n            data['auds'] = self.asr.get_next_feat()\r\n\r\n        audiotype1 = 0\r\n        audiotype2 = 0\r\n        #send audio\r\n        for i in range(2):\r\n            frame,type = self.asr.get_audio_out()\r\n            if i==0:\r\n                audiotype1 = type\r\n            else:\r\n                audiotype2 = type\r\n            #print(f'[INFO] get_audio_out shape ',frame.shape)\r\n            if self.opt.transport=='rtmp':                \r\n                self.streamer.stream_frame_audio(frame)\r\n            else: #webrtc\r\n                frame = (frame * 32767).astype(np.int16)\r\n                new_frame = AudioFrame(format='s16', layout='mono', samples=frame.shape[0])\r\n                new_frame.planes[0].update(frame.tobytes())\r\n                new_frame.sample_rate=16000\r\n                asyncio.run_coroutine_threadsafe(audio_track._queue.put(new_frame), loop)\r\n\r\n        # if self.opt.transport=='rtmp':\r\n        #     for _ in range(2):\r\n        #         frame,type = self.asr.get_audio_out()\r\n        #         audiotype += type\r\n        #         #print(f'[INFO] get_audio_out shape ',frame.shape)                \r\n        #         self.streamer.stream_frame_audio(frame)\r\n        # else: #webrtc\r\n        #     for _ in range(2):\r\n        #         frame,type = self.asr.get_audio_out()\r\n        #         audiotype += type\r\n        #         frame = (frame * 32767).astype(np.int16)\r\n        #         new_frame = AudioFrame(format='s16', layout='mono', samples=frame.shape[0])\r\n        #         new_frame.planes[0].update(frame.tobytes())\r\n        #         new_frame.sample_rate=16000\r\n        #         # if audio_track._queue.qsize()>10:\r\n        #         #     time.sleep(0.1)\r\n        #         asyncio.run_coroutine_threadsafe(audio_track._queue.put(new_frame), loop)  \r\n        #t = time.time()\r\n        if audiotype1!=0 and audiotype2!=0: #全为静音数据\r\n            self.speaking = False\r\n        else:\r\n            self.speaking = True\r\n            \r\n        if audiotype1!=0 and audiotype2!=0 and self.custom_index.get(audiotype1) is not None: #不为推理视频并且有自定义视频\r\n            mirindex = self.mirror_index(len(self.custom_img_cycle[audiotype1]),self.custom_index[audiotype1])\r\n            #imgindex  = self.mirror_index(self.customimg_index)\r\n            #print('custom img index:',imgindex)\r\n            #image = cv2.imread(os.path.join(self.opt.customvideo_img, str(int(imgindex))+'.png'))\r\n            image = self.custom_img_cycle[audiotype1][mirindex]\r\n            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\r\n            self.custom_index[audiotype1] += 1\r\n            if self.opt.transport=='rtmp':\r\n                self.streamer.stream_frame(image)\r\n            else:\r\n                new_frame = VideoFrame.from_ndarray(image, format=\"rgb24\")\r\n                asyncio.run_coroutine_threadsafe(video_track._queue.put(new_frame), loop)\r\n        else: #推理视频+贴回\r\n            outputs = self.trainer.test_gui_with_data(data, self.W, self.H)\r\n            #print('-------ernerf time: ',time.time()-t)\r\n            #print(f'[INFO] outputs shape ',outputs['image'].shape)\r\n            image = (outputs['image'] * 255).astype(np.uint8)\r\n            if not self.opt.fullbody:\r\n                if self.opt.transport=='rtmp':\r\n                    self.streamer.stream_frame(image)\r\n                else:\r\n                    new_frame = VideoFrame.from_ndarray(image, format=\"rgb24\")\r\n                    asyncio.run_coroutine_threadsafe(video_track._queue.put(new_frame), loop)\r\n            else: #fullbody human\r\n                #print(\"frame index:\",data['index'])\r\n                #image_fullbody = cv2.imread(os.path.join(self.opt.fullbody_img, str(data['index'][0])+'.jpg'))\r\n                image_fullbody = self.fullbody_list_cycle[data['index'][0]]\r\n                #image_fullbody = self.imagecache.get_img(data['index'][0])\r\n                image_fullbody = cv2.cvtColor(image_fullbody, cv2.COLOR_BGR2RGB)                \r\n                start_x = self.opt.fullbody_offset_x  # 合并后小图片的起始x坐标\r\n                start_y = self.opt.fullbody_offset_y  # 合并后小图片的起始y坐标\r\n                image_fullbody[start_y:start_y+image.shape[0], start_x:start_x+image.shape[1]] = image\r\n                if self.opt.transport=='rtmp':\r\n                    self.streamer.stream_frame(image_fullbody)\r\n                else:\r\n                    new_frame = VideoFrame.from_ndarray(image_fullbody, format=\"rgb24\")\r\n                    asyncio.run_coroutine_threadsafe(video_track._queue.put(new_frame), loop)\r\n            #self.pipe.stdin.write(image.tostring())        \r\n       \r\n        #ender.record()\r\n        #torch.cuda.synchronize()\r\n        #t = starter.elapsed_time(ender)\r\n            \r\n    def render(self,quit_event,loop=None,audio_track=None,video_track=None):\r\n        #if self.opt.asr:\r\n        #     self.asr.warm_up()\r\n        \r\n        self.init_customindex()\r\n\r\n        if self.opt.transport=='rtmp':\r\n            from rtmp_streaming import StreamerConfig, Streamer\r\n            fps=25\r\n            #push_url='rtmp://localhost/live/livestream' #'data/video/output_0.mp4'\r\n            sc = StreamerConfig()\r\n            sc.source_width = self.W\r\n            sc.source_height = self.H\r\n            sc.stream_width = self.W\r\n            sc.stream_height = self.H\r\n            if self.opt.fullbody:\r\n                sc.source_width = self.opt.fullbody_width\r\n                sc.source_height = self.opt.fullbody_height\r\n                sc.stream_width = self.opt.fullbody_width\r\n                sc.stream_height = self.opt.fullbody_height\r\n            sc.stream_fps = fps\r\n            sc.stream_bitrate = 1000000\r\n            sc.stream_profile = 'baseline' #'high444' # 'main'\r\n            sc.audio_channel = 1\r\n            sc.sample_rate = 16000\r\n            sc.stream_server = self.opt.push_url\r\n            self.streamer = Streamer()\r\n            self.streamer.init(sc)\r\n            #self.streamer.enable_av_debug_log()\r\n\r\n        count=0\r\n        totaltime=0\r\n        _starttime=time.perf_counter()\r\n        _totalframe=0\r\n\r\n        self.tts.render(quit_event)\r\n        while not quit_event.is_set(): #todo\r\n            # update texture every frame\r\n            # audio stream thread...\r\n            t = time.perf_counter()\r\n            # run 2 ASR steps (audio is at 50FPS, video is at 25FPS)\r\n            for _ in range(2):\r\n                self.asr.run_step()\r\n            self.test_step(loop,audio_track,video_track)\r\n            totaltime += (time.perf_counter() - t)\r\n            count += 1\r\n            _totalframe += 1\r\n            if count==100:\r\n                print(f\"------actual avg infer fps:{count/totaltime:.4f}\")\r\n                count=0\r\n                totaltime=0\r\n            if self.opt.transport=='rtmp':\r\n                delay = _starttime+_totalframe*0.04-time.perf_counter() #40ms\r\n                if delay > 0:\r\n                    time.sleep(delay)\r\n            else:\r\n                if video_track._queue.qsize()>=5:\r\n                    #print('sleep qsize=',video_track._queue.qsize())\r\n                    time.sleep(0.04*video_track._queue.qsize()*0.8)\r\n        print('nerfreal thread stop')\r\n            \r\n            "
        },
        {
          "name": "requirements.txt",
          "type": "blob",
          "size": 0.3955078125,
          "content": "torch-ema\nninja\ntrimesh\nopencv-python\ntensorboardX\nnumpy \npandas\ntqdm\nmatplotlib\nPyMCubes\nrich\ndearpygui\npackaging\nscipy\nscikit-learn\n\nface_alignment\npython_speech_features\nnumba\nresampy\n#pyaudio\nsoundfile\neinops\nconfigargparse\n\nlpips==0.1.3\nimageio-ffmpeg\n\ntransformers\nedge_tts\nflask\nflask_sockets\nopencv-python-headless\naiortc\naiohttp_cors\n\nffmpeg-python\nomegaconf\ndiffusers\naccelerate\n\nlibrosa\nopenai\n"
        },
        {
          "name": "ttsreal.py",
          "type": "blob",
          "size": 12.7197265625,
          "content": "###############################################################################\n#  Copyright (C) 2024 LiveTalking@lipku https://github.com/lipku/LiveTalking\n#  email: lipku@foxmail.com\n# \n#  Licensed under the Apache License, Version 2.0 (the \"License\");\n#  you may not use this file except in compliance with the License.\n#  You may obtain a copy of the License at\n#  \n#       http://www.apache.org/licenses/LICENSE-2.0\n# \n#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an \"AS IS\" BASIS,\n#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#  See the License for the specific language governing permissions and\n#  limitations under the License.\n###############################################################################\n\nimport time\nimport numpy as np\nimport soundfile as sf\nimport resampy\nimport asyncio\nimport edge_tts\n\nfrom typing import Iterator\n\nimport requests\n\nimport queue\nfrom queue import Queue\nfrom io import BytesIO\nfrom threading import Thread, Event\nfrom enum import Enum\n\nclass State(Enum):\n    RUNNING=0\n    PAUSE=1\n\nclass BaseTTS:\n    def __init__(self, opt, parent):\n        self.opt=opt\n        self.parent = parent\n\n        self.fps = opt.fps # 20 ms per frame\n        self.sample_rate = 16000\n        self.chunk = self.sample_rate // self.fps # 320 samples per chunk (20ms * 16000 / 1000)\n        self.input_stream = BytesIO()\n\n        self.msgqueue = Queue()\n        self.state = State.RUNNING\n\n    def flush_talk(self):\n        self.msgqueue.queue.clear()\n        self.state = State.PAUSE\n\n    def put_msg_txt(self,msg): \n        if len(msg)>0:\n            self.msgqueue.put(msg)\n\n    def render(self,quit_event):\n        process_thread = Thread(target=self.process_tts, args=(quit_event,))\n        process_thread.start()\n    \n    def process_tts(self,quit_event):        \n        while not quit_event.is_set():\n            try:\n                msg = self.msgqueue.get(block=True, timeout=1)\n                self.state=State.RUNNING\n            except queue.Empty:\n                continue\n            self.txt_to_audio(msg)\n        print('ttsreal thread stop')\n    \n    def txt_to_audio(self,msg):\n        pass\n    \n\n###########################################################################################\nclass EdgeTTS(BaseTTS):\n    def txt_to_audio(self,msg):\n        voicename = \"zh-CN-YunxiaNeural\"\n        text = msg\n        t = time.time()\n        asyncio.new_event_loop().run_until_complete(self.__main(voicename,text))\n        print(f'-------edge tts time:{time.time()-t:.4f}s')\n        if self.input_stream.getbuffer().nbytes<=0: #edgetts err\n            print('edgetts err!!!!!')\n            return\n        \n        self.input_stream.seek(0)\n        stream = self.__create_bytes_stream(self.input_stream)\n        streamlen = stream.shape[0]\n        idx=0\n        while streamlen >= self.chunk and self.state==State.RUNNING:\n            self.parent.put_audio_frame(stream[idx:idx+self.chunk])\n            streamlen -= self.chunk\n            idx += self.chunk\n        #if streamlen>0:  #skip last frame(not 20ms)\n        #    self.queue.put(stream[idx:])\n        self.input_stream.seek(0)\n        self.input_stream.truncate() \n\n    def __create_bytes_stream(self,byte_stream):\n        #byte_stream=BytesIO(buffer)\n        stream, sample_rate = sf.read(byte_stream) # [T*sample_rate,] float64\n        print(f'[INFO]tts audio stream {sample_rate}: {stream.shape}')\n        stream = stream.astype(np.float32)\n\n        if stream.ndim > 1:\n            print(f'[WARN] audio has {stream.shape[1]} channels, only use the first.')\n            stream = stream[:, 0]\n    \n        if sample_rate != self.sample_rate and stream.shape[0]>0:\n            print(f'[WARN] audio sample rate is {sample_rate}, resampling into {self.sample_rate}.')\n            stream = resampy.resample(x=stream, sr_orig=sample_rate, sr_new=self.sample_rate)\n\n        return stream\n    \n    async def __main(self,voicename: str, text: str):\n        try:\n            communicate = edge_tts.Communicate(text, voicename)\n\n            #with open(OUTPUT_FILE, \"wb\") as file:\n            first = True\n            async for chunk in communicate.stream():\n                if first:\n                    first = False\n                if chunk[\"type\"] == \"audio\" and self.state==State.RUNNING:\n                    #self.push_audio(chunk[\"data\"])\n                    self.input_stream.write(chunk[\"data\"])\n                    #file.write(chunk[\"data\"])\n                elif chunk[\"type\"] == \"WordBoundary\":\n                    pass\n        except Exception as e:\n            print(e)\n\n###########################################################################################\nclass VoitsTTS(BaseTTS):\n    def txt_to_audio(self,msg): \n        self.stream_tts(\n            self.gpt_sovits(\n                msg,\n                self.opt.REF_FILE,  \n                self.opt.REF_TEXT,\n                \"zh\", #en args.language,\n                self.opt.TTS_SERVER, #\"http://127.0.0.1:5000\", #args.server_url,\n            )\n        )\n\n    def gpt_sovits(self, text, reffile, reftext,language, server_url) -> Iterator[bytes]:\n        start = time.perf_counter()\n        req={\n            'text':text,\n            'text_lang':language,\n            'ref_audio_path':reffile,\n            'prompt_text':reftext,\n            'prompt_lang':language,\n            'media_type':'ogg',\n            'streaming_mode':True\n        }\n        # req[\"text\"] = text\n        # req[\"text_language\"] = language\n        # req[\"character\"] = character\n        # req[\"emotion\"] = emotion\n        # #req[\"stream_chunk_size\"] = stream_chunk_size  # you can reduce it to get faster response, but degrade quality\n        # req[\"streaming_mode\"] = True\n        try:\n            res = requests.post(\n                f\"{server_url}/tts\",\n                json=req,\n                stream=True,\n            )\n            end = time.perf_counter()\n            print(f\"gpt_sovits Time to make POST: {end-start}s\")\n\n            if res.status_code != 200:\n                print(\"Error:\", res.text)\n                return\n                \n            first = True\n        \n            for chunk in res.iter_content(chunk_size=None): #12800 1280 32K*20ms*2\n                print('chunk len:',len(chunk))\n                if first:\n                    end = time.perf_counter()\n                    print(f\"gpt_sovits Time to first chunk: {end-start}s\")\n                    first = False\n                if chunk and self.state==State.RUNNING:\n                    yield chunk\n            #print(\"gpt_sovits response.elapsed:\", res.elapsed)\n        except Exception as e:\n            print(e)\n\n    def __create_bytes_stream(self,byte_stream):\n        #byte_stream=BytesIO(buffer)\n        stream, sample_rate = sf.read(byte_stream) # [T*sample_rate,] float64\n        print(f'[INFO]tts audio stream {sample_rate}: {stream.shape}')\n        stream = stream.astype(np.float32)\n\n        if stream.ndim > 1:\n            print(f'[WARN] audio has {stream.shape[1]} channels, only use the first.')\n            stream = stream[:, 0]\n    \n        if sample_rate != self.sample_rate and stream.shape[0]>0:\n            print(f'[WARN] audio sample rate is {sample_rate}, resampling into {self.sample_rate}.')\n            stream = resampy.resample(x=stream, sr_orig=sample_rate, sr_new=self.sample_rate)\n\n        return stream\n\n    def stream_tts(self,audio_stream):\n        for chunk in audio_stream:\n            if chunk is not None and len(chunk)>0:          \n                #stream = np.frombuffer(chunk, dtype=np.int16).astype(np.float32) / 32767\n                #stream = resampy.resample(x=stream, sr_orig=32000, sr_new=self.sample_rate)\n                byte_stream=BytesIO(chunk)\n                stream = self.__create_bytes_stream(byte_stream)\n                streamlen = stream.shape[0]\n                idx=0\n                while streamlen >= self.chunk:\n                    self.parent.put_audio_frame(stream[idx:idx+self.chunk])\n                    streamlen -= self.chunk\n                    idx += self.chunk \n\n###########################################################################################\nclass CosyVoiceTTS(BaseTTS):\n    def txt_to_audio(self,msg): \n        self.stream_tts(\n            self.cosy_voice(\n                msg,\n                self.opt.REF_FILE,  \n                self.opt.REF_TEXT,\n                \"zh\", #en args.language,\n                self.opt.TTS_SERVER, #\"http://127.0.0.1:5000\", #args.server_url,\n            )\n        )\n\n    def cosy_voice(self, text, reffile, reftext,language, server_url) -> Iterator[bytes]:\n        start = time.perf_counter()\n        payload = {\n            'tts_text': text,\n            'prompt_text': reftext\n        }\n        try:\n            files = [('prompt_wav', ('prompt_wav', open(reffile, 'rb'), 'application/octet-stream'))]\n            res = requests.request(\"GET\", f\"{server_url}/inference_zero_shot\", data=payload, files=files, stream=True)\n            \n            end = time.perf_counter()\n            print(f\"cosy_voice Time to make POST: {end-start}s\")\n\n            if res.status_code != 200:\n                print(\"Error:\", res.text)\n                return\n                \n            first = True\n        \n            for chunk in res.iter_content(chunk_size=8820): # 882 22.05K*20ms*2\n                if first:\n                    end = time.perf_counter()\n                    print(f\"cosy_voice Time to first chunk: {end-start}s\")\n                    first = False\n                if chunk and self.state==State.RUNNING:\n                    yield chunk\n        except Exception as e:\n            print(e)\n\n    def stream_tts(self,audio_stream):\n        for chunk in audio_stream:\n            if chunk is not None and len(chunk)>0:          \n                stream = np.frombuffer(chunk, dtype=np.int16).astype(np.float32) / 32767\n                stream = resampy.resample(x=stream, sr_orig=22050, sr_new=self.sample_rate)\n                #byte_stream=BytesIO(buffer)\n                #stream = self.__create_bytes_stream(byte_stream)\n                streamlen = stream.shape[0]\n                idx=0\n                while streamlen >= self.chunk:\n                    self.parent.put_audio_frame(stream[idx:idx+self.chunk])\n                    streamlen -= self.chunk\n                    idx += self.chunk \n\n###########################################################################################\nclass XTTS(BaseTTS):\n    def __init__(self, opt, parent):\n        super().__init__(opt,parent)\n        self.speaker = self.get_speaker(opt.REF_FILE, opt.TTS_SERVER)\n\n    def txt_to_audio(self,msg): \n        self.stream_tts(\n            self.xtts(\n                msg,\n                self.speaker,\n                \"zh-cn\", #en args.language,\n                self.opt.TTS_SERVER, #\"http://localhost:9000\", #args.server_url,\n                \"20\" #args.stream_chunk_size\n            )\n        )\n\n    def get_speaker(self,ref_audio,server_url):\n        files = {\"wav_file\": (\"reference.wav\", open(ref_audio, \"rb\"))}\n        response = requests.post(f\"{server_url}/clone_speaker\", files=files)\n        return response.json()\n\n    def xtts(self,text, speaker, language, server_url, stream_chunk_size) -> Iterator[bytes]:\n        start = time.perf_counter()\n        speaker[\"text\"] = text\n        speaker[\"language\"] = language\n        speaker[\"stream_chunk_size\"] = stream_chunk_size  # you can reduce it to get faster response, but degrade quality\n        try:\n            res = requests.post(\n                f\"{server_url}/tts_stream\",\n                json=speaker,\n                stream=True,\n            )\n            end = time.perf_counter()\n            print(f\"xtts Time to make POST: {end-start}s\")\n\n            if res.status_code != 200:\n                print(\"Error:\", res.text)\n                return\n\n            first = True\n        \n            for chunk in res.iter_content(chunk_size=9600): #24K*20ms*2\n                if first:\n                    end = time.perf_counter()\n                    print(f\"xtts Time to first chunk: {end-start}s\")\n                    first = False\n                if chunk:\n                    yield chunk\n        except Exception as e:\n            print(e)\n    \n    def stream_tts(self,audio_stream):\n        for chunk in audio_stream:\n            if chunk is not None and len(chunk)>0:          \n                stream = np.frombuffer(chunk, dtype=np.int16).astype(np.float32) / 32767\n                stream = resampy.resample(x=stream, sr_orig=24000, sr_new=self.sample_rate)\n                #byte_stream=BytesIO(buffer)\n                #stream = self.__create_bytes_stream(byte_stream)\n                streamlen = stream.shape[0]\n                idx=0\n                while streamlen >= self.chunk:\n                    self.parent.put_audio_frame(stream[idx:idx+self.chunk])\n                    streamlen -= self.chunk\n                    idx += self.chunk "
        },
        {
          "name": "ultralight",
          "type": "tree",
          "content": null
        },
        {
          "name": "wav2lip",
          "type": "tree",
          "content": null
        },
        {
          "name": "web",
          "type": "tree",
          "content": null
        },
        {
          "name": "webrtc.py",
          "type": "blob",
          "size": 8.076171875,
          "content": "###############################################################################\r\n#  Copyright (C) 2024 LiveTalking@lipku https://github.com/lipku/LiveTalking\r\n#  email: lipku@foxmail.com\r\n# \r\n#  Licensed under the Apache License, Version 2.0 (the \"License\");\r\n#  you may not use this file except in compliance with the License.\r\n#  You may obtain a copy of the License at\r\n#  \r\n#       http://www.apache.org/licenses/LICENSE-2.0\r\n# \r\n#  Unless required by applicable law or agreed to in writing, software\r\n#  distributed under the License is distributed on an \"AS IS\" BASIS,\r\n#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n#  See the License for the specific language governing permissions and\r\n#  limitations under the License.\r\n###############################################################################\r\n\r\nimport asyncio\r\nimport json\r\nimport logging\r\nimport threading\r\nimport time\r\nfrom typing import Tuple, Dict, Optional, Set, Union\r\nfrom av.frame import Frame\r\nfrom av.packet import Packet\r\nfrom av import AudioFrame\r\nimport fractions\r\nimport numpy as np\r\n\r\nAUDIO_PTIME = 0.020  # 20ms audio packetization\r\nVIDEO_CLOCK_RATE = 90000\r\nVIDEO_PTIME = 1 / 25  # 30fps\r\nVIDEO_TIME_BASE = fractions.Fraction(1, VIDEO_CLOCK_RATE)\r\nSAMPLE_RATE = 16000\r\nAUDIO_TIME_BASE = fractions.Fraction(1, SAMPLE_RATE)\r\n\r\n#from aiortc.contrib.media import MediaPlayer, MediaRelay\r\n#from aiortc.rtcrtpsender import RTCRtpSender\r\nfrom aiortc import (\r\n    MediaStreamTrack,\r\n)\r\n\r\nlogging.basicConfig()\r\nlogger = logging.getLogger(__name__)\r\n\r\n\r\nclass PlayerStreamTrack(MediaStreamTrack):\r\n    \"\"\"\r\n    A video track that returns an animated flag.\r\n    \"\"\"\r\n\r\n    def __init__(self, player, kind):\r\n        super().__init__()  # don't forget this!\r\n        self.kind = kind\r\n        self._player = player\r\n        self._queue = asyncio.Queue()\r\n        self.timelist = [] #记录最近包的时间戳\r\n        if self.kind == 'video':\r\n            self.framecount = 0\r\n            self.lasttime = time.perf_counter()\r\n            self.totaltime = 0\r\n    \r\n    _start: float\r\n    _timestamp: int\r\n\r\n    async def next_timestamp(self) -> Tuple[int, fractions.Fraction]:\r\n        if self.readyState != \"live\":\r\n            raise Exception\r\n\r\n        if self.kind == 'video':\r\n            if hasattr(self, \"_timestamp\"):\r\n                #self._timestamp = (time.time()-self._start) * VIDEO_CLOCK_RATE\r\n                self._timestamp += int(VIDEO_PTIME * VIDEO_CLOCK_RATE)\r\n                wait = self._start + (self._timestamp / VIDEO_CLOCK_RATE) - time.time()\r\n                # wait = self.timelist[0] + len(self.timelist)*VIDEO_PTIME - time.time()               \r\n                if wait>0:\r\n                    await asyncio.sleep(wait)\r\n                # if len(self.timelist)>=100:\r\n                #     self.timelist.pop(0)\r\n                # self.timelist.append(time.time())\r\n            else:\r\n                self._start = time.time()\r\n                self._timestamp = 0\r\n                self.timelist.append(self._start)\r\n                print('video start:',self._start)\r\n            return self._timestamp, VIDEO_TIME_BASE\r\n        else: #audio\r\n            if hasattr(self, \"_timestamp\"):\r\n                #self._timestamp = (time.time()-self._start) * SAMPLE_RATE\r\n                self._timestamp += int(AUDIO_PTIME * SAMPLE_RATE)\r\n                wait = self._start + (self._timestamp / SAMPLE_RATE) - time.time()\r\n                # wait = self.timelist[0] + len(self.timelist)*AUDIO_PTIME - time.time()\r\n                if wait>0:\r\n                    await asyncio.sleep(wait)\r\n                # if len(self.timelist)>=200:\r\n                #     self.timelist.pop(0)\r\n                #     self.timelist.pop(0)\r\n                # self.timelist.append(time.time())\r\n            else:\r\n                self._start = time.time()\r\n                self._timestamp = 0\r\n                self.timelist.append(self._start)\r\n                print('audio start:',self._start)\r\n            return self._timestamp, AUDIO_TIME_BASE\r\n\r\n    async def recv(self) -> Union[Frame, Packet]:\r\n        # frame = self.frames[self.counter % 30]            \r\n        self._player._start(self)\r\n        # if self.kind == 'video':\r\n        #     frame = await self._queue.get()\r\n        # else: #audio\r\n        #     if hasattr(self, \"_timestamp\"):\r\n        #         wait = self._start + self._timestamp / SAMPLE_RATE + AUDIO_PTIME - time.time()\r\n        #         if wait>0:\r\n        #             await asyncio.sleep(wait)\r\n        #         if self._queue.qsize()<1:\r\n        #             #frame = AudioFrame(format='s16', layout='mono', samples=320)\r\n        #             audio = np.zeros((1, 320), dtype=np.int16)\r\n        #             frame = AudioFrame.from_ndarray(audio, layout='mono', format='s16')\r\n        #             frame.sample_rate=16000\r\n        #         else:\r\n        #             frame = await self._queue.get()\r\n        #     else:\r\n        #         frame = await self._queue.get()\r\n        frame = await self._queue.get()\r\n        pts, time_base = await self.next_timestamp()\r\n        frame.pts = pts\r\n        frame.time_base = time_base\r\n        if frame is None:\r\n            self.stop()\r\n            raise Exception\r\n        if self.kind == 'video':\r\n            self.totaltime += (time.perf_counter() - self.lasttime)\r\n            self.framecount += 1\r\n            self.lasttime = time.perf_counter()\r\n            if self.framecount==100:\r\n                print(f\"------actual avg final fps:{self.framecount/self.totaltime:.4f}\")\r\n                self.framecount = 0\r\n                self.totaltime=0\r\n        return frame\r\n    \r\n    def stop(self):\r\n        super().stop()\r\n        if self._player is not None:\r\n            self._player._stop(self)\r\n            self._player = None\r\n\r\ndef player_worker_thread(\r\n    quit_event,\r\n    loop,\r\n    container,\r\n    audio_track,\r\n    video_track\r\n):\r\n    container.render(quit_event,loop,audio_track,video_track)\r\n\r\nclass HumanPlayer:\r\n\r\n    def __init__(\r\n        self, nerfreal, format=None, options=None, timeout=None, loop=False, decode=True\r\n    ):\r\n        self.__thread: Optional[threading.Thread] = None\r\n        self.__thread_quit: Optional[threading.Event] = None\r\n\r\n        # examine streams\r\n        self.__started: Set[PlayerStreamTrack] = set()\r\n        self.__audio: Optional[PlayerStreamTrack] = None\r\n        self.__video: Optional[PlayerStreamTrack] = None\r\n\r\n        self.__audio = PlayerStreamTrack(self, kind=\"audio\")\r\n        self.__video = PlayerStreamTrack(self, kind=\"video\")\r\n\r\n        self.__container = nerfreal\r\n\r\n\r\n    @property\r\n    def audio(self) -> MediaStreamTrack:\r\n        \"\"\"\r\n        A :class:`aiortc.MediaStreamTrack` instance if the file contains audio.\r\n        \"\"\"\r\n        return self.__audio\r\n\r\n    @property\r\n    def video(self) -> MediaStreamTrack:\r\n        \"\"\"\r\n        A :class:`aiortc.MediaStreamTrack` instance if the file contains video.\r\n        \"\"\"\r\n        return self.__video\r\n\r\n    def _start(self, track: PlayerStreamTrack) -> None:\r\n        self.__started.add(track)\r\n        if self.__thread is None:\r\n            self.__log_debug(\"Starting worker thread\")\r\n            self.__thread_quit = threading.Event()\r\n            self.__thread = threading.Thread(\r\n                name=\"media-player\",\r\n                target=player_worker_thread,\r\n                args=(\r\n                    self.__thread_quit,\r\n                    asyncio.get_event_loop(),\r\n                    self.__container,\r\n                    self.__audio,\r\n                    self.__video                   \r\n                ),\r\n            )\r\n            self.__thread.start()\r\n\r\n    def _stop(self, track: PlayerStreamTrack) -> None:\r\n        self.__started.discard(track)\r\n\r\n        if not self.__started and self.__thread is not None:\r\n            self.__log_debug(\"Stopping worker thread\")\r\n            self.__thread_quit.set()\r\n            self.__thread.join()\r\n            self.__thread = None\r\n\r\n        if not self.__started and self.__container is not None:\r\n            #self.__container.close()\r\n            self.__container = None\r\n\r\n    def __log_debug(self, msg: str, *args) -> None:\r\n        logger.debug(f\"HumanPlayer {msg}\", *args)\r\n"
        }
      ]
    }
  ]
}