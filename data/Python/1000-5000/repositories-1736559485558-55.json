{
  "metadata": {
    "timestamp": 1736559485558,
    "page": 55,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjYw",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "lllyasviel/ControlNet-v1-1-nightly",
      "stars": 4851,
      "defaultBranch": "main",
      "files": [
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 1.833984375,
          "content": ".idea/\n\ntraining/\nlightning_logs/\nimage_log/\n\n*.pth\n*.pt\n*.ckpt\n*.safetensors\n\n# Byte-compiled / optimized / DLL files\n__pycache__/\n*.py[cod]\n*$py.class\n\n# C extensions\n*.so\n\n# Distribution / packaging\n.Python\nbuild/\ndevelop-eggs/\ndist/\ndownloads/\neggs/\n.eggs/\nlib/\nlib64/\nparts/\nsdist/\nvar/\nwheels/\npip-wheel-metadata/\nshare/python-wheels/\n*.egg-info/\n.installed.cfg\n*.egg\nMANIFEST\n\n# PyInstaller\n#  Usually these files are written by a python script from a template\n#  before PyInstaller builds the exe, so as to inject date/other infos into it.\n*.manifest\n*.spec\n\n# Installer logs\npip-log.txt\npip-delete-this-directory.txt\n\n# Unit test / coverage reports\nhtmlcov/\n.tox/\n.nox/\n.coverage\n.coverage.*\n.cache\nnosetests.xml\ncoverage.xml\n*.cover\n*.py,cover\n.hypothesis/\n.pytest_cache/\n\n# Translations\n*.mo\n*.pot\n\n# Django stuff:\n*.log\nlocal_settings.py\ndb.sqlite3\ndb.sqlite3-journal\n\n# Flask stuff:\ninstance/\n.webassets-cache\n\n# Scrapy stuff:\n.scrapy\n\n# Sphinx documentation\ndocs/_build/\n\n# PyBuilder\ntarget/\n\n# Jupyter Notebook\n.ipynb_checkpoints\n\n# IPython\nprofile_default/\nipython_config.py\n\n# pyenv\n.python-version\n\n# pipenv\n#   According to pypa/pipenv#598, it is recommended to include Pipfile.lock in version control.\n#   However, in case of collaboration, if having platform-specific dependencies or dependencies\n#   having no cross-platform support, pipenv may install dependencies that don't work, or not\n#   install all needed dependencies.\n#Pipfile.lock\n\n# PEP 582; used by e.g. github.com/David-OConnor/pyflow\n__pypackages__/\n\n# Celery stuff\ncelerybeat-schedule\ncelerybeat.pid\n\n# SageMath parsed files\n*.sage.py\n\n# Environments\n.env\n.venv\nenv/\nvenv/\nENV/\nenv.bak/\nvenv.bak/\n\n# Spyder project settings\n.spyderproject\n.spyproject\n\n# Rope project settings\n.ropeproject\n\n# mkdocs documentation\n/site\n\n# mypy\n.mypy_cache/\n.dmypy.json\ndmypy.json\n\n# Pyre type checker\n.pyre/\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 31.2353515625,
          "content": "# ControlNet 1.1\n\nThis is the official release of ControlNet 1.1.\n\nControlNet 1.1 has the exactly same architecture with ControlNet 1.0. \n\nWe promise that we will not change the neural network architecture before ControlNet 1.5 (at least, and hopefully we will never change the network architecture). Perhaps this is the best news in ControlNet 1.1.\n\nControlNet 1.1 includes all previous models with improved robustness and result quality. Several new models are added.\n\nNote that we are still working on [updating this to A1111](https://github.com/Mikubill/sd-webui-controlnet/issues/736). \n\nThis repo will be merged to [ControlNet](https://github.com/lllyasviel/ControlNet) after we make sure that everything is OK.\n\n**Note that we are actively editing this page now. The information in this page will be more detailed and finalized when ControlNet 1.1 is ready.**\n\n# This Github Repo is NOT an A1111 Extension\n\nPlease do not copy the URL of this repo into your A1111.\n\nIf you want to use ControlNet 1.1 in A1111, you only need to install https://github.com/Mikubill/sd-webui-controlnet , and only follow the instructions in that page.\n\nThis project is for research use and academic experiments. Again, do NOT install \"ControlNet-v1-1-nightly\" into your A1111.\n\n# How to use ControlNet 1.1 in A1111?\n\nThe Beta Test for A1111 Is Started. \n\nThe A1111 plugin is: https://github.com/Mikubill/sd-webui-controlnet\n\nNote that if you use A1111, you only need to follow the instructions in the above link. (You can ignore all installation steps in this page if you use A1111.)\n\n**For researchers who are not familiar with A1111:** The A1111 plugin supports arbitrary combination of arbitrary number of ControlNets, arbitrary community models, arbitrary LoRAs, and arbitrary sampling methods! We should definitely try it!\n\nNote that our official support for “Multi-ControlNet” is A1111-only. Please use [Automatic1111 with Multi-ControlNet](https://github.com/Mikubill/sd-webui-controlnet#Multi-ControlNet) if you want to use multiple ControlNets at the same time. The ControlNet project perfectly supports combining multiple ControlNets, and all production-ready ControlNets are extensively tested with multiple ControlNets combined.\n\n# Model Specification\n\nStarting from ControlNet 1.1, we begin to use the Standard ControlNet Naming Rules (SCNNRs) to name all models. We hope that this naming rule can improve the user experience.\n\n![img](github_docs/imgs/spec.png)\n\nControlNet 1.1 include 14 models (11 production-ready models and 3 experimental models):\n\n    control_v11p_sd15_canny\n    control_v11p_sd15_mlsd\n    control_v11f1p_sd15_depth\n    control_v11p_sd15_normalbae\n    control_v11p_sd15_seg\n    control_v11p_sd15_inpaint\n    control_v11p_sd15_lineart\n    control_v11p_sd15s2_lineart_anime\n    control_v11p_sd15_openpose\n    control_v11p_sd15_scribble\n    control_v11p_sd15_softedge\n    control_v11e_sd15_shuffle\n    control_v11e_sd15_ip2p\n    control_v11f1e_sd15_tile\n\nYou can download all those models from our [HuggingFace Model Page](https://huggingface.co/lllyasviel/ControlNet-v1-1/tree/main). All these models should be put in the folder \"models\".\n\nYou need to download Stable Diffusion 1.5 model [\"v1-5-pruned.ckpt\"](https://huggingface.co/runwayml/stable-diffusion-v1-5/tree/main) and put it in the folder \"models\".\n\nOur python codes will automatically download other annotator models like HED and OpenPose. Nevertheless, if you want to manually download these, you can download all other annotator models from [here](https://huggingface.co/lllyasviel/Annotators/tree/main). All these models should be put in folder \"annotator/ckpts\". \n\nTo install:\n\n    conda env create -f environment.yaml\n    conda activate control-v11\n\nNote that if you use 8GB GPU, you need to set \"save_memory = True\" in \"config.py\".\n\n## ControlNet 1.1 Depth\n\nControl Stable Diffusion with Depth Maps.\n\nModel file: control_v11f1p_sd15_depth.pth\n\nConfig file: control_v11f1p_sd15_depth.yaml\n\nTraining data: Midas depth (resolution 256/384/512) + Leres Depth (resolution 256/384/512) + Zoe Depth (resolution 256/384/512). Multiple depth map generator at multiple resolution as data augmentation.\n\nAcceptable Preprocessors: Depth_Midas, Depth_Leres, Depth_Zoe. This model is highly robust and can work on real depth map from rendering engines.\n\n    python gradio_depth.py\n\nNon-cherry-picked batch test with random seed 12345 (\"a handsome man\"):\n\n![img](github_docs/imgs/depth_1.png)\n\n**Update**\n\n2023/04/14: 72 hours ago we uploaded a wrong model \"control_v11p_sd15_depth\" by mistake. That model is an intermediate checkpoint during the training. That model is not converged and may cause distortion in results. We uploaded the correct depth model as \"control_v11f1p_sd15_depth\". The \"f1\" means bug fix 1. The incorrect model is removed. Sorry for the inconvenience.\n\n**Improvements in Depth 1.1:**\n\n1. The training dataset of previous cnet 1.0 has several problems including (1) a small group of greyscale human images are duplicated thousands of times (!!), causing the previous model somewhat likely to generate grayscale human images; (2) some images has low quality, very blurry, or significant JPEG artifacts; (3) a small group of images has wrong paired prompts caused by a mistake in our data processing scripts. The new model fixed all problems of the training dataset and should be more reasonable in many cases.\n2. The new depth model is a relatively unbiased model. It is not trained with some specific type of depth by some specific depth estimation method. It is not over-fitted to one preprocessor. This means this model will work better with different depth estimation, different preprocessor resolutions, or even with real depth created by 3D engines.\n3. Some reasonable data augmentations are applied to training, like random left-right flipping.\n4. The model is resumed from depth 1.0, and it should work well in all cases where depth 1.0 works well. If not, please open an issue with image, and we will take a look at your case. Depth 1.1 works well in many failure cases of depth 1.0.\n5. If you use Midas depth (the \"depth\" in webui plugin) with 384 preprocessor resolution, the difference between depth 1.0 and 1.1 should be minimal. However, if you try other preprocessor resolutions or other preprocessors (like leres and zoe), the depth 1.1 is expected to be a bit better than 1.0.\n\n## ControlNet 1.1 Normal\n\nControl Stable Diffusion with Normal Maps.\n\nModel file: control_v11p_sd15_normalbae.pth\n\nConfig file: control_v11p_sd15_normalbae.yaml\n\nTraining data: [Bae's](https://github.com/baegwangbin/surface_normal_uncertainty) normalmap estimation method.\n\nAcceptable Preprocessors: Normal BAE. This model can accept normal maps from rendering engines as long as the normal map follows [ScanNet's](http://www.scan-net.org/) protocol. That is to say, the color of your normal map should look like [the second column of this image](https://raw.githubusercontent.com/baegwangbin/surface_normal_uncertainty/main/figs/readme_scannet.png).\n\nNote that this method is much more reasonable than the normal-from-midas method in ControlNet 1.0. The previous method will be abandoned.\n\n    python gradio_normalbae.py\n\nNon-cherry-picked batch test with random seed 12345 (\"a man made of flowers\"):\n\n![img](github_docs/imgs/normal_1.png)\n\nNon-cherry-picked batch test with random seed 12345 (\"room\"):\n\n![img](github_docs/imgs/normal_2.png)\n\n**Improvements in Normal 1.1:**\n\n1. The normal-from-midas method in Normal 1.0 is neither reasonable nor physically correct. That method does not work very well in many images. The normal 1.0 model cannot interpret real normal maps created by rendering engines.\n2. This Normal 1.1 is much more reasonable because the preprocessor is trained to estimate normal maps with a relatively correct protocol (NYU-V2's visualization method). This means the Normal 1.1 can interpret real normal maps from rendering engines as long as the colors are correct (blue is front, red is left, green is top).\n3. In our test, this model is robust and can achieve similar performance to the depth model. In previous CNET 1.0, the Normal 1.0 is not very frequently used. But this Normal 2.0 is much improved and has potential to be used much more frequently.\n\n## ControlNet 1.1 Canny\n\nControl Stable Diffusion with Canny Maps.\n\nModel file: control_v11p_sd15_canny.pth\n\nConfig file: control_v11p_sd15_canny.yaml\n\nTraining data: Canny with random thresholds.\n\nAcceptable Preprocessors: Canny.\n\nWe fixed several problems in previous training datasets.\n\n    python gradio_canny.py\n\nNon-cherry-picked batch test with random seed 12345 (\"dog in a room\"):\n\n![img](github_docs/imgs/canny_1.png)\n\n**Improvements in Canny 1.1:**\n\n1. The training dataset of previous cnet 1.0 has several problems including (1) a small group of greyscale human images are duplicated thousands of times (!!), causing the previous model somewhat likely to generate grayscale human images; (2) some images has low quality, very blurry, or significant JPEG artifacts; (3) a small group of images has wrong paired prompts caused by a mistake in our data processing scripts. The new model fixed all problems of the training dataset and should be more reasonable in many cases.\n2. Because the Canny model is one of the most important (perhaps the most frequently used) ControlNet, we used a fund to train it on a machine with 8 Nvidia A100 80G with batchsize 8×32=256 for 3 days, spending 72×30=2160 USD (8 A100 80G with 30 USD/hour). The model is resumed from Canny 1.0.\n3. Some reasonable data augmentations are applied to training, like random left-right flipping.\n4. Although it is difficult to evaluate a ControlNet, we find Canny 1.1 is a bit more robust and a bit higher visual quality than Canny 1.0. \n\n## ControlNet 1.1 MLSD\n\nControl Stable Diffusion with M-LSD straight lines.\n\nModel file: control_v11p_sd15_mlsd.pth\n\nConfig file: control_v11p_sd15_mlsd.yaml\n\nTraining data: M-LSD Lines.\n\nAcceptable Preprocessors: MLSD.\n\nWe fixed several problems in previous training datasets. The model is resumed from ControlNet 1.0 and trained with 200 GPU hours of A100 80G.\n\n    python gradio_mlsd.py\n\nNon-cherry-picked batch test with random seed 12345 (\"room\"):\n\n![img](github_docs/imgs/mlsd_1.png)\n\n**Improvements in MLSD 1.1:**\n\n1. The training dataset of previous cnet 1.0 has several problems including (1) a small group of greyscale human images are duplicated thousands of times (!!), causing the previous model somewhat likely to generate grayscale human images; (2) some images has low quality, very blurry, or significant JPEG artifacts; (3) a small group of images has wrong paired prompts caused by a mistake in our data processing scripts. The new model fixed all problems of the training dataset and should be more reasonable in many cases.\n2. We enlarged the training dataset by adding 300K more images by using MLSD to find images with more than 16 straight lines in it.\n3. Some reasonable data augmentations are applied to training, like random left-right flipping.\n4. Resumed from MLSD 1.0 with continued training with 200 GPU hours of A100 80G.\n\n## ControlNet 1.1 Scribble\n\nControl Stable Diffusion with Scribbles.\n\nModel file: control_v11p_sd15_scribble.pth\n\nConfig file: control_v11p_sd15_scribble.yaml\n\nTraining data: Synthesized scribbles.\n\nAcceptable Preprocessors: Synthesized scribbles (Scribble_HED, Scribble_PIDI, etc.) or hand-drawn scribbles.\n\nWe fixed several problems in previous training datasets. The model is resumed from ControlNet 1.0 and trained with 200 GPU hours of A100 80G.\n\n    # To test synthesized scribbles\n    python gradio_scribble.py\n    # To test hand-drawn scribbles in an interactive demo\n    python gradio_interactive.py\n\nNon-cherry-picked batch test with random seed 12345 (\"man in library\"):\n\n![img](github_docs/imgs/scribble_1.png)\n\nNon-cherry-picked batch test with random seed 12345 (interactive, \"the beautiful landscape\"):\n\n![img](github_docs/imgs/scribble_2.png)\n\n**Improvements in Scribble 1.1:**\n\n1. The training dataset of previous cnet 1.0 has several problems including (1) a small group of greyscale human images are duplicated thousands of times (!!), causing the previous model somewhat likely to generate grayscale human images; (2) some images has low quality, very blurry, or significant JPEG artifacts; (3) a small group of images has wrong paired prompts caused by a mistake in our data processing scripts. The new model fixed all problems of the training dataset and should be more reasonable in many cases.\n2. We find out that users sometimes like to draw very thick scribbles. Because of that, we used more aggressive random morphological transforms to synthesize scribbles. This model should work well even when the scribbles are relatively thick (the maximum width of training data is 24-pixel-width scribble in a 512 canvas, but it seems to work well even for a bit wider scribbles; the minimum width is 1 pixel).\n3. Resumed from Scribble 1.0, continued with 200 GPU hours of A100 80G.\n\n## ControlNet 1.1 Soft Edge\n\nControl Stable Diffusion with Soft Edges.\n\nModel file: control_v11p_sd15_softedge.pth\n\nConfig file: control_v11p_sd15_softedge.yaml\n\nTraining data: SoftEdge_PIDI, SoftEdge_PIDI_safe, SoftEdge_HED, SoftEdge_HED_safe.\n\nAcceptable Preprocessors: SoftEdge_PIDI, SoftEdge_PIDI_safe, SoftEdge_HED, SoftEdge_HED_safe.\n\nThis model is significantly improved compared to previous model. All users should update as soon as possible.\n\nNew in ControlNet 1.1: now we added a new type of soft edge called \"SoftEdge_safe\". This is motivated by the fact that HED or PIDI tends to hide a corrupted greyscale version of the original image inside the soft estimation, and such hidden patterns can distract ControlNet, leading to bad results. The solution is to use a pre-processing to quantize the edge maps into several levels so that the hidden patterns can be completely removed. The implementation is [in the 78-th line of annotator/util.py](https://github.com/lllyasviel/ControlNet-v1-1-nightly/blob/4c9560ebe7679daac53a0599a11b9b7cd984ac55/annotator/util.py#L78).\n\nThe perforamce can be roughly noted as:\n\nRobustness: SoftEdge_PIDI_safe > SoftEdge_HED_safe >> SoftEdge_PIDI > SoftEdge_HED\n\nMaximum result quality: SoftEdge_HED > SoftEdge_PIDI > SoftEdge_HED_safe > SoftEdge_PIDI_safe\n\nConsidering the trade-off, we recommend to use SoftEdge_PIDI by default. In most cases it works very well.\n\n    python gradio_softedge.py\n\nNon-cherry-picked batch test with random seed 12345 (\"a handsome man\"):\n\n![img](github_docs/imgs/softedge_1.png)\n\n**Improvements in Soft Edge 1.1:**\n\n1. Soft Edge 1.1 was called HED 1.0 in previous ControlNet.\n2. The training dataset of previous cnet 1.0 has several problems including (1) a small group of greyscale human images are duplicated thousands of times (!!), causing the previous model somewhat likely to generate grayscale human images; (2) some images has low quality, very blurry, or significant JPEG artifacts; (3) a small group of images has wrong paired prompts caused by a mistake in our data processing scripts. The new model fixed all problems of the training dataset and should be more reasonable in many cases.\n3. The Soft Edge 1.1 is significantly (in nealy 100\\% cases) better than HED 1.0. This is mainly because HED or PIDI estimator tend to hide a corrupted greyscale version of original image inside the soft edge map and the previous model HED 1.0 is over-fitted to restore that hidden corrupted image rather than perform boundary-aware diffusion. The training of Soft Edge 1.1 used 75\\% \"safe\" filtering to remove such hidden corrupted greyscale images insider control maps. This makes the Soft Edge 1.1 very robust. In out test, Soft Edge 1.1 is as usable as the depth model and has potential to be more frequently used.\n\n## ControlNet 1.1 Segmentation\n\nControl Stable Diffusion with Semantic Segmentation.\n\nModel file: control_v11p_sd15_seg.pth\n\nConfig file: control_v11p_sd15_seg.yaml\n\nTraining data: COCO + ADE20K.\n\nAcceptable Preprocessors: Seg_OFADE20K (Oneformer ADE20K), Seg_OFCOCO (Oneformer COCO), Seg_UFADE20K (Uniformer ADE20K), or manually created masks.\n\nNow the model can receive both type of ADE20K or COCO annotations. We find that recognizing the segmentation protocol is trivial for the ControlNet encoder and training the model of multiple segmentation protocols lead to better performance.\n\n    python gradio_seg.py\n\nNon-cherry-picked batch test with random seed 12345 (ADE20k protocol, \"house\"):\n\n![img](github_docs/imgs/seg_1.png)\n\nNon-cherry-picked batch test with random seed 12345 (COCO protocol, \"house\"):\n\n![img](github_docs/imgs/seg_2.png)\n\n**Improvements in Segmentation 1.1:**\n\n1. COCO protocol is supported. The previous Segmentation 1.0 supports about 150 colors, but Segmentation 1.1 supports another 182 colors from coco.\n2. Resumed from Segmentation 1.0. All previous inputs should still work.\n\n## ControlNet 1.1 Openpose\n\nControl Stable Diffusion with Openpose.\n\nModel file: control_v11p_sd15_openpose.pth\n\nConfig file: control_v11p_sd15_openpose.yaml\n\nThe model is trained and can accept the following combinations:\n\n* Openpose body\n* Openpose hand\n* Openpose face\n* Openpose body + Openpose hand\n* Openpose body + Openpose face\n* Openpose hand + Openpose face\n* Openpose body + Openpose hand + Openpose face\n\nHowever, providing all those combinations is too complicated. We recommend to provide the users with only two choices:\n\n* \"Openpose\" = Openpose body\n* \"Openpose Full\" = Openpose body + Openpose hand + Openpose face\n\nYou can try with the demo:\n\n    python gradio_openpose.py\n\nNon-cherry-picked batch test with random seed 12345 (\"man in suit\"):\n\n![img](github_docs/imgs/openpose_1.png)\n\nNon-cherry-picked batch test with random seed 12345 (multiple people in the wild, \"handsome boys in the party\"):\n\n![img](github_docs/imgs/openpose_2.png)\n\n**Improvements in Openpose 1.1:**\n\n1. The improvement of this model is mainly based on our improved implementation of OpenPose. We carefully reviewed the difference between the pytorch OpenPose and CMU's c++ openpose. Now the processor should be more accurate, especially for hands. The improvement of processor leads to the improvement of Openpose 1.1.\n2. More inputs are supported (hand and face).\n3. The training dataset of previous cnet 1.0 has several problems including (1) a small group of greyscale human images are duplicated thousands of times (!!), causing the previous model somewhat likely to generate grayscale human images; (2) some images has low quality, very blurry, or significant JPEG artifacts; (3) a small group of images has wrong paired prompts caused by a mistake in our data processing scripts. The new model fixed all problems of the training dataset and should be more reasonable in many cases.\n\n## ControlNet 1.1 Lineart\n\nControl Stable Diffusion with Linearts.\n\nModel file: control_v11p_sd15_lineart.pth\n\nConfig file: control_v11p_sd15_lineart.yaml\n\nThis model is trained on awacke1/Image-to-Line-Drawings. The preprocessor can generate detailed or coarse linearts from images (Lineart and Lineart_Coarse). The model is trained with sufficient data augmentation and can receive manually drawn linearts.\n\n    python gradio_lineart.py\n\nNon-cherry-picked batch test with random seed 12345 (detailed lineart extractor, \"bag\"):\n\n![img](github_docs/imgs/lineart_1.png)\n\nNon-cherry-picked batch test with random seed 12345 (coarse lineart extractor, \"Michael Jackson's concert\"):\n\n![img](github_docs/imgs/lineart_2.png)\n\nNon-cherry-picked batch test with random seed 12345 (use manually drawn linearts, \"wolf\"):\n\n![img](github_docs/imgs/lineart_3.png)\n\n\n## ControlNet 1.1 Anime Lineart\n\nControl Stable Diffusion with Anime Linearts.\n\nModel file: control_v11p_sd15s2_lineart_anime.pth\n\nConfig file: control_v11p_sd15s2_lineart_anime.yaml\n\nTraining data and implementation details: (description removed).\n\nThis model can take real anime line drawings or extracted line drawings as inputs.\n\nSome important notice:\n\n1. You need a file \"anything-v3-full.safetensors\" to run the demo. We will not provide the file. Please find that file on the Internet on your own.\n2. This model is trained with 3x token length and clip skip 2.\n3. This is a long prompt model. Unless you use LoRAs, results are better with long prompts.\n4. This model does not support Guess Mode.\n\nDemo:\n\n    python gradio_lineart_anime.py\n\n\nNon-cherry-picked batch test with random seed 12345 (\"1girl, in classroom, skirt, uniform, red hair, bag, green eyes\"):\n\n![img](github_docs/imgs/anime_3.png)\n\nNon-cherry-picked batch test with random seed 12345 (\"1girl, saber, at night, sword, green eyes, golden hair, stocking\"):\n\n![img](github_docs/imgs/anime_4.png)\n\nNon-cherry-picked batch test with random seed 12345 (extracted line drawing, \"1girl, Castle, silver hair, dress, Gemstone, cinematic lighting, mechanical hand, 4k, 8k, extremely detailed, Gothic, green eye\"):\n\n![img](github_docs/imgs/anime_6.png)\n\n## ControlNet 1.1 Shuffle\n\nControl Stable Diffusion with Content Shuffle.\n\nModel file: control_v11e_sd15_shuffle.pth\n\nConfig file: control_v11e_sd15_shuffle.yaml\n\nDemo:\n\n    python gradio_shuffle.py\n\nThe model is trained to reorganize images. [We use a random flow to shuffle the image and control Stable Diffusion to recompose the image.](github_docs/annotator.md#content-reshuffle)\n\nNon-cherry-picked batch test with random seed 12345 (\"hong kong\"):\n\n![img](github_docs/imgs/shuffle_1.png)\n\nIn the 6 images on the right, the left-top one is the \"shuffled\" image. All others are outputs.\n\nIn fact, since the ControlNet is trained to recompose images, we do not even need to shuffle the input - sometimes we can just use the original image as input.\n\nIn this way, this ControlNet can be guided by prompts or other ControlNets to change the image style.\n\nNote that this method has nothing to do with CLIP vision or some other models. \n\nThis is a pure ControlNet.\n\nNon-cherry-picked batch test with random seed 12345 (\"iron man\"):\n\n![img](github_docs/imgs/shuffle_2.png)\n\nNon-cherry-picked batch test with random seed 12345 (\"spider man\"):\n\n![img](github_docs/imgs/shuffle_3.png)\n\n**Multi-ControlNets** (A1111-only)\n\nSource Image (not used):\n\n<img src=\"https://github.com/lllyasviel/ControlNet-v1-1-nightly/assets/19834515/56050654-6a82-495c-8bdc-d63847053e54\" width=\"200\">\n\nCanny Image (Input):\n\n<img src=\"https://github.com/lllyasviel/ControlNet-v1-1-nightly/assets/19834515/5dcb3d28-b845-4752-948d-6357224ca2ef\" width=\"200\">\n\nShuffle Image (Input):\n\n<img src=\"https://github.com/lllyasviel/ControlNet-v1-1-nightly/assets/19834515/c0d98c17-d79b-49d8-96af-89b87c532820\" width=\"200\">\n\nOutputs:\n\n![image](https://github.com/lllyasviel/ControlNet-v1-1-nightly/assets/19834515/a4b30709-8393-43d1-9da2-5c6c5ea70e9c)\n\n(From: https://github.com/Mikubill/sd-webui-controlnet/issues/736#issuecomment-1509986321)\n\n**Important If You Implement Your Own Inference:**\n\nNote that this ControlNet requires to add a global average pooling \" x = torch.mean(x, dim=(2, 3), keepdim=True) \" between the ControlNet Encoder outputs and SD Unet layers. And the ControlNet must be put only on the conditional side of cfg scale. We recommend to use the \"global_average_pooling\" item in the yaml file to control such behaviors.\n\n~Note that this ControlNet Shuffle will be the one and only one image stylization method that we will maintain for the robustness in a long term support. We have tested other CLIP image encoder, Unclip, image tokenization, and image-based prompts but it seems that those methods do not work very well with user prompts or additional/multiple U-Net injections. See also the evidence [here](https://github.com/lllyasviel/ControlNet/issues/255), [here](https://github.com/Mikubill/sd-webui-controlnet/issues/547), and some other related issues.~ After some more recent researches/experiments, we plan to support more types of stylization methods in the future. \n\n## ControlNet 1.1 Instruct Pix2Pix\n\nControl Stable Diffusion with Instruct Pix2Pix.\n\nModel file: control_v11e_sd15_ip2p.pth\n\nConfig file: control_v11e_sd15_ip2p.yaml\n\nDemo:\n\n    python gradio_ip2p.py\n\nThis is a controlnet trained on the [Instruct Pix2Pix dataset](https://github.com/timothybrooks/instruct-pix2pix).\n\nDifferent from official Instruct Pix2Pix, this model is trained with 50\\% instruction prompts and 50\\% description prompts. For example, \"a cute boy\" is a description prompt, while \"make the boy cute\" is a instruction prompt.\n\nBecause this is a ControlNet, you do not need to trouble with original IP2P's double cfg tuning. And, this model can be applied to any base model.\n\nAlso, it seems that instructions like \"make it into X\" works better than \"make Y into X\".\n\nNon-cherry-picked batch test with random seed 12345 (\"make it on fire\"):\n\n![img](github_docs/imgs/ip2p_1.png)\n\nNon-cherry-picked batch test with random seed 12345 (\"make it winter\"):\n\n![img](github_docs/imgs/ip2p_2.png)\n\nWe mark this model as \"experimental\" because it sometimes needs cherry-picking. For example, here is non-cherry-picked batch test with random seed 12345 (\"make he iron man\"):\n\n![img](github_docs/imgs/ip2p_3.png)\n\n\n## ControlNet 1.1 Inpaint\n\nControl Stable Diffusion with Inpaint.\n\nModel file: control_v11p_sd15_inpaint.pth\n\nConfig file: control_v11p_sd15_inpaint.yaml\n\nDemo:\n\n    python gradio_inpaint.py\n\nSome notices:\n\n1. This inpainting ControlNet is trained with 50\\% random masks and 50\\% random optical flow occlusion masks. This means the model can not only support the inpainting application but also work on video optical flow warping. Perhaps we will provide some example in the future (depending on our workloads).\n2. We updated the gradio (2023/5/11) so that the standalone gradio codes in main ControlNet repo also do not change unmasked areas. Automatic 1111 users are not influenced.\n\nNon-cherry-picked batch test with random seed 12345 (\"a handsome man\"):\n\n![img](github_docs/imgs/inpaint_after_fix.png)\n\nSee also the Guidelines for [Using ControlNet Inpaint in Automatic 1111](https://github.com/Mikubill/sd-webui-controlnet/discussions/1143).\n\n## ControlNet 1.1 Tile\n\nUpdate 2023 April 25: The previously unfinished tile model is finished now. The new name is \"control_v11f1e_sd15_tile\". The \"f1e\" means 1st bug fix (\"f1\"), experimental (\"e\").  The previous \"control_v11u_sd15_tile\" is removed. Please update if your model name is \"v11u\".\n\nControl Stable Diffusion with Tiles.\n\nModel file: control_v11f1e_sd15_tile.pth\n\nConfig file: control_v11f1e_sd15_tile.yaml\n\nDemo:\n\n    python gradio_tile.py\n\nThe model can be used in many ways. Overall, the model has two behaviors:\n\n* Ignore the details in an image and generate new details.\n* Ignore global prompts if local tile semantics and prompts mismatch, and guide diffusion with local context.\n\nBecause the model can generate new details and ignore existing image details, we can use this model to remove bad details and add refined details. For example, remove blurring caused by image resizing.\n\nBelow is an example of 8x super resolution. This is a 64x64 dog image.\n\n![p](test_imgs/dog64.png)\n\nNon-cherry-picked batch test with random seed 12345 (\"dog on grassland\"):\n\n![img](github_docs/imgs/tile_new_1.png)\n\nNote that this model is not a super resolution model. It ignores the details in an image and generate new details. This means you can use it to fix bad details in an image.\n\nFor example, below is a dog image corrupted by Real-ESRGAN. This is a typical example that sometimes super resolution methds fail to upscale images when source context is too small.\n\n![p](test_imgs/dog_bad_sr.png)\n\nNon-cherry-picked batch test with random seed 12345 (\"dog on grassland\"):\n\n![img](github_docs/imgs/tile_new_2.png)\n\nIf your image already have good details, you can still use this model to replace image details. Note that Stable Diffusion's I2I can achieve similar effects but this model make it much easier for you to maintain the overall structure and only change details even with denoising strength 1.0 .\n\nNon-cherry-picked batch test with random seed 12345 (\"Silver Armor\"):\n\n![img](github_docs/imgs/tile_new_3.png)\n\nMore and more people begin to think about different methods to diffuse at tiles so that images can be very big (at 4k or 8k). \n\nThe problem is that, in Stable Diffusion, your prompts will always influent each tile.\n\nFor example, if your prompts are \"a beautiful girl\" and you split an image into 4×4=16 blocks and do diffusion in each block, then you are will get 16 \"beautiful girls\" rather than \"a beautiful girl\". This is a well-known problem.\n\nRight now people's solution is to use some meaningless prompts like \"clear, clear, super clear\" to diffuse blocks. But you can expect that the results will be bad if the denonising strength is high. And because the prompts are bad, the contents are pretty random.\n\nControlNet Tile can solve this problem. For a given tile, it recognizes what is inside the tile and increase the influence of that recognized semantics, and it also decreases the influence of global prompts if contents do not match.\n\nNon-cherry-picked batch test with random seed 12345 (\"a handsome man\"):\n\n![img](github_docs/imgs/tile_new_4.png)\n\nYou can see that the prompt is \"a handsome man\" but the model does not paint \"a handsome man\" on that tree leaves. Instead, it recognizes the tree leaves paint accordingly.\n\nIn this way, ControlNet is able to change the behavior of any Stable Diffusion model to perform diffusion in tiles. \n\n**Gallery of ControlNet Tile**\n\n*Note:* Our official support for tiled image upscaling is A1111-only. The gradio example in this repo does not include tiled upscaling scripts. Please use the A1111 extension to perform tiled upscaling (with other tiling scripts like Ultimate SD Upscale or Tiled Diffusion/VAE).\n\nFrom https://github.com/Mikubill/sd-webui-controlnet/discussions/1142#discussioncomment-5788601\n\n(Output, **Click image to see full resolution**)\n\n![grannie-comp](https://user-images.githubusercontent.com/54312595/235352555-846982dc-eba2-4e6a-8dfa-076a5e9ee4fd.jpg)\n\n(Zooming-in of outputs)\n\n![grannie-Comp_face](https://user-images.githubusercontent.com/54312595/235352557-8f90e59d-8d03-4909-b805-8643940973d0.jpg)\n\n![grannie-Comp_torso](https://user-images.githubusercontent.com/54312595/235352562-ad0a5618-a1dd-40d0-9bfe-65e9786b496f.jpg)\n\n![grannie-Comp_torso2](https://user-images.githubusercontent.com/54312595/235352567-4e9a887f-142f-4f65-8084-d4c7f602985b.jpg)\n\nFrom https://github.com/Mikubill/sd-webui-controlnet/discussions/1142#discussioncomment-5788617\n\n(Input)\n\n![image](https://user-images.githubusercontent.com/34932866/235639514-31df5838-e251-4a17-b6ad-a678cdb8a58d.png)\n\n(Output, **Click image to see full resolution**)\n![image](https://user-images.githubusercontent.com/34932866/235639422-1f95d228-f902-4d94-b57b-e67460a719ef.png)\n\nFrom: https://github.com/lllyasviel/ControlNet-v1-1-nightly/issues/50#issuecomment-1541914890\n\n(Input)\n\n![image](https://github.com/lllyasviel/ControlNet-v1-1-nightly/assets/19834515/9132700e-b2f9-4a33-a589-611ba234d325)\n\n(Output, **Click image to see full resolution**, note that this example is extremely challenging)\n\n![image](https://github.com/lllyasviel/ControlNet-v1-1-nightly/assets/19834515/609acf87-1e51-4c03-85dc-37e486566158)\n\nFrom https://github.com/Mikubill/sd-webui-controlnet/discussions/1142#discussioncomment-5796326:\n\n(before)\n\n![2600914554720735184649534855329348215514636378-166329422](https://user-images.githubusercontent.com/31148570/236037445-f91a060b-698a-4cae-bf18-93796351da66.png)\n\n(after, **Click image to see full resolution**)\n![2600914554720735184649534855329348215514636383-1549088886](https://user-images.githubusercontent.com/31148570/236037509-ce24c816-f50f-4fe0-8c19-423bf30dad26.png)\n\n**Comparison to Midjourney V5/V5.1 coming soon.**\n\n# Annotate Your Own Data\n\nWe provide simple python scripts to process images.\n\n[See a gradio example here](github_docs/annotator.md).\n"
        },
        {
          "name": "annotator",
          "type": "tree",
          "content": null
        },
        {
          "name": "cldm",
          "type": "tree",
          "content": null
        },
        {
          "name": "config.py",
          "type": "blob",
          "size": 0.01953125,
          "content": "save_memory = False\n"
        },
        {
          "name": "environment.yaml",
          "type": "blob",
          "size": 0.8427734375,
          "content": "name: control-v11\nchannels:\n  - pytorch\n  - defaults\ndependencies:\n  - python=3.8.5\n  - pip=20.3\n  - cudatoolkit=11.3\n  - pytorch=1.12.1\n  - torchvision=0.13.1\n  - numpy=1.23.1\n  - pip:\n      - gradio==3.16.2\n      - albumentations==1.3.0\n      - opencv-contrib-python==4.3.0.36\n      - imageio==2.9.0\n      - imageio-ffmpeg==0.4.2\n      - pytorch-lightning==1.5.0\n      - omegaconf==2.1.1\n      - test-tube>=0.7.5\n      - streamlit==1.12.1\n      - einops==0.3.0\n      - transformers==4.19.2\n      - webdataset==0.2.5\n      - kornia==0.6\n      - open_clip_torch==2.0.2\n      - invisible-watermark>=0.1.5\n      - streamlit-drawable-canvas==0.8.0\n      - torchmetrics==0.6.0\n      - timm==0.6.12\n      - addict==2.4.0\n      - yapf==0.32.0\n      - prettytable==3.6.0\n      - safetensors==0.2.7\n      - basicsr==1.4.2\n      - fvcore\n      - pycocotools\n      - wandb\n"
        },
        {
          "name": "font",
          "type": "tree",
          "content": null
        },
        {
          "name": "github_docs",
          "type": "tree",
          "content": null
        },
        {
          "name": "gradio_annotator.py",
          "type": "blob",
          "size": 13.146484375,
          "content": "import gradio as gr\n\nfrom annotator.util import resize_image, HWC3\n\n\nmodel_canny = None\n\n\ndef canny(img, res, l, h):\n    img = resize_image(HWC3(img), res)\n    global model_canny\n    if model_canny is None:\n        from annotator.canny import CannyDetector\n        model_canny = CannyDetector()\n    result = model_canny(img, l, h)\n    return [result]\n\n\nmodel_hed = None\n\n\ndef hed(img, res):\n    img = resize_image(HWC3(img), res)\n    global model_hed\n    if model_hed is None:\n        from annotator.hed import HEDdetector\n        model_hed = HEDdetector()\n    result = model_hed(img)\n    return [result]\n\n\nmodel_pidi = None\n\n\ndef pidi(img, res):\n    img = resize_image(HWC3(img), res)\n    global model_pidi\n    if model_pidi is None:\n        from annotator.pidinet import PidiNetDetector\n        model_pidi = PidiNetDetector()\n    result = model_pidi(img)\n    return [result]\n\n\nmodel_mlsd = None\n\n\ndef mlsd(img, res, thr_v, thr_d):\n    img = resize_image(HWC3(img), res)\n    global model_mlsd\n    if model_mlsd is None:\n        from annotator.mlsd import MLSDdetector\n        model_mlsd = MLSDdetector()\n    result = model_mlsd(img, thr_v, thr_d)\n    return [result]\n\n\nmodel_midas = None\n\n\ndef midas(img, res):\n    img = resize_image(HWC3(img), res)\n    global model_midas\n    if model_midas is None:\n        from annotator.midas import MidasDetector\n        model_midas = MidasDetector()\n    result = model_midas(img)\n    return [result]\n\n\nmodel_zoe = None\n\n\ndef zoe(img, res):\n    img = resize_image(HWC3(img), res)\n    global model_zoe\n    if model_zoe is None:\n        from annotator.zoe import ZoeDetector\n        model_zoe = ZoeDetector()\n    result = model_zoe(img)\n    return [result]\n\n\nmodel_normalbae = None\n\n\ndef normalbae(img, res):\n    img = resize_image(HWC3(img), res)\n    global model_normalbae\n    if model_normalbae is None:\n        from annotator.normalbae import NormalBaeDetector\n        model_normalbae = NormalBaeDetector()\n    result = model_normalbae(img)\n    return [result]\n\n\nmodel_openpose = None\n\n\ndef openpose(img, res, hand_and_face):\n    img = resize_image(HWC3(img), res)\n    global model_openpose\n    if model_openpose is None:\n        from annotator.openpose import OpenposeDetector\n        model_openpose = OpenposeDetector()\n    result = model_openpose(img, hand_and_face)\n    return [result]\n\n\nmodel_uniformer = None\n\n\ndef uniformer(img, res):\n    img = resize_image(HWC3(img), res)\n    global model_uniformer\n    if model_uniformer is None:\n        from annotator.uniformer import UniformerDetector\n        model_uniformer = UniformerDetector()\n    result = model_uniformer(img)\n    return [result]\n\n\nmodel_lineart_anime = None\n\n\ndef lineart_anime(img, res):\n    img = resize_image(HWC3(img), res)\n    global model_lineart_anime\n    if model_lineart_anime is None:\n        from annotator.lineart_anime import LineartAnimeDetector\n        model_lineart_anime = LineartAnimeDetector()\n    result = model_lineart_anime(img)\n    return [result]\n\n\nmodel_lineart = None\n\n\ndef lineart(img, res, coarse=False):\n    img = resize_image(HWC3(img), res)\n    global model_lineart\n    if model_lineart is None:\n        from annotator.lineart import LineartDetector\n        model_lineart = LineartDetector()\n    result = model_lineart(img, coarse)\n    return [result]\n\n\nmodel_oneformer_coco = None\n\n\ndef oneformer_coco(img, res):\n    img = resize_image(HWC3(img), res)\n    global model_oneformer_coco\n    if model_oneformer_coco is None:\n        from annotator.oneformer import OneformerCOCODetector\n        model_oneformer_coco = OneformerCOCODetector()\n    result = model_oneformer_coco(img)\n    return [result]\n\n\nmodel_oneformer_ade20k = None\n\n\ndef oneformer_ade20k(img, res):\n    img = resize_image(HWC3(img), res)\n    global model_oneformer_ade20k\n    if model_oneformer_ade20k is None:\n        from annotator.oneformer import OneformerADE20kDetector\n        model_oneformer_ade20k = OneformerADE20kDetector()\n    result = model_oneformer_ade20k(img)\n    return [result]\n\n\nmodel_content_shuffler = None\n\n\ndef content_shuffler(img, res):\n    img = resize_image(HWC3(img), res)\n    global model_content_shuffler\n    if model_content_shuffler is None:\n        from annotator.shuffle import ContentShuffleDetector\n        model_content_shuffler = ContentShuffleDetector()\n    result = model_content_shuffler(img)\n    return [result]\n\n\nmodel_color_shuffler = None\n\n\ndef color_shuffler(img, res):\n    img = resize_image(HWC3(img), res)\n    global model_color_shuffler\n    if model_color_shuffler is None:\n        from annotator.shuffle import ColorShuffleDetector\n        model_color_shuffler = ColorShuffleDetector()\n    result = model_color_shuffler(img)\n    return [result]\n\n\nblock = gr.Blocks().queue()\nwith block:\n    with gr.Row():\n        gr.Markdown(\"## Canny Edge\")\n    with gr.Row():\n        with gr.Column():\n            input_image = gr.Image(source='upload', type=\"numpy\")\n            low_threshold = gr.Slider(label=\"low_threshold\", minimum=1, maximum=255, value=100, step=1)\n            high_threshold = gr.Slider(label=\"high_threshold\", minimum=1, maximum=255, value=200, step=1)\n            resolution = gr.Slider(label=\"resolution\", minimum=256, maximum=1024, value=512, step=64)\n            run_button = gr.Button(label=\"Run\")\n        with gr.Column():\n            gallery = gr.Gallery(label=\"Generated images\", show_label=False).style(height=\"auto\")\n    run_button.click(fn=canny, inputs=[input_image, resolution, low_threshold, high_threshold], outputs=[gallery])\n\n    with gr.Row():\n        gr.Markdown(\"## HED Edge\")\n    with gr.Row():\n        with gr.Column():\n            input_image = gr.Image(source='upload', type=\"numpy\")\n            resolution = gr.Slider(label=\"resolution\", minimum=256, maximum=1024, value=512, step=64)\n            run_button = gr.Button(label=\"Run\")\n        with gr.Column():\n            gallery = gr.Gallery(label=\"Generated images\", show_label=False).style(height=\"auto\")\n    run_button.click(fn=hed, inputs=[input_image, resolution], outputs=[gallery])\n\n    with gr.Row():\n        gr.Markdown(\"## Pidi Edge\")\n    with gr.Row():\n        with gr.Column():\n            input_image = gr.Image(source='upload', type=\"numpy\")\n            resolution = gr.Slider(label=\"resolution\", minimum=256, maximum=1024, value=512, step=64)\n            run_button = gr.Button(label=\"Run\")\n        with gr.Column():\n            gallery = gr.Gallery(label=\"Generated images\", show_label=False).style(height=\"auto\")\n    run_button.click(fn=pidi, inputs=[input_image, resolution], outputs=[gallery])\n\n    with gr.Row():\n        gr.Markdown(\"## MLSD Edge\")\n    with gr.Row():\n        with gr.Column():\n            input_image = gr.Image(source='upload', type=\"numpy\")\n            value_threshold = gr.Slider(label=\"value_threshold\", minimum=0.01, maximum=2.0, value=0.1, step=0.01)\n            distance_threshold = gr.Slider(label=\"distance_threshold\", minimum=0.01, maximum=20.0, value=0.1, step=0.01)\n            resolution = gr.Slider(label=\"resolution\", minimum=256, maximum=1024, value=384, step=64)\n            run_button = gr.Button(label=\"Run\")\n        with gr.Column():\n            gallery = gr.Gallery(label=\"Generated images\", show_label=False).style(height=\"auto\")\n    run_button.click(fn=mlsd, inputs=[input_image, resolution, value_threshold, distance_threshold], outputs=[gallery])\n\n    with gr.Row():\n        gr.Markdown(\"## MIDAS Depth\")\n    with gr.Row():\n        with gr.Column():\n            input_image = gr.Image(source='upload', type=\"numpy\")\n            resolution = gr.Slider(label=\"resolution\", minimum=256, maximum=1024, value=384, step=64)\n            run_button = gr.Button(label=\"Run\")\n        with gr.Column():\n            gallery = gr.Gallery(label=\"Generated images\", show_label=False).style(height=\"auto\")\n    run_button.click(fn=midas, inputs=[input_image, resolution], outputs=[gallery])\n\n\n    with gr.Row():\n        gr.Markdown(\"## Zoe Depth\")\n    with gr.Row():\n        with gr.Column():\n            input_image = gr.Image(source='upload', type=\"numpy\")\n            resolution = gr.Slider(label=\"resolution\", minimum=256, maximum=1024, value=512, step=64)\n            run_button = gr.Button(label=\"Run\")\n        with gr.Column():\n            gallery = gr.Gallery(label=\"Generated images\", show_label=False).style(height=\"auto\")\n    run_button.click(fn=zoe, inputs=[input_image, resolution], outputs=[gallery])\n\n    with gr.Row():\n        gr.Markdown(\"## Normal Bae\")\n    with gr.Row():\n        with gr.Column():\n            input_image = gr.Image(source='upload', type=\"numpy\")\n            resolution = gr.Slider(label=\"resolution\", minimum=256, maximum=1024, value=512, step=64)\n            run_button = gr.Button(label=\"Run\")\n        with gr.Column():\n            gallery = gr.Gallery(label=\"Generated images\", show_label=False).style(height=\"auto\")\n    run_button.click(fn=normalbae, inputs=[input_image, resolution], outputs=[gallery])\n\n    with gr.Row():\n        gr.Markdown(\"## Openpose\")\n    with gr.Row():\n        with gr.Column():\n            input_image = gr.Image(source='upload', type=\"numpy\")\n            hand_and_face = gr.Checkbox(label='Hand and Face', value=False)\n            resolution = gr.Slider(label=\"resolution\", minimum=256, maximum=1024, value=512, step=64)\n            run_button = gr.Button(label=\"Run\")\n        with gr.Column():\n            gallery = gr.Gallery(label=\"Generated images\", show_label=False).style(height=\"auto\")\n    run_button.click(fn=openpose, inputs=[input_image, resolution, hand_and_face], outputs=[gallery])\n\n    with gr.Row():\n        gr.Markdown(\"## Lineart Anime\")\n    with gr.Row():\n        with gr.Column():\n            input_image = gr.Image(source='upload', type=\"numpy\")\n            resolution = gr.Slider(label=\"resolution\", minimum=256, maximum=1024, value=512, step=64)\n            run_button = gr.Button(label=\"Run\")\n        with gr.Column():\n            gallery = gr.Gallery(label=\"Generated images\", show_label=False).style(height=\"auto\")\n    run_button.click(fn=lineart_anime, inputs=[input_image, resolution], outputs=[gallery])\n\n    with gr.Row():\n        gr.Markdown(\"## Lineart\")\n    with gr.Row():\n        with gr.Column():\n            input_image = gr.Image(source='upload', type=\"numpy\")\n            coarse = gr.Checkbox(label='Using coarse model', value=False)\n            resolution = gr.Slider(label=\"resolution\", minimum=256, maximum=1024, value=512, step=64)\n            run_button = gr.Button(label=\"Run\")\n        with gr.Column():\n            gallery = gr.Gallery(label=\"Generated images\", show_label=False).style(height=\"auto\")\n    run_button.click(fn=lineart, inputs=[input_image, resolution, coarse], outputs=[gallery])\n\n    with gr.Row():\n        gr.Markdown(\"## Uniformer Segmentation\")\n    with gr.Row():\n        with gr.Column():\n            input_image = gr.Image(source='upload', type=\"numpy\")\n            resolution = gr.Slider(label=\"resolution\", minimum=256, maximum=1024, value=512, step=64)\n            run_button = gr.Button(label=\"Run\")\n        with gr.Column():\n            gallery = gr.Gallery(label=\"Generated images\", show_label=False).style(height=\"auto\")\n    run_button.click(fn=uniformer, inputs=[input_image, resolution], outputs=[gallery])\n\n    with gr.Row():\n        gr.Markdown(\"## Oneformer COCO Segmentation\")\n    with gr.Row():\n        with gr.Column():\n            input_image = gr.Image(source='upload', type=\"numpy\")\n            resolution = gr.Slider(label=\"resolution\", minimum=256, maximum=1024, value=512, step=64)\n            run_button = gr.Button(label=\"Run\")\n        with gr.Column():\n            gallery = gr.Gallery(label=\"Generated images\", show_label=False).style(height=\"auto\")\n    run_button.click(fn=oneformer_coco, inputs=[input_image, resolution], outputs=[gallery])\n\n    with gr.Row():\n        gr.Markdown(\"## Oneformer ADE20K Segmentation\")\n    with gr.Row():\n        with gr.Column():\n            input_image = gr.Image(source='upload', type=\"numpy\")\n            resolution = gr.Slider(label=\"resolution\", minimum=256, maximum=1024, value=640, step=64)\n            run_button = gr.Button(label=\"Run\")\n        with gr.Column():\n            gallery = gr.Gallery(label=\"Generated images\", show_label=False).style(height=\"auto\")\n    run_button.click(fn=oneformer_ade20k, inputs=[input_image, resolution], outputs=[gallery])\n\n    with gr.Row():\n        gr.Markdown(\"## Content Shuffle\")\n    with gr.Row():\n        with gr.Column():\n            input_image = gr.Image(source='upload', type=\"numpy\")\n            resolution = gr.Slider(label=\"resolution\", minimum=256, maximum=1024, value=512, step=64)\n            run_button = gr.Button(label=\"Run\")\n        with gr.Column():\n            gallery = gr.Gallery(label=\"Generated images\", show_label=False).style(height=\"auto\")\n    run_button.click(fn=content_shuffler, inputs=[input_image, resolution], outputs=[gallery])\n\n    with gr.Row():\n        gr.Markdown(\"## Color Shuffle\")\n    with gr.Row():\n        with gr.Column():\n            input_image = gr.Image(source='upload', type=\"numpy\")\n            resolution = gr.Slider(label=\"resolution\", minimum=256, maximum=1024, value=512, step=64)\n            run_button = gr.Button(label=\"Run\")\n        with gr.Column():\n            gallery = gr.Gallery(label=\"Generated images\", show_label=False).style(height=\"auto\")\n    run_button.click(fn=color_shuffler, inputs=[input_image, resolution], outputs=[gallery])\n\n\nblock.launch(server_name='0.0.0.0')\n"
        },
        {
          "name": "gradio_canny.py",
          "type": "blob",
          "size": 5.4677734375,
          "content": "from share import *\nimport config\n\nimport cv2\nimport einops\nimport gradio as gr\nimport numpy as np\nimport torch\nimport random\n\nfrom pytorch_lightning import seed_everything\nfrom annotator.util import resize_image, HWC3\nfrom annotator.canny import CannyDetector\nfrom cldm.model import create_model, load_state_dict\nfrom cldm.ddim_hacked import DDIMSampler\n\n\npreprocessor = None\n\nmodel_name = 'control_v11p_sd15_canny'\nmodel = create_model(f'./models/{model_name}.yaml').cpu()\nmodel.load_state_dict(load_state_dict('./models/v1-5-pruned.ckpt', location='cuda'), strict=False)\nmodel.load_state_dict(load_state_dict(f'./models/{model_name}.pth', location='cuda'), strict=False)\nmodel = model.cuda()\nddim_sampler = DDIMSampler(model)\n\n\ndef process(det, input_image, prompt, a_prompt, n_prompt, num_samples, image_resolution, detect_resolution, ddim_steps, guess_mode, strength, scale, seed, eta, low_threshold, high_threshold):\n    global preprocessor\n\n    if det == 'Canny':\n        if not isinstance(preprocessor, CannyDetector):\n            preprocessor = CannyDetector()\n\n    with torch.no_grad():\n        input_image = HWC3(input_image)\n\n        if det == 'None':\n            detected_map = input_image.copy()\n        else:\n            detected_map = preprocessor(resize_image(input_image, detect_resolution), low_threshold, high_threshold)\n            detected_map = HWC3(detected_map)\n\n        img = resize_image(input_image, image_resolution)\n        H, W, C = img.shape\n\n        detected_map = cv2.resize(detected_map, (W, H), interpolation=cv2.INTER_LINEAR)\n\n        control = torch.from_numpy(detected_map.copy()).float().cuda() / 255.0\n        control = torch.stack([control for _ in range(num_samples)], dim=0)\n        control = einops.rearrange(control, 'b h w c -> b c h w').clone()\n\n        if seed == -1:\n            seed = random.randint(0, 65535)\n        seed_everything(seed)\n\n        if config.save_memory:\n            model.low_vram_shift(is_diffusing=False)\n\n        cond = {\"c_concat\": [control], \"c_crossattn\": [model.get_learned_conditioning([prompt + ', ' + a_prompt] * num_samples)]}\n        un_cond = {\"c_concat\": None if guess_mode else [control], \"c_crossattn\": [model.get_learned_conditioning([n_prompt] * num_samples)]}\n        shape = (4, H // 8, W // 8)\n\n        if config.save_memory:\n            model.low_vram_shift(is_diffusing=True)\n\n        model.control_scales = [strength * (0.825 ** float(12 - i)) for i in range(13)] if guess_mode else ([strength] * 13)\n        # Magic number. IDK why. Perhaps because 0.825**12<0.01 but 0.826**12>0.01\n\n        samples, intermediates = ddim_sampler.sample(ddim_steps, num_samples,\n                                                     shape, cond, verbose=False, eta=eta,\n                                                     unconditional_guidance_scale=scale,\n                                                     unconditional_conditioning=un_cond)\n\n        if config.save_memory:\n            model.low_vram_shift(is_diffusing=False)\n\n        x_samples = model.decode_first_stage(samples)\n        x_samples = (einops.rearrange(x_samples, 'b c h w -> b h w c') * 127.5 + 127.5).cpu().numpy().clip(0, 255).astype(np.uint8)\n\n        results = [x_samples[i] for i in range(num_samples)]\n    return [detected_map] + results\n\n\nblock = gr.Blocks().queue()\nwith block:\n    with gr.Row():\n        gr.Markdown(\"## Control Stable Diffusion with Canny Edges\")\n    with gr.Row():\n        with gr.Column():\n            input_image = gr.Image(source='upload', type=\"numpy\")\n            prompt = gr.Textbox(label=\"Prompt\")\n            run_button = gr.Button(label=\"Run\")\n            num_samples = gr.Slider(label=\"Images\", minimum=1, maximum=12, value=1, step=1)\n            seed = gr.Slider(label=\"Seed\", minimum=-1, maximum=2147483647, step=1, value=12345)\n            det = gr.Radio(choices=[\"Canny\", \"None\"], type=\"value\", value=\"Canny\", label=\"Preprocessor\")\n            with gr.Accordion(\"Advanced options\", open=False):\n                low_threshold = gr.Slider(label=\"Canny low threshold\", minimum=1, maximum=255, value=100, step=1)\n                high_threshold = gr.Slider(label=\"Canny high threshold\", minimum=1, maximum=255, value=200, step=1)\n                image_resolution = gr.Slider(label=\"Image Resolution\", minimum=256, maximum=768, value=512, step=64)\n                strength = gr.Slider(label=\"Control Strength\", minimum=0.0, maximum=2.0, value=1.0, step=0.01)\n                guess_mode = gr.Checkbox(label='Guess Mode', value=False)\n                detect_resolution = gr.Slider(label=\"Preprocessor Resolution\", minimum=128, maximum=1024, value=512, step=1)\n                ddim_steps = gr.Slider(label=\"Steps\", minimum=1, maximum=100, value=20, step=1)\n                scale = gr.Slider(label=\"Guidance Scale\", minimum=0.1, maximum=30.0, value=9.0, step=0.1)\n                eta = gr.Slider(label=\"DDIM ETA\", minimum=0.0, maximum=1.0, value=1.0, step=0.01)\n                a_prompt = gr.Textbox(label=\"Added Prompt\", value='best quality')\n                n_prompt = gr.Textbox(label=\"Negative Prompt\", value='lowres, bad anatomy, bad hands, cropped, worst quality')\n        with gr.Column():\n            result_gallery = gr.Gallery(label='Output', show_label=False, elem_id=\"gallery\").style(grid=2, height='auto')\n    ips = [det, input_image, prompt, a_prompt, n_prompt, num_samples, image_resolution, detect_resolution, ddim_steps, guess_mode, strength, scale, seed, eta, low_threshold, high_threshold]\n    run_button.click(fn=process, inputs=ips, outputs=[result_gallery])\n\n\nblock.launch(server_name='0.0.0.0')\n"
        },
        {
          "name": "gradio_depth.py",
          "type": "blob",
          "size": 5.337890625,
          "content": "from share import *\nimport config\n\nimport cv2\nimport einops\nimport gradio as gr\nimport numpy as np\nimport torch\nimport random\n\nfrom pytorch_lightning import seed_everything\nfrom annotator.util import resize_image, HWC3\nfrom annotator.midas import MidasDetector\nfrom annotator.zoe import ZoeDetector\nfrom cldm.model import create_model, load_state_dict\nfrom cldm.ddim_hacked import DDIMSampler\n\n\npreprocessor = None\n\nmodel_name = 'control_v11f1p_sd15_depth'\nmodel = create_model(f'./models/{model_name}.yaml').cpu()\nmodel.load_state_dict(load_state_dict('./models/v1-5-pruned.ckpt', location='cuda'), strict=False)\nmodel.load_state_dict(load_state_dict(f'./models/{model_name}.pth', location='cuda'), strict=False)\nmodel = model.cuda()\nddim_sampler = DDIMSampler(model)\n\n\ndef process(det, input_image, prompt, a_prompt, n_prompt, num_samples, image_resolution, detect_resolution, ddim_steps, guess_mode, strength, scale, seed, eta):\n    global preprocessor\n\n    if det == 'Depth_Midas':\n        if not isinstance(preprocessor, MidasDetector):\n            preprocessor = MidasDetector()\n    if det == 'Depth_Zoe':\n        if not isinstance(preprocessor, ZoeDetector):\n            preprocessor = ZoeDetector()\n\n    with torch.no_grad():\n        input_image = HWC3(input_image)\n\n        if det == 'None':\n            detected_map = input_image.copy()\n        else:\n            detected_map = preprocessor(resize_image(input_image, detect_resolution))\n            detected_map = HWC3(detected_map)\n\n        img = resize_image(input_image, image_resolution)\n        H, W, C = img.shape\n\n        detected_map = cv2.resize(detected_map, (W, H), interpolation=cv2.INTER_LINEAR)\n\n        control = torch.from_numpy(detected_map.copy()).float().cuda() / 255.0\n        control = torch.stack([control for _ in range(num_samples)], dim=0)\n        control = einops.rearrange(control, 'b h w c -> b c h w').clone()\n\n        if seed == -1:\n            seed = random.randint(0, 65535)\n        seed_everything(seed)\n\n        if config.save_memory:\n            model.low_vram_shift(is_diffusing=False)\n\n        cond = {\"c_concat\": [control], \"c_crossattn\": [model.get_learned_conditioning([prompt + ', ' + a_prompt] * num_samples)]}\n        un_cond = {\"c_concat\": None if guess_mode else [control], \"c_crossattn\": [model.get_learned_conditioning([n_prompt] * num_samples)]}\n        shape = (4, H // 8, W // 8)\n\n        if config.save_memory:\n            model.low_vram_shift(is_diffusing=True)\n\n        model.control_scales = [strength * (0.825 ** float(12 - i)) for i in range(13)] if guess_mode else ([strength] * 13)\n        # Magic number. IDK why. Perhaps because 0.825**12<0.01 but 0.826**12>0.01\n\n        samples, intermediates = ddim_sampler.sample(ddim_steps, num_samples,\n                                                     shape, cond, verbose=False, eta=eta,\n                                                     unconditional_guidance_scale=scale,\n                                                     unconditional_conditioning=un_cond)\n\n        if config.save_memory:\n            model.low_vram_shift(is_diffusing=False)\n\n        x_samples = model.decode_first_stage(samples)\n        x_samples = (einops.rearrange(x_samples, 'b c h w -> b h w c') * 127.5 + 127.5).cpu().numpy().clip(0, 255).astype(np.uint8)\n\n        results = [x_samples[i] for i in range(num_samples)]\n    return [detected_map] + results\n\n\nblock = gr.Blocks().queue()\nwith block:\n    with gr.Row():\n        gr.Markdown(\"## Control Stable Diffusion with Depth Maps\")\n    with gr.Row():\n        with gr.Column():\n            input_image = gr.Image(source='upload', type=\"numpy\")\n            prompt = gr.Textbox(label=\"Prompt\")\n            run_button = gr.Button(label=\"Run\")\n            num_samples = gr.Slider(label=\"Images\", minimum=1, maximum=12, value=1, step=1)\n            seed = gr.Slider(label=\"Seed\", minimum=-1, maximum=2147483647, step=1, value=12345)\n            det = gr.Radio(choices=[\"Depth_Zoe\", \"Depth_Midas\", \"None\"], type=\"value\", value=\"Depth_Zoe\", label=\"Preprocessor\")\n            with gr.Accordion(\"Advanced options\", open=False):\n                image_resolution = gr.Slider(label=\"Image Resolution\", minimum=256, maximum=768, value=512, step=64)\n                strength = gr.Slider(label=\"Control Strength\", minimum=0.0, maximum=2.0, value=1.0, step=0.01)\n                guess_mode = gr.Checkbox(label='Guess Mode', value=False)\n                detect_resolution = gr.Slider(label=\"Preprocessor Resolution\", minimum=128, maximum=1024, value=512, step=1)\n                ddim_steps = gr.Slider(label=\"Steps\", minimum=1, maximum=100, value=20, step=1)\n                scale = gr.Slider(label=\"Guidance Scale\", minimum=0.1, maximum=30.0, value=9.0, step=0.1)\n                eta = gr.Slider(label=\"DDIM ETA\", minimum=0.0, maximum=1.0, value=1.0, step=0.01)\n                a_prompt = gr.Textbox(label=\"Added Prompt\", value='best quality')\n                n_prompt = gr.Textbox(label=\"Negative Prompt\", value='lowres, bad anatomy, bad hands, cropped, worst quality')\n        with gr.Column():\n            result_gallery = gr.Gallery(label='Output', show_label=False, elem_id=\"gallery\").style(grid=2, height='auto')\n    ips = [det, input_image, prompt, a_prompt, n_prompt, num_samples, image_resolution, detect_resolution, ddim_steps, guess_mode, strength, scale, seed, eta]\n    run_button.click(fn=process, inputs=ips, outputs=[result_gallery])\n\n\nblock.launch(server_name='0.0.0.0')\n"
        },
        {
          "name": "gradio_inpaint.py",
          "type": "blob",
          "size": 5.880859375,
          "content": "from share import *\nimport config\n\nimport cv2\nimport einops\nimport gradio as gr\nimport numpy as np\nimport torch\nimport random\n\nfrom pytorch_lightning import seed_everything\nfrom annotator.util import resize_image, HWC3\nfrom cldm.model import create_model, load_state_dict\nfrom cldm.ddim_hacked import DDIMSampler\n\n\nmodel_name = 'control_v11p_sd15_inpaint'\nmodel = create_model(f'./models/{model_name}.yaml').cpu()\nmodel.load_state_dict(load_state_dict('./models/v1-5-pruned.ckpt', location='cuda'), strict=False)\nmodel.load_state_dict(load_state_dict(f'./models/{model_name}.pth', location='cuda'), strict=False)\nmodel = model.cuda()\nddim_sampler = DDIMSampler(model)\n\n\ndef process(input_image_and_mask, prompt, a_prompt, n_prompt, num_samples, image_resolution, ddim_steps, guess_mode, strength, scale, seed, eta, mask_blur):\n    with torch.no_grad():\n        input_image = HWC3(input_image_and_mask['image'])\n        input_mask = input_image_and_mask['mask']\n\n        img_raw = resize_image(input_image, image_resolution).astype(np.float32)\n        H, W, C = img_raw.shape\n\n        mask_pixel = cv2.resize(input_mask[:, :, 0], (W, H), interpolation=cv2.INTER_LINEAR).astype(np.float32) / 255.0\n        mask_pixel = cv2.GaussianBlur(mask_pixel, (0, 0), mask_blur)\n\n        mask_latent = cv2.resize(mask_pixel, (W // 8, H // 8), interpolation=cv2.INTER_AREA)\n\n        detected_map = img_raw.copy()\n        detected_map[mask_pixel > 0.5] = - 255.0\n\n        control = torch.from_numpy(detected_map.copy()).float().cuda() / 255.0\n        control = torch.stack([control for _ in range(num_samples)], dim=0)\n        control = einops.rearrange(control, 'b h w c -> b c h w').clone()\n\n        mask = 1.0 - torch.from_numpy(mask_latent.copy()).float().cuda()\n        mask = torch.stack([mask for _ in range(num_samples)], dim=0)\n        mask = einops.rearrange(mask, 'b h w -> b 1 h w').clone()\n\n        x0 = torch.from_numpy(img_raw.copy()).float().cuda() / 127.0 - 1.0\n        x0 = torch.stack([x0 for _ in range(num_samples)], dim=0)\n        x0 = einops.rearrange(x0, 'b h w c -> b c h w').clone()\n\n        mask_pixel_batched = mask_pixel[None, :, :, None]\n        img_pixel_batched = img_raw.copy()[None]\n\n        if seed == -1:\n            seed = random.randint(0, 65535)\n        seed_everything(seed)\n\n        if config.save_memory:\n            model.low_vram_shift(is_diffusing=False)\n\n        cond = {\"c_concat\": [control], \"c_crossattn\": [model.get_learned_conditioning([prompt + ', ' + a_prompt] * num_samples)]}\n        un_cond = {\"c_concat\": None if guess_mode else [control], \"c_crossattn\": [model.get_learned_conditioning([n_prompt] * num_samples)]}\n        shape = (4, H // 8, W // 8)\n\n        if config.save_memory:\n            model.low_vram_shift(is_diffusing=False)\n\n        ddim_sampler.make_schedule(ddim_steps, ddim_eta=eta, verbose=True)\n        x0 = model.get_first_stage_encoding(model.encode_first_stage(x0))\n\n        if config.save_memory:\n            model.low_vram_shift(is_diffusing=True)\n\n        model.control_scales = [strength * (0.825 ** float(12 - i)) for i in range(13)] if guess_mode else ([strength] * 13)\n        # Magic number. IDK why. Perhaps because 0.825**12<0.01 but 0.826**12>0.01\n\n        samples, intermediates = ddim_sampler.sample(ddim_steps, num_samples,\n                                                     shape, cond, verbose=False, eta=eta,\n                                                     unconditional_guidance_scale=scale,\n                                                     unconditional_conditioning=un_cond, x0=x0, mask=mask)\n\n        if config.save_memory:\n            model.low_vram_shift(is_diffusing=False)\n\n        x_samples = model.decode_first_stage(samples)\n        x_samples = (einops.rearrange(x_samples, 'b c h w -> b h w c') * 127.5 + 127.5).cpu().numpy().astype(np.float32)\n        x_samples = x_samples * mask_pixel_batched + img_pixel_batched * (1.0 - mask_pixel_batched)\n\n        results = [x_samples[i].clip(0, 255).astype(np.uint8) for i in range(num_samples)]\n    return [detected_map.clip(0, 255).astype(np.uint8)] + results\n\n\nblock = gr.Blocks().queue()\nwith block:\n    with gr.Row():\n        gr.Markdown(\"## Control Stable Diffusion with Inpaint Mask\")\n    with gr.Row():\n        with gr.Column():\n            input_image = gr.Image(source='upload', type=\"numpy\", tool=\"sketch\")\n            prompt = gr.Textbox(label=\"Prompt\")\n            run_button = gr.Button(label=\"Run\")\n            num_samples = gr.Slider(label=\"Images\", minimum=1, maximum=12, value=1, step=1)\n            seed = gr.Slider(label=\"Seed\", minimum=-1, maximum=2147483647, step=1, value=12345)\n            mask_blur = gr.Slider(label=\"Mask Blur\", minimum=0.1, maximum=7.0, value=5.0, step=0.01)\n            with gr.Accordion(\"Advanced options\", open=False):\n                image_resolution = gr.Slider(label=\"Image Resolution\", minimum=256, maximum=768, value=512, step=64)\n                strength = gr.Slider(label=\"Control Strength\", minimum=0.0, maximum=2.0, value=1.0, step=0.01)\n                guess_mode = gr.Checkbox(label='Guess Mode', value=False)\n                ddim_steps = gr.Slider(label=\"Steps\", minimum=1, maximum=100, value=20, step=1)\n                scale = gr.Slider(label=\"Guidance Scale\", minimum=0.1, maximum=30.0, value=9.0, step=0.1)\n                eta = gr.Slider(label=\"DDIM ETA\", minimum=0.0, maximum=1.0, value=1.0, step=0.01)\n                a_prompt = gr.Textbox(label=\"Added Prompt\", value='best quality')\n                n_prompt = gr.Textbox(label=\"Negative Prompt\", value='lowres, bad anatomy, bad hands, cropped, worst quality')\n        with gr.Column():\n            result_gallery = gr.Gallery(label='Output', show_label=False, elem_id=\"gallery\").style(grid=2, height='auto')\n    ips = [input_image, prompt, a_prompt, n_prompt, num_samples, image_resolution, ddim_steps, guess_mode, strength, scale, seed, eta, mask_blur]\n    run_button.click(fn=process, inputs=ips, outputs=[result_gallery])\n\n\nblock.launch(server_name='0.0.0.0')\n"
        },
        {
          "name": "gradio_ip2p.py",
          "type": "blob",
          "size": 4.599609375,
          "content": "from share import *\nimport config\n\nimport cv2\nimport einops\nimport gradio as gr\nimport numpy as np\nimport torch\nimport random\n\nfrom pytorch_lightning import seed_everything\nfrom annotator.util import resize_image, HWC3\nfrom cldm.model import create_model, load_state_dict\nfrom cldm.ddim_hacked import DDIMSampler\n\n\nmodel_name = 'control_v11e_sd15_ip2p'\nmodel = create_model(f'./models/{model_name}.yaml').cpu()\nmodel.load_state_dict(load_state_dict('./models/v1-5-pruned.ckpt', location='cuda'), strict=False)\nmodel.load_state_dict(load_state_dict(f'./models/{model_name}.pth', location='cuda'), strict=False)\nmodel = model.cuda()\nddim_sampler = DDIMSampler(model)\n\n\ndef process(input_image, prompt, a_prompt, n_prompt, num_samples, image_resolution, ddim_steps, guess_mode, strength, scale, seed, eta):\n    with torch.no_grad():\n        input_image = HWC3(input_image)\n\n        detected_map = input_image.copy()\n\n        img = resize_image(input_image, image_resolution)\n        H, W, C = img.shape\n\n        detected_map = cv2.resize(detected_map, (W, H), interpolation=cv2.INTER_LINEAR)\n\n        control = torch.from_numpy(detected_map.copy()).float().cuda() / 255.0\n        control = torch.stack([control for _ in range(num_samples)], dim=0)\n        control = einops.rearrange(control, 'b h w c -> b c h w').clone()\n\n        if seed == -1:\n            seed = random.randint(0, 65535)\n        seed_everything(seed)\n\n        if config.save_memory:\n            model.low_vram_shift(is_diffusing=False)\n\n        cond = {\"c_concat\": [control], \"c_crossattn\": [model.get_learned_conditioning([prompt + ', ' + a_prompt] * num_samples)]}\n        un_cond = {\"c_concat\": None if guess_mode else [control], \"c_crossattn\": [model.get_learned_conditioning([n_prompt] * num_samples)]}\n        shape = (4, H // 8, W // 8)\n\n        if config.save_memory:\n            model.low_vram_shift(is_diffusing=True)\n\n        model.control_scales = [strength * (0.825 ** float(12 - i)) for i in range(13)] if guess_mode else ([strength] * 13)\n        # Magic number. IDK why. Perhaps because 0.825**12<0.01 but 0.826**12>0.01\n\n        samples, intermediates = ddim_sampler.sample(ddim_steps, num_samples,\n                                                     shape, cond, verbose=False, eta=eta,\n                                                     unconditional_guidance_scale=scale,\n                                                     unconditional_conditioning=un_cond)\n\n        if config.save_memory:\n            model.low_vram_shift(is_diffusing=False)\n\n        x_samples = model.decode_first_stage(samples)\n        x_samples = (einops.rearrange(x_samples, 'b c h w -> b h w c') * 127.5 + 127.5).cpu().numpy().clip(0, 255).astype(np.uint8)\n\n        results = [x_samples[i] for i in range(num_samples)]\n    return [detected_map] + results\n\n\nblock = gr.Blocks().queue()\nwith block:\n    with gr.Row():\n        gr.Markdown(\"## Control Stable Diffusion with Instruct Pix2Pix\")\n    with gr.Row():\n        with gr.Column():\n            input_image = gr.Image(source='upload', type=\"numpy\")\n            prompt = gr.Textbox(label=\"Prompt\")\n            run_button = gr.Button(label=\"Run\")\n            num_samples = gr.Slider(label=\"Images\", minimum=1, maximum=12, value=1, step=1)\n            seed = gr.Slider(label=\"Seed\", minimum=-1, maximum=2147483647, step=1, value=12345)\n            det = gr.Radio(choices=[\"None\"], type=\"value\", value=\"None\", label=\"Preprocessor\")\n            with gr.Accordion(\"Advanced options\", open=False):\n                image_resolution = gr.Slider(label=\"Image Resolution\", minimum=256, maximum=768, value=512, step=64)\n                strength = gr.Slider(label=\"Control Strength\", minimum=0.0, maximum=2.0, value=1.0, step=0.01)\n                guess_mode = gr.Checkbox(label='Guess Mode', value=False)\n                ddim_steps = gr.Slider(label=\"Steps\", minimum=1, maximum=100, value=20, step=1)\n                scale = gr.Slider(label=\"Guidance Scale\", minimum=0.1, maximum=30.0, value=9.0, step=0.1)\n                eta = gr.Slider(label=\"DDIM ETA\", minimum=0.0, maximum=1.0, value=1.0, step=0.01)\n                a_prompt = gr.Textbox(label=\"Added Prompt\", value='best quality')\n                n_prompt = gr.Textbox(label=\"Negative Prompt\", value='lowres, bad anatomy, bad hands, cropped, worst quality')\n        with gr.Column():\n            result_gallery = gr.Gallery(label='Output', show_label=False, elem_id=\"gallery\").style(grid=2, height='auto')\n    ips = [input_image, prompt, a_prompt, n_prompt, num_samples, image_resolution, ddim_steps, guess_mode, strength, scale, seed, eta]\n    run_button.click(fn=process, inputs=ips, outputs=[result_gallery])\n\n\nblock.launch(server_name='0.0.0.0')\n"
        },
        {
          "name": "gradio_lineart.py",
          "type": "blob",
          "size": 5.2099609375,
          "content": "from share import *\nimport config\n\nimport cv2\nimport einops\nimport gradio as gr\nimport numpy as np\nimport torch\nimport random\n\nfrom pytorch_lightning import seed_everything\nfrom annotator.util import resize_image, HWC3\nfrom annotator.lineart import LineartDetector\nfrom cldm.model import create_model, load_state_dict\nfrom cldm.ddim_hacked import DDIMSampler\n\n\npreprocessor = None\n\nmodel_name = 'control_v11p_sd15_lineart'\nmodel = create_model(f'./models/{model_name}.yaml').cpu()\nmodel.load_state_dict(load_state_dict('./models/v1-5-pruned.ckpt', location='cuda'), strict=False)\nmodel.load_state_dict(load_state_dict(f'./models/{model_name}.pth', location='cuda'), strict=False)\nmodel = model.cuda()\nddim_sampler = DDIMSampler(model)\n\n\ndef process(det, input_image, prompt, a_prompt, n_prompt, num_samples, image_resolution, detect_resolution, ddim_steps, guess_mode, strength, scale, seed, eta):\n    global preprocessor\n    if 'Lineart' in det:\n        if not isinstance(preprocessor, LineartDetector):\n            preprocessor = LineartDetector()\n\n    with torch.no_grad():\n        input_image = HWC3(input_image)\n\n        if det == 'None':\n            detected_map = input_image.copy()\n        else:\n            detected_map = preprocessor(resize_image(input_image, detect_resolution), coarse='Coarse' in det)\n            detected_map = HWC3(detected_map)\n\n        img = resize_image(input_image, image_resolution)\n        H, W, C = img.shape\n\n        detected_map = cv2.resize(detected_map, (W, H), interpolation=cv2.INTER_LINEAR)\n\n        control = 1.0 - torch.from_numpy(detected_map.copy()).float().cuda() / 255.0\n        control = torch.stack([control for _ in range(num_samples)], dim=0)\n        control = einops.rearrange(control, 'b h w c -> b c h w').clone()\n\n        if seed == -1:\n            seed = random.randint(0, 65535)\n        seed_everything(seed)\n\n        if config.save_memory:\n            model.low_vram_shift(is_diffusing=False)\n\n        cond = {\"c_concat\": [control], \"c_crossattn\": [model.get_learned_conditioning([prompt + ', ' + a_prompt] * num_samples)]}\n        un_cond = {\"c_concat\": None if guess_mode else [control], \"c_crossattn\": [model.get_learned_conditioning([n_prompt] * num_samples)]}\n        shape = (4, H // 8, W // 8)\n\n        if config.save_memory:\n            model.low_vram_shift(is_diffusing=True)\n\n        model.control_scales = [strength * (0.825 ** float(12 - i)) for i in range(13)] if guess_mode else ([strength] * 13)\n        # Magic number. IDK why. Perhaps because 0.825**12<0.01 but 0.826**12>0.01\n\n        samples, intermediates = ddim_sampler.sample(ddim_steps, num_samples,\n                                                     shape, cond, verbose=False, eta=eta,\n                                                     unconditional_guidance_scale=scale,\n                                                     unconditional_conditioning=un_cond)\n\n        if config.save_memory:\n            model.low_vram_shift(is_diffusing=False)\n\n        x_samples = model.decode_first_stage(samples)\n        x_samples = (einops.rearrange(x_samples, 'b c h w -> b h w c') * 127.5 + 127.5).cpu().numpy().clip(0, 255).astype(np.uint8)\n\n        results = [x_samples[i] for i in range(num_samples)]\n    return [detected_map] + results\n\n\nblock = gr.Blocks().queue()\nwith block:\n    with gr.Row():\n        gr.Markdown(\"## Control Stable Diffusion with Lineart\")\n    with gr.Row():\n        with gr.Column():\n            input_image = gr.Image(source='upload', type=\"numpy\")\n            prompt = gr.Textbox(label=\"Prompt\")\n            run_button = gr.Button(label=\"Run\")\n            num_samples = gr.Slider(label=\"Images\", minimum=1, maximum=12, value=1, step=1)\n            seed = gr.Slider(label=\"Seed\", minimum=-1, maximum=2147483647, step=1, value=12345)\n            det = gr.Radio(choices=[\"Lineart\", \"Lineart_Coarse\", \"None\"], type=\"value\", value=\"Lineart\", label=\"Preprocessor\")\n            with gr.Accordion(\"Advanced options\", open=False):\n                image_resolution = gr.Slider(label=\"Image Resolution\", minimum=256, maximum=768, value=512, step=64)\n                strength = gr.Slider(label=\"Control Strength\", minimum=0.0, maximum=2.0, value=1.0, step=0.01)\n                guess_mode = gr.Checkbox(label='Guess Mode', value=False)\n                detect_resolution = gr.Slider(label=\"Preprocessor Resolution\", minimum=128, maximum=1024, value=512, step=1)\n                ddim_steps = gr.Slider(label=\"Steps\", minimum=1, maximum=100, value=20, step=1)\n                scale = gr.Slider(label=\"Guidance Scale\", minimum=0.1, maximum=30.0, value=9.0, step=0.1)\n                eta = gr.Slider(label=\"DDIM ETA\", minimum=0.0, maximum=1.0, value=1.0, step=0.01)\n                a_prompt = gr.Textbox(label=\"Added Prompt\", value='best quality')\n                n_prompt = gr.Textbox(label=\"Negative Prompt\", value='lowres, bad anatomy, bad hands, cropped, worst quality')\n        with gr.Column():\n            result_gallery = gr.Gallery(label='Output', show_label=False, elem_id=\"gallery\").style(grid=2, height='auto')\n    ips = [det, input_image, prompt, a_prompt, n_prompt, num_samples, image_resolution, detect_resolution, ddim_steps, guess_mode, strength, scale, seed, eta]\n    run_button.click(fn=process, inputs=ips, outputs=[result_gallery])\n\n\nblock.launch(server_name='0.0.0.0')\n"
        },
        {
          "name": "gradio_lineart_anime.py",
          "type": "blob",
          "size": 5.23828125,
          "content": "from share import *\nimport config\nfrom cldm.hack import hack_everything\n\n\nhack_everything(clip_skip=2)\n\n\nimport cv2\nimport einops\nimport gradio as gr\nimport numpy as np\nimport torch\nimport random\n\nfrom pytorch_lightning import seed_everything\nfrom annotator.util import resize_image, HWC3\nfrom annotator.lineart_anime import LineartAnimeDetector\nfrom cldm.model import create_model, load_state_dict\nfrom cldm.ddim_hacked import DDIMSampler\n\n\npreprocessor = None\n\nmodel_name = 'control_v11p_sd15s2_lineart_anime'\nmodel = create_model(f'./models/{model_name}.yaml').cpu()\nmodel.load_state_dict(load_state_dict('./models/anything-v3-full.safetensors', location='cuda'), strict=False)\nmodel.load_state_dict(load_state_dict(f'./models/{model_name}.pth', location='cuda'), strict=False)\nmodel = model.cuda()\nddim_sampler = DDIMSampler(model)\n\n\ndef process(det, input_image, prompt, a_prompt, n_prompt, num_samples, image_resolution, detect_resolution, ddim_steps, strength, scale, seed, eta):\n    global preprocessor\n\n    if det == 'Lineart_Anime':\n        if not isinstance(preprocessor, LineartAnimeDetector):\n            preprocessor = LineartAnimeDetector()\n\n    with torch.no_grad():\n        input_image = HWC3(input_image)\n\n        if det == 'None':\n            detected_map = input_image.copy()\n        else:\n            detected_map = preprocessor(resize_image(input_image, detect_resolution))\n            detected_map = HWC3(detected_map)\n\n        img = resize_image(input_image, image_resolution)\n        H, W, C = img.shape\n\n        detected_map = cv2.resize(detected_map, (W, H), interpolation=cv2.INTER_LINEAR)\n\n        control = 1.0 - torch.from_numpy(detected_map.copy()).float().cuda() / 255.0\n        control = torch.stack([control for _ in range(num_samples)], dim=0)\n        control = einops.rearrange(control, 'b h w c -> b c h w').clone()\n\n        if seed == -1:\n            seed = random.randint(0, 65535)\n        seed_everything(seed)\n\n        if config.save_memory:\n            model.low_vram_shift(is_diffusing=False)\n\n        cond = {\"c_concat\": [control], \"c_crossattn\": [model.get_learned_conditioning([prompt + ', ' + a_prompt] * num_samples)]}\n        un_cond = {\"c_concat\": [control], \"c_crossattn\": [model.get_learned_conditioning([n_prompt] * num_samples)]}\n        shape = (4, H // 8, W // 8)\n\n        if config.save_memory:\n            model.low_vram_shift(is_diffusing=True)\n\n        model.control_scales = [strength] * 13\n        samples, intermediates = ddim_sampler.sample(ddim_steps, num_samples,\n                                                     shape, cond, verbose=False, eta=eta,\n                                                     unconditional_guidance_scale=scale,\n                                                     unconditional_conditioning=un_cond)\n\n        if config.save_memory:\n            model.low_vram_shift(is_diffusing=False)\n\n        x_samples = model.decode_first_stage(samples)\n        x_samples = (einops.rearrange(x_samples, 'b c h w -> b h w c') * 127.5 + 127.5).cpu().numpy().clip(0, 255).astype(np.uint8)\n\n        results = [x_samples[i] for i in range(num_samples)]\n    return [detected_map] + results\n\n\nblock = gr.Blocks().queue()\nwith block:\n    with gr.Row():\n        gr.Markdown(\"## Control Anything V3 with Anime Lineart\")\n    with gr.Row():\n        with gr.Column():\n            input_image = gr.Image(source='upload', type=\"numpy\")\n            prompt = gr.Textbox(label=\"Prompt\")\n            a_prompt = gr.Textbox(label=\"Added Prompt (Beginners do not need to change)\", value='masterpiece, best quality, ultra-detailed, illustration, disheveled hair')\n            n_prompt = gr.Textbox(label=\"Negative Prompt (Beginners do not need to change)\",\n                                  value='longbody, lowres, bad anatomy, bad hands, missing fingers, pubic hair,extra digit, fewer digits, cropped, worst quality, low quality')\n            run_button = gr.Button(label=\"Run\")\n            num_samples = gr.Slider(label=\"Images\", minimum=1, maximum=12, value=1, step=1)\n            seed = gr.Slider(label=\"Seed\", minimum=-1, maximum=2147483647, step=1, value=12345)\n            det = gr.Radio(choices=[\"None\", \"Lineart_Anime\"], type=\"value\", value=\"None\", label=\"Preprocessor\")\n            with gr.Accordion(\"Advanced options\", open=False):\n                image_resolution = gr.Slider(label=\"Image Resolution\", minimum=256, maximum=2048, value=512, step=64)\n                strength = gr.Slider(label=\"Control Strength\", minimum=0.0, maximum=2.0, value=1.0, step=0.01)\n                detect_resolution = gr.Slider(label=\"Preprocessor Resolution\", minimum=128, maximum=1024, value=512, step=1)\n                ddim_steps = gr.Slider(label=\"Steps\", minimum=1, maximum=100, value=20, step=1)\n                scale = gr.Slider(label=\"Guidance Scale\", minimum=0.1, maximum=30.0, value=9.0, step=0.1)\n                eta = gr.Slider(label=\"DDIM ETA\", minimum=0.0, maximum=1.0, value=1.0, step=0.01)\n        with gr.Column():\n            result_gallery = gr.Gallery(label='Output', show_label=False, elem_id=\"gallery\").style(grid=2, height='auto')\n    ips = [det, input_image, prompt, a_prompt, n_prompt, num_samples, image_resolution, detect_resolution, ddim_steps, strength, scale, seed, eta]\n    run_button.click(fn=process, inputs=ips, outputs=[result_gallery])\n\n\nblock.launch(server_name='0.0.0.0')\n"
        },
        {
          "name": "gradio_mlsd.py",
          "type": "blob",
          "size": 5.5146484375,
          "content": "from share import *\nimport config\n\nimport cv2\nimport einops\nimport gradio as gr\nimport numpy as np\nimport torch\nimport random\n\nfrom pytorch_lightning import seed_everything\nfrom annotator.util import resize_image, HWC3\nfrom annotator.mlsd import MLSDdetector\nfrom cldm.model import create_model, load_state_dict\nfrom cldm.ddim_hacked import DDIMSampler\n\n\npreprocessor = None\n\nmodel_name = 'control_v11p_sd15_mlsd'\nmodel = create_model(f'./models/{model_name}.yaml').cpu()\nmodel.load_state_dict(load_state_dict('./models/v1-5-pruned.ckpt', location='cuda'), strict=False)\nmodel.load_state_dict(load_state_dict(f'./models/{model_name}.pth', location='cuda'), strict=False)\nmodel = model.cuda()\nddim_sampler = DDIMSampler(model)\n\n\ndef process(det, input_image, prompt, a_prompt, n_prompt, num_samples, image_resolution, detect_resolution, ddim_steps, guess_mode, strength, scale, seed, eta, value_threshold, distance_threshold):\n    global preprocessor\n\n    if det == 'MLSD':\n        if not isinstance(preprocessor, MLSDdetector):\n            preprocessor = MLSDdetector()\n\n    with torch.no_grad():\n        input_image = HWC3(input_image)\n\n        if det == 'None':\n            detected_map = input_image.copy()\n        else:\n            detected_map = preprocessor(resize_image(input_image, detect_resolution), value_threshold, distance_threshold)\n            detected_map = HWC3(detected_map)\n\n        img = resize_image(input_image, image_resolution)\n        H, W, C = img.shape\n\n        detected_map = cv2.resize(detected_map, (W, H), interpolation=cv2.INTER_LINEAR)\n\n        control = torch.from_numpy(detected_map.copy()).float().cuda() / 255.0\n        control = torch.stack([control for _ in range(num_samples)], dim=0)\n        control = einops.rearrange(control, 'b h w c -> b c h w').clone()\n\n        if seed == -1:\n            seed = random.randint(0, 65535)\n        seed_everything(seed)\n\n        if config.save_memory:\n            model.low_vram_shift(is_diffusing=False)\n\n        cond = {\"c_concat\": [control], \"c_crossattn\": [model.get_learned_conditioning([prompt + ', ' + a_prompt] * num_samples)]}\n        un_cond = {\"c_concat\": None if guess_mode else [control], \"c_crossattn\": [model.get_learned_conditioning([n_prompt] * num_samples)]}\n        shape = (4, H // 8, W // 8)\n\n        if config.save_memory:\n            model.low_vram_shift(is_diffusing=True)\n\n        model.control_scales = [strength * (0.825 ** float(12 - i)) for i in range(13)] if guess_mode else ([strength] * 13)\n        # Magic number. IDK why. Perhaps because 0.825**12<0.01 but 0.826**12>0.01\n\n        samples, intermediates = ddim_sampler.sample(ddim_steps, num_samples,\n                                                     shape, cond, verbose=False, eta=eta,\n                                                     unconditional_guidance_scale=scale,\n                                                     unconditional_conditioning=un_cond)\n\n        if config.save_memory:\n            model.low_vram_shift(is_diffusing=False)\n\n        x_samples = model.decode_first_stage(samples)\n        x_samples = (einops.rearrange(x_samples, 'b c h w -> b h w c') * 127.5 + 127.5).cpu().numpy().clip(0, 255).astype(np.uint8)\n\n        results = [x_samples[i] for i in range(num_samples)]\n    return [detected_map] + results\n\n\nblock = gr.Blocks().queue()\nwith block:\n    with gr.Row():\n        gr.Markdown(\"## Control Stable Diffusion with MLSD Lines\")\n    with gr.Row():\n        with gr.Column():\n            input_image = gr.Image(source='upload', type=\"numpy\")\n            prompt = gr.Textbox(label=\"Prompt\")\n            run_button = gr.Button(label=\"Run\")\n            num_samples = gr.Slider(label=\"Images\", minimum=1, maximum=12, value=1, step=1)\n            seed = gr.Slider(label=\"Seed\", minimum=-1, maximum=2147483647, step=1, value=12345)\n            det = gr.Radio(choices=[\"MLSD\", \"None\"], type=\"value\", value=\"MLSD\", label=\"Preprocessor\")\n            with gr.Accordion(\"Advanced options\", open=False):\n                value_threshold = gr.Slider(label=\"Hough value threshold (MLSD)\", minimum=0.01, maximum=2.0, value=0.1, step=0.01)\n                distance_threshold = gr.Slider(label=\"Hough distance threshold (MLSD)\", minimum=0.01, maximum=20.0, value=0.1, step=0.01)\n                image_resolution = gr.Slider(label=\"Image Resolution\", minimum=256, maximum=768, value=512, step=64)\n                strength = gr.Slider(label=\"Control Strength\", minimum=0.0, maximum=2.0, value=1.0, step=0.01)\n                guess_mode = gr.Checkbox(label='Guess Mode', value=False)\n                detect_resolution = gr.Slider(label=\"Preprocessor Resolution\", minimum=128, maximum=1024, value=512, step=1)\n                ddim_steps = gr.Slider(label=\"Steps\", minimum=1, maximum=100, value=20, step=1)\n                scale = gr.Slider(label=\"Guidance Scale\", minimum=0.1, maximum=30.0, value=9.0, step=0.1)\n                eta = gr.Slider(label=\"DDIM ETA\", minimum=0.0, maximum=1.0, value=1.0, step=0.01)\n                a_prompt = gr.Textbox(label=\"Added Prompt\", value='best quality')\n                n_prompt = gr.Textbox(label=\"Negative Prompt\", value='lowres, bad anatomy, bad hands, cropped, worst quality')\n        with gr.Column():\n            result_gallery = gr.Gallery(label='Output', show_label=False, elem_id=\"gallery\").style(grid=2, height='auto')\n    ips = [det, input_image, prompt, a_prompt, n_prompt, num_samples, image_resolution, detect_resolution, ddim_steps, guess_mode, strength, scale, seed, eta, value_threshold, distance_threshold]\n    run_button.click(fn=process, inputs=ips, outputs=[result_gallery])\n\n\nblock.launch(server_name='0.0.0.0')\n"
        },
        {
          "name": "gradio_normalbae.py",
          "type": "blob",
          "size": 5.1865234375,
          "content": "from share import *\nimport config\n\nimport cv2\nimport einops\nimport gradio as gr\nimport numpy as np\nimport torch\nimport random\n\nfrom pytorch_lightning import seed_everything\nfrom annotator.util import resize_image, HWC3\nfrom annotator.normalbae import NormalBaeDetector\nfrom cldm.model import create_model, load_state_dict\nfrom cldm.ddim_hacked import DDIMSampler\n\n\npreprocessor = None\n\nmodel_name = 'control_v11p_sd15_normalbae'\nmodel = create_model(f'./models/{model_name}.yaml').cpu()\nmodel.load_state_dict(load_state_dict('./models/v1-5-pruned.ckpt', location='cuda'), strict=False)\nmodel.load_state_dict(load_state_dict(f'./models/{model_name}.pth', location='cuda'), strict=False)\nmodel = model.cuda()\nddim_sampler = DDIMSampler(model)\n\n\ndef process(det, input_image, prompt, a_prompt, n_prompt, num_samples, image_resolution, detect_resolution, ddim_steps, guess_mode, strength, scale, seed, eta):\n    global preprocessor\n\n    if det == 'Normal_BAE':\n        if not isinstance(preprocessor, NormalBaeDetector):\n            preprocessor = NormalBaeDetector()\n\n    with torch.no_grad():\n        input_image = HWC3(input_image)\n\n        if det == 'None':\n            detected_map = input_image.copy()\n        else:\n            detected_map = preprocessor(resize_image(input_image, detect_resolution))\n            detected_map = HWC3(detected_map)\n\n        img = resize_image(input_image, image_resolution)\n        H, W, C = img.shape\n\n        detected_map = cv2.resize(detected_map, (W, H), interpolation=cv2.INTER_LINEAR)\n\n        control = torch.from_numpy(detected_map.copy()).float().cuda() / 255.0\n        control = torch.stack([control for _ in range(num_samples)], dim=0)\n        control = einops.rearrange(control, 'b h w c -> b c h w').clone()\n\n        if seed == -1:\n            seed = random.randint(0, 65535)\n        seed_everything(seed)\n\n        if config.save_memory:\n            model.low_vram_shift(is_diffusing=False)\n\n        cond = {\"c_concat\": [control], \"c_crossattn\": [model.get_learned_conditioning([prompt + ', ' + a_prompt] * num_samples)]}\n        un_cond = {\"c_concat\": None if guess_mode else [control], \"c_crossattn\": [model.get_learned_conditioning([n_prompt] * num_samples)]}\n        shape = (4, H // 8, W // 8)\n\n        if config.save_memory:\n            model.low_vram_shift(is_diffusing=True)\n\n        model.control_scales = [strength * (0.825 ** float(12 - i)) for i in range(13)] if guess_mode else ([strength] * 13)\n        # Magic number. IDK why. Perhaps because 0.825**12<0.01 but 0.826**12>0.01\n\n        samples, intermediates = ddim_sampler.sample(ddim_steps, num_samples,\n                                                     shape, cond, verbose=False, eta=eta,\n                                                     unconditional_guidance_scale=scale,\n                                                     unconditional_conditioning=un_cond)\n\n        if config.save_memory:\n            model.low_vram_shift(is_diffusing=False)\n\n        x_samples = model.decode_first_stage(samples)\n        x_samples = (einops.rearrange(x_samples, 'b c h w -> b h w c') * 127.5 + 127.5).cpu().numpy().clip(0, 255).astype(np.uint8)\n\n        results = [x_samples[i] for i in range(num_samples)]\n    return [detected_map] + results\n\n\nblock = gr.Blocks().queue()\nwith block:\n    with gr.Row():\n        gr.Markdown(\"## Control Stable Diffusion with Normal Maps\")\n    with gr.Row():\n        with gr.Column():\n            input_image = gr.Image(source='upload', type=\"numpy\")\n            prompt = gr.Textbox(label=\"Prompt\")\n            run_button = gr.Button(label=\"Run\")\n            num_samples = gr.Slider(label=\"Images\", minimum=1, maximum=12, value=1, step=1)\n            seed = gr.Slider(label=\"Seed\", minimum=-1, maximum=2147483647, step=1, value=12345)\n            det = gr.Radio(choices=[\"Normal_BAE\", \"None\"], type=\"value\", value=\"Normal_BAE\", label=\"Preprocessor\")\n            with gr.Accordion(\"Advanced options\", open=False):\n                image_resolution = gr.Slider(label=\"Image Resolution\", minimum=256, maximum=768, value=512, step=64)\n                strength = gr.Slider(label=\"Control Strength\", minimum=0.0, maximum=2.0, value=1.0, step=0.01)\n                guess_mode = gr.Checkbox(label='Guess Mode', value=False)\n                detect_resolution = gr.Slider(label=\"Preprocessor Resolution\", minimum=128, maximum=1024, value=512, step=1)\n                ddim_steps = gr.Slider(label=\"Steps\", minimum=1, maximum=100, value=20, step=1)\n                scale = gr.Slider(label=\"Guidance Scale\", minimum=0.1, maximum=30.0, value=9.0, step=0.1)\n                eta = gr.Slider(label=\"DDIM ETA\", minimum=0.0, maximum=1.0, value=1.0, step=0.01)\n                a_prompt = gr.Textbox(label=\"Added Prompt\", value='best quality')\n                n_prompt = gr.Textbox(label=\"Negative Prompt\", value='lowres, bad anatomy, bad hands, cropped, worst quality')\n        with gr.Column():\n            result_gallery = gr.Gallery(label='Output', show_label=False, elem_id=\"gallery\").style(grid=2, height='auto')\n    ips = [det, input_image, prompt, a_prompt, n_prompt, num_samples, image_resolution, detect_resolution, ddim_steps, guess_mode, strength, scale, seed, eta]\n    run_button.click(fn=process, inputs=ips, outputs=[result_gallery])\n\n\nblock.launch(server_name='0.0.0.0')\n"
        },
        {
          "name": "gradio_openpose.py",
          "type": "blob",
          "size": 5.22265625,
          "content": "from share import *\nimport config\n\nimport cv2\nimport einops\nimport gradio as gr\nimport numpy as np\nimport torch\nimport random\n\nfrom pytorch_lightning import seed_everything\nfrom annotator.util import resize_image, HWC3\nfrom annotator.openpose import OpenposeDetector\nfrom cldm.model import create_model, load_state_dict\nfrom cldm.ddim_hacked import DDIMSampler\n\n\npreprocessor = None\n\nmodel_name = 'control_v11p_sd15_openpose'\nmodel = create_model(f'./models/{model_name}.yaml').cpu()\nmodel.load_state_dict(load_state_dict('./models/v1-5-pruned.ckpt', location='cuda'), strict=False)\nmodel.load_state_dict(load_state_dict(f'./models/{model_name}.pth', location='cuda'), strict=False)\nmodel = model.cuda()\nddim_sampler = DDIMSampler(model)\n\n\ndef process(det, input_image, prompt, a_prompt, n_prompt, num_samples, image_resolution, detect_resolution, ddim_steps, guess_mode, strength, scale, seed, eta):\n    global preprocessor\n\n    if 'Openpose' in det:\n        if not isinstance(preprocessor, OpenposeDetector):\n            preprocessor = OpenposeDetector()\n\n    with torch.no_grad():\n        input_image = HWC3(input_image)\n\n        if det == 'None':\n            detected_map = input_image.copy()\n        else:\n            detected_map = preprocessor(resize_image(input_image, detect_resolution), hand_and_face='Full' in det)\n            detected_map = HWC3(detected_map)\n\n        img = resize_image(input_image, image_resolution)\n        H, W, C = img.shape\n\n        detected_map = cv2.resize(detected_map, (W, H), interpolation=cv2.INTER_LINEAR)\n\n        control = torch.from_numpy(detected_map.copy()).float().cuda() / 255.0\n        control = torch.stack([control for _ in range(num_samples)], dim=0)\n        control = einops.rearrange(control, 'b h w c -> b c h w').clone()\n\n        if seed == -1:\n            seed = random.randint(0, 65535)\n        seed_everything(seed)\n\n        if config.save_memory:\n            model.low_vram_shift(is_diffusing=False)\n\n        cond = {\"c_concat\": [control], \"c_crossattn\": [model.get_learned_conditioning([prompt + ', ' + a_prompt] * num_samples)]}\n        un_cond = {\"c_concat\": None if guess_mode else [control], \"c_crossattn\": [model.get_learned_conditioning([n_prompt] * num_samples)]}\n        shape = (4, H // 8, W // 8)\n\n        if config.save_memory:\n            model.low_vram_shift(is_diffusing=True)\n\n        model.control_scales = [strength * (0.825 ** float(12 - i)) for i in range(13)] if guess_mode else ([strength] * 13)\n        # Magic number. IDK why. Perhaps because 0.825**12<0.01 but 0.826**12>0.01\n\n        samples, intermediates = ddim_sampler.sample(ddim_steps, num_samples,\n                                                     shape, cond, verbose=False, eta=eta,\n                                                     unconditional_guidance_scale=scale,\n                                                     unconditional_conditioning=un_cond)\n\n        if config.save_memory:\n            model.low_vram_shift(is_diffusing=False)\n\n        x_samples = model.decode_first_stage(samples)\n        x_samples = (einops.rearrange(x_samples, 'b c h w -> b h w c') * 127.5 + 127.5).cpu().numpy().clip(0, 255).astype(np.uint8)\n\n        results = [x_samples[i] for i in range(num_samples)]\n    return [detected_map] + results\n\n\nblock = gr.Blocks().queue()\nwith block:\n    with gr.Row():\n        gr.Markdown(\"## Control Stable Diffusion with OpenPose\")\n    with gr.Row():\n        with gr.Column():\n            input_image = gr.Image(source='upload', type=\"numpy\")\n            prompt = gr.Textbox(label=\"Prompt\")\n            run_button = gr.Button(label=\"Run\")\n            num_samples = gr.Slider(label=\"Images\", minimum=1, maximum=12, value=1, step=1)\n            seed = gr.Slider(label=\"Seed\", minimum=-1, maximum=2147483647, step=1, value=12345)\n            det = gr.Radio(choices=[\"Openpose_Full\", \"Openpose\", \"None\"], type=\"value\", value=\"Openpose_Full\", label=\"Preprocessor\")\n            with gr.Accordion(\"Advanced options\", open=False):\n                image_resolution = gr.Slider(label=\"Image Resolution\", minimum=256, maximum=768, value=512, step=64)\n                strength = gr.Slider(label=\"Control Strength\", minimum=0.0, maximum=2.0, value=1.0, step=0.01)\n                guess_mode = gr.Checkbox(label='Guess Mode', value=False)\n                detect_resolution = gr.Slider(label=\"Preprocessor Resolution\", minimum=128, maximum=1024, value=512, step=1)\n                ddim_steps = gr.Slider(label=\"Steps\", minimum=1, maximum=100, value=20, step=1)\n                scale = gr.Slider(label=\"Guidance Scale\", minimum=0.1, maximum=30.0, value=9.0, step=0.1)\n                eta = gr.Slider(label=\"DDIM ETA\", minimum=0.0, maximum=1.0, value=1.0, step=0.01)\n                a_prompt = gr.Textbox(label=\"Added Prompt\", value='best quality')\n                n_prompt = gr.Textbox(label=\"Negative Prompt\", value='lowres, bad anatomy, bad hands, cropped, worst quality')\n        with gr.Column():\n            result_gallery = gr.Gallery(label='Output', show_label=False, elem_id=\"gallery\").style(grid=2, height='auto')\n    ips = [det, input_image, prompt, a_prompt, n_prompt, num_samples, image_resolution, detect_resolution, ddim_steps, guess_mode, strength, scale, seed, eta]\n    run_button.click(fn=process, inputs=ips, outputs=[result_gallery])\n\n\nblock.launch(server_name='0.0.0.0')\n"
        },
        {
          "name": "gradio_scribble.py",
          "type": "blob",
          "size": 5.5859375,
          "content": "from share import *\nimport config\n\nimport cv2\nimport einops\nimport gradio as gr\nimport numpy as np\nimport torch\nimport random\n\nfrom pytorch_lightning import seed_everything\nfrom annotator.util import resize_image, HWC3\nfrom annotator.hed import HEDdetector\nfrom annotator.pidinet import PidiNetDetector\nfrom annotator.util import nms\nfrom cldm.model import create_model, load_state_dict\nfrom cldm.ddim_hacked import DDIMSampler\n\n\npreprocessor = None\n\nmodel_name = 'control_v11p_sd15_scribble'\nmodel = create_model(f'./models/{model_name}.yaml').cpu()\nmodel.load_state_dict(load_state_dict('./models/v1-5-pruned.ckpt', location='cuda'), strict=False)\nmodel.load_state_dict(load_state_dict(f'./models/{model_name}.pth', location='cuda'), strict=False)\nmodel = model.cuda()\nddim_sampler = DDIMSampler(model)\n\n\ndef process(det, input_image, prompt, a_prompt, n_prompt, num_samples, image_resolution, detect_resolution, ddim_steps, guess_mode, strength, scale, seed, eta):\n    global preprocessor\n\n    if 'HED' in det:\n        if not isinstance(preprocessor, HEDdetector):\n            preprocessor = HEDdetector()\n\n    if 'PIDI' in det:\n        if not isinstance(preprocessor, PidiNetDetector):\n            preprocessor = PidiNetDetector()\n\n    with torch.no_grad():\n        input_image = HWC3(input_image)\n\n        if det == 'None':\n            detected_map = input_image.copy()\n        else:\n            detected_map = preprocessor(resize_image(input_image, detect_resolution))\n            detected_map = HWC3(detected_map)\n\n        img = resize_image(input_image, image_resolution)\n        H, W, C = img.shape\n\n        detected_map = cv2.resize(detected_map, (W, H), interpolation=cv2.INTER_LINEAR)\n        detected_map = nms(detected_map, 127, 3.0)\n        detected_map = cv2.GaussianBlur(detected_map, (0, 0), 3.0)\n        detected_map[detected_map > 4] = 255\n        detected_map[detected_map < 255] = 0\n\n        control = torch.from_numpy(detected_map.copy()).float().cuda() / 255.0\n        control = torch.stack([control for _ in range(num_samples)], dim=0)\n        control = einops.rearrange(control, 'b h w c -> b c h w').clone()\n\n        if seed == -1:\n            seed = random.randint(0, 65535)\n        seed_everything(seed)\n\n        if config.save_memory:\n            model.low_vram_shift(is_diffusing=False)\n\n        cond = {\"c_concat\": [control], \"c_crossattn\": [model.get_learned_conditioning([prompt + ', ' + a_prompt] * num_samples)]}\n        un_cond = {\"c_concat\": None if guess_mode else [control], \"c_crossattn\": [model.get_learned_conditioning([n_prompt] * num_samples)]}\n        shape = (4, H // 8, W // 8)\n\n        if config.save_memory:\n            model.low_vram_shift(is_diffusing=True)\n\n        model.control_scales = [strength * (0.825 ** float(12 - i)) for i in range(13)] if guess_mode else ([strength] * 13)\n        # Magic number. IDK why. Perhaps because 0.825**12<0.01 but 0.826**12>0.01\n\n        samples, intermediates = ddim_sampler.sample(ddim_steps, num_samples,\n                                                     shape, cond, verbose=False, eta=eta,\n                                                     unconditional_guidance_scale=scale,\n                                                     unconditional_conditioning=un_cond)\n\n        if config.save_memory:\n            model.low_vram_shift(is_diffusing=False)\n\n        x_samples = model.decode_first_stage(samples)\n        x_samples = (einops.rearrange(x_samples, 'b c h w -> b h w c') * 127.5 + 127.5).cpu().numpy().clip(0, 255).astype(np.uint8)\n\n        results = [x_samples[i] for i in range(num_samples)]\n    return [detected_map] + results\n\n\nblock = gr.Blocks().queue()\nwith block:\n    with gr.Row():\n        gr.Markdown(\"## Control Stable Diffusion with Synthesized Scribble\")\n    with gr.Row():\n        with gr.Column():\n            input_image = gr.Image(source='upload', type=\"numpy\")\n            prompt = gr.Textbox(label=\"Prompt\")\n            run_button = gr.Button(label=\"Run\")\n            num_samples = gr.Slider(label=\"Images\", minimum=1, maximum=12, value=1, step=1)\n            seed = gr.Slider(label=\"Seed\", minimum=-1, maximum=2147483647, step=1, value=12345)\n            det = gr.Radio(choices=[\"Scribble_HED\", \"Scribble_PIDI\", \"None\"], type=\"value\", value=\"Scribble_HED\", label=\"Preprocessor\")\n            with gr.Accordion(\"Advanced options\", open=False):\n                image_resolution = gr.Slider(label=\"Image Resolution\", minimum=256, maximum=768, value=512, step=64)\n                strength = gr.Slider(label=\"Control Strength\", minimum=0.0, maximum=2.0, value=1.0, step=0.01)\n                guess_mode = gr.Checkbox(label='Guess Mode', value=False)\n                detect_resolution = gr.Slider(label=\"Preprocessor Resolution\", minimum=128, maximum=1024, value=512, step=1)\n                ddim_steps = gr.Slider(label=\"Steps\", minimum=1, maximum=100, value=20, step=1)\n                scale = gr.Slider(label=\"Guidance Scale\", minimum=0.1, maximum=30.0, value=9.0, step=0.1)\n                eta = gr.Slider(label=\"DDIM ETA\", minimum=0.0, maximum=1.0, value=1.0, step=0.01)\n                a_prompt = gr.Textbox(label=\"Added Prompt\", value='best quality')\n                n_prompt = gr.Textbox(label=\"Negative Prompt\", value='lowres, bad anatomy, bad hands, cropped, worst quality')\n        with gr.Column():\n            result_gallery = gr.Gallery(label='Output', show_label=False, elem_id=\"gallery\").style(grid=2, height='auto')\n    ips = [det, input_image, prompt, a_prompt, n_prompt, num_samples, image_resolution, detect_resolution, ddim_steps, guess_mode, strength, scale, seed, eta]\n    run_button.click(fn=process, inputs=ips, outputs=[result_gallery])\n\n\nblock.launch(server_name='0.0.0.0')\n"
        },
        {
          "name": "gradio_scribble_interactive.py",
          "type": "blob",
          "size": 5.1884765625,
          "content": "from share import *\nimport config\n\nimport einops\nimport gradio as gr\nimport numpy as np\nimport torch\nimport random\n\nfrom pytorch_lightning import seed_everything\nfrom annotator.util import resize_image, HWC3\nfrom cldm.model import create_model, load_state_dict\nfrom cldm.ddim_hacked import DDIMSampler\n\n\npreprocessor = None\n\nmodel_name = 'control_v11p_sd15_scribble'\nmodel = create_model(f'./models/{model_name}.yaml').cpu()\nmodel.load_state_dict(load_state_dict('./models/v1-5-pruned.ckpt', location='cuda'), strict=False)\nmodel.load_state_dict(load_state_dict(f'./models/{model_name}.pth', location='cuda'), strict=False)\nmodel = model.cuda()\nddim_sampler = DDIMSampler(model)\n\n\ndef process(input_image, prompt, a_prompt, n_prompt, num_samples, image_resolution, ddim_steps, guess_mode, strength, scale, seed, eta):\n    with torch.no_grad():\n        img = resize_image(HWC3(input_image['mask'][:, :, 0]), image_resolution)\n        H, W, C = img.shape\n\n        detected_map = np.zeros_like(img, dtype=np.uint8)\n        detected_map[np.min(img, axis=2) > 127] = 255\n\n        control = torch.from_numpy(detected_map.copy()).float().cuda() / 255.0\n        control = torch.stack([control for _ in range(num_samples)], dim=0)\n        control = einops.rearrange(control, 'b h w c -> b c h w').clone()\n\n        if seed == -1:\n            seed = random.randint(0, 65535)\n        seed_everything(seed)\n\n        if config.save_memory:\n            model.low_vram_shift(is_diffusing=False)\n\n        cond = {\"c_concat\": [control], \"c_crossattn\": [model.get_learned_conditioning([prompt + ', ' + a_prompt] * num_samples)]}\n        un_cond = {\"c_concat\": None if guess_mode else [control], \"c_crossattn\": [model.get_learned_conditioning([n_prompt] * num_samples)]}\n        shape = (4, H // 8, W // 8)\n\n        if config.save_memory:\n            model.low_vram_shift(is_diffusing=True)\n\n        model.control_scales = [strength * (0.825 ** float(12 - i)) for i in range(13)] if guess_mode else ([strength] * 13)\n        # Magic number. IDK why. Perhaps because 0.825**12<0.01 but 0.826**12>0.01\n\n        samples, intermediates = ddim_sampler.sample(ddim_steps, num_samples,\n                                                     shape, cond, verbose=False, eta=eta,\n                                                     unconditional_guidance_scale=scale,\n                                                     unconditional_conditioning=un_cond)\n\n        if config.save_memory:\n            model.low_vram_shift(is_diffusing=False)\n\n        x_samples = model.decode_first_stage(samples)\n        x_samples = (einops.rearrange(x_samples, 'b c h w -> b h w c') * 127.5 + 127.5).cpu().numpy().clip(0, 255).astype(np.uint8)\n\n        results = [x_samples[i] for i in range(num_samples)]\n    return [detected_map] + results\n\n\ndef create_canvas(w, h):\n    return np.zeros(shape=(h, w, 3), dtype=np.uint8) + 255\n\n\nblock = gr.Blocks().queue()\nwith block:\n    with gr.Row():\n        gr.Markdown(\"## Control Stable Diffusion with Interactive Scribbles\")\n    with gr.Row():\n        with gr.Column():\n            canvas_width = gr.Slider(label=\"Canvas Width\", minimum=256, maximum=1024, value=512, step=1)\n            canvas_height = gr.Slider(label=\"Canvas Height\", minimum=256, maximum=1024, value=512, step=1)\n            create_button = gr.Button(label=\"Start\", value='Open drawing canvas!')\n            input_image = gr.Image(source='upload', type='numpy', tool='sketch')\n            gr.Markdown(value='Do not forget to change your brush width to make it thinner. '\n                              'Just click on the small pencil icon in the upper right corner of the above block.')\n            create_button.click(fn=create_canvas, inputs=[canvas_width, canvas_height], outputs=[input_image])\n            prompt = gr.Textbox(label=\"Prompt\")\n            run_button = gr.Button(label=\"Run\")\n            num_samples = gr.Slider(label=\"Images\", minimum=1, maximum=12, value=1, step=1)\n            seed = gr.Slider(label=\"Seed\", minimum=-1, maximum=2147483647, step=1, value=12345)\n            with gr.Accordion(\"Advanced options\", open=False):\n                image_resolution = gr.Slider(label=\"Image Resolution\", minimum=256, maximum=768, value=512, step=64)\n                strength = gr.Slider(label=\"Control Strength\", minimum=0.0, maximum=2.0, value=1.0, step=0.01)\n                guess_mode = gr.Checkbox(label='Guess Mode', value=False)\n                ddim_steps = gr.Slider(label=\"Steps\", minimum=1, maximum=100, value=20, step=1)\n                scale = gr.Slider(label=\"Guidance Scale\", minimum=0.1, maximum=30.0, value=9.0, step=0.1)\n                eta = gr.Slider(label=\"DDIM ETA\", minimum=0.0, maximum=1.0, value=1.0, step=0.01)\n                a_prompt = gr.Textbox(label=\"Added Prompt\", value='best quality')\n                n_prompt = gr.Textbox(label=\"Negative Prompt\", value='lowres, bad anatomy, bad hands, cropped, worst quality')\n        with gr.Column():\n            result_gallery = gr.Gallery(label='Output', show_label=False, elem_id=\"gallery\").style(grid=2, height='auto')\n    ips = [input_image, prompt, a_prompt, n_prompt, num_samples, image_resolution, ddim_steps, guess_mode, strength, scale, seed, eta]\n    run_button.click(fn=process, inputs=ips, outputs=[result_gallery])\n\n\nblock.launch(server_name='0.0.0.0')\n"
        },
        {
          "name": "gradio_seg.py",
          "type": "blob",
          "size": 5.587890625,
          "content": "from share import *\nimport config\n\nimport cv2\nimport einops\nimport gradio as gr\nimport numpy as np\nimport torch\nimport random\n\nfrom pytorch_lightning import seed_everything\nfrom annotator.util import resize_image, HWC3\nfrom annotator.uniformer import UniformerDetector\nfrom annotator.oneformer import OneformerCOCODetector, OneformerADE20kDetector\nfrom cldm.model import create_model, load_state_dict\nfrom cldm.ddim_hacked import DDIMSampler\n\n\npreprocessor = None\n\nmodel_name = 'control_v11p_sd15_seg'\nmodel = create_model(f'./models/{model_name}.yaml').cpu()\nmodel.load_state_dict(load_state_dict('./models/v1-5-pruned.ckpt', location='cuda'), strict=False)\nmodel.load_state_dict(load_state_dict(f'./models/{model_name}.pth', location='cuda'), strict=False)\nmodel = model.cuda()\nddim_sampler = DDIMSampler(model)\n\n\ndef process(det, input_image, prompt, a_prompt, n_prompt, num_samples, image_resolution, detect_resolution, ddim_steps, guess_mode, strength, scale, seed, eta):\n    global preprocessor\n\n    if det == 'Seg_OFCOCO':\n        if not isinstance(preprocessor, OneformerCOCODetector):\n            preprocessor = OneformerCOCODetector()\n    if det == 'Seg_OFADE20K':\n        if not isinstance(preprocessor, OneformerADE20kDetector):\n            preprocessor = OneformerADE20kDetector()\n    if det == 'Seg_UFADE20K':\n        if not isinstance(preprocessor, UniformerDetector):\n            preprocessor = UniformerDetector()\n\n    with torch.no_grad():\n        input_image = HWC3(input_image)\n\n        if det == 'None':\n            detected_map = input_image.copy()\n        else:\n            detected_map = preprocessor(resize_image(input_image, detect_resolution))\n            detected_map = HWC3(detected_map)\n\n        img = resize_image(input_image, image_resolution)\n        H, W, C = img.shape\n\n        detected_map = cv2.resize(detected_map, (W, H), interpolation=cv2.INTER_LINEAR)\n\n        control = torch.from_numpy(detected_map.copy()).float().cuda() / 255.0\n        control = torch.stack([control for _ in range(num_samples)], dim=0)\n        control = einops.rearrange(control, 'b h w c -> b c h w').clone()\n\n        if seed == -1:\n            seed = random.randint(0, 65535)\n        seed_everything(seed)\n\n        if config.save_memory:\n            model.low_vram_shift(is_diffusing=False)\n\n        cond = {\"c_concat\": [control], \"c_crossattn\": [model.get_learned_conditioning([prompt + ', ' + a_prompt] * num_samples)]}\n        un_cond = {\"c_concat\": None if guess_mode else [control], \"c_crossattn\": [model.get_learned_conditioning([n_prompt] * num_samples)]}\n        shape = (4, H // 8, W // 8)\n\n        if config.save_memory:\n            model.low_vram_shift(is_diffusing=True)\n\n        model.control_scales = [strength * (0.825 ** float(12 - i)) for i in range(13)] if guess_mode else ([strength] * 13)\n        # Magic number. IDK why. Perhaps because 0.825**12<0.01 but 0.826**12>0.01\n\n        samples, intermediates = ddim_sampler.sample(ddim_steps, num_samples,\n                                                     shape, cond, verbose=False, eta=eta,\n                                                     unconditional_guidance_scale=scale,\n                                                     unconditional_conditioning=un_cond)\n\n        if config.save_memory:\n            model.low_vram_shift(is_diffusing=False)\n\n        x_samples = model.decode_first_stage(samples)\n        x_samples = (einops.rearrange(x_samples, 'b c h w -> b h w c') * 127.5 + 127.5).cpu().numpy().clip(0, 255).astype(np.uint8)\n\n        results = [x_samples[i] for i in range(num_samples)]\n    return [detected_map] + results\n\n\nblock = gr.Blocks().queue()\nwith block:\n    with gr.Row():\n        gr.Markdown(\"## Control Stable Diffusion with Semantic Segmentation\")\n    with gr.Row():\n        with gr.Column():\n            input_image = gr.Image(source='upload', type=\"numpy\")\n            prompt = gr.Textbox(label=\"Prompt\")\n            run_button = gr.Button(label=\"Run\")\n            num_samples = gr.Slider(label=\"Images\", minimum=1, maximum=12, value=1, step=1)\n            seed = gr.Slider(label=\"Seed\", minimum=-1, maximum=2147483647, step=1, value=12345)\n            det = gr.Radio(choices=[\"Seg_OFADE20K\", \"Seg_OFCOCO\", \"Seg_UFADE20K\", \"None\"], type=\"value\", value=\"Seg_OFADE20K\", label=\"Preprocessor\")\n            with gr.Accordion(\"Advanced options\", open=False):\n                image_resolution = gr.Slider(label=\"Image Resolution\", minimum=256, maximum=768, value=512, step=64)\n                strength = gr.Slider(label=\"Control Strength\", minimum=0.0, maximum=2.0, value=1.0, step=0.01)\n                guess_mode = gr.Checkbox(label='Guess Mode', value=False)\n                detect_resolution = gr.Slider(label=\"Preprocessor Resolution\", minimum=128, maximum=1024, value=512, step=1)\n                ddim_steps = gr.Slider(label=\"Steps\", minimum=1, maximum=100, value=20, step=1)\n                scale = gr.Slider(label=\"Guidance Scale\", minimum=0.1, maximum=30.0, value=9.0, step=0.1)\n                eta = gr.Slider(label=\"DDIM ETA\", minimum=0.0, maximum=1.0, value=1.0, step=0.01)\n                a_prompt = gr.Textbox(label=\"Added Prompt\", value='best quality')\n                n_prompt = gr.Textbox(label=\"Negative Prompt\", value='lowres, bad anatomy, bad hands, cropped, worst quality')\n        with gr.Column():\n            result_gallery = gr.Gallery(label='Output', show_label=False, elem_id=\"gallery\").style(grid=2, height='auto')\n    ips = [det, input_image, prompt, a_prompt, n_prompt, num_samples, image_resolution, detect_resolution, ddim_steps, guess_mode, strength, scale, seed, eta]\n    run_button.click(fn=process, inputs=ips, outputs=[result_gallery])\n\n\nblock.launch(server_name='0.0.0.0')\n"
        },
        {
          "name": "gradio_shuffle.py",
          "type": "blob",
          "size": 4.5859375,
          "content": "from share import *\nimport config\n\nimport cv2\nimport einops\nimport gradio as gr\nimport numpy as np\nimport torch\nimport random\n\nfrom pytorch_lightning import seed_everything\nfrom annotator.util import resize_image, HWC3\nfrom annotator.shuffle import ContentShuffleDetector\nfrom cldm.model import create_model, load_state_dict\nfrom cldm.ddim_hacked import DDIMSampler\n\n\nmodel_name = 'control_v11e_sd15_shuffle'\nmodel = create_model(f'./models/{model_name}.yaml').cpu()\nmodel.load_state_dict(load_state_dict('./models/v1-5-pruned.ckpt', location='cuda'), strict=False)\nmodel.load_state_dict(load_state_dict(f'./models/{model_name}.pth', location='cuda'), strict=False)\nmodel = model.cuda()\nddim_sampler = DDIMSampler(model)\n\npreprocessor = ContentShuffleDetector()\n\n\ndef process(det, input_image, prompt, a_prompt, n_prompt, num_samples, image_resolution, ddim_steps, strength, scale, seed, eta):\n    with torch.no_grad():\n        input_image = HWC3(input_image)\n        detected_map = input_image.copy()\n        img = resize_image(input_image, image_resolution)\n        H, W, C = img.shape\n\n        if det == \"Shuffle\":\n            np.random.seed(seed)\n            detected_map = preprocessor(detected_map, w=W, h=H, f=256)\n        else:\n            detected_map = img.copy()\n\n        control = torch.from_numpy(detected_map.copy()).float().cuda() / 255.0\n        control = torch.stack([control for _ in range(num_samples)], dim=0)\n        control = einops.rearrange(control, 'b h w c -> b c h w').clone()\n\n        if seed == -1:\n            seed = random.randint(0, 65535)\n        seed_everything(seed)\n\n        if config.save_memory:\n            model.low_vram_shift(is_diffusing=False)\n\n        cond = {\"c_concat\": [control], \"c_crossattn\": [model.get_learned_conditioning([prompt + ', ' + a_prompt] * num_samples)]}\n        un_cond = {\"c_concat\": None, \"c_crossattn\": [model.get_learned_conditioning([n_prompt] * num_samples)]}\n        shape = (4, H // 8, W // 8)\n\n        if config.save_memory:\n            model.low_vram_shift(is_diffusing=True)\n\n        model.control_scales = [strength] * 13\n        samples, intermediates = ddim_sampler.sample(ddim_steps, num_samples,\n                                                     shape, cond, verbose=False, eta=eta,\n                                                     unconditional_guidance_scale=scale,\n                                                     unconditional_conditioning=un_cond)\n\n        if config.save_memory:\n            model.low_vram_shift(is_diffusing=False)\n\n        x_samples = model.decode_first_stage(samples)\n        x_samples = (einops.rearrange(x_samples, 'b c h w -> b h w c') * 127.5 + 127.5).cpu().numpy().clip(0, 255).astype(np.uint8)\n\n        results = [x_samples[i] for i in range(num_samples)]\n\n    if det == \"Shuffle\":\n        return [detected_map] + results\n    else:\n        return results\n\n\nblock = gr.Blocks().queue()\nwith block:\n    with gr.Row():\n        gr.Markdown(\"## Control Stable Diffusion with Content Shuffle\")\n    with gr.Row():\n        with gr.Column():\n            input_image = gr.Image(source='upload', type=\"numpy\")\n            prompt = gr.Textbox(label=\"Prompt\")\n            run_button = gr.Button(label=\"Run\")\n            num_samples = gr.Slider(label=\"Images\", minimum=1, maximum=12, value=1, step=1)\n            seed = gr.Slider(label=\"Seed\", minimum=-1, maximum=2147483647, step=1, value=12345)\n            det = gr.Radio(choices=[\"Shuffle\", \"None\"], type=\"value\", value=\"None\", label=\"Preprocessor\")\n            with gr.Accordion(\"Advanced options\", open=False):\n                image_resolution = gr.Slider(label=\"Image Resolution\", minimum=256, maximum=768, value=512, step=64)\n                strength = gr.Slider(label=\"Control Strength\", minimum=0.0, maximum=2.0, value=1.0, step=0.01)\n                ddim_steps = gr.Slider(label=\"Steps\", minimum=1, maximum=100, value=20, step=1)\n                scale = gr.Slider(label=\"Guidance Scale\", minimum=0.1, maximum=30.0, value=9.0, step=0.1)\n                eta = gr.Slider(label=\"DDIM ETA\", minimum=0.0, maximum=1.0, value=1.0, step=0.01)\n                a_prompt = gr.Textbox(label=\"Added Prompt\", value='best quality')\n                n_prompt = gr.Textbox(label=\"Negative Prompt\", value='lowres, bad anatomy, bad hands, cropped, worst quality')\n        with gr.Column():\n            result_gallery = gr.Gallery(label='Output', show_label=False, elem_id=\"gallery\").style(grid=2, height='auto')\n    ips = [det, input_image, prompt, a_prompt, n_prompt, num_samples, image_resolution, ddim_steps, strength, scale, seed, eta]\n    run_button.click(fn=process, inputs=ips, outputs=[result_gallery])\n\n\nblock.launch(server_name='0.0.0.0')\n"
        },
        {
          "name": "gradio_softedge.py",
          "type": "blob",
          "size": 5.4853515625,
          "content": "from share import *\nimport config\n\nimport cv2\nimport einops\nimport gradio as gr\nimport numpy as np\nimport torch\nimport random\n\nfrom pytorch_lightning import seed_everything\nfrom annotator.util import resize_image, HWC3\nfrom annotator.hed import HEDdetector\nfrom annotator.pidinet import PidiNetDetector\nfrom cldm.model import create_model, load_state_dict\nfrom cldm.ddim_hacked import DDIMSampler\n\n\npreprocessor = None\n\nmodel_name = 'control_v11p_sd15_softedge'\nmodel = create_model(f'./models/{model_name}.yaml').cpu()\nmodel.load_state_dict(load_state_dict('./models/v1-5-pruned.ckpt', location='cuda'), strict=False)\nmodel.load_state_dict(load_state_dict(f'./models/{model_name}.pth', location='cuda'), strict=False)\nmodel = model.cuda()\nddim_sampler = DDIMSampler(model)\n\n\ndef process(det, input_image, prompt, a_prompt, n_prompt, num_samples, image_resolution, detect_resolution, ddim_steps, guess_mode, strength, scale, seed, eta, is_safe):\n    global preprocessor\n\n    if 'HED' in det:\n        if not isinstance(preprocessor, HEDdetector):\n            preprocessor = HEDdetector()\n\n    if 'PIDI' in det:\n        if not isinstance(preprocessor, PidiNetDetector):\n            preprocessor = PidiNetDetector()\n\n    with torch.no_grad():\n        input_image = HWC3(input_image)\n\n        if det == 'None':\n            detected_map = input_image.copy()\n        else:\n            detected_map = preprocessor(resize_image(input_image, detect_resolution), safe='safe' in det)\n            detected_map = HWC3(detected_map)\n\n        img = resize_image(input_image, image_resolution)\n        H, W, C = img.shape\n\n        detected_map = cv2.resize(detected_map, (W, H), interpolation=cv2.INTER_LINEAR)\n\n        control = torch.from_numpy(detected_map.copy()).float().cuda() / 255.0\n        control = torch.stack([control for _ in range(num_samples)], dim=0)\n        control = einops.rearrange(control, 'b h w c -> b c h w').clone()\n\n        if seed == -1:\n            seed = random.randint(0, 65535)\n        seed_everything(seed)\n\n        if config.save_memory:\n            model.low_vram_shift(is_diffusing=False)\n\n        cond = {\"c_concat\": [control], \"c_crossattn\": [model.get_learned_conditioning([prompt + ', ' + a_prompt] * num_samples)]}\n        un_cond = {\"c_concat\": None if guess_mode else [control], \"c_crossattn\": [model.get_learned_conditioning([n_prompt] * num_samples)]}\n        shape = (4, H // 8, W // 8)\n\n        if config.save_memory:\n            model.low_vram_shift(is_diffusing=True)\n\n        model.control_scales = [strength * (0.825 ** float(12 - i)) for i in range(13)] if guess_mode else ([strength] * 13)\n        # Magic number. IDK why. Perhaps because 0.825**12<0.01 but 0.826**12>0.01\n\n        samples, intermediates = ddim_sampler.sample(ddim_steps, num_samples,\n                                                     shape, cond, verbose=False, eta=eta,\n                                                     unconditional_guidance_scale=scale,\n                                                     unconditional_conditioning=un_cond)\n\n        if config.save_memory:\n            model.low_vram_shift(is_diffusing=False)\n\n        x_samples = model.decode_first_stage(samples)\n        x_samples = (einops.rearrange(x_samples, 'b c h w -> b h w c') * 127.5 + 127.5).cpu().numpy().clip(0, 255).astype(np.uint8)\n\n        results = [x_samples[i] for i in range(num_samples)]\n    return [detected_map] + results\n\n\nblock = gr.Blocks().queue()\nwith block:\n    with gr.Row():\n        gr.Markdown(\"## Control Stable Diffusion with Soft Edge\")\n    with gr.Row():\n        with gr.Column():\n            input_image = gr.Image(source='upload', type=\"numpy\")\n            prompt = gr.Textbox(label=\"Prompt\")\n            run_button = gr.Button(label=\"Run\")\n            num_samples = gr.Slider(label=\"Images\", minimum=1, maximum=12, value=1, step=1)\n            seed = gr.Slider(label=\"Seed\", minimum=-1, maximum=2147483647, step=1, value=12345)\n            det = gr.Radio(choices=[\"SoftEdge_PIDI\", \"SoftEdge_PIDI_safe\", \"SoftEdge_HED\", \"SoftEdge_HED_safe\", \"None\"], type=\"value\", value=\"SoftEdge_PIDI\", label=\"Preprocessor\")\n            with gr.Accordion(\"Advanced options\", open=False):\n                is_safe = gr.Checkbox(label='Safe', value=False)\n                image_resolution = gr.Slider(label=\"Image Resolution\", minimum=256, maximum=768, value=512, step=64)\n                strength = gr.Slider(label=\"Control Strength\", minimum=0.0, maximum=2.0, value=1.0, step=0.01)\n                guess_mode = gr.Checkbox(label='Guess Mode', value=False)\n                detect_resolution = gr.Slider(label=\"Preprocessor Resolution\", minimum=128, maximum=1024, value=512, step=1)\n                ddim_steps = gr.Slider(label=\"Steps\", minimum=1, maximum=100, value=20, step=1)\n                scale = gr.Slider(label=\"Guidance Scale\", minimum=0.1, maximum=30.0, value=9.0, step=0.1)\n                eta = gr.Slider(label=\"DDIM ETA\", minimum=0.0, maximum=1.0, value=1.0, step=0.01)\n                a_prompt = gr.Textbox(label=\"Added Prompt\", value='best quality')\n                n_prompt = gr.Textbox(label=\"Negative Prompt\", value='lowres, bad anatomy, bad hands, cropped, worst quality')\n        with gr.Column():\n            result_gallery = gr.Gallery(label='Output', show_label=False, elem_id=\"gallery\").style(grid=2, height='auto')\n    ips = [det, input_image, prompt, a_prompt, n_prompt, num_samples, image_resolution, detect_resolution, ddim_steps, guess_mode, strength, scale, seed, eta, is_safe]\n    run_button.click(fn=process, inputs=ips, outputs=[result_gallery])\n\n\nblock.launch(server_name='0.0.0.0')\n"
        },
        {
          "name": "gradio_tile.py",
          "type": "blob",
          "size": 5.1259765625,
          "content": "from share import *\nimport config\n\nimport cv2\nimport einops\nimport gradio as gr\nimport numpy as np\nimport torch\nimport random\n\nfrom pytorch_lightning import seed_everything\nfrom annotator.util import resize_image, HWC3\nfrom cldm.model import create_model, load_state_dict\nfrom cldm.ddim_hacked import DDIMSampler\n\n\nmodel_name = 'control_v11f1e_sd15_tile'\nmodel = create_model(f'./models/{model_name}.yaml').cpu()\nmodel.load_state_dict(load_state_dict('./models/v1-5-pruned.ckpt', location='cuda'), strict=False)\nmodel.load_state_dict(load_state_dict(f'./models/{model_name}.pth', location='cuda'), strict=False)\nmodel = model.cuda()\nddim_sampler = DDIMSampler(model)\n\n\ndef process(input_image, prompt, a_prompt, n_prompt, num_samples, image_resolution, ddim_steps, guess_mode, strength, scale, seed, eta, denoise_strength):\n    global preprocessor\n\n    with torch.no_grad():\n        input_image = HWC3(input_image)\n        detected_map = input_image.copy()\n\n        img = resize_image(input_image, image_resolution)\n        H, W, C = img.shape\n\n        detected_map = cv2.resize(detected_map, (W, H), interpolation=cv2.INTER_LINEAR)\n\n        control = torch.from_numpy(detected_map.copy()).float().cuda() / 255.0\n        control = torch.stack([control for _ in range(num_samples)], dim=0)\n        control = einops.rearrange(control, 'b h w c -> b c h w').clone()\n\n        img = torch.from_numpy(img.copy()).float().cuda() / 127.0 - 1.0\n        img = torch.stack([img for _ in range(num_samples)], dim=0)\n        img = einops.rearrange(img, 'b h w c -> b c h w').clone()\n\n        if seed == -1:\n            seed = random.randint(0, 65535)\n        seed_everything(seed)\n\n        if config.save_memory:\n            model.low_vram_shift(is_diffusing=False)\n\n        cond = {\"c_concat\": [control], \"c_crossattn\": [model.get_learned_conditioning([prompt + ', ' + a_prompt] * num_samples)]}\n        un_cond = {\"c_concat\": None if guess_mode else [control], \"c_crossattn\": [model.get_learned_conditioning([n_prompt] * num_samples)]}\n\n        if config.save_memory:\n            model.low_vram_shift(is_diffusing=False)\n\n        ddim_sampler.make_schedule(ddim_steps, ddim_eta=eta, verbose=True)\n        t_enc = min(int(denoise_strength * ddim_steps), ddim_steps - 1)\n        z = model.get_first_stage_encoding(model.encode_first_stage(img))\n        z_enc = ddim_sampler.stochastic_encode(z, torch.tensor([t_enc] * num_samples).to(model.device))\n\n        if config.save_memory:\n            model.low_vram_shift(is_diffusing=True)\n\n        model.control_scales = [strength * (0.825 ** float(12 - i)) for i in range(13)] if guess_mode else ([strength] * 13)\n        # Magic number. IDK why. Perhaps because 0.825**12<0.01 but 0.826**12>0.01\n\n        samples = ddim_sampler.decode(z_enc, cond, t_enc, unconditional_guidance_scale=scale, unconditional_conditioning=un_cond)\n\n        if config.save_memory:\n            model.low_vram_shift(is_diffusing=False)\n\n        x_samples = model.decode_first_stage(samples)\n        x_samples = (einops.rearrange(x_samples, 'b c h w -> b h w c') * 127.5 + 127.5).cpu().numpy().clip(0, 255).astype(np.uint8)\n\n        results = [x_samples[i] for i in range(num_samples)]\n    return [input_image] + results\n\n\nblock = gr.Blocks().queue()\nwith block:\n    with gr.Row():\n        gr.Markdown(\"## Control Stable Diffusion with Tile\")\n    with gr.Row():\n        with gr.Column():\n            input_image = gr.Image(source='upload', type=\"numpy\")\n            prompt = gr.Textbox(label=\"Prompt\")\n            run_button = gr.Button(label=\"Run\")\n            num_samples = gr.Slider(label=\"Images\", minimum=1, maximum=12, value=1, step=1)\n            seed = gr.Slider(label=\"Seed\", minimum=-1, maximum=2147483647, step=1, value=12345)\n            det = gr.Radio(choices=[\"None\"], type=\"value\", value=\"None\", label=\"Preprocessor\")\n            denoise_strength = gr.Slider(label=\"Denoising Strength\", minimum=0.1, maximum=1.0, value=1.0, step=0.01)\n            with gr.Accordion(\"Advanced options\", open=False):\n                image_resolution = gr.Slider(label=\"Image Resolution\", minimum=256, maximum=2048, value=512, step=64)\n                strength = gr.Slider(label=\"Control Strength\", minimum=0.0, maximum=2.0, value=1.0, step=0.01)\n                guess_mode = gr.Checkbox(label='Guess Mode', value=False)\n                ddim_steps = gr.Slider(label=\"Steps\", minimum=1, maximum=100, value=32, step=1)\n                scale = gr.Slider(label=\"Guidance Scale\", minimum=0.1, maximum=30.0, value=9.0, step=0.1)\n                eta = gr.Slider(label=\"DDIM ETA\", minimum=0.0, maximum=1.0, value=1.0, step=0.01)\n                a_prompt = gr.Textbox(label=\"Added Prompt\", value='best quality')\n                n_prompt = gr.Textbox(label=\"Negative Prompt\", value='blur, lowres, bad anatomy, bad hands, cropped, worst quality')\n        with gr.Column():\n            result_gallery = gr.Gallery(label='Output', show_label=False, elem_id=\"gallery\").style(grid=2, height='auto')\n    ips = [input_image, prompt, a_prompt, n_prompt, num_samples, image_resolution, ddim_steps, guess_mode, strength, scale, seed, eta, denoise_strength]\n    run_button.click(fn=process, inputs=ips, outputs=[result_gallery])\n\n\nblock.launch(server_name='0.0.0.0')\n"
        },
        {
          "name": "ldm",
          "type": "tree",
          "content": null
        },
        {
          "name": "models",
          "type": "tree",
          "content": null
        },
        {
          "name": "share.py",
          "type": "blob",
          "size": 0.1513671875,
          "content": "import config\nfrom cldm.hack import disable_verbosity, enable_sliced_attention\n\n\ndisable_verbosity()\n\nif config.save_memory:\n    enable_sliced_attention()\n"
        },
        {
          "name": "test_imgs",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}