{
  "metadata": {
    "timestamp": 1736560017049,
    "page": 828,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjgzMA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "NExT-GPT/NExT-GPT",
      "stars": 3382,
      "defaultBranch": "main",
      "files": [
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.0244140625,
          "content": "__pycache__/\n.idea\n*.pyc\n"
        },
        {
          "name": "LICENSE.md",
          "type": "blob",
          "size": 1.46484375,
          "content": "BSD 3-Clause License\n\nCopyright 2023 Shengqiong Wu All rights reserved.\n\nRedistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:\n\n1. Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.\n\n2. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution.\n\n3. Neither the name of the copyright holder nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE."
        },
        {
          "name": "NExT-GPT-Lagacy",
          "type": "tree",
          "content": null
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 16.1201171875,
          "content": "# <img src=\"NExT-GPT-Lagacy/code/nextgpt.png\" style=\"width: 5%\"> NExT-GPT: Any-to-Any Multimodal LLM\n[Shengqiong Wu](https://chocowu.github.io/), [Hao Fei](http://haofei.vip/)*, [Leigang Qu](#), [Wei Ji](https://jiwei0523.github.io/), and [Tat-Seng Chua](https://www.chuatatseng.com/).\n(*Correspondence )\n\n**ICML 2024, Oral Paper**\n\n**[NExT++ Research Center](https://www.nextcenter.org/), School of Computing, National University of Singapore**\n\n-----\n\n<a href='https://next-gpt.github.io/'><img src='https://img.shields.io/badge/Project-Page-Green'></a>\n<a href='#'><img src='https://img.shields.io/badge/Demo-Page-purple'></a> \n<a href='https://arxiv.org/pdf/2309.05519'><img src='https://img.shields.io/badge/Paper-PDF-orange'></a> \n![License](https://img.shields.io/badge/License-BSD-blue.svg)\n[![YouTube](https://badges.aleen42.com/src/youtube.svg)](https://www.youtube.com/watch?v=aqw2SCWeWD0)\n\n\nThis repository hosts the code, data and model weight of **NExT-GPT**, the first end-to-end MM-LLM that perceives input and generates output in arbitrary combinations (any-to-any) of text, image, video, and audio and beyond.\n\n\n**Noted**: we wrap the former old codebase into the [NExT-GPT-Lagacy](NExT-GPT-Lagacy). Please refer to this new codebase for all training and tuning procedures.\n\n-----------\n\n## üéâ News \n\n- [x] [2023.09.15] üöÄüöÄ Release the code of NExT-GPT in version `7b_tiva_v0`.\n- [x] [2023.09.27] üî®üß© Added modality-blended batch sampler.\n- [x] [2023.10.01] üì¢üì¢ Release the T2M instruction dataset.\n- [x] [2023.10.04] üëèüëè Release the checkpoint of NExT-GPT in version [7b_tiva_v0](https://huggingface.co/ChocoWu/nextgpt_7b_tiva_v0) .\n- [x] [2023.10.15] üî®üöÄ Update of NExT-GPT in version [7b_tiva_v0](https://huggingface.co/ChocoWu/nextgpt_7b_tiva_v0) .\n- [x] [2024.10.07] üëèüëè Release the data and the corresponding construction methods, please refer [DATA_README.md](data/DATA_README.md) for more details.\n\n\n## üëâ TODO \n- [ ] Updating NExT-GPT in more types&sizes of LLMs.\n- [ ] Empowering NExT-GPT with more modalities of inputs&outputs.\n- [ ] ...\n\n\n\n-----------\n\n## Example Demos\nHere we showcase examples generated from NExT-GPT.\nFor more examples, kindly visit the [webpage](https://next-gpt.github.io/), or the online live [demo](https://acc414b22d6839d28f.gradio.live). \n\n\nhttps://github.com/NExT-GPT/NExT-GPT/assets/18722770/0c2b3d88-a533-4899-ab44-65580fe54538\n\n\nhttps://github.com/NExT-GPT/NExT-GPT/assets/18722770/eb1319a6-38aa-4546-a96e-163207e7de93\n\n\nhttps://github.com/NExT-GPT/NExT-GPT/assets/18722770/36bec0ad-9bad-4bcf-bc37-92b028f1bc6a\n\n\n\n<span id='introduction'/>\n\n## Brief Introduction \n\n\nNExt-GPT is built on top of existing pre-trained LLM, multimodal encoder and SoTA diffusion models, with sufficient end-to-end instruction tuning.\n\n<p align=\"center\" width=\"100%\">\n<a target=\"_blank\"><img src=\"figures/framework.png\" alt=\"Video-LLaMA\" style=\"width: 90%; min-width: 200px; display: block; margin: auto;\"></a>\n</p>\n\n- **Multimodal Encoding Stage.** Leveraging established encoders to encode inputs in various modalities, where these representations are projected into language-like representations comprehensible to the LLM through a projection layer.\n- **LLM Understanding and Reasoning Stage.** Harnessing an existing open-sourced LLM as the core to process input information for semantic understanding and reasoning. The LLM not only directly generates text tokens but also produces unique ‚Äúmodality signal‚Äù tokens that serve as instructions to dictate the decoding layers whether & what modal content to output correspondingly.\n- **Multimodal Generation Stage.** Receiving the multimodal signals with specific instructions from LLM (if any), the Transformer-based output projection layers map the signal token representations into the ones that are understandable to following multimodal decoders.\n\n\nFor more technical details, kindly refer to the [paper](https://arxiv.org/pdf/2309.05519.pdf). \n\n\n-----------\n\n\n<span id='Usage'/>\n\n## Getting Started\n\n\n\n<span id='all_catelogue'/>\n\n### Table of Contents:\n* <a href='#Code Structure'>1. Code Structure</a>\n* <a href='#Environment Preparation'>2. Environment Preparation </a>\n* <a href='#Training on Your Own'>3. Training/Adapting NExt-GPT on Your Own</a>\n  * <a href='#Prepare Pre-trained Checkpoint'>3.1. Preparing Pre-trained Checkpoint</a>\n  * <a href='#Prepare Dataset'>3.2. Preparing Dataset </a>\n  * <a href='#Precompute Embeddings'>3.3. Precomputing Embeddings</a>\n  * <a href='#Train NExT-GPT'>3.4. Training NExT-GPT</a>\n* <a href='#Run NExT-GPT System'>4. Running NExT-GPT System</a>\n  * <a href='#Prepare checkpoints'>4.1. Preparing checkpoints</a>\n  * <a href='#Deploy Demo System'>4.2. Deploying Demo System</a>\n* <a href='#Tuning your own system'>5. Fine-tuning your own System</a>\n  * <a href='#Tuning your own dataset'>5.1. Dataset</a>\n  * <a href='#Tuning your own framework'>5.2. Model Framework</a>\n  * <a href='#Tuning script'>5.3. Fine-tuning</a>\n \n****\n\n\n\n\n\n<span id='Code Structure'/>\n\n### 1. Code Structure \n\n```\n.\n|-- NExT-GPT-Lagacy       # the previous version of the model\n|-- assets\n|-- checkpoints           # save the pretraining and tuning checkpoints\n|-- data  \n|   |-- IT_data\n|   |   |-- MosIT_data\n|   |   |-- T+X-T_data    # text+[image/audio/video] to text instruction data\n|   |   `-- T-T+X_data    # synthesized text to text+[image/audio/video] instruction data\n|   |-- T_X_pair_data     # text-autio pairs data\n|   |   |-- audiocap\n|   |   |-- cc3m\n|   |   `-- webvid\n|   |-- embed \n|   `-- prepare_data.py\n|-- figures\n|-- merge_lora_weights.py\n|-- nextgpt\n|   |-- __init__.py\n|   |-- constants.py\n|   |-- conversation.py\n|   |-- dataset\n|   |   |-- __init__.py\n|   |   |-- audio_processor.py\n|   |   |-- base_dataset.py\n|   |   |-- catalog.py\n|   |   |-- concat_dataset.py\n|   |   |-- dataset_utils.py\n|   |   `-- sampler.py\n|   |-- mm_utils.py\n|   |-- model\n|   |   |-- __init__.py\n|   |   |-- apply_delta.py\n|   |   |-- builder.py\n|   |   |-- consolidate.py\n|   |   |-- language_model\n|   |   |-- make_delta.py\n|   |   |-- multimodal_decoder\n|   |   |-- multimodal_encoder\n|   |   |-- multimodal_projector\n|   |   |-- nextgpt_arch.py\n|   |   `-- utils.py\n|   `-- utils.py\n|-- scripts\n|   |-- finetune.sh\n|   |-- pretrain_dec.sh\n|   |-- pretrain_enc.sh\n|   |-- zero2.json\n|   |-- zero3.json\n|   `-- zero3_offload.json\n|-- LICENSE.md\n|-- README.md\n|-- nextgpt_trainer.py\n|-- predict.py\n|-- preprocess_embeddings.py\n|-- requirements.txt\n|-- train.py\n|-- train_mem.py\n`-- training_utils.py\n```\n\n\n<span id='Environment Preparation'/>\n\n\n### 2. Environment Preparation  <a href='#all_catelogue'>[Back to Top]</a>\nPlease first clone the repo and install the required environment, which can be done by running the following commands:\n```\nconda env create -n nextgpt python=3.8\n\nconda activate nextgpt\n\n# CUDA 12.1\nconda install pytorch==2.1.2 torchvision==0.14.1 torchaudio==0.13.1 pytorch-cuda=11.6 -c pytorch -c nvidia\n\ngit clone https://github.com/NExT-GPT/NExT-GPT.git\ncd NExT-GPT\n\npip install -r requirements.txt\n```\n\n<span id='Training on Your Own'/>\n\n### 3. Training/Adapting NExt-GPT on Your Own \n\n\n\n<span id='Prepare Pre-trained Checkpoint'/>\n\n#### 3.1. Preparing Pre-trained Checkpoint  <a href='#all_catelogue'>[Back to Top]</a>\nNExT-GPT is trained based on following excellent existing models.\nPlease follow the instructions to prepare the checkpoints.\n\n- `ImageBind`\nis the unified image/video/audio encoder. The pre-trained checkpoint can be downloaded from [here](https://dl.fbaipublicfiles.com/imagebind/imagebind_huge.pth) with version `huge`. Afterward, put the `imagebind_huge.pth` file at [[.pretrain_ckpt/imagebind]](./pretrain_ckpt/imagebind). \n- `Vicuna`:\nprepare the pretrained vicuna from [[here]](https://huggingface.co/lmsys/vicuna-7b-v1.5). Then put the pre-trained model at [[./pretrain_ckpt/vicuna-7b-v1.5/]](./pretrain_ckpt/vicuna-7b-v1.5). \n- `Image Diffusion`\nis used to generate images. NExT-GPT uses [Stable Diffusion](https://huggingface.co/stabilityai/stable-diffusion-2) with version `\nv2`. (_will be automatically downloaded_)\n- `Audio Diffusion`\nfor producing audio content. NExT-GPT employs [AudioLDM](https://github.com/haoheliu/AudioLDM) with version `l-full`. (_will be automatically downloaded_)\n- `Video Diffusion`\nfor the video generation. We employ [ZeroScope](https://huggingface.co/cerspense/zeroscope_v2_576w) with version `v2_576w`. (_will be automatically downloaded_)\n\n\n\n<span id='Prepare Dataset'/>\n\n#### 3.2. Preparing Dataset  <a href='#all_catelogue'>[Back to Top]</a>\nPlease download the following datasets used for model training:\n\nA) T-X pairs data\n  - `CC3M` of ***text-image*** pairs, please follow this instruction [[here]](./data/T-X_pair_data/cc3m/prepare.md). Then put the data at [[./data/T-X_pair_data/cc3m]](./data/T-X_pair_data/cc3m).\n  - `WebVid` of ***text-video*** pairs, see the [[instruction]](./data/T-X_pair_data/webvid/prepare.md). The file should be saved at [[./data/T-X_pair_data/webvid]](./data/T-X_pair_data/webvid).\n  - `AudioCap` of ***text-audio*** pairs, see the [[instruction]](./data/T-X_pair_data/audiocap/prepare.md). Save the data in [[./data/T-X_pair_data/audiocap]](./data/T-X_pair_data/audiocap).\n\nB) Instruction data\n  - T+X-T\n    - `LLaVA` of the ***visual instruction data***, download it from [here](https://github.com/haotian-liu/LLaVA/blob/main/docs/Data.md), and then put it at [[./data/IT_data/T+X-T_data/llava]](./data/IT_data/T+X-T_data/llava/).\n    - `Alpaca` of the ***textual instruction data***, download it from [here](https://github.com/tatsu-lab/stanford_alpaca), and then put it at [[./data/IT_data/T+X-T_data/alpaca/]](data/IT_data/T+X-T_data/alpaca/).\n    - `VideoChat`, download the ***video instruction data*** [here](https://github.com/OpenGVLab/InternVideo/tree/main/Data/instruction_data), and then put it at [[./data/IT_data/T+X-T_data/videochat/]](data/IT_data/T+X-T_data/videochat/).\n    \n    Side noteÔºöAfter downloading dataset, please run `prepare_data.py` to preprocess the dataset.\n  - T-X+T (T2M)\n    - The `T-X+T` instruction datasets (T2M) are saved at [[./data/IT_data/T-T+X_data]](./data/IT_data/T-T+X_data).\n   \n  - MosIT\n    - Download the file from [here](), put them in [[./data/IT_data/MosIT_data/]](./data/IT_data/MosIT_data/). (_We are in the process of finalizing the data and handling the copyright issue._) \n\n\n<span id='Precompute Embeddings'/>\n\n#### 3.3. Precomputing Embeddings <a href='#all_catelogue'>[Back to Top]</a>\nIn decoding-side alignment training, we minimize the distance between the representation of signal tokens and captions. \nTo save costs of time and memory, we precompute the text embeddings for image, audio and video captions using the text encoder within the respective diffusion models.  \n\nPlease run this command before the following training of NExT-GPT, where the produced `embedding` file will be saved at [[./data/embed]](./data/embed).\n```angular2html\ncd ./code/\npython preprocess_embeddings.py ../data/T-X_pair_data/cc3m/cc3m_generation.json image ../data/embed/ stabilityai/stable-diffusion-2\n```\n\nNote of arguments:\n- args[1]: path of caption file;\n- args[2]: modality, which can be `image`, `video`, and `audio`;\n- args[3]: saving path of embedding file;\n- args[4]: corresponding pre-trained diffusion model name.\n\n\n\n<span id='Train NExT-GPT'/>\n\n#### 3.4. Training NExT-GPT  <a href='#all_catelogue'>[Back to Top]</a>\n\nFirst of all, please refer to the base configuration file [[training_utils.py]](training_utils.py) for the basic system setting of overall modules, and dataset configuration [nextgpt/dataset/catalog.py](nextgpt/dataset/catalog.py).\nThe whole NExT-GPT training involves 3 steps:\n\n- **Step-1**: Encoding-side LLM-centric Multimodal Alignment. This stage trains the ***input projection layer*** while freezing the ImageBind, LLM, output projection layer.\n  ```angular2html\n  # Encoding-side LLM-centric Multimodal Alignment\n  bash scripts/pretrain_enc.sh\n  ```\n\n\n\n- **Step-2**: Decoding-side Instruction-following Alignment. This stage trains the ***output projection layers*** while freezing the ImageBind, LLM, input projection layers.\n  ```angular2html\n  # Encoding-side LLM-centric Multimodal Alignment\n  bash scripts/pretrain_enc.sh\n  ```\n\n\n\n\n\n- **Step-3**: Instruction Tuning. This stage instruction-tune 1) the ***LLM*** via LoRA, 2) ***input projection layer*** and 3) ***output projection layer*** on the instruction dataset.\n  ```angular2html\n  # Encoding-side LLM-centric Multimodal Alignment\n  bash scripts/pretrain_enc.sh\n  ```\n\n\n\n\n<span id='Run NExT-GPT System'/>\n\n## 4. Running NExT-GPT System <a href='#all_catelogue'>[Back to Top]</a>\n\n\n<span id='Prepare checkpoints'/>\n\n\n#### 4.1. Preparing Checkpoints\n\nFirst, loading the pre-trained NExT-GPT system.\n- **Step-1**: load `Frozen parameters`. Please refer to <a href='#Prepare Pre-trained Checkpoint'>3.1 Preparing Pre-trained Checkpoint</a>.\n\n- **Step-2**: load `Tunable parameters`. Please put the NExT-GPT system at [./checkpoints/nextgpt-v1.5-7b](./checkpoints/nextgpt-v1.5-7b). You may either 1) use the params trained yourselves, or 2) download our checkpoints from [Huggingface](). \n\n\n#### 4.2. Run the Prediction\nUpon completion of the checkpoint loading, you can run the prediction via:\n```angular2html\npython predict.py\n```\n\n---------\n\n\n<span id='Tuning your own system'/>\n\n## 5. Fine-tuning Your Own System <a href='#all_catelogue'>[Back to Top]</a>\n\n\n<span id='Tuning your own dataset'>\n\n#### 5.1. Dataset\nYou can define your own dataset, please refer to the [base_dataset.py](nextgpt/dataset/base_dataset.py), and then add the dataset `catalog` in [catalog.py]([text](nextgpt/dataset/catalog.py)), including the `target` and `parameters`.\n\n\n<span id='Tuning your own framework'>\n\n#### 5.2. Model Framework\n- *Multimodal Encoder*: You can leverage your own multimodal encoder in [multimodal encoder directory](nextgpt/model/multimodal_encoder), and add corresponding code in the [builder.py](nextgpt/model/multimodal_encoder/builder.py).\n- *Multimodal Decoder*: You can add your own multimodal decoder, in  [multimodal decoder directory](nextgpt/model/multimodal_decoder), and modify the corresponding code in the [builder.py](nextgpt/model/multimodal_decoder/builder.py).\n- *Projector*: You can design your own input and output projector in [multimodal projector](nextgpt/model/multimodal_projector/builder.py).  \n\n\n<span id='Tuning script'>\n\n#### 5.3. Fine-tuning\n\nYou can pre-define the model, data, and training parameters in [training_utils.py](training_utils.py).\nPlease refer the [finetune.sh](scripts/finetune.sh) for fine-tuning your own model.\n\n\n\n---------\n\n\n\n## Contact\n\nFor any questions or feedback, feel free to contact [Shengqiong Wu](mailto:swu@u.nus.edu) and [Hao Fei](mailto:haofei37@nus.edu.sg).\n\n\n## Citation\n\nIf you find NextGPT useful in your research or applications, please kindly cite:\n```\n@inproceedings{wu24next,\n  title={{NE}x{T}-{GPT}: Any-to-Any Multimodal {LLM}},\n  author={Wu, Shengqiong and Fei, Hao and Qu, Leigang and Ji, Wei and Chua, Tat-Seng},\n  booktitle={Proceedings of the International Conference on Machine Learning},\n  pages = {53366--53397},\n  year={2024}\n}\n```\n\n\n\n\n\n## Acknowledgements\nYou may refer to related work that serves as foundations for our framework and code repository, \n[Vicuna](https://github.com/lm-sys/FastChat), \n[ImageBind](https://github.com/facebookresearch/ImageBind), \n[Stable Diffusion](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/text2img), \n[AudioLDM](https://github.com/haoheliu/AudioLDM), and\n[Zeroscope](https://huggingface.co/cerspense/zeroscope_v2_576w).\nWe also partially draw inspirations from \n[PandaGPT](https://github.com/yxuansu/PandaGPT),  \n[GILL](https://github.com/kohjingyu/gill/), \n[CoDi](https://codi-gen.github.io/),\n[Video-LLaMA](https://github.com/DAMO-NLP-SG/Video-LLaMA),\n[LLaVA](https://github.com/haotian-liu/LLaVA),\nand [MiniGPT-4](https://github.com/Vision-CAIR/MiniGPT-4).\nThanks for their wonderful works.\n\n\n\n\n## License Notices\nThis repository is under [BSD 3-Clause License](LICENSE.txt).\nNExT-GPT is a research project intended for non-commercial use only. \nOne must NOT use the code of NExT-GPT for any illegal, harmful, violent, racist, or sexual purposes. \nOne is strictly prohibited from engaging in any activity that will potentially violate these guidelines.\nAny potential commercial use of this code should be approved by the authors.\n"
        },
        {
          "name": "assets",
          "type": "tree",
          "content": null
        },
        {
          "name": "data",
          "type": "tree",
          "content": null
        },
        {
          "name": "figures",
          "type": "tree",
          "content": null
        },
        {
          "name": "merge_lora_weights.py",
          "type": "blob",
          "size": 0.9287109375,
          "content": "import argparse\nfrom nextgpt.model.builder import load_pretrained_model\nfrom nextgpt.mm_utils import get_model_name_from_path\n\n\ndef merge_lora(args):\n    # model_name = get_model_name_from_path(args.model_path) \n    model_name = 'nextgpt-v1.5-7b-lora'\n    tokenizer, model, image_processor, video_processor, audio_processor, context_len, model_config = load_pretrained_model(args.model_path, args.model_base, model_name, device_map='cpu')\n\n    model.save_pretrained(args.save_model_path)\n    tokenizer.save_pretrained(args.save_model_path)\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--model-path\", type=str, default='./checkpoints/finetune_1')\n    parser.add_argument(\"--model-base\", type=str, default='./checkpoints/pretrain_dec_1/checkpoint-4')\n    parser.add_argument(\"--save-model-path\", type=str, default='./checkpoints/nextgpt-v1.5-7b-lora')\n\n    args = parser.parse_args()\n\n    merge_lora(args)"
        },
        {
          "name": "nextgpt",
          "type": "tree",
          "content": null
        },
        {
          "name": "nextgpt_trainer.py",
          "type": "blob",
          "size": 13.353515625,
          "content": "# Copyright (c) 2024 PaddlePaddle Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\nimport inspect\nimport os \nfrom typing import List, Optional\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Sampler, RandomSampler\nfrom torch.utils.data import DataLoader, Dataset\nfrom transformers.utils import is_datasets_available\nfrom transformers.trainer_utils import seed_worker\nfrom transformers import Trainer\nfrom transformers.trainer import (\n    is_sagemaker_mp_enabled,\n    get_parameter_names,\n    has_length,\n    ALL_LAYERNORM_LAYERS,\n    logger,\n)\nimport datasets\nfrom typing import List, Optional\n\nfrom nextgpt.dataset.sampler import DistributedMultiDatasetBatchSampler\n\n\ndef maybe_zero_3(param, ignore_status=False, name=None):\n    from deepspeed import zero\n    from deepspeed.runtime.zero.partition_parameters import ZeroParamStatus\n    if hasattr(param, \"ds_id\"):\n        if param.ds_status == ZeroParamStatus.NOT_AVAILABLE:\n            if not ignore_status:\n                print(name, 'no ignore status')\n        with zero.GatheredParameters([param]):\n            param = param.data.detach().cpu().clone()\n    else:\n        param = param.detach().cpu().clone()\n    return param\n\n\ndef get_mm_adapter_state_maybe_zero_3(named_params, keys_to_match):\n    to_return = {k: t for k, t in named_params if any(key_match in k for key_match in keys_to_match)}\n    to_return = {k: maybe_zero_3(v, ignore_status=True, name=k).cpu() for k, v in to_return.items()}\n    return to_return\n\n\ndef split_to_even_chunks(indices, lengths, num_chunks):\n    \"\"\"\n    Split a list of indices into `chunks` chunks of roughly equal lengths.\n    \"\"\"\n    if len(indices) % num_chunks != 0:\n        return [indices[i::num_chunks] for i in range(num_chunks)]\n    num_indices_per_chunk = len(indices) // num_chunks\n    chunks = [[] for _ in range(num_chunks)]\n    chunks_lengths = [(0) for _ in range(num_chunks)]\n    for index in indices:\n        shortest_chunk = chunks_lengths.index(min(chunks_lengths))\n        chunks[shortest_chunk].append(index)\n        chunks_lengths[shortest_chunk] += lengths[index]\n        if len(chunks[shortest_chunk]) == num_indices_per_chunk:\n            chunks_lengths[shortest_chunk] = float(\"inf\")\n    return chunks\n\n\ndef get_modality_length_grouped_indices(lengths, batch_size, world_size, generator=None):\n    # We need to use torch for the random part as a distributed sampler will set the random seed for torch.\n    assert all(l != 0 for l in lengths), \"Should not have zero length.\"\n    if all(l > 0 for l in lengths) or all(l < 0 for l in lengths):\n        # all samples are in the same modality\n        return get_length_grouped_indices(lengths, batch_size, world_size, generator=generator)\n    mm_indices, mm_lengths = zip(*[(i, l) for i, l in enumerate(lengths) if l > 0])\n    lang_indices, lang_lengths = zip(*[(i, -l) for i, l in enumerate(lengths) if l < 0])\n\n    mm_shuffle = [mm_indices[i] for i in get_length_grouped_indices(mm_lengths, batch_size, world_size, generator=None)]\n    lang_shuffle = [lang_indices[i] for i in get_length_grouped_indices(lang_lengths, batch_size, world_size, generator=None)]\n    megabatch_size = world_size * batch_size\n    mm_megabatches = [mm_shuffle[i : i + megabatch_size] for i in range(0, len(mm_shuffle), megabatch_size)]\n    lang_megabatches = [lang_shuffle[i : i + megabatch_size] for i in range(0, len(lang_shuffle), megabatch_size)]\n\n    last_mm = mm_megabatches[-1]\n    last_lang = lang_megabatches[-1]\n    additional_batch = last_mm + last_lang\n    megabatches = mm_megabatches[:-1] + lang_megabatches[:-1]\n    megabatch_indices = torch.randperm(len(megabatches), generator=generator)\n    megabatches = [megabatches[i] for i in megabatch_indices]\n\n    if len(additional_batch) > 0:\n        megabatches.append(sorted(additional_batch))\n\n    return [i for megabatch in megabatches for i in megabatch]\n\n\ndef get_length_grouped_indices(lengths, batch_size, world_size, generator=None, merge=True):\n    # We need to use torch for the random part as a distributed sampler will set the random seed for torch.\n    indices = torch.randperm(len(lengths), generator=generator)\n    megabatch_size = world_size * batch_size\n    megabatches = [indices[i : i + megabatch_size].tolist() for i in range(0, len(lengths), megabatch_size)]\n    megabatches = [sorted(megabatch, key=lambda i: lengths[i], reverse=True) for megabatch in megabatches]\n    megabatches = [split_to_even_chunks(megabatch, lengths, world_size) for megabatch in megabatches]\n\n    return [i for megabatch in megabatches for batch in megabatch for i in batch]\n\n\nclass LengthGroupedSampler(Sampler):\n    \"\"\"\n    Sampler that samples indices in a way that groups together features of the dataset of roughly the same length while\n    keeping a bit of randomness.\n    \"\"\"\n\n    def __init__(\n        self,\n        batch_size: int,\n        world_size: int,\n        lengths: Optional[List[int]] = None,\n        generator=None,\n        group_by_modality: bool = False,\n    ):\n        if lengths is None:\n            raise ValueError(\"Lengths must be provided.\")\n        self.batch_size = batch_size\n        self.world_size = world_size\n        self.lengths = lengths\n        self.generator = generator\n        self.group_by_modality = group_by_modality\n\n    def __len__(self):\n        return len(self.lengths)\n\n    def __iter__(self):\n        if self.group_by_modality:\n            indices = get_modality_length_grouped_indices(\n                self.lengths, self.batch_size, self.world_size, generator=self.generator\n            )\n        else:\n            indices = get_length_grouped_indices(\n                self.lengths, self.batch_size, self.world_size, generator=self.generator\n            )\n        return iter(indices)\n\n\nclass NextGPTTrainer(Trainer):\n    def _get_train_sampler(self) -> Optional[torch.utils.data.Sampler]:\n        if self.train_dataset is None or not has_length(self.train_dataset):\n            return None\n\n        if self.args.group_by_modality_length:\n            lengths = self.train_dataset.modality_lengths\n            # print(lengths)\n            return LengthGroupedSampler(\n                self.args.train_batch_size,\n                world_size=self.args.world_size * self.args.gradient_accumulation_steps,\n                lengths=lengths,\n                group_by_modality=True,\n            )\n        elif self.args.group_by_modality_type:\n            sampler = torch.utils.data.RandomSampler(self.train_dataset)\n            rank = torch.distributed.get_rank()\n            return DistributedMultiDatasetBatchSampler(batch_size=self.args.train_batch_size * self.args.world_size, \n                                                       sampler=sampler,\n                                                       dataset=self.train_dataset,\n                                                       rank=rank,\n                                                       drop_last=True,\n                                                       world_size=self.args.world_size * self.args.gradient_accumulation_steps,\n                                                       )\n        else:\n            return super()._get_train_sampler()\n\n    def create_optimizer(self):\n        \"\"\"\n        Setup the optimizer.\n\n        We provide a reasonable default that works well. If you want to use something else, you can pass a tuple in the\n        Trainer's init through `optimizers`, or subclass and override this method in a subclass.\n        \"\"\"\n        if is_sagemaker_mp_enabled():\n            return super().create_optimizer()\n        \n        opt_model = self.model\n\n        if self.optimizer is None:\n            decay_parameters = get_parameter_names(opt_model, ALL_LAYERNORM_LAYERS)\n            decay_parameters = [name for name in decay_parameters if \"bias\" not in name]\n            if self.args.mm_input_projector_lr is not None:\n                projector_parameters = [name for name, _ in opt_model.named_parameters() if \"mm_input_projector\" in name]\n                if self.args.mm_output_projector_lr is not None:\n                    projector_parameters.extend([name for name, _ in opt_model.named_parameters() if (\"mm_output_img_projector\" in name or \"mm_output_vid_projector\" in name or \"mm_output_aud_projector\" in name)])\n                optimizer_grouped_parameters = [\n                    {\n                        \"params\": [\n                            p for n, p in opt_model.named_parameters() if (n in decay_parameters and n not in projector_parameters and p.requires_grad)\n                        ],\n                        \"weight_decay\": self.args.weight_decay,\n                    },\n                    {\n                        \"params\": [\n                            p for n, p in opt_model.named_parameters() if (n not in decay_parameters and n not in projector_parameters and p.requires_grad)\n                        ],\n                        \"weight_decay\": 0.0,\n                    },\n                    {\n                        \"params\": [\n                            p for n, p in opt_model.named_parameters() if (n in decay_parameters and n in projector_parameters and p.requires_grad)\n                        ],\n                        \"weight_decay\": self.args.weight_decay,\n                        \"lr\": self.args.mm_input_projector_lr,\n                    },\n                    {\n                        \"params\": [\n                            p for n, p in opt_model.named_parameters() if (n not in decay_parameters and n in projector_parameters and p.requires_grad)\n                        ],\n                        \"weight_decay\": 0.0,\n                        \"lr\": self.args.mm_input_projector_lr,\n                    },\n                ]\n            else:\n                optimizer_grouped_parameters = [\n                    {\n                        \"params\": [\n                            p for n, p in opt_model.named_parameters() if (n in decay_parameters and p.requires_grad)\n                        ],\n                        \"weight_decay\": self.args.weight_decay,\n                    },\n                    {\n                        \"params\": [\n                            p for n, p in opt_model.named_parameters() if (n not in decay_parameters and p.requires_grad)\n                        ],\n                        \"weight_decay\": 0.0,\n                    },\n                ]\n            \n            optimizer_cls, optimizer_kwargs = Trainer.get_optimizer_cls_and_kwargs(self.args)\n\n            self.optimizer = optimizer_cls(optimizer_grouped_parameters, **optimizer_kwargs)\n            if optimizer_cls.__name__ == \"Adam8bit\":\n                import bitsandbytes\n\n                manager = bitsandbytes.optim.GlobalOptimManager.get_instance()\n\n                skipped = 0\n                for module in opt_model.modules():\n                    if isinstance(module, nn.Embedding):\n                        skipped += sum({p.data_ptr(): p.numel() for p in module.parameters()}.values())\n                        logger.info(f\"skipped {module}: {skipped/2**20}M params\")\n                        manager.register_module_override(module, \"weight\", {\"optim_bits\": 32})\n                        logger.debug(f\"bitsandbytes: will optimize {module} in fp32\")\n                logger.info(f\"skipped: {skipped/2**20}M params\")\n\n        return self.optimizer\n\n    def _save_checkpoint(self, model, trial, metrics=None):\n        save_flag = False\n\n        def save_adapter(keys_to_match, filename):\n            from transformers.trainer_utils import PREFIX_CHECKPOINT_DIR\n            checkpoint_folder = f\"{PREFIX_CHECKPOINT_DIR}-{self.state.global_step}\"\n            run_dir = self._get_output_dir(trial=trial)\n            output_dir = os.path.join(run_dir, checkpoint_folder)\n\n            weight_to_save = get_mm_adapter_state_maybe_zero_3(self.model.named_parameters(), keys_to_match)\n\n            if self.args.local_rank in [0, -1]:\n                self.model.config.save_pretrained(output_dir)\n                torch.save(weight_to_save, os.path.join(output_dir, filename))\n            return True\n        if getattr(self.args, 'tune_mm_input_adapter', False):\n            print(\"Saving adapter in llava trainer in llava_trainer _save_checkpoint tune_mm_input_adapter ...\")\n            print(getattr(self.args, 'tune_mm_input_adapter', False))\n            keys_to_match = ['mm_input_projector', 'vision_resampler', 'embed_tokens', 'embed_in']\n            save_flag = save_adapter(keys_to_match, 'mm_input_projector.bin')\n\n        if not save_flag:\n            super(NextGPTTrainer, self)._save_checkpoint(model, trial, metrics)\n\n    def _save(self, output_dir: Optional[str] = None, state_dict=None):\n        # if getattr(self.args, 'tune_mm_input_adapter', False) or any(getattr(self.args, f'tune_mm_output_{mod}_adapter', False) for mod in ['img', 'vid', 'aud']):\n        print(\"Saving model in llava trainer, _save ...\")\n        # print(getattr(self.args, 'tune_mm_input_adapter', False))\n        if getattr(self.args, 'tune_mm_input_adapter', False):\n            pass\n        else:\n            super(NextGPTTrainer, self)._save(output_dir, state_dict)"
        },
        {
          "name": "predict.py",
          "type": "blob",
          "size": 10.623046875,
          "content": "import torch\n\nfrom nextgpt.constants import IMAGE_TOKEN_INDEX, DEFAULT_IMAGE_TOKEN\nfrom nextgpt.conversation import conv_templates, SeparatorStyle\nfrom nextgpt.model.builder import load_pretrained_model\nfrom nextgpt.utils import disable_torch_init\nfrom nextgpt.mm_utils import tokenizer_image_token, tokenizer_multiple_token\nfrom transformers.generation.streamers import TextIteratorStreamer\nimport transformers\nfrom dataclasses import dataclass, field\nfrom PIL import Image\nfrom transformers import StoppingCriteria, StoppingCriteriaList\nfrom typing import List\n\nimport requests\nfrom io import BytesIO\nimport scipy\nfrom cog import BasePredictor, Input, Path, ConcatenateIterator\nimport time\nimport subprocess\nfrom threading import Thread\nfrom diffusers.utils import export_to_video\nimport os\nos.environ[\"HUGGINGFACE_HUB_CACHE\"] = os.getcwd() + \"/weights\"\n\n# url for the weights mirror\nREPLICATE_WEIGHTS_URL = None\n\n\n@dataclass\nclass GenerateArguments:\n    # Basic generation arguments\n    top_k: int = field(default=1, metadata={\"help\": \"The number of highest probability tokens to keep for top-k-filtering in the sampling strategy\"})\n    top_p: float = field(default=1.0, metadata={\"help\": \"The cumulative probability for top-p-filtering in the sampling strategy.\"})\n    temperature: float = field(default=1.0, metadata={\"help\": \"The value used to module the next token probabilities. Must be strictly positive.\"},)\n    max_new_tokens: int = field(default=100, metadata={\"help\": \"The maximum number of new tokens to generate. The generation process will stop when reaching this threshold.\"})\n    do_sample: bool = field(default=True, metadata={\"help\": \"Whether to sample from the output distribution to generate new tokens. If False, use argmax.\"})\n    use_cache: bool = field(default=False, metadata={\"help\": \"Whether to cache the hidden states of the model to speed up generation.\"})\n    output_hidden_states: bool = field(default=True,metadata={\"help\": \"Whether to return the hidden states of all intermediate layers.\"})\n    \n    # Image inference arguments\n    guidance_scale_for_img: float = field(default=7.5, metadata={\"help\": \"The scale for the guidance loss of image signal.\"})\n    num_inference_steps_for_img: int = field(default=50, metadata={\"help\": \"The number of inference steps for image signal.\"})\n\n    # Video inference arguments\n    guidance_scale_for_vid: float = field(default=7.5, metadata={\"help\": \"The scale for the guidance loss of video signal.\"})\n    num_inference_steps_for_vid: int = field(default=50, metadata={\"help\": \"The number of inference steps for video signal.\"})\n    height: int = field(default=320, metadata={\"help\": \"The height of the video frame.\"})\n    width: int = field(default=576, metadata={\"help\": \"The width of the video frame.\"})\n    num_frames: int = field(default=16, metadata={\"help\": \"The number of frames in the video.\"})\n\n    # Audio inference arguments\n    guidance_scale_for_aud: float = field(default=7.5, metadata={\"help\": \"The scale for the guidance loss of audio signal.\"})\n    num_inference_steps_for_aud: int = field(default=50, metadata={\"help\": \"The number of inference steps for audio signal.\"})\n    audio_length_in_s: float = field(default=5.0, metadata={\"help\": \"The length of the audio signal in seconds.\"})\n\n\n\nclass StoppingCriteriaSub(StoppingCriteria):\n\n    def __init__(self, stops: List = None, encounters: int = 1):\n        super().__init__()\n        self.stops = stops\n        self.ENCOUNTERS = encounters\n\n    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor):\n        stop_count = 0\n        for stop in self.stops:\n            _stop = torch.tensor(stop).to(input_ids[0].device)\n            indices = torch.where(_stop[0] == input_ids)\n            for i in indices:\n                if len(i) > 0:\n                    if torch.all(input_ids[0][i:i + len(_stop)] == _stop):\n                        stop_count += 1\n        if stop_count >= self.ENCOUNTERS:\n            return True\n        return False\n\n\ndef download_json(url: str, dest: Path):\n    res = requests.get(url, allow_redirects=True)\n    if res.status_code == 200 and res.content:\n        with dest.open(\"wb\") as f:\n            f.write(res.content)\n    else:\n        print(f\"Failed to download {url}. Status code: {res.status_code}\")\n\ndef download_weights(baseurl: str, basedest: str, files: list[str]):\n    basedest = Path(basedest)\n    start = time.time()\n    print(\"downloading to: \", basedest)\n    basedest.mkdir(parents=True, exist_ok=True)\n    for f in files:\n        dest = basedest / f\n        url = os.path.join(REPLICATE_WEIGHTS_URL, baseurl, f)\n        if not dest.exists():\n            print(\"downloading url: \", url)\n            if dest.suffix == \".json\":\n                download_json(url, dest)\n            else:\n                 subprocess.check_call([\"pget\", url, str(dest)], close_fds=False)\n    print(\"downloading took: \", time.time() - start)\n\n\nclass Predictor(BasePredictor):\n    def setup(self, model_base, model_name, model_path, load_8bit=False, load_4bit=False) -> None:\n        \"\"\"Load the model into memory to make running multiple predictions efficient\"\"\"\n        disable_torch_init()\n\n        # ./pretrain_ckpt/vicuna-7b-v1.5\n        self.tokenizer, self.model, self.image_processor, self.video_processor, self.audio_processor, self.context_len, self.model_config = load_pretrained_model(model_base, model_name, model_path, load_8bit=load_8bit, load_4bit=load_4bit) \n                                    \n    def predict(\n        self,\n        image: str = None,\n        prompt: str = None,\n        top_p: float = 1.0,\n        temperature: float = 0.2,\n        max_new_tokens: int = 512,\n    ):\n        \"\"\"Run a single prediction on the model\"\"\"\n\n        # prepare generation arguments\n        parser = transformers.HfArgumentParser(GenerateArguments)\n        generation_args = parser.parse_args_into_dataclasses()[0]\n\n        stopping_criteria = StoppingCriteriaList([StoppingCriteriaSub(stops=[[835]], encounters=1)])\n\n        generation_args.top_p = top_p if top_p is not None else generation_args.top_p\n        generation_args.temperature = temperature if temperature is not None else generation_args.temperature\n        generation_args.max_new_tokens = max_new_tokens if max_new_tokens is not None else generation_args.max_new_tokens\n        generation_args.stopping_criteria = stopping_criteria\n\n        conv_mode = \"llava_v1\"\n        conv = conv_templates[conv_mode].copy()\n    \n        image_data = load_image(str(image))\n        image_tensor = self.image_processor.preprocess(image_data, return_tensors='pt')['pixel_values'].half().cuda()\n    \n        # loop start\n    \n        # just one turn, always prepend image token\n        # inp = DEFAULT_IMAGE_TOKEN + '\\n' + prompt  # prepend image token when need to understanding images content\n        inp = prompt  # no need to prepend image token when generting images\n        conv.append_message(conv.roles[0], inp)\n\n        conv.append_message(conv.roles[1], None)\n        _prompt = conv.get_prompt()\n        print(\"prompt: \", _prompt)\n        input_ids = tokenizer_multiple_token(_prompt, self.tokenizer, IMAGE_TOKEN_INDEX, return_tensors='pt').unsqueeze(0).cuda()\n        print(\"input_ids: \", input_ids)\n        stop_str = conv.sep if conv.sep_style != SeparatorStyle.TWO else conv.sep2\n        keywords = [stop_str]\n        image_signal_token_indices = [self.tokenizer(f\"<image_{i:02d}>\").input_ids for i in range(self.model_config.n_img_tokens)]\n        video_signal_token_indices = [self.tokenizer(f\"<video_{i:02d}>\").input_ids for i in range(self.model_config.n_vid_tokens)]\n        audio_signal_token_indices = [self.tokenizer(f\"<audio_{i:02d}>\").input_ids for i in range(self.model_config.n_aud_tokens)]\n        print(\"image_signal_token_indices: \", image_signal_token_indices)\n        print(\"video_signal_token_indices: \", video_signal_token_indices)\n        print(\"audio_signal_token_indices: \", audio_signal_token_indices)\n        with torch.inference_mode():\n            output = self.model.generate(\n                input_ids=input_ids,\n                # images=image_tensor,\n                image_signal_token_indices=image_signal_token_indices,\n                video_signal_token_indices=video_signal_token_indices,\n                audio_signal_token_indices=audio_signal_token_indices,\n                **generation_args.__dict__\n            )\n            print(\"output: \", output)\n            print(\"output shape: \", self.tokenizer.batch_decode(output['sequences'], skip_special_tokens=False)[0])\n            for k in output.keys():\n                print(k)\n                if 'images' == k:\n                    for m in output['images']:\n                        if isinstance(m, torch.Tensor):\n                            print(m)\n                        else:\n                            if not os.path.exists('./assets/images'):\n                                os.mkdir('./assets/images')\n                            m[0].save(f'./assets/images/{prompt}.jpg')\n\n                elif 'videos' == k:\n                    for idx, m in enumerate(output['videos']):\n                        if isinstance(m, torch.Tensor):\n                            print(m)\n                        else:\n                            if not os.path.exists('./assets/videos'):\n                                os.mkdir('./assets/videos')\n                            video_path = export_to_video(video_frames=m, output_video_path=f'./assets/videos/{prompt}.mp4')\n                            print(\"video path: \", video_path)\n                elif 'audios' == k:\n                    for idx, m in enumerate(output['audios']):\n                        if isinstance(m, torch.Tensor):\n                            print(m)\n                        else:\n                            audio_path = f'./assets/audios/{prompt}.wav'\n                            if not os.path.exists('./assets/audios'):\n                                os.mkdir('./assets/audios')\n                            scipy.io.wavfile.write(audio_path, rate=16000, data=m)\n                            print(\"audio path: \", audio_path)\n                else:\n                    pass\n    \n\ndef load_image(image_file):\n    if image_file.startswith('http') or image_file.startswith('https'):\n        response = requests.get(image_file)\n        image = Image.open(BytesIO(response.content)).convert('RGB')\n    else:\n        image = Image.open(image_file).convert('RGB')\n    return image\n\n\nif __name__ == \"__main__\":\n    predictor = Predictor()\n    predictor.setup(model_base=None, model_name=\"nextgpt-v1.5-7b\", model_path=\"./checkpoints/nextgpt-v1.5-7b\", load_8bit=False, load_4bit=False)\n    # show me a beautiful landscape of \n    # descibe the bird in the image\n    predictor.predict(image=\"./assets/bird_image.jpg\", prompt=\"show me an image of a cute dog running on the grass\")\n\n"
        },
        {
          "name": "preprocess_embeddings.py",
          "type": "blob",
          "size": 9.0849609375,
          "content": "\nimport numpy as np\nimport os\nimport sys\nfrom joblib import Parallel, delayed\nfrom tqdm import tqdm\nimport torch\nimport json\nimport pandas as pd\nimport argparse\n\n# Load a slightly modified version of the Stable Diffusion pipeline.\n# This allows us to extract text embeddings directly (without generating images).\nfrom nextgpt.model.multimodal_decoder.custom_sd import StableDiffusionPipeline\nfrom diffusers import EulerDiscreteScheduler\nfrom nextgpt.model.multimodal_decoder.custom_vd import TextToVideoSDPipeline\nfrom nextgpt.model.multimodal_decoder.custom_ad import AudioLDMPipeline\n\n\n\ndef save_to_path(emb, path):\n    \"\"\"Save embeddings to disk.\"\"\"\n    try:\n        with open(path, 'wb') as wf:\n            np.save(wf, emb)\n    except:\n        print(\"Error with\", path)\n    return path\n\n\nif __name__ == '__main__':\n\n    batch_size = 128\n\n    dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n    clip_output_dir = './data/embed/'\n\n    # video_path = '../data/T-X_pair_data/webvid/webvid.json'\n    audio_path = './data/T_X_pair_data/audiocap/audiocap.json'\n    # img_path = '../data/T-X_pair_data/cc3m/cc3m.json'\n\n    # image_generation_ckpt_path = 'runwayml/stable-diffusion-v1-5'  #  stabilityai/stable-diffusion-2\n    # video_generation_ckpt_path = 'cerspense/zeroscope_v2_XL'\n    audio_generation_ckpt_path = 'cvssp/audioldm'\n\n    data_path = sys.argv[1]\n    modality = sys.argv[2]\n    clip_output_dir = sys.argv[3]\n    ckpt_path = sys.argv[4]\n\n    if not os.path.exists(clip_output_dir):\n        os.makedirs(clip_output_dir, exist_ok=True)\n\n    # Get existing files, so that we don't recompute them.\n    existing_files = set([f.strip('.npy') for f in os.listdir(clip_output_dir)])\n    print(\"found existing files:\", len(existing_files))\n\n    caption_list = []\n    name_list = []\n    if modality == 'audio':\n        print('extract audio caption embedding')\n        with open(data_path, 'r', encoding='utf-8') as f:\n            data = json.load(f)\n\n        for row in tqdm(data, total=len(data)):\n            # one_audio_name, one_caption = row[\"audio_name\"], row[\"caption\"]\n            # if one_audio_name not in existing_files:\n            #     caption_list.append(one_caption)\n            #     name_list.append(one_audio_name)\n            captions = row['audio_captions'] if isinstance(row['audio_captions'], list) else [row['audio_captions']]\n            caption_embs = row['audio_caption_embeddings'] if isinstance(row['audio_caption_embeddings'], list) else [row['audio_caption_embeddings']]\n            for cap, cap_emb in zip(captions, caption_embs):\n                if cap_emb.strip('.npy') not in existing_files:\n                    caption_list.append(cap)\n                    name_list.append(cap_emb.strip('.npy'))\n        pipe = AudioLDMPipeline.from_pretrained(ckpt_path, torch_dtype=dtype)\n        if not torch.cuda.is_available():\n            print('WARNING: using CPU, this will be slow!')\n        else:\n            pipe = pipe.to(\"cuda\")\n    elif modality == 'image':\n        print('extract image caption embedding')\n        with open(data_path, 'r', encoding='utf-8') as f:\n            data = json.load(f)\n        for row in tqdm(data, total=len(data)):\n            # one_image_name, one_caption = row[\"image_name\"], row[\"caption\"]\n            # one_image_name = one_image_name.split('/')[-1]\n            # if one_image_name not in existing_files:\n            #     caption_list.append(one_caption)\n            #     name_list.append(one_image_name)\n            captions = row['image_captions'] if isinstance(row['image_captions'], list) else [row['image_captions']]\n            caption_embs = row['image_caption_embeddings'] if isinstance(row['image_caption_embeddings'], list) else [row['image_caption_embeddings']]\n            for cap, cap_emb in zip(captions, caption_embs):\n                if cap_emb.strip('.npy') not in existing_files:\n                    caption_list.append(cap)\n                    name_list.append(cap_emb.strip('.npy'))\n        scheduler = EulerDiscreteScheduler.from_pretrained(ckpt_path, subfolder=\"scheduler\")\n        pipe = StableDiffusionPipeline.from_pretrained(ckpt_path, scheduler=scheduler, torch_dtype=dtype)\n        if not torch.cuda.is_available():\n            print('WARNING: using CPU, this will be slow!')\n        else:\n            pipe = pipe.to(\"cuda\")\n    elif modality == 'video':\n        print('extract video caption embedding')\n        with open(data_path, 'r', encoding='utf-8') as f:\n            data = json.load(f)\n        for row in tqdm(data, total=len(data)):\n            # one_video_name, one_caption = row[\"video_name\"], row[\"caption\"]\n            # if one_video_name not in existing_files:\n            #     caption_list.append(one_caption)\n            #     name_list.append(one_video_name)\n            captions = row['video_captions'] if isinstance(row['video_captions'], list) else [row['video_captions']]\n            caption_embs = row['video_caption_embeddings'] if isinstance(row['video_caption_embeddings'], list) else [row['video_caption_embeddings']]\n            for cap, cap_emb in zip(captions, caption_embs):\n                if cap_emb.strip('.npy') not in existing_files:\n                    caption_list.append(cap)\n                    name_list.append(cap_emb.strip('.npy'))\n        pipe = TextToVideoSDPipeline.from_pretrained(ckpt_path, torch_dtype=dtype)\n        if not torch.cuda.is_available():\n            print('WARNING: using CPU, this will be slow!')\n        else:\n            pipe = pipe.to(\"cuda\")\n    elif modality == 'mosit_audio':\n        with open(data_path, 'r', encoding='utf-8') as f:\n            data = json.load(f)\n        \n        for row in tqdm(data, total=len(data)):\n            if 'audio_captions' in row:\n                captions = row['audio_captions'] if isinstance(row['audio_captions'], list) else [row['audio_captions']]\n                caption_embs = row['audio_caption_embeddings'] if isinstance(row['audio_caption_embeddings'], list) else [row['audio_caption_embeddings']]\n                for cap, cap_emb in zip(captions, caption_embs):\n                    if cap_emb.strip('.npy') not in existing_files:\n                        caption_list.append(cap)\n                        name_list.append(cap_emb.strip('.npy'))\n        \n        pipe = AudioLDMPipeline.from_pretrained(ckpt_path, torch_dtype=dtype)\n        if not torch.cuda.is_available():\n            print('WARNING: using CPU, this will be slow!')\n        else:\n            pipe = pipe.to(\"cuda\")\n    elif modality == 'mosit_image':\n        with open(data_path, 'r', encoding='utf-8') as f:\n            data = json.load(f)\n        for row in tqdm(data, total=len(data)):\n            if 'image_captions' in row:\n                captions = row['image_captions'] if isinstance(row['image_captions'], list) else [row['image_captions']]\n                caption_embs = row['image_caption_embeddings'] if  isinstance(row['image_caption_embeddings'], list) else [row['image_caption_embeddings']]         \n                for cap, cap_emb in zip(captions, caption_embs):\n                    if cap_emb.strip('.npy') not in existing_files:\n                        caption_list.append(cap)\n                        name_list.append(cap_emb.strip('.npy'))   \n        scheduler = EulerDiscreteScheduler.from_pretrained(ckpt_path, subfolder=\"scheduler\")\n        pipe = StableDiffusionPipeline.from_pretrained(ckpt_path, scheduler=scheduler, torch_dtype=dtype)\n        if not torch.cuda.is_available():\n            print('WARNING: using CPU, this will be slow!')\n        else:\n            pipe = pipe.to(\"cuda\")\n\n    elif modality == 'mosit_video':\n        with open(data_path, 'r', encoding='utf-8') as f:\n            data = json.load(f)\n        for row in tqdm(data, total=len(data)):\n            if 'video_captions' in row:\n                captions = row['video_captions'] if isinstance(row['video_captions'], list) else [row['video_captions']]\n                caption_embs = row['video_caption_embeddings'] if isinstance(row['video_caption_embeddings'], list) else [row['video_caption_embeddings']]\n                for cap, cap_emb in zip(captions, caption_embs):\n                    if cap_emb.strip('.npy') not in existing_files:\n                        caption_list.append(cap)\n                        name_list.append(cap_emb.strip('.npy'))\n        pipe = TextToVideoSDPipeline.from_pretrained(ckpt_path, torch_dtype=dtype)\n        if not torch.cuda.is_available():\n            print('WARNING: using CPU, this will be slow!')\n        else:\n            pipe = pipe.to(\"cuda\")\n\n    print('Total number of captions:', len(caption_list))\n\n    print('Extract embeddings in batches.')\n    num_batches = int(np.ceil(len(caption_list) / batch_size))\n    for i in tqdm(range(num_batches)):\n        start_idx = i * batch_size\n        end_idx = start_idx + batch_size\n        batch_captions = caption_list[start_idx:end_idx]\n        batch_ids = name_list[start_idx:end_idx]\n        prompt_embeds = pipe(batch_captions, return_prompts_only=True).detach().cpu().numpy()\n\n        # Save embeddings to disk in parallel.\n        Parallel(n_jobs=8)(delayed(save_to_path)(\n            prompt_embeds[j, :, ...], os.path.join(clip_output_dir, f'{batch_ids[j]}.npy')\n        ) for j in range(prompt_embeds.shape[0]))"
        },
        {
          "name": "requirements.txt",
          "type": "blob",
          "size": 0.41015625,
          "content": "accelerate==0.33.0\ndatasets==2.21.0\ndecord==0.6.0\ndeepspeed==0.13.1\ndiffusers==0.30.0\neinops==0.8.0\nftfy==6.2.3\ngradio==3.44.0\nimageio==2.35.1\niopath==0.1.9\njoblib==1.3.1\nmatplotlib==3.9.2\nnumpy==1.26.3\npandas==2.2.2\npeft==0.12.0\nPillow==10.2.0\npytorchvideo==0.1.5\nPyYAML==6.0.2\nregex==2024.7.24\nscipy==1.14.2\ntimm==1.0.8\ntorch==2.1.2\ntorchaudio==2.1.2\ntorchvision==0.17.1+cu121\ntransformers==4.42.0\ntensorboard==2.17.1\n"
        },
        {
          "name": "scripts",
          "type": "tree",
          "content": null
        },
        {
          "name": "train.py",
          "type": "blob",
          "size": 25.4326171875,
          "content": "# Copyright (c) 2024 PaddlePaddle Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nimport os\nimport sys\nimport logging\nimport pathlib\nfrom typing import Dict, List, Union\n\nimport transformers\nimport tokenizers\nimport torch\nimport torch.nn as nn\nfrom nextgpt.model import *\nfrom nextgpt.dataset.base_dataset import LazySupervisedDataset, DataCollatorForSupervisedDataset\nfrom nextgpt.dataset.concat_dataset import MyConcatDataset\nfrom training_utils import ModelArguments, DataArguments, TrainingArguments\nfrom nextgpt import conversation as conversation_lib\nfrom nextgpt_trainer import NextGPTTrainer\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n\nlocal_rank = None\n\n\ndef rank0_print(*args):\n    if local_rank == 0:\n        print(*args)\n\n\nfrom packaging import version\nIS_TOKENIZER_GREATER_THAN_0_14 = version.parse(tokenizers.__version__) >= version.parse('0.14')\n\n\n\n\ndef maybe_zero_3(param, ignore_status=False, name=None):\n    from deepspeed import zero\n    from deepspeed.runtime.zero.partition_parameters import ZeroParamStatus\n    if hasattr(param, \"ds_id\"):\n        if param.ds_status == ZeroParamStatus.NOT_AVAILABLE:\n            if not ignore_status:\n                logging.warning(f\"{name}: param.ds_status != ZeroParamStatus.NOT_AVAILABLE: {param.ds_status}\")\n        with zero.GatheredParameters([param]):\n            param = param.data.detach().cpu().clone()\n    else:\n        param = param.detach().cpu().clone()\n    return param\n\n\n# Borrowed from peft.utils.get_peft_model_state_dict\ndef get_peft_state_maybe_zero_3(named_params, bias):\n    if bias == \"none\":\n        to_return = {k: t for k, t in named_params if \"lora_\" in k}\n    elif bias == \"all\":\n        to_return = {k: t for k, t in named_params if \"lora_\" in k or \"bias\" in k}\n    elif bias == \"lora_only\":\n        to_return = {}\n        maybe_lora_bias = {}\n        lora_bias_names = set()\n        for k, t in named_params:\n            if \"lora_\" in k:\n                to_return[k] = t\n                bias_name = k.split(\"lora_\")[0] + \"bias\"\n                lora_bias_names.add(bias_name)\n            elif \"bias\" in k:\n                maybe_lora_bias[k] = t\n        for k, t in maybe_lora_bias:\n            if bias_name in lora_bias_names:\n                to_return[bias_name] = t\n    else:\n        raise NotImplementedError\n    to_return = {k: maybe_zero_3(v, ignore_status=True) for k, v in to_return.items()}\n    return to_return\n\n\ndef get_peft_state_non_lora_maybe_zero_3(named_params, require_grad_only=True):\n    to_return = {k: t for k, t in named_params if \"lora_\" not in k}\n    if require_grad_only:\n        to_return = {k: t for k, t in to_return.items() if t.requires_grad}\n    to_return = {k: maybe_zero_3(v, ignore_status=True).cpu() for k, v in to_return.items()}\n    return to_return\n\n\ndef get_mm_adapter_state_maybe_zero_3(named_params, keys_to_match):\n    to_return = {k: t for k, t in named_params if any(key_match in k for key_match in keys_to_match)}\n    to_return = {k: maybe_zero_3(v, ignore_status=True).cpu() for k, v in to_return.items()}\n    return to_return\n\n\ndef find_all_linear_names(model):\n    cls = torch.nn.Linear\n    lora_module_names = set()\n    multimodal_keywords = ['mm_input_projector', 'mm_output_img_projector', 'mm_output_aud_projector', 'mm_output_vid_projector', 'multimodal_tower', 'vision_resampler']\n    for name, module in model.named_modules():\n        if any(mm_keyword in name for mm_keyword in multimodal_keywords):\n            continue\n        if isinstance(module, cls):\n            names = name.split('.')\n            lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n\n    if 'lm_head' in lora_module_names: # needed for 16-bit\n        lora_module_names.remove('lm_head')\n    return list(lora_module_names)\n\n\ndef safe_save_input_adapter_for_hf_trainer(trainer: transformers.Trainer,\n                                           output_dir: str):\n    # Only save Adapter\n    keys_to_match = ['mm_input_projector', 'embed_tokens', 'embed_in']\n    # if getattr(trainer.args, \"use_im_start_end\", False):\n    #     keys_to_match.extend(['embed_tokens', 'embed_in'])\n\n    weight_to_save = get_mm_adapter_state_maybe_zero_3(trainer.model.named_parameters(), keys_to_match)\n    trainer.model.config.save_pretrained(output_dir)\n\n    current_folder = output_dir.split('/')[-1]\n    parent_folder = os.path.dirname(output_dir)\n    if trainer.args.local_rank == 0 or trainer.args.local_rank == -1:\n        if current_folder.startswith('checkpoint-'):\n            mm_projector_folder = os.path.join(parent_folder, \"mm_input_projector\")\n            os.makedirs(mm_projector_folder, exist_ok=True)\n            torch.save(weight_to_save, os.path.join(mm_projector_folder, f'{current_folder}.bin'))\n        else:\n            torch.save(weight_to_save, os.path.join(output_dir, f'mm_input_projector.bin'))\n    return\n\n\ndef safe_save_output_adapter_for_hf_trainer(trainer: transformers.Trainer,\n                                            output_dir: str):\n    # Only save Adapter\n    keys_to_match = ['mm_output_img_projector', 'mm_output_vid_projector', 'mm_output_aud_projector', 'embed_tokens', 'embed_in']\n    trainer.model.config.save_pretrained(output_dir)\n    weight_to_save = get_mm_adapter_state_maybe_zero_3(trainer.model.named_parameters(), keys_to_match)\n    \n\n    current_folder = output_dir.split('/')[-1]\n    parent_folder = os.path.dirname(output_dir)\n    if trainer.args.local_rank == 0 or trainer.args.local_rank == -1:\n        if current_folder.startswith('checkpoint-'):\n            mm_projector_folder = os.path.join(parent_folder, \"mm_output_projector\")\n            os.makedirs(mm_projector_folder, exist_ok=True)\n            torch.save(weight_to_save, os.path.join(mm_projector_folder, f'{current_folder}.bin'))\n        else:\n            torch.save(weight_to_save, os.path.join(output_dir, f'mm_output_projector.bin'))\n    return\n\n\ndef save_adapter_for_hf_trainer(trainer: transformers.Trainer,\n                                output_dir: str, \n                                keys_to_match: List[str], \n                                adapter_name: str):\n    weight_to_save = get_mm_adapter_state_maybe_zero_3(trainer.model.named_parameters(), keys_to_match)\n    current_folder = output_dir.split('/')[-1]\n    parent_folder = os.path.dirname(output_dir)\n    if trainer.args.local_rank == 0 or trainer.args.local_rank == -1:\n        if current_folder.startswith('checkpoint-'):\n            mm_projector_folder = os.path.join(parent_folder, \"mm_output_projector\")\n            os.makedirs(mm_projector_folder, exist_ok=True)\n            torch.save(weight_to_save, os.path.join(mm_projector_folder, f'{current_folder}.bin'))\n        else:\n            torch.save(weight_to_save, os.path.join(output_dir, adapter_name))\n    return True\n    \n\ndef safe_save_model_for_hf_trainer(trainer: transformers.Trainer,\n                                   output_dir: str):\n    \"\"\"Collects the state dict and dump to disk.\"\"\"\n\n    trainer.model.config.save_pretrained(output_dir)\n    save_flag = False\n    if getattr(trainer.args, 'tune_mm_input_adapter', False):\n        keys_to_match = ['mm_input_projector', 'vision_resampler', 'embed_tokens', 'embed_in']\n        save_flag = save_adapter_for_hf_trainer(trainer, output_dir, keys_to_match, 'mm_input_projector.bin')\n    \n    if any(getattr(trainer.args, f'tune_mm_output_{mod}_adapter', False) for mod in ['img', 'vid', 'aud']):\n        keys_to_match = ['mm_output_img_projector', 'mm_output_vid_projector', 'mm_output_aud_projector', 'embed_tokens', 'embed_in']\n        save_flag = save_adapter_for_hf_trainer(trainer, output_dir, keys_to_match, 'mm_output_projector.bin')\n\n    if save_flag:\n        return\n    # trainer.tokenizer.save_pretrained(output_dir)\n    print(\"save model !!!!!\")\n    if trainer.deepspeed:\n        torch.cuda.synchronize()\n        new_output_dir = output_dir+'/model'\n        if not os.path.exists(new_output_dir):\n            os.makedirs(new_output_dir)\n        trainer.save_model(new_output_dir)\n        return\n\n    state_dict = trainer.model.state_dict()\n    if trainer.args.should_save:\n        cpu_state_dict = {\n            key: value.cpu()\n            for key, value in state_dict.items()\n        }\n        del state_dict\n        trainer._save(output_dir, state_dict=cpu_state_dict)  # noqa\n\n\ndef smart_tokenizer_and_embedding_resize(\n    special_tokens_dict: Dict,\n    tokenizer: transformers.PreTrainedTokenizer,\n    model: transformers.PreTrainedModel,\n):\n    \"\"\"Resize tokenizer and embedding.\n\n    Note: This is the unoptimized version that may make your embedding size not be divisible by 64.\n    \"\"\"\n    num_new_tokens = tokenizer.add_special_tokens(special_tokens_dict)\n    model.resize_token_embeddings(len(tokenizer))\n\n    if num_new_tokens > 0:\n        input_embeddings = model.get_input_embeddings().weight.data\n        output_embeddings = model.get_output_embeddings().weight.data\n\n        input_embeddings_avg = input_embeddings[:-num_new_tokens].mean(\n            dim=0, keepdim=True)\n        output_embeddings_avg = output_embeddings[:-num_new_tokens].mean(\n            dim=0, keepdim=True)\n\n        input_embeddings[-num_new_tokens:] = input_embeddings_avg\n        output_embeddings[-num_new_tokens:] = output_embeddings_avg\n\n\ndef make_supervised_data_module(tokenizer: transformers.PreTrainedTokenizer,\n                                data_args) -> Dict:\n    \"\"\"Make dataset and collator for supervised fine-tuning.\"\"\"\n    # train_dataset = LazySupervisedDataset(tokenizer=tokenizer,\n    #                             data_path=data_args.data_path,\n    #                             data_args=data_args)\n    print(\"Loading datasets...\")\n    train_dataset = MyConcatDataset(dataset_name_list=data_args.dataset_name_list,\n                                    tokenizer=tokenizer,\n                                    data_args=data_args)\n    data_collator = DataCollatorForSupervisedDataset(tokenizer=tokenizer)\n    return dict(train_dataset=train_dataset,\n                eval_dataset=None,\n                data_collator=data_collator)\n\n\ndef train(attn_implementation=None):\n    \n    global local_rank\n    torch.multiprocessing.set_start_method('spawn')# good solution !!!!\n\n    # Arguments\n    parser = transformers.HfArgumentParser((ModelArguments, DataArguments, TrainingArguments))\n\n    model_args, data_args, training_args = parser.parse_args_into_dataclasses()\n    local_rank = training_args.local_rank\n    compute_dtype = (torch.float16 if training_args.fp16 else (torch.bfloat16 if training_args.bf16 else torch.float32))\n\n    bnb_model_from_pretrained_args = {}\n    if training_args.bits in [4, 8]:\n        from transformers import BitsAndBytesConfig\n        bnb_model_from_pretrained_args.update(dict(\n            device_map={\"\": training_args.device},\n            load_in_4bit=training_args.bits == 4,\n            load_in_8bit=training_args.bits == 8,\n            quantization_config=BitsAndBytesConfig(\n                load_in_4bit=training_args.bits == 4,\n                load_in_8bit=training_args.bits == 8,\n                llm_int8_skip_modules=[\"mm_projector\"],\n                llm_int8_threshold=6.0,\n                llm_int8_has_fp16_weight=False,\n                bnb_4bit_compute_dtype=compute_dtype,\n                bnb_4bit_use_double_quant=training_args.double_quant,\n                bnb_4bit_quant_type=training_args.quant_type # {'fp4', 'nf4'}\n            )\n        ))\n\n    if model_args.multimodal_tower is not None:\n        if 'mpt' in model_args.model_name_or_path:\n            config = transformers.AutoConfig.from_pretrained(model_args.model_name_or_path, trust_remote_code=True)\n            config.attn_config['attn_impl'] = training_args.mpt_attn_impl\n            model = LlavaMptForCausalLM.from_pretrained(\n                model_args.model_name_or_path,\n                config=config,\n                cache_dir=training_args.cache_dir,\n                **bnb_model_from_pretrained_args\n            )\n        else:\n            model = NextGPTLlamaForCausalLM.from_pretrained(\n                model_args.model_name_or_path,\n                cache_dir=training_args.cache_dir,\n                attn_implementation=attn_implementation,\n                torch_dtype=(torch.bfloat16 if training_args.bf16 else None),\n                **bnb_model_from_pretrained_args\n            )\n    else:\n        model = transformers.LlamaForCausalLM.from_pretrained(\n            model_args.model_name_or_path,\n            cache_dir=training_args.cache_dir,\n            attn_implementation=attn_implementation,\n            torch_dtype=(torch.bfloat16 if training_args.bf16 else None),\n            **bnb_model_from_pretrained_args\n        )\n    model.config.use_cache = False\n\n\n    if model_args.freeze_backbone:\n        model.model.requires_grad_(False)\n\n    if training_args.bits in [4, 8]:\n        from peft import prepare_model_for_kbit_training\n        model.config.torch_dtype=(torch.float32 if training_args.fp16 else (torch.bfloat16 if training_args.bf16 else torch.float32))\n        model = prepare_model_for_kbit_training(model, use_gradient_checkpointing=training_args.gradient_checkpointing)\n\n    if training_args.gradient_checkpointing:\n        if hasattr(model, \"enable_input_require_grads\"):\n            model.enable_input_require_grads()\n        else:\n            def make_inputs_require_grad(module, input, output):\n                output.requires_grad_(True)\n            model.get_input_embeddings().register_forward_hook(make_inputs_require_grad)\n\n    if training_args.lora_enable:\n        from peft import LoraConfig, get_peft_model\n        lora_config = LoraConfig(\n            r=training_args.lora_r,\n            lora_alpha=training_args.lora_alpha,\n            target_modules=find_all_linear_names(model),\n            lora_dropout=training_args.lora_dropout,\n            bias=training_args.lora_bias,\n            task_type=\"CAUSAL_LM\",\n        )\n        if training_args.bits == 16:\n            if training_args.bf16:\n                model.to(torch.bfloat16)\n            if training_args.fp16:\n                model.to(torch.float16)\n        rank0_print(\"Adding LoRA adapters...\")\n        model = get_peft_model(model, lora_config)\n\n    if 'mpt' in model_args.model_name_or_path:\n        tokenizer = transformers.AutoTokenizer.from_pretrained(\n            model_args.model_name_or_path,\n            cache_dir=training_args.cache_dir,\n            model_max_length=training_args.model_max_length,\n            padding_side=\"right\"\n        )\n    else:\n        tokenizer = transformers.AutoTokenizer.from_pretrained(\n            model_args.model_name_or_path,\n            cache_dir=training_args.cache_dir,\n            model_max_length=training_args.model_max_length,\n            padding_side=\"right\",\n            use_fast=False,\n        )\n\n    if model_args.version == \"v0\":\n        if tokenizer.pad_token is None:\n            smart_tokenizer_and_embedding_resize(\n                special_tokens_dict=dict(pad_token=\"[PAD]\"),\n                tokenizer=tokenizer,\n                model=model,\n            )\n    elif model_args.version == \"v0.5\":\n        tokenizer.pad_token = tokenizer.unk_token\n    else:\n        tokenizer.pad_token = tokenizer.unk_token\n        if model_args.version in conversation_lib.conv_templates:\n            conversation_lib.default_conversation = conversation_lib.conv_templates[model_args.version]\n        else:\n            conversation_lib.default_conversation = conversation_lib.conv_templates[\"vicuna_v1\"]\n        print(f\"Building conversation: {conversation_lib.default_conversation}\")  # \n        print(f\"Building conversation.sep_style: {conversation_lib.default_conversation.sep_style}\")  # SeparatorStyle.TWO\n\n    if model_args.multimodal_tower is not None:\n        model.get_model().initialize_input_multimodal_modules(\n           model_args=model_args,\n           fsdp=training_args.fsdp\n        )\n\n        mulitmodal_tower = model.get_multimodal_tower()\n        mulitmodal_tower.to(dtype=torch.bfloat16 if training_args.bf16 else torch.float16, device=training_args.device)\n        print(mulitmodal_tower.image_processor)\n        print(mulitmodal_tower.video_processor)\n        print(mulitmodal_tower.audio_processor)\n        data_args.image_processor = mulitmodal_tower.image_processor\n        data_args.video_processor = mulitmodal_tower.video_processor\n        data_args.audio_processor = mulitmodal_tower.audio_processor\n        model.config.n_img_tokens = data_args.n_img_tokens = model_args.n_img_tokens\n        model.config.n_vid_tokens = data_args.n_vid_tokens = model_args.n_vid_tokens\n        model.config.n_aud_tokens = data_args.n_aud_tokens = model_args.n_aud_tokens\n        data_args.is_multimodal = True\n\n        model.config.tokenizer_model_max_length = tokenizer.model_max_length\n        \n        model.config.mm_use_img_start_end = data_args.mm_use_img_start_end = model_args.mm_use_img_start_end\n        model.config.mm_input_projector_lr = training_args.mm_input_projector_lr\n        training_args.use_img_start_end = model_args.mm_use_img_start_end\n        model.config.mm_use_img_patch_token = model_args.mm_use_img_patch_token\n\n        model.config.mm_use_vid_start_end = data_args.mm_use_vid_start_end = model_args.mm_use_vid_start_end\n        model.config.mm_use_vid_patch_token = model_args.mm_use_vid_patch_token\n        training_args.use_vid_start_end = model_args.mm_use_vid_start_end\n        \n        model.config.mm_use_aud_start_end = data_args.mm_use_aud_start_end = model_args.mm_use_aud_start_end\n        model.config.mm_use_aud_patch_token = model_args.mm_use_aud_patch_token\n        training_args.use_aud_start_end = model_args.mm_use_aud_start_end\n\n        model.config.mm_output_projector_lr = training_args.mm_output_projector_lr\n    \n    if model_args.image_decoder is not None and model_args.video_decoder is not None and model_args.audio_decoder is not None:\n\n        model.config.layer_idx = model_args.layer_idx   # setting Layer index to extract signal feature from LLM hidden states\n        # model.config.snr_loss = training_args.snr_loss\n        # model_args.has_img_gen_loss = training_args.has_img_gen_loss\n        # model_args.has_vid_gen_loss = training_args.has_vid_gen_loss\n        # model_args.has_aud_gen_loss = training_args.has_aud_gen_loss\n\n        model.get_model().initialize_output_multimodal_modules(\n            model_args=model_args\n        )\n\n        image_decoder = model.get_image_decoder()\n        image_decoder.to(dtype=torch.bfloat16 if training_args.bf16 else torch.float16, device=training_args.device)\n\n        video_decoder = model.get_video_decoder()\n        video_decoder.to(dtype=torch.bfloat16 if training_args.bf16 else torch.float16, device=training_args.device)\n\n        audio_decoder = model.get_audio_decoder()\n        audio_decoder.to(dtype=torch.bfloat16 if training_args.bf16 else torch.float16, device=training_args.device)\n\n        model.get_model().mm_output_img_projector.to(dtype=torch.bfloat16 if training_args.bf16 else torch.float16, device=training_args.device)\n        model.get_model().mm_output_vid_projector.to(dtype=torch.bfloat16 if training_args.bf16 else torch.float16, device=training_args.device)\n        model.get_model().mm_output_aud_projector.to(dtype=torch.bfloat16 if training_args.bf16 else torch.float16, device=training_args.device)\n    \n    model.requires_grad_(False)\n\n    # freeze/unfreeze the mm input adapters   \n    model.config.tune_mm_input_adapter = training_args.tune_mm_input_adapter = model_args.tune_mm_input_adapter\n    if model_args.tune_mm_input_adapter:\n        for p in model.get_model().mm_input_projector.parameters():\n            p.requires_grad = True\n    model.config.freeze_mm_input_adapter = training_args.freeze_mm_input_adapter\n    if training_args.freeze_mm_input_adapter:\n        for p in model.get_model().mm_input_projector.parameters():\n            p.requires_grad = False\n\n    # freeze/unfreeze the mm output image adapters\n    model.config.tune_mm_output_img_adapter = training_args.tune_mm_output_img_adapter = model_args.tune_mm_output_img_adapter\n    if model_args.tune_mm_output_img_adapter:\n        # model.requires_grad_(False)\n        for p in model.get_model().mm_output_img_projector.parameters():\n            p.requires_grad = True\n    model.config.freeze_mm_output_img_adapter = training_args.freeze_mm_output_img_adapter\n    if training_args.freeze_mm_output_img_adapter:\n        for p in model.get_model().mm_output_img_projector.parameters():\n            p.requires_grad = False\n\n    # freeze/unfreeze the mm output video adapters\n    model.config.tune_mm_output_vid_adapter = training_args.tune_mm_output_vid_adapter = model_args.tune_mm_output_vid_adapter\n    if model_args.tune_mm_output_vid_adapter:\n        # model.requires_grad_(False)\n        for p in model.get_model().mm_output_vid_projector.parameters():\n            p.requires_grad = True\n    model.config.freeze_mm_output_vid_adapter = training_args.freeze_mm_output_vid_adapter\n    if training_args.freeze_mm_output_vid_adapter:\n        for p in model.get_model().mm_output_vid_projector.parameters():\n            p.requires_grad = False\n\n    # freeze/unfreeze the mm output audio adapters\n    model.config.tune_mm_output_aud_adapter = training_args.tune_mm_output_aud_adapter = model_args.tune_mm_output_aud_adapter\n    if model_args.tune_mm_output_aud_adapter:\n        # model.requires_grad_(False)\n        for p in model.get_model().mm_output_aud_projector.parameters():\n            p.requires_grad = True\n    model.config.freeze_mm_output_aud_adapter = training_args.freeze_mm_output_aud_adapter\n    if training_args.freeze_mm_output_aud_adapter:\n        for p in model.get_model().mm_output_aud_projector.parameters():\n            p.requires_grad = False\n\n    # # print the model parameters to check if the adapters are trainable\n    # for n, p in model.get_model().mm_input_projector.named_parameters():\n    #     print(n, ': ', p.requires_grad)\n    # for n, p in model.get_model().mm_output_aud_projector.named_parameters():\n    #     print(n, ': ', p.requires_grad)\n    # for n, p in model.get_model().mm_output_vid_projector.named_parameters():\n    #     print(n, ': ', p.requires_grad)\n    # for n, p in model.get_model().mm_output_img_projector.named_parameters():\n    #     print(n, ': ', p.requires_grad)\n\n    # initialize_vision_tokenizer\n    model.initialize_vision_tokenizer(model_args, tokenizer=tokenizer)      \n    model.print_model_parameters()\n\n    print('Testing tokenizer ... \\n')\n    print(\"Tokenizer model max length:\", tokenizer.tokenize(' '.join([f\"<image_{i:02d}>\" for i in range(model_args.n_img_tokens)])))\n    print(\"Tokenizer model max length:\", tokenizer.tokenize('hello generate the tokens, '+' '.join([f\"<image_{i:02d}>\" for i in range(model_args.n_img_tokens)])))\n    print(\"Tokenizer model max length:\", tokenizer.tokenize(' '.join([f\"<video_{i:02d}>\" for i in range(model_args.n_vid_tokens)])))\n    print(\"Tokenizer model max length:\", tokenizer.tokenize(' '.join([f\"<audio_{i:02d}>\" for i in range(model_args.n_aud_tokens)])))\n\n    if training_args.bits in [4, 8]:\n        from peft.tuners.lora import LoraLayer\n        for name, module in model.named_modules():\n            if isinstance(module, LoraLayer):\n                if training_args.bf16:\n                    module = module.to(torch.bfloat16)\n            if 'norm' in name:\n                module = module.to(torch.float32)\n            if 'lm_head' in name or 'embed_tokens' in name:\n                if hasattr(module, 'weight'):\n                    if training_args.bf16 and module.weight.dtype == torch.float32:\n                        module = module.to(torch.bfloat16)\n    \n    data_args.device = training_args.device\n    data_args.version = model_args.version\n    data_module = make_supervised_data_module(tokenizer=tokenizer,\n                                              data_args=data_args)\n    trainer = NextGPTTrainer(model=model,\n                    tokenizer=tokenizer,\n                    args=training_args,\n                    **data_module)\n\n    if list(pathlib.Path(training_args.output_dir).glob(\"checkpoint-*\")):\n        trainer.train(resume_from_checkpoint=True)\n    else:\n        trainer.train()\n    # print(\"trainer.state: \", trainer.state)\n    trainer.save_state()\n\n    model.config.use_cache = True\n\n    if training_args.lora_enable:\n        state_dict = get_peft_state_maybe_zero_3(\n            model.named_parameters(), training_args.lora_bias\n        )\n        non_lora_state_dict = get_peft_state_non_lora_maybe_zero_3(\n            model.named_parameters()\n        )\n        if training_args.local_rank == 0 or training_args.local_rank == -1:\n            model.config.save_pretrained(training_args.output_dir)\n            model.save_pretrained(training_args.output_dir, state_dict=state_dict)\n            torch.save(non_lora_state_dict, os.path.join(training_args.output_dir, 'non_lora_trainables.bin'))\n        \n        # torch.cuda.synchronize()\n        # new_output_dir = training_args.output_dir+'/model'\n        # if not os.path.exists(new_output_dir):\n        #     os.makedirs(new_output_dir)\n        \n        # model.save_pretrained(new_output_dir)\n        # tokenizer.save_pretrained(new_output_dir)\n        # tokenizer.config.save_pretrained(new_output_dir)\n        \n    else:\n        safe_save_model_for_hf_trainer(trainer=trainer,\n                                       output_dir=training_args.output_dir)\n\n\nif __name__ == \"__main__\":\n    train()"
        },
        {
          "name": "train_mem.py",
          "type": "blob",
          "size": 0.099609375,
          "content": "from train import train\n\nif __name__ == \"__main__\":\n    train(attn_implementation=\"flash_attention_2\")"
        },
        {
          "name": "training_utils.py",
          "type": "blob",
          "size": 6.1669921875,
          "content": "from dataclasses import dataclass, field\nfrom typing import List, Optional\nfrom transformers import TrainingArguments\n\n\n@dataclass\nclass TrainingArguments(TrainingArguments):\n    cache_dir: Optional[str] = field(default=None)\n    optim: str = field(default=\"adamw_torch\")\n    remove_unused_columns: bool = field(default=False)\n    freeze_mm_mlp_adapter: bool = field(default=False)\n    mpt_attn_impl: Optional[str] = field(default=\"triton\")\n    model_max_length: int = field(\n        default=512,\n        metadata={\n            \"help\":\n            \"Maximum sequence length. Sequences will be right padded (and possibly truncated).\"\n        },\n    )\n    double_quant: bool = field(\n        default=True,\n        metadata={\"help\": \"Compress the quantization statistics through double quantization.\"}\n    )\n    quant_type: str = field(\n        default=\"nf4\",\n        metadata={\"help\": \"Quantization data type to use. Should be one of `fp4` or `nf4`.\"}\n    )\n    bits: int = field(\n        default=16,\n        metadata={\"help\": \"How many bits to use.\"}\n    )\n\n    # LoRA related parameters\n    lora_enable: bool = field(default=False, metadata={\"help\": \"Whether to use LoRA technique\"})\n    lora_r: int = 64\n    lora_alpha: int = 16\n    lora_dropout: float = 0.05\n    lora_weight_path: str = \"\"\n    lora_bias: str = \"none\"\n    mm_input_projector_lr: Optional[float] = None\n    mm_output_projector_lr: Optional[float] = None\n    group_by_modality_length: bool = field(default=False)\n    group_by_modality_type: bool = field(default=False)\n\n    fine_tune: bool = field(default=False, metadata={\"help\": \"Whether to fine-tune the model.\"})\n    freeze_mm_input_adapter: bool = field(default=False)\n    freeze_mm_output_img_adapter: bool = field(default=False)\n    freeze_mm_output_vid_adapter: bool = field(default=False)\n    freeze_mm_output_aud_adapter: bool = field(default=False)\n\n    has_img_gen_loss: bool = field(default=False)\n    has_vid_gen_loss: bool = field(default=False)\n    has_aud_gen_loss: bool = field(default=False)\n\n\n@dataclass\nclass DataArguments:\n    dataset_name_list: List[str] = field(default=None, metadata={\"help\": \"The list of dataset names\"})\n    lazy_preprocess: bool = False\n    is_multimodal: bool = False\n    image_folder: Optional[str] = field(default=None)\n    image_aspect_ratio: str = 'square'\n    image_caption_emb_folder: Optional[str] = field(default=None)\n\n    video_folder: Optional[str] = field(default=None)\n    video_caption_emb_folder: Optional[str] = field(default=None)\n    audio_folder: Optional[str] = field(default=None)\n    audio_caption_emb_folder: Optional[str] = field(default=None)\n\n    # for preprocessing output image \n    output_image_height: int = 224\n    output_image_width: int = 224\n    resize_mode: str = 'crop'\n\n    # for preprocessing output video\n    output_video_height: int = 320\n    output_video_width: int = 576\n    sample_fps: int = 1\n    max_frames: int = 16\n\n    # for preprocessing output audio\n    sampling_rate: int = 16000\n    duration: float = 10.4\n    max_wav_value: float = 32768.0\n    n_mel_channels: int = 64\n    mel_fmin: int = 0\n    mel_fmax: int = 8000 \n\n\n@dataclass\nclass ModelArguments:\n    model_name_or_path: str = field(\n        default=None, metadata={\"help\": \"Build-in pretrained model name or the path to local model.\"}\n    )\n    version: Optional[str] = field(default=\"v0\")\n    mm_vision_select_layer: Optional[int] = field(default=-1)   # default to the last layer\n    mm_use_im_start_end: bool = field(default=False)\n    mm_use_im_patch_token: bool = field(default=True)\n    mm_patch_merge_type: Optional[str] = field(default='flat')\n    mm_vision_select_feature: Optional[str] = field(default=\"patch\")\n\n    version: Optional[str] = field(default=\"v0\")\n    multimodal_tower: Optional[str] = field(default=None)\n    freeze_backbone: bool = field(default=True)\n    tune_mm_input_adapter: bool = field(default=True)\n    pretrain_mm_input_adapter: Optional[str] = field(default=None)\n    mm_input_projector_type: Optional[str] = field(default='linear')\n\n    tune_mm_output_img_adapter: bool = field(default=True)\n    pretrain_mm_output_img_adapter: Optional[str] = field(default=None)\n    mm_output_img_projector_type: Optional[str] = field(default='transformer')\n    image_decoder: Optional[str] = field(default=None, metadata={\"help\": \"the path for image decoder checkpoint\"})\n    mm_use_img_start_end: bool = field(default=False)\n    mm_use_img_patch_token: bool = field(default=False)\n    \n    tune_mm_output_vid_adapter: bool = field(default=False)\n    pretrain_mm_output_vid_adapter: Optional[str] = field(default=None)\n    mm_output_vid_projector_type: Optional[str] = field(default='transformer')\n    video_decoder: Optional[str] = field(default=None, metadata={\"help\": \"the path for video decoder checkpoint\"})\n    mm_use_vid_start_end: bool = field(default=False)\n    mm_use_vid_patch_token: bool = field(default=True)\n\n    tune_mm_output_aud_adapter: bool = field(default=False)\n    pretrain_mm_output_aud_adapter: Optional[str] = field(default=None)\n    mm_output_aud_projector_type: Optional[str] = field(default='transformer')\n    audio_decoder: Optional[str] = field(default=None, metadata={\"help\": \"the path for audio decoder checkpoint\"})\n    mm_use_aud_start_end: bool = field(default=False)\n    mm_use_aud_patch_token: bool = field(default=True)\n\n    n_img_tokens: int = field(default=4, metadata={\"help\": \"Number of image signal tokens generated by LLM to generate image\"})\n    mm_output_img_num_query_token: int = field(default=77, metadata={\"help\": \"Number of image signal tokens transformed from output projector to generate image\"})\n    n_vid_tokens: int = field(default=24, metadata={\"help\": \"Number of video signal tokens to generate video\"})\n    mm_output_vid_num_query_token: int = field(default=77, metadata={\"help\": \"Number of video signal tokens transformed from output projector to generate video\"})\n    n_aud_tokens: int = field(default=8, metadata={\"help\": \"Number of audio signal tokens to generate audio\"})\n    mm_output_aud_num_query_token: int = field(default=1, metadata={\"help\": \"Number of aduio signal tokens transformed from output projector to generate audio\"})\n    layer_idx: int = field(default=-1, metadata={\"help\": \"Layer index to extract signal feature from LLM hidden states\"})\n\n\n\n\n"
        }
      ]
    }
  ]
}