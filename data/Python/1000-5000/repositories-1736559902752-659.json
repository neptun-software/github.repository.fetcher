{
  "metadata": {
    "timestamp": 1736559902752,
    "page": 659,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjY2MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "cleardusk/3DDFA",
      "stars": 3620,
      "defaultBranch": "master",
      "files": [
        {
          "name": ".dockerignore",
          "type": "blob",
          "size": 0.0048828125,
          "content": ".git\n"
        },
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.4453125,
          "content": ".idea/\n*.pyc\n__pycache__/\nutils/__pycache__/\ntest.data/\ntraining/snapshot/\ntraining/logs/\ntrain.configs/param_all_norm.pkl*\ntrain.configs/param_all_norm_val.pkl*\ntrain.configs/train_aug_120x120.list.*\n\nsamples/*.mat\nsamples/*.ply\nsamples/*.txt\n\n.ipynb_checkpoints/\n*.ipynb\ntmp/\n\nmodels/shape_predictor_68_face_landmarks.dat\n.DS_Store\n\ndemo_obama/\ntodo.md\n\nutils/cython/build\nutils/cython/*.so\nutils/cython/mesh_core_cython.cpp\nPAF/\n\nsamples/*.obj\n.vscode/\n"
        },
        {
          "name": "BFM_Remove_Neck",
          "type": "tree",
          "content": null
        },
        {
          "name": "Dockerfile",
          "type": "blob",
          "size": 0.744140625,
          "content": "FROM python:3.6-slim-stretch\n\nRUN apt-get -y update\nRUN apt-get install -y --fix-missing \\\n    build-essential \\\n    cmake \\\n    gfortran \\\n    git \\\n    wget \\\n    curl \\\n    libjpeg-dev \\\n    liblapack-dev \\\n    libswscale-dev \\\n    pkg-config \\\n    python3-numpy \\\n    zip \\\n    libboost-dev \\\n    libboost-all-dev \\\n    libsm6 \\\n    libxext6 \\\n    libfontconfig1 \\\n    libxrender1 \\\n    && apt-get clean && rm -rf /tmp/* /var/tmp/*\n\nADD requirements.txt /root/requirements.txt\n\nRUN pip3.6 install torch\n\nRUN export CPLUS_INCLUDE_PATH=/usr/local/include/python3.6m && \\\n    pip3.6 install --upgrade pip==9.0.3 && \\\n    pip3.6 install -r /root/requirements.txt\n\nVOLUME [\"/root\"]\n\nWORKDIR /root\n\nENTRYPOINT [\"python\", \"main.py\", \"-f\", \"samples/emma_input.jpg\"]\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 1.04296875,
          "content": "MIT License\n\nCopyright (c) 2018 Jianzhu Guo\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n"
        },
        {
          "name": "benchmark.py",
          "type": "blob",
          "size": 3.470703125,
          "content": "#!/usr/bin/env python3\n# coding: utf-8\n\nimport torch\nimport torch.nn as nn\nimport torch.utils.data as data\nimport torchvision.transforms as transforms\nimport torch.backends.cudnn as cudnn\nimport mobilenet_v1\nimport time\nimport numpy as np\n\nfrom benchmark_aflw2000 import calc_nme as calc_nme_alfw2000\nfrom benchmark_aflw2000 import ana as ana_alfw2000\nfrom benchmark_aflw import calc_nme as calc_nme_alfw\nfrom benchmark_aflw import ana as ana_aflw\n\nfrom utils.ddfa import ToTensorGjz, NormalizeGjz, DDFATestDataset, reconstruct_vertex\nimport argparse\n\n\ndef extract_param(checkpoint_fp, root='', filelists=None, arch='mobilenet_1', num_classes=62, device_ids=[0],\n                  batch_size=128, num_workers=4):\n    map_location = {f'cuda:{i}': 'cuda:0' for i in range(8)}\n    checkpoint = torch.load(checkpoint_fp, map_location=map_location)['state_dict']\n    torch.cuda.set_device(device_ids[0])\n    model = getattr(mobilenet_v1, arch)(num_classes=num_classes)\n    model = nn.DataParallel(model, device_ids=device_ids).cuda()\n    model.load_state_dict(checkpoint)\n\n    dataset = DDFATestDataset(filelists=filelists, root=root,\n                              transform=transforms.Compose([ToTensorGjz(), NormalizeGjz(mean=127.5, std=128)]))\n    data_loader = data.DataLoader(dataset, batch_size=batch_size, num_workers=num_workers)\n\n    cudnn.benchmark = True\n    model.eval()\n\n    end = time.time()\n    outputs = []\n    with torch.no_grad():\n        for _, inputs in enumerate(data_loader):\n            inputs = inputs.cuda()\n            output = model(inputs)\n\n            for i in range(output.shape[0]):\n                param_prediction = output[i].cpu().numpy().flatten()\n\n                outputs.append(param_prediction)\n        outputs = np.array(outputs, dtype=np.float32)\n\n    print(f'Extracting params take {time.time() - end: .3f}s')\n    return outputs\n\n\ndef _benchmark_aflw(outputs):\n    return ana_aflw(calc_nme_alfw(outputs))\n\n\ndef _benchmark_aflw2000(outputs):\n    return ana_alfw2000(calc_nme_alfw2000(outputs))\n\n\ndef benchmark_alfw_params(params):\n    outputs = []\n    for i in range(params.shape[0]):\n        lm = reconstruct_vertex(params[i])\n        outputs.append(lm[:2, :])\n    return _benchmark_aflw(outputs)\n\n\ndef benchmark_aflw2000_params(params):\n    outputs = []\n    for i in range(params.shape[0]):\n        lm = reconstruct_vertex(params[i])\n        outputs.append(lm[:2, :])\n    return _benchmark_aflw2000(outputs)\n\n\ndef benchmark_pipeline(arch, checkpoint_fp):\n    device_ids = [0]\n\n    def aflw():\n        params = extract_param(\n            checkpoint_fp=checkpoint_fp,\n            root='test.data/AFLW_GT_crop',\n            filelists='test.data/AFLW_GT_crop.list',\n            arch=arch,\n            device_ids=device_ids,\n            batch_size=128)\n\n        benchmark_alfw_params(params)\n\n    def aflw2000():\n        params = extract_param(\n            checkpoint_fp=checkpoint_fp,\n            root='test.data/AFLW2000-3D_crop',\n            filelists='test.data/AFLW2000-3D_crop.list',\n            arch=arch,\n            device_ids=device_ids,\n            batch_size=128)\n\n        benchmark_aflw2000_params(params)\n\n    aflw2000()\n    aflw()\n\n\ndef main():\n    parser = argparse.ArgumentParser(description='3DDFA Benchmark')\n    parser.add_argument('--arch', default='mobilenet_1', type=str)\n    parser.add_argument('-c', '--checkpoint-fp', default='models/phase1_wpdc_vdc.pth.tar', type=str)\n    args = parser.parse_args()\n\n    benchmark_pipeline(args.arch, args.checkpoint_fp)\n\n\nif __name__ == '__main__':\n    main()\n"
        },
        {
          "name": "benchmark",
          "type": "tree",
          "content": null
        },
        {
          "name": "benchmark_aflw.py",
          "type": "blob",
          "size": 3.2626953125,
          "content": "#!/usr/bin/env python3\n# coding: utf-8\n\nimport os.path as osp\nimport numpy as np\nfrom math import sqrt\nfrom utils.io import _load\n\nd = 'test.configs'\nyaw_list = _load(osp.join(d, 'AFLW_GT_crop_yaws.npy'))\nroi_boxs = _load(osp.join(d, 'AFLW_GT_crop_roi_box.npy'))\npts68_all = _load(osp.join(d, 'AFLW_GT_pts68.npy'))\npts21_all = _load(osp.join(d, 'AFLW_GT_pts21.npy'))\n\n\ndef ana(nme_list):\n    yaw_list_abs = np.abs(yaw_list)\n    ind_yaw_1 = yaw_list_abs <= 30\n    ind_yaw_2 = np.bitwise_and(yaw_list_abs > 30, yaw_list_abs <= 60)\n    ind_yaw_3 = yaw_list_abs > 60\n\n    nme_1 = nme_list[ind_yaw_1]\n    nme_2 = nme_list[ind_yaw_2]\n    nme_3 = nme_list[ind_yaw_3]\n\n    mean_nme_1 = np.mean(nme_1) * 100\n    mean_nme_2 = np.mean(nme_2) * 100\n    mean_nme_3 = np.mean(nme_3) * 100\n    # mean_nme_all = np.mean(nme_list) * 100\n\n    std_nme_1 = np.std(nme_1) * 100\n    std_nme_2 = np.std(nme_2) * 100\n    std_nme_3 = np.std(nme_3) * 100\n    # std_nme_all = np.std(nme_list) * 100\n\n    mean_all = [mean_nme_1, mean_nme_2, mean_nme_3]\n    mean = np.mean(mean_all)\n    std = np.std(mean_all)\n\n    s1 = '[ 0, 30]\\tMean: \\x1b[32m{:.3f}\\x1b[0m, Std: {:.3f}'.format(mean_nme_1, std_nme_1)\n    s2 = '[30, 60]\\tMean: \\x1b[32m{:.3f}\\x1b[0m, Std: {:.3f}'.format(mean_nme_2, std_nme_2)\n    s3 = '[60, 90]\\tMean: \\x1b[32m{:.3f}\\x1b[0m, Std: {:.3f}'.format(mean_nme_3, std_nme_3)\n    # s4 = '[ 0, 90]\\tMean: \\x1b[31m{:.3f}\\x1b[0m, Std: {:.3f}'.format(mean_nme_all, std_nme_all)\n    s5 = '[ 0, 90]\\tMean: \\x1b[31m{:.3f}\\x1b[0m, Std: \\x1b[31m{:.3f}\\x1b[0m'.format(mean, std)\n\n    s = '\\n'.join([s1, s2, s3, s5])\n    print(s)\n\n    return mean_nme_1, mean_nme_2, mean_nme_3, mean, std\n\n\ndef calc_nme(pts68_fit_all):\n    std_size = 120\n    ind_68to21 = [[18], [20], [22], [23], [25], [27], [37], [37, 38, 39, 40, 41, 42], [40], [43],\n                  [43, 44, 45, 46, 47, 48],\n                  [46], [3], [32], [31], [36], [15], [49], [61, 62, 63, 64, 65, 66, 67, 68], [55], [9]]\n    for i in range(len(ind_68to21)):\n        for j in range(len(ind_68to21[i])):\n            ind_68to21[i][j] -= 1\n\n    nme_list = []\n\n    for i in range(len(roi_boxs)):\n        pts68_fit = pts68_fit_all[i]\n        pts68_gt = pts68_all[i]\n        pts21_gt = pts21_all[i]\n\n        # reconstruct 68 pts\n        sx, sy, ex, ey = roi_boxs[i]\n        scale_x = (ex - sx) / std_size\n        scale_y = (ey - sy) / std_size\n        pts68_fit[0, :] = pts68_fit[0, :] * scale_x + sx\n        pts68_fit[1, :] = pts68_fit[1, :] * scale_y + sy\n\n        # pts68 -> pts21\n        pts21_est = np.zeros_like(pts21_gt, dtype=np.float32)\n        for i in range(21):\n            ind = ind_68to21[i]\n            tmp = np.mean(pts68_fit[:, ind], 1)\n            pts21_est[:, i] = tmp\n\n        # build bbox\n        minx, maxx = np.min(pts68_gt[0, :]), np.max(pts68_gt[0, :])\n        miny, maxy = np.min(pts68_gt[1, :]), np.max(pts68_gt[1, :])\n        llength = sqrt((maxx - minx) * (maxy - miny))\n\n        # nme\n        pt_valid = (pts21_gt[0, :] != -1) & (pts21_gt[1, :] != -1)\n        dis = pts21_est[:, pt_valid] - pts21_gt[:, pt_valid]\n        dis = np.sqrt(np.sum(np.power(dis, 2), 0))\n        dis = np.mean(dis)\n        nme = dis / llength\n        nme_list.append(nme)\n\n    nme_list = np.array(nme_list, dtype=np.float32)\n    return nme_list\n\n\ndef main():\n    pass\n\n\nif __name__ == '__main__':\n    main()\n"
        },
        {
          "name": "benchmark_aflw2000.py",
          "type": "blob",
          "size": 3.322265625,
          "content": "#!/usr/bin/env python3\n# coding: utf-8\n\n\"\"\"\nNotation (2019.09.15): two versions of spliting AFLW2000-3D:\n 1) AFLW2000-3D.pose.npy: according to the fitted pose\n 2) AFLW2000-3D-new.pose: according to AFLW labels \nThere is no obvious difference between these two splits.\n\"\"\"\n\nimport os.path as osp\nimport numpy as np\nfrom math import sqrt\nfrom utils.io import _load\n\nd = 'test.configs'\n\n# [1312, 383, 305], current version\nyaws_list = _load(osp.join(d, 'AFLW2000-3D.pose.npy'))\n\n# [1306, 462, 232], same as paper\n# yaws_list = _load(osp.join(d, 'AFLW2000-3D-new.pose.npy'))\n\n# origin\npts68_all_ori = _load(osp.join(d, 'AFLW2000-3D.pts68.npy'))\n\n# reannonated\npts68_all_re = _load(osp.join(d, 'AFLW2000-3D-Reannotated.pts68.npy'))\nroi_boxs = _load(osp.join(d, 'AFLW2000-3D_crop.roi_box.npy'))\n\n\ndef ana(nme_list):\n    yaw_list_abs = np.abs(yaws_list)\n    ind_yaw_1 = yaw_list_abs <= 30\n    ind_yaw_2 = np.bitwise_and(yaw_list_abs > 30, yaw_list_abs <= 60)\n    ind_yaw_3 = yaw_list_abs > 60\n\n    nme_1 = nme_list[ind_yaw_1]\n    nme_2 = nme_list[ind_yaw_2]\n    nme_3 = nme_list[ind_yaw_3]\n\n    mean_nme_1 = np.mean(nme_1) * 100\n    mean_nme_2 = np.mean(nme_2) * 100\n    mean_nme_3 = np.mean(nme_3) * 100\n    # mean_nme_all = np.mean(nme_list) * 100\n\n    std_nme_1 = np.std(nme_1) * 100\n    std_nme_2 = np.std(nme_2) * 100\n    std_nme_3 = np.std(nme_3) * 100\n    # std_nme_all = np.std(nme_list) * 100\n\n    mean_all = [mean_nme_1, mean_nme_2, mean_nme_3]\n    mean = np.mean(mean_all)\n    std = np.std(mean_all)\n\n    s1 = '[ 0, 30]\\tMean: \\x1b[32m{:.3f}\\x1b[0m, Std: {:.3f}'.format(mean_nme_1, std_nme_1)\n    s2 = '[30, 60]\\tMean: \\x1b[32m{:.3f}\\x1b[0m, Std: {:.3f}'.format(mean_nme_2, std_nme_2)\n    s3 = '[60, 90]\\tMean: \\x1b[32m{:.3f}\\x1b[0m, Std: {:.3f}'.format(mean_nme_3, std_nme_3)\n    # s4 = '[ 0, 90]\\tMean: \\x1b[31m{:.3f}\\x1b[0m, Std: {:.3f}'.format(mean_nme_all, std_nme_all)\n    s5 = '[ 0, 90]\\tMean: \\x1b[31m{:.3f}\\x1b[0m, Std: \\x1b[31m{:.3f}\\x1b[0m'.format(mean, std)\n\n    s = '\\n'.join([s1, s2, s3, s5])\n    print(s)\n\n    return mean_nme_1, mean_nme_2, mean_nme_3, mean, std\n\n\ndef convert_to_ori(lms, i):\n    std_size = 120\n    sx, sy, ex, ey = roi_boxs[i]\n    scale_x = (ex - sx) / std_size\n    scale_y = (ey - sy) / std_size\n    lms[0, :] = lms[0, :] * scale_x + sx\n    lms[1, :] = lms[1, :] * scale_y + sy\n    return lms\n\n\ndef calc_nme(pts68_fit_all, option='ori'):\n    if option == 'ori':\n        pts68_all = pts68_all_ori\n    elif option == 're':\n        pts68_all = pts68_all_re\n    std_size = 120\n\n    nme_list = []\n\n    for i in range(len(roi_boxs)):\n        pts68_fit = pts68_fit_all[i]\n        pts68_gt = pts68_all[i]\n\n        sx, sy, ex, ey = roi_boxs[i]\n        scale_x = (ex - sx) / std_size\n        scale_y = (ey - sy) / std_size\n        pts68_fit[0, :] = pts68_fit[0, :] * scale_x + sx\n        pts68_fit[1, :] = pts68_fit[1, :] * scale_y + sy\n\n        # build bbox\n        minx, maxx = np.min(pts68_gt[0, :]), np.max(pts68_gt[0, :])\n        miny, maxy = np.min(pts68_gt[1, :]), np.max(pts68_gt[1, :])\n        llength = sqrt((maxx - minx) * (maxy - miny))\n\n        #\n        dis = pts68_fit - pts68_gt[:2, :]\n        dis = np.sqrt(np.sum(np.power(dis, 2), 0))\n        dis = np.mean(dis)\n        nme = dis / llength\n        nme_list.append(nme)\n\n    nme_list = np.array(nme_list, dtype=np.float32)\n    return nme_list\n\n\ndef main():\n    pass\n\n\nif __name__ == '__main__':\n    main()\n"
        },
        {
          "name": "c++",
          "type": "tree",
          "content": null
        },
        {
          "name": "demo@obama",
          "type": "tree",
          "content": null
        },
        {
          "name": "docker-compose.yml",
          "type": "blob",
          "size": 0.078125,
          "content": "version: '2.3'\n\nservices:\n\n  3ddfa:\n    build: .\n    volumes:\n      - ./:/root/\n"
        },
        {
          "name": "imgs",
          "type": "tree",
          "content": null
        },
        {
          "name": "main.py",
          "type": "blob",
          "size": 9.2880859375,
          "content": "#!/usr/bin/env python3\n# coding: utf-8\n\n__author__ = 'cleardusk'\n\n\"\"\"\nThe pipeline of 3DDFA prediction: given one image, predict the 3d face vertices, 68 landmarks and visualization.\n\n[todo]\n1. CPU optimization: https://pmchojnacki.wordpress.com/2018/10/07/slow-pytorch-cpu-performance\n\"\"\"\n\nimport torch\nimport torchvision.transforms as transforms\nimport mobilenet_v1\nimport numpy as np\nimport cv2\nimport dlib\nfrom utils.ddfa import ToTensorGjz, NormalizeGjz, str2bool\nimport scipy.io as sio\nfrom utils.inference import get_suffix, parse_roi_box_from_landmark, crop_img, predict_68pts, dump_to_ply, dump_vertex, \\\n    draw_landmarks, predict_dense, parse_roi_box_from_bbox, get_colors, write_obj_with_colors\nfrom utils.cv_plot import plot_pose_box\nfrom utils.estimate_pose import parse_pose\nfrom utils.render import get_depths_image, cget_depths_image, cpncc\nfrom utils.paf import gen_img_paf\nimport argparse\nimport torch.backends.cudnn as cudnn\n\nSTD_SIZE = 120\n\n\ndef main(args):\n    # 1. load pre-tained model\n    checkpoint_fp = 'models/phase1_wpdc_vdc.pth.tar'\n    arch = 'mobilenet_1'\n\n    checkpoint = torch.load(checkpoint_fp, map_location=lambda storage, loc: storage)['state_dict']\n    model = getattr(mobilenet_v1, arch)(num_classes=62)  # 62 = 12(pose) + 40(shape) +10(expression)\n\n    model_dict = model.state_dict()\n    # because the model is trained by multiple gpus, prefix module should be removed\n    for k in checkpoint.keys():\n        model_dict[k.replace('module.', '')] = checkpoint[k]\n    model.load_state_dict(model_dict)\n    if args.mode == 'gpu':\n        cudnn.benchmark = True\n        model = model.cuda()\n    model.eval()\n\n    # 2. load dlib model for face detection and landmark used for face cropping\n    if args.dlib_landmark:\n        dlib_landmark_model = 'models/shape_predictor_68_face_landmarks.dat'\n        face_regressor = dlib.shape_predictor(dlib_landmark_model)\n    if args.dlib_bbox:\n        face_detector = dlib.get_frontal_face_detector()\n\n    # 3. forward\n    tri = sio.loadmat('visualize/tri.mat')['tri']\n    transform = transforms.Compose([ToTensorGjz(), NormalizeGjz(mean=127.5, std=128)])\n    for img_fp in args.files:\n        img_ori = cv2.imread(img_fp)\n        if args.dlib_bbox:\n            rects = face_detector(img_ori, 1)\n        else:\n            rects = []\n\n        if len(rects) == 0:\n            rects = dlib.rectangles()\n            rect_fp = img_fp + '.bbox'\n            lines = open(rect_fp).read().strip().split('\\n')[1:]\n            for l in lines:\n                l, r, t, b = [int(_) for _ in l.split(' ')[1:]]\n                rect = dlib.rectangle(l, r, t, b)\n                rects.append(rect)\n\n        pts_res = []\n        Ps = []  # Camera matrix collection\n        poses = []  # pose collection, [todo: validate it]\n        vertices_lst = []  # store multiple face vertices\n        ind = 0\n        suffix = get_suffix(img_fp)\n        for rect in rects:\n            # whether use dlib landmark to crop image, if not, use only face bbox to calc roi bbox for cropping\n            if args.dlib_landmark:\n                # - use landmark for cropping\n                pts = face_regressor(img_ori, rect).parts()\n                pts = np.array([[pt.x, pt.y] for pt in pts]).T\n                roi_box = parse_roi_box_from_landmark(pts)\n            else:\n                # - use detected face bbox\n                bbox = [rect.left(), rect.top(), rect.right(), rect.bottom()]\n                roi_box = parse_roi_box_from_bbox(bbox)\n\n            img = crop_img(img_ori, roi_box)\n\n            # forward: one step\n            img = cv2.resize(img, dsize=(STD_SIZE, STD_SIZE), interpolation=cv2.INTER_LINEAR)\n            input = transform(img).unsqueeze(0)\n            with torch.no_grad():\n                if args.mode == 'gpu':\n                    input = input.cuda()\n                param = model(input)\n                param = param.squeeze().cpu().numpy().flatten().astype(np.float32)\n\n            # 68 pts\n            pts68 = predict_68pts(param, roi_box)\n\n            # two-step for more accurate bbox to crop face\n            if args.bbox_init == 'two':\n                roi_box = parse_roi_box_from_landmark(pts68)\n                img_step2 = crop_img(img_ori, roi_box)\n                img_step2 = cv2.resize(img_step2, dsize=(STD_SIZE, STD_SIZE), interpolation=cv2.INTER_LINEAR)\n                input = transform(img_step2).unsqueeze(0)\n                with torch.no_grad():\n                    if args.mode == 'gpu':\n                        input = input.cuda()\n                    param = model(input)\n                    param = param.squeeze().cpu().numpy().flatten().astype(np.float32)\n\n                pts68 = predict_68pts(param, roi_box)\n\n            pts_res.append(pts68)\n            P, pose = parse_pose(param)\n            Ps.append(P)\n            poses.append(pose)\n\n            # dense face 3d vertices\n            if args.dump_ply or args.dump_vertex or args.dump_depth or args.dump_pncc or args.dump_obj:\n                vertices = predict_dense(param, roi_box)\n                vertices_lst.append(vertices)\n            if args.dump_ply:\n                dump_to_ply(vertices, tri, '{}_{}.ply'.format(img_fp.replace(suffix, ''), ind))\n            if args.dump_vertex:\n                dump_vertex(vertices, '{}_{}.mat'.format(img_fp.replace(suffix, ''), ind))\n            if args.dump_pts:\n                wfp = '{}_{}.txt'.format(img_fp.replace(suffix, ''), ind)\n                np.savetxt(wfp, pts68, fmt='%.3f')\n                print('Save 68 3d landmarks to {}'.format(wfp))\n            if args.dump_roi_box:\n                wfp = '{}_{}.roibox'.format(img_fp.replace(suffix, ''), ind)\n                np.savetxt(wfp, roi_box, fmt='%.3f')\n                print('Save roi box to {}'.format(wfp))\n            if args.dump_paf:\n                wfp_paf = '{}_{}_paf.jpg'.format(img_fp.replace(suffix, ''), ind)\n                wfp_crop = '{}_{}_crop.jpg'.format(img_fp.replace(suffix, ''), ind)\n                paf_feature = gen_img_paf(img_crop=img, param=param, kernel_size=args.paf_size)\n\n                cv2.imwrite(wfp_paf, paf_feature)\n                cv2.imwrite(wfp_crop, img)\n                print('Dump to {} and {}'.format(wfp_crop, wfp_paf))\n            if args.dump_obj:\n                wfp = '{}_{}.obj'.format(img_fp.replace(suffix, ''), ind)\n                colors = get_colors(img_ori, vertices)\n                write_obj_with_colors(wfp, vertices, tri, colors)\n                print('Dump obj with sampled texture to {}'.format(wfp))\n            ind += 1\n\n        if args.dump_pose:\n            # P, pose = parse_pose(param)  # Camera matrix (without scale), and pose (yaw, pitch, roll, to verify)\n            img_pose = plot_pose_box(img_ori, Ps, pts_res)\n            wfp = img_fp.replace(suffix, '_pose.jpg')\n            cv2.imwrite(wfp, img_pose)\n            print('Dump to {}'.format(wfp))\n        if args.dump_depth:\n            wfp = img_fp.replace(suffix, '_depth.png')\n            # depths_img = get_depths_image(img_ori, vertices_lst, tri-1)  # python version\n            depths_img = cget_depths_image(img_ori, vertices_lst, tri - 1)  # cython version\n            cv2.imwrite(wfp, depths_img)\n            print('Dump to {}'.format(wfp))\n        if args.dump_pncc:\n            wfp = img_fp.replace(suffix, '_pncc.png')\n            pncc_feature = cpncc(img_ori, vertices_lst, tri - 1)  # cython version\n            cv2.imwrite(wfp, pncc_feature[:, :, ::-1])  # cv2.imwrite will swap RGB -> BGR\n            print('Dump to {}'.format(wfp))\n        if args.dump_res:\n            draw_landmarks(img_ori, pts_res, wfp=img_fp.replace(suffix, '_3DDFA.jpg'), show_flg=args.show_flg)\n\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser(description='3DDFA inference pipeline')\n    parser.add_argument('-f', '--files', nargs='+',\n                        help='image files paths fed into network, single or multiple images')\n    parser.add_argument('-m', '--mode', default='cpu', type=str, help='gpu or cpu mode')\n    parser.add_argument('--show_flg', default='true', type=str2bool, help='whether show the visualization result')\n    parser.add_argument('--bbox_init', default='one', type=str,\n                        help='one|two: one-step bbox initialization or two-step')\n    parser.add_argument('--dump_res', default='true', type=str2bool, help='whether write out the visualization image')\n    parser.add_argument('--dump_vertex', default='false', type=str2bool,\n                        help='whether write out the dense face vertices to mat')\n    parser.add_argument('--dump_ply', default='true', type=str2bool)\n    parser.add_argument('--dump_pts', default='true', type=str2bool)\n    parser.add_argument('--dump_roi_box', default='false', type=str2bool)\n    parser.add_argument('--dump_pose', default='true', type=str2bool)\n    parser.add_argument('--dump_depth', default='true', type=str2bool)\n    parser.add_argument('--dump_pncc', default='true', type=str2bool)\n    parser.add_argument('--dump_paf', default='false', type=str2bool)\n    parser.add_argument('--paf_size', default=3, type=int, help='PAF feature kernel size')\n    parser.add_argument('--dump_obj', default='true', type=str2bool)\n    parser.add_argument('--dlib_bbox', default='true', type=str2bool, help='whether use dlib to predict bbox')\n    parser.add_argument('--dlib_landmark', default='true', type=str2bool,\n                        help='whether use dlib landmark to crop image')\n\n    args = parser.parse_args()\n    main(args)\n"
        },
        {
          "name": "mobilenet_v1.py",
          "type": "blob",
          "size": 5.1015625,
          "content": "#!/usr/bin/env python3\n# coding: utf-8\n\nfrom __future__ import division\n\n\"\"\" \nCreates a MobileNet Model as defined in:\nAndrew G. Howard Menglong Zhu Bo Chen, et.al. (2017). \nMobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications. \nCopyright (c) Yang Lu, 2017\n\nModified By cleardusk\n\"\"\"\nimport math\nimport torch.nn as nn\n\n__all__ = ['mobilenet_2', 'mobilenet_1', 'mobilenet_075', 'mobilenet_05', 'mobilenet_025']\n\n\nclass DepthWiseBlock(nn.Module):\n    def __init__(self, inplanes, planes, stride=1, prelu=False):\n        super(DepthWiseBlock, self).__init__()\n        inplanes, planes = int(inplanes), int(planes)\n        self.conv_dw = nn.Conv2d(inplanes, inplanes, kernel_size=3, padding=1, stride=stride, groups=inplanes,\n                                 bias=False)\n        self.bn_dw = nn.BatchNorm2d(inplanes)\n        self.conv_sep = nn.Conv2d(inplanes, planes, kernel_size=1, stride=1, padding=0, bias=False)\n        self.bn_sep = nn.BatchNorm2d(planes)\n        if prelu:\n            self.relu = nn.PReLU()\n        else:\n            self.relu = nn.ReLU(inplace=True)\n\n    def forward(self, x):\n        out = self.conv_dw(x)\n        out = self.bn_dw(out)\n        out = self.relu(out)\n\n        out = self.conv_sep(out)\n        out = self.bn_sep(out)\n        out = self.relu(out)\n\n        return out\n\n\nclass MobileNet(nn.Module):\n    def __init__(self, widen_factor=1.0, num_classes=1000, prelu=False, input_channel=3):\n        \"\"\" Constructor\n        Args:\n            widen_factor: config of widen_factor\n            num_classes: number of classes\n        \"\"\"\n        super(MobileNet, self).__init__()\n\n        block = DepthWiseBlock\n        self.conv1 = nn.Conv2d(input_channel, int(32 * widen_factor), kernel_size=3, stride=2, padding=1,\n                               bias=False)\n\n        self.bn1 = nn.BatchNorm2d(int(32 * widen_factor))\n        if prelu:\n            self.relu = nn.PReLU()\n        else:\n            self.relu = nn.ReLU(inplace=True)\n\n        self.dw2_1 = block(32 * widen_factor, 64 * widen_factor, prelu=prelu)\n        self.dw2_2 = block(64 * widen_factor, 128 * widen_factor, stride=2, prelu=prelu)\n\n        self.dw3_1 = block(128 * widen_factor, 128 * widen_factor, prelu=prelu)\n        self.dw3_2 = block(128 * widen_factor, 256 * widen_factor, stride=2, prelu=prelu)\n\n        self.dw4_1 = block(256 * widen_factor, 256 * widen_factor, prelu=prelu)\n        self.dw4_2 = block(256 * widen_factor, 512 * widen_factor, stride=2, prelu=prelu)\n\n        self.dw5_1 = block(512 * widen_factor, 512 * widen_factor, prelu=prelu)\n        self.dw5_2 = block(512 * widen_factor, 512 * widen_factor, prelu=prelu)\n        self.dw5_3 = block(512 * widen_factor, 512 * widen_factor, prelu=prelu)\n        self.dw5_4 = block(512 * widen_factor, 512 * widen_factor, prelu=prelu)\n        self.dw5_5 = block(512 * widen_factor, 512 * widen_factor, prelu=prelu)\n        self.dw5_6 = block(512 * widen_factor, 1024 * widen_factor, stride=2, prelu=prelu)\n\n        self.dw6 = block(1024 * widen_factor, 1024 * widen_factor, prelu=prelu)\n\n        self.avgpool = nn.AdaptiveAvgPool2d(1)\n        self.fc = nn.Linear(int(1024 * widen_factor), num_classes)\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                m.weight.data.normal_(0, math.sqrt(2. / n))\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n\n        x = self.dw2_1(x)\n        x = self.dw2_2(x)\n        x = self.dw3_1(x)\n        x = self.dw3_2(x)\n        x = self.dw4_1(x)\n        x = self.dw4_2(x)\n        x = self.dw5_1(x)\n        x = self.dw5_2(x)\n        x = self.dw5_3(x)\n        x = self.dw5_4(x)\n        x = self.dw5_5(x)\n        x = self.dw5_6(x)\n        x = self.dw6(x)\n\n        x = self.avgpool(x)\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n\n        return x\n\n\ndef mobilenet(widen_factor=1.0, num_classes=1000):\n    \"\"\"\n    Construct MobileNet.\n    widen_factor=1.0  for mobilenet_1\n    widen_factor=0.75 for mobilenet_075\n    widen_factor=0.5  for mobilenet_05\n    widen_factor=0.25 for mobilenet_025\n    \"\"\"\n    model = MobileNet(widen_factor=widen_factor, num_classes=num_classes)\n    return model\n\n\ndef mobilenet_2(num_classes=62, input_channel=3):\n    model = MobileNet(widen_factor=2.0, num_classes=num_classes, input_channel=input_channel)\n    return model\n\n\ndef mobilenet_1(num_classes=62, input_channel=3):\n    model = MobileNet(widen_factor=1.0, num_classes=num_classes, input_channel=input_channel)\n    return model\n\n\ndef mobilenet_075(num_classes=62, input_channel=3):\n    model = MobileNet(widen_factor=0.75, num_classes=num_classes, input_channel=input_channel)\n    return model\n\n\ndef mobilenet_05(num_classes=62, input_channel=3):\n    model = MobileNet(widen_factor=0.5, num_classes=num_classes, input_channel=input_channel)\n    return model\n\n\ndef mobilenet_025(num_classes=62, input_channel=3):\n    model = MobileNet(widen_factor=0.25, num_classes=num_classes, input_channel=input_channel)\n    return model\n"
        },
        {
          "name": "models",
          "type": "tree",
          "content": null
        },
        {
          "name": "readme.md",
          "type": "blob",
          "size": 15.9501953125,
          "content": "# Face Alignment in Full Pose Range: A 3D Total Solution\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)\n![stars](https://img.shields.io/github/stars/cleardusk/3DDFA.svg?style=flat)\n![GitHub issues](https://img.shields.io/github/issues/cleardusk/3DDFA.svg)\n![GitHub repo size](https://img.shields.io/github/repo-size/cleardusk/3DDFA.svg)\n\n<!-- By [Jianzhu Guo](https://guojianzhu.com/aboutme.html). -->\nBy [Jianzhu Guo](http://guojianzhu.com).\n\n\n<p align=\"center\">\n  <img src=\"samples/obama_three_styles.gif\" alt=\"obama\">\n</p>\n\n**\\[Updates\\]**\n - `2022.5.14`: Recommend a python implementation of face profiling: [face_pose_augmentation](https://github.com/hhj1897/face_pose_augmentation).\n - `2020.8.30`: The pre-trained model and code of ECCV-20 are made public on [3DDFA_V2](https://github.com/cleardusk/3DDFA_V2), the copyright is explained by Jianzhu Guo and the CBSR group.\n - `2020.8.2`: Update a <strong>[simple c++ port](./c++/readme.md)</strong> of this project.\n - `2020.7.3`: The extended work <strong>[Towards Fast, Accurate and Stable 3D Dense Face Alignment](https://guojianzhu.com/assets/pdfs/3162.pdf)</strong> is accepted by [ECCV 2020](https://eccv2020.eu/). See [my page](https://guojianzhu.com) for more details.\n - `2019.9.15`: Some updates, see the commits for details.\n - `2019.6.17`: Adding a [video demo](./video_demo.py) contributed by [zjjMaiMai](https://github.com/zjjMaiMai).\n - `2019.5.2`: Evaluating inference speed on CPU with PyTorch v1.1.0, see [here](#CPU) and [speed_cpu.py](./speed_cpu.py).\n - `2019.4.27`: A simple render pipeline running at ~25ms/frame (720p), see [rendering.py](demo@obama/rendering.py) for more details.\n - `2019.4.24`: Providing the demo building of obama, see [demo@obama/readme.md](demo@obama/readme.md) for more details.\n - `2019.3.28`: Some updates.\n - `2018.12.23`: **Add several features: depth image estimation, PNCC, PAF feature and obj serialization.** See `dump_depth`, `dump_pncc`, `dump_paf`, `dump_obj` options for more details.\n - `2018.12.2`: Support landmark-free face cropping, see `dlib_landmark` option.\n - `2018.12.1`: Refine code and add pose estimation feature, see [utils/estimate_pose.py](./utils/estimate_pose.py) for more details.\n - `2018.11.17`: Refine code and map the 3d vertex to original image space.\n - `2018.11.11`: **Update end-to-end inference pipeline: infer/serialize 3D face shape and 68 landmarks given one arbitrary image, please see readme.md below for more details.**\n - `2018.10.4`: Add Matlab face mesh rendering demo in [visualize](./visualize).\n - `2018.9.9`: Add pre-process of face cropping in [benchmark](./benchmark).\n\n**\\[Todo\\]**\n\n- [x] Add c++ port.\n- [x] Depth image estimation.\n- [x] PNCC (Projected Normalized Coordinate Code).\n- [x] PAF (Pose Adaptive Feature).\n- [x] Obj serialization with sampled texture.\n- [x] Recommendation of fast face detectors: [FaceBoxes.PyTorch](https://github.com/zisianw/FaceBoxes.PyTorch), [libfacedetection](https://github.com/ShiqiYu/libfacedetection), [ZQCNN](https://github.com/zuoqing1988/ZQCNN)\n- [x] [Training details](#Training-details)\n- [x] Face Profiling: [Official Matlab code](https://drive.google.com/open?id=1f4686yak4lGHLD6MfIiX5knVtq8a762d), [Python version](https://github.com/hhj1897/face_pose_augmentation) <!--(http://www.cbsr.ia.ac.cn/users/xiangyuzhu/projects/3DDFA/Code/FaceProfilingRelease_v1.1.zip) -->\n\n## Introduction\nThis repo holds the pytorch improved version of the paper: [Face Alignment in Full Pose Range: A 3D Total Solution](https://arxiv.org/abs/1804.01005). Several works beyond the original paper are added, including the real-time training, training strategies. Therefore, this repo is an improved version of the original work. As far, this repo releases the pre-trained first-stage pytorch models of MobileNet-V1 structure, the pre-processed training&testing dataset and codebase. Note that the inference time is about **0.27ms per image** (input batch with 128 images as an input batch) on GeForce GTX TITAN X.\n<!-- Note that if your academic work use the code of this repo, you should cite this repo not the original paper.-->\n<!-- One related blog will be published for some important technique details in future. -->\n<!-- Why not evaluate it on single image? Because most time for single image is spent on function call. The inference speed is equal to MobileNet-V1 with 120x120x3 tensor as input, therefore it is possible to convert to mobile devices. -->\n\n**This repo will keep updating in my spare time, and any meaningful issues and PR are welcomed.**\n\nSeveral results on ALFW-2000 dataset (inferenced from model *phase1_wpdc_vdc.pth.tar*) are shown below.\n<p align=\"center\">\n  <img src=\"imgs/landmark_3d.jpg\" alt=\"Landmark 3D\" width=\"1000px\">\n</p>\n\n<p align=\"center\">\n  <img src=\"imgs/vertex_3d.jpg\" alt=\"Vertex 3D\" width=\"750px\">\n</p>\n\n## Applications & Features\n#### 1. Face Alignment\n<p align=\"center\">\n  <img src=\"samples/dapeng_3DDFA_trim.gif\" alt=\"dapeng\">\n</p>\n\n#### 2. Face Reconstruction\n<p align=\"center\">\n  <img src=\"samples/5.png\" alt=\"demo\" width=\"750px\">\n</p>\n\n#### 3. 3D Pose Estimation\n<p align=\"center\">\n  <img src=\"samples/pose.png\" alt=\"tongliya\" width=\"750px\">\n</p>\n\n#### 4. Depth Image Estimation\n<p align=\"center\">\n  <img src=\"samples/demo_depth.jpg\" alt=\"demo_depth\" width=\"750px\">\n</p>\n\n#### 5. PNCC & PAF Features\n<p align=\"center\">\n  <img src=\"samples/demo_pncc_paf.jpg\" alt=\"demo_pncc_paf\" width=\"800px\">\n</p>\n\n## Getting started\n### Requirements\n - PyTorch >= 0.4.1 (**PyTorch v1.1.0** is tested successfully on macOS and Linux.)\n - Python >= 3.6 (Numpy, Scipy, Matplotlib)\n - Dlib (Dlib is optionally for face and landmarks detection. There is no need to use Dlib if you can provide face bouding bbox and landmarks. Besides, you can try the two-step inference strategy without initialized landmarks.)\n - OpenCV (Python version, for image IO operations.)\n - Cython (For accelerating depth and PNCC render.)\n - Platform: Linux or macOS (Windows is not tested.)\n\n ```\n # installation structions\n sudo pip3 install torch torchvision # for cpu version. more option to see https://pytorch.org\n sudo pip3 install numpy scipy matplotlib\n sudo pip3 install dlib==19.5.0 # 19.15+ version may cause conflict with pytorch in Linux, this may take several minutes. If 19.5 version raises errors, you may try 19.15+ version.\n sudo pip3 install opencv-python\n sudo pip3 install cython\n ```\n\nIn addition, I strongly recommend using Python3.6+ instead of older version for its better design.\n\n### Usage\n\n1. Clone this repo (this may take some time as it is a little big)\n    ```\n    git clone https://github.com/cleardusk/3DDFA.git  # or git@github.com:cleardusk/3DDFA.git\n    cd 3DDFA\n    ```\n\n   Then, download dlib landmark pre-trained model in [Google Drive](https://drive.google.com/open?id=1kxgOZSds1HuUIlvo5sRH3PJv377qZAkE) or [Baidu Yun](https://pan.baidu.com/s/1bx-GxGf50-KDk4xz3bCYcw), and put it into `models` directory. (To reduce this repo's size, I remove some large size binary files including this model, so you should download it : ) )\n\n\n2. Build cython module (just one line for building)\n   ```\n   cd utils/cython\n   python3 setup.py build_ext -i\n   ```\n   This is for accelerating depth estimation and PNCC render since Python is too slow in for loop.\n   \n    \n3. Run the `main.py` with arbitrary image as input\n    ```\n    python3 main.py -f samples/test1.jpg\n    ```\n    If you can see these output log in terminal, you run it successfully.\n    ```\n    Dump tp samples/test1_0.ply\n    Save 68 3d landmarks to samples/test1_0.txt\n    Dump obj with sampled texture to samples/test1_0.obj\n    Dump tp samples/test1_1.ply\n    Save 68 3d landmarks to samples/test1_1.txt\n    Dump obj with sampled texture to samples/test1_1.obj\n    Dump to samples/test1_pose.jpg\n    Dump to samples/test1_depth.png\n    Dump to samples/test1_pncc.png\n    Save visualization result to samples/test1_3DDFA.jpg\n    ```\n\n    Because `test1.jpg` has two faces, there are two `.ply` and `.obj` files (can be rendered by Meshlab or Microsoft 3D Builder) predicted. Depth, PNCC, PAF and pose estimation are all set true by default. Please run `python3 main.py -h` or review the code for more details.\n\n    The 68 landmarks visualization result `samples/test1_3DDFA.jpg` and pose estimation result `samples/test1_pose.jpg` are shown below:\n\n<p align=\"center\">\n  <img src=\"samples/test1_3DDFA.jpg\" alt=\"samples\" width=\"650px\">\n</p>\n\n<p align=\"center\">\n  <img src=\"samples/test1_pose.jpg\" alt=\"samples\" width=\"650px\">\n</p>\n\n4. Additional example\n\n    ```\n    python3 ./main.py -f samples/emma_input.jpg --bbox_init=two --dlib_bbox=false\n    ```\n\n<p align=\"center\">\n  <img src=\"samples/emma_input_3DDFA.jpg\" alt=\"samples\" width=\"750px\">\n</p>\n\n<p align=\"center\">\n  <img src=\"samples/emma_input_pose.jpg\" alt=\"samples\" width=\"750px\">\n</p>\n\n\n## Inference speed\n### CPU\nJust run\n```\npython3 speed_cpu.py\n```\n\nOn my MBP (i5-8259U CPU @ 2.30GHz on 13-inch MacBook Pro), based on **PyTorch v1.1.0**, with a single input, the running output is:\n```\nInference speed: 14.50Â±0.11 ms\n```\n\n<!-- [speed_cpu.py](./speed_cpu.py) -->\n\n\n### GPU\nWhen input batch size is 128, the total inference time of MobileNet-V1 takes about 34.7ms. The average speed is about **0.27ms/pic**.\n\n<p align=\"center\">\n  <img src=\"imgs/inference_speed.png\" alt=\"Inference speed\" width=\"600px\">\n</p>\n\n## Training details\nThe training scripts lie in `training` directory. The related resources are in below table.\n\n| Data | Download Link | Description |\n|:-:|:-:|:-:|\n| train.configs | [BaiduYun](https://pan.baidu.com/s/1ozZVs26-xE49sF7nystrKQ) or [Google Drive](https://drive.google.com/open?id=1dzwQNZNMppFVShLYoLEfU3EOj3tCeXOD), 217M | The directory containing 3DMM params and filelists of training dataset |\n| train_aug_120x120.zip | [BaiduYun](https://pan.baidu.com/s/19QNGst2E1pRKL7Dtx_L1MA) or [Google Drive](https://drive.google.com/open?id=17LfvBZFAeXt0ACPnVckfdrLTMHUpIQqE), 2.15G | The cropped images of augmentation training dataset |\n| test.data.zip | [BaiduYun](https://pan.baidu.com/s/1DTVGCG5k0jjjhOc8GcSLOw) or [Google Drive](https://drive.google.com/file/d/1r_ciJ1M0BSRTwndIBt42GlPFRv6CvvEP/view?usp=sharing), 151M | The cropped images of AFLW and ALFW-2000-3D testset |\n\nAfter preparing the training dataset and configuration files, go into `training` directory and run the bash scripts to train. `train_wpdc.sh`, `train_vdc.sh` and `train_pdc.sh` are examples of training scripts. After configuring the training and testing sets, just run them for training. Take `train_wpdc.sh` for example as below:\n\n```\n#!/usr/bin/env bash\n\nLOG_ALIAS=$1\nLOG_DIR=\"logs\"\nmkdir -p ${LOG_DIR}\n\nLOG_FILE=\"${LOG_DIR}/${LOG_ALIAS}_`date +'%Y-%m-%d_%H:%M.%S'`.log\"\n#echo $LOG_FILE\n\n./train.py --arch=\"mobilenet_1\" \\\n    --start-epoch=1 \\\n    --loss=wpdc \\\n    --snapshot=\"snapshot/phase1_wpdc\" \\\n    --param-fp-train='../train.configs/param_all_norm.pkl' \\\n    --param-fp-val='../train.configs/param_all_norm_val.pkl' \\\n    --warmup=5 \\\n    --opt-style=resample \\\n    --resample-num=132 \\\n    --batch-size=512 \\\n    --base-lr=0.02 \\\n    --epochs=50 \\\n    --milestones=30,40 \\\n    --print-freq=50 \\\n    --devices-id=0,1 \\\n    --workers=8 \\\n    --filelists-train=\"../train.configs/train_aug_120x120.list.train\" \\\n    --filelists-val=\"../train.configs/train_aug_120x120.list.val\" \\\n    --root=\"/path/to//train_aug_120x120\" \\\n    --log-file=\"${LOG_FILE}\"\n```\n\nThe specific training parameters are all presented in bash scripts, including learning rate, mini-batch size, epochs and so on.\n\n## Evaluation\nFirst, you should download the cropped testset ALFW and ALFW-2000-3D in [test.data.zip](https://pan.baidu.com/s/1DTVGCG5k0jjjhOc8GcSLOw), then unzip it and put it in the root directory.\nNext, run the benchmark code by providing trained model path.\nI have already provided five pre-trained models in `models` directory (seen in below table). These models are trained using different loss in the first stage. The model size is about 13M due to the high efficiency of MobileNet-V1 structure.\n```\npython3 ./benchmark.py -c models/phase1_wpdc_vdc.pth.tar\n```\n\nThe performances of pre-trained models are shown below. In the first stage, the effectiveness of different loss is in order: WPDC > VDC > PDC. While the strategy using VDC to finetune WPDC achieves the best result.\n\n| Model | AFLW (21 pts) | AFLW 2000-3D (68 pts) | Download Link |\n|:-:|:-:|:-:| :-: |\n| *phase1_pdc.pth.tar*  | 6.956Â±0.981 | 5.644Â±1.323 | [Baidu Yun](https://pan.baidu.com/s/1xeyZa4rxVazd_QGWx6QXFw) or [Google Drive](https://drive.google.com/open?id=18UQfDkGNzotKoFV0Lh_O-HnXsp1ABdjl) |\n| *phase1_vdc.pth.tar*  | 6.717Â±0.924 | 5.030Â±1.044 | [Baidu Yun](https://pan.baidu.com/s/10-0YpYKj1_efJYqC1q-aNQ) or [Google Drive](https://drive.google.com/open?id=1iHADYNIQR2Jqvt4nwmnh5n3Axe-HXMRR) |\n| *phase1_wpdc.pth.tar* | 6.348Â±0.929 | 4.759Â±0.996 | [Baidu Yun](https://pan.baidu.com/s/1yqaJ3S3MNpYBgyA5BYtHuw) or [Google Drive](https://drive.google.com/open?id=1ebwkOWjaQ7U4mpA89ldfmjeQdfDDdFS-) |\n| *phase1_wpdc_vdc.pth.tar* | **5.401Â±0.754** | **4.252Â±0.976** | In this repo. |\n\n### About the performance\nBelieve me that the framework of this repo can achieve better performance than [PRNet](https://github.com/YadiraF/PRNet) without increasing any computation budget. Related work is under review and code will be released upon acceptance.\n\n## FQA\n1. Face bounding box initialization\n\n    The original paper shows that using detected bounding box instead of ground truth box will cause a little performance drop. Thus the current face cropping method is robustest. Quantitative results are shown in below table.\n\n<p align=\"center\">\n  <img src=\"imgs/bouding_box_init.png\" alt=\"bounding box\" width=\"500px\">\n</p>\n\n2. Face reconstruction\n   \n   The texture of non-visible area is distorted due to self-occlusion, therefore the non-visible face region may appear strange (a little horrible).\n\n3. About shape and expression parameters clipping\n   \n    The parameters clipping accelerates the training and reconstruction, but degrades the accuracy especially the details like closing eyes. Below is an image, with parameters dimension 40+10, 60+29 and 199+29 (the original one). Compared to shape, expression clipping has more effect on reconstruction accuracy when emotion is involved. Therefore, you can choose a trade-off between the speed/parameter-size and the accuracy. A recommendation of clipping trade-off is 60+29.\n\n<p align=\"center\">\n  <img src=\"imgs/params_clip.jpg\" alt=\"bounding box\" width=\"600px\">\n</p>\n\n## Acknowledgement\n - Thanks for [Yao Feng](https://github.com/YadiraF)'s fantastic works [PRNet](https://github.com/YadiraF/PRNet) and [face3d](https://github.com/YadiraF/face3d).\n - Thanks for this [tweet](https://twitter.com/PyTorch/status/1066064914249367552) of PyTorch.\n\n\nThanks for your interest in this repo. If your work or research benefits from this repo, star it ðŸ˜ƒ\n\nWelcome to focus on my 3D face related works: [MeGlass](https://github.com/cleardusk/MeGlass) and [Face Anti-Spoofing](https://arxiv.org/abs/1901.00488).\n\n## Citation\n**If your work benefits from this repo, please cite three bibs below.**\n\n    @misc{3ddfa_cleardusk,\n      author =       {Guo, Jianzhu and Zhu, Xiangyu and Lei, Zhen},\n      title =        {3DDFA},\n      howpublished = {\\url{https://github.com/cleardusk/3DDFA}},\n      year =         {2018}\n    }\n    \n    @inproceedings{guo2020towards,\n      title=        {Towards Fast, Accurate and Stable 3D Dense Face Alignment},\n      author=       {Guo, Jianzhu and Zhu, Xiangyu and Yang, Yang and Yang, Fan and Lei, Zhen and Li, Stan Z},\n      booktitle=    {Proceedings of the European Conference on Computer Vision (ECCV)},\n      year=         {2020}\n    }\n\n    @article{zhu2017face,\n      title=      {Face alignment in full pose range: A 3d total solution},\n      author=     {Zhu, Xiangyu and Liu, Xiaoming and Lei, Zhen and Li, Stan Z},\n      journal=    {IEEE transactions on pattern analysis and machine intelligence},\n      year=       {2017},\n      publisher=  {IEEE}\n    }\n\n\n## Contact\n**Jianzhu Guo (éƒ­å»ºç )** [[Homepage](http://guojianzhu.com), [Google Scholar](https://scholar.google.com/citations?user=W8_JzNcAAAAJ&hl=en&oi=ao)]:  **jianzhu.guo@nlpr.ia.ac.cn** or **guojianzhu1994@foxmail.com**.\n"
        },
        {
          "name": "requirements.txt",
          "type": "blob",
          "size": 0.189453125,
          "content": "torch>=0.4.1\ntorchvision>=0.2.1\nnumpy>=1.15.4\nscipy>=1.1.0\nmatplotlib==3.0.2\ndlib==19.5.0 # 19.15+ version may cause conflict with pytorch, this may take several minutes\nopencv-python>=3.4.3.18\n"
        },
        {
          "name": "samples",
          "type": "tree",
          "content": null
        },
        {
          "name": "speed_cpu.py",
          "type": "blob",
          "size": 0.677734375,
          "content": "#!/usr/bin/env python3\n# coding: utf-8\n\nimport timeit\nimport numpy as np\n\nSETUP_CODE = '''\nimport mobilenet_v1\nimport torch\n\nmodel = mobilenet_v1.mobilenet_1()\nmodel.eval()\ndata = torch.rand(1, 3, 120, 120)\n'''\n\nTEST_CODE = '''\nwith torch.no_grad():\n    model(data)\n'''\n\n\ndef main():\n    repeat, number = 5, 100\n    res = timeit.repeat(setup=SETUP_CODE,\n                        stmt=TEST_CODE,\n                        repeat=repeat,\n                        number=number)\n    res = np.array(res, dtype=np.float32)\n    res /= number\n    mean, var = np.mean(res), np.std(res)\n    print('Inference speed: {:.2f}Â±{:.2f} ms'.format(mean * 1000, var * 1000))\n\n\nif __name__ == '__main__':\n    main()\n"
        },
        {
          "name": "test.configs",
          "type": "tree",
          "content": null
        },
        {
          "name": "train.configs",
          "type": "tree",
          "content": null
        },
        {
          "name": "train.py",
          "type": "blob",
          "size": 9.705078125,
          "content": "#!/usr/bin/env python3\n# coding: utf-8\n\nimport os.path as osp\nfrom pathlib import Path\nimport numpy as np\nimport argparse\nimport time\nimport logging\n\nimport torch\nimport torch.nn as nn\nimport torchvision.transforms as transforms\nfrom torch.utils.data import DataLoader\nimport mobilenet_v1\nimport torch.backends.cudnn as cudnn\n\nfrom utils.ddfa import DDFADataset, ToTensorGjz, NormalizeGjz\nfrom utils.ddfa import str2bool, AverageMeter\nfrom utils.io import mkdir\nfrom vdc_loss import VDCLoss\nfrom wpdc_loss import WPDCLoss\n\n# global args (configuration)\nargs = None\nlr = None\narch_choices = ['mobilenet_2', 'mobilenet_1', 'mobilenet_075', 'mobilenet_05', 'mobilenet_025']\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser(description='3DMM Fitting')\n    parser.add_argument('-j', '--workers', default=6, type=int)\n    parser.add_argument('--epochs', default=40, type=int)\n    parser.add_argument('--start-epoch', default=1, type=int)\n    parser.add_argument('-b', '--batch-size', default=128, type=int)\n    parser.add_argument('-vb', '--val-batch-size', default=32, type=int)\n    parser.add_argument('--base-lr', '--learning-rate', default=0.001, type=float)\n    parser.add_argument('--momentum', default=0.9, type=float, metavar='M',\n                        help='momentum')\n    parser.add_argument('--weight-decay', '--wd', default=5e-4, type=float)\n    parser.add_argument('--print-freq', '-p', default=20, type=int)\n    parser.add_argument('--resume', default='', type=str, metavar='PATH')\n    parser.add_argument('--devices-id', default='0,1', type=str)\n    parser.add_argument('--filelists-train',\n                        default='', type=str)\n    parser.add_argument('--filelists-val',\n                        default='', type=str)\n    parser.add_argument('--root', default='')\n    parser.add_argument('--snapshot', default='', type=str)\n    parser.add_argument('--log-file', default='output.log', type=str)\n    parser.add_argument('--log-mode', default='w', type=str)\n    parser.add_argument('--size-average', default='true', type=str2bool)\n    parser.add_argument('--num-classes', default=62, type=int)\n    parser.add_argument('--arch', default='mobilenet_1', type=str,\n                        choices=arch_choices)\n    parser.add_argument('--frozen', default='false', type=str2bool)\n    parser.add_argument('--milestones', default='15,25,30', type=str)\n    parser.add_argument('--task', default='all', type=str)\n    parser.add_argument('--test_initial', default='false', type=str2bool)\n    parser.add_argument('--warmup', default=-1, type=int)\n    parser.add_argument('--param-fp-train',\n                        default='',\n                        type=str)\n    parser.add_argument('--param-fp-val',\n                        default='')\n    parser.add_argument('--opt-style', default='resample', type=str)  # resample\n    parser.add_argument('--resample-num', default=132, type=int)\n    parser.add_argument('--loss', default='vdc', type=str)\n\n    global args\n    args = parser.parse_args()\n\n    # some other operations\n    args.devices_id = [int(d) for d in args.devices_id.split(',')]\n    args.milestones = [int(m) for m in args.milestones.split(',')]\n\n    snapshot_dir = osp.split(args.snapshot)[0]\n    mkdir(snapshot_dir)\n\n\ndef print_args(args):\n    for arg in vars(args):\n        s = arg + ': ' + str(getattr(args, arg))\n        logging.info(s)\n\n\ndef adjust_learning_rate(optimizer, epoch, milestones=None):\n    \"\"\"Sets the learning rate: milestone is a list/tuple\"\"\"\n\n    def to(epoch):\n        if epoch <= args.warmup:\n            return 1\n        elif args.warmup < epoch <= milestones[0]:\n            return 0\n        for i in range(1, len(milestones)):\n            if milestones[i - 1] < epoch <= milestones[i]:\n                return i\n        return len(milestones)\n\n    n = to(epoch)\n\n    global lr\n    lr = args.base_lr * (0.2 ** n)\n    for param_group in optimizer.param_groups:\n        param_group['lr'] = lr\n\n\ndef save_checkpoint(state, filename='checkpoint.pth.tar'):\n    torch.save(state, filename)\n    logging.info(f'Save checkpoint to {filename}')\n\n\ndef train(train_loader, model, criterion, optimizer, epoch):\n    batch_time = AverageMeter()\n    data_time = AverageMeter()\n    losses = AverageMeter()\n\n    model.train()\n\n    end = time.time()\n    # loader is batch style\n    # for i, (input, target) in enumerate(train_loader):\n    for i, (input, target) in enumerate(train_loader):\n        target.requires_grad = False\n        target = target.cuda(non_blocking=True)\n        output = model(input)\n\n        data_time.update(time.time() - end)\n\n        if args.loss.lower() == 'vdc':\n            loss = criterion(output, target)\n        elif args.loss.lower() == 'wpdc':\n            loss = criterion(output, target)\n        elif args.loss.lower() == 'pdc':\n            loss = criterion(output, target)\n        else:\n            raise Exception(f'Unknown loss {args.loss}')\n\n        losses.update(loss.item(), input.size(0))\n        # compute gradient and do SGD step\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        # measure elapsed time\n        batch_time.update(time.time() - end)\n        end = time.time()\n\n        # log\n        if i % args.print_freq == 0:\n            logging.info(f'Epoch: [{epoch}][{i}/{len(train_loader)}]\\t'\n                         f'LR: {lr:8f}\\t'\n                         f'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n                         # f'Data {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n                         f'Loss {losses.val:.4f} ({losses.avg:.4f})')\n\n\ndef validate(val_loader, model, criterion, epoch):\n    model.eval()\n\n    end = time.time()\n    with torch.no_grad():\n        losses = []\n        for i, (input, target) in enumerate(val_loader):\n            # compute output\n            target.requires_grad = False\n            target = target.cuda(non_blocking=True)\n            output = model(input)\n\n            loss = criterion(output, target)\n            losses.append(loss.item())\n\n        elapse = time.time() - end\n        loss = np.mean(losses)\n        logging.info(f'Val: [{epoch}][{len(val_loader)}]\\t'\n                     f'Loss {loss:.4f}\\t'\n                     f'Time {elapse:.3f}')\n\n\ndef main():\n    parse_args()  # parse global argsl\n\n    # logging setup\n    logging.basicConfig(\n        format='[%(asctime)s] [p%(process)s] [%(pathname)s:%(lineno)d] [%(levelname)s] %(message)s',\n        level=logging.INFO,\n        handlers=[\n            logging.FileHandler(args.log_file, mode=args.log_mode),\n            logging.StreamHandler()\n        ]\n    )\n\n    print_args(args)  # print args\n\n    # step1: define the model structure\n    model = getattr(mobilenet_v1, args.arch)(num_classes=args.num_classes)\n\n    torch.cuda.set_device(args.devices_id[0])  # fix bug for `ERROR: all tensors must be on devices[0]`\n\n    model = nn.DataParallel(model, device_ids=args.devices_id).cuda()  # -> GPU\n\n    # step2: optimization: loss and optimization method\n    # criterion = nn.MSELoss(size_average=args.size_average).cuda()\n    if args.loss.lower() == 'wpdc':\n        print(args.opt_style)\n        criterion = WPDCLoss(opt_style=args.opt_style).cuda()\n        logging.info('Use WPDC Loss')\n    elif args.loss.lower() == 'vdc':\n        criterion = VDCLoss(opt_style=args.opt_style).cuda()\n        logging.info('Use VDC Loss')\n    elif args.loss.lower() == 'pdc':\n        criterion = nn.MSELoss(size_average=args.size_average).cuda()\n        logging.info('Use PDC loss')\n    else:\n        raise Exception(f'Unknown Loss {args.loss}')\n\n    optimizer = torch.optim.SGD(model.parameters(),\n                                lr=args.base_lr,\n                                momentum=args.momentum,\n                                weight_decay=args.weight_decay,\n                                nesterov=True)\n    # step 2.1 resume\n    if args.resume:\n        if Path(args.resume).is_file():\n            logging.info(f'=> loading checkpoint {args.resume}')\n\n            checkpoint = torch.load(args.resume, map_location=lambda storage, loc: storage)['state_dict']\n            # checkpoint = torch.load(args.resume)['state_dict']\n            model.load_state_dict(checkpoint)\n\n        else:\n            logging.info(f'=> no checkpoint found at {args.resume}')\n\n    # step3: data\n    normalize = NormalizeGjz(mean=127.5, std=128)  # may need optimization\n\n    train_dataset = DDFADataset(\n        root=args.root,\n        filelists=args.filelists_train,\n        param_fp=args.param_fp_train,\n        transform=transforms.Compose([ToTensorGjz(), normalize])\n    )\n    val_dataset = DDFADataset(\n        root=args.root,\n        filelists=args.filelists_val,\n        param_fp=args.param_fp_val,\n        transform=transforms.Compose([ToTensorGjz(), normalize])\n    )\n\n    train_loader = DataLoader(train_dataset, batch_size=args.batch_size, num_workers=args.workers,\n                              shuffle=True, pin_memory=True, drop_last=True)\n    val_loader = DataLoader(val_dataset, batch_size=args.val_batch_size, num_workers=args.workers,\n                            shuffle=False, pin_memory=True)\n\n    # step4: run\n    cudnn.benchmark = True\n    if args.test_initial:\n        logging.info('Testing from initial')\n        validate(val_loader, model, criterion, args.start_epoch)\n\n    for epoch in range(args.start_epoch, args.epochs + 1):\n        # adjust learning rate\n        adjust_learning_rate(optimizer, epoch, args.milestones)\n\n        # train for one epoch\n        train(train_loader, model, criterion, optimizer, epoch)\n        filename = f'{args.snapshot}_checkpoint_epoch_{epoch}.pth.tar'\n        save_checkpoint(\n            {\n                'epoch': epoch,\n                'state_dict': model.state_dict(),\n                # 'optimizer': optimizer.state_dict()\n            },\n            filename\n        )\n\n        validate(val_loader, model, criterion, epoch)\n\n\nif __name__ == '__main__':\n    main()\n"
        },
        {
          "name": "training",
          "type": "tree",
          "content": null
        },
        {
          "name": "utils",
          "type": "tree",
          "content": null
        },
        {
          "name": "vdc_loss.py",
          "type": "blob",
          "size": 3.521484375,
          "content": "#!/usr/bin/env python3\n# coding: utf-8\n\nimport torch\nimport torch.nn as nn\nfrom utils.io import _load, _numpy_to_cuda, _numpy_to_tensor\nfrom utils.params import *\n\n_to_tensor = _numpy_to_cuda  # gpu\n\n\ndef _parse_param_batch(param):\n    \"\"\"Work for both numpy and tensor\"\"\"\n    N = param.shape[0]\n    p_ = param[:, :12].view(N, 3, -1)\n    p = p_[:, :, :3]\n    offset = p_[:, :, -1].view(N, 3, 1)\n    alpha_shp = param[:, 12:52].view(N, -1, 1)\n    alpha_exp = param[:, 52:].view(N, -1, 1)\n    return p, offset, alpha_shp, alpha_exp\n\n\nclass VDCLoss(nn.Module):\n    def __init__(self, opt_style='all'):\n        super(VDCLoss, self).__init__()\n\n        self.u = _to_tensor(u)\n        self.param_mean = _to_tensor(param_mean)\n        self.param_std = _to_tensor(param_std)\n        self.w_shp = _to_tensor(w_shp)\n        self.w_exp = _to_tensor(w_exp)\n\n        self.keypoints = _to_tensor(keypoints)\n        self.u_base = self.u[self.keypoints]\n        self.w_shp_base = self.w_shp[self.keypoints]\n        self.w_exp_base = self.w_exp[self.keypoints]\n\n        self.w_shp_length = self.w_shp.shape[0] // 3\n\n        self.opt_style = opt_style\n\n    def reconstruct_and_parse(self, input, target):\n        # reconstruct\n        param = input * self.param_std + self.param_mean\n        param_gt = target * self.param_std + self.param_mean\n\n        # parse param\n        p, offset, alpha_shp, alpha_exp = _parse_param_batch(param)\n        pg, offsetg, alpha_shpg, alpha_expg = _parse_param_batch(param_gt)\n\n        return (p, offset, alpha_shp, alpha_exp), (pg, offsetg, alpha_shpg, alpha_expg)\n\n    def forward_all(self, input, target):\n        (p, offset, alpha_shp, alpha_exp), (pg, offsetg, alpha_shpg, alpha_expg) \\\n            = self.reconstruct_and_parse(input, target)\n\n        N = input.shape[0]\n        offset[:, -1] = offsetg[:, -1]\n        gt_vertex = pg @ (self.u + self.w_shp @ alpha_shpg + self.w_exp @ alpha_expg) \\\n            .view(N, -1, 3).permute(0, 2, 1) + offsetg\n        vertex = p @ (self.u + self.w_shp @ alpha_shp + self.w_exp @ alpha_exp) \\\n            .view(N, -1, 3).permute(0, 2, 1) + offset\n\n        diff = (gt_vertex - vertex) ** 2\n        loss = torch.mean(diff)\n        return loss\n\n    def forward_resample(self, input, target, resample_num=132):\n        (p, offset, alpha_shp, alpha_exp), (pg, offsetg, alpha_shpg, alpha_expg) \\\n            = self.reconstruct_and_parse(input, target)\n\n        # resample index\n        index = torch.randperm(self.w_shp_length)[:resample_num].reshape(-1, 1)\n        keypoints_resample = torch.cat((3 * index, 3 * index + 1, 3 * index + 2), dim=1).view(-1).cuda()\n        keypoints_mix = torch.cat((self.keypoints, keypoints_resample))\n        w_shp_base = self.w_shp[keypoints_mix]\n        u_base = self.u[keypoints_mix]\n        w_exp_base = self.w_exp[keypoints_mix]\n\n        offset[:, -1] = offsetg[:, -1]\n\n        N = input.shape[0]\n        gt_vertex = pg @ (u_base + w_shp_base @ alpha_shpg + w_exp_base @ alpha_expg) \\\n            .view(N, -1, 3).permute(0, 2, 1) + offsetg\n        vertex = p @ (u_base + w_shp_base @ alpha_shp + w_exp_base @ alpha_exp) \\\n            .view(N, -1, 3).permute(0, 2, 1) + offset\n        diff = (gt_vertex - vertex) ** 2\n        loss = torch.mean(diff)\n        return loss\n\n    def forward(self, input, target):\n        if self.opt_style == 'all':\n            return self.forward_all(input, target)\n        elif self.opt_style == 'resample':\n            return self.forward_resample(input, target)\n        else:\n            raise Exception(f'Unknown opt style: f{opt_style}')\n\n\nif __name__ == '__main__':\n    pass\n"
        },
        {
          "name": "video_demo.py",
          "type": "blob",
          "size": 3.46875,
          "content": "#!/usr/bin/env python3\n# coding: utf-8\nimport torch\nimport torchvision.transforms as transforms\nimport mobilenet_v1\nimport numpy as np\nimport cv2\nimport dlib\nfrom utils.ddfa import ToTensorGjz, NormalizeGjz\nimport scipy.io as sio\nfrom utils.inference import (\n    parse_roi_box_from_landmark,\n    crop_img,\n    predict_68pts,\n    predict_dense,\n)\nfrom utils.cv_plot import plot_kpt\nfrom utils.render import get_depths_image, cget_depths_image, cpncc\nfrom utils.paf import gen_img_paf\nimport argparse\nimport torch.backends.cudnn as cudnn\n\nSTD_SIZE = 120\n\n\ndef main(args):\n    # 0. open video\n    # vc = cv2.VideoCapture(str(args.video) if len(args.video) == 1 else args.video)\n    vc = cv2.VideoCapture(args.video if int(args.video) != 0 else 0)\n\n    # 1. load pre-tained model\n    checkpoint_fp = 'models/phase1_wpdc_vdc.pth.tar'\n    arch = 'mobilenet_1'\n\n    tri = sio.loadmat('visualize/tri.mat')['tri']\n    transform = transforms.Compose([ToTensorGjz(), NormalizeGjz(mean=127.5, std=128)])\n\n    checkpoint = torch.load(checkpoint_fp, map_location=lambda storage, loc: storage)[\n        'state_dict'\n    ]\n    model = getattr(mobilenet_v1, arch)(\n        num_classes=62\n    )  # 62 = 12(pose) + 40(shape) +10(expression)\n\n    model_dict = model.state_dict()\n    # because the model is trained by multiple gpus, prefix module should be removed\n    for k in checkpoint.keys():\n        model_dict[k.replace('module.', '')] = checkpoint[k]\n    model.load_state_dict(model_dict)\n    if args.mode == 'gpu':\n        cudnn.benchmark = True\n        model = model.cuda()\n    model.eval()\n\n    # 2. load dlib model for face detection and landmark used for face cropping\n    dlib_landmark_model = 'models/shape_predictor_68_face_landmarks.dat'\n    face_regressor = dlib.shape_predictor(dlib_landmark_model)\n    face_detector = dlib.get_frontal_face_detector()\n\n    # 3. forward\n    success, frame = vc.read()\n    last_frame_pts = []\n\n    while success:\n        if len(last_frame_pts) == 0:\n            rects = face_detector(frame, 1)\n            for rect in rects:\n                pts = face_regressor(frame, rect).parts()\n                pts = np.array([[pt.x, pt.y] for pt in pts]).T\n                last_frame_pts.append(pts)\n\n        vertices_lst = []\n        for lmk in last_frame_pts:\n            roi_box = parse_roi_box_from_landmark(lmk)\n            img = crop_img(frame, roi_box)\n            img = cv2.resize(\n                img, dsize=(STD_SIZE, STD_SIZE), interpolation=cv2.INTER_LINEAR\n            )\n            input = transform(img).unsqueeze(0)\n            with torch.no_grad():\n                if args.mode == 'gpu':\n                    input = input.cuda()\n                param = model(input)\n                param = param.squeeze().cpu().numpy().flatten().astype(np.float32)\n            pts68 = predict_68pts(param, roi_box)\n            vertex = predict_dense(param, roi_box)\n            lmk[:] = pts68[:2]\n            vertices_lst.append(vertex)\n\n        pncc = cpncc(frame, vertices_lst, tri - 1) / 255.0\n        frame = frame / 255.0 * (1.0 - pncc)\n        cv2.imshow('3ddfa', frame)\n        cv2.waitKey(1)\n        success, frame = vc.read()\n\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser(description='3DDFA inference pipeline')\n    parser.add_argument(\n        '-v',\n        '--video',\n        default='0',\n        type=str,\n        help='video file path or opencv cam index',\n    )\n    parser.add_argument('-m', '--mode', default='cpu', type=str, help='gpu or cpu mode')\n\n    args = parser.parse_args()\n    main(args)\n"
        },
        {
          "name": "visualize.py",
          "type": "blob",
          "size": 3.427734375,
          "content": "#!/usr/bin/env python3\n# coding: utf-8\n\nfrom benchmark import extract_param\nfrom utils.ddfa import reconstruct_vertex\nfrom utils.io import _dump, _load\nimport os.path as osp\nfrom skimage import io\nimport matplotlib.pyplot as plt\nfrom benchmark_aflw2000 import convert_to_ori\nimport scipy.io as sio\n\n\ndef aflw2000():\n    arch = 'mobilenet_1'\n    device_ids = [0]\n    checkpoint_fp = 'models/phase1_wpdc_vdc.pth.tar'\n\n    params = extract_param(\n        checkpoint_fp=checkpoint_fp,\n        root='test.data/AFLW2000-3D_crop',\n        filelists='test.data/AFLW2000-3D_crop.list',\n        arch=arch,\n        device_ids=device_ids,\n        batch_size=128)\n    _dump('res/params_aflw2000.npy', params)\n\n\ndef draw_landmarks():\n    filelists = 'test.data/AFLW2000-3D_crop.list'\n    root = 'AFLW-2000-3D/'\n    fns = open(filelists).read().strip().split('\\n')\n    params = _load('res/params_aflw2000.npy')\n\n    for i in range(2000):\n        plt.close()\n        img_fp = osp.join(root, fns[i])\n        img = io.imread(img_fp)\n        lms = reconstruct_vertex(params[i], dense=False)\n        lms = convert_to_ori(lms, i)\n\n        # print(lms.shape)\n        fig = plt.figure(figsize=plt.figaspect(.5))\n        # fig = plt.figure(figsize=(8, 4))\n        ax = fig.add_subplot(1, 2, 1)\n        ax.imshow(img)\n\n        alpha = 0.8\n        markersize = 4\n        lw = 1.5\n        color = 'w'\n        markeredgecolor = 'black'\n\n        nums = [0, 17, 22, 27, 31, 36, 42, 48, 60, 68]\n        for ind in range(len(nums) - 1):\n            l, r = nums[ind], nums[ind + 1]\n            ax.plot(lms[0, l:r], lms[1, l:r], color=color, lw=lw, alpha=alpha - 0.1)\n\n            ax.plot(lms[0, l:r], lms[1, l:r], marker='o', linestyle='None', markersize=markersize, color=color,\n                    markeredgecolor=markeredgecolor, alpha=alpha)\n\n        ax.axis('off')\n\n        # 3D\n        ax = fig.add_subplot(1, 2, 2, projection='3d')\n        lms[1] = img.shape[1] - lms[1]\n        lms[2] = -lms[2]\n\n        # print(lms)\n        ax.scatter(lms[0], lms[2], lms[1], c=\"cyan\", alpha=1.0, edgecolor='b')\n\n        for ind in range(len(nums) - 1):\n            l, r = nums[ind], nums[ind + 1]\n            ax.plot3D(lms[0, l:r], lms[2, l:r], lms[1, l:r], color='blue')\n\n        ax.view_init(elev=5., azim=-95)\n        # ax.set_xlabel('x')\n        # ax.set_ylabel('y')\n        # ax.set_zlabel('z')\n\n        ax.set_xticklabels([])\n        ax.set_yticklabels([])\n        ax.set_zticklabels([])\n\n        plt.tight_layout()\n        # plt.show()\n\n        wfp = f'res/AFLW-2000-3D/{osp.basename(img_fp)}'\n        plt.savefig(wfp, dpi=200)\n\n\ndef gen_3d_vertex():\n    filelists = 'test.data/AFLW2000-3D_crop.list'\n    root = 'AFLW-2000-3D/'\n    fns = open(filelists).read().strip().split('\\n')\n    params = _load('res/params_aflw2000.npy')\n\n    sel = ['00427', '00439', '00475', '00477', '00497', '00514', '00562', '00623', '01045', '01095', '01104', '01506',\n           '01621', '02214', '02244', '03906', '04157']\n    sel = list(map(lambda x: f'image{x}.jpg', sel))\n    for i in range(2000):\n        fn = fns[i]\n        if fn in sel:\n            vertex = reconstruct_vertex(params[i], dense=True)\n            wfp = osp.join('res/AFLW-2000-3D_vertex/', fn.replace('.jpg', '.mat'))\n            print(wfp)\n            sio.savemat(wfp, {'vertex': vertex})\n\n\ndef main():\n    # step1: extract params\n    # aflw2000()\n\n    # step2: draw landmarks\n    # draw_landmarks()\n\n    # step3: visual 3d vertex\n    gen_3d_vertex()\n\n\nif __name__ == '__main__':\n    main()\n"
        },
        {
          "name": "visualize",
          "type": "tree",
          "content": null
        },
        {
          "name": "wpdc_loss.py",
          "type": "blob",
          "size": 4.43359375,
          "content": "#!/usr/bin/env python3\n# coding: utf-8\n\nimport torch\nimport torch.nn as nn\nfrom math import sqrt\nfrom utils.io import _numpy_to_cuda\nfrom utils.params import *\n\n_to_tensor = _numpy_to_cuda  # gpu\n\n\ndef _parse_param_batch(param):\n    \"\"\"Work for both numpy and tensor\"\"\"\n    N = param.shape[0]\n    p_ = param[:, :12].view(N, 3, -1)\n    p = p_[:, :, :3]\n    offset = p_[:, :, -1].view(N, 3, 1)\n    alpha_shp = param[:, 12:52].view(N, -1, 1)\n    alpha_exp = param[:, 52:].view(N, -1, 1)\n    return p, offset, alpha_shp, alpha_exp\n\n\nclass WPDCLoss(nn.Module):\n    \"\"\"Input and target are all 62-d param\"\"\"\n\n    def __init__(self, opt_style='resample', resample_num=132):\n        super(WPDCLoss, self).__init__()\n        self.opt_style = opt_style\n        self.param_mean = _to_tensor(param_mean)\n        self.param_std = _to_tensor(param_std)\n\n        self.u = _to_tensor(u)\n        self.w_shp = _to_tensor(w_shp)\n        self.w_exp = _to_tensor(w_exp)\n        self.w_norm = _to_tensor(w_norm)\n\n        self.w_shp_length = self.w_shp.shape[0] // 3\n        self.keypoints = _to_tensor(keypoints)\n        self.resample_num = resample_num\n\n    def reconstruct_and_parse(self, input, target):\n        # reconstruct\n        param = input * self.param_std + self.param_mean\n        param_gt = target * self.param_std + self.param_mean\n\n        # parse param\n        p, offset, alpha_shp, alpha_exp = _parse_param_batch(param)\n        pg, offsetg, alpha_shpg, alpha_expg = _parse_param_batch(param_gt)\n\n        return (p, offset, alpha_shp, alpha_exp), (pg, offsetg, alpha_shpg, alpha_expg)\n\n    def _calc_weights_resample(self, input_, target_):\n        # resample index\n        if self.resample_num <= 0:\n            keypoints_mix = self.keypoints\n        else:\n            index = torch.randperm(self.w_shp_length)[:self.resample_num].reshape(-1, 1)\n            keypoints_resample = torch.cat((3 * index, 3 * index + 1, 3 * index + 2), dim=1).view(-1).cuda()\n            keypoints_mix = torch.cat((self.keypoints, keypoints_resample))\n        w_shp_base = self.w_shp[keypoints_mix]\n        u_base = self.u[keypoints_mix]\n        w_exp_base = self.w_exp[keypoints_mix]\n\n        input = torch.tensor(input_.data.clone(), requires_grad=False)\n        target = torch.tensor(target_.data.clone(), requires_grad=False)\n\n        (p, offset, alpha_shp, alpha_exp), (pg, offsetg, alpha_shpg, alpha_expg) \\\n            = self.reconstruct_and_parse(input, target)\n\n        input = self.param_std * input + self.param_mean\n        target = self.param_std * target + self.param_mean\n\n        N = input.shape[0]\n\n        offset[:, -1] = offsetg[:, -1]\n\n        weights = torch.zeros_like(input, dtype=torch.float)\n        tmpv = (u_base + w_shp_base @ alpha_shpg + w_exp_base @ alpha_expg).view(N, -1, 3).permute(0, 2, 1)\n\n        tmpv_norm = torch.norm(tmpv, dim=2)\n        offset_norm = sqrt(w_shp_base.shape[0] // 3)\n\n        # for pose\n        param_diff_pose = torch.abs(input[:, :11] - target[:, :11])\n        for ind in range(11):\n            if ind in [0, 4, 8]:\n                weights[:, ind] = param_diff_pose[:, ind] * tmpv_norm[:, 0]\n            elif ind in [1, 5, 9]:\n                weights[:, ind] = param_diff_pose[:, ind] * tmpv_norm[:, 1]\n            elif ind in [2, 6, 10]:\n                weights[:, ind] = param_diff_pose[:, ind] * tmpv_norm[:, 2]\n            else:\n                weights[:, ind] = param_diff_pose[:, ind] * offset_norm\n\n        ## This is the optimizest version\n        # for shape_exp\n        magic_number = 0.00057339936  # scale\n        param_diff_shape_exp = torch.abs(input[:, 12:] - target[:, 12:])\n        # weights[:, 12:] = magic_number * param_diff_shape_exp * self.w_norm\n        w = torch.cat((w_shp_base, w_exp_base), dim=1)\n        w_norm = torch.norm(w, dim=0)\n        # print('here')\n        weights[:, 12:] = magic_number * param_diff_shape_exp * w_norm\n\n        eps = 1e-6\n        weights[:, :11] += eps\n        weights[:, 12:] += eps\n\n        # normalize the weights\n        maxes, _ = weights.max(dim=1)\n        maxes = maxes.view(-1, 1)\n        weights /= maxes\n\n        # zero the z\n        weights[:, 11] = 0\n\n        return weights\n\n    def forward(self, input, target, weights_scale=10):\n        if self.opt_style == 'resample':\n            weights = self._calc_weights_resample(input, target)\n            loss = weights * (input - target) ** 2\n            return loss.mean()\n        else:\n            raise Exception(f'Unknown opt style: {self.opt_style}')\n\n\nif __name__ == '__main__':\n    pass\n"
        }
      ]
    }
  ]
}