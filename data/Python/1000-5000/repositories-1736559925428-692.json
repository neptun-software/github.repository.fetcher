{
  "metadata": {
    "timestamp": 1736559925428,
    "page": 692,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjcwMA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "pavelgonchar/colornet",
      "stars": 3564,
      "defaultBranch": "master",
      "files": [
        {
          "name": "README.md",
          "type": "blob",
          "size": 2.0283203125,
          "content": "#colornet\nNeural Network to colorize grayscale images\n\nResults\n-------\n\n|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Grayscale&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Prediction&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Ground Truth&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|\n|---|---|---|\n\n![grayscale-pred-groundtruth](summary/295000_0.png?raw=true \"grayscale-pred-groundtruth-295000\")\n\n![grayscale-pred-groundtruth](summary/496000_0.png?raw=true \"grayscale-pred-groundtruth-496000\")\n\n![grayscale-pred-groundtruth](summary/88000_0.png?raw=true \"grayscale-pred-groundtruth-88000\")\n\n![grayscale-pred-groundtruth](summary/99000_0.png?raw=true \"grayscale-pred-groundtruth-99000\")\n\n![grayscale-pred-groundtruth](summary/268000_0.png?raw=true \"grayscale-pred-groundtruth-268000\")\n\n![grayscale-pred-groundtruth](summary/209000_0.png?raw=true \"grayscale-pred-groundtruth-209000\")\n\n![grayscale-pred-groundtruth](summary/264000_0.png?raw=true \"grayscale-pred-groundtruth-264000\")\n\n![grayscale-pred-groundtruth](summary/329000_0.png?raw=true \"grayscale-pred-groundtruth-329000\")\n\n![grayscale-pred-groundtruth](summary/428000_0.png?raw=true \"grayscale-pred-groundtruth-428000\")\n\nEiji K used colornet for anime colorization\n#\n[![Sally the Witch 1966](http://c2n.me/3xw6w3D.jpg)](http://www.youtube.com/watch?v=ysb_5ezwIeM)\n\nSources\n-------\n- [Automatic Colorization](http://tinyclouds.org/colorize/)\n- [Hypercolumns for Object Segmentation and Fine-grained Localization](http://arxiv.org/pdf/1411.5752v2.pdf)\n- [ILSVRC-2014 model (VGG team) with 16 weight layers](https://gist.github.com/ksimonyan/211839e770f7b538e2d8#file-readme-md) and [Tensorflow version](https://github.com/ry/tensorflow-vgg16)\n- [YUV from Wikipedia](https://en.wikipedia.org/wiki/YUV)\n"
        },
        {
          "name": "batchnorm.py",
          "type": "blob",
          "size": 3.009765625,
          "content": "\"\"\"A helper class for managing batch normalization state.                   \n\nThis class is designed to simplify adding batch normalization               \n(http://arxiv.org/pdf/1502.03167v3.pdf) to your model by                    \nmanaging the state variables associated with it.                            \n\nImportant use note:  The function get_assigner() returns                    \nan op that must be executed to save the updated state.                      \nA suggested way to do this is to make execution of the                      \nmodel optimizer force it, e.g., by:                                         \n\n  update_assignments = tf.group(bn1.get_assigner(),                         \n                                bn2.get_assigner())                         \n  with tf.control_dependencies([optimizer]):                                \n    optimizer = tf.group(update_assignments)                                \n\n\"\"\"\n\nimport tensorflow as tf\n\n\nclass ConvolutionalBatchNormalizer(object):\n  \"\"\"Helper class that groups the normalization logic and variables.        \n\n  Use:                                                                      \n      ewma = tf.train.ExponentialMovingAverage(decay=0.99)                  \n      bn = ConvolutionalBatchNormalizer(depth, 0.001, ewma, True)           \n      update_assignments = bn.get_assigner()                                \n      x = bn.normalize(y, train=training?)                                  \n      (the output x will be batch-normalized).                              \n  \"\"\"\n\n  def __init__(self, depth, epsilon, ewma_trainer, scale_after_norm):\n    self.mean = tf.Variable(tf.constant(0.0, shape=[depth]),\n                            trainable=False)\n    self.variance = tf.Variable(tf.constant(1.0, shape=[depth]),\n                                trainable=False)\n    self.beta = tf.Variable(tf.constant(0.0, shape=[depth]))\n    self.gamma = tf.Variable(tf.constant(1.0, shape=[depth]))\n    self.ewma_trainer = ewma_trainer\n    self.epsilon = epsilon\n    self.scale_after_norm = scale_after_norm\n\n  def get_assigner(self):\n    \"\"\"Returns an EWMA apply op that must be invoked after optimization.\"\"\"\n    return self.ewma_trainer.apply([self.mean, self.variance])\n\n  def normalize(self, x, train=True):\n    \"\"\"Returns a batch-normalized version of x.\"\"\"\n    if train:\n      mean, variance = tf.nn.moments(x, [0, 1, 2])\n      assign_mean = self.mean.assign(mean)\n      assign_variance = self.variance.assign(variance)\n      with tf.control_dependencies([assign_mean, assign_variance]):\n        return tf.nn.batch_norm_with_global_normalization(\n            x, mean, variance, self.beta, self.gamma,\n            self.epsilon, self.scale_after_norm)\n    else:\n      mean = self.ewma_trainer.average(self.mean)\n      variance = self.ewma_trainer.average(self.variance)\n      local_beta = tf.identity(self.beta)\n      local_gamma = tf.identity(self.gamma)\n      return tf.nn.batch_norm_with_global_normalization(\n          x, mean, variance, local_beta, local_gamma,\n          self.epsilon, self.scale_after_norm)"
        },
        {
          "name": "summary",
          "type": "tree",
          "content": null
        },
        {
          "name": "train.py",
          "type": "blob",
          "size": 9.9345703125,
          "content": "import tensorflow as tf\nimport numpy as np\nimport glob\nimport sys\nfrom matplotlib import pyplot as plt\nfrom batchnorm import ConvolutionalBatchNormalizer\n\nfilenames = sorted(glob.glob(\"../colornet/*/*.jpg\"))\nbatch_size = 1\nnum_epochs = 1e+9\n\nglobal_step = tf.Variable(0, name='global_step', trainable=False)\nphase_train = tf.placeholder(tf.bool, name='phase_train')\nuv = tf.placeholder(tf.uint8, name='uv')\n\n\ndef read_my_file_format(filename_queue, randomize=False):\n    reader = tf.WholeFileReader()\n    key, file = reader.read(filename_queue)\n    uint8image = tf.image.decode_jpeg(file, channels=3)\n    uint8image = tf.random_crop(uint8image, (224, 224, 3))\n    if randomize:\n        uint8image = tf.image.random_flip_left_right(uint8image)\n        uint8image = tf.image.random_flip_up_down(uint8image, seed=None)\n    float_image = tf.div(tf.cast(uint8image, tf.float32), 255)\n    return float_image\n\n\ndef input_pipeline(filenames, batch_size, num_epochs=None):\n    filename_queue = tf.train.string_input_producer(\n        filenames, num_epochs=num_epochs, shuffle=False)\n    example = read_my_file_format(filename_queue, randomize=False)\n    min_after_dequeue = 100\n    capacity = min_after_dequeue + 3 * batch_size\n    example_batch = tf.train.shuffle_batch(\n        [example], batch_size=batch_size, capacity=capacity,\n        min_after_dequeue=min_after_dequeue)\n    return example_batch\n\n\ndef batch_norm(x, depth, phase_train):\n    with tf.variable_scope('batchnorm'):\n        ewma = tf.train.ExponentialMovingAverage(decay=0.9999)\n        bn = ConvolutionalBatchNormalizer(depth, 0.001, ewma, True)\n        update_assignments = bn.get_assigner()\n        x = bn.normalize(x, train=phase_train)\n    return x\n\n\ndef conv2d(_X, w, sigmoid=False, bn=False):\n    with tf.variable_scope('conv2d'):\n        _X = tf.nn.conv2d(_X, w, [1, 1, 1, 1], 'SAME')\n        if bn:\n            _X = batch_norm(_X, w.get_shape()[3], phase_train)\n        if sigmoid:\n            return tf.sigmoid(_X)\n        else:\n            _X = tf.nn.relu(_X)\n            return tf.maximum(0.01 * _X, _X)\n\n\ndef colornet(_tensors):\n    \"\"\"\n    Network architecture http://tinyclouds.org/colorize/residual_encoder.png\n    \"\"\"\n    with tf.variable_scope('colornet'):\n        # Bx28x28x512 -> batch norm -> 1x1 conv = Bx28x28x256\n        conv1 = tf.nn.relu(tf.nn.conv2d(batch_norm(_tensors[\n                           \"conv4_3\"], 512, phase_train),\n            _tensors[\"weights\"][\"wc1\"], [1, 1, 1, 1], 'SAME'))\n        # upscale to 56x56x256\n        conv1 = tf.image.resize_bilinear(conv1, (56, 56))\n        conv1 = tf.add(conv1, batch_norm(\n            _tensors[\"conv3_3\"], 256, phase_train))\n\n        # Bx56x56x256-> 3x3 conv = Bx56x56x128\n        conv2 = conv2d(conv1, _tensors[\"weights\"][\n                       'wc2'], sigmoid=False, bn=True)\n        # upscale to 112x112x128\n        conv2 = tf.image.resize_bilinear(conv2, (112, 112))\n        conv2 = tf.add(conv2, batch_norm(\n            _tensors[\"conv2_2\"], 128, phase_train))\n\n        # Bx112x112x128 -> 3x3 conv = Bx112x112x64\n        conv3 = conv2d(conv2, _tensors[\"weights\"][\n                       'wc3'], sigmoid=False, bn=True)\n        # upscale to Bx224x224x64\n        conv3 = tf.image.resize_bilinear(conv3, (224, 224))\n        conv3 = tf.add(conv3, batch_norm(_tensors[\"conv1_2\"], 64, phase_train))\n\n        # Bx224x224x64 -> 3x3 conv = Bx224x224x3\n        conv4 = conv2d(conv3, _tensors[\"weights\"][\n                       'wc4'], sigmoid=False, bn=True)\n        conv4 = tf.add(conv4, batch_norm(\n            _tensors[\"grayscale\"], 3, phase_train))\n\n        # Bx224x224x3 -> 3x3 conv = Bx224x224x3\n        conv5 = conv2d(conv4, _tensors[\"weights\"][\n                       'wc5'], sigmoid=False, bn=True)\n        # Bx224x224x3 -> 3x3 conv = Bx224x224x2\n        conv6 = conv2d(conv5, _tensors[\"weights\"][\n                       'wc6'], sigmoid=True, bn=True)\n\n    return conv6\n\n\ndef concat_images(imga, imgb):\n    \"\"\"\n    Combines two color image ndarrays side-by-side.\n    \"\"\"\n    ha, wa = imga.shape[:2]\n    hb, wb = imgb.shape[:2]\n    max_height = np.max([ha, hb])\n    total_width = wa + wb\n    new_img = np.zeros(shape=(max_height, total_width, 3), dtype=np.float32)\n    new_img[:ha, :wa] = imga\n    new_img[:hb, wa:wa + wb] = imgb\n    return new_img\n\n\ndef rgb2yuv(rgb):\n    \"\"\"\n    Convert RGB image into YUV https://en.wikipedia.org/wiki/YUV\n    \"\"\"\n    rgb2yuv_filter = tf.constant(\n        [[[[0.299, -0.169, 0.499],\n           [0.587, -0.331, -0.418],\n            [0.114, 0.499, -0.0813]]]])\n    rgb2yuv_bias = tf.constant([0., 0.5, 0.5])\n\n    temp = tf.nn.conv2d(rgb, rgb2yuv_filter, [1, 1, 1, 1], 'SAME')\n    temp = tf.nn.bias_add(temp, rgb2yuv_bias)\n\n    return temp\n\n\ndef yuv2rgb(yuv):\n    \"\"\"\n    Convert YUV image into RGB https://en.wikipedia.org/wiki/YUV\n    \"\"\"\n    yuv = tf.mul(yuv, 255)\n    yuv2rgb_filter = tf.constant(\n        [[[[1., 1., 1.],\n           [0., -0.34413999, 1.77199996],\n            [1.40199995, -0.71414, 0.]]]])\n    yuv2rgb_bias = tf.constant([-179.45599365, 135.45983887, -226.81599426])\n    temp = tf.nn.conv2d(yuv, yuv2rgb_filter, [1, 1, 1, 1], 'SAME')\n    temp = tf.nn.bias_add(temp, yuv2rgb_bias)\n    temp = tf.maximum(temp, tf.zeros(temp.get_shape(), dtype=tf.float32))\n    temp = tf.minimum(temp, tf.mul(\n        tf.ones(temp.get_shape(), dtype=tf.float32), 255))\n    temp = tf.div(temp, 255)\n    return temp\n\n\nwith open(\"vgg/tensorflow-vgg16/vgg16-20160129.tfmodel\", mode='rb') as f:\n    fileContent = f.read()\n\ngraph_def = tf.GraphDef()\ngraph_def.ParseFromString(fileContent)\n\nwith tf.variable_scope('colornet'):\n    # Store layers weight\n    weights = {\n        # 1x1 conv, 512 inputs, 256 outputs\n        'wc1': tf.Variable(tf.truncated_normal([1, 1, 512, 256], stddev=0.01)),\n        # 3x3 conv, 512 inputs, 128 outputs\n        'wc2': tf.Variable(tf.truncated_normal([3, 3, 256, 128], stddev=0.01)),\n        # 3x3 conv, 256 inputs, 64 outputs\n        'wc3': tf.Variable(tf.truncated_normal([3, 3, 128, 64], stddev=0.01)),\n        # 3x3 conv, 128 inputs, 3 outputs\n        'wc4': tf.Variable(tf.truncated_normal([3, 3, 64, 3], stddev=0.01)),\n        # 3x3 conv, 6 inputs, 3 outputs\n        'wc5': tf.Variable(tf.truncated_normal([3, 3, 3, 3], stddev=0.01)),\n        # 3x3 conv, 3 inputs, 2 outputs\n        'wc6': tf.Variable(tf.truncated_normal([3, 3, 3, 2], stddev=0.01)),\n    }\n\ncolorimage = input_pipeline(filenames, batch_size, num_epochs=num_epochs)\ncolorimage_yuv = rgb2yuv(colorimage)\n\ngrayscale = tf.image.rgb_to_grayscale(colorimage)\ngrayscale_rgb = tf.image.grayscale_to_rgb(grayscale)\ngrayscale_yuv = rgb2yuv(grayscale_rgb)\ngrayscale = tf.concat(3, [grayscale, grayscale, grayscale])\n\ntf.import_graph_def(graph_def, input_map={\"images\": grayscale})\n\ngraph = tf.get_default_graph()\n\nwith tf.variable_scope('vgg'):\n    conv1_2 = graph.get_tensor_by_name(\"import/conv1_2/Relu:0\")\n    conv2_2 = graph.get_tensor_by_name(\"import/conv2_2/Relu:0\")\n    conv3_3 = graph.get_tensor_by_name(\"import/conv3_3/Relu:0\")\n    conv4_3 = graph.get_tensor_by_name(\"import/conv4_3/Relu:0\")\n\ntensors = {\n    \"conv1_2\": conv1_2,\n    \"conv2_2\": conv2_2,\n    \"conv3_3\": conv3_3,\n    \"conv4_3\": conv4_3,\n    \"grayscale\": grayscale,\n    \"weights\": weights\n}\n\n# Construct model\npred = colornet(tensors)\npred_yuv = tf.concat(3, [tf.split(3, 3, grayscale_yuv)[0], pred])\npred_rgb = yuv2rgb(pred_yuv)\n\nloss = tf.square(tf.sub(pred, tf.concat(\n    3, [tf.split(3, 3, colorimage_yuv)[1], tf.split(3, 3, colorimage_yuv)[2]])))\n\nif uv == 1:\n    loss = tf.split(3, 2, loss)[0]\nelif uv == 2:\n    loss = tf.split(3, 2, loss)[1]\nelse:\n    loss = (tf.split(3, 2, loss)[0] + tf.split(3, 2, loss)[1]) / 2\n\nif phase_train:\n    optimizer = tf.train.GradientDescentOptimizer(0.0001)\n    opt = optimizer.minimize(\n        loss, global_step=global_step, gate_gradients=optimizer.GATE_NONE)\n\n# Summaries\ntf.histogram_summary(\"weights1\", weights[\"wc1\"])\ntf.histogram_summary(\"weights2\", weights[\"wc2\"])\ntf.histogram_summary(\"weights3\", weights[\"wc3\"])\ntf.histogram_summary(\"weights4\", weights[\"wc4\"])\ntf.histogram_summary(\"weights5\", weights[\"wc5\"])\ntf.histogram_summary(\"weights6\", weights[\"wc6\"])\ntf.histogram_summary(\"instant_loss\", tf.reduce_mean(loss))\ntf.image_summary(\"colorimage\", colorimage, max_images=1)\ntf.image_summary(\"pred_rgb\", pred_rgb, max_images=1)\ntf.image_summary(\"grayscale\", grayscale_rgb, max_images=1)\n\n# Saver.\nsaver = tf.train.Saver()\n\n# Create the graph, etc.\ninit_op = tf.initialize_all_variables()\n\n# Create a session for running operations in the Graph.\nsess = tf.Session()\n\n# Initialize the variables.\nsess.run(init_op)\n\nmerged = tf.merge_all_summaries()\nwriter = tf.train.SummaryWriter(\"tb_log\", sess.graph_def)\n\n# Start input enqueue threads.\ncoord = tf.train.Coordinator()\nthreads = tf.train.start_queue_runners(sess=sess, coord=coord)\n\ntry:\n    while not coord.should_stop():\n        # Run training steps\n        training_opt = sess.run(opt, feed_dict={phase_train: True, uv: 1})\n        training_opt = sess.run(opt, feed_dict={phase_train: True, uv: 2})\n\n        step = sess.run(global_step)\n\n        if step % 1 == 0:\n            pred_, pred_rgb_, colorimage_, grayscale_rgb_, cost, merged_ = sess.run(\n                [pred, pred_rgb, colorimage, grayscale_rgb, loss, merged], feed_dict={phase_train: False, uv: 3})\n            print {\n                \"step\": step,\n                \"cost\": np.mean(cost)\n            }\n            if step % 1000 == 0:\n                summary_image = concat_images(grayscale_rgb_[0], pred_rgb_[0])\n                summary_image = concat_images(summary_image, colorimage_[0])\n                plt.imsave(\"summary/\" + str(step) + \"_0\", summary_image)\n\n            sys.stdout.flush()\n            writer.add_summary(merged_, step)\n            writer.flush()\n        if step % 100000 == 99998:\n            save_path = saver.save(sess, \"model.ckpt\")\n            print(\"Model saved in file: %s\" % save_path)\n            sys.stdout.flush()\n\nexcept tf.errors.OutOfRangeError:\n    print('Done training -- epoch limit reached')\nfinally:\n    # When done, ask the threads to stop.\n    coord.request_stop()\n\n# Wait for threads to finish.\ncoord.join(threads)\nsess.close()\n"
        },
        {
          "name": "vgg",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}