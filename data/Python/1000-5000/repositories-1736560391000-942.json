{
  "metadata": {
    "timestamp": 1736560391000,
    "page": 942,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjk1MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "wenge-research/YAYI",
      "stars": 3258,
      "defaultBranch": "main",
      "files": [
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 1.796875,
          "content": "# Byte-compiled / optimized / DLL files\n__pycache__/\n*.py[cod]\n*$py.class\n\n# C extensions\n*.so\n\n# Distribution / packaging\n.Python\nbuild/\ndevelop-eggs/\ndist/\ndownloads/\neggs/\n.eggs/\nlib/\nlib64/\nparts/\nsdist/\nvar/\nwheels/\npip-wheel-metadata/\nshare/python-wheels/\n*.egg-info/\n.installed.cfg\n*.egg\nMANIFEST\n\n# PyInstaller\n#  Usually these files are written by a python script from a template\n#  before PyInstaller builds the exe, so as to inject date/other infos into it.\n*.manifest\n*.spec\n\n# Installer logs\npip-log.txt\npip-delete-this-directory.txt\n\n# Unit test / coverage reports\nhtmlcov/\n.tox/\n.nox/\n.coverage\n.coverage.*\n.cache\nnosetests.xml\ncoverage.xml\n*.cover\n*.py,cover\n.hypothesis/\n.pytest_cache/\n\n# Translations\n*.mo\n*.pot\n\n# Django stuff:\n*.log\nlocal_settings.py\ndb.sqlite3\ndb.sqlite3-journal\n\n# Flask stuff:\ninstance/\n.webassets-cache\n\n# Scrapy stuff:\n.scrapy\n\n# Sphinx documentation\ndocs/_build/\n\n# PyBuilder\ntarget/\n\n# Jupyter Notebook\n.ipynb_checkpoints\n\n# IPython\nprofile_default/\nipython_config.py\n\n# pyenv\n.python-version\n\n# pipenv\n#   According to pypa/pipenv#598, it is recommended to include Pipfile.lock in version control.\n#   However, in case of collaboration, if having platform-specific dependencies or dependencies\n#   having no cross-platform support, pipenv may install dependencies that don't work, or not\n#   install all needed dependencies.\n#Pipfile.lock\n\n# PEP 582; used by e.g. github.com/David-OConnor/pyflow\n__pypackages__/\n\n# Celery stuff\ncelerybeat-schedule\ncelerybeat.pid\n\n# SageMath parsed files\n*.sage.py\n\n# Environments\n.env\n.venv\nenv/\nvenv/\nENV/\nenv.bak/\nvenv.bak/\n.vscode\n\n# Spyder project settings\n.spyderproject\n.spyproject\n\n# Rope project settings\n.ropeproject\n\n# mkdocs documentation\n/site\n\n# mypy\n.mypy_cache/\n.dmypy.json\ndmypy.json\n\n# Pyre type checker\n.pyre/\n.DS_Store\n\n# others\ncheckpoints/*"
        },
        {
          "name": "DISCLAIMER",
          "type": "blob",
          "size": 3.4443359375,
          "content": "The software project, data, and models provided by our GitHub project are provided \"as is,\" without warranty of any kind, express or implied, including but not limited to the warranties of merchantability, fitness for a particular purpose, and non-infringement.\n\nIn no event shall the project owners or contributors be liable for any direct, indirect, incidental, special, exemplary, or consequential damages (including, but not limited to, procurement of substitute goods or services; loss of use, data, or profits; or business interruption) however caused and on any theory of liability, whether in contract, strict liability, or tort (including negligence or otherwise) arising in any way out of the use of this software project, data, or models, even if advised of the possibility of such damage.\n\nUsers of this software project, data, and models are solely responsible for any consequences of their use. The project owners and contributors shall not be held responsible for any subsequent or potential harm caused by the use of this software project, data, or models.\n\nBy using this software project, data, or models, users accept and agree to this disclaimer. If users do not agree to the terms of this disclaimer, they should not use this software project, data, or models.\n\nIt is important to note that this software project, data, and models are still in the research phase and are provided for experimental purposes only. As such, the project owners and contributors do not guarantee the accuracy, completeness, or usefulness of the software project, data, or models.\n\nFurthermore, due to the experimental nature of this software project, data, and models, it is possible that they may contain or generate inappropriate responses, errors, or inconsistencies. Users should exercise caution when using this software project, data, or models, and should not rely solely on them for any critical or sensitive tasks.\n\nThe project owners and contributors shall not be held responsible for any damages, losses, or liabilities arising from the use of this software project, data, or models, including but not limited to, any inappropriate responses generated by the software project, data, or models.\n\nBy using this software project, data, or models, users acknowledge and accept the experimental nature of the software project, data, and models, and understand the potential risks and limitations associated with their use. If users do not agree to the terms of this disclaimer, they should not use this software project, data, or models.\n\nThe software project, data, and models provided by our GitHub project are intended for research purposes only. They should not be used for any commercial, business, or legal purposes, and should not be relied upon as a substitute for professional advice or judgment.\n\nUsers of this software project, data, and models are strictly prohibited from using them for any commercial purposes, including but not limited to, selling, licensing, or distributing the software project, data, or models to third parties.\n\nThe project owners and contributors shall not be held responsible for any damages, losses, or liabilities arising from the use of this software project, data, or models for any commercial or business purposes.\n\nBy using this software project, data, or models, users agree to use them for research purposes only, and not for any commercial or business purposes. If users do not agree to the terms of this disclaimer, they should not use this software project, data, or models."
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 11.0908203125,
          "content": "                                 Apache License\n                           Version 2.0, January 2004\n                        http://www.apache.org/licenses/\n\n   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n\n   1. Definitions.\n\n      \"License\" shall mean the terms and conditions for use, reproduction,\n      and distribution as defined by Sections 1 through 9 of this document.\n\n      \"Licensor\" shall mean the copyright owner or entity authorized by\n      the copyright owner that is granting the License.\n\n      \"Legal Entity\" shall mean the union of the acting entity and all\n      other entities that control, are controlled by, or are under common\n      control with that entity. For the purposes of this definition,\n      \"control\" means (i) the power, direct or indirect, to cause the\n      direction or management of such entity, whether by contract or\n      otherwise, or (ii) ownership of fifty percent (50%) or more of the\n      outstanding shares, or (iii) beneficial ownership of such entity.\n\n      \"You\" (or \"Your\") shall mean an individual or Legal Entity\n      exercising permissions granted by this License.\n\n      \"Source\" form shall mean the preferred form for making modifications,\n      including but not limited to software source code, documentation\n      source, and configuration files.\n\n      \"Object\" form shall mean any form resulting from mechanical\n      transformation or translation of a Source form, including but\n      not limited to compiled object code, generated documentation,\n      and conversions to other media types.\n\n      \"Work\" shall mean the work of authorship, whether in Source or\n      Object form, made available under the License, as indicated by a\n      copyright notice that is included in or attached to the work\n      (an example is provided in the Appendix below).\n\n      \"Derivative Works\" shall mean any work, whether in Source or Object\n      form, that is based on (or derived from) the Work and for which the\n      editorial revisions, annotations, elaborations, or other modifications\n      represent, as a whole, an original work of authorship. For the purposes\n      of this License, Derivative Works shall not include works that remain\n      separable from, or merely link (or bind by name) to the interfaces of,\n      the Work and Derivative Works thereof.\n\n      \"Contribution\" shall mean any work of authorship, including\n      the original version of the Work and any modifications or additions\n      to that Work or Derivative Works thereof, that is intentionally\n      submitted to Licensor for inclusion in the Work by the copyright owner\n      or by an individual or Legal Entity authorized to submit on behalf of\n      the copyright owner. For the purposes of this definition, \"submitted\"\n      means any form of electronic, verbal, or written communication sent\n      to the Licensor or its representatives, including but not limited to\n      communication on electronic mailing lists, source code control systems,\n      and issue tracking systems that are managed by, or on behalf of, the\n      Licensor for the purpose of discussing and improving the Work, but\n      excluding communication that is conspicuously marked or otherwise\n      designated in writing by the copyright owner as \"Not a Contribution.\"\n\n      \"Contributor\" shall mean Licensor and any individual or Legal Entity\n      on behalf of whom a Contribution has been received by Licensor and\n      subsequently incorporated within the Work.\n\n   2. Grant of Copyright License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      copyright license to reproduce, prepare Derivative Works of,\n      publicly display, publicly perform, sublicense, and distribute the\n      Work and such Derivative Works in Source or Object form.\n\n   3. Grant of Patent License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      (except as stated in this section) patent license to make, have made,\n      use, offer to sell, sell, import, and otherwise transfer the Work,\n      where such license applies only to those patent claims licensable\n      by such Contributor that are necessarily infringed by their\n      Contribution(s) alone or by combination of their Contribution(s)\n      with the Work to which such Contribution(s) was submitted. If You\n      institute patent litigation against any entity (including a\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\n      or a Contribution incorporated within the Work constitutes direct\n      or contributory patent infringement, then any patent licenses\n      granted to You under this License for that Work shall terminate\n      as of the date such litigation is filed.\n\n   4. Redistribution. You may reproduce and distribute copies of the\n      Work or Derivative Works thereof in any medium, with or without\n      modifications, and in Source or Object form, provided that You\n      meet the following conditions:\n\n      (a) You must give any other recipients of the Work or\n          Derivative Works a copy of this License; and\n\n      (b) You must cause any modified files to carry prominent notices\n          stating that You changed the files; and\n\n      (c) You must retain, in the Source form of any Derivative Works\n          that You distribute, all copyright, patent, trademark, and\n          attribution notices from the Source form of the Work,\n          excluding those notices that do not pertain to any part of\n          the Derivative Works; and\n\n      (d) If the Work includes a \"NOTICE\" text file as part of its\n          distribution, then any Derivative Works that You distribute must\n          include a readable copy of the attribution notices contained\n          within such NOTICE file, excluding those notices that do not\n          pertain to any part of the Derivative Works, in at least one\n          of the following places: within a NOTICE text file distributed\n          as part of the Derivative Works; within the Source form or\n          documentation, if provided along with the Derivative Works; or,\n          within a display generated by the Derivative Works, if and\n          wherever such third-party notices normally appear. The contents\n          of the NOTICE file are for informational purposes only and\n          do not modify the License. You may add Your own attribution\n          notices within Derivative Works that You distribute, alongside\n          or as an addendum to the NOTICE text from the Work, provided\n          that such additional attribution notices cannot be construed\n          as modifying the License.\n\n      You may add Your own copyright statement to Your modifications and\n      may provide additional or different license terms and conditions\n      for use, reproduction, or distribution of Your modifications, or\n      for any such Derivative Works as a whole, provided Your use,\n      reproduction, and distribution of the Work otherwise complies with\n      the conditions stated in this License.\n\n   5. Submission of Contributions. Unless You explicitly state otherwise,\n      any Contribution intentionally submitted for inclusion in the Work\n      by You to the Licensor shall be under the terms and conditions of\n      this License, without any additional terms or conditions.\n      Notwithstanding the above, nothing herein shall supersede or modify\n      the terms of any separate license agreement you may have executed\n      with Licensor regarding such Contributions.\n\n   6. Trademarks. This License does not grant permission to use the trade\n      names, trademarks, service marks, or product names of the Licensor,\n      except as required for reasonable and customary use in describing the\n      origin of the Work and reproducing the content of the NOTICE file.\n\n   7. Disclaimer of Warranty. Unless required by applicable law or\n      agreed to in writing, Licensor provides the Work (and each\n      Contributor provides its Contributions) on an \"AS IS\" BASIS,\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n      implied, including, without limitation, any warranties or conditions\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n      PARTICULAR PURPOSE. You are solely responsible for determining the\n      appropriateness of using or redistributing the Work and assume any\n      risks associated with Your exercise of permissions under this License.\n\n   8. Limitation of Liability. In no event and under no legal theory,\n      whether in tort (including negligence), contract, or otherwise,\n      unless required by applicable law (such as deliberate and grossly\n      negligent acts) or agreed to in writing, shall any Contributor be\n      liable to You for damages, including any direct, indirect, special,\n      incidental, or consequential damages of any character arising as a\n      result of this License or out of the use or inability to use the\n      Work (including but not limited to damages for loss of goodwill,\n      work stoppage, computer failure or malfunction, or any and all\n      other commercial damages or losses), even if such Contributor\n      has been advised of the possibility of such damages.\n\n   9. Accepting Warranty or Additional Liability. While redistributing\n      the Work or Derivative Works thereof, You may choose to offer,\n      and charge a fee for, acceptance of support, warranty, indemnity,\n      or other liability obligations and/or rights consistent with this\n      License. However, in accepting such obligations, You may act only\n      on Your own behalf and on Your sole responsibility, not on behalf\n      of any other Contributor, and only if You agree to indemnify,\n      defend, and hold each Contributor harmless for any liability\n      incurred by, or claims asserted against, such Contributor by reason\n      of your accepting any such warranty or additional liability.\n\n   END OF TERMS AND CONDITIONS\n\n   APPENDIX: How to apply the Apache License to your work.\n\n      To apply the Apache License to your work, attach the following\n      boilerplate notice, with the fields enclosed by brackets \"[]\"\n      replaced with your own identifying information. (Don't include\n      the brackets!)  The text should be enclosed in the appropriate\n      comment syntax for the file format. We also recommend that a\n      file or class name and description of purpose be included on the\n      same \"printed page\" as the copyright notice for easier\n      identification within third-party archives.\n\n   Copyright Beijing Wenge Technology Co.,Ltd.\n\n   Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License."
        },
        {
          "name": "LICENSE_DATA",
          "type": "blob",
          "size": 18.8916015625,
          "content": "Attribution-NonCommercial 4.0 International\n\n=======================================================================\n\nCreative Commons Corporation (\"Creative Commons\") is not a law firm and\ndoes not provide legal services or legal advice. Distribution of\nCreative Commons public licenses does not create a lawyer-client or\nother relationship. Creative Commons makes its licenses and related\ninformation available on an \"as-is\" basis. Creative Commons gives no\nwarranties regarding its licenses, any material licensed under their\nterms and conditions, or any related information. Creative Commons\ndisclaims all liability for damages resulting from their use to the\nfullest extent possible.\n\nUsing Creative Commons Public Licenses\n\nCreative Commons public licenses provide a standard set of terms and\nconditions that creators and other rights holders may use to share\noriginal works of authorship and other material subject to copyright\nand certain other rights specified in the public license below. The\nfollowing considerations are for informational purposes only, are not\nexhaustive, and do not form part of our licenses.\n\n     Considerations for licensors: Our public licenses are\n     intended for use by those authorized to give the public\n     permission to use material in ways otherwise restricted by\n     copyright and certain other rights. Our licenses are\n     irrevocable. Licensors should read and understand the terms\n     and conditions of the license they choose before applying it.\n     Licensors should also secure all rights necessary before\n     applying our licenses so that the public can reuse the\n     material as expected. Licensors should clearly mark any\n     material not subject to the license. This includes other CC-\n     licensed material, or material used under an exception or\n     limitation to copyright. More considerations for licensors:\n    wiki.creativecommons.org/Considerations_for_licensors\n\n     Considerations for the public: By using one of our public\n     licenses, a licensor grants the public permission to use the\n     licensed material under specified terms and conditions. If\n     the licensor's permission is not necessary for any reason--for\n     example, because of any applicable exception or limitation to\n     copyright--then that use is not regulated by the license. Our\n     licenses grant only permissions under copyright and certain\n     other rights that a licensor has authority to grant. Use of\n     the licensed material may still be restricted for other\n     reasons, including because others have copyright or other\n     rights in the material. A licensor may make special requests,\n     such as asking that all changes be marked or described.\n     Although not required by our licenses, you are encouraged to\n     respect those requests where reasonable. More considerations\n     for the public:\n    wiki.creativecommons.org/Considerations_for_licensees\n\n=======================================================================\n\nCreative Commons Attribution-NonCommercial 4.0 International Public\nLicense\n\nBy exercising the Licensed Rights (defined below), You accept and agree\nto be bound by the terms and conditions of this Creative Commons\nAttribution-NonCommercial 4.0 International Public License (\"Public\nLicense\"). To the extent this Public License may be interpreted as a\ncontract, You are granted the Licensed Rights in consideration of Your\nacceptance of these terms and conditions, and the Licensor grants You\nsuch rights in consideration of benefits the Licensor receives from\nmaking the Licensed Material available under these terms and\nconditions.\n\n\nSection 1 -- Definitions.\n\n  a. Adapted Material means material subject to Copyright and Similar\n     Rights that is derived from or based upon the Licensed Material\n     and in which the Licensed Material is translated, altered,\n     arranged, transformed, or otherwise modified in a manner requiring\n     permission under the Copyright and Similar Rights held by the\n     Licensor. For purposes of this Public License, where the Licensed\n     Material is a musical work, performance, or sound recording,\n     Adapted Material is always produced where the Licensed Material is\n     synched in timed relation with a moving image.\n\n  b. Adapter's License means the license You apply to Your Copyright\n     and Similar Rights in Your contributions to Adapted Material in\n     accordance with the terms and conditions of this Public License.\n\n  c. Copyright and Similar Rights means copyright and/or similar rights\n     closely related to copyright including, without limitation,\n     performance, broadcast, sound recording, and Sui Generis Database\n     Rights, without regard to how the rights are labeled or\n     categorized. For purposes of this Public License, the rights\n     specified in Section 2(b)(1)-(2) are not Copyright and Similar\n     Rights.\n  d. Effective Technological Measures means those measures that, in the\n     absence of proper authority, may not be circumvented under laws\n     fulfilling obligations under Article 11 of the WIPO Copyright\n     Treaty adopted on December 20, 1996, and/or similar international\n     agreements.\n\n  e. Exceptions and Limitations means fair use, fair dealing, and/or\n     any other exception or limitation to Copyright and Similar Rights\n     that applies to Your use of the Licensed Material.\n\n  f. Licensed Material means the artistic or literary work, database,\n     or other material to which the Licensor applied this Public\n     License.\n\n  g. Licensed Rights means the rights granted to You subject to the\n     terms and conditions of this Public License, which are limited to\n     all Copyright and Similar Rights that apply to Your use of the\n     Licensed Material and that the Licensor has authority to license.\n\n  h. Licensor means the individual(s) or entity(ies) granting rights\n     under this Public License.\n\n  i. NonCommercial means not primarily intended for or directed towards\n     commercial advantage or monetary compensation. For purposes of\n     this Public License, the exchange of the Licensed Material for\n     other material subject to Copyright and Similar Rights by digital\n     file-sharing or similar means is NonCommercial provided there is\n     no payment of monetary compensation in connection with the\n     exchange.\n\n  j. Share means to provide material to the public by any means or\n     process that requires permission under the Licensed Rights, such\n     as reproduction, public display, public performance, distribution,\n     dissemination, communication, or importation, and to make material\n     available to the public including in ways that members of the\n     public may access the material from a place and at a time\n     individually chosen by them.\n\n  k. Sui Generis Database Rights means rights other than copyright\n     resulting from Directive 96/9/EC of the European Parliament and of\n     the Council of 11 March 1996 on the legal protection of databases,\n     as amended and/or succeeded, as well as other essentially\n     equivalent rights anywhere in the world.\n\n  l. You means the individual or entity exercising the Licensed Rights\n     under this Public License. Your has a corresponding meaning.\n\n\nSection 2 -- Scope.\n\n  a. License grant.\n\n       1. Subject to the terms and conditions of this Public License,\n          the Licensor hereby grants You a worldwide, royalty-free,\n          non-sublicensable, non-exclusive, irrevocable license to\n          exercise the Licensed Rights in the Licensed Material to:\n\n            a. reproduce and Share the Licensed Material, in whole or\n               in part, for NonCommercial purposes only; and\n\n            b. produce, reproduce, and Share Adapted Material for\n               NonCommercial purposes only.\n\n       2. Exceptions and Limitations. For the avoidance of doubt, where\n          Exceptions and Limitations apply to Your use, this Public\n          License does not apply, and You do not need to comply with\n          its terms and conditions.\n\n       3. Term. The term of this Public License is specified in Section\n          6(a).\n\n       4. Media and formats; technical modifications allowed. The\n          Licensor authorizes You to exercise the Licensed Rights in\n          all media and formats whether now known or hereafter created,\n          and to make technical modifications necessary to do so. The\n          Licensor waives and/or agrees not to assert any right or\n          authority to forbid You from making technical modifications\n          necessary to exercise the Licensed Rights, including\n          technical modifications necessary to circumvent Effective\n          Technological Measures. For purposes of this Public License,\n          simply making modifications authorized by this Section 2(a)\n          (4) never produces Adapted Material.\n\n       5. Downstream recipients.\n\n            a. Offer from the Licensor -- Licensed Material. Every\n               recipient of the Licensed Material automatically\n               receives an offer from the Licensor to exercise the\n               Licensed Rights under the terms and conditions of this\n               Public License.\n\n            b. No downstream restrictions. You may not offer or impose\n               any additional or different terms or conditions on, or\n               apply any Effective Technological Measures to, the\n               Licensed Material if doing so restricts exercise of the\n               Licensed Rights by any recipient of the Licensed\n               Material.\n\n       6. No endorsement. Nothing in this Public License constitutes or\n          may be construed as permission to assert or imply that You\n          are, or that Your use of the Licensed Material is, connected\n          with, or sponsored, endorsed, or granted official status by,\n          the Licensor or others designated to receive attribution as\n          provided in Section 3(a)(1)(A)(i).\n\n  b. Other rights.\n\n       1. Moral rights, such as the right of integrity, are not\n          licensed under this Public License, nor are publicity,\n          privacy, and/or other similar personality rights; however, to\n          the extent possible, the Licensor waives and/or agrees not to\n          assert any such rights held by the Licensor to the limited\n          extent necessary to allow You to exercise the Licensed\n          Rights, but not otherwise.\n\n       2. Patent and trademark rights are not licensed under this\n          Public License.\n\n       3. To the extent possible, the Licensor waives any right to\n          collect royalties from You for the exercise of the Licensed\n          Rights, whether directly or through a collecting society\n          under any voluntary or waivable statutory or compulsory\n          licensing scheme. In all other cases the Licensor expressly\n          reserves any right to collect such royalties, including when\n          the Licensed Material is used other than for NonCommercial\n          purposes.\n\n\nSection 3 -- License Conditions.\n\nYour exercise of the Licensed Rights is expressly made subject to the\nfollowing conditions.\n\n  a. Attribution.\n\n       1. If You Share the Licensed Material (including in modified\n          form), You must:\n\n            a. retain the following if it is supplied by the Licensor\n               with the Licensed Material:\n\n                 i. identification of the creator(s) of the Licensed\n                    Material and any others designated to receive\n                    attribution, in any reasonable manner requested by\n                    the Licensor (including by pseudonym if\n                    designated);\n\n                ii. a copyright notice;\n\n               iii. a notice that refers to this Public License;\n\n                iv. a notice that refers to the disclaimer of\n                    warranties;\n\n                 v. a URI or hyperlink to the Licensed Material to the\n                    extent reasonably practicable;\n\n            b. indicate if You modified the Licensed Material and\n               retain an indication of any previous modifications; and\n\n            c. indicate the Licensed Material is licensed under this\n               Public License, and include the text of, or the URI or\n               hyperlink to, this Public License.\n\n       2. You may satisfy the conditions in Section 3(a)(1) in any\n          reasonable manner based on the medium, means, and context in\n          which You Share the Licensed Material. For example, it may be\n          reasonable to satisfy the conditions by providing a URI or\n          hyperlink to a resource that includes the required\n          information.\n\n       3. If requested by the Licensor, You must remove any of the\n          information required by Section 3(a)(1)(A) to the extent\n          reasonably practicable.\n\n       4. If You Share Adapted Material You produce, the Adapter's\n          License You apply must not prevent recipients of the Adapted\n          Material from complying with this Public License.\n\n\nSection 4 -- Sui Generis Database Rights.\n\nWhere the Licensed Rights include Sui Generis Database Rights that\napply to Your use of the Licensed Material:\n\n  a. for the avoidance of doubt, Section 2(a)(1) grants You the right\n     to extract, reuse, reproduce, and Share all or a substantial\n     portion of the contents of the database for NonCommercial purposes\n     only;\n\n  b. if You include all or a substantial portion of the database\n     contents in a database in which You have Sui Generis Database\n     Rights, then the database in which You have Sui Generis Database\n     Rights (but not its individual contents) is Adapted Material; and\n\n  c. You must comply with the conditions in Section 3(a) if You Share\n     all or a substantial portion of the contents of the database.\n\nFor the avoidance of doubt, this Section 4 supplements and does not\nreplace Your obligations under this Public License where the Licensed\nRights include other Copyright and Similar Rights.\n\n\nSection 5 -- Disclaimer of Warranties and Limitation of Liability.\n\n  a. UNLESS OTHERWISE SEPARATELY UNDERTAKEN BY THE LICENSOR, TO THE\n     EXTENT POSSIBLE, THE LICENSOR OFFERS THE LICENSED MATERIAL AS-IS\n     AND AS-AVAILABLE, AND MAKES NO REPRESENTATIONS OR WARRANTIES OF\n     ANY KIND CONCERNING THE LICENSED MATERIAL, WHETHER EXPRESS,\n     IMPLIED, STATUTORY, OR OTHER. THIS INCLUDES, WITHOUT LIMITATION,\n     WARRANTIES OF TITLE, MERCHANTABILITY, FITNESS FOR A PARTICULAR\n     PURPOSE, NON-INFRINGEMENT, ABSENCE OF LATENT OR OTHER DEFECTS,\n     ACCURACY, OR THE PRESENCE OR ABSENCE OF ERRORS, WHETHER OR NOT\n     KNOWN OR DISCOVERABLE. WHERE DISCLAIMERS OF WARRANTIES ARE NOT\n     ALLOWED IN FULL OR IN PART, THIS DISCLAIMER MAY NOT APPLY TO YOU.\n\n  b. TO THE EXTENT POSSIBLE, IN NO EVENT WILL THE LICENSOR BE LIABLE\n     TO YOU ON ANY LEGAL THEORY (INCLUDING, WITHOUT LIMITATION,\n     NEGLIGENCE) OR OTHERWISE FOR ANY DIRECT, SPECIAL, INDIRECT,\n     INCIDENTAL, CONSEQUENTIAL, PUNITIVE, EXEMPLARY, OR OTHER LOSSES,\n     COSTS, EXPENSES, OR DAMAGES ARISING OUT OF THIS PUBLIC LICENSE OR\n     USE OF THE LICENSED MATERIAL, EVEN IF THE LICENSOR HAS BEEN\n     ADVISED OF THE POSSIBILITY OF SUCH LOSSES, COSTS, EXPENSES, OR\n     DAMAGES. WHERE A LIMITATION OF LIABILITY IS NOT ALLOWED IN FULL OR\n     IN PART, THIS LIMITATION MAY NOT APPLY TO YOU.\n\n  c. The disclaimer of warranties and limitation of liability provided\n     above shall be interpreted in a manner that, to the extent\n     possible, most closely approximates an absolute disclaimer and\n     waiver of all liability.\n\n\nSection 6 -- Term and Termination.\n\n  a. This Public License applies for the term of the Copyright and\n     Similar Rights licensed here. However, if You fail to comply with\n     this Public License, then Your rights under this Public License\n     terminate automatically.\n\n  b. Where Your right to use the Licensed Material has terminated under\n     Section 6(a), it reinstates:\n\n       1. automatically as of the date the violation is cured, provided\n          it is cured within 30 days of Your discovery of the\n          violation; or\n\n       2. upon express reinstatement by the Licensor.\n\n     For the avoidance of doubt, this Section 6(b) does not affect any\n     right the Licensor may have to seek remedies for Your violations\n     of this Public License.\n\n  c. For the avoidance of doubt, the Licensor may also offer the\n     Licensed Material under separate terms or conditions or stop\n     distributing the Licensed Material at any time; however, doing so\n     will not terminate this Public License.\n\n  d. Sections 1, 5, 6, 7, and 8 survive termination of this Public\n     License.\n\n\nSection 7 -- Other Terms and Conditions.\n\n  a. The Licensor shall not be bound by any additional or different\n     terms or conditions communicated by You unless expressly agreed.\n\n  b. Any arrangements, understandings, or agreements regarding the\n     Licensed Material not stated herein are separate from and\n     independent of the terms and conditions of this Public License.\n\n\nSection 8 -- Interpretation.\n\n  a. For the avoidance of doubt, this Public License does not, and\n     shall not be interpreted to, reduce, limit, restrict, or impose\n     conditions on any use of the Licensed Material that could lawfully\n     be made without permission under this Public License.\n\n  b. To the extent possible, if any provision of this Public License is\n     deemed unenforceable, it shall be automatically reformed to the\n     minimum extent necessary to make it enforceable. If the provision\n     cannot be reformed, it shall be severed from this Public License\n     without affecting the enforceability of the remaining terms and\n     conditions.\n\n  c. No term or condition of this Public License will be waived and no\n     failure to comply consented to unless expressly agreed to by the\n     Licensor.\n\n  d. Nothing in this Public License constitutes or may be interpreted\n     as a limitation upon, or waiver of, any privileges and immunities\n     that apply to the Licensor or You, including from the legal\n     processes of any jurisdiction or authority.\n\n=======================================================================\n\nCreative Commons is not a party to its public\nlicenses. Notwithstanding, Creative Commons may elect to apply one of\nits public licenses to material it publishes and in those instances\nwill be considered the “Licensor.” The text of the Creative Commons\npublic licenses is dedicated to the public domain under the CC0 Public\nDomain Dedication. Except for the limited purpose of indicating that\nmaterial is shared under a Creative Commons public license or as\notherwise permitted by the Creative Commons policies published at\ncreativecommons.org/policies, Creative Commons does not authorize the\nuse of the trademark \"Creative Commons\" or any other trademark or logo\nof Creative Commons without its prior written consent including,\nwithout limitation, in connection with any unauthorized modifications\nto any of its public licenses or any other arrangements,\nunderstandings, or agreements concerning use of licensed material. For\nthe avoidance of doubt, this paragraph does not form part of the\npublic licenses.\n\nCreative Commons may be contacted at creativecommons.org."
        },
        {
          "name": "LICENSE_MODEL",
          "type": "blob",
          "size": 2.2490234375,
          "content": "1. Definitions\n\n“Licensor” means the YaYi Model Team that distributes its Software.\n\n“Software” means the YaYi model parameters made available under this license.\n\n2. License Grant\n\nSubject to the terms and conditions of this License, the Licensor hereby grants to you a non-exclusive, worldwide, non-transferable, non-sublicensable, revocable, royalty-free copyright license to use the Software solely for your non-commercial research purposes.\n\nThe above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n\n3. Restriction\n\nYou will not use, copy, modify, merge, publish, distribute, reproduce, or create derivative works of the Software, in whole or in part, for any commercial, military, or illegal purposes.\n\nYou will not use the Software for any act that may undermine China's national security and national unity, harm the public interest of society, or infringe upon the rights and interests of human beings.\n\n4. Disclaimer\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\n5. Limitation of Liability\n\nEXCEPT TO THE EXTENT PROHIBITED BY APPLICABLE LAW, IN NO EVENT AND UNDER NO LEGAL THEORY, WHETHER BASED IN TORT, NEGLIGENCE, CONTRACT, LIABILITY, OR OTHERWISE WILL ANY LICENSOR BE LIABLE TO YOU FOR ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES, OR ANY OTHER COMMERCIAL LOSSES, EVEN IF THE LICENSOR HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES.\n\n6. Dispute Resolution\n\nThis license shall be governed and construed in accordance with the laws of People's Republic of China. Any dispute arising from or in connection with this License shall be submitted to Haidian District People's Court in Beijing.\n\nNote that the license is subject to update to a more comprehensive version. For any questions related to the license and copyright, please contact us at YaYi@wenge.com."
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 11.275390625,
          "content": "# 雅意大模型\n\n<div align=\"center\">\n<img src=\"./assets/yayi_dark_small.png\" alt=\"YAYI\" style=\"width: 30%; display: block; margin: auto;\">\n<br>\n\n[![Code License](https://img.shields.io/badge/Code%20License-Apache_2.0-brightgreen.svg)](./LICENSE)\n[![Data License](https://img.shields.io/badge/Data%20License-CC_BY_NC_4.0-red.svg)](./LICENSE_DATA)\n[![Model License](https://img.shields.io/badge/Model%20License-YAYI-blue.svg)](./LICENSE_MODEL)\n\n[[📖README](./README.md)] \n[[🤗HF Repo](https://huggingface.co/wenge-research)]\n[[🔗网页端](https://yayi.wenge.com)]\n\n中文 | [English](./README_EN.md)\n\n</div>\n\n## 介绍\n\n[雅意大模型](https://www.wenge.com/yayi/index.html)在百万级人工构造的高质量领域数据上进行指令微调得到，训练数据覆盖媒体宣传、舆情分析、公共安全、金融风控、城市治理等五大领域，上百种自然语言指令任务。雅意大模型从预训练初始化权重到领域模型的迭代过程中，我们逐步增强了它的中文基础能力和领域分析能力，并增加了多轮对话和部分插件能力。同时，经过数百名用户内测过程中持续不断的人工反馈优化，我们进一步提升了模型性能和安全性。\n\n通过雅意大模型的开源为促进中文预训练大模型开源社区的发展，贡献自己的一份力量，通过开源，与每一位合作伙伴共建雅意大模型生态。\n\n*News: 🔥 雅意大模型已开源基于 LLaMA 2 的中文优化模型版本，探索适用于中文多领域任务的最新实践。*\n\n\n## 模型地址\n\n|  模型名称  | 🤗HF模型标识 |  下载地址  |\n| --------- | ---------    | --------- |\n|  YAYI-7B  | wenge-research/yayi-7b  | [模型下载](https://huggingface.co/wenge-research/yayi-7b)  |\n| YAYI-7B-Llama2 | wenge-research/yayi-7b-llama2 | [模型下载](https://huggingface.co/wenge-research/yayi-7b-llama2) |\n| YAYI-13B-Llama2 | wenge-research/yayi-13b-llama2 | [模型下载](https://huggingface.co/wenge-research/yayi-13b-llama2) |\n\n\n\n## 运行方式\n\n### 环境安装\n1. 下载本仓库内容至本地/远程服务器\n\n```bash\ngit clone https://github.com/wenge-research/YAYI.git\ncd YAYI\n```\n\n2. 创建conda环境\n\n```bash\nconda create --name yayi python=3.8\nconda activate yayi\n```\n\n3. 安装依赖\n\n```bash\npip install -r requirements.txt\n```\n其中 `torch` 和 `transformers` 版本不建议低于推荐版本。\n\n### 模型推理\n\n模型权重（7b版本）已在我们的 [Huggingface 模型仓库](https://huggingface.co/wenge-research) 开源，欢迎下载使用。以下是一个简单调用 `yayi-7b` 进行下游任务推理的示例代码，可在单张 A100/A800/3090 等GPU运行，使用FP16精度推理时约占用 20GB 显存：\n\n```python\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig\nimport torch\n\nyayi_7b_path = \"wenge-research/yayi-7b\"\ntokenizer = AutoTokenizer.from_pretrained(yayi_7b_path)\nmodel = AutoModelForCausalLM.from_pretrained(yayi_7b_path, device_map=\"auto\", torch_dtype=torch.bfloat16)\n\nprompt = \"你好\"\nformatted_prompt = f\"<|System|>:\\nA chat between a human and an AI assistant named YaYi.\\nYaYi is a helpful and harmless language model developed by Beijing Wenge Technology Co.,Ltd.\\n\\n<|Human|>:\\n{prompt}\\n\\n<|YaYi|>:\"\ninputs = tokenizer(formatted_prompt, return_tensors=\"pt\").to(model.device)\n\neos_token_id = tokenizer(\"<|End|>\").input_ids[0]\ngeneration_config = GenerationConfig(\n    eos_token_id=eos_token_id,\n    pad_token_id=eos_token_id,\n    do_sample=True,\n    max_new_tokens=100,\n    temperature=0.3,\n    repetition_penalty=1.1,\n    no_repeat_ngram_size=0\n)\nresponse = model.generate(**inputs, generation_config=generation_config)\nprint(tokenizer.decode(response[0]))\n```\n\n注意，模型训练时添加了 special token `<|End|>` 作为结束符，因此上述代码 `GenerationConfig` 里将 `eos_token_id` 设置为该结束符对应的 token id。基于 LlaMA2 指令微调模型的推理代码稍有不同，具体请参考我们的 [Huggingface 模型仓库](https://huggingface.co/wenge-research) 中的对应版本。\n\n### 模型微调\n\n本项目基于 `deepspeed` 框架进行模型训练，配置完环境后执行相应脚本即可开始训练。支持指令数据全参数微调、指令数据LoRA微调、多轮对话数据全参数微调、多轮对话数据LoRA微调。\n\n#### 1. 指令数据全参数微调\n\n- **数据格式**：参考 [`data/yayi_train_example.json`](data/yayi_train_example.json)，采用 Alpaca 项目的 jsonline 数据格式，每行一条 json 数据，由 `\"instruction\"`、`\"input\"`、`\"output\"` 三个字段组成。其中 `\"instruction\"` 和 `\"input\"` 为指令输入，`\"output\"` 为输出答案。\n- **运行说明**：运行以下命令即可开始全参数微调雅意大模型。该命令支持单机多卡训练，如需配置多机多卡训练，可参考 deepspeed 官方文档。建议使用 4*A100(80G) 以上硬件配置。\n\n    ```\n    deepspeed --num_gpus=8 \\\n        --module training.trainer \\\n        --data-path ./data/yayi_train_example.json \\\n        --input-model ./checkpoints/yayi-7b \\\n        --deepspeed ./config/deepspeed_zero2_bf16.json \\\n        --epochs 2 \\\n        --local-output-dir ./checkpoints \\\n        --per-device-train-batch-size 8 \\\n        --per-device-eval-batch-size 8 \\\n        --logging-steps 1 \\\n        --save-steps 100 \\\n        --save-total-limit 10 \\\n        --eval-steps 100 \\\n        --warmup-steps 100 \\\n        --test-size 400 \\\n        --lr 5e-6 \\\n        --seed 515\n    ```\n\n#### 2. 指令数据 LoRA 微调\n\n- **数据格式**：同上，参考 [`data/yayi_train_example.json`](data/yayi_train_example.json)。\n- **运行说明**：LoRA 是一种低资源高效微调方法，单卡可训练百亿参数模型。本项目主要基于 [`peft`](https://huggingface.co/docs/peft/index) 实现 LoRA 微调，运行以下命令即可开始 LoRA 微调雅意大模型。使用单卡 A100(80G) 即可完成微调，学习率可调整为较大值。其中，`--lora-dim` 设置更新矩阵的秩，该值越大，训练的参数量越大；`--lora-module-name` 设置 LoRA 更新矩阵的模块，可根据模型类型更改。\n\n    ```\n    deepspeed --num_gpus=1 \\\n        --module training.trainer_lora \\\n        --data-path ./data/yayi_train_example.json \\\n        --input-model ./checkpoints/yayi-7b \\\n        --deepspeed ./config/deepspeed_zero2_bf16.json \\\n        --epochs 2 \\\n        --local-output-dir ./checkpoints \\\n        --per-device-train-batch-size 8 \\\n        --per-device-eval-batch-size 8 \\\n        --logging-steps 1 \\\n        --save-steps 100 \\\n        --save-total-limit 10 \\\n        --eval-steps 100 \\\n        --warmup-steps 100 \\\n        --test-size 400 \\\n        --lr 5e-4 \\\n        --seed 515 \\\n        --lora-dim 16 \\\n        --lora-module-name query_key_value\n    ```\n\n#### 3. 多轮对话数据全参数微调\n\n- **数据格式**：参考 [`data/yayi_train_example_multi_rounds.json`](data/yayi_train_example_multi_rounds.json)，是一个标准 JSON 文件，每条数据由 `\"system\"` 和 `\"conversations\"`组成，其中 `\"system\"` 为全局角色设定信息，可为空字符串，`\"conversations\"` 是由 human 和 yayi 两种角色交替进行的多轮对话内容。\n- **运行说明**：运行以下命令即可开始全参数微调雅意大模型，对于多轮对话数据，仅计算模型生成回复的loss。该命令支持单机多卡训练，如需配置多机多卡训练，可参考 deepspeed 官方文档。建议使用 4*A100(80G) 以上硬件配置。\n\n    ```\n    deepspeed --num_gpus=8 \\\n        --module training.trainer_multi_rounds \\\n        --data-path ./data/yayi_train_example_multi_rounds.json \\\n        --input-model ./checkpoints/yayi-7b \\\n        --deepspeed ./config/deepspeed_zero2_bf16.json \\\n        --epochs 2 \\\n        --local-output-dir ./checkpoints \\\n        --per-device-train-batch-size 8 \\\n        --per-device-eval-batch-size 8 \\\n        --logging-steps 1 \\\n        --save-steps 100 \\\n        --save-total-limit 10 \\\n        --eval-steps 100 \\\n        --warmup-steps 100 \\\n        --test-size 400 \\\n        --lr 5e-7 \\\n        --seed 515\n    ```\n\n#### 4. 多轮对话数据 LoRA 微调\n\n- **数据格式**：同上，参考 [`data/yayi_train_example_multi_rounds.json`](data/yayi_train_example_multi_rounds.json)。\n- **运行说明**：参考多轮对话数据全参数微调的数据加载方式，以及指令数据 LoRA 微调方式。\n\n\n## 训练数据\n\n雅意大模型基于中科闻歌百万级高质量领域指令微调数据集训练而来，我们本次开源 5w 条训练数据集，可在我们的 [Huggingface 数据仓库](https://huggingface.co/wenge-research) 下载。数据集主要涵盖了金融、安全、舆情、媒体等几大领域，我们为各领域任务大部分指令数据添加了离散 prompt 前缀，以区分各领域数据。此外，训练数据中还包含部分安全增强数据、插件能力数据、多轮对话数据等。\n\n\n## 相关协议\n\n### 局限性\n基于当前数据和基础模型训练得到的SFT模型，在效果上仍存在以下问题：\n\n1. 在涉及事实性的指令上可能会产生违背事实的错误回答。\n2. 对于具备危害性的指令无法很好的鉴别，可能会产生危害性言论。\n3. 在一些涉及逻辑推理、代码生成、科学计算等场景下模型的能力仍有待提高。\n\n### 免责声明\n\n基于以上模型局限性，我们要求开发者仅将我们开源的代码、数据、模型及后续用此项目生成的衍生物用于研究目的，不得用于商业用途，以及其他会对社会带来危害的用途。请谨慎鉴别和使用雅意大模型生成的内容，请勿将生成的有害内容传播至互联网。若产生不良后果，由传播者自负。\n\n本项目仅可应用于研究目的，项目开发者不承担任何因使用本项目（包含但不限于数据、模型、代码等）导致的危害或损失。详细请参考[免责声明](DISCLAIMER)。\n\n### 开源协议\n\n本项目中的代码依照 [Apache-2.0](LICENSE) 协议开源，数据采用 [CC BY-NC 4.0](LICENSE_DATA) 协议，YAYI 系列模型权重的使用则需要遵循 [Model License](LICENSE_MODEL)。\n\n## 更新日志\n- [2023/08/09] 更新LoRA微调代码以及多轮对话格式数据训练代码。\n- [2023/07/22] 更新中文领域知识增强的 YAYI-7B-Llama2 和 YAYI-13B-Llama2 模型权重。\n- [2023/07/14] 升级模型安全性和拒识能力，新增模型 int8 量化。\n- [2023/06/29] 升级和优化中英文多轮对话能力。\n- [2023/06/03] 雅意大模型正式对外发布并开源 7B 版本模型权重。\n\n## 致谢\n- 本项目分别使用了 BigScience  [bloomz-7b1-mt](https://huggingface.co/bigscience/bloomz-7b1-mt) 以及 Meta [Llama 2](https://huggingface.co/meta-llama) 系列的模型权重作为初始化权重，并进行词表扩展；\n- 本项目训练代码参考了 Databricks 的 [dolly](https://github.com/databrickslabs/dolly) 项目及 Huggingface [transformers](https://github.com/huggingface/transformers) 库；\n- 本项目分布式训练使用了 Microsoft 的 [DeepSpeed](https://github.com/microsoft/deepspeed) 分布式训练工具及 Huggingface transformers 文档中的 [ZeRO stage 2](https://huggingface.co/docs/transformers/main_classes/deepspeed#zero2-config) 配置文件；\n\n\n## Star History\n[![Star History Chart](https://api.star-history.com/svg?repos=wenge-research/YAYI&type=Date)](https://star-history.com/#wenge-research/YAYI&Date)\n"
        },
        {
          "name": "README_EN.md",
          "type": "blob",
          "size": 13.4189453125,
          "content": "# YaYi\n\n<div align=\"center\">\n<img src=\"./assets/yayi_dark_small.png\" alt=\"YaYi\" style=\"width: 30%; display: block; margin: auto;\">\n<br>\n\n[![Code License](https://img.shields.io/badge/Code%20License-Apache_2.0-brightgreen.svg)](./LICENSE)\n[![Data License](https://img.shields.io/badge/Data%20License-CC_BY_NC_4.0-red.svg)](./LICENSE_DATA)\n[![Model License](https://img.shields.io/badge/Model%20License-YaYi-blue.svg)](./LICENSE_MODEL)\n\n[[📖README](./README.md)] \n[[🤗HF Repo](https://huggingface.co/wenge-research)]\n[[🔗WEB](https://yayi.wenge.com)]\n\nEnglish | [中文](./README.md)\n\n</div>\n\n## Introduction\n\n[YaYi](https://www.wenge.com/yayi/index.html) was fine-tuned on millions of artificially constructed high-quality domain data. This training data covers five key domains: media publicity, public opinion analysis, public safety, financial risk control, and urban governance, encompassing over a hundred natural language instruction tasks. Throughout the iterative development process of the YaYi, starting from pre-training initialization weights and progressing to domain-specific model, we have steadily enhanced its foundational Chinese language capabilities and domain analysis capabilities. We've also introduced multi-turn conversation enhancements and integrated various plug-in capabilities. Furthermore, through continuous manual feedback and optimization from hundreds of users during the internal testing phase, we've meticulously refined the model's performance and security.\n\nBy open-sourcing the YaYi model, we will contribute our own efforts to the development of the Chinese pre-trained large language model open-source community. Through this open-source initiative, we seek to collaborate with every partner to build the YaYi model ecosystem together.\n\n*News: 🔥 YaYi has open sourced the Chinese optimization model version based on LLaMA 2 to explore the latest practices suitable for Chinese multi-domain tasks.*\n\n\n## Model download\n\n|  Model  | 🤗HF Model Name |  Download Links  |\n| --------- | ---------    | --------- |\n|  YAYI-7B  | wenge-research/yayi-7b  | [Download](https://huggingface.co/wenge-research/yayi-7b)  |\n| YAYI-7B-Llama2 | wenge-research/yayi-7b-llama2 | [Download](https://huggingface.co/wenge-research/yayi-7b-llama2) |\n| YAYI-13B-Llama2 | wenge-research/yayi-13b-llama2 | [Download](https://huggingface.co/wenge-research/yayi-13b-llama2) |\n\n\n\n## Run\n\n### Setup\n1. Download this repository to your local/remote server.\n\n```bash\ngit clone https://github.com/wenge-research/YAYI.git\ncd YAYI\n```\n\n2. Create conda environment\n\n```bash\nconda create --name yayi python=3.8\nconda activate yayi\n```\n\n3. Install requirements\n\n```bash\npip install -r requirements.txt\n```\nThe `torch` and `transformers` versions are not recommended to be lower than the recommended version.\n\n### Inference\n\nModel weights (7b version) have been open-sourced in our [Huggingface model repository](https://huggingface.co/wenge-research). Feel free to download and use them. Below is a simple example code for invoking yayi-7b for downstream task inference. It can run on a single GPU like A100/A800/3090, and it occupies approximately 20GB of GPU memory when performing inference with FP16 precision:\n\n```python\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig\nimport torch\n\nyayi_7b_path = \"wenge-research/yayi-7b\"\ntokenizer = AutoTokenizer.from_pretrained(yayi_7b_path)\nmodel = AutoModelForCausalLM.from_pretrained(yayi_7b_path, device_map=\"auto\", torch_dtype=torch.bfloat16)\n\nprompt = \"hello!\"\nformatted_prompt = f\"<|System|>:\\nA chat between a human and an AI assistant named YaYi.\\nYaYi is a helpful and harmless language model developed by Beijing Wenge Technology Co.,Ltd.\\n\\n<|Human|>:\\n{prompt}\\n\\n<|YaYi|>:\"\ninputs = tokenizer(formatted_prompt, return_tensors=\"pt\").to(model.device)\n\neos_token_id = tokenizer(\"<|End|>\").input_ids[0]\ngeneration_config = GenerationConfig(\n    eos_token_id=eos_token_id,\n    pad_token_id=eos_token_id,\n    do_sample=True,\n    max_new_tokens=100,\n    temperature=0.3,\n    repetition_penalty=1.1,\n    no_repeat_ngram_size=0\n)\nresponse = model.generate(**inputs, generation_config=generation_config)\nprint(tokenizer.decode(response[0]))\n```\n\nPlease note that a special token `<|End|>` was added as an end-of-sequence marker during model training. Therefore, in the `GenerationConfig` provided above, you should set `eos_token_id` to the token id corresponding to this end-of-sequence marker. The inference code for models fine-tuned based on LlaMA2 instructions may vary slightly; for specific details, please refer to the corresponding version in our [Huggingface model repository](https://huggingface.co/wenge-research).\n\n### Model fine-tuned\n\nThis project utilizes the `deepspeed` framework for model training. After setting up the environment, you can execute the corresponding scripts to commence training. It supports full-parameter fine-tuning on instruction data, LoRA fine-tuning on instruction data, full-parameter fine-tuning on multi-turn dialogue data, and LoRA fine-tuning on multi-turn dialogue data.\n\n#### 1. Full-parameter fine-tuning on instruction data.\n\n- **Data format**: Refer to [`data/yayi_train_example.json`](data/yayi_train_example.json), which follows the jsonline data format from the Alpaca project, with one JSON data entry per line. Each entry consists of `\"instruction\"`、`\"input\"`、`\"output\"`. `\"instruction\"` and `\"input\"` represent the instruction input, while `\"output\"` represents the output answer. \n- **Instructions**: Running the following command will initiate full-parameter fine-tuning of the YaYi model. This command supports training on a single machine with multiple GPUs. If you need to configure multi-machine multi-GPU training, please refer to the official deepspeed documentation. It is recommended to use hardware configurations with 4 or more A100 GPUs (80GB each) or higher.\n\n    ```\n    deepspeed --num_gpus=8 \\\n        --module training.trainer \\\n        --data-path ./data/yayi_train_example.json \\\n        --input-model ./checkpoints/yayi-7b \\\n        --deepspeed ./config/deepspeed_zero2_bf16.json \\\n        --epochs 2 \\\n        --local-output-dir ./checkpoints \\\n        --per-device-train-batch-size 8 \\\n        --per-device-eval-batch-size 8 \\\n        --logging-steps 1 \\\n        --save-steps 100 \\\n        --save-total-limit 10 \\\n        --eval-steps 100 \\\n        --warmup-steps 100 \\\n        --test-size 400 \\\n        --lr 5e-6 \\\n        --seed 515\n    ```\n\n#### 2. LoRA fine-tuning on instruction data\n\n- **Data format**: Same as above, refer to [`data/yayi_train_example.json`](data/yayi_train_example.json).\n- **Instructions**: LoRA is an efficient low-resource fine-tuning method that can train models with hundreds of billions of parameters on a single GPU. This project primarily implements LoRA fine-tuning using [`peft`](https://huggingface.co/docs/peft/index). You can start LoRA fine-tuning of the YaYi model by running the following command. It is possible to complete fine-tuning using a single A100 (80GB) GPU, and you can adjust the learning rate to a higher value. The `--lora-dim`  sets the rank of the update matrix, where a larger value results in more parameters being trained. And the `--lora-module-name` specifies the module for the LoRA update matrix and can be changed based on the model type.\n\n    ```\n    deepspeed --num_gpus=1 \\\n        --module training.trainer_lora \\\n        --data-path ./data/yayi_train_example.json \\\n        --input-model ./checkpoints/yayi-7b \\\n        --deepspeed ./config/deepspeed_zero2_bf16.json \\\n        --epochs 2 \\\n        --local-output-dir ./checkpoints \\\n        --per-device-train-batch-size 8 \\\n        --per-device-eval-batch-size 8 \\\n        --logging-steps 1 \\\n        --save-steps 100 \\\n        --save-total-limit 10 \\\n        --eval-steps 100 \\\n        --warmup-steps 100 \\\n        --test-size 400 \\\n        --lr 5e-4 \\\n        --seed 515 \\\n        --lora-dim 16 \\\n        --lora-module-name query_key_value\n    ```\n\n#### 3. Full-parameter fine-tuning on multi-turn dialogue data\n\n- **Data format**: Refer to [`data/yayi_train_example_multi_rounds.json`](data/yayi_train_example_multi_rounds.json), which is a standard JSON file. Each data entry consists of `\"system\"` and `\"conversations\"`. `\"system\"` contains global role-setting information and can be an empty string. `\"conversations\"` contains multi-turn dialogue content conducted alternately between human and YaYi roles.\n- **Instructions**: Running the following command will initiate full-parameter fine-tuning of the YaYi model. For multi-turn dialogue data, it calculates the loss only for model-generated responses. This command supports training on a single machine with multiple GPUs. If you need to configure multi-machine multi-GPU training, please refer to the official deepspeed documentation. It is recommended to use hardware configurations with 4 or more A100 GPUs (80GB each) or higher.\n\n    ```\n    deepspeed --num_gpus=8 \\\n        --module training.trainer_multi_rounds \\\n        --data-path ./data/yayi_train_example_multi_rounds.json \\\n        --input-model ./checkpoints/yayi-7b \\\n        --deepspeed ./config/deepspeed_zero2_bf16.json \\\n        --epochs 2 \\\n        --local-output-dir ./checkpoints \\\n        --per-device-train-batch-size 8 \\\n        --per-device-eval-batch-size 8 \\\n        --logging-steps 1 \\\n        --save-steps 100 \\\n        --save-total-limit 10 \\\n        --eval-steps 100 \\\n        --warmup-steps 100 \\\n        --test-size 400 \\\n        --lr 5e-7 \\\n        --seed 515\n    ```\n\n#### 4. LoRA fine-tuning on multi-turn dialogue data\n\n- **Data format**: Same as above, refer to [`data/yayi_train_example_multi_rounds.json`](data/yayi_train_example_multi_rounds.json).\n- **Instructions**: Refer to the data loading method for full-parameter fine-tuning on multi-turn dialogue data, as well as the LoRA fine-tuning method for instruction data.\n\n\n## Training data\n\nThe YaYi model was trained on a high-quality domain-specific instruction fine-tuning dataset, which consists of millions of instances provided by Wenge Research. For this open-source release, we have made available a training dataset containing 50,000 samples, which can be downloaded from our [Huggingface data repository](https://huggingface.co/wenge-research). This dataset primarily covers several domains, including finance, security, public opinion analysis, media, and more. We have added discrete prompt prefixes to most of the instruction data to differentiate between various domain-specific data. Additionally, the training data includes some security-enhanced data, plugin capability data, and multi-turn dialogue data.\n\n## Related agreements\n\n### Limitations\nThe SFT model trained based on the current data and base model still exhibits the following issues in terms of performance:\n\n1. It may generate factually incorrect responses for factual instructions.\n2. It struggles to effectively identify harmful instructions, potentially leading to harmful content generation.\n3. Its capabilities in scenarios involving logical reasoning, code generation, scientific computation, and similar tasks still require improvement.\n\n### Disclaimer\n\nDue to the limitations of the model mentioned above, we request that developers use the code, data, models, and any derivatives generated from this project solely for research purposes and refrain from using them for commercial or any other potentially harmful purposes to society. Please exercise caution in evaluating and utilizing content generated by the YaYi model, and do not propagate harmful content on the internet. Any adverse consequences resulting from such actions are the responsibility of the disseminator.\n\nThis project is intended for research purposes only, and the project developers bear no responsibility for any harm or losses incurred due to the use of this project, including but not limited to data, models, code, etc. For more details, please refer to the [Disclaimer](DISCLAIMER).\n\n### License\n\nThe code in this project is open-source under the [Apache-2.0](LICENSE) license, the data follows the [CC BY-NC 4.0](LICENSE_DATA) license, and the usage of YaYi series model weights must adhere to the [Model License](LICENSE_MODEL).\n\n## Update log\n- [2023/08/09] Updated LoRA fine-tuning code and multi-turn dialogue format data training code.\n- [2023/07/22] Updated YAYI-7B-Llama2 and YAYI-13B-Llama2 model weights with enhanced Chinese domain knowledge.\n- [2023/07/14] Enhanced model security and anti-denial capabilities, introducing model int8 quantization.\n- [2023/06/29] Improved and optimized multi-turn dialogue capabilities in both Chinese and English.\n- [2023/06/03] Officially released and open-sourced the 7B version of the YAYI model.\n\n## Acknowledgements\n- In this project, we used model weights from BigScience's [bloomz-7b1-mt](https://huggingface.co/bigscience/bloomz-7b1-mt) and Meta's [Llama 2](https://huggingface.co/meta-llama) series as initialization weights, along with vocabulary expansion.\n- The training code in this project was inspired by Databricks' [dolly](https://github.com/databrickslabs/dolly) project and Huggingface's [transformers](https://github.com/huggingface/transformers) library.\n- Distributed training in this project utilized Microsoft's [DeepSpeed](https://github.com/microsoft/deepspeed) distributed training tool and configuration files from Huggingface transformers' [ZeRO stage 2](https://huggingface.co/docs/transformers/main_classes/deepspeed#zero2-config).\n\n\n## Star History\n[![Star History Chart](https://api.star-history.com/svg?repos=wenge-research/YAYI&type=Date)](https://star-history.com/#wenge-research/YAYI&Date)\n"
        },
        {
          "name": "assets",
          "type": "tree",
          "content": null
        },
        {
          "name": "config",
          "type": "tree",
          "content": null
        },
        {
          "name": "data",
          "type": "tree",
          "content": null
        },
        {
          "name": "requirements.txt",
          "type": "blob",
          "size": 0.1416015625,
          "content": "deepspeed>=0.8.3,<0.9\ntorch==1.13.1\ntransformers>=4.30.1\nclick>=8.0.4,<9\naccelerate==0.22.0\ndatasets==2.12.0\ntensorboard==2.12.2\npydantic==1.10.9"
        },
        {
          "name": "scripts",
          "type": "tree",
          "content": null
        },
        {
          "name": "training",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}