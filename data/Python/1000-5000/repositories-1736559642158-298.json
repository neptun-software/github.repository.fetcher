{
  "metadata": {
    "timestamp": 1736559642158,
    "page": 298,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjMwMA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "nv-tlabs/GET3D",
      "stars": 4274,
      "defaultBranch": "master",
      "files": [
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.009765625,
          "content": ".DS_Store\n"
        },
        {
          "name": "3dgan_data_split",
          "type": "tree",
          "content": null
        },
        {
          "name": "LICENSE.txt",
          "type": "blob",
          "size": 4.3388671875,
          "content": "\nCopyright (c) 2022, NVIDIA Corporation & affiliates. All rights reserved.\n\n\nNVIDIA Source Code License for GET3D\n\n\n=======================================================================\n\n1. Definitions\n\n“Licensor” means any person or entity that distributes its Work.\n\n“Work” means (a) the original work of authorship made available under\nthis license, which may include software, documentation, or other files,\nand (b) any additions to or derivative works  thereof  that are made\navailable under this license.\n\nThe terms “reproduce,” “reproduction,” “derivative works,” and\n“distribution” have the meaning as provided under U.S. copyright law;\nprovided, however, that for the purposes of this license, derivative works\nshall not include works that remain separable from, or merely link\n(or bind by name) to the interfaces of, the Work.\n\nWorks are “made available” under this license by including in or with\nthe Work either (a) a copyright notice referencing the applicability of\nthis license to the Work, or (b) a copy of this license.\n\n2. License Grant\n\n    2.1 Copyright Grant. Subject to the terms and conditions of this license,\n     each Licensor grants to you a perpetual, worldwide, non-exclusive,\n     royalty-free, copyright license to use, reproduce, prepare derivative\n     works of, publicly display, publicly perform, sublicense and distribute\n     its Work and any resulting derivative works in any form.\n\n3. Limitations\n\n    3.1 Redistribution. You may reproduce or distribute the Work only if\n    (a) you do so under this license, (b) you include a complete copy of\n    this license with your distribution, and (c) you retain without\n    modification any copyright, patent, trademark, or attribution notices\n    that are present in the Work.\n\n    3.2 Derivative Works. You may specify that additional or different terms\n     apply to the use, reproduction, and distribution of your derivative\n     works of the Work (“Your Terms”) only if (a) Your Terms provide that the\n     use limitation in Section 3.3 applies to your derivative works, and (b)\n     you identify the specific derivative works that are subject to Your Terms.\n     Notwithstanding Your Terms, this license (including the redistribution\n     requirements in Section 3.1) will continue to apply to the Work itself.\n\n    3.3 Use Limitation. The Work and any derivative works thereof only may be\n     used or intended for use non-commercially. Notwithstanding the foregoing,\n     NVIDIA Corporation and its affiliates may use the Work and any derivative\n     works commercially. As used herein, “non-commercially” means for research\n     or evaluation purposes only.\n\n    3.4 Patent Claims. If you bring or threaten to bring a patent claim against\n     any Licensor (including any claim, cross-claim or counterclaim in a lawsuit)\n     to enforce any patents that you allege are infringed by any Work, then your\n     rights under this license from such Licensor (including the grant in\n     Section 2.1) will terminate immediately.\n\n    3.5 Trademarks. This license does not grant any rights to use any Licensor’s\n     or its affiliates’ names, logos, or trademarks, except as necessary to\n     reproduce the notices described in this license.\n\n    3.6 Termination. If you violate any term of this license, then your rights\n     under this license (including the grant in Section 2.1) will terminate\n     immediately.\n\n4. Disclaimer of Warranty.\n\nTHE WORK IS PROVIDED “AS IS” WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,\nEITHER EXPRESS OR IMPLIED, INCLUDING WARRANTIES OR CONDITIONS OF\nMERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, TITLE OR NON-INFRINGEMENT.\nYOU BEAR THE RISK OF UNDERTAKING ANY ACTIVITIES UNDER THIS LICENSE.\n\n5. Limitation of Liability.\n\nEXCEPT AS PROHIBITED BY APPLICABLE LAW, IN NO EVENT AND UNDER NO LEGAL THEORY,\nWHETHER IN TORT (INCLUDING NEGLIGENCE), CONTRACT, OR OTHERWISE SHALL ANY\nLICENSOR BE LIABLE TO YOU FOR DAMAGES, INCLUDING ANY DIRECT, INDIRECT, SPECIAL,\nINCIDENTAL, OR CONSEQUENTIAL DAMAGES ARISING OUT OF OR RELATED TO THIS LICENSE,\nTHE USE OR INABILITY TO USE THE WORK (INCLUDING BUT NOT LIMITED TO LOSS OF\nGOODWILL, BUSINESS INTERRUPTION, LOST PROFITS OR DATA, COMPUTER FAILURE OR\nMALFUNCTION, OR ANY OTHER DAMAGES OR LOSSES), EVEN IF THE LICENSOR HAS BEEN\nADVISED OF THE POSSIBILITY OF SUCH DAMAGES.\n\n======================================================================="
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 9.455078125,
          "content": "## GET3D: A Generative Model of High Quality 3D Textured Shapes Learned from Images (NeurIPS 2022)<br><sub>Official PyTorch implementation </sub>\n\n![Teaser image](./docs/assets/get3d_model.png)\n\n**GET3D: A Generative Model of High Quality 3D Textured Shapes Learned from Images**<br>\n[Jun Gao](http://www.cs.toronto.edu/~jungao/)\n, [Tianchang Shen](http://www.cs.toronto.edu/~shenti11/)\n, [Zian Wang](http://www.cs.toronto.edu/~zianwang/),\n[Wenzheng Chen](http://www.cs.toronto.edu/~wenzheng/), [Kangxue Yin](https://kangxue.org/)\n, [Daiqing Li](https://scholar.google.ca/citations?user=8q2ISMIAAAAJ&hl=en),\n[Or Litany](https://orlitany.github.io/), [Zan Gojcic](https://zgojcic.github.io/),\n[Sanja Fidler](https://www.cs.toronto.edu/~fidler/) <br>\n**[Paper](https://nv-tlabs.github.io/GET3D/assets/paper.pdf)\n, [Project Page](https://nv-tlabs.github.io/GET3D/)**\n\nAbstract: *As several industries are moving towards modeling massive 3D virtual worlds,\nthe need for content creation tools that can scale in terms of the quantity, quality, and\ndiversity of 3D content is becoming evident. In our work, we aim to train performant 3D\ngenerative models that synthesize textured meshes which can be directly consumed by 3D\nrendering engines, thus immediately usable in downstream applications. Prior works on 3D\ngenerative modeling either lack geometric details, are limited in the mesh topology they\ncan produce, typically do not support textures, or utilize neural renderers in the\nsynthesis process, which makes their use in common 3D software non-trivial. In this work,\nwe introduce GET3D, a Generative model that directly generates Explicit Textured 3D meshes\nwith complex topology, rich geometric details, and high fidelity textures. We bridge\nrecent success in the differentiable surface modeling, differentiable rendering as well as\n2D Generative Adversarial Networks to train our model from 2D image collections. GET3D is\nable to generate high-quality 3D textured meshes, ranging from cars, chairs, animals,\nmotorbikes and human characters to buildings, achieving significant improvements over\nprevious methods.*\n\n![Teaser Results](./docs/assets/teaser_result.jpg)\n\nFor business inquiries, please visit our website and submit the\nform: [NVIDIA Research Licensing](https://www.nvidia.com/en-us/research/inquiries/)\n\n## News\n\n- 2023-09-15: We added the support for [FlexiCubes](https://research.nvidia.com/labs/toronto-ai/flexicubes/) as a drop-in replacement for DMTet. Please refer to this [section](https://github.com/nv-tlabs/GET3D#employing-flexicubes) for more details. \n- 2022-10-13: Pretrained model on Shapenet released! Check more details [here](./pretrained_model)\n- 2022-09-29: Code released!\n- 2022-09-22: Code will be uploaded next week!\n\n## Requirements\n\n* We recommend Linux for performance and compatibility reasons.\n* 1 &ndash; 8 high-end NVIDIA GPUs. We have done all testing and development using V100 or A100\n  GPUs.\n* 64-bit Python 3.8 and PyTorch 1.9.0. See https://pytorch.org for PyTorch install\n  instructions.\n* CUDA toolkit 11.1 or later.  (Why is a separate CUDA toolkit installation required? We\n  use the custom CUDA extensions from the StyleGAN3 repo. Please\n  see [Troubleshooting](https://github.com/NVlabs/stylegan3/blob/main/docs/troubleshooting.md#why-is-cuda-toolkit-installation-necessary))\n  .\n* We also recommend to install Nvdiffrast following instructions\n  from [official repo](https://github.com/NVlabs/nvdiffrast), and\n  install [Kaolin](https://github.com/NVIDIAGameWorks/kaolin).\n* We provide a [script](./install_get3d.sh) to install packages.\n\n### Server usage through Docker\n\n- Build Docker image\n\n```bash\ncd docker\nchmod +x make_image.sh\n./make_image.sh get3d:v1\n```\n\n- Start an interactive docker\n  container: `docker run --gpus device=all -it --rm -v YOUR_LOCAL_FOLDER:MOUNT_FOLDER -it get3d:v1 bash`\n\n## Preparing datasets\n\nGET3D is trained on synthetic dataset. We provide rendering scripts for Shapenet. Please\nrefer to [readme](./render_shapenet_data/README.md) to download shapenet dataset and\nrender it.\n\n## Employing FlexiCubes\nWe integrate [FlexiCubes](https://research.nvidia.com/labs/toronto-ai/flexicubes/), our lastest high-quality isosurface representation. To leverage FlexiCubes as an alternative to DMTet for isosurfacing, simply append `--iso_surface flexicubes` to the following training and inference commands.\n\n## Train the model\n\n#### Clone the gitlab code and necessary files:\n\n```bash\ncd YOUR_CODE_PATH\ngit clone git@github.com:nv-tlabs/GET3D.git\ncd GET3D; mkdir cache; cd cache\nwget https://api.ngc.nvidia.com/v2/models/nvidia/research/stylegan3/versions/1/files/metrics/inception-2015-12-05.pkl\n```\n\n#### Train the model\n\n```bash\ncd YOUR_CODE_PATH \nexport PYTHONPATH=$PWD:$PYTHONPATH\nexport CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7\n```\n\n- Train on the unified generator on cars, motorbikes or chair (Improved generator in\n  Appendix):\n\n```bash\npython train_3d.py --outdir=PATH_TO_LOG --data=PATH_TO_RENDER_IMG --camera_path PATH_TO_RENDER_CAMERA --gpus=8 --batch=32 --gamma=40 --data_camera_mode shapenet_car  --dmtet_scale 1.0  --use_shapenet_split 1  --one_3d_generator 1  --fp32 0\npython train_3d.py --outdir=PATH_TO_LOG --data=PATH_TO_RENDER_IMG --camera_path PATH_TO_RENDER_CAMERA --gpus=8 --batch=32 --gamma=80 --data_camera_mode shapenet_motorbike  --dmtet_scale 1.0  --use_shapenet_split 1  --one_3d_generator 1  --fp32 0\npython train_3d.py --outdir=PATH_TO_LOG --data=PATH_TO_RENDER_IMG --camera_path PATH_TO_RENDER_CAMERA --gpus=8 --batch=32 --gamma=400 --data_camera_mode shapenet_chair  --dmtet_scale 0.8  --use_shapenet_split 1  --one_3d_generator 1  --fp32 0\n```\n\n- If want to train on separate generators (main Figure in the paper):\n\n```bash\npython train_3d.py --outdir=PATH_TO_LOG --data=PATH_TO_RENDER_IMG --camera_path PATH_TO_RENDER_CAMERA --gpus=8 --batch=32 --gamma=40 --data_camera_mode shapenet_car  --dmtet_scale 1.0  --use_shapenet_split 1  --one_3d_generator 0\npython train_3d.py --outdir=PATH_TO_LOG --data=PATH_TO_RENDER_IMG --camera_path PATH_TO_RENDER_CAMERA --gpus=8 --batch=32 --gamma=80 --data_camera_mode shapenet_motorbike  --dmtet_scale 1.0  --use_shapenet_split 1  --one_3d_generator 0\npython train_3d.py --outdir=PATH_TO_LOG --data=PATH_TO_RENDER_IMG --camera_path PATH_TO_RENDER_CAMERA --gpus=8 --batch=32 --gamma=3200 --data_camera_mode shapenet_chair  --dmtet_scale 0.8  --use_shapenet_split 1  --one_3d_generator 0\n```\n\nIf want to debug the model first, reduce the number of gpus to 1 and batch size to 4 via:\n\n```bash\n--gpus=1 --batch=4\n```\n\n## Inference\n\n### Inference on a pretrained model for visualization\n\n- Download pretrained model from [here](https://drive.google.com/drive/folders/1oJ-FmyVYjIwBZKDAQ4N1EEcE9dJjumdW?usp=sharing).\n- Inference could operate on a single GPU with 16 GB memory.\n\n```bash\npython train_3d.py --outdir=save_inference_results/shapenet_car  --gpus=1 --batch=4 --gamma=40 --data_camera_mode shapenet_car  --dmtet_scale 1.0  --use_shapenet_split 1  --one_3d_generator 1  --fp32 0 --inference_vis 1 --resume_pretrain MODEL_PATH\npython train_3d.py --outdir=save_inference_results/shapenet_chair  --gpus=1 --batch=4 --gamma=40 --data_camera_mode shapenet_chair  --dmtet_scale 0.8  --use_shapenet_split 1  --one_3d_generator 1  --fp32 0 --inference_vis 1 --resume_pretrain MODEL_PATH\npython train_3d.py --outdir=save_inference_results/shapenet_motorbike  --gpus=1 --batch=4 --gamma=40 --data_camera_mode shapenet_motorbike  --dmtet_scale 1.0  --use_shapenet_split 1  --one_3d_generator 1  --fp32 0 --inference_vis 1 --resume_pretrain MODEL_PATH\n```\n\n- To generate mesh with textures, add one option to the inference\n  command: `--inference_to_generate_textured_mesh 1`\n\n- To generate the results with latent code interpolation, add one option to the inference\n  command: `--inference_save_interpolation 1`\n\n### Evaluation metrics\n\n##### Compute FID\n\n- To evaluate the model with FID metric, add one option to the inference\n  command: `--inference_compute_fid 1`\n\n##### Compute COV & MMD scores for LFD & CD\n\n- First generate 3D objects for evaluation, add one option to the inference\n  command: `--inference_generate_geo 1`\n- Following [README](./evaluation_scripts/README.md) to compute metrics.\n\n## License\n\nCopyright &copy; 2022, NVIDIA Corporation & affiliates. All rights reserved.\n\nThis work is made available under\nthe [Nvidia Source Code License](https://github.com/nv-tlabs/GET3D/blob/master/LICENSE.txt)\n.\n\n## Broader Information\n\nGET3D builds upon several previous works:\n\n- [Learning Deformable Tetrahedral Meshes for 3D Reconstruction (NeurIPS 2020)](https://nv-tlabs.github.io/DefTet/)\n- [Deep Marching Tetrahedra: a Hybrid Representation for High-Resolution 3D Shape Synthesis (NeurIPS 2021)](https://nv-tlabs.github.io/DMTet/)\n- [Extracting Triangular 3D Models, Materials, and Lighting From Images (CVPR 2022)](https://nvlabs.github.io/nvdiffrec/)\n- [EG3D: Efficient Geometry-aware 3D Generative Adversarial Networks (CVPR 2022)](https://nvlabs.github.io/eg3d/)\n- [DIB-R++: Learning to Predict Lighting and Material with a Hybrid Differentiable Renderer (NeurIPS 2021)](https://nv-tlabs.github.io/DIBRPlus/)\n- [Nvdiffrast – Modular Primitives for High-Performance Differentiable Rendering (SIGRAPH Asia 2020)](https://nvlabs.github.io/nvdiffrast/)\n\n## Citation\n\n```latex\n@inproceedings{gao2022get3d,\ntitle={GET3D: A Generative Model of High Quality 3D Textured Shapes Learned from Images},\nauthor={Jun Gao and Tianchang Shen and Zian Wang and Wenzheng Chen and Kangxue Yin\nand Daiqing Li and Or Litany and Zan Gojcic and Sanja Fidler},\nbooktitle={Advances In Neural Information Processing Systems},\nyear={2022}\n}\n```\n"
        },
        {
          "name": "data",
          "type": "tree",
          "content": null
        },
        {
          "name": "dnnlib",
          "type": "tree",
          "content": null
        },
        {
          "name": "docker",
          "type": "tree",
          "content": null
        },
        {
          "name": "docs",
          "type": "tree",
          "content": null
        },
        {
          "name": "evaluation_scripts",
          "type": "tree",
          "content": null
        },
        {
          "name": "install_get3d.sh",
          "type": "blob",
          "size": 0.9287109375,
          "content": "# Copyright (c) 2022, NVIDIA CORPORATION & AFFILIATES.  All rights reserved.\n#\n# NVIDIA CORPORATION & AFFILIATES and its licensors retain all intellectual property\n# and proprietary rights in and to this software, related documentation\n# and any modifications thereto.  Any use, reproduction, disclosure or\n# distribution of this software and related documentation without an express\n# license agreement from NVIDIA CORPORATION & AFFILIATES is strictly prohibited.\n\nconda create -n get3d python=3.8\nconda activate get3d\npip install torch==1.9.0+cu111 torchvision==0.10.0+cu111 torchaudio==0.9.0 -f https://download.pytorch.org/whl/torch_stable.html\npip install ninja xatlas gdown\npip install git+https://github.com/NVlabs/nvdiffrast/\npip install meshzoo ipdb imageio gputil h5py point-cloud-utils imageio imageio-ffmpeg==0.4.4 pyspng==0.1.0\npip install urllib3\npip install scipy\npip install click\npip install tqdm\npip install opencv-python==4.5.4.58\n\n"
        },
        {
          "name": "metrics",
          "type": "tree",
          "content": null
        },
        {
          "name": "pretrained_model",
          "type": "tree",
          "content": null
        },
        {
          "name": "render_shapenet_data",
          "type": "tree",
          "content": null
        },
        {
          "name": "torch_utils",
          "type": "tree",
          "content": null
        },
        {
          "name": "train_3d.py",
          "type": "blob",
          "size": 19.4248046875,
          "content": "# Copyright (c) 2022, NVIDIA CORPORATION & AFFILIATES.  All rights reserved.\n#\n# NVIDIA CORPORATION & AFFILIATES and its licensors retain all intellectual property\n# and proprietary rights in and to this software, related documentation\n# and any modifications thereto.  Any use, reproduction, disclosure or\n# distribution of this software and related documentation without an express\n# license agreement from NVIDIA CORPORATION & AFFILIATES is strictly prohibited.\n\nimport os\nimport click\nimport re\nimport json\nimport tempfile\nimport torch\nimport dnnlib\nfrom training import training_loop_3d\nfrom metrics import metric_main\nfrom torch_utils import training_stats\nfrom torch_utils import custom_ops\nfrom training import inference_3d\n\n\n# ----------------------------------------------------------------------------\ndef subprocess_fn(rank, c, temp_dir):\n    dnnlib.util.Logger(file_name=os.path.join(c.run_dir, 'log.txt'), file_mode='a', should_flush=True)\n\n    # Init torch.distributed.\n    if c.num_gpus > 1:\n        init_file = os.path.abspath(os.path.join(temp_dir, '.torch_distributed_init'))\n        if os.name == 'nt':\n            init_method = 'file:///' + init_file.replace('\\\\', '/')\n            torch.distributed.init_process_group(\n                backend='gloo', init_method=init_method, rank=rank, world_size=c.num_gpus)\n        else:\n            init_method = f'file://{init_file}'\n            torch.distributed.init_process_group(\n                backend='nccl', init_method=init_method, rank=rank, world_size=c.num_gpus)\n\n    # Init torch_utils.\n    sync_device = torch.device('cuda', rank) if c.num_gpus > 1 else None\n    training_stats.init_multiprocessing(rank=rank, sync_device=sync_device)\n    if rank != 0:\n        custom_ops.verbosity = 'none'\n\n    if c.inference_vis:\n        inference_3d.inference(rank=rank, **c)\n    # Execute training loop.\n    else:\n        training_loop_3d.training_loop(rank=rank, **c)\n\n\n# ----------------------------------------------------------------------------\n\ndef launch_training(c, desc, outdir, dry_run):\n    dnnlib.util.Logger(should_flush=True)\n\n    # Pick output directory.\n    prev_run_dirs = []\n    if os.path.isdir(outdir):\n        prev_run_dirs = [x for x in os.listdir(outdir) if os.path.isdir(os.path.join(outdir, x))]\n    prev_run_ids = [re.match(r'^\\d+', x) for x in prev_run_dirs]\n    prev_run_ids = [int(x.group()) for x in prev_run_ids if x is not None]\n    cur_run_id = max(prev_run_ids, default=-1) + 1\n    if c.inference_vis:\n        c.run_dir = os.path.join(outdir, 'inference')\n    else:\n        c.run_dir = os.path.join(outdir, f'{cur_run_id:05d}-{desc}')\n        assert not os.path.exists(c.run_dir)\n\n    # Print options.\n    print()\n    print('Training options:')\n    print(json.dumps(c, indent=2))\n    print()\n    print(f'Output directory:    {c.run_dir}')\n    print(f'Number of GPUs:      {c.num_gpus}')\n    print(f'Batch size:          {c.batch_size} images')\n    print(f'Training duration:   {c.total_kimg} kimg')\n    print(f'Dataset path:        {c.training_set_kwargs.path}')\n    print(f'Dataset size:        {c.training_set_kwargs.max_size} images')\n    print(f'Dataset resolution:  {c.training_set_kwargs.resolution}')\n    print(f'Dataset labels:      {c.training_set_kwargs.use_labels}')\n    print(f'Dataset x-flips:     {c.training_set_kwargs.xflip}')\n    print()\n\n    # Dry run?\n    if dry_run:\n        print('Dry run; exiting.')\n        return\n\n    # Create output directory.\n    print('Creating output directory...')\n    if not os.path.exists(c.run_dir):\n        os.makedirs(c.run_dir)\n    with open(os.path.join(c.run_dir, 'training_options.json'), 'wt') as f:\n        json.dump(c, f, indent=2)\n\n    # Launch processes.\n    print('Launching processes...')\n    torch.multiprocessing.set_start_method('spawn', force=True)\n    with tempfile.TemporaryDirectory() as temp_dir:\n        if c.num_gpus == 1:\n            subprocess_fn(rank=0, c=c, temp_dir=temp_dir)\n        else:\n            torch.multiprocessing.spawn(fn=subprocess_fn, args=(c, temp_dir), nprocs=c.num_gpus)\n\n\n# ----------------------------------------------------------------------------\ndef init_dataset_kwargs(data, opt=None):\n    try:\n        if opt.use_shapenet_split:\n            dataset_kwargs = dnnlib.EasyDict(\n                class_name='training.dataset.ImageFolderDataset',\n                path=data, use_labels=True, max_size=None, xflip=False,\n                resolution=opt.img_res,\n                data_camera_mode=opt.data_camera_mode,\n                add_camera_cond=opt.add_camera_cond,\n                camera_path=opt.camera_path,\n                split='test' if opt.inference_vis else 'train',\n            )\n        else:\n            dataset_kwargs = dnnlib.EasyDict(\n                class_name='training.dataset.ImageFolderDataset',\n                path=data, use_labels=True, max_size=None, xflip=False, resolution=opt.img_res,\n                data_camera_mode=opt.data_camera_mode,\n                add_camera_cond=opt.add_camera_cond,\n                camera_path=opt.camera_path,\n            )\n        dataset_obj = dnnlib.util.construct_class_by_name(**dataset_kwargs)  # Subclass of training.dataset.Dataset.\n        dataset_kwargs.camera_path = opt.camera_path\n        dataset_kwargs.resolution = dataset_obj.resolution  # Be explicit about resolution.\n        dataset_kwargs.use_labels = dataset_obj.has_labels  # Be explicit about labels.\n        dataset_kwargs.max_size = len(dataset_obj)  # Be explicit about dataset size.\n        return dataset_kwargs, dataset_obj.name\n    except IOError as err:\n        raise click.ClickException(f'--data: {err}')\n\n\n# ----------------------------------------------------------------------------\n\ndef parse_comma_separated_list(s):\n    if isinstance(s, list):\n        return s\n    if s is None or s.lower() == 'none' or s == '':\n        return []\n    return s.split(',')\n\n\n# ----------------------------------------------------------------------------\n\n@click.command()\n# Required from StyleGAN2.\n@click.option('--outdir', help='Where to save the results', metavar='DIR', required=True)\n@click.option('--cfg', help='Base configuration', type=click.Choice(['stylegan3-t', 'stylegan3-r', 'stylegan2']), default='stylegan2')\n@click.option('--gpus', help='Number of GPUs to use', metavar='INT', type=click.IntRange(min=1), required=True)\n@click.option('--batch', help='Total batch size', metavar='INT', type=click.IntRange(min=1), required=True)\n@click.option('--gamma', help='R1 regularization weight', metavar='FLOAT', type=click.FloatRange(min=0), required=True)\n# My custom configs\n### Configs for inference\n@click.option('--resume_pretrain', help='Resume from given network pickle', metavar='[PATH|URL]', type=str)\n@click.option('--inference_vis', help='whther we run infernce', metavar='BOOL', type=bool, default=False, show_default=True)\n@click.option('--inference_to_generate_textured_mesh', help='inference to generate textured meshes', metavar='BOOL', type=bool, default=False, show_default=False)\n@click.option('--inference_save_interpolation', help='inference to generate interpolation results', metavar='BOOL', type=bool, default=False, show_default=False)\n@click.option('--inference_compute_fid', help='inference to generate interpolation results', metavar='BOOL', type=bool, default=False, show_default=False)\n@click.option('--inference_generate_geo', help='inference to generate geometry points', metavar='BOOL', type=bool, default=False, show_default=False)\n### Configs for dataset\n\n@click.option('--data', help='Path to the Training data Images', metavar='[DIR]', type=str, default='./tmp')\n@click.option('--camera_path', help='Path to the camera root', metavar='[DIR]', type=str, default='./tmp')\n@click.option('--img_res', help='The resolution of image', metavar='INT', type=click.IntRange(min=1), default=1024)\n@click.option('--data_camera_mode', help='The type of dataset we are using', type=str, default='shapenet_car', show_default=True)\n@click.option('--use_shapenet_split', help='whether use the training split or all the data for training', metavar='BOOL', type=bool, default=False, show_default=False)\n### Configs for 3D generator##########\n@click.option('--iso_surface', help='Differentiable iso-surfacing method', type=click.Choice(['dmtet', 'flexicubes']), default='dmtet')\n@click.option('--use_style_mixing', help='whether use style mixing for generation during inference', metavar='BOOL', type=bool, default=True, show_default=False)\n@click.option('--one_3d_generator', help='whether we detach the gradient for empty object', metavar='BOOL', type=bool, default=True, show_default=True)\n@click.option('--dmtet_scale', help='Scale for the dimention of dmtet', metavar='FLOAT', type=click.FloatRange(min=0, max=10.0), default=1.0, show_default=True)\n@click.option('--n_implicit_layer', help='Number of Implicit FC layer for XYZPlaneTex model', metavar='INT', type=click.IntRange(min=1), default=1)\n@click.option('--feat_channel', help='Feature channel for TORGB layer', metavar='INT', type=click.IntRange(min=0), default=16)\n@click.option('--mlp_latent_channel', help='mlp_latent_channel for XYZPlaneTex network', metavar='INT', type=click.IntRange(min=8), default=32)\n@click.option('--deformation_multiplier', help='Multiplier for the predicted deformation', metavar='FLOAT', type=click.FloatRange(min=1.0), default=1.0, required=False)\n@click.option('--tri_plane_resolution', help='The resolution for tri plane', metavar='INT', type=click.IntRange(min=1), default=256)\n@click.option('--n_views', help='number of views when training generator', metavar='INT', type=click.IntRange(min=1), default=1)\n@click.option('--use_tri_plane', help='Whether use tri plane representation', metavar='BOOL', type=bool, default=True, show_default=True)\n@click.option('--tet_res', help='Resolution for teteahedron', metavar='INT', type=click.IntRange(min=1), default=90)\n@click.option('--latent_dim', help='Dimention for latent code', metavar='INT', type=click.IntRange(min=1), default=512)\n@click.option('--geometry_type', help='The type of geometry generator', type=str, default='conv3d', show_default=True)\n@click.option('--render_type', help='Type of renderer we used', metavar='STR', type=click.Choice(['neural_render', 'spherical_gaussian']), default='neural_render', show_default=True)\n### Configs for training loss and discriminator#\n@click.option('--d_architecture', help='The architecture for discriminator', metavar='STR', type=str, default='skip', show_default=True)\n@click.option('--use_pl_length', help='whether we apply path length regularization', metavar='BOOL', type=bool, default=False, show_default=False)  # We didn't use path lenth regularzation to avoid nan error\n@click.option('--gamma_mask', help='R1 regularization weight for mask', metavar='FLOAT', type=click.FloatRange(min=0), default=0.0, required=False)\n@click.option('--d_reg_interval', help='The internal for R1 regularization', metavar='INT', type=click.IntRange(min=1), default=16)\n@click.option('--add_camera_cond', help='Whether we add camera as condition for discriminator', metavar='BOOL', type=bool, default=True, show_default=True)\n@click.option('--lambda_flexicubes_surface_reg', help='Weights for flexicubes regularization L_dev', metavar='FLOAT', type=click.FloatRange(min=0), default=0.5, required=False)\n@click.option('--lambda_flexicubes_weights_reg', help='Weights for flexicubes regularization on weights', metavar='FLOAT', type=click.FloatRange(min=0), default=0.1, required=False)\n\n## Miscs\n# Optional features.\n@click.option('--cond', help='Train conditional model', metavar='BOOL', type=bool, default=False, show_default=True)\n@click.option('--freezed', help='Freeze first layers of D', metavar='INT', type=click.IntRange(min=0), default=0, show_default=True)\n# Misc hyperparameters.\n@click.option('--batch-gpu', help='Limit batch size per GPU', metavar='INT', type=click.IntRange(min=1), default=4)\n@click.option('--cbase', help='Capacity multiplier', metavar='INT', type=click.IntRange(min=1), default=32768, show_default=True)\n@click.option('--cmax', help='Max. feature maps', metavar='INT', type=click.IntRange(min=1), default=512, show_default=True)\n@click.option('--glr', help='G learning rate  [default: varies]', metavar='FLOAT', type=click.FloatRange(min=0))\n@click.option('--dlr', help='D learning rate', metavar='FLOAT', type=click.FloatRange(min=0), default=0.002, show_default=True)\n@click.option('--map-depth', help='Mapping network depth  [default: varies]', metavar='INT', type=click.IntRange(min=1))\n@click.option('--mbstd-group', help='Minibatch std group size', metavar='INT', type=click.IntRange(min=1), default=4, show_default=True)\n# Misc settings.\n@click.option('--desc', help='String to include in result dir name', metavar='STR', type=str)\n@click.option('--metrics', help='Quality metrics', metavar='[NAME|A,B,C|none]', type=parse_comma_separated_list, default='fid50k', show_default=True)\n@click.option('--kimg', help='Total training duration', metavar='KIMG', type=click.IntRange(min=1), default=20000, show_default=True)\n@click.option('--tick', help='How often to print progress', metavar='KIMG', type=click.IntRange(min=1), default=1, show_default=True)  ##\n@click.option('--snap', help='How often to save snapshots', metavar='TICKS', type=click.IntRange(min=1), default=50, show_default=True)  ###\n@click.option('--seed', help='Random seed', metavar='INT', type=click.IntRange(min=0), default=0, show_default=True)\n@click.option('--fp32', help='Disable mixed-precision', metavar='BOOL', type=bool, default=True, show_default=True)  # Let's use fp32 all the case without clamping\n@click.option('--nobench', help='Disable cuDNN benchmarking', metavar='BOOL', type=bool, default=False, show_default=True)\n@click.option('--workers', help='DataLoader worker processes', metavar='INT', type=click.IntRange(min=0), default=3, show_default=True)\n@click.option('-n', '--dry-run', help='Print training options and exit', is_flag=True)\ndef main(**kwargs):\n    # Initialize config.\n    print('==> start')\n    opts = dnnlib.EasyDict(kwargs)  # Command line arguments.\n    c = dnnlib.EasyDict()  # Main config dict.\n    c.G_kwargs = dnnlib.EasyDict(\n        class_name=None, z_dim=opts.latent_dim, w_dim=opts.latent_dim, mapping_kwargs=dnnlib.EasyDict())\n    c.D_kwargs = dnnlib.EasyDict(\n        class_name='training.networks_get3d.Discriminator', block_kwargs=dnnlib.EasyDict(),\n        mapping_kwargs=dnnlib.EasyDict(), epilogue_kwargs=dnnlib.EasyDict())\n    c.G_opt_kwargs = dnnlib.EasyDict(class_name='torch.optim.Adam', betas=[0, 0.99], eps=1e-8)\n    c.D_opt_kwargs = dnnlib.EasyDict(class_name='torch.optim.Adam', betas=[0, 0.99], eps=1e-8)\n    c.loss_kwargs = dnnlib.EasyDict(class_name='training.loss.StyleGAN2Loss')\n\n    c.data_loader_kwargs = dnnlib.EasyDict(pin_memory=True, prefetch_factor=2)\n    c.inference_vis = opts.inference_vis\n    # Training set.\n    if opts.inference_vis:\n        c.inference_to_generate_textured_mesh = opts.inference_to_generate_textured_mesh\n        c.inference_save_interpolation = opts.inference_save_interpolation\n        c.inference_compute_fid = opts.inference_compute_fid\n        c.inference_generate_geo = opts.inference_generate_geo\n\n    c.training_set_kwargs, dataset_name = init_dataset_kwargs(data=opts.data, opt=opts)\n    if opts.cond and not c.training_set_kwargs.use_labels:\n        raise click.ClickException('--cond=True requires labels specified in dataset.json')\n    c.training_set_kwargs.split = 'train' if opts.use_shapenet_split else 'all'\n    if opts.use_shapenet_split and opts.inference_vis:\n        c.training_set_kwargs.split = 'test'\n    c.training_set_kwargs.use_labels = opts.cond\n    c.training_set_kwargs.xflip = False\n    # Hyperparameters & settings.p\n    c.G_kwargs.iso_surface = opts.iso_surface\n    c.G_kwargs.one_3d_generator = opts.one_3d_generator\n    c.G_kwargs.n_implicit_layer = opts.n_implicit_layer\n    c.G_kwargs.deformation_multiplier = opts.deformation_multiplier\n    c.resume_pretrain = opts.resume_pretrain\n    c.D_reg_interval = opts.d_reg_interval\n    c.G_kwargs.use_style_mixing = opts.use_style_mixing\n    c.G_kwargs.dmtet_scale = opts.dmtet_scale\n    c.G_kwargs.feat_channel = opts.feat_channel\n    c.G_kwargs.mlp_latent_channel = opts.mlp_latent_channel\n    c.G_kwargs.tri_plane_resolution = opts.tri_plane_resolution\n    c.G_kwargs.n_views = opts.n_views\n\n    c.G_kwargs.render_type = opts.render_type\n    c.G_kwargs.use_tri_plane = opts.use_tri_plane\n    c.D_kwargs.data_camera_mode = opts.data_camera_mode\n    c.D_kwargs.add_camera_cond = opts.add_camera_cond\n\n    c.G_kwargs.tet_res = opts.tet_res\n\n    c.G_kwargs.geometry_type = opts.geometry_type\n    c.num_gpus = opts.gpus\n    c.batch_size = opts.batch\n    c.batch_gpu = opts.batch_gpu or opts.batch // opts.gpus\n    # c.G_kwargs.geo_pos_enc = opts.geo_pos_enc\n    c.G_kwargs.data_camera_mode = opts.data_camera_mode\n    c.G_kwargs.channel_base = c.D_kwargs.channel_base = opts.cbase\n    c.G_kwargs.channel_max = c.D_kwargs.channel_max = opts.cmax\n\n    c.G_kwargs.mapping_kwargs.num_layers = 8\n\n    c.D_kwargs.architecture = opts.d_architecture\n    c.D_kwargs.block_kwargs.freeze_layers = opts.freezed\n    c.D_kwargs.epilogue_kwargs.mbstd_group_size = opts.mbstd_group\n    c.loss_kwargs.gamma_mask = opts.gamma if opts.gamma_mask == 0.0 else opts.gamma_mask\n    c.loss_kwargs.r1_gamma = opts.gamma\n    c.loss_kwargs.lambda_flexicubes_surface_reg = opts.lambda_flexicubes_surface_reg\n    c.loss_kwargs.lambda_flexicubes_weights_reg = opts.lambda_flexicubes_weights_reg\n    c.G_opt_kwargs.lr = (0.002 if opts.cfg == 'stylegan2' else 0.0025) if opts.glr is None else opts.glr\n    c.D_opt_kwargs.lr = opts.dlr\n    c.metrics = opts.metrics\n    c.total_kimg = opts.kimg\n    c.kimg_per_tick = opts.tick\n    c.image_snapshot_ticks = c.network_snapshot_ticks = opts.snap\n    c.random_seed = c.training_set_kwargs.random_seed = opts.seed\n    c.data_loader_kwargs.num_workers = opts.workers\n    c.network_snapshot_ticks = 200\n    # Sanity checks.\n    if c.batch_size % c.num_gpus != 0:\n        raise click.ClickException('--batch must be a multiple of --gpus')\n    if c.batch_size % (c.num_gpus * c.batch_gpu) != 0:\n        raise click.ClickException('--batch must be a multiple of --gpus times --batch-gpu')\n    if c.batch_gpu < c.D_kwargs.epilogue_kwargs.mbstd_group_size:\n        raise click.ClickException('--batch-gpu cannot be smaller than --mbstd')\n    if any(not metric_main.is_valid_metric(metric) for metric in c.metrics):\n        raise click.ClickException(\n            '\\n'.join(['--metrics can only contain the following values:'] + metric_main.list_valid_metrics()))\n\n    # Base configuration.\n    c.ema_kimg = c.batch_size * 10 / 32\n    c.G_kwargs.class_name = 'training.networks_get3d.GeneratorDMTETMesh'\n    c.loss_kwargs.style_mixing_prob = 0.9  # Enable style mixing regularization.\n    c.loss_kwargs.pl_weight = 0.0  # Enable path length regularization.\n    c.G_reg_interval = 4  # Enable lazy regularization for G.\n    c.G_kwargs.fused_modconv_default = 'inference_only'  # Speed up training by using regular convolutions instead of grouped convolutions.\n    # Performance-related toggles.\n    if opts.fp32:\n        c.G_kwargs.num_fp16_res = c.D_kwargs.num_fp16_res = 0\n        c.G_kwargs.conv_clamp = c.D_kwargs.conv_clamp = None\n    if opts.nobench:\n        c.cudnn_benchmark = False\n\n    # Description string.\n    desc = f'{opts.cfg:s}-{dataset_name:s}-gpus{c.num_gpus:d}-batch{c.batch_size:d}-gamma{c.loss_kwargs.r1_gamma:g}'\n    if opts.desc is not None:\n        desc += f'-{opts.desc}'\n    # Launch.\n    print('==> launch training')\n    launch_training(c=c, desc=desc, outdir=opts.outdir, dry_run=opts.dry_run)\n\n\n# ----------------------------------------------------------------------------\n#\nif __name__ == \"__main__\":\n    main()  # pylint: disable=no-value-for-parameter\n"
        },
        {
          "name": "training",
          "type": "tree",
          "content": null
        },
        {
          "name": "uni_rep",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}