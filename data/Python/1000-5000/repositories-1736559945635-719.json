{
  "metadata": {
    "timestamp": 1736559945635,
    "page": 719,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjcyMA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "TencentARC/InstantMesh",
      "stars": 3533,
      "defaultBranch": "main",
      "files": [
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.44921875,
          "content": "# Byte-compiled / optimized / DLL files\n__pycache__/\n*.py[cod]\n*$py.class\n\n# C extensions\n*.so\n\n# Distribution / packaging\n.Python\nbuild/\ndevelop-eggs/\ndist/\neggs/\n.eggs/\n.vscode/\nlib/\nlib64/\nparts/\nsdist/\nvar/\nwheels/\n*.egg-info/\n.installed.cfg\n*.egg\nMANIFEST\n\n.DS_Store\n\ntools/objaverse_rendering/blender-3.2.2-linux-x64/\ntools/objaverse_rendering/output/\nckpts/\nlightning_logs/\nlogs/\n.trash/\n.env/\noutputs/\nfigures*/\n\n# Useless Files\n*.sh\nblender/\n.restore/"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 11.0908203125,
          "content": "                                 Apache License\n                           Version 2.0, January 2004\n                        http://www.apache.org/licenses/\n\n   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n\n   1. Definitions.\n\n      \"License\" shall mean the terms and conditions for use, reproduction,\n      and distribution as defined by Sections 1 through 9 of this document.\n\n      \"Licensor\" shall mean the copyright owner or entity authorized by\n      the copyright owner that is granting the License.\n\n      \"Legal Entity\" shall mean the union of the acting entity and all\n      other entities that control, are controlled by, or are under common\n      control with that entity. For the purposes of this definition,\n      \"control\" means (i) the power, direct or indirect, to cause the\n      direction or management of such entity, whether by contract or\n      otherwise, or (ii) ownership of fifty percent (50%) or more of the\n      outstanding shares, or (iii) beneficial ownership of such entity.\n\n      \"You\" (or \"Your\") shall mean an individual or Legal Entity\n      exercising permissions granted by this License.\n\n      \"Source\" form shall mean the preferred form for making modifications,\n      including but not limited to software source code, documentation\n      source, and configuration files.\n\n      \"Object\" form shall mean any form resulting from mechanical\n      transformation or translation of a Source form, including but\n      not limited to compiled object code, generated documentation,\n      and conversions to other media types.\n\n      \"Work\" shall mean the work of authorship, whether in Source or\n      Object form, made available under the License, as indicated by a\n      copyright notice that is included in or attached to the work\n      (an example is provided in the Appendix below).\n\n      \"Derivative Works\" shall mean any work, whether in Source or Object\n      form, that is based on (or derived from) the Work and for which the\n      editorial revisions, annotations, elaborations, or other modifications\n      represent, as a whole, an original work of authorship. For the purposes\n      of this License, Derivative Works shall not include works that remain\n      separable from, or merely link (or bind by name) to the interfaces of,\n      the Work and Derivative Works thereof.\n\n      \"Contribution\" shall mean any work of authorship, including\n      the original version of the Work and any modifications or additions\n      to that Work or Derivative Works thereof, that is intentionally\n      submitted to Licensor for inclusion in the Work by the copyright owner\n      or by an individual or Legal Entity authorized to submit on behalf of\n      the copyright owner. For the purposes of this definition, \"submitted\"\n      means any form of electronic, verbal, or written communication sent\n      to the Licensor or its representatives, including but not limited to\n      communication on electronic mailing lists, source code control systems,\n      and issue tracking systems that are managed by, or on behalf of, the\n      Licensor for the purpose of discussing and improving the Work, but\n      excluding communication that is conspicuously marked or otherwise\n      designated in writing by the copyright owner as \"Not a Contribution.\"\n\n      \"Contributor\" shall mean Licensor and any individual or Legal Entity\n      on behalf of whom a Contribution has been received by Licensor and\n      subsequently incorporated within the Work.\n\n   2. Grant of Copyright License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      copyright license to reproduce, prepare Derivative Works of,\n      publicly display, publicly perform, sublicense, and distribute the\n      Work and such Derivative Works in Source or Object form.\n\n   3. Grant of Patent License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      (except as stated in this section) patent license to make, have made,\n      use, offer to sell, sell, import, and otherwise transfer the Work,\n      where such license applies only to those patent claims licensable\n      by such Contributor that are necessarily infringed by their\n      Contribution(s) alone or by combination of their Contribution(s)\n      with the Work to which such Contribution(s) was submitted. If You\n      institute patent litigation against any entity (including a\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\n      or a Contribution incorporated within the Work constitutes direct\n      or contributory patent infringement, then any patent licenses\n      granted to You under this License for that Work shall terminate\n      as of the date such litigation is filed.\n\n   4. Redistribution. You may reproduce and distribute copies of the\n      Work or Derivative Works thereof in any medium, with or without\n      modifications, and in Source or Object form, provided that You\n      meet the following conditions:\n\n      (a) You must give any other recipients of the Work or\n          Derivative Works a copy of this License; and\n\n      (b) You must cause any modified files to carry prominent notices\n          stating that You changed the files; and\n\n      (c) You must retain, in the Source form of any Derivative Works\n          that You distribute, all copyright, patent, trademark, and\n          attribution notices from the Source form of the Work,\n          excluding those notices that do not pertain to any part of\n          the Derivative Works; and\n\n      (d) If the Work includes a \"NOTICE\" text file as part of its\n          distribution, then any Derivative Works that You distribute must\n          include a readable copy of the attribution notices contained\n          within such NOTICE file, excluding those notices that do not\n          pertain to any part of the Derivative Works, in at least one\n          of the following places: within a NOTICE text file distributed\n          as part of the Derivative Works; within the Source form or\n          documentation, if provided along with the Derivative Works; or,\n          within a display generated by the Derivative Works, if and\n          wherever such third-party notices normally appear. The contents\n          of the NOTICE file are for informational purposes only and\n          do not modify the License. You may add Your own attribution\n          notices within Derivative Works that You distribute, alongside\n          or as an addendum to the NOTICE text from the Work, provided\n          that such additional attribution notices cannot be construed\n          as modifying the License.\n\n      You may add Your own copyright statement to Your modifications and\n      may provide additional or different license terms and conditions\n      for use, reproduction, or distribution of Your modifications, or\n      for any such Derivative Works as a whole, provided Your use,\n      reproduction, and distribution of the Work otherwise complies with\n      the conditions stated in this License.\n\n   5. Submission of Contributions. Unless You explicitly state otherwise,\n      any Contribution intentionally submitted for inclusion in the Work\n      by You to the Licensor shall be under the terms and conditions of\n      this License, without any additional terms or conditions.\n      Notwithstanding the above, nothing herein shall supersede or modify\n      the terms of any separate license agreement you may have executed\n      with Licensor regarding such Contributions.\n\n   6. Trademarks. This License does not grant permission to use the trade\n      names, trademarks, service marks, or product names of the Licensor,\n      except as required for reasonable and customary use in describing the\n      origin of the Work and reproducing the content of the NOTICE file.\n\n   7. Disclaimer of Warranty. Unless required by applicable law or\n      agreed to in writing, Licensor provides the Work (and each\n      Contributor provides its Contributions) on an \"AS IS\" BASIS,\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n      implied, including, without limitation, any warranties or conditions\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n      PARTICULAR PURPOSE. You are solely responsible for determining the\n      appropriateness of using or redistributing the Work and assume any\n      risks associated with Your exercise of permissions under this License.\n\n   8. Limitation of Liability. In no event and under no legal theory,\n      whether in tort (including negligence), contract, or otherwise,\n      unless required by applicable law (such as deliberate and grossly\n      negligent acts) or agreed to in writing, shall any Contributor be\n      liable to You for damages, including any direct, indirect, special,\n      incidental, or consequential damages of any character arising as a\n      result of this License or out of the use or inability to use the\n      Work (including but not limited to damages for loss of goodwill,\n      work stoppage, computer failure or malfunction, or any and all\n      other commercial damages or losses), even if such Contributor\n      has been advised of the possibility of such damages.\n\n   9. Accepting Warranty or Additional Liability. While redistributing\n      the Work or Derivative Works thereof, You may choose to offer,\n      and charge a fee for, acceptance of support, warranty, indemnity,\n      or other liability obligations and/or rights consistent with this\n      License. However, in accepting such obligations, You may act only\n      on Your own behalf and on Your sole responsibility, not on behalf\n      of any other Contributor, and only if You agree to indemnify,\n      defend, and hold each Contributor harmless for any liability\n      incurred by, or claims asserted against, such Contributor by reason\n      of your accepting any such warranty or additional liability.\n\n   END OF TERMS AND CONDITIONS\n\n   APPENDIX: How to apply the Apache License to your work.\n\n      To apply the Apache License to your work, attach the following\n      boilerplate notice, with the fields enclosed by brackets \"[]\"\n      replaced with your own identifying information. (Don't include\n      the brackets!)  The text should be enclosed in the appropriate\n      comment syntax for the file format. We also recommend that a\n      file or class name and description of purpose be included on the\n      same \"printed page\" as the copyright notice for easier\n      identification within third-party archives.\n\n   Copyright [yyyy] [name of copyright owner]\n\n   Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 6.544921875,
          "content": "<div align=\"center\">\n  \n# InstantMesh: Efficient 3D Mesh Generation from a Single Image with Sparse-view Large Reconstruction Models\n\n<a href=\"https://arxiv.org/abs/2404.07191\"><img src=\"https://img.shields.io/badge/ArXiv-2404.07191-brightgreen\"></a> \n<a href=\"https://huggingface.co/TencentARC/InstantMesh\"><img src=\"https://img.shields.io/badge/%F0%9F%A4%97%20Model_Card-Huggingface-orange\"></a> \n<a href=\"https://huggingface.co/spaces/TencentARC/InstantMesh\"><img src=\"https://img.shields.io/badge/%F0%9F%A4%97%20Gradio%20Demo-Huggingface-orange\"></a> <br>\n<a href=\"https://replicate.com/camenduru/instantmesh\"><img src=\"https://img.shields.io/badge/Demo-Replicate-blue\"></a>\n<a href=\"https://colab.research.google.com/github/camenduru/InstantMesh-jupyter/blob/main/InstantMesh_jupyter.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\"></a>\n<a href=\"https://github.com/jtydhr88/ComfyUI-InstantMesh\"><img src=\"https://img.shields.io/badge/Demo-ComfyUI-8A2BE2\"></a>\n\n</div>\n\n---\n\nThis repo is the official implementation of InstantMesh, a feed-forward framework for efficient 3D mesh generation from a single image based on the LRM/Instant3D architecture.\n\nhttps://github.com/TencentARC/InstantMesh/assets/20635237/dab3511e-e7c6-4c0b-bab7-15772045c47d\n\n# üö© Features and Todo List\n- [x] üî•üî• Release Zero123++ fine-tuning code. \n- [x] üî•üî• Support for running gradio demo on two GPUs to save memory.\n- [x] üî•üî• Support for running demo with docker. Please refer to the [docker](docker/) directory.\n- [x] Release inference and training code.\n- [x] Release model weights.\n- [x] Release huggingface gradio demo. Please try it at [demo](https://huggingface.co/spaces/TencentARC/InstantMesh) link.\n- [ ] Add support for more multi-view diffusion models.\n\n# ‚öôÔ∏è Dependencies and Installation\n\nWe recommend using `Python>=3.10`, `PyTorch>=2.1.0`, and `CUDA>=12.1`.\n```bash\nconda create --name instantmesh python=3.10\nconda activate instantmesh\npip install -U pip\n\n# Ensure Ninja is installed\nconda install Ninja\n\n# Install the correct version of CUDA\nconda install cuda -c nvidia/label/cuda-12.1.0\n\n# Install PyTorch and xformers\n# You may need to install another xformers version if you use a different PyTorch version\npip install torch==2.1.0 torchvision==0.16.0 torchaudio==2.1.0 --index-url https://download.pytorch.org/whl/cu121\npip install xformers==0.0.22.post7\n\n# Install other requirements\npip install -r requirements.txt\n```\n\n# üí´ How to Use\n\n## Download the models\n\nWe provide 4 sparse-view reconstruction model variants and a customized Zero123++ UNet for white-background image generation in the [model card](https://huggingface.co/TencentARC/InstantMesh).\n\nOur inference script will download the models automatically. Alternatively, you can manually download the models and put them under the `ckpts/` directory.\n\nBy default, we use the `instant-mesh-large` reconstruction model variant.\n\n## Start a local gradio demo\n\nTo start a gradio demo in your local machine, simply run:\n```bash\npython app.py\n```\n\nIf you have multiple GPUs in your machine, the demo app will run on two GPUs automatically to save memory. You can also force it to run on a single GPU:\n```bash\nCUDA_VISIBLE_DEVICES=0 python app.py\n```\n\nAlternatively, you can run the demo with docker. Please follow the instructions in the [docker](docker/) directory.\n\n## Running with command line\n\nTo generate 3D meshes from images via command line, simply run:\n```bash\npython run.py configs/instant-mesh-large.yaml examples/hatsune_miku.png --save_video\n```\n\nWe use [rembg](https://github.com/danielgatis/rembg) to segment the foreground object. If the input image already has an alpha mask, please specify the `no_rembg` flag:\n```bash\npython run.py configs/instant-mesh-large.yaml examples/hatsune_miku.png --save_video --no_rembg\n```\n\nBy default, our script exports a `.obj` mesh with vertex colors, please specify the `--export_texmap` flag if you hope to export a mesh with a texture map instead (this will cost longer time):\n```bash\npython run.py configs/instant-mesh-large.yaml examples/hatsune_miku.png --save_video --export_texmap\n```\n\nPlease use a different `.yaml` config file in the [configs](./configs) directory if you hope to use other reconstruction model variants. For example, using the `instant-nerf-large` model for generation:\n```bash\npython run.py configs/instant-nerf-large.yaml examples/hatsune_miku.png --save_video\n```\n**Note:** When using the `NeRF` model variants for image-to-3D generation, exporting a mesh with texture map by specifying `--export_texmap` may cost long time in the UV unwarping step since the default iso-surface extraction resolution is `256`. You can set a lower iso-surface extraction resolution in the config file.\n\n# üíª Training\n\nWe provide our training code to facilitate future research. But we cannot provide the training dataset due to its size. Please refer to our [dataloader](src/data/objaverse.py) for more details.\n\nTo train the sparse-view reconstruction models, please run:\n```bash\n# Training on NeRF representation\npython train.py --base configs/instant-nerf-large-train.yaml --gpus 0,1,2,3,4,5,6,7 --num_nodes 1\n\n# Training on Mesh representation\npython train.py --base configs/instant-mesh-large-train.yaml --gpus 0,1,2,3,4,5,6,7 --num_nodes 1\n```\n\nWe also provide our Zero123++ fine-tuning code since it is frequently requested. The running command is:\n```bash\npython train.py --base configs/zero123plus-finetune.yaml --gpus 0,1,2,3,4,5,6,7 --num_nodes 1\n```\n\n# :books: Citation\n\nIf you find our work useful for your research or applications, please cite using this BibTeX:\n\n```BibTeX\n@article{xu2024instantmesh,\n  title={InstantMesh: Efficient 3D Mesh Generation from a Single Image with Sparse-view Large Reconstruction Models},\n  author={Xu, Jiale and Cheng, Weihao and Gao, Yiming and Wang, Xintao and Gao, Shenghua and Shan, Ying},\n  journal={arXiv preprint arXiv:2404.07191},\n  year={2024}\n}\n```\n\n# ü§ó Acknowledgements\n\nWe thank the authors of the following projects for their excellent contributions to 3D generative AI!\n\n- [Zero123++](https://github.com/SUDO-AI-3D/zero123plus)\n- [OpenLRM](https://github.com/3DTopia/OpenLRM)\n- [FlexiCubes](https://github.com/nv-tlabs/FlexiCubes)\n- [Instant3D](https://instant-3d.github.io/)\n\nThank [@camenduru](https://github.com/camenduru) for implementing [Replicate Demo](https://replicate.com/camenduru/instantmesh) and [Colab Demo](https://colab.research.google.com/github/camenduru/InstantMesh-jupyter/blob/main/InstantMesh_jupyter.ipynb)!  \nThank [@jtydhr88](https://github.com/jtydhr88) for implementing [ComfyUI support](https://github.com/jtydhr88/ComfyUI-InstantMesh)!\n"
        },
        {
          "name": "app.py",
          "type": "blob",
          "size": 13.80859375,
          "content": "import os\nimport imageio\nimport numpy as np\nimport torch\nimport rembg\nfrom PIL import Image\nfrom torchvision.transforms import v2\nfrom pytorch_lightning import seed_everything\nfrom omegaconf import OmegaConf\nfrom einops import rearrange, repeat\nfrom tqdm import tqdm\nfrom diffusers import DiffusionPipeline, EulerAncestralDiscreteScheduler\n\nfrom src.utils.train_util import instantiate_from_config\nfrom src.utils.camera_util import (\n    FOV_to_intrinsics, \n    get_zero123plus_input_cameras,\n    get_circular_camera_poses,\n)\nfrom src.utils.mesh_util import save_obj, save_glb\nfrom src.utils.infer_util import remove_background, resize_foreground, images_to_video\n\nimport tempfile\nfrom huggingface_hub import hf_hub_download\n\n\nif torch.cuda.is_available() and torch.cuda.device_count() >= 2:\n    device0 = torch.device('cuda:0')\n    device1 = torch.device('cuda:1')\nelse:\n    device0 = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    device1 = device0\n\n# Define the cache directory for model files\nmodel_cache_dir = './ckpts/'\nos.makedirs(model_cache_dir, exist_ok=True)\n\ndef get_render_cameras(batch_size=1, M=120, radius=2.5, elevation=10.0, is_flexicubes=False):\n    \"\"\"\n    Get the rendering camera parameters.\n    \"\"\"\n    c2ws = get_circular_camera_poses(M=M, radius=radius, elevation=elevation)\n    if is_flexicubes:\n        cameras = torch.linalg.inv(c2ws)\n        cameras = cameras.unsqueeze(0).repeat(batch_size, 1, 1, 1)\n    else:\n        extrinsics = c2ws.flatten(-2)\n        intrinsics = FOV_to_intrinsics(30.0).unsqueeze(0).repeat(M, 1, 1).float().flatten(-2)\n        cameras = torch.cat([extrinsics, intrinsics], dim=-1)\n        cameras = cameras.unsqueeze(0).repeat(batch_size, 1, 1)\n    return cameras\n\n\ndef images_to_video(images, output_path, fps=30):\n    # images: (N, C, H, W)\n    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n    frames = []\n    for i in range(images.shape[0]):\n        frame = (images[i].permute(1, 2, 0).cpu().numpy() * 255).astype(np.uint8).clip(0, 255)\n        assert frame.shape[0] == images.shape[2] and frame.shape[1] == images.shape[3], \\\n            f\"Frame shape mismatch: {frame.shape} vs {images.shape}\"\n        assert frame.min() >= 0 and frame.max() <= 255, \\\n            f\"Frame value out of range: {frame.min()} ~ {frame.max()}\"\n        frames.append(frame)\n    imageio.mimwrite(output_path, np.stack(frames), fps=fps, codec='h264')\n\n\n###############################################################################\n# Configuration.\n###############################################################################\n\nseed_everything(0)\n\nconfig_path = 'configs/instant-mesh-large.yaml'\nconfig = OmegaConf.load(config_path)\nconfig_name = os.path.basename(config_path).replace('.yaml', '')\nmodel_config = config.model_config\ninfer_config = config.infer_config\n\nIS_FLEXICUBES = True if config_name.startswith('instant-mesh') else False\n\ndevice = torch.device('cuda')\n\n# load diffusion model\nprint('Loading diffusion model ...')\npipeline = DiffusionPipeline.from_pretrained(\n    \"sudo-ai/zero123plus-v1.2\", \n    custom_pipeline=\"zero123plus\",\n    torch_dtype=torch.float16,\n    cache_dir=model_cache_dir\n)\npipeline.scheduler = EulerAncestralDiscreteScheduler.from_config(\n    pipeline.scheduler.config, timestep_spacing='trailing'\n)\n\n# load custom white-background UNet\nunet_ckpt_path = hf_hub_download(repo_id=\"TencentARC/InstantMesh\", filename=\"diffusion_pytorch_model.bin\", repo_type=\"model\", cache_dir=model_cache_dir)\nstate_dict = torch.load(unet_ckpt_path, map_location='cpu')\npipeline.unet.load_state_dict(state_dict, strict=True)\n\npipeline = pipeline.to(device0)\n\n# load reconstruction model\nprint('Loading reconstruction model ...')\nmodel_ckpt_path = hf_hub_download(repo_id=\"TencentARC/InstantMesh\", filename=\"instant_mesh_large.ckpt\", repo_type=\"model\", cache_dir=model_cache_dir)\nmodel = instantiate_from_config(model_config)\nstate_dict = torch.load(model_ckpt_path, map_location='cpu')['state_dict']\nstate_dict = {k[14:]: v for k, v in state_dict.items() if k.startswith('lrm_generator.') and 'source_camera' not in k}\nmodel.load_state_dict(state_dict, strict=True)\n\nmodel = model.to(device1)\nif IS_FLEXICUBES:\n    model.init_flexicubes_geometry(device1, fovy=30.0)\nmodel = model.eval()\n\nprint('Loading Finished!')\n\n\ndef check_input_image(input_image):\n    if input_image is None:\n        raise gr.Error(\"No image uploaded!\")\n\n\ndef preprocess(input_image, do_remove_background):\n\n    rembg_session = rembg.new_session() if do_remove_background else None\n    if do_remove_background:\n        input_image = remove_background(input_image, rembg_session)\n        input_image = resize_foreground(input_image, 0.85)\n\n    return input_image\n\n\ndef generate_mvs(input_image, sample_steps, sample_seed):\n\n    seed_everything(sample_seed)\n    \n    # sampling\n    generator = torch.Generator(device=device0)\n    z123_image = pipeline(\n        input_image, \n        num_inference_steps=sample_steps, \n        generator=generator,\n    ).images[0]\n\n    show_image = np.asarray(z123_image, dtype=np.uint8)\n    show_image = torch.from_numpy(show_image)     # (960, 640, 3)\n    show_image = rearrange(show_image, '(n h) (m w) c -> (n m) h w c', n=3, m=2)\n    show_image = rearrange(show_image, '(n m) h w c -> (n h) (m w) c', n=2, m=3)\n    show_image = Image.fromarray(show_image.numpy())\n\n    return z123_image, show_image\n\n\ndef make_mesh(mesh_fpath, planes):\n\n    mesh_basename = os.path.basename(mesh_fpath).split('.')[0]\n    mesh_dirname = os.path.dirname(mesh_fpath)\n    mesh_glb_fpath = os.path.join(mesh_dirname, f\"{mesh_basename}.glb\")\n        \n    with torch.no_grad():\n        # get mesh\n\n        mesh_out = model.extract_mesh(\n            planes,\n            use_texture_map=False,\n            **infer_config,\n        )\n\n        vertices, faces, vertex_colors = mesh_out\n        vertices = vertices[:, [1, 2, 0]]\n        \n        save_glb(vertices, faces, vertex_colors, mesh_glb_fpath)\n        save_obj(vertices, faces, vertex_colors, mesh_fpath)\n        \n        print(f\"Mesh saved to {mesh_fpath}\")\n\n    return mesh_fpath, mesh_glb_fpath\n\n\ndef make3d(images):\n\n    images = np.asarray(images, dtype=np.float32) / 255.0\n    images = torch.from_numpy(images).permute(2, 0, 1).contiguous().float()     # (3, 960, 640)\n    images = rearrange(images, 'c (n h) (m w) -> (n m) c h w', n=3, m=2)        # (6, 3, 320, 320)\n\n    input_cameras = get_zero123plus_input_cameras(batch_size=1, radius=4.0).to(device1)\n    render_cameras = get_render_cameras(\n        batch_size=1, radius=4.5, elevation=20.0, is_flexicubes=IS_FLEXICUBES).to(device1)\n\n    images = images.unsqueeze(0).to(device1)\n    images = v2.functional.resize(images, (320, 320), interpolation=3, antialias=True).clamp(0, 1)\n\n    mesh_fpath = tempfile.NamedTemporaryFile(suffix=f\".obj\", delete=False).name\n    print(mesh_fpath)\n    mesh_basename = os.path.basename(mesh_fpath).split('.')[0]\n    mesh_dirname = os.path.dirname(mesh_fpath)\n    video_fpath = os.path.join(mesh_dirname, f\"{mesh_basename}.mp4\")\n\n    with torch.no_grad():\n        # get triplane\n        planes = model.forward_planes(images, input_cameras)\n\n        # get video\n        chunk_size = 20 if IS_FLEXICUBES else 1\n        render_size = 384\n        \n        frames = []\n        for i in tqdm(range(0, render_cameras.shape[1], chunk_size)):\n            if IS_FLEXICUBES:\n                frame = model.forward_geometry(\n                    planes,\n                    render_cameras[:, i:i+chunk_size],\n                    render_size=render_size,\n                )['img']\n            else:\n                frame = model.synthesizer(\n                    planes,\n                    cameras=render_cameras[:, i:i+chunk_size],\n                    render_size=render_size,\n                )['images_rgb']\n            frames.append(frame)\n        frames = torch.cat(frames, dim=1)\n\n        images_to_video(\n            frames[0],\n            video_fpath,\n            fps=30,\n        )\n\n        print(f\"Video saved to {video_fpath}\")\n\n    mesh_fpath, mesh_glb_fpath = make_mesh(mesh_fpath, planes)\n\n    return video_fpath, mesh_fpath, mesh_glb_fpath\n\n\nimport gradio as gr\n\n_HEADER_ = '''\n<h2><b>Official ü§ó Gradio Demo</b></h2><h2><a href='https://github.com/TencentARC/InstantMesh' target='_blank'><b>InstantMesh: Efficient 3D Mesh Generation from a Single Image with Sparse-view Large Reconstruction Models</b></a></h2>\n\n**InstantMesh** is a feed-forward framework for efficient 3D mesh generation from a single image based on the LRM/Instant3D architecture.\n\nCode: <a href='https://github.com/TencentARC/InstantMesh' target='_blank'>GitHub</a>. Techenical report: <a href='https://arxiv.org/abs/2404.07191' target='_blank'>ArXiv</a>.\n\n‚ùóÔ∏è‚ùóÔ∏è‚ùóÔ∏è**Important Notes:**\n- Our demo can export a .obj mesh with vertex colors or a .glb mesh now. If you prefer to export a .obj mesh with a **texture map**, please refer to our <a href='https://github.com/TencentARC/InstantMesh?tab=readme-ov-file#running-with-command-line' target='_blank'>Github Repo</a>.\n- The 3D mesh generation results highly depend on the quality of generated multi-view images. Please try a different **seed value** if the result is unsatisfying (Default: 42).\n'''\n\n_CITE_ = r\"\"\"\nIf InstantMesh is helpful, please help to ‚≠ê the <a href='https://github.com/TencentARC/InstantMesh' target='_blank'>Github Repo</a>. Thanks! [![GitHub Stars](https://img.shields.io/github/stars/TencentARC/InstantMesh?style=social)](https://github.com/TencentARC/InstantMesh)\n---\nüìù **Citation**\n\nIf you find our work useful for your research or applications, please cite using this bibtex:\n```bibtex\n@article{xu2024instantmesh,\n  title={InstantMesh: Efficient 3D Mesh Generation from a Single Image with Sparse-view Large Reconstruction Models},\n  author={Xu, Jiale and Cheng, Weihao and Gao, Yiming and Wang, Xintao and Gao, Shenghua and Shan, Ying},\n  journal={arXiv preprint arXiv:2404.07191},\n  year={2024}\n}\n```\n\nüìã **License**\n\nApache-2.0 LICENSE. Please refer to the [LICENSE file](https://huggingface.co/spaces/TencentARC/InstantMesh/blob/main/LICENSE) for details.\n\nüìß **Contact**\n\nIf you have any questions, feel free to open a discussion or contact us at <b>bluestyle928@gmail.com</b>.\n\"\"\"\n\nwith gr.Blocks() as demo:\n    gr.Markdown(_HEADER_)\n    with gr.Row(variant=\"panel\"):\n        with gr.Column():\n            with gr.Row():\n                input_image = gr.Image(\n                    label=\"Input Image\",\n                    image_mode=\"RGBA\",\n                    sources=\"upload\",\n                    width=256,\n                    height=256,\n                    type=\"pil\",\n                    elem_id=\"content_image\",\n                )\n                processed_image = gr.Image(\n                    label=\"Processed Image\", \n                    image_mode=\"RGBA\", \n                    width=256,\n                    height=256,\n                    type=\"pil\", \n                    interactive=False\n                )\n            with gr.Row():\n                with gr.Group():\n                    do_remove_background = gr.Checkbox(\n                        label=\"Remove Background\", value=True\n                    )\n                    sample_seed = gr.Number(value=42, label=\"Seed Value\", precision=0)\n\n                    sample_steps = gr.Slider(\n                        label=\"Sample Steps\",\n                        minimum=30,\n                        maximum=75,\n                        value=75,\n                        step=5\n                    )\n\n            with gr.Row():\n                submit = gr.Button(\"Generate\", elem_id=\"generate\", variant=\"primary\")\n\n            with gr.Row(variant=\"panel\"):\n                gr.Examples(\n                    examples=[\n                        os.path.join(\"examples\", img_name) for img_name in sorted(os.listdir(\"examples\"))\n                    ],\n                    inputs=[input_image],\n                    label=\"Examples\",\n                    examples_per_page=20\n                )\n\n        with gr.Column():\n\n            with gr.Row():\n\n                with gr.Column():\n                    mv_show_images = gr.Image(\n                        label=\"Generated Multi-views\",\n                        type=\"pil\",\n                        width=379,\n                        interactive=False\n                    )\n\n                with gr.Column():\n                    output_video = gr.Video(\n                        label=\"video\", format=\"mp4\",\n                        width=379,\n                        autoplay=True,\n                        interactive=False\n                    )\n\n            with gr.Row():\n                with gr.Tab(\"OBJ\"):\n                    output_model_obj = gr.Model3D(\n                        label=\"Output Model (OBJ Format)\",\n                        #width=768,\n                        interactive=False,\n                    )\n                    gr.Markdown(\"Note: Downloaded .obj model will be flipped. Export .glb instead or manually flip it before usage.\")\n                with gr.Tab(\"GLB\"):\n                    output_model_glb = gr.Model3D(\n                        label=\"Output Model (GLB Format)\",\n                        #width=768,\n                        interactive=False,\n                    )\n                    gr.Markdown(\"Note: The model shown here has a darker appearance. Download to get correct results.\")\n\n            with gr.Row():\n                gr.Markdown('''Try a different <b>seed value</b> if the result is unsatisfying (Default: 42).''')\n\n    gr.Markdown(_CITE_)\n    mv_images = gr.State()\n\n    submit.click(fn=check_input_image, inputs=[input_image]).success(\n        fn=preprocess,\n        inputs=[input_image, do_remove_background],\n        outputs=[processed_image],\n    ).success(\n        fn=generate_mvs,\n        inputs=[processed_image, sample_steps, sample_seed],\n        outputs=[mv_images, mv_show_images],\n    ).success(\n        fn=make3d,\n        inputs=[mv_images],\n        outputs=[output_video, output_model_obj, output_model_glb]\n    )\n\ndemo.queue(max_size=10)\ndemo.launch(server_name=\"0.0.0.0\", server_port=43839)\n"
        },
        {
          "name": "assets",
          "type": "tree",
          "content": null
        },
        {
          "name": "configs",
          "type": "tree",
          "content": null
        },
        {
          "name": "docker",
          "type": "tree",
          "content": null
        },
        {
          "name": "examples",
          "type": "tree",
          "content": null
        },
        {
          "name": "requirements.txt",
          "type": "blob",
          "size": 0.2607421875,
          "content": "pytorch-lightning==2.1.2\ngradio==3.41.2\nhuggingface-hub\neinops\nomegaconf\ntorchmetrics\nwebdataset\naccelerate\ntensorboard\nPyMCubes\ntrimesh\nrembg\ntransformers==4.34.1\ndiffusers==0.20.2\nbitsandbytes\nimageio[ffmpeg]\nxatlas\nplyfile\ngit+https://github.com/NVlabs/nvdiffrast/"
        },
        {
          "name": "run.py",
          "type": "blob",
          "size": 9.69921875,
          "content": "import os\nimport argparse\nimport numpy as np\nimport torch\nimport rembg\nfrom PIL import Image\nfrom torchvision.transforms import v2\nfrom pytorch_lightning import seed_everything\nfrom omegaconf import OmegaConf\nfrom einops import rearrange, repeat\nfrom tqdm import tqdm\nfrom huggingface_hub import hf_hub_download\nfrom diffusers import DiffusionPipeline, EulerAncestralDiscreteScheduler\n\nfrom src.utils.train_util import instantiate_from_config\nfrom src.utils.camera_util import (\n    FOV_to_intrinsics, \n    get_zero123plus_input_cameras,\n    get_circular_camera_poses,\n)\nfrom src.utils.mesh_util import save_obj, save_obj_with_mtl\nfrom src.utils.infer_util import remove_background, resize_foreground, save_video\n\n\ndef get_render_cameras(batch_size=1, M=120, radius=4.0, elevation=20.0, is_flexicubes=False):\n    \"\"\"\n    Get the rendering camera parameters.\n    \"\"\"\n    c2ws = get_circular_camera_poses(M=M, radius=radius, elevation=elevation)\n    if is_flexicubes:\n        cameras = torch.linalg.inv(c2ws)\n        cameras = cameras.unsqueeze(0).repeat(batch_size, 1, 1, 1)\n    else:\n        extrinsics = c2ws.flatten(-2)\n        intrinsics = FOV_to_intrinsics(30.0).unsqueeze(0).repeat(M, 1, 1).float().flatten(-2)\n        cameras = torch.cat([extrinsics, intrinsics], dim=-1)\n        cameras = cameras.unsqueeze(0).repeat(batch_size, 1, 1)\n    return cameras\n\n\ndef render_frames(model, planes, render_cameras, render_size=512, chunk_size=1, is_flexicubes=False):\n    \"\"\"\n    Render frames from triplanes.\n    \"\"\"\n    frames = []\n    for i in tqdm(range(0, render_cameras.shape[1], chunk_size)):\n        if is_flexicubes:\n            frame = model.forward_geometry(\n                planes,\n                render_cameras[:, i:i+chunk_size],\n                render_size=render_size,\n            )['img']\n        else:\n            frame = model.forward_synthesizer(\n                planes,\n                render_cameras[:, i:i+chunk_size],\n                render_size=render_size,\n            )['images_rgb']\n        frames.append(frame)\n    \n    frames = torch.cat(frames, dim=1)[0]    # we suppose batch size is always 1\n    return frames\n\n\n###############################################################################\n# Arguments.\n###############################################################################\n\nparser = argparse.ArgumentParser()\nparser.add_argument('config', type=str, help='Path to config file.')\nparser.add_argument('input_path', type=str, help='Path to input image or directory.')\nparser.add_argument('--output_path', type=str, default='outputs/', help='Output directory.')\nparser.add_argument('--diffusion_steps', type=int, default=75, help='Denoising Sampling steps.')\nparser.add_argument('--seed', type=int, default=42, help='Random seed for sampling.')\nparser.add_argument('--scale', type=float, default=1.0, help='Scale of generated object.')\nparser.add_argument('--distance', type=float, default=4.5, help='Render distance.')\nparser.add_argument('--view', type=int, default=6, choices=[4, 6], help='Number of input views.')\nparser.add_argument('--no_rembg', action='store_true', help='Do not remove input background.')\nparser.add_argument('--export_texmap', action='store_true', help='Export a mesh with texture map.')\nparser.add_argument('--save_video', action='store_true', help='Save a circular-view video.')\nargs = parser.parse_args()\nseed_everything(args.seed)\n\n###############################################################################\n# Stage 0: Configuration.\n###############################################################################\n\nconfig = OmegaConf.load(args.config)\nconfig_name = os.path.basename(args.config).replace('.yaml', '')\nmodel_config = config.model_config\ninfer_config = config.infer_config\n\nIS_FLEXICUBES = True if config_name.startswith('instant-mesh') else False\n\ndevice = torch.device('cuda')\n\n# load diffusion model\nprint('Loading diffusion model ...')\npipeline = DiffusionPipeline.from_pretrained(\n    \"sudo-ai/zero123plus-v1.2\", \n    custom_pipeline=\"zero123plus\",\n    torch_dtype=torch.float16,\n)\npipeline.scheduler = EulerAncestralDiscreteScheduler.from_config(\n    pipeline.scheduler.config, timestep_spacing='trailing'\n)\n\n# load custom white-background UNet\nprint('Loading custom white-background unet ...')\nif os.path.exists(infer_config.unet_path):\n    unet_ckpt_path = infer_config.unet_path\nelse:\n    unet_ckpt_path = hf_hub_download(repo_id=\"TencentARC/InstantMesh\", filename=\"diffusion_pytorch_model.bin\", repo_type=\"model\")\nstate_dict = torch.load(unet_ckpt_path, map_location='cpu')\npipeline.unet.load_state_dict(state_dict, strict=True)\n\npipeline = pipeline.to(device)\n\n# load reconstruction model\nprint('Loading reconstruction model ...')\nmodel = instantiate_from_config(model_config)\nif os.path.exists(infer_config.model_path):\n    model_ckpt_path = infer_config.model_path\nelse:\n    model_ckpt_path = hf_hub_download(repo_id=\"TencentARC/InstantMesh\", filename=f\"{config_name.replace('-', '_')}.ckpt\", repo_type=\"model\")\nstate_dict = torch.load(model_ckpt_path, map_location='cpu')['state_dict']\nstate_dict = {k[14:]: v for k, v in state_dict.items() if k.startswith('lrm_generator.')}\nmodel.load_state_dict(state_dict, strict=True)\n\nmodel = model.to(device)\nif IS_FLEXICUBES:\n    model.init_flexicubes_geometry(device, fovy=30.0)\nmodel = model.eval()\n\n# make output directories\nimage_path = os.path.join(args.output_path, config_name, 'images')\nmesh_path = os.path.join(args.output_path, config_name, 'meshes')\nvideo_path = os.path.join(args.output_path, config_name, 'videos')\nos.makedirs(image_path, exist_ok=True)\nos.makedirs(mesh_path, exist_ok=True)\nos.makedirs(video_path, exist_ok=True)\n\n# process input files\nif os.path.isdir(args.input_path):\n    input_files = [\n        os.path.join(args.input_path, file) \n        for file in os.listdir(args.input_path) \n        if file.endswith('.png') or file.endswith('.jpg') or file.endswith('.webp')\n    ]\nelse:\n    input_files = [args.input_path]\nprint(f'Total number of input images: {len(input_files)}')\n\n\n###############################################################################\n# Stage 1: Multiview generation.\n###############################################################################\n\nrembg_session = None if args.no_rembg else rembg.new_session()\n\noutputs = []\nfor idx, image_file in enumerate(input_files):\n    name = os.path.basename(image_file).split('.')[0]\n    print(f'[{idx+1}/{len(input_files)}] Imagining {name} ...')\n\n    # remove background optionally\n    input_image = Image.open(image_file)\n    if not args.no_rembg:\n        input_image = remove_background(input_image, rembg_session)\n        input_image = resize_foreground(input_image, 0.85)\n    \n    # sampling\n    output_image = pipeline(\n        input_image, \n        num_inference_steps=args.diffusion_steps, \n    ).images[0]\n\n    output_image.save(os.path.join(image_path, f'{name}.png'))\n    print(f\"Image saved to {os.path.join(image_path, f'{name}.png')}\")\n\n    images = np.asarray(output_image, dtype=np.float32) / 255.0\n    images = torch.from_numpy(images).permute(2, 0, 1).contiguous().float()     # (3, 960, 640)\n    images = rearrange(images, 'c (n h) (m w) -> (n m) c h w', n=3, m=2)        # (6, 3, 320, 320)\n\n    outputs.append({'name': name, 'images': images})\n\n# delete pipeline to save memory\ndel pipeline\n\n###############################################################################\n# Stage 2: Reconstruction.\n###############################################################################\n\ninput_cameras = get_zero123plus_input_cameras(batch_size=1, radius=4.0*args.scale).to(device)\nchunk_size = 20 if IS_FLEXICUBES else 1\n\nfor idx, sample in enumerate(outputs):\n    name = sample['name']\n    print(f'[{idx+1}/{len(outputs)}] Creating {name} ...')\n\n    images = sample['images'].unsqueeze(0).to(device)\n    images = v2.functional.resize(images, 320, interpolation=3, antialias=True).clamp(0, 1)\n\n    if args.view == 4:\n        indices = torch.tensor([0, 2, 4, 5]).long().to(device)\n        images = images[:, indices]\n        input_cameras = input_cameras[:, indices]\n\n    with torch.no_grad():\n        # get triplane\n        planes = model.forward_planes(images, input_cameras)\n\n        # get mesh\n        mesh_path_idx = os.path.join(mesh_path, f'{name}.obj')\n\n        mesh_out = model.extract_mesh(\n            planes,\n            use_texture_map=args.export_texmap,\n            **infer_config,\n        )\n        if args.export_texmap:\n            vertices, faces, uvs, mesh_tex_idx, tex_map = mesh_out\n            save_obj_with_mtl(\n                vertices.data.cpu().numpy(),\n                uvs.data.cpu().numpy(),\n                faces.data.cpu().numpy(),\n                mesh_tex_idx.data.cpu().numpy(),\n                tex_map.permute(1, 2, 0).data.cpu().numpy(),\n                mesh_path_idx,\n            )\n        else:\n            vertices, faces, vertex_colors = mesh_out\n            save_obj(vertices, faces, vertex_colors, mesh_path_idx)\n        print(f\"Mesh saved to {mesh_path_idx}\")\n\n        # get video\n        if args.save_video:\n            video_path_idx = os.path.join(video_path, f'{name}.mp4')\n            render_size = infer_config.render_resolution\n            render_cameras = get_render_cameras(\n                batch_size=1, \n                M=120, \n                radius=args.distance, \n                elevation=20.0,\n                is_flexicubes=IS_FLEXICUBES,\n            ).to(device)\n            \n            frames = render_frames(\n                model, \n                planes, \n                render_cameras=render_cameras, \n                render_size=render_size, \n                chunk_size=chunk_size, \n                is_flexicubes=IS_FLEXICUBES,\n            )\n\n            save_video(\n                frames,\n                video_path_idx,\n                fps=30,\n            )\n            print(f\"Video saved to {video_path_idx}\")\n"
        },
        {
          "name": "src",
          "type": "tree",
          "content": null
        },
        {
          "name": "train.py",
          "type": "blob",
          "size": 8.484375,
          "content": "import os, sys\nimport argparse\nimport shutil\nimport subprocess\nfrom omegaconf import OmegaConf\n\nfrom pytorch_lightning import seed_everything\nfrom pytorch_lightning.trainer import Trainer\nfrom pytorch_lightning.strategies import DDPStrategy\nfrom pytorch_lightning.callbacks import Callback\nfrom pytorch_lightning.utilities import rank_zero_only, rank_zero_warn\n\nfrom src.utils.train_util import instantiate_from_config\n\n\n@rank_zero_only\ndef rank_zero_print(*args):\n    print(*args)\n\n\ndef get_parser(**parser_kwargs):\n    def str2bool(v):\n        if isinstance(v, bool):\n            return v\n        if v.lower() in (\"yes\", \"true\", \"t\", \"y\", \"1\"):\n            return True\n        elif v.lower() in (\"no\", \"false\", \"f\", \"n\", \"0\"):\n            return False\n        else:\n            raise argparse.ArgumentTypeError(\"Boolean value expected.\")\n\n    parser = argparse.ArgumentParser(**parser_kwargs)\n    parser.add_argument(\n        \"-r\",\n        \"--resume\",\n        type=str,\n        default=None,\n        help=\"resume from checkpoint\",\n    )\n    parser.add_argument(\n        \"--resume_weights_only\",\n        action=\"store_true\",\n        help=\"only resume model weights\",\n    )\n    parser.add_argument(\n        \"-b\",\n        \"--base\",\n        type=str,\n        default=\"base_config.yaml\",\n        help=\"path to base configs\",\n    )\n    parser.add_argument(\n        \"-n\",\n        \"--name\",\n        type=str,\n        default=\"\",\n        help=\"experiment name\",\n    )\n    parser.add_argument(\n        \"--num_nodes\",\n        type=int,\n        default=1,\n        help=\"number of nodes to use\",\n    )\n    parser.add_argument(\n        \"--gpus\",\n        type=str,\n        default=\"0,\",\n        help=\"gpu ids to use\",\n    )\n    parser.add_argument(\n        \"-s\",\n        \"--seed\",\n        type=int,\n        default=42,\n        help=\"seed for seed_everything\",\n    )\n    parser.add_argument(\n        \"-l\",\n        \"--logdir\",\n        type=str,\n        default=\"logs\",\n        help=\"directory for logging data\",\n    )\n    return parser\n\n\nclass SetupCallback(Callback):\n    def __init__(self, resume, logdir, ckptdir, cfgdir, config):\n        super().__init__()\n        self.resume = resume\n        self.logdir = logdir\n        self.ckptdir = ckptdir\n        self.cfgdir = cfgdir\n        self.config = config\n\n    def on_fit_start(self, trainer, pl_module):\n        if trainer.global_rank == 0:\n            # Create logdirs and save configs\n            os.makedirs(self.logdir, exist_ok=True)\n            os.makedirs(self.ckptdir, exist_ok=True)\n            os.makedirs(self.cfgdir, exist_ok=True)\n\n            rank_zero_print(\"Project config\")\n            rank_zero_print(OmegaConf.to_yaml(self.config))\n            OmegaConf.save(self.config,\n                           os.path.join(self.cfgdir, \"project.yaml\"))\n\n\nclass CodeSnapshot(Callback):\n    \"\"\"\n    Modified from https://github.com/threestudio-project/threestudio/blob/main/threestudio/utils/callbacks.py#L60\n    \"\"\"\n    def __init__(self, savedir):\n        self.savedir = savedir\n\n    def get_file_list(self):\n        return [\n            b.decode()\n            for b in set(\n                subprocess.check_output(\n                    'git ls-files -- \":!:configs/*\"', shell=True\n                ).splitlines()\n            )\n            | set(  # hard code, TODO: use config to exclude folders or files\n                subprocess.check_output(\n                    \"git ls-files --others --exclude-standard\", shell=True\n                ).splitlines()\n            )\n        ]\n\n    @rank_zero_only\n    def save_code_snapshot(self):\n        os.makedirs(self.savedir, exist_ok=True)\n        for f in self.get_file_list():\n            if not os.path.exists(f) or os.path.isdir(f):\n                continue\n            os.makedirs(os.path.join(self.savedir, os.path.dirname(f)), exist_ok=True)\n            shutil.copyfile(f, os.path.join(self.savedir, f))\n\n    def on_fit_start(self, trainer, pl_module):\n        try:\n            self.save_code_snapshot()\n        except:\n            rank_zero_warn(\n                \"Code snapshot is not saved. Please make sure you have git installed and are in a git repository.\"\n            )\n\n\nif __name__ == \"__main__\":\n    # add cwd for convenience and to make classes in this file available when\n    # running as `python main.py`\n    sys.path.append(os.getcwd())\n\n    parser = get_parser()\n    opt, unknown = parser.parse_known_args()\n\n    cfg_fname = os.path.split(opt.base)[-1]\n    cfg_name = os.path.splitext(cfg_fname)[0]\n    exp_name = \"-\" + opt.name if opt.name != \"\" else \"\"\n    logdir = os.path.join(opt.logdir, cfg_name+exp_name)\n\n    ckptdir = os.path.join(logdir, \"checkpoints\")\n    cfgdir = os.path.join(logdir, \"configs\")\n    codedir = os.path.join(logdir, \"code\")\n    seed_everything(opt.seed)\n\n    # init configs\n    config = OmegaConf.load(opt.base)\n    lightning_config = config.lightning\n    trainer_config = lightning_config.trainer\n    \n    trainer_config[\"accelerator\"] = \"gpu\"\n    rank_zero_print(f\"Running on GPUs {opt.gpus}\")\n    ngpu = len(opt.gpus.strip(\",\").split(','))\n    trainer_config['devices'] = ngpu\n\n    trainer_opt = argparse.Namespace(**trainer_config)\n    lightning_config.trainer = trainer_config\n\n    # model\n    model = instantiate_from_config(config.model)\n    if opt.resume and opt.resume_weights_only:\n        model = model.__class__.load_from_checkpoint(opt.resume, **config.model.params)\n    \n    model.logdir = logdir\n\n    # trainer and callbacks\n    trainer_kwargs = dict()\n\n    # logger\n    default_logger_cfg = {\n        \"target\": \"pytorch_lightning.loggers.TensorBoardLogger\",\n        \"params\": {\n            \"name\": \"tensorboard\",\n            \"save_dir\": logdir, \n            \"version\": \"0\",\n        }\n    }\n    logger_cfg = OmegaConf.merge(default_logger_cfg)\n    trainer_kwargs[\"logger\"] = instantiate_from_config(logger_cfg)\n\n    # model checkpoint\n    default_modelckpt_cfg = {\n        \"target\": \"pytorch_lightning.callbacks.ModelCheckpoint\",\n        \"params\": {\n            \"dirpath\": ckptdir,\n            \"filename\": \"{step:08}\",\n            \"verbose\": True,\n            \"save_last\": True,\n            \"every_n_train_steps\": 5000,\n            \"save_top_k\": -1,   # save all checkpoints\n        }\n    }\n\n    if \"modelcheckpoint\" in lightning_config:\n        modelckpt_cfg = lightning_config.modelcheckpoint\n    else:\n        modelckpt_cfg = OmegaConf.create()\n    modelckpt_cfg = OmegaConf.merge(default_modelckpt_cfg, modelckpt_cfg)\n\n    # callbacks\n    default_callbacks_cfg = {\n        \"setup_callback\": {\n            \"target\": \"train.SetupCallback\",\n            \"params\": {\n                \"resume\": opt.resume,\n                \"logdir\": logdir,\n                \"ckptdir\": ckptdir,\n                \"cfgdir\": cfgdir,\n                \"config\": config,\n            }\n        },\n        \"learning_rate_logger\": {\n            \"target\": \"pytorch_lightning.callbacks.LearningRateMonitor\",\n            \"params\": {\n                \"logging_interval\": \"step\",\n            }\n        },\n        \"code_snapshot\": {\n            \"target\": \"train.CodeSnapshot\",\n            \"params\": {\n                \"savedir\": codedir,\n            }\n        },\n    }\n    default_callbacks_cfg[\"checkpoint_callback\"] = modelckpt_cfg\n\n    if \"callbacks\" in lightning_config:\n        callbacks_cfg = lightning_config.callbacks\n    else:\n        callbacks_cfg = OmegaConf.create()\n    callbacks_cfg = OmegaConf.merge(default_callbacks_cfg, callbacks_cfg)\n\n    trainer_kwargs[\"callbacks\"] = [\n        instantiate_from_config(callbacks_cfg[k]) for k in callbacks_cfg]\n    \n    trainer_kwargs['precision'] = '32-true'\n    trainer_kwargs[\"strategy\"] = DDPStrategy(find_unused_parameters=True)\n\n    # trainer\n    trainer = Trainer(**trainer_config, **trainer_kwargs, num_nodes=opt.num_nodes)\n    trainer.logdir = logdir\n\n    # data\n    data = instantiate_from_config(config.data)\n    data.prepare_data()\n    data.setup(\"fit\")\n\n    # configure learning rate\n    base_lr = config.model.base_learning_rate\n    if 'accumulate_grad_batches' in lightning_config.trainer:\n        accumulate_grad_batches = lightning_config.trainer.accumulate_grad_batches\n    else:\n        accumulate_grad_batches = 1\n    rank_zero_print(f\"accumulate_grad_batches = {accumulate_grad_batches}\")\n    lightning_config.trainer.accumulate_grad_batches = accumulate_grad_batches\n    model.learning_rate = base_lr\n    rank_zero_print(\"++++ NOT USING LR SCALING ++++\")\n    rank_zero_print(f\"Setting learning rate to {model.learning_rate:.2e}\")\n\n    # run training loop\n    if opt.resume and not opt.resume_weights_only:\n        trainer.fit(model, data, ckpt_path=opt.resume)\n    else:\n        trainer.fit(model, data)\n"
        },
        {
          "name": "zero123plus",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}