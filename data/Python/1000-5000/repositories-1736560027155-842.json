{
  "metadata": {
    "timestamp": 1736560027155,
    "page": 842,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjg1MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "danielgross/localpilot",
      "stars": 3368,
      "defaultBranch": "main",
      "files": [
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 3.005859375,
          "content": "# Byte-compiled / optimized / DLL files\n__pycache__/\n*.py[cod]\n*$py.class\n\n# C extensions\n*.so\n\n# Distribution / packaging\n.Python\nbuild/\ndevelop-eggs/\ndist/\ndownloads/\neggs/\n.eggs/\nlib/\nlib64/\nparts/\nsdist/\nvar/\nwheels/\nshare/python-wheels/\n*.egg-info/\n.installed.cfg\n*.egg\nMANIFEST\n\n# PyInstaller\n#  Usually these files are written by a python script from a template\n#  before PyInstaller builds the exe, so as to inject date/other infos into it.\n*.manifest\n*.spec\n\n# Installer logs\npip-log.txt\npip-delete-this-directory.txt\n\n# Unit test / coverage reports\nhtmlcov/\n.tox/\n.nox/\n.coverage\n.coverage.*\n.cache\nnosetests.xml\ncoverage.xml\n*.cover\n*.py,cover\n.hypothesis/\n.pytest_cache/\ncover/\n\n# Translations\n*.mo\n*.pot\n\n# Django stuff:\n*.log\nlocal_settings.py\ndb.sqlite3\ndb.sqlite3-journal\n\n# Flask stuff:\ninstance/\n.webassets-cache\n\n# Scrapy stuff:\n.scrapy\n\n# Sphinx documentation\ndocs/_build/\n\n# PyBuilder\n.pybuilder/\ntarget/\n\n# Jupyter Notebook\n.ipynb_checkpoints\n\n# IPython\nprofile_default/\nipython_config.py\n\n# pyenv\n#   For a library or package, you might want to ignore these files since the code is\n#   intended to run in multiple environments; otherwise, check them in:\n# .python-version\n\n# pipenv\n#   According to pypa/pipenv#598, it is recommended to include Pipfile.lock in version control.\n#   However, in case of collaboration, if having platform-specific dependencies or dependencies\n#   having no cross-platform support, pipenv may install dependencies that don't work, or not\n#   install all needed dependencies.\n#Pipfile.lock\n\n# poetry\n#   Similar to Pipfile.lock, it is generally recommended to include poetry.lock in version control.\n#   This is especially recommended for binary packages to ensure reproducibility, and is more\n#   commonly ignored for libraries.\n#   https://python-poetry.org/docs/basic-usage/#commit-your-poetrylock-file-to-version-control\n#poetry.lock\n\n# pdm\n#   Similar to Pipfile.lock, it is generally recommended to include pdm.lock in version control.\n#pdm.lock\n#   pdm stores project-wide configurations in .pdm.toml, but it is recommended to not include it\n#   in version control.\n#   https://pdm.fming.dev/#use-with-ide\n.pdm.toml\n\n# PEP 582; used by e.g. github.com/David-OConnor/pyflow and github.com/pdm-project/pdm\n__pypackages__/\n\n# Celery stuff\ncelerybeat-schedule\ncelerybeat.pid\n\n# SageMath parsed files\n*.sage.py\n\n# Environments\n.env\n.venv\nenv/\nvenv/\nENV/\nenv.bak/\nvenv.bak/\n\n# Spyder project settings\n.spyderproject\n.spyproject\n\n# Rope project settings\n.ropeproject\n\n# mkdocs documentation\n/site\n\n# mypy\n.mypy_cache/\n.dmypy.json\ndmypy.json\n\n# Pyre type checker\n.pyre/\n\n# pytype static type analyzer\n.pytype/\n\n# Cython debug symbols\ncython_debug/\n\n# PyCharm\n#  JetBrains specific template is maintained in a separate JetBrains.gitignore that can\n#  be found at https://github.com/github/gitignore/blob/main/Global/JetBrains.gitignore\n#  and can be added to the global gitignore or merged into this file.  For a more nuclear\n#  option (not recommended) you can uncomment the following to ignore the entire idea folder.\n#.idea/\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 1.04296875,
          "content": "MIT License\n\nCopyright (c) 2023 Daniel Gross\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE."
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 2.056640625,
          "content": "# localpilot\n_Use GitHub Copilot locally on your Macbook with one-click!_\n\n![image](https://github.com/danielgross/localpilot/assets/279531/521d0613-7423-4839-a5e8-42098cd65a5e)\n\n## Demo Video\n\n\nhttps://github.com/danielgross/localpilot/assets/279531/3259981b-39f7-4bfa-8a45-84bde6d4ba4c\n\n\n\n_This video is not sped up or slowed down._\n\n## Installation \n1. First, open VS Code Settings and add the following to your settings.json file: \n```json\n\"github.copilot.advanced\": {\n    \"debug.testOverrideProxyUrl\": \"http://localhost:5001\",\n    \"debug.overrideProxyUrl\": \"http://localhost:5001\"\n}\n```\n\n2. Create a virtualenv to run this Python process, install the requirements, and download the models. \n```python\nvirtualenv venv\nsource venv/bin/activate\npip install -r requirements.txt\n# First setup run. This will download several models to your ~/models folder.\npython app.py --setup \n``` \n\n3. Run it! \n```python\npython app.py\n```\n\nEnjoy your on-device Copilot! \n\n## Caveat FAQ\n\n**Is the code as good as GitHub Copilot?** \n\nFor simple line completions yes. For simple function completions, mostly. For complex functions... maybe. \n\n**Is it as fast as GitHub Copilot?**\n\nOn my Macbook Pro with an Apple M2 Max, the 7b models are roughly as fast. The 34b models are not. Please consider this repo a demonstration of a very inefficient implementation. I'm sure we can make it faster; please do submit a pull request if you'd like to help. For example, I think we need debouncer because sometimes llama.cpp/GGML isn't fast at interrupting itself when a newer request comes in.\n\n**Can this be packaged as a simple Mac app?**\n\nYes!, I'm sure it can be, I just haven't had the time. Please do submit a pull request if you're into that sort of thing!\n\n**Should there be a meta-model that routes to a 1b for autocomplete, 7b for more complex autocomplete, and a 34b for program completion?**\n\nHmm, that seems like an interesting idea.\n\n**OK, but in summary, is it good?** \n\nOnly if your network is bad. I don't think it's competitive if you have fast Internet. But it sure is awesome on airplanes and while tethering!\n\n\n"
        },
        {
          "name": "app.py",
          "type": "blob",
          "size": 2.9033203125,
          "content": "import rumps\nimport requests\nimport threading\nimport subprocess\nimport os\nimport sys\n\nimport config\n\n\ndef setup():\n    if not os.path.exists(config.model_folder):\n        if input(f\"Model folder {config.model_folder} does not exist. Create it? (y/n) \").lower() == 'y':\n            os.mkdir(config.model_folder)\n    current = os.listdir(config.model_folder)\n    for model in config.models:\n        if model == 'default':\n            continue\n        if config.models[model]['type'] == 'local':\n            if config.models[model]['filename'] not in current:\n                if input(f'Model {model} not found in {config.model_folder}. Would you like to download it? (y/n) ').lower() == 'y':\n                    url = config.models[model]['url']\n                    print(f\"Downloading {model} from {url}...\")\n                    subprocess.run(['curl', '-L', url, '-o', os.path.join(\n                        config.model_folder, config.models[model]['filename'])])\n            else:\n                print(f\"Model {model} found in {config.model_folder}.\")\n\n\nclass ModelPickerApp(rumps.App):\n    def __init__(self):\n        super(ModelPickerApp, self).__init__(\"ModelPickerApp\")\n\n        # Dynamically create menu items from the MENUBAR_OPTIONS\n        self.menu_items = {}\n        for option in config.models:\n            if option == 'default':\n                continue\n            self.menu_items[option] = rumps.MenuItem(\n                title=option, callback=self.pick_model)\n\n        self.menu = list(self.menu_items.values())\n        self.menu_items[config.models['default']].state = True\n        self.icon = \"icon.png\"\n\n    def pick_model(self, sender):\n        # Toggle the checked status of the clicked menu item\n        sender.state = not sender.state\n\n        # Send the choice to the local proxy app\n        if sender.state:\n            choice = sender.title\n            try:\n                response = requests.post(\n                    \"http://localhost:5001/set_target\", json={\"target\": choice})\n                if response.status_code == 200:\n                    print(f\"Successfully sent selection: {choice}.\")\n                else:\n                    rumps.alert(\n                        \"Error\", f\"Failed to send selection. Server responded with: {response.status_code}.\")\n            except requests.RequestException as e:\n                rumps.alert(\"Error\", f\"Failed to send selection. Error: {e}.\")\n\n        # If other options were previously selected, deselect them\n        for item in self.menu:\n            if item == 'Quit':\n                continue\n            if item != sender.title:\n                self.menu_items[item].state = False\n\n    def run_server(self):\n        subprocess.run(['python', 'proxy.py'])\n\n\nif __name__ == '__main__':\n    if '--setup' in sys.argv:\n        setup()\n    app = ModelPickerApp()\n    print(\"Running server...\")\n    server_thread = threading.Thread(target=app.run_server)\n    server_thread.start()\n    app.run()\n"
        },
        {
          "name": "config.py",
          "type": "blob",
          "size": 0.8779296875,
          "content": "import os\n\nmodels = {\n    'GitHub': {\n        'domain': 'https://copilot-proxy.githubusercontent.com',\n        'type': 'remote',\n    },\n    'CodeLlama-7b': {\n        'url': 'https://huggingface.co/TheBloke/CodeLlama-7B-GGUF/resolve/main/codellama-7b.Q5_K_S.gguf',\n        'type': 'local',\n        'filename': 'codellama-7b.Q5_K_S.gguf',\n    },\n    'Mistral-7b': {\n        'url': 'https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GGUF/resolve/main/mistral-7b-instruct-v0.1.Q5_K_M.gguf',\n        'type': 'local',\n        'filename': 'mistral-7b-instruct-v0.1.Q5_K_M.gguf',\n    },\n    'CodeLlama-34b': {\n        'url': 'https://huggingface.co/TheBloke/CodeLlama-34B-Instruct-GGUF/resolve/main/codellama-34b-instruct.Q4_K_M.gguf',\n        'type': 'local',\n        'filename': 'codellama-34b-instruct.Q4_K_M.gguf',\n    },\n    'default': 'GitHub',\n}\n\nmodel_folder = os.path.expanduser('~/models')\n"
        },
        {
          "name": "icon.png",
          "type": "blob",
          "size": 2.8779296875,
          "content": null
        },
        {
          "name": "proxy.py",
          "type": "blob",
          "size": 3.0595703125,
          "content": "import config\nimport httpx\nimport os\nimport subprocess\nimport logging\nfrom starlette import applications, responses, exceptions\nfrom starlette.requests import Request\nimport config\n\napp = applications.Starlette()\nstate = config.models[config.models['default']]\nlocal_server_process = None\nlogging.basicConfig(level=logging.DEBUG)\n\n\ndef start_local_server(model_filename):\n    global local_server_process\n    if local_server_process:\n        local_server_process.terminate()\n        local_server_process.wait()\n    cmd = [\"python3\", \"-m\", \"llama_cpp.server\", \"--model\", model_filename,\n           \"--n_gpu_layers\", \"1\", \"--n_ctx\", \"4096\"]  # TODO: set this more correctly\n    logging.debug('Running: %s' % ' '.join(cmd))\n    local_server_process = subprocess.Popen(\n        cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT)\n\n\n@app.route('/set_target', methods=['POST'])\nasync def set_target(request: Request):\n    global state\n    response = await request.json()\n    target = response['target']\n    if target not in config.models:\n        raise exceptions.HTTPException(\n            status_code=400, detail=f'Invalid target: {target}')\n\n    state = config.models[target]\n    if config.models[target].get(\"type\") == \"local\":\n        start_local_server(os.path.join(\n            config.model_folder, config.models[target]['filename']))\n\n    message = f'Target set to {state}'\n    return responses.JSONResponse({'message': message}, status_code=200)\n\n\n@app.route('/{path:path}', methods=['GET', 'POST', 'PUT', 'DELETE'])\nasync def proxy(request: Request):\n    global state\n    path = request.url.path\n    logging.debug(f'Current state: {state}')\n\n    if state['type'] == 'remote':\n        url = f\"{state['domain']}{path}\"\n    elif state['type'] == 'local':\n        url = f\"http://localhost:8000{path}\"\n\n    data = await request.body()\n    headers = dict(request.headers)\n    r = None\n    async with httpx.AsyncClient() as client:\n        try:\n            if request.method == 'GET':\n                r = await client.get(url, params=request.query_params, headers=headers)\n            elif request.method == 'POST':\n                r = await client.post(url, data=data, headers=headers, timeout=30)\n            elif request.method == 'PUT':\n                r = await client.put(url, data=data, headers=headers)\n            elif request.method == 'DELETE':\n                r = await client.delete(url, headers=headers)\n        except httpx.RemoteProtocolError as exc:\n            logging.debug(f'Connection closed prematurely: {exc}')\n    content = r.content if r else ''\n    status_code = r.status_code if r else 204\n    headers = dict(r.headers) if r else dict()\n    return responses.Response(content=content, status_code=status_code, headers=headers)\n\n\n@app.exception_handler(404)\nasync def not_found(request, exc):\n    return responses.JSONResponse({\"error\": \"Not found\"}, status_code=404)\n\n\n@app.exception_handler(500)\nasync def server_error(request, exc):\n    return responses.JSONResponse({\"error\": \"Server error\"}, status_code=500)\n\nif __name__ == '__main__':\n    import uvicorn\n    uvicorn.run(app, host=\"0.0.0.0\", port=5001)\n"
        },
        {
          "name": "requirements.txt",
          "type": "blob",
          "size": 0.5087890625,
          "content": "annotated-types==0.5.0\nanyio==3.7.1\ncertifi==2023.7.22\ncharset-normalizer==3.3.0\nclick==8.1.7\nexceptiongroup==1.1.3\nfastapi==0.103.2\nh11==0.14.0\nhttpcore==0.18.0\nhttpx==0.25.0\nidna==3.4\nllama_cpp_python==0.2.11\nnumpy==1.26.0\npydantic==2.4.2\npydantic-settings==2.0.3\npydantic_core==2.10.1\npyobjc-core==10.0\npyobjc-framework-Cocoa==10.0\npython-dotenv==1.0.0\nrequests==2.31.0\nrumps==0.4.0\nsniffio==1.3.0\nsse-starlette==1.6.5\nstarlette==0.27.0\nstarlette-context==0.3.6\ntyping_extensions==4.8.0\nurllib3==2.0.5\nuvicorn==0.23.2\n"
        }
      ]
    }
  ]
}