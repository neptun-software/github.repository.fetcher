{
  "metadata": {
    "timestamp": 1736559505089,
    "page": 85,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjkw",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "OFA-Sys/Chinese-CLIP",
      "stars": 4777,
      "defaultBranch": "master",
      "files": [
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 1.9384765625,
          "content": "logs/\nwandb/\nmodels/\nfeatures/\nresults/\n\n# Byte-compiled / optimized / DLL files\n__pycache__/\n*.py[cod]\n*$py.class\n\n# C extensions\n*.so\n\n# Distribution / packaging\n.Python\nbuild/\ndevelop-eggs/\ndist/\ndownloads/\neggs/\n.eggs/\nlib/\nlib64/\nparts/\nsdist/\nvar/\nwheels/\npip-wheel-metadata/\nshare/python-wheels/\n*.egg-info/\n.installed.cfg\n*.egg\nMANIFEST\n\n# PyInstaller\n#  Usually these files are written by a python script from a template\n#  before PyInstaller builds the exe, so as to inject date/other infos into it.\n*.manifest\n*.spec\n\n# Installer logs\npip-log.txt\npip-delete-this-directory.txt\n\n# Unit test / coverage reports\nhtmlcov/\n.tox/\n.nox/\n.coverage\n.coverage.*\n.cache\nnosetests.xml\ncoverage.xml\n*.cover\n*.py,cover\n.hypothesis/\n.pytest_cache/\n\n# Translations\n*.mo\n*.pot\n\n# Django stuff:\n*.log\nlocal_settings.py\ndb.sqlite3\ndb.sqlite3-journal\n\n# Flask stuff:\ninstance/\n.webassets-cache\n\n# Scrapy stuff:\n.scrapy\n\n# Sphinx documentation\ndocs/_build/\n\n# PyBuilder\ntarget/\n\n# Jupyter Notebook\n.ipynb_checkpoints\n\n# IPython\nprofile_default/\nipython_config.py\n\n# pyenv\n.python-version\n\n# pipenv\n#   According to pypa/pipenv#598, it is recommended to include Pipfile.lock in version control.\n#   However, in case of collaboration, if having platform-specific dependencies or dependencies\n#   having no cross-platform support, pipenv may install dependencies that don't work, or not\n#   install all needed dependencies.\n#Pipfile.lock\n\n# PEP 582; used by e.g. github.com/David-OConnor/pyflow\n__pypackages__/\n\n# Celery stuff\ncelerybeat-schedule\ncelerybeat.pid\n\n# SageMath parsed files\n*.sage.py\n\n# Environments\n.env\n.venv\nenv/\nvenv/\nENV/\nenv.bak/\nvenv.bak/\n\n# Spyder project settings\n.spyderproject\n.spyproject\n\n# Rope project settings\n.ropeproject\n\n# mkdocs documentation\n/site\n\n# mypy\n.mypy_cache/\n.dmypy.json\ndmypy.json\n\n# Pyre type checker\n.pyre/\nsync.sh\ngpu1sync.sh\n.idea\n*.pdf\n**/._*\n**/*DS_*\n**.jsonl\nsrc/sbatch\nsrc/misc\n.vscode\nsrc/debug\ncore.*\n\n# Allow\n!src/evaluation/misc/results_dbs/*"
        },
        {
          "name": "MIT-LICENSE.txt",
          "type": "blob",
          "size": 1.234375,
          "content": "Copyright (c) 2012-2022 OFA-Sys Team\n\nCopyright (c) 2012-2022 Gabriel Ilharco, Mitchell Wortsman, Nicholas Carlini, Rohan Taori, Achal Dave, Vaishaal Shankar, John Miller, Hongseok Namkoong, Hannaneh Hajishirzi, Ali Farhadi, Ludwig Schmidt\n\nPermission is hereby granted, free of charge, to any person obtaining\na copy of this software and associated documentation files (the\n\"Software\"), to deal in the Software without restriction, including\nwithout limitation the rights to use, copy, modify, merge, publish,\ndistribute, sublicense, and/or sell copies of the Software, and to\npermit persons to whom the Software is furnished to do so, subject to\nthe following conditions:\n\nThe above copyright notice and this permission notice shall be\nincluded in all copies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND,\nEXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF\nMERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\nNONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE\nLIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION\nOF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION\nWITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 33.9306640625,
          "content": "[**ä¸­æ–‡è¯´æ˜**](README.md) | [**English**](README_En.md)\r\n\r\n<p align=\"center\">\r\n    <br>\r\n    <img src=\"assets/Chinese_CLIP_logo_tp_path.svg\" width=\"400\" />\r\n    <br>\r\n<p>\r\n<br>\r\n\r\n<p align=\"center\">\r\n        <a href=\"https://www.modelscope.cn/models?name=clip&tasks=multi-modal-embedding\">ModelScope</a>&nbsp ï½œ &nbsp<a href=\"https://www.modelscope.cn/studios/damo/chinese_clip_applications/summary\">Demo</a>&nbsp ï½œ &nbsp<a href=\"https://arxiv.org/abs/2211.01335\">Paper</a>&nbsp ï½œ &nbspBlog\r\n</p>\r\n<br><br>\r\n\r\næœ¬é¡¹ç›®ä¸ºCLIPæ¨¡å‹çš„**ä¸­æ–‡**ç‰ˆæœ¬ï¼Œä½¿ç”¨å¤§è§„æ¨¡ä¸­æ–‡æ•°æ®è¿›è¡Œè®­ç»ƒï¼ˆ~2äº¿å›¾æ–‡å¯¹ï¼‰ï¼Œæ—¨åœ¨å¸®åŠ©ç”¨æˆ·å¿«é€Ÿå®ç°ä¸­æ–‡é¢†åŸŸçš„[å›¾æ–‡ç‰¹å¾&ç›¸ä¼¼åº¦è®¡ç®—](#APIå¿«é€Ÿä¸Šæ‰‹)ã€[è·¨æ¨¡æ€æ£€ç´¢](#è·¨æ¨¡æ€æ£€ç´¢)ã€[é›¶æ ·æœ¬å›¾ç‰‡åˆ†ç±»](#é›¶æ ·æœ¬å›¾åƒåˆ†ç±»)ç­‰ä»»åŠ¡ã€‚æœ¬é¡¹ç›®ä»£ç åŸºäº<b>[open_clip project](https://github.com/mlfoundations/open_clip)</b>å»ºè®¾ï¼Œå¹¶é’ˆå¯¹ä¸­æ–‡é¢†åŸŸæ•°æ®ä»¥åŠåœ¨ä¸­æ–‡æ•°æ®ä¸Šå®ç°æ›´å¥½çš„æ•ˆæœåšäº†ä¼˜åŒ–ã€‚æœ¬é¡¹ç›®æä¾›äº†APIã€è®­ç»ƒä»£ç å’Œæµ‹è¯•ä»£ç ï¼Œä¸‹æ–‡ä¸­å°†è¯¦ç»†ä»‹ç»ç»†èŠ‚ã€‚\r\n<br><br>\r\n\r\n# æ–°é—»\r\n* 2023.11.30 Chinese-CLIPæ·»åŠ äº†è½¬æ¢Pytorchæ¨¡å‹ä¸ºcoremlæ ¼å¼çš„[è½¬æ¢è„šæœ¬](https://github.com/OFA-Sys/Chinese-CLIP/blob/master/cn_clip/deploy/pytorch_to_coreml.py)ï¼Œç”¨äºéƒ¨ç½²ã€‚ï¼ˆæ„Ÿè°¢[@manymuch](https://github.com/manymuch)è´¡çŒ®ä»£ç â¤ï¸ï¼‰\r\n* 2023.9.8 Chinese-CLIPæ”¯æŒäº†åŸºäº[ModelScope](https://github.com/modelscope/modelscope)åº“çš„[çŸ¥è¯†è’¸é¦å¾®è°ƒåŠŸèƒ½](distillation.md)ã€‚ï¼ˆæ„Ÿè°¢é˜¿é‡Œäº‘PAIå›¢é˜Ÿ[@wuziheng](https://github.com/wuziheng)å’Œ[@Jaskr616](https://github.com/Jaskr616)åŒå­¦[è´¡çŒ®ä»£ç ](https://github.com/OFA-Sys/Chinese-CLIP/pull/195)â¤ï¸ï¼‰\r\n* 2023.5.9 Chinese-CLIPé€‚é…Pytorch2.0ã€‚\r\n* 2023.3.20 æ–°å¢å¯¹æ¯”å­¦ä¹ çš„[æ¢¯åº¦ç´¯ç§¯](#gradient_accumulation)æ”¯æŒï¼Œå¯æ¨¡æ‹Ÿæ›´å¤§batch sizeçš„è®­ç»ƒæ•ˆæœ\r\n* 2023.2.16 æ–°å¢[FlashAttention](https://github.com/HazyResearch/flash-attention)æ”¯æŒï¼Œæå‡è®­ç»ƒé€Ÿåº¦ï¼Œé™ä½æ˜¾å­˜å ç”¨ï¼Œè¯¦è§[flash_attention.md](flash_attention.md)\r\n* 2023.1.15 æ–°å¢éƒ¨ç½²[ONNX](https://onnx.ai/)å’Œ[TensorRT](https://developer.nvidia.com/tensorrt)æ¨¡å‹æ”¯æŒï¼ˆå¹¶æä¾›é¢„è®­ç»ƒTensorRTæ¨¡å‹ï¼‰ï¼Œæå‡ç‰¹å¾æ¨ç†é€Ÿåº¦ï¼Œæ»¡è¶³éƒ¨ç½²éœ€æ±‚ï¼Œè¯¦è§[deployment.md](deployment.md)\r\n* 2022.12.12 æ–°å¢å®ç°[FLIP](https://arxiv.org/abs/2212.00794)è®­ç»ƒç­–ç•¥ï¼Œåœ¨finetuneè®­ç»ƒæ—¶å¯[æ¿€æ´»ä½¿ç”¨](#FLIP)ï¼ˆæ„Ÿè°¢[@zwkkk](https://github.com/zwkkk)åŒå­¦[è´¡çŒ®ä»£ç ](https://github.com/OFA-Sys/Chinese-CLIP/pull/26)â¤ï¸ï¼‰\r\n* 2022.12.3 å…¬å¼€[ELEVATER](https://eval.ai/web/challenges/challenge-page/1832)å›¾åƒåˆ†ç±»æ•°æ®é›†çš„ä¸­æ–‡ç‰ˆæœ¬ï¼Œè¯¦è§[æ•°æ®æ–‡æ¡£](https://github.com/OFA-Sys/Chinese-CLIP/blob/master/zeroshot_dataset.md)\r\n* 2022.12.1 Chinese-CLIPæ¨¡å‹ä»£ç &ç‰¹å¾æå–APIï¼ŒåŒæ­¥åˆå…¥Huggingface transformersğŸ¤—ä»£ç åº“\r\n* 2022.11.22 æ–°å¢[é›¶æ ·æœ¬å›¾åƒåˆ†ç±»](#é›¶æ ·æœ¬å›¾åƒåˆ†ç±»)ä»£ç ï¼Œå¯æ”¯æŒ[ELEVATER benchmark](https://eval.ai/web/challenges/challenge-page/1832)é›¶æ ·æœ¬åˆ†ç±»è¯„æµ‹ä»»åŠ¡\r\n* 2022.11.3 æ–°å¢RN50ï¼ŒViT-H-14æ¨¡å‹ï¼Œå…¬å¼€[æŠ€æœ¯æŠ¥å‘Š](https://arxiv.org/pdf/2211.01335.pdf)\r\n* 2022.9.22 æ–°å¢ViT-L-14ï¼ŒViT-L-14-336æ¨¡å‹\r\n* 2022.7.13 æ–°å¢[å›¾æ–‡ç‰¹å¾æå–å¿«é€ŸAPI](#APIå¿«é€Ÿä¸Šæ‰‹)ï¼Œå‡ è¡Œä»£ç å¿«é€Ÿè°ƒç”¨ä¸­æ–‡CLIPæ¨¡å‹ï¼Œè®¡ç®—å›¾æ–‡ç‰¹å¾&ç›¸ä¼¼åº¦\r\n* 2022.7.8 Chinese-CLIPé¡¹ç›®æ­£å¼å¼€æºï¼Œå¼€æº[å›¾æ–‡æ£€ç´¢](#è·¨æ¨¡æ€æ£€ç´¢)ä»£ç \r\n<br><br>\r\n\r\n# æ¨¡å‹åŠå®éªŒ\r\n<span id=\"model_card\"></span>\r\n## æ¨¡å‹è§„æ¨¡ & ä¸‹è½½é“¾æ¥\r\nChinese-CLIPç›®å‰å¼€æº5ä¸ªä¸åŒè§„æ¨¡ï¼Œå…¶æ¨¡å‹ä¿¡æ¯å’Œä¸‹è½½æ–¹å¼è§ä¸‹è¡¨ï¼š\r\n\r\n<table border=\"1\" width=\"100%\">\r\n    <tr align=\"center\">\r\n        <th>æ¨¡å‹è§„æ¨¡</th><th>ä¸‹è½½é“¾æ¥</th><th>å‚æ•°é‡</th><th>è§†è§‰ä¾§éª¨æ¶</th><th>è§†è§‰ä¾§å‚æ•°é‡</th><th>æ–‡æœ¬ä¾§éª¨æ¶</th><th>æ–‡æœ¬ä¾§å‚æ•°é‡</th><th>åˆ†è¾¨ç‡</th>\r\n    </tr>\r\n    <tr align=\"center\">\r\n        <td>CN-CLIP<sub>RN50</sub></td><td><a href=\"https://clip-cn-beijing.oss-cn-beijing.aliyuncs.com/checkpoints/clip_cn_rn50.pt\">Download</a></td><td>77M</td><td>ResNet50</td><td>38M</td><td>RBT3</td><td>39M</td><td>224</td>\r\n    </tr>\r\n    <tr align=\"center\">\r\n        <td>CN-CLIP<sub>ViT-B/16</sub></td><td><a href=\"https://clip-cn-beijing.oss-cn-beijing.aliyuncs.com/checkpoints/clip_cn_vit-b-16.pt\">Download</a></td><td>188M</td><td>ViT-B/16</td><td>86M</td><td>RoBERTa-wwm-Base</td><td>102M</td><td>224</td>\r\n    </tr>\r\n    <tr align=\"center\">\r\n        <td>CN-CLIP<sub>ViT-L/14</sub></td><td><a href=\"https://clip-cn-beijing.oss-cn-beijing.aliyuncs.com/checkpoints/clip_cn_vit-l-14.pt\">Download</a></td><td>406M</td><td>ViT-L/14</td><td>304M</td><td>RoBERTa-wwm-Base</td><td>102M</td><td>224</td>\r\n    </tr>\r\n    <tr align=\"center\">\r\n        <td>CN-CLIP<sub>ViT-L/14@336px</sub></td><td><a href=\"https://clip-cn-beijing.oss-cn-beijing.aliyuncs.com/checkpoints/clip_cn_vit-l-14-336.pt\">Download</a></td><td>407M</td><td>ViT-L/14</td><td>304M</td><td>RoBERTa-wwm-Base</td><td>102M</td><td>336</td>\r\n    </tr>\r\n    <tr align=\"center\">\r\n        <td>CN-CLIP<sub>ViT-H/14</sub></td><td><a href=\"https://clip-cn-beijing.oss-cn-beijing.aliyuncs.com/checkpoints/clip_cn_vit-h-14.pt\">Download</a></td><td>958M</td><td>ViT-H/14</td><td>632M</td><td>RoBERTa-wwm-Large</td><td>326M</td><td>224</td>\r\n    </tr>\r\n</table>\r\n<br></br>\r\n\r\n## å®éªŒç»“æœ\r\né’ˆå¯¹å›¾æ–‡æ£€ç´¢ä»»åŠ¡ï¼Œæˆ‘ä»¬åœ¨[MUGE Retrieval](https://tianchi.aliyun.com/muge)ã€[Flickr30K-CN](https://github.com/li-xirong/cross-lingual-cap)å’Œ[COCO-CN](https://github.com/li-xirong/coco-cn)ä¸Šè¿›è¡Œäº†zero-shotå’Œfinetuneçš„å®éªŒã€‚é’ˆå¯¹å›¾åƒé›¶æ ·æœ¬åˆ†ç±»ï¼Œæˆ‘ä»¬åœ¨[ELEVATER](https://eval.ai/web/challenges/challenge-page/1832)çš„10ä¸ªæ•°æ®é›†ä¸Šè¿›è¡Œäº†å®éªŒã€‚å®éªŒç»“æœå¦‚ä¸‹è¡¨æ‰€ç¤ºã€‚ç¯‡å¹…æ‰€é™ï¼Œæˆ‘ä»¬è¿™é‡Œç»™å‡ºbaselineæ¨¡å‹å’ŒChinese-CLIPçš„æœ€ä¼˜è§„æ¨¡æ¨¡å‹ç»“æœï¼Œå…³äºChinese-CLIPå„è§„æ¨¡çš„è¯¦ç»†ç»“æœæŒ‡æ ‡ï¼Œè¯·è¯¦è§[Results.md](Results.md)ã€‚\r\n\r\n**MUGE Text-to-Image Retrieval (Official Validation Set)**:\r\n<table border=\"1\" width=\"100%\">\r\n    <tr align=\"center\">\r\n        <th>Setup</th><th colspan=\"4\">Zero-shot</th><th colspan=\"4\">Finetune</th>\r\n    </tr>\r\n    <tr align=\"center\">\r\n        <td>Metric</td><td>R@1</td><td>R@5</td><td>R@10</td><td>MR</td><td>R@1</td><td>R@5</td><td>R@10</td><td>MR</td>\r\n    </tr>\r\n\t<tr align=\"center\">\r\n        <td width=\"120%\">Wukong</td><td>42.7</td><td>69.0</td><td>78.0</td><td>63.2</td><td>52.7</td><td>77.9</td><td>85.6</td><td>72.1</td>\r\n    </tr>\r\n\t<tr align=\"center\">\r\n        <td width=\"120%\">R2D2</td><td>49.5</td><td>75.7</td><td>83.2</td><td>69.5</td><td>60.1</td><td>82.9</td><td>89.4</td><td>77.5</td>\r\n    </tr>\r\n\t<tr align=\"center\">\r\n        <td width=\"120%\">CN-CLIP</td><td>63.0</td><td>84.1</td><td>89.2</td><td>78.8</td><td>68.9</td><td>88.7</td><td>93.1</td><td>83.6</td>\r\n    </tr>\r\n</table>\r\n<br>\r\n\r\n**Flickr30K-CN Retrieval (Official Test Set)**:\r\n<table border=\"1\" width=\"150%\">\r\n\t<tr align=\"center\">\r\n        <th>Task</th><th colspan=\"6\">Text-to-Image</th><th colspan=\"6\">Image-to-Text</th>\r\n    </tr>\r\n    <tr align=\"center\">\r\n        <th>Setup</th><th colspan=\"3\">Zero-shot</th><th colspan=\"3\">Finetune</th><th colspan=\"3\">Zero-shot</th><th colspan=\"3\">Finetune</th>\r\n    </tr>\r\n    <tr align=\"center\">\r\n        <td>Metric</td><td>R@1</td><td>R@5</td><td>R@10</td><td>R@1</td><td>R@5</td><td>R@10</td><td>R@1</td><td>R@5</td><td>R@10</td><td>R@1</td><td>R@5</td><td>R@10</td>\r\n    </tr>\r\n\t<tr align=\"center\">\r\n        <td width=\"120%\">Wukong</td><td>51.7</td><td>78.9</td><td>86.3</td><td>77.4</td><td>94.5</td><td>97.0</td><td>76.1</td><td>94.8</td><td>97.5</td><td>92.7</td><td>99.1</td><td>99.6</td>\r\n    </tr>\r\n\t<tr align=\"center\">\r\n        <td width=\"120%\">Taiyi</td><td>60.8</td><td>85.0</td><td>91.0</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td>\r\n    </tr>\t\r\n\t<tr align=\"center\">\r\n        <td width=\"120%\">R2D2</td><td>60.9</td><td>86.8</td><td>92.7</td><td>84.4</td><td>96.7</td><td>98.4</td><td>77.6</td><td>96.7</td><td>98.9</td><td>95.6</td><td>99.8</td><td>100.0</td>\r\n    </tr>\t\r\n\t<tr align=\"center\">\r\n        <td width=\"120%\">CN-CLIP</td><td>71.2</td><td>91.4</td><td>95.5</td><td>83.8</td><td>96.9</td><td>98.6</td><td>81.6</td><td>97.5</td><td>98.8</td><td>95.3</td><td>99.7</td><td>100.0</td>\r\n    </tr>\r\n</table>\r\n<br>\r\n\r\n**COCO-CN Retrieval (Official Test Set)**:\r\n<table border=\"1\" width=\"150%\">\r\n\t<tr align=\"center\">\r\n        <th>Task</th><th colspan=\"6\">Text-to-Image</th><th colspan=\"6\">Image-to-Text</th>\r\n    </tr>\r\n    <tr align=\"center\">\r\n        <th>Setup</th><th colspan=\"3\">Zero-shot</th><th colspan=\"3\">Finetune</th><th colspan=\"3\">Zero-shot</th><th colspan=\"3\">Finetune</th>\r\n    </tr>\r\n    <tr align=\"center\">\r\n        <td>Metric</td><td>R@1</td><td>R@5</td><td>R@10</td><td>R@1</td><td>R@5</td><td>R@10</td><td>R@1</td><td>R@5</td><td>R@10</td><td>R@1</td><td>R@5</td><td>R@10</td>\r\n    </tr>\r\n\t<tr align=\"center\">\r\n        <td width=\"150%\">Wukong</td><td>53.4</td><td>80.2</td><td>90.1</td><td>74.0</td><td>94.4</td><td>98.1</td><td>55.2</td><td>81.0</td><td>90.6</td><td>73.3</td><td>94.0</td><td>98.0</td>\r\n    </tr>\r\n\t<tr align=\"center\">\r\n        <td width=\"150%\">Taiyi</td><td>60.0</td><td>84.0</td><td>93.3</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td>\r\n    </tr>\t\r\n\t<tr align=\"center\">\r\n        <td width=\"150%\">R2D2</td><td>56.4</td><td>85.0</td><td>93.1</td><td>79.1</td><td>96.5</td><td>98.9</td><td>63.3</td><td>89.3</td><td>95.7</td><td>79.3</td><td>97.1</td><td>98.7</td>\r\n    </tr>\r\n\t<tr align=\"center\">\r\n        <td width=\"150%\">CN-CLIP</td><td>69.2</td><td>89.9</td><td>96.1</td><td>81.5</td><td>96.9</td><td>99.1</td><td>63.0</td><td>86.6</td><td>92.9</td><td>83.5</td><td>97.3</td><td>99.2</td>\r\n    </tr>\r\n</table>\r\n<br>\r\n\r\n**Zero-shot Image Classification**:\r\n<table border=\"1\" width=\"150%\">\r\n\t<tr align=\"center\">\r\n        <th>Task</th><th>CIFAR10</th><th>CIFAR100</th><th>DTD</th><th>EuroSAT</th><th>FER</th><th>FGVC</th><th>KITTI</th><th>MNIST</th><th>PC</th><th>VOC</th>\r\n    </tr>\r\n\t<tr align=\"center\">\r\n        <td width=\"150%\">GIT</td><td>88.5</td><td>61.1</td><td>42.9</td><td>43.4</td><td>41.4</td><td>6.7</td><td>22.1</td><td>68.9</td><td>50.0</td><td>80.2</td>\r\n    </tr>\r\n    \t<tr align=\"center\">\r\n        <td width=\"150%\">ALIGN</td><td>94.9</td><td>76.8</td><td>66.1</td><td>52.1</td><td>50.8</td><td>25.0</td><td>41.2</td><td>74.0</td><td>55.2</td><td>83.0</td>\r\n    </tr>\r\n\t<tr align=\"center\">\r\n        <td width=\"150%\">CLIP</td><td>94.9</td><td>77.0</td><td>56.0</td><td>63.0</td><td>48.3</td><td>33.3</td><td>11.5</td><td>79.0</td><td>62.3</td><td>84.0</td>\r\n    </tr>\r\n    \t<tr align=\"center\">\r\n        <td width=\"150%\">Wukong</td><td>95.4</td><td>77.1</td><td>40.9</td><td>50.3</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td>\r\n    </tr>\r\n    \t<tr align=\"center\">\r\n        <td width=\"150%\">CN-CLIP</td><td>96.0</td><td>79.7</td><td>51.2</td><td>52.0</td><td>55.1</td><td>26.2</td><td>49.9</td><td>79.4</td><td>63.5</td><td>84.9</td>\r\n    </tr>\r\n</table>\r\n\r\n<br><br>\r\n\r\n\r\n# å¼€å§‹ç”¨èµ·æ¥ï¼\r\n## å®‰è£…è¦æ±‚\r\nå¼€å§‹æœ¬é¡¹ç›®å‰ï¼Œéœ€å…ˆæ£€æŸ¥æ˜¯å¦æ»¡è¶³ä¸‹åˆ—ç¯å¢ƒé…ç½®è¦æ±‚:\r\n\r\n* python >= 3.6.4\r\n* pytorch >= 1.8.0 (with torchvision >= 0.9.0)\r\n* CUDA Version >= 10.2\r\n\r\nè¿è¡Œä¸‹åˆ—å‘½ä»¤å³å¯å®‰è£…æœ¬é¡¹ç›®æ‰€éœ€çš„ä¸‰æ–¹åº“ã€‚\r\n\r\n```bash\r\npip install -r requirements.txt\r\n```\r\n\r\n## APIå¿«é€Ÿä¸Šæ‰‹\r\nä¸‹é¢æä¾›ä¸€æ®µç®€å•çš„ä»£ç ç¤ºä¾‹è¯´æ˜å¦‚ä½•ä½¿ç”¨ä¸­æ–‡CLIPçš„APIã€‚å¼€å§‹ä½¿ç”¨å‰ï¼Œè¯·å…ˆå®‰è£…cn_clipï¼š\r\n```bash\r\n# é€šè¿‡pipå®‰è£…\r\npip install cn_clip\r\n\r\n# æˆ–è€…ä»æºä»£ç å®‰è£…\r\ncd Chinese-CLIP\r\npip install -e .\r\n```\r\nå®‰è£…æˆåŠŸåï¼Œå³å¯é€šè¿‡å¦‚ä¸‹æ–¹å¼è½»æ¾è°ƒç”¨APIï¼Œä¼ å…¥æŒ‡å®šå›¾ç‰‡ï¼ˆ[ç¤ºä¾‹](examples/pokemon.jpeg)ï¼‰å’Œæ–‡æœ¬ï¼Œæå–å›¾æ–‡ç‰¹å¾å‘é‡å¹¶è®¡ç®—ç›¸ä¼¼åº¦ï¼š\r\n```python\r\nimport torch \r\nfrom PIL import Image\r\n\r\nimport cn_clip.clip as clip\r\nfrom cn_clip.clip import load_from_name, available_models\r\nprint(\"Available models:\", available_models())  \r\n# Available models: ['ViT-B-16', 'ViT-L-14', 'ViT-L-14-336', 'ViT-H-14', 'RN50']\r\n\r\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\r\nmodel, preprocess = load_from_name(\"ViT-B-16\", device=device, download_root='./')\r\nmodel.eval()\r\nimage = preprocess(Image.open(\"examples/pokemon.jpeg\")).unsqueeze(0).to(device)\r\ntext = clip.tokenize([\"æ°å°¼é¾Ÿ\", \"å¦™è›™ç§å­\", \"å°ç«é¾™\", \"çš®å¡ä¸˜\"]).to(device)\r\n\r\nwith torch.no_grad():\r\n    image_features = model.encode_image(image)\r\n    text_features = model.encode_text(text)\r\n    # å¯¹ç‰¹å¾è¿›è¡Œå½’ä¸€åŒ–ï¼Œè¯·ä½¿ç”¨å½’ä¸€åŒ–åçš„å›¾æ–‡ç‰¹å¾ç”¨äºä¸‹æ¸¸ä»»åŠ¡\r\n    image_features /= image_features.norm(dim=-1, keepdim=True) \r\n    text_features /= text_features.norm(dim=-1, keepdim=True)    \r\n\r\n    logits_per_image, logits_per_text = model.get_similarity(image, text)\r\n    probs = logits_per_image.softmax(dim=-1).cpu().numpy()\r\n\r\nprint(\"Label probs:\", probs)  # [[1.268734e-03 5.436878e-02 6.795761e-04 9.436829e-01]]\r\n```\r\næˆ‘ä»¬ä¹Ÿå‡†å¤‡äº†éƒ¨ç½²ONNXå’ŒTensorRTæ¨¡å‹çš„ç›¸å…³æ”¯æŒï¼Œæµç¨‹è¯¦è§[deployment.md](deployment.md)ã€‚\r\n\r\nå¦‚æœä½ ä¸æ»¡è¶³äºä»…ä»…ä½¿ç”¨APIï¼Œæ¬¢è¿ç»§ç»­é˜…è¯»æœ¬æ–‡æ¡£ï¼Œäº†è§£å¦‚ä½•ä½¿ç”¨æˆ‘ä»¬çš„é¡¹ç›®è¿›è¡ŒCLIPæ¨¡å‹çš„è®­ç»ƒå’Œæµ‹è¯•ã€‚\r\n<br><br>\r\n\r\n\r\n# æ•™ç¨‹\r\nä¸‹æ–‡å°†åŒ…æ‹¬[è·¨æ¨¡æ€æ£€ç´¢æ•™ç¨‹](#è·¨æ¨¡æ€æ£€ç´¢)ï¼ˆåŒ…å«finetuneå’Œinferenceï¼ŒåŠKNNè®¡ç®—ç­‰ï¼‰ä»¥åŠ[é›¶æ ·æœ¬å›¾åƒåˆ†ç±»æ•™ç¨‹](#é›¶æ ·æœ¬å›¾åƒåˆ†ç±»)ã€‚\r\n\r\n## è·¨æ¨¡æ€æ£€ç´¢\r\n### ä»£ç ç»„ç»‡\r\nä¸‹è½½æœ¬é¡¹ç›®å, è¯·åˆ›å»ºæ–°çš„æ–‡ä»¶å¤¹ ```${DATAPATH}``` ä»¥å­˜æ”¾æ•°æ®é›†ã€é¢„è®­ç»ƒckptã€ä»¥åŠfinetuneäº§ç”Ÿçš„æ¨¡å‹æ—¥å¿—&ckptã€‚æ¨èå·¥ä½œåŒºç›®å½•ç»“æ„å¦‚ä¸‹ï¼š\r\n\r\n```\r\nChinese-CLIP/\r\nâ”œâ”€â”€ run_scripts/\r\nâ”‚   â”œâ”€â”€ muge_finetune_vit-b-16_rbt-base.sh\r\nâ”‚   â”œâ”€â”€ flickr30k_finetune_vit-b-16_rbt-base.sh\r\nâ”‚   â””â”€â”€ ...           # æ›´å¤šfinetuneæˆ–è¯„æµ‹è„šæœ¬...\r\nâ””â”€â”€ cn_clip/\r\n    â”œâ”€â”€ clip/\r\n    â”œâ”€â”€ eval/\r\n    â”œâ”€â”€ preprocess/\r\n    â””â”€â”€ training/\r\n\r\n${DATAPATH}\r\nâ”œâ”€â”€ pretrained_weights/\r\nâ”œâ”€â”€ experiments/\r\nâ”œâ”€â”€ deploy/\t      # ç”¨äºå­˜æ”¾ONNX & TensorRTéƒ¨ç½²æ¨¡å‹\r\nâ””â”€â”€ datasets/\r\n    â”œâ”€â”€ MUGE/\r\n    â”œâ”€â”€ Flickr30k-CN/\r\n    â””â”€â”€ .../          # æ›´å¤šè‡ªå®šä¹‰æ•°æ®é›†...\r\n```\r\n\r\n### å‡†å¤‡å·¥ä½œ\r\nè¿™é‡Œæˆ‘ä»¬æä¾›é¢„è®­ç»ƒæ¨¡å‹å‚æ•°çš„ä¸‹è½½æ–¹å¼ï¼Œä»¥åŠè¿›è¡Œfinetuneå‰å¯¹æ•°æ®è¿›è¡Œçš„é¢„å¤„ç†è¿‡ç¨‹\r\n\r\n#### é¢„è®­ç»ƒCKPT\r\n\r\nè¯·å‚è€ƒå‰æ–‡[æ¨¡å‹è§„æ¨¡ & ä¸‹è½½é“¾æ¥](#model_card)éƒ¨åˆ†ï¼Œä¸‹è½½å¯¹åº”æ¨¡å‹ckptã€‚æ¨èå°†ä¸‹è½½çš„ckptæ–‡ä»¶å­˜æ”¾äº`${DATAPATH}/pretrained_weights/`ç›®å½•ä¸‹ã€‚\r\n\r\n#### æ•°æ®é›†æ ¼å¼é¢„å¤„ç†\r\n\r\nä¸ºäº†ä¸Chinese-CLIPä»£ç é€‚é…ï¼ŒåŒæ—¶ä¿è¯æ•°æ®å¤„ç†å’Œè¯»å–çš„æ•ˆç‡ï¼Œæˆ‘ä»¬å»ºè®®å°†è®­ç»ƒ&è¯„æµ‹ä½¿ç”¨çš„å›¾æ–‡æ•°æ®é›†ç»Ÿä¸€ç»„ç»‡æˆå¦‚ä¸‹çš„æ–¹å¼ï¼š\r\n\r\n```\r\n${DATAPATH}\r\nâ””â”€â”€ datasets/\r\n    â””â”€â”€ ${dataset_name}/\r\n        â”œâ”€â”€ train_imgs.tsv      # å›¾ç‰‡id & å›¾ç‰‡å†…å®¹\r\n        â”œâ”€â”€ train_texts.jsonl   # æ–‡æœ¬id & æ–‡æœ¬å†…å®¹ï¼Œè¿åŒåŒ¹é…çš„å›¾ç‰‡idåˆ—è¡¨\r\n        â”œâ”€â”€ valid_imgs.tsv\r\n        â”œâ”€â”€ valid_texts.jsonl\r\n        â”œâ”€â”€ test_imgs.tsv\r\n        â””â”€â”€ test_texts.jsonl\r\n```\r\nå…¶ä¸­`${dataset_name}`ä»£æŒ‡æ•°æ®é›†åç§°ï¼ˆå¦‚MUGEï¼‰\r\n\r\nä¸ºä¿è¯æ–‡ä»¶å¤„ç†æ•ˆç‡ï¼Œæˆ‘ä»¬ä¸æ˜¯å°†å›¾ç‰‡ä»¥å¤§é‡çš„å°æ–‡ä»¶æ–¹å¼å­˜æ”¾ï¼Œè€Œæ˜¯å°†è®­ç»ƒ/éªŒè¯/æµ‹è¯•å›¾ç‰‡ä»¥base64å½¢å¼åˆ†åˆ«å­˜æ”¾åœ¨`${split}_imgs.tsv`æ–‡ä»¶ä¸­ã€‚æ–‡ä»¶æ¯è¡Œè¡¨ç¤ºä¸€å¼ å›¾ç‰‡ï¼ŒåŒ…å«å›¾ç‰‡idï¼ˆintå‹ï¼‰ä¸å›¾ç‰‡base64ï¼Œä»¥tabéš”å¼€ï¼Œæ ¼å¼å¦‚ä¸‹ï¼š\r\n```\r\n1000002\t/9j/4AAQSkZJ...YQj7314oA//2Q==\r\n```\r\n\r\nå°†å›¾ç‰‡åŸå§‹æ–‡ä»¶è½¬æ¢ä¸ºbase64çš„æ–¹å¼éå¸¸ç®€å•ï¼Œè¯·æ‰§è¡Œä»¥ä¸‹pythonä»£ç ï¼š\r\n```python\r\nfrom PIL import Image\r\nfrom io import BytesIO\r\nimport base64\r\n\r\nimg = Image.open(file_name) # è®¿é—®å›¾ç‰‡è·¯å¾„\r\nimg_buffer = BytesIO()\r\nimg.save(img_buffer, format=img.format)\r\nbyte_data = img_buffer.getvalue()\r\nbase64_str = base64.b64encode(byte_data) # bytes\r\nbase64_str = base64_str.decode(\"utf-8\") # str\r\n```\r\n\r\næ–‡æœ¬ä¿¡æ¯åŠå›¾æ–‡å¯¹åŒ¹é…å…³ç³»åˆ™ä¿å­˜åœ¨`${split}_texts.jsonl`æ–‡ä»¶ã€‚æ–‡ä»¶æ¯è¡Œæ˜¯ä¸€è¡Œjsonï¼Œæ ¼å¼å¦‚ä¸‹ï¼š\r\n```\r\n{\"text_id\": 8428, \"text\": \"é«˜çº§æ„Ÿæ‰˜ç‰¹åŒ…æ–œæŒ\", \"image_ids\": [1076345, 517602]}\r\n```\r\nå¯¹äºæµ‹è¯•é›†åªæœ‰æ–‡æœ¬ï¼Œä¸çŸ¥é“å›¾æ–‡å¯¹åŒ¹é…å…³ç³»çš„æƒ…å†µï¼Œæ¯è¡Œçš„`image_ids`å­—æ®µå¤„ç†ä¸ºç©ºåˆ—è¡¨å³å¯ï¼Œå³`\"image_ids\": []`ã€‚\r\n\r\næœ€åï¼Œæˆ‘ä»¬è¿˜éœ€è¦å°†tsvå’Œjsonlæ–‡ä»¶ä¸€èµ·åºåˆ—åŒ–ï¼Œè½¬æ¢ä¸ºå†…å­˜ç´¢å¼•çš„LMDBæ•°æ®åº“æ–‡ä»¶ï¼Œæ–¹ä¾¿è®­ç»ƒæ—¶çš„éšæœºè¯»å–\r\n```\r\npython cn_clip/preprocess/build_lmdb_dataset.py \\\r\n    --data_dir ${DATAPATH}/datasets/${dataset_name}\r\n    --splits train,valid,test\r\n```\r\nä¾‹å¦‚å¯¹äºMUGEæ•°æ®é›†ï¼Œåˆ™`${dataset_name}`è®¾ä¸ºMUGEï¼Œ`--splits`æŒ‡å®šéœ€è¦è½¬æ¢çš„æ•°æ®é›†åˆ’åˆ†ï¼Œä»¥é€—å·ä¸åŠ ç©ºæ ¼åˆ†éš”ã€‚è½¬æ¢åï¼Œæ•°æ®é›†æ–‡ä»¶å¤¹ä¸‹ä¼šå¯¹åº”å¢åŠ ä»¥ä¸‹LMDBåºåˆ—åŒ–æ–‡ä»¶\r\n```\r\n${DATAPATH}\r\nâ””â”€â”€ datasets/\r\n    â””â”€â”€ ${dataset_name}/\r\n        â””â”€â”€ lmdb/\r\n            â”œâ”€â”€ train\r\n            â”‚Â Â  â”œâ”€â”€ imgs\r\n            â”‚Â Â  â””â”€â”€ pairs\r\n            â”œâ”€â”€ valid\r\n            â””â”€â”€ test\r\n```\r\n\r\nä¸ºäº†é™ä½ä¸Šæ‰‹éš¾åº¦ï¼Œæˆ‘ä»¬ä¹Ÿæä¾›äº†æŒ‰ä¸Šè¿°æ­¥éª¤é¢„å¤„ç†å¥½çš„MUGEæ•°æ®ï¼ˆ[ä¸‹è½½é“¾æ¥](https://clip-cn-beijing.oss-cn-beijing.aliyuncs.com/datasets/MUGE.zip)ï¼‰å’ŒFlickr30K-CNæ•°æ®ï¼ˆ[ä¸‹è½½é“¾æ¥](https://clip-cn-beijing.oss-cn-beijing.aliyuncs.com/datasets/Flickr30k-CN.zip)ï¼‰å‹ç¼©åŒ…ï¼Œç›´æ¥ä¸‹è½½è§£å‹å¹¶æ”¾ç½®äº`${DATAPATH}/datasets/`ç›®å½•ä¸‹å³å¯ã€‚å¦‚æœéœ€è¦[COCO-CN](https://github.com/li-xirong/coco-cn)æ•°æ®ï¼Œè¯·å‘åŸä½œè€…è¿›è¡Œç”³è¯·è®¸å¯å®Œæˆåï¼Œé‚®ä»¶è”ç³»æˆ‘ä»¬å§ã€‚\r\n\r\n### æ¨¡å‹finetune\r\n\r\nåœ¨æ­¤æˆ‘ä»¬ä»‹ç»è®­ç»ƒçš„æ­¥éª¤ï¼Œæ–¹ä¾¿å…¶ä»–ç”¨æˆ·äº†è§£æ¨¡å‹ç»†èŠ‚ï¼Œä½¿ç”¨æˆ‘ä»¬æä¾›çš„ä¸­æ–‡CLIPé¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œfinetuneã€‚åŸºäºMUGEå’ŒFlickr30K-CNä¸¤ä¸ªä¸‹æ¸¸æ£€ç´¢æ•°æ®é›†ï¼Œæˆ‘ä»¬æä¾›äº†è®­ç»ƒæ ·ä¾‹è„šæœ¬`run_scripts/muge_finetune_vit-b-16_rbt-base.sh`å’Œ`run_scripts/flickr30k_finetune_vit-b-16_rbt-base.sh`ã€‚<b>è¿è¡Œè„šæœ¬åŒæ—¶æ”¯æŒå•æœºï¼ˆå•å¡æˆ–å¤šå¡ï¼‰å’Œå¤šæœºåˆ†å¸ƒå¼è®­ç»ƒï¼Œè¯·åœ¨è¿è¡Œå‰ï¼Œå…ˆæ ¹æ®è„šæœ¬å¼€å¤´çš„æŒ‡å¼•æ³¨é‡Šï¼Œå¡«å†™å¥½åˆ†å¸ƒå¼ç›¸å…³é…ç½®ï¼Œä¹‹åè¿è¡Œå¦‚ä¸‹å‘½ä»¤å³å¯å¼€å§‹è®­ç»ƒï¼ˆå¤šæœºè®­ç»ƒè¯·åœ¨å„æœºå™¨ä¸Šéƒ½è¿è¡Œå‘½ä»¤ï¼‰ã€‚å¯¹äºæ˜¾å­˜ä¸è¶³çš„æƒ…å†µï¼Œå¯ä»¥è€ƒè™‘æ¿€æ´»é…ç½®é¡¹ä¸­çš„[é‡è®¡ç®—ç­–ç•¥](#checkpointing)ã€‚</b>è®­ç»ƒäº§ç”Ÿçš„logå’Œæ¨¡å‹ckptæ–‡ä»¶ï¼Œä¼šè‡ªåŠ¨ä¿å­˜åœ¨ç”¨æˆ·æŒ‡å®šçš„ç›®å½•ä¸‹ï¼š\r\n\r\n```bash\r\ncd Chinese-CLIP/\r\nbash run_scripts/muge_finetune_vit-b-16_rbt-base.sh ${DATAPATH}\r\n```\r\n\r\nç›¸å…³çš„è®­ç»ƒé…ç½®é¡¹åŒ…æ‹¬:\r\n\r\n+ åˆ†å¸ƒå¼\r\n  + `WORKER_CNT`: è®­ç»ƒçš„æœºå™¨ä¸ªæ•°\r\n  + `GPUS_PER_NODE`: æ¯ä¸ªæœºå™¨ä¸Šçš„GPUä¸ªæ•°\r\n+ è®­ç»ƒ/éªŒè¯æ•°æ®\r\n  + `train-data`: è®­ç»ƒæ•°æ®LMDBç›®å½•ï¼Œå‡†å¤‡LMDBæ•°æ®æ–‡ä»¶çš„é¢„å¤„ç†æµç¨‹è§ä¸Šã€‚\r\n  + `val-data`: éªŒè¯æ•°æ®LMDBç›®å½•ï¼ŒæŒ‡å®šä¸ºNoneæ—¶ï¼Œåˆ™ä¸è¿›è¡Œè®­ç»ƒè¿‡ç¨‹ä¸­çš„éªŒè¯ã€‚\r\n  + `num-workers`: è®­ç»ƒé›†æ•°æ®å¤„ç†ï¼ˆDataLoaderï¼‰çš„è¿›ç¨‹æ•°ï¼Œé»˜è®¤ä¸º4ã€‚\r\n  + `valid-num-workers`: éªŒè¯é›†æ•°æ®å¤„ç†ï¼ˆDataLoaderï¼‰çš„è¿›ç¨‹æ•°ï¼ˆå¦‚æœè¿›è¡ŒéªŒè¯ï¼‰ï¼Œé»˜è®¤ä¸º1ã€‚\r\n+ è®­ç»ƒè¶…å‚æ•°\r\n  + `vision-model`: æŒ‡å®šè§†è§‰backbone, ä» `[\"ViT-B-16\", \"ViT-L-14\", \"ViT-L-14-336\", \"ViT-H-14\", \"RN50\"]`é€‰æ‹©ã€‚\r\n  + `text-model`: æŒ‡å®šæ–‡æœ¬backbone, ä» `[\"RoBERTa-wwm-ext-base-chinese\", \"RoBERTa-wwm-ext-large-chinese\", \"RBT3-chinese\"]`é€‰æ‹©ã€‚\r\n  + `context-length`: æ–‡æœ¬è¾“å…¥åºåˆ—é•¿åº¦ã€‚\r\n  + `warmup`: warmupæ­¥æ•°ã€‚\r\n  + `batch-size`: è®­ç»ƒæ—¶å•å¡batch-sizeã€‚ï¼ˆè¯·ä¿è¯`è®­ç»ƒæ ·æœ¬æ€»æ•° > batch-size * GPUæ•°`ï¼Œè‡³å°‘æ»¡è¶³1ä¸ªè®­ç»ƒbatchï¼‰\r\n  + `lr`: å­¦ä¹ ç‡ã€‚\r\n  + `wd`: weight decayã€‚\r\n  + `max-steps`: è®­ç»ƒæ­¥æ•°ï¼Œä¹Ÿå¯é€šè¿‡`max-epochs`æŒ‡å®šè®­ç»ƒè½®æ•°ã€‚\r\n  + `freeze-vision`: æ˜¯å¦freezeè§†è§‰backboneã€‚\r\n  + `use-augment`: æ˜¯å¦ä½¿ç”¨[AutoAugment](https://arxiv.org/abs/1805.09501)å¯¹å›¾ç‰‡è¿›è¡Œæ•°æ®å¢å¼ºã€‚\r\n  + `valid-batch-size`: éªŒè¯æ—¶å•æœºbatch-sizeã€‚ï¼ˆè¯·ä¿è¯`éªŒè¯é›†æ ·æœ¬æ€»æ•° > batch-size * GPUæ•°`ï¼Œè‡³å°‘æ»¡è¶³1ä¸ªéªŒè¯batchï¼‰\r\n  + `valid-step-interval`å’Œ`valid-epoch-interval`: éªŒè¯step/epoché¢‘ç‡ï¼ŒæŒ‡å®šä¸º-1æ—¶åˆ™åœ¨è®­ç»ƒä¸­ä¸è¿›è¡ŒéªŒè¯ã€‚\r\n  + `grad-checkpointing`: <span id=\"checkpointing\"></span>ä½¿ç”¨[é‡è®¡ç®—ç­–ç•¥](https://pytorch.org/docs/stable/checkpoint.html)ï¼Œåœ¨å‰å‘è¿‡ç¨‹ä¸­ä¸ä¿å­˜ä¸­é—´ç»“æœï¼Œä»¥è®­ç»ƒæ—¶é—´æ¢å–æ›´å°çš„æ˜¾å­˜å¼€é”€ï¼Œé€‚ç”¨äºæ˜¾å­˜ä¸è¶³çš„æƒ…å†µã€‚ï¼ˆ`store_true`å‚æ•°ï¼Œç›´æ¥åœ¨è„šæœ¬ä¸­åŠ ä¸Š`--grad-checkpointing`å³å¯ï¼Œç›®å‰è¦æ±‚Pytorch>1.8.0ï¼‰\r\n  + `mask-ratio`: <span id=\"FLIP\"></span>å‚ç…§[FLIP](https://arxiv.org/abs/2212.00794)çš„ç­–ç•¥ï¼Œåœ¨finetuneæ—¶å¯æŒ‡å®šéšæœºmaskä¸€å®šæ¯”ä¾‹çš„å›¾åƒpatchï¼Œä»¥é™ä½æ˜¾å­˜å¼€é”€ã€åŠ å¿«è®­ç»ƒé€Ÿåº¦ã€‚é»˜è®¤ä¸º0.0ï¼Œå³ä¸æ¿€æ´»è¿™ä¸€ç­–ç•¥ã€‚\r\n  + `use-flash-attention`: ä½¿ç”¨[FlashAttention](https://arxiv.org/abs/2205.14135)ï¼Œå¯åœ¨ä¸å½±å“æ•ˆæœçš„æ¡ä»¶ä¸‹ä¸ºChinese-CLIPçš„finetuneè¿‡ç¨‹æ˜¾è‘—æé€Ÿä»¥åŠé™ä½æ˜¾å­˜å ç”¨ã€‚ï¼ˆ`store_true`å‚æ•°ï¼Œé…ç½®å¥½ç¯å¢ƒåï¼Œåœ¨è„šæœ¬ä¸­åŠ ä¸Š`--use-flash-attention`å³å¯ï¼Œè¯·è¯¦è§[flash_attention.md](flash_attention.md)ï¼‰\r\n  + `accum-freq`: <span id=\"gradient_accumulation\"></span>æ¢¯åº¦ç´¯ç§¯é¢‘ç‡ï¼Œé»˜è®¤ä¸º1ã€‚æŒ‡å®šä¸ºå¤§äº1çš„æ•´æ•°æ—¶å¼€å¯å¯¹æ¯”å­¦ä¹ æ¢¯åº¦ç´¯ç§¯ï¼Œæ¨¡æ‹Ÿæ›´å¤§çš„batch sizeã€‚å¦‚æœå•å¡batch sizeä¸º`m`ï¼Œåˆ™æ€»çš„batch sizeä¸º`accum_freq * m * GPUæ•°`ã€‚\r\n  + `gather-with-grad`: æ˜¯å¦åœ¨åˆ†å¸ƒå¼è®­ç»ƒæ—¶è¿›è¡Œå¸¦æœ‰å®Œæ•´æ¢¯åº¦çš„ç‰¹å¾gatherï¼Œé»˜è®¤å…³é—­ã€‚\r\n+ è¾“å‡ºé€‰é¡¹\r\n  + `name`: æŒ‡å®šè¾“å‡ºè·¯å¾„ã€‚è¶…å‚æ—¥å¿—, è®­ç»ƒæ—¥å¿—ä»¥åŠäº§å‡ºckptå‡ä¼šå­˜æ”¾è‡³ `${DATAPATH}/experiments/${name}/`ã€‚\r\n  + `save-step-frequency`åŠ`save-epoch-frequency`: å­˜ckptçš„æ­¥æ•°æˆ–è½®æ•°é—´éš”ã€‚\r\n  + `report-training-batch-acc`: æ—¥å¿—æ˜¯å¦æŠ¥å‘Šè®­ç»ƒå›¾åˆ°æ–‡&æ–‡åˆ°å›¾batchå‡†ç¡®ç‡ã€‚\r\n+ æƒé‡è¯»å–ç›¸å…³é€‰é¡¹\r\n  + `resume`: æƒé‡è¯»å–çš„è·¯å¾„ã€‚ç¤ºä¾‹è„šæœ¬ä¸­æŒ‡å®šä¸ºé¢„è®­ç»ƒckptè·¯å¾„ï¼Œä¹Ÿå¯ä»¥æŒ‡å®šä¸ºç”¨æˆ·è‡ªå·±finetuneçš„ckptè·¯å¾„åšç»§ç»­è®­ç»ƒã€‚\r\n  + `reset-data-offset`: æ˜¯å¦ä»æ­¤å‰çš„æ•°æ®æ–­ç‚¹ç»­è·‘ã€‚å¦‚batch sizeæˆ–GPUå¡æ•°è¶…å‚æ”¹å˜ï¼Œå»ºè®®æ‰“å¼€æ­¤é€‰é¡¹ã€‚\r\n  + `reset-optimizer`: æ˜¯å¦ä½¿ç”¨optimizer stateã€‚\r\n\r\nè®­ç»ƒå®Œæ¯•ï¼Œlog ä¼šè‡ªåŠ¨å­˜åœ¨`${DATAPATH}/experiments/${name}/out_${timestamp}.log`ï¼Œè®­ç»ƒlogæ ¼å¼å¦‚ä¸‹æ‰€ç¤º:\r\n```\r\n2022-12-11,20:40:34 | INFO | Rank 0 | Global Steps: 1/735 | Train Epoch: 1 [1024/250880 (0%)] | Loss: 2.371020 | Image2Text Acc: 49.90 | Text2Image Acc: 48.73 | Data Time: 1.039s | Batch Time: 3.625s | LR: 0.000000 | logit_scale: 4.605 | Global Batch Size: 1024\r\n```\r\néªŒè¯logæ ¼å¼å¦‚ä¸‹æ‰€ç¤º:\r\n```\r\n2022-12-11,20:42:47 | INFO | Rank 0 | Validation Result (epoch 1 @ 150 steps) | Valid Loss: 0.502810 | Image2Text Acc: 84.95 | Text2Image Acc: 84.26 | logit_scale: 4.605 | Valid Batch Size: 128\r\n```\r\n\r\n**æ³¨æ„**: å¯¹æ¯”å­¦ä¹ çš„è®­ç»ƒæ”¶æ•›å’Œç¨³å®šæ€§å’Œæ€»batch sizeç›¸å…³ã€‚å¦‚æ‚¨ä½¿ç”¨æ›´å°çš„batch sizeï¼ˆç›¸æ¯”é»˜è®¤é…ç½®128 per-GPU \\* 8 GPUï¼‰ï¼Œå»ºè®®ä½¿ç”¨æ›´å°çš„å­¦ä¹ ç‡ã€‚æˆ‘ä»¬æ¨èä½¿ç”¨æ›´å¤šçš„GPUå’Œæ›´å¤§çš„batch sizeä»¥å–å¾—æ›´å¥½çš„æ•ˆæœã€‚\r\n\r\n### é¢„æµ‹åŠè¯„ä¼°\r\n\r\næˆ‘ä»¬æä¾›ç‰¹å¾æå–ã€ä»¥åŠå›¾æ–‡æ£€ç´¢ä»»åŠ¡è¯„ä¼°çš„æµç¨‹ï¼Œå…·ä½“å¦‚ä¸‹ï¼š\r\n\r\n#### å›¾æ–‡ç‰¹å¾æå–\r\n\r\nç›®å‰æœ¬ä»£ç æ”¯æŒä½¿ç”¨GPUå•å¡è¿›è¡Œå›¾æ–‡ç‰¹å¾æå–ï¼Œè¯·å‚è€ƒä½¿ç”¨ä»¥ä¸‹å‘½ä»¤ã€‚æˆ‘ä»¬ä¹Ÿæä¾›äº†éƒ¨ç½²ONNXå’ŒTensorRTæ¨¡å‹ï¼ŒåŠ é€Ÿç‰¹å¾æ¨ç†çš„æ”¯æŒï¼Œè¯¦è§[deployment.md](deployment.md)ã€‚\r\n```bash\r\ncd Chinese-CLIP/\r\nexport CUDA_VISIBLE_DEVICES=0\r\nexport PYTHONPATH=${PYTHONPATH}:`pwd`/cn_clip\r\n\r\nsplit=valid # æŒ‡å®šè®¡ç®—validæˆ–testé›†ç‰¹å¾\r\nresume=${DATAPATH}/pretrained_weights/clip_cn_vit-b-16.pt\r\n\r\npython -u cn_clip/eval/extract_features.py \\\r\n    --extract-image-feats \\\r\n    --extract-text-feats \\\r\n    --image-data=\"${DATAPATH}/datasets/${dataset_name}/lmdb/${split}/imgs\" \\\r\n    --text-data=\"${DATAPATH}/datasets/${dataset_name}/${split}_texts.jsonl\" \\\r\n    --img-batch-size=32 \\\r\n    --text-batch-size=32 \\\r\n    --context-length=52 \\\r\n    --resume=${resume} \\\r\n    --vision-model=ViT-B-16 \\\r\n    --text-model=RoBERTa-wwm-ext-base-chinese\r\n```\r\n\r\näº§å‡ºå›¾æ–‡ç‰¹å¾é»˜è®¤å°†ä¿å­˜äº`${DATAPATH}/datasets/${dataset_name}`ç›®å½•ä¸‹ï¼Œå›¾ç‰‡ç‰¹å¾ä¿å­˜äº`${split}_imgs.img_feat.jsonl`æ–‡ä»¶ï¼Œæ¯è¡Œä»¥jsonå­˜å‚¨ä¸€å¼ å›¾ç‰‡çš„ç‰¹å¾ï¼Œæ ¼å¼å¦‚ä¸‹ï¼š\r\n```\r\n{\"image_id\": 1000002, \"feature\": [0.0198, ..., -0.017, 0.0248]}\r\n```\r\næ–‡æœ¬ç‰¹å¾åˆ™ä¿å­˜äº`${split}_texts.txt_feat.jsonl`ï¼Œæ ¼å¼å¦‚ä¸‹ï¼š\r\n```\r\n{\"text_id\": 248816, \"feature\": [0.1314, ..., 0.0018, -0.0002]}\r\n```\r\n\r\n#### KNNæ£€ç´¢\r\n\r\nå¯¹äºå°è§„æ¨¡çš„å­¦æœ¯æ£€ç´¢æ•°æ®é›†ï¼Œæˆ‘ä»¬æä¾›ä¸€ä¸ªç®€å•çš„KNNæ£€ç´¢å®ç°ï¼Œä¾¿äºè®¡ç®—æ–‡åˆ°å›¾ã€å›¾åˆ°æ–‡æ£€ç´¢çš„top-kå¬å›ç»“æœï¼ˆtipsï¼šå¦‚æƒ³ä»¿ç…§æˆ‘ä»¬åœ¨é¡¹ç›®ä¸­æ­å»º[æ£€ç´¢demo](https://www.modelscope.cn/studios/damo/chinese_clip_applications/summary)ï¼Œå»ºè®®åŸºäºä¸­æ–‡CLIPæ¨¡å‹äº§å‡ºå›¾æ–‡ç‰¹å¾åï¼Œç»“åˆå¼€æºå·¥ç¨‹æ¡†æ¶[clip-retrieval](https://github.com/rom1504/clip-retrieval)æ­å»ºå‰åç«¯æœåŠ¡ã€‚ï¼‰\r\n\r\nå¯¹äºæ–‡åˆ°å›¾æ£€ç´¢ï¼ˆæ–‡æœ¬å¬å›ç›¸å…³å›¾ç‰‡ï¼‰ï¼Œè¯·è¿è¡Œä»¥ä¸‹å‘½ä»¤ï¼š\r\n```bash\r\ncd Chinese-CLIP/\r\nsplit=valid # æŒ‡å®šè®¡ç®—validæˆ–testé›†ç‰¹å¾\r\npython -u cn_clip/eval/make_topk_predictions.py \\\r\n    --image-feats=\"${DATAPATH}/datasets/${dataset_name}/${split}_imgs.img_feat.jsonl\" \\\r\n    --text-feats=\"${DATAPATH}/datasets/${dataset_name}/${split}_texts.txt_feat.jsonl\" \\\r\n    --top-k=10 \\\r\n    --eval-batch-size=32768 \\\r\n    --output=\"${DATAPATH}/datasets/${dataset_name}/${split}_predictions.jsonl\"\r\n```\r\näº§å‡ºçš„ç»“æœä¿å­˜åœ¨æŒ‡å®šçš„jsonlæ–‡ä»¶ä¸­ï¼Œæ¯è¡Œè¡¨ç¤ºä¸€ä¸ªæ–‡æœ¬å¬å›çš„top-kå›¾ç‰‡idï¼Œæ ¼å¼å¦‚ä¸‹ï¼š\r\n```json\r\n{\"text_id\": 153915, \"image_ids\": [5791244, 1009692167, 7454547004, 3564007203, 38130571, 2525270674, 2195419145, 2503091968, 4966265765, 3690431163]}\r\n```\r\n\r\nå¯¹äºå›¾åˆ°æ–‡æ£€ç´¢ï¼ˆå›¾ç‰‡å¬å›ç›¸å…³æ–‡æœ¬ï¼‰ï¼Œç±»ä¼¼åœ°ï¼Œè¯·è¿è¡Œä»¥ä¸‹å‘½ä»¤ï¼š\r\n```bash\r\nsplit=valid # æŒ‡å®šè®¡ç®—validæˆ–testé›†ç‰¹å¾\r\npython -u cn_clip/eval/make_topk_predictions_tr.py \\\r\n    --image-feats=\"${DATAPATH}/datasets/${dataset_name}/${split}_imgs.img_feat.jsonl\" \\\r\n    --text-feats=\"${DATAPATH}/datasets/${dataset_name}/${split}_texts.txt_feat.jsonl\" \\\r\n    --top-k=10 \\\r\n    --eval-batch-size=32768 \\\r\n    --output=\"${DATAPATH}/datasets/${dataset_name}/${split}_tr_predictions.jsonl\"\r\n```\r\näº§å‡ºç»“æœæ¯è¡Œè¡¨ç¤ºä¸€ä¸ªå›¾ç‰‡å¬å›çš„top-kæ–‡æœ¬idï¼Œæ ¼å¼å¦‚ä¸‹ï¼š\r\n```json\r\n{\"image_id\": 977856234, \"text_ids\": [156914, 157914, 158914, 155914, 156179, 158907, 157179, 154179, 154914, 154723]}\r\n```\r\n\r\n#### Recallè®¡ç®—\r\n\r\næˆ‘ä»¬æä¾›äº†è¯„æµ‹è„šæœ¬è®¡ç®—æ£€ç´¢ä»»åŠ¡çš„Recall@1/5/10ï¼ŒåŒæ—¶ç»™å‡ºmean recallï¼ˆRecall@1/5/10çš„å¹³å‡æ•°ï¼‰ã€‚è¿è¡Œå¦‚ä¸‹å‘½ä»¤å³å¯è·å–åˆ†æ•°:\r\n\r\nå¯¹äºæ–‡åˆ°å›¾æ£€ç´¢ï¼Œè¯·è¿è¡Œå‘½ä»¤ï¼š\r\n```bash\r\nsplit=valid # æŒ‡å®šè®¡ç®—validæˆ–testé›†ç‰¹å¾\r\npython cn_clip/eval/evaluation.py \\\r\n    ${DATAPATH}/datasets/${dataset_name}/${split}_texts.jsonl \\\r\n    ${DATAPATH}/datasets/${dataset_name}/${split}_predictions.jsonl \\\r\n    output.json\r\ncat output.json\r\n```\r\n\r\nå¯¹äºå›¾åˆ°æ–‡æ£€ç´¢ï¼Œè¯·å…ˆè¿è¡Œä¸‹é¢çš„å‘½ä»¤ï¼Œå°†å›¾æ–‡å¯¹æ ‡æ³¨çš„jsonlæ–‡ä»¶ç”±æ–‡åˆ°å›¾çš„æ ¼å¼è½¬ä¸ºå›¾åˆ°æ–‡ï¼š\r\n```bash\r\npython cn_clip/eval/transform_ir_annotation_to_tr.py \\\r\n    --input ${DATAPATH}/datasets/${dataset_name}/${split}_texts.jsonl\r\n```\r\nå®Œæˆåï¼Œè¯·è¿è¡Œå‘½ä»¤ï¼š\r\n```bash\r\nsplit=valid # æŒ‡å®šè®¡ç®—validæˆ–testé›†ç‰¹å¾\r\npython cn_clip/eval/evaluation_tr.py \\\r\n    ${DATAPATH}/datasets/${dataset_name}/${split}_texts.tr.jsonl \\\r\n    ${DATAPATH}/datasets/${dataset_name}/${split}_tr_predictions.jsonl \\\r\n    output.json\r\ncat output.json\r\n```\r\næ‰“å°å‡ºçš„ç»“æœæ ¼å¼å°†å¦‚ä¸‹ï¼š\r\n```json\r\n{\"success\": true, \"score\": 85.67, \"scoreJson\": {\"score\": 85.67, \"mean_recall\": 85.67, \"r1\": 71.2, \"r5\": 90.5, \"r10\": 95.3}}\r\n```\r\n\r\nå…³äºæ•´å¥—è·¨æ¨¡æ€æ£€ç´¢çš„è®­ç»ƒå’Œæµ‹è¯•æµç¨‹ï¼Œæˆ‘ä»¬ä»¥MUGEæ£€ç´¢æ•°æ®é›†ï¼ˆ[å¤šæ¨¡æ€ç”µå•†å›¾æ–‡æŒ‘æˆ˜èµ›](https://tianchi.aliyun.com/competition/entrance/532031/introduction)ï¼‰ä¸ºä¾‹ï¼Œä¹Ÿæä¾›äº†ä¸€ä¸ªåŒ…å«ä¸Šè¿°å…¨éƒ¨æµç¨‹å¹¶å¯è¿è¡Œçš„Jupyter Notebookï¼ˆ[ä¸‹è½½é“¾æ¥](https://clip-cn-beijing.oss-cn-beijing.aliyuncs.com/others/Chinese-CLIP-on-MUGE-Retrieval.ipynb)ï¼‰ï¼Œæ¬¢è¿å¤§å®¶ä¸Šæ‰‹å®è·µã€‚\r\n\r\n<br>\r\n\r\n## é›¶æ ·æœ¬å›¾åƒåˆ†ç±»\r\næœ¬éƒ¨åˆ†ä»‹ç»å¦‚ä½•ä½¿ç”¨Chinese-CLIPå®ç°é›¶æ ·æœ¬å›¾åƒåˆ†ç±»ï¼Œä»¥é›¶æ ·æœ¬å›¾åƒåˆ†ç±»Benchmark ELEVATERä¸­çš„æ•°æ®é›†ä¸ºä¾‹ã€‚ELEVATERæ˜¯ç”±å¤šä¸ªçŸ¥åçš„åˆ†ç±»æ•°æ®é›†ï¼ˆåŒ…æ‹¬CIFAR-10ã€CIFAR-100ã€MNISTç­‰ï¼‰ç»„æˆçš„è¯„æµ‹é›†åˆï¼Œè¯„æµ‹æ¨¡å‹åœ¨è¿™äº›æ•°æ®é›†ä¸Šçš„é›¶æ ·æœ¬æ•ˆæœã€‚æˆ‘ä»¬åœ¨å®éªŒä¸­ï¼Œç»™å…¶ä¸­æ¯ä¸ªæ•°æ®é›†å‡†å¤‡äº†ä¸­æ–‡ç‰ˆæœ¬çš„promptã€ç±»åˆ«æ ‡ç­¾è¿åŒåŸå§‹å›¾ç‰‡ï¼Œè¯¦è§[æ•°æ®æ–‡æ¡£](https://github.com/OFA-Sys/Chinese-CLIP/blob/master/zeroshot_dataset.md)ï¼Œç”¨äºæµ‹è¯•Chinese-CLIPæ¨¡å‹ã€‚æ›´å¤šå…³äºè¯¥benchmarkçš„è¯¦æƒ…è¯·ç‚¹å‡»[é“¾æ¥](https://eval.ai/web/challenges/challenge-page/1832/overview)ã€‚å¤§å®¶ä¹Ÿå¯ä»¥å‚è€ƒæˆ‘ä»¬æä¾›çš„æµç¨‹ï¼Œä»¿ç…§åœ¨è‡ªå·±çš„ä¸­æ–‡åˆ†ç±»æ•°æ®é›†å‡†å¤‡æ•°æ®å¹¶è¿›è¡Œæµ‹è¯•ã€‚\r\n<br>\r\n\r\n### å‡†å¤‡å·¥ä½œ\r\né¦–å…ˆå°†æ•°æ®æŒ‰ç…§å¦‚ä¸‹æ ¼å¼è¿›è¡Œå‡†å¤‡ã€‚ç”±äºé›¶æ ·æœ¬å›¾åƒåˆ†ç±»ä»…éœ€æµ‹è¯•ï¼Œå› æ­¤åªéœ€è¦å‡†å¤‡å¥½æµ‹è¯•é›†å’Œé¢„è®­ç»ƒæ¨¡å‹å‚æ•°ï¼ŒæŒ‰ç…§å¦‚ä¸‹ç›®å½•ç»“æ„ï¼Œå­˜æ”¾åœ¨ç”¨æˆ·æŒ‡å®šçš„`${DATAPATH}`ä¸‹ï¼š\r\n```\r\n${DATAPATH}\r\nâ”œâ”€â”€ pretrained_weights/\r\nâ””â”€â”€ datasets/\r\n    â””â”€â”€ ${dataset_name}/\r\n        â”œâ”€â”€ label_cn.txt\r\n        â””â”€â”€ test/\r\n\t    â”œâ”€â”€ 000/ # label idï¼Œå¦‚labelä¸ªæ•°å¤§äº10ï¼Œåˆ™å°†å…¶å‘å·¦è¡¥é›¶åˆ°3ä½æ•°ä¿è¯å­—å…¸åº\r\n\t    â”‚   â”œâ”€â”€ image_0003.jpg # å›¾ç‰‡æ ·æœ¬ï¼Œå‘½åæ— ç‰¹æ®Šè¦æ±‚\r\n\t    â”‚   â”œâ”€â”€ image_0005.jpg\r\n\t    â”‚Â Â  â””â”€â”€ ...\r\n\t    â”œâ”€â”€ 001/\r\n\t    â”‚Â Â  â”œâ”€â”€ image_0001.jpg\r\n\t    â”‚Â Â  â”œâ”€â”€ image_0002.jpg\r\n\t    â”‚Â Â  â””â”€â”€ ...\r\n\t    â””â”€â”€ 002/\r\n\t        â”œâ”€â”€ image_0003.jpg\r\n\t        â”œâ”€â”€ image_0005.jpg\r\n\t        â””â”€â”€ ...\r\n\t    ...\r\n\t\r\n```\r\næµ‹è¯•é›†ä¿è¯testæ–‡ä»¶å¤¹å†…æ•°æ®æŒ‰ç…§labelå¯¹åº”çš„idè¿›è¡Œåˆ’åˆ†ï¼Œå¹¶ä¿è¯idä¸ºå­—å…¸åºï¼ˆ10ä»¥ä¸Šçš„å¤šä½æ•°ï¼Œéœ€å‘å·¦è¡¥é›¶`label.zfill(3)`, å¦‚001ï¼Œ002ç­‰ï¼‰ã€‚`label_cn.txt`ä¸ºæ•°æ®æ ‡ç­¾ï¼Œæ¯è¡Œä¸€ä¸ªæ ‡ç­¾åï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š\r\n```\r\næ‰‹é£ç´\r\né£æœº\r\né”š\r\n...\r\n```\r\næ¯è¡Œçš„æ ‡ç­¾å¯¹åº”çš„label idä¸º`è¡Œå·-1`ï¼Œå¦‚ç¬¬1è¡Œçš„æ ‡ç­¾çš„idä¸º0ï¼Œç¬¬äºŒè¡Œçš„æ ‡ç­¾çš„idä¸º1ã€‚å¦‚æœæ ‡ç­¾æ€»æ•°å¤§äº10ï¼Œåˆ™ç»Ÿä¸€å‘å·¦è¡¥é›¶åˆ°3ä½æ•°ï¼Œæ¯”å¦‚æ ‡ç­¾ä¸ªæ•°ä¸º100ï¼Œæ ‡ç­¾idåˆ™ä¸º`000-099`ã€‚ç”¨æˆ·éœ€ä¸ºæ¯ä¸ªlabel idç”Ÿæˆå¯¹åº”çš„æ–‡ä»¶å¤¹ï¼Œå¹¶å°†æ ‡æ³¨è¯¥labelçš„æ ·æœ¬æ”¾å…¥å…¶ä¸­ã€‚æˆ‘ä»¬ä»¥ELEVATERä¸­çš„**CIFAR-100æ•°æ®é›†**ä¸ºæ ·ä¾‹ï¼Œè¯·ç‚¹å‡»[é“¾æ¥](http://clip-cn-beijing.oss-cn-beijing.aliyuncs.com/datasets/cifar-100.zip)ä¸‹è½½å¤„ç†å¥½çš„æ•°æ®ã€‚å¦‚æœæƒ³å°è¯•åœ¨å…¶ä»–ELEVATERåŒ…å«çš„æ•°æ®é›†ä¸Šæµ‹è¯•Chinese-CLIPï¼Œè¯·å‚è§æˆ‘ä»¬çš„[æ•°æ®æ–‡æ¡£](https://github.com/OFA-Sys/Chinese-CLIP/blob/master/zeroshot_dataset.md)ã€‚\r\n<br>\r\n\r\n### é¢„æµ‹å’Œè¯„ä¼°\r\næˆ‘ä»¬å‡†å¤‡äº†é¢„æµ‹è„šæœ¬ï¼Œè¯·æŸ¥çœ‹`run_scripts/zeroshot_eval.sh`ã€‚è¿è¡Œå‘½ä»¤ä¾‹å­å¦‚ä¸‹ï¼š\r\n```bash\r\nbash run_scripts/zeroshot_eval.sh 0 \\\r\n    ${DATAPATH} ${dataset_name} \\\r\n    ${vision_model} ${text_model} \\\r\n    ${ckpt_path} ${index_file}\r\n```\r\nå…¶ä¸­å„å‚æ•°æ„ä¹‰ä¸ºï¼š\r\n+ ç¬¬ä¸€ä¸ªå…¥å‚`0`ä¸ºGPU id\r\n+ `DATAPATH`å‚è§ä¸Šé¢çš„å‡†å¤‡å·¥ä½œéƒ¨åˆ†ï¼Œæ ¹æ®å®é™…ä½ç½®è¾“å…¥å¯¹åº”è·¯å¾„\r\n+ `dataset_name`å‚è§ä¸Šé¢çš„å‡†å¤‡å·¥ä½œéƒ¨åˆ†ï¼Œè¾“å…¥è¯„æµ‹çš„æ•°æ®é›†ç›®å½•åï¼Œå¦‚`cifar-100`\r\n+ `vision_model`ä¸ºæŒ‡å®šæ¨¡å‹ç±»å‹ï¼Œé€‰é¡¹åŒ…æ‹¬`[\"ViT-B-32\", \"ViT-B-16\", \"ViT-L-14\", \"ViT-L-14-336\", \"RN50\", \"ViT-H-14\"]`\r\n+ `text_model`åŒ…æ‹¬`[\"RoBERTa-wwm-ext-base-chinese\", \"RoBERTa-wwm-ext-large-chinese\", \"RBT3-chinese\"]`\r\n+ `ckpt_path`ä¸ºæ¨¡å‹é¢„è®­ç»ƒckptçš„å®Œæ•´è·¯å¾„\r\n+ `index_file`ï¼ˆå¯é€‰ï¼Œä»…æäº¤ELEVATERå®˜ç½‘è¯„æµ‹éœ€è¦æŒ‡å®šï¼‰ï¼Œè¯·å‚è§[æ•°æ®æ–‡æ¡£](https://github.com/OFA-Sys/Chinese-CLIP/blob/master/zeroshot_dataset.md)\r\n\r\nä¾‹å¦‚ï¼Œç”¨ViT-B/16è§„æ¨¡é¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œè¯„æµ‹CIFAR-100ï¼Œåˆ™è¿è¡Œï¼ˆ`${DATAPATH}`éœ€æ ¹æ®å®é™…æƒ…å†µæ›¿æ¢ï¼‰ï¼š\r\n```bash\r\nbash run_scripts/zeroshot_eval.sh 0 \\\r\n    ${DATAPATH} cifar-100 \\\r\n    ViT-B-16 RoBERTa-wwm-ext-base-chinese \\\r\n    ${DATAPATH}/pretrained_weights/clip_cn_vit-b-16.pt\r\n```\r\n\r\nè¿”å›ç»“æœä¼šæ‰“å°top-1çš„å‡†ç¡®ç‡ã€‚\r\n```\r\nResult:\r\nzeroshot-top1: 0.6444\r\n```\r\nåœ¨CIFAR-100ä¸Šï¼ŒViT-B/16è§„æ¨¡çš„Chinese-CLIPé¢„æœŸåº”è¯¥è¾¾åˆ°64.4%ã€‚æˆ‘ä»¬åœ¨ELEVATERä¸Šå…¶ä»–è§„æ¨¡ã€å…¶ä»–æ•°æ®é›†çš„é›¶æ ·æœ¬åˆ†ç±»ç»“æœï¼Œè¯·è¯¦è§[Results.md](https://github.com/OFA-Sys/Chinese-CLIP/blob/master/Results.md#zeroshot_results)ã€‚\r\n\r\nåŒæ—¶ï¼Œç¨‹åºè¿˜ä¼šå­˜ä¸‹ä¸€ä¸ªjsonæ–‡ä»¶ç”¨äºæäº¤ELEVATERå®˜æ–¹ç”¨ï¼Œjsonæ–‡ä»¶å†…å®¹å¦‚ä¸‹æ‰€ç¤ºï¼š\r\n```json\r\n{\"model_name\": \"CN-CLIP-ViT-B-16\", \"dataset_name\": \"cifar-100\", \"num_trainable_params\": 0, \"num_params\": 188262913, \"num_visual_params\": 86192640, \"num_backbone_params\": 188262913, \"n_shot\": 0, \"rnd_seeds\": [123], \"predictions\": \"prediction probability tensor [size: (1, 10000, 100)]\"}\r\n```\r\nå…¶ä¸­åŒ…æ‹¬æ¨¡å‹å`model_name`ã€æ•°æ®é›†åç§°`dataset_name`ã€æ€»å‚æ•°é‡`num_params`ã€è§†è§‰å¡”çš„å‚æ•°é‡`num_visual_params`ç­‰æ¨¡å‹çš„metaä¿¡æ¯ï¼Œä»¥åŠæ¨¡å‹è¾“å‡ºç»“æœï¼Œå³æ¨¡å‹çš„é¢„æµ‹æ¦‚ç‡tensorï¼Œsizeä¸º`[1, æ ·æœ¬æ•°, æ ‡ç­¾ä¸ªæ•°]`ã€‚\r\n\r\n### é›¶æ ·æœ¬åˆ†ç±»åœ¨çº¿Demo\r\nåŸºäºæˆ‘ä»¬é›†æˆäºHuggingface transformersçš„ç‰¹å¾æå–APIï¼Œæˆ‘ä»¬åœ¨Huggingface Model HubğŸ¤—æä¾›äº†åœ¨çº¿ç®€å•å°è¯•é›¶æ ·æœ¬å›¾åƒåˆ†ç±»çš„demoï¼ˆHosted inference APIï¼‰ï¼Œå„ä¸ªæ¨¡å‹è§„æ¨¡çš„demoé“¾æ¥è§ä¸‹ï¼Œæ¬¢è¿å°è¯•ï¼\r\n- [OFA-Sys/chinese-clip-vit-base-patch16](https://huggingface.co/OFA-Sys/chinese-clip-vit-base-patch16)\r\n- [OFA-Sys/chinese-clip-vit-large-patch14](https://huggingface.co/OFA-Sys/chinese-clip-vit-large-patch14)\r\n- [OFA-Sys/chinese-clip-vit-large-patch14-336px](https://huggingface.co/OFA-Sys/chinese-clip-vit-large-patch14-336px)\r\n- [OFA-Sys/chinese-clip-vit-huge-patch14](https://huggingface.co/OFA-Sys/chinese-clip-vit-huge-patch14)\r\n- **ï¼ˆ12.10æ—¥æ›´æ–°ğŸ”¥ï¼‰**[**åŸºäºHuggingface Spaceséƒ¨ç½²çš„æ–°ç‰ˆdemo**](https://huggingface.co/spaces/OFA-Sys/chinese-clip-zero-shot-image-classification)ï¼šdemoé¡µé¢åŒæ—¶åŒ…å«ä¸Šè¿°4ä¸ªæ¨¡å‹è§„æ¨¡å¯é€‰ï¼Œæ”¯æŒè¾“å…¥è‡ªå®šä¹‰promptæ¨¡æ¿ï¼Œæ¬¢è¿è¯•ç”¨ \r\n<br><br><br>\r\n\r\n# å¼•ç”¨\r\nå¦‚æœè§‰å¾—æœ¬é¡¹ç›®å¥½ç”¨ï¼Œå¸Œæœ›èƒ½ç»™æˆ‘ä»¬æä¸ªstarå¹¶åˆ†äº«ç»™èº«è¾¹çš„ç”¨æˆ·ï¼Œæ¬¢è¿ç»™ç›¸å…³å·¥ä½œcitationï¼Œæ„Ÿè°¢æ”¯æŒï¼\r\n\r\n```\r\n@article{chinese-clip,\r\n  title={Chinese CLIP: Contrastive Vision-Language Pretraining in Chinese},\r\n  author={Yang, An and Pan, Junshu and Lin, Junyang and Men, Rui and Zhang, Yichang and Zhou, Jingren and Zhou, Chang},\r\n  journal={arXiv preprint arXiv:2211.01335},\r\n  year={2022}\r\n}\r\n```\r\n"
        },
        {
          "name": "README_En.md",
          "type": "blob",
          "size": 35.0068359375,
          "content": "[**ä¸­æ–‡è¯´æ˜**](README.md) | [**English**](README_En.md)\r\n\r\n<p align=\"center\">\r\n    <br>\r\n    <img src=\"assets/Chinese_CLIP_logo_tp_path.svg\" width=\"400\" />\r\n    <br>\r\n<p>\r\n<br>\r\n\r\n<p align=\"center\">\r\n        <a href=\"https://www.modelscope.cn/models?name=clip&tasks=multi-modal-embedding\">ModelScope</a>&nbsp ï½œ &nbsp<a href=\"https://www.modelscope.cn/studios/damo/chinese_clip_applications/summary\">Demo</a>&nbsp ï½œ &nbsp<a href=\"https://arxiv.org/abs/2211.01335\">Paper </a>&nbsp ï½œ &nbspBlog\r\n</p>\r\n<br><br>\r\n\r\nThis is the Chinese version of CLIP. We use a large-scale Chinese image-text pair dataset (~200M) to train the model, and we hope that it can help users to conveniently achieve [image representation generation](#api-use-case), [cross-modal retrieval](#cross-modal-retrieval) and [zero-shot image classification](#zero-shot-image-classification) for Chinese data. This repo is based on <b>[open_clip project](https://github.com/mlfoundations/open_clip)</b>. We have made some optimization for better performance on Chinese data, and we provide the details in the following. \r\n<br><br>\r\n\r\n# News\r\n* 2023.11.30 Chinese-CLIP has added the [conversion script](https://github.com/OFA-Sys/Chinese-CLIP/blob/master/cn_clip/deploy/pytorch_to_coreml.py) to transform Pytorch models into coreml format for deployment. (Thanks [@manymuch](https://github.com/manymuch) for [the PR](https://github.com/OFA-Sys/Chinese-CLIP/pull/230) â¤ï¸ï¼‰\r\n* 2023.9.8 Chinese-CLIP has supported [knowledge distillation fine-tuning](distillation_En.md) based on [ModelScope](https://github.com/modelscope/modelscope). (Thanks [@wuziheng](https://github.com/wuziheng) and [@Jaskr616](https://github.com/Jaskr616) from Aliyun PAI Team for [the PR](https://github.com/OFA-Sys/Chinese-CLIP/pull/195)â¤ï¸)\r\n* 2023.5.9 Chinese-CLIP has been adapted to Pytorch2.0.\r\n* 2023.3.20 Support [gradient accumulation](#gradient-accumulation) in contrastive learning to simulate the training effect of a larger batch size.\r\n* 2023.2.16 Support [FlashAttention](https://github.com/HazyResearch/flash-attention) to improve training speed and reduce memory usage. See [flash_attention_En.md](flash_attention_En.md) for more information.\r\n* 2023.1.15 Support the conversion of Pytorch models into [ONNX](https://onnx.ai/) or [TensorRT](https://developer.nvidia.com/tensorrt) formats (and provide pretrained TensorRT models) to improve inference speed and meet deployment requirements. See [deployment_En.md](deployment_En.md) for more information.\r\n* 2022.12.12 Implement [FLIP](https://arxiv.org/abs/2212.00794) strategy, which can be [activated](#FLIP) during finetuning (Thanks [@zwkkk](https://github.com/zwkkk) for [the PR](https://github.com/OFA-Sys/Chinese-CLIP/pull/26) â¤ï¸ï¼‰\r\n* 2022.12.3 The datasets of the Chinese version of the [ELEVATER](https://eval.ai/web/challenges/challenge-page/1832) benchmark are publicly available. See [Notes for datasets](zeroshot_dataset_en.md) for more information. \r\n* 2022.12.1 Chinese-CLIP model & representation generation API are officially merged into Huggingface transformersğŸ¤— codebase.\r\n* 2022.11.22 Release [zero-shot image classification](#zero-shot-image-classification) code. Support [ELEVATER](https://eval.ai/web/challenges/challenge-page/1832) zero-shot classification benchmark.\r\n* 2022.11.3 Release RN50, ViT-H-14 models. Release [technical report](https://arxiv.org/pdf/2211.01335.pdf).\r\n* 2022.9.22 Release ViT-L-14, ViT-L-14-336 models.\r\n* 2022.7.13 Release [fast image & text representation generation API](#api-use-case), which facitilates usage of our CLIP models quickly.\r\n* 2022.7.8 Release the project Chinese-CLIP! Release [image-text retrieval](#cross-modal-retrieval) code.\r\n<br><br>\r\n\r\n# Models and Results\r\n<span id=\"model_card\"></span>\r\n## Model Card\r\nCurrently, we release 5 different sizes of Chinese-CLIP models. Detailed information and download link of each Chinese-CLIP model are provided below:\r\n\r\n<table border=\"1\" width=\"100%\">\r\n    <tr align=\"center\">\r\n        <th>Model</th><th>Ckpt</th><th>#Params (All)</th><th>Backbone (I)</th><th>#Params (I)</th><th>Backbone (T)</th><th>#Params (T)</th><th>Resolution</th>\r\n    </tr>\r\n    <tr align=\"center\">\r\n        <td>CN-CLIP<sub>RN50</sub></td><td><a href=\"https://clip-cn-beijing.oss-cn-beijing.aliyuncs.com/checkpoints/clip_cn_rn50.pt\">Download</a></td><td>77M</td><td>ResNet50</td><td>38M</td><td>RBT3</td><td>39M</td><td>224</td>\r\n    </tr>\r\n    <tr align=\"center\">\r\n        <td>CN-CLIP<sub>ViT-B/16</sub></td><td><a href=\"https://clip-cn-beijing.oss-cn-beijing.aliyuncs.com/checkpoints/clip_cn_vit-b-16.pt\">Download</a></td><td>188M</td><td>ViT-B/16</td><td>86M</td><td>RoBERTa-wwm-Base</td><td>102M</td><td>224</td>\r\n    </tr>\r\n    <tr align=\"center\">\r\n        <td>CN-CLIP<sub>ViT-L/14</sub></td><td><a href=\"https://clip-cn-beijing.oss-cn-beijing.aliyuncs.com/checkpoints/clip_cn_vit-l-14.pt\">Download</a></td><td>406M</td><td>ViT-L/14</td><td>304M</td><td>RoBERTa-wwm-Base</td><td>102M</td><td>224</td>\r\n    </tr>\r\n    <tr align=\"center\">\r\n        <td>CN-CLIP<sub>ViT-L/14@336px</sub></td><td><a href=\"https://clip-cn-beijing.oss-cn-beijing.aliyuncs.com/checkpoints/clip_cn_vit-l-14-336.pt\">Download</a></td><td>407M</td><td>ViT-L/14</td><td>304M</td><td>RoBERTa-wwm-Base</td><td>102M</td><td>336</td>\r\n    </tr>\r\n    <tr align=\"center\">\r\n        <td>CN-CLIP<sub>ViT-H/14</sub></td><td><a href=\"https://clip-cn-beijing.oss-cn-beijing.aliyuncs.com/checkpoints/clip_cn_vit-h-14.pt\">Download</a></td><td>958M</td><td>ViT-H/14</td><td>632M</td><td>RoBERTa-wwm-Large</td><td>326M</td><td>224</td>\r\n    </tr>\r\n</table>\r\n<br></br>\r\n\r\n## Results\r\nWe conducted zero-shot inference and finetuning experiments on [MUGE Retrieval](https://tianchi.aliyun.com/muge), [Flickr30K-CN](https://github.com/li-xirong/cross-lingual-cap) and [COCO-CN](https://github.com/li-xirong/coco-cn) for the evaluation of cross-modal retrieval, and conducted experiments on 10 image classification datasets of the [ELEVATER](https://eval.ai/web/challenges/challenge-page/1832) benchmark for the evaluation of zero-shot image classification. Results are shown below. Due to space limitation, here we only list the performance of the best performing Chinese-CLIP and baseline models. For detailed performance of each Chinese-CLIP model size, please refer to [Results.md](Results.md).\r\n\r\n**MUGE Text-to-Image Retrieval (Official Validation Set)**:\r\n<table border=\"1\" width=\"100%\">\r\n    <tr align=\"center\">\r\n        <th>Setup</th><th colspan=\"4\">Zero-shot</th><th colspan=\"4\">Finetune</th>\r\n    </tr>\r\n    <tr align=\"center\">\r\n        <td>Metric</td><td>R@1</td><td>R@5</td><td>R@10</td><td>MR</td><td>R@1</td><td>R@5</td><td>R@10</td><td>MR</td>\r\n    </tr>\r\n\t<tr align=\"center\">\r\n        <td width=\"120%\">Wukong</td><td>42.7</td><td>69.0</td><td>78.0</td><td>63.2</td><td>52.7</td><td>77.9</td><td>85.6</td><td>72.1</td>\r\n    </tr>\r\n\t<tr align=\"center\">\r\n        <td width=\"120%\">R2D2</td><td>49.5</td><td>75.7</td><td>83.2</td><td>69.5</td><td>60.1</td><td>82.9</td><td>89.4</td><td>77.5</td>\r\n    </tr>\r\n\t<tr align=\"center\">\r\n        <td width=\"120%\">CN-CLIP</td><td>63.0</td><td>84.1</td><td>89.2</td><td>78.8</td><td>68.9</td><td>88.7</td><td>93.1</td><td>83.6</td>\r\n    </tr>\r\n</table>\r\n<br>\r\n\r\n**Flickr30K-CN Retrieval (Official Test Set)**:\r\n<table border=\"1\" width=\"120%\">\r\n\t<tr align=\"center\">\r\n        <th>Task</th><th colspan=\"6\">Text-to-Image</th><th colspan=\"6\">Image-to-Text</th>\r\n    </tr>\r\n    <tr align=\"center\">\r\n        <th>Setup</th><th colspan=\"3\">Zero-shot</th><th colspan=\"3\">Finetune</th><th colspan=\"3\">Zero-shot</th><th colspan=\"3\">Finetune</th>\r\n    </tr>\r\n    <tr align=\"center\">\r\n        <td>Metric</td><td>R@1</td><td>R@5</td><td>R@10</td><td>R@1</td><td>R@5</td><td>R@10</td><td>R@1</td><td>R@5</td><td>R@10</td><td>R@1</td><td>R@5</td><td>R@10</td>\r\n    </tr>\r\n\t<tr align=\"center\">\r\n        <td width=\"120%\">Wukong</td><td>51.7</td><td>78.9</td><td>86.3</td><td>77.4</td><td>94.5</td><td>97.0</td><td>76.1</td><td>94.8</td><td>97.5</td><td>92.7</td><td>99.1</td><td>99.6</td>\r\n    </tr>\r\n\t<tr align=\"center\">\r\n        <td width=\"120%\">Taiyi</td><td>60.8</td><td>85.0</td><td>91.0</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td>\r\n    </tr>\t\r\n\t<tr align=\"center\">\r\n        <td width=\"120%\">R2D2</td><td>60.9</td><td>86.8</td><td>92.7</td><td>84.4</td><td>96.7</td><td>98.4</td><td>77.6</td><td>96.7</td><td>98.9</td><td>95.6</td><td>99.8</td><td>100.0</td>\r\n    </tr>\r\n\t<tr align=\"center\">\r\n        <td width=\"120%\">CN-CLIP</td><td>71.2</td><td>91.4</td><td>95.5</td><td>83.8</td><td>96.9</td><td>98.6</td><td>81.6</td><td>97.5</td><td>98.8</td><td>95.3</td><td>99.7</td><td>100.0</td>\r\n    </tr>\r\n</table>\r\n<br>\r\n\r\n**COCO-CN Retrieval (Official Test Set)**:\r\n<table border=\"1\" width=\"100%\">\r\n\t<tr align=\"center\">\r\n        <th>Task</th><th colspan=\"6\">Text-to-Image</th><th colspan=\"6\">Image-to-Text</th>\r\n    </tr>\r\n    <tr align=\"center\">\r\n        <th>Setup</th><th colspan=\"3\">Zero-shot</th><th colspan=\"3\">Finetune</th><th colspan=\"3\">Zero-shot</th><th colspan=\"3\">Finetune</th>\r\n    </tr>\r\n    <tr align=\"center\">\r\n        <td>Metric</td><td>R@1</td><td>R@5</td><td>R@10</td><td>R@1</td><td>R@5</td><td>R@10</td><td>R@1</td><td>R@5</td><td>R@10</td><td>R@1</td><td>R@5</td><td>R@10</td>\r\n    </tr>\r\n\t<tr align=\"center\">\r\n        <td width=\"120%\">Wukong</td><td>53.4</td><td>80.2</td><td>90.1</td><td>74.0</td><td>94.4</td><td>98.1</td><td>55.2</td><td>81.0</td><td>90.6</td><td>73.3</td><td>94.0</td><td>98.0</td>\r\n    </tr>\r\n\t<tr align=\"center\">\r\n        <td width=\"150%\">Taiyi</td><td>60.0</td><td>84.0</td><td>93.3</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td>\r\n    </tr>\t\r\n\t<tr align=\"center\">\r\n        <td width=\"120%\">R2D2</td><td>56.4</td><td>85.0</td><td>93.1</td><td>79.1</td><td>96.5</td><td>98.9</td><td>63.3</td><td>89.3</td><td>95.7</td><td>79.3</td><td>97.1</td><td>98.7</td>\r\n    </tr>\r\n\t<tr align=\"center\">\r\n        <td width=\"120%\">CN-CLIP</td><td>69.2</td><td>89.9</td><td>96.1</td><td>81.5</td><td>96.9</td><td>99.1</td><td>63.0</td><td>86.6</td><td>92.9</td><td>83.5</td><td>97.3</td><td>99.2</td>\r\n    </tr>\r\n</table>\r\n<br>\r\n\r\n**Zero-shot Image Classification**:\r\n<table border=\"1\" width=\"100%\">\r\n\t<tr align=\"center\">\r\n        <th>Task</th><th>CIFAR10</th><th>CIFAR100</th><th>DTD</th><th>EuroSAT</th><th>FER</th><th>FGVC</th><th>KITTI</th><th>MNIST</th><th>PC</th><th>VOC</th>\r\n    </tr>\r\n\t<tr align=\"center\">\r\n        <td width=\"150%\">GIT</td><td>88.5</td><td>61.1</td><td>42.9</td><td>43.4</td><td>41.4</td><td>6.7</td><td>22.1</td><td>68.9</td><td>50.0</td><td>80.2</td>\r\n    </tr>\r\n    \t<tr align=\"center\">\r\n        <td width=\"150%\">ALIGN</td><td>94.9</td><td>76.8</td><td>66.1</td><td>52.1</td><td>50.8</td><td>25.0</td><td>41.2</td><td>74.0</td><td>55.2</td><td>83.0</td>\r\n    </tr>\r\n\t<tr align=\"center\">\r\n        <td width=\"150%\">CLIP</td><td>94.9</td><td>77.0</td><td>56.0</td><td>63.0</td><td>48.3</td><td>33.3</td><td>11.5</td><td>79.0</td><td>62.3</td><td>84.0</td>\r\n    </tr>\r\n    \t<tr align=\"center\">\r\n        <td width=\"150%\">Wukong</td><td>95.4</td><td>77.1</td><td>40.9</td><td>50.3</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td>\r\n    </tr>\r\n    \t<tr align=\"center\">\r\n        <td width=\"150%\">CN-CLIP</td><td>96.0</td><td>79.7</td><td>51.2</td><td>52.0</td><td>55.1</td><td>26.2</td><td>49.9</td><td>79.4</td><td>63.5</td><td>84.9</td>\r\n    </tr>\r\n</table>\r\n<br><br>\r\n\r\n# Getting Started\r\n## Installation Requirements\r\nTo start with this project, make sure that your environment meets the requirements below:\r\n\r\n* python >= 3.6.4\r\n* pytorch >= 1.8.0 (with torchvision >= 0.9.0)\r\n* CUDA Version >= 10.2\r\n\r\nRun the following command to install required packages.\r\n\r\n```bash\r\npip install -r requirements.txt\r\n```\r\n\r\n## API Use Case\r\nWe provide a simple code snippet to show how to use the API for Chinese-CLIP. For starters, please install cn_clip:\r\n```bash\r\n# to install the latest stable release\r\npip install cn_clip\r\n\r\n# or install from source code\r\ncd Chinese-CLIP\r\npip install -e .\r\n```\r\nAfter installation, use Chinese CLIP to compute the image ([example](examples/pokemon.jpeg)) & text embeddings and similarities as shown below:\r\n```python\r\nimport torch \r\nfrom PIL import Image\r\n\r\nimport cn_clip.clip as clip\r\nfrom cn_clip.clip import load_from_name, available_models\r\nprint(\"Available models:\", available_models())  \r\n# Available models: ['ViT-B-16', 'ViT-L-14', 'ViT-L-14-336', 'ViT-H-14', 'RN50']\r\n\r\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\r\nmodel, preprocess = load_from_name(\"ViT-B-16\", device=device, download_root='./')\r\nmodel.eval()\r\nimage = preprocess(Image.open(\"examples/pokemon.jpeg\")).unsqueeze(0).to(device)\r\ntext = clip.tokenize([\"æ°å°¼é¾Ÿ\", \"å¦™è›™ç§å­\", \"å°ç«é¾™\", \"çš®å¡ä¸˜\"]).to(device)\r\n\r\nwith torch.no_grad():\r\n    image_features = model.encode_image(image)\r\n    text_features = model.encode_text(text)\r\n    # Normalize the features. Please use the normalized features for downstream tasks.\r\n    image_features /= image_features.norm(dim=-1, keepdim=True) \r\n    text_features /= text_features.norm(dim=-1, keepdim=True)      \r\n\r\n    logits_per_image, logits_per_text = model.get_similarity(image, text)\r\n    probs = logits_per_image.softmax(dim=-1).cpu().numpy()\r\n\r\nprint(\"Label probs:\", probs)  # [[1.268734e-03 5.436878e-02 6.795761e-04 9.436829e-01]]\r\n```\r\n\r\nHowever, if you are not satisfied with only using the API, move on for more details about training and inference. \r\n<br><br>\r\n\r\n\r\n# Tutorial\r\n\r\nCurrently, we provide the tutorial of [cross-modal retrieval](#cross-modal-retrieval) and [zero-shot image classification](#zero-shot-image-classification) below.\r\n\r\n## Cross-Modal Retrieval\r\n\r\n### Code Organization\r\n\r\nAfter cloning this project, please create a new directory ```${DATAPATH}``` for datasets, checkpoints and logs. A recommended workspace structure is demonstrated belowï¼š\r\n\r\n```\r\nChinese-CLIP/\r\nâ”œâ”€â”€ run_scripts/\r\nâ”‚   â”œâ”€â”€ muge_finetune_vit-b-16_rbt-base.sh\r\nâ”‚   â”œâ”€â”€ flickr30k_finetune_vit-b-16_rbt-base.sh\r\nâ”‚   â””â”€â”€ ...           # more scripts for finetuning and evaluation...\r\nâ””â”€â”€ src/\r\n    â”œâ”€â”€ clip/\r\n    â”œâ”€â”€ eval/\r\n    â”œâ”€â”€ preprocess/\r\n    â””â”€â”€ training/\r\n\r\n${DATAPATH}\r\nâ”œâ”€â”€ pretrained_weights/\r\nâ”œâ”€â”€ experiments/\r\nâ”œâ”€â”€ deploy/\t      # store ONNX & TensorRT deployment models\r\nâ””â”€â”€ datasets/\r\n    â”œâ”€â”€ MUGE/\r\n    â”œâ”€â”€ Flickr30k-CN/\r\n    â””â”€â”€ .../          # more datasets...\r\n```\r\n\r\n### Preparation\r\nWe provide links for the downloading of pretrained checkpoints, as well as the data preprocessing procedures for finetuning. \r\n\r\n#### Pretrained Checkpoints\r\n\r\nPlease refer to [model card section](#model_card) above and download the model checkpoint. We recommend putting the checkpoint in `${DATAPATH}/pretrained_weights/`. \r\n\r\n#### Data Preprocessing\r\n\r\nWe advise to organize the data in the following way to ensure the efficiency of accessing and processing data:\r\n\r\n```\r\n${DATAPATH}\r\nâ””â”€â”€ datasets/\r\n    â””â”€â”€ ${dataset_name}/\r\n        â”œâ”€â”€ train_imgs.tsv      # image id & image content\r\n        â”œâ”€â”€ train_texts.jsonl   # text id & text content, with list of paired image ids\r\n        â”œâ”€â”€ valid_imgs.tsv\r\n        â”œâ”€â”€ valid_texts.jsonl\r\n        â”œâ”€â”€ test_imgs.tsv\r\n        â””â”€â”€ test_texts.jsonl\r\n```\r\nwhere `${dataset_name}` refers to the name of dataset (e.g., MUGE).\r\n\r\nTo ensure the efficiency of processing data, we did not store images with small files, but instead we encode them to base64 strings and store them in `${split}_imgs.tsv`. Each line represents an image, where there are id (int) and base64 string, split by `\\t`, as shown below:  \r\n```\r\n1000002\t/9j/4AAQSkZJ...YQj7314oA//2Q==\r\n```\r\n\r\nTransforming image files to base64 strings is simple. Run the following code:\r\n```python\r\nfrom PIL import Image\r\nfrom io import BytesIO\r\nimport base64\r\n\r\nimg = Image.open(file_name) # path to file\r\nimg_buffer = BytesIO()\r\nimg.save(img_buffer, format=img.format)\r\nbyte_data = img_buffer.getvalue()\r\nbase64_str = base64.b64encode(byte_data) # bytes\r\nbase64_str = base64_str.decode(\"utf-8\") # str\r\n```\r\n\r\nTexts and image-text pairing relations are stored in `${split}_texts.jsonl`, where each line is a json as shown below:\r\n\r\n```\r\n{\"text_id\": 8428, \"text\": \"é«˜çº§æ„Ÿæ‰˜ç‰¹åŒ…æ–œæŒ\", \"image_ids\": [1076345, 517602]}\r\n```\r\nFor the test set where only the texts are given and the image-text pairing relations are unknown, just leave the `image_ids` field as an empty list, `\"image_ids\": []`.\r\n\r\nFinally, we need to serialize tsv and jsonl and transform them to LMDB files, which is easy for random access during training.\r\n```\r\npython src/preprocess/build_lmdb_dataset.py \\\r\n    --data_dir ${DATAPATH}/datasets/${dataset_name}\r\n    --splits train,valid,test\r\n```\r\nFor example, for the MUGE dataset, we name `${dataset_name}` to MUGE. `--splits` refers to dataset splitsï¼Œsplit by commas without space. After that, there will be LMDB files in the directory.\r\n```\r\n${DATAPATH}\r\nâ””â”€â”€ datasets/\r\n    â””â”€â”€ ${dataset_name}/\r\n        â””â”€â”€ lmdb/\r\n            â”œâ”€â”€ train\r\n            â”‚   â”œâ”€â”€ imgs\r\n            â”‚   â””â”€â”€ pairs\r\n            â”œâ”€â”€ valid\r\n            â””â”€â”€ test\r\n```\r\n\r\nFor easier use, we have provided preprocessed MUGE ([download link](https://clip-cn-beijing.oss-cn-beijing.aliyuncs.com/datasets/MUGE.zip)) and Flickr30K-CN ([download link](https://clip-cn-beijing.oss-cn-beijing.aliyuncs.com/datasets/Flickr30k-CN.zip)) datasets in zip format. To use them, just download and unzip it under `${DATAPATH}/datasets/`. If you need [COCO-CN](https://github.com/li-xirong/coco-cn) dataset, please contact us by email when you have finished applying for permission from the original author.\r\n\r\n### Finetuning\r\n\r\nWe introduce the procedures of training for users to learn about the details of the model. We finetune with the pretrained Chinese CLIP. For MUGE and Flickr30K-CN, we provide scripts `run_scripts/muge_finetune_vit-b-16_rbt-base.sh` and `run_scripts/flickr30k_finetune_vit-b-16_rbt-base.sh`. <b>The scripts support single-worker and distributed training. Before running, follow the instructions at the beggining of the scripts and fill in your configuration for distributed training. Then run the scripts to start your training. If the GPU memory is insufficient, you can consider to [activate the gradient checkpointing strategy](#checkpointing) in the configuration.</b> Logs and checkpoints will be saved at your specified paths. \r\n\r\n```bash\r\ncd Chinese-CLIP/\r\nbash run_scripts/muge_finetune_vit-b-16_rbt-base.sh ${DATAPATH}\r\n```\r\n\r\nThe configuration for training includes:\r\n\r\n+ Distributed training\r\n  + `WORKER_CNT`: the number of machines.\r\n  + `GPUS_PER_NODE`: the number of GPUS on each machine.\r\n+ Data for training/validation\r\n  + `train-data`: directory of training data. Follow the procedures above the create LMDB files.\r\n  + `val-data`: directory of validation data. If set to None, validation during finetuning will be disabled.\r\n  + `num-workers`: the number of workers for training set dataloader, default to 4.\r\n  + `valid-num-workers`: the number of workers for validation set dataloader, default to 1.\r\n+ Training hyper-params\r\n  + `vision-model`: specified visual backbones. Select from `[\"ViT-B-16\", \"ViT-L-14\", \"ViT-L-14-336\", \"ViT-H-14\", \"RN50\"]`.\r\n  + `text-model`: specified language backbones. Select from `[\"RoBERTa-wwm-ext-base-chinese\", \"RoBERTa-wwm-ext-large-chinese\", \"RBT3-chinese\"]`.\r\n  + `context-length`: sequence length for text inputs.\r\n  + `warmup`: steps for warmup.\r\n  + `batch-size`: batch size for a worker (make sure that the number of training samples larger than `batch-size * GPUs`).\r\n  + `lr`: learning rate.\r\n  + `wd`: weight decay.\r\n  + `max-steps`: training steps. Also you can set `max-epochs` to set the number of training epochs.\r\n  + `freeze-vision`: whether to freeze the visual backbone. \r\n  + `use-augment`: whether to use [AutoAugment](https://arxiv.org/abs/1805.09501) for data augmentation. \r\n  + `valid-batch-size`: validation batch size for a worker (make sure that the number of validation samples larger than `valid-batch-size * GPUs`).\r\n  + `valid-step-interval` and `valid-epoch-interval`: validation step / epoch frequency, if set to -1 then validation will be disabled during finetuning.\r\n  + `grad-checkpointing`: <span id=\"checkpointing\"></span>use [gradient checkpointing]((https://pytorch.org/docs/stable/checkpoint.html)) which does not keep the activations during forward computation, this strategy trades more computation and iteration time for less GPU memory cost. (`store_true` argument, just add `--grad-checkpointing` in the script to activate it, requires Pytorch>1.8.0)\r\n  + `mask-ratio`: <span id=\"FLIP\"></span>use [FLIP](https://arxiv.org/abs/2212.00794) strategy which randomly masks a ratio of image patches to save GPU memory and speed up training. Default to 0.0, which disables the strategy.\r\n  + `use-flash-attention`: whether to use [FlashAttention](https://arxiv.org/abs/2205.14135), which can significantly speed up the finetune process and reduce the memory usage. (`store_true` argument, after configuring the environment, just add `--use-flash-attention` in the script to activate it, please see [flash_attention_En.md](flash_attention_En.md) for more information)\r\n  + `accum-freq`: <span id=\"gradient-accumulation\"></span>Gradient accumulation frequency, default is 1. Specify an integer greater than 1 to enable gradient accumulation to simulate a larger batch size. if the batch size for a worker is `m`, the total batch size is `accum_freq * m * GPUs`.\r\n  + `gather-with-grad`: Whether to enable full distributed gradient for feature gather, off by default.\r\n+ Ouputs\r\n  + `name`: specified output path. Hyperparameter logs, training logs, and checkpoints will be saved at `${DATAPATH}/experiments/${name}/`.\r\n  + `save-step-frequency` and `save-epoch-frequency`: the intervals for saving checkpoints.\r\n  + `report-training-batch-acc`: whether to report the in-batch image-to-text and text-to-image retrieval accuracy. \r\n+ Checkpoints\r\n  + `resume`: the checkpoint path for weights to restore. In the provided example script, the path refers to the pretrained checkpoint path. Users can change to your own checkpoint path.\r\n  + `reset-data-offset`: whether to restore training at the data breakpoint.\r\n  + `reset-optimizer`: whether to restore the optimizer state.\r\n\r\nAfter training, the log will be saved at `${DATAPATH}/experiments/${name}/out_${timestamp}.log`. Example of log is shown below:\r\n```\r\n2022-12-11,20:40:34 | INFO | Rank 0 | Global Steps: 1/735 | Train Epoch: 1 [1024/250880 (0%)] | Loss: 2.371020 | Image2Text Acc: 49.90 | Text2Image Acc: 48.73 | Data Time: 1.039s | Batch Time: 3.625s | LR: 0.000000 | logit_scale: 4.605 | Global Batch Size: 1024\r\n```\r\nThe example of validation log is shown below:\r\n```\r\n2022-12-11,20:42:47 | INFO | Rank 0 | Validation Result (epoch 1 @ 150 steps) | Valid Loss: 0.502810 | Image2Text Acc: 84.95 | Text2Image Acc: 84.26 | logit_scale: 4.605 | Valid Batch Size: 128\r\n```\r\n\r\n**Attention**: The convergence and stability of contrastive learning is highly relevant to the total batch size. If you use a smaller batch size, (in comparison with the default 128 per-GPU \\* 8 GPU), we advise you to use a smaller learning rat. We recommend using more GPUs and larger batch size for better performance. \r\n\r\n### Inference and Evaluation\r\n\r\nWe provide procedures for representation generation and cross-modal retrieval, as demonstrated below:\r\n\r\n#### Image/Text Representation Generation\r\n\r\nBy now the code supports representation generation with a single worker, please use the following commands. Besides, we provide support for deploying ONNX and TensorRT models to accelerate feature inference, see [deployment_En.md](deployment_En.md) for details.\r\n```bash\r\ncd Chinese-CLIP/\r\nexport CUDA_VISIBLE_DEVICES=0\r\nexport PYTHONPATH=${PYTHONPATH}:`pwd`/src\r\n\r\nsplit=valid # validation / test set\r\nresume=${DATAPATH}/pretrained_weights/clip_cn_vit-b-16.pt\r\n\r\npython -u src/eval/extract_features.py \\\r\n    --extract-image-feats \\\r\n    --extract-text-feats \\\r\n    --image-data=\"${DATAPATH}/datasets/${dataset_name}/lmdb/${split}/imgs\" \\\r\n    --text-data=\"${DATAPATH}/datasets/${dataset_name}/${split}_texts.jsonl\" \\\r\n    --img-batch-size=32 \\\r\n    --text-batch-size=32 \\\r\n    --context-length=52 \\\r\n    --resume=${resume} \\\r\n    --vision-model=ViT-B-16 \\\r\n    --text-model=RoBERTa-wwm-ext-base-chinese\r\n```\r\n\r\nBy default, the representations are stored at `${DATAPATH}/datasets/${dataset_name}`. Specifically, the image representations are stored at `${split}_imgs.img_feat.jsonl`. Each line stores a json of image representation, as shown below:\r\n```\r\n{\"image_id\": 1000002, \"feature\": [0.0198, ..., -0.017, 0.0248]}\r\n```\r\nText representations are stored at `${split}_texts.txt_feat.jsonl`ï¼Œas shown below:\r\n```\r\n{\"text_id\": 248816, \"feature\": [0.1314, ..., 0.0018, -0.0002]}\r\n```\r\n\r\n#### KNN Retrieval\r\n\r\nFor small-scale retrieval datasets, we provide a simple implementation of KNN retrieval, to facilitate the retrieval of top-k results in cross-modal retrieval. (tips: If you want to build a [retrieval demo](https://www.modelscope.cn/studios/damo/chinese_clip_applications/summary) in your project like us, we suggest first to use Chinese-CLIP to compute image and text embeddings, and then employ an opensource servering framework [clip-retrieval](https://github.com/rom1504/clip-retrieval) to deploy the front-end and back-end servering.)\r\n\r\nFor text-to-image retrieval, run the commands below:\r\n```bash\r\ncd Chinese-CLIP/\r\nsplit=valid # validation / test splits\r\npython -u src/eval/make_topk_predictions.py \\\r\n    --image-feats=\"${DATAPATH}/datasets/${dataset_name}/${split}_imgs.img_feat.jsonl\" \\\r\n    --text-feats=\"${DATAPATH}/datasets/${dataset_name}/${split}_texts.txt_feat.jsonl\" \\\r\n    --top-k=10 \\\r\n    --eval-batch-size=32768 \\\r\n    --output=\"${DATAPATH}/datasets/${dataset_name}/${split}_predictions.jsonl\"\r\n```\r\nResults are stored at specified jsonl files. Each line consists of top-k image ids for a text query, as shown below:\r\n```json\r\n{\"text_id\": 153915, \"image_ids\": [5791244, 1009692167, 7454547004, 3564007203, 38130571, 2525270674, 2195419145, 2503091968, 4966265765, 3690431163]}\r\n```\r\n\r\nFor image-to-text retrieval, run the commands belowï¼š\r\n```bash\r\nsplit=valid # validation / test splits\r\npython -u src/eval/make_topk_predictions_tr.py \\\r\n    --image-feats=\"${DATAPATH}/datasets/${dataset_name}/${split}_imgs.img_feat.jsonl\" \\\r\n    --text-feats=\"${DATAPATH}/datasets/${dataset_name}/${split}_texts.txt_feat.jsonl\" \\\r\n    --top-k=10 \\\r\n    --eval-batch-size=32768 \\\r\n    --output=\"${DATAPATH}/datasets/${dataset_name}/${split}_tr_predictions.jsonl\"\r\n```\r\nResults are stored at specified jsonl files. Each line consists of top-k text ids for an image query, as shown below:\r\n```json\r\n{\"image_id\": 977856234, \"text_ids\": [156914, 157914, 158914, 155914, 156179, 158907, 157179, 154179, 154914, 154723]}\r\n```\r\n\r\n#### Recall Metric\r\n\r\nWe provide scripts for computing the Recall@1/5/10 and mean recall (the mean of Recall@1/5/10). Run the commands to get the scores:\r\n\r\nFor text-to-image retrieval, run the commands below:\r\n```bash\r\nsplit=valid # validation / test splits\r\npython src/eval/evaluation.py \\\r\n        ${DATAPATH}/datasets/${dataset_name}/${split}_texts.jsonl \\\r\n        ${DATAPATH}/datasets/${dataset_name}/${split}_predictions.jsonl \\\r\n        output.json\r\ncat output.json\r\n```\r\n\r\n\r\nFor image-to-text retrieval, run the commands first to transform text-to-image jsonls to image-to-text ones:\r\n```bash\r\npython src/eval/transform_ir_annotation_to_tr.py \\\r\n        --input ${DATAPATH}/datasets/${dataset_name}/${split}_texts.jsonl\r\n```\r\nAfter that, run the following commands\r\n```bash\r\nsplit=valid # validation / test splits\r\npython src/eval/evaluation_tr.py \\\r\n        ${DATAPATH}/datasets/${dataset_name}/${split}_texts.tr.jsonl \\\r\n        ${DATAPATH}/datasets/${dataset_name}/${split}_tr_predictions.jsonl \\\r\n        output.json\r\ncat output.json\r\n```\r\n\r\nThe printed results are shown below:\r\n```json\r\n{\"success\": true, \"score\": 85.67, \"scoreJson\": {\"score\": 85.67, \"mean_recall\": 85.67, \"r1\": 71.2, \"r5\": 90.5, \"r10\": 95.3}}\r\n```\r\n\r\nFor better understanding of cross-modal retrieval by Chinese-CLIP, we also provide a runnable jupyter notebook ([download link](https://clip-cn-beijing.oss-cn-beijing.aliyuncs.com/others/Chinese-CLIP-on-MUGE-Retrieval.ipynb)), which works with the MUGE retrieval dataset (corresponding leaderboard is hosted on [Tianchi](https://tianchi.aliyun.com/competition/entrance/532031/introduction?lang=en-us)) and includes the finetuning and inference process mentioned above. Welcome to try!\r\n\r\n<br>\r\n\r\n## Zero-shot Image Classification\r\nThis section introduces the use of Chinese-CLIP for zero-shot image classification. We use the experiment on a dataset of the benchmark ELEVATER as an example. ELEVATER is a benchmark consist of several widely used classification datasets and evaluates the zero-shot performance on these datasets, including CIFAR-10, CIFAR-100, MNIST, etc. In our experiments, we have perpared Chinese prompts and label names with the original images for each ELEVATER dataset (refer to [Notes for datasets](zeroshot_dataset_en.md) for download) to evaluate Chinese-CLIP. For more information about ELEVATER, please click [this link](https://eval.ai/web/challenges/challenge-page/1832/overview). Users can also follow the procedure below to prepare and evaluate their own classification datasets.\r\n<br><br>\r\n\r\n### Preparation\r\nWe need to prepare only the test set and the pretrained Chinese-CLIP checkpoint. It's recommended to prepare these directories under a user defined `${DATAPATH}` and organize them as follows:\r\n```\r\n${DATAPATH}\r\nâ”œâ”€â”€ pretrained_weights/\r\nâ””â”€â”€ datasets/\r\n    â””â”€â”€ ${dataset_name}/\r\n        â”œâ”€â”€ label_cn.txt\r\n        â””â”€â”€ test/\r\n\t    â”œâ”€â”€ 000/ # label idï¼Œfill 0 by the left to 3 digits so that the labels can be alphabetically ordered\r\n\t    â”‚   â”œâ”€â”€ image_0003.jpg # image sample, no specific requirements for the naming\r\n\t    â”‚   â”œâ”€â”€ image_0005.jpg\r\n\t    â”‚Â Â  â””â”€â”€ ...\r\n\t    â”œâ”€â”€ 001/\r\n\t    â”‚Â Â  â”œâ”€â”€ image_0001.jpg\r\n\t    â”‚Â Â  â”œâ”€â”€ image_0002.jpg\r\n\t    â”‚Â Â  â””â”€â”€ ...\r\n\t    â””â”€â”€ 002/\r\n\t        â”œâ”€â”€ image_0003.jpg\r\n\t        â”œâ”€â”€ image_0005.jpg\r\n\t        â””â”€â”€ ...\r\n\t    ...\r\n\t\r\n```\r\nMake sure the data are categorized by their label id, and make sure the ids are alphabetically orderd (for numbers larger than 10, use`label.zfill(3)` to fill 0 by the left to 3 digits, like 001ï¼Œ002, etc). `label_cn.txt` refers to the file of label names. Each line has a label name, as demonstrated below:\r\n```\r\naccordion\r\nairplane\r\nanchor\r\n...\r\n```\r\nThe label id is `[line number]-1`. For example, the label id for the first line is 0, and the one for the second line is 1. If the number of labels is larger than 10, all labels are filled with 0 by the left to 3-digit numbers. For example, if the number of labels is 100, the ids are `000-099`. Users should create a directory for each label, and put the corresponding samples into the directories. We provide the processed dataset CIFAR-100 as an example, and please click [this link](http://clip-cn-beijing.oss-cn-beijing.aliyuncs.com/datasets/cifar-100.zip) to download the prepared dataset. To evaluate other datasets of ELEVATER, please refer to [Notes for datasets](zeroshot_dataset_en.md) for download.\r\n<br><br>\r\n\r\n### Prediction and Evaluation\r\nWe provide a script for prediction and evaluation. Please check `run_scripts/zeroshot_eval.sh` for more details. An example command is shown below:\r\n```bash\r\nbash run_scripts/zeroshot_eval.sh 0 \\\r\n   ${DATAPATH} ${dataset_name} \\\r\n   ${vision_model} ${text_model} \\\r\n   ${ckpt_path} ${index_file}\r\n```\r\nwhere the arguments stand for:\r\n+ the first argument `0` refers to the GPU ID\r\n+ `DATAPATH` refers to the root directory storing the checkpoint and dataset, as mentioned in Preparation part above\r\n+ `dataset_name` refers to the directory name of the dataset, e.g. cifar-100, as mentioned in Preparation part above\r\n+ `vision_model` refers to the type of vision encoder, including `[\"ViT-B-32\", \"ViT-B-16\", \"ViT-L-14\", \"ViT-L-14-336\", \"RN50\", \"ViT-H-14\"]`\r\n+ `text_model` refers to the type of text encoder, including `[\"RoBERTa-wwm-ext-base-chinese\", \"RoBERTa-wwm-ext-large-chinese\", \"RBT3-chinese\"]`\r\n+ `ckpt_path` refers to the complete path of the pretrained Chinese-CLIP checkpoint\r\n+ `index_file` is optional and only needed when you would like to submit to ELEVATER official website. Please refer to [Notes for datasets](zeroshot_dataset_en.md) for more details\r\n\r\nFor example, to evaluate ViT-B/16 on CIFAR-100, please run (the `${DATAPATH}` should be replaced with your real path):\r\n```bash\r\nbash run_scripts/zeroshot_eval.sh 0 \\\r\n    ${DATAPATH} cifar-100 \\\r\n    ViT-B-16 RoBERTa-wwm-ext-base-chinese \\\r\n    ${DATAPATH}/pretrained_weights/clip_cn_vit-b-16.pt\r\n```\r\n\r\nTop-1 accuracy will be printed. \r\n```\r\nResult:\r\nzeroshot-top1: 0.6444\r\n```\r\nOn CIFAR-100, the ViT-B/16 model of Chinese-CLIP will achieve the accuracy of 64.4%. For the zero-shot evaluation results of other model scales and other datasets, please refer to [Results.md](https://github.com/OFA-Sys/Chinese-CLIP/blob/master/Results.md#zeroshot_results).\r\n\r\nAlso, a json file will be saved, which serves the submission of ELEVATER. An example of the json file is shown belowï¼š\r\n```json\r\n{\"model_name\": \"CN-CLIP-ViT-B-16\", \"dataset_name\": \"cifar-100\", \"num_trainable_params\": 0, \"num_params\": 188262913, \"num_visual_params\": 86192640, \"num_backbone_params\": 188262913, \"n_shot\": 0, \"rnd_seeds\": [123], \"predictions\": \"prediction probability tensor [size: (1, 10000, 100)]\"}\r\n```\r\nIt includes meta data like the name of model `model_name`, the dataset name `dataset_name`, the number of parameters`num_params`, the number of parameters of vision encoder `num_visual_params`, and also the outputs of the model, namely the predicted probability tensor, whose size is `[1, num_samples, num_labels]`. \r\n\r\n### Zero-Shot Classification Online Demo\r\nBased on the representation generation API which we have integrated into Huggingface transformers, we are able to provide online demos of zero-shot classification task on Huggingface Model HubğŸ¤— for each scale of Chinese-CLIP model. The links are given below:\r\n- [OFA-Sys/chinese-clip-vit-base-patch16](https://huggingface.co/OFA-Sys/chinese-clip-vit-base-patch16)\r\n- [OFA-Sys/chinese-clip-vit-large-patch14](https://huggingface.co/OFA-Sys/chinese-clip-vit-large-patch14)\r\n- [OFA-Sys/chinese-clip-vit-large-patch14-336px](https://huggingface.co/OFA-Sys/chinese-clip-vit-large-patch14-336px)\r\n- [OFA-Sys/chinese-clip-vit-huge-patch14](https://huggingface.co/OFA-Sys/chinese-clip-vit-huge-patch14)\r\n- **ï¼ˆUpdate on 12.10ğŸ”¥ï¼‰**[**New version of demo deployed on Huggingface Spaces**](https://huggingface.co/spaces/OFA-Sys/chinese-clip-zero-shot-image-classification): the 4 model scales above are all gathered into the same demo page, supporting customed prompt template by user. **Welcome to try!**\r\n<br><br><br>\r\n\r\n\r\n# Citation\r\nIf you find the project helpful, please star this project and cite the related articles. Thanks for your support!\r\n\r\n```\r\n@article{chinese-clip,\r\n  title={Chinese CLIP: Contrastive Vision-Language Pretraining in Chinese},\r\n  author={Yang, An and Pan, Junshu and Lin, Junyang and Men, Rui and Zhang, Yichang and Zhou, Jingren and Zhou, Chang},\r\n  journal={arXiv preprint arXiv:2211.01335},\r\n  year={2022}\r\n}\r\n```\r\n"
        },
        {
          "name": "Results.md",
          "type": "blob",
          "size": 6.5703125,
          "content": "**MUGE Text-to-Image Retrieval (Official Validation Set)**:\n<table border=\"1\" width=\"100%\">\n    <tr align=\"center\">\n        <th>Setup</th><th colspan=\"4\">Zero-shot</th><th colspan=\"4\">Finetune</th>\n    </tr>\n    <tr align=\"center\">\n        <td>Metric</td><td>R@1</td><td>R@5</td><td>R@10</td><td>MR</td><td>R@1</td><td>R@5</td><td>R@10</td><td>MR</td>\n    </tr>\n\t<tr align=\"center\">\n        <td width=\"120%\">CN-CLIP<sub>RN50</sub></td><td>42.6</td><td>68.6</td><td>77.9</td><td>63.0</td><td>48.6</td><td>75.1</td><td>84.0</td><td>69.2</td>\n    </tr>  \n\t<tr align=\"center\">\n        <td width=\"120%\">CN-CLIP<sub>ViT-B/16</sub></td><td>52.1</td><td>76.7</td><td>84.4</td><td>71.1</td><td>58.4</td><td>83.6</td><td>90.0</td><td>77.4</td>\n    </tr>\n\t<tr align=\"center\">\n        <td width=\"120%\">CN-CLIP<sub>ViT-L/14</sub></td><td>56.3</td><td>79.8</td><td>86.2</td><td>74.1</td><td>63.3</td><td>85.6</td><td>91.3</td><td>80.1</td>\n    </tr>  \n\t<tr align=\"center\">\n        <td width=\"120%\">CN-CLIP<sub>ViT-L/14@336px</sub></td><td>59.0</td><td>81.4</td><td>87.8</td><td>76.1</td><td>65.3</td><td>86.7</td><td>92.1</td><td>81.3</td>\n    </tr>    \n\t<tr align=\"center\">\n        <td width=\"120%\">CN-CLIP<sub>ViT-H/14</sub></td><td><b>63.0</b></td><td><b>84.1</b></td><td><b>89.2</b></td><td><b>78.8</b></td><td><b>68.9</b></td><td><b>88.7</b></td><td><b>93.1</b></td><td><b>83.6</b></td>\n    </tr>    \n</table>\n<br>\n\n**Flickr30K-CN Retrieval (Official Test Set)**:\n<table border=\"1\" width=\"120%\">\n\t<tr align=\"center\">\n        <th>Task</th><th colspan=\"6\">Text-to-Image</th><th colspan=\"6\">Image-to-Text</th>\n    </tr>\n    <tr align=\"center\">\n        <th>Setup</th><th colspan=\"3\">Zero-shot</th><th colspan=\"3\">Finetune</th><th colspan=\"3\">Zero-shot</th><th colspan=\"3\">Finetune</th>\n    </tr>\n    <tr align=\"center\">\n        <td>Metric</td><td>R@1</td><td>R@5</td><td>R@10</td><td>R@1</td><td>R@5</td><td>R@10</td><td>R@1</td><td>R@5</td><td>R@10</td><td>R@1</td><td>R@5</td><td>R@10</td>\n    </tr>\n\t<tr align=\"center\">\n        <td width=\"120%\">CN-CLIP<sub>RN50</sub></td><td>48.8</td><td>76.0</td><td>84.6</td><td>66.7</td><td>89.4</td><td>94.1</td><td>60.0</td><td>85.9</td><td>92.0</td><td>84.2</td><td>96.7</td><td>98.0</td>\n    </tr>  \n\t<tr align=\"center\">\n        <td width=\"120%\">CN-CLIP<sub>ViT-B/16</sub></td><td>62.7</td><td>86.9</td><td>92.8</td><td>79.1</td><td>94.8</td><td>97.4</td><td>74.6</td><td>93.5</td><td>97.1</td><td>93.5</td><td>99.0</td><td>99.5</td>\n    </tr>  \n\t<tr align=\"center\">\n        <td width=\"120%\">CN-CLIP<sub>ViT-L/14</sub></td><td>68.0</td><td>89.7</td><td>94.4</td><td>82.7</td><td>96.7</td><td>98.6</td><td>80.2</td><td>96.6</td><td>98.2</td><td>96.1</td><td>99.5</td><td>99.9</td>\n    </tr>\n\t<tr align=\"center\">\n        <td width=\"120%\">CN-CLIP<sub>ViT-L/14@336px</sub></td><td>69.0</td><td>90.7</td><td>95.4</td><td><b>84.4</b></td><td><b>97.1</b></td><td><b>98.7</b></td><td><b>83.3</b></td><td>97.2</td><td>98.5</td><td><b>96.6</b></td><td><b>99.8</b></td><td><b>100.0</b></td>\n    </tr>\n\t<tr align=\"center\">\n        <td width=\"120%\">CN-CLIP<sub>ViT-H/14</sub></td><td><b>71.2</b></td><td><b>91.4</b></td><td><b>95.5</b></td><td>83.8</td><td>96.9</td><td>98.6</td><td>81.6</td><td><b>97.5</b></td><td><b>98.8</b></td><td>95.3</td><td>99.7</td><td><b>100.0</b></td>\n    </tr>  \n</table>\n<br>\n\n**COCO-CN Retrieval (Official Test Set)**:\n<table border=\"1\" width=\"120%\">\n\t<tr align=\"center\">\n        <th>Task</th><th colspan=\"6\">Text-to-Image</th><th colspan=\"6\">Image-to-Text</th>\n    </tr>\n    <tr align=\"center\">\n        <th>Setup</th><th colspan=\"3\">Zero-shot</th><th colspan=\"3\">Finetune</th><th colspan=\"3\">Zero-shot</th><th colspan=\"3\">Finetune</th>\n    </tr>\n    <tr align=\"center\">\n        <td>Metric</td><td>R@1</td><td>R@5</td><td>R@10</td><td>R@1</td><td>R@5</td><td>R@10</td><td>R@1</td><td>R@5</td><td>R@10</td><td>R@1</td><td>R@5</td><td>R@10</td>\n    </tr>\n\t<tr align=\"center\">\n        <td width=\"120%\">CN-CLIP<sub>RN50</sub></td><td>48.1</td><td>81.3</td><td>90.5</td><td>66.8</td><td>91.1</td><td>97.0</td><td>51.6</td><td>81.2</td><td>90.5</td><td>68.4</td><td>93.3</td><td>97.8</td>\n    </tr>  \n\t<tr align=\"center\">\n        <td width=\"120%\">CN-CLIP<sub>ViT-B/16</sub></td><td>62.2</td><td>86.6</td><td>94.9</td><td>77.0</td><td>97.1</td><td>99.0</td><td>57.0</td><td>84.1</td><td>93.6</td><td>77.4</td><td>96.2</td><td>98.9</td>\n    </tr>  \n\t<tr align=\"center\">\n        <td width=\"120%\">CN-CLIP<sub>ViT-L/14</sub></td><td>64.0</td><td>89.2</td><td>94.4</td><td>78.9</td><td>96.3</td><td>99.0</td><td>60.4</td><td>84.2</td><td>92.9</td><td>80.2</td><td>96.7</td><td>99.2</td>\n    </tr>\n\t<tr align=\"center\">\n        <td width=\"120%\">CN-CLIP<sub>ViT-L/14@336px</sub></td><td>64.7</td><td>89.6</td><td>94.6</td><td>80.1</td><td>96.7</td><td><b>99.2</b></td><td><b>63.4</b></td><td><b>87.2</b></td><td><b>94.4</b></td><td>81.2</td><td>97.2</td><td>99.1</td>\n    </tr>\n\t<tr align=\"center\">\n        <td width=\"120%\">CN-CLIP<sub>ViT-H/14</sub></td><td><b>69.2</b></td><td><b>89.9</b></td><td><b>96.1</b></td><td><b>81.5</b></td><td><b>96.9</b></td><td>99.1</td><td>63.0</td><td>86.6</td><td>92.9</td><td><b>83.5</b></td><td><b>97.3</b></td><td><b>99.2</b></td>\n    </tr>  \n</table>\n<br>\n\n**Zero-shot Image Classification**:<span id=\"zeroshot_results\"></span>\n<table border=\"1\" width=\"150%\">\n\t<tr align=\"center\">\n        <th>Task</th><th>CIFAR10</th><th>CIFAR100</th><th>DTD</th><th>EuroSAT</th><th>FER</th><th>FGVC</th><th>KITTI</th><th>MNIST</th><th>PC</th><th>VOC</th><th>ImageNet</th>\n    </tr>\n\t<tr align=\"center\">\n        <td width=\"150%\">CN-CLIP<sub>RN50</sub></td><td>72.7</td><td>40.6</td><td>36.9</td><td>27.0</td><td>21.9</td><td>5.4</td><td>30.2</td><td>50.2 </td><td>47.7</td><td>82.1</td><td>33.5</td>\n    </tr>\n    \t<tr align=\"center\">\n        <td width=\"150%\">CN-CLIP<sub>ViT-B/16</sub></td><td>92.0</td><td>64.4</td><td>43.6</td><td>46.9</td><td>47.2</td><td>12.8</td><td>33.5</td><td>67.6 </td><td>54.0</td><td>83.3</td><td>48.3</td>\n    </tr>\n\t<tr align=\"center\">\n        <td width=\"150%\">CN-CLIP<sub>ViT-L/14</sub></td><td>94.9</td><td>75.1</td><td>44.2</td><td>56.9</td><td>54.6</td><td>16.0</td><td>49.9</td><td>69.8 </td><td>63.5</td><td>84.5</td><td>54.7</td>\n    </tr>\n    \t<tr align=\"center\">\n        <td width=\"150%\">CN-CLIP<sub>ViT-L/14@336px</sub></td><td>94.1</td><td>73.5</td><td>43.8</td><td>50.7</td><td>55.1</td><td>17.1</td><td>49.8</td><td>65.0</td><td>62.9</td><td>84.9</td><td>56.7</td>\n    </tr>\n    \t<tr align=\"center\">\n        <td width=\"150%\">CN-CLIP<sub>ViT-H/14</sub></td><td>96.0</td><td>79.7</td><td>51.2</td><td>52.0</td><td>49.2</td><td>26.2</td><td>39.1</td><td>79.4</td><td>52.4</td><td>84.9</td><td>59.6</td>\n    </tr>\n</table>\n<br><br>\n"
        },
        {
          "name": "assets",
          "type": "tree",
          "content": null
        },
        {
          "name": "cn_clip",
          "type": "tree",
          "content": null
        },
        {
          "name": "dataset_transform.py",
          "type": "blob",
          "size": 5.384765625,
          "content": "import pandas as pd\nimport os\nimport base64\nimport json\nfrom PIL import Image\nfrom io import BytesIO\nfrom sklearn.model_selection import train_test_split\n\n'''\noriginal_datasetåŸå§‹æ•°æ®çš„è·¯å¾„æ–‡ä»¶å¤¹ï¼Œéœ€ä¿®æ”¹ä¸ºå®é™…çš„è·¯å¾„\n'''\n\n#è®­ç»ƒå’ŒéªŒè¯é›†æ–‡æœ¬æ•°æ®çš„æ–‡ä»¶\ndata1 = pd.read_csv('original_dataset/data1/ImageWordData.csv')\n#è®­ç»ƒå’ŒéªŒè¯é›†å›¾åƒæ•°æ®çš„ç›®å½•\ndata1_images_folder='original_dataset/data1/ImageData'\n\n# å…ˆå°†æ–‡æœ¬åŠå¯¹åº”å›¾åƒidåˆ’åˆ†åˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›†\ntrain_data, val_data = train_test_split(data1, test_size=0.2, random_state=42)\n\n# åˆ›å»ºå‡½æ•°æ¥å¤„ç†æ•°æ®é›†ï¼Œä½¿æ–‡æœ¬å…³è”åˆ°å…¶å¯¹åº”å›¾åƒidçš„å›¾åƒ\ndef process_train_valid(data, images_folder, img_file, txt_file):\n    with open(img_file, 'w') as f_img, open(txt_file, 'w') as f_txt:\n        for index, row in data.iterrows():\n            # å›¾ç‰‡å†…å®¹éœ€è¦è¢«ç¼–ç ä¸ºbase64æ ¼å¼\n            img_path = os.path.join(images_folder, row['image_id'])\n            with open(img_path, 'rb') as f_img_file:\n                img = Image.open(f_img_file)\n                img_buffer = BytesIO()\n                img.save(img_buffer, format=img.format)\n                byte_data = img_buffer.getvalue()\n                base64_str = base64.b64encode(byte_data).decode(\"utf-8\")\n\n            f_img.write(f\"{row['image_id']}\\t{base64_str}\\n\")\n\n            # æ–‡æœ¬å†…å®¹å’Œå›¾ç‰‡idéœ€è¦è¢«å†™å…¥jsonlæ–‡ä»¶\n            text_data = {\"text_id\": row[\"image_id\"], \"text\": row[\"caption\"], \"image_ids\": [row[\"image_id\"]]}\n            f_txt.write(json.dumps(text_data) + '\\n')\n\n# å¤„ç†è®­ç»ƒé›†å’ŒéªŒè¯é›†\n# datasets/DatasetNameä¸ºåœ¨Chinese-CLIPé¡¹ç›®ç›®å½•ä¸‹æ–°å»ºçš„å­˜æ”¾è½¬æ¢åæ•°æ®é›†çš„æ–‡ä»¶å¤¹\nprocess_train_valid(train_data, data1_images_folder, 'Chinese-CLIP/datasets/DatasetName/train_imgs.tsv', 'Chinese-CLIP/datasets/DatasetName/train_texts.jsonl')\nprocess_train_valid(val_data, data1_images_folder, 'Chinese-CLIP/datasets/DatasetName/valid_imgs.tsv', 'Chinese-CLIP/datasets/DatasetName/valid_texts.jsonl')\n\n\n\n# åˆ¶ä½œä»æ–‡æœ¬åˆ°å›¾åƒï¼ˆText_to_Imageï¼‰æ£€ç´¢æ—¶çš„ï¼Œæµ‹è¯•é›†ã€‚data2ä¸ºText_to_Imageæµ‹è¯•æ•°æ®æ–‡ä»¶å¤¹å\nimage_data2 = pd.read_csv('original_dataset/data2/image_data.csv')\nword_test2 = pd.read_csv('original_dataset/data2/word_test.csv')\n# åŸå§‹å›¾åƒæµ‹è¯•é›†ç›®å½•\ndata2_images_folder='original_dataset/data2/ImageData'\n\n# å¤„ç†Text_to_Imageæµ‹è¯•é›†\ndef process_text_to_image(image_data, images_folder, word_test, img_file, txt_file):\n    with open(img_file, 'w') as f_img, open(txt_file, 'w') as f_txt:\n        for index, row in image_data.iterrows():\n            # å›¾ç‰‡å†…å®¹éœ€è¦è¢«ç¼–ç ä¸ºbase64æ ¼å¼\n            img_path = os.path.join(images_folder, row['image_id'])\n            with open(img_path, 'rb') as f_img_file:\n                img = Image.open(f_img_file)\n                img_buffer = BytesIO()\n                img.save(img_buffer, format=img.format)\n                byte_data = img_buffer.getvalue()\n                base64_str = base64.b64encode(byte_data).decode(\"utf-8\")\n\n            f_img.write(f\"{row['image_id']}\\t{base64_str}\\n\")\n\n        for index, row in word_test.iterrows():\n            # æ–‡æœ¬å†…å®¹å’Œå›¾ç‰‡idéœ€è¦è¢«å†™å…¥jsonlæ–‡ä»¶\n            text_data = {\"text_id\": row[\"text_id\"], \"text\": row[\"caption\"], \"image_ids\": []}\n            f_txt.write(json.dumps(text_data) + '\\n')\n\n# datasets/DatasetNameä¸ºåœ¨Chinese-CLIPé¡¹ç›®ç›®å½•ä¸‹æ–°å»ºçš„å­˜æ”¾è½¬æ¢åæ•°æ®é›†çš„æ–‡ä»¶å¤¹\nprocess_text_to_image(image_data2, data2_images_folder, word_test2, 'Chinese-CLIP/datasets/DatasetName/test2_imgs.tsv', 'Chinese-CLIP/datasets/DatasetName/test2_texts.jsonl')\n\n\n\n# åˆ¶ä½œä»å›¾åƒåˆ°æ–‡æœ¬ï¼ˆImage_to_Textï¼‰æ£€ç´¢æ—¶çš„ï¼Œæµ‹è¯•é›†ã€‚data3ä¸ºImage_to_Textæµ‹è¯•æ•°æ®æ–‡ä»¶å¤¹å\nimage_test3 = pd.read_csv('original_dataset/data3/image_test.csv')\nword_data3 = pd.read_csv('original_dataset/data3/word_data.csv')\n# åŸå§‹å›¾åƒæµ‹è¯•é›†ç›®å½•\ndata3_images_folder='original_dataset/data3/ImageData'\n\n# å¤„ç†Image_to_Textæµ‹è¯•é›†é›†\ndef process_image_to_text(image_data, images_folder, word_test, img_file, txt_file):\n    with open(img_file, 'w') as f_img, open(txt_file, 'w') as f_txt:\n        for index, row in image_data.iterrows():\n            # å›¾ç‰‡å†…å®¹éœ€è¦è¢«ç¼–ç ä¸ºbase64æ ¼å¼\n            img_path = os.path.join(images_folder, row['image_id'])\n            with open(img_path, 'rb') as f_img_file:\n                img = Image.open(f_img_file)\n                img_buffer = BytesIO()\n                img.save(img_buffer, format=img.format)\n                byte_data = img_buffer.getvalue()\n                base64_str = base64.b64encode(byte_data).decode(\"utf-8\")\n\n            f_img.write(f\"{row['image_id']}\\t{base64_str}\\n\")\n\n        for index, row in word_test.iterrows():\n            # æ–‡æœ¬å†…å®¹å’Œå›¾ç‰‡idéœ€è¦è¢«å†™å…¥jsonlæ–‡ä»¶\n            text_data = {\"text_id\": row[\"text_id\"], \"text\": row[\"caption\"], \"image_ids\": []}\n            f_txt.write(json.dumps(text_data) + '\\n')\n\n# datasets/DatasetNameä¸ºåœ¨Chinese-CLIPé¡¹ç›®ç›®å½•ä¸‹æ–°å»ºçš„å­˜æ”¾è½¬æ¢åæ•°æ®é›†çš„æ–‡ä»¶å¤¹\nprocess_image_to_text(image_test3, data3_images_folder, word_data3, 'Chinese-CLIP/datasets/DatasetName/test3_imgs.tsv', 'Chinese-CLIP/datasets/DatasetName/test3_texts.jsonl')\n\n\n'''\nåˆ™å°†tsvå’Œjsonlæ–‡ä»¶ä¸€èµ·åºåˆ—åŒ–ï¼Œè½¬æ¢ä¸ºå†…å­˜ç´¢å¼•çš„LMDBæ•°æ®åº“æ–‡ä»¶çš„å‘½ä»¤å¦‚ä¸‹ï¼š\npython ./Chinese-CLIP/cn_clip/preprocess/build_lmdb_dataset.py --data_dir Chinese-CLIP/datasets/DatasetName --splits train,valid,test2,test3\n'''\n\n"
        },
        {
          "name": "deployment.md",
          "type": "blob",
          "size": 25.0244140625,
          "content": "[**ä¸­æ–‡è¯´æ˜**](deployment.md) | [**English**](deployment_En.md)\n\n# Chinese-CLIPæ¨¡å‹éƒ¨ç½²ï¼šONNX & TensorRTæ ¼å¼è½¬æ¢\n\næœ€æ–°çš„Chinese-CLIPä»£ç ï¼Œå·²æ”¯æŒå°†å„è§„æ¨¡çš„Pytorchæ¨¡å‹ï¼Œè½¬æ¢ä¸º[ONNX](https://onnx.ai/)æˆ–[TensorRT](https://developer.nvidia.com/tensorrt)æ ¼å¼ï¼Œä»è€Œç›¸æ¯”åŸå§‹Pytorchæ¨¡å‹ **[æå‡ç‰¹å¾è®¡ç®—çš„æ¨ç†é€Ÿåº¦](#é€Ÿåº¦å¯¹æ¯”ç»“æœ)**ï¼ŒåŒæ—¶ä¸å½±å“ç‰¹å¾æå–çš„ä¸‹æ¸¸ä»»åŠ¡æ•ˆæœã€‚ä¸‹é¢æˆ‘ä»¬ç»™å‡ºåœ¨GPUä¸Šï¼Œå‡†å¤‡ONNXå’ŒTensorRTæ ¼å¼çš„FP16 Chinese-CLIPéƒ¨ç½²æ¨¡å‹çš„æ•´ä¸ªæµç¨‹ï¼ˆåŒæ—¶ç»™å‡ºäº†Chinese-CLIPé¢„è®­ç»ƒTensorRTæ¨¡å‹çš„[ä¸‹è½½æ–¹å¼](#tensorrt_download)ï¼‰ï¼Œå¹¶é™„ä¸Šæ¨¡å‹æ•ˆæœå’Œæ¨ç†é€Ÿåº¦çš„å¯¹æ¯”ï¼Œæ–¹ä¾¿å¤§å®¶ä¸Šæ‰‹åˆ©ç”¨ONNXå’ŒTensorRTåº“åœ¨æ¨ç†æ€§èƒ½ä¸Šçš„ä¼˜åŠ¿ã€‚\n\n## ç¯å¢ƒå‡†å¤‡\n\n+ **GPUç¡¬ä»¶è¦æ±‚**ï¼šè¯·å‡†å¤‡**Voltaæ¶æ„åŠä»¥ä¸Š**çš„Nvidia GPUæ˜¾å¡ï¼ˆé…å¤‡FP16 Tensor Coreï¼‰ï¼ŒNvidiaå„æ¶æ„å¯¹åº”æ˜¾å¡å‹å·è¯·å‚è§[æ­¤æ–‡æ¡£è¡¨æ ¼](https://en.wikipedia.org/wiki/CUDA#GPUs_supported)ã€‚æœ¬æ–‡æˆ‘ä»¬ä»¥T4æ˜¾å¡ä¸ºä¾‹\n+ **CUDA**ï¼šæ¨è[CUDA](https://developer.nvidia.com/cuda-11-6-0-download-archive)ç‰ˆæœ¬11.6åŠä»¥ä¸Šï¼Œæœ¬æ–‡ä»¥11.6ä¸ºä¾‹\n+ **CUDNN**ï¼šæ¨è[CUDNN](https://developer.nvidia.com/rdp/cudnn-archive) 8.6.0åŠä»¥ä¸Šï¼Œæœ¬æ–‡ä»¥8.6.0ä¸ºä¾‹ã€‚è¯·æ³¨æ„TensorRTå’ŒCUDNNæœ‰ç‰ˆæœ¬matchå…³ç³»ï¼Œå¦‚TensorRT 8.5.xå¿…é¡»ä½¿ç”¨CUDNN 8.6.0ï¼Œè¯¦è§TensorRTçš„ç‰ˆæœ¬è¦æ±‚\n+ **ONNX**ï¼šæ³¨æ„æˆ‘ä»¬è½¬æ¢TensorRTæ¨¡å‹æ—¶ï¼Œå°†æ²¿ç€Pytorch â†’ ONNX â†’ TensorRTçš„æ­¥éª¤ï¼Œæ‰€ä»¥å‡†å¤‡TensorRTæ¨¡å‹ä¹Ÿéœ€è¦å…ˆå®‰è£…ONNXåº“ã€‚æœ¬æ–‡ä»¥onnxç‰ˆæœ¬1.13.0ï¼Œonnxruntime-gpuç‰ˆæœ¬1.13.1ï¼Œonnxmltoolsç‰ˆæœ¬1.11.1ä¸ºä¾‹\n+ **TensorRT**ï¼šæ¨è[TensorRT](https://docs.nvidia.com/deeplearning/tensorrt/archives/index.html#trt_8)ç‰ˆæœ¬8.5.xï¼Œæœ¬æ–‡ä»¥8.5.2.2ä¸ºä¾‹ã€‚TensorRTå„ç‰ˆæœ¬å¯¹åº”çš„CUDNNåŒ¹é…ç‰ˆæœ¬ï¼Œè¯·ä»[æ–‡æ¡£é¡µé¢](https://docs.nvidia.com/deeplearning/tensorrt/archives/index.html#trt_8)ï¼ŒæŸ¥é˜…æ­¤TensorRTç‰ˆæœ¬çš„\"NVIDIA TensorRT Support Matrix\"\n+ **Pytorch**ï¼šæ¨è1.12.1åŠä»¥ä¸Šï¼Œæœ¬æ–‡ä»¥1.12.1ä¸ºä¾‹ï¼ˆå»ºè®®ç›´æ¥pipå®‰è£…1.12.1+cu116ï¼Œç¯å¢ƒå°½é‡ä¸è¦å†ä½¿ç”¨condaå®‰è£…cudatoolkitï¼Œé¿å…ç¯å¢ƒCUDNNç‰ˆæœ¬å˜åŒ–ï¼Œå¯¼è‡´TensorRTæŠ¥é”™ï¼‰\n+ [requirements.txt](requirements.txt)è¦æ±‚çš„å…¶ä»–ä¾èµ–é¡¹\n\næ‰§è¡Œä»£ç \n``` \npip install tensorrt==8.5.2.2 onnx==1.13.0 onnxruntime-gpu==1.13.1 onnxmltools==1.11.1\npip install torch==1.12.1+cu116 torchvision==0.13.1+cu116 torchaudio==0.12.1 -f https://download.pytorch.org/whl/torch_stable.html\npip install -r requirements.txt \n```\n\n## è½¬æ¢å’Œè¿è¡ŒONNXæ¨¡å‹\n\n### è½¬æ¢æ¨¡å‹\n\nå°†Pytorchæ¨¡å‹checkpointè½¬æ¢ä¸ºONNXæ ¼å¼çš„ä»£ç ï¼Œè¯·å‚è§`cn_clip/deploy/pytorch_to_onnx.py`ã€‚æˆ‘ä»¬ä»¥è½¬æ¢ViT-B-16è§„æ¨¡çš„Chinese-CLIPé¢„è®­ç»ƒæ¨¡å‹ä¸ºä¾‹ï¼Œå…·ä½“çš„ä»£ç è¿è¡Œæ–¹å¼å¦‚ä¸‹ï¼ˆè¯·å‚è€ƒReadme[ä»£ç ç»„ç»‡éƒ¨åˆ†](https://github.com/OFA-Sys/Chinese-CLIP#ä»£ç ç»„ç»‡)å»ºå¥½`${DATAPATH}`å¹¶æ›¿æ¢ä¸‹é¢çš„è„šæœ¬å†…å®¹ï¼Œå°½é‡ä½¿ç”¨ç›¸å¯¹è·¯å¾„ï¼‰ï¼š\n\n```bash\ncd Chinese-CLIP/\nexport CUDA_VISIBLE_DEVICES=0\nexport PYTHONPATH=${PYTHONPATH}:`pwd`/cn_clip\n\n# ${DATAPATH}çš„æŒ‡å®šï¼Œè¯·å‚è€ƒReadme\"ä»£ç ç»„ç»‡\"éƒ¨åˆ†åˆ›å»ºå¥½ç›®å½•ï¼Œå°½é‡ä½¿ç”¨ç›¸å¯¹è·¯å¾„ï¼šhttps://github.com/OFA-Sys/Chinese-CLIP#ä»£ç ç»„ç»‡\ncheckpoint_path=${DATAPATH}/pretrained_weights/clip_cn_vit-b-16.pt # æŒ‡å®šè¦è½¬æ¢çš„ckptå®Œæ•´è·¯å¾„\nmkdir -p ${DATAPATH}/deploy/ # åˆ›å»ºONNXæ¨¡å‹çš„è¾“å‡ºæ–‡ä»¶å¤¹\n\npython cn_clip/deploy/pytorch_to_onnx.py \\\n       --model-arch ViT-B-16 \\\n       --pytorch-ckpt-path ${checkpoint_path} \\\n       --save-onnx-path ${DATAPATH}/deploy/vit-b-16 \\\n       --convert-text --convert-vision\n```\n\nå…¶ä¸­å„é…ç½®é¡¹å®šä¹‰å¦‚ä¸‹ï¼š\n+ `model-arch`: æ¨¡å‹è§„æ¨¡ï¼Œé€‰é¡¹åŒ…æ‹¬`[\"RN50\", \"ViT-B-16\", \"ViT-L-14\", \"ViT-L-14-336\", \"ViT-H-14\"]`ï¼Œå„è§„æ¨¡ç»†èŠ‚è¯¦è§[Readme](https://github.com/OFA-Sys/Chinese-CLIP#æ¨¡å‹è§„æ¨¡--ä¸‹è½½é“¾æ¥)\n+ `pytorch-ckpt-path`: æŒ‡å®šPytorchæ¨¡å‹ckptè·¯å¾„ï¼Œä¸Šé¢çš„ä»£ç ç¤ºä¾‹ä¸­æˆ‘ä»¬æŒ‡å®šä¸ºé¢„è®­ç»ƒçš„ckptè·¯å¾„ï¼Œä¹Ÿå¯ä»¥æŒ‡å®šä¸ºç”¨æˆ·finetune ckptçš„ä½ç½®ã€‚ckptä¸­çš„å‚æ•°éœ€è¦ä¸`model-arch`æŒ‡å®šçš„æ¨¡å‹è§„æ¨¡å¯¹åº”\n+ `save-onnx-path`: æŒ‡å®šè¾“å‡ºONNXæ ¼å¼æ¨¡å‹çš„è·¯å¾„ï¼ˆå‰ç¼€ï¼‰ã€‚å®Œæˆè½¬æ¢åï¼Œä»£ç å°†åˆ†åˆ«è¾“å‡ºæ–‡æœ¬ä¾§å’Œå›¾åƒä¾§çš„ONNXæ ¼å¼ç¼–ç æ¨¡å‹æ–‡ä»¶ï¼ŒFP32ä¸FP16å„ä¸€ç‰ˆï¼Œè¯¥å‚æ•°å³æŒ‡å®šäº†ä»¥ä¸Šè¾“å‡ºæ–‡ä»¶çš„è·¯å¾„å‰ç¼€\n+ `convert-text`å’Œ`convert-vision`: æŒ‡å®šæ˜¯å¦è½¬æ¢æ–‡æœ¬ä¾§å’Œå›¾åƒä¾§æ¨¡å‹\n+ `context-length`ï¼ˆå¯é€‰ï¼‰: æŒ‡å®šæ–‡æœ¬ä¾§ONNXæ¨¡å‹ï¼Œæ¥æ”¶è¾“å…¥çš„åºåˆ—é•¿åº¦ï¼Œé»˜è®¤ä¸ºæˆ‘ä»¬é¢„è®­ç»ƒckptæ‰€ä½¿ç”¨çš„52\n+ `download-root`ï¼ˆå¯é€‰ï¼‰: å¦‚æœä¸æŒ‡å®š`pytorch-ckpt-path`ï¼Œä»£ç å°†æ ¹æ®`model-arch`è‡ªåŠ¨ä¸‹è½½Chinese-CLIPå®˜æ–¹é¢„è®­ç»ƒckptç”¨äºè½¬æ¢ï¼Œå­˜æ”¾äº`download-root`æŒ‡å®šçš„ç›®å½•\n\nè¿è¡Œæ­¤ä»£ç è½¬æ¢å®Œæˆåï¼Œå°†å¾—åˆ°ä»¥ä¸‹çš„logè¾“å‡ºï¼š\n```\nFinished PyTorch to ONNX conversion...\n>>> The text FP32 ONNX model is saved at ${DATAPATH}/deploy/vit-b-16.txt.fp32.onnx\n>>> The text FP16 ONNX model is saved at ${DATAPATH}/deploy/vit-b-16.txt.fp16.onnx with extra file ${DATAPATH}/deploy/vit-b-16.txt.fp16.onnx.extra_file\n>>> The vision FP32 ONNX model is saved at ${DATAPATH}/deploy/vit-b-16.img.fp32.onnx\n>>> The vision FP16 ONNX model is saved at ${DATAPATH}/deploy/vit-b-16.img.fp16.onnx with extra file ${DATAPATH}/deploy/vit-b-16.img.fp16.onnx.extra_file\n```\n\nä¸Šé¢ç¤ºä¾‹ä»£ç æ‰§è¡Œç»“æŸåï¼Œæˆ‘ä»¬å¾—åˆ°äº†ViT-B-16è§„æ¨¡ï¼ŒChinese-CLIPæ–‡æœ¬ä¾§å’Œå›¾åƒä¾§çš„ONNXæ¨¡å‹ï¼Œå¯ä»¥åˆ†åˆ«ç”¨äºæå–å›¾æ–‡ç‰¹å¾ã€‚è¾“å‡ºONNXæ¨¡å‹çš„è·¯å¾„å‡ä»¥è¿è¡Œè„šæœ¬æ—¶çš„`save-onnx-path`ä¸ºå‰ç¼€ï¼Œåé¢ä¾æ¬¡æ‹¼ä¸Š`.img`/`.txt`ã€`.fp16`/`.fp32`ã€`.onnx`ã€‚æˆ‘ä»¬åç»­å°†ä¸»è¦ä½¿ç”¨FP16æ ¼å¼çš„ONNXæ¨¡å‹`vit-b-16.txt.fp16.onnx`å’Œ`vit-b-16.img.fp16.onnx`\n\næ³¨æ„åˆ°éƒ¨åˆ†ONNXæ¨¡å‹æ–‡ä»¶è¿˜é™„å¸¦æœ‰ä¸€ä¸ªextra_fileï¼Œå…¶ä¹Ÿæ˜¯å¯¹åº”ONNXæ¨¡å‹çš„ä¸€éƒ¨åˆ†ã€‚åœ¨ä½¿ç”¨è¿™äº›ONNXæ¨¡å‹æ—¶ï¼Œç”±äº`.onnx`æ–‡ä»¶å­˜å‚¨äº†extra_fileçš„è·¯å¾„ï¼ˆå¦‚`${DATAPATH}/deploy/vit-b-16.txt.fp16.onnx.extra_file`ï¼‰å¹¶ä¼šæ ¹æ®æ­¤è·¯å¾„è½½å…¥extra_fileï¼Œæ‰€ä»¥ä½¿ç”¨ONNXæ¨¡å‹è¯·ä¸è¦æ”¹åŠ¨å­˜æ”¾çš„è·¯å¾„åï¼Œè½¬æ¢æ—¶`${DATAPATH}`ä¹Ÿå°½é‡ç”¨ç›¸å¯¹è·¯å¾„ï¼ˆå¦‚`../datapath`ï¼‰ï¼Œé¿å…è¿è¡Œæ—¶æŒ‰è·¯å¾„æ‰¾ä¸åˆ°extra_fileæŠ¥é”™\n\n### è¿è¡Œæ¨¡å‹\n\n#### æå–å›¾åƒä¾§ç‰¹å¾\næˆ‘ä»¬åœ¨`Chinese-CLIP/`ç›®å½•ä¸‹ï¼Œä½¿ç”¨ä»¥ä¸‹çš„ç¤ºä¾‹ä»£ç ï¼Œè¯»å–åˆšåˆšè½¬æ¢å¥½çš„ViT-B-16è§„æ¨¡ONNXå›¾åƒä¾§æ¨¡å‹`vit-b-16.img.fp16.onnx`ï¼Œå¹¶ä¸ºReadmeä¸­ç¤ºä¾‹çš„[çš®å¡ä¸˜å›¾ç‰‡](examples/pokemon.jpeg)æå–å›¾åƒä¾§ç‰¹å¾ã€‚æ³¨æ„è½¬æ¢å¥½çš„ONNXæ¨¡å‹åªæ¥å—batchå¤§å°ä¸º1çš„è¾“å…¥ï¼Œå³ä¸€æ¬¡è°ƒç”¨åªå¤„ç†ä¸€å¼ è¾“å…¥å›¾ç‰‡\n\n```python\n# å®Œæˆå¿…è¦çš„importï¼ˆä¸‹æ–‡çœç•¥ï¼‰\nimport onnxruntime\nfrom PIL import Image\nimport numpy as np\nimport torch\nimport argparse\nimport cn_clip.clip as clip\nfrom clip import load_from_name, available_models\nfrom clip.utils import _MODELS, _MODEL_INFO, _download, available_models, create_model, image_transform\n\n# è½½å…¥ONNXå›¾åƒä¾§æ¨¡å‹ï¼ˆ**è¯·æ›¿æ¢${DATAPATH}ä¸ºå®é™…çš„è·¯å¾„**ï¼‰\nimg_sess_options = onnxruntime.SessionOptions()\nimg_run_options = onnxruntime.RunOptions()\nimg_run_options.log_severity_level = 2\nimg_onnx_model_path=\"${DATAPATH}/deploy/vit-b-16.img.fp16.onnx\"\nimg_session = onnxruntime.InferenceSession(img_onnx_model_path,\n                                        sess_options=img_sess_options,\n                                        providers=[\"CUDAExecutionProvider\"])\n\n# é¢„å¤„ç†å›¾ç‰‡\nmodel_arch = \"ViT-B-16\" # è¿™é‡Œæˆ‘ä»¬ä½¿ç”¨çš„æ˜¯ViT-B-16è§„æ¨¡ï¼Œå…¶ä»–è§„æ¨¡è¯·å¯¹åº”ä¿®æ”¹\npreprocess = image_transform(_MODEL_INFO[model_arch]['input_resolution'])\n# ç¤ºä¾‹çš®å¡ä¸˜å›¾ç‰‡ï¼Œé¢„å¤„ç†åå¾—åˆ°[1, 3, åˆ†è¾¨ç‡, åˆ†è¾¨ç‡]å°ºå¯¸çš„Torch Tensor\nimage = preprocess(Image.open(\"examples/pokemon.jpeg\")).unsqueeze(0)\n\n# ç”¨ONNXæ¨¡å‹è®¡ç®—å›¾åƒä¾§ç‰¹å¾\nimage_features = img_session.run([\"unnorm_image_features\"], {\"image\": image.cpu().numpy()})[0] # æœªå½’ä¸€åŒ–çš„å›¾åƒç‰¹å¾\nimage_features = torch.tensor(image_features)\nimage_features /= image_features.norm(dim=-1, keepdim=True) # å½’ä¸€åŒ–åçš„Chinese-CLIPå›¾åƒç‰¹å¾ï¼Œç”¨äºä¸‹æ¸¸ä»»åŠ¡\nprint(image_features.shape) # Torch Tensor shape: [1, ç‰¹å¾å‘é‡ç»´åº¦]\n```\n\n#### æå–æ–‡æœ¬ä¾§ç‰¹å¾\n\nç±»ä¼¼åœ°ï¼Œæˆ‘ä»¬ç”¨å¦‚ä¸‹ä»£ç å®Œæˆæ–‡æœ¬ä¾§ONNXæ¨¡å‹çš„è½½å…¥ä¸ç‰¹å¾è®¡ç®—ï¼Œä¸å›¾åƒä¾§ç›¸åŒï¼Œæ–‡æœ¬ä¾§ONNXéƒ¨ç½²æ¨¡å‹åªæ¥å—batchå¤§å°ä¸º1çš„è¾“å…¥ï¼Œå³ä¸€æ¬¡è°ƒç”¨åªå¤„ç†ä¸€æ¡è¾“å…¥æ–‡æœ¬ã€‚æˆ‘ä»¬ä¸º4æ¡å€™é€‰æ–‡æœ¬ä¾æ¬¡è®¡ç®—ViT-B-16è§„æ¨¡æ¨¡å‹çš„æ–‡æœ¬ç‰¹å¾ã€‚importç›¸å…³ä»£ç ä¸ä¸Šæ–‡ç›¸åŒï¼Œè¿™é‡Œçœç•¥ï¼š\n\n```python\n# è½½å…¥ONNXæ–‡æœ¬ä¾§æ¨¡å‹ï¼ˆ**è¯·æ›¿æ¢${DATAPATH}ä¸ºå®é™…çš„è·¯å¾„**ï¼‰\ntxt_sess_options = onnxruntime.SessionOptions()\ntxt_run_options = onnxruntime.RunOptions()\ntxt_run_options.log_severity_level = 2\ntxt_onnx_model_path=\"${DATAPATH}/deploy/vit-b-16.txt.fp16.onnx\"\ntxt_session = onnxruntime.InferenceSession(txt_onnx_model_path,\n                                        sess_options=txt_sess_options,\n                                        providers=[\"CUDAExecutionProvider\"])\n\n# ä¸º4æ¡è¾“å…¥æ–‡æœ¬è¿›è¡Œåˆ†è¯ã€‚åºåˆ—é•¿åº¦æŒ‡å®šä¸º52ï¼Œéœ€è¦å’Œè½¬æ¢ONNXæ¨¡å‹æ—¶ä¿æŒä¸€è‡´ï¼ˆå‚è§è½¬æ¢æ—¶çš„context-lengthå‚æ•°ï¼‰\ntext = clip.tokenize([\"æ°å°¼é¾Ÿ\", \"å¦™è›™ç§å­\", \"å°ç«é¾™\", \"çš®å¡ä¸˜\"], context_length=52) \n\n# ç”¨ONNXæ¨¡å‹ä¾æ¬¡è®¡ç®—æ–‡æœ¬ä¾§ç‰¹å¾\ntext_features = []\nfor i in range(len(text)):\n    one_text = np.expand_dims(text[i].cpu().numpy(),axis=0)\n    text_feature = txt_session.run([\"unnorm_text_features\"], {\"text\":one_text})[0] # æœªå½’ä¸€åŒ–çš„æ–‡æœ¬ç‰¹å¾\n    text_feature = torch.tensor(text_feature)\n    text_features.append(text_feature)\ntext_features = torch.squeeze(torch.stack(text_features),dim=1) # 4ä¸ªç‰¹å¾å‘é‡stackåˆ°ä¸€èµ·\ntext_features = text_features / text_features.norm(dim=1, keepdim=True) # å½’ä¸€åŒ–åçš„Chinese-CLIPæ–‡æœ¬ç‰¹å¾ï¼Œç”¨äºä¸‹æ¸¸ä»»åŠ¡\nprint(text_features.shape) # Torch Tensor shape: [4, ç‰¹å¾å‘é‡ç»´åº¦]\n```\n\n#### è®¡ç®—å›¾æ–‡ç›¸ä¼¼åº¦\n\nONNXæ¨¡å‹äº§å‡ºçš„å½’ä¸€åŒ–å›¾æ–‡ç‰¹å¾ï¼Œå†…ç§¯åsoftmaxå³å¯è®¡ç®—ç›¸ä¼¼åº¦ï¼ˆéœ€è¦è€ƒè™‘`logit_scale`ï¼‰ï¼Œä¸åŸå§‹Pytorchæ¨¡å‹æ“ä½œç›¸åŒï¼Œå‚è§ä»¥ä¸‹ä»£ç ã€‚importç›¸å…³ä»£ç ä¸ä¸Šæ–‡ç›¸åŒï¼Œè¿™é‡Œçœç•¥ï¼š\n\n```python\n# å†…ç§¯åsoftmax\n# æ³¨æ„åœ¨å†…ç§¯è®¡ç®—æ—¶ï¼Œç”±äºå¯¹æ¯”å­¦ä¹ è®­ç»ƒæ—¶æœ‰temperatureçš„æ¦‚å¿µ\n# éœ€è¦ä¹˜ä¸Šæ¨¡å‹logit_scale.exp()ï¼Œæˆ‘ä»¬çš„é¢„è®­ç»ƒæ¨¡å‹logit_scaleå‡ä¸º4.6052ï¼Œæ‰€ä»¥è¿™é‡Œä¹˜ä»¥100\n# å¯¹äºç”¨æˆ·è‡ªå·±çš„ckptï¼Œè¯·ä½¿ç”¨torch.loadè½½å…¥åï¼ŒæŸ¥çœ‹ckpt['state_dict']['module.logit_scale']æˆ–ckpt['state_dict']['logit_scale']\nlogits_per_image = 100 * image_features @ text_features.t()\nprint(logits_per_image.softmax(dim=-1)) # å›¾æ–‡ç›¸ä¼¼æ¦‚ç‡: [[1.2252e-03, 5.2874e-02, 6.7116e-04, 9.4523e-01]]\n```\n\nå¯ä»¥çœ‹åˆ°ï¼Œç»™å‡ºçš„å›¾æ–‡ç›¸ä¼¼æ¦‚ç‡ï¼Œå’Œ[Readmeä¸­å¿«é€Ÿä½¿ç”¨éƒ¨åˆ†](https://github.com/OFA-Sys/Chinese-CLIP#apiå¿«é€Ÿä¸Šæ‰‹)ï¼ŒåŸºäºPytorchåŒä¸€ä¸ªæ¨¡å‹è®¡ç®—çš„ç»“æœåŸºæœ¬ä¸€è‡´ï¼Œè¯æ˜äº†ONNXæ¨¡å‹ç‰¹å¾è®¡ç®—çš„æ­£ç¡®æ€§ï¼Œè€Œ**ONNXæ¨¡å‹çš„ç‰¹å¾è®¡ç®—é€Ÿåº¦ç›¸æ¯”Pytorchæ›´æœ‰ä¼˜åŠ¿**ï¼ˆè¯¦è§[ä¸‹æ–‡](#é€Ÿåº¦å¯¹æ¯”ç»“æœ)ï¼‰ã€‚\n\n## è½¬æ¢å’Œè¿è¡ŒTensorRTæ¨¡å‹\n\n### è½¬æ¢æ¨¡å‹\n\nç›¸æ¯”ONNXæ¨¡å‹ï¼ŒTensorRTæ¨¡å‹å…·æœ‰æ›´å¿«çš„æ¨ç†é€Ÿåº¦ï¼Œæˆ‘ä»¬æä¾›äº†è½¬æ¢å¥½çš„Chinese-CLIPé¢„è®­ç»ƒTensorRTå›¾åƒä¾§å’Œæ–‡æœ¬ä¾§æ¨¡å‹ï¼ˆ[ä¸‹è½½æ–¹å¼](#tensorrt_download)ï¼‰ã€‚å¦‚å‰æ–‡æ‰€è¯´ï¼Œæˆ‘ä»¬å‡†å¤‡TensorRTæ ¼å¼æ¨¡å‹ï¼Œæ˜¯ç”¨åˆšåˆšå¾—åˆ°çš„ONNXæ ¼å¼æ¨¡å‹è¿›ä¸€æ­¥è½¬åŒ–è€Œæ¥ã€‚å°†ONNXè½¬æ¢ä¸ºTensorRTæ ¼å¼çš„ä»£ç ï¼Œè¯·å‚è§`cn_clip/deploy/onnx_to_tensorrt.py`ã€‚ä»ç„¶ä»¥ViT-B-16è§„æ¨¡ä¸ºä¾‹ï¼Œåˆ©ç”¨åˆšåˆšå¾—åˆ°çš„ONNXæ¨¡å‹`vit-b-16.txt.fp16.onnx`å’Œ`vit-b-16.img.fp16.onnx`ï¼Œåœ¨`Chinese-CLIP/`ä¸‹è¿è¡Œå¦‚ä¸‹ä»£ç ï¼š\n\n```bash\nexport PYTHONPATH=${PYTHONPATH}:`pwd`/cn_clip\n# å¦‚å‰æ–‡ï¼Œ${DATAPATH}è¯·æ ¹æ®å®é™…æƒ…å†µæ›¿æ¢\npython cn_clip/deploy/onnx_to_tensorrt.py \\\n       --model-arch ViT-B-16 \\\n       --convert-text \\\n       --text-onnx-path ${DATAPATH}/deploy/vit-b-16.txt.fp16.onnx \\\n       --convert-vision \\\n       --vision-onnx-path ${DATAPATH}/deploy/vit-b-16.img.fp16.onnx \\\n       --save-tensorrt-path ${DATAPATH}/deploy/vit-b-16 \\\n       --fp16\n```\n\nå…¶ä¸­å„é…ç½®é¡¹å®šä¹‰å¦‚ä¸‹ï¼š\n+ `model-arch`: æ¨¡å‹è§„æ¨¡ï¼Œé€‰é¡¹åŒ…æ‹¬`[\"RN50\", \"ViT-B-16\", \"ViT-L-14\", \"ViT-L-14-336\", \"ViT-H-14\"]`ï¼Œå„è§„æ¨¡ç»†èŠ‚è¯¦è§[Readme](https://github.com/OFA-Sys/Chinese-CLIP#æ¨¡å‹è§„æ¨¡--ä¸‹è½½é“¾æ¥)\n+ `convert-text`å’Œ`convert-vision`: æŒ‡å®šæ˜¯å¦è½¬æ¢æ–‡æœ¬ä¾§å’Œå›¾åƒä¾§æ¨¡å‹\n+ `text-onnx-path`: æŒ‡å®šæ–‡æœ¬ä¾§ONNXæ¨¡å‹æ–‡ä»¶è·¯å¾„ï¼Œéœ€è¦ä¸`model-arch`æŒ‡å®šçš„æ¨¡å‹è§„æ¨¡å¯¹åº”\n+ `vision-onnx-path`: æŒ‡å®šå›¾åƒä¾§ONNXæ¨¡å‹æ–‡ä»¶è·¯å¾„ï¼Œéœ€è¦ä¸`model-arch`æŒ‡å®šçš„æ¨¡å‹è§„æ¨¡å¯¹åº”\n+ `save-tensorrt-path`: æŒ‡å®šè¾“å‡ºTensorRTæ ¼å¼æ¨¡å‹çš„è·¯å¾„ï¼ˆå‰ç¼€ï¼‰ã€‚å®Œæˆè½¬æ¢åï¼Œä»£ç å°†åˆ†åˆ«è¾“å‡ºæ–‡æœ¬ä¾§å’Œå›¾åƒä¾§çš„TensorRTæ ¼å¼ç¼–ç æ¨¡å‹æ–‡ä»¶ï¼Œè¯¥å‚æ•°å³æŒ‡å®šäº†ä»¥ä¸Šè¾“å‡ºæ–‡ä»¶çš„è·¯å¾„å‰ç¼€\n+ `fp16`: æŒ‡å®šè½¬æ¢FP16ç²¾åº¦çš„TensorRTæ ¼å¼æ¨¡å‹\n\næ•´ä¸ªè¿‡ç¨‹æ ¹æ®æ¨¡å‹è§„æ¨¡ä¸åŒï¼Œè€—æ—¶å‡ åˆ†é’Ÿåˆ°åå‡ åˆ†é’Ÿä¸ç­‰ã€‚è¿è¡Œæ­¤ä»£ç è½¬æ¢å®Œæˆåï¼Œå°†å¾—åˆ°ä»¥ä¸‹çš„logè¾“å‡ºï¼š\n```\nFinished ONNX to TensorRT conversion...\n>>> The text FP16 TensorRT model is saved at ${DATAPATH}/deploy/vit-b-16.txt.fp16.trt\n>>> The vision FP16 TensorRT model is saved at ${DATAPATH}/deploy/vit-b-16.img.fp16.trt\n```\n\nä¸Šé¢ç¤ºä¾‹ä»£ç æ‰§è¡Œç»“æŸåï¼Œæˆ‘ä»¬ä½¿ç”¨ONNXæ¨¡å‹ï¼Œå¾—åˆ°äº†ViT-B-16è§„æ¨¡ï¼ŒChinese-CLIPæ–‡æœ¬ä¾§å’Œå›¾åƒä¾§çš„TensorRTæ ¼å¼æ¨¡å‹ï¼Œå¯ä»¥ç”¨äºæå–å›¾æ–‡ç‰¹å¾ã€‚è¾“å‡ºTensorRTæ¨¡å‹çš„è·¯å¾„ä»¥è¿è¡Œè„šæœ¬æ—¶çš„`save-tensorrt-path`ä¸ºå‰ç¼€ï¼Œåé¢ä¾æ¬¡æ‹¼ä¸Š`.img`/`.txt`ã€`.fp16`ã€`.trt`ã€‚æˆ‘ä»¬ä½¿ç”¨ä¸¤ä¸ªè¾“å‡ºæ–‡ä»¶`vit-b-16.txt.fp16.trt`å’Œ`vit-b-16.img.fp16.trt`ã€‚\n\n**å¯¹äºå„è§„æ¨¡Chinese-CLIPé¢„è®­ç»ƒæ¨¡å‹ï¼Œæˆ‘ä»¬æä¾›è½¬æ¢å¥½çš„TensorRTå›¾åƒä¾§å’Œæ–‡æœ¬ä¾§æ¨¡å‹ï¼ˆåŸºäºTensorRT 8.5.2.2ç‰ˆæœ¬ï¼‰**ï¼Œä¸‹è½½æ–¹å¼å¦‚ä¸‹<span id=\"tensorrt_download\"></span>ï¼š\n\n<table border=\"1\" width=\"120%\">\n    <tr align=\"center\">\n        <td><b>æ¨¡å‹è§„æ¨¡</b></td><td><b>TensorRTå›¾åƒä¾§æ¨¡å‹</b></td><td><b>TensorRTæ–‡æœ¬ä¾§æ¨¡å‹</b></td>\n    </tr>\n\t<tr align=\"center\">\n        <td>CN-CLIP<sub>RN50</sub></td><td><a href=\"https://clip-cn-beijing.oss-cn-beijing.aliyuncs.com/checkpoints/rn50.img.fp16.trt\">Download</a></td><td><a href=\"https://clip-cn-beijing.oss-cn-beijing.aliyuncs.com/checkpoints/rn50.img.fp16.trt\">Download</a></td>\n    </tr>  \n\t<tr align=\"center\">\n        <td>CN-CLIP<sub>ViT-B/16</sub></td><td><a href=\"https://clip-cn-beijing.oss-cn-beijing.aliyuncs.com/checkpoints/vit-b-16.img.fp16.trt\">Download</a></td><td><a href=\"https://clip-cn-beijing.oss-cn-beijing.aliyuncs.com/checkpoints/vit-b-16.txt.fp16.trt\">Download</a></td>\n    </tr>  \n\t<tr align=\"center\">\n        <td>CN-CLIP<sub>ViT-L/14</sub></td><td><a href=\"https://clip-cn-beijing.oss-cn-beijing.aliyuncs.com/checkpoints/vit-l-14.img.fp16.trt\">Download</a></td><td><a href=\"https://clip-cn-beijing.oss-cn-beijing.aliyuncs.com/checkpoints/vit-l-14.txt.fp16.trt\">Download</a></td>\n    </tr>\n\t<tr align=\"center\">\n        <td>CN-CLIP<sub>ViT-L/14@336px</sub></td><td><a href=\"https://clip-cn-beijing.oss-cn-beijing.aliyuncs.com/checkpoints/vit-l-14-336.img.fp16.trt\">Download</a></td><td><a href=\"https://clip-cn-beijing.oss-cn-beijing.aliyuncs.com/checkpoints/vit-l-14-336.txt.fp16.trt\">Download</a></td>\n    </tr>\n\t<tr align=\"center\">\n        <td>CN-CLIP<sub>ViT-H/14</sub></td><td><a href=\"https://clip-cn-beijing.oss-cn-beijing.aliyuncs.com/checkpoints/vit-h-14.img.fp16.trt\">Download</a></td><td><a href=\"https://clip-cn-beijing.oss-cn-beijing.aliyuncs.com/checkpoints/vit-h-14.txt.fp16.trt\">Download</a></td>\n    </tr>  \n</table>\n<br>\n\nä¸‹è½½åç›´æ¥ç½®äº`${DATAPATH}/deploy/`ä¸‹å³å¯\n\n### è¿è¡Œæ¨¡å‹\n\nåœ¨è¿è¡ŒTensorRTæ¨¡å‹æ—¶ï¼Œå¦‚æœè½¬æ¢å’Œè¿è¡Œä¸æ˜¯åœ¨åŒä¸€ä¸ªç¯å¢ƒä¸‹ï¼Œè¯·æ³¨æ„è¿è¡Œæ¨¡å‹çš„ç¯å¢ƒTensorRTåº“ç‰ˆæœ¬ä¸è½¬æ¢ä¿æŒä¸€è‡´ï¼Œé¿å…æŠ¥é”™\n\n#### æå–å›¾åƒä¾§ç‰¹å¾\n\nç±»ä¼¼äºONNXæ¨¡å‹è¿è¡Œçš„æµç¨‹ï¼Œæˆ‘ä»¬åœ¨`Chinese-CLIP/`ç›®å½•ä¸‹ï¼Œä½¿ç”¨ä»¥ä¸‹çš„ç¤ºä¾‹ä»£ç ï¼Œè¯»å–åˆšåˆšè½¬æ¢å¥½çš„ViT-B-16è§„æ¨¡TensorRTå›¾åƒä¾§æ¨¡å‹`vit-b-16.img.fp16.trt`ï¼Œå¹¶ä¸ºReadmeä¸­ç¤ºä¾‹çš„[çš®å¡ä¸˜å›¾ç‰‡](examples/pokemon.jpeg)æå–å›¾åƒä¾§ç‰¹å¾ã€‚å’ŒONNXæ¨¡å‹ä¸€æ ·ï¼Œè¿™é‡Œè½¬æ¢å¥½çš„TensorRTæ¨¡å‹ä¹Ÿåªæ¥å—batchå¤§å°ä¸º1çš„è¾“å…¥ï¼Œå³ä¸€æ¬¡è°ƒç”¨åªå¤„ç†ä¸€å¼ è¾“å…¥å›¾ç‰‡\n\n```python\n# å®Œæˆå¿…è¦çš„importï¼ˆä¸‹æ–‡çœç•¥ï¼‰\nfrom cn_clip.deploy.tensorrt_utils import TensorRTModel\nfrom PIL import Image\nimport numpy as np\nimport torch\nimport argparse\nimport cn_clip.clip as clip\nfrom clip import load_from_name, available_models\nfrom clip.utils import _MODELS, _MODEL_INFO, _download, available_models, create_model, image_transform\n\n# è½½å…¥TensorRTå›¾åƒä¾§æ¨¡å‹ï¼ˆ**è¯·æ›¿æ¢${DATAPATH}ä¸ºå®é™…çš„è·¯å¾„**ï¼‰\nimg_trt_model_path=\"${DATAPATH}/deploy/vit-b-16.img.fp16.trt\"\nimg_trt_model = TensorRTModel(img_trt_model_path)\n\n# é¢„å¤„ç†å›¾ç‰‡\nmodel_arch = \"ViT-B-16\" # è¿™é‡Œæˆ‘ä»¬ä½¿ç”¨çš„æ˜¯ViT-B-16è§„æ¨¡ï¼Œå…¶ä»–è§„æ¨¡è¯·å¯¹åº”ä¿®æ”¹\npreprocess = image_transform(_MODEL_INFO[model_arch]['input_resolution'])\n# ç¤ºä¾‹çš®å¡ä¸˜å›¾ç‰‡ï¼Œé¢„å¤„ç†åå¾—åˆ°[1, 3, åˆ†è¾¨ç‡, åˆ†è¾¨ç‡]å°ºå¯¸çš„Torch Tensor\nimage = preprocess(Image.open(\"examples/pokemon.jpeg\")).unsqueeze(0).cuda()\n\n# ç”¨TensorRTæ¨¡å‹è®¡ç®—å›¾åƒä¾§ç‰¹å¾\nimage_features = img_trt_model(inputs={'image': image})['unnorm_image_features'] # æœªå½’ä¸€åŒ–çš„å›¾åƒç‰¹å¾\nimage_features /= image_features.norm(dim=-1, keepdim=True) # å½’ä¸€åŒ–åçš„Chinese-CLIPå›¾åƒç‰¹å¾ï¼Œç”¨äºä¸‹æ¸¸ä»»åŠ¡\nprint(image_features.shape) # Torch Tensor shape: [1, ç‰¹å¾å‘é‡ç»´åº¦]\n```\n\n#### æå–æ–‡æœ¬ä¾§ç‰¹å¾\n\nä¸å›¾åƒä¾§ç±»ä¼¼ï¼Œæˆ‘ä»¬ç”¨å¦‚ä¸‹ä»£ç å®Œæˆæ–‡æœ¬ä¾§TensorRTæ¨¡å‹çš„è½½å…¥ä¸ç‰¹å¾è®¡ç®—ï¼Œä¸å›¾åƒä¾§ç›¸åŒï¼Œæ–‡æœ¬ä¾§TensorRTéƒ¨ç½²æ¨¡å‹åªæ¥å—batchå¤§å°ä¸º1çš„è¾“å…¥ï¼Œå³ä¸€æ¬¡è°ƒç”¨åªå¤„ç†ä¸€æ¡è¾“å…¥æ–‡æœ¬ã€‚TensorRTæ¥å—çš„æ–‡æœ¬åºåˆ—é•¿åº¦å’Œç”¨äºè½¬æ¢çš„ONNXæ¨¡å‹ä¸€è‡´ï¼Œè¯·å‚è§ONNXè½¬æ¢æ—¶çš„context-lengthå‚æ•°ã€‚æˆ‘ä»¬ä¸º4æ¡å€™é€‰æ–‡æœ¬ä¾æ¬¡è®¡ç®—ViT-B-16è§„æ¨¡æ¨¡å‹çš„æ–‡æœ¬ç‰¹å¾ã€‚importç›¸å…³ä»£ç ä¸ä¸Šæ–‡ç›¸åŒï¼Œè¿™é‡Œçœç•¥ï¼š\n\n```python\n# è½½å…¥TensorRTæ–‡æœ¬ä¾§æ¨¡å‹ï¼ˆ**è¯·æ›¿æ¢${DATAPATH}ä¸ºå®é™…çš„è·¯å¾„**ï¼‰\ntxt_trt_model_path=\"${DATAPATH}/deploy/vit-b-16.txt.fp16.trt\"\ntxt_trt_model = TensorRTModel(txt_trt_model_path)\n\n# ä¸º4æ¡è¾“å…¥æ–‡æœ¬è¿›è¡Œåˆ†è¯ã€‚åºåˆ—é•¿åº¦æŒ‡å®šä¸º52ï¼Œéœ€è¦å’Œè½¬æ¢ONNXæ¨¡å‹æ—¶ä¿æŒä¸€è‡´ï¼ˆå‚è§ONNXè½¬æ¢æ—¶çš„context-lengthå‚æ•°ï¼‰\ntext = clip.tokenize([\"æ°å°¼é¾Ÿ\", \"å¦™è›™ç§å­\", \"å°ç«é¾™\", \"çš®å¡ä¸˜\"], context_length=52).cuda()\n\n# ç”¨TensorRTæ¨¡å‹ä¾æ¬¡è®¡ç®—æ–‡æœ¬ä¾§ç‰¹å¾\ntext_features = []\nfor i in range(len(text)):\n    # æœªå½’ä¸€åŒ–çš„æ–‡æœ¬ç‰¹å¾\n    text_feature = txt_trt_model(inputs={'text': torch.unsqueeze(text[i], dim=0)})['unnorm_text_features']\n    text_features.append(text_feature)\ntext_features = torch.squeeze(torch.stack(text_features), dim=1) # 4ä¸ªç‰¹å¾å‘é‡stackåˆ°ä¸€èµ·\ntext_features = text_features / text_features.norm(dim=1, keepdim=True) # å½’ä¸€åŒ–åçš„Chinese-CLIPæ–‡æœ¬ç‰¹å¾ï¼Œç”¨äºä¸‹æ¸¸ä»»åŠ¡\nprint(text_features.shape) # Torch Tensor shape: [4, ç‰¹å¾å‘é‡ç»´åº¦]\n```\n\n#### è®¡ç®—å›¾æ–‡ç›¸ä¼¼åº¦\n\nTensorRTæ¨¡å‹äº§å‡ºçš„å½’ä¸€åŒ–å›¾æ–‡ç‰¹å¾ï¼Œå†…ç§¯åsoftmaxå³å¯è®¡ç®—ç›¸ä¼¼åº¦ï¼ˆéœ€è¦è€ƒè™‘`logit_scale`ï¼‰ï¼Œä¸åŸå§‹Pytorchå’ŒONNXæ¨¡å‹çš„æ“ä½œå‡ç›¸åŒï¼Œå‚è§ä»¥ä¸‹ä»£ç ã€‚importç›¸å…³ä»£ç ä¸ä¸Šæ–‡ç›¸åŒï¼Œè¿™é‡Œçœç•¥ï¼š\n```python\n# å†…ç§¯åsoftmax\n# æ³¨æ„åœ¨å†…ç§¯è®¡ç®—æ—¶ï¼Œç”±äºå¯¹æ¯”å­¦ä¹ è®­ç»ƒæ—¶æœ‰temperatureçš„æ¦‚å¿µ\n# éœ€è¦ä¹˜ä¸Šæ¨¡å‹logit_scale.exp()ï¼Œæˆ‘ä»¬çš„é¢„è®­ç»ƒæ¨¡å‹logit_scaleå‡ä¸º4.6052ï¼Œæ‰€ä»¥è¿™é‡Œä¹˜ä»¥100\n# å¯¹äºç”¨æˆ·è‡ªå·±çš„ckptï¼Œè¯·ä½¿ç”¨torch.loadè½½å…¥åï¼ŒæŸ¥çœ‹ckpt['state_dict']['module.logit_scale']æˆ–ckpt['state_dict']['logit_scale']\nlogits_per_image = 100 * image_features @ text_features.t()\nprint(logits_per_image.softmax(dim=-1)) # å›¾æ–‡ç›¸ä¼¼æ¦‚ç‡: [[1.2475e-03, 5.3037e-02, 6.7583e-04, 9.4504e-01]]\n```\n\nå¯ä»¥çœ‹åˆ°ï¼ŒTensorRTæ¨¡å‹ç»™å‡ºçš„å›¾æ–‡ç›¸ä¼¼æ¦‚ç‡ï¼Œå’Œ[Readmeä¸­å¿«é€Ÿä½¿ç”¨éƒ¨åˆ†](https://github.com/OFA-Sys/Chinese-CLIP#apiå¿«é€Ÿä¸Šæ‰‹)åŸºäºPytorchçš„åŒä¸€ä»½æ¨¡å‹ã€ä»¥åŠä¸Šæ–‡ONNXæ¨¡å‹è®¡ç®—çš„ç»“æœéƒ½åŸºæœ¬ä¸€è‡´ï¼Œè¯æ˜äº†TensorRTæ¨¡å‹ç‰¹å¾è®¡ç®—çš„æ­£ç¡®æ€§ï¼Œè€ŒTensorRTæ¨¡å‹çš„ç‰¹å¾è®¡ç®—é€Ÿåº¦ï¼Œ**ç›¸æ¯”å‰ä¸¤è€…éƒ½æ›´èƒœä¸€ç­¹**ï¼ˆè¯¦è§[ä¸‹æ–‡](#é€Ÿåº¦å¯¹æ¯”ç»“æœ)ï¼‰ã€‚\n\n## æ¨ç†é€Ÿåº¦å¯¹æ¯”\n\n### å¯¹æ¯”å®éªŒè®¾ç½®\n\næˆ‘ä»¬çš„é€Ÿåº¦å¯¹æ¯”å®éªŒï¼Œåœ¨ä¸€å°å•å¡T4 GPUï¼ˆ16GBæ˜¾å­˜ï¼‰æœºå™¨è¿›è¡Œï¼Œé…å¤‡16ä¸ªIntel Xeon(Skylake) Platinum 8163 (2.5GHz) CPU coresï¼Œ64GBå†…å­˜ã€‚è¿›è¡Œé€Ÿåº¦æµ‹è¯•æ—¶ï¼Œæˆ‘ä»¬é‡‡ç”¨ä¸Šé¢çš„ç¤ºä¾‹å›¾ç‰‡å’Œå…¶ä¸­ä¸€ä¸ªå€™é€‰æ–‡æœ¬ï¼Œå¯¹Pytorchã€ONNXå’ŒTensorRTæ¨¡å‹å‡æ‰§è¡Œ100æ¬¡å›¾æ–‡ç‰¹å¾æå–ï¼Œå–è€—æ—¶å¹³å‡å€¼(ms)ã€‚ä»¥ViT-B-16è§„æ¨¡æµ‹é€Ÿä¸ºä¾‹ï¼Œåœ¨`Chinese-CLIP/`ä¸‹æ‰§è¡Œçš„ä»£ç å¦‚ä¸‹ï¼š\n```bash\nexport CUDA_VISIBLE_DEVICES=0\nexport PYTHONPATH=${PYTHONPATH}:`pwd`/cn_clip\n\n# æ›¿æ¢${DATAPATH}ä¸ºå®é™…çš„è·¯å¾„\npython3 cn_clip/deploy/speed_benchmark.py \\\n        --model-arch ViT-B-16 \\\n        --pytorch-ckpt ${DATAPATH}/pretrained_weights/clip_cn_vit-b-16.pt \\\n        --pytorch-precision fp16 \\\n        --onnx-image-model ${DATAPATH}/deploy/vit-b-16.img.fp16.onnx \\\n        --onnx-text-model ${DATAPATH}/deploy/vit-b-16.txt.fp16.onnx \\\n        --tensorrt-image-model ${DATAPATH}/deploy/vit-b-16.img.fp16.trt \\\n        --tensorrt-text-model ${DATAPATH}/deploy/vit-b-16.txt.fp16.trt\n```\n\nåœ¨logè¾“å‡ºä¸­å°†å…ˆåæ‰“å°ä»¥ä¸‹å‡ è¡Œï¼Œå³ä¸ºæµ‹é€Ÿç»“æœï¼š\n```\n[Pytorch image inference speed (batch-size: 1):] mean=11.12ms, sd=0.05ms, min=11.00ms, max=11.32ms, median=11.11ms, 95p=11.20ms, 99p=11.30ms\n[ONNX image inference speed (batch-size: 1):] mean=4.92ms, sd=0.04ms, min=4.82ms, max=5.01ms, median=4.92ms, 95p=4.98ms, 99p=5.00ms\n[TensorRT image inference speed (batch-size: 1):] mean=3.58ms, sd=0.08ms, min=3.30ms, max=3.72ms, median=3.58ms, 95p=3.70ms, 99p=3.72ms\n\n[Pytorch text inference speed (batch-size: 1):] mean=12.47ms, sd=0.07ms, min=12.32ms, max=12.64ms, median=12.48ms, 95p=12.57ms, 99p=12.61ms\n[ONNX text inference speed (batch-size: 1):] mean=3.42ms, sd=0.44ms, min=2.96ms, max=3.89ms, median=3.45ms, 95p=3.87ms, 99p=3.88ms\n[TensorRT text inference speed (batch-size: 1):] mean=1.54ms, sd=0.01ms, min=1.51ms, max=1.57ms, median=1.54ms, 95p=1.56ms, 99p=1.56ms\n```\n\n### é€Ÿåº¦å¯¹æ¯”ç»“æœ\n\næˆ‘ä»¬åˆ—å‡ºæ¨ç†batch sizeä¸º1çš„æƒ…å†µä¸‹ï¼Œæ¯ä¸ªè§„æ¨¡Pytorchã€ONNXå’ŒTensorRTæ¨¡å‹çš„FP16ç²¾åº¦æ¨ç†è€—æ—¶å¯¹æ¯”ï¼Œå¯ä»¥çœ‹åˆ°TensorRTå¯¹äºå°è§„æ¨¡æ¨¡å‹çš„æ¨ç†é€Ÿåº¦æå‡å°¤å…¶æ˜æ˜¾\n<table border=\"1\" width=\"120%\">\n    <tr align=\"center\">\n        <th>å•ä½: ms/æ ·æœ¬</th><th colspan=\"3\">å›¾åƒç‰¹å¾æå–</th><th colspan=\"3\">æ–‡æœ¬ç‰¹å¾æå–</th>\n    </tr>\n    <tr align=\"center\">\n        <td>æ¨¡å‹</td><td>Pytorch</td><td>ONNX</td><td>TensorRT</td><td>Pytorch</td><td>ONNX</td><td>TensorRT</td>\n    </tr>\n\t<tr align=\"center\">\n        <td width=\"120%\">CN-CLIP<sub>RN50</sub></td><td>12.93</td><td>5.04</td><td><b>1.36</b></td><td>3.64</td><td>0.95</td><td><b>0.58</b></td>\n    </tr>  \n\t<tr align=\"center\">\n        <td width=\"120%\">CN-CLIP<sub>ViT-B/16</sub></td><td>11.12</td><td>4.92</td><td><b>3.58</b></td><td>12.47</td><td>3.42</td><td><b>1.54</b></td>\n    </tr>  \n\t<tr align=\"center\">\n        <td width=\"120%\">CN-CLIP<sub>ViT-L/14</sub></td><td>21.19</td><td>17.10</td><td><b>13.08</b></td><td>12.45</td><td>3.48</td><td><b>1.52</b></td>\n    </tr>\n\t<tr align=\"center\">\n        <td width=\"120%\">CN-CLIP<sub>ViT-L/14@336px</sub></td><td>47.11</td><td>48.40</td><td><b>31.59</b></td><td>12.24</td><td>3.25</td><td><b>1.54</b></td>\n    </tr>\n\t<tr align=\"center\">\n        <td width=\"120%\">CN-CLIP<sub>ViT-H/14</sub></td><td>35.10</td><td>34.00</td><td><b>26.98</b></td><td>23.98</td><td>6.01</td><td><b>3.89</b></td>\n    </tr>  \n</table>\n<br>\n\n## ä¸‹æ¸¸æ•ˆæœå¯¹æ¯”\n\næˆ‘ä»¬ä½¿ç”¨Chinese-CLIPå®éªŒä¸­ï¼Œæ‰€æ¶‰åŠçš„MUGEå›¾æ–‡æ£€ç´¢ä»»åŠ¡å¯¹æ¯”ä¸‹æ¸¸æ•ˆæœï¼Œè§‚å¯ŸPytorchã€ONNXå’ŒTensorRT FP16æ¨¡å‹zero-shotçš„è¡¨ç°ã€‚å¦‚[Readmeé¢„æµ‹åŠè¯„ä¼°éƒ¨åˆ†](https://github.com/OFA-Sys/Chinese-CLIP#é¢„æµ‹åŠè¯„ä¼°)éƒ¨åˆ†æ‰€è¿°ï¼ŒMUGEå›¾æ–‡æ£€ç´¢è¯„æµ‹ç»“æœåˆ†ä¸ºå›¾æ–‡ç‰¹å¾æå–ã€KNNæ£€ç´¢å’ŒRecallè®¡ç®—3æ­¥ã€‚ONNXå’ŒTensorRTæ¨¡å‹çš„å›¾æ–‡ç‰¹å¾æå–è„šæœ¬ï¼Œè¯·åˆ†åˆ«å‚è§`cn_clip/eval/extract_features_onnx.py`å’Œ`cn_clip/eval/extract_features_tensorrt.py`ï¼Œç›¸æ¯”äºåŸç”ŸPytorchç‰¹å¾æå–ä½¿ç”¨çš„`extract_features.py`ä»…åšäº†å¾®å°çš„æ”¹åŠ¨ã€‚åç»­çš„KNNå’ŒRecallè®¡ç®—ä½¿ç”¨çš„è„šæœ¬å’Œæµç¨‹å®Œå…¨ä¸å˜ã€‚\n\næˆ‘ä»¬é€‰å–ViT-B-16å’ŒViT-H-14ä¸¤ä¸ªè§„æ¨¡ï¼Œç»“æœå¯¹æ¯”å¦‚ä¸‹ï¼š\n<table border=\"1\" width=\"100%\">\n    <tr align=\"center\">\n        <th>Setup</th><th colspan=\"4\">ViT-B-16 Zero-shot</th><th colspan=\"4\">ViT-H-14 Zero-shot</th>\n    </tr>\n    <tr align=\"center\">\n        <td>Metric</td><td>R@1</td><td>R@5</td><td>R@10</td><td>MR</td><td>R@1</td><td>R@5</td><td>R@10</td><td>MR</td>\n    </tr>\n\t<tr align=\"center\">\n        <td width=\"120%\">Pytorch FP16</sub></td><td>52.1</td><td>76.7</td><td>84.4</td><td>71.1</td><td>63.0</td><td>84.1</td><td>89.2</td><td>78.8</td>\n    </tr>  \n\t<tr align=\"center\">\n        <td width=\"120%\">ONNX FP16</sub></td><td>52.0</td><td>76.8</td><td>84.3</td><td>71.1</td><td>63.1</td><td>84.1</td><td>89.0</td><td>78.8</td>\n    </tr>\n\t<tr align=\"center\">\n        <td width=\"120%\">TensorRT FP16</sub></td><td>52.0</td><td>76.8</td><td>84.2</td><td>71.0</td><td>63.1</td><td>84.2</td><td>89.1</td><td>78.8</td>\n    </tr>\n</table>\n<br>\nç»“æœæŒ‡æ ‡åŸºæœ¬æ˜¯ä¸€è‡´çš„ï¼Œç›¸å·®Â±0.2åœ¨å¯ä»¥æ¥å—çš„èŒƒå›´å†…ï¼ˆæ¢ä¸€å°æœºå™¨å³å¯èƒ½é€ æˆçš„è¯¯å·®é‡çº§ï¼‰ï¼Œè¯æ˜äº†ONNXå’ŒTensorRTæ¨¡å‹çš„è½¬æ¢æ­£ç¡®æ€§ã€‚\n"
        },
        {
          "name": "deployment_En.md",
          "type": "blob",
          "size": 27.4482421875,
          "content": "[**ä¸­æ–‡è¯´æ˜**](deployment.md) | [**English**](deployment_En.md)\n\n# Chinese-CLIP Model Deployment: ONNX & TensorRT Format Conversion\n\nOur latest Chinese-CLIP code supports the conversion of Pytorch models of all scales into [ONNX](https://onnx.ai/) or [TensorRT](https://developer.nvidia.com/tensorrt) formats, thereby **[improving the inference speed of feature calculation](#speed-comparison-results)** compared with the original Pytorch models without affecting the downstream task effect of feature extraction. Below we give the whole process of preparing the FP16 Chinese-CLIP models in ONNX and TensorRT formats on GPU (and also give the [download links](#tensorrt_download) of Chinese-CLIP pretraining TensorRT models), and attach the comparison of model effect and inference speed, so that you can take advantage of the advantages of ONNX and TensorRT library in inference performance.\n\n## Environmental Preparation\n\n+ **GPU hardware requirements**: Please prepare Nvidia GPUs **with Volta architecture and above** (equipped with FP16 Tensor Core). Please refer to [this document](https://en.wikipedia.org/wiki/CUDA#GPUs_supported) for the corresponding GPUs of each Nvidia architecture. Here we take T4 GPU as an example.\n+ **CUDA**: [CUDA](https://developer.nvidia.com/cuda-11-6-0-download-archive) version 11.6 and above is recommended. We take version 11.6 as an example.\n+ **CUDNN**: [CUDNN](https://developer.nvidia.com/rdp/cudnn-archive) version 8.6.0 and above is recommended. We take version 8.6.0 as an example. Please note that TensorRT and CUDNN have version correspondence, e.g. TensorRT 8.5.x must correspond to CUDNN 8.6.0, see the TensorRT version requirements for details.\n+ **ONNX**: Note that when we convert the TensorRT model, we will follow the steps Pytorch â†’ ONNX â†’ TensorRT, so preparing the TensorRT model also requires installing the ONNX library first. Here we take onnx version 1.13.0, onnxruntime-gpu version 1.13.1, and onnxmltools version 1.11.1 as examples.\n+ **TensorRT**: The recommended [TensorRT](https://docs.nvidia.com/deeplearning/tensorrt/archives/index.html#trt_8) version is 8.5.x. We use 8.5.2.2 as an example. For the CUDNN version corresponding to each TensorRT version, please refer to the \"NVIDIA TensorRT Support Matrix\" from this [documentation page]((https://docs.nvidia.com/deeplearning/tensorrt/archives/index.html#trt_8)).\n+ **Pytorch**: Pytorch version 1.12.1 and above is recommended. We take version 1.12.1 as an example. (It is recommended to directly pip install 1.12.1 + cu116, and try not to use conda to install cudatoolkit, avoiding TensorRT errors due to CUDNN version changes. )\n+ Other dependencies as required in [requirements.txt](requirements.txt).\n\nexecutable code\n``` \npip install tensorrt==8.5.2.2 onnx==1.13.0 onnxruntime-gpu==1.13.1 onnxmltools==1.11.1\npip install torch==1.12.1+cu116 torchvision==0.13.1+cu116 torchaudio==0.12.1 -f https://download.pytorch.org/whl/torch_stable.html\npip install -r requirements.txt \n```\n\n## Converting and Running ONNX Models\n\n### Converting Models\n\nFor code to convert a Pytorch checkpoint to ONNX format, see `cn_clip/deploy/pytorch_to_onnx.py`. Let's take the example of converting a ViT-B-16 size Chinese-CLIP pretrained model. You can run the following code (Please refer to the [Code Organization](https://github.com/OFA-Sys/Chinese-CLIP/blob/master/README_En.md#code-organization) section of Readme to set `${DATAPATH}` and replace the script content below, using relative paths where possible. ) :\n\n```bash\ncd Chinese-CLIP/\nexport CUDA_VISIBLE_DEVICES=0\nexport PYTHONPATH=${PYTHONPATH}:`pwd`/cn_clip\n\n# Please refer to the 'Code Organization' section in Readme to set `${DATAPATH}` and replace the script content below, using relative paths where possible: https://github.com/OFA-Sys/Chinese-CLIP/blob/master/README_En.md#code-organization\ncheckpoint_path=${DATAPATH}/pretrained_weights/clip_cn_vit-b-16.pt # Specify the full path to the ckpt to be converted\nmkdir -p ${DATAPATH}/deploy/ # Create output folders for ONNX models\n\npython cn_clip/deploy/pytorch_to_onnx.py \\\n       --model-arch ViT-B-16 \\\n       --pytorch-ckpt-path ${checkpoint_path} \\\n       --save-onnx-path ${DATAPATH}/deploy/vit-b-16 \\\n       --convert-text --convert-vision\n```\n\nEach configuration item is defined as follows:\n\n+ `model-arch`: Model size, options include`[\"RN50\", \"ViT-B-16\", \"ViT-L-14\", \"ViT-L-14-336\", \"ViT-H-14\"]`, details of each model can be found in [Readme](https://github.com/OFA-Sys/Chinese-CLIP/blob/master/README_En.md#model-card). \n+ `pytorch-ckpt-path`: Specify the Pytorch model ckpt path, which in the code example above we specified as the pretrained ckpt path, or you can specify the location of your finetune ckpt. The parameters in ckpt need to correspond to the model size specified by `model-arch`.\n+ `save-onnx-path`: Specifies the path (prefix) to the output ONNX format model. After the conversion is completed, the code will output the text-side and image-side ONNX format encoded model files, one for FP32 and one for FP16, which specifies the path prefix of the above output files.\n+ `convert-text` and `convert-vision`: Specify whether to convert text-side and image-side models.\n+ `context-length` (Optional): Specify the sequence length of the input received by the text-side ONNX model, defaulting to the 52 used in our pretraining ckpt.\n+ `download-root` (Optional): If `pytorch-ckpt-path` is not specified, the code will automatically download the official Chinese-CLIP pretraining ckpt for conversion according to `model-arch`, which is stored in the specified directory `download-root`.\n\nAfter running, the following log output will be obtained:\n```\nFinished PyTorch to ONNX conversion...\n>>> The text FP32 ONNX model is saved at ${DATAPATH}/deploy/vit-b-16.txt.fp32.onnx\n>>> The text FP16 ONNX model is saved at ${DATAPATH}/deploy/vit-b-16.txt.fp16.onnx with extra file ${DATAPATH}/deploy/vit-b-16.txt.fp16.onnx.extra_file\n>>> The vision FP32 ONNX model is saved at ${DATAPATH}/deploy/vit-b-16.img.fp32.onnx\n>>> The vision FP16 ONNX model is saved at ${DATAPATH}/deploy/vit-b-16.img.fp16.onnx with extra file ${DATAPATH}/deploy/vit-b-16.img.fp16.onnx.extra_file\n```\n\nAfter the above code is executed, we get the ViT-B-16 size Chinese-CLIP text-side and image-side ONNX models, which can be used to extract image and text features respectively. The paths to the output ONNX models are all prefixed with `save-onnx-path` at the time of running the script, followed by `.img`/`.txt`, `.fp16`/`.fp32`, and `.onnx`. We will subsequently use mainly ONNX models `vit-b-16.txt.fp16.onnx` and `vit-b-16.img.fp16.onnx` in FP16 format.\n\nNotice that some of the ONNX model files also come with an extra_file, which is also part of the corresponding ONNX model. When using these ONNX models, the path to the extra_file is stored in the `.onnx` file (e.g. `${DATAPATH}/deploy/vit-b-16.txt.fp16.onnx.extra_file`) and the extra_file will be loaded according to this path, so please do not change the path and use relative path for `${DATAPATH}` when converting (e.g. `../datapath`) to avoid errors at runtime when the extra_file is not found.\n\n### Run the Model\n\n#### Extraction of Image-side Features\nWe use the following sample code in the `Chinese-CLIP/` directory to read the converted ViT-B-16 size ONNX image-side model `vit-b-16.img.fp16.onnx` and extract the features of the [Pikachu image](examples/pokemon.jpeg) in Readme. Note that the converted ONNX model only accepts inputs with a batch size of 1, i.e. only one input image is processed in one call.\n\n```python\n# Complete the necessary import (omitted below)\nimport onnxruntime\nfrom PIL import Image\nimport numpy as np\nimport torch\nimport argparse\nimport cn_clip.clip as clip\nfrom clip import load_from_name, available_models\nfrom clip.utils import _MODELS, _MODEL_INFO, _download, available_models, create_model, image_transform\n\n# Load ONNX image-side modelï¼ˆ**Please replace ${DATAPATH} with the actual path**ï¼‰\nimg_sess_options = onnxruntime.SessionOptions()\nimg_run_options = onnxruntime.RunOptions()\nimg_run_options.log_severity_level = 2\nimg_onnx_model_path=\"${DATAPATH}/deploy/vit-b-16.img.fp16.onnx\"\nimg_session = onnxruntime.InferenceSession(img_onnx_model_path,\n                                        sess_options=img_sess_options,\n                                        providers=[\"CUDAExecutionProvider\"])\n\n# Preprocess images\nmodel_arch = \"ViT-B-16\" # Here we use the ViT-B-16 size, other sizes please modify accordingly\npreprocess = image_transform(_MODEL_INFO[model_arch]['input_resolution'])\n# Example Pikachu image, Torch Tensor of [1, 3, resolution, resolution] size after preprocessing\nimage = preprocess(Image.open(\"examples/pokemon.jpeg\")).unsqueeze(0)\n\n# Calculate image-side features with ONNX model\nimage_features = img_session.run([\"unnorm_image_features\"], {\"image\": image.cpu().numpy()})[0] # Unnormalized image features\nimage_features = torch.tensor(image_features)\nimage_features /= image_features.norm(dim=-1, keepdim=True) # Normalized Chinese-CLIP image features for downstream tasks\nprint(image_features.shape) # Torch Tensor shape: [1, feature dimension]\n```\n\n#### Extraction of Text-side Features\n\nSimilarly, we use the following code to complete the loading and feature calculation of the text-side ONNX model. As with the image-side model, the text-side ONNX model only accepts inputs with a batch size of 1, i.e., only one input text is processed in a single call.\n\n```python\n# Load ONNX text-side modelï¼ˆ**Please replace ${DATAPATH} with the actual path**ï¼‰\ntxt_sess_options = onnxruntime.SessionOptions()\ntxt_run_options = onnxruntime.RunOptions()\ntxt_run_options.log_severity_level = 2\ntxt_onnx_model_path=\"${DATAPATH}/deploy/vit-b-16.txt.fp16.onnx\"\ntxt_session = onnxruntime.InferenceSession(txt_onnx_model_path,\n                                        sess_options=txt_sess_options,\n                                        providers=[\"CUDAExecutionProvider\"])\n\n# Tokenize the 4 input texts. The sequence length is specified to 52, which is the same as when converting the ONNX model (see the context_length in the conversion process).\ntext = clip.tokenize([\"æ°å°¼é¾Ÿ\", \"å¦™è›™ç§å­\", \"å°ç«é¾™\", \"çš®å¡ä¸˜\"], context_length=52) \n\n# Calculate text-side features sequentially with ONNX model\ntext_features = []\nfor i in range(len(text)):\n    one_text = np.expand_dims(text[i].cpu().numpy(),axis=0)\n    text_feature = txt_session.run([\"unnorm_text_features\"], {\"text\":one_text})[0] # Unnormalized image features\n    text_feature = torch.tensor(text_feature)\n    text_features.append(text_feature)\ntext_features = torch.squeeze(torch.stack(text_features),dim=1) # 4 feature vectors stacked together\ntext_features = text_features / text_features.norm(dim=1, keepdim=True) # Normalized Chinese-CLIP text features for downstream tasks\nprint(text_features.shape) # Torch Tensor shape: [1, feature dimension]\n```\n\n#### Calculate Image & Text Similarity\n\nSimilarities are calculated from the normalized image and text features produced by the ONNX model after inner product and softmax (need to consider `logit_scale`), which is identical to the original Pytorch models, see the following code:\n\n```python\n# Inner product followed by softmax\n# Note that in the inner product calculation, due to the concept of temperature during contrast learning training, you need to multiply the model logit_scale.exp(), our pretraining model logit_scale is 4.6052, so here multiply by 100.\n# For your own ckpt, please load it using torch.load and then check ckpt['state_dict']['module.logit_scale'] or ckpt['state_dict']['logit_scale'].\nlogits_per_image = 100 * image_features @ text_features.t()\nprint(logits_per_image.softmax(dim=-1)) # Image & text similarity probabilities: [[1.2252e-03, 5.2874e-02, 6.7116e-04, 9.4523e-01]]\n```\n\nWe can see that the output similarities given by the ONNX model are largely consistent with the results calculated in [API Use Case](https://github.com/OFA-Sys/Chinese-CLIP/blob/master/README_En.md#api-use-case) section of Readme based on the same model in Pytorch, proving the correctness of the feature calculation of the ONNX model. However, **the feature calculation speed of ONNX model is more advantageous than that of Pytorch** (see [below](#Speed Comparison Results) for details).\n\n## Converting and Running TensorRT Models\n\n### Converting Models\n\nCompared with ONNX models, TensorRT models have faster inference speed, and we provide converted Chinese-CLIP pretrained TensorRT image-side and text-side models ([download links](#tensorrt_download)). As mentioned above, we prepare the TensorRT format model, which is further transformed using the ONNX format model we just obtained. For the code to convert ONNX to TensorRT format, see `cn_clip/deploy/onnx_to_tensorrt.py`. Still using the ViT-B-16 size Chinese-CLIP as an example, using the ONNX models `vit-b-16.txt.fp16.onnx` and `vit-b-16.img.fp16.onnx`, run the following code in `Chinese-CLIP/`:\n\n```bash\nexport PYTHONPATH=${PYTHONPATH}:`pwd`/cn_clip\n# As above, please replace ${DATAPATH} according to the actual situation\npython cn_clip/deploy/onnx_to_tensorrt.py \\\n       --model-arch ViT-B-16 \\\n       --convert-text \\\n       --text-onnx-path ${DATAPATH}/deploy/vit-b-16.txt.fp16.onnx \\\n       --convert-vision \\\n       --vision-onnx-path ${DATAPATH}/deploy/vit-b-16.img.fp16.onnx \\\n       --save-tensorrt-path ${DATAPATH}/deploy/vit-b-16 \\\n       --fp16\n```\n\nEach configuration item is defined as follows:\n\n+ `model-arch`: Model size, options include`[\"RN50\", \"ViT-B-16\", \"ViT-L-14\", \"ViT-L-14-336\", \"ViT-H-14\"]`, details of each model can be found in [Readme](https://github.com/OFA-Sys/Chinese-CLIP/blob/master/README_En.md#model-card). \n+ `convert-text` and `convert-vision`: Specify whether to convert text-side and image-side models.\n+ `text-onnx-path`: Specify the text-side ONNX model file path, which needs to correspond to the model size specified by `model-arch`.\n+ `vision-onnx-path`: Specify the image-side ONNX model file path, which needs to correspond to the model size specified by `model-arch`.\n+ `save-tensorrt-path`: Specifies the path (prefix) to the output TensorRT format model. After the conversion, the code will output the model files encoded in TensorRT format for the text side and image side respectively, and this parameter specifies the path prefix of the above output files.\n+ `fp16`: Specify models converted to FP16 precision TensorRT format.\n\nThe process takes a few minutes to ten minutes, depending on the model's size. After running, the following log output will be obtained:\n\n```\nFinished ONNX to TensorRT conversion...\n>>> The text FP16 TensorRT model is saved at ${DATAPATH}/deploy/vit-b-16.txt.fp16.trt\n>>> The vision FP16 TensorRT model is saved at ${DATAPATH}/deploy/vit-b-16.img.fp16.trt\n```\n\nAfter the execution of the above sample code, we obtained ViT-B-16 size Chinese-CLIP text-side and image-side TensorRT format models using ONNX models, which can be used to extract image & text features. The paths to the output TensorRT models are all prefixed with `save-tensorrt-path` when running the script, followed by `.img`/`.txt`, `.fp16`, and `.trt`. We use two output files `vit-b-16.txt.fp16.trt` and `vit-b-16.img.fp16.trt`.\n\n**For each size of Chinese-CLIP pretrained models, we provide converted TensorRT image-side and text-side models (based on TensorRT version 8.5.2.2)**, which can be downloaded as follows<span id=\"tensorrt_download\"></span>:\n\n<table border=\"1\" width=\"120%\">\n    <tr align=\"center\">\n        <td><b>Model Size</b></td><td><b>TensorRT Image-side Model</b></td><td><b>TensorRT Text-side Model</b></td>\n    </tr>\n\t<tr align=\"center\">\n        <td>CN-CLIP<sub>RN50</sub></td><td><a href=\"https://clip-cn-beijing.oss-cn-beijing.aliyuncs.com/checkpoints/rn50.img.fp16.trt\">Download</a></td><td><a href=\"https://clip-cn-beijing.oss-cn-beijing.aliyuncs.com/checkpoints/rn50.img.fp16.trt\">Download</a></td>\n    </tr>  \n\t<tr align=\"center\">\n        <td>CN-CLIP<sub>ViT-B/16</sub></td><td><a href=\"https://clip-cn-beijing.oss-cn-beijing.aliyuncs.com/checkpoints/vit-b-16.img.fp16.trt\">Download</a></td><td><a href=\"https://clip-cn-beijing.oss-cn-beijing.aliyuncs.com/checkpoints/vit-b-16.txt.fp16.trt\">Download</a></td>\n    </tr>  \n\t<tr align=\"center\">\n        <td>CN-CLIP<sub>ViT-L/14</sub></td><td><a href=\"https://clip-cn-beijing.oss-cn-beijing.aliyuncs.com/checkpoints/vit-l-14.img.fp16.trt\">Download</a></td><td><a href=\"https://clip-cn-beijing.oss-cn-beijing.aliyuncs.com/checkpoints/vit-l-14.txt.fp16.trt\">Download</a></td>\n    </tr>\n\t<tr align=\"center\">\n        <td>CN-CLIP<sub>ViT-L/14@336px</sub></td><td><a href=\"https://clip-cn-beijing.oss-cn-beijing.aliyuncs.com/checkpoints/vit-l-14-336.img.fp16.trt\">Download</a></td><td><a href=\"https://clip-cn-beijing.oss-cn-beijing.aliyuncs.com/checkpoints/vit-l-14-336.txt.fp16.trt\">Download</a></td>\n    </tr>\n\t<tr align=\"center\">\n        <td>CN-CLIP<sub>ViT-H/14</sub></td><td><a href=\"https://clip-cn-beijing.oss-cn-beijing.aliyuncs.com/checkpoints/vit-h-14.img.fp16.trt\">Download</a></td><td><a href=\"https://clip-cn-beijing.oss-cn-beijing.aliyuncs.com/checkpoints/vit-h-14.txt.fp16.trt\">Download</a></td>\n    </tr>  \n</table>\n\n<br>\n\nJust download it and place it directly under directory `${DATAPATH}/deploy/`.\n\n### Running Models\n\nWhen running the TensorRT model, if the conversion and running are not in the same environment, please note that the TensorRT library version of the environment running the model is consistent with the conversion to avoid errors.\n\n#### Extraction of Image-side Features\n\nSimilar to the process of ONNX models, we use the following sample code in the `Chinese-CLIP/` directory to read the converted ViT-B-16 size TensorRT image-side model `vit-b-16.img.fp16.trt` and extract the features of the [Pikachu image](examples/pokemon.jpeg) in Readme. The converted TensorRT model here also accepts only inputs with a batch size of 1, i.e. only one input image is processed in one call.\n\n```python\n# Complete the necessary import (omitted below)\nfrom cn_clip.deploy.tensorrt_utils import TensorRTModel\nfrom PIL import Image\nimport numpy as np\nimport torch\nimport argparse\nimport cn_clip.clip as clip\nfrom clip import load_from_name, available_models\nfrom clip.utils import _MODELS, _MODEL_INFO, _download, available_models, create_model, image_transform\n\n# Load ONNX image-side modelï¼ˆ**Please replace ${DATAPATH} with the actual path**ï¼‰\nimg_trt_model_path=\"${DATAPATH}/deploy/vit-b-16.img.fp16.trt\"\nimg_trt_model = TensorRTModel(img_trt_model_path)\n\n# Preprocess images\nmodel_arch = \"ViT-B-16\" # Here we use the ViT-B-16 size, other sizes please modify accordingly\npreprocess = image_transform(_MODEL_INFO[model_arch]['input_resolution'])\n# Example Pikachu image, Torch Tensor of [1, 3, resolution, resolution] size after preprocessing\nimage = preprocess(Image.open(\"examples/pokemon.jpeg\")).unsqueeze(0).cuda()\n\n# Calculate image-side features with TensorRT model\nimage_features = img_trt_model(inputs={'image': image})['unnorm_image_features'] # Unnormalized image features\nimage_features /= image_features.norm(dim=-1, keepdim=True) # Normalized Chinese-CLIP image features for downstream tasks\nprint(image_features.shape) # Torch Tensor shape: [1, feature dimension]\n```\n\n#### Extraction of Text-side Features\n\nSimilarly, we use the following code to complete the loading and feature calculation of the text-side TensorRT model. As with the image-side model, the text-side TensorRT model only accepts inputs with a batch size of 1, i.e., only one input text is processed in a single call. The text sequence length accepted by TensorRT is consistent with the used ONNX model, see the context-length parameter during ONNX conversion.\n\n```python\n# Load TensorRT text-side modelï¼ˆ**Please replace ${DATAPATH} with the actual path**ï¼‰\ntxt_trt_model_path=\"${DATAPATH}/deploy/vit-b-16.txt.fp16.trt\"\ntxt_trt_model = TensorRTModel(txt_trt_model_path)\n\n# Tokenize the 4 input texts. The sequence length is specified to 52, which is the same as when converting the ONNX model (see the context_length in the ONNX conversion process).\ntext = clip.tokenize([\"æ°å°¼é¾Ÿ\", \"å¦™è›™ç§å­\", \"å°ç«é¾™\", \"çš®å¡ä¸˜\"], context_length=52).cuda()\n\n# Calculate text-side features sequentially with TensorRT model\ntext_features = []\nfor i in range(len(text)):\n    # Unnormalized image features\n    text_feature = txt_trt_model(inputs={'text': torch.unsqueeze(text[i], dim=0)})['unnorm_text_features']\n    text_features.append(text_feature)\ntext_features = torch.squeeze(torch.stack(text_features), dim=1) # 4 feature vectors stacked together\ntext_features = text_features / text_features.norm(dim=1, keepdim=True) # Normalized Chinese-CLIP text features for downstream tasks\nprint(text_features.shape) # Torch Tensor shape: [1, feature dimension]\n```\n\n#### Calculate Image & Text Similarity\n\nSimilarities are calculated from the normalized image and text features produced by the TensorRT model after inner product and softmax (need to consider `logit_scale`), which is identical to the original Pytorch models and ONNX models, see the following code:\n\n```python\n# Inner product followed by softmax\n# Note that in the inner product calculation, due to the concept of temperature during contrast learning training, you need to multiply the model logit_scale.exp(), our pretraining model logit_scale is 4.6052, so here multiply by 100.\n# For your own ckpt, please load it using torch.load and then check ckpt['state_dict']['module.logit_scale'] or ckpt['state_dict']['logit_scale'].\nlogits_per_image = 100 * image_features @ text_features.t()\nprint(logits_per_image.softmax(dim=-1)) # Image & text similarity probabilities: [[1.2475e-03, 5.3037e-02, 6.7583e-04, 9.4504e-01]]\n```\n\nWe can see that the output similarities given by the TensorRT model are largely consistent with the results calculated in [API Use Case](https://github.com/OFA-Sys/Chinese-CLIP/blob/master/README_En.md#api-use-case) section of Readme based on the same model in Pytorch and the results calculated by ONNX models above, proving the correctness of the feature calculation of the TensorRT model. However, the feature calculation speed of TensorRT model is **superior to both of the previous two** (see [below](#Speed Comparison Results) for details).\n\n## Comparison of Inference Speed\n\n### Comparative Experimental Setup\n\nOur experiments are conducted on a single T4 GPU (16GB memory) machine with 16 Intel Xeon (Skylake) Platinum 8163 (2.5GHz) CPU cores and 64GB memory. We use the above sample image and one of the candidate texts and perform 100 times of image and text feature extraction for both Pytorch, ONNX, and TensorRT models, taking the average time (ms). Taking the speed measurement of ViT-B-16 size model as an example, the code executed under `Chinese-CLIP/` is as follows:\n\n```bash\nexport CUDA_VISIBLE_DEVICES=0\nexport PYTHONPATH=${PYTHONPATH}:`pwd`/cn_clip\n\n# Please replace ${DATAPATH} according to the actual situation\npython3 cn_clip/deploy/speed_benchmark.py \\\n        --model-arch ViT-B-16 \\\n        --pytorch-ckpt ${DATAPATH}/pretrained_weights/clip_cn_vit-b-16.pt \\\n        --pytorch-precision fp16 \\\n        --onnx-image-model ${DATAPATH}/deploy/vit-b-16.img.fp16.onnx \\\n        --onnx-text-model ${DATAPATH}/deploy/vit-b-16.txt.fp16.onnx \\\n        --tensorrt-image-model ${DATAPATH}/deploy/vit-b-16.img.fp16.trt \\\n        --tensorrt-text-model ${DATAPATH}/deploy/vit-b-16.txt.fp16.trt\n```\n\nThe following lines will be printed in the log output:\n\n```\n[Pytorch image inference speed (batch-size: 1):] mean=11.12ms, sd=0.05ms, min=11.00ms, max=11.32ms, median=11.11ms, 95p=11.20ms, 99p=11.30ms\n[ONNX image inference speed (batch-size: 1):] mean=4.92ms, sd=0.04ms, min=4.82ms, max=5.01ms, median=4.92ms, 95p=4.98ms, 99p=5.00ms\n[TensorRT image inference speed (batch-size: 1):] mean=3.58ms, sd=0.08ms, min=3.30ms, max=3.72ms, median=3.58ms, 95p=3.70ms, 99p=3.72ms\n\n[Pytorch text inference speed (batch-size: 1):] mean=12.47ms, sd=0.07ms, min=12.32ms, max=12.64ms, median=12.48ms, 95p=12.57ms, 99p=12.61ms\n[ONNX text inference speed (batch-size: 1):] mean=3.42ms, sd=0.44ms, min=2.96ms, max=3.89ms, median=3.45ms, 95p=3.87ms, 99p=3.88ms\n[TensorRT text inference speed (batch-size: 1):] mean=1.54ms, sd=0.01ms, min=1.51ms, max=1.57ms, median=1.54ms, 95p=1.56ms, 99p=1.56ms\n```\n\n### Speed Comparison Results\n\nWe present a comparison of the FP16 precision inference time for each size of Pytorch, ONNX, and TensorRT models for an inference batch size of 1, and we can see that TensorRT has a particularly significant speedup for small-scale models.\n\n<table border=\"1\" width=\"120%\">\n    <tr align=\"center\">\n        <th>Unit: ms/sample</th><th colspan=\"3\">Image Feature Extraction</th><th colspan=\"3\">Text Feature Extraction</th>\n    </tr>\n    <tr align=\"center\">\n        <td>Models</td><td>Pytorch</td><td>ONNX</td><td>TensorRT</td><td>Pytorch</td><td>ONNX</td><td>TensorRT</td>\n    </tr>\n\t<tr align=\"center\">\n        <td width=\"120%\">CN-CLIP<sub>RN50</sub></td><td>12.93</td><td>5.04</td><td><b>1.36</b></td><td>3.64</td><td>0.95</td><td><b>0.58</b></td>\n    </tr>  \n\t<tr align=\"center\">\n        <td width=\"120%\">CN-CLIP<sub>ViT-B/16</sub></td><td>11.12</td><td>4.92</td><td><b>3.58</b></td><td>12.47</td><td>3.42</td><td><b>1.54</b></td>\n    </tr>  \n\t<tr align=\"center\">\n        <td width=\"120%\">CN-CLIP<sub>ViT-L/14</sub></td><td>21.19</td><td>17.10</td><td><b>13.08</b></td><td>12.45</td><td>3.48</td><td><b>1.52</b></td>\n    </tr>\n\t<tr align=\"center\">\n        <td width=\"120%\">CN-CLIP<sub>ViT-L/14@336px</sub></td><td>47.11</td><td>48.40</td><td><b>31.59</b></td><td>12.24</td><td>3.25</td><td><b>1.54</b></td>\n    </tr>\n\t<tr align=\"center\">\n        <td width=\"120%\">CN-CLIP<sub>ViT-H/14</sub></td><td>35.10</td><td>34.00</td><td><b>26.98</b></td><td>23.98</td><td>6.01</td><td><b>3.89</b></td>\n    </tr>  \n</table>\n<br>\n\n## Comparison of Downstream Tasks\n\nWe observe the zero-shot performance of the Pytorch, ONNX, and TensorRT FP16 models in the MUGE text-to-image retrieval task involved in the Chinese-CLIP experiments. As described in [Inference and Evaluation](https://github.com/OFA-Sys/Chinese-CLIP/blob/master/README_En.md#inference-and-evaluation) section of Readme, the results of MUGE image & text retrieval evaluation are divided into 3 steps: image & text feature extraction, KNN retrieval, and Recall calculation. The image & text feature extraction scripts for ONNX and TensorRT models, please see `cn_clip/eval/extract_features_onnx.py` and `cn_clip/eval/extract_features_tensorrt.py` respectively, compared with `extract_features.py` used for original Pytorch feature extraction, only minor changes have been made. The scripts and processes used for the subsequent KNN and Recall calculations remain exactly the same.\n\nThe results of ViT-B-16 and ViT-H-14 scales of Chinese-CLIP are compared as follows:\n<table border=\"1\" width=\"100%\">\n    <tr align=\"center\">\n        <th>Setup</th><th colspan=\"4\">ViT-B-16 Zero-shot</th><th colspan=\"4\">ViT-H-14 Zero-shot</th>\n    </tr>\n    <tr align=\"center\">\n        <td>Metric</td><td>R@1</td><td>R@5</td><td>R@10</td><td>MR</td><td>R@1</td><td>R@5</td><td>R@10</td><td>MR</td>\n    </tr>\n\t<tr align=\"center\">\n        <td width=\"120%\">Pytorch FP16</sub></td><td>52.1</td><td>76.7</td><td>84.4</td><td>71.1</td><td>63.0</td><td>84.1</td><td>89.2</td><td>78.8</td>\n    </tr>  \n\t<tr align=\"center\">\n        <td width=\"120%\">ONNX FP16</sub></td><td>52.0</td><td>76.8</td><td>84.3</td><td>71.1</td><td>63.1</td><td>84.1</td><td>89.0</td><td>78.8</td>\n    </tr>\n\t<tr align=\"center\">\n        <td width=\"120%\">TensorRT FP16</sub></td><td>52.0</td><td>76.8</td><td>84.2</td><td>71.0</td><td>63.1</td><td>84.2</td><td>89.1</td><td>78.8</td>\n    </tr>\n</table>\n<br>\n\nThe results are basically the same, with a difference of Â±0.2 within an acceptable range (the magnitude of error that can be caused by changing a machine), which proves the correctness of the conversion of the ONNX and TensorRT models.\n"
        },
        {
          "name": "distillation.md",
          "type": "blob",
          "size": 4.16015625,
          "content": "[**ä¸­æ–‡è¯´æ˜**](distillation.md) | [**English**](distillation_En.md)\n\n# ä½¿ç”¨çŸ¥è¯†è’¸é¦æå‡Chinese-CLIPå›¾åƒæ£€ç´¢èƒ½åŠ›\n\næœ¬æ–‡æ¡£æä¾›äº†ä¸€ä¸ªç»“åˆModelScopeæ¨¡å‹åº“ï¼Œæ”¯æŒChinese-CLIPåˆ©ç”¨çŸ¥è¯†è’¸é¦çš„æ–¹æ³•è¿›è¡Œå¾®è°ƒè®­ç»ƒçš„ç¤ºä¾‹ã€‚é€šè¿‡çŸ¥è¯†è’¸é¦çš„åŠŸèƒ½ï¼Œå¯ä»¥ä½¿ç”¨å¤§æ¨¡å‹ï¼ˆå¦‚è¾ƒå¤§è§„æ¨¡ç‰ˆæœ¬çš„Chinese-CLIPæˆ–å…¶ä»–ModelScopeæ”¯æŒçš„å›¾åƒè¡¨å¾æ¨¡å‹ï¼‰è’¸é¦è¾ƒå°è§„æ¨¡Chinese-CLIPï¼Œè¿›ä¸€æ­¥æå‡Chinese-CLIPçš„å›¾åƒæ£€ç´¢ï¼ˆå›¾åˆ°å›¾å¬å›ï¼‰èƒ½åŠ›ã€‚ä½¿ç”¨çš„Teacher modelç”±[ModelScope](https://github.com/modelscope/modelscope)æä¾›ï¼ŒChinese-CLIPå…¨ç³»åˆ—ç›®å‰å‡å·²ä¸Šçº¿ModelScopeã€‚\n\n## ç¯å¢ƒå‡†å¤‡\n\n+ **Turing**ã€**Ampere**ã€**Ada**ã€**Hopper**æ¶æ„çš„Nvidia GPUæ˜¾å¡ï¼ˆå¦‚H100ã€A100ã€RTX 3090ã€T4ã€RTX 2080ï¼‰ï¼ŒNvidiaå„æ¶æ„å¯¹åº”æ˜¾å¡å‹å·å¯å‚è§[æ­¤æ–‡æ¡£è¡¨æ ¼](https://en.wikipedia.org/wiki/CUDA#GPUs_supported)ã€‚\n+ CUDA 11.4åŠä»¥ä¸Šç‰ˆæœ¬ã€‚\n+ Pytorch 1.12åŠä»¥ä¸Šç‰ˆæœ¬ã€‚\n+ [requirements.txt](requirements.txt)è¦æ±‚çš„å…¶ä»–ä¾èµ–é¡¹\n+ **ModelScope**ï¼šé€šè¿‡æ‰§è¡Œ`pip install modelscope`å®‰è£…ModelScopeã€‚\n\n## åœ¨Chinese-CLIPä¸­ç”¨èµ·æ¥ï¼\n\nåœ¨Chinese-CLIP finetuneä¸­å¯¹äºå›¾åƒç«¯åº”ç”¨çŸ¥è¯†è’¸é¦å¹¶ä¸å¤æ‚ã€‚åªéœ€è¦åœ¨finetuneçš„shè„šæœ¬ä¸­åŠ å…¥`--distillation`é…ç½®é¡¹ã€‚\nç„¶ååœ¨é…ç½®é¡¹`--teacher-model-name`å¡«å…¥æ‰€è¦ä½¿ç”¨çš„Teacher modelåç§°ã€‚ç°åœ¨æ”¯æŒçš„Teacher modelåŒ…æ‹¬ä»¥ä¸‹å››ç§ã€‚\n<table border=\"1\" width=\"120%\">\n    <tr align=\"center\">\n        <td><b>Teacher model</b></td><td><b>æ¨¡å‹ä»‹ç»</b></td>\n    </tr>  \n\t<tr align=\"center\">\n        <td>damo/multi-modal_clip-vit-huge-patch14_zh</td><td><a href=\"https://www.modelscope.cn/models/damo/multi-modal_clip-vit-huge-patch14_zh/summary\">CLIPæ¨¡å‹-ä¸­æ–‡-é€šç”¨é¢†åŸŸ-huge</a></td>\n    </tr>\n\t<tr align=\"center\">\n        <td>damo/multi-modal_clip-vit-large-patch14_zh</td><td><a href=\"https://www.modelscope.cn/models/damo/multi-modal_clip-vit-large-patch14_zh/summary\">CLIPæ¨¡å‹-ä¸­æ–‡-é€šç”¨é¢†åŸŸ-large</a></td>\n    </tr>\t    \n    </tr>\n\t<tr align=\"center\">\n        <td>damo/multi-modal_team-vit-large-patch14_multi-modal-similarity</td><td><a href=\"https://www.modelscope.cn/models/damo/multi-modal_team-vit-large-patch14_multi-modal-similarity/summary\">TEAMå›¾æ–‡æ£€ç´¢æ¨¡å‹-ä¸­æ–‡-large</a></td>\n    </tr>  \n\t<tr align=\"center\">\n        <td>damo/multi-modal_rleg-vit-large-patch14</td><td><a href=\"https://www.modelscope.cn/models/damo/multi-modal_rleg-vit-large-patch14/summary\">RLEGç”Ÿæˆå¼å¤šæ¨¡æ€è¡¨å¾æ¨¡å‹-è‹±æ–‡-large</a></td>\n</table>\n<br>\n\næœ€ååœ¨é…ç½®é¡¹`--kd_loss_weight`å¡«å…¥è’¸é¦æŸå¤±çš„æƒå€¼ï¼Œé»˜è®¤å€¼æ˜¯0.5ã€‚\n\n\nå…¶ä¸­å„é…ç½®é¡¹å®šä¹‰å¦‚ä¸‹ï¼š\n+ `distillation`: æ˜¯å¦å¯ç”¨çŸ¥è¯†è’¸é¦å¾®è°ƒæ¨¡å‹å›¾åƒç«¯ã€‚\n+ `teacher-model-name`: æŒ‡å®šä½¿ç”¨çš„Teacher modelã€‚ç›®å‰æ”¯æŒä»¥ä¸Šå››ä¸ªTeacher modelï¼Œå¦‚å¡«å…¥`damo/multi-modal_team-vit-large-patch14_multi-modal-similarity`ã€‚\n+ `kd_loss_weight`ï¼ˆå¯é€‰ï¼‰: è’¸é¦æŸå¤±çš„æƒå€¼ï¼Œé»˜è®¤å€¼æ˜¯0.5ã€‚\n\næˆ‘ä»¬æä¾›äº†æ ·ä¾‹è„šæœ¬`run_scripts/muge_finetune_vit-b-16_rbt-base_distillation.sh`ï¼Œä½¿ç”¨çš„æ˜¯`TEAMå›¾æ–‡æ£€ç´¢æ¨¡å‹-ä¸­æ–‡-large`ä½œä¸ºTeacher modelã€‚\n\n## æ•ˆæœéªŒè¯\nè¿™é‡Œæ˜¯æˆ‘ä»¬æ¨¡å‹(finetune+distillation) vs é¢„è®­ç»ƒæ¨¡å‹ vs finetuneæ¨¡å‹çš„å›¾åƒæ£€ç´¢Top10ç»“æœã€‚å·¦ä¸Šè§’å›¾åƒä½œä¸ºqueryï¼Œå³è¾¹æŒ‰é¡ºåºTop1åˆ°Top10æ£€ç´¢ç»“æœã€‚æœ¬æ¬¡å®éªŒçš„supportæ•°æ®é›†æœ‰10ä¸‡ç”µå•†æ•°æ®é‡ï¼ˆåŒ…æ‹¬é‹å­ã€è¡£æœã€è£¤å­ç­‰ç‰©å“ï¼‰ã€‚\n\næˆ‘ä»¬æ–¹æ³•çš„ä¼˜åŠ¿ï¼š\n+ ç¬¦åˆæ£€ç´¢ä»»åŠ¡åŸºæœ¬è¦æ±‚ï¼šåœ¨ä¿è¯äº†ç±»ç›®ç›¸ä¼¼æ€§çš„å‰æä¸‹ï¼Œå¾ˆå¥½å®ç°äº†å›¾åƒç›¸ä¼¼æ€§ã€‚\n+ æ€§èƒ½å¥½ä¸”é€Ÿåº¦å¿«ï¼šé€šè¿‡è’¸é¦çš„æ–¹æ³•ï¼Œä½¿å¾—baseæ¨¡å‹æœ‰ç€largeæ¨¡å‹ç±»ä¼¼çš„æ£€ç´¢æ•ˆæœã€‚å¹¶ä¸”éƒ¨ç½²åˆ°CPUï¼Œæ£€ç´¢æ¨ç†æ—¶é—´æ§åˆ¶åœ¨äº†100msä»¥å†…ã€‚\n\n<p style=\"text-align: center;\">\n    <img src=\"examples/image_retrieval_result1.jpg\" width=\"400\" /><br>\n    <img src=\"examples/image_retrieval_result3.jpg\" width=\"400\" /><br>\n    <img src=\"examples/image_retrieval_result2.jpg\" width=\"400\" /><br>\n</p>\n\n\n## å¿«é€Ÿä½“éªŒ\nç›¸å…³è§£å†³æ–¹æ¡ˆå·²ç»ä¸Šçº¿é˜¿é‡Œäº‘[PAI-DSW Gallery](https://gallery.pai-ml.com/#/preview/deepLearning/cv/cn_clip_distillation)ã€‚åœ¨PAI-DSW Galleryæä¾›å¯¹åº”çš„Notebookï¼Œæ”¯æŒç”¨æˆ·åˆ©ç”¨è‡ªæœ‰æ•°æ®æ„å»ºä¸“å±æœç´¢æ¨¡å‹ã€‚\n"
        },
        {
          "name": "distillation_En.md",
          "type": "blob",
          "size": 4.720703125,
          "content": "[**ä¸­æ–‡è¯´æ˜**](distillation.md) | [**English**](distillation_En.md)\n\n# Improving Chinese-CLIP Image Retrieval Ability Using Knowledge Distillation\n\nHere we provide an example of knowledge distillation for Chinese-CLIP fine-tuning training, based on [ModelScope](https://github.com/modelscope/modelscope) model library. By using knowledge distillation, smaller Chinese-CLIP models (with better inference speed) can learn from larger models (including larger Chinese-CLIP or other image embedding models on ModelScope) to further improve the image-to-image retrieval ability. The Teacher models used are all from [ModelScope](https://github.com/modelscope/modelscope). Currently, all the Chinese-CLIP have been supported on ModelScope.\n\n## Environmental Preparation\n\n+ Nvidia GPUs **with Turning, Ampere, Ada or Hopper architecture** (such as H100, A100, RTX 3090, T4, and RTX 2080). Please refer to [this document](https://en.wikipedia.org/wiki/CUDA#GPUs_supported) for the corresponding GPUs of each Nvidia architecture.\n+ CUDA 11.4 and above.\n+ PyTorch 1.12 and above.\n+ **ModelScope**ï¼šInstall ModelScope by executing `pip install modelscope`.\n+ Other dependencies as required in [requirements.txt](requirements.txt).\n\n## Use it in Chinese-CLIP!\nIt is not complicated to apply knowledge distillation to the image side in Chinese-CLIP finetune. Just add the `--distillation` configuration item to the sh script of finetune.\nThen fill in the name of the Teacher model to be used in the configuration item `--teacher-model-name`. The currently supported Teacher models include the following four ModelScope-supported models.\n<table border=\"1\" width=\"120%\">\n    <tr align=\"center\">\n        <td><b>Teacher model</b></td><td><b>Model Info</b></td>\n    </tr>  \n\t<tr align=\"center\">\n        <td>damo/multi-modal_clip-vit-huge-patch14_zh</td><td><a href=\"https://www.modelscope.cn/models/damo/multi-modal_clip-vit-huge-patch14_zh/summary\">CLIP model-Chinese-general field-huge</a></td>\n    </tr>\n\t<tr align=\"center\">\n        <td>damo/multi-modal_clip-vit-large-patch14_zh</td><td><a href=\"https://www.modelscope.cn/models/damo/multi-modal_clip-vit-large-patch14_zh/summary\">CLIP model-Chinese-general field-large</a></td>\n    </tr>\t    \n    </tr>\n\t<tr align=\"center\">\n        <td>damo/multi-modal_team-vit-large-patch14_multi-modal-similarity</td><td><a href=\"https://www.modelscope.cn/models/damo/multi-modal_team-vit-large-patch14_multi-modal-similarity/summary\">TEAM image-text retrieval model-Chinese-large</a></td>\n    </tr>  \n\t<tr align=\"center\">\n        <td>damo/multi-modal_rleg-vit-large-patch14</td><td><a href=\"https://www.modelscope.cn/models/damo/multi-modal_rleg-vit-large-patch14/summary\">RLEG Generative Multimodal Representation Model-English-large</a></td>\n</table>\n<br>\n\nFinally, fill in the weight of the distillation loss in the configuration item `--kd_loss_weight`, the default value is 0.5.\n\nThe configuration items are defined as follows:\n+ `distillation`: Whether to enable knowledge distillation to fine-tune the image side of the model.\n+ `teacher-model-name`: Specify the Teacher model to use. Currently supports the above four Teacher models, such as filling in `damo/multi-modal_team-vit-large-patch14_multi-modal-similarity`.\n+ `kd_loss_weight` (optional): Distillation loss weight, default value is 0.5.\n\nWe provide a sample script `run_scripts/muge_finetune_vit-b-16_rbt-base_distillation.sh`, we take the `TEAM image-text retrieval model-Chinese-large` as Teacher model.\n\n## Effect verification\nImage retrieval Top10 results of our model (finetune+distillation) v.s. pre-trained model v.s. finetune model. The image in the upper left corner is used as a query, and the search results are in order from Top1 to Top10 on the right. The support data set in this experiment has 100,000 e-commerce data (including shoes, clothes, pants, etc.).\n\nAdvantages of our approach:\n+ Meet the basic requirements of the retrieval task: under the premise of ensuring the category similarity, the image similarity is well realized.\n+ Good performance and fast speed: Through the distillation method, the base model has a retrieval effect similar to that of the large model. And deployed to the CPU, the retrieval reasoning time is controlled within 100ms.\n\n<p style=\"text-align: center;\">\n    <img src=\"examples/image_retrieval_result1.jpg\" width=\"400\" /><br>\n    <img src=\"examples/image_retrieval_result3.jpg\" width=\"400\" /><br>\n    <img src=\"examples/image_retrieval_result2.jpg\" width=\"400\" /><br>\n</p>\n\n## Quick Start\nA solution of distillation have been launched on Alibaba Cloud [PAI-DSW Gallery](https://gallery.pai-ml.com/#/preview/deepLearning/cv/cn_clip_distillation). The corresponding Jupyter Notebook is provided in PAI-DSW Gallery to support users to build exclusive search models using their own data.\n"
        },
        {
          "name": "examples",
          "type": "tree",
          "content": null
        },
        {
          "name": "flash_attention.md",
          "type": "blob",
          "size": 3.4140625,
          "content": "[**ä¸­æ–‡è¯´æ˜**](flash_attention.md) | [**English**](flash_attention_En.md)\n\n# ä½¿ç”¨FlashAttentionåŠ é€ŸChinese-CLIP\n\nChinese-CLIPè®­ç»ƒç°å·²æ”¯æŒé€šè¿‡[FlashAttention](https://github.com/HazyResearch/flash-attention)åŠ é€Ÿè®­ç»ƒè¿›ç¨‹ã€‚\n\n## ç¯å¢ƒå‡†å¤‡\n\n+ **Turing**ã€**Ampere**ã€**Ada**ã€**Hopper**æ¶æ„çš„Nvidia GPUæ˜¾å¡ï¼ˆå¦‚H100ã€A100ã€RTX 3090ã€T4ã€RTX 2080ï¼‰ï¼ŒNvidiaå„æ¶æ„å¯¹åº”æ˜¾å¡å‹å·å¯å‚è§[æ­¤æ–‡æ¡£è¡¨æ ¼](https://en.wikipedia.org/wiki/CUDA#GPUs_supported)ã€‚\n+ CUDA 11.4åŠä»¥ä¸Šç‰ˆæœ¬ã€‚\n+ Pytorch 1.12åŠä»¥ä¸Šç‰ˆæœ¬ã€‚\n+ **FlashAttention**ï¼šé€šè¿‡æ‰§è¡Œ`pip install flash-attn`å®‰è£…FlashAttentionã€‚\n\næ›´å¤šä¿¡æ¯å¯å‚è§[FlashAttentioné¡¹ç›®ä»“åº“](https://github.com/HazyResearch/flash-attention)ã€‚\n\n## åœ¨Chinese-CLIPä¸­ç”¨èµ·æ¥ï¼\n\nåœ¨Chinese-CLIP finetuneä¸­åº”ç”¨FlashAttentionéå¸¸ç®€å•ï¼Œåªéœ€è¦åœ¨finetuneçš„shè„šæœ¬ä¸­åŠ å…¥`--use-flash-attention`é…ç½®é¡¹å³å¯ã€‚æˆ‘ä»¬æä¾›äº†æ ·ä¾‹è„šæœ¬`run_scripts/muge_finetune_vit-b-16_rbt-base_flashattn.sh`ã€‚\n\n\n## è®­ç»ƒé€Ÿåº¦å’Œæ˜¾å­˜å ç”¨å¯¹æ¯”\n\nå¯ç”¨FlashAttentionå¯åœ¨ä¸å½±å“æ•ˆæœçš„æ¡ä»¶ä¸‹ä¸ºChinese-CLIPçš„finetuneè¿‡ç¨‹æ˜¾è‘—æé€Ÿä»¥åŠé™ä½æ˜¾å­˜å ç”¨ã€‚æˆ‘ä»¬çš„å®éªŒåœ¨ä¸€å°8å¡A100 GPUï¼ˆ80GBæ˜¾å­˜ï¼‰æœºå™¨è¿›è¡Œï¼ŒFlashAttention 0.2.8ï¼ŒPytorch 1.10.1ã€‚\n\næˆ‘ä»¬åˆ†åˆ«åˆ—å‡ºfinetuneè¿‡ç¨‹ä¸­ï¼Œç›¸åŒbatch sizeä¸‹å¯ç”¨FlashAttentionå‰åæ¯ä¸ªè§„æ¨¡æ¨¡å‹çš„FP16ç²¾åº¦finetuneçš„batch timeå’Œæ˜¾å­˜å ç”¨å¯¹æ¯”ï¼Œå¯ä»¥çœ‹åˆ°å¯ç”¨FlashAttentionåï¼Œè®­ç»ƒé€Ÿåº¦æœ‰æ‰€æå‡ï¼Œä¹Ÿæ›´åŠ èŠ‚çº¦æ˜¾å­˜ã€‚å¯¹äºæ›´å¤§è§„æ¨¡æ¨¡å‹çš„è®­ç»ƒé€Ÿåº¦æå‡å’Œæ˜¾å­˜å ç”¨é™ä½æ›´ä¸ºæ˜¾è‘—ã€‚\n\n<table border=\"1\" width=\"120%\">\n    <tr align=\"center\">\n        <th></th><th colspan=\"4\">Batch Time</th>\n    </tr>\n    <th>å•ä½: ç§’/it</th><th>Batch size</th><th>w/o FlashAttention</th><th>w/ FlashAttention</th><th>Speedup</th>\n    </tr>\n    <tr align=\"center\">\n        <td width=\"120%\">CN-CLIP<sub>RN50</sub></td><td>1200*8</td><td>1.710</td><td>1.680</td><td>1.02Ã—</td>\n    </tr>  \n    <tr align=\"center\">\n        <td width=\"120%\">CN-CLIP<sub>ViT-B/16</sub></td><td>450*8</td><td>1.477</td><td>0.960</td><td>1.54Ã—</td>\n    </tr>  \n    <tr align=\"center\">\n        <td width=\"120%\">CN-CLIP<sub>ViT-L/14</sub></td><td>128*8</td><td>1.293</td><td>0.785</td><td>1.65Ã—</td>\n    </tr>\n    <tr align=\"center\">\n        <td width=\"120%\">CN-CLIP<sub>ViT-L/14@336px</sub></td><td>40*8</td><td>1.397</td><td>0.587</td><td>2.38Ã—</td>\n    </tr>\n    <tr align=\"center\">\n        <td width=\"120%\">CN-CLIP<sub>ViT-H/14</sub></td><td>64*8</td><td>1.265</td><td>0.845</td><td>1.50Ã—</td>\n    </tr>  \n</table>\n<br>\n\n<table border=\"1\" width=\"120%\">\n    <tr align=\"center\">\n        <th></th><th colspan=\"4\">æ˜¾å­˜</th>\n    </tr>\n    <th>å•ä½: GB</th><th>Batch size</th><th>w/o FlashAttention</th><th>w/ FlashAttention</th>\n    </tr>\n    <tr align=\"center\">\n        <td width=\"120%\">CN-CLIP<sub>RN50</sub></td><td>1200*8</td><td>79</td><td>75</td>\n    </tr>  \n    <tr align=\"center\">\n        <td width=\"120%\">CN-CLIP<sub>ViT-B/16</sub></td><td>450*8</td><td>80</td><td>56</td>\n    </tr>  \n    <tr align=\"center\">\n        <td width=\"120%\">CN-CLIP<sub>ViT-L/14</sub></td><td>128*8</td><td>77</td><td>50</td>\n    </tr>\n    <tr align=\"center\">\n        <td width=\"120%\">CN-CLIP<sub>ViT-L/14@336px</sub></td><td>40*8</td><td>78</td><td>37</td>\n    </tr>\n    <tr align=\"center\">\n        <td width=\"120%\">CN-CLIP<sub>ViT-H/14</sub></td><td>64*8</td><td>76</td><td>57</td>\n    </tr>  \n</table>\n<br>\n"
        },
        {
          "name": "flash_attention_En.md",
          "type": "blob",
          "size": 3.462890625,
          "content": "[**ä¸­æ–‡è¯´æ˜**](flash_attention.md) | [**English**](flash_attention_En.md)\r\n\r\n# Accelerate Chinese-CLIP with FlashAttention\r\n\r\nChinese-CLIP now supports the acceleration of training process through [FlashAttention](https://github.com/HazyResearch/flash-attention).\r\n\r\n## Environmental Preparation\r\n\r\n+ Nvidia GPUs **with Turning, Ampere, Ada or Hopper architecture** (such as H100, A100, RTX 3090, T4, and RTX 2080). Please refer to [this document](https://en.wikipedia.org/wiki/CUDA#GPUs_supported) for the corresponding GPUs of each Nvidia architecture.\r\n+ CUDA 11.4 and above.\r\n+ PyTorch 1.12 and above.\r\n+ **FlashAttention**ï¼šInstall FlashAttention by executing `pip install flash-attn`.\r\n\r\nPlease refer to the [FlashAttention project repository](https://github.com/HazyResearch/flash-attention) for more information.\r\n\r\n## Use it in Chinese-CLIP!\r\n\r\nApplying FlashAttention to the finetune process of Chinese-CLIP is very simple, just add `--use-flash-attention` to the sh script of finetune. We provide the sample script `run_scripts/muge_finetune_vit-b-16_rbt-base_flashattn.sh`.\r\n\r\n\r\n## Training Speed and Memory Usage Comparison\r\n\r\nEnabling FlashAttention can significantly speed up the finetune process and reduce the memory usage of Chinese-CLIP without affecting the precision. Our experiments are conducted on an 8-card A100 GPU (80GB memory) machineï¼ŒFlashAttention 0.2.8ï¼ŒPytorch 1.10.1.\r\n\r\nWe present the comparison of the batch time and memory usage of FP16 precision finetune for each scale model. The improvement in training speed and reduction in memory usage are more significant for larger models.\r\n\r\n<table border=\"1\" width=\"120%\">\r\n    <tr align=\"center\">\r\n        <th></th><th colspan=\"4\">Batch Time</th>\r\n    </tr>\r\n    <th>Unit: s/it</th><th>Batch size</th><th>w/o FlashAttention</th><th>w/ FlashAttention</th><th>Speedup</th>\r\n    </tr>\r\n    <tr align=\"center\">\r\n        <td width=\"120%\">CN-CLIP<sub>RN50</sub></td><td>1200*8</td><td>1.710</td><td>1.680</td><td>1.02Ã—</td>\r\n    </tr>  \r\n    <tr align=\"center\">\r\n        <td width=\"120%\">CN-CLIP<sub>ViT-B/16</sub></td><td>450*8</td><td>1.477</td><td>0.960</td><td>1.54Ã—</td>\r\n    </tr>  \r\n    <tr align=\"center\">\r\n        <td width=\"120%\">CN-CLIP<sub>ViT-L/14</sub></td><td>128*8</td><td>1.293</td><td>0.785</td><td>1.65Ã—</td>\r\n    </tr>\r\n    <tr align=\"center\">\r\n        <td width=\"120%\">CN-CLIP<sub>ViT-L/14@336px</sub></td><td>40*8</td><td>1.397</td><td>0.587</td><td>2.38Ã—</td>\r\n    </tr>\r\n    <tr align=\"center\">\r\n        <td width=\"120%\">CN-CLIP<sub>ViT-H/14</sub></td><td>64*8</td><td>1.265</td><td>0.845</td><td>1.50Ã—</td>\r\n    </tr>  \r\n</table>\r\n<br>\r\n\r\n<table border=\"1\" width=\"120%\">\r\n    <tr align=\"center\">\r\n        <th></th><th colspan=\"4\">Memory</th>\r\n    </tr>\r\n    <th>Unit: GB</th><th>Batch size</th><th>w/o FlashAttention</th><th>w/ FlashAttention</th>\r\n    </tr>\r\n    <tr align=\"center\">\r\n        <td width=\"120%\">CN-CLIP<sub>RN50</sub></td><td>1200*8</td><td>79</td><td>75</td>\r\n    </tr>  \r\n    <tr align=\"center\">\r\n        <td width=\"120%\">CN-CLIP<sub>ViT-B/16</sub></td><td>450*8</td><td>80</td><td>56</td>\r\n    </tr>  \r\n    <tr align=\"center\">\r\n        <td width=\"120%\">CN-CLIP<sub>ViT-L/14</sub></td><td>128*8</td><td>77</td><td>50</td>\r\n    </tr>\r\n    <tr align=\"center\">\r\n        <td width=\"120%\">CN-CLIP<sub>ViT-L/14@336px</sub></td><td>40*8</td><td>78</td><td>37</td>\r\n    </tr>\r\n    <tr align=\"center\">\r\n        <td width=\"120%\">CN-CLIP<sub>ViT-H/14</sub></td><td>64*8</td><td>76</td><td>57</td>\r\n    </tr>  \r\n</table>\r\n<br>\r\n"
        },
        {
          "name": "requirements.txt",
          "type": "blob",
          "size": 0.0546875,
          "content": "numpy\ntqdm\nsix\ntimm\nlmdb==1.3.0\ntorch>=1.7.1\ntorchvision"
        },
        {
          "name": "run_scripts",
          "type": "tree",
          "content": null
        },
        {
          "name": "setup.py",
          "type": "blob",
          "size": 1.658203125,
          "content": "import os\n\nimport pkg_resources\nfrom setuptools import setup, find_packages\n\npackages = find_packages(exclude=[\"tests*\"])\nwith open('README_En.md', 'r', encoding='utf-8') as fp:\n    long_description = fp.read()\nsetup(\n    name=\"cn_clip\",\n    py_modules=[\"cn_clip\"],\n    version=\"1.5.1\",\n    author=\"OFA-Sys\",\n    author_email=\"\",\n    long_description=long_description,\n    long_description_content_type=\"text/markdown\",\n    packages = packages,\n    keywords='clip',\n    install_requires=[\n        str(r)\n        for r in pkg_resources.parse_requirements(\n            open(os.path.join(os.path.dirname(__file__), \"requirements.txt\"))\n        )\n    ],\n    data_files=[('clip/model_configs', ['cn_clip/clip/model_configs/RoBERTa-wwm-ext-base-chinese.json',\n                                        'cn_clip/clip/model_configs/RoBERTa-wwm-ext-large-chinese.json',\n                                        'cn_clip/clip/model_configs/ViT-B-16.json',\n                                        'cn_clip/clip/model_configs/ViT-B-32.json',\n                                        'cn_clip/clip/model_configs/ViT-L-14.json',\n                                        'cn_clip/clip/model_configs/ViT-L-14-336.json',\n                                        'cn_clip/clip/model_configs/ViT-H-14.json',\n                                        'cn_clip/clip/model_configs/RN50.json',\n                                        'cn_clip/clip/model_configs/RBT3-chinese.json'\n                                        ]),\n                ('clip/', ['cn_clip/clip/vocab.txt'])\n                ],\n    include_package_data=True,\n    url='https://github.com/OFA-Sys/Chinese-CLIP',\n    description='the Chinese version of CLIP.'\n)\n"
        },
        {
          "name": "zeroshot_dataset.md",
          "type": "blob",
          "size": 3.51953125,
          "content": "[**ä¸­æ–‡è¯´æ˜**](zeroshot_dataset.md) | [**English**](zeroshot_dataset_en.md)\n\n# é›¶æ ·æœ¬å›¾åƒåˆ†ç±»æ•°æ®é›†\n\næœ¬æ•°æ®é›†ä¸º[ELEVATER Benchmark](https://eval.ai/web/challenges/challenge-page/1832)çš„å›¾åƒåˆ†ç±»åŸºå‡†çš„**ä¸­æ–‡ç‰ˆ**ï¼Œå…±åŒ…æ‹¬20ä¸ªå›¾åƒåˆ†ç±»æ•°æ®é›†ï¼ŒåŒ…æ‹¬Caltech-101ã€CIFAR-10ã€CIFAR-100ã€MNISTç­‰ã€‚æˆ‘ä»¬æä¾›æ•´ç†å¥½çš„æ•°æ®é›†ï¼Œå¯ä»¥ç›´æ¥æ¥å…¥Chinese CLIPçš„ä»£ç è¿›è¡Œé›¶æ ·æœ¬åˆ†ç±»ã€‚\n\nä¸‹è½½é“¾æ¥ï¼š[ç‚¹å‡»è¿™é‡Œ](https://clip-cn-beijing.oss-cn-beijing.aliyuncs.com/datasets/ELEVATER_all.zip)\n\nImageNetçš„åŸå§‹æ•°æ®è¯·è‡ªè¡Œåˆ°å®˜ç½‘ä¸‹è½½ï¼ˆå¯å‚è€ƒ[æ­¤æ–‡æ¡£](https://gist.github.com/antoinebrl/7d00d5cb6c95ef194c737392ef7e476a)ä¸‹è½½å¹¶å°†éªŒè¯é›†è½¬ä¸ºImageFolderæ ¼å¼ï¼‰ï¼Œæœ¬é¡¹ç›®ä»…æä¾›[ä¸­æ–‡æ ‡ç­¾](http://clip-cn-beijing.oss-cn-beijing.aliyuncs.com/datasets/ImageNet-1K/label_cn.txt)å’Œ[è‹±æ–‡æ ‡ç­¾](http://clip-cn-beijing.oss-cn-beijing.aliyuncs.com/datasets/ImageNet-1K/label.txt)ã€‚\n\n\n## æ•°æ®é›†è¯´æ˜\næˆ‘ä»¬å°†20ä¸ªæ•°æ®é›†åˆ†åˆ«ç½®äº20ä¸ªæ–‡ä»¶å¤¹ä¸­ï¼Œç»Ÿä¸€æ‰“åŒ…ä¸Šä¼ ï¼Œç”¨æˆ·é€šè¿‡ç‚¹å‡»ä¸Šè¿°é“¾æ¥å³å¯ä¸‹è½½å…¨éƒ¨æ•°æ®ã€‚`ELEVATER_all.zip`è§£å‹åï¼Œå°†å¾—åˆ°æ¯ä¸ªæ•°æ®é›†çš„zipå‹ç¼©åŒ…ã€‚é€‰æ‹©å¯¹åº”çš„å‹ç¼©åŒ…å†æ¬¡è§£å‹åï¼Œæ¯ä¸ªæ–‡ä»¶å¤¹çš„å†…å®¹å¦‚ä¸‹æ‰€ç¤ºï¼š\n```\n${dataset_name}\nâ”œâ”€â”€ index.json  # ä¸ªåˆ«æ•°æ®é›†åŒ…å«è¿™ä¸ªæ–‡ä»¶ï¼Œä»…ç”¨äºæäº¤ELEVATER benchmark\nâ”œâ”€â”€ label_cn.txt  # ä¸­æ–‡æ ‡ç­¾åæ–‡ä»¶ï¼Œæ¯ä¸€è¡Œä¸€ä¸ªç±»åˆ«å\nâ”œâ”€â”€ label.txt  # è‹±æ–‡æ ‡ç­¾åæ–‡ä»¶ï¼Œæ¯ä¸€è¡Œä¸€ä¸ªç±»åˆ«å\nâ”œâ”€â”€ test/\nâ”‚Â Â  â”œâ”€â”€ 000/\nâ”‚Â Â  â”œâ”€â”€ 001/\nâ”‚Â Â  â””â”€â”€ 002/\nâ””â”€â”€ train/\n    â”œâ”€â”€ 000/\n    â”œâ”€â”€ 001/\n    â””â”€â”€ 002/\n```\n`${dataset_name}`è¡¨ç¤ºæ¯ä¸ªæ•°æ®é›†çš„æ–‡ä»¶å¤¹è·¯å¾„ï¼Œå¦‚`cifar-100`ï¼Œé‡Œé¢åŒ…æ‹¬`train`å’Œ`test`ä¸¤ä¸ªæ–‡ä»¶å¤¹ï¼Œæ¯ä¸ªæ–‡ä»¶å¤¹åŒ…å«äº†ä»¥idç¼–å·å‘½åçš„æ–‡ä»¶å¤¹ï¼Œåˆ†åˆ«ä»£è¡¨æ¯ä¸€ä¸ªç±»åˆ«ã€‚å¦å¤–è¿˜åŒ…å«3ä¸ªæ–‡ä»¶ï¼Œåˆ†åˆ«ä¸ºä¸­æ–‡æ ‡ç­¾åæ–‡ä»¶`label_cn.txt`å’Œè‹±æ–‡æ ‡ç­¾åæ–‡ä»¶`label.txt`ã€‚å…¶ä¸­ï¼š\n\n* ç±»åˆ«æ•°åœ¨10ä¸ªåŠä»¥ä¸‹çš„æƒ…å†µä¸‹ï¼Œå¦‚10ï¼Œç±»åˆ«çš„idåˆ†åˆ«ä¸º[0-9]\n* ç±»åˆ«æ•°åœ¨10ä¸ªä»¥ä¸Šçš„æƒ…å†µä¸‹ï¼Œå¦‚100ï¼Œç±»åˆ«çš„idåˆ†åˆ«ä¸º[000-099]ï¼Œå³å‘å·¦è¡¥é›¶åˆ°3ä½æ•°ã€‚è¿™æ˜¯ä¸ºäº†ä¿è¯æˆ‘ä»¬çš„idæ˜¯ä»¥å­—å…¸åºè¿›è¡Œæ’åº\n* æ¯ä¸ªidå¯¹åº”çš„ç±»åˆ«æ ‡ç­¾åä¸ºæ ‡ç­¾æ–‡ä»¶ä¸­çš„ç¬¬${id}è¡Œï¼ˆ0-indexï¼‰ï¼Œå¦‚`0`å³å¯¹åº”æ ‡ç­¾æ–‡ä»¶ä¸­çš„ç¬¬0è¡Œçš„ç±»åˆ«åï¼Œ`099`å¯¹åº”çš„æ˜¯æ ‡ç­¾æ–‡ä»¶çš„ç¬¬99è¡Œç±»åˆ«åã€‚\n\nè®­ç»ƒå’Œæµ‹è¯•é›†æ–‡ä»¶å¤¹å†…åŒ…å«çš„å­æ–‡ä»¶å¤¹ç”¨å­—å…¸åºæ’åºçš„åŸå› æ˜¯å› ä¸ºæˆ‘ä»¬çš„ä»£ç ä½¿ç”¨äº†torchvisionçš„datasetï¼Œé»˜è®¤æ–‡ä»¶å¤¹å†…æ•°æ®æŒ‰ç…§ç±»åˆ«å½’ç±»å­æ–‡ä»¶å¤¹ï¼ŒæŒ‰ç…§æ–‡ä»¶åä»¥å­—å…¸åºæ’åºã€‚\n\næ ‡ç­¾æ–‡ä»¶åŒ…å«ä¸­æ–‡ç‰ˆå’ŒåŸç‰ˆä¸¤ä¸ªæ–‡ä»¶ï¼Œæˆ‘ä»¬çš„ä»£ç ä»…éœ€ä½¿ç”¨`label_cn.txt`ï¼Œ`label.txt`ä»…ä¾›å‚è€ƒã€‚æ–‡ä»¶å†…å®¹ä¸ºæ¯ä¸€è¡Œ1ä¸ªç±»åˆ«åï¼Œç¤ºä¾‹å¦‚ä¸‹ï¼š\n```\né£æœº\næ±½è½¦\nâ€¦â€¦\n```\n\n`index.json`ä»…ç”¨äºæäº¤ELEVATER benchmarkä½¿ç”¨ï¼Œä¸”å¹¶éæ¯ä¸ªæ•°æ®é›†éƒ½åŒ…å«æ­¤æ–‡ä»¶ã€‚è¯¥æ–‡ä»¶çš„åŸå› æ˜¯ELEVATERå®˜æ–¹è¯„æµ‹éƒ¨åˆ†æ•°æ®é›†çš„æµ‹è¯•é›†æ ·æœ¬é¡ºåºç»è¿‡è°ƒæ•´ï¼Œå¦‚éœ€ä¿è¯æäº¤ç»“æœæ­£å¸¸éœ€è¦è°ƒæ•´æ ·æœ¬é¡ºåºã€‚å¦‚é‡åˆ°æ•°æ®é›†åŒ…å«æ­¤æ–‡ä»¶ï¼Œåˆ™å¯åœ¨æµ‹è¯•è¿è¡Œå‘½ä»¤ä¸­åŠ ä¸Š` index.json`å³å¯ã€‚\n\nç±»ä¼¼åœ°ï¼Œå¦‚æ‚¨è‡ªè¡Œå‡†å¤‡ImageNetæ•°æ®ï¼Œè¯·å°†ä¸Šè¿°ä¸­æ–‡å’Œè‹±æ–‡æ ‡ç­¾æ–‡ä»¶æ”¾å…¥`${dataset_name}`ï¼Œå¹¶åœ¨å…¶ä¸­åˆ›å»ºç›¸åº”æ–‡ä»¶å¤¹ï¼Œå¦‚`train`å’Œ`test`ï¼Œå°†å›¾ç‰‡æŒ‰ç…§ç±»åˆ«å½’æ¡£å¹¶æ”¾å…¥å¯¹åº”æ–‡ä»¶å¤¹ï¼Œå¹¶ä¿è¯å…¶æŒ‰å­—å…¸åºæ’åºï¼Œå¦‚`000-999`ï¼Œå®ç°çš„æ–‡ä»¶ç»“æ„å’Œä¸Šè¿°ç¤ºä¾‹ä¿æŒä¸€è‡´ï¼Œå³å¯å®ç°é›¶æ ·æœ¬åˆ†ç±»çš„æ•°æ®å‡†å¤‡ã€‚\n"
        },
        {
          "name": "zeroshot_dataset_en.md",
          "type": "blob",
          "size": 3.716796875,
          "content": "[**ä¸­æ–‡è¯´æ˜**](zeroshot_dataset.md) | [**English**](zeroshot_dataset_en.md)\n\n# Zero-shot Image Classification Datasets\n\nThe collection of dataset is the Chinese version of the Image Classification in the Wild in the [ELEVATER Benchmark](https://eval.ai/web/challenges/challenge-page/1832). It consists of 20 datasets, including Caltech-101, CIFAR-10, CIFAR-100, MNIST, etc. We provide our organized datasets, which enable direct usage of our codes on the datasets. \n\nDownload link: [Click here](https://clip-cn-beijing.oss-cn-beijing.aliyuncs.com/datasets/ELEVATER_all.zip)\n\nFor the ImageNet data, please visit the official website ([link](http://image-net.org)). You can refer to [this doc](https://gist.github.com/antoinebrl/7d00d5cb6c95ef194c737392ef7e476a) to prepare the validation set into ImageFolder format. This project only provides the label names in [Chinese](http://clip-cn-beijing.oss-cn-beijing.aliyuncs.com/datasets/ImageNet-1K/label_cn.txt) and [English](http://clip-cn-beijing.oss-cn-beijing.aliyuncs.com/datasets/ImageNet-1K/label.txt).\n\n## Notes\nWe have organized 20 datasets into 20 directories, and zipped and uploaded them. Users can click the link above to download all the datasets. After unzipping the `ELEVATER_all.zip`, you will get the zipped files of each dataset in ELEVATER. Once again after unzipping the dataset zipfile, you will get the dataset directory with the following folder structure:\n```\n${dataset_name}\nâ”œâ”€â”€ index.json  # Some datasets contain this fileï¼Œwhich only serves for the submission to the ELEVATER benchmark\nâ”œâ”€â”€ label_cn.txt  # File of Chinese labelsï¼Œwhere the text in each line refers to the label name\nâ”œâ”€â”€ label.txt  # File of English labelsï¼Œwhere the text in each line refers to the label name\nâ”œâ”€â”€ test/\nâ”‚Â Â  â”œâ”€â”€ 000/\nâ”‚Â Â  â”œâ”€â”€ 001/\nâ”‚Â Â  â””â”€â”€ 002/\nâ””â”€â”€ train/\n    â”œâ”€â”€ 000/\n    â”œâ”€â”€ 001/\n    â””â”€â”€ 002/\n```\n`${dataset_name}` refers to the directory of each dataset, where there are two directories named `train` and `test`. Each directory contains sub-directories named with id, which refers to a category. Additionally, there are 2 files, namely the Chinese label file`label_cn.txt`, the English label file `label.txt`. Note that:\n\n* When the number of labels is no larger than 10, e,g., 10, the ids are [0-9]\n* When the number of labels is larger than 10, e.g., 100, the ids are [000-099]ï¼Œwhich are left padded with 0 to 3-digit numbers. This serves for alphabetic order. \n* Each id refers to the label name in the ${id}-th line of the label file (0-index). For example, `0` refers to the label name in the 0-th line, and `099` refers to the label name in the 99-th line\n\nThe sub-directories of training and test sets are alphatically ordered for the reason that we use `torchvision.dataset`, which requires to organize data to sub-directories based on their labels and alphabetically ordered. \n\nThere are two label files for Chinese and English. We only use `label_cn.txt` in our experiments, and `label.txt` is for reference only. Each line contains a label name. The example is shown below:\n```\né£æœº\næ±½è½¦\nâ€¦â€¦\n```\n\n`index.json` only serves for the submission of the ELEVATER benchmark, and not every dataset contains this file. The existence of this file is due to the specified order of test data. To make your submission available, you need to add ` index.json` in your command. \n\nSimilarly, if you prepare the ImageNet data, please put the label files mentioned above in the directory `${dataset_name}`, and create directories like `train` and `test`, and file the images according to their categories into directories, which should be ordered by the alphabetic order, like `000-999`. The organization should be consistent with the abovementioned example. \n"
        }
      ]
    }
  ]
}