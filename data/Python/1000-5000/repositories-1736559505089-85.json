{
  "metadata": {
    "timestamp": 1736559505089,
    "page": 85,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjkw",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "OFA-Sys/Chinese-CLIP",
      "stars": 4777,
      "defaultBranch": "master",
      "files": [
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 1.9384765625,
          "content": "logs/\nwandb/\nmodels/\nfeatures/\nresults/\n\n# Byte-compiled / optimized / DLL files\n__pycache__/\n*.py[cod]\n*$py.class\n\n# C extensions\n*.so\n\n# Distribution / packaging\n.Python\nbuild/\ndevelop-eggs/\ndist/\ndownloads/\neggs/\n.eggs/\nlib/\nlib64/\nparts/\nsdist/\nvar/\nwheels/\npip-wheel-metadata/\nshare/python-wheels/\n*.egg-info/\n.installed.cfg\n*.egg\nMANIFEST\n\n# PyInstaller\n#  Usually these files are written by a python script from a template\n#  before PyInstaller builds the exe, so as to inject date/other infos into it.\n*.manifest\n*.spec\n\n# Installer logs\npip-log.txt\npip-delete-this-directory.txt\n\n# Unit test / coverage reports\nhtmlcov/\n.tox/\n.nox/\n.coverage\n.coverage.*\n.cache\nnosetests.xml\ncoverage.xml\n*.cover\n*.py,cover\n.hypothesis/\n.pytest_cache/\n\n# Translations\n*.mo\n*.pot\n\n# Django stuff:\n*.log\nlocal_settings.py\ndb.sqlite3\ndb.sqlite3-journal\n\n# Flask stuff:\ninstance/\n.webassets-cache\n\n# Scrapy stuff:\n.scrapy\n\n# Sphinx documentation\ndocs/_build/\n\n# PyBuilder\ntarget/\n\n# Jupyter Notebook\n.ipynb_checkpoints\n\n# IPython\nprofile_default/\nipython_config.py\n\n# pyenv\n.python-version\n\n# pipenv\n#   According to pypa/pipenv#598, it is recommended to include Pipfile.lock in version control.\n#   However, in case of collaboration, if having platform-specific dependencies or dependencies\n#   having no cross-platform support, pipenv may install dependencies that don't work, or not\n#   install all needed dependencies.\n#Pipfile.lock\n\n# PEP 582; used by e.g. github.com/David-OConnor/pyflow\n__pypackages__/\n\n# Celery stuff\ncelerybeat-schedule\ncelerybeat.pid\n\n# SageMath parsed files\n*.sage.py\n\n# Environments\n.env\n.venv\nenv/\nvenv/\nENV/\nenv.bak/\nvenv.bak/\n\n# Spyder project settings\n.spyderproject\n.spyproject\n\n# Rope project settings\n.ropeproject\n\n# mkdocs documentation\n/site\n\n# mypy\n.mypy_cache/\n.dmypy.json\ndmypy.json\n\n# Pyre type checker\n.pyre/\nsync.sh\ngpu1sync.sh\n.idea\n*.pdf\n**/._*\n**/*DS_*\n**.jsonl\nsrc/sbatch\nsrc/misc\n.vscode\nsrc/debug\ncore.*\n\n# Allow\n!src/evaluation/misc/results_dbs/*"
        },
        {
          "name": "MIT-LICENSE.txt",
          "type": "blob",
          "size": 1.234375,
          "content": "Copyright (c) 2012-2022 OFA-Sys Team\n\nCopyright (c) 2012-2022 Gabriel Ilharco, Mitchell Wortsman, Nicholas Carlini, Rohan Taori, Achal Dave, Vaishaal Shankar, John Miller, Hongseok Namkoong, Hannaneh Hajishirzi, Ali Farhadi, Ludwig Schmidt\n\nPermission is hereby granted, free of charge, to any person obtaining\na copy of this software and associated documentation files (the\n\"Software\"), to deal in the Software without restriction, including\nwithout limitation the rights to use, copy, modify, merge, publish,\ndistribute, sublicense, and/or sell copies of the Software, and to\npermit persons to whom the Software is furnished to do so, subject to\nthe following conditions:\n\nThe above copyright notice and this permission notice shall be\nincluded in all copies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND,\nEXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF\nMERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\nNONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE\nLIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION\nOF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION\nWITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 33.9306640625,
          "content": "[**中文说明**](README.md) | [**English**](README_En.md)\r\n\r\n<p align=\"center\">\r\n    <br>\r\n    <img src=\"assets/Chinese_CLIP_logo_tp_path.svg\" width=\"400\" />\r\n    <br>\r\n<p>\r\n<br>\r\n\r\n<p align=\"center\">\r\n        <a href=\"https://www.modelscope.cn/models?name=clip&tasks=multi-modal-embedding\">ModelScope</a>&nbsp ｜ &nbsp<a href=\"https://www.modelscope.cn/studios/damo/chinese_clip_applications/summary\">Demo</a>&nbsp ｜ &nbsp<a href=\"https://arxiv.org/abs/2211.01335\">Paper</a>&nbsp ｜ &nbspBlog\r\n</p>\r\n<br><br>\r\n\r\n本项目为CLIP模型的**中文**版本，使用大规模中文数据进行训练（~2亿图文对），旨在帮助用户快速实现中文领域的[图文特征&相似度计算](#API快速上手)、[跨模态检索](#跨模态检索)、[零样本图片分类](#零样本图像分类)等任务。本项目代码基于<b>[open_clip project](https://github.com/mlfoundations/open_clip)</b>建设，并针对中文领域数据以及在中文数据上实现更好的效果做了优化。本项目提供了API、训练代码和测试代码，下文中将详细介绍细节。\r\n<br><br>\r\n\r\n# 新闻\r\n* 2023.11.30 Chinese-CLIP添加了转换Pytorch模型为coreml格式的[转换脚本](https://github.com/OFA-Sys/Chinese-CLIP/blob/master/cn_clip/deploy/pytorch_to_coreml.py)，用于部署。（感谢[@manymuch](https://github.com/manymuch)贡献代码❤️）\r\n* 2023.9.8 Chinese-CLIP支持了基于[ModelScope](https://github.com/modelscope/modelscope)库的[知识蒸馏微调功能](distillation.md)。（感谢阿里云PAI团队[@wuziheng](https://github.com/wuziheng)和[@Jaskr616](https://github.com/Jaskr616)同学[贡献代码](https://github.com/OFA-Sys/Chinese-CLIP/pull/195)❤️）\r\n* 2023.5.9 Chinese-CLIP适配Pytorch2.0。\r\n* 2023.3.20 新增对比学习的[梯度累积](#gradient_accumulation)支持，可模拟更大batch size的训练效果\r\n* 2023.2.16 新增[FlashAttention](https://github.com/HazyResearch/flash-attention)支持，提升训练速度，降低显存占用，详见[flash_attention.md](flash_attention.md)\r\n* 2023.1.15 新增部署[ONNX](https://onnx.ai/)和[TensorRT](https://developer.nvidia.com/tensorrt)模型支持（并提供预训练TensorRT模型），提升特征推理速度，满足部署需求，详见[deployment.md](deployment.md)\r\n* 2022.12.12 新增实现[FLIP](https://arxiv.org/abs/2212.00794)训练策略，在finetune训练时可[激活使用](#FLIP)（感谢[@zwkkk](https://github.com/zwkkk)同学[贡献代码](https://github.com/OFA-Sys/Chinese-CLIP/pull/26)❤️）\r\n* 2022.12.3 公开[ELEVATER](https://eval.ai/web/challenges/challenge-page/1832)图像分类数据集的中文版本，详见[数据文档](https://github.com/OFA-Sys/Chinese-CLIP/blob/master/zeroshot_dataset.md)\r\n* 2022.12.1 Chinese-CLIP模型代码&特征提取API，同步合入Huggingface transformers🤗代码库\r\n* 2022.11.22 新增[零样本图像分类](#零样本图像分类)代码，可支持[ELEVATER benchmark](https://eval.ai/web/challenges/challenge-page/1832)零样本分类评测任务\r\n* 2022.11.3 新增RN50，ViT-H-14模型，公开[技术报告](https://arxiv.org/pdf/2211.01335.pdf)\r\n* 2022.9.22 新增ViT-L-14，ViT-L-14-336模型\r\n* 2022.7.13 新增[图文特征提取快速API](#API快速上手)，几行代码快速调用中文CLIP模型，计算图文特征&相似度\r\n* 2022.7.8 Chinese-CLIP项目正式开源，开源[图文检索](#跨模态检索)代码\r\n<br><br>\r\n\r\n# 模型及实验\r\n<span id=\"model_card\"></span>\r\n## 模型规模 & 下载链接\r\nChinese-CLIP目前开源5个不同规模，其模型信息和下载方式见下表：\r\n\r\n<table border=\"1\" width=\"100%\">\r\n    <tr align=\"center\">\r\n        <th>模型规模</th><th>下载链接</th><th>参数量</th><th>视觉侧骨架</th><th>视觉侧参数量</th><th>文本侧骨架</th><th>文本侧参数量</th><th>分辨率</th>\r\n    </tr>\r\n    <tr align=\"center\">\r\n        <td>CN-CLIP<sub>RN50</sub></td><td><a href=\"https://clip-cn-beijing.oss-cn-beijing.aliyuncs.com/checkpoints/clip_cn_rn50.pt\">Download</a></td><td>77M</td><td>ResNet50</td><td>38M</td><td>RBT3</td><td>39M</td><td>224</td>\r\n    </tr>\r\n    <tr align=\"center\">\r\n        <td>CN-CLIP<sub>ViT-B/16</sub></td><td><a href=\"https://clip-cn-beijing.oss-cn-beijing.aliyuncs.com/checkpoints/clip_cn_vit-b-16.pt\">Download</a></td><td>188M</td><td>ViT-B/16</td><td>86M</td><td>RoBERTa-wwm-Base</td><td>102M</td><td>224</td>\r\n    </tr>\r\n    <tr align=\"center\">\r\n        <td>CN-CLIP<sub>ViT-L/14</sub></td><td><a href=\"https://clip-cn-beijing.oss-cn-beijing.aliyuncs.com/checkpoints/clip_cn_vit-l-14.pt\">Download</a></td><td>406M</td><td>ViT-L/14</td><td>304M</td><td>RoBERTa-wwm-Base</td><td>102M</td><td>224</td>\r\n    </tr>\r\n    <tr align=\"center\">\r\n        <td>CN-CLIP<sub>ViT-L/14@336px</sub></td><td><a href=\"https://clip-cn-beijing.oss-cn-beijing.aliyuncs.com/checkpoints/clip_cn_vit-l-14-336.pt\">Download</a></td><td>407M</td><td>ViT-L/14</td><td>304M</td><td>RoBERTa-wwm-Base</td><td>102M</td><td>336</td>\r\n    </tr>\r\n    <tr align=\"center\">\r\n        <td>CN-CLIP<sub>ViT-H/14</sub></td><td><a href=\"https://clip-cn-beijing.oss-cn-beijing.aliyuncs.com/checkpoints/clip_cn_vit-h-14.pt\">Download</a></td><td>958M</td><td>ViT-H/14</td><td>632M</td><td>RoBERTa-wwm-Large</td><td>326M</td><td>224</td>\r\n    </tr>\r\n</table>\r\n<br></br>\r\n\r\n## 实验结果\r\n针对图文检索任务，我们在[MUGE Retrieval](https://tianchi.aliyun.com/muge)、[Flickr30K-CN](https://github.com/li-xirong/cross-lingual-cap)和[COCO-CN](https://github.com/li-xirong/coco-cn)上进行了zero-shot和finetune的实验。针对图像零样本分类，我们在[ELEVATER](https://eval.ai/web/challenges/challenge-page/1832)的10个数据集上进行了实验。实验结果如下表所示。篇幅所限，我们这里给出baseline模型和Chinese-CLIP的最优规模模型结果，关于Chinese-CLIP各规模的详细结果指标，请详见[Results.md](Results.md)。\r\n\r\n**MUGE Text-to-Image Retrieval (Official Validation Set)**:\r\n<table border=\"1\" width=\"100%\">\r\n    <tr align=\"center\">\r\n        <th>Setup</th><th colspan=\"4\">Zero-shot</th><th colspan=\"4\">Finetune</th>\r\n    </tr>\r\n    <tr align=\"center\">\r\n        <td>Metric</td><td>R@1</td><td>R@5</td><td>R@10</td><td>MR</td><td>R@1</td><td>R@5</td><td>R@10</td><td>MR</td>\r\n    </tr>\r\n\t<tr align=\"center\">\r\n        <td width=\"120%\">Wukong</td><td>42.7</td><td>69.0</td><td>78.0</td><td>63.2</td><td>52.7</td><td>77.9</td><td>85.6</td><td>72.1</td>\r\n    </tr>\r\n\t<tr align=\"center\">\r\n        <td width=\"120%\">R2D2</td><td>49.5</td><td>75.7</td><td>83.2</td><td>69.5</td><td>60.1</td><td>82.9</td><td>89.4</td><td>77.5</td>\r\n    </tr>\r\n\t<tr align=\"center\">\r\n        <td width=\"120%\">CN-CLIP</td><td>63.0</td><td>84.1</td><td>89.2</td><td>78.8</td><td>68.9</td><td>88.7</td><td>93.1</td><td>83.6</td>\r\n    </tr>\r\n</table>\r\n<br>\r\n\r\n**Flickr30K-CN Retrieval (Official Test Set)**:\r\n<table border=\"1\" width=\"150%\">\r\n\t<tr align=\"center\">\r\n        <th>Task</th><th colspan=\"6\">Text-to-Image</th><th colspan=\"6\">Image-to-Text</th>\r\n    </tr>\r\n    <tr align=\"center\">\r\n        <th>Setup</th><th colspan=\"3\">Zero-shot</th><th colspan=\"3\">Finetune</th><th colspan=\"3\">Zero-shot</th><th colspan=\"3\">Finetune</th>\r\n    </tr>\r\n    <tr align=\"center\">\r\n        <td>Metric</td><td>R@1</td><td>R@5</td><td>R@10</td><td>R@1</td><td>R@5</td><td>R@10</td><td>R@1</td><td>R@5</td><td>R@10</td><td>R@1</td><td>R@5</td><td>R@10</td>\r\n    </tr>\r\n\t<tr align=\"center\">\r\n        <td width=\"120%\">Wukong</td><td>51.7</td><td>78.9</td><td>86.3</td><td>77.4</td><td>94.5</td><td>97.0</td><td>76.1</td><td>94.8</td><td>97.5</td><td>92.7</td><td>99.1</td><td>99.6</td>\r\n    </tr>\r\n\t<tr align=\"center\">\r\n        <td width=\"120%\">Taiyi</td><td>60.8</td><td>85.0</td><td>91.0</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td>\r\n    </tr>\t\r\n\t<tr align=\"center\">\r\n        <td width=\"120%\">R2D2</td><td>60.9</td><td>86.8</td><td>92.7</td><td>84.4</td><td>96.7</td><td>98.4</td><td>77.6</td><td>96.7</td><td>98.9</td><td>95.6</td><td>99.8</td><td>100.0</td>\r\n    </tr>\t\r\n\t<tr align=\"center\">\r\n        <td width=\"120%\">CN-CLIP</td><td>71.2</td><td>91.4</td><td>95.5</td><td>83.8</td><td>96.9</td><td>98.6</td><td>81.6</td><td>97.5</td><td>98.8</td><td>95.3</td><td>99.7</td><td>100.0</td>\r\n    </tr>\r\n</table>\r\n<br>\r\n\r\n**COCO-CN Retrieval (Official Test Set)**:\r\n<table border=\"1\" width=\"150%\">\r\n\t<tr align=\"center\">\r\n        <th>Task</th><th colspan=\"6\">Text-to-Image</th><th colspan=\"6\">Image-to-Text</th>\r\n    </tr>\r\n    <tr align=\"center\">\r\n        <th>Setup</th><th colspan=\"3\">Zero-shot</th><th colspan=\"3\">Finetune</th><th colspan=\"3\">Zero-shot</th><th colspan=\"3\">Finetune</th>\r\n    </tr>\r\n    <tr align=\"center\">\r\n        <td>Metric</td><td>R@1</td><td>R@5</td><td>R@10</td><td>R@1</td><td>R@5</td><td>R@10</td><td>R@1</td><td>R@5</td><td>R@10</td><td>R@1</td><td>R@5</td><td>R@10</td>\r\n    </tr>\r\n\t<tr align=\"center\">\r\n        <td width=\"150%\">Wukong</td><td>53.4</td><td>80.2</td><td>90.1</td><td>74.0</td><td>94.4</td><td>98.1</td><td>55.2</td><td>81.0</td><td>90.6</td><td>73.3</td><td>94.0</td><td>98.0</td>\r\n    </tr>\r\n\t<tr align=\"center\">\r\n        <td width=\"150%\">Taiyi</td><td>60.0</td><td>84.0</td><td>93.3</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td>\r\n    </tr>\t\r\n\t<tr align=\"center\">\r\n        <td width=\"150%\">R2D2</td><td>56.4</td><td>85.0</td><td>93.1</td><td>79.1</td><td>96.5</td><td>98.9</td><td>63.3</td><td>89.3</td><td>95.7</td><td>79.3</td><td>97.1</td><td>98.7</td>\r\n    </tr>\r\n\t<tr align=\"center\">\r\n        <td width=\"150%\">CN-CLIP</td><td>69.2</td><td>89.9</td><td>96.1</td><td>81.5</td><td>96.9</td><td>99.1</td><td>63.0</td><td>86.6</td><td>92.9</td><td>83.5</td><td>97.3</td><td>99.2</td>\r\n    </tr>\r\n</table>\r\n<br>\r\n\r\n**Zero-shot Image Classification**:\r\n<table border=\"1\" width=\"150%\">\r\n\t<tr align=\"center\">\r\n        <th>Task</th><th>CIFAR10</th><th>CIFAR100</th><th>DTD</th><th>EuroSAT</th><th>FER</th><th>FGVC</th><th>KITTI</th><th>MNIST</th><th>PC</th><th>VOC</th>\r\n    </tr>\r\n\t<tr align=\"center\">\r\n        <td width=\"150%\">GIT</td><td>88.5</td><td>61.1</td><td>42.9</td><td>43.4</td><td>41.4</td><td>6.7</td><td>22.1</td><td>68.9</td><td>50.0</td><td>80.2</td>\r\n    </tr>\r\n    \t<tr align=\"center\">\r\n        <td width=\"150%\">ALIGN</td><td>94.9</td><td>76.8</td><td>66.1</td><td>52.1</td><td>50.8</td><td>25.0</td><td>41.2</td><td>74.0</td><td>55.2</td><td>83.0</td>\r\n    </tr>\r\n\t<tr align=\"center\">\r\n        <td width=\"150%\">CLIP</td><td>94.9</td><td>77.0</td><td>56.0</td><td>63.0</td><td>48.3</td><td>33.3</td><td>11.5</td><td>79.0</td><td>62.3</td><td>84.0</td>\r\n    </tr>\r\n    \t<tr align=\"center\">\r\n        <td width=\"150%\">Wukong</td><td>95.4</td><td>77.1</td><td>40.9</td><td>50.3</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td>\r\n    </tr>\r\n    \t<tr align=\"center\">\r\n        <td width=\"150%\">CN-CLIP</td><td>96.0</td><td>79.7</td><td>51.2</td><td>52.0</td><td>55.1</td><td>26.2</td><td>49.9</td><td>79.4</td><td>63.5</td><td>84.9</td>\r\n    </tr>\r\n</table>\r\n\r\n<br><br>\r\n\r\n\r\n# 开始用起来！\r\n## 安装要求\r\n开始本项目前，需先检查是否满足下列环境配置要求:\r\n\r\n* python >= 3.6.4\r\n* pytorch >= 1.8.0 (with torchvision >= 0.9.0)\r\n* CUDA Version >= 10.2\r\n\r\n运行下列命令即可安装本项目所需的三方库。\r\n\r\n```bash\r\npip install -r requirements.txt\r\n```\r\n\r\n## API快速上手\r\n下面提供一段简单的代码示例说明如何使用中文CLIP的API。开始使用前，请先安装cn_clip：\r\n```bash\r\n# 通过pip安装\r\npip install cn_clip\r\n\r\n# 或者从源代码安装\r\ncd Chinese-CLIP\r\npip install -e .\r\n```\r\n安装成功后，即可通过如下方式轻松调用API，传入指定图片（[示例](examples/pokemon.jpeg)）和文本，提取图文特征向量并计算相似度：\r\n```python\r\nimport torch \r\nfrom PIL import Image\r\n\r\nimport cn_clip.clip as clip\r\nfrom cn_clip.clip import load_from_name, available_models\r\nprint(\"Available models:\", available_models())  \r\n# Available models: ['ViT-B-16', 'ViT-L-14', 'ViT-L-14-336', 'ViT-H-14', 'RN50']\r\n\r\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\r\nmodel, preprocess = load_from_name(\"ViT-B-16\", device=device, download_root='./')\r\nmodel.eval()\r\nimage = preprocess(Image.open(\"examples/pokemon.jpeg\")).unsqueeze(0).to(device)\r\ntext = clip.tokenize([\"杰尼龟\", \"妙蛙种子\", \"小火龙\", \"皮卡丘\"]).to(device)\r\n\r\nwith torch.no_grad():\r\n    image_features = model.encode_image(image)\r\n    text_features = model.encode_text(text)\r\n    # 对特征进行归一化，请使用归一化后的图文特征用于下游任务\r\n    image_features /= image_features.norm(dim=-1, keepdim=True) \r\n    text_features /= text_features.norm(dim=-1, keepdim=True)    \r\n\r\n    logits_per_image, logits_per_text = model.get_similarity(image, text)\r\n    probs = logits_per_image.softmax(dim=-1).cpu().numpy()\r\n\r\nprint(\"Label probs:\", probs)  # [[1.268734e-03 5.436878e-02 6.795761e-04 9.436829e-01]]\r\n```\r\n我们也准备了部署ONNX和TensorRT模型的相关支持，流程详见[deployment.md](deployment.md)。\r\n\r\n如果你不满足于仅仅使用API，欢迎继续阅读本文档，了解如何使用我们的项目进行CLIP模型的训练和测试。\r\n<br><br>\r\n\r\n\r\n# 教程\r\n下文将包括[跨模态检索教程](#跨模态检索)（包含finetune和inference，及KNN计算等）以及[零样本图像分类教程](#零样本图像分类)。\r\n\r\n## 跨模态检索\r\n### 代码组织\r\n下载本项目后, 请创建新的文件夹 ```${DATAPATH}``` 以存放数据集、预训练ckpt、以及finetune产生的模型日志&ckpt。推荐工作区目录结构如下：\r\n\r\n```\r\nChinese-CLIP/\r\n├── run_scripts/\r\n│   ├── muge_finetune_vit-b-16_rbt-base.sh\r\n│   ├── flickr30k_finetune_vit-b-16_rbt-base.sh\r\n│   └── ...           # 更多finetune或评测脚本...\r\n└── cn_clip/\r\n    ├── clip/\r\n    ├── eval/\r\n    ├── preprocess/\r\n    └── training/\r\n\r\n${DATAPATH}\r\n├── pretrained_weights/\r\n├── experiments/\r\n├── deploy/\t      # 用于存放ONNX & TensorRT部署模型\r\n└── datasets/\r\n    ├── MUGE/\r\n    ├── Flickr30k-CN/\r\n    └── .../          # 更多自定义数据集...\r\n```\r\n\r\n### 准备工作\r\n这里我们提供预训练模型参数的下载方式，以及进行finetune前对数据进行的预处理过程\r\n\r\n#### 预训练CKPT\r\n\r\n请参考前文[模型规模 & 下载链接](#model_card)部分，下载对应模型ckpt。推荐将下载的ckpt文件存放于`${DATAPATH}/pretrained_weights/`目录下。\r\n\r\n#### 数据集格式预处理\r\n\r\n为了与Chinese-CLIP代码适配，同时保证数据处理和读取的效率，我们建议将训练&评测使用的图文数据集统一组织成如下的方式：\r\n\r\n```\r\n${DATAPATH}\r\n└── datasets/\r\n    └── ${dataset_name}/\r\n        ├── train_imgs.tsv      # 图片id & 图片内容\r\n        ├── train_texts.jsonl   # 文本id & 文本内容，连同匹配的图片id列表\r\n        ├── valid_imgs.tsv\r\n        ├── valid_texts.jsonl\r\n        ├── test_imgs.tsv\r\n        └── test_texts.jsonl\r\n```\r\n其中`${dataset_name}`代指数据集名称（如MUGE）\r\n\r\n为保证文件处理效率，我们不是将图片以大量的小文件方式存放，而是将训练/验证/测试图片以base64形式分别存放在`${split}_imgs.tsv`文件中。文件每行表示一张图片，包含图片id（int型）与图片base64，以tab隔开，格式如下：\r\n```\r\n1000002\t/9j/4AAQSkZJ...YQj7314oA//2Q==\r\n```\r\n\r\n将图片原始文件转换为base64的方式非常简单，请执行以下python代码：\r\n```python\r\nfrom PIL import Image\r\nfrom io import BytesIO\r\nimport base64\r\n\r\nimg = Image.open(file_name) # 访问图片路径\r\nimg_buffer = BytesIO()\r\nimg.save(img_buffer, format=img.format)\r\nbyte_data = img_buffer.getvalue()\r\nbase64_str = base64.b64encode(byte_data) # bytes\r\nbase64_str = base64_str.decode(\"utf-8\") # str\r\n```\r\n\r\n文本信息及图文对匹配关系则保存在`${split}_texts.jsonl`文件。文件每行是一行json，格式如下：\r\n```\r\n{\"text_id\": 8428, \"text\": \"高级感托特包斜挎\", \"image_ids\": [1076345, 517602]}\r\n```\r\n对于测试集只有文本，不知道图文对匹配关系的情况，每行的`image_ids`字段处理为空列表即可，即`\"image_ids\": []`。\r\n\r\n最后，我们还需要将tsv和jsonl文件一起序列化，转换为内存索引的LMDB数据库文件，方便训练时的随机读取\r\n```\r\npython cn_clip/preprocess/build_lmdb_dataset.py \\\r\n    --data_dir ${DATAPATH}/datasets/${dataset_name}\r\n    --splits train,valid,test\r\n```\r\n例如对于MUGE数据集，则`${dataset_name}`设为MUGE，`--splits`指定需要转换的数据集划分，以逗号不加空格分隔。转换后，数据集文件夹下会对应增加以下LMDB序列化文件\r\n```\r\n${DATAPATH}\r\n└── datasets/\r\n    └── ${dataset_name}/\r\n        └── lmdb/\r\n            ├── train\r\n            │   ├── imgs\r\n            │   └── pairs\r\n            ├── valid\r\n            └── test\r\n```\r\n\r\n为了降低上手难度，我们也提供了按上述步骤预处理好的MUGE数据（[下载链接](https://clip-cn-beijing.oss-cn-beijing.aliyuncs.com/datasets/MUGE.zip)）和Flickr30K-CN数据（[下载链接](https://clip-cn-beijing.oss-cn-beijing.aliyuncs.com/datasets/Flickr30k-CN.zip)）压缩包，直接下载解压并放置于`${DATAPATH}/datasets/`目录下即可。如果需要[COCO-CN](https://github.com/li-xirong/coco-cn)数据，请向原作者进行申请许可完成后，邮件联系我们吧。\r\n\r\n### 模型finetune\r\n\r\n在此我们介绍训练的步骤，方便其他用户了解模型细节，使用我们提供的中文CLIP预训练模型进行finetune。基于MUGE和Flickr30K-CN两个下游检索数据集，我们提供了训练样例脚本`run_scripts/muge_finetune_vit-b-16_rbt-base.sh`和`run_scripts/flickr30k_finetune_vit-b-16_rbt-base.sh`。<b>运行脚本同时支持单机（单卡或多卡）和多机分布式训练，请在运行前，先根据脚本开头的指引注释，填写好分布式相关配置，之后运行如下命令即可开始训练（多机训练请在各机器上都运行命令）。对于显存不足的情况，可以考虑激活配置项中的[重计算策略](#checkpointing)。</b>训练产生的log和模型ckpt文件，会自动保存在用户指定的目录下：\r\n\r\n```bash\r\ncd Chinese-CLIP/\r\nbash run_scripts/muge_finetune_vit-b-16_rbt-base.sh ${DATAPATH}\r\n```\r\n\r\n相关的训练配置项包括:\r\n\r\n+ 分布式\r\n  + `WORKER_CNT`: 训练的机器个数\r\n  + `GPUS_PER_NODE`: 每个机器上的GPU个数\r\n+ 训练/验证数据\r\n  + `train-data`: 训练数据LMDB目录，准备LMDB数据文件的预处理流程见上。\r\n  + `val-data`: 验证数据LMDB目录，指定为None时，则不进行训练过程中的验证。\r\n  + `num-workers`: 训练集数据处理（DataLoader）的进程数，默认为4。\r\n  + `valid-num-workers`: 验证集数据处理（DataLoader）的进程数（如果进行验证），默认为1。\r\n+ 训练超参数\r\n  + `vision-model`: 指定视觉backbone, 从 `[\"ViT-B-16\", \"ViT-L-14\", \"ViT-L-14-336\", \"ViT-H-14\", \"RN50\"]`选择。\r\n  + `text-model`: 指定文本backbone, 从 `[\"RoBERTa-wwm-ext-base-chinese\", \"RoBERTa-wwm-ext-large-chinese\", \"RBT3-chinese\"]`选择。\r\n  + `context-length`: 文本输入序列长度。\r\n  + `warmup`: warmup步数。\r\n  + `batch-size`: 训练时单卡batch-size。（请保证`训练样本总数 > batch-size * GPU数`，至少满足1个训练batch）\r\n  + `lr`: 学习率。\r\n  + `wd`: weight decay。\r\n  + `max-steps`: 训练步数，也可通过`max-epochs`指定训练轮数。\r\n  + `freeze-vision`: 是否freeze视觉backbone。\r\n  + `use-augment`: 是否使用[AutoAugment](https://arxiv.org/abs/1805.09501)对图片进行数据增强。\r\n  + `valid-batch-size`: 验证时单机batch-size。（请保证`验证集样本总数 > batch-size * GPU数`，至少满足1个验证batch）\r\n  + `valid-step-interval`和`valid-epoch-interval`: 验证step/epoch频率，指定为-1时则在训练中不进行验证。\r\n  + `grad-checkpointing`: <span id=\"checkpointing\"></span>使用[重计算策略](https://pytorch.org/docs/stable/checkpoint.html)，在前向过程中不保存中间结果，以训练时间换取更小的显存开销，适用于显存不足的情况。（`store_true`参数，直接在脚本中加上`--grad-checkpointing`即可，目前要求Pytorch>1.8.0）\r\n  + `mask-ratio`: <span id=\"FLIP\"></span>参照[FLIP](https://arxiv.org/abs/2212.00794)的策略，在finetune时可指定随机mask一定比例的图像patch，以降低显存开销、加快训练速度。默认为0.0，即不激活这一策略。\r\n  + `use-flash-attention`: 使用[FlashAttention](https://arxiv.org/abs/2205.14135)，可在不影响效果的条件下为Chinese-CLIP的finetune过程显著提速以及降低显存占用。（`store_true`参数，配置好环境后，在脚本中加上`--use-flash-attention`即可，请详见[flash_attention.md](flash_attention.md)）\r\n  + `accum-freq`: <span id=\"gradient_accumulation\"></span>梯度累积频率，默认为1。指定为大于1的整数时开启对比学习梯度累积，模拟更大的batch size。如果单卡batch size为`m`，则总的batch size为`accum_freq * m * GPU数`。\r\n  + `gather-with-grad`: 是否在分布式训练时进行带有完整梯度的特征gather，默认关闭。\r\n+ 输出选项\r\n  + `name`: 指定输出路径。超参日志, 训练日志以及产出ckpt均会存放至 `${DATAPATH}/experiments/${name}/`。\r\n  + `save-step-frequency`及`save-epoch-frequency`: 存ckpt的步数或轮数间隔。\r\n  + `report-training-batch-acc`: 日志是否报告训练图到文&文到图batch准确率。\r\n+ 权重读取相关选项\r\n  + `resume`: 权重读取的路径。示例脚本中指定为预训练ckpt路径，也可以指定为用户自己finetune的ckpt路径做继续训练。\r\n  + `reset-data-offset`: 是否从此前的数据断点续跑。如batch size或GPU卡数超参改变，建议打开此选项。\r\n  + `reset-optimizer`: 是否使用optimizer state。\r\n\r\n训练完毕，log 会自动存在`${DATAPATH}/experiments/${name}/out_${timestamp}.log`，训练log格式如下所示:\r\n```\r\n2022-12-11,20:40:34 | INFO | Rank 0 | Global Steps: 1/735 | Train Epoch: 1 [1024/250880 (0%)] | Loss: 2.371020 | Image2Text Acc: 49.90 | Text2Image Acc: 48.73 | Data Time: 1.039s | Batch Time: 3.625s | LR: 0.000000 | logit_scale: 4.605 | Global Batch Size: 1024\r\n```\r\n验证log格式如下所示:\r\n```\r\n2022-12-11,20:42:47 | INFO | Rank 0 | Validation Result (epoch 1 @ 150 steps) | Valid Loss: 0.502810 | Image2Text Acc: 84.95 | Text2Image Acc: 84.26 | logit_scale: 4.605 | Valid Batch Size: 128\r\n```\r\n\r\n**注意**: 对比学习的训练收敛和稳定性和总batch size相关。如您使用更小的batch size（相比默认配置128 per-GPU \\* 8 GPU），建议使用更小的学习率。我们推荐使用更多的GPU和更大的batch size以取得更好的效果。\r\n\r\n### 预测及评估\r\n\r\n我们提供特征提取、以及图文检索任务评估的流程，具体如下：\r\n\r\n#### 图文特征提取\r\n\r\n目前本代码支持使用GPU单卡进行图文特征提取，请参考使用以下命令。我们也提供了部署ONNX和TensorRT模型，加速特征推理的支持，详见[deployment.md](deployment.md)。\r\n```bash\r\ncd Chinese-CLIP/\r\nexport CUDA_VISIBLE_DEVICES=0\r\nexport PYTHONPATH=${PYTHONPATH}:`pwd`/cn_clip\r\n\r\nsplit=valid # 指定计算valid或test集特征\r\nresume=${DATAPATH}/pretrained_weights/clip_cn_vit-b-16.pt\r\n\r\npython -u cn_clip/eval/extract_features.py \\\r\n    --extract-image-feats \\\r\n    --extract-text-feats \\\r\n    --image-data=\"${DATAPATH}/datasets/${dataset_name}/lmdb/${split}/imgs\" \\\r\n    --text-data=\"${DATAPATH}/datasets/${dataset_name}/${split}_texts.jsonl\" \\\r\n    --img-batch-size=32 \\\r\n    --text-batch-size=32 \\\r\n    --context-length=52 \\\r\n    --resume=${resume} \\\r\n    --vision-model=ViT-B-16 \\\r\n    --text-model=RoBERTa-wwm-ext-base-chinese\r\n```\r\n\r\n产出图文特征默认将保存于`${DATAPATH}/datasets/${dataset_name}`目录下，图片特征保存于`${split}_imgs.img_feat.jsonl`文件，每行以json存储一张图片的特征，格式如下：\r\n```\r\n{\"image_id\": 1000002, \"feature\": [0.0198, ..., -0.017, 0.0248]}\r\n```\r\n文本特征则保存于`${split}_texts.txt_feat.jsonl`，格式如下：\r\n```\r\n{\"text_id\": 248816, \"feature\": [0.1314, ..., 0.0018, -0.0002]}\r\n```\r\n\r\n#### KNN检索\r\n\r\n对于小规模的学术检索数据集，我们提供一个简单的KNN检索实现，便于计算文到图、图到文检索的top-k召回结果（tips：如想仿照我们在项目中搭建[检索demo](https://www.modelscope.cn/studios/damo/chinese_clip_applications/summary)，建议基于中文CLIP模型产出图文特征后，结合开源工程框架[clip-retrieval](https://github.com/rom1504/clip-retrieval)搭建前后端服务。）\r\n\r\n对于文到图检索（文本召回相关图片），请运行以下命令：\r\n```bash\r\ncd Chinese-CLIP/\r\nsplit=valid # 指定计算valid或test集特征\r\npython -u cn_clip/eval/make_topk_predictions.py \\\r\n    --image-feats=\"${DATAPATH}/datasets/${dataset_name}/${split}_imgs.img_feat.jsonl\" \\\r\n    --text-feats=\"${DATAPATH}/datasets/${dataset_name}/${split}_texts.txt_feat.jsonl\" \\\r\n    --top-k=10 \\\r\n    --eval-batch-size=32768 \\\r\n    --output=\"${DATAPATH}/datasets/${dataset_name}/${split}_predictions.jsonl\"\r\n```\r\n产出的结果保存在指定的jsonl文件中，每行表示一个文本召回的top-k图片id，格式如下：\r\n```json\r\n{\"text_id\": 153915, \"image_ids\": [5791244, 1009692167, 7454547004, 3564007203, 38130571, 2525270674, 2195419145, 2503091968, 4966265765, 3690431163]}\r\n```\r\n\r\n对于图到文检索（图片召回相关文本），类似地，请运行以下命令：\r\n```bash\r\nsplit=valid # 指定计算valid或test集特征\r\npython -u cn_clip/eval/make_topk_predictions_tr.py \\\r\n    --image-feats=\"${DATAPATH}/datasets/${dataset_name}/${split}_imgs.img_feat.jsonl\" \\\r\n    --text-feats=\"${DATAPATH}/datasets/${dataset_name}/${split}_texts.txt_feat.jsonl\" \\\r\n    --top-k=10 \\\r\n    --eval-batch-size=32768 \\\r\n    --output=\"${DATAPATH}/datasets/${dataset_name}/${split}_tr_predictions.jsonl\"\r\n```\r\n产出结果每行表示一个图片召回的top-k文本id，格式如下：\r\n```json\r\n{\"image_id\": 977856234, \"text_ids\": [156914, 157914, 158914, 155914, 156179, 158907, 157179, 154179, 154914, 154723]}\r\n```\r\n\r\n#### Recall计算\r\n\r\n我们提供了评测脚本计算检索任务的Recall@1/5/10，同时给出mean recall（Recall@1/5/10的平均数）。运行如下命令即可获取分数:\r\n\r\n对于文到图检索，请运行命令：\r\n```bash\r\nsplit=valid # 指定计算valid或test集特征\r\npython cn_clip/eval/evaluation.py \\\r\n    ${DATAPATH}/datasets/${dataset_name}/${split}_texts.jsonl \\\r\n    ${DATAPATH}/datasets/${dataset_name}/${split}_predictions.jsonl \\\r\n    output.json\r\ncat output.json\r\n```\r\n\r\n对于图到文检索，请先运行下面的命令，将图文对标注的jsonl文件由文到图的格式转为图到文：\r\n```bash\r\npython cn_clip/eval/transform_ir_annotation_to_tr.py \\\r\n    --input ${DATAPATH}/datasets/${dataset_name}/${split}_texts.jsonl\r\n```\r\n完成后，请运行命令：\r\n```bash\r\nsplit=valid # 指定计算valid或test集特征\r\npython cn_clip/eval/evaluation_tr.py \\\r\n    ${DATAPATH}/datasets/${dataset_name}/${split}_texts.tr.jsonl \\\r\n    ${DATAPATH}/datasets/${dataset_name}/${split}_tr_predictions.jsonl \\\r\n    output.json\r\ncat output.json\r\n```\r\n打印出的结果格式将如下：\r\n```json\r\n{\"success\": true, \"score\": 85.67, \"scoreJson\": {\"score\": 85.67, \"mean_recall\": 85.67, \"r1\": 71.2, \"r5\": 90.5, \"r10\": 95.3}}\r\n```\r\n\r\n关于整套跨模态检索的训练和测试流程，我们以MUGE检索数据集（[多模态电商图文挑战赛](https://tianchi.aliyun.com/competition/entrance/532031/introduction)）为例，也提供了一个包含上述全部流程并可运行的Jupyter Notebook（[下载链接](https://clip-cn-beijing.oss-cn-beijing.aliyuncs.com/others/Chinese-CLIP-on-MUGE-Retrieval.ipynb)），欢迎大家上手实践。\r\n\r\n<br>\r\n\r\n## 零样本图像分类\r\n本部分介绍如何使用Chinese-CLIP实现零样本图像分类，以零样本图像分类Benchmark ELEVATER中的数据集为例。ELEVATER是由多个知名的分类数据集（包括CIFAR-10、CIFAR-100、MNIST等）组成的评测集合，评测模型在这些数据集上的零样本效果。我们在实验中，给其中每个数据集准备了中文版本的prompt、类别标签连同原始图片，详见[数据文档](https://github.com/OFA-Sys/Chinese-CLIP/blob/master/zeroshot_dataset.md)，用于测试Chinese-CLIP模型。更多关于该benchmark的详情请点击[链接](https://eval.ai/web/challenges/challenge-page/1832/overview)。大家也可以参考我们提供的流程，仿照在自己的中文分类数据集准备数据并进行测试。\r\n<br>\r\n\r\n### 准备工作\r\n首先将数据按照如下格式进行准备。由于零样本图像分类仅需测试，因此只需要准备好测试集和预训练模型参数，按照如下目录结构，存放在用户指定的`${DATAPATH}`下：\r\n```\r\n${DATAPATH}\r\n├── pretrained_weights/\r\n└── datasets/\r\n    └── ${dataset_name}/\r\n        ├── label_cn.txt\r\n        └── test/\r\n\t    ├── 000/ # label id，如label个数大于10，则将其向左补零到3位数保证字典序\r\n\t    │   ├── image_0003.jpg # 图片样本，命名无特殊要求\r\n\t    │   ├── image_0005.jpg\r\n\t    │   └── ...\r\n\t    ├── 001/\r\n\t    │   ├── image_0001.jpg\r\n\t    │   ├── image_0002.jpg\r\n\t    │   └── ...\r\n\t    └── 002/\r\n\t        ├── image_0003.jpg\r\n\t        ├── image_0005.jpg\r\n\t        └── ...\r\n\t    ...\r\n\t\r\n```\r\n测试集保证test文件夹内数据按照label对应的id进行划分，并保证id为字典序（10以上的多位数，需向左补零`label.zfill(3)`, 如001，002等）。`label_cn.txt`为数据标签，每行一个标签名，如下所示：\r\n```\r\n手风琴\r\n飞机\r\n锚\r\n...\r\n```\r\n每行的标签对应的label id为`行号-1`，如第1行的标签的id为0，第二行的标签的id为1。如果标签总数大于10，则统一向左补零到3位数，比如标签个数为100，标签id则为`000-099`。用户需为每个label id生成对应的文件夹，并将标注该label的样本放入其中。我们以ELEVATER中的**CIFAR-100数据集**为样例，请点击[链接](http://clip-cn-beijing.oss-cn-beijing.aliyuncs.com/datasets/cifar-100.zip)下载处理好的数据。如果想尝试在其他ELEVATER包含的数据集上测试Chinese-CLIP，请参见我们的[数据文档](https://github.com/OFA-Sys/Chinese-CLIP/blob/master/zeroshot_dataset.md)。\r\n<br>\r\n\r\n### 预测和评估\r\n我们准备了预测脚本，请查看`run_scripts/zeroshot_eval.sh`。运行命令例子如下：\r\n```bash\r\nbash run_scripts/zeroshot_eval.sh 0 \\\r\n    ${DATAPATH} ${dataset_name} \\\r\n    ${vision_model} ${text_model} \\\r\n    ${ckpt_path} ${index_file}\r\n```\r\n其中各参数意义为：\r\n+ 第一个入参`0`为GPU id\r\n+ `DATAPATH`参见上面的准备工作部分，根据实际位置输入对应路径\r\n+ `dataset_name`参见上面的准备工作部分，输入评测的数据集目录名，如`cifar-100`\r\n+ `vision_model`为指定模型类型，选项包括`[\"ViT-B-32\", \"ViT-B-16\", \"ViT-L-14\", \"ViT-L-14-336\", \"RN50\", \"ViT-H-14\"]`\r\n+ `text_model`包括`[\"RoBERTa-wwm-ext-base-chinese\", \"RoBERTa-wwm-ext-large-chinese\", \"RBT3-chinese\"]`\r\n+ `ckpt_path`为模型预训练ckpt的完整路径\r\n+ `index_file`（可选，仅提交ELEVATER官网评测需要指定），请参见[数据文档](https://github.com/OFA-Sys/Chinese-CLIP/blob/master/zeroshot_dataset.md)\r\n\r\n例如，用ViT-B/16规模预训练模型进行评测CIFAR-100，则运行（`${DATAPATH}`需根据实际情况替换）：\r\n```bash\r\nbash run_scripts/zeroshot_eval.sh 0 \\\r\n    ${DATAPATH} cifar-100 \\\r\n    ViT-B-16 RoBERTa-wwm-ext-base-chinese \\\r\n    ${DATAPATH}/pretrained_weights/clip_cn_vit-b-16.pt\r\n```\r\n\r\n返回结果会打印top-1的准确率。\r\n```\r\nResult:\r\nzeroshot-top1: 0.6444\r\n```\r\n在CIFAR-100上，ViT-B/16规模的Chinese-CLIP预期应该达到64.4%。我们在ELEVATER上其他规模、其他数据集的零样本分类结果，请详见[Results.md](https://github.com/OFA-Sys/Chinese-CLIP/blob/master/Results.md#zeroshot_results)。\r\n\r\n同时，程序还会存下一个json文件用于提交ELEVATER官方用，json文件内容如下所示：\r\n```json\r\n{\"model_name\": \"CN-CLIP-ViT-B-16\", \"dataset_name\": \"cifar-100\", \"num_trainable_params\": 0, \"num_params\": 188262913, \"num_visual_params\": 86192640, \"num_backbone_params\": 188262913, \"n_shot\": 0, \"rnd_seeds\": [123], \"predictions\": \"prediction probability tensor [size: (1, 10000, 100)]\"}\r\n```\r\n其中包括模型名`model_name`、数据集名称`dataset_name`、总参数量`num_params`、视觉塔的参数量`num_visual_params`等模型的meta信息，以及模型输出结果，即模型的预测概率tensor，size为`[1, 样本数, 标签个数]`。\r\n\r\n### 零样本分类在线Demo\r\n基于我们集成于Huggingface transformers的特征提取API，我们在Huggingface Model Hub🤗提供了在线简单尝试零样本图像分类的demo（Hosted inference API），各个模型规模的demo链接见下，欢迎尝试！\r\n- [OFA-Sys/chinese-clip-vit-base-patch16](https://huggingface.co/OFA-Sys/chinese-clip-vit-base-patch16)\r\n- [OFA-Sys/chinese-clip-vit-large-patch14](https://huggingface.co/OFA-Sys/chinese-clip-vit-large-patch14)\r\n- [OFA-Sys/chinese-clip-vit-large-patch14-336px](https://huggingface.co/OFA-Sys/chinese-clip-vit-large-patch14-336px)\r\n- [OFA-Sys/chinese-clip-vit-huge-patch14](https://huggingface.co/OFA-Sys/chinese-clip-vit-huge-patch14)\r\n- **（12.10日更新🔥）**[**基于Huggingface Spaces部署的新版demo**](https://huggingface.co/spaces/OFA-Sys/chinese-clip-zero-shot-image-classification)：demo页面同时包含上述4个模型规模可选，支持输入自定义prompt模板，欢迎试用 \r\n<br><br><br>\r\n\r\n# 引用\r\n如果觉得本项目好用，希望能给我们提个star并分享给身边的用户，欢迎给相关工作citation，感谢支持！\r\n\r\n```\r\n@article{chinese-clip,\r\n  title={Chinese CLIP: Contrastive Vision-Language Pretraining in Chinese},\r\n  author={Yang, An and Pan, Junshu and Lin, Junyang and Men, Rui and Zhang, Yichang and Zhou, Jingren and Zhou, Chang},\r\n  journal={arXiv preprint arXiv:2211.01335},\r\n  year={2022}\r\n}\r\n```\r\n"
        },
        {
          "name": "README_En.md",
          "type": "blob",
          "size": 35.0068359375,
          "content": "[**中文说明**](README.md) | [**English**](README_En.md)\r\n\r\n<p align=\"center\">\r\n    <br>\r\n    <img src=\"assets/Chinese_CLIP_logo_tp_path.svg\" width=\"400\" />\r\n    <br>\r\n<p>\r\n<br>\r\n\r\n<p align=\"center\">\r\n        <a href=\"https://www.modelscope.cn/models?name=clip&tasks=multi-modal-embedding\">ModelScope</a>&nbsp ｜ &nbsp<a href=\"https://www.modelscope.cn/studios/damo/chinese_clip_applications/summary\">Demo</a>&nbsp ｜ &nbsp<a href=\"https://arxiv.org/abs/2211.01335\">Paper </a>&nbsp ｜ &nbspBlog\r\n</p>\r\n<br><br>\r\n\r\nThis is the Chinese version of CLIP. We use a large-scale Chinese image-text pair dataset (~200M) to train the model, and we hope that it can help users to conveniently achieve [image representation generation](#api-use-case), [cross-modal retrieval](#cross-modal-retrieval) and [zero-shot image classification](#zero-shot-image-classification) for Chinese data. This repo is based on <b>[open_clip project](https://github.com/mlfoundations/open_clip)</b>. We have made some optimization for better performance on Chinese data, and we provide the details in the following. \r\n<br><br>\r\n\r\n# News\r\n* 2023.11.30 Chinese-CLIP has added the [conversion script](https://github.com/OFA-Sys/Chinese-CLIP/blob/master/cn_clip/deploy/pytorch_to_coreml.py) to transform Pytorch models into coreml format for deployment. (Thanks [@manymuch](https://github.com/manymuch) for [the PR](https://github.com/OFA-Sys/Chinese-CLIP/pull/230) ❤️）\r\n* 2023.9.8 Chinese-CLIP has supported [knowledge distillation fine-tuning](distillation_En.md) based on [ModelScope](https://github.com/modelscope/modelscope). (Thanks [@wuziheng](https://github.com/wuziheng) and [@Jaskr616](https://github.com/Jaskr616) from Aliyun PAI Team for [the PR](https://github.com/OFA-Sys/Chinese-CLIP/pull/195)❤️)\r\n* 2023.5.9 Chinese-CLIP has been adapted to Pytorch2.0.\r\n* 2023.3.20 Support [gradient accumulation](#gradient-accumulation) in contrastive learning to simulate the training effect of a larger batch size.\r\n* 2023.2.16 Support [FlashAttention](https://github.com/HazyResearch/flash-attention) to improve training speed and reduce memory usage. See [flash_attention_En.md](flash_attention_En.md) for more information.\r\n* 2023.1.15 Support the conversion of Pytorch models into [ONNX](https://onnx.ai/) or [TensorRT](https://developer.nvidia.com/tensorrt) formats (and provide pretrained TensorRT models) to improve inference speed and meet deployment requirements. See [deployment_En.md](deployment_En.md) for more information.\r\n* 2022.12.12 Implement [FLIP](https://arxiv.org/abs/2212.00794) strategy, which can be [activated](#FLIP) during finetuning (Thanks [@zwkkk](https://github.com/zwkkk) for [the PR](https://github.com/OFA-Sys/Chinese-CLIP/pull/26) ❤️）\r\n* 2022.12.3 The datasets of the Chinese version of the [ELEVATER](https://eval.ai/web/challenges/challenge-page/1832) benchmark are publicly available. See [Notes for datasets](zeroshot_dataset_en.md) for more information. \r\n* 2022.12.1 Chinese-CLIP model & representation generation API are officially merged into Huggingface transformers🤗 codebase.\r\n* 2022.11.22 Release [zero-shot image classification](#zero-shot-image-classification) code. Support [ELEVATER](https://eval.ai/web/challenges/challenge-page/1832) zero-shot classification benchmark.\r\n* 2022.11.3 Release RN50, ViT-H-14 models. Release [technical report](https://arxiv.org/pdf/2211.01335.pdf).\r\n* 2022.9.22 Release ViT-L-14, ViT-L-14-336 models.\r\n* 2022.7.13 Release [fast image & text representation generation API](#api-use-case), which facitilates usage of our CLIP models quickly.\r\n* 2022.7.8 Release the project Chinese-CLIP! Release [image-text retrieval](#cross-modal-retrieval) code.\r\n<br><br>\r\n\r\n# Models and Results\r\n<span id=\"model_card\"></span>\r\n## Model Card\r\nCurrently, we release 5 different sizes of Chinese-CLIP models. Detailed information and download link of each Chinese-CLIP model are provided below:\r\n\r\n<table border=\"1\" width=\"100%\">\r\n    <tr align=\"center\">\r\n        <th>Model</th><th>Ckpt</th><th>#Params (All)</th><th>Backbone (I)</th><th>#Params (I)</th><th>Backbone (T)</th><th>#Params (T)</th><th>Resolution</th>\r\n    </tr>\r\n    <tr align=\"center\">\r\n        <td>CN-CLIP<sub>RN50</sub></td><td><a href=\"https://clip-cn-beijing.oss-cn-beijing.aliyuncs.com/checkpoints/clip_cn_rn50.pt\">Download</a></td><td>77M</td><td>ResNet50</td><td>38M</td><td>RBT3</td><td>39M</td><td>224</td>\r\n    </tr>\r\n    <tr align=\"center\">\r\n        <td>CN-CLIP<sub>ViT-B/16</sub></td><td><a href=\"https://clip-cn-beijing.oss-cn-beijing.aliyuncs.com/checkpoints/clip_cn_vit-b-16.pt\">Download</a></td><td>188M</td><td>ViT-B/16</td><td>86M</td><td>RoBERTa-wwm-Base</td><td>102M</td><td>224</td>\r\n    </tr>\r\n    <tr align=\"center\">\r\n        <td>CN-CLIP<sub>ViT-L/14</sub></td><td><a href=\"https://clip-cn-beijing.oss-cn-beijing.aliyuncs.com/checkpoints/clip_cn_vit-l-14.pt\">Download</a></td><td>406M</td><td>ViT-L/14</td><td>304M</td><td>RoBERTa-wwm-Base</td><td>102M</td><td>224</td>\r\n    </tr>\r\n    <tr align=\"center\">\r\n        <td>CN-CLIP<sub>ViT-L/14@336px</sub></td><td><a href=\"https://clip-cn-beijing.oss-cn-beijing.aliyuncs.com/checkpoints/clip_cn_vit-l-14-336.pt\">Download</a></td><td>407M</td><td>ViT-L/14</td><td>304M</td><td>RoBERTa-wwm-Base</td><td>102M</td><td>336</td>\r\n    </tr>\r\n    <tr align=\"center\">\r\n        <td>CN-CLIP<sub>ViT-H/14</sub></td><td><a href=\"https://clip-cn-beijing.oss-cn-beijing.aliyuncs.com/checkpoints/clip_cn_vit-h-14.pt\">Download</a></td><td>958M</td><td>ViT-H/14</td><td>632M</td><td>RoBERTa-wwm-Large</td><td>326M</td><td>224</td>\r\n    </tr>\r\n</table>\r\n<br></br>\r\n\r\n## Results\r\nWe conducted zero-shot inference and finetuning experiments on [MUGE Retrieval](https://tianchi.aliyun.com/muge), [Flickr30K-CN](https://github.com/li-xirong/cross-lingual-cap) and [COCO-CN](https://github.com/li-xirong/coco-cn) for the evaluation of cross-modal retrieval, and conducted experiments on 10 image classification datasets of the [ELEVATER](https://eval.ai/web/challenges/challenge-page/1832) benchmark for the evaluation of zero-shot image classification. Results are shown below. Due to space limitation, here we only list the performance of the best performing Chinese-CLIP and baseline models. For detailed performance of each Chinese-CLIP model size, please refer to [Results.md](Results.md).\r\n\r\n**MUGE Text-to-Image Retrieval (Official Validation Set)**:\r\n<table border=\"1\" width=\"100%\">\r\n    <tr align=\"center\">\r\n        <th>Setup</th><th colspan=\"4\">Zero-shot</th><th colspan=\"4\">Finetune</th>\r\n    </tr>\r\n    <tr align=\"center\">\r\n        <td>Metric</td><td>R@1</td><td>R@5</td><td>R@10</td><td>MR</td><td>R@1</td><td>R@5</td><td>R@10</td><td>MR</td>\r\n    </tr>\r\n\t<tr align=\"center\">\r\n        <td width=\"120%\">Wukong</td><td>42.7</td><td>69.0</td><td>78.0</td><td>63.2</td><td>52.7</td><td>77.9</td><td>85.6</td><td>72.1</td>\r\n    </tr>\r\n\t<tr align=\"center\">\r\n        <td width=\"120%\">R2D2</td><td>49.5</td><td>75.7</td><td>83.2</td><td>69.5</td><td>60.1</td><td>82.9</td><td>89.4</td><td>77.5</td>\r\n    </tr>\r\n\t<tr align=\"center\">\r\n        <td width=\"120%\">CN-CLIP</td><td>63.0</td><td>84.1</td><td>89.2</td><td>78.8</td><td>68.9</td><td>88.7</td><td>93.1</td><td>83.6</td>\r\n    </tr>\r\n</table>\r\n<br>\r\n\r\n**Flickr30K-CN Retrieval (Official Test Set)**:\r\n<table border=\"1\" width=\"120%\">\r\n\t<tr align=\"center\">\r\n        <th>Task</th><th colspan=\"6\">Text-to-Image</th><th colspan=\"6\">Image-to-Text</th>\r\n    </tr>\r\n    <tr align=\"center\">\r\n        <th>Setup</th><th colspan=\"3\">Zero-shot</th><th colspan=\"3\">Finetune</th><th colspan=\"3\">Zero-shot</th><th colspan=\"3\">Finetune</th>\r\n    </tr>\r\n    <tr align=\"center\">\r\n        <td>Metric</td><td>R@1</td><td>R@5</td><td>R@10</td><td>R@1</td><td>R@5</td><td>R@10</td><td>R@1</td><td>R@5</td><td>R@10</td><td>R@1</td><td>R@5</td><td>R@10</td>\r\n    </tr>\r\n\t<tr align=\"center\">\r\n        <td width=\"120%\">Wukong</td><td>51.7</td><td>78.9</td><td>86.3</td><td>77.4</td><td>94.5</td><td>97.0</td><td>76.1</td><td>94.8</td><td>97.5</td><td>92.7</td><td>99.1</td><td>99.6</td>\r\n    </tr>\r\n\t<tr align=\"center\">\r\n        <td width=\"120%\">Taiyi</td><td>60.8</td><td>85.0</td><td>91.0</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td>\r\n    </tr>\t\r\n\t<tr align=\"center\">\r\n        <td width=\"120%\">R2D2</td><td>60.9</td><td>86.8</td><td>92.7</td><td>84.4</td><td>96.7</td><td>98.4</td><td>77.6</td><td>96.7</td><td>98.9</td><td>95.6</td><td>99.8</td><td>100.0</td>\r\n    </tr>\r\n\t<tr align=\"center\">\r\n        <td width=\"120%\">CN-CLIP</td><td>71.2</td><td>91.4</td><td>95.5</td><td>83.8</td><td>96.9</td><td>98.6</td><td>81.6</td><td>97.5</td><td>98.8</td><td>95.3</td><td>99.7</td><td>100.0</td>\r\n    </tr>\r\n</table>\r\n<br>\r\n\r\n**COCO-CN Retrieval (Official Test Set)**:\r\n<table border=\"1\" width=\"100%\">\r\n\t<tr align=\"center\">\r\n        <th>Task</th><th colspan=\"6\">Text-to-Image</th><th colspan=\"6\">Image-to-Text</th>\r\n    </tr>\r\n    <tr align=\"center\">\r\n        <th>Setup</th><th colspan=\"3\">Zero-shot</th><th colspan=\"3\">Finetune</th><th colspan=\"3\">Zero-shot</th><th colspan=\"3\">Finetune</th>\r\n    </tr>\r\n    <tr align=\"center\">\r\n        <td>Metric</td><td>R@1</td><td>R@5</td><td>R@10</td><td>R@1</td><td>R@5</td><td>R@10</td><td>R@1</td><td>R@5</td><td>R@10</td><td>R@1</td><td>R@5</td><td>R@10</td>\r\n    </tr>\r\n\t<tr align=\"center\">\r\n        <td width=\"120%\">Wukong</td><td>53.4</td><td>80.2</td><td>90.1</td><td>74.0</td><td>94.4</td><td>98.1</td><td>55.2</td><td>81.0</td><td>90.6</td><td>73.3</td><td>94.0</td><td>98.0</td>\r\n    </tr>\r\n\t<tr align=\"center\">\r\n        <td width=\"150%\">Taiyi</td><td>60.0</td><td>84.0</td><td>93.3</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td>\r\n    </tr>\t\r\n\t<tr align=\"center\">\r\n        <td width=\"120%\">R2D2</td><td>56.4</td><td>85.0</td><td>93.1</td><td>79.1</td><td>96.5</td><td>98.9</td><td>63.3</td><td>89.3</td><td>95.7</td><td>79.3</td><td>97.1</td><td>98.7</td>\r\n    </tr>\r\n\t<tr align=\"center\">\r\n        <td width=\"120%\">CN-CLIP</td><td>69.2</td><td>89.9</td><td>96.1</td><td>81.5</td><td>96.9</td><td>99.1</td><td>63.0</td><td>86.6</td><td>92.9</td><td>83.5</td><td>97.3</td><td>99.2</td>\r\n    </tr>\r\n</table>\r\n<br>\r\n\r\n**Zero-shot Image Classification**:\r\n<table border=\"1\" width=\"100%\">\r\n\t<tr align=\"center\">\r\n        <th>Task</th><th>CIFAR10</th><th>CIFAR100</th><th>DTD</th><th>EuroSAT</th><th>FER</th><th>FGVC</th><th>KITTI</th><th>MNIST</th><th>PC</th><th>VOC</th>\r\n    </tr>\r\n\t<tr align=\"center\">\r\n        <td width=\"150%\">GIT</td><td>88.5</td><td>61.1</td><td>42.9</td><td>43.4</td><td>41.4</td><td>6.7</td><td>22.1</td><td>68.9</td><td>50.0</td><td>80.2</td>\r\n    </tr>\r\n    \t<tr align=\"center\">\r\n        <td width=\"150%\">ALIGN</td><td>94.9</td><td>76.8</td><td>66.1</td><td>52.1</td><td>50.8</td><td>25.0</td><td>41.2</td><td>74.0</td><td>55.2</td><td>83.0</td>\r\n    </tr>\r\n\t<tr align=\"center\">\r\n        <td width=\"150%\">CLIP</td><td>94.9</td><td>77.0</td><td>56.0</td><td>63.0</td><td>48.3</td><td>33.3</td><td>11.5</td><td>79.0</td><td>62.3</td><td>84.0</td>\r\n    </tr>\r\n    \t<tr align=\"center\">\r\n        <td width=\"150%\">Wukong</td><td>95.4</td><td>77.1</td><td>40.9</td><td>50.3</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td>\r\n    </tr>\r\n    \t<tr align=\"center\">\r\n        <td width=\"150%\">CN-CLIP</td><td>96.0</td><td>79.7</td><td>51.2</td><td>52.0</td><td>55.1</td><td>26.2</td><td>49.9</td><td>79.4</td><td>63.5</td><td>84.9</td>\r\n    </tr>\r\n</table>\r\n<br><br>\r\n\r\n# Getting Started\r\n## Installation Requirements\r\nTo start with this project, make sure that your environment meets the requirements below:\r\n\r\n* python >= 3.6.4\r\n* pytorch >= 1.8.0 (with torchvision >= 0.9.0)\r\n* CUDA Version >= 10.2\r\n\r\nRun the following command to install required packages.\r\n\r\n```bash\r\npip install -r requirements.txt\r\n```\r\n\r\n## API Use Case\r\nWe provide a simple code snippet to show how to use the API for Chinese-CLIP. For starters, please install cn_clip:\r\n```bash\r\n# to install the latest stable release\r\npip install cn_clip\r\n\r\n# or install from source code\r\ncd Chinese-CLIP\r\npip install -e .\r\n```\r\nAfter installation, use Chinese CLIP to compute the image ([example](examples/pokemon.jpeg)) & text embeddings and similarities as shown below:\r\n```python\r\nimport torch \r\nfrom PIL import Image\r\n\r\nimport cn_clip.clip as clip\r\nfrom cn_clip.clip import load_from_name, available_models\r\nprint(\"Available models:\", available_models())  \r\n# Available models: ['ViT-B-16', 'ViT-L-14', 'ViT-L-14-336', 'ViT-H-14', 'RN50']\r\n\r\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\r\nmodel, preprocess = load_from_name(\"ViT-B-16\", device=device, download_root='./')\r\nmodel.eval()\r\nimage = preprocess(Image.open(\"examples/pokemon.jpeg\")).unsqueeze(0).to(device)\r\ntext = clip.tokenize([\"杰尼龟\", \"妙蛙种子\", \"小火龙\", \"皮卡丘\"]).to(device)\r\n\r\nwith torch.no_grad():\r\n    image_features = model.encode_image(image)\r\n    text_features = model.encode_text(text)\r\n    # Normalize the features. Please use the normalized features for downstream tasks.\r\n    image_features /= image_features.norm(dim=-1, keepdim=True) \r\n    text_features /= text_features.norm(dim=-1, keepdim=True)      \r\n\r\n    logits_per_image, logits_per_text = model.get_similarity(image, text)\r\n    probs = logits_per_image.softmax(dim=-1).cpu().numpy()\r\n\r\nprint(\"Label probs:\", probs)  # [[1.268734e-03 5.436878e-02 6.795761e-04 9.436829e-01]]\r\n```\r\n\r\nHowever, if you are not satisfied with only using the API, move on for more details about training and inference. \r\n<br><br>\r\n\r\n\r\n# Tutorial\r\n\r\nCurrently, we provide the tutorial of [cross-modal retrieval](#cross-modal-retrieval) and [zero-shot image classification](#zero-shot-image-classification) below.\r\n\r\n## Cross-Modal Retrieval\r\n\r\n### Code Organization\r\n\r\nAfter cloning this project, please create a new directory ```${DATAPATH}``` for datasets, checkpoints and logs. A recommended workspace structure is demonstrated below：\r\n\r\n```\r\nChinese-CLIP/\r\n├── run_scripts/\r\n│   ├── muge_finetune_vit-b-16_rbt-base.sh\r\n│   ├── flickr30k_finetune_vit-b-16_rbt-base.sh\r\n│   └── ...           # more scripts for finetuning and evaluation...\r\n└── src/\r\n    ├── clip/\r\n    ├── eval/\r\n    ├── preprocess/\r\n    └── training/\r\n\r\n${DATAPATH}\r\n├── pretrained_weights/\r\n├── experiments/\r\n├── deploy/\t      # store ONNX & TensorRT deployment models\r\n└── datasets/\r\n    ├── MUGE/\r\n    ├── Flickr30k-CN/\r\n    └── .../          # more datasets...\r\n```\r\n\r\n### Preparation\r\nWe provide links for the downloading of pretrained checkpoints, as well as the data preprocessing procedures for finetuning. \r\n\r\n#### Pretrained Checkpoints\r\n\r\nPlease refer to [model card section](#model_card) above and download the model checkpoint. We recommend putting the checkpoint in `${DATAPATH}/pretrained_weights/`. \r\n\r\n#### Data Preprocessing\r\n\r\nWe advise to organize the data in the following way to ensure the efficiency of accessing and processing data:\r\n\r\n```\r\n${DATAPATH}\r\n└── datasets/\r\n    └── ${dataset_name}/\r\n        ├── train_imgs.tsv      # image id & image content\r\n        ├── train_texts.jsonl   # text id & text content, with list of paired image ids\r\n        ├── valid_imgs.tsv\r\n        ├── valid_texts.jsonl\r\n        ├── test_imgs.tsv\r\n        └── test_texts.jsonl\r\n```\r\nwhere `${dataset_name}` refers to the name of dataset (e.g., MUGE).\r\n\r\nTo ensure the efficiency of processing data, we did not store images with small files, but instead we encode them to base64 strings and store them in `${split}_imgs.tsv`. Each line represents an image, where there are id (int) and base64 string, split by `\\t`, as shown below:  \r\n```\r\n1000002\t/9j/4AAQSkZJ...YQj7314oA//2Q==\r\n```\r\n\r\nTransforming image files to base64 strings is simple. Run the following code:\r\n```python\r\nfrom PIL import Image\r\nfrom io import BytesIO\r\nimport base64\r\n\r\nimg = Image.open(file_name) # path to file\r\nimg_buffer = BytesIO()\r\nimg.save(img_buffer, format=img.format)\r\nbyte_data = img_buffer.getvalue()\r\nbase64_str = base64.b64encode(byte_data) # bytes\r\nbase64_str = base64_str.decode(\"utf-8\") # str\r\n```\r\n\r\nTexts and image-text pairing relations are stored in `${split}_texts.jsonl`, where each line is a json as shown below:\r\n\r\n```\r\n{\"text_id\": 8428, \"text\": \"高级感托特包斜挎\", \"image_ids\": [1076345, 517602]}\r\n```\r\nFor the test set where only the texts are given and the image-text pairing relations are unknown, just leave the `image_ids` field as an empty list, `\"image_ids\": []`.\r\n\r\nFinally, we need to serialize tsv and jsonl and transform them to LMDB files, which is easy for random access during training.\r\n```\r\npython src/preprocess/build_lmdb_dataset.py \\\r\n    --data_dir ${DATAPATH}/datasets/${dataset_name}\r\n    --splits train,valid,test\r\n```\r\nFor example, for the MUGE dataset, we name `${dataset_name}` to MUGE. `--splits` refers to dataset splits，split by commas without space. After that, there will be LMDB files in the directory.\r\n```\r\n${DATAPATH}\r\n└── datasets/\r\n    └── ${dataset_name}/\r\n        └── lmdb/\r\n            ├── train\r\n            │   ├── imgs\r\n            │   └── pairs\r\n            ├── valid\r\n            └── test\r\n```\r\n\r\nFor easier use, we have provided preprocessed MUGE ([download link](https://clip-cn-beijing.oss-cn-beijing.aliyuncs.com/datasets/MUGE.zip)) and Flickr30K-CN ([download link](https://clip-cn-beijing.oss-cn-beijing.aliyuncs.com/datasets/Flickr30k-CN.zip)) datasets in zip format. To use them, just download and unzip it under `${DATAPATH}/datasets/`. If you need [COCO-CN](https://github.com/li-xirong/coco-cn) dataset, please contact us by email when you have finished applying for permission from the original author.\r\n\r\n### Finetuning\r\n\r\nWe introduce the procedures of training for users to learn about the details of the model. We finetune with the pretrained Chinese CLIP. For MUGE and Flickr30K-CN, we provide scripts `run_scripts/muge_finetune_vit-b-16_rbt-base.sh` and `run_scripts/flickr30k_finetune_vit-b-16_rbt-base.sh`. <b>The scripts support single-worker and distributed training. Before running, follow the instructions at the beggining of the scripts and fill in your configuration for distributed training. Then run the scripts to start your training. If the GPU memory is insufficient, you can consider to [activate the gradient checkpointing strategy](#checkpointing) in the configuration.</b> Logs and checkpoints will be saved at your specified paths. \r\n\r\n```bash\r\ncd Chinese-CLIP/\r\nbash run_scripts/muge_finetune_vit-b-16_rbt-base.sh ${DATAPATH}\r\n```\r\n\r\nThe configuration for training includes:\r\n\r\n+ Distributed training\r\n  + `WORKER_CNT`: the number of machines.\r\n  + `GPUS_PER_NODE`: the number of GPUS on each machine.\r\n+ Data for training/validation\r\n  + `train-data`: directory of training data. Follow the procedures above the create LMDB files.\r\n  + `val-data`: directory of validation data. If set to None, validation during finetuning will be disabled.\r\n  + `num-workers`: the number of workers for training set dataloader, default to 4.\r\n  + `valid-num-workers`: the number of workers for validation set dataloader, default to 1.\r\n+ Training hyper-params\r\n  + `vision-model`: specified visual backbones. Select from `[\"ViT-B-16\", \"ViT-L-14\", \"ViT-L-14-336\", \"ViT-H-14\", \"RN50\"]`.\r\n  + `text-model`: specified language backbones. Select from `[\"RoBERTa-wwm-ext-base-chinese\", \"RoBERTa-wwm-ext-large-chinese\", \"RBT3-chinese\"]`.\r\n  + `context-length`: sequence length for text inputs.\r\n  + `warmup`: steps for warmup.\r\n  + `batch-size`: batch size for a worker (make sure that the number of training samples larger than `batch-size * GPUs`).\r\n  + `lr`: learning rate.\r\n  + `wd`: weight decay.\r\n  + `max-steps`: training steps. Also you can set `max-epochs` to set the number of training epochs.\r\n  + `freeze-vision`: whether to freeze the visual backbone. \r\n  + `use-augment`: whether to use [AutoAugment](https://arxiv.org/abs/1805.09501) for data augmentation. \r\n  + `valid-batch-size`: validation batch size for a worker (make sure that the number of validation samples larger than `valid-batch-size * GPUs`).\r\n  + `valid-step-interval` and `valid-epoch-interval`: validation step / epoch frequency, if set to -1 then validation will be disabled during finetuning.\r\n  + `grad-checkpointing`: <span id=\"checkpointing\"></span>use [gradient checkpointing]((https://pytorch.org/docs/stable/checkpoint.html)) which does not keep the activations during forward computation, this strategy trades more computation and iteration time for less GPU memory cost. (`store_true` argument, just add `--grad-checkpointing` in the script to activate it, requires Pytorch>1.8.0)\r\n  + `mask-ratio`: <span id=\"FLIP\"></span>use [FLIP](https://arxiv.org/abs/2212.00794) strategy which randomly masks a ratio of image patches to save GPU memory and speed up training. Default to 0.0, which disables the strategy.\r\n  + `use-flash-attention`: whether to use [FlashAttention](https://arxiv.org/abs/2205.14135), which can significantly speed up the finetune process and reduce the memory usage. (`store_true` argument, after configuring the environment, just add `--use-flash-attention` in the script to activate it, please see [flash_attention_En.md](flash_attention_En.md) for more information)\r\n  + `accum-freq`: <span id=\"gradient-accumulation\"></span>Gradient accumulation frequency, default is 1. Specify an integer greater than 1 to enable gradient accumulation to simulate a larger batch size. if the batch size for a worker is `m`, the total batch size is `accum_freq * m * GPUs`.\r\n  + `gather-with-grad`: Whether to enable full distributed gradient for feature gather, off by default.\r\n+ Ouputs\r\n  + `name`: specified output path. Hyperparameter logs, training logs, and checkpoints will be saved at `${DATAPATH}/experiments/${name}/`.\r\n  + `save-step-frequency` and `save-epoch-frequency`: the intervals for saving checkpoints.\r\n  + `report-training-batch-acc`: whether to report the in-batch image-to-text and text-to-image retrieval accuracy. \r\n+ Checkpoints\r\n  + `resume`: the checkpoint path for weights to restore. In the provided example script, the path refers to the pretrained checkpoint path. Users can change to your own checkpoint path.\r\n  + `reset-data-offset`: whether to restore training at the data breakpoint.\r\n  + `reset-optimizer`: whether to restore the optimizer state.\r\n\r\nAfter training, the log will be saved at `${DATAPATH}/experiments/${name}/out_${timestamp}.log`. Example of log is shown below:\r\n```\r\n2022-12-11,20:40:34 | INFO | Rank 0 | Global Steps: 1/735 | Train Epoch: 1 [1024/250880 (0%)] | Loss: 2.371020 | Image2Text Acc: 49.90 | Text2Image Acc: 48.73 | Data Time: 1.039s | Batch Time: 3.625s | LR: 0.000000 | logit_scale: 4.605 | Global Batch Size: 1024\r\n```\r\nThe example of validation log is shown below:\r\n```\r\n2022-12-11,20:42:47 | INFO | Rank 0 | Validation Result (epoch 1 @ 150 steps) | Valid Loss: 0.502810 | Image2Text Acc: 84.95 | Text2Image Acc: 84.26 | logit_scale: 4.605 | Valid Batch Size: 128\r\n```\r\n\r\n**Attention**: The convergence and stability of contrastive learning is highly relevant to the total batch size. If you use a smaller batch size, (in comparison with the default 128 per-GPU \\* 8 GPU), we advise you to use a smaller learning rat. We recommend using more GPUs and larger batch size for better performance. \r\n\r\n### Inference and Evaluation\r\n\r\nWe provide procedures for representation generation and cross-modal retrieval, as demonstrated below:\r\n\r\n#### Image/Text Representation Generation\r\n\r\nBy now the code supports representation generation with a single worker, please use the following commands. Besides, we provide support for deploying ONNX and TensorRT models to accelerate feature inference, see [deployment_En.md](deployment_En.md) for details.\r\n```bash\r\ncd Chinese-CLIP/\r\nexport CUDA_VISIBLE_DEVICES=0\r\nexport PYTHONPATH=${PYTHONPATH}:`pwd`/src\r\n\r\nsplit=valid # validation / test set\r\nresume=${DATAPATH}/pretrained_weights/clip_cn_vit-b-16.pt\r\n\r\npython -u src/eval/extract_features.py \\\r\n    --extract-image-feats \\\r\n    --extract-text-feats \\\r\n    --image-data=\"${DATAPATH}/datasets/${dataset_name}/lmdb/${split}/imgs\" \\\r\n    --text-data=\"${DATAPATH}/datasets/${dataset_name}/${split}_texts.jsonl\" \\\r\n    --img-batch-size=32 \\\r\n    --text-batch-size=32 \\\r\n    --context-length=52 \\\r\n    --resume=${resume} \\\r\n    --vision-model=ViT-B-16 \\\r\n    --text-model=RoBERTa-wwm-ext-base-chinese\r\n```\r\n\r\nBy default, the representations are stored at `${DATAPATH}/datasets/${dataset_name}`. Specifically, the image representations are stored at `${split}_imgs.img_feat.jsonl`. Each line stores a json of image representation, as shown below:\r\n```\r\n{\"image_id\": 1000002, \"feature\": [0.0198, ..., -0.017, 0.0248]}\r\n```\r\nText representations are stored at `${split}_texts.txt_feat.jsonl`，as shown below:\r\n```\r\n{\"text_id\": 248816, \"feature\": [0.1314, ..., 0.0018, -0.0002]}\r\n```\r\n\r\n#### KNN Retrieval\r\n\r\nFor small-scale retrieval datasets, we provide a simple implementation of KNN retrieval, to facilitate the retrieval of top-k results in cross-modal retrieval. (tips: If you want to build a [retrieval demo](https://www.modelscope.cn/studios/damo/chinese_clip_applications/summary) in your project like us, we suggest first to use Chinese-CLIP to compute image and text embeddings, and then employ an opensource servering framework [clip-retrieval](https://github.com/rom1504/clip-retrieval) to deploy the front-end and back-end servering.)\r\n\r\nFor text-to-image retrieval, run the commands below:\r\n```bash\r\ncd Chinese-CLIP/\r\nsplit=valid # validation / test splits\r\npython -u src/eval/make_topk_predictions.py \\\r\n    --image-feats=\"${DATAPATH}/datasets/${dataset_name}/${split}_imgs.img_feat.jsonl\" \\\r\n    --text-feats=\"${DATAPATH}/datasets/${dataset_name}/${split}_texts.txt_feat.jsonl\" \\\r\n    --top-k=10 \\\r\n    --eval-batch-size=32768 \\\r\n    --output=\"${DATAPATH}/datasets/${dataset_name}/${split}_predictions.jsonl\"\r\n```\r\nResults are stored at specified jsonl files. Each line consists of top-k image ids for a text query, as shown below:\r\n```json\r\n{\"text_id\": 153915, \"image_ids\": [5791244, 1009692167, 7454547004, 3564007203, 38130571, 2525270674, 2195419145, 2503091968, 4966265765, 3690431163]}\r\n```\r\n\r\nFor image-to-text retrieval, run the commands below：\r\n```bash\r\nsplit=valid # validation / test splits\r\npython -u src/eval/make_topk_predictions_tr.py \\\r\n    --image-feats=\"${DATAPATH}/datasets/${dataset_name}/${split}_imgs.img_feat.jsonl\" \\\r\n    --text-feats=\"${DATAPATH}/datasets/${dataset_name}/${split}_texts.txt_feat.jsonl\" \\\r\n    --top-k=10 \\\r\n    --eval-batch-size=32768 \\\r\n    --output=\"${DATAPATH}/datasets/${dataset_name}/${split}_tr_predictions.jsonl\"\r\n```\r\nResults are stored at specified jsonl files. Each line consists of top-k text ids for an image query, as shown below:\r\n```json\r\n{\"image_id\": 977856234, \"text_ids\": [156914, 157914, 158914, 155914, 156179, 158907, 157179, 154179, 154914, 154723]}\r\n```\r\n\r\n#### Recall Metric\r\n\r\nWe provide scripts for computing the Recall@1/5/10 and mean recall (the mean of Recall@1/5/10). Run the commands to get the scores:\r\n\r\nFor text-to-image retrieval, run the commands below:\r\n```bash\r\nsplit=valid # validation / test splits\r\npython src/eval/evaluation.py \\\r\n        ${DATAPATH}/datasets/${dataset_name}/${split}_texts.jsonl \\\r\n        ${DATAPATH}/datasets/${dataset_name}/${split}_predictions.jsonl \\\r\n        output.json\r\ncat output.json\r\n```\r\n\r\n\r\nFor image-to-text retrieval, run the commands first to transform text-to-image jsonls to image-to-text ones:\r\n```bash\r\npython src/eval/transform_ir_annotation_to_tr.py \\\r\n        --input ${DATAPATH}/datasets/${dataset_name}/${split}_texts.jsonl\r\n```\r\nAfter that, run the following commands\r\n```bash\r\nsplit=valid # validation / test splits\r\npython src/eval/evaluation_tr.py \\\r\n        ${DATAPATH}/datasets/${dataset_name}/${split}_texts.tr.jsonl \\\r\n        ${DATAPATH}/datasets/${dataset_name}/${split}_tr_predictions.jsonl \\\r\n        output.json\r\ncat output.json\r\n```\r\n\r\nThe printed results are shown below:\r\n```json\r\n{\"success\": true, \"score\": 85.67, \"scoreJson\": {\"score\": 85.67, \"mean_recall\": 85.67, \"r1\": 71.2, \"r5\": 90.5, \"r10\": 95.3}}\r\n```\r\n\r\nFor better understanding of cross-modal retrieval by Chinese-CLIP, we also provide a runnable jupyter notebook ([download link](https://clip-cn-beijing.oss-cn-beijing.aliyuncs.com/others/Chinese-CLIP-on-MUGE-Retrieval.ipynb)), which works with the MUGE retrieval dataset (corresponding leaderboard is hosted on [Tianchi](https://tianchi.aliyun.com/competition/entrance/532031/introduction?lang=en-us)) and includes the finetuning and inference process mentioned above. Welcome to try!\r\n\r\n<br>\r\n\r\n## Zero-shot Image Classification\r\nThis section introduces the use of Chinese-CLIP for zero-shot image classification. We use the experiment on a dataset of the benchmark ELEVATER as an example. ELEVATER is a benchmark consist of several widely used classification datasets and evaluates the zero-shot performance on these datasets, including CIFAR-10, CIFAR-100, MNIST, etc. In our experiments, we have perpared Chinese prompts and label names with the original images for each ELEVATER dataset (refer to [Notes for datasets](zeroshot_dataset_en.md) for download) to evaluate Chinese-CLIP. For more information about ELEVATER, please click [this link](https://eval.ai/web/challenges/challenge-page/1832/overview). Users can also follow the procedure below to prepare and evaluate their own classification datasets.\r\n<br><br>\r\n\r\n### Preparation\r\nWe need to prepare only the test set and the pretrained Chinese-CLIP checkpoint. It's recommended to prepare these directories under a user defined `${DATAPATH}` and organize them as follows:\r\n```\r\n${DATAPATH}\r\n├── pretrained_weights/\r\n└── datasets/\r\n    └── ${dataset_name}/\r\n        ├── label_cn.txt\r\n        └── test/\r\n\t    ├── 000/ # label id，fill 0 by the left to 3 digits so that the labels can be alphabetically ordered\r\n\t    │   ├── image_0003.jpg # image sample, no specific requirements for the naming\r\n\t    │   ├── image_0005.jpg\r\n\t    │   └── ...\r\n\t    ├── 001/\r\n\t    │   ├── image_0001.jpg\r\n\t    │   ├── image_0002.jpg\r\n\t    │   └── ...\r\n\t    └── 002/\r\n\t        ├── image_0003.jpg\r\n\t        ├── image_0005.jpg\r\n\t        └── ...\r\n\t    ...\r\n\t\r\n```\r\nMake sure the data are categorized by their label id, and make sure the ids are alphabetically orderd (for numbers larger than 10, use`label.zfill(3)` to fill 0 by the left to 3 digits, like 001，002, etc). `label_cn.txt` refers to the file of label names. Each line has a label name, as demonstrated below:\r\n```\r\naccordion\r\nairplane\r\nanchor\r\n...\r\n```\r\nThe label id is `[line number]-1`. For example, the label id for the first line is 0, and the one for the second line is 1. If the number of labels is larger than 10, all labels are filled with 0 by the left to 3-digit numbers. For example, if the number of labels is 100, the ids are `000-099`. Users should create a directory for each label, and put the corresponding samples into the directories. We provide the processed dataset CIFAR-100 as an example, and please click [this link](http://clip-cn-beijing.oss-cn-beijing.aliyuncs.com/datasets/cifar-100.zip) to download the prepared dataset. To evaluate other datasets of ELEVATER, please refer to [Notes for datasets](zeroshot_dataset_en.md) for download.\r\n<br><br>\r\n\r\n### Prediction and Evaluation\r\nWe provide a script for prediction and evaluation. Please check `run_scripts/zeroshot_eval.sh` for more details. An example command is shown below:\r\n```bash\r\nbash run_scripts/zeroshot_eval.sh 0 \\\r\n   ${DATAPATH} ${dataset_name} \\\r\n   ${vision_model} ${text_model} \\\r\n   ${ckpt_path} ${index_file}\r\n```\r\nwhere the arguments stand for:\r\n+ the first argument `0` refers to the GPU ID\r\n+ `DATAPATH` refers to the root directory storing the checkpoint and dataset, as mentioned in Preparation part above\r\n+ `dataset_name` refers to the directory name of the dataset, e.g. cifar-100, as mentioned in Preparation part above\r\n+ `vision_model` refers to the type of vision encoder, including `[\"ViT-B-32\", \"ViT-B-16\", \"ViT-L-14\", \"ViT-L-14-336\", \"RN50\", \"ViT-H-14\"]`\r\n+ `text_model` refers to the type of text encoder, including `[\"RoBERTa-wwm-ext-base-chinese\", \"RoBERTa-wwm-ext-large-chinese\", \"RBT3-chinese\"]`\r\n+ `ckpt_path` refers to the complete path of the pretrained Chinese-CLIP checkpoint\r\n+ `index_file` is optional and only needed when you would like to submit to ELEVATER official website. Please refer to [Notes for datasets](zeroshot_dataset_en.md) for more details\r\n\r\nFor example, to evaluate ViT-B/16 on CIFAR-100, please run (the `${DATAPATH}` should be replaced with your real path):\r\n```bash\r\nbash run_scripts/zeroshot_eval.sh 0 \\\r\n    ${DATAPATH} cifar-100 \\\r\n    ViT-B-16 RoBERTa-wwm-ext-base-chinese \\\r\n    ${DATAPATH}/pretrained_weights/clip_cn_vit-b-16.pt\r\n```\r\n\r\nTop-1 accuracy will be printed. \r\n```\r\nResult:\r\nzeroshot-top1: 0.6444\r\n```\r\nOn CIFAR-100, the ViT-B/16 model of Chinese-CLIP will achieve the accuracy of 64.4%. For the zero-shot evaluation results of other model scales and other datasets, please refer to [Results.md](https://github.com/OFA-Sys/Chinese-CLIP/blob/master/Results.md#zeroshot_results).\r\n\r\nAlso, a json file will be saved, which serves the submission of ELEVATER. An example of the json file is shown below：\r\n```json\r\n{\"model_name\": \"CN-CLIP-ViT-B-16\", \"dataset_name\": \"cifar-100\", \"num_trainable_params\": 0, \"num_params\": 188262913, \"num_visual_params\": 86192640, \"num_backbone_params\": 188262913, \"n_shot\": 0, \"rnd_seeds\": [123], \"predictions\": \"prediction probability tensor [size: (1, 10000, 100)]\"}\r\n```\r\nIt includes meta data like the name of model `model_name`, the dataset name `dataset_name`, the number of parameters`num_params`, the number of parameters of vision encoder `num_visual_params`, and also the outputs of the model, namely the predicted probability tensor, whose size is `[1, num_samples, num_labels]`. \r\n\r\n### Zero-Shot Classification Online Demo\r\nBased on the representation generation API which we have integrated into Huggingface transformers, we are able to provide online demos of zero-shot classification task on Huggingface Model Hub🤗 for each scale of Chinese-CLIP model. The links are given below:\r\n- [OFA-Sys/chinese-clip-vit-base-patch16](https://huggingface.co/OFA-Sys/chinese-clip-vit-base-patch16)\r\n- [OFA-Sys/chinese-clip-vit-large-patch14](https://huggingface.co/OFA-Sys/chinese-clip-vit-large-patch14)\r\n- [OFA-Sys/chinese-clip-vit-large-patch14-336px](https://huggingface.co/OFA-Sys/chinese-clip-vit-large-patch14-336px)\r\n- [OFA-Sys/chinese-clip-vit-huge-patch14](https://huggingface.co/OFA-Sys/chinese-clip-vit-huge-patch14)\r\n- **（Update on 12.10🔥）**[**New version of demo deployed on Huggingface Spaces**](https://huggingface.co/spaces/OFA-Sys/chinese-clip-zero-shot-image-classification): the 4 model scales above are all gathered into the same demo page, supporting customed prompt template by user. **Welcome to try!**\r\n<br><br><br>\r\n\r\n\r\n# Citation\r\nIf you find the project helpful, please star this project and cite the related articles. Thanks for your support!\r\n\r\n```\r\n@article{chinese-clip,\r\n  title={Chinese CLIP: Contrastive Vision-Language Pretraining in Chinese},\r\n  author={Yang, An and Pan, Junshu and Lin, Junyang and Men, Rui and Zhang, Yichang and Zhou, Jingren and Zhou, Chang},\r\n  journal={arXiv preprint arXiv:2211.01335},\r\n  year={2022}\r\n}\r\n```\r\n"
        },
        {
          "name": "Results.md",
          "type": "blob",
          "size": 6.5703125,
          "content": "**MUGE Text-to-Image Retrieval (Official Validation Set)**:\n<table border=\"1\" width=\"100%\">\n    <tr align=\"center\">\n        <th>Setup</th><th colspan=\"4\">Zero-shot</th><th colspan=\"4\">Finetune</th>\n    </tr>\n    <tr align=\"center\">\n        <td>Metric</td><td>R@1</td><td>R@5</td><td>R@10</td><td>MR</td><td>R@1</td><td>R@5</td><td>R@10</td><td>MR</td>\n    </tr>\n\t<tr align=\"center\">\n        <td width=\"120%\">CN-CLIP<sub>RN50</sub></td><td>42.6</td><td>68.6</td><td>77.9</td><td>63.0</td><td>48.6</td><td>75.1</td><td>84.0</td><td>69.2</td>\n    </tr>  \n\t<tr align=\"center\">\n        <td width=\"120%\">CN-CLIP<sub>ViT-B/16</sub></td><td>52.1</td><td>76.7</td><td>84.4</td><td>71.1</td><td>58.4</td><td>83.6</td><td>90.0</td><td>77.4</td>\n    </tr>\n\t<tr align=\"center\">\n        <td width=\"120%\">CN-CLIP<sub>ViT-L/14</sub></td><td>56.3</td><td>79.8</td><td>86.2</td><td>74.1</td><td>63.3</td><td>85.6</td><td>91.3</td><td>80.1</td>\n    </tr>  \n\t<tr align=\"center\">\n        <td width=\"120%\">CN-CLIP<sub>ViT-L/14@336px</sub></td><td>59.0</td><td>81.4</td><td>87.8</td><td>76.1</td><td>65.3</td><td>86.7</td><td>92.1</td><td>81.3</td>\n    </tr>    \n\t<tr align=\"center\">\n        <td width=\"120%\">CN-CLIP<sub>ViT-H/14</sub></td><td><b>63.0</b></td><td><b>84.1</b></td><td><b>89.2</b></td><td><b>78.8</b></td><td><b>68.9</b></td><td><b>88.7</b></td><td><b>93.1</b></td><td><b>83.6</b></td>\n    </tr>    \n</table>\n<br>\n\n**Flickr30K-CN Retrieval (Official Test Set)**:\n<table border=\"1\" width=\"120%\">\n\t<tr align=\"center\">\n        <th>Task</th><th colspan=\"6\">Text-to-Image</th><th colspan=\"6\">Image-to-Text</th>\n    </tr>\n    <tr align=\"center\">\n        <th>Setup</th><th colspan=\"3\">Zero-shot</th><th colspan=\"3\">Finetune</th><th colspan=\"3\">Zero-shot</th><th colspan=\"3\">Finetune</th>\n    </tr>\n    <tr align=\"center\">\n        <td>Metric</td><td>R@1</td><td>R@5</td><td>R@10</td><td>R@1</td><td>R@5</td><td>R@10</td><td>R@1</td><td>R@5</td><td>R@10</td><td>R@1</td><td>R@5</td><td>R@10</td>\n    </tr>\n\t<tr align=\"center\">\n        <td width=\"120%\">CN-CLIP<sub>RN50</sub></td><td>48.8</td><td>76.0</td><td>84.6</td><td>66.7</td><td>89.4</td><td>94.1</td><td>60.0</td><td>85.9</td><td>92.0</td><td>84.2</td><td>96.7</td><td>98.0</td>\n    </tr>  \n\t<tr align=\"center\">\n        <td width=\"120%\">CN-CLIP<sub>ViT-B/16</sub></td><td>62.7</td><td>86.9</td><td>92.8</td><td>79.1</td><td>94.8</td><td>97.4</td><td>74.6</td><td>93.5</td><td>97.1</td><td>93.5</td><td>99.0</td><td>99.5</td>\n    </tr>  \n\t<tr align=\"center\">\n        <td width=\"120%\">CN-CLIP<sub>ViT-L/14</sub></td><td>68.0</td><td>89.7</td><td>94.4</td><td>82.7</td><td>96.7</td><td>98.6</td><td>80.2</td><td>96.6</td><td>98.2</td><td>96.1</td><td>99.5</td><td>99.9</td>\n    </tr>\n\t<tr align=\"center\">\n        <td width=\"120%\">CN-CLIP<sub>ViT-L/14@336px</sub></td><td>69.0</td><td>90.7</td><td>95.4</td><td><b>84.4</b></td><td><b>97.1</b></td><td><b>98.7</b></td><td><b>83.3</b></td><td>97.2</td><td>98.5</td><td><b>96.6</b></td><td><b>99.8</b></td><td><b>100.0</b></td>\n    </tr>\n\t<tr align=\"center\">\n        <td width=\"120%\">CN-CLIP<sub>ViT-H/14</sub></td><td><b>71.2</b></td><td><b>91.4</b></td><td><b>95.5</b></td><td>83.8</td><td>96.9</td><td>98.6</td><td>81.6</td><td><b>97.5</b></td><td><b>98.8</b></td><td>95.3</td><td>99.7</td><td><b>100.0</b></td>\n    </tr>  \n</table>\n<br>\n\n**COCO-CN Retrieval (Official Test Set)**:\n<table border=\"1\" width=\"120%\">\n\t<tr align=\"center\">\n        <th>Task</th><th colspan=\"6\">Text-to-Image</th><th colspan=\"6\">Image-to-Text</th>\n    </tr>\n    <tr align=\"center\">\n        <th>Setup</th><th colspan=\"3\">Zero-shot</th><th colspan=\"3\">Finetune</th><th colspan=\"3\">Zero-shot</th><th colspan=\"3\">Finetune</th>\n    </tr>\n    <tr align=\"center\">\n        <td>Metric</td><td>R@1</td><td>R@5</td><td>R@10</td><td>R@1</td><td>R@5</td><td>R@10</td><td>R@1</td><td>R@5</td><td>R@10</td><td>R@1</td><td>R@5</td><td>R@10</td>\n    </tr>\n\t<tr align=\"center\">\n        <td width=\"120%\">CN-CLIP<sub>RN50</sub></td><td>48.1</td><td>81.3</td><td>90.5</td><td>66.8</td><td>91.1</td><td>97.0</td><td>51.6</td><td>81.2</td><td>90.5</td><td>68.4</td><td>93.3</td><td>97.8</td>\n    </tr>  \n\t<tr align=\"center\">\n        <td width=\"120%\">CN-CLIP<sub>ViT-B/16</sub></td><td>62.2</td><td>86.6</td><td>94.9</td><td>77.0</td><td>97.1</td><td>99.0</td><td>57.0</td><td>84.1</td><td>93.6</td><td>77.4</td><td>96.2</td><td>98.9</td>\n    </tr>  \n\t<tr align=\"center\">\n        <td width=\"120%\">CN-CLIP<sub>ViT-L/14</sub></td><td>64.0</td><td>89.2</td><td>94.4</td><td>78.9</td><td>96.3</td><td>99.0</td><td>60.4</td><td>84.2</td><td>92.9</td><td>80.2</td><td>96.7</td><td>99.2</td>\n    </tr>\n\t<tr align=\"center\">\n        <td width=\"120%\">CN-CLIP<sub>ViT-L/14@336px</sub></td><td>64.7</td><td>89.6</td><td>94.6</td><td>80.1</td><td>96.7</td><td><b>99.2</b></td><td><b>63.4</b></td><td><b>87.2</b></td><td><b>94.4</b></td><td>81.2</td><td>97.2</td><td>99.1</td>\n    </tr>\n\t<tr align=\"center\">\n        <td width=\"120%\">CN-CLIP<sub>ViT-H/14</sub></td><td><b>69.2</b></td><td><b>89.9</b></td><td><b>96.1</b></td><td><b>81.5</b></td><td><b>96.9</b></td><td>99.1</td><td>63.0</td><td>86.6</td><td>92.9</td><td><b>83.5</b></td><td><b>97.3</b></td><td><b>99.2</b></td>\n    </tr>  \n</table>\n<br>\n\n**Zero-shot Image Classification**:<span id=\"zeroshot_results\"></span>\n<table border=\"1\" width=\"150%\">\n\t<tr align=\"center\">\n        <th>Task</th><th>CIFAR10</th><th>CIFAR100</th><th>DTD</th><th>EuroSAT</th><th>FER</th><th>FGVC</th><th>KITTI</th><th>MNIST</th><th>PC</th><th>VOC</th><th>ImageNet</th>\n    </tr>\n\t<tr align=\"center\">\n        <td width=\"150%\">CN-CLIP<sub>RN50</sub></td><td>72.7</td><td>40.6</td><td>36.9</td><td>27.0</td><td>21.9</td><td>5.4</td><td>30.2</td><td>50.2 </td><td>47.7</td><td>82.1</td><td>33.5</td>\n    </tr>\n    \t<tr align=\"center\">\n        <td width=\"150%\">CN-CLIP<sub>ViT-B/16</sub></td><td>92.0</td><td>64.4</td><td>43.6</td><td>46.9</td><td>47.2</td><td>12.8</td><td>33.5</td><td>67.6 </td><td>54.0</td><td>83.3</td><td>48.3</td>\n    </tr>\n\t<tr align=\"center\">\n        <td width=\"150%\">CN-CLIP<sub>ViT-L/14</sub></td><td>94.9</td><td>75.1</td><td>44.2</td><td>56.9</td><td>54.6</td><td>16.0</td><td>49.9</td><td>69.8 </td><td>63.5</td><td>84.5</td><td>54.7</td>\n    </tr>\n    \t<tr align=\"center\">\n        <td width=\"150%\">CN-CLIP<sub>ViT-L/14@336px</sub></td><td>94.1</td><td>73.5</td><td>43.8</td><td>50.7</td><td>55.1</td><td>17.1</td><td>49.8</td><td>65.0</td><td>62.9</td><td>84.9</td><td>56.7</td>\n    </tr>\n    \t<tr align=\"center\">\n        <td width=\"150%\">CN-CLIP<sub>ViT-H/14</sub></td><td>96.0</td><td>79.7</td><td>51.2</td><td>52.0</td><td>49.2</td><td>26.2</td><td>39.1</td><td>79.4</td><td>52.4</td><td>84.9</td><td>59.6</td>\n    </tr>\n</table>\n<br><br>\n"
        },
        {
          "name": "assets",
          "type": "tree",
          "content": null
        },
        {
          "name": "cn_clip",
          "type": "tree",
          "content": null
        },
        {
          "name": "dataset_transform.py",
          "type": "blob",
          "size": 5.384765625,
          "content": "import pandas as pd\nimport os\nimport base64\nimport json\nfrom PIL import Image\nfrom io import BytesIO\nfrom sklearn.model_selection import train_test_split\n\n'''\noriginal_dataset原始数据的路径文件夹，需修改为实际的路径\n'''\n\n#训练和验证集文本数据的文件\ndata1 = pd.read_csv('original_dataset/data1/ImageWordData.csv')\n#训练和验证集图像数据的目录\ndata1_images_folder='original_dataset/data1/ImageData'\n\n# 先将文本及对应图像id划分划分训练集和验证集\ntrain_data, val_data = train_test_split(data1, test_size=0.2, random_state=42)\n\n# 创建函数来处理数据集，使文本关联到其对应图像id的图像\ndef process_train_valid(data, images_folder, img_file, txt_file):\n    with open(img_file, 'w') as f_img, open(txt_file, 'w') as f_txt:\n        for index, row in data.iterrows():\n            # 图片内容需要被编码为base64格式\n            img_path = os.path.join(images_folder, row['image_id'])\n            with open(img_path, 'rb') as f_img_file:\n                img = Image.open(f_img_file)\n                img_buffer = BytesIO()\n                img.save(img_buffer, format=img.format)\n                byte_data = img_buffer.getvalue()\n                base64_str = base64.b64encode(byte_data).decode(\"utf-8\")\n\n            f_img.write(f\"{row['image_id']}\\t{base64_str}\\n\")\n\n            # 文本内容和图片id需要被写入jsonl文件\n            text_data = {\"text_id\": row[\"image_id\"], \"text\": row[\"caption\"], \"image_ids\": [row[\"image_id\"]]}\n            f_txt.write(json.dumps(text_data) + '\\n')\n\n# 处理训练集和验证集\n# datasets/DatasetName为在Chinese-CLIP项目目录下新建的存放转换后数据集的文件夹\nprocess_train_valid(train_data, data1_images_folder, 'Chinese-CLIP/datasets/DatasetName/train_imgs.tsv', 'Chinese-CLIP/datasets/DatasetName/train_texts.jsonl')\nprocess_train_valid(val_data, data1_images_folder, 'Chinese-CLIP/datasets/DatasetName/valid_imgs.tsv', 'Chinese-CLIP/datasets/DatasetName/valid_texts.jsonl')\n\n\n\n# 制作从文本到图像（Text_to_Image）检索时的，测试集。data2为Text_to_Image测试数据文件夹名\nimage_data2 = pd.read_csv('original_dataset/data2/image_data.csv')\nword_test2 = pd.read_csv('original_dataset/data2/word_test.csv')\n# 原始图像测试集目录\ndata2_images_folder='original_dataset/data2/ImageData'\n\n# 处理Text_to_Image测试集\ndef process_text_to_image(image_data, images_folder, word_test, img_file, txt_file):\n    with open(img_file, 'w') as f_img, open(txt_file, 'w') as f_txt:\n        for index, row in image_data.iterrows():\n            # 图片内容需要被编码为base64格式\n            img_path = os.path.join(images_folder, row['image_id'])\n            with open(img_path, 'rb') as f_img_file:\n                img = Image.open(f_img_file)\n                img_buffer = BytesIO()\n                img.save(img_buffer, format=img.format)\n                byte_data = img_buffer.getvalue()\n                base64_str = base64.b64encode(byte_data).decode(\"utf-8\")\n\n            f_img.write(f\"{row['image_id']}\\t{base64_str}\\n\")\n\n        for index, row in word_test.iterrows():\n            # 文本内容和图片id需要被写入jsonl文件\n            text_data = {\"text_id\": row[\"text_id\"], \"text\": row[\"caption\"], \"image_ids\": []}\n            f_txt.write(json.dumps(text_data) + '\\n')\n\n# datasets/DatasetName为在Chinese-CLIP项目目录下新建的存放转换后数据集的文件夹\nprocess_text_to_image(image_data2, data2_images_folder, word_test2, 'Chinese-CLIP/datasets/DatasetName/test2_imgs.tsv', 'Chinese-CLIP/datasets/DatasetName/test2_texts.jsonl')\n\n\n\n# 制作从图像到文本（Image_to_Text）检索时的，测试集。data3为Image_to_Text测试数据文件夹名\nimage_test3 = pd.read_csv('original_dataset/data3/image_test.csv')\nword_data3 = pd.read_csv('original_dataset/data3/word_data.csv')\n# 原始图像测试集目录\ndata3_images_folder='original_dataset/data3/ImageData'\n\n# 处理Image_to_Text测试集集\ndef process_image_to_text(image_data, images_folder, word_test, img_file, txt_file):\n    with open(img_file, 'w') as f_img, open(txt_file, 'w') as f_txt:\n        for index, row in image_data.iterrows():\n            # 图片内容需要被编码为base64格式\n            img_path = os.path.join(images_folder, row['image_id'])\n            with open(img_path, 'rb') as f_img_file:\n                img = Image.open(f_img_file)\n                img_buffer = BytesIO()\n                img.save(img_buffer, format=img.format)\n                byte_data = img_buffer.getvalue()\n                base64_str = base64.b64encode(byte_data).decode(\"utf-8\")\n\n            f_img.write(f\"{row['image_id']}\\t{base64_str}\\n\")\n\n        for index, row in word_test.iterrows():\n            # 文本内容和图片id需要被写入jsonl文件\n            text_data = {\"text_id\": row[\"text_id\"], \"text\": row[\"caption\"], \"image_ids\": []}\n            f_txt.write(json.dumps(text_data) + '\\n')\n\n# datasets/DatasetName为在Chinese-CLIP项目目录下新建的存放转换后数据集的文件夹\nprocess_image_to_text(image_test3, data3_images_folder, word_data3, 'Chinese-CLIP/datasets/DatasetName/test3_imgs.tsv', 'Chinese-CLIP/datasets/DatasetName/test3_texts.jsonl')\n\n\n'''\n则将tsv和jsonl文件一起序列化，转换为内存索引的LMDB数据库文件的命令如下：\npython ./Chinese-CLIP/cn_clip/preprocess/build_lmdb_dataset.py --data_dir Chinese-CLIP/datasets/DatasetName --splits train,valid,test2,test3\n'''\n\n"
        },
        {
          "name": "deployment.md",
          "type": "blob",
          "size": 25.0244140625,
          "content": "[**中文说明**](deployment.md) | [**English**](deployment_En.md)\n\n# Chinese-CLIP模型部署：ONNX & TensorRT格式转换\n\n最新的Chinese-CLIP代码，已支持将各规模的Pytorch模型，转换为[ONNX](https://onnx.ai/)或[TensorRT](https://developer.nvidia.com/tensorrt)格式，从而相比原始Pytorch模型 **[提升特征计算的推理速度](#速度对比结果)**，同时不影响特征提取的下游任务效果。下面我们给出在GPU上，准备ONNX和TensorRT格式的FP16 Chinese-CLIP部署模型的整个流程（同时给出了Chinese-CLIP预训练TensorRT模型的[下载方式](#tensorrt_download)），并附上模型效果和推理速度的对比，方便大家上手利用ONNX和TensorRT库在推理性能上的优势。\n\n## 环境准备\n\n+ **GPU硬件要求**：请准备**Volta架构及以上**的Nvidia GPU显卡（配备FP16 Tensor Core），Nvidia各架构对应显卡型号请参见[此文档表格](https://en.wikipedia.org/wiki/CUDA#GPUs_supported)。本文我们以T4显卡为例\n+ **CUDA**：推荐[CUDA](https://developer.nvidia.com/cuda-11-6-0-download-archive)版本11.6及以上，本文以11.6为例\n+ **CUDNN**：推荐[CUDNN](https://developer.nvidia.com/rdp/cudnn-archive) 8.6.0及以上，本文以8.6.0为例。请注意TensorRT和CUDNN有版本match关系，如TensorRT 8.5.x必须使用CUDNN 8.6.0，详见TensorRT的版本要求\n+ **ONNX**：注意我们转换TensorRT模型时，将沿着Pytorch → ONNX → TensorRT的步骤，所以准备TensorRT模型也需要先安装ONNX库。本文以onnx版本1.13.0，onnxruntime-gpu版本1.13.1，onnxmltools版本1.11.1为例\n+ **TensorRT**：推荐[TensorRT](https://docs.nvidia.com/deeplearning/tensorrt/archives/index.html#trt_8)版本8.5.x，本文以8.5.2.2为例。TensorRT各版本对应的CUDNN匹配版本，请从[文档页面](https://docs.nvidia.com/deeplearning/tensorrt/archives/index.html#trt_8)，查阅此TensorRT版本的\"NVIDIA TensorRT Support Matrix\"\n+ **Pytorch**：推荐1.12.1及以上，本文以1.12.1为例（建议直接pip安装1.12.1+cu116，环境尽量不要再使用conda安装cudatoolkit，避免环境CUDNN版本变化，导致TensorRT报错）\n+ [requirements.txt](requirements.txt)要求的其他依赖项\n\n执行代码\n``` \npip install tensorrt==8.5.2.2 onnx==1.13.0 onnxruntime-gpu==1.13.1 onnxmltools==1.11.1\npip install torch==1.12.1+cu116 torchvision==0.13.1+cu116 torchaudio==0.12.1 -f https://download.pytorch.org/whl/torch_stable.html\npip install -r requirements.txt \n```\n\n## 转换和运行ONNX模型\n\n### 转换模型\n\n将Pytorch模型checkpoint转换为ONNX格式的代码，请参见`cn_clip/deploy/pytorch_to_onnx.py`。我们以转换ViT-B-16规模的Chinese-CLIP预训练模型为例，具体的代码运行方式如下（请参考Readme[代码组织部分](https://github.com/OFA-Sys/Chinese-CLIP#代码组织)建好`${DATAPATH}`并替换下面的脚本内容，尽量使用相对路径）：\n\n```bash\ncd Chinese-CLIP/\nexport CUDA_VISIBLE_DEVICES=0\nexport PYTHONPATH=${PYTHONPATH}:`pwd`/cn_clip\n\n# ${DATAPATH}的指定，请参考Readme\"代码组织\"部分创建好目录，尽量使用相对路径：https://github.com/OFA-Sys/Chinese-CLIP#代码组织\ncheckpoint_path=${DATAPATH}/pretrained_weights/clip_cn_vit-b-16.pt # 指定要转换的ckpt完整路径\nmkdir -p ${DATAPATH}/deploy/ # 创建ONNX模型的输出文件夹\n\npython cn_clip/deploy/pytorch_to_onnx.py \\\n       --model-arch ViT-B-16 \\\n       --pytorch-ckpt-path ${checkpoint_path} \\\n       --save-onnx-path ${DATAPATH}/deploy/vit-b-16 \\\n       --convert-text --convert-vision\n```\n\n其中各配置项定义如下：\n+ `model-arch`: 模型规模，选项包括`[\"RN50\", \"ViT-B-16\", \"ViT-L-14\", \"ViT-L-14-336\", \"ViT-H-14\"]`，各规模细节详见[Readme](https://github.com/OFA-Sys/Chinese-CLIP#模型规模--下载链接)\n+ `pytorch-ckpt-path`: 指定Pytorch模型ckpt路径，上面的代码示例中我们指定为预训练的ckpt路径，也可以指定为用户finetune ckpt的位置。ckpt中的参数需要与`model-arch`指定的模型规模对应\n+ `save-onnx-path`: 指定输出ONNX格式模型的路径（前缀）。完成转换后，代码将分别输出文本侧和图像侧的ONNX格式编码模型文件，FP32与FP16各一版，该参数即指定了以上输出文件的路径前缀\n+ `convert-text`和`convert-vision`: 指定是否转换文本侧和图像侧模型\n+ `context-length`（可选）: 指定文本侧ONNX模型，接收输入的序列长度，默认为我们预训练ckpt所使用的52\n+ `download-root`（可选）: 如果不指定`pytorch-ckpt-path`，代码将根据`model-arch`自动下载Chinese-CLIP官方预训练ckpt用于转换，存放于`download-root`指定的目录\n\n运行此代码转换完成后，将得到以下的log输出：\n```\nFinished PyTorch to ONNX conversion...\n>>> The text FP32 ONNX model is saved at ${DATAPATH}/deploy/vit-b-16.txt.fp32.onnx\n>>> The text FP16 ONNX model is saved at ${DATAPATH}/deploy/vit-b-16.txt.fp16.onnx with extra file ${DATAPATH}/deploy/vit-b-16.txt.fp16.onnx.extra_file\n>>> The vision FP32 ONNX model is saved at ${DATAPATH}/deploy/vit-b-16.img.fp32.onnx\n>>> The vision FP16 ONNX model is saved at ${DATAPATH}/deploy/vit-b-16.img.fp16.onnx with extra file ${DATAPATH}/deploy/vit-b-16.img.fp16.onnx.extra_file\n```\n\n上面示例代码执行结束后，我们得到了ViT-B-16规模，Chinese-CLIP文本侧和图像侧的ONNX模型，可以分别用于提取图文特征。输出ONNX模型的路径均以运行脚本时的`save-onnx-path`为前缀，后面依次拼上`.img`/`.txt`、`.fp16`/`.fp32`、`.onnx`。我们后续将主要使用FP16格式的ONNX模型`vit-b-16.txt.fp16.onnx`和`vit-b-16.img.fp16.onnx`\n\n注意到部分ONNX模型文件还附带有一个extra_file，其也是对应ONNX模型的一部分。在使用这些ONNX模型时，由于`.onnx`文件存储了extra_file的路径（如`${DATAPATH}/deploy/vit-b-16.txt.fp16.onnx.extra_file`）并会根据此路径载入extra_file，所以使用ONNX模型请不要改动存放的路径名，转换时`${DATAPATH}`也尽量用相对路径（如`../datapath`），避免运行时按路径找不到extra_file报错\n\n### 运行模型\n\n#### 提取图像侧特征\n我们在`Chinese-CLIP/`目录下，使用以下的示例代码，读取刚刚转换好的ViT-B-16规模ONNX图像侧模型`vit-b-16.img.fp16.onnx`，并为Readme中示例的[皮卡丘图片](examples/pokemon.jpeg)提取图像侧特征。注意转换好的ONNX模型只接受batch大小为1的输入，即一次调用只处理一张输入图片\n\n```python\n# 完成必要的import（下文省略）\nimport onnxruntime\nfrom PIL import Image\nimport numpy as np\nimport torch\nimport argparse\nimport cn_clip.clip as clip\nfrom clip import load_from_name, available_models\nfrom clip.utils import _MODELS, _MODEL_INFO, _download, available_models, create_model, image_transform\n\n# 载入ONNX图像侧模型（**请替换${DATAPATH}为实际的路径**）\nimg_sess_options = onnxruntime.SessionOptions()\nimg_run_options = onnxruntime.RunOptions()\nimg_run_options.log_severity_level = 2\nimg_onnx_model_path=\"${DATAPATH}/deploy/vit-b-16.img.fp16.onnx\"\nimg_session = onnxruntime.InferenceSession(img_onnx_model_path,\n                                        sess_options=img_sess_options,\n                                        providers=[\"CUDAExecutionProvider\"])\n\n# 预处理图片\nmodel_arch = \"ViT-B-16\" # 这里我们使用的是ViT-B-16规模，其他规模请对应修改\npreprocess = image_transform(_MODEL_INFO[model_arch]['input_resolution'])\n# 示例皮卡丘图片，预处理后得到[1, 3, 分辨率, 分辨率]尺寸的Torch Tensor\nimage = preprocess(Image.open(\"examples/pokemon.jpeg\")).unsqueeze(0)\n\n# 用ONNX模型计算图像侧特征\nimage_features = img_session.run([\"unnorm_image_features\"], {\"image\": image.cpu().numpy()})[0] # 未归一化的图像特征\nimage_features = torch.tensor(image_features)\nimage_features /= image_features.norm(dim=-1, keepdim=True) # 归一化后的Chinese-CLIP图像特征，用于下游任务\nprint(image_features.shape) # Torch Tensor shape: [1, 特征向量维度]\n```\n\n#### 提取文本侧特征\n\n类似地，我们用如下代码完成文本侧ONNX模型的载入与特征计算，与图像侧相同，文本侧ONNX部署模型只接受batch大小为1的输入，即一次调用只处理一条输入文本。我们为4条候选文本依次计算ViT-B-16规模模型的文本特征。import相关代码与上文相同，这里省略：\n\n```python\n# 载入ONNX文本侧模型（**请替换${DATAPATH}为实际的路径**）\ntxt_sess_options = onnxruntime.SessionOptions()\ntxt_run_options = onnxruntime.RunOptions()\ntxt_run_options.log_severity_level = 2\ntxt_onnx_model_path=\"${DATAPATH}/deploy/vit-b-16.txt.fp16.onnx\"\ntxt_session = onnxruntime.InferenceSession(txt_onnx_model_path,\n                                        sess_options=txt_sess_options,\n                                        providers=[\"CUDAExecutionProvider\"])\n\n# 为4条输入文本进行分词。序列长度指定为52，需要和转换ONNX模型时保持一致（参见转换时的context-length参数）\ntext = clip.tokenize([\"杰尼龟\", \"妙蛙种子\", \"小火龙\", \"皮卡丘\"], context_length=52) \n\n# 用ONNX模型依次计算文本侧特征\ntext_features = []\nfor i in range(len(text)):\n    one_text = np.expand_dims(text[i].cpu().numpy(),axis=0)\n    text_feature = txt_session.run([\"unnorm_text_features\"], {\"text\":one_text})[0] # 未归一化的文本特征\n    text_feature = torch.tensor(text_feature)\n    text_features.append(text_feature)\ntext_features = torch.squeeze(torch.stack(text_features),dim=1) # 4个特征向量stack到一起\ntext_features = text_features / text_features.norm(dim=1, keepdim=True) # 归一化后的Chinese-CLIP文本特征，用于下游任务\nprint(text_features.shape) # Torch Tensor shape: [4, 特征向量维度]\n```\n\n#### 计算图文相似度\n\nONNX模型产出的归一化图文特征，内积后softmax即可计算相似度（需要考虑`logit_scale`），与原始Pytorch模型操作相同，参见以下代码。import相关代码与上文相同，这里省略：\n\n```python\n# 内积后softmax\n# 注意在内积计算时，由于对比学习训练时有temperature的概念\n# 需要乘上模型logit_scale.exp()，我们的预训练模型logit_scale均为4.6052，所以这里乘以100\n# 对于用户自己的ckpt，请使用torch.load载入后，查看ckpt['state_dict']['module.logit_scale']或ckpt['state_dict']['logit_scale']\nlogits_per_image = 100 * image_features @ text_features.t()\nprint(logits_per_image.softmax(dim=-1)) # 图文相似概率: [[1.2252e-03, 5.2874e-02, 6.7116e-04, 9.4523e-01]]\n```\n\n可以看到，给出的图文相似概率，和[Readme中快速使用部分](https://github.com/OFA-Sys/Chinese-CLIP#api快速上手)，基于Pytorch同一个模型计算的结果基本一致，证明了ONNX模型特征计算的正确性，而**ONNX模型的特征计算速度相比Pytorch更有优势**（详见[下文](#速度对比结果)）。\n\n## 转换和运行TensorRT模型\n\n### 转换模型\n\n相比ONNX模型，TensorRT模型具有更快的推理速度，我们提供了转换好的Chinese-CLIP预训练TensorRT图像侧和文本侧模型（[下载方式](#tensorrt_download)）。如前文所说，我们准备TensorRT格式模型，是用刚刚得到的ONNX格式模型进一步转化而来。将ONNX转换为TensorRT格式的代码，请参见`cn_clip/deploy/onnx_to_tensorrt.py`。仍然以ViT-B-16规模为例，利用刚刚得到的ONNX模型`vit-b-16.txt.fp16.onnx`和`vit-b-16.img.fp16.onnx`，在`Chinese-CLIP/`下运行如下代码：\n\n```bash\nexport PYTHONPATH=${PYTHONPATH}:`pwd`/cn_clip\n# 如前文，${DATAPATH}请根据实际情况替换\npython cn_clip/deploy/onnx_to_tensorrt.py \\\n       --model-arch ViT-B-16 \\\n       --convert-text \\\n       --text-onnx-path ${DATAPATH}/deploy/vit-b-16.txt.fp16.onnx \\\n       --convert-vision \\\n       --vision-onnx-path ${DATAPATH}/deploy/vit-b-16.img.fp16.onnx \\\n       --save-tensorrt-path ${DATAPATH}/deploy/vit-b-16 \\\n       --fp16\n```\n\n其中各配置项定义如下：\n+ `model-arch`: 模型规模，选项包括`[\"RN50\", \"ViT-B-16\", \"ViT-L-14\", \"ViT-L-14-336\", \"ViT-H-14\"]`，各规模细节详见[Readme](https://github.com/OFA-Sys/Chinese-CLIP#模型规模--下载链接)\n+ `convert-text`和`convert-vision`: 指定是否转换文本侧和图像侧模型\n+ `text-onnx-path`: 指定文本侧ONNX模型文件路径，需要与`model-arch`指定的模型规模对应\n+ `vision-onnx-path`: 指定图像侧ONNX模型文件路径，需要与`model-arch`指定的模型规模对应\n+ `save-tensorrt-path`: 指定输出TensorRT格式模型的路径（前缀）。完成转换后，代码将分别输出文本侧和图像侧的TensorRT格式编码模型文件，该参数即指定了以上输出文件的路径前缀\n+ `fp16`: 指定转换FP16精度的TensorRT格式模型\n\n整个过程根据模型规模不同，耗时几分钟到十几分钟不等。运行此代码转换完成后，将得到以下的log输出：\n```\nFinished ONNX to TensorRT conversion...\n>>> The text FP16 TensorRT model is saved at ${DATAPATH}/deploy/vit-b-16.txt.fp16.trt\n>>> The vision FP16 TensorRT model is saved at ${DATAPATH}/deploy/vit-b-16.img.fp16.trt\n```\n\n上面示例代码执行结束后，我们使用ONNX模型，得到了ViT-B-16规模，Chinese-CLIP文本侧和图像侧的TensorRT格式模型，可以用于提取图文特征。输出TensorRT模型的路径以运行脚本时的`save-tensorrt-path`为前缀，后面依次拼上`.img`/`.txt`、`.fp16`、`.trt`。我们使用两个输出文件`vit-b-16.txt.fp16.trt`和`vit-b-16.img.fp16.trt`。\n\n**对于各规模Chinese-CLIP预训练模型，我们提供转换好的TensorRT图像侧和文本侧模型（基于TensorRT 8.5.2.2版本）**，下载方式如下<span id=\"tensorrt_download\"></span>：\n\n<table border=\"1\" width=\"120%\">\n    <tr align=\"center\">\n        <td><b>模型规模</b></td><td><b>TensorRT图像侧模型</b></td><td><b>TensorRT文本侧模型</b></td>\n    </tr>\n\t<tr align=\"center\">\n        <td>CN-CLIP<sub>RN50</sub></td><td><a href=\"https://clip-cn-beijing.oss-cn-beijing.aliyuncs.com/checkpoints/rn50.img.fp16.trt\">Download</a></td><td><a href=\"https://clip-cn-beijing.oss-cn-beijing.aliyuncs.com/checkpoints/rn50.img.fp16.trt\">Download</a></td>\n    </tr>  \n\t<tr align=\"center\">\n        <td>CN-CLIP<sub>ViT-B/16</sub></td><td><a href=\"https://clip-cn-beijing.oss-cn-beijing.aliyuncs.com/checkpoints/vit-b-16.img.fp16.trt\">Download</a></td><td><a href=\"https://clip-cn-beijing.oss-cn-beijing.aliyuncs.com/checkpoints/vit-b-16.txt.fp16.trt\">Download</a></td>\n    </tr>  \n\t<tr align=\"center\">\n        <td>CN-CLIP<sub>ViT-L/14</sub></td><td><a href=\"https://clip-cn-beijing.oss-cn-beijing.aliyuncs.com/checkpoints/vit-l-14.img.fp16.trt\">Download</a></td><td><a href=\"https://clip-cn-beijing.oss-cn-beijing.aliyuncs.com/checkpoints/vit-l-14.txt.fp16.trt\">Download</a></td>\n    </tr>\n\t<tr align=\"center\">\n        <td>CN-CLIP<sub>ViT-L/14@336px</sub></td><td><a href=\"https://clip-cn-beijing.oss-cn-beijing.aliyuncs.com/checkpoints/vit-l-14-336.img.fp16.trt\">Download</a></td><td><a href=\"https://clip-cn-beijing.oss-cn-beijing.aliyuncs.com/checkpoints/vit-l-14-336.txt.fp16.trt\">Download</a></td>\n    </tr>\n\t<tr align=\"center\">\n        <td>CN-CLIP<sub>ViT-H/14</sub></td><td><a href=\"https://clip-cn-beijing.oss-cn-beijing.aliyuncs.com/checkpoints/vit-h-14.img.fp16.trt\">Download</a></td><td><a href=\"https://clip-cn-beijing.oss-cn-beijing.aliyuncs.com/checkpoints/vit-h-14.txt.fp16.trt\">Download</a></td>\n    </tr>  \n</table>\n<br>\n\n下载后直接置于`${DATAPATH}/deploy/`下即可\n\n### 运行模型\n\n在运行TensorRT模型时，如果转换和运行不是在同一个环境下，请注意运行模型的环境TensorRT库版本与转换保持一致，避免报错\n\n#### 提取图像侧特征\n\n类似于ONNX模型运行的流程，我们在`Chinese-CLIP/`目录下，使用以下的示例代码，读取刚刚转换好的ViT-B-16规模TensorRT图像侧模型`vit-b-16.img.fp16.trt`，并为Readme中示例的[皮卡丘图片](examples/pokemon.jpeg)提取图像侧特征。和ONNX模型一样，这里转换好的TensorRT模型也只接受batch大小为1的输入，即一次调用只处理一张输入图片\n\n```python\n# 完成必要的import（下文省略）\nfrom cn_clip.deploy.tensorrt_utils import TensorRTModel\nfrom PIL import Image\nimport numpy as np\nimport torch\nimport argparse\nimport cn_clip.clip as clip\nfrom clip import load_from_name, available_models\nfrom clip.utils import _MODELS, _MODEL_INFO, _download, available_models, create_model, image_transform\n\n# 载入TensorRT图像侧模型（**请替换${DATAPATH}为实际的路径**）\nimg_trt_model_path=\"${DATAPATH}/deploy/vit-b-16.img.fp16.trt\"\nimg_trt_model = TensorRTModel(img_trt_model_path)\n\n# 预处理图片\nmodel_arch = \"ViT-B-16\" # 这里我们使用的是ViT-B-16规模，其他规模请对应修改\npreprocess = image_transform(_MODEL_INFO[model_arch]['input_resolution'])\n# 示例皮卡丘图片，预处理后得到[1, 3, 分辨率, 分辨率]尺寸的Torch Tensor\nimage = preprocess(Image.open(\"examples/pokemon.jpeg\")).unsqueeze(0).cuda()\n\n# 用TensorRT模型计算图像侧特征\nimage_features = img_trt_model(inputs={'image': image})['unnorm_image_features'] # 未归一化的图像特征\nimage_features /= image_features.norm(dim=-1, keepdim=True) # 归一化后的Chinese-CLIP图像特征，用于下游任务\nprint(image_features.shape) # Torch Tensor shape: [1, 特征向量维度]\n```\n\n#### 提取文本侧特征\n\n与图像侧类似，我们用如下代码完成文本侧TensorRT模型的载入与特征计算，与图像侧相同，文本侧TensorRT部署模型只接受batch大小为1的输入，即一次调用只处理一条输入文本。TensorRT接受的文本序列长度和用于转换的ONNX模型一致，请参见ONNX转换时的context-length参数。我们为4条候选文本依次计算ViT-B-16规模模型的文本特征。import相关代码与上文相同，这里省略：\n\n```python\n# 载入TensorRT文本侧模型（**请替换${DATAPATH}为实际的路径**）\ntxt_trt_model_path=\"${DATAPATH}/deploy/vit-b-16.txt.fp16.trt\"\ntxt_trt_model = TensorRTModel(txt_trt_model_path)\n\n# 为4条输入文本进行分词。序列长度指定为52，需要和转换ONNX模型时保持一致（参见ONNX转换时的context-length参数）\ntext = clip.tokenize([\"杰尼龟\", \"妙蛙种子\", \"小火龙\", \"皮卡丘\"], context_length=52).cuda()\n\n# 用TensorRT模型依次计算文本侧特征\ntext_features = []\nfor i in range(len(text)):\n    # 未归一化的文本特征\n    text_feature = txt_trt_model(inputs={'text': torch.unsqueeze(text[i], dim=0)})['unnorm_text_features']\n    text_features.append(text_feature)\ntext_features = torch.squeeze(torch.stack(text_features), dim=1) # 4个特征向量stack到一起\ntext_features = text_features / text_features.norm(dim=1, keepdim=True) # 归一化后的Chinese-CLIP文本特征，用于下游任务\nprint(text_features.shape) # Torch Tensor shape: [4, 特征向量维度]\n```\n\n#### 计算图文相似度\n\nTensorRT模型产出的归一化图文特征，内积后softmax即可计算相似度（需要考虑`logit_scale`），与原始Pytorch和ONNX模型的操作均相同，参见以下代码。import相关代码与上文相同，这里省略：\n```python\n# 内积后softmax\n# 注意在内积计算时，由于对比学习训练时有temperature的概念\n# 需要乘上模型logit_scale.exp()，我们的预训练模型logit_scale均为4.6052，所以这里乘以100\n# 对于用户自己的ckpt，请使用torch.load载入后，查看ckpt['state_dict']['module.logit_scale']或ckpt['state_dict']['logit_scale']\nlogits_per_image = 100 * image_features @ text_features.t()\nprint(logits_per_image.softmax(dim=-1)) # 图文相似概率: [[1.2475e-03, 5.3037e-02, 6.7583e-04, 9.4504e-01]]\n```\n\n可以看到，TensorRT模型给出的图文相似概率，和[Readme中快速使用部分](https://github.com/OFA-Sys/Chinese-CLIP#api快速上手)基于Pytorch的同一份模型、以及上文ONNX模型计算的结果都基本一致，证明了TensorRT模型特征计算的正确性，而TensorRT模型的特征计算速度，**相比前两者都更胜一筹**（详见[下文](#速度对比结果)）。\n\n## 推理速度对比\n\n### 对比实验设置\n\n我们的速度对比实验，在一台单卡T4 GPU（16GB显存）机器进行，配备16个Intel Xeon(Skylake) Platinum 8163 (2.5GHz) CPU cores，64GB内存。进行速度测试时，我们采用上面的示例图片和其中一个候选文本，对Pytorch、ONNX和TensorRT模型均执行100次图文特征提取，取耗时平均值(ms)。以ViT-B-16规模测速为例，在`Chinese-CLIP/`下执行的代码如下：\n```bash\nexport CUDA_VISIBLE_DEVICES=0\nexport PYTHONPATH=${PYTHONPATH}:`pwd`/cn_clip\n\n# 替换${DATAPATH}为实际的路径\npython3 cn_clip/deploy/speed_benchmark.py \\\n        --model-arch ViT-B-16 \\\n        --pytorch-ckpt ${DATAPATH}/pretrained_weights/clip_cn_vit-b-16.pt \\\n        --pytorch-precision fp16 \\\n        --onnx-image-model ${DATAPATH}/deploy/vit-b-16.img.fp16.onnx \\\n        --onnx-text-model ${DATAPATH}/deploy/vit-b-16.txt.fp16.onnx \\\n        --tensorrt-image-model ${DATAPATH}/deploy/vit-b-16.img.fp16.trt \\\n        --tensorrt-text-model ${DATAPATH}/deploy/vit-b-16.txt.fp16.trt\n```\n\n在log输出中将先后打印以下几行，即为测速结果：\n```\n[Pytorch image inference speed (batch-size: 1):] mean=11.12ms, sd=0.05ms, min=11.00ms, max=11.32ms, median=11.11ms, 95p=11.20ms, 99p=11.30ms\n[ONNX image inference speed (batch-size: 1):] mean=4.92ms, sd=0.04ms, min=4.82ms, max=5.01ms, median=4.92ms, 95p=4.98ms, 99p=5.00ms\n[TensorRT image inference speed (batch-size: 1):] mean=3.58ms, sd=0.08ms, min=3.30ms, max=3.72ms, median=3.58ms, 95p=3.70ms, 99p=3.72ms\n\n[Pytorch text inference speed (batch-size: 1):] mean=12.47ms, sd=0.07ms, min=12.32ms, max=12.64ms, median=12.48ms, 95p=12.57ms, 99p=12.61ms\n[ONNX text inference speed (batch-size: 1):] mean=3.42ms, sd=0.44ms, min=2.96ms, max=3.89ms, median=3.45ms, 95p=3.87ms, 99p=3.88ms\n[TensorRT text inference speed (batch-size: 1):] mean=1.54ms, sd=0.01ms, min=1.51ms, max=1.57ms, median=1.54ms, 95p=1.56ms, 99p=1.56ms\n```\n\n### 速度对比结果\n\n我们列出推理batch size为1的情况下，每个规模Pytorch、ONNX和TensorRT模型的FP16精度推理耗时对比，可以看到TensorRT对于小规模模型的推理速度提升尤其明显\n<table border=\"1\" width=\"120%\">\n    <tr align=\"center\">\n        <th>单位: ms/样本</th><th colspan=\"3\">图像特征提取</th><th colspan=\"3\">文本特征提取</th>\n    </tr>\n    <tr align=\"center\">\n        <td>模型</td><td>Pytorch</td><td>ONNX</td><td>TensorRT</td><td>Pytorch</td><td>ONNX</td><td>TensorRT</td>\n    </tr>\n\t<tr align=\"center\">\n        <td width=\"120%\">CN-CLIP<sub>RN50</sub></td><td>12.93</td><td>5.04</td><td><b>1.36</b></td><td>3.64</td><td>0.95</td><td><b>0.58</b></td>\n    </tr>  \n\t<tr align=\"center\">\n        <td width=\"120%\">CN-CLIP<sub>ViT-B/16</sub></td><td>11.12</td><td>4.92</td><td><b>3.58</b></td><td>12.47</td><td>3.42</td><td><b>1.54</b></td>\n    </tr>  \n\t<tr align=\"center\">\n        <td width=\"120%\">CN-CLIP<sub>ViT-L/14</sub></td><td>21.19</td><td>17.10</td><td><b>13.08</b></td><td>12.45</td><td>3.48</td><td><b>1.52</b></td>\n    </tr>\n\t<tr align=\"center\">\n        <td width=\"120%\">CN-CLIP<sub>ViT-L/14@336px</sub></td><td>47.11</td><td>48.40</td><td><b>31.59</b></td><td>12.24</td><td>3.25</td><td><b>1.54</b></td>\n    </tr>\n\t<tr align=\"center\">\n        <td width=\"120%\">CN-CLIP<sub>ViT-H/14</sub></td><td>35.10</td><td>34.00</td><td><b>26.98</b></td><td>23.98</td><td>6.01</td><td><b>3.89</b></td>\n    </tr>  \n</table>\n<br>\n\n## 下游效果对比\n\n我们使用Chinese-CLIP实验中，所涉及的MUGE图文检索任务对比下游效果，观察Pytorch、ONNX和TensorRT FP16模型zero-shot的表现。如[Readme预测及评估部分](https://github.com/OFA-Sys/Chinese-CLIP#预测及评估)部分所述，MUGE图文检索评测结果分为图文特征提取、KNN检索和Recall计算3步。ONNX和TensorRT模型的图文特征提取脚本，请分别参见`cn_clip/eval/extract_features_onnx.py`和`cn_clip/eval/extract_features_tensorrt.py`，相比于原生Pytorch特征提取使用的`extract_features.py`仅做了微小的改动。后续的KNN和Recall计算使用的脚本和流程完全不变。\n\n我们选取ViT-B-16和ViT-H-14两个规模，结果对比如下：\n<table border=\"1\" width=\"100%\">\n    <tr align=\"center\">\n        <th>Setup</th><th colspan=\"4\">ViT-B-16 Zero-shot</th><th colspan=\"4\">ViT-H-14 Zero-shot</th>\n    </tr>\n    <tr align=\"center\">\n        <td>Metric</td><td>R@1</td><td>R@5</td><td>R@10</td><td>MR</td><td>R@1</td><td>R@5</td><td>R@10</td><td>MR</td>\n    </tr>\n\t<tr align=\"center\">\n        <td width=\"120%\">Pytorch FP16</sub></td><td>52.1</td><td>76.7</td><td>84.4</td><td>71.1</td><td>63.0</td><td>84.1</td><td>89.2</td><td>78.8</td>\n    </tr>  \n\t<tr align=\"center\">\n        <td width=\"120%\">ONNX FP16</sub></td><td>52.0</td><td>76.8</td><td>84.3</td><td>71.1</td><td>63.1</td><td>84.1</td><td>89.0</td><td>78.8</td>\n    </tr>\n\t<tr align=\"center\">\n        <td width=\"120%\">TensorRT FP16</sub></td><td>52.0</td><td>76.8</td><td>84.2</td><td>71.0</td><td>63.1</td><td>84.2</td><td>89.1</td><td>78.8</td>\n    </tr>\n</table>\n<br>\n结果指标基本是一致的，相差±0.2在可以接受的范围内（换一台机器即可能造成的误差量级），证明了ONNX和TensorRT模型的转换正确性。\n"
        },
        {
          "name": "deployment_En.md",
          "type": "blob",
          "size": 27.4482421875,
          "content": "[**中文说明**](deployment.md) | [**English**](deployment_En.md)\n\n# Chinese-CLIP Model Deployment: ONNX & TensorRT Format Conversion\n\nOur latest Chinese-CLIP code supports the conversion of Pytorch models of all scales into [ONNX](https://onnx.ai/) or [TensorRT](https://developer.nvidia.com/tensorrt) formats, thereby **[improving the inference speed of feature calculation](#speed-comparison-results)** compared with the original Pytorch models without affecting the downstream task effect of feature extraction. Below we give the whole process of preparing the FP16 Chinese-CLIP models in ONNX and TensorRT formats on GPU (and also give the [download links](#tensorrt_download) of Chinese-CLIP pretraining TensorRT models), and attach the comparison of model effect and inference speed, so that you can take advantage of the advantages of ONNX and TensorRT library in inference performance.\n\n## Environmental Preparation\n\n+ **GPU hardware requirements**: Please prepare Nvidia GPUs **with Volta architecture and above** (equipped with FP16 Tensor Core). Please refer to [this document](https://en.wikipedia.org/wiki/CUDA#GPUs_supported) for the corresponding GPUs of each Nvidia architecture. Here we take T4 GPU as an example.\n+ **CUDA**: [CUDA](https://developer.nvidia.com/cuda-11-6-0-download-archive) version 11.6 and above is recommended. We take version 11.6 as an example.\n+ **CUDNN**: [CUDNN](https://developer.nvidia.com/rdp/cudnn-archive) version 8.6.0 and above is recommended. We take version 8.6.0 as an example. Please note that TensorRT and CUDNN have version correspondence, e.g. TensorRT 8.5.x must correspond to CUDNN 8.6.0, see the TensorRT version requirements for details.\n+ **ONNX**: Note that when we convert the TensorRT model, we will follow the steps Pytorch → ONNX → TensorRT, so preparing the TensorRT model also requires installing the ONNX library first. Here we take onnx version 1.13.0, onnxruntime-gpu version 1.13.1, and onnxmltools version 1.11.1 as examples.\n+ **TensorRT**: The recommended [TensorRT](https://docs.nvidia.com/deeplearning/tensorrt/archives/index.html#trt_8) version is 8.5.x. We use 8.5.2.2 as an example. For the CUDNN version corresponding to each TensorRT version, please refer to the \"NVIDIA TensorRT Support Matrix\" from this [documentation page]((https://docs.nvidia.com/deeplearning/tensorrt/archives/index.html#trt_8)).\n+ **Pytorch**: Pytorch version 1.12.1 and above is recommended. We take version 1.12.1 as an example. (It is recommended to directly pip install 1.12.1 + cu116, and try not to use conda to install cudatoolkit, avoiding TensorRT errors due to CUDNN version changes. )\n+ Other dependencies as required in [requirements.txt](requirements.txt).\n\nexecutable code\n``` \npip install tensorrt==8.5.2.2 onnx==1.13.0 onnxruntime-gpu==1.13.1 onnxmltools==1.11.1\npip install torch==1.12.1+cu116 torchvision==0.13.1+cu116 torchaudio==0.12.1 -f https://download.pytorch.org/whl/torch_stable.html\npip install -r requirements.txt \n```\n\n## Converting and Running ONNX Models\n\n### Converting Models\n\nFor code to convert a Pytorch checkpoint to ONNX format, see `cn_clip/deploy/pytorch_to_onnx.py`. Let's take the example of converting a ViT-B-16 size Chinese-CLIP pretrained model. You can run the following code (Please refer to the [Code Organization](https://github.com/OFA-Sys/Chinese-CLIP/blob/master/README_En.md#code-organization) section of Readme to set `${DATAPATH}` and replace the script content below, using relative paths where possible. ) :\n\n```bash\ncd Chinese-CLIP/\nexport CUDA_VISIBLE_DEVICES=0\nexport PYTHONPATH=${PYTHONPATH}:`pwd`/cn_clip\n\n# Please refer to the 'Code Organization' section in Readme to set `${DATAPATH}` and replace the script content below, using relative paths where possible: https://github.com/OFA-Sys/Chinese-CLIP/blob/master/README_En.md#code-organization\ncheckpoint_path=${DATAPATH}/pretrained_weights/clip_cn_vit-b-16.pt # Specify the full path to the ckpt to be converted\nmkdir -p ${DATAPATH}/deploy/ # Create output folders for ONNX models\n\npython cn_clip/deploy/pytorch_to_onnx.py \\\n       --model-arch ViT-B-16 \\\n       --pytorch-ckpt-path ${checkpoint_path} \\\n       --save-onnx-path ${DATAPATH}/deploy/vit-b-16 \\\n       --convert-text --convert-vision\n```\n\nEach configuration item is defined as follows:\n\n+ `model-arch`: Model size, options include`[\"RN50\", \"ViT-B-16\", \"ViT-L-14\", \"ViT-L-14-336\", \"ViT-H-14\"]`, details of each model can be found in [Readme](https://github.com/OFA-Sys/Chinese-CLIP/blob/master/README_En.md#model-card). \n+ `pytorch-ckpt-path`: Specify the Pytorch model ckpt path, which in the code example above we specified as the pretrained ckpt path, or you can specify the location of your finetune ckpt. The parameters in ckpt need to correspond to the model size specified by `model-arch`.\n+ `save-onnx-path`: Specifies the path (prefix) to the output ONNX format model. After the conversion is completed, the code will output the text-side and image-side ONNX format encoded model files, one for FP32 and one for FP16, which specifies the path prefix of the above output files.\n+ `convert-text` and `convert-vision`: Specify whether to convert text-side and image-side models.\n+ `context-length` (Optional): Specify the sequence length of the input received by the text-side ONNX model, defaulting to the 52 used in our pretraining ckpt.\n+ `download-root` (Optional): If `pytorch-ckpt-path` is not specified, the code will automatically download the official Chinese-CLIP pretraining ckpt for conversion according to `model-arch`, which is stored in the specified directory `download-root`.\n\nAfter running, the following log output will be obtained:\n```\nFinished PyTorch to ONNX conversion...\n>>> The text FP32 ONNX model is saved at ${DATAPATH}/deploy/vit-b-16.txt.fp32.onnx\n>>> The text FP16 ONNX model is saved at ${DATAPATH}/deploy/vit-b-16.txt.fp16.onnx with extra file ${DATAPATH}/deploy/vit-b-16.txt.fp16.onnx.extra_file\n>>> The vision FP32 ONNX model is saved at ${DATAPATH}/deploy/vit-b-16.img.fp32.onnx\n>>> The vision FP16 ONNX model is saved at ${DATAPATH}/deploy/vit-b-16.img.fp16.onnx with extra file ${DATAPATH}/deploy/vit-b-16.img.fp16.onnx.extra_file\n```\n\nAfter the above code is executed, we get the ViT-B-16 size Chinese-CLIP text-side and image-side ONNX models, which can be used to extract image and text features respectively. The paths to the output ONNX models are all prefixed with `save-onnx-path` at the time of running the script, followed by `.img`/`.txt`, `.fp16`/`.fp32`, and `.onnx`. We will subsequently use mainly ONNX models `vit-b-16.txt.fp16.onnx` and `vit-b-16.img.fp16.onnx` in FP16 format.\n\nNotice that some of the ONNX model files also come with an extra_file, which is also part of the corresponding ONNX model. When using these ONNX models, the path to the extra_file is stored in the `.onnx` file (e.g. `${DATAPATH}/deploy/vit-b-16.txt.fp16.onnx.extra_file`) and the extra_file will be loaded according to this path, so please do not change the path and use relative path for `${DATAPATH}` when converting (e.g. `../datapath`) to avoid errors at runtime when the extra_file is not found.\n\n### Run the Model\n\n#### Extraction of Image-side Features\nWe use the following sample code in the `Chinese-CLIP/` directory to read the converted ViT-B-16 size ONNX image-side model `vit-b-16.img.fp16.onnx` and extract the features of the [Pikachu image](examples/pokemon.jpeg) in Readme. Note that the converted ONNX model only accepts inputs with a batch size of 1, i.e. only one input image is processed in one call.\n\n```python\n# Complete the necessary import (omitted below)\nimport onnxruntime\nfrom PIL import Image\nimport numpy as np\nimport torch\nimport argparse\nimport cn_clip.clip as clip\nfrom clip import load_from_name, available_models\nfrom clip.utils import _MODELS, _MODEL_INFO, _download, available_models, create_model, image_transform\n\n# Load ONNX image-side model（**Please replace ${DATAPATH} with the actual path**）\nimg_sess_options = onnxruntime.SessionOptions()\nimg_run_options = onnxruntime.RunOptions()\nimg_run_options.log_severity_level = 2\nimg_onnx_model_path=\"${DATAPATH}/deploy/vit-b-16.img.fp16.onnx\"\nimg_session = onnxruntime.InferenceSession(img_onnx_model_path,\n                                        sess_options=img_sess_options,\n                                        providers=[\"CUDAExecutionProvider\"])\n\n# Preprocess images\nmodel_arch = \"ViT-B-16\" # Here we use the ViT-B-16 size, other sizes please modify accordingly\npreprocess = image_transform(_MODEL_INFO[model_arch]['input_resolution'])\n# Example Pikachu image, Torch Tensor of [1, 3, resolution, resolution] size after preprocessing\nimage = preprocess(Image.open(\"examples/pokemon.jpeg\")).unsqueeze(0)\n\n# Calculate image-side features with ONNX model\nimage_features = img_session.run([\"unnorm_image_features\"], {\"image\": image.cpu().numpy()})[0] # Unnormalized image features\nimage_features = torch.tensor(image_features)\nimage_features /= image_features.norm(dim=-1, keepdim=True) # Normalized Chinese-CLIP image features for downstream tasks\nprint(image_features.shape) # Torch Tensor shape: [1, feature dimension]\n```\n\n#### Extraction of Text-side Features\n\nSimilarly, we use the following code to complete the loading and feature calculation of the text-side ONNX model. As with the image-side model, the text-side ONNX model only accepts inputs with a batch size of 1, i.e., only one input text is processed in a single call.\n\n```python\n# Load ONNX text-side model（**Please replace ${DATAPATH} with the actual path**）\ntxt_sess_options = onnxruntime.SessionOptions()\ntxt_run_options = onnxruntime.RunOptions()\ntxt_run_options.log_severity_level = 2\ntxt_onnx_model_path=\"${DATAPATH}/deploy/vit-b-16.txt.fp16.onnx\"\ntxt_session = onnxruntime.InferenceSession(txt_onnx_model_path,\n                                        sess_options=txt_sess_options,\n                                        providers=[\"CUDAExecutionProvider\"])\n\n# Tokenize the 4 input texts. The sequence length is specified to 52, which is the same as when converting the ONNX model (see the context_length in the conversion process).\ntext = clip.tokenize([\"杰尼龟\", \"妙蛙种子\", \"小火龙\", \"皮卡丘\"], context_length=52) \n\n# Calculate text-side features sequentially with ONNX model\ntext_features = []\nfor i in range(len(text)):\n    one_text = np.expand_dims(text[i].cpu().numpy(),axis=0)\n    text_feature = txt_session.run([\"unnorm_text_features\"], {\"text\":one_text})[0] # Unnormalized image features\n    text_feature = torch.tensor(text_feature)\n    text_features.append(text_feature)\ntext_features = torch.squeeze(torch.stack(text_features),dim=1) # 4 feature vectors stacked together\ntext_features = text_features / text_features.norm(dim=1, keepdim=True) # Normalized Chinese-CLIP text features for downstream tasks\nprint(text_features.shape) # Torch Tensor shape: [1, feature dimension]\n```\n\n#### Calculate Image & Text Similarity\n\nSimilarities are calculated from the normalized image and text features produced by the ONNX model after inner product and softmax (need to consider `logit_scale`), which is identical to the original Pytorch models, see the following code:\n\n```python\n# Inner product followed by softmax\n# Note that in the inner product calculation, due to the concept of temperature during contrast learning training, you need to multiply the model logit_scale.exp(), our pretraining model logit_scale is 4.6052, so here multiply by 100.\n# For your own ckpt, please load it using torch.load and then check ckpt['state_dict']['module.logit_scale'] or ckpt['state_dict']['logit_scale'].\nlogits_per_image = 100 * image_features @ text_features.t()\nprint(logits_per_image.softmax(dim=-1)) # Image & text similarity probabilities: [[1.2252e-03, 5.2874e-02, 6.7116e-04, 9.4523e-01]]\n```\n\nWe can see that the output similarities given by the ONNX model are largely consistent with the results calculated in [API Use Case](https://github.com/OFA-Sys/Chinese-CLIP/blob/master/README_En.md#api-use-case) section of Readme based on the same model in Pytorch, proving the correctness of the feature calculation of the ONNX model. However, **the feature calculation speed of ONNX model is more advantageous than that of Pytorch** (see [below](#Speed Comparison Results) for details).\n\n## Converting and Running TensorRT Models\n\n### Converting Models\n\nCompared with ONNX models, TensorRT models have faster inference speed, and we provide converted Chinese-CLIP pretrained TensorRT image-side and text-side models ([download links](#tensorrt_download)). As mentioned above, we prepare the TensorRT format model, which is further transformed using the ONNX format model we just obtained. For the code to convert ONNX to TensorRT format, see `cn_clip/deploy/onnx_to_tensorrt.py`. Still using the ViT-B-16 size Chinese-CLIP as an example, using the ONNX models `vit-b-16.txt.fp16.onnx` and `vit-b-16.img.fp16.onnx`, run the following code in `Chinese-CLIP/`:\n\n```bash\nexport PYTHONPATH=${PYTHONPATH}:`pwd`/cn_clip\n# As above, please replace ${DATAPATH} according to the actual situation\npython cn_clip/deploy/onnx_to_tensorrt.py \\\n       --model-arch ViT-B-16 \\\n       --convert-text \\\n       --text-onnx-path ${DATAPATH}/deploy/vit-b-16.txt.fp16.onnx \\\n       --convert-vision \\\n       --vision-onnx-path ${DATAPATH}/deploy/vit-b-16.img.fp16.onnx \\\n       --save-tensorrt-path ${DATAPATH}/deploy/vit-b-16 \\\n       --fp16\n```\n\nEach configuration item is defined as follows:\n\n+ `model-arch`: Model size, options include`[\"RN50\", \"ViT-B-16\", \"ViT-L-14\", \"ViT-L-14-336\", \"ViT-H-14\"]`, details of each model can be found in [Readme](https://github.com/OFA-Sys/Chinese-CLIP/blob/master/README_En.md#model-card). \n+ `convert-text` and `convert-vision`: Specify whether to convert text-side and image-side models.\n+ `text-onnx-path`: Specify the text-side ONNX model file path, which needs to correspond to the model size specified by `model-arch`.\n+ `vision-onnx-path`: Specify the image-side ONNX model file path, which needs to correspond to the model size specified by `model-arch`.\n+ `save-tensorrt-path`: Specifies the path (prefix) to the output TensorRT format model. After the conversion, the code will output the model files encoded in TensorRT format for the text side and image side respectively, and this parameter specifies the path prefix of the above output files.\n+ `fp16`: Specify models converted to FP16 precision TensorRT format.\n\nThe process takes a few minutes to ten minutes, depending on the model's size. After running, the following log output will be obtained:\n\n```\nFinished ONNX to TensorRT conversion...\n>>> The text FP16 TensorRT model is saved at ${DATAPATH}/deploy/vit-b-16.txt.fp16.trt\n>>> The vision FP16 TensorRT model is saved at ${DATAPATH}/deploy/vit-b-16.img.fp16.trt\n```\n\nAfter the execution of the above sample code, we obtained ViT-B-16 size Chinese-CLIP text-side and image-side TensorRT format models using ONNX models, which can be used to extract image & text features. The paths to the output TensorRT models are all prefixed with `save-tensorrt-path` when running the script, followed by `.img`/`.txt`, `.fp16`, and `.trt`. We use two output files `vit-b-16.txt.fp16.trt` and `vit-b-16.img.fp16.trt`.\n\n**For each size of Chinese-CLIP pretrained models, we provide converted TensorRT image-side and text-side models (based on TensorRT version 8.5.2.2)**, which can be downloaded as follows<span id=\"tensorrt_download\"></span>:\n\n<table border=\"1\" width=\"120%\">\n    <tr align=\"center\">\n        <td><b>Model Size</b></td><td><b>TensorRT Image-side Model</b></td><td><b>TensorRT Text-side Model</b></td>\n    </tr>\n\t<tr align=\"center\">\n        <td>CN-CLIP<sub>RN50</sub></td><td><a href=\"https://clip-cn-beijing.oss-cn-beijing.aliyuncs.com/checkpoints/rn50.img.fp16.trt\">Download</a></td><td><a href=\"https://clip-cn-beijing.oss-cn-beijing.aliyuncs.com/checkpoints/rn50.img.fp16.trt\">Download</a></td>\n    </tr>  \n\t<tr align=\"center\">\n        <td>CN-CLIP<sub>ViT-B/16</sub></td><td><a href=\"https://clip-cn-beijing.oss-cn-beijing.aliyuncs.com/checkpoints/vit-b-16.img.fp16.trt\">Download</a></td><td><a href=\"https://clip-cn-beijing.oss-cn-beijing.aliyuncs.com/checkpoints/vit-b-16.txt.fp16.trt\">Download</a></td>\n    </tr>  \n\t<tr align=\"center\">\n        <td>CN-CLIP<sub>ViT-L/14</sub></td><td><a href=\"https://clip-cn-beijing.oss-cn-beijing.aliyuncs.com/checkpoints/vit-l-14.img.fp16.trt\">Download</a></td><td><a href=\"https://clip-cn-beijing.oss-cn-beijing.aliyuncs.com/checkpoints/vit-l-14.txt.fp16.trt\">Download</a></td>\n    </tr>\n\t<tr align=\"center\">\n        <td>CN-CLIP<sub>ViT-L/14@336px</sub></td><td><a href=\"https://clip-cn-beijing.oss-cn-beijing.aliyuncs.com/checkpoints/vit-l-14-336.img.fp16.trt\">Download</a></td><td><a href=\"https://clip-cn-beijing.oss-cn-beijing.aliyuncs.com/checkpoints/vit-l-14-336.txt.fp16.trt\">Download</a></td>\n    </tr>\n\t<tr align=\"center\">\n        <td>CN-CLIP<sub>ViT-H/14</sub></td><td><a href=\"https://clip-cn-beijing.oss-cn-beijing.aliyuncs.com/checkpoints/vit-h-14.img.fp16.trt\">Download</a></td><td><a href=\"https://clip-cn-beijing.oss-cn-beijing.aliyuncs.com/checkpoints/vit-h-14.txt.fp16.trt\">Download</a></td>\n    </tr>  \n</table>\n\n<br>\n\nJust download it and place it directly under directory `${DATAPATH}/deploy/`.\n\n### Running Models\n\nWhen running the TensorRT model, if the conversion and running are not in the same environment, please note that the TensorRT library version of the environment running the model is consistent with the conversion to avoid errors.\n\n#### Extraction of Image-side Features\n\nSimilar to the process of ONNX models, we use the following sample code in the `Chinese-CLIP/` directory to read the converted ViT-B-16 size TensorRT image-side model `vit-b-16.img.fp16.trt` and extract the features of the [Pikachu image](examples/pokemon.jpeg) in Readme. The converted TensorRT model here also accepts only inputs with a batch size of 1, i.e. only one input image is processed in one call.\n\n```python\n# Complete the necessary import (omitted below)\nfrom cn_clip.deploy.tensorrt_utils import TensorRTModel\nfrom PIL import Image\nimport numpy as np\nimport torch\nimport argparse\nimport cn_clip.clip as clip\nfrom clip import load_from_name, available_models\nfrom clip.utils import _MODELS, _MODEL_INFO, _download, available_models, create_model, image_transform\n\n# Load ONNX image-side model（**Please replace ${DATAPATH} with the actual path**）\nimg_trt_model_path=\"${DATAPATH}/deploy/vit-b-16.img.fp16.trt\"\nimg_trt_model = TensorRTModel(img_trt_model_path)\n\n# Preprocess images\nmodel_arch = \"ViT-B-16\" # Here we use the ViT-B-16 size, other sizes please modify accordingly\npreprocess = image_transform(_MODEL_INFO[model_arch]['input_resolution'])\n# Example Pikachu image, Torch Tensor of [1, 3, resolution, resolution] size after preprocessing\nimage = preprocess(Image.open(\"examples/pokemon.jpeg\")).unsqueeze(0).cuda()\n\n# Calculate image-side features with TensorRT model\nimage_features = img_trt_model(inputs={'image': image})['unnorm_image_features'] # Unnormalized image features\nimage_features /= image_features.norm(dim=-1, keepdim=True) # Normalized Chinese-CLIP image features for downstream tasks\nprint(image_features.shape) # Torch Tensor shape: [1, feature dimension]\n```\n\n#### Extraction of Text-side Features\n\nSimilarly, we use the following code to complete the loading and feature calculation of the text-side TensorRT model. As with the image-side model, the text-side TensorRT model only accepts inputs with a batch size of 1, i.e., only one input text is processed in a single call. The text sequence length accepted by TensorRT is consistent with the used ONNX model, see the context-length parameter during ONNX conversion.\n\n```python\n# Load TensorRT text-side model（**Please replace ${DATAPATH} with the actual path**）\ntxt_trt_model_path=\"${DATAPATH}/deploy/vit-b-16.txt.fp16.trt\"\ntxt_trt_model = TensorRTModel(txt_trt_model_path)\n\n# Tokenize the 4 input texts. The sequence length is specified to 52, which is the same as when converting the ONNX model (see the context_length in the ONNX conversion process).\ntext = clip.tokenize([\"杰尼龟\", \"妙蛙种子\", \"小火龙\", \"皮卡丘\"], context_length=52).cuda()\n\n# Calculate text-side features sequentially with TensorRT model\ntext_features = []\nfor i in range(len(text)):\n    # Unnormalized image features\n    text_feature = txt_trt_model(inputs={'text': torch.unsqueeze(text[i], dim=0)})['unnorm_text_features']\n    text_features.append(text_feature)\ntext_features = torch.squeeze(torch.stack(text_features), dim=1) # 4 feature vectors stacked together\ntext_features = text_features / text_features.norm(dim=1, keepdim=True) # Normalized Chinese-CLIP text features for downstream tasks\nprint(text_features.shape) # Torch Tensor shape: [1, feature dimension]\n```\n\n#### Calculate Image & Text Similarity\n\nSimilarities are calculated from the normalized image and text features produced by the TensorRT model after inner product and softmax (need to consider `logit_scale`), which is identical to the original Pytorch models and ONNX models, see the following code:\n\n```python\n# Inner product followed by softmax\n# Note that in the inner product calculation, due to the concept of temperature during contrast learning training, you need to multiply the model logit_scale.exp(), our pretraining model logit_scale is 4.6052, so here multiply by 100.\n# For your own ckpt, please load it using torch.load and then check ckpt['state_dict']['module.logit_scale'] or ckpt['state_dict']['logit_scale'].\nlogits_per_image = 100 * image_features @ text_features.t()\nprint(logits_per_image.softmax(dim=-1)) # Image & text similarity probabilities: [[1.2475e-03, 5.3037e-02, 6.7583e-04, 9.4504e-01]]\n```\n\nWe can see that the output similarities given by the TensorRT model are largely consistent with the results calculated in [API Use Case](https://github.com/OFA-Sys/Chinese-CLIP/blob/master/README_En.md#api-use-case) section of Readme based on the same model in Pytorch and the results calculated by ONNX models above, proving the correctness of the feature calculation of the TensorRT model. However, the feature calculation speed of TensorRT model is **superior to both of the previous two** (see [below](#Speed Comparison Results) for details).\n\n## Comparison of Inference Speed\n\n### Comparative Experimental Setup\n\nOur experiments are conducted on a single T4 GPU (16GB memory) machine with 16 Intel Xeon (Skylake) Platinum 8163 (2.5GHz) CPU cores and 64GB memory. We use the above sample image and one of the candidate texts and perform 100 times of image and text feature extraction for both Pytorch, ONNX, and TensorRT models, taking the average time (ms). Taking the speed measurement of ViT-B-16 size model as an example, the code executed under `Chinese-CLIP/` is as follows:\n\n```bash\nexport CUDA_VISIBLE_DEVICES=0\nexport PYTHONPATH=${PYTHONPATH}:`pwd`/cn_clip\n\n# Please replace ${DATAPATH} according to the actual situation\npython3 cn_clip/deploy/speed_benchmark.py \\\n        --model-arch ViT-B-16 \\\n        --pytorch-ckpt ${DATAPATH}/pretrained_weights/clip_cn_vit-b-16.pt \\\n        --pytorch-precision fp16 \\\n        --onnx-image-model ${DATAPATH}/deploy/vit-b-16.img.fp16.onnx \\\n        --onnx-text-model ${DATAPATH}/deploy/vit-b-16.txt.fp16.onnx \\\n        --tensorrt-image-model ${DATAPATH}/deploy/vit-b-16.img.fp16.trt \\\n        --tensorrt-text-model ${DATAPATH}/deploy/vit-b-16.txt.fp16.trt\n```\n\nThe following lines will be printed in the log output:\n\n```\n[Pytorch image inference speed (batch-size: 1):] mean=11.12ms, sd=0.05ms, min=11.00ms, max=11.32ms, median=11.11ms, 95p=11.20ms, 99p=11.30ms\n[ONNX image inference speed (batch-size: 1):] mean=4.92ms, sd=0.04ms, min=4.82ms, max=5.01ms, median=4.92ms, 95p=4.98ms, 99p=5.00ms\n[TensorRT image inference speed (batch-size: 1):] mean=3.58ms, sd=0.08ms, min=3.30ms, max=3.72ms, median=3.58ms, 95p=3.70ms, 99p=3.72ms\n\n[Pytorch text inference speed (batch-size: 1):] mean=12.47ms, sd=0.07ms, min=12.32ms, max=12.64ms, median=12.48ms, 95p=12.57ms, 99p=12.61ms\n[ONNX text inference speed (batch-size: 1):] mean=3.42ms, sd=0.44ms, min=2.96ms, max=3.89ms, median=3.45ms, 95p=3.87ms, 99p=3.88ms\n[TensorRT text inference speed (batch-size: 1):] mean=1.54ms, sd=0.01ms, min=1.51ms, max=1.57ms, median=1.54ms, 95p=1.56ms, 99p=1.56ms\n```\n\n### Speed Comparison Results\n\nWe present a comparison of the FP16 precision inference time for each size of Pytorch, ONNX, and TensorRT models for an inference batch size of 1, and we can see that TensorRT has a particularly significant speedup for small-scale models.\n\n<table border=\"1\" width=\"120%\">\n    <tr align=\"center\">\n        <th>Unit: ms/sample</th><th colspan=\"3\">Image Feature Extraction</th><th colspan=\"3\">Text Feature Extraction</th>\n    </tr>\n    <tr align=\"center\">\n        <td>Models</td><td>Pytorch</td><td>ONNX</td><td>TensorRT</td><td>Pytorch</td><td>ONNX</td><td>TensorRT</td>\n    </tr>\n\t<tr align=\"center\">\n        <td width=\"120%\">CN-CLIP<sub>RN50</sub></td><td>12.93</td><td>5.04</td><td><b>1.36</b></td><td>3.64</td><td>0.95</td><td><b>0.58</b></td>\n    </tr>  \n\t<tr align=\"center\">\n        <td width=\"120%\">CN-CLIP<sub>ViT-B/16</sub></td><td>11.12</td><td>4.92</td><td><b>3.58</b></td><td>12.47</td><td>3.42</td><td><b>1.54</b></td>\n    </tr>  \n\t<tr align=\"center\">\n        <td width=\"120%\">CN-CLIP<sub>ViT-L/14</sub></td><td>21.19</td><td>17.10</td><td><b>13.08</b></td><td>12.45</td><td>3.48</td><td><b>1.52</b></td>\n    </tr>\n\t<tr align=\"center\">\n        <td width=\"120%\">CN-CLIP<sub>ViT-L/14@336px</sub></td><td>47.11</td><td>48.40</td><td><b>31.59</b></td><td>12.24</td><td>3.25</td><td><b>1.54</b></td>\n    </tr>\n\t<tr align=\"center\">\n        <td width=\"120%\">CN-CLIP<sub>ViT-H/14</sub></td><td>35.10</td><td>34.00</td><td><b>26.98</b></td><td>23.98</td><td>6.01</td><td><b>3.89</b></td>\n    </tr>  \n</table>\n<br>\n\n## Comparison of Downstream Tasks\n\nWe observe the zero-shot performance of the Pytorch, ONNX, and TensorRT FP16 models in the MUGE text-to-image retrieval task involved in the Chinese-CLIP experiments. As described in [Inference and Evaluation](https://github.com/OFA-Sys/Chinese-CLIP/blob/master/README_En.md#inference-and-evaluation) section of Readme, the results of MUGE image & text retrieval evaluation are divided into 3 steps: image & text feature extraction, KNN retrieval, and Recall calculation. The image & text feature extraction scripts for ONNX and TensorRT models, please see `cn_clip/eval/extract_features_onnx.py` and `cn_clip/eval/extract_features_tensorrt.py` respectively, compared with `extract_features.py` used for original Pytorch feature extraction, only minor changes have been made. The scripts and processes used for the subsequent KNN and Recall calculations remain exactly the same.\n\nThe results of ViT-B-16 and ViT-H-14 scales of Chinese-CLIP are compared as follows:\n<table border=\"1\" width=\"100%\">\n    <tr align=\"center\">\n        <th>Setup</th><th colspan=\"4\">ViT-B-16 Zero-shot</th><th colspan=\"4\">ViT-H-14 Zero-shot</th>\n    </tr>\n    <tr align=\"center\">\n        <td>Metric</td><td>R@1</td><td>R@5</td><td>R@10</td><td>MR</td><td>R@1</td><td>R@5</td><td>R@10</td><td>MR</td>\n    </tr>\n\t<tr align=\"center\">\n        <td width=\"120%\">Pytorch FP16</sub></td><td>52.1</td><td>76.7</td><td>84.4</td><td>71.1</td><td>63.0</td><td>84.1</td><td>89.2</td><td>78.8</td>\n    </tr>  \n\t<tr align=\"center\">\n        <td width=\"120%\">ONNX FP16</sub></td><td>52.0</td><td>76.8</td><td>84.3</td><td>71.1</td><td>63.1</td><td>84.1</td><td>89.0</td><td>78.8</td>\n    </tr>\n\t<tr align=\"center\">\n        <td width=\"120%\">TensorRT FP16</sub></td><td>52.0</td><td>76.8</td><td>84.2</td><td>71.0</td><td>63.1</td><td>84.2</td><td>89.1</td><td>78.8</td>\n    </tr>\n</table>\n<br>\n\nThe results are basically the same, with a difference of ±0.2 within an acceptable range (the magnitude of error that can be caused by changing a machine), which proves the correctness of the conversion of the ONNX and TensorRT models.\n"
        },
        {
          "name": "distillation.md",
          "type": "blob",
          "size": 4.16015625,
          "content": "[**中文说明**](distillation.md) | [**English**](distillation_En.md)\n\n# 使用知识蒸馏提升Chinese-CLIP图像检索能力\n\n本文档提供了一个结合ModelScope模型库，支持Chinese-CLIP利用知识蒸馏的方法进行微调训练的示例。通过知识蒸馏的功能，可以使用大模型（如较大规模版本的Chinese-CLIP或其他ModelScope支持的图像表征模型）蒸馏较小规模Chinese-CLIP，进一步提升Chinese-CLIP的图像检索（图到图召回）能力。使用的Teacher model由[ModelScope](https://github.com/modelscope/modelscope)提供，Chinese-CLIP全系列目前均已上线ModelScope。\n\n## 环境准备\n\n+ **Turing**、**Ampere**、**Ada**、**Hopper**架构的Nvidia GPU显卡（如H100、A100、RTX 3090、T4、RTX 2080），Nvidia各架构对应显卡型号可参见[此文档表格](https://en.wikipedia.org/wiki/CUDA#GPUs_supported)。\n+ CUDA 11.4及以上版本。\n+ Pytorch 1.12及以上版本。\n+ [requirements.txt](requirements.txt)要求的其他依赖项\n+ **ModelScope**：通过执行`pip install modelscope`安装ModelScope。\n\n## 在Chinese-CLIP中用起来！\n\n在Chinese-CLIP finetune中对于图像端应用知识蒸馏并不复杂。只需要在finetune的sh脚本中加入`--distillation`配置项。\n然后在配置项`--teacher-model-name`填入所要使用的Teacher model名称。现在支持的Teacher model包括以下四种。\n<table border=\"1\" width=\"120%\">\n    <tr align=\"center\">\n        <td><b>Teacher model</b></td><td><b>模型介绍</b></td>\n    </tr>  \n\t<tr align=\"center\">\n        <td>damo/multi-modal_clip-vit-huge-patch14_zh</td><td><a href=\"https://www.modelscope.cn/models/damo/multi-modal_clip-vit-huge-patch14_zh/summary\">CLIP模型-中文-通用领域-huge</a></td>\n    </tr>\n\t<tr align=\"center\">\n        <td>damo/multi-modal_clip-vit-large-patch14_zh</td><td><a href=\"https://www.modelscope.cn/models/damo/multi-modal_clip-vit-large-patch14_zh/summary\">CLIP模型-中文-通用领域-large</a></td>\n    </tr>\t    \n    </tr>\n\t<tr align=\"center\">\n        <td>damo/multi-modal_team-vit-large-patch14_multi-modal-similarity</td><td><a href=\"https://www.modelscope.cn/models/damo/multi-modal_team-vit-large-patch14_multi-modal-similarity/summary\">TEAM图文检索模型-中文-large</a></td>\n    </tr>  \n\t<tr align=\"center\">\n        <td>damo/multi-modal_rleg-vit-large-patch14</td><td><a href=\"https://www.modelscope.cn/models/damo/multi-modal_rleg-vit-large-patch14/summary\">RLEG生成式多模态表征模型-英文-large</a></td>\n</table>\n<br>\n\n最后在配置项`--kd_loss_weight`填入蒸馏损失的权值，默认值是0.5。\n\n\n其中各配置项定义如下：\n+ `distillation`: 是否启用知识蒸馏微调模型图像端。\n+ `teacher-model-name`: 指定使用的Teacher model。目前支持以上四个Teacher model，如填入`damo/multi-modal_team-vit-large-patch14_multi-modal-similarity`。\n+ `kd_loss_weight`（可选）: 蒸馏损失的权值，默认值是0.5。\n\n我们提供了样例脚本`run_scripts/muge_finetune_vit-b-16_rbt-base_distillation.sh`，使用的是`TEAM图文检索模型-中文-large`作为Teacher model。\n\n## 效果验证\n这里是我们模型(finetune+distillation) vs 预训练模型 vs finetune模型的图像检索Top10结果。左上角图像作为query，右边按顺序Top1到Top10检索结果。本次实验的support数据集有10万电商数据量（包括鞋子、衣服、裤子等物品）。\n\n我们方法的优势：\n+ 符合检索任务基本要求：在保证了类目相似性的前提下，很好实现了图像相似性。\n+ 性能好且速度快：通过蒸馏的方法，使得base模型有着large模型类似的检索效果。并且部署到CPU，检索推理时间控制在了100ms以内。\n\n<p style=\"text-align: center;\">\n    <img src=\"examples/image_retrieval_result1.jpg\" width=\"400\" /><br>\n    <img src=\"examples/image_retrieval_result3.jpg\" width=\"400\" /><br>\n    <img src=\"examples/image_retrieval_result2.jpg\" width=\"400\" /><br>\n</p>\n\n\n## 快速体验\n相关解决方案已经上线阿里云[PAI-DSW Gallery](https://gallery.pai-ml.com/#/preview/deepLearning/cv/cn_clip_distillation)。在PAI-DSW Gallery提供对应的Notebook，支持用户利用自有数据构建专属搜索模型。\n"
        },
        {
          "name": "distillation_En.md",
          "type": "blob",
          "size": 4.720703125,
          "content": "[**中文说明**](distillation.md) | [**English**](distillation_En.md)\n\n# Improving Chinese-CLIP Image Retrieval Ability Using Knowledge Distillation\n\nHere we provide an example of knowledge distillation for Chinese-CLIP fine-tuning training, based on [ModelScope](https://github.com/modelscope/modelscope) model library. By using knowledge distillation, smaller Chinese-CLIP models (with better inference speed) can learn from larger models (including larger Chinese-CLIP or other image embedding models on ModelScope) to further improve the image-to-image retrieval ability. The Teacher models used are all from [ModelScope](https://github.com/modelscope/modelscope). Currently, all the Chinese-CLIP have been supported on ModelScope.\n\n## Environmental Preparation\n\n+ Nvidia GPUs **with Turning, Ampere, Ada or Hopper architecture** (such as H100, A100, RTX 3090, T4, and RTX 2080). Please refer to [this document](https://en.wikipedia.org/wiki/CUDA#GPUs_supported) for the corresponding GPUs of each Nvidia architecture.\n+ CUDA 11.4 and above.\n+ PyTorch 1.12 and above.\n+ **ModelScope**：Install ModelScope by executing `pip install modelscope`.\n+ Other dependencies as required in [requirements.txt](requirements.txt).\n\n## Use it in Chinese-CLIP!\nIt is not complicated to apply knowledge distillation to the image side in Chinese-CLIP finetune. Just add the `--distillation` configuration item to the sh script of finetune.\nThen fill in the name of the Teacher model to be used in the configuration item `--teacher-model-name`. The currently supported Teacher models include the following four ModelScope-supported models.\n<table border=\"1\" width=\"120%\">\n    <tr align=\"center\">\n        <td><b>Teacher model</b></td><td><b>Model Info</b></td>\n    </tr>  \n\t<tr align=\"center\">\n        <td>damo/multi-modal_clip-vit-huge-patch14_zh</td><td><a href=\"https://www.modelscope.cn/models/damo/multi-modal_clip-vit-huge-patch14_zh/summary\">CLIP model-Chinese-general field-huge</a></td>\n    </tr>\n\t<tr align=\"center\">\n        <td>damo/multi-modal_clip-vit-large-patch14_zh</td><td><a href=\"https://www.modelscope.cn/models/damo/multi-modal_clip-vit-large-patch14_zh/summary\">CLIP model-Chinese-general field-large</a></td>\n    </tr>\t    \n    </tr>\n\t<tr align=\"center\">\n        <td>damo/multi-modal_team-vit-large-patch14_multi-modal-similarity</td><td><a href=\"https://www.modelscope.cn/models/damo/multi-modal_team-vit-large-patch14_multi-modal-similarity/summary\">TEAM image-text retrieval model-Chinese-large</a></td>\n    </tr>  \n\t<tr align=\"center\">\n        <td>damo/multi-modal_rleg-vit-large-patch14</td><td><a href=\"https://www.modelscope.cn/models/damo/multi-modal_rleg-vit-large-patch14/summary\">RLEG Generative Multimodal Representation Model-English-large</a></td>\n</table>\n<br>\n\nFinally, fill in the weight of the distillation loss in the configuration item `--kd_loss_weight`, the default value is 0.5.\n\nThe configuration items are defined as follows:\n+ `distillation`: Whether to enable knowledge distillation to fine-tune the image side of the model.\n+ `teacher-model-name`: Specify the Teacher model to use. Currently supports the above four Teacher models, such as filling in `damo/multi-modal_team-vit-large-patch14_multi-modal-similarity`.\n+ `kd_loss_weight` (optional): Distillation loss weight, default value is 0.5.\n\nWe provide a sample script `run_scripts/muge_finetune_vit-b-16_rbt-base_distillation.sh`, we take the `TEAM image-text retrieval model-Chinese-large` as Teacher model.\n\n## Effect verification\nImage retrieval Top10 results of our model (finetune+distillation) v.s. pre-trained model v.s. finetune model. The image in the upper left corner is used as a query, and the search results are in order from Top1 to Top10 on the right. The support data set in this experiment has 100,000 e-commerce data (including shoes, clothes, pants, etc.).\n\nAdvantages of our approach:\n+ Meet the basic requirements of the retrieval task: under the premise of ensuring the category similarity, the image similarity is well realized.\n+ Good performance and fast speed: Through the distillation method, the base model has a retrieval effect similar to that of the large model. And deployed to the CPU, the retrieval reasoning time is controlled within 100ms.\n\n<p style=\"text-align: center;\">\n    <img src=\"examples/image_retrieval_result1.jpg\" width=\"400\" /><br>\n    <img src=\"examples/image_retrieval_result3.jpg\" width=\"400\" /><br>\n    <img src=\"examples/image_retrieval_result2.jpg\" width=\"400\" /><br>\n</p>\n\n## Quick Start\nA solution of distillation have been launched on Alibaba Cloud [PAI-DSW Gallery](https://gallery.pai-ml.com/#/preview/deepLearning/cv/cn_clip_distillation). The corresponding Jupyter Notebook is provided in PAI-DSW Gallery to support users to build exclusive search models using their own data.\n"
        },
        {
          "name": "examples",
          "type": "tree",
          "content": null
        },
        {
          "name": "flash_attention.md",
          "type": "blob",
          "size": 3.4140625,
          "content": "[**中文说明**](flash_attention.md) | [**English**](flash_attention_En.md)\n\n# 使用FlashAttention加速Chinese-CLIP\n\nChinese-CLIP训练现已支持通过[FlashAttention](https://github.com/HazyResearch/flash-attention)加速训练进程。\n\n## 环境准备\n\n+ **Turing**、**Ampere**、**Ada**、**Hopper**架构的Nvidia GPU显卡（如H100、A100、RTX 3090、T4、RTX 2080），Nvidia各架构对应显卡型号可参见[此文档表格](https://en.wikipedia.org/wiki/CUDA#GPUs_supported)。\n+ CUDA 11.4及以上版本。\n+ Pytorch 1.12及以上版本。\n+ **FlashAttention**：通过执行`pip install flash-attn`安装FlashAttention。\n\n更多信息可参见[FlashAttention项目仓库](https://github.com/HazyResearch/flash-attention)。\n\n## 在Chinese-CLIP中用起来！\n\n在Chinese-CLIP finetune中应用FlashAttention非常简单，只需要在finetune的sh脚本中加入`--use-flash-attention`配置项即可。我们提供了样例脚本`run_scripts/muge_finetune_vit-b-16_rbt-base_flashattn.sh`。\n\n\n## 训练速度和显存占用对比\n\n启用FlashAttention可在不影响效果的条件下为Chinese-CLIP的finetune过程显著提速以及降低显存占用。我们的实验在一台8卡A100 GPU（80GB显存）机器进行，FlashAttention 0.2.8，Pytorch 1.10.1。\n\n我们分别列出finetune过程中，相同batch size下启用FlashAttention前后每个规模模型的FP16精度finetune的batch time和显存占用对比，可以看到启用FlashAttention后，训练速度有所提升，也更加节约显存。对于更大规模模型的训练速度提升和显存占用降低更为显著。\n\n<table border=\"1\" width=\"120%\">\n    <tr align=\"center\">\n        <th></th><th colspan=\"4\">Batch Time</th>\n    </tr>\n    <th>单位: 秒/it</th><th>Batch size</th><th>w/o FlashAttention</th><th>w/ FlashAttention</th><th>Speedup</th>\n    </tr>\n    <tr align=\"center\">\n        <td width=\"120%\">CN-CLIP<sub>RN50</sub></td><td>1200*8</td><td>1.710</td><td>1.680</td><td>1.02×</td>\n    </tr>  \n    <tr align=\"center\">\n        <td width=\"120%\">CN-CLIP<sub>ViT-B/16</sub></td><td>450*8</td><td>1.477</td><td>0.960</td><td>1.54×</td>\n    </tr>  \n    <tr align=\"center\">\n        <td width=\"120%\">CN-CLIP<sub>ViT-L/14</sub></td><td>128*8</td><td>1.293</td><td>0.785</td><td>1.65×</td>\n    </tr>\n    <tr align=\"center\">\n        <td width=\"120%\">CN-CLIP<sub>ViT-L/14@336px</sub></td><td>40*8</td><td>1.397</td><td>0.587</td><td>2.38×</td>\n    </tr>\n    <tr align=\"center\">\n        <td width=\"120%\">CN-CLIP<sub>ViT-H/14</sub></td><td>64*8</td><td>1.265</td><td>0.845</td><td>1.50×</td>\n    </tr>  \n</table>\n<br>\n\n<table border=\"1\" width=\"120%\">\n    <tr align=\"center\">\n        <th></th><th colspan=\"4\">显存</th>\n    </tr>\n    <th>单位: GB</th><th>Batch size</th><th>w/o FlashAttention</th><th>w/ FlashAttention</th>\n    </tr>\n    <tr align=\"center\">\n        <td width=\"120%\">CN-CLIP<sub>RN50</sub></td><td>1200*8</td><td>79</td><td>75</td>\n    </tr>  \n    <tr align=\"center\">\n        <td width=\"120%\">CN-CLIP<sub>ViT-B/16</sub></td><td>450*8</td><td>80</td><td>56</td>\n    </tr>  \n    <tr align=\"center\">\n        <td width=\"120%\">CN-CLIP<sub>ViT-L/14</sub></td><td>128*8</td><td>77</td><td>50</td>\n    </tr>\n    <tr align=\"center\">\n        <td width=\"120%\">CN-CLIP<sub>ViT-L/14@336px</sub></td><td>40*8</td><td>78</td><td>37</td>\n    </tr>\n    <tr align=\"center\">\n        <td width=\"120%\">CN-CLIP<sub>ViT-H/14</sub></td><td>64*8</td><td>76</td><td>57</td>\n    </tr>  \n</table>\n<br>\n"
        },
        {
          "name": "flash_attention_En.md",
          "type": "blob",
          "size": 3.462890625,
          "content": "[**中文说明**](flash_attention.md) | [**English**](flash_attention_En.md)\r\n\r\n# Accelerate Chinese-CLIP with FlashAttention\r\n\r\nChinese-CLIP now supports the acceleration of training process through [FlashAttention](https://github.com/HazyResearch/flash-attention).\r\n\r\n## Environmental Preparation\r\n\r\n+ Nvidia GPUs **with Turning, Ampere, Ada or Hopper architecture** (such as H100, A100, RTX 3090, T4, and RTX 2080). Please refer to [this document](https://en.wikipedia.org/wiki/CUDA#GPUs_supported) for the corresponding GPUs of each Nvidia architecture.\r\n+ CUDA 11.4 and above.\r\n+ PyTorch 1.12 and above.\r\n+ **FlashAttention**：Install FlashAttention by executing `pip install flash-attn`.\r\n\r\nPlease refer to the [FlashAttention project repository](https://github.com/HazyResearch/flash-attention) for more information.\r\n\r\n## Use it in Chinese-CLIP!\r\n\r\nApplying FlashAttention to the finetune process of Chinese-CLIP is very simple, just add `--use-flash-attention` to the sh script of finetune. We provide the sample script `run_scripts/muge_finetune_vit-b-16_rbt-base_flashattn.sh`.\r\n\r\n\r\n## Training Speed and Memory Usage Comparison\r\n\r\nEnabling FlashAttention can significantly speed up the finetune process and reduce the memory usage of Chinese-CLIP without affecting the precision. Our experiments are conducted on an 8-card A100 GPU (80GB memory) machine，FlashAttention 0.2.8，Pytorch 1.10.1.\r\n\r\nWe present the comparison of the batch time and memory usage of FP16 precision finetune for each scale model. The improvement in training speed and reduction in memory usage are more significant for larger models.\r\n\r\n<table border=\"1\" width=\"120%\">\r\n    <tr align=\"center\">\r\n        <th></th><th colspan=\"4\">Batch Time</th>\r\n    </tr>\r\n    <th>Unit: s/it</th><th>Batch size</th><th>w/o FlashAttention</th><th>w/ FlashAttention</th><th>Speedup</th>\r\n    </tr>\r\n    <tr align=\"center\">\r\n        <td width=\"120%\">CN-CLIP<sub>RN50</sub></td><td>1200*8</td><td>1.710</td><td>1.680</td><td>1.02×</td>\r\n    </tr>  \r\n    <tr align=\"center\">\r\n        <td width=\"120%\">CN-CLIP<sub>ViT-B/16</sub></td><td>450*8</td><td>1.477</td><td>0.960</td><td>1.54×</td>\r\n    </tr>  \r\n    <tr align=\"center\">\r\n        <td width=\"120%\">CN-CLIP<sub>ViT-L/14</sub></td><td>128*8</td><td>1.293</td><td>0.785</td><td>1.65×</td>\r\n    </tr>\r\n    <tr align=\"center\">\r\n        <td width=\"120%\">CN-CLIP<sub>ViT-L/14@336px</sub></td><td>40*8</td><td>1.397</td><td>0.587</td><td>2.38×</td>\r\n    </tr>\r\n    <tr align=\"center\">\r\n        <td width=\"120%\">CN-CLIP<sub>ViT-H/14</sub></td><td>64*8</td><td>1.265</td><td>0.845</td><td>1.50×</td>\r\n    </tr>  \r\n</table>\r\n<br>\r\n\r\n<table border=\"1\" width=\"120%\">\r\n    <tr align=\"center\">\r\n        <th></th><th colspan=\"4\">Memory</th>\r\n    </tr>\r\n    <th>Unit: GB</th><th>Batch size</th><th>w/o FlashAttention</th><th>w/ FlashAttention</th>\r\n    </tr>\r\n    <tr align=\"center\">\r\n        <td width=\"120%\">CN-CLIP<sub>RN50</sub></td><td>1200*8</td><td>79</td><td>75</td>\r\n    </tr>  \r\n    <tr align=\"center\">\r\n        <td width=\"120%\">CN-CLIP<sub>ViT-B/16</sub></td><td>450*8</td><td>80</td><td>56</td>\r\n    </tr>  \r\n    <tr align=\"center\">\r\n        <td width=\"120%\">CN-CLIP<sub>ViT-L/14</sub></td><td>128*8</td><td>77</td><td>50</td>\r\n    </tr>\r\n    <tr align=\"center\">\r\n        <td width=\"120%\">CN-CLIP<sub>ViT-L/14@336px</sub></td><td>40*8</td><td>78</td><td>37</td>\r\n    </tr>\r\n    <tr align=\"center\">\r\n        <td width=\"120%\">CN-CLIP<sub>ViT-H/14</sub></td><td>64*8</td><td>76</td><td>57</td>\r\n    </tr>  \r\n</table>\r\n<br>\r\n"
        },
        {
          "name": "requirements.txt",
          "type": "blob",
          "size": 0.0546875,
          "content": "numpy\ntqdm\nsix\ntimm\nlmdb==1.3.0\ntorch>=1.7.1\ntorchvision"
        },
        {
          "name": "run_scripts",
          "type": "tree",
          "content": null
        },
        {
          "name": "setup.py",
          "type": "blob",
          "size": 1.658203125,
          "content": "import os\n\nimport pkg_resources\nfrom setuptools import setup, find_packages\n\npackages = find_packages(exclude=[\"tests*\"])\nwith open('README_En.md', 'r', encoding='utf-8') as fp:\n    long_description = fp.read()\nsetup(\n    name=\"cn_clip\",\n    py_modules=[\"cn_clip\"],\n    version=\"1.5.1\",\n    author=\"OFA-Sys\",\n    author_email=\"\",\n    long_description=long_description,\n    long_description_content_type=\"text/markdown\",\n    packages = packages,\n    keywords='clip',\n    install_requires=[\n        str(r)\n        for r in pkg_resources.parse_requirements(\n            open(os.path.join(os.path.dirname(__file__), \"requirements.txt\"))\n        )\n    ],\n    data_files=[('clip/model_configs', ['cn_clip/clip/model_configs/RoBERTa-wwm-ext-base-chinese.json',\n                                        'cn_clip/clip/model_configs/RoBERTa-wwm-ext-large-chinese.json',\n                                        'cn_clip/clip/model_configs/ViT-B-16.json',\n                                        'cn_clip/clip/model_configs/ViT-B-32.json',\n                                        'cn_clip/clip/model_configs/ViT-L-14.json',\n                                        'cn_clip/clip/model_configs/ViT-L-14-336.json',\n                                        'cn_clip/clip/model_configs/ViT-H-14.json',\n                                        'cn_clip/clip/model_configs/RN50.json',\n                                        'cn_clip/clip/model_configs/RBT3-chinese.json'\n                                        ]),\n                ('clip/', ['cn_clip/clip/vocab.txt'])\n                ],\n    include_package_data=True,\n    url='https://github.com/OFA-Sys/Chinese-CLIP',\n    description='the Chinese version of CLIP.'\n)\n"
        },
        {
          "name": "zeroshot_dataset.md",
          "type": "blob",
          "size": 3.51953125,
          "content": "[**中文说明**](zeroshot_dataset.md) | [**English**](zeroshot_dataset_en.md)\n\n# 零样本图像分类数据集\n\n本数据集为[ELEVATER Benchmark](https://eval.ai/web/challenges/challenge-page/1832)的图像分类基准的**中文版**，共包括20个图像分类数据集，包括Caltech-101、CIFAR-10、CIFAR-100、MNIST等。我们提供整理好的数据集，可以直接接入Chinese CLIP的代码进行零样本分类。\n\n下载链接：[点击这里](https://clip-cn-beijing.oss-cn-beijing.aliyuncs.com/datasets/ELEVATER_all.zip)\n\nImageNet的原始数据请自行到官网下载（可参考[此文档](https://gist.github.com/antoinebrl/7d00d5cb6c95ef194c737392ef7e476a)下载并将验证集转为ImageFolder格式），本项目仅提供[中文标签](http://clip-cn-beijing.oss-cn-beijing.aliyuncs.com/datasets/ImageNet-1K/label_cn.txt)和[英文标签](http://clip-cn-beijing.oss-cn-beijing.aliyuncs.com/datasets/ImageNet-1K/label.txt)。\n\n\n## 数据集说明\n我们将20个数据集分别置于20个文件夹中，统一打包上传，用户通过点击上述链接即可下载全部数据。`ELEVATER_all.zip`解压后，将得到每个数据集的zip压缩包。选择对应的压缩包再次解压后，每个文件夹的内容如下所示：\n```\n${dataset_name}\n├── index.json  # 个别数据集包含这个文件，仅用于提交ELEVATER benchmark\n├── label_cn.txt  # 中文标签名文件，每一行一个类别名\n├── label.txt  # 英文标签名文件，每一行一个类别名\n├── test/\n│   ├── 000/\n│   ├── 001/\n│   └── 002/\n└── train/\n    ├── 000/\n    ├── 001/\n    └── 002/\n```\n`${dataset_name}`表示每个数据集的文件夹路径，如`cifar-100`，里面包括`train`和`test`两个文件夹，每个文件夹包含了以id编号命名的文件夹，分别代表每一个类别。另外还包含3个文件，分别为中文标签名文件`label_cn.txt`和英文标签名文件`label.txt`。其中：\n\n* 类别数在10个及以下的情况下，如10，类别的id分别为[0-9]\n* 类别数在10个以上的情况下，如100，类别的id分别为[000-099]，即向左补零到3位数。这是为了保证我们的id是以字典序进行排序\n* 每个id对应的类别标签名为标签文件中的第${id}行（0-index），如`0`即对应标签文件中的第0行的类别名，`099`对应的是标签文件的第99行类别名。\n\n训练和测试集文件夹内包含的子文件夹用字典序排序的原因是因为我们的代码使用了torchvision的dataset，默认文件夹内数据按照类别归类子文件夹，按照文件名以字典序排序。\n\n标签文件包含中文版和原版两个文件，我们的代码仅需使用`label_cn.txt`，`label.txt`仅供参考。文件内容为每一行1个类别名，示例如下：\n```\n飞机\n汽车\n……\n```\n\n`index.json`仅用于提交ELEVATER benchmark使用，且并非每个数据集都包含此文件。该文件的原因是ELEVATER官方评测部分数据集的测试集样本顺序经过调整，如需保证提交结果正常需要调整样本顺序。如遇到数据集包含此文件，则可在测试运行命令中加上` index.json`即可。\n\n类似地，如您自行准备ImageNet数据，请将上述中文和英文标签文件放入`${dataset_name}`，并在其中创建相应文件夹，如`train`和`test`，将图片按照类别归档并放入对应文件夹，并保证其按字典序排序，如`000-999`，实现的文件结构和上述示例保持一致，即可实现零样本分类的数据准备。\n"
        },
        {
          "name": "zeroshot_dataset_en.md",
          "type": "blob",
          "size": 3.716796875,
          "content": "[**中文说明**](zeroshot_dataset.md) | [**English**](zeroshot_dataset_en.md)\n\n# Zero-shot Image Classification Datasets\n\nThe collection of dataset is the Chinese version of the Image Classification in the Wild in the [ELEVATER Benchmark](https://eval.ai/web/challenges/challenge-page/1832). It consists of 20 datasets, including Caltech-101, CIFAR-10, CIFAR-100, MNIST, etc. We provide our organized datasets, which enable direct usage of our codes on the datasets. \n\nDownload link: [Click here](https://clip-cn-beijing.oss-cn-beijing.aliyuncs.com/datasets/ELEVATER_all.zip)\n\nFor the ImageNet data, please visit the official website ([link](http://image-net.org)). You can refer to [this doc](https://gist.github.com/antoinebrl/7d00d5cb6c95ef194c737392ef7e476a) to prepare the validation set into ImageFolder format. This project only provides the label names in [Chinese](http://clip-cn-beijing.oss-cn-beijing.aliyuncs.com/datasets/ImageNet-1K/label_cn.txt) and [English](http://clip-cn-beijing.oss-cn-beijing.aliyuncs.com/datasets/ImageNet-1K/label.txt).\n\n## Notes\nWe have organized 20 datasets into 20 directories, and zipped and uploaded them. Users can click the link above to download all the datasets. After unzipping the `ELEVATER_all.zip`, you will get the zipped files of each dataset in ELEVATER. Once again after unzipping the dataset zipfile, you will get the dataset directory with the following folder structure:\n```\n${dataset_name}\n├── index.json  # Some datasets contain this file，which only serves for the submission to the ELEVATER benchmark\n├── label_cn.txt  # File of Chinese labels，where the text in each line refers to the label name\n├── label.txt  # File of English labels，where the text in each line refers to the label name\n├── test/\n│   ├── 000/\n│   ├── 001/\n│   └── 002/\n└── train/\n    ├── 000/\n    ├── 001/\n    └── 002/\n```\n`${dataset_name}` refers to the directory of each dataset, where there are two directories named `train` and `test`. Each directory contains sub-directories named with id, which refers to a category. Additionally, there are 2 files, namely the Chinese label file`label_cn.txt`, the English label file `label.txt`. Note that:\n\n* When the number of labels is no larger than 10, e,g., 10, the ids are [0-9]\n* When the number of labels is larger than 10, e.g., 100, the ids are [000-099]，which are left padded with 0 to 3-digit numbers. This serves for alphabetic order. \n* Each id refers to the label name in the ${id}-th line of the label file (0-index). For example, `0` refers to the label name in the 0-th line, and `099` refers to the label name in the 99-th line\n\nThe sub-directories of training and test sets are alphatically ordered for the reason that we use `torchvision.dataset`, which requires to organize data to sub-directories based on their labels and alphabetically ordered. \n\nThere are two label files for Chinese and English. We only use `label_cn.txt` in our experiments, and `label.txt` is for reference only. Each line contains a label name. The example is shown below:\n```\n飞机\n汽车\n……\n```\n\n`index.json` only serves for the submission of the ELEVATER benchmark, and not every dataset contains this file. The existence of this file is due to the specified order of test data. To make your submission available, you need to add ` index.json` in your command. \n\nSimilarly, if you prepare the ImageNet data, please put the label files mentioned above in the directory `${dataset_name}`, and create directories like `train` and `test`, and file the images according to their categories into directories, which should be ordered by the alphabetic order, like `000-999`. The organization should be consistent with the abovementioned example. \n"
        }
      ]
    }
  ]
}