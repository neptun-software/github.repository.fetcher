{
  "metadata": {
    "timestamp": 1736560010018,
    "page": 819,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjgyMA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "facebookresearch/PyTorch-BigGraph",
      "stars": 3390,
      "defaultBranch": "main",
      "files": [
        {
          "name": ".circleci",
          "type": "tree",
          "content": null
        },
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.107421875,
          "content": "*~\nbuild/\ndata/\ndist/\ndocs/build/\n.DS_Store\n*.egg\n*.egg-info/\n.idea/\nmodel/\n__pycache__/\n*.py[cod]\n*.so\nvenv/\n"
        },
        {
          "name": "CHANGELOG.md",
          "type": "blob",
          "size": 0.037109375,
          "content": "# Changelog\n\n## main\n\nInitial version\n"
        },
        {
          "name": "CODE_OF_CONDUCT.md",
          "type": "blob",
          "size": 3.27734375,
          "content": "# Code of Conduct\n\n## Our Pledge\n\nIn the interest of fostering an open and welcoming environment, we as\ncontributors and maintainers pledge to make participation in our project and\nour community a harassment-free experience for everyone, regardless of age, body\nsize, disability, ethnicity, sex characteristics, gender identity and expression,\nlevel of experience, education, socio-economic status, nationality, personal\nappearance, race, religion, or sexual identity and orientation.\n\n## Our Standards\n\nExamples of behavior that contributes to creating a positive environment\ninclude:\n\n* Using welcoming and inclusive language\n* Being respectful of differing viewpoints and experiences\n* Gracefully accepting constructive criticism\n* Focusing on what is best for the community\n* Showing empathy towards other community members\n\nExamples of unacceptable behavior by participants include:\n\n* The use of sexualized language or imagery and unwelcome sexual attention or\n  advances\n* Trolling, insulting/derogatory comments, and personal or political attacks\n* Public or private harassment\n* Publishing others' private information, such as a physical or electronic\n  address, without explicit permission\n* Other conduct which could reasonably be considered inappropriate in a\n  professional setting\n\n## Our Responsibilities\n\nProject maintainers are responsible for clarifying the standards of acceptable\nbehavior and are expected to take appropriate and fair corrective action in\nresponse to any instances of unacceptable behavior.\n\nProject maintainers have the right and responsibility to remove, edit, or\nreject comments, commits, code, wiki edits, issues, and other contributions\nthat are not aligned to this Code of Conduct, or to ban temporarily or\npermanently any contributor for other behaviors that they deem inappropriate,\nthreatening, offensive, or harmful.\n\n## Scope\n\nThis Code of Conduct applies within all project spaces, and it also applies when\nan individual is representing the project or its community in public spaces.\nExamples of representing a project or community include using an official\nproject e-mail address, posting via an official social media account, or acting\nas an appointed representative at an online or offline event. Representation of\na project may be further defined and clarified by project maintainers.\n\n## Enforcement\n\nInstances of abusive, harassing, or otherwise unacceptable behavior may be\nreported by contacting the project team at <opensource-conduct@fb.com>. All\ncomplaints will be reviewed and investigated and will result in a response that\nis deemed necessary and appropriate to the circumstances. The project team is\nobligated to maintain confidentiality with regard to the reporter of an incident.\nFurther details of specific enforcement policies may be posted separately.\n\nProject maintainers who do not follow or enforce the Code of Conduct in good\nfaith may face temporary or permanent repercussions as determined by other\nmembers of the project's leadership.\n\n## Attribution\n\nThis Code of Conduct is adapted from the [Contributor Covenant][homepage], version 1.4,\navailable at https://www.contributor-covenant.org/version/1/4/code-of-conduct.html\n\n[homepage]: https://www.contributor-covenant.org\n\nFor answers to common questions about this code of conduct, see\nhttps://www.contributor-covenant.org/faq\n\n"
        },
        {
          "name": "CONTRIBUTING.md",
          "type": "blob",
          "size": 1.818359375,
          "content": "# Contributing to torchbiggraph\n\nWe want to make contributing to this project as easy and transparent as\npossible.\n\n## Our Development Process\n\nThis project's source-of-truth is the version in Facebook's internal codebase,\nwhich is continuously synced with the GitHub mirror using\n[ShipIt](https://github.com/facebook/fbshipit). Pull requests on GitHub are\ncopied over using ImportIt (a companion tool for ShipIt).\n\n## Pull Requests\n\nWe actively welcome your pull requests.\n\n1. Fork the repo and create your branch from `main`.\n2. If you've added code that should be tested, add tests.\n3. If you've changed APIs, update the documentation.\n4. Ensure the test suite passes.\n5. Make sure your code lints.\n6. If you haven't already, complete the Contributor License Agreement (\"CLA\").\n\n## Contributor License Agreement (\"CLA\")\n\nIn order to accept your pull request, we need you to submit a CLA. You only need\nto do this once to work on any of Facebook's open source projects.\n\nComplete your CLA here: <https://code.facebook.com/cla>\n\n## Issues\n\nWe use GitHub issues to track public bugs. Please ensure your description is\nclear and has sufficient instructions to be able to reproduce the issue.\n\nFacebook has a [bounty program](https://www.facebook.com/whitehat/) for the safe\ndisclosure of security bugs. In those cases, please go through the process\noutlined on that page and do not file a public issue.\n\n## Coding Style\n\nThis project adheres to the [PEP 8](https://www.python.org/dev/peps/pep-0008/)\nstyle guidelines. It is linted using [Flake8](https://pypi.org/project/flake8/).\nAdditional conventions from the [Black](https://black.readthedocs.io/en/stable/)\nformatter are sometimes adopted.\n\n## License\n\nBy contributing to torchbiggraph, you agree that your contributions will be\nlicensed under the LICENSE.txt file in the root directory of this source tree.\n"
        },
        {
          "name": "LICENSE.txt",
          "type": "blob",
          "size": 1.50390625,
          "content": "BSD License\n\nFor torchbiggraph software\n\nCopyright (c) Facebook, Inc. and its affiliates. All rights reserved.\n\nRedistribution and use in source and binary forms, with or without modification,\nare permitted provided that the following conditions are met:\n\n * Redistributions of source code must retain the above copyright notice, this\n   list of conditions and the following disclaimer.\n\n * Redistributions in binary form must reproduce the above copyright notice,\n   this list of conditions and the following disclaimer in the documentation\n   and/or other materials provided with the distribution.\n\n * Neither the name Facebook nor the names of its contributors may be used to\n   endorse or promote products derived from this software without specific\n   prior written permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND\nANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED\nWARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\nDISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR\nANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES\n(INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;\nLOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON\nANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n(INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS\nSOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n"
        },
        {
          "name": "MANIFEST.in",
          "type": "blob",
          "size": 0.0810546875,
          "content": "global-include LICENSE*\ninclude *.md\ngraft docs\nprune docs/build\nglobal-exclude .*\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 15.494140625,
          "content": "# ![PyTorch-BigGraph](docs/source/_static/logo_color.svg)\n\n[![Support Ukraine](https://img.shields.io/badge/Support-Ukraine-FFD500?style=flat&labelColor=005BBB)](https://opensource.fb.com/support-ukraine) [![CircleCI Status](https://circleci.com/gh/facebookresearch/PyTorch-BigGraph.svg?style=svg)](https://circleci.com/gh/facebookresearch/PyTorch-BigGraph) [![Documentation Status](https://readthedocs.org/projects/torchbiggraph/badge/?version=latest)](https://torchbiggraph.readthedocs.io/en/latest/?badge=latest)\n\nPyTorch-BigGraph (PBG) is a distributed system for learning graph embeddings for large graphs, particularly big web interaction graphs with up to billions of entities and trillions of edges.\n\n\nPBG was introduced in the [PyTorch-BigGraph: A Large-scale Graph Embedding Framework](https://mlsys.org/Conferences/2019/doc/2019/71.pdf) paper, presented at the [SysML conference](https://mlsys.org/) in 2019.\n\n**Update:** *PBG now supports GPU training. Check out the [GPU Training](#gpu-training) section below!*\n\n<!-- toc -->\n- [Overview](#overview)\n- [Requirements](#requirements)\n- [Installation](#installation)\n- [Getting Started](#getting-started)\n  - [Downloading the data](#downloading-the-data)\n  - [Preparing the data](#preparing-the-data)\n  - [Training](#training)\n    - [GPU Training](#gpu-training)\n  - [Evaluation](#evaluation)\n  - [Converting the output](#converting-the-output)\n- [Pre-trained embeddings](#pre-trained-embeddings)\n- [Documentation](#documentation)\n- [Communication](#communication)\n- [Citation](#citation)\n- [License](#license)\n\n<!-- tocstop -->\n\n## Overview \nPBG trains on an input graph by ingesting its list of edges, each identified by its source and target entities and, possibly, a relation type. It outputs a feature vector (embedding) for each entity, trying to place adjacent entities close to each other in the vector space, while pushing unconnected entities apart. Therefore, entities that have a similar distribution of neighbors will end up being nearby.\n\nIt is possible to configure each relation type to calculate this \"proximity score\" in a different way, with the parameters (if any) learned during training. This allows the same underlying entity embeddings to be shared among multiple relation types.\n\nThe generality and extensibility of its model allows PBG to train a number of models from the knowledge graph embedding literature, including [TransE](https://www.utc.fr/~bordesan/dokuwiki/_media/en/transe_nips13.pdf), [RESCAL](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.383.2015&rep=rep1&type=pdf), [DistMult](https://arxiv.org/abs/1412.6575) and [ComplEx](http://proceedings.mlr.press/v48/trouillon16.pdf).\n\nPBG is designed with scale in mind, and achieves it through:\n- *graph partitioning*, so that the model does not have to be fully loaded into memory\n- *multi-threaded computation* on each machine\n- *distributed execution* across multiple machines (optional), all simultaneously operating on disjoint parts of the graph\n- *batched negative sampling*, allowing for processing >1 million edges/sec/machine with 100 negatives per edge\n\nPBG is not optimized for small graphs. If your graph has fewer than 100,000 nodes, consider using [KBC](https://github.com/facebookresearch/kbc) with the ComplEx model and N3 regularizer. KBC produces state-of-the-art embeddings for graphs that can fit on a single GPU. Compared to KBC, PyTorch-BigGraph enables learning on very large graphs whose embeddings wouldn't fit in a single GPU or a single machine, but may not produce high-quality embeddings for small graphs without careful tuning.\n\n## Requirements\n\nPBG is written in Python (version 3.6 or later) and relies on [PyTorch](https://pytorch.org/) (at least version 1.0) and a few other libraries.\n\nAll computations are performed on the CPU, therefore a large number of cores is advisable. No GPU is necessary.\n\nWhen running on multiple machines, they need to be able to communicate to each other at high bandwidth (10 Gbps or higher recommended) and have access to a shared filesystem (for checkpointing). PBG uses [torch.distributed](https://pytorch.org/docs/stable/distributed.html), which uses the Gloo package which runs on top of TCP or MPI.\n\n## Installation\n\nClone the repository (or download it as an archive) and, inside the top-level directory, run:\n```bash\npip install .\n```\n\nPyTorch-BigGraph includes some C++ kernels that are only used for the experimental GPU mode. If you want to use GPU mode, compile the C++ code as follows:\n\n```bash\nPBG_INSTALL_CPP=1 pip install .\n```\n\nEverything will work identically except that you will be able to run GPU training (`torchbiggraph_train_gpu`).\n\n\nThe results of [the paper](https://mlsys.org/Conferences/2019/doc/2019/71.pdf) can easily be reproduced by running the following command (which executes [this script](torchbiggraph/examples/fb15k.py)):\n```bash\ntorchbiggraph_example_fb15k\n```\nThis will download the Freebase 15k knowledge base dataset, put it into the right format, train on it using the ComplEx model and finally perform an evaluation of the learned embeddings that calculates the MRR and other metrics that should match the paper. Another command, `torchbiggraph_example_livejournal`, does the same for the LiveJournal interaction graph dataset.\n\nTo learn how to use PBG, let us walk through what the FB15k script does.\n\n## Getting started\n\n### Downloading the data\n\nFirst, it [retrieves the dataset](https://dl.fbaipublicfiles.com/starspace/fb15k.tgz) and unpacks it, obtaining a directory with three edge sets as TSV files, for training, validation and testing.\n```bash\nwget https://dl.fbaipublicfiles.com/starspace/fb15k.tgz -P data\ntar xf data/fb15k.tgz -C data\n```\n\nEach line of these files contains information about one edge. Using tabs as separators, the lines are divided into columns which contain the identifiers of the source entities, the relation types and the target entities. For example:\n```\n/m/027rn\t/location/country/form_of_government\t/m/06cx9\n/m/017dcd\t/tv/tv_program/regular_cast./tv/regular_tv_appearance/actor\t/m/06v8s0\n/m/07s9rl0\t/media_common/netflix_genre/titles\t/m/0170z3\n/m/01sl1q\t/award/award_winner/awards_won./award/award_honor/award_winner\t/m/044mz_\n/m/0cnk2q\t/soccer/football_team/current_roster./sports/sports_team_roster/position\t/m/02nzb8\n```\n\n### Preparing the data\n\nThen, the script converts the edge lists to PBG's input format. This amounts to assigning a numerical identifier to all entities and relation types, shuffling and partitioning the entities and edges and writing all down in the right format.\n\nLuckily, there is a command that does all of this:\n```bash\ntorchbiggraph_import_from_tsv \\\n  --lhs-col=0 --rel-col=1 --rhs-col=2 \\\n  torchbiggraph/examples/configs/fb15k_config_cpu.py \\\n  data/FB15k/freebase_mtr100_mte100-train.txt \\\n  data/FB15k/freebase_mtr100_mte100-valid.txt \\\n  data/FB15k/freebase_mtr100_mte100-test.txt\n```\nThe outputs will be stored next to the inputs in the `data/FB15k` directory.\n\nThis simple utility is only suitable for small graphs that fit entirely in memory. To handle larger data one will have to implement their own custom preprocessor.\n\n### Training\n\nThe `torchbiggraph_train` command is used to launch training. The training parameters are tucked away in a configuration file, whose path is given to the command. They can however be overridden from the command line with the `--param` flag. The sample config is used for both training and evaluation, so we will have to use the override to specify the edge set to use.\n```bash\ntorchbiggraph_train \\\n  torchbiggraph/examples/configs/fb15k_config_cpu.py \\\n  -p edge_paths=data/FB15k/freebase_mtr100_mte100-train_partitioned\n```\n\nThis will read data from the `entity_path` directory specified in the configuration and the `edge_paths` directory given on the command line. It will write checkpoints (which also double as the output data) to the `checkpoint_path` directory also defined in the configuration, which in this case is `model/fb15k`.\n\nTraining will proceed for 50 epochs in total, with the progress and some statistics logged to the console, for example:\n```\nStarting epoch 1 / 50, edge path 1 / 1, edge chunk 1 / 1\nEdge path: data/FB15k/freebase_mtr100_mte100-train_partitioned\nstill in queue: 0\nSwapping partitioned embeddings None ( 0 , 0 )\n( 0 , 0 ): Loading entities\n( 0 , 0 ): bucket 1 / 1 : Processed 483142 edges in 17.36 s ( 0.028 M/sec ); io: 0.02 s ( 542.52 MB/sec )\n( 0 , 0 ): loss:  309.695 , violators_lhs:  171.846 , violators_rhs:  165.525 , count:  483142\nSwapping partitioned embeddings ( 0 , 0 ) None\nWriting partitioned embeddings\nFinished epoch 1 / 50, edge path 1 / 1, edge chunk 1 / 1\nWriting the metadata\nWriting the checkpoint\nSwitching to the new checkpoint version\n```\n\n### GPU Training\n\n*Warning: GPU Training is still experimental; expect sharp corners and lack of documentation.*\n\n`torchbiggraph_example_fb15k` will automatically detect if a GPU is available and run with the GPU training config. For your own training runs, you will need to change a few parameters to enable GPU training. Lets see how the two FB15k configs differ:\n\n```\n$ diff torchbiggraph/examples/configs/fb15k_config_cpu.py torchbiggraph/examples/configs/fb15k_config_gpu.py\n37a38\n>         batch_size=10000,\n42a44,45\n>         # GPU\n>         num_gpus=1,\n\n```\nThe most important difference is of course `num_gpus=1`, which says to run on 1 GPU. If `num_gpus=N>1`, PBG will recursively shard the embeddings within each partition into `N` subpartitions to run on multiple GPUs. The subpartitions need to fit in GPU memory, so if you get CUDA out-of-memory errors, you'll need to increase `num_partitions` or `num_gpus`.\n\nThe next most important difference for GPU training is that `batch_size` must be much larger. Since training is being performed on a single GPU instead of 40 cores, the batch size can be increased by about that factor as well. We suggest batch size of around 100,000 in order to achieve good speeds for GPU training.\n\nSince evaluation still occurs on CPU, we suggest turning down `eval_fraction` to at most `0.01` so that evaluation does not become a bottleneck (not relevant for FB15k which doesn't do eval during training).\n\nFinally, to take advantage of GPU speed, we suggest turning up `num_uniform_negatives` and/or `num_batch_negatives` to about `1000` rather than their default values of `50` (FB15k already uses 1000 uniform negatives).\n\n### Evaluation\n\nOnce training is complete, the entity embeddings it produced can be evaluated against a held-out edge set. The `torchbiggraph_example_fb15k` command performs a *filtered* evaluation, which calculates the ranks of the edges in the evaluation set by comparing them against all other edges *except* the ones that are true positives in any of the training, validation or test set. Filtered evaluation is used in the literature for FB15k, but does not scale beyond small graphs.\n\nThe final results should match the values of `mrr` (Mean Reciprocal Rank, MRR) and `r10` (Hits@10) reported in [the paper](https://mlsys.org/Conferences/2019/doc/2019/71.pdf):\n```\nStats: pos_rank:  65.4821 , mrr:  0.789921 , r1:  0.738501 , r10:  0.876894 , r50:  0.92647 , auc:  0.989868 , count:  59071\n```\n\nEvaluation can also be run directly from the command line as follows:\n```bash\ntorchbiggraph_eval \\\n  torchbiggraph/examples/configs/fb15k_config_cpu.py \\\n  -p edge_paths=data/FB15k/freebase_mtr100_mte100-test_partitioned \\\n  -p relations.0.all_negs=true \\\n  -p num_uniform_negs=0\n```\n\nHowever, *filtered* evaluation *cannot* be performed on the command line, so the reported results will not match the paper. They will be something like:\n```\nStats: pos_rank:  234.136 , mrr:  0.239957 , r1:  0.131757 , r10:  0.485382 , r50:  0.712693 , auc:  0.989648 , count:  59071\n```\n\n### Converting the output\n\nDuring preprocessing, the entities and relation types had their identifiers converted from strings to ordinals. In order to map the output embeddings back onto the original names, one can do:\n```bash\ntorchbiggraph_export_to_tsv \\\n  torchbiggraph/examples/configs/fb15k_config.py \\\n  --entities-output entity_embeddings.tsv \\\n  --relation-types-output relation_types_parameters.tsv\n```\nThis will create the `entity_embeddings.tsv` file, which is a text file where each line contains the identifier of an entity followed respectively by the components of its embedding, each in a different column, all separated by tabs. For example, with each line shortened for brevity:\n```\n/m/0fphf3v\t-0.524391472\t-0.016430536\t-0.461346656\t-0.394277513\t0.125605106\t...\n/m/01bns_\t-0.122734159\t-0.091636233\t0.506501377\t-0.503864646\t0.215775326\t...\n/m/02ryvsw\t-0.107151665\t0.002058491\t-0.094485454\t-0.129078045\t-0.123694092\t...\n/m/04y6_qr\t-0.577532947\t-0.215747222\t-0.022358289\t-0.352154016\t-0.051905245\t...\n/m/02wrhj\t-0.593656778\t-0.557167351\t0.042525314\t-0.104738958\t-0.265990764\t...\n```\nIt will also create a `relation_types_parameters.tsv` file which contains the parameters of the operators for the relation types. The format is similar to the above, but each line starts with more key columns containing, respectively, the name of a relation type, a side (`lhs` or `rhs`), the name of the operator which is used by that relation type on that side, the name of a parameter of that operator and the shape of the parameter (integers separated by `x`). These columns are followed by the values of the flattened parameter. For example, for two relation types, `foo` and `bar`, respectively using operators `linear` and `complex_diagonal`, with an embedding dimension of 200 and dynamic relations enabled, this file could look like:\n```\nfoo\tlhs\tlinear\tlinear_transformation\t200x200\t-0.683401227\t0.209822774\t-0.047136042\t...\nfoo\trhs\tlinear\tlinear_transformation\t200x200\t-0.695254087\t0.502532542\t-0.131654695\t...\nbar\tlhs\tcomplex_diagonal\treal\t200\t0.263731539\t1.350529909\t1.217602968\t...\nbar\tlhs\tcomplex_diagonal\timag\t200\t-0.089371338\t-0.092713356\t0.025076168\t...\nbar\trhs\tcomplex_diagonal\treal\t200\t-2.350617170\t0.529571176\t0.521403074\t...\nbar\trhs\tcomplex_diagonal\timag\t200\t0.692483306\t0.446569800\t0.235914066\t...\n```\n\n## Pre-trained embeddings\n\nWe trained a PBG model on the full [Wikidata](https://www.wikidata.org/) graph, using a [translation operator](https://torchbiggraph.readthedocs.io/en/latest/scoring.html#operators) to represent relations. It can be downloaded [here](https://dl.fbaipublicfiles.com/torchbiggraph/wikidata_translation_v1.tsv.gz) (36GiB, gzip-compressed). We used the truthy version of data from [here](https://dumps.wikimedia.org/wikidatawiki/entities/) to train our model. The model file is in TSV format as described in the above section. Note that the first line of the file contains the number of entities, the number of relations and the dimension of the embeddings, separated by tabs. The model contains 78 million entities, 4,131 relations and the dimension of the embeddings is 200.\n\n\n## Documentation\n\nMore information can be found in [the full documentation](https://torchbiggraph.readthedocs.io/).\n\n## Communication\n\n- GitHub Issues: Bug reports, feature requests, install issues, etc.\n- The [PyTorch-BigGraph Slack](https://join.slack.com/t/pytorchbiggraph/shared_invite/zt-yxy7zl41-37ypKwOqLHhmMSac5XOh2w) is a forum for online discussion between developers and users, discussing features, collaboration, etc.\n\n## Citation\n\nTo cite this work please use:\n```tex\n@inproceedings{pbg,\n  title={{PyTorch-BigGraph: A Large-scale Graph Embedding System}},\n  author={Lerer, Adam and Wu, Ledell and Shen, Jiajun and Lacroix, Timothee and Wehrstedt, Luca and Bose, Abhijit and Peysakhovich, Alex},\n  booktitle={Proceedings of the 2nd SysML Conference},\n  year={2019},\n  address={Palo Alto, CA, USA}\n}\n```\n\n## License\n\nPyTorch-BigGraph is BSD licensed, as found in the [LICENSE.txt](LICENSE.txt) file.\n"
        },
        {
          "name": "docs",
          "type": "tree",
          "content": null
        },
        {
          "name": "ifbpy.py",
          "type": "blob",
          "size": 0.4296875,
          "content": "#!/usr/bin/env python3\n\n# Copyright (c) Facebook, Inc. and its affiliates.\n# All rights reserved.\n#\n# This source code is licensed under the BSD-style license found in the\n# LICENSE.txt file in the root directory of this source tree.\n\nimport warnings\n\nfrom libfb.py.ipython_par import launch_ipython\n\n\nwarnings.simplefilter(\"ignore\", category=DeprecationWarning)\nwarnings.simplefilter(\"ignore\", category=ResourceWarning)\n\n\nlaunch_ipython()\n"
        },
        {
          "name": "setup.cfg",
          "type": "blob",
          "size": 2.2822265625,
          "content": "[metadata]\nname = torchbiggraph\nversion = file: torchbiggraph/VERSION.txt\nurl = https://github.com/facebookresearch/PyTorch-BigGraph\nproject_urls =\n    Source = https://github.com/facebookresearch/PyTorch-BigGraph\n    Bug Reports = https://github.com/facebookresearch/PyTorch-BigGraph/issues\n    Documentation = https://torchbiggraph.readthedocs.io/\nauthor = Facebook AI Research\nclassifiers =\n    Development Status :: 5 - Production/Stable\n    Environment :: Console\n    Intended Audience :: Science/Research\n    License :: OSI Approved :: BSD License\n    Operating System :: OS Independent\n    Programming Language :: Python\n    Programming Language :: Python :: 3\n    Programming Language :: Python :: 3.6\n    Programming Language :: Python :: 3.7\n    Programming Language :: Python :: 3 :: Only\n    Topic :: Scientific/Engineering :: Artificial Intelligence\n# Already provided as a classifier.\n# license = BSD License\nlicense_files =\n    LICENSE.txt\n    torchbiggraph/examples/LICENSE.txt\ndescription = A distributed system to learn embeddings of large graphs\nlong_description = file: README.md\nlong_description_content_type = text/markdown\nkeywords =\n    machine-learning\n    knowledge-base\n    graph-embedding\n    link-prediction\ntest_suite = test\n\n[options]\nsetup_requires =\n    setuptools >= 39.2\ninstall_requires =\n    attrs >= 18.2\n    h5py >= 2.8\n    numpy\n    setuptools\n    torch >= 1\n    tqdm\npython_requires = >=3.6, <4\npackages = find:\n\n[options.extras_require]\ndocs = Sphinx\nparquet = parquet\n\n[options.entry_points]\nconsole_scripts =\n    torchbiggraph_config = torchbiggraph.config:main\n    torchbiggraph_eval = torchbiggraph.eval:main\n    torchbiggraph_example_fb15k = torchbiggraph.examples.fb15k:main\n    torchbiggraph_example_livejournal = torchbiggraph.examples.livejournal:main\n    torchbiggraph_export_to_tsv = torchbiggraph.converters.export_to_tsv:main\n    torchbiggraph_import_from_tsv = torchbiggraph.converters.import_from_tsv:main\n    torchbiggraph_partitionserver = torchbiggraph.partitionserver:main\n    torchbiggraph_train = torchbiggraph.train:main\n    torchbiggraph_import_from_parquet = torchbiggraph.converters.import_from_parquet:main [parquet]\n\n\n[options.packages.find]\nexclude =\n    docs\n    test\n\n[options.package_data]\ntorchbiggraph =\n    VERSION.txt\ntorchbiggraph.examples =\n    configs/*.py\n"
        },
        {
          "name": "setup.py",
          "type": "blob",
          "size": 0.6630859375,
          "content": "#!/usr/bin/env python3\n\n# Copyright (c) Facebook, Inc. and its affiliates.\n# All rights reserved.\n#\n# This source code is licensed under the BSD-style license found in the\n# LICENSE.txt file in the root directory of this source tree.\n\nimport os\n\nfrom setuptools import setup\nfrom torch.utils import cpp_extension\n\n\nif __name__ == \"__main__\":\n    if int(os.getenv(\"PBG_INSTALL_CPP\", 0)) == 0:\n        setup()\n    else:\n        setup(\n            ext_modules=[\n                cpp_extension.CppExtension(\n                    \"torchbiggraph._C\", [\"torchbiggraph/util.cpp\"]\n                )\n            ],\n            cmdclass={\"build_ext\": cpp_extension.BuildExtension},\n        )\n"
        },
        {
          "name": "test",
          "type": "tree",
          "content": null
        },
        {
          "name": "torchbiggraph",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}