{
  "metadata": {
    "timestamp": 1736560386095,
    "page": 934,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjk0MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "LiuXingMing/SinaSpider",
      "stars": 3270,
      "defaultBranch": "master",
      "files": [
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 1.095703125,
          "content": "### Python template\n# Byte-compiled / optimized / DLL files\n__pycache__/\n*.py[cod]\n*$py.class\n\n# C extensions\n*.so\n\n# Distribution / packaging\n.Python\nenv/\nbuild/\ndevelop-eggs/\ndist/\ndownloads/\neggs/\n.eggs/\nlib/\nlib64/\nparts/\nsdist/\nvar/\n*.egg-info/\n.installed.cfg\n*.egg\n\n# PyInstaller\n#  Usually these files are written by a python script from a template\n#  before PyInstaller builds the exe, so as to inject date/other infos into it.\n*.manifest\n*.spec\n\n# Installer logs\npip-log.txt\npip-delete-this-directory.txt\n\n# Unit test / coverage reports\nhtmlcov/\n.tox/\n.coverage\n.coverage.*\n.cache\nnosetests.xml\ncoverage.xml\n*,cover\n.hypothesis/\n\n# Translations\n*.mo\n*.pot\n\n# Django stuff:\n*.log\nlocal_settings.py\n\n# Flask instance folder\ninstance/\n\n# Scrapy stuff:\n.scrapy\n\n# Sphinx documentation\ndocs/_build/\n\n# PyBuilder\ntarget/\n\n# IPython Notebook\n.ipynb_checkpoints\n\n# pyenv\n.python-version\n\n# celery beat schedule file\ncelerybeat-schedule\n\n# dotenv\n.env\n\n# virtualenv\nvenv/\nENV/\n/.idea\n/.setting\n\n# Spyder project settings\n.spyderproject\n\n# Rope project settings\n.ropeproject\n\n# Created by .ignore support plugin (hsz.mobi)\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 5.966796875,
          "content": "**Sina_Spider1: 《[新浪微博爬虫分享（一天可抓取 1300 万条数据）](http://blog.csdn.net/bone_ace/article/details/50903178)》**\n<br> **Sina_Spider2: 《[新浪微博分布式爬虫分享](http://blog.csdn.net/bone_ace/article/details/50904718)》**\n<br> **Sina_Spider3: 《[新浪微博爬虫分享（2016年12月01日更新）](http://blog.csdn.net/bone_ace/article/details/53379904)》**\n<br>\n<br>\nSina_Spider1为单机版本。<br>\nSina_Spider2在Sina_Spider1的基础上基于scrapy_redis模块实现分布式。<br>\nSina_Spider3增加了Cookie池的维护，优化了种子队列和去重队列。<br>\n<br>\n三个版本的详细介绍请看各自的博客。\n遇到什么问题请尽量留言，方便后来遇到同样问题的同学查看。也可加一下QQ交流群：<a target=\"_blank\" href=\"//shang.qq.com/wpa/qunwpa?idkey=a3e1d79f8c7e12b9db5ac680375d7174a91384f288d3ba16e1781c2587872560\"><img border=\"0\" src=\"http://pub.idqqimg.com/wpa/images/group.png\" alt=\"微博爬虫交流群\" title=\"微博爬虫交流群\"></a>。\n<br><br><br><br>\n --------------------------------------------------------------------------\n<br>\n20161215更新：\n<br>\n有人反映说爬虫一直显示爬了0页，没有抓到数据。\n<br>\n1、把settings.py里面的LOG_LEVEL = 'INFO'一行注释掉，使用默认的\"DEBUG\"日志模式，运行程序可查看是否正常请求网页。\n<br>\n2、注意程序是有去重功能的，所以要清空数据重新跑的话一定要把redis的去重队列删掉，否则起始ID被记录为已爬的话也会出现抓取为空的现象。清空redis数据 运行cleanRedis.py即可。\n<br>\n3、另外，微博开始对IP有限制了，如果爬的快 可能会出现403，大规模抓取的话需要加上代理池。\n<br><br><br><br>\n ---------------------------------------------------------------------------\n<br>\n20170323更新：\n<br>微博从昨天下午三点多开始做了一些改动，原本免验证码获取Cookie的途径已经不能用了。以前为了免验证码登录，到处找途径，可能最近爬的人多了，给封了。\n<br>那么就直面验证码吧，走正常流程登录，才没那么容易被封。此次更新主要在于Cookie的获取途径，其他地方和往常一样（修改了cookies.py，新增了yumdama.py）。\n<br>加了验证码，难度和复杂程度都提高了一点，对于没有编程经验的同学可能会有一些难度。\n<br>验证码处理主要有两种：手动输入和打码平台自动填写（手动输入配置简单，打码平台输入适合大规模抓取）。\n<br><br>手动方式流程：\n<br>\n1、下载PhantomJS.exe，放在python的安装路径（适合Windows系统，Linux请找百度）。\n<br>\n2、运行launch.py启动爬虫，中途会要求输入验证码，查看项目路径下新生成的aa.png，输入验证码 回车，即可。\n<br>\n<br>打码方式流程：\n<br>\n1、下载PhantomJS.exe，放在python的安装路径。\n<br>\n2、安装Python模块PIL（请自行百度，可能道路比较坎坷）\n<br>\n3、验证码打码：我使用的是 http://www.yundama.com/ （真的不是打广告..），将username、password、appkey填入yumdama.py（正确率挺高，weibo.cn正常的验证码是4位字符，1元可以识别200个）。\n<br>（如果一直出现302，调试发现yumdama.py一直返回空字符串，可将yumdama.py中的apiurl改成 'http://api.yundama.net:5678/api.php' 试试，在第38行前后，原值是 'http://api.yundama.com/api.php' 。）\n<br>\n4、cookies.py中设置IDENTIFY=2，运行launch.py启动爬虫即可。\n<br><br><br><br>\n ---------------------------------------------------------------------------\n<br>\n20170405更新：\n<br>微博从4月1日开始对IP限制更严了，很容易就403 Forbidden了，解决的办法是加代理。从16年12月更新代码后爬微博的人多了许多，可能对weibo.cn造成了挺多无效访问。所以此次代码就不更新了，过滤一些爬虫新手，如果仍需大量抓取的，在middleware.py中加几行代码，带上代理就行了，难度也不大。没加代理的同学将爬虫速度再降低一点，还是能跑的。<br>\n可能有挺多同学需要微博数据写论文，在群里找一下已有数据的同学吧，购买代理也不便宜。<br>\n（我也没怎么跑微博，手上也没什么数据）\n<br><br><br><br>\n ---------------------------------------------------------------------------\n<br>\n20170407更新：\n<br>有些同学还用着SinaSpider1，现将SinaSpider1中获取Cookie的代码也作了更新，使用方法和SinaSpider3的一样，见上面的更新说明。\n<br><br><br><br>\n ---------------------------------------------------------------------------\n<br>\n20170410更新：\n<br>许多同学问微博帐号哪里买，淘宝上禁的有一点严，所以直接搜可能没搜到。需要的同学可以搜店铺名称：账号素材生产基地 或 互联网账号营销中心，看店铺里的商品，有老客户链接。偶尔会断货，购买多少自行斟酌。非广告，不需要的请忽略。\n<br><br><br><br>\n ---------------------------------------------------------------------------\n<br>\n20170426更新：\n<br>从昨天下午开始，weibo.cn的登录方式又变了，关闭了原来的登录页面，采用m.weibo.com的登录途径，登录过程中可能会出现图形解锁的验证码。隐约感觉有几个微博官方反爬虫的人正在暗处默默地盯着我，说不定什么时候就要请我去喝茶了。。\n唉，图形解锁应该也是可以破解的，但是最近事多，要过两个星期才有空研究，有需要的可以等等，或者大伙自己可以研究一下，按像素识别。\n<br><br><br><br>\n ---------------------------------------------------------------------------\n<br>\n20170509更新：\n<br>1、http://weibo.cn改成了https://weibo.cn。\n<br>2、图形解锁验证码的破解见博客 [《图形解锁破解（附Python代码）》](http://blog.csdn.net/bone_ace/article/details/71056741) 。微博爬虫的Cookie获取模块请自行更新。\n<br><br>\n<br>\n"
        },
        {
          "name": "Sina_spider1",
          "type": "tree",
          "content": null
        },
        {
          "name": "Sina_spider2",
          "type": "tree",
          "content": null
        },
        {
          "name": "Sina_spider3",
          "type": "tree",
          "content": null
        },
        {
          "name": "pipelines.py",
          "type": "blob",
          "size": 6.810546875,
          "content": "# encoding=utf-8\n\n# __________________________________________\n#   增加了向Mysql数据库中保存pipeline\n#   需要有MysqlDB,同时修改Spider文件，增加Item类所有变量的if else的返回值，使得可以标准化存储       \n#   Updated by Charles Yan\n#   Date:2017.1.4\n#   Added Mysql insert method\n# ------------------------------------------\n\nimport pymongo\nfrom items import InformationItem, TweetsItem, RelationshipsItem\nimport MySQLdb\n\nclass MysqlDBPipleline(object):\n    def __init__(self):\n        self.count = 1\n        self.conn = MySQLdb.connect(\n                host='localhost',\n                port=3306,\n                user='root',\n                #这里填写密码\n                passwd='***',\n                db='SinaWeibo',\n                charset='utf8',\n                )\n        self.cur = self.conn.cursor()\n\n    def process_item(self, item, spider):\n        \"\"\" 判断item的类型，并作相应的处理，再入数据库 \"\"\"\n        if isinstance(item, RelationshipsItem):\n            try:\n                print(\"***********at beginning of saving**********\")\n                print(dict(item))\n                sql = ''\n                sql+=str('INSERT INTO SinaWeibo.Relationship (`Host1`,`Host2`) ')\n                sql+=str(' Values(\\'' )\n                sql+=str(item['Host1'])\n                print(sql)\n                sql+=str('\\', \\'')\n                sql+=str(item['Host2'])\n                sql+=str('\\')')\n                print(\"*********** SQL SYNTAX *********** \")\n                print(''.join(sql))\n                self.cur.execute(sql)\n                self.conn.commit()\n                print(\"saved\")\n                self.count = self.count +1\n                print(self.count)\n            except Exception:\n                pass\n        elif isinstance(item, TweetsItem):\n            try:\n                print(\"***********at beginning of saving**********\")\n                \n                sql = ''\n                sql+=str('INSERT INTO SinaWeibo.Tweets (`weibo_id`,`User_id`,`Content`,`Pubtime`,`Coordinates`,`Tools`,`Likes`,`Comments`,`Transfers`) ')\n                sql+=str(' Values(\\'' )\n                sql+=str(item['_id'])\n           \n                sql+=str('\\', \\'')\n                sql+=str(item['ID'])\n                sql+=str('\\', \\'')\n                sql+=str(item['Content'])\n                sql+=str('\\', \\'')\n                sql+=str(item['PubTime'])\n               \n                sql+=str('\\', \\'')\n               \n                sql+=str(item['Co_oridinates'])\n               \n                sql+=str('\\', \\'')\n                sql+=str(item['Tools'])\n                print(sql)\n                sql+=str('\\', \\'')\n                sql+=str(item['Like'])\n                sql+=str('\\', \\'')\n                sql+=str(item['Comment'])\n                sql+=str('\\', \\'')\n                sql+=str(item['Transfer'])\n                sql+=str('\\')')\n                print(\"*********** SQL SYNTAX *********** \")\n                print(''.join(sql))\n                self.cur.execute(sql)\n                self.conn.commit()\n                print(\"saved\")\n                self.count = self.count +1\n                print(self.count)\n            except Exception:\n                pass\n        elif isinstance(item, InformationItem):\n            try:\n                print(\"***********at beginning of saving**********\")\n                \n                sql = ''\n                sql+=str('INSERT INTO SinaWeibo.Information (`User_id`,`NickName`,`Gender`,`Province`,`City`,`BriefIntroduction`,`Birthday`,`Num_Tweets`,`Num_Follows`,`Num_Fans`,`SexOrientation`,`Sentiment`,`VIPlevel`,`Authentication`,`URL`) ')\n                sql+=str(' Values(\\'' )\n                sql+=str(item['_id'])\n               \n                sql+=str('\\', \\'')\n                sql+=str(item['NickName'])\n                sql+=str('\\', \\'')\n                sql+=str(item['Gender'])\n                sql+=str('\\', \\'')\n                sql+=str(item['Province'])\n                \n                sql+=str('\\', \\'')\n                sql+=str(item['City'])\n                sql+=str('\\', \\'')\n                sql+=str(item['BriefIntroduction'])\n                sql+=str('\\', \\'')\n                print(sql)\n                sql+=str(item['Birthday'])\n                sql+=str('\\', \\'')\n                sql+=str(item['Num_Tweets'])\n               \n                sql+=str('\\', \\'')\n                sql+=str(item['Num_Follows'])\n                sql+=str('\\', \\'')\n                sql+=str(item['Num_Fans'])\n                sql+=str('\\', \\'')\n                \n                sql+=str(item['SexOrientation'])\n                sql+=str('\\', \\'')\n                sql+=str(item['Sentiment'])\n                \n                sql+=str('\\', \\'')\n                sql+=str(item['VIPlevel'])\n                sql+=str('\\', \\'')\n                sql+=str(item['Authentication'])\n                sql+=str('\\', \\'')\n                sql+=str(item['URL'])\n                sql+=str('\\')')\n               \n                print(\"*********** SQL SYNTAX *********** \")\n                print(''.join(sql))\n                self.cur.execute(sql)\n                self.conn.commit()\n                print(\"saved\")\n                self.count = self.count +1\n                print(self.count)\n            except Exception:\n                pass\n            \n            ##在Java开发中，Dao连接会对内存溢出，需要定时断开重连，这里不清楚是否需要，先加上了\n            if self.count == 1000:\n                print(\"try reconnecting\")\n                self.count = 0\n                self.cur.close()\n                self.conn.close()\n                self.conn = MySQLdb.connect(\n                    host='localhost',\n                    port=3306,\n                    user='root',\n                    passwd='***',\n                    db='SinaWeibo',\n                    charset='utf8',\n                )\n                self.cur = self.conn.cursor()\n                print(\"reconnect\")\n                \n        return item\n    \n\n\nclass MongoDBPipleline(object):\n    def __init__(self):\n        clinet = pymongo.MongoClient(\"localhost\", 27017)\n        db = clinet[\"Sina\"]\n        self.Information = db[\"Information\"]\n        self.Tweets = db[\"Tweets\"]\n        self.Relationships = db[\"Relationships\"]\n\n    def process_item(self, item, spider):\n        \"\"\" 判断item的类型，并作相应的处理，再入数据库 \"\"\"\n        if isinstance(item, RelationshipsItem):\n            try:\n                self.Relationships.insert(dict(item))\n            except Exception:\n                pass\n        elif isinstance(item, TweetsItem):\n            try:\n                self.Tweets.insert(dict(item))\n            except Exception:\n                pass\n        elif isinstance(item, InformationItem):\n            try:\n                self.Information.insert(dict(item))\n            except Exception:\n                pass\n        return item\n"
        }
      ]
    }
  ]
}