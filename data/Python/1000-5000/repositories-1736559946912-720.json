{
  "metadata": {
    "timestamp": 1736559946912,
    "page": 720,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjczMA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "unionai-oss/pandera",
      "stars": 3532,
      "defaultBranch": "main",
      "files": [
        {
          "name": ".coveragerc",
          "type": "blob",
          "size": 0.205078125,
          "content": "[run]\nsource = pandera\n\n[report]\nexclude_lines =\n    if self.debug:\n    pragma: no cover\n    raise NotImplementedError\n    if __name__ == .__main__.:\nignore_errors = True\nomit =\n    tests/*\n    pandera/mypy.py\n"
        },
        {
          "name": ".envrc",
          "type": "blob",
          "size": 0.02734375,
          "content": "source activate pandera-dev\n"
        },
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 1.4970703125,
          "content": "*.db\n.vscode\ndask-worker-space\nspark-warehouse\ndocs/source/_contents\ndocs/jupyter_execute\n**.DS_Store\n\n# Byte-compiled / optimized / DLL files\n__pycache__/\n*.py[cod]\n*$py.class\n\n# C extensions\n*.so\n\n# Distribution / packaging\n.Python\nbuild/\ndevelop-eggs/\ndist/\ndownloads/\neggs/\n.eggs/\nlib/\nlib64/\nparts/\nsdist/\nvar/\nwheels/\n*.egg-info/\n.installed.cfg\n*.egg\nMANIFEST\n\n# PyInstaller\n#  Usually these files are written by a python script from a template\n#  before PyInstaller builds the exe, so as to inject date/other infos into it.\n*.manifest\n*.spec\n\n# Installer logs\npip-log.txt\npip-delete-this-directory.txt\n\n# Unit test / coverage reports\nhtmlcov/\n.tox/\n.coverage\n.coverage.*\n.cache\nnosetests.xml\ncoverage.xml\n*.cover\n.hypothesis/\n.pytest_cache/\n\n# Translations\n*.mo\n*.pot\n\n# Django stuff:\n*.log\nlocal_settings.py\ndb.sqlite3\n\n# Flask stuff:\ninstance/\n.webassets-cache\n\n# Scrapy stuff:\n.scrapy\n\n# Sphinx documentation\ndocs/_build/\n\n# PyBuilder\ntarget/\n\n# Jupyter Notebook\n.ipynb_checkpoints\n\n# pyenv\n.python-version\n\n# celery beat schedule file\ncelerybeat-schedule\n\n# SageMath parsed files\n*.sage.py\n\n# Environments\n.env\n.venv\nenv/\nvenv/\nENV/\nenv.bak/\nvenv.bak/\n\n# Spyder project settings\n.spyderproject\n.spyproject\n\n# Rope project settings\n.ropeproject\n\n# mkdocs documentation\n/site\n\n# mypy\n.mypy_cache/\n\n# Pycharm settings\n.idea\n\n# Airspeed Velocity Benchmarks\n/asv_bench/html/\n/asv_bench/results/\n\n# Docs\ndocs/source/reference/generated\n\n# Nox\n.nox\n.nox-*\n\n# ignore markdown files copied from .github\ndocs/source/CONTRIBUTING.md\n"
        },
        {
          "name": ".pre-commit-config.yaml",
          "type": "blob",
          "size": 1.8564453125,
          "content": "exclude: (^asv_bench|setup.py|requirements-dev.txt)\n\nrepos:\n  - repo: https://github.com/pre-commit/pre-commit-hooks\n    rev: v4.1.0\n    hooks:\n      - id: check-ast\n        description: Simply check whether files parse as valid python\n      - id: check-case-conflict\n        description: Check for files that would conflict in case-insensitive filesystems\n      - id: check-merge-conflict\n        description: Check for files that contain merge conflict strings\n      - id: check-yaml\n        description: Attempts to load all yaml files to verify syntax\n      - id: debug-statements\n        description: Check for debugger imports and py37+ breakpoint() calls in python source\n      - id: end-of-file-fixer\n        description: Makes sure files end in a newline and only a newline\n      - id: trailing-whitespace\n        description: Trims trailing whitespace\n      - id: mixed-line-ending\n        description: Replaces or checks mixed line ending\n\n  - repo: https://github.com/pre-commit/mirrors-isort\n    rev: v5.10.1\n    hooks:\n      - id: isort\n        args: [\"--line-length=79\", \"--skip=docs/source/conf.py\", \"--diff\"]\n\n  - repo: https://github.com/ikamensh/flynt\n    rev: \"0.76\"\n    hooks:\n      - id: flynt\n\n  - repo: https://github.com/psf/black\n    rev: 24.4.2\n    hooks:\n      - id: black\n\n  - repo: https://github.com/pycqa/pylint\n    rev: v2.17.3\n    hooks:\n      - id: pylint\n        args: [\"--disable=import-error\"]\n        exclude: (^docs/|^scripts)\n\n  - repo: https://github.com/pre-commit/mirrors-mypy\n    rev: v1.10.0\n    hooks:\n      - id: mypy\n        additional_dependencies:\n          - types-click\n          - types-pytz\n          - types-pyyaml\n          - types-requests\n          - types-setuptools\n        args: [\"pandera\", \"tests\", \"scripts\"]\n        exclude: (^docs/|^tests/mypy/modules/)\n        pass_filenames: false\n        require_serial: true\n        verbose: true\n"
        },
        {
          "name": ".pylintrc",
          "type": "blob",
          "size": 1.13671875,
          "content": "[BASIC]\nignore=mypy.py,noxfile.py,pandera/accessors/pyspark_sql_accessor.py,pandera/engines/pyspark_engine.py,pandera/pyspark.py,pandera/typing/pyspark_sql.py,\nignore-patterns=pandera/api/pyspark/*,tests/pyspark/*\ngood-names=\n    T,\n    F,\n    logger,\n    df,\n    fn,\n    i,\n    e,\n    x,\n    f,\n    k,\n    v,\n    fp,\n    bar,\n    eq,\n    ne,\n    gt,\n    ge,\n    lt,\n    le,\n    dt,\n    tz,\n    TBaseModel,\n    TArraySchemaBase,\n    TDataFrameModel,\n    _DataType\n\n[MESSAGES CONTROL]\ndisable=\n    # C0330 conflicts with black: https://github.com/psf/black/issues/48\n    R0913,\n    duplicate-code,\n    too-many-instance-attributes,\n    no-else-return,\n    inconsistent-return-statements,\n    protected-access,\n    too-many-ancestors,\n    too-many-lines,\n    too-few-public-methods,\n    line-too-long,\n    ungrouped-imports,\n    function-redefined,\n    arguments-differ,\n    unnecessary-dunder-call,\n    use-dict-literal,\n    invalid-name,\n    import-outside-toplevel,\n    missing-class-docstring,\n    missing-function-docstring,\n    fixme,\n    too-many-locals,\n    redefined-outer-name,\n    logging-fstring-interpolation,\n    multiple-statements,\n    cyclic-import\n"
        },
        {
          "name": ".readthedocs.yml",
          "type": "blob",
          "size": 0.935546875,
          "content": "# .readthedocs.yml\n# Read the Docs configuration file\n# See https://docs.readthedocs.io/en/stable/config-file/v2.html for details\n\n# Required\nversion: 2\n\nbuild:\n  os: ubuntu-20.04\n  apt_packages:\n    # Install OpenJDK as Java backend to run PySpark examples.\n    - openjdk-11-jre-headless\n  tools:\n    python: \"mambaforge-4.10\"\n  jobs:\n    post_install:\n      # Install numpy<v2 to avoid breaking docs. This is because the env has\n      # pyspark installed, which requires numpy<2.\n      - python -m pip install \"numpy<2\"\n\n# Build documentation in the docs/ directory with Sphinx\nconda:\n  environment: environment.yml\n\n# Optionally set the version of Python and requirements required to build your docs\npython:\n  install:\n    - method: pip\n      path: .\n\nsphinx:\n  configuration: docs/source/conf.py\n\n# Build documentation with MkDocs\n#mkdocs:\n#  configuration: mkdocs.yml\n\n# Optionally build your docs in additional formats such as PDF and ePub\nformats: []\n"
        },
        {
          "name": "CODE_OF_CONDUCT.md",
          "type": "blob",
          "size": 3.27734375,
          "content": "# Contributor Covenant Code of Conduct\n\n## Our Pledge\n\nIn the interest of fostering an open and welcoming environment, we as\ncontributors and maintainers pledge to making participation in our project and\nour community a harassment-free experience for everyone, regardless of age, body\nsize, disability, ethnicity, sex characteristics, gender identity and expression,\nlevel of experience, education, socio-economic status, nationality, personal\nappearance, race, religion, or sexual identity and orientation.\n\n## Our Standards\n\nExamples of behavior that contributes to creating a positive environment\ninclude:\n\n* Using welcoming and inclusive language\n* Being respectful of differing viewpoints and experiences\n* Gracefully accepting constructive criticism\n* Focusing on what is best for the community\n* Showing empathy towards other community members\n\nExamples of unacceptable behavior by participants include:\n\n* The use of sexualized language or imagery and unwelcome sexual attention or\n advances\n* Trolling, insulting/derogatory comments, and personal or political attacks\n* Public or private harassment\n* Publishing others' private information, such as a physical or electronic\n address, without explicit permission\n* Other conduct which could reasonably be considered inappropriate in a\n professional setting\n\n## Our Responsibilities\n\nProject maintainers are responsible for clarifying the standards of acceptable\nbehavior and are expected to take appropriate and fair corrective action in\nresponse to any instances of unacceptable behavior.\n\nProject maintainers have the right and responsibility to remove, edit, or\nreject comments, commits, code, wiki edits, issues, and other contributions\nthat are not aligned to this Code of Conduct, or to ban temporarily or\npermanently any contributor for other behaviors that they deem inappropriate,\nthreatening, offensive, or harmful.\n\n## Scope\n\nThis Code of Conduct applies both within project spaces and in public spaces\nwhen an individual is representing the project or its community. Examples of\nrepresenting a project or community include using an official project e-mail\naddress, posting via an official social media account, or acting as an appointed\nrepresentative at an online or offline event. Representation of a project may be\nfurther defined and clarified by project maintainers.\n\n## Enforcement\n\nInstances of abusive, harassing, or otherwise unacceptable behavior may be\nreported by contacting the project team at niels.bantilan@gmail.com. All\ncomplaints will be reviewed and investigated and will result in a response that\nis deemed necessary and appropriate to the circumstances. The project team is\nobligated to maintain confidentiality with regard to the reporter of an incident.\nFurther details of specific enforcement policies may be posted separately.\n\nProject maintainers who do not follow or enforce the Code of Conduct in good\nfaith may face temporary or permanent repercussions as determined by other\nmembers of the project's leadership.\n\n## Attribution\n\nThis Code of Conduct is adapted from the [Contributor Covenant][homepage], version 1.4,\navailable at https://www.contributor-covenant.org/version/1/4/code-of-conduct.html\n\n[homepage]: https://www.contributor-covenant.org\n\nFor answers to common questions about this code of conduct, see\nhttps://www.contributor-covenant.org/faq\n"
        },
        {
          "name": "LICENSE.txt",
          "type": "blob",
          "size": 1.0458984375,
          "content": "MIT License\n\nCopyright (c) 2018 Niels Bantilan\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n"
        },
        {
          "name": "Makefile",
          "type": "blob",
          "size": 1.3466796875,
          "content": ".PHONY: tests clean clean-pyc upload-pypi-test upload-pypi requirements docs \\\n\tcode-cov docs-clean requirements-dev.txt\n\nclean:\n\tpython setup.py clean\n\nclean-pyc:\n\tfind . -name '*.pyc' -exec rm {} \\;\n\nupload-pypi-test:\n\tpython setup.py sdist bdist_wheel && \\\n\t\ttwine upload --repository-url https://test.pypi.org/legacy/ dist/* && \\\n\t\trm -rf dist\n\nupload-pypi:\n\tpython setup.py sdist bdist_wheel && \\\n\t\ttwine upload dist/* && \\\n\t\trm -rf dist\n\nrequirements:\n\tpip install -r requirements-dev.txt\n\ndocs-clean:\n\trm -rf docs/source/reference/generated docs/**/generated docs/**/methods docs/_build docs/source/_contents\n\ndocs: docs-clean\n\tpython -m sphinx -W -E \"docs/source\" \"docs/_build\" && make -C docs doctest\n\nquick-docs:\n\tpython -m sphinx -E \"docs/source\" \"docs/_build\" && make -C docs doctest\n\ncode-cov:\n\tpytest --cov-report=html --cov=pandera tests/\n\nnox:\n\tnox -r --envdir .nox-virtualenv\n\nNOX_FLAGS ?= \"-r\"\n\nnox-mamba:\n\tnox -db mamba --envdir .nox-mamba ${NOX_FLAGS}\n\ndeps-from-conda:\n\tpython scripts/generate_pip_deps_from_conda.py\n\nnox-ci-requirements: deps-from-conda\n\tnox -db mamba --envdir .nox-mamba -s ci_requirements ${NOX_FLAGS}\n\nnox-dev-requirements: deps-from-conda\n\tnox -db mamba --envdir .nox-mamba -s dev_requirements ${NOX_FLAGS}\n\nnox-requirements: nox-ci-requirements nox-dev-requirements\n\nnox-tests:\n\tnox -db mamba --envdir .nox-mamba -s tests ${NOX_FLAGS}\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 12.1884765625,
          "content": "<br>\n<div align=\"center\"><a href=\"https://www.union.ai/pandera\"><img src=\"docs/source/_static/pandera-banner.png\" width=\"400\"></a></div>\n\n<h1 align=\"center\">\n  The Open-source Framework for Precision Data Testing\n</h1>\n\n<p align=\"center\">\n  ðŸ“Š ðŸ”Ž âœ…\n</p>\n\n<p align=\"center\">\n  <i>Data validation for scientists, engineers, and analysts seeking correctness.</i>\n</p>\n\n<br>\n\n\n[![CI Build](https://img.shields.io/github/actions/workflow/status/unionai-oss/pandera/ci-tests.yml?branch=main&label=tests&style=for-the-badge)](https://github.com/unionai-oss/pandera/actions/workflows/ci-tests.yml?query=branch%3Amain)\n[![Documentation Status](https://readthedocs.org/projects/pandera/badge/?version=stable&style=for-the-badge)](https://pandera.readthedocs.io/en/stable/?badge=stable)\n[![PyPI version shields.io](https://img.shields.io/pypi/v/pandera.svg?style=for-the-badge)](https://pypi.org/project/pandera/)\n[![PyPI license](https://img.shields.io/pypi/l/pandera.svg?style=for-the-badge)](https://pypi.python.org/pypi/)\n[![pyOpenSci](https://go.union.ai/pandera-pyopensci-badge)](https://github.com/pyOpenSci/software-review/issues/12)\n[![Project Status: Active â€“ The project has reached a stable, usable state and is being actively developed.](https://img.shields.io/badge/repo%20status-Active-Green?style=for-the-badge)](https://www.repostatus.org/#active)\n[![Documentation Status](https://readthedocs.org/projects/pandera/badge/?version=latest&style=for-the-badge)](https://pandera.readthedocs.io/en/latest/?badge=latest)\n[![codecov](https://img.shields.io/codecov/c/github/unionai-oss/pandera?style=for-the-badge)](https://codecov.io/gh/unionai-oss/pandera)\n[![PyPI pyversions](https://img.shields.io/pypi/pyversions/pandera.svg?style=for-the-badge)](https://pypi.python.org/pypi/pandera/)\n[![DOI](https://img.shields.io/badge/DOI-10.5281/zenodo.3385265-blue?style=for-the-badge)](https://doi.org/10.5281/zenodo.3385265)\n[![asv](http://img.shields.io/badge/benchmarked%20by-asv-green.svg?style=for-the-badge)](https://pandera-dev.github.io/pandera-asv-logs/)\n[![Monthly Downloads](https://img.shields.io/pypi/dm/pandera?style=for-the-badge&color=blue)](https://pepy.tech/project/pandera)\n[![Total Downloads](https://img.shields.io/pepy/dt/pandera?style=for-the-badge&color=blue)](https://pepy.tech/project/pandera)\n[![Conda Downloads](https://img.shields.io/conda/dn/conda-forge/pandera?style=for-the-badge)](https://anaconda.org/conda-forge/pandera)\n[![Discord](https://img.shields.io/badge/discord-chat-purple?color=%235765F2&label=discord&logo=discord&style=for-the-badge)](https://discord.gg/vyanhWuaKB)\n\n`pandera` is a [Union.ai](https://union.ai/blog-post/pandera-joins-union-ai) open\nsource project that provides a flexible and expressive API for performing data\nvalidation on dataframe-like objects to make data processing pipelines more readable and robust.\n\nDataframes contain information that `pandera` explicitly validates at runtime.\nThis is useful in production-critical or reproducible research settings. With\n`pandera`, you can:\n\n1. Define a schema once and use it to validate\n   [different dataframe types](https://pandera.readthedocs.io/en/stable/supported_libraries.html)\n   including [pandas](http://pandas.pydata.org), [polars](https://docs.pola.rs/),\n   [dask](https://dask.org), [modin](https://modin.readthedocs.io/),\n   and [pyspark](https://spark.apache.org/docs/3.2.0/api/python/user_guide/pandas_on_spark/index.html).\n1. [Check](https://pandera.readthedocs.io/en/stable/checks.html) the types and\n   properties of columns in a `DataFrame` or values in a `Series`.\n1. Perform more complex statistical validation like\n   [hypothesis testing](https://pandera.readthedocs.io/en/stable/hypothesis.html#hypothesis).\n1. [Parse](https://pandera.readthedocs.io/en/stable/parsers.html) data to standardize\n   the preprocessing steps needed to produce valid data.\n1. Seamlessly integrate with existing data analysis/processing pipelines\n   via [function decorators](https://pandera.readthedocs.io/en/stable/decorators.html#decorators).\n1. Define dataframe models with the\n   [class-based API](https://pandera.readthedocs.io/en/stable/dataframe_models.html#dataframe-models)\n   with pydantic-style syntax and validate dataframes using the typing syntax.\n1. [Synthesize data](https://pandera.readthedocs.io/en/stable/data_synthesis_strategies.html#data-synthesis-strategies)\n   from schema objects for property-based testing with pandas data structures.\n1. [Lazily Validate](https://pandera.readthedocs.io/en/stable/lazy_validation.html)\n   dataframes so that all validation checks are executed before raising an error.\n1. [Integrate](https://pandera.readthedocs.io/en/stable/integrations.html) with\n   a rich ecosystem of python tools like [pydantic](https://pydantic-docs.helpmanual.io),\n   [fastapi](https://fastapi.tiangolo.com/), and [mypy](http://mypy-lang.org/).\n\n## Documentation\n\nThe official documentation is hosted here: https://pandera.readthedocs.io\n\n\n## Install\n\nUsing pip:\n\n```\npip install pandera\n```\n\nUsing conda:\n\n```\nconda install -c conda-forge pandera\n```\n\n### Extras\n\nInstalling additional functionality:\n\n<details>\n\n<summary><i>pip</i></summary>\n\n```bash\npip install 'pandera[hypotheses]' # hypothesis checks\npip install 'pandera[io]'         # yaml/script schema io utilities\npip install 'pandera[strategies]' # data synthesis strategies\npip install 'pandera[mypy]'       # enable static type-linting of pandas\npip install 'pandera[fastapi]'    # fastapi integration\npip install 'pandera[dask]'       # validate dask dataframes\npip install 'pandera[pyspark]'    # validate pyspark dataframes\npip install 'pandera[modin]'      # validate modin dataframes\npip install 'pandera[modin-ray]'  # validate modin dataframes with ray\npip install 'pandera[modin-dask]' # validate modin dataframes with dask\npip install 'pandera[geopandas]'  # validate geopandas geodataframes\npip install 'pandera[polars]'     # validate polars dataframes\n```\n\n</details>\n\n<details>\n\n<summary><i>conda</i></summary>\n\n```bash\nconda install -c conda-forge pandera-hypotheses  # hypothesis checks\nconda install -c conda-forge pandera-io          # yaml/script schema io utilities\nconda install -c conda-forge pandera-strategies  # data synthesis strategies\nconda install -c conda-forge pandera-mypy        # enable static type-linting of pandas\nconda install -c conda-forge pandera-fastapi     # fastapi integration\nconda install -c conda-forge pandera-dask        # validate dask dataframes\nconda install -c conda-forge pandera-pyspark     # validate pyspark dataframes\nconda install -c conda-forge pandera-modin       # validate modin dataframes\nconda install -c conda-forge pandera-modin-ray   # validate modin dataframes with ray\nconda install -c conda-forge pandera-modin-dask  # validate modin dataframes with dask\nconda install -c conda-forge pandera-geopandas   # validate geopandas geodataframes\nconda install -c conda-forge pandera-polars      # validate polars dataframes\n```\n\n</details>\n\n## Quick Start\n\n```python\nimport pandas as pd\nimport pandera as pa\n\n\n# data to validate\ndf = pd.DataFrame({\n    \"column1\": [1, 4, 0, 10, 9],\n    \"column2\": [-1.3, -1.4, -2.9, -10.1, -20.4],\n    \"column3\": [\"value_1\", \"value_2\", \"value_3\", \"value_2\", \"value_1\"]\n})\n\n# define schema\nschema = pa.DataFrameSchema({\n    \"column1\": pa.Column(int, checks=pa.Check.le(10)),\n    \"column2\": pa.Column(float, checks=pa.Check.lt(-1.2)),\n    \"column3\": pa.Column(str, checks=[\n        pa.Check.str_startswith(\"value_\"),\n        # define custom checks as functions that take a series as input and\n        # outputs a boolean or boolean Series\n        pa.Check(lambda s: s.str.split(\"_\", expand=True).shape[1] == 2)\n    ]),\n})\n\nvalidated_df = schema(df)\nprint(validated_df)\n\n#     column1  column2  column3\n#  0        1     -1.3  value_1\n#  1        4     -1.4  value_2\n#  2        0     -2.9  value_3\n#  3       10    -10.1  value_2\n#  4        9    -20.4  value_1\n```\n\n## DataFrame Model\n\n`pandera` also provides an alternative API for expressing schemas inspired\nby [dataclasses](https://docs.python.org/3/library/dataclasses.html) and\n[pydantic](https://pydantic-docs.helpmanual.io/). The equivalent `DataFrameModel`\nfor the above `DataFrameSchema` would be:\n\n\n```python\nfrom pandera.typing import Series\n\nclass Schema(pa.DataFrameModel):\n\n    column1: int = pa.Field(le=10)\n    column2: float = pa.Field(lt=-1.2)\n    column3: str = pa.Field(str_startswith=\"value_\")\n\n    @pa.check(\"column3\")\n    def column_3_check(cls, series: Series[str]) -> Series[bool]:\n        \"\"\"Check that values have two elements after being split with '_'\"\"\"\n        return series.str.split(\"_\", expand=True).shape[1] == 2\n\nSchema.validate(df)\n```\n\n## Development Installation\n\n```\ngit clone https://github.com/pandera-dev/pandera.git\ncd pandera\nexport PYTHON_VERSION=...  # specify desired python version\npip install -r dev/requirements-${PYTHON_VERSION}.txt\npip install -e .\n```\n\n## Tests\n\n```\npip install pytest\npytest tests\n```\n\n## Contributing to pandera [![GitHub contributors](https://img.shields.io/github/contributors/pandera-dev/pandera.svg?style=for-the-badge)](https://github.com/pandera-dev/pandera/graphs/contributors)\n\nAll contributions, bug reports, bug fixes, documentation improvements,\nenhancements and ideas are welcome.\n\nA detailed overview on how to contribute can be found in the\n[contributing guide](https://github.com/pandera-dev/pandera/blob/main/.github/CONTRIBUTING.md)\non GitHub.\n\n## Issues\n\nGo [here](https://github.com/pandera-dev/pandera/issues) to submit feature\nrequests or bugfixes.\n\n## Need Help?\n\nThere are many ways of getting help with your questions. You can ask a question\non [Github Discussions](https://github.com/pandera-dev/pandera/discussions/categories/q-a)\npage or reach out to the maintainers and pandera community on\n[Discord](https://discord.gg/vyanhWuaKB)\n\n## Why `pandera`?\n\n- [dataframe-centric data types](https://pandera.readthedocs.io/en/stable/dtypes.html),\n  [column nullability](https://pandera.readthedocs.io/en/stable/dataframe_schemas.html#null-values-in-columns),\n  and [uniqueness](https://pandera.readthedocs.io/en/stable/dataframe_schemas.html#validating-the-joint-uniqueness-of-columns)\n  are first-class concepts.\n- Define [dataframe models](https://pandera.readthedocs.io/en/stable/schema_models.html) with the class-based API with\n  [pydantic](https://pydantic-docs.helpmanual.io/)-style syntax and validate dataframes using the typing syntax.\n- `check_input` and `check_output` [decorators](https://pandera.readthedocs.io/en/stable/decorators.html#decorators-for-pipeline-integration)\n  enable seamless integration with existing code.\n- [`Check`s](https://pandera.readthedocs.io/en/stable/checks.html) provide flexibility and performance by providing access to `pandas`\n  API by design and offers built-in checks for common data tests.\n- [`Hypothesis`](https://pandera.readthedocs.io/en/stable/hypothesis.html) class provides a tidy-first interface for statistical hypothesis\n  testing.\n- `Check`s and `Hypothesis` objects support both [tidy and wide data validation](https://pandera.readthedocs.io/en/stable/checks.html#wide-checks).\n- Use schemas as generative contracts to [synthesize data](https://pandera.readthedocs.io/en/stable/data_synthesis_strategies.html) for unit testing.\n- [Schema inference](https://pandera.readthedocs.io/en/stable/schema_inference.html) allows you to bootstrap schemas from data.\n\n## How to Cite\n\nIf you use `pandera` in the context of academic or industry research, please\nconsider citing the **paper** and/or **software package**.\n\n### [Paper](https://conference.scipy.org/proceedings/scipy2020/niels_bantilan.html)\n\n```\n@InProceedings{ niels_bantilan-proc-scipy-2020,\n  author    = { {N}iels {B}antilan },\n  title     = { pandera: {S}tatistical {D}ata {V}alidation of {P}andas {D}ataframes },\n  booktitle = { {P}roceedings of the 19th {P}ython in {S}cience {C}onference },\n  pages     = { 116 - 124 },\n  year      = { 2020 },\n  editor    = { {M}eghann {A}garwal and {C}hris {C}alloway and {D}illon {N}iederhut and {D}avid {S}hupe },\n  doi       = { 10.25080/Majora-342d178e-010 }\n}\n```\n\n### Software Package\n\n[![DOI](https://img.shields.io/badge/DOI-10.5281/zenodo.3385265-blue?style=for-the-badge)](https://doi.org/10.5281/zenodo.3385265)\n\n\n## License and Credits\n\n`pandera` is licensed under the [MIT license](license.txt) and is written and\nmaintained by Niels Bantilan (niels@union.ai)\n"
        },
        {
          "name": "asv_bench",
          "type": "tree",
          "content": null
        },
        {
          "name": "ci",
          "type": "tree",
          "content": null
        },
        {
          "name": "dev",
          "type": "tree",
          "content": null
        },
        {
          "name": "docs",
          "type": "tree",
          "content": null
        },
        {
          "name": "environment.yml",
          "type": "blob",
          "size": 1.4169921875,
          "content": "name: pandera-dev\nchannels:\n  - conda-forge\n\ndependencies:\n  # environment management\n  - pip\n\n  # pandera dependencies\n  - packaging >= 20.0\n  - hypothesis >= 6.92.7\n  - numpy >= 1.19.0\n  - pandas\n  - scipy\n  - pyyaml >=5.1\n  - typing_inspect >= 0.6.0\n  - typing_extensions >= 3.7.4.3\n  - frictionless <= 4.40.8  # v5.* introduces breaking changes\n  - pyarrow\n  - pydantic\n\n  # mypy extra\n  - pandas-stubs\n\n  # pyspark extra\n  - pyspark[connect] >= 3.2.0\n\n  # polars extra\n  - polars >= 0.20.0\n\n  # modin extra\n  - modin\n  - protobuf\n\n  # geopandas extra\n  - geopandas\n  - shapely\n\n  # fastapi extra\n  - fastapi\n\n  # testing and dependencies\n  - black >= 24.0\n\n  # testing\n  - isort >= 5.7.0\n  - joblib\n  - mypy = 1.10.0\n  - pylint <= 2.17.3\n  - pytest\n  - pytest-cov\n  - pytest-xdist\n  - pytest-asyncio\n  - pytz\n  - xdoctest\n  - nox\n  - importlib_metadata # required if python < 3.8\n\n  # fastapi testing\n  - uvicorn\n  - python-multipart\n\n  # documentation\n  - sphinx\n  - sphinx-design\n  - sphinx-autodoc-typehints <= 1.14.1\n  - sphinx-copybutton\n  - recommonmark\n  - myst-nb\n\n  # packaging\n  - twine\n\n  # performance testing\n  - asv >= 0.5.1\n\n  # optional\n  - pre_commit\n\n  - pip:\n      # dask extra\n      - dask[dataframe]\n      - distributed\n\n      # docs\n      - furo\n      - sphinx-docsearch\n      - grpcio\n      - ray\n      - typeguard\n      - types-click\n      - types-pytz\n      - types-pyyaml\n      - types-requests\n      - types-setuptools\n"
        },
        {
          "name": "mypy.ini",
          "type": "blob",
          "size": 0.3125,
          "content": "[mypy]\nignore_missing_imports = True\nfollow_imports = skip\nallow_redefinition = True\nwarn_return_any = False\nwarn_unused_configs = True\nshow_error_codes = True\nexclude=(?x)(\n    ^tests/mypy/modules\n    | ^pandera/engines/pyspark_engine\n    | ^pandera/api/pyspark\n    | ^pandera/backends/pyspark\n    | ^tests/pyspark\n  )\n"
        },
        {
          "name": "noxfile.py",
          "type": "blob",
          "size": 13.8837890625,
          "content": "\"\"\"Nox sessions.\"\"\"\n\n# isort: skip_file\nimport os\nimport re\nimport shutil\nimport sys\nimport tempfile\nfrom typing import Dict, List\n\n# setuptools must be imported before distutils !\nimport setuptools\nfrom distutils.core import (\n    run_setup,\n)\n\nimport nox\nfrom nox import Session\nfrom pkg_resources import Requirement, parse_requirements\n\n\nnox.options.sessions = (\n    \"requirements\",\n    \"ci_requirements\",\n    \"tests\",\n    \"docs\",\n    \"doctests\",\n)\n\nDEFAULT_PYTHON = \"3.8\"\nPYTHON_VERSIONS = [\"3.8\", \"3.9\", \"3.10\", \"3.11\"]\nPANDAS_VERSIONS = [\"1.5.3\", \"2.2.2\"]\nPYDANTIC_VERSIONS = [\"1.10.11\", \"2.3.0\"]\n\nPACKAGE = \"pandera\"\n\nSOURCE_PATHS = PACKAGE, \"tests\", \"noxfile.py\"\nREQUIREMENT_PATH = \"requirements.in\"\nALWAYS_USE_PIP = {\n    \"furo\",\n    \"ray\",\n    \"types-click\",\n    \"types-pyyaml\",\n    \"types-setuptools\",\n}\n\nCI_RUN = os.environ.get(\"CI\") == \"true\"\nif CI_RUN:\n    print(\"Running on CI\")\nelse:\n    print(\"Running locally\")\n\nLINE_LENGTH = 79\n\n\ndef _build_setup_requirements() -> Dict[str, List[Requirement]]:\n    \"\"\"Load requirments from setup.py.\"\"\"\n    dist = run_setup(\"setup.py\")\n    reqs = {\"core\": dist.install_requires}  # type: ignore\n    reqs.update(dist.extras_require)  # type: ignore\n    return {\n        extra: list(parse_requirements(reqs)) for extra, reqs in reqs.items()\n    }\n\n\ndef _build_dev_requirements() -> List[Requirement]:\n    \"\"\"Load requirements from file.\"\"\"\n    with open(REQUIREMENT_PATH, \"rt\", encoding=\"utf-8\") as req_file:\n        reqs = []\n        for req in parse_requirements(req_file.read()):\n            req.marker = None\n            reqs.append(req)\n        return reqs\n\n\nSETUP_REQUIREMENTS: Dict[str, List[Requirement]] = _build_setup_requirements()\nDEV_REQUIREMENTS: List[Requirement] = _build_dev_requirements()\n\n\ndef _requirement_to_dict(reqs: List[Requirement]) -> Dict[str, str]:\n    \"\"\"Return a dict {PKG_NAME:PIP_SPECS}.\"\"\"\n    req_dict = {}\n    for req in reqs:\n        specs = req.specs[0] if req.specs else []\n        specs_str = \" \".join([req.unsafe_name, *specs]).replace(\" \", \"\")\n        req_dict[req.unsafe_name] = specs_str\n    return req_dict\n\n\ndef _build_requires() -> Dict[str, Dict[str, str]]:\n    \"\"\"Return a dictionary of requirements {EXTRA_NAME: {PKG_NAME:PIP_SPECS}}.\n\n    Adds fake extras \"core\" and \"all\".\n    \"\"\"\n    extras = {\n        extra: reqs\n        for extra, reqs in SETUP_REQUIREMENTS.items()\n        if extra not in (\"core\", \"all\")\n    }\n    extras[\"all\"] = DEV_REQUIREMENTS\n\n    optionals = [\n        req.project_name\n        for extra, reqs in extras.items()\n        for req in reqs\n        if extra != \"all\"\n    ]\n    requires = {\"all\": _requirement_to_dict(extras[\"all\"])}\n    requires[\"core\"] = {\n        pkg: specs\n        for pkg, specs in requires[\"all\"].items()\n        if pkg not in optionals\n    }\n    requires.update(  # add extras\n        {\n            extra_name: {**_requirement_to_dict(pkgs), **requires[\"core\"]}\n            for extra_name, pkgs in extras.items()\n            if extra_name != \"all\"\n        }\n    )\n    return requires\n\n\nREQUIRES: Dict[str, Dict[str, str]] = _build_requires()\n\nCONDA_ARGS = [\n    \"--channel=conda-forge\",\n    \"--update-specs\",\n]\n\n\ndef extract_requirement_name(spec: str) -> str:\n    \"\"\"\n    Extract name of requirement from dependency string.\n    \"\"\"\n    # Assume name is everything up to the first invalid character\n    match = re.match(r\"^[A-Za-z0-9-_]*\", spec.strip())\n    if not match:\n        raise ValueError(f\"Cannot parse requirement {spec!r}\")\n    return match[0]\n\n\ndef conda_install(session: Session, *args):\n    \"\"\"Use mamba to install conda dependencies.\"\"\"\n    run_args = [\n        \"install\",\n        \"--yes\",\n        *CONDA_ARGS,\n        \"--prefix\",\n        session.virtualenv.location,  # type: ignore\n        *args,\n    ]\n\n    # By default, all dependencies are re-installed from scratch with each\n    # session. Specifying external=True allows access to cached packages, which\n    # decreases runtime of the test sessions.\n    try:\n        session.run(\n            *[\"mamba\", *run_args],\n            external=True,\n        )\n    # pylint: disable=broad-except\n    except Exception:\n        session.run(\n            *[\"conda\", *run_args],\n            external=True,\n        )\n\n\ndef install(session: Session, *args: str):\n    \"\"\"Install dependencies in the appropriate virtual environment\n    (conda or virtualenv) and return the type of the environmment.\"\"\"\n    if isinstance(session.virtualenv, nox.virtualenv.CondaEnv):\n        print(\"using conda installer\")\n        conda_install(session, *args)\n    else:\n        print(\"using pip installer\")\n        session.install(*args)\n\n\ndef install_from_requirements(session: Session, *packages: str) -> None:\n    \"\"\"\n    Install dependencies, respecting the version specified in requirements.\n    \"\"\"\n    for package in packages:\n        try:\n            specs = REQUIRES[\"all\"][package]\n        except KeyError:\n            raise ValueError(\n                f\"{package} cannot be found in {REQUIREMENT_PATH}.\"\n            ) from None\n        install(session, specs)\n\n\ndef install_extras(\n    session: Session,\n    extra: str = \"core\",\n    force_pip: bool = False,\n    pandas: str = \"latest\",\n    pandas_stubs: bool = True,\n) -> None:\n    \"\"\"Install dependencies.\"\"\"\n\n    if isinstance(session.virtualenv, nox.virtualenv.PassthroughEnv):\n        # skip this step if there's no virtual environment specified\n        session.run(\"pip\", \"install\", \"-e\", \".\", \"--no-deps\")\n        return\n\n    specs, pip_specs = [], []\n    pandas_version = \"\" if pandas == \"latest\" else f\"=={pandas}\"\n    for spec in REQUIRES[extra].values():\n        req_name = extract_requirement_name(spec)\n        if req_name == \"pandas-stubs\" and not pandas_stubs:\n            # this is a temporary measure until all pandas-related mypy errors\n            # are addressed\n            continue\n\n        req = Requirement(spec)  # type: ignore\n\n        # this is needed until ray is supported on python 3.10\n        # pylint: disable=line-too-long\n        if req.name in {\"ray\", \"geopandas\"} and session.python == \"3.10\":  # type: ignore[attr-defined]  # noqa\n            continue\n\n        if req.name in ALWAYS_USE_PIP:  # type: ignore[attr-defined]\n            pip_specs.append(spec)\n        elif req_name == \"pandas\" and pandas != \"latest\":\n            specs.append(f\"pandas~={pandas}\")\n        else:\n            specs.append(\n                spec if spec != \"pandas\" else f\"pandas{pandas_version}\"\n            )\n    if extra in {\"core\", \"pyspark\", \"modin\", \"fastapi\"}:\n        specs.append(REQUIRES[\"all\"][\"hypothesis\"])\n\n    # CI installs conda dependencies, so only run this for local runs\n    if (\n        isinstance(session.virtualenv, nox.virtualenv.CondaEnv)\n        and not force_pip\n        and not CI_RUN\n    ):\n        print(\"using conda installer\")\n        conda_install(session, *specs)\n    else:\n        print(\"using pip installer\")\n        session.install(*specs)\n\n    # always use pip for these packages)\n    session.install(*pip_specs)\n    session.install(\"-e\", \".\", \"--no-deps\")  # install pandera\n\n\ndef _generate_pip_deps_from_conda(\n    session: Session, compare: bool = False\n) -> None:\n    args = [\"scripts/generate_pip_deps_from_conda.py\"]\n    if compare:\n        args.append(\"--compare\")\n    session.run(\"python\", *args)\n\n\n@nox.session(python=PYTHON_VERSIONS)\ndef requirements(session: Session) -> None:  # pylint:disable=unused-argument\n    \"\"\"Check that setup.py requirements match requirements-dev.txt\"\"\"\n    install(session, \"pyyaml\")\n    try:\n        _generate_pip_deps_from_conda(session, compare=True)\n    except nox.command.CommandFailed as err:\n        _generate_pip_deps_from_conda(session)\n        print(f\"{REQUIREMENT_PATH} has been re-generated âœ¨ ðŸ° âœ¨\")\n        raise err\n\n    ignored_pkgs = {\"black\", \"pandas\", \"pandas-stubs\", \"modin\"}\n    mismatched = []\n    # only compare package versions, not python version markers.\n    str_dev_reqs = [str(x) for x in DEV_REQUIREMENTS]\n    for extra, reqs in SETUP_REQUIREMENTS.items():\n        for req in reqs:\n            if (\n                req.project_name not in ignored_pkgs\n                and str(req) not in str_dev_reqs\n            ):\n                mismatched.append(f\"{extra}: {req.project_name}\")\n\n    if mismatched:\n        print(\n            f\"Packages {mismatched} defined in setup.py \"\n            + f\"do not match {REQUIREMENT_PATH}.\"\n        )\n        print(\n            \"Modify environment.yml, \"\n            + f\"then run 'nox -s requirements' to generate {REQUIREMENT_PATH}\"\n        )\n        sys.exit(1)\n\n\ndef _ci_requirement_file_name(\n    session: Session,\n    pandas: str,\n    pydantic: str,\n) -> str:\n    return (\n        \"ci/requirements-\"\n        f\"py{session.python}-\"\n        f\"pandas{pandas}-\"\n        f\"pydantic{pydantic}.txt\"\n    )\n\n\nPYTHON_PANDAS_PARAMETER = [\n    (python, pandas)\n    for python in PYTHON_VERSIONS\n    for pandas in PANDAS_VERSIONS\n    if (python, pandas) != (\"3.8\", \"2.2.0\")\n]\n\n\n@nox.session\n@nox.parametrize(\"python,pandas\", PYTHON_PANDAS_PARAMETER)\n@nox.parametrize(\"pydantic\", PYDANTIC_VERSIONS)\ndef ci_requirements(session: Session, pandas: str, pydantic: str) -> None:\n    \"\"\"Install pinned dependencies for CI.\"\"\"\n    if session.python == \"3.8\" and pandas == \"2.2.2\":\n        session.skip()\n\n    _numpy: str | None = None\n    if pandas != \"2.2.2\":\n        _numpy = \"< 2\"\n\n    session.install(\"uv\")\n\n    requirements = []\n    with open(\"requirements.in\") as f:\n        for line in f.readlines():\n            _line = line.strip()\n            if _line == \"pandas\":\n                line = f\"pandas=={pandas}\\n\"\n            if _line == \"pydantic\":\n                line = f\"pydantic=={pydantic}\\n\"\n            if _line.startswith(\"numpy\") and _numpy is not None:\n                print(\"adding numpy constraint <2\")\n                line = f\"{_line}, {_numpy}\\n\"\n            # for some reason uv will try to install an old version of dask,\n            # have to specifically pin dask[dataframe] to a higher version\n            if _line == \"dask[dataframe]\" and session.python in (\n                \"3.9\",\n                \"3.10\",\n                \"3.11\",\n            ):\n                line = \"dask[dataframe]>=2023.9.2\\n\"\n            requirements.append(line)\n\n    with tempfile.NamedTemporaryFile(\"a\") as f:\n        f.writelines(requirements)\n        f.seek(0)\n        session.run(\n            \"uv\",\n            \"pip\",\n            \"compile\",\n            f\"{f.name}\",\n            \"--output-file\",\n            _ci_requirement_file_name(session, pandas, pydantic),\n            \"--no-header\",\n            \"--upgrade\",\n            \"--no-annotate\",\n        )\n\n\n@nox.session(python=PYTHON_VERSIONS)\ndef dev_requirements(session: Session) -> None:\n    \"\"\"Install pinned dependencies for CI.\"\"\"\n    session.install(\"uv\")\n    output_file = f\"dev/requirements-{session.python}.txt\"\n    session.run(\n        \"uv\",\n        \"pip\",\n        \"compile\",\n        \"requirements.in\",\n        \"--output-file\",\n        output_file,\n        \"--no-header\",\n        \"--upgrade\",\n        \"--no-annotate\",\n    )\n\n\nEXTRA_NAMES = [\n    extra\n    for extra in REQUIRES\n    if (\n        extra != \"all\"\n        and \"python_version\" not in extra\n        and extra not in {\"modin\"}\n    )\n]\n\n\n@nox.session\n@nox.parametrize(\"python,pandas\", PYTHON_PANDAS_PARAMETER)\n@nox.parametrize(\"pydantic\", PYDANTIC_VERSIONS)\n@nox.parametrize(\"extra\", EXTRA_NAMES)\ndef tests(session: Session, pandas: str, pydantic: str, extra: str) -> None:\n    \"\"\"Run the test suite.\"\"\"\n\n    if not isinstance(session.virtualenv, nox.virtualenv.PassthroughEnv):\n        session.install(\"uv\")\n        session.run(\n            \"uv\",\n            \"pip\",\n            \"install\",\n            \"-r\",\n            _ci_requirement_file_name(session, pandas, pydantic),\n        )\n\n    session.run(\"pip\", \"list\")\n\n    env = {}\n    if extra.startswith(\"modin\"):\n        extra, engine = extra.split(\"-\")\n        if engine not in {\"dask\", \"ray\"}:\n            raise ValueError(f\"{engine} is not a valid modin engine\")\n        env = {\"CI_MODIN_ENGINES\": engine}\n\n    if session.posargs:\n        args = session.posargs\n    else:\n        path = f\"tests/{extra}/\" if extra != \"all\" else \"tests\"\n        args = []\n        if extra == \"strategies\":\n            profile = \"ci\"\n            # enable threading via pytest-xdist\n            args = [\n                \"-n=auto\",\n                \"-q\",\n                f\"--hypothesis-profile={profile}\",\n            ]\n        args += [\n            f\"--cov={PACKAGE}\",\n            \"--cov-report=term-missing\",\n            \"--cov-report=xml\",\n            \"--cov-append\",\n            \"--verbosity=10\",\n        ]\n        if not CI_RUN:\n            args.append(\"--cov-report=html\")\n        args.append(path)\n\n    session.run(\"pytest\", *args, env=env)\n\n\n@nox.session(python=PYTHON_VERSIONS)\ndef doctests(session: Session) -> None:\n    \"\"\"Build the documentation.\"\"\"\n    install_extras(session, extra=\"all\", force_pip=True)\n    session.run(\"xdoctest\", PACKAGE, \"--quiet\")\n\n\n@nox.session(python=PYTHON_VERSIONS)\ndef docs(session: Session) -> None:\n    \"\"\"Build the documentation.\"\"\"\n    # this is needed until ray and geopandas are supported on python 3.10\n    if session.python == \"3.10\":\n        session.skip()\n\n    install_extras(session, extra=\"all\", force_pip=True)\n    session.chdir(\"docs\")\n\n    # build html docs\n    if not CI_RUN and not session.posargs:\n        shutil.rmtree(\"_build\", ignore_errors=True)\n        shutil.rmtree(\n            os.path.join(\"source\", \"reference\", \"generated\"),\n            ignore_errors=True,\n        )\n        for builder in [\"doctest\", \"html\"]:\n            session.run(\n                \"sphinx-build\",\n                \"-W\",\n                \"-T\",\n                f\"-b={builder}\",\n                \"-d\",\n                os.path.join(\"_build\", \"doctrees\", \"\"),\n                \"source\",\n                os.path.join(\"_build\", builder, \"\"),\n            )\n    else:\n        shutil.rmtree(os.path.join(\"_build\"), ignore_errors=True)\n        args = session.posargs or [\n            \"-v\",\n            \"-W\",\n            \"-E\",\n            \"-b=doctest\",\n            \"source\",\n            \"_build\",\n        ]\n        session.run(\"sphinx-build\", *args)\n"
        },
        {
          "name": "pandera",
          "type": "tree",
          "content": null
        },
        {
          "name": "pyproject.toml",
          "type": "blob",
          "size": 0.3359375,
          "content": "[tool.pyright]\ninclude = [\"pandera\", \"tests\"]\nexclude = [\".nox/**\", \".nox-*/**\"]\n\n[tool.pytest.ini_options]\nlog_cli = true\nlog_cli_level = 20\n\n[tool.black]\nline-length = 79\ntarget-version = [\n  'py37',\n  'py38',\n  'py39',\n  'py310',\n  'py311',\n]\ninclude = '\\.pyi?$'\nexclude = '''\n(\n  \\.git\n  | \\.venv\n  | \\.nox\n  | setup.py\n  | asv_bench\n)\n'''\n"
        },
        {
          "name": "requirements.in",
          "type": "blob",
          "size": 0.8330078125,
          "content": "# This file is auto-generated from environment.yml, do not modify.\n# See that file for comments about the need/usage of each dependency.\n\npip\npackaging >= 20.0\nhypothesis >= 6.92.7\nnumpy >= 1.19.0\npandas\nscipy\npyyaml >=5.1\ntyping_inspect >= 0.6.0\ntyping_extensions >= 3.7.4.3\nfrictionless <= 4.40.8\npyarrow\npydantic\npandas-stubs\npyspark[connect] >= 3.2.0\npolars >= 0.20.0\nmodin\nprotobuf\ngeopandas\nshapely\nfastapi\nblack >= 24.0\nisort >= 5.7.0\njoblib\nmypy == 1.10.0\npylint <= 2.17.3\npytest\npytest-cov\npytest-xdist\npytest-asyncio\npytz\nxdoctest\nnox\nimportlib_metadata\nuvicorn\npython-multipart\nsphinx\nsphinx-design\nsphinx-autodoc-typehints <= 1.14.1\nsphinx-copybutton\nrecommonmark\nmyst-nb\ntwine\nasv >= 0.5.1\npre_commit\ndask[dataframe]\ndistributed\nfuro\nsphinx-docsearch\ngrpcio\nray\ntypeguard\ntypes-click\ntypes-pytz\ntypes-pyyaml\ntypes-requests\ntypes-setuptools\n"
        },
        {
          "name": "scripts",
          "type": "tree",
          "content": null
        },
        {
          "name": "setup.cfg",
          "type": "blob",
          "size": 0.04296875,
          "content": "[isort]\nfloat_to_top = true\nprofile = black\n"
        },
        {
          "name": "setup.py",
          "type": "blob",
          "size": 2.40625,
          "content": "from setuptools import find_packages, setup\n\nwith open(\"README.md\") as f:\n    long_description = f.read()\n\nversion = {}\nwith open(\"pandera/version.py\") as fp:\n    exec(fp.read(), version)\n\n_extras_require = {\n    \"strategies\": [\"hypothesis >= 6.92.7\"],\n    \"hypotheses\": [\"scipy\"],\n    \"io\": [\"pyyaml >= 5.1\", \"black\", \"frictionless <= 4.40.8\"],\n    \"pyspark\": [\"pyspark[connect] >= 3.2.0\"],\n    \"modin\": [\"modin\", \"ray\", \"dask[dataframe]\"],\n    \"modin-ray\": [\"modin\", \"ray\"],\n    \"modin-dask\": [\"modin\", \"dask[dataframe]\"],\n    \"dask\": [\"dask[dataframe]\"],\n    \"mypy\": [\"pandas-stubs\"],\n    \"fastapi\": [\"fastapi\"],\n    \"geopandas\": [\"geopandas\", \"shapely\"],\n    \"polars\": [\"polars >= 0.20.0\"],\n}\n\nextras_require = {\n    **_extras_require,\n    \"all\": list(set(x for y in _extras_require.values() for x in y)),\n}\n\nsetup(\n    name=\"pandera\",\n    version=version[\"__version__\"],\n    author=\"Niels Bantilan\",\n    author_email=\"niels.bantilan@gmail.com\",\n    description=\"A light-weight and flexible data validation and testing tool for statistical data objects.\",\n    long_description=long_description,\n    long_description_content_type=\"text/markdown\",\n    url=\"https://github.com/pandera-dev/pandera\",\n    project_urls={\n        \"Documentation\": \"https://pandera.readthedocs.io\",\n        \"Issue Tracker\": \"https://github.com/pandera-dev/pandera/issues\",\n    },\n    keywords=[\"pandas\", \"validation\", \"data-structures\"],\n    license=\"MIT\",\n    data_files=[(\"\", [\"LICENSE.txt\"])],\n    packages=find_packages(include=[\"pandera*\"]),\n    package_data={\"pandera\": [\"py.typed\"]},\n    install_requires=[\n        \"numpy >= 1.19.0\",\n        \"packaging >= 20.0\",\n        \"pandas >= 1.2.0\",\n        \"pydantic\",\n        \"typeguard\",\n        \"typing_extensions >= 3.7.4.3 ; python_version<'3.8'\",\n        \"typing_inspect >= 0.6.0\",\n    ],\n    extras_require=extras_require,\n    python_requires=\">=3.7\",\n    platforms=\"any\",\n    classifiers=[\n        \"Development Status :: 5 - Production/Stable\",\n        \"Operating System :: OS Independent\",\n        \"License :: OSI Approved :: MIT License\",\n        \"Intended Audience :: Science/Research\",\n        \"Programming Language :: Python\",\n        \"Programming Language :: Python :: 3\",\n        \"Programming Language :: Python :: 3.8\",\n        \"Programming Language :: Python :: 3.9\",\n        \"Programming Language :: Python :: 3.10\",\n        \"Programming Language :: Python :: 3.11\",\n        \"Topic :: Scientific/Engineering\",\n    ],\n)\n"
        },
        {
          "name": "tests",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}