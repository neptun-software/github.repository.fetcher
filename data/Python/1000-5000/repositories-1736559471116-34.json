{
  "metadata": {
    "timestamp": 1736559471116,
    "page": 34,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjQw",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "xxlong0/Wonder3D",
      "stars": 4912,
      "defaultBranch": "main",
      "files": [
        {
          "name": ".DS_Store",
          "type": "blob",
          "size": 12.00390625,
          "content": null
        },
        {
          "name": "1gpu.yaml",
          "type": "blob",
          "size": 0.2939453125,
          "content": "compute_environment: LOCAL_MACHINE\ndistributed_type: 'NO'\ndowncast_bf16: 'no'\ngpu_ids: '0'\nmachine_rank: 0\nmain_training_function: main\nmixed_precision: 'no'\nnum_machines: 1\nnum_processes: 1\nrdzv_backend: static\nsame_network: true\ntpu_env: []\ntpu_use_cluster: false\ntpu_use_sudo: false\nuse_cpu: false\n"
        },
        {
          "name": "8gpu.yaml",
          "type": "blob",
          "size": 0.3134765625,
          "content": "compute_environment: LOCAL_MACHINE\r\ndistributed_type: MULTI_GPU\r\ndowncast_bf16: 'no'\r\ngpu_ids: all\r\nmachine_rank: 0\r\nmain_training_function: main\r\nmixed_precision: 'no'\r\nnum_machines: 1\r\nnum_processes: 8\r\nrdzv_backend: static\r\nsame_network: true\r\ntpu_env: []\r\ntpu_use_cluster: false\r\ntpu_use_sudo: false\r\nuse_cpu: false\r\n"
        },
        {
          "name": "MVControlNet",
          "type": "tree",
          "content": null
        },
        {
          "name": "MVMeshRecon",
          "type": "tree",
          "content": null
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 7.3369140625,
          "content": "# Wonder3D -> Wonder3D++\nSingle Image to 3D using Cross-Domain Diffusion (CVPR 2024 Highlight).\n\nNow extent to **Wonder3D++**!\n## [Paper](https://arxiv.org/abs/2310.15008) | [Project page](https://www.xxlong.site/Wonder3D/) | [Hugging Face Demo](https://huggingface.co/spaces/flamehaze1115/Wonder3D-demo) | [Colab from @camenduru](https://github.com/camenduru/Wonder3D-colab)\n\n![](assets/teaser.png)\n\nWonder3D++ reconstructs highly-detailed textured meshes from a single-view image in only 3 minutes. Wonder3D++ first generates consistent multi-view normal maps with corresponding color images via a **cross-domain diffusion model** and then leverages a **cascaded 3D mesh extraction** method to achieve fast and high-quality reconstruction.\n\n## Collaborations\nOur overarching mission is to enhance the speed, affordability, and quality of 3D AIGC, making the creation of 3D content accessible to all. While significant progress has been achieved in the recent years, we acknowledge there is still a substantial journey ahead. We enthusiastically invite you to engage in discussions and explore potential collaborations in any capacity. <span style=\"color:red\">**If you're interested in connecting or partnering with us, please don't hesitate to reach out via email (xxlong@connect.hku.hk)**</span> .\n\n## News\n- 2025.1.6 An update to our paper **Wonder3D++** is now available in our repository!\n- 2024.12.22 **<span style=\"color:red\">We have extent the [Wonder3D] to a more advanced version, [Wonder3D++]!</span>**.\n- 2024.08.29 Fixed an issue in '/mvdiffusion/pipelines/pipeline_mvdiffusion_image' where cross-domain attention did not work correctly during classifier-free guidance (CFG) inference, causing misalignment between the RGB and normal generation results. To address this issue, we need to place the RGB and normal domain inputs in the first and second halves of the batch, respectively, before feeding them into the model. This approach differs from the typical CFG method, which separates unconditional and conditional inputs into the first and second halves of the batch.\n- 2024.05.29 We release a more powerful MV cross-domain diffusion model [Era3D](https://github.com/pengHTYX/Era3D) that jointly produces 512x512 color images and normal maps, but more importantly Era3D could automatically figure out the focal length and elevation degree of the input image so that avoid geometry distortions.\n- 2024.05.24 We release a large 3D native diffusion model [CraftsMan3D](https://github.com/wyysf-98/CraftsMan) that is directly trained on 3D representation and therefore is capable of producing complex structures.\n- Fixed a severe training bug. The \"zero_init_camera_projection\" in 'configs/train/stage1-mix-6views-lvis.yaml' should be False. Otherwise, the domain control and pose control will be invalid in the training.\n- 2024.03.19 Checkout our new model [GeoWizard](https://github.com/fuxiao0719/GeoWizard) that jointly produces depth and normal with high fidelity from single images.\n- 2024.02 We release the training codes. Welcome to train wonder3D on your personal data.\n- 2023.10 We release the inference model and codes.\n\n\n### Preparation for inference\n\n#### Linux System Setup.\n```angular2html\nconda create -n wonder3d_plus python=3.10\nconda activate wonder3d_plus\npip install -r requirements.txt\n```\n\n### Prepare the training data\nsee [render_codes/README.md](render_codes/README.md).\n\n### Training\nHere we provide two training scripts `train_mvdiffusion_mixed.py` and `train_mvdiffusion_joint.py`. \n\nOur **Multi-Stage Training Scheme** training has three stages: \n1) We initially remove the domain switcher and cross-domain attention layers, modifying only the self-attention layers into a multi-view design;\n2) We introduce the domain switcher, fine-tuning the model to generate either multi-view colors or normals from a single-view color image, guided by the domain switcher;\n3) We add cross-domain attention modules into the SD model, and only optimize the newly added parameters.\n\nYou need to modify `root_dir` that contain the data of the config files `configs/train/stage1-mix-6views-lvis.yaml` and `configs/train/stage2-joint-6views-lvis.yaml` accordingly.\n\n```\n# stage 1:\naccelerate launch --config_file 8gpu.yaml train_mvdiffusion_mixed.py --config configs/train/stage1-mixed-wo-switcher.yaml\n\n# stage 2:\naccelerate launch --config_file 8gpu.yaml train_mvdiffusion_mixed.py --config configs/train/stage1-mixed-6views-image-normal.yaml\n\n# stage 3:\naccelerate launch --config_file 8gpu.yaml train_mvdiffusion_joint_stage3.py --config configs/train/stage3-joint-6views-image-normal.yaml\n```\n\nTo train our multi-view enhancement module:\n```\naccelerate launch --config_file 8gpu.yaml train_controlnet.py  --config configs/train-controlnet/mv_controlnet_train_joint.yaml\n```\n\n### Inference\n1. Make sure you have downloaded the following models.\n```bash\nWonder3D_plus\n|-- ckpts\n    |-- mvdiffusion\n    |-- mv_controlnett\n    |-- scheduler\n    |-- vae\n    ...\n```\nYou also can download the model in python script:\n```bash\nfrom huggingface_hub import snapshot_download\nsnapshot_download(repo_id='flamehaze1115/Wonder3D_plus', local_dir=\"./ckpts\")\n```\n2. [Optional] Download the [SAM](https://huggingface.co/spaces/abhishek/StableSAM/blob/main/sam_vit_h_4b8939.pth) model. Put it to the ``sam_pt`` folder.\nWonder3D_plus\n```\n|-- sam_pt\n    |-- sam_vit_h_4b8939.pth\n```\n3. Predict foreground mask as the alpha channel. We use [Clipdrop](https://clipdrop.co/remove-background) to segment the foreground object interactively. \nYou may also use `rembg` to remove the backgrounds.\n```bash\n# !pip install rembg\nimport rembg\nresult = rembg.remove(result)\nresult.show()\n```\n4. Run Wonder3d++ in a end2end manner. Then you can check the results in the folder `./outputs`. (we use `rembg` to remove backgrounds of the results, but the segmentations are not always perfect. May consider using [Clipdrop](https://clipdrop.co/remove-background) to get masks for the generated normal maps and color images, since the quality of masks will significantly influence the reconstructed mesh quality.) \n```bash\npython run.py  --input_path {Path to input image or directory}\\\n            --output_path {Your output_path} \\\n            --crop_size {Default to 192. Crop size of the input image, this is a relative num that assume the resolution of input image is 256.} \\\n            --camera_type {The projection_type of input image, choose from 'ortho' and 'persp'.}\n            --num_refine {Number of iterative refinement, default to 2.}\n```\n\nsee example:\n\n```bash\npython run.py --input_path example_images/owl.png \\\n--camera_type ortho \\\n--output_path outputs \n```\n\n## Acknowledgement\nWe have intensively borrow codes from the following repositories. Many thanks to the authors for sharing their codes.\n- [stable diffusion](https://github.com/CompVis/stable-diffusion)\n- [Era3D](https://github.com/pengHTYX/Era3D)\n- [Unique3D](https://github.com/AiuniAI/Unique3D)\n- [SyncDreamer](https://github.com/liuyuan-pal/SyncDreamer)\n- [continuous-remeshing](https://github.com/Profactor/continuous-remeshing)\n\n## License\nWonder3D++ and Wonder3D is under [AGPL-3.0](https://www.gnu.org/licenses/agpl-3.0.en.html), so any downstream solution and products (including cloud services) that include wonder3d code or a trained model (both pretrained or custom trained) inside it should be open-sourced to comply with the AGPL conditions. If you have any questions about the usage of Wonder3D++, please contact us first.\n"
        },
        {
          "name": "Wonder3D++_paper.pdf",
          "type": "blob",
          "size": 6931.26171875,
          "content": ""
        },
        {
          "name": "assets",
          "type": "tree",
          "content": null
        },
        {
          "name": "configs",
          "type": "tree",
          "content": null
        },
        {
          "name": "datalist",
          "type": "tree",
          "content": null
        },
        {
          "name": "example_images",
          "type": "tree",
          "content": null
        },
        {
          "name": "mv_diffusion_30",
          "type": "tree",
          "content": null
        },
        {
          "name": "render_codes",
          "type": "tree",
          "content": null
        },
        {
          "name": "requirements.txt",
          "type": "blob",
          "size": 0.8984375,
          "content": "--extra-index-url https://download.pytorch.org/whl/cu121\n\ntorch==2.4.1\ntorchvision\ndiffusers[torch]==0.30.0\ntransformers>=4.25.1\nbitsandbytes==0.35.4\ndecord==0.6.0\npytorch-lightning<2\nomegaconf==2.2.3\nnerfacc==0.3.3\ntrimesh==3.9.8\npyhocon==0.3.57\nicecream==2.1.0\n# PyMCubes==0.1.2\naccelerate\nmodelcards\neinops\nftfy\npiq\nmatplotlib\nopencv-python\nimageio\nimageio-ffmpeg\nscipy\npyransac3d\ntorch_efficient_distloss\ntensorboard\nrembg\nsegment_anything\ngradio==3.50.2\nmosaicml-streaming\nonnxruntime_gpu==1.17.0\n# onnxruntime_gpu @ https://pkgs.dev.azure.com/onnxruntime/2a773b67-e88b-4c7f-9fc0-87d31fea8ef2/_packaging/7fa31e42-5da1-4e84-a664-f2b4129c7d45/pypi/download/onnxruntime-gpu/1.17/onnxruntime_gpu-1.17.0-cp310-cp310-manylinux_2_28_x86_64.whl\npyrender\njaxtyping\npymeshlab\ncholespy\ntorch-scatter\ngit+https://github.com/facebookresearch/pytorch3d.git@stable\ngit+https://github.com/NVlabs/nvdiffrast.git\nopen3d\nfire\ncomet_ml"
        },
        {
          "name": "run.py",
          "type": "blob",
          "size": 9.064453125,
          "content": "import os\nimport torch\nimport argparse\nfrom omegaconf import OmegaConf\nfrom PIL import Image\nimport numpy as np\n\nfrom run_mv_prediction import load_wonder3d_pipeline, pred_multiview_joint\nfrom run_mv_enhancement import load_controlnet_pipeline, pred_enhancement_joint\n\nfrom MVMeshRecon.Coarse_recon import coarse_recon\nfrom MVMeshRecon.Iterative_refine import iterative_refine\nfrom MVMeshRecon.utils.refine_lr_to_sr import sr_front\nfrom rembg import remove\n\n# step 1: Load input image and configuration\ndevice = torch.device('cuda:0')\n\nparser = argparse.ArgumentParser()\nparser.add_argument('--config_mvdiffusion', type=str, default='configs/mvdiffusion-joint.yaml', help='Path to multi-view diffusion config.')\nparser.add_argument('--config_controlnet', type=str, default='configs/controlnet.yaml', help='Path to enhancement controlnet config')\nparser.add_argument('--input_path', type=str, default='example_images', help='Path to input image or directory.')\nparser.add_argument('--output_path', type=str, default='outputs/', help='Output directory.')\nparser.add_argument('--seed', type=int, default=42, help='Random seed for sampling.')\nparser.add_argument('--crop_size', type=int, default=192, help='Crop size of the input image, this is a relative num that assume the resolution of input image is 256.')\nparser.add_argument('--camera_type', type=str, default='ortho', help='ortho or persp')\nparser.add_argument('--num_refine',type=int, default=2, help='number of iterative refinement')\nargs = parser.parse_args()\n\nconfig_mv = OmegaConf.load(args.config_mvdiffusion)\nconfig_controlnet = OmegaConf.load(args.config_controlnet)\n\ndef views_6to4(imgs):\n    outs = []\n    for i in range(6):\n        if i == 1 or i == 5:\n            continue\n        outs.append(imgs[i])\n    return outs\n\ndef add_margin(pil_img, color=0, size=256):\n    width, height = pil_img.size\n    result = Image.new(pil_img.mode, (size, size), color)\n    result.paste(pil_img, ((size - width) // 2, (size - height) // 2))\n    return result\n\ndef process_image(image_input, image_size=2048, crop_size=2048*0.75):\n\n    if np.asarray(image_input).shape[-1] == 3:\n        image_input = remove(image_input)\n\n    if crop_size != -1:\n        alpha_np = np.asarray(image_input)[:, :, 3]\n        coords = np.stack(np.nonzero(alpha_np), 1)[:, (1, 0)]\n        min_x, min_y = np.min(coords, 0)\n        max_x, max_y = np.max(coords, 0)\n        ref_img_ = image_input.crop((min_x, min_y, max_x, max_y))\n\n        width, height = ref_img_.size\n\n        # upsamle the input image if the quality of input image is quite low\n        if width < 400 or height < 400:\n            ref_img_ = sr_front(ref_img_)\n\n        h, w = ref_img_.height, ref_img_.width\n        scale = crop_size / max(h, w)\n        h_, w_ = int(scale * h), int(scale * w)\n        ref_img_ = ref_img_.resize((w_, h_))\n        image_input = add_margin(ref_img_, size=image_size)\n    else:\n        image_input = add_margin(image_input, size=max(image_input.height, image_input.width))\n        image_input = image_input.resize((image_size, image_size))\n\n    return image_input\n\n# load input image\nif os.path.isdir(args.input_path):\n    input_files = [\n        os.path.join(args.input_path, file)\n        for file in os.listdir(args.input_path)\n        if file.endswith('.png') or file.endswith('.jpg') or file.endswith('.webp')\n    ]\nelse:\n    input_files = [args.input_path]\n\nprint(f'Total number of input images: {len(input_files)}')\n\nprint('Loading mv diffusion pipeline ...')\nmv_pipeline = load_wonder3d_pipeline(config_mv).to(device)\n\nprint('Loading mv enhancement pipeline ...')\nenhancement_pipeline = load_controlnet_pipeline(config_controlnet).to(device)\n\nnormals_mv_out, imgs_mv_out = [], []\n\nfor i, image_file in enumerate(input_files):\n    try:\n        # step 2: Multiview inference\n        # preprocess the input image\n        input_image = Image.open(image_file)\n\n        input_image = process_image(image_input=input_image, crop_size=args.crop_size*8, image_size=2048)\n\n        # save front image\n        os.makedirs(os.path.join(args.output_path, os.path.basename(image_file).split('.')[0]), exist_ok=True)\n        input_image.save(os.path.join(args.output_path, os.path.basename(image_file).split('.')[0], 'front_img.png'))\n\n        normals_pred, images_pred = pred_multiview_joint(input_image,\n                                                         mv_pipeline,\n                                                         seed=args.seed,\n                                                         crop_size=args.crop_size,\n                                                         camera_type=args.camera_type,\n                                                         cfg=config_mv,\n                                                         case_name=image_file,\n                                                         output_path=args.output_path)\n\n        normals_mv_out.append((normals_pred))\n        imgs_mv_out.append((images_pred))\n\n        # stage 3: Geometric initialize and coarse reconstruction\n        vertices_init_list, faces_init_list = [], []\n        rendered_imgs, rendered_normals = [], []\n\n        mv_normals, mv_imgs = normals_mv_out[i], imgs_mv_out[i]\n\n        rendered_rgbs, rendered_normal, vertices, faces = coarse_recon(front_image=input_image,\n                                                                         rgbs=mv_imgs,\n                                                                         normals=mv_normals,\n                                                                         camera_type=args.camera_type,\n                                                                         scence_name=os.path.basename(image_file).split('.')[0],\n                                                                         crop_size=args.crop_size,\n                                                                         output_path=args.output_path)\n        vertices_init_list.append(vertices)\n        faces_init_list.append(faces)\n        rendered_imgs.append(rendered_rgbs)\n        rendered_normals.append(rendered_normal)\n\n        # stage 4: MV-Enhancement and iterative refinement\n        mv_normals, mv_imgs = views_6to4(normals_mv_out[i]), views_6to4(imgs_mv_out[i])\n\n        for refine_idx in range(args.num_refine):\n\n            refined_vertices, refined_faces, rendered_refined_imgs, rendered_refined_normals = [], [], [], []\n\n            rendered_mv_normals, rendered_mv_imgs = rendered_normals.pop(0), rendered_imgs.pop(0)\n\n            normals_pred, images_pred = pred_enhancement_joint(mv_image=mv_imgs,\n                                                           mv_normlas=mv_normals,\n                                                           renderd_mv_image=rendered_mv_imgs,\n                                                           renderd_mv_normal=rendered_mv_normals,\n                                                           front_image=input_image,\n                                                           pipeline=enhancement_pipeline,\n                                                           seed=args.seed,\n                                                           crop_size=args.crop_size,\n                                                           camera_type=args.camera_type,\n                                                           cfg=config_controlnet,\n                                                           case_name=image_file,\n                                                           refine_idx=refine_idx,\n                                                           output_path=args.output_path)\n\n            vertices_init, faces_init = vertices_init_list.pop(0), faces_init_list.pop(0)\n\n            rendered_rgbs, rendered_normal, vertices, faces = iterative_refine(vertex_init=vertices_init,\n                                                                                face_init=faces_init,\n                                                                                front_image=input_image,\n                                                                                rgbs=images_pred,\n                                                                                normals=normals_pred,\n                                                                                camera_type=args.camera_type,\n                                                                                scence_name=os.path.basename(image_file).split('.')[0],\n                                                                                crop_size=args.crop_size,\n                                                                                output_path=args.output_path,\n                                                                                refine_idx=refine_idx,\n                                                                                do_sr=(refine_idx==(args.num_refine-1))\n                                                                                )\n            vertices_init_list.append(vertices)\n            faces_init_list.append(faces)\n            rendered_imgs.append(rendered_rgbs)\n            rendered_normals.append(rendered_normal)\n            torch.cuda.empty_cache()\n\n    except AssertionError as e:\n        print(f\"error for {image_file}\")\n\n"
        },
        {
          "name": "run_mv_enhancement.py",
          "type": "blob",
          "size": 8.4794921875,
          "content": "import os\nfrom typing import Dict, Optional, Tuple, List\nfrom PIL import Image\nimport numpy as np\nfrom dataclasses import dataclass\n\nimport torch\nimport torch.utils.checkpoint\nfrom transformers import CLIPImageProcessor, CLIPVisionModelWithProjection\n\nfrom diffusers import (\n    AutoencoderKL,\n    DDPMScheduler,\n    UNet2DConditionModel,\n    UniPCMultistepScheduler,\n    EulerAncestralDiscreteScheduler\n)\n\nfrom collections import defaultdict\nimport rembg\nfrom torchvision.utils import make_grid, save_image\n\nfrom MVControlNet.model.controlnet import ControlNetModel\nfrom MVControlNet.pipeline.pipeline_controlnet_img2img import StableDiffusionControlNetImg2ImgPipeline\n\nfrom MVControlNet.data.enhancement_dataset import InferenceImageDataset\nimport torchvision.transforms as transforms\n\nto_pil = transforms.ToPILImage()\n\nweight_dtype = torch.half\n\n@dataclass\nclass TestConfig:\n    pretrained_model_name_or_path: str\n    controlnet_model_name_or_path: str\n    revision: Optional[str]\n    validation_batch_size: int\n\n    pipe_validation_kwargs: Dict\n    validation_guidance_scales: List[float]\n    camera_embedding_lr_mult: float\n\n    num_views: int\n\n\ndef save_image(tensor, fp):\n    ndarr = tensor.mul(255).add_(0.5).clamp_(0, 255).permute(1, 2, 0).to(\"cpu\", torch.uint8).numpy()\n    # pdb.set_trace()\n    im = Image.fromarray(ndarr)\n    im.save(fp)\n    return ndarr\n\ndef load_image_encoder():\n    image_encoder = CLIPVisionModelWithProjection.from_pretrained(\n        \"h94/IP-Adapter\",\n        subfolder=\"models/image_encoder\",\n        torch_dtype=torch.float16,\n    )\n    return image_encoder\n\ndef save_image_numpy(ndarr, fp):\n    im = Image.fromarray(ndarr)\n    im.save(fp)\n\ndef unmake_grid(img_grid, nrow):\n    img_grid = img_grid[0]\n    _, H, W = img_grid.shape\n    num_images = nrow * (H // (W // nrow))\n    img_height = H // (num_images // nrow)\n    img_width = W // nrow\n\n    images = []\n    for i in range(0, H, img_height):\n        for j in range(0, W, img_width):\n            image = img_grid[:, i:i+img_height, j:j+img_width]\n            images.append(image)\n\n    return images\n\ndef load_controlnet_pipeline(cfg):\n    controlnet = ControlNetModel.from_pretrained(cfg.controlnet_model_name_or_path, low_cpu_mem_usage=False)\n\n    pipeline = StableDiffusionControlNetImg2ImgPipeline.from_pretrained(\n        cfg.pretrained_model_name_or_path,\n        controlnet=controlnet,\n        safety_checker=None,\n        torch_dtype=weight_dtype,\n    )\n    pipeline.scheduler = UniPCMultistepScheduler.from_config(pipeline.scheduler.config)\n    # load ip_adapter to pipeline\n    image_encoder = load_image_encoder().to('cuda:0')\n    pipeline.image_encoder = image_encoder\n    pipeline.load_ip_adapter(\"h94/IP-Adapter\", subfolder=\"models\", weight_name=\"ip-adapter_sd15.safetensors\")\n    pipeline.set_ip_adapter_scale(1.0)\n\n    if torch.cuda.is_available():\n        pipeline.to('cuda:0')\n    # sys.main_lock = threading.Lock()\n    return pipeline\n\nNEG_PROMPT = \"sketch, sculpture, hand drawing, outline, single color, NSFW, lowres, bad anatomy,bad hands, text, error, missing fingers, yellow sleeves, extra digit, fewer digits, cropped, worst quality, low quality, normal quality, jpeg artifacts, signature, watermark, username, blurry,(worst quality:1.4),(low quality:1.4)\"\n\ndef custom_collate(batch):\n\n    batch = [item for item in batch if item is not None]\n    if len(batch) == 0:\n        return None\n    return torch.utils.data.dataloader.default_collate(batch)\n\ndef pred_enhancement_joint(mv_image, mv_normlas, renderd_mv_image, renderd_mv_normal, front_image, pipeline, seed=42, crop_size=192, camera_type='ortho', cfg=None, case_name='img', refine_idx=0, output_path='outputs'):\n    VIEWS = ['front', 'right', 'back', 'left']\n    pipeline.set_progress_bar_config(disable=True)\n\n    if seed is None:\n        generator = None\n    else:\n        generator = torch.Generator(device=pipeline.device).manual_seed(seed)\n\n    # Get the  dataset\n    validation_dataset = InferenceImageDataset(\n        crop_size=crop_size,\n        mv_imgs=mv_image,\n        mv_normals=mv_normlas,\n        renderd_mv_imgs=renderd_mv_image,\n        renderd_mv_normals=renderd_mv_normal,\n        front_img=front_image,\n    )\n\n    # DataLoaders creation:\n    validation_dataloader = torch.utils.data.DataLoader(\n        validation_dataset, batch_size=cfg.validation_batch_size, shuffle=False, num_workers=cfg.dataloader_num_workers,\n        collate_fn=custom_collate\n    )\n\n    images_cond, images_gt, images_pred = [], [], defaultdict(list)\n    batch = next(iter(validation_dataloader))\n\n    # repeat  (2B, Nv, 3, H, W)\n    input_image, input_normal = batch['imgs_in'].to(dtype=weight_dtype), batch['normals_in'].to(\n        dtype=weight_dtype)\n    input_image_mv, input_normal_mv = batch['imgs_mv'].to(dtype=weight_dtype), batch['normals_mv'].to(\n        dtype=weight_dtype)\n\n    inputs = torch.cat([input_image, input_normal], dim=0)\n    inputs_mv = torch.cat([input_image_mv, input_normal_mv], dim=0)\n    images_cond.append(inputs)\n\n    controlnet_image = inputs_mv.to(device=pipeline.device)\n\n    text_color = batch['task_name_color']\n    text_normal = batch['task_name_normal']\n\n    validation_prompt = text_color + text_normal\n\n    input_images_front, input_normal_front = (batch['front_in_color'].to(device=pipeline.device),\n                                              batch['front_in_normal'].to(device=pipeline.device))\n\n    validation_image = torch.cat([input_images_front, input_normal_front], dim=0).to(device=pipeline.device)\n    np_validation_image = np.array(validation_image.to(\"cpu\"))\n\n    num_views = len(VIEWS)\n    with torch.autocast(\"cuda\"):\n        # B*Nv images\n        if refine_idx == 0:\n            strength = 0.2\n            controlnet_condition_scle = 0.7\n        else:\n            strength = 0.1\n            controlnet_condition_scle = 1.\n        for guidance_scale in cfg.validation_guidance_scales:\n            images = pipeline(\n                prompt=validation_prompt,\n                neg_prompt=[NEG_PROMPT] * controlnet_image.shape[0],\n                image=controlnet_image,\n                ip_adapter_image=np_validation_image,\n                control_image=controlnet_image,\n                num_inference_steps=50,\n                strength=strength,\n                height=1024,\n                width=1024,\n                generator=generator,\n                guidance_scale=guidance_scale,\n                output_type='pt',\n                controlnet_conditioning_scale=controlnet_condition_scle\n            ).images\n\n            bsz = images.shape[0] // 2\n            images_pred  = images[:bsz]\n            normals_pred = images[bsz:]\n\n            rm_normals_pil = []\n            colors_pil = []\n            for i in range(bsz):\n                scene = os.path.basename(case_name.split('.')[0])\n                scene_dir = os.path.join(output_path, scene, 'mv-enhancement-'+str(refine_idx), camera_type)\n                normal_dir = os.path.join(scene_dir, \"normals\")\n                color_dir = os.path.join(scene_dir, \"colors\")\n                masked_colors_dir = os.path.join(scene_dir, \"masked_colors\")\n                os.makedirs(normal_dir, exist_ok=True)\n                os.makedirs(masked_colors_dir, exist_ok=True)\n                os.makedirs(color_dir, exist_ok=True)\n                normals = unmake_grid(normals_pred, 2)\n                colors = unmake_grid(images_pred, 2)\n\n                rembg_session = rembg.new_session()\n                for j in range(num_views):\n                    view = VIEWS[j]\n                    idx = i * num_views + j\n\n                    normal = normals[idx]\n                    color = colors[idx]\n\n                    normal_filename = f\"normals_000_{view}.png\"\n                    rgb_filename = f\"rgb_000_{view}.png\"\n                    normal = save_image(normal, os.path.join(normal_dir, normal_filename))\n                    color = save_image(color, os.path.join(color_dir, rgb_filename))\n\n                    rm_normal = rembg.remove(normal, alpha_matting=True, session=rembg_session)\n\n                    save_image_numpy(rm_normal, os.path.join(scene_dir, normal_filename))\n\n                    rm_normals_pil.append(Image.fromarray(rm_normal))\n                    colors_pil.append(to_pil(color))\n\n            save_image(images_pred[0], os.path.join(scene_dir, f'color_grid_img.png'))\n            save_image(normals_pred[0], os.path.join(scene_dir, f'normal_grid_img.png'))\n            save_image(input_image[0], os.path.join(scene_dir, f'color_grid_img_cond.png'))\n            save_image(input_normal[0], os.path.join(scene_dir, f'normal_grid_img_cond.png'))\n            return rm_normals_pil, colors_pil"
        },
        {
          "name": "run_mv_prediction.py",
          "type": "blob",
          "size": 6.3828125,
          "content": "import os\nfrom typing import Dict, Optional, Tuple, List\nfrom PIL import Image\nimport cv2\nimport numpy as np\nfrom dataclasses import dataclass\n\nfrom collections import defaultdict\n\nimport torch\n\nimport torch.utils.checkpoint\nfrom mv_diffusion_30.models.unet_mv2d_condition import UNetMV2DConditionModel\n\nfrom mv_diffusion_30.data.single_image_dataset import SingleImageDataset as MVDiffusionDataset\n\nfrom mv_diffusion_30.pipelines.pipeline_mvdiffusion_image import MVDiffusionImagePipeline\n\nfrom einops import rearrange\nimport rembg\nfrom torchvision.utils import make_grid, save_image\n\nimport torchvision.transforms as transforms\n\nweight_dtype = torch.half\n\nVIEWS = ['front', 'front_right', 'right', 'back', 'left', 'front_left']\n\nto_pil = transforms.ToPILImage()\n@dataclass\nclass TestConfig:\n    pretrained_model_name_or_path: str\n    pretrained_unet_path: Optional[str]\n    revision: Optional[str]\n    validation_batch_size: int\n    dataloader_num_workers: int\n\n    local_rank: int\n\n    pipe_kwargs: Dict\n    pipe_validation_kwargs: Dict\n    unet_from_pretrained_kwargs: Dict\n    validation_guidance_scales: List[float]\n    validation_grid_nrow: int\n    camera_embedding_lr_mult: float\n\n    num_views: int\n    camera_embedding_type: str\n\n    pred_type: str  # joint, or ablation\n    load_task: bool\n\ndef save_image(tensor, fp):\n    ndarr = tensor.mul(255).add_(0.5).clamp_(0, 255).permute(1, 2, 0).to(\"cpu\", torch.uint8).numpy()\n    # pdb.set_trace()\n    im = Image.fromarray(ndarr)\n    im.save(fp)\n    return ndarr\n\n\ndef save_depth_numpy(depth, fp, alpha):\n    depth = depth.mul(0.4).mul(65535.).add_(0.5).to(\"cpu\", torch.float32).numpy().mean(0)\n    print(depth.min(), depth.max())\n\n    depth[alpha < 128] = 0\n\n    depth = depth.astype(np.uint16)\n\n    kernel = np.ones((3, 3), np.uint8)  # kernel for erode\n\n    # erode\n    depth = cv2.erode(depth, kernel, iterations=1)\n\n    im = Image.fromarray(depth)\n    im.save(fp)\n\n\ndef save_image_numpy(ndarr, fp):\n    im = Image.fromarray(ndarr)\n    im.save(fp)\n\n\ndef load_wonder3d_pipeline(cfg):\n    if cfg.pretrained_unet_path:\n        print(\"load pre-trained unet from \", cfg.pretrained_unet_path)\n        unet = UNetMV2DConditionModel.from_pretrained(cfg.pretrained_unet_path, revision=cfg.revision,\n                                                               **cfg.unet_from_pretrained_kwargs)\n\n    pipeline = MVDiffusionImagePipeline.from_pretrained(\n        cfg.pretrained_model_name_or_path,\n        torch_dtype=weight_dtype,\n        pred_type=cfg.pred_type,\n        safety_checker=None,\n        unet=unet\n    )\n\n    if torch.cuda.is_available():\n        pipeline.to('cuda:0')\n\n    return pipeline\n\n\ndef pred_multiview_joint(image, pipeline, seed=42, crop_size=192, camera_type='ortho', cfg=None, case_name='img', output_path='outputs'):\n\n    validation_dataset = MVDiffusionDataset(\n        single_image=image,\n        num_views=6,\n        bg_color='white',\n        img_wh=[256, 256],\n        crop_size=crop_size,\n        cam_types=[camera_type],\n        load_cam_type=True\n    )\n\n    validation_dataloader = torch.utils.data.DataLoader(\n        validation_dataset, batch_size=1, shuffle=False, num_workers=0\n    )\n\n    pipeline.set_progress_bar_config(disable=True)\n\n    generator = torch.Generator(device=pipeline.device).manual_seed(seed)\n\n    images_cond, normals_pred, images_pred = [], defaultdict(list), defaultdict(list)\n\n    batch = next(iter(validation_dataloader))\n\n    # repeat  (2B, Nv, 3, H, W)\n    imgs_in = torch.cat([batch['imgs_in']] * 2, dim=0)\n\n    filename = batch['filename']\n\n    # (2B, Nv, Nce)\n    camera_embeddings = torch.cat([batch['camera_embeddings']] * 2, dim=0)\n\n    task_embeddings = torch.cat([batch['normal_task_embeddings'], batch['color_task_embeddings']], dim=0)\n\n    camera_embeddings = torch.cat([camera_embeddings, task_embeddings], dim=-1)\n\n    # (B*Nv, 3, H, W)\n    imgs_in = rearrange(imgs_in, \"B Nv C H W -> (B Nv) C H W\").to(weight_dtype)\n    # (B*Nv, Nce)\n    camera_embeddings = rearrange(camera_embeddings, \"B Nv Nce -> (B Nv) Nce\").to(weight_dtype)\n\n    images_cond.append(imgs_in)\n    num_views = len(VIEWS)\n    with torch.autocast(\"cuda\"):\n        # B*Nv images\n        for guidance_scale in cfg.validation_guidance_scales:\n            out = pipeline(\n                imgs_in, camera_embeddings, generator=generator, guidance_scale=guidance_scale,\n                output_type='pt', num_images_per_prompt=1, **cfg.pipe_validation_kwargs\n            ).images\n\n            bsz = out.shape[0] // 2\n\n            normals_pred = out[:bsz]\n            images_pred = out[bsz:]\n            color_pred_grid = make_grid(images_pred, nrow=6, padding=0, value_range=(0, 1))\n            normal_pred_grid = make_grid(normals_pred, nrow=6, padding=0, value_range=(0, 1))\n\n            rm_normals = []\n            colors = []\n            for i in range(bsz // num_views):\n                scene = os.path.basename(case_name.split('.')[0])\n                scene_dir = os.path.join(output_path, scene, 'mv', batch['cam_type'][0])\n\n                normal_dir = os.path.join(scene_dir, \"normals\")\n                color_dir = os.path.join(scene_dir, \"colors\")\n                masked_colors_dir = os.path.join(scene_dir, \"masked_colors\")\n                os.makedirs(normal_dir, exist_ok=True)\n                os.makedirs(masked_colors_dir, exist_ok=True)\n                os.makedirs(color_dir, exist_ok=True)\n                print(scene, batch['cam_type'], scene_dir)\n                rembg_session = rembg.new_session()\n                for j in range(num_views):\n                    view = VIEWS[j]\n                    idx = i * num_views + j\n                    normal = normals_pred[idx]\n                    color = images_pred[idx]\n\n                    normal_filename = f\"normals_000_{view}.png\"\n                    rgb_filename = f\"rgb_000_{view}.png\"\n                    normal = save_image(normal, os.path.join(normal_dir, normal_filename))\n                    color = save_image(color, os.path.join(color_dir, rgb_filename))\n\n                    rm_normal = rembg.remove(normal, alpha_matting=True, session=rembg_session)\n                    rm_normals.append(Image.fromarray(rm_normal))\n                    colors.append(to_pil(color))\n\n                    save_image_numpy(rm_normal, os.path.join(scene_dir, normal_filename))\n\n            save_image(color_pred_grid, os.path.join(scene_dir, f'color_grid_img.png'))\n            save_image(normal_pred_grid, os.path.join(scene_dir, f'normal_grid_img.png'))\n\n            return rm_normals, colors"
        },
        {
          "name": "run_train_stage1.sh",
          "type": "blob",
          "size": 0.1962890625,
          "content": "export NCCL_DEBUG=INFO\nexport NCCL_SOCKET_IFNAME=eth0\nexport NCCL_IB_DISABLE=1\n\naccelerate launch --config_file 8gpu.yaml train_mvdiffusion_mixed.py --config configs/train/stage1-mixed-wo-switcher.yaml"
        },
        {
          "name": "run_train_stage2.sh",
          "type": "blob",
          "size": 0.255859375,
          "content": "export NCCL_P2P_DISABLE=1\nexport NCCL_IB_DISABLE=1\nexport NCCL_DEBUG=info\nexport NCCL_SOCKET_IFNAME=eth0\nexport NCCL_P2P_LEVEL=NVL\n\naccelerate launch --config_file 8gpu.yaml train_mvdiffusion_mixed.py --config configs/train/stage1-mixed-6views-image-normal.yaml\n"
        },
        {
          "name": "run_train_stage3.sh",
          "type": "blob",
          "size": 0.2626953125,
          "content": "export NCCL_P2P_DISABLE=1\nexport NCCL_IB_DISABLE=1\nexport NCCL_DEBUG=info\nexport NCCL_SOCKET_IFNAME=eth0\nexport NCCL_P2P_LEVEL=NVL\n\naccelerate launch --config_file 8gpu.yaml train_mvdiffusion_joint_stage3.py --config configs/train/stage3-joint-6views-image-normal.yaml\n"
        },
        {
          "name": "train_controlnet.py",
          "type": "blob",
          "size": 38.1240234375,
          "content": "from comet_ml import Experiment\nfrom comet_ml.integration.pytorch import log_model\n\nimport argparse\nimport contextlib\nimport gc\nimport logging\nimport math\nimport os\nimport random\nimport shutil\nfrom pathlib import Path\nfrom omegaconf import OmegaConf\n\nimport accelerate\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\nimport torch.utils.checkpoint\nimport transformers\nfrom accelerate import Accelerator\nfrom accelerate.logging import get_logger\nfrom accelerate.utils import ProjectConfiguration, set_seed\nfrom huggingface_hub import create_repo, upload_folder\nfrom packaging import version\nfrom PIL import Image\nfrom torchvision import transforms\nfrom tqdm.auto import tqdm\nfrom transformers import AutoTokenizer, PretrainedConfig\nfrom torchvision.transforms import ToTensor\nfrom torchvision.transforms import ToPILImage\n\n\nfrom torchvision.utils import make_grid, save_image\n\nimport diffusers\nfrom diffusers import (\n    AutoencoderKL,\n    # ControlNetModel,\n    DDPMScheduler,\n    UNet2DConditionModel,\n    UniPCMultistepScheduler,\n    EulerAncestralDiscreteScheduler\n)\n\nfrom diffusers.optimization import get_scheduler\nfrom diffusers.utils import check_min_version, is_wandb_available\n# from diffusers.utils.hub_utils import load_or_create_model_card, populate_model_card\nfrom diffusers.utils.import_utils import is_xformers_available\nfrom diffusers.utils.torch_utils import is_compiled_module\nfrom diffusers.utils import _get_model_file\nfrom diffusers.models.modeling_utils import load_state_dict\nfrom safetensors import safe_open\nfrom collections import defaultdict\nfrom transformers import CLIPImageProcessor\n\n\nfrom MVControlNet.model.controlnet import ControlNetModel\nfrom MVControlNet.pipeline.pipeline_controlnet_img2img import StableDiffusionControlNetImg2ImgPipeline\n\nfrom MVControlNet.data.grid_objaverse_data import grid_refine_dataset\n\nfrom dataclasses import dataclass\nfrom typing import Dict, Optional, Tuple, List, Union\n\n\nfrom dataclasses import dataclass\nfrom typing import Optional, Dict, List\nfrom transformers import CLIPVisionModelWithProjection\nimport torch.multiprocessing as mp\n\n\n\nto_pil = ToPILImage()\n\n@dataclass\nclass TrainingConfig:\n    # Validation configurations\n    train_dataset: Dict\n    validation_dataset: Dict\n    vis_dir: str\n    pred_type: str\n\n    # Model and paths\n    pretrained_model_name_or_path: str = \"runwayml/stable-diffusion-v1-5\"\n    controlnet_model_name_or_path: Optional[str] = None\n    revision: Optional[str] = None\n    variant: Optional[str] = None\n    tokenizer_name: Optional[str] = None\n    cache_dir: Optional[str] = None\n    resume_from_checkpoint: Optional[str] = None\n    output_dir: str = \"outputs-controlnet-v0\"\n\n    # Dataset configurations\n    root_dir_ortho: str = \"/mvfs/multiview-data/ortho-13views\"\n    root_dir_persp: str = \"/mvfs/workspace/data/multiview-renders/persp_13views/persp_13views\"\n    pred_ortho: bool = True\n    pred_persp: bool = False\n    object_list: str = \"./datalist/datalist.json\"\n    invalid_list: str = None\n    num_views: int = 4\n    groups_num: int = 1\n    bg_color: str = \"three_choices\"\n    img_wh: Tuple[int, int] = (256, 256)\n    validation: bool = False\n    num_validation_samples: int = 32\n    read_normal: bool = False\n    read_color: bool = True\n    pred_type: str = \"joint\"\n    load_cam_type: bool = True\n\n    # Training parameters\n    seed: int = 42\n    resolution: int = 1024\n    train_batch_size: int = 4\n    validation_batch_size: int = 1\n    validation_train_batch_size: int = 1\n    max_train_steps: Optional[int] = None\n    num_train_epochs: int = 1\n    gradient_accumulation_steps: int = 1\n    gradient_checkpointing: bool = False\n    learning_rate: float = 1e-5\n    scale_lr: bool = False\n    lr_scheduler: str = \"constant\"\n    lr_warmup_steps: int = 500\n    lr_num_cycles: int = 1\n    lr_power: float = 1.0\n    snr_gamma: Optional[float] = 5\n    use_8bit_adam: bool = False\n    allow_tf32: bool = False\n    dataloader_num_workers: int = 0\n    adam_beta1: float = 0.9\n    adam_beta2: float = 0.999\n    adam_weight_decay: float = 1e-2\n    adam_epsilon: float = 1e-8\n    max_grad_norm: float = 1.0\n    noise_offset: float = 0.1\n\n    # Logging and checkpointing\n    logging_dir: str = \"logs\"\n    report_to: str = \"tensorboard\"\n    checkpointing_steps: int = 500\n    checkpoints_total_limit: Optional[int] = None\n    mixed_precision: Optional[str] = None\n    enable_xformers_memory_efficient_attention: bool = False\n    set_grads_to_none: bool = False\n\n    # Validation\n    validation_steps: int = 100\n    validation_sanity_check: bool = False\n    tracker_project_name: str = \"train_controlnet\"\n    num_validation_images: int = 4\n\n\n\nif is_wandb_available():\n    import wandb\n\n# Will error if the minimal version of diffusers is not installed. Remove at your own risks.\ncheck_min_version(\"0.19.3\")\n\nlogger = get_logger(__name__)\n\nimages = []\nto_tensor = ToTensor()\n\n\ndef image_grid(imgs, rows, cols):\n    assert len(imgs) == rows * cols\n\n    w, h = imgs[0].size\n    grid = Image.new(\"RGB\", size=(cols * w, rows * h))\n\n    for i, img in enumerate(imgs):\n        grid.paste(img, box=(i % cols * w, i // cols * h))\n    return grid\n\ndef load_image_encoder():\n    image_encoder = CLIPVisionModelWithProjection.from_pretrained(\n        \"h94/IP-Adapter\",\n        subfolder=\"models/image_encoder\",\n        torch_dtype=torch.float16,\n    )\n    return image_encoder\n\ndef log_validation(\n    dataloader, vae, text_encoder, tokenizer, unet, controlnet, args, accelerator, weight_dtype, global_step, save_dir, cfg, is_final_validation=False\n):\n    logger.info(\"Running validation... \")\n\n    images_cond, images_gt, images_pred = [], [], defaultdict(list)\n    pipeline = StableDiffusionControlNetImg2ImgPipeline.from_pretrained(\n        args.pretrained_model_name_or_path,\n        controlnet=accelerator.unwrap_model(controlnet),\n        safety_checker=None,\n        torch_dtype=weight_dtype,\n    )\n    pipeline.to(device=accelerator.device)\n\n    # pipeline.scheduler = UniPCMultistepScheduler.from_config(pipeline.scheduler.config)\n    pipeline.set_progress_bar_config(disable=True)\n\n    # load ip_adapter to pipeline\n    image_encoder = load_image_encoder().to(accelerator.device)\n    pipeline.image_encoder = image_encoder\n    pipeline.load_ip_adapter(\"h94/IP-Adapter\", subfolder=\"models\", weight_name=\"ip-adapter_sd15.safetensors\")\n    pipeline.set_ip_adapter_scale(1.0)\n\n    if args.enable_xformers_memory_efficient_attention:\n        pipeline.enable_xformers_memory_efficient_attention()\n\n    if args.seed is None:\n        generator = None\n    else:\n        generator = torch.Generator(device=accelerator.device).manual_seed(args.seed)\n\n    image_logs = []\n    inference_ctx = contextlib.nullcontext() if is_final_validation else torch.autocast(\"cuda\")\n    infer_images = []\n\n\n    for i, batch in enumerate(dataloader):\n        print(i, save_dir, len(infer_images))\n\n        input_image, input_normal = batch['img_tensors_cond'].to(dtype=weight_dtype), batch['normal_tensors_cond'].to(\n            dtype=weight_dtype)\n        if cfg.pred_type == 'rgb_only':\n            inputs = input_image\n        else:\n            inputs = torch.cat([input_image, input_normal], dim=0)\n\n        controlnet_image = inputs.to(device=unet.device)\n\n        text_color = batch['task_name_color']\n        text_normal = batch['task_name_normal']\n\n        if cfg.pred_type == 'rgb_only':\n            texts = text_color\n        else:\n            texts = text_color + text_normal\n        validation_prompt = texts\n\n        images_cond.append(input_image)\n\n        images_gt.append(batch[\"img_tensors_out\"])\n\n\n        input_images_front, input_normal_front = (batch['front_in_color'].to(device=unet.device),\n                                                  batch['front_in_normal'].to(device=unet.device))\n\n        NEG_PROMPT = \"sketch, sculpture, hand drawing, outline, single color, NSFW, lowres, bad anatomy,bad hands, text, error, missing fingers, yellow sleeves, extra digit, fewer digits, cropped, worst quality, low quality, normal quality, jpeg artifacts, signature, watermark, username, blurry,(worst quality:1.4),(low quality:1.4)\"\n        if cfg.pred_type == 'rgb_only':\n            validation_image = input_images_front\n        else:\n            validation_image = torch.cat([input_images_front, input_normal_front], dim=0).to(device=unet.device)\n        np_validation_image = np.array(validation_image.to(\"cpu\"))\n        print(controlnet_image[0].shape)\n        with inference_ctx:\n            images = pipeline(\n                prompt=validation_prompt,\n                neg_prompt=[NEG_PROMPT]*controlnet_image.shape[0],\n                image=controlnet_image,\n                ip_adapter_image=np_validation_image,\n                control_image=controlnet_image,\n                num_inference_steps=50,\n                strength=0.2,\n                generator=generator,\n                guidance_scale=1.,\n                output_type='pt',\n            ).images\n\n            length = len(images)\n            for j in range(length):\n                # image = torch.from_numpy(np.array(images[j]).astype(np.float32) / 255.).permute(2, 0, 1)\n                image = images[j]\n                print(torch.min(image), torch.max(image))\n                infer_images.append(image)\n\n\n        # image_logs.append(\n        #     {\"validation_image\": validation_image, \"images\": images, \"validation_prompt\": validation_prompt}\n        # )\n\n    infer_images = torch.stack(infer_images, dim=0)\n    images_cond = torch.stack(images_cond, dim=0)\n    images_gt = torch.stack(images_gt, dim=0)\n\n    tracker_key = \"test\" if is_final_validation else \"validation\"\n    N, B, C, H, W = images_cond.shape\n    images_cond = images_cond.reshape(N*B, C, H, W)\n    N, B, C, H, W = images_gt.shape\n    images_gt = images_gt.reshape(N*B, C, H, W)\n    print(images_gt.shape, images_cond.shape, infer_images.shape)\n    img_infer = make_grid(infer_images, nrow=2, padding=0, value_range=(0, 1))\n    images_cond_grid = make_grid(images_cond, nrow=2, padding=0, value_range=(0, 1))\n    images_gt_grid = make_grid(images_gt, nrow=2, padding=0, value_range=(0, 1))\n\n    print(images_gt_grid.shape, images_cond_grid.shape, img_infer.shape)\n    save_image(images_cond_grid, os.path.join(save_dir, f\"{global_step}-validation-cond.jpg\"))\n    save_image(images_gt_grid, os.path.join(save_dir, f\"{global_step}-validation-gt.jpg\"))\n    save_image(img_infer, os.path.join(save_dir, f\"{global_step}-validation-infer.jpg\"))\n    to_pil(validation_image[0]).save(os.path.join(save_dir, f\"{global_step}-front-case.jpg\"))\n\n    for tracker in accelerator.trackers:\n        if tracker.name == \"tensorboard\":\n            for log in image_logs:\n                images = log[\"images\"]\n                validation_prompt = log[\"validation_prompt\"]\n                validation_image = log[\"validation_image\"]\n\n                formatted_images = []\n\n                formatted_images.append(np.asarray(validation_image))\n\n                for image in images:\n                    formatted_images.append(np.asarray(image))\n\n                formatted_images = np.stack(formatted_images)\n\n                tracker.writer.add_images(validation_prompt, formatted_images, global_step, dataformats=\"NHWC\")\n        elif tracker.name == \"wandb\":\n            formatted_images = []\n\n            for log in image_logs:\n                images = log[\"images\"]\n                validation_prompt = log[\"validation_prompt\"]\n                validation_image = log[\"validation_image\"]\n\n                formatted_images.append(wandb.Image(validation_image, caption=\"Controlnet conditioning\"))\n\n                for image in images:\n                    image = wandb.Image(image, caption=validation_prompt)\n                    formatted_images.append(image)\n\n            tracker.log({tracker_key: formatted_images})\n        else:\n            logger.warning(f\"image logging not implemented for {tracker.name}\")\n\n        gc.collect()\n        torch.cuda.empty_cache()\n\n        return image_logs\n\n\ndef import_model_class_from_model_name_or_path(pretrained_model_name_or_path: str, revision: str):\n    text_encoder_config = PretrainedConfig.from_pretrained(\n        pretrained_model_name_or_path,\n        subfolder=\"text_encoder\",\n        revision=revision,\n    )\n    model_class = text_encoder_config.architectures[0]\n\n    if model_class == \"CLIPTextModel\":\n        from transformers import CLIPTextModel\n\n        return CLIPTextModel\n    elif model_class == \"RobertaSeriesModelWithTransformation\":\n        from diffusers.pipelines.alt_diffusion.modeling_roberta_series import RobertaSeriesModelWithTransformation\n\n        return RobertaSeriesModelWithTransformation\n    else:\n        raise ValueError(f\"{model_class} is not supported.\")\n\ndef main(args):\n    if args.report_to == \"wandb\" and args.hub_token is not None:\n        raise ValueError(\n            \"You cannot use both --report_to=wandb and --hub_token due to a security risk of exposing your token.\"\n            \" Please use `huggingface-cli login` to authenticate with the Hub.\"\n        )\n\n    logging_dir = Path(args.output_dir, args.logging_dir)\n\n    accelerator_project_config = ProjectConfiguration(project_dir=args.output_dir, logging_dir=logging_dir)\n\n    accelerator = Accelerator(\n        gradient_accumulation_steps=args.gradient_accumulation_steps,\n        mixed_precision=args.mixed_precision,\n        log_with=args.report_to,\n        project_config=accelerator_project_config,\n    )\n\n    # Disable AMP for MPS.\n    if torch.backends.mps.is_available():\n        accelerator.native_amp = False\n\n    vis_dir = os.path.join(cfg.output_dir, cfg.vis_dir)\n    if accelerator.is_main_process:\n        os.makedirs(cfg.output_dir, exist_ok=True)\n        os.makedirs(vis_dir, exist_ok=True)\n        OmegaConf.save(cfg, os.path.join(cfg.output_dir, 'config.yaml'))\n\n    # Make one log on every process with the configuration for debugging.\n    logging.basicConfig(\n        format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n        datefmt=\"%m/%d/%Y %H:%M:%S\",\n        level=logging.INFO,\n    )\n    logger.info(accelerator.state, main_process_only=False)\n    if accelerator.is_local_main_process:\n        transformers.utils.logging.set_verbosity_warning()\n        diffusers.utils.logging.set_verbosity_info()\n    else:\n        transformers.utils.logging.set_verbosity_error()\n        diffusers.utils.logging.set_verbosity_error()\n\n    # If passed along, set the training seed now.\n    if args.seed is not None:\n        set_seed(args.seed)\n\n    # Handle the repository creation\n    if accelerator.is_main_process:\n        if args.output_dir is not None:\n            os.makedirs(args.output_dir, exist_ok=True)\n\n        # if args.push_to_hub:\n        #     repo_id = create_repo(\n        #         repo_id=args.hub_model_id or Path(args.output_dir).name, exist_ok=True, token=args.hub_token\n        #     ).repo_id\n\n    # Load the tokenizer\n    if args.tokenizer_name:\n        tokenizer = AutoTokenizer.from_pretrained(args.tokenizer_name, revision=args.revision, use_fast=False)\n    elif args.pretrained_model_name_or_path:\n        tokenizer = AutoTokenizer.from_pretrained(\n            args.pretrained_model_name_or_path,\n            subfolder=\"tokenizer\",\n            revision=args.revision,\n            use_fast=False,\n        )\n\n    # import correct text encoder class\n    text_encoder_cls = import_model_class_from_model_name_or_path(args.pretrained_model_name_or_path, args.revision)\n\n    # Load scheduler and models\n    noise_scheduler = DDPMScheduler.from_pretrained(args.pretrained_model_name_or_path, subfolder=\"scheduler\")\n    text_encoder = text_encoder_cls.from_pretrained(\n        args.pretrained_model_name_or_path, subfolder=\"text_encoder\", revision=args.revision, variant=args.variant\n    )\n    vae = AutoencoderKL.from_pretrained(\n        args.pretrained_model_name_or_path, subfolder=\"vae\", revision=args.revision, variant=args.variant\n    )\n    unet = UNet2DConditionModel.from_pretrained(\n        args.pretrained_model_name_or_path, subfolder=\"unet\", revision=args.revision, variant=args.variant\n    )\n\n    # controlnet = ControlNetModel.from_pretrained(args.controlnet_model_name_or_path, low_cpu_mem_usage=False)\n    if args.controlnet_model_name_or_path:\n        logger.info(f\"Loading existing controlnet weights from {args.controlnet_model_name_or_path}\")\n        controlnet = ControlNetModel.from_pretrained(args.controlnet_model_name_or_path, low_cpu_mem_usage=False)\n        print()\n    else:\n        logger.info(\"Initializing controlnet weights from unet\")\n        controlnet = ControlNetModel.from_unet(unet)\n\n    # Taken from [Sayak Paul's Diffusers PR #6511](https://github.com/huggingface/diffusers/pull/6511/files)\n    def unwrap_model(model):\n        model = accelerator.unwrap_model(model)\n        model = model._orig_mod if is_compiled_module(model) else model\n        return model\n\n    # `accelerate` 0.16.0 will have better support for customized saving\n    if version.parse(accelerate.__version__) >= version.parse(\"0.16.0\"):\n        # create custom saving & loading hooks so that `accelerator.save_state(...)` serializes in a nice format\n        def save_model_hook(models, weights, output_dir):\n            if accelerator.is_main_process:\n                i = len(weights) - 1\n\n                while len(weights) > 0:\n                    weights.pop()\n                    model = models[i]\n\n                    sub_dir = \"controlnet\"\n                    model.save_pretrained(os.path.join(output_dir, sub_dir))\n\n                    i -= 1\n\n        def load_model_hook(models, input_dir):\n            while len(models) > 0:\n                # pop models so that they are not loaded again\n                model = models.pop()\n\n                # load diffusers style into model\n                load_model = ControlNetModel.from_pretrained(input_dir, subfolder=\"controlnet\")\n                model.register_to_config(**load_model.config)\n\n                model.load_state_dict(load_model.state_dict())\n                del load_model\n\n        accelerator.register_save_state_pre_hook(save_model_hook)\n        accelerator.register_load_state_pre_hook(load_model_hook)\n\n    vae.requires_grad_(False)\n    unet.requires_grad_(False)\n    text_encoder.requires_grad_(False)\n    controlnet.train()\n\n\n    if args.enable_xformers_memory_efficient_attention:\n        if is_xformers_available():\n            import xformers\n\n            xformers_version = version.parse(xformers.__version__)\n            if xformers_version == version.parse(\"0.0.16\"):\n                logger.warning(\n                    \"xFormers 0.0.16 cannot be used for training in some GPUs. If you observe problems during training, please update xFormers to at least 0.0.17. See https://huggingface.co/docs/diffusers/main/en/optimization/xformers for more details.\"\n                )\n            unet.enable_xformers_memory_efficient_attention()\n            controlnet.enable_xformers_memory_efficient_attention()\n        else:\n            raise ValueError(\"xformers is not available. Make sure it is installed correctly\")\n\n    if args.gradient_checkpointing:\n        controlnet.enable_gradient_checkpointing()\n\n    # Check that all trainable models are in full precision\n    low_precision_error_string = (\n        \" Please make sure to always have all model weights in full float32 precision when starting training - even if\"\n        \" doing mixed precision training, copy of the weights should still be float32.\"\n    )\n\n    if unwrap_model(controlnet).dtype != torch.float32:\n        raise ValueError(\n            f\"Controlnet loaded as datatype {unwrap_model(controlnet).dtype}. {low_precision_error_string}\"\n        )\n\n    # Enable TF32 for faster training on Ampere GPUs,\n    # cf https://pytorch.org/docs/stable/notes/cuda.html#tensorfloat-32-tf32-on-ampere-devices\n    if args.allow_tf32:\n        torch.backends.cuda.matmul.allow_tf32 = True\n\n    if args.scale_lr:\n        args.learning_rate = (\n            args.learning_rate * args.gradient_accumulation_steps * args.train_batch_size * accelerator.num_processes\n        )\n\n    # Use 8-bit Adam for lower memory usage or to fine-tune the model in 16GB GPUs\n    if args.use_8bit_adam:\n        try:\n            import bitsandbytes as bnb\n        except ImportError:\n            raise ImportError(\n                \"To use 8-bit Adam, please install the bitsandbytes library: `pip install bitsandbytes`.\"\n            )\n\n        optimizer_class = bnb.optim.AdamW8bit\n    else:\n        optimizer_class = torch.optim.AdamW\n\n    # Optimizer creation\n    params_to_optimize = controlnet.parameters()\n    optimizer = optimizer_class(\n        params_to_optimize,\n        lr=args.learning_rate,\n        betas=(args.adam_beta1, args.adam_beta2),\n        weight_decay=args.adam_weight_decay,\n        eps=args.adam_epsilon,\n    )\n\n    train_dataset = grid_refine_dataset(\n        **cfg.train_dataset\n    )\n\n    train_dataloader = torch.utils.data.DataLoader(\n        train_dataset, batch_size=cfg.train_batch_size, shuffle=True,\n        num_workers=cfg.dataloader_num_workers, prefetch_factor = 4\n    )\n\n    validation_dataset = grid_refine_dataset(\n        **cfg.validation_dataset\n    )\n\n    validation_dataloader = torch.utils.data.DataLoader(\n        validation_dataset, batch_size=cfg.validation_batch_size,\n        shuffle=False, num_workers=cfg.dataloader_num_workers,\n    )\n\n    # Scheduler and math around the number of training steps.\n    overrode_max_train_steps = False\n    num_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)\n    if args.max_train_steps is None:\n        args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch\n        overrode_max_train_steps = True\n\n    lr_scheduler = get_scheduler(\n        args.lr_scheduler,\n        optimizer=optimizer,\n        num_warmup_steps=args.lr_warmup_steps * accelerator.num_processes,\n        num_training_steps=args.max_train_steps * accelerator.num_processes,\n        num_cycles=args.lr_num_cycles,\n        power=args.lr_power,\n    )\n\n    # Prepare everything with our `accelerator`.\n    controlnet, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(\n        controlnet, optimizer, train_dataloader, lr_scheduler\n    )\n\n    # For mixed precision training we cast the text_encoder and vae weights to half-precision\n    # as these models are only used for inference, keeping weights in full precision is not required.\n    weight_dtype = torch.float16\n    # accelerator.mixed_precision = args.mixed_precision\n    if accelerator.mixed_precision == \"fp16\":\n        weight_dtype = torch.float16\n    elif accelerator.mixed_precision == \"bf16\":\n        weight_dtype = torch.bfloat16\n\n    # Move vae, unet and text_encoder to device and cast to weight_dtype\n    vae.to(accelerator.device, dtype=weight_dtype)\n    unet.to(accelerator.device, dtype=weight_dtype)\n    text_encoder.to(accelerator.device, dtype=weight_dtype)\n\n    pipeline = StableDiffusionControlNetImg2ImgPipeline.from_pretrained(\n        args.pretrained_model_name_or_path,\n        vae=vae,\n        text_encoder=text_encoder,\n        tokenizer=tokenizer,\n        unet=unet,\n        controlnet=controlnet,\n        safety_checker=None,\n        revision=args.revision,\n        variant=args.variant,\n        torch_dtype=weight_dtype,\n    )\n\n    # load ip_adapter for pipeline\n    image_encoder = load_image_encoder().to(accelerator.device)\n    pipeline.image_encoder = image_encoder\n    pipeline.load_ip_adapter(\"h94/IP-Adapter\", subfolder=\"models\", weight_name=\"ip-adapter_sd15.safetensors\")\n\n    # We need to recalculate our total training steps as the size of the training dataloader may have changed.\n    num_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)\n    if overrode_max_train_steps:\n        args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch\n    # Afterwards we recalculate our number of training epochs\n    args.num_train_epochs = math.ceil(args.max_train_steps / num_update_steps_per_epoch)\n\n    # Train!\n    total_batch_size = args.train_batch_size * accelerator.num_processes * args.gradient_accumulation_steps\n\n    logger.info(\"***** Running training *****\")\n    logger.info(f\"  Num examples = {len(train_dataset)}\")\n    logger.info(f\"  Num batches each epoch = {len(train_dataloader)}\")\n    logger.info(f\"  Num Epochs = {args.num_train_epochs}\")\n    logger.info(f\"  Instantaneous batch size per device = {args.train_batch_size}\")\n    logger.info(f\"  Total train batch size (w. parallel, distributed & accumulation) = {total_batch_size}\")\n    logger.info(f\"  Gradient Accumulation steps = {args.gradient_accumulation_steps}\")\n    logger.info(f\"  Total optimization steps = {args.max_train_steps}\")\n    global_step = 0\n    first_epoch = 0\n\n    def compute_snr(timesteps):\n        \"\"\"\n        Computes SNR as per https://github.com/TiankaiHang/Min-SNR-Diffusion-Training/blob/521b624bd70c67cee4bdf49225915f5945a872e3/guided_diffusion/gaussian_diffusion.py#L847-L849\n        \"\"\"\n        alphas_cumprod = noise_scheduler.alphas_cumprod\n        sqrt_alphas_cumprod = alphas_cumprod ** 0.5\n        sqrt_one_minus_alphas_cumprod = (1.0 - alphas_cumprod) ** 0.5\n\n        # Expand the tensors.\n        # Adapted from https://github.com/TiankaiHang/Min-SNR-Diffusion-Training/blob/521b624bd70c67cee4bdf49225915f5945a872e3/guided_diffusion/gaussian_diffusion.py#L1026\n        sqrt_alphas_cumprod = sqrt_alphas_cumprod.to(device=timesteps.device)[timesteps].float()\n        while len(sqrt_alphas_cumprod.shape) < len(timesteps.shape):\n            sqrt_alphas_cumprod = sqrt_alphas_cumprod[..., None]\n        alpha = sqrt_alphas_cumprod.expand(timesteps.shape)\n\n        sqrt_one_minus_alphas_cumprod = sqrt_one_minus_alphas_cumprod.to(device=timesteps.device)[timesteps].float()\n        while len(sqrt_one_minus_alphas_cumprod.shape) < len(timesteps.shape):\n            sqrt_one_minus_alphas_cumprod = sqrt_one_minus_alphas_cumprod[..., None]\n        sigma = sqrt_one_minus_alphas_cumprod.expand(timesteps.shape)\n\n        # Compute SNR.\n        snr = (alpha / sigma) ** 2\n        return snr\n\n    # Potentially load in the weights and states from a previous save\n    if args.resume_from_checkpoint:\n        if args.resume_from_checkpoint != \"latest\":\n            path = os.path.basename(args.resume_from_checkpoint)\n        else:\n            # Get the most recent checkpoint\n            dirs = os.listdir(args.output_dir)\n            dirs = [d for d in dirs if d.startswith(\"checkpoint\")]\n            dirs = sorted(dirs, key=lambda x: int(x.split(\"-\")[1]))\n            path = dirs[-1] if len(dirs) > 0 else None\n\n        if path is None:\n            accelerator.print(\n                f\"Checkpoint '{args.resume_from_checkpoint}' does not exist. Starting a new training run.\"\n            )\n            args.resume_from_checkpoint = None\n            initial_global_step = 0\n        else:\n            accelerator.print(f\"Resuming from checkpoint {path}\")\n            accelerator.load_state(os.path.join(args.output_dir, path))\n            global_step = int(path.split(\"-\")[1])\n\n            initial_global_step = global_step\n            first_epoch = global_step // num_update_steps_per_epoch\n    else:\n        initial_global_step = 0\n\n    progress_bar = tqdm(\n        range(0, args.max_train_steps),\n        initial=initial_global_step,\n        desc=\"Steps\",\n        # Only show the progress bar once on each machine.\n        disable=not accelerator.is_local_main_process,\n    )\n    if accelerator.is_main_process:\n        experiment = Experiment(\n            api_key=\"your_api_key\",\n            project_name=\"your_project_name\",\n            workspace=\"your_workspace\"\n        )\n\n    image_logs = None\n    for epoch in range(first_epoch, args.num_train_epochs):\n        for step, batch in enumerate(train_dataloader):\n            with accelerator.accumulate(controlnet):\n                # Convert images to latent space\n                latents = vae.encode((batch[\"img_tensors_out\"] * 2.0 - 1.0).to(dtype=weight_dtype)).latent_dist.sample()\n                latents = latents * vae.config.scaling_factor\n                latents_normal = vae.encode((batch[\"normal_tensors_out\"] * 2.0 - 1.0).to(dtype=weight_dtype)).latent_dist.sample()\n                latents_normal = latents_normal * vae.config.scaling_factor\n\n                if cfg.pred_type == 'rgb_only':\n                    latents = latents\n                else:\n                    latents = torch.cat([latents, latents_normal], dim=0)\n\n                input_image, input_normal = batch['img_tensors_cond'].to(dtype=weight_dtype), batch['normal_tensors_cond'].to(dtype=weight_dtype)\n\n                if cfg.pred_type == 'rgb_only':\n                    inputs = input_image\n                else:\n                    inputs = torch.cat([input_image, input_normal], dim=0)\n\n                input_images_front, input_normal_front = batch['front_in_color'].to(dtype=weight_dtype, device=latents.device), batch['front_in_normal'].to(dtype=weight_dtype, device=latents.device)\n                input_fronts = torch.cat([input_images_front, input_normal_front], dim=0)\n                input_fronts = np.array(input_fronts.to(\"cpu\"))\n                image_embeds = pipeline.prepare_ip_adapter_image_embeds(input_fronts,\n                                                                        device=accelerator.device,\n                                                                        num_images_per_prompt=1,\n                                                                        ip_adapter_image_embeds=None,\n                                                                        do_classifier_free_guidance=False)\n\n                # Add image embeds for IP-Adapter\n                added_cond_kwargs = (\n                    {\"image_embeds\": image_embeds}\n                )\n\n                # Sample noise that we'll add to the latents\n                noise = torch.randn_like(latents)\n                # if cfg.noise_offset:\n                #     # https://www.crosslabs.org//blog/diffusion-with-offset-noise\n                #     noise += args.noise_offset * torch.randn(\n                #         (latents.shape[0], latents.shape[1], 1, 1), device=latents.device\n                #     )\n                bsz = latents.shape[0]\n                # Sample a random timestep for each image\n                timesteps = torch.randint(0, noise_scheduler.config.num_train_timesteps, (bsz,), device=latents.device)\n                timesteps = timesteps.long()\n\n                # Add noise to the latents according to the noise magnitude at each timestep\n                # (this is the forward diffusion process)\n                noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps).to(dtype=weight_dtype, device=latents.device)\n\n                # Get the text embedding for conditioning\n                text_color = batch['task_name_color']\n                text_normal = batch['task_name_normal']\n                if cfg.pred_type == 'rgb_only':\n                    texts = text_color\n                else:\n                    texts = text_color + text_normal\n\n                prompt_embeds, _ = pipeline.encode_prompt(\n                    texts,\n                    accelerator.device,\n                    num_images_per_prompt=1,\n                    do_classifier_free_guidance=False,\n                    prompt_embeds=None,\n                )\n                encoder_hidden_states = prompt_embeds\n                controlnet_image = inputs.to(dtype=weight_dtype)\n\n                # print(noisy_latents.shape, controlnet_image.shape)\n                down_block_res_samples, mid_block_res_sample = controlnet(\n                    noisy_latents,\n                    timesteps,\n                    encoder_hidden_states=encoder_hidden_states,\n                    controlnet_cond=controlnet_image,\n                    return_dict=False,\n                )\n\n                # Predict the noise residual with ip_adapter\n                # print(f\"noisy_latents dtype: {noisy_latents.dtype}, encoder_hidden_states dtype: {encoder_hidden_states.dtype}, image_embeds dtype: {image_embeds.dtype}\", weight_dtype)\n                model_pred = unet(\n                    noisy_latents,\n                    timesteps,\n                    encoder_hidden_states=encoder_hidden_states,\n                    down_block_additional_residuals=[\n                        sample.to(dtype=weight_dtype) for sample in down_block_res_samples\n                    ],\n                    mid_block_additional_residual=mid_block_res_sample.to(dtype=weight_dtype),\n                    added_cond_kwargs=added_cond_kwargs,\n                    return_dict=False,\n                )[0]\n\n                # Get the target for loss depending on the prediction type\n                if noise_scheduler.config.prediction_type == \"epsilon\":\n                    target = noise\n                elif noise_scheduler.config.prediction_type == \"v_prediction\":\n                    target = noise_scheduler.get_velocity(latents, noise, timesteps)\n                else:\n                    raise ValueError(f\"Unknown prediction type {noise_scheduler.config.prediction_type}\")\n\n                if cfg.snr_gamma is None:\n                    loss = F.mse_loss(model_pred.float(), target.float(), reduction=\"mean\")\n                else:\n                    # Compute loss-weights as per Section 3.4 of https://arxiv.org/abs/2303.09556.\n                    # Since we predict the noise instead of x_0, the original formulation is slightly changed.\n                    # This is discussed in Section 4.2 of the same paper.\n                    snr = compute_snr(timesteps)\n                    mse_loss_weights = (\n                            torch.stack([snr, cfg.snr_gamma * torch.ones_like(timesteps)], dim=1).min(dim=1)[0] / snr\n                    )\n                    # We first calculate the original loss. Then we mean over the non-batch dimensions and\n                    # rebalance the sample-wise losses with their respective loss weights.\n                    # Finally, we take the mean of the rebalanced loss.\n                    loss = F.mse_loss(model_pred.float(), target.float(), reduction=\"none\")\n                    loss = loss.mean(dim=list(range(1, len(loss.shape)))) * mse_loss_weights\n                    loss = loss.mean()\n\n                accelerator.backward(loss)\n                if accelerator.sync_gradients:\n                    params_to_clip = controlnet.parameters()\n                    accelerator.clip_grad_norm_(params_to_clip, args.max_grad_norm)\n                optimizer.step()\n                lr_scheduler.step()\n                optimizer.zero_grad(set_to_none=args.set_grads_to_none)\n\n            # Checks if the accelerator has performed an optimization step behind the scenes\n            if accelerator.sync_gradients:\n                progress_bar.update(1)\n                global_step += 1\n\n                if accelerator.is_main_process:\n                    if global_step % args.checkpointing_steps == 0:\n                        # _before_ saving state, check if this save would set us over the `checkpoints_total_limit`\n                        if args.checkpoints_total_limit is not None:\n                            checkpoints = os.listdir(args.output_dir)\n                            checkpoints = [d for d in checkpoints if d.startswith(\"checkpoint\")]\n                            checkpoints = sorted(checkpoints, key=lambda x: int(x.split(\"-\")[1]))\n\n                            # before we save the new checkpoint, we need to have at _most_ `checkpoints_total_limit - 1` checkpoints\n                            if len(checkpoints) >= args.checkpoints_total_limit:\n                                num_to_remove = len(checkpoints) - args.checkpoints_total_limit + 1\n                                removing_checkpoints = checkpoints[0:num_to_remove]\n\n                                logger.info(\n                                    f\"{len(checkpoints)} checkpoints already exist, removing {len(removing_checkpoints)} checkpoints\"\n                                )\n                                logger.info(f\"removing checkpoints: {', '.join(removing_checkpoints)}\")\n\n                                for removing_checkpoint in removing_checkpoints:\n                                    removing_checkpoint = os.path.join(args.output_dir, removing_checkpoint)\n                                    shutil.rmtree(removing_checkpoint)\n\n                        save_path = os.path.join(args.output_dir, f\"checkpoint-{global_step}\")\n                        accelerator.save_state(save_path)\n                        logger.info(f\"Saved state to {save_path}\")\n\n                    if global_step % args.validation_steps == 0 or (cfg.validation_sanity_check and global_step == 6):\n                        log_validation(\n                            validation_dataloader,\n                            vae,\n                            text_encoder,\n                            tokenizer,\n                            unet,\n                            controlnet,\n                            args,\n                            accelerator,\n                            weight_dtype,\n                            global_step,\n                            vis_dir,\n                            cfg\n                        )\n                        unet.to(dtype=weight_dtype)\n\n            logs = {\"loss\": loss.detach().item(), \"lr\": lr_scheduler.get_last_lr()[0]}\n            progress_bar.set_postfix(**logs)\n            accelerator.log(logs, step=global_step)\n\n            if global_step >= args.max_train_steps:\n                break\n\n    # Create the pipeline using the trained modules and save it.\n    accelerator.wait_for_everyone()\n    if accelerator.is_main_process:\n        controlnet = unwrap_model(controlnet)\n        controlnet.save_pretrained(args.output_dir)\n\n        # Run a final round of validation.\n        image_logs = None\n        if args.validation_prompt is not None:\n            log_validation(\n                validation_dataloader,\n                vae,\n                text_encoder,\n                tokenizer,\n                unet,\n                controlnet,\n                args,\n                accelerator,\n                weight_dtype,\n                global_step,\n            )\n\n    accelerator.end_training()\n    if accelerator.is_main_process:\n        hyper_params = {\n            \"learning_rate\": 0.0001,\n            \"steps\": 60000,\n            \"batch_size\": 256\n        }\n        experiment.log_parameters(hyper_params)\n        log_model(experiment, model=unet, model_name=\"mv_controlnet_sep\")\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--config', type=str, required=True)\n    args = parser.parse_args()\n    schema = OmegaConf.structured(TrainingConfig)\n    cfg = OmegaConf.load(args.config)\n    cfg = OmegaConf.merge(schema, cfg)\n    main(cfg)"
        },
        {
          "name": "train_controlnet.sh",
          "type": "blob",
          "size": 0.2548828125,
          "content": "export NCCL_P2P_DISABLE=1\nexport NCCL_IB_DISABLE=1\nexport NCCL_DEBUG=info\nexport NCCL_SOCKET_IFNAME=eth0\nexport NCCL_P2P_LEVEL=NVL\n\naccelerate launch --config_file 8gpu.yaml train_controlnet.py \\\n --config configs/train-controlnet/mv_controlnet_train_joint.yaml"
        },
        {
          "name": "train_mvdiffusion_joint.py",
          "type": "blob",
          "size": 36.2470703125,
          "content": "import argparse\nimport datetime\nimport logging\nimport inspect\nimport math\nimport os\nfrom typing import Dict, Optional, Tuple, List\nfrom omegaconf import OmegaConf\nfrom PIL import Image\nimport cv2\nimport numpy as np\nfrom dataclasses import dataclass\nfrom packaging import version\nimport shutil\nfrom collections import defaultdict\n\nimport torch\nimport torch.nn.functional as F\nimport torch.utils.checkpoint\nimport torchvision.transforms.functional as TF\nfrom torchvision.transforms import InterpolationMode\nfrom torchvision.utils import make_grid, save_image\n\nimport transformers\nimport accelerate\nfrom accelerate import Accelerator\nfrom accelerate.logging import get_logger\nfrom accelerate.utils import ProjectConfiguration, set_seed\n\nimport diffusers\nfrom diffusers import AutoencoderKL, DDPMScheduler, DDIMScheduler, StableDiffusionPipeline\nfrom diffusers.optimization import get_scheduler\nfrom diffusers.training_utils import EMAModel\n\nfrom tqdm.auto import tqdm\nfrom transformers import CLIPImageProcessor, CLIPVisionModelWithProjection\n\nfrom mv_diffusion_30.models.unet_mv2d_condition import UNetMV2DConditionModel\n\nfrom mv_diffusion_30.data.objaverse_dataset import ObjaverseDataset as MVDiffusionDataset\n\nfrom mv_diffusion_30.pipelines.pipeline_mvdiffusion_image import MVDiffusionImagePipeline\n\nfrom einops import rearrange\n\nlogger = get_logger(__name__, log_level=\"INFO\")\n\n\n@dataclass\nclass TrainingConfig:\n    pretrained_model_name_or_path: str\n    pretrained_unet_path: Optional[str]\n    revision: Optional[str]\n    train_dataset: Dict\n    validation_dataset: Dict\n    validation_train_dataset: Dict\n    output_dir: str\n    seed: Optional[int]\n    train_batch_size: int\n    validation_batch_size: int\n    validation_train_batch_size: int\n    max_train_steps: int\n    gradient_accumulation_steps: int\n    gradient_checkpointing: bool\n    learning_rate: float\n    scale_lr: bool\n    lr_scheduler: str\n    lr_warmup_steps: int\n    snr_gamma: Optional[float]\n    use_8bit_adam: bool\n    allow_tf32: bool\n    use_ema: bool\n    dataloader_num_workers: int\n    adam_beta1: float\n    adam_beta2: float\n    adam_weight_decay: float\n    adam_epsilon: float\n    max_grad_norm: Optional[float]\n    prediction_type: Optional[str]\n    logging_dir: str\n    vis_dir: str\n    mixed_precision: Optional[str]\n    report_to: Optional[str]\n    local_rank: int\n    checkpointing_steps: int\n    checkpoints_total_limit: Optional[int]\n    resume_from_checkpoint: Optional[str]\n    enable_xformers_memory_efficient_attention: bool\n    validation_steps: int\n    validation_sanity_check: bool\n    tracker_project_name: str\n\n    trainable_modules: Optional[list]\n    use_classifier_free_guidance: bool\n    condition_drop_rate: float\n    scale_input_latents: bool\n\n    pipe_kwargs: Dict\n    pipe_validation_kwargs: Dict\n    unet_from_pretrained_kwargs: Dict\n    validation_guidance_scales: List[float]\n    validation_grid_nrow: int\n    camera_embedding_lr_mult: float\n\n    num_views: int\n    camera_embedding_type: str\n\n    pred_type: str\n\n    drop_type: str\n\n    last_global_step: int\n\n\ndef log_validation(dataloader, vae, feature_extractor, image_encoder, unet, cfg: TrainingConfig, accelerator, weight_dtype, global_step, name, save_dir):\n    logger.info(f\"Running {name} ... \")\n\n    pipeline = MVDiffusionImagePipeline(\n        image_encoder=image_encoder, feature_extractor=feature_extractor, vae=vae, unet=accelerator.unwrap_model(unet), safety_checker=None,\n        scheduler=DDIMScheduler.from_pretrained(cfg.pretrained_model_name_or_path, subfolder=\"scheduler\"),\n        **cfg.pipe_kwargs\n    )\n\n    pipeline.set_progress_bar_config(disable=True)\n\n    if cfg.enable_xformers_memory_efficient_attention:\n        pipeline.enable_xformers_memory_efficient_attention()    \n\n    if cfg.seed is None:\n        generator = None\n    else:\n        generator = torch.Generator(device=accelerator.device).manual_seed(cfg.seed)\n    \n    images_cond, images_gt, images_pred = [], [], defaultdict(list)\n    for i, batch in enumerate(dataloader):\n        # (B, Nv, 3, H, W)\n        imgs_in, colors_out, normals_out = batch['imgs_in'], batch['imgs_out'], batch['normals_out']\n        \n        # repeat  (2B, Nv, 3, H, W)\n        imgs_in = torch.cat([imgs_in]*2, dim=0)\n        imgs_out = torch.cat([normals_out, colors_out], dim=0)\n        \n        # (2B, Nv, Nce)\n        camera_embeddings = torch.cat([batch['camera_embeddings']]*2, dim=0)\n\n        task_embeddings = torch.cat([batch['normal_task_embeddings'], batch['color_task_embeddings']], dim=0)\n\n        camera_task_embeddings = torch.cat([camera_embeddings, task_embeddings], dim=-1)\n\n        # (B*Nv, 3, H, W)\n        imgs_in, imgs_out = rearrange(imgs_in, \"B Nv C H W -> (B Nv) C H W\"), rearrange(imgs_out, \"B Nv C H W -> (B Nv) C H W\")\n        # (B*Nv, Nce)\n        camera_task_embeddings = rearrange(camera_task_embeddings, \"B Nv Nce -> (B Nv) Nce\")\n\n        images_cond.append(imgs_in)\n        images_gt.append(imgs_out)\n        with torch.autocast(\"cuda\"):\n            # B*Nv images\n            for guidance_scale in cfg.validation_guidance_scales:\n                out = pipeline(\n                    imgs_in, camera_task_embeddings, generator=generator, guidance_scale=guidance_scale, output_type='pt', num_images_per_prompt=1, \\\n                    height=imgs_in.shape[1], width=imgs_in.shape[2], **cfg.pipe_validation_kwargs\n                ).images\n                shape = out.shape\n                out0, out1 = out[:shape[0]//2], out[shape[0]//2:]\n                out = []\n                for ii in range(shape[0]//2):\n                    out.append(out0[ii])\n                    out.append(out1[ii])\n                out = torch.stack(out, dim=0)\n                images_pred[f\"{name}-sample_cfg{guidance_scale:.1f}\"].append(out)\n    images_cond_all = torch.cat(images_cond, dim=0)\n    images_gt_all = torch.cat(images_gt, dim=0)\n    images_pred_all = {}\n    for k, v in images_pred.items():\n        images_pred_all[k] = torch.cat(v, dim=0)\n    \n    nrow = cfg.validation_grid_nrow\n    ncol = images_cond_all.shape[0] // nrow\n    images_cond_grid = make_grid(images_cond_all, nrow=nrow, ncol=ncol, padding=0, value_range=(0, 1))\n    images_gt_grid = make_grid(images_gt_all, nrow=nrow, ncol=ncol, padding=0, value_range=(0, 1))\n    images_pred_grid = {}\n    for k, v in images_pred_all.items():\n        images_pred_grid[k] = make_grid(v, nrow=nrow, ncol=ncol, padding=0, value_range=(0, 1))\n    save_image(images_cond_grid, os.path.join(save_dir, f\"{global_step}-{name}-cond.jpg\"))\n    save_image(images_gt_grid, os.path.join(save_dir, f\"{global_step}-{name}-gt.jpg\"))\n    for k, v in images_pred_grid.items():\n        save_image(v, os.path.join(save_dir, f\"{global_step}-{k}.jpg\"))\n    torch.cuda.empty_cache()\n\n\ndef main(\n    cfg: TrainingConfig\n):\n    # override local_rank with envvar\n    env_local_rank = int(os.environ.get(\"LOCAL_RANK\", -1))\n    if env_local_rank != -1 and env_local_rank != cfg.local_rank:\n        cfg.local_rank = env_local_rank\n\n    vis_dir = os.path.join(cfg.output_dir, cfg.vis_dir)\n    logging_dir = os.path.join(cfg.output_dir, cfg.logging_dir)\n    accelerator_project_config = ProjectConfiguration(project_dir=cfg.output_dir, logging_dir=logging_dir)\n\n    accelerator = Accelerator(\n        gradient_accumulation_steps=cfg.gradient_accumulation_steps,\n        mixed_precision=cfg.mixed_precision,\n        log_with=cfg.report_to,\n        project_config=accelerator_project_config,\n    )\n\n    # Make one log on every process with the configuration for debugging.\n    logging.basicConfig(\n        format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n        datefmt=\"%m/%d/%Y %H:%M:%S\",\n        level=logging.INFO,\n    )\n    logger.info(accelerator.state, main_process_only=False)\n    if accelerator.is_local_main_process:\n        transformers.utils.logging.set_verbosity_warning()\n        diffusers.utils.logging.set_verbosity_info()\n    else:\n        transformers.utils.logging.set_verbosity_error()\n        diffusers.utils.logging.set_verbosity_error()\n\n    # If passed along, set the training seed now.\n    if cfg.seed is not None:\n        set_seed(cfg.seed)\n\n    generator = torch.Generator(device=accelerator.device).manual_seed(cfg.seed)\n\n    # Handle the repository creation\n    if accelerator.is_main_process:\n        os.makedirs(cfg.output_dir, exist_ok=True)\n        os.makedirs(vis_dir, exist_ok=True)\n        OmegaConf.save(cfg, os.path.join(cfg.output_dir, 'config.yaml'))\n\n    # Load scheduler, tokenizer and models.\n    noise_scheduler = DDPMScheduler.from_pretrained(cfg.pretrained_model_name_or_path, subfolder=\"scheduler\")\n    image_encoder = CLIPVisionModelWithProjection.from_pretrained(cfg.pretrained_model_name_or_path, subfolder=\"image_encoder\", revision=cfg.revision)\n    feature_extractor = CLIPImageProcessor.from_pretrained(cfg.pretrained_model_name_or_path, subfolder=\"feature_extractor\", revision=cfg.revision)\n    vae = AutoencoderKL.from_pretrained(cfg.pretrained_model_name_or_path, subfolder=\"vae\", revision=cfg.revision)\n    if cfg.pretrained_unet_path is None:\n        unet = UNetMV2DConditionModel.from_pretrained_2d(cfg.pretrained_model_name_or_path, subfolder=\"unet\", revision=cfg.revision, **cfg.unet_from_pretrained_kwargs)\n    else:\n        print(\"load pre-trained unet from \", cfg.pretrained_unet_path)\n        unet = UNetMV2DConditionModel.from_pretrained(cfg.pretrained_unet_path, subfolder=\"unet\", revision=cfg.revision, **cfg.unet_from_pretrained_kwargs)\n    if cfg.use_ema:\n        ema_unet = EMAModel(unet.parameters(), model_cls=UNetMV2DConditionModel, model_config=unet.config)\n\n    def compute_snr(timesteps):\n        \"\"\"\n        Computes SNR as per https://github.com/TiankaiHang/Min-SNR-Diffusion-Training/blob/521b624bd70c67cee4bdf49225915f5945a872e3/guided_diffusion/gaussian_diffusion.py#L847-L849\n        \"\"\"\n        alphas_cumprod = noise_scheduler.alphas_cumprod\n        sqrt_alphas_cumprod = alphas_cumprod**0.5\n        sqrt_one_minus_alphas_cumprod = (1.0 - alphas_cumprod) ** 0.5\n\n        # Expand the tensors.\n        # Adapted from https://github.com/TiankaiHang/Min-SNR-Diffusion-Training/blob/521b624bd70c67cee4bdf49225915f5945a872e3/guided_diffusion/gaussian_diffusion.py#L1026\n        sqrt_alphas_cumprod = sqrt_alphas_cumprod.to(device=timesteps.device)[timesteps].float()\n        while len(sqrt_alphas_cumprod.shape) < len(timesteps.shape):\n            sqrt_alphas_cumprod = sqrt_alphas_cumprod[..., None]\n        alpha = sqrt_alphas_cumprod.expand(timesteps.shape)\n\n        sqrt_one_minus_alphas_cumprod = sqrt_one_minus_alphas_cumprod.to(device=timesteps.device)[timesteps].float()\n        while len(sqrt_one_minus_alphas_cumprod.shape) < len(timesteps.shape):\n            sqrt_one_minus_alphas_cumprod = sqrt_one_minus_alphas_cumprod[..., None]\n        sigma = sqrt_one_minus_alphas_cumprod.expand(timesteps.shape)\n\n        # Compute SNR.\n        snr = (alpha / sigma) ** 2\n        return snr\n    \n    # Freeze vae and text_encoder\n    vae.requires_grad_(False)\n    image_encoder.requires_grad_(False)\n    \n    if cfg.trainable_modules is None:\n        unet.requires_grad_(True)\n    else:\n        unet.requires_grad_(False)\n        for name, module in unet.named_modules():\n            if name.endswith(tuple(cfg.trainable_modules)):\n                for params in module.parameters():\n                    # print(\"trainable: \", params)\n                    params.requires_grad = True                \n\n    # `accelerate` 0.16.0 will have better support for customized saving\n    if version.parse(accelerate.__version__) >= version.parse(\"0.16.0\"):\n        # create custom saving & loading hooks so that `accelerator.save_state(...)` serializes in a nice format\n        def save_model_hook(models, weights, output_dir):\n            if cfg.use_ema:\n                ema_unet.save_pretrained(os.path.join(output_dir, \"unet_ema\"))\n\n            for i, model in enumerate(models):\n                model.save_pretrained(os.path.join(output_dir, \"unet\"))\n\n                # make sure to pop weight so that corresponding model is not saved again\n                weights.pop()\n\n        def load_model_hook(models, input_dir):\n            if cfg.use_ema:\n                load_model = EMAModel.from_pretrained(os.path.join(input_dir, \"unet_ema\"), UNetMV2DConditionModel)\n                ema_unet.load_state_dict(load_model.state_dict())\n                ema_unet.to(accelerator.device)\n                del load_model\n\n            for i in range(len(models)):\n                # pop models so that they are not loaded again\n                model = models.pop()\n\n                # load diffusers style into model\n                load_model = UNetMV2DConditionModel.from_pretrained(input_dir, subfolder=\"unet\")\n                model.register_to_config(**load_model.config)\n\n                model.load_state_dict(load_model.state_dict())\n                del load_model\n\n        accelerator.register_save_state_pre_hook(save_model_hook)\n        accelerator.register_load_state_pre_hook(load_model_hook)\n\n    if cfg.gradient_checkpointing:\n        unet.enable_gradient_checkpointing()\n\n    # Enable TF32 for faster training on Ampere GPUs,\n    # cf https://pytorch.org/docs/stable/notes/cuda.html#tensorfloat-32-tf32-on-ampere-devices\n    if cfg.allow_tf32:\n        torch.backends.cuda.matmul.allow_tf32 = True        \n\n    if cfg.scale_lr:\n        cfg.learning_rate = (\n            cfg.learning_rate * cfg.gradient_accumulation_steps * cfg.train_batch_size * accelerator.num_processes\n        )\n\n    # Initialize the optimizer\n    if cfg.use_8bit_adam:\n        try:\n            import bitsandbytes as bnb\n        except ImportError:\n            raise ImportError(\n                \"Please install bitsandbytes to use 8-bit Adam. You can do so by running `pip install bitsandbytes`\"\n            )\n\n        optimizer_cls = bnb.optim.AdamW8bit\n    else:\n        optimizer_cls = torch.optim.AdamW\n\n    params, params_class_embedding = [], []\n    for name, param in unet.named_parameters():\n        if 'class_embedding' in name:\n            params_class_embedding.append(param)\n        else:\n            params.append(param)\n    optimizer = optimizer_cls(\n        [\n            {\"params\": params, \"lr\": cfg.learning_rate},\n            {\"params\": params_class_embedding, \"lr\": cfg.learning_rate * cfg.camera_embedding_lr_mult}\n        ],\n        betas=(cfg.adam_beta1, cfg.adam_beta2),\n        weight_decay=cfg.adam_weight_decay,\n        eps=cfg.adam_epsilon,\n    )\n\n    lr_scheduler = get_scheduler(\n        cfg.lr_scheduler,\n        optimizer=optimizer,\n        num_warmup_steps=cfg.lr_warmup_steps * accelerator.num_processes,\n        num_training_steps=cfg.max_train_steps * accelerator.num_processes,\n    )\n\n    # Get the training dataset\n    train_dataset = MVDiffusionDataset(\n        **cfg.train_dataset\n    )\n    cfg.validation_dataset.pred_ortho, cfg.validation_dataset.pred_persp = True, False\n    cfg.validation_train_dataset.pred_ortho, cfg.validation_train_dataset.pred_persp = True, False\n    validation_dataset_ortho = MVDiffusionDataset(\n        **cfg.validation_dataset\n    )\n    validation_train_dataset_ortho = MVDiffusionDataset(\n        **cfg.validation_train_dataset\n    )\n\n    cfg.validation_dataset.pred_ortho, cfg.validation_dataset.pred_persp = False, True\n    cfg.validation_train_dataset.pred_ortho, cfg.validation_train_dataset.pred_persp = False, True\n    validation_dataset_persp = MVDiffusionDataset(\n        **cfg.validation_dataset\n    )\n    validation_train_dataset_persp = MVDiffusionDataset(\n        **cfg.validation_train_dataset\n    )\n    # print(validation_dataset_ortho.pred_ortho, validation_dataset_ortho.pred_persp, validation_train_dataset_persp.pred_ortho, validation_train_dataset_persp.pred_persp, validation_dataset_persp.root_dir_persp)\n    # DataLoaders creation:\n    train_dataloader = torch.utils.data.DataLoader(\n        train_dataset, batch_size=cfg.train_batch_size, shuffle=True, num_workers=cfg.dataloader_num_workers,\n    )\n\n    validation_dataloader_ortho = torch.utils.data.DataLoader(\n        validation_dataset_ortho, batch_size=cfg.validation_batch_size, shuffle=False, num_workers=cfg.dataloader_num_workers\n    )\n    validation_dataloader_persp = torch.utils.data.DataLoader(\n        validation_dataset_persp, batch_size=cfg.validation_batch_size, shuffle=False, num_workers=cfg.dataloader_num_workers\n    )\n\n    validation_train_dataloader_ortho = torch.utils.data.DataLoader(\n        validation_train_dataset_ortho, batch_size=cfg.validation_train_batch_size, shuffle=False, num_workers=cfg.dataloader_num_workers\n    )\n    validation_train_dataloader_persp = torch.utils.data.DataLoader(\n        validation_train_dataset_persp, batch_size=cfg.validation_train_batch_size, shuffle=False, num_workers=cfg.dataloader_num_workers\n    )\n\n    # Prepare everything with our `accelerator`.\n    unet, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(\n        unet, optimizer, train_dataloader, lr_scheduler\n    )\n\n    if cfg.use_ema:\n        ema_unet.to(accelerator.device)\n\n    # For mixed precision training we cast all non-trainable weigths (vae, non-lora text_encoder and non-lora unet) to half-precision\n    # as these weights are only used for inference, keeping weights in full precision is not required.\n    weight_dtype = torch.float32\n    if accelerator.mixed_precision == \"fp16\":\n        weight_dtype = torch.float16\n        cfg.mixed_precision = accelerator.mixed_precision\n    elif accelerator.mixed_precision == \"bf16\":\n        weight_dtype = torch.bfloat16\n        cfg.mixed_precision = accelerator.mixed_precision\n\n    # Move text_encode and vae to gpu and cast to weight_dtype\n    image_encoder.to(accelerator.device, dtype=weight_dtype)\n    vae.to(accelerator.device, dtype=weight_dtype)\n\n    clip_image_mean = torch.as_tensor(feature_extractor.image_mean)[:,None,None].to(accelerator.device, dtype=torch.float32)\n    clip_image_std = torch.as_tensor(feature_extractor.image_std)[:,None,None].to(accelerator.device, dtype=torch.float32)\n\n    # We need to recalculate our total training steps as the size of the training dataloader may have changed.\n    num_update_steps_per_epoch = math.ceil(len(train_dataloader) / cfg.gradient_accumulation_steps)\n    num_train_epochs = math.ceil(cfg.max_train_steps / num_update_steps_per_epoch)\n\n    # We need to initialize the trackers we use, and also store our configuration.\n    # The trackers initializes automatically on the main process.\n    if accelerator.is_main_process:\n        # tracker_config = dict(vars(cfg))\n        tracker_config = {}\n        accelerator.init_trackers(cfg.tracker_project_name, tracker_config)    \n\n    # Train!\n    total_batch_size = cfg.train_batch_size * accelerator.num_processes * cfg.gradient_accumulation_steps\n\n    logger.info(\"***** Running training *****\")\n    logger.info(f\"  Num examples = {len(train_dataset)}\")\n    logger.info(f\"  Num Epochs = {num_train_epochs}\")\n    logger.info(f\"  Instantaneous batch size per device = {cfg.train_batch_size}\")\n    logger.info(f\"  Total train batch size (w. parallel, distributed & accumulation) = {total_batch_size}\")\n    logger.info(f\"  Gradient Accumulation steps = {cfg.gradient_accumulation_steps}\")\n    logger.info(f\"  Total optimization steps = {cfg.max_train_steps}\")\n    global_step = 0\n    first_epoch = 0\n\n\n    # Potentially load in the weights and states from a previous save\n    if cfg.resume_from_checkpoint:\n        if cfg.resume_from_checkpoint != \"latest\":\n            path = os.path.basename(cfg.resume_from_checkpoint)\n        else:\n            # Get the most recent checkpoint\n            if os.path.exists(os.path.join(cfg.output_dir, \"checkpoint\")):\n                path = \"checkpoint\"\n            else:\n                dirs = os.listdir(cfg.output_dir)\n                dirs = [d for d in dirs if d.startswith(\"checkpoint\")]\n                dirs = sorted(dirs, key=lambda x: int(x.split(\"-\")[1]))\n                path = dirs[-1] if len(dirs) > 0 else None\n\n        if path is None:\n            accelerator.print(\n                f\"Checkpoint '{cfg.resume_from_checkpoint}' does not exist. Starting a new training run.\"\n            )\n            cfg.resume_from_checkpoint = None\n        else:\n            accelerator.print(f\"Resuming from checkpoint {path}\")\n            accelerator.load_state(os.path.join(cfg.output_dir, path))\n            # global_step = int(path.split(\"-\")[1])\n            global_step = cfg.last_global_step\n\n            resume_global_step = global_step * cfg.gradient_accumulation_steps\n            first_epoch = global_step // num_update_steps_per_epoch\n            resume_step = resume_global_step % (num_update_steps_per_epoch * cfg.gradient_accumulation_steps)        \n\n    # Only show the progress bar once on each machine.\n    progress_bar = tqdm(range(global_step, cfg.max_train_steps), disable=not accelerator.is_local_main_process)\n    progress_bar.set_description(\"Steps\")\n\n    for epoch in range(first_epoch, num_train_epochs):\n        unet.train()\n        train_loss = 0.0\n        for step, batch in enumerate(train_dataloader):\n            # Skip steps until we reach the resumed step\n            if cfg.resume_from_checkpoint and epoch == first_epoch and step < resume_step:\n                if step % cfg.gradient_accumulation_steps == 0:\n                    progress_bar.update(1)\n                continue\n\n            with accelerator.accumulate(unet):\n                # (B, Nv, 3, H, W)\n                imgs_in, colors_out, normals_out = batch['imgs_in'], batch['imgs_out'], batch['normals_out']\n\n                bnm, Nv = imgs_in.shape[:2]\n                \n                # repeat  (2B, Nv, 3, H, W)\n                imgs_in = torch.cat([imgs_in]*2, dim=0)\n                imgs_out = torch.cat([normals_out, colors_out], dim=0)\n                \n                # (2B, Nv, Nce)\n                camera_embeddings = torch.cat([batch['camera_embeddings']]*2, dim=0)\n\n                task_embeddings = torch.cat([batch['normal_task_embeddings'], batch['color_task_embeddings']], dim=0)\n\n                camera_task_embeddings = torch.cat([camera_embeddings, task_embeddings], dim=-1)\n\n                # (B*Nv, 3, H, W)\n                imgs_in, imgs_out = rearrange(imgs_in, \"B Nv C H W -> (B Nv) C H W\"), rearrange(imgs_out, \"B Nv C H W -> (B Nv) C H W\")\n                # (B*Nv, Nce)\n                camera_task_embeddings = rearrange(camera_task_embeddings, \"B Nv Nce -> (B Nv) Nce\")\n                # (B*Nv, Nce')\n                if cfg.camera_embedding_type == 'e_de_da_sincos':\n                    camera_task_embeddings = torch.cat([\n                        torch.sin(camera_task_embeddings),\n                        torch.cos(camera_task_embeddings)\n                    ], dim=-1)\n                else:\n                    raise NotImplementedError\n\n                imgs_in, imgs_out, camera_task_embeddings = imgs_in.to(weight_dtype), imgs_out.to(weight_dtype), camera_task_embeddings.to(weight_dtype)\n\n                # (B*Nv, 4, Hl, Wl)\n                # pdb.set_trace()\n                cond_vae_embeddings = vae.encode(imgs_in * 2.0 - 1.0).latent_dist.mode()\n                if cfg.scale_input_latents:\n                    cond_vae_embeddings = cond_vae_embeddings * vae.config.scaling_factor\n                latents = vae.encode(imgs_out * 2.0 - 1.0).latent_dist.sample() * vae.config.scaling_factor\n\n                # DO NOT use this! Very slow!                \n                # imgs_in_pil = [TF.to_pil_image(img) for img in imgs_in]\n                # imgs_in_proc = feature_extractor(images=imgs_in_pil, return_tensors='pt').pixel_values.to(dtype=latents.dtype, device=latents.device)\n\n                imgs_in_proc = TF.resize(imgs_in, (feature_extractor.crop_size['height'], feature_extractor.crop_size['width']), interpolation=InterpolationMode.BICUBIC)\n                # do the normalization in float32 to preserve precision\n                imgs_in_proc = ((imgs_in_proc.float() - clip_image_mean) / clip_image_std).to(weight_dtype)        \n\n                # (B*Nv, 1, 768)\n                image_embeddings = image_encoder(imgs_in_proc).image_embeds.unsqueeze(1)\n\n                noise = torch.randn_like(latents)\n                bsz = latents.shape[0]\n\n                # same noise for different views of the same object\n                timesteps = torch.randint(0, noise_scheduler.num_train_timesteps, (bsz // cfg.num_views,), device=latents.device).repeat_interleave(cfg.num_views)\n                timesteps = timesteps.long()                \n\n                noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)\n\n                # Conditioning dropout to support classifier-free guidance during inference. For more details\n                # check out the section 3.2.1 of the original paper https://arxiv.org/abs/2211.09800.\n                if cfg.use_classifier_free_guidance and cfg.condition_drop_rate > 0.:\n                    # assert cfg.drop_type == 'drop_as_a_whole'\n                    if cfg.drop_type == 'drop_as_a_whole':\n                        # drop a group of normals and colors as a whole\n                        random_p = torch.rand(bnm, device=latents.device, generator=generator)\n                        \n                        # Sample masks for the conditioning images.\n                        image_mask_dtype = cond_vae_embeddings.dtype\n                        image_mask = 1 - (\n                            (random_p >= cfg.condition_drop_rate).to(image_mask_dtype)\n                            * (random_p < 3 * cfg.condition_drop_rate).to(image_mask_dtype)\n                        )\n                        image_mask = image_mask.reshape(bnm, 1, 1, 1, 1).repeat(1, Nv, 1, 1, 1)\n                        image_mask = rearrange(image_mask, \"B Nv C H W -> (B Nv) C H W\")\n                        image_mask = torch.cat([image_mask]*2, dim=0)\n                        # Final image conditioning.\n                        cond_vae_embeddings = image_mask * cond_vae_embeddings\n\n                        # Sample masks for the conditioning images.\n                        clip_mask_dtype = image_embeddings.dtype\n                        clip_mask = 1 - (\n                            (random_p < 2 * cfg.condition_drop_rate).to(clip_mask_dtype)\n                        )\n                        clip_mask = clip_mask.reshape(bnm, 1, 1, 1).repeat(1, Nv, 1, 1)\n                        clip_mask = rearrange(clip_mask, \"B Nv M C -> (B Nv) M C\")\n                        clip_mask = torch.cat([clip_mask]*2, dim=0)\n                        # Final image conditioning.\n                        image_embeddings = clip_mask * image_embeddings\n                    elif cfg.drop_type == 'drop_independent':\n                        # randomly drop all independently\n                        random_p = torch.rand(bsz, device=latents.device, generator=generator)\n\n                        # Sample masks for the conditioning images.\n                        image_mask_dtype = cond_vae_embeddings.dtype\n                        image_mask = 1 - (\n                            (random_p >= cfg.condition_drop_rate).to(image_mask_dtype)\n                            * (random_p < 3 * cfg.condition_drop_rate).to(image_mask_dtype)\n                        )\n                        image_mask = image_mask.reshape(bsz, 1, 1, 1)\n                        # Final image conditioning.\n                        cond_vae_embeddings = image_mask * cond_vae_embeddings\n\n                        # Sample masks for the conditioning images.\n                        clip_mask_dtype = image_embeddings.dtype\n                        clip_mask = 1 - (\n                            (random_p < 2 * cfg.condition_drop_rate).to(clip_mask_dtype)\n                        )\n                        clip_mask = clip_mask.reshape(bsz, 1, 1)\n                        # Final image conditioning.\n                        image_embeddings = clip_mask * image_embeddings\n                    elif cfg.drop_type == 'drop_joint':\n                        # randomly drop all independently\n                        random_p = torch.rand(bsz//2, device=latents.device, generator=generator)\n\n                        # Sample masks for the conditioning images.\n                        image_mask_dtype = cond_vae_embeddings.dtype\n                        image_mask = 1 - (\n                            (random_p >= cfg.condition_drop_rate).to(image_mask_dtype)\n                            * (random_p < 3 * cfg.condition_drop_rate).to(image_mask_dtype)\n                        )\n                        image_mask = torch.cat([image_mask]*2, dim=0)\n                        image_mask = image_mask.reshape(bsz, 1, 1, 1)\n                        # Final image conditioning.\n                        cond_vae_embeddings = image_mask * cond_vae_embeddings\n\n                        # Sample masks for the conditioning images.\n                        clip_mask_dtype = image_embeddings.dtype\n                        clip_mask = 1 - (\n                            (random_p < 2 * cfg.condition_drop_rate).to(clip_mask_dtype)\n                        )\n                        clip_mask = torch.cat([clip_mask]*2, dim=0)\n                        clip_mask = clip_mask.reshape(bsz, 1, 1)\n                        # Final image conditioning.\n                        image_embeddings = clip_mask * image_embeddings\n                \n                # (B*Nv, 8, Hl, Wl)\n                latent_model_input = torch.cat([noisy_latents, cond_vae_embeddings], dim=1)\n\n                model_pred = unet(\n                    latent_model_input,\n                    timesteps,\n                    encoder_hidden_states=image_embeddings,\n                    class_labels=camera_task_embeddings\n                ).sample\n\n                # Get the target for loss depending on the prediction type\n                if noise_scheduler.config.prediction_type == \"epsilon\":\n                    target = noise\n                elif noise_scheduler.config.prediction_type == \"v_prediction\":\n                    target = noise_scheduler.get_velocity(latents, noise, timesteps)\n                else:\n                    raise ValueError(f\"Unknown prediction type {noise_scheduler.config.prediction_type}\") \n\n                if cfg.snr_gamma is None:\n                    loss = F.mse_loss(model_pred.float(), target.float(), reduction=\"mean\")\n                else:\n                    # Compute loss-weights as per Section 3.4 of https://arxiv.org/abs/2303.09556.\n                    # Since we predict the noise instead of x_0, the original formulation is slightly changed.\n                    # This is discussed in Section 4.2 of the same paper.\n                    snr = compute_snr(timesteps)\n                    mse_loss_weights = (\n                        torch.stack([snr, cfg.snr_gamma * torch.ones_like(timesteps)], dim=1).min(dim=1)[0] / snr\n                    )\n                    # We first calculate the original loss. Then we mean over the non-batch dimensions and\n                    # rebalance the sample-wise losses with their respective loss weights.\n                    # Finally, we take the mean of the rebalanced loss.\n                    loss = F.mse_loss(model_pred.float(), target.float(), reduction=\"none\")\n                    loss = loss.mean(dim=list(range(1, len(loss.shape)))) * mse_loss_weights\n                    loss = loss.mean()                    \n\n                # Gather the losses across all processes for logging (if we use distributed training).\n                avg_loss = accelerator.gather(loss.repeat(cfg.train_batch_size)).mean()\n                train_loss += avg_loss.item() / cfg.gradient_accumulation_steps\n\n                # Backpropagate\n                accelerator.backward(loss)\n\n                if accelerator.sync_gradients and cfg.max_grad_norm is not None:\n                    accelerator.clip_grad_norm_(unet.parameters(), cfg.max_grad_norm)\n                optimizer.step()\n                lr_scheduler.step()\n                optimizer.zero_grad()\n\n            # Checks if the accelerator has performed an optimization step behind the scenes\n            if accelerator.sync_gradients:\n                if cfg.use_ema:\n                    ema_unet.step(unet.parameters())\n                progress_bar.update(1)\n                global_step += 1\n                accelerator.log({\"train_loss\": train_loss}, step=global_step)\n                train_loss = 0.0\n\n                if global_step % cfg.checkpointing_steps == 0:\n                    if accelerator.is_main_process:\n                        save_path = os.path.join(cfg.output_dir, f\"checkpoint\")\n                        accelerator.save_state(save_path)\n                        try:\n                            unet.module.save_pretrained(os.path.join(cfg.output_dir, f\"unet-{global_step}/unet\"))\n                        except:\n                            unet.save_pretrained(os.path.join(cfg.output_dir, f\"unet-{global_step}/unet\"))\n                        logger.info(f\"Saved state to {save_path}\")\n\n                if global_step % cfg.validation_steps == 0 or (cfg.validation_sanity_check and global_step == 1):\n                    if accelerator.is_main_process:\n                        if cfg.use_ema:\n                            # Store the UNet parameters temporarily and load the EMA parameters to perform inference.\n                            ema_unet.store(unet.parameters())\n                            ema_unet.copy_to(unet.parameters())\n                        log_validation(\n                            validation_dataloader_ortho,\n                            vae,\n                            feature_extractor,\n                            image_encoder,\n                            unet,\n                            cfg,\n                            accelerator,\n                            weight_dtype,\n                            global_step,\n                            'validation_ortho',\n                            vis_dir\n                        )\n\n                        log_validation(\n                            validation_dataloader_persp,\n                            vae,\n                            feature_extractor,\n                            image_encoder,\n                            unet,\n                            cfg,\n                            accelerator,\n                            weight_dtype,\n                            global_step,\n                            'validation_persp',\n                            vis_dir\n                        )\n\n                        log_validation(\n                            validation_train_dataloader_ortho,\n                            vae,\n                            feature_extractor,\n                            image_encoder,\n                            unet,\n                            cfg,\n                            accelerator,\n                            weight_dtype,\n                            global_step,\n                            'validation_train_ortho',\n                            vis_dir\n                        )\n                        log_validation(\n                            validation_train_dataloader_persp,\n                            vae,\n                            feature_extractor,\n                            image_encoder,\n                            unet,\n                            cfg,\n                            accelerator,\n                            weight_dtype,\n                            global_step,\n                            'validation_train_persp',\n                            vis_dir\n                        )\n                        if cfg.use_ema:\n                            # Switch back to the original UNet parameters.\n                            ema_unet.restore(unet.parameters())                        \n\n            logs = {\"step_loss\": loss.detach().item(), \"lr\": lr_scheduler.get_last_lr()[0]}\n            progress_bar.set_postfix(**logs)\n\n            if global_step >= cfg.max_train_steps:\n                break\n\n    # Create the pipeline using the trained modules and save it.\n    accelerator.wait_for_everyone()\n    if accelerator.is_main_process:\n        unet = accelerator.unwrap_model(unet)\n        if cfg.use_ema:\n            ema_unet.copy_to(unet.parameters())\n        pipeline = MVDiffusionImagePipeline(\n            image_encoder=image_encoder, feature_extractor=feature_extractor, vae=vae, unet=unet, safety_checker=None,\n            scheduler=DDIMScheduler.from_pretrained(cfg.pretrained_model_name_or_path, subfolder=\"scheduler\"),\n            **cfg.pipe_kwargs\n        )            \n        os.makedirs(os.path.join(cfg.output_dir, \"pipeckpts\"), exist_ok=True)\n        pipeline.save_pretrained(os.path.join(cfg.output_dir, \"pipeckpts\"))\n\n    accelerator.end_training()\n\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--config', type=str, required=True)\n    args = parser.parse_args()\n    schema = OmegaConf.structured(TrainingConfig)\n    cfg = OmegaConf.load(args.config)\n    cfg = OmegaConf.merge(schema, cfg)\n    main(cfg)\n"
        },
        {
          "name": "train_mvdiffusion_mixed.py",
          "type": "blob",
          "size": 38.1171875,
          "content": "from comet_ml import Experiment\nfrom comet_ml.integration.pytorch import log_model\n\nimport argparse\nimport datetime\nimport logging\nimport inspect\nimport math\nimport os\nfrom typing import Dict, Optional, Tuple, List\nfrom omegaconf import OmegaConf\nfrom PIL import Image\nimport cv2\nimport numpy as np\nfrom dataclasses import dataclass\nfrom packaging import version\nimport shutil\nfrom collections import defaultdict\n\nimport torch\nimport torch.nn.functional as F\nimport torch.utils.checkpoint\nimport torchvision.transforms.functional as TF\nfrom torchvision.transforms import InterpolationMode\nfrom torchvision.utils import make_grid, save_image\n\nimport transformers\nimport accelerate\nfrom accelerate import Accelerator\nfrom accelerate.logging import get_logger\nfrom accelerate.utils import ProjectConfiguration, set_seed\n\nimport diffusers\nfrom diffusers import AutoencoderKL, DDPMScheduler, DDIMScheduler, StableDiffusionPipeline\nfrom diffusers.optimization import get_scheduler\nfrom diffusers.training_utils import EMAModel\nfrom diffusers.utils import check_min_version, deprecate, is_wandb_available\nfrom diffusers.utils.import_utils import is_xformers_available\n\nfrom tqdm.auto import tqdm\nfrom transformers import CLIPTextModel, CLIPTokenizer\nfrom transformers import CLIPImageProcessor, CLIPVisionModelWithProjection\n\nfrom mv_diffusion_30.models.unet_mv2d_condition import UNetMV2DConditionModel\n\nfrom mv_diffusion_30.data.objaverse_dataset import ObjaverseDataset as MVDiffusionDataset\n\nfrom mv_diffusion_30.pipelines.pipeline_mvdiffusion_image import MVDiffusionImagePipeline\n\nfrom einops import rearrange\n\nimport time\n\nlogger = get_logger(__name__, log_level=\"INFO\")\n\n\n@dataclass\nclass TrainingConfig:\n    pretrained_model_name_or_path: str\n    pretrained_unet_path: Optional[str]\n    revision: Optional[str]\n    train_dataset: Dict\n    validation_dataset: Dict\n    validation_train_dataset: Dict\n    output_dir: str\n    seed: Optional[int]\n    train_batch_size: int\n    validation_batch_size: int\n    validation_train_batch_size: int\n    max_train_steps: int\n    gradient_accumulation_steps: int\n    gradient_checkpointing: bool\n    learning_rate: float\n    scale_lr: bool\n    lr_scheduler: str\n    lr_warmup_steps: int\n    snr_gamma: Optional[float]\n    use_8bit_adam: bool\n    allow_tf32: bool\n    use_ema: bool\n    dataloader_num_workers: int\n    adam_beta1: float\n    adam_beta2: float\n    adam_weight_decay: float\n    adam_epsilon: float\n    max_grad_norm: Optional[float]\n    prediction_type: Optional[str]\n    logging_dir: str\n    vis_dir: str\n    mixed_precision: Optional[str]\n    report_to: Optional[str]\n    local_rank: int\n    checkpointing_steps: int\n    checkpoints_total_limit: Optional[int]\n    resume_from_checkpoint: Optional[str]\n    enable_xformers_memory_efficient_attention: bool\n    validation_steps: int\n    validation_sanity_check: bool\n    tracker_project_name: str\n\n    trainable_modules: Optional[list]\n    use_classifier_free_guidance: bool\n    condition_drop_rate: float\n    scale_input_latents: bool\n\n    pipe_kwargs: Dict\n    pipe_validation_kwargs: Dict\n    unet_from_pretrained_kwargs: Dict\n    validation_guidance_scales: List[float]\n    validation_grid_nrow: int\n    camera_embedding_lr_mult: float\n\n    num_views: int\n    camera_embedding_type: str\n\n    pred_type: str\n\n    drop_type: str\n\n\ndef log_validation(dataloader, vae, feature_extractor, image_encoder, unet, cfg: TrainingConfig, accelerator,\n                   weight_dtype, global_step, name, save_dir):\n    logger.info(f\"Running {name} ... \")\n\n    pipeline = MVDiffusionImagePipeline(\n        image_encoder=image_encoder, feature_extractor=feature_extractor, vae=vae, unet=accelerator.unwrap_model(unet),\n        safety_checker=None,\n        scheduler=DDIMScheduler.from_pretrained(cfg.pretrained_model_name_or_path, subfolder=\"scheduler\"),\n        **cfg.pipe_kwargs\n    )\n\n    pipeline.set_progress_bar_config(disable=True)\n\n    # if cfg.enable_xformers_memory_efficient_attention:\n    #     pipeline.enable_xformers_memory_efficient_attention()\n    #     pass\n    if cfg.seed is None:\n        generator = None\n    else:\n        generator = torch.Generator(device=accelerator.device).manual_seed(cfg.seed)\n\n    images_cond, images_gt, images_pred = [], [], defaultdict(list)\n\n    for i, batch in enumerate(dataloader):\n        # (B, Nv, 3, H, W)\n        if cfg.pred_type == 'color' or cfg.pred_type == 'mixed_rgb_normal_depth' or cfg.pred_type == 'mixed_color_normal':\n            imgs_in, imgs_out = batch['imgs_in'], batch['imgs_out']\n        elif cfg.pred_type == 'normal':\n            imgs_in, imgs_out = batch['imgs_in'], batch['normals_out']\n        else:\n            imgs_in, imgs_out = batch['imgs_in'], batch['imgs_out']\n        # (B, Nv, Nce)\n        camera_embeddings = batch['camera_embeddings']\n\n        if cfg.pred_type == 'mixed_rgb_normal_depth' or cfg.pred_type == 'color' or cfg.pred_type == 'mixed_color_normal' or cfg.pred_type == 'mixed_rgb_noraml_mask':\n            task_embeddings = batch['task_embeddings']\n            camera_embeddings = torch.cat([camera_embeddings, task_embeddings], dim=-1)\n\n        # (B*Nv, 3, H, W)\n        imgs_in, imgs_out = rearrange(imgs_in, \"B Nv C H W -> (B Nv) C H W\"), rearrange(imgs_out,\n                                                                                        \"B Nv C H W -> (B Nv) C H W\")\n        # (B*Nv, Nce)\n        camera_embeddings = rearrange(camera_embeddings, \"B Nv Nce -> (B Nv) Nce\")\n\n        images_cond.append(imgs_in)\n        images_gt.append(imgs_out)\n        with torch.autocast(\"cuda\"):\n            # B*Nv images\n            for guidance_scale in cfg.validation_guidance_scales:\n                out = pipeline(\n                    imgs_in, camera_embeddings, generator=generator, guidance_scale=guidance_scale, output_type='pt',\n                    num_images_per_prompt=1, **cfg.pipe_validation_kwargs\n                ).images\n                images_pred[f\"{name}-sample_cfg{guidance_scale:.1f}\"].append(out)\n\n    nrow = cfg.validation_grid_nrow\n\n    images_gt_all = torch.cat(images_gt, dim=0)\n    images_gt_grid = make_grid(images_gt_all, nrow=nrow, padding=0, value_range=(0, 1))\n    save_image(images_gt_grid, os.path.join(save_dir, f\"{global_step}-{name}-gt.jpg\"))\n    images_cond_all = torch.cat(images_cond, dim=0)\n\n    images_pred_all = {}\n    for k, v in images_pred.items():\n        images_pred_all[k] = torch.cat(v, dim=0)\n\n\n    images_cond_grid = make_grid(images_cond_all, nrow=nrow, padding=0, value_range=(0, 1))\n\n    images_pred_grid = {}\n    for k, v in images_pred_all.items():\n        images_pred_grid[k] = make_grid(v, nrow=nrow, padding=0, value_range=(0, 1))\n    save_image(images_cond_grid, os.path.join(save_dir, f\"{global_step}-{name}-cond.jpg\"))\n\n    for k, v in images_pred_grid.items():\n        save_image(v, os.path.join(save_dir, f\"{global_step}-{k}.jpg\"))\n    torch.cuda.empty_cache()\n\n\ndef main(\n        cfg: TrainingConfig\n):\n    # override local_rank with envvar\n    env_local_rank = int(os.environ.get(\"LOCAL_RANK\", -1))\n    if env_local_rank != -1 and env_local_rank != cfg.local_rank:\n        cfg.local_rank = env_local_rank\n\n    vis_dir = os.path.join(cfg.output_dir, cfg.vis_dir)\n    logging_dir = os.path.join(cfg.output_dir, cfg.logging_dir)\n    accelerator_project_config = ProjectConfiguration(project_dir=cfg.output_dir, logging_dir=logging_dir)\n\n    accelerator = Accelerator(\n        gradient_accumulation_steps=cfg.gradient_accumulation_steps,\n        mixed_precision=cfg.mixed_precision,\n        log_with=cfg.report_to,\n        project_config=accelerator_project_config,\n    )\n\n    # Make one log on every process with the configuration for debugging.\n    logging.basicConfig(\n        format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n        datefmt=\"%m/%d/%Y %H:%M:%S\",\n        level=logging.INFO,\n    )\n    logger.info(accelerator.state, main_process_only=False)\n    if accelerator.is_local_main_process:\n        transformers.utils.logging.set_verbosity_warning()\n        diffusers.utils.logging.set_verbosity_info()\n    else:\n        transformers.utils.logging.set_verbosity_error()\n        diffusers.utils.logging.set_verbosity_error()\n\n    # If passed along, set the training seed now.\n    if cfg.seed is not None:\n        set_seed(cfg.seed)\n\n    generator = torch.Generator(device=accelerator.device).manual_seed(cfg.seed)\n\n    # Handle the repository creation\n    if accelerator.is_main_process:\n        os.makedirs(cfg.output_dir, exist_ok=True)\n        os.makedirs(vis_dir, exist_ok=True)\n        OmegaConf.save(cfg, os.path.join(cfg.output_dir, 'config.yaml'))\n\n    # Load scheduler, tokenizer and models.\n    noise_scheduler = DDPMScheduler.from_pretrained(cfg.pretrained_model_name_or_path, subfolder=\"scheduler\")\n    image_encoder = CLIPVisionModelWithProjection.from_pretrained(cfg.pretrained_model_name_or_path,\n                                                                  subfolder=\"image_encoder\", revision=cfg.revision)\n    feature_extractor = CLIPImageProcessor.from_pretrained(cfg.pretrained_model_name_or_path,\n                                                           subfolder=\"feature_extractor\", revision=cfg.revision)\n    vae = AutoencoderKL.from_pretrained(cfg.pretrained_model_name_or_path, subfolder=\"vae\", revision=cfg.revision)\n\n    if cfg.pretrained_unet_path is None:\n        unet = UNetMV2DConditionModel.from_pretrained_2d(cfg.pretrained_model_name_or_path, subfolder=\"unet\",\n                                                         revision=cfg.revision, **cfg.unet_from_pretrained_kwargs)\n    else:\n        print(\"load pre-trained unet from \", cfg.pretrained_unet_path)\n        unet = UNetMV2DConditionModel.from_pretrained(cfg.pretrained_unet_path, revision=cfg.revision,\n                                                      **cfg.unet_from_pretrained_kwargs)\n    if cfg.use_ema:\n        ema_unet = EMAModel(unet.parameters(), model_cls=UNetMV2DConditionModel, model_config=unet.config)\n\n    def compute_snr(timesteps):\n        \"\"\"\n        Computes SNR as per https://github.com/TiankaiHang/Min-SNR-Diffusion-Training/blob/521b624bd70c67cee4bdf49225915f5945a872e3/guided_diffusion/gaussian_diffusion.py#L847-L849\n        \"\"\"\n        alphas_cumprod = noise_scheduler.alphas_cumprod\n        sqrt_alphas_cumprod = alphas_cumprod ** 0.5\n        sqrt_one_minus_alphas_cumprod = (1.0 - alphas_cumprod) ** 0.5\n\n        # Expand the tensors.\n        # Adapted from https://github.com/TiankaiHang/Min-SNR-Diffusion-Training/blob/521b624bd70c67cee4bdf49225915f5945a872e3/guided_diffusion/gaussian_diffusion.py#L1026\n        sqrt_alphas_cumprod = sqrt_alphas_cumprod.to(device=timesteps.device)[timesteps].float()\n        while len(sqrt_alphas_cumprod.shape) < len(timesteps.shape):\n            sqrt_alphas_cumprod = sqrt_alphas_cumprod[..., None]\n        alpha = sqrt_alphas_cumprod.expand(timesteps.shape)\n\n        sqrt_one_minus_alphas_cumprod = sqrt_one_minus_alphas_cumprod.to(device=timesteps.device)[timesteps].float()\n        while len(sqrt_one_minus_alphas_cumprod.shape) < len(timesteps.shape):\n            sqrt_one_minus_alphas_cumprod = sqrt_one_minus_alphas_cumprod[..., None]\n        sigma = sqrt_one_minus_alphas_cumprod.expand(timesteps.shape)\n\n        # Compute SNR.\n        snr = (alpha / sigma) ** 2\n        return snr\n\n    # Freeze vae and text_encoder\n    vae.requires_grad_(False)\n    image_encoder.requires_grad_(False)\n\n    if cfg.trainable_modules is None:\n        unet.requires_grad_(True)\n    else:\n        unet.requires_grad_(False)\n        for name, module in unet.named_modules():\n            if name.endswith(tuple(cfg.trainable_modules)):\n                for params in module.parameters():\n                    params.requires_grad = True\n\n    # `accelerate` 0.16.0 will have better support for customized saving\n    if version.parse(accelerate.__version__) >= version.parse(\"0.16.0\"):\n        # create custom saving & loading hooks so that `accelerator.save_state(...)` serializes in a nice format\n        def save_model_hook(models, weights, output_dir):\n            if cfg.use_ema:\n                ema_unet.save_pretrained(os.path.join(output_dir, \"unet_ema\"))\n\n            for i, model in enumerate(models):\n                model.save_pretrained(os.path.join(output_dir, \"unet\"))\n\n                # make sure to pop weight so that corresponding model is not saved again\n                weights.pop()\n\n        def load_model_hook(models, input_dir):\n            if cfg.use_ema:\n                load_model = EMAModel.from_pretrained(os.path.join(input_dir, \"unet_ema\"), UNetMV2DConditionModel)\n                ema_unet.load_state_dict(load_model.state_dict())\n                ema_unet.to(accelerator.device)\n                del load_model\n\n            for i in range(len(models)):\n                # pop models so that they are not loaded again\n                model = models.pop()\n\n                # load diffusers style into model\n                load_model = UNetMV2DConditionModel.from_pretrained(input_dir, subfolder=\"unet\")\n                model.register_to_config(**load_model.config)\n\n                model.load_state_dict(load_model.state_dict())\n                del load_model\n\n        accelerator.register_save_state_pre_hook(save_model_hook)\n        accelerator.register_load_state_pre_hook(load_model_hook)\n\n    if cfg.gradient_checkpointing:\n        unet.enable_gradient_checkpointing()\n\n    # Enable TF32 for faster training on Ampere GPUs,\n    # cf https://pytorch.org/docs/stable/notes/cuda.html#tensorfloat-32-tf32-on-ampere-devices\n    if cfg.allow_tf32:\n        torch.backends.cuda.matmul.allow_tf32 = True\n\n    if cfg.scale_lr:\n        cfg.learning_rate = (\n                cfg.learning_rate * cfg.gradient_accumulation_steps * cfg.train_batch_size * accelerator.num_processes\n        )\n\n    # Initialize the optimizer\n    if cfg.use_8bit_adam:\n        try:\n            import bitsandbytes as bnb\n        except ImportError:\n            raise ImportError(\n                \"Please install bitsandbytes to use 8-bit Adam. You can do so by running `pip install bitsandbytes`\"\n            )\n\n        optimizer_cls = bnb.optim.AdamW8bit\n    else:\n        optimizer_cls = torch.optim.AdamW\n\n    params, params_class_embedding = [], []\n    for name, param in unet.named_parameters():\n        if 'class_embedding' in name:\n            params_class_embedding.append(param)\n        else:\n            params.append(param)\n\n        # \n        # print(f\"Parameter: {name}, Type: {param.dtype}\")\n\n    optimizer = optimizer_cls(\n        [\n            {\"params\": params, \"lr\": cfg.learning_rate},\n            {\"params\": params_class_embedding, \"lr\": cfg.learning_rate * cfg.camera_embedding_lr_mult}\n        ],\n        betas=(cfg.adam_beta1, cfg.adam_beta2),\n        weight_decay=cfg.adam_weight_decay,\n        eps=cfg.adam_epsilon,\n    )\n\n    lr_scheduler = get_scheduler(\n        cfg.lr_scheduler,\n        optimizer=optimizer,\n        num_warmup_steps=cfg.lr_warmup_steps * accelerator.num_processes,\n        num_training_steps=cfg.max_train_steps * accelerator.num_processes,\n    )\n\n    # Get the training dataset\n    train_dataset = MVDiffusionDataset(\n        **cfg.train_dataset\n    )\n    # DataLoaders creation:\n    train_dataloader = torch.utils.data.DataLoader(\n        train_dataset, batch_size=cfg.train_batch_size, shuffle=True, num_workers=cfg.dataloader_num_workers,\n    )\n\n    cam_condition = (cfg.train_dataset.pred_ortho and cfg.train_dataset.pred_persp)\n    if cam_condition:\n        cfg.validation_dataset.pred_ortho, cfg.validation_dataset.pred_persp = True, False\n        cfg.validation_train_dataset.pred_ortho, cfg.validation_train_dataset.pred_persp = True, False\n        validation_dataset_ortho = MVDiffusionDataset(\n            **cfg.validation_dataset\n        )\n        validation_train_dataset_ortho = MVDiffusionDataset(\n            **cfg.validation_train_dataset\n        )\n\n        cfg.validation_dataset.pred_ortho, cfg.validation_dataset.pred_persp = False, True\n        cfg.validation_train_dataset.pred_ortho, cfg.validation_train_dataset.pred_persp = False, True\n        validation_dataset_persp = MVDiffusionDataset(\n            **cfg.validation_dataset\n        )\n        validation_train_dataset_persp = MVDiffusionDataset(\n            **cfg.validation_train_dataset\n        )\n        # print(validation_dataset_ortho.pred_ortho, validation_dataset_ortho.pred_persp, validation_train_dataset_persp.pred_ortho, validation_train_dataset_persp.pred_persp, validation_dataset_persp.root_dir_persp)\n\n        validation_dataloader_ortho = torch.utils.data.DataLoader(\n            validation_dataset_ortho, batch_size=cfg.validation_batch_size, shuffle=False,\n            num_workers=cfg.dataloader_num_workers\n        )\n        validation_dataloader_persp = torch.utils.data.DataLoader(\n            validation_dataset_persp, batch_size=cfg.validation_batch_size, shuffle=False,\n            num_workers=cfg.dataloader_num_workers\n        )\n\n        validation_train_dataloader_ortho = torch.utils.data.DataLoader(\n            validation_train_dataset_ortho, batch_size=cfg.validation_train_batch_size, shuffle=False,\n            num_workers=cfg.dataloader_num_workers\n        )\n        validation_train_dataloader_persp = torch.utils.data.DataLoader(\n            validation_train_dataset_persp, batch_size=cfg.validation_train_batch_size, shuffle=False,\n            num_workers=cfg.dataloader_num_workers\n        )\n    else:\n        validation_dataset = MVDiffusionDataset(\n            **cfg.validation_dataset\n        )\n        validation_train_dataset = MVDiffusionDataset(\n            **cfg.validation_train_dataset\n        )\n        validation_dataloader = torch.utils.data.DataLoader(\n            validation_dataset, batch_size=cfg.validation_batch_size, shuffle=False,\n            num_workers=cfg.dataloader_num_workers\n        )\n        validation_train_dataloader = torch.utils.data.DataLoader(\n            validation_train_dataset, batch_size=cfg.validation_train_batch_size, shuffle=False,\n            num_workers=cfg.dataloader_num_workers\n        )\n\n    # Prepare everything with our `accelerator`.\n    unet, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(\n        unet, optimizer, train_dataloader, lr_scheduler\n    )\n\n    if cfg.use_ema:\n        ema_unet.to(accelerator.device)\n\n    # For mixed precision training we cast all non-trainable weigths (vae, non-lora text_encoder and non-lora unet) to half-precision\n    # as these weights are only used for inference, keeping weights in full precision is not required.\n    weight_dtype = torch.float32\n    if accelerator.mixed_precision == \"fp16\":\n        weight_dtype = torch.float16\n        cfg.mixed_precision = accelerator.mixed_precision\n    elif accelerator.mixed_precision == \"bf16\":\n        weight_dtype = torch.bfloat16\n        cfg.mixed_precision = accelerator.mixed_precision\n\n    # Move text_encode and vae to gpu and cast to weight_dtype\n    image_encoder.to(accelerator.device, dtype=weight_dtype)\n    vae.to(accelerator.device, dtype=weight_dtype)\n\n    clip_image_mean = torch.as_tensor(feature_extractor.image_mean)[:, None, None].to(accelerator.device,\n                                                                                      dtype=torch.float32)\n    clip_image_std = torch.as_tensor(feature_extractor.image_std)[:, None, None].to(accelerator.device,\n                                                                                    dtype=torch.float32)\n\n    # We need to recalculate our total training steps as the size of the training dataloader may have changed.\n    num_update_steps_per_epoch = math.ceil(len(train_dataloader) / cfg.gradient_accumulation_steps)\n    num_train_epochs = math.ceil(cfg.max_train_steps / num_update_steps_per_epoch)\n\n    # We need to initialize the trackers we use, and also store our configuration.\n    # The trackers initializes automatically on the main process.\n    if accelerator.is_main_process:\n        # tracker_config = dict(vars(cfg))\n        tracker_config = {}\n        accelerator.init_trackers(cfg.tracker_project_name, tracker_config)\n\n        # Train!\n    total_batch_size = cfg.train_batch_size * accelerator.num_processes * cfg.gradient_accumulation_steps\n\n    logger.info(\"***** Running training *****\")\n    logger.info(f\"  Num examples = {len(train_dataset)}\")\n    logger.info(f\"  Num Epochs = {num_train_epochs}\")\n    logger.info(f\"  Instantaneous batch size per device = {cfg.train_batch_size}\")\n    logger.info(f\"  Total train batch size (w. parallel, distributed & accumulation) = {total_batch_size}\")\n    logger.info(f\"  Gradient Accumulation steps = {cfg.gradient_accumulation_steps}\")\n    logger.info(f\"  Total optimization steps = {cfg.max_train_steps}\")\n    global_step = 0\n    first_epoch = 0\n\n    # Potentially load in the weights and states from a previous save\n    if cfg.resume_from_checkpoint:\n        if cfg.resume_from_checkpoint != \"latest\":\n            path = os.path.basename(cfg.resume_from_checkpoint)\n        else:\n            # Get the most recent checkpoint\n            if os.path.exists(os.path.join(cfg.output_dir, \"checkpoint\")):\n                path = \"checkpoint\"\n            else:\n                dirs = os.listdir(cfg.output_dir)\n                dirs = [d for d in dirs if d.startswith(\"checkpoint\")]\n                dirs = sorted(dirs, key=lambda x: int(x.split(\"-\")[1]))\n                path = dirs[-1] if len(dirs) > 0 else None\n\n        if path is None:\n            accelerator.print(\n                f\"Checkpoint '{cfg.resume_from_checkpoint}' does not exist. Starting a new training run.\"\n            )\n            cfg.resume_from_checkpoint = None\n        else:\n            accelerator.print(f\"Resuming from checkpoint {path}\")\n            global_step = 0  # 0 or just change this\n\n            resume_global_step = global_step * cfg.gradient_accumulation_steps\n            first_epoch = global_step // num_update_steps_per_epoch\n            resume_step = resume_global_step % (num_update_steps_per_epoch * cfg.gradient_accumulation_steps)\n\n            # Only show the progress bar once on each machine.\n    progress_bar = tqdm(range(global_step, cfg.max_train_steps), disable=not accelerator.is_local_main_process)\n    progress_bar.set_description(\"Steps\")\n    if accelerator.is_main_process:\n        experiment = Experiment(\n            api_key=\"your_api_key\",\n            project_name=\"your_project_name\",\n            workspace=\"your_workspace\"\n        )\n        hyper_params = {\n            \"learning_rate\": 0.0001,\n            \"steps\": 80000,\n            \"batch_size\": 256\n        }\n        experiment.log_parameters(hyper_params)\n\n    for epoch in range(first_epoch, num_train_epochs):\n        unet.train()\n        train_loss = 0.0\n        for step, batch in enumerate(train_dataloader):\n            # Skip steps until we reach the resumed step\n            if cfg.resume_from_checkpoint and epoch == first_epoch and step < resume_step:\n                if step % cfg.gradient_accumulation_steps == 0:\n                    progress_bar.update(1)\n                continue\n\n            with accelerator.accumulate(unet):\n                # (B, Nv, 3, H, W)\n                if cfg.pred_type == 'color' or cfg.pred_type == 'mixed_rgb_normal_depth' or cfg.pred_type == 'mixed_color_normal':\n                    imgs_in, imgs_out = batch['imgs_in'], batch['imgs_out']\n                elif cfg.pred_type == 'normal':\n                    imgs_in, imgs_out = batch['imgs_in'], batch['normals_out']\n                else:\n                    imgs_in, imgs_out = batch['imgs_in'], batch['imgs_out']\n\n                bnm, Nv = imgs_in.shape[0], imgs_in.shape[1]\n\n                # (B, Nv, Nce)\n                camera_embeddings = batch['camera_embeddings']\n\n                if cfg.pred_type == 'mixed_rgb_normal_depth' or cfg.pred_type == 'color' or cfg.pred_type == 'mixed_color_normal' or cfg.pred_type == 'mixed_rgb_noraml_mask':\n                    task_embeddings = batch['task_embeddings']\n                    camera_embeddings = torch.cat([camera_embeddings, task_embeddings], dim=-1)\n\n                # (B*Nv, 3, H, W)\n                imgs_in, imgs_out = rearrange(imgs_in, \"B Nv C H W -> (B Nv) C H W\"), rearrange(imgs_out,\n                                                                                                \"B Nv C H W -> (B Nv) C H W\")\n                # (B*Nv, Nce)\n                camera_embeddings = rearrange(camera_embeddings, \"B Nv Nce -> (B Nv) Nce\")\n                # (B*Nv, Nce')\n                if cfg.camera_embedding_type == 'e_de_da_sincos':\n                    camera_embeddings = torch.cat([\n                        torch.sin(camera_embeddings),\n                        torch.cos(camera_embeddings)\n                    ], dim=-1)\n                else:\n                    raise NotImplementedError\n\n                imgs_in, imgs_out, camera_embeddings = imgs_in.to(weight_dtype), imgs_out.to(\n                    weight_dtype), camera_embeddings.to(weight_dtype)\n\n                # (B*Nv, 4, Hl, Wl)\n                cond_vae_embeddings = vae.encode(imgs_in * 2.0 - 1.0).latent_dist.mode()\n                if cfg.scale_input_latents:\n                    cond_vae_embeddings = cond_vae_embeddings * vae.config.scaling_factor\n                latents = vae.encode(imgs_out * 2.0 - 1.0).latent_dist.sample() * vae.config.scaling_factor\n\n                # DO NOT use this! Very slow!\n                # imgs_in_pil = [TF.to_pil_image(img) for img in imgs_in]\n                # imgs_in_proc = feature_extractor(images=imgs_in_pil, return_tensors='pt').pixel_values.to(dtype=latents.dtype, device=latents.device)\n\n                imgs_in_proc = TF.resize(imgs_in,\n                                         (feature_extractor.crop_size['height'], feature_extractor.crop_size['width']),\n                                         interpolation=InterpolationMode.BICUBIC)\n                # do the normalization in float32 to preserve precision\n                imgs_in_proc = ((imgs_in_proc.float() - clip_image_mean) / clip_image_std).to(weight_dtype)\n\n                # (B*Nv, 1, 768)\n                image_embeddings = image_encoder(imgs_in_proc).image_embeds.unsqueeze(1)\n\n                noise = torch.randn_like(latents)\n                bsz = latents.shape[0]\n\n                # same noise for different views of the same object\n                timesteps = torch.randint(0, noise_scheduler.num_train_timesteps, (bsz // cfg.num_views,),\n                                          device=latents.device).repeat_interleave(cfg.num_views)\n                timesteps = timesteps.long()\n\n                noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)\n\n                # Conditioning dropout to support classifier-free guidance during inference. For more details\n                # check out the section 3.2.1 of the original paper https://arxiv.org/abs/2211.09800.\n                if cfg.use_classifier_free_guidance and cfg.condition_drop_rate > 0.:\n                    if cfg.drop_type == 'drop_as_a_whole':\n                        # drop a group of normals and colors as a whole\n                        random_p = torch.rand(bnm, device=latents.device, generator=generator)\n\n                        # Sample masks for the conditioning images.\n                        image_mask_dtype = cond_vae_embeddings.dtype\n                        image_mask = 1 - (\n                                (random_p >= cfg.condition_drop_rate).to(image_mask_dtype)\n                                * (random_p < 3 * cfg.condition_drop_rate).to(image_mask_dtype)\n                        )\n                        image_mask = image_mask.reshape(bnm, 1, 1, 1, 1).repeat(1, Nv, 1, 1, 1)\n                        image_mask = rearrange(image_mask, \"B Nv C H W -> (B Nv) C H W\")\n                        # Final image conditioning.\n                        cond_vae_embeddings = image_mask * cond_vae_embeddings\n\n                        # Sample masks for the conditioning images.\n                        clip_mask_dtype = image_embeddings.dtype\n                        clip_mask = 1 - (\n                            (random_p < 2 * cfg.condition_drop_rate).to(clip_mask_dtype)\n                        )\n                        clip_mask = clip_mask.reshape(bnm, 1, 1, 1).repeat(1, Nv, 1, 1)\n                        clip_mask = rearrange(clip_mask, \"B Nv M C -> (B Nv) M C\")\n                        # Final image conditioning.\n                        image_embeddings = clip_mask * image_embeddings\n                    elif cfg.drop_type == 'drop_independent':\n                        random_p = torch.rand(bsz, device=latents.device, generator=generator)\n\n                        # Sample masks for the conditioning images.\n                        image_mask_dtype = cond_vae_embeddings.dtype\n                        image_mask = 1 - (\n                                (random_p >= cfg.condition_drop_rate).to(image_mask_dtype)\n                                * (random_p < 3 * cfg.condition_drop_rate).to(image_mask_dtype)\n                        )\n                        image_mask = image_mask.reshape(bsz, 1, 1, 1)\n                        # Final image conditioning.\n                        cond_vae_embeddings = image_mask * cond_vae_embeddings\n\n                        # Sample masks for the conditioning images.\n                        clip_mask_dtype = image_embeddings.dtype\n                        clip_mask = 1 - (\n                            (random_p < 2 * cfg.condition_drop_rate).to(clip_mask_dtype)\n                        )\n                        clip_mask = clip_mask.reshape(bsz, 1, 1)\n                        # Final image conditioning.\n                        image_embeddings = clip_mask * image_embeddings\n\n                # (B*Nv, 8, Hl, Wl)\n                latent_model_input = torch.cat([noisy_latents, cond_vae_embeddings], dim=1)\n\n                model_pred = unet(\n                    latent_model_input,\n                    timesteps,\n                    encoder_hidden_states=image_embeddings,\n                    class_labels=camera_embeddings\n                ).sample\n\n                # Get the target for loss depending on the prediction type\n                if noise_scheduler.config.prediction_type == \"epsilon\":\n                    target = noise\n                elif noise_scheduler.config.prediction_type == \"v_prediction\":\n                    target = noise_scheduler.get_velocity(latents, noise, timesteps)\n                else:\n                    raise ValueError(f\"Unknown prediction type {noise_scheduler.config.prediction_type}\")\n\n                if cfg.snr_gamma is None:\n                    loss = F.mse_loss(model_pred.float(), target.float(), reduction=\"mean\")\n                else:\n                    # Compute loss-weights as per Section 3.4 of https://arxiv.org/abs/2303.09556.\n                    # Since we predict the noise instead of x_0, the original formulation is slightly changed.\n                    # This is discussed in Section 4.2 of the same paper.\n                    snr = compute_snr(timesteps)\n                    mse_loss_weights = (\n                            torch.stack([snr, cfg.snr_gamma * torch.ones_like(timesteps)], dim=1).min(dim=1)[0] / snr\n                    )\n                    # We first calculate the original loss. Then we mean over the non-batch dimensions and\n                    # rebalance the sample-wise losses with their respective loss weights.\n                    # Finally, we take the mean of the rebalanced loss.\n                    loss = F.mse_loss(model_pred.float(), target.float(), reduction=\"none\")\n                    loss = loss.mean(dim=list(range(1, len(loss.shape)))) * mse_loss_weights\n                    loss = loss.mean()\n\n                    # Gather the losses across all processes for logging (if we use distributed training).\n                avg_loss = accelerator.gather(loss.repeat(cfg.train_batch_size)).mean()\n                train_loss += avg_loss.item() / cfg.gradient_accumulation_steps\n\n                # Backpropagate\n                accelerator.backward(loss)\n                # print(loss.dtype)\n                if accelerator.sync_gradients and cfg.max_grad_norm is not None:\n                    accelerator.clip_grad_norm_(unet.parameters(), cfg.max_grad_norm)\n                optimizer.step()\n                lr_scheduler.step()\n                optimizer.zero_grad()\n\n            # Checks if the accelerator has performed an optimization step behind the scenes\n            if accelerator.sync_gradients:\n                if cfg.use_ema:\n                    ema_unet.step(unet.parameters())\n                progress_bar.update(1)\n                global_step += 1\n                accelerator.log({\"train_loss\": train_loss}, step=global_step)\n                train_loss = 0.0\n\n                if global_step % cfg.checkpointing_steps == 0:\n                    if accelerator.is_main_process:\n                        save_path = os.path.join(cfg.output_dir, f\"checkpoint\")\n                        accelerator.save_state(save_path)\n                        try:\n                            unet.module.save_pretrained(os.path.join(cfg.output_dir, f\"unet-{global_step}\"))\n                        except:\n                            unet.save_pretrained(os.path.join(cfg.output_dir, f\"unet-{global_step}\"))\n                        logger.info(f\"Saved state to {save_path}\")\n\n                if global_step % cfg.validation_steps == 0 or (cfg.validation_sanity_check and global_step == 1):\n                    if accelerator.is_main_process:\n                        if cfg.use_ema:\n                            # Store the UNet parameters temporarily and load the EMA parameters to perform inference.\n                            ema_unet.store(unet.parameters())\n                            ema_unet.copy_to(unet.parameters())\n                        if cam_condition:\n                            log_validation(\n                                validation_dataloader_ortho,\n                                vae,\n                                feature_extractor,\n                                image_encoder,\n                                unet,\n                                cfg,\n                                accelerator,\n                                weight_dtype,\n                                global_step,\n                                'validation_ortho',\n                                vis_dir\n                            )\n\n                            log_validation(\n                                validation_dataloader_persp,\n                                vae,\n                                feature_extractor,\n                                image_encoder,\n                                unet,\n                                cfg,\n                                accelerator,\n                                weight_dtype,\n                                global_step,\n                                'validation_persp',\n                                vis_dir\n                            )\n\n                            log_validation(\n                                validation_train_dataloader_ortho,\n                                vae,\n                                feature_extractor,\n                                image_encoder,\n                                unet,\n                                cfg,\n                                accelerator,\n                                weight_dtype,\n                                global_step,\n                                'validation_train_ortho',\n                                vis_dir\n                            )\n                            log_validation(\n                                validation_train_dataloader_persp,\n                                vae,\n                                feature_extractor,\n                                image_encoder,\n                                unet,\n                                cfg,\n                                accelerator,\n                                weight_dtype,\n                                global_step,\n                                'validation_train_persp',\n                                vis_dir\n                            )\n                        else:\n                            log_validation(\n                                validation_dataloader,\n                                vae,\n                                feature_extractor,\n                                image_encoder,\n                                unet,\n                                cfg,\n                                accelerator,\n                                weight_dtype,\n                                global_step,\n                                'validation',\n                                vis_dir\n                            )\n                            log_validation(\n                                validation_train_dataloader,\n                                vae,\n                                feature_extractor,\n                                image_encoder,\n                                unet,\n                                cfg,\n                                accelerator,\n                                weight_dtype,\n                                global_step,\n                                'validation_train',\n                                vis_dir\n                            )\n\n                        if cfg.use_ema:\n                            # Switch back to the original UNet parameters.\n                            ema_unet.restore(unet.parameters())\n\n            logs = {\"step_loss\": loss.detach().item(), \"lr\": lr_scheduler.get_last_lr()[0]}\n            progress_bar.set_postfix(**logs)\n\n            if global_step >= cfg.max_train_steps:\n                break\n\n    # Create the pipeline using the trained modules and save it.\n    accelerator.wait_for_everyone()\n    if accelerator.is_main_process:\n        unet = accelerator.unwrap_model(unet)\n        if cfg.use_ema:\n            ema_unet.copy_to(unet.parameters())\n        pipeline = MVDiffusionImagePipeline(\n            image_encoder=image_encoder, feature_extractor=feature_extractor, vae=vae, unet=unet, safety_checker=None,\n            scheduler=DDIMScheduler.from_pretrained(cfg.pretrained_model_name_or_path, subfolder=\"scheduler\"),\n            **cfg.pipe_kwargs\n        )\n        os.makedirs(os.path.join(cfg.output_dir, \"pipeckpts\"), exist_ok=True)\n        pipeline.save_pretrained(os.path.join(cfg.output_dir, \"pipeckpts\"))\n\n    accelerator.end_training()\n    if accelerator.is_main_process:\n        log_model(experiment, model=unet, model_name=\"mv_depth_normal\")\n\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--config', type=str, required=True)\n    args = parser.parse_args()\n    schema = OmegaConf.structured(TrainingConfig)\n    cfg = OmegaConf.load(args.config)\n    cfg = OmegaConf.merge(schema, cfg)\n    main(cfg)"
        },
        {
          "name": "utils",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}