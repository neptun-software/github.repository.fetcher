{
  "metadata": {
    "timestamp": 1736559623347,
    "page": 269,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjI3MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "InsaneLife/ChineseNLPCorpus",
      "stars": 4349,
      "defaultBranch": "master",
      "files": [
        {
          "name": ".DS_Store",
          "type": "blob",
          "size": 6.00390625,
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.025390625,
          "content": "!/*/README.*\n!/*/readme.*\n"
        },
        {
          "name": "NER",
          "type": "tree",
          "content": null
        },
        {
          "name": "THUCNews",
          "type": "tree",
          "content": null
        },
        {
          "name": "dialogue",
          "type": "tree",
          "content": null
        },
        {
          "name": "news_sohusite_xml",
          "type": "tree",
          "content": null
        },
        {
          "name": "oppo_round1",
          "type": "tree",
          "content": null
        },
        {
          "name": "pic",
          "type": "tree",
          "content": null
        },
        {
          "name": "readme.md",
          "type": "blob",
          "size": 21.28515625,
          "content": "[TOC]\n\n\n# ChineseNlpCorpus\n\n中文自然语言处理数据集，平时做做实验的材料。欢迎补充提交合并。\n\n# 阅读理解\n\n阅读理解数据集按照方法主要有：抽取式、分类（观点提取）。按照篇章又分为单篇章、多篇章，比如有的问题答案可能需要从多个文章中提取，每个文章可能都只是一部分，那么多篇章提取就会面临怎么合并，合并的时候怎么去掉重复的，保留补充的。\n\n| 名称     | 规模                        | 说明                         | 单位 | 论文                                                  | 下载                                                         | 评测                                                         |\n| -------- | --------------------------- | ---------------------------- | ---- | ----------------------------------------------------- | ------------------------------------------------------------ | ------------------------------------------------------------ |\n| DuReader | 30万问题 140万文档 66万答案 | 问答阅读理解数据集 | 百度 | [链接](https://www.aclweb.org/anthology/W18-2605.pdf) | [链接](https://ai.baidu.com/broad/introduction?dataset=dureader) | [2018 NLP Challenge on MRC](http://mrc2018.cipsc.org.cn/) [2019 Language and Intelligence Challenge on MRC](http://lic2019.ccf.org.cn/) |\n| $DuReader_{robust}$ | 2.2万问题 | 单篇章、抽取式阅读理解数据集 | 百度 |   | [链接](https://github.com/PaddlePaddle/Research/tree/master/NLP/DuReader-Robust-BASELINE) | [评测](https://aistudio.baidu.com/aistudio/competition/detail/49/?isFromLUGE=TRUE) |\n| CMRC 2018 | 2万问题 | 篇章片段抽取型阅读理解 | 哈工大讯飞联合实验室 | [链接](https://www.aclweb.org/anthology/D19-1600.pdf) | [链接](https://github.com/ymcui/cmrc2018) | [第二届“讯飞杯”中文机器阅读理解评测](https://hfl-rc.github.io/cmrc2018/) |\n| $DuReader_{yesno}$     | 9万                         | 观点型阅读理解数据集         | 百度                 |                                                       | [链接](https://aistudio.baidu.com/aistudio/competition/detail/49/?isFromLUGE=TRUE) | [评测](https://aistudio.baidu.com/aistudio/competition/detail/49/?isFromLUGE=TRUE) |\n| $DuReader_{checklist}$ | 1万 | 抽取式数据集 | 百度 |                                                       | [链接](https://aistudio.baidu.com/aistudio/competition/detail/49/?isFromLUGE=TRUE) |                                                              |\n\n# 任务型对话数据\n\n## Medical DS\n\n复旦大学发布的基于百度拇指医生上真实对话数据的，面向任务型对话的中文医疗诊断数据集。\n\n| 名称       | 规模                       | 创建日期 | 作者       | 单位     | 论文                                                         | 下载                                                         |\n| ---------- | -------------------------- | -------- | ---------- | -------- | ------------------------------------------------------------ | ------------------------------------------------------------ |\n| Medical DS | 710个对话 67种症状 4种疾病 | 2018年   | Liu et al. | 复旦大学 | [链接](http://www.sdspeople.fudan.edu.cn/zywei/paper/liu-acl2018.pdf) | [链接](http://www.sdspeople.fudan.edu.cn/zywei/data/acl2018-mds.zip) |\n\n## 千言数据集\n\n包含知识对话、推荐对话、画像对话。详细见[官网](https://aistudio.baidu.com/aistudio/competition/detail/48/?isFromLUGE=TRUE)\n千言里面还有很多数据集，见:[https://www.luge.ai/#/](https://www.luge.ai/#/)\n## [CATSLU](https://dl.acm.org/doi/10.1145/3340555.3356098)\n\n之前的一些对话数据集集中于语义理解，而工业界真实情况ASR也会有错误，往往被忽略。[CATSLU](https://dl.acm.org/doi/10.1145/3340555.3356098)而是一个中文语音+NLU文本理解的对话数据集，可以从语音信号到理解端到端进行实验，例如直接从音素建模语言理解（而非word or token）。\n\n数据统计：\n\n![image-20200910233858454](https://raw.githubusercontent.com/InsaneLife/ChineseNLPCorpus/master/pic/image-20200910233858454.png)\n\n官方说明手册：[CATSLU](https://sites.google.com/view/catslu/handbook)\n数据下载：[https://sites.google.com/view/CATSLU/home](https://sites.google.com/view/CATSLU/home)\n\n## NLPCC2018 Shared Task 4\n\n中文呢真实商用车载语音任务型对话系统的对话日志.\n\n| 名称                    | 规模               | 创建日期 | 作者        | 单位 | 论文                                                         | 下载                                                         | 评测                                                         |\n| ----------------------- | ------------------ | -------- | ----------- | ---- | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |\n| NLPCC2018 Shared Task 4 | 5800对话 2.6万问题 | 2018年   | zhao et al. | 腾讯 | [链接](http://tcci.ccf.org.cn/conference/2018/papers/EV33.pdf) | [训练开发集](http://tcci.ccf.org.cn/conference/2018/dldoc/trainingdata04.zip) [测试集](http://tcci.ccf.org.cn/conference/2018/dldoc/tasktestdata04.zip) | [NLPCC 2018 Spoken Language Understanding in Task-oriented Dialog Systems](http://tcci.ccf.org.cn/conference/2018/taskdata.php) |\n\nNLPCC每年都会举办，包含大量中文数据集，如对话、qa、ner、情感检测、摘要等任务\n\n## SMP\n\n这是一系类数据集，每年都会有新的数据集放出。\n### SMP-2020-ECDT小样本对话语言理解数据集\n> 论文中叫FewJoint 基准数据集，来自于讯飞AIUI开放平台上真实用户语料和专家构造的语料(比例大概为3：7)，包含59个真实domain，目前domain最多的对话数据集之一，可以避免构造模拟domain，非常适合小样本和元学习方法评测。其中45个训练domain，5个开发domain，9个测试domain。\n\n数据集介绍：[新闻链接](https://mp.weixin.qq.com/s?__biz=MzIxMjAzNDY5Mg==&mid=2650799572&idx=1&sn=509e256c62d80e2866f38e9d026d4af3&chksm=8f47683fb830e129f0ac7d2ff294ad1bd2cad5dc2050ae1ab81a7b108b79a6edcdba3d8030f9&mpshare=1&scene=1&srcid=1007YJCULNtwsRCUx7b35S0m&sharer_sharetime=1602603945222&sharer_shareid=904fa30621d7b898b031f4fdb5da41fc&key=9ae93b5dab71cae000c0dd901c537565d9fac572f40bafa92d79cee849b96fddbdece4d7151bec0f9a1c330dc3a9ddfe5ff4d742eef3165a71be493cd344e6ebc0a34dd5ebc61cb3c519f3a1d765f480cd5fd85d6b45655cc09b9816726ff06c2480b5287346c11ef1a18c0195b51259bd768110b49eb4b7583b40580369bcd2&ascene=1&uin=MTAxMzA5NjY2NQ%3D%3D&devicetype=Windows+10+x64&version=6300002f&lang=zh_CN&exportkey=ATbSQY9SBUjBETt7KZpV%2BIk%3D&pass_ticket=gGOfSeYJMhUPfn3Fbu8lBtWlGjw%2BANSIQ4rgajKq6vxzOW%2Fm%2Bwcw3YkXM0bkiM%2Bz&wx_header=0)\n\n数据集论文：https://arxiv.org/abs/2009.08138\n数据集下载地址：https://atmahou.github.io/attachments/FewJoint.zip\n小样本工具平台主页地址：https://github.com/AtmaHou/MetaDialog\n\n### SMP-2019-NLU\n包含领域分类、意图识别和语义槽填充三项子任务的数据集。训练数据集下载：[trian.json](./dialogue/SMP-2019-NLU/train.json)，目前只获取到训练集，如果有同学有测试集，欢迎提供。\n\n|        | Train |\n| ------ | ----- |\n| Domain | 24    |\n| Intent | 29    |\n| Slot   | 63    |\n| Samples   | 2579  |\n\n\n\n### SMP-2017\n中文对话意图识别数据集，官方git和数据: [https://github.com/HITlilingzhi/SMP2017ECDT-DATA](https://github.com/HITlilingzhi/SMP2017ECDT-DATA)\n\n数据集：\n\n|               | Train |\n| ------------- | ----- |\n| Train samples | 2299  |\n| Dev samples   | 770   |\n| Test samples  | 666   |\n| Domain        | 31    |\n\n论文：[https://arxiv.org/abs/1709.10217  ](https://arxiv.org/abs/1709.10217)\n\n# 文本分类\n\n## 新闻分类\n\n- 今日头条中文新闻（短文本）分类数据集 ：https://github.com/fateleak/toutiao-text-classfication-dataset\n  - 数据规模：共**38万条**，分布于15个分类中。\n  - 采集时间：2018年05月。\n  - 以0.7 0.15 0.15做分割 。\n- 清华新闻分类语料：\n  - 根据新浪新闻RSS订阅频道2005~2011年间的历史数据筛选过滤生成。\n  - 数据量：**74万篇新闻文档**（2.19 GB）\n  - 小数据实验可以筛选类别：体育, 财经, 房产, 家居, 教育, 科技, 时尚, 时政, 游戏, 娱乐\n  - http://thuctc.thunlp.org/#%E8%8E%B7%E5%8F%96%E9%93%BE%E6%8E%A5\n  - rnn和cnn实验：https://github.com/gaussic/text-classification-cnn-rnn\n- 中科大新闻分类语料库：http://www.nlpir.org/?action-viewnews-itemid-145\n\n\n\n## 情感/观点/评论 倾向性分析\n\n| 数据集                  | 数据概览                                                     | 下载                                                         |\n| ----------------------- | ------------------------------------------------------------ | ------------------------------------------------------------ |\n| ChnSentiCorp_htl_all    | 7000 多条酒店评论数据，5000 多条正向评论，2000 多条负向评论  | [地址](https://github.com/SophonPlus/ChineseNlpCorpus/blob/master/datasets/ChnSentiCorp_htl_all/intro.ipynb) |\n| waimai_10k              | 某外卖平台收集的用户评价，正向 4000 条，负向 约 8000 条      | [地址](https://github.com/SophonPlus/ChineseNlpCorpus/blob/master/datasets/waimai_10k/intro.ipynb) |\n| online_shopping_10_cats | 10 个类别，共 6 万多条评论数据，正、负向评论各约 3 万条， 包括书籍、平板、手机、水果、洗发水、热水器、蒙牛、衣服、计算机、酒店 | [地址](https://github.com/SophonPlus/ChineseNlpCorpus/blob/master/datasets/online_shopping_10_cats/intro.ipynb) |\n| weibo_senti_100k        | 10 万多条，带情感标注 新浪微博，正负向评论约各 5 万条        | [地址](https://github.com/SophonPlus/ChineseNlpCorpus/blob/master/datasets/weibo_senti_100k/intro.ipynb) |\n| simplifyweibo_4_moods   | 36 万多条，带情感标注 新浪微博，包含 4 种情感， 其中喜悦约 20 万条，愤怒、厌恶、低落各约 5 万条 | [地址](https://github.com/SophonPlus/ChineseNlpCorpus/blob/master/datasets/simplifyweibo_4_moods/intro.ipynb) |\n| dmsc_v2                 | 28 部电影，超 70 万 用户，超 200 万条 评分/评论 数据         | [地址](https://github.com/SophonPlus/ChineseNlpCorpus/blob/master/datasets/dmsc_v2/intro.ipynb) |\n| yf_dianping             | 24 万家餐馆，54 万用户，440 万条评论/评分数据                | [地址](https://github.com/SophonPlus/ChineseNlpCorpus/blob/master/datasets/yf_dianping/intro.ipynb) |\n| yf_amazon               | 52 万件商品，1100 多个类目，142 万用户，720 万条评论/评分数据 | [地址](https://github.com/SophonPlus/ChineseNlpCorpus/blob/master/datasets/yf_amazon/intro.ipynb) |\n| 百度千言情感分析数据集  | 包括句子级情感分类（Sentence-level Sentiment Classification）、评价对象级情感分类（Aspect-level Sentiment Classification）、观点抽取（Opinion Target Extraction） | [地址](https://aistudio.baidu.com/aistudio/competition/detail/50/?isFromLUGE=TRUE) |\n\n\n\n\n\n# 实体识别&词性标注&分词\n\n- ## 微博实体识别.\n\n  - https://github.com/hltcoe/golden-horse\n\n- ## boson数据。\n\n  - 包含6种实体类型。\n  - https://github.com/InsaneLife/ChineseNLPCorpus/tree/master/NER/boson\n\n- ## 人民日报数据集。\n\n  - 人名、地名、组织名三种实体类型 \n  - 1998：[https://github.com/InsaneLife/ChineseNLPCorpus/tree/master/NER/renMinRiBao](https://github.com/InsaneLife/ChineseNLPCorpus/tree/master/NER/renMinRiBao) \n  - 2004：https://pan.baidu.com/s/1LDwQjoj7qc-HT9qwhJ3rcA password: 1fa3\n- ## MSRA微软亚洲研究院数据集。\n\n  - 5 万多条中文命名实体识别标注数据（包括地点、机构、人物） \n  - https://github.com/InsaneLife/ChineseNLPCorpus/tree/master/NER/MSRA\n\n- SIGHAN Bakeoff 2005：一共有四个数据集，包含繁体中文和简体中文，下面是简体中文分词数据。\n\n  -  MSR: <http://sighan.cs.uchicago.edu/bakeoff2005/>\n  -  PKU ：<http://sighan.cs.uchicago.edu/bakeoff2005/> \n\n另外这三个链接里面数据集也挺全的，链接：\n\n- [分词](https://github.com/luge-ai/luge-ai/blob/master/lexical-analysis/word-segment.md)\n- [词性标注](https://github.com/luge-ai/luge-ai/blob/master/lexical-analysis/part-of-speech-tagging.md)\n- [命名实体](https://github.com/luge-ai/luge-ai/blob/master/lexical-analysis/name-entity-recognition.md)\n\n# 句法&语义解析\n\n## [依存句法](https://github.com/luge-ai/luge-ai/blob/master/dependency-parsing/dependency-parsing.md)\n\n## 语义解析\n\n- 看方法主要还是转化为分类和ner任务。下载地址：[https://aistudio.baidu.com/aistudio/competition/detail/47/?isFromLUGE=TRUE](https://aistudio.baidu.com/aistudio/competition/detail/47/?isFromLUGE=TRUE)\n\n| 数据集  | 单/多表 | 语言 | 复杂度 | 数据库/表格 | 训练集 | 验证集 | 测试集 | 文档                                                         |\n| :-----: | :-----: | :--: | :----: | :---------: | :----: | :----: | :----: | ------------------------------------------------------------ |\n| NL2SQL  |   单    | 中文 |  简单  | 5,291/5,291 | 41,522 | 4,396  | 8,141  | [NL2SQL](https://arxiv.org/abs/2006.06434)                   |\n| CSpider |   多    | 中英 |  复杂  |   166/876   | 6,831  |  954   | 1,906  | [CSpider](https://arxiv.org/abs/1909.13293)                  |\n|  DuSQL  |   多    | 中文 |  复杂  |   200/813   | 22,521 | 2,482  | 3,759  | [DuSQL](https://www.aclweb.org/anthology/2020.emnlp-main.562.pdf) |\n\n\n\n# 信息抽取\n\n- [实体链指](https://github.com/luge-ai/luge-ai/blob/master/information-extraction/entity_linking.md)\n- [关系抽取](https://github.com/luge-ai/luge-ai/blob/master/information-extraction/relation-extraction.md)\n- [事件抽取](https://github.com/luge-ai/luge-ai/blob/master/information-extraction/event-extraction.md)\n\n# 搜索匹配\n\n## 千言文本相似度\n\n百度千言文本相似度，主要包含LCQMC/BQ Corpus/PAWS-X，见[官网](https://aistudio.baidu.com/aistudio/competition/detail/45/?isFromLUGE=TRUE)，丰富文本匹配的数据，可以作为目标匹配数据集的源域数据，进行多任务学习/迁移学习。\n\n## OPPO手机搜索排序\n\nOPPO手机搜索排序query-title语义匹配数据集。\n\n链接: https://pan.baidu.com/s/1KzLK_4Iv0CHOkkut7TJBkA?pwd=ju52 提取码: ju52 \n\n## 网页搜索结果评价(SogouE)\n\n- 用户查询及相关URL列表 \n\n- https://www.sogou.com/labs/resource/e.php\n\n# 推荐系统\n\n| 数据集      | 数据概览                                                     | 下载地址                                                     |\n| ----------- | ------------------------------------------------------------ | ------------------------------------------------------------ |\n| ez_douban   | 5 万多部电影（3 万多有电影名称，2 万多没有电影名称），2.8 万 用户，280 万条评分数据 | [点击查看](https://github.com/SophonPlus/ChineseNlpCorpus/blob/master/datasets/ez_douban/intro.ipynb) |\n| dmsc_v2     | 28 部电影，超 70 万 用户，超 200 万条 评分/评论 数据         | [点击查看](https://github.com/SophonPlus/ChineseNlpCorpus/blob/master/datasets/dmsc_v2/intro.ipynb) |\n| yf_dianping | 24 万家餐馆，54 万用户，440 万条评论/评分数据                | [点击查看](https://github.com/SophonPlus/ChineseNlpCorpus/blob/master/datasets/yf_dianping/intro.ipynb) |\n| yf_amazon   | 52 万件商品，1100 多个类目，142 万用户，720 万条评论/评分数据 | [点击查看](https://github.com/SophonPlus/ChineseNlpCorpus/blob/master/datasets/yf_amazon/intro.ipynb) |\n\n# 百科数据 \n\n## 维基百科\n\n维基百科会定时将语料库打包发布：\n\n- [数据处理博客](https://blog.csdn.net/wangyangzhizhou/article/details/78348949)\n- https://dumps.wikimedia.org/zhwiki/\n\n## 百度百科\n\n只能自己爬，爬取得链接：`https://pan.baidu.com/share/init?surl=i3wvfil`提取码 neqs 。 \n\n\n\n# 指代消歧 \n\nCoNLL 2012 ：<http://conll.cemantix.org/2012/data.html> \n\n# 预训练：（词向量or模型）\n\n## BERT\n\n1. 开源代码：https://github.com/google-research/bert\n2. 模型下载：[**BERT-Base, Chinese**](https://storage.googleapis.com/bert_models/2018_11_03/chinese_L-12_H-768_A-12.zip): Chinese Simplified and Traditional, 12-layer, 768-hidden, 12-heads, 110M parameters\n\nBERT变种模型：\n\n| 模型                                                         | 参数 | git                                                          |\n| ------------------------------------------------------------ | ---- | ------------------------------------------------------------ |\n| [Chinese-BERT-base](https://storage.googleapis.com/bert_models/2018_11_03/chinese_L-12_H-768_A-12.zip) | 108M | [BERT](https://github.com/google-research/bert)              |\n| [Chinese-BERT-wwm-ext](https://drive.google.com/open?id=1Jzn1hYwmv0kXkfTeIvNT61Rn1IbRc-o8) | 108M | [Chinese-BERT-wwm](https://github.com/ymcui/Chinese-BERT-wwm) |\n| [RBT3](https://drive.google.com/open?id=1-rvV0nBDvRCASbRz8M9Decc3_8Aw-2yi) | 38M  | [Chinese-BERT-wwm](https://github.com/ymcui/Chinese-BERT-wwm) |\n| [ERNIE 1.0 Base 中文](https://ernie-github.cdn.bcebos.com/model-ernie1.0.1.tar.gz) | 108M | [ERNIE](https://github.com/PaddlePaddle/ERNIE)、ernie模型转成tensorflow模型:[tensorflow_ernie](https://github.com/ArthurRizar/tensorflow_ernie) |\n| [RoBERTa-large](https://drive.google.com/open?id=1W3WgPJWGVKlU9wpUYsdZuurAIFKvrl_Y) | 334M | [RoBERT](https://github.com/brightmart/roberta_zh)           |\n| [XLNet-mid](https://drive.google.com/open?id=1342uBc7ZmQwV6Hm6eUIN_OnBSz1LcvfA) | 209M | [XLNet-mid](https://github.com/ymcui/Chinese-PreTrained-XLNet) |\n| [ALBERT-large](https://storage.googleapis.com/albert_zh/albert_large_zh.zip) | 59M  | [Chinese-ALBERT](https://github.com/brightmart/albert_zh)    |\n| [ALBERT-xlarge](https://storage.googleapis.com/albert_zh/albert_xlarge_zh_183k.zip) |      | [Chinese-ALBERT](https://github.com/brightmart/albert_zh)    |\n| [ALBERT-tiny](https://storage.googleapis.com/albert_zh/albert_tiny_489k.zip) | 4M   | [Chinese-ALBERT](https://github.com/brightmart/albert_zh)    |\n| [chinese-roberta-wwm-ext](https://www.paddlepaddle.org.cn/hubdetail?name=chinese-roberta-wwm-ext&en_category=SemanticModel) | 108M | [Chinese-BERT-wwm](https://github.com/ymcui/Chinese-BERT-wwm) |\n| [chinese-roberta-wwm-ext-large](https://www.paddlepaddle.org.cn/hubdetail?name=chinese-roberta-wwm-ext-large&en_category=SemanticModel) | 330M | [Chinese-BERT-wwm](https://github.com/ymcui/Chinese-BERT-wwm) |\n\n## ELMO\n\n1. 开源代码：https://github.com/allenai/bilm-tf\n2. 预训练的模型：https://allennlp.org/elmo\n\n## 腾讯词向量\n\n腾讯AI实验室公开的中文词向量数据集包含800多万中文词汇，其中每个词对应一个200维的向量。\n\n- 下载地址：~~https://ai.tencent.com/ailab/nlp/embedding.html~~，网页已经失效，有网盘链接同学希望分享下\n\n下载地址：[https://ai.tencent.com/ailab/nlp/en/download.html](https://ai.tencent.com/ailab/nlp/en/download.html)\n\n\n## **上百种预训练中文词向量**\n\n[https://github.com/Embedding/Chinese-Word-Vectors](https://link.zhihu.com/?target=https%3A//github.com/Embedding/Chinese-Word-Vectors)\n\n# **中文完形填空数据集**\n\n[https://github.com/ymcui/Chinese-RC-Dataset](https://link.zhihu.com/?target=https%3A//github.com/ymcui/Chinese-RC-Dataset)\n\n\n\n# **中华古诗词数据库**\n\n最全中华古诗词数据集，唐宋两朝近一万四千古诗人, 接近5.5万首唐诗加26万宋诗. 两宋时期1564位词人，21050首词。\n\n[https://github.com/chinese-poetry/chinese-poetry](https://link.zhihu.com/?target=https%3A//github.com/chinese-poetry/chinese-poetry)\n\n\n\n \n\n# **保险行业语料库**\n\n[https://github.com/Samurais/insuranceqa-corpus-zh](https://link.zhihu.com/?target=https%3A//github.com/Samurais/insuranceqa-corpus-zh)\n\n \n\n# **汉语拆字字典**\n\n英文可以做char embedding，中文不妨可以试试拆字\n\n[https://github.com/kfcd/chaizi](https://link.zhihu.com/?target=https%3A//github.com/kfcd/chaizi)\n\n \n\n\n\n# 中文数据集平台\n\n- ## **搜狗实验室**\n\n  搜狗实验室提供了一些高质量的中文文本数据集，时间比较早，多为2012年以前的数据。\n\n  [https://www.sogou.com/labs/resource/list_pingce.php](https://link.zhihu.com/?target=https%3A//www.sogou.com/labs/resource/list_pingce.php)\n\n- ## **中科大自然语言处理与信息检索共享平台**\n\n  [http://www.nlpir.org/?action-category-catid-28](https://link.zhihu.com/?target=http%3A//www.nlpir.org/%3Faction-category-catid-28)\n\n- ## 中文语料小数据\n\n  - 包含了中文命名实体识别、中文关系识别、中文阅读理解等一些小量数据。 \n  - https://github.com/crownpku/Small-Chinese-Corpus\n\n- ## 维基百科数据集\n\n  - https://dumps.wikimedia.org/\n  - 中文维基百科23万条高质量词条数据集（更新至2307）：https://huggingface.co/datasets/pleisto/wikipedia-cn-20230720-filtered\n\n\n\n# NLP工具\n\nTHULAC： [https://github.com/thunlp/THULAC](<https://github.com/thunlp/THULAC> ) ：包括中文分词、词性标注功能。\n\nHanLP：<https://github.com/hankcs/HanLP> \n\n哈工大LTP <https://github.com/HIT-SCIR/ltp> \n\nNLPIR <https://github.com/NLPIR-team/NLPIR>\n\njieba <https://github.com/yanyiwu/cppjieba> \n\n百度千言数据集：[https://github.com/luge-ai/luge-ai](https://github.com/luge-ai/luge-ai)\n"
        },
        {
          "name": "toutiao text classfication dataset",
          "type": "tree",
          "content": null
        },
        {
          "name": "word_vector",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}