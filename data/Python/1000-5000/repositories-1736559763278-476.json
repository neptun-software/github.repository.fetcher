{
  "metadata": {
    "timestamp": 1736559763278,
    "page": 476,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjQ4MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "andabi/deep-voice-conversion",
      "stars": 3929,
      "defaultBranch": "master",
      "files": [
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.0673828125,
          "content": ".idea\nlogdir*\n*.py[cod]\ndatasets\n.DS_Store\noutputs\n__*\nhparams\ntools\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 1.048828125,
          "content": "The MIT License (MIT) Copyright (c) 2018 Dabi Ahn\n\nPermission is hereby granted, free of charge, to any person obtaining a copy of\nthis software and associated documentation files (the \"Software\"), to deal in\nthe Software without restriction, including without limitation the rights to\nuse, copy, modify, merge, publish, distribute, sublicense, and/or sell copies\nof the Software, and to permit persons to whom the Software is furnished to do\nso, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 5.4541015625,
          "content": "# Voice Conversion with Non-Parallel Data\r\n## Subtitle: Speaking like Kate Winslet\r\n> Authors: Dabi Ahn(andabi412@gmail.com), [Kyubyong Park](https://github.com/Kyubyong)(kbpark.linguist@gmail.com)\r\n\r\n## Samples\r\nhttps://soundcloud.com/andabi/sets/voice-style-transfer-to-kate-winslet-with-deep-neural-networks\r\n\r\n## Intro\r\nWhat if you could imitate a famous celebrity's voice or sing like a famous singer?\r\nThis project started with a goal to convert someone's voice to a specific target voice.\r\nSo called, it's voice style transfer.\r\nWe worked on this project that aims to convert someone's voice to a famous English actress [Kate Winslet](https://en.wikipedia.org/wiki/Kate_Winslet)'s \r\n[voice](https://soundcloud.com/andabi/sets/voice-style-transfer-to-kate-winslet-with-deep-neural-networks).\r\nWe implemented a deep neural networks to achieve that and more than 2 hours of audio book sentences read by Kate Winslet are used as a dataset.\r\n\r\n<p align=\"center\"><img src=\"https://raw.githubusercontent.com/andabi/deep-voice-conversion/master/materials/title.png\" width=\"50%\"></p>\r\n\r\n## Model Architecture\r\nThis is a many-to-one voice conversion system.\r\nThe main significance of this work is that we could generate a target speaker's utterances without parallel data like <source's wav, target's wav>, <wav, text> or <wav, phone>, but only waveforms of the target speaker.\r\n(To make these parallel datasets needs a lot of effort.)\r\nAll we need in this project is a number of waveforms of the target speaker's utterances and only a small set of <wav, phone> pairs from a number of anonymous speakers.\r\n\r\n<p align=\"center\"><img src=\"https://raw.githubusercontent.com/andabi/deep-voice-conversion/master/materials/architecture.png\" width=\"85%\"></p>\r\n\r\nThe model architecture consists of two modules:\r\n1. Net1(phoneme classification) classify someone's utterances to one of phoneme classes at every timestep.\r\n    * Phonemes are speaker-independent while waveforms are speaker-dependent.\r\n2. Net2(speech synthesis) synthesize speeches of the target speaker from the phones.\r\n\r\nWe applied CBHG(1-D convolution bank + highway network + bidirectional GRU) modules that are mentioned in [Tacotron](https://arxiv.org/abs/1703.10135).\r\nCBHG is known to be good for capturing features from sequential data.\r\n\r\n### Net1 is a classifier.\r\n* Process: wav -> spectrogram -> mfccs -> phoneme dist.\r\n* Net1 classifies spectrogram to phonemes that consists of 60 English phonemes at every timestep.\r\n  * For each timestep, the input is log magnitude spectrogram and the target is phoneme dist.\r\n* Objective function is cross entropy loss.\r\n* [TIMIT dataset](https://catalog.ldc.upenn.edu/LDC93S1) used.\r\n  * contains 630 speakers' utterances and corresponding phones that speaks similar sentences.\r\n* Over 70% test accuracy\r\n\r\n### Net2 is a synthesizer.\r\nNet2 contains Net1 as a sub-network.\r\n* Process: net1(wav -> spectrogram -> mfccs -> phoneme dist.) -> spectrogram -> wav\r\n* Net2 synthesizes the target speaker's speeches.\r\n  * The input/target is a set of target speaker's utterances.\r\n* Since Net1 is already trained in previous step, the remaining part only should be trained in this step.\r\n* Loss is reconstruction error between input and target. (L2 distance)\r\n* Datasets\r\n    * Target1(anonymous female): [Arctic](http://www.festvox.org/cmu_arctic/) dataset (public)\r\n    * Target2(Kate Winslet): over 2 hours of audio book sentences read by her (private)\r\n* Griffin-Lim reconstruction when reverting wav from spectrogram.\r\n\r\n## Implementations\r\n### Requirements\r\n* python 2.7\r\n* tensorflow >= 1.1\r\n* numpy >= 1.11.1\r\n* librosa == 0.5.1\r\n\r\n### Settings\r\n* sample rate: 16,000Hz\r\n* window length: 25ms\r\n* hop length: 5ms\r\n\r\n### Procedure\r\n* Train phase: Net1 and Net2 should be trained sequentially.\r\n  * Train1(training Net1)\r\n    * Run `train1.py` to train and `eval1.py` to test.\r\n  * Train2(training Net2)\r\n    * Run `train2.py` to train and `eval2.py` to test.\r\n      * Train2 should be trained after Train1 is done!\r\n* Convert phase: feed forward to Net2\r\n    * Run `convert.py` to get result samples.\r\n    * Check Tensorboard's audio tab to listen the samples.\r\n    * Take a look at phoneme dist. visualization on Tensorboard's image tab.\r\n      * x-axis represents phoneme classes and y-axis represents timesteps\r\n      * the first class of x-axis means silence.\r\n\r\n<p align=\"center\"><img src=\"https://raw.githubusercontent.com/andabi/deep-voice-conversion/master/materials/phoneme_dist.png\" width=\"30%\"></p>\r\n\r\n## Tips (Lessons We've learned from this project)\r\n* Window length and hop length have to be small enough to be able to fit in only a phoneme.\r\n* Obviously, sample rate, window length and hop length should be same in both Net1 and Net2.\r\n* Before ISTFT(spectrogram to waveforms), emphasizing on the predicted spectrogram by applying power of 1.0~2.0 is helpful for removing noisy sound.\r\n* It seems that to apply temperature to softmax in Net1 is not so meaningful.\r\n* IMHO, the accuracy of Net1(phoneme classification) does not need to be so perfect.\r\n  * Net2 can reach to near optimal when Net1 accuracy is correct to some extent.\r\n\r\n## References\r\n* [\"Phonetic posteriorgrams for many-to-one voice conversion without parallel data training\"](https://www.researchgate.net/publication/307434911_Phonetic_posteriorgrams_for_many-to-one_voice_conversion_without_parallel_data_training), 2016 IEEE International Conference on Multimedia and Expo (ICME)\r\n* [\"TACOTRON: TOWARDS END-TO-END SPEECH SYNTHESIS\"](https://arxiv.org/abs/1703.10135), Submitted to Interspeech 2017\r\n"
        },
        {
          "name": "audio.py",
          "type": "blob",
          "size": 10.056640625,
          "content": "# -*- coding: utf-8 -*-\n# !/usr/bin/env python\n\nfrom scipy import signal\nfrom pydub import AudioSegment\nimport os\nimport librosa\nimport soundfile as sf\nimport numpy as np\n\n\ndef read_wav(path, sr, duration=None, mono=True):\n    wav, _ = librosa.load(path, mono=mono, sr=sr, duration=duration)\n    return wav\n\n\ndef write_wav(wav, sr, path, format='wav', subtype='PCM_16'):\n    sf.write(path, wav, sr, format=format, subtype=subtype)\n\n\ndef read_mfcc(prefix):\n    filename = '{}.mfcc.npy'.format(prefix)\n    mfcc = np.load(filename)\n    return mfcc\n\n\ndef write_mfcc(prefix, mfcc):\n    filename = '{}.mfcc'.format(prefix)\n    np.save(filename, mfcc)\n\n\ndef read_spectrogram(prefix):\n    filename = '{}.spec.npy'.format(prefix)\n    spec = np.load(filename)\n    return spec\n\n\ndef write_spectrogram(prefix, spec):\n    filename = '{}.spec'.format(prefix)\n    np.save(filename, spec)\n\n\ndef split_wav(wav, top_db):\n    intervals = librosa.effects.split(wav, top_db=top_db)\n    wavs = map(lambda i: wav[i[0]: i[1]], intervals)\n    return wavs\n\n\ndef trim_wav(wav):\n    wav, _ = librosa.effects.trim(wav)\n    return wav\n\n\ndef fix_length(wav, length):\n    if len(wav) != length:\n        wav = librosa.util.fix_length(wav, length)\n    return wav\n\n\ndef crop_random_wav(wav, length):\n    \"\"\"\n    Randomly cropped a part in a wav file.\n    :param wav: a waveform\n    :param length: length to be randomly cropped.\n    :return: a randomly cropped part of wav.\n    \"\"\"\n    assert (wav.ndim <= 2)\n    assert (type(length) == int)\n\n    wav_len = wav.shape[-1]\n    start = np.random.choice(range(np.maximum(1, wav_len - length)), 1)[0]\n    end = start + length\n    if wav.ndim == 1:\n        wav = wav[start:end]\n    else:\n        wav = wav[:, start:end]\n    return wav\n\n\ndef mp3_to_wav(src_path, tar_path):\n    \"\"\"\n    Read mp3 file from source path, convert it to wav and write it to target path. \n    Necessary libraries: ffmpeg, libav.\n\n    :param src_path: source mp3 file path\n    :param tar_path: target wav file path\n    \"\"\"\n    basepath, filename = os.path.split(src_path)\n    os.chdir(basepath)\n    AudioSegment.from_mp3(src_path).export(tar_path, format='wav')\n\n\ndef prepro_audio(source_path, target_path, format=None, sr=None, db=None):\n    \"\"\"\n    Read a wav, change sample rate, format, and average decibel and write to target path.\n    :param source_path: source wav file path\n    :param target_path: target wav file path\n    :param sr: sample rate.\n    :param format: output audio format.\n    :param db: decibel.\n    \"\"\"\n    sound = AudioSegment.from_file(source_path, format)\n    if sr:\n        sound = sound.set_frame_rate(sr)\n    if db:\n        change_dBFS = db - sound.dBFS\n        sound = sound.apply_gain(change_dBFS)\n    sound.export(target_path, 'wav')\n\n\ndef _split_path(path):\n    \"\"\"\n    Split path to basename, filename and extension. For example, 'a/b/c.wav' => ('a/b', 'c', 'wav')\n    :param path: file path\n    :return: basename, filename, and extension\n    \"\"\"\n    basepath, filename = os.path.split(path)\n    filename, extension = os.path.splitext(filename)\n    return basepath, filename, extension\n\n\ndef wav2spec(wav, n_fft, win_length, hop_length, time_first=True):\n    \"\"\"\n    Get magnitude and phase spectrogram from waveforms.\n\n    Parameters\n    ----------\n    wav : np.ndarray [shape=(n,)]\n        The real-valued waveform.\n\n    n_fft : int > 0 [scalar]\n        FFT window size.\n\n    win_length  : int <= n_fft [scalar]\n        The window will be of length `win_length` and then padded\n        with zeros to match `n_fft`.\n\n    hop_length : int > 0 [scalar]\n        Number audio of frames between STFT columns.\n\n    time_first : boolean. optional.\n        if True, time axis is followed by bin axis. In this case, shape of returns is (t, 1 + n_fft/2)\n\n    Returns\n    -------\n    mag : np.ndarray [shape=(t, 1 + n_fft/2) or (1 + n_fft/2, t)]\n        Magnitude spectrogram.\n\n    phase : np.ndarray [shape=(t, 1 + n_fft/2) or (1 + n_fft/2, t)]\n        Phase spectrogram.\n\n    \"\"\"\n    stft = librosa.stft(y=wav, n_fft=n_fft, hop_length=hop_length, win_length=win_length)\n    mag = np.abs(stft)\n    phase = np.angle(stft)\n\n    if time_first:\n        mag = mag.T\n        phase = phase.T\n\n    return mag, phase\n\n\ndef spec2wav(mag, n_fft, win_length, hop_length, num_iters=30, phase=None):\n    \"\"\"\n    Get a waveform from the magnitude spectrogram by Griffin-Lim Algorithm.\n\n    Parameters\n    ----------\n    mag : np.ndarray [shape=(1 + n_fft/2, t)]\n        Magnitude spectrogram.\n\n    n_fft : int > 0 [scalar]\n        FFT window size.\n\n    win_length  : int <= n_fft [scalar]\n        The window will be of length `win_length` and then padded\n        with zeros to match `n_fft`.\n\n    hop_length : int > 0 [scalar]\n        Number audio of frames between STFT columns.\n\n    num_iters: int > 0 [scalar]\n        Number of iterations of Griffin-Lim Algorithm.\n\n    phase : np.ndarray [shape=(1 + n_fft/2, t)]\n        Initial phase spectrogram.\n\n    Returns\n    -------\n    wav : np.ndarray [shape=(n,)]\n        The real-valued waveform.\n\n    \"\"\"\n    assert (num_iters > 0)\n    if phase is None:\n        phase = np.pi * np.random.rand(*mag.shape)\n    stft = mag * np.exp(1.j * phase)\n    wav = None\n    for i in range(num_iters):\n        wav = librosa.istft(stft, win_length=win_length, hop_length=hop_length)\n        if i != num_iters - 1:\n            stft = librosa.stft(wav, n_fft=n_fft, win_length=win_length, hop_length=hop_length)\n            _, phase = librosa.magphase(stft)\n            phase = np.angle(phase)\n            stft = mag * np.exp(1.j * phase)\n    return wav\n\n\ndef preemphasis(wav, coeff=0.97):\n    \"\"\"\n    Emphasize high frequency range of the waveform by increasing power(squared amplitude).\n\n    Parameters\n    ----------\n    wav : np.ndarray [shape=(n,)]\n        Real-valued the waveform.\n\n    coeff: float <= 1 [scalar]\n        Coefficient of pre-emphasis.\n\n    Returns\n    -------\n    preem_wav : np.ndarray [shape=(n,)]\n        The pre-emphasized waveform.\n    \"\"\"\n    preem_wav = signal.lfilter([1, -coeff], [1], wav)\n    return preem_wav\n\n\ndef inv_preemphasis(preem_wav, coeff=0.97):\n    \"\"\"\n    Invert the pre-emphasized waveform to the original waveform.\n\n    Parameters\n    ----------\n    preem_wav : np.ndarray [shape=(n,)]\n        The pre-emphasized waveform.\n\n    coeff: float <= 1 [scalar]\n        Coefficient of pre-emphasis.\n\n    Returns\n    -------\n    wav : np.ndarray [shape=(n,)]\n        Real-valued the waveform.\n    \"\"\"\n    wav = signal.lfilter([1], [1, -coeff], preem_wav)\n    return wav\n\n\ndef linear_to_mel(linear, sr, n_fft, n_mels, **kwargs):\n    \"\"\"\n    Convert a linear-spectrogram to mel-spectrogram.\n    :param linear: Linear-spectrogram.\n    :param sr: Sample rate.\n    :param n_fft: FFT window size.\n    :param n_mels: Number of mel filters.\n    :return: Mel-spectrogram.\n    \"\"\"\n    mel_basis = librosa.filters.mel(sr, n_fft, n_mels, **kwargs)  # (n_mels, 1+n_fft//2)\n    mel = np.dot(mel_basis, linear)  # (n_mels, t) # mel spectrogram\n    return mel\n\n\ndef amp2db(amp):\n    return librosa.amplitude_to_db(amp)\n\n\ndef db2amp(db):\n    return librosa.db_to_amplitude(db)\n\n\ndef normalize_db(db, max_db, min_db):\n    \"\"\"\n    Normalize dB-scaled spectrogram values to be in range of 0~1.\n    :param db: Decibel-scaled spectrogram.\n    :param max_db: Maximum dB.\n    :param min_db: Minimum dB.\n    :return: Normalized spectrogram.\n    \"\"\"\n    norm_db = np.clip((db - min_db) / (max_db - min_db), 0, 1)\n    return norm_db\n\n\ndef denormalize_db(norm_db, max_db, min_db):\n    \"\"\"\n    Denormalize the normalized values to be original dB-scaled value.\n    :param norm_db: Normalized spectrogram.\n    :param max_db: Maximum dB.\n    :param min_db: Minimum dB.\n    :return: Decibel-scaled spectrogram.\n    \"\"\"\n    db = np.clip(norm_db, 0, 1) * (max_db - min_db) + min_db\n    return db\n\n\ndef dynamic_range_compression(db, threshold, ratio, method='downward'):\n    \"\"\"\n    Execute dynamic range compression(https://en.wikipedia.org/wiki/Dynamic_range_compression) to dB.\n    :param db: Decibel-scaled magnitudes\n    :param threshold: Threshold dB\n    :param ratio: Compression ratio.\n    :param method: Downward or upward.\n    :return: Range compressed dB-scaled magnitudes\n    \"\"\"\n    if method is 'downward':\n        db[db > threshold] = (db[db > threshold] - threshold) / ratio + threshold\n    elif method is 'upward':\n        db[db < threshold] = threshold - ((threshold - db[db < threshold]) / ratio)\n    return db\n\n\ndef emphasize_magnitude(mag, power=1.2):\n    \"\"\"\n    Emphasize a magnitude spectrogram by applying power function. This is used for removing noise.\n    :param mag: magnitude spectrogram.\n    :param power: exponent.\n    :return: emphasized magnitude spectrogram.\n    \"\"\"\n    emphasized_mag = np.power(mag, power)\n    return emphasized_mag\n\n\ndef wav2melspec(wav, sr, n_fft, win_length, hop_length, n_mels, time_first=True, **kwargs):\n    # Linear spectrogram\n    mag_spec, phase_spec = wav2spec(wav, n_fft, win_length, hop_length, time_first=False)\n\n    # Mel-spectrogram\n    mel_spec = linear_to_mel(mag_spec, sr, n_fft, n_mels, **kwargs)\n\n    # Time-axis first\n    if time_first:\n        mel_spec = mel_spec.T  # (t, n_mels)\n\n    return mel_spec\n\n\ndef wav2melspec_db(wav, sr, n_fft, win_length, hop_length, n_mels, normalize=False, max_db=None, min_db=None,\n                   time_first=True, **kwargs):\n    # Mel-spectrogram\n    mel_spec = wav2melspec(wav, sr, n_fft, win_length, hop_length, n_mels, time_first=False, **kwargs)\n\n    # Decibel\n    mel_db = librosa.amplitude_to_db(mel_spec)\n\n    # Normalization\n    mel_db = normalize_db(mel_db, max_db, min_db) if normalize else mel_db\n\n    # Time-axis first\n    if time_first:\n        mel_db = mel_db.T  # (t, n_mels)\n\n    return mel_db\n\n\ndef wav2mfcc(wav, sr, n_fft, win_length, hop_length, n_mels, n_mfccs, preemphasis_coeff=0.97, time_first=True,\n             **kwargs):\n    # Pre-emphasis\n    wav_preem = preemphasis(wav, coeff=preemphasis_coeff)\n\n    # Decibel-scaled mel-spectrogram\n    mel_db = wav2melspec_db(wav_preem, sr, n_fft, win_length, hop_length, n_mels, time_first=False, **kwargs)\n\n    # MFCCs\n    mfccs = np.dot(librosa.filters.dct(n_mfccs, mel_db.shape[0]), mel_db)\n\n    # Time-axis first\n    if time_first:\n        mfccs = mfccs.T  # (t, n_mfccs)\n\n    return mfccs"
        },
        {
          "name": "convert.py",
          "type": "blob",
          "size": 5.2255859375,
          "content": "# -*- coding: utf-8 -*-\n# /usr/bin/python2\n\n\nfrom __future__ import print_function\n\nimport argparse\n\nfrom models import Net2\nimport numpy as np\nfrom audio import spec2wav, inv_preemphasis, db2amp, denormalize_db\nimport datetime\nimport tensorflow as tf\nfrom hparam import hparam as hp\nfrom data_load import Net2DataFlow\nfrom tensorpack.predict.base import OfflinePredictor\nfrom tensorpack.predict.config import PredictConfig\nfrom tensorpack.tfutils.sessinit import SaverRestore\nfrom tensorpack.tfutils.sessinit import ChainInit\nfrom tensorpack.callbacks.base import Callback\n\n\n# class ConvertCallback(Callback):\n#     def __init__(self, logdir, test_per_epoch=1):\n#         self.df = Net2DataFlow(hp.convert.data_path, hp.convert.batch_size)\n#         self.logdir = logdir\n#         self.test_per_epoch = test_per_epoch\n#\n#     def _setup_graph(self):\n#         self.predictor = self.trainer.get_predictor(\n#             get_eval_input_names(),\n#             get_eval_output_names())\n#\n#     def _trigger_epoch(self):\n#         if self.epoch_num % self.test_per_epoch == 0:\n#             audio, y_audio, _ = convert(self.predictor, self.df)\n#             # self.trainer.monitors.put_scalar('eval/accuracy', acc)\n#\n#             # Write the result\n#             # tf.summary.audio('A', y_audio, hp.default.sr, max_outputs=hp.convert.batch_size)\n#             # tf.summary.audio('B', audio, hp.default.sr, max_outputs=hp.convert.batch_size)\n\n\ndef convert(predictor, df):\n    pred_spec, y_spec, ppgs = predictor(next(df().get_data()))\n\n    # Denormalizatoin\n    pred_spec = denormalize_db(pred_spec, hp.default.max_db, hp.default.min_db)\n    y_spec = denormalize_db(y_spec, hp.default.max_db, hp.default.min_db)\n\n    # Db to amp\n    pred_spec = db2amp(pred_spec)\n    y_spec = db2amp(y_spec)\n\n    # Emphasize the magnitude\n    pred_spec = np.power(pred_spec, hp.convert.emphasis_magnitude)\n    y_spec = np.power(y_spec, hp.convert.emphasis_magnitude)\n\n    # Spectrogram to waveform\n    audio = np.array(map(lambda spec: spec2wav(spec.T, hp.default.n_fft, hp.default.win_length, hp.default.hop_length,\n                                               hp.default.n_iter), pred_spec))\n    y_audio = np.array(map(lambda spec: spec2wav(spec.T, hp.default.n_fft, hp.default.win_length, hp.default.hop_length,\n                                                 hp.default.n_iter), y_spec))\n\n    # Apply inverse pre-emphasis\n    audio = inv_preemphasis(audio, coeff=hp.default.preemphasis)\n    y_audio = inv_preemphasis(y_audio, coeff=hp.default.preemphasis)\n\n    # if hp.convert.one_full_wav:\n    #     # Concatenate to a wav\n    #     y_audio = np.reshape(y_audio, (1, y_audio.size), order='C')\n    #     audio = np.reshape(audio, (1, audio.size), order='C')\n\n    return audio, y_audio, ppgs\n\n\ndef get_eval_input_names():\n    return ['x_mfccs', 'y_spec', 'y_mel']\n\n\ndef get_eval_output_names():\n    return ['pred_spec', 'y_spec', 'ppgs']\n\n\ndef do_convert(args, logdir1, logdir2):\n    # Load graph\n    model = Net2()\n\n    df = Net2DataFlow(hp.convert.data_path, hp.convert.batch_size)\n\n    ckpt1 = tf.train.latest_checkpoint(logdir1)\n    ckpt2 = '{}/{}'.format(logdir2, args.ckpt) if args.ckpt else tf.train.latest_checkpoint(logdir2)\n    session_inits = []\n    if ckpt2:\n        session_inits.append(SaverRestore(ckpt2))\n    if ckpt1:\n        session_inits.append(SaverRestore(ckpt1, ignore=['global_step']))\n    pred_conf = PredictConfig(\n        model=model,\n        input_names=get_eval_input_names(),\n        output_names=get_eval_output_names(),\n        session_init=ChainInit(session_inits))\n    predictor = OfflinePredictor(pred_conf)\n\n    audio, y_audio, ppgs = convert(predictor, df)\n\n    # Write the result\n    tf.summary.audio('A', y_audio, hp.default.sr, max_outputs=hp.convert.batch_size)\n    tf.summary.audio('B', audio, hp.default.sr, max_outputs=hp.convert.batch_size)\n\n    # Visualize PPGs\n    heatmap = np.expand_dims(ppgs, 3)  # channel=1\n    tf.summary.image('PPG', heatmap, max_outputs=ppgs.shape[0])\n\n    writer = tf.summary.FileWriter(logdir2)\n    with tf.Session() as sess:\n        summ = sess.run(tf.summary.merge_all())\n    writer.add_summary(summ)\n    writer.close()\n\n    # session_conf = tf.ConfigProto(\n    #     allow_soft_placement=True,\n    #     device_count={'CPU': 1, 'GPU': 0},\n    #     gpu_options=tf.GPUOptions(\n    #         allow_growth=True,\n    #         per_process_gpu_memory_fraction=0.6\n    #     ),\n    # )\n\n\ndef get_arguments():\n    parser = argparse.ArgumentParser()\n    parser.add_argument('case1', type=str, help='experiment case name of train1')\n    parser.add_argument('case2', type=str, help='experiment case name of train2')\n    parser.add_argument('-ckpt', help='checkpoint to load model.')\n    arguments = parser.parse_args()\n    return arguments\n\n\nif __name__ == '__main__':\n    args = get_arguments()\n    hp.set_hparam_yaml(args.case2)\n    logdir_train1 = '{}/{}/train1'.format(hp.logdir_path, args.case1)\n    logdir_train2 = '{}/{}/train2'.format(hp.logdir_path, args.case2)\n\n    print('case1: {}, case2: {}, logdir1: {}, logdir2: {}'.format(args.case1, args.case2, logdir_train1, logdir_train2))\n\n    s = datetime.datetime.now()\n\n    do_convert(args, logdir1=logdir_train1, logdir2=logdir_train2)\n\n    e = datetime.datetime.now()\n    diff = e - s\n    print(\"Done. elapsed time:{}s\".format(diff.seconds))\n"
        },
        {
          "name": "data_load.py",
          "type": "blob",
          "size": 5.150390625,
          "content": "# -*- coding: utf-8 -*-\n# /usr/bin/python2\n\nimport glob\nimport random\n\nimport librosa\nimport numpy as np\nfrom tensorpack.dataflow.base import RNGDataFlow\nfrom tensorpack.dataflow.common import BatchData\nfrom tensorpack.dataflow import PrefetchData\nfrom audio import read_wav, preemphasis, amp2db\nfrom hparam import hparam as hp\nfrom utils import normalize_0_1\n\n\nclass DataFlow(RNGDataFlow):\n\n    def __init__(self, data_path, batch_size):\n        self.batch_size = batch_size\n        self.wav_files = glob.glob(data_path)\n\n    def __call__(self, n_prefetch=1000, n_thread=1):\n        df = self\n        df = BatchData(df, self.batch_size)\n        df = PrefetchData(df, n_prefetch, n_thread)\n        return df\n\n\nclass Net1DataFlow(DataFlow):\n\n    def get_data(self):\n        while True:\n            wav_file = random.choice(self.wav_files)\n            yield get_mfccs_and_phones(wav_file=wav_file)\n\n\nclass Net2DataFlow(DataFlow):\n\n    def get_data(self):\n        while True:\n            wav_file = random.choice(self.wav_files)\n            yield get_mfccs_and_spectrogram(wav_file)\n\n\ndef load_data(mode):\n    wav_files = glob.glob(getattr(hp, mode).data_path)\n\n    return wav_files\n\n\ndef wav_random_crop(wav, sr, duration):\n    assert (wav.ndim <= 2)\n\n    target_len = sr * duration\n    wav_len = wav.shape[-1]\n    start = np.random.choice(range(np.maximum(1, wav_len - target_len)), 1)[0]\n    end = start + target_len\n    if wav.ndim == 1:\n        wav = wav[start:end]\n    else:\n        wav = wav[:, start:end]\n    return wav\n\n\ndef get_mfccs_and_phones(wav_file, trim=False, random_crop=True):\n\n    '''This is applied in `train1` or `test1` phase.\n    '''\n\n    # Load\n    wav = read_wav(wav_file, sr=hp.default.sr)\n\n    mfccs, _, _ = _get_mfcc_and_spec(wav, hp.default.preemphasis, hp.default.n_fft,\n                                     hp.default.win_length,\n                                     hp.default.hop_length)\n\n    # timesteps\n    num_timesteps = mfccs.shape[0]\n\n    # phones (targets)\n    phn_file = wav_file.replace(\"WAV.wav\", \"PHN\").replace(\"wav\", \"PHN\")\n    phn2idx, idx2phn = load_vocab()\n    phns = np.zeros(shape=(num_timesteps,))\n    bnd_list = []\n    for line in open(phn_file, 'r').read().splitlines():\n        start_point, _, phn = line.split()\n        bnd = int(start_point) // hp.default.hop_length\n        phns[bnd:] = phn2idx[phn]\n        bnd_list.append(bnd)\n\n    # Trim\n    if trim:\n        start, end = bnd_list[1], bnd_list[-1]\n        mfccs = mfccs[start:end]\n        phns = phns[start:end]\n        assert (len(mfccs) == len(phns))\n\n    # Random crop\n    n_timesteps = (hp.default.duration * hp.default.sr) // hp.default.hop_length + 1\n    if random_crop:\n        start = np.random.choice(range(np.maximum(1, len(mfccs) - n_timesteps)), 1)[0]\n        end = start + n_timesteps\n        mfccs = mfccs[start:end]\n        phns = phns[start:end]\n        assert (len(mfccs) == len(phns))\n\n    # Padding or crop\n    mfccs = librosa.util.fix_length(mfccs, n_timesteps, axis=0)\n    phns = librosa.util.fix_length(phns, n_timesteps, axis=0)\n\n    return mfccs, phns\n\n\ndef get_mfccs_and_spectrogram(wav_file, trim=True, random_crop=False):\n    '''This is applied in `train2`, `test2` or `convert` phase.\n    '''\n\n\n    # Load\n    wav, _ = librosa.load(wav_file, sr=hp.default.sr)\n\n    # Trim\n    if trim:\n        wav, _ = librosa.effects.trim(wav, frame_length=hp.default.win_length, hop_length=hp.default.hop_length)\n\n    if random_crop:\n        wav = wav_random_crop(wav, hp.default.sr, hp.default.duration)\n\n    # Padding or crop\n    length = hp.default.sr * hp.default.duration\n    wav = librosa.util.fix_length(wav, length)\n\n    return _get_mfcc_and_spec(wav, hp.default.preemphasis, hp.default.n_fft, hp.default.win_length, hp.default.hop_length)\n\n\n# TODO refactoring\ndef _get_mfcc_and_spec(wav, preemphasis_coeff, n_fft, win_length, hop_length):\n\n    # Pre-emphasis\n    y_preem = preemphasis(wav, coeff=preemphasis_coeff)\n\n    # Get spectrogram\n    D = librosa.stft(y=y_preem, n_fft=n_fft, hop_length=hop_length, win_length=win_length)\n    mag = np.abs(D)\n\n    # Get mel-spectrogram\n    mel_basis = librosa.filters.mel(hp.default.sr, hp.default.n_fft, hp.default.n_mels)  # (n_mels, 1+n_fft//2)\n    mel = np.dot(mel_basis, mag)  # (n_mels, t) # mel spectrogram\n\n    # Get mfccs, amp to db\n    mag_db = amp2db(mag)\n    mel_db = amp2db(mel)\n    mfccs = np.dot(librosa.filters.dct(hp.default.n_mfcc, mel_db.shape[0]), mel_db)\n\n    # Normalization (0 ~ 1)\n    mag_db = normalize_0_1(mag_db, hp.default.max_db, hp.default.min_db)\n    mel_db = normalize_0_1(mel_db, hp.default.max_db, hp.default.min_db)\n\n    return mfccs.T, mag_db.T, mel_db.T  # (t, n_mfccs), (t, 1+n_fft/2), (t, n_mels)\n\n\nphns = ['h#', 'aa', 'ae', 'ah', 'ao', 'aw', 'ax', 'ax-h', 'axr', 'ay', 'b', 'bcl',\n        'ch', 'd', 'dcl', 'dh', 'dx', 'eh', 'el', 'em', 'en', 'eng', 'epi',\n        'er', 'ey', 'f', 'g', 'gcl', 'hh', 'hv', 'ih', 'ix', 'iy', 'jh',\n        'k', 'kcl', 'l', 'm', 'n', 'ng', 'nx', 'ow', 'oy', 'p', 'pau', 'pcl',\n        'q', 'r', 's', 'sh', 't', 'tcl', 'th', 'uh', 'uw', 'ux', 'v', 'w', 'y', 'z', 'zh']\n\n\ndef load_vocab():\n    phn2idx = {phn: idx for idx, phn in enumerate(phns)}\n    idx2phn = {idx: phn for idx, phn in enumerate(phns)}\n\n    return phn2idx, idx2phn\n\n\n"
        },
        {
          "name": "datasets",
          "type": "tree",
          "content": null
        },
        {
          "name": "docker",
          "type": "tree",
          "content": null
        },
        {
          "name": "eval1.py",
          "type": "blob",
          "size": 1.8896484375,
          "content": "# -*- coding: utf-8 -*-\n# /usr/bin/python2\n\nfrom __future__ import print_function\n\nimport argparse\n\nimport tensorflow as tf\n\nfrom data_load import Net1DataFlow, phns, load_vocab\nfrom hparam import hparam as hp\nfrom models import Net1\nfrom utils import plot_confusion_matrix\nfrom tensorpack.predict.config import PredictConfig\nfrom tensorpack.predict.base import OfflinePredictor\nfrom tensorpack.tfutils.sessinit import SaverRestore\n\n\ndef get_eval_input_names():\n    return ['x_mfccs', 'y_ppgs']\n\n\ndef get_eval_output_names():\n    return ['net1/eval/y_ppg_1d', 'net1/eval/pred_ppg_1d',  'net1/eval/summ_loss', 'net1/eval/summ_acc']\n\n\ndef eval(logdir):\n    # Load graph\n    model = Net1()\n\n    # dataflow\n    df = Net1DataFlow(hp.test1.data_path, hp.test1.batch_size)\n\n    ckpt = tf.train.latest_checkpoint(logdir)\n\n    pred_conf = PredictConfig(\n        model=model,\n        input_names=get_eval_input_names(),\n        output_names=get_eval_output_names())\n    if ckpt:\n        pred_conf.session_init = SaverRestore(ckpt)\n    predictor = OfflinePredictor(pred_conf)\n\n    x_mfccs, y_ppgs = next(df().get_data())\n    y_ppg_1d, pred_ppg_1d, summ_loss, summ_acc = predictor(x_mfccs, y_ppgs)\n\n    # plot confusion matrix\n    _, idx2phn = load_vocab()\n    y_ppg_1d = [idx2phn[i] for i in y_ppg_1d]\n    pred_ppg_1d = [idx2phn[i] for i in pred_ppg_1d]\n    summ_cm = plot_confusion_matrix(y_ppg_1d, pred_ppg_1d, phns)\n\n    writer = tf.summary.FileWriter(logdir)\n    writer.add_summary(summ_loss)\n    writer.add_summary(summ_acc)\n    writer.add_summary(summ_cm)\n    writer.close()\n\n\ndef get_arguments():\n    parser = argparse.ArgumentParser()\n    parser.add_argument('case', type=str, help='experiment case name')\n    arguments = parser.parse_args()\n    return arguments\n\n\nif __name__ == '__main__':\n    args = get_arguments()\n    hp.set_hparam_yaml(args.case)\n    logdir = '{}/train1'.format(hp.logdir)\n    eval(logdir=logdir)\n\n    print(\"Done\")"
        },
        {
          "name": "eval2.py",
          "type": "blob",
          "size": 1.9326171875,
          "content": "# -*- coding: utf-8 -*-\n# /usr/bin/python2\n\nfrom __future__ import print_function\n\nimport tensorflow as tf\nfrom models import Net2\nimport argparse\nfrom hparam import hparam as hp\nfrom tensorpack.predict.base import OfflinePredictor\nfrom tensorpack.predict.config import PredictConfig\nfrom tensorpack.tfutils.sessinit import SaverRestore\nfrom tensorpack.tfutils.sessinit import ChainInit\nfrom data_load import Net2DataFlow\n\n\ndef get_eval_input_names():\n    return ['x_mfccs', 'y_spec']\n\n\ndef get_eval_output_names():\n    return ['net2/eval/summ_loss']\n\n\ndef eval(logdir1, logdir2):\n    # Load graph\n    model = Net2()\n\n    # dataflow\n    df = Net2DataFlow(hp.test2.data_path, hp.test2.batch_size)\n\n    ckpt1 = tf.train.latest_checkpoint(logdir1)\n    ckpt2 = tf.train.latest_checkpoint(logdir2)\n    session_inits = []\n    if ckpt2:\n        session_inits.append(SaverRestore(ckpt2))\n    if ckpt1:\n        session_inits.append(SaverRestore(ckpt1, ignore=['global_step']))\n    pred_conf = PredictConfig(\n        model=model,\n        input_names=get_eval_input_names(),\n        output_names=get_eval_output_names(),\n        session_init=ChainInit(session_inits))\n    predictor = OfflinePredictor(pred_conf)\n\n    x_mfccs, y_spec, _ = next(df().get_data())\n    summ_loss, = predictor(x_mfccs, y_spec)\n\n    writer = tf.summary.FileWriter(logdir2)\n    writer.add_summary(summ_loss)\n    writer.close()\n\n\ndef get_arguments():\n    parser = argparse.ArgumentParser()\n    parser.add_argument('case1', type=str, help='experiment case name of train1')\n    parser.add_argument('case2', type=str, help='experiment case name of train2')\n    arguments = parser.parse_args()\n    return arguments\n\n\nif __name__ == '__main__':\n    args = get_arguments()\n    hp.set_hparam_yaml(args.case2)\n    logdir_train1 = '{}/{}/train1'.format(hp.logdir_path, args.case1)\n    logdir_train2 = '{}/{}/train2'.format(hp.logdir_path, args.case2)\n\n    eval(logdir1=logdir_train1, logdir2=logdir_train2)\n\n    print(\"Done\")"
        },
        {
          "name": "hparam.py",
          "type": "blob",
          "size": 1.96875,
          "content": "# -*- coding: utf-8 -*-\n#!/usr/bin/env python\n\nimport yaml\n\n\ndef load_hparam(filename):\n    stream = open(filename, 'r')\n    docs = yaml.load_all(stream)\n    hparam_dict = dict()\n    for doc in docs:\n        for k, v in doc.items():\n            hparam_dict[k] = v\n    return hparam_dict\n\n\ndef merge_dict(user, default):\n    if isinstance(user, dict) and isinstance(default, dict):\n        for k, v in default.items():\n            if k not in user:\n                user[k] = v\n            else:\n                user[k] = merge_dict(user[k], v)\n    return user\n\n\nclass Dotdict(dict):\n    \"\"\"\n    a dictionary that supports dot notation \n    as well as dictionary access notation \n    usage: d = DotDict() or d = DotDict({'val1':'first'})\n    set attributes: d.val2 = 'second' or d['val2'] = 'second'\n    get attributes: d.val2 or d['val2']\n    \"\"\"\n    __getattr__ = dict.__getitem__\n    __setattr__ = dict.__setitem__\n    __delattr__ = dict.__delitem__\n\n    def __init__(self, dct=None):\n        dct = dict() if not dct else dct\n        for key, value in dct.items():\n            if hasattr(value, 'keys'):\n                value = Dotdict(value)\n            self[key] = value\n\n\nclass Hparam(Dotdict):\n\n    def __init__(self):\n        super(Dotdict, self).__init__()\n\n    __getattr__ = Dotdict.__getitem__\n    __setattr__ = Dotdict.__setitem__\n    __delattr__ = Dotdict.__delitem__\n\n    def set_hparam_yaml(self, case, default_file='hparams/default.yaml', user_file='hparams/hparams.yaml'):\n        default_hp = load_hparam(default_file)\n        user_hp = load_hparam(user_file)\n        hp_dict = Dotdict(merge_dict(user_hp[case], default_hp) if case in user_hp else default_hp)\n        for k, v in hp_dict.items():\n            setattr(self, k, v)\n        self._auto_setting(case)\n\n    def _auto_setting(self, case):\n        setattr(self, 'case', case)\n\n        # logdir for a case is automatically set to [logdir_path]/[case]\n        setattr(self, 'logdir', '{}/{}'.format(hparam.logdir_path, case))\n\nhparam = Hparam()"
        },
        {
          "name": "hparams",
          "type": "tree",
          "content": null
        },
        {
          "name": "materials",
          "type": "tree",
          "content": null
        },
        {
          "name": "models.py",
          "type": "blob",
          "size": 6.2734375,
          "content": "# -*- coding: utf-8 -*-\n# !/usr/bin/env python\n\nimport tensorflow as tf\nfrom tensorpack.graph_builder.model_desc import ModelDesc, InputDesc\nfrom tensorpack.tfutils import (\n    get_current_tower_context, optimizer, gradproc)\nfrom tensorpack.tfutils.scope_utils import auto_reuse_variable_scope\n\nimport tensorpack_extension\nfrom data_load import phns\nfrom hparam import hparam as hp\nfrom modules import prenet, cbhg, normalize\n\n\nclass Net1(ModelDesc):\n    def __init__(self):\n        pass\n\n    def _get_inputs(self):\n        return [InputDesc(tf.float32, (None, None, hp.default.n_mfcc), 'x_mfccs'),\n                InputDesc(tf.int32, (None, None,), 'y_ppgs')]\n\n    def _build_graph(self, inputs):\n        self.x_mfccs, self.y_ppgs = inputs\n        is_training = get_current_tower_context().is_training\n        with tf.variable_scope('net1'):\n            self.ppgs, self.preds, self.logits = self.network(self.x_mfccs, is_training)\n        self.cost = self.loss()\n        acc = self.acc()\n\n        # summaries\n        tf.summary.scalar('net1/train/loss', self.cost)\n        tf.summary.scalar('net1/train/acc', acc)\n\n        if not is_training:\n            # summaries\n            tf.summary.scalar('net1/eval/summ_loss', self.cost)\n            tf.summary.scalar('net1/eval/summ_acc', acc)\n\n            # for confusion matrix\n            tf.reshape(self.y_ppgs, shape=(tf.size(self.y_ppgs),), name='net1/eval/y_ppg_1d')\n            tf.reshape(self.preds, shape=(tf.size(self.preds),), name='net1/eval/pred_ppg_1d')\n\n    def _get_optimizer(self):\n        lr = tf.get_variable('learning_rate', initializer=hp.train1.lr, trainable=False)\n        return tf.train.AdamOptimizer(lr)\n\n    @auto_reuse_variable_scope\n    def network(self, x_mfcc, is_training):\n        # Pre-net\n        prenet_out = prenet(x_mfcc,\n                            num_units=[hp.train1.hidden_units, hp.train1.hidden_units // 2],\n                            dropout_rate=hp.train1.dropout_rate,\n                            is_training=is_training)  # (N, T, E/2)\n\n        # CBHG\n        out = cbhg(prenet_out, hp.train1.num_banks, hp.train1.hidden_units // 2,\n                   hp.train1.num_highway_blocks, hp.train1.norm_type, is_training)\n\n        # Final linear projection\n        logits = tf.layers.dense(out, len(phns))  # (N, T, V)\n        ppgs = tf.nn.softmax(logits / hp.train1.t, name='ppgs')  # (N, T, V)\n        preds = tf.to_int32(tf.argmax(logits, axis=-1))  # (N, T)\n\n        return ppgs, preds, logits\n\n    def loss(self):\n        istarget = tf.sign(tf.abs(tf.reduce_sum(self.x_mfccs, -1)))  # indicator: (N, T)\n        loss = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=self.logits / hp.train1.t,\n                                                              labels=self.y_ppgs)\n        loss *= istarget\n        loss = tf.reduce_mean(loss)\n        return loss\n\n    def acc(self):\n        istarget = tf.sign(tf.abs(tf.reduce_sum(self.x_mfccs, -1)))  # indicator: (N, T)\n        num_hits = tf.reduce_sum(tf.to_float(tf.equal(self.preds, self.y_ppgs)) * istarget)\n        num_targets = tf.reduce_sum(istarget)\n        acc = num_hits / num_targets\n        return acc\n\n\nclass Net2(ModelDesc):\n\n    def _get_inputs(self):\n        n_timesteps = (hp.default.duration * hp.default.sr) // hp.default.hop_length + 1\n\n        return [InputDesc(tf.float32, (None, n_timesteps, hp.default.n_mfcc), 'x_mfccs'),\n                InputDesc(tf.float32, (None, n_timesteps, hp.default.n_fft // 2 + 1), 'y_spec'),\n                InputDesc(tf.float32, (None, n_timesteps, hp.default.n_mels), 'y_mel'), ]\n\n    def _build_graph(self, inputs):\n        self.x_mfcc, self.y_spec, self.y_mel = inputs\n\n        is_training = get_current_tower_context().is_training\n\n        # build net1\n        self.net1 = Net1()\n        with tf.variable_scope('net1'):\n            self.ppgs, _, _ = self.net1.network(self.x_mfcc, is_training)\n        self.ppgs = tf.identity(self.ppgs, name='ppgs')\n\n        # build net2\n        with tf.variable_scope('net2'):\n            self.pred_spec, self.pred_mel = self.network(self.ppgs, is_training)\n        self.pred_spec = tf.identity(self.pred_spec, name='pred_spec')\n\n        self.cost = self.loss()\n\n        # summaries\n        tf.summary.scalar('net2/train/loss', self.cost)\n\n        if not is_training:\n            tf.summary.scalar('net2/eval/summ_loss', self.cost)\n\n    def _get_optimizer(self):\n        gradprocs = [\n            tensorpack_extension.FilterGradientVariables('.*net2.*', verbose=False),\n            gradproc.MapGradient(\n                lambda grad: tf.clip_by_value(grad, hp.train2.clip_value_min, hp.train2.clip_value_max)),\n            gradproc.GlobalNormClip(hp.train2.clip_norm),\n            # gradproc.PrintGradient(),\n            # gradproc.CheckGradient(),\n        ]\n        lr = tf.get_variable('learning_rate', initializer=hp.train2.lr, trainable=False)\n        opt = tf.train.AdamOptimizer(learning_rate=lr)\n        return optimizer.apply_grad_processors(opt, gradprocs)\n\n    @auto_reuse_variable_scope\n    def network(self, ppgs, is_training):\n        # Pre-net\n        prenet_out = prenet(ppgs,\n                            num_units=[hp.train2.hidden_units, hp.train2.hidden_units // 2],\n                            dropout_rate=hp.train2.dropout_rate,\n                            is_training=is_training)  # (N, T, E/2)\n\n        # CBHG1: mel-scale\n        pred_mel = cbhg(prenet_out, hp.train2.num_banks, hp.train2.hidden_units // 2,\n                        hp.train2.num_highway_blocks, hp.train2.norm_type, is_training,\n                        scope=\"cbhg_mel\")\n        pred_mel = tf.layers.dense(pred_mel, self.y_mel.shape[-1], name='pred_mel')  # (N, T, n_mels)\n\n        # CBHG2: linear-scale\n        pred_spec = tf.layers.dense(pred_mel, hp.train2.hidden_units // 2)  # (N, T, n_mels)\n        pred_spec = cbhg(pred_spec, hp.train2.num_banks, hp.train2.hidden_units // 2,\n                   hp.train2.num_highway_blocks, hp.train2.norm_type, is_training, scope=\"cbhg_linear\")\n        pred_spec = tf.layers.dense(pred_spec, self.y_spec.shape[-1], name='pred_spec')  # log magnitude: (N, T, 1+n_fft//2)\n\n        return pred_spec, pred_mel\n\n    def loss(self):\n        loss_spec = tf.reduce_mean(tf.squared_difference(self.pred_spec, self.y_spec))\n        loss_mel = tf.reduce_mean(tf.squared_difference(self.pred_mel, self.y_mel))\n        loss = loss_spec + loss_mel\n        return loss"
        },
        {
          "name": "modules.py",
          "type": "blob",
          "size": 13.8076171875,
          "content": "# -*- coding: utf-8 -*-\n#/usr/bin/python2\n\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\n\ndef embed(inputs, vocab_size, num_units, zero_pad=True, scope=\"embedding\", reuse=None):\n    '''Embeds a given tensor. \n    \n    Args:\n      inputs: A `Tensor` with type `int32` or `int64` containing the ids\n         to be looked up in `lookup table`.\n      vocab_size: An int. Vocabulary size.\n      num_units: An int. Number of embedding hidden units.\n      zero_pad: A boolean. If True, all the values of the fist row (id 0)\n        should be constant zeros.\n      scope: Optional scope for `variable_scope`.  \n      reuse: Boolean, whether to reuse the weights of a previous layer\n        by the same name.\n        \n    Returns:\n      A `Tensor` with one more rank than inputs's. The last dimesionality\n        should be `num_units`.\n    '''\n    with tf.variable_scope(scope, reuse=reuse):\n        lookup_table = tf.get_variable('lookup_table', \n                                       dtype=tf.float32, \n                                       shape=[vocab_size, num_units],\n                                       initializer=tf.truncated_normal_initializer(mean=0.0, stddev=0.01))\n        if zero_pad:\n            lookup_table = tf.concat((tf.zeros(shape=[1, num_units]), \n                                      lookup_table[1:, :]), 0)\n    return tf.nn.embedding_lookup(lookup_table, inputs)   \n \ndef normalize(inputs, \n              type=\"bn\",\n              decay=.999,\n              epsilon=1e-8,\n              is_training=True, \n              reuse=None,\n              activation_fn=None,\n              scope=\"normalize\"):\n    '''Applies {batch|layer} normalization.\n    \n    Args:\n      inputs: A tensor with 2 or more dimensions, where the first dimension has\n        `batch_size`. If type is `bn`, the normalization is over all but \n        the last dimension. Or if type is `ln`, the normalization is over \n        the last dimension. Note that this is different from the native \n        `tf.contrib.layers.batch_norm`. For this I recommend you change\n        a line in ``tensorflow/contrib/layers/python/layers/layer.py` \n        as follows.\n        Before: mean, variance = nn.moments(inputs, axis, keep_dims=True)\n        After: mean, variance = nn.moments(inputs, [-1], keep_dims=True)\n      type: A string. Either \"bn\" or \"ln\".\n      decay: Decay for the moving average. Reasonable values for `decay` are close\n        to 1.0, typically in the multiple-nines range: 0.999, 0.99, 0.9, etc.\n        Lower `decay` value (recommend trying `decay`=0.9) if model experiences\n        reasonably good training performance but poor validation and/or test\n        performance.\n      is_training: Whether or not the layer is in training mode. W\n      activation_fn: Activation function.\n      scope: Optional scope for `variable_scope`.\n      \n    Returns:\n      A tensor with the same shape and data dtype as `inputs`.\n    '''\n    if type==\"bn\":\n        inputs_shape = inputs.get_shape()\n        inputs_rank = inputs_shape.ndims\n\n        # use fused batch norm if inputs_rank in [2, 3, 4] as it is much faster.\n        # pay attention to the fact that fused_batch_norm requires shape to be rank 4 of NHWC.\n        if inputs_rank in [2, 3, 4]:\n            if inputs_rank==2:\n                inputs = tf.expand_dims(inputs, axis=1)\n                inputs = tf.expand_dims(inputs, axis=2)\n            elif inputs_rank==3:\n                inputs = tf.expand_dims(inputs, axis=1)\n\n            outputs = tf.contrib.layers.batch_norm(inputs=inputs,\n                                               decay=decay,\n                                               center=True,\n                                               scale=True,\n                                               updates_collections=None,\n                                               is_training=is_training,\n                                               scope=scope,\n                                               zero_debias_moving_mean=True,\n                                               fused=True,\n                                               reuse=reuse)\n            # restore original shape\n            if inputs_rank==2:\n                outputs = tf.squeeze(outputs, axis=[1, 2])\n            elif inputs_rank==3:\n                outputs = tf.squeeze(outputs, axis=1)\n        else: # fallback to naive batch norm\n            outputs = tf.contrib.layers.batch_norm(inputs=inputs,\n                                               decay=decay,\n                                               center=True,\n                                               scale=True,\n                                               updates_collections=None,\n                                               is_training=is_training,\n                                               scope=scope,\n                                               reuse=reuse,\n                                               fused=False)\n    elif type in (\"ln\",  \"ins\"):\n        reduction_axis = -1 if type==\"ln\" else 1\n        with tf.variable_scope(scope, reuse=reuse):\n            inputs_shape = inputs.get_shape()\n            params_shape = inputs_shape[-1:]\n\n            mean, variance = tf.nn.moments(inputs, [reduction_axis], keep_dims=True)\n            # beta = tf.Variable(tf.zeros(params_shape))\n            beta = tf.get_variable(\"beta\", shape=params_shape, initializer=tf.zeros_initializer)\n            # gamma = tf.Variable(tf.ones(params_shape))\n            gamma = tf.get_variable(\"gamma\", shape=params_shape, initializer=tf.ones_initializer)\n            normalized = (inputs - mean) / ( (variance + epsilon) ** (.5) )\n            outputs = gamma * normalized + beta\n    else:\n        outputs = inputs\n\n    if activation_fn:\n        outputs = activation_fn(outputs)\n\n    return outputs\n\n\n\ndef conv1d(inputs,\n           filters=None, \n           size=1, \n           rate=1,\n           padding=\"SAME\",\n           use_bias=False,\n           activation_fn=None,\n           scope=\"conv1d\",\n           reuse=None):\n    '''\n    Args:\n      inputs: A 3-D tensor with shape of [batch, time, depth].\n      filters: An int. Number of outputs (=activation maps)\n      size: An int. Filter size.\n      rate: An int. Dilation rate.\n      padding: Either `same` or `valid` or `causal` (case-insensitive).\n      use_bias: A boolean.\n      scope: Optional scope for `variable_scope`.\n      reuse: Boolean, whether to reuse the weights of a previous layer\n        by the same name.\n    \n    Returns:\n      A masked tensor of the same shape and dtypes as `inputs`.\n    '''\n    with tf.variable_scope(scope):\n        if padding.lower()==\"causal\":\n            # pre-padding for causality\n            pad_len = (size - 1) * rate  # padding size\n            inputs = tf.pad(inputs, [[0, 0], [pad_len, 0], [0, 0]])\n            padding = \"valid\"\n        \n        if filters is None:\n            filters = inputs.get_shape().as_list[-1]\n        \n        params = {\"inputs\":inputs, \"filters\":filters, \"kernel_size\":size,\n                \"dilation_rate\":rate, \"padding\":padding, \"activation\":activation_fn, \n                \"use_bias\":use_bias, \"reuse\":reuse}\n        \n        outputs = tf.layers.conv1d(**params)\n    return outputs\n\n\ndef conv1d_banks(inputs, K=16, num_units=None, norm_type=None, is_training=True, scope=\"conv1d_banks\", reuse=None):\n    '''Applies a series of conv1d separately.\n    \n    Args:\n      inputs: A 3d tensor with shape of [N, T, C]\n      K: An int. The size of conv1d banks. That is, \n        The `inputs` are convolved with K filters: 1, 2, ..., K.\n      is_training: A boolean. This is passed to an argument of `batch_normalize`.\n    \n    Returns:\n      A 3d tensor with shape of [N, T, K*Hp.embed_size//2].\n    '''\n    with tf.variable_scope(scope, reuse=reuse):\n        outputs = []\n        for k in range(1, K+1):\n            with tf.variable_scope(\"num_{}\".format(k)):\n                output = conv1d(inputs, num_units, k)\n                output = normalize(output, type=norm_type, is_training=is_training, activation_fn=tf.nn.relu)\n            outputs.append(output)\n        outputs = tf.concat(outputs, -1)\n    return outputs # (N, T, Hp.embed_size//2*K)\n\n\ndef gru(inputs, num_units=None, bidirection=False, seqlens=None, scope=\"gru\", reuse=None):\n    '''Applies a GRU.\n    \n    Args:\n      inputs: A 3d tensor with shape of [N, T, C].\n      num_units: An int. The number of hidden units.\n      bidirection: A boolean. If True, bidirectional results \n        are concatenated.\n      scope: Optional scope for `variable_scope`.  \n      reuse: Boolean, whether to reuse the weights of a previous layer\n        by the same name.\n        \n    Returns:\n      If bidirection is True, a 3d tensor with shape of [N, T, 2*num_units],\n        otherwise [N, T, num_units].\n    '''\n    with tf.variable_scope(scope, reuse=reuse):\n        if num_units is None:\n            num_units = inputs.get_shape().as_list[-1]\n            \n        cell = tf.contrib.rnn.GRUCell(num_units)  \n        if bidirection: \n            cell_bw = tf.contrib.rnn.GRUCell(num_units)\n            outputs, _ = tf.nn.bidirectional_dynamic_rnn(cell, cell_bw, inputs, \n                                                         sequence_length=seqlens,\n                                                         dtype=tf.float32)\n            return tf.concat(outputs, 2)  \n        else:\n            outputs, _ = tf.nn.dynamic_rnn(cell, inputs, \n                                           sequence_length=seqlens,\n                                           dtype=tf.float32)\n            return outputs\n\n\ndef attention_decoder(inputs, memory, seqlens=None, num_units=None, scope=\"attention_decoder\", reuse=None):\n    '''Applies a GRU to `inputs`, while attending `memory`.\n    Args:\n      inputs: A 3d tensor with shape of [N, T', C']. Decoder inputs.\n      memory: A 3d tensor with shape of [N, T, C]. Outputs of encoder network.\n      seqlens: A 1d tensor with shape of [N,], dtype of int32.\n      num_units: An int. Attention size.\n      scope: Optional scope for `variable_scope`.  \n      reuse: Boolean, whether to reuse the weights of a previous layer\n        by the same name.\n    \n    Returns:\n      A 3d tensor with shape of [N, T, num_units].    \n    '''\n    with tf.variable_scope(scope, reuse=reuse):\n        if num_units is None:\n            num_units = inputs.get_shape().as_list[-1]\n        \n        attention_mechanism = tf.contrib.seq2seq.BahdanauAttention(num_units, \n                                                                   memory, \n                                                                   memory_sequence_length=seqlens, \n                                                                   normalize=True,\n                                                                   probability_fn=tf.nn.softmax)\n        decoder_cell = tf.contrib.rnn.GRUCell(num_units)\n        cell_with_attention = tf.contrib.seq2seq.AttentionWrapper(decoder_cell, attention_mechanism, num_units)\n        outputs, _ = tf.nn.dynamic_rnn(cell_with_attention, inputs, \n                                       dtype=tf.float32) #( N, T', 16)\n    return outputs\n\ndef prenet(inputs, num_units=None, dropout_rate=0., is_training=True, scope=\"prenet\", reuse=None):\n    '''Prenet for Encoder and Decoder.\n    Args:\n      inputs: A 3D tensor of shape [N, T, hp.embed_size].\n      is_training: A boolean.\n      scope: Optional scope for `variable_scope`.  \n      reuse: Boolean, whether to reuse the weights of a previous layer\n        by the same name.\n        \n    Returns:\n      A 3D tensor of shape [N, T, num_units/2].\n    '''\n    with tf.variable_scope(scope, reuse=reuse):\n        outputs = tf.layers.dense(inputs, units=num_units[0], activation=tf.nn.relu, name=\"dense1\")\n        outputs = tf.layers.dropout(outputs, rate=dropout_rate, training=is_training, name=\"dropout1\")\n        outputs = tf.layers.dense(outputs, units=num_units[1], activation=tf.nn.relu, name=\"dense2\")\n        outputs = tf.layers.dropout(outputs, rate=dropout_rate, training=is_training, name=\"dropout2\")\n\n    return outputs # (N, T, num_units/2)\n\ndef highwaynet(inputs, num_units=None, scope=\"highwaynet\", reuse=None):\n    '''Highway networks, see https://arxiv.org/abs/1505.00387\n\n    Args:\n      inputs: A 3D tensor of shape [N, T, W].\n      num_units: An int or `None`. Specifies the number of units in the highway layer\n             or uses the input size if `None`.\n      scope: Optional scope for `variable_scope`.  \n      reuse: Boolean, whether to reuse the weights of a previous layer\n        by the same name.\n\n    Returns:\n      A 3D tensor of shape [N, T, W].\n    '''\n    if not num_units:\n        num_units = inputs.get_shape()[-1]\n        \n    with tf.variable_scope(scope, reuse=reuse):\n        H = tf.layers.dense(inputs, units=num_units, activation=tf.nn.relu, name=\"dense1\")\n        T = tf.layers.dense(inputs, units=num_units, activation=tf.nn.sigmoid, bias_initializer=tf.constant_initializer(-1.0), name=\"dense2\")\n        C = 1. - T\n        outputs = H * T + inputs * C\n    return outputs\n\n\ndef cbhg(input, num_banks, hidden_units, num_highway_blocks, norm_type='bn', is_training=True, scope=\"cbhg\"):\n    with tf.variable_scope(scope):\n        out = conv1d_banks(input,\n                           K=num_banks,\n                           num_units=hidden_units,\n                           norm_type=norm_type,\n                           is_training=is_training)  # (N, T, K * E / 2)\n\n        out = tf.layers.max_pooling1d(out, 2, 1, padding=\"same\")  # (N, T, K * E / 2)\n\n        out = conv1d(out, hidden_units, 3, scope=\"conv1d_1\")  # (N, T, E/2)\n        out = normalize(out, type=norm_type, is_training=is_training, activation_fn=tf.nn.relu)\n        out = conv1d(out, hidden_units, 3, scope=\"conv1d_2\")  # (N, T, E/2)\n        out += input  # (N, T, E/2) # residual connections\n\n        for i in range(num_highway_blocks):\n            out = highwaynet(out, num_units=hidden_units,\n                             scope='highwaynet_{}'.format(i))  # (N, T, E/2)\n\n        out = gru(out, hidden_units, True)  # (N, T, E)\n    return out"
        },
        {
          "name": "notes",
          "type": "tree",
          "content": null
        },
        {
          "name": "requirements.txt",
          "type": "blob",
          "size": 0.173828125,
          "content": "tensorflow-gpu >= 1.8\nnumpy >= 1.11.1\nlibrosa == 0.5.1\njoblib == 0.11.0\ntensorpack >= 0.8.6\npyyaml\nsoundfile\npydub\ntqdm\ngit+https://github.com/wookayin/tensorflow-plot.git@master"
        },
        {
          "name": "scripts",
          "type": "tree",
          "content": null
        },
        {
          "name": "tensorpack_extension.py",
          "type": "blob",
          "size": 1.9501953125,
          "content": "# -*- coding: utf-8 -*-\n#!/usr/bin/env python\n\nimport re\nfrom tensorpack.utils import logger\nfrom tensorpack.tfutils.gradproc import GradientProcessor\nfrom tensorpack.callbacks.monitor import JSONWriter\nimport tensorflow as tf\n\n\n# class AudioWriter(TrainingMonitor):\n#     \"\"\"\n#     Write summaries to TensorFlow event file.\n#     \"\"\"\n#     def __new__(cls):\n#         if logger.get_logger_dir():\n#             return super(TFEventWriter, cls).__new__(cls)\n#         else:\n#             logger.warn(\"logger directory was not set. Ignore TFEventWriter.\")\n#             return NoOpMonitor()\n#\n#     def _setup_graph(self):\n#         self._writer = tf.summary.FileWriter(logger.get_logger_dir(), graph=tf.get_default_graph())\n#\n#     def process_summary(self, summary):\n#         self._writer.add_summary(summary, self.global_step)\n#\n#     def process_event(self, evt):\n#         self._writer.add_event(evt)\n#\n#     def _trigger(self):     # flush every epoch\n#         self._writer.flush()\n#\n#     def _after_train(self):\n#         self._writer.close()\n#\n\nclass FilterGradientVariables(GradientProcessor):\n    \"\"\"\n    Skip the update of certain variables and print a warning.\n    \"\"\"\n\n    def __init__(self, var_regex='.*', verbose=True):\n        \"\"\"\n        Args:\n            var_regex (string): regular expression to match variable to update.\n            verbose (bool): whether to print warning about None gradients.\n        \"\"\"\n        super(FilterGradientVariables, self).__init__()\n        self._regex = var_regex\n        self._verbose = verbose\n\n    def _process(self, grads):\n        g = []\n        to_print = []\n        for grad, var in grads:\n            if re.match(self._regex, var.op.name):\n                g.append((grad, var))\n            else:\n                to_print.append(var.op.name)\n        if self._verbose and len(to_print):\n            message = ', '.join(to_print)\n            logger.warn(\"No gradient w.r.t these trainable variables: {}\".format(message))\n        return g\n"
        },
        {
          "name": "train1.py",
          "type": "blob",
          "size": 2.2900390625,
          "content": "# -*- coding: utf-8 -*-\n# /usr/bin/python2\n\nfrom __future__ import print_function\n\nimport argparse\nimport multiprocessing\nimport os\n\nfrom tensorpack.callbacks.saver import ModelSaver\nfrom tensorpack.tfutils.sessinit import SaverRestore\nfrom tensorpack.train.interface import TrainConfig\nfrom tensorpack.train.interface import launch_train_with_config\nfrom tensorpack.train.trainers import SyncMultiGPUTrainerReplicated\nfrom tensorpack.utils import logger\nfrom tensorpack.input_source.input_source import QueueInput\nfrom data_load import Net1DataFlow\nfrom hparam import hparam as hp\nfrom models import Net1\nimport tensorflow as tf\n\n\ndef train(args, logdir):\n\n    # model\n    model = Net1()\n\n    # dataflow\n    df = Net1DataFlow(hp.train1.data_path, hp.train1.batch_size)\n\n    # set logger for event and model saver\n    logger.set_logger_dir(logdir)\n\n    session_conf = tf.ConfigProto(\n        gpu_options=tf.GPUOptions(\n            allow_growth=True,\n        ),)\n\n    train_conf = TrainConfig(\n        model=model,\n        data=QueueInput(df(n_prefetch=1000, n_thread=4)),\n        callbacks=[\n            ModelSaver(checkpoint_dir=logdir),\n            # TODO EvalCallback()\n        ],\n        max_epoch=hp.train1.num_epochs,\n        steps_per_epoch=hp.train1.steps_per_epoch,\n        # session_config=session_conf\n    )\n    ckpt = '{}/{}'.format(logdir, args.ckpt) if args.ckpt else tf.train.latest_checkpoint(logdir)\n    if ckpt:\n        train_conf.session_init = SaverRestore(ckpt)\n\n    if args.gpu:\n        os.environ['CUDA_VISIBLE_DEVICES'] = args.gpu\n        train_conf.nr_tower = len(args.gpu.split(','))\n\n    trainer = SyncMultiGPUTrainerReplicated(hp.train1.num_gpu)\n\n    launch_train_with_config(train_conf, trainer=trainer)\n\n\ndef get_arguments():\n    parser = argparse.ArgumentParser()\n    parser.add_argument('case', type=str, help='experiment case name')\n    parser.add_argument('-ckpt', help='checkpoint to load model.')\n    parser.add_argument('-gpu', help='comma separated list of GPU(s) to use.')\n    arguments = parser.parse_args()\n    return arguments\n\nif __name__ == '__main__':\n    args = get_arguments()\n    hp.set_hparam_yaml(args.case)\n    logdir_train1 = '{}/train1'.format(hp.logdir)\n\n    print('case: {}, logdir: {}'.format(args.case1, args.case, logdir_train1))\n\n    train(args, logdir=logdir_train1)\n\n    print(\"Done\")"
        },
        {
          "name": "train2.py",
          "type": "blob",
          "size": 3.2333984375,
          "content": "# -*- coding: utf-8 -*-\n# /usr/bin/python2\n\nfrom __future__ import print_function\n\nimport argparse\nimport os\n\nimport tensorflow as tf\nfrom tensorpack.callbacks.saver import ModelSaver\nfrom tensorpack.input_source.input_source import QueueInput\nfrom tensorpack.tfutils.sessinit import ChainInit\nfrom tensorpack.tfutils.sessinit import SaverRestore\nfrom tensorpack.train.interface import TrainConfig\nfrom tensorpack.train.interface import launch_train_with_config\nfrom tensorpack.train.trainers import SyncMultiGPUTrainerReplicated\nfrom tensorpack.utils import logger\n\nfrom data_load import Net2DataFlow\nfrom hparam import hparam as hp\nfrom models import Net2\nfrom utils import remove_all_files\n\n\ndef train(args, logdir1, logdir2):\n    # model\n    model = Net2()\n\n    # dataflow\n    df = Net2DataFlow(hp.train2.data_path, hp.train2.batch_size)\n\n    # set logger for event and model saver\n    logger.set_logger_dir(logdir2)\n\n    # session_conf = tf.ConfigProto(\n    #     gpu_options=tf.GPUOptions(\n    #         allow_growth=True,\n    #         per_process_gpu_memory_fraction=0.6,\n    #     ),\n    # )\n\n    session_inits = []\n    ckpt2 = '{}/{}'.format(logdir2, args.ckpt) if args.ckpt else tf.train.latest_checkpoint(logdir2)\n    if ckpt2:\n        session_inits.append(SaverRestore(ckpt2))\n    ckpt1 = tf.train.latest_checkpoint(logdir1)\n    if ckpt1:\n        session_inits.append(SaverRestore(ckpt1, ignore=['global_step']))\n    train_conf = TrainConfig(\n        model=model,\n        data=QueueInput(df(n_prefetch=1000, n_thread=4)),\n        callbacks=[\n            # TODO save on prefix net2\n            ModelSaver(checkpoint_dir=logdir2),\n            # ConvertCallback(logdir2, hp.train2.test_per_epoch),\n        ],\n        max_epoch=hp.train2.num_epochs,\n        steps_per_epoch=hp.train2.steps_per_epoch,\n        session_init=ChainInit(session_inits)\n    )\n    if args.gpu:\n        os.environ['CUDA_VISIBLE_DEVICES'] = args.gpu\n        train_conf.nr_tower = len(args.gpu.split(','))\n\n    trainer = SyncMultiGPUTrainerReplicated(hp.train2.num_gpu)\n\n    launch_train_with_config(train_conf, trainer=trainer)\n\n\n# def get_cyclic_lr(step):\n#     lr_margin = hp.train2.lr_cyclic_margin * math.sin(2. * math.pi / hp.train2.lr_cyclic_steps * step)\n#     lr = hp.train2.lr + lr_margin\n#     return lr\n\n\ndef get_arguments():\n    parser = argparse.ArgumentParser()\n    parser.add_argument('case1', type=str, help='experiment case name of train1')\n    parser.add_argument('case2', type=str, help='experiment case name of train2')\n    parser.add_argument('-ckpt', help='checkpoint to load model.')\n    parser.add_argument('-gpu', help='comma separated list of GPU(s) to use.')\n    parser.add_argument('-r', action='store_true', help='start training from the beginning.')\n    arguments = parser.parse_args()\n    return arguments\n\n\nif __name__ == '__main__':\n    args = get_arguments()\n    hp.set_hparam_yaml(args.case2)\n    logdir_train1 = '{}/{}/train1'.format(hp.logdir_path, args.case1)\n    logdir_train2 = '{}/{}/train2'.format(hp.logdir_path, args.case2)\n\n    if args.r:\n        remove_all_files(logdir_train2)\n\n    print('case1: {}, case2: {}, logdir1: {}, logdir2: {}'.format(args.case1, args.case2, logdir_train1, logdir_train2))\n\n    train(args, logdir1=logdir_train1, logdir2=logdir_train2)\n\n    print(\"Done\")\n"
        },
        {
          "name": "utils.py",
          "type": "blob",
          "size": 3.1162109375,
          "content": "# -*- coding: utf-8 -*-\n#!/usr/bin/env python\n\nimport glob\nimport itertools\nimport os\nimport re\nfrom textwrap import wrap\n\nimport matplotlib\nimport numpy as np\nimport tfplot\nfrom sklearn.metrics import confusion_matrix\n\n\ndef split_path(path):\n    '''\n    'a/b/c.wav' => ('a/b', 'c', 'wav')\n    :param path: filepath = 'a/b/c.wav'\n    :return: basename, filename, and extension = ('a/b', 'c', 'wav')\n    '''\n    basepath, filename = os.path.split(path)\n    filename, extension = os.path.splitext(filename)\n    return basepath, filename, extension\n\n\ndef remove_all_files(path):\n    files = glob.glob('{}/*'.format(path))\n    for f in files:\n        os.remove(f)\n\n\ndef normalize_0_1(values, max, min):\n    normalized = np.clip((values - min) / (max - min), 0, 1)\n    return normalized\n\n\ndef denormalize_0_1(normalized, max, min):\n    values =  np.clip(normalized, 0, 1) * (max - min) + min\n    return values\n\n\ndef plot_confusion_matrix(correct_labels, predict_labels, labels, tensor_name='confusion_matrix', normalize=False):\n    ''' \n    Parameters:\n        correct_labels                  : These are your true classification categories.\n        predict_labels                  : These are you predicted classification categories\n        labels                          : This is a list of labels which will be used to display the axix labels\n        title='Confusion matrix'        : Title for your matrix\n        tensor_name = 'MyFigure/image'  : Name for the output summay tensor\n\n    Returns:\n        summary: TensorFlow summary \n\n    Other itema to note:\n        - Depending on the number of category and the data , you may have to modify the figzie, font sizes etc. \n        - Currently, some of the ticks dont line up due to rotations.\n    '''\n    cm = confusion_matrix(correct_labels, predict_labels, labels=labels)\n    if normalize:\n        cm = cm.astype('float') * 10 / cm.sum(axis=1)[:, np.newaxis]\n        cm = np.nan_to_num(cm, copy=True)\n        cm = cm.astype('int')\n\n    np.set_printoptions(precision=2)\n    ###fig, ax = matplotlib.figure.Figure()\n\n    fig = matplotlib.figure.Figure(figsize=(7, 7), dpi=320, facecolor='w', edgecolor='k')\n    ax = fig.add_subplot(1, 1, 1)\n    im = ax.imshow(cm, cmap='Oranges')\n\n    classes = [re.sub(r'([a-z](?=[A-Z])|[A-Z](?=[A-Z][a-z]))', r'\\1 ', x) for x in labels]\n    classes = ['\\n'.join(wrap(l, 40)) for l in classes]\n\n    tick_marks = np.arange(len(classes))\n\n    ax.set_xlabel('Predicted', fontsize=7)\n    ax.set_xticks(tick_marks)\n    c = ax.set_xticklabels(classes, fontsize=4, rotation=-90, ha='center')\n    ax.xaxis.set_label_position('bottom')\n    ax.xaxis.tick_bottom()\n\n    ax.set_ylabel('True Label', fontsize=7)\n    ax.set_yticks(tick_marks)\n    ax.set_yticklabels(classes, fontsize=4, va='center')\n    ax.yaxis.set_label_position('left')\n    ax.yaxis.tick_left()\n\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        ax.text(j, i, format(cm[i, j], 'd') if cm[i, j] != 0 else '.', horizontalalignment=\"center\", fontsize=6,\n                verticalalignment='center', color=\"black\")\n    fig.set_tight_layout(True)\n    summary = tfplot.figure.to_summary(fig, tag=tensor_name)\n    return summary"
        }
      ]
    }
  ]
}