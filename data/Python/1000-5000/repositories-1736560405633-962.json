{
  "metadata": {
    "timestamp": 1736560405633,
    "page": 962,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjk3MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "ShangtongZhang/DeepRL",
      "stars": 3233,
      "defaultBranch": "master",
      "files": [
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 1.1044921875,
          "content": "# Byte-compiled / optimized / DLL files\n__pycache__/\n*.py[cod]\n*$py.class\n\n# C extensions\n*.so\n\n# Distribution / packaging\n.Python\nenv/\nbuild/\ndevelop-eggs/\ndist/\ndownloads/\neggs/\n.eggs/\nlib/\nlib64/\nparts/\nsdist/\nvar/\n*.egg-info/\n.installed.cfg\n*.egg\n\n# PyInstaller\n#  Usually these files are written by a python script from a template\n#  before PyInstaller builds the exe, so as to inject date/other infos into it.\n*.manifest\n*.spec\n\n# Installer logs\npip-log.txt\npip-delete-this-directory.txt\n\n# Unit test / coverage reports\nhtmlcov/\n.tox/\n.coverage\n.coverage.*\n.cache\nnosetests.xml\ncoverage.xml\n*,cover\n.hypothesis/\n\n# Translations\n*.mo\n*.pot\n\n# Django stuff:\n*.log\nlocal_settings.py\n\n# Flask stuff:\ninstance/\n.webassets-cache\n\n# Scrapy stuff:\n.scrapy\n\n# Sphinx documentation\ndocs/_build/\n\n# PyBuilder\ntarget/\n\n# IPython Notebook\n.ipynb_checkpoints\n\n# pyenv\n.python-version\n\n# celery beat schedule file\ncelerybeat-schedule\n\n# dotenv\n.env\n\n# virtualenv\nvenv/\nENV/\n\n# Spyder project settings\n.spyderproject\n\n# Rope project settings\n.ropeproject\n\n.idea\n.vscode\ndata\ndataset\nlog\nold_logs\nfigure\nimages_data\nmjkey.txt\n.DS_Store\ntf_log"
        },
        {
          "name": "Dockerfile",
          "type": "blob",
          "size": 1.9599609375,
          "content": "FROM nvidia/cuda:10.0-base\n\nRUN apt update && DEBIAN_FRONTEND=noninteractive apt install -y --allow-unauthenticated --no-install-recommends \\\n    build-essential apt-utils cmake git curl vim ca-certificates \\\n    libjpeg-dev libpng-dev \\\n    libgtk3.0 libsm6 cmake ffmpeg pkg-config \\\n    qtbase5-dev libqt5opengl5-dev libassimp-dev \\\n    libboost-python-dev libtinyxml-dev bash \\\n    wget unzip libosmesa6-dev software-properties-common \\\n    libopenmpi-dev libglew-dev openssh-server \\\n    libosmesa6-dev libgl1-mesa-glx libgl1-mesa-dev patchelf libglfw3\n\nRUN rm -rf /var/lib/apt/lists/*\n\nARG UID\nRUN useradd -u $UID --create-home user\nUSER user\nWORKDIR /home/user\n\nRUN wget -q https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh && \\\n    bash Miniconda3-latest-Linux-x86_64.sh -b -p miniconda3 && \\\n    rm Miniconda3-latest-Linux-x86_64.sh\nENV PATH /home/user/miniconda3/bin:$PATH\n\nRUN mkdir -p .mujoco \\\n    && wget https://www.roboti.us/download/mjpro150_linux.zip -O mujoco.zip \\\n    && unzip mujoco.zip -d .mujoco \\\n    && rm mujoco.zip\nRUN wget https://www.roboti.us/download/mujoco200_linux.zip -O mujoco.zip \\\n    && unzip mujoco.zip -d .mujoco \\\n    && rm mujoco.zip\n\n# Make sure you have a license, otherwise comment this line out\n# Of course you then cannot use Mujoco and DM Control, but Roboschool is still available\nCOPY ./mjkey.txt .mujoco/mjkey.txt\n\nENV LD_LIBRARY_PATH /home/user/.mujoco/mjpro150/bin:${LD_LIBRARY_PATH}\nENV LD_LIBRARY_PATH /home/user/.mujoco/mjpro200_linux/bin:${LD_LIBRARY_PATH}\n\nRUN conda install -y python=3.6\nRUN conda install mpi4py\nCOPY requirements.txt requirements.txt\nRUN pip install -r requirements.txt\nRUN pip install glfw Cython imageio lockfile\nRUN pip install mujoco-py==1.50.1.68\nRUN pip install git+git://github.com/deepmind/dm_control.git@103834\nRUN pip install git+https://github.com/ShangtongZhang/dm_control2gym.git@scalar_fix\nRUN pip install git+git://github.com/openai/baselines.git@8e56dd#egg=baselines\nWORKDIR /home/user/deep_rl\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 1.046875,
          "content": "MIT License\n\nCopyright (c) 2019 Shangtong Zhang\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 6.8017578125,
          "content": "# DeepRL\n\n> If you have any question or want to report a bug, please open an issue instead of emailing me directly.  \n\nModularized implementation of popular deep RL algorithms in PyTorch.  \nEasy switch between toy tasks and challenging games.\n\nImplemented algorithms:\n* (Double/Dueling/Prioritized) Deep Q-Learning (DQN)\n* Categorical DQN (C51)\n* Quantile Regression DQN (QR-DQN)\n* (Continuous/Discrete) Synchronous Advantage Actor Critic (A2C)\n* Synchronous N-Step Q-Learning (N-Step DQN)\n* Deep Deterministic Policy Gradient (DDPG)\n* Proximal Policy Optimization (PPO)\n* The Option-Critic Architecture (OC)\n* Twined Delayed DDPG (TD3)\n* [Off-PAC-KL/TruncatedETD/DifferentialGQ/MVPI/ReverseRL/COF-PAC/GradientDICE/Bi-Res-DDPG/DAC/Geoff-PAC/QUOTA/ACE](#code-of-my-papers)\n\nThe DQN agent, as well as C51 and QR-DQN, has an asynchronous actor for data generation and an asynchronous replay buffer for transferring data to GPU.\nUsing 1 RTX 2080 Ti and 3 threads, the DQN agent runs for 10M steps (40M frames, 2.5M gradient updates) for Breakout within 6 hours.\n\n# Dependency\n* PyTorch v1.5.1\n* See ```Dockerfile``` and ```requirements.txt``` for more details\n\n# Usage\n\n```examples.py``` contains examples for all the implemented algorithms.  \n```Dockerfile``` contains the environment for generating the curves below.  \nPlease use this bibtex if you want to cite this repo\n```\n@misc{deeprl,\n  author = {Zhang, Shangtong},\n  title = {Modularized Implementation of Deep RL Algorithms in PyTorch},\n  year = {2018},\n  publisher = {GitHub},\n  journal = {GitHub Repository},\n  howpublished = {\\url{https://github.com/ShangtongZhang/DeepRL}},\n}\n```\n\n# Curves (commit ```9e811e```)\n\n## BreakoutNoFrameskip-v4 (1 run)\n\n![Loading...](https://raw.githubusercontent.com/ShangtongZhang/DeepRL/master/images/Breakout.png)\n\n## Mujoco \n\n* DDPG/TD3 evaluation performance.\n![Loading...](https://raw.githubusercontent.com/ShangtongZhang/DeepRL/master/images/mujoco_eval.png)\n(5 runs, mean + standard error)\n\n* PPO online performance. \n![Loading...](https://raw.githubusercontent.com/ShangtongZhang/DeepRL/master/images/PPO.png)\n(5 runs, mean + standard error, smoothed by a window of size 10)\n\n\n# References\n* [Human Level Control through Deep Reinforcement Learning](https://www.nature.com/nature/journal/v518/n7540/full/nature14236.html)\n* [Asynchronous Methods for Deep Reinforcement Learning](https://arxiv.org/abs/1602.01783)\n* [Deep Reinforcement Learning with Double Q-learning](https://arxiv.org/abs/1509.06461)\n* [Dueling Network Architectures for Deep Reinforcement Learning](https://arxiv.org/abs/1511.06581)\n* [Playing Atari with Deep Reinforcement Learning](https://arxiv.org/abs/1312.5602)\n* [HOGWILD!: A Lock-Free Approach to Parallelizing Stochastic Gradient Descent](https://arxiv.org/abs/1106.5730)\n* [Deterministic Policy Gradient Algorithms](http://proceedings.mlr.press/v32/silver14.pdf)\n* [Continuous control with deep reinforcement learning](https://arxiv.org/abs/1509.02971)\n* [High-Dimensional Continuous Control Using Generalized Advantage Estimation](https://arxiv.org/abs/1506.02438)\n* [Hybrid Reward Architecture for Reinforcement Learning](https://arxiv.org/abs/1706.04208)\n* [Trust Region Policy Optimization](https://arxiv.org/abs/1502.05477)\n* [Proximal Policy Optimization Algorithms](https://arxiv.org/abs/1707.06347)\n* [Emergence of Locomotion Behaviours in Rich Environments](https://arxiv.org/abs/1707.02286)\n* [Action-Conditional Video Prediction using Deep Networks in Atari Games](https://arxiv.org/abs/1507.08750)\n* [A Distributional Perspective on Reinforcement Learning](https://arxiv.org/abs/1707.06887)\n* [Distributional Reinforcement Learning with Quantile Regression](https://arxiv.org/abs/1710.10044)\n* [The Option-Critic Architecture](https://arxiv.org/abs/1609.05140)\n* [Addressing Function Approximation Error in Actor-Critic Methods](https://arxiv.org/abs/1802.09477)\n* Some hyper-parameters are from [DeepMind Control Suite](https://arxiv.org/abs/1801.00690), [OpenAI Baselines](https://github.com/openai/baselines) and [Ilya Kostrikov](https://github.com/ikostrikov/pytorch-a2c-ppo-acktr)\n\n# Code of My Papers\n> They are located in other branches of this repo and seem to be good examples for using this codebase.\n* [Global Optimality and Finite Sample Analysis of Softmax Off-Policy Actor Critic under State Distribution Mismatch](https://arxiv.org/abs/2111.02997) [[Off-PAC-KL](https://github.com/ShangtongZhang/DeepRL/tree/Off-PAC-KL)]\n* [Truncated Emphatic Temporal Difference Methods for Prediction and Control](https://arxiv.org/abs/2108.05338) [[TruncatedETD](https://github.com/ShangtongZhang/DeepRL/tree/TruncatedETD)]\n* [A Deeper Look at Discounting Mismatch in Actor-Critic Algorithms](https://arxiv.org/abs/2010.01069) [[Discounting](https://github.com/ShangtongZhang/DeepRL/tree/discounting)]\n* [Breaking the Deadly Triad with a Target Network](https://arxiv.org/abs/2101.08862) [[TargetNetwork](https://github.com/ShangtongZhang/DeepRL/tree/TargetNetwork)]\n* [Average-Reward Off-Policy Policy Evaluation with Function Approximation](https://arxiv.org/abs/2101.02808) [[DifferentialGQ](https://github.com/ShangtongZhang/DeepRL/tree/DifferentialGQ)]\n* [Mean-Variance Policy Iteration for Risk-Averse Reinforcement Learning](https://arxiv.org/abs/2004.10888) [[MVPI](https://github.com/ShangtongZhang/DeepRL/tree/MVPI)]\n* [Learning Retrospective Knowledge with Reverse Reinforcement Learning](https://arxiv.org/abs/2007.06703) [[ReverseRL](https://github.com/ShangtongZhang/DeepRL/tree/ReverseRL)]\n* [Provably Convergent Two-Timescale Off-Policy Actor-Critic with Function Approximation](https://arxiv.org/abs/1911.04384) [[COF-PAC](https://github.com/ShangtongZhang/DeepRL/tree/COF-PAC), [TD3-random](https://github.com/ShangtongZhang/DeepRL/tree/TD3-random)]\n* [GradientDICE: Rethinking Generalized Offline Estimation of Stationary Values](https://arxiv.org/abs/2001.11113) [[GradientDICE](https://github.com/ShangtongZhang/DeepRL/tree/GradientDICE)]\n* [Deep Residual Reinforcement Learning](https://arxiv.org/abs/1905.01072) [[Bi-Res-DDPG](https://github.com/ShangtongZhang/DeepRL/tree/Bi-Res-DDPG)]\n* [Generalized Off-Policy Actor-Critic](https://arxiv.org/abs/1903.11329) [[Geoff-PAC](https://github.com/ShangtongZhang/DeepRL/tree/Geoff-PAC), [TD3-random](https://github.com/ShangtongZhang/DeepRL/tree/TD3-random)]\n* [DAC: The Double Actor-Critic Architecture for Learning Options](https://arxiv.org/abs/1904.12691) [[DAC](https://github.com/ShangtongZhang/DeepRL/tree/DAC)]\n* [QUOTA: The Quantile Option Architecture for Reinforcement Learning](https://arxiv.org/abs/1811.02073) [[QUOTA-discrete](https://github.com/ShangtongZhang/DeepRL/tree/QUOTA-discrete), [QUOTA-continuous](https://github.com/ShangtongZhang/DeepRL/tree/QUOTA-continuous)]\n* [ACE: An Actor Ensemble Algorithm for Continuous Control with Tree Search](https://arxiv.org/abs/1811.02696) [[ACE](https://github.com/ShangtongZhang/DeepRL/tree/ACE)]\n"
        },
        {
          "name": "deep_rl",
          "type": "tree",
          "content": null
        },
        {
          "name": "docker_batch.sh",
          "type": "blob",
          "size": 0.619140625,
          "content": "#!/usr/bin/env bash\nGPUs=(0 1 2 3 4 5 6 7)\n\n#for i in $(seq 0 7); do\n#    for j in $(seq 0 0); do\n#        nohup bash docker_python.sh ${GPUs[$i]} \"template_jobs.py --i $i --j $j\" >| job_${i}_${j}.out &\n#    done\n#done\n\n\nrm -f jobs.txt\ntouch jobs.txt\nfor i in $(seq 0 100); do\n    echo \"$i\" >> jobs.txt\ndone\ncat jobs.txt | xargs -n 1 -P 40 sh -c 'bash docker_python.sh 0 \"template_jobs.py --i $0\"'\nrm -f jobs.txt\n\n\n#rm -f jobs.txt\n#touch jobs.txt\n#for i in $(seq 0 6); do\n#    echo \"$i ${GPUs[$(($i % 8))]}\" >> jobs.txt\n#done\n#cat jobs.txt | xargs -n 2 -P 40 sh -c 'bash docker_python.sh $1 \"template_jobs.py --i $0\"'\n#rm -f jobs.txt\n"
        },
        {
          "name": "docker_build.sh",
          "type": "blob",
          "size": 0.0703125,
          "content": "#!/usr/bin/env bash\n\ndocker build --build-arg UID=$UID -t deep_rl:v1.5 ."
        },
        {
          "name": "docker_clean.sh",
          "type": "blob",
          "size": 0.0419921875,
          "content": "#!/usr/bin/env bash\nrm -rf log tf_log *.out"
        },
        {
          "name": "docker_python.sh",
          "type": "blob",
          "size": 0.2216796875,
          "content": "#!/usr/bin/env bash\n\nif hash nvidia-docker 2>/dev/null; then\n  cmd=nvidia-docker\nelse\n  cmd=docker\nfi\n\nNV_GPU=$1 ${cmd} run --rm -v `pwd`:/home/user/deep_rl --entrypoint '/bin/bash' deep_rl:v1.5 -c \"OMP_NUM_THREADS=1 python $2\""
        },
        {
          "name": "docker_shell.sh",
          "type": "blob",
          "size": 0.16015625,
          "content": "#!/usr/bin/env bash\n\nif hash nvidia-docker 2>/dev/null; then\n  cmd=nvidia-docker\nelse\n  cmd=docker\nfi\n\n${cmd} run --rm -v `pwd`:/home/user/deep_rl -it deep_rl:v1.5\n"
        },
        {
          "name": "docker_stop.sh",
          "type": "blob",
          "size": 0.3056640625,
          "content": "#!/usr/bin/env bash\n\nif hash nvidia-docker 2>/dev/null; then\n  cmd=nvidia-docker\nelse\n  cmd=docker\nfi\n\n${cmd} ps -a | awk '{ print $1,$2 }' | grep deep_rl:v1.5 | awk '{print $1 }' | xargs -I {} ${cmd} kill {}\n${cmd} ps -a | awk '{ print $1,$2 }' | grep deep_rl:v1.5 | awk '{print $1 }' | xargs -I {} ${cmd} rm {}\n"
        },
        {
          "name": "examples.py",
          "type": "blob",
          "size": 22.099609375,
          "content": "#######################################################################\n# Copyright (C) 2017 Shangtong Zhang(zhangshangtong.cpp@gmail.com)    #\n# Permission given to modify the code as long as you keep this        #\n# declaration at the top                                              #\n#######################################################################\n\nfrom deep_rl import *\n\n\n# DQN\ndef dqn_feature(**kwargs):\n    generate_tag(kwargs)\n    kwargs.setdefault('log_level', 0)\n    kwargs.setdefault('n_step', 1)\n    kwargs.setdefault('replay_cls', UniformReplay)\n    kwargs.setdefault('async_replay', True)\n    config = Config()\n    config.merge(kwargs)\n\n    config.task_fn = lambda: Task(config.game)\n    config.eval_env = config.task_fn()\n\n    config.optimizer_fn = lambda params: torch.optim.RMSprop(params, 0.001)\n    config.network_fn = lambda: VanillaNet(config.action_dim, FCBody(config.state_dim))\n    # config.network_fn = lambda: DuelingNet(config.action_dim, FCBody(config.state_dim))\n    config.history_length = 1\n    config.batch_size = 10\n    config.discount = 0.99\n    config.max_steps = 1e5\n\n    replay_kwargs = dict(\n        memory_size=int(1e4),\n        batch_size=config.batch_size,\n        n_step=config.n_step,\n        discount=config.discount,\n        history_length=config.history_length)\n\n    config.replay_fn = lambda: ReplayWrapper(config.replay_cls, replay_kwargs, config.async_replay)\n    config.replay_eps = 0.01\n    config.replay_alpha = 0.5\n    config.replay_beta = LinearSchedule(0.4, 1.0, config.max_steps)\n\n    config.random_action_prob = LinearSchedule(1.0, 0.1, 1e4)\n    config.target_network_update_freq = 200\n    config.exploration_steps = 1000\n    # config.double_q = True\n    config.double_q = False\n    config.sgd_update_frequency = 4\n    config.gradient_clip = 5\n    config.eval_interval = int(5e3)\n    config.async_actor = False\n    run_steps(DQNAgent(config))\n\n\ndef dqn_pixel(**kwargs):\n    generate_tag(kwargs)\n    kwargs.setdefault('log_level', 0)\n    kwargs.setdefault('n_step', 1)\n    kwargs.setdefault('replay_cls', UniformReplay)\n    kwargs.setdefault('async_replay', True)\n    config = Config()\n    config.merge(kwargs)\n\n    config.task_fn = lambda: Task(config.game)\n    config.eval_env = config.task_fn()\n\n    config.optimizer_fn = lambda params: torch.optim.RMSprop(\n        params, lr=0.00025, alpha=0.95, eps=0.01, centered=True)\n    config.network_fn = lambda: VanillaNet(config.action_dim, NatureConvBody(in_channels=config.history_length))\n    # config.network_fn = lambda: DuelingNet(config.action_dim, NatureConvBody(in_channels=config.history_length))\n    config.random_action_prob = LinearSchedule(1.0, 0.01, 1e6)\n    config.batch_size = 32\n    config.discount = 0.99\n    config.history_length = 4\n    config.max_steps = int(2e7)\n    replay_kwargs = dict(\n        memory_size=int(1e6),\n        batch_size=config.batch_size,\n        n_step=config.n_step,\n        discount=config.discount,\n        history_length=config.history_length,\n    )\n    config.replay_fn = lambda: ReplayWrapper(config.replay_cls, replay_kwargs, config.async_replay)\n    config.replay_eps = 0.01\n    config.replay_alpha = 0.5\n    config.replay_beta = LinearSchedule(0.4, 1.0, config.max_steps)\n\n    config.state_normalizer = ImageNormalizer()\n    config.reward_normalizer = SignNormalizer()\n    config.target_network_update_freq = 10000\n    config.exploration_steps = 50000\n    # config.exploration_steps = 100\n    config.sgd_update_frequency = 4\n    config.gradient_clip = 5\n    config.double_q = False\n    config.async_actor = True\n    run_steps(DQNAgent(config))\n\n\n# QR DQN\ndef quantile_regression_dqn_feature(**kwargs):\n    generate_tag(kwargs)\n    kwargs.setdefault('log_level', 0)\n    config = Config()\n    config.merge(kwargs)\n\n    config.task_fn = lambda: Task(config.game)\n    config.eval_env = config.task_fn()\n    config.optimizer_fn = lambda params: torch.optim.RMSprop(params, 0.001)\n    config.network_fn = lambda: QuantileNet(config.action_dim, config.num_quantiles, FCBody(config.state_dim))\n\n    config.batch_size = 10\n    replay_kwargs = dict(\n        memory_size=int(1e4),\n        batch_size=config.batch_size)\n    config.replay_fn = lambda: ReplayWrapper(UniformReplay, replay_kwargs, async=True)\n\n    config.random_action_prob = LinearSchedule(1.0, 0.1, 1e4)\n    config.discount = 0.99\n    config.target_network_update_freq = 200\n    config.exploration_steps = 100\n    config.num_quantiles = 20\n    config.gradient_clip = 5\n    config.sgd_update_frequency = 4\n    config.eval_interval = int(5e3)\n    config.max_steps = 1e5\n    run_steps(QuantileRegressionDQNAgent(config))\n\n\ndef quantile_regression_dqn_pixel(**kwargs):\n    generate_tag(kwargs)\n    kwargs.setdefault('log_level', 0)\n    config = Config()\n    config.merge(kwargs)\n\n    config.task_fn = lambda: Task(config.game)\n    config.eval_env = config.task_fn()\n\n    config.optimizer_fn = lambda params: torch.optim.Adam(params, lr=0.00005, eps=0.01 / 32)\n    config.network_fn = lambda: QuantileNet(config.action_dim, config.num_quantiles, NatureConvBody())\n    config.random_action_prob = LinearSchedule(1.0, 0.01, 1e6)\n\n    config.batch_size = 32\n    replay_kwargs = dict(\n        memory_size=int(1e6),\n        batch_size=config.batch_size,\n        history_length=4,\n    )\n    config.replay_fn = lambda: ReplayWrapper(UniformReplay, replay_kwargs, async=True)\n\n    config.state_normalizer = ImageNormalizer()\n    config.reward_normalizer = SignNormalizer()\n    config.discount = 0.99\n    config.target_network_update_freq = 10000\n    config.exploration_steps = 50000\n    config.sgd_update_frequency = 4\n    config.gradient_clip = 5\n    config.num_quantiles = 200\n    config.max_steps = int(2e7)\n    run_steps(QuantileRegressionDQNAgent(config))\n\n\n# C51\ndef categorical_dqn_feature(**kwargs):\n    generate_tag(kwargs)\n    kwargs.setdefault('log_level', 0)\n    config = Config()\n    config.merge(kwargs)\n\n    config.task_fn = lambda: Task(config.game)\n    config.eval_env = config.task_fn()\n    config.optimizer_fn = lambda params: torch.optim.RMSprop(params, 0.001)\n    config.network_fn = lambda: CategoricalNet(config.action_dim, config.categorical_n_atoms, FCBody(config.state_dim))\n    config.random_action_prob = LinearSchedule(1.0, 0.1, 1e4)\n\n    config.batch_size = 10\n    replay_kwargs = dict(\n        memory_size=int(1e4),\n        batch_size=config.batch_size)\n    config.replay_fn = lambda: ReplayWrapper(UniformReplay, replay_kwargs, async=True)\n\n    config.discount = 0.99\n    config.target_network_update_freq = 200\n    config.exploration_steps = 100\n    config.categorical_v_max = 100\n    config.categorical_v_min = -100\n    config.categorical_n_atoms = 50\n    config.gradient_clip = 5\n    config.sgd_update_frequency = 4\n\n    config.eval_interval = int(5e3)\n    config.max_steps = 1e5\n    run_steps(CategoricalDQNAgent(config))\n\n\ndef categorical_dqn_pixel(**kwargs):\n    generate_tag(kwargs)\n    kwargs.setdefault('log_level', 0)\n    config = Config()\n    config.merge(kwargs)\n\n    config.task_fn = lambda: Task(config.game)\n    config.eval_env = config.task_fn()\n    config.optimizer_fn = lambda params: torch.optim.Adam(params, lr=0.00025, eps=0.01 / 32)\n    config.network_fn = lambda: CategoricalNet(config.action_dim, config.categorical_n_atoms, NatureConvBody())\n    config.random_action_prob = LinearSchedule(1.0, 0.01, 1e6)\n\n    config.batch_size = 32\n    replay_kwargs = dict(\n        memory_size=int(1e6),\n        batch_size=config.batch_size,\n        history_length=4,\n    )\n    config.replay_fn = lambda: ReplayWrapper(UniformReplay, replay_kwargs, async=True)\n\n    config.discount = 0.99\n    config.state_normalizer = ImageNormalizer()\n    config.reward_normalizer = SignNormalizer()\n    config.target_network_update_freq = 10000\n    config.exploration_steps = 50000\n    config.categorical_v_max = 10\n    config.categorical_v_min = -10\n    config.categorical_n_atoms = 51\n    config.sgd_update_frequency = 4\n    config.gradient_clip = 0.5\n    config.max_steps = int(2e7)\n    run_steps(CategoricalDQNAgent(config))\n\n\n# Rainbow\ndef rainbow_feature(**kwargs):\n    generate_tag(kwargs)\n    kwargs.setdefault('log_level', 0)\n    kwargs.setdefault('n_step', 3)\n    kwargs.setdefault('replay_cls', PrioritizedReplay)\n    kwargs.setdefault('async_replay', True)\n    config = Config()\n    config.merge(kwargs)\n\n    config.task_fn = lambda: Task(config.game)\n    config.eval_env = config.task_fn()\n\n    config.max_steps = 1e5\n    config.optimizer_fn = lambda params: torch.optim.RMSprop(params, 0.001)\n    config.noisy_linear = True\n    config.network_fn = lambda: RainbowNet(\n        config.action_dim,\n        config.categorical_n_atoms,\n        FCBody(config.state_dim, noisy_linear=config.noisy_linear),\n        noisy_linear=config.noisy_linear\n    )\n    config.categorical_v_max = 100\n    config.categorical_v_min = -100\n    config.categorical_n_atoms = 50\n\n    config.discount = 0.99\n    config.batch_size = 32\n    replay_kwargs = dict(\n        memory_size=int(1e4),\n        batch_size=config.batch_size,\n        n_step=config.n_step,\n        discount=config.discount,\n        history_length=1)\n\n    config.replay_fn = lambda: ReplayWrapper(config.replay_cls, replay_kwargs, config.async_replay)\n\n    config.replay_eps = 0.01\n    config.replay_alpha = 0.5\n    config.replay_beta = LinearSchedule(0.4, 1, config.max_steps)\n    config.random_action_prob = LinearSchedule(1.0, 0.1, 1e4)\n\n    config.target_network_update_freq = 200\n    config.exploration_steps = 1000\n    config.double_q = True\n    config.sgd_update_frequency = 4\n    config.eval_interval = int(5e3)\n    config.async_actor = True\n    config.gradient_clip = 10\n\n    run_steps(CategoricalDQNAgent(config))\n\n\ndef rainbow_pixel(**kwargs):\n    generate_tag(kwargs)\n    kwargs.setdefault('log_level', 0)\n    kwargs.setdefault('n_step', 1)\n    kwargs.setdefault('replay_cls', PrioritizedReplay)\n    kwargs.setdefault('async_replay', True)\n    kwargs.setdefault('noisy_linear', True)\n    config = Config()\n    config.merge(kwargs)\n\n    config.task_fn = lambda: Task(config.game)\n    config.eval_env = config.task_fn()\n\n    config.max_steps = int(2e7)\n    Config.NOISY_LAYER_STD = 0.5\n    config.optimizer_fn = lambda params: torch.optim.Adam(\n        params, lr=0.000625, eps=1.5e-4)\n    config.network_fn = lambda: RainbowNet(\n        config.action_dim,\n        config.categorical_n_atoms,\n        NatureConvBody(noisy_linear=config.noisy_linear),\n        noisy_linear=config.noisy_linear,\n    )\n    config.categorical_v_max = 10\n    config.categorical_v_min = -10\n    config.categorical_n_atoms = 51\n\n    config.random_action_prob = LinearSchedule(1, 0.01, 25e4)\n\n    config.batch_size = 32\n    config.discount = 0.99\n    config.history_length = 4\n    replay_kwargs = dict(\n        memory_size=int(1e6),\n        batch_size=config.batch_size,\n        n_step=config.n_step,\n        discount=config.discount,\n        history_length=config.history_length,\n    )\n    config.replay_fn = lambda: ReplayWrapper(config.replay_cls, replay_kwargs, config.async_replay)\n    config.replay_eps = 0.01\n    config.replay_alpha = 0.5\n    config.replay_beta = LinearSchedule(0.4, 1.0, config.max_steps)\n\n    config.state_normalizer = ImageNormalizer()\n    config.reward_normalizer = SignNormalizer()\n    config.target_network_update_freq = 2000\n    config.exploration_steps = 20000\n    # config.exploration_steps = 200\n    config.sgd_update_frequency = 4\n    config.double_q = True\n    config.async_actor = True\n    config.gradient_clip = 10\n    run_steps(CategoricalDQNAgent(config))\n\n\n# A2C\ndef a2c_feature(**kwargs):\n    generate_tag(kwargs)\n    kwargs.setdefault('log_level', 0)\n    config = Config()\n    config.merge(kwargs)\n\n    config.num_workers = 5\n    config.task_fn = lambda: Task(config.game, num_envs=config.num_workers)\n    config.eval_env = Task(config.game)\n    config.optimizer_fn = lambda params: torch.optim.RMSprop(params, 0.001)\n    config.network_fn = lambda: CategoricalActorCriticNet(\n        config.state_dim, config.action_dim, FCBody(config.state_dim, gate=F.tanh))\n    config.discount = 0.99\n    config.use_gae = True\n    config.gae_tau = 0.95\n    config.entropy_weight = 0.01\n    config.rollout_length = 5\n    config.gradient_clip = 0.5\n    run_steps(A2CAgent(config))\n\n\ndef a2c_pixel(**kwargs):\n    generate_tag(kwargs)\n    kwargs.setdefault('log_level', 0)\n    config = Config()\n    config.merge(kwargs)\n\n    config.num_workers = 16\n    config.task_fn = lambda: Task(config.game, num_envs=config.num_workers)\n    config.eval_env = Task(config.game)\n    config.optimizer_fn = lambda params: torch.optim.RMSprop(params, lr=1e-4, alpha=0.99, eps=1e-5)\n    config.network_fn = lambda: CategoricalActorCriticNet(config.state_dim, config.action_dim, NatureConvBody())\n    config.state_normalizer = ImageNormalizer()\n    config.reward_normalizer = SignNormalizer()\n    config.discount = 0.99\n    config.use_gae = True\n    config.gae_tau = 1.0\n    config.entropy_weight = 0.01\n    config.rollout_length = 5\n    config.gradient_clip = 5\n    config.max_steps = int(2e7)\n    run_steps(A2CAgent(config))\n\n\ndef a2c_continuous(**kwargs):\n    generate_tag(kwargs)\n    kwargs.setdefault('log_level', 0)\n    config = Config()\n    config.merge(kwargs)\n\n    config.num_workers = 16\n    config.task_fn = lambda: Task(config.game, num_envs=config.num_workers)\n    config.eval_env = Task(config.game)\n    config.optimizer_fn = lambda params: torch.optim.RMSprop(params, lr=0.0007)\n    config.network_fn = lambda: GaussianActorCriticNet(\n        config.state_dim, config.action_dim,\n        actor_body=FCBody(config.state_dim), critic_body=FCBody(config.state_dim))\n    config.discount = 0.99\n    config.use_gae = True\n    config.gae_tau = 1.0\n    config.entropy_weight = 0.01\n    config.rollout_length = 5\n    config.gradient_clip = 5\n    config.max_steps = int(2e7)\n    run_steps(A2CAgent(config))\n\n\n# N-Step DQN\ndef n_step_dqn_feature(**kwargs):\n    generate_tag(kwargs)\n    kwargs.setdefault('log_level', 0)\n    config = Config()\n    config.merge(kwargs)\n\n    config.task_fn = lambda: Task(config.game, num_envs=config.num_workers)\n    config.eval_env = Task(config.game)\n    config.num_workers = 5\n    config.optimizer_fn = lambda params: torch.optim.RMSprop(params, 0.001)\n    config.network_fn = lambda: VanillaNet(config.action_dim, FCBody(config.state_dim))\n    config.random_action_prob = LinearSchedule(1.0, 0.1, 1e4)\n    config.discount = 0.99\n    config.target_network_update_freq = 200\n    config.rollout_length = 5\n    config.gradient_clip = 5\n    run_steps(NStepDQNAgent(config))\n\n\ndef n_step_dqn_pixel(**kwargs):\n    generate_tag(kwargs)\n    kwargs.setdefault('log_level', 0)\n    config = Config()\n    config.merge(kwargs)\n\n    config.task_fn = lambda: Task(config.game, num_envs=config.num_workers)\n    config.eval_env = Task(config.game)\n    config.num_workers = 16\n    config.optimizer_fn = lambda params: torch.optim.RMSprop(params, lr=1e-4, alpha=0.99, eps=1e-5)\n    config.network_fn = lambda: VanillaNet(config.action_dim, NatureConvBody())\n    config.random_action_prob = LinearSchedule(1.0, 0.05, 1e6)\n    config.state_normalizer = ImageNormalizer()\n    config.reward_normalizer = SignNormalizer()\n    config.discount = 0.99\n    config.target_network_update_freq = 10000\n    config.rollout_length = 5\n    config.gradient_clip = 5\n    config.max_steps = int(2e7)\n    run_steps(NStepDQNAgent(config))\n\n\n# Option-Critic\ndef option_critic_feature(**kwargs):\n    generate_tag(kwargs)\n    kwargs.setdefault('log_level', 0)\n    config = Config()\n    config.merge(kwargs)\n\n    config.num_workers = 5\n    config.task_fn = lambda: Task(config.game, num_envs=config.num_workers)\n    config.eval_env = Task(config.game)\n    config.optimizer_fn = lambda params: torch.optim.RMSprop(params, 0.001)\n    config.network_fn = lambda: OptionCriticNet(FCBody(config.state_dim), config.action_dim, num_options=2)\n    config.random_option_prob = LinearSchedule(1.0, 0.1, 1e4)\n    config.discount = 0.99\n    config.target_network_update_freq = 200\n    config.rollout_length = 5\n    config.termination_regularizer = 0.01\n    config.entropy_weight = 0.01\n    config.gradient_clip = 5\n    run_steps(OptionCriticAgent(config))\n\n\ndef option_critic_pixel(**kwargs):\n    generate_tag(kwargs)\n    kwargs.setdefault('log_level', 0)\n    config = Config()\n    config.merge(kwargs)\n\n    config.task_fn = lambda: Task(config.game, num_envs=config.num_workers)\n    config.eval_env = Task(config.game)\n    config.num_workers = 16\n    config.optimizer_fn = lambda params: torch.optim.RMSprop(params, lr=1e-4, alpha=0.99, eps=1e-5)\n    config.network_fn = lambda: OptionCriticNet(NatureConvBody(), config.action_dim, num_options=4)\n    config.random_option_prob = LinearSchedule(0.1)\n    config.state_normalizer = ImageNormalizer()\n    config.reward_normalizer = SignNormalizer()\n    config.discount = 0.99\n    config.target_network_update_freq = 10000\n    config.rollout_length = 5\n    config.gradient_clip = 5\n    config.max_steps = int(2e7)\n    config.entropy_weight = 0.01\n    config.termination_regularizer = 0.01\n    run_steps(OptionCriticAgent(config))\n\n\n# PPO\ndef ppo_continuous(**kwargs):\n    generate_tag(kwargs)\n    kwargs.setdefault('log_level', 0)\n    config = Config()\n    config.merge(kwargs)\n\n    config.task_fn = lambda: Task(config.game)\n    config.eval_env = config.task_fn()\n\n    config.network_fn = lambda: GaussianActorCriticNet(\n        config.state_dim, config.action_dim, actor_body=FCBody(config.state_dim, gate=torch.tanh),\n        critic_body=FCBody(config.state_dim, gate=torch.tanh))\n    config.actor_opt_fn = lambda params: torch.optim.Adam(params, 3e-4)\n    config.critic_opt_fn = lambda params: torch.optim.Adam(params, 1e-3)\n    config.discount = 0.99\n    config.use_gae = True\n    config.gae_tau = 0.95\n    config.gradient_clip = 0.5\n    config.rollout_length = 2048\n    config.optimization_epochs = 10\n    config.mini_batch_size = 64\n    config.ppo_ratio_clip = 0.2\n    config.log_interval = 2048\n    config.max_steps = 3e6\n    config.target_kl = 0.01\n    config.state_normalizer = MeanStdNormalizer()\n    run_steps(PPOAgent(config))\n\n\ndef ppo_pixel(**kwargs):\n    generate_tag(kwargs)\n    kwargs.setdefault('skip', False)\n    config = Config()\n    config.merge(kwargs)\n\n    config.task_fn = lambda: Task(config.game, num_envs=config.num_workers)\n    config.eval_env = Task(config.game)\n    config.num_workers = 8\n    config.optimizer_fn = lambda params: torch.optim.Adam(params, lr=2.5e-4)\n    config.network_fn = lambda: CategoricalActorCriticNet(config.state_dim, config.action_dim, NatureConvBody())\n    config.state_normalizer = ImageNormalizer()\n    config.reward_normalizer = SignNormalizer()\n    config.discount = 0.99\n    config.use_gae = True\n    config.gae_tau = 0.95\n    config.entropy_weight = 0.01\n    config.gradient_clip = 0.5\n    config.rollout_length = 128\n    config.optimization_epochs = 4\n    config.mini_batch_size = config.rollout_length * config.num_workers // 4\n    config.ppo_ratio_clip = 0.1\n    config.log_interval = config.rollout_length * config.num_workers\n    config.shared_repr = True\n    config.max_steps = int(2e7)\n    run_steps(PPOAgent(config))\n\n\n# DDPG\ndef ddpg_continuous(**kwargs):\n    generate_tag(kwargs)\n    kwargs.setdefault('log_level', 0)\n    config = Config()\n    config.merge(kwargs)\n\n    config.task_fn = lambda: Task(config.game)\n    config.eval_env = config.task_fn()\n    config.max_steps = int(1e6)\n    config.eval_interval = int(1e4)\n    config.eval_episodes = 20\n\n    config.network_fn = lambda: DeterministicActorCriticNet(\n        config.state_dim, config.action_dim,\n        actor_body=FCBody(config.state_dim, (400, 300), gate=F.relu),\n        critic_body=FCBody(config.state_dim + config.action_dim, (400, 300), gate=F.relu),\n        actor_opt_fn=lambda params: torch.optim.Adam(params, lr=1e-3),\n        critic_opt_fn=lambda params: torch.optim.Adam(params, lr=1e-3))\n\n    config.replay_fn = lambda: UniformReplay(memory_size=int(1e6), batch_size=100)\n    config.discount = 0.99\n    config.random_process_fn = lambda: OrnsteinUhlenbeckProcess(\n        size=(config.action_dim,), std=LinearSchedule(0.2))\n    config.warm_up = int(1e4)\n    config.target_network_mix = 5e-3\n    run_steps(DDPGAgent(config))\n\n\n# TD3\ndef td3_continuous(**kwargs):\n    generate_tag(kwargs)\n    kwargs.setdefault('log_level', 0)\n    config = Config()\n    config.merge(kwargs)\n\n    config.task_fn = lambda: Task(config.game)\n    config.eval_env = config.task_fn()\n    config.max_steps = int(1e6)\n    config.eval_interval = int(1e4)\n    config.eval_episodes = 20\n\n    config.network_fn = lambda: TD3Net(\n        config.action_dim,\n        actor_body_fn=lambda: FCBody(config.state_dim, (400, 300), gate=F.relu),\n        critic_body_fn=lambda: FCBody(\n            config.state_dim + config.action_dim, (400, 300), gate=F.relu),\n        actor_opt_fn=lambda params: torch.optim.Adam(params, lr=1e-3),\n        critic_opt_fn=lambda params: torch.optim.Adam(params, lr=1e-3))\n\n    replay_kwargs = dict(\n        memory_size=int(1e6),\n        batch_size=100,\n    )\n\n    config.replay_fn = lambda: ReplayWrapper(UniformReplay, replay_kwargs)\n    config.discount = 0.99\n    config.random_process_fn = lambda: GaussianProcess(\n        size=(config.action_dim,), std=LinearSchedule(0.1))\n    config.td3_noise = 0.2\n    config.td3_noise_clip = 0.5\n    config.td3_delay = 2\n    config.warm_up = int(1e4)\n    config.target_network_mix = 5e-3\n    run_steps(TD3Agent(config))\n\n\nif __name__ == '__main__':\n    mkdir('log')\n    mkdir('tf_log')\n    set_one_thread()\n    random_seed()\n    # -1 is CPU, a positive integer is the index of GPU\n    select_device(-1)\n    # select_device(0)\n\n    game = 'CartPole-v0'\n    # dqn_feature(game=game, n_step=1, replay_cls=UniformReplay, async_replay=True, noisy_linear=True)\n    # quantile_regression_dqn_feature(game=game)\n    # categorical_dqn_feature(game=game)\n    # rainbow_feature(game=game)\n    # a2c_feature(game=game)\n    # n_step_dqn_feature(game=game)\n    # option_critic_feature(game=game)\n\n    game = 'HalfCheetah-v2'\n    # game = 'Hopper-v2'\n    # a2c_continuous(game=game)\n    # ppo_continuous(game=game)\n    # ddpg_continuous(game=game)\n    # td3_continuous(game=game)\n\n    game = 'BreakoutNoFrameskip-v4'\n    dqn_pixel(game=game, n_step=1, replay_cls=UniformReplay, async_replay=False)\n    # quantile_regression_dqn_pixel(game=game)\n    # categorical_dqn_pixel(game=game)\n    # rainbow_pixel(game=game, async_replay=False)\n    # a2c_pixel(game=game)\n    # n_step_dqn_pixel(game=game)\n    # option_critic_pixel(game=game)\n    # ppo_pixel(game=game)\n"
        },
        {
          "name": "images",
          "type": "tree",
          "content": null
        },
        {
          "name": "requirements.txt",
          "type": "blob",
          "size": 0.19921875,
          "content": "torch==1.5.1\ngym==0.10.8\ntensorflow==1.15.0\natari-py==0.1.7\nopencv-python==4.0.0.21\nscikit-image==0.14.2\ntqdm==4.31.1\npandas==0.24.1\npathlib==1.0.1\nseaborn==0.9.0\nroboschool==1.0.49\ntorchmeta\ntorchvision\n"
        },
        {
          "name": "setup.py",
          "type": "blob",
          "size": 0.634765625,
          "content": "from setuptools import setup, find_packages\nimport sys\n\nprint('Please install OpenAI Baselines (commit 8e56dd) and requirement.txt')\nif not (sys.version.startswith('3.5') or sys.version.startswith('3.6')):\n    raise Exception('Only Python 3.5 and 3.6 are supported')\n\nsetup(name='deep_rl',\n      packages=[package for package in find_packages()\n                if package.startswith('deep_rl')],\n      install_requires=[],\n      description=\"Modularized Implementation of Deep RL Algorithms\",\n      author=\"Shangtong Zhang\",\n      url='https://github.com/ShangtongZhang/DeepRL',\n      author_email=\"zhangshangtong.cpp@gmail.com\",\n      version=\"1.5\")"
        },
        {
          "name": "template_jobs.py",
          "type": "blob",
          "size": 3.251953125,
          "content": "from examples import *\n\n\ndef batch_atari():\n    cf = Config()\n    cf.add_argument('--i', type=int, default=0)\n    cf.add_argument('--j', type=int, default=0)\n    cf.merge()\n\n    games = [\n        'BreakoutNoFrameskip-v4',\n        # 'AlienNoFrameskip-v4',\n        # 'DemonAttackNoFrameskip-v4',\n        # 'MsPacmanNoFrameskip-v4'\n    ]\n\n    algos = [\n        dqn_pixel,\n        quantile_regression_dqn_pixel,\n        categorical_dqn_pixel,\n        rainbow_pixel,\n        a2c_pixel,\n        n_step_dqn_pixel,\n        option_critic_pixel,\n        ppo_pixel,\n    ]\n\n    params = []\n\n    for game in games:\n        for r in range(1):\n            for algo in algos:\n                params.append([algo, dict(game=game, run=r, remark=algo.__name__)])\n            # for n_step in [1, 2, 3]:\n            #     for double_q in [True, False]:\n            #         params.extend([\n            #             [dqn_pixel,\n            #              dict(game=game, run=r, n_step=n_step, replay_cls=PrioritizedReplay, double_q=double_q,\n            #                   remark=dqn_pixel.__name__)],\n                        # [rainbow_pixel,\n                        #  dict(game=game, run=r, n_step=n_step, noisy_linear=False, remark=rainbow_pixel.__name__)]\n                    # ])\n            # params.append(\n            #     [categorical_dqn_pixel, dict(game=game, run=r, remark=categorical_dqn_pixel.__name__)]),\n            # params.append([dqn_pixel, dict(game=game, run=r, remark=dqn_pixel.__name__)])\n\n    algo, param = params[cf.i]\n    algo(**param)\n    exit()\n\n\ndef batch_mujoco():\n    cf = Config()\n    cf.add_argument('--i', type=int, default=0)\n    cf.add_argument('--j', type=int, default=0)\n    cf.merge()\n\n    games = [\n        'dm-acrobot-swingup',\n        'dm-acrobot-swingup_sparse',\n        'dm-ball_in_cup-catch',\n        'dm-cartpole-swingup',\n        'dm-cartpole-swingup_sparse',\n        'dm-cartpole-balance',\n        'dm-cartpole-balance_sparse',\n        'dm-cheetah-run',\n        'dm-finger-turn_hard',\n        'dm-finger-spin',\n        'dm-finger-turn_easy',\n        'dm-fish-upright',\n        'dm-fish-swim',\n        'dm-hopper-stand',\n        'dm-hopper-hop',\n        'dm-humanoid-stand',\n        'dm-humanoid-walk',\n        'dm-humanoid-run',\n        'dm-manipulator-bring_ball',\n        'dm-pendulum-swingup',\n        'dm-point_mass-easy',\n        'dm-reacher-easy',\n        'dm-reacher-hard',\n        'dm-swimmer-swimmer15',\n        'dm-swimmer-swimmer6',\n        'dm-walker-stand',\n        'dm-walker-walk',\n        'dm-walker-run',\n    ]\n\n    games = [\n        'HalfCheetah-v2',\n        'Walker2d-v2',\n        'Swimmer-v2',\n        'Hopper-v2',\n        'Reacher-v2',\n        'Ant-v2',\n        'Humanoid-v2',\n        'HumanoidStandup-v2',\n    ]\n\n    params = []\n\n    for game in games:\n        if 'Humanoid' in game:\n            algos = [ppo_continuous]\n        else:\n            algos = [ppo_continuous, ddpg_continuous, td3_continuous]\n        for algo in algos:\n            for r in range(5):\n                params.append([algo, dict(game=game, run=r)])\n\n    algo, param = params[cf.i]\n    algo(**param, remark=algo.__name__)\n\n    exit()\n\n\nif __name__ == '__main__':\n    mkdir('log')\n    mkdir('data')\n    random_seed()\n\n    # select_device(0)\n    # batch_atari()\n\n    select_device(-1)\n    batch_mujoco()\n"
        },
        {
          "name": "template_plot.py",
          "type": "blob",
          "size": 2.9052734375,
          "content": "import matplotlib\n# matplotlib.use('Agg')\nimport matplotlib.pyplot as plt\n# plt.rc('text', usetex=True)\nfrom deep_rl import *\n\n\ndef plot_ppo():\n    plotter = Plotter()\n    games = [\n        'HalfCheetah-v2',\n        'Walker2d-v2',\n        'Hopper-v2',\n        'Swimmer-v2',\n        'Reacher-v2',\n        'Ant-v2',\n        'Humanoid-v2',\n        'HumanoidStandup-v2',\n    ]\n\n    patterns = [\n        'remark_ppo',\n    ]\n\n    labels = [\n        'PPO'\n    ]\n\n    plotter.plot_games(games=games,\n                       patterns=patterns,\n                       agg='mean',\n                       downsample=0,\n                       labels=labels,\n                       right_align=False,\n                       tag=plotter.RETURN_TRAIN,\n                       root='./data/benchmark/mujoco',\n                       interpolation=100,\n                       window=10,\n                       )\n\n    # plt.show()\n    plt.tight_layout()\n    plt.savefig('images/PPO.png', bbox_inches='tight')\n\n\ndef plot_ddpg_td3():\n    plotter = Plotter()\n    games = [\n        'HalfCheetah-v2',\n        'Walker2d-v2',\n        'Hopper-v2',\n        'Swimmer-v2',\n        'Reacher-v2',\n        'Ant-v2',\n    ]\n\n    patterns = [\n        'remark_ddpg',\n        'remark_td3',\n    ]\n\n    labels = [\n        'DDPG',\n        'TD3',\n    ]\n\n    plotter.plot_games(games=games,\n                       patterns=patterns,\n                       agg='mean',\n                       downsample=0,\n                       labels=labels,\n                       right_align=False,\n                       tag=plotter.RETURN_TEST,\n                       root='./data/benchmark/mujoco',\n                       interpolation=0,\n                       window=0,\n                       )\n\n    # plt.show()\n    plt.tight_layout()\n    plt.savefig('images/mujoco_eval.png', bbox_inches='tight')\n\n\ndef plot_atari():\n    plotter = Plotter()\n    games = [\n        'BreakoutNoFrameskip-v4',\n    ]\n\n    patterns = [\n        'remark_a2c',\n        'remark_categorical',\n        'remark_dqn',\n        'remark_n_step_dqn',\n        'remark_option_critic',\n        'remark_quantile',\n        'remark_ppo',\n        # 'remark_rainbow',\n    ]\n\n    labels = [\n        'A2C',\n        'C51',\n        'DQN',\n        'N-Step DQN',\n        'OC',\n        'QR-DQN',\n        'PPO',\n        # 'Rainbow'\n    ]\n\n    plotter.plot_games(games=games,\n                       patterns=patterns,\n                       agg='mean',\n                       downsample=100,\n                       labels=labels,\n                       right_align=False,\n                       tag=plotter.RETURN_TRAIN,\n                       root='./data/benchmark/atari',\n                       interpolation=0,\n                       window=100,\n                       )\n\n    # plt.show()\n    plt.tight_layout()\n    plt.savefig('images/Breakout.png', bbox_inches='tight')\n\n\nif __name__ == '__main__':\n    mkdir('images')\n    # plot_ppo()\n    # plot_ddpg_td3()\n    plot_atari()"
        }
      ]
    }
  ]
}