{
  "metadata": {
    "timestamp": 1736559785586,
    "page": 507,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjUxMA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "NervanaSystems/neon",
      "stars": 3872,
      "defaultBranch": "master",
      "files": [
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.4287109375,
          "content": "*.dll\n*.dylib\n*.dSYM\n*.sublime-project\n*.sublime-workspace\n*.pyc\n*.pkl\n*.prm\n*.so\n*.swo\n*.swp\n.DS_Store\nneon/version.py\n*@eaDir\n.pkgs\n*.egg-info\n.venv\n.venv[23]\n.styleenv\n.coverage\nbuild\n*.gz\ngenerated\n*.ropeproject\n*.cubin\n*.hdf5\n*.h5\n*.html\n*.txt\n*.log\nneon_help_output.txt\nneon/backends/util/cuda_capability\nneon/backends/kernels/ptx\nneon/backends/kernels/pre\nneon/backends/kernels/dump\nneon/data/loader/loader\n.idea/\naeon\ndist\nmklml_*\n"
        },
        {
          "name": "CONTRIBUTING.rst",
          "type": "blob",
          "size": 3.990234375,
          "content": "\n.. ---------------------------------------------------------------------------\n.. Copyright 2016-2018 Intel Corporation\n..\n.. Licensed under the Apache License, Version 2.0 (the \"License\");\n.. you may not use this file except in compliance with the License.\n.. You may obtain a copy of the License at\n..\n..      http://www.apache.org/licenses/LICENSE-2.0\n..\n.. Unless required by applicable law or agreed to in writing, software\n.. distributed under the License is distributed on an \"AS IS\" BASIS,\n.. WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n.. See the License for the specific language governing permissions and\n.. limitations under the License.\n.. ---------------------------------------------------------------------------\n\nContribution Process\n--------------------\n\n1. File an issue:\n\n   * Create an issue on github:\n     https://github.com/NervanaSystems/neon/issues\n\n2. Clone and/or update your checked out copy of neon to ensure you have the\n   most recent commits from the master branch:\n\n.. code-block:: bash\n\n    git clone https://github.com/NervanaSystems/neon.git\n    cd neon\n    git fetch origin\n    git checkout master\n    git pull\n\n3. Create a new feature branch for your work and switch to it.  Give it a\n   meaningful name related to the task(s) at hand:\n\n.. code-block:: bash\n\n    git checkout -b my_new_feature_branch\n\n4. Locally build neon\n\n.. code-block:: bash\n\n    make\n\n5. Ideally you'd start by creating one or more unit tests with the\n   functionality you expect your new feature to perform.  These should reside\n   under the appropriate tests subdirectory of whatever you are changing.\n   Then hack away at the code until you feel your feature is complete.  Once\n   satisfied, run the code through the following checks:\n\n.. code-block:: bash\n\n    make check   # ensure this is clean or your patch won't be accepted\n    make test   # ensure all are OK\n    make style  # ensure there are no style related issues\n    make lint   # (optional).  We still have a fair bit to clean up currently!\n\n6. If necessary you may want to update and/or rebuild the documentation.\n   This all exists under doc/source and is in \n   `Sphinx reStructuredText format <http://sphinx-doc.org/rest.html>`_:\n\n.. code-block:: bash\n\n    make html  # builds docs locally, starts a webserver so you can view\n\n7. Commit your changes and push your feature branch to your github fork.  Be\n   sure to add a descriptive message and reference the github issue associated\n   with your task (ex. #1).  You will also want to rebase your commits down to\n   a single sensible commit to make things clean for the merge process:\n\n.. code-block:: bash\n\n    git add my_updated_file.txt\n    git commit -m \"Added new awesome functionality.  Closes issue #1\"\n    git push origin my_new_feature_branch\n\n8. Create a new pull request to get your feature branch merged into master for\n   others to use.  You'll first need to ensure your feature branch contains the\n   latest changes from master.  Furthermore, internal devs will need to assign\n   the request to someone else for a code review.  You must also ensure there\n   are no errors when run through the items defined in step 5.\n\n.. code-block:: bash\n\n    # (external contribs): make a new pull request:\n    https://github.com/NervanaSystems/neon/pulls\n\n    # merge latest master changes into your feature branch\n    git fetch origin\n    git checkout master\n    git pull origin master\n    git checkout my_new_feature_branch\n    git merge master  # you may need to manually resolve any merge conflicts\n\n9. If there are issues you can continue to push commits to your feature branch\n   by following step 7.  They will automatically be added to this same merge\n   request.\n\n10. Once your change has been successfully merged, you can remove the source\n   branch and ensure your local copy is up to date:\n\n.. code-block:: bash\n\n    git fetch origin\n    git checkout master\n    git pull\n    git branch -d my_new_feature_branch\n    git branch -d -r origin/my_new_feature_branch\n\n11. Give yourself a high five for a job well done!\n"
        },
        {
          "name": "ChangeLog",
          "type": "blob",
          "size": 35.4638671875,
          "content": "# ChangeLog\n\n## v2.6.0 (2018-01-05):\n\n* Further optimized MKL backend performance for SSD inference\n* Updated MKLML to version 20171227\n* Enabled neon install with MKLML on Mac OSX\n\n## v2.5.0 (2017-12-21):\n\n* Optimized SSD MKL backend performance (~3X boost version over version)\n* Bumped aeon version to v1.3.0\n* Fixed inference performance issue of MKL batchnorm\n* Fixed batch prediction issue for gpu backend\n* Enabled subset_pct for MNIST_DCGAN example\n* Updated \"make clean\" to clean up mkl artifacts\n* Added dockerfile for IA mkl\n\n## v2.4.0 (2017-11-27):\n\n* Enabled pip install through pypi\n* Updated MKLML to version 20171007 with performance improve of ~3X for mnist datalayer/nondatalayer and ~1.6X for DCGAN/WGAN datalayer\n* Updated resnet model to optimize performance with MKLML 20171007\n* Updated Alexnet weight file and fixed bug for deep dream\n* Fixed faster-rcnn inference model loading issue\n* Added data_loading time measurement and enabled GAN networks benchmarking\n* Updated to Aeon version 1.2.0\n* Enabled neon build with mklEngine on Windows systems\n\n## v2.3.0 (2017-10-27):\n\n* Optimized DeepSpeech2 MKL backend performance (~7X improvement over the CPU backend)\n* Fused convolution and bias layer which significantly boosted AlexNet and VGG performance on Intel architectures with MKL backend\n* Made SSD and Faster-RNN use VGG weight files in new format\n* Fixed use of reset_cells hyperparameter\n* Fixed MKL backend bug for GAN and Faster-RCNN models\n\n## v2.2.0 (2017-09-27):\n\n* Update MKLML version 20170908 that fixes a bug related to data conversions)\n* Add SSD example for bounding box object detection that works for both GPU and MKL backend\n* Add DeepSpeech2 MKL backend optimization that features ~3X improvement\n* Update aeon to 1.0.0 including new version of manifest (doc/source/loading_data.rst#aeon-dataloader)\n* Add CHWD Support for Batch Normalization in mkl backend\n* Modify ResNet-50 model's last layer to match the original ResNet-50 model paper\n* Enable Seq2Seq testing and benchmarking\n\n## v2.1.0 (2017-08-02):\n\n* Set MKL backend (-b mkl) as the default CPU backend on Linux (use -b cpu to specify original CPU backend)\n* Update MKLML version 20170720 (AVX512 code paths enabled by default and conversion optimizations)\n* Simplify ResNet example\n* Makefiles now check for virtualenv and pkg-config (NervanaSystems/neon#383)\n* Fix Deep Speech2 model on MKL backend\n* Fix MKL installation for \"make sysinstall\"\n\n## v2.0.0 (2017-06-27):\n\n* Added support for MKL backend (-b mkl) on Linux, which boosts neon CPU performance significantly\n* Added WGAN model examples for LSUN and MNIST data\n* Enabled WGAN and DCGAN model examples for Python3\n* Added fix (using file locking) to prevent race conditions running multiple jobs on the same machine with multiple GPUs\n* Added functionality to display some information about hardware, OS and model used\n* Updated appdirs to 1.4.3 to be compatibile on Centos 7.3 for appliance\n\n## v1.9.0 (2017-05-03):\n\n* Add support for 3D deconvolution\n* Generative Adversarial Networks (GAN) implementation, and MNIST DCGAN example, following GoodFellow 2014 (http://arXiv.org/abs/1406.2661)\n* Implement Wasserstein GAN cost function and make associated API changes for GAN models\n* Add a new benchmarking script with per-layer timings\n* Add weight clipping for GDM, RMSProp, Adagrad, Adadelta and Adam optimizers\n* Make multicost an explicit choice in mnist_branch.py example\n* Enable NMS kernels to work with normalized boxes and offset\n* Fix missing links in api.rst [#366]\n* Fix docstring for --datatype option to neon [#367]\n* Fix perl shebang in maxas.py and allow for build with numpy 1.12 [#356]\n* Replace os.path.join for Windows interoperability [#351]\n* Update aeon to 0.2.7 to fix a seg fault on termination\n\n## v1.8.2 (2017-02-23):\n\n* Make the whale calls example stable and shuffle dataset before splitting into subsets\n* Reduce default depth in cifar_msra example to 2\n* Fix the formatting of the conv layer description\n* Fix documentation error in the video-c3d example\n* Support greyscale videos\n\n## v1.8.1 (2017-01-17):\n\n* Bug fix: Add dilation to object dict and assign defaults to dil_w = dil_h = 1 [#335, #336]\n* Bug fix: Prevent GPU backend from ignoring non-zero slope in Rectlinclip and change default slope to 0\n* Bug fix: Nesterov momentum was updating velocities incorrectly\n\n## v1.8.0 (2016-12-28):\n\n* Skip Thought Vectors (http://arxiv.org/abs/1506.06726) example\n* Dilated convolution support\n* Nesterov Accelerated Gradient option to SGD optimizer\n* MultiMetric class to allow wrapping Metric classes\n* Support for serializing and deserializing encoder-decoder models\n* Allow specifying the number of time steps to evaluate during beam search\n* A new community-contributed Docker image\n* Improved error messages when a tensor is created with an invalid shape or reshaped to an incompatible size\n* Fix bugs in MultiCost support\n* Documentation fixes [#331]\n\n## v1.7.0 (2016-11-21):\n\n* Update Data Loader to aeon https://github.com/NervanaSystems/aeon for flexible,\n  multi-threaded data loading and transformations\n* Add Neural Machine Translation model\n* Remove Fast RCNN model (use Faster RCNN model instead)\n* Remove music_genres example\n* Fix super blocking for small N with 1D conv\n* Fix update-direct conv kernel for small N\n* Add gradient clipping to Adam optimizer\n* Documentation updates and bug fixes\n\n## v1.6.0 (2016-09-21):\n\n* Faster RCNN model\n* Sequence to Sequence container and char_rae recurrent autoencoder model\n* Reshape Layer that reshapes the input [#221]\n* Pip requirements in requirements.txt updated to latest versions [#289]\n* Remove deprecated data loaders and update docs\n* Use NEON_DATA_CACHE_DIR envvar as archive dir to store DataLoader ingested data\n* Eliminate type conversion for FP16 for CUDA compute capability >= 5.2\n* Use GEMV kernels for batch size 1\n* Alter delta buffers for nesting of merge-broadcast layers\n* Support for ncloud real-time logging\n* Add fast_style Makefile target\n* Fix Python 3 builds on Ubuntu 16.04\n* Run setup.py for sysinstall to generate version.py [#282]\n* Fix broken link in mnist docs\n* Fix conv/deconv tests for CPU execution and fix i32 data type\n* Fix for average pooling with batch size 1\n* Change default scale_min to allow random cropping if omitted\n* Fix yaml loading\n* Fix bug with image resize during injest\n* Update references to the ModelZoo and neon examples to their new locations\n\n## v1.5.4 (2016-07-15):\n\n* Implement Binarized Neural Networks from http://arxiv.org/pdf/1602.02830v3.pdf\n* Bug fixes [#268]\n\n## v1.5.3 (2016-07-07):\n\n* Bug fixes [#267]\n\n## v1.5.2 (2016-07-06):\n\n* Bug fixes to audio loader\n\n## v1.5.1 (2016-06-30):\n\n* Bug fixes\n\n## v1.5.0 (2016-06-29):\n\n### Modifications\n\n* Python2/Python3 compatibility [#191]\n* Support for Pascal GPUs\n* Persistent RNN kernels [#262]\n* Dataloader enhancements (audio loader with examples)\n* HDF5 file data iterator\n* Convolution kernel improvements\n* Winograd kernel for fprop/bprop and 5x5 stride 1 filters\n* API documentation improvements [#234, #244, #263]\n* Cache directory cleanup\n* Reorganization of all unit tests\n* Check for compatible shapes before doing a memcpy [#182, #183]\n* Bug fixes [#231, #241, #253, #257, #259]\n\n## v1.4.0 (2016-04-29):\n\n### Modifications\n\n* VGG16 based Fast R-CNN model using winograd kernels\n* new, backward compatible, generic data loader\n* C3D video loader model trained on UCF101 dataset\n* Deep Dream example\n* make conv layer printout more informative [#222]\n* fix some examples to use new arg override capability\n* improve performance for relu for small N\n* better support for arbitrary batch norm layer placement\n* documentation updates [#210, #213, #236]\n\n## v1.3.0 (2016-03-03):\n\n### Modifications\n\n* winograd kernels and associated autotuning routines\n* benchmarking scripts\n* deprecation of deterministic argument for backend constructor\n* improve batch norm stability with fp16 backend\n* allow strided support for dimshuffle kernel\n* speed up zero momentum gradient descent\n\n## v1.2.2 (2016-02-24):\n\n### Modifications\n\n* benchmarking enhancements\n* fast dimshuffle, transpose, other kernel speedups and refactoring\n* batch norm states fix, deterministic updates\n* example fixes for fast rcnn and conv_autoencoder\n* image decoding rescaling method fix\n* deserialization fixes for RNN's, refactoring\n* caffe compatibility fixes\n* documentation updates\n\n## v1.2.1 (2016-02-05):\n\n### Modifications\n\n* New MergeSum, Colornoise layers\n* support for aspect_ratio scaling augmentation\n* updated IMDB sentiment analysis example\n* generic CSV batchwriter\n* various build and deserialization bugfixes, doc updates\n\n## v1.2.0 (2016-01-29):\n\n### Modifications\n\n* kepler GPU kernel support [#80]\n* new dataloader format, updated docs [#115, #170]\n* new serialization format\n* FastRCNN implementation, ROI pooling support [#135]\n* deep residual nets implementation and example\n* expanded model zoo\n* Ticker dataset and copy, repeat copy tasks\n* autodiff transpose support [#173]\n* numerous bug fixes and documentation updates.\n\n## v1.1.5 (2016-01-13):\n\n### Modifications\n\n* CUDA kernels for lookuptable layer (up to 4x speedup)\n* support for determinstic Conv layer updatesa\n* LRN layer support\n* custom dataset walkthrough utilizing bAbI data\n* reduced number of threads in deep reduction EW kernels [#171]\n* additional (de)serialization routines [#106]\n* CPU tensor slicing fix\n* corrections for PrecisionRecall, MultiLabelStats [#148]\n* explicitly specify python2.7 for virtualenv [#155]\n* default to SM50 when no working GPU found [#186]\n* Add alpha to ELU activation [#164]\n* deconv callback fix [#162]\n* various documentation updates [#151, #152]\n\n## v1.1.4 (2015-12-14):\n\n### Modifications\n\n* Add support for bidirectional RNNs and LSTMs\n* added ELU, leaky ReLU activations\n* significantly faster GPU kernel builds (using ptx instead of cuda-c)\n* data shuffling enhancements, removal of old data loader code.\n* caffe conv, pool, dropout layer matching and compatibility flags\n* add scheduling support for RMSProp\n* callback enhancements, additional unit tests\n* documentation auditing, added links to introductory video tutorials\n\n## v1.1.3 (2015-12-01):\n\n### Modifications\n\n* deconvolution and weight histogram visualization examples and documentation\n* CPU convolution and pooling layer speedups (~2x faster)\n* bAbI question and answer interactive demo, dataset support.\n* various ImageLoader enhancements.\n* interactive usage improvements (shortcut Callback import, multiple Callbacks\n  init, doc updates, single item batch size support)\n* set default verbosity level to warning\n* CIFAR10 example normalization updates\n* CUDA detection enhancements [#132]\n* only parse batch_writer arguments when used as a script, allow undefined\n  global_mean [#137, #140]\n\n## v1.1.2 (2015-11-17):\n\n### Modifications\n\n* completely re-written C++ multithreaded dataloader\n* new weight initialization options for recurrent layers\n* Added deconvolution visualization support (guided backprop)\n* new bAbI question answering example network\n* Improved performance of cifar10_allcnn, word_lstm examples\n* new CUDA-C max and avg pooling kernels\n* Additional bugfixes and documentation updates\n\n## v1.1.1 (2015-11-06):\n\n### Modifications\n\n* Callback initialization bug fix [#127]\n* IMDB LSTM example bug fix [#130]\n* Added cuda-convnet2 style binary dropout variant\n* Added benchmark function to model (separate fprop, bprop, update timings)\n* Remove h_buffer references in lieu of outputs for recurrent layers\n* Multi-cost output buffer bugfix for inference [#131]\n* New timeseries prediction and generation example\n* Change Callback initialization to re-support named arguments.  Separate out\n  these arguments in argparser. [#128]\n\n## v1.1.0 (2015-10-30):\n\n### Modifications\n\n* Sentiment analysis support (LSTM lookupTable based), new IMDB example\n* Support for merge and branch layer stacks via LayerContainers\n  * Sequential, Tree, MergeBroadcast, MergeMultiStream\n* Support for freezing layer stacks\n* Adagrad optimizer support\n* new GPU kernels for fast compounding batch norm, conv and pooling engine\n  updates, new kernel build system and flags.\n* Modifications for Caffe support\n  * conv, pooling, P/Q updates, dropout layer normalization more in-line with\n    Caffe approach.  NOTE: this breaks backwards compatibility with some\n\tstrided conv/pool related models serialized using older versions of neon\n\tas the output sizes may now be different.  See the FAQ for more info.\n  * serialization enhancements to make caffe model import/export easier\n  * use per-channel mean subtraction instead of single global.  NOTE: this\n    breaks backwards compatibility with ImgMaster saved datasets prior to this\n\trevision.  To correct, please use the included `update_dataset_cache.py`\n\tscript in the util directory.\n* Default training cost display during progress bar is now calculated on a\n  rolling window basis rather than from the beginning of each epoch\n* Separate Layer configuration and initialization steps\n* YAML based alexnet example\n* Callback enhancements.\n  * now pass args instead of having to spell out callbacks in each example\n  * Changed validation callback to loss callback, validation_frequency now\n    evaluation_frequency\n  * Generic metric callback.\n* Various bug fixes\n  * non-contiguous array get for GPUTensors\n  * 1D slicing returns 2D matrices\n  * bin/neon serialization fixes for RNNs\n  * 3D conv fixes for fprop, bprop\n  * batch norm inference fix\n  * bias layer size fix\n* Documentation updates and improvements\n\n## v1.0.0 (2015-09-15):\n\n### Modifications\n\nPrimarily bug fixes:\n\n* Ensure root logging handler setup [#82]\n* C++ utility for CUDA compatibility checking [#83]\n* Add predict function to models [#86]\n* Fix bug in learning rate schedule impacting deserialization\n* Speed up batch norm computation\n* Average gradients in OpTree, fix tests\n* Use inference mode for fprop during validation\n* Add top-k misclassifcation metric\n* Simplify maxas install, make vis requirements optional, doc updates.\n\n## v1.0.0rc1 (2015-09-08):\n\n### Modifications\n\n* RNN/LSTM\n  * Code is cleaner and achieves state of the art results on the Penn Tree Bank\n    dataset using RNN/LSTM/GRU\n  * Fast image captioning model (~200x faster than CPU based NeuralTalk) on\n    flickr8k dataset\n* Basic automatic differentiation support\n* Framework for visualizations (supported via callbacks)\n* Top-down refactoring & redesign to enable quicker iteration while keeping the\n  speedups offered by our nervanagpu kernels\n  * Datasets are easier to specify\n  * Backend now uses OpTrees (similar to nervanagpu) to support autodiff\n  * nervanagpu merged in as a neon backend to simplify development and use\n  * YAML syntax is simplified (but not backwards compatible)\n  * Better documentation and wider test coverage\n\nThe following features will be added in upcoming releases:\n\n* Advanced automatic differentiation & computational graph support\n* Support for Kepler and older generation GPUs\n* Multi-GPU support & hyperparameter optimization\n\nThis release was made possible thanks to the heroic efforts of the following\ncontributors:\n* Yinyin Liu\n* Yixing Lao\n* Alex Park\n* Evren Tumer\n* Gabriel Pereyra\n* JD Co-Reyes\n* Will Constable\n* Scott Leishman\n* Angel Zhang\n* Hunter Lang\n* Arjun Bansal\n* Anil Thomas\n* Augustus Odena\n* Urs Koster\n* Scott Gray\n* Jenkins\n\n\n## v0.9.0 (2015-07-20)\n\n### Modifications\n\n* Version bump for the v0.9.0 release. [Scott Leishman]\n\n* Merge branch 'MGPUMerge' into 'master' [Scott Leishman]\n\n  Mgpu merge\n\n  Cleanup on top of Multi-GPU branch, addresses small bugs and makes some readability improvements.\n\n  See merge request !31\n\n* Merge branch 'master' of gitlab.localdomain:algorithms/neon into MGPUMerge. [Scott Leishman]\n\n  Incorporate validation_pct changes.\n\n* Merge branch 'ZebTech-val_split'. [Scott Leishman]\n\n  Adds ability to partition training set cases into validation set.\n  Closes #58\n\n* Add more documentation, remove duplicated test. [Scott Leishman]\n\n* Implemented validation set splitting. [seba-1511]\n\n* Set default device list in gen_backend. [Alex Park]\n\n* Ignore mgpu tensor tests that require more than 1 device when only 1 device is present. [Alex Park]\n\n* Remove references to DIST, MGPU tests with &lt; 2 devices. [Scott Leishman]\n\n* Small fixes and code cleanup for MultiGPU. [Urs Koster]\n\n* Implement multigpu processing using weird trick (data parallel for local layers, model parallel for fully-connected layers) Remove compatibility with cudanet backend for imageset based models Remove mpi-based parallelism Remove dependency on NoPar structures Fix check_grad to initialize with backend Remove need to link and initialize layers separately Add documentation for multiple gpu usage, device id specification. [Alex Park]\n\n\n## v0.8.2 (2015-07-08)\n\n### Modifications\n\n* Version bump for the v0.8.2 release. [Scott Leishman]\n\n* Merge branch 'more_misc_fixes' into 'master' [Urs Koster]\n\n  Various bug fixes\n\n  Collection of fixes to address issues with 1D CPU tensor handling, Leaky ReLU backprop, better GPU / CUDA checking, liveness indication for Tensor values, and a new dataset to highlight building regression models.\n\n  See merge request !27\n\n* Merge branch 'master' into more_misc_fixes. [Scott Leishman]\n\n* Merge branch 'evren/refactor_tox' into 'master' [Scott Leishman]\n\n  Evren/refactor tox\n\n  The jenkins job for neon is using to to run tests with python 2.7 and 3.4 but the xUnit output from nosetests is getting overwritten since nosetests tries to write to the same file for both tests (2.7 and 3.4).  This fix puts makes the two files have different names.\n\n  Instead of changing Makefile, I put the fix in tox.ini.\n\n  Scott, I thought you would be best to look at this.\n\n  See merge request !28\n\n* Try another way without hardwiring test attr filters. [Evren Tumer]\n\n* Changed tox.ini to stop py34 test from overwriting py27 nosetests.xml file. [Evren Tumer]\n\n* Make octal numbers python2 and python3 compliant. [Scott Leishman]\n\n* Use python3 compatible pickle. [Scott Leishman]\n\n* Initial inroads into flatfile dataset. [Scott Leishman]\n\n  Used for streaming (live) inference currently.  Other usecases do not yet work.\n\n* Port of multi learning rule param serialization fix. [Scott Leishman]\n\n  See #40162164201505\n\n* Add missing rectlin_derivative from NervanaGPU backend. [Scott Leishman]\n\n* Fix bug in RectLeaky bprop_func. [Scott Leishman]\n\n  Added unit tests, closes #39973152922213\n\n* Merge branch 'master' into more_misc_fixes. [Scott Leishman]\n\n* Merge branch 'evren/myl-250/serialize-error' into 'master' [Scott Leishman]\n\n  Evren/myl 250/serialize error\n\n  Generated new test for serialization.\n\n  Added feature for retaining a subset of the checkpoint files.\n\n  Added test for checkpoint files.\n\n  See merge request !22\n\n* Python3 compatible print. [Scott Leishman]\n\n* Remove no longer relevant comment. [Scott Leishman]\n\n* Fix small style issue. [Evren Tumer]\n\n* Merge branch 'evren/myl-250/serialize-error' of http://gitlab.localdomain/algorithms/neon into evren/myl-250/serialize-error. [Evren Tumer]\n\n  rebased on master\n\n* Added some docs for new checkpoint saving feature, added skip test for broken serialization tests. [Evren Tumer]\n\n* Have make serialize run test_serialize.py. [Scott Leishman]\n\n* Fixed style errors, remove i1k because it is freezing the test, added 'slow' label to the serialize test. [Evren Tumer]\n\n* Exclude serialization test from make test. [Evren Tumer]\n\n* Added checkpoint test. [Evren Tumer]\n\n* Added test generator, added code to keep some checkpoint files, added comments. [Evren Tumer]\n\n* Clean up hack. [Evren Tumer]\n\n* Added cpu-&gt;gpu handoff and gpu-&gt;cpu handoff tests. [Evren Tumer]\n\n* Fixed issue with python not expanding ~ in paths. [Evren Tumer]\n\n* Added yml and fixed name bug. [Evren Tumer]\n\n* Initial code for new serialization test. [Evren Tumer]\n\n* Added some docs for new checkpoint saving feature, added skip test for broken serialization tests. [Evren Tumer]\n\n* Have make serialize run test_serialize.py. [Scott Leishman]\n\n* Fixed style errors, remove i1k because it is freezing the test, added 'slow' label to the serialize test. [Evren Tumer]\n\n* Exclude serialization test from make test. [Evren Tumer]\n\n* Added checkpoint test. [Evren Tumer]\n\n* Added test generator, added code to keep some checkpoint files, added comments. [Evren Tumer]\n\n* Clean up hack. [Evren Tumer]\n\n* Added cpu-&gt;gpu handoff and gpu-&gt;cpu handoff tests. [Evren Tumer]\n\n* Fixed issue with python not expanding ~ in paths. [Evren Tumer]\n\n* Added yml and fixed name bug. [Evren Tumer]\n\n* Initial code for new serialization test. [Evren Tumer]\n\n* Merge branch 'fully_connected_layer_unit_tests' into 'master' [Scott Leishman]\n\n  Fully connected layer unit tests\n\n  CPU unittests of fprop/brop for FCLayer that check if output/delta buffers are set to the right size.\n\n  See merge request !24\n\n* Minor style fix. [Scott Leishman]\n\n* Tensor comparison fix for val_init test. [Scott Leishman]\n\n* Added numerical checks. [GabrielPereyra]\n\n* Added identity initialization for deterministic testing (doesn't import) [GabrielPereyra]\n\n* Added some GPU (cudanet) tests. [Scott Leishman]\n\n* CPU unit test of fprop/bprop for FCLayer. [GabrielPereyra]\n\n* Restored ANNOTATED_EXAMPLE. [Urs Koster]\n\n* Fixed CPU 1D indexing inconsistency. [Scott Leishman]\n\n  Closes MYL-221, #38758848739023\n\n* Fix CPU backend update when batch_size 1. [Scott Leishman]\n\n  Closes #47, MYL-260, #38750574965487\n\n* Add posix_ipc to dev requirements. [Scott Leishman]\n\n* Remove nvidia-smi dependency for cudanet GPU checking. [Scott Leishman]\n\n  Closes #51\n\n* Added housing data, simple regression network. [Scott Leishman]\n\n  Still to tune example, closes MYL-258, closes #33\n\n* Mark non-persistent tensor values.  Closes MYL-251. [Scott Leishman]\n\n* Merge branch 'NvgpuCompat' into 'master' [Scott Leishman]\n\n  Nvgpu compat\n\n  This is a pretty minor change but makes it easier to keep up to date with changes in nervanagpu because it uses the ng tensor constructors rather than the GPUTensor constructors directly.  (Recent changes to nervanagpu have changed the way the tensors are constructed)\n\n  See merge request !21\n\n* Merge branch 'master' into NvgpuCompat. [Scott Leishman]\n\n* Quick patch for RNN docs. [Scott Leishman]\n\n* Minor fixes. [Scott Leishman]\n\n  Remove now un-needed import, extraneous function calls during fullset\n  prediction.\n\n* Change array creation routines to use nervanagpu calls rather than instantiating GPUTensor directly. [Alex Park]\n\n* Merge branch 'MYL261-RNN2' into 'master' [Scott Leishman]\n\n  RNN and LSTM updates\n\n  Fixes issue with prediction using GPU backend.  Closes #16\n\n  - Minor cleanup to numerical gradient code, removing hardcoded element indices.\n  - Mobydick dataset changed to use only the 96 printable ASCII characters and to remove line breaks from text.\n  - Updated dtype handling so fp64 with CPU backend is supported. Used for numerical gradients.\n  - Some additional documentation for LSTM layer.\n\n  See merge request !20\n\n* Update default logging level for rnn, lstm examples. [Scott Leishman]\n\n* RNN cleanup: - added documentation to annotated_example yaml - restored functionality to generate strings from predictions - cleaned up dtype checking. [Urs Koster]\n\n* Restored ability to run numerical gradients in fp64, in a clean way using the backend_type in the yaml. [Urs Koster]\n\n* Fixes issue with prediction using GPU backend.     Minor cleanup to numerical gradient code, removing hardcoded element indices     Mobydick dataset changed to use only the 96 printable ASCII characters and     to remove line breaks from text. [Urs Koster]\n\n* Merge branch 'bnormfix2' into 'master' [Urs Koster]\n\n  Bnormfix2\n\n  Corrects calculation of global means and variances used during batch norm inference\n\n  - Uses exponential moving average to keep a running estimate of the global average mean and variance\n  - Added some helper functions to ensure efficient computation of moving average without allocating extra space\n  - Requires latest version of cuda-convnet2 to ensure correct computation for cc2 backend\n  - May make things slower for nervanagpu during training due to extra overhead of computing global stats that wasn't happening before\n\n  See merge request !18\n\n* Correct calculation of global means and variances used during batch norm inference. [Urs Koster]\n\n* Merge branch 'misc_fixes' into 'master' [Anil Thomas]\n\n  Miscellaneous fixes and updates\n\n  Collection of various small fixes including:\n  * MOP updates to express value persistence across backend begin and end calls\n  * Removal of extraneous backend clip calls where appropriate\n  * python 3 compatibility fixes\n  * Revamped metrics comparison\n  * training error notation updates\n  * serialization testing fixes\n  * make develop target fixes\n\n  See merge request !17\n\n* Merge branch 'master' into misc_fixes. [Scott Leishman]\n\n* Merge pull request #46 from itsb/master. [Scott Leishman]\n\n  fix broken link in README\n\n* Fix broken link in README. [itsb]\n\n* Merge branch 'rmsprop2' into 'master' [Scott Leishman]\n\n  Rmsprop2\n\n  Implement RMSprop, inheriting from GradientDescentMomentum\n  - Simplify calling of compounded kernels in nervanagpu for learning rules\n  - Change default behavior of gradient descent with momentum if momentum params are not included to behave as if momentum_coef is 0\n  - Change default settings of adadelta to use suggested values for rho and epsilon if not provided\n  - Update documentation for optimizers\n  - Include example of rmsprop in ANNOTATED_EXAMPLE.yaml\n  - Include example of rmsprop in mnist-tuned.yaml\n\n  closes MYL-118, #43\n\n  See merge request !15\n\n* Doc and style updates, plus rms_update fix for cc2. [Scott Leishman]\n\n* Merge branch 'master' into rmsprop2. [Scott Leishman]\n\n* Merge branch 'clients' into 'master' [Anil Thomas]\n\n  Shared-memory based IPC mechanism - this is to support third party\n  applications that want to interface with neon for live inference.\n\n* Shared-memory based IPC mechanism. [Anil Thomas]\n\n  This is to support third party applications that want to interface\n  with neon for live inference.\n\n* Merge branch 'notebook' into 'master' [Anil Thomas]\n\n  iPython notebook example\n\n  Added an iPython notebook example using neon to train an MNIST MLP and visualize results.\n\n  See merge request !13\n\n* Changed default backend to CPU. [Arjun Bansal]\n\n* Added iPython notebook example. [Arjun Bansal]\n\n* - Implement RMSprop, inheriting from GradientDescentMomentum - Simplify calling of compounded kernels in nervanagpu for learning rules - Change default behavior of gradient descent with momentum if momentum params are not included to behave as if momentum_coef is 0 - Change default settings of adadelta to use suggested values for rho and epsilon if not provided - Update documentation for optimizers - Include example of rmsprop in ANNOTATED_EXAMPLE.yaml - Include example of rmsprop in mnist-tuned.yaml. [Alex Park]\n\n  Add init for RMSProp from WeightLayer, Inherit from GradientDescentMomentum because of common characteristics\n\n  Update documentation, make default values for optimizer params, change default behavior of gradient descent with momentum if momentum params are not included to behave as if momentum_coef is 0\n\n  Revert mnist-tuned back to using gradient descent momentum\n\n* Ensure pip utilizes newest cudanet version. [Scott Leishman]\n\n* Merge branch 'BatchNormReshape2' into 'master' [Urs Koster]\n\n  Batch norm reshape\n\n  - Change how reshaping is done for local layers in batch norm and shared biases.\n  - Reduce chance of memory leak in nervanagpu calls by reducing creation of reshape references.\n  - Use new behavior of cudanet to return reshape views rather than reshape underlying tensor\n\n  See merge request !11\n\n* Improved handling of tensor allocations by using views. -clean up unnecessary tensor allocations -rather than reshaping repeatedly, store a reusable reshaped view -Moved reshaping for batch norm -update Makefile dependency for cudanet to 0.2.6. [Urs Koster]\n\n* Fix minor formatting issue in serialize check. [Scott Leishman]\n\n* Added persist_values to tensors and their creation. [Scott Leishman]\n\n  Closes MYL-246.  Note that no tensors have been initialized as not being\n  persistent as of yet (deferring to default in all cases).\n\n* Work around backend clip calls where possible.  Closes MYL-247. [Scott Leishman]\n\n* Remove unused pre-commit hook. [Scott Leishman]\n\n  Prevented make develop based install where users didn't have appropriate pep8\n  version already in place.\n\n* Python3 compatibility fixes.  Closes #35. [Scott Leishman]\n\n* Revamped metrics comparison. [Scott Leishman]\n\n  * Now compare across like backends only (by default).\n  * Make output more tabular, and easier to see at a glance.\n\n* Revert leading zero to colon based notation. [Scott Leishman]\n\n* Leading zero epoch and partial mini-batch display. [Scott Leishman]\n\n  Closes #40.\n\n* Merge branch 'RectleakyGPU' into 'master' [Scott Leishman]\n\n  Rectleaky gpu\n\n  Add RectLeaky to gpu backend to address github issue #39\n\n  See merge request !10\n\n* Restore nervanagpu based sanity checks. [Scott Leishman]\n\n* Add RectLeaky activation for gpu backend. [Alex Park]\n\n* Merge branch 'SerializeSnapshots' into 'master' [Scott Leishman]\n\n  Serialize snapshots\n\n  Add option to yaml allowing model snapshots to be serialized on a schedule.  Snapshots will be serialized to provided `serialize_path` and the schedule can be  either:\n\n  -  explicitly specified using a list of ints, `[N1, N2, N3, ...]`, indicating that serialization will occur after epoch `N1`, epoch `N2`, epoch `N3`, etc., or\n  -  specified using an integer interval, `N`, indicating that serialization will occur every `N` epochs.\n\n  See merge request !8\n\n* Expanded docs. [Scott Leishman]\n\n  Also fixed ANNOTATED_EXAMPLE\n\n* Rewrite save_snapshot to show control flow a little more clearly. [Alex Park]\n\n* Documentation for serialize schedule. [Alex Park]\n\n* Implement snapshot saving according to an epoch list or epoch interval for mlp and rnn models. [Alex Park]\n\n* Merge branch 'ZebTech-cifar100' into 'master' [Scott Leishman]\n\n  Addition of CIFAR100 dataset\n\n* Minor docstring format updates. [Scott Leishman]\n\n* Fixed MNIST and CIFAR100 testing. [seba-1511]\n\n* Merge remote-tracking branch 'neon/master' into cifar100. [seba-1511]\n\n* Support prediction generation for RNNs and LSTMs. [Scott Leishman]\n\n  This fixes #23.\n\n* Merge branch 'cifar100' of https://github.com/ZebTech/neon into ZebTech-cifar100. [Scott Leishman]\n\n* Merge branch 'Kaixhin-docker_readme' [Scott Leishman]\n\n  Added Docker image links to install docs and README.  Fixes #24.\n\n* Move docker image links into source install docs. [Scott Leishman]\n\n  Provided reference links from README and quick-start page.\n\n* Update readme with Docker images. [Kaixhin]\n\n* Removed debugging lines. [seba-1511]\n\n* Added CIFAR100 to the ANNOTED_EXAMPLE. [seba-1511]\n\n* Added CIFAR100 to the documentation. [seba-1511]\n\n* Moved coarse-labels as art of kwargs. [seba-1511]\n\n* Fixed number of classes in docstring. [seba-1511]\n\n* Added CIFAR100 loader. Closes issue #28. [seba-1511]\n\n* Merge branch 'rnn-docs' into 'master' [Scott Leishman]\n\n  Rnn docs\n\n  Added doc-strings describing the dataset format expected for Recurrent Neural Networks (RNNs).\n\n  See merge request !7\n\n* Fix section headings, other typos. [Scott Leishman]\n\n  Also fix minor doc path issue to ensure docstrings are parsed from local\n  build area.\n\n* Updated documentation for recurrent networks and datasets. [Urs Koster]\n\n* Merge branch 'bn-compound2' into 'master' [Scott Leishman]\n\n  Bn compound2\n\n  Added gpu backend calls for fprop and bprop pass of Batch Normalization, which results in a 10% overall speedup on Alexnet. Also deletes minibatch provider at the end of training to free up device DDR for inference.\n\n  See merge request !6\n\n* Removed extraneous RNN train_mode false call. [Scott Leishman]\n\n* Delete minibatch provider after training to save memory. [Urs Koster]\n\n* Added gpu backend calls for fprop and bprop pass of Batch Normalization, which results in a 10% overall. [Urs Koster]\n\n* Merge branch 'noyaml' into 'master' [Scott Leishman]\n\n  Noyaml\n\n  Add example code to create networks without .yaml.\n\n  See merge request !4\n\n* Added noyaml example to docs. [Scott Leishman]\n\n* Merge branch 'master' into noyaml. [Scott Leishman]\n\n* Merge branch 'IntegrationTest' into 'master' [Scott Leishman]\n\n  Added Integration tests\n\n  * Added integration tests based on our current example networks and backends.\n    * Requires Maxwell GPU with nervanagpu and cudanet backends installed, as well as imagenet dataset.\n    * New command line parameter `--integration` that cleans up YAML files to make them more easily\n      reproducible.\n    * Currently requires manual inspection of results relative to prior runs on the same machine to\n      determine if outputs are feasible.\n  * Added tolerances to the serialization tests.\n\n  See merge request !2\n\n* Allow CL params for outfile and logfile to integration tests. [Scott Leishman]\n\n* Serialize Makefile cleanup. [Scott Leishman]\n\n* Merge branch 'master' into IntegrationTest. [Scott Leishman]\n\n* Merge pull request #20 from Kaixhin/change_cuda_check. [Scott Leishman]\n\n  Change GPU check to CUDA SDK check. Closes issue #19\n\n* Change GPU check to CUDA SDK check. Closes issue #19. [Kaixhin]\n\n* Cleanup prior integration approach files. [Scott Leishman]\n\n* Ensure integration tests run from all directories. [Scott Leishman]\n\n* Swap branch with non-branch yaml again. [Alex Park]\n\n* Allow reuse of example yamls for integration testing - Add integration command line option to neon executable - Adjust yaml options in integration mode - Among other things, drop serialization path, remove sources of randomization - Add is_random attribute to layers to identify drop-able layers   (Just dropout for now) - Make dotransforms have the correct behavior in Imageset - Change batch norm warnings to static to avoid proliferation of messages across many layers - Switch branch and small layers for cifar mlp since they were incorrectly labeled - Create integration test script in examples directory that uses existing metrics code - Correct i1k-alexnet-fp16 to use fp16 as backend data type. [Alex Park]\n\n* Added checks for nervanagpu and cudanet in dev tests. [Arjun Bansal]\n\n* Style fixes. [Arjun Bansal]\n\n* Deleted commented layers in tests yamls. [Arjun Bansal]\n\n* Moved test yamls to a separate subdirectory. [Arjun Bansal]\n\n* Deleted the cpu/gpu command line parameters for serialize and integration since they aren't used right now. [Arjun Bansal]\n\n* Added tolerances for serialization tests. [Arjun Bansal]\n\n* Added integration tests. [Arjun Bansal]\n\n* Add example code to create networks without .yaml. [Anil Thomas]\n\n  It is possible to create and use networks without using a .yaml file.\n  This is illustrated by examples/mlp/mnist-small-noyaml.py.\n\n* Simplify the expression for computing output dims. [Anil Thomas]\n\n  This is for computing dimensions of an output feature map in a\n  convolutional or pooling layer.\n\n* Let datasets have access to the experiment object. [Anil Thomas]\n\n  Custom plug-ins may use this functionality instead of defining own\n  experiment classes.\n\n* Documentation link updates. [Scott Leishman]\n\n* Merge pull request #13 from kellyp/master. [Scott Leishman]\n\n  Update readme with correct using neon link\n\n* Update readme with correct using neon link. [Kelly Plummer]\n\n* Fix broken links in docs, remove i1K. [Scott Leishman]\n\n* Convnet/i1k-alexnet-fp16.yaml was using float32 &amp; mb=64. fixed. [Arjun Bansal]\n\n* Change the value of the sumWidth parameter. [Anil Thomas]\n\n  This parameter affects the performance of the weight gradient computation\n  in the cudanet backend.\n\n* Fix computation of the number of samples. [Anil Thomas]\n\n  This issue was causing neon to compute the number of samples\n  incorrectly when &quot;predictions&quot; is specified in the .yaml file\n  and the number of samples in the validation set is different\n  from that in the training set.\n\n\n## v0.8.1 (2015-05-04)\n\n### Modifications\n\n* Initial public release of neon. [Scott Leishman]\n\n\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 11.0908203125,
          "content": "                                 Apache License\n                           Version 2.0, January 2004\n                        http://www.apache.org/licenses/\n\n   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n\n   1. Definitions.\n\n      \"License\" shall mean the terms and conditions for use, reproduction,\n      and distribution as defined by Sections 1 through 9 of this document.\n\n      \"Licensor\" shall mean the copyright owner or entity authorized by\n      the copyright owner that is granting the License.\n\n      \"Legal Entity\" shall mean the union of the acting entity and all\n      other entities that control, are controlled by, or are under common\n      control with that entity. For the purposes of this definition,\n      \"control\" means (i) the power, direct or indirect, to cause the\n      direction or management of such entity, whether by contract or\n      otherwise, or (ii) ownership of fifty percent (50%) or more of the\n      outstanding shares, or (iii) beneficial ownership of such entity.\n\n      \"You\" (or \"Your\") shall mean an individual or Legal Entity\n      exercising permissions granted by this License.\n\n      \"Source\" form shall mean the preferred form for making modifications,\n      including but not limited to software source code, documentation\n      source, and configuration files.\n\n      \"Object\" form shall mean any form resulting from mechanical\n      transformation or translation of a Source form, including but\n      not limited to compiled object code, generated documentation,\n      and conversions to other media types.\n\n      \"Work\" shall mean the work of authorship, whether in Source or\n      Object form, made available under the License, as indicated by a\n      copyright notice that is included in or attached to the work\n      (an example is provided in the Appendix below).\n\n      \"Derivative Works\" shall mean any work, whether in Source or Object\n      form, that is based on (or derived from) the Work and for which the\n      editorial revisions, annotations, elaborations, or other modifications\n      represent, as a whole, an original work of authorship. For the purposes\n      of this License, Derivative Works shall not include works that remain\n      separable from, or merely link (or bind by name) to the interfaces of,\n      the Work and Derivative Works thereof.\n\n      \"Contribution\" shall mean any work of authorship, including\n      the original version of the Work and any modifications or additions\n      to that Work or Derivative Works thereof, that is intentionally\n      submitted to Licensor for inclusion in the Work by the copyright owner\n      or by an individual or Legal Entity authorized to submit on behalf of\n      the copyright owner. For the purposes of this definition, \"submitted\"\n      means any form of electronic, verbal, or written communication sent\n      to the Licensor or its representatives, including but not limited to\n      communication on electronic mailing lists, source code control systems,\n      and issue tracking systems that are managed by, or on behalf of, the\n      Licensor for the purpose of discussing and improving the Work, but\n      excluding communication that is conspicuously marked or otherwise\n      designated in writing by the copyright owner as \"Not a Contribution.\"\n\n      \"Contributor\" shall mean Licensor and any individual or Legal Entity\n      on behalf of whom a Contribution has been received by Licensor and\n      subsequently incorporated within the Work.\n\n   2. Grant of Copyright License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      copyright license to reproduce, prepare Derivative Works of,\n      publicly display, publicly perform, sublicense, and distribute the\n      Work and such Derivative Works in Source or Object form.\n\n   3. Grant of Patent License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      (except as stated in this section) patent license to make, have made,\n      use, offer to sell, sell, import, and otherwise transfer the Work,\n      where such license applies only to those patent claims licensable\n      by such Contributor that are necessarily infringed by their\n      Contribution(s) alone or by combination of their Contribution(s)\n      with the Work to which such Contribution(s) was submitted. If You\n      institute patent litigation against any entity (including a\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\n      or a Contribution incorporated within the Work constitutes direct\n      or contributory patent infringement, then any patent licenses\n      granted to You under this License for that Work shall terminate\n      as of the date such litigation is filed.\n\n   4. Redistribution. You may reproduce and distribute copies of the\n      Work or Derivative Works thereof in any medium, with or without\n      modifications, and in Source or Object form, provided that You\n      meet the following conditions:\n\n      (a) You must give any other recipients of the Work or\n          Derivative Works a copy of this License; and\n\n      (b) You must cause any modified files to carry prominent notices\n          stating that You changed the files; and\n\n      (c) You must retain, in the Source form of any Derivative Works\n          that You distribute, all copyright, patent, trademark, and\n          attribution notices from the Source form of the Work,\n          excluding those notices that do not pertain to any part of\n          the Derivative Works; and\n\n      (d) If the Work includes a \"NOTICE\" text file as part of its\n          distribution, then any Derivative Works that You distribute must\n          include a readable copy of the attribution notices contained\n          within such NOTICE file, excluding those notices that do not\n          pertain to any part of the Derivative Works, in at least one\n          of the following places: within a NOTICE text file distributed\n          as part of the Derivative Works; within the Source form or\n          documentation, if provided along with the Derivative Works; or,\n          within a display generated by the Derivative Works, if and\n          wherever such third-party notices normally appear. The contents\n          of the NOTICE file are for informational purposes only and\n          do not modify the License. You may add Your own attribution\n          notices within Derivative Works that You distribute, alongside\n          or as an addendum to the NOTICE text from the Work, provided\n          that such additional attribution notices cannot be construed\n          as modifying the License.\n\n      You may add Your own copyright statement to Your modifications and\n      may provide additional or different license terms and conditions\n      for use, reproduction, or distribution of Your modifications, or\n      for any such Derivative Works as a whole, provided Your use,\n      reproduction, and distribution of the Work otherwise complies with\n      the conditions stated in this License.\n\n   5. Submission of Contributions. Unless You explicitly state otherwise,\n      any Contribution intentionally submitted for inclusion in the Work\n      by You to the Licensor shall be under the terms and conditions of\n      this License, without any additional terms or conditions.\n      Notwithstanding the above, nothing herein shall supersede or modify\n      the terms of any separate license agreement you may have executed\n      with Licensor regarding such Contributions.\n\n   6. Trademarks. This License does not grant permission to use the trade\n      names, trademarks, service marks, or product names of the Licensor,\n      except as required for reasonable and customary use in describing the\n      origin of the Work and reproducing the content of the NOTICE file.\n\n   7. Disclaimer of Warranty. Unless required by applicable law or\n      agreed to in writing, Licensor provides the Work (and each\n      Contributor provides its Contributions) on an \"AS IS\" BASIS,\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n      implied, including, without limitation, any warranties or conditions\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n      PARTICULAR PURPOSE. You are solely responsible for determining the\n      appropriateness of using or redistributing the Work and assume any\n      risks associated with Your exercise of permissions under this License.\n\n   8. Limitation of Liability. In no event and under no legal theory,\n      whether in tort (including negligence), contract, or otherwise,\n      unless required by applicable law (such as deliberate and grossly\n      negligent acts) or agreed to in writing, shall any Contributor be\n      liable to You for damages, including any direct, indirect, special,\n      incidental, or consequential damages of any character arising as a\n      result of this License or out of the use or inability to use the\n      Work (including but not limited to damages for loss of goodwill,\n      work stoppage, computer failure or malfunction, or any and all\n      other commercial damages or losses), even if such Contributor\n      has been advised of the possibility of such damages.\n\n   9. Accepting Warranty or Additional Liability. While redistributing\n      the Work or Derivative Works thereof, You may choose to offer,\n      and charge a fee for, acceptance of support, warranty, indemnity,\n      or other liability obligations and/or rights consistent with this\n      License. However, in accepting such obligations, You may act only\n      on Your own behalf and on Your sole responsibility, not on behalf\n      of any other Contributor, and only if You agree to indemnify,\n      defend, and hold each Contributor harmless for any liability\n      incurred by, or claims asserted against, such Contributor by reason\n      of your accepting any such warranty or additional liability.\n\n   END OF TERMS AND CONDITIONS\n\n   APPENDIX: How to apply the Apache License to your work.\n\n      To apply the Apache License to your work, attach the following\n      boilerplate notice, with the fields enclosed by brackets \"[]\"\n      replaced with your own identifying information. (Don't include\n      the brackets!)  The text should be enclosed in the appropriate\n      comment syntax for the file format. We also recommend that a\n      file or class name and description of purpose be included on the\n      same \"printed page\" as the copyright notice for easier\n      identification within third-party archives.\n\n   Copyright [yyyy] [name of copyright owner]\n\n   Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.\n"
        },
        {
          "name": "MANIFEST.in",
          "type": "blob",
          "size": 0.017578125,
          "content": "include README.md\n"
        },
        {
          "name": "Makefile",
          "type": "blob",
          "size": 9.5517578125,
          "content": "# ******************************************************************************\n# Copyright 2014-2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ******************************************************************************\n# Top-level control of the building/installation/cleaning of various targets\n#\n# set empty to prevent any implicit rules from firing.\n.SUFFIXES:\n\n# Choose default Python version; overrideable with \"make python2\" or \"make python3\".\n\nPY := $(shell python --version 2>&1  | cut -c8)\nVIRTUALENV_DIR_BASE := .venv\nSTYLEVIRTUALENV_DIR_BASE := .styleenv\n\n# get release version info\nRELEASE := $(strip $(shell grep '^VERSION *=' setup.py | cut -f 2 -d '=' \\\n\t                         | tr -d \"\\'\"))\n\n# basic check to see if any CUDA compatible GPU is installed\n# set this to false to turn off GPU related functionality\nHAS_GPU := $(shell nvcc neon/backends/util/check_gpu.c > /dev/null 2>&1 && ./a.out && rm a.out && echo true)\n\nifdef HAS_GPU\n# Get CUDA_ROOT for LD_RUN_PATH\nexport CUDA_ROOT:=$(abspath $(shell which nvcc)/../..)\nelse\n# Try to find CUDA.  Kernels will still need nvcc in path\nexport CUDA_ROOT:=$(firstword $(wildcard $(addprefix /usr/local/, cuda-8.0 cuda-7.5 cuda-7.0 cuda)))\n\nifdef CUDA_ROOT\nexport PATH:=$(CUDA_ROOT)/bin:$(PATH)\nHAS_GPU := $(shell $(CUDA_ROOT)/bin/nvcc neon/backends/util/check_gpu.c > /dev/null 2>&1 && ./a.out && rm a.out && echo true)\nendif\nendif\nifdef CUDA_ROOT\n# Compiling with LD_RUN_PATH eliminates the need for LD_LIBRARY_PATH\n# when running\nexport LD_RUN_PATH:=$(CUDA_ROOT)/lib64\nendif\n\n# set this to true to install visualization dependencies and functionality\n# (off by default)\nVIS :=\n\n# style checking related\nSTYLE_CHECK_OPTS :=\nSTYLE_CHECK_DIRS := neon bin/* tests examples benchmark\n\n# pytest options\nTEST_OPTS :=\nTEST_DIRS := tests/\n# turn off GPU tests if no GPU present\n# TODO: refactor neon/backends/tests to run under CPU\nifneq ($(HAS_GPU), true)\n\tTEST_DIRS := -k \"cpu or mkl\" -m \"not hasgpu\" $(TEST_DIRS)\nelse\n\tTEST_DIRS := -m \"hasgpu or not hasgpu and not mkl_only\" $(TEST_DIRS)\nendif\n\n# this variable controls where we publish Sphinx docs to\nDOC_DIR := doc\nDOC_PUB_RELEASE_PATH := $(DOC_PUB_PATH)/$(RELEASE)\n\n#neon mklEngine object\nMKL_ENGINE  := neon/backends/mklEngine\n\nVIRTUALENV := $(shell command -v virtualenv 2> /dev/null)\nifndef VIRTUALENV\n    $(error \"virtualenv must be installed. Consider `pip install virtualenv`.\")\nendif\n\nifeq ($(PY), 2)\n\tVIRTUALENV_EXE := virtualenv -p python2.7\n\tPYLINT3K_ARGS := --disable=no-absolute-import\n\tVIRTUALENV_DIR = $(VIRTUALENV_DIR_BASE)$(PY)\n\tACTIVATE = $(VIRTUALENV_DIR)/bin/activate\nelse\n\tVIRTUALENV_EXE := python3 -m venv\n\tPYLINT3K_ARGS :=\n\tVIRTUALENV_DIR = $(VIRTUALENV_DIR_BASE)3\n\tACTIVATE = $(VIRTUALENV_DIR)/bin/activate\nendif\n\n.PHONY: default all env sysinstall sysinstall_nodeps neon_install python2 python3 \\\n\t    sysdeps sysuninstall clean_py clean_so \\\n\t    clean test coverage style lint lint3k check doc html release examples \\\n\t    serialize_check $(MKL_ENGINE)\n\ndefault: env\n\nall:\n\t$(MAKE) PY=3 TEST_OPTS='$(TEST_OPTS)' test\n\t$(MAKE) PY=2 TEST_OPTS='$(TEST_OPTS)' test\n\nenv: $(MKL_ENGINE) $(ACTIVATE)\n\npython2: VIRTUALENV_EXE := virtualenv -p python2.7\npython2: VIRTUALENV_DIR := $(VIRTUALENV_DIR_BASE)2\npython2: ACTIVATE := $(VIRTUALENV_DIR)/bin/activate\npython2: PYLINT3K_ARGS := --disable=no-absolute-import\npython2: env\n\npython3: VIRTUALENV_EXE := python3 -m venv\npython3: VIRTUALENV_DIR := $(VIRTUALENV_DIR_BASE)3\npython3: ACTIVATE := $(VIRTUALENV_DIR)/bin/activate\npython3: PYLINT3K_ARGS :=\npython3: env\n\n$(ACTIVATE): requirements.txt gpu_requirements.txt vis_requirements.txt\n\t@echo \"Updating virtualenv dependencies in: $(VIRTUALENV_DIR)...\"\n\t@test -d $(VIRTUALENV_DIR) || $(VIRTUALENV_EXE) $(VIRTUALENV_DIR)\n\t@. $(ACTIVATE); pip install -U pip\n\t@# cython added separately due to h5py dependency ordering bug.  See:\n\t@# https://github.com/h5py/h5py/issues/535\n\t@. $(ACTIVATE); pip install cython==0.23.1\n\t@. $(ACTIVATE); pip install -r requirements.txt\n\t@. $(ACTIVATE); $(MAKE) aeon_install\nifeq ($(VIS), true)\n\t@echo \"Updating visualization related dependecies in $(VIRTUALENV_DIR)...\"\n\t@. $(ACTIVATE); pip install -r vis_requirements.txt\nendif\n\t@echo\nifeq ($(HAS_GPU), true)\n\t@echo \"Updating GPU dependencies in $(VIRTUALENV_DIR)...\"\n\t@. $(ACTIVATE); pip install -r gpu_requirements.txt\n\t@echo\nendif\n\t@echo \"Installing neon in development mode...\"\n\t@. $(ACTIVATE); python setup.py develop\n\t@rm -f $(VIRTUALENV_DIR_BASE); ln -s $(VIRTUALENV_DIR) $(VIRTUALENV_DIR_BASE)\n\t@echo \"###########################################################\"\n\t@echo \"Setup complete.  Type:\"\n\t@echo \"    . '$(ACTIVATE)'\"\n\t@echo \"to work interactively ($(VIRTUALENV_DIR) also symlinked to $(VIRTUALENV_DIR_BASE))\"\n\t@echo \"###########################################################\"\n\t@touch $(ACTIVATE)\n\t@echo\n$(MKL_ENGINE):\n\t@echo \"Building MKL Engine...\"\n\t@./install_mkl.sh intel\n\n# TODO: handle kernel/.so compilation via setup.py directly\nsysinstall_nodeps: neon_install\nsysinstall: $(MKL_ENGINE) sysdeps neon_install\nneon_install:\n\t@echo \"Installing neon system wide...\"\n\t@python setup.py install\n\t@echo\n\naeon_install:\n\t@echo \"Attempting to install optional aeon dataloader...\"\n\t-@git clone https://github.com/NervanaSystems/aeon.git aeon ; cd aeon && git fetch && git checkout tags/v1.3.0 && mkdir -p build && cd build && rm -rf * && cmake .. && pip install .\n\nsysdeps:\n\t@echo \"Installing neon dependencies system wide...\"\n\t@# cython added separately due to h5py dependency ordering bug.  See:\n\t@# https://github.com/h5py/h5py/issues/535\n\t@pip install cython==0.23.1\n\t@pip install -r requirements.txt\n\t$(MAKE) aeon_install\nifeq ($(VIS), true)\n\t@pip install -r vis_requirements.txt\nendif\nifeq ($(HAS_GPU), true)\n\t@pip install -r gpu_requirements.txt\nendif\n\nsysuninstall:\n\t@echo \"Uninstalling neon system wide...\"\n\t@pip uninstall neon\n\t@echo\n\nclean_py:\n\t@echo \"Cleaning compiled python object files...\"\n\t@find . -name \"*.py[co]\" -type f -delete\n\t@echo\n\nclean_so:\n\t@echo \"Cleaning compiled shared object files...\"\n\t@cd $(MKL_ENGINE) && $(MAKE) clean\n\t@echo\n\nclean: clean_py clean_so\n\t@echo \"Removing virtual environment files...\"\n\t@rm -rf aeon build dist mklml_*\n\t@rm -rf $(VIRTUALENV_DIR_BASE) $(VIRTUALENV_DIR_BASE)2 $(VIRTUALENV_DIR_BASE)3 $(STYLEVIRTUALENV_DIR_BASE)\n\t@echo\n\ntest: env\n\t@echo \"Running unit tests...\"\n\t@. $(ACTIVATE); py.test --cov=neon $(TEST_OPTS) $(TEST_DIRS); coverage xml\n\t@echo\n\nsystest:\n\t@echo \"Running unit tests...\"\n\tpy.test $(TEST_OPTS) $(TEST_DIRS)\n\t@echo\n\nbenchmarks: env\n\t@echo \"Running all benchmarks...\"\n\t@. $(ACTIVATE); tests/run_benchmarks.py\n\t@echo\n\nserialize_check: env\n\t@echo \"Running CPU backend test of model serialization\"\n\t@. $(ACTIVATE); python tests/serialization_check.py -e 10 -b cpu\n\t@echo\n\ncoverage: env\n\t@. $(ACTIVATE); py.test --cov=neon tests/\n\t@echo\n\nstyle: env\n\t@. $(ACTIVATE); flake8 $(STYLE_CHECK_OPTS) $(STYLE_CHECK_DIRS)\n\t@. $(ACTIVATE); pylint --reports=n --py3k $(PYLINT3K_ARGS) --ignore=.venv *\n\t@echo\n\n# doesn't install everything, just runs flake8 and pylint on the code\n# using a style environment\nfast_style:\n\tvirtualenv .styleenv\n\t. .styleenv/bin/activate; pip install `grep flake8 requirements.txt`\n\t. .styleenv/bin/activate; pip install `grep pylint requirements.txt`\n\t. .styleenv/bin/activate; flake8 $(STYLE_CHECK_OPTS) $(STYLE_CHECK_DIRS)\n\t. .styleenv/bin/activate; pylint --reports=n --py3k $(PYLINT3K_ARGS) --ignore=.styleenv *\n\t@echo\n\nlint: env\n\t@. $(ACTIVATE); pylint --output-format=colorized neon\n\t@echo\n\nlint3k: env\n\t@. $(ACTIVATE); pylint --py3k $(PYLINT3K_ARGS) --ignore=.venv *\n\t@echo\n\ncheck: env\n\t@echo \"Running style checks.  Number of style errors is... \"\n\t-@. $(ACTIVATE); flake8 --count $(STYLE_CHECK_OPTS) $(STYLE_CHECK_DIRS) \\\n\t                 > /dev/null\n\t@echo\n\t@echo \"Number of missing docstrings is...\"\n\t-@. $(ACTIVATE); pylint --disable=all --enable=missing-docstring -r n \\\n\t                 neon | grep \"^C\" | wc -l\n\t@echo\n\t@echo \"Running unit tests...\"\n\t-@. $(ACTIVATE); py.test $(TEST_DIRS) | tail -1 | cut -f 2,3 -d ' '\n\t@echo\n\ndoc: env\nifeq ($(HAS_GPU), true)\n\t@. $(ACTIVATE); neon --help > doc/source/neon_help_output.txt\n\t$(MAKE) -C $(DOC_DIR) clean\n\t@. $(ACTIVATE); $(MAKE) -C $(DOC_DIR) html\n\t@echo \"Documentation built in $(DOC_DIR)/build/html\"\nelse\n\t@echo \"Skip documentation built as no GPU found\"\nendif\n\t@echo\n\nhtml: doc\n\t@echo \"To view documents open your browser to: http://localhost:8000\"\n\t@cd $(DOC_DIR)/build/html; python -m SimpleHTTPServer\n\t@echo\n\npublish_doc: doc\nifneq (, $(DOC_PUB_HOST))\n\t@echo \"relpath: $(DOC_PUB_RELEASE_PATH)\"\n\t@-cd $(DOC_DIR)/build/html && \\\n\t\trsync -avz -essh --perms --chmod=ugo+rX . \\\n\t\t$(DOC_PUB_USER)@$(DOC_PUB_HOST):$(DOC_PUB_RELEASE_PATH)\n\t@-ssh $(DOC_PUB_USER)@$(DOC_PUB_HOST) \\\n\t\t'rm -f $(DOC_PUB_PATH)/latest && \\\n\t\t ln -sf $(DOC_PUB_RELEASE_PATH) $(DOC_PUB_PATH)/latest'\nelse\n\t@echo \"Can't publish.  Ensure DOC_PUB_HOST, DOC_PUB_USER, DOC_PUB_PATH set\"\nendif\n\ndist: env\n\t@echo \"Prepping distribution...\"\n\t@python setup.py bdist_wheel\n\nrelease: check dist\n\t@echo \"Bump version number in setup.py\"\n\t@vi setup.py\n\t@echo \"Update ChangeLog\"\n\t@vi ChangeLog\n\t@echo \"TODO: commit changes\"\n\t@echo \"TODO: publish release to PYPI\"\n\t@echo \"TODO (manual script): publish documentation\"\n\t@echo\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 7.6669921875,
          "content": "**DISCONTINUATION OF PROJECT.**  This project will no longer be maintained by Intel.  Intel will not provide or guarantee development of or support for this project, including but not limited to, maintenance, bug fixes, new releases or updates.  Patches to this project are no longer accepted by Intel. If you have an ongoing need to use this project, are interested in independently developing it, or would like to maintain patches for the community, please create your own fork of the project.\n\n# neon\n\n[neon](https://github.com/NervanaSystems/neon) is Intel's reference deep learning framework committed to [best performance](https://github.com/soumith/convnet-benchmarks) on all hardware. Designed for ease-of-use and extensibility.\n\n* [Tutorials](http://neon.nervanasys.com/docs/latest/tutorials.html) and [iPython notebooks](https://github.com/NervanaSystems/meetup) to get users started with using neon for deep learning.\n* Support for commonly used layers: convolution, RNN, LSTM, GRU, BatchNorm, and more.\n* [Model Zoo](https://github.com/NervanaSystems/ModelZoo) contains pre-trained weights and example scripts for start-of-the-art models, including: [VGG](https://github.com/NervanaSystems/ModelZoo/tree/master/ImageClassification/ILSVRC2012/VGG), [Reinforcement learning](https://github.com/NervanaSystems/ModelZoo/tree/master/DeepReinforcement), [Deep Residual Networks](https://github.com/NervanaSystems/ModelZoo/tree/master/SceneClassification/DeepResNet), [Image Captioning](https://github.com/NervanaSystems/ModelZoo/tree/master/ImageCaptioning), [Sentiment analysis](https://github.com/NervanaSystems/ModelZoo/tree/master/NLP/SentimentClassification/IMDB), and [more](http://neon.nervanasys.com/docs/latest/model_zoo.html).\n* Swappable hardware backends: write code once and then deploy on CPUs, GPUs, or Nervana hardware\n\nFor fast iteration and model exploration, neon has the fastest performance among deep learning libraries (2x speed of cuDNNv4, see [benchmarks](https://github.com/soumith/convnet-benchmarks)).\n* 2.5s/macrobatch (3072 images) on AlexNet on Titan X (Full run on 1 GPU ~ 26 hrs)\n* Training VGG with 16-bit floating point on 1 Titan X takes ~10 days (original paper: 4 GPUs for 2-3 weeks)\n\nWe use neon internally at Intel Nervana to solve our customers' problems across many\n[domains](http://www.nervanasys.com/solutions/). We are hiring across several\nroles. Apply [here](http://www.nervanasys.com/careers/)!\n\nSee the [new features](https://github.com/NervanaSystems/neon/blob/master/ChangeLog) in our latest release.\nWe want to highlight that neon v2.0.0+ has been optimized for much better performance on CPUs by enabling Intel Math Kernel Library (MKL). The DNN (Deep Neural Networks) component of MKL that is used by neon is provided free of charge and downloaded automatically as part of the neon installation.\n\n## Quick Install\n\n* [Local install and dependencies](http://neon.nervanasys.com/docs/latest/installation.html)\n\nOn a Mac OSX or Linux machine, enter the following to download and install\nneon (conda users see the [guide](http://neon.nervanasys.com/docs/latest/installation.html)), and use it to train your first multi-layer perceptron. To force a python2 or python3 install, replace `make` below with either `make python2` or `make python3`.\n\n```bash\n    git clone https://github.com/NervanaSystems/neon.git\n    cd neon\n    make\n    . .venv/bin/activate\n```\n\nStarting after neon v2.2.0, the master branch of neon will be updated weekly with work-in-progress toward the next release. Check out a release tag (e.g., \"git checkout v2.2.0\") for a stable release. Or simply check out the \"latest\" release tag to get the latest stable release (i.e., \"git checkout latest\")\n\n* [Install via pypi](https://pypi.python.org/pypi/nervananeon)\n\nFrom version 2.4.0, we re-enabled pip install. Neon can be installed using package name nervananeon. \n\n```bash\n    pip install nervananeon\n```\n\nIt is noted that [aeon](https://aeon.nervanasys.com/index.html/getting_started.html) needs to be installed separately. The latest release v2.6.0 uses aeon v1.3.0.\n\n**Warning**\n\n> Between neon v2.1.0 and v2.2.0, the aeon manifest file format has been changed. When updating from neon < v2.2.0 manifests have to be recreated using ingest scripts (in examples folder) or updated using [this](neon/data/convert_manifest.py) script.\n\n### Use a script to run an example\n\n```bash\n    python examples/mnist_mlp.py \n```\n\n#### Selecting a backend engine from the command line\n\nThe gpu backend is selected by default, so the above command is equivalent to if a compatible GPU resource is found on the system:\n\n```bash\n    python examples/mnist_mlp.py -b gpu\n```\n\nWhen no GPU is available, the **optimized** CPU (MKL) backend is now selected by default as of neon v2.1.0, which means the above command is now equivalent to:\n\n```bash\n    python examples/mnist_mlp.py -b mkl\n```\n\nIf you are interested in comparing the default mkl backend with the non-optimized CPU backend, use the following command:\n\n```bash\n    python examples/mnist_mlp.py -b cpu\n```\n\n### Use a yaml file to run an example\n\nAlternatively, a yaml file may be used run an example.\n\n```bash\n    neon examples/mnist_mlp.yaml\n```\n\nTo select a specific backend in a yaml file, add or modify a line that contains ``backend: mkl`` to enable mkl backend, or ``backend: cpu`` to enable cpu backend.  The gpu backend is selected by default if a GPU is available.\n\n## Recommended Settings for neon with MKL on Intel Architectures\n\nThe Intel Math Kernel Library takes advantages of the parallelization and vectorization capabilities of Intel Xeon and Xeon Phi systems. When hyperthreading is enabled on the system, we recommend \nthe following KMP_AFFINITY setting to make sure parallel threads are 1:1 mapped to the available physical cores. \n\n```bash\n    export OMP_NUM_THREADS=<Number of Physical Cores>\n    export KMP_AFFINITY=compact,1,0,granularity=fine  \n```\nor \n```bash\n    export OMP_NUM_THREADS=<Number of Physical Cores>\n    export KMP_AFFINITY=verbose,granularity=fine,proclist=[0-<Number of Physical Cores>],explicit\n```\nFor more information about KMP_AFFINITY, please check [here](https://software.intel.com/en-us/node/522691).\nWe encourage users to set out trying and establishing their own best performance settings. \n\n\n## Documentation\n\nThe complete documentation for neon is available\n[here](http://neon.nervanasys.com/docs/latest). Some useful starting points are:\n\n* [Tutorials](http://neon.nervanasys.com/docs/latest/tutorials.html) for neon\n* [Overview](http://neon.nervanasys.com/docs/latest/overview.html) of the neon workflow\n* [API](http://neon.nervanasys.com/docs/latest/api.html) documentation\n* [Resources](http://neon.nervanasys.com/docs/latest/resources.html) for neon and deep learning\n\n\n## Support\n\nFor any bugs or feature requests please:\n\n1. Search the open and closed\n   [issues list](https://github.com/NervanaSystems/neon/issues) to see if we're\n   already working on what you have uncovered.\n2. Check that your issue/request hasn't already been addressed in our\n   [Frequently Asked Questions (FAQ)](http://neon.nervanasys.com/docs/latest/faq.html)\n   or [neon-users](https://groups.google.com/forum/#!forum/neon-users) Google\n   group.\n3. File a new [issue](https://github.com/NervanaSystems/neon/issues) or submit\n   a new [pull request](https://github.com/NervanaSystems/neon/pulls) if you\n   have some code you'd like to contribute\n\nFor other questions and discussions please post a message to the\n   [neon-users](https://groups.google.com/forum/?hl=en#!forum/neon-users)\n   Google group\n\n## License\n\nWe are releasing [neon](https://github.com/NervanaSystems/neon) under an open source\n[Apache 2.0](https://www.apache.org/licenses/LICENSE-2.0) License. We welcome you to [contact us](mailto:info@nervanasys.com) with your use cases.\n"
        },
        {
          "name": "bin",
          "type": "tree",
          "content": null
        },
        {
          "name": "doc",
          "type": "tree",
          "content": null
        },
        {
          "name": "docker",
          "type": "tree",
          "content": null
        },
        {
          "name": "examples",
          "type": "tree",
          "content": null
        },
        {
          "name": "gpu_requirements.txt",
          "type": "blob",
          "size": 0.041015625,
          "content": "pycuda>=2015.1\nscikit-cuda==0.5.1\npytools\n"
        },
        {
          "name": "install_mkl.sh",
          "type": "blob",
          "size": 2.822265625,
          "content": "#!/bin/bash\n# ******************************************************************************\n# Copyright 2017-2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ******************************************************************************\nexport CC=\nexport CXX=\nintel=$1\nTHIS_DIR=$(cd $(dirname $0); pwd)\n\nGREEN='\\033[032m'\nNORM='\\033[0m'\nRED='\\033[031m'\n\n#Select Compiler and Find MKL support\nif [[ `uname` == 'Linux' ]]; then\n    ICC_ON=0\n    if [[ $intel == 'intel' ]]; then\n        if [[ `which icc` == '' ]]; then\n            CC=gcc\n        else\n            CC=icc\n        fi\n        if [[ `echo $CC | grep icc` != '' ]]; then\n            ICC_ON=1\n        fi\n    else\n        ICC_ON=0\n        CC=gcc\n    fi\n\n    if [[ $ICC_ON == 1 ]]; then\n       echo -e \"using ${GREEN}Intel compiler ${NORM}...\"\n    else\n       echo -e \"using ${GREEN}GNU compiler${NORM}...\"\n    fi\n\n\n    RETURN_STRING=`./prepare_mkl.sh $ICC_ON`\n    export MKLROOT=`echo $RETURN_STRING | awk -F \"=\" '{print $2}' | awk '{print $1}'`\n    echo -e \"mkl root: ${GREEN}$MKLROOT${NORM}\"\n    export LD_LIBRARY_PATH=$MKLROOT/lib:$LD_LIBRARY_PATH\n    export LIBRARY_PATH=$MKLROOT/lib:$LIBRARY_PATH\n    export CPATH=$MKLROOT/include:$CPATH\n    #build neon mklEngine\n    MKL_ENGINE_PATH='neon/backends/mklEngine'\n    cd $MKL_ENGINE_PATH && make clean && make\n    cd $THIS_DIR\n\nelif [[ `uname` == 'Darwin' ]]; then\n    echo -e \"Mac detected\"\n    RETURN_STRING=`./prepare_mkl.sh 2`\n    export MKLROOT=`echo $RETURN_STRING | awk -F \"=\" '{print $2}' | awk '{print $1}'`\n    echo -e \"mkl root: ${GREEN}$MKLROOT${NORM}\"\n    export LD_LIBRARY_PATH=$MKLROOT/lib:$LD_LIBRARY_PATH\n    export LIBRARY_PATH=$MKLROOT/lib:$LIBRARY_PATH\n    export CPATH=$MKLROOT/include:$CPATH\n    #build neon mklEngine\n    MKL_ENGINE_PATH='neon/backends/mklEngine'\n    export DYLD_LIBRARY_PATH=$MKLROOT/lib:$THIS_DIR/$MKL_ENGINE_PATH:$DYLD_LIBRARY_PATH\n    export MAC_BUILD=1\n    cd $MKL_ENGINE_PATH && make clean && make\n    cp ${MKLROOT}/lib/*.dylib .\n    cd $THIS_DIR\n\nelif [[ \"$(expr substr $(uname -s) 1 10)\" == \"MINGW32_NT\" || \"$(expr substr $(uname -s) 1 7)\" == \"MSYS_NT\" ]]; then\n    echo -e \"Windows detected\"\n    #build neon mklEngine\n    MKL_ENGINE_PATH='neon/backends/mklEngine'\n    cd $MKL_ENGINE_PATH && make clean && ./make_msys64.bat\n    cd $THIS_DIR\n\nelse\n    echo -e \"Environment not supported, skipping MKL install\"\nfi\n"
        },
        {
          "name": "neon",
          "type": "tree",
          "content": null
        },
        {
          "name": "prepare_mkl.sh",
          "type": "blob",
          "size": 2.7548828125,
          "content": "#!/bin/sh\n# ******************************************************************************\n# Copyright 2017-2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ******************************************************************************\n# set -ex\nFindLibrary()\n{\n  case \"$1\" in\n    intel|1)\n      LOCALMKL=`find $DST -name libmklml_intel.so`   # name of MKL SDL lib\n      ;;\n     2)\n      LOCALMKL=`find $DST -name libmklml.dylib`\n      ;;\n     *)\n      LOCALMKL=`find $DST -name libmklml_gnu.so`   # name of MKL SDL lib\n      ;;\n  esac\n\n}\n\nGetVersionName()\n{\nVERSION_LINE=0\nif [ $1 ]; then\n  VERSION_LINE=`grep __INTEL_MKL_BUILD_DATE $1/include/mkl_version.h 2>/dev/null | sed -e 's/.* //'`\nfi\nif [ -z $VERSION_LINE ]; then\n  VERSION_LINE=0\nfi\necho $VERSION_LINE  # Return Version Line\n}\n\n# MKL\nPLATFORM=lnx\nif [ `uname` = 'Darwin' ]; then\n    PLATFORM=mac\nfi\nDST=`dirname $0`\nOMP=0\nVERSION_MATCH=20171227\nARCHIVE_BASE=mklml_${PLATFORM}_2018.0.1.$VERSION_MATCH\nARCHIVE_BASENAME=$ARCHIVE_BASE.tgz\nGITHUB_RELEASE_TAG=v0.12\nMKLURL=\"https://github.com/intel/mkl-dnn/releases/download/$GITHUB_RELEASE_TAG/$ARCHIVE_BASENAME\"\n# there are diffrent MKL lib to be used for GCC and for ICC\nreg='^[0-9]+$'\necho Checking MKLML dependencies...\nVERSION_LINE=`GetVersionName $MKLROOT`\n# Check if MKLROOT is set if positive then set one will be used..\nif [ -z $MKLROOT ] || [ $VERSION_LINE -lt $VERSION_MATCH ]; then\n\t# ..if MKLROOT is not set then check if we have MKL downloaded in proper version\n    VERSION_LINE=`GetVersionName $DST/$ARCHIVE_BASE`\n    if [ $VERSION_LINE -lt $VERSION_MATCH ] ; then\n      #...If it is not then downloaded and unpacked\n      echo Downloading required MKLML version ${ARCHIVE_BASE} ...\n      wget --no-check-certificate -P $DST $MKLURL -O $DST/$ARCHIVE_BASENAME > /dev/null 2>&1\n      tar -xzf $DST/$ARCHIVE_BASENAME -C $DST > /dev/null 2>&1\n    fi\n  FindLibrary $1\n  MKLROOT=$PWD/`echo $LOCALMKL | sed -e 's/lib.*$//'`\n  echo MKLML dependencies installed: MKLROOT=${MKLROOT}\nfi\n\n# Check what MKL lib we have in MKLROOT\nif [ -z `find $MKLROOT -name libmkl_rt.so -print -quit` ]; then\n  LIBRARIES=`basename $LOCALMKL | sed -e 's/^.*lib//' | sed -e 's/\\.so.*$//'`\n  OMP=1\nelse\n  LIBRARIES=\"mkl_rt\"\nfi\n\n\n# return value to calling script (Makefile,cmake)\necho $MKLROOT $LIBRARIES $OMP\n"
        },
        {
          "name": "pylintrc",
          "type": "blob",
          "size": 11.138671875,
          "content": "[MASTER]\n\n# Specify a configuration file.\n#rcfile=\n\n# Python code to execute, usually for sys.path manipulation such as\n# pygtk.require().\n#init-hook=\n\n# Add files or directories to the blacklist. They should be base names, not\n# paths.\nignore=CVS\n\n# Pickle collected data for later comparisons.\npersistent=yes\n\n# List of plugins (as comma separated values of python modules names) to load,\n# usually to register additional checkers.\nload-plugins=\n\n# Use multiple processes to speed up Pylint.\njobs=1\n\n# Allow loading of arbitrary C extensions. Extensions are imported into the\n# active Python interpreter and may run arbitrary code.\nunsafe-load-any-extension=no\n\n# A comma-separated list of package or module names from where C extensions may\n# be loaded. Extensions are loading into the active Python interpreter and may\n# run arbitrary code\nextension-pkg-whitelist=\n\n# Allow optimization of some AST trees. This will activate a peephole AST\n# optimizer, which will apply various small optimizations. For instance, it can\n# be used to obtain the result of joining multiple strings with the addition\n# operator. Joining a lot of strings can lead to a maximum recursion error in\n# Pylint and this flag can prevent that. It has one side effect, the resulting\n# AST will be different than the one from reality.\noptimize-ast=no\n\n\n[MESSAGES CONTROL]\n\n# Only show warnings with the listed confidence levels. Leave empty to show\n# all. Valid levels: HIGH, INFERENCE, INFERENCE_FAILURE, UNDEFINED\nconfidence=\n\n# Enable the message, report, category or checker with the given id(s). You can\n# either give multiple identifier separated by comma (,) or put this option\n# multiple time. See also the \"--disable\" option for examples.\n#enable=\n\n# Disable the message, report, category or checker with the given id(s). You\n# can either give multiple identifiers separated by comma (,) or put this\n# option multiple times (only on the command line, not in the configuration\n# file where it should appear only once).You can also use \"--disable=all\" to\n# disable everything first and then reenable specific checks. For example, if\n# you want to run only the similarities checker, you can use \"--disable=all\n# --enable=similarities\". If you want to run only the classes checker, but have\n# no Warning level messages displayed, use\"--disable=all --enable=classes\n# --disable=W\"\ndisable=E1608,W1627,E1601,E1603,E1602,E1605,E1604,E1607,E1606,W1621,W1620,W1623,W1622,W1625,W1624,W1609,W1608,W1607,W1606,W1605,W1604,W1603,W1602,W1601,W1639,W1640,I0021,W1638,I0020,W1618,W1619,W1630,W1626,W1637,W1634,W1635,W1610,W1611,W1612,W1613,W1614,W1615,W1616,W1617,W1632,W1633,W0704,W1628,W1629,W1636\n\n\n[REPORTS]\n\n# Set the output format. Available formats are text, parseable, colorized, msvs\n# (visual studio) and html. You can also give a reporter class, eg\n# mypackage.mymodule.MyReporterClass.\noutput-format=text\n\n# Put messages in a separate file for each module / package specified on the\n# command line instead of printing them on stdout. Reports (if any) will be\n# written in a file name \"pylint_global.[txt|html]\".\nfiles-output=no\n\n# Tells whether to display a full report or only the messages\nreports=yes\n\n# Python expression which should return a note less than 10 (10 is the highest\n# note). You have access to the variables errors warning, statement which\n# respectively contain the number of errors / warnings messages and the total\n# number of statements analyzed. This is used by the global evaluation report\n# (RP0004).\nevaluation=10.0 - ((float(5 * error + warning + refactor + convention) / statement) * 10)\n\n# Template used to display messages. This is a python new-style format string\n# used to format the message information. See doc for all details\n#msg-template=\n\n\n[BASIC]\n\n# List of builtins function names that should not be used, separated by a comma\nbad-functions=map,filter,input\n\n# Good variable names which should always be accepted, separated by a comma\ngood-names=i,j,k,ex,Run,_\n\n# Bad variable names which should always be refused, separated by a comma\nbad-names=foo,bar,baz,toto,tutu,tata\n\n# Colon-delimited sets of names that determine each other's naming style when\n# the name regexes allow several styles.\nname-group=\n\n# Include a hint for the correct naming format with invalid-name\ninclude-naming-hint=no\n\n# Regular expression matching correct function names\nfunction-rgx=[a-z_][a-z0-9_]{2,30}$\n\n# Naming hint for function names\nfunction-name-hint=[a-z_][a-z0-9_]{2,30}$\n\n# Regular expression matching correct variable names\nvariable-rgx=[a-z_][a-z0-9_]{2,30}$\n\n# Naming hint for variable names\nvariable-name-hint=[a-z_][a-z0-9_]{2,30}$\n\n# Regular expression matching correct constant names\nconst-rgx=(([A-Z_][A-Z0-9_]*)|(__.*__))$\n\n# Naming hint for constant names\nconst-name-hint=(([A-Z_][A-Z0-9_]*)|(__.*__))$\n\n# Regular expression matching correct attribute names\nattr-rgx=[a-z_][a-z0-9_]{2,30}$\n\n# Naming hint for attribute names\nattr-name-hint=[a-z_][a-z0-9_]{2,30}$\n\n# Regular expression matching correct argument names\nargument-rgx=[a-z_][a-z0-9_]{2,30}$\n\n# Naming hint for argument names\nargument-name-hint=[a-z_][a-z0-9_]{2,30}$\n\n# Regular expression matching correct class attribute names\nclass-attribute-rgx=([A-Za-z_][A-Za-z0-9_]{2,30}|(__.*__))$\n\n# Naming hint for class attribute names\nclass-attribute-name-hint=([A-Za-z_][A-Za-z0-9_]{2,30}|(__.*__))$\n\n# Regular expression matching correct inline iteration names\ninlinevar-rgx=[A-Za-z_][A-Za-z0-9_]*$\n\n# Naming hint for inline iteration names\ninlinevar-name-hint=[A-Za-z_][A-Za-z0-9_]*$\n\n# Regular expression matching correct class names\nclass-rgx=[A-Z_][a-zA-Z0-9]+$\n\n# Naming hint for class names\nclass-name-hint=[A-Z_][a-zA-Z0-9]+$\n\n# Regular expression matching correct module names\nmodule-rgx=(([a-z_][a-z0-9_]*)|([A-Z][a-zA-Z0-9]+))$\n\n# Naming hint for module names\nmodule-name-hint=(([a-z_][a-z0-9_]*)|([A-Z][a-zA-Z0-9]+))$\n\n# Regular expression matching correct method names\nmethod-rgx=[a-z_][a-z0-9_]{2,30}$\n\n# Naming hint for method names\nmethod-name-hint=[a-z_][a-z0-9_]{2,30}$\n\n# Regular expression which should only match function or class names that do\n# not require a docstring.\nno-docstring-rgx=__.*__\n\n# Minimum line length for functions/classes that require docstrings, shorter\n# ones are exempt.\ndocstring-min-length=-1\n\n\n[FORMAT]\n\n# Maximum number of characters on a single line.\nmax-line-length=100\n\n# Regexp for a line that is allowed to be longer than the limit.\nignore-long-lines=^\\s*(# )?<?https?://\\S+>?$\n\n# Allow the body of an if to be on the same line as the test if there is no\n# else.\nsingle-line-if-stmt=no\n\n# List of optional constructs for which whitespace checking is disabled\nno-space-check=trailing-comma,dict-separator\n\n# Maximum number of lines in a module\nmax-module-lines=1000\n\n# String used as indentation unit. This is usually \" \" (4 spaces) or \"\\t\" (1\n# tab).\nindent-string='    '\n\n# Number of spaces of indent required inside a hanging or continued line.\nindent-after-paren=4\n\n# Expected format of line ending, e.g. empty (any line ending), LF or CRLF.\nexpected-line-ending-format=\n\n\n[LOGGING]\n\n# Logging modules to check that the string format arguments are in logging\n# function parameter format\nlogging-modules=logging\n\n\n[MISCELLANEOUS]\n\n# List of note tags to take in consideration, separated by a comma.\nnotes=FIXME,XXX,TODO\n\n\n[SIMILARITIES]\n\n# Minimum lines number of a similarity.\nmin-similarity-lines=4\n\n# Ignore comments when computing similarities.\nignore-comments=yes\n\n# Ignore docstrings when computing similarities.\nignore-docstrings=yes\n\n# Ignore imports when computing similarities.\nignore-imports=no\n\n\n[SPELLING]\n\n# Spelling dictionary name. Available dictionaries: none. To make it working\n# install python-enchant package.\nspelling-dict=\n\n# List of comma separated words that should not be checked.\nspelling-ignore-words=\n\n# A path to a file that contains private dictionary; one word per line.\nspelling-private-dict-file=\n\n# Tells whether to store unknown words to indicated private dictionary in\n# --spelling-private-dict-file option instead of raising a message.\nspelling-store-unknown-words=no\n\n\n[TYPECHECK]\n\n# Tells whether missing members accessed in mixin class should be ignored. A\n# mixin class is detected if its name ends with \"mixin\" (case insensitive).\nignore-mixin-members=yes\n\n# List of module names for which member attributes should not be checked\n# (useful for modules/projects where namespaces are manipulated during runtime\n# and thus existing member attributes cannot be deduced by static analysis\nignored-modules=\n\n# List of classes names for which member attributes should not be checked\n# (useful for classes with attributes dynamically set).\nignored-classes=SQLObject\n\n# List of members which are set dynamically and missed by pylint inference\n# system, and so shouldn't trigger E0201 when accessed. Python regular\n# expressions are accepted.\ngenerated-members=REQUEST,acl_users,aq_parent\n\n\n[VARIABLES]\n\n# Tells whether we should check for unused import in __init__ files.\ninit-import=no\n\n# A regular expression matching the name of dummy variables (i.e. expectedly\n# not used).\ndummy-variables-rgx=_$|dummy\n\n# List of additional names supposed to be defined in builtins. Remember that\n# you should avoid to define new builtins when possible.\nadditional-builtins=\n\n# List of strings which can identify a callback function by name. A callback\n# name must start or end with one of those strings.\ncallbacks=cb_,_cb\n\n\n[CLASSES]\n\n# List of method names used to declare (i.e. assign) instance attributes.\ndefining-attr-methods=__init__,__new__,setUp\n\n# List of valid names for the first argument in a class method.\nvalid-classmethod-first-arg=cls\n\n# List of valid names for the first argument in a metaclass class method.\nvalid-metaclass-classmethod-first-arg=mcs\n\n# List of member names, which should be excluded from the protected access\n# warning.\nexclude-protected=_asdict,_fields,_replace,_source,_make\n\n\n[DESIGN]\n\n# Maximum number of arguments for function / method\nmax-args=5\n\n# Argument names that match this expression will be ignored. Default to name\n# with leading underscore\nignored-argument-names=_.*\n\n# Maximum number of locals for function / method body\nmax-locals=15\n\n# Maximum number of return / yield for function / method body\nmax-returns=6\n\n# Maximum number of branch for function / method body\nmax-branches=12\n\n# Maximum number of statements in function / method body\nmax-statements=50\n\n# Maximum number of parents for a class (see R0901).\nmax-parents=7\n\n# Maximum number of attributes for a class (see R0902).\nmax-attributes=7\n\n# Minimum number of public methods for a class (see R0903).\nmin-public-methods=2\n\n# Maximum number of public methods for a class (see R0904).\nmax-public-methods=20\n\n\n[IMPORTS]\n\n# Deprecated modules which should not be used, separated by a comma\ndeprecated-modules=regsub,TERMIOS,Bastion,rexec\n\n# Create a graph of every (i.e. internal and external) dependencies in the\n# given file (report RP0402 must not be disabled)\nimport-graph=\n\n# Create a graph of external dependencies in the given file (report RP0402 must\n# not be disabled)\next-import-graph=\n\n# Create a graph of internal dependencies in the given file (report RP0402 must\n# not be disabled)\nint-import-graph=\n\n\n[EXCEPTIONS]\n\n# Exceptions that will emit a warning when being caught. Defaults to\n# \"Exception\"\novergeneral-exceptions=Exception\n"
        },
        {
          "name": "pytest.ini",
          "type": "blob",
          "size": 0.0771484375,
          "content": "[pytest]\nmarkers =\n\tslow:  slower running tests\n\tgpu: test which require a gpu\n"
        },
        {
          "name": "requirements.txt",
          "type": "blob",
          "size": 0.30859375,
          "content": "configargparse==0.10.0\nnumpy==1.13.1\npyyaml==4.2b1\npep8==1.7.0\nflake8==3.0.4\npytest==3.0.1\npytest-cov==2.3.1\npytest-mock==1.6.0\nposix_ipc==1.0.0\npillow==3.3.2\npylint==1.6.4\nsphinx==1.6.5\nh5py==2.6.0\nappdirs==1.4.3\nfuture==0.15.2\ntqdm==4.8.4\ncffi==1.11.0\nfilelock==2.0.8\npy-cpuinfo==3.2.0\npypandoc==1.4\npandoc==1.0.2\n"
        },
        {
          "name": "setup.cfg",
          "type": "blob",
          "size": 1.2509765625,
          "content": "# ******************************************************************************\n# Copyright 2017-2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ******************************************************************************\n\n[flake8]\nexclude = .git,__init__.py,neon/backends/test_pool.py,neon/backends/kernel_specs.py,neon/backends/convnet-benchmarks.py,neon/backends/layer_gpu.py,neon/backends/nervanagpu.py,neon/backends/float_ew.py,neon/backends/make_kernels.py,neon/backends/kernels/cuda/pooling.py,neon/backends/cuda_batchnorm.py,neon/backends/winograd4.py,neon/backends/winograd.py,neon/backends/winograd_conv.py,neon/backends/winograd4.py,neon/backends/winograd5.py,neon/backends/convolution.py,neon/backends/conv_kernel_test.py\nmax-line-length = 99\n"
        },
        {
          "name": "setup.py",
          "type": "blob",
          "size": 4.7744140625,
          "content": "#!/usr/bin/env python\n# ******************************************************************************\n# Copyright 2017-2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ******************************************************************************\nimport os\nfrom setuptools import setup, find_packages\nimport subprocess\n\n# Define version information\nVERSION = '2.6.0'\nFULLVERSION = VERSION\nwrite_version = True\n\ntry:\n    pipe = subprocess.Popen([\"git\", \"rev-parse\", \"--short\", \"HEAD\"],\n                            stdout=subprocess.PIPE)\n    (so, serr) = pipe.communicate()\n    if pipe.returncode == 0:\n        FULLVERSION += \"+%s\" % so.strip().decode(\"utf-8\")\nexcept Exception:\n    pass\n\n\ntry:\n    import pypandoc\n    readme_file = pypandoc.convert('README.md', 'rst') \nexcept:\n    readme_file = open('README.md').read()\n\n\nif write_version:\n    txt = \"# \" + (\"-\" * 77) + \"\\n\"\n    txt += \"# Copyright 2017-2018 Intel Corporation\\n\"\n    txt += \"#\\n\"\n    txt += \"# Licensed under the Apache License, Version 2.0 \"\n    txt += \"(the \\\"License\\\");\\n\"\n    txt += \"# you may not use this file except in compliance with the \"\n    txt += \"License.\\n\"\n    txt += \"# You may obtain a copy of the License at\\n\"\n    txt += \"#\\n\"\n    txt += \"#      http://www.apache.org/licenses/LICENSE-2.0\\n\"\n    txt += \"#\\n\"\n    txt += \"# Unless required by applicable law or agreed to in writing, \"\n    txt += \"software\\n\"\n    txt += \"# distributed under the License is distributed on an \\\"AS IS\\\" \"\n    txt += \"BASIS,\\n\"\n    txt += \"# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or \"\n    txt += \"implied.\\n\"\n    txt += \"# See the License for the specific language governing permissions \"\n    txt += \"and\\n\"\n    txt += \"# limitations under the License.\\n\"\n    txt += \"# \" + (\"-\" * 77) + \"\\n\"\n    txt += \"\\\"\\\"\\\"\\n%s\\n\\\"\\\"\\\"\\nVERSION = '%s'\\nSHORT_VERSION = '%s'\\n\"\n    fname = os.path.join(os.path.dirname(__file__), 'neon', 'version.py')\n    a = open(fname, 'w')\n    try:\n        a.write(txt % (\"Project version information.\", FULLVERSION, VERSION))\n    finally:\n        a.close()\n\nrequirements = [\n    \"configargparse\",\n    \"numpy\",\n    \"pyyaml\",\n    \"pep8\",\n    \"flake8\",\n    \"pytest\",\n    \"pytest-cov\",\n    \"pytest-mock\",\n    \"posix_ipc\",\n    \"pillow\",\n    \"pylint\",\n    \"sphinx\",\n    \"h5py\",\n    \"appdirs\",\n    \"future\",\n    \"tqdm\",\n    \"cffi\",\n    \"filelock\",\n    \"py-cpuinfo\",\n    \"pypandoc\",\n    \"pandoc\"\n]\n\n\nsetup(name='nervananeon',\n      version=VERSION,\n      description=\"Intel's deep learning framework\",\n      long_description=readme_file,\n      author='Intel Deep Learning System',\n      author_email='intelnervana@intel.com',\n      url='http://www.intelnervana.com',\n      license='License :: OSI Approved :: Apache Software License',\n      scripts=['bin/neon', 'bin/nvis'],\n      packages=find_packages(),\n      install_requires=requirements,\n      package_data={'neon': ['backends/kernels/sass/*.sass',\n                             'backends/kernels/cubin/*.cubin',\n                             'backends/kernels/maxas/*.pl',\n                             'backends/kernels/maxas/MaxAs/*.pm',\n                             'backends/mklEngine/*.so',\n                             'backends/mklEngine/*.dll',\n                             'backends/mklEngine/*.dylib',\n                             'backends/mklEngine/src/*.header',\n                             '../mklml_*/lib/*.so',\n                             '../loader/bin/*.so']},\n      classifiers=['Development Status :: 3 - Alpha',\n                   'Environment :: Console',\n                   'Environment :: Console :: Curses',\n                   'Environment :: Web Environment',\n                   'Intended Audience :: End Users/Desktop',\n                   'Intended Audience :: Developers',\n                   'Intended Audience :: Science/Research',\n                   'License :: OSI Approved :: Apache Software License',\n                   'Operating System :: POSIX',\n                   'Operating System :: MacOS :: MacOS X',\n                   'Programming Language :: Python :: 2',\n                   'Programming Language :: Python :: 3',\n                   'Topic :: Scientific/Engineering :: ' +\n                   'Artificial Intelligence',\n                   'Topic :: Scientific/Engineering :: Information Analysis',\n                   'Topic :: System :: Distributed Computing'])\n"
        },
        {
          "name": "tests",
          "type": "tree",
          "content": null
        },
        {
          "name": "tox.ini",
          "type": "blob",
          "size": 0.08203125,
          "content": "[tox]\nenvlist = py27, py34\n[testenv]\ndeps = -rtox_requirements.txt\ncommands=py.test\n"
        },
        {
          "name": "tox_requirements.txt",
          "type": "blob",
          "size": 0.24609375,
          "content": "pip\ncython==0.23.1\nconfigargparse==0.9.3\nnumpy==1.9.2\npyyaml==5.1\npep8==1.7.0\nflake8==2.4.1\npytest==2.7.3\npytest-cov==2.0.0\nposix_ipc==1.0.0\npillow==2.9.0\npylint==1.4.4\nsphinx==1.3.1\nh5py==2.5.0\nfuture==0.15.2\npycuda>=2015.1\nscikit-cuda==0.5.1\npytools\n"
        },
        {
          "name": "vis_requirements.txt",
          "type": "blob",
          "size": 0.0263671875,
          "content": "bokeh==0.9.3\njinja2==2.8.1\n"
        }
      ]
    }
  ]
}