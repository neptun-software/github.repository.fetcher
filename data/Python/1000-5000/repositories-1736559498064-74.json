{
  "metadata": {
    "timestamp": 1736559498064,
    "page": 74,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjgw",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "victoresque/pytorch-template",
      "stars": 4798,
      "defaultBranch": "master",
      "files": [
        {
          "name": ".flake8",
          "type": "blob",
          "size": 0.0859375,
          "content": "[flake8]\nignore = F401, F403\nmax-line-length = 120\nexclude =\n    .git,\n    __pycache__,\n"
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 1.2509765625,
          "content": "# Byte-compiled / optimized / DLL files\n__pycache__/\n*.py[cod]\n*$py.class\n\n# C extensions\n*.so\n\n# Distribution / packaging\n.Python\nenv/\nbuild/\ndevelop-eggs/\ndist/\ndownloads/\neggs/\n.eggs/\nlib/\nlib64/\nparts/\nsdist/\nvar/\nwheels/\n*.egg-info/\n.installed.cfg\n*.egg\n\n# PyInstaller\n#  Usually these files are written by a python script from a template\n#  before PyInstaller builds the exe, so as to inject date/other infos into it.\n*.manifest\n*.spec\n\n# Installer logs\npip-log.txt\npip-delete-this-directory.txt\n\n# Unit test / coverage reports\nhtmlcov/\n.tox/\n.coverage\n.coverage.*\n.cache\nnosetests.xml\ncoverage.xml\n*.cover\n.hypothesis/\n\n# Translations\n*.mo\n*.pot\n\n# Django stuff:\n*.log\nlocal_settings.py\n\n# Flask stuff:\ninstance/\n.webassets-cache\n\n# Scrapy stuff:\n.scrapy\n\n# Sphinx documentation\ndocs/_build/\n\n# PyBuilder\ntarget/\n\n# Jupyter Notebook\n.ipynb_checkpoints\n\n# pyenv\n.python-version\n\n# celery beat schedule file\ncelerybeat-schedule\n\n# SageMath parsed files\n*.sage.py\n\n# dotenv\n.env\n\n# virtualenv\n.venv\nvenv/\nENV/\n\n# Spyder project settings\n.spyderproject\n.spyproject\n\n# Rope project settings\n.ropeproject\n\n# mkdocs documentation\n/site\n\n# mypy\n.mypy_cache/\n\n# input data, saved log, checkpoints\ndata/\ninput/\nsaved/\ndatasets/\n\n# editor, os cache directory\n.vscode/\n.idea/\n__MACOSX/\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 1.0439453125,
          "content": "MIT License\n\nCopyright (c) 2018 Victor Huang\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 13.4326171875,
          "content": "# PyTorch Template Project\nPyTorch deep learning project made easy.\n\n<!-- @import \"[TOC]\" {cmd=\"toc\" depthFrom=1 depthTo=6 orderedList=false} -->\n\n<!-- code_chunk_output -->\n\n* [PyTorch Template Project](#pytorch-template-project)\n\t* [Requirements](#requirements)\n\t* [Features](#features)\n\t* [Folder Structure](#folder-structure)\n\t* [Usage](#usage)\n\t\t* [Config file format](#config-file-format)\n\t\t* [Using config files](#using-config-files)\n\t\t* [Resuming from checkpoints](#resuming-from-checkpoints)\n    * [Using Multiple GPU](#using-multiple-gpu)\n\t* [Customization](#customization)\n\t\t* [Custom CLI options](#custom-cli-options)\n\t\t* [Data Loader](#data-loader)\n\t\t* [Trainer](#trainer)\n\t\t* [Model](#model)\n\t\t* [Loss](#loss)\n\t\t* [metrics](#metrics)\n\t\t* [Additional logging](#additional-logging)\n\t\t* [Validation data](#validation-data)\n\t\t* [Checkpoints](#checkpoints)\n    * [Tensorboard Visualization](#tensorboard-visualization)\n\t* [Contribution](#contribution)\n\t* [TODOs](#todos)\n\t* [License](#license)\n\t* [Acknowledgements](#acknowledgements)\n\n<!-- /code_chunk_output -->\n\n## Requirements\n* Python >= 3.5 (3.6 recommended)\n* PyTorch >= 0.4 (1.2 recommended)\n* tqdm (Optional for `test.py`)\n* tensorboard >= 1.14 (see [Tensorboard Visualization](#tensorboard-visualization))\n\n## Features\n* Clear folder structure which is suitable for many deep learning projects.\n* `.json` config file support for convenient parameter tuning.\n* Customizable command line options for more convenient parameter tuning.\n* Checkpoint saving and resuming.\n* Abstract base classes for faster development:\n  * `BaseTrainer` handles checkpoint saving/resuming, training process logging, and more.\n  * `BaseDataLoader` handles batch generation, data shuffling, and validation data splitting.\n  * `BaseModel` provides basic model summary.\n\n## Folder Structure\n  ```\n  pytorch-template/\n  │\n  ├── train.py - main script to start training\n  ├── test.py - evaluation of trained model\n  │\n  ├── config.json - holds configuration for training\n  ├── parse_config.py - class to handle config file and cli options\n  │\n  ├── new_project.py - initialize new project with template files\n  │\n  ├── base/ - abstract base classes\n  │   ├── base_data_loader.py\n  │   ├── base_model.py\n  │   └── base_trainer.py\n  │\n  ├── data_loader/ - anything about data loading goes here\n  │   └── data_loaders.py\n  │\n  ├── data/ - default directory for storing input data\n  │\n  ├── model/ - models, losses, and metrics\n  │   ├── model.py\n  │   ├── metric.py\n  │   └── loss.py\n  │\n  ├── saved/\n  │   ├── models/ - trained models are saved here\n  │   └── log/ - default logdir for tensorboard and logging output\n  │\n  ├── trainer/ - trainers\n  │   └── trainer.py\n  │\n  ├── logger/ - module for tensorboard visualization and logging\n  │   ├── visualization.py\n  │   ├── logger.py\n  │   └── logger_config.json\n  │  \n  └── utils/ - small utility functions\n      ├── util.py\n      └── ...\n  ```\n\n## Usage\nThe code in this repo is an MNIST example of the template.\nTry `python train.py -c config.json` to run code.\n\n### Config file format\nConfig files are in `.json` format:\n```javascript\n{\n  \"name\": \"Mnist_LeNet\",        // training session name\n  \"n_gpu\": 1,                   // number of GPUs to use for training.\n  \n  \"arch\": {\n    \"type\": \"MnistModel\",       // name of model architecture to train\n    \"args\": {\n\n    }                \n  },\n  \"data_loader\": {\n    \"type\": \"MnistDataLoader\",         // selecting data loader\n    \"args\":{\n      \"data_dir\": \"data/\",             // dataset path\n      \"batch_size\": 64,                // batch size\n      \"shuffle\": true,                 // shuffle training data before splitting\n      \"validation_split\": 0.1          // size of validation dataset. float(portion) or int(number of samples)\n      \"num_workers\": 2,                // number of cpu processes to be used for data loading\n    }\n  },\n  \"optimizer\": {\n    \"type\": \"Adam\",\n    \"args\":{\n      \"lr\": 0.001,                     // learning rate\n      \"weight_decay\": 0,               // (optional) weight decay\n      \"amsgrad\": true\n    }\n  },\n  \"loss\": \"nll_loss\",                  // loss\n  \"metrics\": [\n    \"accuracy\", \"top_k_acc\"            // list of metrics to evaluate\n  ],                         \n  \"lr_scheduler\": {\n    \"type\": \"StepLR\",                  // learning rate scheduler\n    \"args\":{\n      \"step_size\": 50,          \n      \"gamma\": 0.1\n    }\n  },\n  \"trainer\": {\n    \"epochs\": 100,                     // number of training epochs\n    \"save_dir\": \"saved/\",              // checkpoints are saved in save_dir/models/name\n    \"save_freq\": 1,                    // save checkpoints every save_freq epochs\n    \"verbosity\": 2,                    // 0: quiet, 1: per epoch, 2: full\n  \n    \"monitor\": \"min val_loss\"          // mode and metric for model performance monitoring. set 'off' to disable.\n    \"early_stop\": 10\t                 // number of epochs to wait before early stop. set 0 to disable.\n  \n    \"tensorboard\": true,               // enable tensorboard visualization\n  }\n}\n```\n\nAdd addional configurations if you need.\n\n### Using config files\nModify the configurations in `.json` config files, then run:\n\n  ```\n  python train.py --config config.json\n  ```\n\n### Resuming from checkpoints\nYou can resume from a previously saved checkpoint by:\n\n  ```\n  python train.py --resume path/to/checkpoint\n  ```\n\n### Using Multiple GPU\nYou can enable multi-GPU training by setting `n_gpu` argument of the config file to larger number.\nIf configured to use smaller number of gpu than available, first n devices will be used by default.\nSpecify indices of available GPUs by cuda environmental variable.\n  ```\n  python train.py --device 2,3 -c config.json\n  ```\n  This is equivalent to\n  ```\n  CUDA_VISIBLE_DEVICES=2,3 python train.py -c config.py\n  ```\n\n## Customization\n\n### Project initialization\nUse the `new_project.py` script to make your new project directory with template files.\n`python new_project.py ../NewProject` then a new project folder named 'NewProject' will be made.\nThis script will filter out unneccessary files like cache, git files or readme file. \n\n### Custom CLI options\n\nChanging values of config file is a clean, safe and easy way of tuning hyperparameters. However, sometimes\nit is better to have command line options if some values need to be changed too often or quickly.\n\nThis template uses the configurations stored in the json file by default, but by registering custom options as follows\nyou can change some of them using CLI flags.\n\n  ```python\n  # simple class-like object having 3 attributes, `flags`, `type`, `target`.\n  CustomArgs = collections.namedtuple('CustomArgs', 'flags type target')\n  options = [\n      CustomArgs(['--lr', '--learning_rate'], type=float, target=('optimizer', 'args', 'lr')),\n      CustomArgs(['--bs', '--batch_size'], type=int, target=('data_loader', 'args', 'batch_size'))\n      # options added here can be modified by command line flags.\n  ]\n  ```\n`target` argument should be sequence of keys, which are used to access that option in the config dict. In this example, `target` \nfor the learning rate option is `('optimizer', 'args', 'lr')` because `config['optimizer']['args']['lr']` points to the learning rate.\n`python train.py -c config.json --bs 256` runs training with options given in `config.json` except for the `batch size`\nwhich is increased to 256 by command line options.\n\n\n### Data Loader\n* **Writing your own data loader**\n\n1. **Inherit ```BaseDataLoader```**\n\n    `BaseDataLoader` is a subclass of `torch.utils.data.DataLoader`, you can use either of them.\n\n    `BaseDataLoader` handles:\n    * Generating next batch\n    * Data shuffling\n    * Generating validation data loader by calling\n    `BaseDataLoader.split_validation()`\n\n* **DataLoader Usage**\n\n  `BaseDataLoader` is an iterator, to iterate through batches:\n  ```python\n  for batch_idx, (x_batch, y_batch) in data_loader:\n      pass\n  ```\n* **Example**\n\n  Please refer to `data_loader/data_loaders.py` for an MNIST data loading example.\n\n### Trainer\n* **Writing your own trainer**\n\n1. **Inherit ```BaseTrainer```**\n\n    `BaseTrainer` handles:\n    * Training process logging\n    * Checkpoint saving\n    * Checkpoint resuming\n    * Reconfigurable performance monitoring for saving current best model, and early stop training.\n      * If config `monitor` is set to `max val_accuracy`, which means then the trainer will save a checkpoint `model_best.pth` when `validation accuracy` of epoch replaces current `maximum`.\n      * If config `early_stop` is set, training will be automatically terminated when model performance does not improve for given number of epochs. This feature can be turned off by passing 0 to the `early_stop` option, or just deleting the line of config.\n\n2. **Implementing abstract methods**\n\n    You need to implement `_train_epoch()` for your training process, if you need validation then you can implement `_valid_epoch()` as in `trainer/trainer.py`\n\n* **Example**\n\n  Please refer to `trainer/trainer.py` for MNIST training.\n\n* **Iteration-based training**\n\n  `Trainer.__init__` takes an optional argument, `len_epoch` which controls number of batches(steps) in each epoch.\n\n### Model\n* **Writing your own model**\n\n1. **Inherit `BaseModel`**\n\n    `BaseModel` handles:\n    * Inherited from `torch.nn.Module`\n    * `__str__`: Modify native `print` function to prints the number of trainable parameters.\n\n2. **Implementing abstract methods**\n\n    Implement the foward pass method `forward()`\n\n* **Example**\n\n  Please refer to `model/model.py` for a LeNet example.\n\n### Loss\nCustom loss functions can be implemented in 'model/loss.py'. Use them by changing the name given in \"loss\" in config file, to corresponding name.\n\n### Metrics\nMetric functions are located in 'model/metric.py'.\n\nYou can monitor multiple metrics by providing a list in the configuration file, e.g.:\n  ```json\n  \"metrics\": [\"accuracy\", \"top_k_acc\"],\n  ```\n\n### Additional logging\nIf you have additional information to be logged, in `_train_epoch()` of your trainer class, merge them with `log` as shown below before returning:\n\n  ```python\n  additional_log = {\"gradient_norm\": g, \"sensitivity\": s}\n  log.update(additional_log)\n  return log\n  ```\n\n### Testing\nYou can test trained model by running `test.py` passing path to the trained checkpoint by `--resume` argument.\n\n### Validation data\nTo split validation data from a data loader, call `BaseDataLoader.split_validation()`, then it will return a data loader for validation of size specified in your config file.\nThe `validation_split` can be a ratio of validation set per total data(0.0 <= float < 1.0), or the number of samples (0 <= int < `n_total_samples`).\n\n**Note**: the `split_validation()` method will modify the original data loader\n**Note**: `split_validation()` will return `None` if `\"validation_split\"` is set to `0`\n\n### Checkpoints\nYou can specify the name of the training session in config files:\n  ```json\n  \"name\": \"MNIST_LeNet\",\n  ```\n\nThe checkpoints will be saved in `save_dir/name/timestamp/checkpoint_epoch_n`, with timestamp in mmdd_HHMMSS format.\n\nA copy of config file will be saved in the same folder.\n\n**Note**: checkpoints contain:\n  ```python\n  {\n    'arch': arch,\n    'epoch': epoch,\n    'state_dict': self.model.state_dict(),\n    'optimizer': self.optimizer.state_dict(),\n    'monitor_best': self.mnt_best,\n    'config': self.config\n  }\n  ```\n\n### Tensorboard Visualization\nThis template supports Tensorboard visualization by using either  `torch.utils.tensorboard` or [TensorboardX](https://github.com/lanpa/tensorboardX).\n\n1. **Install**\n\n    If you are using pytorch 1.1 or higher, install tensorboard by 'pip install tensorboard>=1.14.0'.\n\n    Otherwise, you should install tensorboardx. Follow installation guide in [TensorboardX](https://github.com/lanpa/tensorboardX).\n\n2. **Run training** \n\n    Make sure that `tensorboard` option in the config file is turned on.\n\n    ```\n     \"tensorboard\" : true\n    ```\n\n3. **Open Tensorboard server** \n\n    Type `tensorboard --logdir saved/log/` at the project root, then server will open at `http://localhost:6006`\n\nBy default, values of loss and metrics specified in config file, input images, and histogram of model parameters will be logged.\nIf you need more visualizations, use `add_scalar('tag', data)`, `add_image('tag', image)`, etc in the `trainer._train_epoch` method.\n`add_something()` methods in this template are basically wrappers for those of `tensorboardX.SummaryWriter` and `torch.utils.tensorboard.SummaryWriter` modules. \n\n**Note**: You don't have to specify current steps, since `WriterTensorboard` class defined at `logger/visualization.py` will track current steps.\n\n## Contribution\nFeel free to contribute any kind of function or enhancement, here the coding style follows PEP8\n\nCode should pass the [Flake8](http://flake8.pycqa.org/en/latest/) check before committing.\n\n## TODOs\n\n- [ ] Multiple optimizers\n- [ ] Support more tensorboard functions\n- [x] Using fixed random seed\n- [x] Support pytorch native tensorboard\n- [x] `tensorboardX` logger support\n- [x] Configurable logging layout, checkpoint naming\n- [x] Iteration-based training (instead of epoch-based)\n- [x] Adding command line option for fine-tuning\n\n## License\nThis project is licensed under the MIT License. See  LICENSE for more details\n\n## Acknowledgements\nThis project is inspired by the project [Tensorflow-Project-Template](https://github.com/MrGemy95/Tensorflow-Project-Template) by [Mahmoud Gemy](https://github.com/MrGemy95)\n"
        },
        {
          "name": "base",
          "type": "tree",
          "content": null
        },
        {
          "name": "config.json",
          "type": "blob",
          "size": 0.9482421875,
          "content": "{\n    \"name\": \"Mnist_LeNet\",\n    \"n_gpu\": 1,\n\n    \"arch\": {\n        \"type\": \"MnistModel\",\n        \"args\": {}\n    },\n    \"data_loader\": {\n        \"type\": \"MnistDataLoader\",\n        \"args\":{\n            \"data_dir\": \"data/\",\n            \"batch_size\": 128,\n            \"shuffle\": true,\n            \"validation_split\": 0.1,\n            \"num_workers\": 2\n        }\n    },\n    \"optimizer\": {\n        \"type\": \"Adam\",\n        \"args\":{\n            \"lr\": 0.001,\n            \"weight_decay\": 0,\n            \"amsgrad\": true\n        }\n    },\n    \"loss\": \"nll_loss\",\n    \"metrics\": [\n        \"accuracy\", \"top_k_acc\"\n    ],\n    \"lr_scheduler\": {\n        \"type\": \"StepLR\",\n        \"args\": {\n            \"step_size\": 50,\n            \"gamma\": 0.1\n        }\n    },\n    \"trainer\": {\n        \"epochs\": 100,\n\n        \"save_dir\": \"saved/\",\n        \"save_period\": 1,\n        \"verbosity\": 2,\n        \n        \"monitor\": \"min val_loss\",\n        \"early_stop\": 10,\n\n        \"tensorboard\": true\n    }\n}\n"
        },
        {
          "name": "data_loader",
          "type": "tree",
          "content": null
        },
        {
          "name": "logger",
          "type": "tree",
          "content": null
        },
        {
          "name": "model",
          "type": "tree",
          "content": null
        },
        {
          "name": "new_project.py",
          "type": "blob",
          "size": 0.791015625,
          "content": "import sys\nfrom pathlib import Path\nfrom shutil import copytree, ignore_patterns\n\n\n# This script initializes new pytorch project with the template files.\n# Run `python3 new_project.py ../MyNewProject` then new project named \n# MyNewProject will be made\ncurrent_dir = Path()\nassert (current_dir / 'new_project.py').is_file(), 'Script should be executed in the pytorch-template directory'\nassert len(sys.argv) == 2, 'Specify a name for the new project. Example: python3 new_project.py MyNewProject'\n\nproject_name = Path(sys.argv[1])\ntarget_dir = current_dir / project_name\n\nignore = [\".git\", \"data\", \"saved\", \"new_project.py\", \"LICENSE\", \".flake8\", \"README.md\", \"__pycache__\"]\ncopytree(current_dir, target_dir, ignore=ignore_patterns(*ignore))\nprint('New project initialized at', target_dir.absolute().resolve())"
        },
        {
          "name": "parse_config.py",
          "type": "blob",
          "size": 5.9228515625,
          "content": "import os\nimport logging\nfrom pathlib import Path\nfrom functools import reduce, partial\nfrom operator import getitem\nfrom datetime import datetime\nfrom logger import setup_logging\nfrom utils import read_json, write_json\n\n\nclass ConfigParser:\n    def __init__(self, config, resume=None, modification=None, run_id=None):\n        \"\"\"\n        class to parse configuration json file. Handles hyperparameters for training, initializations of modules, checkpoint saving\n        and logging module.\n        :param config: Dict containing configurations, hyperparameters for training. contents of `config.json` file for example.\n        :param resume: String, path to the checkpoint being loaded.\n        :param modification: Dict keychain:value, specifying position values to be replaced from config dict.\n        :param run_id: Unique Identifier for training processes. Used to save checkpoints and training log. Timestamp is being used as default\n        \"\"\"\n        # load config file and apply modification\n        self._config = _update_config(config, modification)\n        self.resume = resume\n\n        # set save_dir where trained model and log will be saved.\n        save_dir = Path(self.config['trainer']['save_dir'])\n\n        exper_name = self.config['name']\n        if run_id is None: # use timestamp as default run-id\n            run_id = datetime.now().strftime(r'%m%d_%H%M%S')\n        self._save_dir = save_dir / 'models' / exper_name / run_id\n        self._log_dir = save_dir / 'log' / exper_name / run_id\n\n        # make directory for saving checkpoints and log.\n        exist_ok = run_id == ''\n        self.save_dir.mkdir(parents=True, exist_ok=exist_ok)\n        self.log_dir.mkdir(parents=True, exist_ok=exist_ok)\n\n        # save updated config file to the checkpoint dir\n        write_json(self.config, self.save_dir / 'config.json')\n\n        # configure logging module\n        setup_logging(self.log_dir)\n        self.log_levels = {\n            0: logging.WARNING,\n            1: logging.INFO,\n            2: logging.DEBUG\n        }\n\n    @classmethod\n    def from_args(cls, args, options=''):\n        \"\"\"\n        Initialize this class from some cli arguments. Used in train, test.\n        \"\"\"\n        for opt in options:\n            args.add_argument(*opt.flags, default=None, type=opt.type)\n        if not isinstance(args, tuple):\n            args = args.parse_args()\n\n        if args.device is not None:\n            os.environ[\"CUDA_VISIBLE_DEVICES\"] = args.device\n        if args.resume is not None:\n            resume = Path(args.resume)\n            cfg_fname = resume.parent / 'config.json'\n        else:\n            msg_no_cfg = \"Configuration file need to be specified. Add '-c config.json', for example.\"\n            assert args.config is not None, msg_no_cfg\n            resume = None\n            cfg_fname = Path(args.config)\n        \n        config = read_json(cfg_fname)\n        if args.config and resume:\n            # update new config for fine-tuning\n            config.update(read_json(args.config))\n\n        # parse custom cli options into dictionary\n        modification = {opt.target : getattr(args, _get_opt_name(opt.flags)) for opt in options}\n        return cls(config, resume, modification)\n\n    def init_obj(self, name, module, *args, **kwargs):\n        \"\"\"\n        Finds a function handle with the name given as 'type' in config, and returns the\n        instance initialized with corresponding arguments given.\n\n        `object = config.init_obj('name', module, a, b=1)`\n        is equivalent to\n        `object = module.name(a, b=1)`\n        \"\"\"\n        module_name = self[name]['type']\n        module_args = dict(self[name]['args'])\n        assert all([k not in module_args for k in kwargs]), 'Overwriting kwargs given in config file is not allowed'\n        module_args.update(kwargs)\n        return getattr(module, module_name)(*args, **module_args)\n\n    def init_ftn(self, name, module, *args, **kwargs):\n        \"\"\"\n        Finds a function handle with the name given as 'type' in config, and returns the\n        function with given arguments fixed with functools.partial.\n\n        `function = config.init_ftn('name', module, a, b=1)`\n        is equivalent to\n        `function = lambda *args, **kwargs: module.name(a, *args, b=1, **kwargs)`.\n        \"\"\"\n        module_name = self[name]['type']\n        module_args = dict(self[name]['args'])\n        assert all([k not in module_args for k in kwargs]), 'Overwriting kwargs given in config file is not allowed'\n        module_args.update(kwargs)\n        return partial(getattr(module, module_name), *args, **module_args)\n\n    def __getitem__(self, name):\n        \"\"\"Access items like ordinary dict.\"\"\"\n        return self.config[name]\n\n    def get_logger(self, name, verbosity=2):\n        msg_verbosity = 'verbosity option {} is invalid. Valid options are {}.'.format(verbosity, self.log_levels.keys())\n        assert verbosity in self.log_levels, msg_verbosity\n        logger = logging.getLogger(name)\n        logger.setLevel(self.log_levels[verbosity])\n        return logger\n\n    # setting read-only attributes\n    @property\n    def config(self):\n        return self._config\n\n    @property\n    def save_dir(self):\n        return self._save_dir\n\n    @property\n    def log_dir(self):\n        return self._log_dir\n\n# helper functions to update config dict with custom cli options\ndef _update_config(config, modification):\n    if modification is None:\n        return config\n\n    for k, v in modification.items():\n        if v is not None:\n            _set_by_path(config, k, v)\n    return config\n\ndef _get_opt_name(flags):\n    for flg in flags:\n        if flg.startswith('--'):\n            return flg.replace('--', '')\n    return flags[0].replace('--', '')\n\ndef _set_by_path(tree, keys, value):\n    \"\"\"Set a value in a nested object in tree by sequence of keys.\"\"\"\n    keys = keys.split(';')\n    _get_by_path(tree, keys[:-1])[keys[-1]] = value\n\ndef _get_by_path(tree, keys):\n    \"\"\"Access a nested object in tree by sequence of keys.\"\"\"\n    return reduce(getitem, keys, tree)\n"
        },
        {
          "name": "requirements.txt",
          "type": "blob",
          "size": 0.05078125,
          "content": "torch>=1.1\ntorchvision\nnumpy\ntqdm\ntensorboard>=1.14\n"
        },
        {
          "name": "test.py",
          "type": "blob",
          "size": 2.65625,
          "content": "import argparse\nimport torch\nfrom tqdm import tqdm\nimport data_loader.data_loaders as module_data\nimport model.loss as module_loss\nimport model.metric as module_metric\nimport model.model as module_arch\nfrom parse_config import ConfigParser\n\n\ndef main(config):\n    logger = config.get_logger('test')\n\n    # setup data_loader instances\n    data_loader = getattr(module_data, config['data_loader']['type'])(\n        config['data_loader']['args']['data_dir'],\n        batch_size=512,\n        shuffle=False,\n        validation_split=0.0,\n        training=False,\n        num_workers=2\n    )\n\n    # build model architecture\n    model = config.init_obj('arch', module_arch)\n    logger.info(model)\n\n    # get function handles of loss and metrics\n    loss_fn = getattr(module_loss, config['loss'])\n    metric_fns = [getattr(module_metric, met) for met in config['metrics']]\n\n    logger.info('Loading checkpoint: {} ...'.format(config.resume))\n    checkpoint = torch.load(config.resume)\n    state_dict = checkpoint['state_dict']\n    if config['n_gpu'] > 1:\n        model = torch.nn.DataParallel(model)\n    model.load_state_dict(state_dict)\n\n    # prepare model for testing\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    model = model.to(device)\n    model.eval()\n\n    total_loss = 0.0\n    total_metrics = torch.zeros(len(metric_fns))\n\n    with torch.no_grad():\n        for i, (data, target) in enumerate(tqdm(data_loader)):\n            data, target = data.to(device), target.to(device)\n            output = model(data)\n\n            #\n            # save sample images, or do something with output here\n            #\n\n            # computing loss, metrics on test set\n            loss = loss_fn(output, target)\n            batch_size = data.shape[0]\n            total_loss += loss.item() * batch_size\n            for i, metric in enumerate(metric_fns):\n                total_metrics[i] += metric(output, target) * batch_size\n\n    n_samples = len(data_loader.sampler)\n    log = {'loss': total_loss / n_samples}\n    log.update({\n        met.__name__: total_metrics[i].item() / n_samples for i, met in enumerate(metric_fns)\n    })\n    logger.info(log)\n\n\nif __name__ == '__main__':\n    args = argparse.ArgumentParser(description='PyTorch Template')\n    args.add_argument('-c', '--config', default=None, type=str,\n                      help='config file path (default: None)')\n    args.add_argument('-r', '--resume', default=None, type=str,\n                      help='path to latest checkpoint (default: None)')\n    args.add_argument('-d', '--device', default=None, type=str,\n                      help='indices of GPUs to enable (default: all)')\n\n    config = ConfigParser.from_args(args)\n    main(config)\n"
        },
        {
          "name": "train.py",
          "type": "blob",
          "size": 2.7685546875,
          "content": "import argparse\nimport collections\nimport torch\nimport numpy as np\nimport data_loader.data_loaders as module_data\nimport model.loss as module_loss\nimport model.metric as module_metric\nimport model.model as module_arch\nfrom parse_config import ConfigParser\nfrom trainer import Trainer\nfrom utils import prepare_device\n\n\n# fix random seeds for reproducibility\nSEED = 123\ntorch.manual_seed(SEED)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\nnp.random.seed(SEED)\n\ndef main(config):\n    logger = config.get_logger('train')\n\n    # setup data_loader instances\n    data_loader = config.init_obj('data_loader', module_data)\n    valid_data_loader = data_loader.split_validation()\n\n    # build model architecture, then print to console\n    model = config.init_obj('arch', module_arch)\n    logger.info(model)\n\n    # prepare for (multi-device) GPU training\n    device, device_ids = prepare_device(config['n_gpu'])\n    model = model.to(device)\n    if len(device_ids) > 1:\n        model = torch.nn.DataParallel(model, device_ids=device_ids)\n\n    # get function handles of loss and metrics\n    criterion = getattr(module_loss, config['loss'])\n    metrics = [getattr(module_metric, met) for met in config['metrics']]\n\n    # build optimizer, learning rate scheduler. delete every lines containing lr_scheduler for disabling scheduler\n    trainable_params = filter(lambda p: p.requires_grad, model.parameters())\n    optimizer = config.init_obj('optimizer', torch.optim, trainable_params)\n    lr_scheduler = config.init_obj('lr_scheduler', torch.optim.lr_scheduler, optimizer)\n\n    trainer = Trainer(model, criterion, metrics, optimizer,\n                      config=config,\n                      device=device,\n                      data_loader=data_loader,\n                      valid_data_loader=valid_data_loader,\n                      lr_scheduler=lr_scheduler)\n\n    trainer.train()\n\n\nif __name__ == '__main__':\n    args = argparse.ArgumentParser(description='PyTorch Template')\n    args.add_argument('-c', '--config', default=None, type=str,\n                      help='config file path (default: None)')\n    args.add_argument('-r', '--resume', default=None, type=str,\n                      help='path to latest checkpoint (default: None)')\n    args.add_argument('-d', '--device', default=None, type=str,\n                      help='indices of GPUs to enable (default: all)')\n\n    # custom cli options to modify configuration from default values given in json file.\n    CustomArgs = collections.namedtuple('CustomArgs', 'flags type target')\n    options = [\n        CustomArgs(['--lr', '--learning_rate'], type=float, target='optimizer;args;lr'),\n        CustomArgs(['--bs', '--batch_size'], type=int, target='data_loader;args;batch_size')\n    ]\n    config = ConfigParser.from_args(args, options)\n    main(config)\n"
        },
        {
          "name": "trainer",
          "type": "tree",
          "content": null
        },
        {
          "name": "utils",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}