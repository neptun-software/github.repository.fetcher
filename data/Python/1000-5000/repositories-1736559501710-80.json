{
  "metadata": {
    "timestamp": 1736559501710,
    "page": 80,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjgw",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "qubvel/segmentation_models",
      "stars": 4789,
      "defaultBranch": "master",
      "files": [
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 1.1884765625,
          "content": "# Byte-compiled / optimized / DLL files\n__pycache__/\n*.py[cod]\n*$py.class\n.idea/\n*ipynb\n\n# C extensions\n*.so\n\n# Distribution / packaging\n.Python\nbuild/\ndevelop-eggs/\ndist/\ndownloads/\neggs/\n.eggs/\nlib/\nlib64/\nparts/\nsdist/\nvar/\nwheels/\n*.egg-info/\n.installed.cfg\n*.egg\nMANIFEST\n\n# PyInstaller\n#  Usually these files are written by a python script from a template\n#  before PyInstaller builds the exe, so as to inject date/other infos into it.\n*.manifest\n*.spec\n\n# Installer logs\npip-log.txt\npip-delete-this-directory.txt\n\n# Unit test / coverage reports\nhtmlcov/\n.tox/\n.coverage\n.coverage.*\n.cache\nnosetests.xml\ncoverage.xml\n*.cover\n.hypothesis/\n.pytest_cache/\n\n# Translations\n*.mo\n*.pot\n\n# Django stuff:\n*.log\nlocal_settings.py\ndb.sqlite3\n\n# Flask stuff:\ninstance/\n.webassets-cache\n\n# Scrapy stuff:\n.scrapy\n\n# Sphinx documentation\ndocs/_build/\n\n# PyBuilder\ntarget/\n\n# Jupyter Notebook\n.ipynb_checkpoints\n\n# pyenv\n.python-version\n\n# celery beat schedule file\ncelerybeat-schedule\n\n# SageMath parsed files\n*.sage.py\n\n# Environments\n.env\n.venv\nenv/\nvenv/\nENV/\nenv.bak/\nvenv.bak/\n\n# Spyder project settings\n.spyderproject\n.spyproject\n\n# Rope project settings\n.ropeproject\n\n# mkdocs documentation\n/site\n\n# mypy\n.mypy_cache/\n"
        },
        {
          "name": ".gitmodules",
          "type": "blob",
          "size": 0,
          "content": ""
        },
        {
          "name": ".travis.yml",
          "type": "blob",
          "size": 2.623046875,
          "content": "sudo: required\ndist: trusty\nlanguage: python\n\nmatrix:\n    include:\n        - python: 3.6\n          env: KERAS_VERSION=2.2.4\n        - python: 3.6\n          env: SM_FRAMEWORK='tf.keras'\n\ngit:\n  submodules: true\n\n\ninstall:\n  # code below is taken from http://conda.pydata.org/docs/travis.html\n  # We do this conditionally because it saves us some downloading if the\n  # version is the same.\n  - wget https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh -O miniconda.sh;\n  - bash miniconda.sh -b -p $HOME/miniconda\n  - export PATH=\"$HOME/miniconda/bin:$PATH\"\n  - hash -r\n  - conda config --set always_yes yes --set changeps1 no\n  - conda update -q conda\n  # Useful for debugging any issues with conda\n  - conda info -a\n\n  - conda create -q -n test-environment python=$TRAVIS_PYTHON_VERSION pytest pandas\n  - source activate test-environment\n  - pip install --only-binary=numpy,scipy numpy nose scipy matplotlib h5py theano\n\n\n  # set library path\n  - export LD_LIBRARY_PATH=$HOME/miniconda/envs/test-environment/lib/:$LD_LIBRARY_PATH\n  - conda install mkl mkl-service\n\n  # install TensorFlow (CPU version).\n  - pip install tensorflow==1.14\n\n  # install keras\n  - if [ -z $KERAS_VERSION ]; then\n      echo \"Using tf.keras\";\n    else\n      echo \"Using keras\";\n      pip install keras==$KERAS_VERSION;\n    fi\n\n  # install lib in develop mode\n  - pip install -e .[tests]\n\n  # detect one of markdown files is changed or not\n  - export DOC_ONLY_CHANGED=False;\n  - if [ $(git diff --name-only HEAD~1 | wc -l) == \"1\" ] && [[ \"$(git diff --name-only HEAD~1)\" == *\"md\" ]]; then\n      export DOC_ONLY_CHANGED=True;\n    fi\n\n# command to run tests\nscript:\n  - export MKL_THREADING_LAYER=\"GNU\"\n  - mkdir -p ~/.keras/models\n  # set up keras backend\n  - if [[ \"$DOC_ONLY_CHANGED\" == \"False\" ]]; then\n        PYTHONPATH=$PWD:$PYTHONPATH py.test tests/;\n    fi\n\ndeploy:\n  provider: pypi\n  user: qubvel\n  password:\n    secure: QA/UJmkXGlXy/6C8X0E/bPf4izu3rJsztaEmqIM1npxPiv2Uf4WFs43vxkMXwfHrflocdfw8SBM8bWnbunGT2SvDdo/MMCMpol7unE74T/RbODYl6aiJWVM3QKOXL8pQD0oQ+03L1YK3nCeSQdePINEPmuFmvwyO40q8Dwv8HBZIGZlEo4SK4xr8ekxfmtbezxQ7vUL3sNcvCJDXrZX/4UdXrhdRk+zYoN3dv8NmM4FmChajq/m5Am9OPdbdUBHmIYmvk7L3IpwJeMMpG5FVdGNVwYj7XNHlcy+KZ2/CKn9EpslRDPxY4650654PmhSZWDctZG7jiFWLCZBUvowiyAOPZknZNgdu5gJAfdg37XS9IP3HgTZN6Jb5Bm0by3IlKt+dTzyJQcUnRql5B1wwEI0XO3/YWQe1GQQphIO1bli9hT8n8xNDNjc49vDlu4zKyaYnQmLhqNxkyeruXSTpc8qTITuS+EGgkAUrrBj/IaFcutIg9WOzvJ3nZO8X8UG7LlyQx4AOpfHP6bynAmlT+UFccCEq66Zoh7teWLk0lUekuYST2iQJ3pwFoQGYJRCsmxsz7J0B9ayFVVT/fg+GZpZm1oTnnJ27hh8LZWv/Cr/WHOBYc3qvigWx4pDssJ+O6z7de3aWrGvzAVgXr190fRdP55a34HhNbiKZ0YWmrTs=\n  on:\n    tags: true\n  skip_existing: true\n  distributions: \"sdist bdist_wheel\"\n"
        },
        {
          "name": "CHANGELOG.md",
          "type": "blob",
          "size": 2.4736328125,
          "content": "# Change Log\n\n**Version 1.0.0**\n\n###### Areas of improvement\n - Support for `keras` and `tf.keras`\n - Losses as classes, base loss operations (sum of losses, multiplied loss)\n - NCHW and NHWC support\n - Removed pure tf operations to work with other keras backends\n - Reduced a number of custom objects for better models serialization and deserialization\n\n###### New featrues\n - New backbones: EfficentNetB[0-7] \n - New loss function: Focal loss \n - New metrics: Precision, Recall\n \n###### API changes\n - `get_preprocessing` moved from `sm.backbones.get_preprocessing` to `sm.get_preprocessing`\n\n**Version 0.2.1** \n\n###### Areas of improvement\n\n - Added `set_regularization` function \n - Added `beta` argument to dice loss\n - Added `threshold` argument for metrics\n - Fixed `prerprocess_input` for mobilenets\n - Fixed missing parameter `interpolation` in `ResizeImage` layer config\n - Some minor improvements in docs, fixed typos\n\n**Version 0.2.0** \n\n###### Areas of improvement\n\n - New backbones (SE-ResNets, SE-ResNeXts, SENet154, MobileNets)\n - Metrcis:  \n    - `iou_score` / `jaccard_score`\n    - `f_score` / `dice_score`\n - Losses:  \n    - `jaccard_loss` \n    - `bce_jaccard_loss`\n    - `cce_jaccard_loss`\n    - `dice_loss`\n    - `bce_dice_loss`\n    - `cce_dice_loss`\n  - Documentation [Read the Docs](https://segmentation-models.readthedocs.io)\n  - Tests + Travis-CI \n    \n###### API changes\n\n - Some parameters renamed (see API docs)\n - `encoder_freeze=True` does not `freeze` BatchNormalization layer of encoder\n\n###### Thanks\n\n[@IlyaOvodov](https://github.com/IlyaOvodov) [#15](https://github.com/qubvel/segmentation_models/issues/15) [#37](https://github.com/qubvel/segmentation_models/pull/37) investigation of `align_corners` parameter in `ResizeImage` layer  \n[@NiklasDL](https://github.com/NiklasDL) [#29](https://github.com/qubvel/segmentation_models/issues/29) investigation about convolution kernel in PSPNet final layers\n\n**Version 0.1.2**  \n\n###### Areas of improvement\n\n - Added PSPModel\n - Prepocessing functions for all backbones: \n```python\nfrom segmentation_models.backbones import get_preprocessing\n\npreprocessing_fn = get_preprocessing('resnet34')\nX = preprocessing_fn(x)\n```\n###### API changes\n- Default param `use_batchnorm=True` for all decoders\n- FPN model `Upsample2D` layer renamed to `ResizeImage`\n\n**Version 0.1.1**  \n - Added `Linknet` model\n - Keras 2.2+ compatibility (fixed import of `_obtain_input_shape`)\n - Small code improvements and bug fixes\n\n**Version 0.1.0**  \n - `Unet` and `FPN` models\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 1.052734375,
          "content": "The MIT License\n\nCopyright (c) 2018, Pavel Yakubovskiy\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in\nall copies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\nTHE SOFTWARE."
        },
        {
          "name": "MANIFEST.in",
          "type": "blob",
          "size": 0.0419921875,
          "content": "include README.md LICENSE requirements.txt\n"
        },
        {
          "name": "README.rst",
          "type": "blob",
          "size": 8.7685546875,
          "content": ".. raw:: html\n\n    <p align=\"center\">\n      <img src=\"https://i.ibb.co/GtxGS8m/Segmentation-Models-V1-Side-3-1.png\">\n      <b>Python library with Neural Networks for Image Segmentation based on <a href=https://www.keras.io>Keras</a> and <a href=https://www.tensorflow.org>TensorFlow</a>.\n      </b>\n      <br></br>\n\n      <a href=\"https://badge.fury.io/py/segmentation-models\" alt=\"PyPI\">\n        <img src=\"https://badge.fury.io/py/segmentation-models.svg\" /></a>\n      <a href=\"https://segmentation-models.readthedocs.io/en/latest/?badge=latest\" alt=\"Documentation\">\n        <img src=\"https://readthedocs.org/projects/segmentation-models/badge/?version=latest\" /></a>\n      <a href=\"https://travis-ci.com/qubvel/segmentation_models\" alt=\"Build Status\">\n        <img src=\"https://travis-ci.com/qubvel/segmentation_models.svg?branch=master\" /></a>\n    </p>\n\n\n**The main features** of this library are:\n\n-  High level API (just two lines of code to create model for segmentation)\n-  **4** models architectures for binary and multi-class image segmentation\n   (including legendary **Unet**)\n-  **25** available backbones for each architecture\n-  All backbones have **pre-trained** weights for faster and better\n   convergence\n- Helpful segmentation losses (Jaccard, Dice, Focal) and metrics (IoU, F-score)\n\n**Important note**\n\n    Some models of version ``1.*`` are not compatible with previously trained models,\n    if you have such models and want to load them - roll back with:\n\n    $ pip install -U segmentation-models==0.2.1\n\nTable of Contents\n~~~~~~~~~~~~~~~~~\n - `Quick start`_\n - `Simple training pipeline`_\n - `Examples`_\n - `Models and Backbones`_\n - `Installation`_\n - `Documentation`_\n - `Change log`_\n - `Citing`_\n - `License`_\n \nQuick start\n~~~~~~~~~~~\nLibrary is build to work together with Keras and TensorFlow Keras frameworks\n\n.. code:: python\n\n    import segmentation_models as sm\n    # Segmentation Models: using `keras` framework.\n\nBy default it tries to import ``keras``, if it is not installed, it will try to start with ``tensorflow.keras`` framework.\nThere are several ways to choose framework:\n\n- Provide environment variable ``SM_FRAMEWORK=keras`` / ``SM_FRAMEWORK=tf.keras`` before import ``segmentation_models``\n- Change framework ``sm.set_framework('keras')`` /  ``sm.set_framework('tf.keras')``\n\nYou can also specify what kind of ``image_data_format`` to use, segmentation-models works with both: ``channels_last`` and ``channels_first``.\nThis can be useful for further model conversion to Nvidia TensorRT format or optimizing model for cpu/gpu computations.\n\n.. code:: python\n\n    import keras\n    # or from tensorflow import keras\n\n    keras.backend.set_image_data_format('channels_last')\n    # or keras.backend.set_image_data_format('channels_first')\n\nCreated segmentation model is just an instance of Keras Model, which can be build as easy as:\n\n.. code:: python\n    \n    model = sm.Unet()\n    \nDepending on the task, you can change the network architecture by choosing backbones with fewer or more parameters and use pretrainded weights to initialize it:\n\n.. code:: python\n\n    model = sm.Unet('resnet34', encoder_weights='imagenet')\n\nChange number of output classes in the model (choose your case):\n\n.. code:: python\n    \n    # binary segmentation (this parameters are default when you call Unet('resnet34')\n    model = sm.Unet('resnet34', classes=1, activation='sigmoid')\n    \n.. code:: python\n    \n    # multiclass segmentation with non overlapping class masks (your classes + background)\n    model = sm.Unet('resnet34', classes=3, activation='softmax')\n    \n.. code:: python\n    \n    # multiclass segmentation with independent overlapping/non-overlapping class masks\n    model = sm.Unet('resnet34', classes=3, activation='sigmoid')\n    \n    \nChange input shape of the model:\n\n.. code:: python\n    \n    # if you set input channels not equal to 3, you have to set encoder_weights=None\n    # how to handle such case with encoder_weights='imagenet' described in docs\n    model = Unet('resnet34', input_shape=(None, None, 6), encoder_weights=None)\n   \nSimple training pipeline\n~~~~~~~~~~~~~~~~~~~~~~~~\n\n.. code:: python\n\n    import segmentation_models as sm\n\n    BACKBONE = 'resnet34'\n    preprocess_input = sm.get_preprocessing(BACKBONE)\n\n    # load your data\n    x_train, y_train, x_val, y_val = load_data(...)\n\n    # preprocess input\n    x_train = preprocess_input(x_train)\n    x_val = preprocess_input(x_val)\n\n    # define model\n    model = sm.Unet(BACKBONE, encoder_weights='imagenet')\n    model.compile(\n        'Adam',\n        loss=sm.losses.bce_jaccard_loss,\n        metrics=[sm.metrics.iou_score],\n    )\n\n    # fit model\n    # if you use data generator use model.fit_generator(...) instead of model.fit(...)\n    # more about `fit_generator` here: https://keras.io/models/sequential/#fit_generator\n    model.fit(\n       x=x_train,\n       y=y_train,\n       batch_size=16,\n       epochs=100,\n       validation_data=(x_val, y_val),\n    )\n\nSame manipulations can be done with ``Linknet``, ``PSPNet`` and ``FPN``. For more detailed information about models API and  use cases `Read the Docs <https://segmentation-models.readthedocs.io/en/latest/>`__.\n\nExamples\n~~~~~~~~\nModels training examples:\n - [Jupyter Notebook] Binary segmentation (`cars`) on CamVid dataset `here <https://github.com/qubvel/segmentation_models/blob/master/examples/binary%20segmentation%20(camvid).ipynb>`__.\n - [Jupyter Notebook] Multi-class segmentation (`cars`, `pedestrians`) on CamVid dataset `here <https://github.com/qubvel/segmentation_models/blob/master/examples/multiclass%20segmentation%20(camvid).ipynb>`__.\n\nModels and Backbones\n~~~~~~~~~~~~~~~~~~~~\n**Models**\n\n-  `Unet <https://arxiv.org/abs/1505.04597>`__\n-  `FPN <http://presentations.cocodataset.org/COCO17-Stuff-FAIR.pdf>`__\n-  `Linknet <https://arxiv.org/abs/1707.03718>`__\n-  `PSPNet <https://arxiv.org/abs/1612.01105>`__\n\n============= ==============\nUnet          Linknet\n============= ==============\n|unet_image|  |linknet_image|\n============= ==============\n\n============= ==============\nPSPNet        FPN\n============= ==============\n|psp_image|   |fpn_image|\n============= ==============\n\n.. _Unet: https://github.com/qubvel/segmentation_models/blob/readme/LICENSE\n.. _Linknet: https://arxiv.org/abs/1707.03718\n.. _PSPNet: https://arxiv.org/abs/1612.01105\n.. _FPN: http://presentations.cocodataset.org/COCO17-Stuff-FAIR.pdf\n\n.. |unet_image| image:: https://github.com/qubvel/segmentation_models/blob/master/images/unet.png\n.. |linknet_image| image:: https://github.com/qubvel/segmentation_models/blob/master/images/linknet.png\n.. |psp_image| image:: https://github.com/qubvel/segmentation_models/blob/master/images/pspnet.png\n.. |fpn_image| image:: https://github.com/qubvel/segmentation_models/blob/master/images/fpn.png\n\n**Backbones**\n\n.. table:: \n\n    =============  ===== \n    Type           Names\n    =============  =====\n    VGG            ``'vgg16' 'vgg19'``\n    ResNet         ``'resnet18' 'resnet34' 'resnet50' 'resnet101' 'resnet152'``\n    SE-ResNet      ``'seresnet18' 'seresnet34' 'seresnet50' 'seresnet101' 'seresnet152'``\n    ResNeXt        ``'resnext50' 'resnext101'``\n    SE-ResNeXt     ``'seresnext50' 'seresnext101'``\n    SENet154       ``'senet154'``\n    DenseNet       ``'densenet121' 'densenet169' 'densenet201'`` \n    Inception      ``'inceptionv3' 'inceptionresnetv2'``\n    MobileNet      ``'mobilenet' 'mobilenetv2'``\n    EfficientNet   ``'efficientnetb0' 'efficientnetb1' 'efficientnetb2' 'efficientnetb3' 'efficientnetb4' 'efficientnetb5' efficientnetb6' efficientnetb7'``\n    =============  =====\n\n.. epigraph::\n    All backbones have weights trained on 2012 ILSVRC ImageNet dataset (``encoder_weights='imagenet'``). \n\n\nInstallation\n~~~~~~~~~~~~\n\n**Requirements**\n\n1) python 3\n2) keras >= 2.2.0 or tensorflow >= 1.13\n3) keras-applications >= 1.0.7, <=1.0.8\n4) image-classifiers == 1.0.*\n5) efficientnet == 1.0.*\n\n**PyPI stable package**\n\n.. code:: bash\n\n    $ pip install -U segmentation-models\n\n**PyPI latest package**\n\n.. code:: bash\n\n    $ pip install -U --pre segmentation-models\n\n**Source latest version**\n\n.. code:: bash\n\n    $ pip install git+https://github.com/qubvel/segmentation_models\n    \nDocumentation\n~~~~~~~~~~~~~\nLatest **documentation** is avaliable on `Read the\nDocs <https://segmentation-models.readthedocs.io/en/latest/>`__\n\nChange Log\n~~~~~~~~~~\nTo see important changes between versions look at CHANGELOG.md_\n\nCiting\n~~~~~~~~\n\n.. code::\n\n    @misc{Yakubovskiy:2019,\n      Author = {Pavel Iakubovskii},\n      Title = {Segmentation Models},\n      Year = {2019},\n      Publisher = {GitHub},\n      Journal = {GitHub repository},\n      Howpublished = {\\url{https://github.com/qubvel/segmentation_models}}\n    } \n\nLicense\n~~~~~~~\nProject is distributed under `MIT Licence`_.\n\n.. _CHANGELOG.md: https://github.com/qubvel/segmentation_models/blob/master/CHANGELOG.md\n.. _`MIT Licence`: https://github.com/qubvel/segmentation_models/blob/master/LICENSE\n"
        },
        {
          "name": "__init__.py",
          "type": "blob",
          "size": 0.0341796875,
          "content": "from .segmentation_models import *\n"
        },
        {
          "name": "docs",
          "type": "tree",
          "content": null
        },
        {
          "name": "examples",
          "type": "tree",
          "content": null
        },
        {
          "name": "images",
          "type": "tree",
          "content": null
        },
        {
          "name": "requirements.txt",
          "type": "blob",
          "size": 0.0771484375,
          "content": "keras_applications>=1.0.7,<=1.0.8\nimage-classifiers==1.0.0\nefficientnet==1.1.1\n"
        },
        {
          "name": "segmentation_models",
          "type": "tree",
          "content": null
        },
        {
          "name": "setup.py",
          "type": "blob",
          "size": 3.6796875,
          "content": "#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\n# Note: To use the 'upload' functionality of this file, you must:\n#   $ pip install twine\n\nimport io\nimport os\nimport sys\nfrom shutil import rmtree\n\nfrom setuptools import find_packages, setup, Command\n\n# Package meta-data.\nNAME = 'segmentation_models'\nDESCRIPTION = 'Image segmentation models with pre-trained backbones with Keras.'\nURL = 'https://github.com/qubvel/segmentation_models'\nEMAIL = 'qubvel@gmail.com'\nAUTHOR = 'Pavel Yakubovskiy'\nREQUIRES_PYTHON = '>=3.0.0'\nVERSION = None\n\n# The rest you shouldn't have to touch too much :)\n# ------------------------------------------------\n# Except, perhaps the License and Trove Classifiers!\n# If you do change the License, remember to change the Trove Classifier for that!\n\nhere = os.path.abspath(os.path.dirname(__file__))\n\n# What packages are required for this module to be executed?\ntry:\n    with open(os.path.join(here, 'requirements.txt'), encoding='utf-8') as f:\n        REQUIRED = f.read().split('\\n')\nexcept:\n    REQUIRED = []\n\n# What packages are optional?\nEXTRAS = {\n    'tests': ['pytest', 'scikit-image'],\n}\n\n# Import the README and use it as the long-description.\n# Note: this will only work if 'README.md' is present in your MANIFEST.in file!\ntry:\n    with io.open(os.path.join(here, 'README.md'), encoding='utf-8') as f:\n        long_description = '\\n' + f.read()\nexcept FileNotFoundError:\n    long_description = DESCRIPTION\n\n# Load the package's __version__.py module as a dictionary.\nabout = {}\nif not VERSION:\n    with open(os.path.join(here, NAME, '__version__.py')) as f:\n        exec(f.read(), about)\nelse:\n    about['__version__'] = VERSION\n\n\nclass UploadCommand(Command):\n    \"\"\"Support setup.py upload.\"\"\"\n\n    description = 'Build and publish the package.'\n    user_options = []\n\n    @staticmethod\n    def status(s):\n        \"\"\"Prints things in bold.\"\"\"\n        print(s)\n\n    def initialize_options(self):\n        pass\n\n    def finalize_options(self):\n        pass\n\n    def run(self):\n        try:\n            self.status('Removing previous builds...')\n            rmtree(os.path.join(here, 'dist'))\n        except OSError:\n            pass\n\n        self.status('Building Source and Wheel (universal) distribution...')\n        os.system('{0} setup.py sdist bdist_wheel --universal'.format(sys.executable))\n\n        self.status('Uploading the package to PyPI via Twine...')\n        os.system('twine upload dist/*')\n\n        self.status('Pushing git tags...')\n        os.system('git tag v{0}'.format(about['__version__']))\n        os.system('git push --tags')\n\n        sys.exit()\n\n\n# Where the magic happens:\nsetup(\n    name=NAME,\n    version=about['__version__'],\n    description=DESCRIPTION,\n    long_description=long_description,\n    long_description_content_type='text/x-rst',\n    author=AUTHOR,\n    author_email=EMAIL,\n    python_requires=REQUIRES_PYTHON,\n    url=URL,\n    packages=find_packages(exclude=('tests', 'docs', 'images', 'examples')),\n    # If your package is a single module, use this instead of 'packages':\n    # py_modules=['mypackage'],\n\n    # entry_points={\n    #     'console_scripts': ['mycli=mymodule:cli'],\n    # },\n    install_requires=REQUIRED,\n    extras_require=EXTRAS,\n    include_package_data=True,\n    license='MIT',\n    classifiers=[\n        # Trove classifiers\n        # Full list: https://pypi.python.org/pypi?%3Aaction=list_classifiers\n        'License :: OSI Approved :: MIT License',\n        'Programming Language :: Python',\n        'Programming Language :: Python :: 3',\n        'Programming Language :: Python :: Implementation :: CPython',\n        'Programming Language :: Python :: Implementation :: PyPy'\n    ],\n    # $ setup.py publish support.\n    cmdclass={\n        'upload': UploadCommand,\n    },\n)"
        },
        {
          "name": "tests",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}