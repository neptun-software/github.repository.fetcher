{
  "metadata": {
    "timestamp": 1736559974835,
    "page": 763,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjc3MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "ZhaoJ9014/face.evoLVe",
      "stars": 3469,
      "defaultBranch": "master",
      "files": [
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.033203125,
          "content": "#test\ntest\nconfig_f.py\ntrain_f.py\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 1.041015625,
          "content": "MIT License\n\nCopyright (c) 2019 Jian Zhao\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 54.556640625,
          "content": "# face.evoLVe: High-Performance Face Recognition Library based on PaddlePaddle & PyTorch\n* Evolve to be more comprehensive, effective and efficient for face related analytics \\& applications! ([WeChat News](https://mp.weixin.qq.com/s/V8VoyMqVvjblH358ozcWEg))\n* About the name:\n  * \"face\" means this repo is dedicated for face related analytics \\& applications.\n  * \"evolve\" means unleash your greatness to be better and better. \"LV\" are capitalized to acknowledge the nurturing of Learning and Vision ([LV](http://www.lv-nus.org)) group, Nation University of Singapore (NUS).\n* This work was done during Jian Zhao served as a short-term \"Texpert\" Research Scientist at Tencent FiT DeepSea AI Lab, Shenzhen, China.\n\n|Author|Jian Zhao|\n|:---:|:---:|\n|Homepage|https://zhaoj9014.github.io|\n\n****\n## License\n\nThe code of [face.evoLVe](#Introduction) is released under the MIT License.\n\n****\n## News\n\n:white_check_mark: **`CLOSED 02 September 2021`**: ~~Baidu PaddlePaddle officially merged [face.evoLVe](#Introduction) to faciliate researches and applications on face-related analytics ([Official Announcement](https://mp.weixin.qq.com/s/JT_4pqRvSsAOhQln0GSH_g)).~~\n\n:white_check_mark: **`CLOSED 03 July 2021`**: ~~Provides training code for the paddlepaddle framework.~~\n\n:white_check_mark: **`CLOSED 04 July 2019`**: ~~We will share several publicly available datasets on face anti-spoofing/liveness detection to facilitate related research and analytics.~~\n\n:white_check_mark: **`CLOSED 07 June 2019`**: ~~We are training a better-performing [IR-152](https://arxiv.org/pdf/1512.03385.pdf) model on [MS-Celeb-1M_Align_112x112](https://arxiv.org/pdf/1607.08221.pdf), and will release the model soon.~~\n\n:white_check_mark: **`CLOSED 23 May 2019`**: ~~We share three publicly available datasets to facilitate research on heterogeneous face recognition and analytics. Please refer to Sec. [Data Zoo](#Data-Zoo) for details.~~\n\n:white_check_mark: **`CLOSED 23 Jan 2019`**: ~~We share the name lists and pair-wise overlapping lists of several widely-used face recognition datasets to help researchers/engineers quickly remove the overlapping parts between their own private datasets and the public datasets. Please refer to Sec. [Data Zoo](#Data-Zoo) for details.~~\n\n:white_check_mark: **`CLOSED 23 Jan 2019`**: ~~The current distributed training schema with multi-GPUs under PyTorch and other mainstream platforms parallels the backbone across multi-GPUs while relying on a single master to compute the final bottleneck (fully-connected/softmax) layer. This is not an issue for conventional face recognition with moderate number of identities. However, it struggles with large-scale face recognition, which requires recognizing millions of identities in the real world. The master can hardly hold the oversized final layer while the slaves still have redundant computation resource, leading to small-batch training or even failed training. To address this problem, we are developing a highly-elegant, effective and efficient distributed training schema with multi-GPUs under PyTorch, supporting not only the backbone, but also the head with the fully-connected (softmax) layer, to facilitate high-performance large-scale face recognition. We will added this support into our repo.~~\n\n:white_check_mark: **`CLOSED 22 Jan 2019`**: ~~We have released two feature extraction APIs for extracting features from pre-trained models, implemented with PyTorch build-in functions and OpenCV, respectively. Please check ```./util/extract_feature_v1.py``` and ```./util/extract_feature_v2.py```.~~\n\n:white_check_mark: **`CLOSED 22 Jan 2019`**: ~~We are fine-tuning our released [IR-50](https://arxiv.org/pdf/1512.03385.pdf) model on our private Asia face data, which will be released soon to facilitate high-performance Asia face recognition.~~\n\n:white_check_mark: **`CLOSED 21 Jan 2019`**: ~~We are training a better-performing [IR-50](https://arxiv.org/pdf/1512.03385.pdf) model on [MS-Celeb-1M_Align_112x112](https://arxiv.org/pdf/1607.08221.pdf), and will replace the current model soon.~~\n  \n****\n## Contents\n* [Introduction](#Introduction)\n* [Pre-Requisites](#Pre-Requisites)\n* [Usage](#Usage)\n* [Face Alignment](#Face-Alignment)\n* [Data Processing](#Data-Processing)\n* [Training and Validation](#Training-and-Validation)\n* [Data Zoo](#Data-Zoo)\n* [Model Zoo](#Model-Zoo)\n* [Achievement](#Achievement)\n* [Acknowledgement](#Acknowledgement)\n* [Donation](#Donation)\n* [Citation](#Citation)\n\n****\n## face.evoLVe for High-Performance Face Recognition\n\n### Introduction \n:information_desk_person:\n\n<img src=\"https://github.com/ZhaoJ9014/face.evoLVe/blob/master/disp/Fig1.png\" width=\"450px\"/>  <img src=\"https://github.com/ZhaoJ9014/face.evoLVe/blob/master/disp/Fig17.png\" width=\"400px\"/>\n\n* This repo provides a comprehensive face recognition library for face related analytics \\& applications, including face alignment (detection, landmark localization, affine transformation, *etc.*), data processing (*e.g.*, augmentation, data balancing, normalization, *etc.*), various backbones (*e.g.*, [ResNet](https://arxiv.org/pdf/1512.03385.pdf), [IR](https://arxiv.org/pdf/1512.03385.pdf), [IR-SE](https://arxiv.org/pdf/1709.01507.pdf), ResNeXt, SE-ResNeXt, DenseNet, [LightCNN](https://arxiv.org/pdf/1511.02683.pdf), MobileNet, ShuffleNet, DPN, *etc.*), various losses (*e.g.*, Softmax, [Focal](https://arxiv.org/pdf/1708.02002.pdf), Center, [SphereFace](https://arxiv.org/pdf/1704.08063.pdf), [CosFace](https://arxiv.org/pdf/1801.09414.pdf), [AmSoftmax](https://arxiv.org/pdf/1801.05599.pdf), [ArcFace](https://arxiv.org/pdf/1801.07698.pdf), Triplet, *etc.*) and bags of tricks for improving performance (*e.g.*, training refinements, model tweaks, knowledge distillation, *etc.*).\n* The current distributed training schema with multi-GPUs under PyTorch and other mainstream platforms parallels the backbone across multi-GPUs while relying on a single master to compute the final bottleneck (fully-connected/softmax) layer. This is not an issue for conventional face recognition with moderate number of identities. However, it struggles with large-scale face recognition, which requires recognizing millions of identities in the real world. The master can hardly hold the oversized final layer while the slaves still have redundant computation resource, leading to small-batch training or even failed training. To address this problem, this repo provides a highly-elegant, effective and efficient distributed training schema with multi-GPUs under PyTorch, supporting not only the backbone, but also the head with the fully-connected (softmax) layer, to facilitate high-performance large-scale face recognition.\n* All data before \\& after alignment, source codes and trained models are provided.\n* This repo can help researchers/engineers develop high-performance deep face recognition models and algorithms quickly for practical use and deployment.\n\n****\n### Pre-Requisites \n:cake:\n\n* Linux or macOS\n* [Python 3.7](https://repo.continuum.io/archive/Anaconda3-2018.12-Linux-x86_64.sh) (for training \\& validation) and [Python 2.7](https://repo.continuum.io/archive/Anaconda2-2018.12-Linux-x86_64.sh) (for visualization w/ tensorboardX)\n* PyTorch 1.0 (for traininig \\& validation, install w/ `pip install torch torchvision`)\n* MXNet 1.3.1 (optional, for data processing, install w/ `pip install mxnet-cu90`)\n* TensorFlow 1.12 (optional, for visualization, install w/ `pip install tensorflow-gpu`)\n* tensorboardX 1.6 (optional, for visualization, install w/ `pip install tensorboardX`)\n* OpenCV 3.4.5 (install w/ `pip install opencv-python`)\n* bcolz 1.2.0 (install w/ `pip install bcolz`)\n\nWhile not required, for optimal performance it is **highly** recommended to run the code using a CUDA enabled GPU. We used 4-8 NVIDIA Tesla P40 in parallel.\n\n****\n### Usage \n:orange_book:\n\n* Clone the repo: `git clone https://github.com/ZhaoJ9014/face.evoLVe.PyTorch.git`.\n* `mkdir data checkpoint log` at appropriate directory to store your train/val/test data, checkpoints and training logs.\n* Prepare your train/val/test data (refer to Sec. [Data Zoo](#Data-Zoo) for publicly available face related databases), and ensure each database folder has the following structure:\n  ```\n  ./data/db_name/\n          -> id1/\n              -> 1.jpg\n              -> ...\n          -> id2/\n              -> 1.jpg\n              -> ...\n          -> ...\n              -> ...\n              -> ...\n  ```\n* Refer to the codes of corresponding sections for specific purposes.\n\n****\n### Face Alignment \n:triangular_ruler:\n\n<img src=\"https://github.com/ZhaoJ9014/face.evoLVe/blob/master/disp/Fig2.png\" width=\"900px\"/>\n<img src=\"https://github.com/ZhaoJ9014/face.evoLVe/blob/master/disp/Fig3.png\" width=\"500px\"/>\n\n* This section is based on the work of [MTCNN](https://arxiv.org/pdf/1604.02878.pdf).\n* Folder: ```./align```\n* Face detection, landmark localization APIs and visualization toy example with ipython notebook:\n  ```python \n  from PIL import Image\n  from detector import detect_faces\n  from visualization_utils import show_results\n\n  img = Image.open('some_img.jpg') # modify the image path to yours\n  bounding_boxes, landmarks = detect_faces(img) # detect bboxes and landmarks for all faces in the image\n  show_results(img, bounding_boxes, landmarks) # visualize the results\n  ``` \n* Face alignment API (perform face detection, landmark localization and alignment with affine transformations on a whole database folder ```source_root``` with the directory structure as demonstrated in Sec. [Usage](#Usage), and store the aligned results to a new folder ```dest_root``` with the same directory structure): \n  ```\n  python face_align.py -source_root [source_root] -dest_root [dest_root] -crop_size [crop_size]\n\n  # python face_align.py -source_root './data/test' -dest_root './data/test_Aligned' -crop_size 112\n  ```\n* For macOS users, there is no need to worry about ```*.DS_Store``` files which may ruin your data, since they will be automatically removed when you run the scripts.\n* Keynotes for customed use: 1) specify the arguments of ```source_root```, ```dest_root``` and ```crop_size``` to your own values when you run ```face_align.py```; 2) pass your customed ```min_face_size```, ```thresholds``` and ```nms_thresholds``` values to the ```detect_faces``` function of ```detector.py``` to match your practical requirements; 3) if you find the speed using face alignment API is a bit slow, you can call face resize API to firstly resize the image whose smaller size is larger than a threshold (specify the arguments of ```source_root```, ```dest_root``` and ```min_side``` to your own values) before calling the face alignment API:\n  ```\n  python face_resize.py\n  ```\n\n****\n### Data Processing \n:bar_chart:\n\n* Folder: ```./balance```\n* Remove low-shot data API (remove the low-shot classes with less than ```min_num``` samples in the training set ```root``` with the directory structure as demonstrated in Sec. [Usage](#Usage) for data balance and effective model training):\n  ```\n  python remove_lowshot.py -root [root] -min_num [min_num]\n\n  # python remove_lowshot.py -root './data/train' -min_num 10\n  ```\n* Keynotes for customed use: specify the arguments of ```root``` and ```min_num``` to your own values when you run ```remove_lowshot.py```.\n* We prefer to include other data processing tricks, *e.g.*, augmentation (flip horizontally, scale hue/satuation/brightness with coefficients uniformly drawn from \\[0.6,1.4\\], add PCA noise with a coefficient sampled from a normal distribution N(0,0.1), *etc.*), weighted random sampling, normalization, *etc.* to the main training script in Sec. [Training and Validation](#Training-and-Validation) to be self-contained.\n\n****\n### Training and Validation \n:coffee:\n\n* Folder: ```./```\n* Configuration API (configurate your overall settings for training \\& validation) ```config.py```:\n  ```python\n  import torch\n\n  configurations = {\n      1: dict(\n          SEED = 1337, # random seed for reproduce results\n\n          DATA_ROOT = '/media/pc/6T/jasonjzhao/data/faces_emore', # the parent root where your train/val/test data are stored\n          MODEL_ROOT = '/media/pc/6T/jasonjzhao/buffer/model', # the root to buffer your checkpoints\n          LOG_ROOT = '/media/pc/6T/jasonjzhao/buffer/log', # the root to log your train/val status\n          BACKBONE_RESUME_ROOT = './', # the root to resume training from a saved checkpoint\n          HEAD_RESUME_ROOT = './', # the root to resume training from a saved checkpoint\n\n          BACKBONE_NAME = 'IR_SE_50', # support: ['ResNet_50', 'ResNet_101', 'ResNet_152', 'IR_50', 'IR_101', 'IR_152', 'IR_SE_50', 'IR_SE_101', 'IR_SE_152']\n          HEAD_NAME = 'ArcFace', # support:  ['Softmax', 'ArcFace', 'CosFace', 'SphereFace', 'Am_softmax']\n          LOSS_NAME = 'Focal', # support: ['Focal', 'Softmax']\n\n          INPUT_SIZE = [112, 112], # support: [112, 112] and [224, 224]\n          RGB_MEAN = [0.5, 0.5, 0.5], # for normalize inputs to [-1, 1]\n          RGB_STD = [0.5, 0.5, 0.5],\n          EMBEDDING_SIZE = 512, # feature dimension\n          BATCH_SIZE = 512,\n          DROP_LAST = True, # whether drop the last batch to ensure consistent batch_norm statistics\n          LR = 0.1, # initial LR\n          NUM_EPOCH = 125, # total epoch number (use the firt 1/25 epochs to warm up)\n          WEIGHT_DECAY = 5e-4, # do not apply to batch_norm parameters\n          MOMENTUM = 0.9,\n          STAGES = [35, 65, 95], # epoch stages to decay learning rate\n\n          DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\"),\n          MULTI_GPU = True, # flag to use multiple GPUs; if you choose to train with single GPU, you should first run \"export CUDA_VISILE_DEVICES=device_id\" to specify the GPU card you want to use\n          GPU_ID = [0, 1, 2, 3], # specify your GPU ids\n          PIN_MEMORY = True,\n          NUM_WORKERS = 0,\n  ),\n  }\n  ```\n* Train \\& validation API (all folks about training \\& validation, *i.e.*, import package, hyperparameters \\& data loaders, model & loss & optimizer, train & validation & save checkpoint) ```train.py```. Since [MS-Celeb-1M](https://arxiv.org/pdf/1607.08221.pdf) serves as an [ImageNet](https://www.researchgate.net/profile/Li_Jia_Li/publication/221361415_ImageNet_a_Large-Scale_Hierarchical_Image_Database/links/00b495388120dbc339000000/ImageNet-a-Large-Scale-Hierarchical-Image-Database.pdf) in the filed of face recognition, we pre-train the [face.evoLVe](#Introduction) models on [MS-Celeb-1M](https://arxiv.org/pdf/1607.08221.pdf) and perform validation on [LFW](https://hal.inria.fr/file/index/docid/321923/filename/Huang_long_eccv2008-lfw.pdf), [CFP_FF](http://www.cfpw.io/paper.pdf), [CFP_FP](http://www.cfpw.io/paper.pdf), [AgeDB](http://openaccess.thecvf.com/content_cvpr_2017_workshops/w33/papers/Moschoglou_AgeDB_The_First_CVPR_2017_paper.pdf), [CALFW](https://arxiv.org/pdf/1708.08197.pdf), [CPLFW](http://www.whdeng.cn/CPLFW/Cross-Pose-LFW.pdf) and [Vggface2_FP](https://arxiv.org/pdf/1710.08092.pdf). Let's dive into details together step by step.\n  * Import necessary packages:\n    ```python\n    import torch\n    import torch.nn as nn\n    import torch.optim as optim\n    import torchvision.transforms as transforms\n    import torchvision.datasets as datasets\n\n    from config import configurations\n    from backbone.model_resnet import ResNet_50, ResNet_101, ResNet_152\n    from backbone.model_irse import IR_50, IR_101, IR_152, IR_SE_50, IR_SE_101, IR_SE_152\n    from head.metrics import ArcFace, CosFace, SphereFace, Am_softmax\n    from loss.focal import FocalLoss\n    from util.utils import make_weights_for_balanced_classes, get_val_data, separate_irse_bn_paras, separate_resnet_bn_paras, warm_up_lr, schedule_lr, perform_val, get_time, buffer_val, AverageMeter, accuracy\n\n    from tensorboardX import SummaryWriter\n    from tqdm import tqdm\n    import os\n    ```\n  * Initialize hyperparameters:\n    ```python\n    cfg = configurations[1]\n\n    SEED = cfg['SEED'] # random seed for reproduce results\n    torch.manual_seed(SEED)\n\n    DATA_ROOT = cfg['DATA_ROOT'] # the parent root where your train/val/test data are stored\n    MODEL_ROOT = cfg['MODEL_ROOT'] # the root to buffer your checkpoints\n    LOG_ROOT = cfg['LOG_ROOT'] # the root to log your train/val status\n    BACKBONE_RESUME_ROOT = cfg['BACKBONE_RESUME_ROOT'] # the root to resume training from a saved checkpoint\n    HEAD_RESUME_ROOT = cfg['HEAD_RESUME_ROOT']  # the root to resume training from a saved checkpoint\n\n    BACKBONE_NAME = cfg['BACKBONE_NAME'] # support: ['ResNet_50', 'ResNet_101', 'ResNet_152', 'IR_50', 'IR_101', 'IR_152', 'IR_SE_50', 'IR_SE_101', 'IR_SE_152']\n    HEAD_NAME = cfg['HEAD_NAME'] # support:  ['Softmax', 'ArcFace', 'CosFace', 'SphereFace', 'Am_softmax']\n    LOSS_NAME = cfg['LOSS_NAME'] # support: ['Focal', 'Softmax']\n\n    INPUT_SIZE = cfg['INPUT_SIZE']\n    RGB_MEAN = cfg['RGB_MEAN'] # for normalize inputs\n    RGB_STD = cfg['RGB_STD']\n    EMBEDDING_SIZE = cfg['EMBEDDING_SIZE'] # feature dimension\n    BATCH_SIZE = cfg['BATCH_SIZE']\n    DROP_LAST = cfg['DROP_LAST'] # whether drop the last batch to ensure consistent batch_norm statistics\n    LR = cfg['LR'] # initial LR\n    NUM_EPOCH = cfg['NUM_EPOCH']\n    WEIGHT_DECAY = cfg['WEIGHT_DECAY']\n    MOMENTUM = cfg['MOMENTUM']\n    STAGES = cfg['STAGES'] # epoch stages to decay learning rate\n\n    DEVICE = cfg['DEVICE']\n    MULTI_GPU = cfg['MULTI_GPU'] # flag to use multiple GPUs\n    GPU_ID = cfg['GPU_ID'] # specify your GPU ids\n    PIN_MEMORY = cfg['PIN_MEMORY']\n    NUM_WORKERS = cfg['NUM_WORKERS']\n    print(\"=\" * 60)\n    print(\"Overall Configurations:\")\n    print(cfg)\n    print(\"=\" * 60)\n\n    writer = SummaryWriter(LOG_ROOT) # writer for buffering intermedium results\n    ```\n  * Train \\& validation data loaders:\n    ```python\n    train_transform = transforms.Compose([ # refer to https://pytorch.org/docs/stable/torchvision/transforms.html for more build-in online data augmentation\n        transforms.Resize([int(128 * INPUT_SIZE[0] / 112), int(128 * INPUT_SIZE[0] / 112)]), # smaller side resized\n        transforms.RandomCrop([INPUT_SIZE[0], INPUT_SIZE[1]]),\n        transforms.RandomHorizontalFlip(),\n        transforms.ToTensor(),\n        transforms.Normalize(mean = RGB_MEAN,\n                             std = RGB_STD),\n    ])\n\n    dataset_train = datasets.ImageFolder(os.path.join(DATA_ROOT, 'imgs'), train_transform)\n\n    # create a weighted random sampler to process imbalanced data\n    weights = make_weights_for_balanced_classes(dataset_train.imgs, len(dataset_train.classes))\n    weights = torch.DoubleTensor(weights)\n    sampler = torch.utils.data.sampler.WeightedRandomSampler(weights, len(weights))\n\n    train_loader = torch.utils.data.DataLoader(\n        dataset_train, batch_size = BATCH_SIZE, sampler = sampler, pin_memory = PIN_MEMORY,\n        num_workers = NUM_WORKERS, drop_last = DROP_LAST\n    )\n\n    NUM_CLASS = len(train_loader.dataset.classes)\n    print(\"Number of Training Classes: {}\".format(NUM_CLASS))\n\n    lfw, cfp_ff, cfp_fp, agedb, calfw, cplfw, vgg2_fp, lfw_issame, cfp_ff_issame, cfp_fp_issame, agedb_issame, calfw_issame, cplfw_issame, vgg2_fp_issame = get_val_data(DATA_ROOT)\n    ```\n  * Define and initialize model (backbone \\& head):\n    ```python\n    BACKBONE_DICT = {'ResNet_50': ResNet_50(INPUT_SIZE), \n                     'ResNet_101': ResNet_101(INPUT_SIZE), \n                     'ResNet_152': ResNet_152(INPUT_SIZE),\n                     'IR_50': IR_50(INPUT_SIZE), \n                     'IR_101': IR_101(INPUT_SIZE), \n                     'IR_152': IR_152(INPUT_SIZE),\n                     'IR_SE_50': IR_SE_50(INPUT_SIZE), \n                     'IR_SE_101': IR_SE_101(INPUT_SIZE), \n                     'IR_SE_152': IR_SE_152(INPUT_SIZE)}\n    BACKBONE = BACKBONE_DICT[BACKBONE_NAME]\n    print(\"=\" * 60)\n    print(BACKBONE)\n    print(\"{} Backbone Generated\".format(BACKBONE_NAME))\n    print(\"=\" * 60)\n\n    HEAD_DICT = {'ArcFace': ArcFace(in_features = EMBEDDING_SIZE, out_features = NUM_CLASS, device_id = GPU_ID),\n                 'CosFace': CosFace(in_features = EMBEDDING_SIZE, out_features = NUM_CLASS, device_id = GPU_ID),\n                 'SphereFace': SphereFace(in_features = EMBEDDING_SIZE, out_features = NUM_CLASS, device_id = GPU_ID),\n                 'Am_softmax': Am_softmax(in_features = EMBEDDING_SIZE, out_features = NUM_CLASS, device_id = GPU_ID)}\n    HEAD = HEAD_DICT[HEAD_NAME]\n    print(\"=\" * 60)\n    print(HEAD)\n    print(\"{} Head Generated\".format(HEAD_NAME))\n    print(\"=\" * 60)\n    ```\n  * Define and initialize loss function:\n    ```python\n    LOSS_DICT = {'Focal': FocalLoss(), \n                 'Softmax': nn.CrossEntropyLoss()}\n    LOSS = LOSS_DICT[LOSS_NAME]\n    print(\"=\" * 60)\n    print(LOSS)\n    print(\"{} Loss Generated\".format(LOSS_NAME))\n    print(\"=\" * 60)\n    ```\n  * Define and initialize optimizer:\n    ```python\n    if BACKBONE_NAME.find(\"IR\") >= 0:\n        backbone_paras_only_bn, backbone_paras_wo_bn = separate_irse_bn_paras(BACKBONE) # separate batch_norm parameters from others; do not do weight decay for batch_norm parameters to improve the generalizability\n        _, head_paras_wo_bn = separate_irse_bn_paras(HEAD)\n    else:\n        backbone_paras_only_bn, backbone_paras_wo_bn = separate_resnet_bn_paras(BACKBONE) # separate batch_norm parameters from others; do not do weight decay for batch_norm parameters to improve the generalizability\n        _, head_paras_wo_bn = separate_resnet_bn_paras(HEAD)\n    OPTIMIZER = optim.SGD([{'params': backbone_paras_wo_bn + head_paras_wo_bn, 'weight_decay': WEIGHT_DECAY}, {'params': backbone_paras_only_bn}], lr = LR, momentum = MOMENTUM)\n    print(\"=\" * 60)\n    print(OPTIMIZER)\n    print(\"Optimizer Generated\")\n    print(\"=\" * 60)\n    ```\n  * Whether resume from a checkpoint or not:\n    ```python\n    if BACKBONE_RESUME_ROOT and HEAD_RESUME_ROOT:\n        print(\"=\" * 60)\n        if os.path.isfile(BACKBONE_RESUME_ROOT) and os.path.isfile(HEAD_RESUME_ROOT):\n            print(\"Loading Backbone Checkpoint '{}'\".format(BACKBONE_RESUME_ROOT))\n            BACKBONE.load_state_dict(torch.load(BACKBONE_RESUME_ROOT))\n            print(\"Loading Head Checkpoint '{}'\".format(HEAD_RESUME_ROOT))\n            HEAD.load_state_dict(torch.load(HEAD_RESUME_ROOT))\n        else:\n            print(\"No Checkpoint Found at '{}' and '{}'. Please Have a Check or Continue to Train from Scratch\".format(BACKBONE_RESUME_ROOT, HEAD_RESUME_ROOT))\n        print(\"=\" * 60)\n    ```\n  * Whether use multi-GPU or not:\n    ```python\n    if MULTI_GPU:\n        # multi-GPU setting\n        BACKBONE = nn.DataParallel(BACKBONE, device_ids = GPU_ID)\n        BACKBONE = BACKBONE.to(DEVICE)\n    else:\n        # single-GPU setting\n        BACKBONE = BACKBONE.to(DEVICE)\n    ```\n  * Minor settings prior to training:\n    ```python\n    DISP_FREQ = len(train_loader) // 100 # frequency to display training loss & acc\n\n    NUM_EPOCH_WARM_UP = NUM_EPOCH // 25  # use the first 1/25 epochs to warm up\n    NUM_BATCH_WARM_UP = len(train_loader) * NUM_EPOCH_WARM_UP  # use the first 1/25 epochs to warm up\n    batch = 0  # batch index\n    ```\n  * Training \\& validation \\& save checkpoint (use the first 1/25 epochs to warm up -- gradually increase LR to the initial value to ensure stable convergence):\n    ```python\n    for epoch in range(NUM_EPOCH): # start training process\n        \n        if epoch == STAGES[0]: # adjust LR for each training stage after warm up, you can also choose to adjust LR manually (with slight modification) once plaueau observed\n            schedule_lr(OPTIMIZER)\n        if epoch == STAGES[1]:\n            schedule_lr(OPTIMIZER)\n        if epoch == STAGES[2]:\n            schedule_lr(OPTIMIZER)\n\n        BACKBONE.train()  # set to training mode\n        HEAD.train()\n\n        losses = AverageMeter()\n        top1 = AverageMeter()\n        top5 = AverageMeter()\n\n        for inputs, labels in tqdm(iter(train_loader)):\n\n            if (epoch + 1 <= NUM_EPOCH_WARM_UP) and (batch + 1 <= NUM_BATCH_WARM_UP): # adjust LR for each training batch during warm up\n                warm_up_lr(batch + 1, NUM_BATCH_WARM_UP, LR, OPTIMIZER)\n\n            # compute output\n            inputs = inputs.to(DEVICE)\n            labels = labels.to(DEVICE).long()\n            features = BACKBONE(inputs)\n            outputs = HEAD(features, labels)\n            loss = LOSS(outputs, labels)\n\n            # measure accuracy and record loss\n            prec1, prec5 = accuracy(outputs.data, labels, topk = (1, 5))\n            losses.update(loss.data.item(), inputs.size(0))\n            top1.update(prec1.data.item(), inputs.size(0))\n            top5.update(prec5.data.item(), inputs.size(0))\n\n            # compute gradient and do SGD step\n            OPTIMIZER.zero_grad()\n            loss.backward()\n            OPTIMIZER.step()\n            \n            # dispaly training loss & acc every DISP_FREQ\n            if ((batch + 1) % DISP_FREQ == 0) and batch != 0:\n                print(\"=\" * 60)\n                print('Epoch {}/{} Batch {}/{}\\t'\n                      'Training Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n                      'Training Prec@1 {top1.val:.3f} ({top1.avg:.3f})\\t'\n                      'Training Prec@5 {top5.val:.3f} ({top5.avg:.3f})'.format(\n                    epoch + 1, NUM_EPOCH, batch + 1, len(train_loader) * NUM_EPOCH, loss = losses, top1 = top1, top5 = top5))\n                print(\"=\" * 60)\n\n            batch += 1 # batch index\n\n        # training statistics per epoch (buffer for visualization)\n        epoch_loss = losses.avg\n        epoch_acc = top1.avg\n        writer.add_scalar(\"Training_Loss\", epoch_loss, epoch + 1)\n        writer.add_scalar(\"Training_Accuracy\", epoch_acc, epoch + 1)\n        print(\"=\" * 60)\n        print('Epoch: {}/{}\\t'\n              'Training Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n              'Training Prec@1 {top1.val:.3f} ({top1.avg:.3f})\\t'\n              'Training Prec@5 {top5.val:.3f} ({top5.avg:.3f})'.format(\n            epoch + 1, NUM_EPOCH, loss = losses, top1 = top1, top5 = top5))\n        print(\"=\" * 60)\n\n        # perform validation & save checkpoints per epoch\n        # validation statistics per epoch (buffer for visualization)\n        print(\"=\" * 60)\n        print(\"Perform Evaluation on LFW, CFP_FF, CFP_FP, AgeDB, CALFW, CPLFW and VGG2_FP, and Save Checkpoints...\")\n        accuracy_lfw, best_threshold_lfw, roc_curve_lfw = perform_val(MULTI_GPU, DEVICE, EMBEDDING_SIZE, BATCH_SIZE, BACKBONE, lfw, lfw_issame)\n        buffer_val(writer, \"LFW\", accuracy_lfw, best_threshold_lfw, roc_curve_lfw, epoch + 1)\n        accuracy_cfp_ff, best_threshold_cfp_ff, roc_curve_cfp_ff = perform_val(MULTI_GPU, DEVICE, EMBEDDING_SIZE, BATCH_SIZE, BACKBONE, cfp_ff, cfp_ff_issame)\n        buffer_val(writer, \"CFP_FF\", accuracy_cfp_ff, best_threshold_cfp_ff, roc_curve_cfp_ff, epoch + 1)\n        accuracy_cfp_fp, best_threshold_cfp_fp, roc_curve_cfp_fp = perform_val(MULTI_GPU, DEVICE, EMBEDDING_SIZE, BATCH_SIZE, BACKBONE, cfp_fp, cfp_fp_issame)\n        buffer_val(writer, \"CFP_FP\", accuracy_cfp_fp, best_threshold_cfp_fp, roc_curve_cfp_fp, epoch + 1)\n        accuracy_agedb, best_threshold_agedb, roc_curve_agedb = perform_val(MULTI_GPU, DEVICE, EMBEDDING_SIZE, BATCH_SIZE, BACKBONE, agedb, agedb_issame)\n        buffer_val(writer, \"AgeDB\", accuracy_agedb, best_threshold_agedb, roc_curve_agedb, epoch + 1)\n        accuracy_calfw, best_threshold_calfw, roc_curve_calfw = perform_val(MULTI_GPU, DEVICE, EMBEDDING_SIZE, BATCH_SIZE, BACKBONE, calfw, calfw_issame)\n        buffer_val(writer, \"CALFW\", accuracy_calfw, best_threshold_calfw, roc_curve_calfw, epoch + 1)\n        accuracy_cplfw, best_threshold_cplfw, roc_curve_cplfw = perform_val(MULTI_GPU, DEVICE, EMBEDDING_SIZE, BATCH_SIZE, BACKBONE, cplfw, cplfw_issame)\n        buffer_val(writer, \"CPLFW\", accuracy_cplfw, best_threshold_cplfw, roc_curve_cplfw, epoch + 1)\n        accuracy_vgg2_fp, best_threshold_vgg2_fp, roc_curve_vgg2_fp = perform_val(MULTI_GPU, DEVICE, EMBEDDING_SIZE, BATCH_SIZE, BACKBONE, vgg2_fp, vgg2_fp_issame)\n        buffer_val(writer, \"VGGFace2_FP\", accuracy_vgg2_fp, best_threshold_vgg2_fp, roc_curve_vgg2_fp, epoch + 1)\n        print(\"Epoch {}/{}, Evaluation: LFW Acc: {}, CFP_FF Acc: {}, CFP_FP Acc: {}, AgeDB Acc: {}, CALFW Acc: {}, CPLFW Acc: {}, VGG2_FP Acc: {}\".format(epoch + 1, NUM_EPOCH, accuracy_lfw, accuracy_cfp_ff, accuracy_cfp_fp, accuracy_agedb, accuracy_calfw, accuracy_cplfw, accuracy_vgg2_fp))\n        print(\"=\" * 60)\n\n        # save checkpoints per epoch\n        if MULTI_GPU:\n            torch.save(BACKBONE.module.state_dict(), os.path.join(MODEL_ROOT, \"Backbone_{}_Epoch_{}_Batch_{}_Time_{}_checkpoint.pth\".format(BACKBONE_NAME, epoch + 1, batch, get_time())))\n            torch.save(HEAD.state_dict(), os.path.join(MODEL_ROOT, \"Head_{}_Epoch_{}_Batch_{}_Time_{}_checkpoint.pth\".format(HEAD_NAME, epoch + 1, batch, get_time())))\n        else:\n            torch.save(BACKBONE.state_dict(), os.path.join(MODEL_ROOT, \"Backbone_{}_Epoch_{}_Batch_{}_Time_{}_checkpoint.pth\".format(BACKBONE_NAME, epoch + 1, batch, get_time())))\n            torch.save(HEAD.state_dict(), os.path.join(MODEL_ROOT, \"Head_{}_Epoch_{}_Batch_{}_Time_{}_checkpoint.pth\".format(HEAD_NAME, epoch + 1, batch, get_time())))\n    ```\n* Now, you can start to play with [face.evoLVe](#Introduction) and run ```train.py```. User friendly information will popped out on your terminal:\n  * About overall configuration:\n  \n    <img src=\"https://github.com/ZhaoJ9014/face.evoLVe/blob/master/disp/Fig4.png\" width=\"900px\"/>\n  \n  * About number of training classes:\n  \n    <img src=\"https://github.com/ZhaoJ9014/face.evoLVe/blob/master/disp/Fig5.png\" width=\"400px\"/>\n  \n  * About backbone details:\n  \n    <img src=\"https://github.com/ZhaoJ9014/face.evoLVe/blob/master/disp/Fig6.png\" width=\"900px\"/>\n  \n  * About head details:\n  \n    <img src=\"https://github.com/ZhaoJ9014/face.evoLVe/blob/master/disp/Fig7.png\" width=\"400px\"/>\n  \n  * About loss details:\n  \n    <img src=\"https://github.com/ZhaoJ9014/face.evoLVe/blob/master/disp/Fig8.png\" width=\"400px\"/>\n  \n  * About optimizer details:\n  \n    <img src=\"https://github.com/ZhaoJ9014/face.evoLVe/blob/master/disp/Fig9.png\" width=\"400px\"/>\n    \n  * About resume training:\n  \n    <img src=\"https://github.com/ZhaoJ9014/face.evoLVe/blob/master/disp/Fig10.png\" width=\"400px\"/>\n  \n  * About training status \\& statistics (when batch index reachs ```DISP_FREQ``` or at the end of each epoch):\n  \n    <img src=\"https://github.com/ZhaoJ9014/face.evoLVe/blob/master/disp/Fig11.png\" width=\"900px\"/>\n  \n  * About validation statistics \\& save checkpoints (at the end of each epoch):\n  \n    <img src=\"https://github.com/ZhaoJ9014/face.evoLVe/blob/master/disp/Fig12.png\" width=\"900px\"/>\n    \n* Monitor on-the-fly GPU occupancy with ```watch -d -n 0.01 nvidia-smi```.\n* Please refer to Sec. [Model Zoo](#Model-Zoo) for specific model weights and corresponding performance.\n* Feature extraction API (extract features from pre-trained models) ```./util/extract_feature_v1.py``` (implemented with PyTorch build-in functions) and ```./util/extract_feature_v2.py``` (implemented with OpenCV).\n* Visualize training \\& validation statistics with tensorboardX (see Sec. [Model Zoo](#Model-Zoo)):\n  ```\n  tensorboard --logdir /media/pc/6T/jasonjzhao/buffer/log\n  ```\n  \n****\n### Data Zoo \n:tiger:\n\n|Database|Version|\\#Identity|\\#Image|\\#Frame|\\#Video|Download Link|\n|:---:|:----:|:-----:|:-----:|:-----:|:-----:|:-----:|\n|[LFW](https://hal.inria.fr/file/index/docid/321923/filename/Huang_long_eccv2008-lfw.pdf)|Raw|5,749|13,233|-|-|[Google Drive](https://drive.google.com/file/d/1JIgAXYqXrH-RbUvcsB3B6LXctLU9ijBA/view?usp=sharing), [Baidu Drive](https://pan.baidu.com/s/1VzSI_xqiBw-uHKyRbi6zzw)|\n|[LFW](https://hal.inria.fr/file/index/docid/321923/filename/Huang_long_eccv2008-lfw.pdf)|Align_250x250|5,749|13,233|-|-|[Google Drive](https://drive.google.com/file/d/11h-QIrhuszY3PzT17Q5eXw8yrewgqX7m/view?usp=sharing), [Baidu Drive](https://pan.baidu.com/s/1Ir8kAcQjBJA6A_pWPL9ozQ)|\n|[LFW](https://hal.inria.fr/file/index/docid/321923/filename/Huang_long_eccv2008-lfw.pdf)|Align_112x112|5,749|13,233|-|-|[Google Drive](https://drive.google.com/file/d/1WO5Meh_yAau00Gm2Rz2Pc0SRldLQYigT/view?usp=sharing), [Baidu Drive](https://pan.baidu.com/s/1Ew5JZ266bkg00jB5ICt78g)|\n|[CALFW](https://arxiv.org/pdf/1708.08197.pdf)|Raw|4,025|12,174|-|-|[Google Drive](https://drive.google.com/file/d/1LcIDIfeZ027tbyUJDbaDt12ZoMVJuoMp/view?usp=sharing), [Baidu Drive](https://pan.baidu.com/s/17IzL_nGzedup1gcPuob0NQ)|\n|[CALFW](https://arxiv.org/pdf/1708.08197.pdf)|Align_112x112|4,025|12,174|-|-|[Google Drive](https://drive.google.com/file/d/1kpmcDeDmPqUcI5uX0MCBzpP_8oQVojzW/view?usp=sharing), [Baidu Drive](https://pan.baidu.com/s/1IxqyLFfHNQaj3ibjc7Vcvg)|\n|[CPLFW](http://www.whdeng.cn/CPLFW/Cross-Pose-LFW.pdf)|Raw|3,884|11,652|-|-|[Google Drive](https://drive.google.com/file/d/1WipxZ1QXs_Fi6Y5qEFDayEgos3rHDRnS/view?usp=sharing), [Baidu Drive](https://pan.baidu.com/s/1gJuZZcm-2crTrqKI0sa5sA)|\n|[CPLFW](http://www.whdeng.cn/CPLFW/Cross-Pose-LFW.pdf)|Align_112x112|3,884|11,652|-|-|[Google Drive](https://drive.google.com/file/d/14vPvDngGzsc94pQ4nRNfuBTxdv7YVn2Q/view?usp=sharing), [Baidu Drive](https://pan.baidu.com/s/1uqK2LAEE91HYqllgsWcj9A)|\n|[CASIA-WebFace](https://arxiv.org/pdf/1411.7923.pdf)|Raw_v1|10,575|494,414|-|-|[Baidu Drive](https://pan.baidu.com/s/1xh073sKX3IYp9xPm9S6F5Q)|\n|[CASIA-WebFace](https://arxiv.org/pdf/1411.7923.pdf)|Raw_v2|10,575|494,414|-|-|[Google Drive](https://drive.google.com/file/d/19R6Svdj5HbUA0y6aJv3P1WkIR5wXeCnO/view?usp=sharing), [Baidu Drive](https://pan.baidu.com/s/1cZqsRxln-JmrA4xevLfjYQ)|\n|[CASIA-WebFace](https://arxiv.org/pdf/1411.7923.pdf)|Clean|10,575|455,594|-|-|[Google Drive](https://drive.google.com/file/d/1wJC2aPA4AC0rI-tAL2BFs2M8vfcpX-w6/view?usp=sharing), [Baidu Drive](https://pan.baidu.com/s/1x_VJlG9WV1OdrrJ7ARUZQw)|\n|[MS-Celeb-1M](https://arxiv.org/pdf/1607.08221.pdf)|Clean|100,000|5,084,127|-|-|[Google Drive](https://drive.google.com/file/d/18FxgfXgKwuYzY3DmWJXNJuY51TPmC9yH/view?usp=sharing)|\n|[MS-Celeb-1M](https://arxiv.org/pdf/1607.08221.pdf)|Align_112x112|85,742|5,822,653|-|-|[Google Drive](https://drive.google.com/file/d/1X202mvYe5tiXFhOx82z4rPiPogXD435i/view?usp=sharing)|\n|[Vggface2](https://arxiv.org/pdf/1710.08092.pdf)|Clean|8,631|3,086,894|-|-|[Google Drive](https://drive.google.com/file/d/1jdZw6ZmB7JRK6RS6QP3YEr2sufJ5ibtO/view?usp=sharing)|\n|[Vggface2_FP](https://arxiv.org/pdf/1710.08092.pdf)|Align_112x112|-|-|-|-|[Google Drive](https://drive.google.com/file/d/1N7QEEQZPJ2s5Hs34urjseFwIoPVSmn4r/view?usp=sharing), [Baidu Drive](https://pan.baidu.com/s/1STSgORPyRT-eyk5seUTcRA)|\n|[AgeDB](http://openaccess.thecvf.com/content_cvpr_2017_workshops/w33/papers/Moschoglou_AgeDB_The_First_CVPR_2017_paper.pdf)|Raw|570|16,488|-|-|[Google Drive](https://drive.google.com/file/d/1FoZDyzTrs8r_oFM3Xqmi3iAHsnoirTRA/view?usp=sharing), [Baidu Drive](https://pan.baidu.com/s/1-E_hkW-bXsXNYRiAhRPM7A)|\n|[AgeDB](http://openaccess.thecvf.com/content_cvpr_2017_workshops/w33/papers/Moschoglou_AgeDB_The_First_CVPR_2017_paper.pdf)|Align_112x112|570|16,488|-|-|[Google Drive](https://drive.google.com/file/d/1AoZrZfym5ZhdTyKSxD0qxa7Xrp2Q1ftp/view?usp=sharing), [Baidu Drive](https://pan.baidu.com/s/1ehwmQ4M7WpLylV83uUBxiA)|\n|[IJB-A](https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Klare_Pushing_the_Frontiers_2015_CVPR_paper.pdf)|Clean|500|5,396|20,369|2,085|[Google Drive](https://drive.google.com/file/d/1WdQ62XJuvw0_K4MUP5nXOhv2RsEBVB1f/view?usp=sharing), [Baidu Drive](https://pan.baidu.com/s/1iN68cdiPO0bTTN_hwmbe9w)|\n|[IJB-B](http://openaccess.thecvf.com/content_cvpr_2017_workshops/w6/papers/Whitelam_IARPA_Janus_Benchmark-B_CVPR_2017_paper.pdf)|Raw|1,845|21,798|55,026|7,011|[Google Drive](https://drive.google.com/file/d/15oibCHL3NX-q-QV8q_UAmbIr9e_M0n1R/view?usp=sharing)|\n|[CFP](http://www.cfpw.io/paper.pdf)|Raw|500|7,000|-|-|[Google Drive](https://drive.google.com/file/d/1tGNtqzWeUx3BYAxRHBbH1Wy7AmyFtZkU/view?usp=sharing), [Baidu Drive](https://pan.baidu.com/s/10Qq64LO_RWKD2cr_D32_6A)|\n|[CFP](http://www.cfpw.io/paper.pdf)|Align_112x112|500|7,000|-|-|[Google Drive](https://drive.google.com/file/d/1-sDn79lTegXRNhFuRnIRsgdU88cBfW6V/view?usp=sharing), [Baidu Drive](https://pan.baidu.com/s/1DpudKyw_XN1Y491n1f-DtA)|\n|[Umdfaces](https://arxiv.org/pdf/1611.01484.pdf)|Align_112x112|8,277|367,888|-|-|[Google Drive](https://drive.google.com/file/d/13IDdIMqPCd8h1vWOYBkW6T5bjAxwmxm5/view?usp=sharing), [Baidu Drive](https://pan.baidu.com/s/1UzrBMguV5YLh8aawIodKeQ)|\n|[CelebA](https://arxiv.org/pdf/1411.7766.pdf)|Raw|10,177|202,599|-|-|[Google Drive](https://drive.google.com/file/d/1FO_p759JtKOf3qOnxOGpmoxCcnKiPdBI/view?usp=sharing), [Baidu Drive](https://pan.baidu.com/s/1DfvDKKEB11MrZcf7hPjJfw)|\n|[CACD-VS](http://cmlab.csie.ntu.edu.tw/~sirius42/papers/chen14eccv.pdf)|Raw|2,000|163,446|-|-|[Google Drive](https://drive.google.com/file/d/1syrMyJGeXYxbjbmWKLxo1ASzpj2DRrk3/view?usp=sharing), [Baidu Drive](https://pan.baidu.com/s/13XI67Zn_D_Kncp_9hlTbQQ)|\n|[YTF](http://www.cs.tau.ac.il/~wolf/ytfaces/WolfHassnerMaoz_CVPR11.pdf)|Align_344x344|1,595|-|3,425|621,127|[Google Drive](https://drive.google.com/file/d/1o_5b7rYcSEFvTmwmEh0eCPsU5kFmKN_Y/view?usp=sharing), [Baidu Drive](https://pan.baidu.com/s/1M43AcijgGrurb0dfFVlDKQ)|\n|[DeepGlint](http://trillionpairs.deepglint.com)|Align_112x112|180,855|6,753,545|-|-|[Google Drive](https://drive.google.com/file/d/1Lqvh24913uquWxa3YS_APluEmbNKQ4Us/view?usp=sharing)|\n|[UTKFace](https://susanqq.github.io/UTKFace/)|Align_200x200|-|23,708|-|-|[Google Drive](https://drive.google.com/file/d/1T5KH-DWXu048im0xBuRK0WEi820T28B-/view?usp=sharing), [Baidu Drive](https://pan.baidu.com/s/12Qp5pdZvitqBYSJHm4ouOw)|\n|[BUAA-VisNir](http://irip.buaa.edu.cn/system/_content/download.jsp?urltype=news.DownloadAttachUrl&owner=1240132834&wbfileid=1277824)|Align_287x287|150|5,952|-|-|[Baidu Drive](https://pan.baidu.com/s/1XcqgcOzYsFZ8THEXg4nwVw), PW: xmbc|\n|[CASIA NIR-VIS 2.0](https://www.cv-foundation.org/openaccess/content_cvpr_workshops_2013/W13/papers/Li_The_CASIA_NIR-VIS_2013_CVPR_paper.pdf)|Align_128x128|725|17,580|-|-|[Baidu Drive](https://pan.baidu.com/s/1MZwONRsPmKTcE1xq6bdDFA), PW: 883b|\n|[Oulu-CASIA](http://www.ee.oulu.fi/~gyzhao/Download/Databases/NIR_VL_FED/Description.pdf)|Raw|80|65,000|-|-|[Baidu Drive](https://pan.baidu.com/s/1HzsmNvA2xvJA-XW8nGKK1A), PW: xxp5|\n|[NUAA-ImposterDB](http://parnec.nuaa.edu.cn/xtan/paper/eccv10r1.pdf)|Raw|15|12,614|-|-|[Baidu Drive](https://pan.baidu.com/s/1WeSvoencoyGIi7SKygnEWw), PW: if3n|\n|[CASIA-SURF](https://arxiv.org/pdf/1812.00408.pdf)|Raw|1,000|-|-|21,000|[Baidu Drive](https://pan.baidu.com/s/1dTGo9xcdTuK54RBgBWJNQg), PW: izb3|\n|[CASIA-FASD](http://www.cbsr.ia.ac.cn/users/zlei/papers/ICB2012/ZHANG-ICB2012.pdf)|Raw|50|-|-|600|[Baidu Drive](https://pan.baidu.com/s/15HyX7tizCCuwN9BKiV9_zA), PW: h5un|\n|[CASIA-MFSD](http://biometrics.cse.msu.edu/Publications/Databases/MSUMobileFaceSpoofing/index.htm)|Raw|50|-|-|600| |\n|[Replay-Attack](https://publications.idiap.ch/downloads/papers/2012/Chingovska_IEEEBIOSIG2012_2012.pdf)|Raw|50|-|-|1,200| |\n|[WebFace260M](https://arxiv.org/abs/2103.04098)|Raw|24M|2M|-||https://www.face-benchmark.org/|\n* Remark: unzip [CASIA-WebFace](https://arxiv.org/pdf/1411.7923.pdf) clean version with \n  ```\n  unzip casia-maxpy-clean.zip    \n  cd casia-maxpy-clean    \n  zip -F CASIA-maxpy-clean.zip --out CASIA-maxpy-clean_fix.zip    \n  unzip CASIA-maxpy-clean_fix.zip\n  ```\n* Remark: after unzip, get image data \\& pair ground truths from [AgeDB](http://openaccess.thecvf.com/content_cvpr_2017_workshops/w33/papers/Moschoglou_AgeDB_The_First_CVPR_2017_paper.pdf), [CFP](http://www.cfpw.io/paper.pdf), [LFW](https://hal.inria.fr/file/index/docid/321923/filename/Huang_long_eccv2008-lfw.pdf) and [VGGFace2_FP](https://arxiv.org/pdf/1710.08092.pdf) align_112x112 versions with \n  ```python\n  import numpy as np\n  import bcolz\n  import os\n\n  def get_pair(root, name):\n      carray = bcolz.carray(rootdir = os.path.join(root, name), mode='r')\n      issame = np.load('{}/{}_list.npy'.format(root, name))\n      return carray, issame\n\n  def get_data(data_root):\n      agedb_30, agedb_30_issame = get_pair(data_root, 'agedb_30')\n      cfp_fp, cfp_fp_issame = get_pair(data_root, 'cfp_fp')\n      lfw, lfw_issame = get_pair(data_root, 'lfw')\n      vgg2_fp, vgg2_fp_issame = get_pair(data_root, 'vgg2_fp')\n      return agedb_30, cfp_fp, lfw, vgg2_fp, agedb_30_issame, cfp_fp_issame, lfw_issame, vgg2_fp_issame\n\n  agedb_30, cfp_fp, lfw, vgg2_fp, agedb_30_issame, cfp_fp_issame, lfw_issame, vgg2_fp_issame = get_data(DATA_ROOT)\n  ```\n* Remark: We share ```MS-Celeb-1M_Top1M_MID2Name.tsv``` ([Google Drive](https://drive.google.com/file/d/15X_mIcmcC38KjHA2NAGUIsNXF_iUeMbX/view?usp=sharing), [Baidu Drive](https://pan.baidu.com/s/1AyZBr_Iow1StS3OzWedT1A)), ```VGGface2_ID2Name.csv``` ([Google Drive](https://drive.google.com/file/d/1tSMrzwkWMCuOycNIjpx9GC3P2Pr1oPOU/view?usp=sharing), [Baidu Drive](https://pan.baidu.com/s/1fRJKvgBxTcd4j6fCfmEUOw)), ```VGGface2_FaceScrub_Overlap.txt``` ([Google Drive](https://drive.google.com/file/d/1M9F29t0WvAIJWhsBn5xyl00VL-7wBYkc/view?usp=sharing), [Baidu Drive](https://pan.baidu.com/s/1ppZ2qcMfZ8bXq5Sf5LHijA)), ```VGGface2_LFW_Overlap.txt``` ([Google Drive](https://drive.google.com/file/d/13MO7su1z0G_Aqc5HwzImBxctORJjyDlO/view?usp=sharing), [Baidu Drive](https://pan.baidu.com/s/1Sl6JIt99oX9G9YwP4HVq2A)), ```CASIA-WebFace_ID2Name.txt``` ([Google Drive](https://drive.google.com/file/d/1Unqo5E5JR2tSNK0g7KhC6uwYM3tTXXVu/view?usp=sharing), [Baidu Drive](https://pan.baidu.com/s/1zlYTOeLRgGPIR-yPeEurRw)), ```CASIA-WebFace_FaceScrub_Overlap.txt``` ([Google Drive](https://drive.google.com/file/d/1xHM6JJXv5cl7xmSbZ1mkXyJpXnEgNV4x/view?usp=sharing), [Baidu Drive](https://pan.baidu.com/s/1smuVPG0j7Zikd7UladBSew)), ```CASIA-WebFace_LFW_Overlap.txt``` ([Google Drive](https://drive.google.com/file/d/1blFEbNGEfncAUQKCeCTb__rv221oZo80/view?usp=sharing), [Baidu Drive](https://pan.baidu.com/s/10rwsLZFA25e6cW1gJDDZGQ)), ```FaceScrub_Name.txt``` ([Google Drive](https://drive.google.com/file/d/1R8MofI3pXGAuHsD5wswZXLPBHg8zll25/view?usp=sharing), [Baidu Drive](https://pan.baidu.com/s/12fryVZO6ytpHhjMidvZpxQ)), ```LFW_Name.txt``` ([Google Drive](https://drive.google.com/file/d/1zC-0R3sL_wf2Oq1exMpDvJUGnW0VPcWs/view?usp=sharing), [Baidu Drive](https://pan.baidu.com/s/1OFW8vJajkvTviUMiSNwdXA)), ```LFW_Log.txt``` ([Google Drive](https://drive.google.com/file/d/1afCfVNnguaCaKktsZn8q5CNlqThfeZYk/view?usp=sharing), [Baidu Drive](https://pan.baidu.com/s/1TsQOez_11WcViTV9eo4qOQ)) to help researchers/engineers quickly remove the overlapping parts between their own private datasets and the public datasets.\n* Due to release license issue, for other face related databases, please make contact with us in person for more details.\n\n****\n### Model Zoo \n:monkey:\n\n* Model\n\n  |Backbone|Head|Loss|Training Data|Download Link|\n  |:---:|:---:|:---:|:---:|:---:|\n  |[IR-50](https://arxiv.org/pdf/1512.03385.pdf)|[ArcFace](https://arxiv.org/pdf/1801.07698.pdf)|[Focal](https://arxiv.org/pdf/1708.02002.pdf)|[MS-Celeb-1M_Align_112x112](https://arxiv.org/pdf/1607.08221.pdf)|[Google Drive](https://drive.google.com/drive/folders/1omzvXV_djVIW2A7I09DWMe9JR-9o_MYh?usp=sharing), [Baidu Drive](https://pan.baidu.com/s/1L8yOF1oZf6JHfeY9iN59Mg)|\n\n  * Setting\n    ```\n    INPUT_SIZE: [112, 112]; RGB_MEAN: [0.5, 0.5, 0.5]; RGB_STD: [0.5, 0.5, 0.5]; BATCH_SIZE: 512 (drop the last batch to ensure consistent batch_norm statistics); Initial LR: 0.1; NUM_EPOCH: 120; WEIGHT_DECAY: 5e-4 (do not apply to batch_norm parameters); MOMENTUM: 0.9; STAGES: [30, 60, 90]; Augmentation: Random Crop + Horizontal Flip; Imbalanced Data Processing: Weighted Random Sampling; Solver: SGD; GPUs: 4 NVIDIA Tesla P40 in Parallel\n    ```\n  * Training \\& validation statistics\n  \n    <img src=\"https://github.com/ZhaoJ9014/face.evoLVe/blob/master/disp/Fig13.png\" width=\"1000px\"/>\n      \n  * Performance\n\n    |[LFW](https://hal.inria.fr/file/index/docid/321923/filename/Huang_long_eccv2008-lfw.pdf)|[CFP_FF](http://www.cfpw.io/paper.pdf)|[CFP_FP](http://www.cfpw.io/paper.pdf)|[AgeDB](http://openaccess.thecvf.com/content_cvpr_2017_workshops/w33/papers/Moschoglou_AgeDB_The_First_CVPR_2017_paper.pdf)|[CALFW](https://arxiv.org/pdf/1708.08197.pdf)|[CPLFW](http://www.whdeng.cn/CPLFW/Cross-Pose-LFW.pdf)|[Vggface2_FP](https://arxiv.org/pdf/1710.08092.pdf)|\n    |:---:|:---:|:---:|:---:|:---:|:---:|:---:|\n    |99.78|99.69|98.14|97.53|95.87|92.45|95.22|\n\n* Model\n\n  |Backbone|Head|Loss|Training Data|Download Link|\n  |:---:|:---:|:---:|:---:|:---:|\n  |[IR-50](https://arxiv.org/pdf/1512.03385.pdf)|[ArcFace](https://arxiv.org/pdf/1801.07698.pdf)|[Focal](https://arxiv.org/pdf/1708.02002.pdf)|Private Asia Face Data|[Google Drive](https://drive.google.com/drive/folders/11TI4Gs_lO-fbts7cgWNqvVfm9nps2msE?usp=sharing), [Baidu Drive](https://pan.baidu.com/s/18BSUeA1bpAeRWTprtHgX9w)|\n\n  * Setting\n    ```\n    INPUT_SIZE: [112, 112]; RGB_MEAN: [0.5, 0.5, 0.5]; RGB_STD: [0.5, 0.5, 0.5]; BATCH_SIZE: 1024 (drop the last batch to ensure consistent batch_norm statistics); Initial LR: 0.01 (initialize weights from the above model pre-trained on MS-Celeb-1M_Align_112x112); NUM_EPOCH: 80; WEIGHT_DECAY: 5e-4 (do not apply to batch_norm parameters); MOMENTUM: 0.9; STAGES: [20, 40, 60]; Augmentation: Random Crop + Horizontal Flip; Imbalanced Data Processing: Weighted Random Sampling; Solver: SGD; GPUs: 8 NVIDIA Tesla P40 in Parallel\n    ```\n\n  * Performance (please perform evaluation on your own Asia face benchmark dataset)\n  \n* Model\n\n  |Backbone|Head|Loss|Training Data|Download Link|\n  |:---:|:---:|:---:|:---:|:---:|\n  |[IR-152](https://arxiv.org/pdf/1512.03385.pdf)|[ArcFace](https://arxiv.org/pdf/1801.07698.pdf)|[Focal](https://arxiv.org/pdf/1708.02002.pdf)|[MS-Celeb-1M_Align_112x112](https://arxiv.org/pdf/1607.08221.pdf)|[Baidu Drive](https://pan.baidu.com/s/1-9sFB3H1mL8bt2jH7EagtA), PW: b197|\n\n  * Setting\n    ```\n    INPUT_SIZE: [112, 112]; RGB_MEAN: [0.5, 0.5, 0.5]; RGB_STD: [0.5, 0.5, 0.5]; BATCH_SIZE: 256 (drop the last batch to ensure consistent batch_norm statistics); Initial LR: 0.01; NUM_EPOCH: 120; WEIGHT_DECAY: 5e-4 (do not apply to batch_norm parameters); MOMENTUM: 0.9; STAGES: [30, 60, 90]; Augmentation: Random Crop + Horizontal Flip; Imbalanced Data Processing: Weighted Random Sampling; Solver: SGD; GPUs: 4 NVIDIA Geforce RTX 2080 Ti in Parallel\n    ```\n  * Training \\& validation statistics\n  \n    <img src=\"https://github.com/ZhaoJ9014/face.evoLVe/blob/master/disp/Fig14.png\" width=\"1000px\"/>\n      \n  * Performance\n\n    |[LFW](https://hal.inria.fr/file/index/docid/321923/filename/Huang_long_eccv2008-lfw.pdf)|[CFP_FF](http://www.cfpw.io/paper.pdf)|[CFP_FP](http://www.cfpw.io/paper.pdf)|[AgeDB](http://openaccess.thecvf.com/content_cvpr_2017_workshops/w33/papers/Moschoglou_AgeDB_The_First_CVPR_2017_paper.pdf)|[CALFW](https://arxiv.org/pdf/1708.08197.pdf)|[CPLFW](http://www.whdeng.cn/CPLFW/Cross-Pose-LFW.pdf)|[Vggface2_FP](https://arxiv.org/pdf/1710.08092.pdf)|\n    |:---:|:---:|:---:|:---:|:---:|:---:|:---:|\n    |99.82|99.83|98.37|98.07|96.03|93.05|95.50|\n\n****\n### Achievement \n:confetti_ball:\n\n* 2017 No.1 on ICCV 2017 MS-Celeb-1M Large-Scale Face Recognition [Hard Set](https://www.msceleb.org/leaderboard/iccvworkshop-c1)/[Random Set](https://www.msceleb.org/leaderboard/iccvworkshop-c1)/[Low-Shot Learning](https://www.msceleb.org/leaderboard/c2) Challenges. [WeChat News](http://mp.weixin.qq.com/s/-G94Mj-8972i2HtEcIZDpA), [NUS ECE News](http://ece.nus.edu.sg/drupal/?q=node/215), [NUS ECE Poster](https://zhaoj9014.github.io/pub/ECE_Poster.jpeg), [Award Certificate for Track-1](https://zhaoj9014.github.io/pub/MS-Track1.jpeg), [Award Certificate for Track-2](https://zhaoj9014.github.io/pub/MS-Track2.jpeg), [Award Ceremony](https://zhaoj9014.github.io/pub/MS-Awards.jpeg).\n* 2017 No.1 on National Institute of Standards and Technology (NIST) IARPA Janus Benchmark A (IJB-A) Unconstrained Face [Verification](https://zhaoj9014.github.io/pub/IJBA_11_report.pdf) challenge and [Identification](https://zhaoj9014.github.io/pub/IJBA_1N_report.pdf) challenge. [WeChat News](https://mp.weixin.qq.com/s/s9H_OXX-CCakrTAQUFDm8g).\n\n* State-of-the-art performance on \n\n    * MS-Celeb-1M (Challenge1 Hard Set Coverage@P=0.95: 79.10%; Challenge1 Random Set Coverage@P=0.95: 87.50%; Challenge2 Development Set Coverage@P=0.99: 100.00%; Challenge2 Base Set Top 1 Accuracy: 99.74%; Challenge2 Novel Set Coverage@P=0.99: 99.01%).\n    * IJB-A (1:1 Veification TAR@FAR=0.1: 99.6%0.1%; 1:1 Veification TAR@FAR=0.01: 99.1%0.2%; 1:1 Veification TAR@FAR=0.001: 97.9%0.4%; 1:N Identification FNIR@FPIR=0.1: 1.3%0.3%; 1:N Identification FNIR@FPIR=0.01: 5.4%4.7%; 1:N Identification Rank1 Accuracy: 99.2%0.1%; 1:N Identification Rank5 Accuracy: 99.7%0.1%; 1:N Identification Rank10 Accuracy: 99.8%0.1%).\n    * IJB-C (1:1 Veification TAR@FAR=1e-5: 82.6%).\n    * Labeled Faces in the Wild (LFW) (Accuracy: 99.85%0.217%).\n    * Celebrities in Frontal-Profile (CFP) (Frontal-Profile Accuracy: 96.01%0.84%; Frontal-Profile EER: 4.43%1.04%; Frontal-Profile AUC: 99.00%0.35%; Frontal-Frontal Accuracy: 99.64%0.25%; Frontal-Frontal EER: 0.54%0.37%; Frontal-Frontal AUC: 99.98%0.03%).\n    * CMU Multi-PIE (Rank1 Accuracy Setting-1 under 90: 76.12%; Rank1 Accuracy Setting-2 under 90: 86.73%).\n    * MORPH Album2 (Rank1 Accuracy Setting-1: 99.65%; Rank1 Accuracy Setting-2: 99.26%).\n    * CACD-VS (Accuracy: 99.76%).\n    * FG-NET (Rank1 Accuracy: 93.20%).\n\n****\n### Acknowledgement \n:two_men_holding_hands:\n\n* This repo is inspired by [InsightFace.MXNet](https://github.com/deepinsight/insightface), [InsightFace.PyTorch](https://github.com/TreB1eN/InsightFace_Pytorch), [ArcFace.PyTorch](https://github.com/ronghuaiyang/arcface-pytorch), [MTCNN.MXNet](https://github.com/pangyupo/mxnet_mtcnn_face_detection) and [PretrainedModels.PyTorch](https://github.com/Cadene/pretrained-models.pytorch).\n* The work of Jian Zhao was partially supported by China Scholarship Council (CSC) grant 201503170248.\n* We would like to thank [Prof. Jiashi Feng](https://sites.google.com/site/jshfeng/), [Dr. Jianshu Li](https://sites.google.com/view/li-js), Mr. Yu Cheng (Learning and Vision group, National University of Singapore), Mr. Yuan Xin, Mr. Di Wu, Mr. Zhenyuan Shen, Mr. Jianwei Liu (Tencent FiT DeepSea AI Lab, China), [Prof. Ran He](http://www.nlpr.ia.ac.cn/english/irds/People/rhe.html), [Prof. Junliang Xing](http://people.ucas.ac.cn/~0051452?language=en), [Mr. Xiang Wu](http://alfredxiangwu.github.io/) (Institute of Automation, Chinese Academy of Sciences), [Prof. Guosheng Hu](https://www.linkedin.com/in/guosheng-hu-6801b333/) (AnyVision Inc., U.K.), [Dr. Lin Xiong](https://bruinxiong.github.io/xionglin.github.io/) (JD Digits, U.S.), Miss Yi Cheng (Panasonic R\\&D Center, Singapore) for helpful discussions.\n\n\n****\n### Citation \n:bookmark_tabs:\n\n- Please consult and consider citing the following papers:\n\n      @article{wu20223d,\n      title={3D-Guided Frontal Face Generation for Pose-Invariant Recognition},\n      author={Wu, Hao and Gu, Jianyang and Fan, Xiaojin and Li, He and Xie, Lidong and Zhao, Jian},\n      journal={T-IST},\n      year={2022}\n      }\n\n\n      @article{wang2021face,\n      title={Face.evoLVe: A High-Performance Face Recognition Library},\n      author={Wang, Qingzhong and Zhang, Pengfei and Xiong, Haoyi and Zhao, Jian},\n      journal={arXiv preprint arXiv:2107.08621},\n      year={2021}\n      }\n      \n      \n      @article{tu2021joint,\n      title={Joint Face Image Restoration and Frontalization for Recognition},\n      author={Tu, Xiaoguang and Zhao, Jian and Liu, Qiankun and Ai, Wenjie and Guo, Guodong and Li, Zhifeng and Liu, Wei and Feng, Jiashi},\n      journal={T-CSVT},\n      year={2021}\n      }\n\n\n      @article{zhao2020towards,\n      title={Towards age-invariant face recognition},\n      author={Zhao, Jian and Yan, Shuicheng and Feng, Jiashi},\n      journal={T-PAMI},\n      year={2020}\n      }\n\n\n      @article{zhao2019recognizing,\n      title={Recognizing Profile Faces by Imagining Frontal View},\n      author={Zhao, Jian and Xing, Junliang and Xiong, Lin and Yan, Shuicheng and Feng, Jiashi},\n      journal={IJCV},\n      pages={1--19},\n      year={2019}\n      }    \n\n\n      @inproceedings{zhao2019multi,\n      title={Multi-Prototype Networks for Unconstrained Set-based Face Recognition},\n      author={Zhao, Jian and Li, Jianshu and Tu, Xiaoguang and Zhao, Fang and Xin, Yuan and Xing, Junliang and Liu, Hengzhu and Yan, Shuicheng and Feng, Jiashi},\n      booktitle={IJCAI},\n      year={2019}\n      }\n\n\n      @inproceedings{zhao2019look,\n      title={Look Across Elapse: Disentangled Representation Learning and Photorealistic Cross-Age Face Synthesis for Age-Invariant Face Recognition},\n      author={Zhao, Jian and Cheng, Yu and Cheng, Yi and Yang, Yang and Lan, Haochong and Zhao, Fang and Xiong, Lin and Xu, Yan and Li, Jianshu and Pranata, Sugiri and others},\n      booktitle={AAAI},\n      year={2019}\n      }\n      \n      \n      @article{zhao20183d,\n      title={3D-Aided Dual-Agent GANs for Unconstrained Face Recognition},\n      author={Zhao, Jian and Xiong, Lin and Li, Jianshu and Xing, Junliang and Yan, Shuicheng and Feng, Jiashi},\n      journal={T-PAMI},\n      year={2018}\n      }\n      \n      \n      @inproceedings{zhao2018towards,\n      title={Towards Pose Invariant Face Recognition in the Wild},\n      author={Zhao, Jian and Cheng, Yu and Xu, Yan and Xiong, Lin and Li, Jianshu and Zhao, Fang and Jayashree, Karlekar and Pranata,         Sugiri and Shen, Shengmei and Xing, Junliang and others},\n      booktitle={CVPR},\n      pages={2207--2216},\n      year={2018}\n      }\n      \n      \n      @inproceedings{zhao3d,\n      title={3D-Aided Deep Pose-Invariant Face Recognition},\n      author={Zhao, Jian and Xiong, Lin and Cheng, Yu and Cheng, Yi and Li, Jianshu and Zhou, Li and Xu, Yan and Karlekar, Jayashree and       Pranata, Sugiri and Shen, Shengmei and others},\n      booktitle={IJCAI},\n      pages={1184--1190},\n      year={2018}\n      }\n      \n      \n      @inproceedings{zhao2018dynamic,\n      title={Dynamic Conditional Networks for Few-Shot Learning},\n      author={Zhao, Fang and Zhao, Jian and Yan, Shuicheng and Feng, Jiashi},\n      booktitle={ECCV},\n      pages={19--35},\n      year={2018}\n      }\n      \n      \n      @inproceedings{zhao2017dual,\n      title={Dual-agent gans for photorealistic and identity preserving profile face synthesis},\n      author={Zhao, Jian and Xiong, Lin and Jayashree, Panasonic Karlekar and Li, Jianshu and Zhao, Fang and Wang, Zhecan and Pranata,           Panasonic Sugiri and Shen, Panasonic Shengmei and Yan, Shuicheng and Feng, Jiashi},\n      booktitle={NeurIPS},\n      pages={66--76},\n      year={2017}\n      }\n      \n      \n      @inproceedings{zhao122017marginalized,\n      title={Marginalized cnn: Learning deep invariant representations},\n      author={Zhao12, Jian and Li, Jianshu and Zhao, Fang and Yan13, Shuicheng and Feng, Jiashi},\n      booktitle={BMVC},\n      year={2017}\n      }\n      \n      \n      @inproceedings{cheng2017know,\n      title={Know you at one glance: A compact vector representation for low-shot learning},\n      author={Cheng, Yu and Zhao, Jian and Wang, Zhecan and Xu, Yan and Jayashree, Karlekar and Shen, Shengmei and Feng, Jiashi},\n      booktitle={ICCVW},\n      pages={1924--1932},\n      year={2017}\n      }\n      \n      \n      @inproceedings{wangconditional,\n      title={Conditional Dual-Agent GANs for Photorealistic and Annotation Preserving Image Synthesis},\n      author={Wang, Zhecan and Zhao, Jian and Cheng, Yu and Xiao, Shengtao and Li, Jianshu and Zhao, Fang and Feng, Jiashi and Kassim, Ashraf},\n      booktitle={BMVCW},\n      }\n"
        },
        {
          "name": "applications",
          "type": "tree",
          "content": null
        },
        {
          "name": "backbone",
          "type": "tree",
          "content": null
        },
        {
          "name": "backup",
          "type": "tree",
          "content": null
        },
        {
          "name": "balance",
          "type": "tree",
          "content": null
        },
        {
          "name": "config.py",
          "type": "blob",
          "size": 1.91796875,
          "content": "import torch\n\n\nconfigurations = {\n    1: dict(\n        SEED = 1337, # random seed for reproduce results\n\n        DATA_ROOT = '/home/peter/Project/face.evoLVe.PyTorch/data', # the parent root where your train/val/test data are stored\n        MODEL_ROOT = '/home/peter/Project/face.evoLVe.PyTorch/model', # the root to buffer your checkpoints\n        LOG_ROOT = '/home/peter/Project/face.evoLVe.PyTorch/log', # the root to log your train/val status\n        BACKBONE_RESUME_ROOT = './', # the root to resume training from a saved checkpoint\n        HEAD_RESUME_ROOT = './', # the root to resume training from a saved checkpoint\n\n        BACKBONE_NAME = 'IR_SE_50', # support: ['ResNet_50', 'ResNet_101', 'ResNet_152', 'IR_50', 'IR_101', 'IR_152', 'IR_SE_50', 'IR_SE_101', 'IR_SE_152']\n        HEAD_NAME = 'ArcFace', # support:  ['Softmax', 'ArcFace', 'CosFace', 'SphereFace', 'Am_softmax']\n        LOSS_NAME = 'Focal', # support: ['Focal', 'Softmax']\n\n        INPUT_SIZE = [112, 112], # support: [112, 112] and [224, 224]\n        RGB_MEAN = [0.5, 0.5, 0.5], # for normalize inputs to [-1, 1]\n        RGB_STD = [0.5, 0.5, 0.5],\n        EMBEDDING_SIZE = 512, # feature dimension\n        BATCH_SIZE = 512,\n        DROP_LAST = True, # whether drop the last batch to ensure consistent batch_norm statistics\n        LR = 0.1, # initial LR\n        NUM_EPOCH = 125, # total epoch number (use the firt 1/25 epochs to warm up)\n        WEIGHT_DECAY = 5e-4, # do not apply to batch_norm parameters\n        MOMENTUM = 0.9,\n        STAGES = [35, 65, 95], # epoch stages to decay learning rate\n\n        DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\"),\n        MULTI_GPU = True, # flag to use multiple GPUs; if you choose to train with single GPU, you should first run \"export CUDA_VISILE_DEVICES=device_id\" to specify the GPU card you want to use\n        GPU_ID = [0, 1, 2, 3], # specify your GPU ids\n        PIN_MEMORY = True,\n        NUM_WORKERS = 0,\n),\n}\n"
        },
        {
          "name": "data_processing",
          "type": "tree",
          "content": null
        },
        {
          "name": "disp",
          "type": "tree",
          "content": null
        },
        {
          "name": "head",
          "type": "tree",
          "content": null
        },
        {
          "name": "loss",
          "type": "tree",
          "content": null
        },
        {
          "name": "paddle",
          "type": "tree",
          "content": null
        },
        {
          "name": "train.py",
          "type": "blob",
          "size": 13.7509765625,
          "content": "import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision.transforms as transforms\nimport torchvision.datasets as datasets\nfrom config import configurations\nfrom backbone.model_resnet import ResNet_50, ResNet_101, ResNet_152\nfrom backbone.model_irse import IR_50, IR_101, IR_152, IR_SE_50, IR_SE_101, IR_SE_152\nfrom head.metrics import ArcFace, CosFace, SphereFace, Am_softmax\nfrom loss.focal import FocalLoss\nfrom util.utils import make_weights_for_balanced_classes, get_val_data, separate_irse_bn_paras, separate_resnet_bn_paras, warm_up_lr, schedule_lr, perform_val, get_time, buffer_val, AverageMeter, accuracy\n\nfrom tensorboardX import SummaryWriter\nfrom tqdm import tqdm\nimport os\n\n\nif __name__ == '__main__':\n\n    #======= hyperparameters & data loaders =======#\n    cfg = configurations[1]\n\n    SEED = cfg['SEED'] # random seed for reproduce results\n    torch.manual_seed(SEED)\n\n    DATA_ROOT = cfg['DATA_ROOT'] # the parent root where your train/val/test data are stored\n    MODEL_ROOT = cfg['MODEL_ROOT'] # the root to buffer your checkpoints\n    LOG_ROOT = cfg['LOG_ROOT'] # the root to log your train/val status\n    BACKBONE_RESUME_ROOT = cfg['BACKBONE_RESUME_ROOT'] # the root to resume training from a saved checkpoint\n    HEAD_RESUME_ROOT = cfg['HEAD_RESUME_ROOT']  # the root to resume training from a saved checkpoint\n\n    BACKBONE_NAME = cfg['BACKBONE_NAME'] # support: ['ResNet_50', 'ResNet_101', 'ResNet_152', 'IR_50', 'IR_101', 'IR_152', 'IR_SE_50', 'IR_SE_101', 'IR_SE_152']\n    HEAD_NAME = cfg['HEAD_NAME'] # support:  ['Softmax', 'ArcFace', 'CosFace', 'SphereFace', 'Am_softmax']\n    LOSS_NAME = cfg['LOSS_NAME'] # support: ['Focal', 'Softmax']\n\n    INPUT_SIZE = cfg['INPUT_SIZE']\n    RGB_MEAN = cfg['RGB_MEAN'] # for normalize inputs\n    RGB_STD = cfg['RGB_STD']\n    EMBEDDING_SIZE = cfg['EMBEDDING_SIZE'] # feature dimension\n    BATCH_SIZE = cfg['BATCH_SIZE']\n    DROP_LAST = cfg['DROP_LAST'] # whether drop the last batch to ensure consistent batch_norm statistics\n    LR = cfg['LR'] # initial LR\n    NUM_EPOCH = cfg['NUM_EPOCH']\n    WEIGHT_DECAY = cfg['WEIGHT_DECAY']\n    MOMENTUM = cfg['MOMENTUM']\n    STAGES = cfg['STAGES'] # epoch stages to decay learning rate\n\n    DEVICE = cfg['DEVICE']\n    MULTI_GPU = cfg['MULTI_GPU'] # flag to use multiple GPUs\n    GPU_ID = cfg['GPU_ID'] # specify your GPU ids\n    PIN_MEMORY = cfg['PIN_MEMORY']\n    NUM_WORKERS = cfg['NUM_WORKERS']\n    print(\"=\" * 60)\n    print(\"Overall Configurations:\")\n    print(cfg)\n    print(\"=\" * 60)\n\n    writer = SummaryWriter(LOG_ROOT) # writer for buffering intermedium results\n\n    train_transform = transforms.Compose([ # refer to https://pytorch.org/docs/stable/torchvision/transforms.html for more build-in online data augmentation\n        transforms.Resize([int(128 * INPUT_SIZE[0] / 112), int(128 * INPUT_SIZE[0] / 112)]), # smaller side resized\n        transforms.RandomCrop([INPUT_SIZE[0], INPUT_SIZE[1]]),\n        transforms.RandomHorizontalFlip(),\n        transforms.ToTensor(),\n        transforms.Normalize(mean = RGB_MEAN,\n                             std = RGB_STD),\n    ])\n\n    dataset_train = datasets.ImageFolder(os.path.join(DATA_ROOT, 'imgs'), train_transform)\n\n    # create a weighted random sampler to process imbalanced data\n    weights = make_weights_for_balanced_classes(dataset_train.imgs, len(dataset_train.classes))\n    weights = torch.DoubleTensor(weights)\n    sampler = torch.utils.data.sampler.WeightedRandomSampler(weights, len(weights))\n\n    train_loader = torch.utils.data.DataLoader(\n        dataset_train, batch_size = BATCH_SIZE, sampler = sampler, pin_memory = PIN_MEMORY,\n        num_workers = NUM_WORKERS, drop_last = DROP_LAST\n    )\n\n    NUM_CLASS = len(train_loader.dataset.classes)\n    print(\"Number of Training Classes: {}\".format(NUM_CLASS))\n\n    lfw, cfp_ff, cfp_fp, agedb, calfw, cplfw, vgg2_fp, lfw_issame, cfp_ff_issame, cfp_fp_issame, agedb_issame, calfw_issame, cplfw_issame, vgg2_fp_issame = get_val_data(DATA_ROOT)\n\n\n    #======= model & loss & optimizer =======#\n    BACKBONE_DICT = {'ResNet_50': ResNet_50(INPUT_SIZE), \n                     'ResNet_101': ResNet_101(INPUT_SIZE), \n                     'ResNet_152': ResNet_152(INPUT_SIZE),\n                     'IR_50': IR_50(INPUT_SIZE), \n                     'IR_101': IR_101(INPUT_SIZE), \n                     'IR_152': IR_152(INPUT_SIZE),\n                     'IR_SE_50': IR_SE_50(INPUT_SIZE), \n                     'IR_SE_101': IR_SE_101(INPUT_SIZE), \n                     'IR_SE_152': IR_SE_152(INPUT_SIZE)}\n    BACKBONE = BACKBONE_DICT[BACKBONE_NAME]\n    print(\"=\" * 60)\n    print(BACKBONE)\n    print(\"{} Backbone Generated\".format(BACKBONE_NAME))\n    print(\"=\" * 60)\n\n    HEAD_DICT = {'ArcFace': ArcFace(in_features = EMBEDDING_SIZE, out_features = NUM_CLASS, device_id = GPU_ID),\n                 'CosFace': CosFace(in_features = EMBEDDING_SIZE, out_features = NUM_CLASS, device_id = GPU_ID),\n                 'SphereFace': SphereFace(in_features = EMBEDDING_SIZE, out_features = NUM_CLASS, device_id = GPU_ID),\n                 'Am_softmax': Am_softmax(in_features = EMBEDDING_SIZE, out_features = NUM_CLASS, device_id = GPU_ID)}\n    HEAD = HEAD_DICT[HEAD_NAME]\n    print(\"=\" * 60)\n    print(HEAD)\n    print(\"{} Head Generated\".format(HEAD_NAME))\n    print(\"=\" * 60)\n\n    LOSS_DICT = {'Focal': FocalLoss(), \n                 'Softmax': nn.CrossEntropyLoss()\n                 'AdaCos' : AdaCos(),\n                 'AdaM_Softmax': AdaM_Softmax() ,\n                 'ArcFace' : ArcFace() ,\n                 'ArcNegFace': ArcNegFace(),\n                 'CircleLoss': Circleloss(),\n                 'CurricularFace': CurricularFace(),\n                 'MagFace' :  MagFace(),\n                 'NPCFace' :  MV_Softmax.py(),\n                 'SST_Prototype' SST_Prototype(),\n                 \n                 }\n    LOSS = LOSS_DICT[LOSS_NAME]\n    print(\"=\" * 60)\n    print(LOSS)\n    print(\"{} Loss Generated\".format(LOSS_NAME))\n    print(\"=\" * 60)\n\n    if BACKBONE_NAME.find(\"IR\") >= 0:\n        backbone_paras_only_bn, backbone_paras_wo_bn = separate_irse_bn_paras(BACKBONE) # separate batch_norm parameters from others; do not do weight decay for batch_norm parameters to improve the generalizability\n        _, head_paras_wo_bn = separate_irse_bn_paras(HEAD)\n    else:\n        backbone_paras_only_bn, backbone_paras_wo_bn = separate_resnet_bn_paras(BACKBONE) # separate batch_norm parameters from others; do not do weight decay for batch_norm parameters to improve the generalizability\n        _, head_paras_wo_bn = separate_resnet_bn_paras(HEAD)\n    OPTIMIZER = optim.SGD([{'params': backbone_paras_wo_bn + head_paras_wo_bn, 'weight_decay': WEIGHT_DECAY}, {'params': backbone_paras_only_bn}], lr = LR, momentum = MOMENTUM)\n    print(\"=\" * 60)\n    print(OPTIMIZER)\n    print(\"Optimizer Generated\")\n    print(\"=\" * 60)\n\n    # optionally resume from a checkpoint\n    if BACKBONE_RESUME_ROOT and HEAD_RESUME_ROOT:\n        print(\"=\" * 60)\n        if os.path.isfile(BACKBONE_RESUME_ROOT) and os.path.isfile(HEAD_RESUME_ROOT):\n            print(\"Loading Backbone Checkpoint '{}'\".format(BACKBONE_RESUME_ROOT))\n            BACKBONE.load_state_dict(torch.load(BACKBONE_RESUME_ROOT))\n            print(\"Loading Head Checkpoint '{}'\".format(HEAD_RESUME_ROOT))\n            HEAD.load_state_dict(torch.load(HEAD_RESUME_ROOT))\n        else:\n            print(\"No Checkpoint Found at '{}' and '{}'. Please Have a Check or Continue to Train from Scratch\".format(BACKBONE_RESUME_ROOT, HEAD_RESUME_ROOT))\n        print(\"=\" * 60)\n\n    if MULTI_GPU:\n        # multi-GPU setting\n        BACKBONE = nn.DataParallel(BACKBONE, device_ids = GPU_ID)\n        BACKBONE = BACKBONE.to(DEVICE)\n    else:\n        # single-GPU setting\n        BACKBONE = BACKBONE.to(DEVICE)\n\n\n    #======= train & validation & save checkpoint =======#\n    DISP_FREQ = len(train_loader) // 100 # frequency to display training loss & acc\n\n    NUM_EPOCH_WARM_UP = NUM_EPOCH // 25  # use the first 1/25 epochs to warm up\n    NUM_BATCH_WARM_UP = len(train_loader) * NUM_EPOCH_WARM_UP  # use the first 1/25 epochs to warm up\n    batch = 0  # batch index\n\n    for epoch in range(NUM_EPOCH): # start training process\n        \n        if epoch == STAGES[0]: # adjust LR for each training stage after warm up, you can also choose to adjust LR manually (with slight modification) once plaueau observed\n            schedule_lr(OPTIMIZER)\n        if epoch == STAGES[1]:\n            schedule_lr(OPTIMIZER)\n        if epoch == STAGES[2]:\n            schedule_lr(OPTIMIZER)\n\n        BACKBONE.train()  # set to training mode\n        HEAD.train()\n\n        losses = AverageMeter()\n        top1 = AverageMeter()\n        top5 = AverageMeter()\n\n        for inputs, labels in tqdm(iter(train_loader)):\n\n            if (epoch + 1 <= NUM_EPOCH_WARM_UP) and (batch + 1 <= NUM_BATCH_WARM_UP): # adjust LR for each training batch during warm up\n                warm_up_lr(batch + 1, NUM_BATCH_WARM_UP, LR, OPTIMIZER)\n\n            # compute output\n            inputs = inputs.to(DEVICE)\n            labels = labels.to(DEVICE).long()\n            features = BACKBONE(inputs)\n            outputs = HEAD(features, labels)\n            loss = LOSS(outputs, labels)\n\n            # measure accuracy and record loss\n            prec1, prec5 = accuracy(outputs.data, labels, topk = (1, 5))\n            losses.update(loss.data.item(), inputs.size(0))\n            top1.update(prec1.data.item(), inputs.size(0))\n            top5.update(prec5.data.item(), inputs.size(0))\n\n            # compute gradient and do SGD step\n            OPTIMIZER.zero_grad()\n            loss.backward()\n            OPTIMIZER.step()\n            \n            # dispaly training loss & acc every DISP_FREQ\n            if ((batch + 1) % DISP_FREQ == 0) and batch != 0:\n                print(\"=\" * 60)\n                print('Epoch {}/{} Batch {}/{}\\t'\n                      'Training Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n                      'Training Prec@1 {top1.val:.3f} ({top1.avg:.3f})\\t'\n                      'Training Prec@5 {top5.val:.3f} ({top5.avg:.3f})'.format(\n                    epoch + 1, NUM_EPOCH, batch + 1, len(train_loader) * NUM_EPOCH, loss = losses, top1 = top1, top5 = top5))\n                print(\"=\" * 60)\n\n            batch += 1 # batch index\n\n        # training statistics per epoch (buffer for visualization)\n        epoch_loss = losses.avg\n        epoch_acc = top1.avg\n        writer.add_scalar(\"Training_Loss\", epoch_loss, epoch + 1)\n        writer.add_scalar(\"Training_Accuracy\", epoch_acc, epoch + 1)\n        print(\"=\" * 60)\n        print('Epoch: {}/{}\\t'\n              'Training Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n              'Training Prec@1 {top1.val:.3f} ({top1.avg:.3f})\\t'\n              'Training Prec@5 {top5.val:.3f} ({top5.avg:.3f})'.format(\n            epoch + 1, NUM_EPOCH, loss = losses, top1 = top1, top5 = top5))\n        print(\"=\" * 60)\n\n        # perform validation & save checkpoints per epoch\n        # validation statistics per epoch (buffer for visualization)\n        print(\"=\" * 60)\n        print(\"Perform Evaluation on LFW, CFP_FF, CFP_FP, AgeDB, CALFW, CPLFW and VGG2_FP, and Save Checkpoints...\")\n        accuracy_lfw, best_threshold_lfw, roc_curve_lfw = perform_val(MULTI_GPU, DEVICE, EMBEDDING_SIZE, BATCH_SIZE, BACKBONE, lfw, lfw_issame)\n        buffer_val(writer, \"LFW\", accuracy_lfw, best_threshold_lfw, roc_curve_lfw, epoch + 1)\n        accuracy_cfp_ff, best_threshold_cfp_ff, roc_curve_cfp_ff = perform_val(MULTI_GPU, DEVICE, EMBEDDING_SIZE, BATCH_SIZE, BACKBONE, cfp_ff, cfp_ff_issame)\n        buffer_val(writer, \"CFP_FF\", accuracy_cfp_ff, best_threshold_cfp_ff, roc_curve_cfp_ff, epoch + 1)\n        accuracy_cfp_fp, best_threshold_cfp_fp, roc_curve_cfp_fp = perform_val(MULTI_GPU, DEVICE, EMBEDDING_SIZE, BATCH_SIZE, BACKBONE, cfp_fp, cfp_fp_issame)\n        buffer_val(writer, \"CFP_FP\", accuracy_cfp_fp, best_threshold_cfp_fp, roc_curve_cfp_fp, epoch + 1)\n        accuracy_agedb, best_threshold_agedb, roc_curve_agedb = perform_val(MULTI_GPU, DEVICE, EMBEDDING_SIZE, BATCH_SIZE, BACKBONE, agedb, agedb_issame)\n        buffer_val(writer, \"AgeDB\", accuracy_agedb, best_threshold_agedb, roc_curve_agedb, epoch + 1)\n        accuracy_calfw, best_threshold_calfw, roc_curve_calfw = perform_val(MULTI_GPU, DEVICE, EMBEDDING_SIZE, BATCH_SIZE, BACKBONE, calfw, calfw_issame)\n        buffer_val(writer, \"CALFW\", accuracy_calfw, best_threshold_calfw, roc_curve_calfw, epoch + 1)\n        accuracy_cplfw, best_threshold_cplfw, roc_curve_cplfw = perform_val(MULTI_GPU, DEVICE, EMBEDDING_SIZE, BATCH_SIZE, BACKBONE, cplfw, cplfw_issame)\n        buffer_val(writer, \"CPLFW\", accuracy_cplfw, best_threshold_cplfw, roc_curve_cplfw, epoch + 1)\n        accuracy_vgg2_fp, best_threshold_vgg2_fp, roc_curve_vgg2_fp = perform_val(MULTI_GPU, DEVICE, EMBEDDING_SIZE, BATCH_SIZE, BACKBONE, vgg2_fp, vgg2_fp_issame)\n        buffer_val(writer, \"VGGFace2_FP\", accuracy_vgg2_fp, best_threshold_vgg2_fp, roc_curve_vgg2_fp, epoch + 1)\n        print(\"Epoch {}/{}, Evaluation: LFW Acc: {}, CFP_FF Acc: {}, CFP_FP Acc: {}, AgeDB Acc: {}, CALFW Acc: {}, CPLFW Acc: {}, VGG2_FP Acc: {}\".format(epoch + 1, NUM_EPOCH, accuracy_lfw, accuracy_cfp_ff, accuracy_cfp_fp, accuracy_agedb, accuracy_calfw, accuracy_cplfw, accuracy_vgg2_fp))\n        print(\"=\" * 60)\n\n        # save checkpoints per epoch\n        if MULTI_GPU:\n            torch.save(BACKBONE.module.state_dict(), os.path.join(MODEL_ROOT, \"Backbone_{}_Epoch_{}_Batch_{}_Time_{}_checkpoint.pth\".format(BACKBONE_NAME, epoch + 1, batch, get_time())))\n            torch.save(HEAD.state_dict(), os.path.join(MODEL_ROOT, \"Head_{}_Epoch_{}_Batch_{}_Time_{}_checkpoint.pth\".format(HEAD_NAME, epoch + 1, batch, get_time())))\n        else:\n            torch.save(BACKBONE.state_dict(), os.path.join(MODEL_ROOT, \"Backbone_{}_Epoch_{}_Batch_{}_Time_{}_checkpoint.pth\".format(BACKBONE_NAME, epoch + 1, batch, get_time())))\n            torch.save(HEAD.state_dict(), os.path.join(MODEL_ROOT, \"Head_{}_Epoch_{}_Batch_{}_Time_{}_checkpoint.pth\".format(HEAD_NAME, epoch + 1, batch, get_time())))\n"
        },
        {
          "name": "util",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}