{
  "metadata": {
    "timestamp": 1736559959982,
    "page": 740,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjc1MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "truefoundry/cognita",
      "stars": 3503,
      "defaultBranch": "main",
      "files": [
        {
          "name": ".dockerignore",
          "type": "blob",
          "size": 0.2822265625,
          "content": ".idea/\n.vscode/\n.ipynb_checkpoints/\n*.py[cod]\n.DS_Store\n__MACOSX/*\n.envs/\n.metrics/\n*.env*\ntempDir/\nrepo-data/\nexamples/\n.git/\nvenv/\n**/__pycache__/\n*.iml\ntest/.pytest_cache\ndist/\ntf-docs-crawled/\ntruefoundry-docs/\n*.zip\nragenv/\nvolumes/\nqdrant_db/\nqdrant_storage/\nsample-data/\nuser_data/\n"
        },
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.349609375,
          "content": "*.iml\n.idea\n**/__pycache__/\ntest/.pytest_cache\ndist/\n.serverless\n.pytest_cache\n.DS_Store\nvenv\n.vscode/\nrepo-data/\n**/.env*\ntempDir/\n!**/.env.example\n.ipynb_checkpoints/\ndeploy.*.py\nragenv\nrequests.txt\nqdrant_db\ntest.py\ntf-docs-crawled/\ntruefoundry-docs/\n*.zip\nqdrant_storage/\n*.pth\n.truefoundry\ninfinity/\nvolumes/\nuser_data/\npgdata/\n*.bak\nmodels_config.yaml\n"
        },
        {
          "name": ".pre-commit-config.yaml",
          "type": "blob",
          "size": 0.76953125,
          "content": "minimum_pre_commit_version: 2.16.0\nrepos:\n  - repo: 'https://github.com/pre-commit/pre-commit-hooks'\n    rev: v2.3.0\n    hooks:\n      - id: check-docstring-first\n      - id: check-merge-conflict\n      - id: trailing-whitespace\n      - id: end-of-file-fixer\n      - id: check-yaml\n      - id: check-ast\n  - repo: https://github.com/PyCQA/autoflake\n    rev: v2.3.1\n    hooks:\n      - id: autoflake\n        args: [--remove-all-unused-imports, --in-place]\n  - repo: 'https://github.com/psf/black'\n    rev: 23.3.0\n    hooks:\n      - id: black\n        args:\n          - --line-length=88\n          - --safe\n          - --target-version=py310\n        stages: [commit]\n  - repo: https://github.com/pycqa/isort\n    rev: 5.12.0\n    hooks:\n      - id: isort\n        args:\n          - --profile=black\n"
        },
        {
          "name": ".tfyignore",
          "type": "blob",
          "size": 0.34765625,
          "content": ".idea/\n.vscode/\n.ipynb_checkpoints/\n*.py[cod]\n.DS_Store\n__MACOSX/*\n.envs/\n.metrics/\n*.env*\ntempDir/\nrepo-data/\nexamples/\n.git/\nvenv/\n**/__pycache__/\n*.iml\ntest/.pytest_cache\ndist/\ntf-docs-crawled/\ntruefoundry-docs/\n*.zip\nragenv/\nqdrant_storage/\nqdrant_db/\n*.pth\n.truefoundry\nvolumes/\npgdata/\nuser_data/\n.env\n*.bak\nnode_modules/\nsample-data/\n.github/\ndocs/\n"
        },
        {
          "name": "CONTRIBUTING.md",
          "type": "blob",
          "size": 2.8310546875,
          "content": "# Cognita Contribution Guidelines\n\nWelcome to the Cognita community! We're thrilled that you're interested in contributing to the Cognita project. Cognita is a collaborative open-source project, and we believe that everyone has something unique to contribute. Below you'll find our guidelines which aim to make contributing to Cognita a respectful and pleasant experience for everyone.\n\n## üåü Community and Open Source\n\nOpen source is at the heart of Cognita. We appreciate feedback, ideas, and enhancements from the community. Whether you're looking to fix a bug, add a new feature, or simply improve the documentation, your contribution is important to us.\n\n## üìö Before You Begin\n\nBefore contributing, please take a moment to read through the [README](./README.md) as it provides a comprehensive understanding of the project and are essential reading to ensure that we're all on the same page.\n\n## üêõ Reporting Issues\n\nIf you've identified a bug or have an idea for an enhancement, please begin by creating an Issue. Here's how:\n\n-   Check the Issue tracker to ensure the bug or enhancement hasn't already been reported.\n-   Clearly describe the issue including steps to reproduce when it is a bug.\n-   Include as much relevant information as possible.\n\n## üí° Ideas and Feedback\n\nWe welcome all ideas and feedback. If you're not ready to open an Issue or if you're just looking for a place to discuss ideas, head over to our [GitHub Discussions](https://github.com/truefoundry/docs-qa-playground/discussions).\n\n\n## üìù Pull Requests\n\nIf you're ready to contribute code or documentation, please submit a Pull Request (PR) to the dev branch. Here's the process:\n\n-   Fork the repository and create your branch from `main`.\n-   Ensure that your code adheres to the existing code style. Use [Black](https://github.com/psf/black) for formatting Python code.\n-   If you're adding a new feature, consider writing unit tests and documenting the feature.\n-   Make sure your code lints (mypy compatibility is optional but encouraged).\n-   Include a clear description of your changes in the PR.\n-   Link to the Issue in your PR description.\n\n### üß™ Tests and Formatting\n\nTo maintain the quality of the codebase, we ask that all contributors:\n\n-   Create and run unit tests to ensure that nothing is broken.\n-   Use [Black](https://github.com/psf/black) to format your code before submitting.\n\n### üîÑ Pull Request Process\n\n-   PRs are reviewed on a regular basis.\n-   Engage in the conversation and make requested updates to your PR if needed.\n-   Once approved, your PR will be merged into the main branch by a maintainer.\n\n## üó®Ô∏è Stay Connected\n\nWe encourage you to participate in discussions. Stay connected, share ideas, and get to know fellow contributors.\n\nYour contributions not only help improve the project but also the wider community of users and developers.\n\nHappy contributing!\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 11.0908203125,
          "content": "                                 Apache License\n                           Version 2.0, January 2004\n                        http://www.apache.org/licenses/\n\n   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n\n   1. Definitions.\n\n      \"License\" shall mean the terms and conditions for use, reproduction,\n      and distribution as defined by Sections 1 through 9 of this document.\n\n      \"Licensor\" shall mean the copyright owner or entity authorized by\n      the copyright owner that is granting the License.\n\n      \"Legal Entity\" shall mean the union of the acting entity and all\n      other entities that control, are controlled by, or are under common\n      control with that entity. For the purposes of this definition,\n      \"control\" means (i) the power, direct or indirect, to cause the\n      direction or management of such entity, whether by contract or\n      otherwise, or (ii) ownership of fifty percent (50%) or more of the\n      outstanding shares, or (iii) beneficial ownership of such entity.\n\n      \"You\" (or \"Your\") shall mean an individual or Legal Entity\n      exercising permissions granted by this License.\n\n      \"Source\" form shall mean the preferred form for making modifications,\n      including but not limited to software source code, documentation\n      source, and configuration files.\n\n      \"Object\" form shall mean any form resulting from mechanical\n      transformation or translation of a Source form, including but\n      not limited to compiled object code, generated documentation,\n      and conversions to other media types.\n\n      \"Work\" shall mean the work of authorship, whether in Source or\n      Object form, made available under the License, as indicated by a\n      copyright notice that is included in or attached to the work\n      (an example is provided in the Appendix below).\n\n      \"Derivative Works\" shall mean any work, whether in Source or Object\n      form, that is based on (or derived from) the Work and for which the\n      editorial revisions, annotations, elaborations, or other modifications\n      represent, as a whole, an original work of authorship. For the purposes\n      of this License, Derivative Works shall not include works that remain\n      separable from, or merely link (or bind by name) to the interfaces of,\n      the Work and Derivative Works thereof.\n\n      \"Contribution\" shall mean any work of authorship, including\n      the original version of the Work and any modifications or additions\n      to that Work or Derivative Works thereof, that is intentionally\n      submitted to Licensor for inclusion in the Work by the copyright owner\n      or by an individual or Legal Entity authorized to submit on behalf of\n      the copyright owner. For the purposes of this definition, \"submitted\"\n      means any form of electronic, verbal, or written communication sent\n      to the Licensor or its representatives, including but not limited to\n      communication on electronic mailing lists, source code control systems,\n      and issue tracking systems that are managed by, or on behalf of, the\n      Licensor for the purpose of discussing and improving the Work, but\n      excluding communication that is conspicuously marked or otherwise\n      designated in writing by the copyright owner as \"Not a Contribution.\"\n\n      \"Contributor\" shall mean Licensor and any individual or Legal Entity\n      on behalf of whom a Contribution has been received by Licensor and\n      subsequently incorporated within the Work.\n\n   2. Grant of Copyright License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      copyright license to reproduce, prepare Derivative Works of,\n      publicly display, publicly perform, sublicense, and distribute the\n      Work and such Derivative Works in Source or Object form.\n\n   3. Grant of Patent License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      (except as stated in this section) patent license to make, have made,\n      use, offer to sell, sell, import, and otherwise transfer the Work,\n      where such license applies only to those patent claims licensable\n      by such Contributor that are necessarily infringed by their\n      Contribution(s) alone or by combination of their Contribution(s)\n      with the Work to which such Contribution(s) was submitted. If You\n      institute patent litigation against any entity (including a\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\n      or a Contribution incorporated within the Work constitutes direct\n      or contributory patent infringement, then any patent licenses\n      granted to You under this License for that Work shall terminate\n      as of the date such litigation is filed.\n\n   4. Redistribution. You may reproduce and distribute copies of the\n      Work or Derivative Works thereof in any medium, with or without\n      modifications, and in Source or Object form, provided that You\n      meet the following conditions:\n\n      (a) You must give any other recipients of the Work or\n          Derivative Works a copy of this License; and\n\n      (b) You must cause any modified files to carry prominent notices\n          stating that You changed the files; and\n\n      (c) You must retain, in the Source form of any Derivative Works\n          that You distribute, all copyright, patent, trademark, and\n          attribution notices from the Source form of the Work,\n          excluding those notices that do not pertain to any part of\n          the Derivative Works; and\n\n      (d) If the Work includes a \"NOTICE\" text file as part of its\n          distribution, then any Derivative Works that You distribute must\n          include a readable copy of the attribution notices contained\n          within such NOTICE file, excluding those notices that do not\n          pertain to any part of the Derivative Works, in at least one\n          of the following places: within a NOTICE text file distributed\n          as part of the Derivative Works; within the Source form or\n          documentation, if provided along with the Derivative Works; or,\n          within a display generated by the Derivative Works, if and\n          wherever such third-party notices normally appear. The contents\n          of the NOTICE file are for informational purposes only and\n          do not modify the License. You may add Your own attribution\n          notices within Derivative Works that You distribute, alongside\n          or as an addendum to the NOTICE text from the Work, provided\n          that such additional attribution notices cannot be construed\n          as modifying the License.\n\n      You may add Your own copyright statement to Your modifications and\n      may provide additional or different license terms and conditions\n      for use, reproduction, or distribution of Your modifications, or\n      for any such Derivative Works as a whole, provided Your use,\n      reproduction, and distribution of the Work otherwise complies with\n      the conditions stated in this License.\n\n   5. Submission of Contributions. Unless You explicitly state otherwise,\n      any Contribution intentionally submitted for inclusion in the Work\n      by You to the Licensor shall be under the terms and conditions of\n      this License, without any additional terms or conditions.\n      Notwithstanding the above, nothing herein shall supersede or modify\n      the terms of any separate license agreement you may have executed\n      with Licensor regarding such Contributions.\n\n   6. Trademarks. This License does not grant permission to use the trade\n      names, trademarks, service marks, or product names of the Licensor,\n      except as required for reasonable and customary use in describing the\n      origin of the Work and reproducing the content of the NOTICE file.\n\n   7. Disclaimer of Warranty. Unless required by applicable law or\n      agreed to in writing, Licensor provides the Work (and each\n      Contributor provides its Contributions) on an \"AS IS\" BASIS,\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n      implied, including, without limitation, any warranties or conditions\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n      PARTICULAR PURPOSE. You are solely responsible for determining the\n      appropriateness of using or redistributing the Work and assume any\n      risks associated with Your exercise of permissions under this License.\n\n   8. Limitation of Liability. In no event and under no legal theory,\n      whether in tort (including negligence), contract, or otherwise,\n      unless required by applicable law (such as deliberate and grossly\n      negligent acts) or agreed to in writing, shall any Contributor be\n      liable to You for damages, including any direct, indirect, special,\n      incidental, or consequential damages of any character arising as a\n      result of this License or out of the use or inability to use the\n      Work (including but not limited to damages for loss of goodwill,\n      work stoppage, computer failure or malfunction, or any and all\n      other commercial damages or losses), even if such Contributor\n      has been advised of the possibility of such damages.\n\n   9. Accepting Warranty or Additional Liability. While redistributing\n      the Work or Derivative Works thereof, You may choose to offer,\n      and charge a fee for, acceptance of support, warranty, indemnity,\n      or other liability obligations and/or rights consistent with this\n      License. However, in accepting such obligations, You may act only\n      on Your own behalf and on Your sole responsibility, not on behalf\n      of any other Contributor, and only if You agree to indemnify,\n      defend, and hold each Contributor harmless for any liability\n      incurred by, or claims asserted against, such Contributor by reason\n      of your accepting any such warranty or additional liability.\n\n   END OF TERMS AND CONDITIONS\n\n   APPENDIX: How to apply the Apache License to your work.\n\n      To apply the Apache License to your work, attach the following\n      boilerplate notice, with the fields enclosed by brackets \"[]\"\n      replaced with your own identifying information. (Don't include\n      the brackets!)  The text should be enclosed in the appropriate\n      comment syntax for the file format. We also recommend that a\n      file or class name and description of purpose be included on the\n      same \"printed page\" as the copyright notice for easier\n      identification within third-party archives.\n\n   Copyright [yyyy] [name of copyright owner]\n\n   Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 22.0185546875,
          "content": "# [Cognita](<(https://cognita.truefoundry.com)>)\n\n![RAG_TF](./docs/images/readme-banner.png)\n\n## Why use Cognita?\n\nLangchain/LlamaIndex provide easy to use abstractions that can be used for quick experimentation and prototyping on jupyter notebooks. But, when things move to production, there are constraints like the components should be modular, easily scalable and extendable. This is where Cognita comes in action.\nCognita uses Langchain/Llamaindex under the hood and provides an organisation to your codebase, where each of the RAG component is modular, API driven and easily extendible. Cognita can be used easily in a [local](#rocket-quickstart-running-cognita-locally) setup, at the same time, offers you a production ready environment along with no-code [UI](./frontend/README.md) support. Cognita also supports incremental indexing by default.\n\nYou can try out Cognita at: [https://cognita.truefoundry.com](https://cognita.truefoundry.com)\n\n![RAG_TF](./docs/images/RAG-TF.gif)\n\n# üéâ What's new in Cognita\n\n- [September, 2024] Cognita now has AudioParser (https://github.com/fedirz/faster-whisper-server) and VideoParser (AudioParser + MultimodalParser).\n- [August, 2024] Cognita has now moved to using pydantic v2.\n- [July, 2024] Introducing `model gateway` a single file to manage all the models and their configurations.\n- [June, 2024] Cognita now supports it's own Metadatastore, powered by Prisma and Postgress. You can now use Cognita via UI completely without the need of `local.metadata.yaml` file. You can create collections, data sources, and index them via UI. This makes it easier to use Cognita without any code changes.\n- [June, 2024] Added one click local deployment of cognita. You can now run the entire cognita system using docker-compose. This makes it easier to test and develop locally.\n- [May, 2024] Added support for Embedding and Reranking using [Infninty Server](https://github.com/michaelfeil/infinity). You can now use hosted services for variatey embeddings and reranking services available on huggingface. This reduces the burden on the main cognita system and makes it more scalable.\n- [May, 2024] Cleaned up requirements for optional package installations for vector dbs, parsers, embedders, and rerankers.\n- [May, 2024] Conditional docker builds with arguments for optional package installations\n- [April, 2024] Support for multi-modal vision parser using GPT-4\n\n# Contents\n\n- [Cognita](#cognita)\n  - [Why use Cognita?](#why-use-cognita)\n- [üéâ What's new in Cognita](#-whats-new-in-cognita)\n- [Contents](#contents)\n  - [Introduction](#introduction)\n    - [Advantages of using Cognita are:](#advantages-of-using-cognita-are)\n    - [Features:](#features)\n- [:rocket: Quickstart: Running Cognita Locally](#rocket-quickstart-running-cognita-locally)\n  - [:whale: Using Docker compose (recommended)](#whale-using-docker-compose-recommended)\n  - [Cognita from source](#cognita-from-source)\n- [:hammer_and_pick: Project Architecture](#hammer_and_pick-project-architecture)\n  - [Cognita Components:](#cognita-components)\n  - [Data Indexing:](#data-indexing)\n  - [:question: Question-Answering using API Server:](#question-question-answering-using-api-server)\n  - [:computer: Code Structure:](#computer-code-structure)\n  - [Customizing the Code for your usecase](#customizing-the-code-for-your-usecase)\n    - [Customizing Dataloaders:](#customizing-dataloaders)\n    - [Customizing Embedder:](#customizing-embedder)\n    - [Customizing Parsers:](#customizing-parsers)\n    - [Adding Custom VectorDB:](#adding-custom-vectordb)\n    - [Rerankers:](#rerankers)\n- [:bulb: Writing your Query Controller (QnA):](#bulb-writing-your-query-controller-qna)\n  - [Steps to add your custom Query Controller:](#steps-to-add-your-custom-query-controller)\n- [:whale: Quickstart: Deployment with Truefoundry:](#whale-quickstart-deployment-with-truefoundry)\n  - [Using the **RAG UI**:](#using-the-rag-ui)\n- [:sparkling_heart: Open Source Contribution](#sparkling_heart-open-source-contribution)\n- [:crystal_ball: Future developments](#crystal_ball-future-developments)\n- [Star History](#star-history)\n\n## Introduction\n\nCognita is an open-source framework to organize your RAG codebase along with a frontend to play around with different RAG customizations. It provides a simple way to organize your codebase so that it becomes easy to test it locally while also being able to deploy it in a production ready environment. The key issues that arise while productionizing RAG system from a Jupyter Notebook are:\n\n1. **Chunking and Embedding Job**: The chunking and embedding code usually needs to be abstracted out and deployed as a job. Sometimes the job will need to run on a schedule or be triggered via an event to keep the data updated.\n2. **Query Service**: The code that generates the answer from the query needs to be wrapped up in a api server like FastAPI and should be deployed as a service. This service should be able to handle multiple queries at the same time and also autoscale with higher traffic.\n3. **LLM / Embedding Model Deployment**: Often times, if we are using open-source models, we load the model in the Jupyter notebook. This will need to be hosted as a separate service in production and model will need to be called as an API.\n4. **Vector DB deployment**: Most testing happens on vector DBs in memory or on disk. However, in production, the DBs need to be deployed in a more scalable and reliable way.\n\nCognita makes it really easy to customize and experiment everything about a RAG system and still be able to deploy it in a good way. It also ships with a UI that makes it easier to try out different RAG configurations and see the results in real time. You can use it locally or with/without using any Truefoundry components. However, using Truefoundry components makes it easier to test different models and deploy the system in a scalable way. Cognita allows you to host multiple RAG systems using one app.\n\n### Advantages of using Cognita are:\n\n1. A central reusable repository of parsers, loaders, embedders and retrievers.\n2. Ability for non-technical users to play with UI - Upload documents and perform QnA using modules built by the development team.\n3. Fully API driven - which allows integration with other systems.\n   > If you use Cognita with Truefoundry AI Gateway, you can get logging, metrics and feedback mechanism for your user queries.\n\n### Features:\n\n1. Support for multiple document retrievers that use `Similarity Search`, `Query Decompostion`, `Document Reranking`, etc\n1. Support for SOTA OpenSource embeddings and reranking from `mixedbread-ai`\n1. Support for using LLMs using `ollama`\n1. Support for incremental indexing that ingests entire documents in batches (reduces compute burden), keeps track of already indexed documents and prevents re-indexing of those docs.\n\n# :rocket: Quickstart: Running Cognita Locally\n\n## :whale: Using Docker compose (recommended - version 25+)\n\nCognita and all of its services can be run using docker-compose. This is the recommended way to run Cognita locally. Install Docker and docker-compose for your system from: [Docker Compose](https://docs.docker.com/compose/install/)\n\n### Configuring Model Providers\n\nBefore starting the services, we need to configure model providers that we would need for embedding and generating answers.\n\nTo start, copy `models_config.sample.yaml` to `models_config.yaml`\n\n```shell\ncp models_config.sample.yaml models_config.yaml\n```\n\nBy default, the config has local providers enabled that need infinity and ollama server to run embedding and LLMs locally.\nHowever, if you have a OpenAI API Key, you can uncomment the `openai` provider in `models_config.yaml` and update `OPENAI_API_KEY` in `compose.env`\n\nNow, you can run the following command to start the services:\n\n```shell\ndocker-compose --env-file compose.env up\n```\n\n- The compose file uses `compose.env` file for environment variables. You can modify it as per your needs.\n- The compose file will start the following services:\n  - `cognita-db` - Postgres instance used to store metadata for collections and data sources.\n  - `qdrant-server` - Used to start local vector db server.\n  - `cognita-backend` - Used to start the FastAPI backend server for Cognita.\n  - `cognita-frontend` - Used to start the frontend for Cognita.\n- Once the services are up, you can access the qdrant server at `http://localhost:6333`, the backend at `http://localhost:8000` and frontend at `http://localhost:5001`.\n\nTo start additional services such as `ollama` and `infinity-server` you can run the following command:\n\n```shell\ndocker-compose --env-file compose.env --profile ollama --profile infinity up\n```\n\n- This will start additional servers for `ollama` and `infinity-server` which can be used for LLM, Embeddings and reranking respectively. You can access the `infinity-server` at `http://localhost:7997`.\n\n- If you want to build backend / frontend image locally, for e.g when you add new requirements/packages/take a new pull from Github you can add `--build` flag to the command.\n\n```shell\ndocker-compose --env-file compose.env up --build\n```\n\nOR\n\n```shell\ndocker-compose --env-file compose.env --profile ollama --profile infinity up --build\n```\n\n## Developing in Cognita\n\nDocker compose is a great way to run the entire Cognita system locally. Any changes that you make in the `backend` folder will be automatically reflected in the running backend server. You can test out different APIs and endpoints by making changes in the backend code.\n\n# :hammer_and_pick: Project Architecture\n\n![](./docs/images/rag_arch.png)\n\nOverall the architecture of Cognita is composed of several entities\n\n## Cognita Components:\n\n1. **Data Sources** - These are the places that contain your documents to be indexed. Usually these are S3 buckets, databases, TrueFoundry Artifacts or even local disk\n\n2. **Metadata Store** - This store contains metadata about the collection themselves. A collection refers to a set of documents from one or more data sources combined. For each collection, the collection metadata stores\n\n   - Name of the collection\n   - Name of the associated Vector DB collection\n   - Linked Data Sources\n   - Parsing Configuration for each data source\n   - Embedding Model and Configuration to be used\n\n3. **LLM Gateway** - This is a central proxy that allows proxying requests to various Embedding and LLM models across many providers with a unified API format. This can be OpenAIChat, OllamaChat, or even TruefoundryChat that uses TF LLM Gateway.\n\n4. **Vector DB** - This stores the embeddings and metadata for parsed files for the collection. It can be queried to get similar chunks or exact matches based on filters. We are currently supporting `Qdrant` and `SingleStore` as our choice of vector database.\n\n5. **Indexing Job** - This is an asynchronous Job responsible for orchestrating the indexing flow. Indexing can be started manually or run regularly on a cron schedule. It will\n\n   - Scan the Data Sources to get list of documents\n   - Check the Vector DB state to filter out unchanged documents\n   - Downloads and parses files to create smaller chunks with associated metadata\n   - Embeds those chunks using the AI Gateway and puts them into Vector DB\n     > The source code for this is in the `backend/indexer/`\n\n6. **API Server** - This component processes the user query to generate answers with references synchronously. Each application has full control over the retrieval and answer process. Broadly speaking, when a user sends a request\n\n   - The corresponsing Query Controller bootstraps retrievers or multi-step agents according to configuration.\n   - User's question is processed and embedded using the AI Gateway.\n   - One or more retrievers interact with the Vector DB to fetch relevant chunks and metadata.\n   - A final answer is formed by using a LLM via the AI Gateway.\n   - Metadata for relevant documents fetched during the process can be optionally enriched. E.g. adding presigned URLs.\n     > The code for this component is in `backend/server/`\n\n## Data Indexing:\n\n1. A Cron on some schedule will trigger the Indexing Job\n1. The data source associated with the collection are **scanned** for all data points (files)\n1. The job compares the VectorDB state with data source state to figure out **newly added files, updated files and deleted files**. The new and updated files are **downloaded**\n1. The newly added files and updated files are **parsed and chunked** into smaller pieces each with their own metadata\n1. The chunks are **embedded** using embedding models like `text-ada-002` from `openai` or `mxbai-embed-large-v1` from `mixedbread-ai`\n1. The embedded chunks are put into VectorDB with auto generated and provided metadata\n\n## :question: Question-Answering using API Server:\n\n1. Users sends a request with their query\n\n2. It is routed to one of the app's query controller\n\n3. One or more retrievers are constructed on top of the Vector DB\n\n4. Then a Question Answering chain / agent is constructed. It embeds the user query and fetches similar chunks.\n\n5. A single shot Question Answering chain just generates an answer given similar chunks. An agent can do multi step reasoning and use many tools before arriving at an answer. In both cases, the API server uses LLM models (like GPT 3.5, GPT 4, etc)\n\n6. Before returning the answer, the metadata for relevant chunks can be updated with things like presigned urls, surrounding slides, external data source links.\n\n7. The answer and relevant document chunks are returned in response.\n\n   **Note:** In case of agents the intermediate steps can also be streamed. It is up to the specific app to decide.\n\n## Customizing the Code for your usecase\n\nCognita goes by the tagline -\n\n> Everything is available and Everything is customizable.\n\nCognita makes it really easy to switch between parsers, loaders, models and retrievers.\n\n### Customizing Dataloaders:\n\n- You can write your own data loader by inherting the `BaseDataLoader` class from `backend/modules/dataloaders/loader.py`\n\n- Finally, register the loader in `backend/modules/dataloaders/__init__.py`\n\n- Testing a dataloader on localdir, in root dir, copy the following code as `test.py` and execute it. We show how to test an existing `LocalDirLoader` here:\n\n  ```python\n  from backend.modules.dataloaders import LocalDirLoader\n  from backend.types import DataSource\n\n  data_source = DataSource(\n  type=\"local\",\n  uri=\"sample-data/creditcards\",\n  )\n\n  loader = LocalDirLoader()\n\n\n  loaded_data_pts = loader.load_full_data(\n      data_source=data_source,\n      dest_dir=\"test/creditcards\",\n  )\n\n\n  for data_pt in loaded_data_pts:\n      print(data_pt)\n  ```\n\n### Customizing Embedder:\n\n- The codebase currently uses `OpenAIEmbeddings` you can registered as `default`.\n- You can register your custom embeddings in `backend/modules/embedder/__init__.py`\n- You can also add your own embedder an example of which is given under `backend/modules/embedder/mixbread_embedder.py`. It inherits langchain embedding class.\n\n### Customizing Parsers:\n\n- You can write your own parser by inherting the `BaseParser` class from `backend/modules/parsers/parser.py`\n\n- Finally, register the parser in `backend/modules/parsers/__init__.py`\n\n- Testing a Parser on a local file, in root dir, copy the following code as `test.py` and execute it. Here we show how we can test existing `MarkdownParser`:\n\n  ```python\n  import asyncio\n  from backend.modules.parsers import MarkdownParser\n\n  parser = MarkdownParser()\n  chunks =  asyncio.run(\n      parser.get_chunks(\n          filepath=\"sample-data/creditcards/diners-club-black.md\",\n      )\n  )\n  print(chunks)\n  ```\n\n### Adding Custom VectorDB:\n\n- To add your own interface for a VectorDB you can inhertit `BaseVectorDB` from `backend/modules/vector_db/base.py`\n\n- Register the vectordb under `backend/modules/vector_db/__init__.py`\n\n# :bulb: Writing your Query Controller (QnA):\n\nCode responsible for implementing the Query interface of RAG application. The methods defined in these query controllers are added routes to your FastAPI server.\n\n## Steps to add your custom Query Controller:\n\n- Add your Query controller class in `backend/modules/query_controllers/`\n\n- Add `query_controller` decorator to your class and pass the name of your custom controller as argument\n\n```controller.py\nfrom backend.server.decorator import query_controller\n\n@query_controller(\"/my-controller\")\nclass MyCustomController():\n    ...\n```\n\n- Add methods to this controller as per your needs and use our http decorators like `post, get, delete` to make your methods an API\n\n```controller.py\nfrom backend.server.decorator import post\n\n@query_controller(\"/my-controller\")\nclass MyCustomController():\n    ...\n\n    @post(\"/answer\")\n    def answer(query: str):\n        # Write code to express your logic for answer\n        # This API will be exposed as POST /my-controller/answer\n        ...\n```\n\n- Import your custom controller class at `backend/modules/query_controllers/__init__.py`\n\n```__init__.py\n...\nfrom backend.modules.query_controllers.sample_controller.controller import MyCustomController\n```\n\n> As an example, we have implemented sample controller in `backend/modules/query_controllers/example`. Please refer for better understanding\n\n# :whale: Quickstart: Deployment with Truefoundry:\n\nTo be able to **Query** on your own documents, follow the steps below:\n\n1.  Register at TrueFoundry, follow [here](https://www.truefoundry.com/register)\n\n    - Fill up the form and register as an organization (let's say <org_name>)\n    - On `Submit`, you will be redirected to your dashboard endpoint ie https://<org_name>.truefoundry.cloud\n    - Complete your email verification\n    - Login to the platform at your dashboard endpoint ie. https://<org_name>.truefoundry.cloud\n\n    `Note: Keep your dashboard endpoint handy, we will refer it as \"TFY_HOST\" and it should have structure like \"https://<org_name>.truefoundry.cloud\"`\n\n2.  Setup a cluster, use TrueFoundry managed for quick setup\n\n    - Give a unique name to your **[Cluster](https://docs.truefoundry.com/docs/workspace)** and click on **Launch Cluster**\n    - It will take few minutes to provision a cluster for you\n    - On **Configure Host Domain** section, click `Register` for the pre-filled IP\n    - Next, `Add` a **Docker Registry** to push your docker images to.\n    - Next, **Deploy a Model**, you can choose to `Skip` this step\n\n3.  Add a **Storage Integration**\n\n4.  Create a **ML Repo**\n\n    - Navigate to **ML Repo** tab\n    - Click on `+ New ML Repo` button on top-right\n    - Give a unique name to your **ML Repo** (say 'docs-qa-llm')\n    - Select **Storage Integration**\n    - On `Submit`, your **ML Repo** will be created\n\n      For more details: [link](https://docs.truefoundry.com/docs/creating-ml-repo-via-ui)\n\n5.  Create a **Workspace**\n\n    - Navigate to **Workspace** tab\n    - Click on `+ New Workspace` button on top-right\n    - Select your **Cluster**\n    - Give a name to your **Workspace** (say 'docs-qa-llm')\n    - Enable **ML Repo Access** and `Add ML Repo Access`\n    - Select your **ML Repo** and role as **Project Admin**\n    - On `Submit`, a new **Workspace** will be created. You can copy the **Workspace FQN** by clicking on **FQN**.\n\n    For more details: [link](https://docs.truefoundry.com/docs/installation-and-setup#5-creating-workspaces)\n\n6.  Deploy **RAG Application**\n\n    - Navigate to **Deployments** tab\n    - Click on `+ New Deployment` buttton on top-right\n    - Select `Application Catalogue`\n    - Select your workspace\n    - Select RAG Application\n    - Fill up the deployment template\n      - Give your deployment a Name\n      - Add ML Repo\n      - You can either add an existing Qdrant DB or create a new one\n      - By default, `main` branch is used for deployment (You will find this option in `Show Advance fields`). You can change the branch name and git repository if required.\n        > Make sure to re-select the main branch, as the SHA commit, does not get updated automatically.\n      - Click on `Submit` your application will be deployed.\n\n## Using the **RAG UI**:\n\nThe following steps will showcase how to use the cognita UI to query documents:\n\n1.  Create Data Source\n\n    - Click on `Data Sources` tab\n      ![Datasource](./docs/images/datasource.png)\n    - Click `+ New Datasource`\n    - Data source type can be either files from local directory, web url, github url or providing Truefoundry artifact FQN.\n      - E.g: If `Localdir` is selected upload files from your machine and click `Submit`.\n    - Created Data sources list will be available in the Data Sources tab.\n      ![DataSourceList](./docs/images/list-datasources-in-collection.png)\n\n2.  Create Collection\n\n    - Click on `Collections` tab\n    - Click `+ New Collection`\n      ![collection](./docs/images/adding-collection.png)\n    - Enter Collection Name\n    - Select Embedding Model\n    - Add earlier created data source and the necessary configuration\n    - Click `Process` to create the collection and index the data.\n      ![ingestionstarted](./docs/images/dataingestion-started.png)\n\n3.  As soon as you create the collection, data ingestion begins, you can view it's status by selecting your collection in collections tab. You can also add additional data sources later on and index them in the collection.\n    ![ingestioncomplete](./docs/images/dataingestion-complete.png)\n\n4.  Response generation\n    ![responsegen](./docs/images/response-generation.png)\n\n    - Select the collection\n    - Select the LLM and it's configuration\n    - Select the document retriever\n    - Write the prompt or use the default prompt\n    - Ask the query\n\n# :sparkling_heart: Open Source Contribution\n\nYour contributions are always welcome! Feel free to contribute ideas, feedback, or create issues and bug reports if you find any! Before contributing, please read the [Contribution Guide](./CONTRIBUTING.md).\n\n# :crystal_ball: Future developments\n\nContributions are welcomed for the following upcoming developments:\n\n- Support for other vector databases like `Chroma`, `Weaviate`, etc\n- Support for `Scalar + Binary Quantization` embeddings.\n- Support for `RAG Evalutaion` of different retrievers.\n- Support for `RAG Visualization`.\n- Support for conversational chatbot with context\n- Support for RAG optimized LLMs like `stable-lm-3b`, `dragon-yi-6b`, etc\n- Support for `GraphDB`\n\n---\n\n# Star History\n\n[![Star History Chart](https://api.star-history.com/svg?repos=truefoundry/cognita&type=Date)](https://star-history.com/#truefoundry/cognita&Date)\n"
        },
        {
          "name": "__init__.py",
          "type": "blob",
          "size": 0,
          "content": ""
        },
        {
          "name": "backend",
          "type": "tree",
          "content": null
        },
        {
          "name": "compose.env",
          "type": "blob",
          "size": 1.6591796875,
          "content": "LOCAL=true\nPROCESS_POOL_WORKERS=2\n\n## POSTGRES\nPOSTGRES_PORT=5432\nPOSTGRES_USER=postgres\nPOSTGRES_PASSWORD=test\n\n## COGNITA_BACKEND VARS\n### Note: If you are changing `COGNITA_BACKEND_PORT`, please make sure to update `VITE_QA_FOUNDRY_URL` to match it. Frontend talks to backend via the host network\n### `MODEL_PROVIDERS_CONFIG_PATH` is relative to cognita root dir\nMODELS_CONFIG_PATH=\"./models_config.yaml\"\nMETADATA_STORE_CONFIG='{\"provider\":\"prisma\"}'\nML_REPO_NAME=''\nVECTOR_DB_CONFIG='{\"provider\":\"qdrant\",\"url\":\"http://qdrant-server:6333\", \"config\": {\"grpc_port\": 6334, \"prefer_grpc\": false}}'\n# MONGO Example\n# VECTOR_DB_CONFIG='{\"provider\":\"mongo\",\"url\":\"connection_uri\", \"config\": {\"database_name\": \"cognita\"}}'\n# Milvus Example\n# VECTOR_DB_CONFIG='{\"provider\":\"Milvus\", \"url\":\"connection_uri\", \"api_key\":\"milvus_auth_token\", \"config\":{\"db_name\":\"cognita\", \"metric_type\":\"COSINE\"}}'\nCOGNITA_BACKEND_PORT=8000\n\nUNSTRUCTURED_IO_URL=http://unstructured-io-parsers:9500/\nUNSTRUCTURED_IO_API_KEY='test'\n\n## COGNITA_FRONTEND VARS\nCOGNITA_FRONTEND_PORT=5001\nVITE_QA_FOUNDRY_URL=http://localhost:8000\nVITE_DOCS_QA_DELETE_COLLECTIONS=true\nVITE_DOCS_QA_STANDALONE_PATH=/\nVITE_DOCS_QA_ENABLE_REDIRECT=false\nVITE_DOCS_QA_MAX_UPLOAD_SIZE_MB=200\n\n## OpenAI\nOPENAI_API_KEY=\n\n## OLLAMA VARS\nOLLAMA_MODEL=qwen2:1.5b\n\n## INFINITY VARS\nINFINITY_EMBEDDING_MODEL=mixedbread-ai/mxbai-embed-large-v1\nINFINITY_RERANKING_MODEL=mixedbread-ai/mxbai-rerank-xsmall-v1\n## INFINITY_API_KEY, only required if you enable API KEY auth on infinity container\nINFINITY_API_KEY='test'\n\n## TFY VARS\nTFY_API_KEY=\nTFY_HOST=\n\n## BRAVE\nBRAVE_API_KEY=\n\n## WHISPER\nWHISPER_PORT=10300\nWHISPER_MODEL=Systran/faster-distil-whisper-large-v3\n"
        },
        {
          "name": "deployment",
          "type": "tree",
          "content": null
        },
        {
          "name": "docker-compose.yaml",
          "type": "blob",
          "size": 6.318359375,
          "content": "version: \"3.8\"\nservices:\n  cognita-db:\n    image: postgres:13\n    container_name: cognita-postgres\n    restart: unless-stopped\n    ports:\n      - \"${POSTGRES_PORT}:5432\"\n    volumes:\n      - ./volumes/pgdata:/var/lib/postgresql/data\n    environment:\n      POSTGRES_USER: ${POSTGRES_USER}\n      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}\n      POSTGRES_DB: cognita-config\n    healthcheck:\n      test:\n        [\n          \"CMD-SHELL\",\n          \"sh -c 'pg_isready -U ${POSTGRES_USER} -d cognita-config'\",\n        ]\n      interval: 30s\n      timeout: 60s\n      retries: 5\n      start_period: 15s\n    networks:\n      - cognita-docker\n\n  ollama-server:\n    image: ollama/ollama:0.1.42\n    pull_policy: if_not_present\n    restart: unless-stopped\n    container_name: ollama\n    profiles:\n      - ollama\n    volumes:\n      - ./volumes/ollama:/root/.ollama\n    ports:\n      - 11434:11434\n    healthcheck:\n      test: [\"CMD\", \"ollama\", \"list\"]\n      interval: 30s\n      timeout: 5s\n      retries: 3\n      start_period: 60s\n    environment:\n      - OLLAMA_KEEP_ALIVE=24h\n      - OLLAMA_HOST=0.0.0.0\n      - OLLAMA_PORT=11434\n    entrypoint: /bin/bash\n    command: -c \"set -e; ollama serve & sleep 10 && ollama pull ${OLLAMA_MODEL} && sleep infinity\"\n    networks:\n      - cognita-docker\n\n  infinity-server:\n    image: michaelf34/infinity:0.0.63\n    pull_policy: if_not_present\n    restart: unless-stopped\n    container_name: infinity\n    profiles:\n      - infinity\n    ports:\n      - \"7997:7997\"\n    volumes:\n      - ./volumes/infinity:/app/.cache\n    healthcheck:\n      test: [\"CMD\", \"curl\", \"-f\", \"http://localhost:7997/health\"]\n      interval: 30s\n      timeout: 5s\n      retries: 10\n      start_period: 30s\n    environment:\n      - INFINITY_MODEL_ID=${INFINITY_EMBEDDING_MODEL};${INFINITY_RERANKING_MODEL}\n      - INFINITY_BATCH_SIZE=8\n      - INFINITY_API_KEY=${INFINITY_API_KEY}\n    command: v2\n    networks:\n      - cognita-docker\n\n  qdrant-server:\n    image: qdrant/qdrant:v1.8.4\n    pull_policy: if_not_present\n    restart: unless-stopped\n    container_name: qdrant\n    ports:\n      - 6333:6333\n      - 6334:6334\n    expose:\n      - 6333\n      - 6334\n      - 6335\n    healthcheck:\n      test: [\"CMD-SHELL\", \"/bin/bash -c ':> /dev/tcp/0.0.0.0/6333'\"]\n      interval: 30s\n      timeout: 5s\n      retries: 3\n      start_period: 10s\n    volumes:\n      - ./volumes/qdrant_storage:/qdrant/storage:z\n    networks:\n      - cognita-docker\n\n  unstructured-io-parsers:\n    # Docs: http://localhost:9500/general/docs\n    image: downloads.unstructured.io/unstructured-io/unstructured-api:0.0.73\n    pull_policy: if_not_present\n    restart: unless-stopped\n    container_name: unstructured-api\n    ports:\n      - 9500:9500\n    expose:\n      - 9500\n    environment:\n      - PORT=9500\n    healthcheck:\n      test:\n        [\n          \"CMD\",\n          \"wget\",\n          \"-O\",\n          \"/dev/null\",\n          \"-o\",\n          \"/dev/null\",\n          \"http://localhost:9500/healthcheck\",\n        ]\n      interval: 30s\n      timeout: 5s\n      retries: 3\n      start_period: 5s\n    networks:\n      - cognita-docker\n\n  # https://github.com/fedirz/faster-whisper-server/tree/master\n  faster-whisper:\n    image: fedirz/faster-whisper-server:latest-cpu\n    pull_policy: if_not_present\n    container_name: faster-whisper\n    restart: unless-stopped\n    healthcheck:\n      test: [\"CMD\", \"curl\", \"-f\", \"http://localhost:8000/health\"]\n      interval: 30s\n      timeout: 5s\n      retries: 3\n      start_period: 10s\n    environment:\n      - WHISPER__MODEL=${WHISPER_MODEL}\n      - WHISPER__INFERENCE_DEVICE=auto\n    ports:\n      - ${WHISPER_PORT}:8000\n    volumes:\n      - ./volumes/hugging_face_cache:/root/.cache/huggingface\n    networks:\n      - cognita-docker\n\n  cognita-backend:\n    build:\n      context: .\n      dockerfile: ./backend/Dockerfile\n    restart: unless-stopped\n    container_name: cognita-backend\n    ports:\n      - \"${COGNITA_BACKEND_PORT}:8000\"\n    depends_on:\n      cognita-db:\n        condition: service_healthy\n        restart: true\n      qdrant-server:\n        condition: service_healthy\n        restart: true\n      unstructured-io-parsers:\n        condition: service_healthy\n        restart: true\n    volumes:\n      - .:/app\n      - ./volumes/pgdata:/var/lib/postgresql/data\n      - ./volumes/user_data:/app/user_data\n    healthcheck:\n      test: [\"CMD\", \"curl\", \"-f\", \"http://localhost:8000/health-check\"]\n      interval: 30s\n      timeout: 5s\n      retries: 3\n      start_period: 10s\n    environment:\n      - DEBUG_MODE=true\n      - LOCAL=${LOCAL}\n      - PROCESS_POOL_WORKERS=${PROCESS_POOL_WORKERS}\n      - LOG_LEVEL=DEBUG\n      - ML_REPO_NAME=${ML_REPO_NAME}\n      - DATABASE_URL=postgresql://${POSTGRES_USER}:${POSTGRES_PASSWORD}@cognita-db:5432/cognita-config\n      - METADATA_STORE_CONFIG=${METADATA_STORE_CONFIG}\n      - VECTOR_DB_CONFIG=${VECTOR_DB_CONFIG}\n      - LOCAL_DATA_DIRECTORY=/app/user_data\n      - UNSTRUCTURED_IO_URL=${UNSTRUCTURED_IO_URL}\n      - UNSTRUCTURED_IO_API_KEY=${UNSTRUCTURED_IO_API_KEY}\n      - INFINITY_API_KEY=${INFINITY_API_KEY}\n      - MODELS_CONFIG_PATH=${MODELS_CONFIG_PATH}\n      - TFY_HOST=${TFY_HOST}\n      - TFY_API_KEY=${TFY_API_KEY}\n      - OPENAI_API_KEY=${OPENAI_API_KEY}\n      - BRAVE_API_KEY=${BRAVE_API_KEY}\n    entrypoint: /bin/bash\n    security_opt:\n      - seccomp:./seccomp.json\n    command: -c \"set -e; prisma db push --schema ./backend/database/schema.prisma && uvicorn --host 0.0.0.0 --port 8000 backend.server.app:app --reload\"\n    networks:\n      - cognita-docker\n\n  cognita-frontend:\n    build:\n      context: ./frontend\n      dockerfile: ./Dockerfile.dev\n      args:\n        - VITE_QA_FOUNDRY_URL=${VITE_QA_FOUNDRY_URL}\n        - VITE_DOCS_QA_DELETE_COLLECTIONS=${VITE_DOCS_QA_DELETE_COLLECTIONS}\n        - VITE_DOCS_QA_STANDALONE_PATH=${VITE_DOCS_QA_STANDALONE_PATH}\n        - VITE_DOCS_QA_ENABLE_REDIRECT=${VITE_DOCS_QA_ENABLE_REDIRECT}\n        - VITE_DOCS_QA_MAX_UPLOAD_SIZE_MB=${VITE_DOCS_QA_MAX_UPLOAD_SIZE_MB}\n        - VITE_USE_LOCAL=${LOCAL}\n    restart: unless-stopped\n    container_name: cognita-frontend\n    ports:\n      - \"${COGNITA_FRONTEND_PORT}:5001\"\n    volumes:\n      - ./frontend:/app\n      - /app/node_modules\n    depends_on:\n      cognita-backend:\n        condition: service_healthy\n        restart: true\n    entrypoint: /bin/bash\n    command: -c \"set -e; yarn install --frozen-lockfile && yarn dev --host 0.0.0.0\"\n    networks:\n      - cognita-docker\n\nnetworks:\n  cognita-docker:\n    external: false\n"
        },
        {
          "name": "docs",
          "type": "tree",
          "content": null
        },
        {
          "name": "frontend",
          "type": "tree",
          "content": null
        },
        {
          "name": "models_config.sample.yaml",
          "type": "blob",
          "size": 2.5498046875,
          "content": "model_providers:\n  ############################ Local ############################################\n  #   Uncomment this provider if you want to use local models providers         #\n  #   using ollama and infinity model server                                    #\n  ###############################################################################\n\n  - provider_name: local-ollama\n    api_format: openai\n    base_url: http://ollama-server:11434/v1/\n    api_key_env_var: \"\"\n    llm_model_ids:\n      - \"qwen2:1.5b\"\n    embedding_model_ids: []\n    reranking_model_ids: []\n    default_headers: {}\n\n  - provider_name: local-infinity\n    api_format: openai\n    base_url: http://infinity-server:7997/\n    api_key_env_var: INFINITY_API_KEY\n    llm_model_ids: []\n    embedding_model_ids:\n      - \"mixedbread-ai/mxbai-embed-large-v1\"\n    reranking_model_ids:\n      - \"mixedbread-ai/mxbai-rerank-xsmall-v1\"\n    default_headers: {}\n\n  - provider_name: faster-whisper\n    api_format: openai\n    base_url: http://faster-whisper:8000\n    api_key_env_var: \"\"\n    llm_model_ids: []\n    embedding_model_ids: []\n    reranking_model_ids: []\n    audio_model_ids:\n      - \"Systran/faster-distil-whisper-large-v3\"\n    default_headers: {}\n############################ OpenAI ###########################################\n#   Uncomment this provider if you want to use OpenAI as a models provider    #\n#   Remember to set `OPENAI_API_KEY` in container environment                 #\n###############################################################################\n\n# - provider_name: openai\n#   api_format: openai\n#   api_key_env_var: OPENAI_API_KEY\n#   llm_model_ids:\n#     - \"gpt-3.5-turbo\"\n#     - \"gpt-4o\"\n#   embedding_model_ids:\n#     - \"text-embedding-3-small\"\n#     - \"text-embedding-ada-002\"\n#   reranking_model_ids: []\n#   default_headers: {}\n\n############################ TrueFoundry ###########################################\n#   Uncomment this provider if you want to use TrueFoundry as a models provider    #\n#   Remember to set `TFY_API_KEY` in container environment                         #\n####################################################################################\n\n# - provider_name: truefoundry\n#   api_format: openai\n#   base_url: https://llm-gateway.truefoundry.com/api/inference/openai\n#   api_key_env_var: TFY_API_KEY\n#   llm_model_ids:\n#     - \"openai-main/gpt-4o-mini\"\n#     - \"openai-main/gpt-4-turbo\"\n#     - \"openai-main/gpt-3-5-turbo\"\n#   embedding_model_ids:\n#     - \"openai-main/text-embedding-3-small\"\n#     - \"openai-main/text-embedding-ada-002\"\n#   reranking_model_ids: []\n#   default_headers: {}\n"
        },
        {
          "name": "sample-data",
          "type": "tree",
          "content": null
        },
        {
          "name": "seccomp.json",
          "type": "blob",
          "size": 0.181640625,
          "content": "{\n  \"comment\": \"Allow create user namespaces\",\n  \"names\": [\n    \"clone\",\n    \"setns\",\n    \"unshare\"\n  ],\n  \"action\": \"SCMP_ACT_ALLOW\",\n  \"args\": [],\n  \"includes\": {},\n  \"excludes\": {}\n}\n"
        },
        {
          "name": "truefoundry.yaml",
          "type": "blob",
          "size": 12.923828125,
          "content": "type: application-set\nname: cognita-app-set\ncomponents:\n  # indexer\n  - name: cas-indexer\n    env:\n      LOG_LEVEL: DEBUG\n      DATABASE_URL: postgresql://admin:password@cas-db-postgresql.cognita-internal.svc.cluster.local:5432/cognita-config\n      VECTOR_DB_CONFIG: >-\n        {\"provider\":\"qdrant\",\"url\":\"http://cas-qdrant.cognita-internal.svc.cluster.local:6333\",\"api_key\":\"\"}\n      MODELS_CONFIG_PATH: ./models_config.truefoundry.yaml\n      METADATA_STORE_CONFIG: '{\"provider\":\"prisma\"}'\n      ML_REPO_NAME: cognita-internal\n      INFINITY_API_KEY: tfy-secret://internal:cognita:INFINITY_API_KEY\n      UNSTRUCTURED_IO_URL: http://cas-unstructured-io.cognita-internal.svc.cluster.local:8000\n      UNSTRUCTURED_IO_API_KEY: tfy-secret://internal:cognita:UNSTRUCTURED_IO_API_KEY\n    type: job\n    image:\n      type: build\n      build_spec:\n        type: dockerfile\n        command: >-\n          /bin/bash -c \"set -e; prisma generate --schema ./backend/database/schema.prisma && python -m backend.indexer.main  --collection_name {{collection_name}}\n          --data_source_fqn {{data_source_fqn}} --data_ingestion_run_name\n          {{data_ingestion_run_name}} --data_ingestion_mode {{data_ingestion_mode}}\n          --raise_error_on_failure  {{raise_error_on_failure}}\"\n        dockerfile_path: ./backend/Dockerfile\n        build_context_path: ./\n      build_source:\n        local_build: false\n        type: local\n    params:\n      - name: collection_name\n        param_type: string\n      - name: data_source_fqn\n        default: \"\"\n        param_type: string\n      - name: data_ingestion_run_name\n        default: \"\"\n        param_type: string\n      - name: data_ingestion_mode\n        default: INCREMENTAL\n        param_type: string\n      - name: raise_error_on_failure\n        default: \"False\"\n        param_type: string\n    retries: 0\n    trigger:\n      type: manual\n    resources:\n      node:\n        type: node_selector\n        capacity_type: spot_fallback_on_demand\n      cpu_limit: 1.5\n      cpu_request: 1\n      memory_limit: 1500\n      memory_request: 1000\n      ephemeral_storage_limit: 2000\n      ephemeral_storage_request: 1000\n\n  # Backend\n  - name: cas-backend\n    env:\n      DATABASE_URL: postgresql://admin:password@cas-db-postgresql.cognita-internal.svc.cluster.local:5432/cognita-config\n      LOG_LEVEL: DEBUG\n      VECTOR_DB_CONFIG: >-\n        {\"provider\":\"qdrant\",\"url\":\"http://cas-qdrant.cognita-internal.svc.cluster.local:6333\",\"api_key\":\"\"}\n      JOB_COMPONENT_NAME: cognita-internal-indexer\n      JOB_FQN: tfy-prod-euwe1:cognita-internal:cas-indexer\n      MODELS_CONFIG_PATH: ./models_config.truefoundry.yaml\n      METADATA_STORE_CONFIG: '{\"provider\":\"prisma\"}'\n      ML_REPO_NAME: cognita-internal\n      INFINITY_API_KEY: tfy-secret://internal:cognita:INFINITY_API_KEY\n      UNSTRUCTURED_IO_URL: http://cas-unstructured-io.cognita-internal.svc.cluster.local:8000\n      UNSTRUCTURED_IO_API_KEY: tfy-secret://internal:cognita:UNSTRUCTURED_IO_API_KEY\n      BRAVE_API_KEY: tfy-secret://internal:cognita:BRAVE_API_KEY\n    type: service\n    image:\n      type: build\n      build_spec:\n        type: dockerfile\n        command: /bin/bash -c \"set -e; prisma db push --schema ./backend/database/schema.prisma && uvicorn --host 0.0.0.0 --port 8000 backend.server.app:app\"\n        dockerfile_path: ./backend/Dockerfile\n        build_context_path: ./\n      build_source:\n        local_build: false\n        type: local\n    ports:\n      - host: cas.truefoundry.com\n        path: /api/\n        port: 8000\n        expose: true\n        protocol: TCP\n        app_protocol: http\n    replicas: 1\n    resources:\n      node:\n        type: node_selector\n        capacity_type: spot_fallback_on_demand\n      cpu_limit: 1\n      cpu_request: 0.5\n      memory_limit: 1000\n      memory_request: 500\n      ephemeral_storage_limit: 2000\n      ephemeral_storage_request: 1000\n    liveness_probe:\n      config:\n        path: /health-check\n        port: 8000\n        type: http\n      period_seconds: 60\n      timeout_seconds: 2\n      failure_threshold: 5\n      success_threshold: 1\n      initial_delay_seconds: 10\n    readiness_probe:\n      config:\n        path: /health-check\n        port: 8000\n        type: http\n      period_seconds: 60\n      timeout_seconds: 2\n      failure_threshold: 5\n      success_threshold: 1\n      initial_delay_seconds: 10\n    mounts:\n      - type: string\n        mount_path: /models_config.truefoundry.yaml\n        data: |\n          model_providers:\n            - provider_name: truefoundry\n              api_format: openai\n              base_url: https://llm-gateway.truefoundry.com/api/inference/openai\n              api_key_env_var: TFY_API_KEY\n              llm_model_ids:\n                - \"openai-main/gpt-4o-mini\"\n                - \"openai-main/gpt-4-turbo\"\n                - \"azure-openai/gpt-4\"\n                - \"together-ai/llama-3-70b-chat-hf\"\n              embedding_model_ids:\n                - \"openai-main/text-embedding-3-small\"\n                - \"openai-main/text-embedding-ada-002\"\n              reranking_model_ids: []\n              default_headers:\n                \"X-TFY-METADATA\": '{\"tfy_log_request\": \"true\", \"Custom-Metadata\": \"Cognita-LLM-Request\"}'\n\n            - provider_name: local-infinity\n              api_format: openai\n              base_url: http://cas-infinity.cognita-internal.svc.cluster.local:8000\n              api_key_env_var: INFINITY_API_KEY\n              llm_model_ids: []\n              embedding_model_ids:\n                - \"mixedbread-ai/mxbai-embed-large-v1\"\n              reranking_model_ids:\n                - \"mixedbread-ai/mxbai-rerank-xsmall-v1\"\n              default_headers: {}\n\n            - provider_name: faster-whisper\n              api_format: openai\n              base_url: http://cas-whisper.cognita-internal.svc.cluster.local:8000\n              api_key_env_var: \"\"\n              llm_model_ids: []\n              embedding_model_ids: []\n              reranking_model_ids: []\n              audio_model_ids:\n                - \"Systran/faster-distil-whisper-large-v3\"\n              default_headers: {}\n\n  # Frontend\n  - name: cas-frontend\n    type: service\n    image:\n      type: build\n      build_spec:\n        type: dockerfile\n        build_args:\n          VITE_DOCS_QA_DELETE_COLLECTIONS: \"true\"\n          VITE_QA_FOUNDRY_URL: https://cas.truefoundry.com/api\n          VITE_DOCS_QA_STANDALONE_PATH: /\n          VITE_DOCS_QA_ENABLE_STANDALONE: \"true\"\n          VITE_DOCS_QA_MAX_UPLOAD_SIZE_MB: 200\n        dockerfile_path: ./frontend/Dockerfile\n        build_context_path: ./frontend\n      build_source:\n        local_build: false\n        type: local\n    ports:\n      - host: cas.truefoundry.com\n        port: 5000\n        expose: true\n        protocol: TCP\n        app_protocol: http\n    replicas: 1\n    resources:\n      cpu_limit: 0.1\n      cpu_request: 0.05\n      memory_limit: 200\n      memory_request: 100\n      ephemeral_storage_limit: 200\n      ephemeral_storage_request: 100\n  - name: cas-qdrant\n    type: helm\n    source:\n      type: helm-repo\n      chart: qdrant\n      version: 0.8.4\n      repo_url: https://qdrant.github.io/qdrant-helm\n    values:\n      service:\n        type: ClusterIP\n        ports:\n          - name: http\n            port: 6333\n            protocol: TCP\n            targetPort: 6333\n            checksEnabled: true\n          - name: grpc\n            port: 6334\n            protocol: TCP\n            targetPort: 6334\n            checksEnabled: false\n          - name: http-p2p\n            port: 6335\n            protocol: TCP\n            targetPort: 6335\n            checksEnabled: false\n      persistence:\n        size: 50G\n      tolerations:\n        - key: kubernetes.azure.com/scalesetpriority\n          value: spot\n          effect: NoSchedule\n          operator: Equal\n        - key: cloud.google.com/gke-spot\n          value: \"true\"\n          effect: NoSchedule\n          operator: Equal\n      replicaCount: 2\n      fullnameOverride: cas-qdrant\n    kustomize:\n      additions:\n        - kind: VirtualService\n          spec:\n            http:\n              - match:\n                  - uri:\n                      prefix: /qdrant/\n                route:\n                  - destination:\n                      host: cas-qdrant.cognita-internal.svc.cluster.local\n                      port:\n                        number: 6333\n                rewrite:\n                  uri: /\n              - match:\n                  - headers:\n                      x-route-service:\n                        exact: qdrant-ui\n                route:\n                  - destination:\n                      host: cas-qdrant.cognita-internal.svc.cluster.local\n                      port:\n                        number: 6333\n                rewrite:\n                  uri: /\n            hosts:\n              - cas-qdrant-ui.truefoundry.com\n            gateways:\n              - istio-system/tfy-wildcard\n          metadata:\n            name: cas-qdrant\n            namespace: cognita-internal\n          apiVersion: networking.istio.io/v1alpha3\n\n  # Qdrant-UI\n  - name: cas-qdrant-ui\n    type: service\n    image:\n      type: build\n      build_source:\n        type: git\n        repo_url: https://github.com/truefoundry/qdrant-web-ui-new\n        branch_name: support-path-based-routing\n        ref: 038f5a4db22b54459e1820ab2ec51771f8f09919\n      build_spec:\n        type: dockerfile\n        dockerfile_path: ./Dockerfile\n        build_context_path: ./\n    ports:\n      - host: cas-qdrant-ui.truefoundry.com\n        path: /qdrant-ui/\n        port: 3000\n        expose: true\n        protocol: TCP\n        app_protocol: http\n    mounts: []\n    replicas: 1\n    resources:\n      node:\n        type: node_selector\n        capacity_type: spot_fallback_on_demand\n      cpu_request: 0.2\n      cpu_limit: 0.5\n      memory_request: 200\n      memory_limit: 500\n      ephemeral_storage_request: 1000\n      ephemeral_storage_limit: 2000\n\n  # Unstructured IO\n  - name: cas-unstructured-io\n    env:\n      UNSTRUCTURED_API_KEY: tfy-secret://internal:cognita:UNSTRUCTURED_IO_API_KEY\n    type: service\n    image:\n      type: image\n      image_uri: downloads.unstructured.io/unstructured-io/unstructured-api:0.0.73\n    ports:\n      - port: 8000\n        expose: false\n        protocol: TCP\n        app_protocol: http\n    mounts: []\n    replicas: 2\n    resources:\n      node:\n        type: node_selector\n        capacity_type: spot_fallback_on_demand\n      cpu_limit: 1.5\n      cpu_request: 0.8\n      memory_limit: 8000\n      memory_request: 4000\n      ephemeral_storage_limit: 2000\n      ephemeral_storage_request: 1500\n\n  # Infinity\n  - env:\n      PORT: \"8000\"\n      API_KEY: tfy-secret://internal:cognita:INFINITY_API_KEY\n      BATCH_SIZE: \"4\"\n    name: cas-infinity\n    type: service\n    image:\n      type: image\n      command: >-\n        infinity_emb v2 --model-id mixedbread-ai/mxbai-embed-large-v1 --model-id\n        mixedbread-ai/mxbai-rerank-xsmall-v1 --port $(PORT) --batch-size\n        $(BATCH_SIZE) --api-key $(API_KEY)\n      image_uri: michaelf34/infinity:0.0.63\n    ports:\n      - port: 8000\n        expose: false\n        protocol: TCP\n        app_protocol: http\n    mounts: []\n    replicas: 2\n    resources:\n      node:\n        type: node_selector\n        capacity_type: spot_fallback_on_demand\n      cpu_limit: 1\n      cpu_request: 0.8\n      memory_limit: 8000\n      memory_request: 4000\n      ephemeral_storage_limit: 2000\n      ephemeral_storage_request: 1500\n\n  # Database\n  - name: cas-db\n    type: helm\n    source:\n      type: oci-repo\n      oci_chart_url: oci://registry-1.docker.io/bitnamicharts/postgresql\n      version: 13.4.3\n    values:\n      auth:\n        database: cognita-config\n        password: password\n        username: admin\n        postgresPassword: password\n        enablePostgresUser: true\n      primary:\n        service:\n          ports:\n            postgresql: 5432\n        resources:\n          limits:\n            cpu: 100m\n            memory: 256Mi\n          requests:\n            cpu: 100m\n            memory: 256Mi\n        persistence:\n          size: 5Gi\n      architecture: standalone\n\n  # Whisper\n  - name: cas-whisper\n    env:\n      WHISPER_PORT: 8000\n      WHISPER__MODEL: Systran/faster-distil-whisper-large-v3\n      WHISPER__INFERENCE_DEVICE: auto\n    type: service\n    image:\n      type: image\n      image_uri: fedirz/faster-whisper-server:latest-cpu\n    ports:\n      - port: 8000\n        expose: false\n        protocol: TCP\n        app_protocol: http\n    mounts: []\n    replicas: 1\n    resources:\n      node:\n        type: node_selector\n        capacity_type: spot_fallback_on_demand\n      cpu_limit: 1\n      cpu_request: 0.8\n      memory_limit: 8000\n      memory_request: 4000\n      ephemeral_storage_limit: 4000\n      ephemeral_storage_request: 2500\n    liveness_probe:\n      config:\n        path: /health\n        port: 8000\n        type: http\n      period_seconds: 60\n      timeout_seconds: 2\n      failure_threshold: 5\n      success_threshold: 1\n      initial_delay_seconds: 10\n    readiness_probe:\n      config:\n        path: /health\n        port: 8000\n        type: http\n      period_seconds: 30\n      timeout_seconds: 2\n      failure_threshold: 5\n      success_threshold: 1\n      initial_delay_seconds: 10\n"
        }
      ]
    }
  ]
}