{
  "metadata": {
    "timestamp": 1736560389401,
    "page": 939,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjk1MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "X-PLUG/MobileAgent",
      "stars": 3263,
      "defaultBranch": "main",
      "files": [
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.0869140625,
          "content": "# Byte-compiled / optimized / DLL files\n__pycache__/\n\n# custom\n**/screenshot/*\n**/temp/*\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 1.037109375,
          "content": "MIT License\n\nCopyright (c) 2022 mPLUG\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n"
        },
        {
          "name": "Mobile-Agent-v2",
          "type": "tree",
          "content": null
        },
        {
          "name": "Mobile-Agent-v3",
          "type": "tree",
          "content": null
        },
        {
          "name": "Mobile-Agent",
          "type": "tree",
          "content": null
        },
        {
          "name": "PC-Agent",
          "type": "tree",
          "content": null
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 5.1357421875,
          "content": "![](assets/logo.png?v=1&type=image)\n<div align=\"center\">\n<h3>Mobile-Agent: The Powerful Mobile Device Operation Assistant Family<h3>\n<div align=\"center\">\n\t<a href=\"https://huggingface.co/spaces/junyangwang0410/Mobile-Agent\"><img src=\"https://huggingface.co/datasets/huggingface/badges/raw/main/open-in-hf-spaces-sm-dark.svg\" alt=\"Open in Spaces\"></a>\n\t<a href=\"https://modelscope.cn/studios/wangjunyang/Mobile-Agent-v2\"><img src=\"assets/Demo-ModelScope-brightgreen.svg\" alt=\"Demo ModelScope\"></a>\n  <a href=\"https://arxiv.org/abs/2401.16158\"><img src=\"https://img.shields.io/badge/Arxiv-2401.16158-b31b1b.svg?logo=arXiv\" alt=\"\"></a>\n  <a href=\"https://arxiv.org/abs/2406.01014 \"><img src=\"https://img.shields.io/badge/Arxiv-2406.01014-b31b1b.svg?logo=arXiv\" alt=\"\"></a>\n</div>\n<p align=\"center\">\n<a href=\"https://trendshift.io/repositories/7423\" target=\"_blank\"><img src=\"https://trendshift.io/api/badge/repositories/7423\" alt=\"MobileAgent | Trendshift\" style=\"width: 250px; height: 55px;\" width=\"250\" height=\"55\"/></a>\n</p>\n</div>\n\n<div align=\"center\">\n<a href=\"README.md\">English</a> | <a href=\"README_zh.md\">ç®€ä½“ä¸­æ–‡</a> | <a href=\"README_ja.md\">æ—¥æœ¬èª</a>\n<hr>\n</div>\n\n## ğŸ“ºDemo\n\n### Mobile-Agent-v3 (Note: The video is not accelerated)\n**YouTube**\n\n[![YouTube](https://img.youtube.com/vi/EMbIpzqJld0/0.jpg)](https://www.youtube.com/watch?v=EMbIpzqJld0)\n\n**Bilibili**\n\n[![Bilibili](https://img.youtube.com/vi/EMbIpzqJld0/0.jpg)](https://www.bilibili.com/video/BV1pPvyekEsa/?share_source=copy_web&vd_source=47ffcd57083495a8965c8cdbe1a751ae)\n\n### PC-Agent\n**Chrome and DingTalk**\n\nhttps://github.com/user-attachments/assets/b890a08f-8a2f-426d-9458-aa3699185030\n\n**Word**\n\nhttps://github.com/user-attachments/assets/37f0a0a5-3d21-4232-9d1d-0fe845d0f77d\n\n### Mobile-Agent-v2\nhttps://github.com/X-PLUG/MobileAgent/assets/127390760/d907795d-b5b9-48bf-b1db-70cf3f45d155\n\n### Mobile-Agent\nhttps://github.com/X-PLUG/MobileAgent/assets/127390760/26c48fb0-67ed-4df6-97b2-aa0c18386d31\n\n\n## ğŸ“¢News\n* ğŸ”¥ğŸ”¥[9.26] Mobile-Agent-v2 has been accepted by **The Thirty-eighth Annual Conference on Neural Information Processing Systems (NeurIPS 2024)**.\n* ğŸ”¥[8.23] We proposed PC-Agent, a **PC** operation assistant supporting both **Mac and Windows** platforms.\n* ğŸ”¥[7.29] Mobile-Agent won the **best demo award** at the ***The 23rd China National Conference on Computational Linguistics*** (CCL 2024). On the CCL 2024, we displayed the upcoming Mobile-Agent-v3. It has smaller memory overhead (8 GB), faster reasoning speed (10s-15s per operation), and all uses open source models. Video demo, please see the last section ğŸ“ºDemo.\n* [6.27] We proposed Demo that can upload mobile phone screenshots to experience Mobile-Agent-V2 in [Hugging Face](https://huggingface.co/spaces/junyangwang0410/Mobile-Agent) and [ModelScope](https://modelscope.cn/studios/wangjunyang/Mobile-Agent-v2). You donâ€™t need to configure models and devices, and you can experience it immediately.\n* [6. 4] Modelscope-Agent has supported Mobile-Agent-V2, based on Android Adb Env, please check in the [application](https://github.com/modelscope/modelscope-agent/tree/master/apps/mobile_agent).\n* [6. 4] We proposed Mobile-Agent-v2, a mobile device operation assistant with effective navigation via multi-agent collaboration.\n* [3.10] Mobile-Agent has been accepted by the **ICLR 2024 Workshop on Large Language Model (LLM) Agents**.\n\n## ğŸ“±Version\n* [Mobile-Agent-v3](Mobile-Agent-v3/README.md)\n* [Mobile-Agent-v2](Mobile-Agent-v2/README.md) - Mobile Device Operation Assistant with Effective Navigation via Multi-Agent Collaboration\n* [Mobile-Agent](Mobile-Agent/README.md) - Autonomous Multi-Modal Mobile Device Agent with Visual Perception\n\n## â­Star History\n[![Star History Chart](https://api.star-history.com/svg?repos=X-PLUG/MobileAgent&type=Date)](https://star-history.com/#X-PLUG/MobileAgent&Date)\n\n## ğŸ“‘Citation\nIf you find Mobile-Agent useful for your research and applications, please cite using this BibTeX:\n```\n@article{wang2024mobile2,\n  title={Mobile-Agent-v2: Mobile Device Operation Assistant with Effective Navigation via Multi-Agent Collaboration},\n  author={Wang, Junyang and Xu, Haiyang and Jia, Haitao and Zhang, Xi and Yan, Ming and Shen, Weizhou and Zhang, Ji and Huang, Fei and Sang, Jitao},\n  journal={arXiv preprint arXiv:2406.01014},\n  year={2024}\n}\n\n@article{wang2024mobile,\n  title={Mobile-Agent: Autonomous Multi-Modal Mobile Device Agent with Visual Perception},\n  author={Wang, Junyang and Xu, Haiyang and Ye, Jiabo and Yan, Ming and Shen, Weizhou and Zhang, Ji and Huang, Fei and Sang, Jitao},\n  journal={arXiv preprint arXiv:2401.16158},\n  year={2024}\n}\n```\n\n## ğŸ“¦Related Projects\n* [AppAgent: Multimodal Agents as Smartphone Users](https://github.com/mnotgod96/AppAgent)\n* [mPLUG-Owl & mPLUG-Owl2: Modularized Multimodal Large Language Model](https://github.com/X-PLUG/mPLUG-Owl)\n* [Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond](https://github.com/QwenLM/Qwen-VL)\n* [GroundingDINO: Marrying DINO with Grounded Pre-Training for Open-Set Object Detection](https://github.com/IDEA-Research/GroundingDINO)\n* [CLIP: Contrastive Language-Image Pretraining](https://github.com/openai/CLIP)\n"
        },
        {
          "name": "README_ja.md",
          "type": "blob",
          "size": 5.6455078125,
          "content": "![](assets/logo.png?v=1&type=image)\n<div align=\"center\">\n<h3>Mobile-Agent: å¼·åŠ›ãªãƒ¢ãƒã‚¤ãƒ«ãƒ‡ãƒã‚¤ã‚¹æ“ä½œã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆãƒ•ã‚¡ãƒŸãƒªãƒ¼<h3>\n<div align=\"center\">\n\t<a href=\"https://huggingface.co/spaces/junyangwang0410/Mobile-Agent\"><img src=\"https://huggingface.co/datasets/huggingface/badges/raw/main/open-in-hf-spaces-sm-dark.svg\" alt=\"Open in Spaces\"></a>\n\t<a href=\"https://modelscope.cn/studios/wangjunyang/Mobile-Agent-v2\"><img src=\"assets/Demo-ModelScope-brightgreen.svg\" alt=\"Demo ModelScope\"></a>\n  <a href=\"https://arxiv.org/abs/2401.16158\"><img src=\"https://img.shields.io/badge/Arxiv-2401.16158-b31b1b.svg?logo=arXiv\" alt=\"\"></a>\n  <a href=\"https://arxiv.org/abs/2406.01014 \"><img src=\"https://img.shields.io/badge/Arxiv-2406.01014-b31b1b.svg?logo=arXiv\" alt=\"\"></a>\n</div>\n<p align=\"center\">\n<a href=\"https://trendshift.io/repositories/7423\" target=\"_blank\"><img src=\"https://trendshift.io/api/badge/repositories/7423\" alt=\"MobileAgent | Trendshift\" style=\"width: 250px; height: 55px;\" width=\"250\" height=\"55\"/></a>\n</p>\n</div>\n\n<div align=\"center\">\n<a href=\"README_ja.md\">æ—¥æœ¬èª</a> | <a href=\"README.md\">English</a> | <a href=\"README_zh.md\">ç®€ä½“ä¸­æ–‡</a>\n<hr>\n</div>\n<!--\næ—¥æœ¬èª | [English](README.md) | [ç®€ä½“ä¸­æ–‡](README_zh.md)\n<hr>\n-->\n\n## ğŸ“ºãƒ‡ãƒ¢\n\n### Mobile-Agent-v3ï¼ˆæ³¨ï¼šãƒ“ãƒ‡ã‚ªã¯åŠ é€Ÿã•ã‚Œã¦ã„ã¾ã›ã‚“ï¼‰\n**YouTube**\n\n[![YouTube](https://img.youtube.com/vi/EMbIpzqJld0/0.jpg)](https://www.youtube.com/watch?v=EMbIpzqJld0)\n\n**Bilibili**\n\n[![Bilibili](https://img.youtube.com/vi/EMbIpzqJld0/0.jpg)](https://www.bilibili.com/video/BV1pPvyekEsa/?share_source=copy_web&vd_source=47ffcd57083495a8965c8cdbe1a751ae)\n\n### PC-Agent\n**Chrome and DingTalk**\n\nhttps://github.com/user-attachments/assets/b890a08f-8a2f-426d-9458-aa3699185030\n\n**Word**\n\nhttps://github.com/user-attachments/assets/37f0a0a5-3d21-4232-9d1d-0fe845d0f77d\n\n### Mobile-Agent-v2\nhttps://github.com/X-PLUG/MobileAgent/assets/127390760/d907795d-b5b9-48bf-b1db-70cf3f45d155\n\n### Mobile-Agent\nhttps://github.com/X-PLUG/MobileAgent/assets/127390760/26c48fb0-67ed-4df6-97b2-aa0c18386d31\n\n\n## ğŸ“¢ãƒ‹ãƒ¥ãƒ¼ã‚¹\n* ğŸ”¥ğŸ”¥[9.26] Mobile-Agent-v2 ã¯ **The Thirty-eighth Annual Conference on Neural Information Processing Systems (NeurIPS 2024)** ã«ã‚ˆã£ã¦æ‰¿èªã•ã‚Œã¾ã—ãŸã€‚\n* ğŸ”¥[8.23] Macã¨Windowsãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ ã«å¯¾å¿œã—ãŸPCæ“ä½œã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã€ŒPC-Agentã€ã‚’ãƒªãƒªãƒ¼ã‚¹ã—ã¾ã—ãŸã€‚\n* ğŸ”¥[7.29] Mobile-Agentã€***è¨ˆç®—è¨€èªå­¦ã«é–¢ã™ã‚‹ç¬¬23å›ä¸­å›½å…¨å›½ä¼šè­°***ï¼ˆCCL 2024ï¼‰ã§**ãƒ™ã‚¹ãƒˆãƒ‡ãƒ¢è³**ã‚’å—è³ã—ã¾ã—ãŸã€‚ CCL 2024ã§ã¯ã€ä»Šå¾Œã®Mobile-Agent-V3ã‚’ç¤ºã—ã¾ã—ãŸã€‚ãƒ¡ãƒ¢ãƒªã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰ï¼ˆ8 GBï¼‰ãŒå°ã•ãã€æ¨è«–é€Ÿåº¦ãŒé«˜ãï¼ˆæ“ä½œã‚ãŸã‚Š10S-15Sï¼‰ã€ã™ã¹ã¦ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ã¦ã„ã¾ã™ã€‚ãƒ“ãƒ‡ã‚ªãƒ‡ãƒ¢ã€ã‚»ã‚¯ã‚·ãƒ§ãƒ³ğŸ“ºDemoã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚\n* [6.27] [Hugging Face](https://huggingface.co/spaces/junyangwang0410/Mobile-Agent)ã¨[ModelScope](https://modelscope.cn/studios/wangjunyang/Mobile-Agent-v2)ã§ã€Mobile-Agent-v2ã®ãƒ‡ãƒ¢ã‚’å…¬é–‹ã—ã¾ã—ãŸã€‚æºå¸¯é›»è©±ã®ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ä½“é¨“ã§ãã¾ã™ã€‚ãƒ¢ãƒ‡ãƒ«ã‚„ãƒ‡ãƒã‚¤ã‚¹ã®è¨­å®šã¯ä¸è¦ã§ã™ã€‚\n* [6. 4] Modelscope-Agentã¯ã€Android Adb Envã«åŸºã¥ã„ã¦Mobile-Agent-V2ã‚’ã‚µãƒãƒ¼ãƒˆã—ã¦ã„ã¾ã™ã€‚è©³ç´°ã¯[ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³](https://github.com/modelscope/modelscope-agent/tree/master/apps/mobile_agent)ã‚’ã”è¦§ãã ã•ã„ã€‚\n* [6. 4] æ–°ä¸–ä»£ã®ãƒ¢ãƒã‚¤ãƒ«ãƒ‡ãƒã‚¤ã‚¹æ“ä½œã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆ Mobile-Agent-v2ã‚’ç™ºè¡¨ã—ã¾ã—ãŸã€‚ãƒãƒ«ãƒã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆå”åŠ›ã«ã‚ˆã‚ŠåŠ¹æœçš„ãªãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³ã‚’å®Ÿç¾ã—ã¾ã™ã€‚\n* [3.10] Mobile-Agentã¯**ICLR 2024 Workshop on Large Language Model (LLM) Agents**ã«æ¡æŠã•ã‚Œã¾ã—ãŸã€‚\n\n## ğŸ“±ãƒãƒ¼ã‚¸ãƒ§ãƒ³\n* [Mobile-Agent-v3](Mobile-Agent-v3/README.md)\n* [Mobile-Agent-v2](Mobile-Agent-v2/README_ja.md) - ãƒãƒ«ãƒã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆå”åŠ›ã«ã‚ˆã‚‹åŠ¹æœçš„ãªãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³ã‚’å®Ÿç¾ã™ã‚‹ãƒ¢ãƒã‚¤ãƒ«ãƒ‡ãƒã‚¤ã‚¹æ“ä½œã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆ\n* [Mobile-Agent](Mobile-Agent/README_ja.md) - è¦–è¦šèªè­˜ã‚’å‚™ãˆãŸè‡ªå¾‹å‹ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ãƒ¢ãƒã‚¤ãƒ«ãƒ‡ãƒã‚¤ã‚¹ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ\n\n## â­ã‚¹ã‚¿ãƒ¼å±¥æ­´\n[![Star History Chart](https://api.star-history.com/svg?repos=X-PLUG/MobileAgent&type=Date)](https://star-history.com/#X-PLUG/MobileAgent&Date)\n\n## ğŸ“‘å¼•ç”¨\nMobile-AgentãŒç ”ç©¶ã‚„ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã«å½¹ç«‹ã¤å ´åˆã¯ã€æ¬¡ã®BibTeXã‚’ä½¿ç”¨ã—ã¦å¼•ç”¨ã—ã¦ãã ã•ã„ï¼š\n```\n@article{wang2024mobile2,\n  title={Mobile-Agent-v2: Mobile Device Operation Assistant with Effective Navigation via Multi-Agent Collaboration},\n  author={Wang, Junyang and Xu, Haiyang and Jia, Haitao and Zhang, Xi and Yan, Ming and Shen, Weizhou and Zhang, Ji and Huang, Fei and Sang, Jitao},\n  journal={arXiv preprint arXiv:2406.01014},\n  year={2024}\n}\n\n@article{wang2024mobile,\n  title={Mobile-Agent: Autonomous Multi-Modal Mobile Device Agent with Visual Perception},\n  author={Wang, Junyang and Xu, Haiyang and Ye, Jiabo and Yan, Ming and Shen, Weizhou and Zhang, Ji and Huang, Fei and Sang, Jitao},\n  journal={arXiv preprint arXiv:2401.16158},\n  year={2024}\n}\n```\n\n## ğŸ“¦é–¢é€£ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ\n* [AppAgent: Multimodal Agents as Smartphone Users](https://github.com/mnotgod96/AppAgent)\n* [mPLUG-Owl & mPLUG-Owl2: Modularized Multimodal Large Language Model](https://github.com/X-PLUG/mPLUG-Owl)\n* [Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond](https://github.com/QwenLM/Qwen-VL)\n* [GroundingDINO: Marrying DINO with Grounded Pre-Training for Open-Set Object Detection](https://github.com/IDEA-Research/GroundingDINO)\n* [CLIP: Contrastive Language-Image Pretraining](https://github.com/openai/CLIP)\n"
        },
        {
          "name": "README_zh.md",
          "type": "blob",
          "size": 5.0654296875,
          "content": "![](assets/logo.png?v=1&type=image)\n<div align=\"center\">\n<h3>Mobile-Agent: å¼ºå¤§çš„ç§»åŠ¨è®¾å¤‡æ“ä½œåŠ©æ‰‹å®¶æ—<h3>\n<div align=\"center\">\n\t<a href=\"https://huggingface.co/spaces/junyangwang0410/Mobile-Agent\"><img src=\"https://huggingface.co/datasets/huggingface/badges/raw/main/open-in-hf-spaces-sm-dark.svg\" alt=\"Open in Spaces\"></a>\n\t<a href=\"https://modelscope.cn/studios/wangjunyang/Mobile-Agent-v2\"><img src=\"assets/Demo-ModelScope-brightgreen.svg\" alt=\"Demo ModelScope\"></a>\n  <a href=\"https://arxiv.org/abs/2401.16158\"><img src=\"https://img.shields.io/badge/Arxiv-2401.16158-b31b1b.svg?logo=arXiv\" alt=\"\"></a>\n  <a href=\"https://arxiv.org/abs/2406.01014 \"><img src=\"https://img.shields.io/badge/Arxiv-2406.01014-b31b1b.svg?logo=arXiv\" alt=\"\"></a>\n</div>\n<p align=\"center\">\n<a href=\"https://trendshift.io/repositories/7423\" target=\"_blank\"><img src=\"https://trendshift.io/api/badge/repositories/7423\" alt=\"MobileAgent | Trendshift\" style=\"width: 250px; height: 55px;\" width=\"250\" height=\"55\"/></a>\n</p>\n</div>\n\n<div align=\"center\">\n<a href=\"README_zh.md\">ç®€ä½“ä¸­æ–‡</a> | <a href=\"README.md\">English</a> | <a href=\"README_ja.md\">æ—¥æœ¬èª</a>\n<hr>\n</div>\n<!--\nç®€ä½“ä¸­æ–‡ | [English](README.md) | [æ—¥æœ¬èª](README_ja.md)\n<hr>\n-->\n\n## ğŸ“ºDemo\n\n### Mobile-Agent-v3ï¼ˆæ³¨æ„ï¼šè¯¥è§†é¢‘æ²¡æœ‰åŠ é€Ÿå¤„ç†ï¼‰\n**YouTube**\n\n[![YouTube](https://img.youtube.com/vi/EMbIpzqJld0/0.jpg)](https://www.youtube.com/watch?v=EMbIpzqJld0)\n\n**å“”å“©å“”å“©**\n\n[![Bilibili](https://img.youtube.com/vi/EMbIpzqJld0/0.jpg)](https://www.bilibili.com/video/BV1pPvyekEsa/?share_source=copy_web&vd_source=47ffcd57083495a8965c8cdbe1a751ae)\n\n### PC-Agent\n**è°·æ­Œæµè§ˆå™¨ä¸é’‰é’‰**\n\nhttps://github.com/user-attachments/assets/b890a08f-8a2f-426d-9458-aa3699185030\n\n**Word**\n\nhttps://github.com/user-attachments/assets/37f0a0a5-3d21-4232-9d1d-0fe845d0f77d\n\n### Mobile-Agent-v2\nhttps://github.com/X-PLUG/MobileAgent/assets/127390760/d907795d-b5b9-48bf-b1db-70cf3f45d155\n\n### Mobile-Agent\nhttps://github.com/X-PLUG/MobileAgent/assets/127390760/26c48fb0-67ed-4df6-97b2-aa0c18386d31\n\n\n## ğŸ“¢æ–°é—»\n* ğŸ”¥ğŸ”¥[9.26] Mobile-Agent-v2 è¢« **The Thirty-eighth Annual Conference on Neural Information Processing Systems (NeurIPS 2024)** æ¥æ”¶ã€‚\n* ğŸ”¥[8.23]æˆ‘ä»¬å‘å¸ƒäº†ä¸€ä¸ªæ”¯æŒMacå’ŒWindowså¹³å°çš„**PC**æ“ä½œåŠ©æ‰‹PC-Agent, é€šè¿‡Mobile-Agent-v2æ¡†æ¶å®ç°ã€‚\n* ğŸ”¥[7.29] Mobile-Agentè·å¾—äº† ***ç¬¬äºŒåä¸‰å±Šä¸­å›½è®¡ç®—è¯­è¨€å­¦å¤§ä¼š*** (CCL 2024) çš„ **æœ€ä½³demoå¥–é¡¹**ã€‚åœ¨CCL 2024ä¸Šï¼Œæˆ‘ä»¬å±•ç¤ºäº†å³å°†å¼€æºçš„Mobile-Agent-v3ï¼Œæ‹¥æœ‰æ›´å°çš„å†…å­˜å¼€é”€ï¼ˆ8 GBï¼‰ã€æ›´å¿«çš„æ¨ç†é€Ÿåº¦ï¼ˆæ¯æ¬¡æ“ä½œ10-15ç§’ï¼‰ï¼Œå¹¶ä¸”ä½¿ç”¨å¼€æºæ¨¡å‹ã€‚è§†é¢‘Demoè¯·è§ä¸Šä¸€ä¸ªæ¿å—ğŸ“ºDemoã€‚\n* [6.27] æˆ‘ä»¬åœ¨[Hugging Face](https://huggingface.co/spaces/junyangwang0410/Mobile-Agent)å’Œ[ModelScope](https://modelscope.cn/studios/wangjunyang/Mobile-Agent-v2)å‘å¸ƒäº†å¯ä»¥ä¸Šä¼ æ‰‹æœºæˆªå›¾ä½“éªŒMobile-Agent-v2çš„Demoï¼Œæ— éœ€é…ç½®æ¨¡å‹å’Œè®¾å¤‡ï¼Œå³åˆ»ä¾¿å¯ä½“éªŒã€‚\n* [6. 4] Modelscope-Agent å·²ç»æ”¯æŒ Mobile-Agent-V2ï¼ŒåŸºäº Android Adb Envï¼Œè¯·æŸ¥çœ‹ [application](https://github.com/modelscope/modelscope-agent/tree/master/apps/mobile_agent)ã€‚\n* [6. 4] æˆ‘ä»¬å‘å¸ƒäº†æ–°ä¸€ä»£ç§»åŠ¨è®¾å¤‡æ“ä½œåŠ©æ‰‹ Mobile-Agent-v2, é€šè¿‡å¤šæ™ºèƒ½ä½“åä½œå®ç°æœ‰æ•ˆå¯¼èˆªã€‚\n* [3.10] Mobile-Agent è¢« **ICLR 2024 Workshop on Large Language Model (LLM) Agents** æ¥æ”¶ã€‚\n\n## ğŸ“±ç‰ˆæœ¬\n* [Mobile-Agent-v3](Mobile-Agent-v3/README_zh.md)\n* [Mobile-Agent-v2](Mobile-Agent-v2/README_zh.md) - é€šè¿‡å¤šä»£ç†åä½œæœ‰æ•ˆå¯¼èˆªçš„ç§»åŠ¨è®¾å¤‡æ“ä½œåŠ©æ‰‹\n* [Mobile-Agent](Mobile-Agent/README_zh.md) - è§†è§‰æ„ŸçŸ¥æ–¹æ¡ˆçš„è‡ªåŠ¨åŒ–ç§»åŠ¨è®¾å¤‡æ“ä½œæ™ºèƒ½ä½“\n\n## â­Starå†å²\n[![Star History Chart](https://api.star-history.com/svg?repos=X-PLUG/MobileAgent&type=Date)](https://star-history.com/#X-PLUG/MobileAgent&Date)\n\n## å¼•ç”¨\nIf you find Mobile-Agent useful for your research and applications, please cite using this BibTeX:\n```\n@article{wang2024mobile2,\n  title={Mobile-Agent-v2: Mobile Device Operation Assistant with Effective Navigation via Multi-Agent Collaboration},\n  author={Wang, Junyang and Xu, Haiyang and Jia Haitao and Zhang Xi and Yan, Ming and Shen, Weizhou and Zhang, Ji and Huang, Fei and Sang, Jitao},\n  journal={arXiv preprint arXiv:2406.01014},\n  year={2024}\n}\n\n@article{wang2024mobile,\n  title={Mobile-Agent: Autonomous Multi-Modal Mobile Device Agent with Visual Perception},\n  author={Wang, Junyang and Xu, Haiyang and Ye, Jiabo and Yan, Ming and Shen, Weizhou and Zhang, Ji and Huang, Fei and Sang, Jitao},\n  journal={arXiv preprint arXiv:2401.16158},\n  year={2024}\n}\n```\n\n## ğŸ“¦ç›¸å…³é¡¹ç›®\n* [AppAgent: Multimodal Agents as Smartphone Users](https://github.com/mnotgod96/AppAgent)\n* [mPLUG-Owl & mPLUG-Owl2: Modularized Multimodal Large Language Model](https://github.com/X-PLUG/mPLUG-Owl)\n* [Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond](https://github.com/QwenLM/Qwen-VL)\n* [GroundingDINO: Marrying DINO with Grounded Pre-Training for Open-Set Object Detection](https://github.com/IDEA-Research/GroundingDINO)\n* [CLIP: Contrastive Language-Image Pretraining](https://github.com/openai/CLIP)\n"
        },
        {
          "name": "assets",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}