{
  "metadata": {
    "timestamp": 1736559490226,
    "page": 62,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjcw",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "google/grr",
      "stars": 4819,
      "defaultBranch": "master",
      "files": [
        {
          "name": ".allstar",
          "type": "tree",
          "content": null
        },
        {
          "name": ".dockerignore",
          "type": "blob",
          "size": 0.548828125,
          "content": "# Unfortunately the ** syntax isn't supported before docker 1.10\n# https://github.com/docker/docker/issues/13113\n# and installing a newer version is difficult.\n*.changes\n*.deb\n*.dsc\n*.egg-info\n*/*/server.local.yaml\n*.pyc\n*/*.pyc\n*/*/*.pyc\n*/*/*/*.pyc\n*/*/*/*/*.pyc\n*_pb2.*\n*/*_pb2.*\n*/*/*_pb2.*\nACKNOWLEDGEMENTS\nAUTHORS\nbuild\ndist\nexecutables\n!executables/windows/templates/unzipsfx/*.exe\ngrr/config/grr_response_templates\ngrr/gui/static/bower_components\ngrr/gui/static/node_modules\ngrr/gui/static/tmp\ngrr/var\ngrr_server*.tar.gz\nLICENSE\nREADME.md\ntravis\nvagrant\n"
        },
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.3623046875,
          "content": "*.pyc\n*_pb2.py\n*.swp\n*.o\n*.so\n*.pb.cc\n*.pb.h\n*~\n.idea/\nbuild/\ndist/\nartifacts/*.yaml\ngrr.egg-info/\nget-pip.py\ngrr/core/grr_response_core/artifacts/*.yaml\n*.egg-info/\n.eggs/\nnode_modules/\ntmp/\ngrr/server/grr_response_server/gui/static/fonts/\ninstallers/\nGRRlog.txt\n*.log\ngrr/server/grr_response_server/gui/static/third-party\ngrr/server/grr_response_server/gui/ui/.angular\n"
        },
        {
          "name": "ACKNOWLEDGEMENTS",
          "type": "blob",
          "size": 1.1923828125,
          "content": "Please see the AUTHORS file for core developer details.\n\nWe want to thank the following people for contributing to GRR (all\nlists in alphabetical order):\n\n* Adam Sindelar <adam.sindelar@gmail.com>\n* Alexandru-Cosmin Mihai <alexcosmin.mihai@gmail.com>\n* Andrew Krug <andrewkrug@gmail.com>\n* Brian Olson <brian@hurrikane.net>\n* Daniel Berbece <daniel.berbece97@gmail.com>\n* Daniel White <dmwhite@google.com>\n* Elizabeth Schweinsberg <eschwein@google.com>\n* Eric Mak <ericmak@gmail.com>\n* Gilad Tsehori <gilad@tsehori.com>\n* Jessica Wilson <jawilson0502@gmail.com>\n* Joachim Metz <joachim.metz@gmail.com>\n* Johan Berggren <jberggren@gmail.com>\n* Jordi Sanchez <parki.san@gmail.com>\n* Kristinn Gudjonsson <kristinn@log2timeline.net>\n* Matthew Iselin <miselin@google.com>\n* Paul Chaignon <paul.chaignon@gmail.com>\n* Ryan Peck <ryan@rypeck.com>\n* Sean Gillespie <sean.gillespie.32@gmail.com>\n* Sebastian Welsh <sebastian.welsh@gmail.com>\n\nThanks go to the following people for providing documentation\nimprovements:\n\n* Ben Uphoff\n* Jordi Sanchez <parki.san@gmail.com>\n* Sebastian Welsh <sebastian.welsh@gmail.com>\n\nWe also want to thank the many people who have taken the time to send\nus bug reports when something went wrong!\n\n"
        },
        {
          "name": "AUTHORS",
          "type": "blob",
          "size": 0.6640625,
          "content": "GRR Rapid Response Framework\nhttps://github.com/google/grr\n\nGRR is developed by (in alphabetical order):\n\n* Mikhail Bushkov <realbushman@gmail.com>\n* Ben Galehouse <bgalehouse@gmail.com>\n* Łukasz Hanuszczak <hanuszczak@google.com>\n* Andreas Moser <grrrrrrrrr@surfsup.at>\n* Denver Ogaro <ogaro@google.com>\n* Max Vogler <maxvo@google.com>\n\nTo reach the authors, please use the GRR development mailing\nlist <grr-dev@googlegroups.com>.\n\nGRR emeriti:\n\n* Darren Bilby <darrenbilby@gmail.com>\n* Germano Caronni <caronni@gmail.com>\n* Greg Castle <github@mailgreg.com>\n* Michael Cohen <scudette@gmail.com>\n* Milosz Lakomy <milosz.lakomy@gmail.com>\n* Dionysis Zindros <dionyziz@gmail.com>\n"
        },
        {
          "name": "CHANGELOG.md",
          "type": "blob",
          "size": 13.9931640625,
          "content": "# Changelog (important and/or breaking changes).\n\nThe format is based on [Keep a Changelog](https://keepachangelog.com/en/1.0.0/),\nand this project adheres to [Semantic Versioning](https://semver.org/spec/v2.0.0.html).\n\n## [Unreleased]\n\n### Added\n\n* Added support for listing `%SystemDrive%\\Users` as a supplementary mechanism\n  for collecting user profiles on Windows (additionally to using data from the\n  registry).\n\n### Removed\n\n* Removed the `ListFlowApplicableParsers` API method.\n* Removed the `ListParsedFlowResults` API method.\n* Removed support for the `GREP` artifact source (these were internal to GRR and\n  not part of the [official specification](https://artifacts.readthedocs.io/en/latest/sources/Format-specification.html).\n\n## [3.4.7.4] - 2024-05-28\n\n### Removed\n\n* Removed support for Chipsec based flows.\n* Removed ClientArtifactCollector flow and related client actions.\n* Removed indexing endpoints on snapshot `uname` (searching is still possible\n  by individual and combination of system name, release and version).\n* Removed support for foreman rules using `uname` of an endpoint (this can be\n  simulated by using 3 rules for system name, release and version).\n* Removed the `provides` field from the `Artifact` message. This change has been\n  done in anticipation of the removal of the same field from the official GitHub\n  repository (ForensicArtifacts/artifacts#275).\n* **GRR server Debian package**. We stopped providing the GRR server Debian\n  package as the main way of distributing GRR server and client binaries.\n  Instead we  make GRR Docker image a preferred way for running GRR in a\n  demo or production environment.  See the documentation [here](https://grr-doc.readthedocs.io/en/latest/installing-and-running-grr/via-docker-compose.html).\n* **Artifact parsers**. ArtifactCollector flow supported parsing collected files\n  and output of executed commands. Its parsers were not properly maintained,\n  were often outdated and fragile. We're converted selected parsers\n  into standalone flows (`CollectDistroInfo`, `CollectInstalledSoftware`, `CollectHardwareInfo`) and removed the artifact parsing subsystem.\n  The ArtifactCollector now works as if \"apply_parsers\" arguments\n  attribute is set to False. At some point the \"apply_parsers\" attribute will be\n  deprecated completely.\n\n### Added\n* GRR docker image which contains all grr server components and client\n  templates. It is available for every new GRR version for download at\n  https://github.com/google/grr/pkgs/container/grr\n* Docker compose configuration file to run all GRR/Fleetspeak components in\n  separate Docker containers.\n* Python API was extended by a function (`DecodeCrowdStrikeQuarantineEncoding`)\n  to decode a crowdstrike quarantine encoded file, given as a\n  `BinaryChunkIterator`.\n\n### Fixed\n\n* YARA memory scanning improvements (matching context options, consuming less bandwidth).\n\n### API removed\n\n* GetClientLoadStats API method (`/api/clients/<client_id>/load-stats/<metric>`).\n  Client load stats collection functionality was removed from GRR, as\n  it was rarely used and Fleetspeak already collects basic client stats anyway.\n  Instead of fixing/maintaining the GRR client load stats logic, we will\n  better to invest into Fleetspeak's client load stats enhancements.\n* ApiReportData definition (used by GetReport, `/api/stats/reports/<name>`)\n  changed: support for stack, line and pie charts removed. All stack/line/pie\n  chart report plugins removed (namely: GRRVersion1ReportPlugin,\n  GRRVersion7ReportPlugin, GRRVersion30ReportPlugin, LastActiveReportPlugin,\n  OSBreakdown1ReportPlugin, OSBreakdown7ReportPlugin, OSBreakdown14ReportPlugin,\n  OSBreakdown30ReportPlugin, OSReleaseBreakdown1ReportPlugin,\n  OSReleaseBreakdown7ReportPlugin, OSReleaseBreakdown14ReportPlugin,\n  OSReleaseBreakdown30ReportPlugin, SystemFlowsReportPlugin,\n  UserFlowsReportPlugin, MostActiveUsersReportPlugin, UserActivityReportPlugin).\n* GetFileDecoders API method\n  (`/api/clients/<client_id>/vfs-decoders/<path:file_path>`). Getting file\n  decoders functionality was removed as it was not used before.\n* GetDecodedFileBlob API method (`/api/clients/<client_id>/vfs-decoded-blob/`).\n  Get decoded file blob functionality was removed as it was unused before. Only\n  one decoder for decoding crowdstrike quarantine encoded files was implemented,\n  this functionality is now exposed via the Python API.\n\n### Planned for removal\n\n* **Built-in cron jobs**. Built-in cron jobs are primarily used for periodic\n  hunts. We will provide documentation on how to easily replicate the\n  current functionality using external scheduling systems (like Linux cron,\n  for example).\n\n  If your workflow depends on GRR built in cron jobs and you anticipate problems\n  when migrating it to external schedulers, please reach out to us via email\n  or GitHub.\n\n## [3.4.7.1] - 2023-10-23\n\n### Added\n\n* Created a flow for collecting an identifier of the CrowdStrike agent.\n* Podman-based zero-setup development environment.\n* Added StatMultipleFiles and HashMultipleFiles flows to be used in\n  UIv2.\n\n### Changed\n\n* Renamed AdminUI.new_hunt_wizard.default_output_plugin to\n  AdminUI.new_hunt_wizard.default_output_plugins (note the \"s\" in the end).\n  The new option accepts a comma-separated list of names.\n* Newly interrogated clients now pick up active hunts automatically.\n* Hunts workflow is now available in the new UI: creating hunts from a flow,\n  duplicating existing hunts, monitoring hunt progress and inspecting results.\n\n### Removed\n\n* Fully removed deprecated use_tsk flag.\n* Removed deprecated plugin_args field from OutputPluginDescriptor.\n* Removed deprecated flows: FingerprintFile, KeepAlive, FingerprintFile, FindFiles, SendFile, Uninstall,\n  UpdateClient, CollectEfiHashes, DumpEfiImage.\n* Deprecated GetFile flow in favor of MultiGetFile.\n* Made FileFinder an alias to ClientFileFinder, using ClientFileFinder\n  by default everywhere. Legacy FileFinder is still available as\n  LegacyFileFinder. Fixed several inconsistencies in ClientFileFinder\n  client action. Same for RegistryFinder.\n* Removed deprecated client actions: EficheckCollectHashes, EficheckDumpImage, Uninstall, SendFile.\n* Removed \"Checks\" functionality.\n\n### API removed\n\n* Deprecated no-op \"keep_client_alive\" attribute in ApiCreateClientApprovalArgs.\n* Deprecated ListClientActionRequests API call (was no-op after Fleetspeak migration).\n\n## [3.4.6.7] - 2023-03-22\n\n### API removed\n\n* Removed the `labels` field from the `Artifact` message. This change has been\n  done in anticipation of the removal of the same field from the official spec\n  of [Forensic Artifacts](https://artifacts.readthedocs.io/en/latest/).\n\n### Added\n\n* Introduced Server.grr_binaries_readonly configuration option (set to False\n  by default). When set to True, binaries and python hacks can't be overriden\n  or deleted.\n* Added configuration option Monitoring.http_address to specify server address\n  of stats server. Default value will remain 127.0.0.1.\n\n\n### Changed\n\n* Updates elasticsearch output plugin post request to _bulk in the\n  elasticsearch api. Adds a terminating \\n and content type headers for\n  application/json.\n\n## [3.4.3.1] - 2021-05-19\n\n### API added\n\n* Introduced KillFleetspeak, RestartFleetspeakGrrService,\n  DeleteFleetspeakPendingMessages, GetFleetspeakPendingMessages,\n  GetFleetspeakPendingMessageCount API methods to provide Fleetspeak-specific\n  capabilities for Fleetspeak-enabled clients.\n* Introduced ListParsedFlowResults and ListFlowApplicableParsers API methods\n  for on-demand artifacts parsing.\n\n### Added\n\n* Introduced Hunt.default_client_rate configuration option.\n\n## [3.4.2.4] - 2020-10-15\n\n### API added\n\n* `GetVersion` method was introduced. It returns information about version of\n  the GRR server.\n* API shell now validates GRR server version and if it discovers that the server\n  is newer than the API client, it will fail on startup. One can bypass this\n  behaviour by using the `--no-check-version` flag.\n\n### API removed\n\n* ListAff4AttributeDescriptors API method (/api/reflection/aff4/attributes)\n  was removed.\n* Support for exporting binary data in the BigQuery output plugin has been\n  removed.\n\n### API changed\n\n* `GetFileDetails` now raises if called on non-existing paths instead of\n  returning a dummy result.\n* `GetVfsFilesArchive` now raises if called on non-existing paths instead of\n  returning an empty archive.\n* All GRR Protocol Buffers messages now have proper package declarations. It\n  means that type URLs of all messages now changed. The Python API client is\n  able to handle legacy type URLs, but if you use raw API calls, makes sure it\n  does not break your workflow.\n\n### Changed\n\n* The server YAML configuration options path_globs_blacklist and\n  path_globs_whitelist in get_flow_files_archive of router_params of\n  ApiCallRobotRouter have been renamed to exclude_path_globs and\n  include_only_path_globs.\n* The server YAML configuration option Artifacts.netgroup_user_blacklist has\n  been renamed to Artifacts.netgroup_ignore_users.\n* The server YAML configuration options labels_whitelist and\n  labels_owners_whitelist in router_params of ApiLabelsRestrictedCallRouter\n  have been renamed to allow_labels and allow_labels_owners.\n* The server YAML configuration option artifacts_whitelist of\n  artifact_collector_flow of router_params of ApiCallRobotRouter has been\n  renamed to allow_artifacts.\n* The `ExecutePythonHack` flow returns a `ExecutePythonHackResponse` message\n  rather than raw string object as a response.\n* ApiHunt.hunt_type was introduced and should be used instead of\n  a now-deprecated ApiHunt.name.\n* Variable hunts now have their arguments filled in the ApiHunt.flow_args\n  attribute.\n* JSON representation of `st_ino`, `st_dev`, `st_nlink`, `st_blocks`,\n  `st_blksize`, `st_rdev` fields of `StatEntry` now use strings rather than\n  integers. This is a consequence of increasing the supported integer size of\n  these values which might be out of bounds for JSON numbers.\n* The `st_crtime` field of `StatEntry` has been renamed to `st_btime`.\n* ArtifactCollectorFlowArgs, ArtifactFilesDownloaderFlowArgs:\n  * use_tsk is replaced with use_raw_filesystem_access\n  * use_tsk is kept for compatibility until 2021-04-01\n    * please migrate away from use_tsk to use_raw_filesystem_access until then\n  * ValueError is raised if both fields are set\n\n## Removed\n\n* WinUserActivityInvestigationArgs:\n  * This message is obsolete, removing it.\n* ClientArtifactCollectorArgs\n  * Removing use_tsk, since it hasn't been used on the client side\n\n## [3.3.0.0] - 2019-05-22\n\n### API changed\n\n* ListFlows no longer includes \"args\" attributes into the returned flows.\n* ListFlowOutputPluginsLogs, ListFlowOutputPluginErrors,\n  ListHuntOutputPluginLogs and ListHuntOutputPluginErrors API calls now always\n  report batch_index and batch_size as 0 and no longer include PluginDescriptor\n  into the reply.\n\n### API removed\n\n* ListHuntCrashes method no longer accepts \"filter\" argument.\n* ListHunts no longer fills \"total_count\" attribute of ApiListHuntsResult.\n* `ApiHunt` no longer has an `expires` field. Instead, `duration` field has\n  been added which can be used to calculate expiry date:\n  `start_time + duration`. Note that if the hunt hasn't been started, it does\n  not have `start_time` and, in consequence, it does not have expiry time as\n  well.\n* `ApiModifyHuntArgs` no longer has an `expires` field. Instead, `duration`\n  field has been added.\n* `artifact` field of `ApiUploadArtifactArgs` no longer accepts arbitrary byte\n  stream. Instead, only proper strings are accepted. Since this field is ought\n  to be the artifact description in the YAML format and YAML is required to be\n  UTF-8 encoded, it makes no sense to accept non-unicode objects.\n\n## [3.2.4.6] - 2018-12-20\n\n### API changed\n\n*  Renamed the task_eta field of the ApiClientActionRequest object to\n   leased_until.\n*  Got rid of ListCronJobFlows and GetCronJobFlow in favor of ListCronJobRuns\n   and GetCronJobRun. ListCronJobRuns/GetCronJobRun return ApiCronJobRun protos\n   instead of ApiFlow returned by deleted ListCronJobFlows/GetCronJobFlow.\n*  Changed CreateCronJob API call to accept newly introduced\n   ApiCreateCronJobArgs instead of an ApiCronJob. ApiCreateCronJobArgs only\n   allows to create hunt-based cron jobs.\n\n### API removed\n\n*  All ApiFlowRequest responses do not fill the AFF4 specific\n   request_state.request field anymore. Similarly, the task_id and payload\n   fields in ApiFlowRequest.responses objects is not populated anymore starting\n   from this release.\n*  Flow log results returned by ApiListFlowLogsHandler do not contain the name\n   of the flow the logs are for anymore.\n*  The `ListPendingGlobalNotifications` and `DeletePendingGlobalNotification`\n   API methods have been deleted, since GRR no longer supports\n   global notifications. The corresponding protos\n   `ApiListPendingGlobalNotificationsResult` and\n   `ApiDeletePendingGlobalNotificationArgs` have been deprecated.\n\n## [3.2.3.2] - 2018-06-28\n\n### API changed\n\n*   GetGrrBinary API method result type has changed. It was changed to return\n    ApiGrrBinary object instead of a binary stream. The old behavior is\n    preserved in a newly introduced GetGrrBinaryBlob method.\n\n## [3.2.2.0] - 2018-03-12\n\n### API added\n\n*   Introduced ApiHuntLog, ApiHuntError and ApiFlowLog that are used in\n    ApiListHuntLogsResult, ApiListHuntErrorsResult and ApiListFlowLogsResult\n    respectively instead of jobs_pb2.FlowLog and jobs_pb2.HuntError. New\n    structures are partially backwards compatible with the old ones when used\n    via JSON (in protobuf format the fields indices is not compatible):\n    \"log_message\", \"flow_name\" and \"backtrace\" fields didn't change. \"client_id\"\n    field doesn't have an AFF4 prefix anymore. \"urn\" field was removed and\n    replaced with \"flow_id\". \"timestamp\" field was added.\n*   Added \"cron_job_id\" attribute to ApiCronJob.\n\n### API removed\n\n*   Removed default \"age\" attribute from the legacy HTTP API JSON. Every value\n    rendered in legacy API responses will be dictionary of {value: ..., type:\n    ...} instead of {value: ..., type: ..., age: ...}.\n*   GetClientVersions API call(/api/clients/<client_id>/versions) does not\n    include metadata (last ping, last clock, last boot time, last crash time)\n    anymore.\n"
        },
        {
          "name": "Dockerfile",
          "type": "blob",
          "size": 2.7431640625,
          "content": "# A Docker image capable of running all GRR components.\n#\n# See https://github.com/google/grr/pkgs/container/grr\n#\n# We have configured Github Actions to trigger an image build every\n# time a new a PUSH happens in the GRR github repository.\n#\n# Examples:\n# - Run a grr server component (e.g. admin_ui):\n#\n#   $ docker run -it \\\n#       -v $(pwd)/docker_config_files/server:/configs \\\n#       ghcr.io/google/grr:latest \\\n#       \"-component\" \"admin_ui\" \\\n#       \"-config\" \"/configs/grr.server.yaml\"\n#\n# - Run the grr client component:\n#   -- Start the container and mount the client config directory:\n#       $ docker run -it \\\n#          -v $(pwd)/docker_config_files/client:/configs \\\n#          --entrypoint /bin/bash \\\n#          ghcr.io/google/grr:latest\n#\n#   -- The previous command will leave you with an open shell in\n#      the container. Repack the client template and install the\n#      resulting debian file inside the container:\n#       root@<CONTAINER ID> $ /configs/repack_install_client.sh\n#\n#   -- Start fleetspeak and grr clients:\n#       root@<CONTAINER ID> $ fleetspeak-client -config /configs/client.config\n#\n#   -- (Optional) To verify if the client runs, check if the two expected\n#      processes are running:\n#       root@<CONTAINER ID> $ ps aux\n#             ...        COMMAND\n#             ...        fleetspeak-client -config /configs/client.config\n#             ...        python -m grr_response_client.client ...\n\nFROM ubuntu:22.04\n\nLABEL maintainer=\"grr-dev@googlegroups.com\"\n\nENV DEBIAN_FRONTEND=noninteractive\n# Buffering output (sometimes indefinitely if a thread is stuck in\n# a loop) makes for a non-optimal user experience when containers\n# are run in the foreground, so we disable that.\nENV PYTHONUNBUFFERED=0\n\nRUN apt-get update && \\\n  apt-get install -y \\\n  default-jre \\\n  python-is-python3 \\\n  python3-dev \\\n  python3-pip \\\n  python3-venv \\\n  python3-mysqldb \\\n  build-essential \\\n  linux-headers-generic \\\n  dh-make \\\n  rpm\n\n# Copy the client installers to the image, only available when\n# building as part of Github Actions.\nCOPY ./_installers* /client_templates\n\nENV VIRTUAL_ENV=/usr/share/grr-server\nENV GRR_SOURCE=/usr/src/grr\n\nRUN python -m venv --system-site-packages $VIRTUAL_ENV\nENV PATH=${VIRTUAL_ENV}/bin:${PATH}\n\nRUN ${VIRTUAL_ENV}/bin/python -m pip install wheel nodeenv grpcio-tools==1.60\n\nRUN ${VIRTUAL_ENV}/bin/nodeenv -p --prebuilt --node=16.13.0\n\nRUN mkdir -p ${GRR_SOURCE}\nADD . ${GRR_SOURCE}\n\nWORKDIR ${GRR_SOURCE}\n\nRUN ${VIRTUAL_ENV}/bin/python -m pip install \\\n  -e grr/proto \\\n  -e grr/core \\\n  -e grr/client \\\n  -e grr/server \\\n  -e grr/client_builder \\\n  -e api_client/python\n\nRUN ${VIRTUAL_ENV}/bin/python grr/proto/makefile.py && \\\n  ${VIRTUAL_ENV}/bin/python grr/core/grr_response_core/artifacts/makefile.py\n\nENTRYPOINT [ \"grr_server\" ]\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 11.091796875,
          "content": "\n                                 Apache License\n                           Version 2.0, January 2004\n                        http://www.apache.org/licenses/\n\n   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n\n   1. Definitions.\n\n      \"License\" shall mean the terms and conditions for use, reproduction,\n      and distribution as defined by Sections 1 through 9 of this document.\n\n      \"Licensor\" shall mean the copyright owner or entity authorized by\n      the copyright owner that is granting the License.\n\n      \"Legal Entity\" shall mean the union of the acting entity and all\n      other entities that control, are controlled by, or are under common\n      control with that entity. For the purposes of this definition,\n      \"control\" means (i) the power, direct or indirect, to cause the\n      direction or management of such entity, whether by contract or\n      otherwise, or (ii) ownership of fifty percent (50%) or more of the\n      outstanding shares, or (iii) beneficial ownership of such entity.\n\n      \"You\" (or \"Your\") shall mean an individual or Legal Entity\n      exercising permissions granted by this License.\n\n      \"Source\" form shall mean the preferred form for making modifications,\n      including but not limited to software source code, documentation\n      source, and configuration files.\n\n      \"Object\" form shall mean any form resulting from mechanical\n      transformation or translation of a Source form, including but\n      not limited to compiled object code, generated documentation,\n      and conversions to other media types.\n\n      \"Work\" shall mean the work of authorship, whether in Source or\n      Object form, made available under the License, as indicated by a\n      copyright notice that is included in or attached to the work\n      (an example is provided in the Appendix below).\n\n      \"Derivative Works\" shall mean any work, whether in Source or Object\n      form, that is based on (or derived from) the Work and for which the\n      editorial revisions, annotations, elaborations, or other modifications\n      represent, as a whole, an original work of authorship. For the purposes\n      of this License, Derivative Works shall not include works that remain\n      separable from, or merely link (or bind by name) to the interfaces of,\n      the Work and Derivative Works thereof.\n\n      \"Contribution\" shall mean any work of authorship, including\n      the original version of the Work and any modifications or additions\n      to that Work or Derivative Works thereof, that is intentionally\n      submitted to Licensor for inclusion in the Work by the copyright owner\n      or by an individual or Legal Entity authorized to submit on behalf of\n      the copyright owner. For the purposes of this definition, \"submitted\"\n      means any form of electronic, verbal, or written communication sent\n      to the Licensor or its representatives, including but not limited to\n      communication on electronic mailing lists, source code control systems,\n      and issue tracking systems that are managed by, or on behalf of, the\n      Licensor for the purpose of discussing and improving the Work, but\n      excluding communication that is conspicuously marked or otherwise\n      designated in writing by the copyright owner as \"Not a Contribution.\"\n\n      \"Contributor\" shall mean Licensor and any individual or Legal Entity\n      on behalf of whom a Contribution has been received by Licensor and\n      subsequently incorporated within the Work.\n\n   2. Grant of Copyright License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      copyright license to reproduce, prepare Derivative Works of,\n      publicly display, publicly perform, sublicense, and distribute the\n      Work and such Derivative Works in Source or Object form.\n\n   3. Grant of Patent License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      (except as stated in this section) patent license to make, have made,\n      use, offer to sell, sell, import, and otherwise transfer the Work,\n      where such license applies only to those patent claims licensable\n      by such Contributor that are necessarily infringed by their\n      Contribution(s) alone or by combination of their Contribution(s)\n      with the Work to which such Contribution(s) was submitted. If You\n      institute patent litigation against any entity (including a\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\n      or a Contribution incorporated within the Work constitutes direct\n      or contributory patent infringement, then any patent licenses\n      granted to You under this License for that Work shall terminate\n      as of the date such litigation is filed.\n\n   4. Redistribution. You may reproduce and distribute copies of the\n      Work or Derivative Works thereof in any medium, with or without\n      modifications, and in Source or Object form, provided that You\n      meet the following conditions:\n\n      (a) You must give any other recipients of the Work or\n          Derivative Works a copy of this License; and\n\n      (b) You must cause any modified files to carry prominent notices\n          stating that You changed the files; and\n\n      (c) You must retain, in the Source form of any Derivative Works\n          that You distribute, all copyright, patent, trademark, and\n          attribution notices from the Source form of the Work,\n          excluding those notices that do not pertain to any part of\n          the Derivative Works; and\n\n      (d) If the Work includes a \"NOTICE\" text file as part of its\n          distribution, then any Derivative Works that You distribute must\n          include a readable copy of the attribution notices contained\n          within such NOTICE file, excluding those notices that do not\n          pertain to any part of the Derivative Works, in at least one\n          of the following places: within a NOTICE text file distributed\n          as part of the Derivative Works; within the Source form or\n          documentation, if provided along with the Derivative Works; or,\n          within a display generated by the Derivative Works, if and\n          wherever such third-party notices normally appear. The contents\n          of the NOTICE file are for informational purposes only and\n          do not modify the License. You may add Your own attribution\n          notices within Derivative Works that You distribute, alongside\n          or as an addendum to the NOTICE text from the Work, provided\n          that such additional attribution notices cannot be construed\n          as modifying the License.\n\n      You may add Your own copyright statement to Your modifications and\n      may provide additional or different license terms and conditions\n      for use, reproduction, or distribution of Your modifications, or\n      for any such Derivative Works as a whole, provided Your use,\n      reproduction, and distribution of the Work otherwise complies with\n      the conditions stated in this License.\n\n   5. Submission of Contributions. Unless You explicitly state otherwise,\n      any Contribution intentionally submitted for inclusion in the Work\n      by You to the Licensor shall be under the terms and conditions of\n      this License, without any additional terms or conditions.\n      Notwithstanding the above, nothing herein shall supersede or modify\n      the terms of any separate license agreement you may have executed\n      with Licensor regarding such Contributions.\n\n   6. Trademarks. This License does not grant permission to use the trade\n      names, trademarks, service marks, or product names of the Licensor,\n      except as required for reasonable and customary use in describing the\n      origin of the Work and reproducing the content of the NOTICE file.\n\n   7. Disclaimer of Warranty. Unless required by applicable law or\n      agreed to in writing, Licensor provides the Work (and each\n      Contributor provides its Contributions) on an \"AS IS\" BASIS,\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n      implied, including, without limitation, any warranties or conditions\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n      PARTICULAR PURPOSE. You are solely responsible for determining the\n      appropriateness of using or redistributing the Work and assume any\n      risks associated with Your exercise of permissions under this License.\n\n   8. Limitation of Liability. In no event and under no legal theory,\n      whether in tort (including negligence), contract, or otherwise,\n      unless required by applicable law (such as deliberate and grossly\n      negligent acts) or agreed to in writing, shall any Contributor be\n      liable to You for damages, including any direct, indirect, special,\n      incidental, or consequential damages of any character arising as a\n      result of this License or out of the use or inability to use the\n      Work (including but not limited to damages for loss of goodwill,\n      work stoppage, computer failure or malfunction, or any and all\n      other commercial damages or losses), even if such Contributor\n      has been advised of the possibility of such damages.\n\n   9. Accepting Warranty or Additional Liability. While redistributing\n      the Work or Derivative Works thereof, You may choose to offer,\n      and charge a fee for, acceptance of support, warranty, indemnity,\n      or other liability obligations and/or rights consistent with this\n      License. However, in accepting such obligations, You may act only\n      on Your own behalf and on Your sole responsibility, not on behalf\n      of any other Contributor, and only if You agree to indemnify,\n      defend, and hold each Contributor harmless for any liability\n      incurred by, or claims asserted against, such Contributor by reason\n      of your accepting any such warranty or additional liability.\n\n   END OF TERMS AND CONDITIONS\n\n   APPENDIX: How to apply the Apache License to your work.\n\n      To apply the Apache License to your work, attach the following\n      boilerplate notice, with the fields enclosed by brackets \"[]\"\n      replaced with your own identifying information. (Don't include\n      the brackets!)  The text should be enclosed in the appropriate\n      comment syntax for the file format. We also recommend that a\n      file or class name and description of purpose be included on the\n      same \"printed page\" as the copyright notice for easier\n      identification within third-party archives.\n\n   Copyright [yyyy] [name of copyright owner]\n\n   Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 1.5673828125,
          "content": "<p align=\"center\">\n<img src=\"https://raw.githubusercontent.com/google/grr/gh-pages/img/grr_logo_real_sm.png\" />\n\n<p align=\"center\">\nGRR Rapid Response is an incident response framework focused on remote live forensics.\n\n[![Build](https://github.com/google/grr/actions/workflows/build.yml/badge.svg?branch=master)](https://github.com/google/grr/actions/workflows/build.yml)\n\nGRR is a python client (agent) that is installed on target systems, and\npython server infrastructure that can manage and talk to clients.<br>\n\n\nDocumentation\n-------------\n\nPlease visit our [documentation website](https://grr-doc.readthedocs.io/) if you want to know more about GRR.\n\n\nContact Us\n----------\n\n* GitHub issues: [github.com/google/grr/issues](https://github.com/google/grr/issues)\n* GRR Users mailing list: [grr-users](https://groups.google.com/forum/#!forum/grr-users)\n* Follow us [on twitter](https://twitter.com/grrresponse) for announcements of GRR user meetups. We use [a gitter chat room](https://gitter.im/google/grr) during meetups.\n\n\nScreenshots\n-----------\n[<img src=\"https://github.com/google/grr/blob/gh-pages/screenshots/endpoint-overview.png\" width=\"330\">](https://github.com/google/grr/blob/gh-pages/screenshots/endpoint-overview.png)\n[<img src=\"https://github.com/google/grr/blob/gh-pages/screenshots/filesystem-overview.png\" width=\"330\">](https://github.com/google/grr/blob/gh-pages/screenshots/filesystem-overview.png)\n[<img src=\"https://github.com/google/grr/blob/gh-pages/screenshots/workflow-results.png\" width=\"330\">](https://github.com/google/grr/blob/gh-pages/screenshots/workflow-results.png)\n"
        },
        {
          "name": "api_client",
          "type": "tree",
          "content": null
        },
        {
          "name": "appveyor",
          "type": "tree",
          "content": null
        },
        {
          "name": "build_requirements.txt",
          "type": "blob",
          "size": 0.07421875,
          "content": "pip==24.0\npytest==6.2.5\npytest-xdist==2.2.1\nsetuptools==69.5.1\nwheel==0.43.0"
        },
        {
          "name": "colab",
          "type": "tree",
          "content": null
        },
        {
          "name": "compose.testing.yaml",
          "type": "blob",
          "size": 0.2314453125,
          "content": "\nservices:\n  grr-admin-ui:\n    image: ghcr.io/google/grr:testing\n\n  grr-fleetspeak-frontend:\n    image: ghcr.io/google/grr:testing\n\n  grr-worker:\n    image: ghcr.io/google/grr:testing\n\n  grr-client:\n    image: ghcr.io/google/grr:testing\n"
        },
        {
          "name": "compose.yaml",
          "type": "blob",
          "size": 5.29296875,
          "content": "version: \"3.8\"\nservices:\n  db:\n    image: mysql:8.2\n    env_file: docker_config_files/mysql/.env\n    container_name: grr-db\n    hostname: mysql-host\n    command: [\n      --max_allowed_packet=40M,\n      --log_bin_trust_function_creators=1,\n      --innodb_redo_log_capacity=167772160,\n      --innodb_log_file_size=2500M\n    ]\n    restart: always\n    volumes:\n      - ./docker_config_files/mysql/init.sh:/docker-entrypoint-initdb.d/init.sh\n      - db_data:/var/lib/mysql:rw\n    ports:\n      - \"3306:3306\"\n    expose:\n      - \"3306\"\n    networks:\n      - server-network\n    healthcheck:\n      test: [\"CMD\", \"mysqladmin\", \"ping\", \"-h\", \"localhost\"]\n      timeout: 5s\n      retries: 10\n\n  grr-admin-ui:\n    build: .\n    image: ghcr.io/google/grr:latest\n    container_name: grr-admin-ui\n    hostname: admin-ui\n    depends_on:\n      db:\n        condition: service_healthy\n      fleetspeak-admin:\n        condition: service_started\n    volumes:\n      - ./docker_config_files:/configs/\n      # Mount a directory for the repacked client installers, so they\n      # can be used in the grr-client container which mounts the same volume.\n      - client_installers:/client_installers\n    ports:\n      - \"8000:8000\"\n    expose:\n      - \"8000\"\n    networks:\n      - server-network\n    entrypoint: [\n      \"/bin/bash\",\n      \"-c\",\n      \"/configs/server/repack_clients.sh && grr_server -component admin_ui -config /configs/server/grr.server.yaml --verbose\"\n    ]\n    healthcheck:\n      # As soon as any files have been written to the /client_installer we\n      # assume the service is healthy.\n      test: \"/configs/healthchecks/grr-admin-ui.sh\"\n      timeout: 10s\n      retries: 10\n    develop:\n      watch:\n        - action: sync+restart\n          path: ./grr\n          target: /usr/src/grr/grr\n          ignore:\n            - client/\n\n  grr-fleetspeak-frontend:\n    build: .\n    image: ghcr.io/google/grr:latest\n    container_name: grr-fleetspeak-frontend\n    hostname: grr-fleetspeak-frontend\n    depends_on:\n      db:\n        condition: service_healthy\n    volumes:\n      - ./docker_config_files:/configs\n    expose:\n      - \"11111\"\n    restart: always\n    networks:\n      - server-network\n    command:\n      - -component\n      - frontend\n      - -config\n      - /configs/server/grr.server.yaml\n      - --verbose\n    develop:\n      watch:\n        - action: sync+restart\n          path: ./grr\n          target: /usr/src/grr/grr\n          ignore:\n            - client/\n\n  fleetspeak-admin:\n    image: ghcr.io/google/fleetspeak:latest\n    container_name: fleetspeak-admin\n    hostname: fleetspeak-admin\n    depends_on:\n      db:\n        condition: service_healthy\n    networks:\n      - server-network\n    expose:\n      - \"4444\"\n    volumes:\n      - ./docker_config_files:/configs\n    entrypoint: [\n      \"server\",\n      \"-components_config\",\n      \"/configs/server/textservices/admin.components.config\",\n      \"-services_config\",\n      \"/configs/server/grr_frontend.service\",\n      \"-alsologtostderr\",\n      \"-v\",\n      \"1000\"\n    ]\n\n  fleetspeak-frontend:\n    image: ghcr.io/google/fleetspeak:latest\n    container_name: fleetspeak-frontend\n    hostname: fleetspeak-frontend\n    depends_on:\n      db:\n        condition: service_healthy\n    networks:\n      - server-network\n    expose:\n      - \"4443\"\n      - \"10000\"\n    volumes:\n      - ./docker_config_files:/configs\n    entrypoint: [\n      \"server\",\n      \"-components_config\",\n      \"/configs/server/textservices/frontend.components.config\",\n      \"-services_config\",\n      \"/configs/server/grr_frontend.service\",\n      \"-alsologtostderr\",\n      \"-v\",\n      \"1000\"\n    ]\n\n  grr-worker:\n    build: .\n    image: ghcr.io/google/grr:latest\n    container_name: grr-worker\n    volumes:\n      - ./docker_config_files:/configs\n    hostname: grr-worker\n    depends_on:\n      db:\n        condition: service_healthy\n    restart: always\n    networks:\n      - server-network\n    command:\n      - -component\n      - worker\n      - -config\n      - /configs/server/grr.server.yaml\n      - --verbose\n    develop:\n      watch:\n        - action: sync+restart\n          path: ./grr\n          target: /usr/src/grr/grr\n          ignore:\n            - client/\n\n  grr-client:\n    build: .\n    image: ubuntu:22.04\n    container_name: grr-client\n    depends_on:\n      db:\n        condition: service_healthy\n      fleetspeak-frontend:\n        condition: service_started\n      grr-admin-ui:\n        # Service is healthy as soon as client installers are repacked.\n        condition: service_healthy\n    volumes:\n      - ./docker_config_files:/configs\n      # Mount the client_installers folder which contains the\n      # repacked templates written by the grr-admin-ui container\n      - client_installers:/client_installers\n      # Mount the client_state volume to preserve the clients state\n      # including the client_id across restarts.\n      - client_state:/client_state\n    networks:\n      - server-network\n    entrypoint: [\n      \"/bin/bash\",\n      \"-c\",\n      \"/configs/client/install_client.sh && fleetspeak-client -config /configs/client/client.config\"\n    ]\n    healthcheck:\n      test: \"/configs/healthchecks/grr-client.sh\"\n      timeout: 10s\n      retries: 10\n    develop:\n      watch:\n        - action: sync+restart\n          path: ./grr\n          target: /usr/src/grr/grr\n          ignore:\n            - server/\n\nvolumes:\n  db_data:\n  client_installers:\n  client_state:\nnetworks:\n  server-network:\n"
        },
        {
          "name": "conftest.py",
          "type": "blob",
          "size": 5.3359375,
          "content": "#!/usr/bin/env python\n\"\"\"A module that configures the behaviour of pytest runner.\"\"\"\n\nimport os\nimport sys\nimport threading\nimport traceback\n\nfrom absl import flags\nimport pytest\n\n# pylint: disable=g-import-not-at-top\ntry:\n  # This depends on grr_response_server, which is NOT available on all\n  # (especially non-Linux development) platforms.\n  from grr.test_lib import testing_startup\nexcept ModuleNotFoundError:\n  testing_startup = None\n# pylint: enable=g-import-not-at-top\n\nFLAGS = flags.FLAGS\n\ntest_args = None\n\n\ndef pytest_cmdline_preparse(config, args):\n  \"\"\"A pytest hook that is called during command-line argument parsing.\"\"\"\n  del config  # Unused.\n\n  try:\n    separator = args.index(\"--\")\n  except ValueError:\n    separator = len(args)\n\n  global test_args\n  test_args = args[separator + 1:]\n  del args[separator:]\n\n\ndef pytest_cmdline_main(config):\n  \"\"\"A pytest hook that is called when the main function is executed.\"\"\"\n\n  if \"PYTEST_XDIST_WORKER\" in os.environ:\n    # If ran concurrently using pytest-xdist (`-n` cli flag), mainargv is the\n    # result of the execution of pytest_cmdline_main in the main process.\n    sys.argv = config.workerinput[\"mainargv\"]\n  else:\n    sys.argv = [\"pytest\"] + test_args\n\n\nlast_module = None\n\n\ndef pytest_runtest_setup(item):\n  \"\"\"A pytest hook that is called before each test item is executed.\"\"\"\n  # We need to re-initialize flags (and hence also testing setup) because\n  # various modules might have various flags defined.\n  global last_module\n  if last_module != item.module:\n    FLAGS(sys.argv)\n    if testing_startup:\n      testing_startup.TestInit()\n  last_module = item.module\n\n\ndef pytest_addoption(parser):\n  \"\"\"A pytest hook that is called during the argument parser initialization.\"\"\"\n  parser.addoption(\n      \"--full_thread_trace\",\n      action=\"store_true\",\n      default=False,\n      help=\"Include a full stacktrace for all thread in case of a thread leak.\",\n  )\n\n\ndef _generate_full_thread_trace():\n  \"\"\"Generates a full stack trace for all currently running threads.\"\"\"\n  threads = threading.enumerate()\n\n  res = \"Stacktrace for:\\n\"\n  for thread in threads:\n    res += \"%s (id %d)\\n\" % (thread.name, thread.ident)\n\n  res += \"\\n\"\n\n  frames = sys._current_frames()  # pylint: disable=protected-access\n  for thread_id, stack in frames.items():\n    res += \"Thread ID: %s\\n\" % thread_id\n    for filename, lineno, name, line in traceback.extract_stack(stack):\n      res += \"File: '%s', line %d, in %s\\n\" % (filename, lineno, name)\n      if line:\n        res += \"  %s\\n\" % (line.strip())\n\n  return res\n\n\nlast_test_name = None\nknown_leaks = []\n\n\n@pytest.fixture(scope=\"function\", autouse=True)\ndef thread_leak_check(request):\n  \"\"\"Makes sure that no threads are left running by any test.\"\"\"\n  global last_test_name\n\n  threads = threading.enumerate()\n\n  # Quoting Python docs (https://docs.python.org/3/library/threading.html):\n  # threading.current_thread():\n  # Return the current Thread object, corresponding to the caller's thread\n  # of control. If the caller's thread of control was not created through\n  # the threading module, a dummy thread object with limited functionality\n  # is returned.\n  #\n  # Quoting Python source\n  # (https://github.com/python/cpython/blob/2a16eea71f56c2d8f38c295c8ce71a9a9a140aff/Lib/threading.py#L1269):\n  # Dummy thread class to represent threads not started here.\n  # These aren't garbage collected when they die, nor can they be waited for.\n  # If they invoke anything in threading.py that calls current_thread(), they\n  # leave an entry in the _active dict forever after.\n  # Their purpose is to return *something* from current_thread().\n  # They are marked as daemon threads so we won't wait for them\n  # when we exit (conform previous semantics).\n  #\n  # See\n  # https://stackoverflow.com/questions/55778365/what-is-dummy-in-threading-current-thread\n  # for additional context.\n  #\n  # Dummy threads are named \"Dummy-*\" and are never deleted, since it's\n  # impossible to detect the termination of alien threads, hence we have to\n  # ignore them.\n  thread_names = [\n      thread.name for thread in threads if not thread.name.startswith(\"Dummy-\")\n  ]\n\n  allowed_thread_names = [\n      \"MainThread\",\n\n      # We start one thread per connector and let them run since there is a lot\n      # of overhead involved.\n      \"ApiRegressionHttpConnectorV1\",\n      \"ApiRegressionHttpConnectorV2\",\n\n      # Selenium takes long to set up, we clean up using an atexit handler.\n      \"SeleniumServerThread\",\n\n      # All these threads are constructed in setUpClass and destroyed in\n      # tearDownClass so they are not real leaks.\n      \"api_integration_server\",\n      \"ApiSslServerTest\",\n      \"GRRHTTPServerTestThread\",\n      \"SharedMemDBTestThread\",\n  ]\n\n  # Remove up to one instance of each allowed thread name.\n  for allowed_name in allowed_thread_names + known_leaks:\n    if allowed_name in thread_names:\n      thread_names.remove(allowed_name)\n\n  current_test_name = request.node.name\n\n  if thread_names:\n    # Store any leaks so we only alert once about each leak.\n    known_leaks.extend(thread_names)\n\n    error_msg = (\"Detected unexpected thread(s): %s. \"\n                 \"Last test was %s, next test is %s.\" %\n                 (thread_names, last_test_name, current_test_name))\n\n    if request.config.getoption(\"full_thread_trace\"):\n      error_msg += \"\\n\\n\" + _generate_full_thread_trace()\n\n    raise RuntimeError(error_msg)\n\n  last_test_name = current_test_name\n"
        },
        {
          "name": "devenv",
          "type": "tree",
          "content": null
        },
        {
          "name": "docker",
          "type": "tree",
          "content": null
        },
        {
          "name": "docker_config_files",
          "type": "tree",
          "content": null
        },
        {
          "name": "grr",
          "type": "tree",
          "content": null
        },
        {
          "name": "gsoc_2021.md",
          "type": "blob",
          "size": 4.2763671875,
          "content": "# GRR Google Summer of Code 2021 Project Ideas\n\nGRR has been accepted as a hosting organization for Google Summer of Code 2021.\n\nThis page contains a list of possible project ideas.\n\n## Next-generation Windows Registry support\n\nPossible mentor: [panhania](https://github.com/panhania)\n\nThe existing support for reading/searching Windows Registry in GRR\nis quite lacking: it builds on poorly chosen abstractions and the\nimplementation is buggy. Given how important Windows Registry is in the Windows\necosystem, this significantly limits GRR's forensic capabilities. The goal of\nthe project would be to build a set of completely new functionalities without\nlooking back at the existing implementation or trying to be backward\ncompatible.\n\nThe project involves:\n\n* Implementing a set of new “client actions” for the GRR\nsecurity agent to read/search Windows registry (“actions” are the fundamental\nprimitives that the GRR agent exposes and makes it possible to extract\ninformation from a particular machine).\n* Implement a set of new [“flows”](https://grr-doc.readthedocs.io/en/latest/investigating-with-grr/flows/index.html) to work\nwith implemented client actions (“flows” are effectively state-machines on the\nGRR backend used to orchestrate multiple client actions).\n* (Stretch goal) Add\nsupport for “raw” mode into the implemented client actions. In “raw mode” GRR\nagent won’t rely on Windows registry calls, but would rather parse the registry\ndirectly.\n\n## Modern user interface for YARA memory scans\n\nPossible mentors: [mbushkov](https://github.com/mbushkov),\n[max-vogler](https://github.com/max-vogler)\n\nOne core GRR digital forensics workflow is scanning the memory of a process for\nindicators of malware using [YARA](https://virustotal.github.io/yara/). With\nGRR's existing user interface, security analysts need to navigate a complex and\ndated application to configure YARA scans. In this project, you will write an\neffective, modern user interface to configure the workflow and show its live\nprogress and results using Angular, TypeScript, and Material Design.\n\n## FUSE filesystem based on GRR’s API\n\nPossible mentor: [mol123](https://github.com/mol123)\n\nUsers (security engineers) use GRR to collect artifacts, such as files,\nregistry entries, etc. from machines of interest. The primary way of accessing\nthe collected artifacts is via a web-based UI. Alternatively, we provide a\nweb-based API\n([docs](https://storage.googleapis.com/autobuilds-grr-openapi/documentation/openapi_documentation.html]))\nfor programmatic access. The goal of this project is to expose the web-based\nAPI as a\n[FUSE](https://en.wikipedia.org/wiki/Filesystem_in_Userspace)\nfilesystem. This would enable users to mount GRR as a filesystem, access the\nmachines under investigation as directories, and access the artifacts collected\nfrom machines using native tools installed on their operating system.\n\n## Flexible repacking of MacOS GRR templates\n\nPossible mentors: [mbushkov](https://github.com/mbushkov),\n[mol123](https://github.com/mol123)\n\nGRR consists of an agent that is installed on target systems, and the server\ninfrastructure that can manage and talk to clients. To make deployment of\nagents easier, GRR offers a repacking mechanism for agent installers. The\n“repacking” is generating agent installers from prebuilt installer templates\nshipped with the GRR server. During the repacking, a deployment-specific\nconfiguration file is added to the agent installer and, optionally, a GRR\nbinary is renamed. Renaming a binary is important for large organizations that\nmight ship differently named binaries to different sub-orgs, and also for\nsecurity reasons, to make it impossible for malware to target GRR process based\non the binary name.\n\nWhile GRR installer templates are built on target platforms (Windows, MacOS,\nUbuntu, RedHat), repacking always happens on the GRR backend (Ubuntu).\nCurrently we support only a limited repacking process for MacOS, and the GRR\nbinary can’t be renamed during the repacking. The goal of the project will be\nto develop a pure-Python solution that will be able to unpack the MacOS\ninstaller template, make necessary changes and repack it into a ready-to-use\ninstaller. The repacking process will need to be flexible enough to support\nrenaming and, possibly, inclusion of arbitrary additional files into the\ninstaller.\n\n"
        },
        {
          "name": "keys",
          "type": "tree",
          "content": null
        },
        {
          "name": "monitoring",
          "type": "tree",
          "content": null
        },
        {
          "name": "pytest.ini",
          "type": "blob",
          "size": 0.28125,
          "content": "[pytest]\npython_files=*_test.py\npython_classes=\npython_functions=test*\n\nnorecursedirs=\n  grr/server/grr_response_server/gui/static/node_modules\n  grr/server/grr_response_server/gui/static/bower_components\n  grr/server/grr_response_server/gui/ui/node_modules\n  *.egg\n  .eggs\n  .git\n  dist\n"
        },
        {
          "name": "terraform",
          "type": "tree",
          "content": null
        },
        {
          "name": "travis",
          "type": "tree",
          "content": null
        },
        {
          "name": "version.ini",
          "type": "blob",
          "size": 0.154296875,
          "content": "[Version]\n\nmajor = 3\nminor = 4\nrevision = 7\nrelease = 6\n\npackageversion = %(major)s.%(minor)s.%(revision)spost%(release)s\npackagedepends = %(packageversion)s\n"
        }
      ]
    }
  ]
}