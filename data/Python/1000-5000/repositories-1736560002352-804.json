{
  "metadata": {
    "timestamp": 1736560002352,
    "page": 804,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjgxMA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "magicleap/SuperGluePretrainedNetwork",
      "stars": 3416,
      "defaultBranch": "master",
      "files": [
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.0224609375,
          "content": "*.pyc\n*.DS_Store\n*.swp\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 6.8447265625,
          "content": "SUPERGLUE: LEARNING FEATURE MATCHING WITH GRAPH NEURAL NETWORKS\nSOFTWARE LICENSE AGREEMENT\nACADEMIC OR NON-PROFIT ORGANIZATION NONCOMMERCIAL RESEARCH USE ONLY\n\nBY USING OR DOWNLOADING THE SOFTWARE, YOU ARE AGREEING TO THE TERMS OF THIS LICENSE AGREEMENT.  IF YOU DO NOT AGREE WITH THESE TERMS, YOU MAY NOT USE OR DOWNLOAD THE SOFTWARE.\n\nThis is a license agreement (\"Agreement\") between your academic institution or non-profit organization or self (called \"Licensee\" or \"You\" in this Agreement) and Magic Leap, Inc. (called \"Licensor\" in this Agreement).  All rights not specifically granted to you in this Agreement are reserved for Licensor. \n\nRESERVATION OF OWNERSHIP AND GRANT OF LICENSE: \nLicensor retains exclusive ownership of any copy of the Software (as defined below) licensed under this Agreement and hereby grants to Licensee a personal, non-exclusive, non-transferable license to use the Software for noncommercial research purposes, without the right to sublicense, pursuant to the terms and conditions of this Agreement.  As used in this Agreement, the term \"Software\" means (i) the actual copy of all or any portion of code for program routines made accessible to Licensee by Licensor pursuant to this Agreement, inclusive of backups, updates, and/or merged copies permitted hereunder or subsequently supplied by Licensor,  including all or any file structures, programming instructions, user interfaces and screen formats and sequences as well as any and all documentation and instructions related to it, and (ii) all or any derivatives and/or modifications created or made by You to any of the items specified in (i).\n\nCONFIDENTIALITY: Licensee acknowledges that the Software is proprietary to Licensor, and as such, Licensee agrees to receive all such materials in confidence and use the Software only in accordance with the terms of this Agreement.  Licensee agrees to use reasonable effort to protect the Software from unauthorized use, reproduction, distribution, or publication.\n\nCOPYRIGHT: The Software is owned by Licensor and is protected by United  States copyright laws and applicable international treaties and/or conventions.\n\nPERMITTED USES:  The Software may be used for your own noncommercial internal research purposes. You understand and agree that Licensor is not obligated to implement any suggestions and/or feedback you might provide regarding the Software, but to the extent Licensor does so, you are not entitled to any compensation related thereto.\n\nDERIVATIVES: You may create derivatives of or make modifications to the Software, however, You agree that all and any such derivatives and modifications will be owned by Licensor and become a part of the Software licensed to You under this Agreement.  You may only use such derivatives and modifications for your own noncommercial internal research purposes, and you may not otherwise use, distribute or copy such derivatives and modifications in violation of this Agreement.\n\nBACKUPS:  If Licensee is an organization, it may make that number of copies of the Software necessary for internal noncommercial use at a single site within its organization provided that all information appearing in or on the original labels, including the copyright and trademark notices are copied onto the labels of the copies.\n\nUSES NOT PERMITTED:  You may not distribute, copy or use the Software except as explicitly permitted herein. Licensee has not been granted any trademark license as part of this Agreement and may not use the name or mark \"Magic Leap\" or any renditions thereof without the prior written permission of Licensor.\n\nYou may not sell, rent, lease, sublicense, lend, time-share or transfer, in whole or in part, or provide third parties access to prior or present versions (or any parts thereof) of the Software.\n\nASSIGNMENT: You may not assign this Agreement or your rights hereunder without the prior written consent of Licensor. Any attempted assignment without such consent shall be null and void.\n\nTERM: The term of the license granted by this Agreement is from Licensee's acceptance of this Agreement by downloading the Software or by using the Software until terminated as provided below.\n\nThe Agreement automatically terminates without notice if you fail to comply with any provision of this Agreement.  Licensee may terminate this Agreement by ceasing using the Software.  Upon any termination of this Agreement, Licensee will delete any and all copies of the Software. You agree that all provisions which operate to protect the proprietary rights of Licensor shall remain in force should breach occur and that the obligation of confidentiality described in this Agreement is binding in perpetuity and, as such, survives the term of the Agreement.\n\nFEE: Provided Licensee abides completely by the terms and conditions of this Agreement, there is no fee due to Licensor for Licensee's use of the Software in accordance with this Agreement.\n\nDISCLAIMER OF WARRANTIES:  THE SOFTWARE IS PROVIDED \"AS-IS\" WITHOUT WARRANTY OF ANY KIND INCLUDING ANY WARRANTIES OF PERFORMANCE OR MERCHANTABILITY OR FITNESS FOR A PARTICULAR USE OR PURPOSE OR OF NON-INFRINGEMENT.  LICENSEE BEARS ALL RISK RELATING TO QUALITY AND PERFORMANCE OF THE SOFTWARE AND RELATED MATERIALS.\n\nSUPPORT AND MAINTENANCE: No Software support or training by the Licensor is provided as part of this Agreement.  \n\nEXCLUSIVE REMEDY AND LIMITATION OF LIABILITY: To the maximum extent permitted under applicable law, Licensor shall not be liable for direct, indirect, special, incidental, or consequential damages or lost profits related to Licensee's use of and/or inability to use the Software, even if Licensor is advised of the possibility of such damage.\n\nEXPORT REGULATION: Licensee agrees to comply with any and all applicable U.S. export control laws, regulations, and/or other laws related to embargoes and sanction programs administered by the Office of Foreign Assets Control.\n\nSEVERABILITY: If any provision(s) of this Agreement shall be held to be invalid, illegal, or unenforceable by a court or other tribunal of competent jurisdiction, the validity, legality and enforceability of the remaining provisions shall not in any way be affected or impaired thereby.\n\nNO IMPLIED WAIVERS: No failure or delay by Licensor in enforcing any right or remedy under this Agreement shall be construed as a waiver of any future or other exercise of such right or remedy by Licensor.\n\nGOVERNING LAW: This Agreement shall be construed and enforced in accordance with the laws of the State of Florida without reference to conflict of laws principles.  You consent to the personal jurisdiction of the courts of this County and waive their rights to venue outside of Broward County, Florida.\n\nENTIRE AGREEMENT AND AMENDMENTS: This Agreement constitutes the sole and entire agreement between Licensee and Licensor as to the matter set forth herein and supersedes any previous agreements, understandings, and arrangements between the parties relating hereto.\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 18.66796875,
          "content": "<img src=\"assets/magicleap.png\" width=\"240\">\n\n### Research @ Magic Leap (CVPR 2020, Oral)\n\n# SuperGlue Inference and Evaluation Demo Script\n\n## Introduction\nSuperGlue is a CVPR 2020 research project done at Magic Leap. The SuperGlue network is a Graph Neural Network combined with an Optimal Matching layer that is trained to perform matching on two sets of sparse image features. This repo includes PyTorch code and pretrained weights for running the SuperGlue matching network on top of [SuperPoint](https://arxiv.org/abs/1712.07629) keypoints and descriptors. Given a pair of images, you can use this repo to extract matching features across the image pair.\n\n<p align=\"center\">\n  <img src=\"assets/teaser.png\" width=\"500\">\n</p>\n\nSuperGlue operates as a \"middle-end,\" performing context aggregation, matching, and filtering in a single end-to-end architecture. For more details, please see:\n\n* Full paper PDF: [SuperGlue: Learning Feature Matching with Graph Neural Networks](https://arxiv.org/abs/1911.11763).\n\n* Authors: *Paul-Edouard Sarlin, Daniel DeTone, Tomasz Malisiewicz, Andrew Rabinovich*\n\n* Website: [psarlin.com/superglue](https://psarlin.com/superglue) for videos, slides, recent updates, and more visualizations.\n\n* `hloc`: a new toolbox for visual localization and SfM with SuperGlue, available at [cvg/Hierarchical-Localization](https://github.com/cvg/Hierarchical-Localization/). Winner of 3 CVPR 2020 competitions on localization and image matching!\n\nWe provide two pre-trained weights files: an indoor model trained on ScanNet data, and an outdoor model trained on MegaDepth data. Both models are inside the [weights directory](./models/weights). By default, the demo will run the **indoor** model.\n\n## Dependencies\n* Python 3 >= 3.5\n* PyTorch >= 1.1\n* OpenCV >= 3.4 (4.1.2.30 recommended for best GUI keyboard interaction, see this [note](#additional-notes))\n* Matplotlib >= 3.1\n* NumPy >= 1.18\n\nSimply run the following command: `pip3 install numpy opencv-python torch matplotlib`\n\n## Contents\nThere are two main top-level scripts in this repo:\n\n1. `demo_superglue.py` : runs a live demo on a webcam, IP camera, image directory or movie file\n2. `match_pairs.py`: reads image pairs from files and dumps matches to disk (also runs evaluation if ground truth relative poses are provided)\n\n## Live Matching Demo Script (`demo_superglue.py`)\nThis demo runs SuperPoint + SuperGlue feature matching on an anchor image and live image. You can update the anchor image by pressing the `n` key. The demo can read image streams from a USB or IP camera, a directory containing images, or a video file. You can pass all of these inputs using the `--input` flag.\n\n### Run the demo on a live webcam\n\nRun the demo on the default USB webcam (ID #0), running on a CUDA GPU if one is found:\n\n```sh\n./demo_superglue.py\n```\n\nKeyboard control:\n\n* `n`: select the current frame as the anchor\n* `e`/`r`: increase/decrease the keypoint confidence threshold\n* `d`/`f`: increase/decrease the match filtering threshold\n* `k`: toggle the visualization of keypoints\n* `q`: quit\n\nRun the demo on 320x240 images running on the CPU:\n\n```sh\n./demo_superglue.py --resize 320 240 --force_cpu\n```\n\nThe `--resize` flag can be used to resize the input image in three ways:\n\n1. `--resize` `width` `height` : will resize to exact `width` x `height` dimensions\n2. `--resize` `max_dimension` : will resize largest input image dimension to `max_dimension`\n3. `--resize` `-1` : will not resize (i.e. use original image dimensions)\n\nThe default will resize images to `640x480`.\n\n### Run the demo on a directory of images\n\nThe `--input` flag also accepts a path to a directory. We provide a directory of sample images from a sequence. To run the demo on the directory of images in `freiburg_sequence/` on a headless server (will not display to the screen) and write the output visualization images to `dump_demo_sequence/`:\n\n```sh\n./demo_superglue.py --input assets/freiburg_sequence/ --output_dir dump_demo_sequence --resize 320 240 --no_display\n```\n\nYou should see this output on the sample Freiburg-TUM RGBD sequence:\n\n<img src=\"assets/freiburg_matches.gif\" width=\"560\">\n\nThe matches are colored by their predicted confidence in a jet colormap (Red: more confident, Blue: less confident).\n\n### Additional useful command line parameters\n* Use `--image_glob` to change the image file extension (default: `*.png`, `*.jpg`, `*.jpeg`).\n* Use `--skip` to skip intermediate frames (default: `1`).\n* Use `--max_length` to cap the total number of frames processed (default: `1000000`).\n* Use `--show_keypoints` to visualize the detected keypoints (default: `False`).\n\n## Run Matching+Evaluation (`match_pairs.py`)\n\nThis repo also contains a script `match_pairs.py` that runs the matching from a list of image pairs. With this script, you can:\n\n* Run the matcher on a set of image pairs (no ground truth needed)\n* Visualize the keypoints and matches, based on their confidence\n* Evaluate and visualize the match correctness, if the ground truth relative poses and intrinsics are provided\n* Save the keypoints, matches, and evaluation results for further processing\n* Collate evaluation results over many pairs and generate result tables\n\n### Matches only mode\n\nThe simplest usage of this script will process the image pairs listed in a given text file and dump the keypoints and matches to compressed numpy `npz` files. We provide the challenging ScanNet pairs from the main paper in `assets/example_indoor_pairs/`. Running the following will run SuperPoint + SuperGlue on each image pair, and dump the results to `dump_match_pairs/`:\n\n```sh\n./match_pairs.py\n```\n\nThe resulting `.npz` files can be read from Python as follows:\n\n```python\n>>> import numpy as np\n>>> path = 'dump_match_pairs/scene0711_00_frame-001680_scene0711_00_frame-001995_matches.npz'\n>>> npz = np.load(path)\n>>> npz.files\n['keypoints0', 'keypoints1', 'matches', 'match_confidence']\n>>> npz['keypoints0'].shape\n(382, 2)\n>>> npz['keypoints1'].shape\n(391, 2)\n>>> npz['matches'].shape\n(382,)\n>>> np.sum(npz['matches']>-1)\n115\n>>> npz['match_confidence'].shape\n(382,)\n```\n\nFor each keypoint in `keypoints0`, the `matches` array indicates the index of the matching keypoint in `keypoints1`, or `-1` if the keypoint is unmatched.\n\n### Visualization mode\n\nYou can add the flag `--viz` to dump image outputs which visualize the matches:\n\n```sh\n./match_pairs.py --viz\n```\n\nYou should see images like this inside of `dump_match_pairs/` (or something very close to it, see this [note](#a-note-on-reproducibility)):\n\n<img src=\"assets/indoor_matches.png\" width=\"560\">\n\nThe matches are colored by their predicted confidence in a jet colormap (Red: more confident, Blue: less confident).\n\n### Evaluation mode\n\nYou can also estimate the pose using RANSAC + Essential Matrix decomposition and evaluate it if the ground truth relative poses and intrinsics are provided in the input `.txt` files. Each `.txt` file contains three key ground truth matrices: a 3x3 intrinsics matrix of image0: `K0`, a 3x3 intrinsics matrix of image1: `K1` , and a 4x4 matrix of the relative pose extrinsics `T_0to1`.\n\nTo run the evaluation on the sample set of images (by default reading `assets/scannet_sample_pairs_with_gt.txt`), you can run:\n\n```sh\n./match_pairs.py --eval\n```\n\n\nSince you enabled `--eval`, you should see collated results printed to the terminal. For the example images provided, you should get the following numbers (or something very close to it, see this [note](#a-note-on-reproducibility)):\n\n```txt\nEvaluation Results (mean over 15 pairs):\nAUC@5    AUC@10  AUC@20  Prec    MScore\n26.99    48.40   64.47   73.52   19.60\n```\n\nThe resulting `.npz` files in `dump_match_pairs/` will now contain scalar values related to the evaluation, computed on the sample images provided. Here is what you should find in one of the generated evaluation files:\n\n```python\n>>> import numpy as np\n>>> path = 'dump_match_pairs/scene0711_00_frame-001680_scene0711_00_frame-001995_evaluation.npz'\n>>> npz = np.load(path)\n>>> print(npz.files)\n['error_t', 'error_R', 'precision', 'matching_score', 'num_correct', 'epipolar_errors']\n```\n\nYou can also visualize the evaluation metrics by running the following command:\n\n```sh\n./match_pairs.py --eval --viz\n```\n\nYou should also now see additional images in `dump_match_pairs/` which visualize the evaluation numbers (or something very close to it, see this [note](#a-note-on-reproducibility)):\n\n<img src=\"assets/indoor_evaluation.png\" width=\"560\">\n\nThe top left corner of the image shows the pose error and number of inliers, while the lines are colored by their epipolar error computed with the ground truth relative pose (red: higher error, green: lower error).\n\n### Running on sample outdoor pairs\n\n<details>\n  <summary>[Click to expand]</summary>\n\nIn this repo, we also provide a few challenging Phototourism pairs, so that you can re-create some of the figures from the paper. Run this script to run matching and visualization (no ground truth is provided, see this [note](#reproducing-outdoor-evaluation-final-table)) on the provided pairs:\n\n```sh\n./match_pairs.py --resize 1600 --superglue outdoor --max_keypoints 2048 --nms_radius 3  --resize_float --input_dir assets/phototourism_sample_images/ --input_pairs assets/phototourism_sample_pairs.txt --output_dir dump_match_pairs_outdoor --viz\n```\n\nYou should now image pairs such as these in `dump_match_pairs_outdoor/` (or something very close to it, see this [note](#a-note-on-reproducibility)):\n\n<img src=\"assets/outdoor_matches.png\" width=\"560\">\n\n</details>\n\n### Recommended settings for indoor / outdoor\n\n<details>\n  <summary>[Click to expand]</summary>\n\nFor **indoor** images, we recommend the following settings (these are the defaults):\n\n```sh\n./match_pairs.py --resize 640 --superglue indoor --max_keypoints 1024 --nms_radius 4\n```\n\nFor **outdoor** images, we recommend the following settings:\n\n```sh\n./match_pairs.py --resize 1600 --superglue outdoor --max_keypoints 2048 --nms_radius 3 --resize_float\n```\n\nYou can provide your own list of pairs `--input_pairs` for images contained in `--input_dir`. Images can be resized before network inference with `--resize`. If you are re-running the same evaluation many times, you can use the `--cache` flag to reuse old computation.\n</details>\n\n### Test set pair file format explained\n\n<details>\n  <summary>[Click to expand]</summary>\n\nWe provide the list of ScanNet test pairs in `assets/scannet_test_pairs_with_gt.txt` (with ground truth) and Phototourism test pairs `assets/phototourism_test_pairs.txt` (without ground truth) used to evaluate the matching from the paper. Each line corresponds to one pair and is structured as follows:\n\n```\npath_image_A path_image_B exif_rotationA exif_rotationB [KA_0 ... KA_8] [KB_0 ... KB_8] [T_AB_0 ... T_AB_15]\n```\n\nThe `path_image_A` and `path_image_B` entries are paths to image A and B, respectively. The `exif_rotation` is an integer in the range [0, 3] that comes from the original EXIF metadata associated with the image, where, 0: no rotation, 1: 90 degree clockwise, 2: 180 degree clockwise, 3: 270 degree clockwise. If the EXIF data is not known, you can just provide a zero here and no rotation will be performed. `KA` and `KB` are the flattened `3x3` matrices of image A and image B intrinsics. `T_AB` is a flattened `4x4` matrix of the extrinsics between the pair.\n</details>\n\n### Reproducing the indoor evaluation on ScanNet\n\n<details>\n  <summary>[Click to expand]</summary>\n\nWe provide the groundtruth for ScanNet in our format in the file `assets/scannet_test_pairs_with_gt.txt` for convenience. In order to reproduce similar tables to what was in the paper, you will need to download the dataset (we do not provide the raw test images). To download the ScanNet dataset, do the following:\n\n1. Head to the [ScanNet](https://github.com/ScanNet/ScanNet) github repo to download the ScanNet test set (100 scenes).\n2. You will need to extract the raw sensor data from the 100 `.sens` files in each scene in the test set using the [SensReader](https://github.com/ScanNet/ScanNet/tree/master/SensReader) tool.\n\nOnce the ScanNet dataset is downloaded in `~/data/scannet`, you can run the following:\n\n```sh\n./match_pairs.py --input_dir ~/data/scannet --input_pairs assets/scannet_test_pairs_with_gt.txt --output_dir dump_scannet_test_results --eval\n```\n\nYou should get the following table for ScanNet (or something very close to it, see this [note](#a-note-on-reproducibility)):\n\n```txt\nEvaluation Results (mean over 1500 pairs):\nAUC@5    AUC@10  AUC@20  Prec    MScore\n16.12    33.76   51.79   84.37   31.14\n```\n\n</details>\n\n### Reproducing the outdoor evaluation on YFCC\n\n<details>\n  <summary>[Click to expand]</summary>\n\nWe provide the groundtruth for YFCC in our format in the file `assets/yfcc_test_pairs_with_gt.txt` for convenience. In order to reproduce similar tables to what was in the paper, you will need to download the dataset (we do not provide the raw test images). To download the YFCC dataset, you can use the [OANet](https://github.com/zjhthu/OANet) repo:\n\n```sh\ngit clone https://github.com/zjhthu/OANet\ncd OANet\nbash download_data.sh raw_data raw_data_yfcc.tar.gz 0 8\ntar -xvf raw_data_yfcc.tar.gz\nmv raw_data/yfcc100m ~/data\n```\n\nOnce the YFCC dataset is downloaded in `~/data/yfcc100m`, you can run the following:\n\n```sh\n./match_pairs.py --input_dir ~/data/yfcc100m --input_pairs assets/yfcc_test_pairs_with_gt.txt --output_dir dump_yfcc_test_results --eval --resize 1600 --superglue outdoor --max_keypoints 2048 --nms_radius 3 --resize_float\n```\n\nYou should get the following table for YFCC (or something very close to it, see this [note](#a-note-on-reproducibility)):\n\n```txt\nEvaluation Results (mean over 4000 pairs):\nAUC@5    AUC@10  AUC@20  Prec    MScore\n39.02    59.51   75.72   98.72   23.61  \n```\n\n</details>\n\n### Reproducing outdoor evaluation on Phototourism\n\n<details>\n  <summary>[Click to expand]</summary>\n\nThe Phototourism results shown in the paper were produced using similar data as the test set from the [Image Matching Challenge 2020](https://vision.uvic.ca/image-matching-challenge/), which holds the ground truth data private for the test set. We list the pairs we used in `assets/phototourism_test_pairs.txt`. To reproduce similar numbers on this test set, please submit to the challenge benchmark. While the challenge is still live, we cannot share the test set publically since we want to help maintain the integrity of the challenge. \n\n</details>\n\n### Correcting EXIF rotation data in YFCC and Phototourism\n\n<details>\n  <summary>[Click to expand]</summary>\n\nIn this repo, we provide manually corrected the EXIF rotation data for the outdoor evaluations on YFCC and Phototourism. For the YFCC dataset we found 7 images with incorrect EXIF rotation flags, resulting in 148 pairs out of 4000 being corrected. For Phototourism, we found 36 images with incorrect EXIF rotation flags, resulting in 212 out of 2200 pairs being corrected.\n\nThe SuperGlue paper reports the results of SuperGlue **without** the corrected rotations, while the numbers in this README are reported **with** the corrected rotations. We found that our final conclusions from the evaluation still hold with or without the corrected rotations. For backwards compatability, we included the original, uncorrected EXIF rotation data in `assets/phototourism_test_pairs_original.txt` and `assets/yfcc_test_pairs_with_gt_original.txt` respectively.\n\n</details>\n\n### Outdoor training / validation scene splits of MegaDepth\n\n<details>\n  <summary>[Click to expand]</summary>\n\nFor training and validation of the outdoor model, we used scenes from the [MegaDepth dataset](http://www.cs.cornell.edu/projects/megadepth/). We provide the list of scenes used to train the outdoor model in the `assets/` directory:\n\n* Training set: `assets/megadepth_train_scenes.txt`\n* Validation set: `assets/megadepth_validation_scenes.txt`\n\n</details>\n\n### A note on reproducibility\n\n<details>\n  <summary>[Click to expand]</summary>\n\nAfter simplifying the model code and evaluation code and preparing it for release, we made some improvements and tweaks that result in slightly different numbers than what was reported in the paper. The numbers and figures reported in the README were done using Ubuntu 16.04, OpenCV 3.4.5, and PyTorch 1.1.0. Even with matching the library versions, we observed some slight differences across Mac and Ubuntu, which we believe are due to differences in OpenCV's image resize function implementation and randomization of RANSAC.\n</details>\n\n### Creating high-quality PDF visualizations and faster visualization with --fast_viz\n\n<details>\n  <summary>[Click to expand]</summary>\n\nWhen generating output images with `match_pairs.py`, the default `--viz` flag uses a Matplotlib renderer which allows for the generation of camera-ready PDF visualizations if you additionally use `--viz_extension pdf` instead of the default png extension.\n\n```\n./match_pairs.py --viz --viz_extension pdf\n```\n\nAlternatively, you might want to save visualization images but have the generation be much faster.  You can use the `--fast_viz` flag to use an OpenCV-based image renderer as follows:\n\n```\n./match_pairs.py --viz --fast_viz\n```\n\nIf you would also like an OpenCV display window to preview the results (you must use non-pdf output and use fast_fiz), simply run:\n\n```\n./match_pairs.py --viz --fast_viz --opencv_display\n```\n\n</details>\n\n\n## BibTeX Citation\nIf you use any ideas from the paper or code from this repo, please consider citing:\n\n```txt\n@inproceedings{sarlin20superglue,\n  author    = {Paul-Edouard Sarlin and\n               Daniel DeTone and\n               Tomasz Malisiewicz and\n               Andrew Rabinovich},\n  title     = {{SuperGlue}: Learning Feature Matching with Graph Neural Networks},\n  booktitle = {CVPR},\n  year      = {2020},\n  url       = {https://arxiv.org/abs/1911.11763}\n}\n```\n\n## Additional Notes\n* For the demo, we found that the keyboard interaction works well with OpenCV 4.1.2.30, older versions were less responsive and the newest version had a [OpenCV bug on Mac](https://stackoverflow.com/questions/60032540/opencv-cv2-imshow-is-not-working-because-of-the-qt)\n* We generally do not recommend to run SuperPoint+SuperGlue below 160x120 resolution (QQVGA) and above 2000x1500\n* We do not intend to release the SuperGlue training code.\n* We do not intend to release the SIFT-based or homography SuperGlue models.\n\n## Legal Disclaimer\nMagic Leap is proud to provide its latest samples, toolkits, and research projects on Github to foster development and gather feedback from the spatial computing community. Use of the resources within this repo is subject to (a) the license(s) included herein, or (b) if no license is included, Magic Leap's [Developer Agreement](https://id.magicleap.com/terms/developer), which is available on our [Developer Portal](https://developer.magicleap.com/).\nIf you need more, just ask on the [forums](https://forum.magicleap.com/hc/en-us/community/topics)!\nWe're thrilled to be part of a well-meaning, friendly and welcoming community of millions.\n"
        },
        {
          "name": "assets",
          "type": "tree",
          "content": null
        },
        {
          "name": "demo_superglue.py",
          "type": "blob",
          "size": 10.4326171875,
          "content": "#! /usr/bin/env python3\n#\n# %BANNER_BEGIN%\n# ---------------------------------------------------------------------\n# %COPYRIGHT_BEGIN%\n#\n#  Magic Leap, Inc. (\"COMPANY\") CONFIDENTIAL\n#\n#  Unpublished Copyright (c) 2020\n#  Magic Leap, Inc., All Rights Reserved.\n#\n# NOTICE:  All information contained herein is, and remains the property\n# of COMPANY. The intellectual and technical concepts contained herein\n# are proprietary to COMPANY and may be covered by U.S. and Foreign\n# Patents, patents in process, and are protected by trade secret or\n# copyright law.  Dissemination of this information or reproduction of\n# this material is strictly forbidden unless prior written permission is\n# obtained from COMPANY.  Access to the source code contained herein is\n# hereby forbidden to anyone except current COMPANY employees, managers\n# or contractors who have executed Confidentiality and Non-disclosure\n# agreements explicitly covering such access.\n#\n# The copyright notice above does not evidence any actual or intended\n# publication or disclosure  of  this source code, which includes\n# information that is confidential and/or proprietary, and is a trade\n# secret, of  COMPANY.   ANY REPRODUCTION, MODIFICATION, DISTRIBUTION,\n# PUBLIC  PERFORMANCE, OR PUBLIC DISPLAY OF OR THROUGH USE  OF THIS\n# SOURCE CODE  WITHOUT THE EXPRESS WRITTEN CONSENT OF COMPANY IS\n# STRICTLY PROHIBITED, AND IN VIOLATION OF APPLICABLE LAWS AND\n# INTERNATIONAL TREATIES.  THE RECEIPT OR POSSESSION OF  THIS SOURCE\n# CODE AND/OR RELATED INFORMATION DOES NOT CONVEY OR IMPLY ANY RIGHTS\n# TO REPRODUCE, DISCLOSE OR DISTRIBUTE ITS CONTENTS, OR TO MANUFACTURE,\n# USE, OR SELL ANYTHING THAT IT  MAY DESCRIBE, IN WHOLE OR IN PART.\n#\n# %COPYRIGHT_END%\n# ----------------------------------------------------------------------\n# %AUTHORS_BEGIN%\n#\n#  Originating Authors: Paul-Edouard Sarlin\n#                       Daniel DeTone\n#                       Tomasz Malisiewicz\n#\n# %AUTHORS_END%\n# --------------------------------------------------------------------*/\n# %BANNER_END%\n\nfrom pathlib import Path\nimport argparse\nimport cv2\nimport matplotlib.cm as cm\nimport torch\n\nfrom models.matching import Matching\nfrom models.utils import (AverageTimer, VideoStreamer,\n                          make_matching_plot_fast, frame2tensor)\n\ntorch.set_grad_enabled(False)\n\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser(\n        description='SuperGlue demo',\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n    parser.add_argument(\n        '--input', type=str, default='0',\n        help='ID of a USB webcam, URL of an IP camera, '\n             'or path to an image directory or movie file')\n    parser.add_argument(\n        '--output_dir', type=str, default=None,\n        help='Directory where to write output frames (If None, no output)')\n\n    parser.add_argument(\n        '--image_glob', type=str, nargs='+', default=['*.png', '*.jpg', '*.jpeg'],\n        help='Glob if a directory of images is specified')\n    parser.add_argument(\n        '--skip', type=int, default=1,\n        help='Images to skip if input is a movie or directory')\n    parser.add_argument(\n        '--max_length', type=int, default=1000000,\n        help='Maximum length if input is a movie or directory')\n    parser.add_argument(\n        '--resize', type=int, nargs='+', default=[640, 480],\n        help='Resize the input image before running inference. If two numbers, '\n             'resize to the exact dimensions, if one number, resize the max '\n             'dimension, if -1, do not resize')\n\n    parser.add_argument(\n        '--superglue', choices={'indoor', 'outdoor'}, default='indoor',\n        help='SuperGlue weights')\n    parser.add_argument(\n        '--max_keypoints', type=int, default=-1,\n        help='Maximum number of keypoints detected by Superpoint'\n             ' (\\'-1\\' keeps all keypoints)')\n    parser.add_argument(\n        '--keypoint_threshold', type=float, default=0.005,\n        help='SuperPoint keypoint detector confidence threshold')\n    parser.add_argument(\n        '--nms_radius', type=int, default=4,\n        help='SuperPoint Non Maximum Suppression (NMS) radius'\n        ' (Must be positive)')\n    parser.add_argument(\n        '--sinkhorn_iterations', type=int, default=20,\n        help='Number of Sinkhorn iterations performed by SuperGlue')\n    parser.add_argument(\n        '--match_threshold', type=float, default=0.2,\n        help='SuperGlue match threshold')\n\n    parser.add_argument(\n        '--show_keypoints', action='store_true',\n        help='Show the detected keypoints')\n    parser.add_argument(\n        '--no_display', action='store_true',\n        help='Do not display images to screen. Useful if running remotely')\n    parser.add_argument(\n        '--force_cpu', action='store_true',\n        help='Force pytorch to run in CPU mode.')\n\n    opt = parser.parse_args()\n    print(opt)\n\n    if len(opt.resize) == 2 and opt.resize[1] == -1:\n        opt.resize = opt.resize[0:1]\n    if len(opt.resize) == 2:\n        print('Will resize to {}x{} (WxH)'.format(\n            opt.resize[0], opt.resize[1]))\n    elif len(opt.resize) == 1 and opt.resize[0] > 0:\n        print('Will resize max dimension to {}'.format(opt.resize[0]))\n    elif len(opt.resize) == 1:\n        print('Will not resize images')\n    else:\n        raise ValueError('Cannot specify more than two integers for --resize')\n\n    device = 'cuda' if torch.cuda.is_available() and not opt.force_cpu else 'cpu'\n    print('Running inference on device \\\"{}\\\"'.format(device))\n    config = {\n        'superpoint': {\n            'nms_radius': opt.nms_radius,\n            'keypoint_threshold': opt.keypoint_threshold,\n            'max_keypoints': opt.max_keypoints\n        },\n        'superglue': {\n            'weights': opt.superglue,\n            'sinkhorn_iterations': opt.sinkhorn_iterations,\n            'match_threshold': opt.match_threshold,\n        }\n    }\n    matching = Matching(config).eval().to(device)\n    keys = ['keypoints', 'scores', 'descriptors']\n\n    vs = VideoStreamer(opt.input, opt.resize, opt.skip,\n                       opt.image_glob, opt.max_length)\n    frame, ret = vs.next_frame()\n    assert ret, 'Error when reading the first frame (try different --input?)'\n\n    frame_tensor = frame2tensor(frame, device)\n    last_data = matching.superpoint({'image': frame_tensor})\n    last_data = {k+'0': last_data[k] for k in keys}\n    last_data['image0'] = frame_tensor\n    last_frame = frame\n    last_image_id = 0\n\n    if opt.output_dir is not None:\n        print('==> Will write outputs to {}'.format(opt.output_dir))\n        Path(opt.output_dir).mkdir(exist_ok=True)\n\n    # Create a window to display the demo.\n    if not opt.no_display:\n        cv2.namedWindow('SuperGlue matches', cv2.WINDOW_NORMAL)\n        cv2.resizeWindow('SuperGlue matches', 640*2, 480)\n    else:\n        print('Skipping visualization, will not show a GUI.')\n\n    # Print the keyboard help menu.\n    print('==> Keyboard control:\\n'\n          '\\tn: select the current frame as the anchor\\n'\n          '\\te/r: increase/decrease the keypoint confidence threshold\\n'\n          '\\td/f: increase/decrease the match filtering threshold\\n'\n          '\\tk: toggle the visualization of keypoints\\n'\n          '\\tq: quit')\n\n    timer = AverageTimer()\n\n    while True:\n        frame, ret = vs.next_frame()\n        if not ret:\n            print('Finished demo_superglue.py')\n            break\n        timer.update('data')\n        stem0, stem1 = last_image_id, vs.i - 1\n\n        frame_tensor = frame2tensor(frame, device)\n        pred = matching({**last_data, 'image1': frame_tensor})\n        kpts0 = last_data['keypoints0'][0].cpu().numpy()\n        kpts1 = pred['keypoints1'][0].cpu().numpy()\n        matches = pred['matches0'][0].cpu().numpy()\n        confidence = pred['matching_scores0'][0].cpu().numpy()\n        timer.update('forward')\n\n        valid = matches > -1\n        mkpts0 = kpts0[valid]\n        mkpts1 = kpts1[matches[valid]]\n        color = cm.jet(confidence[valid])\n        text = [\n            'SuperGlue',\n            'Keypoints: {}:{}'.format(len(kpts0), len(kpts1)),\n            'Matches: {}'.format(len(mkpts0))\n        ]\n        k_thresh = matching.superpoint.config['keypoint_threshold']\n        m_thresh = matching.superglue.config['match_threshold']\n        small_text = [\n            'Keypoint Threshold: {:.4f}'.format(k_thresh),\n            'Match Threshold: {:.2f}'.format(m_thresh),\n            'Image Pair: {:06}:{:06}'.format(stem0, stem1),\n        ]\n        out = make_matching_plot_fast(\n            last_frame, frame, kpts0, kpts1, mkpts0, mkpts1, color, text,\n            path=None, show_keypoints=opt.show_keypoints, small_text=small_text)\n\n        if not opt.no_display:\n            cv2.imshow('SuperGlue matches', out)\n            key = chr(cv2.waitKey(1) & 0xFF)\n            if key == 'q':\n                vs.cleanup()\n                print('Exiting (via q) demo_superglue.py')\n                break\n            elif key == 'n':  # set the current frame as anchor\n                last_data = {k+'0': pred[k+'1'] for k in keys}\n                last_data['image0'] = frame_tensor\n                last_frame = frame\n                last_image_id = (vs.i - 1)\n            elif key in ['e', 'r']:\n                # Increase/decrease keypoint threshold by 10% each keypress.\n                d = 0.1 * (-1 if key == 'e' else 1)\n                matching.superpoint.config['keypoint_threshold'] = min(max(\n                    0.0001, matching.superpoint.config['keypoint_threshold']*(1+d)), 1)\n                print('\\nChanged the keypoint threshold to {:.4f}'.format(\n                    matching.superpoint.config['keypoint_threshold']))\n            elif key in ['d', 'f']:\n                # Increase/decrease match threshold by 0.05 each keypress.\n                d = 0.05 * (-1 if key == 'd' else 1)\n                matching.superglue.config['match_threshold'] = min(max(\n                    0.05, matching.superglue.config['match_threshold']+d), .95)\n                print('\\nChanged the match threshold to {:.2f}'.format(\n                    matching.superglue.config['match_threshold']))\n            elif key == 'k':\n                opt.show_keypoints = not opt.show_keypoints\n\n        timer.update('viz')\n        timer.print()\n\n        if opt.output_dir is not None:\n            #stem = 'matches_{:06}_{:06}'.format(last_image_id, vs.i-1)\n            stem = 'matches_{:06}_{:06}'.format(stem0, stem1)\n            out_file = str(Path(opt.output_dir, stem + '.png'))\n            print('\\nWriting image to {}'.format(out_file))\n            cv2.imwrite(out_file, out)\n\n    cv2.destroyAllWindows()\n    vs.cleanup()\n"
        },
        {
          "name": "match_pairs.py",
          "type": "blob",
          "size": 17.7021484375,
          "content": "#! /usr/bin/env python3\n#\n# %BANNER_BEGIN%\n# ---------------------------------------------------------------------\n# %COPYRIGHT_BEGIN%\n#\n#  Magic Leap, Inc. (\"COMPANY\") CONFIDENTIAL\n#\n#  Unpublished Copyright (c) 2020\n#  Magic Leap, Inc., All Rights Reserved.\n#\n# NOTICE:  All information contained herein is, and remains the property\n# of COMPANY. The intellectual and technical concepts contained herein\n# are proprietary to COMPANY and may be covered by U.S. and Foreign\n# Patents, patents in process, and are protected by trade secret or\n# copyright law.  Dissemination of this information or reproduction of\n# this material is strictly forbidden unless prior written permission is\n# obtained from COMPANY.  Access to the source code contained herein is\n# hereby forbidden to anyone except current COMPANY employees, managers\n# or contractors who have executed Confidentiality and Non-disclosure\n# agreements explicitly covering such access.\n#\n# The copyright notice above does not evidence any actual or intended\n# publication or disclosure  of  this source code, which includes\n# information that is confidential and/or proprietary, and is a trade\n# secret, of  COMPANY.   ANY REPRODUCTION, MODIFICATION, DISTRIBUTION,\n# PUBLIC  PERFORMANCE, OR PUBLIC DISPLAY OF OR THROUGH USE  OF THIS\n# SOURCE CODE  WITHOUT THE EXPRESS WRITTEN CONSENT OF COMPANY IS\n# STRICTLY PROHIBITED, AND IN VIOLATION OF APPLICABLE LAWS AND\n# INTERNATIONAL TREATIES.  THE RECEIPT OR POSSESSION OF  THIS SOURCE\n# CODE AND/OR RELATED INFORMATION DOES NOT CONVEY OR IMPLY ANY RIGHTS\n# TO REPRODUCE, DISCLOSE OR DISTRIBUTE ITS CONTENTS, OR TO MANUFACTURE,\n# USE, OR SELL ANYTHING THAT IT  MAY DESCRIBE, IN WHOLE OR IN PART.\n#\n# %COPYRIGHT_END%\n# ----------------------------------------------------------------------\n# %AUTHORS_BEGIN%\n#\n#  Originating Authors: Paul-Edouard Sarlin\n#                       Daniel DeTone\n#                       Tomasz Malisiewicz\n#\n# %AUTHORS_END%\n# --------------------------------------------------------------------*/\n# %BANNER_END%\n\nfrom pathlib import Path\nimport argparse\nimport random\nimport numpy as np\nimport matplotlib.cm as cm\nimport torch\n\n\nfrom models.matching import Matching\nfrom models.utils import (compute_pose_error, compute_epipolar_error,\n                          estimate_pose, make_matching_plot,\n                          error_colormap, AverageTimer, pose_auc, read_image,\n                          rotate_intrinsics, rotate_pose_inplane,\n                          scale_intrinsics)\n\ntorch.set_grad_enabled(False)\n\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser(\n        description='Image pair matching and pose evaluation with SuperGlue',\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n\n    parser.add_argument(\n        '--input_pairs', type=str, default='assets/scannet_sample_pairs_with_gt.txt',\n        help='Path to the list of image pairs')\n    parser.add_argument(\n        '--input_dir', type=str, default='assets/scannet_sample_images/',\n        help='Path to the directory that contains the images')\n    parser.add_argument(\n        '--output_dir', type=str, default='dump_match_pairs/',\n        help='Path to the directory in which the .npz results and optionally,'\n             'the visualization images are written')\n\n    parser.add_argument(\n        '--max_length', type=int, default=-1,\n        help='Maximum number of pairs to evaluate')\n    parser.add_argument(\n        '--resize', type=int, nargs='+', default=[640, 480],\n        help='Resize the input image before running inference. If two numbers, '\n             'resize to the exact dimensions, if one number, resize the max '\n             'dimension, if -1, do not resize')\n    parser.add_argument(\n        '--resize_float', action='store_true',\n        help='Resize the image after casting uint8 to float')\n\n    parser.add_argument(\n        '--superglue', choices={'indoor', 'outdoor'}, default='indoor',\n        help='SuperGlue weights')\n    parser.add_argument(\n        '--max_keypoints', type=int, default=1024,\n        help='Maximum number of keypoints detected by Superpoint'\n             ' (\\'-1\\' keeps all keypoints)')\n    parser.add_argument(\n        '--keypoint_threshold', type=float, default=0.005,\n        help='SuperPoint keypoint detector confidence threshold')\n    parser.add_argument(\n        '--nms_radius', type=int, default=4,\n        help='SuperPoint Non Maximum Suppression (NMS) radius'\n        ' (Must be positive)')\n    parser.add_argument(\n        '--sinkhorn_iterations', type=int, default=20,\n        help='Number of Sinkhorn iterations performed by SuperGlue')\n    parser.add_argument(\n        '--match_threshold', type=float, default=0.2,\n        help='SuperGlue match threshold')\n\n    parser.add_argument(\n        '--viz', action='store_true',\n        help='Visualize the matches and dump the plots')\n    parser.add_argument(\n        '--eval', action='store_true',\n        help='Perform the evaluation'\n             ' (requires ground truth pose and intrinsics)')\n    parser.add_argument(\n        '--fast_viz', action='store_true',\n        help='Use faster image visualization with OpenCV instead of Matplotlib')\n    parser.add_argument(\n        '--cache', action='store_true',\n        help='Skip the pair if output .npz files are already found')\n    parser.add_argument(\n        '--show_keypoints', action='store_true',\n        help='Plot the keypoints in addition to the matches')\n    parser.add_argument(\n        '--viz_extension', type=str, default='png', choices=['png', 'pdf'],\n        help='Visualization file extension. Use pdf for highest-quality.')\n    parser.add_argument(\n        '--opencv_display', action='store_true',\n        help='Visualize via OpenCV before saving output images')\n    parser.add_argument(\n        '--shuffle', action='store_true',\n        help='Shuffle ordering of pairs before processing')\n    parser.add_argument(\n        '--force_cpu', action='store_true',\n        help='Force pytorch to run in CPU mode.')\n\n    opt = parser.parse_args()\n    print(opt)\n\n    assert not (opt.opencv_display and not opt.viz), 'Must use --viz with --opencv_display'\n    assert not (opt.opencv_display and not opt.fast_viz), 'Cannot use --opencv_display without --fast_viz'\n    assert not (opt.fast_viz and not opt.viz), 'Must use --viz with --fast_viz'\n    assert not (opt.fast_viz and opt.viz_extension == 'pdf'), 'Cannot use pdf extension with --fast_viz'\n\n    if len(opt.resize) == 2 and opt.resize[1] == -1:\n        opt.resize = opt.resize[0:1]\n    if len(opt.resize) == 2:\n        print('Will resize to {}x{} (WxH)'.format(\n            opt.resize[0], opt.resize[1]))\n    elif len(opt.resize) == 1 and opt.resize[0] > 0:\n        print('Will resize max dimension to {}'.format(opt.resize[0]))\n    elif len(opt.resize) == 1:\n        print('Will not resize images')\n    else:\n        raise ValueError('Cannot specify more than two integers for --resize')\n\n    with open(opt.input_pairs, 'r') as f:\n        pairs = [l.split() for l in f.readlines()]\n\n    if opt.max_length > -1:\n        pairs = pairs[0:np.min([len(pairs), opt.max_length])]\n\n    if opt.shuffle:\n        random.Random(0).shuffle(pairs)\n\n    if opt.eval:\n        if not all([len(p) == 38 for p in pairs]):\n            raise ValueError(\n                'All pairs should have ground truth info for evaluation.'\n                'File \\\"{}\\\" needs 38 valid entries per row'.format(opt.input_pairs))\n\n    # Load the SuperPoint and SuperGlue models.\n    device = 'cuda' if torch.cuda.is_available() and not opt.force_cpu else 'cpu'\n    print('Running inference on device \\\"{}\\\"'.format(device))\n    config = {\n        'superpoint': {\n            'nms_radius': opt.nms_radius,\n            'keypoint_threshold': opt.keypoint_threshold,\n            'max_keypoints': opt.max_keypoints\n        },\n        'superglue': {\n            'weights': opt.superglue,\n            'sinkhorn_iterations': opt.sinkhorn_iterations,\n            'match_threshold': opt.match_threshold,\n        }\n    }\n    matching = Matching(config).eval().to(device)\n\n    # Create the output directories if they do not exist already.\n    input_dir = Path(opt.input_dir)\n    print('Looking for data in directory \\\"{}\\\"'.format(input_dir))\n    output_dir = Path(opt.output_dir)\n    output_dir.mkdir(exist_ok=True, parents=True)\n    print('Will write matches to directory \\\"{}\\\"'.format(output_dir))\n    if opt.eval:\n        print('Will write evaluation results',\n              'to directory \\\"{}\\\"'.format(output_dir))\n    if opt.viz:\n        print('Will write visualization images to',\n              'directory \\\"{}\\\"'.format(output_dir))\n\n    timer = AverageTimer(newline=True)\n    for i, pair in enumerate(pairs):\n        name0, name1 = pair[:2]\n        stem0, stem1 = Path(name0).stem, Path(name1).stem\n        matches_path = output_dir / '{}_{}_matches.npz'.format(stem0, stem1)\n        eval_path = output_dir / '{}_{}_evaluation.npz'.format(stem0, stem1)\n        viz_path = output_dir / '{}_{}_matches.{}'.format(stem0, stem1, opt.viz_extension)\n        viz_eval_path = output_dir / \\\n            '{}_{}_evaluation.{}'.format(stem0, stem1, opt.viz_extension)\n\n        # Handle --cache logic.\n        do_match = True\n        do_eval = opt.eval\n        do_viz = opt.viz\n        do_viz_eval = opt.eval and opt.viz\n        if opt.cache:\n            if matches_path.exists():\n                try:\n                    results = np.load(matches_path)\n                except:\n                    raise IOError('Cannot load matches .npz file: %s' %\n                                  matches_path)\n\n                kpts0, kpts1 = results['keypoints0'], results['keypoints1']\n                matches, conf = results['matches'], results['match_confidence']\n                do_match = False\n            if opt.eval and eval_path.exists():\n                try:\n                    results = np.load(eval_path)\n                except:\n                    raise IOError('Cannot load eval .npz file: %s' % eval_path)\n                err_R, err_t = results['error_R'], results['error_t']\n                precision = results['precision']\n                matching_score = results['matching_score']\n                num_correct = results['num_correct']\n                epi_errs = results['epipolar_errors']\n                do_eval = False\n            if opt.viz and viz_path.exists():\n                do_viz = False\n            if opt.viz and opt.eval and viz_eval_path.exists():\n                do_viz_eval = False\n            timer.update('load_cache')\n\n        if not (do_match or do_eval or do_viz or do_viz_eval):\n            timer.print('Finished pair {:5} of {:5}'.format(i, len(pairs)))\n            continue\n\n        # If a rotation integer is provided (e.g. from EXIF data), use it:\n        if len(pair) >= 5:\n            rot0, rot1 = int(pair[2]), int(pair[3])\n        else:\n            rot0, rot1 = 0, 0\n\n        # Load the image pair.\n        image0, inp0, scales0 = read_image(\n            input_dir / name0, device, opt.resize, rot0, opt.resize_float)\n        image1, inp1, scales1 = read_image(\n            input_dir / name1, device, opt.resize, rot1, opt.resize_float)\n        if image0 is None or image1 is None:\n            print('Problem reading image pair: {} {}'.format(\n                input_dir/name0, input_dir/name1))\n            exit(1)\n        timer.update('load_image')\n\n        if do_match:\n            # Perform the matching.\n            pred = matching({'image0': inp0, 'image1': inp1})\n            pred = {k: v[0].cpu().numpy() for k, v in pred.items()}\n            kpts0, kpts1 = pred['keypoints0'], pred['keypoints1']\n            matches, conf = pred['matches0'], pred['matching_scores0']\n            timer.update('matcher')\n\n            # Write the matches to disk.\n            out_matches = {'keypoints0': kpts0, 'keypoints1': kpts1,\n                           'matches': matches, 'match_confidence': conf}\n            np.savez(str(matches_path), **out_matches)\n\n        # Keep the matching keypoints.\n        valid = matches > -1\n        mkpts0 = kpts0[valid]\n        mkpts1 = kpts1[matches[valid]]\n        mconf = conf[valid]\n\n        if do_eval:\n            # Estimate the pose and compute the pose error.\n            assert len(pair) == 38, 'Pair does not have ground truth info'\n            K0 = np.array(pair[4:13]).astype(float).reshape(3, 3)\n            K1 = np.array(pair[13:22]).astype(float).reshape(3, 3)\n            T_0to1 = np.array(pair[22:]).astype(float).reshape(4, 4)\n\n            # Scale the intrinsics to resized image.\n            K0 = scale_intrinsics(K0, scales0)\n            K1 = scale_intrinsics(K1, scales1)\n\n            # Update the intrinsics + extrinsics if EXIF rotation was found.\n            if rot0 != 0 or rot1 != 0:\n                cam0_T_w = np.eye(4)\n                cam1_T_w = T_0to1\n                if rot0 != 0:\n                    K0 = rotate_intrinsics(K0, image0.shape, rot0)\n                    cam0_T_w = rotate_pose_inplane(cam0_T_w, rot0)\n                if rot1 != 0:\n                    K1 = rotate_intrinsics(K1, image1.shape, rot1)\n                    cam1_T_w = rotate_pose_inplane(cam1_T_w, rot1)\n                cam1_T_cam0 = cam1_T_w @ np.linalg.inv(cam0_T_w)\n                T_0to1 = cam1_T_cam0\n\n            epi_errs = compute_epipolar_error(mkpts0, mkpts1, T_0to1, K0, K1)\n            correct = epi_errs < 5e-4\n            num_correct = np.sum(correct)\n            precision = np.mean(correct) if len(correct) > 0 else 0\n            matching_score = num_correct / len(kpts0) if len(kpts0) > 0 else 0\n\n            thresh = 1.  # In pixels relative to resized image size.\n            ret = estimate_pose(mkpts0, mkpts1, K0, K1, thresh)\n            if ret is None:\n                err_t, err_R = np.inf, np.inf\n            else:\n                R, t, inliers = ret\n                err_t, err_R = compute_pose_error(T_0to1, R, t)\n\n            # Write the evaluation results to disk.\n            out_eval = {'error_t': err_t,\n                        'error_R': err_R,\n                        'precision': precision,\n                        'matching_score': matching_score,\n                        'num_correct': num_correct,\n                        'epipolar_errors': epi_errs}\n            np.savez(str(eval_path), **out_eval)\n            timer.update('eval')\n\n        if do_viz:\n            # Visualize the matches.\n            color = cm.jet(mconf)\n            text = [\n                'SuperGlue',\n                'Keypoints: {}:{}'.format(len(kpts0), len(kpts1)),\n                'Matches: {}'.format(len(mkpts0)),\n            ]\n            if rot0 != 0 or rot1 != 0:\n                text.append('Rotation: {}:{}'.format(rot0, rot1))\n\n            # Display extra parameter info.\n            k_thresh = matching.superpoint.config['keypoint_threshold']\n            m_thresh = matching.superglue.config['match_threshold']\n            small_text = [\n                'Keypoint Threshold: {:.4f}'.format(k_thresh),\n                'Match Threshold: {:.2f}'.format(m_thresh),\n                'Image Pair: {}:{}'.format(stem0, stem1),\n            ]\n\n            make_matching_plot(\n                image0, image1, kpts0, kpts1, mkpts0, mkpts1, color,\n                text, viz_path, opt.show_keypoints,\n                opt.fast_viz, opt.opencv_display, 'Matches', small_text)\n\n            timer.update('viz_match')\n\n        if do_viz_eval:\n            # Visualize the evaluation results for the image pair.\n            color = np.clip((epi_errs - 0) / (1e-3 - 0), 0, 1)\n            color = error_colormap(1 - color)\n            deg, delta = ' deg', 'Delta '\n            if not opt.fast_viz:\n                deg, delta = '', '$\\\\Delta$'\n            e_t = 'FAIL' if np.isinf(err_t) else '{:.1f}{}'.format(err_t, deg)\n            e_R = 'FAIL' if np.isinf(err_R) else '{:.1f}{}'.format(err_R, deg)\n            text = [\n                'SuperGlue',\n                '{}R: {}'.format(delta, e_R), '{}t: {}'.format(delta, e_t),\n                'inliers: {}/{}'.format(num_correct, (matches > -1).sum()),\n            ]\n            if rot0 != 0 or rot1 != 0:\n                text.append('Rotation: {}:{}'.format(rot0, rot1))\n\n            # Display extra parameter info (only works with --fast_viz).\n            k_thresh = matching.superpoint.config['keypoint_threshold']\n            m_thresh = matching.superglue.config['match_threshold']\n            small_text = [\n                'Keypoint Threshold: {:.4f}'.format(k_thresh),\n                'Match Threshold: {:.2f}'.format(m_thresh),\n                'Image Pair: {}:{}'.format(stem0, stem1),\n            ]\n\n            make_matching_plot(\n                image0, image1, kpts0, kpts1, mkpts0,\n                mkpts1, color, text, viz_eval_path,\n                opt.show_keypoints, opt.fast_viz,\n                opt.opencv_display, 'Relative Pose', small_text)\n\n            timer.update('viz_eval')\n\n        timer.print('Finished pair {:5} of {:5}'.format(i, len(pairs)))\n\n    if opt.eval:\n        # Collate the results into a final table and print to terminal.\n        pose_errors = []\n        precisions = []\n        matching_scores = []\n        for pair in pairs:\n            name0, name1 = pair[:2]\n            stem0, stem1 = Path(name0).stem, Path(name1).stem\n            eval_path = output_dir / \\\n                '{}_{}_evaluation.npz'.format(stem0, stem1)\n            results = np.load(eval_path)\n            pose_error = np.maximum(results['error_t'], results['error_R'])\n            pose_errors.append(pose_error)\n            precisions.append(results['precision'])\n            matching_scores.append(results['matching_score'])\n        thresholds = [5, 10, 20]\n        aucs = pose_auc(pose_errors, thresholds)\n        aucs = [100.*yy for yy in aucs]\n        prec = 100.*np.mean(precisions)\n        ms = 100.*np.mean(matching_scores)\n        print('Evaluation Results (mean over {} pairs):'.format(len(pairs)))\n        print('AUC@5\\t AUC@10\\t AUC@20\\t Prec\\t MScore\\t')\n        print('{:.2f}\\t {:.2f}\\t {:.2f}\\t {:.2f}\\t {:.2f}\\t'.format(\n            aucs[0], aucs[1], aucs[2], prec, ms))\n"
        },
        {
          "name": "models",
          "type": "tree",
          "content": null
        },
        {
          "name": "requirements.txt",
          "type": "blob",
          "size": 0.0673828125,
          "content": "matplotlib>=3.1.3\ntorch>=1.1.0\nopencv-python==4.1.2.30\nnumpy>=1.18.1\n"
        }
      ]
    }
  ]
}