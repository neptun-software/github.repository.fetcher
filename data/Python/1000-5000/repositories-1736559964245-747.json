{
  "metadata": {
    "timestamp": 1736559964245,
    "page": 747,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjc1MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "salesforce/Merlion",
      "stars": 3494,
      "defaultBranch": "main",
      "files": [
        {
          "name": ".copyright.tmpl",
          "type": "blob",
          "size": 0.1953125,
          "content": "Copyright (c) ${years} ${owner}\nAll rights reserved.\nSPDX-License-Identifier: BSD-3-Clause\nFor full license text, see the LICENSE file in the repo root or https://opensource.org/licenses/BSD-3-Clause\n"
        },
        {
          "name": ".dockerignore",
          "type": "blob",
          "size": 0.19921875,
          "content": "# package\n__pycache__\n*.egg-info\ndocs\ntmp\nts_datasets\n# pytest\n.pytest_cache\n.coverage*\nhtmlcov\n# IDE/system\n.idea\n*.swp\n.DS_Store\nsandbox\n.vscode\nIcon?\n# build files\ndocs/build/*\n.ipynb_checkpoints\nvenv/"
        },
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.1826171875,
          "content": "# package\n__pycache__\n*.egg-info\ntmp\n# pytest\n.pytest_cache\n.coverage*\nhtmlcov\n# IDE/system\n.idea\n*.swp\n.DS_Store\nsandbox\n.vscode\nIcon?\n# build files\ndocs/build/*\n.ipynb_checkpoints\nvenv/"
        },
        {
          "name": ".pre-commit-config.yaml",
          "type": "blob",
          "size": 0.3564453125,
          "content": "repos:\n- repo: https://github.com/psf/black\n  rev: '22.10.0'\n  hooks:\n  - id: black\n    args: [\"--line-length\", \"120\"]\n- repo: https://github.com/johann-petrak/licenseheaders.git\n  rev: 'v0.8.8'\n  hooks:\n    - id: licenseheaders\n      args: [\"-t\", \".copyright.tmpl\", \"-cy\", \"-o\", \"salesforce.com, inc.\",\n             \"-E\", \".py\", \"-x\", \"docs/source/conf.py\", \"-f\"]\n"
        },
        {
          "name": "AUTHORS.md",
          "type": "blob",
          "size": 0.3125,
          "content": "Aadyot Bhatnagar\nPaul Kassianik\nChenghao Liu\nTian Lan\nWenzhuo Yang\nRowan Cassius\nDoyen Sahoo\nDevansh Arpit\nSri Subramanian\nGerald Woo\nAmrita Saha\nArun Kumar Jagota\nGokulakrishnan Gopalakrishnan\nManpreet Singh\nK C Krithika\nSukumar Maddineni\nDaeki Cho\nBo Zong\nYingbo Zhou\nCaiming Xiong\nSilvio Savarese\nSteven Hoi\nHuan Wang"
        },
        {
          "name": "CODEOWNERS",
          "type": "blob",
          "size": 0.13671875,
          "content": "# Comment line immediately above ownership line is reserved for related gus information. Please be careful while editing.\n#ECCN:Open Source\n"
        },
        {
          "name": "CODE_OF_CONDUCT.md",
          "type": "blob",
          "size": 5.037109375,
          "content": "# Salesforce Open Source Community Code of Conduct\n\n## About the Code of Conduct\n\nEquality is a core value at Salesforce. We believe a diverse and inclusive\ncommunity fosters innovation and creativity, and are committed to building a\nculture where everyone feels included.\n\nSalesforce open-source projects are committed to providing a friendly, safe, and\nwelcoming environment for all, regardless of gender identity and expression,\nsexual orientation, disability, physical appearance, body size, ethnicity, nationality, \nrace, age, religion, level of experience, education, socioeconomic status, or \nother similar personal characteristics.\n\nThe goal of this code of conduct is to specify a baseline standard of behavior so\nthat people with different social values and communication styles can work\ntogether effectively, productively, and respectfully in our open source community.\nIt also establishes a mechanism for reporting issues and resolving conflicts.\n\nAll questions and reports of abusive, harassing, or otherwise unacceptable behavior\nin a Salesforce open-source project may be reported by contacting the Salesforce\nOpen Source Conduct Committee at ossconduct@salesforce.com.\n\n## Our Pledge\n\nIn the interest of fostering an open and welcoming environment, we as\ncontributors and maintainers pledge to making participation in our project and\nour community a harassment-free experience for everyone, regardless of gender \nidentity and expression, sexual orientation, disability, physical appearance, \nbody size, ethnicity, nationality, race, age, religion, level of experience, education, \nsocioeconomic status, or other similar personal characteristics.\n\n## Our Standards\n\nExamples of behavior that contributes to creating a positive environment\ninclude:\n\n* Using welcoming and inclusive language\n* Being respectful of differing viewpoints and experiences\n* Gracefully accepting constructive criticism\n* Focusing on what is best for the community\n* Showing empathy toward other community members\n\nExamples of unacceptable behavior by participants include:\n\n* The use of sexualized language or imagery and unwelcome sexual attention or\nadvances\n* Personal attacks, insulting/derogatory comments, or trolling\n* Public or private harassment\n* Publishing, or threatening to publish, others' private information—such as\na physical or electronic address—without explicit permission\n* Other conduct which could reasonably be considered inappropriate in a\nprofessional setting\n* Advocating for or encouraging any of the above behaviors\n\n## Our Responsibilities\n\nProject maintainers are responsible for clarifying the standards of acceptable\nbehavior and are expected to take appropriate and fair corrective action in\nresponse to any instances of unacceptable behavior.\n\nProject maintainers have the right and responsibility to remove, edit, or\nreject comments, commits, code, wiki edits, issues, and other contributions\nthat are not aligned with this Code of Conduct, or to ban temporarily or\npermanently any contributor for other behaviors that they deem inappropriate,\nthreatening, offensive, or harmful.\n\n## Scope\n\nThis Code of Conduct applies both within project spaces and in public spaces\nwhen an individual is representing the project or its community. Examples of\nrepresenting a project or community include using an official project email\naddress, posting via an official social media account, or acting as an appointed\nrepresentative at an online or offline event. Representation of a project may be\nfurther defined and clarified by project maintainers.\n\n## Enforcement\n\nInstances of abusive, harassing, or otherwise unacceptable behavior may be\nreported by contacting the Salesforce Open Source Conduct Committee \nat ossconduct@salesforce.com. All complaints will be reviewed and investigated \nand will result in a response that is deemed necessary and appropriate to the \ncircumstances. The committee is obligated to maintain confidentiality with \nregard to the reporter of an incident. Further details of specific enforcement \npolicies may be posted separately.\n\nProject maintainers who do not follow or enforce the Code of Conduct in good\nfaith may face temporary or permanent repercussions as determined by other\nmembers of the project's leadership and the Salesforce Open Source Conduct \nCommittee.\n\n## Attribution\n\nThis Code of Conduct is adapted from the [Contributor Covenant][contributor-covenant-home],\nversion 1.4, available at https://www.contributor-covenant.org/version/1/4/code-of-conduct.html. \nIt includes adaptions and additions from [Go Community Code of Conduct][golang-coc], \n[CNCF Code of Conduct][cncf-coc], and [Microsoft Open Source Code of Conduct][microsoft-coc].\n\nThis Code of Conduct is licensed under the [Creative Commons Attribution 3.0 License][cc-by-3-us].\n\n[contributor-covenant-home]: https://www.contributor-covenant.org (https://www.contributor-covenant.org/)\n[golang-coc]: https://golang.org/conduct\n[cncf-coc]: https://github.com/cncf/foundation/blob/master/code-of-conduct.md\n[microsoft-coc]: https://opensource.microsoft.com/codeofconduct/\n[cc-by-3-us]: https://creativecommons.org/licenses/by/3.0/us/"
        },
        {
          "name": "CONTRIBUTING.md",
          "type": "blob",
          "size": 9.8271484375,
          "content": "# Contributing to Merlion\nThank you for your interest in contributing to Merlion! This document will help you get started with your contribution.\nBefore you get started, clone this repo, ``pip install pre-commit``, and run ``pre-commit install`` from the root\ndirectory of the repo. This will ensure all files are formatted correctly and contain the appropriate\nlicense headers whenever you make a commit.\n\n## Table of Contents\n1. [Models](#models)\n    1. [Anomaly Detectors](#anomaly-detectors)\n    1. [Forecasters](#forecasters)\n    1. [Forecaster-Based Anomaly Detectors](#forecaster-based-anomaly-detectors)\n1. [Data Pre-Processing Transforms](#data-pre-processing-transforms)\n1. [Datasets](#datasets)\n\n## Models\nMerlion supports two kinds of models: anomaly detectors (`merlion.models.anomaly`) and forecasters\n(`merlion.models.forecast`). Each model should inherit from its respective base class:\n`merlion.models.anomaly.base.DetectorBase` for anomaly detectors, and `merlion.models.forecast.base.ForecasterBase`\nfor forecasters. Make sure to carefully read the [API docs](https://opensource.salesforce.com/Merlion/merlion.models.html)\nfor the appropriate base class and module before  proceeding.\n\nAfter implementing your model, please register it with the [model factory](merlion/models/factory.py), and add it to\nthe appropriate config file for the benchmarking scripts ([`conf/benchmark_anomaly.py`](conf/benchmark_anomaly.json)\nor [`conf/benchmark_forecast.json`](conf/benchmark_forecast.json)). Also implement some [unit tests](tests) to ensure\nthat the model is behaving as expected.\n\nFinally, update the API docs by\n-   Adding your new module to the autosummary block of the appropriate `__init__.py`\n    (e.g. [`merlion/models/anomaly/__init__.py`](merlion/models/anomaly/__init__.py) for an anomaly detector)\n-   Adding your new module to the appropriate Sphinx ReST file (e.g. \n    [`docs/source/merlion.models.anomaly.rst`](docs/source/merlion.models.anomaly.rst) for an anomaly detector)\n\n### Anomaly Detectors\nTo implement a new anomaly detector, you need to do the following:\n-   Implement an appropriate  `ConfigClass` which inherits from `merlion.models.anomaly.base.DetectorConfig`\n-   Implement an initializer that takes an instance of this config class, i.e. you can instantiate\n    `model = ModelClass(config)`. `ModelClass` should inherit from `DetectorBase`.\n-   Set the model class's class variable `config_class` equal to the `ConfigClass` above\n-   Implement the abstract `_train()` method, and have it return the model's sequence of anomaly scores on the train\n    data.\n-   Implement the abstract `_get_anomaly_score()` method, which returns the model's predicted anomaly score on an\n    input time series.\n\nYou may optionally override the following class variables of `ConfigClass` or `ModelClass`:\n-   `ConfigClass._default_transform`: if the `transform` keyword argument is not given when initializing the model's\n    config object, we use this default transform. If not overridden, we just use the `Identity` (i.e. no pre-processing)\n-   `ConfigClass._default_post_rule`: if the `post_rule` keyword argument is not given when initializing the model's\n    config object, we use this default post-rule. This is the rule used to post-process the output of\n    `model.get_anomaly_score()` (e.g. to reduce noise). You can get the post-processed anomaly scores by\n    calling `model.get_anomaly_label()`\n-   `ModelClass._default_post_rule_train_config`: If `post_rule_train_config` is not passed to `model.train()` use this\n    default config to train the post-rule (e.g. select the minimum anomaly score we want to set as a detection\n    threshold).\n\nSee our implementation of [Isolation Forest](merlion/models/anomaly/isolation_forest.py) for a fairly simple example of\nwhat this looks like in practice, and this [notebook](examples/anomaly/3_AnomalyNewModel.ipynb) for a step-by-step\nwalkthrough of a minimal example.\n\n### Forecasters\nTo implement a new forecaster, you need to do the following:\n-   Implement an appropriate  `ConfigClass` which inherits from `merlion.models.forecast.base.ForecasterConfig`. Make\n    sure that this object takes a parameter `max_forecast_steps` if your model has a maximum horizon it is allowed to\n    forecast for.\n-   Implement an initializer that takes an instance of this config class, i.e. you can instantiate\n    `model = ModelClass(config)`. `ModelClass` should inherit from `ForecasterBase`.\n-   Set the model class's class variable `config_class` equal to the `ConfigClass` above\n-   Implement the abstract `_train()` method, and have it return the model's sequence of predictions on the train\n    data. These predictions should have the same format as the output of `model._forecast()` (see below).\n-   Implement the abstract `_forecast()` method, which returns the model's forecast (and optionally the standard errors\n    of that forecast, or `None` if the model does not have uncertainty quantification) on the timestamps of interest.\n\nYou may optionally override the class variable `ConfigClass._default_transform`. This is the default data pre-processing\ntransform used to process the data before giving it to the model, if the `transform` keyword argument is not\ngiven when initializing the config.\n\nSee our implementation of [SARIMA](merlion/models/forecast/sarima.py) for a fairly simple example of what this looks\nlike in practice, and this [notebook](examples/forecast/4_ForecastNewModel.ipynb) for a step-by-step walkthrough of a\nminimal example.\n\n### Forecaster-Based Anomaly Detectors\nForecaster-based anomaly detectors convert a model's forecast into an anomaly score, by comparing the residual between\nthe true value and the forecaster's predicted value. Their base class is\n`merlion.models.anomaly.forecast_based.base.ForecastingDetectorBase`. \n\nConsider a forecaster class `Forecaster` with config class `ForecasterConfig`. It is fairly straightforward to extend\nthis class into an `ForecasterDetectorClass`. You need to do the following things:\n-   Define a config class which inherits from both `ForecasterConfig` and `DetectorConfig`\n    (in that order). You may optionally specify a `_default_post_rule` class variable, but the `_default_transform`\n    will be the same as `ForecasterConfig`.\n-   Implement a model class which inherits from both `ForecastingDetectorBase` and `ForecasterClass` (in that order).\n    You may optionally override the `_default_post_rule_train_config` class variable.\n\nSee our implementation of a [Prophet-based anomaly detector](merlion/models/anomaly/forecast_based/prophet.py) for an\nexample of what this looks like in practice, as well as the forecaster tutorial \n[notebook](examples/forecast/4_ForecastNewModel.ipynb).\n\n## Data Pre-Processing Transforms\nTo implement a new data pre-processing transform, begin by reading the\n[API docs](https://opensource.salesforce.com/Merlion/merlion.transform.html) for the base classes\n`merlion.transform.base.TransformBase` and `merlion.transform.base.InvertibleTransformBase`. Inherit from the\nappropriate base class, depending on whether you transform is invertible. However, even non-invertible transforms\nshould support pseudo-inversion. For example, when you take the moving average of an input time series with\n`merlion.transform.moving_average.MovingAverage`, it isn't guaranteed that the original time series can be recovered.\nHowever, we can approximate the desired inversion by keeping track of the boundary values and performing a\nde-convolution.\n\nTo actually implement a transform, override the abstract methods `train()` (this may be a no-op for\ntransforms with no data-dependent parameters), `__call__()` (actually applying the transform), and `_invert` (inverting\nor pseudo-inverting the transform). Then, register it with the [transform factory](merlion/transform/factory.py).\n\nSpecify the class property `requires_inversion_state` to indicate whether the inversion (or pseudo-inversion) is\nstateless (e.g. `merlion.transform.Rescale` simply rescales an input by a fixed scale and bias) or stateful\n(e.g. `merlion.transform.moving_average.DifferenceTransform` requires us to know the first time & value in the time\nseries, since these are discarded by the forward transform). If the transform requires an inversion state, set\n`self.inversion_state` in the `__call__()` method and use it as needed in the `_invert()` method. Note that the\ninversion state can have whatever data format is most suitable for your purposes.\n\nCheck the implementations of [`DifferenceTransform`](merlion/transform/moving_average.py#L139),\n[`PowerTransform`](merlion/transform/normalize.py#L123), and [`LowerUpperClip`](merlion/transform/bound.py)\nfor a few examples. Add your transforms to whatever module you feel is most appropriate, or create a new file if it\ndoesn't fit in with any of the existing ones. However, if you do add a new file, make sure to add it to the API docs\nby updating the autosummary block of [`merlion/transform/__init__.py`](merlion/transform/__init__.py), and adding the\nnew module to the Sphinx ReST file [`docs/source/merlion.transform.rst`](docs/source/merlion.transform.rst). \n\n## Datasets\nYou can add support for a new dataset of time series by implementing an appropriate data loading class in\n[`ts_datasets`](ts_datasets), and uploading the raw data (potentially compressed) to the [`data`](data) directory.\nIf your dataset has labeled anomalies, it belongs in [`ts_datasets.anomaly`](ts_datasets/ts_datasets/anomaly). If it\ndoes not have labeled anomalies, it belongs in [`ts_datasets.forecast`](ts_datasets/ts_datasets/forecast). See the\n[API docs](https://opensource.salesforce.com/Merlion/ts_datasets.html) for more details.\n\nOnce you've implemented your data loader class, add it to the top-level namespace of the module\n([`ts_datasets/ts_datasets/anomaly/__init__.py`](ts_datasets/ts_datasets/anomaly/__init__.py) or\n[`ts_datasets/ts_datasets/forecast/__init__.py`](ts_datasets/ts_datasets/forecast/__init__.py)) by importing it\nand adding it to `__all__`. \n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 1.447265625,
          "content": "Copyright (c) 2021, Salesforce.com, Inc.\nAll rights reserved.\n\nRedistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:\n\n* Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.\n\n* Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution.\n\n* Neither the name of Salesforce.com nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n"
        },
        {
          "name": "MANIFEST.in",
          "type": "blob",
          "size": 0.3125,
          "content": "include AUTHORS.md CODE_OF_CONDUCT.md LICENSE SECURITY.md\nglobal-exclude *.py[cod]\nexclude benchmark*.py\nrecursive-exclude conf *\nrecursive-exclude data *\nrecursive-exclude docs *\nrecursive-exclude examples *\nrecursive-exclude figures *\nrecursive-exclude tests *\nrecursive-exclude ts_datasets *\nrecursive-exclude venv *\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 16.3896484375,
          "content": "<div align=\"center\">\n<img alt=\"Logo\" src=\"https://github.com/salesforce/Merlion/raw/main/merlion_logo.svg\" width=\"80%\"/>\n</div>\n\n<div align=\"center\">\n  <a href=\"https://github.com/salesforce/Merlion/actions\">\n  <img alt=\"Tests\" src=\"https://github.com/salesforce/Merlion/actions/workflows/tests.yml/badge.svg?branch=main\"/>\n  </a>\n  <a href=\"https://github.com/salesforce/Merlion/actions\">\n  <img alt=\"Coverage\" src=\"https://github.com/salesforce/Merlion/raw/badges/coverage.svg\"/>\n  </a>\n  <a href=\"https://pypi.python.org/pypi/salesforce-merlion\">\n  <img alt=\"PyPI Version\" src=\"https://img.shields.io/pypi/v/salesforce-merlion.svg\"/>\n  </a>\n  <a href=\"https://opensource.salesforce.com/Merlion/index.html\">\n  <img alt=\"docs\" src=\"https://github.com/salesforce/Merlion/actions/workflows/docs.yml/badge.svg\"/>\n  </a>\n</div>\n\n# Merlion: A Machine Learning Library for Time Series\n\n## Table of Contents\n1. [Introduction](#introduction)\n1. [Comparison with Related Libraries](#comparison-with-related-libraries)\n1. [Installation](#installation)\n1. [Documentation](#documentation)\n1. [Getting Started](#getting-started)\n    1. [Anomaly Detection](#anomaly-detection)\n    1. [Forecasting](#forecasting)\n1. [Evaluation and Benchmarking](#evaluation-and-benchmarking)\n1. [Technical Report and Citing Merlion](#technical-report-and-citing-merlion)\n\n## Introduction\nMerlion is a Python library for time series intelligence. It provides an end-to-end machine learning framework that\nincludes loading and transforming data, building and training models, post-processing model outputs, and evaluating\nmodel performance. It supports various time series learning tasks, including forecasting, anomaly detection,\nand change point detection for both univariate and multivariate time series. This library aims to provide engineers and\nresearchers a one-stop solution to rapidly develop models for their specific time series needs, and benchmark them\nacross multiple time series datasets.\n\nMerlion's key features are\n-  Standardized and easily extensible data loading & benchmarking for a wide range of forecasting and anomaly\n   detection datasets. This includes transparent support for custom datasets.\n-  A library of diverse models for anomaly detection, forecasting, and change point detection, all\n   unified under a shared interface. Models include classic statistical methods, tree ensembles, and deep\n   learning approaches. Advanced users may fully configure each model as desired.\n-  Abstract `DefaultDetector` and `DefaultForecaster` models that are efficient, robustly achieve good performance,\n   and provide a starting point for new users.\n-  AutoML for automated hyperaparameter tuning and model selection.\n-  Unified API for using a wide range of models to forecast with\n   [exogenous regressors](https://opensource.salesforce.com/Merlion/tutorials/forecast/3_ForecastExogenous.html).\n-  Practical, industry-inspired post-processing rules for anomaly detectors that make anomaly scores more interpretable,\n   while also reducing the number of false positives.\n-  Easy-to-use ensembles that combine the outputs of multiple models to achieve more robust performance. \n-  Flexible evaluation pipelines that simulate the live deployment & re-training of a model in production,\n   and evaluate performance on both forecasting and anomaly detection.\n-  Native support for visualizing model predictions, including with a clickable visual UI.\n-  Distributed computation [backend](https://opensource.salesforce.com/Merlion/merlion.spark.html) using PySpark,\n   which can be used to serve time series applications at industrial scale.\n\n\n## Comparison with Related Libraries\n\nThe table below provides a visual overview of how Merlion's key features compare to other libraries for time series\nanomaly detection and/or forecasting.\n\n|                     | Merlion | Prophet | Alibi Detect | Kats | darts | statsmodels | nixtla | GluonTS | RRCF | STUMPY | Greykite |pmdarima \n:---                  | :---:     | :---:|  :---:  | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :----: | :---:\n| Univariate Forecasting | ✅ | ✅| | ✅ | ✅ | ✅ | ✅ | ✅ | | |✅| ✅ \n| Multivariate Forecasting | ✅ | | | ✅ | ✅ | ✅| ✅ | ✅ | | | | |\n| Univariate Anomaly Detection | ✅ | ✅ | ✅ | ✅ | ✅ | | | | ✅ | ✅ | ✅ | ✅ | \n| Multivariate Anomaly Detection | ✅ | | ✅ | ✅ | ✅ | | | | ✅ | ✅ | | | |\n| Pre Processing | ✅ | | ✅ | ✅ | ✅ | | ✅ | ✅ | | | ✅ | ✅\n| Post Processing | ✅ | | ✅ | | | | | | | | | |\n| AutoML | ✅ | | | ✅ | | | | | | | | ✅ | | ✅ \n| Ensembles | ✅ | | | ✅ | ✅ | | | | | ✅ | | | | \n| Benchmarking | ✅ | | | | ✅ | ✅ | ✅ | | | | ✅ | \n| Visualization | ✅ | ✅ | | ✅ | ✅ | | | | | | ✅ | \n\nThe following features are new in Merlion 2.0:\n\n|                     | Merlion | Prophet | Alibi Detect | Kats | darts | statsmodels | nixtla | GluonTS | RRCF | STUMPY | Greykite |pmdarima \n:---                  | :---:     | :---:|  :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :----: | :---:\n| Exogenous Regressors   | ✅ | ✅ | | |✅ | ✅ |  | | | | ✅ | ✅\n| Change Point Detection | ✅ | ✅ | ✅ | ✅ | | | | | | | ✅ |\n| Clickable Visual UI    | ✅ | | | | | | | | | | |\n| Distributed Backend    | ✅ | | | | | | ✅ | | | | | \n\n## Installation\n\nMerlion consists of two sub-repos: `merlion` implements the library's core time series intelligence features,\nand `ts_datasets` provides standardized data loaders for multiple time series datasets. These loaders load\ntime series as ``pandas.DataFrame`` s with accompanying metadata.\n\nYou can install `merlion` from PyPI by calling ``pip install salesforce-merlion``. You may install from source by\ncloning this repoand calling ``pip install Merlion/``, or ``pip install -e Merlion/`` to install in editable mode.\nYou may install additional dependencies via ``pip install salesforce-merlion[all]``,  or by calling\n``pip install \"Merlion/[all]\"`` if installing from source. \nIndividually, the optional dependencies include ``dashboard`` for a GUI dashboard,\n``spark`` for a distributed computation backend with PySpark, and ``deep-learning`` for all deep learning models.\n\nTo install the data loading package `ts_datasets`, clone this repo and call ``pip install -e Merlion/ts_datasets/``.\nThis package must be installed in editable mode (i.e. with the ``-e`` flag) if you don't want to manually specify the\nroot directory of every dataset when initializing its data loader.\n\nNote the following external dependencies:\n\n1. Some of our forecasting models depend on OpenMP. If using ``conda``, please ``conda install -c conda-forge lightgbm``\n   before installing our package. This will ensure that OpenMP is configured to work with the ``lightgbm`` package\n   (one of our dependencies) in your ``conda`` environment. If using Mac, please install [Homebrew](https://brew.sh/)\n   and call ``brew install libomp`` so that the OpenMP libary is available for the model.\n\n2. Some of our anomaly detection models depend on the Java Development Kit (JDK). For Ubuntu, call\n   ``sudo apt-get install openjdk-11-jdk``. For Mac OS, install [Homebrew](<https://brew.sh/>) and call\n   ``brew tap adoptopenjdk/openjdk && brew install --cask adoptopenjdk11``. Also ensure that ``java`` can be found\n   on your ``PATH``, and that the ``JAVA_HOME`` environment variable is set.\n\n## Documentation\n\nFor example code and an introduction to Merlion, see the Jupyter notebooks in\n[`examples`](https://github.com/salesforce/Merlion/tree/main/examples), and the guided walkthrough\n[here](https://opensource.salesforce.com/Merlion/tutorials.html). You may find detailed API documentation (including the\nexample code) [here](https://opensource.salesforce.com/Merlion/index.html). The\n[technical report](https://arxiv.org/abs/2109.09265) outlines Merlion's overall architecture\nand presents experimental results on time series anomaly detection & forecasting for both univariate and multivariate\ntime series.\n\n## Getting Started\nThe easiest way to get started is to use the GUI web-based\n[dashboard](https://opensource.salesforce.com/Merlion/merlion.dashboard.html).\nThis dashboard provides a great way to quickly experiment with many models on your own custom datasets.\nTo use it, install Merlion with the optional ``dashboard`` dependency (i.e.\n``pip install salesforce-merlion[dashboard]``), and call ``python -m merlion.dashboard`` from the command line.\nYou can view the dashboard at http://localhost:8050.\nBelow, we show some screenshots of the dashboard for both anomaly detection and forecasting.\n\n![anomaly dashboard](https://github.com/salesforce/Merlion/raw/main/figures/dashboard_anomaly.png)\n\n![forecast dashboard](https://github.com/salesforce/Merlion/raw/main/figures/dashboard_forecast.png)\n\nTo help you get started with using Merlion in your own code, we provide below some minimal examples using Merlion\ndefault models for both anomaly detection and forecasting.\n\n### Anomaly Detection\nHere, we show the code to replicate the results from the anomaly detection dashboard above.\nWe begin by importing Merlion’s `TimeSeries` class and the data loader for the Numenta Anomaly Benchmark `NAB`.\nWe can then divide a specific time series from this dataset into training and testing splits.\n\n```python\nfrom merlion.utils import TimeSeries\nfrom ts_datasets.anomaly import NAB\n\n# Data loader returns pandas DataFrames, which we convert to Merlion TimeSeries\ntime_series, metadata = NAB(subset=\"realKnownCause\")[3]\ntrain_data = TimeSeries.from_pd(time_series[metadata.trainval])\ntest_data = TimeSeries.from_pd(time_series[~metadata.trainval])\ntest_labels = TimeSeries.from_pd(metadata.anomaly[~metadata.trainval])\n```\n\nWe can then initialize and train Merlion’s `DefaultDetector`, which is an anomaly detection model that\nbalances performance with efficiency. We also obtain its predictions on the test split.\n\n```python\nfrom merlion.models.defaults import DefaultDetectorConfig, DefaultDetector\nmodel = DefaultDetector(DefaultDetectorConfig())\nmodel.train(train_data=train_data)\ntest_pred = model.get_anomaly_label(time_series=test_data)\n```\n\nNext, we visualize the model's predictions.\n\n```python\nfrom merlion.plot import plot_anoms\nimport matplotlib.pyplot as plt\nfig, ax = model.plot_anomaly(time_series=test_data)\nplot_anoms(ax=ax, anomaly_labels=test_labels)\nplt.show()\n```\n![anomaly figure](https://github.com/salesforce/Merlion/raw/main/figures/anom_example.png)\n\nFinally, we can quantitatively evaluate the model. The precision and recall come from the fact that the model\nfired 3 alarms, with 2 true positives, 1 false negative, and 1 false positive. We also evaluate the mean time\nthe model took to detect each anomaly that it correctly detected.\n\n```python\nfrom merlion.evaluate.anomaly import TSADMetric\np = TSADMetric.Precision.value(ground_truth=test_labels, predict=test_pred)\nr = TSADMetric.Recall.value(ground_truth=test_labels, predict=test_pred)\nf1 = TSADMetric.F1.value(ground_truth=test_labels, predict=test_pred)\nmttd = TSADMetric.MeanTimeToDetect.value(ground_truth=test_labels, predict=test_pred)\nprint(f\"Precision: {p:.4f}, Recall: {r:.4f}, F1: {f1:.4f}\\n\"\n      f\"Mean Time To Detect: {mttd}\")\n```\n```\nPrecision: 0.6667, Recall: 0.6667, F1: 0.6667\nMean Time To Detect: 1 days 10:22:30\n```\n### Forecasting\nHere, we show the code to replicate the results from the forecasting dashboard above.\nWe begin by importing Merlion’s `TimeSeries` class and the data loader for the `M4` dataset. We can then divide a\nspecific time series from this dataset into training and testing splits.\n\n```python\nfrom merlion.utils import TimeSeries\nfrom ts_datasets.forecast import M4\n\n# Data loader returns pandas DataFrames, which we convert to Merlion TimeSeries\ntime_series, metadata = M4(subset=\"Hourly\")[0]\ntrain_data = TimeSeries.from_pd(time_series[metadata.trainval])\ntest_data = TimeSeries.from_pd(time_series[~metadata.trainval])\n```\n\nWe can then initialize and train Merlion’s `DefaultForecaster`, which is an forecasting model that balances\nperformance with efficiency. We also obtain its predictions on the test split.\n\n```python\nfrom merlion.models.defaults import DefaultForecasterConfig, DefaultForecaster\nmodel = DefaultForecaster(DefaultForecasterConfig())\nmodel.train(train_data=train_data)\ntest_pred, test_err = model.forecast(time_stamps=test_data.time_stamps)\n```\n\nNext, we visualize the model’s predictions.\n\n```python\nimport matplotlib.pyplot as plt\nfig, ax = model.plot_forecast(time_series=test_data, plot_forecast_uncertainty=True)\nplt.show()\n```\n![forecast figure](https://github.com/salesforce/Merlion/raw/main/figures/forecast_example.png)\n\nFinally, we quantitatively evaluate the model. sMAPE measures the error of the prediction on a scale of 0 to 100\n(lower is better), while MSIS evaluates the quality of the 95% confidence band on a scale of 0 to 100 (lower is better).\n\n```python\n# Evaluate the model's predictions quantitatively\nfrom scipy.stats import norm\nfrom merlion.evaluate.forecast import ForecastMetric\n\n# Compute the sMAPE of the predictions (0 to 100, smaller is better)\nsmape = ForecastMetric.sMAPE.value(ground_truth=test_data, predict=test_pred)\n\n# Compute the MSIS of the model's 95% confidence interval (0 to 100, smaller is better)\nlb = TimeSeries.from_pd(test_pred.to_pd() + norm.ppf(0.025) * test_err.to_pd().values)\nub = TimeSeries.from_pd(test_pred.to_pd() + norm.ppf(0.975) * test_err.to_pd().values)\nmsis = ForecastMetric.MSIS.value(ground_truth=test_data, predict=test_pred,\n                                 insample=train_data, lb=lb, ub=ub)\nprint(f\"sMAPE: {smape:.4f}, MSIS: {msis:.4f}\")\n```\n```\nsMAPE: 4.1944, MSIS: 18.9331\n```\n\n## Evaluation and Benchmarking\n\nOne of Merlion's key features is an evaluation pipeline that simulates the live deployment\nof a model on historical data. This enables you to compare models on the datasets relevant\nto them, under the conditions that they may encounter in a production environment. Our\nevaluation pipeline proceeds as follows:\n1. Train an initial model on recent historical training data (designated as the training split of the time series)\n1. At a regular interval (e.g. once per day), retrain the entire model on the most recent data. This can be either the\n   entire history of the time series, or a more limited window (e.g. 4 weeks).\n1. Obtain the model's predictions (anomaly scores or forecasts) for the time series values that occur between\n   re-trainings. You may customize whether this should be done in batch (predicting all values at once),\n   streaming (updating the model's internal state after each data point without fully re-training it),\n   or some intermediate cadence.\n1. Compare the model's predictions against the ground truth (labeled anomalies for anomaly detection, or the actual\n   time series values for forecasting), and report quantitative evaluation metrics.\n\nWe provide scripts that allow you to use this pipeline to evaluate arbitrary models on arbitrary datasets.\nFor example, invoking\n```shell script\npython benchmark_anomaly.py --dataset NAB_realAWSCloudwatch --model IsolationForest --retrain_freq 1d\n``` \nwill evaluate the anomaly detection performance of the `IsolationForest` (retrained once a day) on the\n\"realAWSCloudwatch\" subset of the NAB dataset.  Similarly, invoking\n```shell script\npython benchmark_forecast.py --dataset M4_Hourly --model ETS\n```\nwill evaluate the batch forecasting performance (i.e. no retraining) of `ETS` on the \"Hourly\" subset of the M4 dataset. \nYou can find the results produced by running these scripts in the Experiments section of the\n[technical report](https://arxiv.org/abs/2109.09265).\n\n## Technical Report and Citing Merlion\nYou can find more details in our technical report: https://arxiv.org/abs/2109.09265\n\nIf you're using Merlion in your research or applications, please cite using this BibTeX:\n```\n@article{bhatnagar2021merlion,\n      title={Merlion: A Machine Learning Library for Time Series},\n      author={Aadyot Bhatnagar and Paul Kassianik and Chenghao Liu and Tian Lan and Wenzhuo Yang\n              and Rowan Cassius and Doyen Sahoo and Devansh Arpit and Sri Subramanian and Gerald Woo\n              and Amrita Saha and Arun Kumar Jagota and Gokulakrishnan Gopalakrishnan and Manpreet Singh\n              and K C Krithika and Sukumar Maddineni and Daeki Cho and Bo Zong and Yingbo Zhou\n              and Caiming Xiong and Silvio Savarese and Steven Hoi and Huan Wang},\n      year={2021},\n      eprint={2109.09265},\n      archivePrefix={arXiv},\n      primaryClass={cs.LG}\n}\n```\n\n## To Dos\nWe are striving to leverage the time-series modeling with GPUs to further improve the speed and throughput of Merlion. \nStay tuned ...\n"
        },
        {
          "name": "SECURITY.md",
          "type": "blob",
          "size": 0.390625,
          "content": "## Security\n\nPlease report any security issue to [security@salesforce.com](mailto:security@salesforce.com)\nas soon as it is discovered. This library limits its runtime dependencies in\norder to reduce the total cost of ownership as much as can be, but all consumers\nshould remain vigilant and have their security stakeholders review all third-party\nproducts (3PP) like this one and their dependencies."
        },
        {
          "name": "benchmark_anomaly.py",
          "type": "blob",
          "size": 31.1533203125,
          "content": "#\n# Copyright (c) 2022 salesforce.com, inc.\n# All rights reserved.\n# SPDX-License-Identifier: BSD-3-Clause\n# For full license text, see the LICENSE file in the repo root or https://opensource.org/licenses/BSD-3-Clause\n#\nimport argparse\nimport copy\nimport json\nimport logging\nimport os\nimport sys\nimport time\nimport git\nfrom typing import Tuple\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\n\nfrom merlion.evaluate.anomaly import (\n    TSADEvaluatorConfig,\n    accumulate_tsad_score,\n    TSADScoreAccumulator as ScoreAcc,\n    TSADEvaluator,\n)\nfrom merlion.models.anomaly.base import DetectorBase\nfrom merlion.models.ensemble.anomaly import DetectorEnsemble\nfrom merlion.evaluate.anomaly import TSADMetric, ScoreType\nfrom merlion.models.factory import ModelFactory\nfrom merlion.transform.resample import TemporalResample\nfrom merlion.utils import TimeSeries\nfrom merlion.utils.resample import to_pd_datetime\n\nfrom ts_datasets.anomaly import *\n\nlogger = logging.getLogger(__name__)\n\n# Benchmark code assumes you have created data/<dirname> symlinks to\n# the root directories of all the relevant datasets\nMERLION_ROOT = os.path.dirname(os.path.abspath(__file__))\nCONFIG_JSON = os.path.join(MERLION_ROOT, \"conf\", \"benchmark_anomaly.json\")\nDATADIR = os.path.join(MERLION_ROOT, \"data\")\n\n\ndef parse_args():\n    with open(CONFIG_JSON, \"r\") as f:\n        valid_models = list(json.load(f).keys())\n\n    parser = argparse.ArgumentParser(\n        description=\"Script to benchmark Merlion time series anomaly detection \"\n        \"models. This script assumes that you have pip installed \"\n        \"both merlion (this repo's main package) and ts_datasets \"\n        \"(a sub-repo).\"\n    )\n    parser.add_argument(\n        \"--dataset\",\n        default=\"NAB_all\",\n        help=\"Name of dataset to run benchmark on. See get_dataset() \"\n        \"in ts_datasets/ts_datasets/anomaly/__init__.py for \"\n        \"valid options.\",\n    )\n    parser.add_argument(\"--data_root\", default=None, help=\"Root directory/file of dataset.\")\n    parser.add_argument(\"--data_kwargs\", default=\"{}\", help=\"JSON of keyword arguemtns for the data loader.\")\n    parser.add_argument(\n        \"--models\",\n        type=str,\n        nargs=\"+\",\n        default=[\"DefaultDetector\"],\n        help=\"Name of model (or models in ensemble) to benchmark.\",\n        choices=valid_models,\n    )\n    parser.add_argument(\n        \"--retrain_freq\",\n        type=str,\n        default=\"default\",\n        help=\"String (e.g. 1d, 2w, etc.) specifying how often \"\n        \"to re-train the model before evaluating it on \"\n        \"the next window of data. Note that re-training \"\n        \"is unsupervised, i.e. does not use ground truth \"\n        \"anomaly labels in any way. Default retrain_freq is \"\n        \"1d for univariate data and None for multivariate.\",\n    )\n    parser.add_argument(\n        \"--train_window\",\n        type=str,\n        default=None,\n        help=\"String (e.g. 30d, 6m, etc.) specifying how much \"\n        \"data (in terms of a time window) the model \"\n        \"should train on at any point.\",\n    )\n    parser.add_argument(\n        \"--metric\",\n        type=str,\n        default=\"F1\",\n        choices=list(TSADMetric.__members__.keys()),\n        help=\"Metric to optimize for (where relevant)\",\n    )\n    parser.add_argument(\n        \"--point_adj_metric\",\n        type=str,\n        default=\"PointAdjustedF1\",\n        choices=list(TSADMetric.__members__.keys()),\n        help=\"Final metric to optimize for when evaluating point-adjusted performance\",\n    )\n    parser.add_argument(\n        \"--pointwise_metric\",\n        type=str,\n        default=\"PointwiseF1\",\n        choices=list(TSADMetric.__members__.keys()),\n        help=\"Final metric to optimize for when evaluating pointwise performance\",\n    )\n    parser.add_argument(\"--unsupervised\", action=\"store_true\")\n    parser.add_argument(\n        \"--tune_on_test\",\n        action=\"store_true\",\n        default=False,\n        help=\"Whether to tune the threshold on both train and \"\n        \"test splits of the time series. Useful for \"\n        \"metrics like Best F1, or NAB score with \"\n        \"threshold optimization.\",\n    )\n    parser.add_argument(\n        \"--load_checkpoint\",\n        action=\"store_true\",\n        default=False,\n        help=\"Specify this option if you would like continue \"\n        \"training your model on a dataset from a \"\n        \"checkpoint, instead of restarting from scratch.\",\n    )\n    parser.add_argument(\n        \"--eval_only\",\n        action=\"store_true\",\n        default=False,\n        help=\"Specify this option if you would like to skip \"\n        \"the model training phase, and simply evaluate \"\n        \"on partial saved results.\",\n    )\n    parser.add_argument(\"--debug\", action=\"store_true\", default=False, help=\"Whether to enable INFO-level logs.\")\n    parser.add_argument(\n        \"--visualize\",\n        action=\"store_true\",\n        default=False,\n        help=\"Whether to plot the model's predictions after \"\n        \"training on each example. Mutually exclusive \"\n        \"with running any sort of evaluation.\",\n    )\n    args = parser.parse_args()\n    args.metric = TSADMetric[args.metric]\n    args.pointwise_metric = TSADMetric[args.pointwise_metric]\n    args.visualize = args.visualize and not args.eval_only\n    args.data_kwargs = json.loads(args.data_kwargs)\n    assert isinstance(args.data_kwargs, dict)\n    if args.retrain_freq.lower() in [\"\", \"none\", \"null\"]:\n        args.retrain_freq = None\n    elif args.retrain_freq != \"default\":\n        rf = pd.to_timedelta(args.retrain_freq).total_seconds()\n        if rf % (3600 * 24) == 0:\n            args.retrain_freq = f\"{int(rf/3600/24)}d\"\n        elif rf % 3600 == 0:\n            args.retrain_freq = f\"{int(rf/3600)}h\"\n        elif rf % 60 == 0:\n            args.retrain_freq = f\"{int(rf//60)}min\"\n        else:\n            args.retrain_freq = f\"{int(rf)}s\"\n\n    return args\n\n\ndef get_dataset_name(dataset: TSADBaseDataset):\n    name = type(dataset).__name__\n    if hasattr(dataset, \"subset\") and dataset.subset is not None:\n        name += \"_\" + dataset.subset\n    if isinstance(dataset, CustomAnomalyDataset):\n        root = dataset.rootdir\n        name = os.path.join(name, os.path.basename(os.path.dirname(root) if os.path.isfile(root) else root))\n    return name\n\n\ndef dataset_to_threshold(dataset: TSADBaseDataset, tune_on_test=False):\n    if isinstance(dataset, IOpsCompetition):\n        return 2.25\n    elif isinstance(dataset, NAB):\n        return 3.5\n    elif isinstance(dataset, Synthetic):\n        return 2\n    elif isinstance(dataset, MSL):\n        return 3.0\n    elif isinstance(dataset, SMAP):\n        return 3.5\n    elif isinstance(dataset, SMD):\n        return 3 if not tune_on_test else 2.5\n    elif hasattr(dataset, \"default_threshold\"):\n        return dataset.default_threshold\n    return 3\n\n\ndef resolve_model_name(model_name: str):\n    with open(CONFIG_JSON, \"r\") as f:\n        config_dict = json.load(f)\n\n    if model_name not in config_dict:\n        raise NotImplementedError(\n            f\"Benchmarking not implemented for model {model_name}. Valid model names are {list(config_dict.keys())}\"\n        )\n\n    while \"alias\" in config_dict[model_name]:\n        assert model_name != config_dict[model_name][\"alias\"], \"Alias name cannot be the same as the model name\"\n        model_name = config_dict[model_name][\"alias\"]\n\n    return model_name\n\n\ndef get_model(\n    model_name: str, dataset: TSADBaseDataset, metric: TSADMetric, tune_on_test=False, unsupervised=False\n) -> Tuple[DetectorBase, dict]:\n    with open(CONFIG_JSON, \"r\") as f:\n        config_dict = json.load(f)\n\n    if model_name not in config_dict:\n        raise NotImplementedError(\n            f\"Benchmarking not implemented for model {model_name}. Valid model names are {list(config_dict.keys())}\"\n        )\n\n    while \"alias\" in config_dict[model_name]:\n        model_name = config_dict[model_name][\"alias\"]\n\n    # Load the model with default kwargs, but override with dataset-specific\n    # kwargs where relevant\n    model_configs = config_dict[model_name][\"config\"]\n    model_type = config_dict[model_name].get(\"model_type\", model_name)\n    model_kwargs = model_configs[\"default\"]\n    model_kwargs.update(model_configs.get(type(dataset).__name__, {}))\n    model = ModelFactory.create(name=model_type, **model_kwargs)\n\n    # The post-rule train configs are fully specified for each dataset (where\n    # relevant), with a default option if there is no dataset-specific option.\n    post_rule_train_configs = config_dict[model_name].get(\"post_rule_train_config\", {})\n    d = post_rule_train_configs.get(\"default\", {})\n    d.update(post_rule_train_configs.get(type(dataset).__name__, {}))\n    if len(d) == 0:\n        d = copy.copy(model._default_post_rule_train_config)\n    d[\"metric\"] = None if unsupervised else metric\n    d.update({\"max_early_sec\": dataset.max_lead_sec, \"max_delay_sec\": dataset.max_lag_sec})\n\n    t = dataset_to_threshold(dataset, tune_on_test)\n    model.threshold.alm_threshold = t\n    d[\"unsup_quantile\"] = None\n    return model, d\n\n\ndef df_to_merlion(df: pd.DataFrame, md: pd.DataFrame, get_ground_truth=False, transform=None) -> TimeSeries:\n    \"\"\"Converts a pandas dataframe time series to the Merlion format.\"\"\"\n    if get_ground_truth:\n        if False and \"changepoint\" in md.keys():\n            series = md[\"anomaly\"] | md[\"changepoint\"]\n        else:\n            series = md[\"anomaly\"]\n    else:\n        series = df\n    time_series = TimeSeries.from_pd(series)\n    if transform is not None:\n        time_series = transform(time_series)\n    return time_series\n\n\ndef train_model(\n    model_name,\n    metric,\n    dataset,\n    retrain_freq=None,\n    train_window=None,\n    load_checkpoint=False,\n    visualize=False,\n    debug=False,\n    unsupervised=False,\n    tune_on_test=False,\n):\n    \"\"\"Trains a model on the time series dataset given, and save their predictions to a dataset.\"\"\"\n    resampler = None\n    if isinstance(dataset, IOpsCompetition):\n        resampler = TemporalResample(\"5min\")\n\n    model_name = resolve_model_name(model_name)\n    dataset_name = get_dataset_name(dataset)\n    model_dir = model_name if retrain_freq is None else f\"{model_name}_{retrain_freq}\"\n    dirname = os.path.join(\"results\", \"anomaly\", model_dir)\n    csv = os.path.join(dirname, f\"pred_{dataset_name}.csv.gz\")\n    config_fname = os.path.join(dirname, f\"{dataset_name}_config.json\")\n    checkpoint = os.path.join(dirname, f\"ckpt_{dataset_name}.txt\")\n\n    # Determine where to start within the dataset if there is a checkpoint\n    i0 = 0\n    if os.path.isfile(checkpoint) and os.path.isfile(csv) and load_checkpoint:\n        with open(checkpoint, \"r\") as f:\n            i0 = int(f.read().rstrip(\"\\n\"))\n\n        # Validate & sanitize the existing CSV checkpoint\n        df = pd.read_csv(csv, dtype={\"trainval\": bool, \"idx\": int})\n        df = df[df[\"idx\"] < i0]\n        if set(df[\"idx\"]) == set(range(i0)):\n            df.to_csv(csv, index=False)\n        else:\n            i0 = 0\n\n    model = None\n    for i, (df, md) in enumerate(tqdm(dataset)):\n        if i < i0:\n            continue\n\n        # Reload model & get the train / test split for this time series\n        model, post_rule_train_config = get_model(\n            model_name=model_name, dataset=dataset, metric=metric, tune_on_test=tune_on_test, unsupervised=unsupervised\n        )\n        delay = post_rule_train_config[\"max_delay_sec\"]\n        train_vals = df_to_merlion(df[md.trainval], md[md.trainval], get_ground_truth=False, transform=resampler)\n        test_vals = df_to_merlion(df[~md.trainval], md[~md.trainval], get_ground_truth=False, transform=resampler)\n        train_anom = df_to_merlion(df[md.trainval], md[md.trainval], get_ground_truth=True)\n        test_anom = df_to_merlion(df[~md.trainval], md[~md.trainval], get_ground_truth=True)\n\n        # Set up an evaluator & get predictions\n        evaluator = TSADEvaluator(\n            model=model,\n            config=TSADEvaluatorConfig(\n                train_window=train_window,\n                retrain_freq=retrain_freq,\n                max_delay_sec=delay,\n                max_early_sec=getattr(model.threshold, \"suppress_secs\", delay),\n            ),\n        )\n        train_scores, test_scores = evaluator.get_predict(\n            train_vals=train_vals,\n            test_vals=test_vals,\n            post_process=False,\n            train_kwargs={\"anomaly_labels\": train_anom, \"post_rule_train_config\": post_rule_train_config},\n        )\n\n        # Write the model's predictions to the csv file, starting a new one\n        # if we aren't loading an existing checkpoint. Scores from all time\n        # series in the dataset are combined together in a single csv. Each\n        # line in the csv corresponds to a point in a time series, and contains\n        # the timestamp, raw anomaly score, and index of the time series.\n        if not visualize:\n            if i == i0 == 0:\n                os.makedirs(os.path.dirname(csv), exist_ok=True)\n                os.makedirs(os.path.dirname(checkpoint), exist_ok=True)\n                df = pd.DataFrame({\"timestamp\": [], \"y\": [], \"trainval\": [], \"idx\": []})\n                df.to_csv(csv, index=False)\n\n            df = pd.read_csv(csv)\n            ts_df = pd.concat((train_scores.to_pd(), test_scores.to_pd()))\n            ts_df.columns = [\"y\"]\n            ts_df.loc[:, \"timestamp\"] = ts_df.index.view(int) // 1e9\n            ts_df.loc[:, \"trainval\"] = [j < len(train_scores) for j in range(len(ts_df))]\n            ts_df.loc[:, \"idx\"] = i\n            df = pd.concat((df, ts_df), ignore_index=True)\n            df.to_csv(csv, index=False)\n\n            # Start from time series i+1 if loading a checkpoint.\n            with open(checkpoint, \"w\") as f:\n                f.write(str(i + 1))\n\n        if visualize or debug:\n            # Train the post-rule on the appropriate labels\n            score = test_scores if tune_on_test else train_scores\n            label = test_anom if tune_on_test else train_anom\n            model.train_post_process(\n                train_result=score, anomaly_labels=label, post_rule_train_config=post_rule_train_config\n            )\n\n            # Log (many) evaluation metrics for the time series\n            score_acc = evaluator.evaluate(ground_truth=test_anom, predict=model.threshold(test_scores))\n            mttd = score_acc.mean_time_to_detect()\n            if mttd < pd.to_timedelta(0):\n                mttd = f\"-{-mttd}\"\n            logger.info(f\"\\nPerformance on time series {i+1}/{len(dataset)}\")\n            logger.info(\"Revised Point-Adjusted Metrics\")\n            logger.info(f\"F1 Score:  {score_acc.f1(score_type=ScoreType.RevisedPointAdjusted):.4f}\")\n            logger.info(f\"Precision: {score_acc.precision(score_type=ScoreType.RevisedPointAdjusted):.4f}\")\n            logger.info(f\"Recall:    {score_acc.recall(score_type=ScoreType.RevisedPointAdjusted):.4f}\\n\")\n            logger.info(f\"Mean Time To Detect Anomalies:  {mttd}\")\n            logger.info(f\"Mean Detected Anomaly Duration: {score_acc.mean_detected_anomaly_duration()}\")\n            logger.info(f\"Mean Anomaly Duration:          {score_acc.mean_anomaly_duration()}\\n\")\n\n            if debug:\n                logger.info(f\"Pointwise metrics\")\n                logger.info(f\"F1 Score:  {score_acc.f1(score_type=ScoreType.Pointwise):.4f}\")\n                logger.info(f\"Precision: {score_acc.precision(score_type=ScoreType.Pointwise):.4f}\")\n                logger.info(f\"Recall:    {score_acc.recall(score_type=ScoreType.Pointwise):.4f}\\n\")\n\n                logger.info(\"Point-Adjusted Metrics\")\n                logger.info(f\"F1 Score:  {score_acc.f1(score_type=ScoreType.PointAdjusted):.4f}\")\n                logger.info(f\"Precision: {score_acc.precision(score_type=ScoreType.PointAdjusted):.4f}\")\n                logger.info(f\"Recall:    {score_acc.recall(score_type=ScoreType.PointAdjusted):.4f}\\n\")\n\n                logger.info(f\"NAB Scores\")\n                logger.info(f\"NAB score (balanced): {score_acc.nab_score():.4f}\")\n                logger.info(f\"NAB score (low FP):   {score_acc.nab_score(fp_weight=0.22):.4f}\")\n                logger.info(f\"NAB score (low FN):   {score_acc.nab_score(fn_weight=2.0):.4f}\\n\")\n\n            if visualize:\n                # Make a plot\n                alarms = model.threshold(test_scores)\n                fig = model.get_figure(time_series=test_vals, time_series_prev=train_vals, plot_time_series_prev=True)\n                fig.anom = alarms.univariates[alarms.names[0]]\n                fig, ax = fig.plot(figsize=(1800, 600))\n\n                # Overlay windows indicating the true anomalies\n                all_anom = train_anom + test_anom\n                t, y = zip(*all_anom)\n                y = np.asarray(y).flatten()\n                splits = np.where(y[1:] != y[:-1])[0] + 1\n                splits = np.concatenate(([0], splits, [len(y) - 1]))\n                anom_windows = [(splits[k], splits[k + 1]) for k in range(len(splits) - 1) if y[splits[k]]]\n                for i_0, i_f in anom_windows:\n                    t_0 = to_pd_datetime(t[i_0])\n                    t_f = to_pd_datetime(t[i_f])\n                    ax.axvspan(t_0, t_f, color=\"#d07070\", zorder=-1, alpha=0.5)\n                time.sleep(2)\n                plt.show()\n\n    # Save full experimental config\n    if model is not None and not visualize:\n        full_config = dict(\n            model_config=model.config.to_dict(),\n            evaluator_config=evaluator.config.to_dict(),\n            code_version_info=get_code_version_info(),\n        )\n        os.makedirs(os.path.dirname(config_fname), exist_ok=True)\n        with open(config_fname, \"w\") as f:\n            json.dump(full_config, f, indent=2, sort_keys=True)\n\n\ndef get_code_version_info():\n    return dict(time=str(pd.Timestamp.now()), commit=git.Repo(search_parent_directories=True).head.object.hexsha)\n\n\ndef read_model_predictions(dataset: TSADBaseDataset, model_dir: str):\n    \"\"\"\n    Returns a list of lists all_preds, where all_preds[i] is the model's raw\n    anomaly scores for time series i in the dataset.\n    \"\"\"\n    csv = os.path.join(\"results\", \"anomaly\", model_dir, f\"pred_{get_dataset_name(dataset)}.csv.gz\")\n    preds = pd.read_csv(csv, dtype={\"trainval\": bool, \"idx\": int})\n    preds[\"timestamp\"] = to_pd_datetime(preds[\"timestamp\"])\n    return [preds[preds[\"idx\"] == i].set_index(\"timestamp\") for i in sorted(preds[\"idx\"].unique())]\n\n\ndef evaluate_predictions(\n    model_names,\n    dataset,\n    all_model_preds,\n    metric: TSADMetric,\n    pointwise_metric: TSADMetric,\n    point_adj_metric: TSADMetric,\n    tune_on_test=False,\n    unsupervised=False,\n    debug=False,\n):\n\n    scores_rpa, scores_pw, scores_pa = [], [], []\n    use_ucr_eval = isinstance(dataset, UCR) and (unsupervised or not tune_on_test)\n    resampler = None\n    if isinstance(dataset, IOpsCompetition):\n        resampler = TemporalResample(\"5min\")\n\n    for i, (true, md) in enumerate(tqdm(dataset)):\n        # Get time series for the train & test splits of the ground truth\n        idx = ~md.trainval if tune_on_test else md.trainval\n        true_train = df_to_merlion(true[idx], md[idx], get_ground_truth=True)\n        true_test = df_to_merlion(true[~md.trainval], md[~md.trainval], get_ground_truth=True)\n\n        for acc_id, (simple_threshold, opt_metric, scores) in enumerate(\n            [\n                (use_ucr_eval and not tune_on_test, metric, scores_rpa),\n                (True, pointwise_metric, scores_pw),\n                (True, point_adj_metric, scores_pa),\n            ]\n        ):\n            if acc_id > 0 and use_ucr_eval:\n                scores_pw = scores_rpa\n                scores_pa = scores_rpa\n                continue\n            # For each model, load its raw anomaly scores for the i'th time series\n            # as a UnivariateTimeSeries, and collect all the models' scores as a\n            # TimeSeries. Do this for both the train and test splits.\n            if i >= min(len(p) for p in all_model_preds):\n                break\n            pred = [model_preds[i] for model_preds in all_model_preds]\n            pred_train = [p[~p[\"trainval\"]] if tune_on_test else p[p[\"trainval\"]] for p in pred]\n            pred_train = [TimeSeries.from_pd(p[\"y\"]) for p in pred_train]\n            pred_test = [p[~p[\"trainval\"]] for p in pred]\n            pred_test = [TimeSeries.from_pd(p[\"y\"]) for p in pred_test]\n\n            # Train each model's post rule on the train split\n            models = []\n            for name, train, og_pred in zip(model_names, pred_train, pred):\n                m, prtc = get_model(\n                    model_name=name,\n                    dataset=dataset,\n                    metric=opt_metric,\n                    tune_on_test=tune_on_test,\n                    unsupervised=unsupervised,\n                )\n                m.config.enable_threshold = len(model_names) == 1\n                if simple_threshold:\n                    m.threshold = m.threshold.to_simple_threshold()\n                if tune_on_test and not unsupervised:\n                    m.calibrator.train(TimeSeries.from_pd(og_pred[\"y\"][og_pred[\"trainval\"]]))\n                m.train_post_process(train_result=train, anomaly_labels=true_train, post_rule_train_config=prtc)\n                models.append(m)\n\n            # Get the lead & lag time for the dataset\n            early, delay = dataset.max_lead_sec, dataset.max_lag_sec\n            if early is None:\n                leads = [getattr(m.threshold, \"suppress_secs\", delay) for m in models]\n                leads = [dt for dt in leads if dt is not None]\n                early = None if len(leads) == 0 else max(leads)\n\n            # No further training if we only have 1 model\n            if len(models) == 1:\n                model = models[0]\n                pred_test_raw = pred_test[0]\n\n            # If we have multiple models, train an ensemble model\n            else:\n                threshold = dataset_to_threshold(dataset, tune_on_test)\n                ensemble_threshold_train_config = dict(\n                    metric=opt_metric if tune_on_test else None,\n                    max_early_sec=early,\n                    max_delay_sec=delay,\n                    unsup_quantile=None,\n                )\n\n                # Train the ensemble and its post-rule on the current time series\n                model = DetectorEnsemble(models=models)\n                use_m = [len(p) > 1 for p in zip(models, pred_train)]\n                pred_train = [m.post_rule(p) for m, p, use in zip(models, pred_train, use_m) if use]\n                pred_test = [m.post_rule(p) for m, p, use in zip(models, pred_test, use_m) if use]\n                pred_train = model.train_combiner(pred_train, true_train)\n                if simple_threshold:\n                    model.threshold = model.threshold.to_simple_threshold()\n                model.threshold.alm_threshold = threshold\n                model.train_post_process(\n                    train_result=pred_train,\n                    anomaly_labels=true_train,\n                    post_rule_train_config=ensemble_threshold_train_config,\n                )\n                pred_test_raw = model.combiner(pred_test, true_test)\n\n            # For UCR dataset, the evaluation just checks whether the point with the highest\n            # anomaly score is anomalous or not.\n            if acc_id == 0 and use_ucr_eval and not unsupervised:\n                df = pred_test_raw.to_pd()\n                df[np.abs(df) < df.max()] = 0\n                pred_test = TimeSeries.from_pd(df)\n            else:\n                pred_test = model.post_rule(pred_test_raw)\n\n            # Compute the individual components comprising various scores.\n            score = accumulate_tsad_score(true_test, pred_test, max_early_sec=early, max_delay_sec=delay)\n\n            # Make sure all time series have exactly one detection for UCR dataset (either 1 TP, or 1 FN & 1 FP).\n            if acc_id == 0 and use_ucr_eval:\n                n_anom = score.num_tp_anom + score.num_fn_anom\n                if n_anom == 0:\n                    score.num_tp_anom, score.num_fn_anom, score.num_fp = 0, 0, 0\n                elif score.num_tp_anom > 0:\n                    score.num_tp_anom, score.num_fn_anom, score.num_fp = 1, 0, 0\n                else:\n                    score.num_tp_anom, score.num_fn_anom, score.num_fp = 0, 1, 1\n            scores.append(score)\n\n    # Aggregate statistics from full dataset\n    score_rpa = sum(scores_rpa, ScoreAcc())\n    score_pw = sum(scores_pw, ScoreAcc())\n    score_pa = sum(scores_pa, ScoreAcc())\n\n    # Determine if it's better to have all negatives for each time series if\n    # using the test data in a supervised way.\n    if tune_on_test and not unsupervised:\n        # Convert true positives to false negatives, and remove all false positives.\n        # Keep the updated version if it improves F1 score.\n        for s in sorted(scores_rpa, key=lambda x: x.num_fp, reverse=True):\n            stype = ScoreType.RevisedPointAdjusted\n            sprime = copy.deepcopy(score_rpa)\n            sprime.num_tp_anom -= s.num_tp_anom\n            sprime.num_fn_anom += s.num_tp_anom\n            sprime.num_fp -= s.num_fp\n            sprime.tp_score -= s.tp_score\n            sprime.fp_score -= s.fp_score\n            if score_rpa.f1(stype) < sprime.f1(stype):\n                # Update anomaly durations\n                for duration, delay in zip(s.tp_anom_durations, s.tp_detection_delays):\n                    sprime.tp_anom_durations.remove(duration)\n                    sprime.tp_detection_delays.remove(delay)\n                score_rpa = sprime\n\n        # Repeat for pointwise scores\n        for s in sorted(scores_pw, key=lambda x: x.num_fp, reverse=True):\n            stype = ScoreType.Pointwise\n            sprime = copy.deepcopy(score_pw)\n            sprime.num_tp_pointwise -= s.num_tp_pointwise\n            sprime.num_fn_pointwise += s.num_tp_pointwise\n            sprime.num_fp -= s.num_fp\n            if score_pw.f1(stype) < sprime.f1(stype):\n                score_pw = sprime\n\n        # Repeat for point-adjusted scores\n        for s in sorted(scores_pa, key=lambda x: x.num_fp, reverse=True):\n            stype = ScoreType.PointAdjusted\n            sprime = copy.deepcopy(score_pa)\n            sprime.num_tp_point_adj -= s.num_tp_point_adj\n            sprime.num_fn_point_adj += s.num_tp_point_adj\n            sprime.num_fp -= s.num_fp\n            if score_pa.f1(stype) < sprime.f1(stype):\n                score_pa = sprime\n\n    # Compute MTTD & report F1, precision, and recall\n    mttd = score_rpa.mean_time_to_detect()\n    if mttd < pd.to_timedelta(0):\n        mttd = f\"-{-mttd}\"\n    print()\n    print(\"Revised point-adjusted metrics\")\n    print(f\"F1 score:  {score_rpa.f1(ScoreType.RevisedPointAdjusted):.4f}\")\n    print(f\"Precision: {score_rpa.precision(ScoreType.RevisedPointAdjusted):.4f}\")\n    print(f\"Recall:    {score_rpa.recall(ScoreType.RevisedPointAdjusted):.4f}\")\n    print()\n    print(f\"Mean Time To Detect Anomalies:  {mttd}\")\n    print(f\"Mean Detected Anomaly Duration: {score_rpa.mean_detected_anomaly_duration()}\")\n    print(f\"Mean Anomaly Duration:          {score_rpa.mean_anomaly_duration()}\")\n    print()\n    if debug:\n        print(\"Pointwise metrics\")\n        print(f\"F1 score:  {score_pw.f1(ScoreType.Pointwise):.4f}\")\n        print(f\"Precision: {score_pw.precision(ScoreType.Pointwise):.4f}\")\n        print(f\"Recall:    {score_pw.recall(ScoreType.Pointwise):.4f}\")\n        print()\n        print(\"Point-adjusted metrics\")\n        print(f\"F1 score:  {score_pa.f1(ScoreType.PointAdjusted):.4f}\")\n        print(f\"Precision: {score_pa.precision(ScoreType.PointAdjusted):.4f}\")\n        print(f\"Recall:    {score_pa.recall(ScoreType.PointAdjusted):.4f}\")\n        print()\n        print(\"NAB Scores\")\n        print(f\"NAB Score (balanced):       {score_rpa.nab_score():.4f}\")\n        print(f\"NAB Score (high precision): {score_rpa.nab_score(fp_weight=0.22):.4f}\")\n        print(f\"NAB Score (high recall):    {score_rpa.nab_score(fn_weight=2.0):.4f}\")\n        print()\n\n    return score_rpa, score_pw, score_pa\n\n\ndef main():\n    args = parse_args()\n    level = logging.INFO if args.debug or args.visualize else logging.WARNING\n    logging.basicConfig(\n        format=\"%(asctime)s (%(module)s:%(lineno)d) %(levelname)s: %(message)s\", stream=sys.stdout, level=level\n    )\n    dataset = get_dataset(args.dataset, rootdir=args.data_root, **args.data_kwargs)\n    retrain_freq, train_window = args.retrain_freq, args.train_window\n    univariate = dataset[0][0].shape[1] == 1\n    if retrain_freq == \"default\":\n        retrain_freq = \"1d\" if univariate else None\n        desc = \"univariate\" if univariate else \"multivariate\"\n        logger.warning(f\"Setting retrain_freq = {retrain_freq} for {desc} dataset {type(dataset).__name__}\")\n\n    for model_name in args.models:\n        if not args.eval_only:\n            print(f\"Training model {model_name}...\")\n            train_model(\n                model_name=model_name,\n                dataset=dataset,\n                metric=args.metric,\n                tune_on_test=args.tune_on_test,\n                unsupervised=args.unsupervised,\n                debug=args.debug,\n                visualize=args.visualize,\n                load_checkpoint=args.load_checkpoint,\n                retrain_freq=retrain_freq,\n                train_window=train_window,\n            )\n\n    # Read in & evaluate the models' predictions\n    if args.visualize:\n        logger.info(\"Skipping evaluation because --visualize flag was given.\")\n    else:\n        model_names = [resolve_model_name(name) for name in args.models]\n        model_dirs = [name if retrain_freq is None else f\"{name}_{retrain_freq}\" for name in model_names]\n        all_model_preds = [read_model_predictions(dataset=dataset, model_dir=model_dir) for model_dir in model_dirs]\n        score_acc, pw_score_acc, pa_score_acc = evaluate_predictions(\n            model_names=args.models,\n            dataset=dataset,\n            all_model_preds=all_model_preds,\n            debug=args.debug,\n            metric=args.metric,\n            point_adj_metric=args.point_adj_metric,\n            pointwise_metric=args.pointwise_metric,\n            tune_on_test=args.tune_on_test,\n            unsupervised=args.unsupervised,\n        )\n\n        model_name = \"+\".join(sorted(resolve_model_name(m) for m in args.models))\n        summary = os.path.join(\"results\", \"anomaly\", f\"{get_dataset_name(dataset)}_summary.csv\")\n        if os.path.exists(summary):\n            df = pd.read_csv(summary, index_col=0)\n        else:\n            os.makedirs(os.path.dirname(summary), exist_ok=True)\n            df = pd.DataFrame()\n        if retrain_freq:\n            model_name += f\"_{retrain_freq}\"\n        if args.unsupervised:\n            model_name += \" (Unsupervised)\"\n        if args.tune_on_test:\n            model_name += \" (Use Test Data)\"\n        df.loc[model_name, \"Precision\"] = score_acc.precision(ScoreType.RevisedPointAdjusted)\n        df.loc[model_name, \"Recall\"] = score_acc.recall(ScoreType.RevisedPointAdjusted)\n        df.loc[model_name, \"F1\"] = score_acc.f1(ScoreType.RevisedPointAdjusted)\n        df.loc[model_name, \"Mean Time to Detect\"] = score_acc.mean_time_to_detect()\n        df.loc[model_name, \"PA Precision\"] = pa_score_acc.precision(ScoreType.PointAdjusted)\n        df.loc[model_name, \"PA Recall\"] = pa_score_acc.recall(ScoreType.PointAdjusted)\n        df.loc[model_name, \"PA F1\"] = pa_score_acc.f1(ScoreType.PointAdjusted)\n        df.loc[model_name, \"PW Precision\"] = pw_score_acc.precision(ScoreType.Pointwise)\n        df.loc[model_name, \"PW Recall\"] = pw_score_acc.recall(ScoreType.Pointwise)\n        df.loc[model_name, \"PW F1\"] = pw_score_acc.f1(ScoreType.Pointwise)\n\n        df = df.loc[sorted(df.index)]\n        df.to_csv(summary, index=True)\n\n\nif __name__ == \"__main__\":\n    main()\n"
        },
        {
          "name": "benchmark_forecast.py",
          "type": "blob",
          "size": 20.998046875,
          "content": "#\n# Copyright (c) 2022 salesforce.com, inc.\n# All rights reserved.\n# SPDX-License-Identifier: BSD-3-Clause\n# For full license text, see the LICENSE file in the repo root or https://opensource.org/licenses/BSD-3-Clause\n#\nimport argparse\nfrom collections import OrderedDict\nimport glob\nimport json\nimport logging\nimport math\nimport os\nimport re\nimport sys\nimport git\nfrom typing import Dict, List\n\nimport numpy as np\nimport pandas as pd\nimport tqdm\n\nfrom merlion.evaluate.forecast import ForecastEvaluator, ForecastMetric, ForecastEvaluatorConfig\nfrom merlion.models.ensemble.combine import CombinerBase, Mean, ModelSelector, MetricWeightedMean\nfrom merlion.models.ensemble.forecast import ForecasterEnsembleConfig, ForecasterEnsemble\nfrom merlion.models.factory import ModelFactory\nfrom merlion.models.forecast.base import ForecasterBase\nfrom merlion.transform.resample import TemporalResample, granularity_str_to_seconds\nfrom merlion.utils import TimeSeries, UnivariateTimeSeries\nfrom merlion.utils.resample import infer_granularity, to_pd_datetime\n\nfrom ts_datasets.base import BaseDataset\nfrom ts_datasets.forecast import *\n\nimport matplotlib.pyplot as plt\n\nlogger = logging.getLogger(__name__)\n\nMERLION_ROOT = os.path.dirname(os.path.abspath(__file__))\nCONFIG_JSON = os.path.join(MERLION_ROOT, \"conf\", \"benchmark_forecast.json\")\nDATADIR = os.path.join(MERLION_ROOT, \"data\")\nOUTPUTDIR = os.path.join(MERLION_ROOT, \"results\", \"forecast\")\n\n\ndef parse_args():\n    with open(CONFIG_JSON, \"r\") as f:\n        valid_models = list(json.load(f).keys())\n\n    parser = argparse.ArgumentParser(\n        description=\"Script to benchmark various Merlion forecasting models on \"\n        \"univariate forecasting task. This file assumes that \"\n        \"you have pip installed both merlion (this repo's main \"\n        \"package) and ts_datasets (a sub-repo).\"\n    )\n    parser.add_argument(\n        \"--dataset\",\n        default=\"M4_Hourly\",\n        help=\"Name of dataset to run benchmark on. See get_dataset() \"\n        \"in ts_datasets/ts_datasets/forecast/__init__.py for \"\n        \"valid options.\",\n    )\n    parser.add_argument(\"--data_root\", default=None, help=\"Root directory/file of dataset.\")\n    parser.add_argument(\"--data_kwargs\", default=\"{}\", help=\"JSON of keyword arguments for the data loader.\")\n    parser.add_argument(\n        \"--models\",\n        type=str,\n        nargs=\"*\",\n        default=None,\n        help=\"Name of forecasting model to benchmark.\",\n        choices=valid_models,\n    )\n    parser.add_argument(\n        \"--hash\",\n        type=str,\n        default=None,\n        help=\"Unique identifier for the output file. Can be useful \"\n        \"if doing multiple runs with the same model but different \"\n        \"hyperparameters.\",\n    )\n    parser.add_argument(\n        \"--ensemble_type\",\n        type=str,\n        default=\"selector\",\n        help=\"How to combine multiple models in an ensemble\",\n        choices=[\"mean\", \"err_weighted_mean\", \"selector\"],\n    )\n    parser.add_argument(\n        \"--retrain_type\",\n        type=str,\n        default=\"without_retrain\",\n        help=\"Name of retrain type, should be one of the three \"\n        \"types, without_retrain, sliding_window_retrain\"\n        \"or expanding_window_retrain.\",\n        choices=[\"without_retrain\", \"sliding_window_retrain\", \"expanding_window_retrain\"],\n    )\n    parser.add_argument(\"--n_retrain\", type=int, default=0, help=\"Specify the number of retrain times.\")\n    parser.add_argument(\n        \"--load_checkpoint\",\n        action=\"store_true\",\n        default=False,\n        help=\"Specify this option if you would like continue \"\n        \"training your model on a dataset from a \"\n        \"checkpoint, instead of restarting from scratch.\",\n    )\n    parser.add_argument(\"--debug\", action=\"store_true\", default=False, help=\"Whether to set logging level to debug.\")\n    parser.add_argument(\n        \"--visualize\",\n        action=\"store_true\",\n        default=False,\n        help=\"Whether to plot the model's predictions after \"\n        \"training on each example. Mutually exclusive \"\n        \"with running any sort of evaluation.\",\n    )\n    parser.add_argument(\n        \"--summarize\",\n        action=\"store_true\",\n        default=False,\n        help=\"Specify this option if you want to summarize \"\n        \"all results for a particular dataset. Note \"\n        \"that this option only summarizes the results \"\n        \"that have already been computed! It does not \"\n        \"run any algorithms, aside from the one(s) given \"\n        \"to --models (if any).\",\n    )\n\n    args = parser.parse_args()\n    args.data_kwargs = json.loads(args.data_kwargs)\n    assert isinstance(args.data_kwargs, dict)\n\n    # If not summarizing all results, we need at least one model to evaluate\n    if args.summarize and args.models is None:\n        args.models = []\n    elif not args.summarize:\n        if args.models is None:\n            args.models = [\"ARIMA\"]\n        elif len(args.models) == 0:\n            parser.error(\"At least one model required if --summarize not given\")\n\n    return args\n\n\ndef get_dataset_name(dataset: BaseDataset) -> str:\n    name = type(dataset).__name__\n    if hasattr(dataset, \"subset\") and dataset.subset is not None:\n        name += \"_\" + dataset.subset\n    if isinstance(dataset, CustomDataset):\n        root = dataset.rootdir\n        name = os.path.join(name, os.path.basename(os.path.dirname(root) if os.path.isfile(root) else root))\n    return name\n\n\ndef resolve_model_name(model_name: str):\n    with open(CONFIG_JSON, \"r\") as f:\n        config_dict = json.load(f)\n\n    if model_name not in config_dict:\n        raise NotImplementedError(\n            f\"Benchmarking not implemented for model {model_name}. Valid model names are {list(config_dict.keys())}\"\n        )\n\n    while \"alias\" in config_dict[model_name]:\n        model_name = config_dict[model_name][\"alias\"]\n\n    return model_name\n\n\ndef get_model(model_name: str, dataset: BaseDataset, **kwargs) -> ForecasterBase:\n    \"\"\"Gets the model, configured for the specified dataset.\"\"\"\n    with open(CONFIG_JSON, \"r\") as f:\n        config_dict = json.load(f)\n\n    if model_name not in config_dict:\n        raise NotImplementedError(\n            f\"Benchmarking not implemented for model {model_name}. Valid model names are {list(config_dict.keys())}\"\n        )\n\n    while \"alias\" in config_dict[model_name]:\n        model_name = config_dict[model_name][\"alias\"]\n\n    # Load the model with default kwargs, but override with dataset-specific\n    # kwargs where relevant, as well as manual kwargs\n    model_configs = config_dict[model_name][\"config\"]\n    model_type = config_dict[model_name].get(\"model_type\", model_name)\n    model_kwargs = model_configs[\"default\"]\n    model_kwargs.update(model_configs.get(type(dataset).__name__, {}))\n    model_kwargs.update(kwargs)\n\n    # Override the transform with Identity\n    if \"transform\" in model_kwargs:\n        logger.warning(\n            f\"Data pre-processing transforms currently not \"\n            f\"supported for forecasting. Ignoring \"\n            f\"transform {model_kwargs['transform']} and \"\n            f\"using Identity instead.\"\n        )\n    model_kwargs[\"transform\"] = TemporalResample(\n        granularity=None, aggregation_policy=\"Mean\", missing_value_policy=\"FFill\"\n    )\n\n    return ModelFactory.create(name=model_type, **model_kwargs)\n\n\ndef get_combiner(ensemble_type: str) -> CombinerBase:\n    if ensemble_type == \"mean\":\n        return Mean(abs_score=False)\n    elif ensemble_type == \"selector\":\n        return ModelSelector(metric=ForecastMetric.sMAPE)\n    elif ensemble_type == \"err_weighted_mean\":\n        return MetricWeightedMean(metric=ForecastMetric.sMAPE)\n    else:\n        raise KeyError(f\"ensemble_type {ensemble_type} not supported.\")\n\n\ndef get_dirname(model_names: List[str], ensemble_type: str) -> str:\n    dirname = \"+\".join(sorted(model_names))\n    if len(model_names) > 1:\n        dirname += \"_\" + ensemble_type\n    return dirname\n\n\ndef train_model(\n    model_names: List[str],\n    dataset: BaseDataset,\n    ensemble_type: str,\n    csv: str,\n    config_fname: str,\n    retrain_type: str = \"without_retrain\",\n    n_retrain: int = 10,\n    load_checkpoint: bool = False,\n    visualize: bool = False,\n):\n    \"\"\"\n    Trains all the model on the dataset, and evaluates its predictions for every\n    horizon setting on every time series.\n    \"\"\"\n    model_names = [resolve_model_name(m) for m in model_names]\n    dirname = get_dirname(model_names, ensemble_type)\n    dirname = dirname + \"_\" + retrain_type + str(n_retrain)\n    results_dir = os.path.join(MERLION_ROOT, \"results\", \"forecast\", dirname)\n    os.makedirs(results_dir, exist_ok=True)\n    dataset_name = get_dataset_name(dataset)\n\n    # Determine where to start within the dataset if there is a checkpoint\n    if os.path.isfile(csv) and load_checkpoint:\n        i0 = pd.read_csv(csv).idx.max()\n    else:\n        i0 = -1\n        os.makedirs(os.path.dirname(csv), exist_ok=True)\n        with open(csv, \"w\") as f:\n            f.write(\"idx,name,horizon,retrain_type,n_retrain,RMSE,sMAPE\\n\")\n\n    model = None\n    # loop over dataset\n\n    is_multivariate_data = dataset[0][0].shape[1] > 1\n\n    for i, (df, md) in enumerate(tqdm.tqdm(dataset, desc=f\"{dataset_name} Dataset\")):\n        if i <= i0:\n            continue\n        trainval = md[\"trainval\"]\n\n        # Resample to an appropriate granularity according to metadata\n        if \"granularity\" in md:\n            dt = md[\"granularity\"]\n            df = df.resample(dt, closed=\"right\", label=\"right\").mean().interpolate()\n\n        vals = TimeSeries.from_pd(df)\n        dt = infer_granularity(vals.time_stamps)\n\n        # Get the train/val split\n        t = trainval.index[np.argmax(~trainval)].value // 1e9\n        train_vals, test_vals = vals.bisect(t, t_in_left=False)\n\n        # Compute train_window_len and test_window_len\n        train_start_timestamp = train_vals.univariates[train_vals.names[0]].time_stamps[0]\n        test_start_timestamp = test_vals.univariates[test_vals.names[0]].time_stamps[0]\n        train_window_len = test_start_timestamp - train_start_timestamp\n\n        train_end_timestamp = train_vals.univariates[train_vals.names[0]].time_stamps[-1]\n        test_end_timestamp = test_vals.univariates[test_vals.names[0]].time_stamps[-1]\n        test_window_len = test_end_timestamp - train_end_timestamp\n\n        # Get all the horizon conditions we want to evaluate from metadata\n        if any(\"condition\" in k and isinstance(v, list) for k, v in md.items()):\n            conditions = sum([v for k, v in md.items() if \"condition\" in k and isinstance(v, list)], [])\n            logger.debug(\"\\n\" + \"=\" * 80 + \"\\n\" + df.columns[0] + \"\\n\" + \"=\" * 80 + \"\\n\")\n            horizons = set()\n            for condition in conditions:\n                horizons.update([v for k, v in condition.items() if \"horizon\" in k])\n\n        # For multivariate data, we use a horizon of 3\n        elif is_multivariate_data:\n            horizons = [3 * dt]\n\n        # For univariate data, we predict the entire test data in batch\n        else:\n            horizons = [test_window_len]\n\n        # loop over horizon conditions\n        for horizon in horizons:\n            horizon = granularity_str_to_seconds(horizon)\n            try:\n                max_forecast_steps = int(math.ceil(horizon / dt.total_seconds()))\n            except:\n                window = TimeSeries.from_pd(test_vals.to_pd()[: to_pd_datetime(train_end_timestamp + horizon)])\n                max_forecast_steps = len(TemporalResample(granularity=dt)(window))\n            logger.debug(f\"horizon is {pd.Timedelta(seconds=horizon)} and max_forecast_steps is {max_forecast_steps}\")\n            if retrain_type == \"without_retrain\":\n                retrain_freq = None\n                train_window = None\n                n_retrain = 0\n            elif retrain_type == \"sliding_window_retrain\":\n                retrain_freq = math.ceil(test_window_len / int(n_retrain))\n                train_window = train_window_len\n                horizon = min(retrain_freq, horizon)\n            elif retrain_type == \"expanding_window_retrain\":\n                retrain_freq = math.ceil(test_window_len / int(n_retrain))\n                train_window = None\n                horizon = min(retrain_freq, horizon)\n            else:\n                raise ValueError(\n                    \"the retrain_type should be without_retrain, sliding_window_retrain or expanding_window_retrain\"\n                )\n\n            # Get Model\n            models = [get_model(m, dataset, max_forecast_steps=max_forecast_steps) for m in model_names]\n            if len(models) == 1:\n                model = models[0]\n            else:\n                config = ForecasterEnsembleConfig(combiner=get_combiner(ensemble_type))\n                model = ForecasterEnsemble(config=config, models=models)\n\n            evaluator = ForecastEvaluator(\n                model=model,\n                config=ForecastEvaluatorConfig(train_window=train_window, horizon=horizon, retrain_freq=retrain_freq),\n            )\n\n            # Get Evaluate Results\n            train_result, test_pred = evaluator.get_predict(train_vals=train_vals, test_vals=test_vals)\n\n            rmses = evaluator.evaluate(ground_truth=test_vals, predict=test_pred, metric=ForecastMetric.RMSE)\n            smapes = evaluator.evaluate(ground_truth=test_vals, predict=test_pred, metric=ForecastMetric.sMAPE)\n\n            # Log relevant info to the CSV\n            with open(csv, \"a\") as f:\n                f.write(f\"{i},{df.columns[0]},{horizon},{retrain_type},{n_retrain},{rmses},{smapes}\\n\")\n\n            # generate comparison plot\n            if visualize:\n                name = train_vals.names[0]\n                train_time_stamps = train_vals.univariates[name].time_stamps\n                fig_dir = os.path.join(results_dir, dataset_name + \"_figs\")\n                os.makedirs(fig_dir, exist_ok=True)\n                fig_dataset_dir = os.path.join(fig_dir, df.columns[0])\n                os.makedirs(fig_dataset_dir, exist_ok=True)\n                if train_result[0] is not None:\n                    train_pred = train_result[0]\n                else:\n                    train_pred = TimeSeries({name: UnivariateTimeSeries(train_time_stamps, None)})\n                fig_name = dirname + \"_\" + retrain_type + str(n_retrain) + \"_\" + \"horizon\" + str(int(horizon)) + \".png\"\n                plot_unrolled_compare(\n                    train_vals,\n                    test_vals,\n                    train_pred,\n                    test_pred,\n                    os.path.join(fig_dataset_dir, fig_name),\n                    dirname + f\"(sMAPE={smapes:.4f})\",\n                )\n\n            # Log relevant info to the logger\n            logger.debug(f\"{dirname} {retrain_type} {n_retrain} sMAPE : {smapes:.4f}\\n\")\n\n    # Save full experimental config\n    if model is not None:\n        full_config = dict(\n            model_config=model.config.to_dict(),\n            evaluator_config=evaluator.config.to_dict(),\n            code_version_info=get_code_version_info(),\n        )\n\n        with open(config_fname, \"w\") as f:\n            json.dump(full_config, f, indent=2, sort_keys=True)\n\n\ndef get_code_version_info():\n    return dict(time=str(pd.Timestamp.now()), commit=git.Repo(search_parent_directories=True).head.object.hexsha)\n\n\ndef plot_unrolled_compare(train_vals, test_vals, train_pred, test_pred, outputpath, title):\n    truth_pd = (train_vals + test_vals).to_pd()\n    truth_pd.columns = [\"ground_truth\"]\n    pred_pd = (train_pred + test_pred).to_pd()\n    pred_pd.columns = [\"prediction\"]\n    result_pd = pd.concat([truth_pd, pred_pd], axis=1)\n    plt.figure()\n    plt.rcParams[\"savefig.dpi\"] = 500\n    plt.rcParams[\"figure.dpi\"] = 500\n    result_pd.plot(linewidth=0.5)\n    plt.axvline(train_vals.to_pd().index[-1], color=\"r\")\n    plt.title(title)\n    plt.savefig(outputpath)\n    plt.clf()\n\n\ndef join_dfs(name2df: Dict[str, pd.DataFrame]) -> pd.DataFrame:\n    \"\"\"\n    Joins multiple results dataframes into a single dataframe describing the\n    results from all models.\n    \"\"\"\n    full_df, lsuffix = None, \"\"\n    shared_cols = [\"idx\", \"name\", \"horizon\", \"retrain_type\", \"n_retrain\"]\n    for name, df in name2df.items():\n        df.columns = [c if c in shared_cols else f\"{c}_{name}\" for c in df.columns]\n        if full_df is None:\n            full_df = df\n        else:\n            full_df = full_df.merge(df, how=\"outer\", left_on=shared_cols, right_on=shared_cols)\n    unique_cols = [c for c in full_df.columns if c not in shared_cols]\n    return full_df[shared_cols + unique_cols]\n\n\ndef summarize_full_df(full_df: pd.DataFrame) -> pd.DataFrame:\n    # Get the names of all algorithms which have full results\n    algs = [col[len(\"sMAPE\") :] for col in full_df.columns if col.startswith(\"sMAPE\") and not full_df[col].isna().any()]\n    summary_df = pd.DataFrame({alg.lstrip(\"_\"): [] for alg in algs})\n\n    # Compute pooled (per time series) mean/median sMAPE, RMSE\n    mean_smape, med_smape, mean_rmse, med_rmse = [[] for _ in range(4)]\n\n    for ts_name in np.unique(full_df.name):\n        ts = full_df[full_df.name == ts_name]\n        # append smape\n        smapes = ts[[f\"sMAPE{alg}\" for alg in algs]]\n        mean_smape.append(smapes.mean(axis=0).values)\n        med_smape.append(smapes.median(axis=0).values)\n        # append rmse\n        rmses = ts[[f\"RMSE{alg}\" for alg in algs]]\n        mean_rmse.append(rmses.mean(axis=0).values)\n        med_rmse.append(rmses.median(axis=0).values)\n\n    # Add mean/median loglifts to the summary dataframe\n    summary_df.loc[\"mean_sMAPE\"] = np.mean(mean_smape, axis=0)\n    summary_df.loc[\"median_sMAPE\"] = np.median(med_smape, axis=0)\n    summary_df.loc[\"mean_RMSE\"] = np.mean(mean_rmse, axis=0)\n    summary_df.loc[\"median_RMSE\"] = np.median(med_rmse, axis=0)\n    return summary_df\n\n\ndef main():\n    args = parse_args()\n    logging.basicConfig(\n        format=\"%(asctime)s (%(module)s:%(lineno)d) %(levelname)s: %(message)s\",\n        stream=sys.stdout,\n        level=logging.DEBUG if args.debug else logging.INFO,\n    )\n    dataset = get_dataset(args.dataset, rootdir=args.data_root, **args.data_kwargs)\n    dataset_name = get_dataset_name(dataset)\n\n    if len(args.models) > 0:\n        # Determine the name of the results CSV\n        model_names = [resolve_model_name(m) for m in args.models]\n        dirname = get_dirname(model_names, args.ensemble_type)\n        dirname = dirname + \"_\" + args.retrain_type + str(args.n_retrain)\n        results_dir = os.path.join(MERLION_ROOT, \"results\", \"forecast\", dirname)\n        basename = dataset_name\n        if args.hash is not None:\n            basename += \"_\" + args.hash\n        config_fname = f\"{dataset_name}_config\"\n        csv = os.path.join(results_dir, f\"{basename}.csv\")\n        config_fname = os.path.join(results_dir, f\"{config_fname}.json\")\n\n        train_model(\n            model_names=args.models,\n            dataset=dataset,\n            ensemble_type=args.ensemble_type,\n            retrain_type=args.retrain_type,\n            n_retrain=args.n_retrain,\n            csv=csv,\n            config_fname=config_fname,\n            load_checkpoint=args.load_checkpoint,\n            visualize=args.visualize,\n        )\n\n        # Pool the mean/medium sMAPE, RMSE for all evaluation\n        # settings for each time series, and report summary\n        # pooled statistics.\n        df = pd.read_csv(csv)\n        summary = summarize_full_df(df)\n        summary.to_csv(os.path.join(results_dir, f\"{basename}_summary.csv\"), index=True)\n        summary = summary[summary.columns[0]]\n        logger.info(f\"Pooled mean   sMAPE: {summary['mean_sMAPE']:.4f}\")\n        logger.info(f\"Pooled median sMAPE: {summary['median_sMAPE']:.4f}\")\n        logger.info(f\"Pooled mean   RMSE: {summary['mean_RMSE']:.4f}\")\n        logger.info(f\"Pooled median RMSE: {summary['median_RMSE']:.4f}\")\n\n    # Now we summarize all results. Get all the individual CSV's as dataframes\n    name2df = OrderedDict()\n    prefix = f\"{MERLION_ROOT}/results/forecast/*/{dataset_name}\"\n    csvs = glob.glob(f\"{prefix}.csv\") + glob.glob(f\"{prefix}_*.csv\")\n    csvs = [c for c in csvs if not c.endswith(f\"_summary.csv\")]\n    if len(csvs) == 0:\n        raise RuntimeError(\n            f\"Did not find any pre-computed results files \"\n            f\"for dataset {dataset_name}. Please run this \"\n            f\"script on the dataset with specific algorithms \"\n            f\"before trying to summarize their results.\"\n        )\n    for csv in sorted(csvs):\n        basename = re.search(f\"{dataset_name}.*\\\\.csv\", csv).group(0)\n        model_name = os.path.basename(os.path.dirname(csv[: -len(basename)]))\n        suffix = re.search(f\"(?<={dataset_name}).*(?=\\\\.csv)\", basename).group(0)\n        try:\n            name2df[model_name + suffix] = pd.read_csv(csv)\n        except Exception as e:\n            logger.warning(f'Caught {type(e).__name__}: \"{e}\". Skipping csv file {csv}.')\n            continue\n\n    # Join all the dataframes into one & summarize the results\n    dirname = os.path.join(MERLION_ROOT, \"results\", \"forecast\")\n    full_df = join_dfs(name2df)\n    summary_df = summarize_full_df(full_df)\n    if args.summarize:\n        print(summary_df)\n\n    full_fname, summary_fname = [os.path.join(dirname, f\"{dataset_name}_{x}.csv\") for x in [\"full\", \"summary\"]]\n    os.makedirs(os.path.dirname(full_fname), exist_ok=True)\n    full_df.to_csv(full_fname, index=False)\n    summary_df.to_csv(summary_fname, index=True)\n\n\nif __name__ == \"__main__\":\n    main()\n"
        },
        {
          "name": "conf",
          "type": "tree",
          "content": null
        },
        {
          "name": "data",
          "type": "tree",
          "content": null
        },
        {
          "name": "docker",
          "type": "tree",
          "content": null
        },
        {
          "name": "docs",
          "type": "tree",
          "content": null
        },
        {
          "name": "examples",
          "type": "tree",
          "content": null
        },
        {
          "name": "figures",
          "type": "tree",
          "content": null
        },
        {
          "name": "k8s-spec",
          "type": "tree",
          "content": null
        },
        {
          "name": "merlion",
          "type": "tree",
          "content": null
        },
        {
          "name": "merlion_logo.svg",
          "type": "blob",
          "size": 3.6513671875,
          "content": "<?xml version=\"1.0\" encoding=\"utf-8\"?>\n<svg viewBox=\"0 0 475 125\" xmlns=\"http://www.w3.org/2000/svg\">\n  <defs>\n    <mask id=\"merlion\">\n      <circle style=\"fill: rgb(255, 255, 255); fill-rule: evenodd;\" cx=\"1059.161\" cy=\"-552.178\" r=\"436.487\"/>\n      <path d=\"M 1065.97 743.828 C 1065.97 743.828 1074.82 756.641 1104.97 742.152 C 1104.97 742.152 1104.79 726.934 1076.8 726.191 C 1048.82 725.449 1060.55 729.531 1060.55 729.531 L 1065.97 743.828 Z M 1019.692 105.788 C 1186.644 50.493 1280.918 121.345 1361.865 159.466 C 1413.367 183.72 1460.719 212.499 1408.14 290.586 C 1403.2 325.277 1370.85 326.828 1350.36 434.133 C 1364.9 434.266 1375.96 433.473 1392.43 432.742 C 1390.98 437.477 1385.78 445.145 1380.83 453.422 L 1380.14 453.043 C 1367.93 475.609 1359.83 506.273 1354.92 530.113 L 1354.86 530.352 L 1354.5 532.121 L 1354.48 532.223 C 1350.48 551.68 1348.61 566.539 1348.38 568.461 L 1348.37 568.465 L 1348.36 568.617 L 1348.35 568.672 C 1347.4 575.406 1346.8 581.598 1346.46 586.926 L 1379.69 579.527 C 1351.5 668.84 1366.73 719.148 1303.68 775.535 L 1310.32 803.211 C 1268.22 798.031 1216.68 835.66 1141.58 820.383 C 1115.8 806.504 1111.68 815.742 1081.47 781.215 C 1061.97 758.918 1038.42 764.828 1031.22 731.887 C 999.563 730.07 962.398 727.082 964.949 691.66 C 965.844 679.148 967.336 648.309 971.137 640.098 C 979.477 622.129 989.816 630.41 1007.63 624.125 C 1010.59 627.766 1013.58 630.992 1016.61 633.848 L 1024.87 620.465 C 1024.87 620.465 1028.61 635.293 1040.57 647.945 L 1041.32 649.398 C 1093.13 667.285 1134.97 596.297 1062.68 588.473 L 1045.56 588.574 C 1035.25 596.699 1032.21 608.77 1032.21 608.77 L 1026.25 599.117 L 1006.96 588.813 C 1006.71 567.852 999.313 563.59 1003.22 542.496 C 1019.03 535.438 1036.63 534.711 1058.25 534.066 C 974.945 346.973 912.406 521.961 1006.97 322.227 C 1041.41 246.652 1037.3 179.41 1024.37 124.531 M 962.121 595.965 C 962.121 595.965 749.273 571.44 619.827 426.444 C 640.883 383.102 637.567 365.548 665.799 354.572 C 788.567 533.219 960.399 597.222 956.138 594.244\" style=\"fill:#000000;fill-opacity:1;fill-rule:evenodd;stroke:none'\" transform=\"matrix(1, 0, 0, -1, 0, 0)\"/>\n    </mask>\n  </defs>\n  <g id=\"g10\" transform=\"matrix(0.13, 0, 0, 0.13, -80.57, 135.32)\">\n    <path d=\"m 2274.48,441.102 -0.96,337.796 h -64.27 L 2084.84,568.984 1958.48,778.898 h -64.66 V 441.102 h 73.35 v 198.336 l 98.85,-162.625 h 35.22 l 99.33,166.964 0.48,-202.675 z M 4122.79,778.898 V 573.805 l -167.93,205.093 h -64.67 V 441.102 h 77.21 V 646.191 L 4135.82,441.102 H 4200 V 778.898 Z M 3646.5,435.309 c 106.16,0 184.82,73.832 184.82,174.691 0,100.859 -78.66,174.688 -184.82,174.688 -106.65,0 -184.83,-74.313 -184.83,-174.688 0,-100.375 78.18,-174.691 184.83,-174.691 z m 0,66.593 c -60.32,0 -105.68,43.914 -105.68,108.098 0,64.18 45.36,108.094 105.68,108.094 60.32,0 105.68,-43.914 105.68,-108.094 0,-64.184 -45.36,-108.098 -105.68,-108.098 z m -321.87,-60.8 h 78.17 v 337.796 h -78.17 z m -291.47,0 h 247.55 v 63.699 h -169.38 v 274.097 h -78.17 z m -55.5,0 -75.76,108.574 c 43.92,18.82 69.49,55.98 69.49,106.656 0,75.754 -56.46,122.566 -146.7,122.566 H 2678.47 V 441.102 h 78.18 v 94.097 h 68.04 3.86 l 65.15,-94.097 z m -85.41,215.23 c 0,-36.684 -24.13,-58.879 -71.9,-58.879 h -63.7 v 117.746 h 63.7 c 47.77,0 71.9,-21.715 71.9,-58.867 z m -459.89,-152.5 v 78.18 h 156.84 v 60.801 h -156.84 v 73.351 h 177.59 v 62.734 H 2354.67 V 441.102 h 261.55 v 62.73 h -183.86\" style=\"fill:#02144a;fill-opacity:1;fill-rule:evenodd;stroke:#ffffff;stroke-opacity:1;stroke-width:4;shape-rendering='geometricPrecision'\" id=\"path14\" transform=\"matrix(1, 0, 0, -1, -160, 35)\"/>\n    <circle style=\"fill: rgb(0, 161, 224); fill-rule: evenodd;\" cx=\"1059.161\" cy=\"-552.178\" r=\"436.487\" mask=\"url(#merlion)\"/>\n  </g>\n</svg>"
        },
        {
          "name": "pytest.ini",
          "type": "blob",
          "size": 0.1494140625,
          "content": "[pytest]\nlog_format = %(asctime)s (%(module)s:%(lineno)d) %(levelname)s: %(message)s\nlog_date_format = %Y-%m-%d %H:%M:%S\nlog_cli=true\nlog_cli_level=INFO\n"
        },
        {
          "name": "setup.py",
          "type": "blob",
          "size": 3.0078125,
          "content": "#\n# Copyright (c) 2023 salesforce.com, inc.\n# All rights reserved.\n# SPDX-License-Identifier: BSD-3-Clause\n# For full license text, see the LICENSE file in the repo root or https://opensource.org/licenses/BSD-3-Clause\n#\nfrom setuptools import setup, find_namespace_packages\n\nMERLION_JARS = [\n    \"resources/gson-2.8.9.jar\",\n    \"resources/randomcutforest-core-1.0.jar\",\n    \"resources/randomcutforest-serialization-json-1.0.jar\",\n]\n\nMERLION_DASHBOARD_ASSETS = [\n    \"dashboard/assets/fonts/SalesforceSans-Bold.woff\",\n    \"dashboard/assets/fonts/SalesforceSans-BoldItalic.woff\",\n    \"dashboard/assets/fonts/SalesforceSans-Italic.woff\",\n    \"dashboard/assets/fonts/SalesforceSans-Light.woff\",\n    \"dashboard/assets/fonts/SalesforceSans-LightItalic.woff\",\n    \"dashboard/assets/fonts/SalesforceSans-Regular.woff\",\n    \"dashboard/assets/fonts/SalesforceSans-Thin.woff\",\n    \"dashboard/assets/fonts/SalesforceSans-ThinItalic.woff\",\n    \"dashboard/assets/Acumin-BdPro.otf\",\n    \"dashboard/assets/base.css\",\n    \"dashboard/assets/merlion.css\",\n    \"dashboard/assets/merlion_small.svg\",\n    \"dashboard/assets/modal.css\",\n    \"dashboard/assets/resizing.js\",\n    \"dashboard/assets/styles.css\",\n    \"dashboard/assets/upload.svg\",\n]\n\n# optional dependencies\nextra_require = {\n    \"dashboard\": [\"dash[diskcache]>=2.4\", \"dash_bootstrap_components>=1.0\", \"diskcache\"],\n    \"deep-learning\": [\"torch>=1.9.0\", \"einops>=0.4.0\"],\n    \"spark\": [\"pyspark[sql]>=3\"],\n}\nextra_require[\"all\"] = sum(extra_require.values(), [])\n\n\ndef read_file(fname):\n    with open(fname, \"r\", encoding=\"utf-8\") as f:\n        return f.read()\n\n\nsetup(\n    name=\"salesforce-merlion\",\n    version=\"2.0.2\",\n    author=\", \".join(read_file(\"AUTHORS.md\").split(\"\\n\")),\n    author_email=\"abhatnagar@salesforce.com\",\n    description=\"Merlion: A Machine Learning Framework for Time Series Intelligence\",\n    long_description=read_file(\"README.md\"),\n    long_description_content_type=\"text/markdown\",\n    keywords=\"time series, forecasting, anomaly detection, machine learning, autoML, \"\n    \"ensemble learning, benchmarking, Python, scientific toolkit\",\n    url=\"https://github.com/salesforce/Merlion\",\n    license=\"3-Clause BSD\",\n    packages=find_namespace_packages(include=\"merlion.*\"),\n    package_dir={\"merlion\": \"merlion\"},\n    package_data={\"merlion\": MERLION_JARS + MERLION_DASHBOARD_ASSETS},\n    install_requires=[\n        \"cython\",\n        \"dill\",\n        \"GitPython\",\n        \"py4j\",\n        \"matplotlib\",\n        \"plotly>=4.13\",\n        \"numpy>=1.21,<2.0\",  # 1.21 remediates a security risk\n        \"packaging\",\n        \"pandas>=1.1.0\",  # >=1.1.0 for origin kwarg to df.resample()\n        \"prophet>=1.1\",  # 1.1 removes dependency on pystan\n        \"scikit-learn>=0.22\",  # >=0.22 for changes to isolation forest algorithm\n        \"scipy>=1.6.0\",  # 1.6.0 adds multivariate_t density to scipy.stats\n        \"statsmodels>=0.12.2\",\n        \"lightgbm\",  # if running at MacOS, need OpenMP: \"brew install libomp\"\n        \"tqdm\",\n    ],\n    extras_require=extra_require,\n    python_requires=\">=3.7.0\",\n    zip_safe=False,\n)\n"
        },
        {
          "name": "spark_apps",
          "type": "tree",
          "content": null
        },
        {
          "name": "tests",
          "type": "tree",
          "content": null
        },
        {
          "name": "ts_datasets",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}