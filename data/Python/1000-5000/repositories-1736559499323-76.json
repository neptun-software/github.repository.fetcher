{
  "metadata": {
    "timestamp": 1736559499323,
    "page": 76,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjgw",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "IrisRainbowNeko/genshin_auto_fish",
      "stars": 4793,
      "defaultBranch": "master",
      "files": [
        {
          "name": "README.md",
          "type": "blob",
          "size": 4.3447265625,
          "content": "# Introduction\n**现已支持不同分辨率屏幕**\n\n原神自动钓鱼AI由[YOLOX](https://github.com/Megvii-BaseDetection/YOLOX), DQN两部分模型组成。使用迁移学习，半监督学习进行训练。\n模型也包含一些使用opencv等传统数字图像处理方法实现的不可学习部分。\n\n其中YOLOX用于鱼的定位和类型的识别以及鱼竿落点的定位。DQN用于自适应控制钓鱼过程的点击，让力度落在最佳区域内。\n\n# 安装使用流程\n安装python运行环境（解释器），推荐使用 [anaconda](https://www.anaconda.com/products/individual#Downloads).\n\n## python环境配置\n\n打开anaconda prompt(命令行界面)，创建新python环境并激活:\n```shell\nconda create -n ysfish python=3.8\nconda activate ysfish\n```\n推荐安装<font color=#66CCFF>**python3.8或以下**</font>版本。\n\n## 下载工程代码\n使用git下载，[git安装教程](https://www.cnblogs.com/xiaoliu66/p/9404963.html):\n```shell\ngit clone https://github.com/7eu7d7/genshin_auto_fish.git\n```\n或直接在**github网页端**下载后直接解压。\n\n## 依赖库安装\n切换命令行到本工程所在目录:\n```shell\ncd genshin_auto_fish\n```\n执行以下命令安装依赖:\n```shell\npython -m pip install -U pip\npython requirements.py\n```\n如果要使用显卡进行加速需要 [安装CUDA和cudnn](https://zhuanlan.zhihu.com/p/94220564?utm_source=wechat_session&ivk_sa=1024320u) 安装后无视上面的命令用下面这条安装gpu版:\n```shell\npip install -U pip\npython requirements.py --cuda [cuda版本]\n#例如安装的CUDA11.x\npython requirements.py --cuda 111\n#或者使用代理加速\npython requirements.py --cuda 111 --proxy http://127.0.0.1:1080 # use proxy to speed up\n```\n可能会有Time out之类的报错，多试几遍，github太卡。\n\n## 安装yolox\n切换命令行到本工程所在目录，执行以下命令安装yolox:\n```shell\npython setup.py develop\n```\n\n## 预训练权重下载\n\n- [`best_tiny3.pth`](https://github.com/7eu7d7/genshin_auto_fish/releases/download/weights/best_tiny3.pth)\n- [`fish_genshin_net.pth`](https://github.com/7eu7d7/genshin_auto_fish/releases/download/weights/fish_genshin_net.pth)\n- [`fish_sim_net.pth`](https://github.com/7eu7d7/genshin_auto_fish/releases/download/weights/fish_sim_net.pth)\n- [`yolox_tiny.pth`](https://github.com/Megvii-BaseDetection/YOLOX/releases/download/0.1.1rc0/yolox_tiny.pth)\n\n下载后将权重文件放在 <font color=#66CCFF>**工程目录/weights**</font> 下\n\n# 运行钓鱼AI\n原神需要以1080x1920的分辨率运行，分辨率高的屏幕可以开窗口模式。\n\n命令行窗口一定要以<font color=#66CCFF>**管理员权限**</font>启动\n\n显卡加速\n```shell\npython fishing.py image -f yolox/exp/yolox_tiny_fish.py -c weights/best_tiny3.pth --conf 0.25 --nms 0.45 --tsize 640 --device gpu\n```\ncpu运行\n```shell\npython fishing.py image -f yolox/exp/yolox_tiny_fish.py -c weights/best_tiny3.pth --conf 0.25 --nms 0.45 --tsize 640 --device cpu\n```\n运行后出现**init ok**后按r键开始钓鱼，原神需要全屏。出于性能考虑检测框不会实时显示，处理运算后台进行。\n\n# YOLOX训练工作流程\n<**只用来钓鱼不需要训练，直接用预训练权重就可以**>\n\nYOLOX部分因为打标签太累所以用半监督学习。标注少量样本后训练模型生成其余样本伪标签再人工修正，不断迭代提高精度。\n样本量较少所以使用迁移学习，在COCO预训练的模型上进行fine-tuning.\n\n下载数据集并解压：[原神鱼群数据集](https://1drv.ms/u/s!Agabh9imkP8qhHkZYzKsi_OQ4pfj?e=V2VApo), \n[数据集(迅雷云盘:ugha)](https://pan.xunlei.com/s/VMkCJx-bOnpF431_9R0E8vAsA1)\n\n将yolox/exp/yolox_tiny_fish.py中的self.data_dir的值改为解压后2个文件夹所在的路径。\n\n训练代码:\n```shell\npython yolox_tools/train.py -f yolox/exp/yolox_tiny_fish.py -d 1 -b 8 --fp16 -o -c weights/yolox_tiny.pth\n```\n\n# DQN训练工作流程\n控制力度使用强化学习模型DQN进行训练。两次进度的差值作为reward为模型提供学习方向。模型与环境间交互式学习。\n\n直接在原神内训练耗时较长，太累了。首先制作一个仿真环境，大概模拟钓鱼力度控制操作。在仿真环境内预训练一个模型。\n随后将这一模型迁移至原神内，实现域间迁移。\n\n仿真环境预训练代码:\n```shell\npython train_sim.py\n```\n原神游戏内训练:\n```shell\npython train.py\n```\n"
        },
        {
          "name": "capture.py",
          "type": "blob",
          "size": 0.5390625,
          "content": "import time\nfrom utils import *\n\nimport keyboard\nimport winsound\nimport cv2\n\n'''i=0\nwhile True:\n    keyboard.wait('t')\n    img = pyautogui.screenshot()\n    img.save(f'img_tmp/{i}.png')\n    i+=1'''\n\nim_exit = cv2.imread('./imgs/exit.png')\n\nprint('ok')\nkeyboard.wait('t')\n\nexit_pos = match_img(cap_raw(), im_exit)\ngvars.genshin_window_rect_img = (exit_pos[0] - 32, exit_pos[1] - 19, DEFAULT_MONITOR_WIDTH, DEFAULT_MONITOR_HEIGHT)\n\nfor i in range(56,56+20):\n    img = cap()\n    img.save(f'fish_dataset/{i}.png')\n    time.sleep(0.5)\nwinsound.Beep(500, 500)"
        },
        {
          "name": "config.yaml",
          "type": "blob",
          "size": 0.099609375,
          "content": "---\r\n  windows:\r\n    monitor_width: 1920\r\n    monitor_height: 1080\r\n  game:\r\n    window_name: \"原神\""
        },
        {
          "name": "fisher",
          "type": "tree",
          "content": null
        },
        {
          "name": "fishing.py",
          "type": "blob",
          "size": 6.2802734375,
          "content": "#!/usr/bin/env python3\n# -*- coding:utf-8 -*-\n# Copyright (c) Megvii, Inc. and its affiliates.\n\nimport argparse\nimport os\nimport time\n\nfrom loguru import logger\n\nimport torch\nimport keyboard\nimport winsound\n\nfrom yolox.exp import get_exp\nfrom yolox.utils import fuse_model, get_model_info\n\nfrom fisher.environment import *\nfrom fisher.predictor import *\nfrom fisher.models import FishNet\n\ndef make_parser():\n    parser = argparse.ArgumentParser(\"YOLOX Demo!\")\n    parser.add_argument(\"demo\", default=\"image\", help=\"demo type, eg. image, video and webcam\")\n    parser.add_argument(\"-expn\", \"--experiment-name\", type=str, default=None)\n    parser.add_argument(\"-n\", \"--name\", type=str, default=None, help=\"model name\")\n    parser.add_argument(\"--path\", default=\"./assets/dog.jpg\", help=\"path to images or video\")\n\n    # exp file\n    parser.add_argument(\n        \"-f\",\n        \"--exp_file\",\n        default=None,\n        type=str,\n        help=\"pls input your experiment description file\",\n    )\n    parser.add_argument(\"-c\", \"--ckpt\", default=None, type=str, help=\"ckpt for eval\")\n    parser.add_argument(\n        \"--device\",\n        default=\"cpu\",\n        type=str,\n        help=\"device to run our model, can either be cpu or gpu\",\n    )\n    parser.add_argument(\"--conf\", default=0.3, type=float, help=\"test conf\")\n    parser.add_argument(\"--nms\", default=0.3, type=float, help=\"test nms threshold\")\n    parser.add_argument(\"--tsize\", default=None, type=int, help=\"test img size\")\n    parser.add_argument(\n        \"--fp16\",\n        dest=\"fp16\",\n        default=False,\n        action=\"store_true\",\n        help=\"Adopting mix precision evaluating.\",\n    )\n    parser.add_argument(\n        \"--legacy\",\n        dest=\"legacy\",\n        default=False,\n        action=\"store_true\",\n        help=\"To be compatible with older versions\",\n    )\n    parser.add_argument(\n        \"--fuse\",\n        dest=\"fuse\",\n        default=False,\n        action=\"store_true\",\n        help=\"Fuse conv and bn for testing.\",\n    )\n    parser.add_argument(\n        \"--trt\",\n        dest=\"trt\",\n        default=False,\n        action=\"store_true\",\n        help=\"Using TensorRT model for testing.\",\n    )\n\n    # DQN args\n    parser.add_argument('--n_states', default=3, type=int)\n    parser.add_argument('--n_actions', default=2, type=int)\n    parser.add_argument('--step_tick', default=12, type=int)\n    parser.add_argument('--model_dir', default='./weights/fish_genshin_net.pth', type=str)\n\n    return parser\n\ndef main(exp, args):\n    if not args.experiment_name:\n        args.experiment_name = exp.exp_name\n\n    if args.trt:\n        args.device = \"gpu\"\n\n    logger.info(\"Args: {}\".format(args))\n\n    if args.conf is not None:\n        exp.test_conf = args.conf\n    if args.nms is not None:\n        exp.nmsthre = args.nms\n    if args.tsize is not None:\n        exp.test_size = (args.tsize, args.tsize)\n\n    model = exp.get_model()\n    logger.info(\"Model Summary: {}\".format(get_model_info(model, exp.test_size)))\n\n    if args.device == \"gpu\":\n        model.cuda()\n        if args.fp16:\n            model.half()  # to FP16\n    model.eval()\n\n    if not args.trt:\n        if args.ckpt is None:\n            ckpt_file = os.path.join(file_name, \"best_ckpt.pth\")\n        else:\n            ckpt_file = args.ckpt\n        logger.info(\"loading checkpoint\")\n        ckpt = torch.load(ckpt_file, map_location=\"cpu\")\n        # load the model state dict\n        model.load_state_dict(ckpt[\"model\"])\n        logger.info(\"loaded checkpoint done.\")\n\n    if args.fuse:\n        logger.info(\"\\tFusing model...\")\n        model = fuse_model(model)\n\n    if args.trt:\n        assert not args.fuse, \"TensorRT model is not support model fusing!\"\n        if args.ckpt is None:\n            trt_file = os.path.join(file_name, \"model_trt.pth\")\n        else:\n            trt_file = args.ckpt\n        assert os.path.exists(\n            trt_file\n        ), \"TensorRT model is not found!\\n Run python3 tools/trt.py first!\"\n        model.head.decode_in_inference = False\n        decoder = model.head.decode_outputs\n        logger.info(\"Using TensorRT to inference\")\n    else:\n        trt_file = None\n        decoder = None\n\n    predictor = Predictor(model, exp, FISH_CLASSES, trt_file, decoder, args.device, args.fp16, args.legacy)\n\n    agent = FishNet(in_ch=args.n_states, out_ch=args.n_actions)\n    agent.load_state_dict(torch.load(args.model_dir))\n    agent.eval()\n\n    print('INIT OK')\n    while True:\n        print('Waiting for \"r\" to perform fishing')\n        winsound.Beep(500, 500)\n        keyboard.wait('r')\n        winsound.Beep(500, 500)\n        if args.demo == \"image\":\n            start_fishing(predictor, agent)\n\ndef start_fishing(predictor, agent, bite_timeout=45):\n    ff = FishFind(predictor)\n    env = Fishing(delay=0.1, max_step=10000, show_det=True)\n\n    do_fish_count = 0\n    while True:\n        continue_flag = False\n        if do_fish_count > 4:\n            winsound.Beep(500, 1000)\n            time.sleep(0.5)\n            winsound.Beep(500, 1000)\n            time.sleep(0.5)\n            winsound.Beep(500, 1000)\n            do_fish_count = 0\n            break\n        result: bool = ff.do_fish()\n\n        # continue if no fish found\n        if not result:\n            do_fish_count += 1\n            continue\n\n        do_fish_count = 0\n        winsound.Beep(700, 500)\n        times=0\n        while result is True:\n            if env.is_bite():\n                break\n            time.sleep(0.5)\n            times+=1\n            if times>bite_timeout and not(env.is_bite()):\n                if env.is_fishing():\n                    env.drag()\n                time.sleep(3)\n                times=0\n                continue_flag = True\n                break\n\n        if continue_flag == True:\n            continue\n\n        winsound.Beep(900, 500)\n        env.drag()\n        time.sleep(1)\n\n        state = env.reset()\n        for i in range(env.max_step):\n            state = torch.FloatTensor(state).unsqueeze(0)\n            action = agent(state)\n            action = torch.argmax(action, dim=1).numpy()\n            state, reward, done = env.step(action)\n            if done:\n                break\n        time.sleep(3)\n\n#python fishing.py image -f yolox/exp/yolox_tiny_fish.py -c weights/best_tiny3.pth --conf 0.25 --nms 0.45 --tsize 640 --device gpu\nif __name__ == \"__main__\":\n    args = make_parser().parse_args()\n    exp = get_exp(args.exp_file, args.name)\n\n    main(exp, args)\n"
        },
        {
          "name": "imgs",
          "type": "tree",
          "content": null
        },
        {
          "name": "requirements.py",
          "type": "blob",
          "size": 2.1279296875,
          "content": "# import pip\nimport subprocess\nimport sys\nimport argparse\n\n# use python type hints to make code more readable\nfrom typing import List, Optional\n\n\ndef pip_install(proxy: Optional[str], args: List[str]) -> None:\n    if proxy is None:\n        # pip.main([\"install\", f\"--proxy={proxy}\", *args])\n        subprocess.run(\n            [sys.executable, \"-m\", \"pip\", \"install\", *args],\n            # capture_output=False,\n            check=True,\n        )\n    else:\n        subprocess.run(\n            [sys.executable, \"-m\", \"pip\", \"install\", f\"--proxy={proxy}\", *args],\n            # capture_output=False,\n            check=True,\n        )\n\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"install requirements\")\n    parser.add_argument(\"--cuda\", default=None, type=str)\n    parser.add_argument(\n        \"--proxy\",\n        default=None,\n        type=str,\n        help=\"specify http proxy, [http://127.0.0.1:1080]\",\n    )\n    args = parser.parse_args()\n\n    pkgs = f\"\"\"\n    cython\n    scikit-image\n    loguru\n    matplotlib\n    tabulate\n    tqdm\n    pywin32\n    PyAutoGUI\n    PyYAML>=5.3.1\n    opencv_python\n    keyboard\n    Pillow\n    pymouse\n    numpy>=1.21.1\n    torch==1.8.2+{\"cpu\" if args.cuda is None else \"cu\" + args.cuda} -f https://download.pytorch.org/whl/lts/1.8/torch_lts.html\n    torchvision==0.9.2+{\"cpu\" if args.cuda is None else \"cu\" + args.cuda} --no-deps -f https://download.pytorch.org/whl/lts/1.8/torch_lts.html\n    thop --no-deps\n    git+https://github.com/philferriere/cocoapi.git#subdirectory=PythonAPI\n    \"\"\"\n\n    for line in pkgs.split(\"\\n\"):\n        # handle multiple space in an empty line\n        line = line.strip()\n\n        if len(line) > 0:\n            # use pip's internal APIs in this way is deprecated. This will fail in a future version of pip.\n            # The most reliable approach, and the one that is fully supported, is to run pip in a subprocess.\n            # ref: https://pip.pypa.io/en/latest/user_guide/#using-pip-from-your-program\n            # pip.main(['install', *line.split()])\n\n            pip_install(args.proxy, line.split())\n\n    print(\"\\nsuccessfully installed requirements!\")\n\n\nif __name__ == \"__main__\":\n    main()\n"
        },
        {
          "name": "setup.cfg",
          "type": "blob",
          "size": 0.6005859375,
          "content": "[isort]\nline_length = 100\nmulti_line_output = 3\nbalanced_wrapping = True\nknown_standard_library = setuptools\nknown_third_party = tqdm,loguru\nknown_data_processing = cv2,numpy,scipy,PIL,matplotlib,scikit_image\nknown_datasets = pycocotools\nknown_deeplearning = torch,torchvision,caffe2,onnx,apex,timm,thop,torch2trt,tensorrt,openvino,onnxruntime\nknown_myself = yolox\nsections = FUTURE,STDLIB,THIRDPARTY,data_processing,datasets,deeplearning,myself,FIRSTPARTY,LOCALFOLDER\nno_lines_before=STDLIB,THIRDPARTY,datasets\ndefault_section = FIRSTPARTY\n\n[flake8]\nmax-line-length = 100\nmax-complexity = 18\nexclude = __init__.py\n"
        },
        {
          "name": "setup.py",
          "type": "blob",
          "size": 1.638671875,
          "content": "#!/usr/bin/env python\n# Copyright (c) Megvii, Inc. and its affiliates. All Rights Reserved\n\nimport re\nimport setuptools\nimport glob\nfrom os import path\nimport torch\nfrom torch.utils.cpp_extension import CppExtension\n\ntorch_ver = [int(x) for x in torch.__version__.split(\".\")[:2]]\nassert torch_ver >= [1, 7], \"Requires PyTorch >= 1.7\"\n\n\ndef get_extensions():\n    this_dir = path.dirname(path.abspath(__file__))\n    extensions_dir = path.join(this_dir, \"yolox\", \"layers\", \"csrc\")\n\n    main_source = path.join(extensions_dir, \"vision.cpp\")\n    sources = glob.glob(path.join(extensions_dir, \"**\", \"*.cpp\"))\n\n    sources = [main_source] + sources\n    extension = CppExtension\n\n    extra_compile_args = {\"cxx\": [\"-O3\"]}\n    define_macros = []\n\n    include_dirs = [extensions_dir]\n\n    ext_modules = [\n        extension(\n            \"yolox._C\",\n            sources,\n            include_dirs=include_dirs,\n            define_macros=define_macros,\n            extra_compile_args=extra_compile_args,\n        )\n    ]\n\n    return ext_modules\n\n\nwith open(\"yolox/__init__.py\", \"r\") as f:\n    version = re.search(\n        r'^__version__\\s*=\\s*[\\'\"]([^\\'\"]*)[\\'\"]',\n        f.read(), re.MULTILINE\n    ).group(1)\n\n\nwith open(\"README.md\", \"r\", encoding=\"utf-8\") as f:\n    long_description = f.read()\n\n\nsetuptools.setup(\n    name=\"yolox\",\n    version=version,\n    author=\"basedet team\",\n    python_requires=\">=3.6\",\n    long_description=long_description,\n    ext_modules=get_extensions(),\n    classifiers=[\"Programming Language :: Python :: 3\", \"Operating System :: OS Independent\"],\n    cmdclass={\"build_ext\": torch.utils.cpp_extension.BuildExtension},\n    packages=setuptools.find_packages(),\n)\n"
        },
        {
          "name": "test.py",
          "type": "blob",
          "size": 1.2275390625,
          "content": "import keyboard\nimport winsound\nfrom fisher.models import FishNet\nfrom fisher.environment import *\nimport torch\nimport argparse\n\nparser = argparse.ArgumentParser(description='Test Genshin finsing with DQN')\nparser.add_argument('--n_states', default=3, type=int)\nparser.add_argument('--n_actions', default=2, type=int)\nparser.add_argument('--step_tick', default=12, type=int)\nparser.add_argument('--model_dir', default='./weights/fish_genshin_net.pth', type=str)\nargs = parser.parse_args()\n\nif __name__ == '__main__':\n\n    net = FishNet(in_ch=args.n_states, out_ch=args.n_actions)\n    env = Fishing(delay=0.1, max_step=10000, show_det=True)\n\n    net.load_state_dict(torch.load(args.model_dir))\n    net.eval()\n\n    while True:\n        winsound.Beep(500, 500)\n        keyboard.wait('r')\n        while True:\n            if env.is_bite():\n                break\n            time.sleep(0.5)\n        winsound.Beep(700, 500)\n        env.drag()\n        time.sleep(1)\n\n        state = env.reset()\n        for i in range(10000):\n            state = torch.FloatTensor(state).unsqueeze(0)\n            action = net(state)\n            action = torch.argmax(action, dim=1).numpy()\n            state, reward, done = env.step(action)\n            if done:\n                break"
        },
        {
          "name": "test_sim.py",
          "type": "blob",
          "size": 1.1982421875,
          "content": "from utils.render import *\nfrom fisher.models import FishNet\nfrom fisher.environment import *\nimport torch\nimport argparse\nfrom matplotlib.animation import FFMpegWriter\n\nparser = argparse.ArgumentParser(description='Test Genshin finsing with DQN')\nparser.add_argument('--n_states', default=3, type=int)\nparser.add_argument('--n_actions', default=2, type=int)\nparser.add_argument('--step_tick', default=12, type=int)\nparser.add_argument('--model_dir', default='./output/fish_sim_net_399.pth', type=str)\nargs = parser.parse_args()\n\nif __name__ == '__main__':\n    writer = FFMpegWriter(fps=60)\n    render = PltRender(call_back=writer.grab_frame)\n\n    net = FishNet(in_ch=args.n_states, out_ch=args.n_actions)\n    env = Fishing_sim(step_tick=args.step_tick, drawer=render, stop_tick=10000)\n\n    net.load_state_dict(torch.load(args.model_dir))\n\n    net.eval()\n    state = env.reset()\n    with writer.saving(render.fig, 'out.mp4', 100):\n        for i in range(2000):\n            env.render()\n\n            state = torch.FloatTensor(state).unsqueeze(0)\n            action = net(state)\n            action = torch.argmax(action, dim=1).numpy()\n            state, reward, done = env.step(action)\n            if done:\n                break"
        },
        {
          "name": "train.py",
          "type": "blob",
          "size": 2.3359375,
          "content": "from fisher.agent import DQN\nfrom fisher.models import FishNet\nfrom fisher.environment import *\nimport torch\nimport argparse\nimport os\nimport keyboard\nimport winsound\n\nparser = argparse.ArgumentParser(description='Train Genshin finsing with DQN')\nparser.add_argument('--batch_size', default=32, type=int)\nparser.add_argument('--n_states', default=3, type=int)\nparser.add_argument('--n_actions', default=2, type=int)\nparser.add_argument('--step_tick', default=12, type=int)\nparser.add_argument('--n_episode', default=400, type=int)\nparser.add_argument('--save_dir', default='./output', type=str)\nparser.add_argument('--resume', default='./weights/fish_sim_net_399.pth', type=str)\nargs = parser.parse_args()\n\nif not os.path.exists(args.save_dir):\n    os.makedirs(args.save_dir)\n\nnet = FishNet(in_ch=args.n_states, out_ch=args.n_actions)\nif args.resume:\n    net.load_state_dict(torch.load(args.resume))\n\nagent = DQN(net, args.batch_size, args.n_states, args.n_actions, memory_capacity=1000)\n#env = Fishing_sim(step_tick=args.step_tick)\nenv = Fishing(delay=0.1, max_step=150)\n\nif __name__ == '__main__':\n    # Start training\n    print(\"\\nCollecting experience...\")\n    net.train()\n    for i_episode in range(args.n_episode):\n        winsound.Beep(500, 500)\n        keyboard.wait('r')\n        # play 400 episodes of cartpole game\n        s = env.reset()\n        ep_r = 0\n        while True:\n            if i_episode>200 and i_episode%20==0:\n                env.render()\n            # take action based on the current state\n            a = agent.choose_action(s)\n            # obtain the reward and next state and some other information\n            s_, r, done = env.step(a)\n\n            # store the transitions of states\n            agent.store_transition(s, a, r, s_, int(done))\n\n            ep_r += r\n            # if the experience repaly buffer is filled, DQN begins to learn or update\n            # its parameters.\n            if agent.memory_counter > agent.memory_capacity:\n                agent.train_step()\n                if done:\n                    print('Ep: ', i_episode, ' |', 'Ep_r: ', round(ep_r, 2))\n\n            if done:\n                # if game is over, then skip the while loop.\n                break\n            # use next state to update the current state.\n            s = s_\n        torch.save(net.state_dict(), os.path.join(args.save_dir, f'fish_ys_net_{i_episode}.pth'))"
        },
        {
          "name": "train_mf.py",
          "type": "blob",
          "size": 5.7412109375,
          "content": "from fisher.agent import DQN\nfrom fisher.models import FishNet, MoveFishNet\nfrom fisher.environment import *\nimport torch\nimport argparse\nimport os\nimport keyboard\nimport winsound\nfrom loguru import logger\nfrom fisher.predictor import *\nfrom yolox.exp import get_exp\n\ndef make_parser():\n    parser = argparse.ArgumentParser(\"YOLOX Demo!\")\n    parser.add_argument(\"demo\", default=\"image\", help=\"demo type, eg. image, video and webcam\")\n    parser.add_argument(\"-expn\", \"--experiment-name\", type=str, default=None)\n    parser.add_argument(\"-n\", \"--name\", type=str, default=None, help=\"model name\")\n    parser.add_argument(\"--path\", default=\"./assets/dog.jpg\", help=\"path to images or video\")\n\n    # exp file\n    parser.add_argument(\n        \"-f\",\n        \"--exp_file\",\n        default=None,\n        type=str,\n        help=\"pls input your experiment description file\",\n    )\n    parser.add_argument(\"-c\", \"--ckpt\", default=None, type=str, help=\"ckpt for eval\")\n    parser.add_argument(\n        \"--device\",\n        default=\"cpu\",\n        type=str,\n        help=\"device to run our model, can either be cpu or gpu\",\n    )\n    parser.add_argument(\"--conf\", default=0.3, type=float, help=\"test conf\")\n    parser.add_argument(\"--nms\", default=0.3, type=float, help=\"test nms threshold\")\n    parser.add_argument(\"--tsize\", default=None, type=int, help=\"test img size\")\n    parser.add_argument(\n        \"--fp16\",\n        dest=\"fp16\",\n        default=False,\n        action=\"store_true\",\n        help=\"Adopting mix precision evaluating.\",\n    )\n    parser.add_argument(\n        \"--legacy\",\n        dest=\"legacy\",\n        default=False,\n        action=\"store_true\",\n        help=\"To be compatible with older versions\",\n    )\n    parser.add_argument(\n        \"--fuse\",\n        dest=\"fuse\",\n        default=False,\n        action=\"store_true\",\n        help=\"Fuse conv and bn for testing.\",\n    )\n    parser.add_argument(\n        \"--trt\",\n        dest=\"trt\",\n        default=False,\n        action=\"store_true\",\n        help=\"Using TensorRT model for testing.\",\n    )\n\n    # DQN args\n    parser.add_argument('--batch_size', default=32, type=int)\n    parser.add_argument('--n_states', default=8, type=int)\n    parser.add_argument('--n_actions', default=3, type=int)\n    parser.add_argument('--n_episode', default=400, type=int)\n    parser.add_argument('--save_dir', default='./output', type=str)\n    parser.add_argument('--resume', default=None, type=str)\n\n    return parser\n\ndef get_predictor(exp, args):\n    if not args.experiment_name:\n        args.experiment_name = exp.exp_name\n\n    if args.trt:\n        args.device = \"gpu\"\n\n    logger.info(\"Args: {}\".format(args))\n\n    if args.conf is not None:\n        exp.test_conf = args.conf\n    if args.nms is not None:\n        exp.nmsthre = args.nms\n    if args.tsize is not None:\n        exp.test_size = (args.tsize, args.tsize)\n\n    model = exp.get_model()\n\n    if args.device == \"gpu\":\n        model.cuda()\n        if args.fp16:\n            model.half()  # to FP16\n    model.eval()\n\n    if not args.trt:\n        if args.ckpt is None:\n            ckpt_file = os.path.join(file_name, \"best_ckpt.pth\")\n        else:\n            ckpt_file = args.ckpt\n        logger.info(\"loading checkpoint\")\n        ckpt = torch.load(ckpt_file, map_location=\"cpu\")\n        # load the model state dict\n        model.load_state_dict(ckpt[\"model\"])\n        logger.info(\"loaded checkpoint done.\")\n\n    if args.trt:\n        assert not args.fuse, \"TensorRT model is not support model fusing!\"\n        if args.ckpt is None:\n            trt_file = os.path.join(file_name, \"model_trt.pth\")\n        else:\n            trt_file = args.ckpt\n        assert os.path.exists(\n            trt_file\n        ), \"TensorRT model is not found!\\n Run python3 tools/trt.py first!\"\n        model.head.decode_in_inference = False\n        decoder = model.head.decode_outputs\n        logger.info(\"Using TensorRT to inference\")\n    else:\n        trt_file = None\n        decoder = None\n\n    return Predictor(model, exp, FISH_CLASSES, trt_file, decoder, args.device, args.fp16, args.legacy)\n\nargs = make_parser().parse_args()\nexp = get_exp(args.exp_file, args.name)\n\npredictor = get_predictor(exp, args)\n\nif not os.path.exists(args.save_dir):\n    os.makedirs(args.save_dir)\n\nnet = MoveFishNet(in_ch=args.n_states, out_ch=args.n_actions)\nif args.resume:\n    net.load_state_dict(torch.load(args.resume))\n\nagent = DQN(net, args.batch_size, args.n_states, args.n_actions, memory_capacity=1000, reg=True)\nenv = FishMove(predictor)\n\n#python train_mf.py image -f yolox/exp/yolox_tiny_fish.py -c weights/best_tiny3.pth --conf 0.25 --nms 0.45 --tsize 640 --device gpu\nif __name__ == '__main__':\n    # Start training\n    print(\"\\nCollecting experience...\")\n    net.train()\n    for i_episode in range(args.n_episode):\n        winsound.Beep(500, 500)\n        keyboard.wait('r')\n        # play 400 episodes of cartpole game\n        s = env.reset()\n        ep_r = 0\n        while True:\n            # take action based on the current state\n            a = agent.choose_action(s)\n            # obtain the reward and next state and some other information\n            s_, r, done = env.step(a)\n\n            # store the transitions of states\n            agent.store_transition(s, a, r, s_, int(done))\n\n            ep_r += r\n            # if the experience repaly buffer is filled, DQN begins to learn or update\n            # its parameters.\n            if agent.memory_counter > agent.memory_capacity:\n                agent.train_step()\n                if done:\n                    print('Ep: ', i_episode, ' |', 'Ep_r: ', round(ep_r, 2))\n\n            if done:\n                # if game is over, then skip the while loop.\n                break\n            # use next state to update the current state.\n            s = s_\n        torch.save(net.state_dict(), os.path.join(args.save_dir, f'fish_move_net_{i_episode}.pth'))"
        },
        {
          "name": "train_sim.py",
          "type": "blob",
          "size": 2.26171875,
          "content": "from fisher.agent import DQN\nfrom fisher.models import FishNet\nfrom fisher.environment import *\nimport torch\nimport argparse\nimport os\nfrom utils.render import *\n\nparser = argparse.ArgumentParser(description='Train Genshin finsing simulation with DQN')\nparser.add_argument('--batch_size', default=32, type=int)\nparser.add_argument('--n_states', default=3, type=int)\nparser.add_argument('--n_actions', default=2, type=int)\nparser.add_argument('--step_tick', default=12, type=int)\nparser.add_argument('--n_episode', default=400, type=int)\nparser.add_argument('--save_dir', default='./output', type=str)\nparser.add_argument('--resume', default=None, type=str)\nargs = parser.parse_args()\n\nif not os.path.exists(args.save_dir):\n    os.makedirs(args.save_dir)\n\nnet = FishNet(in_ch=args.n_states, out_ch=args.n_actions)\nif args.resume:\n    net.load_state_dict(torch.load(args.resume))\n\nagent = DQN(net, args.batch_size, args.n_states, args.n_actions, memory_capacity=2000)\nenv = Fishing_sim(step_tick=args.step_tick, drawer=PltRender())\n\nif __name__ == '__main__':\n    # Start training\n    print(\"\\nCollecting experience...\")\n    net.train()\n    for i_episode in range(args.n_episode):\n        #keyboard.wait('r')\n        # play 400 episodes of cartpole game\n        s = env.reset()\n        ep_r = 0\n        while True:\n            if i_episode>200 and i_episode%20==0:\n                env.render()\n            # take action based on the current state\n            a = agent.choose_action(s)\n            # obtain the reward and next state and some other information\n            s_, r, done = env.step(a)\n\n            # store the transitions of states\n            agent.store_transition(s, a, r, s_, int(done))\n\n            ep_r += r\n            # if the experience repaly buffer is filled, DQN begins to learn or update\n            # its parameters.\n            if agent.memory_counter > agent.memory_capacity:\n                agent.train_step()\n                if done:\n                    print('Ep: ', i_episode, ' |', 'Ep_r: ', round(ep_r, 2))\n\n            if done:\n                # if game is over, then skip the while loop.\n                break\n            # use next state to update the current state.\n            s = s_\n    torch.save(net.state_dict(), os.path.join(args.save_dir, f'fish_sim_net_{i_episode}.pth'))"
        },
        {
          "name": "utils",
          "type": "tree",
          "content": null
        },
        {
          "name": "yolox",
          "type": "tree",
          "content": null
        },
        {
          "name": "yolox_tools",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}