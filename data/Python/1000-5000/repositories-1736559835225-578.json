{
  "metadata": {
    "timestamp": 1736559835225,
    "page": 578,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjU4MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "richzhang/PerceptualSimilarity",
      "stars": 3752,
      "defaultBranch": "master",
      "files": [
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.0205078125,
          "content": "*.pyc\n\ncheckpoints/*\n"
        },
        {
          "name": "Dockerfile",
          "type": "blob",
          "size": 1.6318359375,
          "content": "FROM nvidia/cuda:9.0-base-ubuntu16.04\n\nLABEL maintainer=\"Seyoung Park <seyoung.arts.park@protonmail.com>\"\n\n# This Dockerfile is forked from Tensorflow Dockerfile\n\n# Pick up some PyTorch gpu dependencies\nRUN apt-get update && apt-get install -y --no-install-recommends \\\n        build-essential \\\n        cuda-command-line-tools-9-0 \\\n        cuda-cublas-9-0 \\\n        cuda-cufft-9-0 \\\n        cuda-curand-9-0 \\\n        cuda-cusolver-9-0 \\\n        cuda-cusparse-9-0 \\\n        curl \\\n        libcudnn7=7.1.4.18-1+cuda9.0 \\\n        libfreetype6-dev \\\n        libhdf5-serial-dev \\\n        libpng12-dev \\\n        libzmq3-dev \\\n        pkg-config \\\n        python \\\n        python-dev \\\n        rsync \\\n        software-properties-common \\\n        unzip \\\n        && \\\n    apt-get clean && \\\n    rm -rf /var/lib/apt/lists/*\n\n\n# Install miniconda\nRUN apt-get update && apt-get install -y --no-install-recommends \\\n        wget && \\\n    MINICONDA=\"Miniconda3-latest-Linux-x86_64.sh\" && \\\n    wget --quiet https://repo.continuum.io/miniconda/$MINICONDA && \\\n    bash $MINICONDA -b -p /miniconda && \\\n    rm -f $MINICONDA\nENV PATH /miniconda/bin:$PATH\n\n# Install PyTorch\nRUN conda update -n base conda && \\ \n    conda install pytorch torchvision cuda90 -c pytorch\n\n# Install PerceptualSimilarity dependencies\nRUN conda install numpy scipy jupyter matplotlib && \\\n    conda install -c conda-forge scikit-image && \\\n    apt-get install -y python-qt4 && \\\n    pip install opencv-python\n\n# For CUDA profiling, TensorFlow requires CUPTI. Maybe PyTorch needs this too.\nENV LD_LIBRARY_PATH /usr/local/cuda/extras/CUPTI/lib64:$LD_LIBRARY_PATH\n\n# IPython\nEXPOSE 8888\n\nWORKDIR \"/notebooks\"\n\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 1.3251953125,
          "content": "Copyright (c) 2018, Richard Zhang, Phillip Isola, Alexei A. Efros, Eli Shechtman, Oliver Wang\nAll rights reserved.\n\nRedistribution and use in source and binary forms, with or without\nmodification, are permitted provided that the following conditions are met:\n\n* Redistributions of source code must retain the above copyright notice, this\n  list of conditions and the following disclaimer.\n\n* Redistributions in binary form must reproduce the above copyright notice,\n  this list of conditions and the following disclaimer in the documentation\n  and/or other materials provided with the distribution.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\nAND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\nIMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\nDISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE\nFOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\nDAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\nSERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\nCAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,\nOR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\nOF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 10.0322265625,
          "content": "\n## Perceptual Similarity Metric and Dataset [[Project Page]](http://richzhang.github.io/PerceptualSimilarity/)\n\n**The Unreasonable Effectiveness of Deep Features as a Perceptual Metric**  \n[Richard Zhang](https://richzhang.github.io/), [Phillip Isola](http://web.mit.edu/phillipi/), [Alexei A. Efros](http://www.eecs.berkeley.edu/~efros/), [Eli Shechtman](https://research.adobe.com/person/eli-shechtman/), [Oliver Wang](http://www.oliverwang.info/). In [CVPR](https://arxiv.org/abs/1801.03924), 2018.\n\n<img src='https://richzhang.github.io/PerceptualSimilarity/index_files/fig1_v2.jpg' width=1200>\n\n### Quick start\n\nRun `pip install lpips`. The following Python code is all you need.\n\n```python\nimport lpips\nloss_fn_alex = lpips.LPIPS(net='alex') # best forward scores\nloss_fn_vgg = lpips.LPIPS(net='vgg') # closer to \"traditional\" perceptual loss, when used for optimization\n\nimport torch\nimg0 = torch.zeros(1,3,64,64) # image should be RGB, IMPORTANT: normalized to [-1,1]\nimg1 = torch.zeros(1,3,64,64)\nd = loss_fn_alex(img0, img1)\n```\n\nMore thorough information about variants is below. This repository contains our **perceptual metric (LPIPS)** and **dataset (BAPPS)**. It can also be used as a \"perceptual loss\". This uses PyTorch; a Tensorflow alternative is [here](https://github.com/alexlee-gk/lpips-tensorflow).\n\n\n**Table of Contents**<br>\n1. [Learned Perceptual Image Patch Similarity (LPIPS) metric](#1-learned-perceptual-image-patch-similarity-lpips-metric)<br>\n   a. [Basic Usage](#a-basic-usage) If you just want to run the metric through command line, this is all you need.<br>\n   b. [\"Perceptual Loss\" usage](#b-backpropping-through-the-metric)<br>\n   c. [About the metric](#c-about-the-metric)<br>\n2. [Berkeley-Adobe Perceptual Patch Similarity (BAPPS) dataset](#2-berkeley-adobe-perceptual-patch-similarity-bapps-dataset)<br>\n   a. [Download](#a-downloading-the-dataset)<br>\n   b. [Evaluation](#b-evaluating-a-perceptual-similarity-metric-on-a-dataset)<br>\n   c. [About the dataset](#c-about-the-dataset)<br>\n   d. [Train the metric using the dataset](#d-using-the-dataset-to-train-the-metric)<br>\n\n## (0) Dependencies/Setup\n\n### Installation\n- Install PyTorch 1.0+ and torchvision fom http://pytorch.org\n\n```bash\npip install -r requirements.txt\n```\n- Clone this repo:\n```bash\ngit clone https://github.com/richzhang/PerceptualSimilarity\ncd PerceptualSimilarity\n```\n\n## (1) Learned Perceptual Image Patch Similarity (LPIPS) metric\n\nEvaluate the distance between image patches. **Higher means further/more different. Lower means more similar.**\n\n### (A) Basic Usage\n\n#### (A.I) Line commands\n\nExample scripts to take the distance between 2 specific images, all corresponding pairs of images in 2 directories, or all pairs of images within a directory:\n\n```\npython lpips_2imgs.py -p0 imgs/ex_ref.png -p1 imgs/ex_p0.png --use_gpu\npython lpips_2dirs.py -d0 imgs/ex_dir0 -d1 imgs/ex_dir1 -o imgs/example_dists.txt --use_gpu\npython lpips_1dir_allpairs.py -d imgs/ex_dir_pair -o imgs/example_dists_pair.txt --use_gpu\n```\n\n#### (A.II) Python code\n\nFile [test_network.py](test_network.py) shows example usage. This snippet is all you really need.\n\n```python\nimport lpips\nloss_fn = lpips.LPIPS(net='alex')\nd = loss_fn.forward(im0,im1)\n```\n\nVariables ```im0, im1``` is a PyTorch Tensor/Variable with shape ```Nx3xHxW``` (```N``` patches of size ```HxW```, RGB images scaled in `[-1,+1]`). This returns `d`, a length `N` Tensor/Variable.\n\nRun `python test_network.py` to take the distance between example reference image [`ex_ref.png`](imgs/ex_ref.png) to distorted images [`ex_p0.png`](./imgs/ex_p0.png) and [`ex_p1.png`](imgs/ex_p1.png). Before running it - which do you think *should* be closer?\n\n**Some Options** By default in `model.initialize`:\n- By default, `net='alex'`. Network `alex` is fastest, performs the best (as a forward metric), and is the default. For backpropping, `net='vgg'` loss is closer to the traditional \"perceptual loss\".\n- By default, `lpips=True`. This adds a linear calibration on top of intermediate features in the net. Set this to `lpips=False` to equally weight all the features.\n\n### (B) Backpropping through the metric\n\nFile [`lpips_loss.py`](lpips_loss.py) shows how to iteratively optimize using the metric. Run `python lpips_loss.py` for a demo. The code can also be used to implement vanilla VGG loss, without our learned weights.\n\n### (C) About the metric\n\n**Higher means further/more different. Lower means more similar.**\n\nWe found that deep network activations work surprisingly well as a perceptual similarity metric. This was true across network architectures (SqueezeNet [2.8 MB], AlexNet [9.1 MB], and VGG [58.9 MB] provided similar scores) and supervisory signals (unsupervised, self-supervised, and supervised all perform strongly). We slightly improved scores by linearly \"calibrating\" networks - adding a linear layer on top of off-the-shelf classification networks. We provide 3 variants, using linear layers on top of the SqueezeNet, AlexNet (default), and VGG networks.\n\nIf you use LPIPS in your publication, please specify which version you are using. The current version is 0.1. You can set `version='0.0'` for the initial release.\n\n## (2) Berkeley Adobe Perceptual Patch Similarity (BAPPS) dataset\n\n### (A) Downloading the dataset\n\nRun `bash ./scripts/download_dataset.sh` to download and unzip the dataset into directory `./dataset`. It takes [6.6 GB] total. Alternatively, run `bash ./scripts/download_dataset_valonly.sh` to only download the validation set [1.3 GB].\n- 2AFC train [5.3 GB]\n- 2AFC val [1.1 GB]\n- JND val [0.2 GB]  \n\n### (B) Evaluating a perceptual similarity metric on a dataset\n\nScript `test_dataset_model.py` evaluates a perceptual model on a subset of the dataset.\n\n**Dataset flags**\n- `--dataset_mode`: `2afc` or `jnd`, which type of perceptual judgment to evaluate\n- `--datasets`: list the datasets to evaluate\n    - if `--dataset_mode 2afc`: choices are [`train/traditional`, `train/cnn`, `val/traditional`, `val/cnn`, `val/superres`, `val/deblur`, `val/color`, `val/frameinterp`]\n    - if `--dataset_mode jnd`: choices are [`val/traditional`, `val/cnn`]\n    \n**Perceptual similarity model flags**\n- `--model`: perceptual similarity model to use\n    - `lpips` for our LPIPS learned similarity model (linear network on top of internal activations of pretrained network)\n    - `baseline` for a classification network (uncalibrated with all layers averaged)\n    - `l2` for Euclidean distance\n    - `ssim` for Structured Similarity Image Metric\n- `--net`: [`squeeze`,`alex`,`vgg`] for the `net-lin` and `net` models; ignored for `l2` and `ssim` models\n- `--colorspace`: choices are [`Lab`,`RGB`], used for the `l2` and `ssim` models; ignored for `net-lin` and `net` models\n\n**Misc flags**\n- `--batch_size`: evaluation batch size (will default to 1)\n- `--use_gpu`: turn on this flag for GPU usage\n\nAn example usage is as follows: `python ./test_dataset_model.py --dataset_mode 2afc --datasets val/traditional val/cnn --model lpips --net alex --use_gpu --batch_size 50`. This would evaluate our model on the \"traditional\" and \"cnn\" validation datasets.\n\n### (C) About the dataset\n\nThe dataset contains two types of perceptual judgements: **Two Alternative Forced Choice (2AFC)** and **Just Noticeable Differences (JND)**.\n\n**(1) 2AFC** Evaluators were given a patch triplet (1 reference + 2 distorted). They were asked to select which of the distorted was \"closer\" to the reference.\n\nTraining sets contain 2 judgments/triplet.\n- `train/traditional` [56.6k triplets]\n- `train/cnn` [38.1k triplets]\n- `train/mix` [56.6k triplets]\n\nValidation sets contain 5 judgments/triplet.\n- `val/traditional` [4.7k triplets]\n- `val/cnn` [4.7k triplets]\n- `val/superres` [10.9k triplets]\n- `val/deblur` [9.4k triplets]\n- `val/color` [4.7k triplets]\n- `val/frameinterp` [1.9k triplets]\n\nEach 2AFC subdirectory contains the following folders:\n- `ref`: original reference patches\n- `p0,p1`: two distorted patches\n- `judge`: human judgments - 0 if all preferred p0, 1 if all humans preferred p1\n\n**(2) JND** Evaluators were presented with two patches - a reference and a distorted - for a limited time. They were asked if the patches were the same (identically) or different. \n\nEach set contains 3 human evaluations/example.\n- `val/traditional` [4.8k pairs]\n- `val/cnn` [4.8k pairs]\n\nEach JND subdirectory contains the following folders:\n- `p0,p1`: two patches\n- `same`: human judgments: 0 if all humans thought patches were different, 1 if all humans thought patches were same\n\n### (D) Using the dataset to train the metric\n\nSee script `train_test_metric.sh` for an example of training and testing the metric. The script will train a model on the full training set for 10 epochs, and then test the learned metric on all of the validation sets. The numbers should roughly match the **Alex - lin** row in Table 5 in the [paper](https://arxiv.org/abs/1801.03924). The code supports training a linear layer on top of an existing representation. Training will add a subdirectory in the `checkpoints` directory.\n\nYou can also train \"scratch\" and \"tune\" versions by running `train_test_metric_scratch.sh` and `train_test_metric_tune.sh`, respectively. \n\n## Citation\n\nIf you find this repository useful for your research, please use the following.\n\n```\n@inproceedings{zhang2018perceptual,\n  title={The Unreasonable Effectiveness of Deep Features as a Perceptual Metric},\n  author={Zhang, Richard and Isola, Phillip and Efros, Alexei A and Shechtman, Eli and Wang, Oliver},\n  booktitle={CVPR},\n  year={2018}\n}\n```\n\n## Acknowledgements\n\nThis repository borrows partially from the [pytorch-CycleGAN-and-pix2pix](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix) repository. The average precision (AP) code is borrowed from the [py-faster-rcnn](https://github.com/rbgirshick/py-faster-rcnn/blob/master/lib/datasets/voc_eval.py) repository. [Angjoo Kanazawa](https://github.com/akanazawa), [Connelly Barnes](http://www.connellybarnes.com/work/), [Gaurav Mittal](https://github.com/g1910), [wilhelmhb](https://github.com/wilhelmhb), [Filippo Mameli](https://github.com/mameli), [SuperShinyEyes](https://github.com/SuperShinyEyes), [Minyoung Huh](http://people.csail.mit.edu/minhuh/) helped to improve the codebase.\n"
        },
        {
          "name": "data",
          "type": "tree",
          "content": null
        },
        {
          "name": "imgs",
          "type": "tree",
          "content": null
        },
        {
          "name": "lpips",
          "type": "tree",
          "content": null
        },
        {
          "name": "lpips_1dir_allpairs.py",
          "type": "blob",
          "size": 1.6845703125,
          "content": "import argparse\nimport os\nimport lpips\nimport numpy as np\n\nparser = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter)\nparser.add_argument('-d','--dir', type=str, default='./imgs/ex_dir_pair')\nparser.add_argument('-o','--out', type=str, default='./imgs/example_dists.txt')\nparser.add_argument('-v','--version', type=str, default='0.1')\nparser.add_argument('--all-pairs', action='store_true', help='turn on to test all N(N-1)/2 pairs, leave off to just do consecutive pairs (N-1)')\nparser.add_argument('-N', type=int, default=None)\nparser.add_argument('--use_gpu', action='store_true', help='turn on flag to use GPU')\n\nopt = parser.parse_args()\n\n## Initializing the model\nloss_fn = lpips.LPIPS(net='alex',version=opt.version)\nif(opt.use_gpu):\n\tloss_fn.cuda()\n\n# crawl directories\nf = open(opt.out,'w')\nfiles = os.listdir(opt.dir)\nif(opt.N is not None):\n\tfiles = files[:opt.N]\nF = len(files)\n\ndists = []\nfor (ff,file) in enumerate(files[:-1]):\n\timg0 = lpips.im2tensor(lpips.load_image(os.path.join(opt.dir,file))) # RGB image from [-1,1]\n\tif(opt.use_gpu):\n\t\timg0 = img0.cuda()\n\n\tif(opt.all_pairs):\n\t\tfiles1 = files[ff+1:]\n\telse:\n\t\tfiles1 = [files[ff+1],]\n\n\tfor file1 in files1:\n\t\timg1 = lpips.im2tensor(lpips.load_image(os.path.join(opt.dir,file1)))\n\n\t\tif(opt.use_gpu):\n\t\t\timg1 = img1.cuda()\n\n\t\t# Compute distance\n\t\tdist01 = loss_fn.forward(img0,img1)\n\t\tprint('(%s,%s): %.3f'%(file,file1,dist01))\n\t\tf.writelines('(%s,%s): %.6f\\n'%(file,file1,dist01))\n\n\t\tdists.append(dist01.item())\n\navg_dist = np.mean(np.array(dists))\nstderr_dist = np.std(np.array(dists))/np.sqrt(len(dists))\n\nprint('Avg: %.5f +/- %.5f'%(avg_dist,stderr_dist))\nf.writelines('Avg: %.6f +/- %.6f'%(avg_dist,stderr_dist))\n\nf.close()\n"
        },
        {
          "name": "lpips_2dirs.py",
          "type": "blob",
          "size": 1.1533203125,
          "content": "import argparse\nimport os\nimport lpips\n\nparser = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter)\nparser.add_argument('-d0','--dir0', type=str, default='./imgs/ex_dir0')\nparser.add_argument('-d1','--dir1', type=str, default='./imgs/ex_dir1')\nparser.add_argument('-o','--out', type=str, default='./imgs/example_dists.txt')\nparser.add_argument('-v','--version', type=str, default='0.1')\nparser.add_argument('--use_gpu', action='store_true', help='turn on flag to use GPU')\n\nopt = parser.parse_args()\n\n## Initializing the model\nloss_fn = lpips.LPIPS(net='alex',version=opt.version)\nif(opt.use_gpu):\n\tloss_fn.cuda()\n\n# crawl directories\nf = open(opt.out,'w')\nfiles = os.listdir(opt.dir0)\n\nfor file in files:\n\tif(os.path.exists(os.path.join(opt.dir1,file))):\n\t\t# Load images\n\t\timg0 = lpips.im2tensor(lpips.load_image(os.path.join(opt.dir0,file))) # RGB image from [-1,1]\n\t\timg1 = lpips.im2tensor(lpips.load_image(os.path.join(opt.dir1,file)))\n\n\t\tif(opt.use_gpu):\n\t\t\timg0 = img0.cuda()\n\t\t\timg1 = img1.cuda()\n\n\t\t# Compute distance\n\t\tdist01 = loss_fn.forward(img0,img1)\n\t\tprint('%s: %.3f'%(file,dist01))\n\t\tf.writelines('%s: %.6f\\n'%(file,dist01))\n\nf.close()\n"
        },
        {
          "name": "lpips_2imgs.py",
          "type": "blob",
          "size": 0.830078125,
          "content": "import argparse\nimport lpips\n\nparser = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter)\nparser.add_argument('-p0','--path0', type=str, default='./imgs/ex_ref.png')\nparser.add_argument('-p1','--path1', type=str, default='./imgs/ex_p0.png')\nparser.add_argument('-v','--version', type=str, default='0.1')\nparser.add_argument('--use_gpu', action='store_true', help='turn on flag to use GPU')\n\nopt = parser.parse_args()\n\n## Initializing the model\nloss_fn = lpips.LPIPS(net='alex',version=opt.version)\n\nif(opt.use_gpu):\n\tloss_fn.cuda()\n\n# Load images\nimg0 = lpips.im2tensor(lpips.load_image(opt.path0)) # RGB image from [-1,1]\nimg1 = lpips.im2tensor(lpips.load_image(opt.path1))\n\nif(opt.use_gpu):\n\timg0 = img0.cuda()\n\timg1 = img1.cuda()\n\n# Compute distance\ndist01 = loss_fn.forward(img0, img1)\nprint('Distance: %.3f'%dist01)\n"
        },
        {
          "name": "lpips_loss.py",
          "type": "blob",
          "size": 1.6357421875,
          "content": "import numpy as np\nimport torch\nfrom torch.autograd import Variable\nimport matplotlib.pyplot as plt\nimport argparse\nimport lpips\n\nparser = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter)\nparser.add_argument('--ref_path', type=str, default='./imgs/ex_ref.png')\nparser.add_argument('--pred_path', type=str, default='./imgs/ex_p1.png')\nparser.add_argument('--use_gpu', action='store_true', help='turn on flag to use GPU')\n\nopt = parser.parse_args()\n\nloss_fn = lpips.LPIPS(net='vgg')\nif(opt.use_gpu):\n    loss_fn.cuda()\n\nref = lpips.im2tensor(lpips.load_image(opt.ref_path))\npred = Variable(lpips.im2tensor(lpips.load_image(opt.pred_path)), requires_grad=True)\nif(opt.use_gpu):\n    with torch.no_grad():\n        ref = ref.cuda()\n        pred = pred.cuda()\n\noptimizer = torch.optim.Adam([pred,], lr=1e-3, betas=(0.9, 0.999))\n\nplt.ion()\nfig = plt.figure(1)\nax = fig.add_subplot(131)\nax.imshow(lpips.tensor2im(ref))\nax.set_title('target')\nax = fig.add_subplot(133)\nax.imshow(lpips.tensor2im(pred.data))\nax.set_title('initialization')\n\nfor i in range(1000):\n    dist = loss_fn.forward(pred, ref)\n    optimizer.zero_grad()\n    dist.backward()\n    optimizer.step()\n    pred.data = torch.clamp(pred.data, -1, 1)\n    \n    if i % 10 == 0:\n        print('iter %d, dist %.3g' % (i, dist.view(-1).data.cpu().numpy()[0]))\n        pred.data = torch.clamp(pred.data, -1, 1)\n        pred_img = lpips.tensor2im(pred.data)\n\n        ax = fig.add_subplot(132)            \n        ax.imshow(pred_img)\n        ax.set_title('iter %d, dist %.3f' % (i, dist.view(-1).data.cpu().numpy()[0]))\n        plt.pause(5e-2)\n        # plt.imsave('imgs_saved/%04d.jpg'%i,pred_img)\n\n\n"
        },
        {
          "name": "requirements.txt",
          "type": "blob",
          "size": 0.1376953125,
          "content": "torch>=0.4.0\ntorchvision>=0.2.1\nnumpy>=1.14.3\nscipy>=1.0.1\nscikit-image>=0.13.0\nopencv-python>=2.4.11\nmatplotlib>=1.5.1\ntqdm>=4.28.1\njupyter\n"
        },
        {
          "name": "scripts",
          "type": "tree",
          "content": null
        },
        {
          "name": "setup.py",
          "type": "blob",
          "size": 0.8125,
          "content": "\nimport setuptools\nwith open(\"README.md\", \"r\") as fh:\n    long_description = fh.read()\nsetuptools.setup(\n     name='lpips',  \n     version='0.1.4',\n     author=\"Richard Zhang\",\n     author_email=\"rizhang@adobe.com\",\n     description=\"LPIPS Similarity metric\",\n     long_description=long_description,\n     long_description_content_type=\"text/markdown\",\n     url=\"https://github.com/richzhang/PerceptualSimilarity\",\n     packages=['lpips'],\n     package_data={'lpips': ['weights/v0.0/*.pth','weights/v0.1/*.pth']},\n     include_package_data=True,\n     install_requires=[\"torch>=0.4.0\", \"torchvision>=0.2.1\", \"numpy>=1.14.3\", \"scipy>=1.0.1\", \"tqdm>=4.28.1\"],\n     classifiers=[\n         \"Programming Language :: Python :: 3\",\n         \"License :: OSI Approved :: BSD License\",\n         \"Operating System :: OS Independent\",\n     ],\n )\n"
        },
        {
          "name": "test_dataset_model.py",
          "type": "blob",
          "size": 3.0234375,
          "content": "import numpy as np\nimport lpips\nfrom data import data_loader as dl\nimport argparse\nfrom IPython import embed\n\nparser = argparse.ArgumentParser()\nparser.add_argument('--dataset_mode', type=str, default='2afc', help='[2afc,jnd]')\nparser.add_argument('--datasets', type=str, nargs='+', default=['val/traditional','val/cnn','val/superres','val/deblur','val/color','val/frameinterp'], help='datasets to test - for jnd mode: [val/traditional],[val/cnn]; for 2afc mode: [train/traditional],[train/cnn],[train/mix],[val/traditional],[val/cnn],[val/color],[val/deblur],[val/frameinterp],[val/superres]')\nparser.add_argument('--model', type=str, default='lpips', help='distance model type [lpips] for linearly calibrated net, [baseline] for off-the-shelf network, [l2] for euclidean distance, [ssim] for Structured Similarity Image Metric')\nparser.add_argument('--net', type=str, default='alex', help='[squeeze], [alex], or [vgg] for network architectures')\nparser.add_argument('--colorspace', type=str, default='Lab', help='[Lab] or [RGB] for colorspace to use for l2, ssim model types')\nparser.add_argument('--batch_size', type=int, default=50, help='batch size to test image patches in')\nparser.add_argument('--use_gpu', action='store_true', help='turn on flag to use GPU')\nparser.add_argument('--gpu_ids', type=int, nargs='+', default=[0], help='gpus to use')\nparser.add_argument('--nThreads', type=int, default=4, help='number of threads to use in data loader')\n\nparser.add_argument('--model_path', type=str, default=None, help='location of model, will default to ./weights/v[version]/[net_name].pth')\n\nparser.add_argument('--from_scratch', action='store_true', help='model was initialized from scratch')\nparser.add_argument('--train_trunk', action='store_true', help='model trunk was trained/tuned')\nparser.add_argument('--version', type=str, default='0.1', help='v0.1 is latest, v0.0 was original release')\n\nopt = parser.parse_args()\nif(opt.model in ['l2','ssim']):\n\topt.batch_size = 1\n\n# initialize model\ntrainer = lpips.Trainer()\n# trainer.initialize(model=opt.model,net=opt.net,colorspace=opt.colorspace,model_path=opt.model_path,use_gpu=opt.use_gpu)\ntrainer.initialize(model=opt.model, net=opt.net, colorspace=opt.colorspace, \n\tmodel_path=opt.model_path, use_gpu=opt.use_gpu, pnet_rand=opt.from_scratch, pnet_tune=opt.train_trunk,\n\tversion=opt.version, gpu_ids=opt.gpu_ids)\n\nif(opt.model in ['net-lin','net']):\n\tprint('Testing model [%s]-[%s]'%(opt.model,opt.net))\nelif(opt.model in ['l2','ssim']):\n\tprint('Testing model [%s]-[%s]'%(opt.model,opt.colorspace))\n\n# initialize data loader\nfor dataset in opt.datasets:\n\tdata_loader = dl.CreateDataLoader(dataset,dataset_mode=opt.dataset_mode, batch_size=opt.batch_size, nThreads=opt.nThreads)\n\n\t# evaluate model on data\n\tif(opt.dataset_mode=='2afc'):\n\t\t(score, results_verbose) = lpips.score_2afc_dataset(data_loader, trainer.forward, name=dataset)\n\telif(opt.dataset_mode=='jnd'):\n\t\t(score, results_verbose) = lpips.score_jnd_dataset(data_loader, trainer.forward, name=dataset)\n\n\t# print results\n\tprint('  Dataset [%s]: %.2f'%(dataset,100.*score))\n\n"
        },
        {
          "name": "test_network.py",
          "type": "blob",
          "size": 1.43359375,
          "content": "import torch\nimport lpips\nfrom IPython import embed\n\nuse_gpu = False         # Whether to use GPU\nspatial = True         # Return a spatial map of perceptual distance.\n\n# Linearly calibrated models (LPIPS)\nloss_fn = lpips.LPIPS(net='alex', spatial=spatial) # Can also set net = 'squeeze' or 'vgg'\n# loss_fn = lpips.LPIPS(net='alex', spatial=spatial, lpips=False) # Can also set net = 'squeeze' or 'vgg'\n\nif(use_gpu):\n\tloss_fn.cuda()\n\n## Example usage with dummy tensors\ndummy_im0 = torch.zeros(1,3,64,64) # image should be RGB, normalized to [-1,1]\ndummy_im1 = torch.zeros(1,3,64,64)\nif(use_gpu):\n\tdummy_im0 = dummy_im0.cuda()\n\tdummy_im1 = dummy_im1.cuda()\ndist = loss_fn.forward(dummy_im0,dummy_im1)\n\n## Example usage with images\nex_ref = lpips.im2tensor(lpips.load_image('./imgs/ex_ref.png'))\nex_p0 = lpips.im2tensor(lpips.load_image('./imgs/ex_p0.png'))\nex_p1 = lpips.im2tensor(lpips.load_image('./imgs/ex_p1.png'))\nif(use_gpu):\n\tex_ref = ex_ref.cuda()\n\tex_p0 = ex_p0.cuda()\n\tex_p1 = ex_p1.cuda()\n\nex_d0 = loss_fn.forward(ex_ref,ex_p0)\nex_d1 = loss_fn.forward(ex_ref,ex_p1)\n\nif not spatial:\n    print('Distances: (%.3f, %.3f)'%(ex_d0, ex_d1))\nelse:\n    print('Distances: (%.3f, %.3f)'%(ex_d0.mean(), ex_d1.mean()))            # The mean distance is approximately the same as the non-spatial distance\n    \n    # Visualize a spatially-varying distance map between ex_p0 and ex_ref\n    import pylab\n    pylab.imshow(ex_d0[0,0,...].data.cpu().numpy())\n    pylab.show()\n"
        },
        {
          "name": "train.py",
          "type": "blob",
          "size": 5.3779296875,
          "content": "import torch.backends.cudnn as cudnn\ncudnn.benchmark=False\n\nimport numpy as np\nimport time\nimport os\nimport lpips\nfrom data import data_loader as dl\nimport argparse\nfrom util.visualizer import Visualizer\nfrom IPython import embed\n\nparser = argparse.ArgumentParser()\nparser.add_argument('--datasets', type=str, nargs='+', default=['train/traditional','train/cnn','train/mix'], help='datasets to train on: [train/traditional],[train/cnn],[train/mix],[val/traditional],[val/cnn],[val/color],[val/deblur],[val/frameinterp],[val/superres]')\nparser.add_argument('--model', type=str, default='lpips', help='distance model type [lpips] for linearly calibrated net, [baseline] for off-the-shelf network, [l2] for euclidean distance, [ssim] for Structured Similarity Image Metric')\nparser.add_argument('--net', type=str, default='alex', help='[squeeze], [alex], or [vgg] for network architectures')\nparser.add_argument('--batch_size', type=int, default=50, help='batch size to test image patches in')\nparser.add_argument('--use_gpu', action='store_true', help='turn on flag to use GPU')\nparser.add_argument('--gpu_ids', type=int, nargs='+', default=[0], help='gpus to use')\n\nparser.add_argument('--nThreads', type=int, default=4, help='number of threads to use in data loader')\nparser.add_argument('--nepoch', type=int, default=5, help='# epochs at base learning rate')\nparser.add_argument('--nepoch_decay', type=int, default=5, help='# additional epochs at linearly learning rate')\nparser.add_argument('--display_freq', type=int, default=5000, help='frequency (in instances) of showing training results on screen')\nparser.add_argument('--print_freq', type=int, default=5000, help='frequency (in instances) of showing training results on console')\nparser.add_argument('--save_latest_freq', type=int, default=20000, help='frequency (in instances) of saving the latest results')\nparser.add_argument('--save_epoch_freq', type=int, default=1, help='frequency of saving checkpoints at the end of epochs')\nparser.add_argument('--display_id', type=int, default=0, help='window id of the visdom display, [0] for no displaying')\nparser.add_argument('--display_winsize', type=int, default=256,  help='display window size')\nparser.add_argument('--display_port', type=int, default=8001,  help='visdom display port')\nparser.add_argument('--use_html', action='store_true', help='save off html pages')\nparser.add_argument('--checkpoints_dir', type=str, default='checkpoints', help='checkpoints directory')\nparser.add_argument('--name', type=str, default='tmp', help='directory name for training')\n\nparser.add_argument('--from_scratch', action='store_true', help='model was initialized from scratch')\nparser.add_argument('--train_trunk', action='store_true', help='model trunk was trained/tuned')\nparser.add_argument('--train_plot', action='store_true', help='plot saving')\n\nopt = parser.parse_args()\nopt.save_dir = os.path.join(opt.checkpoints_dir,opt.name)\nif(not os.path.exists(opt.save_dir)):\n    os.mkdir(opt.save_dir)\n\n# initialize model\ntrainer = lpips.Trainer()\ntrainer.initialize(model=opt.model, net=opt.net, use_gpu=opt.use_gpu, is_train=True, \n    pnet_rand=opt.from_scratch, pnet_tune=opt.train_trunk, gpu_ids=opt.gpu_ids)\n\n# load data from all training sets\ndata_loader = dl.CreateDataLoader(opt.datasets,dataset_mode='2afc', batch_size=opt.batch_size, serial_batches=False, nThreads=opt.nThreads)\ndataset = data_loader.load_data()\ndataset_size = len(data_loader)\nD = len(dataset)\nprint('Loading %i instances from'%dataset_size,opt.datasets)\nvisualizer = Visualizer(opt)\n\ntotal_steps = 0\nfid = open(os.path.join(opt.checkpoints_dir,opt.name,'train_log.txt'),'w+')\nfor epoch in range(1, opt.nepoch + opt.nepoch_decay + 1):\n    epoch_start_time = time.time()\n    for i, data in enumerate(dataset):\n        iter_start_time = time.time()\n        total_steps += opt.batch_size\n        epoch_iter = total_steps - dataset_size * (epoch - 1)\n\n        trainer.set_input(data)\n        trainer.optimize_parameters()\n\n        if total_steps % opt.display_freq == 0:\n            visualizer.display_current_results(trainer.get_current_visuals(), epoch)\n\n        if total_steps % opt.print_freq == 0:\n            errors = trainer.get_current_errors()\n            t = (time.time()-iter_start_time)/opt.batch_size\n            t2o = (time.time()-epoch_start_time)/3600.\n            t2 = t2o*D/(i+.0001)\n            visualizer.print_current_errors(epoch, epoch_iter, errors, t, t2=t2, t2o=t2o, fid=fid)\n\n            for key in errors.keys():\n                visualizer.plot_current_errors_save(epoch, float(epoch_iter)/dataset_size, opt, errors, keys=[key,], name=key, to_plot=opt.train_plot)\n\n            if opt.display_id > 0:\n                visualizer.plot_current_errors(epoch, float(epoch_iter)/dataset_size, opt, errors)\n\n        if total_steps % opt.save_latest_freq == 0:\n            print('saving the latest model (epoch %d, total_steps %d)' %\n                  (epoch, total_steps))\n            trainer.save(opt.save_dir, 'latest')\n\n    if epoch % opt.save_epoch_freq == 0:\n        print('saving the model at the end of epoch %d, iters %d' %\n              (epoch, total_steps))\n        trainer.save(opt.save_dir, 'latest')\n        trainer.save(opt.save_dir, epoch)\n\n    print('End of epoch %d / %d \\t Time Taken: %d sec' %\n          (epoch, opt.nepoch + opt.nepoch_decay, time.time() - epoch_start_time))\n\n    if epoch > opt.nepoch:\n        trainer.update_learning_rate(opt.nepoch_decay)\n\n# trainer.save_done(True)\nfid.close()\n"
        },
        {
          "name": "util",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}