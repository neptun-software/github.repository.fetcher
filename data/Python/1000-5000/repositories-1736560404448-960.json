{
  "metadata": {
    "timestamp": 1736560404448,
    "page": 960,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjk3MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "dvlab-research/MGM",
      "stars": 3236,
      "defaultBranch": "main",
      "files": [
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 11.0908203125,
          "content": "                                 Apache License\n                           Version 2.0, January 2004\n                        http://www.apache.org/licenses/\n\n   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n\n   1. Definitions.\n\n      \"License\" shall mean the terms and conditions for use, reproduction,\n      and distribution as defined by Sections 1 through 9 of this document.\n\n      \"Licensor\" shall mean the copyright owner or entity authorized by\n      the copyright owner that is granting the License.\n\n      \"Legal Entity\" shall mean the union of the acting entity and all\n      other entities that control, are controlled by, or are under common\n      control with that entity. For the purposes of this definition,\n      \"control\" means (i) the power, direct or indirect, to cause the\n      direction or management of such entity, whether by contract or\n      otherwise, or (ii) ownership of fifty percent (50%) or more of the\n      outstanding shares, or (iii) beneficial ownership of such entity.\n\n      \"You\" (or \"Your\") shall mean an individual or Legal Entity\n      exercising permissions granted by this License.\n\n      \"Source\" form shall mean the preferred form for making modifications,\n      including but not limited to software source code, documentation\n      source, and configuration files.\n\n      \"Object\" form shall mean any form resulting from mechanical\n      transformation or translation of a Source form, including but\n      not limited to compiled object code, generated documentation,\n      and conversions to other media types.\n\n      \"Work\" shall mean the work of authorship, whether in Source or\n      Object form, made available under the License, as indicated by a\n      copyright notice that is included in or attached to the work\n      (an example is provided in the Appendix below).\n\n      \"Derivative Works\" shall mean any work, whether in Source or Object\n      form, that is based on (or derived from) the Work and for which the\n      editorial revisions, annotations, elaborations, or other modifications\n      represent, as a whole, an original work of authorship. For the purposes\n      of this License, Derivative Works shall not include works that remain\n      separable from, or merely link (or bind by name) to the interfaces of,\n      the Work and Derivative Works thereof.\n\n      \"Contribution\" shall mean any work of authorship, including\n      the original version of the Work and any modifications or additions\n      to that Work or Derivative Works thereof, that is intentionally\n      submitted to Licensor for inclusion in the Work by the copyright owner\n      or by an individual or Legal Entity authorized to submit on behalf of\n      the copyright owner. For the purposes of this definition, \"submitted\"\n      means any form of electronic, verbal, or written communication sent\n      to the Licensor or its representatives, including but not limited to\n      communication on electronic mailing lists, source code control systems,\n      and issue tracking systems that are managed by, or on behalf of, the\n      Licensor for the purpose of discussing and improving the Work, but\n      excluding communication that is conspicuously marked or otherwise\n      designated in writing by the copyright owner as \"Not a Contribution.\"\n\n      \"Contributor\" shall mean Licensor and any individual or Legal Entity\n      on behalf of whom a Contribution has been received by Licensor and\n      subsequently incorporated within the Work.\n\n   2. Grant of Copyright License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      copyright license to reproduce, prepare Derivative Works of,\n      publicly display, publicly perform, sublicense, and distribute the\n      Work and such Derivative Works in Source or Object form.\n\n   3. Grant of Patent License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      (except as stated in this section) patent license to make, have made,\n      use, offer to sell, sell, import, and otherwise transfer the Work,\n      where such license applies only to those patent claims licensable\n      by such Contributor that are necessarily infringed by their\n      Contribution(s) alone or by combination of their Contribution(s)\n      with the Work to which such Contribution(s) was submitted. If You\n      institute patent litigation against any entity (including a\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\n      or a Contribution incorporated within the Work constitutes direct\n      or contributory patent infringement, then any patent licenses\n      granted to You under this License for that Work shall terminate\n      as of the date such litigation is filed.\n\n   4. Redistribution. You may reproduce and distribute copies of the\n      Work or Derivative Works thereof in any medium, with or without\n      modifications, and in Source or Object form, provided that You\n      meet the following conditions:\n\n      (a) You must give any other recipients of the Work or\n          Derivative Works a copy of this License; and\n\n      (b) You must cause any modified files to carry prominent notices\n          stating that You changed the files; and\n\n      (c) You must retain, in the Source form of any Derivative Works\n          that You distribute, all copyright, patent, trademark, and\n          attribution notices from the Source form of the Work,\n          excluding those notices that do not pertain to any part of\n          the Derivative Works; and\n\n      (d) If the Work includes a \"NOTICE\" text file as part of its\n          distribution, then any Derivative Works that You distribute must\n          include a readable copy of the attribution notices contained\n          within such NOTICE file, excluding those notices that do not\n          pertain to any part of the Derivative Works, in at least one\n          of the following places: within a NOTICE text file distributed\n          as part of the Derivative Works; within the Source form or\n          documentation, if provided along with the Derivative Works; or,\n          within a display generated by the Derivative Works, if and\n          wherever such third-party notices normally appear. The contents\n          of the NOTICE file are for informational purposes only and\n          do not modify the License. You may add Your own attribution\n          notices within Derivative Works that You distribute, alongside\n          or as an addendum to the NOTICE text from the Work, provided\n          that such additional attribution notices cannot be construed\n          as modifying the License.\n\n      You may add Your own copyright statement to Your modifications and\n      may provide additional or different license terms and conditions\n      for use, reproduction, or distribution of Your modifications, or\n      for any such Derivative Works as a whole, provided Your use,\n      reproduction, and distribution of the Work otherwise complies with\n      the conditions stated in this License.\n\n   5. Submission of Contributions. Unless You explicitly state otherwise,\n      any Contribution intentionally submitted for inclusion in the Work\n      by You to the Licensor shall be under the terms and conditions of\n      this License, without any additional terms or conditions.\n      Notwithstanding the above, nothing herein shall supersede or modify\n      the terms of any separate license agreement you may have executed\n      with Licensor regarding such Contributions.\n\n   6. Trademarks. This License does not grant permission to use the trade\n      names, trademarks, service marks, or product names of the Licensor,\n      except as required for reasonable and customary use in describing the\n      origin of the Work and reproducing the content of the NOTICE file.\n\n   7. Disclaimer of Warranty. Unless required by applicable law or\n      agreed to in writing, Licensor provides the Work (and each\n      Contributor provides its Contributions) on an \"AS IS\" BASIS,\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n      implied, including, without limitation, any warranties or conditions\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n      PARTICULAR PURPOSE. You are solely responsible for determining the\n      appropriateness of using or redistributing the Work and assume any\n      risks associated with Your exercise of permissions under this License.\n\n   8. Limitation of Liability. In no event and under no legal theory,\n      whether in tort (including negligence), contract, or otherwise,\n      unless required by applicable law (such as deliberate and grossly\n      negligent acts) or agreed to in writing, shall any Contributor be\n      liable to You for damages, including any direct, indirect, special,\n      incidental, or consequential damages of any character arising as a\n      result of this License or out of the use or inability to use the\n      Work (including but not limited to damages for loss of goodwill,\n      work stoppage, computer failure or malfunction, or any and all\n      other commercial damages or losses), even if such Contributor\n      has been advised of the possibility of such damages.\n\n   9. Accepting Warranty or Additional Liability. While redistributing\n      the Work or Derivative Works thereof, You may choose to offer,\n      and charge a fee for, acceptance of support, warranty, indemnity,\n      or other liability obligations and/or rights consistent with this\n      License. However, in accepting such obligations, You may act only\n      on Your own behalf and on Your sole responsibility, not on behalf\n      of any other Contributor, and only if You agree to indemnify,\n      defend, and hold each Contributor harmless for any liability\n      incurred by, or claims asserted against, such Contributor by reason\n      of your accepting any such warranty or additional liability.\n\n   END OF TERMS AND CONDITIONS\n\n   APPENDIX: How to apply the Apache License to your work.\n\n      To apply the Apache License to your work, attach the following\n      boilerplate notice, with the fields enclosed by brackets \"[]\"\n      replaced with your own identifying information. (Don't include\n      the brackets!)  The text should be enclosed in the appropriate\n      comment syntax for the file format. We also recommend that a\n      file or class name and description of purpose be included on the\n      same \"printed page\" as the copyright notice for easier\n      identification within third-party archives.\n\n   Copyright [yyyy] [name of copyright owner]\n\n   Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 21.548828125,
          "content": "# Official repo for \"Mini-Gemini: Mining the Potential of Multi-modality Vision Language Models\"\n\n<a href='https://mini-gemini.github.io/'><img src='https://img.shields.io/badge/Project-Page-Green'></a>\n<a href='http://103.170.5.190:7860/'><img src='https://img.shields.io/badge/Project-Demo-violet'></a>\n<a href='https://huggingface.co/spaces/wcy1122/MGM'><img src='https://img.shields.io/badge/🤗-Open%20In%20Spaces-blue.svg'></a>\n<a href='https://arxiv.org/pdf/2403.18814.pdf'><img src='https://img.shields.io/badge/Paper-Arxiv-red'></a>\n<a href='https://huggingface.co/collections/YanweiLi/mgm-6603c50b9b43d044171d0854'><img src='https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Models-blue'></a>\n<a href='https://huggingface.co/collections/YanweiLi/mgm-data-660463ea895a01d8f367624e'><img src='https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Data-green'></a>\n\n\nThe framework supports a series of dense and MoE Large Language Models (LLMs) from 2B to 34B with image understanding, reasoning, and generation simultaneously. We build this repo based on LLaVA.\n\n## Release\n- [05/03] 🔥 We support LLaMA3-based models! Welcome to try them [here](https://huggingface.co/collections/YanweiLi/mgm-6603c50b9b43d044171d0854).\n- [04/15] 🔥 The [Hugging Face demo](https://huggingface.co/spaces/wcy1122/MGM) is available. It's a 13B-HD version, welcome to watch and try.\n- [03/28] 🔥 Mini-Gemini is coming! We release the [paper](https://arxiv.org/pdf/2403.18814.pdf), [demo](http://103.170.5.190:7860/), [code](https://github.com/dvlab-research/MGM), [models](https://huggingface.co/collections/YanweiLi/mgm-6603c50b9b43d044171d0854'), and [data](https://huggingface.co/collections/YanweiLi/mgm-data-660463ea895a01d8f367624e)!\n\n## Contents\n- [Demo](#demo)\n- [Install](#install)\n- [Model](#model)\n- [Preparation](#preparation)\n- [Train](#train)\n- [Evaluation](#evaluation)\n- [Examples](#examples)\n- [Citation](#citation)\n- [Acknowledgement](#acknowledgement)\n- [License](#license)\n\n## Demo\nWe provide some selected examples in this section. More examples can be found in our [project page](https://mini-gemini.github.io/). Feel free to try our online [demo](http://103.170.5.190:7860/)!\n\n<div align=center>\n<img width=\"100%\" src=\"images/teaser.png\"/>\n</div>\n\n## Install\nPlease follow the instructions below to install the required packages.\n\nNOTE: If you want to use the 2B version, please ensure to install the latest version Transformers (>=4.38.0).\n\n1. Clone this repository\n```bash\ngit clone https://github.com/dvlab-research/MGM.git\n```\n\n2. Install Package\n```bash\nconda create -n mgm python=3.10 -y\nconda activate mgm\ncd MGM\npip install --upgrade pip  # enable PEP 660 support\npip install -e .\n```\n\n3. Install additional packages for training cases\n```bash\npip install ninja\npip install flash-attn --no-build-isolation\n```\n\n## Model\nThe framework is conceptually simple: dual vision encoders are utilized to provide low-resolution visual embedding and high-resolution candidates;\npatch info mining is proposed to conduct patch-level mining between high-resolution regions and low-resolution visual queries;\nLLM is utilized to marry text with images for both comprehension and generation at the same time.\n\n<div align=center>\n<img width=\"98%\" src=\"images/pipeline.png\"/>\n</div>\n\nWe provide all our fully finetuned models on Stage 1 and 2 data:\n\n| Model | LR | HR | Base LLM | Vision Encoder | Finetuning Data | Finetuning schedule | Download |\n|----------|----------|----------|----------|----------------|---------------|--------------------|------------------|\n| MGM-2B | 336 | 768 | Gemma-2B | CLIP-L | MGM-Instruct | full_ft-1e | [ckpt](https://huggingface.co/YanweiLi/MGM-2B) |\n| MGM-7B | 336 | 768 | Vicuna-7B-v1.5 | CLIP-L | MGM-Instruct | full_ft-1e | [ckpt](https://huggingface.co/YanweiLi/MGM-7B) |\n| MGM-13B | 336 | 768 | Vicuna-13B-v1.5 | CLIP-L | MGM-Instruct | full_ft-1e | [ckpt](https://huggingface.co/YanweiLi/MGM-13B) |\n| MGM-8B | 336 | 768 | LLaMA-3-8B-Instruct | CLIP-L | MGM-Instruct | full_ft-1e | [ckpt](https://huggingface.co/YanweiLi/MGM-8B) |\n| MGM-8x7B | 336 | 768 | Mixtral-8x7B-Instruct-v0.1 | CLIP-L | MGM-Instruct | full_ft-1e | [ckpt](https://huggingface.co/YanweiLi/MGM-8x7B) |\n| MGM-34B | 336 | 768 | Nous-Hermes-2-Yi-34B | CLIP-L | MGM-Instruct | full_ft-1e | [ckpt](https://huggingface.co/YanweiLi/MGM-34B) |\n| MGM-7B-HD | 672 | 1536 | Vicuna-7B-v1.5 | CLIP-L | MGM-Instruct | full_ft-1e | [ckpt](https://huggingface.co/YanweiLi/MGM-7B-HD) |\n| MGM-13B-HD | 672 | 1536 | Vicuna-13B-v1.5 | CLIP-L | MGM-Instruct | full_ft-1e | [ckpt](https://huggingface.co/YanweiLi/MGM-13B-HD) |\n| MGM-8B-HD | 672 | 1536 | LLaMA-3-8B-Instruct | CLIP-L | MGM-Instruct | full_ft-1e | [ckpt](https://huggingface.co/YanweiLi/MGM-8B-HD) |\n| MGM-8x7B-HD | 672 | 1536 | Mixtral-8x7B-Instruct-v0.1 | CLIP-L | MGM-Instruct | full_ft-1e | [ckpt](https://huggingface.co/YanweiLi/MGM-8x7B-HD) |\n| MGM-34B-HD | 672 | 1536 | Nous-Hermes-2-Yi-34B | CLIP-L | MGM-Instruct | full_ft-1e | [ckpt](https://huggingface.co/YanweiLi/MGM-34B-HD) |\n\nHere are the pretrained weights on Stage 1 data only:\n| Model | LR | HR | Base LLM | Vision Encoder | Pretrain Data | Finetuning schedule | Download |\n|----------|----------|----------|----------|----------------|---------------|--------------------|------------------|\n| MGM-2B | 336 | 768 | Gemma-2B | CLIP-L | MGM-Pretrain | 1e | [ckpt](https://huggingface.co/YanweiLi/MGM-Pretrain/tree/main/MGM-2B) |\n| MGM-7B | 336 | 768 | Vicuna-7B-v1.5 | CLIP-L | MGM-Pretrain | 1e | [ckpt](https://huggingface.co/YanweiLi/MGM-Pretrain/tree/main/MGM-7B) |\n| MGM-13B | 336 | 768 | Vicuna-13B-v1.5 | CLIP-L | MGM-Pretrain | 1e | [ckpt](https://huggingface.co/YanweiLi/MGM-Pretrain/tree/main/MGM-13B) |\n| MGM-8x7B | 336 | 768 | Mixtral-8x7B-Instruct-v0.1 | CLIP-L | MGM-Pretrain | 1e | [ckpt](https://huggingface.co/YanweiLi/MGM-Pretrain/tree/main/MGM-8x7B) |\n| MGM-34B | 336 | 768 | Nous-Hermes-2-Yi-34B | CLIP-L | MGM-Pretrain | 1e | [ckpt](https://huggingface.co/YanweiLi/MGM-Pretrain/tree/main/MGM-34B) |\n\n## Preparation\n### Dataset\nWe provide the processed data for the model training. \nFor model pretraining, please download the following the training image-based data and organize them as:\n\n`->` means put the data in the local folder.\n- [LLaVA Images](https://huggingface.co/datasets/liuhaotian/LLaVA-Pretrain) -> `data/MGM-Pretrain/images`, `data/MGM-Finetune/llava/LLaVA-Pretrain/images`\n- [ALLaVA Caption](https://github.com/FreedomIntelligence/ALLaVA) -> `data/MGM-Pretrain/ALLaVA-4V`\n\nFor model finetuning, please download the following the instruction data and organize them as:\n\n`->` means put the data in the local folder.\n- [COCO train2017](http://images.cocodataset.org/zips/train2017.zip) -> `data/MGM-Finetune/coco`\n- [GQA](https://downloads.cs.stanford.edu/nlp/data/gqa/images.zip) -> `data/MGM-Finetune/gqa`\n- [OCR-VQA](https://drive.google.com/drive/folders/1_GYPY5UkUy7HIcR0zq3ZCFgeZN7BAfm_?usp=sharing) (**we save all files as `.jpg`**) -> `data/MGM-Finetune/ocr_vqa`\n- [TextVQA](https://dl.fbaipublicfiles.com/textvqa/images/train_val_images.zip) (not included for training) -> `data/MGM-Finetune/textvqa`\n- [VisualGenome part1](https://cs.stanford.edu/people/rak248/VG_100K_2/images.zip), [VisualGenome part2](https://cs.stanford.edu/people/rak248/VG_100K_2/images2.zip) -> `data/MGM-Finetune/vg`\n- [ShareGPT4V-100K](https://github.com/InternLM/InternLM-XComposer/blob/main/projects/ShareGPT4V/docs/Data.md) -> `data/MGM-Finetune/sam`, `share_textvqa`, `wikiart`, `web-celebrity`, `web-landmark`\n- [LAION GPT4V](https://huggingface.co/datasets/laion/gpt4v-dataset) -> `data/MGM-Finetune/gpt4v-dataset`\n- [ALLaVA Instruction](https://github.com/FreedomIntelligence/ALLaVA) -> `data/MGM-Pretrain/ALLaVA-4V`\n- [DocVQA](https://www.docvqa.org/datasets/docvqa) -> `data/MGM-Finetune/docvqa`\n- [ChartQA](https://github.com/vis-nlp/ChartQA) -> `data/MGM-Finetune/chartqa`\n- [DVQA](https://github.com/kushalkafle/DVQA_dataset) -> `data/MGM-Finetune/dvqa`\n- [AI2D](https://allenai.org/data/diagrams) -> `data/MGM-Finetune/ai2d`\n\nFor model evaluation, please follow this [link](https://github.com/haotian-liu/LLaVA/blob/main/docs/Evaluation.md) for preparation. We use some extra benchmarks for evaluation. please download the following the training image-based data and organize them as:\n\n`->` means put the data in the local folder.\n- [MMMU](https://mmmu-benchmark.github.io/) -> `data/MGM-Eval/MMMU`\n- [MMB](https://github.com/open-compass/mmbench/) -> `data/MGM-Eval/MMB`\n- [MathVista](https://mathvista.github.io/) -> `data/MGM-Eval/MathVista`\n\n\nPlease put the pretrained data, finetuned data, and eval data in  `MGM-Pretrain`, `MGM-Finetune`, and `MGM-Eval` subset following [Structure](#structure).\n\n\nFor meta info, please download the following files and organize them as in [Structure](#structure).\n\n| Data file name | Size |\n| --- | ---: |\n| [mgm_pretrain.json](https://huggingface.co/datasets/YanweiLi/MGM-Pretrain) | 1.68 G |\n| [mgm_instruction.json](https://huggingface.co/datasets/YanweiLi/MGM-Instruction) | 1.79 G |\n| [mgm_generation_pure_text.json](https://huggingface.co/datasets/YanweiLi/MGM-Instruction) | 0.04 G |\n\nIMPORTANT: `mgm_generation_pure_text.json` is a generation-related subset. **DO NOT** merge it with `mgm_instruction.json` as it is already included in it. You may merge this file with your customized LLM/VLM SFT dataset to enable the reasoning generation ability.\n\n\n### Pretrained Weights\nWe recommend users to download the pretrained weights from the following link [CLIP-Vit-L-336](https://huggingface.co/openai/clip-vit-large-patch14-336), [OpenCLIP-ConvNeXt-L](https://huggingface.co/laion/CLIP-convnext_large_d_320.laion2B-s29B-b131K-ft-soup), [Gemma-2b-it](https://huggingface.co/google/gemma-2b-it), [Vicuna-7b-v1.5](https://huggingface.co/lmsys/vicuna-7b-v1.5), [Vicuna-13b-v1.5](https://huggingface.co/lmsys/vicuna-13b-v1.5), [Mixtral-8x7B-Instruct-v0.1](https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1), and [Nous-Hermes-2-Yi-34B](https://huggingface.co/NousResearch/Nous-Hermes-2-Yi-34B) , and put them in `model_zoo` following [Structure](#structure).\n\n\n### Structure\n\nThe folder structure should be organized as follows before training.\n\n```\nMGM\n├── mgm\n├── scripts\n├── work_dirs\n│   ├── MGM\n│   │   ├── MGM-2B\n│   │   ├── ...\n├── model_zoo\n│   ├── LLM\n│   │   ├── gemma\n│   │   │   ├── gemma-2b-it\n│   │   ├── vicuna\n│   │   │   ├── 7B-V1.5\n│   │   │   ├── 13B-V1.5\n│   │   ├── llama-3\n│   │   │   ├── Meta-Llama-3-8B-Instruct\n│   │   │   ├── Meta-Llama-3-70B-Instruct\n│   │   ├── mixtral\n│   │   │   ├── Mixtral-8x7B-Instruct-v0.1\n│   │   ├── Nous-Hermes-2-Yi-34B\n│   ├── OpenAI\n│   │   ├── clip-vit-large-patch14-336\n│   │   ├── openclip-convnext-large-d-320-laion2B-s29B-b131K-ft-soup\n├── data\n│   ├── MGM-Pretrain\n│   │   ├── mgm_pretrain.json\n│   │   ├── images\n│   │   ├── ALLaVA-4V\n│   ├── MGM-Finetune\n│   │   ├── mgm_instruction.json\n│   │   ├── llava\n│   │   ├── coco\n│   │   ├── gqa\n│   │   ├── ocr_vqa\n│   │   ├── textvqa\n│   │   ├── vg\n│   │   ├── gpt4v-dataset\n│   │   ├── sam\n│   │   ├── share_textvqa\n│   │   ├── wikiart\n│   │   ├── web-celebrity\n│   │   ├── web-landmark\n│   │   ├── ALLaVA-4V\n│   │   ├── docvqa\n│   │   ├── chartqa\n│   │   ├── dvqa\n│   │   ├── ai2d\n│   ├── MGM-Eval\n│   │   ├── MMMU\n│   │   ├── MMB\n│   │   ├── MathVista\n│   │   ├── ...\n```\n\n## Train\n\nThe training process consists of two stages: (1) feature alignment stage: bridge the vision and language tokens; (2) instruction tuning stage: teach the model to follow multimodal instructions.\n\nOur models are trained on 8 A100 GPUs with 80GB memory. To train on fewer GPUs, you can reduce the `per_device_train_batch_size` and increase the `gradient_accumulation_steps` accordingly. Always keep the global batch size the same: `per_device_train_batch_size` x `gradient_accumulation_steps` x `num_gpus`.\n\nPlease make sure you download and organize the data following [Preparation](#preparation) before training.\n\nNOTE: Please set `hostfile` for 2 machine training and `hostfile_4` for 4 machine training.\n\nIf you want to train and finetune the framework, please run the following command for MGM-7B with image size 336:\n\n```bash\nbash scripts/llama/train/stage_1_2_full_v7b_336_hr_768.sh\n```\nor for MGM-13B with image size 336:\n```bash\nbash scripts/llama/train/stage_1_2_full_v13b_336_hr_768.sh\n```\nBecause we reuse the pre-trained projecter weights from the MGM-7B, you can directly use the MGM-7B-HD with image size 672 for stage-2 instruction tuning:\n```bash\nbash scripts/llama/train/stage_2_full_v7b_672_hr_1536.sh\n```\nPlease find more training scripts of `gemma`, `llama`, `mixtral`, and `yi` in `scripts/`.\n\n\n## Evaluation\nWe perform evaluation on several image-based benchmarks. Please download the evaluation data following [Preparation](#preparation) and organize them as in [Structure](#structure).\n\n| Model | LLM | Res. | Link | TextVQA | MMB | MME | MM-Vet | MMMU_val | MMMU_test | MathVista |\n|----------|----------|----------|-----------|---|---|---|---|---|---|---|\nMGM-2B | Gemma-2B | 336 | [ckpt](https://huggingface.co/YanweiLi/MGM-2B) | 56.2 | 59.8 | 1341/312 | 31.1 | 31.7 | 29.1 | 29.4\nMGM-7B | Vicuna-7B-v1.5 | 336 | [ckpt](https://huggingface.co/YanweiLi/MGM-7B) | 65.2 | 69.3 | 1523/316 | 40.8 | 36.1 | 32.8 | 31.4 \nMGM-13B | Vicuna-13B-v1.5 | 336 | [ckpt](https://huggingface.co/YanweiLi/MGM-13B) | 65.9 | 68.5 | 1565/322 | 46.0 | 38.1 | 33.5 | 37.0\nMGM-8B | LLaMA-3-8B-Instruct | 336 | [ckpt](https://huggingface.co/YanweiLi/MGM-8B) | 67.6 | 72.7 | 1606/341 | 47.3 | 38.2 | 36.3 | --\nMGM-8x7B | Mixtral-8x7B-Instruct-v0.1 | 336 | [ckpt](https://huggingface.co/YanweiLi/MGM-8x7B) | 69.2 | 75.6 | 1639/379 | 45.8 | 41.8 | 37.1 | 41.8\nMGM-34B | Nous-Hermes-2-Yi-34B | 336 | [ckpt](https://huggingface.co/YanweiLi/MGM-34B) | 70.1 | 79.6 | 1666/439 | 53.0 | 48.7 | 43.6 | 38.9\nMGM-7B-HD | Vicuna-7B-v1.5 | 672 | [ckpt](https://huggingface.co/YanweiLi/MGM-7B-HD) | 68.4 | 65.8 | 1546/319 | 41.3 | 36.8 | 32.9 | 32.2\nMGM-13B-HD | Vicuna-13B-v1.5 | 672 | [ckpt](https://huggingface.co/YanweiLi/MGM-13B-HD) | 70.2 | 68.6 | 1597/320 | 50.5 | 37.3 | 35.1 | 37.0\nMGM-8B-HD | LLaMA-3-8B-Instruct | 672 | [ckpt](https://huggingface.co/YanweiLi/MGM-8B-HD) | 71.6 | -- | 1532/357 | -- | 37.0 | -- | --\nMGM-8x7B-HD | Mixtral-8x7B-Instruct-v0.1 | 672 | [ckpt](https://huggingface.co/YanweiLi/MGM-8x7B-HD) | 71.9 | 74.7 | 1633/356 | 53.5 | 40.0 | 37.0 | 43.1\nMGM-34B-HD | Nous-Hermes-2-Yi-34B | 672 | [ckpt](https://huggingface.co/YanweiLi/MGM-34B-HD) | 74.1 | 80.6 | 1659/482 | 59.3 | 48.0 | 44.9 | 43.3\n\n\n\nIf you want to evaluate the model on image-based benchmarks, please use the scripts in `scripts/MODEL_PATH/eval`. \nFor example, run the following command for TextVQA evaluation with MGM-7B-HD:\n```bash\nbash scripts/llama/eval/textvqa.sh\n```\nPlease find more evaluation scripts in `scripts/MODEL_PATH`.\n\n\n### CLI Inference\nChat with images without the need of Gradio interface. It also supports multiple GPUs, 4-bit and 8-bit quantized inference. With 4-bit quantization.\nPlease make sure you have installed [diffusers](https://github.com/huggingface/diffusers) and [PaddleOCR](https://github.com/PaddlePaddle/PaddleOCR/blob/release/2.7/README_en.md) (only for better experience with OCR), and try this for image and generation inference:\n\n```bash\npython -m mgm.serve.cli \\\n    --model-path work_dirs/MGM/MGM-13B-HD \\\n    --image-file <path to your image>\n```\n\nor try this better experience with OCR (make sure you have installed [PaddleOCR](https://github.com/PaddlePaddle/PaddleOCR/blob/release/2.7/README_en.md)):\n```bash\npython -m mgm.serve.cli \\\n    --model-path work_dirs/MGM/MGM-13B-HD \\\n    --image-file <path to your image> \\\n    --ocr\n```\n\nor try this for inference with generation (make sure you have installed [diffusers](https://github.com/huggingface/diffusers)):\n```bash\npython -m mgm.serve.cli \\\n    --model-path work_dirs/MGM/MGM-13B-HD \\\n    --image-file <path to your image> \\\n    --gen\n```\n\nYou can also try 8bit or even 4bit for efficient inference \n```bash\npython -m mgm.serve.cli \\\n    --model-path work_dirs/MGM/MGM-13B-HD \\\n    --image-file <path to your image> \\\n    --gen\n    --load-8bit\n```\n\n### Gradio Web UI\n\nHere, we adopt the Gradio UI similar to that in LLaVA to provide a user-friendly interface for our models.\nTo launch a Gradio demo locally, please run the following commands one by one. If you plan to launch multiple model workers to compare between different checkpoints, you only need to launch the controller and the web server *ONCE*.\n\n#### Launch a controller\n```Shell\npython -m mgm.serve.controller --host 0.0.0.0 --port 10000\n```\n\n#### Launch a gradio web server.\n```Shell\npython -m mgm.serve.gradio_web_server --controller http://localhost:10000 --model-list-mode reload\n```\nYou just launched the Gradio web interface. Now, you can open the web interface with the URL printed on the screen. You may notice that there is no model in the model list. Do not worry, as we have not launched any model worker yet. It will be automatically updated when you launch a model worker.\n\n#### Launch a model worker\nThis is the actual *worker* that performs the inference on the GPU.  Each worker is responsible for a single model specified in `--model-path`.\n\n```Shell\npython -m mgm.serve.model_worker --host 0.0.0.0 --controller http://localhost:10000 --port 40000 --worker http://localhost:40000 --model-path work_dirs/MGM/MGM-13B-HD\n```\nWait until the process finishes loading the model and you see \"Uvicorn running on ...\".  Now, refresh your Gradio web UI, and you will see the model you just launched in the model list.\n\nYou can launch as many workers as you want, and compare between different models in the same Gradio interface. Please keep the `--controller` the same, and modify the `--port` and `--worker` to a different port number for each worker.\n```Shell\npython -m mgm.serve.model_worker --host 0.0.0.0 --controller http://localhost:10000 --port <different from 40000, say 40001> --worker http://localhost:<change accordingly, i.e. 40001> --model-path work_dirs/MGM/MGM-34B-HD\n```\n\nIf you are using an Apple device with an M1 or M2 chip, you can specify the mps device by using the `--device` flag: `--device mps`.\n\n#### Launch a model worker (Multiple GPUs, when GPU VRAM <= 24GB)\n\nIf the VRAM of your GPU is less than 24GB (e.g., RTX 3090, RTX 4090, etc.), you may try running it with multiple GPUs. Our latest code base will automatically try to use multiple GPUs if you have more than one GPU. You can specify which GPUs to use with `CUDA_VISIBLE_DEVICES`. Below is an example of running with the first two GPUs.\n\n```Shell\nCUDA_VISIBLE_DEVICES=0,1 python -m mgm.serve.model_worker --host 0.0.0.0 --controller http://localhost:10000 --port 40000 --worker http://localhost:40000 --model-path work_dirs/MGM/MGM-13B-HD\n```\n\n#### Launch a model worker (4-bit, 8-bit inference, quantized)\n\nYou can launch the model worker with quantized bits (4-bit, 8-bit), which allows you to run the inference with reduced GPU memory footprint. Note that inference with quantized bits may not be as accurate as the full-precision model. Simply append `--load-4bit` or `--load-8bit` to the **model worker** command that you are executing. Below is an example of running with 4-bit quantization.\n\n```Shell\npython -m mgm.serve.model_worker --host 0.0.0.0 --controller http://localhost:10000 --port 40000 --worker http://localhost:40000 --model-path work_dirs/MGM/MGM-13B-HD --load-4bit\n```\n\n## Examples\nWe provide some examples in this section. More examples can be found in our [project page](https://mini-gemini.github.io/).\n\n### Hi-Resolution Understanding\n<div align=center>\n<img width=\"98%\" src=\"images/demo_und.png\"/>\n</div>\n\n### Generation with Reasoning\n<div align=center>\n<img width=\"98%\" src=\"images/demo_gen.png\"/>\n</div>\n\n## Citation\nIf you find this repo useful for your research, please consider citing the paper\n```\n@article{li2024mgm,\n  title={Mini-Gemini: Mining the Potential of Multi-modality Vision Language Models},\n  author={Li, Yanwei and Zhang, Yuechen and Wang, Chengyao and Zhong, Zhisheng and Chen, Yixin and Chu, Ruihang and Liu, Shaoteng and Jia, Jiaya},\n  journal={arXiv:2403.18814},\n  year={2023}\n}\n```\n\n## Acknowledgement\nThis project is not affiliated with Google LLC.\n\nWe would like to thank the following repos for their great work:\n\n- This work is built upon the [LLaVA](https://github.com/haotian-liu/LLaVA).\n- This work utilizes LLMs from [Gemma](https://huggingface.co/google/gemma-2b-it), [Vicuna](https://github.com/lm-sys/FastChat), [Mixtral](https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1), and [Nous-Hermes](https://huggingface.co/NousResearch/Nous-Hermes-2-Yi-34B).\n\n## License\n[![Code License](https://img.shields.io/badge/Code%20License-Apache_2.0-yellow.svg)](https://github.com/dvlab-research/MGM/blob/main/LICENSE)\n[![Data License](https://img.shields.io/badge/Data%20License-CC%20By%20NC%204.0-orange.svg)](https://github.com/dvlab-research/MGM/blob/main/DATA_LICENSE)\n[![Weight License](https://img.shields.io/badge/Weight%20License-CC%20By%20NC%204.0-red)](https://github.com/dvlab-research/MGM/blob/main/WEIGHT_LICENSE)\n\nThe data and checkpoint is intended and licensed for research use only. They are also restricted to uses that follow the license agreement of LLaVA, LLaMA, Vicuna and GPT-4. The dataset is CC BY NC 4.0 (allowing only non-commercial use) and models trained using the dataset should not be used outside of research purposes.\n"
        },
        {
          "name": "cog.yaml",
          "type": "blob",
          "size": 0.9580078125,
          "content": "# Configuration for Cog ⚙️\n# Reference: https://github.com/replicate/cog/blob/main/docs/yaml.md\n\nbuild:\n  gpu: true\n\n  python_version: \"3.11\"\n\n  python_packages:\n    - \"torch==2.0.1\"\n    - \"accelerate==0.21.0\"\n    - \"bitsandbytes==0.41.0\"\n    - \"deepspeed==0.9.5\"\n    - \"einops-exts==0.0.4\"\n    - \"einops==0.6.1\"\n    - \"gradio==3.35.2\"\n    - \"gradio_client==0.2.9\"\n    - \"httpx==0.24.0\"\n    - \"markdown2==2.4.10\"\n    - \"numpy==1.26.0\"\n    - \"peft==0.4.0\"\n    - \"scikit-learn==1.2.2\"\n    - \"sentencepiece==0.1.99\"\n    - \"shortuuid==1.0.11\"\n    - \"timm==0.6.13\"\n    - \"tokenizers==0.13.3\"\n    - \"torch==2.0.1\"\n    - \"torchvision==0.15.2\"\n    - \"transformers==4.31.0\"\n    - \"wandb==0.15.12\"\n    - \"wavedrom==2.0.3.post3\"\n    - \"Pygments==2.16.1\"\n  run:\n    - curl -o /usr/local/bin/pget -L \"https://github.com/replicate/pget/releases/download/v0.0.3/pget\" && chmod +x /usr/local/bin/pget\n\n# predict.py defines how predictions are run on your model\npredict: \"predict.py:Predictor\"\n"
        },
        {
          "name": "hostfile",
          "type": "blob",
          "size": 0.03515625,
          "content": "your_ip_0 slots=8\nyour_ip_1 slots=8\n"
        },
        {
          "name": "hostfile_4",
          "type": "blob",
          "size": 0.0693359375,
          "content": "your_ip_0 slots=8\nyour_ip_1 slots=8\nyour_ip_2 slots=8\nyour_ip_3 slots=8"
        },
        {
          "name": "images",
          "type": "tree",
          "content": null
        },
        {
          "name": "mgm",
          "type": "tree",
          "content": null
        },
        {
          "name": "predict.py",
          "type": "blob",
          "size": 5.9267578125,
          "content": "import torch\n\nfrom mgm.constants import IMAGE_TOKEN_INDEX, DEFAULT_IMAGE_TOKEN\nfrom mgm.conversation import conv_templates, SeparatorStyle\nfrom mgm.model.builder import load_pretrained_model\nfrom mgm.utils import disable_torch_init\nfrom mgm.mm_utils import tokenizer_image_token\nfrom transformers.generation.streamers import TextIteratorStreamer\n\nfrom PIL import Image\n\nimport requests\nfrom io import BytesIO\n\nfrom cog import BasePredictor, Input, Path, ConcatenateIterator\nimport time\nimport subprocess\nfrom threading import Thread\n\nimport os\nos.environ[\"HUGGINGFACE_HUB_CACHE\"] = os.getcwd() + \"/weights\"\n\n# url for the weights mirror\nREPLICATE_WEIGHTS_URL = \"https://weights.replicate.delivery/default\"\n# files to download from the weights mirrors\nweights = [\n    {\n        \"dest\": \"liuhaotian/llava-v1.5-13b\",\n        # git commit hash from huggingface\n        \"src\": \"llava-v1.5-13b/006818fc465ebda4c003c0998674d9141d8d95f8\",\n        \"files\": [\n            \"config.json\",\n            \"generation_config.json\",\n            \"pytorch_model-00001-of-00003.bin\",\n            \"pytorch_model-00002-of-00003.bin\",\n            \"pytorch_model-00003-of-00003.bin\",\n            \"pytorch_model.bin.index.json\",\n            \"special_tokens_map.json\",\n            \"tokenizer.model\",\n            \"tokenizer_config.json\",\n        ]\n    },\n    {\n        \"dest\": \"openai/clip-vit-large-patch14-336\",\n        \"src\": \"clip-vit-large-patch14-336/ce19dc912ca5cd21c8a653c79e251e808ccabcd1\",\n        \"files\": [\n            \"config.json\",\n            \"preprocessor_config.json\",\n            \"pytorch_model.bin\"\n        ],\n    }\n]\n\ndef download_json(url: str, dest: Path):\n    res = requests.get(url, allow_redirects=True)\n    if res.status_code == 200 and res.content:\n        with dest.open(\"wb\") as f:\n            f.write(res.content)\n    else:\n        print(f\"Failed to download {url}. Status code: {res.status_code}\")\n\ndef download_weights(baseurl: str, basedest: str, files: list[str]):\n    basedest = Path(basedest)\n    start = time.time()\n    print(\"downloading to: \", basedest)\n    basedest.mkdir(parents=True, exist_ok=True)\n    for f in files:\n        dest = basedest / f\n        url = os.path.join(REPLICATE_WEIGHTS_URL, baseurl, f)\n        if not dest.exists():\n            print(\"downloading url: \", url)\n            if dest.suffix == \".json\":\n                download_json(url, dest)\n            else:\n                subprocess.check_call([\"pget\", url, str(dest)], close_fds=False)\n    print(\"downloading took: \", time.time() - start)\n\nclass Predictor(BasePredictor):\n    def setup(self) -> None:\n        \"\"\"Load the model into memory to make running multiple predictions efficient\"\"\"\n        for weight in weights:\n            download_weights(weight[\"src\"], weight[\"dest\"], weight[\"files\"])\n        disable_torch_init()\n    \n        self.tokenizer, self.model, self.image_processor, self.context_len = load_pretrained_model(\"liuhaotian/llava-v1.5-13b\", model_name=\"llava-v1.5-13b\", model_base=None, load_8bit=False, load_4bit=False)\n\n    def predict(\n        self,\n        image: Path = Input(description=\"Input image\"),\n        prompt: str = Input(description=\"Prompt to use for text generation\"),\n        top_p: float = Input(description=\"When decoding text, samples from the top p percentage of most likely tokens; lower to ignore less likely tokens\", ge=0.0, le=1.0, default=1.0),\n        temperature: float = Input(description=\"Adjusts randomness of outputs, greater than 1 is random and 0 is deterministic\", default=0.2, ge=0.0),\n        max_tokens: int = Input(description=\"Maximum number of tokens to generate. A word is generally 2-3 tokens\", default=1024, ge=0),\n    ) -> ConcatenateIterator[str]:\n        \"\"\"Run a single prediction on the model\"\"\"\n    \n        conv_mode = \"llava_v1\"\n        conv = conv_templates[conv_mode].copy()\n    \n        image_data = load_image(str(image))\n        image_tensor = self.image_processor.preprocess(image_data, return_tensors='pt')['pixel_values'].half().cuda()\n    \n        # loop start\n    \n        # just one turn, always prepend image token\n        inp = DEFAULT_IMAGE_TOKEN + '\\n' + prompt\n        conv.append_message(conv.roles[0], inp)\n\n        conv.append_message(conv.roles[1], None)\n        prompt = conv.get_prompt()\n    \n        input_ids = tokenizer_image_token(prompt, self.tokenizer, IMAGE_TOKEN_INDEX, return_tensors='pt').unsqueeze(0).cuda()\n        stop_str = conv.sep if conv.sep_style != SeparatorStyle.TWO else conv.sep2\n        keywords = [stop_str]\n        streamer = TextIteratorStreamer(self.tokenizer, skip_prompt=True, timeout=20.0)\n    \n        with torch.inference_mode():\n            thread = Thread(target=self.model.generate, kwargs=dict(\n                inputs=input_ids,\n                images=image_tensor,\n                do_sample=True,\n                temperature=temperature,\n                top_p=top_p,\n                max_new_tokens=max_tokens,\n                streamer=streamer,\n                use_cache=True))\n            thread.start()\n            # workaround: second-to-last token is always \" \"\n            # but we want to keep it if it's not the second-to-last token\n            prepend_space = False\n            for new_text in streamer:\n                if new_text == \" \":\n                    prepend_space = True\n                    continue\n                if new_text.endswith(stop_str):\n                    new_text = new_text[:-len(stop_str)].strip()\n                    prepend_space = False\n                elif prepend_space:\n                    new_text = \" \" + new_text\n                    prepend_space = False\n                if len(new_text):\n                    yield new_text\n            if prepend_space:\n                yield \" \"\n            thread.join()\n    \n\ndef load_image(image_file):\n    if image_file.startswith('http') or image_file.startswith('https'):\n        response = requests.get(image_file)\n        image = Image.open(BytesIO(response.content)).convert('RGB')\n    else:\n        image = Image.open(image_file).convert('RGB')\n    return image\n"
        },
        {
          "name": "pyproject.toml",
          "type": "blob",
          "size": 1.3662109375,
          "content": "[build-system]\nrequires = [\"setuptools>=61.0\"]\nbuild-backend = \"setuptools.build_meta\"\n\n[project]\nname = \"mgm\"\nversion = \"1.0.0\"\ndescription = \"Mini-Gemini: Mining the Potential of Multi-modality Vision Language Models.\"\nreadme = \"README.md\"\nrequires-python = \">=3.8\"\nclassifiers = [\n    \"Programming Language :: Python :: 3\",\n    \"License :: OSI Approved :: Apache Software License\",\n]\ndependencies = [\n    \"torch==2.0.1\", \"torchvision==0.15.2\",\n    \"transformers==4.36.2\", \"tokenizers==0.15.0\", \"sentencepiece==0.1.99\", \"shortuuid\",\n    \"accelerate==0.21.0\", \"peft==0.4.0\", \"bitsandbytes==0.41.0\",\n    \"pydantic<2,>=1\", \"markdown2[all]\", \"numpy\", \"scikit-learn==1.2.2\",\n    \"gradio==3.35.2\", \"gradio_client==0.2.9\",\n    \"requests\", \"httpx==0.24.0\", \"uvicorn\", \"fastapi\",\n    \"einops==0.6.1\", \"einops-exts==0.0.4\", \"timm==0.9.16\",\n]\n\n[project.optional-dependencies]\ntrain = [\"deepspeed==0.11.1\", \"ninja\", \"wandb\"]\nbuild = [\"build\", \"twine\"]\n\n[project.urls]\n\"Homepage\" = \"https://github.com/dvlab-research/MGM\"\n\"Bug Tracker\" = \"https://github.com/dvlab-research/MGM/issues\"\n\n[tool.setuptools.packages.find]\nexclude = [\"assets*\", \"benchmark*\", \"docs\", \"dist*\", \"playground*\", \"scripts*\", \"tests*\", \"data*\", \"model_zoo*\", \"work_dirs*\", \"project*\"]\n\n[tool.wheel]\nexclude = [\"assets*\", \"benchmark*\", \"docs\", \"dist*\", \"playground*\", \"scripts*\", \"tests*\", \"data*\", \"model_zoo*\", \"work_dirs*\", \"project*\"]\n"
        },
        {
          "name": "scripts",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}