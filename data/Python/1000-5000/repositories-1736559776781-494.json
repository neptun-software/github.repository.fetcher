{
  "metadata": {
    "timestamp": 1736559776781,
    "page": 494,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjUwMA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "thunlp/OpenKE",
      "stars": 3884,
      "defaultBranch": "OpenKE-PyTorch",
      "files": [
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 1.44140625,
          "content": "# Byte-compiled / optimized / DLL files\n__pycache__/\n*.py[cod]\n*$py.class\n\n# Distribution / packaging\n.Python\nbuild/\ndevelop-eggs/\ndist/\ndownloads/\neggs/\n.eggs/\nlib/\nlib64/\nparts/\nsdist/\nvar/\nwheels/\n*.egg-info/\n.installed.cfg\n*.egg\nMANIFEST\n\n# PyInstaller\n#  Usually these files are written by a python script from a template\n#  before PyInstaller builds the exe, so as to inject date/other infos into it.\n*.manifest\n*.spec\n\n# Installer logs\npip-log.txt\npip-delete-this-directory.txt\n\n# Unit test / coverage reports\nhtmlcov/\n.tox/\n.coverage\n.coverage.*\n.cache\nnosetests.xml\ncoverage.xml\n*.cover\n.hypothesis/\n\n# Translations\n*.mo\n*.pot\n\n# Django stuff:\n*.log\n.static_storage/\n.media/\nlocal_settings.py\n\n# Flask stuff:\ninstance/\n.webassets-cache\n\n# Scrapy stuff:\n.scrapy\n\n# Sphinx documentation\ndocs/_build/\n\n# PyBuilder\ntarget/\n\n# Jupyter Notebook\n.ipynb_checkpoints\n\n# pyenv\n.python-version\n\n# celery beat schedule file\ncelerybeat-schedule\n\n# SageMath parsed files\n*.sage.py\n\n# Environments\n.env\n.venv\nenv/\nvenv/\nENV/\nenv.bak/\nvenv.bak/\n\n# Spyder project settings\n.spyderproject\n.spyproject\n\n# Rope project settings\n.ropeproject\n\n# mkdocs documentation\n/site\n\n# mypy\n.mypy_cache/\n\n# Prerequisites\n*.d\n\n# Compiled Object files\n*.slo\n*.lo\n*.o\n*.obj\n\n# Precompiled Headers\n*.gch\n*.pch\n\n# Compiled Dynamic libraries\n*.so\n*.dylib\n*.dll\n\n# Fortran module files\n*.mod\n*.smod\n\n# Compiled Static libraries\n*.lai\n*.la\n*.a\n*.lib\n\n# Executables\n*.exe\n*.out\n*.app\n\n#Backup files\n*.py.bak\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 12.3134765625,
          "content": "# OpenKE (sub-project of OpenSKL)\n\nOpenKE is a sub-project of OpenSKL, providing an **Open**-source **K**nowledge **E**mbedding toolkit for knowledge representation learning (KRL), with <a href=\"https://ojs.aaai.org/index.php/AAAI/article/view/9491/9350\"> TransR</a> and  <a href=\"https://aclanthology.org/D15-1082.pdf\">PTransE</a> as key features to handle complex relations and relational paths in large-scale knowledge graphs.\n\n## Overview\n\nOpenKE is an efficient implementation based on PyTorch for knowledge embedding. We use C++ to implement some underlying operations such as data preprocessing and negative sampling. For each specific model, it is implemented by PyTorch with Python interfaces so that there is a convenient platform to run models on GPUs. OpenKE contains 4 repositories:\n\n[OpenKE-PyTorch](https://github.com/thunlp/OpenKE/tree/OpenKE-PyTorch): the repository based on PyTorch, which provides the optimized and stable framework for knowledge graph embedding models.\n\n<a href=\"https://github.com/thunlp/OpenKE/tree/OpenKE-Tensorflow1.0\"> OpenKE-Tensorflow1.0</a>: OpenKE implemented with TensorFlow, also providing the optimized and stable framework for knowledge graph embedding models.\n\n<a href=\"https://github.com/thunlp/TensorFlow-TransX\"> TensorFlow-TransX</a>: light and simple version of OpenKE based on TensorFlow, including TransE, TransH, TransR and TransD. \n\n<a href=\"https://github.com/thunlp/Fast-TransX\"> Fast-TransX</a>: efficient lightweight C++ inferences for TransE and its extended models utilizing the framework of OpenKE, including TransH, TransR, TransD, TranSparse and PTransE. \n\nMore information (especially the embedding databases of popular knowledge graphs obtained by OpenKE and related documents) is available on our website \n[http://openke.thunlp.org/](http://openke.thunlp.org/)\n\n## Models\nBesides our proposed TransR and PTransE, we also support the following typical knowledge embedding models:\n\nOpenKE (PyTorch): \n\n*\t RESCAL\n*  DistMult, ComplEx, Analogy\n*  TransE, TransH, TransR, TransD\n*  SimplE\n*\t RotatE\n\nOpenKE (Tensorflow): \n\n*\t RESCAL, HolE\n*  DistMult, ComplEx, Analogy\n*  TransE, TransH, TransR, TransD\n\nTensorFlow-TransX (TensorFlow):\n\n*  TransE, TransH, TransR, TransD\n\nFast-TransX (C++):\n\n*  TransE, TransH, TransR, TransD, TranSparse, PTransE\n\nWe welcome any issues and requests for model implementation and bug fix.\n\n## Evaluation\n\nTo validate the effectiveness of this toolkit, we employ the link prediction task on large-scale knowledge graphs for evaluation.\n\n### Settings\nFor each test triplet, the head is removed and replaced by each of the entities from the entity set in turn. The scores of those corrupted triplets are first computed by the models and then sorted by the order. Then, we get the rank of the correct entity. This whole procedure is also repeated by removing those tail entities. We report the proportion of those correct entities ranked in the top 10/3/1 (Hits@10, Hits@3, Hits@1). The mean rank (MR) and mean reciprocal rank (MRR) of the test triplets under this setting are also reported.\n\nBecause some corrupted triplets may be in the training set and validation set. In this case, those corrupted triplets may be ranked above the test triplet, but this should not be counted as an error because both triplets are true. Hence, we remove those corrupted triplets appearing in the training, validation or test set, which ensures the corrupted triplets are not in the dataset. We report the proportion of those correct entities ranked in the top 10/3/1 (Hits@10 (filter), Hits@3(filter), Hits@1(filter)) under this setting. The mean rank (MR (filter)) and mean reciprocal rank (MRR (filter)) of the test triplets under this setting are also reported.\n\nMore details of the above-mentioned settings can be found from the papers [TransE](http://papers.nips.cc/paper/5071-translating-embeddings-for-modeling-multi-relational-data.pdf), [ComplEx](http://proceedings.mlr.press/v48/trouillon16.pdf).\n\nFor those large-scale entity sets, to corrupt all entities with the whole entity set is time-costing. Hence, we also provide the experimental setting named \"[type constraint](https://www.dbs.ifi.lmu.de/~krompass/papers/TypeConstrainedRepresentationLearningInKnowledgeGraphs.pdf)\" to corrupt entities with some limited entity sets determining by their relations.\n\n### Results\n\nWe have provided the hyper-parameters of some models to achieve the state-of-the-art performace (Hits@10 (filter)) on FB15K237 and WN18RR. These scripts can be founded in the folder \"./examples/\". The results of these models are as follows: the left two columns are the performance implemented by OpenKE, and the right two columns are the performance reported in the original papers. Overall, OpenKE can reproduce the results in the original papers. \n\n|Model\t\t\t|\tWN18RR\t|\tFB15K237\t| WN18RR (Paper\\*)| FB15K237  (Paper\\*)|\n|:-:\t\t|:-:\t|:-:  |:-:  |:-:  |\n|TransE (2013)\t|0.512\t|0.476|0.501|0.486|\n|TransH (2014)\t|0.507\t|0.490|-|-|\n|TransR (2015)\t|0.519\t|0.511|-|-|\n|TransD (2015)\t|0.508\t|0.487|-|-|\n|DistMult (2014)\t|0.479\t|0.419|0.49|0.419|\n|ComplEx (2016)\t|0.485\t|0.426|0.51|0.428|\n|ConvE\t(2017)\t|0.506\t|0.485|0.52|0.501|\n|RotatE (2019)\t|0.549\t|0.479|-|0.480|\n|RotatE+adv (2019)\t|0.565\t|0.522|0.571|0.533|\n\nRotatE has the best performance by representing knowledge in complex space. Our proposed TransR has the second best performance, and the real-valued representations learned by TransR can be more easily integrated with other neural network models, e.g. pre-trained language models. Please refer to our another toolkit [Knowledge-Plugin](https://github.com/THUNLP/Knowledge-Plugin) for such integration.\n\n## Usage\n\n### Installation\n\n1. Install [PyTorch](https://pytorch.org/get-started/locally/)\n\n2. Clone the OpenKE-PyTorch branch:\n```bash\ngit clone -b OpenKE-PyTorch https://github.com/thunlp/OpenKE --depth 1\ncd OpenKE\ncd openke\n```\n3. Compile C++ files\n```bash\nbash make.sh\n```\n4. Quick Start\n```bash\ncd ../\ncp examples/train_transe_FB15K237.py ./\npython train_transe_FB15K237.py\n```\n### Data Format\n\n* For training, datasets contain three files:\n\n  train2id.txt: training file, the first line is the number of triples for training. Then the following lines are all in the format ***(e1, e2, rel)*** which indicates there is a relation ***rel*** between ***e1*** and ***e2*** .\n  **Note that train2id.txt contains ids from entitiy2id.txt and relation2id.txt instead of the names of the entities and relations. If you use your own datasets, please check the format of your training file. Files in the wrong format may cause segmentation fault.**\n\n  entity2id.txt: all entities and corresponding ids, one per line. The first line is the number of entities.\n\n  relation2id.txt: all relations and corresponding ids, one per line. The first line is the number of relations.\n\n* For testing, datasets contain additional two files (totally five files):\n\n  test2id.txt: testing file, the first line is the number of triples for testing. Then the following lines are all in the format ***(e1, e2, rel)*** .\n\n  valid2id.txt: validating file, the first line is the number of triples for validating. Then the following lines are all in the format ***(e1, e2, rel)*** .\n\n  type_constrain.txt: type constraining file, the first line is the number of relations. Then the following lines are type constraints for each relation. For example, the relation with id 1200 has 4 types of head entities, which are 3123, 1034, 58 and 5733. The relation with id 1200 has 4 types of tail entities, which are 12123, 4388, 11087 and 11088. You can get this file through **n-n.py** in folder benchmarks/FB15K .\n\n## Citation\n\nIf you find OpenKE is useful for your research, please consider citing the following papers:\n\n```\n @inproceedings{han2018openke,\n   title={OpenKE: An Open Toolkit for Knowledge Embedding},\n   author={Han, Xu and Cao, Shulin and Lv Xin and Lin, Yankai and Liu, Zhiyuan and Sun, Maosong and Li, Juanzi},\n   booktitle={Proceedings of EMNLP},\n   year={2018}\n }\n```\n\nThis package is mainly contributed (in chronological order) by [Xu Han](https://github.com/THUCSTHanxu13), [Yankai Lin](https://github.com/Mrlyk423), [Ruobing Xie](http://nlp.csai.tsinghua.edu.cn/~xrb/), [Zhiyuan Liu](http://nlp.csai.tsinghua.edu.cn/~lzy/), [Xin Lv](https://github.com/davidlvxin), [Shulin Cao](https://github.com/ShulinCao), [Weize Chen](https://github.com/chenweize1998), [Jingqin Yang](https://github.com/yjqqqaq).\n\n******************\n## About OpenSKL\nOpenSKL project aims to harness the power of both structured knowledge and natural languages via representation learning. All sub-projects of OpenSKL, under the categories of **Algorithm**, **Resource** and **Application**, are as follows.\n\n- **Algorithm**: \n  - [OpenKE](https://www.github.com/thunlp/OpenKE)\n    - An effective and efficient toolkit for representing structured knowledge in large-scale knowledge graphs as embeddings, with <a href=\"https://ojs.aaai.org/index.php/AAAI/article/view/9491/9350\"> TransR</a> and  <a href=\"https://aclanthology.org/D15-1082.pdf\">PTransE</a> as key features to handle complex relations and relational paths.\n    - This toolkit also includes three repositories:\n       - [KB2E](https://www.github.com/thunlp/KB2E)\n       - [TensorFlow-Transx](https://www.github.com/thunlp/TensorFlow-Transx)\n       - [Fast-TransX](https://www.github.com/thunlp/Fast-TransX)\n  - [ERNIE](https://github.com/thunlp/ERNIE)\n    - An effective and efficient toolkit for augmenting pre-trained language models with knowledge graph representations.\n  - [OpenNE](https://www.github.com/thunlp/OpenNE)\n    - An effective and efficient toolkit for representing nodes in large-scale graphs as embeddings, with [TADW](https://www.ijcai.org/Proceedings/15/Papers/299.pdf) as key features to incorporate text attributes of nodes.\n  - [OpenNRE](https://www.github.com/thunlp/OpenNRE)\n    - An effective and efficient toolkit for implementing neural networks for extracting structured knowledge from text, with [ATT](https://aclanthology.org/P16-1200.pdf) as key features to consider relation-associated text information.\n    - This toolkit also includes two repositories:\n      - [JointNRE](https://www.github.com/thunlp/JointNRE)\n      - [NRE](https://github.com/thunlp/NRE)\n- **Resource**:\n  - The embeddings of large-scale knowledge graphs pre-trained by OpenKE, covering three typical large-scale knowledge graphs: Wikidata, Freebase, and XLORE. The embeddings are free to use under the [MIT license](https://opensource.org/license/mit/), and please click the following link to submit [download requests](http://139.129.163.161/download/wikidata).\n  - OpenKE-Wikidata\n    - Wikidata is a free and collaborative database, collecting structured data to provide support for Wikipedia. The original Wikidata contains 20,982,733 entities, 594 relations and 68,904,773 triplets. In particular, Wikidata-5M is the core subgraph of Wikidata, containing  5,040,986 high-frequency entities from Wikidata with their corresponding 927 relations and 24,267,796 triplets.\n    - [TransE version](http://139.129.163.161/download/wikidata): Knowledge embeddings of Wikidata pre-trained by OpenKE. \n    - [TransR version](http://139.129.163.161/download/wikidata) of Wikidata-5M: Knowledge embeddings of Wikidata-5M pre-trained by OpenKE.\n  - OpenKE-Freebase\n    - Freebase was a large collaborative knowledge base consisting of data composed mainly by its community members. It was an online collection of structured data harvested from many sources. Freebase contains 86,054,151 entities, 14,824 relations and 338,586,276 triplets.\n    - [TransE version](http://139.129.163.161/download/wikidata): Knowledge embeddings of Freebase pre-trained by OpenKE. \n  - OpenKE-XLORE\n    - XLORE is one of the most popular Chinese knowledge graphs developed by THUKEG. XLORE contains 10,572,209 entities, 138,581 relations and 35,954,249 triplets.\n    - [TransE version](http://139.129.163.161/download/wikidata): Knowledge embeddings of XLORE pre-trained by OpenKE.\n- **Application**:   \n    - [Knowledge-Plugin](https://github.com/THUNLP/Knowledge-Plugin)\n      - An effective and efficient toolkit of plug-and-play knowledge injection for pre-trained language models. Knowledge-Plugin is general for all kinds of knowledge graph embeddings mentioned above. In the toolkit, we plug the TransR version of Wikidata-5M into BERT as an example of applications. With the TransR embedding, we enhance the knowledge ability of BERT without fine-tuning the original model, e.g., up to 8% improvement on question answering.\n"
        },
        {
          "name": "benchmarks",
          "type": "tree",
          "content": null
        },
        {
          "name": "examples",
          "type": "tree",
          "content": null
        },
        {
          "name": "openke",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}