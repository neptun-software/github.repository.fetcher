{
  "metadata": {
    "timestamp": 1736560417418,
    "page": 979,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjk5MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "THUDM/GLM",
      "stars": 3217,
      "defaultBranch": "main",
      "files": [
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.0849609375,
          "content": ".idea/\n/.ipynb_checkpoints/\n.DS_Store\n*.pyc\nlogs\nruns\nsettings.json\n.gitignore\n.vscode/"
        },
        {
          "name": ".pytorch_pretrained_bert",
          "type": "tree",
          "content": null
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 1.037109375,
          "content": "MIT License\n\nCopyright (c) 2021 THUDM\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 24.263671875,
          "content": "# GLM\n\nGLM is a General Language Model pretrained with an autoregressive blank-filling objective and can be finetuned on\nvarious natural language understanding and generation tasks.\n\nPlease refer to our paper for a detailed description of GLM:\n\n[GLM: General Language Model Pretraining with Autoregressive Blank Infilling](https://arxiv.org/abs/2103.10360) (ACL 2022)\n\nZhengxiao Du*, Yujie Qian*, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, Jie Tang (*: equal contribution)\n\n**News: We release [ChatGLM-6B](https://github.com/THUDM/ChatGLM-6B), an open pre-trained language model with 6 billion parameters optimized for Chinese QA and dialogue based on the GLM framework.**\n\n[//]: # (**We release [GLM-130B]&#40;https://github.com/THUDM/GLM-130B&#41;, an open bilingual &#40;English & Chinese&#41; pre-trained language model with 130 billion parameters based on the GLM framework.**)\n\n## Pretrained Models\n\nYou can download the pretrained models used in the paper\nfrom [OneDrive](https://mailstsinghuaeducn-my.sharepoint.com/:f:/g/personal/zx-du20_mails_tsinghua_edu_cn/En6zA7_utRxHptKWZoDMO14Bkfj3uGRpslYkNvMPdGOmow?e=G0lGSc)\nor [Tsinghua-Cloud](https://cloud.tsinghua.edu.cn/d/13f5b03da9594e5490c4).\n\n| Name              | Params | Language | Corpus                                                                              | Objective      | File                                                               | Config                            |\n|-------------------|--------|----------|-------------------------------------------------------------------------------------|----------------|--------------------------------------------------------------------|-----------------------------------|\n| GLM-Base          | 110M   | English  | Wiki+Book                                                                           | Token          | glm-base-blank.tar.bz2                                             | model_blocklm_base.sh             |\n| GLM-Large         | 335M   | English  | Wiki+Book                                                                           | Token          | glm-large-blank.tar.bz2                                            | model_blocklm_large.sh            |\n| GLM-Large-Chinese | 335M   | Chinese  | [WuDaoCorpora](https://www.sciencedirect.com/science/article/pii/S2666651021000152) | Token+Sent+Doc | glm-large-chinese.tar.bz2                                          | model_blocklm_large_chinese.sh    |\n| GLM-Doc           | 335M   | English  | Wiki+Book                                                                           | Token+Doc      | glm-large-generation.tar.bz2                                       | model_blocklm_large_generation.sh |\n| GLM-410M          | 410M   | English  | Wiki+Book                                                                           | Token+Doc      | glm-1.25-generation.tar.bz2                                        | model_blocklm_1.25_generation.sh  |\n| GLM-515M          | 515M   | English  | Wiki+Book                                                                           | Token+Doc      | glm-1.5-generation.tar.bz2                                         | model_blocklm_1.5_generation.sh   |\n| GLM-RoBERTa       | 335M   | English  | RoBERTa                                                                             | Token          | glm-roberta-large-blank.tar.bz2                                    | model_blocklm_roberta_large.sh    |\n| GLM-2B            | 2B     | English  | [Pile](https://arxiv.org/abs/2101.00027)                                            | Token+Sent+Doc | glm-2b.tar.bz2                                                     | model_blocklm_2B.sh               |\n| GLM-10B           | 10B    | English  | [Pile](https://arxiv.org/abs/2101.00027)                                            | Token+Sent+Doc | [Download](https://lfs.aminer.cn/misc/cogview/glm-10b-1024.zip)    | model_blocklm_10B.sh              |\n| GLM-10B-Chinese   | 10B    | Chinese  | [WuDaoCorpora](https://www.sciencedirect.com/science/article/pii/S2666651021000152) | Token+Sent+Doc | [Download](https://lfs.aminer.cn/misc/cogview/glm-10b-chinese.zip) | model_blocklm_10B_chinese.sh      |\n\nUnzip the downloaded file into a local folder and set `CHECKPOINT_PATH` in the corresponding scripts to the folder path.\n\n## Results\n\n### [SuperGLUE](https://super.gluebenchmark.com)\n\ndev set, single model, single-task finetuning\n\n| Model                                                                                        | COPA | WSC  | RTE  | WiC  | CB        | MultiRC   | BoolQ | ReCoRD    |\n|----------------------------------------------------------------------------------------------|------|------|------|------|-----------|-----------|-------|-----------|\n| GLM-10B                                                                                      | 98.0 | 95.2 | 93.1 | 75.7 | 98.7/98.2 | 88.1/63.3 | 88.7  | 94.4/94.0 |\n| [DeBERTa-XXLarge-v2](https://github.com/microsoft/DeBERTa/tree/master/experiments/superglue) | 97.0 | -    | 93.5 | -    | -         | 87.8/63.6 | 88.3  | 94.1/93.7 |\n\n### Seq2Seq\n\n[CNN/Daily Mail](https://github.com/abisee/cnn-dailymail) (test set, no additional data used)\n\n| Model         | ROUGE-1  | ROUGE-2  | ROUGE-L  |\n|---------------|----------|----------|----------|\n| GLM-10B       | **44.7** | 21.4     | **41.4** |\n| T5-11B        | 43.5     | **21.6** | 40.7     |\n| PEGASUS-Large | 44.2     | 21.5     | **41.4** |\n| BART-Large    | 44.2     | 21.3     | 40.9     |\n\n[XSum](https://github.com/EdinburghNLP/XSum) (test set, no additional data used)\n\n| Model         | ROUGE-1  | ROUGE-2  | ROUGE-L  |\n|---------------|----------|----------|----------|\n| GLM-10B       | **48.9** | **25.7** | **40.4** |\n| PEGASUS-Large | 47.2     | 24.6     | 39.3     |\n| BART-Large    | 45.1     | 22.3     | 37.3     |\n\n### Language Modeling\n\ntest set, zero-shot\n\n| Model              | LAMBADA (accuracy) | Wikitext103 (perplexity) |\n|--------------------|--------------------|--------------------------|\n| GLM-10B (bi)       | 72.35              | 11.33                    |\n| GLM-10B (uni)      | 67.18              | 12.22                    |\n| GPT-2              | 52.66              | 17.48                    |\n| Megatron-LM (8.3B) | 66.51              | 10.81                    |\n| Turing-NLG         | 67.98              | 10.21                    |\n\n## Get Started\n\n### Hugging Face Hub\n\nYou can access GLM models via HuggingFace Hub. Please\ninstall `transformers>=4.23.1` and find all the available models [here](https://huggingface.co/models?filter=glm,thudm).\n\n#### Generation\n```python\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\ntokenizer = AutoTokenizer.from_pretrained(\"THUDM/glm-10b\", trust_remote_code=True)\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\"THUDM/glm-10b\", trust_remote_code=True)\nmodel = model.half().cuda()\nmodel.eval()\n\n# Inference\ninputs = tokenizer(\"Ng is an adjunct professor at [MASK] (formerly associate professor and Director of its Stanford AI Lab or SAIL ). Also a pioneer in online education, Ng co-founded Coursera and deeplearning.ai.\", return_tensors=\"pt\")\ninputs = tokenizer.build_inputs_for_generation(inputs, max_gen_length=512)\ninputs = inputs.to('cuda')\noutputs = model.generate(**inputs, max_length=512, eos_token_id=tokenizer.eop_token_id)\nprint(tokenizer.decode(outputs[0].tolist()))\n\n# Training\ninputs = tokenizer(\n    [\"Tsinghua University is located in [MASK].\", \"One minus one equals zero, is it correct? Answer: [MASK]\"],\n    return_tensors=\"pt\", padding=True)\ninputs = tokenizer.build_inputs_for_generation(inputs, targets=[\"Beijing\", \"No\"], max_gen_length=8, padding=False)\ninputs = inputs.to('cuda')\noutputs = model(**inputs)\nloss = outputs.loss\nlogits = outputs.logits\n```\n#### Classification\n```python\nfrom transformers import AutoTokenizer, AutoModelForMultipleChoice\ntokenizer = AutoTokenizer.from_pretrained(\"THUDM/glm-10b\", trust_remote_code=True)\nmodel = AutoModelForMultipleChoice.from_pretrained(\"THUDM/glm-10b\", trust_remote_code=True)\nmodel = model.half().cuda()\nmodel.eval()\n\ninputs = tokenizer([\"Tsinghua University is located in [MASK].\",\n                    \"One minus one equals zero, is it correct? Answer: [MASK]\"], return_tensors=\"pt\", padding=True)\nchoices = [[\"Beijing\", \"Shanghai\"], [\"Yes\", \"No\"]]\ninputs = tokenizer.build_inputs_for_multiple_choice(inputs, choices)\ninputs = inputs.to('cuda')\noutputs = model(**inputs)\nlogits = outputs.logits\n```\nYou can also convert the finetuned checkpoints with `scripts/convert_glm_checkpoint_to_transformers.py`. \n### Docker Image\n\nWe prepare two docker images based on CUDA 10.2 and CUDA 11.2. You can pull the pre-built images from Docker Hub and run\nwith docker v19.03+\n\n  ```shell\n  docker run --gpus all --rm -it --ipc=host zxdu20/glm-cuda102\n  ```\n\nor replace `glm-cuda102` with `glm-cuda112`.\n\nYou can also modify the image according to your requirements in [docker/cuda102.dockerfile](docker/cuda102.dockerfile)\nand build the image yourself\n\n  ```shell\n    docker build -f cuda102.dockerfile . -t glm-cuda102\n  ```\n\n### Manual Installation\n\nPlease first install PyTorch (we use 1.7.0) and [apex](https://github.com/NVIDIA/apex), and then install other\ndependencies by `pip install -r requirements.txt`\n\n### Clone this repo\n\n  ```shell\n  git clone https://github.com/THUDM/GLM\n  cd GLM\n  ```\n\n### Model Parallelism\n\nIf your encounter the `CUDA out of memory` error, which means you GPU memory is limited, you can try the model\nparallelism to divide the parameters into multiple GPUs. Take the two-way model parallelism as an example. First\nrun `change_mp.py` to divide the checkpoint:\n\n```shell\npython change_mp.py path_to_the_checkpoint 2\n```\n\nThen update the checkpoint path in the model config file (such\nas [config_tasks/model_blocklm_10B.sh](config_tasks/model_blocklm_10B.sh)) and change `MP_SIZE` in the script (such\nas [scripts/ds_finetune_superglue.sh](scripts/ds_finetune_superglue.sh)) to `2`.\n\n## Usage\n\nWe provide scripts for finetuning GLM on some downstream tasks.\n\n### Left-to-Right Generation / Blank Filling (Interactive)\n\n* Change `CHECKPOINT_PATH` to your local path. Run the following script\n\n```\nbash scripts/generate_block.sh \\\n     config_tasks/model_blocklm_10B_chinese.sh\n```\n\nSome models (GLM-2B, GLM-10B, and GLM-10B-Chinese) use three different mask tokens: `[MASK]` for short blank\nfilling, `[sMASK]` for sentence filling, and `[gMASK]` for left-to-right generation.\n\n<details>\n<summary><b>Examples</b></summary>\n\n#### Usage of `[MASK]` (Entity Prediction):\n\n##### Example1\n\nContext: Ng is an adjunct professor at [MASK] (formerly associate professor and Director of its Stanford AI Lab or SAIL ). Also a pioneer in online education, Ng co-founded Coursera and deeplearning.ai.\n\nGLM: the stanford university\n\n##### Example2 (Chinese)\n\nContext: 凯旋门位于意大利米兰市古城堡旁。1807年为纪念[MASK]而建，门高25米，顶上矗立两武士青铜古兵车铸像。\n\nGLM:拿破仑军队攻克米兰城\n\n#### Usage of `[sMASK]` (Sentence Prediction)\n\n##### Example3\n\nContext: There have been various types of pretraining architectures including autoencoding models (e.g., BERT),\nautoregressive models (e.g., GPT), and encoder-decoder models (e.g., T5). [sMASK] We propose a General Language Model (\nGLM) based on autoregressive blank infilling to address this challenge. GLM improves blank filling pretraining by adding\n2D positional encodings and allowing an arbitrary order to predict spans, which results in performance gains over BERT\nand T5 on NLU tasks. Meanwhile, GLM can be pretrained for different types of tasks by varying the number and lengths of\nblanks. On a wide range of tasks across NLU, conditional and unconditional generation, GLM outperforms BERT, T5, and GPT\ngiven the same model sizes and data, and achieves the best performance from a single pretrained model with 1.25×\nparameters of BERT Large, demonstrating its generalizability to different downstream tasks.\n\nGLM: However, there is a growing need to develop a single pretraining model that is not only good at natural language\nunderstanding (NLU) or dialog generation/generation (dialog), but is also able to predict other tasks such as sentiment\nanalysis, conditional generation, or machine translation (MT).\n\n##### Example4 (Chinese)\n\nContext: 工业互联网（Industrial\nInternet）是新一代信息通信技术与工业经济深度融合的新型基础设施、应用模式和工业生态，通过对人、机、物、系统等的全面连接，构建起覆盖全产业链、全价值链的全新制造和服务体系，为工业乃至产业数字化、网络化、智能化发展提供了实现途径，是第四次工业革命的重要基石。[sMASK]\n它以网络为基础、平台为中枢、数据为要素、安全为保障，既是工业数字化、网络化、智能化转型的基础设施，也是互联网、大数据、人工智能与实体经济深度融合的应用模式，同时也是一种新业态、新产业，将重塑企业形态、供应链和产业链。当前，工业互联网融合应用向国民经济重点行业广泛拓展，形成平台化设计、智能化制造、网络化协同、个性化定制、服务化延伸、数字化管理六大新模式，赋能、赋智、赋值作用不断显现，有力的促进了实体经济提质、增效、降本、绿色、安全发展。\n\nGLM: 工业互联网是制造业技术、管理、模式的重大变革,是推动互联网、大数据、人工智能和实体经济深度融合的重要载体,是建设制造强国和网络强国的重要基础。\n\n#### Usage of `[gMASK]` (Long Text Generation)\n\n##### Example5 (Chinese)\n\nContext: 问题：冬天，中国哪座城市最适合避寒？问题描述：能推荐一些国内适合冬天避寒的城市吗？回答用户：旅游爱好者 回答： [gMASK]\n\nGLM:\n海南三亚,三亚的冬天是三亚的旺季,因为冬天国内大部分地方都有雾霾,而三亚却是没有雾霾的地方,所以三亚是冬天最惬意不过的地方了。在东北长大的我觉得三亚简直就是一个天堂,冬天去海口三亚旅游,享受冬天,享受阳光沙滩。但是海口却是很干燥,需要多喝水。\n三亚冬天最热门的玩法就是晒太阳,在海边晒晒太阳,在沙滩上晒晒太阳,感觉整个人都得到了解放。三亚还有一个特色项目,就是海上冲浪,在三亚的沙滩上冲浪也是一件非常刺激的事情。\n海口,海口冬季的阳光十分温暖,海南的冬季也是属于冬季旅游的旺季。冬季的海口最棒的是去海南的热带野生动植物园,那里有数之不尽的热带小动物,在这里可以近距离的和它们接触,海南的热带野生动植物园也是海南的天然氧吧。还可以在海口观澜湖公园里感受海口美丽的海景。\n贵阳,贵州的冬天也是十分温暖的,贵阳也是冬季避寒很好的城市之一。冬季去贵阳玩一定要去黔灵山,黔灵山是贵州香火很旺盛的一个寺庙,寺庙的冬季香火鼎盛,在冬季去寺庙游玩也是一个很好的体验。除了黔灵山,贵阳在冬季还有花溪公园可以去玩,花溪公园也是去当地公园玩最好的选择。\n青岛,青岛的冬天是青岛最舒服的时候,青岛有很多海滨浴场,冬天去海边泡一泡温泉,然后晒晒太阳是一件十分惬意的事情。青岛也有沙滩,冬天在沙滩上晒晒太阳,看看海,再玩玩沙滩游戏,感觉十分快乐的事。\n</details>\n\nYou can also add multiple `[MASK]` and `[sMASK]` in a single example. The model will fill the blanks one by one from left to right. The answer to each blank always begins with a special `<|startofpiece|>`.\n\n<details>\n<summary><b>Examples</b></summary>\n\n##### Example1\n\nContext: There have been various types of pretraining architectures including autoencoding models (e.g., BERT), autoregressive models (e.g., GPT), and [MASK] (e.g., T5). [sMASK] We propose a General Language Model ( GLM) based on autoregressive blank infilling to address this challenge. GLM improves blank filling pretraining by adding 2D positional encodings and allowing an arbitrary order to predict spans, which results in performance gains over [MASK] on NLU tasks. Meanwhile, GLM can be pretrained for different types of tasks by varying the number and lengths of blanks. On a wide range of tasks across NLU, conditional and [MASK], GLM outperforms BERT, T5, and GPT given the same model sizes and data, and achieves the best performance from a single pretrained model with 1.25× parameters of BERT Large , demonstrating its generalizability to different downstream tasks.\n\nGLM: <|startofpiece|> blank filling models<|startofpiece|> However, most of them cannot easily transfer to other downstream tasks due to the different characteristics of these tasks.<|startofpiece|> other pretrained models<|startofpiece|> unconditional reading, and semantic role labeling tasks\n\n##### Example2 (Chinese)\n\nContext: 工业互联网（Industrial Internet）是新一代[MASK]与[MASK]深度融合的新型基础设施、应用模式和工业生态，通过对人、机、物、系统等的全面连接，构建起覆盖全产业链、全价值链的全新制造和服务体系，为工业乃至产业数字化、网络化、智能化发展提供了实现途径，是第四次工业革命的重要基石。[sMASK] 它以网络为基础、平台为中枢、数据为要素、安全为保障，既是工业数字化、网络化、智能化转型的基础设施，也是互联网、大数据、人工智能与实体经济深度融合的应用模式，同时也是一种新业态、新产业，将重塑企业形态、供应链和产业链。当前，工业互联网融合应用向国民经济重点行业广泛拓展，形成[MASK]、智能化制造、[MASK]、个性化定制、服务化延伸、数字化管理六大新模式，赋能、赋智、赋值作用不断显现，有力的促进了实体经济提质、增效、降本、绿色、安全发展。\n\nGLM:\n<|startofpiece|>信息技术(ICT)<|startofpiece|>工业经济(II2O)<|startofpiece|>我国工业互联网是面向工业全领域、全流程、全体系的互联网,具有多产业、多领域融合的特点。<|startofpiece|>网络化协同<|startofpiece|>平台企业\n\n</details>\n\n\n### SuperGLUE\n\n- Download the [SuperGlue](https://super.gluebenchmark.com/tasks) data and check the experiment setup in\n  [scripts/ds_finetune_superglue.sh](scripts/ds_finetune_superglue.sh). Note\n  that `DATA_ROOT, CHECKPOINT_PATH, SAVE_PATH`\n  need to be changed to your local path. You may also change the `batch-size` and `nproc_per_node` according to your\n  available hardware.\n\n- Run the following script (use the COPA dataset as an example)\n\n```\nbash scripts/ds_finetune_superglue.sh \\\n     config_tasks/model_blocklm_10B.sh \\\n     config_tasks/task_copa.sh\n```\n\n- We also implement [P-Tuning](https://arxiv.org/abs/2103.10385) in our code. Run the following script to integrate\n  p-tuning:\n\n```shell\nbash scripts/ds_finetune_superglue_prompt.sh \\\n     config_tasks/model_blocklm_10B.sh \\\n     config_tasks/task_copa.sh\n```\n\n- To apply GLM to a new NLU dataset with cloze-filling finetuning, implement a `DataProcessor` in\n  [tasks/superglue/dataset.py](tasks/superglue/dataset.py) for data loading and add a `PVP` in\n  [tasks/superglue/pvp.py](tasks/superglue/pvp.py) for the cloze question. More details can be found\n  [here](tasks/superglue/README.md).\n\n### Seq2Seq\n\n- Download the [Gigaword](https://github.com/harvardnlp/sent-summary)\n  , [CNN/Daily Mail](https://github.com/artmatsak/cnn-dailymail)\n  or [XSum](https://github.com/EdinburghNLP/XSum/tree/master/XSum-Dataset) dataset and check the experiment setup in\n  [scripts/ds_finetune_seq2seq.sh](scripts/ds_finetune_seq2seq.sh). Change `DATA_ROOT, CHECKPOINT_PATH, SAVE_PATH` to\n  your\n  local path.\n\n- Run the following script (use the CNN/Daily Mail dataset as an example)\n\n  ```\n  bash scripts/ds_finetune_seq2seq.sh \\ \n     config_tasks/model_blocklm_10B.sh \\ \n     config_tasks/seq_cnndm_org.sh\n  ```\n- The summaries are written into `./runs/experiment_name/test.jsonl.hyps`. The references are written\n  into `test.jsonl.refs` in the same directory. For calculating rouge,\n  install [file2rouge](https://github.com/pltrdy/files2rouge) and download Stanford CoreNLP\n  from [here](http://nlp.stanford.edu/software/stanford-corenlp-full-2016-10-31.zip). Run the following script\n  ```\n  bash scripts/evaluate_seq2seq.sh \\\n   ./runs/experiment_name/test.jsonl.hyps ./runs/experiment_name/test.jsonl.refs\n  ```\n\n#### Train with your own data\n\nProcess your seq2seq data into `{split}.source` and `{split}.target`, with each line being the context or the target of\na sample, and `split` being `train`, `val`, and `test`.\n\nRun the following script\n\n```shell\nbash scripts/ds_finetune_seq2seq.sh \\ \n   config_tasks/model_blocklm_10B.sh \\ \n   config_tasks/seq_customization.sh\n```\n\nYou can specify the hyperparameters in `config_tasks/seq_customization.sh`\nand `config_tasks/config_blocklm_10B_cnndm.json`\n\n### Multiple Choice (Zero-shot)\n\n```shell\nbash scripts/evaluate_multichoice.sh config_tasks/model_blocklm_10B.sh\n```\n\nNote that `CHECKPOINT_PATH` and `DATA_PATH` need to be changed to your local path.\n\nThe format of each line of the data file should be\n\n```\n{\"inputs_pretokenized\": \"Context and question here\", \"choices_pretokenized\": [\"Choice 1\", \"Choice 2\", \"Choice 3\"], \"label\": int}\n```\n\n### Language Modeling\n\n#### LAMBADA Cloze Accuracy\n\n* Download the [LAMBADA](https://github.com/cybertronai/bflm/blob/master/lambada_test.jsonl) data and change\n  `DATA_ROOT, CHECKPOINT_PATH` in [scripts/evaluate_lm.sh](scripts/evaluate_lm.sh)\n* Run the following script\n\n```shell\nbash scripts/evaluate_lm.sh \\ \n     config_tasks/model_blocklm_large_generation.sh \\\n     config_tasks/zero_lambada.sh \n```\n\n#### LM Perplexity\n\n* Download\n  our [test set of wikibook](https://mailstsinghuaeducn-my.sharepoint.com/:t:/g/personal/duzx16_mails_tsinghua_edu_cn/EQa_B6KY_q1FjtUeG-T52iMBFtNrfhfHcZbzMxfkJKXKRQ?e=inTdHh)\n  or [Wikitext103](https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-103-v1.zip) dataset and\n  change `DATA_ROOT, CHECKPOINT_PATH`\n  in [scripts/evaluate_lm.sh](scripts/evaluate_lm.sh)\n* Run the following script\n  ```shell\n  bash scripts/evaluate_lm.sh \\ \n     config_tasks/model_blocklm_large_generation.sh \\\n     config_tasks/zero_wikitext.sh \n  ```\n\n### Text Infilling\n\n- Download the [Yahoo](https://github.com/Varal7/blank_language_model) dataset and check the experiment setup in\n  [scripts/finetune_blank.sh](scripts/finetune_blank.sh). Change `DATA_ROOT, CHECKPOINT_PATH, SAVE_PATH` to your\n  local path.\n\n- Run the following script\n\n```\nbash scripts/finetune_blank.sh \\ \n     config_tasks/model_blocklm_large.sh \\ \n     config_tasks/seq_blank.sh\n```\n\n## Pretrain\n\nRun the following script to pre-train the GLM-Large model\n\n```shell\nbash scripts/ds_pretrain_nvidia.sh config/ds_block_large.sh\n```\n\nThe script [scripts/ds_pretrain_nvidia.sh](scripts/ds_pretrain_nvidia.sh) launches the training program with DeepSpeed.\nYou should change `NUM_WORKERS` and `NUM_GPUS_PER_WORKER` to the number of workers and the number of gpus per worker.\nAlso change `HOST_FILE_PATH` to the path to an OpenMPI-style hostfile. More details about DeepSpeed launcher can be\nfound [here](https://www.deepspeed.ai/getting-started/#resource-configuration-multi-node).\n\nThe file [config/ds_block_large.sh](config/ds_block_large.sh) defines the hyperparameters for pretraining. Most of the\narguments are fairly self-explanatory. Specifically, `--train-data` can be multiple keywords defined in `NAMED_CORPORA`\nin [data_utils/corpora.py](data_utils/corpora.py). The hyperparameters of the optimizer are defined in the corresponding\njson file under `config`. The semantics of the json file can be found [here](https://www.deepspeed.ai/docs/config-json).\n\n## Citation\n\nPart of the code is based on [Megatron-LM](https://github.com/NVIDIA/Megatron-LM)\nand [PET](https://github.com/timoschick/pet).\n\nPlease cite our paper if you find this code useful for your research:\n\n```\n@article{DBLP:conf/acl/DuQLDQY022,\n  author    = {Zhengxiao Du and\n               Yujie Qian and\n               Xiao Liu and\n               Ming Ding and\n               Jiezhong Qiu and\n               Zhilin Yang and\n               Jie Tang},\n  title     = {{GLM:} General Language Model Pretraining with Autoregressive Blank Infilling},\n  booktitle = {Proceedings of the 60th Annual Meeting of the Association for Computational\n               Linguistics (Volume 1: Long Papers), {ACL} 2022, Dublin, Ireland,\n               May 22-27, 2022},\n  pages     = {320--335},\n  publisher = {Association for Computational Linguistics},\n  year      = {2022},\n}\n```"
        },
        {
          "name": "arguments.py",
          "type": "blob",
          "size": 27.3427734375,
          "content": "# coding=utf-8\n# Copyright (c) 2019, NVIDIA CORPORATION.  All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"argparser configuration\"\"\"\n\nimport argparse\nimport os\nimport torch\nimport deepspeed\nimport json\nfrom utils import get_hostname\n\n\ndef add_model_config_args(parser):\n    \"\"\"Model arguments\"\"\"\n\n    group = parser.add_argument_group('model', 'model configuration')\n\n    group.add_argument('--transformer-xl', action='store_true', help='use transformer-xl for training')\n    group.add_argument('--pretrained-bert', action='store_true',\n                       help='use a pretrained bert-large-uncased model instead'\n                            'of initializing from scratch. See '\n                            '--tokenizer-model-type to specify which pretrained '\n                            'BERT model to use')\n    group.add_argument('--encoder-decoder', action='store_true',\n                       help=\"use the encoder-decoder architecture for blocklm\")\n    group.add_argument('--attention-dropout', type=float, default=0.1,\n                       help='dropout probability for attention weights')\n    group.add_argument('--num-attention-heads', type=int, default=16,\n                       help='num of transformer attention heads')\n    group.add_argument('--hidden-size', type=int, default=1024,\n                       help='transformer hidden size')\n    group.add_argument('--intermediate-size', type=int, default=None,\n                       help='transformer embedding dimension for FFN'\n                            'set to 4*`--hidden-size` if it is None')\n    group.add_argument('--num-layers', type=int, default=24,\n                       help='num decoder layers')\n    group.add_argument('--layernorm-epsilon', type=float, default=1e-5,\n                       help='layer norm epsilon')\n    group.add_argument('--hidden-dropout', type=float, default=0.1,\n                       help='dropout probability for hidden state transformer')\n    group.add_argument('--output-dropout', type=float, default=0.1,\n                       help='dropout probability for pooled output')\n    group.add_argument('--max-position-embeddings', type=int, default=512,\n                       help='maximum number of position embeddings to use')\n    group.add_argument('--vocab-size', type=int, default=30522,\n                       help='vocab size to use for non-character-level '\n                            'tokenization. This value will only be used when '\n                            'creating a tokenizer')\n    group.add_argument('--deep-init', action='store_true',\n                       help='initialize bert model similar to gpt2 model.'\n                            'scales initialization of projection layers by a '\n                            'factor of 1/sqrt(2N). Necessary to train bert '\n                            'models larger than BERT-Large.')\n    group.add_argument('--make-vocab-size-divisible-by', type=int, default=128,\n                       help='Pad the vocab size to be divisible by this value.'\n                            'This is added for computational efficiency reasons.')\n    group.add_argument('--cpu-optimizer', action='store_true',\n                       help='Run optimizer on CPU')\n    group.add_argument('--cpu_torch_adam', action='store_true',\n                       help='Use Torch Adam as optimizer on CPU.')\n\n    return parser\n\n\ndef add_fp16_config_args(parser):\n    \"\"\"Mixed precision arguments.\"\"\"\n\n    group = parser.add_argument_group('fp16', 'fp16 configurations')\n\n    group.add_argument('--fp16', action='store_true',\n                       help='Run model in fp16 mode')\n    group.add_argument('--fp32-embedding', action='store_true',\n                       help='embedding in fp32')\n    group.add_argument('--fp32-layernorm', action='store_true',\n                       help='layer norm in fp32')\n    group.add_argument('--fp32-tokentypes', action='store_true',\n                       help='embedding token types in fp32')\n    group.add_argument('--fp32-allreduce', action='store_true',\n                       help='all-reduce in fp32')\n    group.add_argument('--hysteresis', type=int, default=2,\n                       help='hysteresis for dynamic loss scaling')\n    group.add_argument('--loss-scale', type=float, default=None,\n                       help='Static loss scaling, positive power of 2 '\n                            'values can improve fp16 convergence. If None, dynamic'\n                            'loss scaling is used.')\n    group.add_argument('--loss-scale-window', type=float, default=1000,\n                       help='Window over which to raise/lower dynamic scale')\n    group.add_argument('--min-scale', type=float, default=1,\n                       help='Minimum loss scale for dynamic loss scale')\n    group.add_argument('--attention-scale', type=float, default=1.0)\n    return parser\n\n\ndef add_training_args(parser):\n    \"\"\"Training arguments.\"\"\"\n\n    group = parser.add_argument_group('train', 'training configurations')\n\n    group.add_argument('--experiment-name', type=str, default=\"glm\",\n                       help=\"The experiment name for summary and checkpoint\")\n    group.add_argument('--batch-size', type=int, default=4,\n                       help='Data Loader batch size')\n    group.add_argument('--gradient-accumulation-steps', type=int, default=1,\n                       help='Data Loader batch size')\n    group.add_argument('--weight-decay', type=float, default=0.01,\n                       help='weight decay coefficient for L2 regularization')\n    group.add_argument('--checkpoint-activations', action='store_true',\n                       help='checkpoint activation to allow for training '\n                            'with larger models and sequences')\n    group.add_argument('--checkpoint-num-layers', type=int, default=1,\n                       help='chunk size (number of layers) for checkpointing')\n    group.add_argument('--deepspeed-activation-checkpointing', action='store_true',\n                       help='uses activation checkpointing from deepspeed')\n    group.add_argument('--epochs', type=int, default=None,\n                       help='Number of finetuning epochs. Zero results in evaluation only.')\n    group.add_argument('--clip-grad', type=float, default=1.0,\n                       help='gradient clipping')\n    group.add_argument('--train-iters', type=int, default=0,\n                       help='total number of iterations to train over all training runs')\n    group.add_argument('--label-smoothing', type=float, default=0.0)\n    group.add_argument('--log-interval', type=int, default=100,\n                       help='report interval')\n    group.add_argument('--summary-dir', type=str, default=\"\", help=\"The directory to store the summary\")\n    group.add_argument('--seed', type=int, default=1234, help='random seed')\n    # Batch producer arguments\n    group.add_argument('--reset-position-ids', action='store_true',\n                       help='Reset position ids after end-of-document token.')\n    group.add_argument('--reset-attention-mask', action='store_true',\n                       help='Reset self attention masks after '\n                            'end-of-document token.')\n\n    # Learning rate.\n    group.add_argument('--lr-decay-iters', type=int, default=None,\n                       help='number of iterations to decay LR over,'\n                            ' If None defaults to `--train-iters`*`--epochs`')\n    group.add_argument('--lr-decay-style', type=str, default='linear',\n                       choices=['constant', 'linear', 'cosine', 'exponential'],\n                       help='learning rate decay function')\n    group.add_argument('--lr-decay-ratio', type=float, default=0.1)\n    group.add_argument('--lr', type=float, default=1.0e-4,\n                       help='initial learning rate')\n    group.add_argument('--warmup', type=float, default=0.01,\n                       help='percentage of data to warmup on (.01 = 1% of all '\n                            'training iters). Default 0.01')\n    group.add_argument('--switch-linear', action='store_true', help=\"Switch to linear decay for cosine decay\")\n    # model checkpointing\n    group.add_argument('--save', type=str, default=None,\n                       help='Output directory to save checkpoints to.')\n    group.add_argument('--new-save-directory', action='store_true')\n    group.add_argument('--save-epoch', type=int, default=1,\n                       help='number of epochs between saves')\n    group.add_argument('--save-interval', type=int, default=5000,\n                       help='number of iterations between saves')\n    group.add_argument('--no-save-optim', action='store_true',\n                       help='Do not save current optimizer.')\n    group.add_argument('--no-save-rng', action='store_true',\n                       help='Do not save current rng state.')\n    group.add_argument('--load', type=str, default=None,\n                       help='Path to a directory containing a model checkpoint.')\n    group.add_argument('--no-load-optim', action='store_true',\n                       help='Do not load optimizer when loading checkpoint.')\n    group.add_argument('--no-load-rng', action='store_true',\n                       help='Do not load rng state when loading checkpoint.')\n    group.add_argument('--no-load-lr-scheduler', action='store_true',\n                       help='Do not load lr scheduler when loading checkpoint.')\n    group.add_argument('--no-deepspeed-load', action='store_true', help='Not use deepspeed when loading checkpoint')\n    group.add_argument('--finetune', action='store_true',\n                       help='Load model for finetuning. Do not load optimizer '\n                            'or rng state from checkpoint and set iteration to 0. '\n                            'Assumed when loading a release checkpoint.')\n    group.add_argument('--resume-dataloader', action='store_true',\n                       help='Resume the dataloader when resuming training. '\n                            'Does not apply to tfrecords dataloader, try resuming'\n                            'with a different seed in this case.')\n    # distributed training args\n    group.add_argument('--distributed-backend', default='nccl',\n                       help='which backend to use for distributed training. One of [gloo, nccl]',\n                       choices=['nccl', 'gloo'])\n    group.add_argument('--DDP-impl', default='torch', choices=['local', 'torch', 'none'],\n                       help='which DistributedDataParallel implementation to use.')\n\n    group.add_argument('--local_rank', type=int, default=None,\n                       help='local rank passed from distributed launcher')\n    # BlockLM training args\n    group.add_argument('--block-lm', action='store_true', help=\"whether use the BlockLM pre-training\")\n    group.add_argument('--masked-lm', action='store_true', help='whether to use the mlm objective')\n    group.add_argument('--bert-prob', type=float, default=0.5)\n    group.add_argument('--gpt-infill-prob', type=float, default=0.5)\n    group.add_argument('--gpt-min-ratio', type=float, default=0.5)\n    group.add_argument('--gap-sentence-prob', type=float, default=0.0)\n    group.add_argument('--gap-sentence-ratio', type=float, default=0.15)\n    group.add_argument('--avg-block-length', type=int, default=3)\n    group.add_argument('--short-seq-prob', type=float, default=0.0)\n    group.add_argument('--single-span-prob', type=float, default=0.0)\n    group.add_argument('--task-mask', action='store_true', help=\"Use different mask for generation and blank filling\")\n    group.add_argument('--no-shuffle-block', action='store_true', help=\"not shuffle the blocks when filling the blank\")\n    group.add_argument('--no-block-position', action='store_true',\n                       help='Use (rough) absolute positions instead of block positions')\n    group.add_argument('--sentinel-token', action='store_true',\n                       help=\"Use sentinel (mask) tokens to replace 2d position encoding\")\n    group.add_argument('--block-mask-prob', type=float, default=0.0)\n    group.add_argument('--context-mask-ratio', type=float, default=0.0)\n    group.add_argument('--random-position', action='store_true',\n                       help=\"Use random start position to cover all the position embeddings\")\n    return parser\n\n\ndef add_evaluation_args(parser):\n    \"\"\"Evaluation arguments.\"\"\"\n\n    group = parser.add_argument_group('validation', 'validation configurations')\n\n    group.add_argument('--eval-batch-size', type=int, default=None,\n                       help='Data Loader batch size for evaluation datasets.'\n                            'Defaults to `--batch-size`')\n    group.add_argument('--eval-iters', type=int, default=100,\n                       help='number of iterations to run for evaluation/'\n                            'validation/test for')\n    group.add_argument('--eval-interval', type=int, default=1000,\n                       help='interval between running evaluation on validation set')\n    group.add_argument('--eval-epoch', type=int, default=1,\n                       help='epoch between running evaluation on validation set')\n    group.add_argument('--eval-seq-length', type=int, default=None,\n                       help='Maximum sequence length to process for '\n                            'evaluation. Defaults to `--seq-length`')\n    group.add_argument('--eval-max-preds-per-seq', type=int, default=None,\n                       help='Maximum number of predictions to use for '\n                            'evaluation. Defaults to '\n                            'math.ceil(`--eval-seq-length`*.15/10)*10')\n    group.add_argument('--overlapping-eval', type=int, default=32)\n\n    return parser\n\n\ndef add_text_generate_args(parser):\n    \"\"\"Text generate arguments.\"\"\"\n\n    group = parser.add_argument_group('Text generation', 'configurations')\n    group.add_argument(\"--temperature\", type=float, default=1.0)\n    group.add_argument(\"--top-p\", type=float, default=0.0)\n    group.add_argument(\"--top-k\", type=int, default=0)\n    group.add_argument(\"--out-seq-length\", type=int, default=256)\n    group.add_argument(\"--num-beams\", type=int, default=1)\n    group.add_argument(\"--length-penalty\", type=float, default=0.0)\n    group.add_argument(\"--no-repeat-ngram-size\", type=int, default=0)\n    group.add_argument(\"--min-tgt-length\", type=int, default=0)\n    group.add_argument(\"--select-topk\", action='store_true')\n    group.add_argument(\"--blank-maskratio\", type=float, default=0.1)\n    return parser\n\n\ndef add_data_args(parser):\n    \"\"\"Train/valid/test data arguments.\"\"\"\n\n    group = parser.add_argument_group('data', 'data configurations')\n\n    group.add_argument('--model-parallel-size', type=int, default=1,\n                       help='size of the model parallel.')\n    group.add_argument('--shuffle', action='store_true',\n                       help='Shuffle data. Shuffling is deterministic '\n                            'based on seed and current epoch.')\n    group.add_argument('--filter-english', action='store_true')\n    group.add_argument('--train-data', nargs='+', default=None,\n                       help='Whitespace separated filenames or corpora names '\n                            'for training.')\n    group.add_argument('--valid-data', nargs='*', default=None,\n                       help=\"\"\"Filename for validation data.\"\"\")\n    group.add_argument('--test-data', nargs='*', default=None,\n                       help=\"\"\"Filename for testing\"\"\")\n    group.add_argument('--data-dir', type=str, default=None, help=\"The data path to all the data files\")\n    group.add_argument('--input-data-sizes-file', type=str, default='sizes.txt',\n                       help='the filename containing all the shards sizes')\n\n    group.add_argument('--delim', default=',',\n                       help='delimiter used to parse csv data files')\n    group.add_argument('--text-key', default='sentence',\n                       help='key to use to extract text from json/csv')\n    group.add_argument('--eval-text-key', default=None,\n                       help='key to use to extract text from '\n                            'json/csv evaluation datasets')\n    group.add_argument('--split', default='1000,1,1',\n                       help='comma-separated list of proportions for training,'\n                            ' validation, and test split')\n\n    group.add_argument('--no-lazy-loader', action='store_true',\n                       help='whether to lazy read the data set')\n    group.add_argument('--half-lazy-loader', action='store_true')\n    group.add_argument('--loader-scatter', type=int, default=None, help='Number of scatters to use for dataloaders')\n    group.add_argument('--loose-json', action='store_true',\n                       help='Use loose json (one json-formatted string per '\n                            'newline), instead of tight json (data file is one '\n                            'json string)')\n    group.add_argument('--presplit-sentences', action='store_true',\n                       help='Dataset content consists of documents where '\n                            'each document consists of newline separated sentences')\n    group.add_argument('--num-workers', type=int, default=2,\n                       help=\"\"\"Number of workers to use for dataloading\"\"\")\n    group.add_argument('--tokenizer-model-type', type=str,\n                       default=None,\n                       help=\"Model type to use for sentencepiece tokenization \\\n                       (one of ['bpe', 'char', 'unigram', 'word']) or \\\n                       bert vocab to use for BertWordPieceTokenizer (one of \\\n                       ['bert-large-uncased', 'bert-large-cased', etc.])\")\n    group.add_argument('--tokenizer-path', type=str, default='tokenizer.model',\n                       help='path used to save/load sentencepiece tokenization '\n                            'models')\n    group.add_argument('--tokenizer-type', type=str,\n                       default='BertWordPieceTokenizer',\n                       choices=['CharacterLevelTokenizer',\n                                'SentencePieceTokenizer',\n                                'BertWordPieceTokenizer',\n                                'GPT2BPETokenizer',\n                                'ChineseSPTokenizer'],\n                       help='what type of tokenizer to use')\n    group.add_argument('--fix-command-token', action='store_true')\n    group.add_argument('--no-pre-tokenize', action='store_true')\n    group.add_argument(\"--cache-dir\", default=None, type=str,\n                       help=\"Where to store pre-trained BERT downloads\")\n    group.add_argument('--use-tfrecords', action='store_true',\n                       help='load `--train-data`, `--valid-data`, '\n                            '`--test-data` from BERT tf records instead of '\n                            'normal data pipeline')\n    group.add_argument('--seq-length', type=int, default=512,\n                       help=\"Maximum sequence length to process\")\n    group.add_argument('--mem-length', type=int, default=0,\n                       help=\"The memory length to preserve\")\n    group.add_argument('--max-preds-per-seq', type=int, default=None,\n                       help='Maximum number of predictions to use per sequence. '\n                            'Defaults to math.ceil(`--seq-length`*.15/10)*10. '\n                            'MUST BE SPECIFIED IF `--use-tfrecords` is True.')\n    group.add_argument('--non-sentence-start', type=float, default=0.0)\n    group.add_argument('--sample-one-document', action='store_true', help='only sample one document in one sample')\n    group.add_argument('--load-splits', type=str, default=None, help=\"The path to load split indices from\")\n    group.add_argument('--save-splits', type=str, default=None, help=\"The path to save split indices to\")\n    group.add_argument('--save-test-data', type=str, default=None, help=\"The path to save the test data\")\n    group.add_argument('--multi-task-data', nargs='*', default=None,\n                       help=\"Downstream task names for multi-task pre-training\")\n    group.add_argument('--multi-task-ratio', type=float, default=0.0, help=\"Ratio for multi-task pre-training\")\n    group.add_argument('--multi-seq-length', type=int, default=None)\n    group.add_argument('--multi-batch-size', type=int, default=None)\n    return parser\n\n\ndef add_finetune_config_args(parser):\n    group = parser.add_argument_group('finetune', 'finetune configurations')\n    group.add_argument('--task', type=str, help='Task name.')\n    group.add_argument('--load-pretrained', type=str, help=\"Load pretrained model\", default=None)\n    group.add_argument('--pool-token', type=str, choices=['start', 'pad', 'cls'],\n                       help='The token to pool the sequence representation', default='cls')\n    group.add_argument('--cloze-eval', action='store_true', help='Evaluation dataset with cloze task')\n    group.add_argument('--multi-token', action='store_true', help='Use multi token for cloze evaluation')\n    group.add_argument('--segment-length', type=int, default=0, help=\"The maximum segment length for cloze evaluation\")\n    group.add_argument('--loss-func', type=str, choices=[\"cross_entropy\", \"hinge\", \"generative\", \"mix\"],\n                       default=\"cross_entropy\")\n    group.add_argument('--block-lm-ratio', type=float, default=0.0)\n    group.add_argument('--adapet', action='store_true', help=\"Use the decoupled cross entropy loss in AdaPET\")\n    group.add_argument('--pattern-id', type=int, default=0)\n    group.add_argument('--fast-decode', action='store_true',\n                       help=\"Fast decode for multi-token cloze. Can only be used without checkpoint activation.\")\n    group.add_argument('--few-superglue', action='store_true')\n    group.add_argument('--eval-valid', action='store_true', help=\"Whether evaluate on the valid set\")\n    group.add_argument('--validation-metric', type=str, default=None)\n    group.add_argument('--unidirectional', action='store_true', help=\"Use the left to right language model\")\n    group.add_argument('--src-seq-length', type=int, default=None)\n    group.add_argument('--tgt-seq-length', type=int, default=None)\n    group.add_argument('--adam-beta1', type=float, default=0.9)\n    group.add_argument('--adam-beta2', type=float, default=0.999)\n    group.add_argument('--adam-eps', type=float, default=1e-8)\n    group.add_argument('--optimizer', type=str, choices=['adam', 'adafactor'], default='adam')\n    group.add_argument('--wsc-negative', action='store_true')\n    group.add_argument('--overwrite', action='store_true')\n    group.add_argument('--no-validation', action='store_true')\n    # Continuous prompt arguments\n    group.add_argument('--continuous-prompt', action='store_true', help=\"Use continuous prompt for PET\")\n    group.add_argument('--num-prompt-tokens', type=int, default=0)\n    group.add_argument('--prompt-func', default='lstm', choices=[\"lstm\", \"mlp\", \"none\"])\n    group.add_argument('--freeze-transformer', action='store_true', default=False)\n    group.add_argument('--tune-prefix-layers', type=int, default=None)\n    group.add_argument('--prefix-prompt', type=int, default=0)\n    group.add_argument('--prompt-init', action='store_true', default=False)\n    group.add_argument('--mask-pad-token', action='store_true')\n    return parser\n\n\ndef get_args():\n    \"\"\"Parse all the args.\"\"\"\n\n    parser = argparse.ArgumentParser(description='PyTorch BERT Model')\n    parser = add_model_config_args(parser)\n    parser = add_fp16_config_args(parser)\n    parser = add_training_args(parser)\n    parser = add_evaluation_args(parser)\n    parser = add_text_generate_args(parser)\n    parser = add_data_args(parser)\n    parser = add_finetune_config_args(parser)\n\n    # Include DeepSpeed configuration arguments\n    parser = deepspeed.add_config_arguments(parser)\n\n    args = parser.parse_args()\n    if not args.train_data and not args.data_dir:\n        print('WARNING: No training data specified')\n\n    args.cuda = torch.cuda.is_available()\n\n    args.rank = int(os.getenv('RANK', '0'))\n    args.world_size = int(os.getenv(\"WORLD_SIZE\", '1'))\n    if hasattr(args, 'deepspeed_mpi') and args.deepspeed_mpi:\n        mpi_define_env(args)\n    elif os.getenv('OMPI_COMM_WORLD_LOCAL_RANK'):\n        # We are using (OpenMPI) mpirun for launching distributed data parallel processes\n        local_rank = int(os.getenv('OMPI_COMM_WORLD_LOCAL_RANK'))\n        local_size = int(os.getenv('OMPI_COMM_WORLD_LOCAL_SIZE'))\n\n        # Possibly running with Slurm\n        num_nodes = int(os.getenv('SLURM_JOB_NUM_NODES', '1'))\n        nodeid = int(os.getenv('SLURM_NODEID', '0'))\n\n        args.local_rank = local_rank\n        args.rank = nodeid * local_size + local_rank\n        args.world_size = num_nodes * local_size\n\n    args.model_parallel_size = min(args.model_parallel_size, args.world_size)\n    if args.rank == 0:\n        print('using world size: {} and model-parallel size: {} '.format(\n            args.world_size, args.model_parallel_size))\n\n    args.dynamic_loss_scale = False\n    if args.loss_scale is None:\n        args.dynamic_loss_scale = True\n        if args.rank == 0:\n            print(' > using dynamic loss scaling')\n\n    # The args fp32_* or fp16_* meant to be active when the\n    # args fp16 is set. So the default behaviour should all\n    # be false.\n    if not args.fp16:\n        args.fp32_embedding = False\n        args.fp32_tokentypes = False\n        args.fp32_layernorm = False\n\n    if hasattr(args, \"deepspeed\") and args.deepspeed and args.deepspeed_config is not None:\n        with open(args.deepspeed_config) as file:\n            deepspeed_config = json.load(file)\n        if \"train_micro_batch_size_per_gpu\" in deepspeed_config:\n            args.batch_size = deepspeed_config[\"train_micro_batch_size_per_gpu\"]\n        if \"gradient_accumulation_steps\" in deepspeed_config:\n            args.gradient_accumulation_steps = deepspeed_config[\"gradient_accumulation_steps\"]\n        else:\n            args.gradient_accumulation_steps = 1\n        if \"optimizer\" in deepspeed_config:\n            optimizer_params_config = deepspeed_config[\"optimizer\"].get(\"params\", {})\n            args.lr = optimizer_params_config.get(\"lr\", args.lr)\n            args.weight_decay = optimizer_params_config.get(\"weight_decay\", args.weight_decay)\n    return args\n\n\ndef mpi_define_env(args):\n    from mpi4py import MPI\n    comm = MPI.COMM_WORLD\n    rank = comm.Get_rank()\n    world_size = comm.Get_size()\n\n    master_addr = None\n    if rank == 0:\n        master_addr = get_hostname()\n    master_addr = comm.bcast(master_addr, root=0)\n\n    # Determine local rank by assuming hostnames are unique\n    proc_name = MPI.Get_processor_name()\n    all_procs = comm.allgather(proc_name)\n    local_rank = sum([i == proc_name for i in all_procs[:rank]])\n\n    os.environ['RANK'] = str(rank)\n    os.environ['WORLD_SIZE'] = str(world_size)\n    args.local_rank = local_rank\n    args.world_size = world_size\n    args.rank = rank\n    os.environ['MASTER_ADDR'] = master_addr\n    os.environ['MASTER_PORT'] = \"29500\"  # TORCH_DISTRIBUTED_DEFAULT_PORT = 29500\n\n    print(\n        \"Discovered MPI settings of world_rank={}, local_rank={}, world_size={}, master_addr={}, master_port={}\"\n            .format(os.environ['RANK'],\n                    args.local_rank,\n                    os.environ['WORLD_SIZE'],\n                    os.environ['MASTER_ADDR'],\n                    os.environ['MASTER_PORT']))\n"
        },
        {
          "name": "blocklm_utils.py",
          "type": "blob",
          "size": 24.85546875,
          "content": "import torch\nimport torch.utils.data\nimport mpu\nimport random\nimport copy\nimport numpy as np\nimport math\nfrom utils import print_rank_0\nfrom scipy.stats import poisson\n\n\ndef rindex(lst, val, start=None):\n    if start is None:\n        start = len(lst) - 1\n    for i in range(start, -1, -1):\n        if lst[i] == val:\n            return i\n    return -1\n\n\ndef index_in_list(lst, val, start=None):\n    if start is None:\n        start = 0\n    for i in range(start, len(lst)):\n        if lst[i] == val:\n            return i\n    return -1\n\n\nclass ConstructBlockStrategy:\n    def __init__(self, args, tokenizer, max_seq_length, bert_prob=1.0, gap_sentence_prob=0.0, gpt_infill_prob=0.5,\n                 gpt_min_ratio=0.5, bert_ratio=0.15, gap_sentence_ratio=0.15, average_block_length=3,\n                 max_block_length=40, block_mask_prob=0.0, context_mask_ratio=0.0, context_mask_range=3,\n                 short_seq_prob=0.0, single_span_prob=0.0, block_position_encoding=True, encoder_decoder=False,\n                 shuffle_blocks=True, sentinel_token=False, task_mask=False, random_position=False, masked_lm=False):\n        self.eod_token = args.eod_token\n        self.tokenizer = tokenizer\n        self.count = 0\n        self.max_seq_length = max_seq_length\n        self.rank = mpu.get_data_parallel_rank()\n        self.world_size = mpu.get_data_parallel_world_size()\n        # self.rank = 0\n        # self.world_size = 1\n        assert 0.0 <= bert_prob <= 1.0\n        self.bert_prob = bert_prob\n        self.gap_sentence_prob = gap_sentence_prob\n        self.gpt_prob = 1 - bert_prob - gap_sentence_prob\n        assert self.gpt_prob >= -1e-10\n        self.infill_prob = gpt_infill_prob\n        self.gpt_min_ratio = gpt_min_ratio\n        self.bert_ratio = bert_ratio\n        self.gap_sentence_ratio = gap_sentence_ratio\n        self.block_length_distribution = [poisson.pmf(i, average_block_length) for i in range(1, max_block_length)]\n        self.block_mask_prob = block_mask_prob\n        self.context_mask_ratio = context_mask_ratio\n        self.context_mask_range = context_mask_range\n        self.short_seq_prob = short_seq_prob\n        self.single_span_prob = single_span_prob\n        self.block_position_encoding = block_position_encoding\n        self.encoder_decoder = encoder_decoder\n        self.shuffle_blocks = shuffle_blocks\n        self.sentinel_token = sentinel_token\n        self.generation_mask = 'gMASK' if task_mask else 'MASK'\n        self.generation_mask = self.tokenizer.get_command(self.generation_mask).Id\n        self.gap_sentence_mask = 'sMASK' if task_mask else 'MASK'\n        self.gap_sentence_mask = self.tokenizer.get_command(self.gap_sentence_mask).Id\n        self.random_position = random_position\n        self.masked_lm = masked_lm\n        print_rank_0(\n            f\"BERT prob {self.bert_prob}, gap sent prob {self.gap_sentence_prob}, GPT prob {self.gpt_prob}, infill prob {self.infill_prob}\")\n        print_rank_0(\n            f\"generation min ratio {self.gpt_min_ratio}, block ratio {self.bert_ratio}, gap sent ratio {self.gap_sentence_ratio}\")\n        print_rank_0(f\"block length distribution {self.block_length_distribution}\")\n        print_rank_0(f\"block mask prob {self.block_mask_prob}, context mask ratio {self.context_mask_ratio}\")\n\n    def contains_sentence_end(self, tok):\n        tok = self.tokenizer.IdToToken(tok)\n        if '.' in tok:\n            return True\n        if '?' in tok:\n            return True\n        if '!' in tok:\n            return True\n        if ';' in tok:\n            return True\n        if ':' in tok:\n            return True\n        if '。' in tok:\n            return True\n        if '？' in tok:\n            return True\n        if '！' in tok:\n            return True\n        if '；' in tok:\n            return True\n        if '…' in tok:\n            return True\n        if '\\n' in tok:\n            return True\n        return False\n\n    @staticmethod\n    def sample_spans(span_lengths, total_length, rng, offset=0):\n        blank_length = total_length - sum(span_lengths)\n        m = blank_length - len(span_lengths) + 1\n        places = [rng.randrange(m + 1) for _ in range(len(span_lengths))]\n        places.sort()\n        spans = []\n        for place, span_length in zip(places, span_lengths):\n            start = offset + place\n            end = offset + place + span_length\n            spans.append((start, end))\n            offset += span_length + 1\n        return spans\n\n    def sample_span_in_document(self, tokens, masked_lengths, rng):\n        rng.shuffle(masked_lengths)\n        mask_spans = []\n        mask_index = 0\n        indices = [-1] + np.where(tokens == self.eod_token)[0].tolist()\n        last_index = len(tokens)\n        documents = []\n        for index in reversed(indices):\n            start_index = index\n            if start_index + 1 < len(tokens) and tokens[start_index + 1] == self.tokenizer.get_command('ENC').Id:\n                start_index += 1\n            length = last_index - start_index - 1\n            if last_index == len(tokens) and length > 0:\n                length -= 1\n            documents.append((start_index + 1, length))\n            last_index = index\n        documents.sort(key=lambda x: x[1])\n        for i, (offset, length) in enumerate(documents):\n            if i == len(documents) - 1:\n                current_masked_length, current_count = 0, 0\n                while mask_index + current_count < len(masked_lengths) and masked_lengths[\n                    mask_index + current_count] + current_masked_length + current_count <= length:\n                    current_masked_length += masked_lengths[mask_index + current_count]\n                    current_count += 1\n                if current_count > 0:\n                    spans = self.sample_spans(masked_lengths[mask_index: mask_index + current_count], length, rng,\n                                              offset=offset)\n                    mask_spans += spans\n                if mask_index + current_count < len(masked_lengths) - 1:\n                    print(length, masked_lengths[mask_index:], masked_lengths[:mask_index], indices)\n            else:\n                current_masked_total = int(length * self.bert_ratio)\n                current_masked_length, current_count = 0, 0\n                while mask_index + current_count < len(masked_lengths) and masked_lengths[\n                    mask_index + current_count] + current_masked_length <= current_masked_total:\n                    current_masked_length += masked_lengths[mask_index + current_count]\n                    current_count += 1\n                if current_count > 0:\n                    spans = self.sample_spans(masked_lengths[mask_index:mask_index + current_count], length,\n                                              rng, offset=offset)\n                    mask_spans += spans\n                    mask_index += current_count\n        return mask_spans\n\n    def make_masked_data(self, tokens, loss_masks, attention_mask, block_spans, rng, task='bert'):\n        position_ids = np.arange(len(tokens), dtype=np.long)\n        targets = copy.deepcopy(tokens)\n        mask_id = self.tokenizer.get_command('MASK').Id\n        mlm_masks = np.zeros(len(tokens), dtype=np.long)\n        for start, end in block_spans:\n            for idx in range(start, end):\n                tokens[idx] = mask_id\n            mlm_masks[start: end] = 1\n        loss_masks = loss_masks * mlm_masks\n        return tokens, targets, loss_masks, position_ids\n\n    def make_block_data(self, tokens, loss_masks, attention_mask, block_spans, rng, task='bert'):\n        text_length = len(tokens)\n        position_ids = np.ones(len(tokens), dtype=np.long)\n        for start, end in block_spans:\n            position_ids[start + 1: end] = 0\n        position_ids = np.cumsum(position_ids) - 1\n        if self.random_position and position_ids[-1] < self.max_seq_length - 1:\n            position_bias = self.max_seq_length - position_ids[-1]\n            position_bias = rng.randrange(0, position_bias)\n            position_ids = position_ids + position_bias\n        if self.encoder_decoder or not self.shuffle_blocks:\n            block_spans.sort(key=lambda x: x[0])\n        else:\n            rng.shuffle(block_spans)\n        if self.sentinel_token:\n            block_spans = [(start, end, idx) for idx, (start, end) in enumerate(block_spans)]\n        else:\n            block_spans = [(start, end, 0) for start, end in block_spans]\n        target_tokens, target_position_ids, target_block_position_ids, targets = [], [], [], []\n        for start, end, idx in block_spans:\n            sop_token = 'sop' if idx == 0 else f\"sop{idx}\"\n            target_tokens.append([self.tokenizer.get_command(sop_token).Id])\n            span_tokens = copy.deepcopy(tokens[start: end])\n            if self.block_mask_prob > 0.0 and task == 'bert':\n                for sub_idx in range(len(span_tokens)):\n                    if random.random() < self.block_mask_prob:\n                        span_tokens[sub_idx] = self.tokenizer.get_command('dBLOCK').Id\n            target_tokens.append(span_tokens)\n            targets.append(tokens[start: end])\n            targets.append([self.tokenizer.get_command('eop').Id])\n            if not self.sentinel_token:\n                target_position_id = position_ids[start: end]\n                target_position_ids.append(target_position_id)\n                target_position_ids.append([target_position_id[0]])\n            else:\n                target_position_ids.append([self.max_seq_length] * (end - start + 1))\n            if self.block_position_encoding:\n                target_block_position_ids.append(np.arange(1, end - start + 2, dtype=np.long))\n            else:\n                target_block_position_ids.append([1] * (end - start + 1))\n        block_spans.sort(key=lambda x: x[0])\n        source_tokens, source_position_ids, local_spans = [], [], []\n        last, current_length = 0, 0\n        for start, end, idx in block_spans:\n            if task == 'generation':\n                mask_id = self.generation_mask\n            elif task == 'gap_sentence':\n                mask_id = self.gap_sentence_mask\n            else:\n                mask_token = 'MASK' if idx == 0 else f'MASK{idx}'\n                mask_id = self.tokenizer.get_command(mask_token).Id\n            local_spans.append((current_length, current_length + start - last))\n            source_tokens.append(tokens[last: start])\n            source_tokens.append([mask_id])\n            source_position_ids.append(position_ids[last: start])\n            source_position_ids.append([position_ids[start]])\n            current_length += start - last + 1\n            last = end\n        if last < len(tokens):\n            local_spans.append((current_length, current_length + len(tokens) - last))\n            source_tokens.append(tokens[last:])\n            source_position_ids.append(position_ids[last:])\n        source_length = sum(map(len, source_tokens))\n        if attention_mask is not None:\n            assert source_length == attention_mask\n        if target_tokens and self.eod_token in np.concatenate(target_tokens).tolist():\n            print(\"Found EOS in target\", self.tokenizer.DecodeIds(tokens))\n            raise RuntimeError\n        if self.encoder_decoder:\n            target_tokens = target_tokens + [self.tokenizer.get_command('eop').Id]\n            loss_masks = np.ones(len(target_tokens), dtype=np.long)\n            return source_tokens, target_tokens, loss_masks\n        else:\n            tokens = np.concatenate(source_tokens + target_tokens)\n            if task == 'bert' and self.context_mask_ratio > 0:\n                mask_candidates = set()\n                for start, end in local_spans:\n                    if start != 0:\n                        local_end = min(end, start + self.context_mask_range)\n                        mask_candidates.update(range(start, local_end))\n                    if end != 0:\n                        local_start = max(start, end - self.context_mask_range)\n                        mask_candidates.update(range(local_start, end))\n                mask_pos = rng.sample(mask_candidates, int(self.context_mask_ratio * text_length))\n                for pos in mask_pos:\n                    tokens[pos] = self.tokenizer.get_command('dBLOCK').Id\n            targets = np.concatenate(source_tokens + targets)\n            loss_masks = np.ones(len(tokens), dtype=np.long)\n            loss_masks[:source_length] = 0\n            position_ids = np.concatenate(source_position_ids + target_position_ids)\n            block_position_ids = np.concatenate(\n                [np.zeros(source_length, dtype=np.long)] + target_block_position_ids)\n            position_ids = np.stack([position_ids, block_position_ids], axis=0)\n            if attention_mask is not None:\n                return tokens, targets, loss_masks, position_ids\n            else:\n                return tokens, targets, loss_masks, position_ids, source_length\n\n    def generate_blank_data(self, sample, masked_lengths, attention_mask, rng, task='bert'):\n        rng.shuffle(masked_lengths)\n        tokens, loss_masks = sample['text'], sample['loss_mask']\n        assert tokens[0] == self.tokenizer.get_command('ENC').Id\n        block_spans = self.sample_span_in_document(tokens, masked_lengths, rng)\n        if len(block_spans) < len(masked_lengths):\n            return None\n        if self.masked_lm:\n            data = self.make_masked_data(tokens, loss_masks, attention_mask, block_spans, rng)\n        else:\n            data = self.make_block_data(tokens, loss_masks, attention_mask, block_spans, rng, task=task)\n        return data\n\n    def split_samples(self, samples, rng):\n        target_length = rng.randrange(32, self.max_seq_length - 1)\n        num_splits = (self.max_seq_length - 1) // target_length\n        new_samples = []\n        cls_id = self.tokenizer.get_command('ENC').Id\n        eos_id = self.tokenizer.get_command('eos').Id\n        for sample in samples:\n            tokens, loss_masks = sample['text'][1:], sample['loss_mask'][1:]\n            for _ in range(num_splits):\n                if target_length >= len(tokens):\n                    new_tokens, new_loss_masks = tokens, loss_masks\n                else:\n                    random_start = rng.randrange(0, len(tokens) - target_length)\n                    while random_start > 0 and (tokens[random_start] == eos_id or not (\n                            self.contains_sentence_end(tokens[random_start - 1]) or tokens[\n                        random_start - 1] == eos_id)):\n                        random_start -= 1\n                    random_end = random_start + target_length\n                    while random_end > random_start and not (\n                            self.contains_sentence_end(tokens[random_end - 1]) or tokens[random_end - 1] == eos_id):\n                        random_end -= 1\n                    if random_end - random_start < target_length // 2:\n                        random_end = random_start + target_length\n                    new_tokens, new_loss_masks = tokens[random_start: random_end], loss_masks[random_start: random_end]\n                new_tokens = np.concatenate(([cls_id], new_tokens))\n                new_loss_masks = np.concatenate(([0], new_loss_masks))\n                new_samples.append({'text': new_tokens, 'loss_mask': new_loss_masks})\n        return new_samples\n\n    def construct_blocks(self, samples):\n        worker_info = torch.utils.data.get_worker_info()\n        if worker_info is not None:\n            worker_id, num_workers = worker_info.id, worker_info.num_workers\n        else:\n            worker_id, num_workers = 0, 1\n        rng = random.Random((self.count * num_workers + worker_id) * self.world_size + self.rank)\n        self.count += 1\n        token_batch, target_batch, loss_mask_batch, position_id_batch = [], [], [], []\n        source_batch, target_batch = [], []\n        if rng.random() < self.short_seq_prob:\n            samples = self.split_samples(samples, rng)\n        rand = rng.random()\n        single_span = rand < self.single_span_prob\n        rand = 0.0 if single_span else rng.random()\n        attention_mask = []\n        if rand < self.bert_prob:\n            mode = 'bert'\n            for sample in samples:\n                if single_span:\n                    masked_lengths = [rng.choices(range(1, len(self.block_length_distribution) + 1),\n                                                  weights=self.block_length_distribution)[0]]\n                    masked_count = masked_lengths[0]\n                else:\n                    masked_lengths, masked_count = [], 0\n                    while masked_count < int(self.bert_ratio * len(sample['text'])):\n                        block_length = rng.choices(range(1, len(self.block_length_distribution) + 1),\n                                                   weights=self.block_length_distribution)[0]\n                        masked_lengths.append(block_length)\n                        masked_count += block_length\n                if self.masked_lm:\n                    sep = len(sample['text'])\n                else:\n                    sep = len(sample['text']) - masked_count + len(masked_lengths)\n                data = self.generate_blank_data(sample, masked_lengths, sep, rng, task='bert')\n                if data is not None:\n                    if self.encoder_decoder:\n                        source_tokens, target_tokens, loss_masks = data\n                        source_batch.append(source_tokens)\n                        target_batch.append(target_tokens)\n                        loss_mask_batch.append(loss_masks)\n                    else:\n                        tokens, targets, loss_masks, position_ids = data\n                        token_batch.append(tokens)\n                        target_batch.append(targets)\n                        loss_mask_batch.append(loss_masks)\n                        position_id_batch.append(position_ids)\n                    attention_mask.append(sep)\n\n        elif rand < self.bert_prob + self.gap_sentence_prob:\n            mode = 'sentence'\n            for sample in samples:\n                tokens, loss_masks = sample['text'], sample['loss_mask']\n                sentence_spans = []\n                last_index = 1 if tokens[0] == self.tokenizer.get_command('ENC').Id else 0\n                for i in range(len(tokens)):\n                    if self.contains_sentence_end(tokens[i]):\n                        if last_index < i + 1:\n                            sentence_spans.append((last_index, i + 1))\n                        last_index = i + 1\n                    elif tokens[i] == self.tokenizer.get_command('eos').Id:\n                        last_index = i + 1\n                if last_index < len(tokens):\n                    sentence_spans.append((last_index, len(tokens)))\n                if not sentence_spans and torch.distributed.get_rank() == 0:\n                    try:\n                        print(self.tokenizer.DecodeIds(tokens[1:]))\n                    except IndexError:\n                        print(tokens[1:])\n                rng.shuffle(sentence_spans)\n                block_spans, block_length = [], 0\n                for start, end in sentence_spans:\n                    block_spans.append((start, end))\n                    block_length += end - start\n                    if block_length >= int(self.gap_sentence_ratio * len(tokens)):\n                        break\n                data = self.make_block_data(tokens, loss_masks, None, block_spans, rng, task='gap_sentence')\n                tokens, targets, loss_masks, position_ids, sep = data\n                token_batch.append(tokens)\n                target_batch.append(targets)\n                loss_mask_batch.append(loss_masks)\n                position_id_batch.append(position_ids)\n                attention_mask.append(sep)\n        else:\n            # start_indices = [index_in_list(sample['loss_mask'], 1) for sample in samples]\n            # end_indices = [rindex(sample['loss_mask'], 1) for sample in samples]\n            # start_index, end_index = max(start_indices), min(end_indices) - self.min_generation_length\n            # if end_index < start_index + 1:\n            #     end_index = start_index + 1\n            # division = rng.randrange(start_index, end_index)\n            mode = 'gpt'\n            max_generation_length = rng.randint(int(self.gpt_min_ratio * min(map(lambda x: len(x['text']), samples))),\n                                                max(map(lambda x: len(x['text']), samples)) - 2)\n            for sample in samples:\n                generation_length = min(max_generation_length, len(sample['text']) - 2)\n                attention_mask.append(len(sample['text']) - generation_length + 1)\n                multiple_doc = index_in_list(sample['text'], self.tokenizer.get_command('eos').Id) not in [-1, len(\n                    sample['text']) - 1]\n                if multiple_doc or rng.random() < self.infill_prob:\n                    division = len(sample['text']) - generation_length\n                    tokens, loss_masks = sample['text'], sample['loss_mask']\n                    source_tokens, target_tokens = tokens[:division], tokens[division:]\n                    target_masks = loss_masks[division:]\n                    tokens = np.concatenate((\n                        source_tokens, [self.generation_mask, self.tokenizer.get_command('sop').Id],\n                        target_tokens[:-1]))\n                    targets = np.concatenate((source_tokens, [self.generation_mask], target_tokens))\n                    loss_masks = np.concatenate((np.zeros(len(source_tokens) + 1, dtype=np.long), target_masks))\n                    token_batch.append(tokens)\n                    target_batch.append(targets)\n                    loss_mask_batch.append(loss_masks)\n                    position_ids = np.arange(len(source_tokens) + len(target_tokens) + 1, dtype=np.long)\n                    position_ids[len(source_tokens) + 1:] = len(source_tokens)\n                    if self.block_position_encoding:\n                        block_position_ids = np.concatenate(\n                            (np.zeros(len(source_tokens), dtype=np.long),\n                             np.arange(len(target_tokens) + 1, dtype=np.long)))\n                    else:\n                        block_position_ids = np.concatenate((np.zeros(len(source_tokens) + 1, dtype=np.long),\n                                                             np.ones(len(target_tokens) + 1, dtype=np.long)))\n                    position_id_batch.append(np.stack([position_ids, block_position_ids], axis=0))\n                else:\n                    tokens, targets, loss_masks, position_ids = self.generate_blank_data(sample, [generation_length],\n                                                                                         attention_mask[-1], rng,\n                                                                                         task='generation')\n                    token_batch.append(tokens)\n                    target_batch.append(targets)\n                    loss_mask_batch.append(loss_masks)\n                    position_id_batch.append(position_ids)\n                    if tokens is None:\n                        print(sample, generation_length, multiple_doc)\n        if self.encoder_decoder:\n            return {\n                'text': torch.tensor(source_batch, dtype=torch.long),\n                'target': torch.tensor(target_batch, dtype=torch.long),\n                'loss_mask': torch.tensor(loss_mask_batch, dtype=torch.long)}\n        else:\n            token_batch, target_batch, loss_mask_batch, position_id_batch = self.pad_batch(token_batch, target_batch,\n                                                                                           loss_mask_batch,\n                                                                                           position_id_batch)\n            return {'text': torch.tensor(token_batch, dtype=torch.long),\n                    'target': torch.tensor(target_batch, dtype=torch.long),\n                    'loss_mask': torch.tensor(loss_mask_batch, dtype=torch.long),\n                    'position_id': torch.tensor(position_id_batch, dtype=torch.long),\n                    'attention_mask': torch.tensor(attention_mask, dtype=torch.long),\n                    'mode': mode}\n\n    @staticmethod\n    def pad_batch(token_batch, target_batch, loss_mask_batch, position_id_batch):\n        seq_lengths = list(map(len, token_batch))\n        if seq_lengths.count(seq_lengths[0]) != len(seq_lengths):\n            max_length = max(seq_lengths)\n            token_batch = [np.concatenate((tokens, np.zeros(max_length - len(tokens), dtype=np.long))) for tokens in\n                           token_batch]\n            target_batch = [np.concatenate((targets, np.zeros(max_length - len(targets), dtype=np.long))) for\n                            targets in\n                            target_batch]\n            loss_mask_batch = [np.concatenate((loss_masks, np.zeros(max_length - len(loss_masks), dtype=np.long)))\n                               for loss_masks in loss_mask_batch]\n            position_id_batch = [\n                np.concatenate((position_ids, np.zeros((2, max_length - position_ids.shape[1]), dtype=np.long)),\n                               axis=1) for position_ids in position_id_batch]\n        return token_batch, target_batch, loss_mask_batch, position_id_batch\n"
        },
        {
          "name": "change_mp.py",
          "type": "blob",
          "size": 6.880859375,
          "content": "import sys\nimport os\nimport torch\nimport copy\n\ncheckpoint = sys.argv[1]\ntarget_mp = int(sys.argv[2])\n\nassert os.path.isdir(checkpoint)\niteration_file = os.path.join(checkpoint, 'latest_checkpointed_iteration.txt')\nif not os.path.exists(iteration_file):\n    iteration_file = os.path.join(checkpoint, 'latest')\nif os.path.exists(iteration_file):\n    with open(iteration_file) as fin:\n        iteration = int(fin.read().strip())\n    checkpoint = os.path.join(checkpoint, str(iteration))\nelse:\n    iteration = None\n\nfilenames = os.listdir(checkpoint)\nfilenames = [filename for filename in filenames if filename.startswith(\"mp_rank_\")]\nfilenames = sorted(filenames,\n                   key=lambda x: int(x.split('_')[2]))\nfilenames = [os.path.join(checkpoint, x) for x in filenames]\n\nif target_mp == len(filenames):\n    print(\"MP size keeps the same.\")\n    exit(0)\n\nif sys.argv[1][-1] == '/':\n    new_checkpoint = sys.argv[1][:-1] + '_MP' + sys.argv[2]\nelse:\n    new_checkpoint = sys.argv[1] + '_MP' + sys.argv[2]\nif not os.path.exists(new_checkpoint):\n    os.mkdir(new_checkpoint)\nif iteration is not None:\n    with open(os.path.join(new_checkpoint, 'latest_checkpointed_iteration.txt'), 'w') as fout:\n        fout.write(\"{}\\n\".format(iteration))\n    with open(os.path.join(new_checkpoint, 'latest'), 'w') as fout:\n        fout.write(\"{}\\n\".format(iteration))\n    new_checkpoint = os.path.join(new_checkpoint, str(iteration))\n    if not os.path.exists(new_checkpoint):\n        os.mkdir(new_checkpoint)\n\npreserve_keys = [\n    \"lr_scheduler\",\n    \"skipped_steps\",\n    \"global_steps\",\n    \"global_samples\",\n    \"dp_world_size\",\n    \"iteration\",\n    \"client_lr_scheduler\",\n    \"np_rng_state\",\n    \"random_rng_state\",\n    \"torch_rng_state\",\n    \"cuda_rng_state\",\n    \"rng_tracker_states\",\n\n]\n\nif target_mp < len(filenames):\n    print(\"Decrease MP size.\")\n    assert len(filenames) % target_mp == 0\n    ratio = len(filenames) // target_mp\n    for i in range(target_mp):\n        start = ratio * i\n        end = ratio * (i + 1)\n        d = torch.load(filenames[start],\n                       map_location='cpu')\n        for k in d.keys():\n            if k != 'module':\n                if k in preserve_keys:\n                    pass\n                elif k == \"mp_world_size\":\n                    d[k] = target_mp\n                else:\n                    d[k] = None\n        for j in range(start + 1, end):\n            d_new = torch.load(filenames[j],\n                               map_location='cpu')\n            for k, v in d_new['module'].items():\n                assert len(v.shape) < 3\n                if len(v.shape) == 2 and 'position' not in k:\n                    if 'query' in k:\n                        size_1 = d['module'][k].shape[0] // 3\n                        size_2 = v.shape[0] // 3\n                        target = d['module'][k]\n                        d['module'][k] = torch.cat([\n                            target[:size_1, :], v[:size_2, :],\n                            target[size_1:size_1 * 2, :], v[size_2:size_2 * 2, :],\n                            target[size_1 * 2:, :], v[size_2 * 2:, :]], 0)\n                    elif 'word' in k or 'h_to_4h' in k or 'relative' in k or \"r_w_bias\" in k or \"r_r_bias\" in k:\n                        d['module'][k] = torch.cat([d['module'][k], v], 0)\n                    else:\n                        d['module'][k] = torch.cat([d['module'][k], v], 1)\n                elif len(v.shape) == 1 and 'query_key_value' in k:\n                    size_1 = d['module'][k].shape[0] // 3\n                    size_2 = v.shape[0] // 3\n                    target = d['module'][k]\n                    d['module'][k] = torch.cat([\n                        target[:size_1], v[:size_2],\n                        target[size_1:size_1 * 2], v[size_2:size_2 * 2],\n                        target[size_1 * 2:], v[size_2 * 2:]], 0)\n                elif len(v.shape) == 1 and ('dense_h_to_4h' in k or \"attention.relative\" in k):\n                    d['module'][k] = torch.cat([d['module'][k], v], 0)\n        filename = os.path.join(new_checkpoint, \"mp_rank_{:02d}_model_states.pt\".format(i))\n        torch.save(d, filename)\n\nif target_mp > len(filenames):\n    print(\"Increase MP size.\")\n    assert target_mp % len(filenames) == 0\n    ratio = target_mp // len(filenames)\n    for i in range(len(filenames)):\n        start = ratio * i\n        end = ratio * (i + 1)\n        d = torch.load(filenames[i],\n                       map_location='cpu')\n        for j in range(start, end):\n            d_new = {}\n            shift = j - start\n            for k, v in d.items():\n                if k != 'module':\n                    if k in preserve_keys:\n                        d_new[k] = copy.deepcopy(d[k])\n                    elif k == \"mp_world_size\":\n                        d_new[k] = target_mp\n                    else:\n                        d_new[k] = None\n            d_new['module'] = {}\n            with torch.no_grad():\n                for k, v in d['module'].items():\n                    assert len(v.shape) < 3\n                    if len(v.shape) == 2 and 'position' not in k:\n                        if 'query' in k:\n                            part = v.shape[0] // ratio // 3\n                            d_new['module'][k] = torch.cat([v[shift * part:(shift + 1) * part, :].clone(),\n                                                            v[(shift + ratio) * part:(shift + 1 + ratio) * part,\n                                                            :].clone(),\n                                                            v[(shift + 2 * ratio) * part:(shift + 1 + 2 * ratio) * part,\n                                                            :].clone()], 0)\n                        elif 'word' in k or 'h_to_4h' in k or 'relative' in k or \"r_w_bias\" in k or \"r_r_bias\" in k:\n                            part = v.shape[0] // ratio\n                            d_new['module'][k] = v[shift * part:(shift + 1) * part, :].clone()\n                        else:\n                            part = v.shape[1] // ratio\n                            d_new['module'][k] = v[:, shift * part:(shift + 1) * part].clone()\n                    elif len(v.shape) == 1 and ('dense_h_to_4h' in k or \"attention.relative\" in k):\n                        part = v.shape[0] // ratio\n                        d_new['module'][k] = v[shift * part:(shift + 1) * part].clone()\n                    elif len(v.shape) == 1 and 'query_key_value' in k:\n                        part = v.shape[0] // ratio // 3\n                        d_new['module'][k] = torch.cat(\n                            [v[shift * part:(shift + 1) * part].clone(),\n                             v[(shift + ratio) * part:(shift + 1 + ratio) * part].clone(),\n                             v[(shift + 2 * ratio) * part:(shift + 1 + 2 * ratio) * part].clone()], 0)\n                    else:\n                        d_new['module'][k] = v.clone()\n            filename = os.path.join(new_checkpoint, \"mp_rank_{:02d}_model_states.pt\".format(j))\n            torch.save(d_new, filename)\n"
        },
        {
          "name": "chinese_sentencepiece",
          "type": "tree",
          "content": null
        },
        {
          "name": "config",
          "type": "tree",
          "content": null
        },
        {
          "name": "config_tasks",
          "type": "tree",
          "content": null
        },
        {
          "name": "configure_data.py",
          "type": "blob",
          "size": 17.908203125,
          "content": "# coding=utf-8\n# Copyright (c) 2019, NVIDIA CORPORATION.  All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"parses arguments and preps data loader\"\"\"\n\nimport os\nimport copy\nimport random\nimport numpy as np\nimport torch\nimport torch.utils.data\nimport data_utils\nfrom blocklm_utils import ConstructBlockStrategy\nfrom data_utils.tokenization import make_tokenizer\nfrom utils import print_rank_0\nfrom itertools import accumulate\nfrom bisect import bisect_right\nfrom tasks.superglue.dataset import SuperGlueDataset\n\nimport mpu\n\n\nclass MultiTaskDataset(torch.utils.data.Dataset):\n    def __init__(self, tasks, datasets, reweight=True, temperature=0.8, max_limit=200000):\n        super(MultiTaskDataset, self).__init__()\n        self.tasks = tasks\n        self.datasets = datasets\n        self.reweight = reweight\n        self.temperature = temperature\n        self.lens = [len(dataset) for dataset in datasets]\n        self.weights = np.array([min(l, max_limit) ** temperature for l in self.lens])\n        self.total_len = sum(self.lens)\n        self.cumulative_lens = list(accumulate(self.lens))\n        if self.reweight:\n            print_rank_0(list(zip(self.tasks, self.lens, self.weights)))\n        else:\n            print_rank_0(list(zip(self.tasks, self.lens)))\n        self.weights /= self.weights.sum()\n\n    def __len__(self):\n        return self.total_len * 1000\n\n    @staticmethod\n    def pet_wrapper(data):\n        text = data['text']\n        loss_mask = data['logit_mask']\n        target = data['target']\n        attention_mask = data['mask']\n        position_id = data['position']\n        label = data['label']\n        if len(text.shape) == 2:\n            text = text[label]\n            loss_mask = loss_mask[label]\n            target = target[label]\n            attention_mask = attention_mask[label]\n            position_id = position_id[label]\n        else:\n            target = target[label]\n        if not target.shape:\n            target = target.repeat(len(text))\n        return {'text': text, 'target': target, 'loss_mask': loss_mask, 'position_id': position_id,\n                'attention_mask': attention_mask}\n\n    def __getitem__(self, idx):\n        if self.reweight:\n            rng = random.Random(idx)\n            rng = np.random.RandomState(seed=[rng.randint(0, 2 ** 32 - 1) for _ in range(16)])\n            dataset_idx = rng.choice(np.arange(len(self.datasets)), p=self.weights)\n            dataset = self.datasets[dataset_idx]\n            sample_idx = rng.choice(np.arange(len(dataset)))\n            item = self.datasets[dataset_idx][sample_idx]\n        else:\n            dataset_idx = bisect_right(self.cumulative_lens, idx)\n            if dataset_idx == 0:\n                sample_idx = idx\n            else:\n                sample_idx = idx - self.cumulative_lens[dataset_idx - 1]\n            item = self.datasets[dataset_idx][sample_idx]\n        item = self.pet_wrapper(item)\n        return item\n\n\nclass DataConfig:\n\n    def __init__(self, defaults=None):\n        super(DataConfig, self).__init__()\n        if defaults is None:\n            defaults = {}\n        self.defaults = defaults\n\n    def apply(self, args, tokenizer):\n        if torch.distributed.get_rank() == 0:\n            print('configuring data')\n        self.apply_defaults(args)\n        return make_loaders(args, tokenizer)\n\n    def set_defaults(self, **kwargs):\n        for k, v in kwargs.items():\n            self.defaults[k] = v\n\n    def apply_defaults(self, args):\n        for k, v in self.defaults.items():\n            k = k.replace('-', '_')\n            if not hasattr(args, k):\n                setattr(args, k, v)\n\n\ndef prepare_tokenizer(args):\n    add_sentinel_token = 0\n    if args.sentinel_token:\n        add_sentinel_token = args.max_position_embeddings\n    tokenizer = make_tokenizer(args.tokenizer_type, None, args.tokenizer_path, args.vocab_size,\n                               args.tokenizer_model_type, add_block_symbols=args.block_lm, cache_dir=args.cache_dir,\n                               add_sentinel_token=add_sentinel_token, add_task_mask=args.task_mask,\n                               add_decoder_mask=args.block_mask_prob > 0.0 or args.context_mask_ratio > 0.0,\n                               fix_command_token=args.fix_command_token)\n    if mpu.get_model_parallel_rank() == 0:\n        num_tokens = tokenizer.num_tokens\n        eod_token = tokenizer.get_command('eos').Id\n        assert eod_token == tokenizer.get_command('pad').Id\n        before = num_tokens\n        after = before\n        multiple = args.make_vocab_size_divisible_by\n        while (after % multiple) != 0:\n            after += 1\n        print_rank_0('> padded vocab (size: {}) with {} dummy '\n                     'tokens (new size: {})'.format(before, after - before, after))\n        print_rank_0('> found end-of-document token: {}'.format(eod_token))\n        token_counts = torch.cuda.LongTensor([after, eod_token])\n    else:\n        token_counts = torch.cuda.LongTensor([0, 0])\n    # Broadcast num tokens.\n    torch.distributed.broadcast(token_counts,\n                                mpu.get_model_parallel_src_rank(),\n                                group=mpu.get_model_parallel_group())\n    num_tokens = token_counts[0].item()\n    eod_token = token_counts[1].item()\n    args.vocab_size, args.eod_token = num_tokens, eod_token\n    return tokenizer\n\n\ndef make_data_loader(dataset, tokenizer, batch_size, num_iters, args, shuffle=False, block_collate=False):\n    world_size = torch.distributed.get_world_size(group=mpu.get_data_parallel_group())\n    rank = torch.distributed.get_rank(group=mpu.get_data_parallel_group())\n    if args.loader_scatter is not None:\n        rank = rank // args.loader_scatter\n        world_size = world_size // args.loader_scatter\n        batch_size = batch_size // args.loader_scatter\n    distributed = world_size > 1\n    if args.transformer_xl:\n        batch_sampler = data_utils.samplers.DistributedSequentialSampler(len(dataset),\n                                                                         num_iters,\n                                                                         batch_size,\n                                                                         rank,\n                                                                         world_size)\n    else:\n        if shuffle:\n            sampler = data_utils.samplers.RandomSampler(dataset, replacement=True,\n                                                        num_samples=batch_size * args.train_iters * args.gradient_accumulation_steps)\n        else:\n            sampler = torch.utils.data.SequentialSampler(dataset)\n        drop_last = distributed\n        # the GPUs in the same model parallel group receive the same data\n        if distributed:\n            batch_sampler = data_utils.samplers.DistributedBatchSampler(sampler, batch_size, drop_last, rank,\n                                                                        world_size,\n                                                                        gradient_accumulation_steps=args.gradient_accumulation_steps)\n        else:\n            batch_sampler = torch.utils.data.BatchSampler(sampler,\n                                                          batch_size,\n                                                          drop_last)\n    collate_fn = None\n    if block_collate:\n        collate_fn = ConstructBlockStrategy(args, tokenizer, args.seq_length, bert_prob=args.bert_prob,\n                                            gap_sentence_prob=args.gap_sentence_prob,\n                                            gap_sentence_ratio=args.gap_sentence_ratio,\n                                            gpt_infill_prob=args.gpt_infill_prob,\n                                            average_block_length=args.avg_block_length,\n                                            gpt_min_ratio=args.gpt_min_ratio,\n                                            block_mask_prob=args.block_mask_prob,\n                                            context_mask_ratio=args.context_mask_ratio,\n                                            short_seq_prob=args.short_seq_prob,\n                                            single_span_prob=args.single_span_prob,\n                                            shuffle_blocks=not args.no_shuffle_block,\n                                            block_position_encoding=not args.no_block_position,\n                                            sentinel_token=args.sentinel_token,\n                                            encoder_decoder=args.encoder_decoder,\n                                            task_mask=args.task_mask, random_position=args.random_position,\n                                            masked_lm=args.masked_lm).construct_blocks\n    data_loader = torch.utils.data.DataLoader(dataset,\n                                              batch_sampler=batch_sampler,\n                                              num_workers=args.num_workers,\n                                              pin_memory=True,\n                                              collate_fn=collate_fn)\n\n    return data_loader\n\n\ndef make_tfrecord_loaders(args):\n    \"\"\"Load train/val/test dataset from shuffled TFRecords\"\"\"\n\n    import data_utils.tf_dl\n    data_set_args = {'batch_size': args.batch_size,\n                     'max_seq_len': args.seq_length,\n                     'max_preds_per_seq': args.max_preds_per_seq,\n                     'train': True,\n                     'num_workers': max(args.num_workers, 1),\n                     'seed': args.seed + args.rank + 1,\n                     'threaded_dl': args.num_workers > 0\n                     }\n    train = data_utils.tf_dl.TFRecordDataLoader(args.train_data,\n                                                **data_set_args)\n    data_set_args['train'] = False\n    if args.eval_seq_length is not None:\n        data_set_args['max_seq_len'] = args.eval_seq_length\n    if args.eval_max_preds_per_seq is not None:\n        data_set_args['max_preds_per_seq'] = args.eval_max_preds_per_seq\n    valid = None\n    if args.valid_data is not None:\n        valid = data_utils.tf_dl.TFRecordDataLoader(args.valid_data,\n                                                    **data_set_args)\n    test = None\n    if args.test_data is not None:\n        test = data_utils.tf_dl.TFRecordDataLoader(args.test_data,\n                                                   **data_set_args)\n    tokenizer = data_utils.make_tokenizer(args.tokenizer_type,\n                                          train,\n                                          args.tokenizer_path,\n                                          args.vocab_size,\n                                          args.tokenizer_model_type,\n                                          cache_dir=args.cache_dir)\n\n    return (train, valid, test), tokenizer\n\n\ndef make_loaders(args, tokenizer):\n    \"\"\"makes training/val/test\"\"\"\n\n    if args.use_tfrecords:\n        return make_tfrecord_loaders(args)\n    world_size = torch.distributed.get_world_size(group=mpu.get_data_parallel_group())\n    if args.loader_scatter is not None:\n        assert world_size % args.loader_scatter == 0\n    batch_size = args.batch_size * world_size\n    eval_batch_size = batch_size\n    if args.eval_batch_size is not None:\n        eval_batch_size = args.eval_batch_size * world_size\n    seq_length = args.seq_length\n    if seq_length < 0:\n        seq_length = seq_length * world_size\n    eval_seq_length = args.eval_seq_length\n    if eval_seq_length is not None and eval_seq_length < 0:\n        eval_seq_length = eval_seq_length * world_size\n    split = get_split(args)\n    data_set_args = {\n        'path': args.train_data,\n        'seq_length': seq_length,\n        'mem_length': args.mem_length,\n        'delim': args.delim,\n        'text_key': args.text_key,\n        'label_key': 'label',\n        'ds_type': args.data_set_type,\n        'split': split,\n        'loose': args.loose_json,\n        'max_preds_per_seq': args.max_preds_per_seq,\n        'presplit_sentences': args.presplit_sentences,\n        'sample_one_document': args.sample_one_document,\n        'filter_english': args.filter_english,\n        'pre_tokenize': not args.no_pre_tokenize,\n        'tokenizer': tokenizer,\n        'save_splits': args.save_splits,\n        'load_splits': args.load_splits,\n        'save_test_data': args.save_test_data,\n        'no_lazy_loader': args.no_lazy_loader,\n        'loader_scatter': args.loader_scatter,\n        'data_parallel_rank': mpu.get_data_parallel_rank(),\n        \"non_sentence_start\": args.non_sentence_start,\n        \"half_lazy_loader\": args.half_lazy_loader\n    }\n\n    eval_set_args = copy.copy(data_set_args)\n    eval_set_args['split'] = [1.]\n    # if optional eval args were set then replace their\n    # equivalent values in the arg dict\n    if eval_seq_length:\n        eval_set_args['seq_length'] = eval_seq_length\n    if args.eval_max_preds_per_seq:\n        eval_set_args['max_preds_per_seq'] = args.eval_max_preds_per_seq\n    if args.eval_text_key is not None:\n        eval_set_args['text_key'] = args.eval_text_key\n\n    # make datasets splits and tokenizer\n    train, valid, test = None, None, None\n\n    if args.train_data is not None:\n        train = data_utils.make_dataset(**data_set_args)\n        if data_utils.should_split(split):\n            train, valid, test = train\n        eval_set_args['tokenizer'] = tokenizer\n\n    # make training and val dataset if necessary\n    if valid is None and args.valid_data is not None:\n        eval_set_args['path'] = args.valid_data\n        valid = data_utils.make_dataset(**eval_set_args)\n        eval_set_args['tokenizer'] = tokenizer\n    if test is None and args.test_data is not None:\n        eval_set_args['path'] = args.test_data\n        test = data_utils.make_dataset(**eval_set_args)\n\n    # wrap datasets with data loader\n    use_block = args.block_lm or args.encoder_decoder\n\n    if train is not None and args.batch_size > 0:\n        train = make_data_loader(train, tokenizer, batch_size, args.train_iters, args, shuffle=args.shuffle,\n                                 block_collate=use_block)\n        args.do_train = True\n    else:\n        args.do_train = False\n    eval_batch_size = eval_batch_size if eval_batch_size != 0 else batch_size\n    if valid is not None:\n        valid = make_data_loader(valid, tokenizer, eval_batch_size, args.train_iters, args, shuffle=args.shuffle,\n                                 block_collate=use_block)\n        args.do_valid = True\n    else:\n        args.do_valid = False\n    if test is not None:\n        test = make_data_loader(test, tokenizer, eval_batch_size, len(test) // eval_batch_size + 1, args,\n                                shuffle=args.shuffle, block_collate=use_block)\n        args.do_test = True\n    else:\n        args.do_test = False\n\n    return train, valid, test\n\n\ndef build_multi_task_dataset(args, tokenizer):\n    task_dirs = {\"mnli\": \"MNLI\", \"cola\": \"CoLA\", \"mrpc\": \"MRPC\", \"qnli\": \"QNLI\", \"qqp\": \"QQP\", \"sst2\": \"SST-2\",\n                 \"agnews\": \"Agnews\", \"yelp-polarity\": \"yelp_review_polarity_csv\", \"yelp-full\": \"yelp_review_full_csv\",\n                 \"yahoo\": \"Yahoo\", \"squad\": \"SQuAD\", \"race\": \"RACE\"}\n    train, valid = None, None\n    if mpu.get_model_parallel_rank() == 0:\n        multi_seq_length = args.seq_length\n        if args.multi_seq_length is not None:\n            multi_seq_length = args.multi_seq_length\n        train_datasets, valid_datasets = [], []\n        for task in args.multi_task_data:\n            task = task.lower()\n            data_dir = os.path.join(args.data_dir, task_dirs[task])\n            train_datasets.append(\n                SuperGlueDataset(args, task, data_dir, multi_seq_length, \"train\", tokenizer, pattern_ensemble=True))\n            valid_datasets.append(\n                SuperGlueDataset(args, task, data_dir, multi_seq_length, \"dev\", tokenizer, pattern_ensemble=True))\n        train = MultiTaskDataset(args.multi_task_data, train_datasets)\n        valid = MultiTaskDataset(args.multi_task_data, valid_datasets)\n        world_size = torch.distributed.get_world_size(group=mpu.get_data_parallel_group())\n        multi_batch_size = args.batch_size * world_size\n        if args.multi_batch_size is not None:\n            multi_batch_size = args.multi_batch_size * world_size\n        train = make_data_loader(train, tokenizer, multi_batch_size, args.train_iters, args, shuffle=True)\n        valid = make_data_loader(valid, tokenizer, multi_batch_size, args.train_iters, args, shuffle=True)\n    return train, valid\n\n\ndef get_split(args):\n    \"\"\"\n    Get dataset splits from comma separated string list\n    \"\"\"\n    splits = []\n    if args.split.find(',') != -1:\n        splits = [float(s) for s in args.split.split(',')]\n    elif args.split.find('/') != -1:\n        splits = [float(s) for s in args.split.split('/')]\n    else:\n        splits = [float(args.split)]\n    split_total = sum(splits)\n    if split_total < 1.:\n        splits.append(1 - split_total)\n    while len(splits) < 3:\n        splits.append(0.)\n    splits = splits[:3]\n    if args.valid_data is not None:\n        splits[1] = 0.\n    if args.test_data is not None:\n        splits[2] = 0.\n    final_sum = sum(splits)\n    return [s / final_sum for s in splits]\n\n\ndef configure_data():\n    \"\"\"add cmdline flags for configuring datasets\"\"\"\n    # These are options that are used by data_utils, but are either\n    # deprecated or not meant to be exposed to the command line user.\n    # These options are intneded to be set in code by specific scripts.\n    defaults = {\n        'world_size': 1,\n        'rank': -1,\n        'persist_state': 0,\n        'lazy': False,\n        'transpose': False,\n        'data_set_type': 'supervised',\n        'seq_length': 256,\n        'eval_seq_length': 256,\n        'samples_per_shard': 100\n    }\n\n    return DataConfig(defaults=defaults)\n"
        },
        {
          "name": "data_utils",
          "type": "tree",
          "content": null
        },
        {
          "name": "docker",
          "type": "tree",
          "content": null
        },
        {
          "name": "examples",
          "type": "tree",
          "content": null
        },
        {
          "name": "finetune_glm.py",
          "type": "blob",
          "size": 22.060546875,
          "content": "\"\"\"Finetune utilities.\"\"\"\n\nimport os\nimport json\n\nimport random\n\nfrom tasks.data_utils import build_data_loader, FakeDataloader\nfrom utils import get_sample_writer, get_log_dir, print_and_save_args, debug_finetune_data\nfrom arguments import get_args\nfrom filelock import FileLock\nimport pretrain_glm\nfrom pretrain_glm import forward_step as lm_forward_step\nimport pathlib\nimport mpu\n\nimport torch\nimport torch.utils.data\nfrom configure_data import prepare_tokenizer\n\nfrom utils import print_rank_0\nfrom utils import Timers\nfrom train_utils import setup_model_and_optimizer, train_step, load_pretrained\nfrom utils import load_checkpoint, save_checkpoint\nfrom pretrain_glm import report_iteration_metrics\nfrom pretrain_glm import evaluate_and_print_results\nfrom pretrain_glm import initialize_distributed\nfrom pretrain_glm import set_random_seed\nfrom configure_data import make_data_loader\n\n\ndef process_batch(batch, args):\n    \"\"\"Process batch and produce inputs for the model.\"\"\"\n    keys = [\"text\", \"label\"]\n    if args.pretrained_bert:\n        keys += [\"padding_mask\", \"types\"]\n    else:\n        keys += [\"mask\", \"position\"]\n        if args.cloze_eval:\n            if args.fast_decode:\n                keys += [\"dec_text\", \"dec_position\", \"dec_mask\", \"dec_target\", \"dec_logit_mask\"]\n            else:\n                keys += [\"target\", \"logit_mask\"]\n                if args.segment_length > 0:\n                    keys += [\"segment_id\"]\n                if args.continuous_prompt:\n                    keys += [\"prompt_pos\"]\n    if args.variable_num_choices:\n        keys.append(\"loss_mask\")\n    # Broadcast data.\n    datatype = torch.int64\n    data_b = mpu.broadcast_data(keys, batch, datatype)\n\n    if \"padding_mask\" in data_b:\n        attention_mask = data_b['padding_mask'].float().cuda().contiguous()\n        if args.fp16:\n            attention_mask = attention_mask.half()\n        data_b[\"padding_mask\"] = attention_mask\n    return data_b\n\n\ntokenizer = None\n\n\ndef mix_forward_step(batch_and_dataloader, model, args, times, mems):\n    use_blocklm = 0\n    if args.block_lm_ratio > 0.0:\n        if mpu.get_model_parallel_rank() == 0:\n            if random.random() > 1 / (1 + args.block_lm_ratio):\n                use_blocklm = 1\n        use_blocklm = torch.cuda.LongTensor([use_blocklm])\n        torch.distributed.broadcast(use_blocklm, mpu.get_model_parallel_src_rank(),\n                                    group=mpu.get_model_parallel_group())\n        use_blocklm = use_blocklm.item()\n    if use_blocklm:\n        return lm_forward_step((batch_and_dataloader[1], None), model, args, times, mems)\n    else:\n        return finetune_forward_step(batch_and_dataloader[0], model, args, times, mems)\n\n\ndef finetune_forward_step(batch, model, args, timers, mems):\n    \"\"\"Simple forward step with cross-entropy loss.\"\"\"\n    # Get the batch.\n    timers('batch generator').start()\n    try:\n        batch_ = next(batch)\n    except BaseException:\n        batch_ = batch\n\n    data = process_batch(batch_, args)\n    timers('batch generator').stop()\n\n    # Forward model.\n    if args.pretrained_bert:\n        tokens, types, labels, attention_mask = data['text'], data['types'], data['label'], data['padding_mask']\n        logits = model(tokens, token_type_ids=types, attention_mask=attention_mask, checkpoint_activations=True)\n    elif args.cloze_eval:\n        tokens, labels, position_ids = data['text'], data['label'], data['position']\n        attention_mask = data['mask']\n\n        if not args.fast_decode:\n            target_ids, logit_mask = data['target'], data['logit_mask']\n            if args.continuous_prompt:\n                prompt_pos = data[\"prompt_pos\"]\n                result = model(tokens, position_ids, attention_mask, target_ids, logit_mask, prompt_pos=prompt_pos)\n            else:\n                result = model(tokens, position_ids, attention_mask, target_ids, logit_mask)\n            if not args.multi_token:\n                logits, lm_logits, *mems = result\n            else:\n                logits, *mems = result\n        else:\n            dec_input_ids, dec_position_ids, dec_attention_mask = data['dec_text'], data['dec_position'], data[\n                'dec_mask']\n            dec_target_ids, dec_logit_mask = data['dec_target'], data['dec_logit_mask']\n            logits, *mems = model(tokens, position_ids, attention_mask, dec_input_ids, dec_position_ids,\n                                  dec_attention_mask, dec_target_ids, dec_logit_mask)\n    else:\n        tokens, labels, position_ids, attention_mask = data['text'], data['label'], data['position'], data['mask']\n        logits, *mems = model(tokens, position_ids, attention_mask)\n\n    if args.adapet:\n        batch_size, num_classes = logits.size()[:2]\n        label_mask = torch.ones(batch_size, num_classes, device=logits.device)\n        label_mask.scatter_(1, labels.unsqueeze(1), -1.0)\n        if \"loss_mask\" in data:\n            loss_mask = data[\"loss_mask\"]\n            label_mask = label_mask * loss_mask\n        loss = logits.contiguous().float() * label_mask\n        loss = loss.sum() / batch_size\n    else:\n        if \"segment_id\" in data:\n            from torch_scatter import scatter_sum\n            if \"loss_mask\" in data:\n                logits = logits * data[\"loss_mask\"]\n            logits = scatter_sum(logits, data[\"segment_id\"], dim=1)\n        elif \"loss_mask\" in data:\n            loss_mask = data[\"loss_mask\"]\n            logits = logits * loss_mask - 10000.0 * (1.0 - loss_mask)\n        if args.loss_func == \"cross_entropy\":\n            # Cross-entropy loss.\n            loss_func = torch.nn.CrossEntropyLoss()\n            loss = loss_func(logits.contiguous().float(), labels)\n        elif args.loss_func == \"hinge\":\n            correct_logits = logits[range(logits.size(0)), labels]\n            hinge_loss = 1 + logits - correct_logits.unsqueeze(1)\n            hinge_loss[hinge_loss < 0.0] = 0.0\n            loss = hinge_loss.sum(dim=1).mean() - 1.0\n        elif args.loss_func == \"generative\" or args.loss_func == \"mix\":\n            batch_size = logits.size(0)\n            loss = - logits[range(batch_size), labels].mean()\n            if args.loss_func == \"mix\":\n                loss_func = torch.nn.CrossEntropyLoss()\n                loss = loss + loss_func(logits.contiguous().float(), labels)\n        else:\n            raise NotImplementedError\n\n    # Reduce loss for logging.\n\n    return loss, mems, 'bert'\n\n\ndef _build_infinite_size_dataloader(dataloader):\n    \"\"\"Build a looped dataloader with infinite size.\"\"\"\n\n    iterator = dataloader.__iter__()\n    while True:\n        try:\n            yield iterator.__next__()\n        except StopIteration:\n            iterator = dataloader.__iter__()\n\n\ndef _build_train_valid_dataloaders(train_dataset, valid_dataset, args):\n    \"\"\"Traing and validation dataloaders.\"\"\"\n    print_rank_0('building train and validation dataloaders ...')\n    # Training dataset.\n    train_dataloader = build_data_loader(train_dataset, args.batch_size, args.num_workers, drop_last=False)\n    # Set the training iterations.\n    args.train_iters_per_epoch = len(train_dataloader)\n    args.train_iters = args.epochs * args.train_iters_per_epoch\n    # Validation dataset. For this dataset, we do not need to set up\n    # shuffling so we can just use a simple infinite loop.\n    valid_dataloader = None\n    if valid_dataset is not None:\n        valid_dataloader_ = build_data_loader(valid_dataset, args.batch_size,\n                                              args.num_workers, drop_last=False)\n        valid_dataloader = _build_infinite_size_dataloader(valid_dataloader_)\n\n    return train_dataloader, valid_dataloader\n\n\ndef _train(model, optimizer, lr_scheduler, forward_step,\n           train_dataloader, valid_dataloader, end_of_epoch_callback, args, timers, summary_writer=None):\n    \"\"\"Train the model.\"\"\"\n\n    # Turn on training mode which enables dropout.\n    model.train()\n\n    # Tracking loss.\n    args.iteration = 0\n    total_lm_loss = 0.0\n    best_score, best_iteration = 0, None\n    # Starting epoch and iteration\n    start_epoch = args.iteration // args.train_iters_per_epoch\n    start_iteration = args.iteration % args.train_iters_per_epoch\n    if not args.block_lm_ratio:\n        valid_dataloader = valid_dataloader[0]\n    # For each remaining epoch\n    timers('interval time').start()\n    for epoch in range(start_epoch, args.epochs):\n        print_rank_0('working on epoch {} ...'.format(epoch))\n\n        # Set the data loader epoch to shuffle the index iterator.\n        if mpu.get_model_parallel_rank() == 0:\n            train_dataloader[0].sampler.set_epoch(args.seed + epoch)\n\n        # For all the batches in the dataset.\n        for iteration_, batch in enumerate(train_dataloader[0]):\n\n            # Ignore the iterations before starting value\n            if iteration_ < start_iteration:\n                continue\n            # Set to zero so the next epoch does not skip any batches.\n            start_iteration = 0\n\n            # Train for one step.\n            if args.block_lm_ratio > 0.0:\n                data = (batch, train_dataloader[1])\n            else:\n                data = batch\n            lm_loss, skipped_iter, _ = train_step(data, model, optimizer, lr_scheduler, args,\n                                                  timers, forward_step_func=forward_step, single_step=True)\n            args.iteration += 1\n            total_lm_loss += lm_loss.data.detach().float()\n\n            # Logging.\n            if args.iteration % args.log_interval == 0:\n                learning_rate = optimizer.param_groups[0]['lr']\n                avg_lm_loss = total_lm_loss.item() / args.log_interval\n                elapsed_time = timers('interval time').elapsed()\n                timers.log(['forward', 'backward', 'allreduce', 'optimizer', 'batch generator'],\n                           normalizer=args.log_interval)\n                report_iteration_metrics(summary_writer, optimizer, learning_rate, avg_lm_loss,\n                                         elapsed_time * 1000.0 / args.log_interval, args.iteration, args.train_iters,\n                                         args)\n                total_lm_loss = 0.0\n\n            # Evaluation\n            if args.eval_interval and valid_dataloader is not None and args.iteration % args.eval_interval == 0:\n                prefix = 'iteration {}'.format(args.iteration)\n                evaluate_and_print_results(prefix, valid_dataloader, model, args, timers, step=args.iteration,\n                                           verbose=False, forward_step_func=forward_step,\n                                           summary_writer=summary_writer)\n\n        # Checkpointing at the end of each epoch.\n        if args.save and (epoch + 1) % args.save_epoch == 0:\n            save_checkpoint(args.iteration, model, optimizer, lr_scheduler, args, only_changed_parameters=True)\n\n        # Callback at the end of each epoch.\n        if end_of_epoch_callback is not None and (epoch + 1) % args.eval_epoch == 0:\n            score_dict = end_of_epoch_callback(model, epoch, summary_writer=summary_writer)\n            if score_dict:\n                validation_metric = args.validation_metric if args.validation_metric else list(score_dict.keys())[0]\n                validation_score = score_dict[validation_metric]\n                if best_iteration is None or validation_score > best_score:\n                    best_iteration = args.iteration\n                    best_score = validation_score\n                    print_rank_0(f\"Found best {validation_metric} {best_score} at {best_iteration}\")\n                    save_checkpoint(args.iteration, model, optimizer, lr_scheduler, args, tag=\"best\", barrier=False,\n                                    only_changed_parameters=True, no_deepspeed=True, no_save_optim=True)\n                    if torch.distributed.get_rank() == 0:\n                        score_dict.update({\"type\": \"validation\", \"epoch\": epoch})\n                        with open(os.path.join(args.log_dir, \"results.json\"), \"w\") as output:\n                            output.write(json.dumps(score_dict) + \"\\n\")\n                        with open(os.path.join(args.save, \"best_checkpointed_iteration.txt\"), \"w\") as output:\n                            output.write(str(best_iteration))\n    torch.distributed.barrier()\n    return best_iteration\n\n\ndef finetune(args, train_valid_datasets_provider, model_kwargs, forward_step=finetune_forward_step,\n             end_of_epoch_callback_provider=None):\n    \"\"\"Main finetune function used across all tasks.\"\"\"\n    global tokenizer\n    timers = Timers()\n    tokenizer = prepare_tokenizer(args)\n    pretrain_glm.tokenizer = tokenizer\n    if args.save:\n        args.save = os.path.join(args.save, args.experiment_name)\n    # Train and validation data loaders.\n    timers('train/valid/test dataset/dataloder').start()\n    train_dataloader, valid_dataloader = None, None\n    train_block_dataloader, valid_block_dataloader = None, None\n    if train_valid_datasets_provider is not None and args.epochs > 0:\n        if mpu.get_model_parallel_rank() == 0:\n            train_dataset, valid_dataset = train_valid_datasets_provider(args, tokenizer)\n            train_dataloader, valid_dataloader = _build_train_valid_dataloaders(train_dataset, valid_dataset, args)\n            if args.no_validation:\n                valid_dataloader = None\n            train_iters = torch.cuda.LongTensor([len(train_dataloader)])\n        else:\n            train_iters = torch.cuda.LongTensor([0])\n        torch.distributed.broadcast(train_iters, mpu.get_model_parallel_src_rank(),\n                                    group=mpu.get_model_parallel_group())\n        if mpu.get_model_parallel_rank() != 0:\n            args.train_iters_per_epoch = train_iters[0].item()\n            args.train_iters = args.epochs * args.train_iters_per_epoch\n\n            train_dataloader = FakeDataloader(args.train_iters_per_epoch)\n            if args.no_validation:\n                valid_dataloader = None\n            else:\n                valid_dataloader = FakeDataloader(None)\n        if args.block_lm_ratio > 0.0:\n            if mpu.get_model_parallel_rank() == 0:\n                train_block_dataset, valid_block_dataset = train_valid_datasets_provider(args, tokenizer,\n                                                                                         pattern_text=True)\n                train_block_dataloader = make_data_loader(train_block_dataset, tokenizer,\n                                                          args.batch_size * mpu.get_data_parallel_world_size(),\n                                                          args.train_iters, args, shuffle=True,\n                                                          block_collate=True)\n                valid_block_dataloader = make_data_loader(valid_block_dataset, tokenizer,\n                                                          args.batch_size * mpu.get_data_parallel_world_size(), (\n                                                                  args.train_iters // args.eval_interval + 1) * args.eval_iters,\n                                                          args, shuffle=True, block_collate=True)\n            else:\n                train_block_dataloader = FakeDataloader(args.train_iters)\n                valid_block_dataloader = FakeDataloader(None)\n            train_block_dataloader, valid_block_dataloader = iter(train_block_dataloader), iter(valid_block_dataloader)\n\n    timers('train/valid/test dataset/dataloder').stop()\n    # Build calback function.\n    timers('callback function').start()\n    end_of_epoch_callback, end_of_train_callback = None, None\n    if end_of_epoch_callback_provider is not None:\n        if train_valid_datasets_provider is not None and args.epochs > 0 and not args.no_validation:\n            end_of_epoch_callback = end_of_epoch_callback_provider(args, tokenizer, is_test=False)\n        end_of_train_callback = end_of_epoch_callback_provider(args, tokenizer, is_test=True)\n    timers('callback function').stop()\n\n    # Build model, optimizer and learning rate scheduler.\n    timers('model and optimizer').start()\n    model, optimizer, lr_scheduler = setup_model_and_optimizer(args, **model_kwargs)\n    timers('model and optimizer').stop()\n\n    # If pretrained checkpoint is provided and we have not trained for\n    # any iteration (i.e., iteration is zero), then load the pretrained\n    # checkpoint.\n    timers('pretrained checkpoint').start()\n    if args.load_pretrained is not None and not args.pretrained_bert:\n        task_tokens = None\n        if args.continuous_prompt and args.prompt_init:\n            if mpu.get_model_parallel_rank() == 0:\n                dataset = train_dataloader.dataset\n                processor, pvp = dataset.processor, dataset.pvp\n                task_tokens = []\n                for label in processor.get_labels():\n                    verbalizer = pvp.verbalize(label)[0]\n                    verbalizer_ids = tokenizer.EncodeAsIds(verbalizer).tokenization\n                    task_tokens += verbalizer_ids\n                print_rank_0(\"Task tokens: \" + tokenizer.DecodeIds(task_tokens))\n                num_task_tokens = len(task_tokens)\n            else:\n                num_task_tokens, task_tokens = 0, []\n            num_task_tokens = torch.cuda.LongTensor([num_task_tokens])\n            torch.distributed.broadcast(num_task_tokens, mpu.get_model_parallel_src_rank(),\n                                        group=mpu.get_model_parallel_group())\n            num_task_tokens = num_task_tokens.item()\n            if num_task_tokens > 0:\n                if mpu.get_model_parallel_rank() == 0:\n                    task_tokens = torch.cuda.LongTensor(task_tokens)\n                else:\n                    task_tokens = torch.empty(num_task_tokens, device=torch.cuda.current_device(), dtype=torch.long)\n                torch.distributed.broadcast(task_tokens, mpu.get_model_parallel_src_rank(),\n                                            group=mpu.get_model_parallel_group())\n                task_tokens = task_tokens.tolist()\n        with FileLock(os.path.join(pathlib.Path.home(), \"checkpoint_lock\"), timeout=-1):\n            load_pretrained(model, args.load_pretrained, args, task_tokens=task_tokens)\n        # This is critical when only model is loaded. We should make sure\n        # master parameters are also updated.\n        if args.fp16 and optimizer is not None:\n            if args.deepspeed:\n                optimizer.refresh_fp32_params()\n            else:\n                optimizer._model_params_to_master_params()\n    if args.load is not None:\n        with FileLock(os.path.join(pathlib.Path.home(), \"checkpoint_lock\"), timeout=-1):\n            load_checkpoint(model, optimizer, lr_scheduler, args, no_deepspeed=args.no_deepspeed_load)\n        # This is critical when only model is loaded. We should make sure\n        # master parameters are also updated.\n        if args.fp16 and optimizer is not None:\n            if args.deepspeed:\n                optimizer.refresh_fp32_params()\n            else:\n                optimizer._model_params_to_master_params()\n    torch.distributed.barrier()\n    timers('pretrained checkpoint').stop()\n    args.iteration = 0\n    summary_writer = None\n    if torch.distributed.get_rank() == 0:\n        args.log_dir = get_log_dir(base=args.summary_dir, name=args.experiment_name)\n        if os.path.exists(os.path.join(args.log_dir, \"test_results.json\")) and args.load is None and not args.overwrite:\n            raise ValueError(\"Output directory ({}) already exists and is not empty.\".format(args.log_dir))\n        summary_writer = get_sample_writer(log_dir=args.log_dir, iteration=args.iteration)\n        print_and_save_args(args, verbose=True, log_dir=args.log_dir)\n\n    # Print setup timing.\n    print_rank_0('done with setups ...')\n    timers.log(['train/valid/test dataset/dataloder', 'callback function',\n                'model and optimizer', 'pretrained checkpoint'])\n    print_rank_0('training ...')\n\n    # Finetune the model.\n    score_dict = None\n    if train_dataloader is not None and args.epochs > 0:\n        if args.block_lm_ratio > 0.0:\n            forward_step = mix_forward_step\n        best_iteration = _train(model, optimizer, lr_scheduler, forward_step,\n                                (train_dataloader, train_block_dataloader), (valid_dataloader, valid_block_dataloader),\n                                end_of_epoch_callback, args, timers,\n                                summary_writer=summary_writer)\n        if end_of_train_callback is not None and best_iteration is not None:\n            with FileLock(os.path.join(pathlib.Path.home(), \"checkpoint_lock\"), timeout=-1):\n                args.load = os.path.join(args.save, \"best\")\n                load_checkpoint(model, optimizer, lr_scheduler, args, no_load_optim=True, no_deepspeed=True)\n                args.load = None\n        torch.distributed.barrier()\n        if end_of_train_callback is not None:\n            score_dict = end_of_train_callback(model, epoch=-1, output_predictions=True)\n    # Or just evaluate.\n    else:\n        if end_of_train_callback is not None:\n            print_rank_0('evaluation only mode, setting epoch to -1')\n            score_dict = end_of_train_callback(model, epoch=-1, output_predictions=True)\n    if score_dict is not None and torch.distributed.get_rank() == 0:\n        score_dict.update({\"type\": \"test\"})\n        with open(os.path.join(args.log_dir, \"test_results.json\"), \"w\") as output:\n            output.write(json.dumps(score_dict) + \"\\n\")\n\n    print_rank_0('done :-)')\n\n\nif __name__ == '__main__':\n    # Disable CuDNN.\n    torch.backends.cudnn.enabled = False\n\n    # Arguments.\n    args = get_args()\n    assert args.finetune\n\n    # Pytorch distributed.\n    initialize_distributed(args)\n\n    # Random seeds for reproducability.\n    set_random_seed(args.seed)\n    from tasks.superglue.dataset import PROCESSORS\n\n    superglue_tasks = list(PROCESSORS.keys())\n    if args.task.lower() in superglue_tasks or args.task.lower() == \"multichoice\":\n        from tasks.superglue.finetune import main\n    elif args.task.lower() in ['lambda', 'wikitext', 'language_model']:\n        from tasks.language_model.finetune import main\n    elif args.task.lower() in ['cnn_dm', 'cnn_dm_original', 'gigaword', 'blank', 'squad_generation', 'squad',\n                               'squad_v1', 'xsum', 'extraction', 'cmrc', 'customization']:\n        from tasks.seq2seq.finetune import main\n    else:\n        raise NotImplementedError('Task {} is not implemented.'.format(args.task))\n\n    main(args)\n"
        },
        {
          "name": "fp16",
          "type": "tree",
          "content": null
        },
        {
          "name": "generate_samples.py",
          "type": "blob",
          "size": 13.5205078125,
          "content": "# coding=utf-8\n# Copyright (c) 2019, NVIDIA CORPORATION.  All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Sample Generate GPT2\"\"\"\n\nimport os\nimport torch\nimport torch.nn.functional as F\nimport time\nfrom datetime import datetime\nfrom arguments import get_args\nfrom pretrain_glm import initialize_distributed\nfrom pretrain_glm import set_random_seed\nfrom pretrain_glm import get_masks_and_position_ids\nfrom utils import load_checkpoint\nfrom configure_data import prepare_tokenizer\nfrom generation_utils import BeamSearchScorer\nimport mpu\n\nfrom train_utils import get_model\nfrom generation_utils import top_k_logits\n\n\ndef setup_model(args):\n    \"\"\"Setup model and optimizer.\"\"\"\n\n    model = get_model(args, model_type=\"generation\")\n\n    # if args.deepspeed:\n    #     print_rank_0(\"DeepSpeed is enabled.\")\n    #\n    #     model, _, _, _ = deepspeed.initialize(\n    #         model=model,\n    #         model_parameters=model.parameters(),\n    #         args=args,\n    #         mpu=mpu,\n    #         dist_init_required=False\n    #     )\n    if args.load_pretrained is not None:\n        args.no_load_optim = True\n        args.load = args.load_pretrained\n        _ = load_checkpoint(\n            model, None, None, args, no_load_rng=True)\n    # if args.deepspeed:\n    #     model = model.module\n\n    return model\n\n\ndef get_batch(context_tokens, device, args):\n    tokens = context_tokens\n    tokens = tokens.view(args.batch_size, -1).contiguous()\n    tokens = tokens.to(device)\n\n    # Get the masks and postition ids.\n    if args.block_lm:\n        attention_mask = torch.tensor([tokens.size(1)], device=device, dtype=torch.long)\n        position_ids = torch.arange(tokens.size(1), device=device, dtype=torch.long)\n        if not args.no_block_position:\n            block_position_ids = torch.zeros(tokens.size(1), device=device, dtype=torch.long)\n            position_ids = torch.stack((position_ids, block_position_ids), dim=0)\n        position_ids = position_ids.unsqueeze(0)\n    else:\n        attention_mask, loss_mask, position_ids = get_masks_and_position_ids(\n            tokens,\n            args.eod_token,\n            reset_position_ids=False,\n            reset_attention_mask=False,\n            set_loss_mask=False,\n            mem_length=args.mem_length)\n\n    return tokens, attention_mask, position_ids\n\n\ndef sample_sequence(model, tokenizer, context_tokens, context_length, args, device, mems=None, end_tokens=None):\n    if not args.block_lm:\n        context_tokens, attention_mask, position_ids = get_batch(context_tokens, device, args)\n        tokens = torch.empty((args.num_beams, 0), device=context_tokens.device, dtype=torch.long)\n    else:\n        tokens = context_tokens.new_full((1, 1), tokenizer.get_command('sop').Id)\n    counter = 0\n    if mems is None:\n        mems = []\n    if end_tokens is None:\n        end_tokens = [args.eod_token]\n    if args.num_beams > 1:\n        beam_scorer = BeamSearchScorer(\n            batch_size=1,\n            max_length=args.out_seq_length,\n            num_beams=args.num_beams,\n            device=context_tokens.device,\n            length_penalty=args.length_penalty,\n            do_early_stopping=False,\n        )\n        beam_scores = torch.zeros(1, dtype=torch.float, device=context_tokens.device)\n    last_beam_num = 1\n    while counter < args.out_seq_length:\n        if counter == 0 and not args.block_lm:\n            next_token_logits, *mems = model(context_tokens, position_ids, attention_mask, *mems)\n        else:\n            if args.block_lm:\n                if args.no_block_position:\n                    position_ids = context_tokens.new_full((last_beam_num, 1), context_length + counter)\n                else:\n                    position_ids = context_tokens.new_ones(last_beam_num, 2, 1)\n                    position_ids[:, 0] = context_length\n                    position_ids[:, 1] = counter + 1\n                attention_mask = context_tokens.new_zeros([1], device=context_tokens.device, dtype=torch.long)\n            else:\n                position_ids = context_tokens.new_ones((last_beam_num, 1)) * (context_length + counter - 1)\n                attention_mask = context_tokens.new_ones(last_beam_num, 1, 1, args.mem_length + 1,\n                                                         device=context_tokens.device, dtype=torch.float)\n            last_token = tokens[:, -1:]\n            next_token_logits, *mems = model(last_token, position_ids, attention_mask, *mems)\n        next_token_logits = next_token_logits[:, -1]\n        if args.num_beams > 1:\n            next_token_scores = F.log_softmax(next_token_logits, dim=-1)\n            next_token_scores = next_token_scores + beam_scores[:, None].expand_as(next_token_scores)\n            vocab_size = next_token_scores.shape[-1]\n            next_token_scores = next_token_scores.view(1, last_beam_num * vocab_size)\n\n            probs = F.softmax(next_token_scores, dim=-1)\n            next_tokens = torch.multinomial(probs, num_samples=2 * args.num_beams)\n            next_token_scores = torch.gather(next_token_scores, -1, next_tokens)\n            next_token_scores, _indices = torch.sort(next_token_scores, descending=True, dim=1)\n            next_tokens = torch.gather(next_tokens, -1, _indices)\n\n            next_indices = next_tokens // vocab_size\n            next_tokens = next_tokens % vocab_size\n            # stateless\n            tokens = tokens.expand((args.num_beams, -1))\n            beam_outputs = beam_scorer.process(\n                tokens,\n                next_token_scores,\n                next_tokens,\n                next_indices,\n                eos_token_id=end_tokens,\n                mems=mems\n            )\n            beam_scores = beam_outputs[\"next_beam_scores\"]\n            beam_next_tokens = beam_outputs[\"next_beam_tokens\"]\n            beam_idx = beam_outputs[\"next_beam_indices\"]\n            beam_next_tokens = beam_next_tokens.unsqueeze(-1)\n            tokens = torch.cat([tokens[beam_idx, :], beam_next_tokens], dim=-1)\n            mems = [mem[beam_idx] for mem in mems] if mems else None\n            if beam_scorer.is_done:\n                break\n            last_beam_num = args.num_beams\n        else:\n            next_token_logits /= args.temperature\n            next_token_logits = top_k_logits(next_token_logits, top_k=args.top_k, top_p=args.top_p)\n            log_probs = F.softmax(next_token_logits, dim=-1)\n            prev = torch.multinomial(log_probs, num_samples=1)[0]\n            is_end = prev.item() in end_tokens\n            if is_end:\n                break\n            prev = prev.view(1, 1)\n            tokens = prev if tokens is None else torch.cat((tokens, prev), dim=1)\n        counter += 1\n        if not args.block_lm and mpu.get_model_parallel_rank() == 0 and counter % 16 == 0:\n            output_tokens_list = tokens.view(-1).contiguous()\n            decode_tokens = tokenizer.DecodeIds(output_tokens_list.tolist())\n            if mpu.get_model_parallel_rank() == 0 and (counter % 128 == 0 or is_end):\n                os.system('clear')\n                trim_decode_tokens = decode_tokens\n                print(trim_decode_tokens, flush=True)\n    if args.num_beams > 1:\n        tokens, mems, _ = beam_scorer.finalize(tokens, beam_scores, next_tokens, next_indices, eos_token_id=args.eod_token,\n                                            mems=mems)\n    return torch.cat((context_tokens, tokens), dim=1), mems\n\n\ndef read_context(tokenizer, args, output):\n    terminate_runs, skip_run = 0, 0\n    if mpu.get_model_parallel_rank() == 0:\n        while True:\n            raw_text = input(\"\\nContext prompt (stop to exit) >>> \")\n            if not raw_text:\n                print('Prompt should not be empty!')\n                continue\n            if raw_text == \"stop\":\n                terminate_runs = 1\n                break\n            generation_mask = '[gMASK]' if args.task_mask else '[MASK]'\n            if args.block_lm and 'MASK]' not in raw_text:\n                raw_text += ' ' + generation_mask\n            output.write(raw_text)\n            context_tokens = tokenizer.EncodeAsIds(raw_text).tokenization\n            if args.block_lm:\n                context_tokens = [tokenizer.get_command('ENC').Id] + context_tokens\n                if not raw_text.endswith('[gMASK]'):\n                    context_tokens = context_tokens + [tokenizer.get_command('eos').Id]\n            context_length = len(context_tokens)\n\n            if context_length >= args.seq_length:\n                print(\"\\nContext length\", context_length,\n                      \"\\nPlease give smaller context than the window length!\")\n                continue\n            break\n    else:\n        context_length = 0\n\n    terminate_runs_tensor = torch.cuda.LongTensor([terminate_runs])\n    torch.distributed.broadcast(terminate_runs_tensor, mpu.get_model_parallel_src_rank(),\n                                group=mpu.get_model_parallel_group())\n    terminate_runs = terminate_runs_tensor[0].item()\n\n    if terminate_runs == 1:\n        return terminate_runs, None, None, None\n\n    context_length_tensor = torch.cuda.LongTensor([context_length])\n\n    torch.distributed.broadcast(context_length_tensor, mpu.get_model_parallel_src_rank(),\n                                group=mpu.get_model_parallel_group())\n    context_length = context_length_tensor[0].item()\n    if mpu.get_model_parallel_rank() == 0:\n        context_tokens_tensor = torch.cuda.LongTensor(context_tokens)\n    else:\n        context_tokens_tensor = torch.cuda.LongTensor([0] * context_length)\n    torch.distributed.broadcast(context_tokens_tensor, mpu.get_model_parallel_src_rank(),\n                                group=mpu.get_model_parallel_group())\n    if mpu.get_model_parallel_rank() != 0:\n        raw_text = tokenizer.DecodeIds(context_tokens_tensor.tolist())\n    return terminate_runs, raw_text, context_tokens_tensor, context_length\n\n\ndef generate_samples(model, tokenizer, args, device):\n    model.eval()\n    output_path = \"./samples\"\n    if not os.path.exists(output_path):\n        os.makedirs(output_path)\n    output_path = os.path.join(output_path, f\"sample-{datetime.now().strftime('%m-%d-%H-%M')}.txt\")\n    with torch.no_grad(), open(output_path, \"w\") as output:\n        while True:\n            torch.distributed.barrier(group=mpu.get_model_parallel_group())\n\n            terminate_runs, raw_text, context_tokens_tensor, context_length = read_context(tokenizer, args, output)\n            if terminate_runs == 1:\n                return\n            start_time = time.time()\n            if args.block_lm:\n                mems = []\n                tokens, attention_mask, position_ids = get_batch(context_tokens_tensor, device, args)\n                mask_tokens = ['MASK', 'sMASK', 'gMASK'] if args.task_mask else ['MASK']\n                mask_tokens = [tokenizer.get_command(token).Id for token in mask_tokens]\n                end_tokens = [tokenizer.get_command('eop').Id, args.eod_token]\n                mask_positions = []\n                for token in mask_tokens:\n                    mask_positions += (context_tokens_tensor == token).nonzero(as_tuple=True)[0].tolist()\n                mask_positions.sort()\n                if args.no_block_position:\n                    for mask_position in mask_positions:\n                        position_ids[0, mask_position + 1:] += args.out_seq_length\n                _, *mems = model(tokens, position_ids, attention_mask, *mems)\n                for mask_position in mask_positions:\n                    if args.no_block_position:\n                        position = position_ids[0, mask_position].item()\n                    else:\n                        position = mask_position\n                    tokens, mems = sample_sequence(model, tokenizer, tokens, position,\n                                                   args, device, mems=mems, end_tokens=end_tokens)\n            else:\n                tokens, _ = sample_sequence(model, tokenizer, context_tokens_tensor, context_length, args, device)\n            output_tokens_list = tokens.view(-1).contiguous()\n            if mpu.get_model_parallel_rank() == 0:\n                os.system('clear')\n                print(\"\\nTaken time {:.2f}\\n\".format(time.time() - start_time), flush=True)\n                print(\"\\nContext:\", raw_text, flush=True)\n                decode_tokens = tokenizer.DecodeIds(output_tokens_list[context_length:].tolist())\n                trim_decode_tokens = decode_tokens\n                print(\"\\nGLM:\", trim_decode_tokens, flush=True)\n                output.write(trim_decode_tokens + \"\\n\")\n\n            torch.distributed.barrier(group=mpu.get_model_parallel_group())\n\n\ndef main():\n    \"\"\"Main training program.\"\"\"\n\n    print('Generate Samples')\n\n    # Disable CuDNN.\n    torch.backends.cudnn.enabled = False\n\n    # Arguments.\n    args = get_args()\n    args.mem_length = args.seq_length + args.mem_length - 1\n\n    # Pytorch distributed.\n    initialize_distributed(args)\n\n    # Random seeds for reproducability.\n    set_random_seed(args.seed)\n\n    # get the tokenizer\n    tokenizer = prepare_tokenizer(args)\n\n    # Model, optimizer, and learning rate.\n    model = setup_model(args)\n\n    # setting default batch size to 1\n    args.batch_size = 1\n\n    # generate samples\n    generate_samples(model, tokenizer, args, torch.cuda.current_device())\n\n\nif __name__ == \"__main__\":\n    main()\n"
        },
        {
          "name": "generation_utils.py",
          "type": "blob",
          "size": 21.8623046875,
          "content": "# coding=utf-8\n# Copyright 2020 The HuggingFace Inc. team\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom abc import ABC, abstractmethod\nfrom collections import UserDict\nfrom typing import Optional, Tuple, List, Iterable\n\nimport torch\n\nPROCESS_INPUTS_DOCSTRING = r\"\"\"\n    Args:\n        input_ids (:obj:`torch.LongTensor` of shape :obj:`(batch_size * num_beams, sequence_length)`):\n            Indices of input sequence tokens in the vocabulary.\n\n            Indices can be obtained using any class inheriting from :class:`~transformers.PretrainedTokenizer`. See\n            :meth:`transformers.PreTrainedTokenizer.encode` and :meth:`transformers.PreTrainedTokenizer.__call__` for\n            details.\n\n            `What are input IDs? <../glossary.html#input-ids>`__\n        next_scores (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, 2 * num_beams)`):\n            Current scores of the top :obj:`2 * num_beams` non-finished beam hypotheses.\n        next_tokens (:obj:`torch.LongTensor` of shape :obj:`(batch_size, 2 * num_beams)`):\n            :obj:`input_ids` of the tokens corresponding to the top :obj:`2 * num_beams` non-finished beam hypotheses.\n        next_indices (:obj:`torch.LongTensor` of shape :obj:`(batch_size, 2 * num_beams)`):\n            Beam indices indicating to which beam hypothesis the :obj:`next_tokens` correspond.\n        pad_token_id (:obj:`int`, `optional`):\n            The id of the `padding` token.\n        eos_token_id (:obj:`int`, `optional`):\n            The id of the `end-of-sequence` token.\n\n    Return:\n        :obj:`UserDict`: A dictionary composed of the fields as defined above:\n\n            - **next_beam_scores** (:obj:`torch.FloatTensor` of shape :obj:`(batch_size * num_beams)`) -- Updated\n              scores of all non-finished beams.\n            - **next_beam_tokens** (:obj:`torch.FloatTensor` of shape :obj:`(batch_size * num_beams)`) -- Next tokens\n              to be added to the non-finished beam_hypotheses.\n            - **next_beam_indices** (:obj:`torch.FloatTensor` of shape :obj:`(batch_size * num_beams)`) -- Beam indices\n              indicating to which beam the next tokens shall be added.\n\n\"\"\"\n\nFINALIZE_INPUTS_DOCSTRING = r\"\"\"\n    Args:\n        input_ids (:obj:`torch.LongTensor` of shape :obj:`(batch_size * num_beams, sequence_length)`):\n            Indices of input sequence tokens in the vocabulary.\n\n            Indices can be obtained using any class inheriting from :class:`~transformers.PretrainedTokenizer`. See\n            :meth:`transformers.PreTrainedTokenizer.encode` and :meth:`transformers.PreTrainedTokenizer.__call__` for\n            details.\n\n            `What are input IDs? <../glossary.html#input-ids>`__\n        final_beam_scores (:obj:`torch.FloatTensor` of shape :obj:`(batch_size * num_beams)`):\n            The final scores of all non-finished beams.\n        final_beam_tokens (:obj:`torch.FloatTensor` of shape :obj:`(batch_size * num_beams)`):\n            The last tokens to be added to the non-finished beam_hypotheses.\n        final_beam_indices (:obj:`torch.FloatTensor` of shape :obj:`(batch_size * num_beams)`):\n            The beam indices indicating to which beam the :obj:`final_beam_tokens` shall be added.\n        pad_token_id (:obj:`int`, `optional`):\n            The id of the `padding` token.\n        eos_token_id (:obj:`int`, `optional`):\n            The id of the `end-of-sequence` token.\n\n    Return:\n        :obj:`torch.LongTensor` of shape :obj:`(batch_size * num_return_sequences, sequence_length)`: The generated\n        sequences. The second dimension (sequence_length) is either equal to :obj:`max_length` or shorter if all\n        batches finished early due to the :obj:`eos_token_id`.\n\n\"\"\"\n\n\ndef top_k_logits(logits, top_k=0, top_p=0.0, filter_value=-float('Inf')):\n    # This function has been mostly taken from huggingface conversational ai code at\n    # https://medium.com/huggingface/how-to-build-a-state-of-the-art-conversational-ai-with-transfer-learning-2d818ac26313\n\n    if top_k > 0:\n        # Remove all tokens with a probability less than the last token of the top-k\n        indices_to_remove = logits < torch.topk(logits, top_k)[0][..., -1, None]\n        logits[indices_to_remove] = filter_value\n\n    if top_p > 0.0:\n        # convert to 1D\n        logits = logits.view(logits.size()[1]).contiguous()\n        sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n        cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n\n        # Remove tokens with cumulative probability above the threshold\n        sorted_indices_to_remove = cumulative_probs > top_p\n        # Shift the indices to the right to keep also the first token above the threshold\n        sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n        sorted_indices_to_remove[..., 0] = 0\n        indices_to_remove = sorted_indices[sorted_indices_to_remove]\n        logits[indices_to_remove] = filter_value\n        # going back to 2D\n        logits = logits.view(1, -1).contiguous()\n\n    return logits\n\n\nclass BeamScorer(ABC):\n    \"\"\"\n    Abstract base class for all beam scorers that are used for :meth:`~transformers.PretrainedModel.beam_search` and\n    :meth:`~transformers.PretrainedModel.beam_sample`.\n    \"\"\"\n\n    @abstractmethod\n    def process(\n            self,\n            input_ids: torch.LongTensor,\n            next_scores: torch.FloatTensor,\n            next_tokens: torch.LongTensor,\n            next_indices: torch.LongTensor,\n            **kwargs\n    ) -> Tuple[torch.Tensor]:\n        raise NotImplementedError(\"This is an abstract method.\")\n\n    @abstractmethod\n    def finalize(\n            self,\n            input_ids: torch.LongTensor,\n            next_scores: torch.FloatTensor,\n            next_tokens: torch.LongTensor,\n            next_indices: torch.LongTensor,\n            **kwargs\n    ) -> torch.LongTensor:\n        raise NotImplementedError(\"This is an abstract method.\")\n\n\nclass BeamSearchScorer(BeamScorer):\n    r\"\"\"\n    :class:`transformers.BeamScorer` implementing standard beam search decoding.\n\n    Adapted in part from `Facebook's XLM beam search code\n    <https://github.com/facebookresearch/XLM/blob/9e6f6814d17be4fe5b15f2e6c43eb2b2d76daeb4/src/model/transformer.py#L529>`__.\n\n    Args:\n        batch_size (:obj:`int`):\n            Batch Size of :obj:`input_ids` for which beam search decoding is run in parallel.\n        max_length (:obj:`int`):\n            The maximum length of the sequence to be generated.\n        num_beams (:obj:`int`):\n            Number of beams for beam search.\n        device (:obj:`torch.device`):\n            Defines the device type (*e.g.*, :obj:`\"cpu\"` or :obj:`\"cuda\"`) on which this instance of\n            :obj:`BeamSearchScorer` will be allocated.\n        length_penalty (:obj:`float`, `optional`, defaults to 1.0):\n            Exponential penalty to the length. 1.0 means no penalty. Set to values < 1.0 in order to encourage the\n            model to generate shorter sequences, to a value > 1.0 in order to encourage the model to produce longer\n            sequences.\n        do_early_stopping (:obj:`bool`, `optional`, defaults to :obj:`False`):\n            Whether to stop the beam search when at least ``num_beams`` sentences are finished per batch or not.\n        num_beam_hyps_to_keep (:obj:`int`, `optional`, defaults to 1):\n            The number of beam hypotheses that shall be returned upon calling\n            :meth:`~transformer.BeamSearchScorer.finalize`.\n    \"\"\"\n\n    def __init__(\n            self,\n            batch_size: int,\n            max_length: int,\n            num_beams: int,\n            device: torch.device,\n            length_penalty: Optional[float] = 1.0,\n            do_early_stopping: Optional[bool] = False,\n            num_beam_hyps_to_keep: Optional[int] = 1,\n    ):\n        self.max_length = max_length\n        self.num_beams = num_beams\n        self.device = device\n        self.length_penalty = length_penalty\n        self.do_early_stopping = do_early_stopping\n        self.num_beam_hyps_to_keep = num_beam_hyps_to_keep\n\n        self._is_init = False\n        self._beam_hyps = [\n            BeamHypotheses(\n                num_beams=self.num_beams,\n                max_length=self.max_length,\n                length_penalty=self.length_penalty,\n                early_stopping=self.do_early_stopping,\n            )\n            for _ in range(batch_size)\n        ]\n        self._done = torch.tensor([False for _ in range(batch_size)], dtype=torch.bool, device=self.device)\n\n        # if not isinstance(num_beams, int) or num_beams <= 1:\n        #     raise ValueError(\n        #         f\"`num_beams` has to be an integer strictly greater than 1, but is {num_beams}. For `num_beams` == 1, one should make use of `greedy_search` instead.\"\n        #     )\n\n    @property\n    def is_done(self) -> bool:\n        return self._done.all()\n\n    def process(\n            self,\n            input_ids: torch.LongTensor,\n            next_scores: torch.FloatTensor,\n            next_tokens: torch.LongTensor,\n            next_indices: torch.LongTensor,\n            pad_token_id: Optional[int] = None,\n            eos_token_id: Optional[int] = None,\n            mems=None\n    ) -> Tuple[torch.Tensor]:\n        cur_len = input_ids.shape[-1]\n        batch_size = len(self._beam_hyps)\n        assert batch_size == (input_ids.shape[0] // self.num_beams)\n        if isinstance(eos_token_id, int):\n            eos_token_id = [eos_token_id]\n        device = next_scores.device\n        next_beam_scores = torch.zeros((batch_size, self.num_beams), dtype=next_scores.dtype, device=device)\n        next_beam_tokens = torch.zeros((batch_size, self.num_beams), dtype=next_tokens.dtype, device=device)\n        next_beam_indices = torch.zeros((batch_size, self.num_beams), dtype=next_indices.dtype, device=device)\n\n        for batch_idx, beam_hyp in enumerate(self._beam_hyps):\n            if self._done[batch_idx]:\n                assert (\n                        len(beam_hyp) >= self.num_beams\n                ), \"Batch can only be done if at least {} beams have been generated\".format(self.num_beams)\n                assert (\n                        eos_token_id is not None and pad_token_id is not None\n                ), \"generated beams >= num_beams -> eos_token_id and pad_token have to be defined\"\n                # pad the batch\n                next_beam_scores[batch_idx, :] = 0\n                next_beam_tokens[batch_idx, :] = pad_token_id\n                next_beam_indices[batch_idx, :] = 0\n                continue\n\n            # next tokens for this sentence\n            beam_idx = 0\n            for beam_token_rank, (next_token, next_score, next_index) in enumerate(\n                    zip(next_tokens[batch_idx], next_scores[batch_idx], next_indices[batch_idx])\n            ):\n                batch_beam_idx = batch_idx * self.num_beams + next_index\n                # add to generated hypotheses if end of sentence\n                if (eos_token_id is not None) and (next_token.item() in eos_token_id):\n                    # if beam_token does not belong to top num_beams tokens, it should not be added\n                    is_beam_token_worse_than_top_num_beams = beam_token_rank >= self.num_beams\n                    if is_beam_token_worse_than_top_num_beams:\n                        continue\n                    beam_hyp.add(\n                        input_ids[batch_beam_idx].clone(),\n                        next_score.item(),\n                        mems=[mem[[next_index.item()]] for mem in mems] if mems else None\n                    )\n                else:\n                    # add next predicted token since it is not eos_token\n                    next_beam_scores[batch_idx, beam_idx] = next_score\n                    next_beam_tokens[batch_idx, beam_idx] = next_token\n                    next_beam_indices[batch_idx, beam_idx] = batch_beam_idx\n                    beam_idx += 1\n\n                # once the beam for next step is full, don't add more tokens to it.\n                if beam_idx == self.num_beams:\n                    break\n\n            if beam_idx < self.num_beams:\n                raise ValueError(\n                    f\"At most {self.num_beams} tokens in {next_tokens[batch_idx]} can be equal to `eos_token_id: {eos_token_id}`. Make sure {next_tokens[batch_idx]} are corrected.\"\n                )\n\n            # Check if we are done so that we can save a pad step if all(done)\n            self._done[batch_idx] = self._done[batch_idx] or beam_hyp.is_done(\n                next_scores[batch_idx].max().item(), cur_len\n            )\n\n        return UserDict(\n            {\n                \"next_beam_scores\": next_beam_scores.view(-1),\n                \"next_beam_tokens\": next_beam_tokens.view(-1),\n                \"next_beam_indices\": next_beam_indices.view(-1),\n            }\n        )\n\n    def finalize(\n            self,\n            input_ids: torch.LongTensor,\n            final_beam_scores: torch.FloatTensor,\n            final_beam_tokens: torch.LongTensor,\n            final_beam_indices: torch.LongTensor,\n            pad_token_id: Optional[int] = None,\n            eos_token_id: Optional[int] = None,\n            mems=None\n    ) -> Tuple[torch.LongTensor, List[torch.Tensor]]:\n        batch_size = len(self._beam_hyps)\n\n        # finalize all open beam hypotheses and add to generated hypotheses\n        for batch_idx, beam_hyp in enumerate(self._beam_hyps):\n            if self._done[batch_idx]:\n                continue\n\n            # need to add best num_beams hypotheses to generated hyps\n            for beam_id in range(self.num_beams):\n                batch_beam_idx = batch_idx * self.num_beams + beam_id\n                final_score = final_beam_scores[batch_beam_idx].item()\n                final_tokens = input_ids[batch_beam_idx]\n                beam_hyp.add(final_tokens, final_score, mems=[mem[[batch_beam_idx]] for mem in mems] if mems else None)\n\n        # select the best hypotheses\n        sent_lengths = input_ids.new(batch_size * self.num_beam_hyps_to_keep)\n        best = []\n\n        # retrieve best hypotheses\n        for i, beam_hyp in enumerate(self._beam_hyps):\n            sorted_hyps = sorted(beam_hyp.beams, key=lambda x: x[0])\n            for j in range(self.num_beam_hyps_to_keep):\n                score, best_hyp, mems = sorted_hyps.pop()\n                sent_lengths[self.num_beam_hyps_to_keep * i + j] = len(best_hyp)\n                best.append((best_hyp, mems, score))\n\n        # prepare for adding eos\n        sent_max_len = sent_lengths.max().item()\n        decoded: torch.LongTensor = input_ids.new(batch_size * self.num_beam_hyps_to_keep, sent_max_len)\n        scores = final_beam_scores.new(batch_size * self.num_beam_hyps_to_keep)\n        # shorter batches are padded if needed\n        if sent_lengths.min().item() != sent_lengths.max().item():\n            assert pad_token_id is not None, \"`pad_token_id` has to be defined\"\n            decoded.fill_(pad_token_id)\n\n        # fill with hypotheses and eos_token_id if the latter fits in\n        mems = []\n        for i, (hypo, mem, score) in enumerate(best):\n            scores[i] = score\n            decoded[i, : sent_lengths[i]] = hypo\n            if sent_lengths[i] < sent_max_len:\n                decoded[i, sent_lengths[i]] = eos_token_id\n            mems.append(mem)\n        mems = [torch.cat([mem[i] for mem in mems], dim=0) for i in range(len(mems[0]))] if mems and mems[0] else None\n        return decoded, mems, scores\n\n\nclass BeamHypotheses:\n    def __init__(self, num_beams: int, max_length: int, length_penalty: float, early_stopping: bool):\n        \"\"\"\n        Initialize n-best list of hypotheses.\n        \"\"\"\n        self.max_length = max_length - 1  # ignoring bos_token\n        self.length_penalty = length_penalty\n        self.early_stopping = early_stopping\n        self.num_beams = num_beams\n        self.beams = []\n        self.worst_score = 1e9\n\n    def __len__(self):\n        \"\"\"\n        Number of hypotheses in the list.\n        \"\"\"\n        return len(self.beams)\n\n    def add(self, hyp: torch.LongTensor, sum_logprobs: float, mems=None):\n        \"\"\"\n        Add a new hypothesis to the list.\n        \"\"\"\n        score = sum_logprobs / (max(hyp.shape[-1], 1) ** self.length_penalty)\n        if len(self) < self.num_beams or score > self.worst_score:\n            self.beams.append((score, hyp, mems))\n            if len(self) > self.num_beams:\n                sorted_next_scores = sorted([(s, idx) for idx, (s, _, _) in enumerate(self.beams)])\n                del self.beams[sorted_next_scores[0][1]]\n                self.worst_score = sorted_next_scores[1][0]\n            else:\n                self.worst_score = min(score, self.worst_score)\n\n    def is_done(self, best_sum_logprobs: float, cur_len: int) -> bool:\n        \"\"\"\n        If there are enough hypotheses and that none of the hypotheses being generated can become better than the worst\n        one in the heap, then we are done with this sentence.\n        \"\"\"\n\n        if len(self) < self.num_beams:\n            return False\n        elif self.early_stopping:\n            return True\n        else:\n            cur_score = best_sum_logprobs / cur_len ** self.length_penalty\n            ret = self.worst_score >= cur_score\n            return ret\n\n\nclass LogitsProcessor(ABC):\n    \"\"\"Abstract base class for all logit processors that can be applied during generation.\"\"\"\n\n    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor) -> torch.FloatTensor:\n        \"\"\"Torch method for processing logits.\"\"\"\n        raise NotImplementedError(\n            f\"{self.__class__} is an abstract class. Only classes inheriting this class can be called.\"\n        )\n\n\nclass LogitsProcessorList(list):\n    \"\"\"\n    This class can be used to create a list of :class:`~transformers.LogitsProcessor` or\n    :class:`~transformers.LogitsWarper` to subsequently process a :obj:`scores` input tensor. This class inherits from\n    list and adds a specific `__call__` method to apply each :class:`~transformers.LogitsProcessor` or\n    :class:`~transformers.LogitsProcessor` to the inputs.\n    \"\"\"\n\n    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor) -> torch.FloatTensor:\n        for processor in self:\n            scores = processor(input_ids, scores)\n        return scores\n\n\nclass MinLengthLogitsProcessor(LogitsProcessor):\n    r\"\"\"\n    :class:`transformers.LogitsProcessor` enforcing a min-length by setting EOS probability to 0.\n\n    Args:\n        min_length (:obj:`int`):\n            The minimum length below which the score of :obj:`eos_token_id` is set to :obj:`-float(\"Inf\")`.\n        eos_token_id (:obj:`int`):\n            The id of the `end-of-sequence` token.\n    \"\"\"\n\n    def __init__(self, min_length: int, eos_token_id: int):\n        if not isinstance(min_length, int) or min_length < 0:\n            raise ValueError(f\"`min_length` has to be a positive integer, but is {min_length}\")\n\n        if not isinstance(eos_token_id, int) or eos_token_id < 0:\n            raise ValueError(f\"`eos_token_id` has to be a positive integer, but is {eos_token_id}\")\n\n        self.min_length = min_length\n        self.eos_token_id = eos_token_id\n\n    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor) -> torch.FloatTensor:\n        cur_len = input_ids.shape[-1]\n        if cur_len < self.min_length:\n            scores[:, self.eos_token_id] = -float(\"inf\")\n        return scores\n\n\nclass NoRepeatNGramLogitsProcessor(LogitsProcessor):\n    r\"\"\"\n    :class:`transformers.LogitsProcessor` that enforces no repetition of n-grams. See `Fairseq\n    <https://github.com/pytorch/fairseq/blob/a07cb6f40480928c9e0548b737aadd36ee66ac76/fairseq/sequence_generator.py#L345>`__.\n\n    Args:\n        ngram_size (:obj:`int`):\n            All ngrams of size :obj:`ngram_size` can only occur once.\n    \"\"\"\n\n    def __init__(self, ngram_size: int):\n        if not isinstance(ngram_size, int) or ngram_size <= 0:\n            raise ValueError(f\"`ngram_size` has to be a strictly positive integer, but is {ngram_size}\")\n        self.ngram_size = ngram_size\n\n    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor) -> torch.FloatTensor:\n        num_batch_hypotheses = scores.shape[0]\n        cur_len = input_ids.shape[-1]\n        banned_batch_tokens = self._calc_banned_ngram_tokens(input_ids, num_batch_hypotheses, cur_len)\n\n        for i, banned_tokens in enumerate(banned_batch_tokens):\n            scores[i, banned_tokens] = -float(\"inf\")\n\n        return scores\n\n    def _calc_banned_ngram_tokens(\n            self, prev_input_ids: torch.Tensor, num_hypos: int, cur_len: int\n    ) -> List[Iterable[int]]:\n        \"\"\"Copied from fairseq for no_repeat_ngram in beam_search\"\"\"\n        if cur_len + 1 < self.ngram_size:\n            # return no banned tokens if we haven't generated no_repeat_ngram_size tokens yet\n            return [[] for _ in range(num_hypos)]\n        generated_ngrams = [{} for _ in range(num_hypos)]\n        for idx in range(num_hypos):\n            gen_tokens = prev_input_ids[idx].tolist()\n            generated_ngram = generated_ngrams[idx]\n            for ngram in zip(*[gen_tokens[i:] for i in range(self.ngram_size)]):\n                prev_ngram_tuple = tuple(ngram[:-1])\n                generated_ngram[prev_ngram_tuple] = generated_ngram.get(prev_ngram_tuple, []) + [ngram[-1]]\n\n        def _get_generated_ngrams(hypo_idx):\n            # Before decoding the next token, prevent decoding of ngrams that have already appeared\n            start_idx = cur_len + 1 - self.ngram_size\n            ngram_idx = tuple(prev_input_ids[hypo_idx, start_idx:cur_len].tolist())\n            return generated_ngrams[hypo_idx].get(ngram_idx, [])\n\n        banned_tokens = [_get_generated_ngrams(hypo_idx) for hypo_idx in range(num_hypos)]\n        return banned_tokens\n"
        },
        {
          "name": "learning_rates.py",
          "type": "blob",
          "size": 3.7109375,
          "content": "# coding=utf-8\n# Copyright (c) 2019, NVIDIA CORPORATION.  All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"PyTorch DataLoader for TFRecords\"\"\"\n\nimport torch\nfrom torch.optim.lr_scheduler import _LRScheduler\nimport math\n\n\nclass AnnealingLR(_LRScheduler):\n    \"\"\"Anneals the learning rate from start to zero along a cosine curve.\"\"\"\n\n    DECAY_STYLES = ['linear', 'cosine', 'exponential', 'constant', 'None']\n\n    def __init__(self, optimizer, start_lr, warmup_iter, num_iters, decay_style=None, last_iter=-1, decay_ratio=0.5):\n        assert warmup_iter <= num_iters\n        self.optimizer = optimizer\n        self.start_lr = start_lr\n        self.warmup_iter = warmup_iter\n        self.num_iters = last_iter + 1\n        self.end_iter = num_iters\n        self.decay_style = decay_style.lower() if isinstance(decay_style, str) else None\n        self.decay_ratio = 1 / decay_ratio\n        self.step(self.num_iters)\n        if not torch.distributed.is_initialized() or torch.distributed.get_rank() == 0:\n            print(f'learning rate decaying style {self.decay_style}, ratio {self.decay_ratio}')\n\n    def get_lr(self):\n        # https://openreview.net/pdf?id=BJYwwY9ll pg. 4\n        if self.warmup_iter > 0 and self.num_iters <= self.warmup_iter:\n            return float(self.start_lr) * self.num_iters / self.warmup_iter\n        else:\n            if self.decay_style == self.DECAY_STYLES[0]:\n                decay_step_ratio = (self.num_iters - self.warmup_iter) / self.end_iter\n                return self.start_lr - self.start_lr * (1 - 1 / self.decay_ratio) * decay_step_ratio\n            elif self.decay_style == self.DECAY_STYLES[1]:\n                decay_step_ratio = min(1.0, (self.num_iters - self.warmup_iter) / self.end_iter)\n                return self.start_lr / self.decay_ratio * (\n                        (math.cos(math.pi * decay_step_ratio) + 1) * (self.decay_ratio - 1) / 2 + 1)\n            elif self.decay_style == self.DECAY_STYLES[2]:\n                # TODO: implement exponential decay\n                return self.start_lr\n            else:\n                return self.start_lr\n\n    def step(self, step_num=None):\n        if step_num is None:\n            step_num = self.num_iters + 1\n        self.num_iters = step_num\n        new_lr = self.get_lr()\n        for group in self.optimizer.param_groups:\n            group['lr'] = new_lr\n\n    def state_dict(self):\n        sd = {\n            # 'start_lr': self.start_lr,\n            'warmup_iter': self.warmup_iter,\n            'num_iters': self.num_iters,\n            'decay_style': self.decay_style,\n            'end_iter': self.end_iter,\n            'decay_ratio': self.decay_ratio\n        }\n        return sd\n\n    def load_state_dict(self, sd):\n        # self.start_lr = sd['start_lr']\n        self.warmup_iter = sd['warmup_iter']\n        self.num_iters = sd['num_iters']\n        # self.end_iter = sd['end_iter']\n        # self.decay_style = sd['decay_style']\n        # if 'decay_ratio' in sd:\n        #     self.decay_ratio = sd['decay_ratio']\n        self.step(self.num_iters)\n\n    def switch_linear(self, args):\n        current_lr = self.get_lr()\n        self.start_lr = current_lr\n        self.end_iter = args.train_iters - self.num_iters\n        self.num_iters = 0\n        self.decay_style = \"linear\"\n"
        },
        {
          "name": "model",
          "type": "tree",
          "content": null
        },
        {
          "name": "mpu",
          "type": "tree",
          "content": null
        },
        {
          "name": "pretrain_glm.py",
          "type": "blob",
          "size": 27.515625,
          "content": "# coding=utf-8\n# Copyright (c) 2019, NVIDIA CORPORATION.  All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Pretrain GPT2\"\"\"\n\n# Flag to use Pytorch ddp which uses overlapping communication and computation.\nfrom datetime import datetime\nimport os\nimport random\nimport math\n\nimport torch.distributed\nfrom filelock import FileLock\nimport numpy as np\nimport torch\n\nimport deepspeed\nfrom contextlib import ExitStack\nfrom arguments import get_args\nfrom configure_data import configure_data, prepare_tokenizer, build_multi_task_dataset\nimport mpu\nimport pathlib\n\nfrom train_utils import setup_model_and_optimizer, train_step\nfrom utils import Timers\nfrom utils import save_checkpoint\nfrom utils import load_checkpoint\nfrom utils import report_memory\nfrom utils import print_and_save_args\nfrom utils import print_rank_0\nfrom utils import get_sample_writer, get_log_dir, get_hostname\nimport torch.distributed as dist\n\n\ndef get_masks_and_position_ids(data,\n                               eod_token,\n                               reset_position_ids,\n                               reset_attention_mask,\n                               loss_mask=None,\n                               attention_mask=None,\n                               set_loss_mask=False,\n                               mem_length=None):\n    # Extract batch size and sequence length.\n    batch_size, seq_length = data.size()\n\n    # Attention mask (lower triangular).\n    if mem_length:\n        if attention_mask is None:\n            attention_mask = torch.ones((1, seq_length, seq_length + mem_length), device=data.device)\n        attention_mask = torch.tril(torch.triu(attention_mask, 1 - seq_length + mem_length), mem_length)\n    else:\n        if reset_attention_mask:\n            att_mask_batch = batch_size\n        else:\n            att_mask_batch = 1\n        if attention_mask is None:\n            attention_mask = torch.ones((att_mask_batch, seq_length, seq_length), device=data.device)\n        attention_mask = torch.tril(attention_mask)\n    attention_mask = attention_mask.unsqueeze(1)\n\n    # Loss mask.\n    if loss_mask is None:\n        loss_mask = torch.ones(data.size(), dtype=torch.float, device=data.device)\n\n    # Position ids.\n    position_ids = torch.arange(seq_length, dtype=torch.long,\n                                device=data.device)\n    position_ids = position_ids.unsqueeze(0).expand_as(data)\n    if set_loss_mask:\n        loss_mask[data == eod_token] = 0.0\n    # We need to clone as the ids will be modifed based on batch index.\n    if reset_position_ids:\n        position_ids = position_ids.clone()\n\n    if reset_position_ids or reset_attention_mask:\n        # Loop through the batches:\n        for b in range(batch_size):\n\n            # Find indecies where EOD token is.\n            eod_index = position_ids[b, data[b] == eod_token]\n            # Detach indecies from positions if going to modify positions.\n            if reset_position_ids:\n                eod_index = eod_index.clone()\n\n            # Loop through EOD indecies:\n            prev_index = 0\n            for j in range(eod_index.size()[0]):\n                i = eod_index[j]\n                # Mask attention loss.\n                if reset_attention_mask:\n                    attention_mask[b, 0, (i + 1):, :(i + 1)] = 0\n                # Reset positions.\n                if reset_position_ids:\n                    position_ids[b, (i + 1):] -= (i + 1 - prev_index)\n                    prev_index = i + 1\n\n    return attention_mask, loss_mask, position_ids\n\n\ndef get_two_batch(data, args):\n    keys = ['text', 'target', 'loss_mask']\n    datatype = torch.int64\n    # Broadcast data.\n    data_b = mpu.broadcast_data(keys, data, datatype)\n    source_tokens = data_b['text'].long()\n    target_tokens = data_b['target'].long()\n    loss_mask = data_b['loss_mask'].float()\n    labels = target_tokens[:, 1:].contiguous()\n    loss_mask = loss_mask[:, 1:].contiguous()\n    target_tokens = target_tokens[:, :-1].contiguous()\n    _, _, source_position_ids = get_masks_and_position_ids(\n        source_tokens,\n        args.eod_token,\n        reset_position_ids=False,\n        reset_attention_mask=False,\n        loss_mask=None,\n        attention_mask=None,\n        set_loss_mask=False)\n    target_mask, _, target_position_ids = get_masks_and_position_ids(\n        target_tokens,\n        args.eod_token,\n        reset_position_ids=False,\n        reset_attention_mask=False,\n        loss_mask=None,\n        attention_mask=None,\n        set_loss_mask=False)\n    if args.fp16:\n        target_mask = target_mask.half()\n    return source_tokens, target_tokens, source_position_ids, target_position_ids, labels, target_mask, loss_mask\n\n\ndef get_batch(data, args):\n    ''' get_batch subdivides the source data into chunks of\n    length args.seq_length. If source is equal to the example\n    output of the data loading example, with a seq_length limit\n    of 2, we'd get the following two Variables for i = 0:\n    ┌ a g m s ┐ ┌ b h n t ┐\n    └ b h n t ┘ └ c i o u ┘\n    Note that despite the name of the function, the subdivison of data is not\n    done along the batch dimension (i.e. dimension 1), since that was handled\n    by the data loader. The chunks are along dimension 0, corresponding\n    to the seq_len dimension in the LSTM. A Variable representing an appropriate\n    shard reset mask of the same dimensions is also returned.\n    '''\n    # Items and their type.\n    keys = ['text', 'loss_mask']\n    if args.transformer_xl or args.block_lm:\n        keys += ['target', 'attention_mask']\n    if args.block_lm:\n        keys += ['position_id']\n    datatype = torch.int64\n\n    # Broadcast data.\n    data_b = mpu.broadcast_data(keys, data, datatype)\n    # Unpack.\n    if args.transformer_xl:\n        tokens = data_b['text'].long()\n        labels = data_b['target'].long()\n        attention_mask = data_b['attention_mask'].float()\n        loss_mask = data_b['loss_mask'].float()\n    elif args.block_lm:\n        tokens = data_b['text'].long()\n        labels = data_b['target'].long()\n        attention_mask = data_b['attention_mask'].long()\n        loss_mask = data_b['loss_mask'].float()\n        position_ids = data_b['position_id'].long()\n    else:\n        tokens_ = data_b['text'].long()\n        loss_mask = data_b['loss_mask'].float()\n        labels = tokens_[:, 1:].contiguous()\n        loss_mask = loss_mask[:, 1:].contiguous()\n        tokens = tokens_[:, :-1].contiguous()\n        attention_mask = None\n\n    # Get the masks and postition ids.\n    if not args.block_lm:\n        attention_mask, loss_mask, position_ids = get_masks_and_position_ids(\n            tokens,\n            args.eod_token,\n            args.reset_position_ids,\n            args.reset_attention_mask,\n            loss_mask=loss_mask,\n            attention_mask=attention_mask,\n            mem_length=args.mem_length,\n            set_loss_mask=not args.transformer_xl)\n        # Convert\n        if args.fp16:\n            attention_mask = attention_mask.half()\n    return tokens, labels, loss_mask, attention_mask, position_ids\n\n\ntokenizer = None\n\n\ndef forward_step(data_iterator, model, args, timers, mems):\n    \"\"\"Forward step.\"\"\"\n\n    # Get the batch.\n    timers('batch generator').start()\n    timers('data loader').start()\n    rand = random.Random(args.iteration * mpu.get_data_parallel_world_size() + mpu.get_data_parallel_rank())\n    if data_iterator[1] and rand.random() < args.multi_task_ratio:\n        data = next(data_iterator[1]) if data_iterator[1] else None\n        data[\"mode\"] = \"multi-task\"\n    else:\n        data = next(data_iterator[0]) if data_iterator[0] else None\n    # print_rank_0(\"data iterator\")\n    timers('data loader').stop()\n    tokens, labels, loss_mask, attention_mask, position_ids = get_batch(data, args)\n    timers('batch generator').stop()\n\n    # print_rank_0(\"get batch\")\n\n    def print_masked_text(batch_id):\n        block_position_ids = position_ids[:, 1]\n        position_ids_ = position_ids[:, 0]\n        sep = attention_mask.item() if torch.numel(attention_mask) == 1 else attention_mask[batch_id].item()\n        text, last_segment = \"\", []\n        for i, token_id in enumerate(tokens[batch_id, :sep].tolist()):\n            token = tokenizer.IdToToken(token_id)\n            if token.startswith('[MASK') or token.endswith('MASK]'):\n                if last_segment:\n                    text += tokenizer.DecodeIds(last_segment)\n                    last_segment = []\n                text += f\" [{position_ids_[batch_id, i].item()}, {token}]\"\n            else:\n                last_segment.append(token_id)\n        if last_segment:\n            text += tokenizer.DecodeIds(last_segment)\n        print(text.encode('utf-8'))\n        last_index = None\n        for i in range(sep, tokens.size(1)):\n            if tokenizer.IdToToken(tokens[batch_id, i].item()).startswith(\"<|startofpiece\"):\n                if last_index is not None:\n                    print(tokenizer.DecodeIds(tokens[batch_id, last_index: i].tolist()).encode('utf-8'), \"|\",\n                          tokenizer.DecodeIds(labels[batch_id, last_index: i].tolist()).encode('utf-8'),\n                          position_ids_[batch_id, last_index: i].tolist(),\n                          block_position_ids[batch_id, last_index:i].tolist())\n                last_index = i\n        if last_index is not None:\n            print(tokenizer.DecodeIds(tokens[batch_id, last_index:].tolist()).encode('utf-8'), \"|\",\n                  tokenizer.DecodeIds(labels[batch_id, last_index:].tolist()).encode('utf-8'),\n                  position_ids_[batch_id, last_index:].tolist(), block_position_ids[batch_id, last_index:].tolist())\n\n    if data is not None and \"mode\" in data:\n        mode = data['mode']\n    else:\n        mode = 'bert'\n\n    logits, *mems = model(tokens, position_ids, attention_mask, *mems)\n    losses = mpu.vocab_parallel_cross_entropy(logits.contiguous().float(),\n                                              labels)\n    loss_mask = loss_mask.view(-1)\n    loss = torch.sum(losses.view(-1) * loss_mask)\n    if loss_mask.sum().item() > 0:\n        loss = loss / loss_mask.sum()\n\n    return loss, mems, mode\n\n\ndef report_iteration_metrics(summary_writer, optimizer, lr, loss, elapsed_time, step, total_step, args):\n    log_string = ' iteration {:8d}/{:8d} |'.format(step, total_step)\n    log_string += ' elapsed time per iteration (ms): {:.1f} |'.format(elapsed_time)\n    log_string += ' learning rate {:.3E} |'.format(lr)\n    log_string += ' lm loss {:.6E} |'.format(loss)\n    if args.fp16:\n        log_string += ' loss scale {:.1f} |'.format(\n            optimizer.cur_scale if args.deepspeed else optimizer.loss_scale)\n    print_rank_0(log_string)\n    if summary_writer is not None:\n        summary_writer.add_scalar(f'Train/lr', lr, step)\n        summary_writer.add_scalar(f'Train/train_loss', loss, step)\n        summary_writer.add_scalar(f'Train/elapsed_time', elapsed_time, step)\n\n\ndef report_evaluate_metrics(summary_writer, prefix, loss, ppl, gpt_loss, bert_loss, sent_loss, multi_loss, step):\n    string = ' validation loss at {}'.format(prefix)\n    string += ' | LM loss: {:.6E}'.format(loss)\n    string += ' | LM PPL: {:.6E}'.format(ppl)\n    if gpt_loss != 0:\n        string += ' | GPT loss: {:.6E}'.format(gpt_loss)\n    if bert_loss != 0:\n        string += ' | BERT loss: {:.6E}'.format(bert_loss)\n    if sent_loss != 0:\n        string += ' | Sent loss: {:.6E}'.format(sent_loss)\n    if multi_loss != 0:\n        string += ' | Multi loss: {:.6E}'.format(multi_loss)\n    length = len(string) + 1\n    print_rank_0('-' * 100)\n    print_rank_0('-' * length)\n    print_rank_0(string)\n    print_rank_0('-' * length)\n    if summary_writer is not None:\n        summary_writer.add_scalar(f'Train/valid_ppl', ppl, step)\n        summary_writer.add_scalar(f'Train/valid_loss', loss, step)\n        if gpt_loss != 0:\n            summary_writer.add_scalar(f'Train/valid_gpt_loss', gpt_loss, step)\n        if bert_loss != 0:\n            summary_writer.add_scalar(f'Train/valid_bert_loss', bert_loss, step)\n        if sent_loss != 0:\n            summary_writer.add_scalar(f'Train/valid_sent_loss', sent_loss, step)\n        if multi_loss != 0:\n            summary_writer.add_scalar(f'Train/valid_multi_loss', multi_loss, step)\n\n\ndef train(model, optimizer, lr_scheduler,\n          train_data_iterator, val_data_iterator, timers, args, summary_writer=None):\n    \"\"\"Train the model.\"\"\"\n\n    # Turn on training mode which enables dropout.\n    model.train()\n\n    # Tracking loss.\n    total_lm_loss = 0.0\n\n    # Iterations.\n    skipped_iters = 0\n\n    timers('interval time').start()\n    report_memory_flag = True\n    mems = []\n    while args.iteration < args.train_iters:\n\n        lm_loss, skipped_iter, mems = train_step(train_data_iterator,\n                                                 model,\n                                                 optimizer,\n                                                 lr_scheduler,\n                                                 args, timers, mems=mems, forward_step_func=forward_step)\n        skipped_iters += skipped_iter\n        args.iteration += 1\n\n        # Update losses.\n        total_lm_loss += lm_loss.data.detach().float()\n\n        # Logging.\n        if args.iteration % args.log_interval == 0:\n            learning_rate = optimizer.param_groups[0]['lr']\n            avg_lm_loss = total_lm_loss.item() / args.log_interval\n            elapsed_time = timers('interval time').elapsed()\n            report_iteration_metrics(summary_writer, optimizer, learning_rate, avg_lm_loss,\n                                     elapsed_time * 1000.0 / args.log_interval, args.iteration, args.train_iters, args)\n            total_lm_loss = 0.0\n            if report_memory_flag:\n                report_memory('after {} iterations'.format(args.iteration))\n                report_memory_flag = False\n            # for i in range(torch.distributed.get_world_size()):\n            #     if i == torch.distributed.get_rank():\n            #         print(get_hostname())\n            #         timers.log(['forward', 'backward', 'optimizer',\n            #                     'batch generator', 'data loader'],\n            #                    normalizer=args.log_interval, reset=False)\n            #     torch.distributed.barrier()\n            if args.deepspeed or args.DDP_impl == 'torch':\n                timers.log(['forward', 'backward', 'optimizer',\n                            'batch generator', 'data loader'],\n                           normalizer=args.log_interval)\n            else:\n                timers.log(['forward', 'backward', 'allreduce', 'optimizer',\n                            'batch generator', 'data loader'],\n                           normalizer=args.log_interval)\n        # Checkpointing\n        if args.save and args.save_interval and args.iteration % args.save_interval == 0:\n            save_checkpoint(args.iteration, model, optimizer, lr_scheduler, args)\n\n        # Evaluation\n        if args.eval_interval and args.iteration % args.eval_interval == 0 and args.do_valid:\n            prefix = 'iteration {}'.format(args.iteration)\n            evaluate_and_print_results(\n                prefix, val_data_iterator, model, args, timers, verbose=False, step=args.iteration,\n                summary_writer=summary_writer, forward_step_func=forward_step)\n\n    return args.iteration, skipped_iters\n\n\ndef evaluate(data_iterator, model, args, timers, forward_step_func, verbose=False):\n    \"\"\"Evaluation.\"\"\"\n    # Turn on evaluation mode which disables dropout.\n    model.eval()\n\n    total_lm_loss, total_gpt_loss, total_bert_loss, total_sent_loss, total_multi_loss = 0, 0, 0, 0, 0\n    gpt_iters, bert_iters, sent_iters, multi_iters = 0, 0, 0, 0\n    mems = []\n    with torch.no_grad():\n        iteration = 0\n        while iteration < args.eval_iters:\n            iteration += 1\n            if verbose and iteration % args.log_interval == 0:\n                print_rank_0('Evaluating iter {}/{}'.format(iteration, args.eval_iters))\n            # Forward evaluation.\n            lm_loss, mems, mode = forward_step_func(data_iterator, model, args, timers, mems=mems)\n\n            '''when contiguous memory optimizations are enabled, the buffers\n            allocated by the optimizations are deallocated during backward pass\n            in the absence of backward pass the buffers should be reset after each\n            forward pass'''\n            if args.deepspeed and args.deepspeed_activation_checkpointing:\n                deepspeed.checkpointing.reset()\n\n            lm_loss = lm_loss.data.detach().float().item()\n            total_lm_loss += lm_loss\n            if mode == 'gpt':\n                total_gpt_loss += lm_loss\n                gpt_iters += 1\n            elif mode == 'bert':\n                total_bert_loss += lm_loss\n                bert_iters += 1\n            elif mode == 'sentence':\n                total_sent_loss += lm_loss\n                sent_iters += 1\n            elif mode == 'multi-task':\n                total_multi_loss += lm_loss\n                multi_iters += 1\n    # Move model back to the train mode.\n    model.train()\n    # Reduce across processes.\n    loss_data = torch.cuda.FloatTensor(\n        [total_lm_loss, total_gpt_loss, total_bert_loss, total_sent_loss, total_multi_loss, gpt_iters, bert_iters,\n         sent_iters, multi_iters])\n    torch.distributed.all_reduce(loss_data, group=mpu.get_data_parallel_group())\n    loss_data = loss_data.tolist()\n    total_lm_loss = loss_data[0] / args.eval_iters / (args.world_size / args.model_parallel_size)\n    total_gpt_loss = loss_data[1] / loss_data[5] if loss_data[5] > 0 else 0\n    total_bert_loss = loss_data[2] / loss_data[6] if loss_data[6] > 0 else 0\n    total_sent_loss = loss_data[3] / loss_data[7] if loss_data[7] > 0 else 0\n    total_multi_loss = loss_data[4] / loss_data[8] if loss_data[8] > 0 else 0\n    return total_lm_loss, total_gpt_loss, total_bert_loss, total_sent_loss, total_multi_loss\n\n\ndef evaluate_and_print_results(prefix, data_iterator, model,\n                               args, timers, forward_step_func, verbose=False, step=None, summary_writer=None):\n    \"\"\"Helper function to evaluate and dump results on screen.\"\"\"\n    lm_loss, gpt_loss, bert_loss, sent_loss, multi_loss = evaluate(data_iterator, model, args, timers, verbose=verbose,\n                                                                   forward_step_func=forward_step_func)\n\n    lm_ppl = math.exp(min(20, lm_loss))\n    report_evaluate_metrics(summary_writer, prefix, lm_loss, lm_ppl, gpt_loss, bert_loss, sent_loss, multi_loss, step)\n\n    return lm_loss\n\n\n'''\n    Optional DeepSpeed Activation Checkpointing features\n    Gives access to partition activations, contiguous memory optimizations\n    and cpu checkpointing.\n\n    Activation checkpoint requires keep track of the random states\n    and setting the random seed for each MP process. Megatron uses\n    mpu.get_cuda_rng_tracker and mpu.model_parallel_cuda_manual_seed\n    for keeping track of the random states and setting the random seeds.\n    Since they are used in places outside of activation checkpointing,\n    we overwrite them to maintain consistency.\n\n    This must be done before all the calls to mpu.model_parallel_cuda_manual_seed\n    '''\n\n\ndef set_deepspeed_activation_checkpointing(args):\n    deepspeed.checkpointing.configure(mpu, deepspeed_config=args.deepspeed_config, num_checkpoints=args.num_layers)\n    mpu.checkpoint = deepspeed.checkpointing.checkpoint\n    mpu.get_cuda_rng_tracker = deepspeed.checkpointing.get_cuda_rng_tracker\n    mpu.model_parallel_cuda_manual_seed = deepspeed.checkpointing.model_parallel_cuda_manual_seed\n\n\ndef initialize_distributed(args):\n    \"\"\"Initialize torch.distributed.\"\"\"\n\n    # Manually set the device ids.\n    device = args.rank % torch.cuda.device_count()\n    if args.local_rank is not None:\n        device = args.local_rank\n    torch.cuda.set_device(device)\n    # Call the init process\n    init_method = 'tcp://'\n    args.master_ip = os.getenv('MASTER_ADDR', 'localhost')\n    args.master_port = os.getenv('MASTER_PORT', '6000')\n    init_method += args.master_ip + ':' + args.master_port\n    if hasattr(deepspeed, \"init_distributed\"):\n        deepspeed.init_distributed(dist_backend=args.distributed_backend)\n    else:\n        torch.distributed.init_process_group(\n            backend=args.distributed_backend,\n            world_size=args.world_size, rank=args.rank,\n            init_method=init_method)\n\n    # Set the model-parallel / data-parallel communicators.\n    mpu.initialize_model_parallel(args.model_parallel_size)\n\n    # Optional DeepSpeed Activation Checkpointing Features\n    #\n    if hasattr(args, \"deepspeed\") and args.deepspeed and args.deepspeed_activation_checkpointing:\n        set_deepspeed_activation_checkpointing(args)\n\n\ndef set_random_seed(seed):\n    \"\"\"Set random seed for reproducability.\"\"\"\n\n    if seed is not None and seed > 0:\n        random.seed(seed)\n        np.random.seed(seed)\n        torch.manual_seed(seed)\n        torch.random.manual_seed(seed)\n        torch.cuda.manual_seed(seed)\n        torch.cuda.manual_seed_all(seed)  # if you are using multi-GPU.\n        mpu.model_parallel_cuda_manual_seed(seed)\n        torch.backends.cudnn.deterministic = True\n\n\ndef get_train_val_test_data(args, tokenizer):\n    \"\"\"Load the data on rank zero and boradcast number of tokens to all GPUS.\"\"\"\n\n    (train_data, val_data, test_data) = (None, None, None)\n    # Data loader only on rank 0 of each model parallel group.\n    if mpu.get_model_parallel_rank() == 0:\n        data_config = configure_data()\n        if args.block_lm:\n            data_set_type = \"Block\"\n        elif args.transformer_xl:\n            data_set_type = \"GPT-XL\"\n        else:\n            data_set_type = \"GPT2\"\n        data_config.set_defaults(data_set_type=data_set_type, transpose=False)\n        train_data, val_data, test_data = data_config.apply(args, tokenizer)\n\n        data_counts = torch.cuda.LongTensor([int(args.do_train), int(args.do_valid), int(args.do_test)])\n    else:\n        data_counts = torch.cuda.LongTensor([0, 0, 0])\n\n    # Broadcast num tokens.\n    torch.distributed.broadcast(data_counts,\n                                mpu.get_model_parallel_src_rank(),\n                                group=mpu.get_model_parallel_group())\n    args.do_train = data_counts[0].item()\n    args.do_valid = data_counts[1].item()\n    args.do_test = data_counts[2].item()\n\n    return train_data, val_data, test_data\n\n\ndef main():\n    \"\"\"Main training program.\"\"\"\n\n    # Disable CuDNN.\n    torch.backends.cudnn.enabled = False\n    # Timer.\n    timers = Timers()\n\n    # Arguments.\n    args = get_args()\n    args.mem_length = args.mem_length if args.transformer_xl else 0\n    if args.load and not args.new_save_directory:\n        args.experiment_name = os.path.basename(os.path.normpath(args.load))\n    else:\n        args.experiment_name = args.experiment_name + datetime.now().strftime(\"%m-%d-%H-%M\")\n    if args.save:\n        args.save = os.path.join(args.save, args.experiment_name)\n    # Pytorch distributed.\n    initialize_distributed(args)\n\n    # Random seeds for reproducability.\n    set_random_seed(args.seed)\n\n    # Data stuff.\n    global tokenizer\n    tokenizer = prepare_tokenizer(args)\n    train_data, val_data, test_data, = get_train_val_test_data(args, tokenizer)\n    multi_train_data, multi_val_data = None, None\n    if args.multi_task_ratio > 0.0:\n        multi_train_data, multi_val_data = build_multi_task_dataset(args, tokenizer)\n\n    # Model, optimizer, and learning rate.\n    model, optimizer, lr_scheduler = setup_model_and_optimizer(args)\n\n    if args.load is not None:\n        with FileLock(os.path.join(pathlib.Path.home(), \"checkpoint_lock\"), timeout=-1):\n            args.iteration = load_checkpoint(model, optimizer, lr_scheduler, args, no_deepspeed=args.no_deepspeed_load)\n        if args.no_load_optim and args.fp16 and optimizer is not None:\n            if args.deepspeed:\n                optimizer.refresh_fp32_params()\n            else:\n                optimizer._model_params_to_master_params()\n    else:\n        args.iteration = 0\n    torch.distributed.barrier()\n    if args.switch_linear:\n        lr_scheduler.switch_linear(args)\n\n    summary_writer = None\n    if torch.distributed.get_rank() == 0:\n        print('Pretrain GPT2 model')\n        args.log_dir = None\n        if args.train_iters > 0:\n            args.log_dir = get_log_dir(base=args.summary_dir, name=args.experiment_name)\n            summary_writer = get_sample_writer(log_dir=args.log_dir, iteration=args.iteration)\n        print_and_save_args(args, verbose=True, log_dir=args.log_dir)\n\n    # Resume data loader if necessary.\n    if args.resume_dataloader:\n        print_rank_0(\"Resume dataloader\")\n        if train_data is not None:\n            train_data.batch_sampler.start_iter = args.iteration % len(train_data)\n        if val_data is not None:\n            start_iter_val = (args.iteration // args.eval_interval) * args.eval_iters\n            val_data.batch_sampler.start_iter = start_iter_val % len(val_data)\n        if multi_train_data is not None:\n            multi_train_data.batch_sampler.start_iter = int(args.iteration * args.multi_task_ratio) % len(\n                multi_train_data)\n        if multi_val_data is not None:\n            start_iter_val = (args.iteration // args.eval_interval) * args.eval_iters * args.multi_task_ratio\n            multi_val_data.batch_sampler.start_iter = start_iter_val % len(multi_val_data)\n    if train_data is not None:\n        train_data_iterator = iter(train_data)\n    else:\n        train_data_iterator = None\n    if multi_train_data is not None:\n        multi_train_iterator = iter(multi_train_data)\n    else:\n        multi_train_iterator = None\n    if val_data is not None:\n        val_data_iterator = iter(val_data)\n    else:\n        val_data_iterator = None\n    if multi_val_data is not None:\n        multi_val_iterator = iter(multi_val_data)\n    else:\n        multi_val_iterator = None\n\n    # TODO: figure out how to properly set this especially when resuming training\n    iteration = 0\n    if args.train_iters > 0:\n        if args.do_train:\n            with ExitStack() as stack:\n                def save_on_exit(args_, model_, optimizer_, lr_scheduler_):\n                    save_checkpoint(args_.iteration, model_, optimizer_, lr_scheduler_, args_)\n\n                # stack.callback(save_on_exit, args, model, optimizer, lr_scheduler)\n                iteration, skipped = train(model, optimizer,\n                                           lr_scheduler,\n                                           (train_data_iterator, multi_train_iterator),\n                                           (val_data_iterator, multi_val_iterator),\n                                           timers, args, summary_writer=summary_writer)\n\n        if args.do_valid:\n            prefix = 'the end of training for val data'\n            val_loss = evaluate_and_print_results(prefix, (val_data_iterator, multi_val_iterator),\n                                                  model, args, timers, verbose=False, forward_step_func=forward_step)\n\n    if args.save and iteration != 0:\n        save_checkpoint(iteration, model, optimizer, lr_scheduler, args)\n\n    if test_data is not None:\n        test_data_iterator = iter(test_data)\n    else:\n        test_data_iterator = None\n\n    if args.do_test:\n        # Run on test data.\n        prefix = 'the end of training for test data'\n        evaluate_and_print_results(prefix, (test_data_iterator, None),\n                                   model, args, timers, verbose=True, forward_step_func=forward_step)\n\n\nif __name__ == \"__main__\":\n    main()\n"
        },
        {
          "name": "process_grid.py",
          "type": "blob",
          "size": 1.783203125,
          "content": "import sys\nimport os\nimport json\nimport glob\nimport statistics\n\npath_pattern = sys.argv[1]\ntarget_type = sys.argv[2]\nbest_value, best_result, best_name = None, None, None\nmean_result = {}\nprint(path_pattern)\nfor dir_path in glob.glob(path_pattern, recursive=True):\n    entry = os.path.basename(dir_path)\n    valid_result = None\n    test_found = os.path.exists(os.path.join(dir_path, \"test_results.json\"))\n    valid_path = os.path.join(dir_path, \"results.json\")\n    if os.path.exists(valid_path):\n        print(entry)\n        with open(valid_path) as file:\n            valid_result = json.load(file)\n    else:\n        print(f\"{entry} no validation results\")\n        continue\n    if not test_found:\n        print(f\"{entry} not tested yet\")\n    if target_type == \"max\":\n        metric = sys.argv[3]\n        metric_value = valid_result[metric]\n        if best_value is None or metric_value > best_value:\n            best_value = metric_value\n            best_result = valid_result\n            best_name = entry\n    elif target_type == \"mean\" or target_type == \"median\":\n        if mean_result:\n            for metric, value in valid_result.items():\n                if metric not in [\"type\", \"epoch\"]:\n                    mean_result[metric].append(value)\n        else:\n            mean_result = {metric: [value] for metric, value in valid_result.items() if\n                           metric not in [\"type\", \"epoch\"]}\n\nif target_type == \"max\":\n    print(f\"Best result found at {best_name}: {best_result}\")\nelif target_type == \"mean\":\n    mean_result = {metric: sum(value) / len(value) for metric, value in mean_result.items()}\n    print(f\"Mean result {mean_result}\")\nelif target_type == \"median\":\n    mean_result = {metric: statistics.median(value) for metric, value in mean_result.items()}\n    print(f\"Mean result {mean_result}\")\n"
        },
        {
          "name": "requirements.txt",
          "type": "blob",
          "size": 0.2021484375,
          "content": "botocore\nboto3\ndeepspeed\nfilelock\nscipy\nnltk\nregex\ntqdm\nmatplotlib\npandas\nrequests\nsentencepiece\nftfy\nlangdetect\nlsh\nscikit_learn\ntensorboardX\ntermcolor\ntldextract\ntransformers\nrouge_score\nfasttext\nunidecode"
        },
        {
          "name": "run_test.py",
          "type": "blob",
          "size": 0.1669921875,
          "content": "import sys\n\nif sys.argv[1] == 'block':\n    from test.test_block import main\n    main()\nelif sys.argv[1] == 'rel_shift':\n    from test.test_rel_shift import main\n    main()"
        },
        {
          "name": "scripts",
          "type": "tree",
          "content": null
        },
        {
          "name": "tasks",
          "type": "tree",
          "content": null
        },
        {
          "name": "test",
          "type": "tree",
          "content": null
        },
        {
          "name": "train_utils.py",
          "type": "blob",
          "size": 16.7392578125,
          "content": "import deepspeed\nimport torch\nfrom apex.optimizers import FusedAdam as Adam\nfrom torch import distributed as dist\n\nimport mpu\nfrom fp16 import FP16_Module, FP16_Optimizer, DynamicLossScaler\nfrom learning_rates import AnnealingLR\nfrom model import GLMModel, glm_get_params_for_weight_decay_optimization\nfrom model import GLMForMultiTokenCloze, GLMForMultiTokenClozeFast, GLMForSingleTokenCloze, GLMForSequenceClassification\nfrom model import PyTorchDistributedDataParallel as TorchDDP, DistributedDataParallel as LocalDDP\nfrom model.modeling_bert import BertForMultipleChoice, BertForSequenceClassification\nfrom utils import print_rank_0, get_checkpoint_name, get_checkpoint_iteration\n\n\ndef load_pretrained(model, checkpoint_path, args, task_tokens=None):\n    load_dir, tag, release, success = get_checkpoint_iteration(checkpoint_path)\n    checkpoint_name = get_checkpoint_name(load_dir, tag, release)\n    if mpu.get_data_parallel_rank() == 0:\n        print('global rank {} is loading pretrained model {}'.format(\n            torch.distributed.get_rank(), checkpoint_name))\n    # Load the checkpoint.\n    sd = torch.load(checkpoint_name, map_location='cpu')\n    if args.deepspeed:\n        model = model.module\n    if isinstance(model, TorchDDP):\n        model = model.module\n    if isinstance(model, FP16_Module):\n        model = model.module\n    if hasattr(model, \"model\"):\n        model = model.model\n\n    # Model.\n    def extend_embedding_weights(state_weights, model_weights):\n        original_length = state_weights.shape[0]\n        assert original_length <= args.max_position_embeddings + 1\n        new_weights = model_weights.clone()\n        new_weights[:original_length] = state_weights\n        return new_weights\n\n    if args.block_lm:\n        if \"transformer.block_position_embeddings.weight\" in sd[\"module\"]:\n            position_weights = sd['module'][\"transformer.position_embeddings.weight\"]\n            if args.max_position_embeddings + 1 > position_weights.shape[0]:\n                sd['module'][\"transformer.position_embeddings.weight\"] = extend_embedding_weights(\n                    position_weights, model.state_dict()[\"transformer.position_embeddings.weight\"].data)\n                print_rank_0(f\"Extend position embedding to {args.max_position_embeddings + 1}\")\n        if \"transformer.block_position_embeddings.weight\" in sd[\"module\"]:\n            block_position_weights = sd['module'][\"transformer.block_position_embeddings.weight\"]\n            if args.max_position_embeddings + 1 > block_position_weights.shape[0]:\n                sd['module'][\"transformer.block_position_embeddings.weight\"] = extend_embedding_weights(\n                    block_position_weights,\n                    model.state_dict()[\"transformer.block_position_embeddings.weight\"].data)\n                print_rank_0(f\"Extend block position embedding to {args.max_position_embeddings + 1}\")\n    missing_keys, unexpected_keys = model.load_state_dict(sd['module'], strict=False)\n    if missing_keys or unexpected_keys:\n        print_rank_0(f\"Missing keys {missing_keys}, unexpected keys {unexpected_keys}\")\n    if args.continuous_prompt and args.prompt_init:\n        model.prompt_spell.init_embedding(model.word_embeddings.weight.data, task_tokens)\n\n\ndef get_model(args, model_type=None, multi_token=True, num_labels=None, spell_length=None):\n    \"\"\"Build the model.\"\"\"\n    print_rank_0('building GLM model ...')\n    if args.pretrained_bert:\n        if model_type == \"multiple_choice\":\n            model = BertForMultipleChoice.from_pretrained(args.tokenizer_model_type,\n                                                          cache_dir=args.cache_dir,\n                                                          fp32_layernorm=args.fp32_layernorm,\n                                                          fp32_embedding=args.fp32_embedding,\n                                                          layernorm_epsilon=args.layernorm_epsilon)\n        elif model_type == \"classification\":\n            model = BertForSequenceClassification.from_pretrained(args.tokenizer_model_type,\n                                                                  cache_dir=args.cache_dir,\n                                                                  fp32_layernorm=args.fp32_layernorm,\n                                                                  fp32_embedding=args.fp32_embedding,\n                                                                  layernorm_epsilon=args.layernorm_epsilon,\n                                                                  num_labels=num_labels)\n        else:\n            raise NotImplementedError\n    else:\n        output_predict, paralle_output = True, True\n        if (model_type == \"multiple_choice\" or model_type == \"classification\") and not args.cloze_eval:\n            output_predict = False\n        if model_type is not None:\n            paralle_output = False\n        if spell_length is not None:\n            print_rank_0(f\"Continuous spell length {spell_length}\")\n        model = GLMModel(num_layers=args.num_layers,\n                         vocab_size=args.vocab_size,\n                         hidden_size=args.hidden_size,\n                         num_attention_heads=args.num_attention_heads,\n                         embedding_dropout_prob=args.hidden_dropout,\n                         attention_dropout_prob=args.attention_dropout,\n                         output_dropout_prob=args.hidden_dropout,\n                         max_sequence_length=args.max_position_embeddings,\n                         max_memory_length=args.mem_length,\n                         checkpoint_activations=args.checkpoint_activations,\n                         checkpoint_num_layers=args.checkpoint_num_layers,\n                         parallel_output=paralle_output,\n                         relative_encoding=args.transformer_xl,\n                         block_position_encoding=args.block_lm and not args.masked_lm,\n                         output_predict=output_predict,\n                         spell_length=spell_length,\n                         spell_func=args.prompt_func,\n                         attention_scale=args.attention_scale)\n        if args.freeze_transformer:\n            model.freeze_transformer(tune_prefix_layers=args.tune_prefix_layers)\n        if model_type is not None:\n            if model_type == 'multiple_choice':\n                if args.cloze_eval:\n                    if multi_token:\n                        if args.fast_decode:\n                            model = GLMForMultiTokenClozeFast(model, length_penalty=args.length_penalty)\n                        else:\n                            model = GLMForMultiTokenCloze(model, length_penalty=args.length_penalty)\n                    else:\n                        model = GLMForSingleTokenCloze(model, take_softmax=args.adapet)\n                else:\n                    model = GLMForSequenceClassification(model, args.hidden_size, args.output_dropout, args.pool_token,\n                                                         num_class=num_labels)\n            elif model_type == 'classification':\n                model = GLMForSequenceClassification(model, args.hidden_size, args.output_dropout, args.pool_token,\n                                                     num_class=num_labels)\n            elif model_type == 'generation':\n                pass\n            else:\n                raise NotImplementedError(model_type)\n\n    if mpu.get_data_parallel_rank() == 0:\n        print(' > number of parameters on model parallel rank {}: {}'.format(\n            mpu.get_model_parallel_rank(),\n            sum([p.nelement() for p in model.parameters()])), flush=True)\n\n    # To prevent OOM for model sizes that cannot fit in GPU memory in full precision\n    if args.fp16:\n        model.half()\n\n    # GPU allocation.\n    model.cuda(torch.cuda.current_device())\n\n    # Fp16 conversion.\n    if args.fp16:\n        model = FP16_Module(model)\n\n    # Wrap model for distributed training.\n    if not args.deepspeed and (args.train_iters or args.epochs):\n        if args.DDP_impl == 'torch':\n            i = torch.cuda.current_device()\n            model = TorchDDP(model, device_ids=[i], output_device=i,\n                             process_group=mpu.get_data_parallel_group())\n        elif args.DDP_impl == 'local':\n            model = LocalDDP(model)\n        else:\n            print_rank_0(\"Skip DDP model\")\n    return model\n\n\ndef get_optimizer_param_groups(model):\n    # Build parameter groups (weight decay and non-decay).\n    while isinstance(model, (LocalDDP, TorchDDP, FP16_Module)):\n        model = model.module\n    param_groups = glm_get_params_for_weight_decay_optimization(model)\n\n    # Add model parallel attribute if it is not set.\n    for param_group in param_groups:\n        # print('## param_group', len(param_group['params']))\n        for param in param_group['params']:\n            if not hasattr(param, 'model_parallel'):\n                param.model_parallel = False\n\n    return param_groups\n\n\ndef get_optimizer(param_groups, args):\n    \"\"\"Set up the optimizer.\"\"\"\n    if args.cpu_optimizer:\n        # Apex FusedAdam uses decoupled weight decay so use the same here\n        if args.cpu_torch_adam:\n            cpu_adam_optimizer = torch.optim.AdamW\n        else:\n            from deepspeed.ops.adam import DeepSpeedCPUAdam\n            cpu_adam_optimizer = DeepSpeedCPUAdam\n        optimizer = cpu_adam_optimizer(param_groups,\n                                       lr=args.lr, weight_decay=args.weight_decay)\n    else:\n        # Use FusedAdam.\n        if args.optimizer == 'adam':\n            optimizer = Adam(param_groups,\n                             lr=args.lr,\n                             weight_decay=args.weight_decay,\n                             betas=(args.adam_beta1, args.adam_beta2),\n                             eps=args.adam_eps)\n        elif args.optimizer == 'adafactor':\n            from transformers import Adafactor\n            optimizer = Adafactor(param_groups, lr=args.lr, relative_step=False, warmup_init=False)\n        else:\n            raise NotImplementedError\n\n    print(f'Optimizer = {optimizer.__class__.__name__}')\n    if hasattr(args, \"deepspeed\") and args.deepspeed:\n        raise NotImplementedError\n        # fp16 wrapper is not required for DeepSpeed.\n        # return optimizer\n\n    # Wrap into fp16 optimizer.\n    if args.fp16:\n        optimizer = FP16_Optimizer(optimizer,\n                                   static_loss_scale=args.loss_scale,\n                                   dynamic_loss_scale=args.dynamic_loss_scale,\n                                   dynamic_loss_args={\n                                       'scale_window': args.loss_scale_window,\n                                       'min_scale': args.min_scale,\n                                       'delayed_shift': args.hysteresis})\n\n    return optimizer\n\n\ndef get_learning_rate_scheduler(optimizer, args):\n    \"\"\"Build the learning rate scheduler.\"\"\"\n\n    # Add linear learning rate scheduler.\n    if args.lr_decay_iters is not None:\n        num_iters = args.lr_decay_iters\n    else:\n        num_iters = args.train_iters\n    if args.finetune:\n        num_iters = num_iters // args.gradient_accumulation_steps\n    num_iters = max(1, num_iters)\n    init_step = -1\n    warmup_iter = args.warmup * num_iters\n    lr_scheduler = AnnealingLR(optimizer,\n                               start_lr=args.lr,\n                               warmup_iter=warmup_iter,\n                               num_iters=num_iters - warmup_iter,\n                               decay_style=args.lr_decay_style,\n                               last_iter=init_step,\n                               decay_ratio=args.lr_decay_ratio)\n\n    return lr_scheduler\n\n\ndef setup_model_and_optimizer(args, model_type=None, multi_token=True, num_labels=None, spell_length=None):\n    \"\"\"Setup model and optimizer.\"\"\"\n\n    model = get_model(args, model_type=model_type, multi_token=multi_token, num_labels=num_labels,\n                      spell_length=spell_length)\n    param_groups = get_optimizer_param_groups(model)\n\n    if args.train_data is not None or args.data_dir is not None and (args.epochs > 0 or args.train_iters > 0):\n        if args.deepspeed:\n            print_rank_0(\"DeepSpeed is enabled.\")\n\n            model, optimizer, _, _ = deepspeed.initialize(\n                model=model,\n                model_parameters=param_groups,\n                args=args,\n                mpu=mpu,\n                dist_init_required=False\n            )\n        else:\n            optimizer = get_optimizer(param_groups, args)\n        lr_scheduler = get_learning_rate_scheduler(optimizer, args)\n    else:\n        optimizer, lr_scheduler = None, None\n\n    return model, optimizer, lr_scheduler\n\n\ndef backward_step(optimizer, model, lm_loss, args, timers):\n    \"\"\"Backward step.\"\"\"\n\n    # Total loss.\n    loss = lm_loss\n\n    # Backward pass.\n    if args.deepspeed:\n        model.backward(loss)\n    else:\n        # optimizer.zero_grad()\n        if args.fp16:\n            optimizer.backward(loss, update_master_grads=False)\n        else:\n            loss.backward()\n\n    if args.deepspeed or args.DDP_impl == 'torch':\n        # DeepSpeed backward propagation already addressed all reduce communication.\n        # Reset the timer to avoid breaking timer logs below.\n        timers('allreduce').reset()\n    else:\n        timers('allreduce').start()\n        model.allreduce_params(reduce_after=False, fp32_allreduce=args.fp32_allreduce)\n        timers('allreduce').stop()\n\n    # Update master gradients.\n    if not args.deepspeed:\n        if args.fp16:\n            optimizer.update_master_grads()\n\n        # Clipping gradients helps prevent the exploding gradient.\n        if args.clip_grad > 0:\n            if not args.fp16:\n                mpu.clip_grad_norm(model.parameters(), args.clip_grad)\n            else:\n                optimizer.clip_master_grads(args.clip_grad)\n\n    return lm_loss\n\n\ndef see_memory_usage(message, force=False):\n    if not force:\n        return\n    dist.barrier()\n    if dist.get_rank() == 0:\n        print(message)\n        print(\"Memory Allocated \", torch.cuda.memory_allocated() / (1024 * 1024 * 1024), \"GigaBytes\")\n        print(\"Max Memory Allocated \", torch.cuda.max_memory_allocated() / (1024 * 1024 * 1024), \"GigaBytes\")\n        print(\"Cache Allocated \", torch.cuda.memory_cached() / (1024 * 1024 * 1024), \"GigaBytes\")\n        print(\"Max cache Allocated \", torch.cuda.max_memory_cached() / (1024 * 1024 * 1024), \"GigaBytes\")\n        print(\" \")\n        # input(\"Press Any Key To Continue ..\")\n\n\ndef train_step(data_iterator, model, optimizer, lr_scheduler, args, timers, forward_step_func, mems=None,\n               single_step=False):\n    \"\"\"Single training step.\"\"\"\n    lm_loss_total, count = 0.0, 0\n    mems = [] if mems is None else mems\n    if not args.deepspeed:\n        optimizer.zero_grad()\n    while True:\n        skipped_iter, complete = 0, False\n        # Forward model for one step.\n        timers('forward').start()\n        lm_loss, mems, _ = forward_step_func(data_iterator, model, args, timers, mems)\n        timers('forward').stop()\n        # print_rank_0(\"Forward step\")\n        if not args.deepspeed:\n            lm_loss /= args.gradient_accumulation_steps\n\n        reduced_loss = lm_loss.detach().clone().view(1)\n        torch.distributed.all_reduce(reduced_loss.data, group=mpu.get_data_parallel_group())\n        reduced_loss.data = reduced_loss.data / (args.world_size / args.model_parallel_size)\n\n        if not DynamicLossScaler._has_inf_or_nan(reduced_loss):\n            lm_loss_total += reduced_loss\n            count += 1\n\n            # Calculate gradients, reduce across processes, and clip.\n            timers('backward').start()\n            backward_step(optimizer, model, lm_loss, args, timers)\n            timers('backward').stop()\n            # print_rank_0(\"Backward step\")\n            # Update parameters.\n            timers('optimizer').start()\n            if args.deepspeed:\n                if model.is_gradient_accumulation_boundary():\n                    model.step()\n                    complete = True\n                    if not (args.fp16 and optimizer.overflow):\n                        lr_scheduler.step()\n                    else:\n                        skipped_iter = 1\n                else:\n                    model.step()\n            else:\n                if count == args.gradient_accumulation_steps:\n                    optimizer.step()\n                    complete = True\n                    # Update learning rate.\n                    if not (args.fp16 and optimizer.overflow):\n                        lr_scheduler.step()\n                    else:\n                        skipped_iter = 1\n            # print_rank_0(\"Optimizer step\")\n            timers('optimizer').stop()\n            if complete:\n                break\n        else:\n            print_rank_0(\"Found NaN loss, skip backward\")\n            del lm_loss, reduced_loss\n            mems = []\n        if single_step:\n            break\n    if args.deepspeed:\n        lm_loss_total = lm_loss_total / count\n    return lm_loss_total, skipped_iter, mems\n"
        },
        {
          "name": "utils.py",
          "type": "blob",
          "size": 18.1708984375,
          "content": "# coding=utf-8\n# Copyright (c) 2019, NVIDIA CORPORATION.  All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Utilities for logging and serialization\"\"\"\n\nimport os\nimport random\nimport time\nimport numpy as np\nimport torch\nimport json\nimport subprocess\n\nfrom fp16 import FP16_Optimizer\nimport mpu\nfrom tensorboardX import SummaryWriter\n\nSUMMARY_WRITER_DIR_NAME = 'runs'\n\n\ndef get_log_dir(name, base):\n    return os.path.join(base, SUMMARY_WRITER_DIR_NAME, name)\n\n\ndef get_sample_writer(log_dir, iteration=0):\n    \"\"\"Returns a tensorboard summary writer\n    \"\"\"\n    return SummaryWriter(\n        log_dir=log_dir, purge_step=iteration)\n\n\ndef print_rank_0(message):\n    if torch.distributed.is_initialized():\n        if torch.distributed.get_rank() == 0:\n            print(message, flush=True)\n    else:\n        print(message, flush=True)\n\n\ndef get_hostname():\n    hostname_cmd = [\"hostname -I\"]\n    result = subprocess.check_output(hostname_cmd, shell=True)\n    master_addr = result.decode('utf-8').split()[0]\n    return master_addr\n\n\ndef get_spare_port(args):\n    if torch.distributed.get_rank() == 0:\n        port = subprocess.check_output([\"shuf -n 1 -i 10000-65535\"], shell=True)\n        port = int(port.strip())\n        if port == args.master_port:\n            port = subprocess.check_output([\"shuf -n 1 -i 10000-65535\"], shell=True)\n            port = int(port.strip())\n        port = torch.cuda.LongTensor([port])\n    else:\n        port = torch.cuda.LongTensor([0])\n    torch.distributed.broadcast(port, 0)\n    port = port.item()\n    return port\n\n\ndef print_and_save_args(args, verbose=True, log_dir=None):\n    \"\"\"Print arguments.\"\"\"\n    if verbose:\n        print('arguments:', flush=True)\n        for arg in vars(args):\n            dots = '.' * (29 - len(arg))\n            print('  {} {} {}'.format(arg, dots, getattr(args, arg)), flush=True)\n    if log_dir is not None:\n        json_file = os.path.join(log_dir, \"config.json\")\n        with open(json_file, \"w\") as output:\n            json.dump(vars(args), output, sort_keys=True)\n        if args.deepspeed and args.deepspeed_config is not None:\n            with open(args.deepspeed_config) as file:\n                deepspeed_config = json.load(file)\n            deepspeed_json_file = os.path.join(log_dir, \"config_gpt_large.json\")\n            with open(deepspeed_json_file, \"w\") as output:\n                json.dump(deepspeed_config, output)\n\n\ndef print_params_min_max_norm(optimizer, iteration):\n    \"\"\"Print min, max, and norm of all parameters.\"\"\"\n    index = 0\n    rank = torch.distributed.get_rank()\n    string = 'iteration, rank, index, model-parallel,min, max, norm\\n'\n    optimizer_ = optimizer\n    if isinstance(optimizer, FP16_Optimizer):\n        optimizer_ = optimizer.optimizer\n    for param_group in optimizer_.param_groups:\n        for param in param_group['params']:\n            index += 1\n            min_ = param.data.min()\n            max_ = param.data.max()\n            norm = param.data.norm()\n            string += '{:7d}, {:4d}, {:4d}, {:2d}, '.format(\n                iteration, rank, index, int(param.model_parallel))\n            string += '{:.6E}, {:.6E}, {:.6E}\\n'.format(min_, max_, norm)\n    print(string, flush=True)\n\n\nclass Timers:\n    \"\"\"Group of timers.\"\"\"\n\n    class Timer:\n        \"\"\"Timer.\"\"\"\n\n        def __init__(self, name):\n            self.name_ = name\n            self.elapsed_ = 0.0\n            self.started_ = False\n            self.start_time = time.time()\n\n        def start(self):\n            \"\"\"Start the timer.\"\"\"\n            assert not self.started_, 'timer has already been started'\n            torch.cuda.synchronize()\n            self.start_time = time.time()\n            self.started_ = True\n\n        def stop(self):\n            \"\"\"Stop the timer.\"\"\"\n            assert self.started_, 'timer is not started'\n            torch.cuda.synchronize()\n            self.elapsed_ += (time.time() - self.start_time)\n            self.started_ = False\n\n        def reset(self):\n            \"\"\"Reset timer.\"\"\"\n            self.elapsed_ = 0.0\n            self.started_ = False\n\n        def elapsed(self, reset=True):\n            \"\"\"Calculate the elapsed time.\"\"\"\n            started_ = self.started_\n            # If the timing in progress, end it first.\n            if self.started_:\n                self.stop()\n            # Get the elapsed time.\n            elapsed_ = self.elapsed_\n            # Reset the elapsed time\n            if reset:\n                self.reset()\n            # If timing was in progress, set it back.\n            if started_:\n                self.start()\n            return elapsed_\n\n    def __init__(self):\n        self.timers = {}\n\n    def __call__(self, name):\n        if name not in self.timers:\n            self.timers[name] = self.Timer(name)\n        return self.timers[name]\n\n    def log(self, names, normalizer=1.0, reset=True):\n        \"\"\"Log a group of timers.\"\"\"\n        assert normalizer > 0.0\n        string = 'time (ms)'\n        for name in names:\n            elapsed_time = self.timers[name].elapsed(\n                reset=reset) * 1000.0 / normalizer\n            string += ' | {}: {:.2f}'.format(name, elapsed_time)\n        print_rank_0(string)\n\n\ndef report_memory(name):\n    \"\"\"Simple GPU memory report.\"\"\"\n\n    mega_bytes = 1024.0 * 1024.0\n    string = name + ' memory (MB)'\n    string += ' | allocated: {}'.format(\n        torch.cuda.memory_allocated() / mega_bytes)\n    string += ' | max allocated: {}'.format(\n        torch.cuda.max_memory_allocated() / mega_bytes)\n    string += ' | cached: {}'.format(torch.cuda.memory_cached() / mega_bytes)\n    string += ' | max cached: {}'.format(\n        torch.cuda.memory_reserved() / mega_bytes)\n    print_rank_0(string)\n\n\ndef get_checkpoint_name(checkpoints_path, iteration, release=False, zero=False):\n    if release:\n        d = 'release'\n    else:\n        d = '{}'.format(iteration)\n    if zero:\n        dp_rank = mpu.get_data_parallel_rank()\n        d += '_zero_dp_rank_{}'.format(dp_rank)\n    return os.path.join(checkpoints_path, d, 'mp_rank_{:02d}_model_states.pt'.format(mpu.get_model_parallel_rank()))\n\n\ndef ensure_directory_exists(filename):\n    dirname = os.path.dirname(filename)\n    if not os.path.exists(dirname):\n        os.makedirs(dirname, exist_ok=True)\n\n\ndef get_checkpoint_tracker_filename(checkpoints_path):\n    return os.path.join(checkpoints_path, 'latest_checkpointed_iteration.txt')\n\n\ndef save_zero_checkpoint(args, iteration, optimizer):\n    zero_sd = {'iteration': iteration,\n               'optimizer_state_dict': optimizer.state_dict()}\n    zero_checkpoint_name = get_checkpoint_name(args.save, iteration, zero=True)\n    ensure_directory_exists(zero_checkpoint_name)\n    torch.save(zero_sd, zero_checkpoint_name)\n    print('  successfully saved {}'.format(zero_checkpoint_name))\n\n\ndef save_checkpoint(iteration, model, optimizer, lr_scheduler, args, tag=None, barrier=True,\n                    only_changed_parameters=False, no_deepspeed=False, no_save_optim=False):\n    \"\"\"Save a model checkpoint.\"\"\"\n    if tag is None:\n        tag = str(iteration)\n    if args.deepspeed and not no_deepspeed:\n        save_ds_checkpoint(iteration, model, lr_scheduler, args, tag=tag)\n    else:\n        # Only rank zer0 of the data parallel writes to the disk.\n\n        if mpu.get_data_parallel_rank() == 0:\n            checkpoint_name = get_checkpoint_name(args.save, tag)\n            print('global rank {} is saving checkpoint at iteration {:7d} to {}'.\n                  format(torch.distributed.get_rank(), iteration, checkpoint_name))\n            sd = {'iteration': iteration}\n            if args.deepspeed:\n                model = model.module\n            state_dict = model.state_dict()\n            if only_changed_parameters:\n                requires_grad_dict = {}\n                for name, parameter in model.named_parameters():\n                    requires_grad_dict[name] = parameter.requires_grad\n                state_dict = {key: value for key, value in state_dict.items() if requires_grad_dict[key]}\n            sd['module'] = state_dict\n\n            # Optimizer stuff.\n            if not args.no_save_optim and not no_save_optim:\n                if optimizer is not None:\n                    sd['optimizer'] = optimizer.state_dict()\n                if lr_scheduler is not None:\n                    sd['lr_scheduler'] = lr_scheduler.state_dict()\n\n            # rng states.\n            if not args.no_save_rng:\n                sd['random_rng_state'] = random.getstate()\n                sd['np_rng_state'] = np.random.get_state()\n                sd['torch_rng_state'] = torch.get_rng_state()\n                sd['cuda_rng_state'] = torch.cuda.get_rng_state()\n                sd['rng_tracker_states'] = mpu.get_cuda_rng_tracker().get_states()\n\n            ensure_directory_exists(checkpoint_name)\n            torch.save(sd, checkpoint_name)\n            print('  successfully saved {}'.format(checkpoint_name))\n\n    # Wait so everyone is done (necessary)\n    if barrier:\n        torch.distributed.barrier()\n    # And update the latest iteration\n    if torch.distributed.get_rank() == 0:\n        tracker_filename = get_checkpoint_tracker_filename(args.save)\n        with open(tracker_filename, 'w') as f:\n            f.write(tag)\n\n\ndef save_ds_checkpoint(iteration, model, lr_scheduler, args, tag):\n    \"\"\"Save a model checkpoint.\"\"\"\n\n    sd = {}\n    sd['iteration'] = iteration\n    if lr_scheduler is not None:\n        sd['client_lr_scheduler'] = lr_scheduler.state_dict()\n    # rng states.\n    if not args.no_save_rng:\n        sd['random_rng_state'] = random.getstate()\n        sd['np_rng_state'] = np.random.get_state()\n        sd['torch_rng_state'] = torch.get_rng_state()\n        sd['cuda_rng_state'] = torch.cuda.get_rng_state()\n        sd['rng_tracker_states'] = mpu.get_cuda_rng_tracker().get_states()\n    model.save_checkpoint(args.save, tag, client_state=sd)\n\n\ndef get_checkpoint_iteration(load_path):\n    # Read the tracker file and set the iteration.\n    tracker_filename = get_checkpoint_tracker_filename(load_path)\n    if not os.path.isfile(tracker_filename):\n        print_rank_0('WARNING: could not find the metadata file {} '.format(\n            tracker_filename))\n        if os.path.isdir(load_path):\n            path = os.path.normpath(load_path)\n            load_dir, tag = os.path.split(path)\n            print_rank_0('Try to directly load the checkpoint from the directory')\n            return load_dir, tag, False, True\n        print_rank_0('    will not load any checkpoints and will start from '\n                     'random')\n        return load_path, 0, False, False\n    with open(tracker_filename, 'r') as f:\n        metastring = f.read().strip()\n        release = metastring == 'release'\n        # try:\n        #     iteration = int(metastring)\n        # except ValueError:\n        #     release = metastring == 'release'\n        #     if not release:\n        #         print_rank_0('ERROR: Invalid metadata file {}. Exiting'.format(\n        #             tracker_filename))\n        #         exit()\n\n    # assert iteration > 0 or release, 'error parsing metadata file {}'.format(\n    #     tracker_filename)\n\n    return load_path, metastring, release, True\n\n\ndef load_checkpoint(model, optimizer, lr_scheduler, args, no_deepspeed=False, no_load_optim=False, no_load_rng=False):\n    \"\"\"Load a model checkpoint.\"\"\"\n\n    load_dir, tag, release, success = get_checkpoint_iteration(args.load)\n\n    if not success:\n        return 0\n\n    if args.deepspeed and not no_deepspeed:\n\n        checkpoint_name, sd = model.load_checkpoint(load_dir, tag,\n                                                    load_optimizer_states=not args.no_load_optim and not no_load_optim,\n                                                    load_lr_scheduler_states=not args.no_load_lr_scheduler)\n        if not args.no_load_lr_scheduler and \"client_lr_scheduler\" in sd:\n            lr_scheduler.load_state_dict(sd[\"client_lr_scheduler\"])\n            print_rank_0(\"Load lr scheduler state\")\n        if checkpoint_name is None:\n            if mpu.get_data_parallel_rank() == 0:\n                print(\"Unable to load checkpoint.\")\n            return tag\n\n    else:\n\n        # Checkpoint.\n        checkpoint_name = get_checkpoint_name(load_dir, tag, release)\n\n        if mpu.get_data_parallel_rank() == 0:\n            print('global rank {} is loading checkpoint {}'.format(\n                torch.distributed.get_rank(), checkpoint_name))\n\n        # Load the checkpoint.\n        sd = torch.load(checkpoint_name, map_location='cpu')\n\n        # Model.\n        if args.deepspeed:\n            model = model.module\n        missing_keys, unexpected_keys = model.load_state_dict(sd['module'], strict=False)\n        if missing_keys or unexpected_keys:\n            print_rank_0(f\"Missing keys {missing_keys}, unexpected keys {unexpected_keys}\")\n\n        # Optimizer.\n        if not release and not args.finetune and not args.no_load_optim and not no_load_optim:\n            try:\n                if optimizer is not None:\n                    optimizer.load_state_dict(sd['optimizer'])\n                if lr_scheduler is not None:\n                    lr_scheduler.load_state_dict(sd['lr_scheduler'])\n            except KeyError:\n                print_rank_0('Unable to load optimizer from checkpoint {}, exiting. '\n                             'Specify --no-load-optim or --finetune to prevent '\n                             'attempting to load the optimizer '\n                             'state.'.format(checkpoint_name))\n\n    # Iterations.\n    if args.finetune or release:\n        iteration = 0\n    else:\n        try:\n            iteration = sd['iteration']\n        except KeyError:\n            try:  # Backward compatible with older checkpoints\n                iteration = sd['total_iters']\n            except KeyError:\n                print_rank_0('A metadata file exists but Unable to load iteration '\n                             ' from checkpoint {}, starting from 0 iteration'.format(checkpoint_name))\n                iteration = 0\n\n    # rng states.\n    if not release and not args.finetune and not args.no_load_rng and not no_load_rng:\n        try:\n            random.setstate(sd['random_rng_state'])\n            np.random.set_state(sd['np_rng_state'])\n            torch.set_rng_state(sd['torch_rng_state'])\n            torch.cuda.set_rng_state(sd['cuda_rng_state'])\n            mpu.get_cuda_rng_tracker().set_states(sd['rng_tracker_states'])\n        except KeyError:\n            print_rank_0('Unable to load random state from checkpoint {}, exiting. '\n                         'Specify --no-load-rng or --finetune to prevent '\n                         'attempting to load the random '\n                         'state.'.format(checkpoint_name))\n\n    if mpu.get_data_parallel_rank() == 0:\n        print('  successfully loaded {}'.format(checkpoint_name))\n\n    return iteration\n\n\ndef load_weights(src, dst, dst2src=False):\n    \"\"\"\n    Loads weights from src to dst via in place copy.\n    src is a huggingface gpt2model, while dst is one of our models.\n    dst2src=True loads parameters from our models into huggingface's.\n    ^dst2src is still untested\n    \"\"\"\n    conv_layer = 'Conv1D' in str(type(src))\n    for n, p in src.named_parameters():\n        if dst2src:\n            data = dst._parameters[n].data\n            load = p.data\n        else:\n            data = p.data\n            load = dst._parameters[n].data\n        if conv_layer and 'weight' in n:\n            data = data.t().contiguous()\n        load.copy_(data)\n\n\n#        dst._parameters[n].data.copy_(data)\n\ndef load_mlp(our, oai, dst2src=False):\n    load_weights(oai.c_fc, our.dense_h_to_4h, dst2src)\n    load_weights(oai.c_proj, our.dense_4h_to_h, dst2src)\n\n\ndef load_attention(our, oai, dst2src=False):\n    load_weights(oai.c_attn, our.query_key_value, dst2src)\n    load_weights(oai.c_proj, our.dense, dst2src)\n\n\ndef load_transformer_layer(our, oai, dst2src=False):\n    load_weights(oai.ln_1, our.input_layernorm, dst2src)\n    load_weights(oai.ln_2, our.post_attention_layernorm, dst2src)\n    load_mlp(our.mlp, oai.mlp, dst2src)\n    load_attention(our.attention, oai.attn, dst2src)\n\n\ndef move_weights(our, oai, dst2src=False):\n    \"\"\"\n    Loads weights from `oai` to `our` via in place copy.\n    `oai` is a huggingface gpt2model, while `our` is one of our models.\n    dst2src=True loads parameters from our models into huggingface's.\n    ^dst2src=True is still untested\n    \"\"\"\n    #    while isinstance(our, (torchDDP, model.distributed.DistributedDataParallel, FP16_Module)):\n    #        our=our.module\n    transformer_model = oai.transformer\n    load_weights(transformer_model.ln_f, our.transformer.final_layernorm, dst2src)\n    load_weights(transformer_model.wte, our.word_embeddings, dst2src)\n    load_weights(transformer_model.wpe, our.position_embeddings, dst2src)\n\n    for our_layer, oai_layer in zip(our.transformer.layers, oai.transformer.h):\n        load_transformer_layer(our_layer, oai_layer, dst2src)\n\n\ndef debug_finetune_data(local_vars, batch_id, tokenizer):\n    tokens, target_ids = local_vars[\"tokens\"], local_vars[\"target_ids\"]\n    attention_mask, logit_mask, position_ids = local_vars[\"attention_mask\"], local_vars[\"logit_mask\"], local_vars[\n        \"position_ids\"]\n    output_tokens = []\n    sep = attention_mask[batch_id].item()\n    for i, token in enumerate(tokens[batch_id][:sep].tolist()):\n        token = tokenizer.IdToToken(token)\n        if token == '[MASK]':\n            token = f\"[{position_ids[batch_id][0, i].item()}]\"\n        output_tokens.append(token)\n    print(\" \".join(output_tokens))\n    target_positions = []\n    for i in range(sep, tokens.size(-1)):\n        if logit_mask[batch_id][i]:\n            target_positions.append(i)\n    print(target_positions)\n    print(tokenizer.DecodeIds(tokens[batch_id][target_positions].tolist()))\n    if len(target_ids.shape) > 2:\n        print(tokenizer.DecodeIds(target_ids[batch_id][target_positions].tolist()))\n    else:\n        print(tokenizer.DecodeIds(target_ids[batch_id].tolist()))\n    print(position_ids[batch_id][:, target_positions])\n"
        }
      ]
    }
  ]
}