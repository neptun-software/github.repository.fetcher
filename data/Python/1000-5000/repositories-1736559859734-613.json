{
  "metadata": {
    "timestamp": 1736559859734,
    "page": 613,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjYyMA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "zjunlp/DeepKE",
      "stars": 3679,
      "defaultBranch": "main",
      "files": [
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.1416015625,
          "content": ".DS_Store\n\n.idea\n.vscode\n\n__pycache__\n*.pyc\n\ntest/.pytest_cache\n\ndata/out\n\nlogs\ncheckpoints\n\ndemo.py\n\notherUtils.py\nmodule/Transformer_offical.py"
        },
        {
          "name": "CITATION.cff",
          "type": "blob",
          "size": 2.365234375,
          "content": "cff-version: \"1.0.0\"\nmessage: \"If you use this toolkit, please cite it using these metadata.\"\ntitle: \"deepke\"\nrepository-code: \"https://https://github.com/zjunlp/DeepKE\"\nauthors: \n  - family-names: Zhang\n    given-names: Ningyu\n  - family-names: Xu\n    given-names: Xin\n  - family-names: Tao\n    given-names: Liankuan\n  - family-names: Yu\n    given-names: Haiyang\n  - family-names: Ye\n    given-names: Hongbin\n  - family-names: Qiao\n    given-names: Shuofei\n  - family-names: Xie\n    given-names: Xin\n  - family-names: Chen\n    given-names: Xiang\n  - family-names: Li\n    given-names: Zhoubo\n  - family-names: Li\n    given-names: Lei\n  - family-names: Liang\n    given-names: Xiaozhuan\n  - family-names: Yao\n    given-names: Yunzhi\n  - family-names: Deng\n    given-names: Shumin\n  - family-names: Wang\n    given-names: Peng\n  - family-names: Zhang\n    given-names: Wen\n  - family-names: Zhang\n    given-names: Zhenru\n  - family-names: Tan\n    given-names: Chuanqi\n  - family-names: Chen\n    given-names: Qiang\n  - family-names: Xiong\n    given-names: Feiyu\n  - family-names: Huang\n    given-names: Fei\n  - family-names: Zheng\n    given-names: Guozhou\n  - family-names: Chen\n    given-names: Huajun\npreferred-citation:\n  type: article\n  title: \"DeepKE: A Deep Learning Based Knowledge Extraction Toolkit for Knowledge Base Population\"\n  authors:\n  - family-names: Zhang\n    given-names: Ningyu\n  - family-names: Xu\n    given-names: Xin\n  - family-names: Tao\n    given-names: Liankuan\n  - family-names: Yu\n    given-names: Haiyang\n  - family-names: Ye\n    given-names: Hongbin\n  - family-names: Qiao\n    given-names: Shuofei\n  - family-names: Xie\n    given-names: Xin\n  - family-names: Chen\n    given-names: Xiang\n  - family-names: Li\n    given-names: Zhoubo\n  - family-names: Li\n    given-names: Lei\n  - family-names: Liang\n    given-names: Xiaozhuan\n  - family-names: Yao\n    given-names: Yunzhi\n  - family-names: Deng\n    given-names: Shumin\n  - family-names: Wang\n    given-names: Peng\n  - family-names: Zhang\n    given-names: Wen\n  - family-names: Zhang\n    given-names: Zhenru\n  - family-names: Tan\n    given-names: Chuanqi\n  - family-names: Chen\n    given-names: Qiang\n  - family-names: Xiong\n    given-names: Feiyu\n  - family-names: Huang\n    given-names: Fei\n  - family-names: Zheng\n    given-names: Guozhou\n  - family-names: Chen\n    given-names: Huajun\n  journal: \"http://arxiv.org/abs/2201.03335\"\n  year: 2022\n  \n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 1.0380859375,
          "content": "MIT License\n\nCopyright (c) 2021 ZJUNLP\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n"
        },
        {
          "name": "MANIFEST.in",
          "type": "blob",
          "size": 0.0234375,
          "content": "include requirements.txt"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 30.7880859375,
          "content": "<p align=\"center\">\n    <a href=\"https://github.com/zjunlp/deepke\"> <img src=\"pics/logo.png\" width=\"400\"/></a>\n<p>\n<p align=\"center\">  \n    <a href=\"http://deepke.zjukg.cn\">\n        <img alt=\"Documentation\" src=\"https://img.shields.io/badge/demo-website-blue\">\n    </a>\n    <a href=\"https://pypi.org/project/deepke/#files\">\n        <img alt=\"PyPI\" src=\"https://img.shields.io/pypi/v/deepke\">\n    </a>\n    <a href=\"https://github.com/zjunlp/DeepKE/blob/master/LICENSE\">\n        <img alt=\"GitHub\" src=\"https://img.shields.io/github/license/zjunlp/deepke\">\n    </a>\n    <a href=\"http://zjunlp.github.io/DeepKE\">\n        <img alt=\"Documentation\" src=\"https://img.shields.io/badge/doc-website-red\">\n    </a>\n    <a href=\"https://colab.research.google.com/drive/1vS8YJhJltzw3hpJczPt24O0Azcs3ZpRi?usp=sharing\">\n        <img alt=\"Open In Colab\" src=\"https://colab.research.google.com/assets/colab-badge.svg\">\n    </a>\n</p>\n\n\n<p align=\"center\">\n    <b> English | <a href=\"https://github.com/zjunlp/DeepKE/blob/main/README_CN.md\">ÁÆÄ‰Ωì‰∏≠Êñá</a> </b>\n</p>\n\n<h1 align=\"center\">\n    <p>A Deep Learning Based Knowledge Extraction Toolkit<br>for Knowledge Graph Construction</p>\n</h1>\n\n\n[DeepKE](https://arxiv.org/pdf/2201.03335.pdf) is a knowledge extraction toolkit for knowledge graph construction supporting **cnSchema**Ôºå**low-resource**, **document-level** and **multimodal** scenarios for *entity*, *relation* and *attribute* extraction. We provide [documents](https://zjunlp.github.io/DeepKE/), [online demo](http://deepke.zjukg.cn/), [paper](https://arxiv.org/pdf/2201.03335.pdf), [slides](https://drive.google.com/file/d/1IIeIZAbVduemqXc4zD40FUMoPHCJinLy/view?usp=sharing) and [poster](https://drive.google.com/file/d/1vd7xVHlWzoAxivN4T5qKrcqIGDcSM1_7/view?usp=sharing) for beginners.\n\n- ‚ùóWant to use **Large Language Models** with DeepKE? Try [DeepKE-LLM](https://github.com/zjunlp/DeepKE/tree/main/example/llm) and [OneKE](https://github.com/zjunlp/DeepKE/blob/main/example/llm/OneKE.md), have fun!\n- ‚ùóWant to train supervised models? Try [Quick Start](#quick-start), we provide the NER models (e.g, [LightNER(COLING'22)](https://github.com/zjunlp/DeepKE/tree/main/example/ner/few-shot), [W2NER(AAAI'22)](https://github.com/zjunlp/DeepKE/tree/main/example/ner/standard/w2ner)), relation extraction models (e.g., [KnowPrompt(WWW'22)](https://github.com/zjunlp/DeepKE/tree/main/example/re/few-shot)), relational triple extraction models (e.g., [ASP(EMNLP'22)](https://github.com/zjunlp/DeepKE/tree/main/example/triple/ASP), [PRGC(ACL'21)](https://github.com/zjunlp/DeepKE/tree/main/example/triple/PRGC), [PURE(NAACL'21)](https://github.com/zjunlp/DeepKE/tree/main/example/triple/PURE)), and release off-the-shelf  models at [DeepKE-cnSchema](https://github.com/zjunlp/DeepKE/tree/main/example/triple/cnschema), have fun!\n- We recommend using Linux; if using Windows, please use `\\\\` in file paths;\n- If HuggingFace is inaccessible, please consider using `wisemodel` or `modescape`.\n\n**If you encounter any issues during the installation of DeepKE and DeepKE-LLM, please check [Tips](https://github.com/zjunlp/DeepKE#tips) or promptly submit an [issue](https://github.com/zjunlp/DeepKE/issues), and we will assist you with resolving the problem!**\n\n\n# Table of Contents\n\n- [Table of Contents](#table-of-contents)\n- [What's New](#whats-new)\n- [Prediction Demo](#prediction-demo)\n- [Model Framework](#model-framework)\n- [Quick Start](#quick-start)\n  - [DeepKE-LLM](#deepke-llm)\n  - [DeepKE](#deepke)\n      - [üîßManual Environment Configuration](#manual-environment-configuration)\n      - [üê≥Building With Docker Images](#building-with-docker-images)\n  - [Requirements](#requirements)\n    - [DeepKE](#deepke-1)\n  - [Introduction of Three Functions](#introduction-of-three-functions)\n    - [1. Named Entity Recognition](#1-named-entity-recognition)\n    - [2. Relation Extraction](#2-relation-extraction)\n    - [3. Attribute Extraction](#3-attribute-extraction)\n    - [4. Event Extraction](#4-event-extraction)\n- [Tips](#tips)\n- [To do](#to-do)\n- [Reading Materials](#reading-materials)\n- [Related Toolkit](#related-toolkit)\n- [Citation](#citation)\n- [Contributors](#contributors)\n- [Other Knowledge Extraction Open-Source Projects](#other-knowledge-extraction-open-source-projects)\n\n<br>\n\n# What's New\n\n* `April, 2024` We release a new bilingual (Chinese and English) schema-based information extraction model called [OneKE](https://huggingface.co/zjunlp/OneKE) based on Chinese-Alpaca-2-13B.\n* `Feb, 2024` We release a large-scale (0.32B tokens) high-quality bilingual (Chinese and English) Information Extraction (IE) instruction dataset named [IEPile](https://huggingface.co/datasets/zjunlp/iepie), along with two models trained with `IEPile`, [baichuan2-13b-iepile-lora](https://huggingface.co/zjunlp/baichuan2-13b-iepile-lora) and [llama2-13b-iepile-lora](https://huggingface.co/zjunlp/llama2-13b-iepile-lora).\n* `Sep 2023` a bilingual Chinese English Information Extraction (IE) instruction dataset called  `InstructIE` was released for the Instruction based Knowledge Graph Construction Task (Instruction based KGC), as detailed in [here](./example/llm/README.md/#data).\n* `June, 2023` We update [DeepKE-LLM](https://github.com/zjunlp/DeepKE/tree/main/example/llm) to support **knowledge extraction** with [KnowLM](https://github.com/zjunlp/KnowLM), [ChatGLM](https://github.com/THUDM/ChatGLM-6B), LLaMA-series, GPT-series etc.\n* `Apr, 2023` We have added new models, including [CP-NER(IJCAI'23)](https://github.com/zjunlp/DeepKE/blob/main/example/ner/cross), [ASP(EMNLP'22)](https://github.com/zjunlp/DeepKE/tree/main/example/triple/ASP), [PRGC(ACL'21)](https://github.com/zjunlp/DeepKE/tree/main/example/triple/PRGC), [PURE(NAACL'21)](https://github.com/zjunlp/DeepKE/tree/main/example/triple/PURE), provided [event extraction](https://github.com/zjunlp/DeepKE/tree/main/example/ee/standard) capabilities (Chinese and English), and offered compatibility with higher versions of Python packages (e.g., Transformers).\n* `Feb, 2023` We have supported using [LLM](https://github.com/zjunlp/DeepKE/tree/main/example/llm) (GPT-3) with in-context learning (based on [EasyInstruct](https://github.com/zjunlp/EasyInstruct)) & data generation, added a NER model [W2NER(AAAI'22)](https://github.com/zjunlp/DeepKE/tree/main/example/ner/standard/w2ner).\n\n<details>\n<summary><b>Previous News</b></summary>\n\n* `Nov, 2022` Add data [annotation instructions](https://github.com/zjunlp/DeepKE/blob/main/README_TAG.md) for entity recognition and relation extraction, automatic labelling of weakly supervised data ([entity extraction](https://github.com/zjunlp/DeepKE/tree/main/example/ner/prepare-data) and [relation extraction](https://github.com/zjunlp/DeepKE/tree/main/example/re/prepare-data)), and optimize [multi-GPU training](https://github.com/zjunlp/DeepKE/tree/main/example/re/standard).\n  \n* `Sept, 2022` The paper [DeepKE: A Deep Learning Based Knowledge Extraction Toolkit for Knowledge Base Population](https://arxiv.org/abs/2201.03335) has been accepted by the EMNLP 2022 System Demonstration Track.\n\n* `Aug, 2022` We have added [data augmentation](https://github.com/zjunlp/DeepKE/tree/main/example/re/few-shot/DA) (Chinese, English) support for [low-resource relation extraction](https://github.com/zjunlp/DeepKE/tree/main/example/re/few-shot).\n\n* `June, 2022` We have added multimodal support for [entity](https://github.com/zjunlp/DeepKE/tree/main/example/ner/multimodal) and [relation extraction](https://github.com/zjunlp/DeepKE/tree/main/example/re/multimodal).\n\n* `May, 2022` We have released [DeepKE-cnschema](https://github.com/zjunlp/DeepKE/blob/main/README_CNSCHEMA.md) with off-the-shelf knowledge extraction models.\n\n* `Jan, 2022` We have released a paper [DeepKE: A Deep Learning Based Knowledge Extraction Toolkit for Knowledge Base Population](https://arxiv.org/abs/2201.03335)\n\n* `Dec, 2021` We have added `dockerfile` to create the enviroment automatically. \n\n* `Nov, 2021` The demo of DeepKE, supporting real-time extration without deploying and training, has been released.\n* The documentation of DeepKE, containing the details of DeepKE such as source codes and datasets, has been released.\n\n* `Oct, 2021` `pip install deepke`\n* The codes of deepke-v2.0 have been released.\n\n* `Aug, 2019` The codes of deepke-v1.0 have been released.\n\n* `Aug, 2018` The project DeepKE startup and codes of deepke-v0.1 have been released.\n  \n\n</details>\n\n# Prediction Demo\n\nThere is a demonstration of prediction. The GIF file is created by [Terminalizer](https://github.com/faressoft/terminalizer). Get the [code](https://drive.google.com/file/d/1r4tWfAkpvynH3CBSgd-XG79rf-pB-KR3/view?usp=share_link).\n<img src=\"pics/demo.gif\" width=\"636\" height=\"494\" align=center>\n\n<br>\n\n# Model Framework\n\n<h3 align=\"center\">\n    <img src=\"pics/architectures.png\">\n</h3>\n\n\n- DeepKE contains a unified framework for **named entity recognition**, **relation extraction** and **attribute extraction**, the three  knowledge extraction functions.\n- Each task can be implemented in different scenarios. For example, we can achieve relation extraction in **standard**, **low-resource (few-shot)**, **document-level** and **multimodal** settings.\n- Each application scenario comprises of three components: **Data** including Tokenizer, Preprocessor and Loader, **Model** including Module, Encoder and Forwarder, **Core** including Training, Evaluation and Prediction. \n\n<br>\n\n# Quick Start\n\n## DeepKE-LLM\n\nIn the era of large models, DeepKE-LLM utilizes a completely new environment dependency.\n\n```\nconda create -n deepke-llm python=3.9\nconda activate deepke-llm\n\ncd example/llm\npip install -r requirements.txt\n```\n\nPlease note that the `requirements.txt` file is located in the `example/llm` folder.\n\n## DeepKE\n- *DeepKE* supports `pip install deepke`. <br>Take the fully supervised relation extraction for example.\n- *DeepKE* supports both **manual** and **docker image** environment configuration, you can choose the appropriate way to build.\n- Highly recommended to install deepke in a Linux environment.\n#### üîßManual Environment Configuration\n\n**Step1** Download the basic code\n\n```bash\ngit clone --depth 1 https://github.com/zjunlp/DeepKE.git\n```\n\n**Step2** Create a virtual environment using `Anaconda` and enter it.<br>\n\n```bash\nconda create -n deepke python=3.8\n\nconda activate deepke\n```\n\n1. Install *DeepKE* with source code\n\n   ```bash\n   pip install -r requirements.txt\n   \n   python setup.py install\n   \n   python setup.py develop\n   ```\n\n2. Install *DeepKE* with `pip` (**NOT recommended!**)\n\n   ```bash\n   pip install deepke\n   ```\n   - Please make sure that pip version <= 24.0\n\n**Step3** Enter the task directory\n\n```bash\ncd DeepKE/example/re/standard\n```\n\n**Step4** Download the dataset, or follow the [annotation instructions](https://github.com/zjunlp/DeepKE/blob/main/README_TAG.md) to obtain data\n\n```bash\nwget 120.27.214.45/Data/re/standard/data.tar.gz\n\ntar -xzvf data.tar.gz\n```\n\nMany types of data formats are supported,and details are in each part. \n\n**Step5** Training (Parameters for training can be changed in the `conf` folder)\n\nWe support visual parameter tuning by using *[wandb](https://docs.wandb.ai/quickstart)*.\n\n```bash\npython run.py\n```\n\n**Step6** Prediction (Parameters for prediction can be changed in the `conf` folder)\n\nModify the path of the trained model in `predict.yaml`.The absolute path of the model needs to be usedÔºåsuch as `xxx/checkpoints/2019-12-03_ 17-35-30/cnn_ epoch21.pth`.\n\n```bash\npython predict.py\n```\n\n - **‚ùóNOTE: if you encounter any errors, please refer to the [Tips](#tips) or submit a GitHub issue.**\n\n\n\n#### üê≥Building With Docker Images\n**Step1** Install the Docker client\n\nInstall Docker and start the Docker service.\n\n**Step2** Pull the docker image and run the container\n\n```bash\ndocker pull zjunlp/deepke:latest\ndocker run -it zjunlp/deepke:latest /bin/bash\n```\n\nThe remaining steps are the same as **Step 3 and onwards** in **Manual Environment Configuration**.\n\n - **‚ùóNOTE: You can refer to the [Tips](#tips) to speed up installation**\n\n## Requirements\n\n\n### DeepKE\n> python == 3.8\n\n- torch>=1.5,<=1.11\n- hydra-core==1.0.6\n- tensorboard==2.4.1\n- matplotlib==3.4.1\n- transformers==4.26.0\n- jieba==0.42.1\n- scikit-learn==0.24.1\n- seqeval==1.2.2\n- opt-einsum==3.3.0\n- wandb==0.12.7\n- ujson==5.6.0\n- huggingface_hub==0.11.0\n- tensorboardX==2.5.1\n- nltk==3.8\n- protobuf==3.20.1\n- numpy==1.21.0\n- ipdb==0.13.11\n- pytorch-crf==0.7.2\n- tqdm==4.66.1\n- openai==0.28.0\n- Jinja2==3.1.2\n- datasets==2.13.2\n- pyhocon==0.3.60\n\n<br>\n\n## Introduction of Three Functions\n\n### 1. Named Entity Recognition\n\n- Named entity recognition seeks to locate and classify named entities mentioned in unstructured text into pre-defined categories such as person names, organizations, locations, organizations, etc.\n\n- The data is stored in `.txt` files. Some instances as following (Users can label data based on the tools [Doccano](https://github.com/doccano/doccano), [MarkTool](https://github.com/FXLP/MarkTool), or they can use the [Weak Supervision](https://github.com/zjunlp/DeepKE/blob/main/example/ner/prepare-data) with DeepKE to obtain data automatically):\n\n  |                           Sentence                           |           Person           |    Location    |          Organization          |\n  | :----------------------------------------------------------: | :------------------------: | :------------: | :----------------------------: |\n  | Êú¨Êä•Âåó‰∫¨9Êúà4Êó•ËÆØËÆ∞ËÄÖÊù®Ê∂åÊä•ÈÅìÔºöÈÉ®ÂàÜÁúÅÂå∫‰∫∫Ê∞ëÊó•Êä•ÂÆ£‰º†ÂèëË°åÂ∑•‰ΩúÂ∫ßË∞à‰ºö9Êúà3Êó•Âú®4Êó•Âú®‰∫¨‰∏æË°å„ÄÇ |            Êù®Ê∂å            |      Âåó‰∫¨      |            ‰∫∫Ê∞ëÊó•Êä•            |\n  | „ÄäÁ∫¢Ê•ºÊ¢¶„ÄãÁî±ÁéãÊâ∂ÊûóÂØºÊºîÔºåÂë®Ê±ùÊòå„ÄÅÁéãËíô„ÄÅÂë®Â≤≠Á≠âÂ§ö‰Ωç‰∏ìÂÆ∂ÂèÇ‰∏éÂà∂‰Ωú„ÄÇ | ÁéãÊâ∂ÊûóÔºåÂë®Ê±ùÊòåÔºåÁéãËíôÔºåÂë®Â≤≠ |            |  |\n  | Áß¶ÂßãÁöáÂÖµÈ©¨‰øë‰Ωç‰∫éÈôïË•øÁúÅË•øÂÆâÂ∏Ç,ÊòØ‰∏ñÁïåÂÖ´Â§ßÂ•áËøπ‰πã‰∏Ä„ÄÇ |           Áß¶ÂßãÁöá           | ÈôïË•øÁúÅÔºåË•øÂÆâÂ∏Ç |                          |\n\n- Read the detailed process in specific README\n  - **[STANDARD (Fully Supervised)](https://github.com/zjunlp/DeepKE/tree/main/example/ner/standard)**\n    \n    ***We [support LLM](https://github.com/zjunlp/DeepKE/tree/main/example/llm) and provide the off-the-shelf model, [DeepKE-cnSchema-NER](https://github.com/zjunlp/DeepKE/blob/main/README_CNSCHEMA_CN.md), which will extract entities in cnSchema without training.***\n\n    **Step1** Enter  `DeepKE/example/ner/standard`.  Download the dataset.\n\n    ```bash\n    wget 120.27.214.45/Data/ner/standard/data.tar.gz\n    \n    tar -xzvf data.tar.gz\n    ```\n\n    **Step2** Training<br>\n\n    The dataset and parameters can be customized in the `data` folder and `conf` folder respectively.\n  \n    ```bash\n    python run.py\n    ```\n\n    **Step3** Prediction\n\n    ```bash\n    python predict.py\n    ```\n  \n  - **[FEW-SHOT](https://github.com/zjunlp/DeepKE/tree/main/example/ner/few-shot)**\n\n    **Step1** Enter  `DeepKE/example/ner/few-shot`.  Download the dataset.\n\n    ```bash\n    wget 120.27.214.45/Data/ner/few_shot/data.tar.gz\n    \n    tar -xzvf data.tar.gz\n    ```\n  \n    **Step2** Training in the low-resouce setting <br>\n  \n    The directory where the model is loaded and saved and the configuration parameters can be cusomized in the `conf` folder.\n  \n    ```bash\n    python run.py +train=few_shot\n    ```\n    \n    Users can modify `load_path` in `conf/train/few_shot.yaml` to use existing loaded model.<br>\n    \n    **Step3** Add `- predict` to `conf/config.yaml`, modify `loda_path` as the model path and `write_path` as the path where the predicted results are saved in `conf/predict.yaml`, and then run `python predict.py`\n    \n    ```bash\n    python predict.py\n    ```\n\n  - **[MULTIMODAL](https://github.com/zjunlp/DeepKE/tree/main/example/ner/multimodal)**\n\n    **Step1** Enter  `DeepKE/example/ner/multimodal`.  Download the dataset.\n\n    ```bash\n    wget 120.27.214.45/Data/ner/multimodal/data.tar.gz\n    \n    tar -xzvf data.tar.gz\n    ```\n\n    We use RCNN detected objects and visual grounding objects from original images as visual local information, where RCNN via [faster_rcnn](https://github.com/pytorch/vision/blob/main/torchvision/models/detection/faster_rcnn.py) and visual grounding via [onestage_grounding](https://github.com/zyang-ur/onestage_grounding).\n\n    **Step2** Training in the multimodal setting <br>\n\n    - The dataset and parameters can be customized in the `data` folder and `conf` folder respectively.\n    - Start with the model trained last time: modify `load_path` in `conf/train.yaml`as the path where the model trained last time was saved. And the path saving logs generated in training can be customized by `log_dir`.\n\n    ```bash\n    python run.py\n    ```\n\n    **Step3** Prediction\n\n    ```bash\n    python predict.py\n    ```\n\n### 2. Relation Extraction\n\n- Relationship extraction is the task of extracting semantic relations between entities from a unstructured text.\n\n- The data is stored in `.csv` files. Some instances as following (Users can label data based on the tools [Doccano](https://github.com/doccano/doccano), [MarkTool](https://github.com/FXLP/MarkTool), or they can use the [Weak Supervision](https://github.com/zjunlp/DeepKE/blob/main/example/re/prepare-data) with DeepKE to obtain data automatically):\n\n  |                        Sentence                        | Relation |    Head    | Head_offset |    Tail    | Tail_offset |\n  | :----------------------------------------------------: | :------: | :--------: | :---------: | :--------: | :---------: |\n  | „ÄäÂ≤≥Áà∂‰πüÊòØÁàπ„ÄãÊòØÁéãÂÜõÊâßÂØºÁöÑÁîµËßÜÂâßÔºåÁî±È©¨ÊÅ©ÁÑ∂„ÄÅËåÉÊòé‰∏ªÊºî„ÄÇ |   ÂØºÊºî   | Â≤≥Áà∂‰πüÊòØÁàπ |      1      |    ÁéãÂÜõ    |      8      |\n  |  „Ää‰πùÁéÑÁè†„ÄãÊòØÂú®Á∫µÊ®™‰∏≠ÊñáÁΩëËøûËΩΩÁöÑ‰∏ÄÈÉ®Â∞èËØ¥Ôºå‰ΩúËÄÖÊòØÈæôÈ©¨„ÄÇ  | ËøûËΩΩÁΩëÁ´ô |   ‰πùÁéÑÁè†   |      1      | Á∫µÊ®™‰∏≠ÊñáÁΩë |      7      |\n  |     ÊèêËµ∑Êù≠Â∑ûÁöÑÁæéÊôØÔºåË•øÊπñÊÄªÊòØÁ¨¨‰∏Ä‰∏™Êò†ÂÖ•ËÑëÊµ∑ÁöÑËØçËØ≠„ÄÇ     | ÊâÄÂú®ÂüéÂ∏Ç |    Ë•øÊπñ    |      8      |    Êù≠Â∑û    |      2      |\n\n- **!NOTE: If there are multiple entity types for one relation, entity types can be prefixed with the relation as inputs.**\n- Read the detailed process in specific README\n\n  - **[STANDARD (Fully Supervised)](https://github.com/zjunlp/DeepKE/tree/main/example/re/standard)** \n\n    ***We [support LLM](https://github.com/zjunlp/DeepKE/tree/main/example/llm) and provide the off-the-shelf model, [DeepKE-cnSchema-RE](https://github.com/zjunlp/DeepKE/blob/main/README_CNSCHEMA_CN.md), which will extract relations in cnSchema without training.***\n\n    **Step1** Enter the `DeepKE/example/re/standard` folder.  Download the dataset.\n\n    ```bash\n    wget 120.27.214.45/Data/re/standard/data.tar.gz\n    \n    tar -xzvf data.tar.gz\n    ```\n\n    **Step2** Training<br>\n\n    The dataset and parameters can be customized in the `data` folder and `conf` folder respectively.\n  \n    ```bash\n    python run.py\n    ```\n\n    **Step3** Prediction\n\n    ```bash\n    python predict.py\n    ```\n  \n  - **[FEW-SHOT](https://github.com/zjunlp/DeepKE/tree/main/example/re/few-shot)**\n\n    **Step1** Enter `DeepKE/example/re/few-shot`. Download the dataset.\n\n    ```bash\n    wget 120.27.214.45/Data/re/few_shot/data.tar.gz\n    \n    tar -xzvf data.tar.gz\n    ```\n\n    **Step 2** Training<br>\n\n    - The dataset and parameters can be customized in the `data` folder and `conf` folder respectively.\n    - Start with the model trained last time: modify `train_from_saved_model` in `conf/train.yaml`as the path where the model trained last time was saved. And the path saving logs generated in training can be customized by `log_dir`. \n  \n    ```bash\n    python run.py\n    ```\n  \n    **Step3** Prediction\n  \n    ```bash\n    python predict.py\n    ```\n  \n  - **[DOCUMENT](https://github.com/zjunlp/DeepKE/tree/main/example/re/document)**<br>\n  \n    **Step1** Enter `DeepKE/example/re/document`.  Download the dataset.\n  \n    ```bash\n    wget 120.27.214.45/Data/re/document/data.tar.gz\n    \n    tar -xzvf data.tar.gz\n    ```\n    \n    **Step2** Training<br>\n  \n    - The dataset and parameters can be customized in the `data` folder and `conf` folder respectively.\n    - Start with the model trained last time: modify `train_from_saved_model` in `conf/train.yaml`as the path where the model trained last time was saved. And the path saving logs generated in training can be customized by `log_dir`. \n    \n    ```bash\n    python run.py\n    ```\n    \n    **Step3** Prediction\n    \n    ```bash\n    python predict.py\n    ```\n\n  - **[MULTIMODAL](https://github.com/zjunlp/DeepKE/tree/main/example/re/multimodal)**\n\n    **Step1** Enter  `DeepKE/example/re/multimodal`.  Download the dataset.\n\n    ```bash\n    wget 120.27.214.45/Data/re/multimodal/data.tar.gz\n    \n    tar -xzvf data.tar.gz\n    ```\n\n    We use RCNN detected objects and visual grounding objects from original images as visual local information, where RCNN via [faster_rcnn](https://github.com/pytorch/vision/blob/main/torchvision/models/detection/faster_rcnn.py) and visual grounding via [onestage_grounding](https://github.com/zyang-ur/onestage_grounding).\n\n    **Step2** Training<br>\n\n    - The dataset and parameters can be customized in the `data` folder and `conf` folder respectively.\n    - Start with the model trained last time: modify `load_path` in `conf/train.yaml`as the path where the model trained last time was saved. And the path saving logs generated in training can be customized by `log_dir`.\n\n    ```bash\n    python run.py\n    ```\n\n    **Step3** Prediction\n\n    ```bash\n    python predict.py\n    ```\n\n### 3. Attribute Extraction\n\n- Attribute extraction is to extract attributes for entities in a unstructed text.\n\n- The data is stored in `.csv` files. Some instances as following:\n\n  |                           Sentence                           |   Att    |   Ent    | Ent_offset |      Val      | Val_offset |\n  | :----------------------------------------------------------: | :------: | :------: | :--------: | :-----------: | :--------: |\n  |          Âº†ÂÜ¨Ê¢ÖÔºåÂ•≥ÔºåÊ±âÊóèÔºå1968Âπ¥2ÊúàÁîüÔºåÊ≤≥ÂçóÊ∑áÂéø‰∫∫           |   Ê∞ëÊóè   |  Âº†ÂÜ¨Ê¢Ö  |     0      |     Ê±âÊóè      |     6      |\n  |ËØ∏Ëëõ‰∫ÆÔºåÂ≠óÂ≠îÊòéÔºå‰∏âÂõΩÊó∂ÊúüÊù∞Âá∫ÁöÑÂÜõ‰∫ãÂÆ∂„ÄÅÊñáÂ≠¶ÂÆ∂„ÄÅÂèëÊòéÂÆ∂„ÄÇ|   Êúù‰ª£   |   ËØ∏Ëëõ‰∫Æ   |     0      |     ‰∏âÂõΩÊó∂Êúü      |     8     |\n  |        2014Âπ¥10Êúà1Êó•ËÆ∏ÈûçÂçéÊâßÂØºÁöÑÁîµÂΩ±„ÄäÈªÑÈáëÊó∂‰ª£„Äã‰∏äÊò†         | ‰∏äÊò†Êó∂Èó¥ | ÈªÑÈáëÊó∂‰ª£ |     19     | 2014Âπ¥10Êúà1Êó• |     0      |\n\n- Read the detailed process in specific README\n  - **[STANDARD (Fully Supervised)](https://github.com/zjunlp/DeepKE/tree/main/example/ae/standard)**\n\n    **Step1** Enter the `DeepKE/example/ae/standard` folder. Download the dataset.\n\n    ```bash\n    wget 120.27.214.45/Data/ae/standard/data.tar.gz\n    \n    tar -xzvf data.tar.gz\n    ```\n\n    **Step2** Training<br>\n\n    The dataset and parameters can be customized in the `data` folder and `conf` folder respectively.\n    \n    ```bash\n    python run.py\n    ```\n    \n    **Step3** Prediction\n    \n    ```bash\n    python predict.py\n    ```\n\n<br>\n\n### 4. Event Extraction\n\n* Event extraction is the task to extract event type, event trigger words, event arguments from a unstructed text.\n* The data is stored in `.tsv` files, some instances are as follows:\n\n<table h style=\"text-align:center\">\n    <tr>\n        <th colspan=\"2\"> Sentence </th>\n        <th> Event type </th>\n        <th> Trigger </th>\n        <th> Role </th>\n        <th> Argument </th>\n    </tr>\n    <tr> \n        <td rowspan=\"3\" colspan=\"2\"> ÊçÆ„ÄäÊ¨ßÊ¥≤Êó∂Êä•„ÄãÊä•ÈÅìÔºåÂΩìÂú∞Êó∂Èó¥27Êó•ÔºåÊ≥ïÂõΩÂ∑¥ÈªéÂç¢ÊµÆÂÆ´ÂçöÁâ©È¶ÜÂëòÂ∑•Âõ†‰∏çÊª°Â∑•‰ΩúÊù°‰ª∂ÊÅ∂ÂåñËÄåÁΩ¢Â∑•ÔºåÂØºËá¥ËØ•ÂçöÁâ©È¶Ü‰πüÂõ†Ê≠§Èó≠Èó®Ë∞¢ÂÆ¢‰∏ÄÂ§©„ÄÇ </td>\n      \t<td rowspan=\"3\"> ÁªÑÁªáË°å‰∏∫-ÁΩ¢Â∑• </td>\n    \t\t<td rowspan=\"3\"> ÁΩ¢Â∑• </td>\n    \t\t<td> ÁΩ¢Â∑•‰∫∫Âëò </td>\n    \t\t<td> Ê≥ïÂõΩÂ∑¥ÈªéÂç¢ÊµÆÂÆ´ÂçöÁâ©È¶ÜÂëòÂ∑• </td>\n    </tr>\n    <tr> \n        <td> Êó∂Èó¥ </td>\n        <td> ÂΩìÂú∞Êó∂Èó¥27Êó• </td>\n    </tr>\n    <tr> \n        <td> ÊâÄÂ±ûÁªÑÁªá </td>\n        <td> Ê≥ïÂõΩÂ∑¥ÈªéÂç¢ÊµÆÂÆ´ÂçöÁâ©È¶Ü </td>\n    </tr>\n    <tr> \n        <td rowspan=\"3\" colspan=\"2\"> ‰∏≠ÂõΩÂ§ñËøê2019Âπ¥‰∏äÂçäÂπ¥ÂΩíÊØçÂáÄÂà©Ê∂¶Â¢ûÈïø17%ÔºöÊî∂Ë¥≠‰∫ÜÂ∞ëÊï∞ËÇ°‰∏úËÇ°ÊùÉ </td>\n      \t<td rowspan=\"3\"> Ë¥¢Áªè/‰∫§Êòì-Âá∫ÂîÆ/Êî∂Ë¥≠ </td>\n    \t\t<td rowspan=\"3\"> Êî∂Ë¥≠ </td>\n    \t\t<td> Âá∫ÂîÆÊñπ </td>\n    \t\t<td> Â∞ëÊï∞ËÇ°‰∏ú </td>\n    </tr>\n    <tr> \n        <td> Êî∂Ë¥≠Êñπ </td>\n        <td> ‰∏≠ÂõΩÂ§ñËøê </td>\n    </tr>\n    <tr> \n        <td> ‰∫§ÊòìÁâ© </td>\n        <td> ËÇ°ÊùÉ </td>\n    </tr>\n    <tr> \n        <td rowspan=\"3\" colspan=\"2\"> ÁæéÂõΩ‰∫öÁâπÂÖ∞Â§ßËà™Â±ï13Êó•ÂèëÁîü‰∏ÄËµ∑Ë°®ÊºîÊú∫Âù†Êú∫‰∫ãÊïÖÔºåÈ£ûË°åÂëòÂºπÂ∞ÑÂá∫Ëà±Âπ∂ÂÆâÂÖ®ÁùÄÈôÜÔºå‰∫ãÊïÖÊ≤°ÊúâÈÄ†Êàê‰∫∫Âëò‰º§‰∫°„ÄÇ </td>\n      \t<td rowspan=\"3\"> ÁÅæÂÆ≥/ÊÑèÂ§ñ-Âù†Êú∫ </td>\n    \t\t<td rowspan=\"3\"> Âù†Êú∫ </td>\n    \t\t<td> Êó∂Èó¥ </td>\n    \t\t<td> 13Êó• </td>\n    </tr>\n    <tr> \n        <td> Âú∞ÁÇπ </td>\n        <td> ÁæéÂõΩ‰∫öÁâπÂÖ∞ </td>\n  \t</tr>\n</table>\n\n* Read the detailed process in specific README\n\n  * [STANDARD(Fully Supervised)](./example/ee/standard/README.md)\n\n    **Step1** Enter the `DeepKE/example/ee/standard` folder. Download the dataset.\n\n    ```bash\n    wget 120.27.214.45/Data/ee/DuEE.zip\n    unzip DuEE.zip\n    ```\n\n    **Step 2** Training\n\n    The dataset and parameters can be customized in the `data` folder and `conf` folder respectively.\n\n    ```bash\n    python run.py\n    ```\n\n    **Step 3** Prediction\n\n    ```bash\n    python predict.py\n    ```\n\n<br>\n\n# Tips\n\n1.```Using nearest mirror```, **[THU](https://mirrors.tuna.tsinghua.edu.cn/help/anaconda/) in China, will speed up the installation of *Anaconda*; [aliyun](http://mirrors.aliyun.com/pypi/simple/) in China, will speed up `pip install XXX`**.\n\n2.When encountering `ModuleNotFoundError: No module named 'past'`Ôºårun `pip install future` .\n\n3.It's slow to install the pretrained language models online. Recommend download pretrained models before use and save them in the `pretrained` folder. Read `README.md` in every task directory to check the specific requirement for saving pretrained models.\n\n4.The old version of *DeepKE* is in the [deepke-v1.0](https://github.com/zjunlp/DeepKE/tree/deepke-v1.0) branch. Users can change the branch to use the old version. The old version has been totally transfered to the standard relation extraction ([example/re/standard](https://github.com/zjunlp/DeepKE/blob/main/example/re/standard/README.md)).\n\n5.If you want to modify the source code, it's recommended to install *DeepKE* with source codes. If not, the modification will not work. See [issue](https://github.com/zjunlp/DeepKE/issues/117)\n\n6.More related low-resource knowledge extraction  works can be found in [Knowledge Extraction in Low-Resource Scenarios: Survey and Perspective](https://arxiv.org/pdf/2202.08063.pdf).\n\n7.Make sure the exact versions of requirements in `requirements.txt`.\n\n# To do\nIn next version, we plan to release a stronger LLM for KE. \n\nMeanwhile, we will offer long-term maintenance to **fix bugs**, **solve issues** and meet **new requests**. So if you have any problems, please put issues to us.\n\n# Reading Materials\n\nData-Efficient Knowledge Graph Construction, È´òÊïàÁü•ËØÜÂõæË∞±ÊûÑÂª∫ ([Tutorial on CCKS 2022](http://sigkg.cn/ccks2022/?page_id=24)) \\[[slides](https://drive.google.com/drive/folders/1xqeREw3dSiw-Y1rxLDx77r0hGUvHnuuE)\\] \n\nEfficient and Robust Knowledge Graph Construction ([Tutorial on AACL-IJCNLP 2022](https://www.aacl2022.org/Program/tutorials)) \\[[slides](https://github.com/NLP-Tutorials/AACL-IJCNLP2022-KGC-Tutorial)\\] \n\nPromptKG Family: a Gallery of Prompt Learning & KG-related Research Works, Toolkits, and Paper-list [[Resources](https://github.com/zjunlp/PromptKG)\\] \n\nKnowledge Extraction in Low-Resource Scenarios: Survey and Perspective \\[[Survey](https://arxiv.org/abs/2202.08063)\\]\\[[Paper-list](https://github.com/zjunlp/Low-resource-KEPapers)\\]\n\n\n# Related Toolkit\n\n[Doccano](https://github.com/doccano/doccano)„ÄÅ[MarkTool](https://github.com/FXLP/MarkTool)„ÄÅ[LabelStudio](https://labelstud.io/ ): Data Annotation Toolkits\n\n[LambdaKG](https://github.com/zjunlp/PromptKG/tree/main/lambdaKG): A library and benchmark for PLM-based KG embeddings\n\n[EasyInstruct](https://github.com/zjunlp/EasyInstruct): An easy-to-use framework to instruct Large Language Models\n\n**Reading Materials**:\n\nData-Efficient Knowledge Graph Construction, È´òÊïàÁü•ËØÜÂõæË∞±ÊûÑÂª∫ ([Tutorial on CCKS 2022](http://sigkg.cn/ccks2022/?page_id=24)) \\[[slides](https://drive.google.com/drive/folders/1xqeREw3dSiw-Y1rxLDx77r0hGUvHnuuE)\\] \n\nEfficient and Robust Knowledge Graph Construction ([Tutorial on AACL-IJCNLP 2022](https://www.aacl2022.org/Program/tutorials)) \\[[slides](https://github.com/NLP-Tutorials/AACL-IJCNLP2022-KGC-Tutorial)\\] \n\nPromptKG Family: a Gallery of Prompt Learning & KG-related Research Works, Toolkits, and Paper-list [[Resources](https://github.com/zjunlp/PromptKG)\\] \n\nKnowledge Extraction in Low-Resource Scenarios: Survey and Perspective \\[[Survey](https://arxiv.org/abs/2202.08063)\\]\\[[Paper-list](https://github.com/zjunlp/Low-resource-KEPapers)\\]\n\n\n**Related Toolkit**:\n\n[Doccano](https://github.com/doccano/doccano)„ÄÅ[MarkTool](https://github.com/FXLP/MarkTool)„ÄÅ[LabelStudio](https://labelstud.io/ ): Data Annotation Toolkits\n\n[LambdaKG](https://github.com/zjunlp/PromptKG/tree/main/lambdaKG): A library and benchmark for PLM-based KG embeddings\n\n[EasyInstruct](https://github.com/zjunlp/EasyInstruct): An easy-to-use framework to instruct Large Language Models\n\n# Citation\n\nPlease cite our paper if you use DeepKE in your work\n\n```bibtex\n@inproceedings{EMNLP2022_Demo_DeepKE,\n  author    = {Ningyu Zhang and\n               Xin Xu and\n               Liankuan Tao and\n               Haiyang Yu and\n               Hongbin Ye and\n               Shuofei Qiao and\n               Xin Xie and\n               Xiang Chen and\n               Zhoubo Li and\n               Lei Li},\n  editor    = {Wanxiang Che and\n               Ekaterina Shutova},\n  title     = {DeepKE: {A} Deep Learning Based Knowledge Extraction Toolkit for Knowledge Base Population},\n  booktitle = {{EMNLP} (Demos)},\n  pages     = {98--108},\n  publisher = {Association for Computational Linguistics},\n  year      = {2022},\n  url       = {https://aclanthology.org/2022.emnlp-demos.10}\n}\n```\n<br>\n\n# Contributors\n\n[Ningyu Zhang](https://person.zju.edu.cn/en/ningyu), [Haofen Wang](https://tjdi.tongji.edu.cn/TeacherDetail.do?id=4991&lang=_en), Fei Huang, Feiyu Xiong, Liankuan Tao, Xin Xu, Honghao Gui,  Zhenru Zhang, Chuanqi Tan, Qiang Chen, Xiaohan Wang, Zekun Xi, Xinrong Li, Haiyang Yu, Hongbin Ye, Shuofei Qiao, Peng Wang, Yuqi Zhu, Xin Xie, Xiang Chen, Zhoubo Li, Lei Li, Xiaozhuan Liang, Yunzhi Yao, Jing Chen, Yuqi Zhu, Shumin Deng, Wen Zhang, Guozhou Zheng, Huajun Chen\n\nCommunity Contributors: [thredreams](https://github.com/thredreams), [eltociear](https://github.com/eltociear), Ziwen Xu, Rui Huang, Xiaolong Weng\n\n# Other Knowledge Extraction Open-Source Projects\n\n- [CogIE](https://github.com/jinzhuoran/CogIE)\n- [OpenNRE](https://github.com/thunlp/OpenNRE)\n- [OmniEvent](https://github.com/THU-KEG/OmniEvent)\n- [OpenUE](https://github.com/zjunlp/OpenUE)\n- [OpenIE](https://stanfordnlp.github.io/CoreNLP/openie.html)\n- [RESIN](https://github.com/RESIN-KAIROS/RESIN-pipeline-public)\n- [ZShot](https://github.com/IBM/zshot)\n- [ZS4IE](https://github.com/BBN-E/ZS4IE)\n- [OmniEvent](https://github.com/THU-KEG/OmniEvent)\n"
        },
        {
          "name": "README_CN.md",
          "type": "blob",
          "size": 29.09375,
          "content": "<p align=\"center\">\n    <a href=\"https://github.com/zjunlp/deepke\"> <img src=\"pics/logo.png\" width=\"400\"/></a>\n<p>\n<p align=\"center\">  \n    <a href=\"http://deepke.zjukg.cn\">\n        <img alt=\"Documentation\" src=\"https://img.shields.io/badge/demo-website-blue\">\n    </a>\n    <a href=\"https://pypi.org/project/deepke/#files\">\n        <img alt=\"PyPI\" src=\"https://img.shields.io/pypi/v/deepke\">\n    </a>\n    <a href=\"https://github.com/zjunlp/DeepKE/blob/master/LICENSE\">\n        <img alt=\"GitHub\" src=\"https://img.shields.io/github/license/zjunlp/deepke\">\n    </a>\n    <a href=\"http://zjunlp.github.io/DeepKE\">\n        <img alt=\"Documentation\" src=\"https://img.shields.io/badge/doc-website-red\">\n    </a>\n    <a href=\"https://colab.research.google.com/drive/1vS8YJhJltzw3hpJczPt24O0Azcs3ZpRi?usp=sharing\">\n        <img alt=\"Open In Colab\" src=\"https://colab.research.google.com/assets/colab-badge.svg\">\n    </a>\n</p>\n\n<p align=\"center\">\n    <b> <a href=\"https://github.com/zjunlp/DeepKE/blob/main/README.md\">English</a> | ÁÆÄ‰Ωì‰∏≠Êñá </b>\n</p>\n\n\n<h1 align=\"center\">\n    <p>Âü∫‰∫éÊ∑±Â∫¶Â≠¶‰π†ÁöÑÂºÄÊ∫ê‰∏≠ÊñáÁü•ËØÜÂõæË∞±ÊäΩÂèñÊ°ÜÊû∂</p>\n</h1>\n\n\n[DeepKE](https://arxiv.org/pdf/2201.03335.pdf) ÊòØ‰∏Ä‰∏™ÂºÄÊ∫êÁöÑÁü•ËØÜÂõæË∞±ÊäΩÂèñ‰∏éÊûÑÂª∫Â∑•ÂÖ∑ÔºåÊîØÊåÅ<b>cnSchema„ÄÅ‰ΩéËµÑÊ∫ê„ÄÅÈïøÁØáÁ´†„ÄÅÂ§öÊ®°ÊÄÅ</b>ÁöÑÁü•ËØÜÊäΩÂèñÂ∑•ÂÖ∑ÔºåÂèØ‰ª•Âü∫‰∫é<b>PyTorch</b>ÂÆûÁé∞<b>ÂëΩÂêçÂÆû‰ΩìËØÜÂà´</b>„ÄÅ<b>ÂÖ≥Á≥ªÊäΩÂèñ</b>Âíå<b>Â±ûÊÄßÊäΩÂèñ</b>ÂäüËÉΩ„ÄÇÂêåÊó∂‰∏∫ÂàùÂ≠¶ËÄÖÊèê‰æõ‰∫Ü[ÊñáÊ°£](https://zjunlp.github.io/DeepKE/)Ôºå[Âú®Á∫øÊºîÁ§∫](http://deepke.zjukg.cn/CN/index.html), [ËÆ∫Êñá](https://arxiv.org/pdf/2201.03335.pdf), [ÊºîÁ§∫ÊñáÁ®ø](https://github.com/zjunlp/DeepKE/blob/main/docs/slides/Slides-DeepKE-cn.pdf)Âíå[Êµ∑Êä•](https://drive.google.com/file/d/1vd7xVHlWzoAxivN4T5qKrcqIGDcSM1_7/view)„ÄÇ\n\n- ‚ùóÊÉ≥Áî®**Â§ßÊ®°Âûã**ÂÅöÊäΩÂèñÂêóÔºüËØïËØï[DeepKE-LLM](https://github.com/zjunlp/DeepKE/tree/main/example/llm/README_CN.md)Âíå[OneKE](https://github.com/zjunlp/DeepKE/blob/main/example/llm/OneKE.md)ÔºÅ\n- ‚ùóÊÉ≥Ëá™Â∑±ÂÖ®ÁõëÁù£ËÆ≠ÊäΩÂèñÊ®°ÂûãÂêóÔºüËØïËØï[Âø´ÈÄü‰∏äÊâã](#Âø´ÈÄü‰∏äÊâã), Êàë‰ª¨Êèê‰æõÂÆû‰ΩìËØÜÂà´Ê®°Âûã (‰æãÂ¶Ç[LightNER(COLING'22)](https://github.com/zjunlp/DeepKE/tree/main/example/ner/few-shot/README_CN.md), [W2NER(AAAI'22)](https://github.com/zjunlp/DeepKE/tree/main/example/ner/standard/w2ner/README_CN.md))„ÄÅÂÖ≥Á≥ªÊäΩÂèñÊ®°Âûã(‰æãÂ¶Ç[KnowPrompt(WWW'22)](https://github.com/zjunlp/DeepKE/tree/main/example/re/few-shot/README_CN.md)), [PRGC(ACL'21)](https://github.com/zjunlp/DeepKE/tree/main/example/triple/PRGC/README_CN.md), [PURE(NAACL'21)](https://github.com/zjunlp/DeepKE/tree/main/example/triple/PURE/README_CN.md)), ÂíåÂü∫‰∫écnSchemaÁöÑÂºÄÁÆ±Âç≥Áî®Ê®°Âûã[DeepKE-cnSchema](https://github.com/zjunlp/DeepKE/tree/main/example/triple/cnschema/README_CN.md)ÔºÅ\n- Êé®Ëçê‰ΩøÁî®LinuxÔºåÂ¶ÇÊûú‰ΩøÁî®WindowsÔºåË∑ØÂæÑ‰∏≠ËØ∑‰ΩøÁî®`\\\\`\n- Â¶ÇÊûúÂõ†‰∏∫ÁΩëÁªúÈóÆÈ¢òÊó†Ê≥ï‰ªéHuggingFace‰∏ãËΩΩÊ®°ÂûãÔºåÂèØ‰ª•‰ΩøÁî®wisemodelÂíåmodescope\n\n**Â¶ÇÊûúÊÇ®Âú®ÂÆâË£ÖDeepKEÂíåDeepKE-LLM‰∏≠ÈÅáÂà∞‰ªª‰ΩïÈóÆÈ¢òÔºà‰∏ÄËà¨ÊòØÂåÖÁöÑÁâàÊú¨ÂÖºÂÆπÊÄßÈóÆÈ¢òÔºâ‰∏çÁî®ÂøÉÊÄ•ÔºåÊÇ®ÂèØ‰ª•Êü•ÈòÖ[Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/zjunlp/DeepKE/blob/main/README_CN.md#%E5%A4%87%E6%B3%A8%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98)ÊàñÁõ¥Êé•Êèê[Issue](https://github.com/zjunlp/DeepKE/issues)ÔºåÊàë‰ª¨‰ºöÂ∞ΩÂÖ®ÂäõÂ∏ÆÂä©ÊÇ®Ëß£ÂÜ≥ÈóÆÈ¢ò**ÔºÅ\n\n# ÁõÆÂΩï\n\n- [ÁõÆÂΩï](#ÁõÆÂΩï)\n- [Êñ∞ÁâàÁâπÊÄß](#Êñ∞ÁâàÁâπÊÄß)\n- [È¢ÑÊµãÊºîÁ§∫](#È¢ÑÊµãÊºîÁ§∫)\n- [Ê®°ÂûãÊû∂ÊûÑ](#Ê®°ÂûãÊû∂ÊûÑ)\n- [Âø´ÈÄü‰∏äÊâã](#Âø´ÈÄü‰∏äÊâã)\n  - [DeepKE-LLM](#deepke-llm)\n  - [DeepKE](#deepke)\n      - [üîß ÊâãÂä®ÁéØÂ¢ÉÈÉ®ÁΩ≤](#-ÊâãÂä®ÁéØÂ¢ÉÈÉ®ÁΩ≤)\n      - [üê≥ Âü∫‰∫éÂÆπÂô®ÈÉ®ÁΩ≤](#-Âü∫‰∫éÂÆπÂô®ÈÉ®ÁΩ≤)\n  - [ÁéØÂ¢É‰æùËµñ](#ÁéØÂ¢É‰æùËµñ)\n    - [DeepKE](#deepke-1)\n  - [ÂÖ∑‰ΩìÂäüËÉΩ‰ªãÁªç](#ÂÖ∑‰ΩìÂäüËÉΩ‰ªãÁªç)\n    - [1. ÂëΩÂêçÂÆû‰ΩìËØÜÂà´NER](#1-ÂëΩÂêçÂÆû‰ΩìËØÜÂà´ner)\n    - [2. ÂÖ≥Á≥ªÊäΩÂèñRE](#2-ÂÖ≥Á≥ªÊäΩÂèñre)\n    - [3. Â±ûÊÄßÊäΩÂèñAE](#3-Â±ûÊÄßÊäΩÂèñae)\n    - [4.‰∫ã‰ª∂ÊäΩÂèñ](#4‰∫ã‰ª∂ÊäΩÂèñ)\n- [Â§áÊ≥®ÔºàÂ∏∏ËßÅÈóÆÈ¢òÔºâ](#Â§áÊ≥®Â∏∏ËßÅÈóÆÈ¢ò)\n- [Êú™Êù•ËÆ°Âàí](#Êú™Êù•ËÆ°Âàí)\n- [ÈòÖËØªËµÑÊñô](#ÈòÖËØªËµÑÊñô)\n- [Áõ∏ÂÖ≥Â∑•ÂÖ∑](#Áõ∏ÂÖ≥Â∑•ÂÖ∑)\n- [ÂºïÁî®](#ÂºïÁî®)\n- [È°πÁõÆË¥°ÁåÆ‰∫∫Âëò](#È°πÁõÆË¥°ÁåÆ‰∫∫Âëò)\n- [ÂÖ∂ÂÆÉÁü•ËØÜÊäΩÂèñÂºÄÊ∫êÂ∑•ÂÖ∑](#ÂÖ∂ÂÆÉÁü•ËØÜÊäΩÂèñÂºÄÊ∫êÂ∑•ÂÖ∑)\n\n<br>\n\n# Êñ∞ÁâàÁâπÊÄß\n* `2024Âπ¥4Êúà`ÂèëÂ∏É‰∏≠Ëã±ÂèåËØ≠Â§ßÊ®°ÂûãÁü•ËØÜÊäΩÂèñÊ°ÜÊû∂[OneKE](http://oneke.openkg.cn/)ÔºåÂêåÊó∂ÂºÄÊ∫êÂü∫‰∫éChinese-Alpaca-2-13BÂÖ®ÂèÇÊï∞ÂæÆË∞ÉÁöÑÁâàÊú¨„ÄÇ\n* `2024Âπ¥2Êúà` ÂèëÂ∏ÉÂ§ßËßÑÊ®°(`0.32B` tokens)**ÂèåËØ≠**(‰∏≠ÊñáÂíåËã±Êñá)‰ø°ÊÅØÊäΩÂèñ(IE)Êåá‰ª§Êï∞ÊçÆÈõÜ[IEPile](https://huggingface.co/datasets/zjunlp/iepie), ‰ª•ÂèäÂü∫‰∫é `IEPile` ËÆ≠ÁªÉÁöÑ‰∏§‰∏™Ê®°Âûã[baichuan2-13b-iepile-lora](https://huggingface.co/zjunlp/baichuan2-13b-iepile-lora)„ÄÅ[llama2-13b-iepile-lora](https://huggingface.co/zjunlp/llama2-13b-iepile-lora)„ÄÇ\n* `2023Âπ¥9Êúà` ‰∏∫Âü∫‰∫éÊåá‰ª§ÁöÑÁü•ËØÜÂõæË∞±ÊûÑÂª∫‰ªªÂä°(Instruction-based KGC)ÂèëÂ∏É‰∫Ü‰∏Ä‰∏™‰∏≠Ëã±ÂèåËØ≠‰ø°ÊÅØÊäΩÂèñ(IE)Êåá‰ª§Êï∞ÊçÆÈõÜ `InstructIE`, ÂÖ∑‰ΩìÂèÇËßÅ[Ê≠§Â§Ñ](./example/llm/README_CN.md/#Êï∞ÊçÆ)„ÄÇ\n* `2023Âπ¥6Êúà` ‰∏∫[DeepKE-LLM](https://github.com/zjunlp/DeepKE/tree/main/example/llm)Êñ∞Â¢ûÂ§ö‰∏™Â§ßÊ®°Âûã(Â¶Ç[ChatGLM](https://github.com/THUDM/ChatGLM-6B)„ÄÅLLaMAÁ≥ªÂàó„ÄÅGPTÁ≥ªÂàó„ÄÅÊäΩÂèñÂ§ßÊ®°Âûã[Êô∫Êûê](https://github.com/zjunlp/KnowLM))ÊîØÊåÅ„ÄÇ\n* `2023Âπ¥4Êúà` Êñ∞Â¢ûÂÆû‰ΩìÂÖ≥Á≥ªÊäΩÂèñÊ®°Âûã[CP-NER(IJCAI'23)](https://github.com/zjunlp/DeepKE/blob/main/example/ner/cross/README_CN.md), [ASP(EMNLP'22)](https://github.com/zjunlp/DeepKE/tree/main/example/triple/ASP/README_CN.md), [PRGC(ACL'21)](https://github.com/zjunlp/DeepKE/tree/main/example/triple/PRGC/README_CN.md), [PURE(NAACL'21)](https://github.com/zjunlp/DeepKE/tree/main/example/triple/PURE/README_CN.md), ÊîØÊåÅ[‰∫ã‰ª∂ÊäΩÂèñ](https://github.com/zjunlp/DeepKE/blob/main/example/ee/standard/README_CN.md)(‰∏≠Êñá„ÄÅËã±Êñá), Êèê‰æõÂØπPythonÂ∫ìÈ´òÁ∫ßÁâàÊú¨ÁöÑÊîØÊåÅ (‰æãÂ¶ÇTransformers)„ÄÇ\n* `2023Âπ¥2Êúà` ÊîØÊåÅ[Â§ßÊ®°Âûã](https://github.com/zjunlp/DeepKE/blob/main/example/llm/README_CN.md) (GPT-3)ÔºåÂåÖÂê´In-context Learning (Âü∫‰∫é [EasyInstruct](https://github.com/zjunlp/EasyInstruct))ÂíåÊï∞ÊçÆÁîüÊàêÔºåÊñ∞Â¢ûÂÆû‰ΩìËØÜÂà´Ê®°Âûã[W2NER(AAAI'22)](https://github.com/zjunlp/DeepKE/blob/main/example/ner/standard/README_CN.md)„ÄÇ\n\n<details>\n<summary><b>ÊóßÁâàÊñ∞Èóª</b></summary>\n\n\n- `2022Âπ¥11Êúà` Êñ∞Â¢ûÂÆû‰ΩìËØÜÂà´„ÄÅÂÖ≥Á≥ªÊäΩÂèñÁöÑ[Êï∞ÊçÆÊ†áÊ≥®ËØ¥Êòé](https://github.com/zjunlp/DeepKE/blob/main/README_TAG_CN.md)ÂíåÂº±ÁõëÁù£Êï∞ÊçÆËá™Âä®Ê†áÊ≥®([ÂÆû‰ΩìËØÜÂà´](https://github.com/zjunlp/DeepKE/blob/main/example/ner/prepare-data/README_CN.md)„ÄÅ[ÂÖ≥Á≥ªÊäΩÂèñ](https://github.com/zjunlp/DeepKE/blob/main/example/re/prepare-data/README_CN.md))ÂäüËÉΩÔºå‰ºòÂåñ[Â§öGPUËÆ≠ÁªÉ](https://github.com/zjunlp/DeepKE/blob/main/example/re/standard/README_CN.md)„ÄÇ\n\n- `2022Âπ¥9Êúà` ËÆ∫Êñá [DeepKE: A Deep Learning Based Knowledge Extraction Toolkit for Knowledge Base Population](https://arxiv.org/abs/2201.03335)Ë¢´EMNLP2022 System Demonstration TrackÂΩïÁî®„ÄÇ\n\n- `2022Âπ¥8Êúà` Êñ∞Â¢ûÈíàÂØπ[‰ΩéËµÑÊ∫êÂÖ≥Á≥ªÊäΩÂèñ](https://github.com/zjunlp/DeepKE/tree/main/example/re/few-shot)ÁöÑ[Êï∞ÊçÆÂ¢ûÂº∫](https://github.com/zjunlp/DeepKE/tree/main/example/re/few-shot/DA) (‰∏≠Êñá„ÄÅËã±Êñá)ÂäüËÉΩ„ÄÇ\n\n\n- `2022Âπ¥6Êúà` Êñ∞Â¢ûÊîØÊåÅÂ§öÊ®°ÊÄÅÂú∫ÊôØÁöÑ[ÂÆû‰ΩìÊäΩÂèñ](https://github.com/zjunlp/DeepKE/tree/main/example/ner/multimodal)„ÄÅ[ÂÖ≥Á≥ªÊäΩÂèñ](https://github.com/zjunlp/DeepKE/tree/main/example/re/multimodal)ÂäüËÉΩ„ÄÇ\n\n- `2022Âπ¥5Êúà` ÂèëÂ∏É[DeepKE-cnschema](https://github.com/zjunlp/DeepKE/blob/main/README_CNSCHEMA_CN.md)ÁâπÂà´ÁâàÊ®°ÂûãÔºåÊîØÊåÅÂü∫‰∫écnSchemaÁöÑÂºÄÁÆ±Âç≥Áî®ÁöÑ‰∏≠ÊñáÂÆû‰ΩìËØÜÂà´ÂíåÂÖ≥Á≥ªÊäΩÂèñ„ÄÇ\n\n- `2022Âπ¥1Êúà` ÂèëÂ∏ÉËÆ∫Êñá [DeepKE: A Deep Learning Based Knowledge Extraction Toolkit for Knowledge Base Population](https://arxiv.org/abs/2201.03335)\n\n- `2021Âπ¥12Êúà` Âä†ÂÖ•`dockerfile`‰ª•‰æøËá™Âä®ÂàõÂª∫ÁéØÂ¢É\n\n- `2021Âπ¥11Êúà` ÂèëÂ∏ÉDeepKE demoÈ°µÈù¢ÔºåÊîØÊåÅÂÆûÊó∂ÊäΩÂèñÔºåÊó†ÈúÄÈÉ®ÁΩ≤ÂíåËÆ≠ÁªÉÊ®°Âûã\n- ÂèëÂ∏ÉDeepKEÊñáÊ°£ÔºåÂåÖÂê´DeepKEÊ∫êÁ†ÅÂíåÊï∞ÊçÆÈõÜÁ≠âËØ¶ÁªÜ‰ø°ÊÅØ\n\n- `2021Âπ¥10Êúà` `pip install deepke`\n- deepke-v2.0ÂèëÂ∏É\n\n- `2019Âπ¥8Êúà` `pip install deepke`\n- deepke-v1.0ÂèëÂ∏É\n\n- `2018Âπ¥8Êúà` DeepKEÈ°πÁõÆÂêØÂä®Ôºådeepke-v0.1‰ª£Á†ÅÂèëÂ∏É\n\n</details>\n\n# È¢ÑÊµãÊºîÁ§∫\n‰∏ãÈù¢‰ΩøÁî®‰∏Ä‰∏™demoÂ±ïÁ§∫È¢ÑÊµãËøáÁ®ã„ÄÇËØ•Âä®ÂõæÁî±[Terminalizer](https://github.com/faressoft/terminalizer)ÁîüÊàêÔºåÁîüÊàê[‰ª£Á†Å](https://drive.google.com/file/d/1r4tWfAkpvynH3CBSgd-XG79rf-pB-KR3/view?usp=share_link)ÂèØÁÇπÂáªËé∑Âèñ„ÄÇ\n<img src=\"pics/demo.gif\" width=\"636\" height=\"494\" align=center>\n\n<br>\n\n# Ê®°ÂûãÊû∂ÊûÑ\n\nDeepkeÁöÑÊû∂ÊûÑÂõæÂ¶Ç‰∏ãÊâÄÁ§∫\n\n<h3 align=\"center\">\n    <img src=\"pics/architectures.png\">\n</h3>\n\n- DeepKE‰∏∫‰∏â‰∏™Áü•ËØÜÊäΩÂèñÂäüËÉΩÔºàÂëΩÂêçÂÆû‰ΩìËØÜÂà´„ÄÅÂÖ≥Á≥ªÊäΩÂèñÂíåÂ±ûÊÄßÊäΩÂèñÔºâËÆæËÆ°‰∫Ü‰∏Ä‰∏™Áªü‰∏ÄÁöÑÊ°ÜÊû∂\n- ÂèØ‰ª•Âú®‰∏çÂêåÂú∫ÊôØ‰∏ãÂÆûÁé∞‰∏çÂêåÂäüËÉΩ„ÄÇÊØîÂ¶ÇÔºåÂèØ‰ª•Âú®Ê†áÂáÜÂÖ®ÁõëÁù£„ÄÅ‰ΩéËµÑÊ∫êÂ∞ëÊ†∑Êú¨„ÄÅÊñáÊ°£Á∫ßÂíåÂ§öÊ®°ÊÄÅËÆæÂÆö‰∏ãËøõË°åÂÖ≥Á≥ªÊäΩÂèñ\n- ÊØè‰∏Ä‰∏™Â∫îÁî®Âú∫ÊôØÁî±‰∏â‰∏™ÈÉ®ÂàÜÁªÑÊàêÔºöDataÈÉ®ÂàÜÂåÖÂê´Tokenizer„ÄÅPreprocessorÂíåLoaderÔºåModelÈÉ®ÂàÜÂåÖÂê´Module„ÄÅEncoderÂíåForwarderÔºåCoreÈÉ®ÂàÜÂåÖÂê´Training„ÄÅEvaluationÂíåPrediction\n\n\n<br>\n\n# Âø´ÈÄü‰∏äÊâã\n\n## DeepKE-LLM\nÂ§ßÊ®°ÂûãÊó∂‰ª£, DeepKE-LLMÈááÁî®ÂÖ®Êñ∞ÁöÑÁéØÂ¢É‰æùËµñÔºåÂº∫ÁÉàÂª∫ËÆÆ‰ΩøÁî®linuxÁéØÂ¢ÉÂÆâË£Ö\n```\nconda create -n deepke-llm python=3.9\nconda activate deepke-llm\n\ncd example/llm\npip install -r requirements.txt\n```\nÊ≥®ÊÑèÔºÅÔºÅÊòØexample/llmÊñá‰ª∂Â§π‰∏ãÁöÑ `requirements.txt`\n\n## DeepKE\n- DeepKEÊîØÊåÅpipÂÆâË£Ö‰ΩøÁî®Ôºå‰∏ã‰ª•Â∏∏ËßÑÂÖ≥Á≥ªÊäΩÂèñÂú∫ÊôØ‰∏∫‰æã\n- DeepKEÊîØÊåÅÊâãÂä®ÁéØÂ¢ÉÈÉ®ÁΩ≤‰∏éÂÆπÂô®ÈÉ®ÁΩ≤ÔºåÊÇ®ÂèØ‰ªªÈÄâ‰∏ÄÁßçÊñπÊ≥ïËøõË°åÂÆâË£Ö\n- Âº∫ÁÉàÂª∫ËÆÆ‰ΩøÁî®linuxÁéØÂ¢ÉÂÆâË£ÖÔºÅ\n#### üîß ÊâãÂä®ÁéØÂ¢ÉÈÉ®ÁΩ≤\n**Step 1**Ôºö‰∏ãËΩΩ‰ª£Á†Å ```git clone --depth 1 https://github.com/zjunlp/DeepKE.git```ÔºàÂà´ÂøòËÆ∞starÂíåforkÂìàÔºÅÔºÅÔºÅÔºâ\n\n**Step 2**Ôºö‰ΩøÁî®anacondaÂàõÂª∫ËôöÊãüÁéØÂ¢ÉÔºåËøõÂÖ•ËôöÊãüÁéØÂ¢ÉÔºàÊèê‰æõ[Dockerfile](https://github.com/zjunlp/DeepKE/tree/main/docker)Ê∫êÁ†ÅÂíå[ÊïôÁ®ã](https://github.com/zjunlp/DeepKE/issues/320)ÂèØËá™Ë°åÂàõÂª∫ÈïúÂÉèÔºõÂèØÂèÇËÄÉ[Â§áÊ≥®ÔºàÂ∏∏ËßÅÈóÆÈ¢òÔºâ](#Â§áÊ≥®Â∏∏ËßÅÈóÆÈ¢ò)‰ΩøÁî®ÈïúÂÉèÂä†ÈÄüÔºâ\n\n```bash\nconda create -n deepke python=3.8\n\nconda activate deepke\n```\n1Ôºâ Âü∫‰∫épipÂÆâË£ÖÔºåÁõ¥Êé•‰ΩøÁî® Ôºà**‰∏çÂª∫ËÆÆ‰ΩøÁî®Ê≠§ÊñπÊ≥ïÔºåÂ≠òÂú®pythonÂåÖÂÖºÂÆπÂÜ≤Á™ÅÈ£éÈô©**Ôºâ\n\n```bash\npip install deepke\n```\n- ËØ∑Á°Æ‰øùÊÇ®ÂΩìÂâçÁöÑpipÁâàÊú¨<=24.0\n  \n2Ôºâ Âü∫‰∫éÊ∫êÁ†ÅÂÆâË£Ö\n\n```bash\npip install -r requirements.txt\n\npython setup.py install\n\npython setup.py develop\n```\n\n**Step 3** ÔºöËøõÂÖ•‰ªªÂä°Êñá‰ª∂Â§πÔºå‰ª•Â∏∏ËßÑÂÖ≥Á≥ªÊäΩÂèñ‰∏∫‰æã\n\n```\ncd DeepKE/example/re/standard\n```\n\n**Step 4**Ôºö‰∏ãËΩΩÊï∞ÊçÆÈõÜÔºåÊàñÊ†πÊçÆ[Êï∞ÊçÆÊ†áÊ≥®ËØ¥Êòé](https://github.com/zjunlp/DeepKE/blob/main/README_TAG_CN.md)Ê†áÊ≥®Êï∞ÊçÆ\n```\nwget 120.27.214.45/Data/re/standard/data.tar.gz\n\ntar -xzvf data.tar.gz\n```\n\nÊîØÊåÅÂ§öÁßçÊï∞ÊçÆÁ±ªÂûãÊ†ºÂºèÔºåÂÖ∑‰ΩìËØ∑ËßÅÂêÑÈÉ®ÂàÜÂ≠êREADME„ÄÇ\n\n**Step 5** ÔºöÊ®°ÂûãËÆ≠ÁªÉÔºåËÆ≠ÁªÉÁî®Âà∞ÁöÑÂèÇÊï∞ÂèØÂú®confÊñá‰ª∂Â§πÂÜÖ‰øÆÊîπ\n\nDeepKE‰ΩøÁî®[wandb](https://docs.wandb.ai/quickstart)ÊîØÊåÅÂèØËßÜÂåñË∞ÉÂèÇ\n\n```\npython run.py\n```\n\n**Step 6** ÔºöÊ®°ÂûãÈ¢ÑÊµã„ÄÇÈ¢ÑÊµãÁî®Âà∞ÁöÑÂèÇÊï∞ÂèØÂú®confÊñá‰ª∂Â§πÂÜÖ‰øÆÊîπ\n\n‰øÆÊîπ`conf/predict.yaml`‰∏≠‰øùÂ≠òËÆ≠ÁªÉÂ•ΩÁöÑÊ®°ÂûãË∑ØÂæÑ„ÄÇÈúÄ‰ΩøÁî®Ê®°ÂûãÁöÑÁªùÂØπË∑ØÂæÑ„ÄÇÂ¶Ç`xxx/checkpoints/2019-12-03_17-35-30/cnn_epoch21.pth`„ÄÇ\n```\npython predict.py\n```\n- **‚ùóÊ≥®ÊÑè: Â¶ÇÊûúÊÇ®Âú®ÂÆâË£ÖÊàñ‰ΩøÁî®ËøáÁ®ã‰∏≠ÈÅáÂà∞‰ªª‰ΩïÈóÆÈ¢òÔºåÊÇ®ÂèØ‰ª•Êü•Áúã[Â§áÊ≥®ÔºàÂ∏∏ËßÅÈóÆÈ¢òÔºâ](#Â§áÊ≥®Â∏∏ËßÅÈóÆÈ¢ò) ÊàñÊèê‰∫§ GitHub issue.**\n\n#### üê≥ Âü∫‰∫éÂÆπÂô®ÈÉ®ÁΩ≤\n\n**Step1** ‰∏ãËΩΩDockerÂÆ¢Êà∑Á´Ø\n\n‰ªéÂÆòÁΩë‰∏ãËΩΩDockerÂÆ¢Êà∑Á´ØÂπ∂ÂêØÂä®DockerÊúçÂä°\n\n**Step2** ÊãâÂèñÈïúÂÉèÂπ∂ËøêË°åÂÆπÂô®\n\n```bash\ndocker pull zjunlp/deepke:latest\ndocker run -it zjunlp/deepke:latest /bin/bash\n```\n\nÂâ©‰ΩôÊ≠•È™§Âêå**ÊâãÂä®ÁéØÂ¢ÉÈÉ®ÁΩ≤**‰∏ÄËäÇ‰∏≠ÁöÑ**Step 3**ÂèäÂêéÁª≠Ê≠•È™§Áõ∏Âêå\n\n - **‚ùóÊ≥®ÊÑè: ÊÇ®ÂèØ‰ª•ÂèÇËÄÉ [Tips](#tips) Êù•Âä†ÈÄüÊÇ®ÁöÑÈÉ®ÁΩ≤**\n<br>\n\n## ÁéØÂ¢É‰æùËµñ\n\n\n### DeepKE\n\n> python == 3.8\n\n- torch>=1.5,<=1.11\n- hydra-core==1.0.6\n- tensorboard==2.4.1\n- matplotlib==3.4.1\n- transformers==4.26.0\n- jieba==0.42.1\n- scikit-learn==0.24.1\n- seqeval==1.2.2\n- opt-einsum==3.3.0\n- wandb==0.12.7\n- ujson==5.6.0\n- huggingface_hub==0.11.0\n- tensorboardX==2.5.1\n- nltk==3.8\n- protobuf==3.20.1\n- numpy==1.21.0\n- ipdb==0.13.11\n- pytorch-crf==0.7.2\n- tqdm==4.66.1\n- openai==0.28.0\n- Jinja2==3.1.2\n- datasets==2.13.2\n- pyhocon==0.3.60\n\n<br>\n\n## ÂÖ∑‰ΩìÂäüËÉΩ‰ªãÁªç\n\n### 1. ÂëΩÂêçÂÆû‰ΩìËØÜÂà´NER\n\n- ÂëΩÂêçÂÆû‰ΩìËØÜÂà´ÊòØ‰ªéÈùûÁªìÊûÑÂåñÁöÑÊñáÊú¨‰∏≠ËØÜÂà´Âá∫ÂÆû‰ΩìÂíåÂÖ∂Á±ªÂûã„ÄÇÊï∞ÊçÆ‰∏∫txtÊñá‰ª∂ÔºåÊ†∑ÂºèËåÉ‰æã‰∏∫(Áî®Êà∑ÂèØ‰ª•Âü∫‰∫éÂ∑•ÂÖ∑[Doccano](https://github.com/doccano/doccano)„ÄÅ[MarkTool](https://github.com/FXLP/MarkTool)Ê†áÊ≥®Êï∞ÊçÆÔºå‰πüÂèØ‰ª•ÈÄöËøáDeepKEËá™Â∏¶ÁöÑ[Âº±ÁõëÁù£ÂäüËÉΩ](https://github.com/zjunlp/DeepKE/blob/main/example/ner/prepare-data/README_CN.md)Ëá™Âä®ÂæóÂà∞Êï∞ÊçÆ)Ôºö\n\n  |                           Sentence                           |           Person           |    Location    |          Organization          |\n  | :----------------------------------------------------------: | :------------------------: | :------------: | :----------------------------: |\n  | Êú¨Êä•Âåó‰∫¨9Êúà4Êó•ËÆØËÆ∞ËÄÖÊù®Ê∂åÊä•ÈÅìÔºöÈÉ®ÂàÜÁúÅÂå∫‰∫∫Ê∞ëÊó•Êä•ÂÆ£‰º†ÂèëË°åÂ∑•‰ΩúÂ∫ßË∞à‰ºö9Êúà3Êó•Âú®4Êó•Âú®‰∫¨‰∏æË°å„ÄÇ |            Êù®Ê∂å            |      Âåó‰∫¨      |            ‰∫∫Ê∞ëÊó•Êä•            |\n  | „ÄäÁ∫¢Ê•ºÊ¢¶„ÄãÁî±ÁéãÊâ∂ÊûóÂØºÊºîÔºåÂë®Ê±ùÊòå„ÄÅÁéãËíô„ÄÅÂë®Â≤≠Á≠âÂ§ö‰Ωç‰∏ìÂÆ∂ÂèÇ‰∏éÂà∂‰Ωú„ÄÇ | ÁéãÊâ∂ÊûóÔºåÂë®Ê±ùÊòåÔºåÁéãËíôÔºåÂë®Â≤≠ |            |  |\n  | Áß¶ÂßãÁöáÂÖµÈ©¨‰øë‰Ωç‰∫éÈôïË•øÁúÅË•øÂÆâÂ∏Ç,ÊòØ‰∏ñÁïåÂÖ´Â§ßÂ•áËøπ‰πã‰∏Ä„ÄÇ |           Áß¶ÂßãÁöá           | ÈôïË•øÁúÅÔºåË•øÂÆâÂ∏Ç |                          |\n\n- ÂÖ∑‰ΩìÊµÅÁ®ãËØ∑ËøõÂÖ•ËØ¶ÁªÜÁöÑREADME‰∏≠\n  - **[Â∏∏ËßÑÂÖ®ÁõëÁù£STANDARD](https://github.com/zjunlp/DeepKE/tree/main/example/ner/standard)**  \n  \n     ***Êàë‰ª¨ËøòÊèê‰æõ‰∫Ü[Â§ßÊ®°ÂûãÊîØÊåÅ](https://github.com/zjunlp/DeepKE/blob/main/example/llm/README_CN.md)ÂíåÂºÄÁÆ±Âç≥Áî®ÁöÑ[DeepKE-cnSchemaÁâπÂà´Áâà](https://github.com/zjunlp/DeepKE/blob/main/README_CNSCHEMA_CN.md)ÔºåÊó†ÈúÄËÆ≠ÁªÉÂç≥ÂèØÊäΩÂèñÊîØÊåÅcnSchemaÁöÑÂÆû‰Ωì***\n  \n     **Step1**: ËøõÂÖ•`DeepKE/example/ner/standard`Ôºå‰∏ãËΩΩÊï∞ÊçÆÈõÜ\n     \n     ```bash\n     wget 120.27.214.45/Data/ner/standard/data.tar.gz\n     \n     tar -xzvf data.tar.gz\n     ```\n     \n     **Step2**: Ê®°ÂûãËÆ≠ÁªÉ<br>\n     \n     Êï∞ÊçÆÈõÜÂíåÂèÇÊï∞ÈÖçÁΩÆÂèØ‰ª•ÂàÜÂà´Âú®`data`Âíå`conf`Êñá‰ª∂Â§π‰∏≠‰øÆÊîπ\n     \n     ```\n     python run.py\n     ```\n     \n     **Step3**: Ê®°ÂûãÈ¢ÑÊµã\n     ```\n     python predict.py\n     ```\n     \n  - **[Â∞ëÊ†∑Êú¨FEW-SHOT](https://github.com/zjunlp/DeepKE/tree/main/example/ner/few-shot)** \n  \n    **Step1**: ËøõÂÖ•`DeepKE/example/ner/few-shot`Ôºå‰∏ãËΩΩÊï∞ÊçÆÈõÜ\n    \n    ```bash\n    wget 120.27.214.45/Data/ner/few_shot/data.tar.gz\n    \n    tar -xzvf data.tar.gz\n    ```\n    \n    **Step2**Ôºö‰ΩéËµÑÊ∫êÂú∫ÊôØ‰∏ãËÆ≠ÁªÉÊ®°Âûã<br>\n    \n    Ê®°ÂûãÂä†ËΩΩÂíå‰øùÂ≠ò‰ΩçÁΩÆ‰ª•ÂèäÂèÇÊï∞ÈÖçÁΩÆÂèØ‰ª•Âú®`conf`Êñá‰ª∂Â§π‰∏≠‰øÆÊîπ\n    \n     ```\n     python run.py +train=few_shot\n     ```\n    \n    Ëã•Ë¶ÅÂä†ËΩΩÊ®°ÂûãÔºå‰øÆÊîπ`few_shot.yaml`‰∏≠ÁöÑ`load_path`Ôºõ<br>\n    \n    **Step3**ÔºöÂú®`config.yaml`‰∏≠ËøΩÂä†`- predict`Ôºå`predict.yaml`‰∏≠‰øÆÊîπ`load_path`‰∏∫Ê®°ÂûãË∑ØÂæÑ‰ª•Âèä`write_path`‰∏∫È¢ÑÊµãÁªìÊûúÁöÑ‰øùÂ≠òË∑ØÂæÑÔºåÂÆåÊàê‰øÆÊîπÂêé‰ΩøÁî®\n    \n    ```\n    python predict.py\n    ```\n\n  - **[Â§öÊ®°ÊÄÅ](https://github.com/zjunlp/DeepKE/tree/main/example/ner/multimodal)**\n\n    **Step1**: ËøõÂÖ• `DeepKE/example/ner/multimodal`Ôºå ‰∏ãËΩΩÊï∞ÊçÆÈõÜ\n\n    ```bash\n    wget 120.27.214.45/Data/ner/multimodal/data.tar.gz\n    \n    tar -xzvf data.tar.gz\n    ```\n\n    Êàë‰ª¨Âú®ÂéüÂßãÂõæÂÉè‰∏ä‰ΩøÁî®[faster_rcnn](https://github.com/pytorch/vision/blob/main/torchvision/models/detection/faster_rcnn.py)Âíå[visual groundingÂ∑•ÂÖ∑](https://github.com/zyang-ur/onestage_grounding)ÂàÜÂà´ÊäΩÂèñRCNN objectsÂíåvisual grounding objectsÊù•‰Ωú‰∏∫Â±ÄÈÉ®ËßÜËßâ‰ø°ÊÅØ\n\n    **Step2** Â§öÊ®°ÊÄÅÂú∫ÊôØ‰∏ãËÆ≠ÁªÉÊ®°Âûã <br>\n\n    - Êï∞ÊçÆÈõÜÂíåÂèÇÊï∞ÈÖçÁΩÆÂèØ‰ª•ÂàÜÂà´ËøõÂÖ•`data`Âíå`conf`Êñá‰ª∂Â§π‰∏≠‰øÆÊîπ\n    - Â¶ÇÈúÄ‰ªé‰∏äÊ¨°ËÆ≠ÁªÉÁöÑÊ®°ÂûãÂºÄÂßãËÆ≠ÁªÉÔºöËÆæÁΩÆ`conf/train.yaml`‰∏≠ÁöÑ`load_path`‰∏∫‰∏äÊ¨°‰øùÂ≠òÊ®°ÂûãÁöÑË∑ØÂæÑÔºåÊØèÊ¨°ËÆ≠ÁªÉÁöÑÊó•ÂøóÈªòËÆ§‰øùÂ≠òÂú®Ê†πÁõÆÂΩïÔºåÂèØÁî®`log_dir`Êù•ÈÖçÁΩÆ\n\n    ```bash\n    python run.py\n    ```\n\n    **Step3** Ê®°ÂûãÈ¢ÑÊµã\n\n    ```bash\n    python predict.py\n    ```\n\n### 2. ÂÖ≥Á≥ªÊäΩÂèñRE\n\n- ÂÖ≥Á≥ªÊäΩÂèñÊòØ‰ªéÈùûÁªìÊûÑÂåñÁöÑÊñáÊú¨‰∏≠ÊäΩÂèñÂá∫ÂÆû‰Ωì‰πãÈó¥ÁöÑÂÖ≥Á≥ªÔºå‰ª•‰∏ã‰∏∫Âá†‰∏™Ê†∑ÂºèËåÉ‰æãÔºåÊï∞ÊçÆ‰∏∫csvÊñá‰ª∂(Áî®Êà∑ÂèØ‰ª•Âü∫‰∫éÂ∑•ÂÖ∑[Doccano](https://github.com/doccano/doccano)„ÄÅ[MarkTool](https://github.com/FXLP/MarkTool)Ê†áÊ≥®Êï∞ÊçÆÔºå‰πüÂèØ‰ª•ÈÄöËøáDeepKEËá™Â∏¶ÁöÑ[Âº±ÁõëÁù£ÂäüËÉΩ](https://github.com/zjunlp/DeepKE/blob/main/example/re/prepare-data/README_CN.md)Ëá™Âä®ÂæóÂà∞Êï∞ÊçÆ)Ôºö\n\n  |                        Sentence                        | Relation |    Head    | Head_offset |    Tail    | Tail_offset |\n  | :----------------------------------------------------: | :------: | :--------: | :---------: | :--------: | :---------: |\n  | „ÄäÂ≤≥Áà∂‰πüÊòØÁàπ„ÄãÊòØÁéãÂÜõÊâßÂØºÁöÑÁîµËßÜÂâßÔºåÁî±È©¨ÊÅ©ÁÑ∂„ÄÅËåÉÊòé‰∏ªÊºî„ÄÇ |   ÂØºÊºî   | Â≤≥Áà∂‰πüÊòØÁàπ |      1      |    ÁéãÂÜõ    |      8      |\n  |  „Ää‰πùÁéÑÁè†„ÄãÊòØÂú®Á∫µÊ®™‰∏≠ÊñáÁΩëËøûËΩΩÁöÑ‰∏ÄÈÉ®Â∞èËØ¥Ôºå‰ΩúËÄÖÊòØÈæôÈ©¨„ÄÇ  | ËøûËΩΩÁΩëÁ´ô |   ‰πùÁéÑÁè†   |      1      | Á∫µÊ®™‰∏≠ÊñáÁΩë |      7      |\n  |     ÊèêËµ∑Êù≠Â∑ûÁöÑÁæéÊôØÔºåË•øÊπñÊÄªÊòØÁ¨¨‰∏Ä‰∏™Êò†ÂÖ•ËÑëÊµ∑ÁöÑËØçËØ≠„ÄÇ     | ÊâÄÂú®ÂüéÂ∏Ç |    Ë•øÊπñ    |      8      |    Êù≠Â∑û    |      2      |\n  \n- **‚ùóNOTE: Â¶ÇÊûúÊÇ®‰ΩøÁî®ÁöÑÂêå‰∏Ä‰∏™ÂÖ≥Á≥ªÂ≠òÂú®Â§öÁßçÂÆû‰ΩìÁ±ªÂûãÔºåÂèØ‰ª•ÈááÂèñÂØπÂÆû‰ΩìÁ±ªÂûãÂä†ÂÖ≥Á≥ªÂâçÁºÄÁöÑÊñπÂºèÊûÑÈÄ†ËæìÂÖ•„ÄÇ**\n\n- ÂÖ∑‰ΩìÊµÅÁ®ãËØ∑ËøõÂÖ•ËØ¶ÁªÜÁöÑREADME‰∏≠ÔºåREÂåÖÊã¨‰∫Ü‰ª•‰∏ã‰∏â‰∏™Â≠êÂäüËÉΩ\n  - **[Â∏∏ËßÑÂÖ®ÁõëÁù£STANDARD](https://github.com/zjunlp/DeepKE/tree/main/example/re/standard)**  \n\n     ***Êàë‰ª¨ËøòÊèê‰æõ‰∫Ü[Â§ßÊ®°ÂûãÊîØÊåÅ](https://github.com/zjunlp/DeepKE/blob/main/example/llm/README_CN.md)ÂíåÂºÄÁÆ±Âç≥Áî®ÁöÑ[DeepKE-cnSchemaÁâπÂà´Áâà](https://github.com/zjunlp/DeepKE/blob/main/README_CNSCHEMA_CN.md)ÔºåÊó†ÈúÄËÆ≠ÁªÉÂç≥ÂèØÊäΩÂèñÊîØÊåÅcnSchemaÁöÑÂÖ≥Á≥ª***\n\n    **Step1**ÔºöËøõÂÖ•`DeepKE/example/re/standard`Ôºå‰∏ãËΩΩÊï∞ÊçÆÈõÜ\n  \n    ```bash\n    wget 120.27.214.45/Data/re/standard/data.tar.gz\n    \n    tar -xzvf data.tar.gz\n    ```\n  \n    **Step2**ÔºöÊ®°ÂûãËÆ≠ÁªÉ<br>\n\n    Êï∞ÊçÆÈõÜÂíåÂèÇÊï∞ÈÖçÁΩÆÂèØ‰ª•ÂàÜÂà´ËøõÂÖ•`data`Âíå`conf`Êñá‰ª∂Â§π‰∏≠‰øÆÊîπ\n  \n    ```\n    python run.py\n    ```\n  \n    **Step3**ÔºöÊ®°ÂûãÈ¢ÑÊµã\n  \n    ```\n    python predict.py\n    ```\n  \n  - **[Â∞ëÊ†∑Êú¨FEW-SHOT](https://github.com/zjunlp/DeepKE/tree/main/example/re/few-shot)**\n  \n    **Step1**ÔºöËøõÂÖ•`DeepKE/example/re/few-shot`Ôºå‰∏ãËΩΩÊï∞ÊçÆÈõÜ\n\n    ```bash\n    wget 120.27.214.45/Data/re/few_shot/data.tar.gz\n    \n    tar -xzvf data.tar.gz\n    ```\n  \n    **Step2**ÔºöÊ®°ÂûãËÆ≠ÁªÉ<br>\n  \n    - Êï∞ÊçÆÈõÜÂíåÂèÇÊï∞ÈÖçÁΩÆÂèØ‰ª•ÂàÜÂà´ËøõÂÖ•`data`Âíå`conf`Êñá‰ª∂Â§π‰∏≠‰øÆÊîπ\n  \n    - Â¶ÇÈúÄ‰ªé‰∏äÊ¨°ËÆ≠ÁªÉÁöÑÊ®°ÂûãÂºÄÂßãËÆ≠ÁªÉÔºöËÆæÁΩÆ`conf/train.yaml`‰∏≠ÁöÑ`train_from_saved_model`‰∏∫‰∏äÊ¨°‰øùÂ≠òÊ®°ÂûãÁöÑË∑ØÂæÑÔºåÊØèÊ¨°ËÆ≠ÁªÉÁöÑÊó•ÂøóÈªòËÆ§‰øùÂ≠òÂú®Ê†πÁõÆÂΩïÔºåÂèØÁî®`log_dir`Êù•ÈÖçÁΩÆ\n  \n    ```\n    python run.py\n    ```\n  \n    **Step3**ÔºöÊ®°ÂûãÈ¢ÑÊµã\n  \n    ```\n    python predict.py\n    ```\n  \n  - **[ÊñáÊ°£Á∫ßDOCUMENT](https://github.com/zjunlp/DeepKE/tree/main/example/re/document)** <br>\n    \n    **Step1**ÔºöËøõÂÖ•`DeepKE/example/re/document`Ôºå‰∏ãËΩΩÊï∞ÊçÆÈõÜ\n    \n    ```bash\n    wget 120.27.214.45/Data/re/document/data.tar.gz\n    \n    tar -xzvf data.tar.gz\n    ```\n    \n    **Step2**ÔºöÊ®°ÂûãËÆ≠ÁªÉ<br>\n    \n    - Êï∞ÊçÆÈõÜÂíåÂèÇÊï∞ÈÖçÁΩÆÂèØ‰ª•ÂàÜÂà´ËøõÂÖ•`data`Âíå`conf`Êñá‰ª∂Â§π‰∏≠‰øÆÊîπ\n    - Â¶ÇÈúÄ‰ªé‰∏äÊ¨°ËÆ≠ÁªÉÁöÑÊ®°ÂûãÂºÄÂßãËÆ≠ÁªÉÔºöËÆæÁΩÆ`conf/train.yaml`‰∏≠ÁöÑ`train_from_saved_model`‰∏∫‰∏äÊ¨°‰øùÂ≠òÊ®°ÂûãÁöÑË∑ØÂæÑÔºåÊØèÊ¨°ËÆ≠ÁªÉÁöÑÊó•ÂøóÈªòËÆ§‰øùÂ≠òÂú®Ê†πÁõÆÂΩïÔºåÂèØÁî®`log_dir`Êù•ÈÖçÁΩÆÔºõ\n    \n    ```\n    python run.py\n    ```\n    **Step3**ÔºöÊ®°ÂûãÈ¢ÑÊµã\n    \n    ```\n    python predict.py\n    ```\n\n  - **[Â§öÊ®°ÊÄÅ](https://github.com/zjunlp/DeepKE/tree/main/example/re/multimodal)**\n\n    **Step1**: ËøõÂÖ• `DeepKE/example/re/multimodal`Ôºå ‰∏ãËΩΩÊï∞ÊçÆÈõÜ\n\n    ```bash\n    wget 120.27.214.45/Data/re/multimodal/data.tar.gz\n    \n    tar -xzvf data.tar.gz\n    ```\n\n    Êàë‰ª¨Âú®ÂéüÂßãÂõæÂÉè‰∏ä‰ΩøÁî®[faster_rcnn](https://github.com/pytorch/vision/blob/main/torchvision/models/detection/faster_rcnn.py)Âíå[visual groundingÂ∑•ÂÖ∑](https://github.com/zyang-ur/onestage_grounding)ÂàÜÂà´ÊäΩÂèñRCNN objectsÂíåvisual grounding objectsÊù•‰Ωú‰∏∫Â±ÄÈÉ®ËßÜËßâ‰ø°ÊÅØ\n\n    **Step2** Ê®°ÂûãËÆ≠ÁªÉ <br>\n\n    - Êï∞ÊçÆÈõÜÂíåÂèÇÊï∞ÈÖçÁΩÆÂèØ‰ª•ÂàÜÂà´ËøõÂÖ•`data`Âíå`conf`Êñá‰ª∂Â§π‰∏≠‰øÆÊîπ\n    - Â¶ÇÈúÄ‰ªé‰∏äÊ¨°ËÆ≠ÁªÉÁöÑÊ®°ÂûãÂºÄÂßãËÆ≠ÁªÉÔºöËÆæÁΩÆ`conf/train.yaml`‰∏≠ÁöÑ`load_path`‰∏∫‰∏äÊ¨°‰øùÂ≠òÊ®°ÂûãÁöÑË∑ØÂæÑÔºåÊØèÊ¨°ËÆ≠ÁªÉÁöÑÊó•ÂøóÈªòËÆ§‰øùÂ≠òÂú®Ê†πÁõÆÂΩïÔºåÂèØÁî®`log_dir`Êù•ÈÖçÁΩÆ\n\n    ```bash\n    python run.py\n    ```\n\n    **Step3** Ê®°ÂûãÈ¢ÑÊµã\n\n    ```bash\n    python predict.py\n    ```\n\n### 3. Â±ûÊÄßÊäΩÂèñAE\n\n- Êï∞ÊçÆ‰∏∫csvÊñá‰ª∂ÔºåÊ†∑ÂºèËåÉ‰æã‰∏∫Ôºö\n\n  |                           Sentence                           |   Att    |   Ent    | Ent_offset |      Val      | Val_offset |\n  | :----------------------------------------------------------: | :------: | :------: | :--------: | :-----------: | :--------: |\n  |          Âº†ÂÜ¨Ê¢ÖÔºåÂ•≥ÔºåÊ±âÊóèÔºå1968Âπ¥2ÊúàÁîüÔºåÊ≤≥ÂçóÊ∑áÂéø‰∫∫           |   Ê∞ëÊóè   |  Âº†ÂÜ¨Ê¢Ö  |     0      |     Ê±âÊóè      |     6      |\n  | ËØ∏Ëëõ‰∫ÆÔºåÂ≠óÂ≠îÊòéÔºå‰∏âÂõΩÊó∂ÊúüÊù∞Âá∫ÁöÑÂÜõ‰∫ãÂÆ∂„ÄÅÊñáÂ≠¶ÂÆ∂„ÄÅÂèëÊòéÂÆ∂„ÄÇ |   Êúù‰ª£   |   ËØ∏Ëëõ‰∫Æ   |     0      |     ‰∏âÂõΩÊó∂Êúü      |     8     |\n  |        2014Âπ¥10Êúà1Êó•ËÆ∏ÈûçÂçéÊâßÂØºÁöÑÁîµÂΩ±„ÄäÈªÑÈáëÊó∂‰ª£„Äã‰∏äÊò†         | ‰∏äÊò†Êó∂Èó¥ | ÈªÑÈáëÊó∂‰ª£ |     19     | 2014Âπ¥10Êúà1Êó• |     0      |\n\n- ÂÖ∑‰ΩìÊµÅÁ®ãËØ∑ËøõÂÖ•ËØ¶ÁªÜÁöÑREADME‰∏≠\n  - **[Â∏∏ËßÑÂÖ®ÁõëÁù£STANDARD](https://github.com/zjunlp/DeepKE/tree/main/example/ae/standard)**  \n    \n    **Step1**ÔºöËøõÂÖ•`DeepKE/example/ae/standard`Ôºå‰∏ãËΩΩÊï∞ÊçÆÈõÜ\n    \n    ```bash\n    wget 120.27.214.45/Data/ae/standard/data.tar.gz\n    \n    tar -xzvf data.tar.gz\n    ```\n    \n    **Step2**ÔºöÊ®°ÂûãËÆ≠ÁªÉ<br>\n\n    Êï∞ÊçÆÈõÜÂíåÂèÇÊï∞ÈÖçÁΩÆÂèØ‰ª•ÂàÜÂà´ËøõÂÖ•`data`Âíå`conf`Êñá‰ª∂Â§π‰∏≠‰øÆÊîπ\n    \n    ```\n    python run.py\n    ```\n    \n    **Step3**ÔºöÊ®°ÂûãÈ¢ÑÊµã\n    \n    ```\n    python predict.py\n    ```\n\n<br>\n\n### 4.‰∫ã‰ª∂ÊäΩÂèñ\n\n* ‰∫ã‰ª∂ÊäΩÂèñÊòØÊåá‰ªé‰∏ÄÊÆµÊó†ÁªìÊûÑÂåñÁöÑÊñáÊú¨‰∏≠ÊäΩÂèñÂá∫Êüê‰∏™‰∫ã‰ª∂ÁöÑ‰∫ã‰ª∂Á±ªÂûã„ÄÅ‰∫ã‰ª∂Ëß¶ÂèëËØç„ÄÅËÆ∫ÂÖÉËßíËâ≤‰ª•ÂèäËÆ∫ÂÖÉ„ÄÇ\n\n* Êï∞ÊçÆ‰∏∫`.tsv`Êñá‰ª∂ÔºåÊ†∑‰æã‰∏∫Ôºö\n\n  <table h style=\"text-align:center\">\n      <tr>\n          <th colspan=\"2\"> Sentence </th>\n          <th> Event type </th>\n          <th> Trigger </th>\n          <th> Role </th>\n          <th> Argument </th>\n      </tr>\n      <tr> \n          <td rowspan=\"3\" colspan=\"2\"> ÊçÆ„ÄäÊ¨ßÊ¥≤Êó∂Êä•„ÄãÊä•ÈÅìÔºåÂΩìÂú∞Êó∂Èó¥27Êó•ÔºåÊ≥ïÂõΩÂ∑¥ÈªéÂç¢ÊµÆÂÆ´ÂçöÁâ©È¶ÜÂëòÂ∑•Âõ†‰∏çÊª°Â∑•‰ΩúÊù°‰ª∂ÊÅ∂ÂåñËÄåÁΩ¢Â∑•ÔºåÂØºËá¥ËØ•ÂçöÁâ©È¶Ü‰πüÂõ†Ê≠§Èó≠Èó®Ë∞¢ÂÆ¢‰∏ÄÂ§©„ÄÇ </td>\n        \t<td rowspan=\"3\"> ÁªÑÁªáË°å‰∏∫-ÁΩ¢Â∑• </td>\n      \t\t<td rowspan=\"3\"> ÁΩ¢Â∑• </td>\n      \t\t<td> ÁΩ¢Â∑•‰∫∫Âëò </td>\n      \t\t<td> Ê≥ïÂõΩÂ∑¥ÈªéÂç¢ÊµÆÂÆ´ÂçöÁâ©È¶ÜÂëòÂ∑• </td>\n      </tr>\n      <tr> \n          <td> Êó∂Èó¥ </td>\n          <td> ÂΩìÂú∞Êó∂Èó¥27Êó• </td>\n      </tr>\n      <tr> \n          <td> ÊâÄÂ±ûÁªÑÁªá </td>\n          <td> Ê≥ïÂõΩÂ∑¥ÈªéÂç¢ÊµÆÂÆ´ÂçöÁâ©È¶Ü </td>\n      </tr>\n      <tr> \n          <td rowspan=\"3\" colspan=\"2\"> ‰∏≠ÂõΩÂ§ñËøê2019Âπ¥‰∏äÂçäÂπ¥ÂΩíÊØçÂáÄÂà©Ê∂¶Â¢ûÈïø17%ÔºöÊî∂Ë¥≠‰∫ÜÂ∞ëÊï∞ËÇ°‰∏úËÇ°ÊùÉ </td>\n        \t<td rowspan=\"3\"> Ë¥¢Áªè/‰∫§Êòì-Âá∫ÂîÆ/Êî∂Ë¥≠ </td>\n      \t\t<td rowspan=\"3\"> Êî∂Ë¥≠ </td>\n      \t\t<td> Âá∫ÂîÆÊñπ </td>\n      \t\t<td> Â∞ëÊï∞ËÇ°‰∏ú </td>\n      </tr>\n      <tr> \n          <td> Êî∂Ë¥≠Êñπ </td>\n          <td> ‰∏≠ÂõΩÂ§ñËøê </td>\n      </tr>\n      <tr> \n          <td> ‰∫§ÊòìÁâ© </td>\n          <td> ËÇ°ÊùÉ </td>\n      </tr>\n      <tr> \n          <td rowspan=\"3\" colspan=\"2\"> ÁæéÂõΩ‰∫öÁâπÂÖ∞Â§ßËà™Â±ï13Êó•ÂèëÁîü‰∏ÄËµ∑Ë°®ÊºîÊú∫Âù†Êú∫‰∫ãÊïÖÔºåÈ£ûË°åÂëòÂºπÂ∞ÑÂá∫Ëà±Âπ∂ÂÆâÂÖ®ÁùÄÈôÜÔºå‰∫ãÊïÖÊ≤°ÊúâÈÄ†Êàê‰∫∫Âëò‰º§‰∫°„ÄÇ </td>\n        \t<td rowspan=\"3\"> ÁÅæÂÆ≥/ÊÑèÂ§ñ-Âù†Êú∫ </td>\n      \t\t<td rowspan=\"3\"> Âù†Êú∫ </td>\n      \t\t<td> Êó∂Èó¥ </td>\n      \t\t<td> 13Êó• </td>\n      </tr>\n      <tr> \n          <td> Âú∞ÁÇπ </td>\n          <td> ÁæéÂõΩ‰∫öÁâπÂÖ∞ </td>\n    \t</tr>\n  </table>\n\n- ÂÖ∑‰ΩìÊµÅÁ®ãËØ∑ËøõÂÖ•ËØ¶ÁªÜÁöÑREADME‰∏≠\n\n  - **[Â∏∏ËßÑÂÖ®ÁõëÁù£STANDARD](./example/ee/standard/README_CN.md)**  \n\n    **Step1**ÔºöËøõÂÖ•`DeepKE/example/ee/standard`Ôºå‰∏ãËΩΩÊï∞ÊçÆÈõÜ\n\n    ```bash\n    wget 120.27.214.45/Data/ee/DuEE.zip\n    unzip DuEE.zip\n    ```\n\n    **Step2**ÔºöÊ®°ÂûãËÆ≠ÁªÉ<br>\n\n    Êï∞ÊçÆÈõÜÂíåÂèÇÊï∞ÈÖçÁΩÆÂèØ‰ª•ÂàÜÂà´ËøõÂÖ•`data`Âíå`conf`Êñá‰ª∂Â§π‰∏≠‰øÆÊîπ\n\n    ```\n    python run.py\n    ```\n\n    **Step3**ÔºöÊ®°ÂûãÈ¢ÑÊµã\n\n    ```\n    python predict.py\n    ```\n\n<br>\n\n# Â§áÊ≥®ÔºàÂ∏∏ËßÅÈóÆÈ¢òÔºâ\n\n1.‰ΩøÁî® Anaconda Êó∂Ôºå```Âª∫ËÆÆÊ∑ªÂä†ÂõΩÂÜÖÈïúÂÉè```Ôºå‰∏ãËΩΩÈÄüÂ∫¶Êõ¥Âø´„ÄÇÂ¶Ç[ÈïúÂÉè](https://mirrors.tuna.tsinghua.edu.cn/help/anaconda/)„ÄÇ\n\n2.‰ΩøÁî® pip Êó∂Ôºå```Âª∫ËÆÆ‰ΩøÁî®ÂõΩÂÜÖÈïúÂÉè```Ôºå‰∏ãËΩΩÈÄüÂ∫¶Êõ¥Âø´ÔºåÂ¶ÇÈòøÈáå‰∫ëÈïúÂÉè„ÄÇ\n\n3.ÂÆâË£ÖÂêéÊèêÁ§∫ `ModuleNotFoundError: No module named 'past'`ÔºåËæìÂÖ•ÂëΩ‰ª§ `pip install future` Âç≥ÂèØËß£ÂÜ≥„ÄÇ\n\n4.‰ΩøÁî®ËØ≠Ë®ÄÈ¢ÑËÆ≠ÁªÉÊ®°ÂûãÊó∂ÔºåÂú®Á∫øÂÆâË£Ö‰∏ãËΩΩÊ®°ÂûãÊØîËæÉÊÖ¢ÔºåÊõ¥Âª∫ËÆÆÊèêÂâç‰∏ãËΩΩÂ•ΩÔºåÂ≠òÊîæÂà∞ pretrained Êñá‰ª∂Â§πÂÜÖ„ÄÇÂÖ∑‰ΩìÂ≠òÊîæÊñá‰ª∂Ë¶ÅÊ±ÇËßÅÊñá‰ª∂Â§πÂÜÖÁöÑ `README.md`„ÄÇ\n\n5.DeepKEËÄÅÁâàÊú¨‰Ωç‰∫é[deepke-v1.0](https://github.com/zjunlp/DeepKE/tree/deepke-v1.0)ÂàÜÊîØÔºåÁî®Êà∑ÂèØÂàáÊç¢ÂàÜÊîØ‰ΩøÁî®ËÄÅÁâàÊú¨ÔºåËÄÅÁâàÊú¨ÁöÑËÉΩÂäõÂ∑≤ÂÖ®ÈÉ®ËøÅÁßªÂà∞Ê†áÂáÜËÆæÂÆöÂÖ≥Á≥ªÊäΩÂèñ([example/re/standard](https://github.com/zjunlp/DeepKE/blob/main/example/re/standard/README.md))‰∏≠„ÄÇ\n\n6.Â¶ÇÊûúÊÇ®ÈúÄË¶ÅÂú®Ê∫êÁ†ÅÁöÑÂü∫Á°Ä‰∏äËøõË°å‰øÆÊîπÔºåÂª∫ËÆÆ‰ΩøÁî®`python setup.py install`ÊñπÂºèÂÆâË£Ö*DeepKE*ÔºåÂ¶ÇÊú™‰ΩøÁî®ËØ•ÊñπÂºèÂÆâË£ÖÔºåÊ∫êÁ†Å‰øÆÊîπÈÉ®ÂàÜ‰∏ç‰ºöÁîüÊïàÔºåËßÅ[ÈóÆÈ¢ò](https://github.com/zjunlp/DeepKE/issues/117)„ÄÇ\n\n7.Êõ¥Â§öÁöÑ‰ΩéËµÑÊ∫êÊäΩÂèñÂ∑•‰ΩúÂèØÊü•ÈòÖËÆ∫Êñá [Knowledge Extraction in Low-Resource Scenarios: Survey and Perspective](https://arxiv.org/pdf/2202.08063.pdf)„ÄÇ\n\n8.Á°Æ‰øù‰ΩøÁî®requirements.txt‰∏≠ÂØπÂ∫îÁöÑÂêÑ‰æùËµñÂåÖÁöÑÁâàÊú¨„ÄÇ\n\n<br>\n\n# Êú™Êù•ËÆ°Âàí\n\n- Âú®DeepKEÁöÑ‰∏ã‰∏Ä‰∏™ÁâàÊú¨‰∏≠ÂèëÂ∏É‰ºòÂåñÂêéÁöÑ‰∏≠Ëã±ÂèåËØ≠ÊäΩÂèñÂ§ßÊ®°Âûã\n- Êàë‰ª¨Êèê‰æõÈïøÊúüÊäÄÊúØÁª¥Êä§ÂíåÁ≠îÁñëËß£ÊÉë„ÄÇÂ¶ÇÊúâÁñëÈóÆÔºåËØ∑Êèê‰∫§issues\n\n\n# ÈòÖËØªËµÑÊñô\n\nData-Efficient Knowledge Graph Construction, È´òÊïàÁü•ËØÜÂõæË∞±ÊûÑÂª∫ ([Tutorial on CCKS 2022](http://sigkg.cn/ccks2022/?page_id=24)) \\[[slides](https://pan.baidu.com/s/1yMskUVU188-4dcf96lVrWg?pwd=gy8y)\\] \n\nEfficient and Robust Knowledge Graph Construction ([Tutorial on AACL-IJCNLP 2022](https://www.aacl2022.org/Program/tutorials)) \\[[slides](https://github.com/NLP-Tutorials/AACL-IJCNLP2022-KGC-Tutorial)\\] \n\nPromptKG Family: a Gallery of Prompt Learning & KG-related Research Works, Toolkits, and Paper-list [[Resources](https://github.com/zjunlp/PromptKG)\\] \n\nKnowledge Extraction in Low-Resource Scenarios: Survey and Perspective \\[[Survey](https://arxiv.org/abs/2202.08063)\\]\\[[Paper-list](https://github.com/zjunlp/Low-resource-KEPapers)\\]\n\nÂü∫‰∫éÂ§ßÊ®°ÂûãÊèêÁ§∫Â≠¶‰π†ÁöÑÊé®ÁêÜÂ∑•‰ΩúÁªºËø∞ \\[[ËÆ∫Êñá](https://arxiv.org/abs/2212.09597)\\]\\[[ÂàóË°®](https://github.com/zjunlp/Prompt4ReasoningPapers)\\]\\[[ppt](https://github.com/zjunlp/Prompt4ReasoningPapers/blob/main/tutorial.pdf)\\]\n\n# Áõ∏ÂÖ≥Â∑•ÂÖ∑\n\n[Doccano](https://github.com/doccano/doccano)„ÄÅ[MarkTool](https://github.com/FXLP/MarkTool)„ÄÅ[LabelStudio](https://labelstud.io/ )ÔºöÂÆû‰ΩìËØÜÂà´ÂÖ≥Á≥ªÊäΩÂèñÊï∞ÊçÆÊ†áÊ≥®Â∑•ÂÖ∑\n\n[LambdaKG](https://github.com/zjunlp/PromptKG/tree/main/lambdaKG): Âü∫‰∫éÈ¢ÑËÆ≠ÁªÉËØ≠Ë®ÄÊ®°ÂûãÁöÑÁü•ËØÜÂõæË∞±Ë°®Á§∫‰∏éÂ∫îÁî®Â∑•ÂÖ∑\n\n[EasyInstruct](https://github.com/zjunlp/EasyInstruct): ‰∏Ä‰∏™Âü∫‰∫éÊåá‰ª§‰ΩøÁî®Â§ßÊ®°ÂûãÁöÑÂ∑•ÂÖ∑\n\n\n# ÂºïÁî®\n\nÂ¶ÇÊûú‰ΩøÁî®DeepKEÔºåËØ∑Êåâ‰ª•‰∏ãÊ†ºÂºèÂºïÁî®\n\n```bibtex\n@inproceedings{DBLP:conf/emnlp/ZhangXTYYQXCLL22,\n  author    = {Ningyu Zhang and\n               Xin Xu and\n               Liankuan Tao and\n               Haiyang Yu and\n               Hongbin Ye and\n               Shuofei Qiao and\n               Xin Xie and\n               Xiang Chen and\n               Zhoubo Li and\n               Lei Li},\n  editor    = {Wanxiang Che and\n               Ekaterina Shutova},\n  title     = {DeepKE: {A} Deep Learning Based Knowledge Extraction Toolkit for Knowledge\n               Base Population},\n  booktitle = {Proceedings of the The 2022 Conference on Empirical Methods in Natural\n               Language Processing, {EMNLP} 2022 - System Demonstrations, Abu Dhabi,\n               UAE, December 7-11, 2022},\n  pages     = {98--108},\n  publisher = {Association for Computational Linguistics},\n  year      = {2022},\n  url       = {https://aclanthology.org/2022.emnlp-demos.10},\n  timestamp = {Thu, 23 Mar 2023 16:56:00 +0100},\n  biburl    = {https://dblp.org/rec/conf/emnlp/ZhangXTYYQXCLL22.bib},\n  bibsource = {dblp computer science bibliography, https://dblp.org}\n}\n```\n\n<br>\n\n# È°πÁõÆË¥°ÁåÆ‰∫∫Âëò\n\n[Âº†ÂÆÅË±´](https://person.zju.edu.cn/ningyu)„ÄÅ[ÁéãÊòäÂ•ã](https://tjdi.tongji.edu.cn/TeacherDetail.do?id=4991&lang=_cn)„ÄÅÈªÑÈùû„ÄÅÁÜäÈ£ûÂÆá„ÄÅÈô∂ËÅîÂÆΩ„ÄÅÂæêÊ¨£„ÄÅÊ°ÇÈ∏øÊµ©„ÄÅÂº†ÁèçËåπ„ÄÅË∞≠‰º†Â•á„ÄÅÈôàÂº∫„ÄÅÁéãÊΩáÂØí„ÄÅ‰π†Ê≥ΩÂù§„ÄÅÊùéÊ¨£Ëç£„ÄÅ‰ΩôÊµ∑Èò≥„ÄÅÂè∂ÂÆèÂΩ¨„ÄÅ‰πîÁ°ïÊñê„ÄÅÁéãÈπè„ÄÅÊú±Èõ®Áê¶„ÄÅË∞¢Ëæõ„ÄÅÈôàÊÉ≥„ÄÅÈªéÊ¥≤Ê≥¢„ÄÅÊùéÁ£ä„ÄÅÊ¢ÅÂ≠ùËΩ¨„ÄÅÂßö‰∫ëÂøó„ÄÅÈôàÈùô„ÄÅÊú±Èõ®Áê¶„ÄÅÈÇìÊ∑ëÊïè„ÄÅÂº†Êñá„ÄÅÈÉëÂõΩËΩ¥„ÄÅÈôàÂçéÈíß\n\nÂºÄÊ∫êÁ§æÂå∫Ë¥°ÁåÆËÄÖ: [thredreams](https://github.com/thredreams)„ÄÅ[eltociear](https://github.com/eltociear)„ÄÅÂæêÂ≠êÊñá„ÄÅÈªÑÁùø„ÄÅÁøÅÊôìÈæô\n\n\n# ÂÖ∂ÂÆÉÁü•ËØÜÊäΩÂèñÂºÄÊ∫êÂ∑•ÂÖ∑\n\n- [CogIE](https://github.com/jinzhuoran/CogIE)\n- [OpenNRE](https://github.com/thunlp/OpenNRE)\n- [OmniEvent](https://github.com/THU-KEG/OmniEvent)\n- [OpenUE](https://github.com/zjunlp/OpenUE)\n- [OpenIE](https://stanfordnlp.github.io/CoreNLP/openie.html)\n- [RESIN](https://github.com/RESIN-KAIROS/RESIN-pipeline-public)\n- [ZShot](https://github.com/IBM/zshot)\n- [OmniEvent](https://github.com/THU-KEG/OmniEvent)\n"
        },
        {
          "name": "README_CNSCHEMA.md",
          "type": "blob",
          "size": 22.9423828125,
          "content": "<p align=\"center\">\n    <a href=\"https://github.com/zjunlp/deepke\"> <img src=\"pics/logo_cnschema.png\" width=\"400\"/></a>\n\n<p>\n<p align=\"center\">  \n    <a href=\"http://deepke.zjukg.cn\">\n        <img alt=\"Documentation\" src=\"https://img.shields.io/badge/demo-website-blue\">\n    </a>\n    <a href=\"https://pypi.org/project/deepke/#files\">\n        <img alt=\"PyPI\" src=\"https://img.shields.io/pypi/v/deepke\">\n    </a>\n    <a href=\"https://github.com/zjunlp/DeepKE/blob/master/LICENSE\">\n        <img alt=\"GitHub\" src=\"https://img.shields.io/github/license/zjunlp/deepke\">\n    </a>\n    <a href=\"http://zjunlp.github.io/DeepKE\">\n        <img alt=\"Documentation\" src=\"https://img.shields.io/badge/doc-website-red\">\n    </a>\n    <a href=\"https://colab.research.google.com/drive/1vS8YJhJltzw3hpJczPt24O0Azcs3ZpRi?usp=sharing\">\n        <img alt=\"Open In Colab\" src=\"https://colab.research.google.com/assets/colab-badge.svg\">\n    </a>\n</p>\n\n<p align=\"center\">\n    <b> English | <a href=\"https://github.com/zjunlp/DeepKE/blob/main/README_CNSCHEMA_CN.md\">ÁÆÄ‰Ωì‰∏≠Êñá</a> </b>\n</p>\n\n<h1 align=\"center\">\n    <p>Off-the-shelf Special Edition for Chinese Knowledge Extraction Toolkit‚Äî‚ÄîDeepKE-cnSchema</p>\n</h1>\n\nDeepKE is a knowledge extraction toolkit based on PyTorch,  supporting **low-resource**, **document-level** and **multimodal** scenarios for **entity**, **relation** and **attribute** extraction. DeepKE-cnSchema is an off-the-shelf version. Users can download the model to realize entity and relation knowledge extraction directly without training.\n\n---\n\n## Catalogue\n\n| Chapter                                                                              | Description                                                                                |\n| ------------------------------------------------------------------------------------ | ------------------------------------------------------------------------------------------ |\n| [Introduction](#Introduction)                                                           | Introduce the basic principles of DeepKE-cnSchema                                          |\n| [Chinese Model Download](#Chinese-Model-Download)                                       | Provide the download address of DeepKE-cnSchema                                            |\n| [Datasets and Chinese Baseline Performance](#Datasets-and-Chinese-Baseline-Performance) | Report the performance of Chinese models                                     |\n| [Quick Load](#Quick-Load)                                                               | Introduce how to use DeepKE-cnSchema for entity and relation extraction |\n| [User-defined Model](#User-defined-Model)                                               | Provide instructions for training models with customized datasets                           |\n| [FAQ](#FAQ)                                                                             | FAQ                                                                                        |\n| [Citation](#Citation)                                                                   | Technical report of this catalogue                                                         |\n\n## Introduction\n\nDeepKE is a knowledge extraction toolkit supporting **low-resource**, **document-level** and **multimodal** scenarios for *entity*, *relation* and *attribute* extraction. We provide [documents](https://zjunlp.github.io/DeepKE/), [Google Colab tutorials](https://colab.research.google.com/drive/1vS8YJhJltzw3hpJczPt24O0Azcs3ZpRi?usp=sharing), [online demo](http://deepke.zjukg.cn/), and [slides](https://github.com/zjunlp/DeepKE/blob/main/docs/slides/Slides-DeepKE-en.pdf) for beginners.\n\nTo promote efficient Chinese knowledge graph construction, we provide DeepKE-cnSchema, a specific version of DeepKE, containing off-the-shelf models based on [cnSchema](https://github.com/OpenKG-ORG/cnSchema). DeepKE-cnSchema supports multiple tasks such as Chinese entity extraction and relation extraction. It can extract 50 relation types and 28 entity types, including common entity types such as person, location, city, institution, etc and the common relation types such as ancestral home, birthplace, nationality and other types.\n\n## Chinese Model Download\n\nFor entity extraction and relation extraction tasks, we provide models based on `RoBERTa-wwm-ext, Chinese` and `BERT-wwm, Chinese` respectively.\n\n| Model                                               | Task                          |                                     Google Download                                     |                              Baidu Netdisk Download                               |\n| :-------------------------------------------------- | :---------------------------- |:---------------------------------------------------------------------------------------:|:---------------------------------------------------------------------------------:|\n| **`DeepKE(NER), RoBERTa-wwm-ext, Chinese`** | **entity extraction**   | **[PyTorch](https://drive.google.com/drive/folders/1T3xf_MXRaVqLV-ST4VqvKoaQqQgRpp67)** |   **[PytorchÔºàpassword:u022Ôºâ](https://pan.baidu.com/s/1hb9XEbK4x5fIyco4DgZZfg)**   |\n| **`DeepKE(NER), BERT-wwm, Chinese`**        | **entity extraction**   | **[PyTorch](https://drive.google.com/drive/folders/1zA8Ichx9nzU3GD92ptdyR_nmARB_7ovg)** |   **[PytorchÔºàpassword:1g0tÔºâ](https://pan.baidu.com/s/10TWE1VA2S-SJgmOm8szRxw)**   |\n| **`DeepKE(NER), BiLSTM-CRF, Chinese`**      | **entity extraction** | **[PyTorch](https://drive.google.com/drive/folders/1n1tzvl6hZYoUUFFWLfkuhkXPx5JB4XK_)** |   **[PytorchÔºàpassword:my4xÔºâ](https://pan.baidu.com/s/1a9ZFFZVQUxmlbLmbVBaTqQ)**   |\n| **`DeepKE(RE), RoBERTa-wwm-ext, Chinese`**  | **relation extraction** | **[PyTorch](https://drive.google.com/drive/folders/1wb_QIZduKDwrHeri0s5byibsSQrrJTEv)** |   **[PytorchÔºàpassword:78pqÔºâ](https://pan.baidu.com/s/1ozFsxExAQTBRs5NbJW7W5g)**   |\n| **`DeepKE(RE), BERT-wwm, Chinese`**         | **relation extraction** | **[PyTorch](https://drive.google.com/drive/folders/1wb_QIZduKDwrHeri0s5byibsSQrrJTEv)** |   **[PytorchÔºàpassword:6psmÔºâ](https://pan.baidu.com/s/1ngvTwg_ZXaenxhOeadWoCA)**   |\n\n### Instructions\n\nIt is recommended to use Baidu Netdisk download in Chinese Mainland, and Google download for overseas users.\n\nAs for the entity extraction model, take pytoch version `DeepKE(RE), RoBERTa-wwm-ext, Chinese` as an example. After downloading, files of the model are obtained:\n\n```\ncheckpoints_robert\n    |- added_tokens.json          # added tokens\n    |- config.json                # config\n    |- eval_results.txt           # evaluation results\n    |- model_config.json          # model config\n    |- pytorch_model.bin          # model\n    |- special_tokens_map.json    # special tokens map\n    |- tokenizer_config.bin       # tokenizer config\n    |- vocab.txt                  # vocabulary\n```\n\nwhere `config.json` and `vocab.txt` is completely consistent with the original Google `RoBERTa-wwm-ext, Chinese`. PyTorch version contains `pytorch_model. bin`, `config. json`, `vocab. txt` file.\n\nAs for the relation extraction model, take pytoch version `DeepKE(RE), RoBERTa-wwm-ext, Chinese` as an example. The model is pth file after downloading.\n\n**After downloading the model, users can directly [quick-load](#Quick-Load) it to extract entity and relation.**\n\n## Datasets and Chinese Baseline Performance\n\n### Datasets\n\nWe have conduct experiments on Chinese named entity recognition and relation extraction datasets. The experimental results are as follows:\n\n### Named Entity Recognition(NER)\n\nDeepKE leverages[`chinese-bert-wwm`](https://drive.google.com/drive/folders/1OLx5tjEriMyzbv0iv_s9lihtXWIjB6OS)and[`chinese-roberta-wwm-ext`](https://drive.google.com/drive/folders/1T3xf_MXRaVqLV-ST4VqvKoaQqQgRpp67)to train and obtain the DeepKE-cnSchema(NER) model. Hyper-parameters used in the model are predefined. Finally, we can obtain the following results after training:\n\n<table>\n    <tr>\n        <th>Model</th>\n        <th>P</th>\n        <th>R</th>\n        <th>F1</th>\n    </tr>\n    <tr>\n        <td><b>DeepKE(NER), RoBERTa-wwm-ext, Chinese</b></td>\n        <td>0.8028</td>\n        <td>0.8612</td>\n        <td>0.8310</td>\n    </tr>\n    <tr>\n\t<td><b>DeepKE(NER), BERT-wwm, Chinese</b></td>\n\t<td>0.7841</td>\n\t<td>0.8587</td>\n\t<td>0.8197</td>\n    </tr>\n</table>\n\n### Relation Extraction(RE)\n\nDeepKE leverages[`chinese-bert-wwm`](https://drive.google.com/drive/folders/1wb_QIZduKDwrHeri0s5byibsSQrrJTEv)and[`chinese-roberta-wwm-ext`](https://drive.google.com/drive/folders/1wb_QIZduKDwrHeri0s5byibsSQrrJTEv)to train and obtain the DeepKE-cnschema(RE) model. Hyper-parameters used in the model are predefined. Finally, we can obtain the following results after  training:\n\n<table>\n    <tr>\n        <th>Model</th>\n        <th>P</th>\n        <th>R</th>\n        <th>F1</th>\n    </tr>\n  <tr>\n        <td><b>DeepKE(RE), RoBERTa-wwm-ext, Chinese</b></td>\n        <td>0.7890</td>\n        <td>0.7370</td>\n        <td>0.7327</td>\n    </tr>\n  <tr>\n        <td><b>DeepKE(RE), BERT-wwm, Chinese</b></td>\n        <td>0.7861</td>\n        <td>0.7506</td>\n        <td>0.7473</td>\n    </tr>\n</table>\n\n### Support Knowledge Schema Type\n\nDeepKE-cnSchema is an off-the-shelf version that supports the Chinese knowledge graphs construction. [CnSchema](https://github.com/OpenKG-ORG/cnSchema) is developed for Chinese information processing, which uses advanced knowledge graphs, natural language processing and machine learning technologies. It integrates structured text data, supports rapid domain knowledge modeling and open data automatic processing across data sources, domains and languages, and provides schema-level support and services for emerging application markets such as intelligent robots, semantic search and intelligent computing. Currently, the Schema types supported by DeepKE-cnSchema are as follows:\n\n#### Entity Schema\n\n| ID | Entity Type | ID | Entity Type | \n| ------------- | :---------- | ------------- | :---------- | \n| 1   | cns:‰∫∫Áâ© YAS  | 2   | cns:ÂΩ±ËßÜ‰ΩúÂìÅ TOJ | \n| 3   | cns:ÁõÆ NGS        | 4   | cns:ÁîüÁâ© QCV   | \n| 5   | cns:Number OKB   | 6   | cns:Date BQF | \n| 7   | cns:ÂõΩÂÆ∂ CAR       | 8   | cns:ÁΩëÁ´ô ZFM   | \n| 9   | cns:ÁΩëÁªúÂ∞èËØ¥ EMT     | 10  | cns:Âõæ‰π¶‰ΩúÂìÅ UER | \n| 11  | cns:Ê≠åÊõ≤ QEE       | 12  | cns:Âú∞ÁÇπ UFT   | \n| 13  | cns:Ê∞îÂÄô GJS       | 14  | cns:Ë°åÊîøÂå∫ SVA  | \n| 15  | cns:TEXT ANO     | 16  | cns:ÂéÜÂè≤‰∫∫Áâ© KEJ | \n| 17  | cns:Â≠¶Ê†° ZDI       | 18  | cns:‰ºÅ‰∏ö CAT   | \n| 19  | cns:Âá∫ÁâàÁ§æ GCK      | 20  | cns:‰π¶Á±ç FQK   | \n| 21  | cns:Èü≥‰πê‰∏ìËæë BAK     | 22  | cns:ÂüéÂ∏Ç RET   | \n| 23  | cns:ÊôØÁÇπ QZP       | 24  | cns:ÁîµËßÜÁªºËâ∫ QAQ | \n| 25  | cns:Êú∫ÊûÑ ZRE       | 26  | cns:‰ΩúÂìÅ TDZ   | \n| 27  | cns:ËØ≠Ë®Ä CVC       | 28  | cns:Â≠¶Áßë‰∏ì‰∏ö PMN | \n\n#### Relation Schema\n\n| ID | Head Entity Type | Tail Entity Type | Relation | ID | Head Entity Type | Tail Entity Type | Relation |\n| --- |:------ |:-----:| ---- | --- |:------ |:-----:| ---- |\n| 1   | cns:Âú∞ÁÇπ     | cns:‰∫∫Áâ©    | cns:Á•ñÁ±ç   | 2   | cns:‰∫∫Áâ©     | cns:‰∫∫Áâ©    | cns:Áà∂‰∫≤   |\n| 3   | cns:Âú∞ÁÇπ     | cns:‰ºÅ‰∏ö    | cns:ÊÄªÈÉ®Âú∞ÁÇπ | 4   | cns:Âú∞ÁÇπ     | cns:‰∫∫Áâ©    | cns:Âá∫ÁîüÂú∞  |\n| 5   | cns:ÁõÆ      | cns:ÁîüÁâ©    | cns:ÁõÆ    | 6   | cns:Number | cns:Ë°åÊîøÂå∫   | cns:Èù¢ÁßØ   |\n| 7   | cns:Text   | cns:Êú∫ÊûÑ    | cns:ÁÆÄÁß∞   | 8   | cns:Date   | cns:ÂΩ±ËßÜ‰ΩúÂìÅ  | cns:‰∏äÊò†Êó∂Èó¥ |\n| 9   | cns:‰∫∫Áâ©     | cns:‰∫∫Áâ©    | cns:Â¶ªÂ≠ê   | 10  | cns:Èü≥‰πê‰∏ìËæë   | cns:Ê≠åÊõ≤    | cns:ÊâÄÂ±û‰∏ìËæë |\n| 11  | cns:Number | cns:‰ºÅ‰∏ö    | cns:Ê≥®ÂÜåËµÑÊú¨ | 12  | cns:ÂüéÂ∏Ç     | cns:ÂõΩÂÆ∂    | cns:È¶ñÈÉΩ   |\n| 13  | cns:‰∫∫Áâ©     | cns:ÂΩ±ËßÜ‰ΩúÂìÅ  | cns:ÂØºÊºî   | 14  | cns:Text   | cns:ÂéÜÂè≤‰∫∫Áâ©  | cns:Â≠ó    |\n| 15  | cns:Number | cns:‰∫∫Áâ©    | cns:Ë∫´È´ò   | 16  | cns:‰ºÅ‰∏ö     | cns:ÂΩ±ËßÜ‰ΩúÂìÅ  | cns:Âá∫ÂìÅÂÖ¨Âè∏ |\n| 17  | cns:Number | cns:Â≠¶Áßë‰∏ì‰∏ö  | cns:‰øÆ‰∏öÂπ¥Èôê | 18  | cns:Date   | cns:‰∫∫Áâ©    | cns:Âá∫ÁîüÊó•Êúü |\n| 19  | cns:‰∫∫Áâ©     | cns:ÂΩ±ËßÜ‰ΩúÂìÅ  | cns:Âà∂Áâá‰∫∫  | 20  | cns:‰∫∫Áâ©     | cns:‰∫∫Áâ©    | cns:ÊØç‰∫≤   |\n| 21  | cns:‰∫∫Áâ©     | cns:ÂΩ±ËßÜ‰ΩúÂìÅ  | cns:ÁºñËæë   | 22  | cns:ÂõΩÂÆ∂     | cns:‰∫∫Áâ©    | cns:ÂõΩÁ±ç   |\n| 23  | cns:‰∫∫Áâ©     | cns:ÂΩ±ËßÜ‰ΩúÂìÅ  | cns:ÁºñÂâß   | 24  | cns:ÁΩëÁ´ô     | cns:ÁΩëÁ´ôÂ∞èËØ¥  | cns:ËøûËΩΩÁΩëÁªú |\n| 25  | cns:‰∫∫Áâ©     | cns:‰∫∫Áâ©    | cns:‰∏àÂ§´   | 26  | cns:Text   | cns:ÂéÜÂè≤‰∫∫Áâ©  | cns:Êúù‰ª£   |\n| 27  | cns:Text   | cns:‰∫∫Áâ©    | cns:Ê∞ëÊóè   | 28  | cns:Text   | cns:ÂéÜÂè≤‰∫∫Áâ©  | cns:Âè∑   |\n| 29  | cns:Âá∫ÁâàÁ§æ    | cns:‰π¶Á±ç    | cns:Âá∫ÁâàÁ§æ  | 30  | cns:‰∫∫Áâ©     | cns:ÁîµËßÜÁªºËâ∫  | cns:‰∏ªÊåÅ‰∫∫  |\n| 31  | cns:Text   | cns:Â≠¶Áßë‰∏ì‰∏ö  | cns:‰∏ì‰∏ö‰ª£Á†Å | 32  | cns:‰∫∫Áâ©     | cns:Ê≠åÊõ≤    | cns:Ê≠åÊâã   |\n| 33  | cns:‰∫∫Áâ©     | cns:Ê≠åÊõ≤    | cns:‰ΩúÊõ≤   | 34  | cns:‰∫∫Áâ©     | cns:ÁΩëÁªúÂ∞èËØ¥  | cns:‰∏ªËßí   |\n| 35  | cns:‰∫∫Áâ©     | cns:‰ºÅ‰∏ö    | cns:Ëë£‰∫ãÈïø  | 36  | cns:Date   | cns:‰ºÅ‰∏ö    | cns:ÊàêÁ´ãÊó∂Èó¥ |\n| 37  | cns:Â≠¶Ê†°     | cns:‰∫∫Áâ©    | cns:ÊØï‰∏öÈô¢Ê†° | 38  | cns:Number | cns:Êú∫ÊûÑ    | cns:Âç†Âú∞Èù¢ÁßØ |\n| 39  | cns:ËØ≠Ë®Ä     | cns:ÂõΩÂÆ∂    | cns:ÂÆòÊñπËØ≠Ë®Ä | 40  | cns:Text   | cns:Ë°åÊîøÂå∫   | cns:‰∫∫Âè£Êï∞Èáè |\n| 41  | cns:Number | cns:Ë°åÊîøÂå∫   | cns:‰∫∫Âè£Êï∞Èáè | 42  | cns:ÂüéÂ∏Ç     | cns:ÊôØÁÇπ    | cns:ÊâÄÂú®ÂüéÂ∏Ç |\n| 43  | cns:‰∫∫Áâ©     | cns:Âõæ‰π¶‰ΩúÂìÅ  | cns:‰ΩúËÄÖ   | 44  | None   | None    | ÂÖ∂‰ªñ |\n| 45  | cns:‰∫∫Áâ©     | cns:Ê≠åÊõ≤    | cns:‰ΩúÊõ≤   | 46  | cns:‰∫∫Áâ©     | cns:Ë°åÊîøÂå∫   | cns:Ê∞îÂÄô   |\n| 47  | cns:‰∫∫Áâ©     | cns:ÁîµËßÜÁªºËâ∫  | cns:ÂòâÂÆæ   | 48  | cns:‰∫∫Áâ©     | cns:ÂΩ±ËßÜ‰ΩúÂìÅ  | cns:‰∏ªÊºî   |\n| 49  | cns:‰ΩúÂìÅ     | cns:ÂΩ±ËßÜ‰ΩúÂìÅ  | cns:ÊîπÁºñËá™  | 50  | cns:‰∫∫Áâ©     | cns:‰ºÅ‰∏ö    | cns:ÂàõÂßã‰∫∫  |\n\n\n## Quick Load\n\n### [Named Entity Recognition(NER)](https://github.com/zjunlp/DeepKE/tree/main/example/ner/standard)\n\nUsers can directly download the [model](https://drive.google.com/drive/folders/1zA8Ichx9nzU3GD92ptdyR_nmARB_7ovg) for usage. The details are as followsÔºö\n\n1. Enter the directory `DeepKE/example/ner/standard`\n2. Create the downloaded folder as `checkpoints` and store it in the directory `DeepKE/example/ner/standard`\n3. Set the parameter `text` in `conf/predict.yaml` as the sentence to be predicted, and modify `hydra/model` in `conf/config.yaml` to `bert` or `lstmcrf` according to the category of the downloaded model (the model downloaded from the above link is the `bert` model)\n\t\n \t> To use the trained model, just set the input sentence \"„ÄäÊòüÁ©∫ÈªëÂ§ú‰º†Â•á„ÄãÊòØËøûËΩΩ‰∫éËµ∑ÁÇπ‰∏≠ÊñáÁΩëÁöÑÁΩëÁªúÂ∞èËØ¥Ôºå‰ΩúËÄÖÊòØÂï§ÈÖíÁöÑÁΩ™Â≠Ω\". After running `python oredict.py`, results can be obtained which show that the entity type \"ÊòüÁ©∫ÈªëÂ§ú‰º†Â•á\" is \"ÁΩëÁªúÂ∞èËØ¥\", \"Ëµ∑ÁÇπ‰∏≠ÊñáÁΩë\" is \"ÁΩëÁ´ô\" and \"Âï§ÈÖíÁöÑÁΩ™Â≠Ω\" is \"‰∫∫Áâ©\".\n  \t> ```\n\t>\ttext=\"„ÄäÊòüÁ©∫ÈªëÂ§ú‰º†Â•á„ÄãÊòØËøûËΩΩ‰∫éËµ∑ÁÇπ‰∏≠ÊñáÁΩëÁöÑÁΩëÁªúÂ∞èËØ¥Ôºå‰ΩúËÄÖÊòØÂï§ÈÖíÁöÑÁΩ™Â≠Ω\"\n\t>\t```\t\n\n\t\t\n4. Predict\n\t```bash\n\tpython predict.py\n\t```\n\n\n\tFinally, output the results:\n\n\t```bash\n\tNERÂè•Â≠êÔºö\n\t„ÄäÊòüÁ©∫ÈªëÂ§ú‰º†Â•á„ÄãÊòØËøûËΩΩ‰∫éËµ∑ÁÇπ‰∏≠ÊñáÁΩëÁöÑÁΩëÁªúÂ∞èËØ¥Ôºå‰ΩúËÄÖÊòØÂï§ÈÖíÁöÑÁΩ™Â≠Ω\n\tNERÁªìÊûúÔºö\n\t[('Êòü','B-UER'),('Á©∫','I-UER'),('Èªë','I-UER'),('Â§ú','I-UER'),('‰º†','I-UER'),('Â•á','I-UER'),('Ëµ∑','B-ZFM'),('ÁÇπ','I-ZFM'),('‰∏≠','I-ZFM'),('Êñá','I-ZFM'),('ÁΩë','I-ZFM'),('Âï§','B-YAS'),('ÈÖí','I-YAS'),('ÁöÑ','I-YAS'),('ÁΩ™','I-YAS'),('Â≠Ω','I-YAS')]\n\t```\n\n### [Relation Extraction(RE)](https://github.com/zjunlp/DeepKE/tree/main/example/re/standard)\nUsers can directly download the [model](https://drive.google.com/drive/folders/1wb_QIZduKDwrHeri0s5byibsSQrrJTEv) for usage. The details are as followsÔºö\n\n1. Enter the directory `DeepKE/example/re/standard`\n2. Modify the parameter `fp`in `conf/config.yaml`to the path of downloaded file, `num_relations`in `conf/embedding.yaml`to 51(relation nums) and `model` in `conf/config.yaml`to `lm`.\n3. Download the [dataset](https://drive.google.com/drive/folders/1UurqpjePe3zhXxbDDNwLAjVvt7UyUMuQ) , and put files in the directory `example/re/standard/data/origin`.\n4. Predict. The text and entity pairs to be predicted are fed to the program through the terminal.\n\n\t```bash\n\tpython predict.py\n\t```\n\n\tTo use the trained model, run `python predict.py` and input the sentence \"Ê≠åÊõ≤„Ää‰∫∫ÁîüÈïøË∑Ø„ÄãÂá∫Ëá™ÂàòÂæ∑ÂçéÂõΩËØ≠‰∏ìËæë„ÄäÁî∑‰∫∫ÁöÑÁà±„ÄãÔºåÁî±ÊùéÊ≥â‰ΩúËØç‰ΩúÊõ≤Ôºå2001Âπ¥Âá∫Ë°åÂèëÁâà\". The given entity pair are \"Áî∑‰∫∫ÁöÑÁà±\" and \"‰∫∫ÁîüÈïøË∑Ø\". Finally, the extracted relation is \"ÊâÄÂ±û‰∏ìËæë\".\n\n\tTo change the text to be predicted, modify the `_get_predict_instance`function in `predict.py` to the following example:\n\n\t```python\n\tdef _get_predict_instance(cfg):\n\t    flag = input('ÊòØÂê¶‰ΩøÁî®ËåÉ‰æã[y/n]ÔºåÈÄÄÂá∫ËØ∑ËæìÂÖ•: exit .... ')\n\t    flag = flag.strip().lower()\n\t    if flag == 'y' or flag == 'yes':\n\t\tsentence = 'Ê≠åÊõ≤„Ää‰∫∫ÁîüÈïøË∑Ø„ÄãÂá∫Ëá™ÂàòÂæ∑ÂçéÂõΩËØ≠‰∏ìËæë„ÄäÁî∑‰∫∫ÁöÑÁà±„ÄãÔºåÁî±ÊùéÊ≥â‰ΩúËØç‰ΩúÊõ≤Ôºå2001Âπ¥Âá∫Ë°åÂèëÁâà'\n\t\thead = 'Áî∑‰∫∫ÁöÑÁà±'\n\t\ttail = '‰∫∫ÁîüÈïøË∑Ø'\n\t\thead_type = 'ÊâÄÂ±û‰∏ìËæë'\n\t\ttail_type = 'Ê≠åÊõ≤'\n\t    elif flag == 'n' or flag == 'no':\n\t\tsentence = input('ËØ∑ËæìÂÖ•Âè•Â≠êÔºö')\n\t\thead = input('ËØ∑ËæìÂÖ•Âè•‰∏≠ÈúÄË¶ÅÈ¢ÑÊµãÂÖ≥Á≥ªÁöÑÂ§¥ÂÆû‰ΩìÔºö')\n\t\thead_type = input('ËØ∑ËæìÂÖ•Â§¥ÂÆû‰ΩìÁ±ªÂûãÔºàÂèØ‰ª•‰∏∫Á©∫ÔºåÊåâenterË∑≥ËøáÔºâÔºö')\n\t\ttail = input('ËØ∑ËæìÂÖ•Âè•‰∏≠ÈúÄË¶ÅÈ¢ÑÊµãÂÖ≥Á≥ªÁöÑÂ∞æÂÆû‰ΩìÔºö')\n\t\ttail_type = input('ËØ∑ËæìÂÖ•Â∞æÂÆû‰ΩìÁ±ªÂûãÔºàÂèØ‰ª•‰∏∫Á©∫ÔºåÊåâenterË∑≥ËøáÔºâÔºö')\n\t    elif flag == 'exit':\n\t\tsys.exit(0)\n\t    else:\n\t\tprint('please input yes or no, or exit!')\n\t\t_get_predict_instance()\n\n\t    instance = dict()\n\t    instance['sentence'] = sentence.strip()\n\t    instance['head'] = head.strip()\n\t    instance['tail'] = tail.strip()\n\t    if head_type.strip() == '' or tail_type.strip() == '':\n\t\tcfg.replace_entity_with_type = False\n\t\tinstance['head_type'] = 'None'\n\t\tinstance['tail_type'] = 'None'\n\t    else:\n\t\tinstance['head_type'] = head_type.strip()\n\t\tinstance['tail_type'] = tail_type.strip()\n\n\t    return instance\n\t```\n\n\tFinally, output the results:\n\n\t```bash\n\t‚ÄúÁî∑‰∫∫ÁöÑÁà±‚ÄùÂíå‚Äú‰∫∫ÁîüÈïøË∑Ø‚ÄùÂú®Âè•‰∏≠ÂÖ≥Á≥ª‰∏∫‚ÄúÊâÄÂ±û‰∏ìËæë‚ÄùÔºåÁΩÆ‰ø°Â∫¶‰∏∫0.99\n\t```\n \t> Note: The model specified in `example/re/standard/conf/model/lm.yaml` will be automatically downloaded from the huggingface website during runtime. If the download fails, please use the huggingface mirror site or download it manually.\n\n### [Joint Entity and Relation Extraction](https://github.com/zjunlp/DeepKE/tree/main/example/triple)\nAfter aforementioned trained models are downloaded, entites and their relations in a text can be extracted together. If there are more than two entities in one sentence, some predicted entity pairs may be incorrect because these entity pairs are not in training sets and need to be exracted further. The detailed steps are as follows:<br>\n1. In `conf`, modify `text` in `predict.yaml` as the sentence to be predicted, `nerfp` as the directory of the trained NER model and `refp` as the directory of the trained RE model.\n2. Predict\n\t```shell\n\tpython predict.py\n\t```\n\tMany results will be output. Take the input text `Ê≠§Â§ñÁΩëÊòì‰∫ëÂπ≥Âè∞Ëøò‰∏äÊû∂‰∫Ü‰∏ÄÁ≥ªÂàóÊ≠åÊõ≤ÔºåÂÖ∂‰∏≠ÂåÖÊã¨Áî∞È¶•ÁîÑÁöÑ„ÄäÂ∞èÂπ∏Ëøê„ÄãÁ≠â` as example.\n\t\n\t(1) Output the result of NER: `[('Áî∞', 'B-YAS'), ('È¶•', 'I-YAS'), ('ÁîÑ', 'I-YAS'), ('Â∞è', 'B-QEE'), ('Âπ∏', 'I-QEE'), ('Ëøê', 'I-QEE')]`\n\t\n\t(2) Output the processed result: `{'Áî∞È¶•ÁîÑ': '‰∫∫Áâ©', 'Â∞èÂπ∏Ëøê': 'Ê≠åÊõ≤'}`\n\t\n\t(3) Output the result of RE: `\"Áî∞È¶•ÁîÑ\" Âíå \"Â∞èÂπ∏Ëøê\" Âú®Âè•‰∏≠ÂÖ≥Á≥ª‰∏∫Ôºö\"Ê≠åÊâã\"ÔºåÁΩÆ‰ø°Â∫¶‰∏∫0.92„ÄÇ`\n\t\n\t(4) Output the result as `jsonld`\n\t\n\t ```bash\n\t    {\n\t      \"@context\": {\n\t\t\"Ê≠åÊâã\": \"https://cnschema.openkg.cn/item/%E6%AD%8C%E6%89%8B/16693#viewPageContent\"\n\t      },\n\t      \"@id\": \"Áî∞È¶•ÁîÑ\",\n\t      \"Ê≠åÊâã\": {\n\t\t\"@id\": \"Â∞èÂπ∏Ëøê\"\n\t      }\n\t    }\n\t  ```\n\n## Custom Models (Advanced Usage)\n\n### Named Entity Recognition (NER)\n\nIf you need to use customized dataset for training, follow the steps bellow:\n\n1. Download customized [dataset](https://drive.google.com/drive/folders/1zA8Ichx9nzU3GD92ptdyR_nmARB_7ovg) and put it into the `data` folder.\n2. Modify the parameter `bert_model`in `train.yaml`of the `conf`folder to the specify model. Users can choose different models to train by modifying the `yaml`file.\n3. Modify `labels` in `train.yaml` as the labels in `data/type.txt`\n4. Train.\n\t```bash\n\tpython run.py\n\t```\n\n### Relation Extraction (RE)\n\nIf you need to use other models for training, follow the steps bellow:\n\n1„ÄÅDownload the customized [dataset](https://drive.google.com/drive/folders/1wb_QIZduKDwrHeri0s5byibsSQrrJTEv) and rename it to `data`.\n\n2„ÄÅModify the parameter `model_name`in `train.yaml`of the `conf`folder to `lm`, `num_relations`in `embedding_yaml`to the number of relations(eg: 51). Users can choose different models to train by modifying the `yaml`file. \n\n3„ÄÅTrain.\n\n\t```bash\n\tpython run.py\n\t```\n\n## FAQ\n\n**Q: How to use this model?**\nA: It is off-the-shelf. After downloading the model, follow the instructions and you can extract the knowledge contained in the predefined cnSchema.\n**If you want to extract knowledge other than cnSchema, you can use the advanced version of customized data for training**\n\n**Q: Is there any other cnSchema extraction model available?**\nA: Unfortunately, we can only support part of knowledge extraction of cnSchema for the time being. More knowledge extraction models will be published in the future.\n\n**Q: I trained better result than you!**\nA: Congratulations!\n\n**Q: Embedding error for customized dataset.**\nA: The Chinese data may contain invisible special characters, which cannot be encoded and thus an error is reported. You can preprocess the Chinese data through the editor or other tools to solve this problem.\n\n## Citation\n\nIf the resources or technologies in this project are helpful to your research work, you are welcome to cite the following papers in your thesis:\n\n```\n@inproceedings{DBLP:conf/emnlp/ZhangXTYYQXCLL22,\n  author    = {Ningyu Zhang and\n               Xin Xu and\n               Liankuan Tao and\n               Haiyang Yu and\n               Hongbin Ye and\n               Shuofei Qiao and\n               Xin Xie and\n               Xiang Chen and\n               Zhoubo Li and\n               Lei Li},\n  editor    = {Wanxiang Che and\n               Ekaterina Shutova},\n  title     = {DeepKE: {A} Deep Learning Based Knowledge Extraction Toolkit for Knowledge\n               Base Population},\n  booktitle = {Proceedings of the The 2022 Conference on Empirical Methods in Natural\n               Language Processing, {EMNLP} 2022 - System Demonstrations, Abu Dhabi,\n               UAE, December 7-11, 2022},\n  pages     = {98--108},\n  publisher = {Association for Computational Linguistics},\n  year      = {2022},\n  url       = {https://aclanthology.org/2022.emnlp-demos.10},\n  timestamp = {Thu, 23 Mar 2023 16:56:00 +0100},\n  biburl    = {https://dblp.org/rec/conf/emnlp/ZhangXTYYQXCLL22.bib},\n  bibsource = {dblp computer science bibliography, https://dblp.org}\n}\n```\n\n## Disclaimers\n\n**The contents of this project are only for technical research reference and shall not be used as any conclusive basis. Users can freely use the model within the scope of the license, but we are not responsible for the direct or indirect losses caused by the use of the project.**\n\n## Problem Feedback\n\nIf you have any questions, please submit them in GitHub issue.\n\n"
        },
        {
          "name": "README_CNSCHEMA_CN.md",
          "type": "blob",
          "size": 21.748046875,
          "content": "<p align=\"center\">\n    <a href=\"https://github.com/zjunlp/deepke\"> <img src=\"pics/logo_cnschema.png\" width=\"400\"/></a>\n<p>\n<p align=\"center\">  \n    <a href=\"http://deepke.zjukg.cn\">\n        <img alt=\"Documentation\" src=\"https://img.shields.io/badge/demo-website-blue\">\n    </a>\n    <a href=\"https://pypi.org/project/deepke/#files\">\n        <img alt=\"PyPI\" src=\"https://img.shields.io/pypi/v/deepke\">\n    </a>\n    <a href=\"https://github.com/zjunlp/DeepKE/blob/master/LICENSE\">\n        <img alt=\"GitHub\" src=\"https://img.shields.io/github/license/zjunlp/deepke\">\n    </a>\n    <a href=\"http://zjunlp.github.io/DeepKE\">\n        <img alt=\"Documentation\" src=\"https://img.shields.io/badge/doc-website-red\">\n    </a>\n    <a href=\"https://colab.research.google.com/drive/1vS8YJhJltzw3hpJczPt24O0Azcs3ZpRi?usp=sharing\">\n        <img alt=\"Open In Colab\" src=\"https://colab.research.google.com/assets/colab-badge.svg\">\n    </a>\n</p>\n\n<p align=\"center\">\n    <b> <a href=\"https://github.com/zjunlp/DeepKE/blob/main/README_CNSCHEMA.md\">English</a> | ÁÆÄ‰Ωì‰∏≠Êñá </b>\n</p>\n\n<h1 align=\"center\">\n    <p>ÂºÄÊ∫ê‰∏≠ÊñáÁü•ËØÜÂõæË∞±ÊäΩÂèñÊ°ÜÊû∂ÂºÄÁÆ±Âç≥Áî®ÁâπÂà´ÁâàDeepKE-cnSchema</p>\n</h1>\n\nDeepKE ÊòØ‰∏Ä‰∏™ÂºÄÊ∫êÁöÑÁü•ËØÜÂõæË∞±ÊäΩÂèñ‰∏éÊûÑÂª∫Â∑•ÂÖ∑ÔºåÊîØÊåÅ<b>‰ΩéËµÑÊ∫ê„ÄÅÈïøÁØáÁ´†„ÄÅÂ§öÊ®°ÊÄÅ</b>ÁöÑÁü•ËØÜÊäΩÂèñÂ∑•ÂÖ∑ÔºåÂèØ‰ª•Âü∫‰∫é<b>PyTorch</b>ÂÆûÁé∞<b>ÂëΩÂêçÂÆû‰ΩìËØÜÂà´</b>„ÄÅ<b>ÂÖ≥Á≥ªÊäΩÂèñ</b>Âíå<b>Â±ûÊÄßÊäΩÂèñ</b>ÂäüËÉΩ„ÄÇÊ≠§ÁâàÊú¨DeepKE-cnSchema‰∏∫ÂºÄÁÆ±Âç≥Áî®ÁâàÊú¨ÔºåÁî®Êà∑‰∏ãËΩΩÊ®°ÂûãÂç≥ÂèØÂÆûÁé∞ÊîØÊåÅcnSchemaÁöÑÂÆû‰ΩìÂíåÂÖ≥Á≥ªÁü•ËØÜÊäΩÂèñ„ÄÇ\n\n---\n\n## ÂÜÖÂÆπÂØºÂºï\n\n| Á´†ËäÇ                          | ÊèèËø∞                                |\n| --------------------------- | --------------------------------- |\n| [ÁÆÄ‰ªã](#ÁÆÄ‰ªã)                   | ‰ªãÁªçDeepKE-cnSchemaÂü∫Êú¨ÂéüÁêÜ             |\n| [‰∏≠ÊñáÊ®°Âûã‰∏ãËΩΩ](#‰∏≠ÊñáÊ®°Âûã‰∏ãËΩΩ)           | Êèê‰æõ‰∫ÜDeepKE-cnSchemaÁöÑ‰∏ãËΩΩÂú∞ÂùÄ           |\n| [Êï∞ÊçÆÈõÜÂèä‰∏≠ÊñáÊ®°ÂûãÊïàÊûú](#Êï∞ÊçÆÈõÜÂèä‰∏≠ÊñáÂü∫Á∫øÁ≥ªÁªüÊïàÊûú) | Êèê‰æõ‰∫Ü‰∏≠ÊñáÊï∞ÊçÆÈõÜ‰ª•Âèä‰∏≠ÊñáÊ®°ÂûãÊïàÊûú                  |\n| [Âø´ÈÄüÂä†ËΩΩ](#Âø´ÈÄüÂä†ËΩΩ)               | ‰ªãÁªç‰∫ÜÂ¶Ç‰Ωï‰ΩøÁî®DeepKE-cnSchemaËøõË°åÂÆû‰ΩìËØÜÂà´„ÄÅÂÖ≥Á≥ªÊäΩÂèñ |\n| [Ëá™ÂÆö‰πâÊ®°Âûã](#Ëá™ÂÆö‰πâÊ®°Âûã)             | Êèê‰æõ‰∫Ü‰ΩøÁî®Ëá™ÂÆö‰πâÊï∞ÊçÆËÆ≠ÁªÉÊ®°ÂûãÁöÑËØ¥Êòé                 |\n| [FAQ](#FAQ)                 | Â∏∏ËßÅÈóÆÈ¢òÁ≠îÁñë                            |\n| [ÂºïÁî®](#ÂºïÁî®)                   | Êú¨ÁõÆÂΩïÁöÑÊäÄÊúØÊä•Âëä                          |\n\n## ÁÆÄ‰ªã\n\nDeepKE ÊòØ‰∏Ä‰∏™ÂºÄÊ∫êÁöÑÁü•ËØÜÂõæË∞±ÊäΩÂèñ‰∏éÊûÑÂª∫Â∑•ÂÖ∑ÔºåÊîØÊåÅ‰ΩéËµÑÊ∫ê„ÄÅÈïøÁØáÁ´†„ÄÅÂ§öÊ®°ÊÄÅÁöÑÁü•ËØÜÊäΩÂèñÂ∑•ÂÖ∑ÔºåÂèØ‰ª•Âü∫‰∫éPyTorchÂÆûÁé∞ÂëΩÂêçÂÆû‰ΩìËØÜÂà´„ÄÅÂÖ≥Á≥ªÊäΩÂèñÂíåÂ±ûÊÄßÊäΩÂèñÂäüËÉΩ„ÄÇÂêåÊó∂‰∏∫ÂàùÂ≠¶ËÄÖÊèê‰æõ‰∫ÜËØ¶Â∞ΩÁöÑ[ÊñáÊ°£](https://zjunlp.github.io/DeepKE/)Ôºå[Google ColabÊïôÁ®ã](https://colab.research.google.com/drive/1vS8YJhJltzw3hpJczPt24O0Azcs3ZpRi?usp=sharing)Ôºå[Âú®Á∫øÊºîÁ§∫](http://deepke.zjukg.cn/)Âíå[ÂπªÁÅØÁâá](https://github.com/zjunlp/DeepKE/blob/main/docs/slides/Slides-DeepKE-cn.pdf)„ÄÇ\n\n‰∏∫‰øÉËøõ‰∏≠ÊñáÈ¢ÜÂüüÁöÑÁü•ËØÜÂõæË∞±ÊûÑÂª∫ÂíåÊñπ‰æøÁî®Êà∑‰ΩøÁî®ÔºåDeepKEÊèê‰æõ‰∫ÜÈ¢ÑËÆ≠ÁªÉÂ•ΩÁöÑÊîØÊåÅ[cnSchema](https://github.com/OpenKG-ORG/cnSchema)ÁöÑÁâπÂà´ÁâàDeepKE-cnSchemaÔºåÊîØÊåÅÂºÄÁÆ±Âç≥Áî®ÁöÑ‰∏≠ÊñáÂÆû‰ΩìÊäΩÂèñÂíåÂÖ≥Á≥ªÊäΩÂèñÁ≠â‰ªªÂä°ÔºåÂèØÊäΩÂèñ50ÁßçÂÖ≥Á≥ªÁ±ªÂûãÂíå28ÁßçÂÆû‰ΩìÁ±ªÂûãÔºåÂÖ∂‰∏≠ÂÆû‰ΩìÁ±ªÂûãÂåÖÂê´‰∫ÜÈÄöÁî®ÁöÑ‰∫∫Áâ©„ÄÅÂú∞ÁÇπ„ÄÅÂüéÂ∏Ç„ÄÅÊú∫ÊûÑÁ≠âÁ±ªÂûãÔºåÂÖ≥Á≥ªÁ±ªÂûãÂåÖÊã¨‰∫ÜÂ∏∏ËßÅÁöÑÁ•ñÁ±ç„ÄÅÂá∫ÁîüÂú∞„ÄÅÂõΩÁ±ç„ÄÅÊúù‰ª£Á≠âÁ±ªÂûã„ÄÇ\n\n## ‰∏≠ÊñáÊ®°Âûã‰∏ãËΩΩ\n\nÂØπ‰∫éÂÆû‰ΩìÊäΩÂèñÂíåÂÖ≥Á≥ªÊäΩÂèñ‰ªªÂä°ÂàÜÂà´Êèê‰æõ‰∫ÜÂü∫‰∫é`RoBERTa-wwm-ext, Chinese`Âíå`BERT-wwm, Chinese`ËÆ≠ÁªÉÁöÑÊ®°Âûã„ÄÇ\n\n| Ê®°ÂûãÁÆÄÁß∞                                        | ÂäüËÉΩ                     |                                        Google‰∏ãËΩΩ                                         |                                   ÁôæÂ∫¶ÁΩëÁõò‰∏ãËΩΩ                                   |\n|:------------------------------------------- |:---------------------- |:---------------------------------------------------------------------------------------:|:--------------------------------------------------------------------------:|\n| **`DeepKE(NER), RoBERTa-wwm-ext, Chinese`** | **ÂÆû‰ΩìÊäΩÂèñ** | **[PyTorch](https://drive.google.com/drive/folders/1T3xf_MXRaVqLV-ST4VqvKoaQqQgRpp67)** |   **[PytorchÔºàÂØÜÁ†Åu022Ôºâ](https://pan.baidu.com/s/1hb9XEbK4x5fIyco4DgZZfg)**   |\n| **`DeepKE(NER), BERT-wwm, Chinese`**        | **ÂÆû‰ΩìÊäΩÂèñ** | **[PyTorch](https://drive.google.com/drive/folders/1zA8Ichx9nzU3GD92ptdyR_nmARB_7ovg)** |   **[PytorchÔºàÂØÜÁ†Å1g0tÔºâ](https://pan.baidu.com/s/10TWE1VA2S-SJgmOm8szRxw)**   |\n| **`DeepKE(NER), BiLSTM-CRF, Chinese`**      | **ÂÆû‰ΩìÊäΩÂèñ** | **[PyTorch](https://drive.google.com/drive/folders/1n1tzvl6hZYoUUFFWLfkuhkXPx5JB4XK_)** |   **[PytorchÔºàÂØÜÁ†Åmy4xÔºâ](https://pan.baidu.com/s/1a9ZFFZVQUxmlbLmbVBaTqQ)**   |\n| **`DeepKE(RE), RoBERTa-wwm-ext, Chinese`**  | **ÂÖ≥Á≥ªÊäΩÂèñ** | **[PyTorch](https://drive.google.com/drive/folders/1wb_QIZduKDwrHeri0s5byibsSQrrJTEv)** |   **[PytorchÔºàÂØÜÁ†Å78pqÔºâ](https://pan.baidu.com/s/1ozFsxExAQTBRs5NbJW7W5g)**   |\n| **`DeepKE(RE), BERT-wwm, Chinese`**         | **ÂÖ≥Á≥ªÊäΩÂèñ** | **[PyTorch](https://drive.google.com/drive/folders/1wb_QIZduKDwrHeri0s5byibsSQrrJTEv)** |   **[PytorchÔºàÂØÜÁ†Å6psmÔºâ](https://pan.baidu.com/s/1ngvTwg_ZXaenxhOeadWoCA)**   |\n\n### ‰ΩøÁî®ËØ¥Êòé\n\n‰∏≠ÂõΩÂ§ßÈôÜÂ¢ÉÂÜÖÂª∫ËÆÆ‰ΩøÁî®ÁôæÂ∫¶ÁΩëÁõò‰∏ãËΩΩÁÇπÔºåÂ¢ÉÂ§ñÁî®Êà∑Âª∫ËÆÆ‰ΩøÁî®Ë∞∑Ê≠å‰∏ãËΩΩÁÇπ„ÄÇ\nÂÆû‰ΩìÊäΩÂèñÊ®°Âûã‰∏≠Ôºå‰ª•PytorchÁâà`DeepKE(RE), RoBERTa-wwm-ext, Chinese`‰∏∫‰æãÔºå‰∏ãËΩΩÂÆåÊØïÂêéÂæóÂà∞Ê®°ÂûãÊñá‰ª∂Ôºö\n\n```\ncheckpoints_robert\n    |- added_tokens.json          # È¢ùÂ§ñÂ¢ûÂä†ËØçË°®\n    |- config.json                # Êï¥‰ΩìÂèÇÊï∞\n    |- eval_results.txt           # È™åËØÅÁªìÊûú\n    |- model_config.json          # Ê®°ÂûãÂèÇÊï∞\n    |- pytorch_model.bin          # Ê®°Âûã\n    |- special_tokens_map.json    # ÁâπÊÆäËØçË°®Êò†Â∞Ñ\n    |- tokenizer_config.bin       # ÂàÜËØçÂô®ÂèÇÊï∞\n    |- vocab.txt                  # ËØçË°®\n```\n\nÂÖ∂‰∏≠`config.json`Âíå`vocab.txt`‰∏éË∞∑Ê≠åÂéüÁâà`RoBERTa-wwm-ext, Chinese`ÂÆåÂÖ®‰∏ÄËá¥„ÄÇ\nPyTorchÁâàÊú¨ÂàôÂåÖÂê´`pytorch_model.bin`, `config.json`, `vocab.txt`Êñá‰ª∂„ÄÇ\n\nÂÖ≥Á≥ªÊäΩÂèñÊ®°Âûã‰∏≠Ôºå‰ª•PytorchÁâà`DeepKE(RE), RoBERTa-wwm-ext, Chinese`‰∏∫‰æãÔºå‰∏ãËΩΩÂêé‰∏∫pthÊñá‰ª∂„ÄÇ\n\n**‰∏ãËΩΩÊ®°ÂûãÂêéÔºåÁî®Êà∑Âç≥ÂèØÁõ¥Êé•[Âø´ÈÄüÂä†ËΩΩ](#Âø´ÈÄüÂä†ËΩΩ)Ê®°ÂûãËøõË°åÂÆû‰ΩìÂÖ≥Á≥ªÊäΩÂèñ„ÄÇ**\n\n## Êï∞ÊçÆÈõÜÂèä‰∏≠ÊñáÂü∫Á∫øÁ≥ªÁªüÊïàÊûú\n\n### Êï∞ÊçÆÈõÜ\n\nÊàë‰ª¨Âú®‰∏≠ÊñáÂÆû‰ΩìËØÜÂà´ÂíåÂÖ≥Á≥ªÊäΩÂèñÊï∞ÊçÆÈõÜ‰∏äËøõË°å‰∫ÜÂÆûÈ™åÔºåÂÆûÈ™åÁªìÊûúÂ¶Ç‰∏ã\n\n### ÂÆû‰ΩìËØÜÂà´ÔºàNERÔºâ\n\nDeepKE‰ΩøÁî®[`chinese-bert-wwm`](https://drive.google.com/drive/folders/1OLx5tjEriMyzbv0iv_s9lihtXWIjB6OS)Âíå[`chinese-roberta-wwm-ext`](https://drive.google.com/drive/folders/1T3xf_MXRaVqLV-ST4VqvKoaQqQgRpp67)‰∏∫Âü∫Á°ÄËÆ≠ÁªÉÂæóÂà∞‰∫ÜDeepKE-cnSchema(NER)Ê®°Âûã„ÄÇÊ®°ÂûãÊâÄ‰ΩøÁî®ÁöÑË∂ÖÂèÇÊï∞Âùá‰∏∫È¢ÑÂÆö‰πâÁöÑÂèÇÊï∞„ÄÇÊúÄÁªàÁªèËøáËÆ≠ÁªÉÂêéÂèØ‰ª•ÂæóÂà∞Â¶Ç‰∏ãË°®ÁöÑÊïàÊûú\n\n<table>\n    <tr>\n        <th>Ê®°Âûã</th>\n        <th>P</th>\n        <th>R</th>\n        <th>F1</th>\n    </tr>\n    <tr>\n        <td><b>DeepKE(NER), RoBERTa-wwm-ext, Chinese</b></td>\n        <td>0.8028</td>\n        <td>0.8612</td>\n        <td>0.8310</td>\n    </tr>\n\n<tr>\n        <td><b>DeepKE(NER), BERT-wwm, Chinese</b></td>\n        <td>0.7841</td>\n        <td>0.8587</td>\n        <td>0.8197</td>\n    </tr>\n\n</table>\n\n### ÂÖ≥Á≥ªÊäΩÂèñÔºàREÔºâ\n\nDeepKE‰ΩøÁî®[`chinese-bert-wwm`](https://drive.google.com/drive/folders/1wb_QIZduKDwrHeri0s5byibsSQrrJTEv)Âíå[`chinese-roberta-wwm-ext`](https://drive.google.com/drive/folders/1wb_QIZduKDwrHeri0s5byibsSQrrJTEv)‰∏∫Âü∫Á°ÄÂæóÂà∞‰∫ÜDeepKE-cnschema(RE)Ê®°Âûã„ÄÇÊ®°ÂûãÊâÄ‰ΩøÁî®ÁöÑË∂ÖÂèÇÊï∞Âùá‰∏∫È¢ÑÂÆö‰πâÁöÑÂèÇÊï∞„ÄÇÊúÄÁªàÁªèËøáËÆ≠ÁªÉÂêéÂèØ‰ª•ÂæóÂà∞Â¶Ç‰∏ãË°®ÁöÑÊïàÊûú\n\n<table>\n    <tr>\n        <th>Ê®°Âûã</th>\n        <th>P</th>\n        <th>R</th>\n        <th>F1</th>\n    </tr>\n  <tr>\n        <td><b>DeepKE(RE), RoBERTa-wwm-ext, Chinese</b></td>\n        <td>0.7890</td>\n        <td>0.7370</td>\n        <td>0.7327</td>\n    </tr>\n  <tr>\n        <td><b>DeepKE(RE), BERT-wwm, Chinese</b></td>\n        <td>0.7861</td>\n        <td>0.7506</td>\n        <td>0.7473</td>\n    </tr>\n\n</table>\n\n### ÊîØÊåÅÁü•ËØÜSchemaÁ±ªÂûã\n\nDeepKE-cnSchemaÁâπÂà´Áâà‰∏∫ÊîØÊåÅ‰∏≠ÊñáÈ¢ÜÂüüÁü•ËØÜÂõæË∞±ÊûÑÂª∫Êé®Âá∫ÁöÑÂºÄÁÆ±Âç≥Áî®ÁâàÊú¨„ÄÇ [CnSchema](https://github.com/OpenKG-ORG/cnSchema)ÊòØÈù¢Âêë‰∏≠Êñá‰ø°ÊÅØÂ§ÑÁêÜÔºåÂà©Áî®ÂÖàËøõÁöÑÁü•ËØÜÂõæË∞±„ÄÅËá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜÂíåÊú∫Âô®Â≠¶‰π†ÊäÄÊúØÔºåËûçÂêàÁªìÊûÑÂåñ‰∏éÊñáÊú¨Êï∞ÊçÆÔºåÊîØÊåÅÂø´ÈÄüÈ¢ÜÂüüÁü•ËØÜÂª∫Ê®°ÔºåÊîØÊåÅË∑®Êï∞ÊçÆÊ∫ê„ÄÅË∑®È¢ÜÂüü„ÄÅË∑®ËØ≠Ë®ÄÁöÑÂºÄÊîæÊï∞ÊçÆËá™Âä®ÂåñÂ§ÑÁêÜÔºå‰∏∫Êô∫ËÉΩÊú∫Âô®‰∫∫„ÄÅËØ≠‰πâÊêúÁ¥¢„ÄÅÊô∫ËÉΩËÆ°ÁÆóÁ≠âÊñ∞ÂÖ¥Â∫îÁî®Â∏ÇÂú∫Êèê‰æõschemaÂ±ÇÈù¢ÁöÑÊîØÊåÅ‰∏éÊúçÂä°„ÄÇÁõÆÂâçÔºåDeepKE-cnSchemaÊîØÊåÅÁöÑSchemaÁ±ªÂûãÂ¶Ç‰∏ãË°®ÊâÄÁ§∫Ôºö\n\n#### ÂÆû‰ΩìSchema\n\n| Â∫èÂè∑  | ÂÆû‰ΩìÁ±ªÂûã    | Â∫èÂè∑  | ÂÆû‰ΩìÁ±ªÂûã  |\n| --- |:------ | --- |:---- |\n| 1   | cns:‰∫∫Áâ© YAS  | 2   | cns:ÂΩ±ËßÜ‰ΩúÂìÅ TOJ | \n| 3   | cns:ÁõÆ NGS        | 4   | cns:ÁîüÁâ© QCV   | \n| 5   | cns:Number OKB   | 6   | cns:Date BQF | \n| 7   | cns:ÂõΩÂÆ∂ CAR       | 8   | cns:ÁΩëÁ´ô ZFM   | \n| 9   | cns:ÁΩëÁªúÂ∞èËØ¥ EMT     | 10  | cns:Âõæ‰π¶‰ΩúÂìÅ UER | \n| 11  | cns:Ê≠åÊõ≤ QEE       | 12  | cns:Âú∞ÁÇπ UFT   | \n| 13  | cns:Ê∞îÂÄô GJS       | 14  | cns:Ë°åÊîøÂå∫ SVA  | \n| 15  | cns:TEXT ANO     | 16  | cns:ÂéÜÂè≤‰∫∫Áâ© KEJ | \n| 17  | cns:Â≠¶Ê†° ZDI       | 18  | cns:‰ºÅ‰∏ö CAT   | \n| 19  | cns:Âá∫ÁâàÁ§æ GCK      | 20  | cns:‰π¶Á±ç FQK   | \n| 21  | cns:Èü≥‰πê‰∏ìËæë BAK     | 22  | cns:ÂüéÂ∏Ç RET   | \n| 23  | cns:ÊôØÁÇπ QZP       | 24  | cns:ÁîµËßÜÁªºËâ∫ QAQ | \n| 25  | cns:Êú∫ÊûÑ ZRE       | 26  | cns:‰ΩúÂìÅ TDZ   | \n| 27  | cns:ËØ≠Ë®Ä CVC       | 28  | cns:Â≠¶Áßë‰∏ì‰∏ö PMN | \n\n#### ÂÖ≥Á≥ªSchema\n\n| Â∫èÂè∑  | Â§¥ÂÆû‰ΩìÁ±ªÂûã  | Â∞æÂÆû‰ΩìÁ±ªÂûã | ÂÖ≥Á≥ª   | Â∫èÂè∑  | Â§¥ÂÆû‰ΩìÁ±ªÂûã  | Â∞æÂÆû‰ΩìÁ±ªÂûã | ÂÖ≥Á≥ª   |\n| --- |:------ |:-----:| ---- | --- |:------ |:-----:| ---- |\n| 1   | cns:Âú∞ÁÇπ     | cns:‰∫∫Áâ©    | cns:Á•ñÁ±ç   | 2   | cns:‰∫∫Áâ©     | cns:‰∫∫Áâ©    | cns:Áà∂‰∫≤   |\n| 3   | cns:Âú∞ÁÇπ     | cns:‰ºÅ‰∏ö    | cns:ÊÄªÈÉ®Âú∞ÁÇπ | 4   | cns:Âú∞ÁÇπ     | cns:‰∫∫Áâ©    | cns:Âá∫ÁîüÂú∞  |\n| 5   | cns:ÁõÆ      | cns:ÁîüÁâ©    | cns:ÁõÆ    | 6   | cns:Number | cns:Ë°åÊîøÂå∫   | cns:Èù¢ÁßØ   |\n| 7   | cns:Text   | cns:Êú∫ÊûÑ    | cns:ÁÆÄÁß∞   | 8   | cns:Date   | cns:ÂΩ±ËßÜ‰ΩúÂìÅ  | cns:‰∏äÊò†Êó∂Èó¥ |\n| 9   | cns:‰∫∫Áâ©     | cns:‰∫∫Áâ©    | cns:Â¶ªÂ≠ê   | 10  | cns:Èü≥‰πê‰∏ìËæë   | cns:Ê≠åÊõ≤    | cns:ÊâÄÂ±û‰∏ìËæë |\n| 11  | cns:Number | cns:‰ºÅ‰∏ö    | cns:Ê≥®ÂÜåËµÑÊú¨ | 12  | cns:ÂüéÂ∏Ç     | cns:ÂõΩÂÆ∂    | cns:È¶ñÈÉΩ   |\n| 13  | cns:‰∫∫Áâ©     | cns:ÂΩ±ËßÜ‰ΩúÂìÅ  | cns:ÂØºÊºî   | 14  | cns:Text   | cns:ÂéÜÂè≤‰∫∫Áâ©  | cns:Â≠ó    |\n| 15  | cns:Number | cns:‰∫∫Áâ©    | cns:Ë∫´È´ò   | 16  | cns:‰ºÅ‰∏ö     | cns:ÂΩ±ËßÜ‰ΩúÂìÅ  | cns:Âá∫ÂìÅÂÖ¨Âè∏ |\n| 17  | cns:Number | cns:Â≠¶Áßë‰∏ì‰∏ö  | cns:‰øÆ‰∏öÂπ¥Èôê | 18  | cns:Date   | cns:‰∫∫Áâ©    | cns:Âá∫ÁîüÊó•Êúü |\n| 19  | cns:‰∫∫Áâ©     | cns:ÂΩ±ËßÜ‰ΩúÂìÅ  | cns:Âà∂Áâá‰∫∫  | 20  | cns:‰∫∫Áâ©     | cns:‰∫∫Áâ©    | cns:ÊØç‰∫≤   |\n| 21  | cns:‰∫∫Áâ©     | cns:ÂΩ±ËßÜ‰ΩúÂìÅ  | cns:ÁºñËæë   | 22  | cns:ÂõΩÂÆ∂     | cns:‰∫∫Áâ©    | cns:ÂõΩÁ±ç   |\n| 23  | cns:‰∫∫Áâ©     | cns:ÂΩ±ËßÜ‰ΩúÂìÅ  | cns:ÁºñÂâß   | 24  | cns:ÁΩëÁ´ô     | cns:ÁΩëÁ´ôÂ∞èËØ¥  | cns:ËøûËΩΩÁΩëÁªú |\n| 25  | cns:‰∫∫Áâ©     | cns:‰∫∫Áâ©    | cns:‰∏àÂ§´   | 26  | cns:Text   | cns:ÂéÜÂè≤‰∫∫Áâ©  | cns:Êúù‰ª£   |\n| 27  | cns:Text   | cns:‰∫∫Áâ©    | cns:Ê∞ëÊóè   | 28  | cns:Text   | cns:ÂéÜÂè≤‰∫∫Áâ©  | cns:Âè∑   |\n| 29  | cns:Âá∫ÁâàÁ§æ    | cns:‰π¶Á±ç    | cns:Âá∫ÁâàÁ§æ  | 30  | cns:‰∫∫Áâ©     | cns:ÁîµËßÜÁªºËâ∫  | cns:‰∏ªÊåÅ‰∫∫  |\n| 31  | cns:Text   | cns:Â≠¶Áßë‰∏ì‰∏ö  | cns:‰∏ì‰∏ö‰ª£Á†Å | 32  | cns:‰∫∫Áâ©     | cns:Ê≠åÊõ≤    | cns:Ê≠åÊâã   |\n| 33  | cns:‰∫∫Áâ©     | cns:Ê≠åÊõ≤    | cns:‰ΩúÊõ≤   | 34  | cns:‰∫∫Áâ©     | cns:ÁΩëÁªúÂ∞èËØ¥  | cns:‰∏ªËßí   |\n| 35  | cns:‰∫∫Áâ©     | cns:‰ºÅ‰∏ö    | cns:Ëë£‰∫ãÈïø  | 36  | cns:Date   | cns:‰ºÅ‰∏ö    | cns:ÊàêÁ´ãÊó∂Èó¥ |\n| 37  | cns:Â≠¶Ê†°     | cns:‰∫∫Áâ©    | cns:ÊØï‰∏öÈô¢Ê†° | 38  | cns:Number | cns:Êú∫ÊûÑ    | cns:Âç†Âú∞Èù¢ÁßØ |\n| 39  | cns:ËØ≠Ë®Ä     | cns:ÂõΩÂÆ∂    | cns:ÂÆòÊñπËØ≠Ë®Ä | 40  | cns:Text   | cns:Ë°åÊîøÂå∫   | cns:‰∫∫Âè£Êï∞Èáè |\n| 41  | cns:Number | cns:Ë°åÊîøÂå∫   | cns:‰∫∫Âè£Êï∞Èáè | 42  | cns:ÂüéÂ∏Ç     | cns:ÊôØÁÇπ    | cns:ÊâÄÂú®ÂüéÂ∏Ç |\n| 43  | cns:‰∫∫Áâ©     | cns:Âõæ‰π¶‰ΩúÂìÅ  | cns:‰ΩúËÄÖ   | 44  | None   | None    | ÂÖ∂‰ªñ |\n| 45  | cns:‰∫∫Áâ©     | cns:Ê≠åÊõ≤    | cns:‰ΩúÊõ≤   | 46  | cns:‰∫∫Áâ©     | cns:Ë°åÊîøÂå∫   | cns:Ê∞îÂÄô   |\n| 47  | cns:‰∫∫Áâ©     | cns:ÁîµËßÜÁªºËâ∫  | cns:ÂòâÂÆæ   | 48  | cns:‰∫∫Áâ©     | cns:ÂΩ±ËßÜ‰ΩúÂìÅ  | cns:‰∏ªÊºî   |\n| 49  | cns:‰ΩúÂìÅ     | cns:ÂΩ±ËßÜ‰ΩúÂìÅ  | cns:ÊîπÁºñËá™  | 50  | cns:‰∫∫Áâ©     | cns:‰ºÅ‰∏ö    | cns:ÂàõÂßã‰∫∫  |\n\n## Âø´ÈÄüÂä†ËΩΩ\n\n### [ÂÆû‰ΩìËØÜÂà´ÔºàNERÔºâ](https://github.com/zjunlp/DeepKE/tree/main/example/ner/standard)\n\nÁî®Êà∑ÂèØ‰ª•Áõ¥Êé•‰∏ãËΩΩ[Ê®°Âûã](https://drive.google.com/drive/folders/1zA8Ichx9nzU3GD92ptdyR_nmARB_7ovg)ËøõË°å‰ΩøÁî®ÔºåÂÖ∑‰ΩìÊµÅÁ®ãÂ¶Ç‰∏ãÔºö\n\n1. ËøõÂÖ•ÁõÆÂΩï`DeepKE/example/ner/standard`\n2. Â∞Ü‰∏ãËΩΩÁöÑÊ®°ÂûãÊñá‰ª∂Â§πÂëΩÂêç‰∏∫`checkpoints`ÔºåÂ≠òÊîæ‰∫éÁõÆÂΩï`DeepKE/example/ner/standard`‰∏ã\n3. ‰øÆÊîπ `conf/predict.yaml`‰∏≠ÁöÑÂèÇÊï∞`text`‰∏∫ÈúÄË¶ÅÈ¢ÑÊµãÁöÑÊñáÊú¨ÔºåÂπ∂ÊåâÁÖß‰∏ãËΩΩÊ®°ÂûãÁöÑÁ±ªÂà´‰øÆÊîπ`conf/config.yaml`‰∏≠ÁöÑÊ®°ÂûãÂêçÁß∞`hydra/model`‰∏∫`bert`ÊàñËÄÖ`lstmcrf`Ôºà‰∏äËø∞ÈìæÊé•‰∏ãËΩΩÁöÑÊ®°Âûã‰∏∫bertÊ®°ÂûãÔºâ\n\n    > ‰ΩøÁî®ËÆ≠ÁªÉÂ•ΩÁöÑÊ®°ÂûãÔºåÂè™ÈúÄËæìÂÖ•Âè•Â≠ê‚Äú„ÄäÊòüÁ©∫ÈªëÂ§ú‰º†Â•á„ÄãÊòØËøûËΩΩ‰∫éËµ∑ÁÇπ‰∏≠ÊñáÁΩëÁöÑÁΩëÁªúÂ∞èËØ¥Ôºå‰ΩúËÄÖÊòØÂï§ÈÖíÁöÑÁΩ™Â≠Ω‚ÄùÔºåËøêË°å```python predict.py```ÂêéÂèØÂæóÂà∞ÁªìÊûúÔºåÁªìÊûúÊòæÁ§∫‚ÄúÊòüÁ©∫ÈªëÂ§ú‰º†Â•á‚ÄùÂÆû‰ΩìÁ±ªÂûã‰∏∫ÁªèËøácnschemaÂØπÈΩêÂêéÁöÑ‚ÄúÁΩëÁªúÂ∞èËØ¥‚ÄùÔºå‚ÄúËµ∑ÁÇπ‰∏≠ÊñáÁΩë‚Äù‰∏∫‚ÄúÁΩëÁ´ô‚ÄùÔºå‚ÄúÂï§ÈÖíÁöÑÁΩ™Â≠Ω‚Äù‰∏∫‚Äú‰∫∫Áâ©„ÄÇ\n    > ```bash\n    > text=‚Äú„ÄäÊòüÁ©∫ÈªëÂ§ú‰º†Â•á„ÄãÊòØËøûËΩΩ‰∫éËµ∑ÁÇπ‰∏≠ÊñáÁΩëÁöÑÁΩëÁªúÂ∞èËØ¥Ôºå‰ΩúËÄÖÊòØÂï§ÈÖíÁöÑÁΩ™Â≠Ω‚Äù\n    > ```\n4. È¢ÑÊµã\n    ```shell\n    python predict.py\n    ```\n\n    ÊúÄÁªàËæìÂá∫ÁªìÊûú\n\n    ```bash\n    NERÂè•Â≠êÔºö\n    „ÄäÊòüÁ©∫ÈªëÂ§ú‰º†Â•á„ÄãÊòØËøûËΩΩ‰∫éËµ∑ÁÇπ‰∏≠ÊñáÁΩëÁöÑÁΩëÁªúÂ∞èËØ¥Ôºå‰ΩúËÄÖÊòØÂï§ÈÖíÁöÑÁΩ™Â≠Ω\n    NERÁªìÊûúÔºö\n    [('Êòü','B-UER'),('Á©∫','I-UER'),('Èªë','I-UER'),('Â§ú','I-UER'),('‰º†','I-UER'),('Â•á','I-UER'),('Ëµ∑','B-ZFM'),('ÁÇπ','I-ZFM'),('‰∏≠','I-ZFM'),('Êñá','I-ZFM'),('ÁΩë','I-ZFM'),('Âï§','B-YAS'),('ÈÖí','I-YAS'),('ÁöÑ','I-YAS'),('ÁΩ™','I-YAS'),('Â≠Ω','I-YAS')]\n    ```\n\n### [ÂÖ≥Á≥ªÊäΩÂèñÔºàREÔºâ](https://github.com/zjunlp/DeepKE/tree/main/example/re/standard)\n\n‰ΩøÁî®ËÄÖÂèØ‰ª•Áõ¥Êé•‰∏ãËΩΩ[Ê®°Âûã](https://drive.google.com/drive/folders/1wb_QIZduKDwrHeri0s5byibsSQrrJTEv)‰ΩøÁî®,Ê≠•È™§Â¶Ç‰∏ãÔºö\n\n1. ËøõÂÖ•ÁõÆÂΩï`example/re/standard`\n2. ‰øÆÊîπ `conf/predict.yaml`‰∏≠ÁöÑÂèÇÊï∞`fp`‰∏∫‰∏ãËΩΩÊñá‰ª∂ÁöÑË∑ØÂæÑÔºå`conf/embedding.yaml`‰∏≠`num_relations`‰∏∫51ÔºàÂÖ≥Á≥ª‰∏™Êï∞Ôºâ,`conf/config.yaml`‰∏≠ÁöÑÂèÇÊï∞model‰∏∫`lm`\n3. ‰∏ãËΩΩ[Êï∞ÊçÆÈõÜ](https://drive.google.com/drive/folders/1UurqpjePe3zhXxbDDNwLAjVvt7UyUMuQ)ÔºåÊîæÂÖ•ÁõÆÂΩï`example/re/standard/data/origin`‰∏≠\n4. ËøõË°åÈ¢ÑÊµãÔºåÈúÄË¶ÅÈ¢ÑÊµãÁöÑÊñáÊú¨ÂèäÂÆû‰ΩìÂØπÈÄöËøáÁªàÁ´ØËæìÂÖ•ÁªôÁ®ãÂ∫è\n\n    ```bash\n    python predict.py\n    ```\n\n    ‰ΩøÁî®ËÆ≠ÁªÉÂ•ΩÁöÑÊ®°ÂûãÔºåËøêË°å```python predict.py```ÂêéÔºåÂè™ÈúÄËæìÂÖ•ÁöÑÂè•Â≠ê‰∏∫‚ÄúÊ≠åÊõ≤„Ää‰∫∫ÁîüÈïøË∑Ø„ÄãÂá∫Ëá™ÂàòÂæ∑ÂçéÂõΩËØ≠‰∏ìËæë„ÄäÁî∑‰∫∫ÁöÑÁà±„ÄãÔºåÁî±ÊùéÊ≥â‰ΩúËØç‰ΩúÊõ≤Ôºå2001Âπ¥Âá∫Ë°åÂèëÁâà‚ÄùÔºåÁªôÂÆöÁöÑÂÆû‰ΩìÂØπ‰∏∫‚ÄúÁî∑‰∫∫ÁöÑÁà±‚ÄùÂíå‚Äú‰∫∫ÁîüÈïøË∑Ø‚ÄùÔºåÂèØÂæóÂà∞ÁªìÊûúÔºåÊúÄÁªàÊäΩÂèñÂá∫ÁöÑÂÖ≥Á≥ª‰∏∫ÁªèËøácnschemaÂØπÈΩêÂêéÁöÑ‚ÄúÊâÄÂ±û‰∏ìËæë‚Äù„ÄÇ\n\n    Â∞Üpredict.pyÁöÑ_get_predict_instanceÂáΩÊï∞‰øÆÊîπÊàêÂ¶Ç‰∏ãËåÉ‰æãÔºåÂç≥ÂèØ‰øÆÊîπÊñáÊú¨ËøõË°åÈ¢ÑÊµã\n\n    ```python\n    def _get_predict_instance(cfg):\n        flag = input('ÊòØÂê¶‰ΩøÁî®ËåÉ‰æã[y/n]ÔºåÈÄÄÂá∫ËØ∑ËæìÂÖ•: exit .... ')\n        flag = flag.strip().lower()\n        if flag == 'y' or flag == 'yes':\n            sentence = 'Ê≠åÊõ≤„Ää‰∫∫ÁîüÈïøË∑Ø„ÄãÂá∫Ëá™ÂàòÂæ∑ÂçéÂõΩËØ≠‰∏ìËæë„ÄäÁî∑‰∫∫ÁöÑÁà±„ÄãÔºåÁî±ÊùéÊ≥â‰ΩúËØç‰ΩúÊõ≤Ôºå2001Âπ¥Âá∫Ë°åÂèëÁâà'\n            head = 'Áî∑‰∫∫ÁöÑÁà±'\n            tail = '‰∫∫ÁîüÈïøË∑Ø'\n            head_type = 'ÊâÄÂ±û‰∏ìËæë'\n            tail_type = 'Ê≠åÊõ≤'\n        elif flag == 'n' or flag == 'no':\n            sentence = input('ËØ∑ËæìÂÖ•Âè•Â≠êÔºö')\n            head = input('ËØ∑ËæìÂÖ•Âè•‰∏≠ÈúÄË¶ÅÈ¢ÑÊµãÂÖ≥Á≥ªÁöÑÂ§¥ÂÆû‰ΩìÔºö')\n            head_type = input('ËØ∑ËæìÂÖ•Â§¥ÂÆû‰ΩìÁ±ªÂûãÔºö')\n            tail = input('ËØ∑ËæìÂÖ•Âè•‰∏≠ÈúÄË¶ÅÈ¢ÑÊµãÂÖ≥Á≥ªÁöÑÂ∞æÂÆû‰ΩìÔºö')\n            tail_type = input('ËØ∑ËæìÂÖ•Â∞æÂÆû‰ΩìÁ±ªÂûãÔºö')\n        elif flag == 'exit':\n            sys.exit(0)\n        else:\n            print('please input yes or no, or exit!')\n            _get_predict_instance()\n\n        instance = dict()\n        instance['sentence'] = sentence.strip()\n        instance['head'] = head.strip()\n        instance['tail'] = tail.strip()\n        if head_type.strip() == '' or tail_type.strip() == '':\n            cfg.replace_entity_with_type = False\n            instance['head_type'] = 'None'\n            instance['tail_type'] = 'None'\n        else:\n            instance['head_type'] = head_type.strip()\n            instance['tail_type'] = tail_type.strip()\n\n        return instance\n    ```\n\n    ÊúÄÁªàËæìÂá∫ÁªìÊûú\n\n    ```bash\n    ‚ÄúÁî∑‰∫∫ÁöÑÁà±‚ÄùÂíå‚Äú‰∫∫ÁîüÈïøË∑Ø‚ÄùÂú®Âè•‰∏≠ÂÖ≥Á≥ª‰∏∫‚ÄúÊâÄÂ±û‰∏ìËæë‚ÄùÔºåÁΩÆ‰ø°Â∫¶‰∏∫0.99\n    ```\n    > Ê≥®ÔºöËøêË°åËøáÁ®ã‰∏≠‰ºöËá™Âä®‰ªéhuggingfaceÁΩëÁ´ô‰∏≠‰∏ãËΩΩ`example/re/standard/conf/model/lm.yaml`Êñá‰ª∂‰∏≠ÊåáÂÆöÁöÑÊ®°ÂûãÔºåÂ¶Ç‰∏ãËΩΩÂ§±Ë¥•ÂèØ‰ª•ÂØªÊâæÂØπÂ∫îÈïúÂÉèÁΩëÁ´ôÊàñËÄÖÊâãÂä®‰∏ãËΩΩ\n\n### [ËÅîÂêà‰∏âÂÖÉÁªÑÊäΩÂèñ](https://github.com/zjunlp/DeepKE/tree/main/example/triple)\nÁî®Êà∑ÂèØ‰ª•ÂÖàÂ∞Ü‰∏äËø∞Ê®°Âûã‰∏ãËΩΩËá≥Êú¨Âú∞ÔºåÁÑ∂Âêé‰ΩøÁî®[example/triple](https://github.com/zjunlp/DeepKE/tree/main/example/triple)‰∏≠ÁöÑ‰ª£Á†ÅËøõË°å‰∏âÂÖÉÁªÑÊäΩÂèñ„ÄÇÂ¶ÇÊûúÂçïÂè•‰∏≠Â≠òÂú®Ë∂ÖËøá‰∏§‰∏™‰ª•‰∏äÁöÑÂÆû‰ΩìÊï∞ÔºåÂèØËÉΩÂú®‰∏Ä‰∫õÂÆû‰ΩìÂØπ‰∏≠‰ºöÂ≠òÂú®È¢ÑÊµã‰∏çÂáÜÁ°ÆÁöÑÈóÆÈ¢òÔºåÈÇ£ÊòØÂõ†‰∏∫Ëøô‰∫õÂÆû‰ΩìÂØπÂπ∂Ê≤°ÊúâË¢´Âä†ÂÖ•ËÆ≠ÁªÉÈõÜ‰∏≠ËøõË°åËÆ≠ÁªÉÔºåÊâÄ‰ª•ÈúÄË¶ÅËøõ‰∏ÄÊ≠•Âà§Êñ≠ÔºåÂÖ∑‰Ωì‰ΩøÁî®Ê≠•È™§Â¶Ç‰∏ãÔºö\n\n1. Â∞Ü`conf`Êñá‰ª∂Â§π‰∏≠ÁöÑ`predict.yaml`‰∏≠ÁöÑ`text`‰øÆÊîπ‰∏∫È¢ÑÊµãÊñáÊú¨Ôºå`nerfp`‰øÆÊîπ‰∏∫nerÊ®°ÂûãÊñá‰ª∂Â§πÂú∞ÂùÄÔºå`refp`‰∏∫reÊ®°ÂûãÂú∞ÂùÄ\n2. ËøõË°åÈ¢ÑÊµã„ÄÇ\n\n    ```bash\n    python predict.py\n    ```\n\n    ÊúüÈó¥Â∞ÜËæìÂá∫ÂêÑ‰∏™‰∏≠Èó¥Ê≠•È™§ÁªìÊûúÔºå‰ª•ËæìÂÖ•ÊñáÊú¨`Ê≠§Â§ñÁΩëÊòì‰∫ëÂπ≥Âè∞Ëøò‰∏äÊû∂‰∫Ü‰∏ÄÁ≥ªÂàóÊ≠åÊõ≤ÔºåÂÖ∂‰∏≠ÂåÖÊã¨Áî∞È¶•ÁîÑÁöÑ„ÄäÂ∞èÂπ∏Ëøê„ÄãÁ≠â`‰∏∫‰æã„ÄÇ\n\n    2.1 ËæìÂá∫ÁªèËøánerÊ®°ÂûãÂêéÂæóÂà∞ÁªìÊûú`[('Áî∞', 'B-YAS'), ('È¶•', 'I-YAS'), ('ÁîÑ', 'I-YAS'), ('Â∞è', 'B-QEE'), ('Âπ∏', 'I-QEE'), ('Ëøê', 'I-QEE')]`„ÄÇ\n\n    2.2 ËæìÂá∫ËøõË°åÂ§ÑÁêÜÂêéÁªìÊûú`{'Áî∞È¶•ÁîÑ': '‰∫∫Áâ©', 'Â∞èÂπ∏Ëøê': 'Ê≠åÊõ≤'}`\n\n    2.3 ËæìÂá∫ÁªèËøáreÊ®°ÂûãÂêéÂæóÂà∞ÁªìÊûú` \"Áî∞È¶•ÁîÑ\" Âíå \"Â∞èÂπ∏Ëøê\" Âú®Âè•‰∏≠ÂÖ≥Á≥ª‰∏∫Ôºö\"Ê≠åÊâã\"ÔºåÁΩÆ‰ø°Â∫¶‰∏∫0.92„ÄÇ`\n\n    2.4 ËæìÂá∫jsonldÊ†ºÂºèÂåñÂêéÁªìÊûú \n    ```bash\n    {\n      \"@context\": {\n        \"Ê≠åÊâã\": \"https://cnschema.openkg.cn/item/%E6%AD%8C%E6%89%8B/16693#viewPageContent\"\n      },\n      \"@id\": \"Áî∞È¶•ÁîÑ\",\n      \"Ê≠åÊâã\": {\n        \"@id\": \"Â∞èÂπ∏Ëøê\"\n      }\n    }\n    ```\n\n\n## Ëá™ÂÆö‰πâÊ®°Âûã\n\n### ÂÆû‰ΩìËØÜÂà´‰ªªÂä°ÔºàNERÔºâ\n\nÂ¶ÇÊûúÈúÄË¶Å‰ΩøÁî®Ëá™ÂÆö‰πâÁöÑÊï∞ÊçÆËøõË°åËÆ≠ÁªÉÔºåÊ≠•È™§Â¶Ç‰∏ãÔºö\n\n1. ‰∏ãËΩΩËá™ÂÆö‰πâÁöÑ[Êï∞ÊçÆÈõÜ](https://drive.google.com/drive/folders/1zA8Ichx9nzU3GD92ptdyR_nmARB_7ovg)ÔºåÂ∞ÜÂÖ∂ÊîæÂÖ•ÂëΩÂêç‰∏∫`data`ÁöÑÊñá‰ª∂Â§π‰∏≠\n2. Â∞Ü`conf`Êñá‰ª∂Â§π‰∏≠ÁöÑ`train.yaml`‰∏≠ÁöÑ`bert_model`‰øÆÊîπ‰∏∫ÊåáÂÆöÊ®°ÂûãÔºåÁî®Êà∑ÂèØ‰ª•ÈÄöËøá‰øÆÊîπyamlÊñá‰ª∂ÈÄâÊã©‰∏çÂêåÁöÑÊ®°ÂûãËøõË°åËÆ≠ÁªÉÔºàÊé®ËçêÁõ¥Êé•‰∏ãËΩΩÊ®°ÂûãÔºåËÆæÁΩÆ`bert_model`‰∏∫Ê®°ÂûãË∑ØÂæÑÔºâ\n3. ‰øÆÊîπ`train.yaml`‰∏≠ÁöÑ`labels`‰∏∫`data/type.txt`‰∏≠ÊâÄÁî®Âà∞ÁöÑÊ†áÁ≠æ\n4. ËøõË°åËÆ≠ÁªÉ\n\n    ```bash\n    python run.py\n    ```\n\n### ÂÖ≥Á≥ªÊäΩÂèñ‰ªªÂä°ÔºàREÔºâ\n\nÂ¶ÇÊûúÈúÄË¶Å‰ΩøÁî®ÂÖ∂‰ªñÊ®°ÂûãËøõË°åËÆ≠ÁªÉÔºåÊ≠•È™§Â¶Ç‰∏ãÔºö\n\n1. ‰∏ãËΩΩËá™ÂÆö‰πâÁöÑ[Êï∞ÊçÆÈõÜ](https://drive.google.com/drive/folders/1wb_QIZduKDwrHeri0s5byibsSQrrJTEv)ÔºåÂ∞ÜÂÖ∂ÈáçÂëΩÂêç‰∏∫`data`\n2. Â∞Ü`conf`Êñá‰ª∂Â§π‰∏≠ÁöÑ`train.yaml`‰∏∫`lm`,`lm.yaml`‰∏≠ÁöÑ`lm_file`‰øÆÊîπ‰∏∫ÊåáÂÆöÈ¢ÑËÆ≠ÁªÉÊ®°ÂûãÔºå`embedding.yaml`‰∏≠`num_relations`‰∏∫ÂÖ≥Á≥ªÁöÑ‰∏™Êï∞Â¶Ç51ÔºåÁî®Êà∑ÂèØ‰ª•ÈÄöËøá‰øÆÊîπyamlÊñá‰ª∂ÈÄâÊã©‰∏çÂêåÁöÑÊ®°ÂûãËøõË°åËÆ≠ÁªÉ\n3. ËøõË°åËÆ≠ÁªÉ„ÄÇ\n\n    ```bash\n    python run.py\n    ```\n\n## FAQ\n\n**Q: Ëøô‰∏™Ê®°ÂûãÊÄé‰πàÁî®Ôºü**\nA: ÂºÄÁÆ±Âç≥Áî®Ôºå‰∏ãËΩΩÂ•ΩÊ®°ÂûãÊåâÁÖß‰ΩøÁî®ËØ¥ÊòéÂ∞±ËÉΩÂ§üÊäΩÂèñÈ¢ÑÂÆö‰πâcnSchemaÂåÖÂê´ÁöÑÁü•ËØÜ„ÄÇ\n**Â¶ÇÊûúÊÉ≥ÊäΩÂèñcnSchema‰πãÂ§ñÁöÑÁü•ËØÜÔºåÂèØ‰ª•‰ΩøÁî®È´òÁ∫ßÁâàÊú¨Ëá™ÂÆö‰πâÊï∞ÊçÆËøõË°åËÆ≠ÁªÉÂì¶**\n\n**Q: ËØ∑ÈóÆÊúâÂÖ∂‰ªñcnSchemaÊäΩÂèñÊ®°ÂûãÊèê‰æõÂêóÔºü**\nA: ÂæàÈÅóÊÜæÔºåÊàë‰ª¨ÊöÇÊó∂Âè™ËÉΩÊîØÊåÅÈÉ®ÂàÜcnSchemaÁöÑÁü•ËØÜÊäΩÂèñÔºåÊú™Êù•‰ºöÂèëÂ∏ÉÊõ¥Â§öÁöÑÁü•ËØÜÊäΩÂèñÊ®°Âûã„ÄÇ\n\n**Q: ÊàëËÆ≠Âá∫Êù•ÊØî‰Ω†Êõ¥Â•ΩÁöÑÁªìÊûúÔºÅ**\nA: ÊÅ≠Âñú‰Ω†„ÄÇ\n\n**Q: Ëá™Â∑±Êï∞ÊçÆËæìÂÖ•ËøõÂéªÁºñÁ†ÅÊä•Èîô**\nA: ÂèØËÉΩ‰∏≠ÊñáËæìÂÖ•Êï∞ÊçÆÂåÖÂê´‰∫Ü‰∏çÂèØËßÅÁöÑÁâπÊÆäÂ≠óÁ¨¶ÔºåËøô‰∫õÂ≠óÁ¨¶Êó†Ê≥ïË¢´Êüê‰∫õÁºñÁ†ÅÂõ†ËÄåÊä•ÈîôÔºåÊÇ®ÂèØ‰ª•ÈÄöËøáÁºñËæëÂô®ÊàñÂÖ∂‰ªñÂ∑•ÂÖ∑È¢ÑÂ§ÑÁêÜ‰∏≠ÊñáÊï∞ÊçÆËß£ÂÜ≥Ëøô‰∏ÄÈóÆÈ¢ò„ÄÇ\n\n## ÂºïÁî®\n\nÂ¶ÇÊûúÊú¨È°πÁõÆ‰∏≠ÁöÑËµÑÊ∫êÊàñÊäÄÊúØÂØπ‰Ω†ÁöÑÁ†îÁ©∂Â∑•‰ΩúÊúâÊâÄÂ∏ÆÂä©ÔºåÊ¨¢ËøéÂú®ËÆ∫Êñá‰∏≠ÂºïÁî®‰∏ãËø∞ËÆ∫Êñá„ÄÇ\n\n```\n@inproceedings{DBLP:conf/emnlp/ZhangXTYYQXCLL22,\n  author    = {Ningyu Zhang and\n               Xin Xu and\n               Liankuan Tao and\n               Haiyang Yu and\n               Hongbin Ye and\n               Shuofei Qiao and\n               Xin Xie and\n               Xiang Chen and\n               Zhoubo Li and\n               Lei Li},\n  editor    = {Wanxiang Che and\n               Ekaterina Shutova},\n  title     = {DeepKE: {A} Deep Learning Based Knowledge Extraction Toolkit for Knowledge\n               Base Population},\n  booktitle = {Proceedings of the The 2022 Conference on Empirical Methods in Natural\n               Language Processing, {EMNLP} 2022 - System Demonstrations, Abu Dhabi,\n               UAE, December 7-11, 2022},\n  pages     = {98--108},\n  publisher = {Association for Computational Linguistics},\n  year      = {2022},\n  url       = {https://aclanthology.org/2022.emnlp-demos.10},\n  timestamp = {Thu, 23 Mar 2023 16:56:00 +0100},\n  biburl    = {https://dblp.org/rec/conf/emnlp/ZhangXTYYQXCLL22.bib},\n  bibsource = {dblp computer science bibliography, https://dblp.org}\n}\n```\n\n## ÂÖçË¥£Â£∞Êòé\n\n**ËØ•È°πÁõÆ‰∏≠ÁöÑÂÜÖÂÆπ‰ªÖ‰æõÊäÄÊúØÁ†îÁ©∂ÂèÇËÄÉÔºå‰∏ç‰Ωú‰∏∫‰ªª‰ΩïÁªìËÆ∫ÊÄß‰æùÊçÆ„ÄÇ‰ΩøÁî®ËÄÖÂèØ‰ª•Âú®ËÆ∏ÂèØËØÅËåÉÂõ¥ÂÜÖ‰ªªÊÑè‰ΩøÁî®ËØ•Ê®°ÂûãÔºå‰ΩÜÊàë‰ª¨‰∏çÂØπÂõ†‰ΩøÁî®ËØ•È°πÁõÆÂÜÖÂÆπÈÄ†ÊàêÁöÑÁõ¥Êé•ÊàñÈó¥Êé•ÊçüÂ§±Ë¥üË¥£„ÄÇ**\n\n## ÈóÆÈ¢òÂèçÈ¶à\n\nÂ¶ÇÊúâÈóÆÈ¢òÔºåËØ∑Âú®GitHub Issue‰∏≠Êèê‰∫§„ÄÇ\n\n"
        },
        {
          "name": "README_TAG.md",
          "type": "blob",
          "size": 24.517578125,
          "content": "<p align=\"center\">\n    <a href=\"https://github.com/zjunlp/deepke\"> <img src=\"pics/logo.png\" width=\"400\"/></a>\n<p>\n<p align=\"center\">  \n    <a href=\"http://deepke.zjukg.cn\">\n        <img alt=\"Documentation\" src=\"https://img.shields.io/badge/demo-website-blue\">\n    </a>\n    <a href=\"https://pypi.org/project/deepke/#files\">\n        <img alt=\"PyPI\" src=\"https://img.shields.io/pypi/v/deepke\">\n    </a>\n    <a href=\"https://github.com/zjunlp/DeepKE/blob/master/LICENSE\">\n        <img alt=\"GitHub\" src=\"https://img.shields.io/github/license/zjunlp/deepke\">\n    </a>\n    <a href=\"http://zjunlp.github.io/DeepKE\">\n        <img alt=\"Documentation\" src=\"https://img.shields.io/badge/doc-website-red\">\n    </a>\n    <a href=\"https://colab.research.google.com/drive/1vS8YJhJltzw3hpJczPt24O0Azcs3ZpRi?usp=sharing\">\n        <img alt=\"Open In Colab\" src=\"https://colab.research.google.com/assets/colab-badge.svg\">\n    </a>\n</p>\n<p align=\"center\">\n    <b> English | <a href=\"https://github.com/zjunlp/DeepKE/blob/main/README_TAG_CN.md\">ÁÆÄ‰Ωì‰∏≠Êñá</a> </b>\n</p>\n\n<h1 align=\"center\">\n    <p>Data Annotation Instructions</p>\n</h1>\n\n\n\nDeepKE is an open source knowledge graph extraction and construction tool that supports **low-resource, long-text and multi-modal** knowledge extraction tools. Based on PyTorch, it can realize named **entity recognition, relation extraction and attribute extraction functions**. This version, DeepKE-cnSchema, is an out-of-the-box version that allows users to download the model for entity and relational knowledge extraction that supports cnSchema.\n\n---\n\n## Content Introduction\n\n| Chapter                   | Description                                             |\n| ------------------------- | ------------------------------------------------------- |\n| [Introduction](#Introduction)             | The basic principles and supported data types of DeepKE |\n| [Manual Data Annotation](#Manual-Data-Annotation)  | How to manually annotate data                           |\n| [Automatic Data Annotation](#Automatic-Data-Annotation) | How to automatically annotate data based on DeepKE      |\n| [FAQ](#FAQ)                       | Frequently Asked Questions                              |\n| [References](#References)                | Technical reports for this catalogue                    |\n\n## Introduction\n\nDeepKE is an open source knowledge graph extraction and construction tool that supports low-resource, long-text and multi-modal knowledge extraction tools. Based on PyTorch, it can realize named entity recognition, relationship extraction and attribute extraction functions. Also available for beginners are detailed [documentation](https://zjunlp.github.io/DeepKE/), [Google Colab tutorials](https://colab.research.google.com/drive/1vS8YJhJltzw3hpJczPt24O0Azcs3ZpRi?usp=sharing),  [online presentations](http://deepke.zjukg.cn/)and [slideshows](https://github.com/zjunlp/DeepKE/blob/main/docs/slides/Slides-DeepKE-cn.pdf).\n\nIt is well known that data is very important for model training. To facilitate the use of this tool, DeepKE provides detailed annotation of entity identification and relationship extraction data, so that users can obtain training data manually or automatically. The annotated data can be directly used by DeepKE for model training.\n\n## Manual Data Annotation\n\n<div align=\"center\">\n  <img src=\"https://raw.githubusercontent.com/doccano/doccano/master/docs/images/logo/doccano.png\">\n</div>\n\n`doccano` is an open source manual data annotation tool. It provides annotation  functions for **text classification**, **sequence labeling**, and **sequence-to-sequence**. So you can create labeled data for sentiment analysis, named entity recognition, text summaries, and so on. Simply create a project and upload the data and start labeling, and you can build a dataset ready for `DeepKE` training in a matter of hours. Using `doccano` to extract annotation data for entity recognition and relationships is described below.\n\nFor details about doccano installation and configuration, see [Github(doccano)](https://github.com/doccano/doccano)\n\nOnce the server is installed and started, point your browser to  `http://0.0.0.0:8000` and click  **Log in**.\n\n<div align=\"center\">\n  <img src=\"pics/doccano_home_en.png\">\n</div>\n\n### Entity Recognition\n\n#### Create a Project\n\n- Create a Project. Click `Create` in the upper left corner to jump to the following interface.\n\n  - Select the Sequence Labeling task.\n  - Fill in the necessary information, such as Project name and Description.\n  - Check configurable attributes such as Allow overlapping entity and Use relation labeling **as required**.\n\n<div align=\"center\">\n  <img src=\"pics/doccano_create_project_en.png\">\n</div>\n\n * When creation is complete, you will automatically be redirected to the project's home page.\n\n#### Adding the corpus\n\n<div align=\"center\">\n  <img src=\"pics/doccano_data_format_en.png\">\n</div>\n\n\n\n\n- `doccano` supports a variety of text formats. The differences are as follows:\n  - `Textfile`ÔºöThe uploaded file is in the format of `txt`. When marking, a whole `txt` file is displayed as one page of content.\n  - `TextLine`ÔºöThe uploaded file is in the format of `txt`. When marking, a line of text in the `txt` file is displayed as a page of content.\n  - `JSONL`ÔºöShort for `JSON Lines`, where each line is a valid `JSON` value;\n  - `CoNLL`Ôºö A file in `CoNLL` format. Each line contains a series of tab-separated words.\n\n<div align=\"center\">\n  <img src=\"pics/doccano_dataset_en.png\">\n</div>\n\n* Click on the tabs of the **Dataset** again, and you'll see that text has been added to the project, one by one, and then you'll mark the text.\n\n#### Data Annotations\n\n- Add task labels\n\n  - The extraction task includes two label types: `Span` and `Relation`. Here, `Span` refers to the target information fragment in the original text, that is, the entity of a certain type in entity recognition.\n  - Fill in the name of the label. In entity recognition, you can write `PER`, `LOC`, `ORG`, etc.\n  - Add the shortcut key corresponding to this label (e.g. set the shortcut key `p` for the `PER` label) and define the label color.\n\n  <div align=\"center\">\n  <img src=\"pics/doccano_create_label_en.png\">\n  </div>\n\n  - Then just add the other tags you need in the same way.\n\n- Task annotation\n\n  - Annotate data. Click the `Annotate` button to the far right of each data to annotate.\n  - The example defines two `Span` type tags for people and places.\n\n  <div align=\"center\">\n  <img src=\"pics/doccano_annotate_en.png\">\n  </div>\n\n#### Exporting training data\n\n- Click on `Options`, `Export Dataset` in the Dataset column to export the annotated data.\n- The markup data is stored in the same text file, one line per sample and in `jsonl` format, which contains the following fieldsÔºö\n  - `id`: The unique identifier `ID` of the sample in the dataset.\n  - `text`: Raw text data.\n  - `entities`: The `Span` tags contained in the data, each `Span` tag contains four fieldsÔºö\n    - `id`: The unique identification ID of `Span` in the dataset.\n    - `start_offset`: The starting position of `Span`.\n    - `end_offset`: The next position from the end of `Span`.\n    - `label`: Type of `Span`.\n\n- Example of exported data\n\n```json\n{\n    \"id\":10,\n    \"text\":\"University of California is located in California, United States.\",\n    \"entities\":[\n        {\n            \"id\":15,\n            \"label\":\"ORG\",\n            \"start_offset\":0,\n            \"end_offset\":24\n        },\n        {\n            \"id\":16,\n            \"label\":\"LOC\",\n            \"start_offset\":39,\n            \"end_offset\":49\n        },\n        {\n            \"id\":17,\n            \"label\":\"LOC\",\n            \"start_offset\":51,\n            \"end_offset\":64\n        }\n    ],\n    \"relations\":[\n        \n    ]\n}\n```\n\n- The input data format for the entity recognition task in `DeepKE` is a `txt` file, with each line including words, separators and labels (see `CoNLL` data format). The exported data will be pre-processed into the `DeepKE` input format for training, please go to the detailed README \n  - [Regular Full Supervision STANDARD](https://github.com/zjunlp/DeepKE/tree/main/example/ner/standard)\n\n### Relation Extraction\n\n- [Create a Project](#Create-a-Project)\n  - Same operation as entity identification, just refer to above.\n- [Adding the corpus](#Adding-the-corpus)\n  - Same operation as entity identification, just refer to above.\n  - **In the input text, the text format `{text}*{head entity}*{tail entity}*{head entity type}*{tail entity type}`, where the head and tail entity types can be empty.**\n\n#### Data Annotations\n\n- Add task labels\n\n  - The extraction task contains two label types: `Span`and `Relation`. The `Relation` type is used here. `Relation` refers to the relation between `Span` in the original text, that is, the relation between two entities in the relation extraction.\n  - Fill in the name of the relationship type. In relation extraction, you can write `Graduation`, `Causal`, etc.\n  - Add the shortcut key corresponding to this relationship type (e.g. set the shortcut key `b` for the `Graduation` label) and define the label color.\n\n  <div align=\"center\">\n  <img src=\"pics/doccano_create_label_re_en.png\">\n  </div>\n\n  - Then just add the other tags you need in the same way.\n\n- Task annotation\n\n  - Annotate data. Click the `Annotate` button to the far right of each data to annotate.\n  - First click on the label of the relationship to be annotated, then click on the corresponding head and tail entities in turn to complete the relationship annotation.\n  - The example defines two `Span` type tags, `PER` and `LOC`, followed by the relationship tag `Graduation` between entities. The `Relation` tag points from the `Subject` corresponding entity to the `Object` corresponding entity.\n\n  <div align=\"center\">\n  <img src=\"pics/doccano_annotate_re_en.png\">\n  </div>\n\n#### Exporting training data\n\n- Click on `Options`, `Export Dataset` in the Dataset column to export the annotated data.\n- The markup data is stored in the same text file, one line per sample and in `jsonl` format, which contains the following fieldsÔºö\n\n  - `id`: The unique identifier `ID` of the sample in the dataset.\n  - `text`: Raw text data.\n  - `entities`: The `Span` tags contained in the data, each `Span` tag contains four fieldsÔºö\n    - `id`: The unique identification ID of `Span` in the dataset.\n    - `start_offset`: The starting position of `Span`.\n    - `end_offset`: The next position from the end of `Span`.\n    - `label`: Type of `Span`.\n\n  - `relations`: `Relation` tags contained in the data, each `Relation` tag contains four fieldsÔºö\n    - `id`: (`Span1`, `Relation`, `Span2`)Triples are uniquely identified in the dataset by their `ID`, and the same  triple in different samples corresponds to the same `ID`.\n    - `from_id`: The identifier `ID` corresponding to `Span1`.\n    - `to_id`: The identifier `ID` corresponding to `Span2`.\n    - `type`: Type of `Relation` .\n\n- Example of exported data\n\n```json\n{\n    \"id\":13,\n    \"text\":\"The collision resulted in two more crashes in the intersection, including a central concrete truck that was about to turn left onto college ave. *collision*crashes**\",\n    \"entities\":[\n        {\n            \"id\":20,\n            \"label\":\"MISC\",\n            \"start_offset\":4,\n            \"end_offset\":13\n        },\n        {\n            \"id\":21,\n            \"label\":\"MISC\",\n            \"start_offset\":35,\n            \"end_offset\":42\n        }\n    ],\n    \"relations\":[\n        {\n            \"id\":2,\n            \"from_id\":20,\n            \"to_id\":21,\n            \"type\":\"Cause-Effect\"\n        }\n    ]\n}\n```\n\n## Automatic Data Annotation\n\n### Entity Recognition\n\nIn order for users to better use `DeepKE` to complete entity recognition tasks, we provide an easy-to-use **dict matching based** entity recognition **automatic annotation** tool.\n\n#### Dict\n\n- The format of DictÔºö\n\n  <h3 align=\"left\">\n      <img src=\"https://github.com/zjunlp/DeepKE/blob/main/example/ner/prepare-data/pics/vocab_dict.png?raw=true\", width=375>\n  </h3>\n\n- Two entity Dicts (one in Chinese and one in English) are provided in advance, and the samples are automatically tagged using the entity dictionary + jieba part-of-speech tagging.\n\n  - In Chinese example dict, we adapt [The People's Daily Dataset](https://github.com/OYE93/Chinese-NLP-Corpus/tree/master/NER/People's Daily) . It is a dataset for NER, concentrating on their types of named entities related to persons(PER), locations(LOC), and organizations(ORG).\n  - In English example dictÔºåwe adapt Conll dataset. It contains named entities related to persons (PER), locations(LOC), and others (MISC).You can get the Conll dataset with the following command.\n\n  ```shell\n  wget 120.27.214.45/Data/ner/few_shot/data.tar.gz\n  ```\n\n  - Pre-provided dict from Google DriveÔºö\n    - [CN(vocab_dict_cn), EN(vocab_dict_en)](https://drive.google.com/drive/folders/1PGANizeTsvEQFYTL8O1jrDLZwk_MPqO0?usp=sharing)\n  - From BaiduNetDisk Ôºö \n    - [CN(vocab_dict_cn), EN(vocab_dict_en)](https://pan.baidu.com/s/1a07W42ZByeZ00MZp5pZgxg) \n    - (x7ba)\n\n- **If you need to build a domain self-built dictionary, please refer to the pre-provided dictionary format (csv)**\n\n  | Entity     | Label |\n  | ---------- | ----- |\n  | Washington | LOC   |\n  | ...        | ...   |\n\n#### Source File\n\n- **The input dictionary** format is `csv` (contains two columns, entities and corresponding labels).\n\n- **Data to be automatically marked** (txt format and separated by lines, as shown in the figure below) should be placed under the `source_data` path, the script will traverse all txt format files in this folder, and automatically mark line by line.\n\n  <h3 align=\"left\">\n      <img src=\"https://github.com/zjunlp/DeepKE/blob/main/example/ner/prepare-data/pics/en_input_data_format.png?raw=true\", width=700>\n  </h3>\n\n#### Output File\n\n- **The output file**(the distribution ratio of `training set`, `validation set`, and `test set` can be customized) can be directly used as training data in DeepKE.\n\n<h3 align=\"left\">\n    <img src=\"https://github.com/zjunlp/DeepKE/blob/main/example/ner/prepare-data/pics/en_output_data_format.png?raw=true\", width=375>\n</h3>\n\n\n#### Environment\n\nImplementation Environment:\n\n- jieba = 0.42.1\n\n#### Args Description\n\n- `language`: `cn` or `en`\n- `source_dir`: Corpus path (traverse all files in txt format under this folder, automatically mark line by line, the default is `source_data`)\n- `dict_dir`: Entity dict path (defaults to `vocab_dict.csv`)\n- `test_rate, dev_rate, test_rate`: The ratio of training_set, validation_set, and test_set (please make sure the sum is `1`, default `0.8:0.1:0.1`)\n\n#### Run\n\n- **Chinese**\n\n```bash\npython prepare_weaksupervised_data.py --language cn --dict_dir vocab_dict_cn.csv\n```\n\n- **English**\n\n```bash\npython prepare_weaksupervised_data.py --language en --dict_dir vocab_dict_en.csv\n```\n\n### Relation extraction\n\nWe provide a simple **distant supervised** based tool to label relation labels for our RE tasks.\n\n#### Source File\n\nWe specify the source file (dataset to be labeled) as `.json` format and include **one** pair of entities, head entity and tail entity respectively. Each piece of data should contain at least the following five items: `sentence`, `head`, `tail`, `head_offset`, `tail_offset`. The detailed json pattern is as follows:\n\n```json\n[\n  {\n    \"sentence\": \"This summer, the United States Embassy in Beirut, Lebanon, once again made its presence felt on the cultural scene by sponsoring a photo exhibition, an experimental jazz performance, a classical music concert and a visit from the Whiffenpoofs, Yale University's a cappella singers.\",\n    \"head\": \"Lebanon\",\n    \"tail\": \"Beirut\",\n    \"head_offset\": \"50\",\n    \"tail_offset\": \"42\",\n    //...\n  },\n  //... \n]\n```\n\n#### Triple File\n\nEntity pairs in source file will be matched with the triples in the triple file. The entity pairs will be labeled with the relation type if matched with the triples in triple file. If there is no triples match, the pairs will be labeled as `None` type.\n\nWe provide an [English](https://drive.google.com/drive/folders/1HHkm3RBI3okiu8jGs-Wn0vP_-0hz7pb2?usp=sharing) and a [Chinese](https://drive.google.com/file/d/1YpaMpivodG39p53MM9sMpB41q4EoiQhH/view?usp=sharing) triple file respectively. The English triple file comes from `NYT` dataset which contains the following relation types:\n\n```python\n\"/business/company/place_founded\",\n\"/people/person/place_lived\",\n\"/location/country/administrative_divisions\",\n\"/business/company/major_shareholders\",\n\"/sports/sports_team_location/teams\",\n\"/people/person/religion\",\n\"/people/person/place_of_birth\",\n\"/people/person/nationality\",\n\"/location/country/capital\",\n\"/business/company/advisors\",\n\"/people/deceased_person/place_of_death\",\n\"/business/company/founders\",\n\"/location/location/contains\",\n\"/people/person/ethnicity\",\n\"/business/company_shareholder/major_shareholder_of\",\n\"/people/ethnicity/geographic_distribution\",\n\"/people/person/profession\",\n\"/business/person/company\",\n\"/people/person/children\",\n\"/location/administrative_division/country\",\n\"/people/ethnicity/people\",\n\"/sports/sports_team/location\",\n\"/location/neighborhood/neighborhood_of\",\n\"/business/company/industry\"\n```\n\nThe Chinese triple file are from [here](https://github.com/DannyLee1991/ExtractTriples) with the following relation types:\n\n```json\n{\"object_type\": \"Âú∞ÁÇπ\", \"predicate\": \"Á•ñÁ±ç\", \"subject_type\": \"‰∫∫Áâ©\"}\n{\"object_type\": \"‰∫∫Áâ©\", \"predicate\": \"Áà∂‰∫≤\", \"subject_type\": \"‰∫∫Áâ©\"}\n{\"object_type\": \"Âú∞ÁÇπ\", \"predicate\": \"ÊÄªÈÉ®Âú∞ÁÇπ\", \"subject_type\": \"‰ºÅ‰∏ö\"}\n{\"object_type\": \"Âú∞ÁÇπ\", \"predicate\": \"Âá∫ÁîüÂú∞\", \"subject_type\": \"‰∫∫Áâ©\"}\n{\"object_type\": \"ÁõÆ\", \"predicate\": \"ÁõÆ\", \"subject_type\": \"ÁîüÁâ©\"}\n{\"object_type\": \"Number\", \"predicate\": \"Èù¢ÁßØ\", \"subject_type\": \"Ë°åÊîøÂå∫\"}\n{\"object_type\": \"Text\", \"predicate\": \"ÁÆÄÁß∞\", \"subject_type\": \"Êú∫ÊûÑ\"}\n{\"object_type\": \"Date\", \"predicate\": \"‰∏äÊò†Êó∂Èó¥\", \"subject_type\": \"ÂΩ±ËßÜ‰ΩúÂìÅ\"}\n{\"object_type\": \"‰∫∫Áâ©\", \"predicate\": \"Â¶ªÂ≠ê\", \"subject_type\": \"‰∫∫Áâ©\"}\n{\"object_type\": \"Èü≥‰πê‰∏ìËæë\", \"predicate\": \"ÊâÄÂ±û‰∏ìËæë\", \"subject_type\": \"Ê≠åÊõ≤\"}\n{\"object_type\": \"Number\", \"predicate\": \"Ê≥®ÂÜåËµÑÊú¨\", \"subject_type\": \"‰ºÅ‰∏ö\"}\n{\"object_type\": \"ÂüéÂ∏Ç\", \"predicate\": \"È¶ñÈÉΩ\", \"subject_type\": \"ÂõΩÂÆ∂\"}\n{\"object_type\": \"‰∫∫Áâ©\", \"predicate\": \"ÂØºÊºî\", \"subject_type\": \"ÂΩ±ËßÜ‰ΩúÂìÅ\"}\n{\"object_type\": \"Text\", \"predicate\": \"Â≠ó\", \"subject_type\": \"ÂéÜÂè≤‰∫∫Áâ©\"}\n{\"object_type\": \"Number\", \"predicate\": \"Ë∫´È´ò\", \"subject_type\": \"‰∫∫Áâ©\"}\n{\"object_type\": \"‰ºÅ‰∏ö\", \"predicate\": \"Âá∫ÂìÅÂÖ¨Âè∏\", \"subject_type\": \"ÂΩ±ËßÜ‰ΩúÂìÅ\"}\n{\"object_type\": \"Number\", \"predicate\": \"‰øÆ‰∏öÂπ¥Èôê\", \"subject_type\": \"Â≠¶Áßë‰∏ì‰∏ö\"}\n{\"object_type\": \"Date\", \"predicate\": \"Âá∫ÁîüÊó•Êúü\", \"subject_type\": \"‰∫∫Áâ©\"}\n{\"object_type\": \"‰∫∫Áâ©\", \"predicate\": \"Âà∂Áâá‰∫∫\", \"subject_type\": \"ÂΩ±ËßÜ‰ΩúÂìÅ\"}\n{\"object_type\": \"‰∫∫Áâ©\", \"predicate\": \"ÊØç‰∫≤\", \"subject_type\": \"‰∫∫Áâ©\"}\n{\"object_type\": \"‰∫∫Áâ©\", \"predicate\": \"ÁºñÂâß\", \"subject_type\": \"ÂΩ±ËßÜ‰ΩúÂìÅ\"}\n{\"object_type\": \"ÂõΩÂÆ∂\", \"predicate\": \"ÂõΩÁ±ç\", \"subject_type\": \"‰∫∫Áâ©\"}\n{\"object_type\": \"Number\", \"predicate\": \"Êµ∑Êãî\", \"subject_type\": \"Âú∞ÁÇπ\"}\n{\"object_type\": \"ÁΩëÁ´ô\", \"predicate\": \"ËøûËΩΩÁΩëÁ´ô\", \"subject_type\": \"ÁΩëÁªúÂ∞èËØ¥\"}\n{\"object_type\": \"‰∫∫Áâ©\", \"predicate\": \"‰∏àÂ§´\", \"subject_type\": \"‰∫∫Áâ©\"}\n{\"object_type\": \"Text\", \"predicate\": \"Êúù‰ª£\", \"subject_type\": \"ÂéÜÂè≤‰∫∫Áâ©\"}\n{\"object_type\": \"Text\", \"predicate\": \"Ê∞ëÊóè\", \"subject_type\": \"‰∫∫Áâ©\"}\n{\"object_type\": \"Text\", \"predicate\": \"Âè∑\", \"subject_type\": \"ÂéÜÂè≤‰∫∫Áâ©\"}\n{\"object_type\": \"Âá∫ÁâàÁ§æ\", \"predicate\": \"Âá∫ÁâàÁ§æ\", \"subject_type\": \"‰π¶Á±ç\"}\n{\"object_type\": \"‰∫∫Áâ©\", \"predicate\": \"‰∏ªÊåÅ‰∫∫\", \"subject_type\": \"ÁîµËßÜÁªºËâ∫\"}\n{\"object_type\": \"Text\", \"predicate\": \"‰∏ì‰∏ö‰ª£Á†Å\", \"subject_type\": \"Â≠¶Áßë‰∏ì‰∏ö\"}\n{\"object_type\": \"‰∫∫Áâ©\", \"predicate\": \"Ê≠åÊâã\", \"subject_type\": \"Ê≠åÊõ≤\"}\n{\"object_type\": \"‰∫∫Áâ©\", \"predicate\": \"‰ΩúËØç\", \"subject_type\": \"Ê≠åÊõ≤\"}\n{\"object_type\": \"‰∫∫Áâ©\", \"predicate\": \"‰∏ªËßí\", \"subject_type\": \"ÁΩëÁªúÂ∞èËØ¥\"}\n{\"object_type\": \"‰∫∫Áâ©\", \"predicate\": \"Ëë£‰∫ãÈïø\", \"subject_type\": \"‰ºÅ‰∏ö\"}\n{\"object_type\": \"Date\", \"predicate\": \"ÊàêÁ´ãÊó•Êúü\", \"subject_type\": \"Êú∫ÊûÑ\"}\n{\"object_type\": \"Â≠¶Ê†°\", \"predicate\": \"ÊØï‰∏öÈô¢Ê†°\", \"subject_type\": \"‰∫∫Áâ©\"}\n{\"object_type\": \"Number\", \"predicate\": \"Âç†Âú∞Èù¢ÁßØ\", \"subject_type\": \"Êú∫ÊûÑ\"}\n{\"object_type\": \"ËØ≠Ë®Ä\", \"predicate\": \"ÂÆòÊñπËØ≠Ë®Ä\", \"subject_type\": \"ÂõΩÂÆ∂\"}\n{\"object_type\": \"Text\", \"predicate\": \"ÈÇÆÊîøÁºñÁ†Å\", \"subject_type\": \"Ë°åÊîøÂå∫\"}\n{\"object_type\": \"Number\", \"predicate\": \"‰∫∫Âè£Êï∞Èáè\", \"subject_type\": \"Ë°åÊîøÂå∫\"}\n{\"object_type\": \"ÂüéÂ∏Ç\", \"predicate\": \"ÊâÄÂú®ÂüéÂ∏Ç\", \"subject_type\": \"ÊôØÁÇπ\"}\n{\"object_type\": \"‰∫∫Áâ©\", \"predicate\": \"‰ΩúËÄÖ\", \"subject_type\": \"Âõæ‰π¶‰ΩúÂìÅ\"}\n{\"object_type\": \"Date\", \"predicate\": \"ÊàêÁ´ãÊó•Êúü\", \"subject_type\": \"‰ºÅ‰∏ö\"}\n{\"object_type\": \"‰∫∫Áâ©\", \"predicate\": \"‰ΩúÊõ≤\", \"subject_type\": \"Ê≠åÊõ≤\"}\n{\"object_type\": \"Ê∞îÂÄô\", \"predicate\": \"Ê∞îÂÄô\", \"subject_type\": \"Ë°åÊîøÂå∫\"}\n{\"object_type\": \"‰∫∫Áâ©\", \"predicate\": \"ÂòâÂÆæ\", \"subject_type\": \"ÁîµËßÜÁªºËâ∫\"}\n{\"object_type\": \"‰∫∫Áâ©\", \"predicate\": \"‰∏ªÊºî\", \"subject_type\": \"ÂΩ±ËßÜ‰ΩúÂìÅ\"}\n{\"object_type\": \"‰ΩúÂìÅ\", \"predicate\": \"ÊîπÁºñËá™\", \"subject_type\": \"ÂΩ±ËßÜ‰ΩúÂìÅ\"}\n{\"object_type\": \"‰∫∫Áâ©\", \"predicate\": \"ÂàõÂßã‰∫∫\", \"subject_type\": \"‰ºÅ‰∏ö\"}\n```\n\nYou can also use your customized triple file, but the file format should be `.csv` and with the following parttern:\n\n| head    | tail   | rel                         |\n| ------- | ------ | --------------------------- |\n| Lebanon | Beirut | /location/location/contains |\n| ...     | ...    | ...                         |\n\n#### Output File\n\nThe output file names are `labeled_train.json`, `labeled_dev.json`, `labeled_test.json` for the `train`, `dev`, `test` dataset. The format of the output file is as follows:\n\n```json\n[\n\t{\n    \"sentence\": \"This summer, the United States Embassy in Beirut, Lebanon, once again made its presence felt on the cultural scene by sponsoring a photo exhibition, an experimental jazz performance, a classical music concert and a visit from the Whiffenpoofs, Yale University's a cappella singers.\",\n    \"head\": \"Lebanon\",\n    \"tail\": \"Beirut\",\n    \"head_offset\": \"50\",\n    \"tail_offset\": \"42\",\n    \"relation\": \"/location/location/contains\",\n    //...\n\t},\n  //...\n]\n```\n\nWe automatically split the source data into three splits with the rate `0.8:0.1:0.1`.You can set your own split rate.\n\n#### Args Description\n\n- `language`: `en` or `cn`\n- `source_file`: data file to be labeled\n- `triple_file`: triple file path\n- `test_rate, dev_rate, test_rate`: The ratio of training_set, validation_set, and test_set (please make sure the sum is `1`, default `0.8:0.1:0.1`)\n\n#### Run\n\n```bash\npython ds_label_data.py --language en --source_file source_data.json --triple_file triple_file.csv\n```\n\n## FAQ\n\n- **Q: How much data do I need to mark?**\n  - AÔºöLabeling is a labor-intensive and time-consuming process that can be very costly. Ideally, the more data that is labeled, the better the model will be. But in practice, this is not always possible. On the one hand, it is necessary to combine practical resources and time; on the other hand, it is important to consider the impact of increasing data volume on the improvement of model effectiveness. In entity identification, relationship extraction usually requires a data volume of `10K` or so.\n\n\n- **Q: Is there any labeled data available?**\n  - A: In entity recognition, two entity dictionaries (one each in English and Chinese) are pre-provided to enable automatic annotation of samples.\n- **Q: Automatic labeling of data training models does not work**\n  - A: There are several possible reasons for the poor results of the modelÔºö\n    - Automatically annotated, manually annotated data contains a high level of noise \n    - Small data size: Large models with many parameters are overfitted on small datasets. \n  - solutionÔºö\n    - Consider checking the quality of the data\n    - Using strategies such as **semi-supervised** training or `Self-Traning`\n    - Increase the volume of data using data enhancement\n\n## References\n\nIf the resources or techniques in this project have been useful to your research, you are welcome to cite the following paper in your thesis.\n\n```\n@article{zhang2022deepke,\n  title={DeepKE: A Deep Learning Based Knowledge Extraction Toolkit for Knowledge Base Population},\n  author={Zhang, Ningyu and Xu, Xin and Tao, Liankuan and Yu, Haiyang and Ye, Hongbin and Qiao, Shuofei and Xie, Xin and Chen, Xiang and Li, Zhoubo and Li, Lei and Liang, Xiaozhuan and others},\n  journal={arXiv preprint arXiv:2201.03335},\n  year={2022}\n}\n```\n\n## Disclaimers\n\n**The contents of this project are for technical research purposes only and are not intended as a basis for any conclusive findings. Users are free to use the model as they wish within the scope of the licence, but we cannot be held responsible for direct or indirect damage resulting from the use of the contents of the project.** \n\n## Feedback\n\nIf you have any questions, please submit them in the GitHub Issue.\n"
        },
        {
          "name": "README_TAG_CN.md",
          "type": "blob",
          "size": 22.580078125,
          "content": "<p align=\"center\">\n    <a href=\"https://github.com/zjunlp/deepke\"> <img src=\"pics/logo.png\" width=\"400\"/></a>\n<p>\n<p align=\"center\">  \n    <a href=\"http://deepke.zjukg.cn\">\n        <img alt=\"Documentation\" src=\"https://img.shields.io/badge/demo-website-blue\">\n    </a>\n    <a href=\"https://pypi.org/project/deepke/#files\">\n        <img alt=\"PyPI\" src=\"https://img.shields.io/pypi/v/deepke\">\n    </a>\n    <a href=\"https://github.com/zjunlp/DeepKE/blob/master/LICENSE\">\n        <img alt=\"GitHub\" src=\"https://img.shields.io/github/license/zjunlp/deepke\">\n    </a>\n    <a href=\"http://zjunlp.github.io/DeepKE\">\n        <img alt=\"Documentation\" src=\"https://img.shields.io/badge/doc-website-red\">\n    </a>\n    <a href=\"https://colab.research.google.com/drive/1vS8YJhJltzw3hpJczPt24O0Azcs3ZpRi?usp=sharing\">\n        <img alt=\"Open In Colab\" src=\"https://colab.research.google.com/assets/colab-badge.svg\">\n    </a>\n</p>\n\n<p align=\"center\">\n    <b> <a href=\"https://github.com/zjunlp/DeepKE/blob/main/README_CNSCHEMA.md\">English</a> | ÁÆÄ‰Ωì‰∏≠Êñá </b>\n</p>\n\n<h1 align=\"center\">\n    <p>Êï∞ÊçÆÊ†áÊ≥®ËØ¥Êòé</p>\n</h1>\n\nDeepKE ÊòØ‰∏Ä‰∏™ÂºÄÊ∫êÁöÑÁü•ËØÜÂõæË∞±ÊäΩÂèñ‰∏éÊûÑÂª∫Â∑•ÂÖ∑ÔºåÊîØÊåÅ<b>‰ΩéËµÑÊ∫ê„ÄÅÈïøÁØáÁ´†„ÄÅÂ§öÊ®°ÊÄÅ</b>ÁöÑÁü•ËØÜÊäΩÂèñÂ∑•ÂÖ∑ÔºåÂèØ‰ª•Âü∫‰∫é<b>PyTorch</b>ÂÆûÁé∞<b>ÂëΩÂêçÂÆû‰ΩìËØÜÂà´</b>„ÄÅ<b>ÂÖ≥Á≥ªÊäΩÂèñ</b>Âíå<b>Â±ûÊÄßÊäΩÂèñ</b>ÂäüËÉΩ„ÄÇÊ≠§ÁâàÊú¨DeepKE-cnSchema‰∏∫ÂºÄÁÆ±Âç≥Áî®ÁâàÊú¨ÔºåÁî®Êà∑‰∏ãËΩΩÊ®°ÂûãÂç≥ÂèØÂÆûÁé∞ÊîØÊåÅcnSchemaÁöÑÂÆû‰ΩìÂíåÂÖ≥Á≥ªÁü•ËØÜÊäΩÂèñ„ÄÇ\n\n---\n\n## ÂÜÖÂÆπÂØºÂºï\n\n| Á´†ËäÇ                      | ÊèèËø∞                   |\n| ----------------------- | -------------------- |\n| [ÁÆÄ‰ªã](#ÁÆÄ‰ªã)               | ‰ªãÁªçDeepKEÂü∫Êú¨ÂéüÁêÜÂíåÊîØÊåÅÁöÑÊï∞ÊçÆÁ±ªÂûã |\n| [‰∫∫Â∑•Êï∞ÊçÆÊ†áÊ≥®](#‰∫∫Â∑•Êï∞ÊçÆÊ†áÊ≥®)       | ‰ªãÁªçÂ¶Ç‰Ωï‰∫∫Â∑•Ê†áÊ≥®Êï∞ÊçÆ           |\n| [Ëá™Âä®Êï∞ÊçÆÊ†áÊ≥®](#Ëá™Âä®Êï∞ÊçÆÊ†áÊ≥®) | ‰ªãÁªçÂ¶Ç‰ΩïÂü∫‰∫éDeepKEËá™Âä®Ê†áÊ≥®Êï∞ÊçÆ   |\n| [FAQ](#FAQ)             | Â∏∏ËßÅÈóÆÈ¢òÁ≠îÁñë               |\n| [ÂºïÁî®](#ÂºïÁî®)               | Êú¨ÁõÆÂΩïÁöÑÊäÄÊúØÊä•Âëä             |\n\n## ÁÆÄ‰ªã\n\nDeepKE ÊòØ‰∏Ä‰∏™ÂºÄÊ∫êÁöÑÁü•ËØÜÂõæË∞±ÊäΩÂèñ‰∏éÊûÑÂª∫Â∑•ÂÖ∑ÔºåÊîØÊåÅ‰ΩéËµÑÊ∫ê„ÄÅÈïøÁØáÁ´†„ÄÅÂ§öÊ®°ÊÄÅÁöÑÁü•ËØÜÊäΩÂèñÂ∑•ÂÖ∑ÔºåÂèØ‰ª•Âü∫‰∫éPyTorchÂÆûÁé∞ÂëΩÂêçÂÆû‰ΩìËØÜÂà´„ÄÅÂÖ≥Á≥ªÊäΩÂèñÂíåÂ±ûÊÄßÊäΩÂèñÂäüËÉΩ„ÄÇÂêåÊó∂‰∏∫ÂàùÂ≠¶ËÄÖÊèê‰æõ‰∫ÜËØ¶Â∞ΩÁöÑ[ÊñáÊ°£](https://zjunlp.github.io/DeepKE/)Ôºå[Google ColabÊïôÁ®ã](https://colab.research.google.com/drive/1vS8YJhJltzw3hpJczPt24O0Azcs3ZpRi?usp=sharing)Ôºå[Âú®Á∫øÊºîÁ§∫](http://deepke.zjukg.cn/)Âíå[ÂπªÁÅØÁâá](https://github.com/zjunlp/DeepKE/blob/main/docs/slides/Slides-DeepKE-cn.pdf)„ÄÇ\n\n‰ºóÊâÄÂë®Áü•ÔºåÊï∞ÊçÆÂØπÊ®°ÂûãËÆ≠ÁªÉÈùûÂ∏∏ÈáçË¶Å„ÄÇ‰∏∫Êñπ‰æøÁî®Êà∑‰ΩøÁî®Êú¨Â∑•ÂÖ∑ÔºåDeepKEÊèê‰æõ‰∫ÜËØ¶ÁªÜÁöÑÂÆû‰ΩìËØÜÂà´„ÄÅÂÖ≥Á≥ªÊäΩÂèñÊï∞ÊçÆÊ†áÊ≥®ËØ¥ÊòéÔºå‰ª•Êñπ‰æøÁî®Êà∑ÈÄöËøá‰∫∫Â∑•ÊàñËá™Âä®ÊñπÂºèÂæóÂà∞ËÆ≠ÁªÉÊï∞ÊçÆÔºåÊ†áÊ≥®Â•ΩÁöÑÊï∞ÊçÆÂèØ‰ª•Áõ¥Êé•‰æõDeepKEËøõË°åÊ®°ÂûãËÆ≠ÁªÉ„ÄÇ\n\n## ‰∫∫Â∑•Êï∞ÊçÆÊ†áÊ≥®\n\n<div align=\"center\">\n  <img src=\"https://raw.githubusercontent.com/doccano/doccano/master/docs/images/logo/doccano.png\">\n</div>\n\n`doccano`ÊòØ‰∏Ä‰∏™ÂºÄÊ∫êÁöÑ‰∫∫Â∑•Êï∞ÊçÆÊ†áÊ≥®Â∑•ÂÖ∑„ÄÇÂÆÉ‰∏∫**ÊñáÊú¨ÂàÜÁ±ª**„ÄÅ**Â∫èÂàóÊ†áËÆ∞**Âíå**Â∫èÂàóÂà∞Â∫èÂàó**Êèê‰æõÊ†áÊ≥®ÂäüËÉΩ„ÄÇÂõ†Ê≠§ÊÇ®ÂèØ‰ª•‰∏∫ÊÉÖÊÑüÂàÜÊûê„ÄÅÂëΩÂêçÂÆû‰ΩìËØÜÂà´„ÄÅÊñáÊú¨ÊëòË¶ÅÁ≠â‰ªªÂä°ÂàõÂª∫Ê†áËÆ∞Êï∞ÊçÆ„ÄÇÂè™ÈúÄÂàõÂª∫‰∏Ä‰∏™È°πÁõÆÂπ∂‰∏ä‰º†Êï∞ÊçÆÂπ∂ÂºÄÂßãÊ†áËÆ∞ÔºåÊÇ®ÂèØ‰ª•Âú®Êï∞Â∞èÊó∂ÂÜÖÊûÑÂª∫ÂèØ‰æõ`DeepKE`ËÆ≠ÁªÉÁöÑÊï∞ÊçÆÈõÜ„ÄÇ‰∏ãÈù¢‰ºö‰ªãÁªç‰ΩøÁî®`doccano`‰∏∫ÂÆû‰ΩìËØÜÂà´ÂíåÂÖ≥Á≥ªÊäΩÂèñÊ†áÊ≥®Êï∞ÊçÆ„ÄÇ\n\ndoccanoÁöÑÂÆâË£Ö‰∏éÈÖçÁΩÆÂèØÂèÇËÄÉ [Github(doccano)](https://github.com/doccano/doccano)\n\nÂÆâË£ÖÂπ∂ÂêØÂä®serverÂêéÔºåÂú®ÊµèËßàÂô®‰∏≠ËÆøÈóÆ`http://0.0.0.0:8000`ÔºåÂπ∂ÁÇπÂáª**ÁôªÈôÜ**„ÄÇ\n\n<div align=\"center\">\n  <img src=\"pics/doccano_home_cn.png\">\n</div>\n\n### ÂÆû‰ΩìËØÜÂà´\n\n#### ÂàõÂª∫È°πÁõÆ\n\n- ÂàõÂª∫È°πÁõÆ„ÄÇÁÇπÂáªÂ∑¶‰∏äËßíÁöÑ`ÂàõÂª∫`ÔºåË∑≥ËΩ¨Ëá≥‰ª•‰∏ãÁïåÈù¢„ÄÇ\n\n    - ÈÄâÊã©Â∫èÂàóÊ†áÊ≥®ÔºàSequence LabelingÔºâ‰ªªÂä°„ÄÇ\n    - Â°´ÂÜôÈ°πÁõÆÂêçÁß∞ÔºàProject nameÔºâ„ÄÅÊèèËø∞ÔºàDescriptionÔºâÁ≠âÂøÖË¶Å‰ø°ÊÅØ„ÄÇ\n    - **ÊåâÈúÄÂãæÈÄâ**ÂÖÅËÆ∏ÂÆû‰ΩìÈáçÂè†ÔºàAllow overlapping entityÔºâ„ÄÅ‰ΩøÁî®ÂÖ≥Á≥ªÊ†áÊ≥®ÔºàUse relation labelingÔºâÁ≠âÂèØÈÖçÁΩÆÂ±ûÊÄß„ÄÇ\n\n<div align=\"center\">\n  <img src=\"pics/doccano_create_project_cn.png\">\n</div>\n\n * Âú®ÂàõÂª∫ÂÆåÊàêÂêéÔºå‰ºöËá™Âä®Ë∑≥ËΩ¨Âà∞È°πÁõÆÁöÑ‰∏ªÈ°µ„ÄÇ\n\n#### Ê∑ªÂä†ËØ≠ÊñôÂ∫ì\n\n<div align=\"center\">\n  <img src=\"pics/doccano_data_format_cn.png\">\n</div>\n\n\n- `doccano`ÊîØÊåÅÂ§öÁßçÊ†ºÂºèÁöÑÊñáÊú¨ÔºåÂÆÉ‰ª¨ÁöÑÂå∫Âà´Â¶Ç‰∏ãÔºö\n    - `Textfile`Ôºö‰∏ä‰º†ÁöÑÊñá‰ª∂‰∏∫`txt`Ê†ºÂºèÔºåÊâìÊ†áÊó∂‰∏ÄÊï¥‰∏™`txt`Êñá‰ª∂ÊòæÁ§∫‰∏∫‰∏ÄÈ°µÂÜÖÂÆπÔºõ\n    - `Textline`Ôºö‰∏ä‰º†ÁöÑÊñá‰ª∂‰∏∫`txt`Ê†ºÂºèÔºåÊâìÊ†áÊó∂`txt`Êñá‰ª∂ÁöÑ‰∏ÄË°åÊñáÂ≠óÊòæÁ§∫‰∏∫‰∏ÄÈ°µÂÜÖÂÆπÔºõ\n    - `JSONL`Ôºö`JSON Lines`ÁöÑÁÆÄÂÜôÔºåÊØèË°åÊòØ‰∏Ä‰∏™ÊúâÊïàÁöÑ`JSON`ÂÄºÔºõ\n    - `CoNLL`Ôºö `CoNLL`Ê†ºÂºèÁöÑÊñá‰ª∂ÔºåÊØèË°åÂùáÂ∏¶Êúâ‰∏ÄÁ≥ªÂàóÂà∂Ë°®Á¨¶ÂàÜÈöîÁöÑÂçïËØçÔºõ\n\n<div align=\"center\">\n  <img src=\"pics/doccano_dataset_cn.png\">\n</div>\n\n* ÂÜçÁÇπÂáª**Êï∞ÊçÆÈõÜ**ÁöÑÊ†áÁ≠æÔºåÂ∞±ÂèØ‰ª•ÁúãÂà∞‰∏ÄÊù°‰∏ÄÊù°ÁöÑÊñáÊú¨Â∑≤ÁªèË¢´Ê∑ªÂä†Âà∞È°πÁõÆ‰∏≠‰∫ÜÔºå‰πãÂêéÊàë‰ª¨Â∞ÜÂØπËøô‰∫õÊñáÊú¨ËøõË°åÊâìÊ†á„ÄÇ\n\n#### Êï∞ÊçÆÊ†áÊ≥®\n\n- Ê∑ªÂä†‰ªªÂä°Ê†áÁ≠æ\n    - ÊäΩÂèñÂºè‰ªªÂä°ÂåÖÂê´`Span`‰∏é`Relation`‰∏§ÁßçÊ†áÁ≠æÁ±ªÂûãÔºåÊ≠§Â§ÑÈááÁî®`Span`Á±ªÂûãÔºå`Span`ÊåáÂéüÊñáÊú¨‰∏≠ÁöÑÁõÆÊ†á‰ø°ÊÅØÁâáÊÆµÔºåÂç≥ÂÆû‰ΩìËØÜÂà´‰∏≠Êüê‰∏™Á±ªÂûãÁöÑÂÆû‰Ωì„ÄÇ\n    - Â°´ÂÖ•Ê†áÁ≠æÁöÑÂêçÂ≠ó„ÄÇÂú®ÂÆû‰ΩìËØÜÂà´‰∏≠ÔºåÂèØ‰ª•ÂÜô`PER`„ÄÅ`LOC`„ÄÅ`ORG`Á≠â„ÄÇ\n    - Ê∑ªÂä†ËØ•Ê†áÁ≠æÂØπÂ∫îÁöÑÂø´Êç∑ÈîÆÔºà‰æãÂ¶ÇÁªô`PER`Ê†áÁ≠æËÆæÁΩÆ‰∏∫Âø´Êç∑ÈîÆ`p`ÔºâÔºåÂπ∂ÂÆö‰πâÊ†áÁ≠æÈ¢úËâ≤„ÄÇ\n\n    <div align=\"center\">\n    <img src=\"pics/doccano_create_label_cn.png\">\n    </div>\n\n    - ‰πãÂêé‰ª•ÂêåÊ†∑ÁöÑÊñπÊ≥ïÊ∑ªÂä†ÂÖ∂‰ªñÊâÄÈúÄË¶ÅÁöÑÊ†áÁ≠æÂç≥ÂèØ„ÄÇ\n\n- ‰ªªÂä°Ê†áÊ≥®\n    - Ê†áÊ≥®Êï∞ÊçÆÔºåÁÇπÂáªÊØèÊù°Êï∞ÊçÆÊúÄÂè≥ËæπÁöÑ`Annotate`ÊåâÈíÆÂºÄÂßãÊ†áÊ≥®„ÄÇ\n    - Á§∫‰æã‰∏≠ÂÆö‰πâ‰∫Ü‰∫∫Áâ©„ÄÅÂú∞ÁÇπ‰∏§Áßç`Span`Á±ªÂûãÊ†áÁ≠æ„ÄÇ\n\n    <div align=\"center\">\n    <img src=\"pics/doccano_annotate_cn.png\">\n    </div>\n\n#### ÂØºÂá∫ËÆ≠ÁªÉÊï∞ÊçÆ\n\n- Âú®Êï∞ÊçÆÈõÜ‰∏ÄÊ†èÁÇπÂáª`Êìç‰Ωú`„ÄÅ`ÂØºÂá∫Êï∞ÊçÆÈõÜ`ÂØºÂá∫Â∑≤Ê†áÊ≥®ÁöÑÊï∞ÊçÆ„ÄÇ\n- Ê†áÊ≥®Êï∞ÊçÆ‰øùÂ≠òÂú®Âêå‰∏Ä‰∏™ÊñáÊú¨Êñá‰ª∂‰∏≠ÔºåÊØèÊù°Ê†∑‰æãÂç†‰∏ÄË°å‰∏îÂ≠òÂÇ®‰∏∫`jsonl`Ê†ºÂºèÔºåÂÖ∂ÂåÖÂê´‰ª•‰∏ãÂ≠óÊÆµ\n    - `id`: Ê†∑Êú¨Âú®Êï∞ÊçÆÈõÜ‰∏≠ÁöÑÂîØ‰∏ÄÊ†áËØÜ`ID`„ÄÇ\n    - `text`: ÂéüÂßãÊñáÊú¨Êï∞ÊçÆ„ÄÇ\n    - `entities`: Êï∞ÊçÆ‰∏≠ÂåÖÂê´ÁöÑ`Span`Ê†áÁ≠æÔºåÊØè‰∏™`Span`Ê†áÁ≠æÂåÖÂê´Âõõ‰∏™Â≠óÊÆµÔºö\n        - `id`: `Span`Âú®Êï∞ÊçÆÈõÜ‰∏≠ÁöÑÂîØ‰∏ÄÊ†áËØÜID„ÄÇ\n        - `start_offset`: `Span`ÁöÑËµ∑Âßã‰ΩçÁΩÆ„ÄÇ\n        - `end_offset`: `Span`ÁªìÊùü‰ΩçÁΩÆÁöÑ‰∏ã‰∏Ä‰∏™‰ΩçÁΩÆ„ÄÇ\n        - `label`: `Span`ÁöÑÁ±ªÂûã„ÄÇ\n\n- ÂØºÂá∫Êï∞ÊçÆÁ§∫‰æã\n\n```json\n{\n    \"id\":8,\n    \"text\":\"ÊõæÁªèÂéªËøáÊ∑±Âú≥„ÄÅÁè†Êµ∑ÁöÑ‰ø≠Ê±§ÈùíÂπ¥ÂÜúÊ∞ëÂõû‰π°ÂêéÊÑèÂë≥Ê∑±ÈïøÂú∞ËØ¥Ôºå‰ªéÂçóÂæÄÂåóËµ∞ÔºåË∂äËµ∞Ë∂ä‰øùÂÆà„ÄÇ\",\n    \"entities\":[\n        {\n            \"id\":9,\n            \"label\":\"LOC\",\n            \"start_offset\":4,\n            \"end_offset\":6\n        },\n        {\n            \"id\":10,\n            \"label\":\"LOC\",\n            \"start_offset\":7,\n            \"end_offset\":9\n        },\n        {\n            \"id\":11,\n            \"label\":\"PER\",\n            \"start_offset\":12,\n            \"end_offset\":16\n        }\n    ],\n    \"relations\":[\n\n    ]\n}\n```\n\n- `DeepKE`‰∏≠ÂÆû‰ΩìËØÜÂà´‰ªªÂä°ËæìÂÖ•Êï∞ÊçÆÊ†ºÂºè‰∏∫`txt`Êñá‰ª∂ÔºåÊØèË°åÂåÖÊã¨ÂçïËØç„ÄÅÂàÜÈöîÁ¨¶„ÄÅÊ†áÁ≠æÔºàÂèØÂèÇËÄÉ`CoNLL`Êï∞ÊçÆÊ†ºÂºèÔºâ„ÄÇÂ∞ÜÂØºÂá∫ÂêéÁöÑÊï∞ÊçÆÈ¢ÑÂ§ÑÁêÜÊàê`DeepKE`ÂèØËæìÂÖ•ÁöÑÊ†ºÂºèÂç≥ÂèØËøõË°åËÆ≠ÁªÉÔºåÂÖ∑‰ΩìÊµÅÁ®ãËØ∑ËøõÂÖ•ËØ¶ÁªÜÁöÑREADME‰∏≠„ÄÇ\n    - [Â∏∏ËßÑÂÖ®ÁõëÁù£STANDARD](https://github.com/zjunlp/DeepKE/tree/main/example/ner/standard)\n\n### ÂÖ≥Á≥ªÊäΩÂèñ\n\n- [ÂàõÂª∫È°πÁõÆ](#ÂàõÂª∫È°πÁõÆ)\n    - ‰∏éÂÆû‰ΩìËØÜÂà´Êìç‰Ωú‰∏ÄËá¥ÔºåÂèÇËÄÉ‰ª•‰∏äÂç≥ÂèØ„ÄÇ\n- [Ê∑ªÂä†ËØ≠ÊñôÂ∫ì](#Ê∑ªÂä†ËØ≠ÊñôÂ∫ì)\n    - ‰∏éÂÆû‰ΩìËØÜÂà´Êìç‰Ωú‰∏ÄËá¥ÔºåÂèÇËÄÉ‰ª•‰∏äÂç≥ÂèØ„ÄÇ\n    - **ËæìÂÖ•ÊñáÊú¨‰∏≠ÔºåÊñáÊú¨Ê†ºÂºè`{ÊñáÊú¨}*{Â§¥ÂÆû‰Ωì}*{Â∞æÂÆû‰Ωì}*{Â§¥ÂÆû‰ΩìÁ±ªÂûã}*{Â∞æÂÆû‰ΩìÁ±ªÂûã}`ÔºåÂÖ∂‰∏≠Â§¥ÂíåÂ∞æÂÆû‰ΩìÁ±ªÂûãÂèØ‰∏∫Á©∫„ÄÇ**\n\n#### Êï∞ÊçÆÊ†áÊ≥®\n\n- Ê∑ªÂä†‰ªªÂä°Ê†áÁ≠æ\n    - ÊäΩÂèñÂºè‰ªªÂä°ÂåÖÂê´`Span`‰∏é`Relation`‰∏§ÁßçÊ†áÁ≠æÁ±ªÂûãÔºåÊ≠§Â§ÑÈááÁî®`Relation`Á±ªÂûãÔºå`Relation`ÊåáÂéüÊñáÊú¨‰∏≠`Span`‰πãÈó¥ÁöÑÂÖ≥Á≥ªÔºåÂç≥ÂÖ≥Á≥ªÊäΩÂèñ‰∏≠‰∏§‰∏™ÂÆû‰ΩìÈó¥ÁöÑÂÖ≥Á≥ª„ÄÇ\n    - Â°´ÂÖ•ÂÖ≥Á≥ªÁ±ªÂûãÁöÑÂêçÂ≠ó„ÄÇÂú®ÂÖ≥Á≥ªÊäΩÂèñ‰∏≠ÔºåÂèØ‰ª•ÂÜô`ÊØï‰∏ö`„ÄÅ`Âõ†Êûú`Á≠â„ÄÇ\n    - Ê∑ªÂä†ËØ•ÂÖ≥Á≥ªÁ±ªÂûãÂØπÂ∫îÁöÑÂø´Êç∑ÈîÆÔºà‰æãÂ¶ÇÁªô`ÊØï‰∏ö`Ê†áÁ≠æËÆæÁΩÆ‰∏∫Âø´Êç∑ÈîÆ`b`ÔºâÔºåÂπ∂ÂÆö‰πâÊ†áÁ≠æÈ¢úËâ≤„ÄÇ\n\n    <div align=\"center\">\n    <img src=\"pics/doccano_create_label_re_cn.png\">\n    </div>\n\n    - ‰πãÂêé‰ª•ÂêåÊ†∑ÁöÑÊñπÊ≥ïÊ∑ªÂä†ÂÖ∂‰ªñÊâÄÈúÄË¶ÅÁöÑÊ†áÁ≠æÂç≥ÂèØ„ÄÇ\n\n- ‰ªªÂä°Ê†áÊ≥®\n    - Ê†áÊ≥®Êï∞ÊçÆÔºåÁÇπÂáªÊØèÊù°Êï∞ÊçÆÊúÄÂè≥ËæπÁöÑ`Annotate`ÊåâÈíÆÂºÄÂßãÊ†áÊ≥®„ÄÇ\n    - È¶ñÂÖàÁÇπÂáªÂæÖÊ†áÊ≥®ÁöÑÂÖ≥Á≥ªÊ†áÁ≠æÔºåÊé•ÁùÄ‰æùÊ¨°ÁÇπÂáªÁõ∏Â∫îÁöÑÂ§¥Â∞æÂÆû‰ΩìÂèØÂÆåÊàêÂÖ≥Á≥ªÊ†áÊ≥®„ÄÇ\n    - Á§∫‰æã‰∏≠ÂÆö‰πâ‰∫Ü`PER`Âíå`LOC`‰∏§Áßç`Span`Á±ªÂûãÊ†áÁ≠æÔºå‰πãÂêéÊ†áÊ≥®ÂÆû‰ΩìÈó¥ÁöÑÂÖ≥Á≥ªÊ†áÁ≠æ`ÊØï‰∏ö`„ÄÇ`Relation`Ê†áÁ≠æÁî±`Subject`ÂØπÂ∫îÂÆû‰ΩìÊåáÂêë`Object`ÂØπÂ∫îÂÆû‰Ωì„ÄÇ\n\n    <div align=\"center\">\n    <img src=\"pics/doccano_annotate_re_cn.png\">\n    </div>\n\n#### ÂØºÂá∫ËÆ≠ÁªÉÊï∞ÊçÆ\n\n- Âú®Êï∞ÊçÆÈõÜ‰∏ÄÊ†èÁÇπÂáª`Êìç‰Ωú`„ÄÅ`ÂØºÂá∫Êï∞ÊçÆÈõÜ`ÂØºÂá∫Â∑≤Ê†áÊ≥®ÁöÑÊï∞ÊçÆ„ÄÇ\n- Ê†áÊ≥®Êï∞ÊçÆ‰øùÂ≠òÂú®Âêå‰∏Ä‰∏™ÊñáÊú¨Êñá‰ª∂‰∏≠ÔºåÊØèÊù°Ê†∑‰æãÂç†‰∏ÄË°å‰∏îÂ≠òÂÇ®‰∏∫`jsonl`Ê†ºÂºèÔºåÂÖ∂ÂåÖÂê´‰ª•‰∏ãÂ≠óÊÆµ\n    - `id`: Ê†∑Êú¨Âú®Êï∞ÊçÆÈõÜ‰∏≠ÁöÑÂîØ‰∏ÄÊ†áËØÜ`ID`„ÄÇ\n    - `text`: ÂéüÂßãÊñáÊú¨Êï∞ÊçÆ„ÄÇ\n    - `entities`: Êï∞ÊçÆ‰∏≠ÂåÖÂê´ÁöÑ`Span`Ê†áÁ≠æÔºåÊØè‰∏™`Span`Ê†áÁ≠æÂåÖÂê´Âõõ‰∏™Â≠óÊÆµÔºö\n        - `id`: `Span`Âú®Êï∞ÊçÆÈõÜ‰∏≠ÁöÑÂîØ‰∏ÄÊ†áËØÜID„ÄÇ\n        - `start_offset`: `Span`ÁöÑËµ∑Âßã‰ΩçÁΩÆ„ÄÇ\n        - `end_offset`: `Span`ÁªìÊùü‰ΩçÁΩÆÁöÑ‰∏ã‰∏Ä‰∏™‰ΩçÁΩÆ„ÄÇ\n        - `label`: `Span`ÁöÑÁ±ªÂûã„ÄÇ\n    - `relations`: Êï∞ÊçÆ‰∏≠ÂåÖÂê´ÁöÑ`Relation`Ê†áÁ≠æÔºåÊØè‰∏™`Relation`Ê†áÁ≠æÂåÖÂê´Âõõ‰∏™Â≠óÊÆµÔºö\n        - `id`: (`Span1`, `Relation`, `Span2`)‰∏âÂÖÉÁªÑÂú®Êï∞ÊçÆÈõÜ‰∏≠ÁöÑÂîØ‰∏ÄÊ†áËØÜ`ID`Ôºå‰∏çÂêåÊ†∑Êú¨‰∏≠ÁöÑÁõ∏Âêå‰∏âÂÖÉÁªÑÂØπÂ∫îÂêå‰∏Ä‰∏™`ID`„ÄÇ\n        - `from_id`: `Span1`ÂØπÂ∫îÁöÑÊ†áËØÜ`ID`„ÄÇ\n        - `to_id`: `Span2`ÂØπÂ∫îÁöÑÊ†áËØÜ`ID`„ÄÇ\n        - `type`: `Relation`Á±ªÂûã„ÄÇ\n\n- ÂØºÂá∫Êï∞ÊçÆÁ§∫‰æã\n\n```json\n{\n    \"id\":12,\n    \"text\":\"Âº†Âª∑Êô∫Ôºå ÊØï‰∏ö‰∫éÂ§ßËøûÂÜõÂåªÂ≠¶Èô¢Ôºå‰ªé‰∫ã‰∏≠ÂåªÊ≤ªÁñó‰∏≠ÊôöÊúüËÇøÁò§‰∏¥Â∫äÂ∑•‰Ωú30‰ΩôÂπ¥*Âº†Âª∂Êô∫*Â§ßËøûÂÜõÂåªÂ≠¶Èô¢*‰∫∫Áâ©*Â≠¶Ê†°\",\n    \"entities\":[\n        {\n            \"id\":18,\n            \"label\":\"PER\",\n            \"start_offset\":0,\n            \"end_offset\":3\n        },\n        {\n            \"id\":19,\n            \"label\":\"ORG\",\n            \"start_offset\":8,\n            \"end_offset\":14\n        }\n    ],\n    \"relations\":[\n        {\n            \"id\":1,\n            \"from_id\":18,\n            \"to_id\":19,\n            \"type\":\"ÊØï‰∏ö\"\n        }\n    ]\n}\n```\n\n## Ëá™Âä®Êï∞ÊçÆÊ†áÊ≥®\n\n### ÂÆû‰ΩìËØÜÂà´\n\n‰∏∫‰∫ÜÁî®Êà∑Êõ¥Â•ΩÁöÑ‰ΩøÁî®`DeepKE`ÂÆåÊàêÂÆû‰ΩìËØÜÂà´‰ªªÂä°ÔºåÊàë‰ª¨Êèê‰æõ‰∏Ä‰∏™ÁÆÄÂçïÊòìÁî®ÁöÑ**Âü∫‰∫éËØçÂÖ∏ÂåπÈÖç**ÁöÑÂÆû‰ΩìËØÜÂà´**Ëá™Âä®Ê†áÊ≥®Â∑•ÂÖ∑**„ÄÇ\n\n#### ËØçÂÖ∏\n- ËØçÂÖ∏Ê†ºÂºèÂ¶Ç‰∏ãÊâÄÁ§∫Ôºö\n    <h3 align=\"left\">\n        <img src=\"https://github.com/zjunlp/DeepKE/blob/main/example/ner/prepare-data/pics/vocab_dict.png?raw=true\", width=375>\n    </h3>\n- È¢ÑÊèê‰æõ‰∫Ü‰∏§‰∏™ÂÆû‰ΩìËØçÂÖ∏Ôºà‰∏≠Ëã±ÊñáÂêÑ‰∏Ä‰∏™ÔºâÔºå‰ΩøÁî®ÂÆû‰ΩìËØçÂÖ∏+jiebaËØçÊÄßÊ†áÊ≥®ÂØπÊ†∑Êú¨ËøõË°åËá™Âä®Ê†áÊ≥®„ÄÇ\n\n    - ‰∏≠ÊñáÁ§∫‰æãËØçÂÖ∏‰∏≠, Êàë‰ª¨ÈááÁî®[(‰∫∫Ê∞ëÊó•Êä•)Êï∞ÊçÆÈõÜ](https://github.com/OYE93/Chinese-NLP-Corpus/tree/master/NER/People's%20Daily). ÂÆÉÊòØNERÁõ∏ÂÖ≥ÁöÑÊï∞ÊçÆÈõÜÔºåÂåÖÂê´‰∫∫Âëò (PER)„ÄÅ‰ΩçÁΩÆ (LOC) ÂíåÁªÑÁªá (ORG) Áõ∏ÂÖ≥ÁöÑÂëΩÂêçÂÆû‰ΩìËØÜÂà´„ÄÇ\n\n    - Ëã±ÊñáÁ§∫‰æãËØçÂÖ∏‰∏≠ÔºåÊàë‰ª¨ÈááÁî®ConllÊï∞ÊçÆÈõÜ„ÄÇÂÆÉÂåÖÂê´‰∫∫Âëò (PER)„ÄÅ‰ΩçÁΩÆ (LOC) ÂíåÂÖ∂‰ªñ (MISC) Áõ∏ÂÖ≥ÁöÑÂëΩÂêçÂÆû‰ΩìËØÜÂà´„ÄÇ‰Ω†ÂèØÈÄöËøáÂ¶Ç‰∏ãÂëΩ‰ª§Ëé∑ÂæóConllÊï∞ÊçÆÈõÜ\n\n    ```shell\n    wget 120.27.214.45/Data/ner/few_shot/data.tar.gz\n    ```\n\n    - È¢ÑÊèê‰æõËØçÂÖ∏Google Drive‰∏ãËΩΩÈìæÊé•Ôºö \n        - [‰∏≠Êñá(vocab_dict_cn), Ëã±Êñá(vocab_dict_en)](https://drive.google.com/drive/folders/1PGANizeTsvEQFYTL8O1jrDLZwk_MPqO0?usp=sharing)\n    - ÁôæÂ∫¶ÁΩëÁõò‰∏ãËΩΩÈìæÊé•Ôºö \n        - [‰∏≠Êñá(vocab_dict_cn), Ëã±Êñá(vocab_dict_en)](https://pan.baidu.com/s/1a07W42ZByeZ00MZp5pZgxg) \n        - ÊèêÂèñÁ†Å(x7ba)\n\n- **Ëã•ÈúÄË¶ÅÊûÑÂª∫È¢ÜÂüüËá™Âª∫ËØçÂÖ∏ÔºåËØ∑ÂèÇÁÖßÈ¢ÑÊèê‰æõËØçÂÖ∏Ê†ºÂºè(csv)**\n\n    | ÂÆû‰Ωì | ËØçÊÄß |\n    |  --------  | ------  |\n    |  Êù≠Â∑û  | LOC  |\n    |  ...  | ...  |\n\n#### Ê∫êÊñá‰ª∂\n\n- **ËæìÂÖ•ÁöÑËØçÂÖ∏**Ê†ºÂºè‰∏∫`csv`ÔºàÂåÖÂê´‰∏§ÂàóÔºåÂàÜÂà´ÊòØÂÆû‰Ωì‰ª•ÂèäÂØπÂ∫îÁöÑÊ†áÁ≠æÔºâ„ÄÇ\n\n- **ÂæÖËá™Âä®ÊâìÊ†áÁöÑÊï∞ÊçÆ**ÔºàtxtÊ†ºÂºèÊåâË°åÂàÜÈöîÔºåÂ¶Ç‰∏ãÂõæÊâÄÁ§∫ÔºâÂ∫îÊîæÂú®`source_data`Ë∑ØÂæÑ‰∏ãÔºåËÑöÊú¨‰ºöÈÅçÂéÜÊ≠§Êñá‰ª∂Â§π‰∏ãÁöÑÊâÄÊúâtxtÊ†ºÂºèÁöÑÊñá‰ª∂ÔºåÈÄêË°åËøõË°åËá™Âä®ÊâìÊ†á„ÄÇÂÖ∑‰ΩìÁ§∫‰æãÂ¶Ç‰∏ãÔºö\n    <h3 align=\"left\">\n        <img src=\"https://github.com/zjunlp/DeepKE/blob/main/example/ner/prepare-data/pics/input_data_format.png?raw=true\", width=700>\n    </h3>\n\n\n#### ËæìÂá∫Êñá‰ª∂\n\n- ËæìÂá∫Êñá‰ª∂ÂåÖÂê´‰∏â‰∏™Ôºö`example_train_cn.txt`, `example_dev_cn.txt`, `example_test_cn.txt`ÔºåÂàÜÂà´ÂØπÂ∫îËÆ≠ÁªÉÈõÜ„ÄÅÈ™åËØÅÈõÜÂíåÊµãËØïÈõÜ„ÄÇÊàë‰ª¨Ëá™Âä®Â∞ÜÊ∫êÊñá‰ª∂Êï∞ÊçÆÂàíÂàÜ‰∏∫‰∏â‰ªΩÔºåÊØî‰æã‰∏∫`0.8:0.1:0.1`„ÄÇËæìÂá∫Êñá‰ª∂ÁöÑÊ†ºÂºèÂ¶Ç‰∏ãÔºö\n    <h3 align=\"left\">\n        <img src=\"https://github.com/zjunlp/DeepKE/blob/main/example/ner/prepare-data/pics/output_data_format.png?raw=true\", width=375>\n    </h3>\n\n#### ÁéØÂ¢É\nËøêË°åÁéØÂ¢É:  \n- jieba = 0.42.1\n\n#### ÂèÇÊï∞Ëß£Èáä\n\n- `language`: ÂèØÈÄâcn(‰∏≠Êñá)Êàñen(Ëã±Êñá)\n- `source_dir`: ËØ≠ÊñôÂ∫ìË∑ØÂæÑÔºàÈÅçÂéÜÊ≠§Êñá‰ª∂Â§π‰∏ãÁöÑÊâÄÊúâtxtÊ†ºÂºèÁöÑÊñá‰ª∂ÔºåÈÄêË°åËøõË°åËá™Âä®ÊâìÊ†áÔºåÈªòËÆ§‰∏∫`source_data`Ôºâ\n- `dict_dir`: ÂÆû‰ΩìËØçÂÖ∏Ë∑ØÂæÑÔºàÈªòËÆ§‰∏∫`vocab_dict.csv`Ôºâ\n- `test_rate, dev_rate, test_rate`: ËÆ≠ÁªÉÈõÜ„ÄÅÈ™åËØÅÈõÜ„ÄÅÊµãËØïÈõÜÂç†ÊØîÔºàËØ∑Á°Æ‰øùÊÄªÂíå‰∏∫`1`ÔºåÈªòËÆ§`0.8:0.1:0.1`Ôºâ\n\n#### ËøêË°å\n\n- **‰∏≠Êñá**\n```bash\npython prepare_weaksupervised_data.py --language cn --dict_dir vocab_dict_cn.csv\n```\n\n- **Ëã±Êñá**\n```bash\npython prepare_weaksupervised_data.py --language en --dict_dir vocab_dict_en.csv\n```\n\n\n### ÂÖ≥Á≥ªÊäΩÂèñ\n\n‰∏∫‰∫ÜÁî®Êà∑Êõ¥Â•ΩÁöÑ‰ΩøÁî®`DeepKE`ÂÆåÊàêÂÖ≥Á≥ªÊäΩÂèñ‰ªªÂä°ÔºåÊàë‰ª¨Êèê‰æõ‰∏Ä‰∏™ÁÆÄÂçïÊòìÁî®ÁöÑÂü∫‰∫éËøúÁ®ãÁõëÁù£ÁöÑÂÖ≥Á≥ªÊ†áÊ≥®Â∑•ÂÖ∑„ÄÇ\n\n#### Ê∫êÊñá‰ª∂\n\nÁî®Êà∑Êèê‰æõÁöÑÊ∫êÊñá‰ª∂ÈúÄË¶Å‰∏∫`.json`ÂΩ¢ÂºèÔºåÂπ∂‰∏îÊØèÊù°Êï∞ÊçÆÂè™ÂåÖÂê´‰∏Ä‰∏™ÂÆû‰ΩìÂØπÔºåÂàÜÂà´‰∏∫Â§¥ÂÆû‰ΩìÂíåÂ∞æÂÆû‰Ωì„ÄÇÊï∞ÊçÆ‰∏≠ÂøÖÈ°ªËá≥Â∞ëÂåÖÂê´‰ª•‰∏ãÂõõ‰∏™Â≠óÊÆµÔºösentence(Âè•Â≠ê), head(Â§¥ÂÆû‰Ωì), tail(Â∞æÂÆû‰Ωì), head_offset(Â§¥ÂÆû‰ΩìÂú®Âè•Â≠ê‰∏≠ÁöÑÂÅèÁßª‰ΩçÁΩÆ), tail_offset(Â∞æÂÆû‰ΩìÂú®Âè•Â≠ê‰∏≠ÁöÑÂÅèÁßª‰ΩçÁΩÆ)„ÄÇÂÖ∑‰ΩìÁöÑjsonÂ≠óÂÖ∏ÂΩ¢ÂºèÂ¶Ç‰∏ãÔºö\n\n```json\n[\n  {\n    \"sentence\": \"Â¶Ç‰ΩïÊºîÂ•ΩËá™Â∑±ÁöÑËßíËâ≤ÔºåËØ∑ËØª„ÄäÊºîÂëòËá™Êàë‰øÆÂÖª„Äã„ÄäÂñúÂâß‰πãÁéã„ÄãÂë®ÊòüÈ©∞Â¥õËµ∑‰∫éÁ©∑Âõ∞ÊΩ¶ÂÄí‰πã‰∏≠ÁöÑÁã¨Èó®ÁßòÁ¨à\",\n    \"head\": \"Âë®ÊòüÈ©∞\",\n    \"tail\": \"ÂñúÂâß‰πãÁéã\",\n    \"head_offset\": \"26\",\n    \"tail_offset\": \"21\",\n    //...\n\t},\n  //...\n]\n```\n\n#### ‰∏âÂÖÉÁªÑÊñá‰ª∂\n\nÊ∫êÊñá‰ª∂‰∏≠ÁöÑÂÆû‰ΩìÂØπÂ∞Ü‰ºö‰∏é‰∏âÂÖÉÁªÑÊñá‰ª∂‰∏≠ÁöÑ‰∏âÂÖÉÁªÑËøõË°åÂ≠óÁ¨¶‰∏≤ÂÖ®ÂåπÈÖç„ÄÇÂ¶ÇÊûúÂåπÈÖçÊàêÂäüÂÆû‰ΩìÂØπÂ∞Ü‰ºöË¢´Ê†áËÆ∞‰∏∫‰∏âÂÖÉÁªÑÁöÑÂÖ≥Á≥ªÊ†áÁ≠æÔºåÂ¶ÇÊûúÊ≤°ÊúâÂåπÈÖçÈ°πÂàôË¢´Ê†áËÆ∞‰∏∫`None`„ÄÇ\n\nÊàë‰ª¨ÂàÜËß£Êèê‰æõ‰∫Ü‰∏Ä‰∏™[Ëã±Êñá](https://drive.google.com/drive/folders/1HHkm3RBI3okiu8jGs-Wn0vP_-0hz7pb2?usp=sharing)Âíå‰∏Ä‰∏™[‰∏≠Êñá](https://drive.google.com/file/d/1YpaMpivodG39p53MM9sMpB41q4EoiQhH/view?usp=sharing)‰∏âÂÖÉÁªÑÊñá‰ª∂„ÄÇËã±Êñá‰∏âÂÖÉÁªÑÊù•Ëá™`NYT`Êï∞ÊçÆÈõÜÔºåÂåÖÂê´Â¶Ç‰∏ãÂÖ≥Á≥ªÁ±ªÂûãÔºö\n\n```python\n\"/business/company/place_founded\",\n\"/people/person/place_lived\",\n\"/location/country/administrative_divisions\",\n\"/business/company/major_shareholders\",\n\"/sports/sports_team_location/teams\",\n\"/people/person/religion\",\n\"/people/person/place_of_birth\",\n\"/people/person/nationality\",\n\"/location/country/capital\",\n\"/business/company/advisors\",\n\"/people/deceased_person/place_of_death\",\n\"/business/company/founders\",\n\"/location/location/contains\",\n\"/people/person/ethnicity\",\n\"/business/company_shareholder/major_shareholder_of\",\n\"/people/ethnicity/geographic_distribution\",\n\"/people/person/profession\",\n\"/business/person/company\",\n\"/people/person/children\",\n\"/location/administrative_division/country\",\n\"/people/ethnicity/people\",\n\"/sports/sports_team/location\",\n\"/location/neighborhood/neighborhood_of\",\n\"/business/company/industry\"\n```\n\n‰∏≠Êñá‰∏âÂÖÉÁªÑÊù•Ëá™GitHub[ÈìæÊé•](https://github.com/DannyLee1991/ExtractTriples)ÔºåÂåÖÂê´Â¶Ç‰∏ãÂÖ≥Á≥ªÁ±ªÂûãÔºö\n\n```json\n{\"object_type\": \"Âú∞ÁÇπ\", \"predicate\": \"Á•ñÁ±ç\", \"subject_type\": \"‰∫∫Áâ©\"}\n{\"object_type\": \"‰∫∫Áâ©\", \"predicate\": \"Áà∂‰∫≤\", \"subject_type\": \"‰∫∫Áâ©\"}\n{\"object_type\": \"Âú∞ÁÇπ\", \"predicate\": \"ÊÄªÈÉ®Âú∞ÁÇπ\", \"subject_type\": \"‰ºÅ‰∏ö\"}\n{\"object_type\": \"Âú∞ÁÇπ\", \"predicate\": \"Âá∫ÁîüÂú∞\", \"subject_type\": \"‰∫∫Áâ©\"}\n{\"object_type\": \"ÁõÆ\", \"predicate\": \"ÁõÆ\", \"subject_type\": \"ÁîüÁâ©\"}\n{\"object_type\": \"Number\", \"predicate\": \"Èù¢ÁßØ\", \"subject_type\": \"Ë°åÊîøÂå∫\"}\n{\"object_type\": \"Text\", \"predicate\": \"ÁÆÄÁß∞\", \"subject_type\": \"Êú∫ÊûÑ\"}\n{\"object_type\": \"Date\", \"predicate\": \"‰∏äÊò†Êó∂Èó¥\", \"subject_type\": \"ÂΩ±ËßÜ‰ΩúÂìÅ\"}\n{\"object_type\": \"‰∫∫Áâ©\", \"predicate\": \"Â¶ªÂ≠ê\", \"subject_type\": \"‰∫∫Áâ©\"}\n{\"object_type\": \"Èü≥‰πê‰∏ìËæë\", \"predicate\": \"ÊâÄÂ±û‰∏ìËæë\", \"subject_type\": \"Ê≠åÊõ≤\"}\n{\"object_type\": \"Number\", \"predicate\": \"Ê≥®ÂÜåËµÑÊú¨\", \"subject_type\": \"‰ºÅ‰∏ö\"}\n{\"object_type\": \"ÂüéÂ∏Ç\", \"predicate\": \"È¶ñÈÉΩ\", \"subject_type\": \"ÂõΩÂÆ∂\"}\n{\"object_type\": \"‰∫∫Áâ©\", \"predicate\": \"ÂØºÊºî\", \"subject_type\": \"ÂΩ±ËßÜ‰ΩúÂìÅ\"}\n{\"object_type\": \"Text\", \"predicate\": \"Â≠ó\", \"subject_type\": \"ÂéÜÂè≤‰∫∫Áâ©\"}\n{\"object_type\": \"Number\", \"predicate\": \"Ë∫´È´ò\", \"subject_type\": \"‰∫∫Áâ©\"}\n{\"object_type\": \"‰ºÅ‰∏ö\", \"predicate\": \"Âá∫ÂìÅÂÖ¨Âè∏\", \"subject_type\": \"ÂΩ±ËßÜ‰ΩúÂìÅ\"}\n{\"object_type\": \"Number\", \"predicate\": \"‰øÆ‰∏öÂπ¥Èôê\", \"subject_type\": \"Â≠¶Áßë‰∏ì‰∏ö\"}\n{\"object_type\": \"Date\", \"predicate\": \"Âá∫ÁîüÊó•Êúü\", \"subject_type\": \"‰∫∫Áâ©\"}\n{\"object_type\": \"‰∫∫Áâ©\", \"predicate\": \"Âà∂Áâá‰∫∫\", \"subject_type\": \"ÂΩ±ËßÜ‰ΩúÂìÅ\"}\n{\"object_type\": \"‰∫∫Áâ©\", \"predicate\": \"ÊØç‰∫≤\", \"subject_type\": \"‰∫∫Áâ©\"}\n{\"object_type\": \"‰∫∫Áâ©\", \"predicate\": \"ÁºñÂâß\", \"subject_type\": \"ÂΩ±ËßÜ‰ΩúÂìÅ\"}\n{\"object_type\": \"ÂõΩÂÆ∂\", \"predicate\": \"ÂõΩÁ±ç\", \"subject_type\": \"‰∫∫Áâ©\"}\n{\"object_type\": \"Number\", \"predicate\": \"Êµ∑Êãî\", \"subject_type\": \"Âú∞ÁÇπ\"}\n{\"object_type\": \"ÁΩëÁ´ô\", \"predicate\": \"ËøûËΩΩÁΩëÁ´ô\", \"subject_type\": \"ÁΩëÁªúÂ∞èËØ¥\"}\n{\"object_type\": \"‰∫∫Áâ©\", \"predicate\": \"‰∏àÂ§´\", \"subject_type\": \"‰∫∫Áâ©\"}\n{\"object_type\": \"Text\", \"predicate\": \"Êúù‰ª£\", \"subject_type\": \"ÂéÜÂè≤‰∫∫Áâ©\"}\n{\"object_type\": \"Text\", \"predicate\": \"Ê∞ëÊóè\", \"subject_type\": \"‰∫∫Áâ©\"}\n{\"object_type\": \"Text\", \"predicate\": \"Âè∑\", \"subject_type\": \"ÂéÜÂè≤‰∫∫Áâ©\"}\n{\"object_type\": \"Âá∫ÁâàÁ§æ\", \"predicate\": \"Âá∫ÁâàÁ§æ\", \"subject_type\": \"‰π¶Á±ç\"}\n{\"object_type\": \"‰∫∫Áâ©\", \"predicate\": \"‰∏ªÊåÅ‰∫∫\", \"subject_type\": \"ÁîµËßÜÁªºËâ∫\"}\n{\"object_type\": \"Text\", \"predicate\": \"‰∏ì‰∏ö‰ª£Á†Å\", \"subject_type\": \"Â≠¶Áßë‰∏ì‰∏ö\"}\n{\"object_type\": \"‰∫∫Áâ©\", \"predicate\": \"Ê≠åÊâã\", \"subject_type\": \"Ê≠åÊõ≤\"}\n{\"object_type\": \"‰∫∫Áâ©\", \"predicate\": \"‰ΩúËØç\", \"subject_type\": \"Ê≠åÊõ≤\"}\n{\"object_type\": \"‰∫∫Áâ©\", \"predicate\": \"‰∏ªËßí\", \"subject_type\": \"ÁΩëÁªúÂ∞èËØ¥\"}\n{\"object_type\": \"‰∫∫Áâ©\", \"predicate\": \"Ëë£‰∫ãÈïø\", \"subject_type\": \"‰ºÅ‰∏ö\"}\n{\"object_type\": \"Date\", \"predicate\": \"ÊàêÁ´ãÊó•Êúü\", \"subject_type\": \"Êú∫ÊûÑ\"}\n{\"object_type\": \"Â≠¶Ê†°\", \"predicate\": \"ÊØï‰∏öÈô¢Ê†°\", \"subject_type\": \"‰∫∫Áâ©\"}\n{\"object_type\": \"Number\", \"predicate\": \"Âç†Âú∞Èù¢ÁßØ\", \"subject_type\": \"Êú∫ÊûÑ\"}\n{\"object_type\": \"ËØ≠Ë®Ä\", \"predicate\": \"ÂÆòÊñπËØ≠Ë®Ä\", \"subject_type\": \"ÂõΩÂÆ∂\"}\n{\"object_type\": \"Text\", \"predicate\": \"ÈÇÆÊîøÁºñÁ†Å\", \"subject_type\": \"Ë°åÊîøÂå∫\"}\n{\"object_type\": \"Number\", \"predicate\": \"‰∫∫Âè£Êï∞Èáè\", \"subject_type\": \"Ë°åÊîøÂå∫\"}\n{\"object_type\": \"ÂüéÂ∏Ç\", \"predicate\": \"ÊâÄÂú®ÂüéÂ∏Ç\", \"subject_type\": \"ÊôØÁÇπ\"}\n{\"object_type\": \"‰∫∫Áâ©\", \"predicate\": \"‰ΩúËÄÖ\", \"subject_type\": \"Âõæ‰π¶‰ΩúÂìÅ\"}\n{\"object_type\": \"Date\", \"predicate\": \"ÊàêÁ´ãÊó•Êúü\", \"subject_type\": \"‰ºÅ‰∏ö\"}\n{\"object_type\": \"‰∫∫Áâ©\", \"predicate\": \"‰ΩúÊõ≤\", \"subject_type\": \"Ê≠åÊõ≤\"}\n{\"object_type\": \"Ê∞îÂÄô\", \"predicate\": \"Ê∞îÂÄô\", \"subject_type\": \"Ë°åÊîøÂå∫\"}\n{\"object_type\": \"‰∫∫Áâ©\", \"predicate\": \"ÂòâÂÆæ\", \"subject_type\": \"ÁîµËßÜÁªºËâ∫\"}\n{\"object_type\": \"‰∫∫Áâ©\", \"predicate\": \"‰∏ªÊºî\", \"subject_type\": \"ÂΩ±ËßÜ‰ΩúÂìÅ\"}\n{\"object_type\": \"‰ΩúÂìÅ\", \"predicate\": \"ÊîπÁºñËá™\", \"subject_type\": \"ÂΩ±ËßÜ‰ΩúÂìÅ\"}\n{\"object_type\": \"‰∫∫Áâ©\", \"predicate\": \"ÂàõÂßã‰∫∫\", \"subject_type\": \"‰ºÅ‰∏ö\"}\n```\n\nÁî®Êà∑ÂèØ‰ª•Ëá™ÂÆö‰πâ‰∏âÂÖÉÁªÑÊñá‰ª∂Ôºå‰ΩÜÊòØÈúÄË¶ÅËÆæÂÆöÊñá‰ª∂ÂΩ¢Âºè‰∏∫`.csv`Âπ∂ÂÖ∑ÊúâÂ¶Ç‰∏ãÊ†ºÂºèÔºö\n\n| Â§¥ÂÆû‰Ωì | Â∞æÂÆû‰Ωì   | ÂÖ≥Á≥ª |\n| ------ | -------- | ---- |\n| Âë®ÊòüÈ©∞ | ÂñúÂâß‰πãÁéã | ‰∏ªÊºî |\n| ...    | ...      | ...  |\n\n#### ËæìÂá∫Êñá‰ª∂\n\nËæìÂá∫Êñá‰ª∂ÂåÖÂê´‰∏â‰∏™Ôºö`labeled_train.json`, `labeled_dev.json`, `labeled_test.json`ÔºåÂàÜÂà´ÂØπÂ∫îËÆ≠ÁªÉÈõÜ„ÄÅÈ™åËØÅÈõÜÂíåÊµãËØïÈõÜ„ÄÇÊàë‰ª¨Ëá™Âä®Â∞ÜÊ∫êÊñá‰ª∂Êï∞ÊçÆÂàíÂàÜ‰∏∫‰∏â‰ªΩÔºåÊØî‰æã‰∏∫0.8:0.1:0.1„ÄÇËæìÂá∫Êñá‰ª∂ÁöÑÊ†ºÂºèÂ¶Ç‰∏ãÔºö\n\n```json\n[\n\t{\n    \"sentence\": \"Â¶Ç‰ΩïÊºîÂ•ΩËá™Â∑±ÁöÑËßíËâ≤ÔºåËØ∑ËØª„ÄäÊºîÂëòËá™Êàë‰øÆÂÖª„Äã„ÄäÂñúÂâß‰πãÁéã„ÄãÂë®ÊòüÈ©∞Â¥õËµ∑‰∫éÁ©∑Âõ∞ÊΩ¶ÂÄí‰πã‰∏≠ÁöÑÁã¨Èó®ÁßòÁ¨à\",\n    \"head\": \"Âë®ÊòüÈ©∞\",\n    \"tail\": \"ÂñúÂâß‰πãÁéã\",\n    \"head_offset\": \"26\",\n    \"tail_offset\": \"21\",\n    \"relation\": \"‰∏ªÊºî\",\n    //...\n\t},\n  //...\n]\n```\n\n#### ÂèÇÊï∞ÊèèËø∞\n\n- `language`: 'en'ÂØπÂ∫îËã±ÊñáÊï∞ÊçÆÔºå'cn'ÂØπÂ∫î‰∏≠ÊñáÊï∞ÊçÆ\n- `source_file`: ÈúÄË¶ÅÊ†áËÆ∞ÁöÑÊ∫êÊñá‰ª∂\n- `triple_file`: ‰∏âÂÖÉÁªÑÊñá‰ª∂\n- `test_rate, dev_rate, test_rate`:  ËÆ≠ÁªÉÈõÜ„ÄÅÈ™åËØÅÈõÜÂíåÊµãËØïÈõÜÁöÑÂàíÂàÜÊØî‰æãÔºåÈúÄË¶Å‰øùËØÅÊØî‰æã‰πãÂíå‰∏∫1ÔºåÈªòËÆ§‰∏∫0.8:0.1:0.1„ÄÇ\n\n#### ËøêË°å\n\n```bash\npython ds_label_data.py --language en --source_file source_data.json --triple_file triple_file.csv\n```\n\n\n\n## FAQ\n\n- **Q: ÊàëÈúÄË¶ÅÊ†áÊ≥®Â§öÂ∞ëÊï∞ÊçÆ**\n    - A: Ê†áÊ≥®ÊòØ‰∏Ä‰∏™ËÄóË¥π‰∫∫Âäõ‰∏éÊó∂Èó¥ÁöÑËøáÁ®ãÔºåÊàêÊú¨ÈùûÂ∏∏Â§ß„ÄÇ‰ªéÁêÜÊÉ≥ËßíÂ∫¶ÁúãÊ†áÊ≥®ÁöÑÊï∞ÊçÆÊï∞ÈáèË∂äÂ§öÔºåËÆ≠ÁªÉÂæóÂà∞ÁöÑÊ®°ÂûãÊïàÊûú‰πü‰ºöË∂äÂ•Ω„ÄÇ‰ΩÜÊòØÂÆûÈôÖÊÉÖÂÜµÂæÄÂæÄÂπ∂‰∏çÂÖÅËÆ∏„ÄÇ‰∏ÄÊñπÈù¢ÔºåÈúÄË¶ÅÁªìÂêàÂÆûÈôÖÁöÑËµÑÊ∫ê‰∏éÊó∂Èó¥ÔºõÂè¶‰∏ÄÊñπÈù¢ÔºåË¶ÅËÄÉËôëÊï∞ÊçÆÈáèÂ¢ûÂä†ÂØπÊ®°ÂûãÊïàÊûúÊèêÂçáÂ∏¶Êù•ÁöÑÂΩ±Âìç„ÄÇÂú®ÂÆû‰ΩìËØÜÂà´„ÄÅÂÖ≥Á≥ªÊäΩÂèñ‰∏≠ÈÄöÂ∏∏ÈúÄË¶Å`10K`Â∑¶Âè≥ÁöÑÊï∞ÊçÆÈáè„ÄÇ\n\n\n- **Q: ËØ∑ÈóÆÊúâÊ†áÊ≥®Â•ΩÁöÑÊï∞ÊçÆÊèê‰æõÂêó**\n    - A: Âú®ÂÆû‰ΩìËØÜÂà´‰∏≠ÔºåÈ¢ÑÊèê‰æõ‰∫Ü‰∏§‰∏™ÂÆû‰ΩìËØçÂÖ∏Ôºà‰∏≠Ëã±ÊñáÂêÑ‰∏Ä‰∏™ÔºâÔºåËÉΩÂ§üÂØπÊ†∑Êú¨ËøõË°åËá™Âä®Ê†áÊ≥®„ÄÇ\n\n- **Q: Ëá™Âä®Ê†áÊ≥®Êï∞ÊçÆËÆ≠Âá∫Êù•ÁöÑÊ®°ÂûãÊïàÊûú‰∏çË°å**\n    - A: Ê®°ÂûãÁöÑÊïàÊûúÂ∑ÆÂèØËÉΩÊúâ‰ª•‰∏ãÂá†‰∏™ÂéüÂõ†Ôºö\n        - Ëá™Âä®Ê†áÊ≥®„ÄÅ‰∫∫Â∑•Ê†áÊ≥®ÁöÑÊï∞ÊçÆÂê´ÊúâËæÉÂ§öÁöÑÂô™Â£∞Ôºå\n        - Êï∞ÊçÆÈáèËæÉÂ∞èÔºöÂèÇÊï∞ÈáèÂæàÂ§öÁöÑÂ§ßÊ®°ÂûãÂú®Â∞èÊï∞ÊçÆÈõÜ‰∏äËøáÊãüÂêà\n    - solutionÔºö\n        - ÂèØ‰ª•ËÄÉËôëÊ£ÄÊü•Êï∞ÊçÆÁöÑË¥®Èáè\n        - Âà©Áî®**ÂçäÁõëÁù£**ËÆ≠ÁªÉÊàñËÄÖ`Self-Traning`Á≠âÁ≠ñÁï•\n        - ‰ΩøÁî®Êï∞ÊçÆÂ¢ûÂº∫Â¢ûÂ§ßÊï∞ÊçÆÈáè\n\n## ÂºïÁî®\n\nÂ¶ÇÊûúÊú¨È°πÁõÆ‰∏≠ÁöÑËµÑÊ∫êÊàñÊäÄÊúØÂØπ‰Ω†ÁöÑÁ†îÁ©∂Â∑•‰ΩúÊúâÊâÄÂ∏ÆÂä©ÔºåÊ¨¢ËøéÂú®ËÆ∫Êñá‰∏≠ÂºïÁî®‰∏ãËø∞ËÆ∫Êñá„ÄÇ\n\n```\n@article{zhang2022deepke,\n  title={DeepKE: A Deep Learning Based Knowledge Extraction Toolkit for Knowledge Base Population},\n  author={Zhang, Ningyu and Xu, Xin and Tao, Liankuan and Yu, Haiyang and Ye, Hongbin and Qiao, Shuofei and Xie, Xin and Chen, Xiang and Li, Zhoubo and Li, Lei and Liang, Xiaozhuan and others},\n  journal={arXiv preprint arXiv:2201.03335},\n  year={2022}\n}\n```\n\n## ÂÖçË¥£Â£∞Êòé\n\n**ËØ•È°πÁõÆ‰∏≠ÁöÑÂÜÖÂÆπ‰ªÖ‰æõÊäÄÊúØÁ†îÁ©∂ÂèÇËÄÉÔºå‰∏ç‰Ωú‰∏∫‰ªª‰ΩïÁªìËÆ∫ÊÄß‰æùÊçÆ„ÄÇ‰ΩøÁî®ËÄÖÂèØ‰ª•Âú®ËÆ∏ÂèØËØÅËåÉÂõ¥ÂÜÖ‰ªªÊÑè‰ΩøÁî®ËØ•Ê®°ÂûãÔºå‰ΩÜÊàë‰ª¨‰∏çÂØπÂõ†‰ΩøÁî®ËØ•È°πÁõÆÂÜÖÂÆπÈÄ†ÊàêÁöÑÁõ¥Êé•ÊàñÈó¥Êé•ÊçüÂ§±Ë¥üË¥£„ÄÇ**\n\n## ÈóÆÈ¢òÂèçÈ¶à\n\nÂ¶ÇÊúâÈóÆÈ¢òÔºåËØ∑Âú®GitHub Issue‰∏≠Êèê‰∫§„ÄÇ\n"
        },
        {
          "name": "docker",
          "type": "tree",
          "content": null
        },
        {
          "name": "docs",
          "type": "tree",
          "content": null
        },
        {
          "name": "example",
          "type": "tree",
          "content": null
        },
        {
          "name": "pics",
          "type": "tree",
          "content": null
        },
        {
          "name": "pretrained",
          "type": "tree",
          "content": null
        },
        {
          "name": "requirements.txt",
          "type": "blob",
          "size": 0.373046875,
          "content": "torch>=1.5,<=1.11\nhydra-core==1.0.6\ntensorboard==2.4.1\nmatplotlib==3.4.1\ntransformers==4.26.0\njieba==0.42.1\nscikit-learn==0.24.1\nseqeval==1.2.2\nopt-einsum==3.3.0\nwandb==0.12.7\nujson==5.6.0\nhuggingface_hub==0.11.0\ntensorboardX==2.5.1\nnltk==3.8\nprotobuf==3.20.1\nnumpy==1.21.0\nipdb==0.13.11\npytorch-crf==0.7.2\ntqdm==4.66.1\nopenai==0.28.0\nJinja2==3.1.2\ndatasets==2.13.2\npyhocon==0.3.60\n"
        },
        {
          "name": "setup.py",
          "type": "blob",
          "size": 0.9091796875,
          "content": "from setuptools import setup, find_packages\n\n\n# with open(\"requirements.txt\") as requirements_file:\n#     requirements = requirements_file.read().splitlines()\n    \nsetup(\n    name='deepke',  # ÊâìÂåÖÂêéÁöÑÂåÖÊñá‰ª∂Âêç\n    version='2.2.7',    #ÁâàÊú¨Âè∑\n    keywords=[\"pip\", \"RE\",\"NER\",\"AE\"],    # ÂÖ≥ÈîÆÂ≠ó\n    description='DeepKE is a knowledge extraction toolkit for knowledge graph construction supporting low-resource, document-level and multimodal scenarios for entity, relation and attribute extraction.',  # ËØ¥Êòé\n    license=\"MIT\",  # ËÆ∏ÂèØ\n    url='https://github.com/zjunlp/deepke',\n    author='ZJUNLP',\n    author_email='zhangningyu@zju.edu.cn',\n    include_package_data=True,\n    platforms=\"any\",\n    package_dir={\"\": \"src\"},\n    packages=find_packages(\"src\"),\n    # install_requires=requirements,\n    classifiers=[\n        \"Programming Language :: Python :: 3\",\n        \"Operating System :: OS Independent\",\n    ]\n)\n"
        },
        {
          "name": "src",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}