{
  "metadata": {
    "timestamp": 1736559859734,
    "page": 613,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjYyMA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "zjunlp/DeepKE",
      "stars": 3679,
      "defaultBranch": "main",
      "files": [
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.1416015625,
          "content": ".DS_Store\n\n.idea\n.vscode\n\n__pycache__\n*.pyc\n\ntest/.pytest_cache\n\ndata/out\n\nlogs\ncheckpoints\n\ndemo.py\n\notherUtils.py\nmodule/Transformer_offical.py"
        },
        {
          "name": "CITATION.cff",
          "type": "blob",
          "size": 2.365234375,
          "content": "cff-version: \"1.0.0\"\nmessage: \"If you use this toolkit, please cite it using these metadata.\"\ntitle: \"deepke\"\nrepository-code: \"https://https://github.com/zjunlp/DeepKE\"\nauthors: \n  - family-names: Zhang\n    given-names: Ningyu\n  - family-names: Xu\n    given-names: Xin\n  - family-names: Tao\n    given-names: Liankuan\n  - family-names: Yu\n    given-names: Haiyang\n  - family-names: Ye\n    given-names: Hongbin\n  - family-names: Qiao\n    given-names: Shuofei\n  - family-names: Xie\n    given-names: Xin\n  - family-names: Chen\n    given-names: Xiang\n  - family-names: Li\n    given-names: Zhoubo\n  - family-names: Li\n    given-names: Lei\n  - family-names: Liang\n    given-names: Xiaozhuan\n  - family-names: Yao\n    given-names: Yunzhi\n  - family-names: Deng\n    given-names: Shumin\n  - family-names: Wang\n    given-names: Peng\n  - family-names: Zhang\n    given-names: Wen\n  - family-names: Zhang\n    given-names: Zhenru\n  - family-names: Tan\n    given-names: Chuanqi\n  - family-names: Chen\n    given-names: Qiang\n  - family-names: Xiong\n    given-names: Feiyu\n  - family-names: Huang\n    given-names: Fei\n  - family-names: Zheng\n    given-names: Guozhou\n  - family-names: Chen\n    given-names: Huajun\npreferred-citation:\n  type: article\n  title: \"DeepKE: A Deep Learning Based Knowledge Extraction Toolkit for Knowledge Base Population\"\n  authors:\n  - family-names: Zhang\n    given-names: Ningyu\n  - family-names: Xu\n    given-names: Xin\n  - family-names: Tao\n    given-names: Liankuan\n  - family-names: Yu\n    given-names: Haiyang\n  - family-names: Ye\n    given-names: Hongbin\n  - family-names: Qiao\n    given-names: Shuofei\n  - family-names: Xie\n    given-names: Xin\n  - family-names: Chen\n    given-names: Xiang\n  - family-names: Li\n    given-names: Zhoubo\n  - family-names: Li\n    given-names: Lei\n  - family-names: Liang\n    given-names: Xiaozhuan\n  - family-names: Yao\n    given-names: Yunzhi\n  - family-names: Deng\n    given-names: Shumin\n  - family-names: Wang\n    given-names: Peng\n  - family-names: Zhang\n    given-names: Wen\n  - family-names: Zhang\n    given-names: Zhenru\n  - family-names: Tan\n    given-names: Chuanqi\n  - family-names: Chen\n    given-names: Qiang\n  - family-names: Xiong\n    given-names: Feiyu\n  - family-names: Huang\n    given-names: Fei\n  - family-names: Zheng\n    given-names: Guozhou\n  - family-names: Chen\n    given-names: Huajun\n  journal: \"http://arxiv.org/abs/2201.03335\"\n  year: 2022\n  \n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 1.0380859375,
          "content": "MIT License\n\nCopyright (c) 2021 ZJUNLP\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n"
        },
        {
          "name": "MANIFEST.in",
          "type": "blob",
          "size": 0.0234375,
          "content": "include requirements.txt"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 30.7880859375,
          "content": "<p align=\"center\">\n    <a href=\"https://github.com/zjunlp/deepke\"> <img src=\"pics/logo.png\" width=\"400\"/></a>\n<p>\n<p align=\"center\">  \n    <a href=\"http://deepke.zjukg.cn\">\n        <img alt=\"Documentation\" src=\"https://img.shields.io/badge/demo-website-blue\">\n    </a>\n    <a href=\"https://pypi.org/project/deepke/#files\">\n        <img alt=\"PyPI\" src=\"https://img.shields.io/pypi/v/deepke\">\n    </a>\n    <a href=\"https://github.com/zjunlp/DeepKE/blob/master/LICENSE\">\n        <img alt=\"GitHub\" src=\"https://img.shields.io/github/license/zjunlp/deepke\">\n    </a>\n    <a href=\"http://zjunlp.github.io/DeepKE\">\n        <img alt=\"Documentation\" src=\"https://img.shields.io/badge/doc-website-red\">\n    </a>\n    <a href=\"https://colab.research.google.com/drive/1vS8YJhJltzw3hpJczPt24O0Azcs3ZpRi?usp=sharing\">\n        <img alt=\"Open In Colab\" src=\"https://colab.research.google.com/assets/colab-badge.svg\">\n    </a>\n</p>\n\n\n<p align=\"center\">\n    <b> English | <a href=\"https://github.com/zjunlp/DeepKE/blob/main/README_CN.md\">ç®€ä½“ä¸­æ–‡</a> </b>\n</p>\n\n<h1 align=\"center\">\n    <p>A Deep Learning Based Knowledge Extraction Toolkit<br>for Knowledge Graph Construction</p>\n</h1>\n\n\n[DeepKE](https://arxiv.org/pdf/2201.03335.pdf) is a knowledge extraction toolkit for knowledge graph construction supporting **cnSchema**ï¼Œ**low-resource**, **document-level** and **multimodal** scenarios for *entity*, *relation* and *attribute* extraction. We provide [documents](https://zjunlp.github.io/DeepKE/), [online demo](http://deepke.zjukg.cn/), [paper](https://arxiv.org/pdf/2201.03335.pdf), [slides](https://drive.google.com/file/d/1IIeIZAbVduemqXc4zD40FUMoPHCJinLy/view?usp=sharing) and [poster](https://drive.google.com/file/d/1vd7xVHlWzoAxivN4T5qKrcqIGDcSM1_7/view?usp=sharing) for beginners.\n\n- â—Want to use **Large Language Models** with DeepKE? Try [DeepKE-LLM](https://github.com/zjunlp/DeepKE/tree/main/example/llm) and [OneKE](https://github.com/zjunlp/DeepKE/blob/main/example/llm/OneKE.md), have fun!\n- â—Want to train supervised models? Try [Quick Start](#quick-start), we provide the NER models (e.g, [LightNER(COLING'22)](https://github.com/zjunlp/DeepKE/tree/main/example/ner/few-shot), [W2NER(AAAI'22)](https://github.com/zjunlp/DeepKE/tree/main/example/ner/standard/w2ner)), relation extraction models (e.g., [KnowPrompt(WWW'22)](https://github.com/zjunlp/DeepKE/tree/main/example/re/few-shot)), relational triple extraction models (e.g., [ASP(EMNLP'22)](https://github.com/zjunlp/DeepKE/tree/main/example/triple/ASP), [PRGC(ACL'21)](https://github.com/zjunlp/DeepKE/tree/main/example/triple/PRGC), [PURE(NAACL'21)](https://github.com/zjunlp/DeepKE/tree/main/example/triple/PURE)), and release off-the-shelf  models at [DeepKE-cnSchema](https://github.com/zjunlp/DeepKE/tree/main/example/triple/cnschema), have fun!\n- We recommend using Linux; if using Windows, please use `\\\\` in file paths;\n- If HuggingFace is inaccessible, please consider using `wisemodel` or `modescape`.\n\n**If you encounter any issues during the installation of DeepKE and DeepKE-LLM, please check [Tips](https://github.com/zjunlp/DeepKE#tips) or promptly submit an [issue](https://github.com/zjunlp/DeepKE/issues), and we will assist you with resolving the problem!**\n\n\n# Table of Contents\n\n- [Table of Contents](#table-of-contents)\n- [What's New](#whats-new)\n- [Prediction Demo](#prediction-demo)\n- [Model Framework](#model-framework)\n- [Quick Start](#quick-start)\n  - [DeepKE-LLM](#deepke-llm)\n  - [DeepKE](#deepke)\n      - [ğŸ”§Manual Environment Configuration](#manual-environment-configuration)\n      - [ğŸ³Building With Docker Images](#building-with-docker-images)\n  - [Requirements](#requirements)\n    - [DeepKE](#deepke-1)\n  - [Introduction of Three Functions](#introduction-of-three-functions)\n    - [1. Named Entity Recognition](#1-named-entity-recognition)\n    - [2. Relation Extraction](#2-relation-extraction)\n    - [3. Attribute Extraction](#3-attribute-extraction)\n    - [4. Event Extraction](#4-event-extraction)\n- [Tips](#tips)\n- [To do](#to-do)\n- [Reading Materials](#reading-materials)\n- [Related Toolkit](#related-toolkit)\n- [Citation](#citation)\n- [Contributors](#contributors)\n- [Other Knowledge Extraction Open-Source Projects](#other-knowledge-extraction-open-source-projects)\n\n<br>\n\n# What's New\n\n* `April, 2024` We release a new bilingual (Chinese and English) schema-based information extraction model called [OneKE](https://huggingface.co/zjunlp/OneKE) based on Chinese-Alpaca-2-13B.\n* `Feb, 2024` We release a large-scale (0.32B tokens) high-quality bilingual (Chinese and English) Information Extraction (IE) instruction dataset named [IEPile](https://huggingface.co/datasets/zjunlp/iepie), along with two models trained with `IEPile`, [baichuan2-13b-iepile-lora](https://huggingface.co/zjunlp/baichuan2-13b-iepile-lora) and [llama2-13b-iepile-lora](https://huggingface.co/zjunlp/llama2-13b-iepile-lora).\n* `Sep 2023` a bilingual Chinese English Information Extraction (IE) instruction dataset called  `InstructIE` was released for the Instruction based Knowledge Graph Construction Task (Instruction based KGC), as detailed in [here](./example/llm/README.md/#data).\n* `June, 2023` We update [DeepKE-LLM](https://github.com/zjunlp/DeepKE/tree/main/example/llm) to support **knowledge extraction** with [KnowLM](https://github.com/zjunlp/KnowLM), [ChatGLM](https://github.com/THUDM/ChatGLM-6B), LLaMA-series, GPT-series etc.\n* `Apr, 2023` We have added new models, including [CP-NER(IJCAI'23)](https://github.com/zjunlp/DeepKE/blob/main/example/ner/cross), [ASP(EMNLP'22)](https://github.com/zjunlp/DeepKE/tree/main/example/triple/ASP), [PRGC(ACL'21)](https://github.com/zjunlp/DeepKE/tree/main/example/triple/PRGC), [PURE(NAACL'21)](https://github.com/zjunlp/DeepKE/tree/main/example/triple/PURE), provided [event extraction](https://github.com/zjunlp/DeepKE/tree/main/example/ee/standard) capabilities (Chinese and English), and offered compatibility with higher versions of Python packages (e.g., Transformers).\n* `Feb, 2023` We have supported using [LLM](https://github.com/zjunlp/DeepKE/tree/main/example/llm) (GPT-3) with in-context learning (based on [EasyInstruct](https://github.com/zjunlp/EasyInstruct)) & data generation, added a NER model [W2NER(AAAI'22)](https://github.com/zjunlp/DeepKE/tree/main/example/ner/standard/w2ner).\n\n<details>\n<summary><b>Previous News</b></summary>\n\n* `Nov, 2022` Add data [annotation instructions](https://github.com/zjunlp/DeepKE/blob/main/README_TAG.md) for entity recognition and relation extraction, automatic labelling of weakly supervised data ([entity extraction](https://github.com/zjunlp/DeepKE/tree/main/example/ner/prepare-data) and [relation extraction](https://github.com/zjunlp/DeepKE/tree/main/example/re/prepare-data)), and optimize [multi-GPU training](https://github.com/zjunlp/DeepKE/tree/main/example/re/standard).\n  \n* `Sept, 2022` The paper [DeepKE: A Deep Learning Based Knowledge Extraction Toolkit for Knowledge Base Population](https://arxiv.org/abs/2201.03335) has been accepted by the EMNLP 2022 System Demonstration Track.\n\n* `Aug, 2022` We have added [data augmentation](https://github.com/zjunlp/DeepKE/tree/main/example/re/few-shot/DA) (Chinese, English) support for [low-resource relation extraction](https://github.com/zjunlp/DeepKE/tree/main/example/re/few-shot).\n\n* `June, 2022` We have added multimodal support for [entity](https://github.com/zjunlp/DeepKE/tree/main/example/ner/multimodal) and [relation extraction](https://github.com/zjunlp/DeepKE/tree/main/example/re/multimodal).\n\n* `May, 2022` We have released [DeepKE-cnschema](https://github.com/zjunlp/DeepKE/blob/main/README_CNSCHEMA.md) with off-the-shelf knowledge extraction models.\n\n* `Jan, 2022` We have released a paper [DeepKE: A Deep Learning Based Knowledge Extraction Toolkit for Knowledge Base Population](https://arxiv.org/abs/2201.03335)\n\n* `Dec, 2021` We have added `dockerfile` to create the enviroment automatically. \n\n* `Nov, 2021` The demo of DeepKE, supporting real-time extration without deploying and training, has been released.\n* The documentation of DeepKE, containing the details of DeepKE such as source codes and datasets, has been released.\n\n* `Oct, 2021` `pip install deepke`\n* The codes of deepke-v2.0 have been released.\n\n* `Aug, 2019` The codes of deepke-v1.0 have been released.\n\n* `Aug, 2018` The project DeepKE startup and codes of deepke-v0.1 have been released.\n  \n\n</details>\n\n# Prediction Demo\n\nThere is a demonstration of prediction. The GIF file is created by [Terminalizer](https://github.com/faressoft/terminalizer). Get the [code](https://drive.google.com/file/d/1r4tWfAkpvynH3CBSgd-XG79rf-pB-KR3/view?usp=share_link).\n<img src=\"pics/demo.gif\" width=\"636\" height=\"494\" align=center>\n\n<br>\n\n# Model Framework\n\n<h3 align=\"center\">\n    <img src=\"pics/architectures.png\">\n</h3>\n\n\n- DeepKE contains a unified framework for **named entity recognition**, **relation extraction** and **attribute extraction**, the three  knowledge extraction functions.\n- Each task can be implemented in different scenarios. For example, we can achieve relation extraction in **standard**, **low-resource (few-shot)**, **document-level** and **multimodal** settings.\n- Each application scenario comprises of three components: **Data** including Tokenizer, Preprocessor and Loader, **Model** including Module, Encoder and Forwarder, **Core** including Training, Evaluation and Prediction. \n\n<br>\n\n# Quick Start\n\n## DeepKE-LLM\n\nIn the era of large models, DeepKE-LLM utilizes a completely new environment dependency.\n\n```\nconda create -n deepke-llm python=3.9\nconda activate deepke-llm\n\ncd example/llm\npip install -r requirements.txt\n```\n\nPlease note that the `requirements.txt` file is located in the `example/llm` folder.\n\n## DeepKE\n- *DeepKE* supports `pip install deepke`. <br>Take the fully supervised relation extraction for example.\n- *DeepKE* supports both **manual** and **docker image** environment configuration, you can choose the appropriate way to build.\n- Highly recommended to install deepke in a Linux environment.\n#### ğŸ”§Manual Environment Configuration\n\n**Step1** Download the basic code\n\n```bash\ngit clone --depth 1 https://github.com/zjunlp/DeepKE.git\n```\n\n**Step2** Create a virtual environment using `Anaconda` and enter it.<br>\n\n```bash\nconda create -n deepke python=3.8\n\nconda activate deepke\n```\n\n1. Install *DeepKE* with source code\n\n   ```bash\n   pip install -r requirements.txt\n   \n   python setup.py install\n   \n   python setup.py develop\n   ```\n\n2. Install *DeepKE* with `pip` (**NOT recommended!**)\n\n   ```bash\n   pip install deepke\n   ```\n   - Please make sure that pip version <= 24.0\n\n**Step3** Enter the task directory\n\n```bash\ncd DeepKE/example/re/standard\n```\n\n**Step4** Download the dataset, or follow the [annotation instructions](https://github.com/zjunlp/DeepKE/blob/main/README_TAG.md) to obtain data\n\n```bash\nwget 120.27.214.45/Data/re/standard/data.tar.gz\n\ntar -xzvf data.tar.gz\n```\n\nMany types of data formats are supported,and details are in each part. \n\n**Step5** Training (Parameters for training can be changed in the `conf` folder)\n\nWe support visual parameter tuning by using *[wandb](https://docs.wandb.ai/quickstart)*.\n\n```bash\npython run.py\n```\n\n**Step6** Prediction (Parameters for prediction can be changed in the `conf` folder)\n\nModify the path of the trained model in `predict.yaml`.The absolute path of the model needs to be usedï¼Œsuch as `xxx/checkpoints/2019-12-03_ 17-35-30/cnn_ epoch21.pth`.\n\n```bash\npython predict.py\n```\n\n - **â—NOTE: if you encounter any errors, please refer to the [Tips](#tips) or submit a GitHub issue.**\n\n\n\n#### ğŸ³Building With Docker Images\n**Step1** Install the Docker client\n\nInstall Docker and start the Docker service.\n\n**Step2** Pull the docker image and run the container\n\n```bash\ndocker pull zjunlp/deepke:latest\ndocker run -it zjunlp/deepke:latest /bin/bash\n```\n\nThe remaining steps are the same as **Step 3 and onwards** in **Manual Environment Configuration**.\n\n - **â—NOTE: You can refer to the [Tips](#tips) to speed up installation**\n\n## Requirements\n\n\n### DeepKE\n> python == 3.8\n\n- torch>=1.5,<=1.11\n- hydra-core==1.0.6\n- tensorboard==2.4.1\n- matplotlib==3.4.1\n- transformers==4.26.0\n- jieba==0.42.1\n- scikit-learn==0.24.1\n- seqeval==1.2.2\n- opt-einsum==3.3.0\n- wandb==0.12.7\n- ujson==5.6.0\n- huggingface_hub==0.11.0\n- tensorboardX==2.5.1\n- nltk==3.8\n- protobuf==3.20.1\n- numpy==1.21.0\n- ipdb==0.13.11\n- pytorch-crf==0.7.2\n- tqdm==4.66.1\n- openai==0.28.0\n- Jinja2==3.1.2\n- datasets==2.13.2\n- pyhocon==0.3.60\n\n<br>\n\n## Introduction of Three Functions\n\n### 1. Named Entity Recognition\n\n- Named entity recognition seeks to locate and classify named entities mentioned in unstructured text into pre-defined categories such as person names, organizations, locations, organizations, etc.\n\n- The data is stored in `.txt` files. Some instances as following (Users can label data based on the tools [Doccano](https://github.com/doccano/doccano), [MarkTool](https://github.com/FXLP/MarkTool), or they can use the [Weak Supervision](https://github.com/zjunlp/DeepKE/blob/main/example/ner/prepare-data) with DeepKE to obtain data automatically):\n\n  |                           Sentence                           |           Person           |    Location    |          Organization          |\n  | :----------------------------------------------------------: | :------------------------: | :------------: | :----------------------------: |\n  | æœ¬æŠ¥åŒ—äº¬9æœˆ4æ—¥è®¯è®°è€…æ¨æ¶ŒæŠ¥é“ï¼šéƒ¨åˆ†çœåŒºäººæ°‘æ—¥æŠ¥å®£ä¼ å‘è¡Œå·¥ä½œåº§è°ˆä¼š9æœˆ3æ—¥åœ¨4æ—¥åœ¨äº¬ä¸¾è¡Œã€‚ |            æ¨æ¶Œ            |      åŒ—äº¬      |            äººæ°‘æ—¥æŠ¥            |\n  | ã€Šçº¢æ¥¼æ¢¦ã€‹ç”±ç‹æ‰¶æ—å¯¼æ¼”ï¼Œå‘¨æ±æ˜Œã€ç‹è’™ã€å‘¨å²­ç­‰å¤šä½ä¸“å®¶å‚ä¸åˆ¶ä½œã€‚ | ç‹æ‰¶æ—ï¼Œå‘¨æ±æ˜Œï¼Œç‹è’™ï¼Œå‘¨å²­ |            |  |\n  | ç§¦å§‹çš‡å…µé©¬ä¿‘ä½äºé™•è¥¿çœè¥¿å®‰å¸‚,æ˜¯ä¸–ç•Œå…«å¤§å¥‡è¿¹ä¹‹ä¸€ã€‚ |           ç§¦å§‹çš‡           | é™•è¥¿çœï¼Œè¥¿å®‰å¸‚ |                          |\n\n- Read the detailed process in specific README\n  - **[STANDARD (Fully Supervised)](https://github.com/zjunlp/DeepKE/tree/main/example/ner/standard)**\n    \n    ***We [support LLM](https://github.com/zjunlp/DeepKE/tree/main/example/llm) and provide the off-the-shelf model, [DeepKE-cnSchema-NER](https://github.com/zjunlp/DeepKE/blob/main/README_CNSCHEMA_CN.md), which will extract entities in cnSchema without training.***\n\n    **Step1** Enter  `DeepKE/example/ner/standard`.  Download the dataset.\n\n    ```bash\n    wget 120.27.214.45/Data/ner/standard/data.tar.gz\n    \n    tar -xzvf data.tar.gz\n    ```\n\n    **Step2** Training<br>\n\n    The dataset and parameters can be customized in the `data` folder and `conf` folder respectively.\n  \n    ```bash\n    python run.py\n    ```\n\n    **Step3** Prediction\n\n    ```bash\n    python predict.py\n    ```\n  \n  - **[FEW-SHOT](https://github.com/zjunlp/DeepKE/tree/main/example/ner/few-shot)**\n\n    **Step1** Enter  `DeepKE/example/ner/few-shot`.  Download the dataset.\n\n    ```bash\n    wget 120.27.214.45/Data/ner/few_shot/data.tar.gz\n    \n    tar -xzvf data.tar.gz\n    ```\n  \n    **Step2** Training in the low-resouce setting <br>\n  \n    The directory where the model is loaded and saved and the configuration parameters can be cusomized in the `conf` folder.\n  \n    ```bash\n    python run.py +train=few_shot\n    ```\n    \n    Users can modify `load_path` in `conf/train/few_shot.yaml` to use existing loaded model.<br>\n    \n    **Step3** Add `- predict` to `conf/config.yaml`, modify `loda_path` as the model path and `write_path` as the path where the predicted results are saved in `conf/predict.yaml`, and then run `python predict.py`\n    \n    ```bash\n    python predict.py\n    ```\n\n  - **[MULTIMODAL](https://github.com/zjunlp/DeepKE/tree/main/example/ner/multimodal)**\n\n    **Step1** Enter  `DeepKE/example/ner/multimodal`.  Download the dataset.\n\n    ```bash\n    wget 120.27.214.45/Data/ner/multimodal/data.tar.gz\n    \n    tar -xzvf data.tar.gz\n    ```\n\n    We use RCNN detected objects and visual grounding objects from original images as visual local information, where RCNN via [faster_rcnn](https://github.com/pytorch/vision/blob/main/torchvision/models/detection/faster_rcnn.py) and visual grounding via [onestage_grounding](https://github.com/zyang-ur/onestage_grounding).\n\n    **Step2** Training in the multimodal setting <br>\n\n    - The dataset and parameters can be customized in the `data` folder and `conf` folder respectively.\n    - Start with the model trained last time: modify `load_path` in `conf/train.yaml`as the path where the model trained last time was saved. And the path saving logs generated in training can be customized by `log_dir`.\n\n    ```bash\n    python run.py\n    ```\n\n    **Step3** Prediction\n\n    ```bash\n    python predict.py\n    ```\n\n### 2. Relation Extraction\n\n- Relationship extraction is the task of extracting semantic relations between entities from a unstructured text.\n\n- The data is stored in `.csv` files. Some instances as following (Users can label data based on the tools [Doccano](https://github.com/doccano/doccano), [MarkTool](https://github.com/FXLP/MarkTool), or they can use the [Weak Supervision](https://github.com/zjunlp/DeepKE/blob/main/example/re/prepare-data) with DeepKE to obtain data automatically):\n\n  |                        Sentence                        | Relation |    Head    | Head_offset |    Tail    | Tail_offset |\n  | :----------------------------------------------------: | :------: | :--------: | :---------: | :--------: | :---------: |\n  | ã€Šå²³çˆ¶ä¹Ÿæ˜¯çˆ¹ã€‹æ˜¯ç‹å†›æ‰§å¯¼çš„ç”µè§†å‰§ï¼Œç”±é©¬æ©ç„¶ã€èŒƒæ˜ä¸»æ¼”ã€‚ |   å¯¼æ¼”   | å²³çˆ¶ä¹Ÿæ˜¯çˆ¹ |      1      |    ç‹å†›    |      8      |\n  |  ã€Šä¹ç„ç ã€‹æ˜¯åœ¨çºµæ¨ªä¸­æ–‡ç½‘è¿è½½çš„ä¸€éƒ¨å°è¯´ï¼Œä½œè€…æ˜¯é¾™é©¬ã€‚  | è¿è½½ç½‘ç«™ |   ä¹ç„ç    |      1      | çºµæ¨ªä¸­æ–‡ç½‘ |      7      |\n  |     æèµ·æ­å·çš„ç¾æ™¯ï¼Œè¥¿æ¹–æ€»æ˜¯ç¬¬ä¸€ä¸ªæ˜ å…¥è„‘æµ·çš„è¯è¯­ã€‚     | æ‰€åœ¨åŸå¸‚ |    è¥¿æ¹–    |      8      |    æ­å·    |      2      |\n\n- **!NOTE: If there are multiple entity types for one relation, entity types can be prefixed with the relation as inputs.**\n- Read the detailed process in specific README\n\n  - **[STANDARD (Fully Supervised)](https://github.com/zjunlp/DeepKE/tree/main/example/re/standard)** \n\n    ***We [support LLM](https://github.com/zjunlp/DeepKE/tree/main/example/llm) and provide the off-the-shelf model, [DeepKE-cnSchema-RE](https://github.com/zjunlp/DeepKE/blob/main/README_CNSCHEMA_CN.md), which will extract relations in cnSchema without training.***\n\n    **Step1** Enter the `DeepKE/example/re/standard` folder.  Download the dataset.\n\n    ```bash\n    wget 120.27.214.45/Data/re/standard/data.tar.gz\n    \n    tar -xzvf data.tar.gz\n    ```\n\n    **Step2** Training<br>\n\n    The dataset and parameters can be customized in the `data` folder and `conf` folder respectively.\n  \n    ```bash\n    python run.py\n    ```\n\n    **Step3** Prediction\n\n    ```bash\n    python predict.py\n    ```\n  \n  - **[FEW-SHOT](https://github.com/zjunlp/DeepKE/tree/main/example/re/few-shot)**\n\n    **Step1** Enter `DeepKE/example/re/few-shot`. Download the dataset.\n\n    ```bash\n    wget 120.27.214.45/Data/re/few_shot/data.tar.gz\n    \n    tar -xzvf data.tar.gz\n    ```\n\n    **Step 2** Training<br>\n\n    - The dataset and parameters can be customized in the `data` folder and `conf` folder respectively.\n    - Start with the model trained last time: modify `train_from_saved_model` in `conf/train.yaml`as the path where the model trained last time was saved. And the path saving logs generated in training can be customized by `log_dir`. \n  \n    ```bash\n    python run.py\n    ```\n  \n    **Step3** Prediction\n  \n    ```bash\n    python predict.py\n    ```\n  \n  - **[DOCUMENT](https://github.com/zjunlp/DeepKE/tree/main/example/re/document)**<br>\n  \n    **Step1** Enter `DeepKE/example/re/document`.  Download the dataset.\n  \n    ```bash\n    wget 120.27.214.45/Data/re/document/data.tar.gz\n    \n    tar -xzvf data.tar.gz\n    ```\n    \n    **Step2** Training<br>\n  \n    - The dataset and parameters can be customized in the `data` folder and `conf` folder respectively.\n    - Start with the model trained last time: modify `train_from_saved_model` in `conf/train.yaml`as the path where the model trained last time was saved. And the path saving logs generated in training can be customized by `log_dir`. \n    \n    ```bash\n    python run.py\n    ```\n    \n    **Step3** Prediction\n    \n    ```bash\n    python predict.py\n    ```\n\n  - **[MULTIMODAL](https://github.com/zjunlp/DeepKE/tree/main/example/re/multimodal)**\n\n    **Step1** Enter  `DeepKE/example/re/multimodal`.  Download the dataset.\n\n    ```bash\n    wget 120.27.214.45/Data/re/multimodal/data.tar.gz\n    \n    tar -xzvf data.tar.gz\n    ```\n\n    We use RCNN detected objects and visual grounding objects from original images as visual local information, where RCNN via [faster_rcnn](https://github.com/pytorch/vision/blob/main/torchvision/models/detection/faster_rcnn.py) and visual grounding via [onestage_grounding](https://github.com/zyang-ur/onestage_grounding).\n\n    **Step2** Training<br>\n\n    - The dataset and parameters can be customized in the `data` folder and `conf` folder respectively.\n    - Start with the model trained last time: modify `load_path` in `conf/train.yaml`as the path where the model trained last time was saved. And the path saving logs generated in training can be customized by `log_dir`.\n\n    ```bash\n    python run.py\n    ```\n\n    **Step3** Prediction\n\n    ```bash\n    python predict.py\n    ```\n\n### 3. Attribute Extraction\n\n- Attribute extraction is to extract attributes for entities in a unstructed text.\n\n- The data is stored in `.csv` files. Some instances as following:\n\n  |                           Sentence                           |   Att    |   Ent    | Ent_offset |      Val      | Val_offset |\n  | :----------------------------------------------------------: | :------: | :------: | :--------: | :-----------: | :--------: |\n  |          å¼ å†¬æ¢…ï¼Œå¥³ï¼Œæ±‰æ—ï¼Œ1968å¹´2æœˆç”Ÿï¼Œæ²³å—æ·‡å¿äºº           |   æ°‘æ—   |  å¼ å†¬æ¢…  |     0      |     æ±‰æ—      |     6      |\n  |è¯¸è‘›äº®ï¼Œå­—å­”æ˜ï¼Œä¸‰å›½æ—¶æœŸæ°å‡ºçš„å†›äº‹å®¶ã€æ–‡å­¦å®¶ã€å‘æ˜å®¶ã€‚|   æœä»£   |   è¯¸è‘›äº®   |     0      |     ä¸‰å›½æ—¶æœŸ      |     8     |\n  |        2014å¹´10æœˆ1æ—¥è®¸éåæ‰§å¯¼çš„ç”µå½±ã€Šé»„é‡‘æ—¶ä»£ã€‹ä¸Šæ˜          | ä¸Šæ˜ æ—¶é—´ | é»„é‡‘æ—¶ä»£ |     19     | 2014å¹´10æœˆ1æ—¥ |     0      |\n\n- Read the detailed process in specific README\n  - **[STANDARD (Fully Supervised)](https://github.com/zjunlp/DeepKE/tree/main/example/ae/standard)**\n\n    **Step1** Enter the `DeepKE/example/ae/standard` folder. Download the dataset.\n\n    ```bash\n    wget 120.27.214.45/Data/ae/standard/data.tar.gz\n    \n    tar -xzvf data.tar.gz\n    ```\n\n    **Step2** Training<br>\n\n    The dataset and parameters can be customized in the `data` folder and `conf` folder respectively.\n    \n    ```bash\n    python run.py\n    ```\n    \n    **Step3** Prediction\n    \n    ```bash\n    python predict.py\n    ```\n\n<br>\n\n### 4. Event Extraction\n\n* Event extraction is the task to extract event type, event trigger words, event arguments from a unstructed text.\n* The data is stored in `.tsv` files, some instances are as follows:\n\n<table h style=\"text-align:center\">\n    <tr>\n        <th colspan=\"2\"> Sentence </th>\n        <th> Event type </th>\n        <th> Trigger </th>\n        <th> Role </th>\n        <th> Argument </th>\n    </tr>\n    <tr> \n        <td rowspan=\"3\" colspan=\"2\"> æ®ã€Šæ¬§æ´²æ—¶æŠ¥ã€‹æŠ¥é“ï¼Œå½“åœ°æ—¶é—´27æ—¥ï¼Œæ³•å›½å·´é»å¢æµ®å®«åšç‰©é¦†å‘˜å·¥å› ä¸æ»¡å·¥ä½œæ¡ä»¶æ¶åŒ–è€Œç½¢å·¥ï¼Œå¯¼è‡´è¯¥åšç‰©é¦†ä¹Ÿå› æ­¤é—­é—¨è°¢å®¢ä¸€å¤©ã€‚ </td>\n      \t<td rowspan=\"3\"> ç»„ç»‡è¡Œä¸º-ç½¢å·¥ </td>\n    \t\t<td rowspan=\"3\"> ç½¢å·¥ </td>\n    \t\t<td> ç½¢å·¥äººå‘˜ </td>\n    \t\t<td> æ³•å›½å·´é»å¢æµ®å®«åšç‰©é¦†å‘˜å·¥ </td>\n    </tr>\n    <tr> \n        <td> æ—¶é—´ </td>\n        <td> å½“åœ°æ—¶é—´27æ—¥ </td>\n    </tr>\n    <tr> \n        <td> æ‰€å±ç»„ç»‡ </td>\n        <td> æ³•å›½å·´é»å¢æµ®å®«åšç‰©é¦† </td>\n    </tr>\n    <tr> \n        <td rowspan=\"3\" colspan=\"2\"> ä¸­å›½å¤–è¿2019å¹´ä¸ŠåŠå¹´å½’æ¯å‡€åˆ©æ¶¦å¢é•¿17%ï¼šæ”¶è´­äº†å°‘æ•°è‚¡ä¸œè‚¡æƒ </td>\n      \t<td rowspan=\"3\"> è´¢ç»/äº¤æ˜“-å‡ºå”®/æ”¶è´­ </td>\n    \t\t<td rowspan=\"3\"> æ”¶è´­ </td>\n    \t\t<td> å‡ºå”®æ–¹ </td>\n    \t\t<td> å°‘æ•°è‚¡ä¸œ </td>\n    </tr>\n    <tr> \n        <td> æ”¶è´­æ–¹ </td>\n        <td> ä¸­å›½å¤–è¿ </td>\n    </tr>\n    <tr> \n        <td> äº¤æ˜“ç‰© </td>\n        <td> è‚¡æƒ </td>\n    </tr>\n    <tr> \n        <td rowspan=\"3\" colspan=\"2\"> ç¾å›½äºšç‰¹å…°å¤§èˆªå±•13æ—¥å‘ç”Ÿä¸€èµ·è¡¨æ¼”æœºå æœºäº‹æ•…ï¼Œé£è¡Œå‘˜å¼¹å°„å‡ºèˆ±å¹¶å®‰å…¨ç€é™†ï¼Œäº‹æ•…æ²¡æœ‰é€ æˆäººå‘˜ä¼¤äº¡ã€‚ </td>\n      \t<td rowspan=\"3\"> ç¾å®³/æ„å¤–-å æœº </td>\n    \t\t<td rowspan=\"3\"> å æœº </td>\n    \t\t<td> æ—¶é—´ </td>\n    \t\t<td> 13æ—¥ </td>\n    </tr>\n    <tr> \n        <td> åœ°ç‚¹ </td>\n        <td> ç¾å›½äºšç‰¹å…° </td>\n  \t</tr>\n</table>\n\n* Read the detailed process in specific README\n\n  * [STANDARD(Fully Supervised)](./example/ee/standard/README.md)\n\n    **Step1** Enter the `DeepKE/example/ee/standard` folder. Download the dataset.\n\n    ```bash\n    wget 120.27.214.45/Data/ee/DuEE.zip\n    unzip DuEE.zip\n    ```\n\n    **Step 2** Training\n\n    The dataset and parameters can be customized in the `data` folder and `conf` folder respectively.\n\n    ```bash\n    python run.py\n    ```\n\n    **Step 3** Prediction\n\n    ```bash\n    python predict.py\n    ```\n\n<br>\n\n# Tips\n\n1.```Using nearest mirror```, **[THU](https://mirrors.tuna.tsinghua.edu.cn/help/anaconda/) in China, will speed up the installation of *Anaconda*; [aliyun](http://mirrors.aliyun.com/pypi/simple/) in China, will speed up `pip install XXX`**.\n\n2.When encountering `ModuleNotFoundError: No module named 'past'`ï¼Œrun `pip install future` .\n\n3.It's slow to install the pretrained language models online. Recommend download pretrained models before use and save them in the `pretrained` folder. Read `README.md` in every task directory to check the specific requirement for saving pretrained models.\n\n4.The old version of *DeepKE* is in the [deepke-v1.0](https://github.com/zjunlp/DeepKE/tree/deepke-v1.0) branch. Users can change the branch to use the old version. The old version has been totally transfered to the standard relation extraction ([example/re/standard](https://github.com/zjunlp/DeepKE/blob/main/example/re/standard/README.md)).\n\n5.If you want to modify the source code, it's recommended to install *DeepKE* with source codes. If not, the modification will not work. See [issue](https://github.com/zjunlp/DeepKE/issues/117)\n\n6.More related low-resource knowledge extraction  works can be found in [Knowledge Extraction in Low-Resource Scenarios: Survey and Perspective](https://arxiv.org/pdf/2202.08063.pdf).\n\n7.Make sure the exact versions of requirements in `requirements.txt`.\n\n# To do\nIn next version, we plan to release a stronger LLM for KE. \n\nMeanwhile, we will offer long-term maintenance to **fix bugs**, **solve issues** and meet **new requests**. So if you have any problems, please put issues to us.\n\n# Reading Materials\n\nData-Efficient Knowledge Graph Construction, é«˜æ•ˆçŸ¥è¯†å›¾è°±æ„å»º ([Tutorial on CCKS 2022](http://sigkg.cn/ccks2022/?page_id=24)) \\[[slides](https://drive.google.com/drive/folders/1xqeREw3dSiw-Y1rxLDx77r0hGUvHnuuE)\\] \n\nEfficient and Robust Knowledge Graph Construction ([Tutorial on AACL-IJCNLP 2022](https://www.aacl2022.org/Program/tutorials)) \\[[slides](https://github.com/NLP-Tutorials/AACL-IJCNLP2022-KGC-Tutorial)\\] \n\nPromptKG Family: a Gallery of Prompt Learning & KG-related Research Works, Toolkits, and Paper-list [[Resources](https://github.com/zjunlp/PromptKG)\\] \n\nKnowledge Extraction in Low-Resource Scenarios: Survey and Perspective \\[[Survey](https://arxiv.org/abs/2202.08063)\\]\\[[Paper-list](https://github.com/zjunlp/Low-resource-KEPapers)\\]\n\n\n# Related Toolkit\n\n[Doccano](https://github.com/doccano/doccano)ã€[MarkTool](https://github.com/FXLP/MarkTool)ã€[LabelStudio](https://labelstud.io/ ): Data Annotation Toolkits\n\n[LambdaKG](https://github.com/zjunlp/PromptKG/tree/main/lambdaKG): A library and benchmark for PLM-based KG embeddings\n\n[EasyInstruct](https://github.com/zjunlp/EasyInstruct): An easy-to-use framework to instruct Large Language Models\n\n**Reading Materials**:\n\nData-Efficient Knowledge Graph Construction, é«˜æ•ˆçŸ¥è¯†å›¾è°±æ„å»º ([Tutorial on CCKS 2022](http://sigkg.cn/ccks2022/?page_id=24)) \\[[slides](https://drive.google.com/drive/folders/1xqeREw3dSiw-Y1rxLDx77r0hGUvHnuuE)\\] \n\nEfficient and Robust Knowledge Graph Construction ([Tutorial on AACL-IJCNLP 2022](https://www.aacl2022.org/Program/tutorials)) \\[[slides](https://github.com/NLP-Tutorials/AACL-IJCNLP2022-KGC-Tutorial)\\] \n\nPromptKG Family: a Gallery of Prompt Learning & KG-related Research Works, Toolkits, and Paper-list [[Resources](https://github.com/zjunlp/PromptKG)\\] \n\nKnowledge Extraction in Low-Resource Scenarios: Survey and Perspective \\[[Survey](https://arxiv.org/abs/2202.08063)\\]\\[[Paper-list](https://github.com/zjunlp/Low-resource-KEPapers)\\]\n\n\n**Related Toolkit**:\n\n[Doccano](https://github.com/doccano/doccano)ã€[MarkTool](https://github.com/FXLP/MarkTool)ã€[LabelStudio](https://labelstud.io/ ): Data Annotation Toolkits\n\n[LambdaKG](https://github.com/zjunlp/PromptKG/tree/main/lambdaKG): A library and benchmark for PLM-based KG embeddings\n\n[EasyInstruct](https://github.com/zjunlp/EasyInstruct): An easy-to-use framework to instruct Large Language Models\n\n# Citation\n\nPlease cite our paper if you use DeepKE in your work\n\n```bibtex\n@inproceedings{EMNLP2022_Demo_DeepKE,\n  author    = {Ningyu Zhang and\n               Xin Xu and\n               Liankuan Tao and\n               Haiyang Yu and\n               Hongbin Ye and\n               Shuofei Qiao and\n               Xin Xie and\n               Xiang Chen and\n               Zhoubo Li and\n               Lei Li},\n  editor    = {Wanxiang Che and\n               Ekaterina Shutova},\n  title     = {DeepKE: {A} Deep Learning Based Knowledge Extraction Toolkit for Knowledge Base Population},\n  booktitle = {{EMNLP} (Demos)},\n  pages     = {98--108},\n  publisher = {Association for Computational Linguistics},\n  year      = {2022},\n  url       = {https://aclanthology.org/2022.emnlp-demos.10}\n}\n```\n<br>\n\n# Contributors\n\n[Ningyu Zhang](https://person.zju.edu.cn/en/ningyu), [Haofen Wang](https://tjdi.tongji.edu.cn/TeacherDetail.do?id=4991&lang=_en), Fei Huang, Feiyu Xiong, Liankuan Tao, Xin Xu, Honghao Gui,  Zhenru Zhang, Chuanqi Tan, Qiang Chen, Xiaohan Wang, Zekun Xi, Xinrong Li, Haiyang Yu, Hongbin Ye, Shuofei Qiao, Peng Wang, Yuqi Zhu, Xin Xie, Xiang Chen, Zhoubo Li, Lei Li, Xiaozhuan Liang, Yunzhi Yao, Jing Chen, Yuqi Zhu, Shumin Deng, Wen Zhang, Guozhou Zheng, Huajun Chen\n\nCommunity Contributors: [thredreams](https://github.com/thredreams), [eltociear](https://github.com/eltociear), Ziwen Xu, Rui Huang, Xiaolong Weng\n\n# Other Knowledge Extraction Open-Source Projects\n\n- [CogIE](https://github.com/jinzhuoran/CogIE)\n- [OpenNRE](https://github.com/thunlp/OpenNRE)\n- [OmniEvent](https://github.com/THU-KEG/OmniEvent)\n- [OpenUE](https://github.com/zjunlp/OpenUE)\n- [OpenIE](https://stanfordnlp.github.io/CoreNLP/openie.html)\n- [RESIN](https://github.com/RESIN-KAIROS/RESIN-pipeline-public)\n- [ZShot](https://github.com/IBM/zshot)\n- [ZS4IE](https://github.com/BBN-E/ZS4IE)\n- [OmniEvent](https://github.com/THU-KEG/OmniEvent)\n"
        },
        {
          "name": "README_CN.md",
          "type": "blob",
          "size": 29.09375,
          "content": "<p align=\"center\">\n    <a href=\"https://github.com/zjunlp/deepke\"> <img src=\"pics/logo.png\" width=\"400\"/></a>\n<p>\n<p align=\"center\">  \n    <a href=\"http://deepke.zjukg.cn\">\n        <img alt=\"Documentation\" src=\"https://img.shields.io/badge/demo-website-blue\">\n    </a>\n    <a href=\"https://pypi.org/project/deepke/#files\">\n        <img alt=\"PyPI\" src=\"https://img.shields.io/pypi/v/deepke\">\n    </a>\n    <a href=\"https://github.com/zjunlp/DeepKE/blob/master/LICENSE\">\n        <img alt=\"GitHub\" src=\"https://img.shields.io/github/license/zjunlp/deepke\">\n    </a>\n    <a href=\"http://zjunlp.github.io/DeepKE\">\n        <img alt=\"Documentation\" src=\"https://img.shields.io/badge/doc-website-red\">\n    </a>\n    <a href=\"https://colab.research.google.com/drive/1vS8YJhJltzw3hpJczPt24O0Azcs3ZpRi?usp=sharing\">\n        <img alt=\"Open In Colab\" src=\"https://colab.research.google.com/assets/colab-badge.svg\">\n    </a>\n</p>\n\n<p align=\"center\">\n    <b> <a href=\"https://github.com/zjunlp/DeepKE/blob/main/README.md\">English</a> | ç®€ä½“ä¸­æ–‡ </b>\n</p>\n\n\n<h1 align=\"center\">\n    <p>åŸºäºæ·±åº¦å­¦ä¹ çš„å¼€æºä¸­æ–‡çŸ¥è¯†å›¾è°±æŠ½å–æ¡†æ¶</p>\n</h1>\n\n\n[DeepKE](https://arxiv.org/pdf/2201.03335.pdf) æ˜¯ä¸€ä¸ªå¼€æºçš„çŸ¥è¯†å›¾è°±æŠ½å–ä¸æ„å»ºå·¥å…·ï¼Œæ”¯æŒ<b>cnSchemaã€ä½èµ„æºã€é•¿ç¯‡ç« ã€å¤šæ¨¡æ€</b>çš„çŸ¥è¯†æŠ½å–å·¥å…·ï¼Œå¯ä»¥åŸºäº<b>PyTorch</b>å®ç°<b>å‘½åå®ä½“è¯†åˆ«</b>ã€<b>å…³ç³»æŠ½å–</b>å’Œ<b>å±æ€§æŠ½å–</b>åŠŸèƒ½ã€‚åŒæ—¶ä¸ºåˆå­¦è€…æä¾›äº†[æ–‡æ¡£](https://zjunlp.github.io/DeepKE/)ï¼Œ[åœ¨çº¿æ¼”ç¤º](http://deepke.zjukg.cn/CN/index.html), [è®ºæ–‡](https://arxiv.org/pdf/2201.03335.pdf), [æ¼”ç¤ºæ–‡ç¨¿](https://github.com/zjunlp/DeepKE/blob/main/docs/slides/Slides-DeepKE-cn.pdf)å’Œ[æµ·æŠ¥](https://drive.google.com/file/d/1vd7xVHlWzoAxivN4T5qKrcqIGDcSM1_7/view)ã€‚\n\n- â—æƒ³ç”¨**å¤§æ¨¡å‹**åšæŠ½å–å—ï¼Ÿè¯•è¯•[DeepKE-LLM](https://github.com/zjunlp/DeepKE/tree/main/example/llm/README_CN.md)å’Œ[OneKE](https://github.com/zjunlp/DeepKE/blob/main/example/llm/OneKE.md)ï¼\n- â—æƒ³è‡ªå·±å…¨ç›‘ç£è®­æŠ½å–æ¨¡å‹å—ï¼Ÿè¯•è¯•[å¿«é€Ÿä¸Šæ‰‹](#å¿«é€Ÿä¸Šæ‰‹), æˆ‘ä»¬æä¾›å®ä½“è¯†åˆ«æ¨¡å‹ (ä¾‹å¦‚[LightNER(COLING'22)](https://github.com/zjunlp/DeepKE/tree/main/example/ner/few-shot/README_CN.md), [W2NER(AAAI'22)](https://github.com/zjunlp/DeepKE/tree/main/example/ner/standard/w2ner/README_CN.md))ã€å…³ç³»æŠ½å–æ¨¡å‹(ä¾‹å¦‚[KnowPrompt(WWW'22)](https://github.com/zjunlp/DeepKE/tree/main/example/re/few-shot/README_CN.md)), [PRGC(ACL'21)](https://github.com/zjunlp/DeepKE/tree/main/example/triple/PRGC/README_CN.md), [PURE(NAACL'21)](https://github.com/zjunlp/DeepKE/tree/main/example/triple/PURE/README_CN.md)), å’ŒåŸºäºcnSchemaçš„å¼€ç®±å³ç”¨æ¨¡å‹[DeepKE-cnSchema](https://github.com/zjunlp/DeepKE/tree/main/example/triple/cnschema/README_CN.md)ï¼\n- æ¨èä½¿ç”¨Linuxï¼Œå¦‚æœä½¿ç”¨Windowsï¼Œè·¯å¾„ä¸­è¯·ä½¿ç”¨`\\\\`\n- å¦‚æœå› ä¸ºç½‘ç»œé—®é¢˜æ— æ³•ä»HuggingFaceä¸‹è½½æ¨¡å‹ï¼Œå¯ä»¥ä½¿ç”¨wisemodelå’Œmodescope\n\n**å¦‚æœæ‚¨åœ¨å®‰è£…DeepKEå’ŒDeepKE-LLMä¸­é‡åˆ°ä»»ä½•é—®é¢˜ï¼ˆä¸€èˆ¬æ˜¯åŒ…çš„ç‰ˆæœ¬å…¼å®¹æ€§é—®é¢˜ï¼‰ä¸ç”¨å¿ƒæ€¥ï¼Œæ‚¨å¯ä»¥æŸ¥é˜…[å¸¸è§é—®é¢˜](https://github.com/zjunlp/DeepKE/blob/main/README_CN.md#%E5%A4%87%E6%B3%A8%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98)æˆ–ç›´æ¥æ[Issue](https://github.com/zjunlp/DeepKE/issues)ï¼Œæˆ‘ä»¬ä¼šå°½å…¨åŠ›å¸®åŠ©æ‚¨è§£å†³é—®é¢˜**ï¼\n\n# ç›®å½•\n\n- [ç›®å½•](#ç›®å½•)\n- [æ–°ç‰ˆç‰¹æ€§](#æ–°ç‰ˆç‰¹æ€§)\n- [é¢„æµ‹æ¼”ç¤º](#é¢„æµ‹æ¼”ç¤º)\n- [æ¨¡å‹æ¶æ„](#æ¨¡å‹æ¶æ„)\n- [å¿«é€Ÿä¸Šæ‰‹](#å¿«é€Ÿä¸Šæ‰‹)\n  - [DeepKE-LLM](#deepke-llm)\n  - [DeepKE](#deepke)\n      - [ğŸ”§ æ‰‹åŠ¨ç¯å¢ƒéƒ¨ç½²](#-æ‰‹åŠ¨ç¯å¢ƒéƒ¨ç½²)\n      - [ğŸ³ åŸºäºå®¹å™¨éƒ¨ç½²](#-åŸºäºå®¹å™¨éƒ¨ç½²)\n  - [ç¯å¢ƒä¾èµ–](#ç¯å¢ƒä¾èµ–)\n    - [DeepKE](#deepke-1)\n  - [å…·ä½“åŠŸèƒ½ä»‹ç»](#å…·ä½“åŠŸèƒ½ä»‹ç»)\n    - [1. å‘½åå®ä½“è¯†åˆ«NER](#1-å‘½åå®ä½“è¯†åˆ«ner)\n    - [2. å…³ç³»æŠ½å–RE](#2-å…³ç³»æŠ½å–re)\n    - [3. å±æ€§æŠ½å–AE](#3-å±æ€§æŠ½å–ae)\n    - [4.äº‹ä»¶æŠ½å–](#4äº‹ä»¶æŠ½å–)\n- [å¤‡æ³¨ï¼ˆå¸¸è§é—®é¢˜ï¼‰](#å¤‡æ³¨å¸¸è§é—®é¢˜)\n- [æœªæ¥è®¡åˆ’](#æœªæ¥è®¡åˆ’)\n- [é˜…è¯»èµ„æ–™](#é˜…è¯»èµ„æ–™)\n- [ç›¸å…³å·¥å…·](#ç›¸å…³å·¥å…·)\n- [å¼•ç”¨](#å¼•ç”¨)\n- [é¡¹ç›®è´¡çŒ®äººå‘˜](#é¡¹ç›®è´¡çŒ®äººå‘˜)\n- [å…¶å®ƒçŸ¥è¯†æŠ½å–å¼€æºå·¥å…·](#å…¶å®ƒçŸ¥è¯†æŠ½å–å¼€æºå·¥å…·)\n\n<br>\n\n# æ–°ç‰ˆç‰¹æ€§\n* `2024å¹´4æœˆ`å‘å¸ƒä¸­è‹±åŒè¯­å¤§æ¨¡å‹çŸ¥è¯†æŠ½å–æ¡†æ¶[OneKE](http://oneke.openkg.cn/)ï¼ŒåŒæ—¶å¼€æºåŸºäºChinese-Alpaca-2-13Bå…¨å‚æ•°å¾®è°ƒçš„ç‰ˆæœ¬ã€‚\n* `2024å¹´2æœˆ` å‘å¸ƒå¤§è§„æ¨¡(`0.32B` tokens)**åŒè¯­**(ä¸­æ–‡å’Œè‹±æ–‡)ä¿¡æ¯æŠ½å–(IE)æŒ‡ä»¤æ•°æ®é›†[IEPile](https://huggingface.co/datasets/zjunlp/iepie), ä»¥åŠåŸºäº `IEPile` è®­ç»ƒçš„ä¸¤ä¸ªæ¨¡å‹[baichuan2-13b-iepile-lora](https://huggingface.co/zjunlp/baichuan2-13b-iepile-lora)ã€[llama2-13b-iepile-lora](https://huggingface.co/zjunlp/llama2-13b-iepile-lora)ã€‚\n* `2023å¹´9æœˆ` ä¸ºåŸºäºæŒ‡ä»¤çš„çŸ¥è¯†å›¾è°±æ„å»ºä»»åŠ¡(Instruction-based KGC)å‘å¸ƒäº†ä¸€ä¸ªä¸­è‹±åŒè¯­ä¿¡æ¯æŠ½å–(IE)æŒ‡ä»¤æ•°æ®é›† `InstructIE`, å…·ä½“å‚è§[æ­¤å¤„](./example/llm/README_CN.md/#æ•°æ®)ã€‚\n* `2023å¹´6æœˆ` ä¸º[DeepKE-LLM](https://github.com/zjunlp/DeepKE/tree/main/example/llm)æ–°å¢å¤šä¸ªå¤§æ¨¡å‹(å¦‚[ChatGLM](https://github.com/THUDM/ChatGLM-6B)ã€LLaMAç³»åˆ—ã€GPTç³»åˆ—ã€æŠ½å–å¤§æ¨¡å‹[æ™ºæ](https://github.com/zjunlp/KnowLM))æ”¯æŒã€‚\n* `2023å¹´4æœˆ` æ–°å¢å®ä½“å…³ç³»æŠ½å–æ¨¡å‹[CP-NER(IJCAI'23)](https://github.com/zjunlp/DeepKE/blob/main/example/ner/cross/README_CN.md), [ASP(EMNLP'22)](https://github.com/zjunlp/DeepKE/tree/main/example/triple/ASP/README_CN.md), [PRGC(ACL'21)](https://github.com/zjunlp/DeepKE/tree/main/example/triple/PRGC/README_CN.md), [PURE(NAACL'21)](https://github.com/zjunlp/DeepKE/tree/main/example/triple/PURE/README_CN.md), æ”¯æŒ[äº‹ä»¶æŠ½å–](https://github.com/zjunlp/DeepKE/blob/main/example/ee/standard/README_CN.md)(ä¸­æ–‡ã€è‹±æ–‡), æä¾›å¯¹Pythonåº“é«˜çº§ç‰ˆæœ¬çš„æ”¯æŒ (ä¾‹å¦‚Transformers)ã€‚\n* `2023å¹´2æœˆ` æ”¯æŒ[å¤§æ¨¡å‹](https://github.com/zjunlp/DeepKE/blob/main/example/llm/README_CN.md) (GPT-3)ï¼ŒåŒ…å«In-context Learning (åŸºäº [EasyInstruct](https://github.com/zjunlp/EasyInstruct))å’Œæ•°æ®ç”Ÿæˆï¼Œæ–°å¢å®ä½“è¯†åˆ«æ¨¡å‹[W2NER(AAAI'22)](https://github.com/zjunlp/DeepKE/blob/main/example/ner/standard/README_CN.md)ã€‚\n\n<details>\n<summary><b>æ—§ç‰ˆæ–°é—»</b></summary>\n\n\n- `2022å¹´11æœˆ` æ–°å¢å®ä½“è¯†åˆ«ã€å…³ç³»æŠ½å–çš„[æ•°æ®æ ‡æ³¨è¯´æ˜](https://github.com/zjunlp/DeepKE/blob/main/README_TAG_CN.md)å’Œå¼±ç›‘ç£æ•°æ®è‡ªåŠ¨æ ‡æ³¨([å®ä½“è¯†åˆ«](https://github.com/zjunlp/DeepKE/blob/main/example/ner/prepare-data/README_CN.md)ã€[å…³ç³»æŠ½å–](https://github.com/zjunlp/DeepKE/blob/main/example/re/prepare-data/README_CN.md))åŠŸèƒ½ï¼Œä¼˜åŒ–[å¤šGPUè®­ç»ƒ](https://github.com/zjunlp/DeepKE/blob/main/example/re/standard/README_CN.md)ã€‚\n\n- `2022å¹´9æœˆ` è®ºæ–‡ [DeepKE: A Deep Learning Based Knowledge Extraction Toolkit for Knowledge Base Population](https://arxiv.org/abs/2201.03335)è¢«EMNLP2022 System Demonstration Trackå½•ç”¨ã€‚\n\n- `2022å¹´8æœˆ` æ–°å¢é’ˆå¯¹[ä½èµ„æºå…³ç³»æŠ½å–](https://github.com/zjunlp/DeepKE/tree/main/example/re/few-shot)çš„[æ•°æ®å¢å¼º](https://github.com/zjunlp/DeepKE/tree/main/example/re/few-shot/DA) (ä¸­æ–‡ã€è‹±æ–‡)åŠŸèƒ½ã€‚\n\n\n- `2022å¹´6æœˆ` æ–°å¢æ”¯æŒå¤šæ¨¡æ€åœºæ™¯çš„[å®ä½“æŠ½å–](https://github.com/zjunlp/DeepKE/tree/main/example/ner/multimodal)ã€[å…³ç³»æŠ½å–](https://github.com/zjunlp/DeepKE/tree/main/example/re/multimodal)åŠŸèƒ½ã€‚\n\n- `2022å¹´5æœˆ` å‘å¸ƒ[DeepKE-cnschema](https://github.com/zjunlp/DeepKE/blob/main/README_CNSCHEMA_CN.md)ç‰¹åˆ«ç‰ˆæ¨¡å‹ï¼Œæ”¯æŒåŸºäºcnSchemaçš„å¼€ç®±å³ç”¨çš„ä¸­æ–‡å®ä½“è¯†åˆ«å’Œå…³ç³»æŠ½å–ã€‚\n\n- `2022å¹´1æœˆ` å‘å¸ƒè®ºæ–‡ [DeepKE: A Deep Learning Based Knowledge Extraction Toolkit for Knowledge Base Population](https://arxiv.org/abs/2201.03335)\n\n- `2021å¹´12æœˆ` åŠ å…¥`dockerfile`ä»¥ä¾¿è‡ªåŠ¨åˆ›å»ºç¯å¢ƒ\n\n- `2021å¹´11æœˆ` å‘å¸ƒDeepKE demoé¡µé¢ï¼Œæ”¯æŒå®æ—¶æŠ½å–ï¼Œæ— éœ€éƒ¨ç½²å’Œè®­ç»ƒæ¨¡å‹\n- å‘å¸ƒDeepKEæ–‡æ¡£ï¼ŒåŒ…å«DeepKEæºç å’Œæ•°æ®é›†ç­‰è¯¦ç»†ä¿¡æ¯\n\n- `2021å¹´10æœˆ` `pip install deepke`\n- deepke-v2.0å‘å¸ƒ\n\n- `2019å¹´8æœˆ` `pip install deepke`\n- deepke-v1.0å‘å¸ƒ\n\n- `2018å¹´8æœˆ` DeepKEé¡¹ç›®å¯åŠ¨ï¼Œdeepke-v0.1ä»£ç å‘å¸ƒ\n\n</details>\n\n# é¢„æµ‹æ¼”ç¤º\nä¸‹é¢ä½¿ç”¨ä¸€ä¸ªdemoå±•ç¤ºé¢„æµ‹è¿‡ç¨‹ã€‚è¯¥åŠ¨å›¾ç”±[Terminalizer](https://github.com/faressoft/terminalizer)ç”Ÿæˆï¼Œç”Ÿæˆ[ä»£ç ](https://drive.google.com/file/d/1r4tWfAkpvynH3CBSgd-XG79rf-pB-KR3/view?usp=share_link)å¯ç‚¹å‡»è·å–ã€‚\n<img src=\"pics/demo.gif\" width=\"636\" height=\"494\" align=center>\n\n<br>\n\n# æ¨¡å‹æ¶æ„\n\nDeepkeçš„æ¶æ„å›¾å¦‚ä¸‹æ‰€ç¤º\n\n<h3 align=\"center\">\n    <img src=\"pics/architectures.png\">\n</h3>\n\n- DeepKEä¸ºä¸‰ä¸ªçŸ¥è¯†æŠ½å–åŠŸèƒ½ï¼ˆå‘½åå®ä½“è¯†åˆ«ã€å…³ç³»æŠ½å–å’Œå±æ€§æŠ½å–ï¼‰è®¾è®¡äº†ä¸€ä¸ªç»Ÿä¸€çš„æ¡†æ¶\n- å¯ä»¥åœ¨ä¸åŒåœºæ™¯ä¸‹å®ç°ä¸åŒåŠŸèƒ½ã€‚æ¯”å¦‚ï¼Œå¯ä»¥åœ¨æ ‡å‡†å…¨ç›‘ç£ã€ä½èµ„æºå°‘æ ·æœ¬ã€æ–‡æ¡£çº§å’Œå¤šæ¨¡æ€è®¾å®šä¸‹è¿›è¡Œå…³ç³»æŠ½å–\n- æ¯ä¸€ä¸ªåº”ç”¨åœºæ™¯ç”±ä¸‰ä¸ªéƒ¨åˆ†ç»„æˆï¼šDataéƒ¨åˆ†åŒ…å«Tokenizerã€Preprocessorå’ŒLoaderï¼ŒModeléƒ¨åˆ†åŒ…å«Moduleã€Encoderå’ŒForwarderï¼ŒCoreéƒ¨åˆ†åŒ…å«Trainingã€Evaluationå’ŒPrediction\n\n\n<br>\n\n# å¿«é€Ÿä¸Šæ‰‹\n\n## DeepKE-LLM\nå¤§æ¨¡å‹æ—¶ä»£, DeepKE-LLMé‡‡ç”¨å…¨æ–°çš„ç¯å¢ƒä¾èµ–ï¼Œå¼ºçƒˆå»ºè®®ä½¿ç”¨linuxç¯å¢ƒå®‰è£…\n```\nconda create -n deepke-llm python=3.9\nconda activate deepke-llm\n\ncd example/llm\npip install -r requirements.txt\n```\næ³¨æ„ï¼ï¼æ˜¯example/llmæ–‡ä»¶å¤¹ä¸‹çš„ `requirements.txt`\n\n## DeepKE\n- DeepKEæ”¯æŒpipå®‰è£…ä½¿ç”¨ï¼Œä¸‹ä»¥å¸¸è§„å…³ç³»æŠ½å–åœºæ™¯ä¸ºä¾‹\n- DeepKEæ”¯æŒæ‰‹åŠ¨ç¯å¢ƒéƒ¨ç½²ä¸å®¹å™¨éƒ¨ç½²ï¼Œæ‚¨å¯ä»»é€‰ä¸€ç§æ–¹æ³•è¿›è¡Œå®‰è£…\n- å¼ºçƒˆå»ºè®®ä½¿ç”¨linuxç¯å¢ƒå®‰è£…ï¼\n#### ğŸ”§ æ‰‹åŠ¨ç¯å¢ƒéƒ¨ç½²\n**Step 1**ï¼šä¸‹è½½ä»£ç  ```git clone --depth 1 https://github.com/zjunlp/DeepKE.git```ï¼ˆåˆ«å¿˜è®°starå’Œforkå“ˆï¼ï¼ï¼ï¼‰\n\n**Step 2**ï¼šä½¿ç”¨anacondaåˆ›å»ºè™šæ‹Ÿç¯å¢ƒï¼Œè¿›å…¥è™šæ‹Ÿç¯å¢ƒï¼ˆæä¾›[Dockerfile](https://github.com/zjunlp/DeepKE/tree/main/docker)æºç å’Œ[æ•™ç¨‹](https://github.com/zjunlp/DeepKE/issues/320)å¯è‡ªè¡Œåˆ›å»ºé•œåƒï¼›å¯å‚è€ƒ[å¤‡æ³¨ï¼ˆå¸¸è§é—®é¢˜ï¼‰](#å¤‡æ³¨å¸¸è§é—®é¢˜)ä½¿ç”¨é•œåƒåŠ é€Ÿï¼‰\n\n```bash\nconda create -n deepke python=3.8\n\nconda activate deepke\n```\n1ï¼‰ åŸºäºpipå®‰è£…ï¼Œç›´æ¥ä½¿ç”¨ ï¼ˆ**ä¸å»ºè®®ä½¿ç”¨æ­¤æ–¹æ³•ï¼Œå­˜åœ¨pythonåŒ…å…¼å®¹å†²çªé£é™©**ï¼‰\n\n```bash\npip install deepke\n```\n- è¯·ç¡®ä¿æ‚¨å½“å‰çš„pipç‰ˆæœ¬<=24.0\n  \n2ï¼‰ åŸºäºæºç å®‰è£…\n\n```bash\npip install -r requirements.txt\n\npython setup.py install\n\npython setup.py develop\n```\n\n**Step 3** ï¼šè¿›å…¥ä»»åŠ¡æ–‡ä»¶å¤¹ï¼Œä»¥å¸¸è§„å…³ç³»æŠ½å–ä¸ºä¾‹\n\n```\ncd DeepKE/example/re/standard\n```\n\n**Step 4**ï¼šä¸‹è½½æ•°æ®é›†ï¼Œæˆ–æ ¹æ®[æ•°æ®æ ‡æ³¨è¯´æ˜](https://github.com/zjunlp/DeepKE/blob/main/README_TAG_CN.md)æ ‡æ³¨æ•°æ®\n```\nwget 120.27.214.45/Data/re/standard/data.tar.gz\n\ntar -xzvf data.tar.gz\n```\n\næ”¯æŒå¤šç§æ•°æ®ç±»å‹æ ¼å¼ï¼Œå…·ä½“è¯·è§å„éƒ¨åˆ†å­READMEã€‚\n\n**Step 5** ï¼šæ¨¡å‹è®­ç»ƒï¼Œè®­ç»ƒç”¨åˆ°çš„å‚æ•°å¯åœ¨confæ–‡ä»¶å¤¹å†…ä¿®æ”¹\n\nDeepKEä½¿ç”¨[wandb](https://docs.wandb.ai/quickstart)æ”¯æŒå¯è§†åŒ–è°ƒå‚\n\n```\npython run.py\n```\n\n**Step 6** ï¼šæ¨¡å‹é¢„æµ‹ã€‚é¢„æµ‹ç”¨åˆ°çš„å‚æ•°å¯åœ¨confæ–‡ä»¶å¤¹å†…ä¿®æ”¹\n\nä¿®æ”¹`conf/predict.yaml`ä¸­ä¿å­˜è®­ç»ƒå¥½çš„æ¨¡å‹è·¯å¾„ã€‚éœ€ä½¿ç”¨æ¨¡å‹çš„ç»å¯¹è·¯å¾„ã€‚å¦‚`xxx/checkpoints/2019-12-03_17-35-30/cnn_epoch21.pth`ã€‚\n```\npython predict.py\n```\n- **â—æ³¨æ„: å¦‚æœæ‚¨åœ¨å®‰è£…æˆ–ä½¿ç”¨è¿‡ç¨‹ä¸­é‡åˆ°ä»»ä½•é—®é¢˜ï¼Œæ‚¨å¯ä»¥æŸ¥çœ‹[å¤‡æ³¨ï¼ˆå¸¸è§é—®é¢˜ï¼‰](#å¤‡æ³¨å¸¸è§é—®é¢˜) æˆ–æäº¤ GitHub issue.**\n\n#### ğŸ³ åŸºäºå®¹å™¨éƒ¨ç½²\n\n**Step1** ä¸‹è½½Dockerå®¢æˆ·ç«¯\n\nä»å®˜ç½‘ä¸‹è½½Dockerå®¢æˆ·ç«¯å¹¶å¯åŠ¨DockeræœåŠ¡\n\n**Step2** æ‹‰å–é•œåƒå¹¶è¿è¡Œå®¹å™¨\n\n```bash\ndocker pull zjunlp/deepke:latest\ndocker run -it zjunlp/deepke:latest /bin/bash\n```\n\nå‰©ä½™æ­¥éª¤åŒ**æ‰‹åŠ¨ç¯å¢ƒéƒ¨ç½²**ä¸€èŠ‚ä¸­çš„**Step 3**åŠåç»­æ­¥éª¤ç›¸åŒ\n\n - **â—æ³¨æ„: æ‚¨å¯ä»¥å‚è€ƒ [Tips](#tips) æ¥åŠ é€Ÿæ‚¨çš„éƒ¨ç½²**\n<br>\n\n## ç¯å¢ƒä¾èµ–\n\n\n### DeepKE\n\n> python == 3.8\n\n- torch>=1.5,<=1.11\n- hydra-core==1.0.6\n- tensorboard==2.4.1\n- matplotlib==3.4.1\n- transformers==4.26.0\n- jieba==0.42.1\n- scikit-learn==0.24.1\n- seqeval==1.2.2\n- opt-einsum==3.3.0\n- wandb==0.12.7\n- ujson==5.6.0\n- huggingface_hub==0.11.0\n- tensorboardX==2.5.1\n- nltk==3.8\n- protobuf==3.20.1\n- numpy==1.21.0\n- ipdb==0.13.11\n- pytorch-crf==0.7.2\n- tqdm==4.66.1\n- openai==0.28.0\n- Jinja2==3.1.2\n- datasets==2.13.2\n- pyhocon==0.3.60\n\n<br>\n\n## å…·ä½“åŠŸèƒ½ä»‹ç»\n\n### 1. å‘½åå®ä½“è¯†åˆ«NER\n\n- å‘½åå®ä½“è¯†åˆ«æ˜¯ä»éç»“æ„åŒ–çš„æ–‡æœ¬ä¸­è¯†åˆ«å‡ºå®ä½“å’Œå…¶ç±»å‹ã€‚æ•°æ®ä¸ºtxtæ–‡ä»¶ï¼Œæ ·å¼èŒƒä¾‹ä¸º(ç”¨æˆ·å¯ä»¥åŸºäºå·¥å…·[Doccano](https://github.com/doccano/doccano)ã€[MarkTool](https://github.com/FXLP/MarkTool)æ ‡æ³¨æ•°æ®ï¼Œä¹Ÿå¯ä»¥é€šè¿‡DeepKEè‡ªå¸¦çš„[å¼±ç›‘ç£åŠŸèƒ½](https://github.com/zjunlp/DeepKE/blob/main/example/ner/prepare-data/README_CN.md)è‡ªåŠ¨å¾—åˆ°æ•°æ®)ï¼š\n\n  |                           Sentence                           |           Person           |    Location    |          Organization          |\n  | :----------------------------------------------------------: | :------------------------: | :------------: | :----------------------------: |\n  | æœ¬æŠ¥åŒ—äº¬9æœˆ4æ—¥è®¯è®°è€…æ¨æ¶ŒæŠ¥é“ï¼šéƒ¨åˆ†çœåŒºäººæ°‘æ—¥æŠ¥å®£ä¼ å‘è¡Œå·¥ä½œåº§è°ˆä¼š9æœˆ3æ—¥åœ¨4æ—¥åœ¨äº¬ä¸¾è¡Œã€‚ |            æ¨æ¶Œ            |      åŒ—äº¬      |            äººæ°‘æ—¥æŠ¥            |\n  | ã€Šçº¢æ¥¼æ¢¦ã€‹ç”±ç‹æ‰¶æ—å¯¼æ¼”ï¼Œå‘¨æ±æ˜Œã€ç‹è’™ã€å‘¨å²­ç­‰å¤šä½ä¸“å®¶å‚ä¸åˆ¶ä½œã€‚ | ç‹æ‰¶æ—ï¼Œå‘¨æ±æ˜Œï¼Œç‹è’™ï¼Œå‘¨å²­ |            |  |\n  | ç§¦å§‹çš‡å…µé©¬ä¿‘ä½äºé™•è¥¿çœè¥¿å®‰å¸‚,æ˜¯ä¸–ç•Œå…«å¤§å¥‡è¿¹ä¹‹ä¸€ã€‚ |           ç§¦å§‹çš‡           | é™•è¥¿çœï¼Œè¥¿å®‰å¸‚ |                          |\n\n- å…·ä½“æµç¨‹è¯·è¿›å…¥è¯¦ç»†çš„READMEä¸­\n  - **[å¸¸è§„å…¨ç›‘ç£STANDARD](https://github.com/zjunlp/DeepKE/tree/main/example/ner/standard)**  \n  \n     ***æˆ‘ä»¬è¿˜æä¾›äº†[å¤§æ¨¡å‹æ”¯æŒ](https://github.com/zjunlp/DeepKE/blob/main/example/llm/README_CN.md)å’Œå¼€ç®±å³ç”¨çš„[DeepKE-cnSchemaç‰¹åˆ«ç‰ˆ](https://github.com/zjunlp/DeepKE/blob/main/README_CNSCHEMA_CN.md)ï¼Œæ— éœ€è®­ç»ƒå³å¯æŠ½å–æ”¯æŒcnSchemaçš„å®ä½“***\n  \n     **Step1**: è¿›å…¥`DeepKE/example/ner/standard`ï¼Œä¸‹è½½æ•°æ®é›†\n     \n     ```bash\n     wget 120.27.214.45/Data/ner/standard/data.tar.gz\n     \n     tar -xzvf data.tar.gz\n     ```\n     \n     **Step2**: æ¨¡å‹è®­ç»ƒ<br>\n     \n     æ•°æ®é›†å’Œå‚æ•°é…ç½®å¯ä»¥åˆ†åˆ«åœ¨`data`å’Œ`conf`æ–‡ä»¶å¤¹ä¸­ä¿®æ”¹\n     \n     ```\n     python run.py\n     ```\n     \n     **Step3**: æ¨¡å‹é¢„æµ‹\n     ```\n     python predict.py\n     ```\n     \n  - **[å°‘æ ·æœ¬FEW-SHOT](https://github.com/zjunlp/DeepKE/tree/main/example/ner/few-shot)** \n  \n    **Step1**: è¿›å…¥`DeepKE/example/ner/few-shot`ï¼Œä¸‹è½½æ•°æ®é›†\n    \n    ```bash\n    wget 120.27.214.45/Data/ner/few_shot/data.tar.gz\n    \n    tar -xzvf data.tar.gz\n    ```\n    \n    **Step2**ï¼šä½èµ„æºåœºæ™¯ä¸‹è®­ç»ƒæ¨¡å‹<br>\n    \n    æ¨¡å‹åŠ è½½å’Œä¿å­˜ä½ç½®ä»¥åŠå‚æ•°é…ç½®å¯ä»¥åœ¨`conf`æ–‡ä»¶å¤¹ä¸­ä¿®æ”¹\n    \n     ```\n     python run.py +train=few_shot\n     ```\n    \n    è‹¥è¦åŠ è½½æ¨¡å‹ï¼Œä¿®æ”¹`few_shot.yaml`ä¸­çš„`load_path`ï¼›<br>\n    \n    **Step3**ï¼šåœ¨`config.yaml`ä¸­è¿½åŠ `- predict`ï¼Œ`predict.yaml`ä¸­ä¿®æ”¹`load_path`ä¸ºæ¨¡å‹è·¯å¾„ä»¥åŠ`write_path`ä¸ºé¢„æµ‹ç»“æœçš„ä¿å­˜è·¯å¾„ï¼Œå®Œæˆä¿®æ”¹åä½¿ç”¨\n    \n    ```\n    python predict.py\n    ```\n\n  - **[å¤šæ¨¡æ€](https://github.com/zjunlp/DeepKE/tree/main/example/ner/multimodal)**\n\n    **Step1**: è¿›å…¥ `DeepKE/example/ner/multimodal`ï¼Œ ä¸‹è½½æ•°æ®é›†\n\n    ```bash\n    wget 120.27.214.45/Data/ner/multimodal/data.tar.gz\n    \n    tar -xzvf data.tar.gz\n    ```\n\n    æˆ‘ä»¬åœ¨åŸå§‹å›¾åƒä¸Šä½¿ç”¨[faster_rcnn](https://github.com/pytorch/vision/blob/main/torchvision/models/detection/faster_rcnn.py)å’Œ[visual groundingå·¥å…·](https://github.com/zyang-ur/onestage_grounding)åˆ†åˆ«æŠ½å–RCNN objectså’Œvisual grounding objectsæ¥ä½œä¸ºå±€éƒ¨è§†è§‰ä¿¡æ¯\n\n    **Step2** å¤šæ¨¡æ€åœºæ™¯ä¸‹è®­ç»ƒæ¨¡å‹ <br>\n\n    - æ•°æ®é›†å’Œå‚æ•°é…ç½®å¯ä»¥åˆ†åˆ«è¿›å…¥`data`å’Œ`conf`æ–‡ä»¶å¤¹ä¸­ä¿®æ”¹\n    - å¦‚éœ€ä»ä¸Šæ¬¡è®­ç»ƒçš„æ¨¡å‹å¼€å§‹è®­ç»ƒï¼šè®¾ç½®`conf/train.yaml`ä¸­çš„`load_path`ä¸ºä¸Šæ¬¡ä¿å­˜æ¨¡å‹çš„è·¯å¾„ï¼Œæ¯æ¬¡è®­ç»ƒçš„æ—¥å¿—é»˜è®¤ä¿å­˜åœ¨æ ¹ç›®å½•ï¼Œå¯ç”¨`log_dir`æ¥é…ç½®\n\n    ```bash\n    python run.py\n    ```\n\n    **Step3** æ¨¡å‹é¢„æµ‹\n\n    ```bash\n    python predict.py\n    ```\n\n### 2. å…³ç³»æŠ½å–RE\n\n- å…³ç³»æŠ½å–æ˜¯ä»éç»“æ„åŒ–çš„æ–‡æœ¬ä¸­æŠ½å–å‡ºå®ä½“ä¹‹é—´çš„å…³ç³»ï¼Œä»¥ä¸‹ä¸ºå‡ ä¸ªæ ·å¼èŒƒä¾‹ï¼Œæ•°æ®ä¸ºcsvæ–‡ä»¶(ç”¨æˆ·å¯ä»¥åŸºäºå·¥å…·[Doccano](https://github.com/doccano/doccano)ã€[MarkTool](https://github.com/FXLP/MarkTool)æ ‡æ³¨æ•°æ®ï¼Œä¹Ÿå¯ä»¥é€šè¿‡DeepKEè‡ªå¸¦çš„[å¼±ç›‘ç£åŠŸèƒ½](https://github.com/zjunlp/DeepKE/blob/main/example/re/prepare-data/README_CN.md)è‡ªåŠ¨å¾—åˆ°æ•°æ®)ï¼š\n\n  |                        Sentence                        | Relation |    Head    | Head_offset |    Tail    | Tail_offset |\n  | :----------------------------------------------------: | :------: | :--------: | :---------: | :--------: | :---------: |\n  | ã€Šå²³çˆ¶ä¹Ÿæ˜¯çˆ¹ã€‹æ˜¯ç‹å†›æ‰§å¯¼çš„ç”µè§†å‰§ï¼Œç”±é©¬æ©ç„¶ã€èŒƒæ˜ä¸»æ¼”ã€‚ |   å¯¼æ¼”   | å²³çˆ¶ä¹Ÿæ˜¯çˆ¹ |      1      |    ç‹å†›    |      8      |\n  |  ã€Šä¹ç„ç ã€‹æ˜¯åœ¨çºµæ¨ªä¸­æ–‡ç½‘è¿è½½çš„ä¸€éƒ¨å°è¯´ï¼Œä½œè€…æ˜¯é¾™é©¬ã€‚  | è¿è½½ç½‘ç«™ |   ä¹ç„ç    |      1      | çºµæ¨ªä¸­æ–‡ç½‘ |      7      |\n  |     æèµ·æ­å·çš„ç¾æ™¯ï¼Œè¥¿æ¹–æ€»æ˜¯ç¬¬ä¸€ä¸ªæ˜ å…¥è„‘æµ·çš„è¯è¯­ã€‚     | æ‰€åœ¨åŸå¸‚ |    è¥¿æ¹–    |      8      |    æ­å·    |      2      |\n  \n- **â—NOTE: å¦‚æœæ‚¨ä½¿ç”¨çš„åŒä¸€ä¸ªå…³ç³»å­˜åœ¨å¤šç§å®ä½“ç±»å‹ï¼Œå¯ä»¥é‡‡å–å¯¹å®ä½“ç±»å‹åŠ å…³ç³»å‰ç¼€çš„æ–¹å¼æ„é€ è¾“å…¥ã€‚**\n\n- å…·ä½“æµç¨‹è¯·è¿›å…¥è¯¦ç»†çš„READMEä¸­ï¼ŒREåŒ…æ‹¬äº†ä»¥ä¸‹ä¸‰ä¸ªå­åŠŸèƒ½\n  - **[å¸¸è§„å…¨ç›‘ç£STANDARD](https://github.com/zjunlp/DeepKE/tree/main/example/re/standard)**  \n\n     ***æˆ‘ä»¬è¿˜æä¾›äº†[å¤§æ¨¡å‹æ”¯æŒ](https://github.com/zjunlp/DeepKE/blob/main/example/llm/README_CN.md)å’Œå¼€ç®±å³ç”¨çš„[DeepKE-cnSchemaç‰¹åˆ«ç‰ˆ](https://github.com/zjunlp/DeepKE/blob/main/README_CNSCHEMA_CN.md)ï¼Œæ— éœ€è®­ç»ƒå³å¯æŠ½å–æ”¯æŒcnSchemaçš„å…³ç³»***\n\n    **Step1**ï¼šè¿›å…¥`DeepKE/example/re/standard`ï¼Œä¸‹è½½æ•°æ®é›†\n  \n    ```bash\n    wget 120.27.214.45/Data/re/standard/data.tar.gz\n    \n    tar -xzvf data.tar.gz\n    ```\n  \n    **Step2**ï¼šæ¨¡å‹è®­ç»ƒ<br>\n\n    æ•°æ®é›†å’Œå‚æ•°é…ç½®å¯ä»¥åˆ†åˆ«è¿›å…¥`data`å’Œ`conf`æ–‡ä»¶å¤¹ä¸­ä¿®æ”¹\n  \n    ```\n    python run.py\n    ```\n  \n    **Step3**ï¼šæ¨¡å‹é¢„æµ‹\n  \n    ```\n    python predict.py\n    ```\n  \n  - **[å°‘æ ·æœ¬FEW-SHOT](https://github.com/zjunlp/DeepKE/tree/main/example/re/few-shot)**\n  \n    **Step1**ï¼šè¿›å…¥`DeepKE/example/re/few-shot`ï¼Œä¸‹è½½æ•°æ®é›†\n\n    ```bash\n    wget 120.27.214.45/Data/re/few_shot/data.tar.gz\n    \n    tar -xzvf data.tar.gz\n    ```\n  \n    **Step2**ï¼šæ¨¡å‹è®­ç»ƒ<br>\n  \n    - æ•°æ®é›†å’Œå‚æ•°é…ç½®å¯ä»¥åˆ†åˆ«è¿›å…¥`data`å’Œ`conf`æ–‡ä»¶å¤¹ä¸­ä¿®æ”¹\n  \n    - å¦‚éœ€ä»ä¸Šæ¬¡è®­ç»ƒçš„æ¨¡å‹å¼€å§‹è®­ç»ƒï¼šè®¾ç½®`conf/train.yaml`ä¸­çš„`train_from_saved_model`ä¸ºä¸Šæ¬¡ä¿å­˜æ¨¡å‹çš„è·¯å¾„ï¼Œæ¯æ¬¡è®­ç»ƒçš„æ—¥å¿—é»˜è®¤ä¿å­˜åœ¨æ ¹ç›®å½•ï¼Œå¯ç”¨`log_dir`æ¥é…ç½®\n  \n    ```\n    python run.py\n    ```\n  \n    **Step3**ï¼šæ¨¡å‹é¢„æµ‹\n  \n    ```\n    python predict.py\n    ```\n  \n  - **[æ–‡æ¡£çº§DOCUMENT](https://github.com/zjunlp/DeepKE/tree/main/example/re/document)** <br>\n    \n    **Step1**ï¼šè¿›å…¥`DeepKE/example/re/document`ï¼Œä¸‹è½½æ•°æ®é›†\n    \n    ```bash\n    wget 120.27.214.45/Data/re/document/data.tar.gz\n    \n    tar -xzvf data.tar.gz\n    ```\n    \n    **Step2**ï¼šæ¨¡å‹è®­ç»ƒ<br>\n    \n    - æ•°æ®é›†å’Œå‚æ•°é…ç½®å¯ä»¥åˆ†åˆ«è¿›å…¥`data`å’Œ`conf`æ–‡ä»¶å¤¹ä¸­ä¿®æ”¹\n    - å¦‚éœ€ä»ä¸Šæ¬¡è®­ç»ƒçš„æ¨¡å‹å¼€å§‹è®­ç»ƒï¼šè®¾ç½®`conf/train.yaml`ä¸­çš„`train_from_saved_model`ä¸ºä¸Šæ¬¡ä¿å­˜æ¨¡å‹çš„è·¯å¾„ï¼Œæ¯æ¬¡è®­ç»ƒçš„æ—¥å¿—é»˜è®¤ä¿å­˜åœ¨æ ¹ç›®å½•ï¼Œå¯ç”¨`log_dir`æ¥é…ç½®ï¼›\n    \n    ```\n    python run.py\n    ```\n    **Step3**ï¼šæ¨¡å‹é¢„æµ‹\n    \n    ```\n    python predict.py\n    ```\n\n  - **[å¤šæ¨¡æ€](https://github.com/zjunlp/DeepKE/tree/main/example/re/multimodal)**\n\n    **Step1**: è¿›å…¥ `DeepKE/example/re/multimodal`ï¼Œ ä¸‹è½½æ•°æ®é›†\n\n    ```bash\n    wget 120.27.214.45/Data/re/multimodal/data.tar.gz\n    \n    tar -xzvf data.tar.gz\n    ```\n\n    æˆ‘ä»¬åœ¨åŸå§‹å›¾åƒä¸Šä½¿ç”¨[faster_rcnn](https://github.com/pytorch/vision/blob/main/torchvision/models/detection/faster_rcnn.py)å’Œ[visual groundingå·¥å…·](https://github.com/zyang-ur/onestage_grounding)åˆ†åˆ«æŠ½å–RCNN objectså’Œvisual grounding objectsæ¥ä½œä¸ºå±€éƒ¨è§†è§‰ä¿¡æ¯\n\n    **Step2** æ¨¡å‹è®­ç»ƒ <br>\n\n    - æ•°æ®é›†å’Œå‚æ•°é…ç½®å¯ä»¥åˆ†åˆ«è¿›å…¥`data`å’Œ`conf`æ–‡ä»¶å¤¹ä¸­ä¿®æ”¹\n    - å¦‚éœ€ä»ä¸Šæ¬¡è®­ç»ƒçš„æ¨¡å‹å¼€å§‹è®­ç»ƒï¼šè®¾ç½®`conf/train.yaml`ä¸­çš„`load_path`ä¸ºä¸Šæ¬¡ä¿å­˜æ¨¡å‹çš„è·¯å¾„ï¼Œæ¯æ¬¡è®­ç»ƒçš„æ—¥å¿—é»˜è®¤ä¿å­˜åœ¨æ ¹ç›®å½•ï¼Œå¯ç”¨`log_dir`æ¥é…ç½®\n\n    ```bash\n    python run.py\n    ```\n\n    **Step3** æ¨¡å‹é¢„æµ‹\n\n    ```bash\n    python predict.py\n    ```\n\n### 3. å±æ€§æŠ½å–AE\n\n- æ•°æ®ä¸ºcsvæ–‡ä»¶ï¼Œæ ·å¼èŒƒä¾‹ä¸ºï¼š\n\n  |                           Sentence                           |   Att    |   Ent    | Ent_offset |      Val      | Val_offset |\n  | :----------------------------------------------------------: | :------: | :------: | :--------: | :-----------: | :--------: |\n  |          å¼ å†¬æ¢…ï¼Œå¥³ï¼Œæ±‰æ—ï¼Œ1968å¹´2æœˆç”Ÿï¼Œæ²³å—æ·‡å¿äºº           |   æ°‘æ—   |  å¼ å†¬æ¢…  |     0      |     æ±‰æ—      |     6      |\n  | è¯¸è‘›äº®ï¼Œå­—å­”æ˜ï¼Œä¸‰å›½æ—¶æœŸæ°å‡ºçš„å†›äº‹å®¶ã€æ–‡å­¦å®¶ã€å‘æ˜å®¶ã€‚ |   æœä»£   |   è¯¸è‘›äº®   |     0      |     ä¸‰å›½æ—¶æœŸ      |     8     |\n  |        2014å¹´10æœˆ1æ—¥è®¸éåæ‰§å¯¼çš„ç”µå½±ã€Šé»„é‡‘æ—¶ä»£ã€‹ä¸Šæ˜          | ä¸Šæ˜ æ—¶é—´ | é»„é‡‘æ—¶ä»£ |     19     | 2014å¹´10æœˆ1æ—¥ |     0      |\n\n- å…·ä½“æµç¨‹è¯·è¿›å…¥è¯¦ç»†çš„READMEä¸­\n  - **[å¸¸è§„å…¨ç›‘ç£STANDARD](https://github.com/zjunlp/DeepKE/tree/main/example/ae/standard)**  \n    \n    **Step1**ï¼šè¿›å…¥`DeepKE/example/ae/standard`ï¼Œä¸‹è½½æ•°æ®é›†\n    \n    ```bash\n    wget 120.27.214.45/Data/ae/standard/data.tar.gz\n    \n    tar -xzvf data.tar.gz\n    ```\n    \n    **Step2**ï¼šæ¨¡å‹è®­ç»ƒ<br>\n\n    æ•°æ®é›†å’Œå‚æ•°é…ç½®å¯ä»¥åˆ†åˆ«è¿›å…¥`data`å’Œ`conf`æ–‡ä»¶å¤¹ä¸­ä¿®æ”¹\n    \n    ```\n    python run.py\n    ```\n    \n    **Step3**ï¼šæ¨¡å‹é¢„æµ‹\n    \n    ```\n    python predict.py\n    ```\n\n<br>\n\n### 4.äº‹ä»¶æŠ½å–\n\n* äº‹ä»¶æŠ½å–æ˜¯æŒ‡ä»ä¸€æ®µæ— ç»“æ„åŒ–çš„æ–‡æœ¬ä¸­æŠ½å–å‡ºæŸä¸ªäº‹ä»¶çš„äº‹ä»¶ç±»å‹ã€äº‹ä»¶è§¦å‘è¯ã€è®ºå…ƒè§’è‰²ä»¥åŠè®ºå…ƒã€‚\n\n* æ•°æ®ä¸º`.tsv`æ–‡ä»¶ï¼Œæ ·ä¾‹ä¸ºï¼š\n\n  <table h style=\"text-align:center\">\n      <tr>\n          <th colspan=\"2\"> Sentence </th>\n          <th> Event type </th>\n          <th> Trigger </th>\n          <th> Role </th>\n          <th> Argument </th>\n      </tr>\n      <tr> \n          <td rowspan=\"3\" colspan=\"2\"> æ®ã€Šæ¬§æ´²æ—¶æŠ¥ã€‹æŠ¥é“ï¼Œå½“åœ°æ—¶é—´27æ—¥ï¼Œæ³•å›½å·´é»å¢æµ®å®«åšç‰©é¦†å‘˜å·¥å› ä¸æ»¡å·¥ä½œæ¡ä»¶æ¶åŒ–è€Œç½¢å·¥ï¼Œå¯¼è‡´è¯¥åšç‰©é¦†ä¹Ÿå› æ­¤é—­é—¨è°¢å®¢ä¸€å¤©ã€‚ </td>\n        \t<td rowspan=\"3\"> ç»„ç»‡è¡Œä¸º-ç½¢å·¥ </td>\n      \t\t<td rowspan=\"3\"> ç½¢å·¥ </td>\n      \t\t<td> ç½¢å·¥äººå‘˜ </td>\n      \t\t<td> æ³•å›½å·´é»å¢æµ®å®«åšç‰©é¦†å‘˜å·¥ </td>\n      </tr>\n      <tr> \n          <td> æ—¶é—´ </td>\n          <td> å½“åœ°æ—¶é—´27æ—¥ </td>\n      </tr>\n      <tr> \n          <td> æ‰€å±ç»„ç»‡ </td>\n          <td> æ³•å›½å·´é»å¢æµ®å®«åšç‰©é¦† </td>\n      </tr>\n      <tr> \n          <td rowspan=\"3\" colspan=\"2\"> ä¸­å›½å¤–è¿2019å¹´ä¸ŠåŠå¹´å½’æ¯å‡€åˆ©æ¶¦å¢é•¿17%ï¼šæ”¶è´­äº†å°‘æ•°è‚¡ä¸œè‚¡æƒ </td>\n        \t<td rowspan=\"3\"> è´¢ç»/äº¤æ˜“-å‡ºå”®/æ”¶è´­ </td>\n      \t\t<td rowspan=\"3\"> æ”¶è´­ </td>\n      \t\t<td> å‡ºå”®æ–¹ </td>\n      \t\t<td> å°‘æ•°è‚¡ä¸œ </td>\n      </tr>\n      <tr> \n          <td> æ”¶è´­æ–¹ </td>\n          <td> ä¸­å›½å¤–è¿ </td>\n      </tr>\n      <tr> \n          <td> äº¤æ˜“ç‰© </td>\n          <td> è‚¡æƒ </td>\n      </tr>\n      <tr> \n          <td rowspan=\"3\" colspan=\"2\"> ç¾å›½äºšç‰¹å…°å¤§èˆªå±•13æ—¥å‘ç”Ÿä¸€èµ·è¡¨æ¼”æœºå æœºäº‹æ•…ï¼Œé£è¡Œå‘˜å¼¹å°„å‡ºèˆ±å¹¶å®‰å…¨ç€é™†ï¼Œäº‹æ•…æ²¡æœ‰é€ æˆäººå‘˜ä¼¤äº¡ã€‚ </td>\n        \t<td rowspan=\"3\"> ç¾å®³/æ„å¤–-å æœº </td>\n      \t\t<td rowspan=\"3\"> å æœº </td>\n      \t\t<td> æ—¶é—´ </td>\n      \t\t<td> 13æ—¥ </td>\n      </tr>\n      <tr> \n          <td> åœ°ç‚¹ </td>\n          <td> ç¾å›½äºšç‰¹å…° </td>\n    \t</tr>\n  </table>\n\n- å…·ä½“æµç¨‹è¯·è¿›å…¥è¯¦ç»†çš„READMEä¸­\n\n  - **[å¸¸è§„å…¨ç›‘ç£STANDARD](./example/ee/standard/README_CN.md)**  \n\n    **Step1**ï¼šè¿›å…¥`DeepKE/example/ee/standard`ï¼Œä¸‹è½½æ•°æ®é›†\n\n    ```bash\n    wget 120.27.214.45/Data/ee/DuEE.zip\n    unzip DuEE.zip\n    ```\n\n    **Step2**ï¼šæ¨¡å‹è®­ç»ƒ<br>\n\n    æ•°æ®é›†å’Œå‚æ•°é…ç½®å¯ä»¥åˆ†åˆ«è¿›å…¥`data`å’Œ`conf`æ–‡ä»¶å¤¹ä¸­ä¿®æ”¹\n\n    ```\n    python run.py\n    ```\n\n    **Step3**ï¼šæ¨¡å‹é¢„æµ‹\n\n    ```\n    python predict.py\n    ```\n\n<br>\n\n# å¤‡æ³¨ï¼ˆå¸¸è§é—®é¢˜ï¼‰\n\n1.ä½¿ç”¨ Anaconda æ—¶ï¼Œ```å»ºè®®æ·»åŠ å›½å†…é•œåƒ```ï¼Œä¸‹è½½é€Ÿåº¦æ›´å¿«ã€‚å¦‚[é•œåƒ](https://mirrors.tuna.tsinghua.edu.cn/help/anaconda/)ã€‚\n\n2.ä½¿ç”¨ pip æ—¶ï¼Œ```å»ºè®®ä½¿ç”¨å›½å†…é•œåƒ```ï¼Œä¸‹è½½é€Ÿåº¦æ›´å¿«ï¼Œå¦‚é˜¿é‡Œäº‘é•œåƒã€‚\n\n3.å®‰è£…åæç¤º `ModuleNotFoundError: No module named 'past'`ï¼Œè¾“å…¥å‘½ä»¤ `pip install future` å³å¯è§£å†³ã€‚\n\n4.ä½¿ç”¨è¯­è¨€é¢„è®­ç»ƒæ¨¡å‹æ—¶ï¼Œåœ¨çº¿å®‰è£…ä¸‹è½½æ¨¡å‹æ¯”è¾ƒæ…¢ï¼Œæ›´å»ºè®®æå‰ä¸‹è½½å¥½ï¼Œå­˜æ”¾åˆ° pretrained æ–‡ä»¶å¤¹å†…ã€‚å…·ä½“å­˜æ”¾æ–‡ä»¶è¦æ±‚è§æ–‡ä»¶å¤¹å†…çš„ `README.md`ã€‚\n\n5.DeepKEè€ç‰ˆæœ¬ä½äº[deepke-v1.0](https://github.com/zjunlp/DeepKE/tree/deepke-v1.0)åˆ†æ”¯ï¼Œç”¨æˆ·å¯åˆ‡æ¢åˆ†æ”¯ä½¿ç”¨è€ç‰ˆæœ¬ï¼Œè€ç‰ˆæœ¬çš„èƒ½åŠ›å·²å…¨éƒ¨è¿ç§»åˆ°æ ‡å‡†è®¾å®šå…³ç³»æŠ½å–([example/re/standard](https://github.com/zjunlp/DeepKE/blob/main/example/re/standard/README.md))ä¸­ã€‚\n\n6.å¦‚æœæ‚¨éœ€è¦åœ¨æºç çš„åŸºç¡€ä¸Šè¿›è¡Œä¿®æ”¹ï¼Œå»ºè®®ä½¿ç”¨`python setup.py install`æ–¹å¼å®‰è£…*DeepKE*ï¼Œå¦‚æœªä½¿ç”¨è¯¥æ–¹å¼å®‰è£…ï¼Œæºç ä¿®æ”¹éƒ¨åˆ†ä¸ä¼šç”Ÿæ•ˆï¼Œè§[é—®é¢˜](https://github.com/zjunlp/DeepKE/issues/117)ã€‚\n\n7.æ›´å¤šçš„ä½èµ„æºæŠ½å–å·¥ä½œå¯æŸ¥é˜…è®ºæ–‡ [Knowledge Extraction in Low-Resource Scenarios: Survey and Perspective](https://arxiv.org/pdf/2202.08063.pdf)ã€‚\n\n8.ç¡®ä¿ä½¿ç”¨requirements.txtä¸­å¯¹åº”çš„å„ä¾èµ–åŒ…çš„ç‰ˆæœ¬ã€‚\n\n<br>\n\n# æœªæ¥è®¡åˆ’\n\n- åœ¨DeepKEçš„ä¸‹ä¸€ä¸ªç‰ˆæœ¬ä¸­å‘å¸ƒä¼˜åŒ–åçš„ä¸­è‹±åŒè¯­æŠ½å–å¤§æ¨¡å‹\n- æˆ‘ä»¬æä¾›é•¿æœŸæŠ€æœ¯ç»´æŠ¤å’Œç­”ç–‘è§£æƒ‘ã€‚å¦‚æœ‰ç–‘é—®ï¼Œè¯·æäº¤issues\n\n\n# é˜…è¯»èµ„æ–™\n\nData-Efficient Knowledge Graph Construction, é«˜æ•ˆçŸ¥è¯†å›¾è°±æ„å»º ([Tutorial on CCKS 2022](http://sigkg.cn/ccks2022/?page_id=24)) \\[[slides](https://pan.baidu.com/s/1yMskUVU188-4dcf96lVrWg?pwd=gy8y)\\] \n\nEfficient and Robust Knowledge Graph Construction ([Tutorial on AACL-IJCNLP 2022](https://www.aacl2022.org/Program/tutorials)) \\[[slides](https://github.com/NLP-Tutorials/AACL-IJCNLP2022-KGC-Tutorial)\\] \n\nPromptKG Family: a Gallery of Prompt Learning & KG-related Research Works, Toolkits, and Paper-list [[Resources](https://github.com/zjunlp/PromptKG)\\] \n\nKnowledge Extraction in Low-Resource Scenarios: Survey and Perspective \\[[Survey](https://arxiv.org/abs/2202.08063)\\]\\[[Paper-list](https://github.com/zjunlp/Low-resource-KEPapers)\\]\n\nåŸºäºå¤§æ¨¡å‹æç¤ºå­¦ä¹ çš„æ¨ç†å·¥ä½œç»¼è¿° \\[[è®ºæ–‡](https://arxiv.org/abs/2212.09597)\\]\\[[åˆ—è¡¨](https://github.com/zjunlp/Prompt4ReasoningPapers)\\]\\[[ppt](https://github.com/zjunlp/Prompt4ReasoningPapers/blob/main/tutorial.pdf)\\]\n\n# ç›¸å…³å·¥å…·\n\n[Doccano](https://github.com/doccano/doccano)ã€[MarkTool](https://github.com/FXLP/MarkTool)ã€[LabelStudio](https://labelstud.io/ )ï¼šå®ä½“è¯†åˆ«å…³ç³»æŠ½å–æ•°æ®æ ‡æ³¨å·¥å…·\n\n[LambdaKG](https://github.com/zjunlp/PromptKG/tree/main/lambdaKG): åŸºäºé¢„è®­ç»ƒè¯­è¨€æ¨¡å‹çš„çŸ¥è¯†å›¾è°±è¡¨ç¤ºä¸åº”ç”¨å·¥å…·\n\n[EasyInstruct](https://github.com/zjunlp/EasyInstruct): ä¸€ä¸ªåŸºäºæŒ‡ä»¤ä½¿ç”¨å¤§æ¨¡å‹çš„å·¥å…·\n\n\n# å¼•ç”¨\n\nå¦‚æœä½¿ç”¨DeepKEï¼Œè¯·æŒ‰ä»¥ä¸‹æ ¼å¼å¼•ç”¨\n\n```bibtex\n@inproceedings{DBLP:conf/emnlp/ZhangXTYYQXCLL22,\n  author    = {Ningyu Zhang and\n               Xin Xu and\n               Liankuan Tao and\n               Haiyang Yu and\n               Hongbin Ye and\n               Shuofei Qiao and\n               Xin Xie and\n               Xiang Chen and\n               Zhoubo Li and\n               Lei Li},\n  editor    = {Wanxiang Che and\n               Ekaterina Shutova},\n  title     = {DeepKE: {A} Deep Learning Based Knowledge Extraction Toolkit for Knowledge\n               Base Population},\n  booktitle = {Proceedings of the The 2022 Conference on Empirical Methods in Natural\n               Language Processing, {EMNLP} 2022 - System Demonstrations, Abu Dhabi,\n               UAE, December 7-11, 2022},\n  pages     = {98--108},\n  publisher = {Association for Computational Linguistics},\n  year      = {2022},\n  url       = {https://aclanthology.org/2022.emnlp-demos.10},\n  timestamp = {Thu, 23 Mar 2023 16:56:00 +0100},\n  biburl    = {https://dblp.org/rec/conf/emnlp/ZhangXTYYQXCLL22.bib},\n  bibsource = {dblp computer science bibliography, https://dblp.org}\n}\n```\n\n<br>\n\n# é¡¹ç›®è´¡çŒ®äººå‘˜\n\n[å¼ å®è±«](https://person.zju.edu.cn/ningyu)ã€[ç‹æ˜Šå¥‹](https://tjdi.tongji.edu.cn/TeacherDetail.do?id=4991&lang=_cn)ã€é»„éã€ç†Šé£å®‡ã€é™¶è”å®½ã€å¾æ¬£ã€æ¡‚é¸¿æµ©ã€å¼ çèŒ¹ã€è°­ä¼ å¥‡ã€é™ˆå¼ºã€ç‹æ½‡å¯’ã€ä¹ æ³½å¤ã€ææ¬£è£ã€ä½™æµ·é˜³ã€å¶å®å½¬ã€ä¹”ç¡•æ–ã€ç‹é¹ã€æœ±é›¨ç¦ã€è°¢è¾›ã€é™ˆæƒ³ã€é»æ´²æ³¢ã€æç£Šã€æ¢å­è½¬ã€å§šäº‘å¿—ã€é™ˆé™ã€æœ±é›¨ç¦ã€é‚“æ·‘æ•ã€å¼ æ–‡ã€éƒ‘å›½è½´ã€é™ˆåé’§\n\nå¼€æºç¤¾åŒºè´¡çŒ®è€…: [thredreams](https://github.com/thredreams)ã€[eltociear](https://github.com/eltociear)ã€å¾å­æ–‡ã€é»„ç¿ã€ç¿æ™“é¾™\n\n\n# å…¶å®ƒçŸ¥è¯†æŠ½å–å¼€æºå·¥å…·\n\n- [CogIE](https://github.com/jinzhuoran/CogIE)\n- [OpenNRE](https://github.com/thunlp/OpenNRE)\n- [OmniEvent](https://github.com/THU-KEG/OmniEvent)\n- [OpenUE](https://github.com/zjunlp/OpenUE)\n- [OpenIE](https://stanfordnlp.github.io/CoreNLP/openie.html)\n- [RESIN](https://github.com/RESIN-KAIROS/RESIN-pipeline-public)\n- [ZShot](https://github.com/IBM/zshot)\n- [OmniEvent](https://github.com/THU-KEG/OmniEvent)\n"
        },
        {
          "name": "README_CNSCHEMA.md",
          "type": "blob",
          "size": 22.9423828125,
          "content": "<p align=\"center\">\n    <a href=\"https://github.com/zjunlp/deepke\"> <img src=\"pics/logo_cnschema.png\" width=\"400\"/></a>\n\n<p>\n<p align=\"center\">  \n    <a href=\"http://deepke.zjukg.cn\">\n        <img alt=\"Documentation\" src=\"https://img.shields.io/badge/demo-website-blue\">\n    </a>\n    <a href=\"https://pypi.org/project/deepke/#files\">\n        <img alt=\"PyPI\" src=\"https://img.shields.io/pypi/v/deepke\">\n    </a>\n    <a href=\"https://github.com/zjunlp/DeepKE/blob/master/LICENSE\">\n        <img alt=\"GitHub\" src=\"https://img.shields.io/github/license/zjunlp/deepke\">\n    </a>\n    <a href=\"http://zjunlp.github.io/DeepKE\">\n        <img alt=\"Documentation\" src=\"https://img.shields.io/badge/doc-website-red\">\n    </a>\n    <a href=\"https://colab.research.google.com/drive/1vS8YJhJltzw3hpJczPt24O0Azcs3ZpRi?usp=sharing\">\n        <img alt=\"Open In Colab\" src=\"https://colab.research.google.com/assets/colab-badge.svg\">\n    </a>\n</p>\n\n<p align=\"center\">\n    <b> English | <a href=\"https://github.com/zjunlp/DeepKE/blob/main/README_CNSCHEMA_CN.md\">ç®€ä½“ä¸­æ–‡</a> </b>\n</p>\n\n<h1 align=\"center\">\n    <p>Off-the-shelf Special Edition for Chinese Knowledge Extraction Toolkitâ€”â€”DeepKE-cnSchema</p>\n</h1>\n\nDeepKE is a knowledge extraction toolkit based on PyTorch,  supporting **low-resource**, **document-level** and **multimodal** scenarios for **entity**, **relation** and **attribute** extraction. DeepKE-cnSchema is an off-the-shelf version. Users can download the model to realize entity and relation knowledge extraction directly without training.\n\n---\n\n## Catalogue\n\n| Chapter                                                                              | Description                                                                                |\n| ------------------------------------------------------------------------------------ | ------------------------------------------------------------------------------------------ |\n| [Introduction](#Introduction)                                                           | Introduce the basic principles of DeepKE-cnSchema                                          |\n| [Chinese Model Download](#Chinese-Model-Download)                                       | Provide the download address of DeepKE-cnSchema                                            |\n| [Datasets and Chinese Baseline Performance](#Datasets-and-Chinese-Baseline-Performance) | Report the performance of Chinese models                                     |\n| [Quick Load](#Quick-Load)                                                               | Introduce how to use DeepKE-cnSchema for entity and relation extraction |\n| [User-defined Model](#User-defined-Model)                                               | Provide instructions for training models with customized datasets                           |\n| [FAQ](#FAQ)                                                                             | FAQ                                                                                        |\n| [Citation](#Citation)                                                                   | Technical report of this catalogue                                                         |\n\n## Introduction\n\nDeepKE is a knowledge extraction toolkit supporting **low-resource**, **document-level** and **multimodal** scenarios for *entity*, *relation* and *attribute* extraction. We provide [documents](https://zjunlp.github.io/DeepKE/), [Google Colab tutorials](https://colab.research.google.com/drive/1vS8YJhJltzw3hpJczPt24O0Azcs3ZpRi?usp=sharing), [online demo](http://deepke.zjukg.cn/), and [slides](https://github.com/zjunlp/DeepKE/blob/main/docs/slides/Slides-DeepKE-en.pdf) for beginners.\n\nTo promote efficient Chinese knowledge graph construction, we provide DeepKE-cnSchema, a specific version of DeepKE, containing off-the-shelf models based on [cnSchema](https://github.com/OpenKG-ORG/cnSchema). DeepKE-cnSchema supports multiple tasks such as Chinese entity extraction and relation extraction. It can extract 50 relation types and 28 entity types, including common entity types such as person, location, city, institution, etc and the common relation types such as ancestral home, birthplace, nationality and other types.\n\n## Chinese Model Download\n\nFor entity extraction and relation extraction tasks, we provide models based on `RoBERTa-wwm-ext, Chinese` and `BERT-wwm, Chinese` respectively.\n\n| Model                                               | Task                          |                                     Google Download                                     |                              Baidu Netdisk Download                               |\n| :-------------------------------------------------- | :---------------------------- |:---------------------------------------------------------------------------------------:|:---------------------------------------------------------------------------------:|\n| **`DeepKE(NER), RoBERTa-wwm-ext, Chinese`** | **entity extraction**   | **[PyTorch](https://drive.google.com/drive/folders/1T3xf_MXRaVqLV-ST4VqvKoaQqQgRpp67)** |   **[Pytorchï¼ˆpassword:u022ï¼‰](https://pan.baidu.com/s/1hb9XEbK4x5fIyco4DgZZfg)**   |\n| **`DeepKE(NER), BERT-wwm, Chinese`**        | **entity extraction**   | **[PyTorch](https://drive.google.com/drive/folders/1zA8Ichx9nzU3GD92ptdyR_nmARB_7ovg)** |   **[Pytorchï¼ˆpassword:1g0tï¼‰](https://pan.baidu.com/s/10TWE1VA2S-SJgmOm8szRxw)**   |\n| **`DeepKE(NER), BiLSTM-CRF, Chinese`**      | **entity extraction** | **[PyTorch](https://drive.google.com/drive/folders/1n1tzvl6hZYoUUFFWLfkuhkXPx5JB4XK_)** |   **[Pytorchï¼ˆpassword:my4xï¼‰](https://pan.baidu.com/s/1a9ZFFZVQUxmlbLmbVBaTqQ)**   |\n| **`DeepKE(RE), RoBERTa-wwm-ext, Chinese`**  | **relation extraction** | **[PyTorch](https://drive.google.com/drive/folders/1wb_QIZduKDwrHeri0s5byibsSQrrJTEv)** |   **[Pytorchï¼ˆpassword:78pqï¼‰](https://pan.baidu.com/s/1ozFsxExAQTBRs5NbJW7W5g)**   |\n| **`DeepKE(RE), BERT-wwm, Chinese`**         | **relation extraction** | **[PyTorch](https://drive.google.com/drive/folders/1wb_QIZduKDwrHeri0s5byibsSQrrJTEv)** |   **[Pytorchï¼ˆpassword:6psmï¼‰](https://pan.baidu.com/s/1ngvTwg_ZXaenxhOeadWoCA)**   |\n\n### Instructions\n\nIt is recommended to use Baidu Netdisk download in Chinese Mainland, and Google download for overseas users.\n\nAs for the entity extraction model, take pytoch version `DeepKE(RE), RoBERTa-wwm-ext, Chinese` as an example. After downloading, files of the model are obtained:\n\n```\ncheckpoints_robert\n    |- added_tokens.json          # added tokens\n    |- config.json                # config\n    |- eval_results.txt           # evaluation results\n    |- model_config.json          # model config\n    |- pytorch_model.bin          # model\n    |- special_tokens_map.json    # special tokens map\n    |- tokenizer_config.bin       # tokenizer config\n    |- vocab.txt                  # vocabulary\n```\n\nwhere `config.json` and `vocab.txt` is completely consistent with the original Google `RoBERTa-wwm-ext, Chinese`. PyTorch version contains `pytorch_model. bin`, `config. json`, `vocab. txt` file.\n\nAs for the relation extraction model, take pytoch version `DeepKE(RE), RoBERTa-wwm-ext, Chinese` as an example. The model is pth file after downloading.\n\n**After downloading the model, users can directly [quick-load](#Quick-Load) it to extract entity and relation.**\n\n## Datasets and Chinese Baseline Performance\n\n### Datasets\n\nWe have conduct experiments on Chinese named entity recognition and relation extraction datasets. The experimental results are as follows:\n\n### Named Entity Recognition(NER)\n\nDeepKE leverages[`chinese-bert-wwm`](https://drive.google.com/drive/folders/1OLx5tjEriMyzbv0iv_s9lihtXWIjB6OS)and[`chinese-roberta-wwm-ext`](https://drive.google.com/drive/folders/1T3xf_MXRaVqLV-ST4VqvKoaQqQgRpp67)to train and obtain the DeepKE-cnSchema(NER) model. Hyper-parameters used in the model are predefined. Finally, we can obtain the following results after training:\n\n<table>\n    <tr>\n        <th>Model</th>\n        <th>P</th>\n        <th>R</th>\n        <th>F1</th>\n    </tr>\n    <tr>\n        <td><b>DeepKE(NER), RoBERTa-wwm-ext, Chinese</b></td>\n        <td>0.8028</td>\n        <td>0.8612</td>\n        <td>0.8310</td>\n    </tr>\n    <tr>\n\t<td><b>DeepKE(NER), BERT-wwm, Chinese</b></td>\n\t<td>0.7841</td>\n\t<td>0.8587</td>\n\t<td>0.8197</td>\n    </tr>\n</table>\n\n### Relation Extraction(RE)\n\nDeepKE leverages[`chinese-bert-wwm`](https://drive.google.com/drive/folders/1wb_QIZduKDwrHeri0s5byibsSQrrJTEv)and[`chinese-roberta-wwm-ext`](https://drive.google.com/drive/folders/1wb_QIZduKDwrHeri0s5byibsSQrrJTEv)to train and obtain the DeepKE-cnschema(RE) model. Hyper-parameters used in the model are predefined. Finally, we can obtain the following results after  training:\n\n<table>\n    <tr>\n        <th>Model</th>\n        <th>P</th>\n        <th>R</th>\n        <th>F1</th>\n    </tr>\n  <tr>\n        <td><b>DeepKE(RE), RoBERTa-wwm-ext, Chinese</b></td>\n        <td>0.7890</td>\n        <td>0.7370</td>\n        <td>0.7327</td>\n    </tr>\n  <tr>\n        <td><b>DeepKE(RE), BERT-wwm, Chinese</b></td>\n        <td>0.7861</td>\n        <td>0.7506</td>\n        <td>0.7473</td>\n    </tr>\n</table>\n\n### Support Knowledge Schema Type\n\nDeepKE-cnSchema is an off-the-shelf version that supports the Chinese knowledge graphs construction. [CnSchema](https://github.com/OpenKG-ORG/cnSchema) is developed for Chinese information processing, which uses advanced knowledge graphs, natural language processing and machine learning technologies. It integrates structured text data, supports rapid domain knowledge modeling and open data automatic processing across data sources, domains and languages, and provides schema-level support and services for emerging application markets such as intelligent robots, semantic search and intelligent computing. Currently, the Schema types supported by DeepKE-cnSchema are as follows:\n\n#### Entity Schema\n\n| ID | Entity Type | ID | Entity Type | \n| ------------- | :---------- | ------------- | :---------- | \n| 1   | cns:äººç‰© YAS  | 2   | cns:å½±è§†ä½œå“ TOJ | \n| 3   | cns:ç›® NGS        | 4   | cns:ç”Ÿç‰© QCV   | \n| 5   | cns:Number OKB   | 6   | cns:Date BQF | \n| 7   | cns:å›½å®¶ CAR       | 8   | cns:ç½‘ç«™ ZFM   | \n| 9   | cns:ç½‘ç»œå°è¯´ EMT     | 10  | cns:å›¾ä¹¦ä½œå“ UER | \n| 11  | cns:æ­Œæ›² QEE       | 12  | cns:åœ°ç‚¹ UFT   | \n| 13  | cns:æ°”å€™ GJS       | 14  | cns:è¡Œæ”¿åŒº SVA  | \n| 15  | cns:TEXT ANO     | 16  | cns:å†å²äººç‰© KEJ | \n| 17  | cns:å­¦æ ¡ ZDI       | 18  | cns:ä¼ä¸š CAT   | \n| 19  | cns:å‡ºç‰ˆç¤¾ GCK      | 20  | cns:ä¹¦ç± FQK   | \n| 21  | cns:éŸ³ä¹ä¸“è¾‘ BAK     | 22  | cns:åŸå¸‚ RET   | \n| 23  | cns:æ™¯ç‚¹ QZP       | 24  | cns:ç”µè§†ç»¼è‰º QAQ | \n| 25  | cns:æœºæ„ ZRE       | 26  | cns:ä½œå“ TDZ   | \n| 27  | cns:è¯­è¨€ CVC       | 28  | cns:å­¦ç§‘ä¸“ä¸š PMN | \n\n#### Relation Schema\n\n| ID | Head Entity Type | Tail Entity Type | Relation | ID | Head Entity Type | Tail Entity Type | Relation |\n| --- |:------ |:-----:| ---- | --- |:------ |:-----:| ---- |\n| 1   | cns:åœ°ç‚¹     | cns:äººç‰©    | cns:ç¥–ç±   | 2   | cns:äººç‰©     | cns:äººç‰©    | cns:çˆ¶äº²   |\n| 3   | cns:åœ°ç‚¹     | cns:ä¼ä¸š    | cns:æ€»éƒ¨åœ°ç‚¹ | 4   | cns:åœ°ç‚¹     | cns:äººç‰©    | cns:å‡ºç”Ÿåœ°  |\n| 5   | cns:ç›®      | cns:ç”Ÿç‰©    | cns:ç›®    | 6   | cns:Number | cns:è¡Œæ”¿åŒº   | cns:é¢ç§¯   |\n| 7   | cns:Text   | cns:æœºæ„    | cns:ç®€ç§°   | 8   | cns:Date   | cns:å½±è§†ä½œå“  | cns:ä¸Šæ˜ æ—¶é—´ |\n| 9   | cns:äººç‰©     | cns:äººç‰©    | cns:å¦»å­   | 10  | cns:éŸ³ä¹ä¸“è¾‘   | cns:æ­Œæ›²    | cns:æ‰€å±ä¸“è¾‘ |\n| 11  | cns:Number | cns:ä¼ä¸š    | cns:æ³¨å†Œèµ„æœ¬ | 12  | cns:åŸå¸‚     | cns:å›½å®¶    | cns:é¦–éƒ½   |\n| 13  | cns:äººç‰©     | cns:å½±è§†ä½œå“  | cns:å¯¼æ¼”   | 14  | cns:Text   | cns:å†å²äººç‰©  | cns:å­—    |\n| 15  | cns:Number | cns:äººç‰©    | cns:èº«é«˜   | 16  | cns:ä¼ä¸š     | cns:å½±è§†ä½œå“  | cns:å‡ºå“å…¬å¸ |\n| 17  | cns:Number | cns:å­¦ç§‘ä¸“ä¸š  | cns:ä¿®ä¸šå¹´é™ | 18  | cns:Date   | cns:äººç‰©    | cns:å‡ºç”Ÿæ—¥æœŸ |\n| 19  | cns:äººç‰©     | cns:å½±è§†ä½œå“  | cns:åˆ¶ç‰‡äºº  | 20  | cns:äººç‰©     | cns:äººç‰©    | cns:æ¯äº²   |\n| 21  | cns:äººç‰©     | cns:å½±è§†ä½œå“  | cns:ç¼–è¾‘   | 22  | cns:å›½å®¶     | cns:äººç‰©    | cns:å›½ç±   |\n| 23  | cns:äººç‰©     | cns:å½±è§†ä½œå“  | cns:ç¼–å‰§   | 24  | cns:ç½‘ç«™     | cns:ç½‘ç«™å°è¯´  | cns:è¿è½½ç½‘ç»œ |\n| 25  | cns:äººç‰©     | cns:äººç‰©    | cns:ä¸ˆå¤«   | 26  | cns:Text   | cns:å†å²äººç‰©  | cns:æœä»£   |\n| 27  | cns:Text   | cns:äººç‰©    | cns:æ°‘æ—   | 28  | cns:Text   | cns:å†å²äººç‰©  | cns:å·   |\n| 29  | cns:å‡ºç‰ˆç¤¾    | cns:ä¹¦ç±    | cns:å‡ºç‰ˆç¤¾  | 30  | cns:äººç‰©     | cns:ç”µè§†ç»¼è‰º  | cns:ä¸»æŒäºº  |\n| 31  | cns:Text   | cns:å­¦ç§‘ä¸“ä¸š  | cns:ä¸“ä¸šä»£ç  | 32  | cns:äººç‰©     | cns:æ­Œæ›²    | cns:æ­Œæ‰‹   |\n| 33  | cns:äººç‰©     | cns:æ­Œæ›²    | cns:ä½œæ›²   | 34  | cns:äººç‰©     | cns:ç½‘ç»œå°è¯´  | cns:ä¸»è§’   |\n| 35  | cns:äººç‰©     | cns:ä¼ä¸š    | cns:è‘£äº‹é•¿  | 36  | cns:Date   | cns:ä¼ä¸š    | cns:æˆç«‹æ—¶é—´ |\n| 37  | cns:å­¦æ ¡     | cns:äººç‰©    | cns:æ¯•ä¸šé™¢æ ¡ | 38  | cns:Number | cns:æœºæ„    | cns:å åœ°é¢ç§¯ |\n| 39  | cns:è¯­è¨€     | cns:å›½å®¶    | cns:å®˜æ–¹è¯­è¨€ | 40  | cns:Text   | cns:è¡Œæ”¿åŒº   | cns:äººå£æ•°é‡ |\n| 41  | cns:Number | cns:è¡Œæ”¿åŒº   | cns:äººå£æ•°é‡ | 42  | cns:åŸå¸‚     | cns:æ™¯ç‚¹    | cns:æ‰€åœ¨åŸå¸‚ |\n| 43  | cns:äººç‰©     | cns:å›¾ä¹¦ä½œå“  | cns:ä½œè€…   | 44  | None   | None    | å…¶ä»– |\n| 45  | cns:äººç‰©     | cns:æ­Œæ›²    | cns:ä½œæ›²   | 46  | cns:äººç‰©     | cns:è¡Œæ”¿åŒº   | cns:æ°”å€™   |\n| 47  | cns:äººç‰©     | cns:ç”µè§†ç»¼è‰º  | cns:å˜‰å®¾   | 48  | cns:äººç‰©     | cns:å½±è§†ä½œå“  | cns:ä¸»æ¼”   |\n| 49  | cns:ä½œå“     | cns:å½±è§†ä½œå“  | cns:æ”¹ç¼–è‡ª  | 50  | cns:äººç‰©     | cns:ä¼ä¸š    | cns:åˆ›å§‹äºº  |\n\n\n## Quick Load\n\n### [Named Entity Recognition(NER)](https://github.com/zjunlp/DeepKE/tree/main/example/ner/standard)\n\nUsers can directly download the [model](https://drive.google.com/drive/folders/1zA8Ichx9nzU3GD92ptdyR_nmARB_7ovg) for usage. The details are as followsï¼š\n\n1. Enter the directory `DeepKE/example/ner/standard`\n2. Create the downloaded folder as `checkpoints` and store it in the directory `DeepKE/example/ner/standard`\n3. Set the parameter `text` in `conf/predict.yaml` as the sentence to be predicted, and modify `hydra/model` in `conf/config.yaml` to `bert` or `lstmcrf` according to the category of the downloaded model (the model downloaded from the above link is the `bert` model)\n\t\n \t> To use the trained model, just set the input sentence \"ã€Šæ˜Ÿç©ºé»‘å¤œä¼ å¥‡ã€‹æ˜¯è¿è½½äºèµ·ç‚¹ä¸­æ–‡ç½‘çš„ç½‘ç»œå°è¯´ï¼Œä½œè€…æ˜¯å•¤é…’çš„ç½ªå­½\". After running `python oredict.py`, results can be obtained which show that the entity type \"æ˜Ÿç©ºé»‘å¤œä¼ å¥‡\" is \"ç½‘ç»œå°è¯´\", \"èµ·ç‚¹ä¸­æ–‡ç½‘\" is \"ç½‘ç«™\" and \"å•¤é…’çš„ç½ªå­½\" is \"äººç‰©\".\n  \t> ```\n\t>\ttext=\"ã€Šæ˜Ÿç©ºé»‘å¤œä¼ å¥‡ã€‹æ˜¯è¿è½½äºèµ·ç‚¹ä¸­æ–‡ç½‘çš„ç½‘ç»œå°è¯´ï¼Œä½œè€…æ˜¯å•¤é…’çš„ç½ªå­½\"\n\t>\t```\t\n\n\t\t\n4. Predict\n\t```bash\n\tpython predict.py\n\t```\n\n\n\tFinally, output the results:\n\n\t```bash\n\tNERå¥å­ï¼š\n\tã€Šæ˜Ÿç©ºé»‘å¤œä¼ å¥‡ã€‹æ˜¯è¿è½½äºèµ·ç‚¹ä¸­æ–‡ç½‘çš„ç½‘ç»œå°è¯´ï¼Œä½œè€…æ˜¯å•¤é…’çš„ç½ªå­½\n\tNERç»“æœï¼š\n\t[('æ˜Ÿ','B-UER'),('ç©º','I-UER'),('é»‘','I-UER'),('å¤œ','I-UER'),('ä¼ ','I-UER'),('å¥‡','I-UER'),('èµ·','B-ZFM'),('ç‚¹','I-ZFM'),('ä¸­','I-ZFM'),('æ–‡','I-ZFM'),('ç½‘','I-ZFM'),('å•¤','B-YAS'),('é…’','I-YAS'),('çš„','I-YAS'),('ç½ª','I-YAS'),('å­½','I-YAS')]\n\t```\n\n### [Relation Extraction(RE)](https://github.com/zjunlp/DeepKE/tree/main/example/re/standard)\nUsers can directly download the [model](https://drive.google.com/drive/folders/1wb_QIZduKDwrHeri0s5byibsSQrrJTEv) for usage. The details are as followsï¼š\n\n1. Enter the directory `DeepKE/example/re/standard`\n2. Modify the parameter `fp`in `conf/config.yaml`to the path of downloaded file, `num_relations`in `conf/embedding.yaml`to 51(relation nums) and `model` in `conf/config.yaml`to `lm`.\n3. Download the [dataset](https://drive.google.com/drive/folders/1UurqpjePe3zhXxbDDNwLAjVvt7UyUMuQ) , and put files in the directory `example/re/standard/data/origin`.\n4. Predict. The text and entity pairs to be predicted are fed to the program through the terminal.\n\n\t```bash\n\tpython predict.py\n\t```\n\n\tTo use the trained model, run `python predict.py` and input the sentence \"æ­Œæ›²ã€Šäººç”Ÿé•¿è·¯ã€‹å‡ºè‡ªåˆ˜å¾·åå›½è¯­ä¸“è¾‘ã€Šç”·äººçš„çˆ±ã€‹ï¼Œç”±ææ³‰ä½œè¯ä½œæ›²ï¼Œ2001å¹´å‡ºè¡Œå‘ç‰ˆ\". The given entity pair are \"ç”·äººçš„çˆ±\" and \"äººç”Ÿé•¿è·¯\". Finally, the extracted relation is \"æ‰€å±ä¸“è¾‘\".\n\n\tTo change the text to be predicted, modify the `_get_predict_instance`function in `predict.py` to the following example:\n\n\t```python\n\tdef _get_predict_instance(cfg):\n\t    flag = input('æ˜¯å¦ä½¿ç”¨èŒƒä¾‹[y/n]ï¼Œé€€å‡ºè¯·è¾“å…¥: exit .... ')\n\t    flag = flag.strip().lower()\n\t    if flag == 'y' or flag == 'yes':\n\t\tsentence = 'æ­Œæ›²ã€Šäººç”Ÿé•¿è·¯ã€‹å‡ºè‡ªåˆ˜å¾·åå›½è¯­ä¸“è¾‘ã€Šç”·äººçš„çˆ±ã€‹ï¼Œç”±ææ³‰ä½œè¯ä½œæ›²ï¼Œ2001å¹´å‡ºè¡Œå‘ç‰ˆ'\n\t\thead = 'ç”·äººçš„çˆ±'\n\t\ttail = 'äººç”Ÿé•¿è·¯'\n\t\thead_type = 'æ‰€å±ä¸“è¾‘'\n\t\ttail_type = 'æ­Œæ›²'\n\t    elif flag == 'n' or flag == 'no':\n\t\tsentence = input('è¯·è¾“å…¥å¥å­ï¼š')\n\t\thead = input('è¯·è¾“å…¥å¥ä¸­éœ€è¦é¢„æµ‹å…³ç³»çš„å¤´å®ä½“ï¼š')\n\t\thead_type = input('è¯·è¾“å…¥å¤´å®ä½“ç±»å‹ï¼ˆå¯ä»¥ä¸ºç©ºï¼ŒæŒ‰enterè·³è¿‡ï¼‰ï¼š')\n\t\ttail = input('è¯·è¾“å…¥å¥ä¸­éœ€è¦é¢„æµ‹å…³ç³»çš„å°¾å®ä½“ï¼š')\n\t\ttail_type = input('è¯·è¾“å…¥å°¾å®ä½“ç±»å‹ï¼ˆå¯ä»¥ä¸ºç©ºï¼ŒæŒ‰enterè·³è¿‡ï¼‰ï¼š')\n\t    elif flag == 'exit':\n\t\tsys.exit(0)\n\t    else:\n\t\tprint('please input yes or no, or exit!')\n\t\t_get_predict_instance()\n\n\t    instance = dict()\n\t    instance['sentence'] = sentence.strip()\n\t    instance['head'] = head.strip()\n\t    instance['tail'] = tail.strip()\n\t    if head_type.strip() == '' or tail_type.strip() == '':\n\t\tcfg.replace_entity_with_type = False\n\t\tinstance['head_type'] = 'None'\n\t\tinstance['tail_type'] = 'None'\n\t    else:\n\t\tinstance['head_type'] = head_type.strip()\n\t\tinstance['tail_type'] = tail_type.strip()\n\n\t    return instance\n\t```\n\n\tFinally, output the results:\n\n\t```bash\n\tâ€œç”·äººçš„çˆ±â€å’Œâ€œäººç”Ÿé•¿è·¯â€åœ¨å¥ä¸­å…³ç³»ä¸ºâ€œæ‰€å±ä¸“è¾‘â€ï¼Œç½®ä¿¡åº¦ä¸º0.99\n\t```\n \t> Note: The model specified in `example/re/standard/conf/model/lm.yaml` will be automatically downloaded from the huggingface website during runtime. If the download fails, please use the huggingface mirror site or download it manually.\n\n### [Joint Entity and Relation Extraction](https://github.com/zjunlp/DeepKE/tree/main/example/triple)\nAfter aforementioned trained models are downloaded, entites and their relations in a text can be extracted together. If there are more than two entities in one sentence, some predicted entity pairs may be incorrect because these entity pairs are not in training sets and need to be exracted further. The detailed steps are as follows:<br>\n1. In `conf`, modify `text` in `predict.yaml` as the sentence to be predicted, `nerfp` as the directory of the trained NER model and `refp` as the directory of the trained RE model.\n2. Predict\n\t```shell\n\tpython predict.py\n\t```\n\tMany results will be output. Take the input text `æ­¤å¤–ç½‘æ˜“äº‘å¹³å°è¿˜ä¸Šæ¶äº†ä¸€ç³»åˆ—æ­Œæ›²ï¼Œå…¶ä¸­åŒ…æ‹¬ç”°é¦¥ç”„çš„ã€Šå°å¹¸è¿ã€‹ç­‰` as example.\n\t\n\t(1) Output the result of NER: `[('ç”°', 'B-YAS'), ('é¦¥', 'I-YAS'), ('ç”„', 'I-YAS'), ('å°', 'B-QEE'), ('å¹¸', 'I-QEE'), ('è¿', 'I-QEE')]`\n\t\n\t(2) Output the processed result: `{'ç”°é¦¥ç”„': 'äººç‰©', 'å°å¹¸è¿': 'æ­Œæ›²'}`\n\t\n\t(3) Output the result of RE: `\"ç”°é¦¥ç”„\" å’Œ \"å°å¹¸è¿\" åœ¨å¥ä¸­å…³ç³»ä¸ºï¼š\"æ­Œæ‰‹\"ï¼Œç½®ä¿¡åº¦ä¸º0.92ã€‚`\n\t\n\t(4) Output the result as `jsonld`\n\t\n\t ```bash\n\t    {\n\t      \"@context\": {\n\t\t\"æ­Œæ‰‹\": \"https://cnschema.openkg.cn/item/%E6%AD%8C%E6%89%8B/16693#viewPageContent\"\n\t      },\n\t      \"@id\": \"ç”°é¦¥ç”„\",\n\t      \"æ­Œæ‰‹\": {\n\t\t\"@id\": \"å°å¹¸è¿\"\n\t      }\n\t    }\n\t  ```\n\n## Custom Models (Advanced Usage)\n\n### Named Entity Recognition (NER)\n\nIf you need to use customized dataset for training, follow the steps bellow:\n\n1. Download customized [dataset](https://drive.google.com/drive/folders/1zA8Ichx9nzU3GD92ptdyR_nmARB_7ovg) and put it into the `data` folder.\n2. Modify the parameter `bert_model`in `train.yaml`of the `conf`folder to the specify model. Users can choose different models to train by modifying the `yaml`file.\n3. Modify `labels` in `train.yaml` as the labels in `data/type.txt`\n4. Train.\n\t```bash\n\tpython run.py\n\t```\n\n### Relation Extraction (RE)\n\nIf you need to use other models for training, follow the steps bellow:\n\n1ã€Download the customized [dataset](https://drive.google.com/drive/folders/1wb_QIZduKDwrHeri0s5byibsSQrrJTEv) and rename it to `data`.\n\n2ã€Modify the parameter `model_name`in `train.yaml`of the `conf`folder to `lm`, `num_relations`in `embedding_yaml`to the number of relations(eg: 51). Users can choose different models to train by modifying the `yaml`file. \n\n3ã€Train.\n\n\t```bash\n\tpython run.py\n\t```\n\n## FAQ\n\n**Q: How to use this model?**\nA: It is off-the-shelf. After downloading the model, follow the instructions and you can extract the knowledge contained in the predefined cnSchema.\n**If you want to extract knowledge other than cnSchema, you can use the advanced version of customized data for training**\n\n**Q: Is there any other cnSchema extraction model available?**\nA: Unfortunately, we can only support part of knowledge extraction of cnSchema for the time being. More knowledge extraction models will be published in the future.\n\n**Q: I trained better result than you!**\nA: Congratulations!\n\n**Q: Embedding error for customized dataset.**\nA: The Chinese data may contain invisible special characters, which cannot be encoded and thus an error is reported. You can preprocess the Chinese data through the editor or other tools to solve this problem.\n\n## Citation\n\nIf the resources or technologies in this project are helpful to your research work, you are welcome to cite the following papers in your thesis:\n\n```\n@inproceedings{DBLP:conf/emnlp/ZhangXTYYQXCLL22,\n  author    = {Ningyu Zhang and\n               Xin Xu and\n               Liankuan Tao and\n               Haiyang Yu and\n               Hongbin Ye and\n               Shuofei Qiao and\n               Xin Xie and\n               Xiang Chen and\n               Zhoubo Li and\n               Lei Li},\n  editor    = {Wanxiang Che and\n               Ekaterina Shutova},\n  title     = {DeepKE: {A} Deep Learning Based Knowledge Extraction Toolkit for Knowledge\n               Base Population},\n  booktitle = {Proceedings of the The 2022 Conference on Empirical Methods in Natural\n               Language Processing, {EMNLP} 2022 - System Demonstrations, Abu Dhabi,\n               UAE, December 7-11, 2022},\n  pages     = {98--108},\n  publisher = {Association for Computational Linguistics},\n  year      = {2022},\n  url       = {https://aclanthology.org/2022.emnlp-demos.10},\n  timestamp = {Thu, 23 Mar 2023 16:56:00 +0100},\n  biburl    = {https://dblp.org/rec/conf/emnlp/ZhangXTYYQXCLL22.bib},\n  bibsource = {dblp computer science bibliography, https://dblp.org}\n}\n```\n\n## Disclaimers\n\n**The contents of this project are only for technical research reference and shall not be used as any conclusive basis. Users can freely use the model within the scope of the license, but we are not responsible for the direct or indirect losses caused by the use of the project.**\n\n## Problem Feedback\n\nIf you have any questions, please submit them in GitHub issue.\n\n"
        },
        {
          "name": "README_CNSCHEMA_CN.md",
          "type": "blob",
          "size": 21.748046875,
          "content": "<p align=\"center\">\n    <a href=\"https://github.com/zjunlp/deepke\"> <img src=\"pics/logo_cnschema.png\" width=\"400\"/></a>\n<p>\n<p align=\"center\">  \n    <a href=\"http://deepke.zjukg.cn\">\n        <img alt=\"Documentation\" src=\"https://img.shields.io/badge/demo-website-blue\">\n    </a>\n    <a href=\"https://pypi.org/project/deepke/#files\">\n        <img alt=\"PyPI\" src=\"https://img.shields.io/pypi/v/deepke\">\n    </a>\n    <a href=\"https://github.com/zjunlp/DeepKE/blob/master/LICENSE\">\n        <img alt=\"GitHub\" src=\"https://img.shields.io/github/license/zjunlp/deepke\">\n    </a>\n    <a href=\"http://zjunlp.github.io/DeepKE\">\n        <img alt=\"Documentation\" src=\"https://img.shields.io/badge/doc-website-red\">\n    </a>\n    <a href=\"https://colab.research.google.com/drive/1vS8YJhJltzw3hpJczPt24O0Azcs3ZpRi?usp=sharing\">\n        <img alt=\"Open In Colab\" src=\"https://colab.research.google.com/assets/colab-badge.svg\">\n    </a>\n</p>\n\n<p align=\"center\">\n    <b> <a href=\"https://github.com/zjunlp/DeepKE/blob/main/README_CNSCHEMA.md\">English</a> | ç®€ä½“ä¸­æ–‡ </b>\n</p>\n\n<h1 align=\"center\">\n    <p>å¼€æºä¸­æ–‡çŸ¥è¯†å›¾è°±æŠ½å–æ¡†æ¶å¼€ç®±å³ç”¨ç‰¹åˆ«ç‰ˆDeepKE-cnSchema</p>\n</h1>\n\nDeepKE æ˜¯ä¸€ä¸ªå¼€æºçš„çŸ¥è¯†å›¾è°±æŠ½å–ä¸æ„å»ºå·¥å…·ï¼Œæ”¯æŒ<b>ä½èµ„æºã€é•¿ç¯‡ç« ã€å¤šæ¨¡æ€</b>çš„çŸ¥è¯†æŠ½å–å·¥å…·ï¼Œå¯ä»¥åŸºäº<b>PyTorch</b>å®ç°<b>å‘½åå®ä½“è¯†åˆ«</b>ã€<b>å…³ç³»æŠ½å–</b>å’Œ<b>å±æ€§æŠ½å–</b>åŠŸèƒ½ã€‚æ­¤ç‰ˆæœ¬DeepKE-cnSchemaä¸ºå¼€ç®±å³ç”¨ç‰ˆæœ¬ï¼Œç”¨æˆ·ä¸‹è½½æ¨¡å‹å³å¯å®ç°æ”¯æŒcnSchemaçš„å®ä½“å’Œå…³ç³»çŸ¥è¯†æŠ½å–ã€‚\n\n---\n\n## å†…å®¹å¯¼å¼•\n\n| ç« èŠ‚                          | æè¿°                                |\n| --------------------------- | --------------------------------- |\n| [ç®€ä»‹](#ç®€ä»‹)                   | ä»‹ç»DeepKE-cnSchemaåŸºæœ¬åŸç†             |\n| [ä¸­æ–‡æ¨¡å‹ä¸‹è½½](#ä¸­æ–‡æ¨¡å‹ä¸‹è½½)           | æä¾›äº†DeepKE-cnSchemaçš„ä¸‹è½½åœ°å€           |\n| [æ•°æ®é›†åŠä¸­æ–‡æ¨¡å‹æ•ˆæœ](#æ•°æ®é›†åŠä¸­æ–‡åŸºçº¿ç³»ç»Ÿæ•ˆæœ) | æä¾›äº†ä¸­æ–‡æ•°æ®é›†ä»¥åŠä¸­æ–‡æ¨¡å‹æ•ˆæœ                  |\n| [å¿«é€ŸåŠ è½½](#å¿«é€ŸåŠ è½½)               | ä»‹ç»äº†å¦‚ä½•ä½¿ç”¨DeepKE-cnSchemaè¿›è¡Œå®ä½“è¯†åˆ«ã€å…³ç³»æŠ½å– |\n| [è‡ªå®šä¹‰æ¨¡å‹](#è‡ªå®šä¹‰æ¨¡å‹)             | æä¾›äº†ä½¿ç”¨è‡ªå®šä¹‰æ•°æ®è®­ç»ƒæ¨¡å‹çš„è¯´æ˜                 |\n| [FAQ](#FAQ)                 | å¸¸è§é—®é¢˜ç­”ç–‘                            |\n| [å¼•ç”¨](#å¼•ç”¨)                   | æœ¬ç›®å½•çš„æŠ€æœ¯æŠ¥å‘Š                          |\n\n## ç®€ä»‹\n\nDeepKE æ˜¯ä¸€ä¸ªå¼€æºçš„çŸ¥è¯†å›¾è°±æŠ½å–ä¸æ„å»ºå·¥å…·ï¼Œæ”¯æŒä½èµ„æºã€é•¿ç¯‡ç« ã€å¤šæ¨¡æ€çš„çŸ¥è¯†æŠ½å–å·¥å…·ï¼Œå¯ä»¥åŸºäºPyTorchå®ç°å‘½åå®ä½“è¯†åˆ«ã€å…³ç³»æŠ½å–å’Œå±æ€§æŠ½å–åŠŸèƒ½ã€‚åŒæ—¶ä¸ºåˆå­¦è€…æä¾›äº†è¯¦å°½çš„[æ–‡æ¡£](https://zjunlp.github.io/DeepKE/)ï¼Œ[Google Colabæ•™ç¨‹](https://colab.research.google.com/drive/1vS8YJhJltzw3hpJczPt24O0Azcs3ZpRi?usp=sharing)ï¼Œ[åœ¨çº¿æ¼”ç¤º](http://deepke.zjukg.cn/)å’Œ[å¹»ç¯ç‰‡](https://github.com/zjunlp/DeepKE/blob/main/docs/slides/Slides-DeepKE-cn.pdf)ã€‚\n\nä¸ºä¿ƒè¿›ä¸­æ–‡é¢†åŸŸçš„çŸ¥è¯†å›¾è°±æ„å»ºå’Œæ–¹ä¾¿ç”¨æˆ·ä½¿ç”¨ï¼ŒDeepKEæä¾›äº†é¢„è®­ç»ƒå¥½çš„æ”¯æŒ[cnSchema](https://github.com/OpenKG-ORG/cnSchema)çš„ç‰¹åˆ«ç‰ˆDeepKE-cnSchemaï¼Œæ”¯æŒå¼€ç®±å³ç”¨çš„ä¸­æ–‡å®ä½“æŠ½å–å’Œå…³ç³»æŠ½å–ç­‰ä»»åŠ¡ï¼Œå¯æŠ½å–50ç§å…³ç³»ç±»å‹å’Œ28ç§å®ä½“ç±»å‹ï¼Œå…¶ä¸­å®ä½“ç±»å‹åŒ…å«äº†é€šç”¨çš„äººç‰©ã€åœ°ç‚¹ã€åŸå¸‚ã€æœºæ„ç­‰ç±»å‹ï¼Œå…³ç³»ç±»å‹åŒ…æ‹¬äº†å¸¸è§çš„ç¥–ç±ã€å‡ºç”Ÿåœ°ã€å›½ç±ã€æœä»£ç­‰ç±»å‹ã€‚\n\n## ä¸­æ–‡æ¨¡å‹ä¸‹è½½\n\nå¯¹äºå®ä½“æŠ½å–å’Œå…³ç³»æŠ½å–ä»»åŠ¡åˆ†åˆ«æä¾›äº†åŸºäº`RoBERTa-wwm-ext, Chinese`å’Œ`BERT-wwm, Chinese`è®­ç»ƒçš„æ¨¡å‹ã€‚\n\n| æ¨¡å‹ç®€ç§°                                        | åŠŸèƒ½                     |                                        Googleä¸‹è½½                                         |                                   ç™¾åº¦ç½‘ç›˜ä¸‹è½½                                   |\n|:------------------------------------------- |:---------------------- |:---------------------------------------------------------------------------------------:|:--------------------------------------------------------------------------:|\n| **`DeepKE(NER), RoBERTa-wwm-ext, Chinese`** | **å®ä½“æŠ½å–** | **[PyTorch](https://drive.google.com/drive/folders/1T3xf_MXRaVqLV-ST4VqvKoaQqQgRpp67)** |   **[Pytorchï¼ˆå¯†ç u022ï¼‰](https://pan.baidu.com/s/1hb9XEbK4x5fIyco4DgZZfg)**   |\n| **`DeepKE(NER), BERT-wwm, Chinese`**        | **å®ä½“æŠ½å–** | **[PyTorch](https://drive.google.com/drive/folders/1zA8Ichx9nzU3GD92ptdyR_nmARB_7ovg)** |   **[Pytorchï¼ˆå¯†ç 1g0tï¼‰](https://pan.baidu.com/s/10TWE1VA2S-SJgmOm8szRxw)**   |\n| **`DeepKE(NER), BiLSTM-CRF, Chinese`**      | **å®ä½“æŠ½å–** | **[PyTorch](https://drive.google.com/drive/folders/1n1tzvl6hZYoUUFFWLfkuhkXPx5JB4XK_)** |   **[Pytorchï¼ˆå¯†ç my4xï¼‰](https://pan.baidu.com/s/1a9ZFFZVQUxmlbLmbVBaTqQ)**   |\n| **`DeepKE(RE), RoBERTa-wwm-ext, Chinese`**  | **å…³ç³»æŠ½å–** | **[PyTorch](https://drive.google.com/drive/folders/1wb_QIZduKDwrHeri0s5byibsSQrrJTEv)** |   **[Pytorchï¼ˆå¯†ç 78pqï¼‰](https://pan.baidu.com/s/1ozFsxExAQTBRs5NbJW7W5g)**   |\n| **`DeepKE(RE), BERT-wwm, Chinese`**         | **å…³ç³»æŠ½å–** | **[PyTorch](https://drive.google.com/drive/folders/1wb_QIZduKDwrHeri0s5byibsSQrrJTEv)** |   **[Pytorchï¼ˆå¯†ç 6psmï¼‰](https://pan.baidu.com/s/1ngvTwg_ZXaenxhOeadWoCA)**   |\n\n### ä½¿ç”¨è¯´æ˜\n\nä¸­å›½å¤§é™†å¢ƒå†…å»ºè®®ä½¿ç”¨ç™¾åº¦ç½‘ç›˜ä¸‹è½½ç‚¹ï¼Œå¢ƒå¤–ç”¨æˆ·å»ºè®®ä½¿ç”¨è°·æ­Œä¸‹è½½ç‚¹ã€‚\nå®ä½“æŠ½å–æ¨¡å‹ä¸­ï¼Œä»¥Pytorchç‰ˆ`DeepKE(RE), RoBERTa-wwm-ext, Chinese`ä¸ºä¾‹ï¼Œä¸‹è½½å®Œæ¯•åå¾—åˆ°æ¨¡å‹æ–‡ä»¶ï¼š\n\n```\ncheckpoints_robert\n    |- added_tokens.json          # é¢å¤–å¢åŠ è¯è¡¨\n    |- config.json                # æ•´ä½“å‚æ•°\n    |- eval_results.txt           # éªŒè¯ç»“æœ\n    |- model_config.json          # æ¨¡å‹å‚æ•°\n    |- pytorch_model.bin          # æ¨¡å‹\n    |- special_tokens_map.json    # ç‰¹æ®Šè¯è¡¨æ˜ å°„\n    |- tokenizer_config.bin       # åˆ†è¯å™¨å‚æ•°\n    |- vocab.txt                  # è¯è¡¨\n```\n\nå…¶ä¸­`config.json`å’Œ`vocab.txt`ä¸è°·æ­ŒåŸç‰ˆ`RoBERTa-wwm-ext, Chinese`å®Œå…¨ä¸€è‡´ã€‚\nPyTorchç‰ˆæœ¬åˆ™åŒ…å«`pytorch_model.bin`, `config.json`, `vocab.txt`æ–‡ä»¶ã€‚\n\nå…³ç³»æŠ½å–æ¨¡å‹ä¸­ï¼Œä»¥Pytorchç‰ˆ`DeepKE(RE), RoBERTa-wwm-ext, Chinese`ä¸ºä¾‹ï¼Œä¸‹è½½åä¸ºpthæ–‡ä»¶ã€‚\n\n**ä¸‹è½½æ¨¡å‹åï¼Œç”¨æˆ·å³å¯ç›´æ¥[å¿«é€ŸåŠ è½½](#å¿«é€ŸåŠ è½½)æ¨¡å‹è¿›è¡Œå®ä½“å…³ç³»æŠ½å–ã€‚**\n\n## æ•°æ®é›†åŠä¸­æ–‡åŸºçº¿ç³»ç»Ÿæ•ˆæœ\n\n### æ•°æ®é›†\n\næˆ‘ä»¬åœ¨ä¸­æ–‡å®ä½“è¯†åˆ«å’Œå…³ç³»æŠ½å–æ•°æ®é›†ä¸Šè¿›è¡Œäº†å®éªŒï¼Œå®éªŒç»“æœå¦‚ä¸‹\n\n### å®ä½“è¯†åˆ«ï¼ˆNERï¼‰\n\nDeepKEä½¿ç”¨[`chinese-bert-wwm`](https://drive.google.com/drive/folders/1OLx5tjEriMyzbv0iv_s9lihtXWIjB6OS)å’Œ[`chinese-roberta-wwm-ext`](https://drive.google.com/drive/folders/1T3xf_MXRaVqLV-ST4VqvKoaQqQgRpp67)ä¸ºåŸºç¡€è®­ç»ƒå¾—åˆ°äº†DeepKE-cnSchema(NER)æ¨¡å‹ã€‚æ¨¡å‹æ‰€ä½¿ç”¨çš„è¶…å‚æ•°å‡ä¸ºé¢„å®šä¹‰çš„å‚æ•°ã€‚æœ€ç»ˆç»è¿‡è®­ç»ƒåå¯ä»¥å¾—åˆ°å¦‚ä¸‹è¡¨çš„æ•ˆæœ\n\n<table>\n    <tr>\n        <th>æ¨¡å‹</th>\n        <th>P</th>\n        <th>R</th>\n        <th>F1</th>\n    </tr>\n    <tr>\n        <td><b>DeepKE(NER), RoBERTa-wwm-ext, Chinese</b></td>\n        <td>0.8028</td>\n        <td>0.8612</td>\n        <td>0.8310</td>\n    </tr>\n\n<tr>\n        <td><b>DeepKE(NER), BERT-wwm, Chinese</b></td>\n        <td>0.7841</td>\n        <td>0.8587</td>\n        <td>0.8197</td>\n    </tr>\n\n</table>\n\n### å…³ç³»æŠ½å–ï¼ˆREï¼‰\n\nDeepKEä½¿ç”¨[`chinese-bert-wwm`](https://drive.google.com/drive/folders/1wb_QIZduKDwrHeri0s5byibsSQrrJTEv)å’Œ[`chinese-roberta-wwm-ext`](https://drive.google.com/drive/folders/1wb_QIZduKDwrHeri0s5byibsSQrrJTEv)ä¸ºåŸºç¡€å¾—åˆ°äº†DeepKE-cnschema(RE)æ¨¡å‹ã€‚æ¨¡å‹æ‰€ä½¿ç”¨çš„è¶…å‚æ•°å‡ä¸ºé¢„å®šä¹‰çš„å‚æ•°ã€‚æœ€ç»ˆç»è¿‡è®­ç»ƒåå¯ä»¥å¾—åˆ°å¦‚ä¸‹è¡¨çš„æ•ˆæœ\n\n<table>\n    <tr>\n        <th>æ¨¡å‹</th>\n        <th>P</th>\n        <th>R</th>\n        <th>F1</th>\n    </tr>\n  <tr>\n        <td><b>DeepKE(RE), RoBERTa-wwm-ext, Chinese</b></td>\n        <td>0.7890</td>\n        <td>0.7370</td>\n        <td>0.7327</td>\n    </tr>\n  <tr>\n        <td><b>DeepKE(RE), BERT-wwm, Chinese</b></td>\n        <td>0.7861</td>\n        <td>0.7506</td>\n        <td>0.7473</td>\n    </tr>\n\n</table>\n\n### æ”¯æŒçŸ¥è¯†Schemaç±»å‹\n\nDeepKE-cnSchemaç‰¹åˆ«ç‰ˆä¸ºæ”¯æŒä¸­æ–‡é¢†åŸŸçŸ¥è¯†å›¾è°±æ„å»ºæ¨å‡ºçš„å¼€ç®±å³ç”¨ç‰ˆæœ¬ã€‚ [CnSchema](https://github.com/OpenKG-ORG/cnSchema)æ˜¯é¢å‘ä¸­æ–‡ä¿¡æ¯å¤„ç†ï¼Œåˆ©ç”¨å…ˆè¿›çš„çŸ¥è¯†å›¾è°±ã€è‡ªç„¶è¯­è¨€å¤„ç†å’Œæœºå™¨å­¦ä¹ æŠ€æœ¯ï¼Œèåˆç»“æ„åŒ–ä¸æ–‡æœ¬æ•°æ®ï¼Œæ”¯æŒå¿«é€Ÿé¢†åŸŸçŸ¥è¯†å»ºæ¨¡ï¼Œæ”¯æŒè·¨æ•°æ®æºã€è·¨é¢†åŸŸã€è·¨è¯­è¨€çš„å¼€æ”¾æ•°æ®è‡ªåŠ¨åŒ–å¤„ç†ï¼Œä¸ºæ™ºèƒ½æœºå™¨äººã€è¯­ä¹‰æœç´¢ã€æ™ºèƒ½è®¡ç®—ç­‰æ–°å…´åº”ç”¨å¸‚åœºæä¾›schemaå±‚é¢çš„æ”¯æŒä¸æœåŠ¡ã€‚ç›®å‰ï¼ŒDeepKE-cnSchemaæ”¯æŒçš„Schemaç±»å‹å¦‚ä¸‹è¡¨æ‰€ç¤ºï¼š\n\n#### å®ä½“Schema\n\n| åºå·  | å®ä½“ç±»å‹    | åºå·  | å®ä½“ç±»å‹  |\n| --- |:------ | --- |:---- |\n| 1   | cns:äººç‰© YAS  | 2   | cns:å½±è§†ä½œå“ TOJ | \n| 3   | cns:ç›® NGS        | 4   | cns:ç”Ÿç‰© QCV   | \n| 5   | cns:Number OKB   | 6   | cns:Date BQF | \n| 7   | cns:å›½å®¶ CAR       | 8   | cns:ç½‘ç«™ ZFM   | \n| 9   | cns:ç½‘ç»œå°è¯´ EMT     | 10  | cns:å›¾ä¹¦ä½œå“ UER | \n| 11  | cns:æ­Œæ›² QEE       | 12  | cns:åœ°ç‚¹ UFT   | \n| 13  | cns:æ°”å€™ GJS       | 14  | cns:è¡Œæ”¿åŒº SVA  | \n| 15  | cns:TEXT ANO     | 16  | cns:å†å²äººç‰© KEJ | \n| 17  | cns:å­¦æ ¡ ZDI       | 18  | cns:ä¼ä¸š CAT   | \n| 19  | cns:å‡ºç‰ˆç¤¾ GCK      | 20  | cns:ä¹¦ç± FQK   | \n| 21  | cns:éŸ³ä¹ä¸“è¾‘ BAK     | 22  | cns:åŸå¸‚ RET   | \n| 23  | cns:æ™¯ç‚¹ QZP       | 24  | cns:ç”µè§†ç»¼è‰º QAQ | \n| 25  | cns:æœºæ„ ZRE       | 26  | cns:ä½œå“ TDZ   | \n| 27  | cns:è¯­è¨€ CVC       | 28  | cns:å­¦ç§‘ä¸“ä¸š PMN | \n\n#### å…³ç³»Schema\n\n| åºå·  | å¤´å®ä½“ç±»å‹  | å°¾å®ä½“ç±»å‹ | å…³ç³»   | åºå·  | å¤´å®ä½“ç±»å‹  | å°¾å®ä½“ç±»å‹ | å…³ç³»   |\n| --- |:------ |:-----:| ---- | --- |:------ |:-----:| ---- |\n| 1   | cns:åœ°ç‚¹     | cns:äººç‰©    | cns:ç¥–ç±   | 2   | cns:äººç‰©     | cns:äººç‰©    | cns:çˆ¶äº²   |\n| 3   | cns:åœ°ç‚¹     | cns:ä¼ä¸š    | cns:æ€»éƒ¨åœ°ç‚¹ | 4   | cns:åœ°ç‚¹     | cns:äººç‰©    | cns:å‡ºç”Ÿåœ°  |\n| 5   | cns:ç›®      | cns:ç”Ÿç‰©    | cns:ç›®    | 6   | cns:Number | cns:è¡Œæ”¿åŒº   | cns:é¢ç§¯   |\n| 7   | cns:Text   | cns:æœºæ„    | cns:ç®€ç§°   | 8   | cns:Date   | cns:å½±è§†ä½œå“  | cns:ä¸Šæ˜ æ—¶é—´ |\n| 9   | cns:äººç‰©     | cns:äººç‰©    | cns:å¦»å­   | 10  | cns:éŸ³ä¹ä¸“è¾‘   | cns:æ­Œæ›²    | cns:æ‰€å±ä¸“è¾‘ |\n| 11  | cns:Number | cns:ä¼ä¸š    | cns:æ³¨å†Œèµ„æœ¬ | 12  | cns:åŸå¸‚     | cns:å›½å®¶    | cns:é¦–éƒ½   |\n| 13  | cns:äººç‰©     | cns:å½±è§†ä½œå“  | cns:å¯¼æ¼”   | 14  | cns:Text   | cns:å†å²äººç‰©  | cns:å­—    |\n| 15  | cns:Number | cns:äººç‰©    | cns:èº«é«˜   | 16  | cns:ä¼ä¸š     | cns:å½±è§†ä½œå“  | cns:å‡ºå“å…¬å¸ |\n| 17  | cns:Number | cns:å­¦ç§‘ä¸“ä¸š  | cns:ä¿®ä¸šå¹´é™ | 18  | cns:Date   | cns:äººç‰©    | cns:å‡ºç”Ÿæ—¥æœŸ |\n| 19  | cns:äººç‰©     | cns:å½±è§†ä½œå“  | cns:åˆ¶ç‰‡äºº  | 20  | cns:äººç‰©     | cns:äººç‰©    | cns:æ¯äº²   |\n| 21  | cns:äººç‰©     | cns:å½±è§†ä½œå“  | cns:ç¼–è¾‘   | 22  | cns:å›½å®¶     | cns:äººç‰©    | cns:å›½ç±   |\n| 23  | cns:äººç‰©     | cns:å½±è§†ä½œå“  | cns:ç¼–å‰§   | 24  | cns:ç½‘ç«™     | cns:ç½‘ç«™å°è¯´  | cns:è¿è½½ç½‘ç»œ |\n| 25  | cns:äººç‰©     | cns:äººç‰©    | cns:ä¸ˆå¤«   | 26  | cns:Text   | cns:å†å²äººç‰©  | cns:æœä»£   |\n| 27  | cns:Text   | cns:äººç‰©    | cns:æ°‘æ—   | 28  | cns:Text   | cns:å†å²äººç‰©  | cns:å·   |\n| 29  | cns:å‡ºç‰ˆç¤¾    | cns:ä¹¦ç±    | cns:å‡ºç‰ˆç¤¾  | 30  | cns:äººç‰©     | cns:ç”µè§†ç»¼è‰º  | cns:ä¸»æŒäºº  |\n| 31  | cns:Text   | cns:å­¦ç§‘ä¸“ä¸š  | cns:ä¸“ä¸šä»£ç  | 32  | cns:äººç‰©     | cns:æ­Œæ›²    | cns:æ­Œæ‰‹   |\n| 33  | cns:äººç‰©     | cns:æ­Œæ›²    | cns:ä½œæ›²   | 34  | cns:äººç‰©     | cns:ç½‘ç»œå°è¯´  | cns:ä¸»è§’   |\n| 35  | cns:äººç‰©     | cns:ä¼ä¸š    | cns:è‘£äº‹é•¿  | 36  | cns:Date   | cns:ä¼ä¸š    | cns:æˆç«‹æ—¶é—´ |\n| 37  | cns:å­¦æ ¡     | cns:äººç‰©    | cns:æ¯•ä¸šé™¢æ ¡ | 38  | cns:Number | cns:æœºæ„    | cns:å åœ°é¢ç§¯ |\n| 39  | cns:è¯­è¨€     | cns:å›½å®¶    | cns:å®˜æ–¹è¯­è¨€ | 40  | cns:Text   | cns:è¡Œæ”¿åŒº   | cns:äººå£æ•°é‡ |\n| 41  | cns:Number | cns:è¡Œæ”¿åŒº   | cns:äººå£æ•°é‡ | 42  | cns:åŸå¸‚     | cns:æ™¯ç‚¹    | cns:æ‰€åœ¨åŸå¸‚ |\n| 43  | cns:äººç‰©     | cns:å›¾ä¹¦ä½œå“  | cns:ä½œè€…   | 44  | None   | None    | å…¶ä»– |\n| 45  | cns:äººç‰©     | cns:æ­Œæ›²    | cns:ä½œæ›²   | 46  | cns:äººç‰©     | cns:è¡Œæ”¿åŒº   | cns:æ°”å€™   |\n| 47  | cns:äººç‰©     | cns:ç”µè§†ç»¼è‰º  | cns:å˜‰å®¾   | 48  | cns:äººç‰©     | cns:å½±è§†ä½œå“  | cns:ä¸»æ¼”   |\n| 49  | cns:ä½œå“     | cns:å½±è§†ä½œå“  | cns:æ”¹ç¼–è‡ª  | 50  | cns:äººç‰©     | cns:ä¼ä¸š    | cns:åˆ›å§‹äºº  |\n\n## å¿«é€ŸåŠ è½½\n\n### [å®ä½“è¯†åˆ«ï¼ˆNERï¼‰](https://github.com/zjunlp/DeepKE/tree/main/example/ner/standard)\n\nç”¨æˆ·å¯ä»¥ç›´æ¥ä¸‹è½½[æ¨¡å‹](https://drive.google.com/drive/folders/1zA8Ichx9nzU3GD92ptdyR_nmARB_7ovg)è¿›è¡Œä½¿ç”¨ï¼Œå…·ä½“æµç¨‹å¦‚ä¸‹ï¼š\n\n1. è¿›å…¥ç›®å½•`DeepKE/example/ner/standard`\n2. å°†ä¸‹è½½çš„æ¨¡å‹æ–‡ä»¶å¤¹å‘½åä¸º`checkpoints`ï¼Œå­˜æ”¾äºç›®å½•`DeepKE/example/ner/standard`ä¸‹\n3. ä¿®æ”¹ `conf/predict.yaml`ä¸­çš„å‚æ•°`text`ä¸ºéœ€è¦é¢„æµ‹çš„æ–‡æœ¬ï¼Œå¹¶æŒ‰ç…§ä¸‹è½½æ¨¡å‹çš„ç±»åˆ«ä¿®æ”¹`conf/config.yaml`ä¸­çš„æ¨¡å‹åç§°`hydra/model`ä¸º`bert`æˆ–è€…`lstmcrf`ï¼ˆä¸Šè¿°é“¾æ¥ä¸‹è½½çš„æ¨¡å‹ä¸ºbertæ¨¡å‹ï¼‰\n\n    > ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹ï¼Œåªéœ€è¾“å…¥å¥å­â€œã€Šæ˜Ÿç©ºé»‘å¤œä¼ å¥‡ã€‹æ˜¯è¿è½½äºèµ·ç‚¹ä¸­æ–‡ç½‘çš„ç½‘ç»œå°è¯´ï¼Œä½œè€…æ˜¯å•¤é…’çš„ç½ªå­½â€ï¼Œè¿è¡Œ```python predict.py```åå¯å¾—åˆ°ç»“æœï¼Œç»“æœæ˜¾ç¤ºâ€œæ˜Ÿç©ºé»‘å¤œä¼ å¥‡â€å®ä½“ç±»å‹ä¸ºç»è¿‡cnschemaå¯¹é½åçš„â€œç½‘ç»œå°è¯´â€ï¼Œâ€œèµ·ç‚¹ä¸­æ–‡ç½‘â€ä¸ºâ€œç½‘ç«™â€ï¼Œâ€œå•¤é…’çš„ç½ªå­½â€ä¸ºâ€œäººç‰©ã€‚\n    > ```bash\n    > text=â€œã€Šæ˜Ÿç©ºé»‘å¤œä¼ å¥‡ã€‹æ˜¯è¿è½½äºèµ·ç‚¹ä¸­æ–‡ç½‘çš„ç½‘ç»œå°è¯´ï¼Œä½œè€…æ˜¯å•¤é…’çš„ç½ªå­½â€\n    > ```\n4. é¢„æµ‹\n    ```shell\n    python predict.py\n    ```\n\n    æœ€ç»ˆè¾“å‡ºç»“æœ\n\n    ```bash\n    NERå¥å­ï¼š\n    ã€Šæ˜Ÿç©ºé»‘å¤œä¼ å¥‡ã€‹æ˜¯è¿è½½äºèµ·ç‚¹ä¸­æ–‡ç½‘çš„ç½‘ç»œå°è¯´ï¼Œä½œè€…æ˜¯å•¤é…’çš„ç½ªå­½\n    NERç»“æœï¼š\n    [('æ˜Ÿ','B-UER'),('ç©º','I-UER'),('é»‘','I-UER'),('å¤œ','I-UER'),('ä¼ ','I-UER'),('å¥‡','I-UER'),('èµ·','B-ZFM'),('ç‚¹','I-ZFM'),('ä¸­','I-ZFM'),('æ–‡','I-ZFM'),('ç½‘','I-ZFM'),('å•¤','B-YAS'),('é…’','I-YAS'),('çš„','I-YAS'),('ç½ª','I-YAS'),('å­½','I-YAS')]\n    ```\n\n### [å…³ç³»æŠ½å–ï¼ˆREï¼‰](https://github.com/zjunlp/DeepKE/tree/main/example/re/standard)\n\nä½¿ç”¨è€…å¯ä»¥ç›´æ¥ä¸‹è½½[æ¨¡å‹](https://drive.google.com/drive/folders/1wb_QIZduKDwrHeri0s5byibsSQrrJTEv)ä½¿ç”¨,æ­¥éª¤å¦‚ä¸‹ï¼š\n\n1. è¿›å…¥ç›®å½•`example/re/standard`\n2. ä¿®æ”¹ `conf/predict.yaml`ä¸­çš„å‚æ•°`fp`ä¸ºä¸‹è½½æ–‡ä»¶çš„è·¯å¾„ï¼Œ`conf/embedding.yaml`ä¸­`num_relations`ä¸º51ï¼ˆå…³ç³»ä¸ªæ•°ï¼‰,`conf/config.yaml`ä¸­çš„å‚æ•°modelä¸º`lm`\n3. ä¸‹è½½[æ•°æ®é›†](https://drive.google.com/drive/folders/1UurqpjePe3zhXxbDDNwLAjVvt7UyUMuQ)ï¼Œæ”¾å…¥ç›®å½•`example/re/standard/data/origin`ä¸­\n4. è¿›è¡Œé¢„æµ‹ï¼Œéœ€è¦é¢„æµ‹çš„æ–‡æœ¬åŠå®ä½“å¯¹é€šè¿‡ç»ˆç«¯è¾“å…¥ç»™ç¨‹åº\n\n    ```bash\n    python predict.py\n    ```\n\n    ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹ï¼Œè¿è¡Œ```python predict.py```åï¼Œåªéœ€è¾“å…¥çš„å¥å­ä¸ºâ€œæ­Œæ›²ã€Šäººç”Ÿé•¿è·¯ã€‹å‡ºè‡ªåˆ˜å¾·åå›½è¯­ä¸“è¾‘ã€Šç”·äººçš„çˆ±ã€‹ï¼Œç”±ææ³‰ä½œè¯ä½œæ›²ï¼Œ2001å¹´å‡ºè¡Œå‘ç‰ˆâ€ï¼Œç»™å®šçš„å®ä½“å¯¹ä¸ºâ€œç”·äººçš„çˆ±â€å’Œâ€œäººç”Ÿé•¿è·¯â€ï¼Œå¯å¾—åˆ°ç»“æœï¼Œæœ€ç»ˆæŠ½å–å‡ºçš„å…³ç³»ä¸ºç»è¿‡cnschemaå¯¹é½åçš„â€œæ‰€å±ä¸“è¾‘â€ã€‚\n\n    å°†predict.pyçš„_get_predict_instanceå‡½æ•°ä¿®æ”¹æˆå¦‚ä¸‹èŒƒä¾‹ï¼Œå³å¯ä¿®æ”¹æ–‡æœ¬è¿›è¡Œé¢„æµ‹\n\n    ```python\n    def _get_predict_instance(cfg):\n        flag = input('æ˜¯å¦ä½¿ç”¨èŒƒä¾‹[y/n]ï¼Œé€€å‡ºè¯·è¾“å…¥: exit .... ')\n        flag = flag.strip().lower()\n        if flag == 'y' or flag == 'yes':\n            sentence = 'æ­Œæ›²ã€Šäººç”Ÿé•¿è·¯ã€‹å‡ºè‡ªåˆ˜å¾·åå›½è¯­ä¸“è¾‘ã€Šç”·äººçš„çˆ±ã€‹ï¼Œç”±ææ³‰ä½œè¯ä½œæ›²ï¼Œ2001å¹´å‡ºè¡Œå‘ç‰ˆ'\n            head = 'ç”·äººçš„çˆ±'\n            tail = 'äººç”Ÿé•¿è·¯'\n            head_type = 'æ‰€å±ä¸“è¾‘'\n            tail_type = 'æ­Œæ›²'\n        elif flag == 'n' or flag == 'no':\n            sentence = input('è¯·è¾“å…¥å¥å­ï¼š')\n            head = input('è¯·è¾“å…¥å¥ä¸­éœ€è¦é¢„æµ‹å…³ç³»çš„å¤´å®ä½“ï¼š')\n            head_type = input('è¯·è¾“å…¥å¤´å®ä½“ç±»å‹ï¼š')\n            tail = input('è¯·è¾“å…¥å¥ä¸­éœ€è¦é¢„æµ‹å…³ç³»çš„å°¾å®ä½“ï¼š')\n            tail_type = input('è¯·è¾“å…¥å°¾å®ä½“ç±»å‹ï¼š')\n        elif flag == 'exit':\n            sys.exit(0)\n        else:\n            print('please input yes or no, or exit!')\n            _get_predict_instance()\n\n        instance = dict()\n        instance['sentence'] = sentence.strip()\n        instance['head'] = head.strip()\n        instance['tail'] = tail.strip()\n        if head_type.strip() == '' or tail_type.strip() == '':\n            cfg.replace_entity_with_type = False\n            instance['head_type'] = 'None'\n            instance['tail_type'] = 'None'\n        else:\n            instance['head_type'] = head_type.strip()\n            instance['tail_type'] = tail_type.strip()\n\n        return instance\n    ```\n\n    æœ€ç»ˆè¾“å‡ºç»“æœ\n\n    ```bash\n    â€œç”·äººçš„çˆ±â€å’Œâ€œäººç”Ÿé•¿è·¯â€åœ¨å¥ä¸­å…³ç³»ä¸ºâ€œæ‰€å±ä¸“è¾‘â€ï¼Œç½®ä¿¡åº¦ä¸º0.99\n    ```\n    > æ³¨ï¼šè¿è¡Œè¿‡ç¨‹ä¸­ä¼šè‡ªåŠ¨ä»huggingfaceç½‘ç«™ä¸­ä¸‹è½½`example/re/standard/conf/model/lm.yaml`æ–‡ä»¶ä¸­æŒ‡å®šçš„æ¨¡å‹ï¼Œå¦‚ä¸‹è½½å¤±è´¥å¯ä»¥å¯»æ‰¾å¯¹åº”é•œåƒç½‘ç«™æˆ–è€…æ‰‹åŠ¨ä¸‹è½½\n\n### [è”åˆä¸‰å…ƒç»„æŠ½å–](https://github.com/zjunlp/DeepKE/tree/main/example/triple)\nç”¨æˆ·å¯ä»¥å…ˆå°†ä¸Šè¿°æ¨¡å‹ä¸‹è½½è‡³æœ¬åœ°ï¼Œç„¶åä½¿ç”¨[example/triple](https://github.com/zjunlp/DeepKE/tree/main/example/triple)ä¸­çš„ä»£ç è¿›è¡Œä¸‰å…ƒç»„æŠ½å–ã€‚å¦‚æœå•å¥ä¸­å­˜åœ¨è¶…è¿‡ä¸¤ä¸ªä»¥ä¸Šçš„å®ä½“æ•°ï¼Œå¯èƒ½åœ¨ä¸€äº›å®ä½“å¯¹ä¸­ä¼šå­˜åœ¨é¢„æµ‹ä¸å‡†ç¡®çš„é—®é¢˜ï¼Œé‚£æ˜¯å› ä¸ºè¿™äº›å®ä½“å¯¹å¹¶æ²¡æœ‰è¢«åŠ å…¥è®­ç»ƒé›†ä¸­è¿›è¡Œè®­ç»ƒï¼Œæ‰€ä»¥éœ€è¦è¿›ä¸€æ­¥åˆ¤æ–­ï¼Œå…·ä½“ä½¿ç”¨æ­¥éª¤å¦‚ä¸‹ï¼š\n\n1. å°†`conf`æ–‡ä»¶å¤¹ä¸­çš„`predict.yaml`ä¸­çš„`text`ä¿®æ”¹ä¸ºé¢„æµ‹æ–‡æœ¬ï¼Œ`nerfp`ä¿®æ”¹ä¸ºneræ¨¡å‹æ–‡ä»¶å¤¹åœ°å€ï¼Œ`refp`ä¸ºreæ¨¡å‹åœ°å€\n2. è¿›è¡Œé¢„æµ‹ã€‚\n\n    ```bash\n    python predict.py\n    ```\n\n    æœŸé—´å°†è¾“å‡ºå„ä¸ªä¸­é—´æ­¥éª¤ç»“æœï¼Œä»¥è¾“å…¥æ–‡æœ¬`æ­¤å¤–ç½‘æ˜“äº‘å¹³å°è¿˜ä¸Šæ¶äº†ä¸€ç³»åˆ—æ­Œæ›²ï¼Œå…¶ä¸­åŒ…æ‹¬ç”°é¦¥ç”„çš„ã€Šå°å¹¸è¿ã€‹ç­‰`ä¸ºä¾‹ã€‚\n\n    2.1 è¾“å‡ºç»è¿‡neræ¨¡å‹åå¾—åˆ°ç»“æœ`[('ç”°', 'B-YAS'), ('é¦¥', 'I-YAS'), ('ç”„', 'I-YAS'), ('å°', 'B-QEE'), ('å¹¸', 'I-QEE'), ('è¿', 'I-QEE')]`ã€‚\n\n    2.2 è¾“å‡ºè¿›è¡Œå¤„ç†åç»“æœ`{'ç”°é¦¥ç”„': 'äººç‰©', 'å°å¹¸è¿': 'æ­Œæ›²'}`\n\n    2.3 è¾“å‡ºç»è¿‡reæ¨¡å‹åå¾—åˆ°ç»“æœ` \"ç”°é¦¥ç”„\" å’Œ \"å°å¹¸è¿\" åœ¨å¥ä¸­å…³ç³»ä¸ºï¼š\"æ­Œæ‰‹\"ï¼Œç½®ä¿¡åº¦ä¸º0.92ã€‚`\n\n    2.4 è¾“å‡ºjsonldæ ¼å¼åŒ–åç»“æœ \n    ```bash\n    {\n      \"@context\": {\n        \"æ­Œæ‰‹\": \"https://cnschema.openkg.cn/item/%E6%AD%8C%E6%89%8B/16693#viewPageContent\"\n      },\n      \"@id\": \"ç”°é¦¥ç”„\",\n      \"æ­Œæ‰‹\": {\n        \"@id\": \"å°å¹¸è¿\"\n      }\n    }\n    ```\n\n\n## è‡ªå®šä¹‰æ¨¡å‹\n\n### å®ä½“è¯†åˆ«ä»»åŠ¡ï¼ˆNERï¼‰\n\nå¦‚æœéœ€è¦ä½¿ç”¨è‡ªå®šä¹‰çš„æ•°æ®è¿›è¡Œè®­ç»ƒï¼Œæ­¥éª¤å¦‚ä¸‹ï¼š\n\n1. ä¸‹è½½è‡ªå®šä¹‰çš„[æ•°æ®é›†](https://drive.google.com/drive/folders/1zA8Ichx9nzU3GD92ptdyR_nmARB_7ovg)ï¼Œå°†å…¶æ”¾å…¥å‘½åä¸º`data`çš„æ–‡ä»¶å¤¹ä¸­\n2. å°†`conf`æ–‡ä»¶å¤¹ä¸­çš„`train.yaml`ä¸­çš„`bert_model`ä¿®æ”¹ä¸ºæŒ‡å®šæ¨¡å‹ï¼Œç”¨æˆ·å¯ä»¥é€šè¿‡ä¿®æ”¹yamlæ–‡ä»¶é€‰æ‹©ä¸åŒçš„æ¨¡å‹è¿›è¡Œè®­ç»ƒï¼ˆæ¨èç›´æ¥ä¸‹è½½æ¨¡å‹ï¼Œè®¾ç½®`bert_model`ä¸ºæ¨¡å‹è·¯å¾„ï¼‰\n3. ä¿®æ”¹`train.yaml`ä¸­çš„`labels`ä¸º`data/type.txt`ä¸­æ‰€ç”¨åˆ°çš„æ ‡ç­¾\n4. è¿›è¡Œè®­ç»ƒ\n\n    ```bash\n    python run.py\n    ```\n\n### å…³ç³»æŠ½å–ä»»åŠ¡ï¼ˆREï¼‰\n\nå¦‚æœéœ€è¦ä½¿ç”¨å…¶ä»–æ¨¡å‹è¿›è¡Œè®­ç»ƒï¼Œæ­¥éª¤å¦‚ä¸‹ï¼š\n\n1. ä¸‹è½½è‡ªå®šä¹‰çš„[æ•°æ®é›†](https://drive.google.com/drive/folders/1wb_QIZduKDwrHeri0s5byibsSQrrJTEv)ï¼Œå°†å…¶é‡å‘½åä¸º`data`\n2. å°†`conf`æ–‡ä»¶å¤¹ä¸­çš„`train.yaml`ä¸º`lm`,`lm.yaml`ä¸­çš„`lm_file`ä¿®æ”¹ä¸ºæŒ‡å®šé¢„è®­ç»ƒæ¨¡å‹ï¼Œ`embedding.yaml`ä¸­`num_relations`ä¸ºå…³ç³»çš„ä¸ªæ•°å¦‚51ï¼Œç”¨æˆ·å¯ä»¥é€šè¿‡ä¿®æ”¹yamlæ–‡ä»¶é€‰æ‹©ä¸åŒçš„æ¨¡å‹è¿›è¡Œè®­ç»ƒ\n3. è¿›è¡Œè®­ç»ƒã€‚\n\n    ```bash\n    python run.py\n    ```\n\n## FAQ\n\n**Q: è¿™ä¸ªæ¨¡å‹æ€ä¹ˆç”¨ï¼Ÿ**\nA: å¼€ç®±å³ç”¨ï¼Œä¸‹è½½å¥½æ¨¡å‹æŒ‰ç…§ä½¿ç”¨è¯´æ˜å°±èƒ½å¤ŸæŠ½å–é¢„å®šä¹‰cnSchemaåŒ…å«çš„çŸ¥è¯†ã€‚\n**å¦‚æœæƒ³æŠ½å–cnSchemaä¹‹å¤–çš„çŸ¥è¯†ï¼Œå¯ä»¥ä½¿ç”¨é«˜çº§ç‰ˆæœ¬è‡ªå®šä¹‰æ•°æ®è¿›è¡Œè®­ç»ƒå“¦**\n\n**Q: è¯·é—®æœ‰å…¶ä»–cnSchemaæŠ½å–æ¨¡å‹æä¾›å—ï¼Ÿ**\nA: å¾ˆé—æ†¾ï¼Œæˆ‘ä»¬æš‚æ—¶åªèƒ½æ”¯æŒéƒ¨åˆ†cnSchemaçš„çŸ¥è¯†æŠ½å–ï¼Œæœªæ¥ä¼šå‘å¸ƒæ›´å¤šçš„çŸ¥è¯†æŠ½å–æ¨¡å‹ã€‚\n\n**Q: æˆ‘è®­å‡ºæ¥æ¯”ä½ æ›´å¥½çš„ç»“æœï¼**\nA: æ­å–œä½ ã€‚\n\n**Q: è‡ªå·±æ•°æ®è¾“å…¥è¿›å»ç¼–ç æŠ¥é”™**\nA: å¯èƒ½ä¸­æ–‡è¾“å…¥æ•°æ®åŒ…å«äº†ä¸å¯è§çš„ç‰¹æ®Šå­—ç¬¦ï¼Œè¿™äº›å­—ç¬¦æ— æ³•è¢«æŸäº›ç¼–ç å› è€ŒæŠ¥é”™ï¼Œæ‚¨å¯ä»¥é€šè¿‡ç¼–è¾‘å™¨æˆ–å…¶ä»–å·¥å…·é¢„å¤„ç†ä¸­æ–‡æ•°æ®è§£å†³è¿™ä¸€é—®é¢˜ã€‚\n\n## å¼•ç”¨\n\nå¦‚æœæœ¬é¡¹ç›®ä¸­çš„èµ„æºæˆ–æŠ€æœ¯å¯¹ä½ çš„ç ”ç©¶å·¥ä½œæœ‰æ‰€å¸®åŠ©ï¼Œæ¬¢è¿åœ¨è®ºæ–‡ä¸­å¼•ç”¨ä¸‹è¿°è®ºæ–‡ã€‚\n\n```\n@inproceedings{DBLP:conf/emnlp/ZhangXTYYQXCLL22,\n  author    = {Ningyu Zhang and\n               Xin Xu and\n               Liankuan Tao and\n               Haiyang Yu and\n               Hongbin Ye and\n               Shuofei Qiao and\n               Xin Xie and\n               Xiang Chen and\n               Zhoubo Li and\n               Lei Li},\n  editor    = {Wanxiang Che and\n               Ekaterina Shutova},\n  title     = {DeepKE: {A} Deep Learning Based Knowledge Extraction Toolkit for Knowledge\n               Base Population},\n  booktitle = {Proceedings of the The 2022 Conference on Empirical Methods in Natural\n               Language Processing, {EMNLP} 2022 - System Demonstrations, Abu Dhabi,\n               UAE, December 7-11, 2022},\n  pages     = {98--108},\n  publisher = {Association for Computational Linguistics},\n  year      = {2022},\n  url       = {https://aclanthology.org/2022.emnlp-demos.10},\n  timestamp = {Thu, 23 Mar 2023 16:56:00 +0100},\n  biburl    = {https://dblp.org/rec/conf/emnlp/ZhangXTYYQXCLL22.bib},\n  bibsource = {dblp computer science bibliography, https://dblp.org}\n}\n```\n\n## å…è´£å£°æ˜\n\n**è¯¥é¡¹ç›®ä¸­çš„å†…å®¹ä»…ä¾›æŠ€æœ¯ç ”ç©¶å‚è€ƒï¼Œä¸ä½œä¸ºä»»ä½•ç»“è®ºæ€§ä¾æ®ã€‚ä½¿ç”¨è€…å¯ä»¥åœ¨è®¸å¯è¯èŒƒå›´å†…ä»»æ„ä½¿ç”¨è¯¥æ¨¡å‹ï¼Œä½†æˆ‘ä»¬ä¸å¯¹å› ä½¿ç”¨è¯¥é¡¹ç›®å†…å®¹é€ æˆçš„ç›´æ¥æˆ–é—´æ¥æŸå¤±è´Ÿè´£ã€‚**\n\n## é—®é¢˜åé¦ˆ\n\nå¦‚æœ‰é—®é¢˜ï¼Œè¯·åœ¨GitHub Issueä¸­æäº¤ã€‚\n\n"
        },
        {
          "name": "README_TAG.md",
          "type": "blob",
          "size": 24.517578125,
          "content": "<p align=\"center\">\n    <a href=\"https://github.com/zjunlp/deepke\"> <img src=\"pics/logo.png\" width=\"400\"/></a>\n<p>\n<p align=\"center\">  \n    <a href=\"http://deepke.zjukg.cn\">\n        <img alt=\"Documentation\" src=\"https://img.shields.io/badge/demo-website-blue\">\n    </a>\n    <a href=\"https://pypi.org/project/deepke/#files\">\n        <img alt=\"PyPI\" src=\"https://img.shields.io/pypi/v/deepke\">\n    </a>\n    <a href=\"https://github.com/zjunlp/DeepKE/blob/master/LICENSE\">\n        <img alt=\"GitHub\" src=\"https://img.shields.io/github/license/zjunlp/deepke\">\n    </a>\n    <a href=\"http://zjunlp.github.io/DeepKE\">\n        <img alt=\"Documentation\" src=\"https://img.shields.io/badge/doc-website-red\">\n    </a>\n    <a href=\"https://colab.research.google.com/drive/1vS8YJhJltzw3hpJczPt24O0Azcs3ZpRi?usp=sharing\">\n        <img alt=\"Open In Colab\" src=\"https://colab.research.google.com/assets/colab-badge.svg\">\n    </a>\n</p>\n<p align=\"center\">\n    <b> English | <a href=\"https://github.com/zjunlp/DeepKE/blob/main/README_TAG_CN.md\">ç®€ä½“ä¸­æ–‡</a> </b>\n</p>\n\n<h1 align=\"center\">\n    <p>Data Annotation Instructions</p>\n</h1>\n\n\n\nDeepKE is an open source knowledge graph extraction and construction tool that supports **low-resource, long-text and multi-modal** knowledge extraction tools. Based on PyTorch, it can realize named **entity recognition, relation extraction and attribute extraction functions**. This version, DeepKE-cnSchema, is an out-of-the-box version that allows users to download the model for entity and relational knowledge extraction that supports cnSchema.\n\n---\n\n## Content Introduction\n\n| Chapter                   | Description                                             |\n| ------------------------- | ------------------------------------------------------- |\n| [Introduction](#Introduction)             | The basic principles and supported data types of DeepKE |\n| [Manual Data Annotation](#Manual-Data-Annotation)  | How to manually annotate data                           |\n| [Automatic Data Annotation](#Automatic-Data-Annotation) | How to automatically annotate data based on DeepKE      |\n| [FAQ](#FAQ)                       | Frequently Asked Questions                              |\n| [References](#References)                | Technical reports for this catalogue                    |\n\n## Introduction\n\nDeepKE is an open source knowledge graph extraction and construction tool that supports low-resource, long-text and multi-modal knowledge extraction tools. Based on PyTorch, it can realize named entity recognition, relationship extraction and attribute extraction functions. Also available for beginners are detailed [documentation](https://zjunlp.github.io/DeepKE/), [Google Colab tutorials](https://colab.research.google.com/drive/1vS8YJhJltzw3hpJczPt24O0Azcs3ZpRi?usp=sharing),  [online presentations](http://deepke.zjukg.cn/)and [slideshows](https://github.com/zjunlp/DeepKE/blob/main/docs/slides/Slides-DeepKE-cn.pdf).\n\nIt is well known that data is very important for model training. To facilitate the use of this tool, DeepKE provides detailed annotation of entity identification and relationship extraction data, so that users can obtain training data manually or automatically. The annotated data can be directly used by DeepKE for model training.\n\n## Manual Data Annotation\n\n<div align=\"center\">\n  <img src=\"https://raw.githubusercontent.com/doccano/doccano/master/docs/images/logo/doccano.png\">\n</div>\n\n`doccano` is an open source manual data annotation tool. It provides annotation  functions for **text classification**, **sequence labeling**, and **sequence-to-sequence**. So you can create labeled data for sentiment analysis, named entity recognition, text summaries, and so on. Simply create a project and upload the data and start labeling, and you can build a dataset ready for `DeepKE` training in a matter of hours. Using `doccano` to extract annotation data for entity recognition and relationships is described below.\n\nFor details about doccano installation and configuration, see [Github(doccano)](https://github.com/doccano/doccano)\n\nOnce the server is installed and started, point your browser to  `http://0.0.0.0:8000` and click  **Log in**.\n\n<div align=\"center\">\n  <img src=\"pics/doccano_home_en.png\">\n</div>\n\n### Entity Recognition\n\n#### Create a Project\n\n- Create a Project. Click `Create` in the upper left corner to jump to the following interface.\n\n  - Select the Sequence Labeling task.\n  - Fill in the necessary information, such as Project name and Description.\n  - Check configurable attributes such as Allow overlapping entity and Use relation labeling **as required**.\n\n<div align=\"center\">\n  <img src=\"pics/doccano_create_project_en.png\">\n</div>\n\n * When creation is complete, you will automatically be redirected to the project's home page.\n\n#### Adding the corpus\n\n<div align=\"center\">\n  <img src=\"pics/doccano_data_format_en.png\">\n</div>\n\n\n\n\n- `doccano` supports a variety of text formats. The differences are as follows:\n  - `Textfile`ï¼šThe uploaded file is in the format of `txt`. When marking, a whole `txt` file is displayed as one page of content.\n  - `TextLine`ï¼šThe uploaded file is in the format of `txt`. When marking, a line of text in the `txt` file is displayed as a page of content.\n  - `JSONL`ï¼šShort for `JSON Lines`, where each line is a valid `JSON` value;\n  - `CoNLL`ï¼š A file in `CoNLL` format. Each line contains a series of tab-separated words.\n\n<div align=\"center\">\n  <img src=\"pics/doccano_dataset_en.png\">\n</div>\n\n* Click on the tabs of the **Dataset** again, and you'll see that text has been added to the project, one by one, and then you'll mark the text.\n\n#### Data Annotations\n\n- Add task labels\n\n  - The extraction task includes two label types: `Span` and `Relation`. Here, `Span` refers to the target information fragment in the original text, that is, the entity of a certain type in entity recognition.\n  - Fill in the name of the label. In entity recognition, you can write `PER`, `LOC`, `ORG`, etc.\n  - Add the shortcut key corresponding to this label (e.g. set the shortcut key `p` for the `PER` label) and define the label color.\n\n  <div align=\"center\">\n  <img src=\"pics/doccano_create_label_en.png\">\n  </div>\n\n  - Then just add the other tags you need in the same way.\n\n- Task annotation\n\n  - Annotate data. Click the `Annotate` button to the far right of each data to annotate.\n  - The example defines two `Span` type tags for people and places.\n\n  <div align=\"center\">\n  <img src=\"pics/doccano_annotate_en.png\">\n  </div>\n\n#### Exporting training data\n\n- Click on `Options`, `Export Dataset` in the Dataset column to export the annotated data.\n- The markup data is stored in the same text file, one line per sample and in `jsonl` format, which contains the following fieldsï¼š\n  - `id`: The unique identifier `ID` of the sample in the dataset.\n  - `text`: Raw text data.\n  - `entities`: The `Span` tags contained in the data, each `Span` tag contains four fieldsï¼š\n    - `id`: The unique identification ID of `Span` in the dataset.\n    - `start_offset`: The starting position of `Span`.\n    - `end_offset`: The next position from the end of `Span`.\n    - `label`: Type of `Span`.\n\n- Example of exported data\n\n```json\n{\n    \"id\":10,\n    \"text\":\"University of California is located in California, United States.\",\n    \"entities\":[\n        {\n            \"id\":15,\n            \"label\":\"ORG\",\n            \"start_offset\":0,\n            \"end_offset\":24\n        },\n        {\n            \"id\":16,\n            \"label\":\"LOC\",\n            \"start_offset\":39,\n            \"end_offset\":49\n        },\n        {\n            \"id\":17,\n            \"label\":\"LOC\",\n            \"start_offset\":51,\n            \"end_offset\":64\n        }\n    ],\n    \"relations\":[\n        \n    ]\n}\n```\n\n- The input data format for the entity recognition task in `DeepKE` is a `txt` file, with each line including words, separators and labels (see `CoNLL` data format). The exported data will be pre-processed into the `DeepKE` input format for training, please go to the detailed README \n  - [Regular Full Supervision STANDARD](https://github.com/zjunlp/DeepKE/tree/main/example/ner/standard)\n\n### Relation Extraction\n\n- [Create a Project](#Create-a-Project)\n  - Same operation as entity identification, just refer to above.\n- [Adding the corpus](#Adding-the-corpus)\n  - Same operation as entity identification, just refer to above.\n  - **In the input text, the text format `{text}*{head entity}*{tail entity}*{head entity type}*{tail entity type}`, where the head and tail entity types can be empty.**\n\n#### Data Annotations\n\n- Add task labels\n\n  - The extraction task contains two label types: `Span`and `Relation`. The `Relation` type is used here. `Relation` refers to the relation between `Span` in the original text, that is, the relation between two entities in the relation extraction.\n  - Fill in the name of the relationship type. In relation extraction, you can write `Graduation`, `Causal`, etc.\n  - Add the shortcut key corresponding to this relationship type (e.g. set the shortcut key `b` for the `Graduation` label) and define the label color.\n\n  <div align=\"center\">\n  <img src=\"pics/doccano_create_label_re_en.png\">\n  </div>\n\n  - Then just add the other tags you need in the same way.\n\n- Task annotation\n\n  - Annotate data. Click the `Annotate` button to the far right of each data to annotate.\n  - First click on the label of the relationship to be annotated, then click on the corresponding head and tail entities in turn to complete the relationship annotation.\n  - The example defines two `Span` type tags, `PER` and `LOC`, followed by the relationship tag `Graduation` between entities. The `Relation` tag points from the `Subject` corresponding entity to the `Object` corresponding entity.\n\n  <div align=\"center\">\n  <img src=\"pics/doccano_annotate_re_en.png\">\n  </div>\n\n#### Exporting training data\n\n- Click on `Options`, `Export Dataset` in the Dataset column to export the annotated data.\n- The markup data is stored in the same text file, one line per sample and in `jsonl` format, which contains the following fieldsï¼š\n\n  - `id`: The unique identifier `ID` of the sample in the dataset.\n  - `text`: Raw text data.\n  - `entities`: The `Span` tags contained in the data, each `Span` tag contains four fieldsï¼š\n    - `id`: The unique identification ID of `Span` in the dataset.\n    - `start_offset`: The starting position of `Span`.\n    - `end_offset`: The next position from the end of `Span`.\n    - `label`: Type of `Span`.\n\n  - `relations`: `Relation` tags contained in the data, each `Relation` tag contains four fieldsï¼š\n    - `id`: (`Span1`, `Relation`, `Span2`)Triples are uniquely identified in the dataset by their `ID`, and the same  triple in different samples corresponds to the same `ID`.\n    - `from_id`: The identifier `ID` corresponding to `Span1`.\n    - `to_id`: The identifier `ID` corresponding to `Span2`.\n    - `type`: Type of `Relation` .\n\n- Example of exported data\n\n```json\n{\n    \"id\":13,\n    \"text\":\"The collision resulted in two more crashes in the intersection, including a central concrete truck that was about to turn left onto college ave. *collision*crashes**\",\n    \"entities\":[\n        {\n            \"id\":20,\n            \"label\":\"MISC\",\n            \"start_offset\":4,\n            \"end_offset\":13\n        },\n        {\n            \"id\":21,\n            \"label\":\"MISC\",\n            \"start_offset\":35,\n            \"end_offset\":42\n        }\n    ],\n    \"relations\":[\n        {\n            \"id\":2,\n            \"from_id\":20,\n            \"to_id\":21,\n            \"type\":\"Cause-Effect\"\n        }\n    ]\n}\n```\n\n## Automatic Data Annotation\n\n### Entity Recognition\n\nIn order for users to better use `DeepKE` to complete entity recognition tasks, we provide an easy-to-use **dict matching based** entity recognition **automatic annotation** tool.\n\n#### Dict\n\n- The format of Dictï¼š\n\n  <h3 align=\"left\">\n      <img src=\"https://github.com/zjunlp/DeepKE/blob/main/example/ner/prepare-data/pics/vocab_dict.png?raw=true\", width=375>\n  </h3>\n\n- Two entity Dicts (one in Chinese and one in English) are provided in advance, and the samples are automatically tagged using the entity dictionary + jieba part-of-speech tagging.\n\n  - In Chinese example dict, we adapt [The People's Daily Dataset](https://github.com/OYE93/Chinese-NLP-Corpus/tree/master/NER/People's Daily) . It is a dataset for NER, concentrating on their types of named entities related to persons(PER), locations(LOC), and organizations(ORG).\n  - In English example dictï¼Œwe adapt Conll dataset. It contains named entities related to persons (PER), locations(LOC), and others (MISC).You can get the Conll dataset with the following command.\n\n  ```shell\n  wget 120.27.214.45/Data/ner/few_shot/data.tar.gz\n  ```\n\n  - Pre-provided dict from Google Driveï¼š\n    - [CN(vocab_dict_cn), EN(vocab_dict_en)](https://drive.google.com/drive/folders/1PGANizeTsvEQFYTL8O1jrDLZwk_MPqO0?usp=sharing)\n  - From BaiduNetDisk ï¼š \n    - [CN(vocab_dict_cn), EN(vocab_dict_en)](https://pan.baidu.com/s/1a07W42ZByeZ00MZp5pZgxg) \n    - (x7ba)\n\n- **If you need to build a domain self-built dictionary, please refer to the pre-provided dictionary format (csv)**\n\n  | Entity     | Label |\n  | ---------- | ----- |\n  | Washington | LOC   |\n  | ...        | ...   |\n\n#### Source File\n\n- **The input dictionary** format is `csv` (contains two columns, entities and corresponding labels).\n\n- **Data to be automatically marked** (txt format and separated by lines, as shown in the figure below) should be placed under the `source_data` path, the script will traverse all txt format files in this folder, and automatically mark line by line.\n\n  <h3 align=\"left\">\n      <img src=\"https://github.com/zjunlp/DeepKE/blob/main/example/ner/prepare-data/pics/en_input_data_format.png?raw=true\", width=700>\n  </h3>\n\n#### Output File\n\n- **The output file**(the distribution ratio of `training set`, `validation set`, and `test set` can be customized) can be directly used as training data in DeepKE.\n\n<h3 align=\"left\">\n    <img src=\"https://github.com/zjunlp/DeepKE/blob/main/example/ner/prepare-data/pics/en_output_data_format.png?raw=true\", width=375>\n</h3>\n\n\n#### Environment\n\nImplementation Environment:\n\n- jieba = 0.42.1\n\n#### Args Description\n\n- `language`: `cn` or `en`\n- `source_dir`: Corpus path (traverse all files in txt format under this folder, automatically mark line by line, the default is `source_data`)\n- `dict_dir`: Entity dict path (defaults to `vocab_dict.csv`)\n- `test_rate, dev_rate, test_rate`: The ratio of training_set, validation_set, and test_set (please make sure the sum is `1`, default `0.8:0.1:0.1`)\n\n#### Run\n\n- **Chinese**\n\n```bash\npython prepare_weaksupervised_data.py --language cn --dict_dir vocab_dict_cn.csv\n```\n\n- **English**\n\n```bash\npython prepare_weaksupervised_data.py --language en --dict_dir vocab_dict_en.csv\n```\n\n### Relation extraction\n\nWe provide a simple **distant supervised** based tool to label relation labels for our RE tasks.\n\n#### Source File\n\nWe specify the source file (dataset to be labeled) as `.json` format and include **one** pair of entities, head entity and tail entity respectively. Each piece of data should contain at least the following five items: `sentence`, `head`, `tail`, `head_offset`, `tail_offset`. The detailed json pattern is as follows:\n\n```json\n[\n  {\n    \"sentence\": \"This summer, the United States Embassy in Beirut, Lebanon, once again made its presence felt on the cultural scene by sponsoring a photo exhibition, an experimental jazz performance, a classical music concert and a visit from the Whiffenpoofs, Yale University's a cappella singers.\",\n    \"head\": \"Lebanon\",\n    \"tail\": \"Beirut\",\n    \"head_offset\": \"50\",\n    \"tail_offset\": \"42\",\n    //...\n  },\n  //... \n]\n```\n\n#### Triple File\n\nEntity pairs in source file will be matched with the triples in the triple file. The entity pairs will be labeled with the relation type if matched with the triples in triple file. If there is no triples match, the pairs will be labeled as `None` type.\n\nWe provide an [English](https://drive.google.com/drive/folders/1HHkm3RBI3okiu8jGs-Wn0vP_-0hz7pb2?usp=sharing) and a [Chinese](https://drive.google.com/file/d/1YpaMpivodG39p53MM9sMpB41q4EoiQhH/view?usp=sharing) triple file respectively. The English triple file comes from `NYT` dataset which contains the following relation types:\n\n```python\n\"/business/company/place_founded\",\n\"/people/person/place_lived\",\n\"/location/country/administrative_divisions\",\n\"/business/company/major_shareholders\",\n\"/sports/sports_team_location/teams\",\n\"/people/person/religion\",\n\"/people/person/place_of_birth\",\n\"/people/person/nationality\",\n\"/location/country/capital\",\n\"/business/company/advisors\",\n\"/people/deceased_person/place_of_death\",\n\"/business/company/founders\",\n\"/location/location/contains\",\n\"/people/person/ethnicity\",\n\"/business/company_shareholder/major_shareholder_of\",\n\"/people/ethnicity/geographic_distribution\",\n\"/people/person/profession\",\n\"/business/person/company\",\n\"/people/person/children\",\n\"/location/administrative_division/country\",\n\"/people/ethnicity/people\",\n\"/sports/sports_team/location\",\n\"/location/neighborhood/neighborhood_of\",\n\"/business/company/industry\"\n```\n\nThe Chinese triple file are from [here](https://github.com/DannyLee1991/ExtractTriples) with the following relation types:\n\n```json\n{\"object_type\": \"åœ°ç‚¹\", \"predicate\": \"ç¥–ç±\", \"subject_type\": \"äººç‰©\"}\n{\"object_type\": \"äººç‰©\", \"predicate\": \"çˆ¶äº²\", \"subject_type\": \"äººç‰©\"}\n{\"object_type\": \"åœ°ç‚¹\", \"predicate\": \"æ€»éƒ¨åœ°ç‚¹\", \"subject_type\": \"ä¼ä¸š\"}\n{\"object_type\": \"åœ°ç‚¹\", \"predicate\": \"å‡ºç”Ÿåœ°\", \"subject_type\": \"äººç‰©\"}\n{\"object_type\": \"ç›®\", \"predicate\": \"ç›®\", \"subject_type\": \"ç”Ÿç‰©\"}\n{\"object_type\": \"Number\", \"predicate\": \"é¢ç§¯\", \"subject_type\": \"è¡Œæ”¿åŒº\"}\n{\"object_type\": \"Text\", \"predicate\": \"ç®€ç§°\", \"subject_type\": \"æœºæ„\"}\n{\"object_type\": \"Date\", \"predicate\": \"ä¸Šæ˜ æ—¶é—´\", \"subject_type\": \"å½±è§†ä½œå“\"}\n{\"object_type\": \"äººç‰©\", \"predicate\": \"å¦»å­\", \"subject_type\": \"äººç‰©\"}\n{\"object_type\": \"éŸ³ä¹ä¸“è¾‘\", \"predicate\": \"æ‰€å±ä¸“è¾‘\", \"subject_type\": \"æ­Œæ›²\"}\n{\"object_type\": \"Number\", \"predicate\": \"æ³¨å†Œèµ„æœ¬\", \"subject_type\": \"ä¼ä¸š\"}\n{\"object_type\": \"åŸå¸‚\", \"predicate\": \"é¦–éƒ½\", \"subject_type\": \"å›½å®¶\"}\n{\"object_type\": \"äººç‰©\", \"predicate\": \"å¯¼æ¼”\", \"subject_type\": \"å½±è§†ä½œå“\"}\n{\"object_type\": \"Text\", \"predicate\": \"å­—\", \"subject_type\": \"å†å²äººç‰©\"}\n{\"object_type\": \"Number\", \"predicate\": \"èº«é«˜\", \"subject_type\": \"äººç‰©\"}\n{\"object_type\": \"ä¼ä¸š\", \"predicate\": \"å‡ºå“å…¬å¸\", \"subject_type\": \"å½±è§†ä½œå“\"}\n{\"object_type\": \"Number\", \"predicate\": \"ä¿®ä¸šå¹´é™\", \"subject_type\": \"å­¦ç§‘ä¸“ä¸š\"}\n{\"object_type\": \"Date\", \"predicate\": \"å‡ºç”Ÿæ—¥æœŸ\", \"subject_type\": \"äººç‰©\"}\n{\"object_type\": \"äººç‰©\", \"predicate\": \"åˆ¶ç‰‡äºº\", \"subject_type\": \"å½±è§†ä½œå“\"}\n{\"object_type\": \"äººç‰©\", \"predicate\": \"æ¯äº²\", \"subject_type\": \"äººç‰©\"}\n{\"object_type\": \"äººç‰©\", \"predicate\": \"ç¼–å‰§\", \"subject_type\": \"å½±è§†ä½œå“\"}\n{\"object_type\": \"å›½å®¶\", \"predicate\": \"å›½ç±\", \"subject_type\": \"äººç‰©\"}\n{\"object_type\": \"Number\", \"predicate\": \"æµ·æ‹”\", \"subject_type\": \"åœ°ç‚¹\"}\n{\"object_type\": \"ç½‘ç«™\", \"predicate\": \"è¿è½½ç½‘ç«™\", \"subject_type\": \"ç½‘ç»œå°è¯´\"}\n{\"object_type\": \"äººç‰©\", \"predicate\": \"ä¸ˆå¤«\", \"subject_type\": \"äººç‰©\"}\n{\"object_type\": \"Text\", \"predicate\": \"æœä»£\", \"subject_type\": \"å†å²äººç‰©\"}\n{\"object_type\": \"Text\", \"predicate\": \"æ°‘æ—\", \"subject_type\": \"äººç‰©\"}\n{\"object_type\": \"Text\", \"predicate\": \"å·\", \"subject_type\": \"å†å²äººç‰©\"}\n{\"object_type\": \"å‡ºç‰ˆç¤¾\", \"predicate\": \"å‡ºç‰ˆç¤¾\", \"subject_type\": \"ä¹¦ç±\"}\n{\"object_type\": \"äººç‰©\", \"predicate\": \"ä¸»æŒäºº\", \"subject_type\": \"ç”µè§†ç»¼è‰º\"}\n{\"object_type\": \"Text\", \"predicate\": \"ä¸“ä¸šä»£ç \", \"subject_type\": \"å­¦ç§‘ä¸“ä¸š\"}\n{\"object_type\": \"äººç‰©\", \"predicate\": \"æ­Œæ‰‹\", \"subject_type\": \"æ­Œæ›²\"}\n{\"object_type\": \"äººç‰©\", \"predicate\": \"ä½œè¯\", \"subject_type\": \"æ­Œæ›²\"}\n{\"object_type\": \"äººç‰©\", \"predicate\": \"ä¸»è§’\", \"subject_type\": \"ç½‘ç»œå°è¯´\"}\n{\"object_type\": \"äººç‰©\", \"predicate\": \"è‘£äº‹é•¿\", \"subject_type\": \"ä¼ä¸š\"}\n{\"object_type\": \"Date\", \"predicate\": \"æˆç«‹æ—¥æœŸ\", \"subject_type\": \"æœºæ„\"}\n{\"object_type\": \"å­¦æ ¡\", \"predicate\": \"æ¯•ä¸šé™¢æ ¡\", \"subject_type\": \"äººç‰©\"}\n{\"object_type\": \"Number\", \"predicate\": \"å åœ°é¢ç§¯\", \"subject_type\": \"æœºæ„\"}\n{\"object_type\": \"è¯­è¨€\", \"predicate\": \"å®˜æ–¹è¯­è¨€\", \"subject_type\": \"å›½å®¶\"}\n{\"object_type\": \"Text\", \"predicate\": \"é‚®æ”¿ç¼–ç \", \"subject_type\": \"è¡Œæ”¿åŒº\"}\n{\"object_type\": \"Number\", \"predicate\": \"äººå£æ•°é‡\", \"subject_type\": \"è¡Œæ”¿åŒº\"}\n{\"object_type\": \"åŸå¸‚\", \"predicate\": \"æ‰€åœ¨åŸå¸‚\", \"subject_type\": \"æ™¯ç‚¹\"}\n{\"object_type\": \"äººç‰©\", \"predicate\": \"ä½œè€…\", \"subject_type\": \"å›¾ä¹¦ä½œå“\"}\n{\"object_type\": \"Date\", \"predicate\": \"æˆç«‹æ—¥æœŸ\", \"subject_type\": \"ä¼ä¸š\"}\n{\"object_type\": \"äººç‰©\", \"predicate\": \"ä½œæ›²\", \"subject_type\": \"æ­Œæ›²\"}\n{\"object_type\": \"æ°”å€™\", \"predicate\": \"æ°”å€™\", \"subject_type\": \"è¡Œæ”¿åŒº\"}\n{\"object_type\": \"äººç‰©\", \"predicate\": \"å˜‰å®¾\", \"subject_type\": \"ç”µè§†ç»¼è‰º\"}\n{\"object_type\": \"äººç‰©\", \"predicate\": \"ä¸»æ¼”\", \"subject_type\": \"å½±è§†ä½œå“\"}\n{\"object_type\": \"ä½œå“\", \"predicate\": \"æ”¹ç¼–è‡ª\", \"subject_type\": \"å½±è§†ä½œå“\"}\n{\"object_type\": \"äººç‰©\", \"predicate\": \"åˆ›å§‹äºº\", \"subject_type\": \"ä¼ä¸š\"}\n```\n\nYou can also use your customized triple file, but the file format should be `.csv` and with the following parttern:\n\n| head    | tail   | rel                         |\n| ------- | ------ | --------------------------- |\n| Lebanon | Beirut | /location/location/contains |\n| ...     | ...    | ...                         |\n\n#### Output File\n\nThe output file names are `labeled_train.json`, `labeled_dev.json`, `labeled_test.json` for the `train`, `dev`, `test` dataset. The format of the output file is as follows:\n\n```json\n[\n\t{\n    \"sentence\": \"This summer, the United States Embassy in Beirut, Lebanon, once again made its presence felt on the cultural scene by sponsoring a photo exhibition, an experimental jazz performance, a classical music concert and a visit from the Whiffenpoofs, Yale University's a cappella singers.\",\n    \"head\": \"Lebanon\",\n    \"tail\": \"Beirut\",\n    \"head_offset\": \"50\",\n    \"tail_offset\": \"42\",\n    \"relation\": \"/location/location/contains\",\n    //...\n\t},\n  //...\n]\n```\n\nWe automatically split the source data into three splits with the rate `0.8:0.1:0.1`.You can set your own split rate.\n\n#### Args Description\n\n- `language`: `en` or `cn`\n- `source_file`: data file to be labeled\n- `triple_file`: triple file path\n- `test_rate, dev_rate, test_rate`: The ratio of training_set, validation_set, and test_set (please make sure the sum is `1`, default `0.8:0.1:0.1`)\n\n#### Run\n\n```bash\npython ds_label_data.py --language en --source_file source_data.json --triple_file triple_file.csv\n```\n\n## FAQ\n\n- **Q: How much data do I need to mark?**\n  - Aï¼šLabeling is a labor-intensive and time-consuming process that can be very costly. Ideally, the more data that is labeled, the better the model will be. But in practice, this is not always possible. On the one hand, it is necessary to combine practical resources and time; on the other hand, it is important to consider the impact of increasing data volume on the improvement of model effectiveness. In entity identification, relationship extraction usually requires a data volume of `10K` or so.\n\n\n- **Q: Is there any labeled data available?**\n  - A: In entity recognition, two entity dictionaries (one each in English and Chinese) are pre-provided to enable automatic annotation of samples.\n- **Q: Automatic labeling of data training models does not work**\n  - A: There are several possible reasons for the poor results of the modelï¼š\n    - Automatically annotated, manually annotated data contains a high level of noise \n    - Small data size: Large models with many parameters are overfitted on small datasets. \n  - solutionï¼š\n    - Consider checking the quality of the data\n    - Using strategies such as **semi-supervised** training or `Self-Traning`\n    - Increase the volume of data using data enhancement\n\n## References\n\nIf the resources or techniques in this project have been useful to your research, you are welcome to cite the following paper in your thesis.\n\n```\n@article{zhang2022deepke,\n  title={DeepKE: A Deep Learning Based Knowledge Extraction Toolkit for Knowledge Base Population},\n  author={Zhang, Ningyu and Xu, Xin and Tao, Liankuan and Yu, Haiyang and Ye, Hongbin and Qiao, Shuofei and Xie, Xin and Chen, Xiang and Li, Zhoubo and Li, Lei and Liang, Xiaozhuan and others},\n  journal={arXiv preprint arXiv:2201.03335},\n  year={2022}\n}\n```\n\n## Disclaimers\n\n**The contents of this project are for technical research purposes only and are not intended as a basis for any conclusive findings. Users are free to use the model as they wish within the scope of the licence, but we cannot be held responsible for direct or indirect damage resulting from the use of the contents of the project.** \n\n## Feedback\n\nIf you have any questions, please submit them in the GitHub Issue.\n"
        },
        {
          "name": "README_TAG_CN.md",
          "type": "blob",
          "size": 22.580078125,
          "content": "<p align=\"center\">\n    <a href=\"https://github.com/zjunlp/deepke\"> <img src=\"pics/logo.png\" width=\"400\"/></a>\n<p>\n<p align=\"center\">  \n    <a href=\"http://deepke.zjukg.cn\">\n        <img alt=\"Documentation\" src=\"https://img.shields.io/badge/demo-website-blue\">\n    </a>\n    <a href=\"https://pypi.org/project/deepke/#files\">\n        <img alt=\"PyPI\" src=\"https://img.shields.io/pypi/v/deepke\">\n    </a>\n    <a href=\"https://github.com/zjunlp/DeepKE/blob/master/LICENSE\">\n        <img alt=\"GitHub\" src=\"https://img.shields.io/github/license/zjunlp/deepke\">\n    </a>\n    <a href=\"http://zjunlp.github.io/DeepKE\">\n        <img alt=\"Documentation\" src=\"https://img.shields.io/badge/doc-website-red\">\n    </a>\n    <a href=\"https://colab.research.google.com/drive/1vS8YJhJltzw3hpJczPt24O0Azcs3ZpRi?usp=sharing\">\n        <img alt=\"Open In Colab\" src=\"https://colab.research.google.com/assets/colab-badge.svg\">\n    </a>\n</p>\n\n<p align=\"center\">\n    <b> <a href=\"https://github.com/zjunlp/DeepKE/blob/main/README_CNSCHEMA.md\">English</a> | ç®€ä½“ä¸­æ–‡ </b>\n</p>\n\n<h1 align=\"center\">\n    <p>æ•°æ®æ ‡æ³¨è¯´æ˜</p>\n</h1>\n\nDeepKE æ˜¯ä¸€ä¸ªå¼€æºçš„çŸ¥è¯†å›¾è°±æŠ½å–ä¸æ„å»ºå·¥å…·ï¼Œæ”¯æŒ<b>ä½èµ„æºã€é•¿ç¯‡ç« ã€å¤šæ¨¡æ€</b>çš„çŸ¥è¯†æŠ½å–å·¥å…·ï¼Œå¯ä»¥åŸºäº<b>PyTorch</b>å®ç°<b>å‘½åå®ä½“è¯†åˆ«</b>ã€<b>å…³ç³»æŠ½å–</b>å’Œ<b>å±æ€§æŠ½å–</b>åŠŸèƒ½ã€‚æ­¤ç‰ˆæœ¬DeepKE-cnSchemaä¸ºå¼€ç®±å³ç”¨ç‰ˆæœ¬ï¼Œç”¨æˆ·ä¸‹è½½æ¨¡å‹å³å¯å®ç°æ”¯æŒcnSchemaçš„å®ä½“å’Œå…³ç³»çŸ¥è¯†æŠ½å–ã€‚\n\n---\n\n## å†…å®¹å¯¼å¼•\n\n| ç« èŠ‚                      | æè¿°                   |\n| ----------------------- | -------------------- |\n| [ç®€ä»‹](#ç®€ä»‹)               | ä»‹ç»DeepKEåŸºæœ¬åŸç†å’Œæ”¯æŒçš„æ•°æ®ç±»å‹ |\n| [äººå·¥æ•°æ®æ ‡æ³¨](#äººå·¥æ•°æ®æ ‡æ³¨)       | ä»‹ç»å¦‚ä½•äººå·¥æ ‡æ³¨æ•°æ®           |\n| [è‡ªåŠ¨æ•°æ®æ ‡æ³¨](#è‡ªåŠ¨æ•°æ®æ ‡æ³¨) | ä»‹ç»å¦‚ä½•åŸºäºDeepKEè‡ªåŠ¨æ ‡æ³¨æ•°æ®   |\n| [FAQ](#FAQ)             | å¸¸è§é—®é¢˜ç­”ç–‘               |\n| [å¼•ç”¨](#å¼•ç”¨)               | æœ¬ç›®å½•çš„æŠ€æœ¯æŠ¥å‘Š             |\n\n## ç®€ä»‹\n\nDeepKE æ˜¯ä¸€ä¸ªå¼€æºçš„çŸ¥è¯†å›¾è°±æŠ½å–ä¸æ„å»ºå·¥å…·ï¼Œæ”¯æŒä½èµ„æºã€é•¿ç¯‡ç« ã€å¤šæ¨¡æ€çš„çŸ¥è¯†æŠ½å–å·¥å…·ï¼Œå¯ä»¥åŸºäºPyTorchå®ç°å‘½åå®ä½“è¯†åˆ«ã€å…³ç³»æŠ½å–å’Œå±æ€§æŠ½å–åŠŸèƒ½ã€‚åŒæ—¶ä¸ºåˆå­¦è€…æä¾›äº†è¯¦å°½çš„[æ–‡æ¡£](https://zjunlp.github.io/DeepKE/)ï¼Œ[Google Colabæ•™ç¨‹](https://colab.research.google.com/drive/1vS8YJhJltzw3hpJczPt24O0Azcs3ZpRi?usp=sharing)ï¼Œ[åœ¨çº¿æ¼”ç¤º](http://deepke.zjukg.cn/)å’Œ[å¹»ç¯ç‰‡](https://github.com/zjunlp/DeepKE/blob/main/docs/slides/Slides-DeepKE-cn.pdf)ã€‚\n\nä¼—æ‰€å‘¨çŸ¥ï¼Œæ•°æ®å¯¹æ¨¡å‹è®­ç»ƒéå¸¸é‡è¦ã€‚ä¸ºæ–¹ä¾¿ç”¨æˆ·ä½¿ç”¨æœ¬å·¥å…·ï¼ŒDeepKEæä¾›äº†è¯¦ç»†çš„å®ä½“è¯†åˆ«ã€å…³ç³»æŠ½å–æ•°æ®æ ‡æ³¨è¯´æ˜ï¼Œä»¥æ–¹ä¾¿ç”¨æˆ·é€šè¿‡äººå·¥æˆ–è‡ªåŠ¨æ–¹å¼å¾—åˆ°è®­ç»ƒæ•°æ®ï¼Œæ ‡æ³¨å¥½çš„æ•°æ®å¯ä»¥ç›´æ¥ä¾›DeepKEè¿›è¡Œæ¨¡å‹è®­ç»ƒã€‚\n\n## äººå·¥æ•°æ®æ ‡æ³¨\n\n<div align=\"center\">\n  <img src=\"https://raw.githubusercontent.com/doccano/doccano/master/docs/images/logo/doccano.png\">\n</div>\n\n`doccano`æ˜¯ä¸€ä¸ªå¼€æºçš„äººå·¥æ•°æ®æ ‡æ³¨å·¥å…·ã€‚å®ƒä¸º**æ–‡æœ¬åˆ†ç±»**ã€**åºåˆ—æ ‡è®°**å’Œ**åºåˆ—åˆ°åºåˆ—**æä¾›æ ‡æ³¨åŠŸèƒ½ã€‚å› æ­¤æ‚¨å¯ä»¥ä¸ºæƒ…æ„Ÿåˆ†æã€å‘½åå®ä½“è¯†åˆ«ã€æ–‡æœ¬æ‘˜è¦ç­‰ä»»åŠ¡åˆ›å»ºæ ‡è®°æ•°æ®ã€‚åªéœ€åˆ›å»ºä¸€ä¸ªé¡¹ç›®å¹¶ä¸Šä¼ æ•°æ®å¹¶å¼€å§‹æ ‡è®°ï¼Œæ‚¨å¯ä»¥åœ¨æ•°å°æ—¶å†…æ„å»ºå¯ä¾›`DeepKE`è®­ç»ƒçš„æ•°æ®é›†ã€‚ä¸‹é¢ä¼šä»‹ç»ä½¿ç”¨`doccano`ä¸ºå®ä½“è¯†åˆ«å’Œå…³ç³»æŠ½å–æ ‡æ³¨æ•°æ®ã€‚\n\ndoccanoçš„å®‰è£…ä¸é…ç½®å¯å‚è€ƒ [Github(doccano)](https://github.com/doccano/doccano)\n\nå®‰è£…å¹¶å¯åŠ¨serveråï¼Œåœ¨æµè§ˆå™¨ä¸­è®¿é—®`http://0.0.0.0:8000`ï¼Œå¹¶ç‚¹å‡»**ç™»é™†**ã€‚\n\n<div align=\"center\">\n  <img src=\"pics/doccano_home_cn.png\">\n</div>\n\n### å®ä½“è¯†åˆ«\n\n#### åˆ›å»ºé¡¹ç›®\n\n- åˆ›å»ºé¡¹ç›®ã€‚ç‚¹å‡»å·¦ä¸Šè§’çš„`åˆ›å»º`ï¼Œè·³è½¬è‡³ä»¥ä¸‹ç•Œé¢ã€‚\n\n    - é€‰æ‹©åºåˆ—æ ‡æ³¨ï¼ˆSequence Labelingï¼‰ä»»åŠ¡ã€‚\n    - å¡«å†™é¡¹ç›®åç§°ï¼ˆProject nameï¼‰ã€æè¿°ï¼ˆDescriptionï¼‰ç­‰å¿…è¦ä¿¡æ¯ã€‚\n    - **æŒ‰éœ€å‹¾é€‰**å…è®¸å®ä½“é‡å ï¼ˆAllow overlapping entityï¼‰ã€ä½¿ç”¨å…³ç³»æ ‡æ³¨ï¼ˆUse relation labelingï¼‰ç­‰å¯é…ç½®å±æ€§ã€‚\n\n<div align=\"center\">\n  <img src=\"pics/doccano_create_project_cn.png\">\n</div>\n\n * åœ¨åˆ›å»ºå®Œæˆåï¼Œä¼šè‡ªåŠ¨è·³è½¬åˆ°é¡¹ç›®çš„ä¸»é¡µã€‚\n\n#### æ·»åŠ è¯­æ–™åº“\n\n<div align=\"center\">\n  <img src=\"pics/doccano_data_format_cn.png\">\n</div>\n\n\n- `doccano`æ”¯æŒå¤šç§æ ¼å¼çš„æ–‡æœ¬ï¼Œå®ƒä»¬çš„åŒºåˆ«å¦‚ä¸‹ï¼š\n    - `Textfile`ï¼šä¸Šä¼ çš„æ–‡ä»¶ä¸º`txt`æ ¼å¼ï¼Œæ‰“æ ‡æ—¶ä¸€æ•´ä¸ª`txt`æ–‡ä»¶æ˜¾ç¤ºä¸ºä¸€é¡µå†…å®¹ï¼›\n    - `Textline`ï¼šä¸Šä¼ çš„æ–‡ä»¶ä¸º`txt`æ ¼å¼ï¼Œæ‰“æ ‡æ—¶`txt`æ–‡ä»¶çš„ä¸€è¡Œæ–‡å­—æ˜¾ç¤ºä¸ºä¸€é¡µå†…å®¹ï¼›\n    - `JSONL`ï¼š`JSON Lines`çš„ç®€å†™ï¼Œæ¯è¡Œæ˜¯ä¸€ä¸ªæœ‰æ•ˆçš„`JSON`å€¼ï¼›\n    - `CoNLL`ï¼š `CoNLL`æ ¼å¼çš„æ–‡ä»¶ï¼Œæ¯è¡Œå‡å¸¦æœ‰ä¸€ç³»åˆ—åˆ¶è¡¨ç¬¦åˆ†éš”çš„å•è¯ï¼›\n\n<div align=\"center\">\n  <img src=\"pics/doccano_dataset_cn.png\">\n</div>\n\n* å†ç‚¹å‡»**æ•°æ®é›†**çš„æ ‡ç­¾ï¼Œå°±å¯ä»¥çœ‹åˆ°ä¸€æ¡ä¸€æ¡çš„æ–‡æœ¬å·²ç»è¢«æ·»åŠ åˆ°é¡¹ç›®ä¸­äº†ï¼Œä¹‹åæˆ‘ä»¬å°†å¯¹è¿™äº›æ–‡æœ¬è¿›è¡Œæ‰“æ ‡ã€‚\n\n#### æ•°æ®æ ‡æ³¨\n\n- æ·»åŠ ä»»åŠ¡æ ‡ç­¾\n    - æŠ½å–å¼ä»»åŠ¡åŒ…å«`Span`ä¸`Relation`ä¸¤ç§æ ‡ç­¾ç±»å‹ï¼Œæ­¤å¤„é‡‡ç”¨`Span`ç±»å‹ï¼Œ`Span`æŒ‡åŸæ–‡æœ¬ä¸­çš„ç›®æ ‡ä¿¡æ¯ç‰‡æ®µï¼Œå³å®ä½“è¯†åˆ«ä¸­æŸä¸ªç±»å‹çš„å®ä½“ã€‚\n    - å¡«å…¥æ ‡ç­¾çš„åå­—ã€‚åœ¨å®ä½“è¯†åˆ«ä¸­ï¼Œå¯ä»¥å†™`PER`ã€`LOC`ã€`ORG`ç­‰ã€‚\n    - æ·»åŠ è¯¥æ ‡ç­¾å¯¹åº”çš„å¿«æ·é”®ï¼ˆä¾‹å¦‚ç»™`PER`æ ‡ç­¾è®¾ç½®ä¸ºå¿«æ·é”®`p`ï¼‰ï¼Œå¹¶å®šä¹‰æ ‡ç­¾é¢œè‰²ã€‚\n\n    <div align=\"center\">\n    <img src=\"pics/doccano_create_label_cn.png\">\n    </div>\n\n    - ä¹‹åä»¥åŒæ ·çš„æ–¹æ³•æ·»åŠ å…¶ä»–æ‰€éœ€è¦çš„æ ‡ç­¾å³å¯ã€‚\n\n- ä»»åŠ¡æ ‡æ³¨\n    - æ ‡æ³¨æ•°æ®ï¼Œç‚¹å‡»æ¯æ¡æ•°æ®æœ€å³è¾¹çš„`Annotate`æŒ‰é’®å¼€å§‹æ ‡æ³¨ã€‚\n    - ç¤ºä¾‹ä¸­å®šä¹‰äº†äººç‰©ã€åœ°ç‚¹ä¸¤ç§`Span`ç±»å‹æ ‡ç­¾ã€‚\n\n    <div align=\"center\">\n    <img src=\"pics/doccano_annotate_cn.png\">\n    </div>\n\n#### å¯¼å‡ºè®­ç»ƒæ•°æ®\n\n- åœ¨æ•°æ®é›†ä¸€æ ç‚¹å‡»`æ“ä½œ`ã€`å¯¼å‡ºæ•°æ®é›†`å¯¼å‡ºå·²æ ‡æ³¨çš„æ•°æ®ã€‚\n- æ ‡æ³¨æ•°æ®ä¿å­˜åœ¨åŒä¸€ä¸ªæ–‡æœ¬æ–‡ä»¶ä¸­ï¼Œæ¯æ¡æ ·ä¾‹å ä¸€è¡Œä¸”å­˜å‚¨ä¸º`jsonl`æ ¼å¼ï¼Œå…¶åŒ…å«ä»¥ä¸‹å­—æ®µ\n    - `id`: æ ·æœ¬åœ¨æ•°æ®é›†ä¸­çš„å”¯ä¸€æ ‡è¯†`ID`ã€‚\n    - `text`: åŸå§‹æ–‡æœ¬æ•°æ®ã€‚\n    - `entities`: æ•°æ®ä¸­åŒ…å«çš„`Span`æ ‡ç­¾ï¼Œæ¯ä¸ª`Span`æ ‡ç­¾åŒ…å«å››ä¸ªå­—æ®µï¼š\n        - `id`: `Span`åœ¨æ•°æ®é›†ä¸­çš„å”¯ä¸€æ ‡è¯†IDã€‚\n        - `start_offset`: `Span`çš„èµ·å§‹ä½ç½®ã€‚\n        - `end_offset`: `Span`ç»“æŸä½ç½®çš„ä¸‹ä¸€ä¸ªä½ç½®ã€‚\n        - `label`: `Span`çš„ç±»å‹ã€‚\n\n- å¯¼å‡ºæ•°æ®ç¤ºä¾‹\n\n```json\n{\n    \"id\":8,\n    \"text\":\"æ›¾ç»å»è¿‡æ·±åœ³ã€ç æµ·çš„ä¿­æ±¤é’å¹´å†œæ°‘å›ä¹¡åæ„å‘³æ·±é•¿åœ°è¯´ï¼Œä»å—å¾€åŒ—èµ°ï¼Œè¶Šèµ°è¶Šä¿å®ˆã€‚\",\n    \"entities\":[\n        {\n            \"id\":9,\n            \"label\":\"LOC\",\n            \"start_offset\":4,\n            \"end_offset\":6\n        },\n        {\n            \"id\":10,\n            \"label\":\"LOC\",\n            \"start_offset\":7,\n            \"end_offset\":9\n        },\n        {\n            \"id\":11,\n            \"label\":\"PER\",\n            \"start_offset\":12,\n            \"end_offset\":16\n        }\n    ],\n    \"relations\":[\n\n    ]\n}\n```\n\n- `DeepKE`ä¸­å®ä½“è¯†åˆ«ä»»åŠ¡è¾“å…¥æ•°æ®æ ¼å¼ä¸º`txt`æ–‡ä»¶ï¼Œæ¯è¡ŒåŒ…æ‹¬å•è¯ã€åˆ†éš”ç¬¦ã€æ ‡ç­¾ï¼ˆå¯å‚è€ƒ`CoNLL`æ•°æ®æ ¼å¼ï¼‰ã€‚å°†å¯¼å‡ºåçš„æ•°æ®é¢„å¤„ç†æˆ`DeepKE`å¯è¾“å…¥çš„æ ¼å¼å³å¯è¿›è¡Œè®­ç»ƒï¼Œå…·ä½“æµç¨‹è¯·è¿›å…¥è¯¦ç»†çš„READMEä¸­ã€‚\n    - [å¸¸è§„å…¨ç›‘ç£STANDARD](https://github.com/zjunlp/DeepKE/tree/main/example/ner/standard)\n\n### å…³ç³»æŠ½å–\n\n- [åˆ›å»ºé¡¹ç›®](#åˆ›å»ºé¡¹ç›®)\n    - ä¸å®ä½“è¯†åˆ«æ“ä½œä¸€è‡´ï¼Œå‚è€ƒä»¥ä¸Šå³å¯ã€‚\n- [æ·»åŠ è¯­æ–™åº“](#æ·»åŠ è¯­æ–™åº“)\n    - ä¸å®ä½“è¯†åˆ«æ“ä½œä¸€è‡´ï¼Œå‚è€ƒä»¥ä¸Šå³å¯ã€‚\n    - **è¾“å…¥æ–‡æœ¬ä¸­ï¼Œæ–‡æœ¬æ ¼å¼`{æ–‡æœ¬}*{å¤´å®ä½“}*{å°¾å®ä½“}*{å¤´å®ä½“ç±»å‹}*{å°¾å®ä½“ç±»å‹}`ï¼Œå…¶ä¸­å¤´å’Œå°¾å®ä½“ç±»å‹å¯ä¸ºç©ºã€‚**\n\n#### æ•°æ®æ ‡æ³¨\n\n- æ·»åŠ ä»»åŠ¡æ ‡ç­¾\n    - æŠ½å–å¼ä»»åŠ¡åŒ…å«`Span`ä¸`Relation`ä¸¤ç§æ ‡ç­¾ç±»å‹ï¼Œæ­¤å¤„é‡‡ç”¨`Relation`ç±»å‹ï¼Œ`Relation`æŒ‡åŸæ–‡æœ¬ä¸­`Span`ä¹‹é—´çš„å…³ç³»ï¼Œå³å…³ç³»æŠ½å–ä¸­ä¸¤ä¸ªå®ä½“é—´çš„å…³ç³»ã€‚\n    - å¡«å…¥å…³ç³»ç±»å‹çš„åå­—ã€‚åœ¨å…³ç³»æŠ½å–ä¸­ï¼Œå¯ä»¥å†™`æ¯•ä¸š`ã€`å› æœ`ç­‰ã€‚\n    - æ·»åŠ è¯¥å…³ç³»ç±»å‹å¯¹åº”çš„å¿«æ·é”®ï¼ˆä¾‹å¦‚ç»™`æ¯•ä¸š`æ ‡ç­¾è®¾ç½®ä¸ºå¿«æ·é”®`b`ï¼‰ï¼Œå¹¶å®šä¹‰æ ‡ç­¾é¢œè‰²ã€‚\n\n    <div align=\"center\">\n    <img src=\"pics/doccano_create_label_re_cn.png\">\n    </div>\n\n    - ä¹‹åä»¥åŒæ ·çš„æ–¹æ³•æ·»åŠ å…¶ä»–æ‰€éœ€è¦çš„æ ‡ç­¾å³å¯ã€‚\n\n- ä»»åŠ¡æ ‡æ³¨\n    - æ ‡æ³¨æ•°æ®ï¼Œç‚¹å‡»æ¯æ¡æ•°æ®æœ€å³è¾¹çš„`Annotate`æŒ‰é’®å¼€å§‹æ ‡æ³¨ã€‚\n    - é¦–å…ˆç‚¹å‡»å¾…æ ‡æ³¨çš„å…³ç³»æ ‡ç­¾ï¼Œæ¥ç€ä¾æ¬¡ç‚¹å‡»ç›¸åº”çš„å¤´å°¾å®ä½“å¯å®Œæˆå…³ç³»æ ‡æ³¨ã€‚\n    - ç¤ºä¾‹ä¸­å®šä¹‰äº†`PER`å’Œ`LOC`ä¸¤ç§`Span`ç±»å‹æ ‡ç­¾ï¼Œä¹‹åæ ‡æ³¨å®ä½“é—´çš„å…³ç³»æ ‡ç­¾`æ¯•ä¸š`ã€‚`Relation`æ ‡ç­¾ç”±`Subject`å¯¹åº”å®ä½“æŒ‡å‘`Object`å¯¹åº”å®ä½“ã€‚\n\n    <div align=\"center\">\n    <img src=\"pics/doccano_annotate_re_cn.png\">\n    </div>\n\n#### å¯¼å‡ºè®­ç»ƒæ•°æ®\n\n- åœ¨æ•°æ®é›†ä¸€æ ç‚¹å‡»`æ“ä½œ`ã€`å¯¼å‡ºæ•°æ®é›†`å¯¼å‡ºå·²æ ‡æ³¨çš„æ•°æ®ã€‚\n- æ ‡æ³¨æ•°æ®ä¿å­˜åœ¨åŒä¸€ä¸ªæ–‡æœ¬æ–‡ä»¶ä¸­ï¼Œæ¯æ¡æ ·ä¾‹å ä¸€è¡Œä¸”å­˜å‚¨ä¸º`jsonl`æ ¼å¼ï¼Œå…¶åŒ…å«ä»¥ä¸‹å­—æ®µ\n    - `id`: æ ·æœ¬åœ¨æ•°æ®é›†ä¸­çš„å”¯ä¸€æ ‡è¯†`ID`ã€‚\n    - `text`: åŸå§‹æ–‡æœ¬æ•°æ®ã€‚\n    - `entities`: æ•°æ®ä¸­åŒ…å«çš„`Span`æ ‡ç­¾ï¼Œæ¯ä¸ª`Span`æ ‡ç­¾åŒ…å«å››ä¸ªå­—æ®µï¼š\n        - `id`: `Span`åœ¨æ•°æ®é›†ä¸­çš„å”¯ä¸€æ ‡è¯†IDã€‚\n        - `start_offset`: `Span`çš„èµ·å§‹ä½ç½®ã€‚\n        - `end_offset`: `Span`ç»“æŸä½ç½®çš„ä¸‹ä¸€ä¸ªä½ç½®ã€‚\n        - `label`: `Span`çš„ç±»å‹ã€‚\n    - `relations`: æ•°æ®ä¸­åŒ…å«çš„`Relation`æ ‡ç­¾ï¼Œæ¯ä¸ª`Relation`æ ‡ç­¾åŒ…å«å››ä¸ªå­—æ®µï¼š\n        - `id`: (`Span1`, `Relation`, `Span2`)ä¸‰å…ƒç»„åœ¨æ•°æ®é›†ä¸­çš„å”¯ä¸€æ ‡è¯†`ID`ï¼Œä¸åŒæ ·æœ¬ä¸­çš„ç›¸åŒä¸‰å…ƒç»„å¯¹åº”åŒä¸€ä¸ª`ID`ã€‚\n        - `from_id`: `Span1`å¯¹åº”çš„æ ‡è¯†`ID`ã€‚\n        - `to_id`: `Span2`å¯¹åº”çš„æ ‡è¯†`ID`ã€‚\n        - `type`: `Relation`ç±»å‹ã€‚\n\n- å¯¼å‡ºæ•°æ®ç¤ºä¾‹\n\n```json\n{\n    \"id\":12,\n    \"text\":\"å¼ å»·æ™ºï¼Œ æ¯•ä¸šäºå¤§è¿å†›åŒ»å­¦é™¢ï¼Œä»äº‹ä¸­åŒ»æ²»ç–—ä¸­æ™šæœŸè‚¿ç˜¤ä¸´åºŠå·¥ä½œ30ä½™å¹´*å¼ å»¶æ™º*å¤§è¿å†›åŒ»å­¦é™¢*äººç‰©*å­¦æ ¡\",\n    \"entities\":[\n        {\n            \"id\":18,\n            \"label\":\"PER\",\n            \"start_offset\":0,\n            \"end_offset\":3\n        },\n        {\n            \"id\":19,\n            \"label\":\"ORG\",\n            \"start_offset\":8,\n            \"end_offset\":14\n        }\n    ],\n    \"relations\":[\n        {\n            \"id\":1,\n            \"from_id\":18,\n            \"to_id\":19,\n            \"type\":\"æ¯•ä¸š\"\n        }\n    ]\n}\n```\n\n## è‡ªåŠ¨æ•°æ®æ ‡æ³¨\n\n### å®ä½“è¯†åˆ«\n\nä¸ºäº†ç”¨æˆ·æ›´å¥½çš„ä½¿ç”¨`DeepKE`å®Œæˆå®ä½“è¯†åˆ«ä»»åŠ¡ï¼Œæˆ‘ä»¬æä¾›ä¸€ä¸ªç®€å•æ˜“ç”¨çš„**åŸºäºè¯å…¸åŒ¹é…**çš„å®ä½“è¯†åˆ«**è‡ªåŠ¨æ ‡æ³¨å·¥å…·**ã€‚\n\n#### è¯å…¸\n- è¯å…¸æ ¼å¼å¦‚ä¸‹æ‰€ç¤ºï¼š\n    <h3 align=\"left\">\n        <img src=\"https://github.com/zjunlp/DeepKE/blob/main/example/ner/prepare-data/pics/vocab_dict.png?raw=true\", width=375>\n    </h3>\n- é¢„æä¾›äº†ä¸¤ä¸ªå®ä½“è¯å…¸ï¼ˆä¸­è‹±æ–‡å„ä¸€ä¸ªï¼‰ï¼Œä½¿ç”¨å®ä½“è¯å…¸+jiebaè¯æ€§æ ‡æ³¨å¯¹æ ·æœ¬è¿›è¡Œè‡ªåŠ¨æ ‡æ³¨ã€‚\n\n    - ä¸­æ–‡ç¤ºä¾‹è¯å…¸ä¸­, æˆ‘ä»¬é‡‡ç”¨[(äººæ°‘æ—¥æŠ¥)æ•°æ®é›†](https://github.com/OYE93/Chinese-NLP-Corpus/tree/master/NER/People's%20Daily). å®ƒæ˜¯NERç›¸å…³çš„æ•°æ®é›†ï¼ŒåŒ…å«äººå‘˜ (PER)ã€ä½ç½® (LOC) å’Œç»„ç»‡ (ORG) ç›¸å…³çš„å‘½åå®ä½“è¯†åˆ«ã€‚\n\n    - è‹±æ–‡ç¤ºä¾‹è¯å…¸ä¸­ï¼Œæˆ‘ä»¬é‡‡ç”¨Conllæ•°æ®é›†ã€‚å®ƒåŒ…å«äººå‘˜ (PER)ã€ä½ç½® (LOC) å’Œå…¶ä»– (MISC) ç›¸å…³çš„å‘½åå®ä½“è¯†åˆ«ã€‚ä½ å¯é€šè¿‡å¦‚ä¸‹å‘½ä»¤è·å¾—Conllæ•°æ®é›†\n\n    ```shell\n    wget 120.27.214.45/Data/ner/few_shot/data.tar.gz\n    ```\n\n    - é¢„æä¾›è¯å…¸Google Driveä¸‹è½½é“¾æ¥ï¼š \n        - [ä¸­æ–‡(vocab_dict_cn), è‹±æ–‡(vocab_dict_en)](https://drive.google.com/drive/folders/1PGANizeTsvEQFYTL8O1jrDLZwk_MPqO0?usp=sharing)\n    - ç™¾åº¦ç½‘ç›˜ä¸‹è½½é“¾æ¥ï¼š \n        - [ä¸­æ–‡(vocab_dict_cn), è‹±æ–‡(vocab_dict_en)](https://pan.baidu.com/s/1a07W42ZByeZ00MZp5pZgxg) \n        - æå–ç (x7ba)\n\n- **è‹¥éœ€è¦æ„å»ºé¢†åŸŸè‡ªå»ºè¯å…¸ï¼Œè¯·å‚ç…§é¢„æä¾›è¯å…¸æ ¼å¼(csv)**\n\n    | å®ä½“ | è¯æ€§ |\n    |  --------  | ------  |\n    |  æ­å·  | LOC  |\n    |  ...  | ...  |\n\n#### æºæ–‡ä»¶\n\n- **è¾“å…¥çš„è¯å…¸**æ ¼å¼ä¸º`csv`ï¼ˆåŒ…å«ä¸¤åˆ—ï¼Œåˆ†åˆ«æ˜¯å®ä½“ä»¥åŠå¯¹åº”çš„æ ‡ç­¾ï¼‰ã€‚\n\n- **å¾…è‡ªåŠ¨æ‰“æ ‡çš„æ•°æ®**ï¼ˆtxtæ ¼å¼æŒ‰è¡Œåˆ†éš”ï¼Œå¦‚ä¸‹å›¾æ‰€ç¤ºï¼‰åº”æ”¾åœ¨`source_data`è·¯å¾„ä¸‹ï¼Œè„šæœ¬ä¼šéå†æ­¤æ–‡ä»¶å¤¹ä¸‹çš„æ‰€æœ‰txtæ ¼å¼çš„æ–‡ä»¶ï¼Œé€è¡Œè¿›è¡Œè‡ªåŠ¨æ‰“æ ‡ã€‚å…·ä½“ç¤ºä¾‹å¦‚ä¸‹ï¼š\n    <h3 align=\"left\">\n        <img src=\"https://github.com/zjunlp/DeepKE/blob/main/example/ner/prepare-data/pics/input_data_format.png?raw=true\", width=700>\n    </h3>\n\n\n#### è¾“å‡ºæ–‡ä»¶\n\n- è¾“å‡ºæ–‡ä»¶åŒ…å«ä¸‰ä¸ªï¼š`example_train_cn.txt`, `example_dev_cn.txt`, `example_test_cn.txt`ï¼Œåˆ†åˆ«å¯¹åº”è®­ç»ƒé›†ã€éªŒè¯é›†å’Œæµ‹è¯•é›†ã€‚æˆ‘ä»¬è‡ªåŠ¨å°†æºæ–‡ä»¶æ•°æ®åˆ’åˆ†ä¸ºä¸‰ä»½ï¼Œæ¯”ä¾‹ä¸º`0.8:0.1:0.1`ã€‚è¾“å‡ºæ–‡ä»¶çš„æ ¼å¼å¦‚ä¸‹ï¼š\n    <h3 align=\"left\">\n        <img src=\"https://github.com/zjunlp/DeepKE/blob/main/example/ner/prepare-data/pics/output_data_format.png?raw=true\", width=375>\n    </h3>\n\n#### ç¯å¢ƒ\nè¿è¡Œç¯å¢ƒ:  \n- jieba = 0.42.1\n\n#### å‚æ•°è§£é‡Š\n\n- `language`: å¯é€‰cn(ä¸­æ–‡)æˆ–en(è‹±æ–‡)\n- `source_dir`: è¯­æ–™åº“è·¯å¾„ï¼ˆéå†æ­¤æ–‡ä»¶å¤¹ä¸‹çš„æ‰€æœ‰txtæ ¼å¼çš„æ–‡ä»¶ï¼Œé€è¡Œè¿›è¡Œè‡ªåŠ¨æ‰“æ ‡ï¼Œé»˜è®¤ä¸º`source_data`ï¼‰\n- `dict_dir`: å®ä½“è¯å…¸è·¯å¾„ï¼ˆé»˜è®¤ä¸º`vocab_dict.csv`ï¼‰\n- `test_rate, dev_rate, test_rate`: è®­ç»ƒé›†ã€éªŒè¯é›†ã€æµ‹è¯•é›†å æ¯”ï¼ˆè¯·ç¡®ä¿æ€»å’Œä¸º`1`ï¼Œé»˜è®¤`0.8:0.1:0.1`ï¼‰\n\n#### è¿è¡Œ\n\n- **ä¸­æ–‡**\n```bash\npython prepare_weaksupervised_data.py --language cn --dict_dir vocab_dict_cn.csv\n```\n\n- **è‹±æ–‡**\n```bash\npython prepare_weaksupervised_data.py --language en --dict_dir vocab_dict_en.csv\n```\n\n\n### å…³ç³»æŠ½å–\n\nä¸ºäº†ç”¨æˆ·æ›´å¥½çš„ä½¿ç”¨`DeepKE`å®Œæˆå…³ç³»æŠ½å–ä»»åŠ¡ï¼Œæˆ‘ä»¬æä¾›ä¸€ä¸ªç®€å•æ˜“ç”¨çš„åŸºäºè¿œç¨‹ç›‘ç£çš„å…³ç³»æ ‡æ³¨å·¥å…·ã€‚\n\n#### æºæ–‡ä»¶\n\nç”¨æˆ·æä¾›çš„æºæ–‡ä»¶éœ€è¦ä¸º`.json`å½¢å¼ï¼Œå¹¶ä¸”æ¯æ¡æ•°æ®åªåŒ…å«ä¸€ä¸ªå®ä½“å¯¹ï¼Œåˆ†åˆ«ä¸ºå¤´å®ä½“å’Œå°¾å®ä½“ã€‚æ•°æ®ä¸­å¿…é¡»è‡³å°‘åŒ…å«ä»¥ä¸‹å››ä¸ªå­—æ®µï¼šsentence(å¥å­), head(å¤´å®ä½“), tail(å°¾å®ä½“), head_offset(å¤´å®ä½“åœ¨å¥å­ä¸­çš„åç§»ä½ç½®), tail_offset(å°¾å®ä½“åœ¨å¥å­ä¸­çš„åç§»ä½ç½®)ã€‚å…·ä½“çš„jsonå­—å…¸å½¢å¼å¦‚ä¸‹ï¼š\n\n```json\n[\n  {\n    \"sentence\": \"å¦‚ä½•æ¼”å¥½è‡ªå·±çš„è§’è‰²ï¼Œè¯·è¯»ã€Šæ¼”å‘˜è‡ªæˆ‘ä¿®å…»ã€‹ã€Šå–œå‰§ä¹‹ç‹ã€‹å‘¨æ˜Ÿé©°å´›èµ·äºç©·å›°æ½¦å€’ä¹‹ä¸­çš„ç‹¬é—¨ç§˜ç¬ˆ\",\n    \"head\": \"å‘¨æ˜Ÿé©°\",\n    \"tail\": \"å–œå‰§ä¹‹ç‹\",\n    \"head_offset\": \"26\",\n    \"tail_offset\": \"21\",\n    //...\n\t},\n  //...\n]\n```\n\n#### ä¸‰å…ƒç»„æ–‡ä»¶\n\næºæ–‡ä»¶ä¸­çš„å®ä½“å¯¹å°†ä¼šä¸ä¸‰å…ƒç»„æ–‡ä»¶ä¸­çš„ä¸‰å…ƒç»„è¿›è¡Œå­—ç¬¦ä¸²å…¨åŒ¹é…ã€‚å¦‚æœåŒ¹é…æˆåŠŸå®ä½“å¯¹å°†ä¼šè¢«æ ‡è®°ä¸ºä¸‰å…ƒç»„çš„å…³ç³»æ ‡ç­¾ï¼Œå¦‚æœæ²¡æœ‰åŒ¹é…é¡¹åˆ™è¢«æ ‡è®°ä¸º`None`ã€‚\n\næˆ‘ä»¬åˆ†è§£æä¾›äº†ä¸€ä¸ª[è‹±æ–‡](https://drive.google.com/drive/folders/1HHkm3RBI3okiu8jGs-Wn0vP_-0hz7pb2?usp=sharing)å’Œä¸€ä¸ª[ä¸­æ–‡](https://drive.google.com/file/d/1YpaMpivodG39p53MM9sMpB41q4EoiQhH/view?usp=sharing)ä¸‰å…ƒç»„æ–‡ä»¶ã€‚è‹±æ–‡ä¸‰å…ƒç»„æ¥è‡ª`NYT`æ•°æ®é›†ï¼ŒåŒ…å«å¦‚ä¸‹å…³ç³»ç±»å‹ï¼š\n\n```python\n\"/business/company/place_founded\",\n\"/people/person/place_lived\",\n\"/location/country/administrative_divisions\",\n\"/business/company/major_shareholders\",\n\"/sports/sports_team_location/teams\",\n\"/people/person/religion\",\n\"/people/person/place_of_birth\",\n\"/people/person/nationality\",\n\"/location/country/capital\",\n\"/business/company/advisors\",\n\"/people/deceased_person/place_of_death\",\n\"/business/company/founders\",\n\"/location/location/contains\",\n\"/people/person/ethnicity\",\n\"/business/company_shareholder/major_shareholder_of\",\n\"/people/ethnicity/geographic_distribution\",\n\"/people/person/profession\",\n\"/business/person/company\",\n\"/people/person/children\",\n\"/location/administrative_division/country\",\n\"/people/ethnicity/people\",\n\"/sports/sports_team/location\",\n\"/location/neighborhood/neighborhood_of\",\n\"/business/company/industry\"\n```\n\nä¸­æ–‡ä¸‰å…ƒç»„æ¥è‡ªGitHub[é“¾æ¥](https://github.com/DannyLee1991/ExtractTriples)ï¼ŒåŒ…å«å¦‚ä¸‹å…³ç³»ç±»å‹ï¼š\n\n```json\n{\"object_type\": \"åœ°ç‚¹\", \"predicate\": \"ç¥–ç±\", \"subject_type\": \"äººç‰©\"}\n{\"object_type\": \"äººç‰©\", \"predicate\": \"çˆ¶äº²\", \"subject_type\": \"äººç‰©\"}\n{\"object_type\": \"åœ°ç‚¹\", \"predicate\": \"æ€»éƒ¨åœ°ç‚¹\", \"subject_type\": \"ä¼ä¸š\"}\n{\"object_type\": \"åœ°ç‚¹\", \"predicate\": \"å‡ºç”Ÿåœ°\", \"subject_type\": \"äººç‰©\"}\n{\"object_type\": \"ç›®\", \"predicate\": \"ç›®\", \"subject_type\": \"ç”Ÿç‰©\"}\n{\"object_type\": \"Number\", \"predicate\": \"é¢ç§¯\", \"subject_type\": \"è¡Œæ”¿åŒº\"}\n{\"object_type\": \"Text\", \"predicate\": \"ç®€ç§°\", \"subject_type\": \"æœºæ„\"}\n{\"object_type\": \"Date\", \"predicate\": \"ä¸Šæ˜ æ—¶é—´\", \"subject_type\": \"å½±è§†ä½œå“\"}\n{\"object_type\": \"äººç‰©\", \"predicate\": \"å¦»å­\", \"subject_type\": \"äººç‰©\"}\n{\"object_type\": \"éŸ³ä¹ä¸“è¾‘\", \"predicate\": \"æ‰€å±ä¸“è¾‘\", \"subject_type\": \"æ­Œæ›²\"}\n{\"object_type\": \"Number\", \"predicate\": \"æ³¨å†Œèµ„æœ¬\", \"subject_type\": \"ä¼ä¸š\"}\n{\"object_type\": \"åŸå¸‚\", \"predicate\": \"é¦–éƒ½\", \"subject_type\": \"å›½å®¶\"}\n{\"object_type\": \"äººç‰©\", \"predicate\": \"å¯¼æ¼”\", \"subject_type\": \"å½±è§†ä½œå“\"}\n{\"object_type\": \"Text\", \"predicate\": \"å­—\", \"subject_type\": \"å†å²äººç‰©\"}\n{\"object_type\": \"Number\", \"predicate\": \"èº«é«˜\", \"subject_type\": \"äººç‰©\"}\n{\"object_type\": \"ä¼ä¸š\", \"predicate\": \"å‡ºå“å…¬å¸\", \"subject_type\": \"å½±è§†ä½œå“\"}\n{\"object_type\": \"Number\", \"predicate\": \"ä¿®ä¸šå¹´é™\", \"subject_type\": \"å­¦ç§‘ä¸“ä¸š\"}\n{\"object_type\": \"Date\", \"predicate\": \"å‡ºç”Ÿæ—¥æœŸ\", \"subject_type\": \"äººç‰©\"}\n{\"object_type\": \"äººç‰©\", \"predicate\": \"åˆ¶ç‰‡äºº\", \"subject_type\": \"å½±è§†ä½œå“\"}\n{\"object_type\": \"äººç‰©\", \"predicate\": \"æ¯äº²\", \"subject_type\": \"äººç‰©\"}\n{\"object_type\": \"äººç‰©\", \"predicate\": \"ç¼–å‰§\", \"subject_type\": \"å½±è§†ä½œå“\"}\n{\"object_type\": \"å›½å®¶\", \"predicate\": \"å›½ç±\", \"subject_type\": \"äººç‰©\"}\n{\"object_type\": \"Number\", \"predicate\": \"æµ·æ‹”\", \"subject_type\": \"åœ°ç‚¹\"}\n{\"object_type\": \"ç½‘ç«™\", \"predicate\": \"è¿è½½ç½‘ç«™\", \"subject_type\": \"ç½‘ç»œå°è¯´\"}\n{\"object_type\": \"äººç‰©\", \"predicate\": \"ä¸ˆå¤«\", \"subject_type\": \"äººç‰©\"}\n{\"object_type\": \"Text\", \"predicate\": \"æœä»£\", \"subject_type\": \"å†å²äººç‰©\"}\n{\"object_type\": \"Text\", \"predicate\": \"æ°‘æ—\", \"subject_type\": \"äººç‰©\"}\n{\"object_type\": \"Text\", \"predicate\": \"å·\", \"subject_type\": \"å†å²äººç‰©\"}\n{\"object_type\": \"å‡ºç‰ˆç¤¾\", \"predicate\": \"å‡ºç‰ˆç¤¾\", \"subject_type\": \"ä¹¦ç±\"}\n{\"object_type\": \"äººç‰©\", \"predicate\": \"ä¸»æŒäºº\", \"subject_type\": \"ç”µè§†ç»¼è‰º\"}\n{\"object_type\": \"Text\", \"predicate\": \"ä¸“ä¸šä»£ç \", \"subject_type\": \"å­¦ç§‘ä¸“ä¸š\"}\n{\"object_type\": \"äººç‰©\", \"predicate\": \"æ­Œæ‰‹\", \"subject_type\": \"æ­Œæ›²\"}\n{\"object_type\": \"äººç‰©\", \"predicate\": \"ä½œè¯\", \"subject_type\": \"æ­Œæ›²\"}\n{\"object_type\": \"äººç‰©\", \"predicate\": \"ä¸»è§’\", \"subject_type\": \"ç½‘ç»œå°è¯´\"}\n{\"object_type\": \"äººç‰©\", \"predicate\": \"è‘£äº‹é•¿\", \"subject_type\": \"ä¼ä¸š\"}\n{\"object_type\": \"Date\", \"predicate\": \"æˆç«‹æ—¥æœŸ\", \"subject_type\": \"æœºæ„\"}\n{\"object_type\": \"å­¦æ ¡\", \"predicate\": \"æ¯•ä¸šé™¢æ ¡\", \"subject_type\": \"äººç‰©\"}\n{\"object_type\": \"Number\", \"predicate\": \"å åœ°é¢ç§¯\", \"subject_type\": \"æœºæ„\"}\n{\"object_type\": \"è¯­è¨€\", \"predicate\": \"å®˜æ–¹è¯­è¨€\", \"subject_type\": \"å›½å®¶\"}\n{\"object_type\": \"Text\", \"predicate\": \"é‚®æ”¿ç¼–ç \", \"subject_type\": \"è¡Œæ”¿åŒº\"}\n{\"object_type\": \"Number\", \"predicate\": \"äººå£æ•°é‡\", \"subject_type\": \"è¡Œæ”¿åŒº\"}\n{\"object_type\": \"åŸå¸‚\", \"predicate\": \"æ‰€åœ¨åŸå¸‚\", \"subject_type\": \"æ™¯ç‚¹\"}\n{\"object_type\": \"äººç‰©\", \"predicate\": \"ä½œè€…\", \"subject_type\": \"å›¾ä¹¦ä½œå“\"}\n{\"object_type\": \"Date\", \"predicate\": \"æˆç«‹æ—¥æœŸ\", \"subject_type\": \"ä¼ä¸š\"}\n{\"object_type\": \"äººç‰©\", \"predicate\": \"ä½œæ›²\", \"subject_type\": \"æ­Œæ›²\"}\n{\"object_type\": \"æ°”å€™\", \"predicate\": \"æ°”å€™\", \"subject_type\": \"è¡Œæ”¿åŒº\"}\n{\"object_type\": \"äººç‰©\", \"predicate\": \"å˜‰å®¾\", \"subject_type\": \"ç”µè§†ç»¼è‰º\"}\n{\"object_type\": \"äººç‰©\", \"predicate\": \"ä¸»æ¼”\", \"subject_type\": \"å½±è§†ä½œå“\"}\n{\"object_type\": \"ä½œå“\", \"predicate\": \"æ”¹ç¼–è‡ª\", \"subject_type\": \"å½±è§†ä½œå“\"}\n{\"object_type\": \"äººç‰©\", \"predicate\": \"åˆ›å§‹äºº\", \"subject_type\": \"ä¼ä¸š\"}\n```\n\nç”¨æˆ·å¯ä»¥è‡ªå®šä¹‰ä¸‰å…ƒç»„æ–‡ä»¶ï¼Œä½†æ˜¯éœ€è¦è®¾å®šæ–‡ä»¶å½¢å¼ä¸º`.csv`å¹¶å…·æœ‰å¦‚ä¸‹æ ¼å¼ï¼š\n\n| å¤´å®ä½“ | å°¾å®ä½“   | å…³ç³» |\n| ------ | -------- | ---- |\n| å‘¨æ˜Ÿé©° | å–œå‰§ä¹‹ç‹ | ä¸»æ¼” |\n| ...    | ...      | ...  |\n\n#### è¾“å‡ºæ–‡ä»¶\n\nè¾“å‡ºæ–‡ä»¶åŒ…å«ä¸‰ä¸ªï¼š`labeled_train.json`, `labeled_dev.json`, `labeled_test.json`ï¼Œåˆ†åˆ«å¯¹åº”è®­ç»ƒé›†ã€éªŒè¯é›†å’Œæµ‹è¯•é›†ã€‚æˆ‘ä»¬è‡ªåŠ¨å°†æºæ–‡ä»¶æ•°æ®åˆ’åˆ†ä¸ºä¸‰ä»½ï¼Œæ¯”ä¾‹ä¸º0.8:0.1:0.1ã€‚è¾“å‡ºæ–‡ä»¶çš„æ ¼å¼å¦‚ä¸‹ï¼š\n\n```json\n[\n\t{\n    \"sentence\": \"å¦‚ä½•æ¼”å¥½è‡ªå·±çš„è§’è‰²ï¼Œè¯·è¯»ã€Šæ¼”å‘˜è‡ªæˆ‘ä¿®å…»ã€‹ã€Šå–œå‰§ä¹‹ç‹ã€‹å‘¨æ˜Ÿé©°å´›èµ·äºç©·å›°æ½¦å€’ä¹‹ä¸­çš„ç‹¬é—¨ç§˜ç¬ˆ\",\n    \"head\": \"å‘¨æ˜Ÿé©°\",\n    \"tail\": \"å–œå‰§ä¹‹ç‹\",\n    \"head_offset\": \"26\",\n    \"tail_offset\": \"21\",\n    \"relation\": \"ä¸»æ¼”\",\n    //...\n\t},\n  //...\n]\n```\n\n#### å‚æ•°æè¿°\n\n- `language`: 'en'å¯¹åº”è‹±æ–‡æ•°æ®ï¼Œ'cn'å¯¹åº”ä¸­æ–‡æ•°æ®\n- `source_file`: éœ€è¦æ ‡è®°çš„æºæ–‡ä»¶\n- `triple_file`: ä¸‰å…ƒç»„æ–‡ä»¶\n- `test_rate, dev_rate, test_rate`:  è®­ç»ƒé›†ã€éªŒè¯é›†å’Œæµ‹è¯•é›†çš„åˆ’åˆ†æ¯”ä¾‹ï¼Œéœ€è¦ä¿è¯æ¯”ä¾‹ä¹‹å’Œä¸º1ï¼Œé»˜è®¤ä¸º0.8:0.1:0.1ã€‚\n\n#### è¿è¡Œ\n\n```bash\npython ds_label_data.py --language en --source_file source_data.json --triple_file triple_file.csv\n```\n\n\n\n## FAQ\n\n- **Q: æˆ‘éœ€è¦æ ‡æ³¨å¤šå°‘æ•°æ®**\n    - A: æ ‡æ³¨æ˜¯ä¸€ä¸ªè€—è´¹äººåŠ›ä¸æ—¶é—´çš„è¿‡ç¨‹ï¼Œæˆæœ¬éå¸¸å¤§ã€‚ä»ç†æƒ³è§’åº¦çœ‹æ ‡æ³¨çš„æ•°æ®æ•°é‡è¶Šå¤šï¼Œè®­ç»ƒå¾—åˆ°çš„æ¨¡å‹æ•ˆæœä¹Ÿä¼šè¶Šå¥½ã€‚ä½†æ˜¯å®é™…æƒ…å†µå¾€å¾€å¹¶ä¸å…è®¸ã€‚ä¸€æ–¹é¢ï¼Œéœ€è¦ç»“åˆå®é™…çš„èµ„æºä¸æ—¶é—´ï¼›å¦ä¸€æ–¹é¢ï¼Œè¦è€ƒè™‘æ•°æ®é‡å¢åŠ å¯¹æ¨¡å‹æ•ˆæœæå‡å¸¦æ¥çš„å½±å“ã€‚åœ¨å®ä½“è¯†åˆ«ã€å…³ç³»æŠ½å–ä¸­é€šå¸¸éœ€è¦`10K`å·¦å³çš„æ•°æ®é‡ã€‚\n\n\n- **Q: è¯·é—®æœ‰æ ‡æ³¨å¥½çš„æ•°æ®æä¾›å—**\n    - A: åœ¨å®ä½“è¯†åˆ«ä¸­ï¼Œé¢„æä¾›äº†ä¸¤ä¸ªå®ä½“è¯å…¸ï¼ˆä¸­è‹±æ–‡å„ä¸€ä¸ªï¼‰ï¼Œèƒ½å¤Ÿå¯¹æ ·æœ¬è¿›è¡Œè‡ªåŠ¨æ ‡æ³¨ã€‚\n\n- **Q: è‡ªåŠ¨æ ‡æ³¨æ•°æ®è®­å‡ºæ¥çš„æ¨¡å‹æ•ˆæœä¸è¡Œ**\n    - A: æ¨¡å‹çš„æ•ˆæœå·®å¯èƒ½æœ‰ä»¥ä¸‹å‡ ä¸ªåŸå› ï¼š\n        - è‡ªåŠ¨æ ‡æ³¨ã€äººå·¥æ ‡æ³¨çš„æ•°æ®å«æœ‰è¾ƒå¤šçš„å™ªå£°ï¼Œ\n        - æ•°æ®é‡è¾ƒå°ï¼šå‚æ•°é‡å¾ˆå¤šçš„å¤§æ¨¡å‹åœ¨å°æ•°æ®é›†ä¸Šè¿‡æ‹Ÿåˆ\n    - solutionï¼š\n        - å¯ä»¥è€ƒè™‘æ£€æŸ¥æ•°æ®çš„è´¨é‡\n        - åˆ©ç”¨**åŠç›‘ç£**è®­ç»ƒæˆ–è€…`Self-Traning`ç­‰ç­–ç•¥\n        - ä½¿ç”¨æ•°æ®å¢å¼ºå¢å¤§æ•°æ®é‡\n\n## å¼•ç”¨\n\nå¦‚æœæœ¬é¡¹ç›®ä¸­çš„èµ„æºæˆ–æŠ€æœ¯å¯¹ä½ çš„ç ”ç©¶å·¥ä½œæœ‰æ‰€å¸®åŠ©ï¼Œæ¬¢è¿åœ¨è®ºæ–‡ä¸­å¼•ç”¨ä¸‹è¿°è®ºæ–‡ã€‚\n\n```\n@article{zhang2022deepke,\n  title={DeepKE: A Deep Learning Based Knowledge Extraction Toolkit for Knowledge Base Population},\n  author={Zhang, Ningyu and Xu, Xin and Tao, Liankuan and Yu, Haiyang and Ye, Hongbin and Qiao, Shuofei and Xie, Xin and Chen, Xiang and Li, Zhoubo and Li, Lei and Liang, Xiaozhuan and others},\n  journal={arXiv preprint arXiv:2201.03335},\n  year={2022}\n}\n```\n\n## å…è´£å£°æ˜\n\n**è¯¥é¡¹ç›®ä¸­çš„å†…å®¹ä»…ä¾›æŠ€æœ¯ç ”ç©¶å‚è€ƒï¼Œä¸ä½œä¸ºä»»ä½•ç»“è®ºæ€§ä¾æ®ã€‚ä½¿ç”¨è€…å¯ä»¥åœ¨è®¸å¯è¯èŒƒå›´å†…ä»»æ„ä½¿ç”¨è¯¥æ¨¡å‹ï¼Œä½†æˆ‘ä»¬ä¸å¯¹å› ä½¿ç”¨è¯¥é¡¹ç›®å†…å®¹é€ æˆçš„ç›´æ¥æˆ–é—´æ¥æŸå¤±è´Ÿè´£ã€‚**\n\n## é—®é¢˜åé¦ˆ\n\nå¦‚æœ‰é—®é¢˜ï¼Œè¯·åœ¨GitHub Issueä¸­æäº¤ã€‚\n"
        },
        {
          "name": "docker",
          "type": "tree",
          "content": null
        },
        {
          "name": "docs",
          "type": "tree",
          "content": null
        },
        {
          "name": "example",
          "type": "tree",
          "content": null
        },
        {
          "name": "pics",
          "type": "tree",
          "content": null
        },
        {
          "name": "pretrained",
          "type": "tree",
          "content": null
        },
        {
          "name": "requirements.txt",
          "type": "blob",
          "size": 0.373046875,
          "content": "torch>=1.5,<=1.11\nhydra-core==1.0.6\ntensorboard==2.4.1\nmatplotlib==3.4.1\ntransformers==4.26.0\njieba==0.42.1\nscikit-learn==0.24.1\nseqeval==1.2.2\nopt-einsum==3.3.0\nwandb==0.12.7\nujson==5.6.0\nhuggingface_hub==0.11.0\ntensorboardX==2.5.1\nnltk==3.8\nprotobuf==3.20.1\nnumpy==1.21.0\nipdb==0.13.11\npytorch-crf==0.7.2\ntqdm==4.66.1\nopenai==0.28.0\nJinja2==3.1.2\ndatasets==2.13.2\npyhocon==0.3.60\n"
        },
        {
          "name": "setup.py",
          "type": "blob",
          "size": 0.9091796875,
          "content": "from setuptools import setup, find_packages\n\n\n# with open(\"requirements.txt\") as requirements_file:\n#     requirements = requirements_file.read().splitlines()\n    \nsetup(\n    name='deepke',  # æ‰“åŒ…åçš„åŒ…æ–‡ä»¶å\n    version='2.2.7',    #ç‰ˆæœ¬å·\n    keywords=[\"pip\", \"RE\",\"NER\",\"AE\"],    # å…³é”®å­—\n    description='DeepKE is a knowledge extraction toolkit for knowledge graph construction supporting low-resource, document-level and multimodal scenarios for entity, relation and attribute extraction.',  # è¯´æ˜\n    license=\"MIT\",  # è®¸å¯\n    url='https://github.com/zjunlp/deepke',\n    author='ZJUNLP',\n    author_email='zhangningyu@zju.edu.cn',\n    include_package_data=True,\n    platforms=\"any\",\n    package_dir={\"\": \"src\"},\n    packages=find_packages(\"src\"),\n    # install_requires=requirements,\n    classifiers=[\n        \"Programming Language :: Python :: 3\",\n        \"Operating System :: OS Independent\",\n    ]\n)\n"
        },
        {
          "name": "src",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}