{
  "metadata": {
    "timestamp": 1736559824735,
    "page": 562,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjU3MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "mlfoundations/open_flamingo",
      "stars": 3795,
      "defaultBranch": "main",
      "files": [
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 1.85546875,
          "content": "*.pt\n*.json\n\nwandb/\n\n# Byte-compiled / optimized / DLL files\n__pycache__/\n*.py[cod]\n*$py.class\n\n# C extensions\n*.so\n\n# Distribution / packaging\n.Python\nbuild/\ndevelop-eggs/\ndist/\ndownloads/\neggs/\n.eggs/\nlib/\nlib64/\nparts/\nsdist/\nvar/\nwheels/\npip-wheel-metadata/\nshare/python-wheels/\n*.egg-info/\n.installed.cfg\n*.egg\nMANIFEST\n\n# PyInstaller\n#  Usually these files are written by a python script from a template\n#  before PyInstaller builds the exe, so as to inject date/other infos into it.\n*.manifest\n*.spec\n\n# Installer logs\npip-log.txt\npip-delete-this-directory.txt\n\n# Unit test / coverage reports\nhtmlcov/\n.tox/\n.nox/\n.coverage\n.coverage.*\n.cache\nnosetests.xml\ncoverage.xml\n*.cover\n*.py,cover\n.hypothesis/\n.pytest_cache/\n\n# Translations\n*.mo\n*.pot\n\n# Django stuff:\n*.log\nlocal_settings.py\ndb.sqlite3\ndb.sqlite3-journal\n\n# Flask stuff:\ninstance/\n.webassets-cache\n\n# Scrapy stuff:\n.scrapy\n\n# Sphinx documentation\ndocs/_build/\n\n# PyBuilder\ntarget/\n\n# Jupyter Notebook\n.ipynb_checkpoints\n\n# IPython\nprofile_default/\nipython_config.py\n\n# pyenv\n.python-version\n\n# pipenv\n#   According to pypa/pipenv#598, it is recommended to include Pipfile.lock in version control.\n#   However, in case of collaboration, if having platform-specific dependencies or dependencies\n#   having no cross-platform support, pipenv may install dependencies that don't work, or not\n#   install all needed dependencies.\n#Pipfile.lock\n\n# PEP 582; used by e.g. github.com/David-OConnor/pyflow\n__pypackages__/\n\n# Celery stuff\ncelerybeat-schedule\ncelerybeat.pid\n\n# SageMath parsed files\n*.sage.py\n\n# Environments\n.env\n.venv\nenv/\nvenv/\nENV/\nenv.bak/\nvenv.bak/\n\n# Pycharm project settings\n.idea\n\n# Spyder project settings\n.spyderproject\n.spyproject\n\n# Rope project settings\n.ropeproject\n\n# mkdocs documentation\n/site\n\n# mypy\n.mypy_cache/\n.dmypy.json\ndmypy.json\n\n*.out\nsrc/wandb\nwandb\n\n# Pyre type checker\n.pyre/\n\n# Cache\ncache/\n\n__*.sh"
        },
        {
          "name": ".pre-commit-config.yaml",
          "type": "blob",
          "size": 0.15625,
          "content": "---\ndefault_language_version:\n    python: python3\n\nrepos:\n  - repo: https://github.com/psf/black-pre-commit-mirror\n    rev: 23.9.1\n    hooks:\n      - id: black\n"
        },
        {
          "name": "HISTORY.md",
          "type": "blob",
          "size": 0.421875,
          "content": "## 2.0.0\n* Add gradient checkpointing, FullyShardedDataParallel\n* Model releases \n    * (CLIP ViT-L-14 / MPT-1B)\n    * (CLIP ViT-L-14 / MPT-1B Dolly)\n    * (CLIP ViT-L-14 / RedPajama-3B)\n    * (CLIP ViT-L-14 / RedPajama-3B Instruct)\n    * (CLIP ViT-L-14 / MPT-7B)\n* Remove color jitter when training\n* Fix cross-attention bug when calling generate()\n\n## 1.0.0\n\n* Initial code release\n* Early model release (CLIP ViT-L-14 / LLaMA-7B)"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 1.25390625,
          "content": "MIT License\n\nCopyright (c) 2023 Anas Awadalla, Irena Gao, Joshua Gardner,  Jack Hessel, Yusuf Hanafy, Wanrong Zhu, Kalyani Marathe,  Yonatan Bitton, Samir Gadre, Jenia Jitsev, Simon Kornblith,  Pang Wei Koh, Gabriel Ilharco, Mitchell Wortsman, Ludwig Schmidt.\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n"
        },
        {
          "name": "Makefile",
          "type": "blob",
          "size": 0.70703125,
          "content": "install: ## [Local development] Upgrade pip, install requirements, install package.\n\tpython -m pip install -U pip\n\tpython -m pip install -e .\n\ninstall-dev: ## [Local development] Install test requirements\n\tpython -m pip install -r requirements-dev.txt\n\nlint: ## [Local development] Run mypy, pylint and black\n\tpython -m mypy open_flamingo\n\tpython -m pylint open_flamingo\n\tpython -m black --check -l 120 open_flamingo\n\nblack: ## [Local development] Auto-format python code using black\n\tpython -m black -l 120 .\n\n.PHONY: help\n\nhelp: # Run `make help` to get help on the make commands\n\t@grep -E '^[0-9a-zA-Z_-]+:.*?## .*$$' $(MAKEFILE_LIST) | sort | awk 'BEGIN {FS = \":.*?## \"}; {printf \"\\033[36m%-30s\\033[0m %s\\n\", $$1, $$2}'\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 13.4833984375,
          "content": "# ðŸ¦© OpenFlamingo\n\n[![PyPI version](https://badge.fury.io/py/open_flamingo.svg)](https://badge.fury.io/py/open_flamingo)\n\n[Paper](https://arxiv.org/abs/2308.01390) | Blog posts: [1](https://laion.ai/blog/open-flamingo/), [2](https://laion.ai/blog/open-flamingo-v2/) | [Demo](https://huggingface.co/spaces/openflamingo/OpenFlamingo)\n\nWelcome to our open source implementation of DeepMind's [Flamingo](https://www.deepmind.com/blog/tackling-multiple-tasks-with-a-single-visual-language-model)! \n\nIn this repository, we provide a PyTorch implementation for training and evaluating OpenFlamingo models.\nIf you have any questions, please feel free to open an issue. We also welcome contributions!\n\n# Table of Contents\n- [Installation](#installation)\n- [Approach](#approach)\n  * [Model architecture](#model-architecture)\n- [Usage](#usage)\n  * [Initializing an OpenFlamingo model](#initializing-an-openflamingo-model)\n  * [Generating text](#generating-text)\n- [Training](#training)\n  * [Dataset](#dataset)\n- [Evaluation](#evaluation)\n- [Future plans](#future-plans)\n- [Team](#team)\n- [Acknowledgments](#acknowledgments)\n- [Citing](#citing)\n\n# Installation\n\nTo install the package in an existing environment, run \n```\npip install open-flamingo\n```\n\nor to create a conda environment for running OpenFlamingo, run\n```\nconda env create -f environment.yml\n```\n\nTo install training or eval dependencies, run one of the first two commands. To install everything, run the third command.\n```\npip install open-flamingo[training]\npip install open-flamingo[eval]\npip install open-flamingo[all]\n```\n\nThere are three `requirements.txt` files: \n- `requirements.txt` \n- `requirements-training.txt`\n- `requirements-eval.txt`\n\nDepending on your use case, you can install any of these with `pip install -r <requirements-file.txt>`. The base file contains only the dependencies needed for running the model.\n\n## Development\n\nWe use pre-commit hooks to align formatting with the checks in the repository. \n1. To install pre-commit, run\n    ```\n    pip install pre-commit\n    ```\n    or use brew for MacOS\n    ```\n    brew install pre-commit\n    ```\n2. Check the version installed with\n    ```\n    pre-commit --version\n    ```\n3. Then at the root of this repository, run\n    ```\n    pre-commit install\n    ```\nThen every time we run git commit, the checks are run. If the files are reformatted by the hooks, run `git add` for your changed files and `git commit` again\n\n# Approach\nOpenFlamingo is a multimodal language model that can be used for a variety of tasks. It is trained on a large multimodal dataset (e.g. Multimodal C4) and can be used to generate text conditioned on interleaved images/text. For example, OpenFlamingo can be used to generate a caption for an image, or to generate a question given an image and a text passage. The benefit of this approach is that we are able to rapidly adapt to new tasks using in-context learning.\n\n## Model architecture\nOpenFlamingo combines a pretrained vision encoder and a language model using cross attention layers. The model architecture is shown below.\n\n![OpenFlamingo architecture](docs/flamingo.png) \nCredit: [Flamingo](https://www.deepmind.com/blog/tackling-multiple-tasks-with-a-single-visual-language-model)\n\n# Usage\n## Initializing an OpenFlamingo model\nWe support pretrained vision encoders from the [OpenCLIP](https://github.com/mlfoundations/open_clip) package, which includes OpenAI's pretrained models. \nWe also support pretrained language models from the `transformers` package, such as [MPT](https://huggingface.co/models?search=mosaicml%20mpt), [RedPajama](https://huggingface.co/models?search=redpajama), [LLaMA](https://huggingface.co/models?search=llama), [OPT](https://huggingface.co/models?search=opt), [GPT-Neo](https://huggingface.co/models?search=gpt-neo), [GPT-J](https://huggingface.co/models?search=gptj), and [Pythia](https://huggingface.co/models?search=pythia) models.\n\n``` python\nfrom open_flamingo import create_model_and_transforms\n\nmodel, image_processor, tokenizer = create_model_and_transforms(\n    clip_vision_encoder_path=\"ViT-L-14\",\n    clip_vision_encoder_pretrained=\"openai\",\n    lang_encoder_path=\"anas-awadalla/mpt-1b-redpajama-200b\",\n    tokenizer_path=\"anas-awadalla/mpt-1b-redpajama-200b\",\n    cross_attn_every_n_layers=1,\n    cache_dir=\"PATH/TO/CACHE/DIR\"  # Defaults to ~/.cache\n)\n```\n\n## Released OpenFlamingo models\nWe have trained the following OpenFlamingo models so far.\n\n|# params|Language model|Vision encoder|Xattn interval*|COCO 4-shot CIDEr|VQAv2 4-shot Accuracy|Weights|\n|------------|--------------|--------------|----------|-----------|-------|----|\n|3B| anas-awadalla/mpt-1b-redpajama-200b | openai CLIP ViT-L/14 | 1 | 77.3 | 45.8 |[Link](https://huggingface.co/openflamingo/OpenFlamingo-3B-vitl-mpt1b)|\n|3B| anas-awadalla/mpt-1b-redpajama-200b-dolly | openai CLIP ViT-L/14 | 1 | 82.7 | 45.7 |[Link](https://huggingface.co/openflamingo/OpenFlamingo-3B-vitl-mpt1b-langinstruct)|\n|4B| togethercomputer/RedPajama-INCITE-Base-3B-v1 | openai CLIP ViT-L/14 | 2 | 81.8 | 49.0 | [Link](https://huggingface.co/openflamingo/OpenFlamingo-4B-vitl-rpj3b)|\n|4B| togethercomputer/RedPajama-INCITE-Instruct-3B-v1 | openai CLIP ViT-L/14 | 2 | 85.8 | 49.0 | [Link](https://huggingface.co/openflamingo/OpenFlamingo-4B-vitl-rpj3b-langinstruct)|\n|9B| anas-awadalla/mpt-7b | openai CLIP ViT-L/14 | 4 | 89.0 | 54.8 | [Link](https://huggingface.co/openflamingo/OpenFlamingo-9B-vitl-mpt7b)|\n\n*\\* Xattn interval refers to the `--cross_attn_every_n_layers` argument.*\n\nNote: as part of our v2 release, we have deprecated a previous LLaMA-based checkpoint. However, you can continue to use our older checkpoint using the new codebase.\n\n## Downloading pretrained weights\n\nTo instantiate an OpenFlamingo model with one of our released weights, initialize the model as above and use the following code.\n\n```python\n# grab model checkpoint from huggingface hub\nfrom huggingface_hub import hf_hub_download\nimport torch\n\ncheckpoint_path = hf_hub_download(\"openflamingo/OpenFlamingo-3B-vitl-mpt1b\", \"checkpoint.pt\")\nmodel.load_state_dict(torch.load(checkpoint_path), strict=False)\n```\n\n## Generating text\nBelow is an example of generating text conditioned on interleaved images/text. In particular, let's try few-shot image captioning.\n\n``` python\nfrom PIL import Image\nimport requests\nimport torch\n\n\"\"\"\nStep 1: Load images\n\"\"\"\ndemo_image_one = Image.open(\n    requests.get(\n        \"http://images.cocodataset.org/val2017/000000039769.jpg\", stream=True\n    ).raw\n)\n\ndemo_image_two = Image.open(\n    requests.get(\n        \"http://images.cocodataset.org/test-stuff2017/000000028137.jpg\",\n        stream=True\n    ).raw\n)\n\nquery_image = Image.open(\n    requests.get(\n        \"http://images.cocodataset.org/test-stuff2017/000000028352.jpg\", \n        stream=True\n    ).raw\n)\n\n\n\"\"\"\nStep 2: Preprocessing images\nDetails: For OpenFlamingo, we expect the image to be a torch tensor of shape \n batch_size x num_media x num_frames x channels x height x width. \n In this case batch_size = 1, num_media = 3, num_frames = 1,\n channels = 3, height = 224, width = 224.\n\"\"\"\nvision_x = [image_processor(demo_image_one).unsqueeze(0), image_processor(demo_image_two).unsqueeze(0), image_processor(query_image).unsqueeze(0)]\nvision_x = torch.cat(vision_x, dim=0)\nvision_x = vision_x.unsqueeze(1).unsqueeze(0)\n\n\"\"\"\nStep 3: Preprocessing text\nDetails: In the text we expect an <image> special token to indicate where an image is.\n We also expect an <|endofchunk|> special token to indicate the end of the text \n portion associated with an image.\n\"\"\"\ntokenizer.padding_side = \"left\" # For generation padding tokens should be on the left\nlang_x = tokenizer(\n    [\"<image>An image of two cats.<|endofchunk|><image>An image of a bathroom sink.<|endofchunk|><image>An image of\"],\n    return_tensors=\"pt\",\n)\n\n\n\"\"\"\nStep 4: Generate text\n\"\"\"\ngenerated_text = model.generate(\n    vision_x=vision_x,\n    lang_x=lang_x[\"input_ids\"],\n    attention_mask=lang_x[\"attention_mask\"],\n    max_new_tokens=20,\n    num_beams=3,\n)\n\nprint(\"Generated text: \", tokenizer.decode(generated_text[0]))\n```\n\n# Training\nWe provide training scripts in `open_flamingo/train`. We provide an example Slurm script in `open_flamingo/scripts/run_train.py`, as well as the following example command:\n```\ntorchrun --nnodes=1 --nproc_per_node=4 open_flamingo/train/train.py \\\n  --lm_path anas-awadalla/mpt-1b-redpajama-200b \\\n  --tokenizer_path anas-awadalla/mpt-1b-redpajama-200b \\\n  --cross_attn_every_n_layers 1 \\\n  --dataset_resampled \\\n  --batch_size_mmc4 32 \\\n  --batch_size_laion 64 \\\n  --train_num_samples_mmc4 125000\\\n  --train_num_samples_laion 250000 \\\n  --loss_multiplier_laion 0.2 \\\n  --workers=4 \\\n  --run_name OpenFlamingo-3B-vitl-mpt1b \\\n  --num_epochs 480 \\\n  --warmup_steps  1875 \\\n  --mmc4_textsim_threshold 0.24 \\\n  --laion_shards \"/path/to/shards/shard-{0000..0999}.tar\" \\\n  --mmc4_shards \"/path/to/shards/shard-{0000..0999}.tar\" \\\n  --report_to_wandb\n```\n\n*Note: The MPT-1B [base](https://huggingface.co/mosaicml/mpt-1b-redpajama-200b)  and [instruct](https://huggingface.co/mosaicml/mpt-1b-redpajama-200b-dolly) modeling code does not accept the `labels` kwarg or compute cross-entropy loss directly within `forward()`, as expected by our codebase. We suggest using a modified version of the MPT-1B models found [here](https://huggingface.co/anas-awadalla/mpt-1b-redpajama-200b) and [here](https://huggingface.co/anas-awadalla/mpt-1b-redpajama-200b-dolly).*\n\nFor more details, see our [training README](https://github.com/mlfoundations/open_flamingo/tree/main/open_flamingo/train).\n\n\n# Evaluation\nAn example evaluation script is at `open_flamingo/scripts/run_eval.sh`. Please see our [evaluation README](https://github.com/mlfoundations/open_flamingo/tree/main/open_flamingo/eval) for more details.\n\n\nTo run evaluations on OKVQA you will need to run the following command:\n```\nimport nltk\nnltk.download('wordnet')\n```\n\n\n# Future plans\n- [ ] Add support for video input\n\n# Team\n\nOpenFlamingo is developed by:\n\n[Anas Awadalla*](https://anas-awadalla.streamlit.app/), [Irena Gao*](https://i-gao.github.io/), [Joshua Gardner](https://homes.cs.washington.edu/~jpgard/), [Jack Hessel](https://jmhessel.com/), [Yusuf Hanafy](https://www.linkedin.com/in/yusufhanafy/), [Wanrong Zhu](https://wanrong-zhu.com/), [Kalyani Marathe](https://sites.google.com/uw.edu/kalyanimarathe/home?authuser=0), [Yonatan Bitton](https://yonatanbitton.github.io/), [Samir Gadre](https://sagadre.github.io/), [Shiori Sagawa](https://cs.stanford.edu/~ssagawa/), [Jenia Jitsev](https://scholar.google.de/citations?user=p1FuAMkAAAAJ&hl=en), [Simon Kornblith](https://simonster.com/), [Pang Wei Koh](https://koh.pw/), [Gabriel Ilharco](https://gabrielilharco.com/), [Mitchell Wortsman](https://mitchellnw.github.io/), [Ludwig Schmidt](https://people.csail.mit.edu/ludwigs/).\n\nThe team is primarily from the University of Washington, Stanford, AI2, UCSB, and Google.\n\n# Acknowledgments\nThis code is based on Lucidrains' [flamingo implementation](https://github.com/lucidrains/flamingo-pytorch) and David Hansmair's [flamingo-mini repo](https://github.com/dhansmair/flamingo-mini). Thank you for making your code public! We also thank the [OpenCLIP](https://github.com/mlfoundations/open_clip) team as we use their data loading code and take inspiration from their library design.\n\nWe would also like to thank [Jean-Baptiste Alayrac](https://www.jbalayrac.com) and [Antoine Miech](https://antoine77340.github.io) for their advice, [Rohan Taori](https://www.rohantaori.com/), [Nicholas Schiefer](https://nicholasschiefer.com/), [Deep Ganguli](https://hai.stanford.edu/people/deep-ganguli), [Thomas Liao](https://thomasliao.com/), [Tatsunori Hashimoto](https://thashim.github.io/), and [Nicholas Carlini](https://nicholas.carlini.com/) for their help with assessing the safety risks of our release, and to [Stability AI](https://stability.ai) for providing us with compute resources to train these models.\n\n# Citing\nIf you found this repository useful, please consider citing:\n\n```\n@article{awadalla2023openflamingo,\n  title={OpenFlamingo: An Open-Source Framework for Training Large Autoregressive Vision-Language Models},\n  author={Anas Awadalla and Irena Gao and Josh Gardner and Jack Hessel and Yusuf Hanafy and Wanrong Zhu and Kalyani Marathe and Yonatan Bitton and Samir Gadre and Shiori Sagawa and Jenia Jitsev and Simon Kornblith and Pang Wei Koh and Gabriel Ilharco and Mitchell Wortsman and Ludwig Schmidt},\n  journal={arXiv preprint arXiv:2308.01390},\n  year={2023}\n}\n```\n\n```\n@software{anas_awadalla_2023_7733589,\n  author = {Awadalla, Anas and Gao, Irena and Gardner, Joshua and Hessel, Jack and Hanafy, Yusuf and Zhu, Wanrong and Marathe, Kalyani and Bitton, Yonatan and Gadre, Samir and Jitsev, Jenia and Kornblith, Simon and Koh, Pang Wei and Ilharco, Gabriel and Wortsman, Mitchell and Schmidt, Ludwig},\n  title = {OpenFlamingo},\n  month        = mar,\n  year         = 2023,\n  publisher    = {Zenodo},\n  version      = {v0.1.1},\n  doi          = {10.5281/zenodo.7733589},\n  url          = {https://doi.org/10.5281/zenodo.7733589}\n}\n```\n\n```\n@article{Alayrac2022FlamingoAV,\n  title={Flamingo: a Visual Language Model for Few-Shot Learning},\n  author={Jean-Baptiste Alayrac and Jeff Donahue and Pauline Luc and Antoine Miech and Iain Barr and Yana Hasson and Karel Lenc and Arthur Mensch and Katie Millican and Malcolm Reynolds and Roman Ring and Eliza Rutherford and Serkan Cabi and Tengda Han and Zhitao Gong and Sina Samangooei and Marianne Monteiro and Jacob Menick and Sebastian Borgeaud and Andy Brock and Aida Nematzadeh and Sahand Sharifzadeh and Mikolaj Binkowski and Ricardo Barreira and Oriol Vinyals and Andrew Zisserman and Karen Simonyan},\n  journal={ArXiv},\n  year={2022},\n  volume={abs/2204.14198}\n}\n```\n"
        },
        {
          "name": "TERMS_AND_CONDITIONS.md",
          "type": "blob",
          "size": 1.9052734375,
          "content": "**Please read the following information carefully before proceeding.**\n\nOpenFlamingo is a **research prototype** that aims to enable users to interact with AI through both language and images. AI agents equipped with both language and visual understanding can be useful on a larger variety of tasks compared to models that communicate solely via language. By releasing an open-source research prototype, we hope to help the research community better understand the risks and limitations of modern visual-language AI models and accelerate the development of safer and more reliable methods.\n\n- [ ] I understand that OpenFlamingo is a research prototype and I will only use it for non-commercial research purposes.\n\n**Limitations.** OpenFlamingo is built on top of the LLaMA large language model developed by Meta AI. Large language models, including LLaMA, are trained on mostly unfiltered internet data, and have been shown to be able to produce toxic, unethical, inaccurate, and harmful content. On top of this, OpenFlamingoâ€™s ability to support visual inputs creates additional risks, since it can be used in a wider variety of applications; image+text models may carry additional risks specific to multimodality. Please use discretion when assessing the accuracy or appropriateness of the modelâ€™s outputs, and be mindful before sharing its results.\n\n- [ ] I understand that OpenFlamingo may produce unintended, inappropriate, offensive, and/or inaccurate results. I agree to take full responsibility for any use of the OpenFlamingo outputs that I generate.\n\n**Privacy and data collection.** This demo does NOT store any personal information on its users, and it does NOT store user queries.\n\n**Licensing.** As OpenFlamingo is built on top of the LLaMA large language model from Meta AI, the LLaMA license agreement (as documented in the Meta request form) also applies.\n\n- [ ] I have read and agree to the terms of the LLaMA license agreement.\n"
        },
        {
          "name": "_optim_utils.py",
          "type": "blob",
          "size": 69.115234375,
          "content": "import copy\nimport functools\nimport warnings\nfrom dataclasses import dataclass\nfrom typing import (\n    Any,\n    cast,\n    Dict,\n    Iterable,\n    Iterator,\n    List,\n    NamedTuple,\n    Optional,\n    Sequence,\n    Set,\n    Tuple,\n    Union,\n)\n\nimport torch\nimport torch.distributed as dist\nimport torch.distributed.fsdp._traversal_utils as traversal_utils\nimport torch.nn as nn\nfrom torch.distributed._shard.sharded_tensor import ShardedTensor\nfrom torch.distributed.fsdp._common_utils import (\n    _apply_to_modules,\n    _FSDPState,\n    _get_module_fsdp_state_if_fully_sharded_module,\n    _get_param_to_fqns,\n    _module_handles,\n    clean_tensor_name,\n)\nfrom torch.distributed.fsdp._fsdp_extensions import _ext_chunk_tensor\nfrom torch.distributed.fsdp._runtime_utils import _clear_grads_if_needed, _lazy_init\nfrom torch.distributed.fsdp._shard_utils import _gather_state_dict\nfrom torch.distributed.fsdp.api import ShardingStrategy\nfrom torch.distributed.fsdp.flat_param import FlatParameter, FlatParamHandle\n\n\n@dataclass\nclass FSDPParamInfo:\n    state: _FSDPState\n    flat_param: FlatParameter\n    param_indices: Dict[str, int]\n\n\ndef sorted_items(dictionary: Dict[str, Any]) -> Iterator[Tuple[str, Any]]:\n    keys = sorted(dictionary.keys())\n    for k in keys:\n        yield k, dictionary[k]\n\n\nclass _ConsolidatedOptimState:\n    \"\"\"\n    This holds the consolidated optimizer state on the target rank. Positive-\n    dimension tensor state is communicated across ranks, while zero-dimension\n    tensor state and non-tensor state is taken directly from the target rank.\n\n    PyTorch version 1.12 moved to using zero-dimension tensors for scalar\n    values, but user implemented optimizers may still use float (i.e. a\n    non-tensor). Thus, we support both and handle them identically.\n\n    Attributes:\n        tensor_state (Dict[str, torch.Tensor]): Mapping from positive-dimension\n            tensor state name to the unsharded flattened tensor representing\n            the state.\n        zero_dim_tensor_state (Dict[str, torch.Tensor]): Mapping from zero-\n            dimension tensor state name to its value.\n        non_tensor_state (Dict[str, Any]): Mapping from non-tensor state\n            name to its value.\n    \"\"\"\n\n    tensor_state: Dict[str, torch.Tensor] = {}\n    zero_dim_tensor_state: Dict[str, torch.Tensor] = {}\n    non_tensor_state: Dict[str, Any] = {}\n\n\nclass _PosDimTensorInfo(NamedTuple):\n    \"\"\"\n    Meatadata for positive-dimension tensors used internally for\n    :meth:`scatter_full_optim_state_dict`.\n\n    Attributes:\n        shape (torch.Size): Sharded tensor shape (which is equal to the\n            unsharded tensor shape if the tensor is optimizer state for a\n            non-FSDP parameter and is hence not sharded).\n        dtype (torch.dtype): Data type of the tensor.\n    \"\"\"\n\n    shape: torch.Size\n    dtype: torch.dtype\n\n\nclass _OptimStateKey(NamedTuple):\n    \"\"\"\n    This represents an optimizer state key that may be used commonly across\n    ranks. It is based on the unflattened parameter names rather than parameter\n    IDs to make it indepenendent of each rank's own optimizer construction.\n    \"\"\"\n\n    unflat_param_names: Tuple[str, ...]\n    is_fsdp_managed: bool\n\n\ndef _unflatten_optim_state(\n    fsdp_param_info: FSDPParamInfo,\n    flat_param_state: Dict[str, Any],\n    to_save: bool,\n    shard_state: bool,\n) -> List[Dict[str, Any]]:\n    \"\"\"\n    Unflattens the optimizer state, consisting of the \"state\" part and the\n    \"param_groups\" part. Unflattening the \"state\" part involves consolidating\n    the state on the target rank and remapping from flattened to unflattened\n    parameter IDs, and the \"param_groups\" part only involves remapping from\n    flattened to unflattened parameter IDs.\n\n    Args:\n        fsdp_param_info (FSDPParamInfo): The fsdp state and the target flatten\n            parameter.\n        flat_param_state (Dict[str, Any]): Entry for the flattened parameter\n            in the \"state\" part of the optimizer state dict.\n        to_save (bool): Whether to save the state on this rank.\n\n    Returns:\n        List[Dict[str, Any]]: A :class:`list` holding the entries in the\n        \"state\" part of the optimizer state dict corresponding to the\n        unflattened parameters comprising the flattened parameter if on the\n        target rank or an empty :class:`list` otherwise. The final optimizer\n        state dict will need to map these entries using the proper unflattened\n        parameter IDs.\n    \"\"\"\n    assert (\n        not shard_state or to_save\n    ), \"If ``shard_state`` is True, ``to_save`` has to be True.\"\n    consolidated_state = _communicate_optim_state(\n        fsdp_param_info,\n        flat_param_state,\n    )\n    if to_save:\n        unflat_param_state = _unflatten_communicated_optim_state(\n            fsdp_param_info,\n            consolidated_state,\n            shard_state,\n        )\n        for optim_state in unflat_param_state:\n            for key in list(optim_state.keys()):\n                state = optim_state[key]\n                if isinstance(state, torch.Tensor):\n                    optim_state[key] = state.cpu()\n        return unflat_param_state\n    else:\n        return []\n\n\ndef _is_zero_dim_tensor(x: Any) -> bool:\n    return torch.is_tensor(x) and x.dim() == 0\n\n\ndef _communicate_optim_state(\n    fsdp_param_info: FSDPParamInfo,\n    flat_param_state: Dict[str, Any],\n) -> _ConsolidatedOptimState:\n    \"\"\"\n    Communicates the optimizer state for a flattened parameter across ranks.\n    All ranks will hold the entire non-sharded optimizer state on GPU.\n\n    If ``N`` is the number of tensor optimizer states in the optimizer state\n    dict, then the communication complexity is 0 if ``N = 0`` and ``N + 1``\n    otherwise (where the plus 1 comes from all-gathering the padding per rank).\n\n    Args:\n        fsdp_param_info (FSDPParamInfo): The fsdp state and the target flatten\n            parameter.\n        flat_param_state (Dict[str, Any]): The entry in the \"state\" part of the\n            optimizer state dict corresponding to the flattened parameter.\n\n    Returns:\n        ConsolidatedOptimState: Consolidated optimizer state for the target\n            flattened parameter.\n    \"\"\"\n    fsdp_state = fsdp_param_info.state\n    flat_param = fsdp_param_info.flat_param\n    state = _ConsolidatedOptimState()\n    tensor_state, zero_dim_tensor_state, non_tensor_state = (\n        state.tensor_state,\n        state.zero_dim_tensor_state,\n        state.non_tensor_state,\n    )\n\n    for state_name, value in sorted_items(flat_param_state):\n        # Positive-dimension tensor state: communicate across ranks\n        if torch.is_tensor(value) and value.dim() > 0:\n            # If the parameter is not sharded, then neither is the\n            # positive-dimension tensor state, so no need to communicate it --\n            # we take the target rank's value\n            if (\n                fsdp_state.world_size == 1\n                or fsdp_state.sharding_strategy == ShardingStrategy.NO_SHARD\n            ):\n                tensor_state[state_name] = value\n                continue\n            if not value.is_cuda:\n                value = value.to(fsdp_state.compute_device)\n            # Assume that positive-dimension tensor optimizer state\n            # has the same shape as the sharded flattened parameter\n            buffer_size = flat_param._full_param_padded.size()  # type: ignore[attr-defined]\n            tensor_buffer = value.new_zeros(*buffer_size)\n            dist.all_gather_into_tensor(\n                tensor_buffer, value, group=fsdp_state.process_group\n            )\n            torch.cuda.synchronize()\n            unpadded_numel = cast(\n                nn.Parameter, flat_param._unpadded_unsharded_size\n            ).numel()\n            tensor_state[state_name] = tensor_buffer[:unpadded_numel]\n        # Zero-dimension tensor state and non-tensor state: take this rank's\n        # value directly\n        else:\n            if _is_zero_dim_tensor(value):\n                zero_dim_tensor_state[state_name] = value\n            else:\n                non_tensor_state[state_name] = value\n    return state\n\n\ndef _unflatten_communicated_optim_state(\n    fsdp_param_info: FSDPParamInfo,\n    state: _ConsolidatedOptimState,\n    shard_state: bool,\n) -> List[Dict[str, Any]]:\n    \"\"\"\n    Unflattens the communicated optimizer state (given by ``tensor_state``,\n    ``non_tensor_state``, and ``zero_dim_tensor_state``) for a single flattened\n    parameter. This should only be called on the target rank.\n\n    Args:\n        fsdp_param_info (FSDPParamInfo): The fsdp state and the target flatten\n            parameter.\n        state (_ConsolidatedOptimState): Consolidated optimizer state.\n\n    Returns:\n        List[Dict[str, Any]]: A :class:`list` holding the entries in the\n        \"state\" part of the optimizer state dict corresponding to the\n        unflattened parameters comprising the flattened parameter. The final\n        optimizer state dict will need to map these entries using the proper\n        unflattened parameter IDs.\n    \"\"\"\n    fsdp_state = fsdp_param_info.state\n    flat_param = fsdp_param_info.flat_param\n    unflat_param_state: List[Dict[str, Any]] = []\n    flat_param_views: Dict[str, Iterator] = {}\n    num_unflat_params = flat_param._num_params\n    tensor_state, zero_dim_tensor_state, non_tensor_state = (\n        state.tensor_state,\n        state.zero_dim_tensor_state,\n        state.non_tensor_state,\n    )\n\n    for _ in range(num_unflat_params):\n        unflat_state_param = {}\n        # Add positive-dimension tensor state: unflatten with views\n        for state_name, flat_tensor in sorted_items(tensor_state):\n            views_generated = state_name in flat_param_views\n            if not views_generated:\n                views = FlatParamHandle._get_unflat_views(flat_param, flat_tensor)\n                flat_param_views[state_name] = views\n            else:\n                views = flat_param_views[state_name]\n            optim_state: Union[torch.Tensor, ShardedTensor] = next(views)\n            if shard_state:\n                assert fsdp_state.process_group is not None\n                optim_state = _ext_chunk_tensor(\n                    optim_state,\n                    fsdp_state.rank,\n                    fsdp_state.world_size,\n                    torch.cuda.device_count(),\n                    fsdp_state.process_group,\n                )\n            unflat_state_param[state_name] = optim_state\n\n        # Add zero-dimension tensor state: take the target rank's value\n        for state_name, zero_dim_tensor in sorted_items(zero_dim_tensor_state):\n            unflat_state_param[state_name] = zero_dim_tensor\n        # Add non-tensor state: take the target rank's value\n        for state_name, non_tensor in sorted_items(non_tensor_state):\n            unflat_state_param[state_name] = non_tensor\n        unflat_param_state.append(unflat_state_param)\n    return unflat_param_state\n\n\ndef _flatten_optim_state_dict(\n    optim_state_dict: Dict[str, Any],\n    model: nn.Module,\n    shard_state: bool,\n    use_orig_params: bool = False,\n    optim: Optional[torch.optim.Optimizer] = None,\n) -> Dict[str, Any]:\n    \"\"\"\n    Flattens the full optimizer state dict, still keying by unflattened\n    parameter names. If ``shard_state=True``, then FSDP-managed\n    ``FlatParameter`` 's optimizer states are sharded, and otherwise, they are\n    kept unsharded.\n\n    If ``use_orig_params`` is True, each rank will have all FSDP-managed\n    parameters but some of these parameters may be empty due to the sharding.\n    For a regular optim.Optimizer, states for those empty parameters will\n    not be initialized. So, when aggregating the FQNs across ranks, no assert\n    will be raised on a rank even if it does not have all the states -- it is\n    valid and FSDP know how to aggregate them. However, FSDP has to ignore\n    handling those parameters that are not managed by FSDP and do not exist on\n    the local rank -- it is managed by other parallelism and FSDP does not\n    know ho to handle/aggregate them.\n\n    Note that ``_flatten_tensor_optim_state`` does not need ``optim`` to\n    flatten/shard the state. However, NamedOptimizer and KeyedOptimizer require\n    all the states even if the corresponding parameters are empty. To this end,\n    ``optim`` will be used to to get the initial state of the empty parameters.\n    ``optim`` should only be non-None if the ``optim` is KeyedOptimizer or\n    NamedOptimizer.\n\n    Returns:\n        Dict[str, Any]: The flattened optimizer state dict.\n    \"\"\"\n    unflat_osd = optim_state_dict\n    if \"state\" not in unflat_osd or \"param_groups\" not in unflat_osd:\n        raise ValueError(\n            '`optim_state_dict` must have the keys \"state\" and '\n            '\"param_groups\" to be a valid optimizer state dict'\n        )\n    param_to_fqns = _get_param_to_fqns(model)\n    fqn_to_fsdp_param_info = _get_fqn_to_fsdp_param_info(model)\n\n    # Construct the \"state\" part\n    flat_osd_state: Dict[Union[_OptimStateKey, str], Any] = {}\n    unflat_osd_state = unflat_osd[\"state\"]\n    all_state_keys = set(unflat_osd_state.keys())\n\n    # local_state_dict is used to construct states of empty parameters.\n    # This should only be used if is_named_optimizer=True.\n    local_state_dict: Dict[str, Any] = {}\n    local_state_clean_fqns: Dict[str, str] = {}\n    if optim is not None:\n        local_state_dict = optim.state_dict()[\"state\"]\n        for fqn in local_state_dict.keys():\n            clean_fqn = clean_tensor_name(fqn)\n            local_state_clean_fqns[clean_fqn] = fqn\n\n    for param, unflat_param_names in param_to_fqns.items():\n        fqn = unflat_param_names[0]\n        if fqn not in unflat_osd_state:\n            continue\n        all_state_keys.difference_update(unflat_param_names)\n        if fqn in fqn_to_fsdp_param_info:\n            fsdp_param_info = fqn_to_fsdp_param_info[fqn]\n            if use_orig_params:\n                assert (\n                    shard_state\n                ), \"If use_orig_params is True, shard_state must be True.\"\n                flat_state = _shard_orig_param_state(\n                    fsdp_param_info,\n                    fqn,\n                    unflat_osd_state[fqn],\n                )\n            else:\n                flat_state = _flatten_optim_state(\n                    fsdp_param_info,\n                    unflat_osd_state,\n                    unflat_param_names,\n                    shard_state,\n                )\n            key = _OptimStateKey(tuple(unflat_param_names), True)\n            # Only include non-empty states since as expected by\n            # `torch.optim.Optimizer` s unless the optimizer is KeyedOptimizer\n            # or NamedOptimizer.\n            if flat_state:\n                flat_osd_state[key] = flat_state\n            elif optim is not None:  # NamedOptimizer or KeyedOptimizer case.\n                assert len(unflat_param_names) == 1\n                local_wrapped_fqn = local_state_clean_fqns.get(fqn, \"\")\n                if local_wrapped_fqn:\n                    flat_osd_state[key] = copy.deepcopy(\n                        local_state_dict[local_wrapped_fqn]\n                    )\n        else:  # do not flatten non-FSDP parameters' states\n            assert len(unflat_param_names) == 1\n            key = _OptimStateKey(tuple(unflat_param_names), False)\n            flat_osd_state[key] = copy.copy(unflat_osd_state[fqn])\n\n    # Handle user-defined state, states that are not accosiated with parameters.\n    for key in all_state_keys:\n        flat_osd_state[key] = copy.copy(unflat_osd_state[key])\n\n    # Construct the \"param_groups\" part -- copy as is since it will be\n    # rekeyed later according to the target rank's optimizer\n    flat_osd_param_groups = copy.deepcopy(unflat_osd[\"param_groups\"])\n    return {\"state\": flat_osd_state, \"param_groups\": flat_osd_param_groups}\n\n\ndef _flatten_optim_state(\n    fsdp_param_info: FSDPParamInfo,\n    unflat_osd_state: Dict[str, Dict[str, Any]],\n    unflat_param_names: List[str],\n    shard_state: bool,\n) -> Dict[str, Any]:\n    \"\"\"\n    Flattens the optimizer state in ``full_optim_state_dict`` for a single\n    flattened parameter in ``fsdp_param_info`` corresponding to the unflattened\n    parameter names in ``unflat_param_names``.\n\n    Args:\n        unflat_osd_state (Dict[str, Dict[str, Any]]): The \"state\" part of the\n            optimizer state dict corresponding to the unflattened parameters.\n        unflat_param_names (List[str]): A :class:`list` of unflattened\n            parameter names corresponding to the flattened parameter\n            ``flat_param``.\n        fsdp_param_info (FSDPParamInfo): The fsdp state and the target flatten\n            parameter.\n        shard_state (bool): Whether to shard flattened positive-dimension\n            tensor state; if ``False``, then the full flattened tensor is\n            kept in the returned :class:`dict.\n\n    Returns:\n        Dict[str, Any]: A :class:`dict` mapping state names to their values for\n        a particular flattened parameter. The sharded optimizer state dict's\n        \"state\" part will map a key to this returned value.\n    \"\"\"\n    fsdp_state = fsdp_param_info.state\n    flat_param = fsdp_param_info.flat_param\n    num_unflat_params = len(unflat_param_names)\n    assert num_unflat_params > 0, (\n        \"Expects at least one unflattened parameter corresponding to the \"\n        \"flattened parameter\"\n    )\n    unflat_param_shapes = flat_param._shapes\n    num_unflat_param_shapes = len(unflat_param_shapes)\n    assert (\n        num_unflat_params == num_unflat_param_shapes\n    ), f\"Expects {num_unflat_params} shapes but got {num_unflat_param_shapes}\"\n\n    # Check if these unflattened parameters have any optimizer state\n    has_state = [\n        bool(unflat_param_name in unflat_osd_state)\n        for unflat_param_name in unflat_param_names\n    ]\n    # If none of the unflattened parameters comprising this flattened parameter\n    # have any state, then we do not want an entry in the optimizer state dict\n    if not any(has_state):\n        return {}  # no need to flatten any state\n    # There may still be some unflattened parameters with state and some\n    # without\n    unflat_param_states = [\n        _gather_state_dict(\n            unflat_osd_state[unflat_param_name], pg=fsdp_state.process_group\n        )\n        if unflat_param_name in unflat_osd_state\n        else None\n        for unflat_param_name in unflat_param_names\n    ]\n    # Check that the unflattened parameters have the same state names\n    state_names = None\n    for unflat_param_state in unflat_param_states:\n        if unflat_param_state is None:\n            continue\n        if state_names is None:\n            state_names = set(unflat_param_state.keys())\n        else:\n            if state_names != set(unflat_param_state.keys()):\n                raise ValueError(\n                    \"Differing optimizer state names for the unflattened \"\n                    f\"parameters: {unflat_param_names}\"\n                )\n    assert state_names is not None\n\n    # Flatten the state\n    flat_state: Dict[str, Any] = {}\n    for state_name in state_names:\n        state_values = [\n            unflat_param_state[state_name] if unflat_param_state is not None else None\n            for unflat_param_state in unflat_param_states\n        ]\n        non_none_state_values = [v for v in state_values if v is not None]\n        are_pos_dim_tensors = are_zero_dim_tensors = are_non_tensors = True\n        for v in non_none_state_values:\n            are_pos_dim_tensors &= torch.is_tensor(v) and v.dim() > 0\n            are_zero_dim_tensors &= _is_zero_dim_tensor(v)\n            are_non_tensors &= not torch.is_tensor(v)\n        types = {type(v) for v in non_none_state_values}\n        if len(types) != 1 or not (\n            are_pos_dim_tensors or are_zero_dim_tensors or are_non_tensors\n        ):\n            raise ValueError(\n                f\"Differing optimizer state types for state {state_name}, \"\n                f\"values {non_none_state_values}, and unflattened parameter \"\n                f\"names {unflat_param_names}\"\n            )\n        if are_pos_dim_tensors:\n            flat_tensor = _flatten_tensor_optim_state(\n                state_name,\n                state_values,\n                unflat_param_names,\n                unflat_param_shapes,\n                flat_param,\n            )\n            if shard_state:\n                # Shard the flattened tensor immediately to minimize max memory\n                # usage\n                sharded_flat_tensor, _ = FlatParamHandle._get_shard(\n                    flat_tensor,\n                    fsdp_state.rank,\n                    fsdp_state.world_size,\n                )\n                flat_state[state_name] = sharded_flat_tensor\n            else:\n                flat_state[state_name] = flat_tensor\n        elif are_zero_dim_tensors:\n            flat_state[state_name] = _flatten_zero_dim_tensor_optim_state(\n                state_name,\n                state_values,\n                unflat_param_names,\n            )\n        else:\n            assert are_non_tensors\n            flat_state[state_name] = _flatten_non_tensor_optim_state(\n                state_name,\n                state_values,\n                unflat_param_names,\n            )\n\n    return flat_state\n\n\ndef _flatten_tensor_optim_state(\n    state_name: str,\n    pos_dim_tensors: List[torch.Tensor],\n    unflat_param_names: List[str],\n    unflat_param_shapes: Sequence[torch.Size],\n    flat_param: FlatParameter,\n) -> torch.Tensor:\n    \"\"\"\n    Flattens the positive-dimension tensor optimizer state given by the values\n    ``tensors`` for the state ``state_name`` for a single flattened parameter\n    ``flat_param`` corresponding to the unflattened parameter names\n    ``unflat_param_names`` and unflatted parameter shapes\n    ``unflat_param_shapes``. This flattens each unflattened parameter's tensor\n    state into one tensor.\n\n    NOTE: We use zero tensors for any unflattened parameters without state\n    since some value is required to fill those entries. This assumes that the\n    zero tensor is mathematically equivalent to having no state, which is true\n    for Adam's \"exp_avg\" and \"exp_avg_sq\" but may not be true for all\n    optimizers.\n\n    Args:\n        state_name (str): Optimizer state name.\n        pos_dim_tensors (List[torch.Tensor]): Positive-dimension tensor\n            optimizer state values for the unflattened parameters corresponding\n            to the single flattened parameter.\n        unflat_param_names (List[str]): A :class:`list` of unflattened\n            parameter names corresponding to the single flattened parameter.\n        unflat_param_shapes (List[torch.Size]): Unflattened parameter shapes\n            corresponding to the single flattened parameter.\n        flat_param (FlatParameter): The flattened parameter.\n\n    Returns:\n        torch.Tensor: A flattened tensor containing the optimizer state\n        corresponding to ``state_name`` constructed by concatenating the\n        unflattened parameter tensor states in ``pos_dim_tensors`` (using zero\n        tensors for any unflattened parameters without the state).\n    \"\"\"\n    non_none_tensors = [t for t in pos_dim_tensors if t is not None]\n    # Check that all are tensors with the same dtype\n    dtypes = {t.dtype for t in non_none_tensors}\n    if len(dtypes) != 1:\n        raise ValueError(\n            \"All unflattened parameters comprising a single flattened \"\n            \"parameter must have positive-dimension tensor state with the \"\n            f\"same dtype but got dtypes {dtypes} for state {state_name} and \"\n            f\"unflattened parameter names {unflat_param_names}\"\n        )\n    dtype = next(iter(dtypes))\n    # Check that each tensor state matches its parameter's shape\n    for tensor, shape in zip(pos_dim_tensors, unflat_param_shapes):\n        if tensor is None and len(shape) == 0:\n            raise ValueError(\"Flattening a zero-dimension parameter is not supported\")\n        elif tensor is not None and tensor.shape != shape:\n            raise ValueError(\n                \"Tensor optimizer state does not have same shape as its \"\n                f\"parameter: {tensor.shape} {shape}\"\n            )\n    # Flatten the tensor states: we do not need to add any padding since the\n    # flattened optimizer state tensor sharded via `_get_shard()`, which pads\n    # the shard as needed (just like for the flattened parameter)\n    cpu_device = torch.device(\"cpu\")\n    tensors = [\n        torch.flatten(state_value.to(cpu_device))\n        if state_value is not None\n        else torch.flatten(\n            torch.zeros(\n                size=shape,\n                dtype=dtype,\n                device=cpu_device,\n            )\n        )\n        for state_value, shape in zip(pos_dim_tensors, unflat_param_shapes)\n    ]\n    flat_tensor = torch.cat(tensors)\n    flat_param_shape = flat_param._unpadded_unsharded_size  # type: ignore[attr-defined]\n    assert flat_tensor.shape == flat_param_shape, (\n        f\"tensor optim state: {flat_tensor.shape} \"\n        f\"flattened parameter: {flat_param_shape}\"\n    )\n    return flat_tensor\n\n\ndef _flatten_zero_dim_tensor_optim_state(\n    state_name: str,\n    zero_dim_tensors: List[torch.Tensor],\n    unflat_param_names: List[str],\n) -> torch.Tensor:\n    \"\"\"\n    Flattens the zero-dimension tensor optimizer state given by the values\n    ``zero_dim_tensors`` for the state ``state_name`` for a single flattened\n    parameter corresponding to the unflattened parameter names\n    ``unflat_param_names`` by enforcing that all tensors are the same and using\n    that common value.\n\n    NOTE: The requirement that the tensors are the same across all unflattened\n    parameters comprising the flattened parameter is needed to maintain the\n    invariant that FSDP performs the same computation as its non-sharded\n    equivalent. This means that none of the unflattened parameters can be\n    missing this state since imposing a value may differ from having no value.\n    For example, for Adam's \"step\", no value means maximum bias correction,\n    while having some positive value means less bias correction.\n\n    Args:\n        state_name (str): Optimizer state name.\n        zero_dim_tensors (List[torch.Tensor]): Zero-dimension optimizer state\n            for the unflattened parameters corresponding to the single\n            flattened parameter.\n        unflat_param_names (List[str]): A :class:`list` of unflattened\n            parameter names corresponding to the single flattened parameter.\n\n    Returns:\n        torch.Tensor: A zero-dimensional tensor giving the value of the state\n        ``state_name`` for all unflattened parameters corresponding to the\n        names ``unflat_param_names``.\n    \"\"\"\n    non_none_tensors = [t for t in zero_dim_tensors if t is not None]\n    # Enforce that all have the same value and dtype\n    values_set = {t.item() if t is not None else None for t in zero_dim_tensors}\n    dtypes = {t.dtype if t is not None else None for t in zero_dim_tensors}\n    if (\n        len(non_none_tensors) != len(zero_dim_tensors)\n        or len(values_set) != 1\n        or len(dtypes) != 1\n    ):\n        raise ValueError(\n            \"All unflattened parameters comprising a single flattened \"\n            \"parameter must have scalar state with the same value and dtype \"\n            f\"but got values {values_set} and dtypes {dtypes} for state \"\n            f\"{state_name} and unflattened parameter names \"\n            f\"{unflat_param_names}\"\n        )\n    value = next(iter(values_set))\n    dtype = next(iter(dtypes))\n    return torch.tensor(value, dtype=dtype, device=torch.device(\"cpu\"))\n\n\ndef _flatten_non_tensor_optim_state(\n    state_name: str,\n    non_tensors: List[Any],\n    unflat_param_names: List[str],\n) -> Any:\n    \"\"\"\n    Flattens the non-tensor optimizer state given by the values ``non_tensors``\n    for the state ``state_name`` for a single flattened parameter corresponding\n    to the unflattened parameter names ``unflat_param_names`` by enforcing that\n    all values are the same and using that common value.\n\n    See the note in :func:`_flatten_zero_dim_tensor_optim_state`.\n\n    Args:\n        state_name (str): Optimizer state name.\n        non_tensors (List[Any]): Non-tensor optimizer state for the unflattened\n            parameters corresponding to the single flattened parameter.\n        unflat_param_names (List[str]): A :class:`list` of unflattened\n            parameter names corresponding to the single flattened parameter.\n\n    Returns:\n        Any: A non-tensor giving the value of the state ``state_name`` for all\n        unflattened parameters corresponding to the names\n        ``unflat_param_names``.\n    \"\"\"\n    non_none_non_tensors = [nt for nt in non_tensors if nt is not None]\n    # Enforce that all have the same value (same type already checked)\n    non_tensor_set = set(non_tensors)\n    if len(non_none_non_tensors) != len(non_tensors) or len(non_tensor_set) != 1:\n        raise ValueError(\n            \"All unflattened parameters comprising a single flattened \"\n            \"parameter must have scalar state with the same value and dtype \"\n            f\"but got values {non_tensor_set} for state {state_name} and  \"\n            f\"unflattened parameter names {unflat_param_names}\"\n        )\n    non_tensor = next(iter(non_tensor_set))\n    return non_tensor\n\n\ndef _process_pos_dim_tensor_state(\n    flat_optim_state_dict: Dict[str, Any],\n    world_size: int,\n) -> Dict[str, Any]:\n    \"\"\"\n    Processes positive-dimension tensor states in ``flat_optim_state_dict`` by\n    replacing them with metadata. This is done so the processed optimizer state\n    dict can be broadcast from rank 0 to all ranks without copying those tensor\n    states, and thus, this is meant to only be called on rank 0.\n\n    Args:\n        flat_optim_state_dict (Dict[str, Any]): Flattened optimizer state dict\n            with the positive-dimension tensor states unsharded.\n\n    Returns:\n        Dict[str, Any]: The flattened optimizer state dict with positive-\n        dimension tensor states replaced by metadata.\n    \"\"\"\n    flat_osd = flat_optim_state_dict  # alias\n    no_tensor_osd: Dict[str, Any] = {\"state\": {}}\n    for key, param_state in flat_osd[\"state\"].items():\n        no_tensor_osd[\"state\"][key] = {}\n        for state_name, value in sorted_items(param_state):\n            is_pos_dim_tensor_state = torch.is_tensor(value) and value.dim() > 0\n            if not is_pos_dim_tensor_state:\n                no_tensor_osd[\"state\"][key][state_name] = value\n                continue\n            if key.is_fsdp_managed:  # FSDP parameter\n                sharded_size = FlatParamHandle._get_sharded_size(\n                    value, rank=0, world_size=world_size\n                )\n                assert len(sharded_size) == 1, f\"{sharded_size}\"\n                info = _PosDimTensorInfo(sharded_size, value.dtype)\n            else:  # non-FSDP parameter\n                info = _PosDimTensorInfo(value.shape, value.dtype)\n            no_tensor_osd[\"state\"][key][state_name] = info\n    no_tensor_osd[\"param_groups\"] = flat_osd[\"param_groups\"]\n    return no_tensor_osd\n\n\ndef _broadcast_processed_optim_state_dict(\n    processed_optim_state_dict: Optional[Dict[str, Any]],\n    rank: int,\n    group,\n) -> Dict[str, Any]:\n    \"\"\"\n    Broadcasts the processed optimizer state dict from rank 0 to all ranks.\n\n    Args:\n        processed_optim_state_dict (Optional[Dict[str, Any]]): The flattened\n            optimizer state dict with positive-dimension tensor states replaced\n            with metadata if on rank 0; ignored otherwise.\n\n    Returns:\n        Dict[str, Any]: The processed optimizer state dict.\n    \"\"\"\n    # Broadcast the two data structures rank 0 to all ranks\n    obj_list = [processed_optim_state_dict] if rank == 0 else [None]\n    dist.broadcast_object_list(obj_list, src=0, group=group)\n    processed_optim_state_dict = obj_list[0]  # type: ignore[assignment]\n    assert processed_optim_state_dict is not None\n    # Keep zero-dimension tensors on CPU\n    return processed_optim_state_dict\n\n\ndef _broadcast_pos_dim_tensor_states(\n    processed_optim_state_dict: Dict[str, Any],\n    flat_optim_state_dict: Optional[Dict[str, Any]],\n    rank: int,\n    world_size: int,\n    group,\n    broadcast_device: torch.device,\n) -> Dict[str, Any]:\n    \"\"\"\n    Takes ``processed_optim_state_dict``, which has metadata in place of\n    positive-dimension tensor states, and broadcasts those tensor states from\n    rank 0 to all ranks. For tensor states corresponding to FSDP parameters,\n    rank 0 shards the tensor and broadcasts shard-by-shard, and for tensor\n    states corresponding to non-FSDP parameters, rank 0 broadcasts the full\n    tensor.\n\n    Args:\n        processed_optim_state_dict (Dict[str, Any]): The flattened optimizer\n            state dict with positive-dimension tensor states replaced with\n            metadata; this should be returned by\n            :meth:`_process_pos_dim_tensor_state` and non-empty on all ranks.\n        flat_optim_state_dict (Optional[Dict[str, Any]]): The flattened\n            unsharded optimizer state dict with the actual positive-dimension\n            tensor states if on rank 0; ignored on nonzero ranks.\n\n    Returns:\n        Dict[str, Any]: The optimizer state dict with the positive-dimension\n        tensor state correctly populated via ``broadcast()`` s from rank 0.\n    \"\"\"\n    assert (\n        rank != 0 or flat_optim_state_dict is not None\n    ), \"Expects rank 0 to pass in the flattened optimizer state dict\"\n    no_tensor_osd = processed_optim_state_dict  # alias\n    flat_osd = flat_optim_state_dict  # alias\n    for key, param_state in no_tensor_osd[\"state\"].items():\n        for state_name, value in sorted_items(param_state):\n            is_pos_dim_tensor_state = isinstance(value, _PosDimTensorInfo)\n            if not is_pos_dim_tensor_state:\n                continue\n            if rank == 0:\n                assert flat_osd is not None\n                unsharded_tensor = flat_osd[\"state\"][key][state_name]\n            else:\n                unsharded_tensor = None\n            shape, dtype = value.shape, value.dtype\n            if key.is_fsdp_managed:  # FSDP parameter\n                _broadcast_sharded_pos_dim_tensor_state(\n                    unsharded_tensor,\n                    param_state,\n                    state_name,\n                    shape,\n                    dtype,\n                    broadcast_device,\n                    rank,\n                    world_size,\n                    group,\n                )  # modify `param_state` destructively\n            else:  # non-FSDP parameter\n                _broadcast_unsharded_pos_dim_tensor_state(\n                    unsharded_tensor,\n                    param_state,\n                    state_name,\n                    shape,\n                    dtype,\n                    broadcast_device,\n                    rank,\n                    group,\n                )  # modify `param_state` destructively\n    return no_tensor_osd\n\n\ndef _broadcast_sharded_pos_dim_tensor_state(\n    unsharded_tensor: Optional[torch.Tensor],\n    param_state: Dict[str, Any],\n    state_name: str,\n    shape: torch.Size,\n    dtype: torch.dtype,\n    broadcast_device: torch.device,\n    rank: int,\n    world_size: int,\n    group,\n) -> None:\n    \"\"\"\n    Broadcasts positive-dimension tensor state for the state ``state_name``\n    corresponding to an FSDP parameter shard-by-shard, only to be saved on the\n    relevant rank. This modifies ``param_state`` destructively.\n\n    Args:\n        unsharded_tensor (Optional[torch.Tensor]): Unsharded tensor from which\n            to broadcast shards if on rank 0; ignored otherwise.\n        shape (torch.Size): Shape of the sharded tensor; same on all ranks.\n    \"\"\"\n    get_shard: Optional[functools.partial[Tuple[torch.Tensor, int]]] = None\n    if rank == 0:\n        assert (\n            unsharded_tensor is not None\n        ), \"Expects rank 0 to pass in the unsharded tensor\"\n        get_shard = functools.partial(\n            FlatParamHandle._get_shard,\n            unsharded_tensor,\n        )\n    for target_rank in range(1, world_size):\n        if rank == 0:\n            assert get_shard is not None\n            sharded_tensor = get_shard(target_rank, world_size)[0].to(broadcast_device)\n        else:\n            sharded_tensor = torch.zeros(\n                shape,\n                requires_grad=False,\n                dtype=dtype,\n                device=broadcast_device,\n            )\n        dist.broadcast(sharded_tensor, src=0, group=group)\n        # Only keep the shard on the target rank and keep it on the broadcast\n        # device, which is typically GPU\n        if rank == target_rank:\n            param_state[state_name] = sharded_tensor\n        else:\n            del sharded_tensor\n    # Lastly, shard on rank 0\n    if rank != 0:\n        return\n    param_state[state_name] = get_shard(0, world_size)[0].to(broadcast_device)  # type: ignore[misc]\n\n\ndef _broadcast_unsharded_pos_dim_tensor_state(\n    unsharded_tensor: Optional[torch.Tensor],\n    param_state: Dict[str, Any],\n    state_name: str,\n    shape: torch.Size,\n    dtype: torch.dtype,\n    broadcast_device: torch.device,\n    rank: int,\n    group,\n) -> None:\n    \"\"\"\n    Broadcasts positive-dimension tensor state for the state ``state_name``\n    corresponding to an unsharded non-FSDP parameter from rank 0 to all ranks.\n    This modifies ``param_state`` destructively.\n\n    Args:\n        unsharded_tensor (Optional[torch.Tensor]): Unsharded tensor to\n            broadcast if on rank 0; ignored otherwise.\n    \"\"\"\n    if rank == 0:\n        assert (\n            unsharded_tensor is not None\n        ), \"Expects rank 0 to pass in the unsharded tensor\"\n        assert (\n            shape == unsharded_tensor.shape\n        ), f\"Shape mismatch: {shape} {unsharded_tensor.shape}\"\n        assert (\n            dtype == unsharded_tensor.dtype\n        ), f\"dtype mismatch: {dtype} {unsharded_tensor.dtype}\"\n        unsharded_tensor = unsharded_tensor.to(broadcast_device)\n    else:\n        unsharded_tensor = torch.zeros(\n            shape,\n            requires_grad=False,\n            dtype=dtype,\n            device=broadcast_device,\n        )\n    dist.broadcast(unsharded_tensor, src=0, group=group)\n    # Keep the tensor on the broadcast device, which is typically GPU\n    param_state[state_name] = unsharded_tensor\n\n\ndef _rekey_sharded_optim_state_dict(\n    sharded_osd: Dict[str, Any],\n    model: nn.Module,\n    optim: torch.optim.Optimizer,\n    optim_input: Optional[\n        Union[\n            List[Dict[str, Any]],\n            Iterable[nn.Parameter],\n        ]\n    ],\n    using_optim_input: bool,\n    is_named_optimizer: bool = False,\n) -> Dict[str, Any]:\n    \"\"\"\n    Rekeys the optimizer state dict from unflattened parameter names to\n    flattened parameter IDs according to the calling rank's ``optim``, which\n    may be different across ranks. In particular, the unflattened parameter\n    names are represented as :class:`_OptimStateKey` s.\n    \"\"\"\n    param_to_fqns = _get_param_to_fqns(model)\n    flat_param_to_fqn = _get_flat_param_to_fqn(model)\n    param_to_param_key: Dict[nn.Parameter, Union[int, str]] = cast(\n        Dict[nn.Parameter, Union[int, str]],\n        (\n            _get_param_to_param_id_from_optim_input(model, optim_input)\n            if using_optim_input\n            else _get_param_to_param_key(\n                optim, model, is_named_optimizer, param_to_fqns, flat_param_to_fqn\n            )\n        ),\n    )\n    # All parameter keys in `param_to_param_key` should be in\n    # `param_to_fqns` -- strict inequality follows when not all parameters are\n    # passed to the optimizer\n    assert len(param_to_param_key) <= len(param_to_fqns)\n\n    unflat_param_names_to_flat_param_key: Dict[\n        Tuple[str, ...], Union[int, str]\n    ] = {}  # for \"state\"\n    unflat_param_name_to_flat_param_key: Dict[\n        str, Union[int, str]\n    ] = {}  # for \"param_groups\"\n    for param, unflat_param_names in param_to_fqns.items():\n        if param not in param_to_param_key:\n            # This parameter was not passed to the optimizer\n            continue\n        flat_param_key = param_to_param_key[param]\n        unflat_param_names_to_flat_param_key[tuple(unflat_param_names)] = flat_param_key\n        for unflat_param_name in unflat_param_names:\n            unflat_param_name_to_flat_param_key[unflat_param_name] = flat_param_key\n\n    sharded_osd_state = sharded_osd[\"state\"]\n    rekeyed_osd_state: Dict[Union[str, int], Any] = {}\n    for key, param_state in sharded_osd_state.items():\n        if isinstance(key, str):\n            rekeyed_osd_state[key] = param_state\n            continue\n        flat_param_key = unflat_param_names_to_flat_param_key.get(\n            key.unflat_param_names, key.unflat_param_names\n        )\n        rekeyed_osd_state[flat_param_key] = param_state\n\n    rekeyed_osd_param_groups: List[Dict[str, Any]] = []\n    for unflat_param_group in sharded_osd[\"param_groups\"]:\n        flat_param_group = copy.deepcopy(unflat_param_group)\n        flat_param_keys = sorted(\n            {\n                unflat_param_name_to_flat_param_key[unflat_param_name]\n                for unflat_param_name in unflat_param_group[\"params\"]\n            }\n        )\n        flat_param_group[\"params\"] = flat_param_keys\n        rekeyed_osd_param_groups.append(flat_param_group)\n\n    return {\"state\": rekeyed_osd_state, \"param_groups\": rekeyed_osd_param_groups}\n\n\ndef _get_param_id_to_param_from_optim_input(\n    model: nn.Module,\n    optim_input: Optional[\n        Union[\n            List[Dict[str, Any]],\n            Iterable[nn.Parameter],\n        ]\n    ] = None,\n) -> Dict[int, nn.Parameter]:\n    \"\"\"\n    Constructs a mapping from parameter IDs to parameters. This may be used\n    both for models with ``FlatParameter`` s and without.\n\n    NOTE: This method is only preserved for backward compatibility. The method\n    :meth:`_get_param_key_to_param` is the preferred code path that does not\n    rely on ``optim_input``.\n\n    NOTE: We critically assume that, whether the optimizer input is a list of\n    parameters or a list of parameter groups, :class:`torch.optim.Optimizer`\n    enumerates the parameter IDs in order. In other words, for a parameter list\n    input, the parameter IDs should be in that list order, and for a parameter\n    groups input, the parameter IDs should be in order within each parameter\n    group and in order across parameter groups.\n\n    Args:\n        model (nn.Module): Model whose parameters are passed into the\n            optimizer.\n        optim_input (Optional[Union[List[Dict[str, Any]],\n        Iterable[nn.Parameter]]]): Input passed into the optimizer\n            representing either a :class:`list` of parameter groups or an\n            iterable of parameters; if ``None``, then this method assumes the\n            input was ``model.parameters()``. (Default: ``None``)\n\n    Returns:\n        List[nn.Parameter]: Mapping from parameter IDs to parameters,\n        where the parameter ID is implicitly the index in the :class:`list`.\n    \"\"\"\n    # Assume the standard case of passing `model.parameters()` to the optimizer\n    # if `optim_input` is not specified\n    if optim_input is None:\n        return {pid: param for pid, param in enumerate(model.parameters())}\n    try:\n        params = cast(List[nn.Parameter], list(optim_input))\n    except TypeError as e:\n        raise TypeError(\n            \"Optimizer input should be an iterable of Tensors or dicts, \"\n            f\"but got {optim_input}\"\n        ) from e\n    if len(params) == 0:\n        raise ValueError(\"Optimizer input should not be empty\")\n\n    # Check if the optimizer input represents tensors or parameter groups\n    all_tensors = True\n    all_dicts = True\n    for param in params:\n        all_tensors &= isinstance(param, torch.Tensor)\n        all_dicts &= isinstance(param, dict)\n    if not all_tensors and not all_dicts:\n        raise TypeError(\"Optimizer input should be an iterable of Tensors or dicts\")\n    if all_tensors:\n        return {pid: param for pid, param in enumerate(params)}\n    assert all_dicts\n    param_id_to_param: List[nn.Parameter] = []\n    for param_group in params:\n        has_params_key = \"params\" in param_group  # type: ignore[operator]\n        assert has_params_key, (\n            'A parameter group should map \"params\" to a list of the '\n            \"parameters in the group\"\n        )\n        for param in param_group[\"params\"]:  # type: ignore[index]\n            # Implicitly map `flat_param_id` (current length of the list) to\n            # `param`\n            param_id_to_param.append(param)\n    return {pid: param for pid, param in enumerate(param_id_to_param)}\n\n\ndef _get_flat_param_to_fqn(model: torch.nn.Module) -> Dict[nn.Parameter, str]:\n    def module_fn(module, prefix, flat_param_to_fqn):\n        for param_name, param in module.named_parameters(recurse=False):\n            if type(param) is not FlatParameter:\n                continue\n            fqn = clean_tensor_name(prefix + param_name)\n            flat_param_to_fqn[param] = fqn\n\n    def return_fn(flat_param_to_fqn):\n        return flat_param_to_fqn\n\n    flat_param_to_fqn_ret: Dict[torch.nn.Parameter, str] = {}\n    return _apply_to_modules(\n        model,\n        module_fn,\n        return_fn,\n        [fqn for fqn, _ in model.named_parameters()],\n        flat_param_to_fqn_ret,\n    )\n\n\ndef _get_param_key_to_param(\n    optim: torch.optim.Optimizer,\n    model: Optional[nn.Module] = None,\n    is_named_optimizer: bool = False,\n    param_to_fqns: Optional[Dict[nn.Parameter, List[str]]] = None,\n    flat_param_to_fqn: Optional[Dict[nn.Parameter, str]] = None,\n) -> Dict[Union[int, str], nn.Parameter]:\n    \"\"\"\n    Constructs a mapping from parameter keys to parameters. For the regular\n    optimizers, the keys are parameter IDs. For NamedOptimizer, the keys\n    are FQNs. This API may be used both for models with ``FlatParameter`` s and\n    without.\n    \"\"\"\n    clean_fqn_to_curr_fqn: Dict[str, str] = {}\n    if is_named_optimizer:\n        assert (\n            param_to_fqns is not None and flat_param_to_fqn is not None\n        ), \"The optimizer is a NamedOptimizer, `param_to_fqns` must not be None.\"\n        assert model is not None\n        for key, _ in model.named_parameters():\n            clean_fqn_to_curr_fqn[clean_tensor_name(key)] = key\n\n    param_key_to_param: Dict[Union[str, int], nn.Parameter] = {}\n    pid = 0\n    for param_group in optim.param_groups:\n        if is_named_optimizer:\n            for param in param_group[\"params\"]:\n                assert flat_param_to_fqn is not None\n                if param in flat_param_to_fqn:\n                    # FlatParameter case\n                    key = flat_param_to_fqn[param]\n                else:\n                    assert param_to_fqns is not None\n                    # use_orig_params case\n                    assert len(param_to_fqns[param]) == 1\n                    key = param_to_fqns[param][0]\n                key = clean_fqn_to_curr_fqn[key]\n                param_key_to_param[key] = param\n        else:\n            for param in param_group[\"params\"]:\n                param_key_to_param[pid] = param\n                pid += 1\n\n    return param_key_to_param\n\n\ndef _get_param_to_param_key(\n    optim: torch.optim.Optimizer,\n    model: Optional[nn.Module] = None,\n    is_named_optimizer: bool = False,\n    param_to_fqns: Optional[Dict[nn.Parameter, List[str]]] = None,\n    flat_param_to_fqn: Optional[Dict[nn.Parameter, str]] = None,\n) -> Dict[nn.Parameter, Union[int, str]]:\n    \"\"\"\n    Constructs the inverse mapping of :func:`_get_param_key_to_param`. This API\n    only supports the case where `optim` is a regular optimizer, not NamedOptimizer.\n    So the parameter keys will be parameter id.\n    \"\"\"\n    param_id_to_param = _get_param_key_to_param(\n        optim, model, is_named_optimizer, param_to_fqns, flat_param_to_fqn\n    )\n    return {param: param_id for param_id, param in param_id_to_param.items()}\n\n\ndef _get_param_to_param_id_from_optim_input(\n    model: nn.Module,\n    optim_input: Optional[\n        Union[\n            List[Dict[str, Any]],\n            Iterable[nn.Parameter],\n        ]\n    ] = None,\n) -> Dict[nn.Parameter, int]:\n    \"\"\"Constructs the inverse mapping of :func:`_get_param_id_to_param_from_optim_input`.\"\"\"\n    param_id_to_param = _get_param_id_to_param_from_optim_input(model, optim_input)\n    return {param: param_id for param_id, param in param_id_to_param.items()}\n\n\ndef _check_missing_keys_on_rank(\n    r0_optim_state_keys: List[_OptimStateKey],\n    optim_state_key_to_param_key: Dict[_OptimStateKey, Union[str, int]],\n    param_key_to_param: Dict[Union[str, int], nn.Parameter],\n    group: Optional[dist.ProcessGroup],\n) -> None:\n    # Ensure that all ranks have at least the optimizer states needed by\n    # rank 0's optimizer\n    missing_keys: List[_OptimStateKey] = []\n    for r0_optim_state_key in r0_optim_state_keys:\n        if r0_optim_state_key not in optim_state_key_to_param_key:\n            # A parameter from rank 0's optimizer does not exist for this\n            # rank's optimizer\n            missing_keys.append(r0_optim_state_key)\n            continue\n        param_key = optim_state_key_to_param_key[r0_optim_state_key]\n        if isinstance(param_key, int):\n            assert param_key >= 0 and param_key < len(\n                param_key_to_param\n            ), \"Check the `param_key_to_param` construction\"\n    device = torch.device(\"cuda\", torch.cuda.current_device())\n    num_missing = torch.tensor([len(missing_keys)], dtype=torch.int32, device=device)\n    dist.all_reduce(num_missing, group=group)\n    if num_missing.item() > 0:\n        obj_list = [None for _ in range(dist.get_world_size(group))]\n        dist.all_gather_object(obj_list, missing_keys, group=group)\n        error_msg = (\n            \"FSDP currently requires each rank to have at least the \"\n            \"optimizer states needed by rank 0's optimizer but some ranks \"\n            \"are missing some of those states\"\n        )\n        for rank, keys in enumerate(obj_list):\n            keys = cast(List[_OptimStateKey], keys)\n            if len(keys) > 0:\n                error_msg += (\n                    f\"\\nRank {rank} is missing states for the parameters: \"\n                    f\"{[key.unflat_param_names for key in keys]}\"\n                )\n        raise RuntimeError(error_msg)\n\n\ndef _map_param_key_to_optim_keys(\n    optim_state_dict: Dict[str, Any],\n    group: Optional[dist.ProcessGroup],\n    param_key_to_param: Dict[Union[int, str], nn.Parameter],\n    param_to_fqns: Dict[nn.Parameter, List[str]],\n    fqn_to_fsdp_param_info: Dict[str, FSDPParamInfo],\n    merge_keys: bool = False,\n) -> Tuple[List[_OptimStateKey], Dict[_OptimStateKey, Union[int, str]]]:\n    \"\"\"\n    Construct the local mapping between the ``_OptimStateKey`` and parameter keys\n    and all the ``_OptimStateKey`` across ranks. If ``merge_keys`` is False, rank0\n    must contain all the ``_OptimStateKey``, an exception will be raised otherwise.\n    Note that ``merge_keys`` should equal to ``use_orig_params``.\n    \"\"\"\n    rank = dist.get_rank(group)\n    optim_state_key_to_param_key: Dict[_OptimStateKey, Union[int, str]] = {}  # local\n    all_optim_state_keys: List[_OptimStateKey] = []\n\n    for param_key, param in param_key_to_param.items():\n        # Do not include parameters without state to avoid empty mappings\n        # just like in normal `torch.optim.Optimizer.state_dict()`\n        if param_key not in optim_state_dict[\"state\"]:\n            continue\n        fqns = param_to_fqns[param]\n        is_fsdp_managed = isinstance(param, FlatParameter)\n        if is_fsdp_managed:\n            assert fqns[0] in fqn_to_fsdp_param_info, (\n                fqns[0],\n                list(fqn_to_fsdp_param_info.keys()),\n            )\n        is_fsdp_managed = fqns[0] in fqn_to_fsdp_param_info\n        optim_state_key = _OptimStateKey(\n            unflat_param_names=tuple(fqns),\n            is_fsdp_managed=is_fsdp_managed,\n        )\n        if rank == 0 or merge_keys:\n            all_optim_state_keys.append(optim_state_key)\n        optim_state_key_to_param_key[optim_state_key] = param_key\n\n    if merge_keys:\n        all_keys: List[List[_OptimStateKey]] = [\n            [] for _ in range(dist.get_world_size(group))\n        ]\n        dist.all_gather_object(all_keys, all_optim_state_keys, group=group)\n        merge_all_optim_state_keys = [\n            key for local_keys in all_keys for key in local_keys\n        ]\n        all_optim_state_keys = sorted(set(merge_all_optim_state_keys))\n    else:\n        key_obj_list: List[Optional[List[_OptimStateKey]]] = (\n            [all_optim_state_keys] if rank == 0 else [None]\n        )\n        dist.broadcast_object_list(key_obj_list, src=0, group=group)\n        assert key_obj_list[0] is not None\n        all_optim_state_keys = key_obj_list[0]\n        _check_missing_keys_on_rank(\n            all_optim_state_keys,\n            optim_state_key_to_param_key,\n            param_key_to_param,\n            group,\n        )\n\n    return all_optim_state_keys, optim_state_key_to_param_key\n\n\ndef _unflatten_param_groups(\n    state_dict: Dict[str, Any],\n    param_key_to_param: Dict[Union[int, str], nn.Parameter],\n    param_to_fqns: Dict[nn.Parameter, List[str]],\n) -> List[Dict[str, Any]]:\n    param_groups: List[Dict[str, Any]] = []\n    for flat_param_group in state_dict[\"param_groups\"]:\n        unflat_param_group = copy.deepcopy(flat_param_group)\n        param_group_params = [\n            param_key_to_param[flat_param_key]\n            for flat_param_key in flat_param_group[\"params\"]\n        ]\n        nested_unflat_param_names = [\n            param_to_fqns[param] for param in param_group_params\n        ]\n        unflat_param_group[\"params\"] = [\n            unflat_param_name\n            for unflat_param_names in nested_unflat_param_names\n            for unflat_param_name in unflat_param_names\n        ]  # flatten the list of lists\n        param_groups.append(unflat_param_group)\n    return param_groups\n\n\ndef _is_named_optimizer(optim_state_dict: Dict[str, Any]) -> bool:\n    state = optim_state_dict.get(\"state\", None)\n    if not state:\n        # If we cannot find a state, assume it is not NamedOptimizer as\n        # NamedOptimizer has eagerly initialization.\n        return False\n    try:\n        key = next(iter(state.keys()))\n    except Exception as e:\n        raise Exception(optim_state_dict) from e\n    return isinstance(key, str)\n\n\ndef _optim_state_dict(\n    model: nn.Module,\n    optim: torch.optim.Optimizer,\n    optim_state_dict: Dict[str, Any],\n    optim_input: Optional[\n        Union[\n            List[Dict[str, Any]],\n            Iterable[nn.Parameter],\n        ]\n    ],\n    rank0_only: bool,\n    shard_state: bool,\n    group: Optional[dist.ProcessGroup],\n    using_optim_input: bool,\n    use_orig_params: bool = False,\n) -> Dict[str, Any]:\n    \"\"\"\n    Consolidates the optimizer state and returns it as a :class:`dict`\n    following the convention of :meth:`torch.optim.Optimizer.state_dict`,\n    i.e. with keys ``\"state\"`` and ``\"param_groups\"``.\n    The flattened parameters in ``FSDP`` modules contained in ``model``\n    are mapped back to their unflattened parameters.\n\n    Parameter keys are not well-defined. For a regular optimizer, the optimizer\n    state_dict contains a mapping from parameter IDs to parameter states.\n    Parameter IDs are the order of parameters in ``optim.param_groups()`` across\n    all the groups. This API also allows user to pass ``optim_input`` for the\n    mapping between parameters and parameter IDs. Using ``optim_input`` is being\n    deprecated.\n\n    If the optimizer is a ``NamedOptimizer``, the optimizer state_dict does not\n    contain parameter IDs mapping but a mapping from parameter FQNs to parameter\n    states. This API finds the mapping from FQNs to parameters if the optimizer\n    is a ``NamedOptimizer``.\n\n    If ``use_orig_params`` is True, each rank will have all FSDP-managed\n    parameters but some of these parameters may be empty due to the sharding.\n    For a regular optim.Optimizer, states for those empty parameters will\n    not be initialized. So, when aggregating the FQNs across ranks, no assert\n    will be raised on a rank even if it does not have all the states -- it is\n    valid and FSDP know how to aggregate them. However, FSDP has to ignore\n    handling those parameters that are not managed by FSDP and do not exist on\n    the local rank -- it is managed by other parallelism and FSDP does not\n    know ho to handle/aggregate them.\n\n    Args:\n        model (nn.Module): Root module (which may or may not be a\n            :class:`FullyShardedDataParallel` instance) whose parameters\n            were passed into the optimizer ``optim``.\n        optim (torch.optim.Optimizer): Optimizer for ``model`` 's\n            parameters.\n        rank0_only (bool): If ``True``, saves the populated :class:`dict`\n            only on rank 0; if ``False``, saves it on all ranks. (Default:\n            ``True``)\n        shard_state (bool): If ``True``, shard and distribute all\n            non-zero-dimension states.\n\n    Returns:\n        Dict[str, Any]: A :class:`dict` containing the optimizer state for\n        ``model`` 's original unflattened parameters and including keys\n        \"state\" and \"param_groups\" following the convention of\n        :meth:`torch.optim.Optimizer.state_dict`. If ``rank0_only=False``,\n        then nonzero ranks return an empty :class:`dict`.\n    \"\"\"\n    _clear_grads_if_needed(traversal_utils._get_fsdp_handles(model))\n    to_save = not rank0_only or (dist.get_rank(group) == 0 or shard_state)\n    fsdp_osd: Dict[str, Any] = {\"state\": {}, \"param_groups\": []} if to_save else {}\n    fsdp_osd_state: Dict[str, Any] = fsdp_osd[\"state\"] if to_save else {}\n    param_to_fqns = _get_param_to_fqns(model)\n    flat_param_to_fqn = _get_flat_param_to_fqn(model)\n    is_named_optimizer = _is_named_optimizer(optim_state_dict)\n\n    param_key_to_param = cast(\n        Dict[Union[int, str], nn.Parameter],\n        (\n            _get_param_id_to_param_from_optim_input(model, optim_input)\n            if using_optim_input\n            else _get_param_key_to_param(\n                optim, model, is_named_optimizer, param_to_fqns, flat_param_to_fqn\n            )\n        ),\n    )\n    fqn_to_fsdp_param_info = _get_fqn_to_fsdp_param_info(model)\n\n    all_optim_state_keys, optim_state_key_to_param_key = _map_param_key_to_optim_keys(\n        optim_state_dict,\n        group,\n        param_key_to_param,\n        param_to_fqns,\n        fqn_to_fsdp_param_info,\n        merge_keys=use_orig_params,\n    )\n\n    # Iterate in rank 0's flattened parameter ID order to ensure aligned\n    # all-gathers across ranks\n    for optim_state_key in all_optim_state_keys:\n        param_key: Union[str, int, None] = optim_state_key_to_param_key.get(\n            optim_state_key, None\n        )\n\n        if param_key is None:\n            assert use_orig_params, (\n                \"If use_orig_params is False, we must be able to find the \"\n                f\"corresponding param id. {optim_state_key} {param_key}\"\n            )\n            if not optim_state_key.is_fsdp_managed:\n                continue\n\n        if optim_state_key.is_fsdp_managed:\n            # If there are multiple unflat_param_names (not use_orig_params),\n            # they share the same FSDPParamInfo. So the first unflat_param_name\n            # is sufficient to fetch the FSDPParamInfo.\n            fqn = optim_state_key.unflat_param_names[0]\n            fsdp_param_info = fqn_to_fsdp_param_info[fqn]\n            if use_orig_params:\n                state = (\n                    {} if param_key is None else optim_state_dict[\"state\"][param_key]\n                )\n                unflat_state = [\n                    _gather_orig_param_state(\n                        fsdp_param_info, fqn, state, shard_state, group\n                    )\n                ]\n            else:\n                unflat_state = _unflatten_optim_state(\n                    fsdp_param_info,\n                    optim_state_dict[\"state\"][param_key],\n                    to_save,\n                    shard_state,\n                )\n            if to_save:\n                assert len(unflat_state) == len(optim_state_key.unflat_param_names)\n                for unflat_param_name, unflat_param_state in zip(\n                    optim_state_key.unflat_param_names,\n                    unflat_state,\n                ):\n                    fsdp_osd_state[unflat_param_name] = unflat_param_state\n        elif to_save:\n            assert len(optim_state_key.unflat_param_names) == 1\n            unflat_param_name = optim_state_key.unflat_param_names[0]\n            fsdp_osd_state[unflat_param_name] = copy.copy(\n                optim_state_dict[\"state\"][param_key]\n            )\n            for state_name, value in sorted_items(fsdp_osd_state[unflat_param_name]):\n                if torch.is_tensor(value):\n                    fsdp_osd_state[unflat_param_name][state_name] = value.cpu()\n\n    if to_save:\n        flat_param_fqns = set(flat_param_to_fqn.values())\n        for key, value in optim_state_dict[\"state\"].items():\n            if key in fsdp_osd_state:\n                continue\n            if key in flat_param_fqns:\n                continue\n            if key in param_key_to_param:\n                continue\n            # This key is not recognized by FSDP. It may be a user-defined state\n            # or some parameters state that FSDP is unable to map from\n            # ``optim.param_groups``.\n            warnings.warn(\n                f\"Found a optim state, {key}, that FSDP cannot process. FSDP \"\n                \"will directly copy everything to the returned state_dict. In \"\n                \"most cases, this is a user-defined state that is not \"\n                \"associated with any particular parameter. Another possible \"\n                \"case is this state is managed by DMP. Otherwise, there may \"\n                \" be a mismatched assumption of optim_state_dict of this mode.\"\n            )\n            fsdp_osd_state[key] = value\n\n        fsdp_osd[\"param_groups\"] = _unflatten_param_groups(\n            optim_state_dict, param_key_to_param, param_to_fqns\n        )\n\n    return fsdp_osd\n\n\ndef _get_fqn_to_fsdp_param_info(model: nn.Module) -> Dict[str, FSDPParamInfo]:\n    \"\"\"\n    Construct the mapping from a param's fqn to its corresponding ``FSDPParamInfo``\n    if the param is managed by FSDP. ``FlatParameter._fqns`` only stores the first\n    FQN of a shared parameter. So the keys in the mapping are guaranteed to map\n    to unique parameters.\n    \"\"\"\n\n    def module_fn(module, prefix, fqn_to_param_info):\n        fsdp_state = _get_module_fsdp_state_if_fully_sharded_module(module)\n        if fsdp_state is None:\n            return\n        _lazy_init(fsdp_state, module)\n        handles = _module_handles(fsdp_state, module)\n        if not handles:\n            return\n        flat_param = handles[0].flat_param\n        fsdp_param_info = FSDPParamInfo(fsdp_state, flat_param, {})\n        for idx, local_fqn in enumerate(flat_param._fqns):\n            fqn = clean_tensor_name(prefix + local_fqn)\n            if fqn in fqn_to_param_info:\n                assert fqn_to_param_info[fqn].flat_param == flat_param\n            fqn_to_param_info[fqn] = fsdp_param_info\n            fsdp_param_info.param_indices[fqn] = idx\n\n    def return_fn(fqn_to_param_info):\n        return fqn_to_param_info\n\n    fqn_to_param_info: Dict[str, FSDPParamInfo] = {}\n    # FlatParameter._fqns stores the local fqn, starting from the root of the\n    # FSDP. Using _apply_to_modules() with model (may not be the FSDP root\n    # module) allows us to construct the global fqn.\n    return _apply_to_modules(\n        model,\n        module_fn,\n        return_fn,\n        [fqn for fqn, _ in model.named_parameters()],\n        fqn_to_param_info,\n    )\n\n\n@dataclass\nclass StateInfo:\n    tensors: Dict[str, _PosDimTensorInfo]\n    scalar_tensors: Dict[str, torch.Tensor]\n    non_tensors: Dict[str, Any]\n\n\n@dataclass\nclass AllGatherInfo:\n    tensors: List[torch.Tensor]\n    numels: List[int]\n    work: Optional[dist.Work]\n\n\ndef _all_gather_optim_state(\n    fsdp_state: _FSDPState,\n    optim_state: Dict[str, Any],\n    group=None,\n) -> Dict[str, Any]:\n    \"\"\"\n    All-gathering state from all the ranks. This API is slow as it uses\n    ``all_gather_object``. However, optim state_dict is not in the critical path.\n    We can fuse the communication across differnt state if the performance\n    becomes a problem.\n    \"\"\"\n    # Allgather the scalar tensor state, non-tensor states and tensors metadata.\n    processed_state = StateInfo({}, {}, {})\n    for state_name, value in sorted_items(optim_state):\n        if torch.is_tensor(value):\n            if value.dim() == 0:\n                # Ensure that `step` is on CPU.\n                processed_state.scalar_tensors[state_name] = value.cpu()\n            else:\n                processed_state.tensors[state_name] = _PosDimTensorInfo(\n                    value.shape, value.dtype\n                )\n        else:\n            processed_state.non_tensors = value\n    object_list: List[StateInfo] = [\n        processed_state for _ in range(fsdp_state.world_size)\n    ]\n    dist.all_gather_object(object_list, processed_state, group=group)\n\n    # Convert the gathered, pre-proccessed state of each rank to the original one.\n    gathered_state: Dict[str, Any] = {}\n\n    all_tensor_states = sorted(\n        {n for state in object_list for n in state.tensors.keys()}\n    )\n    empty_ranks: Set[int] = set()\n    for name in all_tensor_states:\n        numels = []\n        dtype = torch.float\n        _empty_ranks: Set[int] = set()\n        for rank, object_state in enumerate(object_list):\n            numels.append(0)\n            info = object_state.tensors.get(name, None)\n            if info is not None:\n                numels[-1] = info.shape.numel()\n                dtype = info.dtype\n            if numels[-1] == 0:\n                _empty_ranks.add(rank)\n\n        empty_func = functools.partial(\n            torch.empty, dtype=dtype, device=fsdp_state.compute_device\n        )\n        if empty_ranks:\n            assert empty_ranks == _empty_ranks\n        empty_ranks = _empty_ranks\n        local_state = optim_state.get(name, empty_func(0))\n        local_state = local_state.to(fsdp_state.compute_device)\n        tensors = [\n            empty_func(numel) if rank != fsdp_state.rank else local_state\n            for rank, numel in enumerate(numels)\n        ]\n        work = dist.all_gather(\n            tensors, local_state, group=fsdp_state.process_group, async_op=True\n        )\n        gathered_state[name] = AllGatherInfo(tensors, numels, work)\n\n    for rank, object_state in enumerate(object_list):\n        if rank in empty_ranks:\n            continue\n        for name, non_tensor_value in object_state.non_tensors.items():\n            curr_non_tensor_value = gathered_state.get(name, None)\n            assert (\n                curr_non_tensor_value is None\n                or curr_non_tensor_value == non_tensor_value\n            ), f\"Different ranks have different values for {name}.\"\n            gathered_state[name] = non_tensor_value\n\n        for name, scalar_tensor_value in object_state.scalar_tensors.items():\n            curr_scalar_tensor_value = gathered_state.get(name, None)\n            assert curr_scalar_tensor_value is None or torch.equal(\n                scalar_tensor_value, curr_scalar_tensor_value\n            ), f\"Different ranks have different values for {name}.\"\n            gathered_state[name] = scalar_tensor_value\n\n    for name, value in list(gathered_state.items()):\n        if not isinstance(value, AllGatherInfo):\n            continue\n        assert value.work is not None\n        value.work.wait()\n        gathered_state[name] = torch.cat(\n            [\n                rank_tensor[:rank_numel]\n                for rank_tensor, rank_numel in zip(value.tensors, value.numels)\n                if rank_numel > 0\n            ]\n        )\n\n    return gathered_state\n\n\ndef _gather_orig_param_state(\n    fsdp_param_info: FSDPParamInfo,\n    fqn: str,\n    optim_state: Dict[str, Any],\n    shard_state: bool,\n    group=None,\n) -> Dict[str, Any]:\n    \"\"\"\n    Gather the optimizer state for the original parameter with the name ``fqn``.\n    This API should only be used when ``use_orig_params`` is True.\n    \"\"\"\n    fsdp_state = fsdp_param_info.state\n    assert (\n        fsdp_state._use_orig_params\n    ), \"_gather_orig_param_state only support use_orig_params=True case\"\n    flat_param = fsdp_param_info.flat_param\n    param_idx = fsdp_param_info.param_indices[fqn]\n    if (\n        fsdp_state.world_size == 1\n        or fsdp_state.sharding_strategy == ShardingStrategy.NO_SHARD\n    ):\n        return optim_state\n\n    gathered_state = _all_gather_optim_state(fsdp_state, optim_state, group=group)\n\n    # Unflatten state values.\n    for state_name, value in list(gathered_state.items()):\n        if not torch.is_tensor(value) or value.dim() == 0:\n            continue\n\n        value = value[: flat_param._numels[param_idx]].reshape(\n            flat_param._shapes[param_idx]\n        )\n        if shard_state:\n            assert fsdp_state.process_group is not None\n            value = _ext_chunk_tensor(\n                value,\n                fsdp_state.rank,\n                fsdp_state.world_size,\n                torch.cuda.device_count(),\n                fsdp_state.process_group,\n            )\n        value = value.cpu()\n        gathered_state[state_name] = value\n    return gathered_state\n\n\ndef _shard_orig_param_state(\n    fsdp_param_info: FSDPParamInfo,\n    fqn: str,\n    optim_state: Dict[str, Any],\n) -> Dict[str, Any]:\n    \"\"\"\n    Shard the optimizer state for the original parameter with the name ``fqn``.\n    This API should only be used when ``use_orig_params`` is True.\n    \"\"\"\n    if not optim_state:\n        return {}\n    fsdp_state = fsdp_param_info.state\n    flat_param = fsdp_param_info.flat_param\n    param_idx = fsdp_param_info.param_indices[fqn]\n\n    optim_state = _gather_state_dict(optim_state, fsdp_state.process_group)\n    start, end = flat_param._shard_indices  # type: ignore[attr-defined]\n    if not (start <= param_idx <= end and flat_param._shard_param_offsets):  # type: ignore[attr-defined]\n        return {}\n    param_start, param_end = flat_param._shard_param_offsets[param_idx - start]  # type: ignore[attr-defined]\n\n    # Flatten and shard the state.\n    new_optim_state: Dict[str, Any] = {}\n    for state_name, value in optim_state.items():\n        if torch.is_tensor(value) and value.dim() > 0:\n            value = value.flatten()[param_start : param_end + 1]\n        new_optim_state[state_name] = value\n    return new_optim_state\n"
        },
        {
          "name": "docs",
          "type": "tree",
          "content": null
        },
        {
          "name": "environment.yml",
          "type": "blob",
          "size": 0.2109375,
          "content": "name: openflamingo\nchannels:\n  - defaults\ndependencies:\n  - python=3.9\n  - conda-forge::openjdk\n  - pip\n  - pip:\n    - -r requirements.txt\n    - -r requirements-training.txt\n    - -r requirements-eval.txt\n    - -e .\n"
        },
        {
          "name": "open_flamingo",
          "type": "tree",
          "content": null
        },
        {
          "name": "requirements-eval.txt",
          "type": "blob",
          "size": 0.109375,
          "content": "scipy\ntorchvision\nnltk\ninflection\npycocoevalcap\npycocotools\ntqdm\nscikit-learn\nblack\nmypy\npylint\npytest\nrequests\n"
        },
        {
          "name": "requirements-training.txt",
          "type": "blob",
          "size": 0.044921875,
          "content": "torchvision\nbraceexpand\nwebdataset\ntqdm\nwandb\n"
        },
        {
          "name": "requirements.txt",
          "type": "blob",
          "size": 0.0966796875,
          "content": "einops\neinops-exts\ntransformers>=4.28.1\ntorch==2.0.1\npillow\nopen_clip_torch>=2.16.0 \nsentencepiece\n"
        },
        {
          "name": "setup.py",
          "type": "blob",
          "size": 1.6044921875,
          "content": "from pathlib import Path\n\nfrom setuptools import find_packages, setup\n\nif __name__ == \"__main__\":\n    with Path(Path(__file__).parent, \"README.md\").open(encoding=\"utf-8\") as file:\n        long_description = file.read()\n\n    REQUIREMENTS = [\n        \"einops\",\n        \"einops-exts\",\n        \"transformers>=4.28.1\",\n        \"torch==2.0.1\",\n        \"pillow\",\n        \"open_clip_torch>=2.16.0\",\n        \"sentencepiece\",\n    ]\n\n    EVAL = [\n        \"scipy\",\n        \"torchvision\",\n        \"nltk\",\n        \"inflection\",\n        \"pycocoevalcap\",\n        \"pycocotools\",\n        \"tqdm\",\n    ]\n\n    TRAINING = [\n        \"wandb\",\n        \"torchvision\",\n        \"braceexpand\",\n        \"webdataset\",\n        \"tqdm\",\n    ]\n\n    setup(\n        name=\"open_flamingo\",\n        packages=find_packages(),\n        include_package_data=True,\n        version=\"2.0.1\",\n        license=\"MIT\",\n        description=\"An open-source framework for training large multimodal models\",\n        long_description=long_description,\n        long_description_content_type=\"text/markdown\",\n        data_files=[(\".\", [\"README.md\"])],\n        keywords=[\"machine learning\"],\n        install_requires=REQUIREMENTS,\n        extras_require={\n            \"eval\": EVAL,\n            \"training\": TRAINING,\n            \"all\": list(set(REQUIREMENTS + EVAL + TRAINING)),\n        },\n        classifiers=[\n            \"Development Status :: 4 - Beta\",\n            \"Intended Audience :: Developers\",\n            \"Topic :: Scientific/Engineering :: Artificial Intelligence\",\n            \"License :: OSI Approved :: MIT License\",\n            \"Programming Language :: Python :: 3.9\",\n        ],\n    )\n"
        }
      ]
    }
  ]
}