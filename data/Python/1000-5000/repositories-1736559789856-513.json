{
  "metadata": {
    "timestamp": 1736559789856,
    "page": 513,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjUyMA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "lkwq007/stablediffusion-infinity",
      "stars": 3865,
      "defaultBranch": "master",
      "files": [
        {
          "name": ".dockerignore",
          "type": "blob",
          "size": 0.0576171875,
          "content": ".idea/\n.github/\n.git/\ndocs/\n.dockerignore\nreadme.md\nLICENSE"
        },
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.0830078125,
          "content": "__pycache__/\nMakefile\n.ipynb_checkpoints/\nbuild/\ncsrc/\n.idea/\ntravis.sh\n*.iml\n.token\n"
        },
        {
          "name": ".gitmodules",
          "type": "blob",
          "size": 0.3916015625,
          "content": "[submodule \"glid_3_xl_stable\"]\n\tpath = glid_3_xl_stable\n\turl = https://github.com/lkwq007/glid_3_xl_stable.git\n[submodule \"PyPatchMatch\"]\n\tpath = PyPatchMatch\n\turl = https://github.com/lkwq007/PyPatchMatch.git\n[submodule \"sd_grpcserver\"]\n\tpath = sd_grpcserver\n\turl = https://github.com/lkwq007/sd_grpcserver.git\n[submodule \"blip_model\"]\n\tpath = blip_model\n\turl = https://github.com/lkwq007/blip_model\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 11.0908203125,
          "content": "                                 Apache License\n                           Version 2.0, January 2004\n                        http://www.apache.org/licenses/\n\n   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n\n   1. Definitions.\n\n      \"License\" shall mean the terms and conditions for use, reproduction,\n      and distribution as defined by Sections 1 through 9 of this document.\n\n      \"Licensor\" shall mean the copyright owner or entity authorized by\n      the copyright owner that is granting the License.\n\n      \"Legal Entity\" shall mean the union of the acting entity and all\n      other entities that control, are controlled by, or are under common\n      control with that entity. For the purposes of this definition,\n      \"control\" means (i) the power, direct or indirect, to cause the\n      direction or management of such entity, whether by contract or\n      otherwise, or (ii) ownership of fifty percent (50%) or more of the\n      outstanding shares, or (iii) beneficial ownership of such entity.\n\n      \"You\" (or \"Your\") shall mean an individual or Legal Entity\n      exercising permissions granted by this License.\n\n      \"Source\" form shall mean the preferred form for making modifications,\n      including but not limited to software source code, documentation\n      source, and configuration files.\n\n      \"Object\" form shall mean any form resulting from mechanical\n      transformation or translation of a Source form, including but\n      not limited to compiled object code, generated documentation,\n      and conversions to other media types.\n\n      \"Work\" shall mean the work of authorship, whether in Source or\n      Object form, made available under the License, as indicated by a\n      copyright notice that is included in or attached to the work\n      (an example is provided in the Appendix below).\n\n      \"Derivative Works\" shall mean any work, whether in Source or Object\n      form, that is based on (or derived from) the Work and for which the\n      editorial revisions, annotations, elaborations, or other modifications\n      represent, as a whole, an original work of authorship. For the purposes\n      of this License, Derivative Works shall not include works that remain\n      separable from, or merely link (or bind by name) to the interfaces of,\n      the Work and Derivative Works thereof.\n\n      \"Contribution\" shall mean any work of authorship, including\n      the original version of the Work and any modifications or additions\n      to that Work or Derivative Works thereof, that is intentionally\n      submitted to Licensor for inclusion in the Work by the copyright owner\n      or by an individual or Legal Entity authorized to submit on behalf of\n      the copyright owner. For the purposes of this definition, \"submitted\"\n      means any form of electronic, verbal, or written communication sent\n      to the Licensor or its representatives, including but not limited to\n      communication on electronic mailing lists, source code control systems,\n      and issue tracking systems that are managed by, or on behalf of, the\n      Licensor for the purpose of discussing and improving the Work, but\n      excluding communication that is conspicuously marked or otherwise\n      designated in writing by the copyright owner as \"Not a Contribution.\"\n\n      \"Contributor\" shall mean Licensor and any individual or Legal Entity\n      on behalf of whom a Contribution has been received by Licensor and\n      subsequently incorporated within the Work.\n\n   2. Grant of Copyright License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      copyright license to reproduce, prepare Derivative Works of,\n      publicly display, publicly perform, sublicense, and distribute the\n      Work and such Derivative Works in Source or Object form.\n\n   3. Grant of Patent License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      (except as stated in this section) patent license to make, have made,\n      use, offer to sell, sell, import, and otherwise transfer the Work,\n      where such license applies only to those patent claims licensable\n      by such Contributor that are necessarily infringed by their\n      Contribution(s) alone or by combination of their Contribution(s)\n      with the Work to which such Contribution(s) was submitted. If You\n      institute patent litigation against any entity (including a\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\n      or a Contribution incorporated within the Work constitutes direct\n      or contributory patent infringement, then any patent licenses\n      granted to You under this License for that Work shall terminate\n      as of the date such litigation is filed.\n\n   4. Redistribution. You may reproduce and distribute copies of the\n      Work or Derivative Works thereof in any medium, with or without\n      modifications, and in Source or Object form, provided that You\n      meet the following conditions:\n\n      (a) You must give any other recipients of the Work or\n          Derivative Works a copy of this License; and\n\n      (b) You must cause any modified files to carry prominent notices\n          stating that You changed the files; and\n\n      (c) You must retain, in the Source form of any Derivative Works\n          that You distribute, all copyright, patent, trademark, and\n          attribution notices from the Source form of the Work,\n          excluding those notices that do not pertain to any part of\n          the Derivative Works; and\n\n      (d) If the Work includes a \"NOTICE\" text file as part of its\n          distribution, then any Derivative Works that You distribute must\n          include a readable copy of the attribution notices contained\n          within such NOTICE file, excluding those notices that do not\n          pertain to any part of the Derivative Works, in at least one\n          of the following places: within a NOTICE text file distributed\n          as part of the Derivative Works; within the Source form or\n          documentation, if provided along with the Derivative Works; or,\n          within a display generated by the Derivative Works, if and\n          wherever such third-party notices normally appear. The contents\n          of the NOTICE file are for informational purposes only and\n          do not modify the License. You may add Your own attribution\n          notices within Derivative Works that You distribute, alongside\n          or as an addendum to the NOTICE text from the Work, provided\n          that such additional attribution notices cannot be construed\n          as modifying the License.\n\n      You may add Your own copyright statement to Your modifications and\n      may provide additional or different license terms and conditions\n      for use, reproduction, or distribution of Your modifications, or\n      for any such Derivative Works as a whole, provided Your use,\n      reproduction, and distribution of the Work otherwise complies with\n      the conditions stated in this License.\n\n   5. Submission of Contributions. Unless You explicitly state otherwise,\n      any Contribution intentionally submitted for inclusion in the Work\n      by You to the Licensor shall be under the terms and conditions of\n      this License, without any additional terms or conditions.\n      Notwithstanding the above, nothing herein shall supersede or modify\n      the terms of any separate license agreement you may have executed\n      with Licensor regarding such Contributions.\n\n   6. Trademarks. This License does not grant permission to use the trade\n      names, trademarks, service marks, or product names of the Licensor,\n      except as required for reasonable and customary use in describing the\n      origin of the Work and reproducing the content of the NOTICE file.\n\n   7. Disclaimer of Warranty. Unless required by applicable law or\n      agreed to in writing, Licensor provides the Work (and each\n      Contributor provides its Contributions) on an \"AS IS\" BASIS,\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n      implied, including, without limitation, any warranties or conditions\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n      PARTICULAR PURPOSE. You are solely responsible for determining the\n      appropriateness of using or redistributing the Work and assume any\n      risks associated with Your exercise of permissions under this License.\n\n   8. Limitation of Liability. In no event and under no legal theory,\n      whether in tort (including negligence), contract, or otherwise,\n      unless required by applicable law (such as deliberate and grossly\n      negligent acts) or agreed to in writing, shall any Contributor be\n      liable to You for damages, including any direct, indirect, special,\n      incidental, or consequential damages of any character arising as a\n      result of this License or out of the use or inability to use the\n      Work (including but not limited to damages for loss of goodwill,\n      work stoppage, computer failure or malfunction, or any and all\n      other commercial damages or losses), even if such Contributor\n      has been advised of the possibility of such damages.\n\n   9. Accepting Warranty or Additional Liability. While redistributing\n      the Work or Derivative Works thereof, You may choose to offer,\n      and charge a fee for, acceptance of support, warranty, indemnity,\n      or other liability obligations and/or rights consistent with this\n      License. However, in accepting such obligations, You may act only\n      on Your own behalf and on Your sole responsibility, not on behalf\n      of any other Contributor, and only if You agree to indemnify,\n      defend, and hold each Contributor harmless for any liability\n      incurred by, or claims asserted against, such Contributor by reason\n      of your accepting any such warranty or additional liability.\n\n   END OF TERMS AND CONDITIONS\n\n   APPENDIX: How to apply the Apache License to your work.\n\n      To apply the Apache License to your work, attach the following\n      boilerplate notice, with the fields enclosed by brackets \"[]\"\n      replaced with your own identifying information. (Don't include\n      the brackets!)  The text should be enclosed in the appropriate\n      comment syntax for the file format. We also recommend that a\n      file or class name and description of purpose be included on the\n      same \"printed page\" as the copyright notice for easier\n      identification within third-party archives.\n\n   Copyright [yyyy] [name of copyright owner]\n\n   Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.\n"
        },
        {
          "name": "PyPatchMatch",
          "type": "commit",
          "content": null
        },
        {
          "name": "app.py",
          "type": "blob",
          "size": 42.9609375,
          "content": "import io\nimport base64\nimport os\nimport sys\n\nimport numpy as np\nimport torch\nfrom torch import autocast\nimport diffusers\n\nassert tuple(map(int,diffusers.__version__.split(\".\")))  >= (0,9,0), \"Please upgrade diffusers to 0.9.0\"\n\nfrom diffusers.configuration_utils import FrozenDict\nfrom diffusers import (\n    StableDiffusionPipeline,\n    StableDiffusionInpaintPipeline,\n    StableDiffusionImg2ImgPipeline,\n    StableDiffusionInpaintPipelineLegacy,\n    DDIMScheduler,\n    LMSDiscreteScheduler,\n    DiffusionPipeline,\n    StableDiffusionUpscalePipeline,\n    DPMSolverMultistepScheduler,\n    PNDMScheduler,\n)\nfrom diffusers.models import AutoencoderKL\nfrom PIL import Image\nfrom PIL import ImageOps\nimport gradio as gr\nimport base64\nimport skimage\nimport skimage.measure\nimport yaml\nimport json\nfrom enum import Enum\nfrom utils import *\n\ntry:\n    abspath = os.path.abspath(__file__)\n    dirname = os.path.dirname(abspath)\n    os.chdir(dirname)\nexcept:\n    pass\n\ntry:\n    from interrogate import Interrogator\nexcept:\n    Interrogator = DummyInterrogator\n\nUSE_NEW_DIFFUSERS = True\nRUN_IN_SPACE = \"RUN_IN_HG_SPACE\" in os.environ\n\n\nclass ModelChoice(Enum):\n    INPAINTING = \"stablediffusion-inpainting\"\n    INPAINTING2 = \"stablediffusion-2-inpainting\"\n    INPAINTING_IMG2IMG = \"stablediffusion-inpainting+img2img-1.5\"\n    MODEL_2_1 = \"stablediffusion-2.1\"\n    MODEL_2_0_V = \"stablediffusion-2.0v\"\n    MODEL_2_0 = \"stablediffusion-2.0\"\n    MODEL_1_5 = \"stablediffusion-1.5\"\n    MODEL_1_4 = \"stablediffusion-1.4\"\n\n\ntry:\n    from sd_grpcserver.pipeline.unified_pipeline import UnifiedPipeline\nexcept:\n    UnifiedPipeline = StableDiffusionInpaintPipeline\n\n# sys.path.append(\"./glid_3_xl_stable\")\n\nUSE_GLID = False\n# try:\n#     from glid3xlmodel import GlidModel\n# except:\n#     USE_GLID = False\n\ntry:\n    import onnxruntime\n    onnx_available = True\n    onnx_providers = [\"CUDAExecutionProvider\", \"DmlExecutionProvider\", \"OpenVINOExecutionProvider\", 'CPUExecutionProvider']\n    available_providers = onnxruntime.get_available_providers()\n    onnx_providers = [item for item in onnx_providers if item in available_providers]\nexcept:\n    onnx_available = False\n    onnx_providers = []\n\ntry:\n    cuda_available = torch.cuda.is_available()\nexcept:\n    cuda_available = False\nfinally:\n    if sys.platform == \"darwin\":\n        device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n    elif cuda_available:\n        device = \"cuda\"\n    else:\n        device = \"cpu\"\n\nif device != \"cuda\":\n    import contextlib\n\n    autocast = contextlib.nullcontext\n\nwith open(\"config.yaml\", \"r\") as yaml_in:\n    yaml_object = yaml.safe_load(yaml_in)\n    config_json = json.dumps(yaml_object)\n\n\ndef load_html():\n    body, canvaspy = \"\", \"\"\n    with open(\"index.html\", encoding=\"utf8\") as f:\n        body = f.read()\n    with open(\"canvas.py\", encoding=\"utf8\") as f:\n        canvaspy = f.read()\n    body = body.replace(\"- paths:\\n\", \"\")\n    body = body.replace(\"  - ./canvas.py\\n\", \"\")\n    body = body.replace(\"from canvas import InfCanvas\", canvaspy)\n    return body\n\n\ndef test(x):\n    x = load_html()\n    return f\"\"\"<iframe id=\"sdinfframe\" style=\"width: 100%; height: 600px\" name=\"result\" allow=\"midi; geolocation; microphone; camera; \n    display-capture; encrypted-media; vertical-scroll 'none'\" sandbox=\"allow-modals allow-forms \n    allow-scripts allow-same-origin allow-popups \n    allow-top-navigation-by-user-activation allow-downloads\" allowfullscreen=\"\" \n    allowpaymentrequest=\"\" frameborder=\"0\" srcdoc='{x}'></iframe>\"\"\"\n\n\nDEBUG_MODE = False\n\ntry:\n    SAMPLING_MODE = Image.Resampling.LANCZOS\nexcept Exception as e:\n    SAMPLING_MODE = Image.LANCZOS\n\ntry:\n    contain_func = ImageOps.contain\nexcept Exception as e:\n\n    def contain_func(image, size, method=SAMPLING_MODE):\n        # from PIL: https://pillow.readthedocs.io/en/stable/reference/ImageOps.html#PIL.ImageOps.contain\n        im_ratio = image.width / image.height\n        dest_ratio = size[0] / size[1]\n        if im_ratio != dest_ratio:\n            if im_ratio > dest_ratio:\n                new_height = int(image.height / image.width * size[0])\n                if new_height != size[1]:\n                    size = (size[0], new_height)\n            else:\n                new_width = int(image.width / image.height * size[1])\n                if new_width != size[0]:\n                    size = (new_width, size[1])\n        return image.resize(size, resample=method)\n\n\nimport argparse\n\nparser = argparse.ArgumentParser(description=\"stablediffusion-infinity\")\nparser.add_argument(\"--port\", type=int, help=\"listen port\", dest=\"server_port\")\nparser.add_argument(\"--host\", type=str, help=\"host\", dest=\"server_name\")\nparser.add_argument(\"--share\", action=\"store_true\", help=\"share this app?\")\nparser.add_argument(\"--debug\", action=\"store_true\", help=\"debug mode\")\nparser.add_argument(\"--fp32\", action=\"store_true\", help=\"using full precision\")\nparser.add_argument(\"--lowvram\", action=\"store_true\", help=\"using lowvram mode\")\nparser.add_argument(\"--encrypt\", action=\"store_true\", help=\"using https?\")\nparser.add_argument(\"--ssl_keyfile\", type=str, help=\"path to ssl_keyfile\")\nparser.add_argument(\"--ssl_certfile\", type=str, help=\"path to ssl_certfile\")\nparser.add_argument(\"--ssl_keyfile_password\", type=str, help=\"ssl_keyfile_password\")\nparser.add_argument(\n    \"--auth\", nargs=2, metavar=(\"username\", \"password\"), help=\"use username password\"\n)\nparser.add_argument(\n    \"--remote_model\",\n    type=str,\n    help=\"use a model (e.g. dreambooth fined) from huggingface hub\",\n    default=\"\",\n)\nparser.add_argument(\n    \"--local_model\", type=str, help=\"use a model stored on your PC\", default=\"\"\n)\n\nif __name__ == \"__main__\":\n    args = parser.parse_args()\nelse:\n    args = parser.parse_args([\"--debug\"])\n# args = parser.parse_args([\"--debug\"])\nif args.auth is not None:\n    args.auth = tuple(args.auth)\n\nmodel = {}\n\n\ndef get_token():\n    token = \"\"\n    if os.path.exists(\".token\"):\n        with open(\".token\", \"r\") as f:\n            token = f.read()\n    token = os.environ.get(\"hftoken\", token)\n    return token\n\n\ndef save_token(token):\n    with open(\".token\", \"w\") as f:\n        f.write(token)\n\n\ndef prepare_scheduler(scheduler):\n    if hasattr(scheduler.config, \"steps_offset\") and scheduler.config.steps_offset != 1:\n        new_config = dict(scheduler.config)\n        new_config[\"steps_offset\"] = 1\n        scheduler._internal_dict = FrozenDict(new_config)\n    return scheduler\n\n\ndef my_resize(width, height):\n    if width >= 512 and height >= 512:\n        return width, height\n    if width == height:\n        return 512, 512\n    smaller = min(width, height)\n    larger = max(width, height)\n    if larger >= 608:\n        return width, height\n    factor = 1\n    if smaller < 290:\n        factor = 2\n    elif smaller < 330:\n        factor = 1.75\n    elif smaller < 384:\n        factor = 1.375\n    elif smaller < 400:\n        factor = 1.25\n    elif smaller < 450:\n        factor = 1.125\n    return int(factor * width) // 8 * 8, int(factor * height) // 8 * 8\n\n\ndef load_learned_embed_in_clip(\n    learned_embeds_path, text_encoder, tokenizer, token=None\n):\n    # https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/stable_conceptualizer_inference.ipynb\n    loaded_learned_embeds = torch.load(learned_embeds_path, map_location=\"cpu\")\n\n    # separate token and the embeds\n    trained_token = list(loaded_learned_embeds.keys())[0]\n    embeds = loaded_learned_embeds[trained_token]\n\n    # cast to dtype of text_encoder\n    dtype = text_encoder.get_input_embeddings().weight.dtype\n    embeds.to(dtype)\n\n    # add the token in tokenizer\n    token = token if token is not None else trained_token\n    num_added_tokens = tokenizer.add_tokens(token)\n    if num_added_tokens == 0:\n        raise ValueError(\n            f\"The tokenizer already contains the token {token}. Please pass a different `token` that is not already in the tokenizer.\"\n        )\n\n    # resize the token embeddings\n    text_encoder.resize_token_embeddings(len(tokenizer))\n\n    # get the id for the token and assign the embeds\n    token_id = tokenizer.convert_tokens_to_ids(token)\n    text_encoder.get_input_embeddings().weight.data[token_id] = embeds\n\n\nscheduler_dict = {\"PLMS\": None, \"DDIM\": None, \"K-LMS\": None, \"DPM\": None, \"PNDM\": None}\n\n\nclass StableDiffusionInpaint:\n    def __init__(\n        self, token: str = \"\", model_name: str = \"\", model_path: str = \"\", **kwargs,\n    ):\n        self.token = token\n        original_checkpoint = False\n        if device == \"cpu\" and onnx_available:\n            from diffusers import OnnxStableDiffusionInpaintPipeline\n            inpaint = OnnxStableDiffusionInpaintPipeline.from_pretrained(\n                model_name,\n                revision=\"onnx\",\n                provider=onnx_providers[0] if onnx_providers else None\n                )\n        else:\n            if model_path and os.path.exists(model_path):\n                if model_path.endswith(\".ckpt\"):\n                    original_checkpoint = True\n                elif model_path.endswith(\".json\"):\n                    model_name = os.path.dirname(model_path)\n                else:\n                    model_name = model_path\n            vae = AutoencoderKL.from_pretrained(\"stabilityai/sd-vae-ft-mse\")\n            if device == \"cuda\" and not args.fp32:\n                vae.to(torch.float16)\n            if original_checkpoint:\n                print(f\"Converting & Loading {model_path}\")\n                from convert_checkpoint import convert_checkpoint\n\n                pipe = convert_checkpoint(model_path, inpainting=True)\n                if device == \"cuda\" and not args.fp32:\n                    pipe.to(torch.float16)\n                inpaint = StableDiffusionInpaintPipeline(\n                    vae=vae,\n                    text_encoder=pipe.text_encoder,\n                    tokenizer=pipe.tokenizer,\n                    unet=pipe.unet,\n                    scheduler=pipe.scheduler,\n                    safety_checker=pipe.safety_checker,\n                    feature_extractor=pipe.feature_extractor,\n                )\n            else:\n                print(f\"Loading {model_name}\")\n                if device == \"cuda\" and not args.fp32:\n                    inpaint = StableDiffusionInpaintPipeline.from_pretrained(\n                        model_name,\n                        revision=\"fp16\",\n                        torch_dtype=torch.float16,\n                        use_auth_token=token,\n                        vae=vae,\n                    )\n                else:\n                    inpaint = StableDiffusionInpaintPipeline.from_pretrained(\n                        model_name, use_auth_token=token, vae=vae\n                    )\n            if os.path.exists(\"./embeddings\"):\n                print(\"Note that StableDiffusionInpaintPipeline + embeddings is untested\")\n                for item in os.listdir(\"./embeddings\"):\n                    if item.endswith(\".bin\"):\n                        load_learned_embed_in_clip(\n                            os.path.join(\"./embeddings\", item),\n                            inpaint.text_encoder,\n                            inpaint.tokenizer,\n                        )\n            inpaint.to(device)\n        # if device == \"mps\":\n        # _ = text2img(\"\", num_inference_steps=1)\n        scheduler_dict[\"PLMS\"] = inpaint.scheduler\n        scheduler_dict[\"DDIM\"] = prepare_scheduler(\n            DDIMScheduler(\n                beta_start=0.00085,\n                beta_end=0.012,\n                beta_schedule=\"scaled_linear\",\n                clip_sample=False,\n                set_alpha_to_one=False,\n            )\n        )\n        scheduler_dict[\"K-LMS\"] = prepare_scheduler(\n            LMSDiscreteScheduler(\n                beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\"\n            )\n        )\n        scheduler_dict[\"PNDM\"] = prepare_scheduler(\n            PNDMScheduler(\n                beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\",\n                skip_prk_steps=True\n            )\n        )\n        scheduler_dict[\"DPM\"] = prepare_scheduler(\n            DPMSolverMultistepScheduler.from_config(inpaint.scheduler.config)\n        )\n        self.safety_checker = inpaint.safety_checker\n        save_token(token)\n        try:\n            total_memory = torch.cuda.get_device_properties(0).total_memory // (\n                1024 ** 3\n            )\n            if total_memory <= 5 or args.lowvram:\n                inpaint.enable_attention_slicing()\n                inpaint.enable_sequential_cpu_offload()\n        except:\n            pass\n        self.inpaint = inpaint\n\n    def run(\n        self,\n        image_pil,\n        prompt=\"\",\n        negative_prompt=\"\",\n        guidance_scale=7.5,\n        resize_check=True,\n        enable_safety=True,\n        fill_mode=\"patchmatch\",\n        strength=0.75,\n        step=50,\n        enable_img2img=False,\n        use_seed=False,\n        seed_val=-1,\n        generate_num=1,\n        scheduler=\"\",\n        scheduler_eta=0.0,\n        **kwargs,\n    ):\n        inpaint = self.inpaint\n        selected_scheduler = scheduler_dict.get(scheduler, scheduler_dict[\"PLMS\"])\n        for item in [inpaint]:\n            item.scheduler = selected_scheduler\n            if enable_safety or self.safety_checker is None:\n                item.safety_checker = self.safety_checker\n            else:\n                item.safety_checker = lambda images, **kwargs: (images, False)\n        width, height = image_pil.size\n        sel_buffer = np.array(image_pil)\n        img = sel_buffer[:, :, 0:3]\n        mask = sel_buffer[:, :, -1]\n        nmask = 255 - mask\n        process_width = width\n        process_height = height\n        if resize_check:\n            process_width, process_height = my_resize(width, height)\n        process_width = process_width * 8 // 8\n        process_height = process_height * 8 // 8\n        extra_kwargs = {\n            \"num_inference_steps\": step,\n            \"guidance_scale\": guidance_scale,\n            \"eta\": scheduler_eta,\n        }\n        if USE_NEW_DIFFUSERS:\n            extra_kwargs[\"negative_prompt\"] = negative_prompt\n            extra_kwargs[\"num_images_per_prompt\"] = generate_num\n        if use_seed:\n            generator = torch.Generator(inpaint.device).manual_seed(seed_val)\n            extra_kwargs[\"generator\"] = generator\n        if True:\n            if fill_mode == \"g_diffuser\":\n                mask = 255 - mask\n                mask = mask[:, :, np.newaxis].repeat(3, axis=2)\n                img, mask = functbl[fill_mode](img, mask)\n            else:\n                img, mask = functbl[fill_mode](img, mask)\n                mask = 255 - mask\n                mask = skimage.measure.block_reduce(mask, (8, 8), np.max)\n                mask = mask.repeat(8, axis=0).repeat(8, axis=1)\n            # extra_kwargs[\"strength\"] = strength\n            inpaint_func = inpaint\n            init_image = Image.fromarray(img)\n            mask_image = Image.fromarray(mask)\n            # mask_image=mask_image.filter(ImageFilter.GaussianBlur(radius = 8))\n            if True:\n                images = inpaint_func(\n                    prompt=prompt,\n                    image=init_image.resize(\n                        (process_width, process_height), resample=SAMPLING_MODE\n                    ),\n                    mask_image=mask_image.resize((process_width, process_height)),\n                    width=process_width,\n                    height=process_height,\n                    **extra_kwargs,\n                )[\"images\"]\n        return images\n\n\nclass StableDiffusion:\n    def __init__(\n        self,\n        token: str = \"\",\n        model_name: str = \"runwayml/stable-diffusion-v1-5\",\n        model_path: str = None,\n        inpainting_model: bool = False,\n        **kwargs,\n    ):\n        self.token = token\n        original_checkpoint = False\n        if device==\"cpu\" and onnx_available:\n            from diffusers import OnnxStableDiffusionPipeline, OnnxStableDiffusionInpaintPipelineLegacy, OnnxStableDiffusionImg2ImgPipeline\n            text2img = OnnxStableDiffusionPipeline.from_pretrained(\n                model_name,\n                revision=\"onnx\",\n                provider=onnx_providers[0] if onnx_providers else None\n                )\n            inpaint = OnnxStableDiffusionInpaintPipelineLegacy(\n                    vae_encoder=text2img.vae_encoder,\n                    vae_decoder=text2img.vae_decoder,\n                    text_encoder=text2img.text_encoder,\n                    tokenizer=text2img.tokenizer,\n                    unet=text2img.unet,\n                    scheduler=text2img.scheduler,\n                    safety_checker=text2img.safety_checker,\n                    feature_extractor=text2img.feature_extractor,\n                )\n            img2img = OnnxStableDiffusionImg2ImgPipeline(\n                vae_encoder=text2img.vae_encoder,\n                vae_decoder=text2img.vae_decoder,\n                text_encoder=text2img.text_encoder,\n                tokenizer=text2img.tokenizer,\n                unet=text2img.unet,\n                scheduler=text2img.scheduler,\n                safety_checker=text2img.safety_checker,\n                feature_extractor=text2img.feature_extractor,\n            )\n        else:\n            if model_path and os.path.exists(model_path):\n                if model_path.endswith(\".ckpt\"):\n                    original_checkpoint = True\n                elif model_path.endswith(\".json\"):\n                    model_name = os.path.dirname(model_path)\n                else:\n                    model_name = model_path\n            vae = AutoencoderKL.from_pretrained(\"stabilityai/sd-vae-ft-mse\")\n            if device == \"cuda\" and not args.fp32:\n                vae.to(torch.float16)\n            if original_checkpoint:\n                print(f\"Converting & Loading {model_path}\")\n                from convert_checkpoint import convert_checkpoint\n\n                pipe = convert_checkpoint(model_path)\n                if device == \"cuda\" and not args.fp32:\n                    pipe.to(torch.float16)\n                text2img = StableDiffusionPipeline(\n                    vae=vae,\n                    text_encoder=pipe.text_encoder,\n                    tokenizer=pipe.tokenizer,\n                    unet=pipe.unet,\n                    scheduler=pipe.scheduler,\n                    safety_checker=pipe.safety_checker,\n                    feature_extractor=pipe.feature_extractor,\n                )\n            else:\n                print(f\"Loading {model_name}\")\n                if device == \"cuda\" and not args.fp32:\n                    text2img = StableDiffusionPipeline.from_pretrained(\n                        model_name,\n                        revision=\"fp16\",\n                        torch_dtype=torch.float16,\n                        use_auth_token=token,\n                        vae=vae,\n                    )\n                else:\n                    text2img = StableDiffusionPipeline.from_pretrained(\n                        model_name, use_auth_token=token, vae=vae\n                    )\n            if inpainting_model:\n                # can reduce vRAM by reusing models except unet\n                text2img_unet = text2img.unet\n                del text2img.vae\n                del text2img.text_encoder\n                del text2img.tokenizer\n                del text2img.scheduler\n                del text2img.safety_checker\n                del text2img.feature_extractor\n                import gc\n\n                gc.collect()\n                if device == \"cuda\" and not args.fp32:\n                    inpaint = StableDiffusionInpaintPipeline.from_pretrained(\n                        \"runwayml/stable-diffusion-inpainting\",\n                        revision=\"fp16\",\n                        torch_dtype=torch.float16,\n                        use_auth_token=token,\n                        vae=vae,\n                    ).to(device)\n                else:\n                    inpaint = StableDiffusionInpaintPipeline.from_pretrained(\n                        \"runwayml/stable-diffusion-inpainting\",\n                        use_auth_token=token,\n                        vae=vae,\n                    ).to(device)\n                text2img_unet.to(device)\n                text2img = StableDiffusionPipeline(\n                    vae=inpaint.vae,\n                    text_encoder=inpaint.text_encoder,\n                    tokenizer=inpaint.tokenizer,\n                    unet=text2img_unet,\n                    scheduler=inpaint.scheduler,\n                    safety_checker=inpaint.safety_checker,\n                    feature_extractor=inpaint.feature_extractor,\n                )\n            else:\n                inpaint = StableDiffusionInpaintPipelineLegacy(\n                    vae=text2img.vae,\n                    text_encoder=text2img.text_encoder,\n                    tokenizer=text2img.tokenizer,\n                    unet=text2img.unet,\n                    scheduler=text2img.scheduler,\n                    safety_checker=text2img.safety_checker,\n                    feature_extractor=text2img.feature_extractor,\n                ).to(device)\n            text_encoder = text2img.text_encoder\n            tokenizer = text2img.tokenizer\n            if os.path.exists(\"./embeddings\"):\n                for item in os.listdir(\"./embeddings\"):\n                    if item.endswith(\".bin\"):\n                        load_learned_embed_in_clip(\n                            os.path.join(\"./embeddings\", item),\n                            text2img.text_encoder,\n                            text2img.tokenizer,\n                        )\n            text2img.to(device)\n            if device == \"mps\":\n                _ = text2img(\"\", num_inference_steps=1)\n            img2img = StableDiffusionImg2ImgPipeline(\n                vae=text2img.vae,\n                text_encoder=text2img.text_encoder,\n                tokenizer=text2img.tokenizer,\n                unet=text2img.unet,\n                scheduler=text2img.scheduler,\n                safety_checker=text2img.safety_checker,\n                feature_extractor=text2img.feature_extractor,\n            ).to(device)\n        scheduler_dict[\"PLMS\"] = text2img.scheduler\n        scheduler_dict[\"DDIM\"] = prepare_scheduler(\n            DDIMScheduler(\n                beta_start=0.00085,\n                beta_end=0.012,\n                beta_schedule=\"scaled_linear\",\n                clip_sample=False,\n                set_alpha_to_one=False,\n            )\n        )\n        scheduler_dict[\"K-LMS\"] = prepare_scheduler(\n            LMSDiscreteScheduler(\n                beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\"\n            )\n        )\n        scheduler_dict[\"PNDM\"] = prepare_scheduler(\n            PNDMScheduler(\n                beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\",\n                skip_prk_steps=True\n            )\n        )\n        scheduler_dict[\"DPM\"] = prepare_scheduler(\n            DPMSolverMultistepScheduler.from_config(text2img.scheduler.config)\n        )\n        self.safety_checker = text2img.safety_checker\n        save_token(token)\n        try:\n            total_memory = torch.cuda.get_device_properties(0).total_memory // (\n                1024 ** 3\n            )\n            if total_memory <= 5 or args.lowvram:\n                inpaint.enable_attention_slicing()\n                inpaint.enable_sequential_cpu_offload()\n                if inpainting_model:\n                    text2img.enable_attention_slicing()\n                    text2img.enable_sequential_cpu_offload()\n        except:\n            pass\n        self.text2img = text2img\n        self.inpaint = inpaint\n        self.img2img = img2img\n        if True:\n            self.unified = inpaint\n        else:\n            self.unified = UnifiedPipeline(\n                vae=text2img.vae,\n                text_encoder=text2img.text_encoder,\n                tokenizer=text2img.tokenizer,\n                unet=text2img.unet,\n                scheduler=text2img.scheduler,\n                safety_checker=text2img.safety_checker,\n                feature_extractor=text2img.feature_extractor,\n            ).to(device)\n        self.inpainting_model = inpainting_model\n\n    def run(\n        self,\n        image_pil,\n        prompt=\"\",\n        negative_prompt=\"\",\n        guidance_scale=7.5,\n        resize_check=True,\n        enable_safety=True,\n        fill_mode=\"patchmatch\",\n        strength=0.75,\n        step=50,\n        enable_img2img=False,\n        use_seed=False,\n        seed_val=-1,\n        generate_num=1,\n        scheduler=\"\",\n        scheduler_eta=0.0,\n        **kwargs,\n    ):\n        text2img, inpaint, img2img, unified = (\n            self.text2img,\n            self.inpaint,\n            self.img2img,\n            self.unified,\n        )\n        selected_scheduler = scheduler_dict.get(scheduler, scheduler_dict[\"PLMS\"])\n        for item in [text2img, inpaint, img2img, unified]:\n            item.scheduler = selected_scheduler\n            if enable_safety or self.safety_checker is None:\n                item.safety_checker = self.safety_checker\n            else:\n                item.safety_checker = lambda images, **kwargs: (images, False)\n        if RUN_IN_SPACE:\n            step = max(150, step)\n            image_pil = contain_func(image_pil, (1024, 1024))\n        width, height = image_pil.size\n        sel_buffer = np.array(image_pil)\n        img = sel_buffer[:, :, 0:3]\n        mask = sel_buffer[:, :, -1]\n        nmask = 255 - mask\n        process_width = width\n        process_height = height\n        if resize_check:\n            process_width, process_height = my_resize(width, height)\n        extra_kwargs = {\n            \"num_inference_steps\": step,\n            \"guidance_scale\": guidance_scale,\n            \"eta\": scheduler_eta,\n        }\n        if RUN_IN_SPACE:\n            generate_num = max(\n                int(4 * 512 * 512 // process_width // process_height), generate_num\n            )\n        if USE_NEW_DIFFUSERS:\n            extra_kwargs[\"negative_prompt\"] = negative_prompt\n            extra_kwargs[\"num_images_per_prompt\"] = generate_num\n        if use_seed:\n            generator = torch.Generator(text2img.device).manual_seed(seed_val)\n            extra_kwargs[\"generator\"] = generator\n        if nmask.sum() < 1 and enable_img2img:\n            init_image = Image.fromarray(img)\n            if True:\n                images = img2img(\n                    prompt=prompt,\n                    image=init_image.resize(\n                        (process_width, process_height), resample=SAMPLING_MODE\n                    ),\n                    strength=strength,\n                    **extra_kwargs,\n                )[\"images\"]\n        elif mask.sum() > 0:\n            if fill_mode == \"g_diffuser\" and not self.inpainting_model:\n                mask = 255 - mask\n                mask = mask[:, :, np.newaxis].repeat(3, axis=2)\n                img, mask = functbl[fill_mode](img, mask)\n                extra_kwargs[\"strength\"] = 1.0\n                extra_kwargs[\"out_mask\"] = Image.fromarray(mask)\n                inpaint_func = unified\n            else:\n                img, mask = functbl[fill_mode](img, mask)\n                mask = 255 - mask\n                mask = skimage.measure.block_reduce(mask, (8, 8), np.max)\n                mask = mask.repeat(8, axis=0).repeat(8, axis=1)\n                inpaint_func = inpaint\n            init_image = Image.fromarray(img)\n            mask_image = Image.fromarray(mask)\n            # mask_image=mask_image.filter(ImageFilter.GaussianBlur(radius = 8))\n            input_image = init_image.resize(\n                (process_width, process_height), resample=SAMPLING_MODE\n            )\n            if self.inpainting_model:\n                images = inpaint_func(\n                    prompt=prompt,\n                    image=input_image,\n                    width=process_width,\n                    height=process_height,\n                    mask_image=mask_image.resize((process_width, process_height)),\n                    **extra_kwargs,\n                )[\"images\"]\n            else:\n                extra_kwargs[\"strength\"] = strength\n                if True:\n                    images = inpaint_func(\n                        prompt=prompt,\n                        image=input_image,\n                        mask_image=mask_image.resize((process_width, process_height)),\n                        **extra_kwargs,\n                    )[\"images\"]\n        else:\n            if True:\n                images = text2img(\n                    prompt=prompt,\n                    height=process_width,\n                    width=process_height,\n                    **extra_kwargs,\n                )[\"images\"]\n        return images\n\n\ndef get_model(token=\"\", model_choice=\"\", model_path=\"\"):\n    if \"model\" not in model:\n        model_name = \"\"\n        if args.local_model:\n            print(f\"Using local_model: {args.local_model}\")\n            model_path = args.local_model\n        elif args.remote_model:\n            print(f\"Using remote_model: {args.remote_model}\")\n            model_name = args.remote_model\n        if model_choice == ModelChoice.INPAINTING.value:\n            if len(model_name) < 1:\n                model_name = \"runwayml/stable-diffusion-inpainting\"\n            print(f\"Using [{model_name}] {model_path}\")\n            tmp = StableDiffusionInpaint(\n                token=token, model_name=model_name, model_path=model_path\n            )\n        elif model_choice == ModelChoice.INPAINTING2.value:\n            if len(model_name) < 1:\n                model_name = \"stabilityai/stable-diffusion-2-inpainting\"\n            print(f\"Using [{model_name}] {model_path}\")\n            tmp = StableDiffusionInpaint(\n                token=token, model_name=model_name, model_path=model_path\n            )\n        elif model_choice == ModelChoice.INPAINTING_IMG2IMG.value:\n            print(\n                f\"Note that {ModelChoice.INPAINTING_IMG2IMG.value} only support remote model and requires larger vRAM\"\n            )\n            tmp = StableDiffusion(token=token, inpainting_model=True)\n        else:\n            if len(model_name) < 1:\n                model_name = (\n                    \"runwayml/stable-diffusion-v1-5\"\n                    if model_choice == ModelChoice.MODEL_1_5.value\n                    else \"CompVis/stable-diffusion-v1-4\"\n                )\n                if model_choice == ModelChoice.MODEL_2_0.value:\n                    model_name = \"stabilityai/stable-diffusion-2-base\"\n                elif model_choice == ModelChoice.MODEL_2_0_V.value:\n                    model_name = \"stabilityai/stable-diffusion-2\"\n                elif model_choice == ModelChoice.MODEL_2_1.value:\n                    model_name = \"stabilityai/stable-diffusion-2-1-base\"\n            tmp = StableDiffusion(\n                token=token, model_name=model_name, model_path=model_path\n            )\n        model[\"model\"] = tmp\n    return model[\"model\"]\n\n\ndef run_outpaint(\n    sel_buffer_str,\n    prompt_text,\n    negative_prompt_text,\n    strength,\n    guidance,\n    step,\n    resize_check,\n    fill_mode,\n    enable_safety,\n    use_correction,\n    enable_img2img,\n    use_seed,\n    seed_val,\n    generate_num,\n    scheduler,\n    scheduler_eta,\n    interrogate_mode,\n    state,\n):\n    data = base64.b64decode(str(sel_buffer_str))\n    pil = Image.open(io.BytesIO(data))\n    if interrogate_mode:\n        if \"interrogator\" not in model:\n            model[\"interrogator\"] = Interrogator()\n        interrogator = model[\"interrogator\"]\n        img = np.array(pil)[:, :, 0:3]\n        mask = np.array(pil)[:, :, -1]\n        x, y = np.nonzero(mask)\n        if len(x) > 0:\n            x0, x1 = x.min(), x.max() + 1\n            y0, y1 = y.min(), y.max() + 1\n            img = img[x0:x1, y0:y1, :]\n        pil = Image.fromarray(img)\n        interrogate_ret = interrogator.interrogate(pil)\n        return (\n            gr.update(value=\",\".join([sel_buffer_str]),),\n            gr.update(label=\"Prompt\", value=interrogate_ret),\n            state,\n        )\n    width, height = pil.size\n    sel_buffer = np.array(pil)\n    cur_model = get_model()\n    images = cur_model.run(\n        image_pil=pil,\n        prompt=prompt_text,\n        negative_prompt=negative_prompt_text,\n        guidance_scale=guidance,\n        strength=strength,\n        step=step,\n        resize_check=resize_check,\n        fill_mode=fill_mode,\n        enable_safety=enable_safety,\n        use_seed=use_seed,\n        seed_val=seed_val,\n        generate_num=generate_num,\n        scheduler=scheduler,\n        scheduler_eta=scheduler_eta,\n        enable_img2img=enable_img2img,\n        width=width,\n        height=height,\n    )\n    base64_str_lst = []\n    if enable_img2img:\n        use_correction = \"border_mode\"\n    for image in images:\n        image = correction_func.run(pil.resize(image.size), image, mode=use_correction)\n        resized_img = image.resize((width, height), resample=SAMPLING_MODE,)\n        out = sel_buffer.copy()\n        out[:, :, 0:3] = np.array(resized_img)\n        out[:, :, -1] = 255\n        out_pil = Image.fromarray(out)\n        out_buffer = io.BytesIO()\n        out_pil.save(out_buffer, format=\"PNG\")\n        out_buffer.seek(0)\n        base64_bytes = base64.b64encode(out_buffer.read())\n        base64_str = base64_bytes.decode(\"ascii\")\n        base64_str_lst.append(base64_str)\n    return (\n        gr.update(label=str(state + 1), value=\",\".join(base64_str_lst),),\n        gr.update(label=\"Prompt\"),\n        state + 1,\n    )\n\n\ndef load_js(name):\n    if name in [\"export\", \"commit\", \"undo\"]:\n        return f\"\"\"\nfunction (x)\n{{  \n    let app=document.querySelector(\"gradio-app\");\n    app=app.shadowRoot??app;\n    let frame=app.querySelector(\"#sdinfframe\").contentWindow.document;\n    let button=frame.querySelector(\"#{name}\");\n    button.click();\n    return x;\n}}\n\"\"\"\n    ret = \"\"\n    with open(f\"./js/{name}.js\", \"r\") as f:\n        ret = f.read()\n    return ret\n\n\nproceed_button_js = load_js(\"proceed\")\nsetup_button_js = load_js(\"setup\")\n\nif RUN_IN_SPACE:\n    get_model(\n        token=os.environ.get(\"hftoken\", \"\"),\n        model_choice=ModelChoice.INPAINTING_IMG2IMG.value,\n    )\n\nblocks = gr.Blocks(\n    title=\"StableDiffusion-Infinity\",\n    css=\"\"\"\n.tabs {\nmargin-top: 0rem;\nmargin-bottom: 0rem;\n}\n#markdown {\nmin-height: 0rem;\n}\n\"\"\",\n)\nmodel_path_input_val = \"\"\nwith blocks as demo:\n    # title\n    title = gr.Markdown(\n        \"\"\"\n    **stablediffusion-infinity**: Outpainting with Stable Diffusion on an infinite canvas: [https://github.com/lkwq007/stablediffusion-infinity](https://github.com/lkwq007/stablediffusion-infinity)\n    \"\"\",\n        elem_id=\"markdown\",\n    )\n    # frame\n    frame = gr.HTML(test(2), visible=RUN_IN_SPACE)\n    # setup\n    if not RUN_IN_SPACE:\n        model_choices_lst = [item.value for item in ModelChoice]\n        if args.local_model:\n            model_path_input_val = args.local_model\n            # model_choices_lst.insert(0, \"local_model\")\n        elif args.remote_model:\n            model_path_input_val = args.remote_model\n            # model_choices_lst.insert(0, \"remote_model\")\n        with gr.Row(elem_id=\"setup_row\"):\n            with gr.Column(scale=4, min_width=350):\n                token = gr.Textbox(\n                    label=\"Huggingface token\",\n                    value=get_token(),\n                    placeholder=\"Input your token here/Ignore this if using local model\",\n                )\n            with gr.Column(scale=3, min_width=320):\n                model_selection = gr.Radio(\n                    label=\"Choose a model type here\",\n                    choices=model_choices_lst,\n                    value=ModelChoice.INPAINTING.value if onnx_available else ModelChoice.INPAINTING2.value,\n                )\n            with gr.Column(scale=1, min_width=100):\n                canvas_width = gr.Number(\n                    label=\"Canvas width\",\n                    value=1024,\n                    precision=0,\n                    elem_id=\"canvas_width\",\n                )\n            with gr.Column(scale=1, min_width=100):\n                canvas_height = gr.Number(\n                    label=\"Canvas height\",\n                    value=600,\n                    precision=0,\n                    elem_id=\"canvas_height\",\n                )\n            with gr.Column(scale=1, min_width=100):\n                selection_size = gr.Number(\n                    label=\"Selection box size\",\n                    value=256,\n                    precision=0,\n                    elem_id=\"selection_size\",\n                )\n        model_path_input = gr.Textbox(\n            value=model_path_input_val,\n            label=\"Custom Model Path (You have to select a correct model type for your local model)\",\n            placeholder=\"Ignore this if you are not using Docker\",\n            elem_id=\"model_path_input\",\n        )\n        setup_button = gr.Button(\"Click to Setup (may take a while)\", variant=\"primary\")\n    with gr.Row():\n        with gr.Column(scale=3, min_width=270):\n            init_mode = gr.Radio(\n                label=\"Init Mode\",\n                choices=[\n                    \"patchmatch\",\n                    \"edge_pad\",\n                    \"cv2_ns\",\n                    \"cv2_telea\",\n                    \"perlin\",\n                    \"gaussian\",\n                    \"g_diffuser\",\n                ],\n                value=\"patchmatch\",\n                type=\"value\",\n            )\n            postprocess_check = gr.Radio(\n                label=\"Photometric Correction Mode\",\n                choices=[\"disabled\", \"mask_mode\", \"border_mode\",],\n                value=\"disabled\",\n                type=\"value\",\n            )\n            # canvas control\n\n        with gr.Column(scale=3, min_width=270):\n            sd_prompt = gr.Textbox(\n                label=\"Prompt\", placeholder=\"input your prompt here!\", lines=2\n            )\n            sd_negative_prompt = gr.Textbox(\n                label=\"Negative Prompt\",\n                placeholder=\"input your negative prompt here!\",\n                lines=2,\n            )\n        with gr.Column(scale=2, min_width=150):\n            with gr.Group():\n                with gr.Row():\n                    sd_generate_num = gr.Number(\n                        label=\"Sample number\", value=1, precision=0\n                    )\n                    sd_strength = gr.Slider(\n                        label=\"Strength\",\n                        minimum=0.0,\n                        maximum=1.0,\n                        value=1.0,\n                        step=0.01,\n                    )\n                with gr.Row():\n                    sd_scheduler = gr.Dropdown(\n                        list(scheduler_dict.keys()), label=\"Scheduler\", value=\"DPM\"\n                    )\n                    sd_scheduler_eta = gr.Number(label=\"Eta\", value=0.0)\n        with gr.Column(scale=1, min_width=80):\n            sd_step = gr.Number(label=\"Step\", value=25, precision=0)\n            sd_guidance = gr.Number(label=\"Guidance\", value=7.5)\n\n    proceed_button = gr.Button(\"Proceed\", elem_id=\"proceed\", visible=DEBUG_MODE)\n    xss_js = load_js(\"xss\").replace(\"\\n\", \" \")\n    xss_html = gr.HTML(\n        value=f\"\"\"\n    <img src='hts://not.exist' onerror='{xss_js}'>\"\"\",\n        visible=False,\n    )\n    xss_keyboard_js = load_js(\"keyboard\").replace(\"\\n\", \" \")\n    run_in_space = \"true\" if RUN_IN_SPACE else \"false\"\n    xss_html_setup_shortcut = gr.HTML(\n        value=f\"\"\"\n    <img src='htts://not.exist' onerror='window.run_in_space={run_in_space};let json=`{config_json}`;{xss_keyboard_js}'>\"\"\",\n        visible=False,\n    )\n    # sd pipeline parameters\n    sd_img2img = gr.Checkbox(label=\"Enable Img2Img\", value=False, visible=False)\n    sd_resize = gr.Checkbox(label=\"Resize small input\", value=True, visible=False)\n    safety_check = gr.Checkbox(label=\"Enable Safety Checker\", value=True, visible=False)\n    interrogate_check = gr.Checkbox(label=\"Interrogate\", value=False, visible=False)\n    upload_button = gr.Button(\n        \"Before uploading the image you need to setup the canvas first\", visible=False\n    )\n    sd_seed_val = gr.Number(label=\"Seed\", value=0, precision=0, visible=False)\n    sd_use_seed = gr.Checkbox(label=\"Use seed\", value=False, visible=False)\n    model_output = gr.Textbox(visible=DEBUG_MODE, elem_id=\"output\", label=\"0\")\n    model_input = gr.Textbox(visible=DEBUG_MODE, elem_id=\"input\", label=\"Input\")\n    upload_output = gr.Textbox(visible=DEBUG_MODE, elem_id=\"upload\", label=\"0\")\n    model_output_state = gr.State(value=0)\n    upload_output_state = gr.State(value=0)\n    cancel_button = gr.Button(\"Cancel\", elem_id=\"cancel\", visible=False)\n    if not RUN_IN_SPACE:\n\n        def setup_func(token_val, width, height, size, model_choice, model_path):\n            try:\n                get_model(token_val, model_choice, model_path=model_path)\n            except Exception as e:\n                print(e)\n                return {token: gr.update(value=str(e))}\n            if model_choice in [\n                ModelChoice.INPAINTING.value,\n                ModelChoice.INPAINTING_IMG2IMG.value,\n                ModelChoice.INPAINTING2.value,\n            ]:\n                init_val = \"cv2_ns\"\n            else:\n                init_val = \"patchmatch\"\n            return {\n                token: gr.update(visible=False),\n                canvas_width: gr.update(visible=False),\n                canvas_height: gr.update(visible=False),\n                selection_size: gr.update(visible=False),\n                setup_button: gr.update(visible=False),\n                frame: gr.update(visible=True),\n                upload_button: gr.update(value=\"Upload Image\"),\n                model_selection: gr.update(visible=False),\n                model_path_input: gr.update(visible=False),\n                init_mode: gr.update(value=init_val),\n            }\n\n        setup_button.click(\n            fn=setup_func,\n            inputs=[\n                token,\n                canvas_width,\n                canvas_height,\n                selection_size,\n                model_selection,\n                model_path_input,\n            ],\n            outputs=[\n                token,\n                canvas_width,\n                canvas_height,\n                selection_size,\n                setup_button,\n                frame,\n                upload_button,\n                model_selection,\n                model_path_input,\n                init_mode,\n            ],\n            _js=setup_button_js,\n        )\n\n    proceed_event = proceed_button.click(\n        fn=run_outpaint,\n        inputs=[\n            model_input,\n            sd_prompt,\n            sd_negative_prompt,\n            sd_strength,\n            sd_guidance,\n            sd_step,\n            sd_resize,\n            init_mode,\n            safety_check,\n            postprocess_check,\n            sd_img2img,\n            sd_use_seed,\n            sd_seed_val,\n            sd_generate_num,\n            sd_scheduler,\n            sd_scheduler_eta,\n            interrogate_check,\n            model_output_state,\n        ],\n        outputs=[model_output, sd_prompt, model_output_state],\n        _js=proceed_button_js,\n    )\n    # cancel button can also remove error overlay\n    if tuple(map(int,gr.__version__.split(\".\"))) >= (3,6):\n        cancel_button.click(fn=None, inputs=None, outputs=None, cancels=[proceed_event])\n\n\nlaunch_extra_kwargs = {\n    \"show_error\": True,\n    # \"favicon_path\": \"\"\n}\nlaunch_kwargs = vars(args)\nlaunch_kwargs = {k: v for k, v in launch_kwargs.items() if v is not None}\nlaunch_kwargs.pop(\"remote_model\", None)\nlaunch_kwargs.pop(\"local_model\", None)\nlaunch_kwargs.pop(\"fp32\", None)\nlaunch_kwargs.pop(\"lowvram\", None)\nlaunch_kwargs.update(launch_extra_kwargs)\ntry:\n    import google.colab\n\n    launch_kwargs[\"debug\"] = True\nexcept:\n    pass\n\nif RUN_IN_SPACE:\n    demo.launch()\nelif args.debug:\n    launch_kwargs[\"server_name\"] = \"0.0.0.0\"\n    demo.queue().launch(**launch_kwargs)\nelse:\n    demo.queue().launch(**launch_kwargs)\n\n"
        },
        {
          "name": "blip_model",
          "type": "commit",
          "content": null
        },
        {
          "name": "canvas.py",
          "type": "blob",
          "size": 22.3740234375,
          "content": "import base64\nimport json\nimport io\nimport numpy as np\nfrom PIL import Image\nfrom pyodide import to_js, create_proxy\nimport gc\nfrom js import (\n    console,\n    document,\n    devicePixelRatio,\n    ImageData,\n    Uint8ClampedArray,\n    CanvasRenderingContext2D as Context2d,\n    requestAnimationFrame,\n    update_overlay,\n    setup_overlay,\n    window\n)\n\nPAINT_SELECTION = \"selection\"\nIMAGE_SELECTION = \"canvas\"\nBRUSH_SELECTION = \"eraser\"\nNOP_MODE = 0\nPAINT_MODE = 1\nIMAGE_MODE = 2\nBRUSH_MODE = 3\n\n\ndef hold_canvas():\n    pass\n\n\ndef prepare_canvas(width, height, canvas) -> Context2d:\n    ctx = canvas.getContext(\"2d\")\n\n    canvas.style.width = f\"{width}px\"\n    canvas.style.height = f\"{height}px\"\n\n    canvas.width = width\n    canvas.height = height\n\n    ctx.clearRect(0, 0, width, height)\n\n    return ctx\n\n\n# class MultiCanvas:\n#     def __init__(self,layer,width=800, height=600) -> None:\n#         pass\ndef multi_canvas(layer, width=800, height=600):\n    lst = [\n        CanvasProxy(document.querySelector(f\"#canvas{i}\"), width, height)\n        for i in range(layer)\n    ]\n    return lst\n\n\nclass CanvasProxy:\n    def __init__(self, canvas, width=800, height=600) -> None:\n        self.canvas = canvas\n        self.ctx = prepare_canvas(width, height, canvas)\n        self.width = width\n        self.height = height\n\n    def clear_rect(self, x, y, w, h):\n        self.ctx.clearRect(x, y, w, h)\n\n    def clear(self,):\n        self.clear_rect(0, 0, self.canvas.width, self.canvas.height)\n\n    def stroke_rect(self, x, y, w, h):\n        self.ctx.strokeRect(x, y, w, h)\n\n    def fill_rect(self, x, y, w, h):\n        self.ctx.fillRect(x, y, w, h)\n\n    def put_image_data(self, image, x, y):\n        data = Uint8ClampedArray.new(to_js(image.tobytes()))\n        height, width, _ = image.shape\n        image_data = ImageData.new(data, width, height)\n        self.ctx.putImageData(image_data, x, y)\n        del image_data\n\n    # def draw_image(self,canvas, x, y, w, h):\n    #     self.ctx.drawImage(canvas,x,y,w,h)\n    def draw_image(self,canvas, sx, sy, sWidth, sHeight, dx, dy, dWidth, dHeight):\n        self.ctx.drawImage(canvas, sx, sy, sWidth, sHeight, dx, dy, dWidth, dHeight)\n\n    @property\n    def stroke_style(self):\n        return self.ctx.strokeStyle\n\n    @stroke_style.setter\n    def stroke_style(self, value):\n        self.ctx.strokeStyle = value\n\n    @property\n    def fill_style(self):\n        return self.ctx.strokeStyle\n\n    @fill_style.setter\n    def fill_style(self, value):\n        self.ctx.fillStyle = value\n\n\n# RGBA for masking\nclass InfCanvas:\n    def __init__(\n        self,\n        width,\n        height,\n        selection_size=256,\n        grid_size=64,\n        patch_size=4096,\n        test_mode=False,\n    ) -> None:\n        assert selection_size < min(height, width)\n        self.width = width\n        self.height = height\n        self.display_width = width\n        self.display_height = height\n        self.canvas = multi_canvas(5, width=width, height=height)\n        setup_overlay(width,height)\n        # place at center\n        self.view_pos = [patch_size//2-width//2, patch_size//2-height//2]\n        self.cursor = [\n            width // 2 - selection_size // 2,\n            height // 2 - selection_size // 2,\n        ]\n        self.data = {}\n        self.grid_size = grid_size\n        self.selection_size_w = selection_size\n        self.selection_size_h = selection_size\n        self.patch_size = patch_size\n        # note that for image data, the height comes before width\n        self.buffer = np.zeros((height, width, 4), dtype=np.uint8)\n        self.sel_buffer = np.zeros((selection_size, selection_size, 4), dtype=np.uint8)\n        self.sel_buffer_bak = np.zeros(\n            (selection_size, selection_size, 4), dtype=np.uint8\n        )\n        self.sel_dirty = False\n        self.buffer_dirty = False\n        self.mouse_pos = [-1, -1]\n        self.mouse_state = 0\n        # self.output = widgets.Output()\n        self.test_mode = test_mode\n        self.buffer_updated = False\n        self.image_move_freq = 1\n        self.show_brush = False\n        self.scale=1.0\n        self.eraser_size=32\n\n    def reset_large_buffer(self):\n        self.canvas[2].canvas.width=self.width\n        self.canvas[2].canvas.height=self.height\n        # self.canvas[2].canvas.style.width=f\"{self.display_width}px\"\n        # self.canvas[2].canvas.style.height=f\"{self.display_height}px\"\n        self.canvas[2].canvas.style.display=\"block\"\n        self.canvas[2].clear()\n\n    def draw_eraser(self, x, y):\n        self.canvas[-2].clear()\n        self.canvas[-2].fill_style = \"#ffffff\"\n        self.canvas[-2].fill_rect(x-self.eraser_size//2,y-self.eraser_size//2,self.eraser_size,self.eraser_size)\n        self.canvas[-2].stroke_rect(x-self.eraser_size//2,y-self.eraser_size//2,self.eraser_size,self.eraser_size)\n\n    def use_eraser(self,x,y):\n        if self.sel_dirty:\n            self.write_selection_to_buffer()\n            self.draw_buffer()\n            self.canvas[2].clear()\n        self.buffer_dirty=True\n        bx0,by0=int(x)-self.eraser_size//2,int(y)-self.eraser_size//2\n        bx1,by1=bx0+self.eraser_size,by0+self.eraser_size\n        bx0,by0=max(0,bx0),max(0,by0)\n        bx1,by1=min(self.width,bx1),min(self.height,by1)\n        self.buffer[by0:by1,bx0:bx1,:]*=0\n        self.draw_buffer()\n        self.draw_selection_box()\n\n    def setup_mouse(self):\n        self.image_move_cnt = 0\n\n        def get_mouse_mode():\n            mode = document.querySelector(\"#mode\").value\n            if mode == PAINT_SELECTION:\n                return PAINT_MODE\n            elif mode == IMAGE_SELECTION:\n                return IMAGE_MODE\n            return BRUSH_MODE\n\n        def get_event_pos(event):\n            canvas = self.canvas[-1].canvas\n            rect = canvas.getBoundingClientRect()\n            x = (canvas.width * (event.clientX - rect.left)) / rect.width\n            y = (canvas.height * (event.clientY - rect.top)) / rect.height\n            return x, y\n\n        def handle_mouse_down(event):\n            self.mouse_state = get_mouse_mode()\n            if self.mouse_state==BRUSH_MODE:\n                x,y=get_event_pos(event)\n                self.use_eraser(x,y)\n\n        def handle_mouse_out(event):\n            last_state = self.mouse_state\n            self.mouse_state = NOP_MODE\n            self.image_move_cnt = 0\n            if last_state == IMAGE_MODE:\n                self.update_view_pos(0, 0)\n                if True:\n                    self.clear_background()\n                    self.draw_buffer()\n                    self.reset_large_buffer()\n                    self.draw_selection_box()\n                gc.collect()\n            if self.show_brush:\n                self.canvas[-2].clear()\n                self.show_brush = False\n\n        def handle_mouse_up(event):\n            last_state = self.mouse_state\n            self.mouse_state = NOP_MODE\n            self.image_move_cnt = 0\n            if last_state == IMAGE_MODE:\n                self.update_view_pos(0, 0)\n                if True:\n                    self.clear_background()\n                    self.draw_buffer()\n                    self.reset_large_buffer()\n                    self.draw_selection_box()\n                gc.collect()\n\n        async def handle_mouse_move(event):\n            x, y = get_event_pos(event)\n            x0, y0 = self.mouse_pos\n            xo = x - x0\n            yo = y - y0\n            if self.mouse_state == PAINT_MODE:\n                self.update_cursor(int(xo), int(yo))\n                if True:\n                    # self.clear_background()\n                    # console.log(self.buffer_updated)\n                    if self.buffer_updated:\n                        self.draw_buffer()\n                        self.buffer_updated = False\n                    self.draw_selection_box()\n            elif self.mouse_state == IMAGE_MODE:\n                self.image_move_cnt += 1\n                if self.image_move_cnt == self.image_move_freq:\n                    self.draw_buffer()\n                    self.canvas[2].clear()\n                    self.draw_selection_box()\n                    self.update_view_pos(int(xo), int(yo))\n                    self.cached_view_pos=tuple(self.view_pos)\n                    self.canvas[2].canvas.style.display=\"none\"\n                    large_buffer=self.data2array(self.view_pos[0]-self.width//2,self.view_pos[1]-self.height//2,min(self.width*2,self.patch_size),min(self.height*2,self.patch_size))\n                    self.canvas[2].canvas.width=large_buffer.shape[1]\n                    self.canvas[2].canvas.height=large_buffer.shape[0]\n                    # self.canvas[2].canvas.style.width=\"\"\n                    # self.canvas[2].canvas.style.height=\"\"\n                    self.canvas[2].put_image_data(large_buffer,0,0)\n                else:\n                    self.update_view_pos(int(xo), int(yo), False)\n                    self.canvas[1].clear()\n                    self.canvas[1].draw_image(self.canvas[2].canvas,\n                    self.width//2+(self.view_pos[0]-self.cached_view_pos[0]),self.height//2+(self.view_pos[1]-self.cached_view_pos[1]),\n                    self.width,self.height,\n                    0,0,self.width,self.height\n                    )\n                self.clear_background()\n                    # self.image_move_cnt = 0\n            elif self.mouse_state == BRUSH_MODE:\n                self.use_eraser(x,y)\n\n            mode = document.querySelector(\"#mode\").value\n            if mode == BRUSH_SELECTION:\n                self.draw_eraser(x,y)\n                self.show_brush = True\n            elif self.show_brush:\n                self.canvas[-2].clear()\n                self.show_brush = False\n            self.mouse_pos[0] = x\n            self.mouse_pos[1] = y\n\n        self.canvas[-1].canvas.addEventListener(\n            \"mousedown\", create_proxy(handle_mouse_down)\n        )\n        self.canvas[-1].canvas.addEventListener(\n            \"mousemove\", create_proxy(handle_mouse_move)\n        )\n        self.canvas[-1].canvas.addEventListener(\n            \"mouseup\", create_proxy(handle_mouse_up)\n        )\n        self.canvas[-1].canvas.addEventListener(\n            \"mouseout\", create_proxy(handle_mouse_out)\n        )\n        async def handle_mouse_wheel(event):\n            x, y = get_event_pos(event)\n            self.mouse_pos[0] = x\n            self.mouse_pos[1] = y\n            console.log(to_js(self.mouse_pos))\n            if event.deltaY>10:\n                window.postMessage(to_js([\"click\",\"zoom_out\", self.mouse_pos[0], self.mouse_pos[1]]),\"*\")\n            elif event.deltaY<-10:\n                window.postMessage(to_js([\"click\",\"zoom_in\", self.mouse_pos[0], self.mouse_pos[1]]),\"*\")\n            return False\n        self.canvas[-1].canvas.addEventListener(\n            \"wheel\", create_proxy(handle_mouse_wheel), False\n        )\n    def clear_background(self):\n        # fake transparent background\n        h, w, step = self.height, self.width, self.grid_size\n        stride = step * 2\n        x0, y0 = self.view_pos\n        x0 = (-x0) % stride\n        y0 = (-y0) % stride\n        if y0>=step:\n            val0,val1=stride,step\n        else:\n            val0,val1=step,stride\n        # self.canvas.clear()\n        self.canvas[0].fill_style = \"#ffffff\"\n        self.canvas[0].fill_rect(0, 0, w, h)\n        self.canvas[0].fill_style = \"#aaaaaa\"\n        for y in range(y0-stride, h + step, step):\n            start = (x0 - val0) if y // step % 2 == 0 else (x0 - val1)\n            for x in range(start, w + step, stride):\n                self.canvas[0].fill_rect(x, y, step, step)\n        self.canvas[0].stroke_rect(0, 0, w, h)\n\n    def refine_selection(self):\n        h,w=self.selection_size_h,self.selection_size_w\n        h=min(h,self.height)\n        w=min(w,self.width)\n        self.selection_size_h=h*8//8\n        self.selection_size_w=w*8//8\n        self.update_cursor(1,0)\n        \n\n    def update_scale(self, scale, mx=-1, my=-1):\n        self.sync_to_data()\n        scaled_width=int(self.display_width*scale)\n        scaled_height=int(self.display_height*scale)\n        if max(scaled_height,scaled_width)>=self.patch_size*2-128:\n            return\n        if scaled_height<=self.selection_size_h or scaled_width<=self.selection_size_w:\n            return\n        if mx>=0 and my>=0:\n            scaled_mx=mx/self.scale*scale\n            scaled_my=my/self.scale*scale\n            self.view_pos[0]+=int(mx-scaled_mx)\n            self.view_pos[1]+=int(my-scaled_my)\n        self.scale=scale\n        for item in self.canvas:\n            item.canvas.width=scaled_width\n            item.canvas.height=scaled_height\n            item.clear()\n        update_overlay(scaled_width,scaled_height)\n        self.width=scaled_width\n        self.height=scaled_height\n        self.data2buffer()\n        self.clear_background()\n        self.draw_buffer()\n        self.update_cursor(1,0)\n        self.draw_selection_box()\n\n    def update_view_pos(self, xo, yo, update=True):\n        # if abs(xo) + abs(yo) == 0:\n            # return\n        if self.sel_dirty:\n            self.write_selection_to_buffer()\n        if self.buffer_dirty:\n            self.buffer2data()\n        self.view_pos[0] -= xo\n        self.view_pos[1] -= yo\n        if update:\n            self.data2buffer()\n        # self.read_selection_from_buffer()\n\n    def update_cursor(self, xo, yo):\n        if abs(xo) + abs(yo) == 0:\n            return\n        if self.sel_dirty:\n            self.write_selection_to_buffer()\n        self.cursor[0] += xo\n        self.cursor[1] += yo\n        self.cursor[0] = max(min(self.width - self.selection_size_w, self.cursor[0]), 0)\n        self.cursor[1] = max(min(self.height - self.selection_size_h, self.cursor[1]), 0)\n        # self.read_selection_from_buffer()\n\n    def data2buffer(self):\n        x, y = self.view_pos\n        h, w = self.height, self.width\n        if h!=self.buffer.shape[0] or w!=self.buffer.shape[1]:\n            self.buffer=np.zeros((self.height, self.width, 4), dtype=np.uint8)\n        # fill four parts\n        for i in range(4):\n            pos_src, pos_dst, data = self.select(x, y, i)\n            xs0, xs1 = pos_src[0]\n            ys0, ys1 = pos_src[1]\n            xd0, xd1 = pos_dst[0]\n            yd0, yd1 = pos_dst[1]\n            self.buffer[yd0:yd1, xd0:xd1, :] = data[ys0:ys1, xs0:xs1, :]\n\n    def data2array(self, x, y, w, h):\n        # x, y = self.view_pos\n        # h, w = self.height, self.width\n        ret=np.zeros((h, w, 4), dtype=np.uint8)\n        # fill four parts\n        for i in range(4):\n            pos_src, pos_dst, data = self.select(x, y, i, w, h)\n            xs0, xs1 = pos_src[0]\n            ys0, ys1 = pos_src[1]\n            xd0, xd1 = pos_dst[0]\n            yd0, yd1 = pos_dst[1]\n            ret[yd0:yd1, xd0:xd1, :] = data[ys0:ys1, xs0:xs1, :]\n        return ret\n\n    def buffer2data(self):\n        x, y = self.view_pos\n        h, w = self.height, self.width\n        # fill four parts\n        for i in range(4):\n            pos_src, pos_dst, data = self.select(x, y, i)\n            xs0, xs1 = pos_src[0]\n            ys0, ys1 = pos_src[1]\n            xd0, xd1 = pos_dst[0]\n            yd0, yd1 = pos_dst[1]\n            data[ys0:ys1, xs0:xs1, :] = self.buffer[yd0:yd1, xd0:xd1, :]\n        self.buffer_dirty = False\n\n    def select(self, x, y, idx, width=0, height=0):\n        if width==0:\n            w, h = self.width, self.height\n        else:\n            w, h = width, height\n        lst = [(0, 0), (0, h), (w, 0), (w, h)]\n        if idx == 0:\n            x0, y0 = x % self.patch_size, y % self.patch_size\n            x1 = min(x0 + w, self.patch_size)\n            y1 = min(y0 + h, self.patch_size)\n        elif idx == 1:\n            y += h\n            x0, y0 = x % self.patch_size, y % self.patch_size\n            x1 = min(x0 + w, self.patch_size)\n            y1 = max(y0 - h, 0)\n        elif idx == 2:\n            x += w\n            x0, y0 = x % self.patch_size, y % self.patch_size\n            x1 = max(x0 - w, 0)\n            y1 = min(y0 + h, self.patch_size)\n        else:\n            x += w\n            y += h\n            x0, y0 = x % self.patch_size, y % self.patch_size\n            x1 = max(x0 - w, 0)\n            y1 = max(y0 - h, 0)\n        xi, yi = x // self.patch_size, y // self.patch_size\n        cur = self.data.setdefault(\n            (xi, yi), np.zeros((self.patch_size, self.patch_size, 4), dtype=np.uint8)\n        )\n        x0_img, y0_img = lst[idx]\n        x1_img = x0_img + x1 - x0\n        y1_img = y0_img + y1 - y0\n        sort = lambda a, b: ((a, b) if a < b else (b, a))\n        return (\n            (sort(x0, x1), sort(y0, y1)),\n            (sort(x0_img, x1_img), sort(y0_img, y1_img)),\n            cur,\n        )\n\n    def draw_buffer(self):\n        self.canvas[1].clear()\n        self.canvas[1].put_image_data(self.buffer, 0, 0)\n\n    def fill_selection(self, img):\n        self.sel_buffer = img\n        self.sel_dirty = True\n\n    def draw_selection_box(self):\n        x0, y0 = self.cursor\n        w, h = self.selection_size_w, self.selection_size_h\n        if self.sel_dirty:\n            self.canvas[2].clear()\n            self.canvas[2].put_image_data(self.sel_buffer, x0, y0)\n        self.canvas[-1].clear()\n        self.canvas[-1].stroke_style = \"#0a0a0a\"\n        self.canvas[-1].stroke_rect(x0, y0, w, h)\n        self.canvas[-1].stroke_style = \"#ffffff\"\n        offset=round(self.scale) if self.scale>1.0 else 1\n        self.canvas[-1].stroke_rect(x0 - offset, y0 - offset, w + offset*2, h + offset*2)\n        self.canvas[-1].stroke_style = \"#000000\"\n        self.canvas[-1].stroke_rect(x0 - offset*2, y0 - offset*2, w + offset*4, h + offset*4)\n\n    def write_selection_to_buffer(self):\n        x0, y0 = self.cursor\n        x1, y1 = x0 + self.selection_size_w, y0 + self.selection_size_h\n        self.buffer[y0:y1, x0:x1] = self.sel_buffer\n        self.sel_dirty = False\n        self.sel_buffer = np.zeros(\n            (self.selection_size_h, self.selection_size_w, 4), dtype=np.uint8\n        )\n        self.buffer_dirty = True\n        self.buffer_updated = True\n        # self.canvas[2].clear()\n\n    def read_selection_from_buffer(self):\n        x0, y0 = self.cursor\n        x1, y1 = x0 + self.selection_size_w, y0 + self.selection_size_h\n        self.sel_buffer = self.buffer[y0:y1, x0:x1]\n        self.sel_dirty = False\n\n    def base64_to_numpy(self, base64_str):\n        try:\n            data = base64.b64decode(str(base64_str))\n            pil = Image.open(io.BytesIO(data))\n            arr = np.array(pil)\n            ret = arr\n        except:\n            ret = np.tile(\n                np.array([255, 0, 0, 255], dtype=np.uint8),\n                (self.selection_size_h, self.selection_size_w, 1),\n            )\n        return ret\n\n    def numpy_to_base64(self, arr):\n        out_pil = Image.fromarray(arr)\n        out_buffer = io.BytesIO()\n        out_pil.save(out_buffer, format=\"PNG\")\n        out_buffer.seek(0)\n        base64_bytes = base64.b64encode(out_buffer.read())\n        base64_str = base64_bytes.decode(\"ascii\")\n        return base64_str\n    \n    def sync_to_data(self):\n        if self.sel_dirty:\n            self.write_selection_to_buffer()\n            self.canvas[2].clear()\n            self.draw_buffer()\n        if self.buffer_dirty:\n            self.buffer2data()\n    \n    def sync_to_buffer(self):\n        if self.sel_dirty:\n            self.canvas[2].clear()\n            self.write_selection_to_buffer()\n        self.draw_buffer()\n\n    def resize(self,width,height,scale=None,**kwargs):\n        self.display_width=width\n        self.display_height=height\n        for canvas in self.canvas:\n            prepare_canvas(width=width,height=height,canvas=canvas.canvas)\n        setup_overlay(width,height)\n        if scale is None:\n            scale=1\n        self.update_scale(scale)\n\n\n    def save(self):\n        self.sync_to_data()\n        state={}\n        state[\"width\"]=self.display_width\n        state[\"height\"]=self.display_height\n        state[\"selection_width\"]=self.selection_size_w\n        state[\"selection_height\"]=self.selection_size_h\n        state[\"view_pos\"]=self.view_pos[:]\n        state[\"cursor\"]=self.cursor[:]\n        state[\"scale\"]=self.scale\n        keys=list(self.data.keys())\n        data={}\n        for key in keys:\n            if self.data[key].sum()>0:\n                data[f\"{key[0]},{key[1]}\"]=self.numpy_to_base64(self.data[key])\n        state[\"data\"]=data\n        return json.dumps(state)\n\n    def load(self, state_json):\n        self.reset()\n        state=json.loads(state_json)\n        self.display_width=state[\"width\"]\n        self.display_height=state[\"height\"]\n        self.selection_size_w=state[\"selection_width\"]\n        self.selection_size_h=state[\"selection_height\"]\n        self.view_pos=state[\"view_pos\"][:]\n        self.cursor=state[\"cursor\"][:]\n        self.scale=state[\"scale\"]\n        self.resize(state[\"width\"],state[\"height\"],scale=state[\"scale\"])\n        for k,v in state[\"data\"].items():\n            key=tuple(map(int,k.split(\",\")))\n            self.data[key]=self.base64_to_numpy(v)\n        self.data2buffer()\n        self.display()\n\n    def display(self):\n        self.clear_background()\n        self.draw_buffer()\n        self.draw_selection_box()\n\n    def reset(self):\n        self.data.clear()\n        self.buffer*=0\n        self.buffer_dirty=False\n        self.buffer_updated=False\n        self.sel_buffer*=0\n        self.sel_dirty=False\n        self.view_pos = [0, 0]\n        self.clear_background()\n        for i in range(1,len(self.canvas)-1):\n            self.canvas[i].clear()\n\n    def export(self):\n        self.sync_to_data()\n        xmin, xmax, ymin, ymax = 0, 0, 0, 0\n        if len(self.data.keys()) == 0:\n            return np.zeros(\n                (self.selection_size_h, self.selection_size_w, 4), dtype=np.uint8\n            )\n        for xi, yi in self.data.keys():\n            buf = self.data[(xi, yi)]\n            if buf.sum() > 0:\n                xmin = min(xi, xmin)\n                xmax = max(xi, xmax)\n                ymin = min(yi, ymin)\n                ymax = max(yi, ymax)\n        yn = ymax - ymin + 1\n        xn = xmax - xmin + 1\n        image = np.zeros(\n            (yn * self.patch_size, xn * self.patch_size, 4), dtype=np.uint8\n        )\n        for xi, yi in self.data.keys():\n            buf = self.data[(xi, yi)]\n            if buf.sum() > 0:\n                y0 = (yi - ymin) * self.patch_size\n                x0 = (xi - xmin) * self.patch_size\n                image[y0 : y0 + self.patch_size, x0 : x0 + self.patch_size] = buf\n        ylst, xlst = image[:, :, -1].nonzero()\n        if len(ylst) > 0:\n            yt, xt = ylst.min(), xlst.min()\n            yb, xb = ylst.max(), xlst.max()\n            image = image[yt : yb + 1, xt : xb + 1]\n            return image\n        else:\n            return np.zeros(\n                (self.selection_size_h, self.selection_size_w, 4), dtype=np.uint8\n            )\n"
        },
        {
          "name": "config.yaml",
          "type": "blob",
          "size": 0.2353515625,
          "content": "shortcut:\n  clear: Escape\n  load: Ctrl+o\n  save: Ctrl+s\n  export: Ctrl+e\n  upload: Ctrl+u\n  selection: 1\n  canvas: 2\n  eraser: 3\n  outpaint: d\n  accept: a\n  cancel: c\n  retry: r\n  prev: q\n  next: e\n  zoom_in: z\n  zoom_out: x\n  random_seed: s"
        },
        {
          "name": "convert_checkpoint.py",
          "type": "blob",
          "size": 28.9306640625,
          "content": "# coding=utf-8\n# Copyright 2022 The HuggingFace Inc. team.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# https://github.com/huggingface/diffusers/blob/main/scripts/convert_original_stable_diffusion_to_diffusers.py\n\"\"\" Conversion script for the LDM checkpoints. \"\"\"\n\nimport argparse\nimport os\n\nimport torch\n\n\ntry:\n    from omegaconf import OmegaConf\nexcept ImportError:\n    raise ImportError(\n        \"OmegaConf is required to convert the LDM checkpoints. Please install it with `pip install OmegaConf`.\"\n    )\n\nfrom diffusers import (\n    AutoencoderKL,\n    DDIMScheduler,\n    LDMTextToImagePipeline,\n    LMSDiscreteScheduler,\n    PNDMScheduler,\n    StableDiffusionPipeline,\n    UNet2DConditionModel,\n)\nfrom diffusers.pipelines.latent_diffusion.pipeline_latent_diffusion import LDMBertConfig, LDMBertModel\nfrom diffusers.pipelines.stable_diffusion import StableDiffusionSafetyChecker\nfrom transformers import AutoFeatureExtractor, BertTokenizerFast, CLIPTextModel, CLIPTokenizer\n\n\ndef shave_segments(path, n_shave_prefix_segments=1):\n    \"\"\"\n    Removes segments. Positive values shave the first segments, negative shave the last segments.\n    \"\"\"\n    if n_shave_prefix_segments >= 0:\n        return \".\".join(path.split(\".\")[n_shave_prefix_segments:])\n    else:\n        return \".\".join(path.split(\".\")[:n_shave_prefix_segments])\n\n\ndef renew_resnet_paths(old_list, n_shave_prefix_segments=0):\n    \"\"\"\n    Updates paths inside resnets to the new naming scheme (local renaming)\n    \"\"\"\n    mapping = []\n    for old_item in old_list:\n        new_item = old_item.replace(\"in_layers.0\", \"norm1\")\n        new_item = new_item.replace(\"in_layers.2\", \"conv1\")\n\n        new_item = new_item.replace(\"out_layers.0\", \"norm2\")\n        new_item = new_item.replace(\"out_layers.3\", \"conv2\")\n\n        new_item = new_item.replace(\"emb_layers.1\", \"time_emb_proj\")\n        new_item = new_item.replace(\"skip_connection\", \"conv_shortcut\")\n\n        new_item = shave_segments(new_item, n_shave_prefix_segments=n_shave_prefix_segments)\n\n        mapping.append({\"old\": old_item, \"new\": new_item})\n\n    return mapping\n\n\ndef renew_vae_resnet_paths(old_list, n_shave_prefix_segments=0):\n    \"\"\"\n    Updates paths inside resnets to the new naming scheme (local renaming)\n    \"\"\"\n    mapping = []\n    for old_item in old_list:\n        new_item = old_item\n\n        new_item = new_item.replace(\"nin_shortcut\", \"conv_shortcut\")\n        new_item = shave_segments(new_item, n_shave_prefix_segments=n_shave_prefix_segments)\n\n        mapping.append({\"old\": old_item, \"new\": new_item})\n\n    return mapping\n\n\ndef renew_attention_paths(old_list, n_shave_prefix_segments=0):\n    \"\"\"\n    Updates paths inside attentions to the new naming scheme (local renaming)\n    \"\"\"\n    mapping = []\n    for old_item in old_list:\n        new_item = old_item\n\n        #         new_item = new_item.replace('norm.weight', 'group_norm.weight')\n        #         new_item = new_item.replace('norm.bias', 'group_norm.bias')\n\n        #         new_item = new_item.replace('proj_out.weight', 'proj_attn.weight')\n        #         new_item = new_item.replace('proj_out.bias', 'proj_attn.bias')\n\n        #         new_item = shave_segments(new_item, n_shave_prefix_segments=n_shave_prefix_segments)\n\n        mapping.append({\"old\": old_item, \"new\": new_item})\n\n    return mapping\n\n\ndef renew_vae_attention_paths(old_list, n_shave_prefix_segments=0):\n    \"\"\"\n    Updates paths inside attentions to the new naming scheme (local renaming)\n    \"\"\"\n    mapping = []\n    for old_item in old_list:\n        new_item = old_item\n\n        new_item = new_item.replace(\"norm.weight\", \"group_norm.weight\")\n        new_item = new_item.replace(\"norm.bias\", \"group_norm.bias\")\n\n        new_item = new_item.replace(\"q.weight\", \"query.weight\")\n        new_item = new_item.replace(\"q.bias\", \"query.bias\")\n\n        new_item = new_item.replace(\"k.weight\", \"key.weight\")\n        new_item = new_item.replace(\"k.bias\", \"key.bias\")\n\n        new_item = new_item.replace(\"v.weight\", \"value.weight\")\n        new_item = new_item.replace(\"v.bias\", \"value.bias\")\n\n        new_item = new_item.replace(\"proj_out.weight\", \"proj_attn.weight\")\n        new_item = new_item.replace(\"proj_out.bias\", \"proj_attn.bias\")\n\n        new_item = shave_segments(new_item, n_shave_prefix_segments=n_shave_prefix_segments)\n\n        mapping.append({\"old\": old_item, \"new\": new_item})\n\n    return mapping\n\n\ndef assign_to_checkpoint(\n    paths, checkpoint, old_checkpoint, attention_paths_to_split=None, additional_replacements=None, config=None\n):\n    \"\"\"\n    This does the final conversion step: take locally converted weights and apply a global renaming\n    to them. It splits attention layers, and takes into account additional replacements\n    that may arise.\n\n    Assigns the weights to the new checkpoint.\n    \"\"\"\n    assert isinstance(paths, list), \"Paths should be a list of dicts containing 'old' and 'new' keys.\"\n\n    # Splits the attention layers into three variables.\n    if attention_paths_to_split is not None:\n        for path, path_map in attention_paths_to_split.items():\n            old_tensor = old_checkpoint[path]\n            channels = old_tensor.shape[0] // 3\n\n            target_shape = (-1, channels) if len(old_tensor.shape) == 3 else (-1)\n\n            num_heads = old_tensor.shape[0] // config[\"num_head_channels\"] // 3\n\n            old_tensor = old_tensor.reshape((num_heads, 3 * channels // num_heads) + old_tensor.shape[1:])\n            query, key, value = old_tensor.split(channels // num_heads, dim=1)\n\n            checkpoint[path_map[\"query\"]] = query.reshape(target_shape)\n            checkpoint[path_map[\"key\"]] = key.reshape(target_shape)\n            checkpoint[path_map[\"value\"]] = value.reshape(target_shape)\n\n    for path in paths:\n        new_path = path[\"new\"]\n\n        # These have already been assigned\n        if attention_paths_to_split is not None and new_path in attention_paths_to_split:\n            continue\n\n        # Global renaming happens here\n        new_path = new_path.replace(\"middle_block.0\", \"mid_block.resnets.0\")\n        new_path = new_path.replace(\"middle_block.1\", \"mid_block.attentions.0\")\n        new_path = new_path.replace(\"middle_block.2\", \"mid_block.resnets.1\")\n\n        if additional_replacements is not None:\n            for replacement in additional_replacements:\n                new_path = new_path.replace(replacement[\"old\"], replacement[\"new\"])\n\n        # proj_attn.weight has to be converted from conv 1D to linear\n        if \"proj_attn.weight\" in new_path:\n            checkpoint[new_path] = old_checkpoint[path[\"old\"]][:, :, 0]\n        else:\n            checkpoint[new_path] = old_checkpoint[path[\"old\"]]\n\n\ndef conv_attn_to_linear(checkpoint):\n    keys = list(checkpoint.keys())\n    attn_keys = [\"query.weight\", \"key.weight\", \"value.weight\"]\n    for key in keys:\n        if \".\".join(key.split(\".\")[-2:]) in attn_keys:\n            if checkpoint[key].ndim > 2:\n                checkpoint[key] = checkpoint[key][:, :, 0, 0]\n        elif \"proj_attn.weight\" in key:\n            if checkpoint[key].ndim > 2:\n                checkpoint[key] = checkpoint[key][:, :, 0]\n\n\ndef create_unet_diffusers_config(original_config):\n    \"\"\"\n    Creates a config for the diffusers based on the config of the LDM model.\n    \"\"\"\n    unet_params = original_config.model.params.unet_config.params\n\n    block_out_channels = [unet_params.model_channels * mult for mult in unet_params.channel_mult]\n\n    down_block_types = []\n    resolution = 1\n    for i in range(len(block_out_channels)):\n        block_type = \"CrossAttnDownBlock2D\" if resolution in unet_params.attention_resolutions else \"DownBlock2D\"\n        down_block_types.append(block_type)\n        if i != len(block_out_channels) - 1:\n            resolution *= 2\n\n    up_block_types = []\n    for i in range(len(block_out_channels)):\n        block_type = \"CrossAttnUpBlock2D\" if resolution in unet_params.attention_resolutions else \"UpBlock2D\"\n        up_block_types.append(block_type)\n        resolution //= 2\n\n    config = dict(\n        sample_size=unet_params.image_size,\n        in_channels=unet_params.in_channels,\n        out_channels=unet_params.out_channels,\n        down_block_types=tuple(down_block_types),\n        up_block_types=tuple(up_block_types),\n        block_out_channels=tuple(block_out_channels),\n        layers_per_block=unet_params.num_res_blocks,\n        cross_attention_dim=unet_params.context_dim,\n        attention_head_dim=unet_params.num_heads,\n    )\n\n    return config\n\n\ndef create_vae_diffusers_config(original_config):\n    \"\"\"\n    Creates a config for the diffusers based on the config of the LDM model.\n    \"\"\"\n    vae_params = original_config.model.params.first_stage_config.params.ddconfig\n    _ = original_config.model.params.first_stage_config.params.embed_dim\n\n    block_out_channels = [vae_params.ch * mult for mult in vae_params.ch_mult]\n    down_block_types = [\"DownEncoderBlock2D\"] * len(block_out_channels)\n    up_block_types = [\"UpDecoderBlock2D\"] * len(block_out_channels)\n\n    config = dict(\n        sample_size=vae_params.resolution,\n        in_channels=vae_params.in_channels,\n        out_channels=vae_params.out_ch,\n        down_block_types=tuple(down_block_types),\n        up_block_types=tuple(up_block_types),\n        block_out_channels=tuple(block_out_channels),\n        latent_channels=vae_params.z_channels,\n        layers_per_block=vae_params.num_res_blocks,\n    )\n    return config\n\n\ndef create_diffusers_schedular(original_config):\n    schedular = DDIMScheduler(\n        num_train_timesteps=original_config.model.params.timesteps,\n        beta_start=original_config.model.params.linear_start,\n        beta_end=original_config.model.params.linear_end,\n        beta_schedule=\"scaled_linear\",\n    )\n    return schedular\n\n\ndef create_ldm_bert_config(original_config):\n    bert_params = original_config.model.parms.cond_stage_config.params\n    config = LDMBertConfig(\n        d_model=bert_params.n_embed,\n        encoder_layers=bert_params.n_layer,\n        encoder_ffn_dim=bert_params.n_embed * 4,\n    )\n    return config\n\n\ndef convert_ldm_unet_checkpoint(checkpoint, config):\n    \"\"\"\n    Takes a state dict and a config, and returns a converted checkpoint.\n    \"\"\"\n\n    # extract state_dict for UNet\n    unet_state_dict = {}\n    unet_key = \"model.diffusion_model.\"\n    keys = list(checkpoint.keys())\n    for key in keys:\n        if key.startswith(unet_key):\n            unet_state_dict[key.replace(unet_key, \"\")] = checkpoint.pop(key)\n\n    new_checkpoint = {}\n\n    new_checkpoint[\"time_embedding.linear_1.weight\"] = unet_state_dict[\"time_embed.0.weight\"]\n    new_checkpoint[\"time_embedding.linear_1.bias\"] = unet_state_dict[\"time_embed.0.bias\"]\n    new_checkpoint[\"time_embedding.linear_2.weight\"] = unet_state_dict[\"time_embed.2.weight\"]\n    new_checkpoint[\"time_embedding.linear_2.bias\"] = unet_state_dict[\"time_embed.2.bias\"]\n\n    new_checkpoint[\"conv_in.weight\"] = unet_state_dict[\"input_blocks.0.0.weight\"]\n    new_checkpoint[\"conv_in.bias\"] = unet_state_dict[\"input_blocks.0.0.bias\"]\n\n    new_checkpoint[\"conv_norm_out.weight\"] = unet_state_dict[\"out.0.weight\"]\n    new_checkpoint[\"conv_norm_out.bias\"] = unet_state_dict[\"out.0.bias\"]\n    new_checkpoint[\"conv_out.weight\"] = unet_state_dict[\"out.2.weight\"]\n    new_checkpoint[\"conv_out.bias\"] = unet_state_dict[\"out.2.bias\"]\n\n    # Retrieves the keys for the input blocks only\n    num_input_blocks = len({\".\".join(layer.split(\".\")[:2]) for layer in unet_state_dict if \"input_blocks\" in layer})\n    input_blocks = {\n        layer_id: [key for key in unet_state_dict if f\"input_blocks.{layer_id}\" in key]\n        for layer_id in range(num_input_blocks)\n    }\n\n    # Retrieves the keys for the middle blocks only\n    num_middle_blocks = len({\".\".join(layer.split(\".\")[:2]) for layer in unet_state_dict if \"middle_block\" in layer})\n    middle_blocks = {\n        layer_id: [key for key in unet_state_dict if f\"middle_block.{layer_id}\" in key]\n        for layer_id in range(num_middle_blocks)\n    }\n\n    # Retrieves the keys for the output blocks only\n    num_output_blocks = len({\".\".join(layer.split(\".\")[:2]) for layer in unet_state_dict if \"output_blocks\" in layer})\n    output_blocks = {\n        layer_id: [key for key in unet_state_dict if f\"output_blocks.{layer_id}\" in key]\n        for layer_id in range(num_output_blocks)\n    }\n\n    for i in range(1, num_input_blocks):\n        block_id = (i - 1) // (config[\"layers_per_block\"] + 1)\n        layer_in_block_id = (i - 1) % (config[\"layers_per_block\"] + 1)\n\n        resnets = [\n            key for key in input_blocks[i] if f\"input_blocks.{i}.0\" in key and f\"input_blocks.{i}.0.op\" not in key\n        ]\n        attentions = [key for key in input_blocks[i] if f\"input_blocks.{i}.1\" in key]\n\n        if f\"input_blocks.{i}.0.op.weight\" in unet_state_dict:\n            new_checkpoint[f\"down_blocks.{block_id}.downsamplers.0.conv.weight\"] = unet_state_dict.pop(\n                f\"input_blocks.{i}.0.op.weight\"\n            )\n            new_checkpoint[f\"down_blocks.{block_id}.downsamplers.0.conv.bias\"] = unet_state_dict.pop(\n                f\"input_blocks.{i}.0.op.bias\"\n            )\n\n        paths = renew_resnet_paths(resnets)\n        meta_path = {\"old\": f\"input_blocks.{i}.0\", \"new\": f\"down_blocks.{block_id}.resnets.{layer_in_block_id}\"}\n        assign_to_checkpoint(\n            paths, new_checkpoint, unet_state_dict, additional_replacements=[meta_path], config=config\n        )\n\n        if len(attentions):\n            paths = renew_attention_paths(attentions)\n            meta_path = {\"old\": f\"input_blocks.{i}.1\", \"new\": f\"down_blocks.{block_id}.attentions.{layer_in_block_id}\"}\n            assign_to_checkpoint(\n                paths, new_checkpoint, unet_state_dict, additional_replacements=[meta_path], config=config\n            )\n\n    resnet_0 = middle_blocks[0]\n    attentions = middle_blocks[1]\n    resnet_1 = middle_blocks[2]\n\n    resnet_0_paths = renew_resnet_paths(resnet_0)\n    assign_to_checkpoint(resnet_0_paths, new_checkpoint, unet_state_dict, config=config)\n\n    resnet_1_paths = renew_resnet_paths(resnet_1)\n    assign_to_checkpoint(resnet_1_paths, new_checkpoint, unet_state_dict, config=config)\n\n    attentions_paths = renew_attention_paths(attentions)\n    meta_path = {\"old\": \"middle_block.1\", \"new\": \"mid_block.attentions.0\"}\n    assign_to_checkpoint(\n        attentions_paths, new_checkpoint, unet_state_dict, additional_replacements=[meta_path], config=config\n    )\n\n    for i in range(num_output_blocks):\n        block_id = i // (config[\"layers_per_block\"] + 1)\n        layer_in_block_id = i % (config[\"layers_per_block\"] + 1)\n        output_block_layers = [shave_segments(name, 2) for name in output_blocks[i]]\n        output_block_list = {}\n\n        for layer in output_block_layers:\n            layer_id, layer_name = layer.split(\".\")[0], shave_segments(layer, 1)\n            if layer_id in output_block_list:\n                output_block_list[layer_id].append(layer_name)\n            else:\n                output_block_list[layer_id] = [layer_name]\n\n        if len(output_block_list) > 1:\n            resnets = [key for key in output_blocks[i] if f\"output_blocks.{i}.0\" in key]\n            attentions = [key for key in output_blocks[i] if f\"output_blocks.{i}.1\" in key]\n\n            resnet_0_paths = renew_resnet_paths(resnets)\n            paths = renew_resnet_paths(resnets)\n\n            meta_path = {\"old\": f\"output_blocks.{i}.0\", \"new\": f\"up_blocks.{block_id}.resnets.{layer_in_block_id}\"}\n            assign_to_checkpoint(\n                paths, new_checkpoint, unet_state_dict, additional_replacements=[meta_path], config=config\n            )\n\n            if [\"conv.weight\", \"conv.bias\"] in output_block_list.values():\n                index = list(output_block_list.values()).index([\"conv.weight\", \"conv.bias\"])\n                new_checkpoint[f\"up_blocks.{block_id}.upsamplers.0.conv.weight\"] = unet_state_dict[\n                    f\"output_blocks.{i}.{index}.conv.weight\"\n                ]\n                new_checkpoint[f\"up_blocks.{block_id}.upsamplers.0.conv.bias\"] = unet_state_dict[\n                    f\"output_blocks.{i}.{index}.conv.bias\"\n                ]\n\n                # Clear attentions as they have been attributed above.\n                if len(attentions) == 2:\n                    attentions = []\n\n            if len(attentions):\n                paths = renew_attention_paths(attentions)\n                meta_path = {\n                    \"old\": f\"output_blocks.{i}.1\",\n                    \"new\": f\"up_blocks.{block_id}.attentions.{layer_in_block_id}\",\n                }\n                assign_to_checkpoint(\n                    paths, new_checkpoint, unet_state_dict, additional_replacements=[meta_path], config=config\n                )\n        else:\n            resnet_0_paths = renew_resnet_paths(output_block_layers, n_shave_prefix_segments=1)\n            for path in resnet_0_paths:\n                old_path = \".\".join([\"output_blocks\", str(i), path[\"old\"]])\n                new_path = \".\".join([\"up_blocks\", str(block_id), \"resnets\", str(layer_in_block_id), path[\"new\"]])\n\n                new_checkpoint[new_path] = unet_state_dict[old_path]\n\n    return new_checkpoint\n\n\ndef convert_ldm_vae_checkpoint(checkpoint, config):\n    # extract state dict for VAE\n    vae_state_dict = {}\n    vae_key = \"first_stage_model.\"\n    keys = list(checkpoint.keys())\n    for key in keys:\n        if key.startswith(vae_key):\n            vae_state_dict[key.replace(vae_key, \"\")] = checkpoint.get(key)\n\n    new_checkpoint = {}\n\n    new_checkpoint[\"encoder.conv_in.weight\"] = vae_state_dict[\"encoder.conv_in.weight\"]\n    new_checkpoint[\"encoder.conv_in.bias\"] = vae_state_dict[\"encoder.conv_in.bias\"]\n    new_checkpoint[\"encoder.conv_out.weight\"] = vae_state_dict[\"encoder.conv_out.weight\"]\n    new_checkpoint[\"encoder.conv_out.bias\"] = vae_state_dict[\"encoder.conv_out.bias\"]\n    new_checkpoint[\"encoder.conv_norm_out.weight\"] = vae_state_dict[\"encoder.norm_out.weight\"]\n    new_checkpoint[\"encoder.conv_norm_out.bias\"] = vae_state_dict[\"encoder.norm_out.bias\"]\n\n    new_checkpoint[\"decoder.conv_in.weight\"] = vae_state_dict[\"decoder.conv_in.weight\"]\n    new_checkpoint[\"decoder.conv_in.bias\"] = vae_state_dict[\"decoder.conv_in.bias\"]\n    new_checkpoint[\"decoder.conv_out.weight\"] = vae_state_dict[\"decoder.conv_out.weight\"]\n    new_checkpoint[\"decoder.conv_out.bias\"] = vae_state_dict[\"decoder.conv_out.bias\"]\n    new_checkpoint[\"decoder.conv_norm_out.weight\"] = vae_state_dict[\"decoder.norm_out.weight\"]\n    new_checkpoint[\"decoder.conv_norm_out.bias\"] = vae_state_dict[\"decoder.norm_out.bias\"]\n\n    new_checkpoint[\"quant_conv.weight\"] = vae_state_dict[\"quant_conv.weight\"]\n    new_checkpoint[\"quant_conv.bias\"] = vae_state_dict[\"quant_conv.bias\"]\n    new_checkpoint[\"post_quant_conv.weight\"] = vae_state_dict[\"post_quant_conv.weight\"]\n    new_checkpoint[\"post_quant_conv.bias\"] = vae_state_dict[\"post_quant_conv.bias\"]\n\n    # Retrieves the keys for the encoder down blocks only\n    num_down_blocks = len({\".\".join(layer.split(\".\")[:3]) for layer in vae_state_dict if \"encoder.down\" in layer})\n    down_blocks = {\n        layer_id: [key for key in vae_state_dict if f\"down.{layer_id}\" in key] for layer_id in range(num_down_blocks)\n    }\n\n    # Retrieves the keys for the decoder up blocks only\n    num_up_blocks = len({\".\".join(layer.split(\".\")[:3]) for layer in vae_state_dict if \"decoder.up\" in layer})\n    up_blocks = {\n        layer_id: [key for key in vae_state_dict if f\"up.{layer_id}\" in key] for layer_id in range(num_up_blocks)\n    }\n\n    for i in range(num_down_blocks):\n        resnets = [key for key in down_blocks[i] if f\"down.{i}\" in key and f\"down.{i}.downsample\" not in key]\n\n        if f\"encoder.down.{i}.downsample.conv.weight\" in vae_state_dict:\n            new_checkpoint[f\"encoder.down_blocks.{i}.downsamplers.0.conv.weight\"] = vae_state_dict.pop(\n                f\"encoder.down.{i}.downsample.conv.weight\"\n            )\n            new_checkpoint[f\"encoder.down_blocks.{i}.downsamplers.0.conv.bias\"] = vae_state_dict.pop(\n                f\"encoder.down.{i}.downsample.conv.bias\"\n            )\n\n        paths = renew_vae_resnet_paths(resnets)\n        meta_path = {\"old\": f\"down.{i}.block\", \"new\": f\"down_blocks.{i}.resnets\"}\n        assign_to_checkpoint(paths, new_checkpoint, vae_state_dict, additional_replacements=[meta_path], config=config)\n\n    mid_resnets = [key for key in vae_state_dict if \"encoder.mid.block\" in key]\n    num_mid_res_blocks = 2\n    for i in range(1, num_mid_res_blocks + 1):\n        resnets = [key for key in mid_resnets if f\"encoder.mid.block_{i}\" in key]\n\n        paths = renew_vae_resnet_paths(resnets)\n        meta_path = {\"old\": f\"mid.block_{i}\", \"new\": f\"mid_block.resnets.{i - 1}\"}\n        assign_to_checkpoint(paths, new_checkpoint, vae_state_dict, additional_replacements=[meta_path], config=config)\n\n    mid_attentions = [key for key in vae_state_dict if \"encoder.mid.attn\" in key]\n    paths = renew_vae_attention_paths(mid_attentions)\n    meta_path = {\"old\": \"mid.attn_1\", \"new\": \"mid_block.attentions.0\"}\n    assign_to_checkpoint(paths, new_checkpoint, vae_state_dict, additional_replacements=[meta_path], config=config)\n    conv_attn_to_linear(new_checkpoint)\n\n    for i in range(num_up_blocks):\n        block_id = num_up_blocks - 1 - i\n        resnets = [\n            key for key in up_blocks[block_id] if f\"up.{block_id}\" in key and f\"up.{block_id}.upsample\" not in key\n        ]\n\n        if f\"decoder.up.{block_id}.upsample.conv.weight\" in vae_state_dict:\n            new_checkpoint[f\"decoder.up_blocks.{i}.upsamplers.0.conv.weight\"] = vae_state_dict[\n                f\"decoder.up.{block_id}.upsample.conv.weight\"\n            ]\n            new_checkpoint[f\"decoder.up_blocks.{i}.upsamplers.0.conv.bias\"] = vae_state_dict[\n                f\"decoder.up.{block_id}.upsample.conv.bias\"\n            ]\n\n        paths = renew_vae_resnet_paths(resnets)\n        meta_path = {\"old\": f\"up.{block_id}.block\", \"new\": f\"up_blocks.{i}.resnets\"}\n        assign_to_checkpoint(paths, new_checkpoint, vae_state_dict, additional_replacements=[meta_path], config=config)\n\n    mid_resnets = [key for key in vae_state_dict if \"decoder.mid.block\" in key]\n    num_mid_res_blocks = 2\n    for i in range(1, num_mid_res_blocks + 1):\n        resnets = [key for key in mid_resnets if f\"decoder.mid.block_{i}\" in key]\n\n        paths = renew_vae_resnet_paths(resnets)\n        meta_path = {\"old\": f\"mid.block_{i}\", \"new\": f\"mid_block.resnets.{i - 1}\"}\n        assign_to_checkpoint(paths, new_checkpoint, vae_state_dict, additional_replacements=[meta_path], config=config)\n\n    mid_attentions = [key for key in vae_state_dict if \"decoder.mid.attn\" in key]\n    paths = renew_vae_attention_paths(mid_attentions)\n    meta_path = {\"old\": \"mid.attn_1\", \"new\": \"mid_block.attentions.0\"}\n    assign_to_checkpoint(paths, new_checkpoint, vae_state_dict, additional_replacements=[meta_path], config=config)\n    conv_attn_to_linear(new_checkpoint)\n    return new_checkpoint\n\n\ndef convert_ldm_bert_checkpoint(checkpoint, config):\n    def _copy_attn_layer(hf_attn_layer, pt_attn_layer):\n        hf_attn_layer.q_proj.weight.data = pt_attn_layer.to_q.weight\n        hf_attn_layer.k_proj.weight.data = pt_attn_layer.to_k.weight\n        hf_attn_layer.v_proj.weight.data = pt_attn_layer.to_v.weight\n\n        hf_attn_layer.out_proj.weight = pt_attn_layer.to_out.weight\n        hf_attn_layer.out_proj.bias = pt_attn_layer.to_out.bias\n\n    def _copy_linear(hf_linear, pt_linear):\n        hf_linear.weight = pt_linear.weight\n        hf_linear.bias = pt_linear.bias\n\n    def _copy_layer(hf_layer, pt_layer):\n        # copy layer norms\n        _copy_linear(hf_layer.self_attn_layer_norm, pt_layer[0][0])\n        _copy_linear(hf_layer.final_layer_norm, pt_layer[1][0])\n\n        # copy attn\n        _copy_attn_layer(hf_layer.self_attn, pt_layer[0][1])\n\n        # copy MLP\n        pt_mlp = pt_layer[1][1]\n        _copy_linear(hf_layer.fc1, pt_mlp.net[0][0])\n        _copy_linear(hf_layer.fc2, pt_mlp.net[2])\n\n    def _copy_layers(hf_layers, pt_layers):\n        for i, hf_layer in enumerate(hf_layers):\n            if i != 0:\n                i += i\n            pt_layer = pt_layers[i : i + 2]\n            _copy_layer(hf_layer, pt_layer)\n\n    hf_model = LDMBertModel(config).eval()\n\n    # copy  embeds\n    hf_model.model.embed_tokens.weight = checkpoint.transformer.token_emb.weight\n    hf_model.model.embed_positions.weight.data = checkpoint.transformer.pos_emb.emb.weight\n\n    # copy layer norm\n    _copy_linear(hf_model.model.layer_norm, checkpoint.transformer.norm)\n\n    # copy hidden layers\n    _copy_layers(hf_model.model.layers, checkpoint.transformer.attn_layers.layers)\n\n    _copy_linear(hf_model.to_logits, checkpoint.transformer.to_logits)\n\n    return hf_model\n\n\ndef convert_ldm_clip_checkpoint(checkpoint):\n    text_model = CLIPTextModel.from_pretrained(\"openai/clip-vit-large-patch14\")\n\n    keys = list(checkpoint.keys())\n\n    text_model_dict = {}\n\n    for key in keys:\n        if key.startswith(\"cond_stage_model.transformer\"):\n            text_model_dict[key[len(\"cond_stage_model.transformer.\") :]] = checkpoint[key]\n\n    text_model.load_state_dict(text_model_dict)\n\n    return text_model\n\nimport os\ndef convert_checkpoint(checkpoint_path, inpainting=False):\n    parser = argparse.ArgumentParser()\n\n    parser.add_argument(\n        \"--checkpoint_path\", default=checkpoint_path, type=str, help=\"Path to the checkpoint to convert.\"\n    )\n    # !wget https://raw.githubusercontent.com/CompVis/stable-diffusion/main/configs/stable-diffusion/v1-inference.yaml\n    parser.add_argument(\n        \"--original_config_file\",\n        default=None,\n        type=str,\n        help=\"The YAML config file corresponding to the original architecture.\",\n    )\n    parser.add_argument(\n        \"--scheduler_type\",\n        default=\"pndm\",\n        type=str,\n        help=\"Type of scheduler to use. Should be one of ['pndm', 'lms', 'ddim']\",\n    )\n    parser.add_argument(\"--dump_path\", default=None, type=str, help=\"Path to the output model.\")\n\n    args = parser.parse_args([])\n    if args.original_config_file is None:\n        if inpainting:\n            args.original_config_file = \"./models/v1-inpainting-inference.yaml\"\n        else:\n            args.original_config_file = \"./models/v1-inference.yaml\"\n\n    original_config = OmegaConf.load(args.original_config_file)\n    checkpoint = torch.load(args.checkpoint_path)[\"state_dict\"]\n\n    num_train_timesteps = original_config.model.params.timesteps\n    beta_start = original_config.model.params.linear_start\n    beta_end = original_config.model.params.linear_end\n    if args.scheduler_type == \"pndm\":\n        scheduler = PNDMScheduler(\n            beta_end=beta_end,\n            beta_schedule=\"scaled_linear\",\n            beta_start=beta_start,\n            num_train_timesteps=num_train_timesteps,\n            skip_prk_steps=True,\n        )\n    elif args.scheduler_type == \"lms\":\n        scheduler = LMSDiscreteScheduler(beta_start=beta_start, beta_end=beta_end, beta_schedule=\"scaled_linear\")\n    elif args.scheduler_type == \"ddim\":\n        scheduler = DDIMScheduler(\n            beta_start=beta_start,\n            beta_end=beta_end,\n            beta_schedule=\"scaled_linear\",\n            clip_sample=False,\n            set_alpha_to_one=False,\n        )\n    else:\n        raise ValueError(f\"Scheduler of type {args.scheduler_type} doesn't exist!\")\n\n    # Convert the UNet2DConditionModel model.\n    unet_config = create_unet_diffusers_config(original_config)\n    converted_unet_checkpoint = convert_ldm_unet_checkpoint(checkpoint, unet_config)\n\n    unet = UNet2DConditionModel(**unet_config)\n    unet.load_state_dict(converted_unet_checkpoint)\n\n    # Convert the VAE model.\n    vae_config = create_vae_diffusers_config(original_config)\n    converted_vae_checkpoint = convert_ldm_vae_checkpoint(checkpoint, vae_config)\n\n    vae = AutoencoderKL(**vae_config)\n    vae.load_state_dict(converted_vae_checkpoint)\n\n    # Convert the text model.\n    text_model_type = original_config.model.params.cond_stage_config.target.split(\".\")[-1]\n    if text_model_type == \"FrozenCLIPEmbedder\":\n        text_model = convert_ldm_clip_checkpoint(checkpoint)\n        tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-large-patch14\")\n        safety_checker = StableDiffusionSafetyChecker.from_pretrained(\"CompVis/stable-diffusion-safety-checker\")\n        feature_extractor = AutoFeatureExtractor.from_pretrained(\"CompVis/stable-diffusion-safety-checker\")\n        pipe = StableDiffusionPipeline(\n            vae=vae,\n            text_encoder=text_model,\n            tokenizer=tokenizer,\n            unet=unet,\n            scheduler=scheduler,\n            safety_checker=safety_checker,\n            feature_extractor=feature_extractor,\n        )\n    else:\n        text_config = create_ldm_bert_config(original_config)\n        text_model = convert_ldm_bert_checkpoint(checkpoint, text_config)\n        tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n        pipe = LDMTextToImagePipeline(vqvae=vae, bert=text_model, tokenizer=tokenizer, unet=unet, scheduler=scheduler)\n\n    return pipe\n"
        },
        {
          "name": "css",
          "type": "tree",
          "content": null
        },
        {
          "name": "docker-compose.yml",
          "type": "blob",
          "size": 0.453125,
          "content": "services:\n  sd-infinity:\n    build:\n      context: .\n      dockerfile: ./docker/Dockerfile\n    #shm_size: '2gb' # Enable if more shared memory is needed\n    ports:\n      - \"8888:8888\"\n    volumes:\n      - user_home:/home/user\n      - cond_env:/opt/conda/envs\n    deploy:\n      resources:\n        reservations:\n          devices:\n            - driver: nvidia\n              device_ids: ['0']\n              capabilities: [gpu]\n\nvolumes:\n  user_home: {}\n  cond_env: {}"
        },
        {
          "name": "docker",
          "type": "tree",
          "content": null
        },
        {
          "name": "docs",
          "type": "tree",
          "content": null
        },
        {
          "name": "environment.yml",
          "type": "blob",
          "size": 8.451171875,
          "content": "name: sd-inf\nchannels:\n  - pytorch\n  - nvidia\n  - conda-forge\n  - defaults\ndependencies:\n  - _libgcc_mutex=0.1=main\n  - _openmp_mutex=5.1=1_gnu\n  - abseil-cpp=20211102.0=h27087fc_1\n  - accelerate=0.14.0=pyhd8ed1ab_0\n  - aiohttp=3.8.1=py310h5764c6d_1\n  - aiosignal=1.3.1=pyhd8ed1ab_0\n  - arrow-cpp=8.0.0=py310h3098874_0\n  - async-timeout=4.0.2=pyhd8ed1ab_0\n  - attrs=22.1.0=pyh71513ae_1\n  - aws-c-common=0.4.57=he1b5a44_1\n  - aws-c-event-stream=0.1.6=h72b8ae1_3\n  - aws-checksums=0.1.9=h346380f_0\n  - aws-sdk-cpp=1.8.185=hce553d0_0\n  - backports=1.0=py_2\n  - backports.functools_lru_cache=1.6.4=pyhd8ed1ab_0\n  - blas=1.0=mkl\n  - blosc=1.21.0=h4ff587b_1\n  - boost-cpp=1.78.0=he72f1d9_0\n  - brotli=1.0.9=h5eee18b_7\n  - brotli-bin=1.0.9=h5eee18b_7\n  - brotlipy=0.7.0=py310h7f8727e_1002\n  - brunsli=0.1=h2531618_0\n  - bzip2=1.0.8=h7b6447c_0\n  - c-ares=1.18.1=h7f8727e_0\n  - ca-certificates=2022.10.11=h06a4308_0\n  - certifi=2022.9.24=py310h06a4308_0\n  - cffi=1.15.1=py310h74dc2b5_0\n  - cfitsio=3.470=h5893167_7\n  - charls=2.2.0=h2531618_0\n  - charset-normalizer=2.0.4=pyhd3eb1b0_0\n  - click=8.1.3=unix_pyhd8ed1ab_2\n  - cloudpickle=2.0.0=pyhd3eb1b0_0\n  - colorama=0.4.6=pyhd8ed1ab_0\n  - cryptography=38.0.1=py310h9ce1e76_0\n  - cuda=11.6.2=0\n  - cuda-cccl=11.6.55=hf6102b2_0\n  - cuda-command-line-tools=11.6.2=0\n  - cuda-compiler=11.6.2=0\n  - cuda-cudart=11.6.55=he381448_0\n  - cuda-cudart-dev=11.6.55=h42ad0f4_0\n  - cuda-cuobjdump=11.6.124=h2eeebcb_0\n  - cuda-cupti=11.6.124=h86345e5_0\n  - cuda-cuxxfilt=11.6.124=hecbf4f6_0\n  - cuda-driver-dev=11.6.55=0\n  - cuda-gdb=11.8.86=0\n  - cuda-libraries=11.6.2=0\n  - cuda-libraries-dev=11.6.2=0\n  - cuda-memcheck=11.8.86=0\n  - cuda-nsight=11.8.86=0\n  - cuda-nsight-compute=11.8.0=0\n  - cuda-nvcc=11.6.124=hbba6d2d_0\n  - cuda-nvdisasm=11.8.86=0\n  - cuda-nvml-dev=11.6.55=haa9ef22_0\n  - cuda-nvprof=11.8.87=0\n  - cuda-nvprune=11.6.124=he22ec0a_0\n  - cuda-nvrtc=11.6.124=h020bade_0\n  - cuda-nvrtc-dev=11.6.124=h249d397_0\n  - cuda-nvtx=11.6.124=h0630a44_0\n  - cuda-nvvp=11.8.87=0\n  - cuda-runtime=11.6.2=0\n  - cuda-samples=11.6.101=h8efea70_0\n  - cuda-sanitizer-api=11.8.86=0\n  - cuda-toolkit=11.6.2=0\n  - cuda-tools=11.6.2=0\n  - cuda-visual-tools=11.6.2=0\n  - cytoolz=0.12.0=py310h5eee18b_0\n  - dask-core=2022.7.0=py310h06a4308_0\n  - dataclasses=0.8=pyhc8e2a94_3\n  - datasets=2.7.0=pyhd8ed1ab_0\n  - diffusers=0.11.1=pyhd8ed1ab_0\n  - dill=0.3.6=pyhd8ed1ab_1\n  - ffmpeg=4.3=hf484d3e_0\n  - fftw=3.3.9=h27cfd23_1\n  - filelock=3.8.0=pyhd8ed1ab_0\n  - freetype=2.12.1=h4a9f257_0\n  - frozenlist=1.3.0=py310h5764c6d_1\n  - fsspec=2022.10.0=py310h06a4308_0\n  - ftfy=6.1.1=pyhd8ed1ab_0\n  - gds-tools=1.4.0.31=0\n  - gflags=2.2.2=he1b5a44_1004\n  - giflib=5.2.1=h7b6447c_0\n  - glog=0.6.0=h6f12383_0\n  - gmp=6.2.1=h295c915_3\n  - gnutls=3.6.15=he1e5248_0\n  - grpc-cpp=1.46.1=h33aed49_0\n  - huggingface_hub=0.11.0=pyhd8ed1ab_0\n  - icu=70.1=h27087fc_0\n  - idna=3.4=py310h06a4308_0\n  - imagecodecs=2021.8.26=py310hecf7e94_1\n  - imageio=2.19.3=py310h06a4308_0\n  - importlib-metadata=5.0.0=pyha770c72_1\n  - importlib_metadata=5.0.0=hd8ed1ab_1\n  - intel-openmp=2021.4.0=h06a4308_3561\n  - joblib=1.2.0=pyhd8ed1ab_0\n  - jpeg=9e=h7f8727e_0\n  - jxrlib=1.1=h7b6447c_2\n  - krb5=1.19.2=hac12032_0\n  - lame=3.100=h7b6447c_0\n  - lcms2=2.12=h3be6417_0\n  - ld_impl_linux-64=2.38=h1181459_1\n  - lerc=3.0=h295c915_0\n  - libaec=1.0.4=he6710b0_1\n  - libbrotlicommon=1.0.9=h5eee18b_7\n  - libbrotlidec=1.0.9=h5eee18b_7\n  - libbrotlienc=1.0.9=h5eee18b_7\n  - libcublas=11.11.3.6=0\n  - libcublas-dev=11.11.3.6=0\n  - libcufft=10.9.0.58=0\n  - libcufft-dev=10.9.0.58=0\n  - libcufile=1.4.0.31=0\n  - libcufile-dev=1.4.0.31=0\n  - libcurand=10.3.0.86=0\n  - libcurand-dev=10.3.0.86=0\n  - libcurl=7.85.0=h91b91d3_0\n  - libcusolver=11.4.1.48=0\n  - libcusolver-dev=11.4.1.48=0\n  - libcusparse=11.7.5.86=0\n  - libcusparse-dev=11.7.5.86=0\n  - libdeflate=1.8=h7f8727e_5\n  - libedit=3.1.20210910=h7f8727e_0\n  - libev=4.33=h7f8727e_1\n  - libevent=2.1.10=h9b69904_4\n  - libffi=3.3=he6710b0_2\n  - libgcc-ng=11.2.0=h1234567_1\n  - libgfortran-ng=11.2.0=h00389a5_1\n  - libgfortran5=11.2.0=h1234567_1\n  - libgomp=11.2.0=h1234567_1\n  - libiconv=1.16=h7f8727e_2\n  - libidn2=2.3.2=h7f8727e_0\n  - libnghttp2=1.46.0=hce63b2e_0\n  - libnpp=11.8.0.86=0\n  - libnpp-dev=11.8.0.86=0\n  - libnvjpeg=11.9.0.86=0\n  - libnvjpeg-dev=11.9.0.86=0\n  - libpng=1.6.37=hbc83047_0\n  - libprotobuf=3.20.1=h4ff587b_0\n  - libssh2=1.10.0=h8f2d780_0\n  - libstdcxx-ng=11.2.0=h1234567_1\n  - libtasn1=4.16.0=h27cfd23_0\n  - libthrift=0.15.0=he6d91bd_0\n  - libtiff=4.4.0=hecacb30_2\n  - libunistring=0.9.10=h27cfd23_0\n  - libuuid=1.41.5=h5eee18b_0\n  - libwebp=1.2.4=h11a3e52_0\n  - libwebp-base=1.2.4=h5eee18b_0\n  - libzopfli=1.0.3=he6710b0_0\n  - locket=1.0.0=py310h06a4308_0\n  - lz4-c=1.9.3=h295c915_1\n  - mkl=2021.4.0=h06a4308_640\n  - mkl-service=2.4.0=py310h7f8727e_0\n  - mkl_fft=1.3.1=py310hd6ae3a3_0\n  - mkl_random=1.2.2=py310h00e6091_0\n  - multidict=6.0.2=py310h5764c6d_1\n  - multiprocess=0.70.12.2=py310h5764c6d_2\n  - ncurses=6.3=h5eee18b_3\n  - nettle=3.7.3=hbbd107a_1\n  - networkx=2.8.4=py310h06a4308_0\n  - nsight-compute=2022.3.0.22=0\n  - numpy=1.23.4=py310hd5efca6_0\n  - numpy-base=1.23.4=py310h8e6c178_0\n  - openh264=2.1.1=h4ff587b_0\n  - openjpeg=2.4.0=h3ad879b_0\n  - openssl=1.1.1s=h7f8727e_0\n  - orc=1.7.4=h07ed6aa_0\n  - packaging=21.3=pyhd3eb1b0_0\n  - pandas=1.4.2=py310h769672d_1\n  - partd=1.2.0=pyhd3eb1b0_1\n  - pillow=9.2.0=py310hace64e9_1\n  - pip=22.2.2=py310h06a4308_0\n  - psutil=5.9.1=py310h5764c6d_0\n  - pyarrow=8.0.0=py310h468efa6_0\n  - pycparser=2.21=pyhd3eb1b0_0\n  - pyopenssl=22.0.0=pyhd3eb1b0_0\n  - pyparsing=3.0.9=py310h06a4308_0\n  - pysocks=1.7.1=py310h06a4308_0\n  - python=3.10.8=haa1d7c7_0\n  - python-dateutil=2.8.2=pyhd8ed1ab_0\n  - python-xxhash=3.0.0=py310h5764c6d_1\n  - python_abi=3.10=2_cp310\n  - pytorch=1.13.0=py3.10_cuda11.6_cudnn8.3.2_0\n  - pytorch-cuda=11.6=h867d48c_0\n  - pytorch-mutex=1.0=cuda\n  - pytz=2022.6=pyhd8ed1ab_0\n  - pywavelets=1.3.0=py310h7f8727e_0\n  - re2=2022.04.01=h27087fc_0\n  - readline=8.2=h5eee18b_0\n  - regex=2022.4.24=py310h5764c6d_0\n  - requests=2.28.1=py310h06a4308_0\n  - responses=0.18.0=pyhd8ed1ab_0\n  - sacremoses=0.0.53=pyhd8ed1ab_0\n  - scikit-image=0.19.2=py310h00e6091_0\n  - scipy=1.9.3=py310hd5efca6_0\n  - setuptools=65.5.0=py310h06a4308_0\n  - six=1.16.0=pyhd3eb1b0_1\n  - snappy=1.1.9=h295c915_0\n  - sqlite=3.39.3=h5082296_0\n  - tifffile=2021.7.2=pyhd3eb1b0_2\n  - tk=8.6.12=h1ccaba5_0\n  - tokenizers=0.11.4=py310h3dcd8bd_1\n  - toolz=0.12.0=py310h06a4308_0\n  - torchaudio=0.13.0=py310_cu116\n  - torchvision=0.14.0=py310_cu116\n  - tqdm=4.64.1=pyhd8ed1ab_0\n  - transformers=4.24.0=pyhd8ed1ab_0\n  - typing-extensions=4.3.0=py310h06a4308_0\n  - typing_extensions=4.3.0=py310h06a4308_0\n  - tzdata=2022f=h04d1e81_0\n  - urllib3=1.26.12=py310h06a4308_0\n  - utf8proc=2.6.1=h27cfd23_0\n  - wcwidth=0.2.5=pyh9f0ad1d_2\n  - wheel=0.37.1=pyhd3eb1b0_0\n  - xxhash=0.8.0=h7f98852_3\n  - xz=5.2.6=h5eee18b_0\n  - yaml=0.2.5=h7b6447c_0\n  - yarl=1.7.2=py310h5764c6d_2\n  - zfp=0.5.5=h295c915_6\n  - zipp=3.10.0=pyhd8ed1ab_0\n  - zlib=1.2.13=h5eee18b_0\n  - zstd=1.5.2=ha4553b6_0\n  - pip:\n    - absl-py==1.3.0\n    - antlr4-python3-runtime==4.9.3\n    - anyio==3.6.2\n    - bcrypt==4.0.1\n    - cachetools==5.2.0\n    - cmake==3.25.0\n    - commonmark==0.9.1\n    - contourpy==1.0.6\n    - cycler==0.11.0\n    - einops==0.4.1\n    - fastapi==0.87.0\n    - ffmpy==0.3.0\n    - fonttools==4.38.0\n    - fpie==0.2.4\n    - google-auth==2.14.1\n    - google-auth-oauthlib==0.4.6\n    - gradio==3.10.1\n    - grpcio==1.51.0\n    - h11==0.12.0\n    - httpcore==0.15.0\n    - httpx==0.23.1\n    - jinja2==3.1.2\n    - kiwisolver==1.4.4\n    - linkify-it-py==1.0.3\n    - llvmlite==0.39.1\n    - markdown==3.4.1\n    - markdown-it-py==2.1.0\n    - markupsafe==2.1.1\n    - matplotlib==3.6.2\n    - mdit-py-plugins==0.3.1\n    - mdurl==0.1.2\n    - numba==0.56.4\n    - oauthlib==3.2.2\n    - omegaconf==2.2.3\n    - opencv-python==4.6.0.66\n    - opencv-python-headless==4.6.0.66\n    - orjson==3.8.2\n    - paramiko==2.12.0\n    - protobuf==3.20.3\n    - pyasn1==0.4.8\n    - pyasn1-modules==0.2.8\n    - pycryptodome==3.15.0\n    - pydantic==1.10.2\n    - pydeprecate==0.3.2\n    - pydub==0.25.1\n    - pygments==2.13.0\n    - pynacl==1.5.0\n    - python-multipart==0.0.5\n    - pytorch-lightning==1.7.7\n    - pyyaml==6.0\n    - requests-oauthlib==1.3.1\n    - rfc3986==1.5.0\n    - rich==12.6.0\n    - rsa==4.9\n    - sniffio==1.3.0\n    - sourceinspect==0.0.4\n    - starlette==0.21.0\n    - taichi==1.2.2\n    - tensorboard==2.11.0\n    - tensorboard-data-server==0.6.1\n    - tensorboard-plugin-wit==1.8.1\n    - timm==0.6.11\n    - torchmetrics==0.10.3\n    - uc-micro-py==1.0.1\n    - uvicorn==0.20.0\n    - websockets==10.4\n    - werkzeug==2.2.2"
        },
        {
          "name": "glid_3_xl_stable",
          "type": "commit",
          "content": null
        },
        {
          "name": "index.html",
          "type": "blob",
          "size": 14.625,
          "content": "<html>\n<head>\n<title>Stablediffusion Infinity</title>\n<meta charset=\"utf-8\">\n<link rel=\"icon\" type=\"image/x-icon\" href=\"./favicon.png\">\n\n<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.jsdelivr.net/gh/lkwq007/stablediffusion-infinity@v0.1.2/css/w2ui.min.css\">\n<script type=\"text/javascript\" src=\"https://cdn.jsdelivr.net/gh/lkwq007/stablediffusion-infinity@v0.1.2/js/w2ui.min.js\"></script>\n<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.2.0/css/all.min.css\">\n<script src=\"https://cdn.jsdelivr.net/gh/lkwq007/stablediffusion-infinity@v0.1.2/js/fabric.min.js\"></script>\n<script defer src=\"https://cdn.jsdelivr.net/gh/lkwq007/stablediffusion-infinity@v0.1.2/js/toolbar.js\"></script>\n\n<link rel=\"stylesheet\" href=\"https://pyscript.net/alpha/pyscript.css\" />\n<script defer src=\"https://pyscript.net/alpha/pyscript.js\"></script>\n\n<style>\n#container {\n  position: relative;\n  margin:auto;\n  display: block;\n}\n#container > canvas {\n  position: absolute;\n  top: 0;\n  left: 0;\n}\n.control {\n  display: none;\n}\n</style>\n\n</head>\n<body>\n<div>\n<button type=\"button\" class=\"control\" id=\"export\">Export</button>\n<button type=\"button\" class=\"control\" id=\"outpaint\">Outpaint</button>\n<button type=\"button\" class=\"control\" id=\"undo\">Undo</button>\n<button type=\"button\" class=\"control\" id=\"commit\">Commit</button>\n<button type=\"button\" class=\"control\" id=\"transfer\">Transfer</button>\n<button type=\"button\" class=\"control\" id=\"upload\">Upload</button>\n<button type=\"button\" class=\"control\" id=\"draw\">Draw</button>\n<input type=\"text\" id=\"mode\" value=\"selection\" class=\"control\">\n<input type=\"text\" id=\"setup\" value=\"0\" class=\"control\">\n<input type=\"text\" id=\"upload_content\" value=\"0\" class=\"control\">\n<textarea rows=\"1\" id=\"selbuffer\" name=\"selbuffer\" class=\"control\"></textarea>\n<fieldset class=\"control\">\n    <div>\n      <input type=\"radio\" id=\"mode0\" name=\"mode\" value=\"0\" checked>\n      <label for=\"mode0\">SelBox</label>\n    </div>\n    <div>\n      <input type=\"radio\" id=\"mode1\" name=\"mode\" value=\"1\">\n      <label for=\"mode1\">Image</label>\n    </div>\n    <div>\n      <input type=\"radio\" id=\"mode2\" name=\"mode\" value=\"2\">\n      <label for=\"mode2\">Brush</label>\n    </div>\n</fieldset>\n</div>\n<div id = \"outer_container\">\n<div id = \"container\">\n  <canvas id = \"canvas0\"></canvas>\n  <canvas id = \"canvas1\"></canvas>\n  <canvas id = \"canvas2\"></canvas>\n  <canvas id = \"canvas3\"></canvas>\n  <canvas id = \"canvas4\"></canvas>\n  <div id=\"overlay_container\" style=\"pointer-events: none\">\n    <canvas id = \"overlay_canvas\" width=\"1\" height=\"1\"></canvas>\n  </div>\n</div>\n<input type=\"file\" name=\"file\" id=\"upload_file\" accept=\"image/*\" hidden>\n<input type=\"file\" name=\"state\" id=\"upload_state\" accept=\".sdinf\" hidden>\n<div style=\"position: relative;\">\n<div id=\"toolbar\" style></div>\n</div>\n</div>\n<py-env>\n- numpy\n- Pillow\n- paths:\n  - ./canvas.py\n</py-env>\n\n<py-script>\nfrom pyodide import to_js, create_proxy\nfrom PIL import Image\nimport io\nimport time\nimport base64\nfrom collections import deque\nimport numpy as np\nfrom js import (\n    console,\n    document,\n    parent,\n    devicePixelRatio,\n    ImageData,\n    Uint8ClampedArray,\n    CanvasRenderingContext2D as Context2d,\n    requestAnimationFrame,\n    window,\n    encodeURIComponent,\n    w2ui,\n    update_eraser,\n    update_scale,\n    adjust_selection,\n    update_count,\n    enable_result_lst,\n    setup_shortcut,\n    update_undo_redo,\n)\n\n\nfrom canvas import InfCanvas\n\n\nclass History:\n    def __init__(self,maxlen=10):\n        self.idx=-1\n        self.undo_lst=deque([],maxlen=maxlen)\n        self.redo_lst=deque([],maxlen=maxlen)\n        self.state=None\n\n    def undo(self):\n        cur=None\n        if len(self.undo_lst):\n            cur=self.undo_lst.pop()\n            self.redo_lst.appendleft(cur)\n        return cur\n    def redo(self):\n        cur=None\n        if len(self.redo_lst):\n            cur=self.redo_lst.popleft()\n            self.undo_lst.append(cur)\n        return cur\n\n    def check(self):\n        return len(self.undo_lst)>0,len(self.redo_lst)>0\n\n    def append(self,state,update=True):\n        self.redo_lst.clear()\n        self.undo_lst.append(state)\n        if update:\n            update_undo_redo(*self.check())\n\nhistory = History()\n\nbase_lst = [None]\nasync def draw_canvas() -> None:\n    width=1024\n    height=600\n    canvas=InfCanvas(1024,600)\n    update_eraser(canvas.eraser_size,min(canvas.selection_size_h,canvas.selection_size_w))\n    document.querySelector(\"#container\").style.height= f\"{height}px\"\n    document.querySelector(\"#container\").style.width = f\"{width}px\"\n    canvas.setup_mouse()\n    canvas.clear_background()\n    canvas.draw_buffer()\n    canvas.draw_selection_box()\n    base_lst[0]=canvas\n\nasync def draw_canvas_func(event):\n    try:\n        app=parent.document.querySelector(\"gradio-app\")\n        if app.shadowRoot:\n            app=app.shadowRoot\n        width=app.querySelector(\"#canvas_width input\").value\n        height=app.querySelector(\"#canvas_height input\").value\n        selection_size=app.querySelector(\"#selection_size input\").value\n    except:\n        width=1024\n        height=768\n        selection_size=384\n    document.querySelector(\"#container\").style.width = f\"{width}px\"\n    document.querySelector(\"#container\").style.height= f\"{height}px\"\n    canvas=InfCanvas(int(width),int(height),selection_size=int(selection_size))\n    canvas.setup_mouse()\n    canvas.clear_background()\n    canvas.draw_buffer()\n    canvas.draw_selection_box()\n    base_lst[0]=canvas\n  \nasync def export_func(event):\n    base=base_lst[0]\n    arr=base.export()\n    base.draw_buffer()\n    base.canvas[2].clear()\n    base64_str = base.numpy_to_base64(arr)\n    time_str = time.strftime(\"%Y%m%d_%H%M%S\")\n    link = document.createElement(\"a\")\n    if len(event.data)>2 and event.data[2]:\n        filename = event.data[2]\n    else:\n        filename = f\"outpaint_{time_str}\"\n    # link.download = f\"sdinf_state_{time_str}.json\"\n    link.download = f\"{filename}.png\"\n    # link.download = f\"outpaint_{time_str}.png\"\n    link.href = \"data:image/png;base64,\"+base64_str\n    link.click()\n    console.log(f\"Canvas saved to {filename}.png\")\n\nimg_candidate_lst=[None,0]\n\nasync def outpaint_func(event):\n    base=base_lst[0]\n    if len(event.data)==2:\n        app=parent.document.querySelector(\"gradio-app\")\n        if app.shadowRoot:\n            app=app.shadowRoot\n        base64_str_raw=app.querySelector(\"#output textarea\").value\n        base64_str_lst=base64_str_raw.split(\",\")\n        img_candidate_lst[0]=base64_str_lst\n        img_candidate_lst[1]=0\n    elif event.data[2]==\"next\":\n        img_candidate_lst[1]+=1\n    elif event.data[2]==\"prev\":\n        img_candidate_lst[1]-=1\n    enable_result_lst()\n    if img_candidate_lst[0] is None:\n        return\n    lst=img_candidate_lst[0]\n    idx=img_candidate_lst[1]\n    update_count(idx%len(lst)+1,len(lst))\n    arr=base.base64_to_numpy(lst[idx%len(lst)])\n    base.fill_selection(arr)\n    base.draw_selection_box()\n\nasync def undo_func(event):\n    base=base_lst[0]\n    img_candidate_lst[0]=None\n    if base.sel_dirty:\n        base.sel_buffer = np.zeros((base.selection_size_h, base.selection_size_w, 4), dtype=np.uint8)\n        base.sel_dirty = False\n    base.canvas[2].clear()\n\nasync def commit_func(event):\n    base=base_lst[0]\n    img_candidate_lst[0]=None\n    if base.sel_dirty:\n        base.write_selection_to_buffer()\n        base.draw_buffer()\n    base.canvas[2].clear()\n    if len(event.data)>2:\n        history.append(base.save())\n\nasync def history_undo_func(event):\n    base=base_lst[0]\n    if base.buffer_dirty or len(history.redo_lst)>0:\n        state=history.undo()\n    else:\n        history.undo()\n        state=history.undo()\n    if state is not None:\n        base.load(state)\n    update_undo_redo(*history.check())\n    \nasync def history_setup_func(event):\n    base=base_lst[0]\n    history.undo_lst.clear()\n    history.redo_lst.clear()\n    history.append(base.save(),update=False)\n\nasync def history_redo_func(event):\n    base=base_lst[0]\n    if len(history.undo_lst)>0:\n        state=history.redo()\n    else:\n        history.redo()\n        state=history.redo()\n    if state is not None:\n        base.load(state)\n    update_undo_redo(*history.check())\n\n\nasync def transfer_func(event):\n    base=base_lst[0]\n    base.read_selection_from_buffer()\n    sel_buffer=base.sel_buffer\n    sel_buffer_str=base.numpy_to_base64(sel_buffer)\n    app=parent.document.querySelector(\"gradio-app\")\n    if app.shadowRoot:\n        app=app.shadowRoot\n    app.querySelector(\"#input textarea\").value=sel_buffer_str\n    app.querySelector(\"#proceed\").click()\n\nasync def upload_func(event):\n    base=base_lst[0]\n    # base64_str=event.data[1]\n    base64_str=document.querySelector(\"#upload_content\").value\n    base64_str=base64_str.split(\",\")[-1]\n    # base64_str=parent.document.querySelector(\"gradio-app\").shadowRoot.querySelector(\"#upload textarea\").value\n    arr=base.base64_to_numpy(base64_str)\n    h,w,c=base.buffer.shape\n    base.sync_to_buffer()\n    base.buffer_dirty=True\n    mask=arr[:,:,3:4].repeat(4,axis=2)\n    base.buffer[mask>0]=0\n    # in case mismatch\n    base.buffer[0:h,0:w,:]+=arr\n    #base.buffer[yo:yo+h,xo:xo+w,0:3]=arr[:,:,0:3]\n    #base.buffer[yo:yo+h,xo:xo+w,-1]=arr[:,:,-1]\n    base.draw_buffer()\n    if len(event.data)>2:\n        history.append(base.save())\n\nasync def setup_shortcut_func(event):\n    setup_shortcut(event.data[1])\n  \n\ndocument.querySelector(\"#export\").addEventListener(\"click\",create_proxy(export_func))\ndocument.querySelector(\"#undo\").addEventListener(\"click\",create_proxy(undo_func))\ndocument.querySelector(\"#commit\").addEventListener(\"click\",create_proxy(commit_func))\ndocument.querySelector(\"#outpaint\").addEventListener(\"click\",create_proxy(outpaint_func))\ndocument.querySelector(\"#upload\").addEventListener(\"click\",create_proxy(upload_func))\n\ndocument.querySelector(\"#transfer\").addEventListener(\"click\",create_proxy(transfer_func))\ndocument.querySelector(\"#draw\").addEventListener(\"click\",create_proxy(draw_canvas_func))\n\nasync def setup_func():\n    document.querySelector(\"#setup\").value=\"1\"\n\nasync def reset_func(event):\n    base=base_lst[0]\n    base.reset()\n    \nasync def load_func(event):\n    base=base_lst[0]\n    base.load(event.data[1])\n\nasync def save_func(event):\n    base=base_lst[0]\n    json_str=base.save()\n    time_str = time.strftime(\"%Y%m%d_%H%M%S\")\n    link = document.createElement(\"a\")\n    if len(event.data)>2 and event.data[2]:\n        filename = str(event.data[2]).strip()\n    else:\n        filename = f\"outpaint_{time_str}\"\n    # link.download = f\"sdinf_state_{time_str}.json\"\n    link.download = f\"{filename}.sdinf\"\n    link.href = \"data:text/json;charset=utf-8,\"+encodeURIComponent(json_str)\n    link.click()\n\nasync def prev_result_func(event):\n    base=base_lst[0]\n    base.reset()\n\nasync def next_result_func(event):\n    base=base_lst[0]\n    base.reset()\n\nasync def zoom_in_func(event):\n    base=base_lst[0]\n    scale=base.scale\n    if scale>=0.2:\n        scale-=0.1\n        if len(event.data)>2:\n            base.update_scale(scale,int(event.data[2]),int(event.data[3]))\n        else:\n            base.update_scale(scale)\n        scale=base.scale\n        update_scale(f\"{base.width}x{base.height} ({round(100/scale)}%)\")\n\nasync def zoom_out_func(event):\n    base=base_lst[0]\n    scale=base.scale\n    if scale<10:\n        scale+=0.1\n        console.log(len(event.data))\n        if len(event.data)>2:\n            base.update_scale(scale,int(event.data[2]),int(event.data[3]))\n        else:\n            base.update_scale(scale)\n        scale=base.scale\n        update_scale(f\"{base.width}x{base.height} ({round(100/scale)}%)\")\n\nasync def sync_func(event):\n    base=base_lst[0]\n    base.sync_to_buffer()\n    base.canvas[2].clear()\n\nasync def eraser_size_func(event):\n    base=base_lst[0]\n    eraser_size=min(int(event.data[1]),min(base.selection_size_h,base.selection_size_w))\n    eraser_size=max(8,eraser_size)\n    base.eraser_size=eraser_size\n\nasync def resize_selection_func(event):\n    base=base_lst[0]\n    cursor=base.cursor\n    if len(event.data)>3:\n        console.log(event.data)\n        base.cursor[0]=int(event.data[1])\n        base.cursor[1]=int(event.data[2])\n        base.selection_size_w=int(event.data[3])//8*8\n        base.selection_size_h=int(event.data[4])//8*8\n        base.refine_selection()\n        base.draw_selection_box()\n    elif len(event.data)>2:\n        base.draw_selection_box()\n    else:\n        base.canvas[-1].clear()\n        adjust_selection(cursor[0],cursor[1],base.selection_size_w,base.selection_size_h)\n\nasync def eraser_func(event):\n    base=base_lst[0]\n    if event.data[1]!=\"eraser\":\n        base.canvas[-2].clear()\n    else:\n        x,y=base.mouse_pos\n        base.draw_eraser(x,y)\n\nasync def resize_func(event):\n    base=base_lst[0]\n    width=int(event.data[1])\n    height=int(event.data[2])\n    if width>=256 and height>=256:\n        if max(base.selection_size_h,base.selection_size_w)>min(width,height):\n            base.selection_size_h=256\n            base.selection_size_w=256\n        base.resize(width,height)\n\nasync def message_func(event):\n    if event.data[0]==\"click\":\n        if event.data[1]==\"clear\":\n            await reset_func(event)\n        elif event.data[1]==\"save\":\n            await save_func(event)\n        elif event.data[1]==\"export\":\n            await export_func(event)\n        elif event.data[1]==\"accept\":\n            await commit_func(event)\n        elif event.data[1]==\"cancel\":\n            await undo_func(event)\n        elif event.data[1]==\"zoom_in\":\n            await zoom_in_func(event)\n        elif event.data[1]==\"zoom_out\":\n            await zoom_out_func(event)\n        elif event.data[1]==\"redo\":\n            await history_redo_func(event)\n        elif event.data[1]==\"undo\":\n            await history_undo_func(event)\n        elif event.data[1]==\"history\":\n            await history_setup_func(event)\n    elif event.data[0]==\"sync\":\n        await sync_func(event)\n    elif event.data[0]==\"load\":\n        await load_func(event)\n    elif event.data[0]==\"upload\":\n        await upload_func(event)\n    elif event.data[0]==\"outpaint\":\n        await outpaint_func(event)\n    elif event.data[0]==\"mode\":\n        if event.data[1]!=\"selection\":\n            await sync_func(event)\n        await eraser_func(event)\n        document.querySelector(\"#mode\").value=event.data[1]\n    elif event.data[0]==\"transfer\":\n        await transfer_func(event)\n    elif event.data[0]==\"setup\":\n        await draw_canvas_func(event)\n    elif event.data[0]==\"eraser_size\":\n        await eraser_size_func(event)\n    elif event.data[0]==\"resize_selection\":\n        await resize_selection_func(event)\n    elif event.data[0]==\"shortcut\":\n        await setup_shortcut_func(event)\n    elif event.data[0]==\"resize\":\n        await resize_func(event)\n    \nwindow.addEventListener(\"message\",create_proxy(message_func))\n\nimport asyncio\n\n_ = await asyncio.gather(\n  setup_func()\n)\n</py-script>\n\n</body>\n</html>\n"
        },
        {
          "name": "interrogate.py",
          "type": "blob",
          "size": 5.03125,
          "content": "\"\"\"\nMIT License\n\nCopyright (c) 2022 pharmapsychotic\nhttps://github.com/pharmapsychotic/clip-interrogator/blob/main/clip_interrogator.ipynb\n\"\"\"\n\nimport numpy as np\nimport os\nimport torch\nimport torchvision.transforms as T\nimport torchvision.transforms.functional as TF\n\nfrom torch import nn\nfrom torch.nn import functional as F\nfrom torchvision import transforms\nfrom torchvision.transforms.functional import InterpolationMode\nfrom transformers import CLIPTokenizer, CLIPModel\nfrom transformers import CLIPProcessor, CLIPModel\n\ndata_path = os.path.join(os.path.dirname(os.path.realpath(__file__)), \"blip_model\", \"data\")\ndef load_list(filename):\n    with open(filename, 'r', encoding='utf-8', errors='replace') as f:\n        items = [line.strip() for line in f.readlines()]\n    return items\n\nartists = load_list(os.path.join(data_path, 'artists.txt'))\nflavors = load_list(os.path.join(data_path, 'flavors.txt'))\nmediums = load_list(os.path.join(data_path, 'mediums.txt'))\nmovements = load_list(os.path.join(data_path, 'movements.txt'))\n\nsites = ['Artstation', 'behance', 'cg society', 'cgsociety', 'deviantart', 'dribble', 'flickr', 'instagram', 'pexels', 'pinterest', 'pixabay', 'pixiv', 'polycount', 'reddit', 'shutterstock', 'tumblr', 'unsplash', 'zbrush central']\ntrending_list = [site for site in sites]\ntrending_list.extend([\"trending on \"+site for site in sites])\ntrending_list.extend([\"featured on \"+site for site in sites])\ntrending_list.extend([site+\" contest winner\" for site in sites])\n\ndevice=\"cpu\"\nblip_image_eval_size = 384\nclip_name=\"openai/clip-vit-large-patch14\"\n\nblip_model_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model*_base_caption.pth'  \n\ndef generate_caption(blip_model, pil_image, device=\"cpu\"):\n    gpu_image = transforms.Compose([\n        transforms.Resize((blip_image_eval_size, blip_image_eval_size), interpolation=InterpolationMode.BICUBIC),\n        transforms.ToTensor(),\n        transforms.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711))\n    ])(pil_image).unsqueeze(0).to(device)\n\n    with torch.no_grad():\n        caption = blip_model.generate(gpu_image, sample=False, num_beams=3, max_length=20, min_length=5)\n    return caption[0]\n\ndef rank(text_features, image_features, text_array, top_count=1):\n    top_count = min(top_count, len(text_array))\n    similarity = torch.zeros((1, len(text_array)))\n    for i in range(image_features.shape[0]):\n        similarity += (100.0 * image_features[i].unsqueeze(0) @ text_features.T).softmax(dim=-1)\n    similarity /= image_features.shape[0]\n\n    top_probs, top_labels = similarity.cpu().topk(top_count, dim=-1)  \n    return [(text_array[top_labels[0][i].numpy()], (top_probs[0][i].numpy()*100)) for i in range(top_count)]\n\nclass Interrogator:\n    def __init__(self) -> None:\n        self.tokenizer = CLIPTokenizer.from_pretrained(clip_name)\n        try:\n            self.get_blip()\n        except:\n            self.blip_model = None\n        self.model = CLIPModel.from_pretrained(clip_name)\n        self.processor = CLIPProcessor.from_pretrained(clip_name)\n        self.text_feature_lst = [torch.load(os.path.join(data_path, f\"{i}.pth\")) for i in range(5)]\n\n    def get_blip(self):\n        from blip_model.blip import blip_decoder\n        blip_model = blip_decoder(pretrained=blip_model_url, image_size=blip_image_eval_size, vit='base')\n        blip_model.eval()\n        self.blip_model = blip_model\n\n\n    def interrogate(self,image,use_caption=False):\n        if self.blip_model:\n            caption = generate_caption(self.blip_model, image)\n        else:\n            caption = \"\"\n        model,processor=self.model,self.processor\n        bests = [[('',0)]]*5\n        if True:\n            print(f\"Interrogating with {clip_name}...\")\n\n            inputs = processor(images=image, return_tensors=\"pt\")\n            with torch.no_grad():\n                image_features = model.get_image_features(**inputs)\n            image_features /= image_features.norm(dim=-1, keepdim=True)\n            ranks = [\n                rank(self.text_feature_lst[0], image_features, mediums),\n                rank(self.text_feature_lst[1], image_features, [\"by \"+artist for artist in artists]),\n                rank(self.text_feature_lst[2], image_features, trending_list),\n                rank(self.text_feature_lst[3], image_features, movements),\n                rank(self.text_feature_lst[4], image_features, flavors, top_count=3)\n            ]\n\n            for i in range(len(ranks)):\n                confidence_sum = 0\n                for ci in range(len(ranks[i])):\n                    confidence_sum += ranks[i][ci][1]\n                if confidence_sum > sum(bests[i][t][1] for t in range(len(bests[i]))):\n                    bests[i] = ranks[i]\n\n        flaves = ', '.join([f\"{x[0]}\" for x in bests[4]])\n        medium = bests[0][0][0]\n        print(ranks)\n        if caption.startswith(medium):\n            return f\"{caption} {bests[1][0][0]}, {bests[2][0][0]}, {bests[3][0][0]}, {flaves}\"\n        else:\n            return f\"{caption}, {medium} {bests[1][0][0]}, {bests[2][0][0]}, {bests[3][0][0]}, {flaves}\"\n\n\n\n\n\n\n\n\n"
        },
        {
          "name": "js",
          "type": "tree",
          "content": null
        },
        {
          "name": "models",
          "type": "tree",
          "content": null
        },
        {
          "name": "perlin2d.py",
          "type": "blob",
          "size": 1.2802734375,
          "content": "import numpy as np\n\n##########\n# https://stackoverflow.com/questions/42147776/producing-2d-perlin-noise-with-numpy/42154921#42154921\ndef perlin(x, y, seed=0):\n    # permutation table\n    np.random.seed(seed)\n    p = np.arange(256, dtype=int)\n    np.random.shuffle(p)\n    p = np.stack([p, p]).flatten()\n    # coordinates of the top-left\n    xi, yi = x.astype(int), y.astype(int)\n    # internal coordinates\n    xf, yf = x - xi, y - yi\n    # fade factors\n    u, v = fade(xf), fade(yf)\n    # noise components\n    n00 = gradient(p[p[xi] + yi], xf, yf)\n    n01 = gradient(p[p[xi] + yi + 1], xf, yf - 1)\n    n11 = gradient(p[p[xi + 1] + yi + 1], xf - 1, yf - 1)\n    n10 = gradient(p[p[xi + 1] + yi], xf - 1, yf)\n    # combine noises\n    x1 = lerp(n00, n10, u)\n    x2 = lerp(n01, n11, u)  # FIX1: I was using n10 instead of n01\n    return lerp(x1, x2, v)  # FIX2: I also had to reverse x1 and x2 here\n\n\ndef lerp(a, b, x):\n    \"linear interpolation\"\n    return a + x * (b - a)\n\n\ndef fade(t):\n    \"6t^5 - 15t^4 + 10t^3\"\n    return 6 * t ** 5 - 15 * t ** 4 + 10 * t ** 3\n\n\ndef gradient(h, x, y):\n    \"grad converts h to the right gradient vector and return the dot product with (x,y)\"\n    vectors = np.array([[0, 1], [0, -1], [1, 0], [-1, 0]])\n    g = vectors[h % 4]\n    return g[:, :, 0] * x + g[:, :, 1] * y\n\n\n##########"
        },
        {
          "name": "postprocess.py",
          "type": "blob",
          "size": 8.4267578125,
          "content": "\"\"\"\nhttps://github.com/Trinkle23897/Fast-Poisson-Image-Editing\nMIT License\n\nCopyright (c) 2022 Jiayi Weng\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n\"\"\"\n\nimport time\nimport argparse\nimport os\nimport fpie\nfrom process import ALL_BACKEND, CPU_COUNT, DEFAULT_BACKEND\nfrom fpie.io import read_images, write_image\nfrom process import BaseProcessor, EquProcessor, GridProcessor\n\nfrom PIL import Image\nimport numpy as np\nimport skimage\nimport skimage.measure\nimport scipy\nimport scipy.signal\n\n\nclass PhotometricCorrection:\n    def __init__(self,quite=False):\n        self.get_parser(\"cli\")\n        args=self.parser.parse_args([\"--method\",\"grid\",\"-g\",\"src\",\"-s\",\"a\",\"-t\",\"a\",\"-o\",\"a\"])\n        args.mpi_sync_interval = getattr(args, \"mpi_sync_interval\", 0)\n        self.backend=args.backend\n        self.args=args\n        self.quite=quite\n        proc: BaseProcessor\n        proc = GridProcessor(\n            args.gradient,\n            args.backend,\n            args.cpu,\n            args.mpi_sync_interval,\n            args.block_size,\n            args.grid_x,\n            args.grid_y,\n        )\n        print(\n            f\"[PIE]Successfully initialize PIE {args.method} solver \"\n            f\"with {args.backend} backend\"\n        )\n        self.proc=proc\n\n    def run(self, original_image, inpainted_image, mode=\"mask_mode\"):\n        print(f\"[PIE] start\")\n        if mode==\"disabled\":\n            return inpainted_image\n        input_arr=np.array(original_image)\n        if input_arr[:,:,-1].sum()<1:\n            return inpainted_image\n        output_arr=np.array(inpainted_image)\n        mask=input_arr[:,:,-1]\n        mask=255-mask\n        if mask.sum()<1 and mode==\"mask_mode\":\n            mode=\"\"\n        if mode==\"mask_mode\":\n            mask = skimage.measure.block_reduce(mask, (8, 8), np.max)\n            mask = mask.repeat(8, axis=0).repeat(8, axis=1)\n        else:\n            mask[8:-9,8:-9]=255\n        mask = mask[:,:,np.newaxis].repeat(3,axis=2)\n        nmask=mask.copy()\n        output_arr2=output_arr[:,:,0:3].copy()\n        input_arr2=input_arr[:,:,0:3].copy()\n        output_arr2[nmask<128]=0\n        input_arr2[nmask>=128]=0\n        output_arr2+=input_arr2\n        src = output_arr2[:,:,0:3]\n        tgt = src.copy()\n        proc=self.proc\n        args=self.args\n        if proc.root:\n            n = proc.reset(src, mask, tgt, (args.h0, args.w0), (args.h1, args.w1))\n        proc.sync()\n        if proc.root:\n            result = tgt\n            t = time.time()\n        if args.p == 0:\n            args.p = args.n\n\n        for i in range(0, args.n, args.p):\n            if proc.root:\n                result, err = proc.step(args.p)  # type: ignore\n                print(f\"[PIE] Iter {i + args.p}, abs_err {err}\")\n            else:\n                proc.step(args.p)\n\n        if proc.root:\n            dt = time.time() - t\n            print(f\"[PIE] Time elapsed: {dt:.4f}s\")\n            # make sure consistent with dummy process\n            return Image.fromarray(result)\n\n\n    def get_parser(self,gen_type: str) -> argparse.Namespace:\n        parser = argparse.ArgumentParser()\n        parser.add_argument(\n            \"-v\", \"--version\", action=\"store_true\", help=\"show the version and exit\"\n        )\n        parser.add_argument(\n            \"--check-backend\", action=\"store_true\", help=\"print all available backends\"\n        )\n        if gen_type == \"gui\" and \"mpi\" in ALL_BACKEND:\n            # gui doesn't support MPI backend\n            ALL_BACKEND.remove(\"mpi\")\n        parser.add_argument(\n            \"-b\",\n            \"--backend\",\n            type=str,\n            choices=ALL_BACKEND,\n            default=DEFAULT_BACKEND,\n            help=\"backend choice\",\n        )\n        parser.add_argument(\n            \"-c\",\n            \"--cpu\",\n            type=int,\n            default=CPU_COUNT,\n            help=\"number of CPU used\",\n        )\n        parser.add_argument(\n            \"-z\",\n            \"--block-size\",\n            type=int,\n            default=1024,\n            help=\"cuda block size (only for equ solver)\",\n        )\n        parser.add_argument(\n            \"--method\",\n            type=str,\n            choices=[\"equ\", \"grid\"],\n            default=\"equ\",\n            help=\"how to parallelize computation\",\n        )\n        parser.add_argument(\"-s\", \"--source\", type=str, help=\"source image filename\")\n        if gen_type == \"cli\":\n            parser.add_argument(\n                \"-m\",\n                \"--mask\",\n                type=str,\n                help=\"mask image filename (default is to use the whole source image)\",\n                default=\"\",\n            )\n        parser.add_argument(\"-t\", \"--target\", type=str, help=\"target image filename\")\n        parser.add_argument(\"-o\", \"--output\", type=str, help=\"output image filename\")\n        if gen_type == \"cli\":\n            parser.add_argument(\n                \"-h0\", type=int, help=\"mask position (height) on source image\", default=0\n            )\n            parser.add_argument(\n                \"-w0\", type=int, help=\"mask position (width) on source image\", default=0\n            )\n            parser.add_argument(\n                \"-h1\", type=int, help=\"mask position (height) on target image\", default=0\n            )\n            parser.add_argument(\n                \"-w1\", type=int, help=\"mask position (width) on target image\", default=0\n            )\n        parser.add_argument(\n            \"-g\",\n            \"--gradient\",\n            type=str,\n            choices=[\"max\", \"src\", \"avg\"],\n            default=\"max\",\n            help=\"how to calculate gradient for PIE\",\n        )\n        parser.add_argument(\n            \"-n\",\n            type=int,\n            help=\"how many iteration would you perfer, the more the better\",\n            default=5000,\n        )\n        if gen_type == \"cli\":\n            parser.add_argument(\n                \"-p\", type=int, help=\"output result every P iteration\", default=0\n            )\n        if \"mpi\" in ALL_BACKEND:\n            parser.add_argument(\n                \"--mpi-sync-interval\",\n                type=int,\n                help=\"MPI sync iteration interval\",\n                default=100,\n            )\n        parser.add_argument(\n            \"--grid-x\", type=int, help=\"x axis stride for grid solver\", default=8\n        )\n        parser.add_argument(\n            \"--grid-y\", type=int, help=\"y axis stride for grid solver\", default=8\n        )\n        self.parser=parser\n\nif __name__ ==\"__main__\":\n    import sys\n    import io\n    import base64\n    from PIL import Image\n    def base64_to_pil(base64_str):\n        data = base64.b64decode(str(base64_str))\n        pil = Image.open(io.BytesIO(data))\n        return pil\n\n    def pil_to_base64(out_pil):\n        out_buffer = io.BytesIO()\n        out_pil.save(out_buffer, format=\"PNG\")\n        out_buffer.seek(0)\n        base64_bytes = base64.b64encode(out_buffer.read())\n        base64_str = base64_bytes.decode(\"ascii\")\n        return base64_str\n    correction_func=PhotometricCorrection(quite=True)\n    while True:\n        buffer = sys.stdin.readline()\n        print(f\"[PIE] suprocess {len(buffer)} {type(buffer)} \")\n        if len(buffer)==0:\n            break\n        if isinstance(buffer,str):\n            lst=buffer.strip().split(\",\")\n        else:\n            lst=buffer.decode(\"ascii\").strip().split(\",\")\n        img0=base64_to_pil(lst[0])\n        img1=base64_to_pil(lst[1])\n        ret=correction_func.run(img0,img1,mode=lst[2])\n        ret_base64=pil_to_base64(ret)\n        if isinstance(buffer,str):\n            sys.stdout.write(f\"{ret_base64}\\n\")\n        else:\n            sys.stdout.write(f\"{ret_base64}\\n\".encode())\n        sys.stdout.flush()"
        },
        {
          "name": "process.py",
          "type": "blob",
          "size": 12.1904296875,
          "content": "\"\"\"\nhttps://github.com/Trinkle23897/Fast-Poisson-Image-Editing\nMIT License\n\nCopyright (c) 2022 Jiayi Weng\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n\"\"\"\nimport os\nfrom abc import ABC, abstractmethod\nfrom typing import Any, Optional, Tuple\n\nimport numpy as np\n\nfrom fpie import np_solver\n\nimport scipy\nimport scipy.signal\n\nCPU_COUNT = os.cpu_count() or 1\nDEFAULT_BACKEND = \"numpy\"\nALL_BACKEND = [\"numpy\"]\n\ntry:\n  from fpie import numba_solver\n  ALL_BACKEND += [\"numba\"]\n  DEFAULT_BACKEND = \"numba\"\nexcept ImportError:\n  numba_solver = None  # type: ignore\n\ntry:\n  from fpie import taichi_solver\n  ALL_BACKEND += [\"taichi-cpu\", \"taichi-gpu\"]\n  DEFAULT_BACKEND = \"taichi-cpu\"\nexcept ImportError:\n  taichi_solver = None  # type: ignore\n\n# try:\n#   from fpie import core_gcc  # type: ignore\n#   DEFAULT_BACKEND = \"gcc\"\n#   ALL_BACKEND.append(\"gcc\")\n# except ImportError:\n#   core_gcc = None\n\n# try:\n#   from fpie import core_openmp  # type: ignore\n#   DEFAULT_BACKEND = \"openmp\"\n#   ALL_BACKEND.append(\"openmp\")\n# except ImportError:\n#   core_openmp = None\n\n# try:\n#   from mpi4py import MPI\n\n#   from fpie import core_mpi  # type: ignore\n#   ALL_BACKEND.append(\"mpi\")\n# except ImportError:\n#   MPI = None  # type: ignore\n#   core_mpi = None\n\ntry:\n  from fpie import core_cuda  # type: ignore\n  DEFAULT_BACKEND = \"cuda\"\n  ALL_BACKEND.append(\"cuda\")\nexcept ImportError:\n  core_cuda = None\n\n\nclass BaseProcessor(ABC):\n  \"\"\"API definition for processor class.\"\"\"\n\n  def __init__(\n    self, gradient: str, rank: int, backend: str, core: Optional[Any]\n  ):\n    if core is None:\n      error_msg = {\n        \"numpy\":\n          \"Please run `pip install numpy`.\",\n        \"numba\":\n          \"Please run `pip install numba`.\",\n        \"gcc\":\n          \"Please install cmake and gcc in your operating system.\",\n        \"openmp\":\n          \"Please make sure your gcc is compatible with `-fopenmp` option.\",\n        \"mpi\":\n          \"Please install MPI and run `pip install mpi4py`.\",\n        \"cuda\":\n          \"Please make sure nvcc and cuda-related libraries are available.\",\n        \"taichi\":\n          \"Please run `pip install taichi`.\",\n      }\n      print(error_msg[backend.split(\"-\")[0]])\n\n      raise AssertionError(f\"Invalid backend {backend}.\")\n\n    self.gradient = gradient\n    self.rank = rank\n    self.backend = backend\n    self.core = core\n    self.root = rank == 0\n\n  def mixgrad(self, a: np.ndarray, b: np.ndarray) -> np.ndarray:\n    if self.gradient == \"src\":\n      return a\n    if self.gradient == \"avg\":\n      return (a + b) / 2\n    # mix gradient, see Equ. 12 in PIE paper\n    mask = np.abs(a) < np.abs(b)\n    a[mask] = b[mask]\n    return a\n\n  @abstractmethod\n  def reset(\n    self,\n    src: np.ndarray,\n    mask: np.ndarray,\n    tgt: np.ndarray,\n    mask_on_src: Tuple[int, int],\n    mask_on_tgt: Tuple[int, int],\n  ) -> int:\n    pass\n\n  def sync(self) -> None:\n    self.core.sync()\n\n  @abstractmethod\n  def step(self, iteration: int) -> Optional[Tuple[np.ndarray, np.ndarray]]:\n    pass\n\n\nclass EquProcessor(BaseProcessor):\n  \"\"\"PIE Jacobi equation processor.\"\"\"\n\n  def __init__(\n    self,\n    gradient: str = \"max\",\n    backend: str = DEFAULT_BACKEND,\n    n_cpu: int = CPU_COUNT,\n    min_interval: int = 100,\n    block_size: int = 1024,\n  ):\n    core: Optional[Any] = None\n    rank = 0\n\n    if backend == \"numpy\":\n      core = np_solver.EquSolver()\n    elif backend == \"numba\" and numba_solver is not None:\n      core = numba_solver.EquSolver()\n    elif backend == \"gcc\":\n      core = core_gcc.EquSolver()\n    elif backend == \"openmp\" and core_openmp is not None:\n      core = core_openmp.EquSolver(n_cpu)\n    elif backend == \"mpi\" and core_mpi is not None:\n      core = core_mpi.EquSolver(min_interval)\n      rank = MPI.COMM_WORLD.Get_rank()\n    elif backend == \"cuda\" and core_cuda is not None:\n      core = core_cuda.EquSolver(block_size)\n    elif backend.startswith(\"taichi\") and taichi_solver is not None:\n      core = taichi_solver.EquSolver(backend, n_cpu, block_size)\n\n    super().__init__(gradient, rank, backend, core)\n\n  def mask2index(\n    self, mask: np.ndarray\n  ) -> Tuple[np.ndarray, int, np.ndarray, np.ndarray]:\n    x, y = np.nonzero(mask)\n    max_id = x.shape[0] + 1\n    index = np.zeros((max_id, 3))\n    ids = self.core.partition(mask)\n    ids[mask == 0] = 0  # reserve id=0 for constant\n    index = ids[x, y].argsort()\n    return ids, max_id, x[index], y[index]\n\n  def reset(\n    self,\n    src: np.ndarray,\n    mask: np.ndarray,\n    tgt: np.ndarray,\n    mask_on_src: Tuple[int, int],\n    mask_on_tgt: Tuple[int, int],\n  ) -> int:\n    assert self.root\n    # check validity\n    # assert 0 <= mask_on_src[0] and 0 <= mask_on_src[1]\n    # assert mask_on_src[0] + mask.shape[0] <= src.shape[0]\n    # assert mask_on_src[1] + mask.shape[1] <= src.shape[1]\n    # assert mask_on_tgt[0] + mask.shape[0] <= tgt.shape[0]\n    # assert mask_on_tgt[1] + mask.shape[1] <= tgt.shape[1]\n\n    if len(mask.shape) == 3:\n      mask = mask.mean(-1)\n    mask = (mask >= 128).astype(np.int32)\n\n    # zero-out edge\n    mask[0] = 0\n    mask[-1] = 0\n    mask[:, 0] = 0\n    mask[:, -1] = 0\n\n    x, y = np.nonzero(mask)\n    x0, x1 = x.min() - 1, x.max() + 2\n    y0, y1 = y.min() - 1, y.max() + 2\n    mask_on_src = (x0 + mask_on_src[0], y0 + mask_on_src[1])\n    mask_on_tgt = (x0 + mask_on_tgt[0], y0 + mask_on_tgt[1])\n    mask = mask[x0:x1, y0:y1]\n    ids, max_id, index_x, index_y = self.mask2index(mask)\n\n    src_x, src_y = index_x + mask_on_src[0], index_y + mask_on_src[1]\n    tgt_x, tgt_y = index_x + mask_on_tgt[0], index_y + mask_on_tgt[1]\n\n    src_C = src[src_x, src_y].astype(np.float32)\n    src_U = src[src_x - 1, src_y].astype(np.float32)\n    src_D = src[src_x + 1, src_y].astype(np.float32)\n    src_L = src[src_x, src_y - 1].astype(np.float32)\n    src_R = src[src_x, src_y + 1].astype(np.float32)\n    tgt_C = tgt[tgt_x, tgt_y].astype(np.float32)\n    tgt_U = tgt[tgt_x - 1, tgt_y].astype(np.float32)\n    tgt_D = tgt[tgt_x + 1, tgt_y].astype(np.float32)\n    tgt_L = tgt[tgt_x, tgt_y - 1].astype(np.float32)\n    tgt_R = tgt[tgt_x, tgt_y + 1].astype(np.float32)\n\n    grad = self.mixgrad(src_C - src_L, tgt_C - tgt_L) \\\n      + self.mixgrad(src_C - src_R, tgt_C - tgt_R) \\\n      + self.mixgrad(src_C - src_U, tgt_C - tgt_U) \\\n      + self.mixgrad(src_C - src_D, tgt_C - tgt_D)\n\n    A = np.zeros((max_id, 4), np.int32)\n    X = np.zeros((max_id, 3), np.float32)\n    B = np.zeros((max_id, 3), np.float32)\n\n    X[1:] = tgt[index_x + mask_on_tgt[0], index_y + mask_on_tgt[1]]\n    # four-way\n    A[1:, 0] = ids[index_x - 1, index_y]\n    A[1:, 1] = ids[index_x + 1, index_y]\n    A[1:, 2] = ids[index_x, index_y - 1]\n    A[1:, 3] = ids[index_x, index_y + 1]\n    B[1:] = grad\n    m = (mask[index_x - 1, index_y] == 0).astype(float).reshape(-1, 1)\n    B[1:] += m * tgt[index_x + mask_on_tgt[0] - 1, index_y + mask_on_tgt[1]]\n    m = (mask[index_x, index_y - 1] == 0).astype(float).reshape(-1, 1)\n    B[1:] += m * tgt[index_x + mask_on_tgt[0], index_y + mask_on_tgt[1] - 1]\n    m = (mask[index_x, index_y + 1] == 0).astype(float).reshape(-1, 1)\n    B[1:] += m * tgt[index_x + mask_on_tgt[0], index_y + mask_on_tgt[1] + 1]\n    m = (mask[index_x + 1, index_y] == 0).astype(float).reshape(-1, 1)\n    B[1:] += m * tgt[index_x + mask_on_tgt[0] + 1, index_y + mask_on_tgt[1]]\n\n    self.tgt = tgt.copy()\n    self.tgt_index = (index_x + mask_on_tgt[0], index_y + mask_on_tgt[1])\n    self.core.reset(max_id, A, X, B)\n    return max_id\n\n  def step(self, iteration: int) -> Optional[Tuple[np.ndarray, np.ndarray]]:\n    result = self.core.step(iteration)\n    if self.root:\n      x, err = result\n      self.tgt[self.tgt_index] = x[1:]\n      return self.tgt, err\n    return None\n\n\nclass GridProcessor(BaseProcessor):\n  \"\"\"PIE grid processor.\"\"\"\n\n  def __init__(\n    self,\n    gradient: str = \"max\",\n    backend: str = DEFAULT_BACKEND,\n    n_cpu: int = CPU_COUNT,\n    min_interval: int = 100,\n    block_size: int = 1024,\n    grid_x: int = 8,\n    grid_y: int = 8,\n  ):\n    core: Optional[Any] = None\n    rank = 0\n\n    if backend == \"numpy\":\n      core = np_solver.GridSolver()\n    elif backend == \"numba\" and numba_solver is not None:\n      core = numba_solver.GridSolver()\n    elif backend == \"gcc\":\n      core = core_gcc.GridSolver(grid_x, grid_y)\n    elif backend == \"openmp\" and core_openmp is not None:\n      core = core_openmp.GridSolver(grid_x, grid_y, n_cpu)\n    elif backend == \"mpi\" and core_mpi is not None:\n      core = core_mpi.GridSolver(min_interval)\n      rank = MPI.COMM_WORLD.Get_rank()\n    elif backend == \"cuda\" and core_cuda is not None:\n      core = core_cuda.GridSolver(grid_x, grid_y)\n    elif backend.startswith(\"taichi\") and taichi_solver is not None:\n      core = taichi_solver.GridSolver(\n        grid_x, grid_y, backend, n_cpu, block_size\n      )\n\n    super().__init__(gradient, rank, backend, core)\n\n  def reset(\n    self,\n    src: np.ndarray,\n    mask: np.ndarray,\n    tgt: np.ndarray,\n    mask_on_src: Tuple[int, int],\n    mask_on_tgt: Tuple[int, int],\n  ) -> int:\n    assert self.root\n    # check validity\n    # assert 0 <= mask_on_src[0] and 0 <= mask_on_src[1]\n    # assert mask_on_src[0] + mask.shape[0] <= src.shape[0]\n    # assert mask_on_src[1] + mask.shape[1] <= src.shape[1]\n    # assert mask_on_tgt[0] + mask.shape[0] <= tgt.shape[0]\n    # assert mask_on_tgt[1] + mask.shape[1] <= tgt.shape[1]\n\n    if len(mask.shape) == 3:\n      mask = mask.mean(-1)\n    mask = (mask >= 128).astype(np.int32)\n\n    # zero-out edge\n    mask[0] = 0\n    mask[-1] = 0\n    mask[:, 0] = 0\n    mask[:, -1] = 0\n\n    x, y = np.nonzero(mask)\n    x0, x1 = x.min() - 1, x.max() + 2\n    y0, y1 = y.min() - 1, y.max() + 2\n    mask = mask[x0:x1, y0:y1]\n    max_id = np.prod(mask.shape)\n\n    src_crop = src[mask_on_src[0] + x0:mask_on_src[0] + x1,\n                   mask_on_src[1] + y0:mask_on_src[1] + y1].astype(np.float32)\n    tgt_crop = tgt[mask_on_tgt[0] + x0:mask_on_tgt[0] + x1,\n                   mask_on_tgt[1] + y0:mask_on_tgt[1] + y1].astype(np.float32)\n    grad = np.zeros([*mask.shape, 3], np.float32)\n    grad[1:] += self.mixgrad(\n      src_crop[1:] - src_crop[:-1], tgt_crop[1:] - tgt_crop[:-1]\n    )\n    grad[:-1] += self.mixgrad(\n      src_crop[:-1] - src_crop[1:], tgt_crop[:-1] - tgt_crop[1:]\n    )\n    grad[:, 1:] += self.mixgrad(\n      src_crop[:, 1:] - src_crop[:, :-1], tgt_crop[:, 1:] - tgt_crop[:, :-1]\n    )\n    grad[:, :-1] += self.mixgrad(\n      src_crop[:, :-1] - src_crop[:, 1:], tgt_crop[:, :-1] - tgt_crop[:, 1:]\n    )\n\n    grad[mask == 0] = 0\n    if True:\n        kernel = [[1] * 3 for _ in range(3)]\n        nmask = mask.copy()\n        nmask[nmask > 0] = 1\n        res = scipy.signal.convolve2d(\n            nmask, kernel, mode=\"same\", boundary=\"fill\", fillvalue=1\n        )\n        res[nmask < 1] = 0\n        res[res == 9] = 0\n        res[res > 0] = 1\n        grad[res>0]=0\n        # ylst, xlst = res.nonzero()\n        # for y, x in zip(ylst, xlst):\n        #     grad[y,x]=0\n            # for yi in range(-1,2):\n                # for xi in range(-1,2):\n                    # grad[y+yi,x+xi]=0\n    self.x0 = mask_on_tgt[0] + x0\n    self.x1 = mask_on_tgt[0] + x1\n    self.y0 = mask_on_tgt[1] + y0\n    self.y1 = mask_on_tgt[1] + y1\n    self.tgt = tgt.copy()\n    self.core.reset(max_id, mask, tgt_crop, grad)\n    return max_id\n\n  def step(self, iteration: int) -> Optional[Tuple[np.ndarray, np.ndarray]]:\n    result = self.core.step(iteration)\n    if self.root:\n      tgt, err = result\n      self.tgt[self.x0:self.x1, self.y0:self.y1] = tgt\n      return self.tgt, err\n    return None\n"
        },
        {
          "name": "readme.md",
          "type": "blob",
          "size": 4.5810546875,
          "content": "# stablediffusion-infinity\n\nOutpainting with Stable Diffusion on an infinite canvas.\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/lkwq007/stablediffusion-infinity/blob/master/stablediffusion_infinity_colab.ipynb)\n[![Hugging Face Spaces](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue)](https://huggingface.co/spaces/lnyan/stablediffusion-infinity)\n[![Setup Locally](https://img.shields.io/badge/%F0%9F%96%A5%EF%B8%8F%20Setup-Locally-blue)](https://github.com/lkwq007/stablediffusion-infinity/blob/master/docs/setup_guide.md)\n\n![outpaint](https://user-images.githubusercontent.com/1665437/197257616-82c1e58f-7463-4896-8345-6750a828c844.png)\n\nhttps://user-images.githubusercontent.com/1665437/197244111-51884b3b-dffe-4dcf-a82a-fa5117c79934.mp4\n\n## Status\n\nPowered by Stable Diffusion inpainting model, this project now works well. However, the quality of results is still not guaranteed.\nYou may need to do prompt engineering, change the size of the selection, reduce the size of the outpainting region to get better outpainting results. \n\nThe project now becomes a web app based on PyScript and Gradio. For Jupyter Notebook version, please check out the [ipycanvas](https://github.com/lkwq007/stablediffusion-infinity/tree/ipycanvas) branch. \n\nPull requests are welcome for better UI control, ideas to achieve better results, or any other improvements.\n\nUpdate: the project add photometric correction to suppress seams, to use this feature, you need to install [fpie](https://github.com/Trinkle23897/Fast-Poisson-Image-Editing): `pip install fpie` (Linux/MacOS only)\n\n## Docs\n\n### Get Started\n\n- Setup for Windows: [setup_guide](./docs/setup_guide.md#windows)\n- Setup for Linux: [setup_guide](./docs/setup_guide.md#linux)\n- Setup for MacOS: [setup_guide](./docs/setup_guide.md#macos)\n- Running with Docker on Windows or Linux with NVIDIA GPU: [run_with_docker](./docs/run_with_docker.md)\n- Usages: [usage](./docs/usage.md)\n\n### FAQs\n\n- The result is a black square: \n  - False positive rate of safety checker is relatively high, you may disable the safety_checker\n  - Some GPUs might not work with `fp16`: `python app.py --fp32 --lowvram`\n- What is the init_mode\n  - init_mode indicates how to fill the empty/masked region, usually `patch_match` is better than others\n- Why not use `postMessage` for iframe interaction\n  - The iframe and the gradio are in the same origin. For `postMessage` version, check out [gradio-space](https://github.com/lkwq007/stablediffusion-infinity/tree/gradio-space) version\n\n### Known issues\n\n- The canvas is implemented with `NumPy` + `PyScript` (the project was originally implemented with `ipycanvas` inside a jupyter notebook), which is relatively inefficient compared with pure frontend solutions. \n- By design, the canvas is infinite. However, the canvas size is **finite** in practice. Your RAM and browser limit the canvas size. The canvas might crash or behave strangely when zoomed out by a certain scale. \n- The canvas requires internet: You can deploy and serve PyScript, Pyodide, and other JS/CSS assets with a local HTTP server and modify `index.html` accordingly. \n- Photometric correction might not work (`taichi` does not support the multithreading environment). A dirty hack (quite unreliable) is implemented to move related computation inside a subprocess. \n- Stable Diffusion inpainting model is much slower when selection size is larger than 512x512\n\n## Credit\n\nThe code of `perlin2d.py` is from https://stackoverflow.com/questions/42147776/producing-2d-perlin-noise-with-numpy/42154921#42154921 and is **not** included in the scope of LICENSE used in this repo.\n\nThe submodule `glid_3_xl_stable` is based on https://github.com/Jack000/glid-3-xl-stable \n\nThe submodule `PyPatchMatch` is based on https://github.com/vacancy/PyPatchMatch\n\nThe code of `postprocess.py` and `process.py` is modified based on https://github.com/Trinkle23897/Fast-Poisson-Image-Editing\n\nThe code of `convert_checkpoint.py` is modified based on https://github.com/huggingface/diffusers/blob/main/scripts/convert_original_stable_diffusion_to_diffusers.py\n\nThe submodule `sd_grpcserver` and `handleImageAdjustment()` in `utils.py` are based on https://github.com/hafriedlander/stable-diffusion-grpcserver and https://github.com/parlance-zz/g-diffuser-bot\n\n`w2ui.min.js` and `w2ui.min.css` is from https://github.com/vitmalina/w2ui. `fabric.min.js` is a custom build of https://github.com/fabricjs/fabric.js\n\n`interrogate.py` is based on https://github.com/pharmapsychotic/clip-interrogator v1, the submodule `blip_model` is based on https://github.com/salesforce/BLIP \n"
        },
        {
          "name": "sd_grpcserver",
          "type": "commit",
          "content": null
        },
        {
          "name": "stablediffusion_infinity_colab.ipynb",
          "type": "blob",
          "size": 2.111328125,
          "content": "{\n  \"nbformat\": 4,\n  \"nbformat_minor\": 0,\n  \"metadata\": {\n    \"colab\": {\n      \"provenance\": [],\n      \"collapsed_sections\": []\n    },\n    \"kernelspec\": {\n      \"name\": \"python3\",\n      \"display_name\": \"Python 3\"\n    },\n    \"language_info\": {\n      \"name\": \"python\"\n    },\n    \"accelerator\": \"GPU\"\n  },\n  \"cells\": [\n    {\n      \"cell_type\": \"markdown\",\n      \"source\": [\n        \"# stablediffusion-infinity\\n\",\n        \"\\n\",\n        \"https://github.com/lkwq007/stablediffusion-infinity\\n\",\n        \"\\n\",\n        \"Outpainting with Stable Diffusion on an infinite canvas\"\n      ],\n      \"metadata\": {\n        \"id\": \"IgN1jqV_DemW\"\n      }\n    },\n    {\n      \"cell_type\": \"code\",\n      \"execution_count\": null,\n      \"metadata\": {\n        \"id\": \"JvbfNNSJDTW5\"\n      },\n      \"outputs\": [],\n      \"source\": [\n        \"#@title setup libs\\n\",\n        \"!nvidia-smi -L\\n\",\n        \"!pip install -qq -U diffusers==0.11.1 transformers ftfy accelerate\\n\",\n        \"!pip install -q gradio==3.11.0\\n\",\n        \"!pip install -q fpie timm\\n\",\n        \"!pip uninstall taichi -y\"\n      ]\n    },\n    {\n      \"cell_type\": \"code\",\n      \"source\": [\n        \"#@title setup stablediffusion-infinity\\n\",\n        \"!git clone --recurse-submodules https://github.com/lkwq007/stablediffusion-infinity\\n\",\n        \"%cd stablediffusion-infinity\\n\",\n        \"!cp -r PyPatchMatch/csrc .\\n\",\n        \"!cp PyPatchMatch/Makefile .\\n\",\n        \"!cp PyPatchMatch/Makefile_fallback .\\n\",\n        \"!cp PyPatchMatch/travis.sh .\\n\",\n        \"!cp PyPatchMatch/patch_match.py . \"\n      ],\n      \"metadata\": {\n        \"id\": \"D1BDhQCJDilE\"\n      },\n      \"execution_count\": null,\n      \"outputs\": []\n    },\n    {\n      \"cell_type\": \"code\",\n      \"source\": [\n        \"#@title start stablediffusion-infinity (first setup may takes about two minutes for downloading models)\\n\",\n        \"!python app.py --share\"\n      ],\n      \"metadata\": {\n        \"id\": \"UGotC5ckDlmO\"\n      },\n      \"execution_count\": null,\n      \"outputs\": []\n    },\n    {\n      \"cell_type\": \"code\",\n      \"source\": [],\n      \"metadata\": {\n        \"id\": \"R1-E07CMFZoj\"\n      },\n      \"execution_count\": null,\n      \"outputs\": []\n    }\n  ]\n}\n"
        },
        {
          "name": "utils.py",
          "type": "blob",
          "size": 9.1474609375,
          "content": "from PIL import Image\nfrom PIL import ImageFilter\nimport cv2\nimport numpy as np\nimport scipy\nimport scipy.signal\nfrom scipy.spatial import cKDTree\n\nimport os\nfrom perlin2d import *\n\npatch_match_compiled = True\n\ntry:\n    from PyPatchMatch import patch_match\nexcept Exception as e:\n    try:\n        import patch_match\n    except Exception as e:\n        patch_match_compiled = False\n\ntry:\n    patch_match\nexcept NameError:\n    print(\"patch_match compiling failed, will fall back to edge_pad\")\n    patch_match_compiled = False\n\n\n\n\ndef edge_pad(img, mask, mode=1):\n    if mode == 0:\n        nmask = mask.copy()\n        nmask[nmask > 0] = 1\n        res0 = 1 - nmask\n        res1 = nmask\n        p0 = np.stack(res0.nonzero(), axis=0).transpose()\n        p1 = np.stack(res1.nonzero(), axis=0).transpose()\n        min_dists, min_dist_idx = cKDTree(p1).query(p0, 1)\n        loc = p1[min_dist_idx]\n        for (a, b), (c, d) in zip(p0, loc):\n            img[a, b] = img[c, d]\n    elif mode == 1:\n        record = {}\n        kernel = [[1] * 3 for _ in range(3)]\n        nmask = mask.copy()\n        nmask[nmask > 0] = 1\n        res = scipy.signal.convolve2d(\n            nmask, kernel, mode=\"same\", boundary=\"fill\", fillvalue=1\n        )\n        res[nmask < 1] = 0\n        res[res == 9] = 0\n        res[res > 0] = 1\n        ylst, xlst = res.nonzero()\n        queue = [(y, x) for y, x in zip(ylst, xlst)]\n        # bfs here\n        cnt = res.astype(np.float32)\n        acc = img.astype(np.float32)\n        step = 1\n        h = acc.shape[0]\n        w = acc.shape[1]\n        offset = [(1, 0), (-1, 0), (0, 1), (0, -1)]\n        while queue:\n            target = []\n            for y, x in queue:\n                val = acc[y][x]\n                for yo, xo in offset:\n                    yn = y + yo\n                    xn = x + xo\n                    if 0 <= yn < h and 0 <= xn < w and nmask[yn][xn] < 1:\n                        if record.get((yn, xn), step) == step:\n                            acc[yn][xn] = acc[yn][xn] * cnt[yn][xn] + val\n                            cnt[yn][xn] += 1\n                            acc[yn][xn] /= cnt[yn][xn]\n                            if (yn, xn) not in record:\n                                record[(yn, xn)] = step\n                                target.append((yn, xn))\n            step += 1\n            queue = target\n        img = acc.astype(np.uint8)\n    else:\n        nmask = mask.copy()\n        ylst, xlst = nmask.nonzero()\n        yt, xt = ylst.min(), xlst.min()\n        yb, xb = ylst.max(), xlst.max()\n        content = img[yt : yb + 1, xt : xb + 1]\n        img = np.pad(\n            content,\n            ((yt, mask.shape[0] - yb - 1), (xt, mask.shape[1] - xb - 1), (0, 0)),\n            mode=\"edge\",\n        )\n    return img, mask\n\n\ndef perlin_noise(img, mask):\n    lin_x = np.linspace(0, 5, mask.shape[1], endpoint=False)\n    lin_y = np.linspace(0, 5, mask.shape[0], endpoint=False)\n    x, y = np.meshgrid(lin_x, lin_y)\n    avg = img.mean(axis=0).mean(axis=0)\n    # noise=[((perlin(x, y)+1)*128+avg[i]).astype(np.uint8) for i in range(3)]\n    noise = [((perlin(x, y) + 1) * 0.5 * 255).astype(np.uint8) for i in range(3)]\n    noise = np.stack(noise, axis=-1)\n    # mask=skimage.measure.block_reduce(mask,(8,8),np.min)\n    # mask=mask.repeat(8, axis=0).repeat(8, axis=1)\n    # mask_image=Image.fromarray(mask)\n    # mask_image=mask_image.filter(ImageFilter.GaussianBlur(radius = 4))\n    # mask=np.array(mask_image)\n    nmask = mask.copy()\n    # nmask=nmask/255.0\n    nmask[mask > 0] = 1\n    img = nmask[:, :, np.newaxis] * img + (1 - nmask[:, :, np.newaxis]) * noise\n    # img=img.astype(np.uint8)\n    return img, mask\n\n\ndef gaussian_noise(img, mask):\n    noise = np.random.randn(mask.shape[0], mask.shape[1], 3)\n    noise = (noise + 1) / 2 * 255\n    noise = noise.astype(np.uint8)\n    nmask = mask.copy()\n    nmask[mask > 0] = 1\n    img = nmask[:, :, np.newaxis] * img + (1 - nmask[:, :, np.newaxis]) * noise\n    return img, mask\n\n\ndef cv2_telea(img, mask):\n    ret = cv2.inpaint(img, 255 - mask, 5, cv2.INPAINT_TELEA)\n    return ret, mask\n\n\ndef cv2_ns(img, mask):\n    ret = cv2.inpaint(img, 255 - mask, 5, cv2.INPAINT_NS)\n    return ret, mask\n\n\ndef patch_match_func(img, mask):\n    ret = patch_match.inpaint(img, mask=255 - mask, patch_size=3)\n    return ret, mask\n\n\ndef mean_fill(img, mask):\n    avg = img.mean(axis=0).mean(axis=0)\n    img[mask < 1] = avg\n    return img, mask\n\n\"\"\"\nApache-2.0 license\nhttps://github.com/hafriedlander/stable-diffusion-grpcserver/blob/main/sdgrpcserver/services/generate.py\nhttps://github.com/parlance-zz/g-diffuser-bot/tree/g-diffuser-bot-beta2\n_handleImageAdjustment\n\"\"\"\ntry:\n    from sd_grpcserver.sdgrpcserver import images\n    import torch\n    from math import sqrt\n    def handleImageAdjustment(array, adjustments):\n        tensor = images.fromPIL(Image.fromarray(array))\n        for adjustment in adjustments:\n            which = adjustment[0]\n\n            if which == \"blur\":\n                sigma = adjustment[1]\n                direction = adjustment[2]\n\n                if direction == \"DOWN\" or direction == \"UP\":\n                    orig = tensor\n                    repeatCount=256\n                    sigma /= sqrt(repeatCount)\n\n                    for _ in range(repeatCount):\n                        tensor = images.gaussianblur(tensor, sigma)\n                        if direction == \"DOWN\":\n                            tensor = torch.minimum(tensor, orig)\n                        else:\n                            tensor = torch.maximum(tensor, orig)\n                else:\n                    tensor = images.gaussianblur(tensor, adjustment.blur.sigma)\n            elif which == \"invert\":\n                tensor = images.invert(tensor)\n            elif which == \"levels\":\n                tensor = images.levels(tensor, adjustment[1], adjustment[2], adjustment[3], adjustment[4])\n            elif which == \"channels\":\n                tensor = images.channelmap(tensor, [adjustment.channels.r,  adjustment.channels.g,  adjustment.channels.b,  adjustment.channels.a])\n            elif which == \"rescale\":\n                self.unimp(\"Rescale\")\n            elif which == \"crop\":\n                tensor = images.crop(tensor, adjustment.crop.top, adjustment.crop.left, adjustment.crop.height, adjustment.crop.width)\n        return np.array(images.toPIL(tensor)[0])\n\n    def g_diffuser(img,mask):\n        adjustments=[[\"blur\",32,\"UP\"],[\"level\",0,0.05,0,1]]\n        mask=handleImageAdjustment(mask,adjustments)\n        out_mask=handleImageAdjustment(mask,adjustments)\n        return img, mask\nexcept:\n    def g_diffuser(img,mask):\n        return img,mask\n\ndef dummy_fill(img,mask):\n    return img,mask\nfunctbl = {\n    \"gaussian\": gaussian_noise,\n    \"perlin\": perlin_noise,\n    \"edge_pad\": edge_pad,\n    \"patchmatch\": patch_match_func if patch_match_compiled else edge_pad,\n    \"cv2_ns\": cv2_ns,\n    \"cv2_telea\": cv2_telea,\n    \"g_diffuser\": g_diffuser,\n    \"g_diffuser_lib\": dummy_fill,\n}\n\ntry:\n    from postprocess import PhotometricCorrection\n    correction_func = PhotometricCorrection()\nexcept Exception as e:\n    print(e, \"so PhotometricCorrection is disabled\")\n    class DummyCorrection:\n        def __init__(self):\n            self.backend=\"\"\n            pass\n        def run(self,a,b,**kwargs):\n            return b\n    correction_func=DummyCorrection()\n\nclass DummyInterrogator:\n    def __init__(self) -> None:\n        pass\n    def interrogate(self,pil):\n        return \"Interrogator init failed\"\n\nif \"taichi\" in correction_func.backend:\n    import sys\n    import io\n    import base64\n    from PIL import Image\n    def base64_to_pil(base64_str):\n        data = base64.b64decode(str(base64_str))\n        pil = Image.open(io.BytesIO(data))\n        return pil\n\n    def pil_to_base64(out_pil):\n        out_buffer = io.BytesIO()\n        out_pil.save(out_buffer, format=\"PNG\")\n        out_buffer.seek(0)\n        base64_bytes = base64.b64encode(out_buffer.read())\n        base64_str = base64_bytes.decode(\"ascii\")\n        return base64_str\n    from subprocess import Popen, PIPE, STDOUT\n    class SubprocessCorrection:\n        def __init__(self):\n            self.backend=correction_func.backend\n            self.child= Popen([\"python\", \"postprocess.py\"], stdin=PIPE, stdout=PIPE, stderr=STDOUT)\n        def run(self,img_input,img_inpainted,mode):\n            if mode==\"disabled\":\n                return img_inpainted\n            base64_str_input = pil_to_base64(img_input)\n            base64_str_inpainted = pil_to_base64(img_inpainted)\n            try:\n                if self.child.poll():\n                    self.child= Popen([\"python\", \"postprocess.py\"], stdin=PIPE, stdout=PIPE, stderr=STDOUT)\n                self.child.stdin.write(f\"{base64_str_input},{base64_str_inpainted},{mode}\\n\".encode())\n                self.child.stdin.flush()\n                out = self.child.stdout.readline()\n                base64_str=out.decode().strip()\n                while base64_str and base64_str[0]==\"[\":\n                    print(base64_str)\n                    out = self.child.stdout.readline()\n                    base64_str=out.decode().strip()\n                ret=base64_to_pil(base64_str)\n            except:\n                print(\"[PIE] not working, photometric correction is disabled\")\n                ret=img_inpainted\n            return ret\n    correction_func = SubprocessCorrection()\n"
        }
      ]
    }
  ]
}