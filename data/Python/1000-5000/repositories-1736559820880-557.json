{
  "metadata": {
    "timestamp": 1736559820880,
    "page": 557,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjU2MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "QwenLM/Qwen2.5-Coder",
      "stars": 3804,
      "defaultBranch": "main",
      "files": [
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 3.02734375,
          "content": "# Byte-compiled / optimized / DLL files\n__pycache__/\n*.py[cod]\n*$py.class\n\n.DS_Store\n.gitconfig\n\n# C extensions\n*.so\n\n# Distribution / packaging\n.Python\nbuild/\ndevelop-eggs/\ndist/\ndownloads/\neggs/\n.eggs/\nlib/\nlib64/\nparts/\nsdist/\nvar/\nwheels/\nshare/python-wheels/\n*.egg-info/\n.installed.cfg\n*.egg\nMANIFEST\n\n# PyInstaller\n#  Usually these files are written by a python script from a template\n#  before PyInstaller builds the exe, so as to inject date/other infos into it.\n*.manifest\n*.spec\n\n# Installer logs\npip-log.txt\npip-delete-this-directory.txt\n\n# Unit test / coverage reports\nhtmlcov/\n.tox/\n.nox/\n.coverage\n.coverage.*\n.cache\nnosetests.xml\ncoverage.xml\n*.cover\n*.py,cover\n.hypothesis/\n.pytest_cache/\ncover/\n\n# Translations\n*.mo\n*.pot\n\n# Django stuff:\n*.log\nlocal_settings.py\ndb.sqlite3\ndb.sqlite3-journal\n\n# Flask stuff:\ninstance/\n.webassets-cache\n\n# Scrapy stuff:\n.scrapy\n\n# Sphinx documentation\ndocs/_build/\n\n# PyBuilder\n.pybuilder/\ntarget/\n\n# Jupyter Notebook\n.ipynb_checkpoints\n\n# IPython\nprofile_default/\nipython_config.py\n\n# pyenv\n#   For a library or package, you might want to ignore these files since the code is\n#   intended to run in multiple environments; otherwise, check them in:\n# .python-version\n\n# pipenv\n#   According to pypa/pipenv#598, it is recommended to include Pipfile.lock in version control.\n#   However, in case of collaboration, if having platform-specific dependencies or dependencies\n#   having no cross-platform support, pipenv may install dependencies that don't work, or not\n#   install all needed dependencies.\n#Pipfile.lock\n\n# poetry\n#   Similar to Pipfile.lock, it is generally recommended to include poetry.lock in version control.\n#   This is especially recommended for binary packages to ensure reproducibility, and is more\n#   commonly ignored for libraries.\n#   https://python-poetry.org/docs/basic-usage/#commit-your-poetrylock-file-to-version-control\n#poetry.lock\n\n# pdm\n#   Similar to Pipfile.lock, it is generally recommended to include pdm.lock in version control.\n#pdm.lock\n#   pdm stores project-wide configurations in .pdm.toml, but it is recommended to not include it\n#   in version control.\n#   https://pdm.fming.dev/#use-with-ide\n.pdm.toml\n\n# PEP 582; used by e.g. github.com/David-OConnor/pyflow and github.com/pdm-project/pdm\n__pypackages__/\n\n# Celery stuff\ncelerybeat-schedule\ncelerybeat.pid\n\n# SageMath parsed files\n*.sage.py\n\n# Environments\n.env\n.venv\nenv/\nvenv/\nENV/\nenv.bak/\nvenv.bak/\n\n# Spyder project settings\n.spyderproject\n.spyproject\n\n# Rope project settings\n.ropeproject\n\n# mkdocs documentation\n/site\n\n# mypy\n.mypy_cache/\n.dmypy.json\ndmypy.json\n\n# Pyre type checker\n.pyre/\n\n# pytype static type analyzer\n.pytype/\n\n# Cython debug symbols\ncython_debug/\n\n# PyCharm\n#  JetBrains specific template is maintained in a separate JetBrains.gitignore that can\n#  be found at https://github.com/github/gitignore/blob/main/Global/JetBrains.gitignore\n#  and can be added to the global gitignore or merged into this file.  For a more nuclear\n#  option (not recommended) you can uncomment the following to ignore the entire idea folder.\n#.idea/\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 28.431640625,
          "content": "<a name=\"readme-top\"></a>\n\n<p align=\"center\">\n    <img src=\"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2.5/Qwen2.5-Coder/qwen2.5-coder-logo\" width=\"400\"/>\n<p>\n\n<p align=\"center\">\n    <img src=\"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2.5/Qwen2.5-Coder-Family/main_fig_32b_white.jpg\" width=\"400\"/>\n<p>\n\n\n<p align=\"center\">\n        ü§ó <a href=\"https://huggingface.co/collections/Qwen/qwen25-coder-66eaa22e6f99801bf65b0c2f\">Hugging Face</a>&nbsp&nbsp | &nbsp&nbspü§ñ <a href=\"https://modelscope.cn/organization/qwen\">ModelScope</a>&nbsp&nbsp | &nbsp&nbspüíª <a href=\"https://www.kaggle.com/models/qwen-lm/qwen2.5-coder\">Kaggle</a>&nbsp&nbsp | &nbsp&nbsp üìë <a href=\"https://qwenlm.github.io/blog/qwen2.5-coder-family\">Blog</a> &nbsp&nbsp ÔΩú &nbsp&nbspüìñ <a href=\"https://qwen.readthedocs.io/\">Documentation</a>\n<br>\nüñ•Ô∏è <a href=\"https://huggingface.co/spaces/Qwen/Qwen2.5-Coder-demo\">Demo</a>&nbsp&nbsp | üñº <a href=\"https://huggingface.co/spaces/Qwen/Qwen2.5-Coder-Artifacts\">Artifacts</a>&nbsp&nbsp | &nbsp&nbspüí¨ <a href=\"https://github.com/QwenLM/Qwen/blob/main/assets/wechat.png\">WeChat (ÂæÆ‰ø°)</a>&nbsp&nbsp | &nbsp&nbspü´® <a href=\"https://discord.gg/CV4E9rpNSD\">Discord</a>&nbsp&nbsp | &nbsp&nbsp üìÑ<a href=\"https://arxiv.org/abs/2409.12186\">Arxiv</a>&nbsp&nbsp | &nbsp&nbspüñ•Ô∏è <a href=\"https://gallery.pai-ml.com/#/preview/deepLearning/nlp/qwen2-5_coder_7b\">PAI-DSW</a>&nbsp&nbsp\n</p>\n\n\nVisit our Hugging Face or ModelScope organization (click links above), search checkpoints with names starting with `Qwen2.5-Coder-`, and you will find all you need! Enjoy!\n\n# Qwen2.5-Coder Series: Powerful, Diverse, Practical.\n\n## Introduction\n\nToday, we are excited to open source the ‚ÄúPowerful‚Äù, ‚ÄúDiverse‚Äù, and ‚ÄúPractical‚Äù **Qwen2.5-Coder** series (formerly known as CodeQwen1.5), dedicated to continuously promoting the development of Open CodeLLMs.\n\nüíª Powerful: Qwen2.5-Coder-32B-Instruct has become the current SOTA open-source code model, matching the coding capabilities of GPT-4o. While demonstrating strong and comprehensive coding abilities, it also possesses good general and mathematical skills;\n\nüìö Diverse: Building on the previously open-sourced two sizes of 1.5B / 7B, this release brings four model sizes, including 0.5B / 3B / 14B / 32B. As of now, Qwen2.5-Coder has covered six mainstream model sizes to meet the needs of different developers;\n\nüõ† Practical: We explore the practicality of Qwen2.5-Coder in two scenarios, including code assistants and Artifacts, with some examples showcasing the potential applications of Qwen2.5-Coder in real-world scenarios;\n\n## Basic information\n\n1. ‚ú® Supporting long context understanding and generation with the context length of 128K tokens;\n2. ‚ú® Supporting 92 coding languages;\n```\n['ada', 'agda', 'alloy', 'antlr', 'applescript', 'assembly', 'augeas', 'awk', 'batchfile', 'bluespec', 'c', 'c#', 'c++', 'clojure', 'cmake', 'coffeescript', 'common-lisp', 'css', 'cuda', 'dart', 'dockerfile', 'elixir', 'elm', 'emacs-lisp', 'erlang', 'f#', 'fortran', 'glsl', 'go', 'groovy', 'haskell', 'html', 'idris', 'isabelle', 'java', 'java-server-pages', 'javascript', 'json', 'julia', 'jupyter-notebook', 'kotlin', 'lean', 'literate-agda', 'literate-coffeescript', 'literate-haskell', 'lua', 'makefile', 'maple', 'markdown', 'mathematica', 'matlab', 'objectc++', 'ocaml', 'pascal', 'perl', 'php', 'powershell', 'prolog', 'protocol-buffer', 'python', 'r', 'racket', 'restructuredtext', 'rmarkdown', 'ruby', 'rust', 'sas', 'scala', 'scheme', 'shell', 'smalltalk', 'solidity', 'sparql', 'sql', 'stan', 'standard-ml', 'stata', 'swift', 'systemverilog', 'tcl', 'tcsh', 'tex', 'thrift', 'typescript', 'verilog', 'vhdl', 'visual-basic', 'vue', 'xslt', 'yacc', 'yaml', 'zig']\n```\n3. ‚ú® Retain strengths in math and general capabilities from base model\n\n> [!Important]\n> We updated both the special tokens and their corresponding token ids, in order to maintain consistency with Qwen2.5. The new special tokens are as the following:\n\n```json\n{\n  \"<|fim_prefix|>\": 151659, \n  \"<|fim_middle|>\": 151660, \n  \"<|fim_suffix|>\": 151661, \n  \"<|fim_pad|>\": 151662, \n  \"<|repo_name|>\": 151663, \n  \"<|file_sep|>\": 151664, \n  \"<|im_start|>\": 151644, \n  \"<|im_end|>\": 151645\n}\n```\n\n| model name                  | type     | length | Download                                                                                                                                                                        |\n|-----------------------------|----------|--------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| Qwen2.5-Coder-0.5B          | base     | 32k    | ü§ó [Hugging Face](https://huggingface.co/Qwen/Qwen2.5-Coder-0.5B) ‚Ä¢ ü§ñ [ModelScope](https://modelscope.cn/models/Qwen/Qwen2.5-Coder-0.5B)                                       |\n| Qwen2.5-Coder-1.5B          | base     | 32k    | ü§ó [Hugging Face](https://huggingface.co/Qwen/Qwen2.5-Coder-1.5B) ‚Ä¢ ü§ñ [ModelScope](https://modelscope.cn/models/Qwen/Qwen2.5-Coder-1.5B)                                       |\n| Qwen2.5-Coder-3B            | base     | 32k    | ü§ó [Hugging Face](https://huggingface.co/Qwen/Qwen2.5-Coder-3B) ‚Ä¢ ü§ñ [ModelScope](https://modelscope.cn/models/Qwen/Qwen2.5-Coder-3B)                                           |\n| Qwen2.5-Coder-7B            | base     | 128k   | ü§ó [Hugging Face](https://huggingface.co/Qwen/Qwen2.5-Coder-7B) ‚Ä¢ ü§ñ [ModelScope](https://modelscope.cn/models/Qwen/Qwen2.5-Coder-7B)                                           |\n| Qwen2.5-Coder-14B           | base     | 128k   | ü§ó [Hugging Face](https://huggingface.co/Qwen/Qwen2.5-Coder-14B) ‚Ä¢ ü§ñ [ModelScope](https://modelscope.cn/models/Qwen/Qwen2.5-Coder-14B)                                         |\n| Qwen2.5-Coder-32B           | base     | 128k   | ü§ó [Hugging Face](https://huggingface.co/Qwen/Qwen2.5-Coder-32B) ‚Ä¢ ü§ñ [ModelScope](https://modelscope.cn/models/Qwen/Qwen2.5-Coder-32B)                                         |\n| Qwen2.5-Coder-0.5B-instruct | instruct | 32k    | ü§ó [Hugging Face](https://huggingface.co/Qwen/Qwen2.5-Coder-0.5B-Instruct) ‚Ä¢ ü§ñ [ModelScope](https://modelscope.cn/models/Qwen/Qwen2.5-Coder-0.5B-Instruct)                     |\n| Qwen2.5-Coder-1.5B-instruct | instruct | 32k     | ü§ó [Hugging Face](https://huggingface.co/Qwen/Qwen2.5-Coder-1.5B-Instruct) ‚Ä¢ ü§ñ [ModelScope](https://modelscope.cn/models/Qwen/Qwen2.5-Coder-1.5B-Instruct)                     |\n| Qwen2.5-Coder-3B-instruct   | instruct | 32k     | ü§ó [Hugging Face](https://huggingface.co/Qwen/Qwen2.5-Coder-3B-Instruct) ‚Ä¢ ü§ñ [ModelScope](https://modelscope.cn/models/Qwen/Qwen2.5-Coder-3B-Instruct)                         |\n| Qwen2.5-Coder-7B-instruct   | instruct | 128k   | ü§ó [Hugging Face](https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct) ‚Ä¢ ü§ñ [ModelScope](https://modelscope.cn/models/Qwen/Qwen2.5-Coder-7B-Instruct)                         |\n| Qwen2.5-Coder-14B-instruct  | instruct | 128k   | ü§ó [Hugging Face](https://huggingface.co/Qwen/Qwen2.5-Coder-14B-Instruct) ‚Ä¢ ü§ñ [ModelScope](https://modelscope.cn/models/Qwen/Qwen2.5-Coder-14B-Instruct)                       |\n| Qwen2.5-Coder-32B-instruct  | instruct | 128k   | ü§ó [Hugging Face](https://huggingface.co/Qwen/Qwen2.5-Coder-32B-Instruct) ‚Ä¢ ü§ñ [ModelScope](https://modelscope.cn/models/Qwen/Qwen2.5-Coder-32B-Instruct)                       |\n| Qwen2.5-Coder-0.5B-Instruct-AWQ       | instruct | 32k   | ü§ó [Hugging Face](https://huggingface.co/Qwen/Qwen2.5-Coder-0.5B-Instruct-AWQ) ‚Ä¢ ü§ñ [ModelScope](https://modelscope.cn/models/Qwen/Qwen2.5-Coder-0.5B-Instruct-AWQ)             |\n| Qwen2.5-Coder-0.5B-Instruct-GGUF      | instruct | 32k   | ü§ó [Hugging Face](https://huggingface.co/Qwen/Qwen2.5-Coder-0.5B-Instruct-GGUF) ‚Ä¢ ü§ñ [ModelScope](https://modelscope.cn/models/Qwen/Qwen2.5-Coder-0.5B-Instruct-GGUF)           |\n| Qwen2.5-Coder-0.5B-Instruct-GPTQ-Int4 | instruct | 32k   | ü§ó [Hugging Face](https://huggingface.co/Qwen/Qwen2.5-Coder-0.5B-Instruct-GPTQ-Int4) ‚Ä¢ ü§ñ [ModelScope](https://modelscope.cn/models/Qwen/Qwen2.5-Coder-0.5B-Instruct-GPTQ-Int4) |\n| Qwen2.5-Coder-0.5B-Instruct-GPTQ-Int8 | instruct | 32k   | ü§ó [Hugging Face](https://huggingface.co/Qwen/Qwen2.5-Coder-0.5B-Instruct-GPTQ-Int8) ‚Ä¢ ü§ñ [ModelScope](https://modelscope.cn/models/Qwen/Qwen2.5-Coder-0.5B-Instruct-GPTQ-Int8) |\n| Qwen2.5-Coder-1.5B-Instruct-AWQ       | instruct | 32k   | ü§ó [Hugging Face](https://huggingface.co/Qwen/Qwen2.5-Coder-1.5B-Instruct-AWQ) ‚Ä¢ ü§ñ [ModelScope](https://modelscope.cn/models/Qwen/Qwen2.5-Coder-1.5B-Instruct-AWQ)             |\n| Qwen2.5-Coder-1.5B-Instruct-GGUF      | instruct | 32k   | ü§ó [Hugging Face](https://huggingface.co/Qwen/Qwen2.5-Coder-1.5B-Instruct-GGUF) ‚Ä¢ ü§ñ [ModelScope](https://modelscope.cn/models/Qwen/Qwen2.5-Coder-1.5B-Instruct-GGUF)           |\n| Qwen2.5-Coder-1.5B-Instruct-GPTQ-Int4 | instruct | 32k   | ü§ó [Hugging Face](https://huggingface.co/Qwen/Qwen2.5-Coder-1.5B-Instruct-GPTQ-Int4) ‚Ä¢ ü§ñ [ModelScope](https://modelscope.cn/models/Qwen/Qwen2.5-Coder-1.5B-Instruct-GPTQ-Int4) |\n| Qwen2.5-Coder-1.5B-Instruct-GPTQ-Int8 | instruct | 32k   | ü§ó [Hugging Face](https://huggingface.co/Qwen/Qwen2.5-Coder-1.5B-Instruct-GPTQ-Int8) ‚Ä¢ ü§ñ [ModelScope](https://modelscope.cn/models/Qwen/Qwen2.5-Coder-1.5B-Instruct-GPTQ-Int8) |\n| Qwen2.5-Coder-3B-Instruct-AWQ       | instruct | 32k   | ü§ó [Hugging Face](https://huggingface.co/Qwen/Qwen2.5-Coder-3B-Instruct-AWQ) ‚Ä¢ ü§ñ [ModelScope](https://modelscope.cn/models/Qwen/Qwen2.5-Coder-3B-Instruct-AWQ)                 |\n| Qwen2.5-Coder-3B-Instruct-GGUF      | instruct | 32k   | ü§ó [Hugging Face](https://huggingface.co/Qwen/Qwen2.5-Coder-3B-Instruct-GGUF) ‚Ä¢ ü§ñ [ModelScope](https://modelscope.cn/models/Qwen/Qwen2.5-Coder-3B-Instruct-GGUF)               |\n| Qwen2.5-Coder-3B-Instruct-GPTQ-Int4 | instruct | 32k   | ü§ó [Hugging Face](https://huggingface.co/Qwen/Qwen2.5-Coder-3B-Instruct-GPTQ-Int4) ‚Ä¢ ü§ñ [ModelScope](https://modelscope.cn/models/Qwen/Qwen2.5-Coder-3B-Instruct-GPTQ-Int4)     |\n| Qwen2.5-Coder-3B-Instruct-GPTQ-Int8 | instruct | 32k   | ü§ó [Hugging Face](https://huggingface.co/Qwen/Qwen2.5-Coder-3B-Instruct-GPTQ-Int8) ‚Ä¢ ü§ñ [ModelScope](https://modelscope.cn/models/Qwen/Qwen2.5-Coder-3B-Instruct-GPTQ-Int8)     |\n| Qwen2.5-Coder-7B-Instruct-AWQ         | instruct | 128k   | ü§ó [Hugging Face](https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct-AWQ) ‚Ä¢ ü§ñ [ModelScope](https://modelscope.cn/models/Qwen/Qwen2.5-Coder-7B-Instruct-AWQ)             |\n| Qwen2.5-Coder-7B-Instruct-GGUF        | instruct | 128k   | ü§ó [Hugging Face](https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct-GGUF) ‚Ä¢ ü§ñ [ModelScope](https://modelscope.cn/models/Qwen/Qwen2.5-Coder-7B-Instruct-GGUF)           |\n| Qwen2.5-Coder-7B-Instruct-GPTQ-Int4   | instruct | 128k   | ü§ó [Hugging Face](https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct-GPTQ-Int4) ‚Ä¢ ü§ñ [ModelScope](https://modelscope.cn/models/Qwen/Qwen2.5-Coder-7B-Instruct-GPTQ-Int4) |\n| Qwen2.5-Coder-7B-Instruct-GPTQ-Int8   | instruct | 128k   | ü§ó [Hugging Face](https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct-GPTQ-Int8) ‚Ä¢ ü§ñ [ModelScope](https://modelscope.cn/models/Qwen/Qwen2.5-Coder-7B-Instruct-GPTQ-Int8) |\n| Qwen2.5-Coder-14B-Instruct-AWQ         | instruct | 128k   | ü§ó [Hugging Face](https://huggingface.co/Qwen/Qwen2.5-Coder-14B-Instruct-AWQ) ‚Ä¢ ü§ñ [ModelScope](https://modelscope.cn/models/Qwen/Qwen2.5-Coder-14B-Instruct-AWQ)             |\n| Qwen2.5-Coder-14B-Instruct-GGUF        | instruct | 128k   | ü§ó [Hugging Face](https://huggingface.co/Qwen/Qwen2.5-Coder-14B-Instruct-GGUF) ‚Ä¢ ü§ñ [ModelScope](https://modelscope.cn/models/Qwen/Qwen2.5-Coder-14B-Instruct-GGUF)           |\n| Qwen2.5-Coder-14B-Instruct-GPTQ-Int4   | instruct | 128k   | ü§ó [Hugging Face](https://huggingface.co/Qwen/Qwen2.5-Coder-14B-Instruct-GPTQ-Int4) ‚Ä¢ ü§ñ [ModelScope](https://modelscope.cn/models/Qwen/Qwen2.5-Coder-14B-Instruct-GPTQ-Int4) |\n| Qwen2.5-Coder-14B-Instruct-GPTQ-Int8   | instruct | 128k   | ü§ó [Hugging Face](https://huggingface.co/Qwen/Qwen2.5-Coder-14B-Instruct-GPTQ-Int8) ‚Ä¢ ü§ñ [ModelScope](https://modelscope.cn/models/Qwen/Qwen2.5-Coder-14B-Instruct-GPTQ-Int8) |\n| Qwen2.5-Coder-32B-Instruct-AWQ         | instruct | 128k   | ü§ó [Hugging Face](https://huggingface.co/Qwen/Qwen2.5-Coder-32B-Instruct-AWQ) ‚Ä¢ ü§ñ [ModelScope](https://modelscope.cn/models/Qwen/Qwen2.5-Coder-32B-Instruct-AWQ)             |\n| Qwen2.5-Coder-32B-Instruct-GGUF        | instruct | 128k   | ü§ó [Hugging Face](https://huggingface.co/Qwen/Qwen2.5-Coder-32B-Instruct-GGUF) ‚Ä¢ ü§ñ [ModelScope](https://modelscope.cn/models/Qwen/Qwen2.5-Coder-32B-Instruct-GGUF)           |\n| Qwen2.5-Coder-32B-Instruct-GPTQ-Int4   | instruct | 128k   | ü§ó [Hugging Face](https://huggingface.co/Qwen/Qwen2.5-Coder-32B-Instruct-GPTQ-Int4) ‚Ä¢ ü§ñ [ModelScope](https://modelscope.cn/models/Qwen/Qwen2.5-Coder-32B-Instruct-GPTQ-Int4) |\n| Qwen2.5-Coder-32B-Instruct-GPTQ-Int8   | instruct | 128k   | ü§ó [Hugging Face](https://huggingface.co/Qwen/Qwen2.5-Coder-32B-Instruct-GPTQ-Int8) ‚Ä¢ ü§ñ [ModelScope](https://modelscope.cn/models/Qwen/Qwen2.5-Coder-32B-Instruct-GPTQ-Int8) |\n\n\nDetailed performance and introduction are shown in this <a href=\"https://qwenlm.github.io/blog/qwen2.5-coder-family\"> üìë blog</a>.\n\n## Requirements\n* `python>=3.9`\n* `transformers>4.37.0` for Qwen2.5 dense models.\n\n> [!Warning]\n> <div align=\"center\">\n> <b>\n> üö® This is a must because `transformers` integrated Qwen2 codes since `4.37.0`.\n> </b>\n> </div>\n\nYou can install the required packages with the following command:\n```bash\npip install -r requirements.txt\n```\n\n## Quick Start\n\n> [!Important]\n> **Qwen2.5-Coder-\\[0.5-32\\]B-Instruct** are instruction models for chatting;\n>\n> **Qwen2.5-Coder-\\[0.5-32\\]B** is a base model typically used for completion, serving as a better starting point for fine-tuning.\n>\n### üëâüèª Chat with Qwen2.5-Coder-32B-Instruct\nYou can just write several lines of code with `transformers` to chat with Qwen2.5-Coder-32B-Instruct. Essentially, we build the tokenizer and the model with `from_pretrained` method, and we use generate method to perform chatting with the help of chat template provided by the tokenizer. Below is an example of how to chat with Qwen2.5-Coder-32B-Instruct:\n\n```python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_name = \"Qwen/Qwen2.5-Coder-32B-Instruct\"\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    torch_dtype=\"auto\",\n    device_map=\"auto\"\n)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\nprompt = \"write a quick sort algorithm.\"\nmessages = [\n    {\"role\": \"system\", \"content\": \"You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\"},\n    {\"role\": \"user\", \"content\": prompt}\n]\ntext = tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True\n)\nmodel_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n\ngenerated_ids = model.generate(\n    **model_inputs,\n    max_new_tokens=512\n)\ngenerated_ids = [\n    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n]\n\nresponse = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n```\nThe `apply_chat_template()` function is used to convert the messages into a format that the model can understand.\nThe `add_generation_prompt` argument is used to add a generation prompt, which refers to `<|im_start|>assistant\\n` to the input. Notably, we apply ChatML template for chat models following our previous practice.\nThe `max_new_tokens` argument is used to set the maximum length of the response. The `tokenizer.batch_decode()` function is used to decode the response. In terms of the input, the above messages is an example to show how to format your dialog history and system prompt.\nYou can use the other size of instruct model in the same way.\n\n### üëâüèª Code with Qwen2.5-Coder-32B\n\n#### 1. Basic Usage\nThe model completes the code snippets according to the given prompts, without any additional formatting, which is usually termed as `code completion` in the code generation tasks.\n\nEssentially, we build the tokenizer and the model with `from_pretrained` method, and we use generate method to perform code completion. Below is an example on how to chat with Qwen2.5-Coder-32B:\n```python\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\ndevice = \"cuda\" # the device to load the model onto\n\n# Now you do not need to add \"trust_remote_code=True\"\nTOKENIZER = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-Coder-32B\")\nMODEL = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen2.5-Coder-32B\", device_map=\"auto\").eval()\n\n# tokenize the input into tokens\ninput_text = \"#write a quick sort algorithm\"\nmodel_inputs = TOKENIZER([input_text], return_tensors=\"pt\").to(device)\n\n# Use `max_new_tokens` to control the maximum output length.\ngenerated_ids = MODEL.generate(model_inputs.input_ids, max_new_tokens=512, do_sample=False)[0]\n# The generated_ids include prompt_ids, so we only need to decode the tokens after prompt_ids.\noutput_text = TOKENIZER.decode(generated_ids[len(model_inputs.input_ids[0]):], skip_special_tokens=True)\n\nprint(f\"Prompt: {input_text}\\n\\nGenerated text: {output_text}\")\n```\nThe `max_new_tokens` argument is used to set the maximum length of the response.\nThe `input_text` could be any text that you would like model to continue with.\n\n\n#### 2. Processing Long Texts\n\nThe current `config.json` is set for context length up to 32,768 tokens.\nTo handle extensive inputs exceeding 32,768 tokens, we utilize [YaRN](https://arxiv.org/abs/2309.00071), a technique for enhancing model length extrapolation, ensuring optimal performance on lengthy texts.\n\nFor supported frameworks, you could add the following to `config.json` to enable YaRN:\n```json\n{\n  ...,\n  \"rope_scaling\": {\n    \"factor\": 4.0,\n    \"original_max_position_embeddings\": 32768,\n    \"type\": \"yarn\"\n  }\n}\n```\n\n#### 3. File-Level Code Completion (Fill in the middle)\nThe code insertion task, also referred to as the \"fill-in-the-middle\" challenge, requires the insertion of code segments in a manner that bridges the gaps within a given code context.\nFor an approach aligned with best practices, we recommend adhering to the formatting guidelines outlined in the paper \"Efficient Training of Language Models to Fill in the Middle\"[[arxiv](https://arxiv.org/abs/2207.14255)]. This involves the use of three specialized tokens`<fim_prefix>`, `<fim_suffix>`, and `<fim_middle>` to denote the respective segments of the code structure.\nThe prompt should be structured as follows:\n```python\nprompt = '<|fim_prefix|>' + prefix_code + '<|fim_suffix|>' + suffix_code + '<|fim_middle|>'\n```\nFollowing the approach mentioned, an example would be structured in this manner:\n\n```python\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n# load model\ndevice = \"cuda\" # the device to load the model onto\n\nTOKENIZER = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-Coder-32B\")\nMODEL = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen2.5-Coder-32B\", device_map=\"auto\").eval()\n\ninput_text = \"\"\"<|fim_prefix|>def quicksort(arr):\n    if len(arr) <= 1:\n        return arr\n    pivot = arr[len(arr) // 2]\n    <|fim_suffix|>\n    middle = [x for x in arr if x == pivot]\n    right = [x for x in arr if x > pivot]\n    return quicksort(left) + middle + quicksort(right)<|fim_middle|>\"\"\"\n\nmodel_inputs = TOKENIZER([input_text], return_tensors=\"pt\").to(device)\n\n# Use `max_new_tokens` to control the maximum output length.\ngenerated_ids = MODEL.generate(model_inputs.input_ids, max_new_tokens=512, do_sample=False)[0]\n# The generated_ids include prompt_ids, we only need to decode the tokens after prompt_ids.\noutput_text = TOKENIZER.decode(generated_ids[len(model_inputs.input_ids[0]):], skip_special_tokens=True)\n\nprint(f\"Prompt: {input_text}\\n\\nGenerated text: {output_text}\")\n```\n\n#### 4. Repository-Level Code Completion\nThe repository level code completion task involves feeding the model the content of multiple files from the same repository. This enables the model to understand the interrelationships between different calls within these files, thereby facilitating the completion of code content.\nWe recommend using the two special tokens `<|repo_name|>` and `<|file_sep|>` to indicate the repository structure.\nFor example, assuming the repository name is stored in `repo_name`, and it contains files with their respective paths and contents listed as [(`file_path1`, `file_content1`), (`file_path2`, `file_content2`)], the format of the final input prompt would be as follows:\n```python\ninput_text = f'''<|repo_name|>{repo_name}\n<|file_sep|>{file_path1} \n{file_content1}\n<|file_sep|>{file_path2} \n{file_content2}'''\n```\n\n<details><summary>üëáüèª Below is a complete example of a repository level code completion task: <i>:: click to expand ::</i></summary>\n<div>\n\n```python\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\ndevice = \"cuda\" # the device to load the model onto\n\n# Now you do not need to add \"trust_remote_code=True\"\nTOKENIZER = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-Coder-32B\")\nMODEL = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen2.5-Coder-32B\", device_map=\"auto\").eval()\n\n# tokenize the input into tokens\ninput_text = \"\"\"<|repo_name|>library-system\n<|file_sep|>library.py\nclass Book:\n    def __init__(self, title, author, isbn, copies):\n        self.title = title\n        self.author = author\n        self.isbn = isbn\n        self.copies = copies\n\n    def __str__(self):\n        return f\"Title: {self.title}, Author: {self.author}, ISBN: {self.isbn}, Copies: {self.copies}\"\n\nclass Library:\n    def __init__(self):\n        self.books = []\n\n    def add_book(self, title, author, isbn, copies):\n        book = Book(title, author, isbn, copies)\n        self.books.append(book)\n\n    def find_book(self, isbn):\n        for book in self.books:\n            if book.isbn == isbn:\n                return book\n        return None\n\n    def list_books(self):\n        return self.books\n\n<|file_sep|>student.py\nclass Student:\n    def __init__(self, name, id):\n        self.name = name\n        self.id = id\n        self.borrowed_books = []\n\n    def borrow_book(self, book, library):\n        if book and book.copies > 0:\n            self.borrowed_books.append(book)\n            book.copies -= 1\n            return True\n        return False\n\n    def return_book(self, book, library):\n        if book in self.borrowed_books:\n            self.borrowed_books.remove(book)\n            book.copies += 1\n            return True\n        return False\n\n<|file_sep|>main.py\nfrom library import Library\nfrom student import Student\n\ndef main():\n    # Set up the library with some books\n    library = Library()\n    library.add_book(\"The Great Gatsby\", \"F. Scott Fitzgerald\", \"1234567890\", 3)\n    library.add_book(\"To Kill a Mockingbird\", \"Harper Lee\", \"1234567891\", 2)\n    \n    # Set up a student\n    student = Student(\"Alice\", \"S1\")\n    \n    # Student borrows a book\n\"\"\"\nmodel_inputs = TOKENIZER([input_text], return_tensors=\"pt\").to(device)\n\n# Use `max_new_tokens` to control the maximum output length.\ngenerated_ids = MODEL.generate(model_inputs.input_ids, max_new_tokens=1024, do_sample=False)[0]\n# The generated_ids include prompt_ids, so we only need to decode the tokens after prompt_ids.\noutput_text = TOKENIZER.decode(generated_ids[len(model_inputs.input_ids[0]):], skip_special_tokens=True)\n\nprint(f\"Prompt: \\n{input_text}\\n\\nGenerated text: \\n{output_text}\")\n\n```\nThe expected output as following:\n```python\nGenerated text:\n    book = library.find_book(\"1234567890\")\n    if student.borrow_book(book, library):\n    print(f\"{student.name} borrowed {book.title}\")\n    else:\n    print(f\"{student.name} could not borrow {book.title}\")\n    \n        # Student returns a book\n        if student.return_book(book, library):\n            print(f\"{student.name} returned {book.title}\")\n        else:\n            print(f\"{student.name} could not return {book.title}\")\n        \n        # List all books in the library\n        print(\"All books in the library:\")\n        for book in library.list_books():\n            print(book)\n\nif __name__ == \"__main__\":\nmain()\n```\n\n</div>\n</details>\n\n### üëâüèª Deploying Qwen2.5-Coder with vLLM\nAs a family member of Qwen2.5, Qwen2.5-Coder are supported by vLLM. The detail tutorial  could be found in [Qwen tutorial](https://qwen.readthedocs.io/en/latest/deployment/vllm.html).\nHere, we give you an simple example of offline batched inference in vLLM.\n\n#### Offline Batched Inference\n```python\nfrom transformers import AutoTokenizer\nfrom vllm import LLM, SamplingParams\n# Initialize the tokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-Coder-32B\")\n\n# Pass the default decoding hyperparameters of Qwen1.5-32B-Chat\n# max_tokens is for the maximum length for generation.\nsampling_params = SamplingParams(temperature=0.7, top_p=0.8, repetition_penalty=1.05, max_tokens=1024)\n\n# Input the model name or path. Can be GPTQ or AWQ models.\nllm = LLM(model=\"Qwen/Qwen2.5-Coder-32B\")\n\n# Prepare your prompts\nprompt = \"#write a quick sort algorithm.\\ndef quick_sort(\"\n\n# generate outputs\noutputs = llm.generate([prompt], sampling_params)\n\n# Print the outputs.\nfor output in outputs:\n    prompt = output.prompt\n    generated_text = output.outputs[0].text\n    print(f\"Prompt: {prompt!r}, Generated text: {generated_text!r}\")\n```\n\n#### Multi-GPU Distributed Serving\nTo scale up your serving throughputs, distributed serving helps you by leveraging more GPU devices.\nWhen using ultra-long sequences for inference, it might cause insufficient GPU memory. Here, we demonstrate how to run Qwen2.5-Coder-32B with tensor parallelism just by passing in the argument `tensor_parallel_size`.\n```python\nllm = LLM(model=\"Qwen/Qwen2.5-Coder-32B\", tensor_parallel_size=8)\n```\n### üëâüèª Gradio interface ü§ó\n\nWe also provide a Gradio <a href='https://github.com/gradio-app/gradio'><img src='https://img.shields.io/github/stars/gradio-app/gradio'></a> interface for a better experience, just run by:\n\n```bash\ncd demo/chatbot/\n# For Linux and Windows users (and macOS with Intel??)\npython app.py \n\n# For macOS with Apple Silicon users, Intel not supported, this maybe 20x slower than RTX 4090\nPYTORCH_ENABLE_MPS_FALLBACK=1 python app.py\n```\n\nWe also provide a Gradio interface of artifacts mode:\n```bash\ncd demo/artifacts/\npython app.py\n```\n\nYou can specify the `--server_port`, `--share`, `--server_name` arguments to satisfy your needs!\n\n**Or, try it out effortlessly on HuggingFace: [„Äåchatbot demo„Äç](https://huggingface.co/spaces/Qwen/Qwen2.5-Coder-demo) ü§ó [„Äåartifacts demo„Äç](https://huggingface.co/spaces/Qwen/Qwen2.5-Coder-Artifacts)**\n\n## Performance\nFor more information, please  refer to the <a href=\"https://arxiv.org/abs/2409.12186\">Qwen2.5-Coder Technical Report</a>.\n\n## Star History\n\n[![Star History Chart](https://api.star-history.com/svg?repos=QwenLM/Qwen2.5-Coder&type=Date)](https://star-history.com/#QwenLM/Qwen2.5-Coder&Date)\n\n## Citation\nIf you find our work helpful, feel free to give us a cite.\n\n```bibtex\n@article{hui2024qwen2,\n  title={Qwen2. 5-Coder Technical Report},\n  author={Hui, Binyuan and Yang, Jian and Cui, Zeyu and Yang, Jiaxi and Liu, Dayiheng and Zhang, Lei and Liu, Tianyu and Zhang, Jiajun and Yu, Bowen and Dang, Kai and others},\n  journal={arXiv preprint arXiv:2409.12186},\n  year={2024}\n}\n@article{qwen2,\n    title={Qwen2 Technical Report},\n    author={An Yang and Baosong Yang and Binyuan Hui and Bo Zheng and Bowen Yu and Chang Zhou and Chengpeng Li and Chengyuan Li and Dayiheng Liu and Fei Huang and Guanting Dong and Haoran Wei and Huan Lin and Jialong Tang and Jialin Wang and Jian Yang and Jianhong Tu and Jianwei Zhang and Jianxin Ma and Jin Xu and Jingren Zhou and Jinze Bai and Jinzheng He and Junyang Lin and Kai Dang and Keming Lu and Keqin Chen and Kexin Yang and Mei Li and Mingfeng Xue and Na Ni and Pei Zhang and Peng Wang and Ru Peng and Rui Men and Ruize Gao and Runji Lin and Shijie Wang and Shuai Bai and Sinan Tan and Tianhang Zhu and Tianhao Li and Tianyu Liu and Wenbin Ge and Xiaodong Deng and Xiaohuan Zhou and Xingzhang Ren and Xinyu Zhang and Xipin Wei and Xuancheng Ren and Yang Fan and Yang Yao and Yichang Zhang and Yu Wan and Yunfei Chu and Yuqiong Liu and Zeyu Cui and Zhenru Zhang and Zhihao Fan},\n    journal={arXiv preprint arXiv:2407.10671},\n    year={2024}\n}\n```\n\n## Contact Us\nIf you are interested to leave a message to either our research team or product team, join our [Discord](https://discord.gg/z3GAxXZ9Ce) or [WeChat groups](https://github.com/QwenLM/Qwen/blob/main/assets/wechat.png)!\n\n<p align=\"right\" style=\"font-size: 14px; color: #555; margin-top: 20px;\">\n    <a href=\"#readme-top\" style=\"text-decoration: none; color: #007bff; font-weight: bold;\">\n        ‚Üë Back to Top ‚Üë\n    </a>\n</p>\n"
        },
        {
          "name": "demo",
          "type": "tree",
          "content": null
        },
        {
          "name": "examples",
          "type": "tree",
          "content": null
        },
        {
          "name": "finetuning",
          "type": "tree",
          "content": null
        },
        {
          "name": "qwencoder-eval",
          "type": "tree",
          "content": null
        },
        {
          "name": "requirements.txt",
          "type": "blob",
          "size": 0.0537109375,
          "content": "torch\ntransformers==4.39.1\naccelerate\nsafetensors\nvllm\n"
        }
      ]
    }
  ]
}