{
  "metadata": {
    "timestamp": 1736559971231,
    "page": 757,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjc2MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "princeton-nlp/SimCSE",
      "stars": 3483,
      "defaultBranch": "main",
      "files": [
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 1.822265625,
          "content": "# Byte-compiled / optimized / DLL files\n__pycache__/\nsimcse/__pycache__/\n*.py[cod]\n*$py.class\n\n# C extensions\n*.so\n\n# Distribution / packaging\n.Python\nbuild/\ndevelop-eggs/\ndist/\ndownloads/\neggs/\n.eggs/\nlib/\nlib64/\nparts/\nsdist/\nvar/\nwheels/\npip-wheel-metadata/\nshare/python-wheels/\n*.egg-info/\n.installed.cfg\n*.egg\nMANIFEST\n\n# PyInstaller\n#  Usually these files are written by a python script from a template\n#  before PyInstaller builds the exe, so as to inject date/other infos into it.\n*.manifest\n*.spec\n\n# Installer logs\npip-log.txt\npip-delete-this-directory.txt\n\n# Unit test / coverage reports\nhtmlcov/\n.tox/\n.nox/\n.coverage\n.coverage.*\n.cache\nnosetests.xml\ncoverage.xml\n*.cover\n*.py,cover\n.hypothesis/\n.pytest_cache/\n\n# Translations\n*.mo\n*.pot\n\n# Django stuff:\n*.log\nlocal_settings.py\ndb.sqlite3\ndb.sqlite3-journal\n\n# Flask stuff:\ninstance/\n.webassets-cache\n\n# Scrapy stuff:\n.scrapy\n\n# Sphinx documentation\ndocs/_build/\n\n# PyBuilder\ntarget/\n\n# Jupyter Notebook\n.ipynb_checkpoints\n\n# IPython\nprofile_default/\nipython_config.py\n\n# pyenv\n.python-version\n\n# pipenv\n#   According to pypa/pipenv#598, it is recommended to include Pipfile.lock in version control.\n#   However, in case of collaboration, if having platform-specific dependencies or dependencies\n#   having no cross-platform support, pipenv may install dependencies that don't work, or not\n#   install all needed dependencies.\n#Pipfile.lock\n\n# PEP 582; used by e.g. github.com/David-OConnor/pyflow\n__pypackages__/\n\n# Celery stuff\ncelerybeat-schedule\ncelerybeat.pid\n\n# SageMath parsed files\n*.sage.py\n\n# Environments\n.env\n.venv\nenv/\nvenv/\nENV/\nenv.bak/\nvenv.bak/\n\n# Spyder project settings\n.spyderproject\n.spyproject\n\n# Rope project settings\n.ropeproject\n\n# mkdocs documentation\n/site\n\n# mypy\n.mypy_cache/\n.dmypy.json\ndmypy.json\n\n# Pyre type checker\n.pyre/\n.DS_Store\n.vscode\ndata\nresult\n.venv-cu11\nwandb\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 1.068359375,
          "content": "MIT License\n\nCopyright (c) 2021 Princeton Natural Language Processing\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 16.81640625,
          "content": "## SimCSE: Simple Contrastive Learning of Sentence Embeddings\n\nThis repository contains the code and pre-trained models for our paper [SimCSE: Simple Contrastive Learning of Sentence Embeddings](https://arxiv.org/abs/2104.08821).\n\n**************************** **Updates** ****************************\n\n<!-- Thanks for your interest in our repo! -->\n\n<!-- Probably you will think this as another *\"empty\"* repo of a preprint paper ðŸ¥±.\nWait a minute! The authors are working day and night ðŸ’ª, to make the code and models available, so you can explore our state-of-the-art sentence embeddings.\nWe anticipate the code will be out * **in one week** *. -->\n\n<!-- * 4/26: SimCSE is now on [Gradio Web Demo](https://gradio.app/g/AK391/SimCSE) (Thanks [@AK391](https://github.com/AK391)!). Try it out! -->\n* 8/31: Our paper has been accepted to EMNLP! Please check out our [updated paper](https://arxiv.org/pdf/2104.08821.pdf) (with updated numbers and baselines). \n* 5/12: We updated our [unsupervised models](#model-list) with new hyperparameters and better performance.\n* 5/10: We released our [sentence embedding tool](#getting-started) and [demo code](./demo).\n* 4/23: We released our [training code](#training).\n* 4/20: We released our [model checkpoints](#use-our-models-out-of-the-box) and [evaluation code](#evaluation).\n* 4/18: We released [our paper](https://arxiv.org/pdf/2104.08821.pdf). Check it out!\n\n\n## Quick Links\n\n  - [Overview](#overview)\n  - [Getting Started](#getting-started)\n  - [Model List](#model-list)\n  - [Use SimCSE with Huggingface](#use-simcse-with-huggingface)\n  - [Train SimCSE](#train-simcse)\n    - [Requirements](#requirements)\n    - [Evaluation](#evaluation)\n    - [Training](#training)\n  - [Bugs or Questions?](#bugs-or-questions)\n  - [Citation](#citation)\n  - [SimCSE Elsewhere](#simcse-elsewhere)\n\n## Overview\n\nWe propose a simple contrastive learning framework that works with both unlabeled and labeled data. Unsupervised SimCSE simply takes an input sentence and predicts itself in a contrastive learning framework, with only standard dropout used as noise. Our supervised SimCSE incorporates annotated pairs from NLI datasets into contrastive learning by using `entailment` pairs as positives and `contradiction` pairs as hard negatives. The following figure is an illustration of our models.\n\n![](figure/model.png)\n\n## Getting Started\n\nWe provide an easy-to-use sentence embedding tool based on our SimCSE model (see our [Wiki](https://github.com/princeton-nlp/SimCSE/wiki) for detailed usage). To use the tool, first install the `simcse` package from PyPI\n```bash\npip install simcse\n```\n\nOr directly install it from our code\n```bash\npython setup.py install\n```\n\nNote that if you want to enable GPU encoding, you should install the correct version of PyTorch that supports CUDA. See [PyTorch official website](https://pytorch.org) for instructions.\n\nAfter installing the package, you can load our model by just two lines of code\n```python\nfrom simcse import SimCSE\nmodel = SimCSE(\"princeton-nlp/sup-simcse-bert-base-uncased\")\n```\nSee [model list](#model-list) for a full list of available models. \n\nThen you can use our model for **encoding sentences into embeddings**\n```python\nembeddings = model.encode(\"A woman is reading.\")\n```\n\n**Compute the cosine similarities** between two groups of sentences\n```python\nsentences_a = ['A woman is reading.', 'A man is playing a guitar.']\nsentences_b = ['He plays guitar.', 'A woman is making a photo.']\nsimilarities = model.similarity(sentences_a, sentences_b)\n```\n\nOr build index for a group of sentences and **search** among them\n```python\nsentences = ['A woman is reading.', 'A man is playing a guitar.']\nmodel.build_index(sentences)\nresults = model.search(\"He plays guitar.\")\n```\n\nWe also support [faiss](https://github.com/facebookresearch/faiss), an efficient similarity search library. Just install the package following [instructions](https://github.com/princeton-nlp/SimCSE/wiki/Installation) here and `simcse` will automatically use `faiss` for efficient search.\n\n**WARNING**: We have found that `faiss` did not well support Nvidia AMPERE GPUs (3090 and A100). In that case, you should change to other GPUs or install the CPU version of `faiss` package.\n\nWe also provide an easy-to-build [demo website](./demo) to show how SimCSE can be used in sentence retrieval. The code is based on [DensePhrases](https://arxiv.org/abs/2012.12624)' [repo](https://github.com/princeton-nlp/DensePhrases) and [demo](http://densephrases.korea.ac.kr) (a lot of thanks to the authors of DensePhrases). \n\n## Model List\n\nOur released models are listed as following. You can import these models by using the `simcse` package or using [HuggingFace's Transformers](https://github.com/huggingface/transformers). \n|              Model              | Avg. STS |\n|:-------------------------------|:--------:|\n|  [princeton-nlp/unsup-simcse-bert-base-uncased](https://huggingface.co/princeton-nlp/unsup-simcse-bert-base-uncased) |   76.25 |\n| [princeton-nlp/unsup-simcse-bert-large-uncased](https://huggingface.co/princeton-nlp/unsup-simcse-bert-large-uncased) |   78.41  |\n|    [princeton-nlp/unsup-simcse-roberta-base](https://huggingface.co/princeton-nlp/unsup-simcse-roberta-base)    |   76.57  |\n|    [princeton-nlp/unsup-simcse-roberta-large](https://huggingface.co/princeton-nlp/unsup-simcse-roberta-large)   |   78.90  |\n|   [princeton-nlp/sup-simcse-bert-base-uncased](https://huggingface.co/princeton-nlp/sup-simcse-bert-base-uncased)  |   81.57  |\n|  [princeton-nlp/sup-simcse-bert-large-uncased](https://huggingface.co/princeton-nlp/sup-simcse-bert-large-uncased)  |   82.21  |\n|     [princeton-nlp/sup-simcse-roberta-base](https://huggingface.co/princeton-nlp/sup-simcse-roberta-base)     |   82.52  |\n|     [princeton-nlp/sup-simcse-roberta-large](https://huggingface.co/princeton-nlp/sup-simcse-roberta-large)    |   83.76  |\n\nNote that the results are slightly better than what we have reported in the current version of the paper after adopting a new set of hyperparameters (for hyperparamters, see the [training](#training) section).\n\n**Naming rules**: `unsup` and `sup` represent \"unsupervised\" (trained on Wikipedia corpus) and \"supervised\" (trained on NLI datasets) respectively.\n\n## Use SimCSE with Huggingface\n\nBesides using our provided sentence embedding tool, you can also easily import our models with HuggingFace's `transformers`:\n```python\nimport torch\nfrom scipy.spatial.distance import cosine\nfrom transformers import AutoModel, AutoTokenizer\n\n# Import our models. The package will take care of downloading the models automatically\ntokenizer = AutoTokenizer.from_pretrained(\"princeton-nlp/sup-simcse-bert-base-uncased\")\nmodel = AutoModel.from_pretrained(\"princeton-nlp/sup-simcse-bert-base-uncased\")\n\n# Tokenize input texts\ntexts = [\n    \"There's a kid on a skateboard.\",\n    \"A kid is skateboarding.\",\n    \"A kid is inside the house.\"\n]\ninputs = tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\")\n\n# Get the embeddings\nwith torch.no_grad():\n    embeddings = model(**inputs, output_hidden_states=True, return_dict=True).pooler_output\n\n# Calculate cosine similarities\n# Cosine similarities are in [-1, 1]. Higher means more similar\ncosine_sim_0_1 = 1 - cosine(embeddings[0], embeddings[1])\ncosine_sim_0_2 = 1 - cosine(embeddings[0], embeddings[2])\n\nprint(\"Cosine similarity between \\\"%s\\\" and \\\"%s\\\" is: %.3f\" % (texts[0], texts[1], cosine_sim_0_1))\nprint(\"Cosine similarity between \\\"%s\\\" and \\\"%s\\\" is: %.3f\" % (texts[0], texts[2], cosine_sim_0_2))\n```\n\nIf you encounter any problem when directly loading the models by HuggingFace's API, you can also download the models manually from the above table and use `model = AutoModel.from_pretrained({PATH TO THE DOWNLOAD MODEL})`.\n\n## Train SimCSE\n\nIn the following section, we describe how to train a SimCSE model by using our code.\n\n### Requirements\n\nFirst, install PyTorch by following the instructions from [the official website](https://pytorch.org). To faithfully reproduce our results, please use the correct `1.7.1` version corresponding to your platforms/CUDA versions. PyTorch version higher than `1.7.1` should also work. For example, if you use Linux and **CUDA11** ([how to check CUDA version](https://varhowto.com/check-cuda-version/)), install PyTorch by the following command,\n\n```bash\npip install torch==1.7.1+cu110 -f https://download.pytorch.org/whl/torch_stable.html\n```\n\nIf you instead use **CUDA** `<11` or **CPU**, install PyTorch by the following command,\n\n```bash\npip install torch==1.7.1\n```\n\n\nThen run the following script to install the remaining dependencies,\n\n```bash\npip install -r requirements.txt\n```\n\n### Evaluation\nOur evaluation code for sentence embeddings is based on a modified version of [SentEval](https://github.com/facebookresearch/SentEval). It evaluates sentence embeddings on semantic textual similarity (STS) tasks and downstream transfer tasks. For STS tasks, our evaluation takes the \"all\" setting, and report Spearman's correlation. See [our paper](https://arxiv.org/pdf/2104.08821.pdf) (Appendix B) for evaluation details.\n\nBefore evaluation, please download the evaluation datasets by running\n```bash\ncd SentEval/data/downstream/\nbash download_dataset.sh\n```\n\nThen come back to the root directory, you can evaluate any `transformers`-based pre-trained models using our evaluation code. For example,\n```bash\npython evaluation.py \\\n    --model_name_or_path princeton-nlp/sup-simcse-bert-base-uncased \\\n    --pooler cls \\\n    --task_set sts \\\n    --mode test\n```\nwhich is expected to output the results in a tabular format:\n```\n------ test ------\n+-------+-------+-------+-------+-------+--------------+-----------------+-------+\n| STS12 | STS13 | STS14 | STS15 | STS16 | STSBenchmark | SICKRelatedness |  Avg. |\n+-------+-------+-------+-------+-------+--------------+-----------------+-------+\n| 75.30 | 84.67 | 80.19 | 85.40 | 80.82 |    84.26     |      80.39      | 81.58 |\n+-------+-------+-------+-------+-------+--------------+-----------------+-------+\n```\n\nArguments for the evaluation script are as follows,\n\n* `--model_name_or_path`: The name or path of a `transformers`-based pre-trained checkpoint. You can directly use the models in the above table, e.g., `princeton-nlp/sup-simcse-bert-base-uncased`.\n* `--pooler`: Pooling method. Now we support\n    * `cls` (default): Use the representation of `[CLS]` token. A linear+activation layer is applied after the representation (it's in the standard BERT implementation). If you use **supervised SimCSE**, you should use this option.\n    * `cls_before_pooler`: Use the representation of `[CLS]` token without the extra linear+activation. If you use **unsupervised SimCSE**, you should take this option.\n    * `avg`: Average embeddings of the last layer. If you use checkpoints of SBERT/SRoBERTa ([paper](https://arxiv.org/abs/1908.10084)), you should use this option.\n    * `avg_top2`: Average embeddings of the last two layers.\n    * `avg_first_last`: Average embeddings of the first and last layers. If you use vanilla BERT or RoBERTa, this works the best. Note that in the paper we reported the average of last layer and the static word embedding; we fixed this to be last and first layer average and it led to better performance. See [this issue](https://github.com/princeton-nlp/SimCSE/issues/285) for a detailed discussion.\n* `--mode`: Evaluation mode\n    * `test` (default): The default test mode. To faithfully reproduce our results, you should use this option.\n    * `dev`: Report the development set results. Note that in STS tasks, only `STS-B` and `SICK-R` have development sets, so we only report their numbers. It also takes a fast mode for transfer tasks, so the running time is much shorter than the `test` mode (though numbers are slightly lower).\n    * `fasttest`: It is the same as `test`, but with a fast mode so the running time is much shorter, but the reported numbers may be lower (only for transfer tasks).\n* `--task_set`: What set of tasks to evaluate on (if set, it will override `--tasks`)\n    * `sts` (default): Evaluate on STS tasks, including `STS 12~16`, `STS-B` and `SICK-R`. This is the most commonly-used set of tasks to evaluate the quality of sentence embeddings.\n    * `transfer`: Evaluate on transfer tasks.\n    * `full`: Evaluate on both STS and transfer tasks.\n    * `na`: Manually set tasks by `--tasks`.\n* `--tasks`: Specify which dataset(s) to evaluate on. Will be overridden if `--task_set` is not `na`. See the code for a full list of tasks.\n\n### Training\n\n**Data**\n\nFor unsupervised SimCSE, we sample 1 million sentences from English Wikipedia; for supervised SimCSE, we use the SNLI and MNLI datasets. You can run `data/download_wiki.sh` and `data/download_nli.sh` to download the two datasets.\n\n**Training scripts**\n\nWe provide example training scripts for both unsupervised and supervised SimCSE. In `run_unsup_example.sh`, we provide a single-GPU (or CPU) example for the unsupervised version, and in `run_sup_example.sh` we give a **multiple-GPU** example for the supervised version. Both scripts call `train.py` for training. We explain the arguments in following:\n* `--train_file`: Training file path. We support \"txt\" files (one line for one sentence) and \"csv\" files (2-column: pair data with no hard negative; 3-column: pair data with one corresponding hard negative instance). You can use our provided Wikipedia or NLI data, or you can use your own data with the same format.\n* `--model_name_or_path`: Pre-trained checkpoints to start with. For now we support BERT-based models (`bert-base-uncased`, `bert-large-uncased`, etc.) and RoBERTa-based models (`RoBERTa-base`, `RoBERTa-large`, etc.).\n* `--temp`: Temperature for the contrastive loss.\n* `--pooler_type`: Pooling method. It's the same as the `--pooler_type` in the [evaluation part](#evaluation).\n* `--mlp_only_train`: We have found that for unsupervised SimCSE, it works better to train the model with MLP layer but test the model without it. You should use this argument when training unsupervised SimCSE models.\n* `--hard_negative_weight`: If using hard negatives (i.e., there are 3 columns in the training file), this is the logarithm of the weight. For example, if the weight is 1, then this argument should be set as 0 (default value).\n* `--do_mlm`: Whether to use the MLM auxiliary objective. If True:\n  * `--mlm_weight`: Weight for the MLM objective.\n  * `--mlm_probability`: Masking rate for the MLM objective.\n\nAll the other arguments are standard Huggingface's `transformers` training arguments. Some of the often-used arguments are: `--output_dir`, `--learning_rate`, `--per_device_train_batch_size`. In our example scripts, we also set to evaluate the model on the STS-B development set (need to download the dataset following the [evaluation](#evaluation) section) and save the best checkpoint.\n\nFor results in the paper, we use Nvidia 3090 GPUs with CUDA 11. Using different types of devices or different versions of CUDA/other softwares may lead to slightly different performance.\n\n**Hyperparameters**\n\nWe use the following hyperparamters for training SimCSE:\n\n|               | Unsup. BERT | Unsup. RoBERTa | Sup.      |\n|:--------------|:-----------:|:--------------:|:---------:|\n| Batch size    | 64          | 512            | 512       |\n| Learning rate (base)  | 3e-5 | 1e-5 | 5e-5 |\n| Learning rate (large) | 1e-5 | 3e-5 | 1e-5 |\n\n\n**Convert models**\n\nOur saved checkpoints are slightly different from Huggingface's pre-trained checkpoints. Run `python simcse_to_huggingface.py --path {PATH_TO_CHECKPOINT_FOLDER}` to convert it. After that, you can evaluate it by our [evaluation](#evaluation) code or directly use it [out of the box](#use-our-models-out-of-the-box).\n\n\n\n## Bugs or questions?\n\nIf you have any questions related to the code or the paper, feel free to email Tianyu (`tianyug@cs.princeton.edu`) and Xingcheng (`yxc18@mails.tsinghua.edu.cn`). If you encounter any problems when using the code, or want to report a bug, you can open an issue. Please try to specify the problem with details so we can help you better and quicker!\n\n## Citation\n\nPlease cite our paper if you use SimCSE in your work:\n\n```bibtex\n@inproceedings{gao2021simcse,\n   title={{SimCSE}: Simple Contrastive Learning of Sentence Embeddings},\n   author={Gao, Tianyu and Yao, Xingcheng and Chen, Danqi},\n   booktitle={Empirical Methods in Natural Language Processing (EMNLP)},\n   year={2021}\n}\n```\n\n## SimCSE Elsewhere\n\nWe thank the community's efforts for extending SimCSE!\n\n- [Jianlin Su](https://github.com/bojone) has provided [a Chinese version of SimCSE](https://github.com/bojone/SimCSE).\n- [AK391](https://github.com/AK391) integrated to [Huggingface Spaces](https://huggingface.co/spaces) with [Gradio](https://github.com/gradio-app/gradio). See demo: [![Hugging Face Spaces](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue)](https://huggingface.co/spaces/akhaliq/SimCSE)\n- [Nils Reimers](https://github.com/nreimers) has implemented a `sentence-transformers`-based [training code](https://colab.research.google.com/drive/1gAjXcI4uSxDE_IcvZdswFYVAo7XvPeoU?usp=sharing#scrollTo=UXUsikOc6oiB) for SimCSE.\n"
        },
        {
          "name": "SentEval",
          "type": "tree",
          "content": null
        },
        {
          "name": "data",
          "type": "tree",
          "content": null
        },
        {
          "name": "demo",
          "type": "tree",
          "content": null
        },
        {
          "name": "evaluation.py",
          "type": "blob",
          "size": 7.9365234375,
          "content": "import sys\nimport io, os\nimport numpy as np\nimport logging\nimport argparse\nfrom prettytable import PrettyTable\nimport torch\nimport transformers\nfrom transformers import AutoModel, AutoTokenizer\n\n# Set up logger\nlogging.basicConfig(format='%(asctime)s : %(message)s', level=logging.DEBUG)\n\n# Set PATHs\nPATH_TO_SENTEVAL = './SentEval'\nPATH_TO_DATA = './SentEval/data'\n\n# Import SentEval\nsys.path.insert(0, PATH_TO_SENTEVAL)\nimport senteval\n\ndef print_table(task_names, scores):\n    tb = PrettyTable()\n    tb.field_names = task_names\n    tb.add_row(scores)\n    print(tb)\n\ndef main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--model_name_or_path\", type=str, \n            help=\"Transformers' model name or path\")\n    parser.add_argument(\"--pooler\", type=str, \n            choices=['cls', 'cls_before_pooler', 'avg', 'avg_top2', 'avg_first_last'], \n            default='cls', \n            help=\"Which pooler to use\")\n    parser.add_argument(\"--mode\", type=str, \n            choices=['dev', 'test', 'fasttest'],\n            default='test', \n            help=\"What evaluation mode to use (dev: fast mode, dev results; test: full mode, test results); fasttest: fast mode, test results\")\n    parser.add_argument(\"--task_set\", type=str, \n            choices=['sts', 'transfer', 'full', 'na'],\n            default='sts',\n            help=\"What set of tasks to evaluate on. If not 'na', this will override '--tasks'\")\n    parser.add_argument(\"--tasks\", type=str, nargs='+', \n            default=['STS12', 'STS13', 'STS14', 'STS15', 'STS16',\n                     'MR', 'CR', 'MPQA', 'SUBJ', 'SST2', 'TREC', 'MRPC',\n                     'SICKRelatedness', 'STSBenchmark'], \n            help=\"Tasks to evaluate on. If '--task_set' is specified, this will be overridden\")\n    \n    args = parser.parse_args()\n    \n    # Load transformers' model checkpoint\n    model = AutoModel.from_pretrained(args.model_name_or_path)\n    tokenizer = AutoTokenizer.from_pretrained(args.model_name_or_path)\n    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n    model = model.to(device)\n    \n    # Set up the tasks\n    if args.task_set == 'sts':\n        args.tasks = ['STS12', 'STS13', 'STS14', 'STS15', 'STS16', 'STSBenchmark', 'SICKRelatedness']\n    elif args.task_set == 'transfer':\n        args.tasks = ['MR', 'CR', 'MPQA', 'SUBJ', 'SST2', 'TREC', 'MRPC']\n    elif args.task_set == 'full':\n        args.tasks = ['STS12', 'STS13', 'STS14', 'STS15', 'STS16', 'STSBenchmark', 'SICKRelatedness']\n        args.tasks += ['MR', 'CR', 'MPQA', 'SUBJ', 'SST2', 'TREC', 'MRPC']\n\n    # Set params for SentEval\n    if args.mode == 'dev' or args.mode == 'fasttest':\n        # Fast mode\n        params = {'task_path': PATH_TO_DATA, 'usepytorch': True, 'kfold': 5}\n        params['classifier'] = {'nhid': 0, 'optim': 'rmsprop', 'batch_size': 128,\n                                         'tenacity': 3, 'epoch_size': 2}\n    elif args.mode == 'test':\n        # Full mode\n        params = {'task_path': PATH_TO_DATA, 'usepytorch': True, 'kfold': 10}\n        params['classifier'] = {'nhid': 0, 'optim': 'adam', 'batch_size': 64,\n                                         'tenacity': 5, 'epoch_size': 4}\n    else:\n        raise NotImplementedError\n\n    # SentEval prepare and batcher\n    def prepare(params, samples):\n        return\n    \n    def batcher(params, batch, max_length=None):\n        # Handle rare token encoding issues in the dataset\n        if len(batch) >= 1 and len(batch[0]) >= 1 and isinstance(batch[0][0], bytes):\n            batch = [[word.decode('utf-8') for word in s] for s in batch]\n\n        sentences = [' '.join(s) for s in batch]\n\n        # Tokenization\n        if max_length is not None:\n            batch = tokenizer.batch_encode_plus(\n                sentences,\n                return_tensors='pt',\n                padding=True,\n                max_length=max_length,\n                truncation=True\n            )\n        else:\n            batch = tokenizer.batch_encode_plus(\n                sentences,\n                return_tensors='pt',\n                padding=True,\n            )\n\n        # Move to the correct device\n        for k in batch:\n            batch[k] = batch[k].to(device)\n        \n        # Get raw embeddings\n        with torch.no_grad():\n            outputs = model(**batch, output_hidden_states=True, return_dict=True)\n            last_hidden = outputs.last_hidden_state\n            pooler_output = outputs.pooler_output\n            hidden_states = outputs.hidden_states\n\n        # Apply different poolers\n        if args.pooler == 'cls':\n            # There is a linear+activation layer after CLS representation\n            return pooler_output.cpu()\n        elif args.pooler == 'cls_before_pooler':\n            return last_hidden[:, 0].cpu()\n        elif args.pooler == \"avg\":\n            return ((last_hidden * batch['attention_mask'].unsqueeze(-1)).sum(1) / batch['attention_mask'].sum(-1).unsqueeze(-1)).cpu()\n        elif args.pooler == \"avg_first_last\":\n            first_hidden = hidden_states[1]\n            last_hidden = hidden_states[-1]\n            pooled_result = ((first_hidden + last_hidden) / 2.0 * batch['attention_mask'].unsqueeze(-1)).sum(1) / batch['attention_mask'].sum(-1).unsqueeze(-1)\n            return pooled_result.cpu()\n        elif args.pooler == \"avg_top2\":\n            second_last_hidden = hidden_states[-2]\n            last_hidden = hidden_states[-1]\n            pooled_result = ((last_hidden + second_last_hidden) / 2.0 * batch['attention_mask'].unsqueeze(-1)).sum(1) / batch['attention_mask'].sum(-1).unsqueeze(-1)\n            return pooled_result.cpu()\n        else:\n            raise NotImplementedError\n\n    results = {}\n\n    for task in args.tasks:\n        se = senteval.engine.SE(params, batcher, prepare)\n        result = se.eval(task)\n        results[task] = result\n    \n    # Print evaluation results\n    if args.mode == 'dev':\n        print(\"------ %s ------\" % (args.mode))\n\n        task_names = []\n        scores = []\n        for task in ['STSBenchmark', 'SICKRelatedness']:\n            task_names.append(task)\n            if task in results:\n                scores.append(\"%.2f\" % (results[task]['dev']['spearman'][0] * 100))\n            else:\n                scores.append(\"0.00\")\n        print_table(task_names, scores)\n\n        task_names = []\n        scores = []\n        for task in ['MR', 'CR', 'SUBJ', 'MPQA', 'SST2', 'TREC', 'MRPC']:\n            task_names.append(task)\n            if task in results:\n                scores.append(\"%.2f\" % (results[task]['devacc']))    \n            else:\n                scores.append(\"0.00\")\n        task_names.append(\"Avg.\")\n        scores.append(\"%.2f\" % (sum([float(score) for score in scores]) / len(scores)))\n        print_table(task_names, scores)\n\n    elif args.mode == 'test' or args.mode == 'fasttest':\n        print(\"------ %s ------\" % (args.mode))\n\n        task_names = []\n        scores = []\n        for task in ['STS12', 'STS13', 'STS14', 'STS15', 'STS16', 'STSBenchmark', 'SICKRelatedness']:\n            task_names.append(task)\n            if task in results:\n                if task in ['STS12', 'STS13', 'STS14', 'STS15', 'STS16']:\n                    scores.append(\"%.2f\" % (results[task]['all']['spearman']['all'] * 100))\n                else:\n                    scores.append(\"%.2f\" % (results[task]['test']['spearman'].correlation * 100))\n            else:\n                scores.append(\"0.00\")\n        task_names.append(\"Avg.\")\n        scores.append(\"%.2f\" % (sum([float(score) for score in scores]) / len(scores)))\n        print_table(task_names, scores)\n\n        task_names = []\n        scores = []\n        for task in ['MR', 'CR', 'SUBJ', 'MPQA', 'SST2', 'TREC', 'MRPC']:\n            task_names.append(task)\n            if task in results:\n                scores.append(\"%.2f\" % (results[task]['acc']))    \n            else:\n                scores.append(\"0.00\")\n        task_names.append(\"Avg.\")\n        scores.append(\"%.2f\" % (sum([float(score) for score in scores]) / len(scores)))\n        print_table(task_names, scores)\n\n\nif __name__ == \"__main__\":\n    main()\n"
        },
        {
          "name": "figure",
          "type": "tree",
          "content": null
        },
        {
          "name": "requirements.txt",
          "type": "blob",
          "size": 0.0888671875,
          "content": "transformers==4.2.1\nscipy\ndatasets\npandas\nscikit-learn\nprettytable\ngradio\ntorch\nsetuptools\n"
        },
        {
          "name": "run_sup_example.sh",
          "type": "blob",
          "size": 1.166015625,
          "content": "#!/bin/bash\n\n# In this example, we show how to train SimCSE using multiple GPU cards and PyTorch's distributed data parallel on supervised NLI dataset.\n# Set how many GPUs to use\n\nNUM_GPU=4\n\n# Randomly set a port number\n# If you encounter \"address already used\" error, just run again or manually set an available port id.\nPORT_ID=$(expr $RANDOM + 1000)\n\n# Allow multiple threads\nexport OMP_NUM_THREADS=8\n\n# Use distributed data parallel\n# If you only want to use one card, uncomment the following line and comment the line with \"torch.distributed.launch\"\n# python train.py \\\npython -m torch.distributed.launch --nproc_per_node $NUM_GPU --master_port $PORT_ID train.py \\\n    --model_name_or_path bert-base-uncased \\\n    --train_file data/nli_for_simcse.csv \\\n    --output_dir result/my-sup-simcse-bert-base-uncased \\\n    --num_train_epochs 3 \\\n    --per_device_train_batch_size 128 \\\n    --learning_rate 5e-5 \\\n    --max_seq_length 32 \\\n    --evaluation_strategy steps \\\n    --metric_for_best_model stsb_spearman \\\n    --load_best_model_at_end \\\n    --eval_steps 125 \\\n    --pooler_type cls \\\n    --overwrite_output_dir \\\n    --temp 0.05 \\\n    --do_train \\\n    --do_eval \\\n    --fp16 \\\n    \"$@\"\n"
        },
        {
          "name": "run_unsup_example.sh",
          "type": "blob",
          "size": 0.775390625,
          "content": "#!/bin/bash\n\n# In this example, we show how to train SimCSE on unsupervised Wikipedia data.\n# If you want to train it with multiple GPU cards, see \"run_sup_example.sh\"\n# about how to use PyTorch's distributed data parallel.\n\npython train.py \\\n    --model_name_or_path bert-base-uncased \\\n    --train_file data/wiki1m_for_simcse.txt \\\n    --output_dir result/my-unsup-simcse-bert-base-uncased \\\n    --num_train_epochs 1 \\\n    --per_device_train_batch_size 64 \\\n    --learning_rate 3e-5 \\\n    --max_seq_length 32 \\\n    --evaluation_strategy steps \\\n    --metric_for_best_model stsb_spearman \\\n    --load_best_model_at_end \\\n    --eval_steps 125 \\\n    --pooler_type cls \\\n    --mlp_only_train \\\n    --overwrite_output_dir \\\n    --temp 0.05 \\\n    --do_train \\\n    --do_eval \\\n    --fp16 \\\n    \"$@\"\n"
        },
        {
          "name": "setup.py",
          "type": "blob",
          "size": 0.7490234375,
          "content": "import io\nfrom setuptools import setup, find_packages\n\nwith io.open('./README.md', encoding='utf-8') as f:\n    readme = f.read()\n\nsetup(\n    name='simcse',\n    packages=['simcse'],\n    version='0.4',\n    license='MIT',\n    description='A sentence embedding tool based on SimCSE',\n    author='Tianyu Gao, Xingcheng Yao, Danqi Chen',\n    author_email='tianyug@cs.princeton.edu',\n    url='https://github.com/princeton-nlp/SimCSE',\n    download_url='https://github.com/princeton-nlp/SimCSE/archive/refs/tags/0.4.tar.gz',\n    keywords=['sentence', 'embedding', 'simcse', 'nlp'],\n    install_requires=[\n        \"tqdm\",\n        \"scikit-learn\",\n        \"scipy>=1.5.4,<1.6\",\n        \"transformers\",\n        \"torch\",\n        \"numpy>=1.19.5,<1.20\",\n        \"setuptools\"\n    ]\n)\n"
        },
        {
          "name": "simcse",
          "type": "tree",
          "content": null
        },
        {
          "name": "simcse_to_huggingface.py",
          "type": "blob",
          "size": 1.2958984375,
          "content": "\"\"\"\nConvert SimCSE's checkpoints to Huggingface style.\n\"\"\"\n\nimport argparse\nimport torch\nimport os\nimport json\n\n\ndef main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--path\", type=str, help=\"Path of SimCSE checkpoint folder\")\n    args = parser.parse_args()\n\n    print(\"SimCSE checkpoint -> Huggingface checkpoint for {}\".format(args.path))\n\n    state_dict = torch.load(os.path.join(args.path, \"pytorch_model.bin\"), map_location=torch.device(\"cpu\"))\n    new_state_dict = {}\n    for key, param in state_dict.items():\n        # Replace \"mlp\" to \"pooler\"\n        if \"mlp\" in key:\n            key = key.replace(\"mlp\", \"pooler\")\n\n        # Delete \"bert\" or \"roberta\" prefix\n        if \"bert.\" in key:\n            key = key.replace(\"bert.\", \"\")\n        if \"roberta.\" in key:\n            key = key.replace(\"roberta.\", \"\")\n\n        new_state_dict[key] = param\n\n    torch.save(new_state_dict, os.path.join(args.path, \"pytorch_model.bin\"))\n\n    # Change architectures in config.json\n    config = json.load(open(os.path.join(args.path, \"config.json\")))\n    for i in range(len(config[\"architectures\"])):\n        config[\"architectures\"][i] = config[\"architectures\"][i].replace(\"ForCL\", \"Model\")\n    json.dump(config, open(os.path.join(args.path, \"config.json\"), \"w\"), indent=2)\n\n\nif __name__ == \"__main__\":\n    main()\n"
        },
        {
          "name": "slides",
          "type": "tree",
          "content": null
        },
        {
          "name": "train.py",
          "type": "blob",
          "size": 23.4765625,
          "content": "import logging\nimport math\nimport os\nimport sys\nfrom dataclasses import dataclass, field\nfrom typing import Optional, Union, List, Dict, Tuple\nimport torch\nimport collections\nimport random\n\nfrom datasets import load_dataset\n\nimport transformers\nfrom transformers import (\n    CONFIG_MAPPING,\n    MODEL_FOR_MASKED_LM_MAPPING,\n    AutoConfig,\n    AutoModelForMaskedLM,\n    AutoModelForSequenceClassification,\n    AutoTokenizer,\n    DataCollatorForLanguageModeling,\n    DataCollatorWithPadding,\n    HfArgumentParser,\n    Trainer,\n    TrainingArguments,\n    default_data_collator,\n    set_seed,\n    EvalPrediction,\n    BertModel,\n    BertForPreTraining,\n    RobertaModel\n)\nfrom transformers.tokenization_utils_base import BatchEncoding, PaddingStrategy, PreTrainedTokenizerBase\nfrom transformers.trainer_utils import is_main_process\nfrom transformers.data.data_collator import DataCollatorForLanguageModeling\nfrom transformers.file_utils import cached_property, torch_required, is_torch_available, is_torch_tpu_available\nfrom simcse.models import RobertaForCL, BertForCL\nfrom simcse.trainers import CLTrainer\n\nlogger = logging.getLogger(__name__)\nMODEL_CONFIG_CLASSES = list(MODEL_FOR_MASKED_LM_MAPPING.keys())\nMODEL_TYPES = tuple(conf.model_type for conf in MODEL_CONFIG_CLASSES)\n\n@dataclass\nclass ModelArguments:\n    \"\"\"\n    Arguments pertaining to which model/config/tokenizer we are going to fine-tune, or train from scratch.\n    \"\"\"\n\n    # Huggingface's original arguments\n    model_name_or_path: Optional[str] = field(\n        default=None,\n        metadata={\n            \"help\": \"The model checkpoint for weights initialization.\"\n            \"Don't set if you want to train a model from scratch.\"\n        },\n    )\n    model_type: Optional[str] = field(\n        default=None,\n        metadata={\"help\": \"If training from scratch, pass a model type from the list: \" + \", \".join(MODEL_TYPES)},\n    )\n    config_name: Optional[str] = field(\n        default=None, metadata={\"help\": \"Pretrained config name or path if not the same as model_name\"}\n    )\n    tokenizer_name: Optional[str] = field(\n        default=None, metadata={\"help\": \"Pretrained tokenizer name or path if not the same as model_name\"}\n    )\n    cache_dir: Optional[str] = field(\n        default=None,\n        metadata={\"help\": \"Where do you want to store the pretrained models downloaded from huggingface.co\"},\n    )\n    use_fast_tokenizer: bool = field(\n        default=True,\n        metadata={\"help\": \"Whether to use one of the fast tokenizer (backed by the tokenizers library) or not.\"},\n    )\n    model_revision: str = field(\n        default=\"main\",\n        metadata={\"help\": \"The specific model version to use (can be a branch name, tag name or commit id).\"},\n    )\n    use_auth_token: bool = field(\n        default=False,\n        metadata={\n            \"help\": \"Will use the token generated when running `transformers-cli login` (necessary to use this script \"\n            \"with private models).\"\n        },\n    )\n\n    # SimCSE's arguments\n    temp: float = field(\n        default=0.05,\n        metadata={\n            \"help\": \"Temperature for softmax.\"\n        }\n    )\n    pooler_type: str = field(\n        default=\"cls\",\n        metadata={\n            \"help\": \"What kind of pooler to use (cls, cls_before_pooler, avg, avg_top2, avg_first_last).\"\n        }\n    ) \n    hard_negative_weight: float = field(\n        default=0,\n        metadata={\n            \"help\": \"The **logit** of weight for hard negatives (only effective if hard negatives are used).\"\n        }\n    )\n    do_mlm: bool = field(\n        default=False,\n        metadata={\n            \"help\": \"Whether to use MLM auxiliary objective.\"\n        }\n    )\n    mlm_weight: float = field(\n        default=0.1,\n        metadata={\n            \"help\": \"Weight for MLM auxiliary objective (only effective if --do_mlm).\"\n        }\n    )\n    mlp_only_train: bool = field(\n        default=False,\n        metadata={\n            \"help\": \"Use MLP only during training\"\n        }\n    )\n\n\n@dataclass\nclass DataTrainingArguments:\n    \"\"\"\n    Arguments pertaining to what data we are going to input our model for training and eval.\n    \"\"\"\n\n    # Huggingface's original arguments. \n    dataset_name: Optional[str] = field(\n        default=None, metadata={\"help\": \"The name of the dataset to use (via the datasets library).\"}\n    )\n    dataset_config_name: Optional[str] = field(\n        default=None, metadata={\"help\": \"The configuration name of the dataset to use (via the datasets library).\"}\n    )\n    overwrite_cache: bool = field(\n        default=False, metadata={\"help\": \"Overwrite the cached training and evaluation sets\"}\n    )\n    validation_split_percentage: Optional[int] = field(\n        default=5,\n        metadata={\n            \"help\": \"The percentage of the train set used as validation set in case there's no validation split\"\n        },\n    )\n    preprocessing_num_workers: Optional[int] = field(\n        default=None,\n        metadata={\"help\": \"The number of processes to use for the preprocessing.\"},\n    )\n\n    # SimCSE's arguments\n    train_file: Optional[str] = field(\n        default=None, \n        metadata={\"help\": \"The training data file (.txt or .csv).\"}\n    )\n    max_seq_length: Optional[int] = field(\n        default=32,\n        metadata={\n            \"help\": \"The maximum total input sequence length after tokenization. Sequences longer \"\n            \"than this will be truncated.\"\n        },\n    )\n    pad_to_max_length: bool = field(\n        default=False,\n        metadata={\n            \"help\": \"Whether to pad all samples to `max_seq_length`. \"\n            \"If False, will pad the samples dynamically when batching to the maximum length in the batch.\"\n        },\n    )\n    mlm_probability: float = field(\n        default=0.15, \n        metadata={\"help\": \"Ratio of tokens to mask for MLM (only effective if --do_mlm)\"}\n    )\n\n    def __post_init__(self):\n        if self.dataset_name is None and self.train_file is None and self.validation_file is None:\n            raise ValueError(\"Need either a dataset name or a training/validation file.\")\n        else:\n            if self.train_file is not None:\n                extension = self.train_file.split(\".\")[-1]\n                assert extension in [\"csv\", \"json\", \"txt\"], \"`train_file` should be a csv, a json or a txt file.\"\n\n\n@dataclass\nclass OurTrainingArguments(TrainingArguments):\n    # Evaluation\n    ## By default, we evaluate STS (dev) during training (for selecting best checkpoints) and evaluate \n    ## both STS and transfer tasks (dev) at the end of training. Using --eval_transfer will allow evaluating\n    ## both STS and transfer tasks (dev) during training.\n    eval_transfer: bool = field(\n        default=False,\n        metadata={\"help\": \"Evaluate transfer task dev sets (in validation).\"}\n    )\n\n    @cached_property\n    @torch_required\n    def _setup_devices(self) -> \"torch.device\":\n        logger.info(\"PyTorch: setting up devices\")\n        if self.no_cuda:\n            device = torch.device(\"cpu\")\n            self._n_gpu = 0\n        elif is_torch_tpu_available():\n            import torch_xla.core.xla_model as xm\n            device = xm.xla_device()\n            self._n_gpu = 0\n        elif self.local_rank == -1:\n            # if n_gpu is > 1 we'll use nn.DataParallel.\n            # If you only want to use a specific subset of GPUs use `CUDA_VISIBLE_DEVICES=0`\n            # Explicitly set CUDA to the first (index 0) CUDA device, otherwise `set_device` will\n            # trigger an error that a device index is missing. Index 0 takes into account the\n            # GPUs available in the environment, so `CUDA_VISIBLE_DEVICES=1,2` with `cuda:0`\n            # will use the first GPU in that env, i.e. GPU#1\n            device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n            # Sometimes the line in the postinit has not been run before we end up here, so just checking we're not at\n            # the default value.\n            self._n_gpu = torch.cuda.device_count()\n        else:\n            # Here, we'll use torch.distributed.\n            # Initializes the distributed backend which will take care of synchronizing nodes/GPUs\n            #\n            # deepspeed performs its own DDP internally, and requires the program to be started with:\n            # deepspeed  ./program.py\n            # rather than:\n            # python -m torch.distributed.launch --nproc_per_node=2 ./program.py\n            if self.deepspeed:\n                from .integrations import is_deepspeed_available\n\n                if not is_deepspeed_available():\n                    raise ImportError(\"--deepspeed requires deepspeed: `pip install deepspeed`.\")\n                import deepspeed\n\n                deepspeed.init_distributed()\n            else:\n                torch.distributed.init_process_group(backend=\"nccl\")\n            device = torch.device(\"cuda\", self.local_rank)\n            self._n_gpu = 1\n\n        if device.type == \"cuda\":\n            torch.cuda.set_device(device)\n\n        return device\n\n\ndef main():\n    # See all possible arguments in src/transformers/training_args.py\n    # or by passing the --help flag to this script.\n    # We now keep distinct sets of args, for a cleaner separation of concerns.\n\n    parser = HfArgumentParser((ModelArguments, DataTrainingArguments, OurTrainingArguments))\n    if len(sys.argv) == 2 and sys.argv[1].endswith(\".json\"):\n        # If we pass only one argument to the script and it's the path to a json file,\n        # let's parse it to get our arguments.\n        model_args, data_args, training_args = parser.parse_json_file(json_file=os.path.abspath(sys.argv[1]))\n    else:\n        model_args, data_args, training_args = parser.parse_args_into_dataclasses()\n\n    if (\n        os.path.exists(training_args.output_dir)\n        and os.listdir(training_args.output_dir)\n        and training_args.do_train\n        and not training_args.overwrite_output_dir\n    ):\n        raise ValueError(\n            f\"Output directory ({training_args.output_dir}) already exists and is not empty.\"\n            \"Use --overwrite_output_dir to overcome.\"\n        )\n\n    # Setup logging\n    logging.basicConfig(\n        format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n        datefmt=\"%m/%d/%Y %H:%M:%S\",\n        level=logging.INFO if is_main_process(training_args.local_rank) else logging.WARN,\n    )\n\n    # Log on each process the small summary:\n    logger.warning(\n        f\"Process rank: {training_args.local_rank}, device: {training_args.device}, n_gpu: {training_args.n_gpu}\"\n        + f\" distributed training: {bool(training_args.local_rank != -1)}, 16-bits training: {training_args.fp16}\"\n    )\n    # Set the verbosity to info of the Transformers logger (on main process only):\n    if is_main_process(training_args.local_rank):\n        transformers.utils.logging.set_verbosity_info()\n        transformers.utils.logging.enable_default_handler()\n        transformers.utils.logging.enable_explicit_format()\n    logger.info(\"Training/evaluation parameters %s\", training_args)\n\n    # Set seed before initializing model.\n    set_seed(training_args.seed)\n\n    # Get the datasets: you can either provide your own CSV/JSON/TXT training and evaluation files (see below)\n    # or just provide the name of one of the public datasets available on the hub at https://huggingface.co/datasets/\n    # (the dataset will be downloaded automatically from the datasets Hub\n    #\n    # For CSV/JSON files, this script will use the column called 'text' or the first column. You can easily tweak this\n    # behavior (see below)\n    #\n    # In distributed training, the load_dataset function guarantee that only one local process can concurrently\n    # download the dataset.\n    data_files = {}\n    if data_args.train_file is not None:\n        data_files[\"train\"] = data_args.train_file\n    extension = data_args.train_file.split(\".\")[-1]\n    if extension == \"txt\":\n        extension = \"text\"\n    if extension == \"csv\":\n        datasets = load_dataset(extension, data_files=data_files, cache_dir=\"./data/\", delimiter=\"\\t\" if \"tsv\" in data_args.train_file else \",\")\n    else:\n        datasets = load_dataset(extension, data_files=data_files, cache_dir=\"./data/\")\n\n    # See more about loading any type of standard or custom dataset (from files, python dict, pandas DataFrame, etc) at\n    # https://huggingface.co/docs/datasets/loading_datasets.html.\n\n    # Load pretrained model and tokenizer\n    #\n    # Distributed training:\n    # The .from_pretrained methods guarantee that only one local process can concurrently\n    # download model & vocab.\n    config_kwargs = {\n        \"cache_dir\": model_args.cache_dir,\n        \"revision\": model_args.model_revision,\n        \"use_auth_token\": True if model_args.use_auth_token else None,\n    }\n    if model_args.config_name:\n        config = AutoConfig.from_pretrained(model_args.config_name, **config_kwargs)\n    elif model_args.model_name_or_path:\n        config = AutoConfig.from_pretrained(model_args.model_name_or_path, **config_kwargs)\n    else:\n        config = CONFIG_MAPPING[model_args.model_type]()\n        logger.warning(\"You are instantiating a new config instance from scratch.\")\n\n    tokenizer_kwargs = {\n        \"cache_dir\": model_args.cache_dir,\n        \"use_fast\": model_args.use_fast_tokenizer,\n        \"revision\": model_args.model_revision,\n        \"use_auth_token\": True if model_args.use_auth_token else None,\n    }\n    if model_args.tokenizer_name:\n        tokenizer = AutoTokenizer.from_pretrained(model_args.tokenizer_name, **tokenizer_kwargs)\n    elif model_args.model_name_or_path:\n        tokenizer = AutoTokenizer.from_pretrained(model_args.model_name_or_path, **tokenizer_kwargs)\n    else:\n        raise ValueError(\n            \"You are instantiating a new tokenizer from scratch. This is not supported by this script.\"\n            \"You can do it from another script, save it, and load it from here, using --tokenizer_name.\"\n        )\n\n    if model_args.model_name_or_path:\n        if 'roberta' in model_args.model_name_or_path:\n            model = RobertaForCL.from_pretrained(\n                model_args.model_name_or_path,\n                from_tf=bool(\".ckpt\" in model_args.model_name_or_path),\n                config=config,\n                cache_dir=model_args.cache_dir,\n                revision=model_args.model_revision,\n                use_auth_token=True if model_args.use_auth_token else None,\n                model_args=model_args                  \n            )\n        elif 'bert' in model_args.model_name_or_path:\n            model = BertForCL.from_pretrained(\n                model_args.model_name_or_path,\n                from_tf=bool(\".ckpt\" in model_args.model_name_or_path),\n                config=config,\n                cache_dir=model_args.cache_dir,\n                revision=model_args.model_revision,\n                use_auth_token=True if model_args.use_auth_token else None,\n                model_args=model_args\n            )\n            if model_args.do_mlm:\n                pretrained_model = BertForPreTraining.from_pretrained(model_args.model_name_or_path)\n                model.lm_head.load_state_dict(pretrained_model.cls.predictions.state_dict())\n        else:\n            raise NotImplementedError\n    else:\n        raise NotImplementedError\n        logger.info(\"Training new model from scratch\")\n        model = AutoModelForMaskedLM.from_config(config)\n\n    model.resize_token_embeddings(len(tokenizer))\n\n    # Prepare features\n    column_names = datasets[\"train\"].column_names\n    sent2_cname = None\n    if len(column_names) == 2:\n        # Pair datasets\n        sent0_cname = column_names[0]\n        sent1_cname = column_names[1]\n    elif len(column_names) == 3:\n        # Pair datasets with hard negatives\n        sent0_cname = column_names[0]\n        sent1_cname = column_names[1]\n        sent2_cname = column_names[2]\n    elif len(column_names) == 1:\n        # Unsupervised datasets\n        sent0_cname = column_names[0]\n        sent1_cname = column_names[0]\n    else:\n        raise NotImplementedError\n\n    def prepare_features(examples):\n        # padding = longest (default)\n        #   If no sentence in the batch exceed the max length, then use\n        #   the max sentence length in the batch, otherwise use the \n        #   max sentence length in the argument and truncate those that\n        #   exceed the max length.\n        # padding = max_length (when pad_to_max_length, for pressure test)\n        #   All sentences are padded/truncated to data_args.max_seq_length.\n        total = len(examples[sent0_cname])\n\n        # Avoid \"None\" fields \n        for idx in range(total):\n            if examples[sent0_cname][idx] is None:\n                examples[sent0_cname][idx] = \" \"\n            if examples[sent1_cname][idx] is None:\n                examples[sent1_cname][idx] = \" \"\n        \n        sentences = examples[sent0_cname] + examples[sent1_cname]\n\n        # If hard negative exists\n        if sent2_cname is not None:\n            for idx in range(total):\n                if examples[sent2_cname][idx] is None:\n                    examples[sent2_cname][idx] = \" \"\n            sentences += examples[sent2_cname]\n\n        sent_features = tokenizer(\n            sentences,\n            max_length=data_args.max_seq_length,\n            truncation=True,\n            padding=\"max_length\" if data_args.pad_to_max_length else False,\n        )\n\n        features = {}\n        if sent2_cname is not None:\n            for key in sent_features:\n                features[key] = [[sent_features[key][i], sent_features[key][i+total], sent_features[key][i+total*2]] for i in range(total)]\n        else:\n            for key in sent_features:\n                features[key] = [[sent_features[key][i], sent_features[key][i+total]] for i in range(total)]\n            \n        return features\n\n    if training_args.do_train:\n        train_dataset = datasets[\"train\"].map(\n            prepare_features,\n            batched=True,\n            num_proc=data_args.preprocessing_num_workers,\n            remove_columns=column_names,\n            load_from_cache_file=not data_args.overwrite_cache,\n        )\n\n    # Data collator\n    @dataclass\n    class OurDataCollatorWithPadding:\n\n        tokenizer: PreTrainedTokenizerBase\n        padding: Union[bool, str, PaddingStrategy] = True\n        max_length: Optional[int] = None\n        pad_to_multiple_of: Optional[int] = None\n        mlm: bool = True\n        mlm_probability: float = data_args.mlm_probability\n\n        def __call__(self, features: List[Dict[str, Union[List[int], List[List[int]], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n            special_keys = ['input_ids', 'attention_mask', 'token_type_ids', 'mlm_input_ids', 'mlm_labels']\n            bs = len(features)\n            if bs > 0:\n                num_sent = len(features[0]['input_ids'])\n            else:\n                return\n            flat_features = []\n            for feature in features:\n                for i in range(num_sent):\n                    flat_features.append({k: feature[k][i] if k in special_keys else feature[k] for k in feature})\n\n            batch = self.tokenizer.pad(\n                flat_features,\n                padding=self.padding,\n                max_length=self.max_length,\n                pad_to_multiple_of=self.pad_to_multiple_of,\n                return_tensors=\"pt\",\n            )\n            if model_args.do_mlm:\n                batch[\"mlm_input_ids\"], batch[\"mlm_labels\"] = self.mask_tokens(batch[\"input_ids\"])\n\n            batch = {k: batch[k].view(bs, num_sent, -1) if k in special_keys else batch[k].view(bs, num_sent, -1)[:, 0] for k in batch}\n\n            if \"label\" in batch:\n                batch[\"labels\"] = batch[\"label\"]\n                del batch[\"label\"]\n            if \"label_ids\" in batch:\n                batch[\"labels\"] = batch[\"label_ids\"]\n                del batch[\"label_ids\"]\n\n            return batch\n        \n        def mask_tokens(\n            self, inputs: torch.Tensor, special_tokens_mask: Optional[torch.Tensor] = None\n        ) -> Tuple[torch.Tensor, torch.Tensor]:\n            \"\"\"\n            Prepare masked tokens inputs/labels for masked language modeling: 80% MASK, 10% random, 10% original.\n            \"\"\"\n            inputs = inputs.clone()\n            labels = inputs.clone()\n            # We sample a few tokens in each sequence for MLM training (with probability `self.mlm_probability`)\n            probability_matrix = torch.full(labels.shape, self.mlm_probability)\n            if special_tokens_mask is None:\n                special_tokens_mask = [\n                    self.tokenizer.get_special_tokens_mask(val, already_has_special_tokens=True) for val in labels.tolist()\n                ]\n                special_tokens_mask = torch.tensor(special_tokens_mask, dtype=torch.bool)\n            else:\n                special_tokens_mask = special_tokens_mask.bool()\n\n            probability_matrix.masked_fill_(special_tokens_mask, value=0.0)\n            masked_indices = torch.bernoulli(probability_matrix).bool()\n            labels[~masked_indices] = -100  # We only compute loss on masked tokens\n\n            # 80% of the time, we replace masked input tokens with tokenizer.mask_token ([MASK])\n            indices_replaced = torch.bernoulli(torch.full(labels.shape, 0.8)).bool() & masked_indices\n            inputs[indices_replaced] = self.tokenizer.convert_tokens_to_ids(self.tokenizer.mask_token)\n\n            # 10% of the time, we replace masked input tokens with random word\n            indices_random = torch.bernoulli(torch.full(labels.shape, 0.5)).bool() & masked_indices & ~indices_replaced\n            random_words = torch.randint(len(self.tokenizer), labels.shape, dtype=torch.long)\n            inputs[indices_random] = random_words[indices_random]\n\n            # The rest of the time (10% of the time) we keep the masked input tokens unchanged\n            return inputs, labels\n\n    data_collator = default_data_collator if data_args.pad_to_max_length else OurDataCollatorWithPadding(tokenizer)\n\n    trainer = CLTrainer(\n        model=model,\n        args=training_args,\n        train_dataset=train_dataset if training_args.do_train else None,\n        tokenizer=tokenizer,\n        data_collator=data_collator,\n    )\n    trainer.model_args = model_args\n\n    # Training\n    if training_args.do_train:\n        model_path = (\n            model_args.model_name_or_path\n            if (model_args.model_name_or_path is not None and os.path.isdir(model_args.model_name_or_path))\n            else None\n        )\n        train_result = trainer.train(model_path=model_path)\n        trainer.save_model()  # Saves the tokenizer too for easy upload\n\n        output_train_file = os.path.join(training_args.output_dir, \"train_results.txt\")\n        if trainer.is_world_process_zero():\n            with open(output_train_file, \"w\") as writer:\n                logger.info(\"***** Train results *****\")\n                for key, value in sorted(train_result.metrics.items()):\n                    logger.info(f\"  {key} = {value}\")\n                    writer.write(f\"{key} = {value}\\n\")\n\n            # Need to save the state, since Trainer.save_model saves only the tokenizer with the model\n            trainer.state.save_to_json(os.path.join(training_args.output_dir, \"trainer_state.json\"))\n\n    # Evaluation\n    results = {}\n    if training_args.do_eval:\n        logger.info(\"*** Evaluate ***\")\n        results = trainer.evaluate(eval_senteval_transfer=True)\n\n        output_eval_file = os.path.join(training_args.output_dir, \"eval_results.txt\")\n        if trainer.is_world_process_zero():\n            with open(output_eval_file, \"w\") as writer:\n                logger.info(\"***** Eval results *****\")\n                for key, value in sorted(results.items()):\n                    logger.info(f\"  {key} = {value}\")\n                    writer.write(f\"{key} = {value}\\n\")\n\n    return results\n\ndef _mp_fn(index):\n    # For xla_spawn (TPUs)\n    main()\n\n\nif __name__ == \"__main__\":\n    main()\n"
        }
      ]
    }
  ]
}