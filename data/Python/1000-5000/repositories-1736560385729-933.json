{
  "metadata": {
    "timestamp": 1736560385729,
    "page": 933,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjk0MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "PetarV-/GAT",
      "stars": 3272,
      "defaultBranch": "master",
      "files": [
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 1.1298828125,
          "content": "# Byte-compiled / optimized / DLL files\n__pycache__/\n*.py[cod]\n*$py.class\n\n# C extensions\n*.so\n\n# Distribution / packaging\n.Python\nenv/\nbuild/\ndevelop-eggs/\ndist/\ndownloads/\neggs/\n.eggs/\nlib/\nlib64/\nparts/\nsdist/\nvar/\nwheels/\n*.egg-info/\n.installed.cfg\n*.egg\n\n# PyInstaller\n#  Usually these files are written by a python script from a template\n#  before PyInstaller builds the exe, so as to inject date/other infos into it.\n*.manifest\n*.spec\n\n# Installer logs\npip-log.txt\npip-delete-this-directory.txt\n\n# Unit test / coverage reports\nhtmlcov/\n.tox/\n.coverage\n.coverage.*\n.cache\nnosetests.xml\ncoverage.xml\n*.cover\n.hypothesis/\n\n# Translations\n*.mo\n*.pot\n\n# Django stuff:\n*.log\nlocal_settings.py\n\n# Flask stuff:\ninstance/\n.webassets-cache\n\n# Scrapy stuff:\n.scrapy\n\n# Sphinx documentation\ndocs/_build/\n\n# PyBuilder\ntarget/\n\n# Jupyter Notebook\n.ipynb_checkpoints\n\n# pyenv\n.python-version\n\n# celery beat schedule file\ncelerybeat-schedule\n\n# SageMath parsed files\n*.sage.py\n\n# dotenv\n.env\n\n# virtualenv\n.venv\nvenv/\nENV/\n\n# Spyder project settings\n.spyderproject\n.spyproject\n\n# Rope project settings\n.ropeproject\n\n# mkdocs documentation\n/site\n\n# mypy\n.mypy_cache/\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 1.0498046875,
          "content": "MIT License\n\nCopyright (c) 2018 Petar Veličković\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 3.6162109375,
          "content": "# GAT\nGraph Attention Networks (Veličković *et al.*, ICLR 2018): [https://arxiv.org/abs/1710.10903](https://arxiv.org/abs/1710.10903)\n\nGAT layer            |  t-SNE + Attention coefficients on Cora\n:-------------------------:|:-------------------------:\n![](https://camo.githubusercontent.com/4fe1a90e67d17a2330d7cfcddc930d5f7501750c/68747470733a2f2f7777772e64726f70626f782e636f6d2f732f71327a703170366b37396a6a6431352f6761745f6c617965722e706e673f7261773d31)  |  ![](https://raw.githubusercontent.com/PetarV-/GAT/gh-pages/assets/t-sne.png)\n\n## Overview\nHere we provide the implementation of a Graph Attention Network (GAT) layer in TensorFlow, along with a minimal execution example (on the Cora dataset). The repository is organised as follows:\n- `data/` contains the necessary dataset files for Cora;\n- `models/` contains the implementation of the GAT network (`gat.py`);\n- `pre_trained/` contains a pre-trained Cora model (achieving 84.4% accuracy on the test set);\n- `utils/` contains:\n    * an implementation of an attention head, along with an experimental sparse version (`layers.py`);\n    * preprocessing subroutines (`process.py`);\n    * preprocessing utilities for the PPI benchmark (`process_ppi.py`).\n\nFinally, `execute_cora.py` puts all of the above together and may be used to execute a full training run on Cora.\n\n## Sparse version\nAn experimental sparse version is also available, working only when the batch size is equal to 1.\nThe sparse model may be found at `models/sp_gat.py`.\n\nYou may execute a full training run of the sparse model on Cora through `execute_cora_sparse.py`.\n\n## Dependencies\n\nThe script has been tested running under Python 3.5.2, with the following packages installed (along with their dependencies):\n\n- `numpy==1.14.1`\n- `scipy==1.0.0`\n- `networkx==2.1`\n- `tensorflow-gpu==1.6.0`\n\nIn addition, CUDA 9.0 and cuDNN 7 have been used.\n\n## Reference\nIf you make advantage of the GAT model in your research, please cite the following in your manuscript:\n\n```\n@article{\n  velickovic2018graph,\n  title=\"{Graph Attention Networks}\",\n  author={Veli{\\v{c}}kovi{\\'{c}}, Petar and Cucurull, Guillem and Casanova, Arantxa and Romero, Adriana and Li{\\`{o}}, Pietro and Bengio, Yoshua},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJXMpikCZ},\n  note={accepted as poster},\n}\n```\n\nFor getting started with GATs, as well as graph representation learning in general, we **highly** recommend the [pytorch-GAT](https://github.com/gordicaleksa/pytorch-GAT) repository by [Aleksa Gordić](https://github.com/gordicaleksa). It ships with an inductive (PPI) example as well.\n\nGAT is a popular method for graph representation learning, with optimised implementations within virtually all standard GRL libraries:\n- \\[PyTorch\\] [PyTorch Geometric](https://pytorch-geometric.readthedocs.io/en/latest/)\n- \\[PyTorch/TensorFlow\\] [Deep Graph Library](https://www.dgl.ai/)\n- \\[TensorFlow\\] [Spektral](https://graphneural.network/)\n- \\[JAX\\] [jraph](https://github.com/deepmind/jraph)\n\nWe recommend using either one of those (depending on your favoured framework), as their implementations have been more readily battle-tested.\n\nEarly on post-release, two unofficial ports of the GAT model to various frameworks quickly surfaced. To honour the effort of their developers as early adopters of the GAT layer, we leave pointers to them here.\n- \\[Keras\\] [keras-gat](https://github.com/danielegrattarola/keras-gat), developed by [Daniele Grattarola](https://github.com/danielegrattarola);\n- \\[PyTorch\\] [pyGAT](https://github.com/Diego999/pyGAT), developed by [Diego Antognini](https://github.com/Diego999).\n\n## License\nMIT\n"
        },
        {
          "name": "data",
          "type": "tree",
          "content": null
        },
        {
          "name": "execute_cora.py",
          "type": "blob",
          "size": 6.537109375,
          "content": "import time\nimport numpy as np\nimport tensorflow as tf\n\nfrom models import GAT\nfrom utils import process\n\ncheckpt_file = 'pre_trained/cora/mod_cora.ckpt'\n\ndataset = 'cora'\n\n# training params\nbatch_size = 1\nnb_epochs = 100000\npatience = 100\nlr = 0.005  # learning rate\nl2_coef = 0.0005  # weight decay\nhid_units = [8] # numbers of hidden units per each attention head in each layer\nn_heads = [8, 1] # additional entry for the output layer\nresidual = False\nnonlinearity = tf.nn.elu\nmodel = GAT\n\nprint('Dataset: ' + dataset)\nprint('----- Opt. hyperparams -----')\nprint('lr: ' + str(lr))\nprint('l2_coef: ' + str(l2_coef))\nprint('----- Archi. hyperparams -----')\nprint('nb. layers: ' + str(len(hid_units)))\nprint('nb. units per layer: ' + str(hid_units))\nprint('nb. attention heads: ' + str(n_heads))\nprint('residual: ' + str(residual))\nprint('nonlinearity: ' + str(nonlinearity))\nprint('model: ' + str(model))\n\nadj, features, y_train, y_val, y_test, train_mask, val_mask, test_mask = process.load_data(dataset)\nfeatures, spars = process.preprocess_features(features)\n\nnb_nodes = features.shape[0]\nft_size = features.shape[1]\nnb_classes = y_train.shape[1]\n\nadj = adj.todense()\n\nfeatures = features[np.newaxis]\nadj = adj[np.newaxis]\ny_train = y_train[np.newaxis]\ny_val = y_val[np.newaxis]\ny_test = y_test[np.newaxis]\ntrain_mask = train_mask[np.newaxis]\nval_mask = val_mask[np.newaxis]\ntest_mask = test_mask[np.newaxis]\n\nbiases = process.adj_to_bias(adj, [nb_nodes], nhood=1)\n\nwith tf.Graph().as_default():\n    with tf.name_scope('input'):\n        ftr_in = tf.placeholder(dtype=tf.float32, shape=(batch_size, nb_nodes, ft_size))\n        bias_in = tf.placeholder(dtype=tf.float32, shape=(batch_size, nb_nodes, nb_nodes))\n        lbl_in = tf.placeholder(dtype=tf.int32, shape=(batch_size, nb_nodes, nb_classes))\n        msk_in = tf.placeholder(dtype=tf.int32, shape=(batch_size, nb_nodes))\n        attn_drop = tf.placeholder(dtype=tf.float32, shape=())\n        ffd_drop = tf.placeholder(dtype=tf.float32, shape=())\n        is_train = tf.placeholder(dtype=tf.bool, shape=())\n\n    logits = model.inference(ftr_in, nb_classes, nb_nodes, is_train,\n                                attn_drop, ffd_drop,\n                                bias_mat=bias_in,\n                                hid_units=hid_units, n_heads=n_heads,\n                                residual=residual, activation=nonlinearity)\n    log_resh = tf.reshape(logits, [-1, nb_classes])\n    lab_resh = tf.reshape(lbl_in, [-1, nb_classes])\n    msk_resh = tf.reshape(msk_in, [-1])\n    loss = model.masked_softmax_cross_entropy(log_resh, lab_resh, msk_resh)\n    accuracy = model.masked_accuracy(log_resh, lab_resh, msk_resh)\n\n    train_op = model.training(loss, lr, l2_coef)\n\n    saver = tf.train.Saver()\n\n    init_op = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())\n\n    vlss_mn = np.inf\n    vacc_mx = 0.0\n    curr_step = 0\n\n    with tf.Session() as sess:\n        sess.run(init_op)\n\n        train_loss_avg = 0\n        train_acc_avg = 0\n        val_loss_avg = 0\n        val_acc_avg = 0\n\n        for epoch in range(nb_epochs):\n            tr_step = 0\n            tr_size = features.shape[0]\n\n            while tr_step * batch_size < tr_size:\n                _, loss_value_tr, acc_tr = sess.run([train_op, loss, accuracy],\n                    feed_dict={\n                        ftr_in: features[tr_step*batch_size:(tr_step+1)*batch_size],\n                        bias_in: biases[tr_step*batch_size:(tr_step+1)*batch_size],\n                        lbl_in: y_train[tr_step*batch_size:(tr_step+1)*batch_size],\n                        msk_in: train_mask[tr_step*batch_size:(tr_step+1)*batch_size],\n                        is_train: True,\n                        attn_drop: 0.6, ffd_drop: 0.6})\n                train_loss_avg += loss_value_tr\n                train_acc_avg += acc_tr\n                tr_step += 1\n\n            vl_step = 0\n            vl_size = features.shape[0]\n\n            while vl_step * batch_size < vl_size:\n                loss_value_vl, acc_vl = sess.run([loss, accuracy],\n                    feed_dict={\n                        ftr_in: features[vl_step*batch_size:(vl_step+1)*batch_size],\n                        bias_in: biases[vl_step*batch_size:(vl_step+1)*batch_size],\n                        lbl_in: y_val[vl_step*batch_size:(vl_step+1)*batch_size],\n                        msk_in: val_mask[vl_step*batch_size:(vl_step+1)*batch_size],\n                        is_train: False,\n                        attn_drop: 0.0, ffd_drop: 0.0})\n                val_loss_avg += loss_value_vl\n                val_acc_avg += acc_vl\n                vl_step += 1\n\n            print('Training: loss = %.5f, acc = %.5f | Val: loss = %.5f, acc = %.5f' %\n                    (train_loss_avg/tr_step, train_acc_avg/tr_step,\n                    val_loss_avg/vl_step, val_acc_avg/vl_step))\n\n            if val_acc_avg/vl_step >= vacc_mx or val_loss_avg/vl_step <= vlss_mn:\n                if val_acc_avg/vl_step >= vacc_mx and val_loss_avg/vl_step <= vlss_mn:\n                    vacc_early_model = val_acc_avg/vl_step\n                    vlss_early_model = val_loss_avg/vl_step\n                    saver.save(sess, checkpt_file)\n                vacc_mx = np.max((val_acc_avg/vl_step, vacc_mx))\n                vlss_mn = np.min((val_loss_avg/vl_step, vlss_mn))\n                curr_step = 0\n            else:\n                curr_step += 1\n                if curr_step == patience:\n                    print('Early stop! Min loss: ', vlss_mn, ', Max accuracy: ', vacc_mx)\n                    print('Early stop model validation loss: ', vlss_early_model, ', accuracy: ', vacc_early_model)\n                    break\n\n            train_loss_avg = 0\n            train_acc_avg = 0\n            val_loss_avg = 0\n            val_acc_avg = 0\n\n        saver.restore(sess, checkpt_file)\n\n        ts_size = features.shape[0]\n        ts_step = 0\n        ts_loss = 0.0\n        ts_acc = 0.0\n\n        while ts_step * batch_size < ts_size:\n            loss_value_ts, acc_ts = sess.run([loss, accuracy],\n                feed_dict={\n                    ftr_in: features[ts_step*batch_size:(ts_step+1)*batch_size],\n                    bias_in: biases[ts_step*batch_size:(ts_step+1)*batch_size],\n                    lbl_in: y_test[ts_step*batch_size:(ts_step+1)*batch_size],\n                    msk_in: test_mask[ts_step*batch_size:(ts_step+1)*batch_size],\n                    is_train: False,\n                    attn_drop: 0.0, ffd_drop: 0.0})\n            ts_loss += loss_value_ts\n            ts_acc += acc_ts\n            ts_step += 1\n\n        print('Test loss:', ts_loss/ts_step, '; Test accuracy:', ts_acc/ts_step)\n\n        sess.close()\n"
        },
        {
          "name": "execute_cora_sparse.py",
          "type": "blob",
          "size": 7.2802734375,
          "content": "import time\nimport scipy.sparse as sp\nimport numpy as np\nimport tensorflow as tf\nimport argparse\n\nfrom models import GAT\nfrom models import SpGAT\nfrom utils import process\n\ncheckpt_file = 'pre_trained/cora/mod_cora.ckpt'\n\ndataset = 'cora'\n\n# training params\nbatch_size = 1\nnb_epochs = 100000\npatience = 100\nlr = 0.005  # learning rate\nl2_coef = 0.0005  # weight decay\nhid_units = [8] # numbers of hidden units per each attention head in each layer\nn_heads = [8, 1] # additional entry for the output layer\nresidual = False\nnonlinearity = tf.nn.elu\n# model = GAT\nmodel = SpGAT\n\nprint('Dataset: ' + dataset)\nprint('----- Opt. hyperparams -----')\nprint('lr: ' + str(lr))\nprint('l2_coef: ' + str(l2_coef))\nprint('----- Archi. hyperparams -----')\nprint('nb. layers: ' + str(len(hid_units)))\nprint('nb. units per layer: ' + str(hid_units))\nprint('nb. attention heads: ' + str(n_heads))\nprint('residual: ' + str(residual))\nprint('nonlinearity: ' + str(nonlinearity))\nprint('model: ' + str(model))\n\nsparse = True\n\nadj, features, y_train, y_val, y_test, train_mask, val_mask, test_mask = process.load_data(dataset)\nfeatures, spars = process.preprocess_features(features)\n\nnb_nodes = features.shape[0]\nft_size = features.shape[1]\nnb_classes = y_train.shape[1]\n\nfeatures = features[np.newaxis]\ny_train = y_train[np.newaxis]\ny_val = y_val[np.newaxis]\ny_test = y_test[np.newaxis]\ntrain_mask = train_mask[np.newaxis]\nval_mask = val_mask[np.newaxis]\ntest_mask = test_mask[np.newaxis]\n\nif sparse:\n    biases = process.preprocess_adj_bias(adj)\nelse:\n    adj = adj.todense()\n    adj = adj[np.newaxis]\n    biases = process.adj_to_bias(adj, [nb_nodes], nhood=1)\n\nwith tf.Graph().as_default():\n    with tf.name_scope('input'):\n        ftr_in = tf.placeholder(dtype=tf.float32, shape=(batch_size, nb_nodes, ft_size))\n        if sparse:\n            #bias_idx = tf.placeholder(tf.int64)\n            #bias_val = tf.placeholder(tf.float32)\n            #bias_shape = tf.placeholder(tf.int64)\n            bias_in = tf.sparse_placeholder(dtype=tf.float32)\n        else:\n            bias_in = tf.placeholder(dtype=tf.float32, shape=(batch_size, nb_nodes, nb_nodes))\n        lbl_in = tf.placeholder(dtype=tf.int32, shape=(batch_size, nb_nodes, nb_classes))\n        msk_in = tf.placeholder(dtype=tf.int32, shape=(batch_size, nb_nodes))\n        attn_drop = tf.placeholder(dtype=tf.float32, shape=())\n        ffd_drop = tf.placeholder(dtype=tf.float32, shape=())\n        is_train = tf.placeholder(dtype=tf.bool, shape=())\n\n    logits = model.inference(ftr_in, nb_classes, nb_nodes, is_train,\n                                attn_drop, ffd_drop,\n                                bias_mat=bias_in,\n                                hid_units=hid_units, n_heads=n_heads,\n                                residual=residual, activation=nonlinearity)\n    log_resh = tf.reshape(logits, [-1, nb_classes])\n    lab_resh = tf.reshape(lbl_in, [-1, nb_classes])\n    msk_resh = tf.reshape(msk_in, [-1])\n    loss = model.masked_softmax_cross_entropy(log_resh, lab_resh, msk_resh)\n    accuracy = model.masked_accuracy(log_resh, lab_resh, msk_resh)\n\n    train_op = model.training(loss, lr, l2_coef)\n\n    saver = tf.train.Saver()\n\n    init_op = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())\n\n    vlss_mn = np.inf\n    vacc_mx = 0.0\n    curr_step = 0\n\n    with tf.Session() as sess:\n        sess.run(init_op)\n\n        train_loss_avg = 0\n        train_acc_avg = 0\n        val_loss_avg = 0\n        val_acc_avg = 0\n\n        for epoch in range(nb_epochs):\n            tr_step = 0\n            tr_size = features.shape[0]\n\n            while tr_step * batch_size < tr_size:\n                if sparse:\n                    bbias = biases\n                else:\n                    bbias = biases[tr_step*batch_size:(tr_step+1)*batch_size]\n\n                _, loss_value_tr, acc_tr = sess.run([train_op, loss, accuracy],\n                    feed_dict={\n                        ftr_in: features[tr_step*batch_size:(tr_step+1)*batch_size],\n                        bias_in: bbias,\n                        lbl_in: y_train[tr_step*batch_size:(tr_step+1)*batch_size],\n                        msk_in: train_mask[tr_step*batch_size:(tr_step+1)*batch_size],\n                        is_train: True,\n                        attn_drop: 0.6, ffd_drop: 0.6})\n                train_loss_avg += loss_value_tr\n                train_acc_avg += acc_tr\n                tr_step += 1\n\n            vl_step = 0\n            vl_size = features.shape[0]\n\n            while vl_step * batch_size < vl_size:\n                if sparse:\n                    bbias = biases\n                else:\n                    bbias = biases[vl_step*batch_size:(vl_step+1)*batch_size]\n                loss_value_vl, acc_vl = sess.run([loss, accuracy],\n                    feed_dict={\n                        ftr_in: features[vl_step*batch_size:(vl_step+1)*batch_size],\n                        bias_in: bbias,\n                        lbl_in: y_val[vl_step*batch_size:(vl_step+1)*batch_size],\n                        msk_in: val_mask[vl_step*batch_size:(vl_step+1)*batch_size],\n                        is_train: False,\n                        attn_drop: 0.0, ffd_drop: 0.0})\n                val_loss_avg += loss_value_vl\n                val_acc_avg += acc_vl\n                vl_step += 1\n\n            print('Training: loss = %.5f, acc = %.5f | Val: loss = %.5f, acc = %.5f' %\n                    (train_loss_avg/tr_step, train_acc_avg/tr_step,\n                    val_loss_avg/vl_step, val_acc_avg/vl_step))\n\n            if val_acc_avg/vl_step >= vacc_mx or val_loss_avg/vl_step <= vlss_mn:\n                if val_acc_avg/vl_step >= vacc_mx and val_loss_avg/vl_step <= vlss_mn:\n                    vacc_early_model = val_acc_avg/vl_step\n                    vlss_early_model = val_loss_avg/vl_step\n                    saver.save(sess, checkpt_file)\n                vacc_mx = np.max((val_acc_avg/vl_step, vacc_mx))\n                vlss_mn = np.min((val_loss_avg/vl_step, vlss_mn))\n                curr_step = 0\n            else:\n                curr_step += 1\n                if curr_step == patience:\n                    print('Early stop! Min loss: ', vlss_mn, ', Max accuracy: ', vacc_mx)\n                    print('Early stop model validation loss: ', vlss_early_model, ', accuracy: ', vacc_early_model)\n                    break\n\n            train_loss_avg = 0\n            train_acc_avg = 0\n            val_loss_avg = 0\n            val_acc_avg = 0\n\n        saver.restore(sess, checkpt_file)\n\n        ts_size = features.shape[0]\n        ts_step = 0\n        ts_loss = 0.0\n        ts_acc = 0.0\n\n        while ts_step * batch_size < ts_size:\n            if sparse:\n                bbias = biases\n            else:\n                bbias = biases[ts_step*batch_size:(ts_step+1)*batch_size]\n            loss_value_ts, acc_ts = sess.run([loss, accuracy],\n                feed_dict={\n                    ftr_in: features[ts_step*batch_size:(ts_step+1)*batch_size],\n                    bias_in: bbias,\n                    lbl_in: y_test[ts_step*batch_size:(ts_step+1)*batch_size],\n                    msk_in: test_mask[ts_step*batch_size:(ts_step+1)*batch_size],\n                    is_train: False,\n                    attn_drop: 0.0, ffd_drop: 0.0})\n            ts_loss += loss_value_ts\n            ts_acc += acc_ts\n            ts_step += 1\n\n        print('Test loss:', ts_loss/ts_step, '; Test accuracy:', ts_acc/ts_step)\n\n        sess.close()\n"
        },
        {
          "name": "models",
          "type": "tree",
          "content": null
        },
        {
          "name": "pre_trained",
          "type": "tree",
          "content": null
        },
        {
          "name": "utils",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}