{
  "metadata": {
    "timestamp": 1736559558828,
    "page": 169,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjE3MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "shibing624/text2vec",
      "stars": 4579,
      "defaultBranch": "master",
      "files": [
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 1.2001953125,
          "content": "# Byte-compiled / optimized / DLL files\n__pycache__/\n*.py[cod]\n*$py.class\n\n# C extensions\n*.so\n\n# Distribution / packaging\n.Python\nbuild/\ndevelop-eggs/\ndist/\ndownloads/\neggs/\n.eggs/\nlib/\nlib64/\nparts/\nsdist/\nvar/\nwheels/\n*.egg-info/\n.installed.cfg\n*.egg\nMANIFEST\n\n# PyInstaller\n#  Usually these files are written by a python script from a template\n#  before PyInstaller builds the exe, so as to inject date/other infos into it.\n*.manifest\n*.spec\n\n# Installer logs\npip-log.txt\npip-delete-this-directory.txt\n\n# Unit test / coverage reports\nhtmlcov/\n.tox/\n.coverage\n.coverage.*\n.cache\nnosetests.xml\ncoverage.xml\n*.cover\n.hypothesis/\n.pytest_cache/\n\n# Translations\n*.mo\n*.pot\n\n# Django stuff:\n*.log\nlocal_settings.py\ndb.sqlite3\n\n# Flask stuff:\ninstance/\n.webassets-cache\n\n# Scrapy stuff:\n.scrapy\n\n# Sphinx documentation\ndocs/_build/\n\n# PyBuilder\ntarget/\n\n# Jupyter Notebook\n.ipynb_checkpoints\n\n# pyenv\n.python-version\n\n# celery beat schedule file\ncelerybeat-schedule\n\n# SageMath parsed files\n*.sage.py\n\n# Environments\n.env\n.venv\nenv/\nvenv/\nENV/\nenv.bak/\nvenv.bak/\n\n# Spyder project settings\n.spyderproject\n.spyproject\n\n# Rope project settings\n.ropeproject\n\n# mkdocs documentation\n/site\n\n# mypy\n.mypy_cache/\n.idea\n*.DS_Store\n.DS_Store"
        },
        {
          "name": "CITATION.cff",
          "type": "blob",
          "size": 0.3037109375,
          "content": "cff-version: 1.2.0\nmessage: \"If you use this software, please cite it as below.\"\nauthors:\n  - family-names: Xu\n    given-names: Ming\n    orcid: https://orcid.org/0000-0003-3402-7159\ntitle: \"Text2vec: Text to vector toolkit\"\nversion: 1.1.3\ndate-released: 2022-02-27\nurl: \"https://github.com/shibing624/text2vec\"\n"
        },
        {
          "name": "CONTRIBUTING.md",
          "type": "blob",
          "size": 0.5029296875,
          "content": "# Contributing\n\nWe are happy to accept your contributions to make `text2vec` better and more awesome! To avoid unnecessary work on either\nside, please stick to the following process:\n\n1. Check if there is already [an issue](https://github.com/shibing624/text2vec/issues) for your concern.\n2. If there is not, open a new one to start a discussion. We hate to close finished PRs!\n3. If we decide your concern needs code changes, we would be happy to accept a pull request. Please consider the\ncommit guidelines below."
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 11.0908203125,
          "content": "                                 Apache License\n                           Version 2.0, January 2004\n                        http://www.apache.org/licenses/\n\n   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n\n   1. Definitions.\n\n      \"License\" shall mean the terms and conditions for use, reproduction,\n      and distribution as defined by Sections 1 through 9 of this document.\n\n      \"Licensor\" shall mean the copyright owner or entity authorized by\n      the copyright owner that is granting the License.\n\n      \"Legal Entity\" shall mean the union of the acting entity and all\n      other entities that control, are controlled by, or are under common\n      control with that entity. For the purposes of this definition,\n      \"control\" means (i) the power, direct or indirect, to cause the\n      direction or management of such entity, whether by contract or\n      otherwise, or (ii) ownership of fifty percent (50%) or more of the\n      outstanding shares, or (iii) beneficial ownership of such entity.\n\n      \"You\" (or \"Your\") shall mean an individual or Legal Entity\n      exercising permissions granted by this License.\n\n      \"Source\" form shall mean the preferred form for making modifications,\n      including but not limited to software source code, documentation\n      source, and configuration files.\n\n      \"Object\" form shall mean any form resulting from mechanical\n      transformation or translation of a Source form, including but\n      not limited to compiled object code, generated documentation,\n      and conversions to other media types.\n\n      \"Work\" shall mean the work of authorship, whether in Source or\n      Object form, made available under the License, as indicated by a\n      copyright notice that is included in or attached to the work\n      (an example is provided in the Appendix below).\n\n      \"Derivative Works\" shall mean any work, whether in Source or Object\n      form, that is based on (or derived from) the Work and for which the\n      editorial revisions, annotations, elaborations, or other modifications\n      represent, as a whole, an original work of authorship. For the purposes\n      of this License, Derivative Works shall not include works that remain\n      separable from, or merely link (or bind by name) to the interfaces of,\n      the Work and Derivative Works thereof.\n\n      \"Contribution\" shall mean any work of authorship, including\n      the original version of the Work and any modifications or additions\n      to that Work or Derivative Works thereof, that is intentionally\n      submitted to Licensor for inclusion in the Work by the copyright owner\n      or by an individual or Legal Entity authorized to submit on behalf of\n      the copyright owner. For the purposes of this definition, \"submitted\"\n      means any form of electronic, verbal, or written communication sent\n      to the Licensor or its representatives, including but not limited to\n      communication on electronic mailing lists, source code control systems,\n      and issue tracking systems that are managed by, or on behalf of, the\n      Licensor for the purpose of discussing and improving the Work, but\n      excluding communication that is conspicuously marked or otherwise\n      designated in writing by the copyright owner as \"Not a Contribution.\"\n\n      \"Contributor\" shall mean Licensor and any individual or Legal Entity\n      on behalf of whom a Contribution has been received by Licensor and\n      subsequently incorporated within the Work.\n\n   2. Grant of Copyright License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      copyright license to reproduce, prepare Derivative Works of,\n      publicly display, publicly perform, sublicense, and distribute the\n      Work and such Derivative Works in Source or Object form.\n\n   3. Grant of Patent License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      (except as stated in this section) patent license to make, have made,\n      use, offer to sell, sell, import, and otherwise transfer the Work,\n      where such license applies only to those patent claims licensable\n      by such Contributor that are necessarily infringed by their\n      Contribution(s) alone or by combination of their Contribution(s)\n      with the Work to which such Contribution(s) was submitted. If You\n      institute patent litigation against any entity (including a\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\n      or a Contribution incorporated within the Work constitutes direct\n      or contributory patent infringement, then any patent licenses\n      granted to You under this License for that Work shall terminate\n      as of the date such litigation is filed.\n\n   4. Redistribution. You may reproduce and distribute copies of the\n      Work or Derivative Works thereof in any medium, with or without\n      modifications, and in Source or Object form, provided that You\n      meet the following conditions:\n\n      (a) You must give any other recipients of the Work or\n          Derivative Works a copy of this License; and\n\n      (b) You must cause any modified files to carry prominent notices\n          stating that You changed the files; and\n\n      (c) You must retain, in the Source form of any Derivative Works\n          that You distribute, all copyright, patent, trademark, and\n          attribution notices from the Source form of the Work,\n          excluding those notices that do not pertain to any part of\n          the Derivative Works; and\n\n      (d) If the Work includes a \"NOTICE\" text file as part of its\n          distribution, then any Derivative Works that You distribute must\n          include a readable copy of the attribution notices contained\n          within such NOTICE file, excluding those notices that do not\n          pertain to any part of the Derivative Works, in at least one\n          of the following places: within a NOTICE text file distributed\n          as part of the Derivative Works; within the Source form or\n          documentation, if provided along with the Derivative Works; or,\n          within a display generated by the Derivative Works, if and\n          wherever such third-party notices normally appear. The contents\n          of the NOTICE file are for informational purposes only and\n          do not modify the License. You may add Your own attribution\n          notices within Derivative Works that You distribute, alongside\n          or as an addendum to the NOTICE text from the Work, provided\n          that such additional attribution notices cannot be construed\n          as modifying the License.\n\n      You may add Your own copyright statement to Your modifications and\n      may provide additional or different license terms and conditions\n      for use, reproduction, or distribution of Your modifications, or\n      for any such Derivative Works as a whole, provided Your use,\n      reproduction, and distribution of the Work otherwise complies with\n      the conditions stated in this License.\n\n   5. Submission of Contributions. Unless You explicitly state otherwise,\n      any Contribution intentionally submitted for inclusion in the Work\n      by You to the Licensor shall be under the terms and conditions of\n      this License, without any additional terms or conditions.\n      Notwithstanding the above, nothing herein shall supersede or modify\n      the terms of any separate license agreement you may have executed\n      with Licensor regarding such Contributions.\n\n   6. Trademarks. This License does not grant permission to use the trade\n      names, trademarks, service marks, or product names of the Licensor,\n      except as required for reasonable and customary use in describing the\n      origin of the Work and reproducing the content of the NOTICE file.\n\n   7. Disclaimer of Warranty. Unless required by applicable law or\n      agreed to in writing, Licensor provides the Work (and each\n      Contributor provides its Contributions) on an \"AS IS\" BASIS,\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n      implied, including, without limitation, any warranties or conditions\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n      PARTICULAR PURPOSE. You are solely responsible for determining the\n      appropriateness of using or redistributing the Work and assume any\n      risks associated with Your exercise of permissions under this License.\n\n   8. Limitation of Liability. In no event and under no legal theory,\n      whether in tort (including negligence), contract, or otherwise,\n      unless required by applicable law (such as deliberate and grossly\n      negligent acts) or agreed to in writing, shall any Contributor be\n      liable to You for damages, including any direct, indirect, special,\n      incidental, or consequential damages of any character arising as a\n      result of this License or out of the use or inability to use the\n      Work (including but not limited to damages for loss of goodwill,\n      work stoppage, computer failure or malfunction, or any and all\n      other commercial damages or losses), even if such Contributor\n      has been advised of the possibility of such damages.\n\n   9. Accepting Warranty or Additional Liability. While redistributing\n      the Work or Derivative Works thereof, You may choose to offer,\n      and charge a fee for, acceptance of support, warranty, indemnity,\n      or other liability obligations and/or rights consistent with this\n      License. However, in accepting such obligations, You may act only\n      on Your own behalf and on Your sole responsibility, not on behalf\n      of any other Contributor, and only if You agree to indemnify,\n      defend, and hold each Contributor harmless for any liability\n      incurred by, or claims asserted against, such Contributor by reason\n      of your accepting any such warranty or additional liability.\n\n   END OF TERMS AND CONDITIONS\n\n   APPENDIX: How to apply the Apache License to your work.\n\n      To apply the Apache License to your work, attach the following\n      boilerplate notice, with the fields enclosed by brackets \"[]\"\n      replaced with your own identifying information. (Don't include\n      the brackets!)  The text should be enclosed in the appropriate\n      comment syntax for the file format. We also recommend that a\n      file or class name and description of purpose be included on the\n      same \"printed page\" as the copyright notice for easier\n      identification within third-party archives.\n\n   Copyright [yyyy] [name of copyright owner]\n\n   Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.\n"
        },
        {
          "name": "MANIFEST.in",
          "type": "blob",
          "size": 0.0146484375,
          "content": "exclude tests/*"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 49.1591796875,
          "content": "[**ğŸ‡¨ğŸ‡³ä¸­æ–‡**](https://github.com/shibing624/text2vec/blob/master/README.md) | [**ğŸŒEnglish**](https://github.com/shibing624/text2vec/blob/master/README_EN.md) | [**ğŸ“–æ–‡æ¡£/Docs**](https://github.com/shibing624/text2vec/wiki) | [**ğŸ¤–æ¨¡å‹/Models**](https://huggingface.co/shibing624) \n\n<div align=\"center\">\n  <a href=\"https://github.com/shibing624/text2vec\">\n    <img src=\"https://github.com/shibing624/text2vec/blob/master/docs/t2v-logo.png\" height=\"150\" alt=\"Logo\">\n  </a>\n</div>\n\n-----------------\n\n# Text2vec: Text to Vector\n[![PyPI version](https://badge.fury.io/py/text2vec.svg)](https://badge.fury.io/py/text2vec)\n[![Downloads](https://static.pepy.tech/badge/text2vec)](https://pepy.tech/project/text2vec)\n[![Contributions welcome](https://img.shields.io/badge/contributions-welcome-brightgreen.svg)](CONTRIBUTING.md)\n[![License Apache 2.0](https://img.shields.io/badge/license-Apache%202.0-blue.svg)](LICENSE)\n[![python_version](https://img.shields.io/badge/Python-3.5%2B-green.svg)](requirements.txt)\n[![GitHub issues](https://img.shields.io/github/issues/shibing624/text2vec.svg)](https://github.com/shibing624/text2vec/issues)\n[![Wechat Group](https://img.shields.io/badge/wechat-group-green.svg?logo=wechat)](#Contact)\n\n\n**Text2vec**: Text to Vector, Get Sentence Embeddings. æ–‡æœ¬å‘é‡åŒ–ï¼ŒæŠŠæ–‡æœ¬(åŒ…æ‹¬è¯ã€å¥å­ã€æ®µè½)è¡¨å¾ä¸ºå‘é‡çŸ©é˜µã€‚\n\n**text2vec**å®ç°äº†Word2Vecã€RankBM25ã€BERTã€Sentence-BERTã€CoSENTç­‰å¤šç§æ–‡æœ¬è¡¨å¾ã€æ–‡æœ¬ç›¸ä¼¼åº¦è®¡ç®—æ¨¡å‹ï¼Œå¹¶åœ¨æ–‡æœ¬è¯­ä¹‰åŒ¹é…ï¼ˆç›¸ä¼¼åº¦è®¡ç®—ï¼‰ä»»åŠ¡ä¸Šæ¯”è¾ƒäº†å„æ¨¡å‹çš„æ•ˆæœã€‚\n\n### News\n[2023/09/20] v1.2.9ç‰ˆæœ¬: æ”¯æŒå¤šå¡æ¨ç†ï¼ˆå¤šè¿›ç¨‹å®ç°å¤šGPUã€å¤šCPUæ¨ç†ï¼‰ï¼Œæ–°å¢å‘½ä»¤è¡Œå·¥å…·ï¼ˆCLIï¼‰ï¼Œå¯ä»¥è„šæœ¬æ‰§è¡Œæ‰¹é‡æ–‡æœ¬å‘é‡åŒ–ï¼Œè¯¦è§[Release-v1.2.9](https://github.com/shibing624/text2vec/releases/tag/1.2.9)\n\n[2023/09/03] v1.2.4ç‰ˆæœ¬: æ”¯æŒFlagEmbeddingæ¨¡å‹è®­ç»ƒï¼Œå‘å¸ƒäº†ä¸­æ–‡åŒ¹é…æ¨¡å‹[shibing624/text2vec-bge-large-chinese](https://huggingface.co/shibing624/text2vec-bge-large-chinese)ï¼Œç”¨CoSENTæ–¹æ³•ç›‘ç£è®­ç»ƒï¼ŒåŸºäº`BAAI/bge-large-zh-noinstruct`ç”¨ä¸­æ–‡åŒ¹é…æ•°æ®é›†è®­ç»ƒå¾—åˆ°ï¼Œå¹¶åœ¨ä¸­æ–‡æµ‹è¯•é›†è¯„ä¼°ç›¸å¯¹äºåŸæ¨¡å‹æ•ˆæœæœ‰æå‡ï¼ŒçŸ­æ–‡æœ¬åŒºåˆ†åº¦ä¸Šæå‡æ˜æ˜¾ï¼Œè¯¦è§[Release-v1.2.4](https://github.com/shibing624/text2vec/releases/tag/1.2.4)\n\n[2023/07/17] v1.2.2ç‰ˆæœ¬: æ”¯æŒå¤šå¡è®­ç»ƒï¼Œå‘å¸ƒäº†å¤šè¯­è¨€åŒ¹é…æ¨¡å‹[shibing624/text2vec-base-multilingual](https://huggingface.co/shibing624/text2vec-base-multilingual)ï¼Œç”¨CoSENTæ–¹æ³•è®­ç»ƒï¼ŒåŸºäº`sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2`ç”¨äººå·¥æŒ‘é€‰åçš„å¤šè¯­è¨€STSæ•°æ®é›†[shibing624/nli-zh-all/text2vec-base-multilingual-dataset](https://huggingface.co/datasets/shibing624/nli-zh-all/tree/main/text2vec-base-multilingual-dataset)è®­ç»ƒå¾—åˆ°ï¼Œå¹¶åœ¨ä¸­è‹±æ–‡æµ‹è¯•é›†è¯„ä¼°ç›¸å¯¹äºåŸæ¨¡å‹æ•ˆæœæœ‰æå‡ï¼Œè¯¦è§[Release-v1.2.2](https://github.com/shibing624/text2vec/releases/tag/1.2.2)\n\n[2023/06/19] v1.2.1ç‰ˆæœ¬: æ›´æ–°äº†ä¸­æ–‡åŒ¹é…æ¨¡å‹`shibing624/text2vec-base-chinese-nli`ä¸ºæ–°ç‰ˆ[shibing624/text2vec-base-chinese-sentence](https://huggingface.co/shibing624/text2vec-base-chinese-sentence)ï¼Œé’ˆå¯¹CoSENTçš„lossè®¡ç®—å¯¹æ’åºæ•æ„Ÿç‰¹ç‚¹ï¼Œäººå·¥æŒ‘é€‰å¹¶æ•´ç†å‡ºé«˜è´¨é‡çš„æœ‰ç›¸å…³æ€§æ’åºçš„STSæ•°æ®é›†[shibing624/nli-zh-all/text2vec-base-chinese-sentence-dataset](https://huggingface.co/datasets/shibing624/nli-zh-all/tree/main/text2vec-base-chinese-sentence-dataset)ï¼Œåœ¨å„è¯„ä¼°é›†è¡¨ç°ç›¸å¯¹ä¹‹å‰æœ‰æå‡ï¼›å‘å¸ƒäº†é€‚ç”¨äºs2pçš„ä¸­æ–‡åŒ¹é…æ¨¡å‹[shibing624/text2vec-base-chinese-paraphrase](https://huggingface.co/shibing624/text2vec-base-chinese-paraphrase)ï¼Œè¯¦è§[Release-v1.2.1](https://github.com/shibing624/text2vec/releases/tag/1.2.1)\n\n[2023/06/15] v1.2.0ç‰ˆæœ¬: å‘å¸ƒäº†ä¸­æ–‡åŒ¹é…æ¨¡å‹[shibing624/text2vec-base-chinese-nli](https://huggingface.co/shibing624/text2vec-base-chinese-nli)ï¼ŒåŸºäº`nghuyong/ernie-3.0-base-zh`æ¨¡å‹ï¼Œä½¿ç”¨äº†ä¸­æ–‡NLIæ•°æ®é›†[shibing624/nli_zh](https://huggingface.co/datasets/shibing624/nli_zh)å…¨éƒ¨è¯­æ–™è®­ç»ƒçš„CoSENTæ–‡æœ¬åŒ¹é…æ¨¡å‹ï¼Œåœ¨å„è¯„ä¼°é›†è¡¨ç°æå‡æ˜æ˜¾ï¼Œè¯¦è§[Release-v1.2.0](https://github.com/shibing624/text2vec/releases/tag/1.2.0)\n\n[2022/03/12] v1.1.4ç‰ˆæœ¬: å‘å¸ƒäº†ä¸­æ–‡åŒ¹é…æ¨¡å‹[shibing624/text2vec-base-chinese](https://huggingface.co/shibing624/text2vec-base-chinese)ï¼ŒåŸºäºä¸­æ–‡STSè®­ç»ƒé›†è®­ç»ƒçš„CoSENTåŒ¹é…æ¨¡å‹ã€‚è¯¦è§[Release-v1.1.4](https://github.com/shibing624/text2vec/releases/tag/1.1.4)\n\n\n**Guide**\n- [Features](#Features)\n- [Evaluation](#Evaluation)\n- [Install](#install)\n- [Usage](#usage)\n- [Contact](#Contact)\n- [References](#references)\n\n\n## Features\n### æ–‡æœ¬å‘é‡è¡¨ç¤ºæ¨¡å‹\n- [Word2Vec](https://github.com/shibing624/text2vec/blob/master/text2vec/word2vec.py)ï¼šé€šè¿‡è…¾è®¯AI Labå¼€æºçš„å¤§è§„æ¨¡é«˜è´¨é‡ä¸­æ–‡[è¯å‘é‡æ•°æ®ï¼ˆ800ä¸‡ä¸­æ–‡è¯è½»é‡ç‰ˆï¼‰](https://pan.baidu.com/s/1La4U4XNFe8s5BJqxPQpeiQ) (æ–‡ä»¶åï¼šlight_Tencent_AILab_ChineseEmbedding.bin å¯†ç : taweï¼‰å®ç°è¯å‘é‡æ£€ç´¢ï¼Œæœ¬é¡¹ç›®å®ç°äº†å¥å­ï¼ˆè¯å‘é‡æ±‚å¹³å‡ï¼‰çš„word2vecå‘é‡è¡¨ç¤º\n- [SBERT(Sentence-BERT)](https://github.com/shibing624/text2vec/blob/master/text2vec/sentencebert_model.py)ï¼šæƒè¡¡æ€§èƒ½å’Œæ•ˆç‡çš„å¥å‘é‡è¡¨ç¤ºæ¨¡å‹ï¼Œè®­ç»ƒæ—¶é€šè¿‡æœ‰ç›‘ç£è®­ç»ƒBERTå’Œsoftmaxåˆ†ç±»å‡½æ•°ï¼Œæ–‡æœ¬åŒ¹é…é¢„æµ‹æ—¶ç›´æ¥å–å¥å­å‘é‡åšä½™å¼¦ï¼Œå¥å­è¡¨å¾æ–¹æ³•ï¼Œæœ¬é¡¹ç›®åŸºäºPyTorchå¤ç°äº†Sentence-BERTæ¨¡å‹çš„è®­ç»ƒå’Œé¢„æµ‹\n- [CoSENT(Cosine Sentence)](https://github.com/shibing624/text2vec/blob/master/text2vec/cosent_model.py)ï¼šCoSENTæ¨¡å‹æå‡ºäº†ä¸€ç§æ’åºçš„æŸå¤±å‡½æ•°ï¼Œä½¿è®­ç»ƒè¿‡ç¨‹æ›´è´´è¿‘é¢„æµ‹ï¼Œæ¨¡å‹æ”¶æ•›é€Ÿåº¦å’Œæ•ˆæœæ¯”Sentence-BERTæ›´å¥½ï¼Œæœ¬é¡¹ç›®åŸºäºPyTorchå®ç°äº†CoSENTæ¨¡å‹çš„è®­ç»ƒå’Œé¢„æµ‹\n- [BGE(BAAI general embedding)](https://github.com/shibing624/text2vec/blob/master/text2vec/bge_model.py)ï¼šBGEæ¨¡å‹æŒ‰ç…§[retromae](https://github.com/staoxiao/RetroMAE)æ–¹æ³•è¿›è¡Œé¢„è®­ç»ƒï¼Œ[å‚è€ƒè®ºæ–‡](https://aclanthology.org/2022.emnlp-main.35.pdf)ï¼Œå†ä½¿ç”¨å¯¹æ¯”å­¦ä¹ finetuneå¾®è°ƒè®­ç»ƒæ¨¡å‹ï¼Œæœ¬é¡¹ç›®åŸºäºPyTorchå®ç°äº†BGEæ¨¡å‹çš„å¾®è°ƒè®­ç»ƒå’Œé¢„æµ‹\n\n\nè¯¦ç»†æ–‡æœ¬å‘é‡è¡¨ç¤ºæ–¹æ³•è§wiki: [æ–‡æœ¬å‘é‡è¡¨ç¤ºæ–¹æ³•](https://github.com/shibing624/text2vec/wiki/%E6%96%87%E6%9C%AC%E5%90%91%E9%87%8F%E8%A1%A8%E7%A4%BA%E6%96%B9%E6%B3%95)\n## Evaluation\n\næ–‡æœ¬åŒ¹é…\n\n#### è‹±æ–‡åŒ¹é…æ•°æ®é›†çš„è¯„æµ‹ç»“æœï¼š\n\n\n| Arch   | BaseModel                                        | Model                                                                                                                | English-STS-B | \n|:-------|:------------------------------------------------|:---------------------------------------------------------------------------------------------------------------------|:-------------:|\n| GloVe  | glove                                           | Avg_word_embeddings_glove_6B_300d                                                                                    |     61.77     |\n| BERT   | bert-base-uncased                               | BERT-base-cls                                                                                                        |     20.29     |\n| BERT   | bert-base-uncased                               | BERT-base-first_last_avg                                                                                             |     59.04     |\n| BERT   | bert-base-uncased                               | BERT-base-first_last_avg-whiten(NLI)                                                                                 |     63.65     |\n| SBERT  | sentence-transformers/bert-base-nli-mean-tokens | SBERT-base-nli-cls                                                                                                   |     73.65     |\n| SBERT  | sentence-transformers/bert-base-nli-mean-tokens | SBERT-base-nli-first_last_avg                                                                                        |     77.96     |\n| CoSENT | bert-base-uncased                               | CoSENT-base-first_last_avg                                                                                           |     69.93     |\n| CoSENT | sentence-transformers/bert-base-nli-mean-tokens | CoSENT-base-nli-first_last_avg                                                                                       |     79.68     |\n| CoSENT | sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2 | [shibing624/text2vec-base-multilingual](https://huggingface.co/shibing624/text2vec-base-multilingual)                |     80.12     |\n\n#### ä¸­æ–‡åŒ¹é…æ•°æ®é›†çš„è¯„æµ‹ç»“æœï¼š\n\n\n| Arch   | BaseModel                    | Model           | ATEC  |  BQ   | LCQMC | PAWSX | STS-B |  Avg  | \n|:-------|:----------------------------|:--------------------|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|\n| SBERT  | bert-base-chinese           | SBERT-bert-base     | 46.36 | 70.36 | 78.72 | 46.86 | 66.41 | 61.74 |\n| SBERT  | hfl/chinese-macbert-base    | SBERT-macbert-base  | 47.28 | 68.63 | 79.42 | 55.59 | 64.82 | 63.15 |\n| SBERT  | hfl/chinese-roberta-wwm-ext | SBERT-roberta-ext   | 48.29 | 69.99 | 79.22 | 44.10 | 72.42 | 62.80 |\n| CoSENT | bert-base-chinese           | CoSENT-bert-base    | 49.74 | 72.38 | 78.69 | 60.00 | 79.27 | 68.01 |\n| CoSENT | hfl/chinese-macbert-base    | CoSENT-macbert-base | 50.39 | 72.93 | 79.17 | 60.86 | 79.30 | 68.53 |\n| CoSENT | hfl/chinese-roberta-wwm-ext | CoSENT-roberta-ext  | 50.81 | 71.45 | 79.31 | 61.56 | 79.96 | 68.61 |\n\nè¯´æ˜ï¼š\n- ç»“æœè¯„æµ‹æŒ‡æ ‡ï¼šspearmanç³»æ•°\n- ä¸ºè¯„æµ‹æ¨¡å‹èƒ½åŠ›ï¼Œç»“æœå‡åªç”¨è¯¥æ•°æ®é›†çš„trainè®­ç»ƒï¼Œåœ¨testä¸Šè¯„ä¼°å¾—åˆ°çš„è¡¨ç°ï¼Œæ²¡ç”¨å¤–éƒ¨æ•°æ®\n- `SBERT-macbert-base`æ¨¡å‹ï¼Œæ˜¯ç”¨SBertæ–¹æ³•è®­ç»ƒï¼Œè¿è¡Œ[examples/training_sup_text_matching_model.py](https://github.com/shibing624/text2vec/blob/master/examples/training_sup_text_matching_model.py)ä»£ç å¯è®­ç»ƒæ¨¡å‹\n- `sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2`æ¨¡å‹æ˜¯ç”¨SBertè®­ç»ƒï¼Œæ˜¯`paraphrase-MiniLM-L12-v2`æ¨¡å‹çš„å¤šè¯­è¨€ç‰ˆæœ¬ï¼Œæ”¯æŒä¸­æ–‡ã€è‹±æ–‡ç­‰\n\n\n### Release Models\n- æœ¬é¡¹ç›®releaseæ¨¡å‹çš„ä¸­æ–‡åŒ¹é…è¯„æµ‹ç»“æœï¼š\n\n| Arch       | BaseModel                                                   | Model                                                                                                                                             | ATEC  |  BQ   | LCQMC | PAWSX | STS-B | SOHU-dd | SOHU-dc |    Avg    |  QPS  |\n|:-----------|:------------------------------------------------------------|:--------------------------------------------------------------------------------------------------------------------------------------------------|:-----:|:-----:|:-----:|:-----:|:-----:|:-------:|:-------:|:---------:|:-----:|\n| Word2Vec   | word2vec                                                    | [w2v-light-tencent-chinese](https://modelscope.cn/models/lili666/text2vec-word2vec-tencent-chinese)                                               | 20.00 | 31.49 | 59.46 | 2.57  | 55.78 |  55.04  |  20.70  |   35.03   | 23769 |\n| SBERT      | xlm-roberta-base                                            | [sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2](https://huggingface.co/sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2) | 18.42 | 38.52 | 63.96 | 10.14 | 78.90 |  63.01  |  52.28  |   46.46   | 3138  |\n| CoSENT     | hfl/chinese-macbert-base                                    | [shibing624/text2vec-base-chinese](https://huggingface.co/shibing624/text2vec-base-chinese)                                                       | 31.93 | 42.67 | 70.16 | 17.21 | 79.30 |  70.27  |  50.42  |   51.61   | 3008  |\n| CoSENT     | hfl/chinese-lert-large                                      | [GanymedeNil/text2vec-large-chinese](https://huggingface.co/GanymedeNil/text2vec-large-chinese)                                                   | 32.61 | 44.59 | 69.30 | 14.51 | 79.44 |  73.01  |  59.04  |   53.12   | 2092  |\n| CoSENT     | nghuyong/ernie-3.0-base-zh                                  | [shibing624/text2vec-base-chinese-sentence](https://huggingface.co/shibing624/text2vec-base-chinese-sentence)                                     | 43.37 | 61.43 | 73.48 | 38.90 | 78.25 |  70.60  |  53.08  |   59.87   | 3089  |\n| CoSENT     | nghuyong/ernie-3.0-base-zh                                  | [shibing624/text2vec-base-chinese-paraphrase](https://huggingface.co/shibing624/text2vec-base-chinese-paraphrase)                                 | 44.89 | 63.58 | 74.24 | 40.90 | 78.93 |  76.70  |  63.30  | **63.08** | 3066  |\n| CoSENT     | sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2 | [shibing624/text2vec-base-multilingual](https://huggingface.co/shibing624/text2vec-base-multilingual)                                             | 32.39 | 50.33 | 65.64 | 32.56 | 74.45 |  68.88  |  51.17  |   53.67   | 3138  |\n| CoSENT     | BAAI/bge-large-zh-noinstruct                                | [shibing624/text2vec-bge-large-chinese](https://huggingface.co/shibing624/text2vec-bge-large-chinese)                                             | 38.41 | 61.34 | 71.72 | 35.15 | 76.44 |  71.81  |  63.15  |   59.72   |  844  |\n\n\nè¯´æ˜ï¼š\n- ç»“æœè¯„æµ‹æŒ‡æ ‡ï¼šspearmanç³»æ•°\n- `shibing624/text2vec-base-chinese`æ¨¡å‹ï¼Œæ˜¯ç”¨CoSENTæ–¹æ³•è®­ç»ƒï¼ŒåŸºäº`hfl/chinese-macbert-base`åœ¨ä¸­æ–‡STS-Bæ•°æ®è®­ç»ƒå¾—åˆ°ï¼Œå¹¶åœ¨ä¸­æ–‡STS-Bæµ‹è¯•é›†è¯„ä¼°è¾¾åˆ°è¾ƒå¥½æ•ˆæœï¼Œè¿è¡Œ[examples/training_sup_text_matching_model.py](https://github.com/shibing624/text2vec/blob/master/examples/training_sup_text_matching_model.py)ä»£ç å¯è®­ç»ƒæ¨¡å‹ï¼Œæ¨¡å‹æ–‡ä»¶å·²ç»ä¸Šä¼ HF model hubï¼Œä¸­æ–‡é€šç”¨è¯­ä¹‰åŒ¹é…ä»»åŠ¡æ¨èä½¿ç”¨\n- `shibing624/text2vec-base-chinese-sentence`æ¨¡å‹ï¼Œæ˜¯ç”¨CoSENTæ–¹æ³•è®­ç»ƒï¼ŒåŸºäº`nghuyong/ernie-3.0-base-zh`ç”¨äººå·¥æŒ‘é€‰åçš„ä¸­æ–‡STSæ•°æ®é›†[shibing624/nli-zh-all/text2vec-base-chinese-sentence-dataset](https://huggingface.co/datasets/shibing624/nli-zh-all/tree/main/text2vec-base-chinese-sentence-dataset)è®­ç»ƒå¾—åˆ°ï¼Œå¹¶åœ¨ä¸­æ–‡å„NLIæµ‹è¯•é›†è¯„ä¼°è¾¾åˆ°è¾ƒå¥½æ•ˆæœï¼Œè¿è¡Œ[examples/training_sup_text_matching_model_jsonl_data.py](https://github.com/shibing624/text2vec/blob/master/examples/training_sup_text_matching_model_jsonl_data.py)ä»£ç å¯è®­ç»ƒæ¨¡å‹ï¼Œæ¨¡å‹æ–‡ä»¶å·²ç»ä¸Šä¼ HF model hubï¼Œä¸­æ–‡s2s(å¥å­vså¥å­)è¯­ä¹‰åŒ¹é…ä»»åŠ¡æ¨èä½¿ç”¨\n- `shibing624/text2vec-base-chinese-paraphrase`æ¨¡å‹ï¼Œæ˜¯ç”¨CoSENTæ–¹æ³•è®­ç»ƒï¼ŒåŸºäº`nghuyong/ernie-3.0-base-zh`ç”¨äººå·¥æŒ‘é€‰åçš„ä¸­æ–‡STSæ•°æ®é›†[shibing624/nli-zh-all/text2vec-base-chinese-paraphrase-dataset](https://huggingface.co/datasets/shibing624/nli-zh-all/tree/main/text2vec-base-chinese-paraphrase-dataset)ï¼Œæ•°æ®é›†ç›¸å¯¹äº[shibing624/nli-zh-all/text2vec-base-chinese-sentence-dataset](https://huggingface.co/datasets/shibing624/nli-zh-all/tree/main/text2vec-base-chinese-sentence-dataset)åŠ å…¥äº†s2p(sentence to paraphrase)æ•°æ®ï¼Œå¼ºåŒ–äº†å…¶é•¿æ–‡æœ¬çš„è¡¨å¾èƒ½åŠ›ï¼Œå¹¶åœ¨ä¸­æ–‡å„NLIæµ‹è¯•é›†è¯„ä¼°è¾¾åˆ°SOTAï¼Œè¿è¡Œ[examples/training_sup_text_matching_model_jsonl_data.py](https://github.com/shibing624/text2vec/blob/master/examples/training_sup_text_matching_model_jsonl_data.py)ä»£ç å¯è®­ç»ƒæ¨¡å‹ï¼Œæ¨¡å‹æ–‡ä»¶å·²ç»ä¸Šä¼ HF model hubï¼Œä¸­æ–‡s2p(å¥å­vsæ®µè½)è¯­ä¹‰åŒ¹é…ä»»åŠ¡æ¨èä½¿ç”¨\n- `shibing624/text2vec-base-multilingual`æ¨¡å‹ï¼Œæ˜¯ç”¨CoSENTæ–¹æ³•è®­ç»ƒï¼ŒåŸºäº`sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2`ç”¨äººå·¥æŒ‘é€‰åçš„å¤šè¯­è¨€STSæ•°æ®é›†[shibing624/nli-zh-all/text2vec-base-multilingual-dataset](https://huggingface.co/datasets/shibing624/nli-zh-all/tree/main/text2vec-base-multilingual-dataset)è®­ç»ƒå¾—åˆ°ï¼Œå¹¶åœ¨ä¸­è‹±æ–‡æµ‹è¯•é›†è¯„ä¼°ç›¸å¯¹äºåŸæ¨¡å‹æ•ˆæœæœ‰æå‡ï¼Œè¿è¡Œ[examples/training_sup_text_matching_model_jsonl_data.py](https://github.com/shibing624/text2vec/blob/master/examples/training_sup_text_matching_model_jsonl_data.py)ä»£ç å¯è®­ç»ƒæ¨¡å‹ï¼Œæ¨¡å‹æ–‡ä»¶å·²ç»ä¸Šä¼ HF model hubï¼Œå¤šè¯­è¨€è¯­ä¹‰åŒ¹é…ä»»åŠ¡æ¨èä½¿ç”¨\n- `shibing624/text2vec-bge-large-chinese`æ¨¡å‹ï¼Œæ˜¯ç”¨CoSENTæ–¹æ³•è®­ç»ƒï¼ŒåŸºäº`BAAI/bge-large-zh-noinstruct`ç”¨äººå·¥æŒ‘é€‰åçš„ä¸­æ–‡STSæ•°æ®é›†[shibing624/nli-zh-all/text2vec-base-chinese-paraphrase-dataset](https://huggingface.co/datasets/shibing624/nli-zh-all/tree/main/text2vec-base-chinese-paraphrase-dataset)è®­ç»ƒå¾—åˆ°ï¼Œå¹¶åœ¨ä¸­æ–‡æµ‹è¯•é›†è¯„ä¼°ç›¸å¯¹äºåŸæ¨¡å‹æ•ˆæœæœ‰æå‡ï¼Œåœ¨çŸ­æ–‡æœ¬åŒºåˆ†åº¦ä¸Šæå‡æ˜æ˜¾ï¼Œè¿è¡Œ[examples/training_sup_text_matching_model_jsonl_data.py](https://github.com/shibing624/text2vec/blob/master/examples/training_sup_text_matching_model_jsonl_data.py)ä»£ç å¯è®­ç»ƒæ¨¡å‹ï¼Œæ¨¡å‹æ–‡ä»¶å·²ç»ä¸Šä¼ HF model hubï¼Œä¸­æ–‡s2s(å¥å­vså¥å­)è¯­ä¹‰åŒ¹é…ä»»åŠ¡æ¨èä½¿ç”¨\n- `w2v-light-tencent-chinese`æ˜¯è…¾è®¯è¯å‘é‡çš„Word2Vecæ¨¡å‹ï¼ŒCPUåŠ è½½ä½¿ç”¨ï¼Œé€‚ç”¨äºä¸­æ–‡å­—é¢åŒ¹é…ä»»åŠ¡å’Œç¼ºå°‘æ•°æ®çš„å†·å¯åŠ¨æƒ…å†µ\n- å„é¢„è®­ç»ƒæ¨¡å‹å‡å¯ä»¥é€šè¿‡transformersè°ƒç”¨ï¼Œå¦‚MacBERTæ¨¡å‹ï¼š`--model_name hfl/chinese-macbert-base` æˆ–è€…robertaæ¨¡å‹ï¼š`--model_name uer/roberta-medium-wwm-chinese-cluecorpussmall`\n- ä¸ºæµ‹è¯„æ¨¡å‹çš„é²æ£’æ€§ï¼ŒåŠ å…¥äº†æœªè®­ç»ƒè¿‡çš„SOHUæµ‹è¯•é›†ï¼Œç”¨äºæµ‹è¯•æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼›ä¸ºè¾¾åˆ°å¼€ç®±å³ç”¨çš„å®ç”¨æ•ˆæœï¼Œä½¿ç”¨äº†æœé›†åˆ°çš„å„ä¸­æ–‡åŒ¹é…æ•°æ®é›†ï¼Œæ•°æ®é›†ä¹Ÿä¸Šä¼ åˆ°HF datasets[é“¾æ¥è§ä¸‹æ–¹](#æ•°æ®é›†)\n- ä¸­æ–‡åŒ¹é…ä»»åŠ¡å®éªŒè¡¨æ˜ï¼Œpoolingæœ€ä¼˜æ˜¯`EncoderType.FIRST_LAST_AVG`å’Œ`EncoderType.MEAN`ï¼Œä¸¤è€…é¢„æµ‹æ•ˆæœå·®å¼‚å¾ˆå°\n- ä¸­æ–‡åŒ¹é…è¯„æµ‹ç»“æœå¤ç°ï¼Œå¯ä»¥ä¸‹è½½ä¸­æ–‡åŒ¹é…æ•°æ®é›†åˆ°`examples/data`ï¼Œè¿è¡Œ [tests/model_spearman.py](https://github.com/shibing624/text2vec/blob/master/tests/model_spearman.py) ä»£ç å¤ç°è¯„æµ‹ç»“æœ\n- QPSçš„GPUæµ‹è¯•ç¯å¢ƒæ˜¯Tesla V100ï¼Œæ˜¾å­˜32GB\n\næ¨¡å‹è®­ç»ƒå®éªŒæŠ¥å‘Šï¼š[å®éªŒæŠ¥å‘Š](https://github.com/shibing624/text2vec/blob/master/docs/model_report.md)\n## Demo\n\nOfficial Demo: https://www.mulanai.com/product/short_text_sim/\n\nHuggingFace Demo: https://huggingface.co/spaces/shibing624/text2vec\n\n![](https://github.com/shibing624/text2vec/blob/master/docs/hf.png)\n\nrun example: [examples/gradio_demo.py](https://github.com/shibing624/text2vec/blob/master/examples/gradio_demo.py) to see the demo:\n```shell\npython examples/gradio_demo.py\n```\n\n## Install\n```shell\npip install torch # conda install pytorch\npip install -U text2vec\n```\n\nor\n\n```shell\npip install torch # conda install pytorch\npip install -r requirements.txt\n\ngit clone https://github.com/shibing624/text2vec.git\ncd text2vec\npip install --no-deps .\n```\n\n## Usage\n\n### æ–‡æœ¬å‘é‡è¡¨å¾\n\nåŸºäº`pretrained model`è®¡ç®—æ–‡æœ¬å‘é‡ï¼š\n\n```zsh\n>>> from text2vec import SentenceModel\n>>> m = SentenceModel()\n>>> m.encode(\"å¦‚ä½•æ›´æ¢èŠ±å‘—ç»‘å®šé“¶è¡Œå¡\")\nEmbedding shape: (768,)\n```\n\nexample: [examples/computing_embeddings_demo.py](https://github.com/shibing624/text2vec/blob/master/examples/computing_embeddings_demo.py)\n\n```python\nimport sys\n\nsys.path.append('..')\nfrom text2vec import SentenceModel\nfrom text2vec import Word2Vec\n\n\ndef compute_emb(model):\n    # Embed a list of sentences\n    sentences = [\n        'å¡',\n        'é“¶è¡Œå¡',\n        'å¦‚ä½•æ›´æ¢èŠ±å‘—ç»‘å®šé“¶è¡Œå¡',\n        'èŠ±å‘—æ›´æ”¹ç»‘å®šé“¶è¡Œå¡',\n        'This framework generates embeddings for each input sentence',\n        'Sentences are passed as a list of string.',\n        'The quick brown fox jumps over the lazy dog.'\n    ]\n    sentence_embeddings = model.encode(sentences)\n    print(type(sentence_embeddings), sentence_embeddings.shape)\n\n    # The result is a list of sentence embeddings as numpy arrays\n    for sentence, embedding in zip(sentences, sentence_embeddings):\n        print(\"Sentence:\", sentence)\n        print(\"Embedding shape:\", embedding.shape)\n        print(\"Embedding head:\", embedding[:10])\n        print()\n\n\nif __name__ == \"__main__\":\n    # ä¸­æ–‡å¥å‘é‡æ¨¡å‹(CoSENT)ï¼Œä¸­æ–‡è¯­ä¹‰åŒ¹é…ä»»åŠ¡æ¨èï¼Œæ”¯æŒfine-tuneç»§ç»­è®­ç»ƒ\n    t2v_model = SentenceModel(\"shibing624/text2vec-base-chinese\")\n    compute_emb(t2v_model)\n\n    # æ”¯æŒå¤šè¯­è¨€çš„å¥å‘é‡æ¨¡å‹ï¼ˆCoSENTï¼‰ï¼Œå¤šè¯­è¨€ï¼ˆåŒ…æ‹¬ä¸­è‹±æ–‡ï¼‰è¯­ä¹‰åŒ¹é…ä»»åŠ¡æ¨èï¼Œæ”¯æŒfine-tuneç»§ç»­è®­ç»ƒ\n    sbert_model = SentenceModel(\"shibing624/text2vec-base-multilingual\")\n    compute_emb(sbert_model)\n\n    # ä¸­æ–‡è¯å‘é‡æ¨¡å‹(word2vec)ï¼Œä¸­æ–‡å­—é¢åŒ¹é…ä»»åŠ¡å’Œå†·å¯åŠ¨é€‚ç”¨\n    w2v_model = Word2Vec(\"w2v-light-tencent-chinese\")\n    compute_emb(w2v_model)\n\n```\n\noutput:\n```\n<class 'numpy.ndarray'> (7, 768)\nSentence: å¡\nEmbedding shape: (768,)\n\nSentence: é“¶è¡Œå¡\nEmbedding shape: (768,)\n ... \n```\n\n- è¿”å›å€¼`embeddings`æ˜¯`numpy.ndarray`ç±»å‹ï¼Œshapeä¸º`(sentences_size, model_embedding_size)`ï¼Œä¸‰ä¸ªæ¨¡å‹ä»»é€‰ä¸€ç§å³å¯ï¼Œæ¨èç”¨ç¬¬ä¸€ä¸ªã€‚\n- `shibing624/text2vec-base-chinese`æ¨¡å‹æ˜¯CoSENTæ–¹æ³•åœ¨ä¸­æ–‡STS-Bæ•°æ®é›†è®­ç»ƒå¾—åˆ°çš„ï¼Œæ¨¡å‹å·²ç»ä¸Šä¼ åˆ°huggingfaceçš„\næ¨¡å‹åº“[shibing624/text2vec-base-chinese](https://huggingface.co/shibing624/text2vec-base-chinese)ï¼Œ\næ˜¯`text2vec.SentenceModel`æŒ‡å®šçš„é»˜è®¤æ¨¡å‹ï¼Œå¯ä»¥é€šè¿‡ä¸Šé¢ç¤ºä¾‹è°ƒç”¨ï¼Œæˆ–è€…å¦‚ä¸‹æ‰€ç¤ºç”¨[transformersåº“](https://github.com/huggingface/transformers)è°ƒç”¨ï¼Œ\næ¨¡å‹è‡ªåŠ¨ä¸‹è½½åˆ°æœ¬æœºè·¯å¾„ï¼š`~/.cache/huggingface/transformers`\n- `w2v-light-tencent-chinese`æ˜¯é€šè¿‡gensimåŠ è½½çš„Word2Vecæ¨¡å‹ï¼Œä½¿ç”¨è…¾è®¯è¯å‘é‡è®¡ç®—å„å­—è¯çš„è¯å‘é‡ï¼Œå¥å­å‘é‡é€šè¿‡å•è¯è¯\nå‘é‡å–å¹³å‡å€¼å¾—åˆ°ï¼Œæ¨¡å‹è‡ªåŠ¨ä¸‹è½½åˆ°æœ¬æœºè·¯å¾„ï¼š`~/.text2vec/datasets/light_Tencent_AILab_ChineseEmbedding.bin`\n- `text2vec`æ”¯æŒå¤šå¡æ¨ç†(è®¡ç®—æ–‡æœ¬å‘é‡): [examples/computing_embeddings_multi_gpu_demo.py](https://github.com/shibing624/text2vec/blob/master/examples/computing_embeddings_multi_gpu_demo.py)\n\n#### Usage (HuggingFace Transformers)\nWithout [text2vec](https://github.com/shibing624/text2vec), you can use the model like this: \n\nFirst, you pass your input through the transformer model, then you have to apply the right pooling-operation on-top of the contextualized word embeddings.\n\nexample: [examples/use_origin_transformers_demo.py](https://github.com/shibing624/text2vec/blob/master/examples/use_origin_transformers_demo.py)\n\n```python\nimport os\nimport torch\nfrom transformers import AutoTokenizer, AutoModel\n\nos.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"\n\n\n# Mean Pooling - Take attention mask into account for correct averaging\ndef mean_pooling(model_output, attention_mask):\n    token_embeddings = model_output[0]  # First element of model_output contains all token embeddings\n    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n\n\n# Load model from HuggingFace Hub\ntokenizer = AutoTokenizer.from_pretrained('shibing624/text2vec-base-chinese')\nmodel = AutoModel.from_pretrained('shibing624/text2vec-base-chinese')\nsentences = ['å¦‚ä½•æ›´æ¢èŠ±å‘—ç»‘å®šé“¶è¡Œå¡', 'èŠ±å‘—æ›´æ”¹ç»‘å®šé“¶è¡Œå¡']\n# Tokenize sentences\nencoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')\n\n# Compute token embeddings\nwith torch.no_grad():\n    model_output = model(**encoded_input)\n# Perform pooling. In this case, max pooling.\nsentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\nprint(\"Sentence embeddings:\")\nprint(sentence_embeddings)\n```\n\n#### Usage (sentence-transformers)\n[sentence-transformers](https://github.com/UKPLab/sentence-transformers) is a popular library to compute dense vector representations for sentences.\n\nInstall sentence-transformers:\n```shell\npip install -U sentence-transformers\n```\nThen load model and predict:\n```python\nfrom sentence_transformers import SentenceTransformer\n\nm = SentenceTransformer(\"shibing624/text2vec-base-chinese\")\nsentences = ['å¦‚ä½•æ›´æ¢èŠ±å‘—ç»‘å®šé“¶è¡Œå¡', 'èŠ±å‘—æ›´æ”¹ç»‘å®šé“¶è¡Œå¡']\n\nsentence_embeddings = m.encode(sentences)\nprint(\"Sentence embeddings:\")\nprint(sentence_embeddings)\n```\n\n#### `Word2Vec`è¯å‘é‡\n\næä¾›`Word2Vec`è¯å‘é‡ï¼Œè½»é‡ç‰ˆè…¾è®¯è¯å‘é‡`light_Tencent_AILab_ChineseEmbedding.bin`ï¼ŒäºŒè¿›åˆ¶æ–‡ä»¶ï¼Œ111Mï¼Œæ˜¯ç®€åŒ–åçš„é«˜é¢‘143613ä¸ªè¯ï¼Œæ¯ä¸ªè¯å‘é‡è¿˜æ˜¯200ç»´ï¼ˆè·ŸåŸç‰ˆä¸€æ ·ï¼‰ï¼Œè¿è¡Œç¨‹åºï¼Œè‡ªåŠ¨ä¸‹è½½åˆ° `~/.text2vec/datasets/light_Tencent_AILab_ChineseEmbedding.bin`\n\næ¨¡å‹åœ°å€ï¼š[Modelscope](https://modelscope.cn/models/lili666/text2vec-word2vec-tencent-chinese/summary) | [ç™¾åº¦äº‘ç›˜-å¯†ç :tawe](https://pan.baidu.com/s/1La4U4XNFe8s5BJqxPQpeiQ) | [è°·æ­Œäº‘ç›˜](https://drive.google.com/u/0/uc?id=1iQo9tBb2NgFOBxx0fA16AZpSgc-bG_Rp&export=download)\n\n### å‘½ä»¤è¡Œæ¨¡å¼ï¼ˆCLIï¼‰\n\næ”¯æŒæ‰¹é‡è·å–æ–‡æœ¬å‘é‡\n\ncode: [cli.py](https://github.com/shibing624/text2vec/blob/master/text2vec/cli.py)\n\n```\n> text2vec -h                                    \nusage: text2vec [-h] --input_file INPUT_FILE [--output_file OUTPUT_FILE] [--model_type MODEL_TYPE] [--model_name MODEL_NAME] [--encoder_type ENCODER_TYPE]\n                [--batch_size BATCH_SIZE] [--max_seq_length MAX_SEQ_LENGTH] [--chunk_size CHUNK_SIZE] [--device DEVICE]\n                [--show_progress_bar SHOW_PROGRESS_BAR] [--normalize_embeddings NORMALIZE_EMBEDDINGS]\n\ntext2vec cli\n\noptional arguments:\n  -h, --help            show this help message and exit\n  --input_file INPUT_FILE\n                        input file path, text file, required\n  --output_file OUTPUT_FILE\n                        output file path, output csv file, default text_embs.csv\n  --model_type MODEL_TYPE\n                        model type: sentencemodel, word2vec, default sentencemodel\n  --model_name MODEL_NAME\n                        model name or path, default shibing624/text2vec-base-chinese\n  --encoder_type ENCODER_TYPE\n                        encoder type: MEAN, CLS, POOLER, FIRST_LAST_AVG, LAST_AVG, default MEAN\n  --batch_size BATCH_SIZE\n                        batch size, default 32\n  --max_seq_length MAX_SEQ_LENGTH\n                        max sequence length, default 256\n  --chunk_size CHUNK_SIZE\n                        chunk size to save partial results, default 1000\n  --device DEVICE       device: cpu, cuda, default None\n  --show_progress_bar SHOW_PROGRESS_BAR\n                        show progress bar, default True\n  --normalize_embeddings NORMALIZE_EMBEDDINGS\n                        normalize embeddings, default False\n  --multi_gpu MULTI_GPU\n                        multi gpu, default False\n```\n\nrunï¼š\n\n```shell\npip install text2vec -U\ntext2vec --input_file input.txt --output_file out.csv --batch_size 128 --multi_gpu True\n```\n\n> è¾“å…¥æ–‡ä»¶ï¼ˆrequiredï¼‰ï¼š`input.txt`ï¼Œformatï¼šä¸€å¥è¯ä¸€è¡Œçš„å¥å­æ–‡æœ¬ã€‚\n\n## ä¸‹æ¸¸ä»»åŠ¡\n### 1. å¥å­ç›¸ä¼¼åº¦è®¡ç®—\n\nexample: [examples/semantic_text_similarity_demo.py](https://github.com/shibing624/text2vec/blob/master/examples/semantic_text_similarity_demo.py)\n\n```python\nimport sys\n\nsys.path.append('..')\nfrom text2vec import Similarity\n\n# Two lists of sentences\nsentences1 = ['å¦‚ä½•æ›´æ¢èŠ±å‘—ç»‘å®šé“¶è¡Œå¡',\n              'The cat sits outside',\n              'A man is playing guitar',\n              'The new movie is awesome']\n\nsentences2 = ['èŠ±å‘—æ›´æ”¹ç»‘å®šé“¶è¡Œå¡',\n              'The dog plays in the garden',\n              'A woman watches TV',\n              'The new movie is so great']\n\nsim_model = Similarity()\nfor i in range(len(sentences1)):\n    for j in range(len(sentences2)):\n        score = sim_model.get_score(sentences1[i], sentences2[j])\n        print(\"{} \\t\\t {} \\t\\t Score: {:.4f}\".format(sentences1[i], sentences2[j], score))\n```\n\noutput:\n```shell\nå¦‚ä½•æ›´æ¢èŠ±å‘—ç»‘å®šé“¶è¡Œå¡ \t\t èŠ±å‘—æ›´æ”¹ç»‘å®šé“¶è¡Œå¡ \t\t Score: 0.9477\nå¦‚ä½•æ›´æ¢èŠ±å‘—ç»‘å®šé“¶è¡Œå¡ \t\t The dog plays in the garden \t\t Score: -0.1748\nå¦‚ä½•æ›´æ¢èŠ±å‘—ç»‘å®šé“¶è¡Œå¡ \t\t A woman watches TV \t\t Score: -0.0839\nå¦‚ä½•æ›´æ¢èŠ±å‘—ç»‘å®šé“¶è¡Œå¡ \t\t The new movie is so great \t\t Score: -0.0044\nThe cat sits outside \t\t èŠ±å‘—æ›´æ”¹ç»‘å®šé“¶è¡Œå¡ \t\t Score: -0.0097\nThe cat sits outside \t\t The dog plays in the garden \t\t Score: 0.1908\nThe cat sits outside \t\t A woman watches TV \t\t Score: -0.0203\nThe cat sits outside \t\t The new movie is so great \t\t Score: 0.0302\nA man is playing guitar \t\t èŠ±å‘—æ›´æ”¹ç»‘å®šé“¶è¡Œå¡ \t\t Score: -0.0010\nA man is playing guitar \t\t The dog plays in the garden \t\t Score: 0.1062\nA man is playing guitar \t\t A woman watches TV \t\t Score: 0.0055\nA man is playing guitar \t\t The new movie is so great \t\t Score: 0.0097\nThe new movie is awesome \t\t èŠ±å‘—æ›´æ”¹ç»‘å®šé“¶è¡Œå¡ \t\t Score: 0.0302\nThe new movie is awesome \t\t The dog plays in the garden \t\t Score: -0.0160\nThe new movie is awesome \t\t A woman watches TV \t\t Score: 0.1321\nThe new movie is awesome \t\t The new movie is so great \t\t Score: 0.9591\n```\n\n> å¥å­ä½™å¼¦ç›¸ä¼¼åº¦å€¼`score`èŒƒå›´æ˜¯[-1, 1]ï¼Œå€¼è¶Šå¤§è¶Šç›¸ä¼¼ã€‚\n\n### 2. æ–‡æœ¬åŒ¹é…æœç´¢\n\nä¸€èˆ¬åœ¨æ–‡æ¡£å€™é€‰é›†ä¸­æ‰¾ä¸queryæœ€ç›¸ä¼¼çš„æ–‡æœ¬ï¼Œå¸¸ç”¨äºQAåœºæ™¯çš„é—®å¥ç›¸ä¼¼åŒ¹é…ã€æ–‡æœ¬ç›¸ä¼¼æ£€ç´¢ç­‰ä»»åŠ¡ã€‚\n\n\nexample: [examples/semantic_search_demo.py](https://github.com/shibing624/text2vec/blob/master/examples/semantic_search_demo.py)\n\n```python\nimport sys\n\nsys.path.append('..')\nfrom text2vec import SentenceModel, cos_sim, semantic_search\n\nembedder = SentenceModel()\n\n# Corpus with example sentences\ncorpus = [\n    'èŠ±å‘—æ›´æ”¹ç»‘å®šé“¶è¡Œå¡',\n    'æˆ‘ä»€ä¹ˆæ—¶å€™å¼€é€šäº†èŠ±å‘—',\n    'A man is eating food.',\n    'A man is eating a piece of bread.',\n    'The girl is carrying a baby.',\n    'A man is riding a horse.',\n    'A woman is playing violin.',\n    'Two men pushed carts through the woods.',\n    'A man is riding a white horse on an enclosed ground.',\n    'A monkey is playing drums.',\n    'A cheetah is running behind its prey.'\n]\ncorpus_embeddings = embedder.encode(corpus)\n\n# Query sentences:\nqueries = [\n    'å¦‚ä½•æ›´æ¢èŠ±å‘—ç»‘å®šé“¶è¡Œå¡',\n    'A man is eating pasta.',\n    'Someone in a gorilla costume is playing a set of drums.',\n    'A cheetah chases prey on across a field.']\n\nfor query in queries:\n    query_embedding = embedder.encode(query)\n    hits = semantic_search(query_embedding, corpus_embeddings, top_k=5)\n    print(\"\\n\\n======================\\n\\n\")\n    print(\"Query:\", query)\n    print(\"\\nTop 5 most similar sentences in corpus:\")\n    hits = hits[0]  # Get the hits for the first query\n    for hit in hits:\n        print(corpus[hit['corpus_id']], \"(Score: {:.4f})\".format(hit['score']))\n```\noutput:\n```shell\nQuery: å¦‚ä½•æ›´æ¢èŠ±å‘—ç»‘å®šé“¶è¡Œå¡\nTop 5 most similar sentences in corpus:\nèŠ±å‘—æ›´æ”¹ç»‘å®šé“¶è¡Œå¡ (Score: 0.9477)\næˆ‘ä»€ä¹ˆæ—¶å€™å¼€é€šäº†èŠ±å‘— (Score: 0.3635)\nA man is eating food. (Score: 0.0321)\nA man is riding a horse. (Score: 0.0228)\nTwo men pushed carts through the woods. (Score: 0.0090)\n\n======================\nQuery: A man is eating pasta.\nTop 5 most similar sentences in corpus:\nA man is eating food. (Score: 0.6734)\nA man is eating a piece of bread. (Score: 0.4269)\nA man is riding a horse. (Score: 0.2086)\nA man is riding a white horse on an enclosed ground. (Score: 0.1020)\nA cheetah is running behind its prey. (Score: 0.0566)\n\n======================\nQuery: Someone in a gorilla costume is playing a set of drums.\nTop 5 most similar sentences in corpus:\nA monkey is playing drums. (Score: 0.8167)\nA cheetah is running behind its prey. (Score: 0.2720)\nA woman is playing violin. (Score: 0.1721)\nA man is riding a horse. (Score: 0.1291)\nA man is riding a white horse on an enclosed ground. (Score: 0.1213)\n\n======================\nQuery: A cheetah chases prey on across a field.\nTop 5 most similar sentences in corpus:\nA cheetah is running behind its prey. (Score: 0.9147)\nA monkey is playing drums. (Score: 0.2655)\nA man is riding a horse. (Score: 0.1933)\nA man is riding a white horse on an enclosed ground. (Score: 0.1733)\nA man is eating food. (Score: 0.0329)\n```\n\n \n\n## ä¸‹æ¸¸ä»»åŠ¡æ”¯æŒåº“\n**similaritiesåº“[æ¨è]**\n\næ–‡æœ¬ç›¸ä¼¼åº¦è®¡ç®—å’Œæ–‡æœ¬åŒ¹é…æœç´¢ä»»åŠ¡ï¼Œæ¨èä½¿ç”¨ [similaritiesåº“](https://github.com/shibing624/similarities) ï¼Œå…¼å®¹æœ¬é¡¹ç›®releaseçš„\nWord2vecã€SBERTã€Cosentç±»è¯­ä¹‰åŒ¹é…æ¨¡å‹ï¼Œè¿˜æ”¯æŒäº¿çº§å›¾æ–‡æœç´¢ï¼Œæ”¯æŒ**æ–‡æœ¬è¯­ä¹‰å»é‡**ï¼Œ**å›¾ç‰‡å»é‡**ç­‰åŠŸèƒ½ã€‚\n\nå®‰è£…ï¼š\n```pip install -U similarities```\n\nå¥å­ç›¸ä¼¼åº¦è®¡ç®—ï¼š\n```python\nfrom similarities import BertSimilarity\n\nm = BertSimilarity()\nr = m.similarity('å¦‚ä½•æ›´æ¢èŠ±å‘—ç»‘å®šé“¶è¡Œå¡', 'èŠ±å‘—æ›´æ”¹ç»‘å®šé“¶è¡Œå¡')\nprint(f\"similarity score: {float(r)}\")  # similarity score: 0.855146050453186\n```\n\n## Models\n\n### CoSENT model\n\nCoSENTï¼ˆCosine Sentenceï¼‰æ–‡æœ¬åŒ¹é…æ¨¡å‹ï¼Œåœ¨Sentence-BERTä¸Šæ”¹è¿›äº†CosineRankLossçš„å¥å‘é‡æ–¹æ¡ˆ\n\n\nNetwork structure:\n\nTraining:\n\n<img src=\"docs/cosent_train.png\" width=\"300\" />\n\n\nInference:\n\n<img src=\"docs/inference.png\" width=\"300\" />\n\n#### CoSENT ç›‘ç£æ¨¡å‹\nè®­ç»ƒå’Œé¢„æµ‹CoSENTæ¨¡å‹ï¼š\n\n- åœ¨ä¸­æ–‡STS-Bæ•°æ®é›†è®­ç»ƒå’Œè¯„ä¼°`CoSENT`æ¨¡å‹\n\nexample: [examples/training_sup_text_matching_model.py](https://github.com/shibing624/text2vec/blob/master/examples/training_sup_text_matching_model.py)\n\n```shell\ncd examples\npython training_sup_text_matching_model.py --model_arch cosent --do_train --do_predict --num_epochs 10 --model_name hfl/chinese-macbert-base --output_dir ./outputs/STS-B-cosent\n```\n\n- åœ¨èš‚èšé‡‘èåŒ¹é…æ•°æ®é›†ATECä¸Šè®­ç»ƒå’Œè¯„ä¼°`CoSENT`æ¨¡å‹\n\næ”¯æŒè¿™äº›ä¸­æ–‡åŒ¹é…æ•°æ®é›†çš„ä½¿ç”¨ï¼š'ATEC', 'STS-B', 'BQ', 'LCQMC', 'PAWSX'ï¼Œå…·ä½“å‚è€ƒHuggingFace datasets [https://huggingface.co/datasets/shibing624/nli_zh](https://huggingface.co/datasets/shibing624/nli_zh)\n```shell\npython training_sup_text_matching_model.py --task_name ATEC --model_arch cosent --do_train --do_predict --num_epochs 10 --model_name hfl/chinese-macbert-base --output_dir ./outputs/ATEC-cosent\n```\n\n- åœ¨è‡ªæœ‰ä¸­æ–‡æ•°æ®é›†ä¸Šè®­ç»ƒæ¨¡å‹\n\nexample: [examples/training_sup_text_matching_model_mydata.py](https://github.com/shibing624/text2vec/blob/master/examples/training_sup_text_matching_model_mydata.py)\n\nå•å¡è®­ç»ƒï¼š\n```shell\nCUDA_VISIBLE_DEVICES=0 python training_sup_text_matching_model_mydata.py --do_train --do_predict\n```\n\nå¤šå¡è®­ç»ƒï¼š\n```shell\nCUDA_VISIBLE_DEVICES=0,1 torchrun --nproc_per_node 2  training_sup_text_matching_model_mydata.py --do_train --do_predict --output_dir outputs/STS-B-text2vec-macbert-v1 --batch_size 64 --bf16 --data_parallel \n```\n\nè®­ç»ƒé›†æ ¼å¼å‚è€ƒ[examples/data/STS-B/STS-B.valid.data](https://github.com/shibing624/text2vec/blob/master/examples/data/STS-B/STS-B.valid.data)\n\n```shell\nsentence1   sentence2   label\nä¸€ä¸ªå¥³å­©åœ¨ç»™å¥¹çš„å¤´å‘åšå‘å‹ã€‚\tä¸€ä¸ªå¥³å­©åœ¨æ¢³å¤´ã€‚\t2\nä¸€ç¾¤ç”·äººåœ¨æµ·æ»©ä¸Šè¸¢è¶³çƒã€‚\tä¸€ç¾¤ç”·å­©åœ¨æµ·æ»©ä¸Šè¸¢è¶³çƒã€‚\t3\nä¸€ä¸ªå¥³äººåœ¨æµ‹é‡å¦ä¸€ä¸ªå¥³äººçš„è„šè¸ã€‚\tå¥³äººæµ‹é‡å¦ä¸€ä¸ªå¥³äººçš„è„šè¸ã€‚\t5\n```\n\n`label`å¯ä»¥æ˜¯0ï¼Œ1æ ‡ç­¾ï¼Œ0ä»£è¡¨ä¸¤ä¸ªå¥å­ä¸ç›¸ä¼¼ï¼Œ1ä»£è¡¨ç›¸ä¼¼ï¼›ä¹Ÿå¯ä»¥æ˜¯0-5çš„è¯„åˆ†ï¼Œè¯„åˆ†è¶Šé«˜ï¼Œè¡¨ç¤ºä¸¤ä¸ªå¥å­è¶Šç›¸ä¼¼ã€‚æ¨¡å‹éƒ½èƒ½æ”¯æŒã€‚\n\n\n- åœ¨è‹±æ–‡STS-Bæ•°æ®é›†è®­ç»ƒå’Œè¯„ä¼°`CoSENT`æ¨¡å‹\n\nexample: [examples/training_sup_text_matching_model_en.py](https://github.com/shibing624/text2vec/blob/master/examples/training_sup_text_matching_model_en.py)\n\n```shell\ncd examples\npython training_sup_text_matching_model_en.py --model_arch cosent --do_train --do_predict --num_epochs 10 --model_name bert-base-uncased  --output_dir ./outputs/STS-B-en-cosent\n```\n\n#### CoSENT æ— ç›‘ç£æ¨¡å‹\n- åœ¨è‹±æ–‡NLIæ•°æ®é›†è®­ç»ƒ`CoSENT`æ¨¡å‹ï¼Œåœ¨STS-Bæµ‹è¯•é›†è¯„ä¼°æ•ˆæœ\n\nexample: [examples/training_unsup_text_matching_model_en.py](https://github.com/shibing624/text2vec/blob/master/examples/training_unsup_text_matching_model_en.py)\n\n```shell\ncd examples\npython training_unsup_text_matching_model_en.py --model_arch cosent --do_train --do_predict --num_epochs 10 --model_name bert-base-uncased --output_dir ./outputs/STS-B-en-unsup-cosent\n```\n\n\n### Sentence-BERT model\n\nSentence-BERTæ–‡æœ¬åŒ¹é…æ¨¡å‹ï¼Œè¡¨å¾å¼å¥å‘é‡è¡¨ç¤ºæ–¹æ¡ˆ\n\nNetwork structure:\n\nTraining:\n\n<img src=\"docs/sbert_train.png\" width=\"300\" />\n\n\nInference:\n\n<img src=\"docs/sbert_inference.png\" width=\"300\" />\n\n#### SentenceBERT ç›‘ç£æ¨¡å‹\n- åœ¨ä¸­æ–‡STS-Bæ•°æ®é›†è®­ç»ƒå’Œè¯„ä¼°`SBERT`æ¨¡å‹\n\nexample: [examples/training_sup_text_matching_model.py](https://github.com/shibing624/text2vec/blob/master/examples/training_sup_text_matching_model.py)\n\n```shell\ncd examples\npython training_sup_text_matching_model.py --model_arch sentencebert --do_train --do_predict --num_epochs 10 --model_name hfl/chinese-macbert-base --output_dir ./outputs/STS-B-sbert\n```\n- åœ¨è‹±æ–‡STS-Bæ•°æ®é›†è®­ç»ƒå’Œè¯„ä¼°`SBERT`æ¨¡å‹\n\nexample: [examples/training_sup_text_matching_model_en.py](https://github.com/shibing624/text2vec/blob/master/examples/training_sup_text_matching_model_en.py)\n\n```shell\ncd examples\npython training_sup_text_matching_model_en.py --model_arch sentencebert --do_train --do_predict --num_epochs 10 --model_name bert-base-uncased --output_dir ./outputs/STS-B-en-sbert\n```\n\n#### SentenceBERT æ— ç›‘ç£æ¨¡å‹\n- åœ¨è‹±æ–‡NLIæ•°æ®é›†è®­ç»ƒ`SBERT`æ¨¡å‹ï¼Œåœ¨STS-Bæµ‹è¯•é›†è¯„ä¼°æ•ˆæœ\n\nexample: [examples/training_unsup_text_matching_model_en.py](https://github.com/shibing624/text2vec/blob/master/examples/training_unsup_text_matching_model_en.py)\n\n```shell\ncd examples\npython training_unsup_text_matching_model_en.py --model_arch sentencebert --do_train --do_predict --num_epochs 10 --model_name bert-base-uncased --output_dir ./outputs/STS-B-en-unsup-sbert\n```\n\n### BERT-Match model\nBERTæ–‡æœ¬åŒ¹é…æ¨¡å‹ï¼ŒåŸç”ŸBERTåŒ¹é…ç½‘ç»œç»“æ„ï¼Œäº¤äº’å¼å¥å‘é‡åŒ¹é…æ¨¡å‹\n\nNetwork structure:\n\nTraining and inference:\n\n<img src=\"docs/bert-fc-train.png\" width=\"300\" />\n\nè®­ç»ƒè„šæœ¬åŒä¸Š[examples/training_sup_text_matching_model.py](https://github.com/shibing624/text2vec/blob/master/examples/training_sup_text_matching_model.py)ã€‚\n\n\n\n### BGE model\n\n#### BGE ç›‘ç£æ¨¡å‹\n- åœ¨ä¸­æ–‡STS-Bæ•°æ®é›†è®­ç»ƒå’Œè¯„ä¼°`BGE`æ¨¡å‹\n\nexample: [examples/training_bge_model_mydata.py](https://github.com/shibing624/text2vec/blob/master/examples/training_bge_model_mydata.py)\n\n```shell\ncd examples\npython training_bge_model_mydata.py --model_arch bge --do_train --do_predict --num_epochs 4 --output_dir ./outputs/STS-B-bge-v1 --batch_size 4 --save_model_every_epoch --bf16\n```\n\n- è‡ªå»ºBGEè®­ç»ƒé›†\n\nBGEæ¨¡å‹å¾®è°ƒè®­ç»ƒï¼Œä½¿ç”¨å¯¹æ¯”å­¦ä¹ è®­ç»ƒæ¨¡å‹ï¼Œè¾“å…¥æ•°æ®çš„æ ¼å¼æ˜¯ä¸€ä¸ªä¸‰å…ƒç»„' (query, positive, negative) '\n\n```shell\ncd examples/data\npython build_zh_bge_dataset.py\npython hard_negatives_mine.py\n```\n1. `build_zh_bge_dataset.py` åŸºäºä¸­æ–‡STS-Bç”Ÿæˆä¸‰å…ƒç»„è®­ç»ƒé›†ï¼Œæ ¼å¼å¦‚ä¸‹ï¼š\n```json lines\n{\"query\":\"ä¸€ä¸ªç”·äººæ­£åœ¨å¾€é”…é‡Œå€’æ²¹ã€‚\",\"pos\":[\"ä¸€ä¸ªç”·äººæ­£åœ¨å¾€é”…é‡Œå€’æ²¹ã€‚\"],\"neg\":[\"äº²ä¿„å†›é˜Ÿè¿›å…¥å…‹é‡Œç±³äºšä¹Œå…‹å…°æµ·å†›åŸºåœ°\",\"é…æœ‰æœ¨åˆ¶å®¶å…·çš„ä¼˜é›…é¤å…ã€‚\",\"é©¬é›…ç“¦è’‚è¦æ±‚æ€»ç»Ÿç»Ÿæ²»æŸ¥è°Ÿå’Œå…‹ä»€ç±³å°”\",\"éå…¸è¿˜å¤ºå»äº†å¤šä¼¦å¤šåœ°åŒº44äººçš„ç”Ÿå‘½ï¼Œå…¶ä¸­åŒ…æ‹¬ä¸¤åæŠ¤å£«å’Œä¸€ååŒ»ç”Ÿã€‚\",\"åœ¨ä¸€æ¬¡é‡‡è®¿ä¸­ï¼Œèº«ä¸ºçŠ¯ç½ªå­¦å®¶çš„å¸Œåˆ©è¯´ï¼Œè¿™é‡Œå’Œå…¨å›½å„åœ°çš„è®¸å¤šè®®å‘˜éƒ½å¯¹æ­»åˆ‘æŠ±æœ‰æˆ’å¿ƒã€‚\",\"è±šé¼ åƒèƒ¡èåœã€‚\",\"ç‹—å˜´é‡Œå¼ç€ä¸€æ ¹æ£å­åœ¨æ°´ä¸­æ¸¸æ³³ã€‚\",\"æ‹‰é‡ŒÂ·ä½©å¥‡è¯´Androidå¾ˆé‡è¦ï¼Œä¸æ˜¯å…³é”®\",\"æ³•å›½ã€æ¯”åˆ©æ—¶ã€å¾·å›½ã€ç‘å…¸ã€æ„å¤§åˆ©å’Œè‹±å›½ä¸ºå°åº¦è®¡åˆ’å‘ç¼…ç”¸å‡ºå”®çš„å…ˆè¿›è½»å‹ç›´å‡æœºæä¾›é›¶éƒ¨ä»¶å’ŒæŠ€æœ¯ã€‚\",\"å·´æ—èµ›é©¬ä¼šåœ¨åŠ¨ä¹±ä¸­è¿›è¡Œ\"]}\n```\n2. `hard_negatives_mine.py` ä½¿ç”¨faissç›¸ä¼¼åŒ¹é…ï¼ŒæŒ–æ˜éš¾è´Ÿä¾‹ã€‚\n\n\n### æ¨¡å‹è’¸é¦ï¼ˆModel Distillationï¼‰\n\nç”±äºtext2vecè®­ç»ƒçš„æ¨¡å‹å¯ä»¥ä½¿ç”¨[sentence-transformers](https://github.com/UKPLab/sentence-transformers)åº“åŠ è½½ï¼Œæ­¤å¤„å¤ç”¨å…¶æ¨¡å‹è’¸é¦æ–¹æ³•[distillation](https://github.com/UKPLab/sentence-transformers/tree/master/examples/training/distillation)ã€‚\n\n1. æ¨¡å‹é™ç»´ï¼Œå‚è€ƒ[dimensionality_reduction.py](https://github.com/UKPLab/sentence-transformers/blob/master/examples/training/distillation/dimensionality_reduction.py)ä½¿ç”¨PCAå¯¹æ¨¡å‹è¾“å‡ºembeddingé™ç»´ï¼Œå¯å‡å°‘milvusç­‰å‘é‡æ£€ç´¢æ•°æ®åº“çš„å­˜å‚¨å‹åŠ›ï¼Œè¿˜èƒ½è½»å¾®æå‡æ¨¡å‹æ•ˆæœã€‚\n2. æ¨¡å‹è’¸é¦ï¼Œå‚è€ƒ[model_distillation.py](https://github.com/UKPLab/sentence-transformers/blob/master/examples/training/distillation/model_distillation.py)ä½¿ç”¨è’¸é¦æ–¹æ³•ï¼Œå°†Teacherå¤§æ¨¡å‹è’¸é¦åˆ°æ›´å°‘layerså±‚æ•°çš„studentæ¨¡å‹ä¸­ï¼Œåœ¨æƒè¡¡æ•ˆæœçš„æƒ…å†µä¸‹ï¼Œå¯å¤§å¹…æå‡æ¨¡å‹é¢„æµ‹é€Ÿåº¦ã€‚\n\n### æ¨¡å‹éƒ¨ç½²\n\næä¾›ä¸¤ç§éƒ¨ç½²æ¨¡å‹ï¼Œæ­å»ºæœåŠ¡çš„æ–¹æ³•ï¼š 1ï¼‰åŸºäºJinaæ­å»ºgRPCæœåŠ¡ã€æ¨èã€‘ï¼›2ï¼‰åŸºäºFastAPIæ­å»ºåŸç”ŸHttpæœåŠ¡ã€‚\n\n#### JinaæœåŠ¡\né‡‡ç”¨C/Sæ¨¡å¼æ­å»ºé«˜æ€§èƒ½æœåŠ¡ï¼Œæ”¯æŒdockeräº‘åŸç”Ÿï¼ŒgRPC/HTTP/WebSocketï¼Œæ”¯æŒå¤šä¸ªæ¨¡å‹åŒæ—¶é¢„æµ‹ï¼ŒGPUå¤šå¡å¤„ç†ã€‚\n\n- å®‰è£…ï¼š\n```pip install jina```\n\n- å¯åŠ¨æœåŠ¡ï¼š\n\nexample: [examples/jina_server_demo.py](examples/jina_server_demo.py)\n```python\nfrom jina import Flow\n\nport = 50001\nf = Flow(port=port).add(\n    uses='jinahub://Text2vecEncoder',\n    uses_with={'model_name': 'shibing624/text2vec-base-chinese'}\n)\n\nwith f:\n    # backend server forever\n    f.block()\n```\n\nè¯¥æ¨¡å‹é¢„æµ‹æ–¹æ³•ï¼ˆexecutorï¼‰å·²ç»ä¸Šä¼ åˆ°[JinaHub](https://hub.jina.ai/executor/eq45c9uq)ï¼Œé‡Œé¢åŒ…æ‹¬dockerã€k8séƒ¨ç½²æ–¹æ³•ã€‚\n\n- è°ƒç”¨æœåŠ¡ï¼š\n\n\n```python\nfrom jina import Client\nfrom docarray import Document, DocumentArray\n\nport = 50001\n\nc = Client(port=port)\n\ndata = ['å¦‚ä½•æ›´æ¢èŠ±å‘—ç»‘å®šé“¶è¡Œå¡',\n        'èŠ±å‘—æ›´æ”¹ç»‘å®šé“¶è¡Œå¡']\nprint(\"data:\", data)\nprint('data embs:')\nr = c.post('/', inputs=DocumentArray([Document(text='å¦‚ä½•æ›´æ¢èŠ±å‘—ç»‘å®šé“¶è¡Œå¡'), Document(text='èŠ±å‘—æ›´æ”¹ç»‘å®šé“¶è¡Œå¡')]))\nprint(r.embeddings)\n```\n\næ‰¹é‡è°ƒç”¨æ–¹æ³•è§example: [examples/jina_client_demo.py](https://github.com/shibing624/text2vec/blob/master/examples/jina_client_demo.py)\n\n\n#### FastAPIæœåŠ¡\n\n- å®‰è£…ï¼š\n```pip install fastapi uvicorn```\n\n- å¯åŠ¨æœåŠ¡ï¼š\n\nexample: [examples/fastapi_server_demo.py](https://github.com/shibing624/text2vec/blob/master/examples/fastapi_server_demo.py)\n```shell\ncd examples\npython fastapi_server_demo.py\n```\n\n- è°ƒç”¨æœåŠ¡ï¼š\n```shell\ncurl -X 'GET' \\\n  'http://0.0.0.0:8001/emb?q=hello' \\\n  -H 'accept: application/json'\n```\n\n\n## Dataset\n\n- æœ¬é¡¹ç›®releaseçš„æ•°æ®é›†ï¼š\n\n| Dataset                    | Introduce                                                                | Download Link                                                                                                                                                                                                                                                                                         |\n|:---------------------------|:-------------------------------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| shibing624/nli-zh-all      | ä¸­æ–‡è¯­ä¹‰åŒ¹é…æ•°æ®åˆé›†ï¼Œæ•´åˆäº†æ–‡æœ¬æ¨ç†ï¼Œç›¸ä¼¼ï¼Œæ‘˜è¦ï¼Œé—®ç­”ï¼ŒæŒ‡ä»¤å¾®è°ƒç­‰ä»»åŠ¡çš„820ä¸‡é«˜è´¨é‡æ•°æ®ï¼Œå¹¶è½¬åŒ–ä¸ºåŒ¹é…æ ¼å¼æ•°æ®é›†                | [https://huggingface.co/datasets/shibing624/nli-zh-all](https://huggingface.co/datasets/shibing624/nli-zh-all)                                                                                                                                                                                        |\n| shibing624/snli-zh         | ä¸­æ–‡SNLIå’ŒMultiNLIæ•°æ®é›†ï¼Œç¿»è¯‘è‡ªè‹±æ–‡SNLIå’ŒMultiNLI                                    | [https://huggingface.co/datasets/shibing624/snli-zh](https://huggingface.co/datasets/shibing624/snli-zh)                                                                                                                                                                                              |\n| shibing624/nli_zh          | ä¸­æ–‡è¯­ä¹‰åŒ¹é…æ•°æ®é›†ï¼Œæ•´åˆäº†ä¸­æ–‡ATECã€BQã€LCQMCã€PAWSXã€STS-Bå…±5ä¸ªä»»åŠ¡çš„æ•°æ®é›†                        | [https://huggingface.co/datasets/shibing624/nli_zh](https://huggingface.co/datasets/shibing624/nli_zh) </br> or </br> [ç™¾åº¦ç½‘ç›˜(æå–ç :qkt6)](https://pan.baidu.com/s/1d6jSiU1wHQAEMWJi7JJWCQ) </br> or </br> [github](https://github.com/shibing624/text2vec/releases/download/1.1.2/senteval_cn.zip) </br> |\n| shibing624/sts-sohu2021    | ä¸­æ–‡è¯­ä¹‰åŒ¹é…æ•°æ®é›†ï¼Œ2021æœç‹æ ¡å›­æ–‡æœ¬åŒ¹é…ç®—æ³•å¤§èµ›æ•°æ®é›†                                            | [https://huggingface.co/datasets/shibing624/sts-sohu2021](https://huggingface.co/datasets/shibing624/sts-sohu2021)                                                                                                                                                                                    |\n| ATEC                       | ä¸­æ–‡ATECæ•°æ®é›†ï¼Œèš‚èšé‡‘æœQ-Qpairæ•°æ®é›†                                                 | [ATEC](https://github.com/IceFlameWorm/NLP_Datasets/tree/master/ATEC)                                                                                                                                                                                                                                 |\n| BQ                         | ä¸­æ–‡BQ(Bank Question)æ•°æ®é›†ï¼Œé“¶è¡ŒQ-Qpairæ•°æ®é›†                                      | [BQ](http://icrc.hitsz.edu.cn/info/1037/1162.htm)                                                                                                                                                                                                                                                     |\n| LCQMC                      | ä¸­æ–‡LCQMC(large-scale Chinese question matching corpus)æ•°æ®é›†ï¼ŒQ-Qpairæ•°æ®é›†      | [LCQMC](http://icrc.hitsz.edu.cn/Article/show/171.html)                                                                                                                                                                                                                                               |\n| PAWSX                      | ä¸­æ–‡PAWS(Paraphrase Adversaries from Word Scrambling)æ•°æ®é›†ï¼ŒQ-Qpairæ•°æ®é›†        | [PAWSX](https://arxiv.org/abs/1908.11828)                                                                                                                                                                                                                                                             |\n| STS-B                      | ä¸­æ–‡STS-Bæ•°æ®é›†ï¼Œä¸­æ–‡è‡ªç„¶è¯­è¨€æ¨ç†æ•°æ®é›†ï¼Œä»è‹±æ–‡STS-Bç¿»è¯‘ä¸ºä¸­æ–‡çš„æ•°æ®é›†                                 | [STS-B](https://github.com/pluto-junzeng/CNSD)                                                                                                                                                                                                                                                        |\n\n\nå¸¸ç”¨è‹±æ–‡åŒ¹é…æ•°æ®é›†ï¼š\n\n- è‹±æ–‡åŒ¹é…æ•°æ®é›†ï¼šmulti_nli: https://huggingface.co/datasets/multi_nli\n- è‹±æ–‡åŒ¹é…æ•°æ®é›†ï¼šsnli: https://huggingface.co/datasets/snli\n- https://huggingface.co/datasets/metaeval/cnli\n- https://huggingface.co/datasets/mteb/stsbenchmark-sts\n- https://huggingface.co/datasets/JeremiahZ/simcse_sup_nli\n- https://huggingface.co/datasets/MoritzLaurer/multilingual-NLI-26lang-2mil7\n\n\næ•°æ®é›†ä½¿ç”¨ç¤ºä¾‹ï¼š\n```shell\npip install datasets\n```\n\n```python\nfrom datasets import load_dataset\n\ndataset = load_dataset(\"shibing624/nli_zh\", \"STS-B\") # ATEC or BQ or LCQMC or PAWSX or STS-B\nprint(dataset)\nprint(dataset['test'][0])\n```\n\noutput:\n```shell\nDatasetDict({\n    train: Dataset({\n        features: ['sentence1', 'sentence2', 'label'],\n        num_rows: 5231\n    })\n    validation: Dataset({\n        features: ['sentence1', 'sentence2', 'label'],\n        num_rows: 1458\n    })\n    test: Dataset({\n        features: ['sentence1', 'sentence2', 'label'],\n        num_rows: 1361\n    })\n})\n{'sentence1': 'ä¸€ä¸ªå¥³å­©åœ¨ç»™å¥¹çš„å¤´å‘åšå‘å‹ã€‚', 'sentence2': 'ä¸€ä¸ªå¥³å­©åœ¨æ¢³å¤´ã€‚', 'label': 2}\n```\n\n## Contact\n\n- Issue(å»ºè®®)ï¼š[![GitHub issues](https://img.shields.io/github/issues/shibing624/text2vec.svg)](https://github.com/shibing624/text2vec/issues)\n- é‚®ä»¶æˆ‘ï¼šxuming: xuming624@qq.com\n- å¾®ä¿¡æˆ‘ï¼šåŠ æˆ‘*å¾®ä¿¡å·ï¼šxuming624, å¤‡æ³¨ï¼šå§“å-å…¬å¸-NLP* è¿›NLPäº¤æµç¾¤ã€‚\n\n<img src=\"docs/wechat.jpeg\" width=\"200\" />\n\n\n## Citation\n\nå¦‚æœä½ åœ¨ç ”ç©¶ä¸­ä½¿ç”¨äº†text2vecï¼Œè¯·æŒ‰å¦‚ä¸‹æ ¼å¼å¼•ç”¨ï¼š\n\nAPA:\n```latex\nXu, M. Text2vec: Text to vector toolkit (Version 1.1.2) [Computer software]. https://github.com/shibing624/text2vec\n```\n\nBibTeX:\n```latex\n@misc{Text2vec,\n  author = {Ming Xu},\n  title = {Text2vec: Text to vector toolkit},\n  year = {2023},\n  publisher = {GitHub},\n  journal = {GitHub repository},\n  howpublished = {\\url{https://github.com/shibing624/text2vec}},\n}\n```\n\n## License\n\n\næˆæƒåè®®ä¸º [The Apache License 2.0](LICENSE)ï¼Œå¯å…è´¹ç”¨åšå•†ä¸šç”¨é€”ã€‚è¯·åœ¨äº§å“è¯´æ˜ä¸­é™„åŠ text2vecçš„é“¾æ¥å’Œæˆæƒåè®®ã€‚\n\n\n## Contribute\né¡¹ç›®ä»£ç è¿˜å¾ˆç²—ç³™ï¼Œå¦‚æœå¤§å®¶å¯¹ä»£ç æœ‰æ‰€æ”¹è¿›ï¼Œæ¬¢è¿æäº¤å›æœ¬é¡¹ç›®ï¼Œåœ¨æäº¤ä¹‹å‰ï¼Œæ³¨æ„ä»¥ä¸‹ä¸¤ç‚¹ï¼š\n\n - åœ¨`tests`æ·»åŠ ç›¸åº”çš„å•å…ƒæµ‹è¯•\n - ä½¿ç”¨`python -m pytest -v`æ¥è¿è¡Œæ‰€æœ‰å•å…ƒæµ‹è¯•ï¼Œç¡®ä¿æ‰€æœ‰å•æµ‹éƒ½æ˜¯é€šè¿‡çš„\n\nä¹‹åå³å¯æäº¤PRã€‚\n\n## References\n- [å°†å¥å­è¡¨ç¤ºä¸ºå‘é‡ï¼ˆä¸Šï¼‰ï¼šæ— ç›‘ç£å¥å­è¡¨ç¤ºå­¦ä¹ ï¼ˆsentence embeddingï¼‰](https://www.cnblogs.com/llhthinker/p/10335164.html)\n- [å°†å¥å­è¡¨ç¤ºä¸ºå‘é‡ï¼ˆä¸‹ï¼‰ï¼šæ— ç›‘ç£å¥å­è¡¨ç¤ºå­¦ä¹ ï¼ˆsentence embeddingï¼‰](https://www.cnblogs.com/llhthinker/p/10341841.html)\n- [A Simple but Tough-to-Beat Baseline for Sentence Embeddings[Sanjeev Arora and Yingyu Liang and Tengyu Ma, 2017]](https://openreview.net/forum?id=SyK00v5xx)\n- [å››ç§è®¡ç®—æ–‡æœ¬ç›¸ä¼¼åº¦çš„æ–¹æ³•å¯¹æ¯”[Yves Peirsman]](https://zhuanlan.zhihu.com/p/37104535)\n- [Improvements to BM25 and Language Models Examined](http://www.cs.otago.ac.nz/homepages/andrew/papers/2014-2.pdf)\n- [CoSENTï¼šæ¯”Sentence-BERTæ›´æœ‰æ•ˆçš„å¥å‘é‡æ–¹æ¡ˆ](https://kexue.fm/archives/8847)\n- [è°ˆè°ˆæ–‡æœ¬åŒ¹é…å’Œå¤šè½®æ£€ç´¢](https://zhuanlan.zhihu.com/p/111769969)\n- [Sentence-transformers](https://www.sbert.net/examples/applications/computing-embeddings/README.html)\n- [One Embedder, Any Task: Instruction-Finetuned Text Embeddings](https://arxiv.org/abs/2212.09741)\n"
        },
        {
          "name": "README_EN.md",
          "type": "blob",
          "size": 39.931640625,
          "content": "[**ğŸ‡¨ğŸ‡³ä¸­æ–‡**](https://github.com/shibing624/text2vec/blob/master/README.md) | [**ğŸŒEnglish**](https://github.com/shibing624/text2vec/blob/master/README_EN.md) | [**ğŸ“–æ–‡æ¡£/Docs**](https://github.com/shibing624/text2vec/wiki) | [**ğŸ¤–æ¨¡å‹/Models**](https://huggingface.co/shibing624) \n\n<div align=\"center\">\n  <a href=\"https://github.com/shibing624/text2vec\">\n    <img src=\"https://github.com/shibing624/text2vec/blob/master/docs/t2v-logo.png\" height=\"150\" alt=\"Logo\">\n  </a>\n</div>\n\n-----------------\n\n# Text2vec: Text to Vector\n[![PyPI version](https://badge.fury.io/py/text2vec.svg)](https://badge.fury.io/py/text2vec)\n[![Downloads](https://pepy.tech/badge/text2vec)](https://pepy.tech/project/text2vec)\n[![Contributions welcome](https://img.shields.io/badge/contributions-welcome-brightgreen.svg)](CONTRIBUTING.md)\n[![License Apache 2.0](https://img.shields.io/badge/license-Apache%202.0-blue.svg)](LICENSE)\n[![python_version](https://img.shields.io/badge/Python-3.5%2B-green.svg)](requirements.txt)\n[![GitHub issues](https://img.shields.io/github/issues/shibing624/text2vec.svg)](https://github.com/shibing624/text2vec/issues)\n[![Wechat Group](http://vlog.sfyc.ltd/wechat_everyday/wxgroup_logo.png?imageView2/0/w/60/h/20)](#Contact)\n\n**Text2vec**: Text to Vector, Get Sentence Embeddings. Text vectorization, representing text (including words, sentences, paragraphs) as a vector matrix.\n\n**text2vec** implements Word2Vec, RankBM25, BERT, Sentence-BERT, CoSENT and other text representation and text similarity calculation models, and compares the effects of each model on the text semantic matching (similarity calculation) task.\n\n**Guide**\n- [Feature](#Feature)\n- [Evaluation](#Evaluation)\n- [Install](#install)\n- [Usage](#usage)\n- [Contact](#Contact)\n- [Reference](#reference)\n\n\n# Feature\n### æ–‡æœ¬å‘é‡è¡¨ç¤ºæ¨¡å‹\n- [Word2Vec](https://github.com/shibing624/text2vec/blob/master/text2vec/word2vec.py): large-scale high-quality Chinese [word vector data (8 million Chinese words light weight) through Tencent AI Lab open source version)](https://pan.baidu.com/s/1La4U4XNFe8s5BJqxPQpeiQ) (file name: light_Tencent_AILab_ChineseEmbedding.bin password: tawe) to achieve word vector retrieval, this project realizes word2vec vector representation of sentences (word vector average)\n- [SBERT(Sentence-BERT)](https://github.com/shibing624/text2vec/blob/master/text2vec/sentencebert_model.py): A sentence vector representation model that balances performance and efficiency, and supervises the upper layer during training Classification function, direct sentence vector as cosine when text matching prediction, this project reproduces the training and prediction of Sentence-BERT model based on PyTorch\n- [CoSENT(Cosine Sentence)](https://github.com/shibing624/text2vec/blob/master/text2vec/cosent_model.py): The CoSENT model proposes a sorted loss function to make the training process closer to the prediction, The model convergence speed and effect are better than Sentence-BERT. This project implements the training and prediction of the CoSENT model based on PyTorch\n# Evaluation\n\n### æ–‡æœ¬åŒ¹é…\n\n- è‹±æ–‡åŒ¹é…æ•°æ®é›†çš„è¯„æµ‹ç»“æœï¼š\n\n| Arch | Backbone | Model Name | English-STS-B | \n| :-- | :--- | :--- | :-: |\n| GloVe | glove | Avg_word_embeddings_glove_6B_300d | 61.77 |\n| BERT | bert-base-uncased | BERT-base-cls | 20.29 |\n| BERT | bert-base-uncased | BERT-base-first_last_avg | 59.04 |\n| BERT | bert-base-uncased | BERT-base-first_last_avg-whiten(NLI) | 63.65 |\n| SBERT | sentence-transformers/bert-base-nli-mean-tokens | SBERT-base-nli-cls | 73.65 |\n| SBERT | sentence-transformers/bert-base-nli-mean-tokens | SBERT-base-nli-first_last_avg | 77.96 |\n| SBERT | xlm-roberta-base | paraphrase-multilingual-MiniLM-L12-v2 | 84.42 |\n| CoSENT | bert-base-uncased | CoSENT-base-first_last_avg | 69.93 |\n| CoSENT | sentence-transformers/bert-base-nli-mean-tokens | CoSENT-base-nli-first_last_avg | 79.68 |\n\n- Evaluation results of Chinese matching dataset:\n\n| Arch | Backbone | Model Name | ATEC | BQ | LCQMC | PAWSX | STS-B | Avg | QPS |\n| :-- | :--- | :--- | :-: | :-: | :-: | :-: | :-: | :-: | :-: |\n| CoSENT | hfl/chinese-macbert-base | CoSENT-macbert-base | 50.39 | **72.93** | **79.17** | **60.86** | **80.51** | **68.77**  | 3008 |\n| CoSENT | Langboat/mengzi-bert-base | CoSENT-mengzi-base | **50.52** | 72.27 | 78.69 | 12.89 | 80.15 | 58.90 | 2502 |\n| CoSENT | bert-base-chinese | CoSENT-bert-base | 49.74 | 72.38 | 78.69 | 60.00 | 80.14 | 68.19 | 2653 |\n| SBERT | bert-base-chinese | SBERT-bert-base | 46.36 | 70.36 | 78.72 | 46.86 | 66.41 | 61.74 | 3365 |\n| SBERT | hfl/chinese-macbert-base | SBERT-macbert-base | 47.28 | 68.63 | **79.42** | 55.59 | 64.82 | 63.15 | 2948 |\n| CoSENT | hfl/chinese-roberta-wwm-ext | CoSENT-roberta-ext | **50.81** | **71.45** | **79.31** | **61.56** | **81.13** | **68.85** | - |\n| SBERT | hfl/chinese-roberta-wwm-ext | SBERT-roberta-ext | 48.29 | 69.99 | 79.22 | 44.10 | 72.42 | 62.80 | - |\n\n- Chinese matching evaluation results of the release model of this project:\n\n| Arch | Backbone | Model Name | ATEC  |  BQ   | LCQMC | PAWSX | STS-B |    Avg    | QPS |\n| :-- | :--- | :---- |:-----:|:-----:|:-----:|:-----:|:-----:|:---------:| :-: |\n| Word2Vec | word2vec | w2v-light-tencent-chinese | 20.00 | 31.49 | 59.46 | 2.57  | 55.78 |   33.86   | 23769 |\n| SBERT | xlm-roberta-base | paraphrase-multilingual-MiniLM-L12-v2 | 18.42 | 38.52 | 63.96 | 10.14 | 78.90 |   41.99   | 3138 |\n| CoSENT | hfl/chinese-macbert-base | [shibing624/text2vec-base-chinese](https://huggingface.co/shibing624/text2vec-base-chinese) | 31.93 | 42.67 | 70.16 | 17.21 | 79.30 | **48.25** | 3008 |\n| CoSENT | hfl/chinese-lert-large | [GanymedeNil/text2vec-large-chinese](https://huggingface.co/GanymedeNil/text2vec-large-chinese) | 32.61 | 44.59 | 69.30 | 14.51 | 79.44 |   48.08   | 1046 |\n\n\nEvaluation conclusion:\n- The result values are all using the spearman coefficient\n- The results only use the train training of the data set, and evaluate the performance obtained on the test, without using external data\n- [shibing624/text2vec-base-chinese](https://huggingface.co/shibing624/text2vec-base-chinese) model is trained with CoSENT method, based on MacBERT in Chinese STS-B data training, and in Chinese STS -B test set evaluation reaches SOTA, run [examples/training_sup_text_matching_model.py](https://github.com/shibing624/text2vec/blob/master/examples/training_sup_text_matching_model.py) code to train the model, the model file has been uploaded to huggingface The model library [shibing624/text2vec-base-chinese](https://huggingface.co/shibing624/text2vec-base-chinese), recommended for Chinese semantic matching tasks\n- The `SBERT-macbert-base` model is trained with the SBERT method, and the code can be trained by running [examples/training_sup_text_matching_model.py](https://github.com/shibing624/text2vec/blob/master/examples/training_sup_text_matching_model.py) Model\n- `paraphrase-multilingual-MiniLM-L12-v2` model name is [sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2](https://huggingface.co/sentence-transformers/paraphrase-multilingual-MiniLM-L12 -v2), trained with SBERT, is a multilingual version of the `paraphrase-MiniLM-L12-v2` model, supports Chinese, English, etc.\n- `w2v-light-tencent-chinese` is the Word2Vec model of Tencent word vectors, which is loaded and used by CPU, and is suitable for Chinese literal matching tasks and cold start situations with lack of data\n- Each pre-trained model can be called through transformers, such as MacBERT model: `--model_name hfl/chinese-macbert-base` or roberta model: `--model_name uer/roberta-medium-wwm-chinese-cluecorpussmall`\n- Chinese matching data set download [link below] (#data set)\n- The Chinese matching task experiment shows that the optimal pooling is `first_last_avg`, that is, `EncoderType.FIRST_LAST_AVG` of SentenceModel, which has little difference in prediction effect from the method of `EncoderType.MEAN`\n- Chinese matching evaluation results are reappearing, you can download the Chinese matching dataset to `examples/data`, run [tests/test_model_spearman.py](https://github.com/shibing624/text2vec/blob/master/tests/test_model_spearman. py) code to reproduce the evaluation results\n- The GPU test environment of QPS is Tesla V100 with 32GB of video memory\n# Demo\n\nOfficial Demo: https://www.mulanai.com/product/short_text_sim/\n\nHuggingFace Demo: https://huggingface.co/spaces/shibing624/text2vec\n\n![](docs/hf.png)\n\nrun example: [examples/gradio_demo.py](https://github.com/shibing624/text2vec/blob/master/examples/gradio_demo.py) to see the demo:\n```shell\npython examples/gradio_demo.py\n```\n\n# Install\n```shell\npip install torch # conda install pytorch\npip install -U text2vec\n```\n\nor\n\n```shell\npip install torch # conda install pytorch\npip install -r requirements.txt\n\ngit clone https://github.com/shibing624/text2vec.git\ncd text2vec\npip install --no-deps .\n```\n\n# Usage\n\n## æ–‡æœ¬å‘é‡è¡¨å¾\n\nCompute text vectors based on `pretrained model`:\n\n```zsh\n>>> from text2vec import SentenceModel\n>>> m = SentenceModel()\n>>> m.encode(\"å¦‚ä½•æ›´æ¢èŠ±å‘—ç»‘å®šé“¶è¡Œå¡\")\nEmbedding shape: (768,)\n```\n\nexample: [examples/computing_embeddings_demo.py](https://github.com/shibing624/text2vec/blob/master/examples/computing_embeddings_demo.py)\n\n```python\nimport sys\n\nsys.path.append('..')\nfrom text2vec import SentenceModel\nfrom text2vec import Word2Vec\n\n\ndef compute_emb(model):\n    # Embed a list of sentences\n    sentences = [\n        'å¡',\n        'é“¶è¡Œå¡',\n        'å¦‚ä½•æ›´æ¢èŠ±å‘—ç»‘å®šé“¶è¡Œå¡',\n        'èŠ±å‘—æ›´æ”¹ç»‘å®šé“¶è¡Œå¡',\n        'This framework generates embeddings for each input sentence',\n        'Sentences are passed as a list of string.',\n        'The quick brown fox jumps over the lazy dog.'\n    ]\n    sentence_embeddings = model.encode(sentences)\n    print(type(sentence_embeddings), sentence_embeddings.shape)\n\n    # The result is a list of sentence embeddings as numpy arrays\n    for sentence, embedding in zip(sentences, sentence_embeddings):\n        print(\"Sentence:\", sentence)\n        print(\"Embedding shape:\", embedding.shape)\n        print(\"Embedding head:\", embedding[:10])\n        print()\n\n\nif __name__ == \"__main__\":\n    # ä¸­æ–‡å¥å‘é‡æ¨¡å‹(CoSENT)ï¼Œä¸­æ–‡è¯­ä¹‰åŒ¹é…ä»»åŠ¡æ¨èï¼Œæ”¯æŒfine-tuneç»§ç»­è®­ç»ƒ\n    t2v_model = SentenceModel(\"shibing624/text2vec-base-chinese\")\n    compute_emb(t2v_model)\n\n    # æ”¯æŒå¤šè¯­è¨€çš„å¥å‘é‡æ¨¡å‹ï¼ˆSentence-BERTï¼‰ï¼Œè‹±æ–‡è¯­ä¹‰åŒ¹é…ä»»åŠ¡æ¨èï¼Œæ”¯æŒfine-tuneç»§ç»­è®­ç»ƒ\n    sbert_model = SentenceModel(\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\")\n    compute_emb(sbert_model)\n\n    # ä¸­æ–‡è¯å‘é‡æ¨¡å‹(word2vec)ï¼Œä¸­æ–‡å­—é¢åŒ¹é…ä»»åŠ¡å’Œå†·å¯åŠ¨é€‚ç”¨\n    w2v_model = Word2Vec(\"w2v-light-tencent-chinese\")\n    compute_emb(w2v_model)\n\n```\n\noutput:\n```\n<class 'numpy.ndarray'> (7, 768)\nSentence: å¡\nEmbedding shape: (768,)\n\nSentence: é“¶è¡Œå¡\nEmbedding shape: (768,)\n ... \n```\n\n- The return value `embeddings` is of `numpy.ndarray` type, and the shape is `(sentences_size, model_embedding_size)`. You can choose one of the three models, and the first one is recommended.\n- The `shibing624/text2vec-base-chinese` model is trained by the CoSENT method on the Chinese STS-B dataset, and the model has been uploaded to huggingface\nModel library [shibing624/text2vec-base-chinese](https://huggingface.co/shibing624/text2vec-base-chinese),\nIt is the default model specified by `text2vec.SentenceModel`, which can be called by the above example, or by [transformers library](https://github.com/huggingface/transformers) as shown below,\nThe model is automatically downloaded to the local path: `~/.cache/huggingface/transformers`\n- The `sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2` model is a multilingual sentence vector model of Sentence-BERT,\nSuitable for paraphrase recognition and text matching, the model can be called through `text2vec.SentenceModel` and [sentence-transformers library]((https://github.com/UKPLab/sentence-transformers))\n- `w2v-light-tencent-chinese` is a Word2Vec model loaded by gensim, using the Tencent word vector `Tencent_AILab_ChineseEmbedding.tar.gz` to calculate the word vector of each word, and the sentence vector through the word word\nThe average value of the vector is obtained, and the model is automatically downloaded to the local path: `~/.text2vec/datasets/light_Tencent_AILab_ChineseEmbedding.bin`\n#### Usage (HuggingFace Transformers)\nWithout [text2vec](https://github.com/shibing624/text2vec), you can use the model like this: \n\nFirst, you pass your input through the transformer model, then you have to apply the right pooling-operation on-top of the contextualized word embeddings.\n\nexample: [examples/use_origin_transformers_demo.py](https://github.com/shibing624/text2vec/blob/master/examples/use_origin_transformers_demo.py)\n\n```python\nimport os\nimport torch\nfrom transformers import AutoTokenizer, AutoModel\n\nos.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"\n\n\n# Mean Pooling - Take attention mask into account for correct averaging\ndef mean_pooling(model_output, attention_mask):\n    token_embeddings = model_output[0]  # First element of model_output contains all token embeddings\n    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n\n\n# Load model from HuggingFace Hub\ntokenizer = AutoTokenizer.from_pretrained('shibing624/text2vec-base-chinese')\nmodel = AutoModel.from_pretrained('shibing624/text2vec-base-chinese')\nsentences = ['å¦‚ä½•æ›´æ¢èŠ±å‘—ç»‘å®šé“¶è¡Œå¡', 'èŠ±å‘—æ›´æ”¹ç»‘å®šé“¶è¡Œå¡']\n# Tokenize sentences\nencoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')\n\n# Compute token embeddings\nwith torch.no_grad():\n    model_output = model(**encoded_input)\n# Perform pooling. In this case, max pooling.\nsentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\nprint(\"Sentence embeddings:\")\nprint(sentence_embeddings)\n```\n\n#### Usage (sentence-transformers)\n[sentence-transformers](https://github.com/UKPLab/sentence-transformers) is a popular library to compute dense vector representations for sentences.\n\nInstall sentence-transformers:\n```shell\npip install -U sentence-transformers\n```\nThen load model and predict:\n```python\nfrom sentence_transformers import SentenceTransformer\n\nm = SentenceTransformer(\"shibing624/text2vec-base-chinese\")\nsentences = ['å¦‚ä½•æ›´æ¢èŠ±å‘—ç»‘å®šé“¶è¡Œå¡', 'èŠ±å‘—æ›´æ”¹ç»‘å®šé“¶è¡Œå¡']\n\nsentence_embeddings = m.encode(sentences)\nprint(\"Sentence embeddings:\")\nprint(sentence_embeddings)\n```\n\n#### `Word2Vec` word vector model\nTwo `Word2Vec` word vectors are provided, choose one:\n\n   - Lightweight version of Tencent Word Vector [Baidu Cloud Disk-Password: tawe](https://pan.baidu.com/s/1La4U4XNFe8s5BJqxPQpeiQ) or [Google Cloud Disk](https://drive.google.com/u/ 0/uc?id=1iQo9tBb2NgFOBxx0fA16AZpSgc-bG_Rp&export=download), binary file, 111M, is a simplified high-frequency 143613 words, each word vector is still 200 dimensions (same as the original version), run the program, and automatically download to `~/ .text2vec/datasets/light_Tencent_AILab_ChineseEmbedding.bin`\n   - Tencent word vector - official full volume, 6.78G put: `~/.text2vec/datasets/Tencent_AILab_ChineseEmbedding.txt`, Tencent word vector homepage: https://ai.tencent.com/ailab/nlp/zh/index.html Word vector download address: https://ai.tencent.com/ailab/nlp/en/download.html For more information, see [Tencent Word Vector Introduction-wiki](https://github.com/shibing624/text2vec/wiki/ %E8%85%BE%E8%AE%AF%E8%AF%8D%E5%90%91%E9%87%8F%E4%BB%8B%E7%BB%8D)\n\n## Downstream tasks\n### 1. Sentence similarity calculation\n\nexample: [examples/semantic_text_similarity_demo.py](https://github.com/shibing624/text2vec/blob/master/examples/semantic_text_similarity_demo.py)\n\n```python\nimport sys\n\nsys.path.append('..')\nfrom text2vec import Similarity\n\n# Two lists of sentences\nsentences1 = ['å¦‚ä½•æ›´æ¢èŠ±å‘—ç»‘å®šé“¶è¡Œå¡',\n              'The cat sits outside',\n              'A man is playing guitar',\n              'The new movie is awesome']\n\nsentences2 = ['èŠ±å‘—æ›´æ”¹ç»‘å®šé“¶è¡Œå¡',\n              'The dog plays in the garden',\n              'A woman watches TV',\n              'The new movie is so great']\n\nsim_model = Similarity()\nfor i in range(len(sentences1)):\n    for j in range(len(sentences2)):\n        score = sim_model.get_score(sentences1[i], sentences2[j])\n        print(\"{} \\t\\t {} \\t\\t Score: {:.4f}\".format(sentences1[i], sentences2[j], score))\n```\n\noutput:\n```shell\nå¦‚ä½•æ›´æ¢èŠ±å‘—ç»‘å®šé“¶è¡Œå¡ \t\t èŠ±å‘—æ›´æ”¹ç»‘å®šé“¶è¡Œå¡ \t\t Score: 0.9477\nå¦‚ä½•æ›´æ¢èŠ±å‘—ç»‘å®šé“¶è¡Œå¡ \t\t The dog plays in the garden \t\t Score: -0.1748\nå¦‚ä½•æ›´æ¢èŠ±å‘—ç»‘å®šé“¶è¡Œå¡ \t\t A woman watches TV \t\t Score: -0.0839\nå¦‚ä½•æ›´æ¢èŠ±å‘—ç»‘å®šé“¶è¡Œå¡ \t\t The new movie is so great \t\t Score: -0.0044\nThe cat sits outside \t\t èŠ±å‘—æ›´æ”¹ç»‘å®šé“¶è¡Œå¡ \t\t Score: -0.0097\nThe cat sits outside \t\t The dog plays in the garden \t\t Score: 0.1908\nThe cat sits outside \t\t A woman watches TV \t\t Score: -0.0203\nThe cat sits outside \t\t The new movie is so great \t\t Score: 0.0302\nA man is playing guitar \t\t èŠ±å‘—æ›´æ”¹ç»‘å®šé“¶è¡Œå¡ \t\t Score: -0.0010\nA man is playing guitar \t\t The dog plays in the garden \t\t Score: 0.1062\nA man is playing guitar \t\t A woman watches TV \t\t Score: 0.0055\nA man is playing guitar \t\t The new movie is so great \t\t Score: 0.0097\nThe new movie is awesome \t\t èŠ±å‘—æ›´æ”¹ç»‘å®šé“¶è¡Œå¡ \t\t Score: 0.0302\nThe new movie is awesome \t\t The dog plays in the garden \t\t Score: -0.0160\nThe new movie is awesome \t\t A woman watches TV \t\t Score: 0.1321\nThe new movie is awesome \t\t The new movie is so great \t\t Score: 0.9591\n```\n\n> Sentence cosine similarity value `score` ranges from [-1, 1], the larger the value, the more similar it is.\n\n### 2. Text matching search\n\nGenerally, the text that is most similar to the query is found in the document candidate set, which is often used in tasks such as question similarity matching and text similarity retrieval in QA scenarios.\n\n\nexample: [examples/semantic_search_demo.py](https://github.com/shibing624/text2vec/blob/master/examples/semantic_search_demo.py)\n\n```python\nimport sys\n\nsys.path.append('..')\nfrom text2vec import SentenceModel, cos_sim, semantic_search\n\nembedder = SentenceModel()\n\n# Corpus with example sentences\ncorpus = [\n    'èŠ±å‘—æ›´æ”¹ç»‘å®šé“¶è¡Œå¡',\n    'æˆ‘ä»€ä¹ˆæ—¶å€™å¼€é€šäº†èŠ±å‘—',\n    'A man is eating food.',\n    'A man is eating a piece of bread.',\n    'The girl is carrying a baby.',\n    'A man is riding a horse.',\n    'A woman is playing violin.',\n    'Two men pushed carts through the woods.',\n    'A man is riding a white horse on an enclosed ground.',\n    'A monkey is playing drums.',\n    'A cheetah is running behind its prey.'\n]\ncorpus_embeddings = embedder.encode(corpus)\n\n# Query sentences:\nqueries = [\n    'å¦‚ä½•æ›´æ¢èŠ±å‘—ç»‘å®šé“¶è¡Œå¡',\n    'A man is eating pasta.',\n    'Someone in a gorilla costume is playing a set of drums.',\n    'A cheetah chases prey on across a field.']\n\nfor query in queries:\n    query_embedding = embedder.encode(query)\n    hits = semantic_search(query_embedding, corpus_embeddings, top_k=5)\n    print(\"\\n\\n======================\\n\\n\")\n    print(\"Query:\", query)\n    print(\"\\nTop 5 most similar sentences in corpus:\")\n    hits = hits[0]  # Get the hits for the first query\n    for hit in hits:\n        print(corpus[hit['corpus_id']], \"(Score: {:.4f})\".format(hit['score']))\n```\noutput:\n```shell\nQuery: å¦‚ä½•æ›´æ¢èŠ±å‘—ç»‘å®šé“¶è¡Œå¡\nTop 5 most similar sentences in corpus:\nèŠ±å‘—æ›´æ”¹ç»‘å®šé“¶è¡Œå¡ (Score: 0.9477)\næˆ‘ä»€ä¹ˆæ—¶å€™å¼€é€šäº†èŠ±å‘— (Score: 0.3635)\nA man is eating food. (Score: 0.0321)\nA man is riding a horse. (Score: 0.0228)\nTwo men pushed carts through the woods. (Score: 0.0090)\n\n======================\nQuery: A man is eating pasta.\nTop 5 most similar sentences in corpus:\nA man is eating food. (Score: 0.6734)\nA man is eating a piece of bread. (Score: 0.4269)\nA man is riding a horse. (Score: 0.2086)\nA man is riding a white horse on an enclosed ground. (Score: 0.1020)\nA cheetah is running behind its prey. (Score: 0.0566)\n\n======================\nQuery: Someone in a gorilla costume is playing a set of drums.\nTop 5 most similar sentences in corpus:\nA monkey is playing drums. (Score: 0.8167)\nA cheetah is running behind its prey. (Score: 0.2720)\nA woman is playing violin. (Score: 0.1721)\nA man is riding a horse. (Score: 0.1291)\nA man is riding a white horse on an enclosed ground. (Score: 0.1213)\n\n======================\nQuery: A cheetah chases prey on across a field.\nTop 5 most similar sentences in corpus:\nA cheetah is running behind its prey. (Score: 0.9147)\nA monkey is playing drums. (Score: 0.2655)\nA man is riding a horse. (Score: 0.1933)\nA man is riding a white horse on an enclosed ground. (Score: 0.1733)\nA man is eating food. (Score: 0.0329)\n```\n\n\n## Downstream task support library\n**similarities library [recommended]**\n\nFor text similarity calculation and text matching search tasks, it is recommended to use [similarities library](https://github.com/shibing624/similarities), which is compatible with the release of this project\nWord2vec, SBERT, and Cosent semantic matching models also support literal dimension similarity calculations, matching search algorithms, and text and images.\n\nInstall:\n```pip install -U similarities```\n\nSentence similarity calculation:\n```python\nfrom similarities import Similarity\n\nm = Similarity()\nr = m.similarity('å¦‚ä½•æ›´æ¢èŠ±å‘—ç»‘å®šé“¶è¡Œå¡', 'èŠ±å‘—æ›´æ”¹ç»‘å®šé“¶è¡Œå¡')\nprint(f\"similarity score: {float(r)}\")  # similarity score: 0.855146050453186\n```\n\n# Models\n\n## CoSENT model\n\nCoSENT (Cosine Sentence) text matching model, improved the sentence vector scheme of CosineRankLoss on Sentence-BERT\n\n\nNetwork structure:\n\nTraining:\n\n<img src=\"docs/cosent_train.png\" width=\"300\" />\n\n\nInference:\n\n<img src=\"docs/inference.png\" width=\"300\" />\n\n#### CoSENT Supervised Model\nTrain and predict CoSENT model:\n\n- Train and evaluate the `CoSENT` model on the Chinese STS-B dataset\n\nexample: [examples/training_sup_text_matching_model.py](https://github.com/shibing624/text2vec/blob/master/examples/training_sup_text_matching_model.py)\n\n```shell\ncd examples\npython training_sup_text_matching_model.py --model_arch cosent --do_train --do_predict --num_epochs 10 --model_name hfl/chinese-macbert-base --output_dir ./outputs/STS-B-cosent\n```\n\n- Train and evaluate the `CoSENT` model on the Ant Financial matching dataset ATEC\n\nSupport the use of these Chinese matching datasets: 'ATEC', 'STS-B', 'BQ', 'LCQMC', 'PAWSX', for details refer to HuggingFace datasets [https://huggingface.co/datasets/shibing624/nli_zh] (https://huggingface.co/datasets/shibing624/nli_zh)\n```shell\npython training_sup_text_matching_model.py --task_name ATEC --model_arch cosent --do_train --do_predict --num_epochs 10 --model_name hfl/chinese-macbert-base --output_dir ./outputs/ATEC-cosent\n```\n\n- Train the model on our own Chinese dataset\n\nexample: [examples/training_sup_text_matching_model_selfdata.py](https://github.com/shibing624/text2vec/blob/master/examples/training_sup_text_matching_model_selfdata.py)\n\n```shell\npython training_sup_text_matching_model_selfdata.py --do_train --do_predict\n```\n\n- Train and evaluate the `CoSENT` model on the English STS-B dataset\n\nexample: [examples/training_sup_text_matching_model_en.py](https://github.com/shibing624/text2vec/blob/master/examples/training_sup_text_matching_model_en.py)\n\n```shell\ncd examples\npython training_sup_text_matching_model_en.py --model_arch cosent --do_train --do_predict --num_epochs 10 --model_name bert-base-uncased  --output_dir ./outputs/STS-B-en-cosent\n```\n\n#### CoSENT Unsupervised Model\n- Train the `CoSENT` model on the English NLI dataset and evaluate the effect on the STS-B test set\n\nexample: [examples/training_unsup_text_matching_model_en.py](https://github.com/shibing624/text2vec/blob/master/examples/training_unsup_text_matching_model_en.py)\n\n```shell\ncd examples\npython training_unsup_text_matching_model_en.py --model_arch cosent --do_train --do_predict --num_epochs 10 --model_name bert-base-uncased --output_dir ./outputs/STS-B-en-unsup-cosent\n```\n\n\n## Sentence-BERT model\n\nSentence-BERT text matching model, representational sentence vector representation scheme\n\nNetwork structure:\n\nTraining:\n\n<img src=\"docs/sbert_train.png\" width=\"300\" />\n\n\nInference:\n\n<img src=\"docs/sbert_inference.png\" width=\"300\" />\n\n#### SentenceBERT Supervised Model\n- Train and evaluate the `SBERT` model on the Chinese STS-B dataset\n\nexample: [examples/training_sup_text_matching_model.py](https://github.com/shibing624/text2vec/blob/master/examples/training_sup_text_matching_model.py)\n\n```shell\ncd examples\npython training_sup_text_matching_model.py --model_arch sentencebert --do_train --do_predict --num_epochs 10 --model_name hfl/chinese-macbert-base --output_dir ./outputs/STS-B-sbert\n```\n- Train and evaluate the `SBERT` model on the English STS-B dataset\n\nexample: [examples/training_sup_text_matching_model_en.py](https://github.com/shibing624/text2vec/blob/master/examples/training_sup_text_matching_model_en.py)\n\n```shell\ncd examples\npython training_sup_text_matching_model_en.py --model_arch sentencebert --do_train --do_predict --num_epochs 10 --model_name bert-base-uncased --output_dir ./outputs/STS-B-en-sbert\n```\n\n#### SentenceBERT Unsupervised Model\n- Train the `SBERT` model on the English NLI dataset and evaluate the effect on the STS-B test set\n\nexample: [examples/training_unsup_text_matching_model_en.py](https://github.com/shibing624/text2vec/blob/master/examples/training_unsup_text_matching_model_en.py)\n\n```shell\ncd examples\npython training_unsup_text_matching_model_en.py --model_arch sentencebert --do_train --do_predict --num_epochs 10 --model_name bert-base-uncased --output_dir ./outputs/STS-B-en-unsup-sbert\n```\n\n## BERT-Match model\nBERT text matching model, native BERT matching network structure, interactive sentence vector matching model\n\nNetwork structure:\n\nTraining and inference:\n\n<img src=\"docs/bert-fc-train.png\" width=\"300\" />\nThe training script is the same as above [examples/training_sup_text_matching_model.py](https://github.com/shibing624/text2vec/blob/master/examples/training_sup_text_matching_model.py).\n\n\n## Model Distillation\n\nSince the model trained by text2vec can be loaded using the [sentence-transformers](https://github.com/UKPLab/sentence-transformers) library, its model distillation method [distillation](https://github.com/ UKPLab/sentence-transformers/tree/master/examples/training/distillation).\n\n1. Model dimension reduction, refer to [dimensionality_reduction.py](https://github.com/UKPLab/sentence-transformers/blob/master/examples/training/distillation/dimensionality_reduction.py) use PCA to reduce the dimensionality of the model output embedding, It can reduce the storage pressure of vector retrieval databases such as milvus and slightly improve the model effect.\n2. For model distillation, refer to [model_distillation.py](https://github.com/UKPLab/sentence-transformers/blob/master/examples/training/distillation/model_distillation.py) and use the distillation method to distill the Teacher model to In the student model with fewer layers, the prediction speed of the model can be greatly improved under the condition of weighing the effect.\n\n## Model Deployment\n\nProvide two deployment models and methods of building services: 1) Building gRPC services based on Jina [recommended]; 2) Building native Http services based on FastAPI.\n\n### Jina Service\nIt adopts C/S mode to build high-performance services, supports docker cloud native, gRPC/HTTP/WebSocket, supports simultaneous prediction of multiple models, and multi-GPU processing.\n\n- Install:\n```pip install jina```\n\n- Start the service:\n\nexample: [examples/jina_server_demo.py](examples/jina_server_demo.py)\n```python\nfrom jina import Flow\n\nport = 50001\nf = Flow(port=port).add(\n    uses='jinahub://Text2vecEncoder',\n    uses_with={'model_name': 'shibing624/text2vec-base-chinese'}\n)\n\nwith f:\n    # backend server forever\n    f.block()\n```\n\nThe model prediction method (executor) has been uploaded to [JinaHub](https://hub.jina.ai/executor/eq45c9uq), which includes docker and k8s deployment methods.\n\n- call service:\n\n\n```python\nfrom jina import Client\nfrom docarray import Document, DocumentArray\n\nport = 50001\n\nc = Client(port=port)\n\ndata = ['å¦‚ä½•æ›´æ¢èŠ±å‘—ç»‘å®šé“¶è¡Œå¡',\n        'èŠ±å‘—æ›´æ”¹ç»‘å®šé“¶è¡Œå¡']\nprint(\"data:\", data)\nprint('data embs:')\nr = c.post('/', inputs=DocumentArray([Document(text='å¦‚ä½•æ›´æ¢èŠ±å‘—ç»‘å®šé“¶è¡Œå¡'), Document(text='èŠ±å‘—æ›´æ”¹ç»‘å®šé“¶è¡Œå¡')]))\nprint(r.embeddings)\n```\n\nSee example for batch call method: [examples/jina_client_demo.py](https://github.com/shibing624/text2vec/blob/master/examples/jina_client_demo.py)\n\n\n### Fast API service\n\n- Install:\n```pip install fastapi uvicorn```\n\n- Start the service:\n\nexample: [examples/fastapi_server_demo.py](https://github.com/shibing624/text2vec/blob/master/examples/fastapi_server_demo.py)\n```shell\ncd examples\npython fastapi_server_demo.py\n```\n\n- è°ƒç”¨æœåŠ¡ï¼š\n```shell\ncurl -X 'GET' \\\n  'http://0.0.0.0:8001/emb?q=hello' \\\n  -H 'accept: application/json'\n```\n\n\n## Dataset\n\n- The data set of this project release:\n\n| Dataset           | Introduce                                                           | Download Link                                                                                                                                                                                                                                                                                         |\n|:------------------|:--------------------------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| shibing624/nli_zh | ä¸­æ–‡è¯­ä¹‰åŒ¹é…æ•°æ®é›†ï¼Œæ•´åˆäº†ä¸­æ–‡ATECã€BQã€LCQMCã€PAWSXã€STS-Bå…±5ä¸ªä»»åŠ¡çš„æ•°æ®é›†                   | [https://huggingface.co/datasets/shibing624/nli_zh](https://huggingface.co/datasets/shibing624/nli_zh) </br> or </br> [ç™¾åº¦ç½‘ç›˜(æå–ç :qkt6)](https://pan.baidu.com/s/1d6jSiU1wHQAEMWJi7JJWCQ) </br> or </br> [github](https://github.com/shibing624/text2vec/releases/download/1.1.2/senteval_cn.zip) </br> |\n| ATEC              | ä¸­æ–‡ATECæ•°æ®é›†ï¼Œèš‚èšé‡‘æœQ-Qpairæ•°æ®é›†                                            | [ATEC](https://github.com/IceFlameWorm/NLP_Datasets/tree/master/ATEC)|\n| BQ                | ä¸­æ–‡BQ(Bank Question)æ•°æ®é›†ï¼Œé“¶è¡ŒQ-Qpairæ•°æ®é›†                                 | [BQ](http://icrc.hitsz.edu.cn/info/1037/1162.htm)|\n| LCQMC              | ä¸­æ–‡LCQMC(large-scale Chinese question matching corpus)æ•°æ®é›†ï¼ŒQ-Qpairæ•°æ®é›† | [LCQMC](http://icrc.hitsz.edu.cn/Article/show/171.html)|\n| PAWSX              | ä¸­æ–‡PAWS(Paraphrase Adversaries from Word Scrambling)æ•°æ®é›†ï¼ŒQ-Qpairæ•°æ®é›†   | [PAWSX](https://arxiv.org/abs/1908.11828)|\n| STS-B              | ä¸­æ–‡STS-Bæ•°æ®é›†ï¼Œä¸­æ–‡è‡ªç„¶è¯­è¨€æ¨ç†æ•°æ®é›†ï¼Œä»è‹±æ–‡STS-Bç¿»è¯‘ä¸ºä¸­æ–‡çš„æ•°æ®é›†                              | [STS-B](https://github.com/pluto-junzeng/CNSD)|\n\nChinese semantic matching dataset `shibing624/nli_zh`, including [ATEC](https://github.com/IceFlameWorm/NLP_Datasets/tree/master/ATEC), [BQ](http://icrc.hitsz.edu.cn /info/1037/1162.htm),\n[LCQMC](http://icrc.hitsz.edu.cn/Article/show/171.html), [PAWSX](https://arxiv.org/abs/1908.11828), [STS-B](https: //github.com/pluto-junzeng/CNSD) a total of 5 tasks.\nYou can download it yourself from the link corresponding to the dataset, or you can download it from [Baidu Netdisk (extraction code: qkt6)](https://pan.baidu.com/s/1d6jSiU1wHQAEMWJi7JJWCQ).\nAmong them, the senteval_cn directory is a summary of the evaluation data set, and senteval_cn.zip is the packaging of the senteval directory, whichever is better.\n\n\nDataset usage example:\n```shell\npip install datasets\n```\n\n```python\nfrom datasets import load_dataset\n\ndataset = load_dataset(\"shibing624/nli_zh\", \"STS-B\") # ATEC or BQ or LCQMC or PAWSX or STS-B\nprint(dataset)\nprint(dataset['test'][0])\n```\n\noutput:\n```shell\nDatasetDict({\n    train: Dataset({\n        features: ['sentence1', 'sentence2', 'label'],\n        num_rows: 5231\n    })\n    validation: Dataset({\n        features: ['sentence1', 'sentence2', 'label'],\n        num_rows: 1458\n    })\n    test: Dataset({\n        features: ['sentence1', 'sentence2', 'label'],\n        num_rows: 1361\n    })\n})\n{'sentence1': 'ä¸€ä¸ªå¥³å­©åœ¨ç»™å¥¹çš„å¤´å‘åšå‘å‹ã€‚', 'sentence2': 'ä¸€ä¸ªå¥³å­©åœ¨æ¢³å¤´ã€‚', 'label': 2}\n```\n\n\n<details>\n<summary>Introduction to text vector methods</summary>\n\n# Question\nWhat about text vector representation? Which model is better for text matching tasks?\n\nThe success of many NLP tasks is inseparable from training high-quality and effective text representation vectors. Especially text semantic matching (Semantic Textual Similarity, such as paraphrase detection, QA question pair matching), text vector retrieval (Dense Text Retrieval) and other tasks.\n# Solution\n### Traditional Approach: Feature-Based Matching\n\n- Based on TF-IDF, BM25, Jaccord, SimHash, LDA and other algorithms to extract the vocabulary, topic and other features of the two texts, and then use the machine learning model (LR, xgboost) to train the classification model\n- Pros: better interpretability\n- Disadvantages: Rely on manual search for features, generalization ability is average, and due to the limitation of the number of features, the effect of the model is relatively average\n\nRepresentative model:\n- BM25\n\nThe BM25 algorithm calculates the matching score between the fields of the candidate sentence by the degree of coverage of the qurey field. The candidate with a higher score has a better matching degree with the query, and it mainly solves the problem of similarity at the lexical level.\n\n### Deep Methods: Representation-Based Matching\n- Based on the representation matching method, the two texts are processed separately in the initial stage, and the deep neural network is used to encode (encode) to obtain the text representation (embedding), and then perform similarity calculation on the two representations to obtain two text similarity\n- Advantages: The BERT-based model has achieved good performance in text representation and text matching tasks through supervised Fine-tune\n- Disadvantage: The sentence vector derived by BERT itself (without Fine-tune, averaging all word vectors) is of low quality, even inferior to the result of Glove, so it is difficult to reflect the semantic similarity of two sentences\n\n> The main reasons are:\n>\n> 1.BERT tends to encode all sentences into a small spatial region, which makes most sentence pairs have high similarity scores, even those semantically completely unrelated sentence pairs.\n>\n> 2. The aggregation phenomenon represented by the BERT sentence vector is related to the high-frequency words in the sentence. Specifically, when the sentence vector is calculated by means of the average word vector, the word vector of those high-frequency words will dominate the sentence vector, making it difficult to reflect its original semantics. When some high-frequency words are removed when calculating sentence vectors, the aggregation phenomenon can be alleviated to a certain extent, but the representation ability will be reduced.\n\nModelsï¼š\n\n- [DSSM(2013)](https://posenhuang.github.io/papers/cikm2013_DSSM_fullversion.pdf)\n- [CDSSM(2014)](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/www2014_cdssm_p07.pdf)\n- [ARC I(2014)](https://arxiv.org/pdf/1503.03244.pdf)\n- [Siamese Network(2016)](https://www.aclweb.org/anthology/W16-1617.pdf)\n- [InferSent(2017)](https://arxiv.org/pdf/1705.02364.pdf)\n- [BERT(2018)](https://arxiv.org/pdf/1810.04805.pdf)\n- [Sentence-BERT(2019)](https://arxiv.org/abs/1908.10084)\n- [BERT-flow(2020)](https://arxiv.org/abs/2011.05864)\n- [SimCSE(2021)](https://arxiv.org/abs/2104.08821)\n- [ConSERT(2021)](https://aclanthology.org/2021.acl-long.393/)\n- [CoSENT(2022)](https://kexue.fm/archives/8847)\n\nSince the BERT model brought about earth-shaking changes in the NLP industry in 2018, the models before 2018 will not be discussed and compared here (if you are interested in learning, you can refer to the open source [MatchZoo] of the Chinese Academy of Sciences (https://github.com /NTMC-Community/MatchZoo) and [MatchZoo-py](https://github.com/NTMC-Community/MatchZoo-py)).\n\nTherefore, this project mainly investigates the following vector representation models that are better than native BERT and suitable for text matching: Sentence-BERT (2019), BERT-flow (2020), SimCSE (2021), CoSENT (2022).\n\n### Deep Approach: Interaction-Based Matching\n\n- Based on the interactive matching method, it is believed that calculating the similarity of the text in the final stage will be too dependent on the quality of the text representation, and will also lose the basic text features (such as morphology, syntax, etc.), so it is proposed to match the text as early as possible. Features interact, capture more basic features, and finally calculate a matching score based on these basic matching features at a high level\n- Advantages: end-to-end processing of interaction-based matching model, good effect\n- Disadvantage: The input requirement of this type of model (Cross-Encoder) is two sentences, and the output is the similarity value of the sentence pair. The model will not generate sentence embedding (sentence embedding), and we cannot input a single sentence to the model . Therefore, such models are not practical for tasks that require text vector representations\n\n\nModels:\n\n- [ARC II(2014)](https://arxiv.org/pdf/1503.03244.pdf)\n- [MV-LSTM(2015)](https://arxiv.org/pdf/1511.08277.pdf)\n- [MatchPyramid(2016)](https://arxiv.org/pdf/1602.06359.pdf)\n- [DRMM(2016)](https://www.bigdatalab.ac.cn/~gjf/papers/2016/CIKM2016a_guo.pdf)\n- [Conv-KNRM(2018)](https://www.cs.cmu.edu/~zhuyund/papers/WSDM_2018_Dai.pdf)\n- [RE2(2019)](https://www.aclweb.org/anthology/P19-1465.pdf)\n- [Keyword-BERT(2020)](https://arxiv.org/ftp/arxiv/papers/2003/2003.11516.pdf)\n\nCross-Encoder is suitable for vector retrieval fine sorting.\n\n</details>\n\n\n\n# Contact\n\n- Issue (suggestion): [![GitHub issues](https://img.shields.io/github/issues/shibing624/text2vec.svg)](https://github.com/shibing624/text2vec/issues)\n- Email me: xuming: xuming624@qq.com\n- WeChat me: Add me *Wechat ID: xuming624, Remarks: Name-Company-NLP* Enter the NLP exchange group.\n\n<img src=\"docs/wechat.jpeg\" width=\"200\" />\n\n\n# Citation\n\nIf you use text2vec in your research, please cite it in the following format:\n\nAPA:\n```latex\nXu, M. Text2vec: Text to vector toolkit (Version 1.1.2) [Computer software]. https://github.com/shibing624/text2vec\n```\n\nBibTeX:\n```latex\n@misc{Text2vec,\n  author = {Xu, Ming},\n  title = {Text2vec: Text to vector toolkit},\n  year = {2022},\n  publisher = {GitHub},\n  journal = {GitHub repository},\n  howpublished = {\\url{https://github.com/shibing624/text2vec}},\n}\n```\n\n# License\nThe authorization agreement is [The Apache License 2.0](LICENSE), which can be used for commercial purposes free of charge. Please attach text2vec's link and license agreement in the product description.\n\n\n# Contribute\nThe project code is still very rough. If you have improved the code, you are welcome to submit it back to this project. Before submitting, please pay attention to the following two points:\n\n  - Add corresponding unit tests in `tests`\n  - Use `python -m pytest -v` to run all unit tests to ensure that all unit tests are passed\n\nThen you can submit a PR.\n\n# Reference\n- [å°†å¥å­è¡¨ç¤ºä¸ºå‘é‡ï¼ˆä¸Šï¼‰ï¼šæ— ç›‘ç£å¥å­è¡¨ç¤ºå­¦ä¹ ï¼ˆsentence embeddingï¼‰](https://www.cnblogs.com/llhthinker/p/10335164.html)\n- [å°†å¥å­è¡¨ç¤ºä¸ºå‘é‡ï¼ˆä¸‹ï¼‰ï¼šæ— ç›‘ç£å¥å­è¡¨ç¤ºå­¦ä¹ ï¼ˆsentence embeddingï¼‰](https://www.cnblogs.com/llhthinker/p/10341841.html)\n- [A Simple but Tough-to-Beat Baseline for Sentence Embeddings[Sanjeev Arora and Yingyu Liang and Tengyu Ma, 2017]](https://openreview.net/forum?id=SyK00v5xx)\n- [å››ç§è®¡ç®—æ–‡æœ¬ç›¸ä¼¼åº¦çš„æ–¹æ³•å¯¹æ¯”[Yves Peirsman]](https://zhuanlan.zhihu.com/p/37104535)\n- [Improvements to BM25 and Language Models Examined](http://www.cs.otago.ac.nz/homepages/andrew/papers/2014-2.pdf)\n- [CoSENTï¼šæ¯”Sentence-BERTæ›´æœ‰æ•ˆçš„å¥å‘é‡æ–¹æ¡ˆ](https://kexue.fm/archives/8847)\n- [è°ˆè°ˆæ–‡æœ¬åŒ¹é…å’Œå¤šè½®æ£€ç´¢](https://zhuanlan.zhihu.com/p/111769969)\n- [Sentence-transformers](https://www.sbert.net/examples/applications/computing-embeddings/README.html)\n"
        },
        {
          "name": "docs",
          "type": "tree",
          "content": null
        },
        {
          "name": "examples",
          "type": "tree",
          "content": null
        },
        {
          "name": "requirements.txt",
          "type": "blob",
          "size": 0.0703125,
          "content": "jieba>=0.39\nloguru\ntransformers>=4.6.0\ndatasets\ntqdm\nscikit-learn\npandas"
        },
        {
          "name": "setup.py",
          "type": "blob",
          "size": 1.6142578125,
          "content": "# -*- coding: utf-8 -*-\nimport sys\n\nfrom setuptools import setup, find_packages\n\n# Avoids IDE errors, but actual version is read from version.py\n__version__ = \"\"\nexec(open('text2vec/version.py').read())\n\nif sys.version_info < (3,):\n    sys.exit('Sorry, Python3 is required.')\n\nwith open('README.md', 'r', encoding='utf-8') as f:\n    readme = f.read()\n\nsetup(\n    name='text2vec',\n    version=__version__,\n    description='Text to vector Tool, encode text',\n    long_description=readme,\n    long_description_content_type='text/markdown',\n    author='XuMing',\n    author_email='xuming624@qq.com',\n    url='https://github.com/shibing624/text2vec',\n    license=\"Apache License 2.0\",\n    zip_safe=False,\n    python_requires=\">=3.6.0\",\n    entry_points={\"console_scripts\": [\"text2vec = text2vec.cli:main\"]},\n    classifiers=[\n        \"Development Status :: 5 - Production/Stable\",\n        \"Intended Audience :: Developers\",\n        \"Intended Audience :: Education\",\n        \"Intended Audience :: Science/Research\",\n        \"License :: OSI Approved :: Apache Software License\",\n        \"Operating System :: OS Independent\",\n        \"Programming Language :: Python :: 3\",\n        \"Topic :: Scientific/Engineering :: Artificial Intelligence\",\n    ],\n    keywords='word embedding,text2vec,Chinese Text Similarity Calculation Tool,similarity,word2vec',\n    install_requires=[\n        \"jieba\",\n        \"loguru\",\n        \"transformers\",\n        \"datasets\",\n        \"tqdm\",\n        \"scikit-learn\",\n        \"pandas\",\n    ],\n    packages=find_packages(exclude=['tests']),\n    package_dir={'text2vec': 'text2vec'},\n    package_data={'text2vec': ['*.*', 'data/*.txt']}\n)\n"
        },
        {
          "name": "tests",
          "type": "tree",
          "content": null
        },
        {
          "name": "text2vec",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}