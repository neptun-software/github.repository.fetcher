{
  "metadata": {
    "timestamp": 1736559558828,
    "page": 169,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjE3MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "shibing624/text2vec",
      "stars": 4579,
      "defaultBranch": "master",
      "files": [
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 1.2001953125,
          "content": "# Byte-compiled / optimized / DLL files\n__pycache__/\n*.py[cod]\n*$py.class\n\n# C extensions\n*.so\n\n# Distribution / packaging\n.Python\nbuild/\ndevelop-eggs/\ndist/\ndownloads/\neggs/\n.eggs/\nlib/\nlib64/\nparts/\nsdist/\nvar/\nwheels/\n*.egg-info/\n.installed.cfg\n*.egg\nMANIFEST\n\n# PyInstaller\n#  Usually these files are written by a python script from a template\n#  before PyInstaller builds the exe, so as to inject date/other infos into it.\n*.manifest\n*.spec\n\n# Installer logs\npip-log.txt\npip-delete-this-directory.txt\n\n# Unit test / coverage reports\nhtmlcov/\n.tox/\n.coverage\n.coverage.*\n.cache\nnosetests.xml\ncoverage.xml\n*.cover\n.hypothesis/\n.pytest_cache/\n\n# Translations\n*.mo\n*.pot\n\n# Django stuff:\n*.log\nlocal_settings.py\ndb.sqlite3\n\n# Flask stuff:\ninstance/\n.webassets-cache\n\n# Scrapy stuff:\n.scrapy\n\n# Sphinx documentation\ndocs/_build/\n\n# PyBuilder\ntarget/\n\n# Jupyter Notebook\n.ipynb_checkpoints\n\n# pyenv\n.python-version\n\n# celery beat schedule file\ncelerybeat-schedule\n\n# SageMath parsed files\n*.sage.py\n\n# Environments\n.env\n.venv\nenv/\nvenv/\nENV/\nenv.bak/\nvenv.bak/\n\n# Spyder project settings\n.spyderproject\n.spyproject\n\n# Rope project settings\n.ropeproject\n\n# mkdocs documentation\n/site\n\n# mypy\n.mypy_cache/\n.idea\n*.DS_Store\n.DS_Store"
        },
        {
          "name": "CITATION.cff",
          "type": "blob",
          "size": 0.3037109375,
          "content": "cff-version: 1.2.0\nmessage: \"If you use this software, please cite it as below.\"\nauthors:\n  - family-names: Xu\n    given-names: Ming\n    orcid: https://orcid.org/0000-0003-3402-7159\ntitle: \"Text2vec: Text to vector toolkit\"\nversion: 1.1.3\ndate-released: 2022-02-27\nurl: \"https://github.com/shibing624/text2vec\"\n"
        },
        {
          "name": "CONTRIBUTING.md",
          "type": "blob",
          "size": 0.5029296875,
          "content": "# Contributing\n\nWe are happy to accept your contributions to make `text2vec` better and more awesome! To avoid unnecessary work on either\nside, please stick to the following process:\n\n1. Check if there is already [an issue](https://github.com/shibing624/text2vec/issues) for your concern.\n2. If there is not, open a new one to start a discussion. We hate to close finished PRs!\n3. If we decide your concern needs code changes, we would be happy to accept a pull request. Please consider the\ncommit guidelines below."
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 11.0908203125,
          "content": "                                 Apache License\n                           Version 2.0, January 2004\n                        http://www.apache.org/licenses/\n\n   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n\n   1. Definitions.\n\n      \"License\" shall mean the terms and conditions for use, reproduction,\n      and distribution as defined by Sections 1 through 9 of this document.\n\n      \"Licensor\" shall mean the copyright owner or entity authorized by\n      the copyright owner that is granting the License.\n\n      \"Legal Entity\" shall mean the union of the acting entity and all\n      other entities that control, are controlled by, or are under common\n      control with that entity. For the purposes of this definition,\n      \"control\" means (i) the power, direct or indirect, to cause the\n      direction or management of such entity, whether by contract or\n      otherwise, or (ii) ownership of fifty percent (50%) or more of the\n      outstanding shares, or (iii) beneficial ownership of such entity.\n\n      \"You\" (or \"Your\") shall mean an individual or Legal Entity\n      exercising permissions granted by this License.\n\n      \"Source\" form shall mean the preferred form for making modifications,\n      including but not limited to software source code, documentation\n      source, and configuration files.\n\n      \"Object\" form shall mean any form resulting from mechanical\n      transformation or translation of a Source form, including but\n      not limited to compiled object code, generated documentation,\n      and conversions to other media types.\n\n      \"Work\" shall mean the work of authorship, whether in Source or\n      Object form, made available under the License, as indicated by a\n      copyright notice that is included in or attached to the work\n      (an example is provided in the Appendix below).\n\n      \"Derivative Works\" shall mean any work, whether in Source or Object\n      form, that is based on (or derived from) the Work and for which the\n      editorial revisions, annotations, elaborations, or other modifications\n      represent, as a whole, an original work of authorship. For the purposes\n      of this License, Derivative Works shall not include works that remain\n      separable from, or merely link (or bind by name) to the interfaces of,\n      the Work and Derivative Works thereof.\n\n      \"Contribution\" shall mean any work of authorship, including\n      the original version of the Work and any modifications or additions\n      to that Work or Derivative Works thereof, that is intentionally\n      submitted to Licensor for inclusion in the Work by the copyright owner\n      or by an individual or Legal Entity authorized to submit on behalf of\n      the copyright owner. For the purposes of this definition, \"submitted\"\n      means any form of electronic, verbal, or written communication sent\n      to the Licensor or its representatives, including but not limited to\n      communication on electronic mailing lists, source code control systems,\n      and issue tracking systems that are managed by, or on behalf of, the\n      Licensor for the purpose of discussing and improving the Work, but\n      excluding communication that is conspicuously marked or otherwise\n      designated in writing by the copyright owner as \"Not a Contribution.\"\n\n      \"Contributor\" shall mean Licensor and any individual or Legal Entity\n      on behalf of whom a Contribution has been received by Licensor and\n      subsequently incorporated within the Work.\n\n   2. Grant of Copyright License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      copyright license to reproduce, prepare Derivative Works of,\n      publicly display, publicly perform, sublicense, and distribute the\n      Work and such Derivative Works in Source or Object form.\n\n   3. Grant of Patent License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      (except as stated in this section) patent license to make, have made,\n      use, offer to sell, sell, import, and otherwise transfer the Work,\n      where such license applies only to those patent claims licensable\n      by such Contributor that are necessarily infringed by their\n      Contribution(s) alone or by combination of their Contribution(s)\n      with the Work to which such Contribution(s) was submitted. If You\n      institute patent litigation against any entity (including a\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\n      or a Contribution incorporated within the Work constitutes direct\n      or contributory patent infringement, then any patent licenses\n      granted to You under this License for that Work shall terminate\n      as of the date such litigation is filed.\n\n   4. Redistribution. You may reproduce and distribute copies of the\n      Work or Derivative Works thereof in any medium, with or without\n      modifications, and in Source or Object form, provided that You\n      meet the following conditions:\n\n      (a) You must give any other recipients of the Work or\n          Derivative Works a copy of this License; and\n\n      (b) You must cause any modified files to carry prominent notices\n          stating that You changed the files; and\n\n      (c) You must retain, in the Source form of any Derivative Works\n          that You distribute, all copyright, patent, trademark, and\n          attribution notices from the Source form of the Work,\n          excluding those notices that do not pertain to any part of\n          the Derivative Works; and\n\n      (d) If the Work includes a \"NOTICE\" text file as part of its\n          distribution, then any Derivative Works that You distribute must\n          include a readable copy of the attribution notices contained\n          within such NOTICE file, excluding those notices that do not\n          pertain to any part of the Derivative Works, in at least one\n          of the following places: within a NOTICE text file distributed\n          as part of the Derivative Works; within the Source form or\n          documentation, if provided along with the Derivative Works; or,\n          within a display generated by the Derivative Works, if and\n          wherever such third-party notices normally appear. The contents\n          of the NOTICE file are for informational purposes only and\n          do not modify the License. You may add Your own attribution\n          notices within Derivative Works that You distribute, alongside\n          or as an addendum to the NOTICE text from the Work, provided\n          that such additional attribution notices cannot be construed\n          as modifying the License.\n\n      You may add Your own copyright statement to Your modifications and\n      may provide additional or different license terms and conditions\n      for use, reproduction, or distribution of Your modifications, or\n      for any such Derivative Works as a whole, provided Your use,\n      reproduction, and distribution of the Work otherwise complies with\n      the conditions stated in this License.\n\n   5. Submission of Contributions. Unless You explicitly state otherwise,\n      any Contribution intentionally submitted for inclusion in the Work\n      by You to the Licensor shall be under the terms and conditions of\n      this License, without any additional terms or conditions.\n      Notwithstanding the above, nothing herein shall supersede or modify\n      the terms of any separate license agreement you may have executed\n      with Licensor regarding such Contributions.\n\n   6. Trademarks. This License does not grant permission to use the trade\n      names, trademarks, service marks, or product names of the Licensor,\n      except as required for reasonable and customary use in describing the\n      origin of the Work and reproducing the content of the NOTICE file.\n\n   7. Disclaimer of Warranty. Unless required by applicable law or\n      agreed to in writing, Licensor provides the Work (and each\n      Contributor provides its Contributions) on an \"AS IS\" BASIS,\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n      implied, including, without limitation, any warranties or conditions\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n      PARTICULAR PURPOSE. You are solely responsible for determining the\n      appropriateness of using or redistributing the Work and assume any\n      risks associated with Your exercise of permissions under this License.\n\n   8. Limitation of Liability. In no event and under no legal theory,\n      whether in tort (including negligence), contract, or otherwise,\n      unless required by applicable law (such as deliberate and grossly\n      negligent acts) or agreed to in writing, shall any Contributor be\n      liable to You for damages, including any direct, indirect, special,\n      incidental, or consequential damages of any character arising as a\n      result of this License or out of the use or inability to use the\n      Work (including but not limited to damages for loss of goodwill,\n      work stoppage, computer failure or malfunction, or any and all\n      other commercial damages or losses), even if such Contributor\n      has been advised of the possibility of such damages.\n\n   9. Accepting Warranty or Additional Liability. While redistributing\n      the Work or Derivative Works thereof, You may choose to offer,\n      and charge a fee for, acceptance of support, warranty, indemnity,\n      or other liability obligations and/or rights consistent with this\n      License. However, in accepting such obligations, You may act only\n      on Your own behalf and on Your sole responsibility, not on behalf\n      of any other Contributor, and only if You agree to indemnify,\n      defend, and hold each Contributor harmless for any liability\n      incurred by, or claims asserted against, such Contributor by reason\n      of your accepting any such warranty or additional liability.\n\n   END OF TERMS AND CONDITIONS\n\n   APPENDIX: How to apply the Apache License to your work.\n\n      To apply the Apache License to your work, attach the following\n      boilerplate notice, with the fields enclosed by brackets \"[]\"\n      replaced with your own identifying information. (Don't include\n      the brackets!)  The text should be enclosed in the appropriate\n      comment syntax for the file format. We also recommend that a\n      file or class name and description of purpose be included on the\n      same \"printed page\" as the copyright notice for easier\n      identification within third-party archives.\n\n   Copyright [yyyy] [name of copyright owner]\n\n   Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.\n"
        },
        {
          "name": "MANIFEST.in",
          "type": "blob",
          "size": 0.0146484375,
          "content": "exclude tests/*"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 49.1591796875,
          "content": "[**🇨🇳中文**](https://github.com/shibing624/text2vec/blob/master/README.md) | [**🌐English**](https://github.com/shibing624/text2vec/blob/master/README_EN.md) | [**📖文档/Docs**](https://github.com/shibing624/text2vec/wiki) | [**🤖模型/Models**](https://huggingface.co/shibing624) \n\n<div align=\"center\">\n  <a href=\"https://github.com/shibing624/text2vec\">\n    <img src=\"https://github.com/shibing624/text2vec/blob/master/docs/t2v-logo.png\" height=\"150\" alt=\"Logo\">\n  </a>\n</div>\n\n-----------------\n\n# Text2vec: Text to Vector\n[![PyPI version](https://badge.fury.io/py/text2vec.svg)](https://badge.fury.io/py/text2vec)\n[![Downloads](https://static.pepy.tech/badge/text2vec)](https://pepy.tech/project/text2vec)\n[![Contributions welcome](https://img.shields.io/badge/contributions-welcome-brightgreen.svg)](CONTRIBUTING.md)\n[![License Apache 2.0](https://img.shields.io/badge/license-Apache%202.0-blue.svg)](LICENSE)\n[![python_version](https://img.shields.io/badge/Python-3.5%2B-green.svg)](requirements.txt)\n[![GitHub issues](https://img.shields.io/github/issues/shibing624/text2vec.svg)](https://github.com/shibing624/text2vec/issues)\n[![Wechat Group](https://img.shields.io/badge/wechat-group-green.svg?logo=wechat)](#Contact)\n\n\n**Text2vec**: Text to Vector, Get Sentence Embeddings. 文本向量化，把文本(包括词、句子、段落)表征为向量矩阵。\n\n**text2vec**实现了Word2Vec、RankBM25、BERT、Sentence-BERT、CoSENT等多种文本表征、文本相似度计算模型，并在文本语义匹配（相似度计算）任务上比较了各模型的效果。\n\n### News\n[2023/09/20] v1.2.9版本: 支持多卡推理（多进程实现多GPU、多CPU推理），新增命令行工具（CLI），可以脚本执行批量文本向量化，详见[Release-v1.2.9](https://github.com/shibing624/text2vec/releases/tag/1.2.9)\n\n[2023/09/03] v1.2.4版本: 支持FlagEmbedding模型训练，发布了中文匹配模型[shibing624/text2vec-bge-large-chinese](https://huggingface.co/shibing624/text2vec-bge-large-chinese)，用CoSENT方法监督训练，基于`BAAI/bge-large-zh-noinstruct`用中文匹配数据集训练得到，并在中文测试集评估相对于原模型效果有提升，短文本区分度上提升明显，详见[Release-v1.2.4](https://github.com/shibing624/text2vec/releases/tag/1.2.4)\n\n[2023/07/17] v1.2.2版本: 支持多卡训练，发布了多语言匹配模型[shibing624/text2vec-base-multilingual](https://huggingface.co/shibing624/text2vec-base-multilingual)，用CoSENT方法训练，基于`sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2`用人工挑选后的多语言STS数据集[shibing624/nli-zh-all/text2vec-base-multilingual-dataset](https://huggingface.co/datasets/shibing624/nli-zh-all/tree/main/text2vec-base-multilingual-dataset)训练得到，并在中英文测试集评估相对于原模型效果有提升，详见[Release-v1.2.2](https://github.com/shibing624/text2vec/releases/tag/1.2.2)\n\n[2023/06/19] v1.2.1版本: 更新了中文匹配模型`shibing624/text2vec-base-chinese-nli`为新版[shibing624/text2vec-base-chinese-sentence](https://huggingface.co/shibing624/text2vec-base-chinese-sentence)，针对CoSENT的loss计算对排序敏感特点，人工挑选并整理出高质量的有相关性排序的STS数据集[shibing624/nli-zh-all/text2vec-base-chinese-sentence-dataset](https://huggingface.co/datasets/shibing624/nli-zh-all/tree/main/text2vec-base-chinese-sentence-dataset)，在各评估集表现相对之前有提升；发布了适用于s2p的中文匹配模型[shibing624/text2vec-base-chinese-paraphrase](https://huggingface.co/shibing624/text2vec-base-chinese-paraphrase)，详见[Release-v1.2.1](https://github.com/shibing624/text2vec/releases/tag/1.2.1)\n\n[2023/06/15] v1.2.0版本: 发布了中文匹配模型[shibing624/text2vec-base-chinese-nli](https://huggingface.co/shibing624/text2vec-base-chinese-nli)，基于`nghuyong/ernie-3.0-base-zh`模型，使用了中文NLI数据集[shibing624/nli_zh](https://huggingface.co/datasets/shibing624/nli_zh)全部语料训练的CoSENT文本匹配模型，在各评估集表现提升明显，详见[Release-v1.2.0](https://github.com/shibing624/text2vec/releases/tag/1.2.0)\n\n[2022/03/12] v1.1.4版本: 发布了中文匹配模型[shibing624/text2vec-base-chinese](https://huggingface.co/shibing624/text2vec-base-chinese)，基于中文STS训练集训练的CoSENT匹配模型。详见[Release-v1.1.4](https://github.com/shibing624/text2vec/releases/tag/1.1.4)\n\n\n**Guide**\n- [Features](#Features)\n- [Evaluation](#Evaluation)\n- [Install](#install)\n- [Usage](#usage)\n- [Contact](#Contact)\n- [References](#references)\n\n\n## Features\n### 文本向量表示模型\n- [Word2Vec](https://github.com/shibing624/text2vec/blob/master/text2vec/word2vec.py)：通过腾讯AI Lab开源的大规模高质量中文[词向量数据（800万中文词轻量版）](https://pan.baidu.com/s/1La4U4XNFe8s5BJqxPQpeiQ) (文件名：light_Tencent_AILab_ChineseEmbedding.bin 密码: tawe）实现词向量检索，本项目实现了句子（词向量求平均）的word2vec向量表示\n- [SBERT(Sentence-BERT)](https://github.com/shibing624/text2vec/blob/master/text2vec/sentencebert_model.py)：权衡性能和效率的句向量表示模型，训练时通过有监督训练BERT和softmax分类函数，文本匹配预测时直接取句子向量做余弦，句子表征方法，本项目基于PyTorch复现了Sentence-BERT模型的训练和预测\n- [CoSENT(Cosine Sentence)](https://github.com/shibing624/text2vec/blob/master/text2vec/cosent_model.py)：CoSENT模型提出了一种排序的损失函数，使训练过程更贴近预测，模型收敛速度和效果比Sentence-BERT更好，本项目基于PyTorch实现了CoSENT模型的训练和预测\n- [BGE(BAAI general embedding)](https://github.com/shibing624/text2vec/blob/master/text2vec/bge_model.py)：BGE模型按照[retromae](https://github.com/staoxiao/RetroMAE)方法进行预训练，[参考论文](https://aclanthology.org/2022.emnlp-main.35.pdf)，再使用对比学习finetune微调训练模型，本项目基于PyTorch实现了BGE模型的微调训练和预测\n\n\n详细文本向量表示方法见wiki: [文本向量表示方法](https://github.com/shibing624/text2vec/wiki/%E6%96%87%E6%9C%AC%E5%90%91%E9%87%8F%E8%A1%A8%E7%A4%BA%E6%96%B9%E6%B3%95)\n## Evaluation\n\n文本匹配\n\n#### 英文匹配数据集的评测结果：\n\n\n| Arch   | BaseModel                                        | Model                                                                                                                | English-STS-B | \n|:-------|:------------------------------------------------|:---------------------------------------------------------------------------------------------------------------------|:-------------:|\n| GloVe  | glove                                           | Avg_word_embeddings_glove_6B_300d                                                                                    |     61.77     |\n| BERT   | bert-base-uncased                               | BERT-base-cls                                                                                                        |     20.29     |\n| BERT   | bert-base-uncased                               | BERT-base-first_last_avg                                                                                             |     59.04     |\n| BERT   | bert-base-uncased                               | BERT-base-first_last_avg-whiten(NLI)                                                                                 |     63.65     |\n| SBERT  | sentence-transformers/bert-base-nli-mean-tokens | SBERT-base-nli-cls                                                                                                   |     73.65     |\n| SBERT  | sentence-transformers/bert-base-nli-mean-tokens | SBERT-base-nli-first_last_avg                                                                                        |     77.96     |\n| CoSENT | bert-base-uncased                               | CoSENT-base-first_last_avg                                                                                           |     69.93     |\n| CoSENT | sentence-transformers/bert-base-nli-mean-tokens | CoSENT-base-nli-first_last_avg                                                                                       |     79.68     |\n| CoSENT | sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2 | [shibing624/text2vec-base-multilingual](https://huggingface.co/shibing624/text2vec-base-multilingual)                |     80.12     |\n\n#### 中文匹配数据集的评测结果：\n\n\n| Arch   | BaseModel                    | Model           | ATEC  |  BQ   | LCQMC | PAWSX | STS-B |  Avg  | \n|:-------|:----------------------------|:--------------------|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|\n| SBERT  | bert-base-chinese           | SBERT-bert-base     | 46.36 | 70.36 | 78.72 | 46.86 | 66.41 | 61.74 |\n| SBERT  | hfl/chinese-macbert-base    | SBERT-macbert-base  | 47.28 | 68.63 | 79.42 | 55.59 | 64.82 | 63.15 |\n| SBERT  | hfl/chinese-roberta-wwm-ext | SBERT-roberta-ext   | 48.29 | 69.99 | 79.22 | 44.10 | 72.42 | 62.80 |\n| CoSENT | bert-base-chinese           | CoSENT-bert-base    | 49.74 | 72.38 | 78.69 | 60.00 | 79.27 | 68.01 |\n| CoSENT | hfl/chinese-macbert-base    | CoSENT-macbert-base | 50.39 | 72.93 | 79.17 | 60.86 | 79.30 | 68.53 |\n| CoSENT | hfl/chinese-roberta-wwm-ext | CoSENT-roberta-ext  | 50.81 | 71.45 | 79.31 | 61.56 | 79.96 | 68.61 |\n\n说明：\n- 结果评测指标：spearman系数\n- 为评测模型能力，结果均只用该数据集的train训练，在test上评估得到的表现，没用外部数据\n- `SBERT-macbert-base`模型，是用SBert方法训练，运行[examples/training_sup_text_matching_model.py](https://github.com/shibing624/text2vec/blob/master/examples/training_sup_text_matching_model.py)代码可训练模型\n- `sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2`模型是用SBert训练，是`paraphrase-MiniLM-L12-v2`模型的多语言版本，支持中文、英文等\n\n\n### Release Models\n- 本项目release模型的中文匹配评测结果：\n\n| Arch       | BaseModel                                                   | Model                                                                                                                                             | ATEC  |  BQ   | LCQMC | PAWSX | STS-B | SOHU-dd | SOHU-dc |    Avg    |  QPS  |\n|:-----------|:------------------------------------------------------------|:--------------------------------------------------------------------------------------------------------------------------------------------------|:-----:|:-----:|:-----:|:-----:|:-----:|:-------:|:-------:|:---------:|:-----:|\n| Word2Vec   | word2vec                                                    | [w2v-light-tencent-chinese](https://modelscope.cn/models/lili666/text2vec-word2vec-tencent-chinese)                                               | 20.00 | 31.49 | 59.46 | 2.57  | 55.78 |  55.04  |  20.70  |   35.03   | 23769 |\n| SBERT      | xlm-roberta-base                                            | [sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2](https://huggingface.co/sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2) | 18.42 | 38.52 | 63.96 | 10.14 | 78.90 |  63.01  |  52.28  |   46.46   | 3138  |\n| CoSENT     | hfl/chinese-macbert-base                                    | [shibing624/text2vec-base-chinese](https://huggingface.co/shibing624/text2vec-base-chinese)                                                       | 31.93 | 42.67 | 70.16 | 17.21 | 79.30 |  70.27  |  50.42  |   51.61   | 3008  |\n| CoSENT     | hfl/chinese-lert-large                                      | [GanymedeNil/text2vec-large-chinese](https://huggingface.co/GanymedeNil/text2vec-large-chinese)                                                   | 32.61 | 44.59 | 69.30 | 14.51 | 79.44 |  73.01  |  59.04  |   53.12   | 2092  |\n| CoSENT     | nghuyong/ernie-3.0-base-zh                                  | [shibing624/text2vec-base-chinese-sentence](https://huggingface.co/shibing624/text2vec-base-chinese-sentence)                                     | 43.37 | 61.43 | 73.48 | 38.90 | 78.25 |  70.60  |  53.08  |   59.87   | 3089  |\n| CoSENT     | nghuyong/ernie-3.0-base-zh                                  | [shibing624/text2vec-base-chinese-paraphrase](https://huggingface.co/shibing624/text2vec-base-chinese-paraphrase)                                 | 44.89 | 63.58 | 74.24 | 40.90 | 78.93 |  76.70  |  63.30  | **63.08** | 3066  |\n| CoSENT     | sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2 | [shibing624/text2vec-base-multilingual](https://huggingface.co/shibing624/text2vec-base-multilingual)                                             | 32.39 | 50.33 | 65.64 | 32.56 | 74.45 |  68.88  |  51.17  |   53.67   | 3138  |\n| CoSENT     | BAAI/bge-large-zh-noinstruct                                | [shibing624/text2vec-bge-large-chinese](https://huggingface.co/shibing624/text2vec-bge-large-chinese)                                             | 38.41 | 61.34 | 71.72 | 35.15 | 76.44 |  71.81  |  63.15  |   59.72   |  844  |\n\n\n说明：\n- 结果评测指标：spearman系数\n- `shibing624/text2vec-base-chinese`模型，是用CoSENT方法训练，基于`hfl/chinese-macbert-base`在中文STS-B数据训练得到，并在中文STS-B测试集评估达到较好效果，运行[examples/training_sup_text_matching_model.py](https://github.com/shibing624/text2vec/blob/master/examples/training_sup_text_matching_model.py)代码可训练模型，模型文件已经上传HF model hub，中文通用语义匹配任务推荐使用\n- `shibing624/text2vec-base-chinese-sentence`模型，是用CoSENT方法训练，基于`nghuyong/ernie-3.0-base-zh`用人工挑选后的中文STS数据集[shibing624/nli-zh-all/text2vec-base-chinese-sentence-dataset](https://huggingface.co/datasets/shibing624/nli-zh-all/tree/main/text2vec-base-chinese-sentence-dataset)训练得到，并在中文各NLI测试集评估达到较好效果，运行[examples/training_sup_text_matching_model_jsonl_data.py](https://github.com/shibing624/text2vec/blob/master/examples/training_sup_text_matching_model_jsonl_data.py)代码可训练模型，模型文件已经上传HF model hub，中文s2s(句子vs句子)语义匹配任务推荐使用\n- `shibing624/text2vec-base-chinese-paraphrase`模型，是用CoSENT方法训练，基于`nghuyong/ernie-3.0-base-zh`用人工挑选后的中文STS数据集[shibing624/nli-zh-all/text2vec-base-chinese-paraphrase-dataset](https://huggingface.co/datasets/shibing624/nli-zh-all/tree/main/text2vec-base-chinese-paraphrase-dataset)，数据集相对于[shibing624/nli-zh-all/text2vec-base-chinese-sentence-dataset](https://huggingface.co/datasets/shibing624/nli-zh-all/tree/main/text2vec-base-chinese-sentence-dataset)加入了s2p(sentence to paraphrase)数据，强化了其长文本的表征能力，并在中文各NLI测试集评估达到SOTA，运行[examples/training_sup_text_matching_model_jsonl_data.py](https://github.com/shibing624/text2vec/blob/master/examples/training_sup_text_matching_model_jsonl_data.py)代码可训练模型，模型文件已经上传HF model hub，中文s2p(句子vs段落)语义匹配任务推荐使用\n- `shibing624/text2vec-base-multilingual`模型，是用CoSENT方法训练，基于`sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2`用人工挑选后的多语言STS数据集[shibing624/nli-zh-all/text2vec-base-multilingual-dataset](https://huggingface.co/datasets/shibing624/nli-zh-all/tree/main/text2vec-base-multilingual-dataset)训练得到，并在中英文测试集评估相对于原模型效果有提升，运行[examples/training_sup_text_matching_model_jsonl_data.py](https://github.com/shibing624/text2vec/blob/master/examples/training_sup_text_matching_model_jsonl_data.py)代码可训练模型，模型文件已经上传HF model hub，多语言语义匹配任务推荐使用\n- `shibing624/text2vec-bge-large-chinese`模型，是用CoSENT方法训练，基于`BAAI/bge-large-zh-noinstruct`用人工挑选后的中文STS数据集[shibing624/nli-zh-all/text2vec-base-chinese-paraphrase-dataset](https://huggingface.co/datasets/shibing624/nli-zh-all/tree/main/text2vec-base-chinese-paraphrase-dataset)训练得到，并在中文测试集评估相对于原模型效果有提升，在短文本区分度上提升明显，运行[examples/training_sup_text_matching_model_jsonl_data.py](https://github.com/shibing624/text2vec/blob/master/examples/training_sup_text_matching_model_jsonl_data.py)代码可训练模型，模型文件已经上传HF model hub，中文s2s(句子vs句子)语义匹配任务推荐使用\n- `w2v-light-tencent-chinese`是腾讯词向量的Word2Vec模型，CPU加载使用，适用于中文字面匹配任务和缺少数据的冷启动情况\n- 各预训练模型均可以通过transformers调用，如MacBERT模型：`--model_name hfl/chinese-macbert-base` 或者roberta模型：`--model_name uer/roberta-medium-wwm-chinese-cluecorpussmall`\n- 为测评模型的鲁棒性，加入了未训练过的SOHU测试集，用于测试模型的泛化能力；为达到开箱即用的实用效果，使用了搜集到的各中文匹配数据集，数据集也上传到HF datasets[链接见下方](#数据集)\n- 中文匹配任务实验表明，pooling最优是`EncoderType.FIRST_LAST_AVG`和`EncoderType.MEAN`，两者预测效果差异很小\n- 中文匹配评测结果复现，可以下载中文匹配数据集到`examples/data`，运行 [tests/model_spearman.py](https://github.com/shibing624/text2vec/blob/master/tests/model_spearman.py) 代码复现评测结果\n- QPS的GPU测试环境是Tesla V100，显存32GB\n\n模型训练实验报告：[实验报告](https://github.com/shibing624/text2vec/blob/master/docs/model_report.md)\n## Demo\n\nOfficial Demo: https://www.mulanai.com/product/short_text_sim/\n\nHuggingFace Demo: https://huggingface.co/spaces/shibing624/text2vec\n\n![](https://github.com/shibing624/text2vec/blob/master/docs/hf.png)\n\nrun example: [examples/gradio_demo.py](https://github.com/shibing624/text2vec/blob/master/examples/gradio_demo.py) to see the demo:\n```shell\npython examples/gradio_demo.py\n```\n\n## Install\n```shell\npip install torch # conda install pytorch\npip install -U text2vec\n```\n\nor\n\n```shell\npip install torch # conda install pytorch\npip install -r requirements.txt\n\ngit clone https://github.com/shibing624/text2vec.git\ncd text2vec\npip install --no-deps .\n```\n\n## Usage\n\n### 文本向量表征\n\n基于`pretrained model`计算文本向量：\n\n```zsh\n>>> from text2vec import SentenceModel\n>>> m = SentenceModel()\n>>> m.encode(\"如何更换花呗绑定银行卡\")\nEmbedding shape: (768,)\n```\n\nexample: [examples/computing_embeddings_demo.py](https://github.com/shibing624/text2vec/blob/master/examples/computing_embeddings_demo.py)\n\n```python\nimport sys\n\nsys.path.append('..')\nfrom text2vec import SentenceModel\nfrom text2vec import Word2Vec\n\n\ndef compute_emb(model):\n    # Embed a list of sentences\n    sentences = [\n        '卡',\n        '银行卡',\n        '如何更换花呗绑定银行卡',\n        '花呗更改绑定银行卡',\n        'This framework generates embeddings for each input sentence',\n        'Sentences are passed as a list of string.',\n        'The quick brown fox jumps over the lazy dog.'\n    ]\n    sentence_embeddings = model.encode(sentences)\n    print(type(sentence_embeddings), sentence_embeddings.shape)\n\n    # The result is a list of sentence embeddings as numpy arrays\n    for sentence, embedding in zip(sentences, sentence_embeddings):\n        print(\"Sentence:\", sentence)\n        print(\"Embedding shape:\", embedding.shape)\n        print(\"Embedding head:\", embedding[:10])\n        print()\n\n\nif __name__ == \"__main__\":\n    # 中文句向量模型(CoSENT)，中文语义匹配任务推荐，支持fine-tune继续训练\n    t2v_model = SentenceModel(\"shibing624/text2vec-base-chinese\")\n    compute_emb(t2v_model)\n\n    # 支持多语言的句向量模型（CoSENT），多语言（包括中英文）语义匹配任务推荐，支持fine-tune继续训练\n    sbert_model = SentenceModel(\"shibing624/text2vec-base-multilingual\")\n    compute_emb(sbert_model)\n\n    # 中文词向量模型(word2vec)，中文字面匹配任务和冷启动适用\n    w2v_model = Word2Vec(\"w2v-light-tencent-chinese\")\n    compute_emb(w2v_model)\n\n```\n\noutput:\n```\n<class 'numpy.ndarray'> (7, 768)\nSentence: 卡\nEmbedding shape: (768,)\n\nSentence: 银行卡\nEmbedding shape: (768,)\n ... \n```\n\n- 返回值`embeddings`是`numpy.ndarray`类型，shape为`(sentences_size, model_embedding_size)`，三个模型任选一种即可，推荐用第一个。\n- `shibing624/text2vec-base-chinese`模型是CoSENT方法在中文STS-B数据集训练得到的，模型已经上传到huggingface的\n模型库[shibing624/text2vec-base-chinese](https://huggingface.co/shibing624/text2vec-base-chinese)，\n是`text2vec.SentenceModel`指定的默认模型，可以通过上面示例调用，或者如下所示用[transformers库](https://github.com/huggingface/transformers)调用，\n模型自动下载到本机路径：`~/.cache/huggingface/transformers`\n- `w2v-light-tencent-chinese`是通过gensim加载的Word2Vec模型，使用腾讯词向量计算各字词的词向量，句子向量通过单词词\n向量取平均值得到，模型自动下载到本机路径：`~/.text2vec/datasets/light_Tencent_AILab_ChineseEmbedding.bin`\n- `text2vec`支持多卡推理(计算文本向量): [examples/computing_embeddings_multi_gpu_demo.py](https://github.com/shibing624/text2vec/blob/master/examples/computing_embeddings_multi_gpu_demo.py)\n\n#### Usage (HuggingFace Transformers)\nWithout [text2vec](https://github.com/shibing624/text2vec), you can use the model like this: \n\nFirst, you pass your input through the transformer model, then you have to apply the right pooling-operation on-top of the contextualized word embeddings.\n\nexample: [examples/use_origin_transformers_demo.py](https://github.com/shibing624/text2vec/blob/master/examples/use_origin_transformers_demo.py)\n\n```python\nimport os\nimport torch\nfrom transformers import AutoTokenizer, AutoModel\n\nos.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"\n\n\n# Mean Pooling - Take attention mask into account for correct averaging\ndef mean_pooling(model_output, attention_mask):\n    token_embeddings = model_output[0]  # First element of model_output contains all token embeddings\n    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n\n\n# Load model from HuggingFace Hub\ntokenizer = AutoTokenizer.from_pretrained('shibing624/text2vec-base-chinese')\nmodel = AutoModel.from_pretrained('shibing624/text2vec-base-chinese')\nsentences = ['如何更换花呗绑定银行卡', '花呗更改绑定银行卡']\n# Tokenize sentences\nencoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')\n\n# Compute token embeddings\nwith torch.no_grad():\n    model_output = model(**encoded_input)\n# Perform pooling. In this case, max pooling.\nsentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\nprint(\"Sentence embeddings:\")\nprint(sentence_embeddings)\n```\n\n#### Usage (sentence-transformers)\n[sentence-transformers](https://github.com/UKPLab/sentence-transformers) is a popular library to compute dense vector representations for sentences.\n\nInstall sentence-transformers:\n```shell\npip install -U sentence-transformers\n```\nThen load model and predict:\n```python\nfrom sentence_transformers import SentenceTransformer\n\nm = SentenceTransformer(\"shibing624/text2vec-base-chinese\")\nsentences = ['如何更换花呗绑定银行卡', '花呗更改绑定银行卡']\n\nsentence_embeddings = m.encode(sentences)\nprint(\"Sentence embeddings:\")\nprint(sentence_embeddings)\n```\n\n#### `Word2Vec`词向量\n\n提供`Word2Vec`词向量，轻量版腾讯词向量`light_Tencent_AILab_ChineseEmbedding.bin`，二进制文件，111M，是简化后的高频143613个词，每个词向量还是200维（跟原版一样），运行程序，自动下载到 `~/.text2vec/datasets/light_Tencent_AILab_ChineseEmbedding.bin`\n\n模型地址：[Modelscope](https://modelscope.cn/models/lili666/text2vec-word2vec-tencent-chinese/summary) | [百度云盘-密码:tawe](https://pan.baidu.com/s/1La4U4XNFe8s5BJqxPQpeiQ) | [谷歌云盘](https://drive.google.com/u/0/uc?id=1iQo9tBb2NgFOBxx0fA16AZpSgc-bG_Rp&export=download)\n\n### 命令行模式（CLI）\n\n支持批量获取文本向量\n\ncode: [cli.py](https://github.com/shibing624/text2vec/blob/master/text2vec/cli.py)\n\n```\n> text2vec -h                                    \nusage: text2vec [-h] --input_file INPUT_FILE [--output_file OUTPUT_FILE] [--model_type MODEL_TYPE] [--model_name MODEL_NAME] [--encoder_type ENCODER_TYPE]\n                [--batch_size BATCH_SIZE] [--max_seq_length MAX_SEQ_LENGTH] [--chunk_size CHUNK_SIZE] [--device DEVICE]\n                [--show_progress_bar SHOW_PROGRESS_BAR] [--normalize_embeddings NORMALIZE_EMBEDDINGS]\n\ntext2vec cli\n\noptional arguments:\n  -h, --help            show this help message and exit\n  --input_file INPUT_FILE\n                        input file path, text file, required\n  --output_file OUTPUT_FILE\n                        output file path, output csv file, default text_embs.csv\n  --model_type MODEL_TYPE\n                        model type: sentencemodel, word2vec, default sentencemodel\n  --model_name MODEL_NAME\n                        model name or path, default shibing624/text2vec-base-chinese\n  --encoder_type ENCODER_TYPE\n                        encoder type: MEAN, CLS, POOLER, FIRST_LAST_AVG, LAST_AVG, default MEAN\n  --batch_size BATCH_SIZE\n                        batch size, default 32\n  --max_seq_length MAX_SEQ_LENGTH\n                        max sequence length, default 256\n  --chunk_size CHUNK_SIZE\n                        chunk size to save partial results, default 1000\n  --device DEVICE       device: cpu, cuda, default None\n  --show_progress_bar SHOW_PROGRESS_BAR\n                        show progress bar, default True\n  --normalize_embeddings NORMALIZE_EMBEDDINGS\n                        normalize embeddings, default False\n  --multi_gpu MULTI_GPU\n                        multi gpu, default False\n```\n\nrun：\n\n```shell\npip install text2vec -U\ntext2vec --input_file input.txt --output_file out.csv --batch_size 128 --multi_gpu True\n```\n\n> 输入文件（required）：`input.txt`，format：一句话一行的句子文本。\n\n## 下游任务\n### 1. 句子相似度计算\n\nexample: [examples/semantic_text_similarity_demo.py](https://github.com/shibing624/text2vec/blob/master/examples/semantic_text_similarity_demo.py)\n\n```python\nimport sys\n\nsys.path.append('..')\nfrom text2vec import Similarity\n\n# Two lists of sentences\nsentences1 = ['如何更换花呗绑定银行卡',\n              'The cat sits outside',\n              'A man is playing guitar',\n              'The new movie is awesome']\n\nsentences2 = ['花呗更改绑定银行卡',\n              'The dog plays in the garden',\n              'A woman watches TV',\n              'The new movie is so great']\n\nsim_model = Similarity()\nfor i in range(len(sentences1)):\n    for j in range(len(sentences2)):\n        score = sim_model.get_score(sentences1[i], sentences2[j])\n        print(\"{} \\t\\t {} \\t\\t Score: {:.4f}\".format(sentences1[i], sentences2[j], score))\n```\n\noutput:\n```shell\n如何更换花呗绑定银行卡 \t\t 花呗更改绑定银行卡 \t\t Score: 0.9477\n如何更换花呗绑定银行卡 \t\t The dog plays in the garden \t\t Score: -0.1748\n如何更换花呗绑定银行卡 \t\t A woman watches TV \t\t Score: -0.0839\n如何更换花呗绑定银行卡 \t\t The new movie is so great \t\t Score: -0.0044\nThe cat sits outside \t\t 花呗更改绑定银行卡 \t\t Score: -0.0097\nThe cat sits outside \t\t The dog plays in the garden \t\t Score: 0.1908\nThe cat sits outside \t\t A woman watches TV \t\t Score: -0.0203\nThe cat sits outside \t\t The new movie is so great \t\t Score: 0.0302\nA man is playing guitar \t\t 花呗更改绑定银行卡 \t\t Score: -0.0010\nA man is playing guitar \t\t The dog plays in the garden \t\t Score: 0.1062\nA man is playing guitar \t\t A woman watches TV \t\t Score: 0.0055\nA man is playing guitar \t\t The new movie is so great \t\t Score: 0.0097\nThe new movie is awesome \t\t 花呗更改绑定银行卡 \t\t Score: 0.0302\nThe new movie is awesome \t\t The dog plays in the garden \t\t Score: -0.0160\nThe new movie is awesome \t\t A woman watches TV \t\t Score: 0.1321\nThe new movie is awesome \t\t The new movie is so great \t\t Score: 0.9591\n```\n\n> 句子余弦相似度值`score`范围是[-1, 1]，值越大越相似。\n\n### 2. 文本匹配搜索\n\n一般在文档候选集中找与query最相似的文本，常用于QA场景的问句相似匹配、文本相似检索等任务。\n\n\nexample: [examples/semantic_search_demo.py](https://github.com/shibing624/text2vec/blob/master/examples/semantic_search_demo.py)\n\n```python\nimport sys\n\nsys.path.append('..')\nfrom text2vec import SentenceModel, cos_sim, semantic_search\n\nembedder = SentenceModel()\n\n# Corpus with example sentences\ncorpus = [\n    '花呗更改绑定银行卡',\n    '我什么时候开通了花呗',\n    'A man is eating food.',\n    'A man is eating a piece of bread.',\n    'The girl is carrying a baby.',\n    'A man is riding a horse.',\n    'A woman is playing violin.',\n    'Two men pushed carts through the woods.',\n    'A man is riding a white horse on an enclosed ground.',\n    'A monkey is playing drums.',\n    'A cheetah is running behind its prey.'\n]\ncorpus_embeddings = embedder.encode(corpus)\n\n# Query sentences:\nqueries = [\n    '如何更换花呗绑定银行卡',\n    'A man is eating pasta.',\n    'Someone in a gorilla costume is playing a set of drums.',\n    'A cheetah chases prey on across a field.']\n\nfor query in queries:\n    query_embedding = embedder.encode(query)\n    hits = semantic_search(query_embedding, corpus_embeddings, top_k=5)\n    print(\"\\n\\n======================\\n\\n\")\n    print(\"Query:\", query)\n    print(\"\\nTop 5 most similar sentences in corpus:\")\n    hits = hits[0]  # Get the hits for the first query\n    for hit in hits:\n        print(corpus[hit['corpus_id']], \"(Score: {:.4f})\".format(hit['score']))\n```\noutput:\n```shell\nQuery: 如何更换花呗绑定银行卡\nTop 5 most similar sentences in corpus:\n花呗更改绑定银行卡 (Score: 0.9477)\n我什么时候开通了花呗 (Score: 0.3635)\nA man is eating food. (Score: 0.0321)\nA man is riding a horse. (Score: 0.0228)\nTwo men pushed carts through the woods. (Score: 0.0090)\n\n======================\nQuery: A man is eating pasta.\nTop 5 most similar sentences in corpus:\nA man is eating food. (Score: 0.6734)\nA man is eating a piece of bread. (Score: 0.4269)\nA man is riding a horse. (Score: 0.2086)\nA man is riding a white horse on an enclosed ground. (Score: 0.1020)\nA cheetah is running behind its prey. (Score: 0.0566)\n\n======================\nQuery: Someone in a gorilla costume is playing a set of drums.\nTop 5 most similar sentences in corpus:\nA monkey is playing drums. (Score: 0.8167)\nA cheetah is running behind its prey. (Score: 0.2720)\nA woman is playing violin. (Score: 0.1721)\nA man is riding a horse. (Score: 0.1291)\nA man is riding a white horse on an enclosed ground. (Score: 0.1213)\n\n======================\nQuery: A cheetah chases prey on across a field.\nTop 5 most similar sentences in corpus:\nA cheetah is running behind its prey. (Score: 0.9147)\nA monkey is playing drums. (Score: 0.2655)\nA man is riding a horse. (Score: 0.1933)\nA man is riding a white horse on an enclosed ground. (Score: 0.1733)\nA man is eating food. (Score: 0.0329)\n```\n\n \n\n## 下游任务支持库\n**similarities库[推荐]**\n\n文本相似度计算和文本匹配搜索任务，推荐使用 [similarities库](https://github.com/shibing624/similarities) ，兼容本项目release的\nWord2vec、SBERT、Cosent类语义匹配模型，还支持亿级图文搜索，支持**文本语义去重**，**图片去重**等功能。\n\n安装：\n```pip install -U similarities```\n\n句子相似度计算：\n```python\nfrom similarities import BertSimilarity\n\nm = BertSimilarity()\nr = m.similarity('如何更换花呗绑定银行卡', '花呗更改绑定银行卡')\nprint(f\"similarity score: {float(r)}\")  # similarity score: 0.855146050453186\n```\n\n## Models\n\n### CoSENT model\n\nCoSENT（Cosine Sentence）文本匹配模型，在Sentence-BERT上改进了CosineRankLoss的句向量方案\n\n\nNetwork structure:\n\nTraining:\n\n<img src=\"docs/cosent_train.png\" width=\"300\" />\n\n\nInference:\n\n<img src=\"docs/inference.png\" width=\"300\" />\n\n#### CoSENT 监督模型\n训练和预测CoSENT模型：\n\n- 在中文STS-B数据集训练和评估`CoSENT`模型\n\nexample: [examples/training_sup_text_matching_model.py](https://github.com/shibing624/text2vec/blob/master/examples/training_sup_text_matching_model.py)\n\n```shell\ncd examples\npython training_sup_text_matching_model.py --model_arch cosent --do_train --do_predict --num_epochs 10 --model_name hfl/chinese-macbert-base --output_dir ./outputs/STS-B-cosent\n```\n\n- 在蚂蚁金融匹配数据集ATEC上训练和评估`CoSENT`模型\n\n支持这些中文匹配数据集的使用：'ATEC', 'STS-B', 'BQ', 'LCQMC', 'PAWSX'，具体参考HuggingFace datasets [https://huggingface.co/datasets/shibing624/nli_zh](https://huggingface.co/datasets/shibing624/nli_zh)\n```shell\npython training_sup_text_matching_model.py --task_name ATEC --model_arch cosent --do_train --do_predict --num_epochs 10 --model_name hfl/chinese-macbert-base --output_dir ./outputs/ATEC-cosent\n```\n\n- 在自有中文数据集上训练模型\n\nexample: [examples/training_sup_text_matching_model_mydata.py](https://github.com/shibing624/text2vec/blob/master/examples/training_sup_text_matching_model_mydata.py)\n\n单卡训练：\n```shell\nCUDA_VISIBLE_DEVICES=0 python training_sup_text_matching_model_mydata.py --do_train --do_predict\n```\n\n多卡训练：\n```shell\nCUDA_VISIBLE_DEVICES=0,1 torchrun --nproc_per_node 2  training_sup_text_matching_model_mydata.py --do_train --do_predict --output_dir outputs/STS-B-text2vec-macbert-v1 --batch_size 64 --bf16 --data_parallel \n```\n\n训练集格式参考[examples/data/STS-B/STS-B.valid.data](https://github.com/shibing624/text2vec/blob/master/examples/data/STS-B/STS-B.valid.data)\n\n```shell\nsentence1   sentence2   label\n一个女孩在给她的头发做发型。\t一个女孩在梳头。\t2\n一群男人在海滩上踢足球。\t一群男孩在海滩上踢足球。\t3\n一个女人在测量另一个女人的脚踝。\t女人测量另一个女人的脚踝。\t5\n```\n\n`label`可以是0，1标签，0代表两个句子不相似，1代表相似；也可以是0-5的评分，评分越高，表示两个句子越相似。模型都能支持。\n\n\n- 在英文STS-B数据集训练和评估`CoSENT`模型\n\nexample: [examples/training_sup_text_matching_model_en.py](https://github.com/shibing624/text2vec/blob/master/examples/training_sup_text_matching_model_en.py)\n\n```shell\ncd examples\npython training_sup_text_matching_model_en.py --model_arch cosent --do_train --do_predict --num_epochs 10 --model_name bert-base-uncased  --output_dir ./outputs/STS-B-en-cosent\n```\n\n#### CoSENT 无监督模型\n- 在英文NLI数据集训练`CoSENT`模型，在STS-B测试集评估效果\n\nexample: [examples/training_unsup_text_matching_model_en.py](https://github.com/shibing624/text2vec/blob/master/examples/training_unsup_text_matching_model_en.py)\n\n```shell\ncd examples\npython training_unsup_text_matching_model_en.py --model_arch cosent --do_train --do_predict --num_epochs 10 --model_name bert-base-uncased --output_dir ./outputs/STS-B-en-unsup-cosent\n```\n\n\n### Sentence-BERT model\n\nSentence-BERT文本匹配模型，表征式句向量表示方案\n\nNetwork structure:\n\nTraining:\n\n<img src=\"docs/sbert_train.png\" width=\"300\" />\n\n\nInference:\n\n<img src=\"docs/sbert_inference.png\" width=\"300\" />\n\n#### SentenceBERT 监督模型\n- 在中文STS-B数据集训练和评估`SBERT`模型\n\nexample: [examples/training_sup_text_matching_model.py](https://github.com/shibing624/text2vec/blob/master/examples/training_sup_text_matching_model.py)\n\n```shell\ncd examples\npython training_sup_text_matching_model.py --model_arch sentencebert --do_train --do_predict --num_epochs 10 --model_name hfl/chinese-macbert-base --output_dir ./outputs/STS-B-sbert\n```\n- 在英文STS-B数据集训练和评估`SBERT`模型\n\nexample: [examples/training_sup_text_matching_model_en.py](https://github.com/shibing624/text2vec/blob/master/examples/training_sup_text_matching_model_en.py)\n\n```shell\ncd examples\npython training_sup_text_matching_model_en.py --model_arch sentencebert --do_train --do_predict --num_epochs 10 --model_name bert-base-uncased --output_dir ./outputs/STS-B-en-sbert\n```\n\n#### SentenceBERT 无监督模型\n- 在英文NLI数据集训练`SBERT`模型，在STS-B测试集评估效果\n\nexample: [examples/training_unsup_text_matching_model_en.py](https://github.com/shibing624/text2vec/blob/master/examples/training_unsup_text_matching_model_en.py)\n\n```shell\ncd examples\npython training_unsup_text_matching_model_en.py --model_arch sentencebert --do_train --do_predict --num_epochs 10 --model_name bert-base-uncased --output_dir ./outputs/STS-B-en-unsup-sbert\n```\n\n### BERT-Match model\nBERT文本匹配模型，原生BERT匹配网络结构，交互式句向量匹配模型\n\nNetwork structure:\n\nTraining and inference:\n\n<img src=\"docs/bert-fc-train.png\" width=\"300\" />\n\n训练脚本同上[examples/training_sup_text_matching_model.py](https://github.com/shibing624/text2vec/blob/master/examples/training_sup_text_matching_model.py)。\n\n\n\n### BGE model\n\n#### BGE 监督模型\n- 在中文STS-B数据集训练和评估`BGE`模型\n\nexample: [examples/training_bge_model_mydata.py](https://github.com/shibing624/text2vec/blob/master/examples/training_bge_model_mydata.py)\n\n```shell\ncd examples\npython training_bge_model_mydata.py --model_arch bge --do_train --do_predict --num_epochs 4 --output_dir ./outputs/STS-B-bge-v1 --batch_size 4 --save_model_every_epoch --bf16\n```\n\n- 自建BGE训练集\n\nBGE模型微调训练，使用对比学习训练模型，输入数据的格式是一个三元组' (query, positive, negative) '\n\n```shell\ncd examples/data\npython build_zh_bge_dataset.py\npython hard_negatives_mine.py\n```\n1. `build_zh_bge_dataset.py` 基于中文STS-B生成三元组训练集，格式如下：\n```json lines\n{\"query\":\"一个男人正在往锅里倒油。\",\"pos\":[\"一个男人正在往锅里倒油。\"],\"neg\":[\"亲俄军队进入克里米亚乌克兰海军基地\",\"配有木制家具的优雅餐厅。\",\"马雅瓦蒂要求总统统治查谟和克什米尔\",\"非典还夺去了多伦多地区44人的生命，其中包括两名护士和一名医生。\",\"在一次采访中，身为犯罪学家的希利说，这里和全国各地的许多议员都对死刑抱有戒心。\",\"豚鼠吃胡萝卜。\",\"狗嘴里叼着一根棍子在水中游泳。\",\"拉里·佩奇说Android很重要，不是关键\",\"法国、比利时、德国、瑞典、意大利和英国为印度计划向缅甸出售的先进轻型直升机提供零部件和技术。\",\"巴林赛马会在动乱中进行\"]}\n```\n2. `hard_negatives_mine.py` 使用faiss相似匹配，挖掘难负例。\n\n\n### 模型蒸馏（Model Distillation）\n\n由于text2vec训练的模型可以使用[sentence-transformers](https://github.com/UKPLab/sentence-transformers)库加载，此处复用其模型蒸馏方法[distillation](https://github.com/UKPLab/sentence-transformers/tree/master/examples/training/distillation)。\n\n1. 模型降维，参考[dimensionality_reduction.py](https://github.com/UKPLab/sentence-transformers/blob/master/examples/training/distillation/dimensionality_reduction.py)使用PCA对模型输出embedding降维，可减少milvus等向量检索数据库的存储压力，还能轻微提升模型效果。\n2. 模型蒸馏，参考[model_distillation.py](https://github.com/UKPLab/sentence-transformers/blob/master/examples/training/distillation/model_distillation.py)使用蒸馏方法，将Teacher大模型蒸馏到更少layers层数的student模型中，在权衡效果的情况下，可大幅提升模型预测速度。\n\n### 模型部署\n\n提供两种部署模型，搭建服务的方法： 1）基于Jina搭建gRPC服务【推荐】；2）基于FastAPI搭建原生Http服务。\n\n#### Jina服务\n采用C/S模式搭建高性能服务，支持docker云原生，gRPC/HTTP/WebSocket，支持多个模型同时预测，GPU多卡处理。\n\n- 安装：\n```pip install jina```\n\n- 启动服务：\n\nexample: [examples/jina_server_demo.py](examples/jina_server_demo.py)\n```python\nfrom jina import Flow\n\nport = 50001\nf = Flow(port=port).add(\n    uses='jinahub://Text2vecEncoder',\n    uses_with={'model_name': 'shibing624/text2vec-base-chinese'}\n)\n\nwith f:\n    # backend server forever\n    f.block()\n```\n\n该模型预测方法（executor）已经上传到[JinaHub](https://hub.jina.ai/executor/eq45c9uq)，里面包括docker、k8s部署方法。\n\n- 调用服务：\n\n\n```python\nfrom jina import Client\nfrom docarray import Document, DocumentArray\n\nport = 50001\n\nc = Client(port=port)\n\ndata = ['如何更换花呗绑定银行卡',\n        '花呗更改绑定银行卡']\nprint(\"data:\", data)\nprint('data embs:')\nr = c.post('/', inputs=DocumentArray([Document(text='如何更换花呗绑定银行卡'), Document(text='花呗更改绑定银行卡')]))\nprint(r.embeddings)\n```\n\n批量调用方法见example: [examples/jina_client_demo.py](https://github.com/shibing624/text2vec/blob/master/examples/jina_client_demo.py)\n\n\n#### FastAPI服务\n\n- 安装：\n```pip install fastapi uvicorn```\n\n- 启动服务：\n\nexample: [examples/fastapi_server_demo.py](https://github.com/shibing624/text2vec/blob/master/examples/fastapi_server_demo.py)\n```shell\ncd examples\npython fastapi_server_demo.py\n```\n\n- 调用服务：\n```shell\ncurl -X 'GET' \\\n  'http://0.0.0.0:8001/emb?q=hello' \\\n  -H 'accept: application/json'\n```\n\n\n## Dataset\n\n- 本项目release的数据集：\n\n| Dataset                    | Introduce                                                                | Download Link                                                                                                                                                                                                                                                                                         |\n|:---------------------------|:-------------------------------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| shibing624/nli-zh-all      | 中文语义匹配数据合集，整合了文本推理，相似，摘要，问答，指令微调等任务的820万高质量数据，并转化为匹配格式数据集                | [https://huggingface.co/datasets/shibing624/nli-zh-all](https://huggingface.co/datasets/shibing624/nli-zh-all)                                                                                                                                                                                        |\n| shibing624/snli-zh         | 中文SNLI和MultiNLI数据集，翻译自英文SNLI和MultiNLI                                    | [https://huggingface.co/datasets/shibing624/snli-zh](https://huggingface.co/datasets/shibing624/snli-zh)                                                                                                                                                                                              |\n| shibing624/nli_zh          | 中文语义匹配数据集，整合了中文ATEC、BQ、LCQMC、PAWSX、STS-B共5个任务的数据集                        | [https://huggingface.co/datasets/shibing624/nli_zh](https://huggingface.co/datasets/shibing624/nli_zh) </br> or </br> [百度网盘(提取码:qkt6)](https://pan.baidu.com/s/1d6jSiU1wHQAEMWJi7JJWCQ) </br> or </br> [github](https://github.com/shibing624/text2vec/releases/download/1.1.2/senteval_cn.zip) </br> |\n| shibing624/sts-sohu2021    | 中文语义匹配数据集，2021搜狐校园文本匹配算法大赛数据集                                            | [https://huggingface.co/datasets/shibing624/sts-sohu2021](https://huggingface.co/datasets/shibing624/sts-sohu2021)                                                                                                                                                                                    |\n| ATEC                       | 中文ATEC数据集，蚂蚁金服Q-Qpair数据集                                                 | [ATEC](https://github.com/IceFlameWorm/NLP_Datasets/tree/master/ATEC)                                                                                                                                                                                                                                 |\n| BQ                         | 中文BQ(Bank Question)数据集，银行Q-Qpair数据集                                      | [BQ](http://icrc.hitsz.edu.cn/info/1037/1162.htm)                                                                                                                                                                                                                                                     |\n| LCQMC                      | 中文LCQMC(large-scale Chinese question matching corpus)数据集，Q-Qpair数据集      | [LCQMC](http://icrc.hitsz.edu.cn/Article/show/171.html)                                                                                                                                                                                                                                               |\n| PAWSX                      | 中文PAWS(Paraphrase Adversaries from Word Scrambling)数据集，Q-Qpair数据集        | [PAWSX](https://arxiv.org/abs/1908.11828)                                                                                                                                                                                                                                                             |\n| STS-B                      | 中文STS-B数据集，中文自然语言推理数据集，从英文STS-B翻译为中文的数据集                                 | [STS-B](https://github.com/pluto-junzeng/CNSD)                                                                                                                                                                                                                                                        |\n\n\n常用英文匹配数据集：\n\n- 英文匹配数据集：multi_nli: https://huggingface.co/datasets/multi_nli\n- 英文匹配数据集：snli: https://huggingface.co/datasets/snli\n- https://huggingface.co/datasets/metaeval/cnli\n- https://huggingface.co/datasets/mteb/stsbenchmark-sts\n- https://huggingface.co/datasets/JeremiahZ/simcse_sup_nli\n- https://huggingface.co/datasets/MoritzLaurer/multilingual-NLI-26lang-2mil7\n\n\n数据集使用示例：\n```shell\npip install datasets\n```\n\n```python\nfrom datasets import load_dataset\n\ndataset = load_dataset(\"shibing624/nli_zh\", \"STS-B\") # ATEC or BQ or LCQMC or PAWSX or STS-B\nprint(dataset)\nprint(dataset['test'][0])\n```\n\noutput:\n```shell\nDatasetDict({\n    train: Dataset({\n        features: ['sentence1', 'sentence2', 'label'],\n        num_rows: 5231\n    })\n    validation: Dataset({\n        features: ['sentence1', 'sentence2', 'label'],\n        num_rows: 1458\n    })\n    test: Dataset({\n        features: ['sentence1', 'sentence2', 'label'],\n        num_rows: 1361\n    })\n})\n{'sentence1': '一个女孩在给她的头发做发型。', 'sentence2': '一个女孩在梳头。', 'label': 2}\n```\n\n## Contact\n\n- Issue(建议)：[![GitHub issues](https://img.shields.io/github/issues/shibing624/text2vec.svg)](https://github.com/shibing624/text2vec/issues)\n- 邮件我：xuming: xuming624@qq.com\n- 微信我：加我*微信号：xuming624, 备注：姓名-公司-NLP* 进NLP交流群。\n\n<img src=\"docs/wechat.jpeg\" width=\"200\" />\n\n\n## Citation\n\n如果你在研究中使用了text2vec，请按如下格式引用：\n\nAPA:\n```latex\nXu, M. Text2vec: Text to vector toolkit (Version 1.1.2) [Computer software]. https://github.com/shibing624/text2vec\n```\n\nBibTeX:\n```latex\n@misc{Text2vec,\n  author = {Ming Xu},\n  title = {Text2vec: Text to vector toolkit},\n  year = {2023},\n  publisher = {GitHub},\n  journal = {GitHub repository},\n  howpublished = {\\url{https://github.com/shibing624/text2vec}},\n}\n```\n\n## License\n\n\n授权协议为 [The Apache License 2.0](LICENSE)，可免费用做商业用途。请在产品说明中附加text2vec的链接和授权协议。\n\n\n## Contribute\n项目代码还很粗糙，如果大家对代码有所改进，欢迎提交回本项目，在提交之前，注意以下两点：\n\n - 在`tests`添加相应的单元测试\n - 使用`python -m pytest -v`来运行所有单元测试，确保所有单测都是通过的\n\n之后即可提交PR。\n\n## References\n- [将句子表示为向量（上）：无监督句子表示学习（sentence embedding）](https://www.cnblogs.com/llhthinker/p/10335164.html)\n- [将句子表示为向量（下）：无监督句子表示学习（sentence embedding）](https://www.cnblogs.com/llhthinker/p/10341841.html)\n- [A Simple but Tough-to-Beat Baseline for Sentence Embeddings[Sanjeev Arora and Yingyu Liang and Tengyu Ma, 2017]](https://openreview.net/forum?id=SyK00v5xx)\n- [四种计算文本相似度的方法对比[Yves Peirsman]](https://zhuanlan.zhihu.com/p/37104535)\n- [Improvements to BM25 and Language Models Examined](http://www.cs.otago.ac.nz/homepages/andrew/papers/2014-2.pdf)\n- [CoSENT：比Sentence-BERT更有效的句向量方案](https://kexue.fm/archives/8847)\n- [谈谈文本匹配和多轮检索](https://zhuanlan.zhihu.com/p/111769969)\n- [Sentence-transformers](https://www.sbert.net/examples/applications/computing-embeddings/README.html)\n- [One Embedder, Any Task: Instruction-Finetuned Text Embeddings](https://arxiv.org/abs/2212.09741)\n"
        },
        {
          "name": "README_EN.md",
          "type": "blob",
          "size": 39.931640625,
          "content": "[**🇨🇳中文**](https://github.com/shibing624/text2vec/blob/master/README.md) | [**🌐English**](https://github.com/shibing624/text2vec/blob/master/README_EN.md) | [**📖文档/Docs**](https://github.com/shibing624/text2vec/wiki) | [**🤖模型/Models**](https://huggingface.co/shibing624) \n\n<div align=\"center\">\n  <a href=\"https://github.com/shibing624/text2vec\">\n    <img src=\"https://github.com/shibing624/text2vec/blob/master/docs/t2v-logo.png\" height=\"150\" alt=\"Logo\">\n  </a>\n</div>\n\n-----------------\n\n# Text2vec: Text to Vector\n[![PyPI version](https://badge.fury.io/py/text2vec.svg)](https://badge.fury.io/py/text2vec)\n[![Downloads](https://pepy.tech/badge/text2vec)](https://pepy.tech/project/text2vec)\n[![Contributions welcome](https://img.shields.io/badge/contributions-welcome-brightgreen.svg)](CONTRIBUTING.md)\n[![License Apache 2.0](https://img.shields.io/badge/license-Apache%202.0-blue.svg)](LICENSE)\n[![python_version](https://img.shields.io/badge/Python-3.5%2B-green.svg)](requirements.txt)\n[![GitHub issues](https://img.shields.io/github/issues/shibing624/text2vec.svg)](https://github.com/shibing624/text2vec/issues)\n[![Wechat Group](http://vlog.sfyc.ltd/wechat_everyday/wxgroup_logo.png?imageView2/0/w/60/h/20)](#Contact)\n\n**Text2vec**: Text to Vector, Get Sentence Embeddings. Text vectorization, representing text (including words, sentences, paragraphs) as a vector matrix.\n\n**text2vec** implements Word2Vec, RankBM25, BERT, Sentence-BERT, CoSENT and other text representation and text similarity calculation models, and compares the effects of each model on the text semantic matching (similarity calculation) task.\n\n**Guide**\n- [Feature](#Feature)\n- [Evaluation](#Evaluation)\n- [Install](#install)\n- [Usage](#usage)\n- [Contact](#Contact)\n- [Reference](#reference)\n\n\n# Feature\n### 文本向量表示模型\n- [Word2Vec](https://github.com/shibing624/text2vec/blob/master/text2vec/word2vec.py): large-scale high-quality Chinese [word vector data (8 million Chinese words light weight) through Tencent AI Lab open source version)](https://pan.baidu.com/s/1La4U4XNFe8s5BJqxPQpeiQ) (file name: light_Tencent_AILab_ChineseEmbedding.bin password: tawe) to achieve word vector retrieval, this project realizes word2vec vector representation of sentences (word vector average)\n- [SBERT(Sentence-BERT)](https://github.com/shibing624/text2vec/blob/master/text2vec/sentencebert_model.py): A sentence vector representation model that balances performance and efficiency, and supervises the upper layer during training Classification function, direct sentence vector as cosine when text matching prediction, this project reproduces the training and prediction of Sentence-BERT model based on PyTorch\n- [CoSENT(Cosine Sentence)](https://github.com/shibing624/text2vec/blob/master/text2vec/cosent_model.py): The CoSENT model proposes a sorted loss function to make the training process closer to the prediction, The model convergence speed and effect are better than Sentence-BERT. This project implements the training and prediction of the CoSENT model based on PyTorch\n# Evaluation\n\n### 文本匹配\n\n- 英文匹配数据集的评测结果：\n\n| Arch | Backbone | Model Name | English-STS-B | \n| :-- | :--- | :--- | :-: |\n| GloVe | glove | Avg_word_embeddings_glove_6B_300d | 61.77 |\n| BERT | bert-base-uncased | BERT-base-cls | 20.29 |\n| BERT | bert-base-uncased | BERT-base-first_last_avg | 59.04 |\n| BERT | bert-base-uncased | BERT-base-first_last_avg-whiten(NLI) | 63.65 |\n| SBERT | sentence-transformers/bert-base-nli-mean-tokens | SBERT-base-nli-cls | 73.65 |\n| SBERT | sentence-transformers/bert-base-nli-mean-tokens | SBERT-base-nli-first_last_avg | 77.96 |\n| SBERT | xlm-roberta-base | paraphrase-multilingual-MiniLM-L12-v2 | 84.42 |\n| CoSENT | bert-base-uncased | CoSENT-base-first_last_avg | 69.93 |\n| CoSENT | sentence-transformers/bert-base-nli-mean-tokens | CoSENT-base-nli-first_last_avg | 79.68 |\n\n- Evaluation results of Chinese matching dataset:\n\n| Arch | Backbone | Model Name | ATEC | BQ | LCQMC | PAWSX | STS-B | Avg | QPS |\n| :-- | :--- | :--- | :-: | :-: | :-: | :-: | :-: | :-: | :-: |\n| CoSENT | hfl/chinese-macbert-base | CoSENT-macbert-base | 50.39 | **72.93** | **79.17** | **60.86** | **80.51** | **68.77**  | 3008 |\n| CoSENT | Langboat/mengzi-bert-base | CoSENT-mengzi-base | **50.52** | 72.27 | 78.69 | 12.89 | 80.15 | 58.90 | 2502 |\n| CoSENT | bert-base-chinese | CoSENT-bert-base | 49.74 | 72.38 | 78.69 | 60.00 | 80.14 | 68.19 | 2653 |\n| SBERT | bert-base-chinese | SBERT-bert-base | 46.36 | 70.36 | 78.72 | 46.86 | 66.41 | 61.74 | 3365 |\n| SBERT | hfl/chinese-macbert-base | SBERT-macbert-base | 47.28 | 68.63 | **79.42** | 55.59 | 64.82 | 63.15 | 2948 |\n| CoSENT | hfl/chinese-roberta-wwm-ext | CoSENT-roberta-ext | **50.81** | **71.45** | **79.31** | **61.56** | **81.13** | **68.85** | - |\n| SBERT | hfl/chinese-roberta-wwm-ext | SBERT-roberta-ext | 48.29 | 69.99 | 79.22 | 44.10 | 72.42 | 62.80 | - |\n\n- Chinese matching evaluation results of the release model of this project:\n\n| Arch | Backbone | Model Name | ATEC  |  BQ   | LCQMC | PAWSX | STS-B |    Avg    | QPS |\n| :-- | :--- | :---- |:-----:|:-----:|:-----:|:-----:|:-----:|:---------:| :-: |\n| Word2Vec | word2vec | w2v-light-tencent-chinese | 20.00 | 31.49 | 59.46 | 2.57  | 55.78 |   33.86   | 23769 |\n| SBERT | xlm-roberta-base | paraphrase-multilingual-MiniLM-L12-v2 | 18.42 | 38.52 | 63.96 | 10.14 | 78.90 |   41.99   | 3138 |\n| CoSENT | hfl/chinese-macbert-base | [shibing624/text2vec-base-chinese](https://huggingface.co/shibing624/text2vec-base-chinese) | 31.93 | 42.67 | 70.16 | 17.21 | 79.30 | **48.25** | 3008 |\n| CoSENT | hfl/chinese-lert-large | [GanymedeNil/text2vec-large-chinese](https://huggingface.co/GanymedeNil/text2vec-large-chinese) | 32.61 | 44.59 | 69.30 | 14.51 | 79.44 |   48.08   | 1046 |\n\n\nEvaluation conclusion:\n- The result values are all using the spearman coefficient\n- The results only use the train training of the data set, and evaluate the performance obtained on the test, without using external data\n- [shibing624/text2vec-base-chinese](https://huggingface.co/shibing624/text2vec-base-chinese) model is trained with CoSENT method, based on MacBERT in Chinese STS-B data training, and in Chinese STS -B test set evaluation reaches SOTA, run [examples/training_sup_text_matching_model.py](https://github.com/shibing624/text2vec/blob/master/examples/training_sup_text_matching_model.py) code to train the model, the model file has been uploaded to huggingface The model library [shibing624/text2vec-base-chinese](https://huggingface.co/shibing624/text2vec-base-chinese), recommended for Chinese semantic matching tasks\n- The `SBERT-macbert-base` model is trained with the SBERT method, and the code can be trained by running [examples/training_sup_text_matching_model.py](https://github.com/shibing624/text2vec/blob/master/examples/training_sup_text_matching_model.py) Model\n- `paraphrase-multilingual-MiniLM-L12-v2` model name is [sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2](https://huggingface.co/sentence-transformers/paraphrase-multilingual-MiniLM-L12 -v2), trained with SBERT, is a multilingual version of the `paraphrase-MiniLM-L12-v2` model, supports Chinese, English, etc.\n- `w2v-light-tencent-chinese` is the Word2Vec model of Tencent word vectors, which is loaded and used by CPU, and is suitable for Chinese literal matching tasks and cold start situations with lack of data\n- Each pre-trained model can be called through transformers, such as MacBERT model: `--model_name hfl/chinese-macbert-base` or roberta model: `--model_name uer/roberta-medium-wwm-chinese-cluecorpussmall`\n- Chinese matching data set download [link below] (#data set)\n- The Chinese matching task experiment shows that the optimal pooling is `first_last_avg`, that is, `EncoderType.FIRST_LAST_AVG` of SentenceModel, which has little difference in prediction effect from the method of `EncoderType.MEAN`\n- Chinese matching evaluation results are reappearing, you can download the Chinese matching dataset to `examples/data`, run [tests/test_model_spearman.py](https://github.com/shibing624/text2vec/blob/master/tests/test_model_spearman. py) code to reproduce the evaluation results\n- The GPU test environment of QPS is Tesla V100 with 32GB of video memory\n# Demo\n\nOfficial Demo: https://www.mulanai.com/product/short_text_sim/\n\nHuggingFace Demo: https://huggingface.co/spaces/shibing624/text2vec\n\n![](docs/hf.png)\n\nrun example: [examples/gradio_demo.py](https://github.com/shibing624/text2vec/blob/master/examples/gradio_demo.py) to see the demo:\n```shell\npython examples/gradio_demo.py\n```\n\n# Install\n```shell\npip install torch # conda install pytorch\npip install -U text2vec\n```\n\nor\n\n```shell\npip install torch # conda install pytorch\npip install -r requirements.txt\n\ngit clone https://github.com/shibing624/text2vec.git\ncd text2vec\npip install --no-deps .\n```\n\n# Usage\n\n## 文本向量表征\n\nCompute text vectors based on `pretrained model`:\n\n```zsh\n>>> from text2vec import SentenceModel\n>>> m = SentenceModel()\n>>> m.encode(\"如何更换花呗绑定银行卡\")\nEmbedding shape: (768,)\n```\n\nexample: [examples/computing_embeddings_demo.py](https://github.com/shibing624/text2vec/blob/master/examples/computing_embeddings_demo.py)\n\n```python\nimport sys\n\nsys.path.append('..')\nfrom text2vec import SentenceModel\nfrom text2vec import Word2Vec\n\n\ndef compute_emb(model):\n    # Embed a list of sentences\n    sentences = [\n        '卡',\n        '银行卡',\n        '如何更换花呗绑定银行卡',\n        '花呗更改绑定银行卡',\n        'This framework generates embeddings for each input sentence',\n        'Sentences are passed as a list of string.',\n        'The quick brown fox jumps over the lazy dog.'\n    ]\n    sentence_embeddings = model.encode(sentences)\n    print(type(sentence_embeddings), sentence_embeddings.shape)\n\n    # The result is a list of sentence embeddings as numpy arrays\n    for sentence, embedding in zip(sentences, sentence_embeddings):\n        print(\"Sentence:\", sentence)\n        print(\"Embedding shape:\", embedding.shape)\n        print(\"Embedding head:\", embedding[:10])\n        print()\n\n\nif __name__ == \"__main__\":\n    # 中文句向量模型(CoSENT)，中文语义匹配任务推荐，支持fine-tune继续训练\n    t2v_model = SentenceModel(\"shibing624/text2vec-base-chinese\")\n    compute_emb(t2v_model)\n\n    # 支持多语言的句向量模型（Sentence-BERT），英文语义匹配任务推荐，支持fine-tune继续训练\n    sbert_model = SentenceModel(\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\")\n    compute_emb(sbert_model)\n\n    # 中文词向量模型(word2vec)，中文字面匹配任务和冷启动适用\n    w2v_model = Word2Vec(\"w2v-light-tencent-chinese\")\n    compute_emb(w2v_model)\n\n```\n\noutput:\n```\n<class 'numpy.ndarray'> (7, 768)\nSentence: 卡\nEmbedding shape: (768,)\n\nSentence: 银行卡\nEmbedding shape: (768,)\n ... \n```\n\n- The return value `embeddings` is of `numpy.ndarray` type, and the shape is `(sentences_size, model_embedding_size)`. You can choose one of the three models, and the first one is recommended.\n- The `shibing624/text2vec-base-chinese` model is trained by the CoSENT method on the Chinese STS-B dataset, and the model has been uploaded to huggingface\nModel library [shibing624/text2vec-base-chinese](https://huggingface.co/shibing624/text2vec-base-chinese),\nIt is the default model specified by `text2vec.SentenceModel`, which can be called by the above example, or by [transformers library](https://github.com/huggingface/transformers) as shown below,\nThe model is automatically downloaded to the local path: `~/.cache/huggingface/transformers`\n- The `sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2` model is a multilingual sentence vector model of Sentence-BERT,\nSuitable for paraphrase recognition and text matching, the model can be called through `text2vec.SentenceModel` and [sentence-transformers library]((https://github.com/UKPLab/sentence-transformers))\n- `w2v-light-tencent-chinese` is a Word2Vec model loaded by gensim, using the Tencent word vector `Tencent_AILab_ChineseEmbedding.tar.gz` to calculate the word vector of each word, and the sentence vector through the word word\nThe average value of the vector is obtained, and the model is automatically downloaded to the local path: `~/.text2vec/datasets/light_Tencent_AILab_ChineseEmbedding.bin`\n#### Usage (HuggingFace Transformers)\nWithout [text2vec](https://github.com/shibing624/text2vec), you can use the model like this: \n\nFirst, you pass your input through the transformer model, then you have to apply the right pooling-operation on-top of the contextualized word embeddings.\n\nexample: [examples/use_origin_transformers_demo.py](https://github.com/shibing624/text2vec/blob/master/examples/use_origin_transformers_demo.py)\n\n```python\nimport os\nimport torch\nfrom transformers import AutoTokenizer, AutoModel\n\nos.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"\n\n\n# Mean Pooling - Take attention mask into account for correct averaging\ndef mean_pooling(model_output, attention_mask):\n    token_embeddings = model_output[0]  # First element of model_output contains all token embeddings\n    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n\n\n# Load model from HuggingFace Hub\ntokenizer = AutoTokenizer.from_pretrained('shibing624/text2vec-base-chinese')\nmodel = AutoModel.from_pretrained('shibing624/text2vec-base-chinese')\nsentences = ['如何更换花呗绑定银行卡', '花呗更改绑定银行卡']\n# Tokenize sentences\nencoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')\n\n# Compute token embeddings\nwith torch.no_grad():\n    model_output = model(**encoded_input)\n# Perform pooling. In this case, max pooling.\nsentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\nprint(\"Sentence embeddings:\")\nprint(sentence_embeddings)\n```\n\n#### Usage (sentence-transformers)\n[sentence-transformers](https://github.com/UKPLab/sentence-transformers) is a popular library to compute dense vector representations for sentences.\n\nInstall sentence-transformers:\n```shell\npip install -U sentence-transformers\n```\nThen load model and predict:\n```python\nfrom sentence_transformers import SentenceTransformer\n\nm = SentenceTransformer(\"shibing624/text2vec-base-chinese\")\nsentences = ['如何更换花呗绑定银行卡', '花呗更改绑定银行卡']\n\nsentence_embeddings = m.encode(sentences)\nprint(\"Sentence embeddings:\")\nprint(sentence_embeddings)\n```\n\n#### `Word2Vec` word vector model\nTwo `Word2Vec` word vectors are provided, choose one:\n\n   - Lightweight version of Tencent Word Vector [Baidu Cloud Disk-Password: tawe](https://pan.baidu.com/s/1La4U4XNFe8s5BJqxPQpeiQ) or [Google Cloud Disk](https://drive.google.com/u/ 0/uc?id=1iQo9tBb2NgFOBxx0fA16AZpSgc-bG_Rp&export=download), binary file, 111M, is a simplified high-frequency 143613 words, each word vector is still 200 dimensions (same as the original version), run the program, and automatically download to `~/ .text2vec/datasets/light_Tencent_AILab_ChineseEmbedding.bin`\n   - Tencent word vector - official full volume, 6.78G put: `~/.text2vec/datasets/Tencent_AILab_ChineseEmbedding.txt`, Tencent word vector homepage: https://ai.tencent.com/ailab/nlp/zh/index.html Word vector download address: https://ai.tencent.com/ailab/nlp/en/download.html For more information, see [Tencent Word Vector Introduction-wiki](https://github.com/shibing624/text2vec/wiki/ %E8%85%BE%E8%AE%AF%E8%AF%8D%E5%90%91%E9%87%8F%E4%BB%8B%E7%BB%8D)\n\n## Downstream tasks\n### 1. Sentence similarity calculation\n\nexample: [examples/semantic_text_similarity_demo.py](https://github.com/shibing624/text2vec/blob/master/examples/semantic_text_similarity_demo.py)\n\n```python\nimport sys\n\nsys.path.append('..')\nfrom text2vec import Similarity\n\n# Two lists of sentences\nsentences1 = ['如何更换花呗绑定银行卡',\n              'The cat sits outside',\n              'A man is playing guitar',\n              'The new movie is awesome']\n\nsentences2 = ['花呗更改绑定银行卡',\n              'The dog plays in the garden',\n              'A woman watches TV',\n              'The new movie is so great']\n\nsim_model = Similarity()\nfor i in range(len(sentences1)):\n    for j in range(len(sentences2)):\n        score = sim_model.get_score(sentences1[i], sentences2[j])\n        print(\"{} \\t\\t {} \\t\\t Score: {:.4f}\".format(sentences1[i], sentences2[j], score))\n```\n\noutput:\n```shell\n如何更换花呗绑定银行卡 \t\t 花呗更改绑定银行卡 \t\t Score: 0.9477\n如何更换花呗绑定银行卡 \t\t The dog plays in the garden \t\t Score: -0.1748\n如何更换花呗绑定银行卡 \t\t A woman watches TV \t\t Score: -0.0839\n如何更换花呗绑定银行卡 \t\t The new movie is so great \t\t Score: -0.0044\nThe cat sits outside \t\t 花呗更改绑定银行卡 \t\t Score: -0.0097\nThe cat sits outside \t\t The dog plays in the garden \t\t Score: 0.1908\nThe cat sits outside \t\t A woman watches TV \t\t Score: -0.0203\nThe cat sits outside \t\t The new movie is so great \t\t Score: 0.0302\nA man is playing guitar \t\t 花呗更改绑定银行卡 \t\t Score: -0.0010\nA man is playing guitar \t\t The dog plays in the garden \t\t Score: 0.1062\nA man is playing guitar \t\t A woman watches TV \t\t Score: 0.0055\nA man is playing guitar \t\t The new movie is so great \t\t Score: 0.0097\nThe new movie is awesome \t\t 花呗更改绑定银行卡 \t\t Score: 0.0302\nThe new movie is awesome \t\t The dog plays in the garden \t\t Score: -0.0160\nThe new movie is awesome \t\t A woman watches TV \t\t Score: 0.1321\nThe new movie is awesome \t\t The new movie is so great \t\t Score: 0.9591\n```\n\n> Sentence cosine similarity value `score` ranges from [-1, 1], the larger the value, the more similar it is.\n\n### 2. Text matching search\n\nGenerally, the text that is most similar to the query is found in the document candidate set, which is often used in tasks such as question similarity matching and text similarity retrieval in QA scenarios.\n\n\nexample: [examples/semantic_search_demo.py](https://github.com/shibing624/text2vec/blob/master/examples/semantic_search_demo.py)\n\n```python\nimport sys\n\nsys.path.append('..')\nfrom text2vec import SentenceModel, cos_sim, semantic_search\n\nembedder = SentenceModel()\n\n# Corpus with example sentences\ncorpus = [\n    '花呗更改绑定银行卡',\n    '我什么时候开通了花呗',\n    'A man is eating food.',\n    'A man is eating a piece of bread.',\n    'The girl is carrying a baby.',\n    'A man is riding a horse.',\n    'A woman is playing violin.',\n    'Two men pushed carts through the woods.',\n    'A man is riding a white horse on an enclosed ground.',\n    'A monkey is playing drums.',\n    'A cheetah is running behind its prey.'\n]\ncorpus_embeddings = embedder.encode(corpus)\n\n# Query sentences:\nqueries = [\n    '如何更换花呗绑定银行卡',\n    'A man is eating pasta.',\n    'Someone in a gorilla costume is playing a set of drums.',\n    'A cheetah chases prey on across a field.']\n\nfor query in queries:\n    query_embedding = embedder.encode(query)\n    hits = semantic_search(query_embedding, corpus_embeddings, top_k=5)\n    print(\"\\n\\n======================\\n\\n\")\n    print(\"Query:\", query)\n    print(\"\\nTop 5 most similar sentences in corpus:\")\n    hits = hits[0]  # Get the hits for the first query\n    for hit in hits:\n        print(corpus[hit['corpus_id']], \"(Score: {:.4f})\".format(hit['score']))\n```\noutput:\n```shell\nQuery: 如何更换花呗绑定银行卡\nTop 5 most similar sentences in corpus:\n花呗更改绑定银行卡 (Score: 0.9477)\n我什么时候开通了花呗 (Score: 0.3635)\nA man is eating food. (Score: 0.0321)\nA man is riding a horse. (Score: 0.0228)\nTwo men pushed carts through the woods. (Score: 0.0090)\n\n======================\nQuery: A man is eating pasta.\nTop 5 most similar sentences in corpus:\nA man is eating food. (Score: 0.6734)\nA man is eating a piece of bread. (Score: 0.4269)\nA man is riding a horse. (Score: 0.2086)\nA man is riding a white horse on an enclosed ground. (Score: 0.1020)\nA cheetah is running behind its prey. (Score: 0.0566)\n\n======================\nQuery: Someone in a gorilla costume is playing a set of drums.\nTop 5 most similar sentences in corpus:\nA monkey is playing drums. (Score: 0.8167)\nA cheetah is running behind its prey. (Score: 0.2720)\nA woman is playing violin. (Score: 0.1721)\nA man is riding a horse. (Score: 0.1291)\nA man is riding a white horse on an enclosed ground. (Score: 0.1213)\n\n======================\nQuery: A cheetah chases prey on across a field.\nTop 5 most similar sentences in corpus:\nA cheetah is running behind its prey. (Score: 0.9147)\nA monkey is playing drums. (Score: 0.2655)\nA man is riding a horse. (Score: 0.1933)\nA man is riding a white horse on an enclosed ground. (Score: 0.1733)\nA man is eating food. (Score: 0.0329)\n```\n\n\n## Downstream task support library\n**similarities library [recommended]**\n\nFor text similarity calculation and text matching search tasks, it is recommended to use [similarities library](https://github.com/shibing624/similarities), which is compatible with the release of this project\nWord2vec, SBERT, and Cosent semantic matching models also support literal dimension similarity calculations, matching search algorithms, and text and images.\n\nInstall:\n```pip install -U similarities```\n\nSentence similarity calculation:\n```python\nfrom similarities import Similarity\n\nm = Similarity()\nr = m.similarity('如何更换花呗绑定银行卡', '花呗更改绑定银行卡')\nprint(f\"similarity score: {float(r)}\")  # similarity score: 0.855146050453186\n```\n\n# Models\n\n## CoSENT model\n\nCoSENT (Cosine Sentence) text matching model, improved the sentence vector scheme of CosineRankLoss on Sentence-BERT\n\n\nNetwork structure:\n\nTraining:\n\n<img src=\"docs/cosent_train.png\" width=\"300\" />\n\n\nInference:\n\n<img src=\"docs/inference.png\" width=\"300\" />\n\n#### CoSENT Supervised Model\nTrain and predict CoSENT model:\n\n- Train and evaluate the `CoSENT` model on the Chinese STS-B dataset\n\nexample: [examples/training_sup_text_matching_model.py](https://github.com/shibing624/text2vec/blob/master/examples/training_sup_text_matching_model.py)\n\n```shell\ncd examples\npython training_sup_text_matching_model.py --model_arch cosent --do_train --do_predict --num_epochs 10 --model_name hfl/chinese-macbert-base --output_dir ./outputs/STS-B-cosent\n```\n\n- Train and evaluate the `CoSENT` model on the Ant Financial matching dataset ATEC\n\nSupport the use of these Chinese matching datasets: 'ATEC', 'STS-B', 'BQ', 'LCQMC', 'PAWSX', for details refer to HuggingFace datasets [https://huggingface.co/datasets/shibing624/nli_zh] (https://huggingface.co/datasets/shibing624/nli_zh)\n```shell\npython training_sup_text_matching_model.py --task_name ATEC --model_arch cosent --do_train --do_predict --num_epochs 10 --model_name hfl/chinese-macbert-base --output_dir ./outputs/ATEC-cosent\n```\n\n- Train the model on our own Chinese dataset\n\nexample: [examples/training_sup_text_matching_model_selfdata.py](https://github.com/shibing624/text2vec/blob/master/examples/training_sup_text_matching_model_selfdata.py)\n\n```shell\npython training_sup_text_matching_model_selfdata.py --do_train --do_predict\n```\n\n- Train and evaluate the `CoSENT` model on the English STS-B dataset\n\nexample: [examples/training_sup_text_matching_model_en.py](https://github.com/shibing624/text2vec/blob/master/examples/training_sup_text_matching_model_en.py)\n\n```shell\ncd examples\npython training_sup_text_matching_model_en.py --model_arch cosent --do_train --do_predict --num_epochs 10 --model_name bert-base-uncased  --output_dir ./outputs/STS-B-en-cosent\n```\n\n#### CoSENT Unsupervised Model\n- Train the `CoSENT` model on the English NLI dataset and evaluate the effect on the STS-B test set\n\nexample: [examples/training_unsup_text_matching_model_en.py](https://github.com/shibing624/text2vec/blob/master/examples/training_unsup_text_matching_model_en.py)\n\n```shell\ncd examples\npython training_unsup_text_matching_model_en.py --model_arch cosent --do_train --do_predict --num_epochs 10 --model_name bert-base-uncased --output_dir ./outputs/STS-B-en-unsup-cosent\n```\n\n\n## Sentence-BERT model\n\nSentence-BERT text matching model, representational sentence vector representation scheme\n\nNetwork structure:\n\nTraining:\n\n<img src=\"docs/sbert_train.png\" width=\"300\" />\n\n\nInference:\n\n<img src=\"docs/sbert_inference.png\" width=\"300\" />\n\n#### SentenceBERT Supervised Model\n- Train and evaluate the `SBERT` model on the Chinese STS-B dataset\n\nexample: [examples/training_sup_text_matching_model.py](https://github.com/shibing624/text2vec/blob/master/examples/training_sup_text_matching_model.py)\n\n```shell\ncd examples\npython training_sup_text_matching_model.py --model_arch sentencebert --do_train --do_predict --num_epochs 10 --model_name hfl/chinese-macbert-base --output_dir ./outputs/STS-B-sbert\n```\n- Train and evaluate the `SBERT` model on the English STS-B dataset\n\nexample: [examples/training_sup_text_matching_model_en.py](https://github.com/shibing624/text2vec/blob/master/examples/training_sup_text_matching_model_en.py)\n\n```shell\ncd examples\npython training_sup_text_matching_model_en.py --model_arch sentencebert --do_train --do_predict --num_epochs 10 --model_name bert-base-uncased --output_dir ./outputs/STS-B-en-sbert\n```\n\n#### SentenceBERT Unsupervised Model\n- Train the `SBERT` model on the English NLI dataset and evaluate the effect on the STS-B test set\n\nexample: [examples/training_unsup_text_matching_model_en.py](https://github.com/shibing624/text2vec/blob/master/examples/training_unsup_text_matching_model_en.py)\n\n```shell\ncd examples\npython training_unsup_text_matching_model_en.py --model_arch sentencebert --do_train --do_predict --num_epochs 10 --model_name bert-base-uncased --output_dir ./outputs/STS-B-en-unsup-sbert\n```\n\n## BERT-Match model\nBERT text matching model, native BERT matching network structure, interactive sentence vector matching model\n\nNetwork structure:\n\nTraining and inference:\n\n<img src=\"docs/bert-fc-train.png\" width=\"300\" />\nThe training script is the same as above [examples/training_sup_text_matching_model.py](https://github.com/shibing624/text2vec/blob/master/examples/training_sup_text_matching_model.py).\n\n\n## Model Distillation\n\nSince the model trained by text2vec can be loaded using the [sentence-transformers](https://github.com/UKPLab/sentence-transformers) library, its model distillation method [distillation](https://github.com/ UKPLab/sentence-transformers/tree/master/examples/training/distillation).\n\n1. Model dimension reduction, refer to [dimensionality_reduction.py](https://github.com/UKPLab/sentence-transformers/blob/master/examples/training/distillation/dimensionality_reduction.py) use PCA to reduce the dimensionality of the model output embedding, It can reduce the storage pressure of vector retrieval databases such as milvus and slightly improve the model effect.\n2. For model distillation, refer to [model_distillation.py](https://github.com/UKPLab/sentence-transformers/blob/master/examples/training/distillation/model_distillation.py) and use the distillation method to distill the Teacher model to In the student model with fewer layers, the prediction speed of the model can be greatly improved under the condition of weighing the effect.\n\n## Model Deployment\n\nProvide two deployment models and methods of building services: 1) Building gRPC services based on Jina [recommended]; 2) Building native Http services based on FastAPI.\n\n### Jina Service\nIt adopts C/S mode to build high-performance services, supports docker cloud native, gRPC/HTTP/WebSocket, supports simultaneous prediction of multiple models, and multi-GPU processing.\n\n- Install:\n```pip install jina```\n\n- Start the service:\n\nexample: [examples/jina_server_demo.py](examples/jina_server_demo.py)\n```python\nfrom jina import Flow\n\nport = 50001\nf = Flow(port=port).add(\n    uses='jinahub://Text2vecEncoder',\n    uses_with={'model_name': 'shibing624/text2vec-base-chinese'}\n)\n\nwith f:\n    # backend server forever\n    f.block()\n```\n\nThe model prediction method (executor) has been uploaded to [JinaHub](https://hub.jina.ai/executor/eq45c9uq), which includes docker and k8s deployment methods.\n\n- call service:\n\n\n```python\nfrom jina import Client\nfrom docarray import Document, DocumentArray\n\nport = 50001\n\nc = Client(port=port)\n\ndata = ['如何更换花呗绑定银行卡',\n        '花呗更改绑定银行卡']\nprint(\"data:\", data)\nprint('data embs:')\nr = c.post('/', inputs=DocumentArray([Document(text='如何更换花呗绑定银行卡'), Document(text='花呗更改绑定银行卡')]))\nprint(r.embeddings)\n```\n\nSee example for batch call method: [examples/jina_client_demo.py](https://github.com/shibing624/text2vec/blob/master/examples/jina_client_demo.py)\n\n\n### Fast API service\n\n- Install:\n```pip install fastapi uvicorn```\n\n- Start the service:\n\nexample: [examples/fastapi_server_demo.py](https://github.com/shibing624/text2vec/blob/master/examples/fastapi_server_demo.py)\n```shell\ncd examples\npython fastapi_server_demo.py\n```\n\n- 调用服务：\n```shell\ncurl -X 'GET' \\\n  'http://0.0.0.0:8001/emb?q=hello' \\\n  -H 'accept: application/json'\n```\n\n\n## Dataset\n\n- The data set of this project release:\n\n| Dataset           | Introduce                                                           | Download Link                                                                                                                                                                                                                                                                                         |\n|:------------------|:--------------------------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| shibing624/nli_zh | 中文语义匹配数据集，整合了中文ATEC、BQ、LCQMC、PAWSX、STS-B共5个任务的数据集                   | [https://huggingface.co/datasets/shibing624/nli_zh](https://huggingface.co/datasets/shibing624/nli_zh) </br> or </br> [百度网盘(提取码:qkt6)](https://pan.baidu.com/s/1d6jSiU1wHQAEMWJi7JJWCQ) </br> or </br> [github](https://github.com/shibing624/text2vec/releases/download/1.1.2/senteval_cn.zip) </br> |\n| ATEC              | 中文ATEC数据集，蚂蚁金服Q-Qpair数据集                                            | [ATEC](https://github.com/IceFlameWorm/NLP_Datasets/tree/master/ATEC)|\n| BQ                | 中文BQ(Bank Question)数据集，银行Q-Qpair数据集                                 | [BQ](http://icrc.hitsz.edu.cn/info/1037/1162.htm)|\n| LCQMC              | 中文LCQMC(large-scale Chinese question matching corpus)数据集，Q-Qpair数据集 | [LCQMC](http://icrc.hitsz.edu.cn/Article/show/171.html)|\n| PAWSX              | 中文PAWS(Paraphrase Adversaries from Word Scrambling)数据集，Q-Qpair数据集   | [PAWSX](https://arxiv.org/abs/1908.11828)|\n| STS-B              | 中文STS-B数据集，中文自然语言推理数据集，从英文STS-B翻译为中文的数据集                              | [STS-B](https://github.com/pluto-junzeng/CNSD)|\n\nChinese semantic matching dataset `shibing624/nli_zh`, including [ATEC](https://github.com/IceFlameWorm/NLP_Datasets/tree/master/ATEC), [BQ](http://icrc.hitsz.edu.cn /info/1037/1162.htm),\n[LCQMC](http://icrc.hitsz.edu.cn/Article/show/171.html), [PAWSX](https://arxiv.org/abs/1908.11828), [STS-B](https: //github.com/pluto-junzeng/CNSD) a total of 5 tasks.\nYou can download it yourself from the link corresponding to the dataset, or you can download it from [Baidu Netdisk (extraction code: qkt6)](https://pan.baidu.com/s/1d6jSiU1wHQAEMWJi7JJWCQ).\nAmong them, the senteval_cn directory is a summary of the evaluation data set, and senteval_cn.zip is the packaging of the senteval directory, whichever is better.\n\n\nDataset usage example:\n```shell\npip install datasets\n```\n\n```python\nfrom datasets import load_dataset\n\ndataset = load_dataset(\"shibing624/nli_zh\", \"STS-B\") # ATEC or BQ or LCQMC or PAWSX or STS-B\nprint(dataset)\nprint(dataset['test'][0])\n```\n\noutput:\n```shell\nDatasetDict({\n    train: Dataset({\n        features: ['sentence1', 'sentence2', 'label'],\n        num_rows: 5231\n    })\n    validation: Dataset({\n        features: ['sentence1', 'sentence2', 'label'],\n        num_rows: 1458\n    })\n    test: Dataset({\n        features: ['sentence1', 'sentence2', 'label'],\n        num_rows: 1361\n    })\n})\n{'sentence1': '一个女孩在给她的头发做发型。', 'sentence2': '一个女孩在梳头。', 'label': 2}\n```\n\n\n<details>\n<summary>Introduction to text vector methods</summary>\n\n# Question\nWhat about text vector representation? Which model is better for text matching tasks?\n\nThe success of many NLP tasks is inseparable from training high-quality and effective text representation vectors. Especially text semantic matching (Semantic Textual Similarity, such as paraphrase detection, QA question pair matching), text vector retrieval (Dense Text Retrieval) and other tasks.\n# Solution\n### Traditional Approach: Feature-Based Matching\n\n- Based on TF-IDF, BM25, Jaccord, SimHash, LDA and other algorithms to extract the vocabulary, topic and other features of the two texts, and then use the machine learning model (LR, xgboost) to train the classification model\n- Pros: better interpretability\n- Disadvantages: Rely on manual search for features, generalization ability is average, and due to the limitation of the number of features, the effect of the model is relatively average\n\nRepresentative model:\n- BM25\n\nThe BM25 algorithm calculates the matching score between the fields of the candidate sentence by the degree of coverage of the qurey field. The candidate with a higher score has a better matching degree with the query, and it mainly solves the problem of similarity at the lexical level.\n\n### Deep Methods: Representation-Based Matching\n- Based on the representation matching method, the two texts are processed separately in the initial stage, and the deep neural network is used to encode (encode) to obtain the text representation (embedding), and then perform similarity calculation on the two representations to obtain two text similarity\n- Advantages: The BERT-based model has achieved good performance in text representation and text matching tasks through supervised Fine-tune\n- Disadvantage: The sentence vector derived by BERT itself (without Fine-tune, averaging all word vectors) is of low quality, even inferior to the result of Glove, so it is difficult to reflect the semantic similarity of two sentences\n\n> The main reasons are:\n>\n> 1.BERT tends to encode all sentences into a small spatial region, which makes most sentence pairs have high similarity scores, even those semantically completely unrelated sentence pairs.\n>\n> 2. The aggregation phenomenon represented by the BERT sentence vector is related to the high-frequency words in the sentence. Specifically, when the sentence vector is calculated by means of the average word vector, the word vector of those high-frequency words will dominate the sentence vector, making it difficult to reflect its original semantics. When some high-frequency words are removed when calculating sentence vectors, the aggregation phenomenon can be alleviated to a certain extent, but the representation ability will be reduced.\n\nModels：\n\n- [DSSM(2013)](https://posenhuang.github.io/papers/cikm2013_DSSM_fullversion.pdf)\n- [CDSSM(2014)](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/www2014_cdssm_p07.pdf)\n- [ARC I(2014)](https://arxiv.org/pdf/1503.03244.pdf)\n- [Siamese Network(2016)](https://www.aclweb.org/anthology/W16-1617.pdf)\n- [InferSent(2017)](https://arxiv.org/pdf/1705.02364.pdf)\n- [BERT(2018)](https://arxiv.org/pdf/1810.04805.pdf)\n- [Sentence-BERT(2019)](https://arxiv.org/abs/1908.10084)\n- [BERT-flow(2020)](https://arxiv.org/abs/2011.05864)\n- [SimCSE(2021)](https://arxiv.org/abs/2104.08821)\n- [ConSERT(2021)](https://aclanthology.org/2021.acl-long.393/)\n- [CoSENT(2022)](https://kexue.fm/archives/8847)\n\nSince the BERT model brought about earth-shaking changes in the NLP industry in 2018, the models before 2018 will not be discussed and compared here (if you are interested in learning, you can refer to the open source [MatchZoo] of the Chinese Academy of Sciences (https://github.com /NTMC-Community/MatchZoo) and [MatchZoo-py](https://github.com/NTMC-Community/MatchZoo-py)).\n\nTherefore, this project mainly investigates the following vector representation models that are better than native BERT and suitable for text matching: Sentence-BERT (2019), BERT-flow (2020), SimCSE (2021), CoSENT (2022).\n\n### Deep Approach: Interaction-Based Matching\n\n- Based on the interactive matching method, it is believed that calculating the similarity of the text in the final stage will be too dependent on the quality of the text representation, and will also lose the basic text features (such as morphology, syntax, etc.), so it is proposed to match the text as early as possible. Features interact, capture more basic features, and finally calculate a matching score based on these basic matching features at a high level\n- Advantages: end-to-end processing of interaction-based matching model, good effect\n- Disadvantage: The input requirement of this type of model (Cross-Encoder) is two sentences, and the output is the similarity value of the sentence pair. The model will not generate sentence embedding (sentence embedding), and we cannot input a single sentence to the model . Therefore, such models are not practical for tasks that require text vector representations\n\n\nModels:\n\n- [ARC II(2014)](https://arxiv.org/pdf/1503.03244.pdf)\n- [MV-LSTM(2015)](https://arxiv.org/pdf/1511.08277.pdf)\n- [MatchPyramid(2016)](https://arxiv.org/pdf/1602.06359.pdf)\n- [DRMM(2016)](https://www.bigdatalab.ac.cn/~gjf/papers/2016/CIKM2016a_guo.pdf)\n- [Conv-KNRM(2018)](https://www.cs.cmu.edu/~zhuyund/papers/WSDM_2018_Dai.pdf)\n- [RE2(2019)](https://www.aclweb.org/anthology/P19-1465.pdf)\n- [Keyword-BERT(2020)](https://arxiv.org/ftp/arxiv/papers/2003/2003.11516.pdf)\n\nCross-Encoder is suitable for vector retrieval fine sorting.\n\n</details>\n\n\n\n# Contact\n\n- Issue (suggestion): [![GitHub issues](https://img.shields.io/github/issues/shibing624/text2vec.svg)](https://github.com/shibing624/text2vec/issues)\n- Email me: xuming: xuming624@qq.com\n- WeChat me: Add me *Wechat ID: xuming624, Remarks: Name-Company-NLP* Enter the NLP exchange group.\n\n<img src=\"docs/wechat.jpeg\" width=\"200\" />\n\n\n# Citation\n\nIf you use text2vec in your research, please cite it in the following format:\n\nAPA:\n```latex\nXu, M. Text2vec: Text to vector toolkit (Version 1.1.2) [Computer software]. https://github.com/shibing624/text2vec\n```\n\nBibTeX:\n```latex\n@misc{Text2vec,\n  author = {Xu, Ming},\n  title = {Text2vec: Text to vector toolkit},\n  year = {2022},\n  publisher = {GitHub},\n  journal = {GitHub repository},\n  howpublished = {\\url{https://github.com/shibing624/text2vec}},\n}\n```\n\n# License\nThe authorization agreement is [The Apache License 2.0](LICENSE), which can be used for commercial purposes free of charge. Please attach text2vec's link and license agreement in the product description.\n\n\n# Contribute\nThe project code is still very rough. If you have improved the code, you are welcome to submit it back to this project. Before submitting, please pay attention to the following two points:\n\n  - Add corresponding unit tests in `tests`\n  - Use `python -m pytest -v` to run all unit tests to ensure that all unit tests are passed\n\nThen you can submit a PR.\n\n# Reference\n- [将句子表示为向量（上）：无监督句子表示学习（sentence embedding）](https://www.cnblogs.com/llhthinker/p/10335164.html)\n- [将句子表示为向量（下）：无监督句子表示学习（sentence embedding）](https://www.cnblogs.com/llhthinker/p/10341841.html)\n- [A Simple but Tough-to-Beat Baseline for Sentence Embeddings[Sanjeev Arora and Yingyu Liang and Tengyu Ma, 2017]](https://openreview.net/forum?id=SyK00v5xx)\n- [四种计算文本相似度的方法对比[Yves Peirsman]](https://zhuanlan.zhihu.com/p/37104535)\n- [Improvements to BM25 and Language Models Examined](http://www.cs.otago.ac.nz/homepages/andrew/papers/2014-2.pdf)\n- [CoSENT：比Sentence-BERT更有效的句向量方案](https://kexue.fm/archives/8847)\n- [谈谈文本匹配和多轮检索](https://zhuanlan.zhihu.com/p/111769969)\n- [Sentence-transformers](https://www.sbert.net/examples/applications/computing-embeddings/README.html)\n"
        },
        {
          "name": "docs",
          "type": "tree",
          "content": null
        },
        {
          "name": "examples",
          "type": "tree",
          "content": null
        },
        {
          "name": "requirements.txt",
          "type": "blob",
          "size": 0.0703125,
          "content": "jieba>=0.39\nloguru\ntransformers>=4.6.0\ndatasets\ntqdm\nscikit-learn\npandas"
        },
        {
          "name": "setup.py",
          "type": "blob",
          "size": 1.6142578125,
          "content": "# -*- coding: utf-8 -*-\nimport sys\n\nfrom setuptools import setup, find_packages\n\n# Avoids IDE errors, but actual version is read from version.py\n__version__ = \"\"\nexec(open('text2vec/version.py').read())\n\nif sys.version_info < (3,):\n    sys.exit('Sorry, Python3 is required.')\n\nwith open('README.md', 'r', encoding='utf-8') as f:\n    readme = f.read()\n\nsetup(\n    name='text2vec',\n    version=__version__,\n    description='Text to vector Tool, encode text',\n    long_description=readme,\n    long_description_content_type='text/markdown',\n    author='XuMing',\n    author_email='xuming624@qq.com',\n    url='https://github.com/shibing624/text2vec',\n    license=\"Apache License 2.0\",\n    zip_safe=False,\n    python_requires=\">=3.6.0\",\n    entry_points={\"console_scripts\": [\"text2vec = text2vec.cli:main\"]},\n    classifiers=[\n        \"Development Status :: 5 - Production/Stable\",\n        \"Intended Audience :: Developers\",\n        \"Intended Audience :: Education\",\n        \"Intended Audience :: Science/Research\",\n        \"License :: OSI Approved :: Apache Software License\",\n        \"Operating System :: OS Independent\",\n        \"Programming Language :: Python :: 3\",\n        \"Topic :: Scientific/Engineering :: Artificial Intelligence\",\n    ],\n    keywords='word embedding,text2vec,Chinese Text Similarity Calculation Tool,similarity,word2vec',\n    install_requires=[\n        \"jieba\",\n        \"loguru\",\n        \"transformers\",\n        \"datasets\",\n        \"tqdm\",\n        \"scikit-learn\",\n        \"pandas\",\n    ],\n    packages=find_packages(exclude=['tests']),\n    package_dir={'text2vec': 'text2vec'},\n    package_data={'text2vec': ['*.*', 'data/*.txt']}\n)\n"
        },
        {
          "name": "tests",
          "type": "tree",
          "content": null
        },
        {
          "name": "text2vec",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}