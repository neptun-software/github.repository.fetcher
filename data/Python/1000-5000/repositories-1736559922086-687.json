{
  "metadata": {
    "timestamp": 1736559922086,
    "page": 687,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjY5MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "dataabc/weibo-crawler",
      "stars": 3569,
      "defaultBranch": "master",
      "files": [
        {
          "name": ".dockerignore",
          "type": "blob",
          "size": 7.1328125,
          "content": "### Project ###\n.dockerignore\n.git*\njs.json\nlog/\nweibo/\n\n# Created by https://www.toptal.com/developers/gitignore/api/python,jetbrains,visualstudiocode,windows,macos,linux\n# Edit at https://www.toptal.com/developers/gitignore?templates=python,jetbrains,visualstudiocode,windows,macos,linux\n\n### JetBrains ###\n# Covers JetBrains IDEs: IntelliJ, RubyMine, PhpStorm, AppCode, PyCharm, CLion, Android Studio, WebStorm and Rider\n# Reference: https://intellij-support.jetbrains.com/hc/en-us/articles/206544839\n\n# User-specific stuff\n.idea/**/workspace.xml\n.idea/**/tasks.xml\n.idea/**/usage.statistics.xml\n.idea/**/dictionaries\n.idea/**/shelf\n\n# AWS User-specific\n.idea/**/aws.xml\n\n# Generated files\n.idea/**/contentModel.xml\n\n# Sensitive or high-churn files\n.idea/**/dataSources/\n.idea/**/dataSources.ids\n.idea/**/dataSources.local.xml\n.idea/**/sqlDataSources.xml\n.idea/**/dynamic.xml\n.idea/**/uiDesigner.xml\n.idea/**/dbnavigator.xml\n\n# Gradle\n.idea/**/gradle.xml\n.idea/**/libraries\n\n# Gradle and Maven with auto-import\n# When using Gradle or Maven with auto-import, you should exclude module files,\n# since they will be recreated, and may cause churn.  Uncomment if using\n# auto-import.\n# .idea/artifacts\n# .idea/compiler.xml\n# .idea/jarRepositories.xml\n# .idea/modules.xml\n# .idea/*.iml\n# .idea/modules\n# *.iml\n# *.ipr\n\n# CMake\ncmake-build-*/\n\n# Mongo Explorer plugin\n.idea/**/mongoSettings.xml\n\n# File-based project format\n*.iws\n\n# IntelliJ\nout/\n\n# mpeltonen/sbt-idea plugin\n.idea_modules/\n\n# JIRA plugin\natlassian-ide-plugin.xml\n\n# Cursive Clojure plugin\n.idea/replstate.xml\n\n# SonarLint plugin\n.idea/sonarlint/\n\n# Crashlytics plugin (for Android Studio and IntelliJ)\ncom_crashlytics_export_strings.xml\ncrashlytics.properties\ncrashlytics-build.properties\nfabric.properties\n\n# Editor-based Rest Client\n.idea/httpRequests\n\n# Android studio 3.1+ serialized cache file\n.idea/caches/build_file_checksums.ser\n\n### JetBrains Patch ###\n# Comment Reason: https://github.com/joeblau/gitignore.io/issues/186#issuecomment-215987721\n\n# *.iml\n# modules.xml\n# .idea/misc.xml\n# *.ipr\n\n# Sonarlint plugin\n# https://plugins.jetbrains.com/plugin/7973-sonarlint\n.idea/**/sonarlint/\n\n# SonarQube Plugin\n# https://plugins.jetbrains.com/plugin/7238-sonarqube-community-plugin\n.idea/**/sonarIssues.xml\n\n# Markdown Navigator plugin\n# https://plugins.jetbrains.com/plugin/7896-markdown-navigator-enhanced\n.idea/**/markdown-navigator.xml\n.idea/**/markdown-navigator-enh.xml\n.idea/**/markdown-navigator/\n\n# Cache file creation bug\n# See https://youtrack.jetbrains.com/issue/JBR-2257\n.idea/$CACHE_FILE$\n\n# CodeStream plugin\n# https://plugins.jetbrains.com/plugin/12206-codestream\n.idea/codestream.xml\n\n### Linux ###\n*~\n\n# temporary files which can be created if a process still has a handle open of a deleted file\n.fuse_hidden*\n\n# KDE directory preferences\n.directory\n\n# Linux trash folder which might appear on any partition or disk\n.Trash-*\n\n# .nfs files are created when an open file is removed but is still being accessed\n.nfs*\n\n### macOS ###\n# General\n.DS_Store\n.AppleDouble\n.LSOverride\n\n# Icon must end with two \\r\nIcon\n\n\n# Thumbnails\n._*\n\n# Files that might appear in the root of a volume\n.DocumentRevisions-V100\n.fseventsd\n.Spotlight-V100\n.TemporaryItems\n.Trashes\n.VolumeIcon.icns\n.com.apple.timemachine.donotpresent\n\n# Directories potentially created on remote AFP share\n.AppleDB\n.AppleDesktop\nNetwork Trash Folder\nTemporary Items\n.apdisk\n\n### macOS Patch ###\n# iCloud generated files\n*.icloud\n\n### Python ###\n# Byte-compiled / optimized / DLL files\n__pycache__/\n*.py[cod]\n*$py.class\n\n# C extensions\n*.so\n\n# Distribution / packaging\n.Python\nbuild/\ndevelop-eggs/\ndist/\ndownloads/\neggs/\n.eggs/\nlib/\nlib64/\nparts/\nsdist/\nvar/\nwheels/\nshare/python-wheels/\n*.egg-info/\n.installed.cfg\n*.egg\nMANIFEST\n\n# PyInstaller\n#  Usually these files are written by a python script from a template\n#  before PyInstaller builds the exe, so as to inject date/other infos into it.\n*.manifest\n*.spec\n\n# Installer logs\npip-log.txt\npip-delete-this-directory.txt\n\n# Unit test / coverage reports\nhtmlcov/\n.tox/\n.nox/\n.coverage\n.coverage.*\n.cache\nnosetests.xml\ncoverage.xml\n*.cover\n*.py,cover\n.hypothesis/\n.pytest_cache/\ncover/\n\n# Translations\n*.mo\n*.pot\n\n# Django stuff:\n*.log\nlocal_settings.py\ndb.sqlite3\ndb.sqlite3-journal\n\n# Flask stuff:\ninstance/\n.webassets-cache\n\n# Scrapy stuff:\n.scrapy\n\n# Sphinx documentation\ndocs/_build/\n\n# PyBuilder\n.pybuilder/\ntarget/\n\n# Jupyter Notebook\n.ipynb_checkpoints\n\n# IPython\nprofile_default/\nipython_config.py\n\n# pyenv\n#   For a library or package, you might want to ignore these files since the code is\n#   intended to run in multiple environments; otherwise, check them in:\n# .python-version\n\n# pipenv\n#   According to pypa/pipenv#598, it is recommended to include Pipfile.lock in version control.\n#   However, in case of collaboration, if having platform-specific dependencies or dependencies\n#   having no cross-platform support, pipenv may install dependencies that don't work, or not\n#   install all needed dependencies.\n#Pipfile.lock\n\n# poetry\n#   Similar to Pipfile.lock, it is generally recommended to include poetry.lock in version control.\n#   This is especially recommended for binary packages to ensure reproducibility, and is more\n#   commonly ignored for libraries.\n#   https://python-poetry.org/docs/basic-usage/#commit-your-poetrylock-file-to-version-control\n#poetry.lock\n\n# pdm\n#   Similar to Pipfile.lock, it is generally recommended to include pdm.lock in version control.\n#pdm.lock\n#   pdm stores project-wide configurations in .pdm.toml, but it is recommended to not include it\n#   in version control.\n#   https://pdm.fming.dev/#use-with-ide\n.pdm.toml\n\n# PEP 582; used by e.g. github.com/David-OConnor/pyflow and github.com/pdm-project/pdm\n__pypackages__/\n\n# Celery stuff\ncelerybeat-schedule\ncelerybeat.pid\n\n# SageMath parsed files\n*.sage.py\n\n# Environments\n.env\n.venv\nenv/\nvenv/\nENV/\nenv.bak/\nvenv.bak/\n\n# Spyder project settings\n.spyderproject\n.spyproject\n\n# Rope project settings\n.ropeproject\n\n# mkdocs documentation\n/site\n\n# mypy\n.mypy_cache/\n.dmypy.json\ndmypy.json\n\n# Pyre type checker\n.pyre/\n\n# pytype static type analyzer\n.pytype/\n\n# Cython debug symbols\ncython_debug/\n\n# PyCharm\n#  JetBrains specific template is maintained in a separate JetBrains.gitignore that can\n#  be found at https://github.com/github/gitignore/blob/main/Global/JetBrains.gitignore\n#  and can be added to the global gitignore or merged into this file.  For a more nuclear\n#  option (not recommended) you can uncomment the following to ignore the entire idea folder.\n#.idea/\n\n### VisualStudioCode ###\n.vscode/\n\n# Local History for Visual Studio Code\n.history/\n\n# Built Visual Studio Code Extensions\n*.vsix\n\n### VisualStudioCode Patch ###\n# Ignore all local history of files\n.history\n.ionide\n\n# Support for Project snippet scope\n.vscode/*.code-snippets\n\n# Ignore code-workspaces\n*.code-workspace\n\n### Windows ###\n# Windows thumbnail cache files\nThumbs.db\nThumbs.db:encryptable\nehthumbs.db\nehthumbs_vista.db\n\n# Dump file\n*.stackdump\n\n# Folder config file\n[Dd]esktop.ini\n\n# Recycle Bin used on file shares\n$RECYCLE.BIN/\n\n# Windows Installer files\n*.cab\n*.msi\n*.msix\n*.msm\n*.msp\n\n# Windows shortcuts\n*.lnk\n\n# End of https://www.toptal.com/developers/gitignore/api/python,jetbrains,visualstudiocode,windows,macos,linux"
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 7.2451171875,
          "content": "### Project ###\nlog/\nweibo/\n\n# Created by https://www.toptal.com/developers/gitignore/api/python,jetbrains,visualstudiocode,windows,macos,linux\n# Edit at https://www.toptal.com/developers/gitignore?templates=python,jetbrains,visualstudiocode,windows,macos,linux\n\n### JetBrains ###\n# Covers JetBrains IDEs: IntelliJ, RubyMine, PhpStorm, AppCode, PyCharm, CLion, Android Studio, WebStorm and Rider\n# Reference: https://intellij-support.jetbrains.com/hc/en-us/articles/206544839\n\n# User-specific stuff\n.idea/**/workspace.xml\n.idea/**/tasks.xml\n.idea/**/usage.statistics.xml\n.idea/**/dictionaries\n.idea/**/shelf\n\n# AWS User-specific\n.idea/**/aws.xml\n\n# Generated files\n.idea/**/contentModel.xml\n\n# Sensitive or high-churn files\n.idea/**/dataSources/\n.idea/**/dataSources.ids\n.idea/**/dataSources.local.xml\n.idea/**/sqlDataSources.xml\n.idea/**/dynamic.xml\n.idea/**/uiDesigner.xml\n.idea/**/dbnavigator.xml\n\n# Gradle\n.idea/**/gradle.xml\n.idea/**/libraries\n\n# Gradle and Maven with auto-import\n# When using Gradle or Maven with auto-import, you should exclude module files,\n# since they will be recreated, and may cause churn.  Uncomment if using\n# auto-import.\n# .idea/artifacts\n# .idea/compiler.xml\n# .idea/jarRepositories.xml\n# .idea/modules.xml\n# .idea/*.iml\n# .idea/modules\n# *.iml\n# *.ipr\n\n# CMake\ncmake-build-*/\n\n# Mongo Explorer plugin\n.idea/**/mongoSettings.xml\n\n# File-based project format\n*.iws\n\n# IntelliJ\nout/\n\n# mpeltonen/sbt-idea plugin\n.idea_modules/\n\n# JIRA plugin\natlassian-ide-plugin.xml\n\n# Cursive Clojure plugin\n.idea/replstate.xml\n\n# SonarLint plugin\n.idea/sonarlint/\n\n# Crashlytics plugin (for Android Studio and IntelliJ)\ncom_crashlytics_export_strings.xml\ncrashlytics.properties\ncrashlytics-build.properties\nfabric.properties\n\n# Editor-based Rest Client\n.idea/httpRequests\n\n# Android studio 3.1+ serialized cache file\n.idea/caches/build_file_checksums.ser\n\n### JetBrains Patch ###\n# Comment Reason: https://github.com/joeblau/gitignore.io/issues/186#issuecomment-215987721\n\n# *.iml\n# modules.xml\n# .idea/misc.xml\n# *.ipr\n\n# Sonarlint plugin\n# https://plugins.jetbrains.com/plugin/7973-sonarlint\n.idea/**/sonarlint/\n\n# SonarQube Plugin\n# https://plugins.jetbrains.com/plugin/7238-sonarqube-community-plugin\n.idea/**/sonarIssues.xml\n\n# Markdown Navigator plugin\n# https://plugins.jetbrains.com/plugin/7896-markdown-navigator-enhanced\n.idea/**/markdown-navigator.xml\n.idea/**/markdown-navigator-enh.xml\n.idea/**/markdown-navigator/\n\n# Cache file creation bug\n# See https://youtrack.jetbrains.com/issue/JBR-2257\n.idea/$CACHE_FILE$\n\n# CodeStream plugin\n# https://plugins.jetbrains.com/plugin/12206-codestream\n.idea/codestream.xml\n\n### Linux ###\n*~\n\n# temporary files which can be created if a process still has a handle open of a deleted file\n.fuse_hidden*\n\n# KDE directory preferences\n.directory\n\n# Linux trash folder which might appear on any partition or disk\n.Trash-*\n\n# .nfs files are created when an open file is removed but is still being accessed\n.nfs*\n\n### macOS ###\n# General\n.DS_Store\n.AppleDouble\n.LSOverride\n\n# Icon must end with two \\r\nIcon\n\n\n# Thumbnails\n._*\n\n# Files that might appear in the root of a volume\n.DocumentRevisions-V100\n.fseventsd\n.Spotlight-V100\n.TemporaryItems\n.Trashes\n.VolumeIcon.icns\n.com.apple.timemachine.donotpresent\n\n# Directories potentially created on remote AFP share\n.AppleDB\n.AppleDesktop\nNetwork Trash Folder\nTemporary Items\n.apdisk\n\n### macOS Patch ###\n# iCloud generated files\n*.icloud\n\n### Python ###\n# Byte-compiled / optimized / DLL files\n__pycache__/\n*.py[cod]\n*$py.class\n\n# C extensions\n*.so\n\n# Distribution / packaging\n.Python\nbuild/\ndevelop-eggs/\ndist/\ndownloads/\neggs/\n.eggs/\nlib/\nlib64/\nparts/\nsdist/\nvar/\nwheels/\nshare/python-wheels/\n*.egg-info/\n.installed.cfg\n*.egg\nMANIFEST\n\n# PyInstaller\n#  Usually these files are written by a python script from a template\n#  before PyInstaller builds the exe, so as to inject date/other infos into it.\n*.manifest\n*.spec\n\n# Installer logs\npip-log.txt\npip-delete-this-directory.txt\n\n# Unit test / coverage reports\nhtmlcov/\n.tox/\n.nox/\n.coverage\n.coverage.*\n.cache\nnosetests.xml\ncoverage.xml\n*.cover\n*.py,cover\n.hypothesis/\n.pytest_cache/\ncover/\n\n# Translations\n*.mo\n*.pot\n\n# Django stuff:\n*.log\nlocal_settings.py\ndb.sqlite3\ndb.sqlite3-journal\n\n# Flask stuff:\ninstance/\n.webassets-cache\n\n# Scrapy stuff:\n.scrapy\n\n# Sphinx documentation\ndocs/_build/\n\n# PyBuilder\n.pybuilder/\ntarget/\n\n# Jupyter Notebook\n.ipynb_checkpoints\n\n# IPython\nprofile_default/\nipython_config.py\n\n# pyenv\n#   For a library or package, you might want to ignore these files since the code is\n#   intended to run in multiple environments; otherwise, check them in:\n# .python-version\n\n# pipenv\n#   According to pypa/pipenv#598, it is recommended to include Pipfile.lock in version control.\n#   However, in case of collaboration, if having platform-specific dependencies or dependencies\n#   having no cross-platform support, pipenv may install dependencies that don't work, or not\n#   install all needed dependencies.\n#Pipfile.lock\n\n# poetry\n#   Similar to Pipfile.lock, it is generally recommended to include poetry.lock in version control.\n#   This is especially recommended for binary packages to ensure reproducibility, and is more\n#   commonly ignored for libraries.\n#   https://python-poetry.org/docs/basic-usage/#commit-your-poetrylock-file-to-version-control\n#poetry.lock\n\n# pdm\n#   Similar to Pipfile.lock, it is generally recommended to include pdm.lock in version control.\n#pdm.lock\n#   pdm stores project-wide configurations in .pdm.toml, but it is recommended to not include it\n#   in version control.\n#   https://pdm.fming.dev/#use-with-ide\n.pdm.toml\n\n# PEP 582; used by e.g. github.com/David-OConnor/pyflow and github.com/pdm-project/pdm\n__pypackages__/\n\n# Celery stuff\ncelerybeat-schedule\ncelerybeat.pid\n\n# SageMath parsed files\n*.sage.py\n\n# Environments\n.env\n.venv\nenv/\nvenv/\nENV/\nenv.bak/\nvenv.bak/\n\n# Spyder project settings\n.spyderproject\n.spyproject\n\n# Rope project settings\n.ropeproject\n\n# mkdocs documentation\n/site\n\n# mypy\n.mypy_cache/\n.dmypy.json\ndmypy.json\n\n# Pyre type checker\n.pyre/\n\n# pytype static type analyzer\n.pytype/\n\n# Cython debug symbols\ncython_debug/\n\n# PyCharm\n#  JetBrains specific template is maintained in a separate JetBrains.gitignore that can\n#  be found at https://github.com/github/gitignore/blob/main/Global/JetBrains.gitignore\n#  and can be added to the global gitignore or merged into this file.  For a more nuclear\n#  option (not recommended) you can uncomment the following to ignore the entire idea folder.\n#.idea/\n\n### VisualStudioCode ###\n.vscode/*\n!.vscode/settings.json\n!.vscode/tasks.json\n!.vscode/launch.json\n!.vscode/extensions.json\n!.vscode/*.code-snippets\n\n# Local History for Visual Studio Code\n.history/\n\n# Built Visual Studio Code Extensions\n*.vsix\n\n### VisualStudioCode Patch ###\n# Ignore all local history of files\n.history\n.ionide\n\n# Support for Project snippet scope\n.vscode/*.code-snippets\n\n# Ignore code-workspaces\n*.code-workspace\n\n### Windows ###\n# Windows thumbnail cache files\nThumbs.db\nThumbs.db:encryptable\nehthumbs.db\nehthumbs_vista.db\n\n# Dump file\n*.stackdump\n\n# Folder config file\n[Dd]esktop.ini\n\n# Recycle Bin used on file shares\n$RECYCLE.BIN/\n\n# Windows Installer files\n*.cab\n*.msi\n*.msix\n*.msm\n*.msp\n\n# Windows shortcuts\n*.lnk\n\n# End of https://www.toptal.com/developers/gitignore/api/python,jetbrains,visualstudiocode,windows,macos,linux\n\n\njs.json\nuser_id_list.txt\n"
        },
        {
          "name": ".vscode",
          "type": "tree",
          "content": null
        },
        {
          "name": "API.md",
          "type": "blob",
          "size": 7.771484375,
          "content": "# Weibo API 说明文档\n\n本文档详细介绍了基于 Flask 框架构建的 Weibo API 的各个端点及其使用方法。API 主要用于刷新微博数据、查询任务状态以及获取微博信息。\n\n## 目录\n1. [概述](#概述)\n2. [配置说明](#配置说明)\n3. [API 端点](#api-端点)\n    - [刷新微博数据](#刷新微博数据)\n    - [查询任务状态](#查询任务状态)\n    - [获取所有微博](#获取所有微博)\n    - [获取单条微博详情](#获取单条微博详情)\n4. [定时任务](#定时任务)\n5. [错误处理](#错误处理)\n6. [日志记录](#日志记录)\n\n---\n\n## 概述\n\n该 API 旨在通过定时任务和手动触发的任务来抓取和管理微博数据。主要功能包括：\n- 刷新指定用户的微博数据\n- 查询任务的执行状态\n- 获取所有抓取到的微博\n- 获取单条微博的详细信息\n\n## 配置说明\n\n在运行 API 之前，需要配置service.py相关参数。配置项包括用户ID列表、数据库路径、日志配置、下载选项等。\n\n以下是主要配置项的说明：\n\n```python\nconfig = {\n    \"user_id_list\": [\n        \"6067225218\", \n        \"1445403190\"\n    ],\n    \"only_crawl_original\": 1,\n    \"since_date\": 1,\n    \"start_page\": 1,\n    \"write_mode\": [\n        \"csv\",\n        \"json\",\n        \"sqlite\"\n    ],\n    \"original_pic_download\": 0,\n    \"retweet_pic_download\": 0,\n    \"original_video_download\": 0,\n    \"retweet_video_download\": 0,\n    \"download_comment\": 0,\n    \"comment_max_download_count\": 100,\n    \"download_repost\": 0,\n    \"repost_max_download_count\": 100,\n    \"user_id_as_folder_name\": 0,\n    \"remove_html_tag\": 1,\n    \"cookie\": \"your weibo cookie\",\n    \"mysql_config\": {\n        \"host\": \"localhost\",\n        \"port\": 3306,\n        \"user\": \"root\",\n        \"password\": \"123456\",\n        \"charset\": \"utf8mb4\"\n    },\n    \"mongodb_URI\": \"mongodb://[username:password@]host[:port][/[defaultauthdb][?options]]\",\n    \"post_config\": {\n        \"api_url\": \"https://api.example.com\",\n        \"api_token\": \"\"\n    }\n}\n```\n\n- **user_id_list**: 需要抓取微博的用户ID列表。\n- **only_crawl_original**: 是否仅抓取原创微博（1为是，0为否）。\n- **since_date**: 起始日期。\n- **start_page**: 起始页码。\n- **write_mode**: 数据存储模式，可选择 CSV、JSON、SQLite 等。\n- **original_pic_download**: 是否下载原创微博的图片（1为是，0为否）。\n- **retweet_pic_download**: 是否下载转发微博的图片（1为是，0为否）。\n- **original_video_download**: 是否下载原创微博的视频（1为是，0为否）。\n- **retweet_video_download**: 是否下载转发微博的视频（1为是，0为否）。\n- **download_comment**: 是否下载评论（1为是，0为否）。\n- **comment_max_download_count**: 评论的最大下载数量。\n- **download_repost**: 是否下载转发信息（1为是，0为否）。\n- **repost_max_download_count**: 转发的最大下载数量。\n- **user_id_as_folder_name**: 是否使用用户ID作为文件夹名称（1为是，0为否）。\n- **remove_html_tag**: 是否移除HTML标签（1为是，0为否）。\n- **cookie**: 用于认证的微博Cookie。\n- **mysql_config**: MySQL数据库的配置信息。\n- **mongodb_URI**: MongoDB的连接URI。\n- **post_config**: POST请求的配置，包括API URL和Token。\n\n## API 端点\n\n### 刷新微博数据\n\n**URL:** `/refresh`\n\n**方法:** `POST`\n\n**描述:** 手动触发刷新指定用户的微博数据。\n\n**请求参数:**\n\n- `user_id_list` (可选): 需要刷新微博数据的用户ID列表。格式为JSON数组。例如：\n  ```json\n  {\n      \"user_id_list\": [\"6067225218\", \"1445403190\"]\n  }\n  ```\n\n**响应:**\n\n- **202 Accepted**\n  ```json\n  {\n      \"task_id\": \"任务ID\",\n      \"status\": \"Task started\",\n      \"state\": \"PENDING\",\n      \"progress\": 0,\n      \"user_id_list\": [\"6067225218\", \"1445403190\"]\n  }\n  ```\n- **400 Bad Request** (参数无效)\n  ```json\n  {\n      \"error\": \"Invalid user_id_list parameter\"\n  }\n  ```\n- **409 Conflict** (已有任务在运行)\n  ```json\n  {\n      \"task_id\": \"当前运行的任务ID\",\n      \"status\": \"Task already running\",\n      \"state\": \"PROGRESS\",\n      \"progress\": 50\n  }\n  ```\n\n### 查询任务状态\n\n**URL:** `/task/<task_id>`\n\n**方法:** `GET`\n\n**描述:** 查询指定任务的状态和进度。\n\n**URL 参数:**\n\n- `task_id` (必需): 需要查询的任务ID。\n\n**响应:**\n\n- **200 OK**\n  ```json\n  {\n      \"state\": \"任务状态\",\n      \"progress\": 进度百分比,\n      \"result\": {\n          \"message\": \"微博列表已刷新\"\n      }\n  }\n  ```\n  或者在任务失败时：\n  ```json\n  {\n      \"state\": \"FAILED\",\n      \"progress\": 50,\n      \"error\": \"错误信息\"\n  }\n  ```\n- **404 Not Found** (任务不存在)\n  ```json\n  {\n      \"error\": \"Task not found\"\n  }\n  ```\n\n### 获取所有微博\n\n**URL:** `/weibos`\n\n**方法:** `GET`\n\n**描述:** 获取数据库中所有抓取到的微博，按创建时间倒序排列。\n\n**响应:**\n\n- **200 OK**\n  ```json\n  [\n      {\n          \"id\": \"微博ID\",\n          \"content\": \"微博内容\",\n          \"created_at\": \"创建时间\",\n          ...\n      },\n      ...\n  ]\n  ```\n- **500 Internal Server Error** (服务器错误)\n  ```json\n  {\n      \"error\": \"错误信息\"\n  }\n  ```\n\n### 获取单条微博详情\n\n**URL:** `/weibos/<weibo_id>`\n\n**方法:** `GET`\n\n**描述:** 获取指定ID的微博详细信息。\n\n**URL 参数:**\n\n- `weibo_id` (必需): 需要获取的微博ID。\n\n**响应:**\n\n- **200 OK**\n  ```json\n  {\n      \"id\": \"微博ID\",\n      \"content\": \"微博内容\",\n      \"created_at\": \"创建时间\",\n      ...\n  }\n  ```\n- **404 Not Found** (微博不存在)\n  ```json\n  {\n      \"error\": \"Weibo not found\"\n  }\n  ```\n- **500 Internal Server Error** (服务器错误)\n  ```json\n  {\n      \"error\": \"错误信息\"\n  }\n  ```\n\n## 定时任务\n\nAPI 启动后，会在后台启动一个定时任务线程，每隔10分钟自动触发一次刷新任务，以确保微博数据的及时更新。如果当前有任务正在运行，定时任务会跳过本次执行。\n\n**定时任务行为:**\n- 检查是否有正在运行的任务。\n- 如果没有正在运行的任务，则创建并启动一个新的刷新任务。\n- 如果有任务在运行，则等待下一个周期。\n\n**错误处理:**\n- 如果定时任务执行过程中发生错误，会记录日志并在1分钟后重试。\n\n## 错误处理\n\nAPI 在处理请求时可能会遇到各种错误，主要包括：\n\n- **400 Bad Request:** 请求参数无效。\n- **404 Not Found:** 请求的资源不存在（如任务ID或微博ID）。\n- **409 Conflict:** 资源冲突，如已有任务在运行。\n- **500 Internal Server Error:** 服务器内部错误。\n\n错误响应的格式统一为包含 `error` 字段的 JSON 对象，例如：\n\n```json\n{\n    \"error\": \"错误信息\"\n}\n```\n\n## 日志记录\n\nAPI 使用 Python 的 `logging` 模块进行日志记录。日志文件存储在 `log/` 目录下，默认日志配置文件为 `logging.conf`。主要记录以下内容：\n\n- 服务启动和关闭日志\n- 任务的启动、完成和失败日志\n- 定时任务的执行情况\n- 异常和错误信息\n\n确保 `log/` 目录存在，API 会在启动时自动创建该目录（如果不存在）。\n\n---\n\n## 启动和运行\n\n确保所有依赖库已安装，并正确配置了 `logging.conf` 文件和数据库路径。启动 API 的命令如下：\n\n```bash\npython service.py\n```\n\nAPI 将在默认的 `5000` 端口启动，并自动开始监听请求和执行定时任务。\n\n---\n\n## 注意事项\n\n- **并发任务控制:** API 限制同一时间只能运行一个刷新任务，以避免数据冲突和资源竞争。\n- **数据存储:** 默认使用 SQLite 数据库存储微博数据，配置中可根据需要调整为其他存储方式（如 MySQL、MongoDB）。\n- **安全性:** 确保 `cookie` 和数据库的敏感信息安全存储，避免泄露。\n\n如有任何疑问或问题，请参考源代码中的注释或联系开发团队。\n"
        },
        {
          "name": "Dockerfile",
          "type": "blob",
          "size": 0.3544921875,
          "content": "# 设置基础镜像\nFROM python:3.12.0-alpine\n\n# 设置工作目录\nWORKDIR /app\n\n# 复制项目文件到工作目录\nCOPY requirements.txt .\n\n# 安装依赖包\nRUN pip install --no-cache-dir -r requirements.txt\n\n# 复制项目文件到工作目录\nCOPY . .\n\n# 设置循环间隔\nENV schedule_interval=1\n\n# 运行应用\nCMD python __main__.py $schedule_interval\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 48.16015625,
          "content": "# Weibo Crawler\n\n- [Weibo Crawler](#weibo-crawler)\n  - [功能](#功能)\n  - [输出](#输出)\n  - [实例](#实例)\n  - [运行环境](#运行环境)\n  - [使用说明](#使用说明)\n    - [1.下载脚本](#1下载脚本)\n    - [2.安装依赖](#2安装依赖)\n    - [3.程序设置](#3程序设置)\n    - [4.设置数据库（可选）](#4设置数据库可选)\n    - [5.运行脚本](#5运行脚本)\n    - [6.按需求修改脚本（可选）](#6按需求修改脚本可选)\n    - [7.定期自动爬取微博（可选）](#7定期自动爬取微博可选)\n    - [8.使用docker](#8使用docker)\n  - [如何获取user_id](#如何获取user_id)\n  - [添加cookie与不添加cookie的区别（可选）](#添加cookie与不添加cookie的区别可选)\n  - [如何获取cookie（可选）](#如何获取cookie可选)\n  - [如何检测cookie是否有效（可选）](#如何检测cookie是否有效可选)\n  - [API服务](#api服务)\n\n## 功能\n\n连续爬取**一个**或**多个**新浪微博用户（如[Dear-迪丽热巴](https://weibo.cn/u/1669879400)、[郭碧婷](https://weibo.cn/u/1729370543)）的数据，并将结果信息写入文件。写入信息几乎包括了用户微博的所有数据，主要有**用户信息**和**微博信息**两大类，前者包含用户昵称、关注数、粉丝数、微博数等等；后者包含微博正文、发布时间、发布工具、评论数等等，因为内容太多，这里不再赘述，详细内容见[输出](#输出)部分。具体的写入文件类型如下：\n\n* 写入**csv文件**（默认）\n* 写入**json文件**（可选）\n* 写入**MySQL数据库**（可选）\n* 写入**MongoDB数据库**（可选）\n* 写入**SQLite数据库**（可选）\n* 下载用户**原创**微博中的原始**图片**（可选）\n* 下载用户**转发**微博中的原始**图片**（可选）\n* 下载用户**原创**微博中的**视频**（可选）\n* 下载用户**转发**微博中的**视频**（可选）\n* 下载用户**原创**微博**Live Photo**中的**视频**（可选）\n* 下载用户**转发**微博**Live Photo**中的**视频**（可选）\n* 下载用户**原创和转发**微博下的一级评论（可选）\n* 下载用户**原创和转发**微博下的转发（可选）\n\n如果你只对用户信息感兴趣，而不需要爬用户的微博，也可以通过设置实现只爬取微博用户信息的功能。程序也可以实现**爬取结果自动更新**，即：现在爬取了目标用户的微博，几天之后，目标用户可能又发新微博了。通过设置，可以实现每隔几天**增量爬取**用户这几天发的新微博。具体方法见[定期自动爬取微博](#7定期自动爬取微博可选)。\n\n## 输出\n\n**用户信息**\n\n* 用户id：微博用户id，如\"1669879400\"\n* 用户昵称：微博用户昵称，如\"Dear-迪丽热巴\"\n* 性别：微博用户性别\n* 生日：用户出生日期\n* 所在地：用户所在地\n* 教育经历：用户上学时学校的名字\n* 公司：用户所属公司名字\n* 阳光信用：用户的阳光信用\n* 微博注册时间：用户微博注册日期\n* 微博数：用户的全部微博数（转发微博+原创微博）\n* 粉丝数：用户的粉丝数\n* 关注数：用户关注的微博数量\n* 简介：用户简介\n* 主页地址：微博移动版主页url，如<https://m.weibo.cn/u/1669879400?uid=1669879400&luicode=10000011&lfid=1005051669879400>\n* 头像url：用户头像url\n* 高清头像url：用户高清头像url\n* 微博等级：用户微博等级\n* 会员等级：微博会员用户等级，普通用户该等级为0\n* 是否认证：用户是否认证，为布尔类型\n* 认证类型：用户认证类型，如个人认证、企业认证、政府认证等\n* 认证信息：为认证用户特有，用户信息栏显示的认证信息\n\n***\n**微博信息**\n\n* 微博id：微博的id，为一串数字形式\n* 微博bid：微博的bid，与[cookie版](https://github.com/dataabc/weiboSpider)中的微博id是同一个值\n* 微博内容：微博正文\n* 头条文章url：微博中头条文章的url，如果微博中存在头条文章，就获取该头条文章的url，否则该值为''\n* 原始图片url：原创微博图片和转发微博转发理由中图片的url，若某条微博存在多张图片，则每个url以英文逗号分隔，若没有图片则值为''\n* 视频url: 微博中的视频url和Live Photo中的视频url，若某条微博存在多个视频，则每个url以英文分号分隔，若没有视频则值为''\n* 微博发布位置：位置微博中的发布位置\n* 微博发布时间：微博发布时的时间，精确到天\n* 点赞数：微博被赞的数量\n* 转发数：微博被转发的数量\n* 评论数：微博被评论的数量\n* 微博发布工具：微博的发布工具，如iPhone客户端、HUAWEI Mate 20 Pro等，若没有则值为''\n* 话题：微博话题，即两个#中的内容，若存在多个话题，每个url以英文逗号分隔，若没有则值为''\n* @用户：微博@的用户，若存在多个@用户，每个url以英文逗号分隔，若没有则值为''\n* 原始微博：为转发微博所特有，是转发微博中那条被转发的微博，存储为字典形式，包含了上述微博信息中的所有内容，如微博id、微博内容等等\n* 结果文件：保存在当前目录weibo文件夹下以用户昵称为名的文件夹里，名字为\"user_id.csv\"形式\n* 微博图片：微博中的图片，保存在以用户昵称为名的文件夹下的img文件夹里\n* 微博视频：微博中的视频，保存在以用户昵称为名的文件夹下的video文件夹里\n\n## 实例\n\n以爬取迪丽热巴的微博为例，我们需要修改**config.json**文件，文件内容如下：\n\n```\n{\n    \"user_id_list\": [\"1669879400\"],\n    \"only_crawl_original\": 1,\n    \"since_date\": \"1900-01-01\",\n    \"query_list\": [],\n    \"write_mode\": [\"csv\"],\n    \"original_pic_download\": 1,\n    \"retweet_pic_download\": 0,\n    \"original_video_download\": 1,\n    \"retweet_video_download\": 0,\n    \"cookie\": \"your cookie\"\n}\n```\n\n对于上述参数的含义以及取值范围，这里仅作简单介绍，详细信息见[程序设置](#3程序设置)。\n>**user_id_list**代表我们要爬取的微博用户的user_id，可以是一个或多个，也可以是文件路径，微博用户Dear-迪丽热巴的user_id为1669879400，具体如何获取user_id见[如何获取user_id](#如何获取user_id)；\n**only_crawl_original**的值为1代表爬取全部原创微博，值为0代表爬取全部微博（原创+转发）；\n**since_date**代表我们要爬取since_date日期之后发布的微博，因为我要爬迪丽热巴的全部原创微博，所以since_date设置了一个非常早的值；\nquery_list代表要爬取的微博关键词，为空（[]）则爬取全部；\n**write_mode**代表结果文件的保存类型，我想要把结果写入csv文件和json文件，所以它的值为[\"csv\", \"json\"]，如果你想写入数据库，具体设置见[设置数据库](#4设置数据库可选)；\n**original_pic_download**值为1代表下载原创微博中的图片，值为0代表不下载；\n**retweet_pic_download**值为1代表下载转发微博中的图片，值为0代表不下载；\n**original_video_download**值为1代表下载原创微博中的视频，值为0代表不下载；\n**retweet_video_download**值为1代表下载转发微博中的视频，值为0代表不下载；\n**cookie**是可选参数，可填可不填，具体区别见[添加cookie与不添加cookie的区别](#添加cookie与不添加cookie的区别可选)。\n\n配置完成后运行程序：\n\n```bash\npython weibo.py\n```\n\n程序会自动生成一个weibo文件夹，我们以后爬取的所有微博都被存储在weibo文件夹里。然后程序在该文件夹下生成一个名为\"Dear-迪丽热巴\"的文件夹，迪丽热巴的所有微博爬取结果都在这里。\"Dear-迪丽热巴\"文件夹里包含一个csv文件、一个img文件夹和一个video文件夹，img文件夹用来存储下载到的图片，video文件夹用来存储下载到的视频。如果你设置了保存数据库功能，这些信息也会保存在数据库里，数据库设置见[设置数据库](#4设置数据库可选)部分。\n\n**csv文件结果如下所示：**\n![](https://github.com/dataabc/media/blob/master/weibo-crawler/images/weibo_csv.png)*1669879400.csv*\n\n本csv文件是爬取“全部微博”(原创微博+转发微博)的结果文件。因为迪丽热巴很多微博本身都没有图片、发布工具、位置、话题和@用户等信息，所以当这些内容没有时对应位置为空。\"是否原创\"列用来标记是否为原创微博，\n当为转发微博时，文件中还包含转发微博的信息。为了简便起见，姑且将转发微博中被转发的原始微博称为**源微博**，它的用户id、昵称、微博id等都在名称前加上源字，以便与目标用户自己发的微博区分。对于转发微博，程序除了获取用户原创部分的信息，还会获取**源用户id**、**源用户昵称**、**源微博id**、**源微博正文**、**源微博原始图片url**、**源微博位置**、**源微博日期**、**源微博工具**、**源微博点赞数**、**源微博评论数**、**源微博转发数**、**源微博话题**、**源微博@用户**等信息。原创微博因为没有这些转发信息，所以对应位置为空。若爬取的是\"全部**原创**微博\"，则csv文件中不会包含\"是否原创\"及其之后的转发属性列；\n\n为了说明json结果文件格式，这里以迪丽热巴2019年12月27日到2019年12月28日发的2条微博为例。\n\n**json结果文件格式如下：**\n\n```\n{\n    \"user\": {\n        \"id\": \"1669879400\",\n        \"screen_name\": \"Dear-迪丽热巴\",\n        \"gender\": \"f\",\n        \"birthday\": \"双子座\",\n        \"location\": \"上海\",\n        \"education\": \"上海戏剧学院\",\n        \"company\": \"嘉行传媒\",\n        \"registration_time\": \"2010-07-02\",\n        \"sunshine\": \"信用极好\",\n        \"statuses_count\": 1121,\n        \"followers_count\": 66395881,\n        \"follow_count\": 250,\n        \"description\": \"一只喜欢默默表演的小透明。工作联系jaywalk@jaywalk.com.cn 🍒\",\n        \"profile_url\": \"https://m.weibo.cn/u/1669879400?uid=1669879400&luicode=10000011&lfid=1005051669879400\",\n        \"profile_image_url\": \"https://tvax2.sinaimg.cn/crop.0.0.1080.1080.180/63885668ly8gb5sqc19mqj20u00u0mz5.jpg?KID=imgbed,tva&Expires=1584108150&ssig=Zay1N7KhK1\",\n        \"avatar_hd\": \"https://wx2.sinaimg.cn/orj480/63885668ly8gb5sqc19mqj20u00u0mz5.jpg\",\n        \"urank\": 44,\n        \"mbrank\": 7,\n        \"verified\": true,\n        \"verified_type\": 0,\n        \"verified_reason\": \"嘉行传媒签约演员　\"\n    },\n    \"weibo\": [\n        {\n            \"user_id\": 1669879400,\n            \"screen_name\": \"Dear-迪丽热巴\",\n            \"id\": 4454572602912349,\n            \"bid\": \"ImTGkcdDn\",\n            \"text\": \"今天的#星光大赏#  \",\n            \"pics\": \"https://wx3.sinaimg.cn/large/63885668ly1gacppdn1nmj21yi2qp7wk.jpg,https://wx4.sinaimg.cn/large/63885668ly1gacpphkj5gj22ik3t0b2d.jpg,https://wx4.sinaimg.cn/large/63885668ly1gacppb4atej22yo4g04qr.jpg,https://wx2.sinaimg.cn/large/63885668ly1gacpn0eeyij22yo4g04qr.jpg\",\n            \"video_url\": \"\",\n            \"location\": \"\",\n            \"created_at\": \"2019-12-28\",\n            \"source\": \"\",\n            \"attitudes_count\": 551894,\n            \"comments_count\": 182010,\n            \"reposts_count\": 1000000,\n            \"topics\": \"星光大赏\",\n            \"at_users\": \"\"\n        },\n        {\n            \"user_id\": 1669879400,\n            \"screen_name\": \"Dear-迪丽热巴\",\n            \"id\": 4454081098040623,\n            \"bid\": \"ImGTzxJJt\",\n            \"text\": \"我最爱用的娇韵诗双萃精华穿上限量“金”装啦，希望阿丝儿们跟我一起在新的一年更美更年轻，喜笑颜开没有细纹困扰！限定新春礼盒还有祝福悄悄话，大家了解一下～\",\n            \"pics\": \"\",\n            \"video_url\": \"\",\n            \"location\": \"\",\n            \"created_at\": \"2019-12-27\",\n            \"source\": \"\",\n            \"attitudes_count\": 190840,\n            \"comments_count\": 43523,\n            \"reposts_count\": 1000000,\n            \"topics\": \"\",\n            \"at_users\": \"\",\n            \"retweet\": {\n                \"user_id\": 1684832145,\n                \"screen_name\": \"法国娇韵诗\",\n                \"id\": 4454028484570123,\n                \"bid\": \"ImFwIjaTF\",\n                \"text\": \"#点萃成金 年轻焕新# 将源自天然的植物力量，转化为滴滴珍贵如金的双萃精华。这份点萃成金的独到匠心，只为守护娇粉们的美丽而来。点击视频，与@Dear-迪丽热巴 一同邂逅新年限量版黄金双萃，以闪耀开运金，送上新春宠肌臻礼。 跟着迪迪选年货，还有双重新春惊喜，爱丽丝们看这里！ 第一重参与微淘活动邀请好友关注娇韵诗天猫旗舰店，就有机会赢取限量款热巴新年礼盒，打开就能聆听仙女迪亲口送出的新春祝福哦！点击网页链接下单晒热巴同款黄金双萃，并且@法国娇韵诗，更有机会获得热巴亲笔签名的礼盒哦！ 第二重转评说出新年希望娇韵诗为你解决的肌肤愿望，截止至1/10，小娇将从铁粉中抽取1位娇粉送出限量版热巴定制礼盒，抽取3位娇粉送出热巴明信片1张～ #迪丽热巴代言娇韵诗#养成同款御龄美肌，就从现在开始。法国娇韵诗的微博视频\",\n                \"pics\": \"\",\n                \"video_url\": \"http://f.video.weibocdn.com/003vQjnRlx07zFkxIMjS010412003bNx0E010.mp4?label=mp4_hd&template=852x480.25.0&trans_finger=62b30a3f061b162e421008955c73f536&Expires=1578322522&ssig=P3ozrNA3mv&KID=unistore,video\",\n                \"location\": \"\",\n                \"created_at\": \"2019-12-27\",\n                \"source\": \"微博 weibo.com\",\n                \"attitudes_count\": 18389,\n                \"comments_count\": 3201,\n                \"reposts_count\": 1000000,\n                \"topics\": \"点萃成金 年轻焕新,迪丽热巴代言娇韵诗\",\n                \"at_users\": \"Dear-迪丽热巴,法国娇韵诗\"\n            }\n        }\n    ]\n}\n```\n\n*1669879400.json*\n\n**下载的图片如下所示：**\n\n![](https://github.com/dataabc/media/blob/master/weibo-crawler/images/img.png)*img文件夹*\n\n本次下载了788张图片，大小一共1.21GB，包括她原创微博中的所有图片。图片名为yyyymmdd+微博id的形式，若某条微博存在多张图片，则图片名中还会包括它在微博图片中的序号。若某图片下载失败，程序则会以“weibo_id:pic_url”的形式将出错微博id和图片url写入同文件夹下的not_downloaded.txt里；若图片全部下载成功则不会生成not_downloaded.txt；\n\n**下载的视频如下所示：**\n![](https://github.com/dataabc/media/blob/master/weibo-crawler/images/video.png)*video文件夹*\n\n本次下载了66个视频，是她原创微博中的视频和原创微博Live Photo中的视频，视频名为yyyymmdd+微博id的形式。有三个视频因为网络原因下载失败，程序将它们的微博id和视频url分别以“weibo_id:video_url”的形式写到了同文件夹下的not_downloaded.txt里。\n\n因为我本地没有安装MySQL数据库和MongoDB数据库，所以暂时设置成不写入数据库。如果你想要将爬取结果写入数据库，只需要先安装数据库（MySQL或MongoDB），再安装对应包（pymysql或pymongo），然后将mysql_write或mongodb_write值设置为1即可。写入MySQL需要用户名、密码等配置信息，这些配置如何设置见[设置数据库](#4设置数据库可选)部分。\n\n## 运行环境\n\n* 开发语言：python2/python3\n\n* 系统： Windows/Linux/macOS\n\n## 使用说明\n\n### 1.下载脚本\n\n```bash\ngit clone https://github.com/dataabc/weibo-crawler.git\n```\n\n运行上述命令，将本项目下载到当前目录，如果下载成功当前目录会出现一个名为\"weibo-crawler\"的文件夹；\n\n### 2.安装依赖\n\n```bash\npip install -r requirements.txt\n```\n\n### 3.程序设置\n\n打开**config.json**文件，你会看到如下内容：\n\n```\n{\n    \"user_id_list\": [\"1669879400\"],\n    \"only_crawl_original\": 1,\n    \"remove_html_tag\": 1,\n    \"since_date\": \"2018-01-01\",\n    \"write_mode\": [\"csv\"],\n    \"original_pic_download\": 1,\n    \"retweet_pic_download\": 0,\n    \"original_video_download\": 1,\n    \"retweet_video_download\": 0,\n    \"download_comment\":1,\n    \"comment_max_download_count\":1000,\n    \"download_repost\": 1,\n    \"repost_max_download_count\": 1000,\n    \"user_id_as_folder_name\": 0,\n    \"cookie\": \"your cookie\",\n    \"mysql_config\": {\n        \"host\": \"localhost\",\n        \"port\": 3306,\n        \"user\": \"root\",\n        \"password\": \"123456\",\n        \"charset\": \"utf8mb4\"\n    },\n    \"mongodb_URI\": \"mongodb://[username:password@]host[:port][/[defaultauthdb][?options]]\",\n    \"post_config\": {\n        \"api_url\": \"https://api.example.com\",\n        \"api_token\": \"\"\n    }\n}\n```\n\n下面讲解每个参数的含义与设置方法。\n\n**设置user_id_list**\n\nuser_id_list是我们要爬取的微博的id，可以是一个，也可以是多个，例如：\n\n```\n\"user_id_list\": [\"1223178222\", \"1669879400\", \"1729370543\"],\n```\n\n上述代码代表我们要连续爬取user_id分别为“1223178222”、 “1669879400”、 “1729370543”的三个用户的微博，具体如何获取user_id见[如何获取user_id](#如何获取user_id)。\n\nuser_id_list的值也可以是文件路径，我们可以把要爬的所有微博用户的user_id都写到txt文件里，然后把文件的位置路径赋值给user_id_list。\n\n在txt文件中，每个user_id占一行，也可以在user_id后面加注释（可选），如用户昵称等信息，user_id和注释之间必需要有空格，文件名任意，类型为txt，位置位于本程序的同目录下，文件内容示例如下：\n\n```\n1223178222 胡歌\n1669879400 迪丽热巴\n1729370543 郭碧婷\n```\n\n假如文件叫user_id_list.txt，则user_id_list设置代码为：\n\n```\n\"user_id_list\": \"user_id_list.txt\",\n```\n\n**设置only_crawl_original**\n\nonly_crawl_original控制爬取范围，值为1代表爬取全部原创微博，值为0代表爬取全部微博（原创+转发）。例如，如果要爬全部原创微博，请使用如下代码：\n\n```\n\"only_crawl_original\": 1,\n```\n\n**设置since_date**\n\nsince_date值可以是日期，也可以是整数。如果是日期，代表爬取该日期之后的微博，格式应为“yyyy-mm-dd”，如：\n\n```\n\"since_date\": \"2018-01-01\",\n```\n\n代表爬取从2018年1月1日到现在的微博。\n\n如果是整数，代表爬取最近n天的微博，如:\n\n```\n\"since_date\": 10,\n```\n\n代表爬取最近10天的微博，这个说法不是特别准确，准确说是爬取发布时间从**10天前到本程序开始执行时**之间的微博。\n\n**since_date是所有user的爬取起始时间，非常不灵活。如果你要爬多个用户，并且想单独为每个用户设置一个since_date，可以使用[定期自动爬取微博](#7定期自动爬取微博可选)方法二中的方法，该方法可以为多个用户设置不同的since_date，非常灵活**。\n\n**设置query_list(可选)**\n\nquery_list是一个关键词字符串列表或以`,`分隔关键词的字符串，用于指定关键词搜索爬取，若为空`[]`或`\"\"`则爬取全部微博。例如要爬取用户包含“梦想”和“希望”的微博，则设定如下：\n\n```\n\"query_list\": [\"梦想\",\"希望\"],\n\"query_list\": \"梦想,希望\",\n```\n\n请注意，关键词搜索必须设定`cookie`信息。\n**query_list是所有user的爬取关键词，非常不灵活。如果你要爬多个用户，并且想单独为每个用户设置一个query_list，可以使用[定期自动爬取微博](#7定期自动爬取微博可选)方法二中的方法，该方法可以为多个用户设置不同的query_list，非常灵活**。\n\n**设置remove_html_tag**\n\nremove_html_tag控制是否移除抓取到的weibo正文和评论中的html tag，值为1代表移除，值为0代表不移除，如\n\n```\n\"remove_html_tag\": 1,\n```\n\n代表移除html tag。例如`专属新意，色彩启程~<a href='/n/路易威登'>@路易威登</a> CAPUCINES 手袋正合我意，打开灵感包袋的搭配新方式！`会被处理成`专属新意，色彩启程~@路易威登 CAPUCINES 手袋正合我意，打开灵感包袋的搭配新方式！`。\n\n**设置write_mode**\n\nwrite_mode控制结果文件格式，取值范围是csv、json、post、mongo、mysql和sqlite，分别代表将结果写入csv、json文件，通过POST发出，MongoDB、MySQL和SQLite数据库。write_mode可以同时包含这些取值中的一个或几个，如：\n\n```\n\"write_mode\": [\"csv\", \"json\"],\n```\n\n代表将结果信息写入csv文件和json文件。特别注意，如果你想写入数据库，除了在write_mode添加对应数据库的名字外，还应该安装相关数据库和对应python模块，具体操作见[设置数据库](#4设置数据库可选)部分。\n\n**设置original_pic_download**\n\noriginal_pic_download控制是否下载**原创**微博中的图片，值为1代表下载，值为0代表不下载，如\n\n```\n\"original_pic_download\": 1,\n```\n\n代表下载原创微博中的图片。\n\n**设置retweet_pic_download**\n\nretweet_pic_download控制是否下载**转发**微博中的图片，值为1代表下载，值为0代表不下载，如\n\n```\n\"retweet_pic_download\": 0,\n```\n\n代表不下载转发微博中的图片。特别注意，本设置只有在爬全部微博（原创+转发），即only_crawl_original值为0时生效，否则程序会跳过转发微博的图片下载。\n\n**设置original_video_download**\n\noriginal_video_download控制是否下载**原创**微博中的视频和**原创**微博**Live Photo**中的视频，值为1代表下载，值为0代表不下载，如\n\n```\n\"original_video_download\": 1,\n```\n\n代表下载原创微博中的视频和原创微博Live Photo中的视频。\n\n**设置retweet_video_download**\n\nretweet_video_download控制是否下载**转发**微博中的视频和**转发**微博**Live Photo**中的视频，值为1代表下载，值为0代表不下载，如\n\n```\n\"retweet_video_download\": 0,\n```\n\n代表不下载转发微博中的视频和转发微博Live Photo中的视频。特别注意，本设置只有在爬全部微博（原创+转发），即only_crawl_original值为0时生效，否则程序会跳过转发微博的视频下载。\n\n**设置user_id_as_folder_name**\n\nuser_id_as_folder_name控制结果文件的目录名，可取值为0和1，默认为0：\n\n```\n\"user_id_as_folder_name\": 0,\n```\n\n值为0，表示将结果文件保存在以用户昵称为名的文件夹里，这样结果更清晰；值为1表示将结果文件保存在以用户id为名的文件夹里，这样能保证多次爬取的一致性，因为用户昵称可变，用户id不可变。\n\n**设置download_comment**\n\ndownload_comment控制是否下载每条微博下的一级评论（不包括对评论的评论），仅当write_mode中有sqlite时有效，可取值为0和1，默认为1：\n\n```\n\"download_comment\": 1,\n```\n\n值为1，表示下载微博评论；值为0，表示不下载微博评论。\n\n**设置comment_max_download_count**\n\ncomment_max_download_count控制下载评论的最大数量，仅当write_mode中有sqlite时有效，默认为1000：\n\n```\n\"comment_max_download_count\": 1000,\n```\n\n**设置download_repost**\n\ndownload_repost控制是否下载每条微博下的转发，仅当write_mode中有sqlite时有效，可取值为0和1，默认为1：\n\n```\n\"download_repost\": 1,\n```\n\n值为1，表示下载微博转发；值为0，表示不下载微博转发。\n\n**设置repost_max_download_count**\n\nrepost_max_download_count控制下载转发的最大数量，仅当write_mode中有sqlite时有效，默认为1000：\n\n```\n\"repost_max_download_count\": 1000,\n```\n\n值为1000，表示最多下载每条微博下的1000条转发。\n\n**设置cookie（可选）**\n\ncookie为可选参数，即可填可不填，具体区别见[添加cookie与不添加cookie的区别](#添加cookie与不添加cookie的区别可选)。cookie默认配置如下：\n\n```\n\"cookie\": \"your cookie\",\n```\n\n如果想要设置cookie，可以按照[如何获取cookie](#如何获取cookie可选)中的方法，获取cookie，并将上面的\"your cookie\"替换成真实的cookie即可。\n\n**设置mysql_config（可选）**\n\nmysql_config控制mysql参数配置。如果你不需要将结果信息写入mysql，这个参数可以忽略，即删除或保留都无所谓；如果你需要写入mysql且config.json文件中mysql_config的配置与你的mysql配置不一样，请将该值改成你自己mysql中的参数配置。\n\n**设置mongodb_URI（可选）**\n\nmongodb_URI是mongodb的连接字符串。如果你不需要将结果信息写入mongodb，这个参数可以忽略，即删除或保留都无所谓；如果你需要写入mongodb，则需要配置为[完整的mongodb URI](https://www.mongodb.com/docs/manual/reference/connection-string/)。\n\n**设置start_page（可选）**\n\nstart_page为爬取微博的初始页数，默认参数为1，即从所爬取用户的当前第一页微博内容开始爬取。\n若在大批量爬取微博时出现中途被限制中断的情况，可通过查看csv文件内目前已爬取到的微博数除以10，向下取整后的值即为中断页数，手动设置start_page参数为中断页数，重新运行即可从被中断的节点继续爬取剩余微博内容。\n\n### 4.设置数据库（可选）\n\n本部分是可选部分，如果不需要将爬取信息写入数据库，可跳过这一步。本程序目前支持MySQL数据库和MongoDB数据库，如果你需要写入其它数据库，可以参考这两个数据库的写法自己编写。\n\n**MySQL数据库写入**\n\n要想将爬取信息写入MySQL，请根据自己的系统环境安装MySQL，然后命令行执行：\n\n```bash\npip install pymysql\n```\n\n**MongoDB数据库写入**\n\n要想将爬取信息写入MongoDB，请根据自己的系统环境安装MongoDB，然后命令行执行：\n\n```\npip install pymongo\n```\n\nMySQL和MongDB数据库的写入内容一样。程序首先会创建一个名为\"weibo\"的数据库，然后再创建\"user\"表和\"weibo\"表，包含爬取的所有内容。爬取到的微博**用户信息**或插入或更新，都会存储到user表里；爬取到的**微博信息**或插入或更新，都会存储到weibo表里，两个表通过user_id关联。如果想了解两个表的具体字段，请点击\"详情\"。\n<details>\n\n<summary>详情</summary>\n\n**user**表\n\n**id**：微博用户id，如\"1669879400\"；\n\n**screen_name**：微博用户昵称，如\"Dear-迪丽热巴\"；\n\n**gender**：微博用户性别，取值为f或m，分别代表女和男；\n\n**birthday**：生日；\n\n**location**：所在地；\n\n**education**：教育经历；\n\n**company**：公司；\n\n**sunshine**：阳光信用；\n\n**registration_time**：注册时间；\n\n**statuses_count**：微博数；\n\n**followers_count**：粉丝数；\n\n**follow_count**：关注数；\n\n**description**：微博简介；\n\n**profile_url**：微博主页，如<https://m.weibo.cn/u/1669879400?uid=1669879400&luicode=10000011&lfid=1005051669879400>;\n\n**profile_image_url**：微博头像url；\n\n**avatar_hd**：微博高清头像url；\n\n**urank**：微博等级；\n\n**mbrank**：微博会员等级，普通用户会员等级为0；\n\n**verified**：微博是否认证，取值为true和false；\n\n**verified_type**：微博认证类型，没有认证值为-1，个人认证值为0，企业认证值为2，政府认证值为3，这些类型仅是个人猜测，应该不全，大家可以根据实际情况判断；\n\n**verified_reason**：微博认证信息，只有认证用户拥有此属性。\n\n***\n**weibo**表\n\n**user_id**：存储微博用户id，如\"1669879400\"；\n\n**screen_name**：存储微博昵称，如\"Dear-迪丽热巴\"；\n\n**id**：存储微博id；\n\n**text**：存储微博正文；\n\n**article_url**：存储微博中头条文章的url，如果微博中存在头条文章，就获取该头条文章的url，否则该值为''；\n\n**pics**：存储原创微博的原始图片url。若某条微博有多张图片，则存储多个url，以英文逗号分割；若该微博没有图片，则值为''；\n\n**video_url**：存储原创微博的视频url和Live Photo中的视频url。若某条微博有多个视频，则存储多个url，以英文分号分割；若该微博没有视频，则值为''；\n\n**location**：存储微博的发布位置。若某条微博没有位置信息，则值为''；\n\n**created_at**：存储微博的发布时间；\n\n**source**：存储微博的发布工具；\n\n**attitudes_count**：存储微博获得的点赞数；\n\n**comments_count**：存储微博获得的评论数；\n\n**reposts_count**：存储微博获得的转发数；\n\n**topics**：存储微博话题，即两个#中的内容。若某条微博没有话题信息，则值为''；\n\n**at_users**：存储微博@的用户。若某条微博没有@的用户，则值为''；\n\n**retweet_id**：存储转发微博中原始微博的微博id。若某条微博为原创微博，则值为''。\n\n</details>\n**SQLite数据库写入**\n\n脚本会自动建立并配置数据库文件`weibodata.db`。\n\n### 5.运行脚本\n\n大家可以根据自己的运行环境选择运行方式，Linux可以通过\n\n```bash\npython weibo.py\n```\n\n运行;\n\n### 6.按需求修改脚本（可选）\n\n本部分为可选部分，如果你不需要自己修改代码或添加新功能，可以忽略此部分。\n\n本程序所有代码都位于weibo.py文件，程序主体是一个Weibo类，上述所有功能都是通过在main函数调用Weibo类实现的，默认的调用代码如下：\n\n```python\n        if not os.path.isfile('./config.json'):\n            sys.exit(u'当前路径：%s 不存在配置文件config.json' %\n                     (os.path.split(os.path.realpath(__file__))[0] + os.sep))\n        with open('./config.json') as f:\n            config = json.loads(f.read())\n        wb = Weibo(config)\n        wb.start()  # 爬取微博信息\n```\n\n用户可以按照自己的需求调用或修改Weibo类。\n\n通过执行本程序，我们可以得到很多信息：\n\n**wb.user**：存储目标微博用户信息；\n\nwb.user包含爬取到的微博用户信息，如**用户id**、**用户昵称**、**性别**、**生日**、**所在地**、**教育经历**、**公司**、**阳光信用**、**微博注册时间**、**微博数**、**粉丝数**、**关注数**、**简介**、**主页地址**、**头像url**、**高清头像url**、**微博等级**、**会员等级**、**是否认证**、**认证类型**、**认证信息**等，大家可以点击\"详情\"查看具体用法。\n\n<details>\n\n<summary>详情</summary>\n\n**id**：微博用户id，取值方式为wb.user['id'],由一串数字组成；\n\n**screen_name**：微博用户昵称，取值方式为wb.user['screen_name']；\n\n**gender**：微博用户性别，取值方式为wb.user['gender']，取值为f或m，分别代表女和男；\n\n**birthday**：微博用户生日，取值方式为wb.user['birthday']，若用户没有填写该信息，则值为''；\n\n**location**：微博用户所在地，取值方式为wb.user['location']，若用户没有填写该信息，则值为''；\n\n**education**：微博用户上学时的学校，取值方式为wb.user['education']，若用户没有填写该信息，则值为''；\n\n**company**：微博用户所属的公司，取值方式为wb.user['company']，若用户没有填写该信息，则值为''；\n\n**sunshine**：微博用户的阳光信用，取值方式为wb.user['sunshine']；\n\n**registration_time**：微博用户的注册时间，取值方式为wb.user['registration_time']；\n\n**statuses_count**：微博数，取值方式为wb.user['statuses_count']；\n\n**followers_count**：微博粉丝数，取值方式为wb.user['followers_count']；\n\n**follow_count**：微博关注数，取值方式为wb.user['follow_count']；\n\n**description**：微博简介，取值方式为wb.user['description']；\n\n**profile_url**：微博主页，取值方式为wb.user['profile_url'];\n\n**profile_image_url**：微博头像url，取值方式为wb.user['profile_image_url']；\n\n**avatar_hd**：微博高清头像url，取值方式为wb.user['avatar_hd']；\n\n**urank**：微博等级，取值方式为wb.user['urank']；\n\n**mbrank**：微博会员等级，取值方式为wb.user['mbrank']，普通用户会员等级为0；\n\n**verified**：微博是否认证，取值方式为wb.user['verified']，取值为true和false；\n\n**verified_type**：微博认证类型，取值方式为wb.user['verified_type']，没有认证值为-1，个人认证值为0，企业认证值为2，政府认证值为3，这些类型仅是个人猜测，应该不全，大家可以根据实际情况判断；\n\n**verified_reason**：微博认证信息，取值方式为wb.user['verified_reason']，只有认证用户拥有此属性。\n\n</details>\n\n**wb.weibo**：存储爬取到的所有微博信息；\n\nwb.weibo包含爬取到的所有微博信息，如**微博id**、**正文**、**原始图片url**、**视频url**、**位置**、**日期**、**发布工具**、**点赞数**、**转发数**、**评论数**、**话题**、**@用户**等。如果爬的是全部微博(原创+转发)，除上述信息之外，还包含**原始用户id**、**原始用户昵称**、**原始微博id**、**原始微博正文**、**原始微博原始图片url**、**原始微博位置**、**原始微博日期**、**原始微博工具**、**原始微博点赞数**、**原始微博评论数**、**原始微博转发数**、**原始微博话题**、**原始微博@用户**等信息。wb.weibo是一个列表，包含了爬取的所有微博信息。wb.weibo[0]为爬取的第一条微博，wb.weibo[1]为爬取的第二条微博，以此类推。当only_crawl_original=1时，wb.weibo[0]为爬取的第一条**原创**微博，以此类推。wb.weibo[0]['id']为第一条微博的id，wb.weibo[0]['text']为第一条微博的正文，wb.weibo[0]['created_at']为第一条微博的发布时间，还有其它很多信息不在赘述，大家可以点击下面的\"详情\"查看具体用法。\n<details>\n\n<summary>详情</summary>\n\n**user_id**：存储微博用户id。如wb.weibo[0]['user_id']为最新一条微博的用户id；\n\n**screen_name**：存储微博昵称。如wb.weibo[0]['screen_name']为最新一条微博的昵称；\n\n**id**：存储微博id。如wb.weibo[0]['id']为最新一条微博的id；\n\n**text**：存储微博正文。如wb.weibo[0]['text']为最新一条微博的正文；\n\n**article_url**：存储微博中头条文章的url。如wb.weibo[0]['article_url']为最新一条微博的头条文章url，若微博中不存在头条文章，则该值为''；\n\n**pics**：存储原创微博的原始图片url。如wb.weibo[0]['pics']为最新一条微博的原始图片url，若该条微博有多张图片，则存储多个url，以英文逗号分割；若该微博没有图片，则值为''；\n\n**video_url**：存储原创微博的视频url和原创微博Live Photo中的视频url。如wb.weibo[0]['video_url']为最新一条微博的视频url，若该条微博有多个视频，则存储多个url，以英文分号分割；若该微博没有视频，则值为''；\n\n**location**：存储微博的发布位置。如wb.weibo[0]['location']为最新一条微博的发布位置，若该条微博没有位置信息，则值为''；\n\n**created_at**：存储微博的发布时间。如wb.weibo[0]['created_at']为最新一条微博的发布时间；\n\n**source**：存储微博的发布工具。如wb.weibo[0]['source']为最新一条微博的发布工具；\n\n**attitudes_count**：存储微博获得的点赞数。如wb.weibo[0]['attitudes_count']为最新一条微博获得的点赞数；\n\n**comments_count**：存储微博获得的评论数。如wb.weibo[0]['comments_count']为最新一条微博获得的评论数；\n\n**reposts_count**：存储微博获得的转发数。如wb.weibo[0]['reposts_count']为最新一条微博获得的转发数；\n\n**topics**：存储微博话题，即两个#中的内容。如wb.weibo[0]['topics']为最新一条微博的话题，若该条微博没有话题信息，则值为''；\n\n**at_users**：存储微博@的用户。如wb.weibo[0]['at_users']为最新一条微博@的用户，若该条微博没有@的用户，则值为''；\n\n**retweet**：存储转发微博中原始微博的全部信息。假如wb.weibo[0]为转发微博，则wb.weibo[0]['retweet']为该转发微博的原始微博，它存储的属性与wb.weibo[0]一样，只是没有retweet属性;若该条微博为原创微博，则wb[0]没有\"retweet\"属性，大家可以点击\"详情\"查看具体用法。\n\n<details>\n\n<summary>详情</summary>\n\n假设爬取到的第i条微博为转发微博，则它存在以下信息：\n\n**user_id**：存储原始微博用户id。wb.weibo[i-1]['retweet']['user_id']为该原始微博的用户id；\n\n**screen_name**：存储原始微博昵称。wb.weibo[i-1]['retweet']['screen_name']为该原始微博的昵称；\n\n**id**：存储原始微博id。wb.weibo[i-1]['retweet']['id']为该原始微博的id；\n\n**text**：存储原始微博正文。wb.weibo[i-1]['retweet']['text']为该原始微博的正文；\n\n**article_url**：存储原始微博中头条文章的url。如wb.weibo[i-1]['retweet']['article_url']为该原始微博的头条文章url，若原始微博中不存在头条文章，则该值为''；\n\n**pics**：存储原始微博的原始图片url。wb.weibo[i-1]['retweet']['pics']为该原始微博的原始图片url，若该原始微博有多张图片，则存储多个url，以英文逗号分割；若该原始微博没有图片，则值为''；\n\n**video_url**：存储原始微博的视频url和原始微博Live Photo中的视频url。如wb.weibo[i-1]['retweet']['video_url']为该原始微博的视频url，若该原始微博有多个视频，则存储多个url，以英文分号分割；若该微博没有视频，则值为''；\n\n**location**：存储原始微博的发布位置。wb.weibo[i-1]['retweet']['location']为该原始微博的发布位置，若该原始微博没有位置信息，则值为''；\n\n**created_at**：存储原始微博的发布时间。wb.weibo[i-1]['retweet']['created_at']为该原始微博的发布时间；\n\n**source**：存储原始微博的发布工具。wb.weibo[i-1]['retweet']['source']为该原始微博的发布工具；\n\n**attitudes_count**：存储原始微博获得的点赞数。wb.weibo[i-1]['retweet']['attitudes_count']为该原始微博获得的点赞数；\n\n**comments_count**：存储原始微博获得的评论数。wb.weibo[i-1]['retweet']['comments_count']为该原始微博获得的评论数；\n\n**reposts_count**：存储原始微博获得的转发数。wb.weibo[i-1]['retweet']['reposts_count']为该原始微博获得的转发数；\n\n**topics**：存储原始微博话题，即两个#中的内容。wb.weibo[i-1]['retweet']['topics']为该原始微博的话题，若该原始微博没有话题信息，则值为''；\n\n**at_users**：存储原始微博@的用户。wb.weibo[i-1]['retweet']['at_users']为该原始微博@的用户，若该原始微博没有@的用户，则值为''。\n\n</details>\n\n</details>\n\n### 7.定期自动爬取微博（可选）\n\n我们爬取了微博以后，很多微博账号又可能发了一些新微博，定期自动爬取微博就是每隔一段时间自动运行程序，自动爬取这段时间产生的新微博（忽略以前爬过的旧微博）。本部分为可选部分，如果不需要可以忽略。\n\n思路是**利用第三方软件，如crontab，让程序每隔一段时间运行一次**。因为是要跳过以前爬过的旧微博，只爬新微博。所以需要**设置一个动态的since_date**。很多时候我们使用的since_date是固定的，比如since_date=\"2018-01-01\"，程序就会按照这个设置从最新的微博一直爬到发布时间为2018-01-01的微博（包括这个时间）。因为我们想追加新微博，跳过旧微博。第二次爬取时since_date值就应该是当前时间到上次爬取的时间。 如果我们使用最原始的方式实现追加爬取，应该是这样：\n\n```\n假如程序第一次执行时间是2019-06-06，since_date假如为2018-01-01，那这一次就是爬取从2018-01-01到2019-06-06这段时间用户所发的微博；\n第二次爬取，我们想要接着上次的爬，需要手动将since_date值设置为上次程序执行的日期，即2019-06-06\n```\n\n上面的方法太麻烦，因为每次都要手动设置since_date。因此我们需要动态设置since_date，即程序根据实际情况，自动生成since_date。\n有两种方法实现动态更新since_date：\n\n**方法一：将since_date设置成整数**\n\n将config.json文件中的since_date设置成整数，如：\n\n```\n\"since_date\": 10,\n```\n\n这个配置告诉程序爬取最近10天的微博，更准确说是爬取发布时间从10天前到本程序开始执行时之间的微博。这样since_date就是一个动态的变量，每次程序执行时，它的值就是当前日期减10。配合crontab每9天或10天执行一次，就实现了定期追加爬取。\n\n**方法二：将上次执行程序的时间写入文件（推荐）**\n\n这个方法很简单，就是用户把要爬的用户id写入txt文件，然后再把文件路径赋值给config.json中的user_id_list参数。\n\ntxt文件名格式可以参考[程序设置](#3程序设置)中的设置user_id_list部分，这样设置就全部结束了。\n\n说下这个方法的原理和好处，假如你的txt文件内容为：\n\n```\n1669879400\n1223178222 胡歌\n1729370543 郭碧婷 2019-01-01\n```\n\n第一次执行时，因为第一行和第二行都没有写时间，程序会按照config.json文件中since_date的值爬取，第三行有时间“2019-01-01”，程序就会把这个时间当作since_date。每个用户爬取结束程序都会自动更新txt文件，每一行第一部分是user_id，第二部分是用户昵称，第三部分是程序准备爬取该用户第一条微博（最新微博）时的日期。爬完三个用户后，txt文件的内容自动更新为：\n\n```\n1669879400 Dear-迪丽热巴 2020-01-18\n1223178222 胡歌 2020-01-18\n1729370543 郭碧婷 2020-01-18\n```\n\n下次再爬取微博的时候，程序会把每行的时间数据作为since_date。这样的好处一是不用修改since_date，程序自动更新；二是每一个用户都可以单独拥有只属于自己的since_date，每个用户的since_date相互独立，互不干扰，格式为`yyyy-mm-dd`或整数。比如，现在又添加了一个新用户，以杨紫的微博为例，你想获取她2018-01-23到现在的全部微博，可以这样修改txt文件：\n\n```\n1669879400 迪丽热巴 2020-01-18\n1223178222 胡歌 2020-01-18\n1729370543 郭碧婷 2020-01-18\n1227368500 杨紫 3 梦想,希望\n```\n\n注意每一行的用户配置参数以空格分隔，如果第一个参数全部由数字组成，程序就认为此行为一个用户的配置，否则程序会认为该行只是注释，跳过该行；第二个参数可以为任意格式，建议写用户昵称；第三个如果是日期格式（yyyy-mm-dd），程序就将该日期设置为用户自己的since_date，否则使用config.json中的since_date爬取该用户的微博，第二个参数和第三个参数也可以不填。\n也可以设置第四个参数，将被读取为query_list。\n\n**方法三：将`const.py`文件中的运行模式改为`append`**\n\n以追加模式运行程序，每次运行，每个id只获取最新的微博，而不是全部，避免频繁备份微博导致过多的请求次数。\n\n注意：\n\n* 该模式会跳过置顶微博。\n* 若采集信息后用户又编辑微博，则不会记录编辑内容。\n\n### 8.使用docker\n\n**docker run**\n\n```shell\ndocker build -t weibo-crawler .\ndocker run -it -d \\\n  -v path/to/config.json:/app/config.json \\\n  -v path/to/user_id_list.txt:/app/user_id_list.txt \\ # 可选: 使用方法二（将上次执行程序时间写入文件）定期自动爬取微博时，保存修改到host\n  -v path/to/weibo:/app/weibo \\\n  -e schedule_interval=1 \\ # 可选：循环间隔（分钟）\n  weibo-crawler\n```\n\n**docker compose**\n\n```yaml\nversion: '3'\nservices:\n  weibo-crawler:\n    build:\n      context: .\n      dockerfile: Dockerfile\n    volumes:\n      - path/to/config.json:/app/config.json\n      - path/to/user_id_list.txt:/app/user_id_list.txt # 可选: 使用方法二（将上次执行程序时间写入文件）定期自动爬取微博时，保存修改到host\n      - path/to/weibo:/app/weibo\n    environment:\n      - schedule_interval=1 # 可选：循环间隔（分钟）\n```\n\n## 如何获取user_id\n\n1.打开网址<https://weibo.cn>，搜索我们要找的人，如\"迪丽热巴\"，进入她的主页；\n\n![](https://github.com/dataabc/media/blob/master/weiboSpider/images/user_home.png)\n2.按照上图箭头所指，点击\"资料\"链接，跳转到用户资料页面；\n\n![](https://github.com/dataabc/media/blob/master/weiboSpider/images/user_info.png)\n如上图所示，迪丽热巴微博资料页的地址为\"<https://weibo.cn/1669879400/info>\"，其中的\"1669879400\"即为此微博的user_id。\n\n事实上，此微博的user_id也包含在用户主页(<https://weibo.cn/u/1669879400?f=search_0>)中，之所以我们还要点击主页中的\"资料\"来获取user_id，是因为很多用户的主页不是\"<https://weibo.cn/user_id?f=search_0>\"的形式，而是\"<https://weibo.cn/个性域名?f=search_0>\"或\"<https://weibo.cn/微号?f=search_0>\"的形式。其中\"微号\"和user_id都是一串数字，如果仅仅通过主页地址提取user_id，很容易将\"微号\"误认为user_id。\n\n## 添加cookie与不添加cookie的区别（可选）\n\n对于微博数2000条及以下的微博用户，不添加cookie可以获取其用户信息和大部分微博；对于微博数2000条以上的微博用户，不添加cookie可以获取其用户信息和最近2000条微博中的大部分，添加cookie可以获取其全部微博。以2020年1月2日迪丽热巴的微博为例，此时她共有1085条微博，在不添加cookie的情况下，可以获取到1026条微博，大约占全部微博的94.56%，而在添加cookie后，可以获取全部微博。其他用户类似，大部分都可以在不添加cookie的情况下获取到90%以上的微博，在添加cookie后可以获取全部微博。具体原因是，大部分微博内容都可以在[移动版](https://m.weibo.cn/)匿名获取，少量微博需要用户登录才可以获取，所以这部分微博在不添加cookie时是无法获取的。\n\n有少部分微博用户，不添加cookie可以获取其微博，无法获取其用户信息。对于这种情况，要想获取其用户信息，是需要cookie的。\n\n如需抓取微博转发，请添加cookie。\n\n## 如何获取cookie（可选）\n\n1.用Chrome打开<https://passport.weibo.cn/signin/login>；\n\n2.输入微博的用户名、密码，登录，如图所示：\n![](https://github.com/dataabc/media/blob/master/weiboSpider/images/cookie1.png)\n登录成功后会跳转到<https://m.weibo.cn>;\n\n3.按F12键打开Chrome开发者工具，在地址栏输入并跳转到<https://weibo.cn>，跳转后会显示如下类似界面:\n![](https://github.com/dataabc/media/blob/master/weiboSpider/images/cookie2.png)\n4.依此点击Chrome开发者工具中的Network->Name中的weibo.cn->Headers->Request Headers，\"Cookie:\"后的值即为我们要找的cookie值，复制即可，如图所示：\n![](https://github.com/dataabc/media/blob/master/weiboSpider/images/cookie3.png)\n\n## 如何检测cookie是否有效（可选）\n\n本程序cookie检查的逻辑是：使用cookie来源账号发布**限定范围的**微博，若cookie可用，则可以读取到该微博，否则读取不到。\n\n**操作方法**\n\n1. 使用cookie的来源账号发布一条微博，该账号和微博需要满足以下条件：\n\n   * 该微博必须是**非公开可见**的，后续需要根据可见性判断cookie是否有效；\n\n   * 该微博需要是最近5条微博，不能在发布测试用微博内容后又发很多新微博；\n\n   * 在`config.json`配置中的since_date之后，该账号必须有大于9条微博。\n\n2. 将`const.py`文件中`'CHECK': False`中的`False`改为`True`，`'HIDDEN_WEIBO': '微博内容'`中的`微博内容`改为你发的限定范围的微博。\n\n3. 将提供cookie的微博id放置在`config.json`文件中`\"user_id_list\"`设置项数组中的第一个。例如提供cookie的微博id为`123456`，则`\"user_id_list\"`设置为`\"user_id_list\":[\"123456\", \"<其余id...>\"]`。\n\n注：本方法也将会抓取提供cookie账号的微博内容。\n\n在间歇运行程序时，cookie无效会导致程序不能按照预设目标执行，因此可以打开cookie通知功能。本项目使用开源项目[pushdeer](https://github.com/easychen/pushdeer)进行通知，在使用前用户需要申请push_key，具体可查看官网了解。打开方法为：\n\n1. 在`const.py`文件中，将`'NOTIFY': False`中的`False`设为`True`；\n2. 将`'PUSH_KEY': ''`的`''`替换为`'<你的push_key>'`\n\n## API服务\n\n根目录下service.py提供了一个简单的restful api示例，运行`python service.py`启动服务后可以通过http请求定时更新以及查询微博\n\n文档参考[API说明](./API.md)\n"
        },
        {
          "name": "__main__.py",
          "type": "blob",
          "size": 1.14453125,
          "content": "import argparse\nfrom time import sleep\n\nimport schedule\n\nimport const\nimport weibo\nfrom util.notify import push_deer\n\n\ndef main(schedule_interval):\n    \"\"\"\n    主函数，用于设置定时任务和执行微博爬虫脚本。\n\n    Parameters:\n        schedule_interval (int): 循环间隔，以分钟为单位。\n\n    Returns:\n        None\n    \"\"\"\n    schedule.every(schedule_interval).minutes.do(weibo.main)  # 每隔指定的时间间隔执行一次main函数\n    weibo.logger.info('循环间隔设置为%d分钟', schedule_interval)\n\n    weibo.main()  # 立即执行一次\n    while True:\n        try:\n            schedule.run_pending()\n            sleep(1)\n        except KeyboardInterrupt:\n            schedule.cancel_job(weibo.main)\n            break\n        except Exception as error:\n            if const.NOTIFY[\"NOTIFY\"]:\n                push_deer(f\"weibo-crawler运行出错, 错误为{error}\")\n                weibo.logger.exception(error)\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument('schedule_interval', type=int, help='循环间隔（分钟）')\n    args = parser.parse_args()\n\n    main(args.schedule_interval)\n"
        },
        {
          "name": "config.json",
          "type": "blob",
          "size": 0.8369140625,
          "content": "{\n    \"user_id_list\": \"user_id_list.txt\",\n    \"only_crawl_original\": 0,\n    \"since_date\": 1,\n    \"start_page\": 1,\n    \"write_mode\": [\n        \"csv\"\n    ],\n    \"original_pic_download\": 1,\n    \"retweet_pic_download\": 0,\n    \"original_video_download\": 1,\n    \"retweet_video_download\": 0,\n    \"download_comment\": 1,\n    \"comment_max_download_count\": 100,\n    \"download_repost\": 1,\n    \"repost_max_download_count\": 100,\n    \"user_id_as_folder_name\": 0,\n    \"remove_html_tag\": 1,\n    \"cookie\": \"your cookie\",\n    \"mysql_config\": {\n        \"host\": \"localhost\",\n        \"port\": 3306,\n        \"user\": \"root\",\n        \"password\": \"123456\",\n        \"charset\": \"utf8mb4\"\n    },\n    \"mongodb_URI\": \"mongodb://[username:password@]host[:port][/[defaultauthdb][?options]]\",\n    \"post_config\": {\n        \"api_url\": \"https://api.example.com\",\n        \"api_token\": \"\"\n    }\n}\n"
        },
        {
          "name": "const.py",
          "type": "blob",
          "size": 1.4833984375,
          "content": "import const\n\n\"\"\"\n运行模式\n可以是追加模式append或覆盖模式overwrite\nappend模式：仅可在sqlite启用时使用。每次运行每个id只获取最新的微博，对于以往的即使是编辑过的微博，也不再获取。\noverwrite模式：每次运行都会获取全量微博。\n注意：overwrite模式下暂不能记录上次获取微博的id，因此从overwrite模式转为append模式时，仍需获取所有数据\n\"\"\"\nconst.MODE = \"overwrite\"\n\n\"\"\"\n检查cookie是否有效\n默认不需要检查cookie\n如果检查cookie，需要参考以下链接设置\nconfig中science_date一定要确保测试号获得的微博数在不含测试微博的情况下大于9\nhttps://github.com/dataabc/weibo-crawler#%E5%A6%82%E4%BD%95%E6%A3%80%E6%B5%8Bcookie%E6%98%AF%E5%90%A6%E6%9C%89%E6%95%88%E5%8F%AF%E9%80%89\n\"\"\"\nconst.CHECK_COOKIE = {\n    \"CHECK\": False,  # 是否检查cookie\n    \"CHECKED\": False,  # 这里不要动，判断已检查了cookie的标志位\n    \"EXIT_AFTER_CHECK\": False,  # 这里不要动，append模式中已完成增量微博抓取，仅等待cookie检查的标志位\n    \"HIDDEN_WEIBO\": \"微博内容\",  # 你可能发现平台会自动给你的微博自动加个空格，但这里你不用加空格\n    \"GUESS_PIN\": False,  # 这里不要动，因为微博取消了“置顶”字样的显示，因此默认猜测所有人第一条都是置顶\n}\nconst.NOTIFY = {\n    \"NOTIFY\": False,  # 是否通知\n    \"PUSH_KEY\": \"\",  # 这里使用push_deer做通知，填入pushdeer的pushkey\n}\n"
        },
        {
          "name": "logging.conf",
          "type": "blob",
          "size": 0.9091796875,
          "content": "[loggers]\nkeys=root,weibo\n\n[handlers]\nkeys=consoleHandler,fileHandler,errorHandler\n\n[formatters]\nkeys=consoleFormatter,fileFormatter,errorFormatter\n\n[logger_root]\nlevel=DEBUG\nhandlers=consoleHandler,fileHandler,errorHandler\n\n[logger_weibo]\nlevel=DEBUG\nhandlers=consoleHandler,fileHandler,errorHandler\nqualname=weibo\npropagate=0\n\n[handler_consoleHandler]\nclass=StreamHandler\nlevel=DEBUG\nformatter=consoleFormatter\nargs=(sys.stdout,)\n\n[handler_fileHandler]\nclass=handlers.TimedRotatingFileHandler\nlevel=INFO\nformatter=fileFormatter\nargs=('log/all.log', 'D', 1, 5, 'utf-8', False, False)\n\n[handler_errorHandler]\nclass=FileHandler\nlevel=WARNING\nformatter=errorFormatter\nargs=('log/error.log', 'a','utf-8')\n\n[formatter_consoleFormatter]\nformat=%(message)s\n\n[formatter_fileFormatter]\nformat=%(asctime)s - %(levelname)s - %(message)s\n\n[formatter_errorFormatter]\nformat=%(asctime)s - %(levelname)s - %(filename)s[:%(lineno)d] - %(message)s"
        },
        {
          "name": "requirements.txt",
          "type": "blob",
          "size": 0.0859375,
          "content": "lxml==5.3.0\npymongo==4.6.3\nPyMySQL==1.1.1\nRequests==2.32.0\nschedule==1.2.1\ntqdm==4.66.3\n"
        },
        {
          "name": "service.py",
          "type": "blob",
          "size": 7.833984375,
          "content": "from weibo import Weibo, handle_config_renaming\nimport const\nimport logging\nimport logging.config\nimport os\nfrom flask import Flask, jsonify, request\nimport sqlite3\nimport json\nfrom concurrent.futures import ThreadPoolExecutor\nimport threading\nimport uuid\nimport time\nfrom datetime import datetime\n\n# 1896820725 天津股侠 2024-12-09T16:47:04\n\nDATABASE_PATH = './weibo/weibodata.db'\nprint(DATABASE_PATH)\n\n# 如果日志文件夹不存在，则创建\nif not os.path.isdir(\"log/\"):\n    os.makedirs(\"log/\")\nlogging_path = os.path.split(os.path.realpath(__file__))[0] + os.sep + \"logging.conf\"\nlogging.config.fileConfig(logging_path)\nlogger = logging.getLogger(\"api\")\n\nconfig = {\n    \"user_id_list\": [\n        \"6067225218\", \n        \"1445403190\"\n        ],\n    \"only_crawl_original\": 1,\n    \"since_date\": 1,\n    \"start_page\": 1,\n    \"write_mode\": [\n        \"csv\",\n        \"json\",\n        \"sqlite\"\n    ],\n    \"original_pic_download\": 0,\n    \"retweet_pic_download\": 0,\n    \"original_video_download\": 0,\n    \"retweet_video_download\": 0,\n    \"download_comment\": 0,\n    \"comment_max_download_count\": 100,\n    \"download_repost\": 0,\n    \"repost_max_download_count\": 100,\n    \"user_id_as_folder_name\": 0,\n    \"remove_html_tag\": 1,\n    \"cookie\": \"your weibo cookie\",\n    \"mysql_config\": {\n        \"host\": \"localhost\",\n        \"port\": 3306,\n        \"user\": \"root\",\n        \"password\": \"123456\",\n        \"charset\": \"utf8mb4\"\n    },\n    \"mongodb_URI\": \"mongodb://[username:password@]host[:port][/[defaultauthdb][?options]]\",\n    \"post_config\": {\n        \"api_url\": \"https://api.example.com\",\n        \"api_token\": \"\"\n    }\n}\n\napp = Flask(__name__)\napp.config['JSON_AS_ASCII'] = False  # 确保JSON响应中的中文不会被转义\napp.config['JSONIFY_MIMETYPE'] = 'application/json;charset=utf-8'\n\n# 添加线程池和任务状态跟踪\nexecutor = ThreadPoolExecutor(max_workers=1)  # 限制只有1个worker避免并发爬取\ntasks = {}  # 存储任务状态\n\n# 在executor定义后添加任务锁相关变量\ncurrent_task_id = None\ntask_lock = threading.Lock()\n\ndef get_running_task():\n    \"\"\"获取当前运行的任务信息\"\"\"\n    if current_task_id and current_task_id in tasks:\n        task = tasks[current_task_id]\n        if task['state'] in ['PENDING', 'PROGRESS']:\n            return current_task_id, task\n    return None, None\n\ndef get_config(user_id_list=None):\n    \"\"\"获取配置，允许动态设置user_id_list\"\"\"\n    current_config = config.copy()\n    if user_id_list:\n        current_config['user_id_list'] = user_id_list\n    handle_config_renaming(current_config, oldName=\"filter\", newName=\"only_crawl_original\")\n    handle_config_renaming(current_config, oldName=\"result_dir_name\", newName=\"user_id_as_folder_name\")\n    return current_config\n\ndef run_refresh_task(task_id, user_id_list=None):\n    global current_task_id\n    try:\n        tasks[task_id]['state'] = 'PROGRESS'\n        tasks[task_id]['progress'] = 0\n        \n        config = get_config(user_id_list)\n        wb = Weibo(config)\n        tasks[task_id]['progress'] = 50\n        \n        wb.start()  # 爬取微博信息\n        tasks[task_id]['progress'] = 100\n        tasks[task_id]['state'] = 'SUCCESS'\n        tasks[task_id]['result'] = {\"message\": \"微博列表已刷新\"}\n        \n    except Exception as e:\n        tasks[task_id]['state'] = 'FAILED'\n        tasks[task_id]['error'] = str(e)\n        logger.exception(e)\n    finally:\n        with task_lock:\n            if current_task_id == task_id:\n                current_task_id = None\n\n@app.route('/refresh', methods=['POST'])\ndef refresh():\n    global current_task_id\n    \n    # 获取请求参数\n    data = request.get_json()\n    user_id_list = data.get('user_id_list') if data else None\n    \n    # 验证参数\n    if not user_id_list or not isinstance(user_id_list, list):\n        return jsonify({\n            'error': 'Invalid user_id_list parameter'\n        }), 400\n    \n    # 检查是否有正在运行的任务\n    with task_lock:\n        running_task_id, running_task = get_running_task()\n        if running_task:\n            return jsonify({\n                'task_id': running_task_id,\n                'status': 'Task already running',\n                'state': running_task['state'],\n                'progress': running_task['progress']\n            }), 409  # 409 Conflict\n        \n        # 创建新任务\n        task_id = str(uuid.uuid4())\n        tasks[task_id] = {\n            'state': 'PENDING',\n            'progress': 0,\n            'created_at': datetime.now().isoformat(),\n            'user_id_list': user_id_list\n        }\n        current_task_id = task_id\n        \n    executor.submit(run_refresh_task, task_id, user_id_list)\n    return jsonify({\n        'task_id': task_id,\n        'status': 'Task started',\n        'state': 'PENDING',\n        'progress': 0,\n        'user_id_list': user_id_list\n    }), 202\n\n@app.route('/task/<task_id>', methods=['GET'])\ndef get_task_status(task_id):\n    task = tasks.get(task_id)\n    if not task:\n        return jsonify({'error': 'Task not found'}), 404\n        \n    response = {\n        'state': task['state'],\n        'progress': task['progress']\n    }\n    \n    if task['state'] == 'SUCCESS':\n        response['result'] = task.get('result')\n    elif task['state'] == 'FAILED':\n        response['error'] = task.get('error')\n        \n    return jsonify(response)\n\n@app.route('/weibos', methods=['GET'])\ndef get_weibos():\n    try:\n        conn = sqlite3.connect(DATABASE_PATH)\n        cursor = conn.cursor()\n        # 按created_at倒序查询所有微博\n        cursor.execute(\"SELECT * FROM weibo ORDER BY created_at DESC\")\n        columns = [column[0] for column in cursor.description]\n        weibos = []\n        for row in cursor.fetchall():\n            weibo = dict(zip(columns, row))\n            weibos.append(weibo)\n        conn.close()\n        res1 = json.dumps(weibos, ensure_ascii=False)\n        print(res1)\n        res = jsonify(weibos)\n        print(res)\n        return res, 200\n    except Exception as e:\n        logger.exception(e)\n        return {\"error\": str(e)}, 500\n\n@app.route('/weibos/<weibo_id>', methods=['GET'])\ndef get_weibo_detail(weibo_id):\n    try:\n        conn = sqlite3.connect(DATABASE_PATH)\n        cursor = conn.cursor()\n        cursor.execute(\"SELECT * FROM weibo WHERE id=?\", (weibo_id,))\n        columns = [column[0] for column in cursor.description]\n        row = cursor.fetchone()\n        conn.close()\n        \n        if row:\n            weibo = dict(zip(columns, row))\n            return jsonify(weibo), 200\n        else:\n            return {\"error\": \"Weibo not found\"}, 404\n    except Exception as e:\n        logger.exception(e)\n        return {\"error\": str(e)}, 500\n\ndef schedule_refresh():\n    \"\"\"定时刷新任务\"\"\"\n    while True:\n        try:\n            # 检查是否有运行中的任务\n            running_task_id, running_task = get_running_task()\n            if not running_task:\n                task_id = str(uuid.uuid4())\n                tasks[task_id] = {\n                    'state': 'PENDING',\n                    'progress': 0,\n                    'created_at': datetime.now().isoformat(),\n                    'user_id_list': config['user_id_list']  # 使用默认配置\n                }\n                with task_lock:\n                    global current_task_id\n                    current_task_id = task_id\n                executor.submit(run_refresh_task, task_id, config['user_id_list'])\n                logger.info(f\"Scheduled task {task_id} started\")\n            \n            time.sleep(600)  # 10分钟间隔\n        except Exception as e:\n            logger.exception(\"Schedule task error\")\n            time.sleep(60)  # 发生错误时等待1分钟后重试\n\nif __name__ == \"__main__\":\n    # 启动定时任务线程\n    scheduler_thread = threading.Thread(target=schedule_refresh, daemon=True)\n    scheduler_thread.start()\n    \n    logger.info(\"服务启动\")\n    # 启动Flask应用\n    app.run(debug=True, use_reloader=False)  # 关闭reloader避免启动两次"
        },
        {
          "name": "util",
          "type": "tree",
          "content": null
        },
        {
          "name": "weibo.py",
          "type": "blob",
          "size": 93.2431640625,
          "content": "#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\nimport codecs\nimport copy\nimport csv\nimport json\nimport logging\nimport logging.config\nimport math\nimport os\nimport random\nimport re\nimport sqlite3\nimport sys\nimport warnings\nimport webbrowser\nfrom collections import OrderedDict\nfrom datetime import date, datetime, timedelta\nfrom pathlib import Path\nfrom time import sleep\n\nimport requests\nfrom requests.exceptions import RequestException\nfrom lxml import etree\nfrom requests.adapters import HTTPAdapter\nfrom tqdm import tqdm\n\nimport const\nfrom util import csvutil\nfrom util.dateutil import convert_to_days_ago\nfrom util.notify import push_deer\n\nwarnings.filterwarnings(\"ignore\")\n\n# 如果日志文件夹不存在，则创建\nif not os.path.isdir(\"log/\"):\n    os.makedirs(\"log/\")\nlogging_path = os.path.split(os.path.realpath(__file__))[0] + os.sep + \"logging.conf\"\nlogging.config.fileConfig(logging_path)\nlogger = logging.getLogger(\"weibo\")\n\n# 日期时间格式\nDTFORMAT = \"%Y-%m-%dT%H:%M:%S\"\n\nclass Weibo(object):\n    def __init__(self, config):\n        \"\"\"Weibo类初始化\"\"\"\n        self.validate_config(config)\n        self.only_crawl_original = config[\"only_crawl_original\"]  # 取值范围为0、1,程序默认值为0,代表要爬取用户的全部微博,1代表只爬取用户的原创微博\n        self.remove_html_tag = config[\n            \"remove_html_tag\"\n        ]  # 取值范围为0、1, 0代表不移除微博中的html tag, 1代表移除\n        since_date = config[\"since_date\"]\n        # since_date 若为整数，则取该天数之前的日期；若为 yyyy-mm-dd，则增加时间\n        if isinstance(since_date, int):\n            since_date = date.today() - timedelta(since_date)\n            since_date = since_date.strftime(DTFORMAT)\n        elif self.is_date(since_date):\n            since_date = \"{}T00:00:00\".format(since_date)\n        elif self.is_datetime(since_date):\n            pass\n        else:\n            logger.error(\"since_date 格式不正确，请确认配置是否正确\")\n            sys.exit()\n        self.since_date = since_date  # 起始时间，即爬取发布日期从该值到现在的微博，形式为yyyy-mm-ddThh:mm:ss，如：2023-08-21T09:23:03\n        self.start_page = config.get(\"start_page\", 1)  # 开始爬的页，如果中途被限制而结束可以用此定义开始页码\n        self.write_mode = config[\n            \"write_mode\"\n        ]  # 结果信息保存类型，为list形式，可包含csv、mongo和mysql三种类型\n        self.original_pic_download = config[\n            \"original_pic_download\"\n        ]  # 取值范围为0、1, 0代表不下载原创微博图片,1代表下载\n        self.retweet_pic_download = config[\n            \"retweet_pic_download\"\n        ]  # 取值范围为0、1, 0代表不下载转发微博图片,1代表下载\n        self.original_video_download = config[\n            \"original_video_download\"\n        ]  # 取值范围为0、1, 0代表不下载原创微博视频,1代表下载\n        self.retweet_video_download = config[\n            \"retweet_video_download\"\n        ]  # 取值范围为0、1, 0代表不下载转发微博视频,1代表下载\n        self.download_comment = config[\"download_comment\"]  # 1代表下载评论,0代表不下载\n        self.comment_max_download_count = config[\n            \"comment_max_download_count\"\n        ]  # 如果设置了下评论，每条微博评论数会限制在这个值内\n        self.download_repost = config[\"download_repost\"]  # 1代表下载转发,0代表不下载\n        self.repost_max_download_count = config[\n            \"repost_max_download_count\"\n        ]  # 如果设置了下转发，每条微博转发数会限制在这个值内\n        self.user_id_as_folder_name = config.get(\n            \"user_id_as_folder_name\", 0\n        )  # 结果目录名，取值为0或1，决定结果文件存储在用户昵称文件夹里还是用户id文件夹里\n        cookie = config.get(\"cookie\")  # 微博cookie，可填可不填\n        user_agent = \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/86.0.4240.111 Safari/537.36\"\n        self.headers = {\"User_Agent\": user_agent, \"Cookie\": cookie}\n        self.mysql_config = config.get(\"mysql_config\")  # MySQL数据库连接配置，可以不填\n        self.mongodb_URI = config.get(\"mongodb_URI\")  # MongoDB数据库连接字符串，可以不填\n        self.post_config = config.get(\"post_config\")  # post_config，可以不填\n        user_id_list = config[\"user_id_list\"]\n        # 避免卡住\n        if isinstance(user_id_list, list):\n            random.shuffle(user_id_list)\n\n        query_list = config.get(\"query_list\") or []\n        if isinstance(query_list, str):\n            query_list = query_list.split(\",\")\n        self.query_list = query_list\n        if not isinstance(user_id_list, list):\n            if not os.path.isabs(user_id_list):\n                user_id_list = (\n                    os.path.split(os.path.realpath(__file__))[0] + os.sep + user_id_list\n                )\n            self.user_config_file_path = user_id_list  # 用户配置文件路径\n            user_config_list = self.get_user_config_list(user_id_list)\n        else:\n            self.user_config_file_path = \"\"\n            user_config_list = [\n                {\n                    \"user_id\": user_id,\n                    \"since_date\": self.since_date,\n                    \"query_list\": query_list,\n                }\n                for user_id in user_id_list\n            ]\n\n        self.user_config_list = user_config_list  # 要爬取的微博用户的user_config列表\n        self.user_config = {}  # 用户配置,包含用户id和since_date\n        self.start_date = \"\"  # 获取用户第一条微博时的日期\n        self.query = \"\"\n        self.user = {}  # 存储目标微博用户信息\n        self.got_count = 0  # 存储爬取到的微博数\n        self.weibo = []  # 存储爬取到的所有微博信息\n        self.weibo_id_list = []  # 存储爬取到的所有微博id\n        self.long_sleep_count_before_each_user = 0 #每个用户前的长时间sleep避免被ban\n\n    def validate_config(self, config):\n        \"\"\"验证配置是否正确\"\"\"\n\n        # 验证如下1/0相关值\n        argument_list = [\n            \"only_crawl_original\",\n            \"original_pic_download\",\n            \"retweet_pic_download\",\n            \"original_video_download\",\n            \"retweet_video_download\",\n            \"download_comment\",\n            \"download_repost\",\n        ]\n        for argument in argument_list:\n            if config[argument] != 0 and config[argument] != 1:\n                logger.warning(\"%s值应为0或1,请重新输入\", config[argument])\n                sys.exit()\n\n        # 验证query_list\n        query_list = config.get(\"query_list\") or []\n        if (not isinstance(query_list, list)) and (not isinstance(query_list, str)):\n            logger.warning(\"query_list值应为list类型或字符串,请重新输入\")\n            sys.exit()\n\n        # 验证write_mode\n        write_mode = [\"csv\", \"json\", \"mongo\", \"mysql\", \"sqlite\", \"post\"]\n        if not isinstance(config[\"write_mode\"], list):\n            sys.exit(\"write_mode值应为list类型\")\n        for mode in config[\"write_mode\"]:\n            if mode not in write_mode:\n                logger.warning(\n                    \"%s为无效模式，请从csv、json、mongo和mysql中挑选一个或多个作为write_mode\", mode\n                )\n                sys.exit()\n        # 验证运行模式\n        if \"sqlite\" not in config[\"write_mode\"] and const.MODE == \"append\":\n            logger.warning(\"append模式下请将sqlite加入write_mode中\")\n            sys.exit()\n\n        # 验证user_id_list\n        user_id_list = config[\"user_id_list\"]\n        if (not isinstance(user_id_list, list)) and (not user_id_list.endswith(\".txt\")):\n            logger.warning(\"user_id_list值应为list类型或txt文件路径\")\n            sys.exit()\n        if not isinstance(user_id_list, list):\n            if not os.path.isabs(user_id_list):\n                user_id_list = (\n                    os.path.split(os.path.realpath(__file__))[0] + os.sep + user_id_list\n                )\n            if not os.path.isfile(user_id_list):\n                logger.warning(\"不存在%s文件\", user_id_list)\n                sys.exit()\n\n        # 验证since_date\n        since_date = config[\"since_date\"]\n        if (not isinstance(since_date, int)) and (not self.is_datetime(since_date)) and (not self.is_date(since_date)):\n            logger.warning(\"since_date值应为yyyy-mm-dd形式、yyyy-mm-ddTHH:MM:SS形式或整数，请重新输入\")\n            sys.exit()\n\n        comment_max_count = config[\"comment_max_download_count\"]\n        if not isinstance(comment_max_count, int):\n            logger.warning(\"最大下载评论数 (comment_max_download_count) 应为整数类型\")\n            sys.exit()\n        elif comment_max_count < 0:\n            logger.warning(\"最大下载评论数 (comment_max_download_count) 应该为正整数\")\n            sys.exit()\n\n        repost_max_count = config[\"repost_max_download_count\"]\n        if not isinstance(repost_max_count, int):\n            logger.warning(\"最大下载转发数 (repost_max_download_count) 应为整数类型\")\n            sys.exit()\n        elif repost_max_count < 0:\n            logger.warning(\"最大下载转发数 (repost_max_download_count) 应该为正整数\")\n            sys.exit()\n\n    def is_datetime(self, since_date):\n        \"\"\"判断日期格式是否为 %Y-%m-%dT%H:%M:%S\"\"\"\n        try:\n            datetime.strptime(since_date, DTFORMAT)\n            return True\n        except ValueError:\n            return False\n    \n    def is_date(self, since_date):\n        \"\"\"判断日期格式是否为 %Y-%m-%d\"\"\"\n        try:\n            datetime.strptime(since_date, \"%Y-%m-%d\")\n            return True\n        except ValueError:\n            return False\n\n    def get_json(self, params):\n        \"\"\"获取网页中的 JSON 数据\"\"\"\n        url = \"https://m.weibo.cn/api/container/getIndex?\"\n        try:\n            r = requests.get(url, params=params, headers=self.headers, verify=False, timeout=10)\n            r.raise_for_status()  # 如果响应状态码不是 200，会抛出 HTTPError\n            response_json = r.json()\n            return response_json, r.status_code\n        except RequestException as e:\n            logger.error(f\"请求失败，错误信息：{e}\")\n            return {}, 500\n        except ValueError as ve:\n            logger.error(f\"JSON 解码失败，错误信息：{ve}\")\n            return {}, 500\n\n    def handle_captcha(self, js):\n        \"\"\"\n        处理验证码挑战，提示用户手动完成验证。\n\n        参数:\n            js (dict): API 返回的 JSON 数据。\n\n        返回:\n            bool: 如果用户成功完成验证码，返回 True；否则返回 False。\n        \"\"\"\n        logger.debug(f\"收到的 JSON 数据：{js}\")\n        \n        captcha_url = js.get(\"url\")\n        if captcha_url:\n            logger.warning(\"检测到验证码挑战。正在打开验证码页面以供手动验证。\")\n            webbrowser.open(captcha_url)\n        else:\n            logger.warning(\"检测到可能的验证码挑战，但未提供验证码 URL。请手动检查浏览器并完成验证码验证。\")\n            return False\n        \n        logger.info(\"请在打开的浏览器窗口中完成验证码验证。\")\n        while True:\n            try:\n                            # 等待用户输入\n                user_input = input(\"完成验证码后，请输入 'y' 继续，或输入 'q' 退出：\").strip().lower()\n\n                if user_input == 'y':\n                    logger.info(\"用户输入 'y'，继续爬取。\")\n                    return True\n                elif user_input == 'q':\n                    logger.warning(\"用户选择退出，程序中止。\")\n                    sys.exit(\"用户选择退出，程序中止。\")\n                else:\n                    logger.warning(\"无效输入，请重新输入 'y' 或 'q'。\")\n            except EOFError:\n                logger.error(\"读取用户输入时发生 EOFError，程序退出。\")\n                sys.exit(\"输入流已关闭，程序中止。\")\n    \n    def get_weibo_json(self, page):\n        \"\"\"获取网页中微博json数据\"\"\"\n        url = \"https://m.weibo.cn/api/container/getIndex?\"\n        params = (\n            {\n                \"container_ext\": \"profile_uid:\" + str(self.user_config[\"user_id\"]),\n                \"containerid\": \"100103type=401&q=\" + self.query,\n                \"page_type\": \"searchall\",\n            }\n            if self.query\n            else {\"containerid\": \"230413\" + str(self.user_config[\"user_id\"])}\n        )\n        params[\"page\"] = page\n\n        max_retries = 5\n        retries = 0\n        backoff_factor = 5\n\n        while retries < max_retries:\n            try:\n                response = requests.get(url, params=params, headers=self.headers, timeout=10)\n                response.raise_for_status()  # 如果响应状态码不是 200，会抛出 HTTPError\n                js = response.json()\n                if 'data' in js:\n                    logger.info(f\"成功获取到页面 {page} 的数据。\")\n                    return js\n                else:\n                    logger.warning(\"未能获取到数据，可能需要验证码验证。\")\n                    if self.handle_captcha(js):\n                        logger.info(\"用户已完成验证码验证，继续请求数据。\")\n                        retries = 0  # 重置重试计数器\n                        continue\n                    else:\n                        logger.error(\"验证码验证失败或未完成，程序将退出。\")\n                        sys.exit()\n            except RequestException as e:\n                retries += 1\n                sleep_time = backoff_factor * (2 ** retries)\n                logger.error(f\"请求失败，错误信息：{e}。等待 {sleep_time} 秒后重试...\")\n                sleep(sleep_time)\n            except ValueError as ve:\n                retries += 1\n                sleep_time = backoff_factor * (2 ** retries)\n                logger.error(f\"JSON 解码失败，错误信息：{ve}。等待 {sleep_time} 秒后重试...\")\n                sleep(sleep_time)\n        logger.error(\"超过最大重试次数，跳过当前页面。\")\n        return {}\n    \n    def user_to_csv(self):\n        \"\"\"将爬取到的用户信息写入csv文件\"\"\"\n        file_dir = os.path.split(os.path.realpath(__file__))[0] + os.sep + \"weibo\"\n        if not os.path.isdir(file_dir):\n            os.makedirs(file_dir)\n        file_path = file_dir + os.sep + \"users.csv\"\n        self.user_csv_file_path = file_path\n        result_headers = [\n            \"用户id\",\n            \"昵称\",\n            \"性别\",\n            \"生日\",\n            \"所在地\",\n            \"学习经历\",\n            \"公司\",\n            \"注册时间\",\n            \"阳光信用\",\n            \"微博数\",\n            \"粉丝数\",\n            \"关注数\",\n            \"简介\",\n            \"主页\",\n            \"头像\",\n            \"高清头像\",\n            \"微博等级\",\n            \"会员等级\",\n            \"是否认证\",\n            \"认证类型\",\n            \"认证信息\",\n            \"上次记录微博信息\",\n        ]\n        result_data = [\n            [\n                v.encode(\"utf-8\") if \"unicode\" in str(type(v)) else v\n                for v in self.user.values()\n            ]\n        ]\n        # 已经插入信息的用户无需重复插入，返回的id是空字符串或微博id 发布日期%Y-%m-%d\n        last_weibo_msg = csvutil.insert_or_update_user(\n            logger, result_headers, result_data, file_path\n        )\n        self.last_weibo_id = last_weibo_msg.split(\" \")[0] if last_weibo_msg else \"\"\n        self.last_weibo_date = (\n            last_weibo_msg.split(\" \")[1]\n            if last_weibo_msg\n            else self.user_config[\"since_date\"]\n        )\n\n    def user_to_mongodb(self):\n        \"\"\"将爬取的用户信息写入MongoDB数据库\"\"\"\n        user_list = [self.user]\n        self.info_to_mongodb(\"user\", user_list)\n        logger.info(\"%s信息写入MongoDB数据库完毕\", self.user[\"screen_name\"])\n\n    def user_to_mysql(self):\n        \"\"\"将爬取的用户信息写入MySQL数据库\"\"\"\n        mysql_config = {\n            \"host\": \"localhost\",\n            \"port\": 3306,\n            \"user\": \"root\",\n            \"password\": \"123456\",\n            \"charset\": \"utf8mb4\",\n        }\n        # 创建'weibo'数据库\n        create_database = \"\"\"CREATE DATABASE IF NOT EXISTS weibo DEFAULT\n                         CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci\"\"\"\n        self.mysql_create_database(mysql_config, create_database)\n        # 创建'user'表\n        create_table = \"\"\"\n                CREATE TABLE IF NOT EXISTS user (\n                id varchar(20) NOT NULL,\n                screen_name varchar(30),\n                gender varchar(10),\n                statuses_count INT,\n                followers_count INT,\n                follow_count INT,\n                registration_time varchar(20),\n                sunshine varchar(20),\n                birthday varchar(40),\n                location varchar(200),\n                education varchar(200),\n                company varchar(200),\n                description varchar(400),\n                profile_url varchar(200),\n                profile_image_url varchar(200),\n                avatar_hd varchar(200),\n                urank INT,\n                mbrank INT,\n                verified BOOLEAN DEFAULT 0,\n                verified_type INT,\n                verified_reason varchar(140),\n                PRIMARY KEY (id)\n                ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4\"\"\"\n        self.mysql_create_table(mysql_config, create_table)\n        self.mysql_insert(mysql_config, \"user\", [self.user])\n        logger.info(\"%s信息写入MySQL数据库完毕\", self.user[\"screen_name\"])\n\n    def user_to_database(self):\n        \"\"\"将用户信息写入文件/数据库\"\"\"\n        self.user_to_csv()\n        if \"mysql\" in self.write_mode:\n            self.user_to_mysql()\n        if \"mongo\" in self.write_mode:\n            self.user_to_mongodb()\n        if \"sqlite\" in self.write_mode:\n            self.user_to_sqlite()\n\n    def get_user_info(self):\n        \"\"\"获取用户信息\"\"\"\n        params = {\"containerid\": \"100505\" + str(self.user_config[\"user_id\"])}\n        url = \"https://m.weibo.cn/api/container/getIndex\"\n        \n        # 这里在读取下一个用户的时候很容易被ban，需要优化休眠时长\n        # 加一个count，不需要一上来啥都没干就sleep\n        if self.long_sleep_count_before_each_user > 0:\n            sleep_time = random.randint(30, 60)\n            # 添加log，否则一般用户不知道以为程序卡了\n            logger.info(f\"\"\"短暂sleep {sleep_time}秒，避免被ban\"\"\")        \n            sleep(sleep_time)\n            logger.info(\"sleep结束\")  \n        self.long_sleep_count_before_each_user = self.long_sleep_count_before_each_user + 1      \n\n        max_retries = 5  # 设置最大重试次数，避免无限循环\n        retries = 0\n        backoff_factor = 5  # 指数退避的基数（秒）\n        \n        while retries < max_retries:\n            try:\n                response = requests.get(url, params=params, headers=self.headers, timeout=10)\n                response.raise_for_status()\n                js = response.json()\n                if 'data' in js and 'userInfo' in js['data']:\n                    info = js[\"data\"][\"userInfo\"]\n                    user_info = OrderedDict()\n                    user_info[\"id\"] = self.user_config[\"user_id\"]\n                    user_info[\"screen_name\"] = info.get(\"screen_name\", \"\")\n                    user_info[\"gender\"] = info.get(\"gender\", \"\")\n                    params = {\n                        \"containerid\": \"230283\" + str(self.user_config[\"user_id\"]) + \"_-_INFO\"\n                    }\n                    zh_list = [\"生日\", \"所在地\", \"小学\", \"初中\", \"高中\", \"大学\", \"公司\", \"注册时间\", \"阳光信用\"]\n                    en_list = [\n                        \"birthday\",\n                        \"location\",\n                        \"education\",\n                        \"education\",\n                        \"education\",\n                        \"education\",\n                        \"company\",\n                        \"registration_time\",\n                        \"sunshine\",\n                    ]\n                    for i in en_list:\n                        user_info[i] = \"\"\n                    js, _ = self.get_json(params)\n                    if js[\"ok\"]:\n                        cards = js[\"data\"][\"cards\"]\n                        if isinstance(cards, list) and len(cards) > 1:\n                            card_list = cards[0][\"card_group\"] + cards[1][\"card_group\"]\n                            for card in card_list:\n                                if card.get(\"item_name\") in zh_list:\n                                    user_info[\n                                        en_list[zh_list.index(card.get(\"item_name\"))]\n                                    ] = card.get(\"item_content\", \"\")\n                    user_info[\"statuses_count\"] = self.string_to_int(\n                        info.get(\"statuses_count\", 0)\n                    )\n                    user_info[\"followers_count\"] = self.string_to_int(\n                        info.get(\"followers_count\", 0)\n                    )\n                    user_info[\"follow_count\"] = self.string_to_int(info.get(\"follow_count\", 0))\n                    user_info[\"description\"] = info.get(\"description\", \"\")\n                    user_info[\"profile_url\"] = info.get(\"profile_url\", \"\")\n                    user_info[\"profile_image_url\"] = info.get(\"profile_image_url\", \"\")\n                    user_info[\"avatar_hd\"] = info.get(\"avatar_hd\", \"\")\n                    user_info[\"urank\"] = info.get(\"urank\", 0)\n                    user_info[\"mbrank\"] = info.get(\"mbrank\", 0)\n                    user_info[\"verified\"] = info.get(\"verified\", False)\n                    user_info[\"verified_type\"] = info.get(\"verified_type\", -1)\n                    user_info[\"verified_reason\"] = info.get(\"verified_reason\", \"\")\n                    self.user = self.standardize_info(user_info)\n                    self.user_to_database()\n                    logger.info(f\"成功获取到用户 {self.user_config['user_id']} 的信息。\")\n                    return 0\n                else:\n                    logger.warning(\"未能获取到用户信息，可能需要验证码验证。\")\n                    if self.handle_captcha(js):\n                        logger.info(\"用户已完成验证码验证，继续请求用户信息。\")\n                        retries = 0  # 重置重试计数器\n                        continue\n                    else:\n                        logger.error(\"验证码验证失败或未完成，程序将退出。\")\n                        sys.exit()\n            except RequestException as e:\n                retries += 1\n                sleep_time = backoff_factor * (2 ** retries)\n                logger.error(f\"请求失败，错误信息：{e}。等待 {sleep_time} 秒后重试...\")\n                sleep(sleep_time)\n            except ValueError as ve:\n                retries += 1\n                sleep_time = backoff_factor * (2 ** retries)\n                logger.error(f\"JSON 解码失败，错误信息：{ve}。等待 {sleep_time} 秒后重试...\")\n                sleep(sleep_time)\n        logger.error(\"超过最大重试次数，程序将退出。\")\n        sys.exit(\"超过最大重试次数，程序已退出。\")\n\n    def get_long_weibo(self, id):\n        \"\"\"获取长微博\"\"\"\n        for i in range(5):\n            url = \"https://m.weibo.cn/detail/%s\" % id\n            logger.info(f\"\"\"URL: {url} \"\"\")\n            html = requests.get(url, headers=self.headers, verify=False).text\n            html = html[html.find('\"status\":') :]\n            html = html[: html.rfind('\"call\"')]\n            html = html[: html.rfind(\",\")]\n            html = \"{\" + html + \"}\"\n            js = json.loads(html, strict=False)\n            weibo_info = js.get(\"status\")\n            if weibo_info:\n                weibo = self.parse_weibo(weibo_info)\n                return weibo\n            sleep(random.randint(6, 10))\n\n    def get_pics(self, weibo_info):\n        \"\"\"获取微博原始图片url\"\"\"\n        if weibo_info.get(\"pics\"):\n            pic_info = weibo_info[\"pics\"]\n            pic_list = [pic[\"large\"][\"url\"] for pic in pic_info]\n            pics = \",\".join(pic_list)\n        else:\n            pics = \"\"\n        return pics\n\n    def get_live_photo(self, weibo_info):\n        \"\"\"获取live photo中的视频url\"\"\"\n        live_photo_list = weibo_info.get(\"live_photo\", [])\n        return live_photo_list\n\n    def get_video_url(self, weibo_info):\n        \"\"\"获取微博视频url\"\"\"\n        video_url = \"\"\n        video_url_list = []\n        if weibo_info.get(\"page_info\"):\n            if (\n                weibo_info[\"page_info\"].get(\"urls\")\n                or weibo_info[\"page_info\"].get(\"media_info\")\n            ) and weibo_info[\"page_info\"].get(\"type\") == \"video\":\n                media_info = weibo_info[\"page_info\"][\"urls\"]\n                if not media_info:\n                    media_info = weibo_info[\"page_info\"][\"media_info\"]\n                video_url = media_info.get(\"mp4_720p_mp4\")\n                if not video_url:\n                    video_url = media_info.get(\"mp4_hd_url\")\n                if not video_url:\n                    video_url = media_info.get(\"hevc_mp4_hd\")\n                if not video_url:\n                    video_url = media_info.get(\"mp4_sd_url\")\n                if not video_url:\n                    video_url = media_info.get(\"mp4_ld_mp4\")\n                if not video_url:\n                    video_url = media_info.get(\"stream_url_hd\")\n                if not video_url:\n                    video_url = media_info.get(\"stream_url\")\n        if video_url:\n            video_url_list.append(video_url)\n        live_photo_list = self.get_live_photo(weibo_info)\n        if live_photo_list:\n            video_url_list += live_photo_list\n        return \";\".join(video_url_list)\n\n    def download_one_file(self, url, file_path, type, weibo_id):\n        \"\"\"下载单个文件(图片/视频)\"\"\"\n        try:\n\n            file_exist = os.path.isfile(file_path)\n            need_download = (not file_exist)\n            sqlite_exist = False\n            if \"sqlite\" in self.write_mode:\n                sqlite_exist = self.sqlite_exist_file(file_path)\n                if not sqlite_exist: \n                    need_download = True\n\n            if not need_download:\n                return \n\n            s = requests.Session()\n            s.mount('http://', HTTPAdapter(max_retries=5))\n            s.mount('https://', HTTPAdapter(max_retries=5))\n            try_count = 0\n            success = False\n            MAX_TRY_COUNT = 3\n            detected_extension = None\n            while try_count < MAX_TRY_COUNT:\n                try:\n                    response = s.get(\n                        url, headers=self.headers, timeout=(5, 10), verify=False\n                    )\n                    response.raise_for_status()\n                    downloaded = response.content\n                    try_count += 1\n\n                    # 获取文件后缀\n                    url_path = url.split('?')[0]  # 去除URL中的参数\n                    inferred_extension = os.path.splitext(url_path)[1].lower().strip('.')\n\n                    # 通过 Magic Number 检测文件类型\n                    if downloaded.startswith(b'\\xFF\\xD8\\xFF'):\n                        # JPEG 文件\n                        if not downloaded.endswith(b'\\xff\\xd9'):\n                            logger.debug(f\"[DEBUG] JPEG 文件不完整: {url} ({try_count}/{MAX_TRY_COUNT})\")\n                            continue  # 文件不完整，继续重试\n                        detected_extension = '.jpg'\n                    elif downloaded.startswith(b'\\x89PNG\\r\\n\\x1A\\n'):\n                        # PNG 文件\n                        if not downloaded.endswith(b'IEND\\xaeB`\\x82'):\n                            logger.debug(f\"[DEBUG] PNG 文件不完整: {url} ({try_count}/{MAX_TRY_COUNT})\")\n                            continue  # 文件不完整，继续重试\n                        detected_extension = '.png'\n                    else:\n                        # 其他类型，使用原有逻辑处理\n                        if inferred_extension in ['mp4', 'mov', 'webm', 'gif', 'bmp', 'tiff']:\n                            detected_extension = '.' + inferred_extension\n                        else:\n                            # 尝试从 Content-Type 获取扩展名\n                            content_type = response.headers.get('Content-Type', '').lower()\n                            if 'image/jpeg' in content_type:\n                                detected_extension = '.jpg'\n                            elif 'image/png' in content_type:\n                                detected_extension = '.png'\n                            elif 'video/mp4' in content_type:\n                                detected_extension = '.mp4'\n                            elif 'video/quicktime' in content_type:\n                                detected_extension = '.mov'\n                            elif 'video/webm' in content_type:\n                                detected_extension = '.webm'\n                            elif 'image/gif' in content_type:\n                                detected_extension = '.gif'\n                            else:\n                                # 使用原有的扩展名，如果无法确定\n                                detected_extension = '.' + inferred_extension if inferred_extension else ''\n\n                    # 动态调整文件路径的扩展名\n                    if detected_extension:\n                        file_path = re.sub(r'\\.\\w+$', detected_extension, file_path)\n\n                    # 保存文件\n                    if not os.path.isfile(file_path):\n                        with open(file_path, \"wb\") as f:\n                            f.write(downloaded)\n                            logger.debug(\"[DEBUG] save \" + file_path)\n\n                    success = True\n                    logger.debug(\"[DEBUG] success \" + url + \"  \" + str(try_count))\n                    break  # 下载成功，退出重试循环\n\n                except RequestException as e:\n                    try_count += 1\n                    logger.error(f\"[ERROR] 请求失败，错误信息：{e}。尝试次数：{try_count}/{MAX_TRY_COUNT}\")\n                    sleep_time = 2 ** try_count  # 指数退避\n                    sleep(sleep_time)\n                except Exception as e:\n                    logger.exception(f\"[ERROR] 下载过程中发生错误: {e}\")\n                    break  # 对于其他异常，退出重试\n\n            if success:\n                if \"sqlite\" in self.write_mode and not sqlite_exist:\n                    self.insert_file_sqlite(\n                        file_path, weibo_id, url, downloaded\n                    )\n            else:\n                logger.debug(\"[DEBUG] failed \" + url + \" TOTALLY\")\n                error_file = self.get_filepath(type) + os.sep + \"not_downloaded.txt\"\n                with open(error_file, \"ab\") as f:\n                    error_entry = f\"{weibo_id}:{file_path}:{url}\\n\"\n                    f.write(error_entry.encode(sys.stdout.encoding))\n        except Exception as e:\n            error_file = self.get_filepath(type) + os.sep + \"not_downloaded.txt\"\n            with open(error_file, \"ab\") as f:\n                error_entry = f\"{weibo_id}:{file_path}:{url}\\n\"\n                f.write(error_entry.encode(sys.stdout.encoding))\n            logger.exception(e)\n\n    def sqlite_exist_file(self, url):\n        if not os.path.exists(self.get_sqlte_path()):\n            return True\n        con = self.get_sqlite_connection()\n        cur = con.cursor()\n\n        query_sql = \"\"\"SELECT url FROM bins WHERE path=? \"\"\"\n        count = cur.execute(query_sql, (url,)).fetchone()\n        con.close()\n        if count is None:\n            return False\n\n        return True\n\n    def insert_file_sqlite(self, file_path, weibo_id, url, binary):\n        if not weibo_id:\n            return\n        extension = Path(file_path).suffix\n        if not extension:\n            return\n        if len(binary) <= 0:\n            return\n\n        file_data = OrderedDict()\n        file_data[\"weibo_id\"] = weibo_id\n        file_data[\"ext\"] = extension\n        file_data[\"data\"] = binary\n        file_data[\"path\"] = file_path\n        file_data[\"url\"] = url\n\n        con = self.get_sqlite_connection()\n        self.sqlite_insert(con, file_data, \"bins\")\n        con.close()\n\n    def handle_download(self, file_type, file_dir, urls, w):\n        \"\"\"处理下载相关操作\"\"\"\n        file_prefix = w[\"created_at\"][:11].replace(\"-\", \"\") + \"_\" + str(w[\"id\"])\n        if file_type == \"img\":\n            if \",\" in urls:\n                url_list = urls.split(\",\")\n                for i, url in enumerate(url_list):\n                    index = url.rfind(\".\")\n                    if len(url) - index >= 5:\n                        file_suffix = \".jpg\"\n                    else:\n                        file_suffix = url[index:]\n                    file_name = file_prefix + \"_\" + str(i + 1) + file_suffix\n                    file_path = file_dir + os.sep + file_name\n                    self.download_one_file(url, file_path, file_type, w[\"id\"])\n            else:\n                index = urls.rfind(\".\")\n                if len(urls) - index > 5:\n                    file_suffix = \".jpg\"\n                else:\n                    file_suffix = urls[index:]\n                file_name = file_prefix + file_suffix\n                file_path = file_dir + os.sep + file_name\n                self.download_one_file(urls, file_path, file_type, w[\"id\"])\n        else:\n            file_suffix = \".mp4\"\n            if \";\" in urls:\n                url_list = urls.split(\";\")\n                if url_list[0].endswith(\".mov\"):\n                    file_suffix = \".mov\"\n                for i, url in enumerate(url_list):\n                    file_name = file_prefix + \"_\" + str(i + 1) + file_suffix\n                    file_path = file_dir + os.sep + file_name\n                    self.download_one_file(url, file_path, file_type, w[\"id\"])\n            else:\n                if urls.endswith(\".mov\"):\n                    file_suffix = \".mov\"\n                file_name = file_prefix + file_suffix\n                file_path = file_dir + os.sep + file_name\n                self.download_one_file(urls, file_path, file_type, w[\"id\"])\n\n    def download_files(self, file_type, weibo_type, wrote_count):\n        \"\"\"下载文件(图片/视频)\"\"\"\n        try:\n            describe = \"\"\n            if file_type == \"img\":\n                describe = \"图片\"\n                key = \"pics\"\n            else:\n                describe = \"视频\"\n                key = \"video_url\"\n            if weibo_type == \"original\":\n                describe = \"原创微博\" + describe\n            else:\n                describe = \"转发微博\" + describe\n            logger.info(\"即将进行%s下载\", describe)\n            file_dir = self.get_filepath(file_type)\n            file_dir = file_dir + os.sep + describe\n            if not os.path.isdir(file_dir):\n                os.makedirs(file_dir)\n            for w in tqdm(self.weibo[wrote_count:], desc=\"Download progress\"):\n                if weibo_type == \"retweet\":\n                    if w.get(\"retweet\"):\n                        w = w[\"retweet\"]\n                    else:\n                        continue\n                if w.get(key):\n                    self.handle_download(file_type, file_dir, w.get(key), w)\n            logger.info(\"%s下载完毕,保存路径:\", describe)\n            logger.info(file_dir)\n        except Exception as e:\n            logger.exception(e)\n\n    def get_location(self, selector):\n        \"\"\"获取微博发布位置\"\"\"\n        location_icon = \"timeline_card_small_location_default.png\"\n        span_list = selector.xpath(\"//span\")\n        location = \"\"\n        for i, span in enumerate(span_list):\n            if span.xpath(\"img/@src\"):\n                if location_icon in span.xpath(\"img/@src\")[0]:\n                    location = span_list[i + 1].xpath(\"string(.)\")\n                    break\n        return location\n\n    def get_article_url(self, selector):\n        \"\"\"获取微博中头条文章的url\"\"\"\n        article_url = \"\"\n        text = selector.xpath(\"string(.)\")\n        if text.startswith(\"发布了头条文章\"):\n            url = selector.xpath(\"//a/@data-url\")\n            if url and url[0].startswith(\"http://t.cn\"):\n                article_url = url[0]\n        return article_url\n\n    def get_topics(self, selector):\n        \"\"\"获取参与的微博话题\"\"\"\n        span_list = selector.xpath(\"//span[@class='surl-text']\")\n        topics = \"\"\n        topic_list = []\n        for span in span_list:\n            text = span.xpath(\"string(.)\")\n            if len(text) > 2 and text[0] == \"#\" and text[-1] == \"#\":\n                topic_list.append(text[1:-1])\n        if topic_list:\n            topics = \",\".join(topic_list)\n        return topics\n\n    def get_at_users(self, selector):\n        \"\"\"获取@用户\"\"\"\n        a_list = selector.xpath(\"//a\")\n        at_users = \"\"\n        at_list = []\n        for a in a_list:\n            if \"@\" + a.xpath(\"@href\")[0][3:] == a.xpath(\"string(.)\"):\n                at_list.append(a.xpath(\"string(.)\")[1:])\n        if at_list:\n            at_users = \",\".join(at_list)\n        return at_users\n\n    def string_to_int(self, string):\n        \"\"\"字符串转换为整数\"\"\"\n        if isinstance(string, int):\n            return string\n        elif string.endswith(\"万+\"):\n            string = string[:-2] + \"0000\"\n        elif string.endswith(\"万\"):\n            string = float(string[:-1]) * 10000\n        elif string.endswith(\"亿\"):\n            string = float(string[:-1]) * 100000000\n        return int(string)\n\n    def standardize_date(self, created_at):\n        \"\"\"标准化微博发布时间\"\"\"\n        if \"刚刚\" in created_at:\n            ts = datetime.now()\n        elif \"分钟\" in created_at:\n            minute = created_at[: created_at.find(\"分钟\")]\n            minute = timedelta(minutes=int(minute))\n            ts = datetime.now() - minute\n        elif \"小时\" in created_at:\n            hour = created_at[: created_at.find(\"小时\")]\n            hour = timedelta(hours=int(hour))\n            ts = datetime.now() - hour\n        elif \"昨天\" in created_at:\n            day = timedelta(days=1)\n            ts = datetime.now() - day\n        else:\n            created_at = created_at.replace(\"+0800 \", \"\")\n            ts = datetime.strptime(created_at, \"%c\")\n\n        created_at = ts.strftime(DTFORMAT)\n        full_created_at = ts.strftime(\"%Y-%m-%d %H:%M:%S\")\n        return created_at, full_created_at\n\n    def standardize_info(self, weibo):\n        \"\"\"标准化信息，去除乱码\"\"\"\n        for k, v in weibo.items():\n            if (\n                \"bool\" not in str(type(v))\n                and \"int\" not in str(type(v))\n                and \"list\" not in str(type(v))\n                and \"long\" not in str(type(v))\n            ):\n                weibo[k] = (\n                    v.replace(\"\\u200b\", \"\")\n                    .encode(sys.stdout.encoding, \"ignore\")\n                    .decode(sys.stdout.encoding)\n                )\n        return weibo\n\n    def parse_weibo(self, weibo_info):\n        weibo = OrderedDict()\n        if weibo_info[\"user\"]:\n            weibo[\"user_id\"] = weibo_info[\"user\"][\"id\"]\n            weibo[\"screen_name\"] = weibo_info[\"user\"][\"screen_name\"]\n        else:\n            weibo[\"user_id\"] = \"\"\n            weibo[\"screen_name\"] = \"\"\n        weibo[\"id\"] = int(weibo_info[\"id\"])\n        weibo[\"bid\"] = weibo_info[\"bid\"]\n        text_body = weibo_info[\"text\"]\n        selector = etree.HTML(f\"{text_body}<hr>\" if text_body.isspace() else text_body)\n        if self.remove_html_tag:\n            text_list = selector.xpath(\"//text()\")\n            # 若text_list中的某个字符串元素以 @ 或 # 开始，则将该元素与前后元素合并为新元素，否则会带来没有必要的换行\n            text_list_modified = []\n            for ele in range(len(text_list)):\n                if ele > 0 and (text_list[ele-1].startswith(('@','#')) or text_list[ele].startswith(('@','#'))):\n                    text_list_modified[-1] += text_list[ele]\n                else:\n                    text_list_modified.append(text_list[ele])\n            weibo[\"text\"] = \"\\n\".join(text_list_modified)\n        else:\n            weibo[\"text\"] = text_body\n        weibo[\"article_url\"] = self.get_article_url(selector)\n        weibo[\"pics\"] = self.get_pics(weibo_info)\n        weibo[\"video_url\"] = self.get_video_url(weibo_info)\n        weibo[\"location\"] = self.get_location(selector)\n        weibo[\"created_at\"] = weibo_info[\"created_at\"]\n        weibo[\"source\"] = weibo_info[\"source\"]\n        weibo[\"attitudes_count\"] = self.string_to_int(\n            weibo_info.get(\"attitudes_count\", 0)\n        )\n        weibo[\"comments_count\"] = self.string_to_int(\n            weibo_info.get(\"comments_count\", 0)\n        )\n        weibo[\"reposts_count\"] = self.string_to_int(weibo_info.get(\"reposts_count\", 0))\n        weibo[\"topics\"] = self.get_topics(selector)\n        weibo[\"at_users\"] = self.get_at_users(selector)\n        return self.standardize_info(weibo)\n\n    def print_user_info(self):\n        \"\"\"打印用户信息\"\"\"\n        logger.info(\"+\" * 100)\n        logger.info(\"用户信息\")\n        logger.info(\"用户id：%s\", self.user[\"id\"])\n        logger.info(\"用户昵称：%s\", self.user[\"screen_name\"])\n        gender = \"女\" if self.user[\"gender\"] == \"f\" else \"男\"\n        logger.info(\"性别：%s\", gender)\n        logger.info(\"生日：%s\", self.user[\"birthday\"])\n        logger.info(\"所在地：%s\", self.user[\"location\"])\n        logger.info(\"教育经历：%s\", self.user[\"education\"])\n        logger.info(\"公司：%s\", self.user[\"company\"])\n        logger.info(\"阳光信用：%s\", self.user[\"sunshine\"])\n        logger.info(\"注册时间：%s\", self.user[\"registration_time\"])\n        logger.info(\"微博数：%d\", self.user[\"statuses_count\"])\n        logger.info(\"粉丝数：%d\", self.user[\"followers_count\"])\n        logger.info(\"关注数：%d\", self.user[\"follow_count\"])\n        logger.info(\"url：https://m.weibo.cn/profile/%s\", self.user[\"id\"])\n        if self.user.get(\"verified_reason\"):\n            logger.info(self.user[\"verified_reason\"])\n        logger.info(self.user[\"description\"])\n        logger.info(\"+\" * 100)\n\n    def print_one_weibo(self, weibo):\n        \"\"\"打印一条微博\"\"\"\n        try:\n            logger.info(\"微博id：%d\", weibo[\"id\"])\n            logger.info(\"微博正文：%s\", weibo[\"text\"])\n            logger.info(\"原始图片url：%s\", weibo[\"pics\"])\n            logger.info(\"微博位置：%s\", weibo[\"location\"])\n            logger.info(\"发布时间：%s\", weibo[\"created_at\"])\n            logger.info(\"发布工具：%s\", weibo[\"source\"])\n            logger.info(\"点赞数：%d\", weibo[\"attitudes_count\"])\n            logger.info(\"评论数：%d\", weibo[\"comments_count\"])\n            logger.info(\"转发数：%d\", weibo[\"reposts_count\"])\n            logger.info(\"话题：%s\", weibo[\"topics\"])\n            logger.info(\"@用户：%s\", weibo[\"at_users\"])\n            logger.info(\"url：https://m.weibo.cn/detail/%d\", weibo[\"id\"])\n        except OSError:\n            pass\n\n    def print_weibo(self, weibo):\n        \"\"\"打印微博，若为转发微博，会同时打印原创和转发部分\"\"\"\n        if weibo.get(\"retweet\"):\n            logger.info(\"*\" * 100)\n            logger.info(\"转发部分：\")\n            self.print_one_weibo(weibo[\"retweet\"])\n            logger.info(\"*\" * 100)\n            logger.info(\"原创部分：\")\n        self.print_one_weibo(weibo)\n        logger.info(\"-\" * 120)\n\n    def get_one_weibo(self, info):\n        \"\"\"获取一条微博的全部信息\"\"\"\n        try:\n            weibo_info = info[\"mblog\"]\n            weibo_id = weibo_info[\"id\"]\n            retweeted_status = weibo_info.get(\"retweeted_status\")\n            is_long = (\n                True if weibo_info.get(\"pic_num\") > 9 else weibo_info.get(\"isLongText\")\n            )\n            if retweeted_status and retweeted_status.get(\"id\"):  # 转发\n                retweet_id = retweeted_status.get(\"id\")\n                is_long_retweet = retweeted_status.get(\"isLongText\")\n                if is_long:\n                    weibo = self.get_long_weibo(weibo_id)\n                    if not weibo:\n                        weibo = self.parse_weibo(weibo_info)\n                else:\n                    weibo = self.parse_weibo(weibo_info)\n                if is_long_retweet:\n                    retweet = self.get_long_weibo(retweet_id)\n                    if not retweet:\n                        retweet = self.parse_weibo(retweeted_status)\n                else:\n                    retweet = self.parse_weibo(retweeted_status)\n                (\n                    retweet[\"created_at\"],\n                    retweet[\"full_created_at\"],\n                ) = self.standardize_date(retweeted_status[\"created_at\"])\n                weibo[\"retweet\"] = retweet\n            else:  # 原创\n                if is_long:\n                    weibo = self.get_long_weibo(weibo_id)\n                    if not weibo:\n                        weibo = self.parse_weibo(weibo_info)\n                else:\n                    weibo = self.parse_weibo(weibo_info)\n            weibo[\"created_at\"], weibo[\"full_created_at\"] = self.standardize_date(\n                weibo_info[\"created_at\"]\n            )\n            return weibo\n        except Exception as e:\n            logger.exception(e)\n\n    def get_weibo_comments(self, weibo, max_count, on_downloaded):\n        \"\"\"\n        :weibo standardlized weibo\n        :max_count 最大允许下载数\n        :on_downloaded 下载完成时的实例方法回调\n        \"\"\"\n        if weibo[\"comments_count\"] == 0:\n            return\n\n        logger.info(\n            \"正在下载评论 微博id:{id}\".format(id=weibo[\"id\"])\n        )\n        self._get_weibo_comments_cookie(weibo, 0, max_count, None, on_downloaded)\n\n    def get_weibo_reposts(self, weibo, max_count, on_downloaded):\n        \"\"\"\n        :weibo standardlized weibo\n        :max_count 最大允许下载数\n        :on_downloaded 下载完成时的实例方法回调\n        \"\"\"\n        if weibo[\"reposts_count\"] == 0:\n            return\n\n        logger.info(\n            \"正在下载转发 微博id:{id}\".format(id=weibo[\"id\"])\n        )\n        self._get_weibo_reposts_cookie(weibo, 0, max_count, 1, on_downloaded)\n\n    def _get_weibo_comments_cookie(\n        self, weibo, cur_count, max_count, max_id, on_downloaded\n    ):\n        \"\"\"\n        :weibo standardlized weibo\n        :cur_count  已经下载的评论数\n        :max_count 最大允许下载数\n        :max_id 微博返回的max_id参数\n        :on_downloaded 下载完成时的实例方法回调\n        \"\"\"\n        if cur_count >= max_count:\n            return\n\n        id = weibo[\"id\"]\n        params = {\"mid\": id}\n        if max_id:\n            params[\"max_id\"] = max_id\n        url = \"https://m.weibo.cn/comments/hotflow?max_id_type=0\"\n        req = requests.get(\n            url,\n            params=params,\n            headers=self.headers,\n        )\n        json = None\n        error = False\n        try:\n            json = req.json()\n        except Exception as e:\n            # 没有cookie会抓取失败\n            # 微博日期小于某个日期的用这个url会被403 需要用老办法尝试一下\n            error = True\n\n        if error:\n            # 最大好像只能有50条 TODO: improvement\n            self._get_weibo_comments_nocookie(weibo, 0, max_count, 1, on_downloaded)\n            return\n\n        data = json.get(\"data\")\n        if not data:\n            # 新接口没有抓取到的老接口也试一下\n            self._get_weibo_comments_nocookie(weibo, 0, max_count, 1, on_downloaded)\n            return\n\n        comments = data.get(\"data\")\n        count = len(comments)\n        if count == 0:\n            # 没有了可以直接跳出递归\n            return\n\n        if on_downloaded:\n            on_downloaded(weibo, comments)\n\n        # 随机睡眠一下\n        if max_count % 40 == 0:\n            sleep(random.randint(1, 5))\n\n        cur_count += count\n        max_id = data.get(\"max_id\")\n\n        if max_id == 0:\n            return\n\n        self._get_weibo_comments_cookie(\n            weibo, cur_count, max_count, max_id, on_downloaded\n        )\n\n    def _get_weibo_comments_nocookie(\n        self, weibo, cur_count, max_count, page, on_downloaded\n    ):\n        \"\"\"\n        :weibo standardlized weibo\n        :cur_count  已经下载的评论数\n        :max_count 最大允许下载数\n        :page 下载的页码 从 1 开始\n        :on_downloaded 下载完成时的实例方法回调\n        \"\"\"\n        if cur_count >= max_count:\n            return\n        id = weibo[\"id\"]\n        url = \"https://m.weibo.cn/api/comments/show?id={id}&page={page}\".format(\n            id=id, page=page\n        )\n        req = requests.get(url)\n        json = None\n        try:\n            json = req.json()\n        except Exception as e:\n            logger.warning(\"未能抓取完整评论 微博id: {id}\".format(id=id))\n            return\n\n        data = json.get(\"data\")\n        if not data:\n            return\n        comments = data.get(\"data\")\n        count = len(comments)\n        if count == 0:\n            # 没有了可以直接跳出递归\n            return\n\n        if on_downloaded:\n            on_downloaded(weibo, comments)\n\n        cur_count += count\n        page += 1\n\n        # 随机睡眠一下\n        if page % 2 == 0:\n            sleep(random.randint(1, 5))\n\n        req_page = data.get(\"max\")\n\n        if req_page == 0:\n            return\n\n        if page > req_page:\n            return\n        self._get_weibo_comments_nocookie(\n            weibo, cur_count, max_count, page, on_downloaded\n        )\n\n    def _get_weibo_reposts_cookie(\n        self, weibo, cur_count, max_count, page, on_downloaded\n    ):\n        \"\"\"\n        :weibo standardlized weibo\n        :cur_count  已经下载的转发数\n        :max_count 最大允许下载数\n        :page 下载的页码 从 1 开始\n        :on_downloaded 下载完成时的实例方法回调\n        \"\"\"\n        if cur_count >= max_count:\n            return\n        id = weibo[\"id\"]\n        url = \"https://m.weibo.cn/api/statuses/repostTimeline\"\n        params = {\"id\": id, \"page\": page}\n        req = requests.get(\n            url,\n            params=params,\n            headers=self.headers,\n        )\n\n        json = None\n        try:\n            json = req.json()\n        except Exception as e:\n            logger.warning(\n                \"未能抓取完整转发 微博id: {id}\".format(id=id)\n            )\n            return\n\n        data = json.get(\"data\")\n        if not data:\n            return\n        reposts = data.get(\"data\")\n        count = len(reposts)\n        if count == 0:\n            # 没有了可以直接跳出递归\n            return\n\n        if on_downloaded:\n            on_downloaded(weibo, reposts)\n\n        cur_count += count\n        page += 1\n\n        # 随机睡眠一下\n        if page % 2 == 0:\n            sleep(random.randint(2, 5))\n\n        req_page = data.get(\"max\")\n\n        if req_page == 0:\n            return\n\n        if page > req_page:\n            return\n        self._get_weibo_reposts_cookie(weibo, cur_count, max_count, page, on_downloaded)\n\n    def is_pinned_weibo(self, info):\n        \"\"\"判断微博是否为置顶微博\"\"\"\n        weibo_info = info[\"mblog\"]\n        isTop = weibo_info.get(\"isTop\")\n        if isTop:\n            return True\n        else:\n            return False\n    \n\n    def get_one_page(self, page):\n        \"\"\"获取一页的全部微博\"\"\"\n        try:\n            js = self.get_weibo_json(page)\n            import json\n            with open('js.json','w') as f:\n                #写入方式1，等价于下面这行\n                json.dump(js,f) #把列表numbers内容写入到\"list.json\"文件中\n            if js[\"ok\"]:\n                weibos = js[\"data\"][\"cards\"]\n                \n                if self.query:\n                    weibos = weibos[0][\"card_group\"]\n                # 如果需要检查cookie，在循环第一个人的时候，就要看看仅自己可见的信息有没有，要是没有直接报错\n                for w in weibos:\n                    if w[\"card_type\"] == 11:\n                        temp = w.get(\"card_group\",[0])\n                        if len(temp) >= 1:\n                            w = temp[0] or w\n                        else:\n                            w = w\n                    if w[\"card_type\"] == 9:\n                        wb = self.get_one_weibo(w)\n                        if wb:\n                            if (\n                                const.CHECK_COOKIE[\"CHECK\"]\n                                and (not const.CHECK_COOKIE[\"CHECKED\"])\n                                and wb[\"text\"].startswith(\n                                    const.CHECK_COOKIE[\"HIDDEN_WEIBO\"]\n                                )\n                            ):\n                                const.CHECK_COOKIE[\"CHECKED\"] = True\n                                logger.info(\"cookie检查通过\")\n                                if const.CHECK_COOKIE[\"EXIT_AFTER_CHECK\"]:\n                                    return True\n                            if wb[\"id\"] in self.weibo_id_list:\n                                continue\n                            created_at = datetime.strptime(wb[\"created_at\"], DTFORMAT)\n                            since_date = datetime.strptime(\n                                self.user_config[\"since_date\"], DTFORMAT\n                            )\n                            if const.MODE == \"append\":\n                                # append模式下不会对置顶微博做任何处理\n\n                                # 由于微博本身的调整，下面判断是否为置顶的代码已失效，默认所有用户第一条均为置顶\n                                # if self.is_pinned_weibo(w):\n                                #     continue\n                                if const.CHECK_COOKIE[\"GUESS_PIN\"]:\n                                    const.CHECK_COOKIE[\"GUESS_PIN\"] = False\n                                    continue\n\n                                if self.first_crawler:\n                                    # 置顶微博的具体时间不好判定，将非置顶微博当成最新微博，写入上次抓取id的csv\n                                    self.latest_weibo_id = str(wb[\"id\"])\n                                    csvutil.update_last_weibo_id(\n                                        wb[\"user_id\"],\n                                        str(wb[\"id\"]) + \" \" + wb[\"created_at\"],\n                                        self.user_csv_file_path,\n                                    )\n                                    self.first_crawler = False\n                                if str(wb[\"id\"]) == self.last_weibo_id:\n                                    if const.CHECK_COOKIE[\"CHECK\"] and (\n                                        not const.CHECK_COOKIE[\"CHECKED\"]\n                                    ):\n                                        # 已经爬取过最新的了，只是没检查到cookie，一旦检查通过，直接放行\n                                        const.CHECK_COOKIE[\"EXIT_AFTER_CHECK\"] = True\n                                        continue\n                                    if self.last_weibo_id == self.latest_weibo_id:\n                                        logger.info(\n                                            \"{} 用户没有发新微博\".format(\n                                                self.user[\"screen_name\"]\n                                            )\n                                        )\n                                    else:\n                                        logger.info(\n                                            \"增量获取微博完毕，将最新微博id从 {} 变更为 {}\".format(\n                                                self.last_weibo_id, self.latest_weibo_id\n                                            )\n                                        )\n                                    return True\n                                # 上一次标记的微博被删了，就把上一条微博时间记录推前两天，多抓点评论或者微博内容修改\n                                # TODO 更加合理的流程是，即使读取到上次更新微博id，也抓取增量评论，由此获得更多的评论\n                                since_date = datetime.strptime(\n                                    convert_to_days_ago(self.last_weibo_date, 1),\n                                    DTFORMAT,\n                                )\n                            if created_at < since_date:\n                                if self.is_pinned_weibo(w):\n                                    continue\n                                # 如果要检查还没有检查cookie，不能直接跳出\n                                elif const.CHECK_COOKIE[\"CHECK\"] and (\n                                    not const.CHECK_COOKIE[\"CHECKED\"]\n                                ):\n                                    continue\n                                else:\n                                    logger.info(\n                                        \"{}已获取{}({})的第{}页{}微博{}\".format(\n                                            \"-\" * 30,\n                                            self.user[\"screen_name\"],\n                                            self.user[\"id\"],\n                                            page,\n                                            '包含\"' + self.query + '\"的'\n                                            if self.query\n                                            else \"\",\n                                            \"-\" * 30,\n                                        )\n                                    )\n                                    return True\n                            if (not self.only_crawl_original) or (\"retweet\" not in wb.keys()):\n                                self.weibo.append(wb)\n                                self.weibo_id_list.append(wb[\"id\"])\n                                self.got_count += 1\n                                # 这里是系统日志输出，尽量别太杂\n                                logger.info(\n                                    \"已获取用户 {} 的微博，内容为 {}\".format(\n                                        self.user[\"screen_name\"], wb[\"text\"]\n                                    )\n                                )\n                                # self.print_weibo(wb)\n                            else:\n                                logger.info(\"正在过滤转发微博\")\n                    \n                if const.CHECK_COOKIE[\"CHECK\"] and not const.CHECK_COOKIE[\"CHECKED\"]:\n                    logger.warning(\"经检查，cookie无效，系统退出\")\n                    if const.NOTIFY[\"NOTIFY\"]:\n                        push_deer(\"经检查，cookie无效，系统退出\")\n                    sys.exit()\n            else:\n                return True\n            logger.info(\n                \"{}已获取{}({})的第{}页微博{}\".format(\n                    \"-\" * 30, self.user[\"screen_name\"], self.user[\"id\"], page, \"-\" * 30\n                )\n            )\n        except Exception as e:\n            logger.exception(e)\n\n    def get_page_count(self):\n        \"\"\"获取微博页数\"\"\"\n        try:\n            weibo_count = self.user[\"statuses_count\"]\n            page_count = int(math.ceil(weibo_count / 10.0))\n            return page_count\n        except KeyError:\n            logger.exception(\n                \"程序出错，错误原因可能为以下两者：\\n\"\n                \"1.user_id不正确；\\n\"\n                \"2.此用户微博可能需要设置cookie才能爬取。\\n\"\n                \"解决方案：\\n\"\n                \"请参考\\n\"\n                \"https://github.com/dataabc/weibo-crawler#如何获取user_id\\n\"\n                \"获取正确的user_id；\\n\"\n                \"或者参考\\n\"\n                \"https://github.com/dataabc/weibo-crawler#3程序设置\\n\"\n                \"中的“设置cookie”部分设置cookie信息\"\n            )\n\n    def get_write_info(self, wrote_count):\n        \"\"\"获取要写入的微博信息\"\"\"\n        write_info = []\n        for w in self.weibo[wrote_count:]:\n            wb = OrderedDict()\n            for k, v in w.items():\n                if k not in [\"user_id\", \"screen_name\", \"retweet\"]:\n                    if \"unicode\" in str(type(v)):\n                        v = v.encode(\"utf-8\")\n                    if k == \"id\":\n                        v = str(v) + \"\\t\"\n                    wb[k] = v\n            if not self.only_crawl_original:\n                if w.get(\"retweet\"):\n                    wb[\"is_original\"] = False\n                    for k2, v2 in w[\"retweet\"].items():\n                        if \"unicode\" in str(type(v2)):\n                            v2 = v2.encode(\"utf-8\")\n                        if k2 == \"id\":\n                            v2 = str(v2) + \"\\t\"\n                        wb[\"retweet_\" + k2] = v2\n                else:\n                    wb[\"is_original\"] = True\n            write_info.append(wb)\n        return write_info\n\n    def get_filepath(self, type):\n        \"\"\"获取结果文件路径\"\"\"\n        try:\n            dir_name = self.user[\"screen_name\"]\n            if self.user_id_as_folder_name:\n                dir_name = str(self.user_config[\"user_id\"])\n            file_dir = (\n                os.path.split(os.path.realpath(__file__))[0]\n                + os.sep\n                + \"weibo\"\n                + os.sep\n                + dir_name\n            )\n            if type == \"img\" or type == \"video\":\n                file_dir = file_dir + os.sep + type\n            if not os.path.isdir(file_dir):\n                os.makedirs(file_dir)\n            if type == \"img\" or type == \"video\":\n                return file_dir\n            file_path = file_dir + os.sep + str(self.user_config[\"user_id\"]) + \".\" + type\n            return file_path\n        except Exception as e:\n            logger.exception(e)\n\n    def get_result_headers(self):\n        \"\"\"获取要写入结果文件的表头\"\"\"\n        result_headers = [\n            \"id\",\n            \"bid\",\n            \"正文\",\n            \"头条文章url\",\n            \"原始图片url\",\n            \"视频url\",\n            \"位置\",\n            \"日期\",\n            \"工具\",\n            \"点赞数\",\n            \"评论数\",\n            \"转发数\",\n            \"话题\",\n            \"@用户\",\n            \"完整日期\",\n        ]\n        if not self.only_crawl_original:\n            result_headers2 = [\"是否原创\", \"源用户id\", \"源用户昵称\"]\n            result_headers3 = [\"源微博\" + r for r in result_headers]\n            result_headers = result_headers + result_headers2 + result_headers3\n        return result_headers\n\n    def write_csv(self, wrote_count):\n        \"\"\"将爬到的信息写入csv文件\"\"\"\n        write_info = self.get_write_info(wrote_count)\n        result_headers = self.get_result_headers()\n        result_data = [w.values() for w in write_info]\n        file_path = self.get_filepath(\"csv\")\n        self.csv_helper(result_headers, result_data, file_path)\n\n    def csv_helper(self, headers, result_data, file_path):\n        \"\"\"将指定信息写入csv文件\"\"\"\n        if not os.path.isfile(file_path):\n            is_first_write = 1\n        else:\n            is_first_write = 0\n        if sys.version < \"3\":  # python2.x\n            with open(file_path, \"ab\") as f:\n                f.write(codecs.BOM_UTF8)\n                writer = csv.writer(f)\n                if is_first_write:\n                    writer.writerows([headers])\n                writer.writerows(result_data)\n        else:  # python3.x\n\n            with open(file_path, \"a\", encoding=\"utf-8-sig\", newline=\"\") as f:\n                writer = csv.writer(f)\n                if is_first_write:\n                    writer.writerows([headers])\n                writer.writerows(result_data)\n        if headers[0] == \"id\":\n            logger.info(\"%d条微博写入csv文件完毕,保存路径:\", self.got_count)\n        else:\n            logger.info(\"%s 信息写入csv文件完毕，保存路径:\", self.user[\"screen_name\"])\n        logger.info(file_path)\n\n    def update_json_data(self, data, weibo_info):\n        \"\"\"更新要写入json结果文件中的数据，已经存在于json中的信息更新为最新值，不存在的信息添加到data中\"\"\"\n        data[\"user\"] = self.user\n        if data.get(\"weibo\"):\n            is_new = 1  # 待写入微博是否全部为新微博，即待写入微博与json中的数据不重复\n            for old in data[\"weibo\"]:\n                if weibo_info[-1][\"id\"] == old[\"id\"]:\n                    is_new = 0\n                    break\n            if is_new == 0:\n                for new in weibo_info:\n                    flag = 1\n                    for i, old in enumerate(data[\"weibo\"]):\n                        if new[\"id\"] == old[\"id\"]:\n                            data[\"weibo\"][i] = new\n                            flag = 0\n                            break\n                    if flag:\n                        data[\"weibo\"].append(new)\n            else:\n                data[\"weibo\"] += weibo_info\n        else:\n            data[\"weibo\"] = weibo_info\n        return data\n\n    def write_json(self, wrote_count):\n        \"\"\"将爬到的信息写入json文件\"\"\"\n        data = {}\n        path = self.get_filepath(\"json\")\n        if os.path.isfile(path):\n            with codecs.open(path, \"r\", encoding=\"utf-8\") as f:\n                data = json.load(f)\n        weibo_info = self.weibo[wrote_count:]\n        data = self.update_json_data(data, weibo_info)\n        with codecs.open(path, \"w\", encoding=\"utf-8\") as f:\n            json.dump(data, f, ensure_ascii=False)\n        logger.info(\"%d条微博写入json文件完毕,保存路径:\", self.got_count)\n        logger.info(path)\n\n    def send_post_request_with_token(self, url, data, token, max_retries, backoff_factor):\n        headers = {\n            'Content-Type': 'application/json',\n            'api-token': f'{token}',\n        }\n        for attempt in range(max_retries + 1):\n            try:\n                response = requests.post(url, json=data, headers=headers)\n                if response.status_code == requests.codes.ok:\n                    return response.json()\n                else:\n                    raise RequestException(f\"Unexpected response status: {response.status_code}\")\n            except RequestException as e:\n                if attempt < max_retries:\n                    sleep(backoff_factor * (attempt + 1))  # 逐步增加等待时间，避免频繁重试\n                    continue\n                else:\n                    logger.error(f\"在尝试{max_retries}次发出POST连接后，请求失败：{e}\")\n\n    def write_post(self, wrote_count):\n        \"\"\"将爬到的信息通过POST发出\"\"\"\n        data = {}\n        data['user'] = self.user\n        weibo_info = self.weibo[wrote_count:]\n        if data.get('weibo'):\n            data['weibo'] += weibo_info\n        else:\n            data['weibo'] = weibo_info\n\n        if data:\n            self.send_post_request_with_token(self.post_config[\"api_url\"], data, self.post_config[\"api_token\"], 3, 2)\n            logger.info(u'%d条微博通过POST发送到 %s', len(data['weibo']), self.post_config[\"api_url\"])\n        else:\n            logger.info(u'没有获取到微博，略过API POST')\n\n\n    def info_to_mongodb(self, collection, info_list):\n        \"\"\"将爬取的信息写入MongoDB数据库\"\"\"\n        try:\n            import pymongo\n        except ImportError:\n            logger.warning(\"系统中可能没有安装pymongo库，请先运行 pip install pymongo ，再运行程序\")\n            sys.exit()\n        try:\n            from pymongo import MongoClient\n\n            client = MongoClient(self.mongodb_URI)\n            db = client[\"weibo\"]\n            collection = db[collection]\n            if len(self.write_mode) > 1:\n                new_info_list = copy.deepcopy(info_list)\n            else:\n                new_info_list = info_list\n            for info in new_info_list:\n                if not collection.find_one({\"id\": info[\"id\"]}):\n                    collection.insert_one(info)\n                else:\n                    collection.update_one({\"id\": info[\"id\"]}, {\"$set\": info})\n        except pymongo.errors.ServerSelectionTimeoutError:\n            logger.warning(\"系统中可能没有安装或启动MongoDB数据库，请先根据系统环境安装或启动MongoDB，再运行程序\")\n            sys.exit()\n\n    def weibo_to_mongodb(self, wrote_count):\n        \"\"\"将爬取的微博信息写入MongoDB数据库\"\"\"\n        self.info_to_mongodb(\"weibo\", self.weibo[wrote_count:])\n        logger.info(\"%d条微博写入MongoDB数据库完毕\", self.got_count)\n\n    def mysql_create(self, connection, sql):\n        \"\"\"创建MySQL数据库或表\"\"\"\n        try:\n            with connection.cursor() as cursor:\n                cursor.execute(sql)\n        finally:\n            connection.close()\n\n    def mysql_create_database(self, mysql_config, sql):\n        \"\"\"创建MySQL数据库\"\"\"\n        try:\n            import pymysql\n        except ImportError:\n            logger.warning(\"系统中可能没有安装pymysql库，请先运行 pip install pymysql ，再运行程序\")\n            sys.exit()\n        try:\n            if self.mysql_config:\n                mysql_config = self.mysql_config\n            connection = pymysql.connect(**mysql_config)\n            self.mysql_create(connection, sql)\n        except pymysql.OperationalError:\n            logger.warning(\"系统中可能没有安装或正确配置MySQL数据库，请先根据系统环境安装或配置MySQL，再运行程序\")\n            sys.exit()\n\n    def mysql_create_table(self, mysql_config, sql):\n        \"\"\"创建MySQL表\"\"\"\n        import pymysql\n\n        if self.mysql_config:\n            mysql_config = self.mysql_config\n        mysql_config[\"db\"] = \"weibo\"\n        connection = pymysql.connect(**mysql_config)\n        self.mysql_create(connection, sql)\n\n    def mysql_insert(self, mysql_config, table, data_list):\n        \"\"\"\n        向MySQL表插入或更新数据\n\n        Parameters\n        ----------\n        mysql_config: map\n            MySQL配置表\n        table: str\n            要插入的表名\n        data_list: list\n            要插入的数据列表\n\n        Returns\n        -------\n        bool: SQL执行结果\n        \"\"\"\n        import pymysql\n\n        if len(data_list) > 0:\n            keys = \", \".join(data_list[0].keys())\n            values = \", \".join([\"%s\"] * len(data_list[0]))\n            if self.mysql_config:\n                mysql_config = self.mysql_config\n            mysql_config[\"db\"] = \"weibo\"\n            connection = pymysql.connect(**mysql_config)\n            cursor = connection.cursor()\n            sql = \"\"\"INSERT INTO {table}({keys}) VALUES ({values}) ON\n                     DUPLICATE KEY UPDATE\"\"\".format(\n                table=table, keys=keys, values=values\n            )\n            update = \",\".join(\n                [\" {key} = values({key})\".format(key=key) for key in data_list[0]]\n            )\n            sql += update\n            try:\n                cursor.executemany(sql, [tuple(data.values()) for data in data_list])\n                connection.commit()\n            except Exception as e:\n                connection.rollback()\n                logger.exception(e)\n            finally:\n                connection.close()\n\n    def weibo_to_mysql(self, wrote_count):\n        \"\"\"将爬取的微博信息写入MySQL数据库\"\"\"\n        mysql_config = {\n            \"host\": \"localhost\",\n            \"port\": 3306,\n            \"user\": \"root\",\n            \"password\": \"123456\",\n            \"charset\": \"utf8mb4\",\n        }\n        # 创建'weibo'表\n        create_table = \"\"\"\n                CREATE TABLE IF NOT EXISTS weibo (\n                id varchar(20) NOT NULL,\n                bid varchar(12) NOT NULL,\n                user_id varchar(20),\n                screen_name varchar(30),\n                text text,\n                article_url varchar(100),\n                topics varchar(200),\n                at_users varchar(1000),\n                pics varchar(3000),\n                video_url varchar(1000),\n                location varchar(100),\n                created_at DATETIME,\n                source varchar(30),\n                attitudes_count INT,\n                comments_count INT,\n                reposts_count INT,\n                retweet_id varchar(20),\n                PRIMARY KEY (id)\n                ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4\"\"\"\n        self.mysql_create_table(mysql_config, create_table)\n\n        # 要插入的微博列表\n        weibo_list = []\n        # 要插入的转发微博列表\n        retweet_list = []\n        if len(self.write_mode) > 1:\n            info_list = copy.deepcopy(self.weibo[wrote_count:])\n        else:\n            info_list = self.weibo[wrote_count:]\n        for w in info_list:\n            w[\"created_at\"] = w[\"full_created_at\"]\n            del w[\"full_created_at\"]\n\n            if \"retweet\" in w:\n                r = w[\"retweet\"]\n                r[\"retweet_id\"] = \"\"\n                r[\"created_at\"] = r[\"full_created_at\"]\n                del r[\"full_created_at\"]\n                retweet_list.append(r)\n                w[\"retweet_id\"] = r[\"id\"]\n                del w[\"retweet\"]\n            else:\n                w[\"retweet_id\"] = \"\"\n            weibo_list.append(w)\n        # 在'weibo'表中插入或更新微博数据\n        self.mysql_insert(mysql_config, \"weibo\", retweet_list)\n        self.mysql_insert(mysql_config, \"weibo\", weibo_list)\n        logger.info(\"%d条微博写入MySQL数据库完毕\", self.got_count)\n\n    def weibo_to_sqlite(self, wrote_count):\n        con = self.get_sqlite_connection()\n        weibo_list = []\n        retweet_list = []\n        if len(self.write_mode) > 1:\n            info_list = copy.deepcopy(self.weibo[wrote_count:])\n        else:\n            info_list = self.weibo[wrote_count:]\n        for w in info_list:\n            if \"retweet\" in w:\n                w[\"retweet\"][\"retweet_id\"] = \"\"\n                retweet_list.append(w[\"retweet\"])\n                w[\"retweet_id\"] = w[\"retweet\"][\"id\"]\n                del w[\"retweet\"]\n            else:\n                w[\"retweet_id\"] = \"\"\n            weibo_list.append(w)\n\n        comment_max_count = self.comment_max_download_count\n        repost_max_count = self.comment_max_download_count\n        download_comment = self.download_comment and comment_max_count > 0\n        download_repost = self.download_repost and repost_max_count > 0\n\n        count = 0\n        for weibo in weibo_list:\n            self.sqlite_insert_weibo(con, weibo)\n            if (download_comment) and (weibo[\"comments_count\"] > 0):\n                self.get_weibo_comments(\n                    weibo, comment_max_count, self.sqlite_insert_comments\n                )\n                count += 1\n                # 为防止被ban抓取一定数量的评论后随机睡3到6秒\n                if count % 20:\n                    sleep(random.randint(3, 6))\n            if (download_repost) and (weibo[\"reposts_count\"] > 0):\n                self.get_weibo_reposts(\n                    weibo, repost_max_count, self.sqlite_insert_reposts\n                )\n                count += 1\n                # 为防止被ban抓取一定数量的转发后随机睡3到6秒\n                if count % 20:\n                    sleep(random.randint(3, 6))\n\n        for weibo in retweet_list:\n            self.sqlite_insert_weibo(con, weibo)\n\n        con.close()\n\n    def sqlite_insert_comments(self, weibo, comments):\n        if not comments or len(comments) == 0:\n            return\n        con = self.get_sqlite_connection()\n        for comment in comments:\n            data = self.parse_sqlite_comment(comment, weibo)\n            self.sqlite_insert(con, data, \"comments\")\n\n        con.close()\n\n    def sqlite_insert_reposts(self, weibo, reposts):\n        if not reposts or len(reposts) == 0:\n            return\n        con = self.get_sqlite_connection()\n        for repost in reposts:\n            data = self.parse_sqlite_repost(repost, weibo)\n            self.sqlite_insert(con, data, \"reposts\")\n\n        con.close()\n\n    def parse_sqlite_comment(self, comment, weibo):\n        if not comment:\n            return\n        sqlite_comment = OrderedDict()\n        sqlite_comment[\"id\"] = comment[\"id\"]\n\n        self._try_get_value(\"bid\", \"bid\", sqlite_comment, comment)\n        self._try_get_value(\"root_id\", \"rootid\", sqlite_comment, comment)\n        self._try_get_value(\"created_at\", \"created_at\", sqlite_comment, comment)\n        sqlite_comment[\"weibo_id\"] = weibo[\"id\"]\n\n        sqlite_comment[\"user_id\"] = comment[\"user\"][\"id\"]\n        sqlite_comment[\"user_screen_name\"] = comment[\"user\"][\"screen_name\"]\n        self._try_get_value(\n            \"user_avatar_url\", \"avatar_hd\", sqlite_comment, comment[\"user\"]\n        )\n        if self.remove_html_tag:\n            sqlite_comment[\"text\"] = re.sub('<[^<]+?>', '', comment[\"text\"]).replace('\\n', '').strip()\n        else:\n            sqlite_comment[\"text\"] = comment[\"text\"]\n        \n        sqlite_comment[\"pic_url\"] = \"\"\n        if comment.get(\"pic\"):\n            sqlite_comment[\"pic_url\"] = comment[\"pic\"][\"large\"][\"url\"]\n        self._try_get_value(\"like_count\", \"like_count\", sqlite_comment, comment)\n        return sqlite_comment\n\n    def parse_sqlite_repost(self, repost, weibo):\n        if not repost:\n            return\n        sqlite_repost = OrderedDict()\n        sqlite_repost[\"id\"] = repost[\"id\"]\n\n        self._try_get_value(\"bid\", \"bid\", sqlite_repost, repost)\n        self._try_get_value(\"created_at\", \"created_at\", sqlite_repost, repost)\n        sqlite_repost[\"weibo_id\"] = weibo[\"id\"]\n\n        sqlite_repost[\"user_id\"] = repost[\"user\"][\"id\"]\n        sqlite_repost[\"user_screen_name\"] = repost[\"user\"][\"screen_name\"]\n        self._try_get_value(\n            \"user_avatar_url\", \"profile_image_url\", sqlite_repost, repost[\"user\"]\n        )\n        text = repost.get(\"raw_text\")\n        if text:\n            text = text.split(\"//\", 1)[0]\n        if text is None or text == \"\" or text == \"Repost\":\n            text = \"转发微博\"\n        sqlite_repost[\"text\"] = text\n        self._try_get_value(\"like_count\", \"attitudes_count\", sqlite_repost, repost)\n        return sqlite_repost\n\n    def _try_get_value(self, source_name, target_name, dict, json):\n        dict[source_name] = \"\"\n        value = json.get(target_name)\n        if value:\n            dict[source_name] = value\n\n    def sqlite_insert_weibo(self, con: sqlite3.Connection, weibo: dict):\n        sqlite_weibo = self.parse_sqlite_weibo(weibo)\n        self.sqlite_insert(con, sqlite_weibo, \"weibo\")\n\n    def parse_sqlite_weibo(self, weibo):\n        if not weibo:\n            return\n        sqlite_weibo = OrderedDict()\n        sqlite_weibo[\"user_id\"] = weibo[\"user_id\"]\n        sqlite_weibo[\"id\"] = weibo[\"id\"]\n        sqlite_weibo[\"bid\"] = weibo[\"bid\"]\n        sqlite_weibo[\"screen_name\"] = weibo[\"screen_name\"]\n        sqlite_weibo[\"text\"] = weibo[\"text\"]\n        sqlite_weibo[\"article_url\"] = weibo[\"article_url\"]\n        sqlite_weibo[\"topics\"] = weibo[\"topics\"]\n        sqlite_weibo[\"pics\"] = weibo[\"pics\"]\n        sqlite_weibo[\"video_url\"] = weibo[\"video_url\"]\n        sqlite_weibo[\"location\"] = weibo[\"location\"]\n        sqlite_weibo[\"created_at\"] = weibo[\"full_created_at\"]\n        sqlite_weibo[\"source\"] = weibo[\"source\"]\n        sqlite_weibo[\"attitudes_count\"] = weibo[\"attitudes_count\"]\n        sqlite_weibo[\"comments_count\"] = weibo[\"comments_count\"]\n        sqlite_weibo[\"reposts_count\"] = weibo[\"reposts_count\"]\n        sqlite_weibo[\"retweet_id\"] = weibo[\"retweet_id\"]\n        sqlite_weibo[\"at_users\"] = weibo[\"at_users\"]\n        return sqlite_weibo\n\n    def user_to_sqlite(self):\n        con = self.get_sqlite_connection()\n        self.sqlite_insert_user(con, self.user)\n        con.close()\n\n    def sqlite_insert_user(self, con: sqlite3.Connection, user: dict):\n        sqlite_user = self.parse_sqlite_user(user)\n        self.sqlite_insert(con, sqlite_user, \"user\")\n\n    def parse_sqlite_user(self, user):\n        if not user:\n            return\n        sqlite_user = OrderedDict()\n        sqlite_user[\"id\"] = user[\"id\"]\n        sqlite_user[\"nick_name\"] = user[\"screen_name\"]\n        sqlite_user[\"gender\"] = user[\"gender\"]\n        sqlite_user[\"follower_count\"] = user[\"followers_count\"]\n        sqlite_user[\"follow_count\"] = user[\"follow_count\"]\n        sqlite_user[\"birthday\"] = user[\"birthday\"]\n        sqlite_user[\"location\"] = user[\"location\"]\n        sqlite_user[\"edu\"] = user[\"education\"]\n        sqlite_user[\"company\"] = user[\"company\"]\n        sqlite_user[\"reg_date\"] = user[\"registration_time\"]\n        sqlite_user[\"main_page_url\"] = user[\"profile_url\"]\n        sqlite_user[\"avatar_url\"] = user[\"avatar_hd\"]\n        sqlite_user[\"bio\"] = user[\"description\"]\n        return sqlite_user\n\n    def sqlite_insert(self, con: sqlite3.Connection, data: dict, table: str):\n        if not data:\n            return\n        cur = con.cursor()\n        keys = \",\".join(data.keys())\n        values = \",\".join([\"?\"] * len(data))\n        sql = \"\"\"INSERT OR REPLACE INTO {table}({keys}) VALUES({values})\n                \"\"\".format(\n            table=table, keys=keys, values=values\n        )\n        cur.execute(sql, list(data.values()))\n        con.commit()\n\n    def get_sqlite_connection(self):\n        path = self.get_sqlte_path()\n        create = False\n        if not os.path.exists(path):\n            create = True\n\n        con = sqlite3.connect(path)\n\n        if create == True:\n            self.create_sqlite_table(connection=con)\n\n        return con\n\n    def create_sqlite_table(self, connection: sqlite3.Connection):\n        sql = self.get_sqlite_create_sql()\n        cur = connection.cursor()\n        cur.executescript(sql)\n        connection.commit()\n\n    def get_sqlte_path(self):\n        return \"./weibo/weibodata.db\"\n\n    def get_sqlite_create_sql(self):\n        create_sql = \"\"\"\n                CREATE TABLE IF NOT EXISTS user (\n                    id varchar(64) NOT NULL\n                    ,nick_name varchar(64) NOT NULL\n                    ,gender varchar(6)\n                    ,follower_count integer\n                    ,follow_count integer\n                    ,birthday varchar(10)\n                    ,location varchar(32)\n                    ,edu varchar(32)\n                    ,company varchar(32)\n                    ,reg_date DATETIME\n                    ,main_page_url text\n                    ,avatar_url text\n                    ,bio text\n                    ,PRIMARY KEY (id)\n                );\n\n                CREATE TABLE IF NOT EXISTS weibo (\n                    id varchar(20) NOT NULL\n                    ,bid varchar(12) NOT NULL\n                    ,user_id varchar(20)\n                    ,screen_name varchar(30)\n                    ,text varchar(2000)\n                    ,article_url varchar(100)\n                    ,topics varchar(200)\n                    ,at_users varchar(1000)\n                    ,pics varchar(3000)\n                    ,video_url varchar(1000)\n                    ,location varchar(100)\n                    ,created_at DATETIME\n                    ,source varchar(30)\n                    ,attitudes_count INT\n                    ,comments_count INT\n                    ,reposts_count INT\n                    ,retweet_id varchar(20)\n                    ,PRIMARY KEY (id)\n                );\n\n                CREATE TABLE IF NOT EXISTS bins (\n                    id integer PRIMARY KEY AUTOINCREMENT\n                    ,ext varchar(10) NOT NULL /*file extension*/\n                    ,data blob NOT NULL\n                    ,weibo_id varchar(20)\n                    ,comment_id varchar(20)\n                    ,path text\n                    ,url text\n                );\n\n                CREATE TABLE IF NOT EXISTS comments (\n                    id varchar(20) NOT NULL\n                    ,bid varchar(20) NOT NULL\n                    ,weibo_id varchar(32) NOT NULL\n                    ,root_id varchar(20) \n                    ,user_id varchar(20) NOT NULL\n                    ,created_at varchar(20)\n                    ,user_screen_name varchar(64) NOT NULL\n                    ,user_avatar_url text\n                    ,text varchar(1000)\n                    ,pic_url text\n                    ,like_count integer\n                    ,PRIMARY KEY (id)\n                );\n\n                CREATE TABLE IF NOT EXISTS reposts (\n                    id varchar(20) NOT NULL\n                    ,bid varchar(20) NOT NULL\n                    ,weibo_id varchar(32) NOT NULL\n                    ,user_id varchar(20) NOT NULL\n                    ,created_at varchar(20)\n                    ,user_screen_name varchar(64) NOT NULL\n                    ,user_avatar_url text\n                    ,text varchar(1000)\n                    ,like_count integer\n                    ,PRIMARY KEY (id)\n                );\n                \"\"\"\n        return create_sql\n\n    def update_user_config_file(self, user_config_file_path):\n        \"\"\"更新用户配置文件\"\"\"\n        with open(user_config_file_path, \"rb\") as f:\n            try:\n                lines = f.read().splitlines()\n                lines = [line.decode(\"utf-8-sig\") for line in lines]\n            except UnicodeDecodeError:\n                logger.error(\"%s文件应为utf-8编码，请先将文件编码转为utf-8再运行程序\", user_config_file_path)\n                sys.exit()\n            for i, line in enumerate(lines):\n                info = line.split(\" \")\n                if len(info) > 0 and info[0].isdigit():\n                    if self.user_config[\"user_id\"] == info[0]:\n                        if len(info) == 1:\n                            info.append(self.user[\"screen_name\"])\n                            info.append(self.start_date)\n                        if len(info) == 2:\n                            info.append(self.start_date)\n                        if len(info) > 2:\n                            info[2] = self.start_date\n                        lines[i] = \" \".join(info)\n                        break\n        with codecs.open(user_config_file_path, \"w\", encoding=\"utf-8\") as f:\n            f.write(\"\\n\".join(lines))\n\n    def write_data(self, wrote_count):\n        \"\"\"将爬到的信息写入文件或数据库\"\"\"\n        if self.got_count > wrote_count:\n            if \"csv\" in self.write_mode:\n                self.write_csv(wrote_count)\n            if \"json\" in self.write_mode:\n                self.write_json(wrote_count)\n            if \"post\" in self.write_mode:\n                self.write_post(wrote_count)\n            if \"mysql\" in self.write_mode:\n                self.weibo_to_mysql(wrote_count)\n            if \"mongo\" in self.write_mode:\n                self.weibo_to_mongodb(wrote_count)\n            if \"sqlite\" in self.write_mode:\n                self.weibo_to_sqlite(wrote_count)\n            if self.original_pic_download:\n                self.download_files(\"img\", \"original\", wrote_count)\n            if self.original_video_download:\n                self.download_files(\"video\", \"original\", wrote_count)\n            if not self.only_crawl_original:\n                if self.retweet_pic_download:\n                    self.download_files(\"img\", \"retweet\", wrote_count)\n                if self.retweet_video_download:\n                    self.download_files(\"video\", \"retweet\", wrote_count)\n\n    def get_pages(self):\n        \"\"\"获取全部微博\"\"\"\n        try:\n            # 用户id不可用\n            if self.get_user_info() != 0:\n                return\n            logger.info(\"准备搜集 {} 的微博\".format(self.user[\"screen_name\"]))\n            if const.MODE == \"append\" and (\n                \"first_crawler\" not in self.__dict__ or self.first_crawler is False\n            ):\n                # 本次运行的某用户首次抓取，用于标记最新的微博id\n                self.first_crawler = True\n                const.CHECK_COOKIE[\"GUESS_PIN\"] = True\n            since_date = datetime.strptime(self.user_config[\"since_date\"], DTFORMAT)\n            today = datetime.today()\n            if since_date <= today:    # since_date 若为未来则无需执行\n                page_count = self.get_page_count()\n                wrote_count = 0\n                page1 = 0\n                random_pages = random.randint(1, 5)\n                self.start_date = datetime.now().strftime(DTFORMAT)\n                pages = range(self.start_page, page_count + 1)\n                for page in tqdm(pages, desc=\"Progress\"):\n                    is_end = self.get_one_page(page)\n                    if is_end:\n                        break\n\n                    if page % 20 == 0:  # 每爬20页写入一次文件\n                        self.write_data(wrote_count)\n                        wrote_count = self.got_count\n\n                    # 通过加入随机等待避免被限制。爬虫速度过快容易被系统限制(一段时间后限\n                    # 制会自动解除)，加入随机等待模拟人的操作，可降低被系统限制的风险。默\n                    # 认是每爬取1到5页随机等待6到10秒，如果仍然被限，可适当增加sleep时间\n                    if (page - page1) % random_pages == 0 and page < page_count:\n                        sleep(random.randint(6, 10))\n                        page1 = page\n                        random_pages = random.randint(1, 5)\n\n                self.write_data(wrote_count)  # 将剩余不足20页的微博写入文件\n            logger.info(\"微博爬取完成，共爬取%d条微博\", self.got_count)\n        except Exception as e:\n            logger.exception(e)\n\n    def get_user_config_list(self, file_path):\n        \"\"\"获取文件中的微博id信息\"\"\"\n        with open(file_path, \"rb\") as f:\n            try:\n                lines = f.read().splitlines() \n                lines = [line.decode(\"utf-8-sig\") for line in lines]\n            except UnicodeDecodeError:\n                logger.error(\"%s文件应为utf-8编码，请先将文件编码转为utf-8再运行程序\", file_path)\n                sys.exit()\n            user_config_list = []\n            # 分行解析配置，添加到user_config_list\n            for line in lines:\n                info = line.strip().split(\" \")    # 去除字符串首尾空白字符\n                if len(info) > 0 and info[0].isdigit():\n                    user_config = {}\n                    user_config[\"user_id\"] = info[0]\n                    # 根据配置文件行的字段数确定 since_date 的值\n                    if len(info) == 3:\n                        if self.is_datetime(info[2]):\n                            user_config[\"since_date\"] = info[2]\n                        elif self.is_date(info[2]):\n                            user_config[\"since_date\"] = \"{}T00:00:00\".format(info[2])\n                        elif info[2].isdigit():\n                            since_date = date.today() - timedelta(int(info[2]))\n                            user_config[\"since_date\"] = since_date.strftime(DTFORMAT)\n                        else:\n                            logger.error(\"since_date 格式不正确，请确认配置是否正确\")\n                            sys.exit()\n                    else:\n                        user_config[\"since_date\"] = self.since_date\n                    # 若超过3个字段，则第四个字段为 query_list                    \n                    if len(info) > 3:\n                        user_config[\"query_list\"] = info[3].split(\",\")\n                    else:\n                        user_config[\"query_list\"] = self.query_list\n                    if user_config not in user_config_list:\n                        user_config_list.append(user_config)\n        return user_config_list\n\n    def initialize_info(self, user_config):\n        \"\"\"初始化爬虫信息\"\"\"\n        self.weibo = []\n        self.user = {}\n        self.user_config = user_config\n        self.got_count = 0\n        self.weibo_id_list = []\n\n    def start(self):\n        \"\"\"运行爬虫\"\"\"\n        try:\n            for user_config in self.user_config_list:\n                if len(user_config[\"query_list\"]):\n                    for query in user_config[\"query_list\"]:\n                        self.query = query\n                        self.initialize_info(user_config)\n                        self.get_pages()\n                else:\n                    self.initialize_info(user_config)\n                    self.get_pages()\n                logger.info(\"信息抓取完毕\")\n                logger.info(\"*\" * 100)\n                if self.user_config_file_path and self.user:\n                    self.update_user_config_file(self.user_config_file_path)\n        except Exception as e:\n            logger.exception(e)\n\n\ndef handle_config_renaming(config, oldName, newName):\n    if oldName in config and newName not in config:\n        config[newName] = config[oldName]\n        del config[oldName]\n\ndef get_config():\n    \"\"\"获取config.json文件信息\"\"\"\n    config_path = os.path.split(os.path.realpath(__file__))[0] + os.sep + \"config.json\"\n    if not os.path.isfile(config_path):\n        logger.warning(\n            \"当前路径：%s 不存在配置文件config.json\",\n            (os.path.split(os.path.realpath(__file__))[0] + os.sep),\n        )\n        sys.exit()\n    try:\n        with open(config_path, encoding=\"utf-8\") as f:\n            config = json.loads(f.read())\n            # 重命名一些key, 但向前兼容\n            handle_config_renaming(config, oldName=\"filter\", newName=\"only_crawl_original\")\n            handle_config_renaming(config, oldName=\"result_dir_name\", newName=\"user_id_as_folder_name\")\n            return config\n    except ValueError:\n        logger.error(\n            \"config.json 格式不正确，请参考 \" \"https://github.com/dataabc/weibo-crawler#3程序设置\"\n        )\n        sys.exit()\n\n\ndef main():\n    try:\n        config = get_config()\n        wb = Weibo(config)\n        wb.start()  # 爬取微博信息\n        if const.NOTIFY[\"NOTIFY\"]:\n            push_deer(\"更新了一次微博\")\n    except Exception as e:\n        if const.NOTIFY[\"NOTIFY\"]:\n            push_deer(\"weibo-crawler运行出错，错误为{}\".format(e))\n        logger.exception(e)\n\n\nif __name__ == \"__main__\":\n    main()\n"
        }
      ]
    }
  ]
}