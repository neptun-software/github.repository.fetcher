{
  "metadata": {
    "timestamp": 1736559546237,
    "page": 148,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjE1MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "isl-org/MiDaS",
      "stars": 4620,
      "defaultBranch": "master",
      "files": [
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 1.2041015625,
          "content": "# Byte-compiled / optimized / DLL files\n__pycache__/\n*.py[cod]\n*$py.class\n\n# C extensions\n*.so\n\n# Distribution / packaging\n.Python\nbuild/\ndevelop-eggs/\ndist/\ndownloads/\neggs/\n.eggs/\nlib/\nlib64/\nparts/\nsdist/\nvar/\nwheels/\n*.egg-info/\n.installed.cfg\n*.egg\nMANIFEST\n\n# PyInstaller\n#  Usually these files are written by a python script from a template\n#  before PyInstaller builds the exe, so as to inject date/other infos into it.\n*.manifest\n*.spec\n\n# Installer logs\npip-log.txt\npip-delete-this-directory.txt\n\n# Unit test / coverage reports\nhtmlcov/\n.tox/\n.coverage\n.coverage.*\n.cache\nnosetests.xml\ncoverage.xml\n*.cover\n.hypothesis/\n.pytest_cache/\n\n# Translations\n*.mo\n*.pot\n\n# Django stuff:\n*.log\nlocal_settings.py\ndb.sqlite3\n\n# Flask stuff:\ninstance/\n.webassets-cache\n\n# Scrapy stuff:\n.scrapy\n\n# Sphinx documentation\ndocs/_build/\n\n# PyBuilder\ntarget/\n\n# Jupyter Notebook\n.ipynb_checkpoints\n\n# pyenv\n.python-version\n\n# celery beat schedule file\ncelerybeat-schedule\n\n# SageMath parsed files\n*.sage.py\n\n# Environments\n.env\n.venv\nenv/\nvenv/\nENV/\nenv.bak/\nvenv.bak/\n\n# Spyder project settings\n.spyderproject\n.spyproject\n\n# Rope project settings\n.ropeproject\n\n# mkdocs documentation\n/site\n\n# mypy\n.mypy_cache/\n\n*.png\n*.pfm\n*.jpg\n*.jpeg\n*.pt"
        },
        {
          "name": "Dockerfile",
          "type": "blob",
          "size": 0.900390625,
          "content": "# enables cuda support in docker\nFROM nvidia/cuda:10.2-cudnn7-runtime-ubuntu18.04\n\n# install python 3.6, pip and requirements for opencv-python \n# (see https://github.com/NVIDIA/nvidia-docker/issues/864)\nRUN apt-get update && apt-get -y install \\\n    python3 \\\n    python3-pip \\\n    libsm6 \\\n    libxext6 \\\n    libxrender-dev \\\n    curl \\\n    && rm -rf /var/lib/apt/lists/*\n\n# install python dependencies\nRUN pip3 install --upgrade pip\nRUN pip3 install torch~=1.8 torchvision opencv-python-headless~=3.4 timm\n\n# copy inference code\nWORKDIR /opt/MiDaS\nCOPY ./midas ./midas\nCOPY ./*.py ./\n\n# download model weights so the docker image can be used offline\nRUN cd weights && {curl -OL https://github.com/isl-org/MiDaS/releases/download/v3/dpt_hybrid_384.pt; cd -; }\nRUN python3 run.py --model_type dpt_hybrid; exit 0\n\n# entrypoint (dont forget to mount input and output directories)\nCMD python3 run.py --model_type dpt_hybrid\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 1.072265625,
          "content": "MIT License\n\nCopyright (c) 2019 Intel ISL (Intel Intelligent Systems Lab)\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 22.9208984375,
          "content": "## Towards Robust Monocular Depth Estimation: Mixing Datasets for Zero-shot Cross-dataset Transfer\n\nThis repository contains code to compute depth from a single image. It accompanies our [paper](https://arxiv.org/abs/1907.01341v3):\n\n>Towards Robust Monocular Depth Estimation: Mixing Datasets for Zero-shot Cross-dataset Transfer  \nRené Ranftl, Katrin Lasinger, David Hafner, Konrad Schindler, Vladlen Koltun\n\n\nand our [preprint](https://arxiv.org/abs/2103.13413):\n\n> Vision Transformers for Dense Prediction  \n> René Ranftl, Alexey Bochkovskiy, Vladlen Koltun\n\nFor the latest release MiDaS 3.1, a [technical report](https://arxiv.org/pdf/2307.14460.pdf) and [video](https://www.youtube.com/watch?v=UjaeNNFf9sE&t=3s) are available.\n\nMiDaS was trained on up to 12 datasets (ReDWeb, DIML, Movies, MegaDepth, WSVD, TartanAir, HRWSI, ApolloScape, BlendedMVS, IRS, KITTI, NYU Depth V2) with\nmulti-objective optimization. \nThe original model that was trained on 5 datasets  (`MIX 5` in the paper) can be found [here](https://github.com/isl-org/MiDaS/releases/tag/v2).\nThe figure below shows an overview of the different MiDaS models; the bubble size scales with number of parameters.\n\n![](figures/Improvement_vs_FPS.png)\n\n### Setup \n\n1) Pick one or more models and download the corresponding weights to the `weights` folder:\n\nMiDaS 3.1\n- For highest quality: [dpt_beit_large_512](https://github.com/isl-org/MiDaS/releases/download/v3_1/dpt_beit_large_512.pt)\n- For moderately less quality, but better speed-performance trade-off: [dpt_swin2_large_384](https://github.com/isl-org/MiDaS/releases/download/v3_1/dpt_swin2_large_384.pt)\n- For embedded devices: [dpt_swin2_tiny_256](https://github.com/isl-org/MiDaS/releases/download/v3_1/dpt_swin2_tiny_256.pt), [dpt_levit_224](https://github.com/isl-org/MiDaS/releases/download/v3_1/dpt_levit_224.pt)\n- For inference on Intel CPUs, OpenVINO may be used for the small legacy model: openvino_midas_v21_small [.xml](https://github.com/isl-org/MiDaS/releases/download/v3_1/openvino_midas_v21_small_256.xml), [.bin](https://github.com/isl-org/MiDaS/releases/download/v3_1/openvino_midas_v21_small_256.bin)\n\nMiDaS 3.0: Legacy transformer models [dpt_large_384](https://github.com/isl-org/MiDaS/releases/download/v3/dpt_large_384.pt) and [dpt_hybrid_384](https://github.com/isl-org/MiDaS/releases/download/v3/dpt_hybrid_384.pt)\n\nMiDaS 2.1: Legacy convolutional models [midas_v21_384](https://github.com/isl-org/MiDaS/releases/download/v2_1/midas_v21_384.pt) and [midas_v21_small_256](https://github.com/isl-org/MiDaS/releases/download/v2_1/midas_v21_small_256.pt) \n\n1) Set up dependencies: \n\n    ```shell\n    conda env create -f environment.yaml\n    conda activate midas-py310\n    ```\n\n#### optional\n\nFor the Next-ViT model, execute\n\n```shell\ngit submodule add https://github.com/isl-org/Next-ViT midas/external/next_vit\n```\n\nFor the OpenVINO model, install\n\n```shell\npip install openvino\n```\n    \n### Usage\n\n1) Place one or more input images in the folder `input`.\n\n2) Run the model with\n\n   ```shell\n   python run.py --model_type <model_type> --input_path input --output_path output\n   ```\n   where ```<model_type>``` is chosen from [dpt_beit_large_512](#model_type), [dpt_beit_large_384](#model_type),\n   [dpt_beit_base_384](#model_type), [dpt_swin2_large_384](#model_type), [dpt_swin2_base_384](#model_type),\n   [dpt_swin2_tiny_256](#model_type), [dpt_swin_large_384](#model_type), [dpt_next_vit_large_384](#model_type),\n   [dpt_levit_224](#model_type), [dpt_large_384](#model_type), [dpt_hybrid_384](#model_type),\n   [midas_v21_384](#model_type), [midas_v21_small_256](#model_type), [openvino_midas_v21_small_256](#model_type).\n \n3) The resulting depth maps are written to the `output` folder.\n\n#### optional\n\n1) By default, the inference resizes the height of input images to the size of a model to fit into the encoder. This\n   size is given by the numbers in the model names of the [accuracy table](#accuracy). Some models do not only support a single\n   inference height but a range of different heights. Feel free to explore different heights by appending the extra \n   command line argument `--height`. Unsupported height values will throw an error. Note that using this argument may\n   decrease the model accuracy.\n2) By default, the inference keeps the aspect ratio of input images when feeding them into the encoder if this is\n   supported by a model (all models except for Swin, Swin2, LeViT). In order to resize to a square resolution,\n   disregarding the aspect ratio while preserving the height, use the command line argument `--square`. \n\n#### via Camera\n\n   If you want the input images to be grabbed from the camera and shown in a window, leave the input and output paths\n   away and choose a model type as shown above:\n\n   ```shell\n   python run.py --model_type <model_type> --side\n   ```\n\n   The argument `--side` is optional and causes both the input RGB image and the output depth map to be shown \n   side-by-side for comparison.\n\n#### via Docker\n\n1) Make sure you have installed Docker and the\n   [NVIDIA Docker runtime](https://github.com/NVIDIA/nvidia-docker/wiki/Installation-\\(Native-GPU-Support\\)).\n\n2) Build the Docker image:\n\n    ```shell\n    docker build -t midas .\n    ```\n\n3) Run inference:\n\n    ```shell\n    docker run --rm --gpus all -v $PWD/input:/opt/MiDaS/input -v $PWD/output:/opt/MiDaS/output -v $PWD/weights:/opt/MiDaS/weights midas\n    ```\n\n   This command passes through all of your NVIDIA GPUs to the container, mounts the\n   `input` and `output` directories and then runs the inference.\n\n#### via PyTorch Hub\n\nThe pretrained model is also available on [PyTorch Hub](https://pytorch.org/hub/intelisl_midas_v2/)\n\n#### via TensorFlow or ONNX\n\nSee [README](https://github.com/isl-org/MiDaS/tree/master/tf) in the `tf` subdirectory.\n\nCurrently only supports MiDaS v2.1. \n\n\n#### via Mobile (iOS / Android)\n\nSee [README](https://github.com/isl-org/MiDaS/tree/master/mobile) in the `mobile` subdirectory.\n\n#### via ROS1 (Robot Operating System)\n\nSee [README](https://github.com/isl-org/MiDaS/tree/master/ros) in the `ros` subdirectory.\n\nCurrently only supports MiDaS v2.1. DPT-based models to be added. \n\n\n### Accuracy\n\nWe provide a **zero-shot error** $\\epsilon_d$ which is evaluated for 6 different datasets\n(see [paper](https://arxiv.org/abs/1907.01341v3)). **Lower error values are better**. \n$\\color{green}{\\textsf{Overall model quality is represented by the improvement}}$ ([Imp.](#improvement)) with respect to\nMiDaS 3.0 DPT<sub>L-384</sub>. The models are grouped by the height used for inference, whereas the square training resolution is given by \nthe numbers in the model names. The table also shows the **number of parameters** (in millions) and the \n**frames per second** for inference at the training resolution (for GPU RTX 3090):\n\n| MiDaS Model                                                                                                           | DIW </br><sup>WHDR</sup> | Eth3d </br><sup>AbsRel</sup> | Sintel </br><sup>AbsRel</sup> |   TUM </br><sup>δ1</sup> | KITTI </br><sup>δ1</sup> | NYUv2 </br><sup>δ1</sup> | $\\color{green}{\\textsf{Imp.}}$ </br><sup>%</sup> | Par.</br><sup>M</sup> | FPS</br><sup>&nbsp;</sup> |\n|-----------------------------------------------------------------------------------------------------------------------|-------------------------:|-----------------------------:|------------------------------:|-------------------------:|-------------------------:|-------------------------:|-------------------------------------------------:|----------------------:|--------------------------:|\n| **Inference height 512**                                                                                              |                          |                              |                               |                          |                          |                          |                                                  |                       |                           |\n| [v3.1 BEiT<sub>L-512</sub>](https://github.com/isl-org/MiDaS/releases/download/v3_1/dpt_beit_large_512.pt)                                                                                     |                   0.1137 |                       0.0659 |                        0.2366 |                 **6.13** |                   11.56* |                **1.86*** |                     $\\color{green}{\\textsf{19}}$ |               **345** |                   **5.7** |\n| [v3.1 BEiT<sub>L-512</sub>](https://github.com/isl-org/MiDaS/releases/download/v3_1/dpt_beit_large_512.pt)$\\tiny{\\square}$                                                                     |               **0.1121** |                   **0.0614** |                    **0.2090** |                     6.46 |                **5.00*** |                    1.90* |                     $\\color{green}{\\textsf{34}}$ |               **345** |                   **5.7** |\n|                                                                                                                       |                          |                              |                               |                          |                          |                          |                                                  |                       |                           |\n| **Inference height 384**                                                                                              |                          |                              |                               |                          |                          |                          |                                                  |                       |                           |\n| [v3.1 BEiT<sub>L-512</sub>](https://github.com/isl-org/MiDaS/releases/download/v3_1/dpt_beit_large_512.pt)                                                                                     |                   0.1245 |                       0.0681 |                    **0.2176** |                 **6.13** |                    6.28* |                **2.16*** |                     $\\color{green}{\\textsf{28}}$ |                   345 |                        12 |\n| [v3.1 Swin2<sub>L-384</sub>](https://github.com/isl-org/MiDaS/releases/download/v3_1/dpt_swin2_large_384.pt)$\\tiny{\\square}$                                                                    |                   0.1106 |                       0.0732 |                        0.2442 |                     8.87 |                **5.84*** |                    2.92* |                     $\\color{green}{\\textsf{22}}$ |                   213 |                        41 |\n| [v3.1 Swin2<sub>B-384</sub>](https://github.com/isl-org/MiDaS/releases/download/v3_1/dpt_swin2_base_384.pt)$\\tiny{\\square}$                                                                    |                   0.1095 |                       0.0790 |                        0.2404 |                     8.93 |                    5.97* |                    3.28* |                     $\\color{green}{\\textsf{22}}$ |                   102 |                        39 |\n| [v3.1 Swin<sub>L-384</sub>](https://github.com/isl-org/MiDaS/releases/download/v3_1/dpt_swin_large_384.pt)$\\tiny{\\square}$                                                                     |                   0.1126 |                       0.0853 |                        0.2428 |                     8.74 |                    6.60* |                    3.34* |                     $\\color{green}{\\textsf{17}}$ |                   213 |                        49 |\n| [v3.1 BEiT<sub>L-384</sub>](https://github.com/isl-org/MiDaS/releases/download/v3_1/dpt_beit_large_384.pt)                                                                                     |                   0.1239 |                   **0.0667** |                        0.2545 |                     7.17 |                    9.84* |                    2.21* |                     $\\color{green}{\\textsf{17}}$ |                   344 |                        13 |\n| [v3.1 Next-ViT<sub>L-384</sub>](https://github.com/isl-org/MiDaS/releases/download/v3_1/dpt_next_vit_large_384.pt)                                                                                 |               **0.1031** |                       0.0954 |                        0.2295 |                     9.21 |                    6.89* |                    3.47* |                     $\\color{green}{\\textsf{16}}$ |                **72** |                        30 |\n| [v3.1 BEiT<sub>B-384</sub>](https://github.com/isl-org/MiDaS/releases/download/v3_1/dpt_beit_base_384.pt)                                                                                     |                   0.1159 |                       0.0967 |                        0.2901 |                     9.88 |                   26.60* |                    3.91* |                    $\\color{green}{\\textsf{-31}}$ |                   112 |                        31 |\n| [v3.0 DPT<sub>L-384</sub>](https://github.com/isl-org/MiDaS/releases/download/v3/dpt_large_384.pt)        |                   0.1082 |                       0.0888 |                        0.2697 |                     9.97 |                     8.46 |                     8.32 |                      $\\color{green}{\\textsf{0}}$ |                   344 |                    **61** |\n| [v3.0 DPT<sub>H-384</sub>](https://github.com/isl-org/MiDaS/releases/download/v3/dpt_hybrid_384.pt)       |                   0.1106 |                       0.0934 |                        0.2741 |                    10.89 |                    11.56 |                     8.69 |                    $\\color{green}{\\textsf{-10}}$ |                   123 |                        50 |\n| [v2.1 Large<sub>384</sub>](https://github.com/isl-org/MiDaS/releases/download/v2_1/midas_v21_384.pt)       |                   0.1295 |                       0.1155 |                        0.3285 |                    12.51 |                    16.08 |                     8.71 |                    $\\color{green}{\\textsf{-32}}$ |                   105 |                        47 |\n|                                                                                                                       |                          |                              |                               |                          |                          |                          |                                                  |                       |                           |\n| **Inference height 256**                                                                                              |                          |                              |                               |                          |                          |                          |                                                  |                       |                           |\n| [v3.1 Swin2<sub>T-256</sub>](https://github.com/isl-org/MiDaS/releases/download/v3_1/dpt_swin2_tiny_256.pt)$\\tiny{\\square}$                                                                    |               **0.1211** |                   **0.1106** |                    **0.2868** |                **13.43** |               **10.13*** |                **5.55*** |                    $\\color{green}{\\textsf{-11}}$ |                    42 |                        64 |\n| [v2.1 Small<sub>256</sub>](https://github.com/isl-org/MiDaS/releases/download/v2_1/midas_v21_small_256.pt) |                   0.1344 |                       0.1344 |                        0.3370 |                    14.53 |                    29.27 |                    13.43 |                    $\\color{green}{\\textsf{-76}}$ |                **21** |                    **90** |\n|                                                                                                                       |                          |                              |                               |                          |                          |                          |                                                  |                       |                           |\n| **Inference height 224**                                                                                              |                          |                              |                               |                          |                          |                          |                                                  |                       |                           |\n| [v3.1 LeViT<sub>224</sub>](https://github.com/isl-org/MiDaS/releases/download/v3_1/dpt_levit_224.pt)$\\tiny{\\square}$                                                                      |               **0.1314** |                   **0.1206** |                    **0.3148** |                **18.21** |               **15.27*** |                **8.64*** |                    $\\color{green}{\\textsf{-40}}$ |                **51** |                    **73** |\n\n&ast; No zero-shot error, because models are also trained on KITTI and NYU Depth V2\\\n$\\square$ Validation performed at **square resolution**, either because the transformer encoder backbone of a model \ndoes not support non-square resolutions (Swin, Swin2, LeViT) or for comparison with these models. All other \nvalidations keep the aspect ratio. A difference in resolution limits the comparability of the zero-shot error and the\nimprovement, because these quantities are averages over the pixels of an image and do not take into account the \nadvantage of more details due to a higher resolution.\\\nBest values per column and same validation height in bold\n\n#### Improvement\n\nThe improvement in the above table is defined as the relative zero-shot error with respect to MiDaS v3.0 \nDPT<sub>L-384</sub> and averaging over the datasets. So, if $\\epsilon_d$ is the zero-shot error for dataset $d$, then\nthe $\\color{green}{\\textsf{improvement}}$ is given by $100(1-(1/6)\\sum_d\\epsilon_d/\\epsilon_{d,\\rm{DPT_{L-384}}})$%.\n\nNote that the improvements of 10% for MiDaS v2.0 &rarr; v2.1 and 21% for MiDaS v2.1 &rarr; v3.0 are not visible from the\nimprovement column (Imp.) in the table but would require an evaluation with respect to MiDaS v2.1 Large<sub>384</sub>\nand v2.0 Large<sub>384</sub> respectively instead of v3.0 DPT<sub>L-384</sub>.\n\n### Depth map comparison\n\nZoom in for better visibility\n![](figures/Comparison.png)\n\n### Speed on Camera Feed\t\n\nTest configuration\t\n- Windows 10\t\n- 11th Gen Intel Core i7-1185G7 3.00GHz\t\n- 16GB RAM\t\n- Camera resolution 640x480\t\n- openvino_midas_v21_small_256\t\n\nSpeed: 22 FPS\n\n### Applications\n\nMiDaS is used in the following other projects from Intel Labs:\n\n- [ZoeDepth](https://arxiv.org/pdf/2302.12288.pdf) (code available [here](https://github.com/isl-org/ZoeDepth)): MiDaS computes the relative depth map given an image. For metric depth estimation, ZoeDepth can be used, which combines MiDaS with a metric depth binning module appended to the decoder.\n- [LDM3D](https://arxiv.org/pdf/2305.10853.pdf) (Hugging Face model available [here](https://huggingface.co/Intel/ldm3d-4c)): LDM3D is an extension of vanilla stable diffusion designed to generate joint image and depth data from a text prompt. The depth maps used for supervision when training LDM3D have been computed using MiDaS.\n\n### Changelog\n\n* [Dec 2022] Released [MiDaS v3.1](https://arxiv.org/pdf/2307.14460.pdf):\n    - New models based on 5 different types of transformers ([BEiT](https://arxiv.org/pdf/2106.08254.pdf), [Swin2](https://arxiv.org/pdf/2111.09883.pdf), [Swin](https://arxiv.org/pdf/2103.14030.pdf), [Next-ViT](https://arxiv.org/pdf/2207.05501.pdf), [LeViT](https://arxiv.org/pdf/2104.01136.pdf))\n    - Training datasets extended from 10 to 12, including also KITTI and NYU Depth V2 using [BTS](https://github.com/cleinc/bts) split\n    - Best model, BEiT<sub>Large 512</sub>, with resolution 512x512, is on average about [28% more accurate](#Accuracy) than MiDaS v3.0\n    - Integrated live depth estimation from camera feed\n* [Sep 2021] Integrated to [Huggingface Spaces](https://huggingface.co/spaces) with [Gradio](https://github.com/gradio-app/gradio). See [Gradio Web Demo](https://huggingface.co/spaces/akhaliq/DPT-Large).\n* [Apr 2021] Released MiDaS v3.0:\n    - New models based on [Dense Prediction Transformers](https://arxiv.org/abs/2103.13413) are on average [21% more accurate](#Accuracy) than MiDaS v2.1\n    - Additional models can be found [here](https://github.com/isl-org/DPT)\n* [Nov 2020] Released MiDaS v2.1:\n\t- New model that was trained on 10 datasets and is on average about [10% more accurate](#Accuracy) than [MiDaS v2.0](https://github.com/isl-org/MiDaS/releases/tag/v2)\n\t- New light-weight model that achieves [real-time performance](https://github.com/isl-org/MiDaS/tree/master/mobile) on mobile platforms.\n\t- Sample applications for [iOS](https://github.com/isl-org/MiDaS/tree/master/mobile/ios) and [Android](https://github.com/isl-org/MiDaS/tree/master/mobile/android)\n\t- [ROS package](https://github.com/isl-org/MiDaS/tree/master/ros) for easy deployment on robots\n* [Jul 2020] Added TensorFlow and ONNX code. Added [online demo](http://35.202.76.57/).\n* [Dec 2019] Released new version of MiDaS - the new model is significantly more accurate and robust\n* [Jul 2019] Initial release of MiDaS ([Link](https://github.com/isl-org/MiDaS/releases/tag/v1))\n\n### Citation\n\nPlease cite our paper if you use this code or any of the models:\n```\n@ARTICLE {Ranftl2022,\n    author  = \"Ren\\'{e} Ranftl and Katrin Lasinger and David Hafner and Konrad Schindler and Vladlen Koltun\",\n    title   = \"Towards Robust Monocular Depth Estimation: Mixing Datasets for Zero-Shot Cross-Dataset Transfer\",\n    journal = \"IEEE Transactions on Pattern Analysis and Machine Intelligence\",\n    year    = \"2022\",\n    volume  = \"44\",\n    number  = \"3\"\n}\n```\n\nIf you use a DPT-based model, please also cite:\n\n```\n@article{Ranftl2021,\n\tauthor    = {Ren\\'{e} Ranftl and Alexey Bochkovskiy and Vladlen Koltun},\n\ttitle     = {Vision Transformers for Dense Prediction},\n\tjournal   = {ICCV},\n\tyear      = {2021},\n}\n```\n\nPlease cite the technical report for MiDaS 3.1 models:\n\n```\n@article{birkl2023midas,\n      title={MiDaS v3.1 -- A Model Zoo for Robust Monocular Relative Depth Estimation},\n      author={Reiner Birkl and Diana Wofk and Matthias M{\\\"u}ller},\n      journal={arXiv preprint arXiv:2307.14460},\n      year={2023}\n}\n```\n\nFor ZoeDepth, please use\n\n```\n@article{bhat2023zoedepth,\n  title={Zoedepth: Zero-shot transfer by combining relative and metric depth},\n  author={Bhat, Shariq Farooq and Birkl, Reiner and Wofk, Diana and Wonka, Peter and M{\\\"u}ller, Matthias},\n  journal={arXiv preprint arXiv:2302.12288},\n  year={2023}\n}\n```\n\nand for LDM3D\n\n```\n@article{stan2023ldm3d,\n  title={LDM3D: Latent Diffusion Model for 3D},\n  author={Stan, Gabriela Ben Melech and Wofk, Diana and Fox, Scottie and Redden, Alex and Saxton, Will and Yu, Jean and Aflalo, Estelle and Tseng, Shao-Yen and Nonato, Fabio and Muller, Matthias and others},\n  journal={arXiv preprint arXiv:2305.10853},\n  year={2023}\n}\n```\n\n### Acknowledgements\n\nOur work builds on and uses code from [timm](https://github.com/rwightman/pytorch-image-models) and [Next-ViT](https://github.com/bytedance/Next-ViT). \nWe'd like to thank the authors for making these libraries available.\n\n### License \n\nMIT License \n"
        },
        {
          "name": "SECURITY.md",
          "type": "blob",
          "size": 0.396484375,
          "content": "# Security Policy\nIntel is committed to rapidly addressing security vulnerabilities affecting our customers and providing clear guidance on the solution, impact, severity and mitigation. \n\n## Reporting a Vulnerability\nPlease report any security vulnerabilities in this project utilizing the guidelines [here](https://www.intel.com/content/www/us/en/security-center/vulnerability-handling-guidelines.html).\n"
        },
        {
          "name": "environment.yaml",
          "type": "blob",
          "size": 0.2880859375,
          "content": "name: midas-py310\nchannels:\n  - pytorch\n  - defaults\ndependencies:\n  - nvidia::cudatoolkit=11.7\n  - python=3.10.8\n  - pytorch::pytorch=1.13.0\n  - torchvision=0.14.0\n  - pip=22.3.1\n  - numpy=1.23.4\n  - pip:\n    - opencv-python==4.6.0.66\n    - imutils==0.5.4\n    - timm==0.6.12\n    - einops==0.6.0"
        },
        {
          "name": "figures",
          "type": "tree",
          "content": null
        },
        {
          "name": "hubconf.py",
          "type": "blob",
          "size": 13.4345703125,
          "content": "dependencies = [\"torch\"]\n\nimport torch\n\nfrom midas.dpt_depth import DPTDepthModel\nfrom midas.midas_net import MidasNet\nfrom midas.midas_net_custom import MidasNet_small\n\ndef DPT_BEiT_L_512(pretrained=True, **kwargs):\n    \"\"\" # This docstring shows up in hub.help()\n    MiDaS DPT_BEiT_L_512 model for monocular depth estimation\n    pretrained (bool): load pretrained weights into model\n    \"\"\"\n\n    model = DPTDepthModel(\n            path=None,\n            backbone=\"beitl16_512\",\n            non_negative=True,\n        )\n\n    if pretrained:\n        checkpoint = (\n            \"https://github.com/isl-org/MiDaS/releases/download/v3_1/dpt_beit_large_512.pt\"\n        )\n        state_dict = torch.hub.load_state_dict_from_url(\n            checkpoint, map_location=torch.device('cpu'), progress=True, check_hash=True\n        )\n        model.load_state_dict(state_dict)\n\n    return model\n\ndef DPT_BEiT_L_384(pretrained=True, **kwargs):\n    \"\"\" # This docstring shows up in hub.help()\n    MiDaS DPT_BEiT_L_384 model for monocular depth estimation\n    pretrained (bool): load pretrained weights into model\n    \"\"\"\n\n    model = DPTDepthModel(\n            path=None,\n            backbone=\"beitl16_384\",\n            non_negative=True,\n        )\n\n    if pretrained:\n        checkpoint = (\n            \"https://github.com/isl-org/MiDaS/releases/download/v3_1/dpt_beit_large_384.pt\"\n        )\n        state_dict = torch.hub.load_state_dict_from_url(\n            checkpoint, map_location=torch.device('cpu'), progress=True, check_hash=True\n        )\n        model.load_state_dict(state_dict)\n\n    return model\n\ndef DPT_BEiT_B_384(pretrained=True, **kwargs):\n    \"\"\" # This docstring shows up in hub.help()\n    MiDaS DPT_BEiT_B_384 model for monocular depth estimation\n    pretrained (bool): load pretrained weights into model\n    \"\"\"\n\n    model = DPTDepthModel(\n            path=None,\n            backbone=\"beitb16_384\",\n            non_negative=True,\n        )\n\n    if pretrained:\n        checkpoint = (\n            \"https://github.com/isl-org/MiDaS/releases/download/v3_1/dpt_beit_base_384.pt\"\n        )\n        state_dict = torch.hub.load_state_dict_from_url(\n            checkpoint, map_location=torch.device('cpu'), progress=True, check_hash=True\n        )\n        model.load_state_dict(state_dict)\n\n    return model\n\ndef DPT_SwinV2_L_384(pretrained=True, **kwargs):\n    \"\"\" # This docstring shows up in hub.help()\n    MiDaS DPT_SwinV2_L_384 model for monocular depth estimation\n    pretrained (bool): load pretrained weights into model\n    \"\"\"\n\n    model = DPTDepthModel(\n            path=None,\n            backbone=\"swin2l24_384\",\n            non_negative=True,\n        )\n\n    if pretrained:\n        checkpoint = (\n            \"https://github.com/isl-org/MiDaS/releases/download/v3_1/dpt_swin2_large_384.pt\"\n        )\n        state_dict = torch.hub.load_state_dict_from_url(\n            checkpoint, map_location=torch.device('cpu'), progress=True, check_hash=True\n        )\n        model.load_state_dict(state_dict)\n\n    return model\n\ndef DPT_SwinV2_B_384(pretrained=True, **kwargs):\n    \"\"\" # This docstring shows up in hub.help()\n    MiDaS DPT_SwinV2_B_384 model for monocular depth estimation\n    pretrained (bool): load pretrained weights into model\n    \"\"\"\n\n    model = DPTDepthModel(\n            path=None,\n            backbone=\"swin2b24_384\",\n            non_negative=True,\n        )\n\n    if pretrained:\n        checkpoint = (\n            \"https://github.com/isl-org/MiDaS/releases/download/v3_1/dpt_swin2_base_384.pt\"\n        )\n        state_dict = torch.hub.load_state_dict_from_url(\n            checkpoint, map_location=torch.device('cpu'), progress=True, check_hash=True\n        )\n        model.load_state_dict(state_dict)\n\n    return model\n\ndef DPT_SwinV2_T_256(pretrained=True, **kwargs):\n    \"\"\" # This docstring shows up in hub.help()\n    MiDaS DPT_SwinV2_T_256 model for monocular depth estimation\n    pretrained (bool): load pretrained weights into model\n    \"\"\"\n\n    model = DPTDepthModel(\n            path=None,\n            backbone=\"swin2t16_256\",\n            non_negative=True,\n        )\n\n    if pretrained:\n        checkpoint = (\n            \"https://github.com/isl-org/MiDaS/releases/download/v3_1/dpt_swin2_tiny_256.pt\"\n        )\n        state_dict = torch.hub.load_state_dict_from_url(\n            checkpoint, map_location=torch.device('cpu'), progress=True, check_hash=True\n        )\n        model.load_state_dict(state_dict)\n\n    return model\n\ndef DPT_Swin_L_384(pretrained=True, **kwargs):\n    \"\"\" # This docstring shows up in hub.help()\n    MiDaS DPT_Swin_L_384 model for monocular depth estimation\n    pretrained (bool): load pretrained weights into model\n    \"\"\"\n\n    model = DPTDepthModel(\n            path=None,\n            backbone=\"swinl12_384\",\n            non_negative=True,\n        )\n\n    if pretrained:\n        checkpoint = (\n            \"https://github.com/isl-org/MiDaS/releases/download/v3_1/dpt_swin_large_384.pt\"\n        )\n        state_dict = torch.hub.load_state_dict_from_url(\n            checkpoint, map_location=torch.device('cpu'), progress=True, check_hash=True\n        )\n        model.load_state_dict(state_dict)\n\n    return model\n\ndef DPT_Next_ViT_L_384(pretrained=True, **kwargs):\n    \"\"\" # This docstring shows up in hub.help()\n    MiDaS DPT_Next_ViT_L_384 model for monocular depth estimation\n    pretrained (bool): load pretrained weights into model\n    \"\"\"\n\n    model = DPTDepthModel(\n            path=None,\n            backbone=\"next_vit_large_6m\",\n            non_negative=True,\n        )\n\n    if pretrained:\n        checkpoint = (\n            \"https://github.com/isl-org/MiDaS/releases/download/v3_1/dpt_next_vit_large_384.pt\"\n        )\n        state_dict = torch.hub.load_state_dict_from_url(\n            checkpoint, map_location=torch.device('cpu'), progress=True, check_hash=True\n        )\n        model.load_state_dict(state_dict)\n\n    return model\n\ndef DPT_LeViT_224(pretrained=True, **kwargs):\n    \"\"\" # This docstring shows up in hub.help()\n    MiDaS DPT_LeViT_224 model for monocular depth estimation\n    pretrained (bool): load pretrained weights into model\n    \"\"\"\n\n    model = DPTDepthModel(\n            path=None,\n            backbone=\"levit_384\",\n            non_negative=True,\n            head_features_1=64,\n            head_features_2=8,\n        )\n\n    if pretrained:\n        checkpoint = (\n            \"https://github.com/isl-org/MiDaS/releases/download/v3_1/dpt_levit_224.pt\"\n        )\n        state_dict = torch.hub.load_state_dict_from_url(\n            checkpoint, map_location=torch.device('cpu'), progress=True, check_hash=True\n        )\n        model.load_state_dict(state_dict)\n\n    return model\n\ndef DPT_Large(pretrained=True, **kwargs):\n    \"\"\" # This docstring shows up in hub.help()\n    MiDaS DPT-Large model for monocular depth estimation\n    pretrained (bool): load pretrained weights into model\n    \"\"\"\n\n    model = DPTDepthModel(\n            path=None,\n            backbone=\"vitl16_384\",\n            non_negative=True,\n        )\n\n    if pretrained:\n        checkpoint = (\n            \"https://github.com/isl-org/MiDaS/releases/download/v3/dpt_large_384.pt\"\n        )\n        state_dict = torch.hub.load_state_dict_from_url(\n            checkpoint, map_location=torch.device('cpu'), progress=True, check_hash=True\n        )\n        model.load_state_dict(state_dict)\n\n    return model\n    \ndef DPT_Hybrid(pretrained=True, **kwargs):\n    \"\"\" # This docstring shows up in hub.help()\n    MiDaS DPT-Hybrid model for monocular depth estimation\n    pretrained (bool): load pretrained weights into model\n    \"\"\"\n\n    model = DPTDepthModel(\n            path=None,\n            backbone=\"vitb_rn50_384\",\n            non_negative=True,\n        )\n\n    if pretrained:\n        checkpoint = (\n            \"https://github.com/isl-org/MiDaS/releases/download/v3/dpt_hybrid_384.pt\"\n        )\n        state_dict = torch.hub.load_state_dict_from_url(\n            checkpoint, map_location=torch.device('cpu'), progress=True, check_hash=True\n        )\n        model.load_state_dict(state_dict)\n\n    return model\n    \ndef MiDaS(pretrained=True, **kwargs):\n    \"\"\" # This docstring shows up in hub.help()\n    MiDaS v2.1 model for monocular depth estimation\n    pretrained (bool): load pretrained weights into model\n    \"\"\"\n\n    model = MidasNet()\n\n    if pretrained:\n        checkpoint = (\n            \"https://github.com/isl-org/MiDaS/releases/download/v2_1/midas_v21_384.pt\"\n        )\n        state_dict = torch.hub.load_state_dict_from_url(\n            checkpoint, map_location=torch.device('cpu'), progress=True, check_hash=True\n        )\n        model.load_state_dict(state_dict)\n\n    return model\n\ndef MiDaS_small(pretrained=True, **kwargs):\n    \"\"\" # This docstring shows up in hub.help()\n    MiDaS v2.1 small model for monocular depth estimation on resource-constrained devices\n    pretrained (bool): load pretrained weights into model\n    \"\"\"\n\n    model = MidasNet_small(None, features=64, backbone=\"efficientnet_lite3\", exportable=True, non_negative=True, blocks={'expand': True})\n\n    if pretrained:\n        checkpoint = (\n            \"https://github.com/isl-org/MiDaS/releases/download/v2_1/midas_v21_small_256.pt\"\n        )\n        state_dict = torch.hub.load_state_dict_from_url(\n            checkpoint, map_location=torch.device('cpu'), progress=True, check_hash=True\n        )\n        model.load_state_dict(state_dict)\n\n    return model\n\n\ndef transforms():\n    import cv2\n    from torchvision.transforms import Compose\n    from midas.transforms import Resize, NormalizeImage, PrepareForNet\n    from midas import transforms\n\n    transforms.default_transform = Compose(\n        [\n            lambda img: {\"image\": img / 255.0},\n            Resize(\n                384,\n                384,\n                resize_target=None,\n                keep_aspect_ratio=True,\n                ensure_multiple_of=32,\n                resize_method=\"upper_bound\",\n                image_interpolation_method=cv2.INTER_CUBIC,\n            ),\n            NormalizeImage(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n            PrepareForNet(),\n            lambda sample: torch.from_numpy(sample[\"image\"]).unsqueeze(0),\n        ]\n    )\n\n    transforms.small_transform = Compose(\n        [\n            lambda img: {\"image\": img / 255.0},\n            Resize(\n                256,\n                256,\n                resize_target=None,\n                keep_aspect_ratio=True,\n                ensure_multiple_of=32,\n                resize_method=\"upper_bound\",\n                image_interpolation_method=cv2.INTER_CUBIC,\n            ),\n            NormalizeImage(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n            PrepareForNet(),\n            lambda sample: torch.from_numpy(sample[\"image\"]).unsqueeze(0),\n        ]\n    )\n\n    transforms.dpt_transform = Compose(\n        [\n            lambda img: {\"image\": img / 255.0},\n            Resize(\n                384,\n                384,\n                resize_target=None,\n                keep_aspect_ratio=True,\n                ensure_multiple_of=32,\n                resize_method=\"minimal\",\n                image_interpolation_method=cv2.INTER_CUBIC,\n            ),\n            NormalizeImage(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n            PrepareForNet(),\n            lambda sample: torch.from_numpy(sample[\"image\"]).unsqueeze(0),\n        ]\n    )\n\n    transforms.beit512_transform = Compose(\n        [\n            lambda img: {\"image\": img / 255.0},\n            Resize(\n                512,\n                512,\n                resize_target=None,\n                keep_aspect_ratio=True,\n                ensure_multiple_of=32,\n                resize_method=\"minimal\",\n                image_interpolation_method=cv2.INTER_CUBIC,\n            ),\n            NormalizeImage(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n            PrepareForNet(),\n            lambda sample: torch.from_numpy(sample[\"image\"]).unsqueeze(0),\n        ]\n    )\n\n    transforms.swin384_transform = Compose(\n        [\n            lambda img: {\"image\": img / 255.0},\n            Resize(\n                384,\n                384,\n                resize_target=None,\n                keep_aspect_ratio=False,\n                ensure_multiple_of=32,\n                resize_method=\"minimal\",\n                image_interpolation_method=cv2.INTER_CUBIC,\n            ),\n            NormalizeImage(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n            PrepareForNet(),\n            lambda sample: torch.from_numpy(sample[\"image\"]).unsqueeze(0),\n        ]\n    )\n\n    transforms.swin256_transform = Compose(\n        [\n            lambda img: {\"image\": img / 255.0},\n            Resize(\n                256,\n                256,\n                resize_target=None,\n                keep_aspect_ratio=False,\n                ensure_multiple_of=32,\n                resize_method=\"minimal\",\n                image_interpolation_method=cv2.INTER_CUBIC,\n            ),\n            NormalizeImage(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n            PrepareForNet(),\n            lambda sample: torch.from_numpy(sample[\"image\"]).unsqueeze(0),\n        ]\n    )\n\n    transforms.levit_transform = Compose(\n        [\n            lambda img: {\"image\": img / 255.0},\n            Resize(\n                224,\n                224,\n                resize_target=None,\n                keep_aspect_ratio=False,\n                ensure_multiple_of=32,\n                resize_method=\"minimal\",\n                image_interpolation_method=cv2.INTER_CUBIC,\n            ),\n            NormalizeImage(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n            PrepareForNet(),\n            lambda sample: torch.from_numpy(sample[\"image\"]).unsqueeze(0),\n        ]\n    )\n\n    return transforms\n"
        },
        {
          "name": "input",
          "type": "tree",
          "content": null
        },
        {
          "name": "midas",
          "type": "tree",
          "content": null
        },
        {
          "name": "mobile",
          "type": "tree",
          "content": null
        },
        {
          "name": "output",
          "type": "tree",
          "content": null
        },
        {
          "name": "ros",
          "type": "tree",
          "content": null
        },
        {
          "name": "run.py",
          "type": "blob",
          "size": 11.013671875,
          "content": "\"\"\"Compute depth maps for images in the input folder.\n\"\"\"\nimport os\nimport glob\nimport torch\nimport utils\nimport cv2\nimport argparse\nimport time\n\nimport numpy as np\n\nfrom imutils.video import VideoStream\nfrom midas.model_loader import default_models, load_model\n\nfirst_execution = True\ndef process(device, model, model_type, image, input_size, target_size, optimize, use_camera):\n    \"\"\"\n    Run the inference and interpolate.\n\n    Args:\n        device (torch.device): the torch device used\n        model: the model used for inference\n        model_type: the type of the model\n        image: the image fed into the neural network\n        input_size: the size (width, height) of the neural network input (for OpenVINO)\n        target_size: the size (width, height) the neural network output is interpolated to\n        optimize: optimize the model to half-floats on CUDA?\n        use_camera: is the camera used?\n\n    Returns:\n        the prediction\n    \"\"\"\n    global first_execution\n\n    if \"openvino\" in model_type:\n        if first_execution or not use_camera:\n            print(f\"    Input resized to {input_size[0]}x{input_size[1]} before entering the encoder\")\n            first_execution = False\n\n        sample = [np.reshape(image, (1, 3, *input_size))]\n        prediction = model(sample)[model.output(0)][0]\n        prediction = cv2.resize(prediction, dsize=target_size,\n                                interpolation=cv2.INTER_CUBIC)\n    else:\n        sample = torch.from_numpy(image).to(device).unsqueeze(0)\n\n        if optimize and device == torch.device(\"cuda\"):\n            if first_execution:\n                print(\"  Optimization to half-floats activated. Use with caution, because models like Swin require\\n\"\n                      \"  float precision to work properly and may yield non-finite depth values to some extent for\\n\"\n                      \"  half-floats.\")\n            sample = sample.to(memory_format=torch.channels_last)\n            sample = sample.half()\n\n        if first_execution or not use_camera:\n            height, width = sample.shape[2:]\n            print(f\"    Input resized to {width}x{height} before entering the encoder\")\n            first_execution = False\n\n        prediction = model.forward(sample)\n        prediction = (\n            torch.nn.functional.interpolate(\n                prediction.unsqueeze(1),\n                size=target_size[::-1],\n                mode=\"bicubic\",\n                align_corners=False,\n            )\n            .squeeze()\n            .cpu()\n            .numpy()\n        )\n\n    return prediction\n\n\ndef create_side_by_side(image, depth, grayscale):\n    \"\"\"\n    Take an RGB image and depth map and place them side by side. This includes a proper normalization of the depth map\n    for better visibility.\n\n    Args:\n        image: the RGB image\n        depth: the depth map\n        grayscale: use a grayscale colormap?\n\n    Returns:\n        the image and depth map place side by side\n    \"\"\"\n    depth_min = depth.min()\n    depth_max = depth.max()\n    normalized_depth = 255 * (depth - depth_min) / (depth_max - depth_min)\n    normalized_depth *= 3\n\n    right_side = np.repeat(np.expand_dims(normalized_depth, 2), 3, axis=2) / 3\n    if not grayscale:\n        right_side = cv2.applyColorMap(np.uint8(right_side), cv2.COLORMAP_INFERNO)\n\n    if image is None:\n        return right_side\n    else:\n        return np.concatenate((image, right_side), axis=1)\n\n\ndef run(input_path, output_path, model_path, model_type=\"dpt_beit_large_512\", optimize=False, side=False, height=None,\n        square=False, grayscale=False):\n    \"\"\"Run MonoDepthNN to compute depth maps.\n\n    Args:\n        input_path (str): path to input folder\n        output_path (str): path to output folder\n        model_path (str): path to saved model\n        model_type (str): the model type\n        optimize (bool): optimize the model to half-floats on CUDA?\n        side (bool): RGB and depth side by side in output images?\n        height (int): inference encoder image height\n        square (bool): resize to a square resolution?\n        grayscale (bool): use a grayscale colormap?\n    \"\"\"\n    print(\"Initialize\")\n\n    # select device\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    print(\"Device: %s\" % device)\n\n    model, transform, net_w, net_h = load_model(device, model_path, model_type, optimize, height, square)\n\n    # get input\n    if input_path is not None:\n        image_names = glob.glob(os.path.join(input_path, \"*\"))\n        num_images = len(image_names)\n    else:\n        print(\"No input path specified. Grabbing images from camera.\")\n\n    # create output folder\n    if output_path is not None:\n        os.makedirs(output_path, exist_ok=True)\n\n    print(\"Start processing\")\n\n    if input_path is not None:\n        if output_path is None:\n            print(\"Warning: No output path specified. Images will be processed but not shown or stored anywhere.\")\n        for index, image_name in enumerate(image_names):\n\n            print(\"  Processing {} ({}/{})\".format(image_name, index + 1, num_images))\n\n            # input\n            original_image_rgb = utils.read_image(image_name)  # in [0, 1]\n            image = transform({\"image\": original_image_rgb})[\"image\"]\n\n            # compute\n            with torch.no_grad():\n                prediction = process(device, model, model_type, image, (net_w, net_h), original_image_rgb.shape[1::-1],\n                                     optimize, False)\n\n            # output\n            if output_path is not None:\n                filename = os.path.join(\n                    output_path, os.path.splitext(os.path.basename(image_name))[0] + '-' + model_type\n                )\n                if not side:\n                    utils.write_depth(filename, prediction, grayscale, bits=2)\n                else:\n                    original_image_bgr = np.flip(original_image_rgb, 2)\n                    content = create_side_by_side(original_image_bgr*255, prediction, grayscale)\n                    cv2.imwrite(filename + \".png\", content)\n                utils.write_pfm(filename + \".pfm\", prediction.astype(np.float32))\n\n    else:\n        with torch.no_grad():\n            fps = 1\n            video = VideoStream(0).start()\n            time_start = time.time()\n            frame_index = 0\n            while True:\n                frame = video.read()\n                if frame is not None:\n                    original_image_rgb = np.flip(frame, 2)  # in [0, 255] (flip required to get RGB)\n                    image = transform({\"image\": original_image_rgb/255})[\"image\"]\n\n                    prediction = process(device, model, model_type, image, (net_w, net_h),\n                                         original_image_rgb.shape[1::-1], optimize, True)\n\n                    original_image_bgr = np.flip(original_image_rgb, 2) if side else None\n                    content = create_side_by_side(original_image_bgr, prediction, grayscale)\n                    cv2.imshow('MiDaS Depth Estimation - Press Escape to close window ', content/255)\n\n                    if output_path is not None:\n                        filename = os.path.join(output_path, 'Camera' + '-' + model_type + '_' + str(frame_index))\n                        cv2.imwrite(filename + \".png\", content)\n\n                    alpha = 0.1\n                    if time.time()-time_start > 0:\n                        fps = (1 - alpha) * fps + alpha * 1 / (time.time()-time_start)  # exponential moving average\n                        time_start = time.time()\n                    print(f\"\\rFPS: {round(fps,2)}\", end=\"\")\n\n                    if cv2.waitKey(1) == 27:  # Escape key\n                        break\n\n                    frame_index += 1\n        print()\n\n    print(\"Finished\")\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n\n    parser.add_argument('-i', '--input_path',\n                        default=None,\n                        help='Folder with input images (if no input path is specified, images are tried to be grabbed '\n                             'from camera)'\n                        )\n\n    parser.add_argument('-o', '--output_path',\n                        default=None,\n                        help='Folder for output images'\n                        )\n\n    parser.add_argument('-m', '--model_weights',\n                        default=None,\n                        help='Path to the trained weights of model'\n                        )\n\n    parser.add_argument('-t', '--model_type',\n                        default='dpt_beit_large_512',\n                        help='Model type: '\n                             'dpt_beit_large_512, dpt_beit_large_384, dpt_beit_base_384, dpt_swin2_large_384, '\n                             'dpt_swin2_base_384, dpt_swin2_tiny_256, dpt_swin_large_384, dpt_next_vit_large_384, '\n                             'dpt_levit_224, dpt_large_384, dpt_hybrid_384, midas_v21_384, midas_v21_small_256 or '\n                             'openvino_midas_v21_small_256'\n                        )\n\n    parser.add_argument('-s', '--side',\n                        action='store_true',\n                        help='Output images contain RGB and depth images side by side'\n                        )\n\n    parser.add_argument('--optimize', dest='optimize', action='store_true', help='Use half-float optimization')\n    parser.set_defaults(optimize=False)\n\n    parser.add_argument('--height',\n                        type=int, default=None,\n                        help='Preferred height of images feed into the encoder during inference. Note that the '\n                             'preferred height may differ from the actual height, because an alignment to multiples of '\n                             '32 takes place. Many models support only the height chosen during training, which is '\n                             'used automatically if this parameter is not set.'\n                        )\n    parser.add_argument('--square',\n                        action='store_true',\n                        help='Option to resize images to a square resolution by changing their widths when images are '\n                             'fed into the encoder during inference. If this parameter is not set, the aspect ratio of '\n                             'images is tried to be preserved if supported by the model.'\n                        )\n    parser.add_argument('--grayscale',\n                        action='store_true',\n                        help='Use a grayscale colormap instead of the inferno one. Although the inferno colormap, '\n                             'which is used by default, is better for visibility, it does not allow storing 16-bit '\n                             'depth values in PNGs but only 8-bit ones due to the precision limitation of this '\n                             'colormap.'\n                        )\n\n    args = parser.parse_args()\n\n\n    if args.model_weights is None:\n        args.model_weights = default_models[args.model_type]\n\n    # set torch options\n    torch.backends.cudnn.enabled = True\n    torch.backends.cudnn.benchmark = True\n\n    # compute depth maps\n    run(args.input_path, args.output_path, args.model_weights, args.model_type, args.optimize, args.side, args.height,\n        args.square, args.grayscale)\n"
        },
        {
          "name": "tf",
          "type": "tree",
          "content": null
        },
        {
          "name": "utils.py",
          "type": "blob",
          "size": 4.7646484375,
          "content": "\"\"\"Utils for monoDepth.\n\"\"\"\nimport sys\nimport re\nimport numpy as np\nimport cv2\nimport torch\n\n\ndef read_pfm(path):\n    \"\"\"Read pfm file.\n\n    Args:\n        path (str): path to file\n\n    Returns:\n        tuple: (data, scale)\n    \"\"\"\n    with open(path, \"rb\") as file:\n\n        color = None\n        width = None\n        height = None\n        scale = None\n        endian = None\n\n        header = file.readline().rstrip()\n        if header.decode(\"ascii\") == \"PF\":\n            color = True\n        elif header.decode(\"ascii\") == \"Pf\":\n            color = False\n        else:\n            raise Exception(\"Not a PFM file: \" + path)\n\n        dim_match = re.match(r\"^(\\d+)\\s(\\d+)\\s$\", file.readline().decode(\"ascii\"))\n        if dim_match:\n            width, height = list(map(int, dim_match.groups()))\n        else:\n            raise Exception(\"Malformed PFM header.\")\n\n        scale = float(file.readline().decode(\"ascii\").rstrip())\n        if scale < 0:\n            # little-endian\n            endian = \"<\"\n            scale = -scale\n        else:\n            # big-endian\n            endian = \">\"\n\n        data = np.fromfile(file, endian + \"f\")\n        shape = (height, width, 3) if color else (height, width)\n\n        data = np.reshape(data, shape)\n        data = np.flipud(data)\n\n        return data, scale\n\n\ndef write_pfm(path, image, scale=1):\n    \"\"\"Write pfm file.\n\n    Args:\n        path (str): pathto file\n        image (array): data\n        scale (int, optional): Scale. Defaults to 1.\n    \"\"\"\n\n    with open(path, \"wb\") as file:\n        color = None\n\n        if image.dtype.name != \"float32\":\n            raise Exception(\"Image dtype must be float32.\")\n\n        image = np.flipud(image)\n\n        if len(image.shape) == 3 and image.shape[2] == 3:  # color image\n            color = True\n        elif (\n            len(image.shape) == 2 or len(image.shape) == 3 and image.shape[2] == 1\n        ):  # greyscale\n            color = False\n        else:\n            raise Exception(\"Image must have H x W x 3, H x W x 1 or H x W dimensions.\")\n\n        file.write(\"PF\\n\" if color else \"Pf\\n\".encode())\n        file.write(\"%d %d\\n\".encode() % (image.shape[1], image.shape[0]))\n\n        endian = image.dtype.byteorder\n\n        if endian == \"<\" or endian == \"=\" and sys.byteorder == \"little\":\n            scale = -scale\n\n        file.write(\"%f\\n\".encode() % scale)\n\n        image.tofile(file)\n\n\ndef read_image(path):\n    \"\"\"Read image and output RGB image (0-1).\n\n    Args:\n        path (str): path to file\n\n    Returns:\n        array: RGB image (0-1)\n    \"\"\"\n    img = cv2.imread(path)\n\n    if img.ndim == 2:\n        img = cv2.cvtColor(img, cv2.COLOR_GRAY2BGR)\n\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) / 255.0\n\n    return img\n\n\ndef resize_image(img):\n    \"\"\"Resize image and make it fit for network.\n\n    Args:\n        img (array): image\n\n    Returns:\n        tensor: data ready for network\n    \"\"\"\n    height_orig = img.shape[0]\n    width_orig = img.shape[1]\n\n    if width_orig > height_orig:\n        scale = width_orig / 384\n    else:\n        scale = height_orig / 384\n\n    height = (np.ceil(height_orig / scale / 32) * 32).astype(int)\n    width = (np.ceil(width_orig / scale / 32) * 32).astype(int)\n\n    img_resized = cv2.resize(img, (width, height), interpolation=cv2.INTER_AREA)\n\n    img_resized = (\n        torch.from_numpy(np.transpose(img_resized, (2, 0, 1))).contiguous().float()\n    )\n    img_resized = img_resized.unsqueeze(0)\n\n    return img_resized\n\n\ndef resize_depth(depth, width, height):\n    \"\"\"Resize depth map and bring to CPU (numpy).\n\n    Args:\n        depth (tensor): depth\n        width (int): image width\n        height (int): image height\n\n    Returns:\n        array: processed depth\n    \"\"\"\n    depth = torch.squeeze(depth[0, :, :, :]).to(\"cpu\")\n\n    depth_resized = cv2.resize(\n        depth.numpy(), (width, height), interpolation=cv2.INTER_CUBIC\n    )\n\n    return depth_resized\n\ndef write_depth(path, depth, grayscale, bits=1):\n    \"\"\"Write depth map to png file.\n\n    Args:\n        path (str): filepath without extension\n        depth (array): depth\n        grayscale (bool): use a grayscale colormap?\n    \"\"\"\n    if not grayscale:\n        bits = 1\n\n    if not np.isfinite(depth).all():\n        depth=np.nan_to_num(depth, nan=0.0, posinf=0.0, neginf=0.0)\n        print(\"WARNING: Non-finite depth values present\")\n\n    depth_min = depth.min()\n    depth_max = depth.max()\n\n    max_val = (2**(8*bits))-1\n\n    if depth_max - depth_min > np.finfo(\"float\").eps:\n        out = max_val * (depth - depth_min) / (depth_max - depth_min)\n    else:\n        out = np.zeros(depth.shape, dtype=depth.dtype)\n\n    if not grayscale:\n        out = cv2.applyColorMap(np.uint8(out), cv2.COLORMAP_INFERNO)\n\n    if bits == 1:\n        cv2.imwrite(path + \".png\", out.astype(\"uint8\"))\n    elif bits == 2:\n        cv2.imwrite(path + \".png\", out.astype(\"uint16\"))\n\n    return\n"
        },
        {
          "name": "weights",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}