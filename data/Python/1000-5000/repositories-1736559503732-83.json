{
  "metadata": {
    "timestamp": 1736559503732,
    "page": 83,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjkw",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "open-mmlab/OpenPCDet",
      "stars": 4780,
      "defaultBranch": "master",
      "files": [
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.1298828125,
          "content": "**__pycache__**\n**build**\n**egg-info**\n**dist**\ndata/\n*.pyc\nvenv/\n*.idea/\n*.so\n*.yaml\n*.sh\n*.pth\n*.pkl\n*.zip\n*.bin\noutput\nversion.py\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 11.0908203125,
          "content": "                                 Apache License\n                           Version 2.0, January 2004\n                        http://www.apache.org/licenses/\n\n   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n\n   1. Definitions.\n\n      \"License\" shall mean the terms and conditions for use, reproduction,\n      and distribution as defined by Sections 1 through 9 of this document.\n\n      \"Licensor\" shall mean the copyright owner or entity authorized by\n      the copyright owner that is granting the License.\n\n      \"Legal Entity\" shall mean the union of the acting entity and all\n      other entities that control, are controlled by, or are under common\n      control with that entity. For the purposes of this definition,\n      \"control\" means (i) the power, direct or indirect, to cause the\n      direction or management of such entity, whether by contract or\n      otherwise, or (ii) ownership of fifty percent (50%) or more of the\n      outstanding shares, or (iii) beneficial ownership of such entity.\n\n      \"You\" (or \"Your\") shall mean an individual or Legal Entity\n      exercising permissions granted by this License.\n\n      \"Source\" form shall mean the preferred form for making modifications,\n      including but not limited to software source code, documentation\n      source, and configuration files.\n\n      \"Object\" form shall mean any form resulting from mechanical\n      transformation or translation of a Source form, including but\n      not limited to compiled object code, generated documentation,\n      and conversions to other media types.\n\n      \"Work\" shall mean the work of authorship, whether in Source or\n      Object form, made available under the License, as indicated by a\n      copyright notice that is included in or attached to the work\n      (an example is provided in the Appendix below).\n\n      \"Derivative Works\" shall mean any work, whether in Source or Object\n      form, that is based on (or derived from) the Work and for which the\n      editorial revisions, annotations, elaborations, or other modifications\n      represent, as a whole, an original work of authorship. For the purposes\n      of this License, Derivative Works shall not include works that remain\n      separable from, or merely link (or bind by name) to the interfaces of,\n      the Work and Derivative Works thereof.\n\n      \"Contribution\" shall mean any work of authorship, including\n      the original version of the Work and any modifications or additions\n      to that Work or Derivative Works thereof, that is intentionally\n      submitted to Licensor for inclusion in the Work by the copyright owner\n      or by an individual or Legal Entity authorized to submit on behalf of\n      the copyright owner. For the purposes of this definition, \"submitted\"\n      means any form of electronic, verbal, or written communication sent\n      to the Licensor or its representatives, including but not limited to\n      communication on electronic mailing lists, source code control systems,\n      and issue tracking systems that are managed by, or on behalf of, the\n      Licensor for the purpose of discussing and improving the Work, but\n      excluding communication that is conspicuously marked or otherwise\n      designated in writing by the copyright owner as \"Not a Contribution.\"\n\n      \"Contributor\" shall mean Licensor and any individual or Legal Entity\n      on behalf of whom a Contribution has been received by Licensor and\n      subsequently incorporated within the Work.\n\n   2. Grant of Copyright License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      copyright license to reproduce, prepare Derivative Works of,\n      publicly display, publicly perform, sublicense, and distribute the\n      Work and such Derivative Works in Source or Object form.\n\n   3. Grant of Patent License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      (except as stated in this section) patent license to make, have made,\n      use, offer to sell, sell, import, and otherwise transfer the Work,\n      where such license applies only to those patent claims licensable\n      by such Contributor that are necessarily infringed by their\n      Contribution(s) alone or by combination of their Contribution(s)\n      with the Work to which such Contribution(s) was submitted. If You\n      institute patent litigation against any entity (including a\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\n      or a Contribution incorporated within the Work constitutes direct\n      or contributory patent infringement, then any patent licenses\n      granted to You under this License for that Work shall terminate\n      as of the date such litigation is filed.\n\n   4. Redistribution. You may reproduce and distribute copies of the\n      Work or Derivative Works thereof in any medium, with or without\n      modifications, and in Source or Object form, provided that You\n      meet the following conditions:\n\n      (a) You must give any other recipients of the Work or\n          Derivative Works a copy of this License; and\n\n      (b) You must cause any modified files to carry prominent notices\n          stating that You changed the files; and\n\n      (c) You must retain, in the Source form of any Derivative Works\n          that You distribute, all copyright, patent, trademark, and\n          attribution notices from the Source form of the Work,\n          excluding those notices that do not pertain to any part of\n          the Derivative Works; and\n\n      (d) If the Work includes a \"NOTICE\" text file as part of its\n          distribution, then any Derivative Works that You distribute must\n          include a readable copy of the attribution notices contained\n          within such NOTICE file, excluding those notices that do not\n          pertain to any part of the Derivative Works, in at least one\n          of the following places: within a NOTICE text file distributed\n          as part of the Derivative Works; within the Source form or\n          documentation, if provided along with the Derivative Works; or,\n          within a display generated by the Derivative Works, if and\n          wherever such third-party notices normally appear. The contents\n          of the NOTICE file are for informational purposes only and\n          do not modify the License. You may add Your own attribution\n          notices within Derivative Works that You distribute, alongside\n          or as an addendum to the NOTICE text from the Work, provided\n          that such additional attribution notices cannot be construed\n          as modifying the License.\n\n      You may add Your own copyright statement to Your modifications and\n      may provide additional or different license terms and conditions\n      for use, reproduction, or distribution of Your modifications, or\n      for any such Derivative Works as a whole, provided Your use,\n      reproduction, and distribution of the Work otherwise complies with\n      the conditions stated in this License.\n\n   5. Submission of Contributions. Unless You explicitly state otherwise,\n      any Contribution intentionally submitted for inclusion in the Work\n      by You to the Licensor shall be under the terms and conditions of\n      this License, without any additional terms or conditions.\n      Notwithstanding the above, nothing herein shall supersede or modify\n      the terms of any separate license agreement you may have executed\n      with Licensor regarding such Contributions.\n\n   6. Trademarks. This License does not grant permission to use the trade\n      names, trademarks, service marks, or product names of the Licensor,\n      except as required for reasonable and customary use in describing the\n      origin of the Work and reproducing the content of the NOTICE file.\n\n   7. Disclaimer of Warranty. Unless required by applicable law or\n      agreed to in writing, Licensor provides the Work (and each\n      Contributor provides its Contributions) on an \"AS IS\" BASIS,\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n      implied, including, without limitation, any warranties or conditions\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n      PARTICULAR PURPOSE. You are solely responsible for determining the\n      appropriateness of using or redistributing the Work and assume any\n      risks associated with Your exercise of permissions under this License.\n\n   8. Limitation of Liability. In no event and under no legal theory,\n      whether in tort (including negligence), contract, or otherwise,\n      unless required by applicable law (such as deliberate and grossly\n      negligent acts) or agreed to in writing, shall any Contributor be\n      liable to You for damages, including any direct, indirect, special,\n      incidental, or consequential damages of any character arising as a\n      result of this License or out of the use or inability to use the\n      Work (including but not limited to damages for loss of goodwill,\n      work stoppage, computer failure or malfunction, or any and all\n      other commercial damages or losses), even if such Contributor\n      has been advised of the possibility of such damages.\n\n   9. Accepting Warranty or Additional Liability. While redistributing\n      the Work or Derivative Works thereof, You may choose to offer,\n      and charge a fee for, acceptance of support, warranty, indemnity,\n      or other liability obligations and/or rights consistent with this\n      License. However, in accepting such obligations, You may act only\n      on Your own behalf and on Your sole responsibility, not on behalf\n      of any other Contributor, and only if You agree to indemnify,\n      defend, and hold each Contributor harmless for any liability\n      incurred by, or claims asserted against, such Contributor by reason\n      of your accepting any such warranty or additional liability.\n\n   END OF TERMS AND CONDITIONS\n\n   APPENDIX: How to apply the Apache License to your work.\n\n      To apply the Apache License to your work, attach the following\n      boilerplate notice, with the fields enclosed by brackets \"[]\"\n      replaced with your own identifying information. (Don't include\n      the brackets!)  The text should be enclosed in the appropriate\n      comment syntax for the file format. We also recommend that a\n      file or class name and description of purpose be included on the\n      same \"printed page\" as the copyright notice for easier\n      identification within third-party archives.\n\n   Copyright [yyyy] [name of copyright owner]\n\n   Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 23.6181640625,
          "content": "<img src=\"docs/open_mmlab.png\" align=\"right\" width=\"30%\">\n\n# OpenPCDet\n\n`OpenPCDet` is a clear, simple, self-contained open source project for LiDAR-based 3D object detection. \n\nIt is also the official code release of [`[PointRCNN]`](https://arxiv.org/abs/1812.04244), [`[Part-A2-Net]`](https://arxiv.org/abs/1907.03670), [`[PV-RCNN]`](https://arxiv.org/abs/1912.13192), [`[Voxel R-CNN]`](https://arxiv.org/abs/2012.15712), [`[PV-RCNN++]`](https://arxiv.org/abs/2102.00463) and [`[MPPNet]`](https://arxiv.org/abs/2205.05979). \n\n**Highlights**: \n* `OpenPCDet` has been updated to `v0.6.0` (Sep. 2022).\n* The codes of PV-RCNN++ has been supported.\n* The codes of MPPNet has been supported. \n* The multi-modal 3D detection approaches on Nuscenes have been supported. \n\n## Overview\n- [Changelog](#changelog)\n- [Design Pattern](#openpcdet-design-pattern)\n- [Model Zoo](#model-zoo)\n- [Installation](docs/INSTALL.md)\n- [Quick Demo](docs/DEMO.md)\n- [Getting Started](docs/GETTING_STARTED.md)\n- [Citation](#citation)\n\n\n## Changelog\n[2023-06-30] **NEW:** Added support for [`DSVT`](https://arxiv.org/abs/2301.06051), which achieves state-of-the-art performance on large-scale Waymo Open Dataset with real-time inference speed (27HZ with TensorRT).\n\n[2023-05-13] **NEW:** Added support for the multi-modal 3D object detection models on Nuscenes dataset.  \n* Support multi-modal Nuscenes detection (See the [GETTING_STARTED.md](docs/GETTING_STARTED.md) to process data).\n* Support [TransFusion-Lidar](https://arxiv.org/abs/2203.11496) head, which ahcieves 69.43% NDS on Nuscenes validation dataset.\n* Support [`BEVFusion`](https://arxiv.org/abs/2205.13542), which fuses multi-modal information on BEV space and reaches 70.98% NDS on Nuscenes validation dataset. (see the [guideline](docs/guidelines_of_approaches/bevfusion.md) on how to train/test with BEVFusion).\n\n[2023-04-02] Added support for [`VoxelNeXt`](https://arxiv.org/abs/2303.11301) on Nuscenes, Waymo, and Argoverse2 datasets. It is a fully sparse 3D object detection network, which is a clean sparse CNNs network and predicts 3D objects directly upon voxels.\n\n[2022-09-02] **NEW:** Update `OpenPCDet` to v0.6.0:\n* Official code release of [`MPPNet`](https://arxiv.org/abs/2205.05979) for temporal 3D object detection, which supports long-term multi-frame 3D object detection and ranks 1st place on [3D detection learderboard](https://waymo.com/open/challenges/2020/3d-detection) of Waymo Open Dataset on Sept. 2th, 2022. For validation dataset, MPPNet achieves 74.96%, 75.06% and 74.52% for vehicle, pedestrian and cyclist classes in terms of mAPH@Level_2. (see the [guideline](docs/guidelines_of_approaches/mppnet.md) on how to train/test with MPPNet).\n* Support multi-frame training/testing on Waymo Open Dataset (see the [change log](docs/changelog.md) for more details on how to process data).\n* Support to save changing training details (e.g., loss, iter, epoch) to file (previous tqdm progress bar is still supported by using `--use_tqdm_to_record`). Please use `pip install gpustat` if you also want to log the GPU related information.\n* Support to save latest model every 5 mintues, so you can restore the model training from latest status instead of previous epoch.   \n\n[2022-08-22] Added support for [custom dataset tutorial and template](docs/CUSTOM_DATASET_TUTORIAL.md) \n\n[2022-07-05] Added support for the 3D object detection backbone network [`Focals Conv`](https://openaccess.thecvf.com/content/CVPR2022/papers/Chen_Focal_Sparse_Convolutional_Networks_for_3D_Object_Detection_CVPR_2022_paper.pdf).\n\n[2022-02-12] Added support for using docker. Please refer to the guidance in [./docker](./docker).\n\n[2022-02-07] Added support for Centerpoint models on Nuscenes Dataset.\n\n[2022-01-14] Added support for dynamic pillar voxelization, following the implementation proposed in [`H^23D R-CNN`](https://arxiv.org/abs/2107.14391) with unique operation and [`torch_scatter`](https://github.com/rusty1s/pytorch_scatter) package.\n\n[2022-01-05] **NEW:** Update `OpenPCDet` to v0.5.2:\n* The code of [`PV-RCNN++`](https://arxiv.org/abs/2102.00463) has been released to this repo, with higher performance, faster training/inference speed and less memory consumption than PV-RCNN.\n* Add performance of several models trained with full training set of [Waymo Open Dataset](#waymo-open-dataset-baselines).\n* Support Lyft dataset, see the pull request [here](https://github.com/open-mmlab/OpenPCDet/pull/720).\n\n\n[2021-12-09] **NEW:**  Update `OpenPCDet` to v0.5.1:\n* Add PointPillar related baseline configs/results on [Waymo Open Dataset](#waymo-open-dataset-baselines).\n* Support Pandaset dataloader, see the pull request [here](https://github.com/open-mmlab/OpenPCDet/pull/396).\n* Support a set of new augmentations, see the pull request [here](https://github.com/open-mmlab/OpenPCDet/pull/653).\n\n[2021-12-01] **NEW:** `OpenPCDet` v0.5.0 is released with the following features:\n* Improve the performance of all models on [Waymo Open Dataset](#waymo-open-dataset-baselines). Note that you need to re-prepare the training/validation data and ground-truth database of Waymo Open Dataset (see [GETTING_STARTED.md](docs/GETTING_STARTED.md)). \n* Support anchor-free [CenterHead](pcdet/models/dense_heads/center_head.py), add configs of `CenterPoint` and `PV-RCNN with CenterHead`.\n* Support lastest **PyTorch 1.1~1.10** and **spconv 1.0~2.x**, where **spconv 2.x** should be easy to install with pip and faster than previous version (see the official update of spconv [here](https://github.com/traveller59/spconv)).  \n* Support config [`USE_SHARED_MEMORY`](tools/cfgs/dataset_configs/waymo_dataset.yaml) to use shared memory to potentially speed up the training process in case you suffer from an IO problem.  \n* Support better and faster [visualization script](tools/visual_utils/open3d_vis_utils.py), and you need to install [Open3D](https://github.com/isl-org/Open3D) firstly. \n\n[2021-06-08] Added support for the voxel-based 3D object detection model [`Voxel R-CNN`](#KITTI-3D-Object-Detection-Baselines).\n\n[2021-05-14] Added support for the monocular 3D object detection model [`CaDDN`](#KITTI-3D-Object-Detection-Baselines).\n\n[2020-11-27] Bugfixed: Please re-prepare the validation infos of Waymo dataset (version 1.2) if you would like to \nuse our provided Waymo evaluation tool (see [PR](https://github.com/open-mmlab/OpenPCDet/pull/383)). \nNote that you do not need to re-prepare the training data and ground-truth database. \n\n[2020-11-10] The [Waymo Open Dataset](#waymo-open-dataset-baselines) has been supported with state-of-the-art results. Currently we provide the \nconfigs and results of `SECOND`, `PartA2` and `PV-RCNN` on the Waymo Open Dataset, and more models could be easily supported by modifying their dataset configs. \n\n[2020-08-10] Bugfixed: The provided NuScenes models have been updated to fix the loading bugs. Please redownload it if you need to use the pretrained NuScenes models.\n\n[2020-07-30] `OpenPCDet` v0.3.0 is released with the following features:\n   * The Point-based and Anchor-Free models ([`PointRCNN`](#KITTI-3D-Object-Detection-Baselines), [`PartA2-Free`](#KITTI-3D-Object-Detection-Baselines)) are supported now.\n   * The NuScenes dataset is supported with strong baseline results ([`SECOND-MultiHead (CBGS)`](#NuScenes-3D-Object-Detection-Baselines) and [`PointPillar-MultiHead`](#NuScenes-3D-Object-Detection-Baselines)).\n   * High efficiency than last version, support **PyTorch 1.1~1.7** and **spconv 1.0~1.2** simultaneously.\n   \n[2020-07-17]  Add simple visualization codes and a quick demo to test with custom data. \n\n[2020-06-24] `OpenPCDet` v0.2.0 is released with pretty new structures to support more models and datasets. \n\n[2020-03-16] `OpenPCDet` v0.1.0 is released. \n\n\n## Introduction\n\n\n### What does `OpenPCDet` toolbox do?\n\nNote that we have upgrated `PCDet` from `v0.1` to `v0.2` with pretty new structures to support various datasets and models.\n\n`OpenPCDet` is a general PyTorch-based codebase for 3D object detection from point cloud. \nIt currently supports multiple state-of-the-art 3D object detection methods with highly refactored codes for both one-stage and two-stage 3D detection frameworks.\n\nBased on `OpenPCDet` toolbox, we win the Waymo Open Dataset challenge in [3D Detection](https://waymo.com/open/challenges/3d-detection/), \n[3D Tracking](https://waymo.com/open/challenges/3d-tracking/), [Domain Adaptation](https://waymo.com/open/challenges/domain-adaptation/) \nthree tracks among all LiDAR-only methods, and the Waymo related models will be released to `OpenPCDet` soon.    \n\nWe are actively updating this repo currently, and more datasets and models will be supported soon. \nContributions are also welcomed. \n\n### `OpenPCDet` design pattern\n\n* Data-Model separation with unified point cloud coordinate for easily extending to custom datasets:\n<p align=\"center\">\n  <img src=\"docs/dataset_vs_model.png\" width=\"95%\" height=\"320\">\n</p>\n\n* Unified 3D box definition: (x, y, z, dx, dy, dz, heading).\n\n* Flexible and clear model structure to easily support various 3D detection models: \n<p align=\"center\">\n  <img src=\"docs/model_framework.png\" width=\"95%\">\n</p>\n\n* Support various models within one framework as: \n<p align=\"center\">\n  <img src=\"docs/multiple_models_demo.png\" width=\"95%\">\n</p>\n\n\n### Currently Supported Features\n\n- [x] Support both one-stage and two-stage 3D object detection frameworks\n- [x] Support distributed training & testing with multiple GPUs and multiple machines\n- [x] Support multiple heads on different scales to detect different classes\n- [x] Support stacked version set abstraction to encode various number of points in different scenes\n- [x] Support Adaptive Training Sample Selection (ATSS) for target assignment\n- [x] Support RoI-aware point cloud pooling & RoI-grid point cloud pooling\n- [x] Support GPU version 3D IoU calculation and rotated NMS \n\n\n## Model Zoo\n\n### KITTI 3D Object Detection Baselines\nSelected supported methods are shown in the below table. The results are the 3D detection performance of moderate difficulty on the *val* set of KITTI dataset.\n* All LiDAR-based models are trained with 8 GTX 1080Ti GPUs and are available for download. \n* The training time is measured with 8 TITAN XP GPUs and PyTorch 1.5.\n\n|                                             | training time | Car@R11 | Pedestrian@R11 | Cyclist@R11  | download | \n|---------------------------------------------|----------:|:-------:|:-------:|:-------:|:---------:|\n| [PointPillar](tools/cfgs/kitti_models/pointpillar.yaml) |~1.2 hours| 77.28 | 52.29 | 62.68 | [model-18M](https://drive.google.com/file/d/1wMxWTpU1qUoY3DsCH31WJmvJxcjFXKlm/view?usp=sharing) | \n| [SECOND](tools/cfgs/kitti_models/second.yaml)       |  ~1.7 hours  | 78.62 | 52.98 | 67.15 | [model-20M](https://drive.google.com/file/d/1-01zsPOsqanZQqIIyy7FpNXStL3y4jdR/view?usp=sharing) |\n| [SECOND-IoU](tools/cfgs/kitti_models/second_iou.yaml)       | -  | 79.09 | 55.74 | 71.31 | [model-46M](https://drive.google.com/file/d/1AQkeNs4bxhvhDQ-5sEo_yvQUlfo73lsW/view?usp=sharing) |\n| [PointRCNN](tools/cfgs/kitti_models/pointrcnn.yaml) | ~3 hours | 78.70 | 54.41 | 72.11 | [model-16M](https://drive.google.com/file/d/1BCX9wMn-GYAfSOPpyxf6Iv6fc0qKLSiU/view?usp=sharing)| \n| [PointRCNN-IoU](tools/cfgs/kitti_models/pointrcnn_iou.yaml) | ~3 hours | 78.75 | 58.32 | 71.34 | [model-16M](https://drive.google.com/file/d/1V0vNZ3lAHpEEt0MlT80eL2f41K2tHm_D/view?usp=sharing)|\n| [Part-A2-Free](tools/cfgs/kitti_models/PartA2_free.yaml)   | ~3.8 hours| 78.72 | 65.99 | 74.29 | [model-226M](https://drive.google.com/file/d/1lcUUxF8mJgZ_e-tZhP1XNQtTBuC-R0zr/view?usp=sharing) |\n| [Part-A2-Anchor](tools/cfgs/kitti_models/PartA2.yaml)    | ~4.3 hours| 79.40 | 60.05 | 69.90 | [model-244M](https://drive.google.com/file/d/10GK1aCkLqxGNeX3lVu8cLZyE0G8002hY/view?usp=sharing) |\n| [PV-RCNN](tools/cfgs/kitti_models/pv_rcnn.yaml) | ~5 hours| 83.61 | 57.90 | 70.47 | [model-50M](https://drive.google.com/file/d/1lIOq4Hxr0W3qsX83ilQv0nk1Cls6KAr-/view?usp=sharing) |\n| [Voxel R-CNN (Car)](tools/cfgs/kitti_models/voxel_rcnn_car.yaml) | ~2.2 hours| 84.54 | - | - | [model-28M](https://drive.google.com/file/d/19_jiAeGLz7V0wNjSJw4cKmMjdm5EW5By/view?usp=sharing) |\n| [Focals Conv - F](tools/cfgs/kitti_models/voxel_rcnn_car_focal_multimodal.yaml) | ~4 hours| 85.66 | - | - | [model-30M](https://drive.google.com/file/d/1u2Vcg7gZPOI-EqrHy7_6fqaibvRt2IjQ/view?usp=sharing) |\n||\n| [CaDDN (Mono)](tools/cfgs/kitti_models/CaDDN.yaml) |~15 hours| 21.38 | 13.02 | 9.76 | [model-774M](https://drive.google.com/file/d/1OQTO2PtXT8GGr35W9m2GZGuqgb6fyU1V/view?usp=sharing) |\n\n### Waymo Open Dataset Baselines\nWe provide the setting of [`DATA_CONFIG.SAMPLED_INTERVAL`](tools/cfgs/dataset_configs/waymo_dataset.yaml) on the Waymo Open Dataset (WOD) to subsample partial samples for training and evaluation, \nso you could also play with WOD by setting a smaller `DATA_CONFIG.SAMPLED_INTERVAL` even if you only have limited GPU resources. \n\nBy default, all models are trained with **a single frame** of **20% data (~32k frames)** of all the training samples on 8 GTX 1080Ti GPUs, and the results of each cell here are mAP/mAPH calculated by the official Waymo evaluation metrics on the **whole** validation set (version 1.2).    \n\n|    Performance@(train with 20\\% Data)            | Vec_L1 | Vec_L2 | Ped_L1 | Ped_L2 | Cyc_L1 | Cyc_L2 |  \n|---------------------------------------------|----------:|:-------:|:-------:|:-------:|:-------:|:-------:|\n| [SECOND](tools/cfgs/waymo_models/second.yaml) | 70.96/70.34|62.58/62.02|65.23/54.24\t|57.22/47.49|\t57.13/55.62 |\t54.97/53.53 | \n| [PointPillar](tools/cfgs/waymo_models/pointpillar_1x.yaml) | 70.43/69.83 |\t62.18/61.64 | 66.21/46.32|58.18/40.64|55.26/51.75|53.18/49.80 |\n[CenterPoint-Pillar](tools/cfgs/waymo_models/centerpoint_pillar_1x.yaml)| 70.50/69.96|62.18/61.69|73.11/61.97|65.06/55.00|65.44/63.85|62.98/61.46| \n[CenterPoint-Dynamic-Pillar](tools/cfgs/waymo_models/centerpoint_dyn_pillar_1x.yaml)| 70.46/69.93|62.06/61.58|73.92/63.35|65.91/56.33|66.24/64.69|63.73/62.24| \n[CenterPoint](tools/cfgs/waymo_models/centerpoint_without_resnet.yaml)| 71.33/70.76|63.16/62.65|\t72.09/65.49\t|64.27/58.23|\t68.68/67.39\t|66.11/64.87|\n| [CenterPoint (ResNet)](tools/cfgs/waymo_models/centerpoint.yaml)|72.76/72.23|64.91/64.42\t|74.19/67.96\t|66.03/60.34|\t71.04/69.79\t|68.49/67.28 |\n| [Part-A2-Anchor](tools/cfgs/waymo_models/PartA2.yaml) | 74.66/74.12\t|65.82/65.32\t|71.71/62.24\t|62.46/54.06\t|66.53/65.18\t|64.05/62.75 |\n| [PV-RCNN (AnchorHead)](tools/cfgs/waymo_models/pv_rcnn.yaml) | 75.41/74.74\t|67.44/66.80\t|71.98/61.24\t|63.70/53.95\t|65.88/64.25\t|63.39/61.82 | \n| [PV-RCNN (CenterHead)](tools/cfgs/waymo_models/pv_rcnn_with_centerhead_rpn.yaml) | 75.95/75.43\t|68.02/67.54\t|75.94/69.40\t|67.66/61.62\t|70.18/68.98\t|67.73/66.57|\n| [Voxel R-CNN (CenterHead)-Dynamic-Voxel](tools/cfgs/waymo_models/voxel_rcnn_with_centerhead_dyn_voxel.yaml) | 76.13/75.66\t|68.18/67.74\t|78.20/71.98\t|69.29/63.59\t| 70.75/69.68\t|68.25/67.21|\n| [PV-RCNN++](tools/cfgs/waymo_models/pv_rcnn_plusplus.yaml) | 77.82/77.32|\t69.07/68.62|\t77.99/71.36|\t69.92/63.74|\t71.80/70.71|\t69.31/68.26|\n| [PV-RCNN++ (ResNet)](tools/cfgs/waymo_models/pv_rcnn_plusplus_resnet.yaml) |77.61/77.14|\t69.18/68.75|\t79.42/73.31|\t70.88/65.21|\t72.50/71.39|\t69.84/68.77|\n\nHere we also provide the performance of several models trained on the full training set (refer to the paper of [PV-RCNN++](https://arxiv.org/abs/2102.00463)):\n\n| Performance@(train with 100\\% Data)                                                       | Vec_L1 | Vec_L2 | Ped_L1 | Ped_L2 | Cyc_L1 | Cyc_L2 |  \n|-------------------------------------------------------------------------------------------|----------:|:-------:|:-------:|:-------:|:-------:|:-------:|\n| [SECOND](tools/cfgs/waymo_models/second.yaml)                                             | 72.27/71.69 | 63.85/63.33 | 68.70/58.18 | 60.72/51.31 | 60.62/59.28 | 58.34/57.05 | \n| [CenterPoint-Pillar](tools/cfgs/waymo_models/centerpoint_pillar_1x.yaml)                  | 73.37/72.86 | 65.09/64.62 | 75.35/65.11 | 67.61/58.25 | 67.76/66.22 | 65.25/63.77 | \n| [Part-A2-Anchor](tools/cfgs/waymo_models/PartA2.yaml)                                     | 77.05/76.51 | 68.47/67.97 | 75.24/66.87 | 66.18/58.62 | 68.60/67.36 | 66.13/64.93 |\n| [VoxelNeXt-2D](tools/cfgs/waymo_models/voxelnext2d_ioubranch.yaml)                        | 77.94/77.47\t|69.68/69.25\t|80.24/73.47\t|72.23/65.88\t|73.33/72.20\t|70.66/69.56 | \n| [VoxelNeXt](tools/cfgs/waymo_models/voxelnext_ioubranch_large.yaml)                       | 78.16/77.70\t|69.86/69.42\t|81.47/76.30\t|73.48/68.63\t|76.06/74.90\t|73.29/72.18 |\n| [PV-RCNN (CenterHead)](tools/cfgs/waymo_models/pv_rcnn_with_centerhead_rpn.yaml)          | 78.00/77.50 | 69.43/68.98 | 79.21/73.03 | 70.42/64.72 | 71.46/70.27 | 68.95/67.79 |\n| [PV-RCNN++](tools/cfgs/waymo_models/pv_rcnn_plusplus.yaml)                                | 79.10/78.63 | 70.34/69.91 | 80.62/74.62 | 71.86/66.30 | 73.49/72.38 | 70.70/69.62 |\n| [PV-RCNN++ (ResNet)](tools/cfgs/waymo_models/pv_rcnn_plusplus_resnet.yaml)                | 79.25/78.78 | 70.61/70.18 | 81.83/76.28 | 73.17/68.00 | 73.72/72.66 | 71.21/70.19 |\n| [DSVT-Pillar](tools/cfgs/waymo_models/dsvt_pillar.yaml)                             | 79.44/78.97 | 71.24/70.81 | 83.00/77.22 | 75.45/69.95 | 76.70/75.70 | 73.83/72.86 |\n| [DSVT-Voxel](tools/cfgs/waymo_models/dsvt_voxel.yaml)                             | 79.77/79.31 | 71.67/71.25 | 83.75/78.92 | 76.21/71.57 | 77.57/76.58 | 74.70/73.73 |\n| [PV-RCNN++ (ResNet, 2 frames)](tools/cfgs/waymo_models/pv_rcnn_plusplus_resnet_2frames.yaml) | 80.17/79.70 | 72.14/71.70 | 83.48/80.42 | 75.54/72.61 | 74.63/73.75 | 72.35/71.50 |\n| [MPPNet (4 frames)](docs/guidelines_of_approaches/mppnet.md)                              | 81.54/81.06 | 74.07/73.61 | 84.56/81.94 | 77.20/74.67 | 77.15/76.50 | 75.01/74.38 |\n| [MPPNet (16 frames)](docs/guidelines_of_approaches/mppnet.md)                             | 82.74/82.28 | 75.41/74.96 | 84.69/82.25 | 77.43/75.06 | 77.28/76.66 | 75.13/74.52 |\n\n\n\n\n\n\n\nWe could not provide the above pretrained models due to [Waymo Dataset License Agreement](https://waymo.com/open/terms/), \nbut you could easily achieve similar performance by training with the default configs.\n\n### NuScenes 3D Object Detection Baselines\nAll models are trained with 8 GPUs and are available for download. For training BEVFusion, please refer to the [guideline](docs/guidelines_of_approaches/bevfusion.md).\n\n|                                                                                                    |   mATE |  mASE  |  mAOE  | mAVE  | mAAE  |  mAP  |  NDS   |                                              download                                              | \n|----------------------------------------------------------------------------------------------------|-------:|:------:|:------:|:-----:|:-----:|:-----:|:------:|:--------------------------------------------------------------------------------------------------:|\n| [PointPillar-MultiHead](tools/cfgs/nuscenes_models/cbgs_pp_multihead.yaml)                         | 33.87\t | 26.00  | 32.07\t | 28.74 | 20.15 | 44.63 | 58.23\t |  [model-23M](https://drive.google.com/file/d/1p-501mTWsq0G9RzroTWSXreIMyTUUpBM/view?usp=sharing)   | \n| [SECOND-MultiHead (CBGS)](tools/cfgs/nuscenes_models/cbgs_second_multihead.yaml)                   |  31.15 | \t25.51 | \t26.64 | 26.26 | 20.46 | 50.59 | 62.29  |  [model-35M](https://drive.google.com/file/d/1bNzcOnE3u9iooBFMk2xK7HqhdeQ_nwTq/view?usp=sharing)   |\n| [CenterPoint-PointPillar](tools/cfgs/nuscenes_models/cbgs_dyn_pp_centerpoint.yaml)                 |  31.13 | \t26.04 | \t42.92 | 23.90 | 19.14 | 50.03 | 60.70  |  [model-23M](https://drive.google.com/file/d/1UvGm6mROMyJzeSRu7OD1leU_YWoAZG7v/view?usp=sharing)   |\n| [CenterPoint (voxel_size=0.1)](tools/cfgs/nuscenes_models/cbgs_voxel01_res3d_centerpoint.yaml)     |  30.11 | \t25.55 | \t38.28 | 21.94 | 18.87 | 56.03 | 64.54  |  [model-34M](https://drive.google.com/file/d/1Cz-J1c3dw7JAWc25KRG1XQj8yCaOlexQ/view?usp=sharing)   |\n| [CenterPoint (voxel_size=0.075)](tools/cfgs/nuscenes_models/cbgs_voxel0075_res3d_centerpoint.yaml) |  28.80 | \t25.43 | \t37.27 | 21.55 | 18.24 | 59.22 | 66.48  |  [model-34M](https://drive.google.com/file/d/1XOHAWm1MPkCKr1gqmc3TWi5AYZgPsgxU/view?usp=sharing)   |\n| [VoxelNeXt (voxel_size=0.075)](tools/cfgs/nuscenes_models/cbgs_voxel0075_voxelnext.yaml)   |  30.11 | \t25.23 | \t40.57 | 21.69 | 18.56 | 60.53 | 66.65  | [model-31M](https://drive.google.com/file/d/1IV7e7G9X-61KXSjMGtQo579pzDNbhwvf/view?usp=share_link) |\n| [TransFusion-L*](tools/cfgs/nuscenes_models/transfusion_lidar.yaml)   |  27.96 | \t25.37 | \t29.35 | 27.31 | 18.55 | 64.58 | 69.43  | [model-32M](https://drive.google.com/file/d/1cuZ2qdDnxSwTCsiXWwbqCGF-uoazTXbz/view?usp=share_link) |\n| [BEVFusion](tools/cfgs/nuscenes_models/bevfusion.yaml)   |  28.03 | \t25.43 | \t30.19 | 26.76 | 18.48 | 67.75 | 70.98  | [model-157M](https://drive.google.com/file/d/1X50b-8immqlqD8VPAUkSKI0Ls-4k37g9/view?usp=share_link) |\n\n*: Use the fade strategy, which disables data augmentations in the last several epochs during training.\n\n### ONCE 3D Object Detection Baselines\nAll models are trained with 8 GPUs.\n\n|                                                        | Vehicle | Pedestrian | Cyclist | mAP    |\n| ------------------------------------------------------ | :-----: | :--------: | :-----: | :----: |\n| [PointRCNN](tools/cfgs/once_models/pointrcnn.yaml)     | 52.09   | 4.28       | 29.84   | 28.74  |\n| [PointPillar](tools/cfgs/once_models/pointpillar.yaml) | 68.57   | 17.63      | 46.81   | 44.34  |\n| [SECOND](tools/cfgs/once_models/second.yaml)           | 71.19   | 26.44      | 58.04   | 51.89  |\n| [PV-RCNN](tools/cfgs/once_models/pv_rcnn.yaml)         | 77.77   | 23.50      | 59.37   | 53.55  |\n| [CenterPoint](tools/cfgs/once_models/centerpoint.yaml) | 78.02   | 49.74      | 67.22   | 64.99  |\n\n### Argoverse2 3D Object Detection Baselines\nAll models are trained with 4 GPUs.\n\n|                                                         | mAP  |                                              download                                              | \n|---------------------------------------------------------|:----:|:--------------------------------------------------------------------------------------------------:|\n| [VoxelNeXt](tools/cfgs/argo2_models/cbgs_voxel01_voxelnext.yaml)        | 30.5 | [model-32M](https://drive.google.com/file/d/1YP2UOz-yO-cWfYQkIqILEu6bodvCBVrR/view?usp=share_link) |\n\n### Other datasets\nWelcome to support other datasets by submitting pull request. \n\n## Installation\n\nPlease refer to [INSTALL.md](docs/INSTALL.md) for the installation of `OpenPCDet`.\n\n\n## Quick Demo\nPlease refer to [DEMO.md](docs/DEMO.md) for a quick demo to test with a pretrained model and \nvisualize the predicted results on your custom data or the original KITTI data.\n\n## Getting Started\n\nPlease refer to [GETTING_STARTED.md](docs/GETTING_STARTED.md) to learn more usage about this project.\n\n\n## License\n\n`OpenPCDet` is released under the [Apache 2.0 license](LICENSE).\n\n## Acknowledgement\n`OpenPCDet` is an open source project for LiDAR-based 3D scene perception that supports multiple\nLiDAR-based perception models as shown above. Some parts of `PCDet` are learned from the official released codes of the above supported methods. \nWe would like to thank for their proposed methods and the official implementation.   \n\nWe hope that this repo could serve as a strong and flexible codebase to benefit the research community by speeding up the process of reimplementing previous works and/or developing new methods.\n\n\n## Citation \nIf you find this project useful in your research, please consider cite:\n\n\n```\n@misc{openpcdet2020,\n    title={OpenPCDet: An Open-source Toolbox for 3D Object Detection from Point Clouds},\n    author={OpenPCDet Development Team},\n    howpublished = {\\url{https://github.com/open-mmlab/OpenPCDet}},\n    year={2020}\n}\n```\n\n## Contribution\nWelcome to be a member of the OpenPCDet development team by contributing to this repo, and feel free to contact us for any potential contributions. \n\n\n"
        },
        {
          "name": "data",
          "type": "tree",
          "content": null
        },
        {
          "name": "docker",
          "type": "tree",
          "content": null
        },
        {
          "name": "docs",
          "type": "tree",
          "content": null
        },
        {
          "name": "pcdet",
          "type": "tree",
          "content": null
        },
        {
          "name": "requirements.txt",
          "type": "blob",
          "size": 0.1259765625,
          "content": "numpy\nllvmlite\nnumba\ntorch>=1.1\ntensorboardX\neasydict\npyyaml\nscikit-image\ntqdm\ntorchvision\nSharedArray\nopencv-python\npyquaternion"
        },
        {
          "name": "setup.py",
          "type": "blob",
          "size": 4.373046875,
          "content": "import os\nimport subprocess\n\nfrom setuptools import find_packages, setup\nfrom torch.utils.cpp_extension import BuildExtension, CUDAExtension\n\n\ndef get_git_commit_number():\n    if not os.path.exists('.git'):\n        return '0000000'\n\n    cmd_out = subprocess.run(['git', 'rev-parse', 'HEAD'], stdout=subprocess.PIPE)\n    git_commit_number = cmd_out.stdout.decode('utf-8')[:7]\n    return git_commit_number\n\n\ndef make_cuda_ext(name, module, sources):\n    cuda_ext = CUDAExtension(\n        name='%s.%s' % (module, name),\n        sources=[os.path.join(*module.split('.'), src) for src in sources]\n    )\n    return cuda_ext\n\n\ndef write_version_to_file(version, target_file):\n    with open(target_file, 'w') as f:\n        print('__version__ = \"%s\"' % version, file=f)\n\n\nif __name__ == '__main__':\n    version = '0.6.0+%s' % get_git_commit_number()\n    write_version_to_file(version, 'pcdet/version.py')\n\n    setup(\n        name='pcdet',\n        version=version,\n        description='OpenPCDet is a general codebase for 3D object detection from point cloud',\n        install_requires=[\n            'numpy',\n            'llvmlite',\n            'numba',\n            'tensorboardX',\n            'easydict',\n            'pyyaml',\n            'scikit-image',\n            'tqdm',\n            'SharedArray',\n            # 'spconv',  # spconv has different names depending on the cuda version\n        ],\n\n        author='Shaoshuai Shi',\n        author_email='shaoshuaics@gmail.com',\n        license='Apache License 2.0',\n        packages=find_packages(exclude=['tools', 'data', 'output']),\n        cmdclass={\n            'build_ext': BuildExtension,\n        },\n        ext_modules=[\n            make_cuda_ext(\n                name='iou3d_nms_cuda',\n                module='pcdet.ops.iou3d_nms',\n                sources=[\n                    'src/iou3d_cpu.cpp',\n                    'src/iou3d_nms_api.cpp',\n                    'src/iou3d_nms.cpp',\n                    'src/iou3d_nms_kernel.cu',\n                ]\n            ),\n            make_cuda_ext(\n                name='roiaware_pool3d_cuda',\n                module='pcdet.ops.roiaware_pool3d',\n                sources=[\n                    'src/roiaware_pool3d.cpp',\n                    'src/roiaware_pool3d_kernel.cu',\n                ]\n            ),\n            make_cuda_ext(\n                name='roipoint_pool3d_cuda',\n                module='pcdet.ops.roipoint_pool3d',\n                sources=[\n                    'src/roipoint_pool3d.cpp',\n                    'src/roipoint_pool3d_kernel.cu',\n                ]\n            ),\n            make_cuda_ext(\n                name='pointnet2_stack_cuda',\n                module='pcdet.ops.pointnet2.pointnet2_stack',\n                sources=[\n                    'src/pointnet2_api.cpp',\n                    'src/ball_query.cpp',\n                    'src/ball_query_gpu.cu',\n                    'src/group_points.cpp',\n                    'src/group_points_gpu.cu',\n                    'src/sampling.cpp',\n                    'src/sampling_gpu.cu', \n                    'src/interpolate.cpp', \n                    'src/interpolate_gpu.cu',\n                    'src/voxel_query.cpp', \n                    'src/voxel_query_gpu.cu',\n                    'src/vector_pool.cpp',\n                    'src/vector_pool_gpu.cu'\n                ],\n            ),\n            make_cuda_ext(\n                name='pointnet2_batch_cuda',\n                module='pcdet.ops.pointnet2.pointnet2_batch',\n                sources=[\n                    'src/pointnet2_api.cpp',\n                    'src/ball_query.cpp',\n                    'src/ball_query_gpu.cu',\n                    'src/group_points.cpp',\n                    'src/group_points_gpu.cu',\n                    'src/interpolate.cpp',\n                    'src/interpolate_gpu.cu',\n                    'src/sampling.cpp',\n                    'src/sampling_gpu.cu',\n\n                ],\n            ),\n            make_cuda_ext(\n                name=\"bev_pool_ext\",\n                module=\"pcdet.ops.bev_pool\",\n                sources=[\n                    \"src/bev_pool.cpp\",\n                    \"src/bev_pool_cuda.cu\",\n                ],\n            ),\n            make_cuda_ext(\n                name='ingroup_inds_cuda',\n                module='pcdet.ops.ingroup_inds',\n                sources=[\n                    'src/ingroup_inds.cpp',\n                    'src/ingroup_inds_kernel.cu',\n                ]\n            ),\n        ],\n    )\n"
        },
        {
          "name": "tools",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}