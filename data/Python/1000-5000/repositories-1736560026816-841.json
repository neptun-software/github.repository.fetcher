{
  "metadata": {
    "timestamp": 1736560026816,
    "page": 841,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjg1MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "facebookresearch/esm",
      "stars": 3368,
      "defaultBranch": "main",
      "files": [
        {
          "name": ".flake8",
          "type": "blob",
          "size": 0.13671875,
          "content": "[flake8]\nmax-line-length = 99\nignore = E203,W503\nexclude =\n    .git,\n    __pycache__,\n    build,\n    dist,\n    experimental\n    third_party\n"
        },
        {
          "name": ".git-blame-ignore-revs",
          "type": "blob",
          "size": 0.0693359375,
          "content": "# Migrate code style to Black\n8bc7e948cd9bf0b6d1f2113e221ef548ef663377\n"
        },
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.2626953125,
          "content": "# tensor dumps\n*.pt\n# Compiler Output #\n###################\n*.py[cod]\n*.so\n*.o\n*.exe\n*.class\n\n# Folders #\n###########\nbin/\nbuild/\ndist/\nlocal/\ntmp/\n__pycache__/\n*.egg-info/\n.idea/\n.ipynb_checkpoints/\n.vscode/\nesm/dev\n\n# Junk #\n########\n.DS_Store*\n.*.swp\n*.swp\n*.log\n*~\n"
        },
        {
          "name": "CODE_OF_CONDUCT.rst",
          "type": "blob",
          "size": 0.26171875,
          "content": "Code of Conduct\n===============\n\nFacebook has adopted a Code of Conduct that we expect project participants to adhere to. Please `read the full text`__ so that you can understand what actions will and will not be tolerated.\n\n__ https://code.facebook.com/codeofconduct\n"
        },
        {
          "name": "CONTRIBUTING.md",
          "type": "blob",
          "size": 1.2236328125,
          "content": "# Contributing to esm\nWe want to make contributing to this project as easy and transparent as\npossible.\n\n## Pull Requests\nWe actively welcome your pull requests.\n\n1. Fork the repo and create your branch from `master`.\n2. If you've added code that should be tested, add tests.\n3. If you've changed APIs, update the documentation.\n4. Ensure the test suite passes.\n5. Make sure your code lints.\n6. If you haven't already, complete the Contributor License Agreement (\"CLA\").\n\n## Contributor License Agreement (\"CLA\")\nIn order to accept your pull request, we need you to submit a CLA. You only need\nto do this once to work on any of Facebook's open source projects.\n\nComplete your CLA here: <https://code.facebook.com/cla>\n\n## Issues\nWe use GitHub issues to track public bugs. Please ensure your description is\nclear and has sufficient instructions to be able to reproduce the issue.\n\nFacebook has a [bounty program](https://www.facebook.com/whitehat/) for the safe\ndisclosure of security bugs. In those cases, please go through the process\noutlined on that page and do not file a public issue.\n\n## License\nBy contributing to icp-block-mdp, you agree that your contributions will be licensed\nunder the LICENSE file in the root directory of this source tree.\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 1.0625,
          "content": "MIT License\n\nCopyright (c) Meta Platforms, Inc. and affiliates.\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 45.234375,
          "content": "# Evolutionary Scale Modeling\n\n[![atlas](https://user-images.githubusercontent.com/3605224/199301187-a9e38b3f-71a7-44be-94f4-db0d66143c53.png)](https://esmatlas.com)\n\n***Update April 2023:*** Code for the two simultaneous preprints on protein design is now released! Code for \"Language models generalize beyond natural proteins\" is under [examples/lm-design/](examples/lm-design/). Code for \"A high-level programming language for generative protein design\" is under [examples/protein-programming-language/](examples/protein-programming-language/).\n\nThis repository contains code and pre-trained weights for **Transformer protein language models** from the Meta Fundamental AI Research Protein Team (FAIR), including our state-of-the-art [**ESM-2** and **ESMFold**](#esmfold), as well as [**MSA Transformer**](https://www.biorxiv.org/content/10.1101/2021.02.12.430858v1), [**ESM-1v**](#zs_variant) for predicting variant effects and [**ESM-IF1**](#invf) for inverse folding.\nTransformer protein language models were introduced in the [2019 preprint](https://doi.org/10.1101/622803) of the paper [\"Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences\"](https://doi.org/10.1073/pnas.2016239118).\nESM-2 outperforms all tested single-sequence protein language models across a range of structure prediction tasks.\nESMFold harnesses the ESM-2 language model to generate accurate structure predictions end to end directly from the sequence of a protein.\n\nIn November 2022, we released `v0` of the [ESM Metagenomic Atlas](https://esmatlas.com), an open atlas of 617 million predicted metagenomic protein structures.\nThe Atlas was updated in March 2023 in collaboration with EBI. The new `v2023_02` adds another 150 million predicted structures to the Atlas, as well as pre-computed ESM2 embeddings.\nBulk download, blog post and the resources provided on the Atlas website are documented [on this README](#atlas).\n\nIn December 2022, we released two simultaneous preprints on protein design.\n* \"Language models generalize beyond natural proteins\" ([PAPER](https://doi.org/10.1101/2022.12.21.521521), [CODE](examples/lm-design/)) uses ESM2 to design de novo proteins. The code and data associated with the preprint can be found [here](examples/lm-design/).\n* \"A high-level programming language for generative protein design\" ([PAPER](https://doi.org/10.1101/2022.12.21.521526), [CODE](examples/protein-programming-language/)) uses ESMFold to design proteins according to a high-level programming language.\n\n\n\n<details><summary><b>Citation</b></summary>\nFor ESM2, ESMFold and ESM Atlas:\n```bibtex\n@article{lin2023evolutionary,\ntitle = {Evolutionary-scale prediction of atomic-level protein structure with a language model},\nauthor = {Zeming Lin  and Halil Akin  and Roshan Rao  and Brian Hie  and Zhongkai Zhu  and Wenting Lu  and Nikita Smetanin  and Robert Verkuil  and Ori Kabeli  and Yaniv Shmueli  and Allan dos Santos Costa  and Maryam Fazel-Zarandi  and Tom Sercu  and Salvatore Candido  and Alexander Rives },\njournal = {Science},\nvolume = {379},\nnumber = {6637},\npages = {1123-1130},\nyear = {2023},\ndoi = {10.1126/science.ade2574},\nURL = {https://www.science.org/doi/abs/10.1126/science.ade2574},\nnote={Earlier versions as preprint: bioRxiv 2022.07.20.500902},\n}\n```\n\nFor transformer protein language models:\n```bibtex\n@article{rives2021biological,\n  title={Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences},\n  author={Rives, Alexander and Meier, Joshua and Sercu, Tom and Goyal, Siddharth and Lin, Zeming and Liu, Jason and Guo, Demi and Ott, Myle and Zitnick, C Lawrence and Ma, Jerry and others},\n  journal={Proceedings of the National Academy of Sciences},\n  volume={118},\n  number={15},\n  pages={e2016239118},\n  year={2021},\n  publisher={National Acad Sciences},\n  note={bioRxiv 10.1101/622803},\n  doi={10.1073/pnas.2016239118},\n  url={https://www.pnas.org/doi/full/10.1073/pnas.2016239118},\n}\n```\n</details>\n\n<details open><summary><b>Table of contents</b></summary>\n\n- [Main models you should use](#main-models)\n- [Usage](#usage)\n  - [Quick Start](#quickstart)\n  - [Getting Started with this repository](#repostart)\n  - [ESMFold Structure Prediction](#esmfold)\n  - [Compute embeddings in bulk from FASTA](#bulk_fasta)\n  - [CPU offloading for inference with large models](#fsdp)\n  - [Zero-shot variant prediction](#zs_variant)\n  - [Inverse folding](#invf)\n- [ESM Metagenomic Atlas](#atlas)\n- [Notebooks](#notebooks)\n- [Available Models and Datasets](#available)\n  - [Pre-trained Models](#available-models)\n  - [ESM Structural Split Dataset](#available-esmssd)\n  - [Pre-training Dataset Split](#available-pretraining-split)\n  - [Comparison to related works](#perf_related)\n- [Citations](#citations)\n- [License](#license)\n</details>\n\n<details><summary><b>What's New</b></summary>\n\n- April 2023: Code for the protein design preprints released under [examples/lm-design/](examples/lm-design/).\n- March 2023: We release an update to the ESM Metagenomic Atlas, `v2023_02`. See [website](https://esmatlas.com/) and [bulk download details](#atlas).\n- December 2022: The Meta Fundamental AI Research Protein Team (FAIR) released two simultaneous preprints on protein design:\n[\"Language models generalize beyond natural proteins\" (Verkuil, Kabeli, et al., 2022)](https://doi.org/10.1101/2022.12.21.521521), and [\"A high-level programming language for generative protein design\" (Hie, Candido, et al., 2022)](https://doi.org/10.1101/2022.12.21.521521).\n- November 2022: ESM Metagenomic Atlas, a repository of 600M+ metagenomics structures released, see [website](https://esmatlas.com/) and [bulk download details](#atlas)\n- November 2022: ESMFold - new end-to-end structure prediction model released (see [Lin et al. 2022](https://www.science.org/doi/abs/10.1126/science.ade2574))\n- August 2022: ESM-2 - new SOTA Language Models released (see [Lin et al. 2022](https://www.science.org/doi/abs/10.1126/science.ade2574))\n- April 2022: New inverse folding model ESM-IF1 released, trained on CATH and UniRef50 predicted structures.\n- August 2021: Added flexibility to tokenizer to allow for spaces and special tokens (like `<mask>`) in sequence.\n- July 2021: New pre-trained model ESM-1v released, trained on UniRef90 (see [Meier et al. 2021](https://doi.org/10.1101/2021.07.09.450648)).\n- July 2021: New MSA Transformer released, with a minor fix in the row positional embeddings (`ESM-MSA-1b`).\n- Feb 2021: MSA Transformer added (see [Rao et al. 2021](https://www.biorxiv.org/content/10.1101/2021.02.12.430858v1)). Example usage in [notebook](#notebooks).\n- Dec 2020: [Self-Attention Contacts](#notebooks) for all pre-trained models (see [Rao et al. 2020](https://doi.org/10.1101/2020.12.15.422761))\n- Dec 2020: Added new pre-trained model [ESM-1b](#perf_related) (see [Rives et al. 2019](https://doi.org/10.1101/622803) Appendix B)\n- Dec 2020: [ESM Structural Split Dataset](#available-esmssd) (see [Rives et al. 2019](https://doi.org/10.1101/622803) Appendix A.10)\n\n</details>\n\n## Main models you should use <a name=\"main-models\"></a>\n\n| Shorthand | `esm.pretrained.`           | Dataset | Description  |\n|-----------|-----------------------------|---------|--------------|\n| ESM-2    | `esm2_t36_3B_UR50D()` `esm2_t48_15B_UR50D()`       | UR50 (sample UR90)  | SOTA general-purpose protein language model. Can be used to predict structure, function and other protein properties directly from individual sequences. Released with [Lin et al. 2022](https://www.science.org/doi/abs/10.1126/science.ade2574) (Aug 2022 update). |\n| ESMFold   | `esmfold_v1()`         | PDB + UR50 | End-to-end single sequence 3D structure predictor (Nov 2022 update). |\n| ESM-MSA-1b| `esm_msa1b_t12_100M_UR50S()` |  UR50 + MSA  | MSA Transformer language model. Can be used to extract embeddings from an MSA. Enables SOTA inference of structure. Released with [Rao et al. 2021](https://www.biorxiv.org/content/10.1101/2021.02.12.430858v2) (ICML'21 version, June 2021).  |\n| ESM-1v    | `esm1v_t33_650M_UR90S_1()` ... `esm1v_t33_650M_UR90S_5()`| UR90  | Language model specialized for prediction of variant effects. Enables SOTA zero-shot prediction of the functional effects of sequence variations. Same architecture as ESM-1b, but trained on UniRef90. Released with [Meier et al. 2021](https://doi.org/10.1101/2021.07.09.450648). |\n| ESM-IF1  | `esm_if1_gvp4_t16_142M_UR50()` | CATH + UR50 | Inverse folding model. Can be used to design sequences for given structures, or to predict functional effects of sequence variation for given structures. Enables SOTA fixed backbone sequence design. Released with [Hsu et al. 2022](https://doi.org/10.1101/2022.04.10.487779). |\n\nFor a complete list of available models, with details and release notes, see [Pre-trained Models](#available-models).\n\n\n## Usage <a name=\"usage\"></a>\n\n### Quick start <a name=\"quickstart\"></a>\n\nAn easy way to get started is to load ESM or ESMFold through the [HuggingFace transformers library](https://huggingface.co/docs/transformers/model_doc/esm),\nwhich has simplified the ESMFold dependencies and provides a standardized API and tools to work with state-of-the-art pretrained models.\n\nAlternatively, [ColabFold](https://colab.research.google.com/github/sokrypton/ColabFold/blob/main/ESMFold.ipynb) has integrated ESMFold so that you can \neasily run it directly in the browser on a Google Colab instance.\n\nWe also provide an API which you can access through curl or on [the ESM Metagenomic Atlas web page](https://esmatlas.com/resources?action=fold).\n```\ncurl -X POST --data \"KVFGRCELAAAMKRHGLDNYRGYSLGNWVCAAKFESNFNTQATNRNTDGSTDYGILQINSRWWCNDGRTPGSRNLCNIPCSALLSSDITASVNCAKKIVSDGNGMNAWVAWRNRCKGTDVQAWIRGCRL\" https://api.esmatlas.com/foldSequence/v1/pdb/\n```\n\nFor ESM-MSA-1b, ESM-IF1, or any of the other models you can use the original implementation from our repo directly via the instructions below.\n\n### Getting started with this repo <a name=\"repostart\"></a>\n\nAs a prerequisite, you must have PyTorch installed to use this repository.\n\nYou can use this one-liner for installation, using the latest release of esm:\n\n```bash\npip install fair-esm  # latest release, OR:\npip install git+https://github.com/facebookresearch/esm.git  # bleeding edge, current repo main branch\n```\n\nTo use the ESMFold model, make sure you start from an environment with python <= 3.9 and pytorch installed.\nThen add the `[esmfold]` option to your pip install, which will install the dependencies for OpenFold\nautomatically. Openfold installation requires `nvcc`.\n\n```bash\npip install \"fair-esm[esmfold]\"\n# OpenFold and its remaining dependency\npip install 'dllogger @ git+https://github.com/NVIDIA/dllogger.git'\npip install 'openfold @ git+https://github.com/aqlaboratory/openfold.git@4b41059694619831a7db195b7e0988fc4ff3a307'\n```\n\n**NOTE**: If openfold installation fails, please double check that `nvcc` is available and that a cuda-compatable version of PyTorch has been installed.\n\nAlternatively, we provide the `esmfold` conda environment, which can be built via `conda env create -f environment.yml`.\n\nWe also support PyTorch Hub, which removes the need to clone and/or install this repository yourself:\n\n```python\nimport torch\nmodel, alphabet = torch.hub.load(\"facebookresearch/esm:main\", \"esm2_t33_650M_UR50D\")\n```\n\nAfter pip install, you can load and use a pretrained model as follows:\n\n```python\nimport torch\nimport esm\n\n# Load ESM-2 model\nmodel, alphabet = esm.pretrained.esm2_t33_650M_UR50D()\nbatch_converter = alphabet.get_batch_converter()\nmodel.eval()  # disables dropout for deterministic results\n\n# Prepare data (first 2 sequences from ESMStructuralSplitDataset superfamily / 4)\ndata = [\n    (\"protein1\", \"MKTVRQERLKSIVRILERSKEPVSGAQLAEELSVSRQVIVQDIAYLRSLGYNIVATPRGYVLAGG\"),\n    (\"protein2\", \"KALTARQQEVFDLIRDHISQTGMPPTRAEIAQRLGFRSPNAAEEHLKALARKGVIEIVSGASRGIRLLQEE\"),\n    (\"protein2 with mask\",\"KALTARQQEVFDLIRD<mask>ISQTGMPPTRAEIAQRLGFRSPNAAEEHLKALARKGVIEIVSGASRGIRLLQEE\"),\n    (\"protein3\",  \"K A <mask> I S Q\"),\n]\nbatch_labels, batch_strs, batch_tokens = batch_converter(data)\nbatch_lens = (batch_tokens != alphabet.padding_idx).sum(1)\n\n# Extract per-residue representations (on CPU)\nwith torch.no_grad():\n    results = model(batch_tokens, repr_layers=[33], return_contacts=True)\ntoken_representations = results[\"representations\"][33]\n\n# Generate per-sequence representations via averaging\n# NOTE: token 0 is always a beginning-of-sequence token, so the first residue is token 1.\nsequence_representations = []\nfor i, tokens_len in enumerate(batch_lens):\n    sequence_representations.append(token_representations[i, 1 : tokens_len - 1].mean(0))\n\n# Look at the unsupervised self-attention map contact predictions\nimport matplotlib.pyplot as plt\nfor (_, seq), tokens_len, attention_contacts in zip(data, batch_lens, results[\"contacts\"]):\n    plt.matshow(attention_contacts[: tokens_len, : tokens_len])\n    plt.title(seq)\n    plt.show()\n```\n\n\n### ESMFold Structure Prediction <a name=\"esmfold\"></a>\n\nAfter installing with the `[esmfold]` option, you can use the ESMFold structure prediction model as follows:\n\n```python\nimport torch\nimport esm\n\nmodel = esm.pretrained.esmfold_v1()\nmodel = model.eval().cuda()\n\n# Optionally, uncomment to set a chunk size for axial attention. This can help reduce memory.\n# Lower sizes will have lower memory requirements at the cost of increased speed.\n# model.set_chunk_size(128)\n\nsequence = \"MKTVRQERLKSIVRILERSKEPVSGAQLAEELSVSRQVIVQDIAYLRSLGYNIVATPRGYVLAGG\"\n# Multimer prediction can be done with chains separated by ':'\n\nwith torch.no_grad():\n    output = model.infer_pdb(sequence)\n\nwith open(\"result.pdb\", \"w\") as f:\n    f.write(output)\n\nimport biotite.structure.io as bsio\nstruct = bsio.load_structure(\"result.pdb\", extra_fields=[\"b_factor\"])\nprint(struct.b_factor.mean())  # this will be the pLDDT\n# 88.3\n```\n\n\nBesides `esm.pretrained.esmfold_v1()` which is the best performing model we recommend using, we\nalso provide `esm.pretrained.esmfold_v0()` which was used for the experiments in\n[Lin et al. 2022](https://www.science.org/doi/abs/10.1126/science.ade2574).\n\nWe also provide a command line interface (`esm-fold`) that efficiently predicts structures in bulk from a FASTA file using ESMFold:\n```\nusage: esm-fold [-h] -i FASTA -o PDB [--num-recycles NUM_RECYCLES]\n                [--max-tokens-per-batch MAX_TOKENS_PER_BATCH]\n                [--chunk-size CHUNK_SIZE] [--cpu-only] [--cpu-offload]\n\noptional arguments:\n  -h, --help            show this help message and exit\n  -i FASTA, --fasta FASTA\n                        Path to input FASTA file\n  -o PDB, --pdb PDB     Path to output PDB directory\n  --num-recycles NUM_RECYCLES\n                        Number of recycles to run. Defaults to number used in\n                        training (4).\n  --max-tokens-per-batch MAX_TOKENS_PER_BATCH\n                        Maximum number of tokens per gpu forward-pass. This\n                        will group shorter sequences together for batched\n                        prediction. Lowering this can help with out of memory\n                        issues, if these occur on short sequences.\n  --chunk-size CHUNK_SIZE\n                        Chunks axial attention computation to reduce memory\n                        usage from O(L^2) to O(L). Equivalent to running a for\n                        loop over chunks of of each dimension. Lower values\n                        will result in lower memory usage at the cost of\n                        speed. Recommended values: 128, 64, 32. Default: None.\n  --cpu-only            CPU only\n  --cpu-offload         Enable CPU offloading\n```\n\nThe command will make one prediction for every sequence in the fasta file. Multimers can be predicted and should be entered in the fasta file as a single sequence, with chains seprated by a \":\" character.\n\nBy default, predictions will be batched together so that shorter sequences are predicted simultaneously. This can be disabled by setting `--max-tokens-per-batch=0`. Batching can significantly improve prediction speed on shorter sequences.\n\nThe `--cpu-offload` flag can be useful for making predictions on longer sequences. It will attempt to offload some parameters to the CPU RAM, rather than storing on GPU.\n\nFinally, the ablation experiments for LMs of varying sizes [Lin et al. 2022 table S1](https://www.science.org/doi/abs/10.1126/science.ade2574) are released as `esm.pretrained.esmfold_structure_module_only_*()`. We don't recommend using these models for structure prediction.\n\n\n### Compute embeddings in bulk from FASTA <a name=\"bulk_fasta\"></a>\n\nWe provide a command line interface (`esm-extract`) that efficiently extracts embeddings in bulk for a FASTA file from the ESM:\n```\nusage: esm-extract [-h] [--toks_per_batch TOKS_PER_BATCH]\n                   [--repr_layers REPR_LAYERS [REPR_LAYERS ...]] --include\n                   {mean,per_tok,bos,contacts}\n                   [{mean,per_tok,bos,contacts} ...]\n                   [--truncation_seq_length TRUNCATION_SEQ_LENGTH]\n                   model_location fasta_file output_dir\n\nExtract per-token representations and model outputs for sequences in a FASTA\nfile\n\npositional arguments:\n  model_location        PyTorch model file OR name of pretrained model to\n                        download (see README for models)\n  fasta_file            FASTA file on which to extract representations\n  output_dir            output directory for extracted representations\n\noptional arguments:\n  -h, --help            show this help message and exit\n  --toks_per_batch TOKS_PER_BATCH\n                        maximum batch size\n  --repr_layers REPR_LAYERS [REPR_LAYERS ...]\n                        layers indices from which to extract representations\n                        (0 to num_layers, inclusive)\n  --include {mean,per_tok,bos,contacts} [{mean,per_tok,bos,contacts} ...]\n                        specify which representations to return\n  --truncation_seq_length TRUNCATION_SEQ_LENGTH\n                        truncate sequences longer than the given value\n```\n\nThe following commands allow the extraction of the final-layer embedding for a FASTA file from the ESM-2 model:\n\n```bash\nesm-extract esm2_t33_650M_UR50D examples/data/some_proteins.fasta \\\n  examples/data/some_proteins_emb_esm2 --repr_layers 0 32 33 --include\n```\n```bash\npython scripts/extract.py esm2_t33_650M_UR50D examples/data/some_proteins.fasta \\\n  examples/data/some_proteins_emb_esm2 --repr_layers 0 32 33 --include mean per_tok\n```\n\nA cuda device is optional and will be auto-detected.\n\nDirectory `some_proteins_emb_esm2/` now contains one `.pt` file per FASTA sequence; use `torch.load()` to load them.\n`scripts/extract.py` has flags that determine what's included in the `.pt` file:\n* `--repr-layers` (default: final only) selects which layers to include embeddings from.\n* `--include` specifies what embeddings to save. You can use the following:\n  * `per_tok` includes the full sequence, with an embedding per amino acid (seq_len x hidden_dim).\n  * `mean` includes the embeddings averaged over the full sequence, per layer.\n  * `bos` includes the embeddings from the beginning-of-sequence token.\n  (NOTE: Don't use with the pre-trained models - we trained without bos-token supervision)\n\n\n### CPU offloading for inference with large models <a name=\"fsdp\"></a>\nIf you want to load very large models like 15B and/or do inference on long sequences on your machine, regular GPU inference may lead to OOM errors.\nWe show how to load the model with Fairscale's [Fully Sharded Data Parallel (FSDP)](https://fairscale.readthedocs.io/en/stable/api/nn/fsdp.html) and\nuse its CPU offloading feature.\nThis allows to do inference of large models on a single GPU.\nPlease check out `examples/esm2_infer_fairscale_fsdp_cpu_offloading.py` for more details.\n\n### Zero-shot variant prediction <a name=\"zs_variant\"></a>\nSee \"[examples/variant-prediction/](examples/variant-prediction/)\" for code and pre-trained weights for the ESM-1v models described in\n[Language models enable zero-shot prediction of the effects of mutations on protein function. (Meier et al. 2021)](https://doi.org/10.1101/2021.07.09.450648).\n\nNote that ESM-2 could be used for variant prediction as well, and is expected to have similar performance to ESM-1v.\n\n### Inverse folding <a name=\"invf\"></a>\nSee \"[examples/inverse_folding/](examples/inverse_folding/)\" for detailed user guide. The ESM-IF1 model is described as `GVPTransformer` in [Learning inverse folding from millions of predicted structures. (Hsu et al. 2022)](https://doi.org/10.1101/2022.04.10.487779).\n\nWe also provide a colab notebook for the sequence design and sequence scoring functionalities.\n\n[<img src=\"https://colab.research.google.com/assets/colab-badge.svg\">](https://colab.research.google.com/github/facebookresearch/esm/blob/main/examples/inverse_folding/notebook_multichain.ipynb)\n\nThe ESM-IF1 inverse folding model is built for predicting protein sequences\nfrom their backbone atom coordinates. We provide scripts here 1) to sample sequence\ndesigns for a given structure and 2) to score sequences for a given structure.\n\nTrained with 12M protein structures predicted by AlphaFold2, the ESM-IF1\nmodel consists of invariant geometric input processing layers followed by a\nsequence-to-sequence transformer, and achieves 51% native sequence recovery on\nstructurally held-out backbones with 72% recovery for buried residues.\nThe model is also trained with span masking to tolerate missing backbone\ncoordinates and therefore can predict sequences for partially masked structures.\n\n#### Sample sequence designs for a given structure\nThe environment setup is described in [this subsection of examples/inverse_folding](examples/inverse_folding#recommended-environment).\n\nTo sample sequences for a given structure in PDB or mmCIF format, use the\n`sample_sequences.py` script. The input file can have either `.pdb` or\n`.cif` as suffix.\n\nFor example, to sample 3 sequence designs for the golgi casein kinase structure\n(PDB [5YH2](https://www.rcsb.org/structure/5yh2); [PDB Molecule of the Month\nfrom January 2022](https://pdb101.rcsb.org/motm/265)), we can run the following\ncommand from the esm root directory:\n```bash\npython examples/inverse_folding/sample_sequences.py examples/inverse_folding/data/5YH2.pdb \\\n  --chain C --temperature 1 --num-samples 3 --outpath examples/inverse_folding/output/sampled_sequences.fasta\n```\n\nThe sampled sequences will be saved in a fasta format to the specified output file.\n\nThe temperature parameter controls the sharpness of the probability\ndistribution for sequence sampling. Higher sampling temperatures yield more\ndiverse sequences but likely with lower native sequence recovery.\nThe default sampling temperature is 1. To optimize for native sequence\nrecovery, we recommend sampling with low temperature such as 1e-6.\n\n#### Scoring sequences\nTo score the conditional log-likelihoods for sequences conditioned on a given\nstructure, use the `score_log_likelihoods.py` script.\n\nFor example, to score the sequences in `examples/inverse_folding/data/5YH2_mutated_seqs.fasta`\naccording to the structure in `examples/inverse_folding/data/5YH2.pdb`, we can run\nthe following command from the esm root directory:\n```\npython examples/inverse_folding/score_log_likelihoods.py examples/inverse_folding/data/5YH2.pdb \\\n  examples/inverse_folding/data/5YH2_mutated_seqs.fasta --chain C \\\n  --outpath examples/inverse_folding/output/5YH2_mutated_seqs_scores.csv\n```\n\nThe conditional log-likelihoods are saved in a csv format in the specified output path.\nThe output values are the average log-likelihoods averaged over all amino acids in a sequence.\n\nFor more information, see \"[./examples/inverse_folding/](examples/inverse_folding/)\" for detailed user guide.\n\n## ESM Metagenomic Atlas <a name=\"atlas\"></a>\n\nPlease visit the [ESM Metagenomic Atlas](https://esmatlas.com/) website, and\nsee our [blog post](https://ai.facebook.com/blog/protein-folding-esmfold-metagenomics/) to learn more.\n\nBulk download instructions available at a seperate README [here](scripts/atlas/README.md).\n\nThe Atlas resources include a page to [fold a sequence using ESMFold](https://esmatlas.com/resources?action=fold),\nsearching a subset of the ESM Atlas by [structure](https://esmatlas.com/resources?action=search_structure) or \n[sequence](https://esmatlas.com/resources?action=search_sequence),\nas well as an [API](https://esmatlas.com/about#api) to access those resources programmatically.\n\nFoldseek provides search against the Atlas without the length limitation [here](https://search.foldseek.com/search).\n\n\n## Notebooks <a name=\"notebooks\"></a>\n\n### Inverse folding - predicting or scoring sequences based on backbone structures\n\n[<img src=\"https://colab.research.google.com/assets/colab-badge.svg\">](https://colab.research.google.com/github/facebookresearch/esm/blob/main/examples/inverse_folding/notebook.ipynb)\n\nThe ESM-IF1 inverse folding model predicts protein sequences from their backbone atom coordinates, trained with 12M protein structures predicted by AlphaFold2.\nThis notetook guide you through examples of sampling sequences, calculating conditional log-likelihoods, and extracting encoder output as structure representation.\n\n### Supervised variant prediction - training a classifier on the embeddings\n\n[<img src=\"https://colab.research.google.com/assets/colab-badge.svg\">](https://colab.research.google.com/github/facebookresearch/esm/blob/main/examples/sup_variant_prediction.ipynb)\n\n\nTo help you get started with using the embeddings, this [jupyter notebook tutorial](examples/sup_variant_prediction.ipynb) shows how to train a supervised variant predictor using embeddings from ESM-1.\nYou can adopt a similar protocol to train a model for any downstream task, even with limited data.\nFirst you can obtain the embeddings for ``examples/data/P62593.fasta`` either by [downloading the precomputed](https://dl.fbaipublicfiles.com/fair-esm/examples/P62593_reprs.tar.gz) embeddings\nas instructed in the notebook or by running the following:\n\n```bash\n# Obtain the embeddings\npython scripts/extract.py esm1v_t33_650M_UR90S_1 examples/data/P62593.fasta \\\n  examples/data/P62593_emb_esm1v --repr_layers 33 --include mean\n```\n\nThen, follow the remaining instructions in the tutorial. You can also run the tutorial in a [colab notebook](https://colab.research.google.com/github/facebookresearch/esm/blob/main/examples/sup_variant_prediction.ipynb).\n\n**Note, alternatively use [the newer instructions for zero-shot variant prediction](examples/variant-prediction/),\nwhich predicts mutational effects without any supervised training.**\n\n\n### Unsupervised contact prediction\n[<img src=\"https://colab.research.google.com/assets/colab-badge.svg\">](https://colab.research.google.com/github/facebookresearch/esm/blob/main/examples/contact_prediction.ipynb)\n\nThis [jupyter notebook tutorial](examples/contact_prediction.ipynb) demonstrates contact prediction with both the ESM-2 and MSA Transformer (ESM-MSA-1) models.\nContact prediction is based on a logistic regression over the model's attention maps.\nThis methodology is based on our ICLR 2021 paper,\n[Transformer protein language models are unsupervised structure learners. (Rao et al. 2020)](https://doi.org/10.1101/2020.12.15.422761)\nThe MSA Transformer (ESM-MSA-1) takes a multiple sequence alignment (MSA) as input, and uses the tied row self-attention maps in the same way.\nSee [MSA Transformer. (Rao et al. 2021)](https://www.biorxiv.org/content/10.1101/2021.02.12.430858v1).\n\nTo get unsupervised attention-based contacts, call `model.predict_contacts(tokens)` or `model(tokens, return_contacts=True)`.\n\n\n### ESMStructuralSplitDataset and self-attention contact prediction\n[<img src=\"https://colab.research.google.com/assets/colab-badge.svg\">](https://colab.research.google.com/github/facebookresearch/esm/blob/main/examples/esm_structural_dataset.ipynb)\n\nAnd this [jupyter notebook tutorial](examples/esm_structural_dataset.ipynb) shows how to load and index the `ESMStructuralSplitDataset`,\nand computes the self-attention map unsupervised contact predictions using ESM-2.\n\n\n## Available Models and Datasets <a name=\"available\"></a>\n\n### Pre-trained Models <a name=\"available-models\"></a>\n\n| Shorthand | `esm.pretrained.`           | #layers | #params | Dataset | Embedding Dim |  Model URL (automatically downloaded to `~/.cache/torch/hub/checkpoints`) |\n|-----------|---------------------|---------|-------------|---------|---------------|-----------------------------------------------------------------------|\n| ESM-2     | `esm2_t48_15B_UR50D`         | 48           | 15B         | UR50/D 2021_04                           | 5120 |  https://dl.fbaipublicfiles.com/fair-esm/models/esm2_t48_15B_UR50D.pt          |\n|           | `esm2_t36_3B_UR50D`          | 36           | 3B          | UR50/D 2021_04                           | 2560 |  https://dl.fbaipublicfiles.com/fair-esm/models/esm2_t36_3B_UR50D.pt           |\n|           | `esm2_t33_650M_UR50D`        | 33           | 650M        | UR50/D 2021_04                           | 1280 |  https://dl.fbaipublicfiles.com/fair-esm/models/esm2_t33_650M_UR50D.pt         |\n|           | `esm2_t30_150M_UR50D`        | 30           | 150M        | UR50/D 2021_04                           | 640  |  https://dl.fbaipublicfiles.com/fair-esm/models/esm2_t30_150M_UR50D.pt         |\n|           | `esm2_t12_35M_UR50D`         | 12           | 35M         | UR50/D 2021_04                           | 480  |  https://dl.fbaipublicfiles.com/fair-esm/models/esm2_t12_35M_UR50D.pt          |\n|           | `esm2_t6_8M_UR50D`           | 6            | 8M          | UR50/D 2021_04                           | 320  |  https://dl.fbaipublicfiles.com/fair-esm/models/esm2_t6_8M_UR50D.pt            |\n| ESMFold   | `esmfold_v1`                 | 48 (+36)     | 690M (+3B)  | UR50/D 2021_04                           | -    |  https://dl.fbaipublicfiles.com/fair-esm/models/esmfold_3B_v1.pt               |\n|           | `esmfold_v0`                 | 48 (+36)     | 690M (+3B)  | UR50/D 2021_04                           | -    |  https://dl.fbaipublicfiles.com/fair-esm/models/esmfold_3B_v0.pt               |\n|           | `esmfold_structure_module_only_*`              | 0 (+various) | various     | UR50/D 2021_04                           | -    |  https://dl.fbaipublicfiles.com/fair-esm/models/esmfold_structure_module_only_*                  |\n| ESM-IF1   | `esm_if1_gvp4_t16_142M_UR50` | 20           | 124M        | CATH 4.3 + predicted structures for UR50 | 512  | https://dl.fbaipublicfiles.com/fair-esm/models/esm_if1_gvp4_t16_142M_UR50.pt   |\n| ESM-1v    | `esm1v_t33_650M_UR90S_[1-5]` | 33           | 650M        | UR90/S 2020_03                           | 1280 | https://dl.fbaipublicfiles.com/fair-esm/models/esm1v_t33_650M_UR90S_1.pt       |\n| ESM-MSA-1b| `esm_msa1b_t12_100M_UR50S`   | 12           | 100M        | UR50/S + MSA 2018_03                     | 768  | https://dl.fbaipublicfiles.com/fair-esm/models/esm_msa1b_t12_100M_UR50S.pt     |\n| ESM-MSA-1 | `esm_msa1_t12_100M_UR50S`    | 12           | 100M        | UR50/S + MSA 2018_03                     | 768  | https://dl.fbaipublicfiles.com/fair-esm/models/esm_msa1_t12_100M_UR50S.pt      |\n| ESM-1b    | `esm1b_t33_650M_UR50S`       | 33           | 650M        | UR50/S 2018_03                           | 1280 | https://dl.fbaipublicfiles.com/fair-esm/models/esm1b_t33_650M_UR50S.pt         |\n| ESM-1     | `esm1_t34_670M_UR50S`        | 34           | 670M        | UR50/S 2018_03                           | 1280 |  https://dl.fbaipublicfiles.com/fair-esm/models/esm1_t34_670M_UR50S.pt         |\n|           | `esm1_t34_670M_UR50D`        | 34           | 670M        | UR50/D 2018_03                           | 1280 |  https://dl.fbaipublicfiles.com/fair-esm/models/esm1_t34_670M_UR50D.pt         |\n|           | `esm1_t34_670M_UR100`        | 34           | 670M        | UR100 2018_03                            | 1280 |  https://dl.fbaipublicfiles.com/fair-esm/models/esm1_t34_670M_UR100.pt         |\n|           | `esm1_t12_85M_UR50S`         | 12           | 85M         | UR50/S 2018_03                           | 768  |  https://dl.fbaipublicfiles.com/fair-esm/models/esm1_t12_85M_UR50S.pt          |\n|           | `esm1_t6_43M_UR50S`          | 6            | 43M         | UR50/S 2018_03                           | 768  |  https://dl.fbaipublicfiles.com/fair-esm/models/esm1_t6_43M_UR50S.pt           |\n\n\nHere is a chronological list of the released models and the paper they were introduced in:\n\n| Shorthand  | Release Notes |\n|------------|---------------|\n| ESM-1      | Released with Rives et al. 2019 (Aug 2020 update). |\n| ESM-1b     | Released with Rives et al. 2019 (Dec 2020 update). See Appendix B. |\n| ESM-MSA-1  | Released with Rao et al. 2021 (Preprint v1). |\n| ESM-MSA-1b | Released with Rao et al. 2021 (ICML'21 version, June 2021). |\n| ESM-1v     | Released with Meier et al. 2021. |\n| ESM-IF1    | Released with Hsu et al. 2022. |\n| ESM-2      | Released with Lin et al. 2022. |\n\n### ESM Structural Split Dataset <a name=\"available-esmssd\"></a>\nThis is a five-fold cross validation dataset of protein domain structures that can be used to measure generalization of representations\nacross different levels of structural dissimilarity.\nThe dataset implements structural holdouts at the family, superfamily, and fold\nlevel. The SCOPe database is used to classify domains. Independently for each level of structural hold-out,\nthe domains are split into 5 equal sets, i.e. five sets of folds, superfamilies, or families. This ensures\nthat for each of the five partitions, structures having the same classification do not appear in both the\ntrain and test sets. For a given classification level each structure appears in a test set once, so that\nin the cross validation experiment each of the structures will be evaluated exactly once.\n\nThe dataset provides 3d coordinates, distance maps, and secondary structure labels.\nFor further details on the construction of the dataset\nsee [Rives et al. 2019](https://doi.org/10.1101/622803) Appendix A.10.\n\nThis [jupyter notebook tutorial](examples/esm_structural_dataset.ipynb) shows how to load and index the `ESMStructuralSplitDataset`.\n\n`ESMStructuralSplitDataset`, upon initializing, will download `splits` and `pkl`.\nWe also provide `msas` for each of the domains. The data can be directly downloaded below.\n\n| Name   | Description                                                                   | URL                                                                   |\n|--------|-------------------------------------------------------------------------------|-----------------------------------------------------------------------|\n| splits | train/valid splits                                                            | https://dl.fbaipublicfiles.com/fair-esm/structural-data/splits.tar.gz |\n| pkl    | pkl objects containing sequence, SSP labels, distance map, and 3d coordinates | https://dl.fbaipublicfiles.com/fair-esm/structural-data/pkl.tar.gz    |\n| msas   | a3m files containing MSA for each domain                                      | https://dl.fbaipublicfiles.com/fair-esm/structural-data/msas.tar.gz   |\n\n### Pre-training Dataset Split  <a name=\"available-pretraining-split\"></a>\nThe split files establishing which UniRef50 clusters were used as held-out evaluation set for pre-training\nin [Rives et al. 2019](https://doi.org/10.1101/622803) and [Rao et al. 2021](https://doi.org/10.1101/2021.02.12.430858) can be found here:\n* [UniRef50 IDs of evaluation set](https://dl.fbaipublicfiles.com/fair-esm/pretraining-data/uniref201803_ur50_valid_headers.txt.gz): 3.016 M clusters\n* [UniRef100 IDs of evaluation set](https://dl.fbaipublicfiles.com/fair-esm/pretraining-data/uniref201803_ur100_valid_headers.txt.gz): 13.745 M proteins, expanding the same UniRef50 clusters.\n\nThese files only contain only the UniRef50 IDs and UniRef100 IDs corresponding to the [UniRef database, 2018-03 release](https://ftp.uniprot.org/pub/databases/uniprot/previous_releases/release-2018_03/uniref/)\nwhich is released by the UniProt Consortium under a [Creative Commons Attribution (CC BY 4.0) License](https://www.uniprot.org/help/license).\n\n\n### Comparison to related works <a name=\"perf_related\"></a>\n<!--\nDO NOT EDIT THIS TABLE! This is the source of truth:\nhttps://docs.google.com/spreadsheets/d/1RPvWF47rIMEr-Jg-SRCoGElHcwCl5d7RyEeSyPgp59A/edit#gid=0\nexported via https://www.tablesgenerator.com/html_tables\n-->\n\n<table class=\"tg\">\n<thead>\n  <tr>\n    <th class=\"tg-0thz\"><span style=\"font-weight:bold\">Task</span></th>\n    <th class=\"tg-j6zm\" colspan=\"3\"><span style=\"font-weight:bold\">Unsupervised contact prediction</span></th>\n    <th class=\"tg-j6zm\" colspan=\"2\"><span style=\"font-weight:bold\">Structure Prediction</span></th>\n  </tr>\n</thead>\n<tbody>\n  <tr>\n    <td class=\"tg-j6zm\"><span style=\"font-weight:bold\">Test set</span></td>\n    <td class=\"tg-j6zm\"><span style=\"font-weight:bold\">Large valid</span></td>\n    <td class=\"tg-j6zm\"><span style=\"font-weight:bold\">CASP14</span></td>\n    <td class=\"tg-j6zm\"><span style=\"font-weight:bold\">CAMEO (Apr-Jun 2022)</span></td>\n    <td class=\"tg-j6zm\"><span style=\"font-weight:bold\">CASP14</span></td>\n    <td class=\"tg-j6zm\"><span style=\"font-weight:bold\">CAMEO (Apr-Jun 2022)</span></td>\n  </tr>\n  <tr>\n    <td class=\"tg-7zrl\">Gremlin (Potts)</td>\n    <td class=\"tg-7zrl\">39.3</td>\n    <td class=\"tg-7zrl\"></td>\n    <td class=\"tg-7zrl\"></td>\n    <td class=\"tg-7zrl\"></td>\n    <td class=\"tg-7zrl\"></td>\n  </tr>\n  <tr>\n    <td class=\"tg-7zrl\">TAPE</td>\n    <td class=\"tg-7zrl\">11.2</td>\n    <td class=\"tg-7zrl\"></td>\n    <td class=\"tg-7zrl\"></td>\n    <td class=\"tg-7zrl\"></td>\n    <td class=\"tg-7zrl\"></td>\n  </tr>\n  <tr>\n    <td class=\"tg-7zrl\">ProtBert-BFD</td>\n    <td class=\"tg-7zrl\">34.1</td>\n    <td class=\"tg-7zrl\"></td>\n    <td class=\"tg-7zrl\"></td>\n    <td class=\"tg-7zrl\"></td>\n    <td class=\"tg-7zrl\"></td>\n  </tr>\n  <tr>\n    <td class=\"tg-7zrl\">Prot-T5-XL-BFD</td>\n    <td class=\"tg-7zrl\">35.6</td>\n    <td class=\"tg-7zrl\"></td>\n    <td class=\"tg-7zrl\"></td>\n    <td class=\"tg-2b7s\">46.1</td>\n    <td class=\"tg-2b7s\">62.6</td>\n  </tr>\n  <tr>\n    <td class=\"tg-7zrl\">Prot-T5-XL-Ur50 (3B)</td>\n    <td class=\"tg-7zrl\">47.9</td>\n    <td class=\"tg-7zrl\"></td>\n    <td class=\"tg-7zrl\"></td>\n    <td class=\"tg-2b7s\">49.8</td>\n    <td class=\"tg-2b7s\">69.4</td>\n  </tr>\n  <tr>\n    <td class=\"tg-7zrl\">ESM-1</td>\n    <td class=\"tg-7zrl\">33.7</td>\n    <td class=\"tg-7zrl\"></td>\n    <td class=\"tg-7zrl\"></td>\n    <td class=\"tg-7zrl\"></td>\n    <td class=\"tg-7zrl\"></td>\n  </tr>\n  <tr>\n    <td class=\"tg-7zrl\">ESM-1b</td>\n    <td class=\"tg-7zrl\">41.1</td>\n    <td class=\"tg-7zrl\">24.4</td>\n    <td class=\"tg-7zrl\">39</td>\n    <td class=\"tg-2b7s\">41.6</td>\n    <td class=\"tg-2b7s\">64.5</td>\n  </tr>\n  <tr>\n    <td class=\"tg-7zrl\">ESM-1v</td>\n    <td class=\"tg-7zrl\">35.3</td>\n    <td class=\"tg-7zrl\"></td>\n    <td class=\"tg-7zrl\"></td>\n    <td class=\"tg-7zrl\"></td>\n    <td class=\"tg-7zrl\"></td>\n  </tr>\n  <tr>\n    <td class=\"tg-7zrl\">ESM-MSA-1b</td>\n    <td class=\"tg-7zrl\">57.4</td>\n    <td class=\"tg-7zrl\"></td>\n    <td class=\"tg-7zrl\"></td>\n    <td class=\"tg-7zrl\"></td>\n    <td class=\"tg-7zrl\"></td>\n  </tr>\n  <tr>\n    <td class=\"tg-7zrl\">ESM-2 (8M)</td>\n    <td class=\"tg-7zrl\">15.9</td>\n    <td class=\"tg-7zrl\">9.8</td>\n    <td class=\"tg-7zrl\">15.7</td>\n    <td class=\"tg-2b7s\">36.7</td>\n    <td class=\"tg-2b7s\">48.1</td>\n  </tr>\n  <tr>\n    <td class=\"tg-7zrl\">ESM-2 (35M)</td>\n    <td class=\"tg-7zrl\">28.8</td>\n    <td class=\"tg-7zrl\">16.4</td>\n    <td class=\"tg-7zrl\">28.4</td>\n    <td class=\"tg-2b7s\">41.4</td>\n    <td class=\"tg-2b7s\">56.4</td>\n  </tr>\n  <tr>\n    <td class=\"tg-7zrl\">ESM-2 (150M)</td>\n    <td class=\"tg-7zrl\">42.2</td>\n    <td class=\"tg-7zrl\">26.8</td>\n    <td class=\"tg-7zrl\">40.1</td>\n    <td class=\"tg-2b7s\">49.0</td>\n    <td class=\"tg-2b7s\">64.9</td>\n  </tr>\n  <tr>\n    <td class=\"tg-7zrl\">ESM-2 (700M)</td>\n    <td class=\"tg-7zrl\">50.1</td>\n    <td class=\"tg-7zrl\">32.5</td>\n    <td class=\"tg-7zrl\">47.6</td>\n    <td class=\"tg-2b7s\">51.3</td>\n    <td class=\"tg-2b7s\">70.1</td>\n  </tr>\n  <tr>\n    <td class=\"tg-7zrl\">ESM-2 (3B)</td>\n    <td class=\"tg-7zrl\">52.7</td>\n    <td class=\"tg-7zrl\">34.0</td>\n    <td class=\"tg-7zrl\">49.9</td>\n    <td class=\"tg-2b7s\">52.5</td>\n    <td class=\"tg-2b7s\">71.8</td>\n  </tr>\n  <tr>\n    <td class=\"tg-7zrl\">ESM-2 (15B)</td>\n    <td class=\"tg-7zrl\">54.5</td>\n    <td class=\"tg-7zrl\">37.0</td>\n    <td class=\"tg-7zrl\">51.7</td>\n    <td class=\"tg-2b7s\">55.4</td>\n    <td class=\"tg-2b7s\">72.1</td>\n  </tr>\n</tbody>\n</table>\n\nComparison to related protein language models on structure prediction tasks.\n\n* All contact numbers are the top-L,LR precision metric, where long range means sequence separation of at least 24 residues\n* For unsupervised contact prediction, a sparse linear combination of the attention heads is used to directly predict protein contacts,\nfitted with logistic regression on 20 structures.\nFor more details on the method, see [Rao et al. 2020](https://doi.org/10.1101/2020.12.15.422761).\n* For structure prediction, an AlphaFold2 structure module is trained directly from the frozen language model embeddings.\nFor more details on the method, see [Lin et al. 2022](https://www.science.org/doi/abs/10.1126/science.ade2574).\n* Direct coupling analysis methods (Gremlin, mfDCA, Psicov) and ESM-MSA-1 use the [trRosetta MSAs](https://yanglab.nankai.edu.cn/trRosetta/benchmark/), while other methods predict from single sequence.\n\n\n## Citations <a name=\"citations\"></a>\n\nIf you find the models useful in your research, we ask that you cite the relevant paper:\n\n```bibtex\n@article{rives2019biological,\n  author={Rives, Alexander and Meier, Joshua and Sercu, Tom and Goyal, Siddharth and Lin, Zeming and Liu, Jason and Guo, Demi and Ott, Myle and Zitnick, C. Lawrence and Ma, Jerry and Fergus, Rob},\n  title={Biological Structure and Function Emerge from Scaling Unsupervised Learning to 250 Million Protein Sequences},\n  year={2019},\n  doi={10.1101/622803},\n  url={https://www.biorxiv.org/content/10.1101/622803v4},\n  journal={PNAS}\n}\n```\n\nFor the self-attention contact prediction:\n\n```bibtex\n@article{rao2020transformer,\n  author = {Rao, Roshan M and Meier, Joshua and Sercu, Tom and Ovchinnikov, Sergey and Rives, Alexander},\n  title={Transformer protein language models are unsupervised structure learners},\n  year={2020},\n  doi={10.1101/2020.12.15.422761},\n  url={https://www.biorxiv.org/content/10.1101/2020.12.15.422761v1},\n  journal={bioRxiv}\n}\n```\n\nFor the MSA Transformer:\n\n```bibtex\n@article{rao2021msa,\n  author = {Rao, Roshan and Liu, Jason and Verkuil, Robert and Meier, Joshua and Canny, John F. and Abbeel, Pieter and Sercu, Tom and Rives, Alexander},\n  title={MSA Transformer},\n  year={2021},\n  doi={10.1101/2021.02.12.430858},\n  url={https://www.biorxiv.org/content/10.1101/2021.02.12.430858v1},\n  journal={bioRxiv}\n}\n```\n\nFor variant prediction using ESM-1v:\n\n```bibtex\n@article{meier2021language,\n  author = {Meier, Joshua and Rao, Roshan and Verkuil, Robert and Liu, Jason and Sercu, Tom and Rives, Alexander},\n  title = {Language models enable zero-shot prediction of the effects of mutations on protein function},\n  year={2021},\n  doi={10.1101/2021.07.09.450648},\n  url={https://www.biorxiv.org/content/10.1101/2021.07.09.450648v1},\n  journal={bioRxiv}\n}\n```\n\nFor inverse folding using ESM-IF1:\n\n```bibtex\n@article{hsu2022learning,\n\tauthor = {Hsu, Chloe and Verkuil, Robert and Liu, Jason and Lin, Zeming and Hie, Brian and Sercu, Tom and Lerer, Adam and Rives, Alexander},\n\ttitle = {Learning inverse folding from millions of predicted structures},\n\tyear = {2022},\n\tdoi = {10.1101/2022.04.10.487779},\n\turl = {https://www.biorxiv.org/content/early/2022/04/10/2022.04.10.487779},\n\tjournal = {ICML}\n}\n```\n\nFor the ESM-2 language model and ESMFold:\n\n```bibtex\n@article{lin2022language,\n  title={Language models of protein sequences at the scale of evolution enable accurate structure prediction},\n  author={Lin, Zeming and Akin, Halil and Rao, Roshan and Hie, Brian and Zhu, Zhongkai and Lu, Wenting and Smetanin, Nikita and dos Santos Costa, Allan and Fazel-Zarandi, Maryam and Sercu, Tom and Candido, Sal and others},\n  journal={bioRxiv},\n  year={2022},\n  publisher={Cold Spring Harbor Laboratory}\n}\n```\n\nMuch of this code builds on the [fairseq](https://github.com/pytorch/fairseq) sequence modeling framework. We use fairseq internally for our protein language modeling research. We highly recommend trying it out if you'd like to pre-train protein language models from scratch.\n\nAdditionally, if you would like to use the variant prediction benchmark from Meier et al. (2021), we provide a bibtex file with citations for all data in [./examples/variant-prediction/mutation_data.bib](./examples/variant-prediction/mutation_data.bib). You can cite each paper individually, or add all citations in bulk using the LaTeX command:\n\n```tex\n\\nocite{wrenbeck2017deep,klesmith2015comprehensive,haddox2018mapping,romero2015dissecting,firnberg2014comprehensive,deng2012deep,stiffler2015evolvability,jacquier2013capturing,findlay2018comprehensive,mclaughlin2012spatial,kitzman2015massively,doud2016accurate,pokusaeva2019experimental,mishra2016systematic,kelsic2016rna,melnikov2014comprehensive,brenan2016phenotypic,rockah2015systematic,wu2015functional,aakre2015evolving,qi2014quantitative,matreyek2018multiplex,bandaru2017deconstruction,roscoe2013analyses,roscoe2014systematic,mavor2016determination,chan2017correlation,melamed2013deep,starita2013activity,araya2012fundamental}\n```\n\n## License <a name=\"license\"></a>\n\nThis source code is licensed under the MIT license found in the `LICENSE` file\nin the root directory of this source tree.\n\nESM Metagenomic Atlas (also referred to as ESM Metagenomic Structure Atlas or ESM Atlas) data is available under a CC BY 4.0 license for academic and commercial use. Copyright (c) Meta Platforms, Inc. All Rights Reserved. Use of the ESM Metagenomic Atlas data is subject to the Meta Open Source [Terms of Use](https://opensource.fb.com/legal/terms/) and [Privacy Policy](https://opensource.fb.com/legal/privacy/).\n"
        },
        {
          "name": "environment.yml",
          "type": "blob",
          "size": 0.8671875,
          "content": "name: esmfold\nchannels:\n  - conda-forge\n  - bioconda\n  - pytorch\ndependencies:\n  - conda-forge::python=3.7\n  - conda-forge::setuptools=59.5.0\n  - conda-forge::pip\n  - conda-forge::openmm=7.5.1\n  - conda-forge::pdbfixer\n  - conda-forge::cudatoolkit==11.3.*\n  - conda-forge::einops\n  - conda-forge::fairscale\n  - conda-forge::omegaconf\n  - conda-forge::hydra-core\n  - conda-forge::pandas\n  - conda-forge::pytest\n  - bioconda::hmmer==3.3.2\n  - bioconda::hhsuite==3.3.0\n  - bioconda::kalign2==2.04\n  - pytorch::pytorch=1.12.*\n  - pip:\n      - biopython==1.79\n      - deepspeed==0.5.9\n      - dm-tree==0.1.6\n      - ml-collections==0.1.0\n      - numpy==1.21.2\n      - PyYAML==5.4.1\n      - requests==2.26.0\n      - scipy==1.7.1\n      - tqdm==4.62.2\n      - typing-extensions==3.10.0.2\n      - pytorch_lightning==1.5.10\n      - wandb==0.12.21\n      - git+https://github.com/NVIDIA/dllogger.git\n"
        },
        {
          "name": "esm",
          "type": "tree",
          "content": null
        },
        {
          "name": "examples",
          "type": "tree",
          "content": null
        },
        {
          "name": "hubconf.py",
          "type": "blob",
          "size": 1.2412109375,
          "content": "# Copyright (c) Meta Platforms, Inc. and affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\ndependencies = [\"torch\"]\n\nfrom esm.pretrained import (\n    esm1_t6_43M_UR50S,\n    esm1_t12_85M_UR50S,\n    esm1_t34_670M_UR50D,\n    esm1_t34_670M_UR50S,\n    esm1_t34_670M_UR100,\n    esm1b_t33_650M_UR50S,\n    esm1v_t33_650M_UR90S,\n    esm1v_t33_650M_UR90S_1,\n    esm1v_t33_650M_UR90S_2,\n    esm1v_t33_650M_UR90S_3,\n    esm1v_t33_650M_UR90S_4,\n    esm1v_t33_650M_UR90S_5,\n    esm2_t6_8M_UR50D,\n    esm2_t12_35M_UR50D,\n    esm2_t30_150M_UR50D,\n    esm2_t33_650M_UR50D,\n    esm2_t36_3B_UR50D,\n    esm2_t48_15B_UR50D,\n    esm_if1_gvp4_t16_142M_UR50,\n    esm_msa1_t12_100M_UR50S,\n    esm_msa1b_t12_100M_UR50S,\n    esmfold_structure_module_only_8M,\n    esmfold_structure_module_only_8M_270K,\n    esmfold_structure_module_only_35M,\n    esmfold_structure_module_only_35M_270K,\n    esmfold_structure_module_only_150M,\n    esmfold_structure_module_only_150M_270K,\n    esmfold_structure_module_only_650M,\n    esmfold_structure_module_only_650M_270K,\n    esmfold_structure_module_only_3B,\n    esmfold_structure_module_only_3B_270K,\n    esmfold_structure_module_only_15B,\n    esmfold_v0,\n    esmfold_v1,\n)\n"
        },
        {
          "name": "pyproject.toml",
          "type": "blob",
          "size": 0.12890625,
          "content": "[tool.black]\nline-length = 99\nexclude = '''\n/(\n    \\.git\n  | __pycache__\n  | build\n  | dist\n  | experimental\n  | third_party\n)/\n'''\n"
        },
        {
          "name": "scripts",
          "type": "tree",
          "content": null
        },
        {
          "name": "setup.py",
          "type": "blob",
          "size": 1.478515625,
          "content": "# Copyright (c) Meta Platforms, Inc. and affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nfrom setuptools import setup\n\n\nwith open(\"esm/version.py\") as infile:\n    exec(infile.read())\n\nwith open(\"README.md\") as f:\n    readme = f.read()\n\nextras = {\n    \"esmfold\": [ # OpenFold does not automatically pip install requirements, so we add them here.\n        \"biopython\",\n        \"deepspeed==0.5.9\",\n        \"dm-tree\",\n        \"pytorch-lightning\",\n        \"omegaconf\",\n        \"ml-collections\",\n        \"einops\",\n        \"scipy\",\n    ]\n}\n\nsources = {\n    \"esm\": \"esm\",\n    \"esm.model\": \"esm/model\",\n    \"esm.inverse_folding\": \"esm/inverse_folding\",\n    \"esm.esmfold.v1\": \"esm/esmfold/v1\",\n    \"esm.scripts\": \"scripts\"\n}\n\nsetup(\n    name=\"fair-esm\",\n    version=version,\n    description=\"Evolutionary Scale Modeling (esm): Pretrained language models for proteins. From Facebook AI Research.\",\n    long_description=readme,\n    long_description_content_type=\"text/markdown\",\n    author=\"Facebook AI Research\",\n    url=\"https://github.com/facebookresearch/esm\",\n    license=\"MIT\",\n    packages=sources.keys(),\n    package_dir=sources,\n    extras_require=extras,\n    data_files=[(\"source_docs/esm\", [\"LICENSE\", \"README.md\", \"CODE_OF_CONDUCT.rst\"])],\n    zip_safe=True,\n    entry_points={\n        \"console_scripts\": [\n            \"esm-extract=esm.scripts.extract:main\",\n            \"esm-fold=esm.scripts.fold:main\",\n        ]\n    },\n)\n"
        },
        {
          "name": "tests",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}