{
  "metadata": {
    "timestamp": 1736559468026,
    "page": 30,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjMw",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "minimaxir/textgenrnn",
      "stars": 4935,
      "defaultBranch": "master",
      "files": [
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.1865234375,
          "content": "/old\n__pycache__\n/build\n/dist\n/textgenrnn.egg-info\n/.ipynb_checkpoints\n/docs/.ipynb_checkpoints\n/docs/textgenrnn_texts.txt\n/docs/hn_weights.hdf5\n*.pyc\ntest.py\n*.hdf5\n*vocab.json\n*config.json\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 2.111328125,
          "content": "MIT License\n\nCopyright (c) 2017-2020 Max Woolf\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n\n---\n\nMIT License\n\nCopyright (c) 2017 Bjarke Felbo, Han Thi Nguyen\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n"
        },
        {
          "name": "MANIFEST.in",
          "type": "blob",
          "size": 0.10546875,
          "content": "include textgenrnn/textgenrnn_weights.hdf5\ninclude textgenrnn/textgenrnn_vocab.json\nglobal-exclude .DS_Store"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 11.98828125,
          "content": "# textgenrnn\n\n![dank text](/docs/textgenrnn_console.gif)\n\nEasily train your own text-generating neural network of any size and complexity on any text dataset with a few lines of code, or quickly train on a text using a pretrained model.\n\ntextgenrnn is a Python 3 module on top of [Keras](https://github.com/fchollet/keras)/[TensorFlow](https://www.tensorflow.org) for creating [char-rnn](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)s, with many cool features:\n\n* A modern neural network architecture which utilizes new techniques as attention-weighting and skip-embedding to accelerate training and improve model quality.\n* Train on and generate text at either the character-level or word-level.\n* Configure RNN size, the number of RNN layers, and whether to use bidirectional RNNs.\n* Train on any generic input text file, including large files.\n* Train models on a GPU and then use them to generate text with a CPU.\n* Utilize a powerful CuDNN implementation of RNNs when trained on the GPU, which massively speeds up training time as opposed to typical LSTM implementations.\n* Train the model using contextual labels, allowing it to learn faster and produce better results in some cases.\n\nYou can play with textgenrnn and train any text file with a GPU *for free* in this [Colaboratory Notebook](https://drive.google.com/file/d/1mMKGnVxirJnqDViH7BDJxFqWrsXlPSoK/view?usp=sharing)! Read [this blog post](http://minimaxir.com/2018/05/text-neural-networks/) or [watch this video](https://www.youtube.com/watch?v=RW7mP6BfZuY) for more information!\n\n## Examples\n\n```python\nfrom textgenrnn import textgenrnn\n\ntextgen = textgenrnn()\ntextgen.generate()\n```\n\n```text\n[Spoiler] Anyone else find this post and their person that was a little more than I really like the Star Wars in the fire or health and posting a personal house of the 2016 Letter for the game in a report of my backyard.\n```\n\nThe included model can easily be trained on new texts, and can generate appropriate text *even after a single pass of the input data*.\n\n```python\ntextgen.train_from_file('hacker_news_2000.txt', num_epochs=1)\ntextgen.generate()\n```\n\n```text\nProject State Project Firefox\n```\n\nThe model weights are relatively small (2 MB on disk), and they can easily be saved and loaded into a new textgenrnn instance. As a result, you can play with models which have been trained on hundreds of passes through the data. (in fact, textgenrnn learns *so well* that you have to increase the temperature significantly for creative output!)\n\n```python\ntextgen_2 = textgenrnn('/weights/hacker_news.hdf5')\ntextgen_2.generate(3, temperature=1.0)\n```\n\n```text\nWhy we got money “regular alter”\n\nUrburg to Firefox acquires Nelf Multi Shamn\n\nKubernetes by Google’s Bern\n```\n\nYou can also train a new model, with support for word level embeddings and bidirectional RNN layers by adding `new_model=True` to any train function.\n\n## Interactive Mode\n\nIt's also possible to get involved in how the output unfolds, step by step. Interactive mode will suggest you the *top N* options for the next char/word, and allows you to pick one.  \n  \nWhen running textgenrnn in the terminal, pass `interactive=True` and `top=N` to `generate`. N defaults to 3.\n\n```python\nfrom textgenrnn import textgenrnn\n\ntextgen = textgenrnn()\ntextgen.generate(interactive=True, top_n=5)\n```\n\n![word_level_demo](/docs/textgenrnn_interactive.gif)\n  \nThis can add a *human touch* to the output; it feels like you're the writer! ([reference](https://fivethirtyeight.com/features/some-like-it-bot/))\n  \n## Usage\n\ntextgenrnn can be installed [from pypi](https://pypi.python.org/pypi/textgenrnn) via `pip`:\n\n```sh\npip3 install textgenrnn\n```\n\nFor the latest textgenrnn, *you must have a minimum TensorFlow version of 2.1.0*.\n\nYou can view a demo of common features and model configuration options in [this Jupyter Notebook](/docs/textgenrnn-demo.ipynb).\n\n`/datasets` contains example datasets using Hacker News/Reddit data for training textgenrnn.\n\n`/weights` contains further-pretrained models on the aforementioned datasets which can be loaded into textgenrnn.\n\n`/outputs` contains examples of text generated from the above pretrained models.\n\n## Neural Network Architecture and Implementation\n\ntextgenrnn is based off of the [char-rnn](https://github.com/karpathy/char-rnn) project by [Andrej Karpathy](https://twitter.com/karpathy) with a few modern optimizations, such as the ability to work with very small text sequences.\n\n![default model](/docs/default_model.png)\n\nThe included pretrained-model follows a [neural network architecture](https://github.com/bfelbo/DeepMoji/blob/master/deepmoji/model_def.py) inspired by [DeepMoji](https://github.com/bfelbo/DeepMoji). For the default model, textgenrnn takes in an input of up to 40 characters, converts each character to a 100-D character embedding vector, and feeds those into a 128-cell long-short-term-memory (LSTM) recurrent layer. Those outputs are then fed into *another* 128-cell LSTM. All three layers are then fed into an Attention layer to weight the most important temporal features and average them together (and since the embeddings + 1st LSTM are skip-connected into the attention layer, the model updates can backpropagate to them more easily and prevent vanishing gradients). That output is mapped to probabilities for up to [394 different characters](/textgenrnn/textgenrnn_vocab.json) that they are the next character in the sequence, including uppercase characters, lowercase, punctuation, and emoji. (if training a new model on a new dataset, all of the numeric parameters above can be configured)\n\n![context model](/docs/context_model.png)\n\nAlternatively, if context labels are provided with each text document, the model can be trained in a contextual mode, where the model learns the text *given the context* so the recurrent layers learn the *decontextualized* language. The text-only path can piggy-back off the decontextualized layers; in all, this results in much faster training and better quantitative and qualitative model performance than just training the model gien the text alone.\n\nThe model weights included with the package are trained on hundreds of thousands of text documents from Reddit submissions ([via BigQuery](http://minimaxir.com/2015/10/reddit-bigquery/)), from a very *diverse* variety of subreddits. The network was also trained using the decontextual approach noted above in order to both improve training performance and mitigate authorial bias.\n\nWhen fine-tuning the model on a new dataset of texts using textgenrnn, all layers are retrained. However, since the original pretrained network has a much more robust \"knowledge\" initially, the new textgenrnn trains faster and more accurately in the end, and can potentially learn new relationships not present in the original dataset (e.g. the [pretrained character embeddings](http://minimaxir.com/2017/04/char-embeddings/) include the context for the character for all possible types of modern internet grammar).\n\nAdditionally, the retraining is done with a momentum-based optimizer and a linearly decaying learning rate, both of which prevent exploding gradients and makes it much less likely that the model diverges after training for a long time.\n\n## Notes\n\n* **You will not get quality generated text 100% of the time**, even with a heavily-trained neural network. That's the primary reason viral [blog posts](http://aiweirdness.com/post/170685749687/candy-heart-messages-written-by-a-neural-network)/[Twitter tweets](https://twitter.com/botnikstudios/status/955870327652970496) utilizing NN text generation often generate lots of texts and curate/edit the best ones afterward.\n\n* **Results will vary greatly between datasets**. Because the pretrained neural network is relatively small, it cannot store as much data as RNNs typically flaunted in blog posts. For best results, use a dataset with at least 2,000-5,000 documents. If a dataset is smaller, you'll need to train it for longer by setting `num_epochs` higher when calling a training method and/or training a new model from scratch. Even then, there is currently no good heuristic for determining a \"good\" model.\n\n* A GPU is not required to retrain textgenrnn, but it will take much longer to train on a CPU. If you do use a GPU, I recommend increasing the `batch_size` parameter for better hardware utilization.\n\n## Future Plans for textgenrnn\n\n* More formal documentation\n\n* A web-based implementation using tensorflow.js (works especially well due to the network's small size)\n\n* A way to visualize the attention-layer outputs to see how the network \"learns.\"\n\n* A mode to allow the model architecture to be used for chatbot conversations (may be released as a separate project)\n\n* More depth toward context (positional context + allowing multiple context labels)\n\n* A larger pretrained network which can accommodate longer character sequences and a more indepth understanding of language, creating better generated sentences.\n\n* Hierarchical softmax activation for word-level models (once Keras has good support for it).\n\n* FP16 for superfast training on Volta/TPUs (once Keras has good support for it).\n\n## Articles/Projects using textgenrnn\n\n### Articles\n\n* Lifehacker: [How to Train Your Own Neural Network](https://lifehacker.com/we-trained-an-ai-to-generate-lifehacker-headlines-1826616918) by Beth Skwarecki\n* New York Times: [Let Our Algorithm Choose Your Halloween Costume](https://www.nytimes.com/interactive/2018/10/26/opinion/halloween-spooky-costumes-machine-learning-generator.html) by Janelle Shane\n* CNN Business: [This quirky experiment highlights AI's biggest challenges](https://www.cnn.com/2018/11/09/tech/janelle-shane-ai/index.html) by Rachel Metz\n\n### Projects\n\n* [Tweet Generator](https://github.com/minimaxir/tweet-generator) — Train a neural network optimized for generating tweets based off of any number of Twitter users\n* [Hacker News Simulator](https://twitter.com/hackernews_nn) — Twitter bot trained on 300,000+ Hacker News submissions using textgenrnn.\n* [SubredditRNN](https://www.reddit.com/r/subredditnn) — Reddit Subreddit where all submitted content is from textgenrnn bots.\n* [Human-AI Collaborated Pizzas](https://howtogeneratealmostanything.com/food/2018/08/30/episode2.html) — Pizza recepies generated with textgenrnn and made in real life.\n* [Board Game Titles](https://boardgamegeek.com/thread/2105706/i-trained-neural-network-17000-game-titles-bgg)\n* [Video Game Discussion Forum Titles](https://www.resetera.com/threads/i-trained-an-ai-on-tens-of-thousands-of-resetera-post-titles-and-discovered-how-the-world-ends.82679/)\n* [A.I Created Cakes](https://www.cupcaikes.com/index.html)\n* [AI Created Cookies](http://aiweirdness.com/post/180892528177/aw-yeah-its-time-for-cookies-with-neural-networks)\n* [AI Generated Songs](http://aiweirdness.com/post/180654319147/how-to-begin-a-song)\n\n### Tweets\n\n* [BuzzFeed YouTube Videos](https://twitter.com/minimaxir/status/1064604986951163905)\n* [AWS Services](https://twitter.com/jamesoff/status/1073647847130742787)\n* [Recipes + D&D Spells + Heavy Metal Names](https://twitter.com/ThomasClaburn/status/1049069940571955201)\n* [RPG Adventure Names](https://twitter.com/400goblins/status/1036794962740953088)\n* [The Onion + Cosmopolitan](https://twitter.com/BBCPARLlAMENT/status/1014834653113585664)\n* [Google Conference Room Names](https://twitter.com/tensafefrogs/status/1009912151060951045)\n* [Sith Lords](https://twitter.com/JanelleCShane/status/1002573232103305216)\n\n## Maintainer/Creator\n\nMax Woolf ([@minimaxir](http://minimaxir.com))\n\n*Max's open-source projects are supported by his [Patreon](https://www.patreon.com/minimaxir). If you found this project helpful, any monetary contributions to the Patreon are appreciated and will be put to good creative use.*\n\n## Credits\n\nAndrej Karpathy for the original proposal of the char-rnn via the blog post [The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/).\n\n[Daniel Grijalva](https://github.com/Juanets) for [contributing](https://github.com/minimaxir/textgenrnn/pull/52) an interactive mode.\n\n## License\n\nMIT\n\nAttention-layer code used from [DeepMoji](https://github.com/bfelbo/DeepMoji) (MIT Licensed)\n"
        },
        {
          "name": "datasets",
          "type": "tree",
          "content": null
        },
        {
          "name": "docs",
          "type": "tree",
          "content": null
        },
        {
          "name": "outputs",
          "type": "tree",
          "content": null
        },
        {
          "name": "requirements.txt",
          "type": "blob",
          "size": 0.0400390625,
          "content": "h5py\nscikit-learn\ntqdm\ntensorflow>=2.1.0\n"
        },
        {
          "name": "setup.cfg",
          "type": "blob",
          "size": 0.0380859375,
          "content": "[metadata]\ndescription-file = README.md"
        },
        {
          "name": "setup.py",
          "type": "blob",
          "size": 1.716796875,
          "content": "from setuptools import setup, find_packages\n\nlong_description = '''\nEasily train your own text-generating neural network of\nany size and complexity on any text dataset with a few lines\nof code, or quickly train on a text using a pretrained model.\n\n- A modern neural network architecture which utilizes new techniques as\nattention-weighting and skip-embedding to accelerate training\nand improve model quality.\n- Able to train on and generate text at either the\ncharacter-level or word-level.\n- Able to configure RNN size, the number of RNN layers,\nand whether to use bidirectional RNNs.\n- Able to train on any generic input text file, including large files.\n- Able to train models on a GPU and then use them with a CPU.\n- Able to utilize a powerful CuDNN implementation of RNNs\nwhen trained on the GPU, which massively speeds up training time as\nopposed to normal LSTM implementations.\n- Able to train the model using contextual labels,\nallowing it to learn faster and produce better results in some cases.\n- Able to generate text interactively for customized stories.\n'''\n\n\nsetup(\n    name='textgenrnn',\n    packages=['textgenrnn'],  # this must be the same as the name above\n    version='2.0.0',\n    description='Easily train your own text-generating neural network ' \\\n    'of any size and complexity',\n    long_description=long_description,\n    long_description_content_type='text/markdown',\n    author='Max Woolf',\n    author_email='max@minimaxir.com',\n    url='https://github.com/minimaxir/textgenrnn',\n    keywords=['deep learning', 'tensorflow', 'keras', 'text generation'],\n    classifiers=[],\n    license='MIT',\n    python_requires='>=3.5',\n    include_package_data=True,\n    install_requires=['h5py', 'scikit-learn', 'tqdm', 'tensorflow>=2.1.0']\n)\n"
        },
        {
          "name": "textgenrnn",
          "type": "tree",
          "content": null
        },
        {
          "name": "weights",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}