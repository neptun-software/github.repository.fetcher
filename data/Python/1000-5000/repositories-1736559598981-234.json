{
  "metadata": {
    "timestamp": 1736559598981,
    "page": 234,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjI0MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "NVlabs/neuralangelo",
      "stars": 4436,
      "defaultBranch": "main",
      "files": [
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 3.53515625,
          "content": "checkpoints\n\n# Other uncheckable file types\n*.zip\n*.exe\n*.dll\n*.swp\n*.vscode\n*.ipynb\n*.DS_Store\n*.pyc\n\n# Credential information that should never be checked in\n*.secret\n\n# Data types\n*.png\n*.hdr\n*.jpg\n*.jpeg\n*.pgm\n*.tiff\n*.tif\n*.mp4\n*.MOV\n*.tar\n*.tar.gz\n*.pkl\n*.pt\n*.bin\n*.ply\n\n# log folder\nlogs/\n\n# dataset folder\ndatasets/\n/datasets/\n\n# config folder\n!projects/neuralangelo/configs/custom/template.yaml\nprojects/neuralangelo/configs/custom\n\n# ------------------------ BELOW IS AUTO-GENERATED FOR PYTHON REPOS ------------------------\n\n# Byte-compiled / optimized / DLL files\n__pycache__/\n*.py[cod]\n*$py.class\n\n# C extensions\n*.so\n\n# Distribution / packaging\n.Python\nbuild/\ndevelop-eggs/\ndist/\ndownloads/\neggs/\n.eggs/\nlib/\nlib64/\nparts/\nsdist/\nvar/\nwheels/\nshare/python-wheels/\n*.egg-info/\n.installed.cfg\n*.egg\nMANIFEST\n\n# PyInstaller\n#  Usually these files are written by a python script from a template\n#  before PyInstaller builds the exe, so as to inject date/other infos into it.\n*.manifest\n*.spec\n\n# Installer logs\npip-log.txt\npip-delete-this-directory.txt\n\n# Unit test / coverage reports\nhtmlcov/\n.tox/\n.nox/\n.coverage\n.coverage.*\n.cache\nnosetests.xml\ncoverage.xml\n*.cover\n*.py,cover\n.hypothesis/\n.pytest_cache/\ncover/\n\n# Translations\n*.mo\n*.pot\n\n# Django stuff:\n*.log\nlocal_settings.py\ndb.sqlite3\ndb.sqlite3-journal\n\n# Flask stuff:\ninstance/\n.webassets-cache\n\n# Scrapy stuff:\n.scrapy\n\n# Sphinx documentation\ndocs/_build/\n\n# PyBuilder\n.pybuilder/\ntarget/\n\n# Jupyter Notebook\n.ipynb_checkpoints\n\n# IPython\nprofile_default/\nipython_config.py\n\n# pyenv\n#   For a library or package, you might want to ignore these files since the code is\n#   intended to run in multiple environments; otherwise, check them in:\n# .python-version\n\n# pipenv\n#   According to pypa/pipenv#598, it is recommended to include Pipfile.lock in version control.\n#   However, in case of collaboration, if having platform-specific dependencies or dependencies\n#   having no cross-platform support, pipenv may install dependencies that don't work, or not\n#   install all needed dependencies.\n#Pipfile.lock\n\n# poetry\n#   Similar to Pipfile.lock, it is generally recommended to include poetry.lock in version control.\n#   This is especially recommended for binary packages to ensure reproducibility, and is more\n#   commonly ignored for libraries.\n#   https://python-poetry.org/docs/basic-usage/#commit-your-poetrylock-file-to-version-control\n#poetry.lock\n\n# pdm\n#   Similar to Pipfile.lock, it is generally recommended to include pdm.lock in version control.\n#pdm.lock\n#   pdm stores project-wide configurations in .pdm.toml, but it is recommended to not include it\n#   in version control.\n#   https://pdm.fming.dev/#use-with-ide\n.pdm.toml\n\n# PEP 582; used by e.g. github.com/David-OConnor/pyflow and github.com/pdm-project/pdm\n__pypackages__/\n\n# Celery stuff\ncelerybeat-schedule\ncelerybeat.pid\n\n# SageMath parsed files\n*.sage.py\n\n# Environments\n.env\n.venv\nenv/\nvenv/\nENV/\nenv.bak/\nvenv.bak/\n\n# Spyder project settings\n.spyderproject\n.spyproject\n\n# Rope project settings\n.ropeproject\n\n# mkdocs documentation\n/site\n\n# mypy\n.mypy_cache/\n.dmypy.json\ndmypy.json\n\n# Pyre type checker\n.pyre/\n\n# pytype static type analyzer\n.pytype/\n\n# Cython debug symbols\ncython_debug/\n\n# PyCharm\n#  JetBrains specific template is maintained in a separate JetBrains.gitignore that can\n#  be found at https://github.com/github/gitignore/blob/main/Global/JetBrains.gitignore\n#  and can be added to the global gitignore or merged into this file.  For a more nuclear\n#  option (not recommended) you can uncomment the following to ignore the entire idea folder.\n#.idea/\nCLIP\n"
        },
        {
          "name": ".gitmodules",
          "type": "blob",
          "size": 0.1015625,
          "content": "[submodule \"third_party/colmap\"]\n\tpath = third_party/colmap\n\turl = https://github.com/colmap/colmap.git\n"
        },
        {
          "name": ".pre-commit-config.yaml",
          "type": "blob",
          "size": 0.1396484375,
          "content": "repos:\n- repo: https://github.com/pycqa/flake8\n  rev: 4.0.0\n  hooks:\n  - id: flake8\n    args: [--max-line-length=120]\n    exclude: third_party\n"
        },
        {
          "name": "DATA_PROCESSING.md",
          "type": "blob",
          "size": 7.09375,
          "content": "# Data Preparation\n\n*Note: please use respecting the license terms of each dataset. Each user is responsible for checking the content of datasets and the applicable licenses and determining if suitable for the intended use.*\n\nThe following sections provide a guide on how to preprocess input videos for Neuralangelo.\n\n## Prerequisites\nInitialize the COLMAP submodule:\n```bash\ngit submodule update --init --recursive\n```\n\n## Self-captured video sequence\nTo capture your own data, we recommend using a high shutter speed to avoid motion blur (which is very common when using a phone camera). We provide a synthetic [Lego sequence](https://drive.google.com/file/d/1yWoZ4Hk3FgmV3pd34ZbW7jEqgqyJgzHy/view?usp=drive_link) (from the original [NeRF](https://github.com/bmild/nerf)) as a toy example video for testing the workflow. There are two steps:\n1. [preprocessing](#preprocessing) the data and running COLMAP,\n2. [inspecting](#inspect-and-adjust-colmap-results) and refining the bounding sphere of interest for running Neuralangelo.\n\n### Preprocessing\nFirst, set some environment variables:\n```bash\nSEQUENCE=lego\nPATH_TO_VIDEO=lego.mp4\nDOWNSAMPLE_RATE=2\nSCENE_TYPE=object\n```\nwhere\n- `SEQUENCE`: your custom name for the video sequence.\n- `PATH_TO_VIDEO`: absolute/relative path to your video.\n- `DOWNSAMPLE_RATE`: temporal downsampling rate of video sequence (for extracting video frames).\n- `SCENE_TYPE`: can be one of ` {outdoor,indoor,object}`.\n\nTo preprocess your data, you can choose to either\n\n- Run the following end-to-end script:\n    ```bash\n    bash projects/neuralangelo/scripts/preprocess.sh ${SEQUENCE} ${PATH_TO_VIDEO} ${DOWNSAMPLE_RATE} ${SCENE_TYPE}\n    ```\n\n- Or you can follow the steps below if you want more fine-grained control:\n\n    1. Extract images from the input video\n\n        ```bash\n        bash projects/neuralangelo/scripts/run_ffmpeg.sh ${SEQUENCE} ${PATH_TO_VIDEO} ${DOWNSAMPLE_RATE}\n        ```\n        This will create a directory `datasets/{SEQUENCE}_ds{DOWNSAMPLE_RATE}` (set as `DATA_PATH` onwards), which stores all the processed data.\n        The extracted images will be stored in `{DATA_PATH}/images_raw`.\n\n    2. Run COLMAP\n\n        ```bash\n        DATA_PATH=datasets/${SEQUENCE}_ds${DOWNSAMPLE_RATE}\n        bash projects/neuralangelo/scripts/run_colmap.sh ${DATA_PATH}\n        ```\n        `DATA_PATH`: path to processed data.\n\n        After COLMAP finishes, the folder structure will look like following:\n        ```\n        DATA_PATH\n        ├─ database.db      (COLMAP database)\n        ├─ images           (undistorted input images)\n        ├─ images_raw       (raw input images)\n        ├─ sparse           (COLMAP data from SfM)\n        │  ├─ cameras.bin   (camera parameters)\n        │  ├─ images.bin    (images and camera poses)\n        │  ├─ points3D.bin  (sparse point clouds)\n        │  ├─ 0             (a directory containing individual SfM models. There could also be 1, 2... etc.)\n        │  ...\n        ├─ stereo (COLMAP data for MVS, not used here)\n        ...\n        ```\n        `{DATA_PATH}/images` will be the input image observations for surface reconstruction.\n\n    3. Generate JSON file for data loading\n\n        In this step, we define the bounding region for reconstruction and convert the COLMAP data to JSON format following Instant NGP.\n        It is strongly recommended to [inspect](#inspect-and-adjust-colmap-results) the results to verify and adjust the bounding region for improved performance.\n        ```bash\n        python3 projects/neuralangelo/scripts/convert_data_to_json.py --data_dir ${DATA_PATH} --scene_type ${SCENE_TYPE}\n        ```\n        The JSON file will be generated in `{DATA_PATH}/transforms.json`.\n\n    4. Config files\n\n        Use the following to configure and generate your config files:\n        ```bash\n        python3 projects/neuralangelo/scripts/generate_config.py --sequence_name ${SEQUENCE} --data_dir ${DATA_PATH} --scene_type ${SCENE_TYPE}\n        ```\n        The config file will be generated as `projects/neuralangelo/configs/custom/{SEQUENCE}.yaml`.\n        You can add the `--help` flag to list all arguments; for example, consider adding `--auto_exposure_wb` for modeling varying lighting/appearances in the video.\n        Alternatively, you can directly modify the hyperparameters in the generated config file.\n\n### Inspect and adjust COLMAP results\n\nFor certain cases, the camera poses estimated by COLMAP could be erroneous. In addition, the automated estimation of the bounding sphere could be inaccurate (which ideally should include the scene/object of interest). It is highly recommended that the bounding sphere is adjusted. \nWe offer some tools to to inspect and adjust the pre-processing results. Below are some options:\n\n- Blender: Download [Blender](https://www.blender.org/download/) and follow the instructions in our [add-on repo](https://github.com/mli0603/BlenderNeuralangelo). The add-on will save your adjustment of the bounding sphere.\n- This [Jupyter notebook](projects/neuralangelo/scripts/visualize_colmap.ipynb) (using K3D) can be helpful for visualizing the COLMAP results. You can adjust the bounding sphere by manually specifying the refining sphere center and size in the `data.readjust` config.\n\nFor certain cases, an exhaustive feature matcher may be able to estimate more accurate camera poses.\nThis could be done by changing `sequential_matcher` to `exhaustive_matcher` in [run_colmap.sh](https://github.com/NVlabs/neuralangelo/blob/main/projects/neuralangelo/scripts/run_colmap.sh#L24).\nHowever, this would take more time to process and could sometimes result in \"broken trajectories\" (from COLMAP failing due to ambiguous matches).\nFor more details, please refer to the COLMAP [documentation](https://colmap.github.io/).\n\n## DTU dataset\nYou can run the following command to download [the DTU dataset](https://roboimagedata.compute.dtu.dk/?page_id=36) that is preprocessed by NeuS authors and generate json files:\n```bash\nPATH_TO_DTU=datasets/dtu  # Modify this to be the DTU dataset root directory.\nbash projects/neuralangelo/scripts/preprocess_dtu.sh ${PATH_TO_DTU}\n```\n\n## Tanks and Temples dataset\nDownload the data from [Tanks and Temples](https://tanksandtemples.org/download/) website.\nYou will also need to download additional [COLMAP/camera/alignment](https://drive.google.com/file/d/1jAr3IDvhVmmYeDWi0D_JfgiHcl70rzVE/view?resourcekey=) and the images of each scene.  \nThe file structure should look like (you need to move the downloaded images to folder `images_raw`):\n```\ntanks_and_temples\n├─ Barn\n│  ├─ Barn_COLMAP_SfM.log   (camera poses)\n│  ├─ Barn.json             (cropfiles)\n│  ├─ Barn.ply              (ground-truth point cloud)\n│  ├─ Barn_trans.txt        (colmap-to-ground-truth transformation)\n│  └─ images_raw            (raw input images downloaded from Tanks and Temples website)\n│     ├─ 000001.png\n│     ├─ 000002.png\n│     ...\n├─ Caterpillar\n│  ├─ ...\n...\n```\nRun the following command to generate json files:\n```bash\nPATH_TO_TNT=datasets/tanks_and_temples  # Modify this to be the Tanks and Temples root directory.\nbash projects/neuralangelo/scripts/preprocess_tnt.sh ${PATH_TO_TNT}\n```\n"
        },
        {
          "name": "LICENSE.md",
          "type": "blob",
          "size": 4.349609375,
          "content": "# NVIDIA Source Code License for Neuralangelo\n\n## 1. Definitions \n \n- “Licensor” means any person or entity that distributes its Work. \n\n-  “Software” means the original work of authorship made available under this License. \n\n-  “Work” means the Software and any additions to or derivative works of the Software that are made available under this License. \n\n-  “NVIDIA Processors” means any central processing unit (CPU), graphics processing unit (GPU), field-programmable gate array (FPGA), application-specific integrated circuit (ASIC) or any combination thereof designed, made, sold, or provided by NVIDIA or its affiliates. \n\n-  The terms “reproduce,” “reproduction,” “derivative works,” and “distribution” have the meaning as provided under U.S. copyright law; provided, however, that for the purposes of this License, derivative works shall not include works that remain separable from, or merely link (or bind by name) to the interfaces of, the Work. \n\n-  Works, including the Software, are “made available” under this License by including in or with the Work either (a) a copyright notice referencing the applicability of this License to the Work, or (b) a copy of this License. \n\n## 2. License Grant \n\n### 2.1 Copyright Grant. \n\nSubject to the terms and conditions of this License, each Licensor grants to you a perpetual, worldwide, non-exclusive, royalty-free, copyright license to reproduce, prepare derivative works of, publicly display, publicly perform, sublicense and distribute its Work and any resulting derivative works in any form.\n\n## 3. Limitations\n\n### 3.1 Redistribution. \n\nYou may reproduce or distribute the Work only if (a) you do so under this License, (b) you include a complete copy of this License with your distribution, and (c) you retain without modification any copyright, patent, trademark, or attribution notices that are present in the Work.\n\n### 3.2 Derivative Works.\n\nYou may specify that additional or different terms apply to the use, reproduction, and distribution of your derivative works of the Work (“Your Terms”) only if (a) Your Terms provide that the use limitation in Section 3.3 applies to your derivative works, and (b) you identify the specific derivative works that are subject to Your Terms. Notwithstanding Your Terms, this License (including the redistribution requirements in Section 3.1) will continue to apply to the Work itself.\n \n### 3.3 Use Limitation.\n\nThe Work and any derivative works thereof only may be used or intended for use non-commercially and with NVIDIA Processors. Notwithstanding the foregoing, NVIDIA and its affiliates may use the Work and any derivative works commercially. As used herein, “non-commercially” means for research or evaluation purposes only.\n\n### 3.4 Patent Claims.\n\nIf you bring or threaten to bring a patent claim against any Licensor (including any claim, cross-claim or counterclaim in a lawsuit) to enforce any patents that you allege are infringed by any Work, then your rights under this License from such Licensor (including the grant in Section 2.1) will terminate immediately.\n\n### 3.5 Trademarks.\n\nThis License does not grant any rights to use any Licensor’s or its affiliates’ names, logos, or trademarks, except as necessary to reproduce the notices described in this License.\n\n### 3.6 Termination.\n\nIf you violate any term of this License, then your rights under this License (including the grant in Section 2.1) will terminate immediately.\n\n## 4. Disclaimer of Warranty. \n\nTHE WORK IS PROVIDED “AS IS” WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, EITHER EXPRESS OR IMPLIED, INCLUDING WARRANTIES OR CONDITIONS OF M ERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, TITLE OR NON-INFRINGEMENT. YOU BEAR THE RISK OF UNDERTAKING ANY ACTIVITIES UNDER THIS LICENSE.  \n\n## 5. Limitation of Liability. \n\nEXCEPT AS PROHIBITED BY APPLICABLE LAW, IN NO EVENT AND UNDER NO LEGAL THEORY, WHETHER IN TORT (INCLUDING NEGLIGENCE), CONTRACT, OR OTHERWISE SHALL ANY LICENSOR BE LIABLE TO YOU FOR DAMAGES, INCLUDING ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, OR CONSEQUENTIAL DAMAGES ARISING OUT OF OR RELATED TO THIS LICENSE, THE USE OR INABILITY TO USE THE WORK (INCLUDING BUT NOT LIMITED TO LOSS OF GOODWILL, BUSINESS INTERRUPTION, LOST PROFITS OR DATA, COMPUTER FAILURE OR MALFUNCTION, OR ANY OTHER COMM ERCIAL DAMAGES OR LOSSES), EVEN IF THE LICENSOR HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES. \n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 6.0263671875,
          "content": "# Neuralangelo\nThis is the official implementation of **Neuralangelo: High-Fidelity Neural Surface Reconstruction**.\n\n[Zhaoshuo Li](https://mli0603.github.io/),\n[Thomas Müller](https://tom94.net/),\n[Alex Evans](https://research.nvidia.com/person/alex-evans),\n[Russell H. Taylor](https://www.cs.jhu.edu/~rht/),\n[Mathias Unberath](https://mathiasunberath.github.io/),\n[Ming-Yu Liu](https://mingyuliu.net/),\n[Chen-Hsuan Lin](https://chenhsuanlin.bitbucket.io/)  \nIEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2023\n\n### [Project page](https://research.nvidia.com/labs/dir/neuralangelo/) | [Paper](https://arxiv.org/abs/2306.03092/) | [Colab notebook](https://colab.research.google.com/drive/13u8DX9BNzQwiyPPCB7_4DbSxiQ5-_nGF)\n\n<img src=\"assets/teaser.gif\">\n\nThe code is built upon the Imaginaire library from the Deep Imagination Research Group at NVIDIA.  \nFor business inquiries, please submit the [NVIDIA research licensing form](https://www.nvidia.com/en-us/research/inquiries/).\n\n--------------------------------------\n\n## Installation\nWe offer two ways to setup the environment:\n1. We provide prebuilt Docker images, where\n    - `docker.io/chenhsuanlin/colmap:3.8` is for running COLMAP and the data preprocessing scripts. This includes the prebuilt COLMAP library (CUDA-supported).\n    - `docker.io/chenhsuanlin/neuralangelo:23.04-py3` is for running the main Neuralangelo pipeline.\n\n    The corresponding Dockerfiles can be found in the `docker` directory.\n2. The conda environment for Neuralangelo. Install the dependencies and activate the environment `neuralangelo` with\n    ```bash\n    conda env create --file neuralangelo.yaml\n    conda activate neuralangelo\n    ```\nFor COLMAP, alternative installation options are also available on the [COLMAP website](https://colmap.github.io/).\n\n--------------------------------------\n\n## Data preparation\nPlease refer to [Data Preparation](DATA_PROCESSING.md) for step-by-step instructions.  \nWe assume known camera poses for each extracted frame from the video.\nThe code uses the same json format as [Instant NGP](https://github.com/NVlabs/instant-ngp).\n\n--------------------------------------\n\n## Run Neuralangelo!\n```bash\nEXPERIMENT=toy_example\nGROUP=example_group\nNAME=example_name\nCONFIG=projects/neuralangelo/configs/custom/${EXPERIMENT}.yaml\nGPUS=1  # use >1 for multi-GPU training!\ntorchrun --nproc_per_node=${GPUS} train.py \\\n    --logdir=logs/${GROUP}/${NAME} \\\n    --config=${CONFIG} \\\n    --show_pbar\n```\nSome useful notes:\n- This codebase supports logging with [Weights & Biases](https://wandb.ai/site). You should have a W&B account for this.\n    - Add `--wandb` to the command line argument to enable W&B logging.\n    - Add `--wandb_name` to specify the W&B project name.\n    - More detailed control can be found in the `init_wandb()` function in `imaginaire/trainers/base.py`.\n- Configs can be overridden through the command line (e.g. `--optim.params.lr=1e-2`).\n- Set `--checkpoint={CHECKPOINT_PATH}` to initialize with a certain checkpoint; set `--resume` to resume training.\n- If appearance embeddings are enabled, make sure `data.num_images` is set to the number of training images.\n\n--------------------------------------\n\n## Isosurface extraction\nUse the following command to run isosurface mesh extraction:\n```bash\nCHECKPOINT=logs/${GROUP}/${NAME}/xxx.pt\nOUTPUT_MESH=xxx.ply\nCONFIG=logs/${GROUP}/${NAME}/config.yaml\nRESOLUTION=2048\nBLOCK_RES=128\nGPUS=1  # use >1 for multi-GPU mesh extraction\ntorchrun --nproc_per_node=${GPUS} projects/neuralangelo/scripts/extract_mesh.py \\\n    --config=${CONFIG} \\\n    --checkpoint=${CHECKPOINT} \\\n    --output_file=${OUTPUT_MESH} \\\n    --resolution=${RESOLUTION} \\\n    --block_res=${BLOCK_RES}\n```\nSome useful notes:\n- Add `--textured` to extract meshes with textures.\n- Add `--keep_lcc` to remove noises. May also remove thin structures.\n- Lower `BLOCK_RES` to reduce GPU memory usage.\n- Lower `RESOLUTION` to reduce mesh size.\n\n--------------------------------------\n\n## Frequently asked questions (FAQ)\n1. **Q:** CUDA out of memory. How do I decrease the memory footprint?  \n    **A:** Neuralangelo requires at least 24GB GPU memory with our default configuration. If you run out of memory, consider adjusting the following hyperparameters under `model.object.sdf.encoding.hashgrid` (with suggested values):\n\n    | GPU VRAM      | Hyperparameter          |\n    | :-----------: | :---------------------: |\n    | 8GB           | `dict_size=20`, `dim=4` |\n    | 12GB          | `dict_size=21`, `dim=4` |\n    | 16GB          | `dict_size=21`, `dim=8` |\n\n    Please note that the above hyperparameter adjustment may sacrifice the reconstruction quality.\n\n   If Neuralangelo runs fine during training but CUDA out of memory during evaluation, consider adjusting the evaluation parameters under `data.val`, including setting smaller `image_size` (e.g., maximum resolution 200x200), and setting `batch_size=1`, `subset=1`.\n\n2. **Q:** The reconstruction of my custom dataset is bad. What can I do?  \n    **A:** It is worth looking into the following:\n    - The camera poses recovered by COLMAP may be off. We have implemented tools (using [Blender](https://github.com/mli0603/BlenderNeuralangelo) or [Jupyter notebook](projects/neuralangelo/scripts/visualize_colmap.ipynb)) to inspect the COLMAP results.\n    - The computed bounding regions may be off and/or too small/large. Please refer to [data preprocessing](DATA_PROCESSING.md) on how to adjust the bounding regions manually.\n    - The video capture sequence may contain significant motion blur or out-of-focus frames. Higher shutter speed (reducing motion blur) and smaller aperture (increasing focus range) are very helpful.\n\n--------------------------------------\n\n## Citation\nIf you find our code useful for your research, please cite\n```\n@inproceedings{li2023neuralangelo,\n  title={Neuralangelo: High-Fidelity Neural Surface Reconstruction},\n  author={Li, Zhaoshuo and M\\\"uller, Thomas and Evans, Alex and Taylor, Russell H and Unberath, Mathias and Liu, Ming-Yu and Lin, Chen-Hsuan},\n  booktitle={IEEE Conference on Computer Vision and Pattern Recognition ({CVPR})},\n  year={2023}\n}\n```\n"
        },
        {
          "name": "assets",
          "type": "tree",
          "content": null
        },
        {
          "name": "docker",
          "type": "tree",
          "content": null
        },
        {
          "name": "imaginaire",
          "type": "tree",
          "content": null
        },
        {
          "name": "neuralangelo.yaml",
          "type": "blob",
          "size": 0.369140625,
          "content": "# conda env create --file neuralangelo.yaml\nname: neuralangelo\nchannels:\n  - conda-forge\n  - pytorch\ndependencies:\n  # general\n  - gpustat\n  - gdown\n  - cudatoolkit-dev\n  - cmake\n  # python general\n  - python=3.8\n  - pip\n  - numpy\n  - scipy\n  - ipython\n  - jupyterlab\n  - cython\n  - ninja\n  - diskcache\n  # pytorch\n  - pytorch\n  - torchvision\n  - pip:\n    - -r requirements.txt\n"
        },
        {
          "name": "projects",
          "type": "tree",
          "content": null
        },
        {
          "name": "requirements.txt",
          "type": "blob",
          "size": 0.3662109375,
          "content": "addict\ngdown\ngit+https://github.com/NVlabs/tiny-cuda-nn/#subdirectory=bindings/torch\ngpustat\nicecream\nimageio-ffmpeg\nimutils\nipdb\nk3d\nkornia\nlpips\nmatplotlib\nmediapy\nnvidia-ml-py3\nopen3d\nopencv-python-headless\nOpenEXR\npathlib\npillow\nplotly\npyequilib\npyexr\nPyMCubes\npyquaternion\npyyaml\nrequests\nscikit-image\nscikit-video\nscipy\nseaborn\ntensorboard\ntermcolor\ntqdm\ntrimesh\nwandb\n"
        },
        {
          "name": "third_party",
          "type": "tree",
          "content": null
        },
        {
          "name": "train.py",
          "type": "blob",
          "size": 4.033203125,
          "content": "'''\n-----------------------------------------------------------------------------\nCopyright (c) 2023, NVIDIA CORPORATION. All rights reserved.\n\nNVIDIA CORPORATION and its licensors retain all intellectual property\nand proprietary rights in and to this software, related documentation\nand any modifications thereto. Any use, reproduction, disclosure or\ndistribution of this software and related documentation without an express\nlicense agreement from NVIDIA CORPORATION is strictly prohibited.\n-----------------------------------------------------------------------------\n'''\n\nimport argparse\nimport os\n\nimport imaginaire.config\nfrom imaginaire.config import Config, recursive_update_strict, parse_cmdline_arguments\nfrom imaginaire.utils.cudnn import init_cudnn\nfrom imaginaire.utils.distributed import init_dist, get_world_size, master_only_print as print, is_master\nfrom imaginaire.utils.gpu_affinity import set_affinity\nfrom imaginaire.trainers.utils.logging import init_logging\nfrom imaginaire.trainers.utils.get_trainer import get_trainer\nfrom imaginaire.utils.set_random_seed import set_random_seed\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser(description='Training')\n    parser.add_argument('--config', help='Path to the training config file.', required=True)\n    parser.add_argument('--logdir', help='Dir for saving logs and models.', default=None)\n    parser.add_argument('--checkpoint', default=None, help='Checkpoint path.')\n    parser.add_argument('--seed', type=int, default=0, help='Random seed.')\n    parser.add_argument('--local_rank', type=int, default=os.getenv('LOCAL_RANK', 0))\n    parser.add_argument('--single_gpu', action='store_true')\n    parser.add_argument('--debug', action='store_true')\n    parser.add_argument('--profile', action='store_true')\n    parser.add_argument('--show_pbar', action='store_true')\n    parser.add_argument('--wandb', action='store_true', help=\"Enable using Weights & Biases as the logger\")\n    parser.add_argument('--wandb_name', default='default', type=str)\n    parser.add_argument('--resume', action='store_true')\n    args, cfg_cmd = parser.parse_known_args()\n    return args, cfg_cmd\n\n\ndef main():\n    args, cfg_cmd = parse_args()\n    set_affinity(args.local_rank)\n    cfg = Config(args.config)\n\n    cfg_cmd = parse_cmdline_arguments(cfg_cmd)\n    recursive_update_strict(cfg, cfg_cmd)\n\n    # If args.single_gpu is set to True, we will disable distributed data parallel.\n    if not args.single_gpu:\n        # this disables nccl timeout\n        os.environ[\"NCLL_BLOCKING_WAIT\"] = \"0\"\n        os.environ[\"NCCL_ASYNC_ERROR_HANDLING\"] = \"0\"\n        cfg.local_rank = args.local_rank\n        init_dist(cfg.local_rank, rank=-1, world_size=-1)\n    print(f\"Training with {get_world_size()} GPUs.\")\n\n    # set random seed by rank\n    set_random_seed(args.seed, by_rank=True)\n\n    # Global arguments.\n    imaginaire.config.DEBUG = args.debug\n\n    # Create log directory for storing training results.\n    cfg.logdir = init_logging(args.config, args.logdir, makedir=True)\n\n    # Print and save final config\n    if is_master():\n        cfg.print_config()\n        cfg.save_config(cfg.logdir)\n\n    # Initialize cudnn.\n    init_cudnn(cfg.cudnn.deterministic, cfg.cudnn.benchmark)\n\n    # Initialize data loaders and models.\n    trainer = get_trainer(cfg, is_inference=False, seed=args.seed)\n    trainer.set_data_loader(cfg, split=\"train\")\n    trainer.set_data_loader(cfg, split=\"val\")\n    trainer.checkpointer.load(args.checkpoint, args.resume, load_sch=True, load_opt=True)\n\n    # Initialize Wandb.\n    trainer.init_wandb(cfg,\n                       project=args.wandb_name,\n                       mode=\"disabled\" if args.debug or not args.wandb else \"online\",\n                       resume=args.resume,\n                       use_group=True)\n\n    trainer.mode = 'train'\n    # Start training.\n    trainer.train(cfg,\n                  trainer.train_data_loader,\n                  single_gpu=args.single_gpu,\n                  profile=args.profile,\n                  show_pbar=args.show_pbar)\n\n    # Finalize training.\n    trainer.finalize(cfg)\n\n\nif __name__ == \"__main__\":\n    main()\n"
        }
      ]
    }
  ]
}