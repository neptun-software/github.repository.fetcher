{
  "metadata": {
    "timestamp": 1736559717024,
    "page": 408,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjQxMA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "ali-vilab/AnyDoor",
      "stars": 4059,
      "defaultBranch": "main",
      "files": [
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 1.9052734375,
          "content": ".idea/\n**/.DS_Store\ntraining/\nlightning_logs/\nimage_log/\n\n#*.pth\n*.pt\n*.ckpt\n*.safetensors\n\ngradio_pose2image_private.py\ngradio_canny2image_private.py\n\n# Byte-compiled / optimized / DLL files\n__pycache__/\n*.py[cod]\n*$py.class\n\n# C extensions\n*.so\n\n# Distribution / packaging\n.Python\nbuild/\ndevelop-eggs/\ndist/\ndownloads/\neggs/\n.eggs/\nlib/\nlib64/\nparts/\nsdist/\nvar/\nwheels/\npip-wheel-metadata/\nshare/python-wheels/\n*.egg-info/\n.installed.cfg\n*.egg\nMANIFEST\n\n# PyInstaller\n#  Usually these files are written by a python script from a template\n#  before PyInstaller builds the exe, so as to inject date/other infos into it.\n*.manifest\n*.spec\n\n# Installer logs\npip-log.txt\npip-delete-this-directory.txt\n\n# Unit test / coverage reports\nhtmlcov/\n.tox/\n.nox/\n.coverage\n.coverage.*\n.cache\nnosetests.xml\ncoverage.xml\n*.cover\n*.py,cover\n.hypothesis/\n.pytest_cache/\n\n# Translations\n*.mo\n*.pot\n\n# Django stuff:\n*.log\nlocal_settings.py\ndb.sqlite3\ndb.sqlite3-journal\n\n# Flask stuff:\ninstance/\n.webassets-cache\n\n# Scrapy stuff:\n.scrapy\n\n# Sphinx documentation\ndocs/_build/\n\n# PyBuilder\ntarget/\n\n# Jupyter Notebook\n.ipynb_checkpoints\n\n# IPython\nprofile_default/\nipython_config.py\n\n# pyenv\n.python-version\n\n# pipenv\n#   According to pypa/pipenv#598, it is recommended to include Pipfile.lock in version control.\n#   However, in case of collaboration, if having platform-specific dependencies or dependencies\n#   having no cross-platform support, pipenv may install dependencies that don't work, or not\n#   install all needed dependencies.\n#Pipfile.lock\n\n# PEP 582; used by e.g. github.com/David-OConnor/pyflow\n__pypackages__/\n\n# Celery stuff\ncelerybeat-schedule\ncelerybeat.pid\n\n# SageMath parsed files\n*.sage.py\n\n# Environments\n.env\n.venv\nenv/\nvenv/\nENV/\nenv.bak/\nvenv.bak/\n\n# Spyder project settings\n.spyderproject\n.spyproject\n\n# Rope project settings\n.ropeproject\n\n# mkdocs documentation\n/site\n\n# mypy\n.mypy_cache/\n.dmypy.json\ndmypy.json\n\n# Pyre type checker\n.pyre/\n"
        },
        {
          "name": "LICENSE.txt",
          "type": "blob",
          "size": 1.0595703125,
          "content": "MIT License\n\nCopyright (c) 2023 DAMO Vision Intelligence Lab\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n"
        },
        {
          "name": "assets",
          "type": "tree",
          "content": null
        },
        {
          "name": "cldm",
          "type": "tree",
          "content": null
        },
        {
          "name": "cog.yaml",
          "type": "blob",
          "size": 0.9990234375,
          "content": "# Configuration for Cog 锔\n# Reference: https://github.com/replicate/cog/blob/main/docs/yaml.md\nbuild:\n  gpu: true\n  system_packages:\n    - \"mesa-common-dev\"\n  python_version: \"3.8.5\"\n  python_packages:\n    - \"albumentations==1.3.0\"\n    - \"einops==0.3.0\"\n    - \"fvcore==0.1.5.post20221221\"\n    - \"gradio==3.39.0\"\n    - \"numpy==1.23.1\"\n    - \"omegaconf==2.1.1\"\n    - \"open_clip_torch==2.17.1\"\n    - \"opencv_python==4.7.0.72\"\n    - \"opencv_python_headless==4.7.0.72\"\n    - \"Pillow==9.4.0\"\n    - \"pytorch_lightning==1.5.0\"\n    - \"safetensors==0.2.7\"\n    - \"scipy==1.9.1\"\n    - \"setuptools==66.0.0\"\n    - \"share==1.0.4\"\n    - \"submitit==1.5.1\"\n    - \"timm==0.6.12\"\n    - \"torch==2.0.0\"\n    - \"torchmetrics==0.6.0\"\n    - \"tqdm==4.65.0\"\n    - \"transformers==4.19.2\"\n    - \"xformers==0.0.18\"\n\n  run:\n    - curl -o /usr/local/bin/pget -L \"https://github.com/replicate/pget/releases/download/v0.3.1/pget\" && chmod +x /usr/local/bin/pget\n\n# predict.py defines how predictions are run on your model\npredict: \"predict.py:Predictor\""
        },
        {
          "name": "configs",
          "type": "tree",
          "content": null
        },
        {
          "name": "datasets",
          "type": "tree",
          "content": null
        },
        {
          "name": "dinov2",
          "type": "tree",
          "content": null
        },
        {
          "name": "environment.yaml",
          "type": "blob",
          "size": 4.2978515625,
          "content": "name: anydoor\nchannels:\n  - defaults\ndependencies:\n  - _libgcc_mutex=0.1=main\n  - _openmp_mutex=5.1=1_gnu\n  - ca-certificates=2023.08.22=h06a4308_0\n  - ld_impl_linux-64=2.38=h1181459_1\n  - libffi=3.3=he6710b0_2\n  - libgcc-ng=11.2.0=h1234567_1\n  - libgomp=11.2.0=h1234567_1\n  - libstdcxx-ng=11.2.0=h1234567_1\n  - ncurses=6.4=h6a678d5_0\n  - openssl=1.1.1w=h7f8727e_0\n  - pip=23.3.1=py38h06a4308_0\n  - python=3.8.5=h7579374_1\n  - readline=8.2=h5eee18b_0\n  - sqlite=3.41.2=h5eee18b_0\n  - tk=8.6.12=h1ccaba5_0\n  - wheel=0.41.2=py38h06a4308_0\n  - xz=5.4.5=h5eee18b_0\n  - zlib=1.2.13=h5eee18b_0\n  - pip:\n    - absl-py==2.0.0\n    - aiofiles==23.2.1\n    - aiohttp==3.9.1\n    - aiosignal==1.3.1\n    - albumentations==1.3.0\n    - altair==5.2.0\n    - annotated-types==0.6.0\n    - antlr4-python3-runtime==4.8\n    - anyio==3.7.1\n    - async-timeout==4.0.3\n    - attrs==23.1.0\n    - cachetools==5.3.2\n    - certifi==2023.11.17\n    - charset-normalizer==3.3.2\n    - click==8.1.7\n    - cloudpickle==3.0.0\n    - cmake==3.28.0\n    - contourpy==1.1.1\n    - cycler==0.12.1\n    - cython==3.0.6\n    - einops==0.3.0\n    - exceptiongroup==1.2.0\n    - fastapi==0.105.0\n    - ffmpy==0.3.1\n    - filelock==3.13.1\n    - fonttools==4.46.0\n    - frozenlist==1.4.0\n    - fsspec==2023.12.2\n    - ftfy==6.1.3\n    - future==0.18.3\n    - fvcore==0.1.5.post20221221\n    - google-auth==2.25.2\n    - google-auth-oauthlib==1.0.0\n    - gradio==3.39.0\n    - gradio-client==0.7.2\n    - grpcio==1.60.0\n    - h11==0.14.0\n    - httpcore==1.0.2\n    - httpx==0.25.2\n    - huggingface-hub==0.19.4\n    - idna==3.6\n    - imageio==2.33.1\n    - importlib-metadata==7.0.0\n    - importlib-resources==6.1.1\n    - iopath==0.1.10\n    - jinja2==3.1.2\n    - joblib==1.3.2\n    - jsonschema==4.20.0\n    - jsonschema-specifications==2023.11.2\n    - kiwisolver==1.4.5\n    - lazy-loader==0.3\n    - linkify-it-py==2.0.2\n    - lit==17.0.6\n    - lvis==0.5.3\n    - markdown==3.5.1\n    - markdown-it-py==2.2.0\n    - markupsafe==2.1.3\n    - matplotlib==3.7.4\n    - mdit-py-plugins==0.3.3\n    - mdurl==0.1.2\n    - mpmath==1.3.0\n    - multidict==6.0.4\n    - mypy-extensions==1.0.0\n    - networkx==3.1\n    - numpy==1.23.1\n    - nvidia-cublas-cu11==11.10.3.66\n    - nvidia-cuda-cupti-cu11==11.7.101\n    - nvidia-cuda-nvrtc-cu11==11.7.99\n    - nvidia-cuda-runtime-cu11==11.7.99\n    - nvidia-cudnn-cu11==8.5.0.96\n    - nvidia-cufft-cu11==10.9.0.58\n    - nvidia-curand-cu11==10.2.10.91\n    - nvidia-cusolver-cu11==11.4.0.1\n    - nvidia-cusparse-cu11==11.7.4.91\n    - nvidia-nccl-cu11==2.14.3\n    - nvidia-nvtx-cu11==11.7.91\n    - oauthlib==3.2.2\n    - omegaconf==2.1.1\n    - open-clip-torch==2.17.1\n    - opencv-contrib-python==4.3.0.36\n    - opencv-python==4.7.0.72\n    - opencv-python-headless==4.7.0.72\n    - orjson==3.9.10\n    - packaging==23.2\n    - pandas==2.0.3\n    - pillow==9.4.0\n    - pkgutil-resolve-name==1.3.10\n    - portalocker==2.8.2\n    - protobuf==3.20.3\n    - pyasn1==0.5.1\n    - pyasn1-modules==0.3.0\n    - pycocotools==2.0.7\n    - pydantic==2.5.2\n    - pydantic-core==2.14.5\n    - pydeprecate==0.3.1\n    - pydub==0.25.1\n    - pyparsing==3.1.1\n    - pyre-extensions==0.0.23\n    - python-dateutil==2.8.2\n    - python-multipart==0.0.6\n    - pytorch-lightning==1.5.0\n    - pytz==2023.3.post1\n    - pywavelets==1.4.1\n    - pyyaml==6.0.1\n    - qudida==0.0.4\n    - referencing==0.32.0\n    - regex==2023.10.3\n    - requests==2.31.0\n    - requests-oauthlib==1.3.1\n    - rpds-py==0.13.2\n    - rsa==4.9\n    - safetensors==0.2.7\n    - scikit-image==0.21.0\n    - scikit-learn==1.3.2\n    - scipy==1.9.1\n    - semantic-version==2.10.0\n    - sentencepiece==0.1.99\n    - setuptools==66.0.0\n    - share==1.0.4\n    - six==1.16.0\n    - sniffio==1.3.0\n    - starlette==0.27.0\n    - submitit==1.5.1\n    - sympy==1.12\n    - tabulate==0.9.0\n    - tensorboard==2.14.0\n    - tensorboard-data-server==0.7.2\n    - termcolor==2.4.0\n    - threadpoolctl==3.2.0\n    - tifffile==2023.7.10\n    - timm==0.6.12\n    - tokenizers==0.12.1\n    - toolz==0.12.0\n    - torch==2.0.0\n    - torchmetrics==0.6.0\n    - torchvision==0.15.1\n    - tqdm==4.65.0\n    - transformers==4.19.2\n    - triton==2.0.0\n    - typing-extensions==4.9.0\n    - typing-inspect==0.9.0\n    - tzdata==2023.3\n    - uc-micro-py==1.0.2\n    - urllib3==2.1.0\n    - uvicorn==0.24.0.post1\n    - wcwidth==0.2.12\n    - websockets==11.0.3\n    - werkzeug==3.0.1\n    - xformers==0.0.18\n    - yacs==0.1.8\n    - yarl==1.9.4\n    - zipp==3.17.0\n\n"
        },
        {
          "name": "examples",
          "type": "tree",
          "content": null
        },
        {
          "name": "iseg",
          "type": "tree",
          "content": null
        },
        {
          "name": "ldm",
          "type": "tree",
          "content": null
        },
        {
          "name": "predict.py",
          "type": "blob",
          "size": 11.6357421875,
          "content": "# Prediction interface for Cog 锔\n# https://github.com/replicate/cog/blob/main/docs/python.md\nfrom cog import BasePredictor, Input, Path\nimport os\nimport cv2\nimport time\nimport torch\nimport einops\nimport random\nimport subprocess\nimport numpy as np\nfrom cldm.ddim_hacked import DDIMSampler\nfrom cldm.model import create_model, load_state_dict\nfrom cldm.hack import disable_verbosity\nfrom datasets.data_utils import * \nfrom omegaconf import OmegaConf\n\n\nsave_memory = False\nMODEL_URL = \"https://weights.replicate.delivery/default/ali-vilab/anydoor.tar\"\nMODEL_CACHE=\"checkpoints\"\n\n\ndef download(url, dest):\n    start = time.time()\n    print(\"downloading url: \", url)\n    print(\"downloading to: \", dest)\n    subprocess.check_call([\"pget\", \"-x\", url, dest], close_fds=False)\n    print(\"downloading took: \", time.time() - start)\n\ndef process_pairs(ref_image, ref_mask, tar_image, tar_mask, max_ratio = 0.8, enable_shape_control = False):\n    # ========= Reference ===========\n    # ref expand \n    ref_box_yyxx = get_bbox_from_mask(ref_mask)\n\n    # ref filter mask \n    ref_mask_3 = np.stack([ref_mask,ref_mask,ref_mask],-1)\n    masked_ref_image = ref_image * ref_mask_3 + np.ones_like(ref_image) * 255 * (1-ref_mask_3)\n\n    y1,y2,x1,x2 = ref_box_yyxx\n    masked_ref_image = masked_ref_image[y1:y2,x1:x2,:]\n    ref_mask = ref_mask[y1:y2,x1:x2]\n\n    ratio = np.random.randint(11, 15) / 10 #11,13\n    masked_ref_image, ref_mask = expand_image_mask(masked_ref_image, ref_mask, ratio=ratio)\n    ref_mask_3 = np.stack([ref_mask,ref_mask,ref_mask],-1)\n\n    # to square and resize\n    masked_ref_image = pad_to_square(masked_ref_image, pad_value = 255, random = False)\n    masked_ref_image = cv2.resize(masked_ref_image.astype(np.uint8), (224,224) ).astype(np.uint8)\n\n    ref_mask_3 = pad_to_square(ref_mask_3 * 255, pad_value = 0, random = False)\n    ref_mask_3 = cv2.resize(ref_mask_3.astype(np.uint8), (224,224) ).astype(np.uint8)\n    ref_mask = ref_mask_3[:,:,0]\n\n    # collage aug \n    masked_ref_image_compose, ref_mask_compose =  masked_ref_image, ref_mask\n    ref_mask_3 = np.stack([ref_mask_compose,ref_mask_compose,ref_mask_compose],-1)\n    ref_image_collage = sobel(masked_ref_image_compose, ref_mask_compose/255)\n\n    # ========= Target ===========\n    tar_box_yyxx = get_bbox_from_mask(tar_mask)\n    tar_box_yyxx = expand_bbox(tar_mask, tar_box_yyxx, ratio=[1.1,1.2]) #1.1  1.3\n    tar_box_yyxx_full = tar_box_yyxx\n    \n    # crop\n    tar_box_yyxx_crop =  expand_bbox(tar_image, tar_box_yyxx, ratio=[1.3, 3.0])   \n    tar_box_yyxx_crop = box2squre(tar_image, tar_box_yyxx_crop) # crop box\n    y1,y2,x1,x2 = tar_box_yyxx_crop\n\n    cropped_target_image = tar_image[y1:y2,x1:x2,:]\n    cropped_tar_mask = tar_mask[y1:y2,x1:x2]\n\n    tar_box_yyxx = box_in_box(tar_box_yyxx, tar_box_yyxx_crop)\n    y1,y2,x1,x2 = tar_box_yyxx\n\n    # collage\n    ref_image_collage = cv2.resize(ref_image_collage.astype(np.uint8), (x2-x1, y2-y1))\n    ref_mask_compose = cv2.resize(ref_mask_compose.astype(np.uint8), (x2-x1, y2-y1))\n    ref_mask_compose = (ref_mask_compose > 128).astype(np.uint8)\n\n    collage = cropped_target_image.copy() \n    collage[y1:y2,x1:x2,:] = ref_image_collage\n\n    collage_mask = cropped_target_image.copy() * 0.0\n    collage_mask[y1:y2,x1:x2,:] = 1.0\n    if enable_shape_control:\n        collage_mask = np.stack([cropped_tar_mask,cropped_tar_mask,cropped_tar_mask],-1)\n\n    # the size before pad\n    H1, W1 = collage.shape[0], collage.shape[1]\n\n    cropped_target_image = pad_to_square(cropped_target_image, pad_value = 0, random = False).astype(np.uint8)\n    collage = pad_to_square(collage, pad_value = 0, random = False).astype(np.uint8)\n    collage_mask = pad_to_square(collage_mask, pad_value = 2, random = False).astype(np.uint8)\n\n    # the size after pad\n    H2, W2 = collage.shape[0], collage.shape[1]\n\n    cropped_target_image = cv2.resize(cropped_target_image.astype(np.uint8), (512,512)).astype(np.float32)\n    collage = cv2.resize(collage.astype(np.uint8), (512,512)).astype(np.float32)\n    collage_mask  = cv2.resize(collage_mask.astype(np.uint8), (512,512),  interpolation = cv2.INTER_NEAREST).astype(np.float32)\n    collage_mask[collage_mask == 2] = -1\n\n    masked_ref_image = masked_ref_image  / 255 \n    cropped_target_image = cropped_target_image / 127.5 - 1.0\n    collage = collage / 127.5 - 1.0 \n    collage = np.concatenate([collage, collage_mask[:,:,:1]  ] , -1)\n    \n    item = dict(ref=masked_ref_image.copy(), jpg=cropped_target_image.copy(), hint=collage.copy(), \n                extra_sizes=np.array([H1, W1, H2, W2]), \n                tar_box_yyxx_crop=np.array( tar_box_yyxx_crop ),\n                tar_box_yyxx=np.array(tar_box_yyxx_full),\n                 ) \n    return item\n\ndef crop_back( pred, tar_image,  extra_sizes, tar_box_yyxx_crop):\n    H1, W1, H2, W2 = extra_sizes\n    y1,y2,x1,x2 = tar_box_yyxx_crop    \n    pred = cv2.resize(pred, (W2, H2))\n    m = 5 # maigin_pixel\n\n    if W1 == H1:\n        tar_image[y1+m :y2-m, x1+m:x2-m, :] =  pred[m:-m, m:-m]\n        return tar_image\n\n    if W1 < W2:\n        pad1 = int((W2 - W1) / 2)\n        pad2 = W2 - W1 - pad1\n        pred = pred[:,pad1: -pad2, :]\n    else:\n        pad1 = int((H2 - H1) / 2)\n        pad2 = H2 - H1 - pad1\n        pred = pred[pad1: -pad2, :, :]\n\n    gen_image = tar_image.copy()\n    gen_image[y1+m :y2-m, x1+m:x2-m, :] =  pred[m:-m, m:-m]\n    return gen_image\n\nclass Predictor(BasePredictor):\n    def setup(self) -> None:\n        \"\"\"Load the model into memory to make running multiple predictions efficient\"\"\"\n        # if checkpoints folder does not exist, create it\n        if not os.path.exists(MODEL_CACHE):\n            download(MODEL_URL, MODEL_CACHE)\n        \n        disable_verbosity()\n        cv2.setNumThreads(0)\n        cv2.ocl.setUseOpenCL(False)\n        config = OmegaConf.load('./configs/inference.yaml')\n        model_ckpt =  config.pretrained_model\n        model_config = config.config_file\n        model = create_model(model_config).cpu()\n        model.load_state_dict(load_state_dict(model_ckpt, location='cuda'))\n        self.model = model.cuda()\n        self.ddim_sampler = DDIMSampler(model)\n\n\n    def inference_single_image(self, ref_image, ref_mask, tar_image, tar_mask, strength, ddim_steps, guidance_scale, seed, enable_shape_control):\n        item = process_pairs(ref_image, ref_mask, tar_image, tar_mask, enable_shape_control)\n        ref = item['ref'] * 255\n        tar = item['jpg'] * 127.5 + 127.5\n        hint = item['hint'] * 127.5 + 127.5\n\n        hint_image = hint[:,:,:-1]\n        hint_mask = item['hint'][:,:,-1] * 255\n        hint_mask = np.stack([hint_mask,hint_mask,hint_mask],-1)\n        ref = cv2.resize(ref.astype(np.uint8), (512,512))\n\n        seed = random.randint(0, 65535)\n        if save_memory:\n            self.model.low_vram_shift(is_diffusing=False)\n\n        ref = item['ref']\n        tar = item['jpg'] \n        hint = item['hint']\n        num_samples = 1\n        control = torch.from_numpy(hint.copy()).float().cuda() \n        control = torch.stack([control for _ in range(num_samples)], dim=0)\n        control = einops.rearrange(control, 'b h w c -> b c h w').clone()\n        clip_input = torch.from_numpy(ref.copy()).float().cuda() \n        clip_input = torch.stack([clip_input for _ in range(num_samples)], dim=0)\n        clip_input = einops.rearrange(clip_input, 'b h w c -> b c h w').clone()\n        guess_mode = False\n        H,W = 512,512\n        cond = {\"c_concat\": [control], \"c_crossattn\": [self.model.get_learned_conditioning( clip_input )]}\n        un_cond = {\"c_concat\": None if guess_mode else [control], \"c_crossattn\": [self.model.get_learned_conditioning([torch.zeros((1,3,224,224))] * num_samples)]}\n        shape = (4, H // 8, W // 8)\n\n        if save_memory:\n            self.model.low_vram_shift(is_diffusing=True)\n\n        # ====\n        num_samples = 1 #gr.Slider(label=\"Images\", minimum=1, maximum=12, value=1, step=1)\n        image_resolution = 512  #gr.Slider(label=\"Image Resolution\", minimum=256, maximum=768, value=512, step=64)\n        #strength = 1  #gr.Slider(label=\"Control Strength\", minimum=0.0, maximum=2.0, value=1.0, step=0.01)\n        guess_mode = False #gr.Checkbox(label='Guess Mode', value=False)\n        #detect_resolution = 512  #gr.Slider(label=\"Segmentation Resolution\", minimum=128, maximum=1024, value=512, step=1)\n        #ddim_steps = 50 #gr.Slider(label=\"Steps\", minimum=1, maximum=100, value=20, step=1)\n        scale = guidance_scale  #gr.Slider(label=\"Guidance Scale\", minimum=0.1, maximum=30.0, value=9.0, step=0.1)\n        #seed = -1  #gr.Slider(label=\"Seed\", minimum=-1, maximum=2147483647, step=1, randomize=True)\n        eta = 0.0 #gr.Number(label=\"eta (DDIM)\", value=0.0)\n\n        self.model.control_scales = [strength * (0.825 ** float(12 - i)) for i in range(13)] if guess_mode else ([strength] * 13)\n        samples, intermediates = self.ddim_sampler.sample(ddim_steps, num_samples,\n                                                        shape, cond, verbose=False, eta=eta,\n                                                        unconditional_guidance_scale=scale,\n                                                        unconditional_conditioning=un_cond)\n        if save_memory:\n            self.model.low_vram_shift(is_diffusing=False)\n\n        x_samples = self.model.decode_first_stage(samples)\n        x_samples = (einops.rearrange(x_samples, 'b c h w -> b h w c') * 127.5 + 127.5).cpu().numpy()\n\n        result = x_samples[0][:,:,::-1]\n        result = np.clip(result,0,255)\n\n        pred = x_samples[0]\n        pred = np.clip(pred,0,255)[1:,:,:]\n        sizes = item['extra_sizes']\n        tar_box_yyxx_crop = item['tar_box_yyxx_crop'] \n        gen_image = crop_back(pred, tar_image, sizes, tar_box_yyxx_crop) \n        return gen_image\n\n    def predict(\n        self,\n        reference_image_path: Path = Input(description=\"Source Image\"),\n        reference_image_mask: Path = Input(description=\"Source Image\"),\n        bg_image_path: Path = Input(description=\"Target Image\"),\n        bg_mask_path: Path = Input(description=\"Target Image mask\"),\n        control_strength: float = Input(description=\"Control Strength\", default=1.0, ge=0.0, le=2.0),\n        steps: int = Input(description=\"Steps\", default=50, ge=1, le=100),\n        guidance_scale: float = Input(description=\"Guidance Scale\", default=4.5, ge=0.1, le=30.0),\n        enable_shape_control: bool = Input(description=\"Enable Shape Control\", default=False),\n        seed: int = Input(description=\"Random seed. Leave blank to randomize the seed\", default=None),\n    ) -> Path:\n        \"\"\"Run a single prediction on the model\"\"\"\n        if seed is None:\n            seed = int.from_bytes(os.urandom(4), \"big\")\n        print(f\"Using seed: {seed}\")\n\n        save_path = \"/tmp/output.png\"\n        image = cv2.imread(str(reference_image_path), cv2.IMREAD_UNCHANGED)\n        if image.shape[2] == 1:\n            image = cv2.cvtColor(image, cv2.COLOR_GRAY2BGR)\n        elif image.shape[2] == 4:\n            image = cv2.cvtColor(image, cv2.COLOR_BGRA2BGR)\n        ref_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        ref_mask = (cv2.imread(str(reference_image_mask))[:,:,-1] > 128).astype(np.uint8)\n\n        # background image\n        back_image = cv2.imread(str(bg_image_path)).astype(np.uint8)\n        back_image = cv2.cvtColor(back_image, cv2.COLOR_BGR2RGB)\n\n       # background mask \n        tar_mask = cv2.imread(str(bg_mask_path))[:,:,0] > 128\n        tar_mask = tar_mask.astype(np.uint8)\n        \n        gen_image = self.inference_single_image(\n            ref_image,ref_mask, back_image.copy(), tar_mask,\n            control_strength, steps, guidance_scale, seed, enable_shape_control)\n        h,w = back_image.shape[0], back_image.shape[0]\n        ref_image = cv2.resize(ref_image, (w,h))\n        vis_image = cv2.hconcat([gen_image])\n        cv2.imwrite(save_path, vis_image [:,:,::-1])\n\n        return Path(save_path)"
        },
        {
          "name": "readme.md",
          "type": "blob",
          "size": 6.48828125,
          "content": "<p align=\"center\">\n\n  <h2 align=\"center\">AnyDoor: Zero-shot Object-level Image Customization</h2>\n  <p align=\"center\">\n    <a href=\"https://xavierchen34.github.io/\"><strong>Xi Chen</strong></a>\n    路\n    <a href=\"https://scholar.google.com/citations?user=JYVCn3AAAAAJ&hl=en\"><strong>Lianghua Huang</strong></a>\n    路\n    <a href=\"https://scholar.google.com/citations?user=8zksQb4AAAAJ&hl=zh-CN\"><strong>Yu Liu</strong></a>\n    路\n    <a href=\"https://shenyujun.github.io/\"><strong>Yujun Shen</strong></a>\n    路\n    <a href=\"https://scholar.google.com/citations?user=7LhjCn0AAAAJ&hl=en\"><strong>Deli Zhao</strong></a>\n    路\n    <a href=\"https://hszhao.github.io/\"><strong>Hengshuang Zhao</strong></a>\n    <br>\n    <br>\n        <a href=\"https://arxiv.org/abs/2307.09481\"><img src='https://img.shields.io/badge/arXiv-AnyDoor-red' alt='Paper PDF'></a>\n        <a href='https://ali-vilab.github.io/AnyDoor-Page/'><img src='https://img.shields.io/badge/Project_Page-AnyDoor-green' alt='Project Page'></a>\n        <a href='https://modelscope.cn/studios/damo/AnyDoor-online/summary'><img src='https://img.shields.io/badge/ModelScope-AnyDoor-yellow'></a>\n        <a href='https://huggingface.co/spaces/xichenhku/AnyDoor-online'><img src='https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue'></a>\n        <a href='https://replicate.com/lucataco/anydoor'><img src='https://replicate.com/lucataco/anydoor/badge'></a>\n    <br>\n    <b>The University of Hong Kong &nbsp; | &nbsp;  Alibaba Group  | &nbsp;  Ant Group </b>\n  </p>\n  \n  <table align=\"center\">\n    <tr>\n    <td>\n      <img src=\"assets/Figures/Teaser.png\">\n    </td>\n    </tr>\n  </table>\n\n## News\n* **[2023.12.17]** Release train & inference & demo code, and pretrained checkpoint.\n* **[2023.12.24]**  Support online demo on [ModelScope](https://modelscope.cn/studios/damo/AnyDoor-online/summary) and \n [HuggingFace](https://huggingface.co/spaces/xichenhku/AnyDoor-online).\n* **[Soon]** Release the new version paper.\n* **[On-going]** Scale-up the training data and release stronger models as the foundaition model for downstream region-to-region generation tasks.\n* **[On-going]** Release specific-designed models for downstream tasks like virtual tryon, face swap, text and logo transfer, etc.\n\n\n## Installation\nInstall with `conda`: \n```bash\nconda env create -f environment.yaml\nconda activate anydoor\n```\nor `pip`:\n```bash\npip install -r requirements.txt\n```\nAdditionally, for training, you need to install panopticapi, pycocotools, and lvis-api.\n```bash\npip install git+https://github.com/cocodataset/panopticapi.git\n\npip install pycocotools -i https://pypi.douban.com/simple\n\npip install lvis\n```\n## Download Checkpoints\nDownload AnyDoor checkpoint: \n* [ModelScope](https://modelscope.cn/models/damo/AnyDoor/files)\n* [HuggingFace](https://huggingface.co/spaces/xichenhku/AnyDoor/tree/main)\n\n**Note:** We include all the optimizer params for Adam, so the checkpoint is big. You could only keep the \"state_dict\" to make it much smaller.\n\n\nDownload DINOv2 checkpoint and revise `/configs/anydoor.yaml` for the path (line 83)\n* URL: https://github.com/facebookresearch/dinov2?tab=readme-ov-file\n\nDownload Stable Diffusion V2.1 if you want to train from scratch.\n* URL: https://huggingface.co/stabilityai/stable-diffusion-2-1/tree/main\n\n\n## Inference\nWe provide inference code in `run_inference.py` (from Line 222 - ) for both inference single image and inference a dataset (VITON-HD Test). You should modify the data path and run the following code. The generated results are provided in `examples/TestDreamBooth/GEN` for single image, and `VITONGEN` for VITON-HD Test.\n\n```bash\npython run_inference.py\n```\nThe inferenced results on VITON-Test would be like [garment, ground truth, generation].\n\n*Noticing that AnyDoor does not contain any specific design/tuning for tryon, we think it would be helpful to add skeleton infos or warped garment, and tune on tryon data to make it better :)*\n  <table align=\"center\">\n    <tr>\n    <td>\n      <img src=\"assets/Figures/tryon.png\">\n    </td>\n    </tr>\n  </table>\n\n\nOur evaluation data for DreamBooth an COCOEE coud be downloaded at Google Drive:\n* URL: [to be released]\n\n\n\n\n\n## Gradio demo \nCurrently, we suport local gradio demo. To launch it, you should firstly modify `/configs/demo.yaml` for the path to the pretrained model, and `/configs/anydoor.yaml` for the path to DINOv2(line 83). \n\nAfterwards, run the script:\n```bash\npython run_gradio_demo.py\n```\nThe gradio demo would look like the UI shown below:\n\n*  This version requires users to annotate the mask of the target object, too coarse mask would influence the generation quality. We plan to add mask refine module or interactive segmentation modules in the demo.\n\n*  We provide an segmentation module to refine the user annotated reference mask. We could chose to disable it by setting  `use_interactive_seg: False` in `/configs/demo.yaml`.\n\n<table align=\"center\">\n  <tr>\n  <td>\n    <img src=\"assets/Figures/gradio.png\">\n  </td>\n  </tr>\n</table>\n\n## Train\n\n### Prepare datasets\n* Download the datasets that present in `/configs/datasets.yaml` and modify the corresponding paths.\n* You could prepare you own datasets according to the formates of files in `./datasets`.\n* If you use UVO dataset, you need to process the json following `./datasets/Preprocess/uvo_process.py`\n* You could refer to `run_dataset_debug.py` to verify you data is correct.\n\n\n### Prepare initial weight\n* If your would like to train from scratch, convert the downloaded SD weights to control copy by running:\n```bash\nsh ./scripts/convert_weight.sh  \n```\n### Start training\n* Modify the training hyper-parameters in `run_train_anydoor.py` Line 26-34 according to your training resources. We verify that using 2-A100 GPUs with batch accumulation=1 could get satisfactory results after 300,000 iterations.\n\n\n* Start training by executing: \n```bash\nsh ./scripts/train.sh  \n```\n\n##  Community Contributions\n@bdsqlsz\n\n* AnyDoor for windows: https://github.com/sdbds/AnyDoor-for-windows\n* Pruned model: https://modelscope.cn/models/bdsqlsz/AnyDoor-Pruned/summary\n\n## Acknowledgements\nThis project is developped on the codebase of [ControlNet](https://github.com/lllyasviel/ControlNet). We  appreciate this great work! \n\n\n## Citation\nIf you find this codebase useful for your research, please use the following entry.\n```BibTeX\n@article{chen2023anydoor,\n  title={Anydoor: Zero-shot object-level image customization},\n  author={Chen, Xi and Huang, Lianghua and Liu, Yu and Shen, Yujun and Zhao, Deli and Zhao, Hengshuang},\n  journal={arXiv preprint arXiv:2307.09481},\n  year={2023}\n}\n```\n"
        },
        {
          "name": "requirements.txt",
          "type": "blob",
          "size": 0.427734375,
          "content": "albumentations==1.3.0\neinops==0.3.0\nfvcore==0.1.5.post20221221\ngradio==3.39.0\nnumpy==1.23.1\nomegaconf==2.1.1\nopen_clip_torch==2.17.1\nopencv_contrib_python==4.3.0.36\nopencv_python==4.7.0.72\nopencv_python_headless==4.7.0.72\nPillow==9.4.0\npytorch_lightning==1.5.0\nsafetensors==0.2.7\nscipy==1.9.1\nsetuptools==66.0.0\nshare==1.0.4\nsubmitit==1.5.1\ntimm==0.6.12\ntorch==2.0.0\ntorchmetrics==0.6.0\ntqdm==4.65.0\ntransformers==4.19.2\nxformers==0.0.18\n"
        },
        {
          "name": "run_dataset_debug.py",
          "type": "blob",
          "size": 2.171875,
          "content": "from datasets.ytb_vos import YoutubeVOSDataset\nfrom datasets.ytb_vis import YoutubeVISDataset\nfrom datasets.saliency_modular import SaliencyDataset\nfrom datasets.vipseg import VIPSegDataset\nfrom datasets.mvimagenet import MVImageNetDataset\nfrom datasets.sam import SAMDataset\nfrom datasets.dreambooth import DreamBoothDataset\nfrom datasets.uvo import UVODataset\nfrom datasets.uvo_val import UVOValDataset\nfrom datasets.mose import MoseDataset\nfrom datasets.vitonhd import VitonHDDataset\nfrom datasets.fashiontryon import FashionTryonDataset\nfrom datasets.lvis import LvisDataset\nfrom torch.utils.data import ConcatDataset\nfrom torch.utils.data import DataLoader\nimport numpy as np \nimport cv2\nfrom omegaconf import OmegaConf\n\n# Datasets\nDConf = OmegaConf.load('./configs/datasets.yaml')\ndataset1 = YoutubeVOSDataset(**DConf.Train.YoutubeVOS)  \ndataset2 = SaliencyDataset(**DConf.Train.Saliency) \ndataset3 = VIPSegDataset(**DConf.Train.VIPSeg) \ndataset4 = YoutubeVISDataset(**DConf.Train.YoutubeVIS) \ndataset5 = MVImageNetDataset(**DConf.Train.MVImageNet)\ndataset6 = SAMDataset(**DConf.Train.SAM)\ndataset7 = UVODataset(**DConf.Train.UVO.train)\ndataset8 = VitonHDDataset(**DConf.Train.VitonHD)\ndataset9 = UVOValDataset(**DConf.Train.UVO.val)\ndataset10 = MoseDataset(**DConf.Train.Mose)\ndataset11 = FashionTryonDataset(**DConf.Train.FashionTryon)\ndataset12 = LvisDataset(**DConf.Train.Lvis)\n\ndataset = dataset5\n\n\ndef vis_sample(item):\n    ref = item['ref']* 255\n    tar = item['jpg'] * 127.5 + 127.5\n    hint = item['hint'] * 127.5 + 127.5\n    step = item['time_steps']\n    print(ref.shape, tar.shape, hint.shape, step.shape)\n\n    ref = ref[0].numpy()\n    tar = tar[0].numpy()\n    hint_image = hint[0, :,:,:-1].numpy()\n    hint_mask = hint[0, :,:,-1].numpy()\n    hint_mask = np.stack([hint_mask,hint_mask,hint_mask],-1)\n    ref = cv2.resize(ref.astype(np.uint8), (512,512))\n    vis = cv2.hconcat([ref.astype(np.float32), hint_image.astype(np.float32), hint_mask.astype(np.float32), tar.astype(np.float32) ])\n    cv2.imwrite('sample_vis.jpg',vis[:,:,::-1])\n\n\ndataloader = DataLoader(dataset, num_workers=8, batch_size=4, shuffle=True)\nprint('len dataloader: ', len(dataloader))\nfor data in dataloader:  \n    vis_sample(data) \n\n\n"
        },
        {
          "name": "run_gradio_demo.py",
          "type": "blob",
          "size": 12.8642578125,
          "content": "import cv2\nimport einops\nimport numpy as np\nimport torch\nimport random\nimport gradio as gr\nimport os\nimport albumentations as A\nfrom PIL import Image\nimport torchvision.transforms as T\nfrom datasets.data_utils import * \nfrom cldm.model import create_model, load_state_dict\nfrom cldm.ddim_hacked import DDIMSampler\nfrom omegaconf import OmegaConf\nfrom cldm.hack import disable_verbosity, enable_sliced_attention\n\n\ncv2.setNumThreads(0)\ncv2.ocl.setUseOpenCL(False)\n\nsave_memory = False\ndisable_verbosity()\nif save_memory:\n    enable_sliced_attention()\n\n\nconfig = OmegaConf.load('./configs/demo.yaml')\nmodel_ckpt =  config.pretrained_model\nmodel_config = config.config_file\nuse_interactive_seg = config.config_file\n\nmodel = create_model(model_config ).cpu()\nmodel.load_state_dict(load_state_dict(model_ckpt, location='cuda'))\nmodel = model.cuda()\nddim_sampler = DDIMSampler(model)\n\nif use_interactive_seg:\n    from iseg.coarse_mask_refine_util import BaselineModel\n    model_path = './iseg/coarse_mask_refine.pth'\n    iseg_model = BaselineModel().eval()\n    weights = torch.load(model_path , map_location='cpu')['state_dict']\n    iseg_model.load_state_dict(weights, strict= True)\n\n\ndef process_image_mask(image_np, mask_np):\n    img = torch.from_numpy(image_np.transpose((2, 0, 1)))\n    img = img.float().div(255).unsqueeze(0)\n    mask = torch.from_numpy(mask_np).float().unsqueeze(0).unsqueeze(0)\n    pred = iseg_model(img, mask)['instances'][0,0].detach().numpy() > 0.5 \n    return pred.astype(np.uint8)\n\ndef crop_back( pred, tar_image,  extra_sizes, tar_box_yyxx_crop):\n    H1, W1, H2, W2 = extra_sizes\n    y1,y2,x1,x2 = tar_box_yyxx_crop    \n    pred = cv2.resize(pred, (W2, H2))\n    m = 3 # maigin_pixel\n\n    if W1 == H1:\n        tar_image[y1+m :y2-m, x1+m:x2-m, :] =  pred[m:-m, m:-m]\n        return tar_image\n\n    if W1 < W2:\n        pad1 = int((W2 - W1) / 2)\n        pad2 = W2 - W1 - pad1\n        pred = pred[:,pad1: -pad2, :]\n    else:\n        pad1 = int((H2 - H1) / 2)\n        pad2 = H2 - H1 - pad1\n        pred = pred[pad1: -pad2, :, :]\n    tar_image[y1+m :y2-m, x1+m:x2-m, :] =  pred[m:-m, m:-m]\n    return tar_image\n\ndef inference_single_image(ref_image, \n                           ref_mask, \n                           tar_image, \n                           tar_mask, \n                           strength, \n                           ddim_steps, \n                           scale, \n                           seed,\n                           enable_shape_control,\n                           ):\n    raw_background = tar_image.copy()\n    item = process_pairs(ref_image, ref_mask, tar_image, tar_mask, enable_shape_control = enable_shape_control)\n\n    ref = item['ref']\n    hint = item['hint']\n    num_samples = 1\n\n    control = torch.from_numpy(hint.copy()).float().cuda() \n    control = torch.stack([control for _ in range(num_samples)], dim=0)\n    control = einops.rearrange(control, 'b h w c -> b c h w').clone()\n\n\n    clip_input = torch.from_numpy(ref.copy()).float().cuda() \n    clip_input = torch.stack([clip_input for _ in range(num_samples)], dim=0)\n    clip_input = einops.rearrange(clip_input, 'b h w c -> b c h w').clone()\n\n    H,W = 512,512\n\n    cond = {\"c_concat\": [control], \"c_crossattn\": [model.get_learned_conditioning( clip_input )]}\n    un_cond = {\"c_concat\": [control], \n               \"c_crossattn\": [model.get_learned_conditioning([torch.zeros((1,3,224,224))] * num_samples)]}\n    shape = (4, H // 8, W // 8)\n\n    if save_memory:\n        model.low_vram_shift(is_diffusing=True)\n\n    model.control_scales = ([strength] * 13)\n    samples, _ = ddim_sampler.sample(ddim_steps, num_samples,\n                                     shape, cond, verbose=False, eta=0,\n                                     unconditional_guidance_scale=scale,\n                                     unconditional_conditioning=un_cond)\n\n    if save_memory:\n        model.low_vram_shift(is_diffusing=False)\n\n    x_samples = model.decode_first_stage(samples)\n    x_samples = (einops.rearrange(x_samples, 'b c h w -> b h w c') * 127.5 + 127.5).cpu().numpy()\n\n    result = x_samples[0][:,:,::-1]\n    result = np.clip(result,0,255)\n\n    pred = x_samples[0]\n    pred = np.clip(pred,0,255)[1:,:,:]\n    sizes = item['extra_sizes']\n    tar_box_yyxx_crop = item['tar_box_yyxx_crop'] \n    tar_image = crop_back(pred, tar_image, sizes, tar_box_yyxx_crop) \n\n    # keep background unchanged\n    y1,y2,x1,x2 = item['tar_box_yyxx']\n    raw_background[y1:y2, x1:x2, :] = tar_image[y1:y2, x1:x2, :]\n    return raw_background\n\n\ndef process_pairs(ref_image, ref_mask, tar_image, tar_mask, max_ratio = 0.8, enable_shape_control = False):\n    # ========= Reference ===========\n    # ref expand \n    ref_box_yyxx = get_bbox_from_mask(ref_mask)\n\n    # ref filter mask \n    ref_mask_3 = np.stack([ref_mask,ref_mask,ref_mask],-1)\n    masked_ref_image = ref_image * ref_mask_3 + np.ones_like(ref_image) * 255 * (1-ref_mask_3)\n\n    y1,y2,x1,x2 = ref_box_yyxx\n    masked_ref_image = masked_ref_image[y1:y2,x1:x2,:]\n    ref_mask = ref_mask[y1:y2,x1:x2]\n\n    ratio = np.random.randint(11, 15) / 10 #11,13\n    masked_ref_image, ref_mask = expand_image_mask(masked_ref_image, ref_mask, ratio=ratio)\n    ref_mask_3 = np.stack([ref_mask,ref_mask,ref_mask],-1)\n\n    # to square and resize\n    masked_ref_image = pad_to_square(masked_ref_image, pad_value = 255, random = False)\n    masked_ref_image = cv2.resize(masked_ref_image.astype(np.uint8), (224,224) ).astype(np.uint8)\n\n    ref_mask_3 = pad_to_square(ref_mask_3 * 255, pad_value = 0, random = False)\n    ref_mask_3 = cv2.resize(ref_mask_3.astype(np.uint8), (224,224) ).astype(np.uint8)\n    ref_mask = ref_mask_3[:,:,0]\n\n    # collage aug \n    masked_ref_image_compose, ref_mask_compose =  masked_ref_image, ref_mask\n    ref_mask_3 = np.stack([ref_mask_compose,ref_mask_compose,ref_mask_compose],-1)\n    ref_image_collage = sobel(masked_ref_image_compose, ref_mask_compose/255)\n\n    # ========= Target ===========\n    tar_box_yyxx = get_bbox_from_mask(tar_mask)\n    tar_box_yyxx = expand_bbox(tar_mask, tar_box_yyxx, ratio=[1.1,1.2]) #1.1  1.3\n    tar_box_yyxx_full = tar_box_yyxx\n    \n    # crop\n    tar_box_yyxx_crop =  expand_bbox(tar_image, tar_box_yyxx, ratio=[1.3, 3.0])   \n    tar_box_yyxx_crop = box2squre(tar_image, tar_box_yyxx_crop) # crop box\n    y1,y2,x1,x2 = tar_box_yyxx_crop\n\n    cropped_target_image = tar_image[y1:y2,x1:x2,:]\n    cropped_tar_mask = tar_mask[y1:y2,x1:x2]\n\n    tar_box_yyxx = box_in_box(tar_box_yyxx, tar_box_yyxx_crop)\n    y1,y2,x1,x2 = tar_box_yyxx\n\n    # collage\n    ref_image_collage = cv2.resize(ref_image_collage.astype(np.uint8), (x2-x1, y2-y1))\n    ref_mask_compose = cv2.resize(ref_mask_compose.astype(np.uint8), (x2-x1, y2-y1))\n    ref_mask_compose = (ref_mask_compose > 128).astype(np.uint8)\n\n    collage = cropped_target_image.copy() \n    collage[y1:y2,x1:x2,:] = ref_image_collage\n\n    collage_mask = cropped_target_image.copy() * 0.0\n    collage_mask[y1:y2,x1:x2,:] = 1.0\n    if enable_shape_control:\n        collage_mask = np.stack([cropped_tar_mask,cropped_tar_mask,cropped_tar_mask],-1)\n\n    # the size before pad\n    H1, W1 = collage.shape[0], collage.shape[1]\n\n    cropped_target_image = pad_to_square(cropped_target_image, pad_value = 0, random = False).astype(np.uint8)\n    collage = pad_to_square(collage, pad_value = 0, random = False).astype(np.uint8)\n    collage_mask = pad_to_square(collage_mask, pad_value = 2, random = False).astype(np.uint8)\n\n    # the size after pad\n    H2, W2 = collage.shape[0], collage.shape[1]\n\n    cropped_target_image = cv2.resize(cropped_target_image.astype(np.uint8), (512,512)).astype(np.float32)\n    collage = cv2.resize(collage.astype(np.uint8), (512,512)).astype(np.float32)\n    collage_mask  = cv2.resize(collage_mask.astype(np.uint8), (512,512),  interpolation = cv2.INTER_NEAREST).astype(np.float32)\n    collage_mask[collage_mask == 2] = -1\n\n    masked_ref_image = masked_ref_image  / 255 \n    cropped_target_image = cropped_target_image / 127.5 - 1.0\n    collage = collage / 127.5 - 1.0 \n    collage = np.concatenate([collage, collage_mask[:,:,:1]  ] , -1)\n    \n    item = dict(ref=masked_ref_image.copy(), jpg=cropped_target_image.copy(), hint=collage.copy(), \n                extra_sizes=np.array([H1, W1, H2, W2]), \n                tar_box_yyxx_crop=np.array( tar_box_yyxx_crop ),\n                tar_box_yyxx=np.array(tar_box_yyxx_full),\n                 ) \n    return item\n\n\nref_dir='./examples/Gradio/FG'\nimage_dir='./examples/Gradio/BG'\nref_list=[os.path.join(ref_dir,file) for file in os.listdir(ref_dir) if '.jpg' in file or '.png' in file or '.jpeg' in file ]\nref_list.sort()\nimage_list=[os.path.join(image_dir,file) for file in os.listdir(image_dir) if '.jpg' in file or '.png' in file or '.jpeg' in file]\nimage_list.sort()\n\ndef mask_image(image, mask):\n    blanc = np.ones_like(image) * 255\n    mask = np.stack([mask,mask,mask],-1) / 255\n    masked_image = mask * ( 0.5 * blanc + 0.5 * image) + (1-mask) * image\n    return masked_image.astype(np.uint8)\n\ndef run_local(base,\n              ref,\n              *args):\n    image = base[\"image\"].convert(\"RGB\")\n    mask = base[\"mask\"].convert(\"L\")\n    ref_image = ref[\"image\"].convert(\"RGB\")\n    ref_mask = ref[\"mask\"].convert(\"L\")\n    image = np.asarray(image)\n    mask = np.asarray(mask)\n    mask = np.where(mask > 128, 1, 0).astype(np.uint8)\n    ref_image = np.asarray(ref_image)\n    ref_mask = np.asarray(ref_mask)\n    ref_mask = np.where(ref_mask > 128, 1, 0).astype(np.uint8)\n\n    if ref_mask.sum() == 0:\n        raise gr.Error('No mask for the reference image.')\n\n    if mask.sum() == 0:\n        raise gr.Error('No mask for the background image.')\n\n    if reference_mask_refine:\n        ref_mask = process_image_mask(ref_image, ref_mask)\n\n    synthesis = inference_single_image(ref_image.copy(), ref_mask.copy(), image.copy(), mask.copy(), *args)\n    synthesis = torch.from_numpy(synthesis).permute(2, 0, 1)\n    synthesis = synthesis.permute(1, 2, 0).numpy()\n    return [synthesis]\n\n\n\nwith gr.Blocks() as demo:\n    with gr.Column():\n        gr.Markdown(\"#  Play with AnyDoor to Teleport your Target Objects! \")\n        with gr.Row():\n            baseline_gallery = gr.Gallery(label='Output', show_label=True, elem_id=\"gallery\", columns=1, height=768)\n            with gr.Accordion(\"Advanced Option\", open=True):\n                num_samples = 1\n                strength = gr.Slider(label=\"Control Strength\", minimum=0.0, maximum=2.0, value=1.0, step=0.01)\n                ddim_steps = gr.Slider(label=\"Steps\", minimum=1, maximum=100, value=30, step=1)\n                scale = gr.Slider(label=\"Guidance Scale\", minimum=0.1, maximum=30.0, value=4.5, step=0.1)\n                seed = gr.Slider(label=\"Seed\", minimum=-1, maximum=999999999, step=1, value=-1)\n                reference_mask_refine = gr.Checkbox(label='Reference Mask Refine', value=False, interactive = True)\n                enable_shape_control = gr.Checkbox(label='Enable Shape Control', value=False, interactive = True)\n                \n                gr.Markdown(\"### Guidelines\")\n                gr.Markdown(\" Higher guidance-scale makes higher fidelity, while lower one makes more harmonized blending.\")\n                gr.Markdown(\" Users should annotate the mask of the target object, too coarse mask would lead to bad generation.\\\n                              Reference Mask Refine provides a segmentation model to refine the coarse mask. \")\n                gr.Markdown(\" Enable shape control means the generation results would consider user-drawn masks to control the shape & pose; otherwise it \\\n                              considers the location and size to adjust automatically.\")\n\n    \n        gr.Markdown(\"# Upload / Select Images for the Background (left) and Reference Object (right)\")\n        gr.Markdown(\"### You could draw coarse masks on the background to indicate the desired location and shape.\")\n        gr.Markdown(\"### <u>Do not forget</u> to annotate the target object on the reference image.\")\n        with gr.Row():\n            base = gr.Image(label=\"Background\", source=\"upload\", tool=\"sketch\", type=\"pil\", height=512, brush_color='#FFFFFF', mask_opacity=0.5)\n            ref = gr.Image(label=\"Reference\", source=\"upload\", tool=\"sketch\", type=\"pil\", height=512, brush_color='#FFFFFF', mask_opacity=0.5)\n        run_local_button = gr.Button(label=\"Generate\", value=\"Run\")\n\n        with gr.Row():\n            with gr.Column():\n                gr.Examples(image_list, inputs=[base],label=\"Examples - Background Image\",examples_per_page=16)\n            with gr.Column():\n                gr.Examples(ref_list, inputs=[ref],label=\"Examples - Reference Object\",examples_per_page=16)\n        \n    run_local_button.click(fn=run_local, \n                           inputs=[base, \n                                   ref, \n                                   strength, \n                                   ddim_steps, \n                                   scale, \n                                   seed,\n                                   enable_shape_control, \n                                   ], \n                           outputs=[baseline_gallery]\n                        )\n\ndemo.launch(server_name=\"0.0.0.0\")\n"
        },
        {
          "name": "run_inference.py",
          "type": "blob",
          "size": 11.283203125,
          "content": "import cv2\nimport einops\nimport numpy as np\nimport torch\nimport random\nfrom pytorch_lightning import seed_everything\nfrom cldm.model import create_model, load_state_dict\nfrom cldm.ddim_hacked import DDIMSampler\nfrom cldm.hack import disable_verbosity, enable_sliced_attention\nfrom datasets.data_utils import * \ncv2.setNumThreads(0)\ncv2.ocl.setUseOpenCL(False)\nimport albumentations as A\nfrom omegaconf import OmegaConf\nfrom PIL import Image\n\n\nsave_memory = False\ndisable_verbosity()\nif save_memory:\n    enable_sliced_attention()\n\n\nconfig = OmegaConf.load('./configs/inference.yaml')\nmodel_ckpt =  config.pretrained_model\nmodel_config = config.config_file\n\nmodel = create_model(model_config ).cpu()\nmodel.load_state_dict(load_state_dict(model_ckpt, location='cuda'))\nmodel = model.cuda()\nddim_sampler = DDIMSampler(model)\n\n\n\ndef aug_data_mask(image, mask):\n    transform = A.Compose([\n        A.HorizontalFlip(p=0.5),\n        A.RandomBrightnessContrast(p=0.5),\n        ])\n    transformed = transform(image=image.astype(np.uint8), mask = mask)\n    transformed_image = transformed[\"image\"]\n    transformed_mask = transformed[\"mask\"]\n    return transformed_image, transformed_mask\n\n\ndef process_pairs(ref_image, ref_mask, tar_image, tar_mask):\n    # ========= Reference ===========\n    # ref expand \n    ref_box_yyxx = get_bbox_from_mask(ref_mask)\n\n    # ref filter mask \n    ref_mask_3 = np.stack([ref_mask,ref_mask,ref_mask],-1)\n    masked_ref_image = ref_image * ref_mask_3 + np.ones_like(ref_image) * 255 * (1-ref_mask_3)\n\n    y1,y2,x1,x2 = ref_box_yyxx\n    masked_ref_image = masked_ref_image[y1:y2,x1:x2,:]\n    ref_mask = ref_mask[y1:y2,x1:x2]\n\n\n    ratio = np.random.randint(12, 13) / 10\n    masked_ref_image, ref_mask = expand_image_mask(masked_ref_image, ref_mask, ratio=ratio)\n    ref_mask_3 = np.stack([ref_mask,ref_mask,ref_mask],-1)\n\n    # to square and resize\n    masked_ref_image = pad_to_square(masked_ref_image, pad_value = 255, random = False)\n    masked_ref_image = cv2.resize(masked_ref_image, (224,224) ).astype(np.uint8)\n\n    ref_mask_3 = pad_to_square(ref_mask_3 * 255, pad_value = 0, random = False)\n    ref_mask_3 = cv2.resize(ref_mask_3, (224,224) ).astype(np.uint8)\n    ref_mask = ref_mask_3[:,:,0]\n\n    # ref aug \n    masked_ref_image_aug = masked_ref_image #aug_data(masked_ref_image) \n\n    # collage aug \n    masked_ref_image_compose, ref_mask_compose = masked_ref_image, ref_mask #aug_data_mask(masked_ref_image, ref_mask) \n    masked_ref_image_aug = masked_ref_image_compose.copy()\n    ref_mask_3 = np.stack([ref_mask_compose,ref_mask_compose,ref_mask_compose],-1)\n    ref_image_collage = sobel(masked_ref_image_compose, ref_mask_compose/255)\n\n    # ========= Target ===========\n    tar_box_yyxx = get_bbox_from_mask(tar_mask)\n    tar_box_yyxx = expand_bbox(tar_mask, tar_box_yyxx, ratio=[1.1,1.2])\n\n    # crop\n    tar_box_yyxx_crop =  expand_bbox(tar_image, tar_box_yyxx, ratio=[1.5, 3])    #1.2 1.6\n    tar_box_yyxx_crop = box2squre(tar_image, tar_box_yyxx_crop) # crop box\n    y1,y2,x1,x2 = tar_box_yyxx_crop\n\n    cropped_target_image = tar_image[y1:y2,x1:x2,:]\n    tar_box_yyxx = box_in_box(tar_box_yyxx, tar_box_yyxx_crop)\n    y1,y2,x1,x2 = tar_box_yyxx\n\n    # collage\n    ref_image_collage = cv2.resize(ref_image_collage, (x2-x1, y2-y1))\n    ref_mask_compose = cv2.resize(ref_mask_compose.astype(np.uint8), (x2-x1, y2-y1))\n    ref_mask_compose = (ref_mask_compose > 128).astype(np.uint8)\n\n    collage = cropped_target_image.copy() \n    collage[y1:y2,x1:x2,:] = ref_image_collage\n\n    collage_mask = cropped_target_image.copy() * 0.0\n    collage_mask[y1:y2,x1:x2,:] = 1.0\n\n    # the size before pad\n    H1, W1 = collage.shape[0], collage.shape[1]\n    cropped_target_image = pad_to_square(cropped_target_image, pad_value = 0, random = False).astype(np.uint8)\n    collage = pad_to_square(collage, pad_value = 0, random = False).astype(np.uint8)\n    collage_mask = pad_to_square(collage_mask, pad_value = -1, random = False).astype(np.uint8)\n\n    # the size after pad\n    H2, W2 = collage.shape[0], collage.shape[1]\n    cropped_target_image = cv2.resize(cropped_target_image, (512,512)).astype(np.float32)\n    collage = cv2.resize(collage, (512,512)).astype(np.float32)\n    collage_mask  = (cv2.resize(collage_mask, (512,512)).astype(np.float32) > 0.5).astype(np.float32)\n\n    masked_ref_image_aug = masked_ref_image_aug  / 255 \n    cropped_target_image = cropped_target_image / 127.5 - 1.0\n    collage = collage / 127.5 - 1.0 \n    collage = np.concatenate([collage, collage_mask[:,:,:1]  ] , -1)\n\n    item = dict(ref=masked_ref_image_aug.copy(), jpg=cropped_target_image.copy(), hint=collage.copy(), extra_sizes=np.array([H1, W1, H2, W2]), tar_box_yyxx_crop=np.array( tar_box_yyxx_crop ) ) \n    return item\n\n\ndef crop_back( pred, tar_image,  extra_sizes, tar_box_yyxx_crop):\n    H1, W1, H2, W2 = extra_sizes\n    y1,y2,x1,x2 = tar_box_yyxx_crop    \n    pred = cv2.resize(pred, (W2, H2))\n    m = 5 # maigin_pixel\n\n    if W1 == H1:\n        tar_image[y1+m :y2-m, x1+m:x2-m, :] =  pred[m:-m, m:-m]\n        return tar_image\n\n    if W1 < W2:\n        pad1 = int((W2 - W1) / 2)\n        pad2 = W2 - W1 - pad1\n        pred = pred[:,pad1: -pad2, :]\n    else:\n        pad1 = int((H2 - H1) / 2)\n        pad2 = H2 - H1 - pad1\n        pred = pred[pad1: -pad2, :, :]\n\n    gen_image = tar_image.copy()\n    gen_image[y1+m :y2-m, x1+m:x2-m, :] =  pred[m:-m, m:-m]\n    return gen_image\n\n\ndef inference_single_image(ref_image, ref_mask, tar_image, tar_mask, guidance_scale = 5.0):\n    item = process_pairs(ref_image, ref_mask, tar_image, tar_mask)\n    ref = item['ref'] * 255\n    tar = item['jpg'] * 127.5 + 127.5\n    hint = item['hint'] * 127.5 + 127.5\n\n    hint_image = hint[:,:,:-1]\n    hint_mask = item['hint'][:,:,-1] * 255\n    hint_mask = np.stack([hint_mask,hint_mask,hint_mask],-1)\n    ref = cv2.resize(ref.astype(np.uint8), (512,512))\n\n    seed = random.randint(0, 65535)\n    if save_memory:\n        model.low_vram_shift(is_diffusing=False)\n\n    ref = item['ref']\n    tar = item['jpg'] \n    hint = item['hint']\n    num_samples = 1\n\n    control = torch.from_numpy(hint.copy()).float().cuda() \n    control = torch.stack([control for _ in range(num_samples)], dim=0)\n    control = einops.rearrange(control, 'b h w c -> b c h w').clone()\n\n\n    clip_input = torch.from_numpy(ref.copy()).float().cuda() \n    clip_input = torch.stack([clip_input for _ in range(num_samples)], dim=0)\n    clip_input = einops.rearrange(clip_input, 'b h w c -> b c h w').clone()\n\n    guess_mode = False\n    H,W = 512,512\n\n    cond = {\"c_concat\": [control], \"c_crossattn\": [model.get_learned_conditioning( clip_input )]}\n    un_cond = {\"c_concat\": None if guess_mode else [control], \"c_crossattn\": [model.get_learned_conditioning([torch.zeros((1,3,224,224))] * num_samples)]}\n    shape = (4, H // 8, W // 8)\n\n    if save_memory:\n        model.low_vram_shift(is_diffusing=True)\n\n    # ====\n    num_samples = 1 #gr.Slider(label=\"Images\", minimum=1, maximum=12, value=1, step=1)\n    image_resolution = 512  #gr.Slider(label=\"Image Resolution\", minimum=256, maximum=768, value=512, step=64)\n    strength = 1  #gr.Slider(label=\"Control Strength\", minimum=0.0, maximum=2.0, value=1.0, step=0.01)\n    guess_mode = False #gr.Checkbox(label='Guess Mode', value=False)\n    #detect_resolution = 512  #gr.Slider(label=\"Segmentation Resolution\", minimum=128, maximum=1024, value=512, step=1)\n    ddim_steps = 50 #gr.Slider(label=\"Steps\", minimum=1, maximum=100, value=20, step=1)\n    scale = guidance_scale  #gr.Slider(label=\"Guidance Scale\", minimum=0.1, maximum=30.0, value=9.0, step=0.1)\n    seed = -1  #gr.Slider(label=\"Seed\", minimum=-1, maximum=2147483647, step=1, randomize=True)\n    eta = 0.0 #gr.Number(label=\"eta (DDIM)\", value=0.0)\n\n    model.control_scales = [strength * (0.825 ** float(12 - i)) for i in range(13)] if guess_mode else ([strength] * 13)  # Magic number. IDK why. Perhaps because 0.825**12<0.01 but 0.826**12>0.01\n    samples, intermediates = ddim_sampler.sample(ddim_steps, num_samples,\n                                                    shape, cond, verbose=False, eta=eta,\n                                                    unconditional_guidance_scale=scale,\n                                                    unconditional_conditioning=un_cond)\n    if save_memory:\n        model.low_vram_shift(is_diffusing=False)\n\n    x_samples = model.decode_first_stage(samples)\n    x_samples = (einops.rearrange(x_samples, 'b c h w -> b h w c') * 127.5 + 127.5).cpu().numpy()#.clip(0, 255).astype(np.uint8)\n\n    result = x_samples[0][:,:,::-1]\n    result = np.clip(result,0,255)\n\n    pred = x_samples[0]\n    pred = np.clip(pred,0,255)[1:,:,:]\n    sizes = item['extra_sizes']\n    tar_box_yyxx_crop = item['tar_box_yyxx_crop'] \n    gen_image = crop_back(pred, tar_image, sizes, tar_box_yyxx_crop) \n    return gen_image\n\n\nif __name__ == '__main__': \n    '''\n    # ==== Example for inferring a single image ===\n    reference_image_path = './examples/TestDreamBooth/FG/01.png'\n    bg_image_path = './examples/TestDreamBooth/BG/000000309203_GT.png'\n    bg_mask_path = './examples/TestDreamBooth/BG/000000309203_mask.png'\n    save_path = './examples/TestDreamBooth/GEN/gen_res.png'\n\n    # reference image + reference mask\n    # You could use the demo of SAM to extract RGB-A image with masks\n    # https://segment-anything.com/demo\n    image = cv2.imread( reference_image_path, cv2.IMREAD_UNCHANGED)\n    mask = (image[:,:,-1] > 128).astype(np.uint8)\n    image = image[:,:,:-1]\n    image = cv2.cvtColor(image.copy(), cv2.COLOR_BGR2RGB)\n    ref_image = image \n    ref_mask = mask\n\n    # background image\n    back_image = cv2.imread(bg_image_path).astype(np.uint8)\n    back_image = cv2.cvtColor(back_image, cv2.COLOR_BGR2RGB)\n\n    # background mask \n    tar_mask = cv2.imread(bg_mask_path)[:,:,0] > 128\n    tar_mask = tar_mask.astype(np.uint8)\n    \n    gen_image = inference_single_image(ref_image, ref_mask, back_image.copy(), tar_mask)\n    h,w = back_image.shape[0], back_image.shape[0]\n    ref_image = cv2.resize(ref_image, (w,h))\n    vis_image = cv2.hconcat([ref_image, back_image, gen_image])\n    \n    cv2.imwrite(save_path, vis_image [:,:,::-1])\n    '''\n    #'''\n    # ==== Example for inferring VITON-HD Test dataset ===\n\n    from omegaconf import OmegaConf\n    import os \n    DConf = OmegaConf.load('./configs/datasets.yaml')\n    save_dir = './VITONGEN'\n    if not os.path.exists(save_dir):\n        os.mkdir(save_dir)\n\n    test_dir = DConf.Test.VitonHDTest.image_dir\n    image_names = os.listdir(test_dir)\n    \n    for image_name in image_names:\n        ref_image_path = os.path.join(test_dir, image_name)\n        tar_image_path = ref_image_path.replace('/cloth/', '/image/')\n        ref_mask_path = ref_image_path.replace('/cloth/','/cloth-mask/')\n        tar_mask_path = ref_image_path.replace('/cloth/', '/image-parse-v3/').replace('.jpg','.png')\n\n        ref_image = cv2.imread(ref_image_path)\n        ref_image = cv2.cvtColor(ref_image, cv2.COLOR_BGR2RGB)\n\n        gt_image = cv2.imread(tar_image_path)\n        gt_image = cv2.cvtColor(gt_image, cv2.COLOR_BGR2RGB)\n\n        ref_mask = (cv2.imread(ref_mask_path) > 128).astype(np.uint8)[:,:,0]\n\n        tar_mask = Image.open(tar_mask_path ).convert('P')\n        tar_mask= np.array(tar_mask)\n        tar_mask = tar_mask == 5\n\n        gen_image = inference_single_image(ref_image, ref_mask, gt_image.copy(), tar_mask)\n        gen_path = os.path.join(save_dir, image_name)\n\n        vis_image = cv2.hconcat([ref_image, gt_image, gen_image])\n        cv2.imwrite(gen_path, vis_image[:,:,::-1])\n    #'''\n\n    \n\n"
        },
        {
          "name": "run_train_anydoor.py",
          "type": "blob",
          "size": 2.7314453125,
          "content": "import pytorch_lightning as pl\nfrom torch.utils.data import DataLoader\nfrom datasets.ytb_vos import YoutubeVOSDataset\nfrom datasets.ytb_vis import YoutubeVISDataset\nfrom datasets.saliency_modular import SaliencyDataset\nfrom datasets.vipseg import VIPSegDataset\nfrom datasets.mvimagenet import MVImageNetDataset\nfrom datasets.sam import SAMDataset\nfrom datasets.uvo import UVODataset\nfrom datasets.uvo_val import UVOValDataset\nfrom datasets.mose import MoseDataset\nfrom datasets.vitonhd import VitonHDDataset\nfrom datasets.fashiontryon import FashionTryonDataset\nfrom datasets.lvis import LvisDataset\nfrom cldm.logger import ImageLogger\nfrom cldm.model import create_model, load_state_dict\nfrom torch.utils.data import ConcatDataset\nfrom cldm.hack import disable_verbosity, enable_sliced_attention\nfrom omegaconf import OmegaConf\n\nsave_memory = False\ndisable_verbosity()\nif save_memory:\n    enable_sliced_attention()\n\n# Configs\nresume_path = 'path/to/weight'\nbatch_size = 16\nlogger_freq = 1000\nlearning_rate = 1e-5\nsd_locked = False\nonly_mid_control = False\nn_gpus = 2\naccumulate_grad_batches=1\n\n# First use cpu to load models. Pytorch Lightning will automatically move it to GPUs.\nmodel = create_model('./configs/anydoor.yaml').cpu()\nmodel.load_state_dict(load_state_dict(resume_path, location='cpu'))\nmodel.learning_rate = learning_rate\nmodel.sd_locked = sd_locked\nmodel.only_mid_control = only_mid_control\n\n# Datasets\nDConf = OmegaConf.load('./configs/datasets.yaml')\ndataset1 = YoutubeVOSDataset(**DConf.Train.YoutubeVOS)  \ndataset2 =  SaliencyDataset(**DConf.Train.Saliency) \ndataset3 = VIPSegDataset(**DConf.Train.VIPSeg) \ndataset4 = YoutubeVISDataset(**DConf.Train.YoutubeVIS) \ndataset5 = MVImageNetDataset(**DConf.Train.MVImageNet)\ndataset6 = SAMDataset(**DConf.Train.SAM)\ndataset7 = UVODataset(**DConf.Train.UVO.train)\ndataset8 = VitonHDDataset(**DConf.Train.VitonHD)\ndataset9 = UVOValDataset(**DConf.Train.UVO.val)\ndataset10 = MoseDataset(**DConf.Train.Mose)\ndataset11 = FashionTryonDataset(**DConf.Train.FashionTryon)\ndataset12 = LvisDataset(**DConf.Train.Lvis)\n\nimage_data = [dataset2, dataset6, dataset12]\nvideo_data = [dataset1, dataset3, dataset4, dataset7, dataset9, dataset10 ]\ntryon_data = [dataset8, dataset11]\nthreed_data = [dataset5]\n\n# The ratio of each dataset is adjusted by setting the __len__ \ndataset = ConcatDataset( image_data + video_data + tryon_data +  threed_data + video_data + tryon_data +  threed_data  )\ndataloader = DataLoader(dataset, num_workers=8, batch_size=batch_size, shuffle=True)\nlogger = ImageLogger(batch_frequency=logger_freq)\ntrainer = pl.Trainer(gpus=n_gpus, strategy=\"ddp\", precision=16, accelerator=\"gpu\", callbacks=[logger], progress_bar_refresh_rate=1, accumulate_grad_batches=accumulate_grad_batches)\n\n# Train!\ntrainer.fit(model, dataloader)\n"
        },
        {
          "name": "scripts",
          "type": "tree",
          "content": null
        },
        {
          "name": "tool_add_control_sd21.py",
          "type": "blob",
          "size": 1.4462890625,
          "content": "import sys\nimport os\n\nassert len(sys.argv) == 3, 'Args are wrong.'\n\ninput_path = sys.argv[1]\noutput_path = sys.argv[2]\n\nassert os.path.exists(input_path), 'Input model does not exist.'\nassert not os.path.exists(output_path), 'Output filename already exists.'\nassert os.path.exists(os.path.dirname(output_path)), 'Output path is not valid.'\n\nimport torch\nfrom share import *\nfrom cldm.model import create_model\n\n\ndef get_node_name(name, parent_name):\n    if len(name) <= len(parent_name):\n        return False, ''\n    p = name[:len(parent_name)]\n    if p != parent_name:\n        return False, ''\n    return True, name[len(parent_name):]\n\n\nmodel = create_model(config_path='./models/anydoor.yaml')\n\npretrained_weights = torch.load(input_path)\nif 'state_dict' in pretrained_weights:\n    pretrained_weights = pretrained_weights['state_dict']\n\nscratch_dict = model.state_dict()\n\ntarget_dict = {}\nfor k in scratch_dict.keys():\n\n    is_control, name = get_node_name(k, 'control_')\n    if 'control_model.input_blocks.0.0' in k:\n        print('skipped key: ', k)\n        continue\n\n    if is_control:\n        copy_k = 'model.diffusion_' + name\n    else:\n        copy_k = k\n    if copy_k in pretrained_weights:\n        target_dict[k] = pretrained_weights[copy_k].clone()\n    else:\n        target_dict[k] = scratch_dict[k].clone()\n        print(f'These weights are newly added: {k}')\n\nmodel.load_state_dict(target_dict, strict=False)\ntorch.save(model.state_dict(), output_path)\nprint('Done.')\n"
        }
      ]
    }
  ]
}