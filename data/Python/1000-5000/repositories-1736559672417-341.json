{
  "metadata": {
    "timestamp": 1736559672417,
    "page": 341,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjM1MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "kwai/DouZero",
      "stars": 4169,
      "defaultBranch": "main",
      "files": [
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.10546875,
          "content": "*.swp\n*.so\n__pycache__\n.DS_Store\n*.egg-info\n*.pyc\ndist/\ndouzero_checkpoints\nmost_recent_model\neval_data.pkl\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 11.0908203125,
          "content": "                                 Apache License\n                           Version 2.0, January 2004\n                        http://www.apache.org/licenses/\n\n   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n\n   1. Definitions.\n\n      \"License\" shall mean the terms and conditions for use, reproduction,\n      and distribution as defined by Sections 1 through 9 of this document.\n\n      \"Licensor\" shall mean the copyright owner or entity authorized by\n      the copyright owner that is granting the License.\n\n      \"Legal Entity\" shall mean the union of the acting entity and all\n      other entities that control, are controlled by, or are under common\n      control with that entity. For the purposes of this definition,\n      \"control\" means (i) the power, direct or indirect, to cause the\n      direction or management of such entity, whether by contract or\n      otherwise, or (ii) ownership of fifty percent (50%) or more of the\n      outstanding shares, or (iii) beneficial ownership of such entity.\n\n      \"You\" (or \"Your\") shall mean an individual or Legal Entity\n      exercising permissions granted by this License.\n\n      \"Source\" form shall mean the preferred form for making modifications,\n      including but not limited to software source code, documentation\n      source, and configuration files.\n\n      \"Object\" form shall mean any form resulting from mechanical\n      transformation or translation of a Source form, including but\n      not limited to compiled object code, generated documentation,\n      and conversions to other media types.\n\n      \"Work\" shall mean the work of authorship, whether in Source or\n      Object form, made available under the License, as indicated by a\n      copyright notice that is included in or attached to the work\n      (an example is provided in the Appendix below).\n\n      \"Derivative Works\" shall mean any work, whether in Source or Object\n      form, that is based on (or derived from) the Work and for which the\n      editorial revisions, annotations, elaborations, or other modifications\n      represent, as a whole, an original work of authorship. For the purposes\n      of this License, Derivative Works shall not include works that remain\n      separable from, or merely link (or bind by name) to the interfaces of,\n      the Work and Derivative Works thereof.\n\n      \"Contribution\" shall mean any work of authorship, including\n      the original version of the Work and any modifications or additions\n      to that Work or Derivative Works thereof, that is intentionally\n      submitted to Licensor for inclusion in the Work by the copyright owner\n      or by an individual or Legal Entity authorized to submit on behalf of\n      the copyright owner. For the purposes of this definition, \"submitted\"\n      means any form of electronic, verbal, or written communication sent\n      to the Licensor or its representatives, including but not limited to\n      communication on electronic mailing lists, source code control systems,\n      and issue tracking systems that are managed by, or on behalf of, the\n      Licensor for the purpose of discussing and improving the Work, but\n      excluding communication that is conspicuously marked or otherwise\n      designated in writing by the copyright owner as \"Not a Contribution.\"\n\n      \"Contributor\" shall mean Licensor and any individual or Legal Entity\n      on behalf of whom a Contribution has been received by Licensor and\n      subsequently incorporated within the Work.\n\n   2. Grant of Copyright License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      copyright license to reproduce, prepare Derivative Works of,\n      publicly display, publicly perform, sublicense, and distribute the\n      Work and such Derivative Works in Source or Object form.\n\n   3. Grant of Patent License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      (except as stated in this section) patent license to make, have made,\n      use, offer to sell, sell, import, and otherwise transfer the Work,\n      where such license applies only to those patent claims licensable\n      by such Contributor that are necessarily infringed by their\n      Contribution(s) alone or by combination of their Contribution(s)\n      with the Work to which such Contribution(s) was submitted. If You\n      institute patent litigation against any entity (including a\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\n      or a Contribution incorporated within the Work constitutes direct\n      or contributory patent infringement, then any patent licenses\n      granted to You under this License for that Work shall terminate\n      as of the date such litigation is filed.\n\n   4. Redistribution. You may reproduce and distribute copies of the\n      Work or Derivative Works thereof in any medium, with or without\n      modifications, and in Source or Object form, provided that You\n      meet the following conditions:\n\n      (a) You must give any other recipients of the Work or\n          Derivative Works a copy of this License; and\n\n      (b) You must cause any modified files to carry prominent notices\n          stating that You changed the files; and\n\n      (c) You must retain, in the Source form of any Derivative Works\n          that You distribute, all copyright, patent, trademark, and\n          attribution notices from the Source form of the Work,\n          excluding those notices that do not pertain to any part of\n          the Derivative Works; and\n\n      (d) If the Work includes a \"NOTICE\" text file as part of its\n          distribution, then any Derivative Works that You distribute must\n          include a readable copy of the attribution notices contained\n          within such NOTICE file, excluding those notices that do not\n          pertain to any part of the Derivative Works, in at least one\n          of the following places: within a NOTICE text file distributed\n          as part of the Derivative Works; within the Source form or\n          documentation, if provided along with the Derivative Works; or,\n          within a display generated by the Derivative Works, if and\n          wherever such third-party notices normally appear. The contents\n          of the NOTICE file are for informational purposes only and\n          do not modify the License. You may add Your own attribution\n          notices within Derivative Works that You distribute, alongside\n          or as an addendum to the NOTICE text from the Work, provided\n          that such additional attribution notices cannot be construed\n          as modifying the License.\n\n      You may add Your own copyright statement to Your modifications and\n      may provide additional or different license terms and conditions\n      for use, reproduction, or distribution of Your modifications, or\n      for any such Derivative Works as a whole, provided Your use,\n      reproduction, and distribution of the Work otherwise complies with\n      the conditions stated in this License.\n\n   5. Submission of Contributions. Unless You explicitly state otherwise,\n      any Contribution intentionally submitted for inclusion in the Work\n      by You to the Licensor shall be under the terms and conditions of\n      this License, without any additional terms or conditions.\n      Notwithstanding the above, nothing herein shall supersede or modify\n      the terms of any separate license agreement you may have executed\n      with Licensor regarding such Contributions.\n\n   6. Trademarks. This License does not grant permission to use the trade\n      names, trademarks, service marks, or product names of the Licensor,\n      except as required for reasonable and customary use in describing the\n      origin of the Work and reproducing the content of the NOTICE file.\n\n   7. Disclaimer of Warranty. Unless required by applicable law or\n      agreed to in writing, Licensor provides the Work (and each\n      Contributor provides its Contributions) on an \"AS IS\" BASIS,\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n      implied, including, without limitation, any warranties or conditions\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n      PARTICULAR PURPOSE. You are solely responsible for determining the\n      appropriateness of using or redistributing the Work and assume any\n      risks associated with Your exercise of permissions under this License.\n\n   8. Limitation of Liability. In no event and under no legal theory,\n      whether in tort (including negligence), contract, or otherwise,\n      unless required by applicable law (such as deliberate and grossly\n      negligent acts) or agreed to in writing, shall any Contributor be\n      liable to You for damages, including any direct, indirect, special,\n      incidental, or consequential damages of any character arising as a\n      result of this License or out of the use or inability to use the\n      Work (including but not limited to damages for loss of goodwill,\n      work stoppage, computer failure or malfunction, or any and all\n      other commercial damages or losses), even if such Contributor\n      has been advised of the possibility of such damages.\n\n   9. Accepting Warranty or Additional Liability. While redistributing\n      the Work or Derivative Works thereof, You may choose to offer,\n      and charge a fee for, acceptance of support, warranty, indemnity,\n      or other liability obligations and/or rights consistent with this\n      License. However, in accepting such obligations, You may act only\n      on Your own behalf and on Your sole responsibility, not on behalf\n      of any other Contributor, and only if You agree to indemnify,\n      defend, and hold each Contributor harmless for any liability\n      incurred by, or claims asserted against, such Contributor by reason\n      of your accepting any such warranty or additional liability.\n\n   END OF TERMS AND CONDITIONS\n\n   APPENDIX: How to apply the Apache License to your work.\n\n      To apply the Apache License to your work, attach the following\n      boilerplate notice, with the fields enclosed by brackets \"[]\"\n      replaced with your own identifying information. (Don't include\n      the brackets!)  The text should be enclosed in the appropriate\n      comment syntax for the file format. We also recommend that a\n      file or class name and description of purpose be included on the\n      same \"printed page\" as the copyright notice for easier\n      identification within third-party archives.\n\n   Copyright [yyyy] [name of copyright owner]\n\n   Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 14.07421875,
          "content": "# [ICML 2021] DouZero: Mastering DouDizhu with Self-Play Deep Reinforcement Learning\n<img width=\"500\" src=\"https://raw.githubusercontent.com/kwai/DouZero/main/imgs/douzero_logo.jpg\" alt=\"Logo\" />\n\n[![Building](https://github.com/kwai/DouZero/actions/workflows/python-package.yml/badge.svg)](https://github.com/kwai/DouZero/actions/workflows/python-package.yml)\n[![PyPI version](https://badge.fury.io/py/douzero.svg)](https://badge.fury.io/py/douzero)\n[![Downloads](https://pepy.tech/badge/douzero)](https://pepy.tech/project/douzero)\n[![Downloads](https://pepy.tech/badge/douzero/month)](https://pepy.tech/project/douzero)\n[![License](https://img.shields.io/badge/License-Apache_2.0-blue.svg)](https://opensource.org/licenses/Apache-2.0)\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/daochenzha/douzero-colab/blob/main/douzero-colab.ipynb)\n\n[中文文档](README.zh-CN.md)\n\nDouZero is a reinforcement learning framework for [DouDizhu](https://en.wikipedia.org/wiki/Dou_dizhu) ([斗地主](https://baike.baidu.com/item/%E6%96%97%E5%9C%B0%E4%B8%BB/177997)), the most popular card game in China. It is a shedding-type game where the player’s objective is to empty one’s hand of all cards before other players. DouDizhu is a very challenging domain with competition, collaboration, imperfect information, large state space, and particularly a massive set of possible actions where the legal actions vary significantly from turn to turn. DouZero is developed by AI Platform, Kwai Inc. (快手).\n\n*   Online Demo: [https://www.douzero.org/](https://www.douzero.org/)\n       * :loudspeaker: New Version with Bid（叫牌版本）: [https://www.douzero.org/bid](https://www.douzero.org/bid)\n*   Run the Demo Locally: [https://github.com/datamllab/rlcard-showdown](https://github.com/datamllab/rlcard-showdown)\n*   Video: [YouTube](https://youtu.be/inHIi8sej7Y)\n*   Paper: [https://arxiv.org/abs/2106.06135](https://arxiv.org/abs/2106.06135) \n*   Related Project: [RLCard Project](https://github.com/datamllab/rlcard)\n*   Related Resources: [Awesome-Game-AI](https://github.com/datamllab/awesome-game-ai)\n*   Google Colab: [jupyter notebook](https://github.com/daochenzha/douzero-colab/blob/main/douzero-colab.ipynb)\n*   Unofficial improved versions of DouZero by the community: [[DouZero ResNet]](https://github.com/Vincentzyx/Douzero_Resnet) [[DouZero FullAuto]](https://github.com/Vincentzyx/DouZero_For_HLDDZ_FullAuto)\n*   Zhihu: [https://zhuanlan.zhihu.com/p/526723604](https://zhuanlan.zhihu.com/p/526723604)\n*   Miscellaneous Resources:\n\t*   Check out our open-sourced [Large Time Series Model (LTSM)](https://github.com/daochenzha/ltsm)!\n\t*   Have you heard of data-centric AI? Please check out our [data-centric AI survey](https://arxiv.org/abs/2303.10158) and [awesome data-centric AI resources](https://github.com/daochenzha/data-centric-AI)!\n\n**Community:**\n*  **Slack**: Discuss in [DouZero](https://join.slack.com/t/douzero/shared_invite/zt-rg3rygcw-ouxxDk5o4O0bPZ23vpdwxA) channel.\n*  **QQ Group**: Join our QQ group to discuss. Password: douzeroqqgroup\n\n\t*  Group 1: 819204202\n\t*  Group 2: 954183174\n\t*  Group 3: 834954839\n\t*  Group 4: 211434658\n\t*  Group 5: 189203636\n\n**News:**\n*   Thanks for the contribution of [@Vincentzyx](https://github.com/Vincentzyx) for enabling CPU training. Now Windows users can train with CPUs.\n\n<img width=\"500\" src=\"https://douzero.org/public/demo.gif\" alt=\"Demo\" />\n\n## Cite this Work\nIf you find this project helpful in your research, please cite our paper:\n\nZha, Daochen et al. “DouZero: Mastering DouDizhu with Self-Play Deep Reinforcement Learning.” ICML (2021).\n\n```bibtex\n@InProceedings{pmlr-v139-zha21a,\n  title = \t {DouZero: Mastering DouDizhu with Self-Play Deep Reinforcement Learning},\n  author =       {Zha, Daochen and Xie, Jingru and Ma, Wenye and Zhang, Sheng and Lian, Xiangru and Hu, Xia and Liu, Ji},\n  booktitle = \t {Proceedings of the 38th International Conference on Machine Learning},\n  pages = \t {12333--12344},\n  year = \t {2021},\n  editor = \t {Meila, Marina and Zhang, Tong},\n  volume = \t {139},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {18--24 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v139/zha21a/zha21a.pdf},\n  url = \t {http://proceedings.mlr.press/v139/zha21a.html},\n  abstract = \t {Games are abstractions of the real world, where artificial agents learn to compete and cooperate with other agents. While significant achievements have been made in various perfect- and imperfect-information games, DouDizhu (a.k.a. Fighting the Landlord), a three-player card game, is still unsolved. DouDizhu is a very challenging domain with competition, collaboration, imperfect information, large state space, and particularly a massive set of possible actions where the legal actions vary significantly from turn to turn. Unfortunately, modern reinforcement learning algorithms mainly focus on simple and small action spaces, and not surprisingly, are shown not to make satisfactory progress in DouDizhu. In this work, we propose a conceptually simple yet effective DouDizhu AI system, namely DouZero, which enhances traditional Monte-Carlo methods with deep neural networks, action encoding, and parallel actors. Starting from scratch in a single server with four GPUs, DouZero outperformed all the existing DouDizhu AI programs in days of training and was ranked the first in the Botzone leaderboard among 344 AI agents. Through building DouZero, we show that classic Monte-Carlo methods can be made to deliver strong results in a hard domain with a complex action space. The code and an online demo are released at https://github.com/kwai/DouZero with the hope that this insight could motivate future work.}\n}\n```\n\n## What Makes DouDizhu Challenging?\nIn addition to the challenge of imperfect information, DouDizhu has huge state and action spaces. In particular, the action space of DouDizhu is 10^4 (see [this table](https://github.com/datamllab/rlcard#available-environments)). Unfortunately, most reinforcement learning algorithms can only handle very small action spaces. Moreover, the players in DouDizhu need to both compete and cooperate with others in a partially-observable environment with limited communication, i.e., two Peasants players will play as a team to fight against the Landlord player. Modeling both competing and cooperation is an open research challenge.\n\nIn this work, we propose Deep Monte Carlo (DMC) algorithm with action encoding and parallel actors. This leads to a very simple yet surprisingly effective solution for DouDizhu. Please read [our paper](https://arxiv.org/abs/2106.06135) for more details.\n\n## Installation\nThe training code is designed for GPUs. Thus, you need to first install CUDA if you want to train models. You may refer to [this guide](https://docs.nvidia.com/cuda/index.html#installation-guides). For evaluation, CUDA is optional and you can use CPU for evaluation.\n\nFirst, clone the repo with (if you are in China and Github is slow, you can use the mirror in [Gitee](https://gitee.com/daochenzha/DouZero)):\n```\ngit clone https://github.com/kwai/DouZero.git\n```\nMake sure you have python 3.6+ installed. Install dependencies:\n```\ncd douzero\npip3 install -r requirements.txt\n```\nWe recommend installing the stable version of DouZero with\n```\npip3 install douzero\n```\nIf you are in China and the above command is too slow, you can use the mirror provided by Tsinghua University:\n```\npip3 install douzero -i https://pypi.tuna.tsinghua.edu.cn/simple\n```\nor install the up-to-date version (it could be not stable) with\n```\npip3 install -e .\n```\nNote that Windows users can only use CPU as actors. See [Issues in Windows](README.md#issues-in-windows) about why GPUs are not supported. Nonetheless, Windows users can still [run the demo locally](https://github.com/datamllab/rlcard-showdown).  \n\n## Training\nTo use GPU for training, run\n```\npython3 train.py\n```\nThis will train DouZero on one GPU. To train DouZero on multiple GPUs. Use the following arguments.\n*   `--gpu_devices`: what gpu devices are visible\n*   `--num_actor_devices`: how many of the GPU deveices will be used for simulation, i.e., self-play\n*   `--num_actors`: how many actor processes will be used for each device\n*   `--training_device`: which device will be used for training DouZero\n\nFor example, if we have 4 GPUs, where we want to use the first 3 GPUs to have 15 actors each for simulating and the 4th GPU for training, we can run the following command:\n```\npython3 train.py --gpu_devices 0,1,2,3 --num_actor_devices 3 --num_actors 15 --training_device 3\n```\nTo use CPU training or simulation (Windows can only use CPU for actors), use the following arguments:\n*   `--training_device cpu`: Use CPU to train the model\n*   `--actor_device_cpu`: Use CPU as actors\n\nFor example, use the following command to run everything on CPU:\n```\npython3 train.py --actor_device_cpu --training_device cpu\n```\nThe following command only runs actors on CPU:\n```\npython3 train.py --actor_device_cpu\n```\nFor more customized configuration of training, see the following optional arguments:\n```\n--xpid XPID           Experiment id (default: douzero)\n--save_interval SAVE_INTERVAL\n                      Time interval (in minutes) at which to save the model\n--objective {adp,wp}  Use ADP or WP as reward (default: ADP)\n--actor_device_cpu    Use CPU as actor device\n--gpu_devices GPU_DEVICES\n                      Which GPUs to be used for training\n--num_actor_devices NUM_ACTOR_DEVICES\n                      The number of devices used for simulation\n--num_actors NUM_ACTORS\n                      The number of actors for each simulation device\n--training_device TRAINING_DEVICE\n                      The index of the GPU used for training models. `cpu`\n                \t  means using cpu\n--load_model          Load an existing model\n--disable_checkpoint  Disable saving checkpoint\n--savedir SAVEDIR     Root dir where experiment data will be saved\n--total_frames TOTAL_FRAMES\n                      Total environment frames to train for\n--exp_epsilon EXP_EPSILON\n                      The probability for exploration\n--batch_size BATCH_SIZE\n                      Learner batch size\n--unroll_length UNROLL_LENGTH\n                      The unroll length (time dimension)\n--num_buffers NUM_BUFFERS\n                      Number of shared-memory buffers\n--num_threads NUM_THREADS\n                      Number learner threads\n--max_grad_norm MAX_GRAD_NORM\n                      Max norm of gradients\n--learning_rate LEARNING_RATE\n                      Learning rate\n--alpha ALPHA         RMSProp smoothing constant\n--momentum MOMENTUM   RMSProp momentum\n--epsilon EPSILON     RMSProp epsilon\n```\n\n## Evaluation\nThe evaluation can be performed with GPU or CPU (GPU will be much faster). Pretrained model is available at [Google Drive](https://drive.google.com/drive/folders/1NmM2cXnI5CIWHaLJeoDZMiwt6lOTV_UB?usp=sharing) or [百度网盘](https://pan.baidu.com/s/18g-JUKad6D8rmBONXUDuOQ), 提取码: 4624. Put pre-trained weights in `baselines/`. The performance is evaluated through self-play. We have provided pre-trained models and some heuristics as baselines:\n*   [random](douzero/evaluation/random_agent.py): agents that play randomly (uniformly)\n*   [rlcard](douzero/evaluation/rlcard_agent.py): the rule-based agent in [RLCard](https://github.com/datamllab/rlcard)\n*   SL (`baselines/sl/`): the pre-trained deep agents on human data\n*   DouZero-ADP (`baselines/douzero_ADP/`): the pretrained DouZero agents with Average Difference Points (ADP) as objective\n*   DouZero-WP (`baselines/douzero_WP/`): the pretrained DouZero agents with Winning Percentage (WP) as objective\n\n### Step 1: Generate evaluation data\n```\npython3 generate_eval_data.py\n```\nSome important hyperparameters are as follows.\n*   `--output`: where the pickled data will be saved\n*   `--num_games`: how many random games will be generated, default 10000\n\n### Step 2: Self-Play\n```\npython3 evaluate.py\n```\nSome important hyperparameters are as follows.\n*   `--landlord`: which agent will play as Landlord, which can be random, rlcard, or the path of the pre-trained model\n*   `--landlord_up`: which agent will play as LandlordUp (the one plays before the Landlord), which can be random, rlcard, or the path of the pre-trained model\n*   `--landlord_down`: which agent will play as LandlordDown (the one plays after the Landlord), which can be random, rlcard, or the path of the pre-trained model\n*   `--eval_data`: the pickle file that contains evaluation data\n*   `--num_workers`: how many subprocesses will be used\n*   `--gpu_device`: which GPU to use. It will use CPU by default\n\nFor example, the following command evaluates DouZero-ADP in Landlord position against random agents\n```\npython3 evaluate.py --landlord baselines/douzero_ADP/landlord.ckpt --landlord_up random --landlord_down random\n```\nThe following command evaluates DouZero-ADP in Peasants position against RLCard agents\n```\npython3 evaluate.py --landlord rlcard --landlord_up baselines/douzero_ADP/landlord_up.ckpt --landlord_down baselines/douzero_ADP/landlord_down.ckpt\n```\nBy default, our model will be saved in `douzero_checkpoints/douzero` every half an hour. We provide a script to help you identify the most recent checkpoint. Run\n```\nsh get_most_recent.sh douzero_checkpoints/douzero/\n```\nThe most recent model will be in `most_recent_model`.\n\n## Issues in Windows\nYou may encounter `operation not supported` error if you use a Windows system to train with GPU as actors. This is because doing multiprocessing on CUDA tensors is not supported in Windows. However, our code extensively operates on the CUDA tensors since the code is optimized for GPUs. Please contact us if you find any solutions!\n\n## Core Team\n*   Algorithm: [Daochen Zha](https://github.com/daochenzha), [Jingru Xie](https://github.com/karoka), Wenye Ma, Sheng Zhang, [Xiangru Lian](https://xrlian.com/), Xia Hu, [Ji Liu](http://jiliu-ml.org/)\n*   GUI Demo: [Songyi Huang](https://github.com/hsywhu)\n*   Community contributors: [@Vincentzyx](https://github.com/Vincentzyx)\n\n## Acknowlegements\n*   The demo is largely based on [RLCard-Showdown](https://github.com/datamllab/rlcard-showdown)\n*   Code implementation is inspired by [TorchBeast](https://github.com/facebookresearch/torchbeast)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"
        },
        {
          "name": "README.zh-CN.md",
          "type": "blob",
          "size": 13.1025390625,
          "content": "# [ICML 2021] DouZero: 从零开始通过自我博弈强化学习来学打斗地主\n<img width=\"500\" src=\"https://gitee.com/daochenzha/DouZero/raw/main/imgs/douzero_logo.jpg\" alt=\"Logo\" />\n\n[![Building](https://github.com/kwai/DouZero/actions/workflows/python-package.yml/badge.svg)](https://github.com/kwai/DouZero/actions/workflows/python-package.yml)\n[![PyPI version](https://badge.fury.io/py/douzero.svg)](https://badge.fury.io/py/douzero)\n[![Downloads](https://pepy.tech/badge/douzero)](https://pepy.tech/project/douzero)\n[![Downloads](https://pepy.tech/badge/douzero/month)](https://pepy.tech/project/douzero)\n[![License](https://img.shields.io/badge/License-Apache_2.0-blue.svg)](https://opensource.org/licenses/Apache-2.0)\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/daochenzha/douzero-colab/blob/main/douzero-colab.ipynb)\n\n[English README](README.md)\n\nDouZero是一个为斗地主设计的强化学习框架。斗地主十分具有挑战性。它包含合作、竞争、非完全信息、庞大的状态空间。斗地主也有非常大的动作空间，并且每一步合法的牌型会非常不一样。DouZero由快手AI平台部开发。\n\n*   在线演示: [https://www.douzero.org/](https://www.douzero.org/)\n      * :loudspeaker: 抢先体验叫牌版本（调试中）: [https://www.douzero.org/bid](https://www.douzero.org/bid)\n*   离线运行演示: [https://github.com/datamllab/rlcard-showdown](https://github.com/datamllab/rlcard-showdown)\n*   论文: [https://arxiv.org/abs/2106.06135](https://arxiv.org/abs/2106.06135) \n*   视频: [YouTube](https://youtu.be/inHIi8sej7Y)\n*   论文: [https://arxiv.org/abs/2106.06135](https://arxiv.org/abs/2106.06135) \n*   相关仓库: [RLCard Project](https://github.com/datamllab/rlcard)\n*   相关资源: [Awesome-Game-AI](https://github.com/datamllab/awesome-game-ai)\n*   由社区贡献者开发的非官方改进版: [[DouZero ResNet]](https://github.com/Vincentzyx/Douzero_Resnet) [[DouZero FullAuto]](https://github.com/Vincentzyx/DouZero_For_HLDDZ_FullAuto)\n*   知乎：[https://zhuanlan.zhihu.com/p/526723604](https://zhuanlan.zhihu.com/p/526723604)\n*   杂项资源：您听说过以数据为中心的人工智能吗？请查看我们的 [data-centric AI survey](https://arxiv.org/abs/2303.10158) 和 [awesome data-centric AI resources](https://github.com/daochenzha/data-centric-AI)!\n\n**社区:**\n*  **Slack**: 加入 [DouZero](https://join.slack.com/t/douzero/shared_invite/zt-rg3rygcw-ouxxDk5o4O0bPZ23vpdwxA) 频道.\n*  **QQ群**: 加入我们的QQ群讨论。密码: douzeroqqgroup\n\t*  一群：819204202\n\t*  二群：954183174\n\t*  三群：834954839\n\t*  四群：211434658\n\t*  五群：189203636\n\n**最新动态:**\n*   感谢[@Vincentzyx](https://github.com/Vincentzyx)实现了CPU训练。现在Windows用户也能用CPU训练了。\n\n<img width=\"500\" src=\"https://douzero.org/public/demo.gif\" alt=\"Demo\" />\n\n## 引用\n如果您用到我们的项目，请添加以下引用：\n\nZha, Daochen et al. “DouZero: Mastering DouDizhu with Self-Play Deep Reinforcement Learning.” ICML (2021).\n\n```bibtex\n@InProceedings{pmlr-v139-zha21a,\n  title = \t {DouZero: Mastering DouDizhu with Self-Play Deep Reinforcement Learning},\n  author =       {Zha, Daochen and Xie, Jingru and Ma, Wenye and Zhang, Sheng and Lian, Xiangru and Hu, Xia and Liu, Ji},\n  booktitle = \t {Proceedings of the 38th International Conference on Machine Learning},\n  pages = \t {12333--12344},\n  year = \t {2021},\n  editor = \t {Meila, Marina and Zhang, Tong},\n  volume = \t {139},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {18--24 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v139/zha21a/zha21a.pdf},\n  url = \t {http://proceedings.mlr.press/v139/zha21a.html},\n  abstract = \t {Games are abstractions of the real world, where artificial agents learn to compete and cooperate with other agents. While significant achievements have been made in various perfect- and imperfect-information games, DouDizhu (a.k.a. Fighting the Landlord), a three-player card game, is still unsolved. DouDizhu is a very challenging domain with competition, collaboration, imperfect information, large state space, and particularly a massive set of possible actions where the legal actions vary significantly from turn to turn. Unfortunately, modern reinforcement learning algorithms mainly focus on simple and small action spaces, and not surprisingly, are shown not to make satisfactory progress in DouDizhu. In this work, we propose a conceptually simple yet effective DouDizhu AI system, namely DouZero, which enhances traditional Monte-Carlo methods with deep neural networks, action encoding, and parallel actors. Starting from scratch in a single server with four GPUs, DouZero outperformed all the existing DouDizhu AI programs in days of training and was ranked the first in the Botzone leaderboard among 344 AI agents. Through building DouZero, we show that classic Monte-Carlo methods can be made to deliver strong results in a hard domain with a complex action space. The code and an online demo are released at https://github.com/kwai/DouZero with the hope that this insight could motivate future work.}\n}\n```\n\n## 为什么斗地主具有挑战性\n除了非完全信息带来的挑战外，斗地主本身也包含巨大的状态和动作空间。具体来说，斗地主的动作空间大小高达10^4（详见[该表格](https://github.com/datamllab/rlcard#available-environments)）。不幸的是，大部分强化学习算法都只能处理很小的动作空间。并且，斗地主的玩家需要在部分可观测的环境中，与其他玩家对抗或合作，例如：两个农民玩家需要作为一个团队对抗地主玩家。对对抗和合作同时进行建模一直以来是学术界的一个开放性问题。\n\n在本研究工作中，我们提出了将深度蒙特卡洛（Deep Monte Carlo, DMC）与动作编码和并行演员（Parallel Actors）相结合的方法，为斗地主提供了一个简单而有效的解决方案，详见[我们的论文](https://arxiv.org/abs/2106.06135)。\n\n## 安装\n训练部分的代码是基于GPU设计的，因此如果想要训练模型，您需要先安装CUDA。安装步骤可以参考[官网教程](https://docs.nvidia.com/cuda/index.html#installation-guides)。对于评估部分，CUDA是可选项，您可以使用CPU进行评估。\n\n首先，克隆本仓库（如果您访问Github较慢，国内用户可以使用[Gitee镜像](https://gitee.com/daochenzha/DouZero)）：\n```\ngit clone https://github.com/kwai/DouZero.git\n```\n\n确保您已经安装好Python 3.6及以上版本，然后安装依赖：\n```\ncd douzero\npip3 install -r requirements.txt\n```\n我们推荐通过以下命令安装稳定版本的Douzero：\n```\npip3 install douzero\n```\n如果您访问较慢，国内用户可以通过清华镜像源安装：\n```\npip3 install douzero -i https://pypi.tuna.tsinghua.edu.cn/simple\n```\n或是安装最新版本（可能不稳定）：\n```\npip3 install -e .\n```\n注意，Windows用户只能用CPU来模拟。关于为什么GPU会出问题，详见[Windows下的问题](README.zh-CN.md#Windows下的问题)。但Windows用户仍可以[在本地运行演示](https://github.com/datamllab/rlcard-showdown)。\n\n## 训练\n假定您至少拥有一块可用的GPU，运行\n```\npython3 train.py\n```\n这会使用一块GPU训练DouZero。如果需要用多个GPU训练Douzero，使用以下参数：\n*   `--gpu_devices`: 用作训练的GPU设备名\n*   `--num_actor_devices`: 被用来进行模拟（如自我对弈）的GPU数量\n*   `--num_actors`: 每个设备的演员进程数\n*   `--training_device`: 用来进行模型训练的设备\n\n例如，如果我们拥有4块GPU，我们想用前3个GPU进行模拟，每个GPU拥有15个演员，而使用第四个GPU进行训练，我们可以运行以下命令：\n```\npython3 train.py --gpu_devices 0,1,2,3 --num_actor_devices 3 --num_actors 15 --training_device 3\n```\n如果用CPU进行训练和模拟（Windows用户只能用CPU进行模拟），用以下参数：\n*   `--training_device cpu`: 用CPU来训练\n*   `--actor_device_cpu`: 用CPU来模拟\n\n例如，用以下命令完全在CPU上运行：\n```\npython3 train.py --actor_device_cpu --training_device cpu\n```\n以下命令仅仅用CPU来跑模拟：\n```\npython3 train.py --actor_device_cpu\n```\n\n其他定制化的训练配置可以参考以下可选项：\n```\n--xpid XPID           实验id（默认值：douzero）\n--save_interval SAVE_INTERVAL\n                      保存模型的时间间隔（以分钟为单位）\n--objective {adp,wp}  使用ADP或者WP作为奖励（默认值：ADP）\n--actor_device_cpu    用CPU进行模拟\n--gpu_devices GPU_DEVICES\n                      用作训练的GPU设备名\n--num_actor_devices NUM_ACTOR_DEVICES\n                      被用来进行模拟（如自我对弈）的GPU数量\n--num_actors NUM_ACTORS\n                      每个设备的演员进程数\n--training_device TRAINING_DEVICE\n                      用来进行模型训练的设备。`cpu`表示用CPU训练\n--load_model          读取已有的模型\n--disable_checkpoint  禁用保存检查点\n--savedir SAVEDIR     实验数据存储跟路径\n--total_frames TOTAL_FRAMES\n                      Total environment frames to train for\n--exp_epsilon EXP_EPSILON\n                      探索概率\n--batch_size BATCH_SIZE\n                      训练批尺寸\n--unroll_length UNROLL_LENGTH\n                      展开长度（时间维度）\n--num_buffers NUM_BUFFERS\n                      共享内存缓冲区的数量\n--num_threads NUM_THREADS\n                      学习者线程数\n--max_grad_norm MAX_GRAD_NORM\n                      最大梯度范数\n--learning_rate LEARNING_RATE\n                      学习率\n--alpha ALPHA         RMSProp平滑常数\n--momentum MOMENTUM   RMSProp momentum\n--epsilon EPSILON     RMSProp epsilon\n```\n\n## 评估\n评估可以使用GPU或CPU进行（GPU效率会高得多）。预训练模型可以通过[Google Drive](https://drive.google.com/drive/folders/1NmM2cXnI5CIWHaLJeoDZMiwt6lOTV_UB?usp=sharing)或[百度网盘](https://pan.baidu.com/s/18g-JUKad6D8rmBONXUDuOQ), 提取码: 4624 下载。将预训练权重放到`baselines/`目录下。模型性能通过自我对弈进行评估。我们提供了一些其他预训练模型和一些启发式方法作为基准：\n*   [random](douzero/evaluation/random_agent.py): 智能体随机出牌（均匀选择）\n*   [rlcard](douzero/evaluation/rlcard_agent.py): [RLCard](https://github.com/datamllab/rlcard)项目中的规则模型\n*   SL (`baselines/sl/`): 基于人类数据进行深度学习的预训练模型\n*   DouZero-ADP (`baselines/douzero_ADP/`): 以平均分数差异（Average Difference Points, ADP）为目标训练的Douzero智能体\n*   DouZero-WP (`baselines/douzero_WP/`): 以胜率（Winning Percentage, WP）为目标训练的Douzero智能体\n\n### 第1步：生成评估数据\n```\npython3 generate_eval_data.py\n```\n以下为一些重要的超参数。\n*   `--output`: pickle数据存储路径\n*   `--num_games`: 生成数据的游戏局数，默认值 10000\n\n## 第2步：自我对弈\n```\npython3 evaluate.py\n```\n以下为一些重要的超参数。\n*   `--landlord`: 扮演地主的智能体，可选值：random, rlcard或预训练模型的路径\n*   `--landlord_up`: 扮演地主上家的智能体，可选值：random, rlcard或预训练模型的路径\n*   `--landlord_down`: 扮演地主下家的智能体，可选值：random, rlcard或预训练模型的路径\n*   `--eval_data`: 包含评估数据的pickle文件\n*   `--num_workers`: 用多少个进程进行模拟\n*   `--gpu_device`: 用哪个GPU设备进行模拟。默认用CPU\n\n例如，可以通过以下命令评估DouZero-ADP智能体作为地主对阵随机智能体\n```\npython3 evaluate.py --landlord baselines/douzero_ADP/landlord.ckpt --landlord_up random --landlord_down random\n```\n以下命令可以评估DouZero-ADP智能体作为农民对阵RLCard智能体\n```\npython3 evaluate.py --landlord rlcard --landlord_up baselines/douzero_ADP/landlord_up.ckpt --landlord_down baselines/douzero_ADP/landlord_down.ckpt\n```\n默认情况下，我们的模型会每半小时保存在`douzero_checkpoints/douzero`路径下。我们提供了一个脚本帮助您定位最近一次保存检查点。运行\n```\nsh get_most_recent.sh douzero_checkpoints/douzero/\n```\n之后您可以在`most_recent_model`路径下找到最近一次保存的模型。\n\n## Windows下的问题\n如果您使用的是Windows系统并用GPU进行模拟，您将可能遇到`operation not supported`错误。这是由于Windows系统不支持CUDA tensor上的多进程。但是，由于我们的代码是对GPU进行优化，有对CUDA tensor的大量操作。如果您有解决方案，请联系我们！\n\n## 核心团队\n*   算法：[Daochen Zha](https://github.com/daochenzha), [Jingru Xie](https://github.com/karoka), Wenye Ma, Sheng Zhang, [Xiangru Lian](https://xrlian.com/), Xia Hu, [Ji Liu](http://jiliu-ml.org/)\n*   GUI演示：[Songyi Huang](https://github.com/hsywhu)\n*   社区贡献者: [@Vincentzyx](https://github.com/Vincentzyx)\n\n## 鸣谢\n*   本演示基于[RLCard-Showdown](https://github.com/datamllab/rlcard-showdown)项目\n*   代码实现受到[TorchBeast](https://github.com/facebookresearch/torchbeast)项目的启发\n"
        },
        {
          "name": "baselines",
          "type": "tree",
          "content": null
        },
        {
          "name": "douzero",
          "type": "tree",
          "content": null
        },
        {
          "name": "evaluate.py",
          "type": "blob",
          "size": 0.984375,
          "content": "import os \nimport argparse\n\nfrom douzero.evaluation.simulation import evaluate\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser(\n                    'Dou Dizhu Evaluation')\n    parser.add_argument('--landlord', type=str,\n            default='baselines/douzero_ADP/landlord.ckpt')\n    parser.add_argument('--landlord_up', type=str,\n            default='baselines/sl/landlord_up.ckpt')\n    parser.add_argument('--landlord_down', type=str,\n            default='baselines/sl/landlord_down.ckpt')\n    parser.add_argument('--eval_data', type=str,\n            default='eval_data.pkl')\n    parser.add_argument('--num_workers', type=int, default=5)\n    parser.add_argument('--gpu_device', type=str, default='')\n    args = parser.parse_args()\n\n    os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'\n    os.environ[\"CUDA_VISIBLE_DEVICES\"] = args.gpu_device\n\n    evaluate(args.landlord,\n             args.landlord_up,\n             args.landlord_down,\n             args.eval_data,\n             args.num_workers)\n"
        },
        {
          "name": "generate_eval_data.py",
          "type": "blob",
          "size": 1.203125,
          "content": "import argparse\nimport pickle\nimport numpy as np\n\ndeck = []\nfor i in range(3, 15):\n    deck.extend([i for _ in range(4)])\ndeck.extend([17 for _ in range(4)])\ndeck.extend([20, 30])\n\ndef get_parser():\n    parser = argparse.ArgumentParser(description='DouZero: random data generator')\n    parser.add_argument('--output', default='eval_data', type=str)\n    parser.add_argument('--num_games', default=10000, type=int)\n    return parser\n    \ndef generate():\n    _deck = deck.copy()\n    np.random.shuffle(_deck)\n    card_play_data = {'landlord': _deck[:20],\n                      'landlord_up': _deck[20:37],\n                      'landlord_down': _deck[37:54],\n                      'three_landlord_cards': _deck[17:20],\n                      }\n    for key in card_play_data:\n        card_play_data[key].sort()\n    return card_play_data\n\n\nif __name__ == '__main__':\n    flags = get_parser().parse_args()\n    output_pickle = flags.output + '.pkl'\n\n    print(\"output_pickle:\", output_pickle)\n    print(\"generating data...\")\n\n    data = []\n    for _ in range(flags.num_games):\n        data.append(generate())\n\n    print(\"saving pickle file...\")\n    with open(output_pickle,'wb') as g:\n        pickle.dump(data,g,pickle.HIGHEST_PROTOCOL)\n\n\n\n\n"
        },
        {
          "name": "get_most_recent.sh",
          "type": "blob",
          "size": 0.5341796875,
          "content": "checkpoint_dir=$1\n\nlandlord_path=$landlord_dir`ls -v \"$checkpoint_dir\"landlord_weights* | tail -1`\nlandlord_up_path=$landlord_up_dir`ls -v \"$checkpoint_dir\"landlord_up_weights* | tail -1`\nlandlord_down_path=$landlord_down_dir`ls -v \"$checkpoint_dir\"landlord_down_weights* | tail -1`\n\necho $landlord_path\necho $landlord_up_path\necho $landlord_down_path\n\nmkdir -p most_recent_model\n\ncp $landlord_path most_recent_model/landlord.ckpt\ncp $landlord_up_path most_recent_model/landlord_up.ckpt\ncp $landlord_down_path most_recent_model/landlord_down.ckpt\n"
        },
        {
          "name": "imgs",
          "type": "tree",
          "content": null
        },
        {
          "name": "requirements.txt",
          "type": "blob",
          "size": 0.0361328125,
          "content": "torch>=1.6.0\nGitPython\ngitdb2\nrlcard\n"
        },
        {
          "name": "setup.py",
          "type": "blob",
          "size": 0.9794921875,
          "content": "import setuptools\n\nVERSION = '1.1.0'\n\nwith open(\"README.md\", \"r\", encoding=\"utf8\") as fh:\n    long_description = fh.read()\n\nsetuptools.setup(\n    name=\"douzero\",\n    version=VERSION,\n    author=\"Daochen Zha\",\n    author_email=\"daochen.zha@tamu.edu\",\n    description=\"DouZero DouDizhu AI\",\n    long_description=long_description,\n    long_description_content_type=\"text/markdown\",\n    url=\"https://github.com/kwai/DouZero\",\n    license='Apache License 2.0',\n    keywords=[\"DouDizhu\", \"AI\", \"Reinforcment Learning\", \"RL\", \"Torch\", \"Poker\"],\n    packages=setuptools.find_packages(),\n    install_requires=[\n        'torch',\n        'rlcard'\n    ],\n    requires_python='>=3.6',\n    classifiers=[\n        \"Programming Language :: Python :: 3.9\",\n        \"Programming Language :: Python :: 3.8\",\n        \"Programming Language :: Python :: 3.7\",\n        \"Programming Language :: Python :: 3.6\",\n        \"License :: OSI Approved :: Apache Software License\",\n        \"Operating System :: OS Independent\",\n    ],\n)\n"
        },
        {
          "name": "train.py",
          "type": "blob",
          "size": 0.1806640625,
          "content": "import os\n\nfrom douzero.dmc import parser, train\n\nif __name__ == '__main__':\n    flags = parser.parse_args()\n    os.environ[\"CUDA_VISIBLE_DEVICES\"] = flags.gpu_devices\n    train(flags)\n"
        }
      ]
    }
  ]
}