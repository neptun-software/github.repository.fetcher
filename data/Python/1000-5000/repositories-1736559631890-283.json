{
  "metadata": {
    "timestamp": 1736559631890,
    "page": 283,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjI5MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "Kyubyong/transformer",
      "stars": 4313,
      "defaultBranch": "master",
      "files": [
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 1.150390625,
          "content": "# Byte-compiled / optimized / DLL files\n__pycache__/\n*.py[cod]\n*$py.class\n\n# C extensions\n*.so\n\n# Distribution / packaging\n.Python\nbuild/\ndevelop-eggs/\ndist/\ndownloads/\neggs/\n.eggs/\nlib/\nlib64/\nparts/\nsdist/\nvar/\nwheels/\n*.egg-info/\n.installed.cfg\n*.egg\n\n# PyInstaller\n#  Usually these files are written by a python script from a template\n#  before PyInstaller builds the exe, so as to inject date/other infos into it.\n*.manifest\n*.spec\n\n# Installer logs\npip-log.txt\npip-delete-this-directory.txt\n\n# Unit test / coverage reports\nhtmlcov/\n.tox/\n.coverage\n.coverage.*\n.cache\nnosetests.xml\ncoverage.xml\n*.cover\n.hypothesis/\n\n# Translations\n*.mo\n*.pot\n\n# Django stuff:\n*.log\nlocal_settings.py\n\n# Flask stuff:\ninstance/\n.webassets-cache\n\n# Scrapy stuff:\n.scrapy\n\n# Sphinx documentation\ndocs/_build/\n\n# PyBuilder\ntarget/\n\n# Jupyter Notebook\n.ipynb_checkpoints\n\n# pyenv\n.python-version\n\n# celery beat schedule file\ncelerybeat-schedule\n\n# SageMath parsed files\n*.sage.py\n\n# Environments\n.env\n.venv\nenv/\nvenv/\nENV/\n\n# Spyder project settings\n.spyderproject\n.spyproject\n\n# Rope project settings\n.ropeproject\n\n# mkdocs documentation\n/site\n\n# mypy\n.mypy_cache/\n\ncorpora\nlogdir\npreprocessed\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 11.0908203125,
          "content": "                                 Apache License\n                           Version 2.0, January 2004\n                        http://www.apache.org/licenses/\n\n   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n\n   1. Definitions.\n\n      \"License\" shall mean the terms and conditions for use, reproduction,\n      and distribution as defined by Sections 1 through 9 of this document.\n\n      \"Licensor\" shall mean the copyright owner or entity authorized by\n      the copyright owner that is granting the License.\n\n      \"Legal Entity\" shall mean the union of the acting entity and all\n      other entities that control, are controlled by, or are under common\n      control with that entity. For the purposes of this definition,\n      \"control\" means (i) the power, direct or indirect, to cause the\n      direction or management of such entity, whether by contract or\n      otherwise, or (ii) ownership of fifty percent (50%) or more of the\n      outstanding shares, or (iii) beneficial ownership of such entity.\n\n      \"You\" (or \"Your\") shall mean an individual or Legal Entity\n      exercising permissions granted by this License.\n\n      \"Source\" form shall mean the preferred form for making modifications,\n      including but not limited to software source code, documentation\n      source, and configuration files.\n\n      \"Object\" form shall mean any form resulting from mechanical\n      transformation or translation of a Source form, including but\n      not limited to compiled object code, generated documentation,\n      and conversions to other media types.\n\n      \"Work\" shall mean the work of authorship, whether in Source or\n      Object form, made available under the License, as indicated by a\n      copyright notice that is included in or attached to the work\n      (an example is provided in the Appendix below).\n\n      \"Derivative Works\" shall mean any work, whether in Source or Object\n      form, that is based on (or derived from) the Work and for which the\n      editorial revisions, annotations, elaborations, or other modifications\n      represent, as a whole, an original work of authorship. For the purposes\n      of this License, Derivative Works shall not include works that remain\n      separable from, or merely link (or bind by name) to the interfaces of,\n      the Work and Derivative Works thereof.\n\n      \"Contribution\" shall mean any work of authorship, including\n      the original version of the Work and any modifications or additions\n      to that Work or Derivative Works thereof, that is intentionally\n      submitted to Licensor for inclusion in the Work by the copyright owner\n      or by an individual or Legal Entity authorized to submit on behalf of\n      the copyright owner. For the purposes of this definition, \"submitted\"\n      means any form of electronic, verbal, or written communication sent\n      to the Licensor or its representatives, including but not limited to\n      communication on electronic mailing lists, source code control systems,\n      and issue tracking systems that are managed by, or on behalf of, the\n      Licensor for the purpose of discussing and improving the Work, but\n      excluding communication that is conspicuously marked or otherwise\n      designated in writing by the copyright owner as \"Not a Contribution.\"\n\n      \"Contributor\" shall mean Licensor and any individual or Legal Entity\n      on behalf of whom a Contribution has been received by Licensor and\n      subsequently incorporated within the Work.\n\n   2. Grant of Copyright License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      copyright license to reproduce, prepare Derivative Works of,\n      publicly display, publicly perform, sublicense, and distribute the\n      Work and such Derivative Works in Source or Object form.\n\n   3. Grant of Patent License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      (except as stated in this section) patent license to make, have made,\n      use, offer to sell, sell, import, and otherwise transfer the Work,\n      where such license applies only to those patent claims licensable\n      by such Contributor that are necessarily infringed by their\n      Contribution(s) alone or by combination of their Contribution(s)\n      with the Work to which such Contribution(s) was submitted. If You\n      institute patent litigation against any entity (including a\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\n      or a Contribution incorporated within the Work constitutes direct\n      or contributory patent infringement, then any patent licenses\n      granted to You under this License for that Work shall terminate\n      as of the date such litigation is filed.\n\n   4. Redistribution. You may reproduce and distribute copies of the\n      Work or Derivative Works thereof in any medium, with or without\n      modifications, and in Source or Object form, provided that You\n      meet the following conditions:\n\n      (a) You must give any other recipients of the Work or\n          Derivative Works a copy of this License; and\n\n      (b) You must cause any modified files to carry prominent notices\n          stating that You changed the files; and\n\n      (c) You must retain, in the Source form of any Derivative Works\n          that You distribute, all copyright, patent, trademark, and\n          attribution notices from the Source form of the Work,\n          excluding those notices that do not pertain to any part of\n          the Derivative Works; and\n\n      (d) If the Work includes a \"NOTICE\" text file as part of its\n          distribution, then any Derivative Works that You distribute must\n          include a readable copy of the attribution notices contained\n          within such NOTICE file, excluding those notices that do not\n          pertain to any part of the Derivative Works, in at least one\n          of the following places: within a NOTICE text file distributed\n          as part of the Derivative Works; within the Source form or\n          documentation, if provided along with the Derivative Works; or,\n          within a display generated by the Derivative Works, if and\n          wherever such third-party notices normally appear. The contents\n          of the NOTICE file are for informational purposes only and\n          do not modify the License. You may add Your own attribution\n          notices within Derivative Works that You distribute, alongside\n          or as an addendum to the NOTICE text from the Work, provided\n          that such additional attribution notices cannot be construed\n          as modifying the License.\n\n      You may add Your own copyright statement to Your modifications and\n      may provide additional or different license terms and conditions\n      for use, reproduction, or distribution of Your modifications, or\n      for any such Derivative Works as a whole, provided Your use,\n      reproduction, and distribution of the Work otherwise complies with\n      the conditions stated in this License.\n\n   5. Submission of Contributions. Unless You explicitly state otherwise,\n      any Contribution intentionally submitted for inclusion in the Work\n      by You to the Licensor shall be under the terms and conditions of\n      this License, without any additional terms or conditions.\n      Notwithstanding the above, nothing herein shall supersede or modify\n      the terms of any separate license agreement you may have executed\n      with Licensor regarding such Contributions.\n\n   6. Trademarks. This License does not grant permission to use the trade\n      names, trademarks, service marks, or product names of the Licensor,\n      except as required for reasonable and customary use in describing the\n      origin of the Work and reproducing the content of the NOTICE file.\n\n   7. Disclaimer of Warranty. Unless required by applicable law or\n      agreed to in writing, Licensor provides the Work (and each\n      Contributor provides its Contributions) on an \"AS IS\" BASIS,\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n      implied, including, without limitation, any warranties or conditions\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n      PARTICULAR PURPOSE. You are solely responsible for determining the\n      appropriateness of using or redistributing the Work and assume any\n      risks associated with Your exercise of permissions under this License.\n\n   8. Limitation of Liability. In no event and under no legal theory,\n      whether in tort (including negligence), contract, or otherwise,\n      unless required by applicable law (such as deliberate and grossly\n      negligent acts) or agreed to in writing, shall any Contributor be\n      liable to You for damages, including any direct, indirect, special,\n      incidental, or consequential damages of any character arising as a\n      result of this License or out of the use or inability to use the\n      Work (including but not limited to damages for loss of goodwill,\n      work stoppage, computer failure or malfunction, or any and all\n      other commercial damages or losses), even if such Contributor\n      has been advised of the possibility of such damages.\n\n   9. Accepting Warranty or Additional Liability. While redistributing\n      the Work or Derivative Works thereof, You may choose to offer,\n      and charge a fee for, acceptance of support, warranty, indemnity,\n      or other liability obligations and/or rights consistent with this\n      License. However, in accepting such obligations, You may act only\n      on Your own behalf and on Your sole responsibility, not on behalf\n      of any other Contributor, and only if You agree to indemnify,\n      defend, and hold each Contributor harmless for any liability\n      incurred by, or claims asserted against, such Contributor by reason\n      of your accepting any such warranty or additional liability.\n\n   END OF TERMS AND CONDITIONS\n\n   APPENDIX: How to apply the Apache License to your work.\n\n      To apply the Apache License to your work, attach the following\n      boilerplate notice, with the fields enclosed by brackets \"{}\"\n      replaced with your own identifying information. (Don't include\n      the brackets!)  The text should be enclosed in the appropriate\n      comment syntax for the file format. We also recommend that a\n      file or class name and description of purpose be included on the\n      same \"printed page\" as the copyright notice for easier\n      identification within third-party archives.\n\n   Copyright {yyyy} {name of copyright owner}\n\n   Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 2.734375,
          "content": "# **[UPDATED]** A TensorFlow Implementation of [Attention Is All You Need](https://arxiv.org/abs/1706.03762)\n\nWhen I opened this repository in 2017, there was no official code yet.\nI tried to implement the paper as I understood, but to no surprise\nit had several bugs. I realized them mostly thanks to people who issued here, so\nI'm very grateful to all of them. Though there is the [official implementation](https://github.com/tensorflow/tensor2tensor) as well as\nseveral other unofficial github repos, I decided to update my own one.\nThis update focuses on:\n* readable / understandable code writing\n* modularization (but not too much)\n* revising known bugs. (masking, positional encoding, ...)\n* updating to TF1.12. (tf.data, ...)\n* adding some missing components (bpe, shared weight matrix, ...)\n* including useful comments in the code.\n\nI still stick to IWSLT 2016 de-en. I guess if you'd like to test on a big data such\nas WMT, you would rely on the official implementation.\nAfter all, it's pleasant to check quickly if your model works.\nThe initial code for TF1.2 is moved to the [tf1.2_lecacy](tf1.2_legacy) folder for the record.\n\n## Requirements\n* python==3.x (Let's move on to python 3 if you still use python 2)\n* tensorflow==1.12.0\n* numpy>=1.15.4\n* sentencepiece==0.1.8\n* tqdm>=4.28.1\n\n## Training\n* STEP 1. Run the command below to download [IWSLT 2016 German–English parallel corpus](https://wit3.fbk.eu/download.php?release=2016-01&type=texts&slang=de&tlang=en).\n```\nbash download.sh\n```\n It should be extracted to `iwslt2016/de-en` folder automatically.\n* STEP 2. Run the command below to create preprocessed train/eval/test data.\n```\npython prepro.py\n```\nIf you want to change the vocabulary size (default:32000), do this.\n```\npython prepro.py --vocab_size 8000\n```\nIt should create two folders `iwslt2016/prepro` and `iwslt2016/segmented`.\n\n* STEP 3. Run the following command.\n```\npython train.py\n```\nCheck `hparams.py` to see which parameters are possible. For example,\n```\npython train.py --logdir myLog --batch_size 256 --dropout_rate 0.5\n```\n\n* STEP 3. Or download the pretrained models.\n```\nwget https://dl.dropbox.com/s/4lom1czy5xfzr4q/log.zip; unzip log.zip; rm log.zip\n```\n\n\n## Training Loss Curve\n<img src=\"fig/loss.png\">\n\n## Learning rate\n<img src=\"fig/lr.png\">\n\n## Bleu score on devset\n<img src=\"fig/bleu.png\">\n\n\n## Inference (=test)\n* Run\n```\npython test.py --ckpt log/1/iwslt2016_E19L2.64-29146 (OR yourCkptFile OR yourCkptFileDirectory)\n```\n\n## Results\n* Typically, machine translation is evaluated with Bleu score.\n* All evaluation results are available in [eval/1](eval/1) and [test/1](test/1).\n\n|tst2013 (dev) | tst2014 (test) |\n|--|--|\n|28.06|23.88|\n\n## Notes\n* Beam decoding will be added soon.\n* I'm going to update the code when TF2.0 comes out if possible."
        },
        {
          "name": "data_load.py",
          "type": "blob",
          "size": 5.0498046875,
          "content": "# -*- coding: utf-8 -*-\r\n#/usr/bin/python3\r\n'''\r\nFeb. 2019 by kyubyong park.\r\nkbpark.linguist@gmail.com.\r\nhttps://www.github.com/kyubyong/transformer\r\n\r\nNote.\r\nif safe, entities on the source side have the prefix 1, and the target side 2, for convenience.\r\nFor example, fpath1, fpath2 means source file path and target file path, respectively.\r\n'''\r\nimport tensorflow as tf\r\nfrom utils import calc_num_batches\r\n\r\ndef load_vocab(vocab_fpath):\r\n    '''Loads vocabulary file and returns idx<->token maps\r\n    vocab_fpath: string. vocabulary file path.\r\n    Note that these are reserved\r\n    0: <pad>, 1: <unk>, 2: <s>, 3: </s>\r\n\r\n    Returns\r\n    two dictionaries.\r\n    '''\r\n    vocab = [line.split()[0] for line in open(vocab_fpath, 'r').read().splitlines()]\r\n    token2idx = {token: idx for idx, token in enumerate(vocab)}\r\n    idx2token = {idx: token for idx, token in enumerate(vocab)}\r\n    return token2idx, idx2token\r\n\r\ndef load_data(fpath1, fpath2, maxlen1, maxlen2):\r\n    '''Loads source and target data and filters out too lengthy samples.\r\n    fpath1: source file path. string.\r\n    fpath2: target file path. string.\r\n    maxlen1: source sent maximum length. scalar.\r\n    maxlen2: target sent maximum length. scalar.\r\n\r\n    Returns\r\n    sents1: list of source sents\r\n    sents2: list of target sents\r\n    '''\r\n    sents1, sents2 = [], []\r\n    with open(fpath1, 'r') as f1, open(fpath2, 'r') as f2:\r\n        for sent1, sent2 in zip(f1, f2):\r\n            if len(sent1.split()) + 1 > maxlen1: continue # 1: </s>\r\n            if len(sent2.split()) + 1 > maxlen2: continue  # 1: </s>\r\n            sents1.append(sent1.strip())\r\n            sents2.append(sent2.strip())\r\n    return sents1, sents2\r\n\r\n\r\ndef encode(inp, type, dict):\r\n    '''Converts string to number. Used for `generator_fn`.\r\n    inp: 1d byte array.\r\n    type: \"x\" (source side) or \"y\" (target side)\r\n    dict: token2idx dictionary\r\n\r\n    Returns\r\n    list of numbers\r\n    '''\r\n    inp_str = inp.decode(\"utf-8\")\r\n    if type==\"x\": tokens = inp_str.split() + [\"</s>\"]\r\n    else: tokens = [\"<s>\"] + inp_str.split() + [\"</s>\"]\r\n\r\n    x = [dict.get(t, dict[\"<unk>\"]) for t in tokens]\r\n    return x\r\n\r\ndef generator_fn(sents1, sents2, vocab_fpath):\r\n    '''Generates training / evaluation data\r\n    sents1: list of source sents\r\n    sents2: list of target sents\r\n    vocab_fpath: string. vocabulary file path.\r\n\r\n    yields\r\n    xs: tuple of\r\n        x: list of source token ids in a sent\r\n        x_seqlen: int. sequence length of x\r\n        sent1: str. raw source (=input) sentence\r\n    labels: tuple of\r\n        decoder_input: decoder_input: list of encoded decoder inputs\r\n        y: list of target token ids in a sent\r\n        y_seqlen: int. sequence length of y\r\n        sent2: str. target sentence\r\n    '''\r\n    token2idx, _ = load_vocab(vocab_fpath)\r\n    for sent1, sent2 in zip(sents1, sents2):\r\n        x = encode(sent1, \"x\", token2idx)\r\n        y = encode(sent2, \"y\", token2idx)\r\n        decoder_input, y = y[:-1], y[1:]\r\n\r\n        x_seqlen, y_seqlen = len(x), len(y)\r\n        yield (x, x_seqlen, sent1), (decoder_input, y, y_seqlen, sent2)\r\n\r\ndef input_fn(sents1, sents2, vocab_fpath, batch_size, shuffle=False):\r\n    '''Batchify data\r\n    sents1: list of source sents\r\n    sents2: list of target sents\r\n    vocab_fpath: string. vocabulary file path.\r\n    batch_size: scalar\r\n    shuffle: boolean\r\n\r\n    Returns\r\n    xs: tuple of\r\n        x: int32 tensor. (N, T1)\r\n        x_seqlens: int32 tensor. (N,)\r\n        sents1: str tensor. (N,)\r\n    ys: tuple of\r\n        decoder_input: int32 tensor. (N, T2)\r\n        y: int32 tensor. (N, T2)\r\n        y_seqlen: int32 tensor. (N, )\r\n        sents2: str tensor. (N,)\r\n    '''\r\n    shapes = (([None], (), ()),\r\n              ([None], [None], (), ()))\r\n    types = ((tf.int32, tf.int32, tf.string),\r\n             (tf.int32, tf.int32, tf.int32, tf.string))\r\n    paddings = ((0, 0, ''),\r\n                (0, 0, 0, ''))\r\n\r\n    dataset = tf.data.Dataset.from_generator(\r\n        generator_fn,\r\n        output_shapes=shapes,\r\n        output_types=types,\r\n        args=(sents1, sents2, vocab_fpath))  # <- arguments for generator_fn. converted to np string arrays\r\n\r\n    if shuffle: # for training\r\n        dataset = dataset.shuffle(128*batch_size)\r\n\r\n    dataset = dataset.repeat()  # iterate forever\r\n    dataset = dataset.padded_batch(batch_size, shapes, paddings).prefetch(1)\r\n\r\n    return dataset\r\n\r\ndef get_batch(fpath1, fpath2, maxlen1, maxlen2, vocab_fpath, batch_size, shuffle=False):\r\n    '''Gets training / evaluation mini-batches\r\n    fpath1: source file path. string.\r\n    fpath2: target file path. string.\r\n    maxlen1: source sent maximum length. scalar.\r\n    maxlen2: target sent maximum length. scalar.\r\n    vocab_fpath: string. vocabulary file path.\r\n    batch_size: scalar\r\n    shuffle: boolean\r\n\r\n    Returns\r\n    batches\r\n    num_batches: number of mini-batches\r\n    num_samples\r\n    '''\r\n    sents1, sents2 = load_data(fpath1, fpath2, maxlen1, maxlen2)\r\n    batches = input_fn(sents1, sents2, vocab_fpath, batch_size, shuffle=shuffle)\r\n    num_batches = calc_num_batches(len(sents1), batch_size)\r\n    return batches, num_batches, len(sents1)\r\n"
        },
        {
          "name": "download.sh",
          "type": "blob",
          "size": 0.140625,
          "content": "#!/bin/bash\n\nmkdir iwslt2016 | wget -qO- --show-progress https://wit3.fbk.eu/archive/2016-01//texts/de/en/de-en.tgz | tar xz; mv de-en iwslt2016"
        },
        {
          "name": "eval",
          "type": "tree",
          "content": null
        },
        {
          "name": "fig",
          "type": "tree",
          "content": null
        },
        {
          "name": "hparams.py",
          "type": "blob",
          "size": 2.94140625,
          "content": "import argparse\n\nclass Hparams:\n    parser = argparse.ArgumentParser()\n\n    # prepro\n    parser.add_argument('--vocab_size', default=32000, type=int)\n\n    # train\n    ## files\n    parser.add_argument('--train1', default='iwslt2016/segmented/train.de.bpe',\n                             help=\"german training segmented data\")\n    parser.add_argument('--train2', default='iwslt2016/segmented/train.en.bpe',\n                             help=\"english training segmented data\")\n    parser.add_argument('--eval1', default='iwslt2016/segmented/eval.de.bpe',\n                             help=\"german evaluation segmented data\")\n    parser.add_argument('--eval2', default='iwslt2016/segmented/eval.en.bpe',\n                             help=\"english evaluation segmented data\")\n    parser.add_argument('--eval3', default='iwslt2016/prepro/eval.en',\n                             help=\"english evaluation unsegmented data\")\n\n    ## vocabulary\n    parser.add_argument('--vocab', default='iwslt2016/segmented/bpe.vocab',\n                        help=\"vocabulary file path\")\n\n    # training scheme\n    parser.add_argument('--batch_size', default=128, type=int)\n    parser.add_argument('--eval_batch_size', default=128, type=int)\n\n    parser.add_argument('--lr', default=0.0003, type=float, help=\"learning rate\")\n    parser.add_argument('--warmup_steps', default=4000, type=int)\n    parser.add_argument('--logdir', default=\"log/1\", help=\"log directory\")\n    parser.add_argument('--num_epochs', default=20, type=int)\n    parser.add_argument('--evaldir', default=\"eval/1\", help=\"evaluation dir\")\n\n    # model\n    parser.add_argument('--d_model', default=512, type=int,\n                        help=\"hidden dimension of encoder/decoder\")\n    parser.add_argument('--d_ff', default=2048, type=int,\n                        help=\"hidden dimension of feedforward layer\")\n    parser.add_argument('--num_blocks', default=6, type=int,\n                        help=\"number of encoder/decoder blocks\")\n    parser.add_argument('--num_heads', default=8, type=int,\n                        help=\"number of attention heads\")\n    parser.add_argument('--maxlen1', default=100, type=int,\n                        help=\"maximum length of a source sequence\")\n    parser.add_argument('--maxlen2', default=100, type=int,\n                        help=\"maximum length of a target sequence\")\n    parser.add_argument('--dropout_rate', default=0.3, type=float)\n    parser.add_argument('--smoothing', default=0.1, type=float,\n                        help=\"label smoothing rate\")\n\n    # test\n    parser.add_argument('--test1', default='iwslt2016/segmented/test.de.bpe',\n                        help=\"german test segmented data\")\n    parser.add_argument('--test2', default='iwslt2016/prepro/test.en',\n                        help=\"english test data\")\n    parser.add_argument('--ckpt', help=\"checkpoint file path\")\n    parser.add_argument('--test_batch_size', default=128, type=int)\n    parser.add_argument('--testdir', default=\"test/1\", help=\"test result dir\")"
        },
        {
          "name": "model.py",
          "type": "blob",
          "size": 7.4814453125,
          "content": "# -*- coding: utf-8 -*-\n# /usr/bin/python3\n'''\nFeb. 2019 by kyubyong park.\nkbpark.linguist@gmail.com.\nhttps://www.github.com/kyubyong/transformer\n\nTransformer network\n'''\nimport tensorflow as tf\n\nfrom data_load import load_vocab\nfrom modules import get_token_embeddings, ff, positional_encoding, multihead_attention, label_smoothing, noam_scheme\nfrom utils import convert_idx_to_token_tensor\nfrom tqdm import tqdm\nimport logging\n\nlogging.basicConfig(level=logging.INFO)\n\nclass Transformer:\n    '''\n    xs: tuple of\n        x: int32 tensor. (N, T1)\n        x_seqlens: int32 tensor. (N,)\n        sents1: str tensor. (N,)\n    ys: tuple of\n        decoder_input: int32 tensor. (N, T2)\n        y: int32 tensor. (N, T2)\n        y_seqlen: int32 tensor. (N, )\n        sents2: str tensor. (N,)\n    training: boolean.\n    '''\n    def __init__(self, hp):\n        self.hp = hp\n        self.token2idx, self.idx2token = load_vocab(hp.vocab)\n        self.embeddings = get_token_embeddings(self.hp.vocab_size, self.hp.d_model, zero_pad=True)\n\n    def encode(self, xs, training=True):\n        '''\n        Returns\n        memory: encoder outputs. (N, T1, d_model)\n        '''\n        with tf.variable_scope(\"encoder\", reuse=tf.AUTO_REUSE):\n            x, seqlens, sents1 = xs\n\n            # src_masks\n            src_masks = tf.math.equal(x, 0) # (N, T1)\n\n            # embedding\n            enc = tf.nn.embedding_lookup(self.embeddings, x) # (N, T1, d_model)\n            enc *= self.hp.d_model**0.5 # scale\n\n            enc += positional_encoding(enc, self.hp.maxlen1)\n            enc = tf.layers.dropout(enc, self.hp.dropout_rate, training=training)\n\n            ## Blocks\n            for i in range(self.hp.num_blocks):\n                with tf.variable_scope(\"num_blocks_{}\".format(i), reuse=tf.AUTO_REUSE):\n                    # self-attention\n                    enc = multihead_attention(queries=enc,\n                                              keys=enc,\n                                              values=enc,\n                                              key_masks=src_masks,\n                                              num_heads=self.hp.num_heads,\n                                              dropout_rate=self.hp.dropout_rate,\n                                              training=training,\n                                              causality=False)\n                    # feed forward\n                    enc = ff(enc, num_units=[self.hp.d_ff, self.hp.d_model])\n        memory = enc\n        return memory, sents1, src_masks\n\n    def decode(self, ys, memory, src_masks, training=True):\n        '''\n        memory: encoder outputs. (N, T1, d_model)\n        src_masks: (N, T1)\n\n        Returns\n        logits: (N, T2, V). float32.\n        y_hat: (N, T2). int32\n        y: (N, T2). int32\n        sents2: (N,). string.\n        '''\n        with tf.variable_scope(\"decoder\", reuse=tf.AUTO_REUSE):\n            decoder_inputs, y, seqlens, sents2 = ys\n\n            # tgt_masks\n            tgt_masks = tf.math.equal(decoder_inputs, 0)  # (N, T2)\n\n            # embedding\n            dec = tf.nn.embedding_lookup(self.embeddings, decoder_inputs)  # (N, T2, d_model)\n            dec *= self.hp.d_model ** 0.5  # scale\n\n            dec += positional_encoding(dec, self.hp.maxlen2)\n            dec = tf.layers.dropout(dec, self.hp.dropout_rate, training=training)\n\n            # Blocks\n            for i in range(self.hp.num_blocks):\n                with tf.variable_scope(\"num_blocks_{}\".format(i), reuse=tf.AUTO_REUSE):\n                    # Masked self-attention (Note that causality is True at this time)\n                    dec = multihead_attention(queries=dec,\n                                              keys=dec,\n                                              values=dec,\n                                              key_masks=tgt_masks,\n                                              num_heads=self.hp.num_heads,\n                                              dropout_rate=self.hp.dropout_rate,\n                                              training=training,\n                                              causality=True,\n                                              scope=\"self_attention\")\n\n                    # Vanilla attention\n                    dec = multihead_attention(queries=dec,\n                                              keys=memory,\n                                              values=memory,\n                                              key_masks=src_masks,\n                                              num_heads=self.hp.num_heads,\n                                              dropout_rate=self.hp.dropout_rate,\n                                              training=training,\n                                              causality=False,\n                                              scope=\"vanilla_attention\")\n                    ### Feed Forward\n                    dec = ff(dec, num_units=[self.hp.d_ff, self.hp.d_model])\n\n        # Final linear projection (embedding weights are shared)\n        weights = tf.transpose(self.embeddings) # (d_model, vocab_size)\n        logits = tf.einsum('ntd,dk->ntk', dec, weights) # (N, T2, vocab_size)\n        y_hat = tf.to_int32(tf.argmax(logits, axis=-1))\n\n        return logits, y_hat, y, sents2\n\n    def train(self, xs, ys):\n        '''\n        Returns\n        loss: scalar.\n        train_op: training operation\n        global_step: scalar.\n        summaries: training summary node\n        '''\n        # forward\n        memory, sents1, src_masks = self.encode(xs)\n        logits, preds, y, sents2 = self.decode(ys, memory, src_masks)\n\n        # train scheme\n        y_ = label_smoothing(tf.one_hot(y, depth=self.hp.vocab_size))\n        ce = tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=y_)\n        nonpadding = tf.to_float(tf.not_equal(y, self.token2idx[\"<pad>\"]))  # 0: <pad>\n        loss = tf.reduce_sum(ce * nonpadding) / (tf.reduce_sum(nonpadding) + 1e-7)\n\n        global_step = tf.train.get_or_create_global_step()\n        lr = noam_scheme(self.hp.lr, global_step, self.hp.warmup_steps)\n        optimizer = tf.train.AdamOptimizer(lr)\n        train_op = optimizer.minimize(loss, global_step=global_step)\n\n        tf.summary.scalar('lr', lr)\n        tf.summary.scalar(\"loss\", loss)\n        tf.summary.scalar(\"global_step\", global_step)\n\n        summaries = tf.summary.merge_all()\n\n        return loss, train_op, global_step, summaries\n\n    def eval(self, xs, ys):\n        '''Predicts autoregressively\n        At inference, input ys is ignored.\n        Returns\n        y_hat: (N, T2)\n        '''\n        decoder_inputs, y, y_seqlen, sents2 = ys\n\n        decoder_inputs = tf.ones((tf.shape(xs[0])[0], 1), tf.int32) * self.token2idx[\"<s>\"]\n        ys = (decoder_inputs, y, y_seqlen, sents2)\n\n        memory, sents1, src_masks = self.encode(xs, False)\n\n        logging.info(\"Inference graph is being built. Please be patient.\")\n        for _ in tqdm(range(self.hp.maxlen2)):\n            logits, y_hat, y, sents2 = self.decode(ys, memory, src_masks, False)\n            if tf.reduce_sum(y_hat, 1) == self.token2idx[\"<pad>\"]: break\n\n            _decoder_inputs = tf.concat((decoder_inputs, y_hat), 1)\n            ys = (_decoder_inputs, y, y_seqlen, sents2)\n\n        # monitor a random sample\n        n = tf.random_uniform((), 0, tf.shape(y_hat)[0]-1, tf.int32)\n        sent1 = sents1[n]\n        pred = convert_idx_to_token_tensor(y_hat[n], self.idx2token)\n        sent2 = sents2[n]\n\n        tf.summary.text(\"sent1\", sent1)\n        tf.summary.text(\"pred\", pred)\n        tf.summary.text(\"sent2\", sent2)\n        summaries = tf.summary.merge_all()\n\n        return y_hat, summaries\n\n"
        },
        {
          "name": "modules.py",
          "type": "blob",
          "size": 11.1884765625,
          "content": "# -*- coding: utf-8 -*-\n#/usr/bin/python3\n'''\nFeb. 2019 by kyubyong park.\nkbpark.linguist@gmail.com.\nhttps://www.github.com/kyubyong/transformer.\n\nBuilding blocks for Transformer\n'''\n\nimport numpy as np\nimport tensorflow as tf\n\ndef ln(inputs, epsilon = 1e-8, scope=\"ln\"):\n    '''Applies layer normalization. See https://arxiv.org/abs/1607.06450.\n    inputs: A tensor with 2 or more dimensions, where the first dimension has `batch_size`.\n    epsilon: A floating number. A very small number for preventing ZeroDivision Error.\n    scope: Optional scope for `variable_scope`.\n      \n    Returns:\n      A tensor with the same shape and data dtype as `inputs`.\n    '''\n    with tf.variable_scope(scope, reuse=tf.AUTO_REUSE):\n        inputs_shape = inputs.get_shape()\n        params_shape = inputs_shape[-1:]\n    \n        mean, variance = tf.nn.moments(inputs, [-1], keep_dims=True)\n        beta= tf.get_variable(\"beta\", params_shape, initializer=tf.zeros_initializer())\n        gamma = tf.get_variable(\"gamma\", params_shape, initializer=tf.ones_initializer())\n        normalized = (inputs - mean) / ( (variance + epsilon) ** (.5) )\n        outputs = gamma * normalized + beta\n        \n    return outputs\n\ndef get_token_embeddings(vocab_size, num_units, zero_pad=True):\n    '''Constructs token embedding matrix.\n    Note that the column of index 0's are set to zeros.\n    vocab_size: scalar. V.\n    num_units: embedding dimensionalty. E.\n    zero_pad: Boolean. If True, all the values of the first row (id = 0) should be constant zero\n    To apply query/key masks easily, zero pad is turned on.\n\n    Returns\n    weight variable: (V, E)\n    '''\n    with tf.variable_scope(\"shared_weight_matrix\"):\n        embeddings = tf.get_variable('weight_mat',\n                                   dtype=tf.float32,\n                                   shape=(vocab_size, num_units),\n                                   initializer=tf.contrib.layers.xavier_initializer())\n        if zero_pad:\n            embeddings = tf.concat((tf.zeros(shape=[1, num_units]),\n                                    embeddings[1:, :]), 0)\n    return embeddings\n\ndef scaled_dot_product_attention(Q, K, V, key_masks,\n                                 causality=False, dropout_rate=0.,\n                                 training=True,\n                                 scope=\"scaled_dot_product_attention\"):\n    '''See 3.2.1.\n    Q: Packed queries. 3d tensor. [N, T_q, d_k].\n    K: Packed keys. 3d tensor. [N, T_k, d_k].\n    V: Packed values. 3d tensor. [N, T_k, d_v].\n    key_masks: A 2d tensor with shape of [N, key_seqlen]\n    causality: If True, applies masking for future blinding\n    dropout_rate: A floating point number of [0, 1].\n    training: boolean for controlling droput\n    scope: Optional scope for `variable_scope`.\n    '''\n    with tf.variable_scope(scope, reuse=tf.AUTO_REUSE):\n        d_k = Q.get_shape().as_list()[-1]\n\n        # dot product\n        outputs = tf.matmul(Q, tf.transpose(K, [0, 2, 1]))  # (N, T_q, T_k)\n\n        # scale\n        outputs /= d_k ** 0.5\n\n        # key masking\n        outputs = mask(outputs, key_masks=key_masks, type=\"key\")\n\n        # causality or future blinding masking\n        if causality:\n            outputs = mask(outputs, type=\"future\")\n\n        # softmax\n        outputs = tf.nn.softmax(outputs)\n        attention = tf.transpose(outputs, [0, 2, 1])\n        tf.summary.image(\"attention\", tf.expand_dims(attention[:1], -1))\n\n        # # query masking\n        # outputs = mask(outputs, Q, K, type=\"query\")\n\n        # dropout\n        outputs = tf.layers.dropout(outputs, rate=dropout_rate, training=training)\n\n        # weighted sum (context vectors)\n        outputs = tf.matmul(outputs, V)  # (N, T_q, d_v)\n\n    return outputs\n\n\ndef mask(inputs, key_masks=None, type=None):\n    \"\"\"Masks paddings on keys or queries to inputs\n    inputs: 3d tensor. (h*N, T_q, T_k)\n    key_masks: 3d tensor. (N, 1, T_k)\n    type: string. \"key\" | \"future\"\n\n    e.g.,\n    >> inputs = tf.zeros([2, 2, 3], dtype=tf.float32)\n    >> key_masks = tf.constant([[0., 0., 1.],\n                                [0., 1., 1.]])\n    >> mask(inputs, key_masks=key_masks, type=\"key\")\n    array([[[ 0.0000000e+00,  0.0000000e+00, -4.2949673e+09],\n        [ 0.0000000e+00,  0.0000000e+00, -4.2949673e+09]],\n\n       [[ 0.0000000e+00, -4.2949673e+09, -4.2949673e+09],\n        [ 0.0000000e+00, -4.2949673e+09, -4.2949673e+09]],\n\n       [[ 0.0000000e+00,  0.0000000e+00, -4.2949673e+09],\n        [ 0.0000000e+00,  0.0000000e+00, -4.2949673e+09]],\n\n       [[ 0.0000000e+00, -4.2949673e+09, -4.2949673e+09],\n        [ 0.0000000e+00, -4.2949673e+09, -4.2949673e+09]]], dtype=float32)\n    \"\"\"\n    padding_num = -2 ** 32 + 1\n    if type in (\"k\", \"key\", \"keys\"):\n        key_masks = tf.to_float(key_masks)\n        key_masks = tf.tile(key_masks, [tf.shape(inputs)[0] // tf.shape(key_masks)[0], 1]) # (h*N, seqlen)\n        key_masks = tf.expand_dims(key_masks, 1)  # (h*N, 1, seqlen)\n        outputs = inputs + key_masks * padding_num\n    # elif type in (\"q\", \"query\", \"queries\"):\n    #     # Generate masks\n    #     masks = tf.sign(tf.reduce_sum(tf.abs(queries), axis=-1))  # (N, T_q)\n    #     masks = tf.expand_dims(masks, -1)  # (N, T_q, 1)\n    #     masks = tf.tile(masks, [1, 1, tf.shape(keys)[1]])  # (N, T_q, T_k)\n    #\n    #     # Apply masks to inputs\n    #     outputs = inputs*masks\n    elif type in (\"f\", \"future\", \"right\"):\n        diag_vals = tf.ones_like(inputs[0, :, :])  # (T_q, T_k)\n        tril = tf.linalg.LinearOperatorLowerTriangular(diag_vals).to_dense()  # (T_q, T_k)\n        future_masks = tf.tile(tf.expand_dims(tril, 0), [tf.shape(inputs)[0], 1, 1])  # (N, T_q, T_k)\n\n        paddings = tf.ones_like(future_masks) * padding_num\n        outputs = tf.where(tf.equal(future_masks, 0), paddings, inputs)\n    else:\n        print(\"Check if you entered type correctly!\")\n\n    return outputs\n\n\ndef multihead_attention(queries, keys, values, key_masks,\n                        num_heads=8, \n                        dropout_rate=0,\n                        training=True,\n                        causality=False,\n                        scope=\"multihead_attention\"):\n    '''Applies multihead attention. See 3.2.2\n    queries: A 3d tensor with shape of [N, T_q, d_model].\n    keys: A 3d tensor with shape of [N, T_k, d_model].\n    values: A 3d tensor with shape of [N, T_k, d_model].\n    key_masks: A 2d tensor with shape of [N, key_seqlen]\n    num_heads: An int. Number of heads.\n    dropout_rate: A floating point number.\n    training: Boolean. Controller of mechanism for dropout.\n    causality: Boolean. If true, units that reference the future are masked.\n    scope: Optional scope for `variable_scope`.\n        \n    Returns\n      A 3d tensor with shape of (N, T_q, C)  \n    '''\n    d_model = queries.get_shape().as_list()[-1]\n    with tf.variable_scope(scope, reuse=tf.AUTO_REUSE):\n        # Linear projections\n        Q = tf.layers.dense(queries, d_model, use_bias=True) # (N, T_q, d_model)\n        K = tf.layers.dense(keys, d_model, use_bias=True) # (N, T_k, d_model)\n        V = tf.layers.dense(values, d_model, use_bias=True) # (N, T_k, d_model)\n        \n        # Split and concat\n        Q_ = tf.concat(tf.split(Q, num_heads, axis=2), axis=0) # (h*N, T_q, d_model/h)\n        K_ = tf.concat(tf.split(K, num_heads, axis=2), axis=0) # (h*N, T_k, d_model/h)\n        V_ = tf.concat(tf.split(V, num_heads, axis=2), axis=0) # (h*N, T_k, d_model/h)\n\n        # Attention\n        outputs = scaled_dot_product_attention(Q_, K_, V_, key_masks, causality, dropout_rate, training)\n\n        # Restore shape\n        outputs = tf.concat(tf.split(outputs, num_heads, axis=0), axis=2 ) # (N, T_q, d_model)\n              \n        # Residual connection\n        outputs += queries\n              \n        # Normalize\n        outputs = ln(outputs)\n \n    return outputs\n\ndef ff(inputs, num_units, scope=\"positionwise_feedforward\"):\n    '''position-wise feed forward net. See 3.3\n    \n    inputs: A 3d tensor with shape of [N, T, C].\n    num_units: A list of two integers.\n    scope: Optional scope for `variable_scope`.\n\n    Returns:\n      A 3d tensor with the same shape and dtype as inputs\n    '''\n    with tf.variable_scope(scope, reuse=tf.AUTO_REUSE):\n        # Inner layer\n        outputs = tf.layers.dense(inputs, num_units[0], activation=tf.nn.relu)\n\n        # Outer layer\n        outputs = tf.layers.dense(outputs, num_units[1])\n\n        # Residual connection\n        outputs += inputs\n        \n        # Normalize\n        outputs = ln(outputs)\n    \n    return outputs\n\ndef label_smoothing(inputs, epsilon=0.1):\n    '''Applies label smoothing. See 5.4 and https://arxiv.org/abs/1512.00567.\n    inputs: 3d tensor. [N, T, V], where V is the number of vocabulary.\n    epsilon: Smoothing rate.\n    \n    For example,\n    \n    ```\n    import tensorflow as tf\n    inputs = tf.convert_to_tensor([[[0, 0, 1], \n       [0, 1, 0],\n       [1, 0, 0]],\n\n      [[1, 0, 0],\n       [1, 0, 0],\n       [0, 1, 0]]], tf.float32)\n       \n    outputs = label_smoothing(inputs)\n    \n    with tf.Session() as sess:\n        print(sess.run([outputs]))\n    \n    >>\n    [array([[[ 0.03333334,  0.03333334,  0.93333334],\n        [ 0.03333334,  0.93333334,  0.03333334],\n        [ 0.93333334,  0.03333334,  0.03333334]],\n\n       [[ 0.93333334,  0.03333334,  0.03333334],\n        [ 0.93333334,  0.03333334,  0.03333334],\n        [ 0.03333334,  0.93333334,  0.03333334]]], dtype=float32)]   \n    ```    \n    '''\n    V = inputs.get_shape().as_list()[-1] # number of channels\n    return ((1-epsilon) * inputs) + (epsilon / V)\n    \ndef positional_encoding(inputs,\n                        maxlen,\n                        masking=True,\n                        scope=\"positional_encoding\"):\n    '''Sinusoidal Positional_Encoding. See 3.5\n    inputs: 3d tensor. (N, T, E)\n    maxlen: scalar. Must be >= T\n    masking: Boolean. If True, padding positions are set to zeros.\n    scope: Optional scope for `variable_scope`.\n\n    returns\n    3d tensor that has the same shape as inputs.\n    '''\n\n    E = inputs.get_shape().as_list()[-1] # static\n    N, T = tf.shape(inputs)[0], tf.shape(inputs)[1] # dynamic\n    with tf.variable_scope(scope, reuse=tf.AUTO_REUSE):\n        # position indices\n        position_ind = tf.tile(tf.expand_dims(tf.range(T), 0), [N, 1]) # (N, T)\n\n        # First part of the PE function: sin and cos argument\n        position_enc = np.array([\n            [pos / np.power(10000, (i-i%2)/E) for i in range(E)]\n            for pos in range(maxlen)])\n\n        # Second part, apply the cosine to even columns and sin to odds.\n        position_enc[:, 0::2] = np.sin(position_enc[:, 0::2])  # dim 2i\n        position_enc[:, 1::2] = np.cos(position_enc[:, 1::2])  # dim 2i+1\n        position_enc = tf.convert_to_tensor(position_enc, tf.float32) # (maxlen, E)\n\n        # lookup\n        outputs = tf.nn.embedding_lookup(position_enc, position_ind)\n\n        # masks\n        if masking:\n            outputs = tf.where(tf.equal(inputs, 0), inputs, outputs)\n\n        return tf.to_float(outputs)\n\ndef noam_scheme(init_lr, global_step, warmup_steps=4000.):\n    '''Noam scheme learning rate decay\n    init_lr: initial learning rate. scalar.\n    global_step: scalar.\n    warmup_steps: scalar. During warmup_steps, learning rate increases\n        until it reaches init_lr.\n    '''\n    step = tf.cast(global_step + 1, dtype=tf.float32)\n    return init_lr * warmup_steps ** 0.5 * tf.minimum(step * warmup_steps ** -1.5, step ** -0.5)"
        },
        {
          "name": "multi-bleu.perl",
          "type": "blob",
          "size": 4.9306640625,
          "content": "#!/usr/bin/perl -w\n\nuse strict;\n\nif (!scalar(@ARGV)) {\n  print STDERR \"Syntax: multi-bleu.perl [-length_analysis bucket] [ref-stem] < [system-output]\nIf one reference translation: ref-stem is filename\nIf multiple reference translations: ref-stem[0,1,2,...] is filename\\n\";\n}\n\nmy $length_analysis;\nif ($ARGV[0] eq '-length_analysis') {\n  shift @ARGV;\n  $length_analysis = shift @ARGV;\n}\n\nmy @CORRECT_BUCKET;\nmy @TOTAL_BUCKET;\nmy @COUNT_LENGTH;\nmy $max_bucket=0;\n\nmy $stem = $ARGV[0];\nmy @REF;\nmy $ref=0;\nwhile(-e \"$stem$ref\") {\n    &add_to_ref(\"$stem$ref\",\\@REF);\n    $ref++;\n}\n&add_to_ref($stem,\\@REF) if -e $stem;\ndie(\"did not find any reference translations at $stem\") unless scalar @REF;\n\nsub add_to_ref {\n    my ($file,$REF) = @_;\n    my $s=0;\n    open(REF,$file);\n    while(<REF>) {\n    chop;\n    push @{$$REF[$s++]}, $_;\n    }\n    close(REF);\n}\n\nmy(@CORRECT,@TOTAL,$length_translation,$length_reference);\nmy $s=0;\nwhile(<STDIN>) {\n    chop;\n    my @WORD = split;\n    my %REF_NGRAM = ();\n    my $length_translation_this_sentence = scalar(@WORD);\n    my ($closest_diff,$closest_length) = (9999,9999);\n    my $bucket;\n    foreach my $reference (@{$REF[$s]}) {\n#      print \"$s $_ <=> $reference\\n\";\n    my @WORD = split(/ /,$reference);\n    my $length = scalar(@WORD);\n    if ($length_analysis) {\n        $bucket = int($length/$length_analysis);\n        $max_bucket=$bucket if ($bucket>$max_bucket);\n    }\n    if (abs($length_translation_this_sentence-$length) < $closest_diff) {\n        $closest_diff = abs($length_translation_this_sentence-$length);\n        $closest_length = $length;\n#    print \"$i: closest diff = abs($length_translation_this_sentence-$length)<BR>\\n\";\n    }\n    for(my $n=1;$n<=4;$n++) {\n        my %REF_NGRAM_N = ();\n        for(my $start=0;$start<=$#WORD-($n-1);$start++) {\n        my $ngram = \"$n\";\n        for(my $w=0;$w<$n;$w++) {\n            $ngram .= \" \".$WORD[$start+$w];\n        }\n        $REF_NGRAM_N{$ngram}++;\n        }\n        foreach my $ngram (keys %REF_NGRAM_N) {\n        if (!defined($REF_NGRAM{$ngram}) ||\n            $REF_NGRAM{$ngram} < $REF_NGRAM_N{$ngram}) {\n            $REF_NGRAM{$ngram} = $REF_NGRAM_N{$ngram};\n#        print \"$i: REF_NGRAM{$ngram} = $REF_NGRAM{$ngram}<BR>\\n\";\n        }\n        }\n    }\n    }\n    if ($bucket) {\n        $COUNT_LENGTH[$bucket]++;\n    }\n    $length_translation += $length_translation_this_sentence;\n    $length_reference += $closest_length;\n    for(my $n=1;$n<=4;$n++) {\n    my %T_NGRAM = ();\n    for(my $start=0;$start<=$#WORD-($n-1);$start++) {\n        my $ngram = \"$n\";\n        for(my $w=0;$w<$n;$w++) {\n        $ngram .= \" \".$WORD[$start+$w];\n        }\n        $T_NGRAM{$ngram}++;\n    }\n    foreach my $ngram (keys %T_NGRAM) {\n        $ngram =~ /^(\\d+) /;\n        my $n = $1;\n#    print \"$i e $ngram $T_NGRAM{$ngram}<BR>\\n\";\n        $TOTAL[$n] += $T_NGRAM{$ngram};\n        if ($bucket) {\n            $TOTAL_BUCKET[$bucket][$n] += $T_NGRAM{$ngram};\n        }\n        if (defined($REF_NGRAM{$ngram})) {\n        if ($REF_NGRAM{$ngram} >= $T_NGRAM{$ngram}) {\n            if ($bucket) {\n                $CORRECT_BUCKET[$bucket][$n] += $T_NGRAM{$ngram};\n            }\n            $CORRECT[$n] += $T_NGRAM{$ngram};\n#        print \"$i e correct1 $T_NGRAM{$ngram}<BR>\\n\";\n        }\n        else {\n            if ($bucket) {\n                $CORRECT_BUCKET[$bucket][$n] += $REF_NGRAM{$ngram};\n            }\n            $CORRECT[$n] += $REF_NGRAM{$ngram};\n#        print \"$i e correct2 $REF_NGRAM{$ngram}<BR>\\n\";\n        }\n        }\n    }\n    }\n    $s++;\n}\nmy $brevity_penalty = 1;\nif ($length_translation<$length_reference) {\n    $brevity_penalty = exp(1-$length_reference/$length_translation);\n}\nmy $bleu = $brevity_penalty * exp((my_log( $CORRECT[1]/$TOTAL[1] ) +\n                   my_log( $CORRECT[2]/$TOTAL[2] ) +\n                   my_log( $CORRECT[3]/$TOTAL[3] ) +\n                   my_log( $CORRECT[4]/$TOTAL[4] ) ) / 4);\n\nprintf \"BLEU = %.2f, %.1f/%.1f/%.1f/%.1f (BP=%.3f, ration=%.3f)\\n\",\n    100*$bleu,\n    100*$CORRECT[1]/$TOTAL[1],\n    100*$CORRECT[2]/$TOTAL[2],\n    100*$CORRECT[3]/$TOTAL[3],\n    100*$CORRECT[4]/$TOTAL[4],\n    $brevity_penalty,\n    $length_translation / $length_reference;\n\nif ($length_analysis) {\n    print \"\\nLENGTH ANALYSIS:\\n\";\n    for(my $b=int(1/$length_analysis); $b<=$max_bucket; $b++) {\n        my $range=$b;\n        if ($length_analysis != 1) {\n            $range=($b*$length_analysis+1).\"-\".(($b+1)*$length_analysis);\n        }\n        print \"$range\";;\n        if ($TOTAL_BUCKET[$b] && $TOTAL_BUCKET[$b][4] && $CORRECT_BUCKET[$b][4]) {\n          printf \"\\t%d\\t%.2f\", $COUNT_LENGTH[$b],\n                   100*$brevity_penalty * exp((my_log( $CORRECT_BUCKET[$b][1]/$TOTAL_BUCKET[$b][1] ) +\n                   my_log( $CORRECT_BUCKET[$b][2]/$TOTAL_BUCKET[$b][2] ) +\n                   my_log( $CORRECT_BUCKET[$b][3]/$TOTAL_BUCKET[$b][3] ) +\n                   my_log( $CORRECT_BUCKET[$b][4]/$TOTAL_BUCKET[$b][4] ) ) / 4);\n        }\n        print \"\\n\";\n    }\n}\n\nsub my_log {\n  return -9999999999 unless $_[0];\n  return log($_[0]);\n}\n"
        },
        {
          "name": "prepro.py",
          "type": "blob",
          "size": 4.6171875,
          "content": "# -*- coding: utf-8 -*-\r\n#/usr/bin/python3\r\n'''\r\nFeb. 2019 by kyubyong park.\r\nkbpark.linguist@gmail.com.\r\nhttps://www.github.com/kyubyong/transformer.\r\n\r\nPreprocess the iwslt 2016 datasets.\r\n'''\r\n\r\nimport os\r\nimport errno\r\nimport sentencepiece as spm\r\nimport re\r\nfrom hparams import Hparams\r\nimport logging\r\n\r\nlogging.basicConfig(level=logging.INFO)\r\n\r\ndef prepro(hp):\r\n    \"\"\"Load raw data -> Preprocessing -> Segmenting with sentencepice\r\n    hp: hyperparams. argparse.\r\n    \"\"\"\r\n    logging.info(\"# Check if raw files exist\")\r\n    train1 = \"iwslt2016/de-en/train.tags.de-en.de\"\r\n    train2 = \"iwslt2016/de-en/train.tags.de-en.en\"\r\n    eval1 = \"iwslt2016/de-en/IWSLT16.TED.tst2013.de-en.de.xml\"\r\n    eval2 = \"iwslt2016/de-en/IWSLT16.TED.tst2013.de-en.en.xml\"\r\n    test1 = \"iwslt2016/de-en/IWSLT16.TED.tst2014.de-en.de.xml\"\r\n    test2 = \"iwslt2016/de-en/IWSLT16.TED.tst2014.de-en.en.xml\"\r\n    for f in (train1, train2, eval1, eval2, test1, test2):\r\n        if not os.path.isfile(f):\r\n            raise FileNotFoundError(errno.ENOENT, os.strerror(errno.ENOENT), f)\r\n\r\n    logging.info(\"# Preprocessing\")\r\n    # train\r\n    _prepro = lambda x:  [line.strip() for line in open(x, 'r').read().split(\"\\n\") \\\r\n                      if not line.startswith(\"<\")]\r\n    prepro_train1, prepro_train2 = _prepro(train1), _prepro(train2)\r\n    assert len(prepro_train1)==len(prepro_train2), \"Check if train source and target files match.\"\r\n\r\n    # eval\r\n    _prepro = lambda x: [re.sub(\"<[^>]+>\", \"\", line).strip() \\\r\n                     for line in open(x, 'r').read().split(\"\\n\") \\\r\n                     if line.startswith(\"<seg id\")]\r\n    prepro_eval1, prepro_eval2 = _prepro(eval1), _prepro(eval2)\r\n    assert len(prepro_eval1) == len(prepro_eval2), \"Check if eval source and target files match.\"\r\n\r\n    # test\r\n    prepro_test1, prepro_test2 = _prepro(test1), _prepro(test2)\r\n    assert len(prepro_test1) == len(prepro_test2), \"Check if test source and target files match.\"\r\n\r\n    logging.info(\"Let's see how preprocessed data look like\")\r\n    logging.info(\"prepro_train1:\", prepro_train1[0])\r\n    logging.info(\"prepro_train2:\", prepro_train2[0])\r\n    logging.info(\"prepro_eval1:\", prepro_eval1[0])\r\n    logging.info(\"prepro_eval2:\", prepro_eval2[0])\r\n    logging.info(\"prepro_test1:\", prepro_test1[0])\r\n    logging.info(\"prepro_test2:\", prepro_test2[0])\r\n\r\n    logging.info(\"# write preprocessed files to disk\")\r\n    os.makedirs(\"iwslt2016/prepro\", exist_ok=True)\r\n    def _write(sents, fname):\r\n        with open(fname, 'w') as fout:\r\n            fout.write(\"\\n\".join(sents))\r\n\r\n    _write(prepro_train1, \"iwslt2016/prepro/train.de\")\r\n    _write(prepro_train2, \"iwslt2016/prepro/train.en\")\r\n    _write(prepro_train1+prepro_train2, \"iwslt2016/prepro/train\")\r\n    _write(prepro_eval1, \"iwslt2016/prepro/eval.de\")\r\n    _write(prepro_eval2, \"iwslt2016/prepro/eval.en\")\r\n    _write(prepro_test1, \"iwslt2016/prepro/test.de\")\r\n    _write(prepro_test2, \"iwslt2016/prepro/test.en\")\r\n\r\n    logging.info(\"# Train a joint BPE model with sentencepiece\")\r\n    os.makedirs(\"iwslt2016/segmented\", exist_ok=True)\r\n    train = '--input=iwslt2016/prepro/train --pad_id=0 --unk_id=1 \\\r\n             --bos_id=2 --eos_id=3\\\r\n             --model_prefix=iwslt2016/segmented/bpe --vocab_size={} \\\r\n             --model_type=bpe'.format(hp.vocab_size)\r\n    spm.SentencePieceTrainer.Train(train)\r\n\r\n    logging.info(\"# Load trained bpe model\")\r\n    sp = spm.SentencePieceProcessor()\r\n    sp.Load(\"iwslt2016/segmented/bpe.model\")\r\n\r\n    logging.info(\"# Segment\")\r\n    def _segment_and_write(sents, fname):\r\n        with open(fname, \"w\") as fout:\r\n            for sent in sents:\r\n                pieces = sp.EncodeAsPieces(sent)\r\n                fout.write(\" \".join(pieces) + \"\\n\")\r\n\r\n    _segment_and_write(prepro_train1, \"iwslt2016/segmented/train.de.bpe\")\r\n    _segment_and_write(prepro_train2, \"iwslt2016/segmented/train.en.bpe\")\r\n    _segment_and_write(prepro_eval1, \"iwslt2016/segmented/eval.de.bpe\")\r\n    _segment_and_write(prepro_eval2, \"iwslt2016/segmented/eval.en.bpe\")\r\n    _segment_and_write(prepro_test1, \"iwslt2016/segmented/test.de.bpe\")\r\n\r\n    logging.info(\"Let's see how segmented data look like\")\r\n    print(\"train1:\", open(\"iwslt2016/segmented/train.de.bpe\",'r').readline())\r\n    print(\"train2:\", open(\"iwslt2016/segmented/train.en.bpe\", 'r').readline())\r\n    print(\"eval1:\", open(\"iwslt2016/segmented/eval.de.bpe\", 'r').readline())\r\n    print(\"eval2:\", open(\"iwslt2016/segmented/eval.en.bpe\", 'r').readline())\r\n    print(\"test1:\", open(\"iwslt2016/segmented/test.de.bpe\", 'r').readline())\r\n\r\nif __name__ == '__main__':\r\n    hparams = Hparams()\r\n    parser = hparams.parser\r\n    hp = parser.parse_args()\r\n    prepro(hp)\r\n    logging.info(\"Done\")"
        },
        {
          "name": "requirements.txt",
          "type": "blob",
          "size": 0.0654296875,
          "content": "tensorflow==1.12.0\nnumpy>=1.15.4\nsentencepiece==0.1.8\ntqdm>=4.28.1\n"
        },
        {
          "name": "test.py",
          "type": "blob",
          "size": 1.8525390625,
          "content": "# -*- coding: utf-8 -*-\n#/usr/bin/python2\n'''\nFeb. 2019 by kyubyong park.\nkbpark.linguist@gmail.com.\nhttps://www.github.com/kyubyong/transformer\n\nInference\n'''\n\nimport os\n\nimport tensorflow as tf\n\nfrom data_load import get_batch\nfrom model import Transformer\nfrom hparams import Hparams\nfrom utils import get_hypotheses, calc_bleu, postprocess, load_hparams\nimport logging\n\nlogging.basicConfig(level=logging.INFO)\n\nlogging.info(\"# hparams\")\nhparams = Hparams()\nparser = hparams.parser\nhp = parser.parse_args()\nload_hparams(hp, hp.ckpt)\n\nlogging.info(\"# Prepare test batches\")\ntest_batches, num_test_batches, num_test_samples  = get_batch(hp.test1, hp.test1,\n                                              100000, 100000,\n                                              hp.vocab, hp.test_batch_size,\n                                              shuffle=False)\niter = tf.data.Iterator.from_structure(test_batches.output_types, test_batches.output_shapes)\nxs, ys = iter.get_next()\n\ntest_init_op = iter.make_initializer(test_batches)\n\nlogging.info(\"# Load model\")\nm = Transformer(hp)\ny_hat, _ = m.eval(xs, ys)\n\nlogging.info(\"# Session\")\nwith tf.Session() as sess:\n    ckpt_ = tf.train.latest_checkpoint(hp.ckpt)\n    ckpt = hp.ckpt if ckpt_ is None else ckpt_ # None: ckpt is a file. otherwise dir.\n    saver = tf.train.Saver()\n\n    saver.restore(sess, ckpt)\n\n    sess.run(test_init_op)\n\n    logging.info(\"# get hypotheses\")\n    hypotheses = get_hypotheses(num_test_batches, num_test_samples, sess, y_hat, m.idx2token)\n\n    logging.info(\"# write results\")\n    model_output = ckpt.split(\"/\")[-1]\n    if not os.path.exists(hp.testdir): os.makedirs(hp.testdir)\n    translation = os.path.join(hp.testdir, model_output)\n    with open(translation, 'w') as fout:\n        fout.write(\"\\n\".join(hypotheses))\n\n    logging.info(\"# calc bleu score and append it to translation\")\n    calc_bleu(hp.test2, translation)\n\n"
        },
        {
          "name": "test",
          "type": "tree",
          "content": null
        },
        {
          "name": "tf1.2_legacy",
          "type": "tree",
          "content": null
        },
        {
          "name": "train.py",
          "type": "blob",
          "size": 3.6826171875,
          "content": "# -*- coding: utf-8 -*-\n#/usr/bin/python3\n'''\nFeb. 2019 by kyubyong park.\nkbpark.linguist@gmail.com.\nhttps://www.github.com/kyubyong/transformer\n'''\nimport tensorflow as tf\n\nfrom model import Transformer\nfrom tqdm import tqdm\nfrom data_load import get_batch\nfrom utils import save_hparams, save_variable_specs, get_hypotheses, calc_bleu\nimport os\nfrom hparams import Hparams\nimport math\nimport logging\n\nlogging.basicConfig(level=logging.INFO)\n\n\nlogging.info(\"# hparams\")\nhparams = Hparams()\nparser = hparams.parser\nhp = parser.parse_args()\nsave_hparams(hp, hp.logdir)\n\nlogging.info(\"# Prepare train/eval batches\")\ntrain_batches, num_train_batches, num_train_samples = get_batch(hp.train1, hp.train2,\n                                             hp.maxlen1, hp.maxlen2,\n                                             hp.vocab, hp.batch_size,\n                                             shuffle=True)\neval_batches, num_eval_batches, num_eval_samples = get_batch(hp.eval1, hp.eval2,\n                                             100000, 100000,\n                                             hp.vocab, hp.batch_size,\n                                             shuffle=False)\n\n# create a iterator of the correct shape and type\niter = tf.data.Iterator.from_structure(train_batches.output_types, train_batches.output_shapes)\nxs, ys = iter.get_next()\n\ntrain_init_op = iter.make_initializer(train_batches)\neval_init_op = iter.make_initializer(eval_batches)\n\nlogging.info(\"# Load model\")\nm = Transformer(hp)\nloss, train_op, global_step, train_summaries = m.train(xs, ys)\ny_hat, eval_summaries = m.eval(xs, ys)\n# y_hat = m.infer(xs, ys)\n\nlogging.info(\"# Session\")\nsaver = tf.train.Saver(max_to_keep=hp.num_epochs)\nwith tf.Session() as sess:\n    ckpt = tf.train.latest_checkpoint(hp.logdir)\n    if ckpt is None:\n        logging.info(\"Initializing from scratch\")\n        sess.run(tf.global_variables_initializer())\n        save_variable_specs(os.path.join(hp.logdir, \"specs\"))\n    else:\n        saver.restore(sess, ckpt)\n\n    summary_writer = tf.summary.FileWriter(hp.logdir, sess.graph)\n\n    sess.run(train_init_op)\n    total_steps = hp.num_epochs * num_train_batches\n    _gs = sess.run(global_step)\n    for i in tqdm(range(_gs, total_steps+1)):\n        _, _gs, _summary = sess.run([train_op, global_step, train_summaries])\n        epoch = math.ceil(_gs / num_train_batches)\n        summary_writer.add_summary(_summary, _gs)\n\n        if _gs and _gs % num_train_batches == 0:\n            logging.info(\"epoch {} is done\".format(epoch))\n            _loss = sess.run(loss) # train loss\n\n            logging.info(\"# test evaluation\")\n            _, _eval_summaries = sess.run([eval_init_op, eval_summaries])\n            summary_writer.add_summary(_eval_summaries, _gs)\n\n            logging.info(\"# get hypotheses\")\n            hypotheses = get_hypotheses(num_eval_batches, num_eval_samples, sess, y_hat, m.idx2token)\n\n            logging.info(\"# write results\")\n            model_output = \"iwslt2016_E%02dL%.2f\" % (epoch, _loss)\n            if not os.path.exists(hp.evaldir): os.makedirs(hp.evaldir)\n            translation = os.path.join(hp.evaldir, model_output)\n            with open(translation, 'w') as fout:\n                fout.write(\"\\n\".join(hypotheses))\n\n            logging.info(\"# calc bleu score and append it to translation\")\n            calc_bleu(hp.eval3, translation)\n\n            logging.info(\"# save models\")\n            ckpt_name = os.path.join(hp.logdir, model_output)\n            saver.save(sess, ckpt_name, global_step=_gs)\n            logging.info(\"after training of {} epochs, {} has been saved.\".format(epoch, ckpt_name))\n\n            logging.info(\"# fall back to train mode\")\n            sess.run(train_init_op)\n    summary_writer.close()\n\n\nlogging.info(\"Done\")\n"
        },
        {
          "name": "utils.py",
          "type": "blob",
          "size": 4.9111328125,
          "content": "# -*- coding: utf-8 -*-\n# /usr/bin/python3\n'''\nFeb. 2019 by kyubyong park.\nkbpark.linguist@gmail.com.\nhttps://www.github.com/kyubyong/transformer.\n\nUtility functions\n'''\n\nimport tensorflow as tf\n# from tensorflow.python import pywrap_tensorflow\n# import numpy as np\nimport json\nimport os, re\nimport logging\n\nlogging.basicConfig(level=logging.INFO)\n\ndef calc_num_batches(total_num, batch_size):\n    '''Calculates the number of batches.\n    total_num: total sample number\n    batch_size\n\n    Returns\n    number of batches, allowing for remainders.'''\n    return total_num // batch_size + int(total_num % batch_size != 0)\n\ndef convert_idx_to_token_tensor(inputs, idx2token):\n    '''Converts int32 tensor to string tensor.\n    inputs: 1d int32 tensor. indices.\n    idx2token: dictionary\n\n    Returns\n    1d string tensor.\n    '''\n    def my_func(inputs):\n        return \" \".join(idx2token[elem] for elem in inputs)\n\n    return tf.py_func(my_func, [inputs], tf.string)\n\n# # def pad(x, maxlen):\n# #     '''Pads x, list of sequences, and make it as a numpy array.\n# #     x: list of sequences. e.g., [[2, 3, 4], [5, 6, 7, 8, 9], ...]\n# #     maxlen: scalar\n# #\n# #     Returns\n# #     numpy int32 array of (len(x), maxlen)\n# #     '''\n# #     padded = []\n# #     for seq in x:\n# #         seq += [0] * (maxlen - len(seq))\n# #         padded.append(seq)\n# #\n# #     arry = np.array(padded, np.int32)\n# #     assert arry.shape == (len(x), maxlen), \"Failed to make an array\"\n#\n#     return arry\n\ndef postprocess(hypotheses, idx2token):\n    '''Processes translation outputs.\n    hypotheses: list of encoded predictions\n    idx2token: dictionary\n\n    Returns\n    processed hypotheses\n    '''\n    _hypotheses = []\n    for h in hypotheses:\n        sent = \"\".join(idx2token[idx] for idx in h)\n        sent = sent.split(\"</s>\")[0].strip()\n        sent = sent.replace(\"▁\", \" \") # remove bpe symbols\n        _hypotheses.append(sent.strip())\n    return _hypotheses\n\ndef save_hparams(hparams, path):\n    '''Saves hparams to path\n    hparams: argsparse object.\n    path: output directory.\n\n    Writes\n    hparams as literal dictionary to path.\n    '''\n    if not os.path.exists(path): os.makedirs(path)\n    hp = json.dumps(vars(hparams))\n    with open(os.path.join(path, \"hparams\"), 'w') as fout:\n        fout.write(hp)\n\ndef load_hparams(parser, path):\n    '''Loads hparams and overrides parser\n    parser: argsparse parser\n    path: directory or file where hparams are saved\n    '''\n    if not os.path.isdir(path):\n        path = os.path.dirname(path)\n    d = open(os.path.join(path, \"hparams\"), 'r').read()\n    flag2val = json.loads(d)\n    for f, v in flag2val.items():\n        parser.f = v\n\ndef save_variable_specs(fpath):\n    '''Saves information about variables such as\n    their name, shape, and total parameter number\n    fpath: string. output file path\n\n    Writes\n    a text file named fpath.\n    '''\n    def _get_size(shp):\n        '''Gets size of tensor shape\n        shp: TensorShape\n\n        Returns\n        size\n        '''\n        size = 1\n        for d in range(len(shp)):\n            size *=shp[d]\n        return size\n\n    params, num_params = [], 0\n    for v in tf.global_variables():\n        params.append(\"{}==={}\".format(v.name, v.shape))\n        num_params += _get_size(v.shape)\n    print(\"num_params: \", num_params)\n    with open(fpath, 'w') as fout:\n        fout.write(\"num_params: {}\\n\".format(num_params))\n        fout.write(\"\\n\".join(params))\n    logging.info(\"Variables info has been saved.\")\n\ndef get_hypotheses(num_batches, num_samples, sess, tensor, dict):\n    '''Gets hypotheses.\n    num_batches: scalar.\n    num_samples: scalar.\n    sess: tensorflow sess object\n    tensor: target tensor to fetch\n    dict: idx2token dictionary\n\n    Returns\n    hypotheses: list of sents\n    '''\n    hypotheses = []\n    for _ in range(num_batches):\n        h = sess.run(tensor)\n        hypotheses.extend(h.tolist())\n    hypotheses = postprocess(hypotheses, dict)\n\n    return hypotheses[:num_samples]\n\ndef calc_bleu(ref, translation):\n    '''Calculates bleu score and appends the report to translation\n    ref: reference file path\n    translation: model output file path\n\n    Returns\n    translation that the bleu score is appended to'''\n    get_bleu_score = \"perl multi-bleu.perl {} < {} > {}\".format(ref, translation, \"temp\")\n    os.system(get_bleu_score)\n    bleu_score_report = open(\"temp\", \"r\").read()\n    with open(translation, \"a\") as fout:\n        fout.write(\"\\n{}\".format(bleu_score_report))\n    try:\n        score = re.findall(\"BLEU = ([^,]+)\", bleu_score_report)[0]\n        new_translation = translation + \"B{}\".format(score)\n        os.system(\"mv {} {}\".format(translation, new_translation))\n        os.remove(translation)\n\n    except: pass\n    os.remove(\"temp\")\n\n\n# def get_inference_variables(ckpt, filter):\n#     reader = pywrap_tensorflow.NewCheckpointReader(ckpt)\n#     var_to_shape_map = reader.get_variable_to_shape_map()\n#     vars = [v for v in sorted(var_to_shape_map) if filter not in v]\n#     return vars\n\n\n\n\n"
        }
      ]
    }
  ]
}