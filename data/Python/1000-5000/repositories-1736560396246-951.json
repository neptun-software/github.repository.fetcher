{
  "metadata": {
    "timestamp": 1736560396246,
    "page": 951,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjk2MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "piskvorky/smart_open",
      "stars": 3250,
      "defaultBranch": "develop",
      "files": [
        {
          "name": ".flake8",
          "type": "blob",
          "size": 0.404296875,
          "content": "[flake8]\n# E121: Continuation line under-indented for hanging indent\n# E123: Continuation line missing indentation or outdented\n# E125:\tContinuation line with same indent as next logical line\n# E128: Continuation line under-indented for visual indent\n# E226: Missing whitespace around arithmetic operator\n# W503: Line break occurred before a binary operator\nignore=E121,E123,E125,E128,E226,W503\nmax-line-length=110"
        },
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.7373046875,
          "content": "# Byte-compiled / optimized / DLL files\n__pycache__/\n*.py[cod]\n\n# C extensions\n*.so\n\n# Distribution / packaging\n.Python\nenv/\nbuild/\ndevelop-eggs/\ndist/\ndownloads/\neggs/\nlib/\nlib64/\nparts/\nsdist/\nvar/\n*.egg-info/\n.installed.cfg\n*.egg\n\n# PyInstaller\n#  Usually these files are written by a python script from a template\n#  before PyInstaller builds the exe, so as to inject date/other infos into it.\n*.manifest\n*.spec\n\n# Installer logs\npip-log.txt\npip-delete-this-directory.txt\n\n# Unit test / coverage reports\nhtmlcov/\n.tox/\n.coverage\n.cache\nnosetests.xml\ncoverage.xml\n\n# Translations\n*.mo\n*.pot\n\n# Django stuff:\n*.log\n\n# Sphinx documentation\ndocs/_build/\n\n# PyBuilder\ntarget/\n\n# vim\n*.swp\n*.swo\n\n# PyCharm\n.idea/\n\n# VSCode\n.vscode/\n\n# env files\n.env\n.venv\n"
        },
        {
          "name": "CHANGELOG.md",
          "type": "blob",
          "size": 38.6162109375,
          "content": "# 7.1.0, 2024-12-17\n\n- Add support for python 3.13 (PR [#847](https://github.com/piskvorky/smart_open/pull/847), [@ddelange](https://github.com/ddelange))\n- Propagate uri to compression_wrapper (PR [#842](https://github.com/piskvorky/smart_open/pull/842), [@ddelange](https://github.com/ddelange))\n\n# 7.0.5, 2024-10-04\n\n- Fix zstd compression in ab mode (PR [#833](https://github.com/piskvorky/smart_open/pull/833), [@ddelange](https://github.com/ddelange))\n- Fix close function not neing able to upload a compressed S3 (PR [#838](https://github.com/piskvorky/smart_open/pull/838), [@jbarragan-bridge](https://github.com/jbarragan-bridge))\n- Fix test_http.request_callback (PR [#828](https://github.com/piskvorky/smart_open/pull/828), [@ddelange](https://github.com/ddelange))\n- Update readline logic for azure to match s3 (PR [#826](https://github.com/piskvorky/smart_open/pull/826), [@quantumfusion](https://github.com/quantumfusion))\n- Make http handler take an optional requests.Session (PR [#825](https://github.com/piskvorky/smart_open/pull/825), [@arondaniel](https://github.com/arondaniel))\n- Ensure no side effects on SinglepartWriter exception (PR [#820](https://github.com/piskvorky/smart_open/pull/820), [@donsokolone](https://github.com/donsokolone))\n- Add support for `get_blob_kwargs` to GCS blob read operations (PR [#817](https://github.com/piskvorky/smart_open/pull/817), [@thejcannon](https://github.com/thejcannon))\n\n# 7.0.4, 2024-03-26\n\n* Fix wb mode with zstd compression (PR [#815](https://github.com/piskvorky/smart_open/pull/815), [@djudd](https://github.com/djudd))\n* Remove GCS bucket.exists call to avoid storage.buckets.get permission (PR [#813](https://github.com/piskvorky/smart_open/pull/813), [@ddelange](https://github.com/ddelange))\n\n# 7.0.3, 2024-03-21\n\n* add support for zst writing (PR [#812](https://github.com/piskvorky/smart_open/pull/812), [@mpenkov](https://github.com/mpenkov))\n* roll back PR [#812](https://github.com/piskvorky/smart_open/pull/788), restore compatibility with built-in open function ([@mpenkov](https://github.com/mpenkov))\n\n# 7.0.2, 2024-03-21\n\n* Add `__next__` method to FileLikeProxy (PR [#811](https://github.com/piskvorky/smart_open/pull/811), [@ddelange](https://github.com/ddelange))\n* Fix python_requires minimum python version in setup.py (PR [#807](https://github.com/piskvorky/smart_open/pull/807), [@pressler-vsc](https://github.com/pressler-vsc))\n* Add activity check to cached sftp connections (PR [#808](https://github.com/piskvorky/smart_open/pull/808), [@greg-offerfit](https://github.com/greg-offerfit))\n\n# 7.0.1, 2024-02-26\n\n* Do not touch botocore unless it is installed (PR [#803](https://github.com/piskvorky/smart_open/pull/803), [@ddelange](https://github.com/ddelange))\n\n# 7.0.0, 2024-02-26\n\n* Upgrade dev status classifier to stable (PR [#798](https://github.com/piskvorky/smart_open/pull/798), [@seebi](https://github.com/seebi))\n* Add zstandard compression support (PR [#801](https://github.com/piskvorky/smart_open/pull/801), [@rlrs](https://github.com/rlrs))\n* Support moto 4 & 5 (PR [#802](https://github.com/piskvorky/smart_open/pull/802), [@jayvdb](https://github.com/jayvdb))\n* Add logic for handling large files in MultipartWriter uploads to S3 (PR [#796](https://github.com/piskvorky/smart_open/pull/796), [@jakkdl](https://github.com/jakkdl))\n* Add support for SSH connection via aliases from `~/.ssh/config` (PR [#790](https://github.com/piskvorky/smart_open/pull/790), [@wbeardall](https://github.com/wbeardall))\n* Secure the connection using SSL when connecting to the FTPS server (PR [#793](https://github.com/piskvorky/smart_open/pull/793), [@wammaster](https://github.com/wammaster))\n* Make GCS I/O 1000x faster by avoiding unnecessary API call (PR [#788](https://github.com/piskvorky/smart_open/pull/788), [@JohnHBrock](https://github.com/JohnHBrock))\n* Retry finalizing multipart S3 upload (PR [#785](https://github.com/piskvorky/smart_open/pull/785), [@ddelange](https://github.com/ddelange))\n* Handle exceptions during writes to Azure (PR [#783](https://github.com/piskvorky/smart_open/pull/783), [@ddelange](https://github.com/ddelange))\n* Fix formatting of python code in MIGRATING_FROM_OLDER_VERSIONS.rst (PR [#795](https://github.com/piskvorky/smart_open/pull/795), [@kenahoo](https://github.com/kenahoo))\n* Fix __str__ method in SinglepartWriter (PR [#791](https://github.com/piskvorky/smart_open/pull/791), [@ThosRTanner](https://github.com/ThosRTanner))\n* Fix `KeyError: 'ContentRange'` when received full content from S3 (PR [#789](https://github.com/piskvorky/smart_open/pull/789), [@messense](https://github.com/messense))\n* Propagate __exit__ call to the underlying filestream (PR [#786](https://github.com/piskvorky/smart_open/pull/786), [@ddelange](https://github.com/ddelange))\n\n## 6.4.0, 2023-09-07\n\n* Ignore S3 seeks to the current position (PR [#782](https://github.com/RaRe-Technologies/smart_open/pull/782), [@beck3905](https://github.com/beck3905))\n* Set binary mode prior to FTP write (PR [#781](https://github.com/RaRe-Technologies/smart_open/pull/781), [@beck3905](https://github.com/beck3905))\n* Improve S3 URI Parsing for URIs with \"@\", \"/\", and \":\" (PR [#776](https://github.com/RaRe-Technologies/smart_open/pull/776), [@rileypeterson](https://github.com/rileypeterson))\n* Add python 3.11 to setup.py (PR [#775](https://github.com/RaRe-Technologies/smart_open/pull/775), [@tooptoop4](https://github.com/tooptoop4))\n* Fix retrieving empty but existing object from S3 (PR [#771](https://github.com/RaRe-Technologies/smart_open/pull/771), [@Darkheir](https://github.com/Darkheir))\n* Avoid overfilling buffer when reading from Azure (PR [#767](https://github.com/RaRe-Technologies/smart_open/pull/767), [@ronreiter](https://github.com/ronreiter))\n* Add required import for example to work (PR [#756](https://github.com/RaRe-Technologies/smart_open/pull/756), [@jensenbox](https://github.com/jensenbox))\n\n## 6.3.0, 2022-12-12\n\n* Refactor Google Cloud Storage to use blob.open (__[ddelange](https://github.com/ddelange)__, [#744](https://github.com/RaRe-Technologies/smart_open/pull/744))\n* Add FTP/FTPS support (#33) (__[RachitSharma2001](https://github.com/RachitSharma2001)__, [#739](https://github.com/RaRe-Technologies/smart_open/pull/739))\n* Bring back compression_wrapper(filename) + use case-insensitive extension matching (__[piskvorky](https://github.com/piskvorky)__, [#737](https://github.com/RaRe-Technologies/smart_open/pull/737))\n* Fix avoidable S3 race condition (#693) (__[RachitSharma2001](https://github.com/RachitSharma2001)__, [#735](https://github.com/RaRe-Technologies/smart_open/pull/735))\n* setup.py: Remove pathlib2 (__[jayvdb](https://github.com/jayvdb)__, [#733](https://github.com/RaRe-Technologies/smart_open/pull/733))\n* Add flake8 config globally (__[cadnce](https://github.com/cadnce)__, [#732](https://github.com/RaRe-Technologies/smart_open/pull/732))\n* Added buffer_size parameter to http module (__[mullenkamp](https://github.com/mullenkamp)__, [#730](https://github.com/RaRe-Technologies/smart_open/pull/730))\n* Added documentation to support GCS anonymously (__[cadnce](https://github.com/cadnce)__, [#728](https://github.com/RaRe-Technologies/smart_open/pull/728))\n* Reconnect inactive sftp clients automatically (__[Kache](https://github.com/Kache)__, [#719](https://github.com/RaRe-Technologies/smart_open/pull/719))\n\n# 6.2.0, 14 September 2022\n\n- Fix quadratic time ByteBuffer operations (PR [#711](https://github.com/RaRe-Technologies/smart_open/pull/711), [@Joshua-Landau-Anthropic](https://github.com/Joshua-Landau-Anthropic))\n\n# 6.1.0, 21 August 2022\n\n- Add cert parameter to http transport params (PR [#703](https://github.com/RaRe-Technologies/smart_open/pull/703), [@stev-0](https://github.com/stev-0))\n- Allow passing additional kwargs for Azure writes (PR [#702](https://github.com/RaRe-Technologies/smart_open/pull/702), [@ddelange](https://github.com/ddelange))\n\n# 6.0.0, 24 April 2022\n\nThis release deprecates the old `ignore_ext` parameter.\nUse the `compression` parameter instead.\n\n```python\nfin = smart_open.open(\"/path/file.gz\", ignore_ext=True)  # No\nfin = smart_open.open(\"/path/file.gz\", compression=\"disable\")  # Yes\n\nfin = smart_open.open(\"/path/file.gz\", ignore_ext=False)  # No\nfin = smart_open.open(\"/path/file.gz\")  # Yes\nfin = smart_open.open(\"/path/file.gz\", compression=\"infer_from_extension\")  # Yes, if you want to be explicit\n\nfin = smart_open.open(\"/path/file\", compression=\".gz\")  # Yes\n```\n\n- Make Python 3.7 the required minimum (PR [#688](https://github.com/RaRe-Technologies/smart_open/pull/688), [@mpenkov](https://github.com/mpenkov))\n- Drop deprecated ignore_ext parameter (PR [#661](https://github.com/RaRe-Technologies/smart_open/pull/661), [@mpenkov](https://github.com/mpenkov)) \n- Drop support for passing buffers to smart_open.open (PR [#660](https://github.com/RaRe-Technologies/smart_open/pull/660), [@mpenkov](https://github.com/mpenkov))\n- Support working directly with file descriptors (PR [#659](https://github.com/RaRe-Technologies/smart_open/pull/659), [@mpenkov](https://github.com/mpenkov))\n- Added support for viewfs:// URLs (PR [#665](https://github.com/RaRe-Technologies/smart_open/pull/665), [@ChandanChainani](https://github.com/ChandanChainani))\n- Fix AttributeError when reading passthrough zstandard (PR [#658](https://github.com/RaRe-Technologies/smart_open/pull/658), [@mpenkov](https://github.com/mpenkov))\n- Make UploadFailedError picklable (PR [#689](https://github.com/RaRe-Technologies/smart_open/pull/689), [@birgerbr](https://github.com/birgerbr))\n- Support container client and blob client for azure blob storage (PR [#652](https://github.com/RaRe-Technologies/smart_open/pull/652), [@cbare](https://github.com/cbare))\n- Pin google-cloud-storage to >=1.31.1 in extras (PR [#687](https://github.com/RaRe-Technologies/smart_open/pull/687), [@PLPeeters](https://github.com/PLPeeters))\n- Expose certain transport-specific methods e.g. to_boto3 in top layer (PR [#664](https://github.com/RaRe-Technologies/smart_open/pull/664), [@mpenkov](https://github.com/mpenkov))\n- Use pytest instead of parameterizedtestcase (PR [#657](https://github.com/RaRe-Technologies/smart_open/pull/657), [@mpenkov](https://github.com/mpenkov))\n\n# 5.2.1, 28 August 2021\n\n- make HTTP/S seeking less strict (PR [#646](https://github.com/RaRe-Technologies/smart_open/pull/646), [@mpenkov](https://github.com/mpenkov))\n\n# 5.2.0, 18 August 2021\n\n- Work around changes to `urllib.parse.urlsplit` (PR [#633](https://github.com/RaRe-Technologies/smart_open/pull/633), [@judahrand](https://github.com/judahrand))\n- New blob_properties transport parameter for GCS (PR [#632](https://github.com/RaRe-Technologies/smart_open/pull/632), [@FHTheron](https://github.com/FHTheron))\n- Don't leak compressed stream (PR [#636](https://github.com/RaRe-Technologies/smart_open/pull/636), [@ampanasiuk](https://github.com/ampanasiuk))\n- Change python_requires version to fix PEP 440 issue (PR [#639](https://github.com/RaRe-Technologies/smart_open/pull/639), [@lucasvieirasilva](https://github.com/lucasvieirasilva))\n- New max_concurrency transport parameter for azure (PR [#642](https://github.com/RaRe-Technologies/smart_open/pull/642), [@omBratteng](https://github.com/omBratteng))\n\n# 5.1.0, 25 May 2021\n\nThis release introduces a new top-level parameter: `compression`.\nIt controls compression behavior and partially overlaps with the old `ignore_ext` parameter.\nFor details, see the README.rst file.\nYou may continue to use `ignore_ext` parameter for now, but it will be deprecated in the next major release.\n\n- Add warning for recently deprecated s3 parameters (PR [#618](https://github.com/RaRe-Technologies/smart_open/pull/618), [@mpenkov](https://github.com/mpenkov))\n- Add new top-level compression parameter (PR [#609](https://github.com/RaRe-Technologies/smart_open/pull/609), [@dmcguire81](https://github.com/dmcguire81))\n- Drop mock dependency; standardize on unittest.mock (PR [#621](https://github.com/RaRe-Technologies/smart_open/pull/621), [@musicinmybrain](https://github.com/musicinmybrain))\n- Fix to_boto3 method (PR [#619](https://github.com/RaRe-Technologies/smart_open/pull/619), [@mpenkov](https://github.com/mpenkov))\n\n# 5.0.0, 30 Mar 2021\n\nThis release modifies the handling of transport parameters for the S3 back-end in a backwards-incompatible way.\nSee [the migration docs](MIGRATING_FROM_OLDER_VERSIONS.rst) for details.\n\n- Refactor S3, replace high-level resource/session API with low-level client API (PR [#583](https://github.com/RaRe-Technologies/smart_open/pull/583), [@mpenkov](https://github.com/mpenkov))\n- Fix potential infinite loop when reading from webhdfs (PR [#597](https://github.com/RaRe-Technologies/smart_open/pull/597), [@traboukos](https://github.com/traboukos))\n- Add timeout parameter for http/https (PR [#594](https://github.com/RaRe-Technologies/smart_open/pull/594), [@dustymugs](https://github.com/dustymugs))\n- Remove `tests` directory from package (PR [#589](https://github.com/RaRe-Technologies/smart_open/pull/589), [@e-nalepa](https://github.com/e-nalepa))\n\n# 4.2.0, 15 Feb 2021\n\n- Support tell() for text mode write on s3/gcs/azure (PR [#582](https://github.com/RaRe-Technologies/smart_open/pull/582), [@markopy](https://github.com/markopy))\n- Implement option to use a custom buffer during S3 writes (PR [#547](https://github.com/RaRe-Technologies/smart_open/pull/547), [@mpenkov](https://github.com/mpenkov))\n\n# 4.1.2, 18 Jan 2021\n\n- Correctly pass boto3 resource to writers (PR [#576](https://github.com/RaRe-Technologies/smart_open/pull/576), [@jackluo923](https://github.com/jackluo923))\n- Improve robustness of S3 reading (PR [#552](https://github.com/RaRe-Technologies/smart_open/pull/552), [@mpenkov](https://github.com/mpenkov))\n- Replace codecs with TextIOWrapper to fix newline issues when reading text files (PR [#578](https://github.com/RaRe-Technologies/smart_open/pull/578), [@markopy](https://github.com/markopy))\n\n# 4.1.0, 30 Dec 2020\n\n- Refactor `s3` submodule to minimize resource usage (PR [#569](https://github.com/RaRe-Technologies/smart_open/pull/569), [@mpenkov](https://github.com/mpenkov))\n- Change `download_as_string` to `download_as_bytes` in `gcs` submodule (PR [#571](https://github.com/RaRe-Technologies/smart_open/pull/571), [@alexandreyc](https://github.com/alexandreyc))\n\n# 4.0.1, 27 Nov 2020\n\n- Exclude `requests` from `install_requires` dependency list.\n  If you need it, use `pip install smart_open[http]` or `pip install smart_open[webhdfs]`.\n\n# 4.0.0, 24 Nov 2020\n\n- Fix reading empty file or seeking past end of file for s3 backend (PR [#549](https://github.com/RaRe-Technologies/smart_open/pull/549), [@jcushman](https://github.com/jcushman))\n- Fix handling of rt/wt mode when working with gzip compression (PR [#559](https://github.com/RaRe-Technologies/smart_open/pull/559), [@mpenkov](https://github.com/mpenkov))\n- Bump minimum Python version to 3.6 (PR [#562](https://github.com/RaRe-Technologies/smart_open/pull/562), [@mpenkov](https://github.com/mpenkov))\n\n# 3.0.0, 8 Oct 2020\n\nThis release modifies the behavior of setup.py with respect to dependencies.\nPreviously, `boto3` and other AWS-related packages were installed by default.\nNow, in order to install them, you need to run either:\n\n    pip install smart_open[s3]\n\nto install the AWS dependencies only, or\n\n    pip install smart_open[all]\n\nto install all dependencies, including AWS, GCS, etc.\n\n# 2.2.1, 1 Oct 2020\n\n- Include S3 dependencies by default, because removing them in the 2.2.0 minor release was a mistake.\n\n# 2.2.0, 25 Sep 2020\n\nThis release modifies the behavior of setup.py with respect to dependencies.\nPreviously, `boto3` and other AWS-related packages were installed by default.\nNow, in order to install them, you need to run either:\n\n    pip install smart_open[s3]\n\nto install the AWS dependencies only, or\n\n    pip install smart_open[all]\n\nto install all dependencies, including AWS, GCS, etc.\n\nSummary of changes:\n\n- Correctly pass `newline` parameter to built-in `open` function (PR [#478](https://github.com/RaRe-Technologies/smart_open/pull/478), [@burkovae](https://github.com/burkovae))\n- Remove boto as a dependency (PR [#523](https://github.com/RaRe-Technologies/smart_open/pull/523), [@isobit](https://github.com/isobit))\n- Performance improvement: avoid redundant GetObject API queries in s3.Reader (PR [#495](https://github.com/RaRe-Technologies/smart_open/pull/495), [@jcushman](https://github.com/jcushman))\n- Support installing smart_open without AWS dependencies (PR [#534](https://github.com/RaRe-Technologies/smart_open/pull/534), [@justindujardin](https://github.com/justindujardin))\n- Take object version into account in `to_boto3` method (PR [#539](https://github.com/RaRe-Technologies/smart_open/pull/539), [@interpolatio](https://github.com/interpolatio))\n\n## Deprecations\n\nFunctionality on the left hand side will be removed in future releases.\nUse the functions on the right hand side instead.\n\n- `smart_open.s3_iter_bucket` → `smart_open.s3.iter_bucket`\n\n# 2.1.1, 27 Aug 2020\n\n  - Bypass unnecessary GCS storage.buckets.get permission (PR [#516](https://github.com/RaRe-Technologies/smart_open/pull/516), [@gelioz](https://github.com/gelioz))\n  - Allow SFTP connection with SSH key (PR [#522](https://github.com/RaRe-Technologies/smart_open/pull/522), [@rostskadat](https://github.com/rostskadat))\n\n# 2.1.0, 1 July 2020\n\n  - Azure storage blob support ([@nclsmitchell](https://github.com/nclsmitchell) and [@petedannemann](https://github.com/petedannemann))\n  - Correctly pass `newline` parameter to built-in `open` function (PR [#478](https://github.com/RaRe-Technologies/smart_open/pull/478), [@burkovae](https://github.com/burkovae))\n  - Ensure GCS objects always have a .name attribute (PR [#506](https://github.com/RaRe-Technologies/smart_open/pull/506), [@todor-markov](https://github.com/todor-markov))\n  - Use exception chaining to convey the original cause of the exception (PR [#508](https://github.com/RaRe-Technologies/smart_open/pull/508), [@cool-RR](https://github.com/cool-RR))\n\n# 2.0.0, 27 April 2020, \"Python 3\"\n\n  - **This version supports Python 3 only** (3.5+).\n    - If you still need Python 2, install the smart_open==1.10.1 legacy release instead.\n  - Prevent smart_open from writing to logs on import (PR [#476](https://github.com/RaRe-Technologies/smart_open/pull/476), [@mpenkov](https://github.com/mpenkov))\n  - Modify setup.py to explicitly support only Py3.5 and above (PR [#471](https://github.com/RaRe-Technologies/smart_open/pull/471), [@Amertz08](https://github.com/Amertz08))\n  - Include all the test_data in setup.py (PR [#473](https://github.com/RaRe-Technologies/smart_open/pull/473), [@sikuan](https://github.com/sikuan))\n\n# 1.10.1, 26 April 2020\n\n  - This is the last version to support Python 2.7. Versions 1.11 and above will support Python 3 only.\n  - Use only if you need Python 2.\n\n# 1.11.1, 8 Apr 2020\n\n  - Add missing boto dependency (Issue [#468](https://github.com/RaRe-Technologies/smart_open/issues/468))\n\n# 1.11.0, 8 Apr 2020\n\n  - Fix GCS multiple writes (PR [#421](https://github.com/RaRe-Technologies/smart_open/pull/421), [@petedannemann](https://github.com/petedannemann))\n  - Implemented efficient readline for ByteBuffer (PR [#426](https://github.com/RaRe-Technologies/smart_open/pull/426), [@mpenkov](https://github.com/mpenkov))\n  - Fix WebHDFS read method (PR [#433](https://github.com/RaRe-Technologies/smart_open/pull/433), [@mpenkov](https://github.com/mpenkov))\n  - Make S3 uploads more robust (PR [#434](https://github.com/RaRe-Technologies/smart_open/pull/434), [@mpenkov](https://github.com/mpenkov))\n  - Add pathlib monkeypatch with replacement of `pathlib.Path.open` (PR [#436](https://github.com/RaRe-Technologies/smart_open/pull/436), [@menshikh-iv](https://github.com/menshikh-iv))\n  - Fix error when calling str() or repr() on GCS SeekableBufferedInputBase (PR [#442](https://github.com/RaRe-Technologies/smart_open/pull/442), [@robcowie](https://github.com/robcowie))\n  - Move optional dependencies to extras (PR [#454](https://github.com/RaRe-Technologies/smart_open/pull/454), [@Amertz08](https://github.com/Amertz08))\n  - Correctly handle GCS paths that contain '?' char  (PR [#460](https://github.com/RaRe-Technologies/smart_open/pull/460), [@chakruperitus](https://github.com/chakruperitus))\n  - Make our doctools submodule more robust (PR [#467](https://github.com/RaRe-Technologies/smart_open/pull/467), [@mpenkov](https://github.com/mpenkov))\n\nStarting with this release, you will have to run:\n\n    pip install smart_open[gcs] to use the GCS transport.\n\nIn the future, all extra dependencies will be optional.  If you want to continue installing all of them, use:\n\n\tpip install smart_open[all]\n\nSee the README.rst for details.\n\n# 1.10.0, 16 Mar 2020\n\n  - Various webhdfs improvements (PR [#383](https://github.com/RaRe-Technologies/smart_open/pull/383), [@mrk-its](https://github.com/mrk-its))\n  - Fixes \"the connection was closed by the remote peer\" error (PR [#389](https://github.com/RaRe-Technologies/smart_open/pull/389), [@Gapex](https://github.com/Gapex))\n  - allow use of S3 single part uploads (PR [#400](https://github.com/RaRe-Technologies/smart_open/pull/400), [@adrpar](https://github.com/adrpar))\n  - Add test data in package via MANIFEST.in (PR [#401](https://github.com/RaRe-Technologies/smart_open/pull/401), [@jayvdb](https://github.com/jayvdb))\n  - Google Cloud Storage (GCS) (PR [#404](https://github.com/RaRe-Technologies/smart_open/pull/404), [@petedannemann](https://github.com/petedannemann))\n  - Implement to_boto3 function for S3 I/O. (PR [#405](https://github.com/RaRe-Technologies/smart_open/pull/405), [@mpenkov](https://github.com/mpenkov))\n  - enable smart_open to operate without docstrings (PR [#406](https://github.com/RaRe-Technologies/smart_open/pull/406), [@mpenkov](https://github.com/mpenkov))\n  - Implement object_kwargs parameter (PR [#411](https://github.com/RaRe-Technologies/smart_open/pull/411), [@mpenkov](https://github.com/mpenkov))\n  - Remove dependency on old boto library (PR [#413](https://github.com/RaRe-Technologies/smart_open/pull/413), [@mpenkov](https://github.com/mpenkov))\n  - implemented efficient readline for ByteBuffer (PR [#426](https://github.com/RaRe-Technologies/smart_open/pull/426), [@mpenkov](https://github.com/mpenkov))\n  - improve buffering efficiency (PR [#427](https://github.com/RaRe-Technologies/smart_open/pull/427), [@mpenkov](https://github.com/mpenkov))\n  - fix WebHDFS read method (PR [#433](https://github.com/RaRe-Technologies/smart_open/pull/433), [@mpenkov](https://github.com/mpenkov))\n  - Make S3 uploads more robust (PR [#434](https://github.com/RaRe-Technologies/smart_open/pull/434), [@mpenkov](https://github.com/mpenkov))\n\n# 1.9.0, 3 Nov 2019\n\n  - Add version_id transport parameter for fetching a specific S3 object version (PR [#325](https://github.com/RaRe-Technologies/smart_open/pull/325), [@interpolatio](https://github.com/interpolatio))\n  - Document passthrough use case (PR [#333](https://github.com/RaRe-Technologies/smart_open/pull/333), [@mpenkov](https://github.com/mpenkov))\n  - Support seeking over HTTP and HTTPS (PR [#339](https://github.com/RaRe-Technologies/smart_open/pull/339), [@interpolatio](https://github.com/interpolatio))\n  - Add support for rt, rt+, wt, wt+, at, at+ methods (PR [#342](https://github.com/RaRe-Technologies/smart_open/pull/342), [@interpolatio](https://github.com/interpolatio))\n  - Change VERSION to version.py (PR [#349](https://github.com/RaRe-Technologies/smart_open/pull/349), [@mpenkov](https://github.com/mpenkov))\n  - Adding howto guides (PR [#355](https://github.com/RaRe-Technologies/smart_open/pull/355), [@mpenkov](https://github.com/mpenkov))\n  - smart_open/s3: Initial implementations of str and repr (PR [#359](https://github.com/RaRe-Technologies/smart_open/pull/359), [@ZlatSic](https://github.com/ZlatSic))\n  - Support writing any bytes-like object to S3. (PR [#361](https://github.com/RaRe-Technologies/smart_open/pull/361), [@gilbsgilbs](https://github.com/gilbsgilbs))\n\n# 1.8.4, 2 Jun 2019\n\n  - Don't use s3 bucket_head to check for bucket existence (PR [#315](https://github.com/RaRe-Technologies/smart_open/pull/315), [@caboteria](https://github.com/caboteria))\n  - Dont list buckets in s3 tests (PR [#318](https://github.com/RaRe-Technologies/smart_open/pull/318), [@caboteria](https://github.com/caboteria))\n  - Use warnings.warn instead of logger.warning (PR [#321](https://github.com/RaRe-Technologies/smart_open/pull/321), [@mpenkov](https://github.com/mpenkov))\n  - Optimize reading from S3 (PR [#322](https://github.com/RaRe-Technologies/smart_open/pull/322), [@mpenkov](https://github.com/mpenkov))\n\n# 1.8.3, 26 April 2019\n\n  - Improve S3 read performance by not copying buffer (PR [#284](https://github.com/RaRe-Technologies/smart_open/pull/284), [@aperiodic](https://github.com/aperiodic))\n  - accept bytearray and memoryview as input to write in s3 submodule (PR [#293](https://github.com/RaRe-Technologies/smart_open/pull/293), [@bmizhen-exos](https://github.com/bmizhen-exos))\n  - Fix two S3 bugs (PR [#307](https://github.com/RaRe-Technologies/smart_open/pull/307), [@mpenkov](https://github.com/mpenkov))\n  - Minor fixes: bz2file dependency, paramiko warning handling (PR [#309](https://github.com/RaRe-Technologies/smart_open/pull/309), [@mpenkov](https://github.com/mpenkov))\n  - improve unit tests (PR [#310](https://github.com/RaRe-Technologies/smart_open/pull/310), [@mpenkov](https://github.com/mpenkov))\n\n# 1.8.2, 17 April 2019\n\n  - Removed dependency on lzma (PR [#262](https://github.com/RaRe-Technologies/smart_open/pull/282), [@tdhopper](https://github.com/tdhopper))\n  - backward compatibility fixes (PR [#294](https://github.com/RaRe-Technologies/smart_open/pull/294), [@mpenkov](https://github.com/mpenkov))\n  - Minor fixes (PR [#291](https://github.com/RaRe-Technologies/smart_open/pull/291), [@mpenkov](https://github.com/mpenkov))\n  - Fix #289: the smart_open package now correctly exposes a `__version__` attribute\n  - Fix #285: handle edge case with question marks in an S3 URL\n\nThis release rolls back support for transparently decompressing .xz files,\npreviously introduced in 1.8.1.  This is a useful feature, but it requires a\ntricky dependency.  It's still possible to handle .xz files with relatively\nlittle effort. Please see the\n[README.rst](https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#supported-compression-formats)\nfile for details.\n\n# 1.8.1, 6 April 2019\n\n  - Added support for .xz / lzma (PR [#262](https://github.com/RaRe-Technologies/smart_open/pull/262), [@vmarkovtsev](https://github.com/vmarkovtsev))\n  - Added streaming HTTP support (PR [#236](https://github.com/RaRe-Technologies/smart_open/pull/236), [@handsomezebra](https://github.com/handsomezebra))\n  - Fix handling of \"+\" mode, refactor tests (PR [#263](https://github.com/RaRe-Technologies/smart_open/pull/263), [@vmarkovtsev](https://github.com/vmarkovtsev))\n  - Added support for SSH/SCP/SFTP (PR [#58](https://github.com/RaRe-Technologies/smart_open/pull/58), [@val314159](https://github.com/val314159) & [@mpenkov](https://github.com/mpenkov))\n  - Added new feature: compressor registry (PR [#266](https://github.com/RaRe-Technologies/smart_open/pull/266), [@mpenkov](https://github.com/mpenkov))\n  - Implemented new `smart_open.open` function (PR [#268](https://github.com/RaRe-Technologies/smart_open/pull/268), [@mpenkov](https://github.com/mpenkov))\n\n## smart_open.open\n\nThis new function replaces `smart_open.smart_open`, which is now deprecated.\nMain differences:\n\n- ignore_extension → ignore_ext\n- new `transport_params` dict parameter to contain keyword parameters for the transport layer (S3, HTTPS, HDFS, etc).\n\nMain advantages of the new function:\n\n- Simpler interface for the user, less parameters\n- Greater API flexibility: adding additional keyword arguments will no longer require updating the top-level interface\n- Better documentation for keyword parameters (previously, they were documented via examples only)\n\nThe old `smart_open.smart_open` function is deprecated, but continues to work as previously.\n\n\n# 1.8.0, 17th January 2019\n\n  - Add `python3.7` support (PR [#240](https://github.com/RaRe-Technologies/smart_open/pull/240), [@menshikh-iv](https://github.com/menshikh-iv))\n  - Add `http/https` schema correctly (PR [#242](https://github.com/RaRe-Technologies/smart_open/pull/242), [@gliv](https://github.com/gliv))\n  - Fix url parsing for `S3` (PR [#235](https://github.com/RaRe-Technologies/smart_open/pull/235), [@rileypeterson](https://github.com/rileypeterson))\n  - Clean up `_parse_uri_s3x`, resolve edge cases (PR [#237](https://github.com/RaRe-Technologies/smart_open/pull/237), [@mpenkov](https://github.com/mpenkov))\n  - Handle leading slash in local path edge case (PR [#238](https://github.com/RaRe-Technologies/smart_open/pull/238), [@mpenkov](https://github.com/mpenkov))\n  - Roll back README changes (PR [#239](https://github.com/RaRe-Technologies/smart_open/pull/239), [@mpenkov](https://github.com/mpenkov))\n  - Add example how to work with Digital Ocean spaces and boto profile (PR [#248](https://github.com/RaRe-Technologies/smart_open/pull/248), [@navado](https://github.com/@navado) & [@mpenkov](https://github.com/mpenkov))\n  - Fix boto fail to load gce plugin (PR [#255](https://github.com/RaRe-Technologies/smart_open/pull/255), [@menshikh-iv](https://github.com/menshikh-iv))\n  - Drop deprecated `sudo` from travis config (PR [#256](https://github.com/RaRe-Technologies/smart_open/pull/256), [@cclauss](https://github.com/cclauss))\n  - Raise `ValueError` if s3 key does not exist (PR [#245](https://github.com/RaRe-Technologies/smart_open/pull/245), [@adrpar](https://github.com/adrpar))\n  - Ensure `_list_bucket` uses continuation token for subsequent pages (PR [#246](https://github.com/RaRe-Technologies/smart_open/pull/246), [@tcsavage](https://github.com/tcsavage))\n\n# 1.7.1, 18th September 2018\n\n  - Unpin boto/botocore for regular installation. Fix #227 (PR [#232](https://github.com/RaRe-Technologies/smart_open/pull/232), [@menshikh-iv](https://github.com/menshikh-iv))\n\n# 1.7.0, 18th September 2018\n\n  - Drop support for `python3.3` and `python3.4` & workaround for broken `moto` (PR [#225](https://github.com/RaRe-Technologies/smart_open/pull/225), [@menshikh-iv](https://github.com/menshikh-iv))\n  - Add `s3a://` support for `S3`. Fix #210 (PR [#229](https://github.com/RaRe-Technologies/smart_open/pull/229), [@mpenkov](https://github.com/mpenkov))\n  - Allow use `@` in object (key) names for `S3`. Fix #94 (PRs [#204](https://github.com/RaRe-Technologies/smart_open/pull/204) & [#224](https://github.com/RaRe-Technologies/smart_open/pull/224), [@dkasyanov](https://github.com/dkasyanov) & [@mpenkov](https://github.com/mpenkov))\n  - Make `close` idempotent & add dummy `flush` for `S3` (PR [#212](https://github.com/RaRe-Technologies/smart_open/pull/212), [@mpenkov](https://github.com/mpenkov))\n  - Use built-in `open` whenever possible. Fix #207 (PR [#208](https://github.com/RaRe-Technologies/smart_open/pull/208), [@mpenkov](https://github.com/mpenkov))\n  - Fix undefined name `uri` in `smart_open_lib.py`. Fix #213 (PR [#214](https://github.com/RaRe-Technologies/smart_open/pull/214), [@cclauss](https://github.com/cclauss))\n  - Fix new unittests from [#212](https://github.com/RaRe-Technologies/smart_open/pull/212) (PR [#219](https://github.com/RaRe-Technologies/smart_open/pull/219), [@mpenkov](https://github.com/mpenkov))\n  - Reorganize README & make examples py2/py3 compatible (PR [#211](https://github.com/RaRe-Technologies/smart_open/pull/211), [@piskvorky](https://github.com/piskvorky))\n\n# 1.6.0, 29th June 2018\n\n  - Migrate to `boto3`. Fix #43 (PR [#164](https://github.com/RaRe-Technologies/smart_open/pull/164), [@mpenkov](https://github.com/mpenkov))\n  - Refactoring smart_open to share compression and encoding functionality (PR [#185](https://github.com/RaRe-Technologies/smart_open/pull/185), [@mpenkov](https://github.com/mpenkov))\n  - Drop `python2.6` compatibility. Fix #156 (PR [#192](https://github.com/RaRe-Technologies/smart_open/pull/192), [@mpenkov](https://github.com/mpenkov))\n  - Accept a custom `boto3.Session` instance (support STS AssumeRole). Fix #130, #149, #199 (PR [#201](https://github.com/RaRe-Technologies/smart_open/pull/201), [@eschwartz](https://github.com/eschwartz))\n  - Accept `multipart_upload` parameters (supports ServerSideEncryption) for `S3`. Fix (PR [#202](https://github.com/RaRe-Technologies/smart_open/pull/202), [@eschwartz](https://github.com/eschwartz))\n  - Add support for `pathlib.Path`. Fix #170 (PR [#175](https://github.com/RaRe-Technologies/smart_open/pull/175), [@clintval](https://github.com/clintval))\n  - Fix performance regression using local file-system. Fix #184 (PR [#190](https://github.com/RaRe-Technologies/smart_open/pull/190), [@mpenkov](https://github.com/mpenkov))\n  - Replace `ParsedUri` class with functions, cleanup internal argument parsing (PR [#191](https://github.com/RaRe-Technologies/smart_open/pull/191), [@mpenkov](https://github.com/mpenkov))\n  - Handle edge case (read 0 bytes) in read function. Fix #171 (PR [#193](https://github.com/RaRe-Technologies/smart_open/pull/193), [@mpenkov](https://github.com/mpenkov))\n  - Fix bug with changing `f._current_pos` when call `f.readline()` (PR [#182](https://github.com/RaRe-Technologies/smart_open/pull/182), [@inksink](https://github.com/inksink))\n  - Сlose the old body explicitly after `seek` for `S3`. Fix #187 (PR [#188](https://github.com/RaRe-Technologies/smart_open/pull/188), [@inksink](https://github.com/inksink))\n\n# 1.5.7, 18th March 2018\n\n  - Fix author/maintainer fields in `setup.py`, avoid bug from `setuptools==39.0.0` and add workaround for `botocore` and `python==3.3`. Fix #176 (PR [#178](https://github.com/RaRe-Technologies/smart_open/pull/178) & [#177](https://github.com/RaRe-Technologies/smart_open/pull/177), [@menshikh-iv](https://github.com/menshikh-iv) & [@baldwindc](https://github.com/baldwindc))\n\n# 1.5.6, 28th December 2017\n\n  - Improve S3 read performance. Fix #152 (PR [#157](https://github.com/RaRe-Technologies/smart_open/pull/157), [@mpenkov](https://github.com/mpenkov))\n  - Add integration testing + benchmark with real S3. Partial fix #151, #156 (PR [#158](https://github.com/RaRe-Technologies/smart_open/pull/158), [@menshikh-iv](https://github.com/menshikh-iv) & [@mpenkov](https://github.com/mpenkov))\n  - Disable integration testing if secure vars isn't defined (PR [#157](https://github.com/RaRe-Technologies/smart_open/pull/158), [@menshikh-iv](https://github.com/menshikh-iv))\n\n# 1.5.5, 6th December 2017\n\n  - Fix problems from 1.5.4 release. Fix #153, #154 , partial fix #152 (PR [#155](https://github.com/RaRe-Technologies/smart_open/pull/155), [@mpenkov](https://github.com/mpenkov))\n\n# 1.5.4, 30th November 2017\n\n  - Add naitive .gz support for HDFS (PR [#128](https://github.com/RaRe-Technologies/smart_open/pull/128), [@yupbank](https://github.com/yupbank))\n  - Drop python2.6 support + fix style (PR [#137](https://github.com/RaRe-Technologies/smart_open/pull/137), [@menshikh-iv](https://github.com/menshikh-iv))\n  - Create separate compression-specific layer. Fix [#91](https://github.com/RaRe-Technologies/smart_open/issues/91) (PR [#131](https://github.com/RaRe-Technologies/smart_open/pull/131), [@mpenkov](https://github.com/mpenkov))\n  - Fix ResourceWarnings + replace deprecated assertEquals (PR [#140](https://github.com/RaRe-Technologies/smart_open/pull/140), [@horpto](https://github.com/horpto))\n  - Add encoding parameter to smart_open. Fix [#142](https://github.com/RaRe-Technologies/smart_open/issues/142) (PR [#143](https://github.com/RaRe-Technologies/smart_open/pull/143), [@mpenkov](https://github.com/mpenkov))\n  - Add encoding tests for readers. Fix [#145](https://github.com/RaRe-Technologies/smart_open/issues/145), partial fix [#146](https://github.com/RaRe-Technologies/smart_open/issues/146) (PR [#147](https://github.com/RaRe-Technologies/smart_open/pull/147), [@mpenkov](https://github.com/mpenkov))\n  - Fix file mode for updating case (PR [#150](https://github.com/RaRe-Technologies/smart_open/pull/150), [@menshikh-iv](https://github.com/menshikh-iv))\n\n# 1.5.3, 18th May 2017\n\n  - Remove GET parameters from url. Fix #120 (PR #121, @mcrowson)\n\n# 1.5.2, 12th Apr 2017\n\n  - Enable compressed formats over http. Avoid filehandle leak. Fix #109 and #110. (PR #112, @robottwo )\n  - Make possible to change number of retries (PR #102, @shaform)\t\n\n# 1.5.1, 16th Mar 2017\n\n  - Bugfix for compressed formats (PR #110, @tmylk)\n\n# 1.5.0, 14th Mar 2017\n\n  - HTTP/HTTPS read support w/ Kerberos (PR #107, @robottwo)\n\n# 1.4.0, 13th Feb 2017\n\n  - HdfsOpenWrite implementation similar to read (PR #106, @skibaa)  \n  - Support custom S3 server host, port, ssl. (PR #101, @robottwo)\n  - Add retry around `s3_iter_bucket_process_key` to address S3 Read Timeout errors. (PR #96, @bbbco)  \n  - Include tests data in sdist + install them. (PR #105, @cournape)\n  \n# 1.3.5, 5th October 2016\n\n# - Add MANIFEST.in required for conda-forge recip (PR #90, @tmylk)\n  - Fix #92. Allow hash in filename (PR #93, @tmylk)\n\n# 1.3.4, 26th August 2016\n\n  - Relative path support (PR #73, @yupbank)\n  - Move gzipstream module to smart_open package (PR #81, @mpenkov)\n  - Ensure reader objects never return None (PR #81, @mpenkov)\n  - Ensure read functions never return more bytes than asked for (PR #84, @mpenkov)\n  - Add support for reading gzipped objects until EOF, e.g. read() (PR #81, @mpenkov)\n  - Add missing parameter to read_from_buffer call (PR #84, @mpenkov)\n  - Add unit tests for gzipstream (PR #84, @mpenkov)\n  - Bundle gzipstream to enable streaming of gzipped content from S3 (PR #73, @mpenkov)\n  - Update gzipstream to avoid deep recursion (PR #73, @mpenkov)\n  - Implemented readline for S3 (PR #73, @mpenkov)\n  - Added pip requirements.txt (PR #73, @mpenkov)\n  - Invert NO_MULTIPROCESSING flag (PR #79, @Janrain-Colin)\n  - Add ability to add query to webhdfs uri. (PR #78, @ellimilial)\n\n# 1.3.3, 16th May 2016\n\n  - Accept an instance of boto.s3.key.Key to smart_open (PR #38, @asieira)\n  - Allow passing `encrypt_key` and other parameters to `initiate_multipart_upload` (PR #63, @asieira)\n  - Allow passing boto `host` and `profile_name` to smart_open (PR #71 #68, @robcowie)\n  - Write an empty key to S3 even if nothing is written to S3OpenWrite (PR #61, @petedmarsh)\n  - Support `LC_ALL=C` environment variable setup (PR #40, @nikicc)\n  - Python 3.5 support\n\n# 1.3.2, 3rd January 2016\n\n  - Bug fix release to enable 'wb+' file mode (PR #50)\n\n\n# 1.3.1, 18th December 2015\n\n  - Disable multiprocessing if unavailable. Allows to run on Google Compute Engine. (PR #41, @nikicc)\n  - Httpretty updated to allow LC_ALL=C locale config. (PR #39, @jsphpl)\n  - Accept an instance of boto.s3.key.Key (PR #38, @asieira)\n\n\n# 1.3.0, 19th September 2015\n\n  - WebHDFS read/write (PR #29, @ziky90)\n  - re-upload last S3 chunk in failed upload (PR #20, @andreycizov)\n  - return the entire key in s3_iter_bucket instead of only the key name (PR #22, @salilb)\n  - pass optional keywords on S3 write (PR #30, @val314159)\n  - smart_open a no-op if passed a file-like object with a read attribute (PR #32, @gojomo)\n  - various improvements to testing (PR #30, @val314159)\n\n\n# 1.1.0, 1st February 2015\n\n  - support for multistream bzip files (PR #9, @pombredanne)\n  - introduce this CHANGELOG\n\n"
        },
        {
          "name": "CONTRIBUTING.md",
          "type": "blob",
          "size": 0.3955078125,
          "content": "# Quickstart\n\nClone the repo and use a python installation to create a venv:\n\n```sh\ngit clone git@github.com:RaRe-Technologies/smart_open.git\ncd smart_open\npython -m venv .venv\n```\n\nActivate the venv to start working and install test deps:\n\n```sh\n.venv/bin/activate\npip install -e \".[test]\"\n```\n\nTests should pass:\n\n```sh\npytest\n```\n\nThats it! When you're done, deactivate the venv:\n\n```sh\ndeactivate\n```\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 1.05859375,
          "content": "The MIT License (MIT)\n\nCopyright (c) 2015 Radim Řehůřek\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n\n"
        },
        {
          "name": "MANIFEST.in",
          "type": "blob",
          "size": 0.095703125,
          "content": "include LICENSE\ninclude README.rst\ninclude MIGRATING_FROM_OLDER_VERSIONS.rst\ninclude CHANGELOG.md\n"
        },
        {
          "name": "MIGRATING_FROM_OLDER_VERSIONS.rst",
          "type": "blob",
          "size": 10.0302734375,
          "content": "Migrating to the new compression parameter\n==========================================\n\nsmart_open versions 6.0.0 and above no longer support the ``ignore_ext`` parameter.\nUse the ``compression`` parameter instead:\n\n.. code-block:: python\n\n    fin = smart_open.open(\"/path/file.gz\", ignore_ext=True)  # No\n    fin = smart_open.open(\"/path/file.gz\", compression=\"disable\")  # Yes\n    \n    fin = smart_open.open(\"/path/file.gz\", ignore_ext=False)  # No\n    fin = smart_open.open(\"/path/file.gz\")  # Yes\n    fin = smart_open.open(\"/path/file.gz\", compression=\"infer_from_extension\")  # Yes, if you want to be explicit\n    \n    fin = smart_open.open(\"/path/file\", compression=\".gz\")  # Yes\n\n\nMigrating to the new client-based S3 API\n========================================\n\nVersion of smart_open prior to 5.0.0 used the boto3 `resource API`_ for communicating with S3.\nThis API was easy to integrate for smart_open developers, but this came at a cost: it was not thread- or multiprocess-safe.\nFurthermore, as smart_open supported more and more options, the transport parameter list grew, making it less maintainable.\n\nStarting with version 5.0.0, smart_open uses the `client API`_ instead of the resource API.\nFunctionally, very little changes for the smart_open user. \nThe only difference is in passing transport parameters to the S3 backend.\n\nMore specifically, the following S3 transport parameters are no longer supported:\n\n- `multipart_upload_kwargs`\n- `object_kwargs`\n- `resource`\n- `resource_kwargs`\n- `session`\n- `singlepart_upload_kwargs`\n\n**If you weren't using the above parameters, nothing changes for you.**\n\nHowever, if you were using any of the above, then you need to adjust your code.\nHere are some quick recipes below.\n\nIf you were previously passing `session`, then construct an S3 client from the session and pass that instead.\nFor example, before:\n\n.. code-block:: python\n\n    smart_open.open('s3://bucket/key', transport_params={'session': session})\n\nAfter:\n\n.. code-block:: python\n\n    smart_open.open('s3://bucket/key', transport_params={'client': session.client('s3')})\n\nIf you were passing `resource`, then replace the resource with a client, and pass that instead.\nFor example, before:\n\n.. code-block:: python\n\n    resource = session.resource('s3', **resource_kwargs)\n    smart_open.open('s3://bucket/key', transport_params={'resource': resource})\n\nAfter:\n\n.. code-block:: python\n\n    client = session.client('s3')\n    smart_open.open('s3://bucket/key', transport_params={'client': client})\n\nIf you were passing any of the `*_kwargs` parameters, you will need to include them in `client_kwargs`, keeping in mind the following transformations.\n\n========================== ====================================== ==========================\nParameter name             Resource API method                    Client API function\n========================== ====================================== ==========================\n`multipart_upload_kwargs`  `S3.Object.initiate_multipart_upload`_ `S3.Client.create_multipart_upload`_\n`object_kwargs`            `S3.Object.get`_                       `S3.Client.get_object`_\n`resource_kwargs`          S3.resource                            `S3.client`_\n`singlepart_upload_kwargs` `S3.Object.put`_                       `S3.Client.put_object`_\n========================== ====================================== ==========================\n\nMost of the above is self-explanatory, with the exception of `resource_kwargs`.\nThese were previously used mostly for passing a custom endpoint URL.\n\nThe `client_kwargs` dict can thus contain the following members:\n\n- `S3.Client`: initializer parameters, e.g. those to pass directly to the `boto3.client` function, such as `endpoint_url`.\n- `S3.Client.create_multipart_upload`\n- `S3.Client.get_object`\n- `S3.Client.put_object`\n\nHere's a before-and-after example for connecting to a custom endpoint.  Before:\n\n.. code-block:: python\n\n    session = boto3.Session(profile_name='digitalocean')\n    resource_kwargs = {'endpoint_url': 'https://ams3.digitaloceanspaces.com'}\n    with open('s3://bucket/key.txt', 'wb', transport_params={'resource_kwarg': resource_kwargs}) as fout:\n        fout.write(b'here we stand')\n\nAfter:\n\n.. code-block:: python\n\n    session = boto3.Session(profile_name='digitalocean')\n    client = session.client('s3', endpoint_url='https://ams3.digitaloceanspaces.com')\n    with open('s3://bucket/key.txt', 'wb', transport_params={'client': client}) as fout:\n        fout.write(b'here we stand')\n\nSee `README <README.rst>`_ and `HOWTO <howto.md>`_ for more examples.\n\n.. _resource API: https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/s3.html#service-resource\n.. _S3.Object.initiate_multipart_upload: https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/s3.html#S3.Object.initiate_multipart_upload\n.. _S3.Object.get: https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/s3.html#S3.ObjectSummary.get\n.. _S3.Object.put: https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/s3.html#S3.ObjectSummary.put\n\n.. _client API: https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/s3.html#client\n.. _S3.Client: https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/s3.html#client\n.. _S3.Client.create_multipart_upload: https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/s3.html#S3.Client.create_multipart_upload\n.. _S3.Client.get_object: https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/s3.html#S3.Client.get_object\n.. _S3.Client.put_object: https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/s3.html#S3.Client.put_object\n\nMigrating to the new dependency management subsystem\n====================================================\n\nSmart_open has grown over the years to cover a lot of different storages, each with a different set of library dependencies. Not everybody needs *all* of them, so to make each smart_open installation leaner and faster, version 3.0.0 introduced a new, backward-incompatible installation method:\n\n* smart_open < 3.0.0: All dependencies were installed by default. No way to select just a subset during installation.\n* smart_open >= 3.0.0: No dependencies installed by default. Install the ones you need with e.g. ``pip install smart_open[s3]`` (only AWS), or ``smart_open[all]`` (install everything = same behaviour as < 3.0.0; use this for backward compatibility). \n\nYou can read more about the motivation and internal discussions for this change  `here <https://github.com/RaRe-Technologies/smart_open/issues/443>`_.\n\nMigrating to the new ``open`` function\n======================================\n\nSince 1.8.1, there is a ``smart_open.open`` function that replaces ``smart_open.smart_open``.\nThe new function offers several advantages over the old one:\n\n- 100% compatible with the built-in ``open`` function (aka ``io.open``): it accepts all\n  the parameters that the built-in ``open`` accepts.\n- The default open mode is now \"r\", the same as for the built-in ``open``.\n  The default for the old ``smart_open.smart_open`` function used to be \"rb\".\n- Fully documented keyword parameters (try ``help(\"smart_open.open\")``)\n\nThe instructions below will help you migrate to the new function painlessly.\n\nFirst, update your imports:\n\n.. code-block:: python\n\n  >>> from smart_open import smart_open  # before\n  >>> from smart_open import open  # after\n\nIn general, ``smart_open`` uses ``io.open`` directly, where possible, so if your\ncode already uses ``open`` for local file I/O, then it will continue to work.\nIf you want to continue using the built-in ``open`` function for e.g. debugging,\nthen you can ``import smart_open`` and use ``smart_open.open``.\n\n**The default read mode is now \"r\" (read text).**\nIf your code was implicitly relying on the default mode being \"rb\" (read\nbinary), you'll need to update it and pass \"rb\" explicitly.\n\nBefore:\n\n.. code-block:: python\n\n  >>> import smart_open\n  >>> smart_open.smart_open('s3://commoncrawl/robots.txt').read(32)  # 'rb' used to be the default\n  b'User-Agent: *\\nDisallow: /'\n\nAfter:\n\n.. code-block:: python\n\n  >>> import smart_open\n  >>> smart_open.open('s3://commoncrawl/robots.txt', 'rb').read(32)\n  b'User-Agent: *\\nDisallow: /'\n\nThe ``ignore_extension`` keyword parameter is now called ``ignore_ext``.\nIt behaves identically otherwise.\n\nThe most significant change is in the handling on keyword parameters for the\ntransport layer, e.g. HTTP, S3, etc. The old function accepted these directly:\n\n.. code-block:: python\n\n  >>> url = 's3://smart-open-py37-benchmark-results/test.txt'\n  >>> session = boto3.Session(profile_name='smart_open')\n  >>> smart_open.smart_open(url, 'r', session=session).read(32)\n  'first line\\nsecond line\\nthird lin'\n\nThe new function accepts a ``transport_params`` keyword argument.  It's a dict.\nPut your transport parameters in that dictionary.\n\n.. code-block:: python\n\n  >>> url = 's3://smart-open-py37-benchmark-results/test.txt'\n  >>> params = {'session': boto3.Session(profile_name='smart_open')}\n  >>> open(url, 'r', transport_params=params).read(32)\n  'first line\\nsecond line\\nthird lin'\n\nRenamed parameters:\n\n- ``s3_upload`` ->  ``multipart_upload_kwargs``\n- ``s3_session`` -> ``session``\n\nRemoved parameters:\n\n- ``profile_name``\n\n**The profile_name parameter has been removed.**\nPass an entire ``boto3.Session`` object instead.\n\nBefore:\n\n.. code-block:: python\n\n  >>> url = 's3://smart-open-py37-benchmark-results/test.txt'\n  >>> smart_open.smart_open(url, 'r', profile_name='smart_open').read(32)\n  'first line\\nsecond line\\nthird lin'\n\nAfter:\n\n.. code-block:: python\n\n  >>> url = 's3://smart-open-py37-benchmark-results/test.txt'\n  >>> params = {'session': boto3.Session(profile_name='smart_open')}\n  >>> open(url, 'r', transport_params=params).read(32)\n  'first line\\nsecond line\\nthird lin'\n\nSee ``help(\"smart_open.open\")`` for the full list of acceptable parameter names,\nor view the help online `here <https://github.com/RaRe-Technologies/smart_open/blob/master/help.txt>`__.\n\nIf you pass an invalid parameter name, the ``smart_open.open`` function will warn you about it.\nKeep an eye on your logs for WARNING messages from ``smart_open``.\n"
        },
        {
          "name": "README.rst",
          "type": "blob",
          "size": 20.9287109375,
          "content": "======================================================\nsmart_open — utils for streaming large files in Python\n======================================================\n\n\n|License|_ |GHA|_ |Coveralls|_ |Downloads|_\n\n.. |License| image:: https://img.shields.io/pypi/l/smart_open.svg\n.. |GHA| image:: https://github.com/RaRe-Technologies/smart_open/workflows/Test/badge.svg\n.. |Coveralls| image:: https://coveralls.io/repos/github/RaRe-Technologies/smart_open/badge.svg?branch=develop\n.. |Downloads| image:: https://pepy.tech/badge/smart-open/month\n.. _License: https://github.com/RaRe-Technologies/smart_open/blob/master/LICENSE\n.. _GHA: https://github.com/RaRe-Technologies/smart_open/actions?query=workflow%3ATest\n.. _Coveralls: https://coveralls.io/github/RaRe-Technologies/smart_open?branch=HEAD\n.. _Downloads: https://pypi.org/project/smart-open/\n\n\nWhat?\n=====\n\n``smart_open`` is a Python 3 library for **efficient streaming of very large files** from/to storages such as S3, GCS, Azure Blob Storage, HDFS, WebHDFS, HTTP, HTTPS, SFTP, or local filesystem. It supports transparent, on-the-fly (de-)compression for a variety of different formats.\n\n``smart_open`` is a drop-in replacement for Python's built-in ``open()``: it can do anything ``open`` can (100% compatible, falls back to native ``open`` wherever possible), plus lots of nifty extra stuff on top.\n\n**Python 2.7 is no longer supported. If you need Python 2.7, please use** `smart_open 1.10.1 <https://github.com/RaRe-Technologies/smart_open/releases/tag/1.10.0>`_, **the last version to support Python 2.**\n\nWhy?\n====\n\nWorking with large remote files, for example using Amazon's `boto3 <https://boto3.amazonaws.com/v1/documentation/api/latest/index.html>`_ Python library, is a pain.\n``boto3``'s ``Object.upload_fileobj()`` and ``Object.download_fileobj()`` methods require gotcha-prone boilerplate to use successfully, such as constructing file-like object wrappers.\n``smart_open`` shields you from that. It builds on boto3 and other remote storage libraries, but offers a **clean unified Pythonic API**. The result is less code for you to write and fewer bugs to make.\n\n\nHow?\n=====\n\n``smart_open`` is well-tested, well-documented, and has a simple Pythonic API:\n\n\n.. _doctools_before_examples:\n\n.. code-block:: python\n\n  >>> from smart_open import open\n  >>>\n  >>> # stream lines from an S3 object\n  >>> for line in open('s3://commoncrawl/robots.txt'):\n  ...    print(repr(line))\n  ...    break\n  'User-Agent: *\\n'\n\n  >>> # stream from/to compressed files, with transparent (de)compression:\n  >>> for line in open('smart_open/tests/test_data/1984.txt.gz', encoding='utf-8'):\n  ...    print(repr(line))\n  'It was a bright cold day in April, and the clocks were striking thirteen.\\n'\n  'Winston Smith, his chin nuzzled into his breast in an effort to escape the vile\\n'\n  'wind, slipped quickly through the glass doors of Victory Mansions, though not\\n'\n  'quickly enough to prevent a swirl of gritty dust from entering along with him.\\n'\n\n  >>> # can use context managers too:\n  >>> with open('smart_open/tests/test_data/1984.txt.gz') as fin:\n  ...    with open('smart_open/tests/test_data/1984.txt.bz2', 'w') as fout:\n  ...        for line in fin:\n  ...           fout.write(line)\n  74\n  80\n  78\n  79\n\n  >>> # can use any IOBase operations, like seek\n  >>> with open('s3://commoncrawl/robots.txt', 'rb') as fin:\n  ...     for line in fin:\n  ...         print(repr(line.decode('utf-8')))\n  ...         break\n  ...     offset = fin.seek(0)  # seek to the beginning\n  ...     print(fin.read(4))\n  'User-Agent: *\\n'\n  b'User'\n\n  >>> # stream from HTTP\n  >>> for line in open('http://example.com/index.html'):\n  ...     print(repr(line))\n  ...     break\n  '<!doctype html>\\n'\n\n.. _doctools_after_examples:\n\nOther examples of URLs that ``smart_open`` accepts::\n\n    s3://my_bucket/my_key\n    s3://my_key:my_secret@my_bucket/my_key\n    s3://my_key:my_secret@my_server:my_port@my_bucket/my_key\n    gs://my_bucket/my_blob\n    azure://my_bucket/my_blob\n    hdfs:///path/file\n    hdfs://path/file\n    webhdfs://host:port/path/file\n    ./local/path/file\n    ~/local/path/file\n    local/path/file\n    ./local/path/file.gz\n    file:///home/user/file\n    file:///home/user/file.bz2\n    [ssh|scp|sftp]://username@host//path/file\n    [ssh|scp|sftp]://username@host/path/file\n    [ssh|scp|sftp]://username:password@host/path/file\n\n\nDocumentation\n=============\n\nInstallation\n------------\n\n``smart_open`` supports a wide range of storage solutions, including AWS S3, Google Cloud and Azure.\nEach individual solution has its own dependencies.\nBy default, ``smart_open`` does not install any dependencies, in order to keep the installation size small.\nYou can install these dependencies explicitly using::\n\n    pip install smart_open[azure] # Install Azure deps\n    pip install smart_open[gcs] # Install GCS deps\n    pip install smart_open[s3] # Install S3 deps\n\nOr, if you don't mind installing a large number of third party libraries, you can install all dependencies using::\n\n    pip install smart_open[all]\n\nBe warned that this option increases the installation size significantly, e.g. over 100MB.\n\nIf you're upgrading from ``smart_open`` versions 2.x and below, please check out the `Migration Guide <MIGRATING_FROM_OLDER_VERSIONS.rst>`_.\n\nBuilt-in help\n-------------\n\nFor detailed API info, see the online help:\n\n.. code-block:: python\n\n    help('smart_open')\n\nor click `here <https://github.com/RaRe-Technologies/smart_open/blob/master/help.txt>`__ to view the help in your browser.\n\nMore examples\n-------------\n\nFor the sake of simplicity, the examples below assume you have all the dependencies installed, i.e. you have done::\n\n    pip install smart_open[all]\n\n.. code-block:: python\n\n    >>> import os, boto3\n    >>> from smart_open import open\n    >>>\n    >>> # stream content *into* S3 (write mode) using a custom session\n    >>> session = boto3.Session(\n    ...     aws_access_key_id=os.environ['AWS_ACCESS_KEY_ID'],\n    ...     aws_secret_access_key=os.environ['AWS_SECRET_ACCESS_KEY'],\n    ... )\n    >>> url = 's3://smart-open-py37-benchmark-results/test.txt'\n    >>> with open(url, 'wb', transport_params={'client': session.client('s3')}) as fout:\n    ...     bytes_written = fout.write(b'hello world!')\n    ...     print(bytes_written)\n    12\n\n.. code-block:: python\n\n    # stream from HDFS\n    for line in open('hdfs://user/hadoop/my_file.txt', encoding='utf8'):\n        print(line)\n\n    # stream from WebHDFS\n    for line in open('webhdfs://host:port/user/hadoop/my_file.txt'):\n        print(line)\n\n    # stream content *into* HDFS (write mode):\n    with open('hdfs://host:port/user/hadoop/my_file.txt', 'wb') as fout:\n        fout.write(b'hello world')\n\n    # stream content *into* WebHDFS (write mode):\n    with open('webhdfs://host:port/user/hadoop/my_file.txt', 'wb') as fout:\n        fout.write(b'hello world')\n\n    # stream from a completely custom s3 server, like s3proxy:\n    for line in open('s3u://user:secret@host:port@mybucket/mykey.txt'):\n        print(line)\n\n    # Stream to Digital Ocean Spaces bucket providing credentials from boto3 profile\n    session = boto3.Session(profile_name='digitalocean')\n    client = session.client('s3', endpoint_url='https://ams3.digitaloceanspaces.com')\n    transport_params = {'client': client}\n    with open('s3://bucket/key.txt', 'wb', transport_params=transport_params) as fout:\n        fout.write(b'here we stand')\n\n    # stream from GCS\n    for line in open('gs://my_bucket/my_file.txt'):\n        print(line)\n\n    # stream content *into* GCS (write mode):\n    with open('gs://my_bucket/my_file.txt', 'wb') as fout:\n        fout.write(b'hello world')\n\n    # stream from Azure Blob Storage\n    connect_str = os.environ['AZURE_STORAGE_CONNECTION_STRING']\n    transport_params = {\n        'client': azure.storage.blob.BlobServiceClient.from_connection_string(connect_str),\n    }\n    for line in open('azure://mycontainer/myfile.txt', transport_params=transport_params):\n        print(line)\n\n    # stream content *into* Azure Blob Storage (write mode):\n    connect_str = os.environ['AZURE_STORAGE_CONNECTION_STRING']\n    transport_params = {\n        'client': azure.storage.blob.BlobServiceClient.from_connection_string(connect_str),\n    }\n    with open('azure://mycontainer/my_file.txt', 'wb', transport_params=transport_params) as fout:\n        fout.write(b'hello world')\n\nCompression Handling\n--------------------\n\nThe top-level `compression` parameter controls compression/decompression behavior when reading and writing.\nThe supported values for this parameter are:\n\n- ``infer_from_extension`` (default behavior)\n- ``disable``\n- ``.gz``\n- ``.bz2``\n- ``.zst``\n\nBy default, ``smart_open`` determines the compression algorithm to use based on the file extension.\n\n.. code-block:: python\n\n    >>> from smart_open import open, register_compressor\n    >>> with open('smart_open/tests/test_data/1984.txt.gz') as fin:\n    ...     print(fin.read(32))\n    It was a bright cold day in Apri\n\nYou can override this behavior to either disable compression, or explicitly specify the algorithm to use.\nTo disable compression:\n\n.. code-block:: python\n\n    >>> from smart_open import open, register_compressor\n    >>> with open('smart_open/tests/test_data/1984.txt.gz', 'rb', compression='disable') as fin:\n    ...     print(fin.read(32))\n    b'\\x1f\\x8b\\x08\\x08\\x85F\\x94\\\\\\x00\\x031984.txt\\x005\\x8f=r\\xc3@\\x08\\x85{\\x9d\\xe2\\x1d@'\n\n\nTo specify the algorithm explicitly (e.g. for non-standard file extensions):\n\n.. code-block:: python\n\n    >>> from smart_open import open, register_compressor\n    >>> with open('smart_open/tests/test_data/1984.txt.gzip', compression='.gz') as fin:\n    ...     print(fin.read(32))\n    It was a bright cold day in Apri\n\nYou can also easily add support for other file extensions and compression formats.\nFor example, to open xz-compressed files:\n\n.. code-block:: python\n\n    >>> import lzma, os\n    >>> from smart_open import open, register_compressor\n\n    >>> def _handle_xz(file_obj, mode):\n    ...      return lzma.LZMAFile(filename=file_obj, mode=mode, format=lzma.FORMAT_XZ)\n\n    >>> register_compressor('.xz', _handle_xz)\n\n    >>> with open('smart_open/tests/test_data/1984.txt.xz') as fin:\n    ...     print(fin.read(32))\n    It was a bright cold day in Apri\n\n``lzma`` is in the standard library in Python 3.3 and greater.\nFor 2.7, use `backports.lzma`_.\n\n.. _backports.lzma: https://pypi.org/project/backports.lzma/\n\nTransport-specific Options\n--------------------------\n\n``smart_open`` supports a wide range of transport options out of the box, including:\n\n- S3\n- HTTP, HTTPS (read-only)\n- SSH, SCP and SFTP\n- WebHDFS\n- GCS\n- Azure Blob Storage\n\nEach option involves setting up its own set of parameters.\nFor example, for accessing S3, you often need to set up authentication, like API keys or a profile name.\n``smart_open``'s ``open`` function accepts a keyword argument ``transport_params`` which accepts additional parameters for the transport layer.\nHere are some examples of using this parameter:\n\n.. code-block:: python\n\n  >>> import boto3\n  >>> fin = open('s3://commoncrawl/robots.txt', transport_params=dict(client=boto3.client('s3')))\n  >>> fin = open('s3://commoncrawl/robots.txt', transport_params=dict(buffer_size=1024))\n\nFor the full list of keyword arguments supported by each transport option, see the documentation:\n\n.. code-block:: python\n\n  help('smart_open.open')\n\nS3 Credentials\n--------------\n\n``smart_open`` uses the ``boto3`` library to talk to S3.\n``boto3`` has several `mechanisms <https://boto3.amazonaws.com/v1/documentation/api/latest/guide/configuration.html>`__ for determining the credentials to use.\nBy default, ``smart_open`` will defer to ``boto3`` and let the latter take care of the credentials.\nThere are several ways to override this behavior.\n\nThe first is to pass a ``boto3.Client`` object as a transport parameter to the ``open`` function.\nYou can customize the credentials when constructing the session for the client.\n``smart_open`` will then use the session when talking to S3.\n\n.. code-block:: python\n\n    session = boto3.Session(\n        aws_access_key_id=ACCESS_KEY,\n        aws_secret_access_key=SECRET_KEY,\n        aws_session_token=SESSION_TOKEN,\n    )\n    client = session.client('s3', endpoint_url=..., config=...)\n    fin = open('s3://bucket/key', transport_params={'client': client})\n\nYour second option is to specify the credentials within the S3 URL itself:\n\n.. code-block:: python\n\n    fin = open('s3://aws_access_key_id:aws_secret_access_key@bucket/key', ...)\n\n*Important*: The two methods above are **mutually exclusive**. If you pass an AWS client *and* the URL contains credentials, ``smart_open`` will ignore the latter.\n\n*Important*: ``smart_open`` ignores configuration files from the older ``boto`` library.\nPort your old ``boto`` settings to ``boto3`` in order to use them with ``smart_open``.\n\nS3 Advanced Usage\n-----------------\n\nAdditional keyword arguments can be propagated to the boto3 methods that are used by ``smart_open`` under the hood using the ``client_kwargs`` transport parameter.\n\nFor instance, to upload a blob with Metadata, ACL, StorageClass, these keyword arguments can be passed to ``create_multipart_upload`` (`docs <https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/s3.html#S3.Client.create_multipart_upload>`__).\n\n.. code-block:: python\n\n    kwargs = {'Metadata': {'version': 2}, 'ACL': 'authenticated-read', 'StorageClass': 'STANDARD_IA'}\n    fout = open('s3://bucket/key', 'wb', transport_params={'client_kwargs': {'S3.Client.create_multipart_upload': kwargs}})\n\nIterating Over an S3 Bucket's Contents\n--------------------------------------\n\nSince going over all (or select) keys in an S3 bucket is a very common operation, there's also an extra function ``smart_open.s3.iter_bucket()`` that does this efficiently, **processing the bucket keys in parallel** (using multiprocessing):\n\n.. code-block:: python\n\n  >>> from smart_open import s3\n  >>> # we use workers=1 for reproducibility; you should use as many workers as you have cores\n  >>> bucket = 'silo-open-data'\n  >>> prefix = 'Official/annual/monthly_rain/'\n  >>> for key, content in s3.iter_bucket(bucket, prefix=prefix, accept_key=lambda key: '/201' in key, workers=1, key_limit=3):\n  ...     print(key, round(len(content) / 2**20))\n  Official/annual/monthly_rain/2010.monthly_rain.nc 13\n  Official/annual/monthly_rain/2011.monthly_rain.nc 13\n  Official/annual/monthly_rain/2012.monthly_rain.nc 13\n\nGCS Credentials\n---------------\n``smart_open`` uses the ``google-cloud-storage`` library to talk to GCS.\n``google-cloud-storage`` uses the ``google-cloud`` package under the hood to handle authentication.\nThere are several `options <https://googleapis.dev/python/google-api-core/latest/auth.html>`__ to provide\ncredentials.\nBy default, ``smart_open`` will defer to ``google-cloud-storage`` and let it take care of the credentials.\n\nTo override this behavior, pass a ``google.cloud.storage.Client`` object as a transport parameter to the ``open`` function.\nYou can `customize the credentials <https://googleapis.dev/python/storage/latest/client.html>`__\nwhen constructing the client. ``smart_open`` will then use the client when talking to GCS. To follow allow with\nthe example below, `refer to Google's guide <https://cloud.google.com/storage/docs/reference/libraries#setting_up_authentication>`__\nto setting up GCS authentication with a service account.\n\n.. code-block:: python\n\n    import os\n    from google.cloud.storage import Client\n    service_account_path = os.environ['GOOGLE_APPLICATION_CREDENTIALS']\n    client = Client.from_service_account_json(service_account_path)\n    fin = open('gs://gcp-public-data-landsat/index.csv.gz', transport_params=dict(client=client))\n\nIf you need more credential options, you can create an explicit ``google.auth.credentials.Credentials`` object\nand pass it to the Client. To create an API token for use in the example below, refer to the\n`GCS authentication guide <https://cloud.google.com/storage/docs/authentication#apiauth>`__.\n\n.. code-block:: python\n\n\timport os\n\tfrom google.auth.credentials import Credentials\n\tfrom google.cloud.storage import Client\n\ttoken = os.environ['GOOGLE_API_TOKEN']\n\tcredentials = Credentials(token=token)\n\tclient = Client(credentials=credentials)\n\tfin = open('gs://gcp-public-data-landsat/index.csv.gz', transport_params={'client': client})\n\nGCS Advanced Usage\n------------------\n\nAdditional keyword arguments can be propagated to the GCS open method (`docs <https://cloud.google.com/python/docs/reference/storage/latest/google.cloud.storage.blob.Blob#google_cloud_storage_blob_Blob_open>`__), which is used by ``smart_open`` under the hood, using the ``blob_open_kwargs`` transport parameter.\n\nAdditionally keyword arguments can be propagated to the GCS ``get_blob`` method (`docs <https://cloud.google.com/python/docs/reference/storage/latest/google.cloud.storage.bucket.Bucket#google_cloud_storage_bucket_Bucket_get_blob>`__) when in a read-mode, using the ``get_blob_kwargs`` transport parameter.\n\nAdditional blob properties (`docs <https://cloud.google.com/python/docs/reference/storage/latest/google.cloud.storage.blob.Blob#properties>`__) can be set before an upload, as long as they are not read-only, using the ``blob_properties`` transport parameter.\n\n.. code-block:: python\n\n    open_kwargs = {'predefined_acl': 'authenticated-read'}\n    properties = {'metadata': {'version': 2}, 'storage_class': 'COLDLINE'}\n    fout = open('gs://bucket/key', 'wb', transport_params={'blob_open_kwargs': open_kwargs, 'blob_properties': properties})\n\nAzure Credentials\n-----------------\n\n``smart_open`` uses the ``azure-storage-blob`` library to talk to Azure Blob Storage.\nBy default, ``smart_open`` will defer to ``azure-storage-blob`` and let it take care of the credentials.\n\nAzure Blob Storage does not have any ways of inferring credentials therefore, passing a ``azure.storage.blob.BlobServiceClient``\nobject as a transport parameter to the ``open`` function is required.\nYou can `customize the credentials <https://docs.microsoft.com/en-us/azure/storage/common/storage-samples-python#authentication>`__\nwhen constructing the client. ``smart_open`` will then use the client when talking to. To follow allow with\nthe example below, `refer to Azure's guide <https://docs.microsoft.com/en-us/azure/storage/blobs/storage-quickstart-blobs-python#copy-your-credentials-from-the-azure-portal>`__\nto setting up authentication.\n\n.. code-block:: python\n\n    import os\n    from azure.storage.blob import BlobServiceClient\n    azure_storage_connection_string = os.environ['AZURE_STORAGE_CONNECTION_STRING']\n    client = BlobServiceClient.from_connection_string(azure_storage_connection_string)\n    fin = open('azure://my_container/my_blob.txt', transport_params={'client': client})\n\nIf you need more credential options, refer to the\n`Azure Storage authentication guide <https://docs.microsoft.com/en-us/azure/storage/common/storage-samples-python#authentication>`__.\n\nAzure Advanced Usage\n--------------------\n\nAdditional keyword arguments can be propagated to the ``commit_block_list`` method (`docs <https://azuresdkdocs.blob.core.windows.net/$web/python/azure-storage-blob/12.14.1/azure.storage.blob.html#azure.storage.blob.BlobClient.commit_block_list>`__), which is used by ``smart_open`` under the hood for uploads, using the ``blob_kwargs`` transport parameter.\n\n.. code-block:: python\n\n    kwargs = {'metadata': {'version': 2}}\n    fout = open('azure://container/key', 'wb', transport_params={'blob_kwargs': kwargs})\n\nDrop-in replacement of ``pathlib.Path.open``\n--------------------------------------------\n\n``smart_open.open`` can also be used with ``Path`` objects.\nThe built-in `Path.open()` is not able to read text from compressed files, so use ``patch_pathlib`` to replace it with `smart_open.open()` instead.\nThis can be helpful when e.g. working with compressed files.\n\n.. code-block:: python\n\n    >>> from pathlib import Path\n    >>> from smart_open.smart_open_lib import patch_pathlib\n    >>>\n    >>> _ = patch_pathlib()  # replace `Path.open` with `smart_open.open`\n    >>>\n    >>> path = Path(\"smart_open/tests/test_data/crime-and-punishment.txt.gz\")\n    >>>\n    >>> with path.open(\"r\") as infile:\n    ...     print(infile.readline()[:41])\n    В начале июля, в чрезвычайно жаркое время\n\nHow do I ...?\n=============\n\nSee `this document <howto.md>`__.\n\nExtending ``smart_open``\n========================\n\nSee `this document <extending.md>`__.\n\nTesting ``smart_open``\n======================\n\n``smart_open`` comes with a comprehensive suite of unit tests.\nBefore you can run the test suite, install the test dependencies::\n\n    pip install -e .[test]\n\nNow, you can run the unit tests::\n\n    pytest smart_open\n\nThe tests are also run automatically with `Travis CI <https://travis-ci.org/RaRe-Technologies/smart_open>`_ on every commit push & pull request.\n\nComments, bug reports\n=====================\n\n``smart_open`` lives on `Github <https://github.com/RaRe-Technologies/smart_open>`_. You can file\nissues or pull requests there. Suggestions, pull requests and improvements welcome!\n\n----------------\n\n``smart_open`` is open source software released under the `MIT license <https://github.com/piskvorky/smart_open/blob/master/LICENSE>`_.\nCopyright (c) 2015-now `Radim Řehůřek <https://radimrehurek.com>`_.\n"
        },
        {
          "name": "benchmark",
          "type": "tree",
          "content": null
        },
        {
          "name": "ci_helpers",
          "type": "tree",
          "content": null
        },
        {
          "name": "extending.md",
          "type": "blob",
          "size": 5.1318359375,
          "content": "# Extending `smart_open`\n\nThis document targets potential contributors to `smart_open`.\nCurrently, there are two main directions for extending existing `smart_open` functionality:\n\n1. Add a new transport mechanism\n2. Add a new compression format\n\nThe first is by far the more challenging, and also the more welcome.\n\n## New transport mechanisms\n\nEach transport mechanism lives in its own submodule.\nFor example, currently we have:\n\n- [smart_open.local_file](smart_open/local_file.py)\n- [smart_open.s3](smart_open/s3.py)\n- [smart_open.ssh](smart_open/ssh.py)\n- ... and others\n\nSo, to implement a new transport mechanism, you need to create a new module.\nYour module must expose the following (see [smart_open.http](smart_open/http.py) for the full implementation):\n\n```python\nSCHEMA = ...\n\"\"\"The name of the mechanism, e.g. s3, ssh, etc.\n\nThis is the part that goes before the `://` in a URL, e.g. `s3://`.\"\"\"\n\nURI_EXAMPLES = ('xxx://foo/bar', 'zzz://baz/boz')\n\"\"\"This will appear in the documentation of the the `parse_uri` function.\"\"\"\n\nMISSING_DEPS = False\n\"\"\"Wrap transport-specific imports in a try/catch and set this to True if\nany imports are not found. Seting MISSING_DEPS to True will cause the library\nto suggest installing its dependencies with an example pip command.\n\nIf your transport has no external dependencies, you can omit this variable.\n\"\"\"\n\ndef parse_uri(uri_as_str):\n    \"\"\"Parse the specified URI into a dict.\n\n    At a bare minimum, the dict must have `schema` member.\n    \"\"\"\n    return dict(schema=XXX_SCHEMA, ...)\n\n\ndef open_uri(uri_as_str, mode, transport_params):\n    \"\"\"Return a file-like object pointing to the URI.\n\n    Parameters:\n\n    uri_as_str: str\n        The URI to open\n    mode: str\n        Either \"rb\" or \"wb\".  You don't need to implement text modes,\n        `smart_open` does that for you, outside of the transport layer.\n    transport_params: dict\n        Any additional parameters to pass to the `open` function (see below).\n\n    \"\"\"\n    #\n    # Parse the URI using parse_uri\n    # Consolidate the parsed URI with transport_params, if needed\n    # Pass everything to the open function (see below).\n    #\n    ...\n\n\ndef open(..., mode, param1=None, param2=None, paramN=None):\n    \"\"\"This function does the hard work.\n\n    The keyword parameters are the transport_params from the `open_uri`\n    function.\n\n    \"\"\"\n    ...\n```\n\nHave a look at the existing mechanisms to see how they work.\nYou may define other functions and classes as necessary for your implementation.\n\nOnce your module is working, register it in the [smart_open.transport](smart_open/transport.py) submodule.\nThe `register_transport()` function updates a mapping from schemes to the modules that implement functionality for them.\n\nOnce you've registered your new transport module, the following will happen automagically:\n\n1. `smart_open` will be able to open any URI supported by your module\n2. The docstring for the `smart_open.open` function will contain a section\n   detailing the parameters for your transport module.\n3. The docstring for the `parse_uri` function will include the schemas and\n   examples supported by your module.\n\nYou can confirm the documentation changes by running:\n\n    python -c 'help(\"smart_open\")'\n\nand verify that documentation for your new submodule shows up.\n\n### What's the difference between the `open_uri` and `open` functions?\n\nThere are several key differences between the two.\n\nFirst, the parameters to `open_uri` are the same for _all transports_.\nOn the other hand, the parameters to the `open` function can differ from transport to transport.\n\nSecond, the responsibilities of the two functions are also different.\nThe `open` function opens the remote object.\nThe `open_uri` function deals with parsing transport-specific details out of the URI, and then delegates to `open`.\n\nThe `open` function contains documentation for transport parameters.\nThis documentation gets parsed by the `doctools` module and appears in various docstrings.\n\nSome of these differences are by design; others as a consequence of evolution.\n\n## New compression mechanisms\n\nThe compression layer is self-contained in the `smart_open.compression` submodule.\n\nTo add support for a new compressor:\n\n- Create a new function to handle your compression format (given an extension)\n- Add your compressor to the registry\n\nFor example:\n\n```python\ndef _handle_xz(file_obj, mode):\n    import lzma\n    return lzma.LZMAFile(filename=file_obj, mode=mode, format=lzma.FORMAT_XZ)\n\n\nregister_compressor('.xz', _handle_xz)\n```\n\nThere are many compression formats out there, and supporting all of them is beyond the scope of `smart_open`.\nWe want our code's functionality to cover the bare minimum required to satisfy 80% of our users.\nWe leave the remaining 20% of users with the ability to deal with compression in their own code, using the trivial mechanism described above.\n\nDocumentation\n-------------\n\nOnce you've contributed your extension, please add it to the documentation so that it is discoverable for other users.\nSome notable files:\n\n- setup.py: See the `description` keyword.  Not all contributions will affect this.\n- README.rst\n- howto.md (if your extension solves a specific problem that doesn't get covered by other documentation)\n"
        },
        {
          "name": "help.txt",
          "type": "blob",
          "size": 12.541015625,
          "content": "Help on package smart_open:\n\nNAME\n    smart_open\n\nDESCRIPTION\n    Utilities for streaming to/from several file-like data storages: S3 / HDFS / local\n    filesystem / compressed files, and many more, using a simple, Pythonic API.\n    \n    The streaming makes heavy use of generators and pipes, to avoid loading\n    full file contents into memory, allowing work with arbitrarily large files.\n    \n    The main functions are:\n    \n    * `open()`, which opens the given file for reading/writing\n    * `parse_uri()`\n    * `s3_iter_bucket()`, which goes over all keys in an S3 bucket in parallel\n    * `register_compressor()`, which registers callbacks for transparent compressor handling\n\nPACKAGE CONTENTS\n    azure\n    bytebuffer\n    compression\n    concurrency\n    constants\n    doctools\n    gcs\n    hdfs\n    http\n    local_file\n    s3\n    smart_open_lib\n    ssh\n    tests (package)\n    transport\n    utils\n    version\n    webhdfs\n\nFUNCTIONS\n    open(uri, mode='r', buffering=-1, encoding=None, errors=None, newline=None, closefd=True, opener=None, ignore_ext=False, transport_params=None)\n        Open the URI object, returning a file-like object.\n        \n        The URI is usually a string in a variety of formats.\n        For a full list of examples, see the :func:`parse_uri` function.\n        \n        The URI may also be one of:\n        \n        - an instance of the pathlib.Path class\n        - a stream (anything that implements io.IOBase-like functionality)\n        \n        Parameters\n        ----------\n        uri: str or object\n            The object to open.\n        mode: str, optional\n            Mimicks built-in open parameter of the same name.\n        buffering: int, optional\n            Mimicks built-in open parameter of the same name.\n        encoding: str, optional\n            Mimicks built-in open parameter of the same name.\n        errors: str, optional\n            Mimicks built-in open parameter of the same name.\n        newline: str, optional\n            Mimicks built-in open parameter of the same name.\n        closefd: boolean, optional\n            Mimicks built-in open parameter of the same name.  Ignored.\n        opener: object, optional\n            Mimicks built-in open parameter of the same name.  Ignored.\n        ignore_ext: boolean, optional\n            Disable transparent compression/decompression based on the file extension.\n        transport_params: dict, optional\n            Additional parameters for the transport layer (see notes below).\n        \n        Returns\n        -------\n        A file-like object.\n        \n        Notes\n        -----\n        smart_open has several implementations for its transport layer (e.g. S3, HTTP).\n        Each transport layer has a different set of keyword arguments for overriding\n        default behavior.  If you specify a keyword argument that is *not* supported\n        by the transport layer being used, smart_open will ignore that argument and\n        log a warning message.\n        \n        smart_open supports the following transport mechanisms:\n        \n        azure (smart_open/azure.py)\n        ~~~~~~~~~~~~~~~~~~~~~~~~~~~\n        Implements file-like objects for reading and writing to/from Azure Blob Storage.\n        \n        buffer_size: int, optional\n            The buffer size to use when performing I/O. For reading only.\n        min_part_size: int, optional\n            The minimum part size for multipart uploads.  For writing only.\n        \n        file (smart_open/local_file.py)\n        ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n        Implements the transport for the file:// schema.\n        \n        gs (smart_open/gcs.py)\n        ~~~~~~~~~~~~~~~~~~~~~~\n        Implements file-like objects for reading and writing to/from GCS.\n        \n        buffer_size: int, optional\n            The buffer size to use when performing I/O. For reading only.\n        min_part_size: int, optional\n            The minimum part size for multipart uploads.  For writing only.\n        client: google.cloud.storage.Client, optional\n            The GCS client to use when working with google-cloud-storage.\n        \n        hdfs (smart_open/hdfs.py)\n        ~~~~~~~~~~~~~~~~~~~~~~~~~\n        Implements reading and writing to/from HDFS.\n        \n        http (smart_open/http.py)\n        ~~~~~~~~~~~~~~~~~~~~~~~~~\n        Implements file-like objects for reading from http.\n        \n        kerberos: boolean, optional\n            If True, will attempt to use the local Kerberos credentials\n        user: str, optional\n            The username for authenticating over HTTP\n        password: str, optional\n            The password for authenticating over HTTP\n        cert: str/tuple, optional\n            If String, path to ssl client cert file (.pem). If Tuple, (‘cert’, ‘key’)\n        headers: dict, optional\n            Any headers to send in the request. If ``None``, the default headers are sent:\n            ``{'Accept-Encoding': 'identity'}``. To use no headers at all,\n            set this variable to an empty dict, ``{}``.\n        \n        s3 (smart_open/s3.py)\n        ~~~~~~~~~~~~~~~~~~~~~\n        Implements file-like objects for reading and writing from/to AWS S3.\n        \n        buffer_size: int, optional\n            The buffer size to use when performing I/O.\n        min_part_size: int, optional\n            The minimum part size for multipart uploads.  For writing only.\n        multipart_upload: bool, optional\n            Default: `True`\n            If set to `True`, will use multipart upload for writing to S3. If set\n            to `False`, S3 upload will use the S3 Single-Part Upload API, which\n            is more ideal for small file sizes.\n            For writing only.\n        version_id: str, optional\n            Version of the object, used when reading object.\n            If None, will fetch the most recent version.\n        defer_seek: boolean, optional\n            Default: `False`\n            If set to `True` on a file opened for reading, GetObject will not be\n            called until the first seek() or read().\n            Avoids redundant API queries when seeking before reading.\n        client: object, optional\n            The S3 client to use when working with boto3.\n            If you don't specify this, then smart_open will create a new client for you.\n        client_kwargs: dict, optional\n            Additional parameters to pass to the relevant functions of the client.\n            The keys are fully qualified method names, e.g. `S3.Client.create_multipart_upload`.\n            The values are kwargs to pass to that method each time it is called.\n        writebuffer: IO[bytes], optional\n            By default, this module will buffer data in memory using io.BytesIO\n            when writing. Pass another binary IO instance here to use it instead.\n            For example, you may pass a file object to buffer to local disk instead\n            of in RAM. Use this to keep RAM usage low at the expense of additional\n            disk IO. If you pass in an open file, then you are responsible for\n            cleaning it up after writing completes.\n        \n        scp (smart_open/ssh.py)\n        ~~~~~~~~~~~~~~~~~~~~~~~\n        Implements I/O streams over SSH.\n        \n        mode: str, optional\n            The mode to use for opening the file.\n        host: str, optional\n            The hostname of the remote machine.  May not be None.\n        user: str, optional\n            The username to use to login to the remote machine.\n            If None, defaults to the name of the current user.\n        password: str, optional\n            The password to use to login to the remote machine.\n        port: int, optional\n            The port to connect to.\n        transport_params: dict, optional\n            Any additional settings to be passed to paramiko.SSHClient.connect\n        \n        webhdfs (smart_open/webhdfs.py)\n        ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n        Implements reading and writing to/from WebHDFS.\n        \n        min_part_size: int, optional\n            For writing only.\n        \n        Examples\n        --------\n        \n        >>> from smart_open import open\n        >>>\n        >>> # stream lines from an S3 object\n        >>> for line in open('s3://commoncrawl/robots.txt'):\n        ...    print(repr(line))\n        ...    break\n        'User-Agent: *\\n'\n        \n        >>> # stream from/to compressed files, with transparent (de)compression:\n        >>> for line in open('smart_open/tests/test_data/1984.txt.gz', encoding='utf-8'):\n        ...    print(repr(line))\n        'It was a bright cold day in April, and the clocks were striking thirteen.\\n'\n        'Winston Smith, his chin nuzzled into his breast in an effort to escape the vile\\n'\n        'wind, slipped quickly through the glass doors of Victory Mansions, though not\\n'\n        'quickly enough to prevent a swirl of gritty dust from entering along with him.\\n'\n        \n        >>> # can use context managers too:\n        >>> with open('smart_open/tests/test_data/1984.txt.gz') as fin:\n        ...    with open('smart_open/tests/test_data/1984.txt.bz2', 'w') as fout:\n        ...        for line in fin:\n        ...           fout.write(line)\n        \n        >>> # can use any IOBase operations, like seek\n        >>> with open('s3://commoncrawl/robots.txt', 'rb') as fin:\n        ...     for line in fin:\n        ...         print(repr(line.decode('utf-8')))\n        ...         break\n        ...     offset = fin.seek(0)  # seek to the beginning\n        ...     print(fin.read(4))\n        'User-Agent: *\\n'\n        b'User'\n        \n        >>> # stream from HTTP\n        >>> for line in open('http://example.com/index.html'):\n        ...     print(repr(line))\n        ...     break\n        \n        This function also supports transparent compression and decompression \n        using the following codecs:\n        \n        * .bz2\n        * .gz\n        \n        The function depends on the file extension to determine the appropriate codec.\n        \n        \n        See Also\n        --------\n        - `Standard library reference <https://docs.python.org/3.7/library/functions.html#open>`__\n        - `smart_open README.rst\n          <https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst>`__\n    \n    parse_uri(uri_as_string)\n        Parse the given URI from a string.\n        \n        Parameters\n        ----------\n        uri_as_string: str\n            The URI to parse.\n        \n        Returns\n        -------\n        collections.namedtuple\n            The parsed URI.\n        \n        Notes\n        -----\n        Supported URI schemes are:\n        \n        * azure\n        * file\n        * gs\n        * hdfs\n        * http\n        * s3\n        * scp\n        * webhdfs\n        \n        Valid URI examples::\n        \n        * ./local/path/file\n        * ~/local/path/file\n        * local/path/file\n        * ./local/path/file.gz\n        * file:///home/user/file\n        * file:///home/user/file.bz2\n        * hdfs:///path/file\n        * hdfs://path/file\n        * s3://my_bucket/my_key\n        * s3://my_key:my_secret@my_bucket/my_key\n        * s3://my_key:my_secret@my_server:my_port@my_bucket/my_key\n        * ssh://username@host/path/file\n        * ssh://username@host//path/file\n        * scp://username@host/path/file\n        * sftp://username@host/path/file\n        * webhdfs://host:port/path/file\n    \n    register_compressor(ext, callback)\n        Register a callback for transparently decompressing files with a specific extension.\n        \n        Parameters\n        ----------\n        ext: str\n            The extension.  Must include the leading period, e.g. ``.gz``.\n        callback: callable\n            The callback.  It must accept two position arguments, file_obj and mode.\n            This function will be called when ``smart_open`` is opening a file with\n            the specified extension.\n        \n        Examples\n        --------\n        \n        Instruct smart_open to use the `lzma` module whenever opening a file\n        with a .xz extension (see README.rst for the complete example showing I/O):\n        \n        >>> def _handle_xz(file_obj, mode):\n        ...     import lzma\n        ...     return lzma.LZMAFile(filename=file_obj, mode=mode, format=lzma.FORMAT_XZ)\n        >>>\n        >>> register_compressor('.xz', _handle_xz)\n    \n    s3_iter_bucket(bucket_name, prefix='', accept_key=None, key_limit=None, workers=16, retries=3, **session_kwargs)\n        Deprecated.  Use smart_open.s3.iter_bucket instead.\n    \n    smart_open(uri, mode='rb', buffering=-1, encoding=None, errors=None, newline=None, closefd=True, opener=None, ignore_extension=False, **kwargs)\n\nDATA\n    __all__ = ['open', 'parse_uri', 'register_compressor', 's3_iter_bucket...\n\nVERSION\n    4.1.2.dev0\n\nFILE\n    /Users/misha/git/smart_open/smart_open/__init__.py\n\n\n"
        },
        {
          "name": "howto.md",
          "type": "blob",
          "size": 17.21875,
          "content": "# How-to Guides\n\nThe howtos are **goal-oriented guides** that demonstrate **how to solve a specific problem** using `smart_open`.\n\n## How to Add a New Guide\n\nThe guides are code snippets compatible with Python's [doctest](https://docs.python.org/2/library/doctest.html) module.\nLines that start with `>>>` and `...` are Python commands to run via the interpreter.\nLines without the above prefixes are expected standard output from the commands.\nThe `doctest` module runs the commands and ensures that their output matches the expected values.\n\n```python\n>>> foo = 'bar'\n>>> print(foo)\nbar\n\n```\n\nSome tips:\n\n- Enclose the snippets with markdowns triple backticks to get free syntax highlighting\n- End your example with a blank line to let `doctest` know the triple backticks aren't part of the example\n\nFinally, ensure all the guides still work by running:\n\n    python -m doctest howto.md\n\nThe above command shouldn't print anything to standard output/error and return zero, provided your local environment is set up correctly:\n\n- you have a working Internet connection\n- localstack is running, and the `mybucket` S3 bucket has been created\n- the GITHUB_TOKEN environment variable is set to a valid [access token](https://docs.github.com/en/authentication/keeping-your-account-and-data-secure/creating-a-personal-access-token)\n\n## How to Read/Write Zip Files\n\n`smart_open` does not support reading/writing zip files out of the box.\nHowever, you can easily integrate `smart_open` with the standard library's [zipfile](https://docs.python.org/3.5/library/zipfile.html) module:\n\n- `smart_open` handles the I/O\n- `zipfile` handles the compression, decompression, and file member lookup\n\nReading example:\n\n```python\n>>> from smart_open import open\n>>> import zipfile\n>>> with open('sampledata/hello.zip', 'rb') as fin:\n...     with zipfile.ZipFile(fin) as zip:\n...         for info in zip.infolist():\n...             file_bytes = zip.read(info.filename)\n...             print('%r: %r' % (info.filename, file_bytes.decode('utf-8')))\n'hello/': ''\n'hello/en.txt': 'hello world!\\n'\n'hello/ru.txt': 'здравствуй, мир!\\n'\n\n```\n\nWriting example:\n\n```python\n>>> from smart_open import open\n>>> import os\n>>> import tempfile\n>>> import zipfile\n>>> tmp = tempfile.NamedTemporaryFile(prefix='smart_open-howto-', suffix='.zip', delete=False)\n>>> with open(tmp.name, 'wb') as fout:\n... \twith zipfile.ZipFile(fout, 'w') as zip:\n...\t\t\tzip.writestr('hello/en.txt', 'hello world!\\n')\n...\t\t\tzip.writestr('hello/ru.txt', 'здравствуй, мир!\\n')\n>>> os.unlink(tmp.name)  # comment this line to keep the file for later\n\n```\n\n## How to access S3 anonymously\n\nThe `boto3` library that `smart_open` uses for accessing S3 signs each request using your `boto3` credentials.\nIf you'd like to access S3 without using an S3 account, then you need disable this signing mechanism.\n\n```python\n>>> import boto3\n>>> import botocore\n>>> import botocore.client\n>>> from smart_open import open\n>>> config = botocore.client.Config(signature_version=botocore.UNSIGNED)\n>>> params = {'client': boto3.client('s3', config=config)}\n>>> with open('s3://commoncrawl/robots.txt', transport_params=params) as fin:\n...    fin.readline()\n'User-Agent: *\\n'\n\n```\n## How to Access S3 Object Properties\n\nWhen working with AWS S3, you may want to look beyond the abstraction\nprovided by `smart_open` and communicate with `boto3` directly in order to\nsatisfy your use case.\n\nFor example:\n\n- Access the object's properties, such as the content type, timestamp of the last change, etc.\n- Access version information for the object (versioned buckets only)\n- Copy the object to another location\n- Apply an ACL to the object\n- and anything else specified in the [boto3 S3 Object API](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/s3.html#object).\n\nTo enable such use cases, the file-like objects returned by `smart_open` have a special `to_boto3` method.\nThis returns a `boto3.s3.Object` that you can work with directly.\nFor example, let's get the content type of a publicly available file:\n\n```python\n>>> import boto3\n>>> from smart_open import open\n>>> resource = boto3.resource('s3')  # Pass additional resource parameters here\n>>> with open('s3://commoncrawl/robots.txt') as fin:\n...    print(fin.readline().rstrip())\n...    boto3_s3_object = fin.to_boto3(resource)\n...    print(repr(boto3_s3_object))\n...    print(boto3_s3_object.content_type)  # Using the boto3 API here\nUser-Agent: *\ns3.Object(bucket_name='commoncrawl', key='robots.txt')\ntext/plain\n\n```\n\nThis works only when reading and writing via S3.\n\n## How to Access a Specific Version of an S3 Object\n\nThe ``version_id`` transport parameter enables you to get the desired version of the object from an S3 bucket.\n\n.. Important::\n    S3 disables version control by default.\n    Before using the ``version_id`` parameter, you must explicitly enable version control for your S3 bucket.\n    Read https://docs.aws.amazon.com/AmazonS3/latest/dev/Versioning.html for details.\n\n```python\n>>> import boto3\n>>> from smart_open import open\n>>> versions = ['KiQpZPsKI5Dm2oJZy_RzskTOtl2snjBg', 'N0GJcE3TQCKtkaS.gF.MUBZS85Gs3hzn']\n>>> for v in versions:\n...     with open('s3://smart-open-versioned/demo.txt', transport_params={'version_id': v}) as fin:\n...         print(v, repr(fin.read()))\nKiQpZPsKI5Dm2oJZy_RzskTOtl2snjBg 'second version\\n'\nN0GJcE3TQCKtkaS.gF.MUBZS85Gs3hzn 'first version\\n'\n\n>>> # If you don't specify a version, smart_open will read the most recent one\n>>> with open('s3://smart-open-versioned/demo.txt') as fin:\n...     print(repr(fin.read()))\n'second version\\n'\n\n```\n\nThis works only when reading via S3.\n\n## How to Access the Underlying boto3 Object\n\nAt some stage in your workflow, you may opt to work with `boto3` directly.\nYou can do this by calling to the `to_boto3()` method.\nYou can then interact with the object using the `boto3` API:\n\n\n```python\n>>> import boto3\n>>> from smart_open import open\n>>> resource = boto3.resource('s3')  # Pass additional resource parameters here\n>>> with open('s3://commoncrawl/robots.txt') as fin:\n...     boto3_object = fin.to_boto3(resource)\n...     print(boto3_object)\n...     print(boto3_object.get()['LastModified'])\ns3.Object(bucket_name='commoncrawl', key='robots.txt')\n2016-05-21 18:17:43+00:00\n\n```\n\nThis works only when reading and writing via S3.\n\nFor versioned objects, the returned object will be slightly different:\n\n```python\n>>> from smart_open import open\n>>> resource = boto3.resource('s3')\n>>> params = {'version_id': 'KiQpZPsKI5Dm2oJZy_RzskTOtl2snjBg'}\n>>> with open('s3://smart-open-versioned/demo.txt', transport_params=params) as fin:\n...     print(fin.to_boto3(resource))\ns3.ObjectVersion(bucket_name='smart-open-versioned', object_key='demo.txt', id='KiQpZPsKI5Dm2oJZy_RzskTOtl2snjBg')\n\n```\n\n## How to Read from S3 Efficiently\n\nUnder the covers, `smart_open` uses the [boto3 client API](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/s3.html#client) to read from S3.\nBy default, calling `smart_open.open` with an S3 URL will create its own boto3 client.\nThese are expensive operations: they require both CPU time to construct the objects from a low-level API definition, and memory to store the objects once they have been created.\nIt is possible to save both CPU time and memory by sharing the same resource across multiple `smart_open.open` calls, for example:\n\n```python\n>>> import boto3\n>>> from smart_open import open\n>>> tp = {'client': boto3.client('s3')}\n>>> for month in (1, 2, 3):\n...     url = 's3://nyc-tlc/trip data/yellow_tripdata_2020-%02d.csv' % month\n...     with open(url, transport_params=tp) as fin:\n...         _ = fin.readline()  # skip CSV header\n...         print(fin.readline().strip())\n1,2020-01-01 00:28:15,2020-01-01 00:33:03,1,1.20,1,N,238,239,1,6,3,0.5,1.47,0,0.3,11.27,2.5\n1,2020-02-01 00:17:35,2020-02-01 00:30:32,1,2.60,1,N,145,7,1,11,0.5,0.5,2.45,0,0.3,14.75,0\n1,2020-03-01 00:31:13,2020-03-01 01:01:42,1,4.70,1,N,88,255,1,22,3,0.5,2,0,0.3,27.8,2.5\n\n```\n\nClients are thread-safe and multiprocess-safe, so you may share them between other threads and subprocesses.\n\n## How to Write to S3 Efficiently\n\nBy default, `smart_open` buffers the most recent part of a multipart upload in memory.\nThe default part size is 50MB.\nIf you're concerned about memory usage, then you have two options.\nThe first option is to use smaller part sizes (e.g. 5MB, the lowest value permitted by AWS):\n\n```python\nimport boto3\nfrom smart_open import open\ntp = {'min_part_size': 5 * 1024**2}\nwith open('s3://bucket/key', 'w', transport_params=tp) as fout:\n    fout.write(lots_of_data)\n```\n\nThis will split your upload into smaller parts.\nBe warned that AWS enforces a [limit](https://docs.aws.amazon.com/AmazonS3/latest/userguide/qfacts.html) of a maximum of 10,000 parts per upload.\n\nThe second option is to use a temporary file as a buffer instead.\n\n```python\nimport boto3\nfrom smart_open import open\nwith tempfile.NamedTemporaryFile() as tmp:\n    tp = {'writebuffer': tmp}\n    with open('s3://bucket/key', 'w', transport_params=tp) as fout:\n        fout.write(lots_of_data)\n```\n\nThis option reduces memory usage at the expense of additional disk I/O (writing to and reading from a hard disk is slower).\n\n## How to Specify the Request Payer (S3 only)\n\nSome public buckets require you to [pay for S3 requests for the data in the bucket](https://docs.aws.amazon.com/AmazonS3/latest/dev/RequesterPaysBuckets.html).\nThis relieves the bucket owner of the data transfer costs, and spreads them among the consumers of the data.\n\nTo access such buckets, you need to pass some special transport parameters:\n\n```python\n>>> from smart_open import open\n>>> params = {'client_kwargs': {'S3.Client.get_object': {'RequestPayer': 'requester'}}}\n>>> with open('s3://arxiv/pdf/arXiv_pdf_manifest.xml', transport_params=params) as fin:\n...    print(fin.readline())\n<?xml version='1.0' standalone='yes'?>\n<BLANKLINE>\n\n```\n\nThis works only when reading and writing via S3.\n\n## How to Make S3 I/O Robust to Network Errors\n\nBoto3 has a [built-in mechanism](https://boto3.amazonaws.com/v1/documentation/api/latest/guide/retries.html) for retrying after a recoverable error.\nYou can fine-tune it using several ways:\n\n### Pre-configuring a boto3 client and then passing the client to smart_open\n\n```python\n>>> import boto3\n>>> import botocore.config\n>>> import smart_open\n>>> config = botocore.config.Config(retries={'mode': 'standard'})\n>>> client = boto3.client('s3', config=config)\n>>> tp = {'client': client}\n>>> with open('s3://commoncrawl/robots.txt', transport_params=tp) as fin:\n...     print(fin.readline())\nUser-Agent: *\n<BLANKLINE>\n\n```\n\nTo verify your settings have effect:\n\n```python\nimport logging\nlogging.getLogger('smart_open.s3').setLevel(logging.DEBUG)\n```\n\nand check the log output of your code.\n\n## How to Pass Additional Parameters to boto3\n\n`boto3` is a highly configurable library, and each function call accepts many optional parameters.\n`smart_open` does not attempt to replicate this behavior, since most of these parameters often do not influence the behavior of `smart_open` itself.\nInstead, `smart_open` offers the caller of the function to pass additional parameters as necessary:\n\n```python\n>>> import boto3\n>>> from smart_open import open\n>>> transport_params = {'client_kwargs': {'S3.Client.get_object': {'RequestPayer': 'requester'}}}\n>>> with open('s3://arxiv/pdf/arXiv_pdf_manifest.xml', transport_params=params) as fin:\n...    print(fin.readline())\n<?xml version='1.0' standalone='yes'?>\n<BLANKLINE>\n\n```\n\nThe above example influences how the [S3.Client.get_object function](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/s3.html#S3.Client.get_object) gets called by `smart_open` when reading the specified URL.\nMore specifically, the `RequestPayer` parameter will be set to `requester` **for each call**.\nInfluential functions include:\n\n- S3.Client (the initializer function)\n- S3.Client.abort_multipart_upload\n- S3.Client.complete_multipart_upload\n- S3.Client.create_multipart_upload\n- S3.Client.get_object\n- S3.Client.head_bucket\n- S3.Client.put_object\n- S3.Client.upload_part\n\nIf you choose to pass additional parameters, keep the following in mind:\n\n1. Study the [boto3 client API](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/s3.html#S3.Client) and ensure the function and parameters are valid.\n2. Study the [code for the smart_open.s3 submodule](smart_open/s3.py) and ensure `smart_open` is actually calling the function you're passing additional parameters for.\n\nFinally, in some cases, it's possible to work directly with `boto3` without going through `smart_open`.\nFor example, setting the ACL for an object is possible after the object is created (with `boto3`), as opposed to at creation time (with `smart_open`).\nMore specifically, here's the direct method:\n\n```python\nimport boto3\nimport smart_open\nwith open('s3://bucket/key', 'wb') as fout:\n    fout.write(b'hello world!')\nclient = boto3.client('s3')\nclient.put_object_acl(ACL=acl_as_string)\n```\n\nHere's the same code that passes the above parameter via `smart_open`:\n\n```python\nimport smart_open\ntp = {'client_kwargs': {'S3.Client.create_multipart_upload': {'ACL': acl_as_string}}}\nwith open('s3://bucket/key', 'wb', transport_params=tp) as fout:\n    fout.write(b'hello world!')\n```\n\nIf passing everything via `smart_open` feels awkward, try passing part of the parameters directly to `boto3`.\n\n## How to Read from Github API\n\nThe Github API allows users access to, among many other things, read files from repositories that you have \naccess to. Below is an example for how users can read a file with smart_open. For more info, see the \n[Github API documentation](https://docs.github.com/en/rest/reference/repos#contents).\n\n```python\n>>> import base64\n>>> import gzip\n>>> import json\n>>> import os\n>>> from smart_open import open\n>>> owner, repo, path = \"RaRe-Technologies\", \"smart_open\", \"howto.md\"\n>>> github_token = os.environ['GITHUB_TOKEN']\n>>> url = f\"https://api.github.com/repos/{owner}/{repo}/contents/{path}\"\n>>> params = {\"headers\" : {\"Authorization\" : \"Bearer \" + github_token}}\n>>> with open(url, 'rb', transport_params=params) as fin:\n...     response = json.loads(gzip.decompress(fin.read()))\n>>> response[\"path\"]\n'howto.md'\n\n```\n\nNote: If you are accessing a file in a Github Enterprise org, you will likely have a different base dns than\n      the `https://api.github.com/` in the example.\n\n## How to Read/Write from localstack\n\n[localstack](https://github.com/localstack/localstack) is a convenient test framework for developing cloud apps.\nYou run it locally on your machine and behaves almost identically to the real AWS.\nThis makes it useful for testing your code offline, without requiring you to set up mocks or test harnesses.\n\nFirst, install localstack and start it:\n\n    $ pip install localstack\n    $ localstack start\n\nThe start command is blocking, so you'll need to run it in a separate terminal session or run it in the background.\nBefore we can read/write, we'll need to create a bucket:\n\n    $ aws --endpoint-url http://localhost:4566 s3api create-bucket --bucket mybucket\n\nwhere `http://localhost:4566` is the default host/port that localstack uses to listen for requests.\n\nYou can now read/write to the bucket the same way you would to a real S3 bucket:\n\n```python\n>>> import boto3\n>>> from smart_open import open\n>>> client = boto3.client('s3', endpoint_url='http://localhost:4566')\n>>> tparams = {'client': client}\n>>> with open('s3://mybucket/hello.txt', 'wt', transport_params=tparams) as fout:\n...     _ = fout.write('hello world!')\n>>> with open('s3://mybucket/hello.txt', 'rt', transport_params=tparams) as fin:\n...     fin.read()\n'hello world!'\n\n```\n\nYou can also access it using the CLI:\n\n    $ aws --endpoint-url http://localhost:4566 s3 ls s3://mybucket/\n    2020-12-09 15:56:22         12 hello.txt\n\n## How to Download a Whole Directory From Google Cloud\n\nObject storage providers generally don't provide real directories, and instead\nemulate them using object name patterns (see\n[here](https://stackoverflow.com/questions/38416598/how-to-create-an-empty-folder-on-google-storage-with-google-api/38417397#38417397)\nfor an explanation). To download all files in a directory you can do this:\n\n```python\n>>> from google.cloud import storage\n>>> from smart_open import open\n>>> client = storage.Client()\n>>> bucket_name = \"gcp-public-data-landsat\"\n>>> prefix = \"LC08/01/044/034/LC08_L1GT_044034_20130330_20170310_01_T2/\"\n>>> for blob in client.list_blobs(client.get_bucket(bucket_name), prefix=prefix):\n...      with open(f\"gs://{bucket_name}/{blob.name}\") as f:\n...          print(f.name)\n...          break # just show the first iteration for the test\nLC08/01/044/034/LC08_L1GT_044034_20130330_20170310_01_T2/LC08_L1GT_044034_20130330_20170310_01_T2_ANG.txt\n\n```\n\n## How to Access Google Cloud Anonymously\n\nThe `google-cloud-storage` library that `smart_open` uses expects credentials and authenticated access by default.\nIf you would like to access GCS without using an account you need to explicitly use an anonymous client.\n\n```python\n>>> from google.cloud import storage\n>>> from smart_open import open\n>>> client = storage.Client.create_anonymous_client()\n>>> f = open(\"gs://gcp-public-data-landsat/index.csv.gz\", transport_params=dict(client=client))\n>>> f.readline()\n'SCENE_ID,PRODUCT_ID,SPACECRAFT_ID,SENSOR_ID,DATE_ACQUIRED,COLLECTION_NUMBER,COLLECTION_CATEGORY,SENSING_TIME,DATA_TYPE,WRS_PATH,WRS_ROW,CLOUD_COVER,NORTH_LAT,SOUTH_LAT,WEST_LON,EAST_LON,TOTAL_SIZE,BASE_URL\\n'\n\n```\n"
        },
        {
          "name": "integration-tests",
          "type": "tree",
          "content": null
        },
        {
          "name": "pyproject.toml",
          "type": "blob",
          "size": 0.0517578125,
          "content": "[tool.pytest.ini_options]\ntestpaths = [\"smart_open\"]\n"
        },
        {
          "name": "release",
          "type": "tree",
          "content": null
        },
        {
          "name": "sampledata",
          "type": "tree",
          "content": null
        },
        {
          "name": "setup.py",
          "type": "blob",
          "size": 3.12890625,
          "content": "#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n#\n# Copyright (C) 2015 Radim Rehurek <me@radimrehurek.com>\n#\n# This code is distributed under the terms and conditions\n# from the MIT License (MIT).\n\n\nimport io\nimport os\n\nfrom setuptools import setup, find_packages\n\n\ndef _get_version():\n    curr_dir = os.path.dirname(os.path.abspath(__file__))\n    with open(os.path.join(curr_dir, 'smart_open', 'version.py')) as fin:\n        line = fin.readline().strip()\n        parts = line.split(' ')\n        assert len(parts) == 3\n        assert parts[0] == '__version__'\n        assert parts[1] == '='\n        return parts[2].strip('\\'\"')\n\n\n#\n# We cannot do \"from smart_open.version import __version__\" because that will\n# require the dependencies for smart_open to already be in place, and that is\n# not necessarily the case when running setup.py for the first time.\n#\n__version__ = _get_version()\n\n\ndef read(fname):\n    return io.open(os.path.join(os.path.dirname(__file__), fname), encoding='utf-8').read()\n\nbase_deps = ['wrapt']\naws_deps = ['boto3']\ngcs_deps = ['google-cloud-storage>=2.6.0']\nazure_deps = ['azure-storage-blob', 'azure-common', 'azure-core']\nhttp_deps = ['requests']\nssh_deps = ['paramiko']\nzst_deps = ['zstandard']\n\nall_deps = aws_deps + gcs_deps + azure_deps + http_deps + ssh_deps + zst_deps\ntests_require = all_deps + [\n    'moto[server]',\n    'responses',\n    'pytest',\n    'pytest-rerunfailures',\n    'pytest_benchmark',\n    'awscli',\n    'pyopenssl',\n    'numpy',\n]\n\nsetup(\n    name='smart_open',\n    version=__version__,\n    description='Utils for streaming large files (S3, HDFS, GCS, Azure Blob Storage, gzip, bz2...)',\n    long_description=read('README.rst'),\n    packages=find_packages(exclude=[\"smart_open.tests*\"]),\n    author='Radim Rehurek',\n    author_email='me@radimrehurek.com',\n    maintainer='Radim Rehurek',\n    maintainer_email='me@radimrehurek.com',\n\n    url='https://github.com/piskvorky/smart_open',\n    download_url='http://pypi.python.org/pypi/smart_open',\n\n    keywords='file streaming, s3, hdfs, gcs, azure blob storage',\n\n    license='MIT',\n    platforms='any',\n\n    install_requires=base_deps,\n    tests_require=tests_require,\n    extras_require={\n        'test': tests_require,\n        's3': aws_deps,\n        'gcs': gcs_deps,\n        'azure': azure_deps,\n        'all': all_deps,\n        'http': http_deps,\n        'webhdfs': http_deps,\n        'ssh': ssh_deps,\n        'zst': zst_deps,\n    },\n    python_requires=\">=3.7,<4.0\",\n\n    test_suite=\"smart_open.tests\",\n\n    classifiers=[\n        'Development Status :: 5 - Production/Stable',\n        'Environment :: Console',\n        'Intended Audience :: Developers',\n        'License :: OSI Approved :: MIT License',\n        'Operating System :: OS Independent',\n        'Programming Language :: Python :: 3.7',\n        'Programming Language :: Python :: 3.8',\n        'Programming Language :: Python :: 3.9',\n        'Programming Language :: Python :: 3.10',\n        'Programming Language :: Python :: 3.11',\n        'Programming Language :: Python :: 3.12',\n        'Programming Language :: Python :: 3.13',\n        'Topic :: System :: Distributed Computing',\n        'Topic :: Database :: Front-Ends',\n    ],\n)\n"
        },
        {
          "name": "smart_open",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}