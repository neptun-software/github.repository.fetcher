{
  "metadata": {
    "timestamp": 1736559783227,
    "page": 503,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjUxMA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "apple/ml-mgie",
      "stars": 3875,
      "defaultBranch": "main",
      "files": [
        {
          "name": ".gitmodules",
          "type": "blob",
          "size": 0.076171875,
          "content": "[submodule \"LLaVA\"]\n\tpath = LLaVA\n\turl = https://github.com/haotian-liu/LLaVA\n"
        },
        {
          "name": "CODE_OF_CONDUCT.md",
          "type": "blob",
          "size": 3.2783203125,
          "content": "# Code of Conduct\n\n## Our Pledge\n\nIn the interest of fostering an open and welcoming environment, we as\ncontributors and maintainers pledge to making participation in our project and\nour community a harassment-free experience for everyone, regardless of age, body\nsize, disability, ethnicity, sex characteristics, gender identity and expression,\nlevel of experience, education, socio-economic status, nationality, personal\nappearance, race, religion, or sexual identity and orientation.\n\n## Our Standards\n\nExamples of behavior that contributes to creating a positive environment\ninclude:\n\n* Using welcoming and inclusive language\n* Being respectful of differing viewpoints and experiences\n* Gracefully accepting constructive criticism\n* Focusing on what is best for the community\n* Showing empathy towards other community members\n\nExamples of unacceptable behavior by participants include:\n\n* The use of sexualized language or imagery and unwelcome sexual attention or\n  advances\n* Trolling, insulting/derogatory comments, and personal or political attacks\n* Public or private harassment\n* Publishing others' private information, such as a physical or electronic\n  address, without explicit permission\n* Other conduct which could reasonably be considered inappropriate in a\n  professional setting\n\n## Our Responsibilities\n\nProject maintainers are responsible for clarifying the standards of acceptable\nbehavior and are expected to take appropriate and fair corrective action in\nresponse to any instances of unacceptable behavior.\n\nProject maintainers have the right and responsibility to remove, edit, or\nreject comments, commits, code, wiki edits, issues, and other contributions\nthat are not aligned to this Code of Conduct, or to ban temporarily or\npermanently any contributor for other behaviors that they deem inappropriate,\nthreatening, offensive, or harmful.\n\n## Scope\n\nThis Code of Conduct applies within all project spaces, and it also applies when\nan individual is representing the project or its community in public spaces.\nExamples of representing a project or community include using an official\nproject e-mail address, posting via an official social media account, or acting\nas an appointed representative at an online or offline event. Representation of\na project may be further defined and clarified by project maintainers.\n\n## Enforcement\n\nInstances of abusive, harassing, or otherwise unacceptable behavior may be\nreported by contacting the open source team at [opensource-conduct@group.apple.com](mailto:opensource-conduct@group.apple.com). All\ncomplaints will be reviewed and investigated and will result in a response that\nis deemed necessary and appropriate to the circumstances. The project team is\nobligated to maintain confidentiality with regard to the reporter of an incident.\nFurther details of specific enforcement policies may be posted separately.\n\nProject maintainers who do not follow or enforce the Code of Conduct in good\nfaith may face temporary or permanent repercussions as determined by other\nmembers of the project's leadership.\n\n## Attribution\n\nThis Code of Conduct is adapted from the [Contributor Covenant](https://www.contributor-covenant.org), version 1.4,\navailable at [https://www.contributor-covenant.org/version/1/4/code-of-conduct.html](https://www.contributor-covenant.org/version/1/4/code-of-conduct.html)"
        },
        {
          "name": "CONTRIBUTING.md",
          "type": "blob",
          "size": 0.73046875,
          "content": "# Contribution Guide\n\nThanks for your interest in contributing. This project was released to accompany a research paper for purposes of reproducibility, and beyond its publication there are limited plans for future development of the repository.\n\nWhile we welcome new pull requests and issues please note that our response may be limited. Forks and out-of-tree improvements are strongly encouraged.\n\n## Before you get started\n\nBy submitting a pull request, you represent that you have the right to license your contribution to Apple and the community, and agree by submitting the patch that your contributions are licensed under the [LICENSE](LICENSE).\n\nWe ask that all community members read and observe our [Code of Conduct](CODE_OF_CONDUCT.md).\n"
        },
        {
          "name": "LICENSE.txt",
          "type": "blob",
          "size": 2.2626953125,
          "content": "Copyright (C) 2024 Apple Inc. All Rights Reserved.\n\nIMPORTANT:  This Apple software is supplied to you by Apple\nInc. (\"Apple\") in consideration of your agreement to the following\nterms, and your use, installation, modification or redistribution of\nthis Apple software constitutes acceptance of these terms.  If you do\nnot agree with these terms, please do not use, install, modify or\nredistribute this Apple software.\n\nIn consideration of your agreement to abide by the following terms, and\nsubject to these terms, Apple grants you a personal, non-exclusive\nlicense, under Apple's copyrights in this original Apple software (the\n\"Apple Software\"), to use, reproduce, modify and redistribute the Apple\nSoftware, with or without modifications, in source and/or binary forms;\nprovided that if you redistribute the Apple Software in its entirety and\nwithout modifications, you must retain this notice and the following\ntext and disclaimers in all such redistributions of the Apple Software.\nNeither the name, trademarks, service marks or logos of Apple Inc. may\nbe used to endorse or promote products derived from the Apple Software\nwithout specific prior written permission from Apple.  Except as\nexpressly stated in this notice, no other rights or licenses, express or\nimplied, are granted by Apple herein, including but not limited to any\npatent rights that may be infringed by your derivative works or by other\nworks in which the Apple Software may be incorporated.\n\nThe Apple Software is provided by Apple on an \"AS IS\" basis.  APPLE\nMAKES NO WARRANTIES, EXPRESS OR IMPLIED, INCLUDING WITHOUT LIMITATION\nTHE IMPLIED WARRANTIES OF NON-INFRINGEMENT, MERCHANTABILITY AND FITNESS\nFOR A PARTICULAR PURPOSE, REGARDING THE APPLE SOFTWARE OR ITS USE AND\nOPERATION ALONE OR IN COMBINATION WITH YOUR PRODUCTS.\n\nIN NO EVENT SHALL APPLE BE LIABLE FOR ANY SPECIAL, INDIRECT, INCIDENTAL\nOR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\nSUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\nINTERRUPTION) ARISING IN ANY WAY OUT OF THE USE, REPRODUCTION,\nMODIFICATION AND/OR DISTRIBUTION OF THE APPLE SOFTWARE, HOWEVER CAUSED\nAND WHETHER UNDER THEORY OF CONTRACT, TORT (INCLUDING NEGLIGENCE),\nSTRICT LIABILITY OR OTHERWISE, EVEN IF APPLE HAS BEEN ADVISED OF THE\nPOSSIBILITY OF SUCH DAMAGE.\n"
        },
        {
          "name": "LLaVA",
          "type": "commit",
          "content": null
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 5.353515625,
          "content": "# Guiding Instruction-based Image Editing via Multimodal Large Language Models\nThis repo contains the code for [Guiding Instruction-based Image Editing via Multimodal Large Language Models](https://arxiv.org/abs/2309.17102) (ICLR'24 Spotlight)\n\n## Overview\nMGIE is an implementation of <br>\n\"[Guiding Instruction-based Image Editing via Multimodal Large Language Models](https://arxiv.org/abs/2309.17102)\" <br>\n[Tsu-Jui Fu](https://scholar.google.com/citations?user=7QRDcC0AAAAJ), [Wenze Hu](https://scholar.google.com/citations?user=0YPYs5UAAAAJ), [Xianzhi Du](https://scholar.google.com/citations?user=l1hP40AAAAAJ), [William Yang Wang](https://scholar.google.com/citations?user=gf8Ms_8AAAAJ), [Yinfei Yang](https://scholar.google.com/citations?user=kvDbu90AAAAJ), and [Zhe Gan](https://scholar.google.com/citations?user=E64XWyMAAAAJ) <br>\nin International Conference on Learning Representations (**ICLR**) 2024\n\n<img src='./mgie.png' width='70%' />\n\nInstruction-based image editing improves the controllability and flexibility of image manipulation via natural commands without elaborate descriptions or regional masks. However, human instructions are sometimes too brief for current methods to capture and follow. Multimodal large language models (MLLMs) show promising capabilities in cross-modal understanding and visual-aware response generation via LMs. We investigate **how MLLMs facilitate edit instructions** and present MLLM-Guided Image Editing (MGIE). MGIE learns to **derive expressive instructions** and provides explicit guidance. The editing model **jointly captures this visual imagination and performs manipulation** through end-to-end training.\n\n## Requirements\n```\nconda create -n mgie python=3.10 -y\nconda activate mgie\nconda update -n base -c defaults conda setuptools -y\nconda install -c conda-forge git git-lfs ffmpeg vim htop ninja gpustat -y\nconda clean -a -y\n\npip install -U pip cmake cython==0.29.36 pydantic==1.10 numpy\npip install -U gdown pydrive2 wget jupyter jupyterlab jupyterthemes ipython\npip install -U sentencepiece transformers diffusers tokenizers datasets gradio==3.37 accelerate evaluate git+https://github.com/openai/CLIP.git\npip install -U https://download.pytorch.org/whl/cu113/torch-1.12.0%2Bcu113-cp310-cp310-linux_x86_64.whl https://download.pytorch.org/whl/cu113/torchvision-0.13.0%2Bcu113-cp310-cp310-linux_x86_64.whl https://download.pytorch.org/whl/cu113/torchaudio-0.12.0%2Bcu113-cp310-cp310-linux_x86_64.whl\npip install -U deepspeed\n\n# git clone this repo\ncd ml-mgie\ngit submodule update --init --recursive\ncd LLaVA\npip install -e .\npip install -U https://download.pytorch.org/whl/cu113/torch-1.12.0%2Bcu113-cp310-cp310-linux_x86_64.whl https://download.pytorch.org/whl/cu113/torchvision-0.13.0%2Bcu113-cp310-cp310-linux_x86_64.whl https://download.pytorch.org/whl/cu113/torchaudio-0.12.0%2Bcu113-cp310-cp310-linux_x86_64.whl\npip install -U ninja flash-attn==1.0.2\npip install -U pydrive2 gdown wget\n\ncd ..\ncp mgie_llava.py LLaVA/llava/model/llava.py\ncp mgie_train.py LLaVA/llava/train/train.py\n```\n\n## Quick Start\nPut official [LLaVA-7B](https://huggingface.co/liuhaotian/LLaVA-Lightning-7B-delta-v1-1) in [_ckpt/LLaVA-7B-v1](_ckpt) and download pre-trained [ckpt](https://docs-assets.developer.apple.com/ml-research/models/mgie/mgie_7b.tar.gz) (on IPr2Pr + MagicBrush) in [_ckpt/mgie_7b](_ckpt)\n```\ndemo.ipynb\n```\n<img src='./demo.png' width='50%' />\n\nNotices: Apple's rights in the attached weight differentials are hereby licensed under the CC-BY-NC license. Apple makes no representations with regards to LLaMa or any other third party software, which are subject to their own terms.\n\n## Usage\n### Data\nDownload CLIP-filtered [IPr2Pr](https://github.com/timothybrooks/instruct-pix2pix) and process (including summarized expressive instruction) in [_data](_data)\n```\nprocess_data.ipynb\n```\nThere are [examples](_data) to help prepare the data\n\n### Train\nPut [Vicuna-7B](https://huggingface.co/lmsys/vicuna-7b-delta-v1.1) and [LLaVA-7B](https://huggingface.co/liuhaotian/LLaVA-Lightning-7B-delta-v1-1) in [_ckpt/vicuna-7b-v1.1](_ckpt) and [_ckpt/LLaVA-7B-v1](_ckpt)\n```\nWANDB_DISABLED='true' torchrun --nnodes=1 --nproc_per_node=8 --master_port=7122 LLaVA/llava/train/train_mem.py --model_name_or_path ./_ckpt/vicuna-7b-v1.1 --version v1 --vision_tower openai/clip-vit-large-patch14 --mm_vision_select_layer -2 --mm_use_im_start_end True --bf16 True --output_dir _snapshot/mgie --num_train_epochs 40 --per_device_train_batch_size 4 --per_device_eval_batch_size 2 --dataloader_num_workers 2 --gradient_accumulation_steps 1 --evaluation_strategy 'no' --save_strategy 'steps' --save_steps 2000 --save_total_limit 10 --learning_rate 5e-4 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type 'cosine' --logging_steps 1 --tf32 True --model_max_length 512 --gradient_checkpointing True --lazy_preprocess True\n```\n\n### Inference\nExtract trained ckpt in [_ckpt/mgie_7b](_ckpt)\n```\nextract_ckpt.ipynb\n```\nRun our demo\n```\ndemo.ipynb\n```\n\n## Citation\n```\n@inproceedings{fu2024mgie,\n  author = {Tsu-Jui Fu and Wenze Hu and Xianzhi Du and William Yang Wang and Yinfei Yang, and Zhe Gan}, \n  title = {{Guiding Instruction-based Image Editing via Multimodal Large Language Models}}, \n  booktitle = {International Conference on Learning Representations (ICLR)}, \n  year = {2024} \n}\n```\n\n## Acknowledgement\n+ [LLaVA](https://github.com/haotian-liu/LLaVA/tree/7ace501183c4bdec6052ec1a30039cdc3242a67c): the codebase we built upon\n"
        },
        {
          "name": "_ckpt",
          "type": "tree",
          "content": null
        },
        {
          "name": "_data",
          "type": "tree",
          "content": null
        },
        {
          "name": "_input",
          "type": "tree",
          "content": null
        },
        {
          "name": "demo.ipynb",
          "type": "blob",
          "size": 7.4208984375,
          "content": "{\n \"cells\": [\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"id\": \"b05c1c61\",\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"#\\n\",\n    \"# For licensing see accompanying LICENSE file.\\n\",\n    \"# Copyright (C) 2024 Apple Inc. All Rights Reserved.\\n\",\n    \"#\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"id\": \"d1c35b9f-06d2-4bc6-a13a-5c68730530c9\",\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"import os\\n\",\n    \"\\n\",\n    \"from tqdm.auto import tqdm\\n\",\n    \"\\n\",\n    \"from PIL import Image\\n\",\n    \"\\n\",\n    \"import torch as T\\n\",\n    \"import transformers, diffusers\\n\",\n    \"\\n\",\n    \"from llava.conversation import conv_templates\\n\",\n    \"from llava.model import *\\n\",\n    \"\\n\",\n    \"def crop_resize(f, sz=512):\\n\",\n    \"    w, h = f.size\\n\",\n    \"    if w>h:\\n\",\n    \"        p = (w-h)//2\\n\",\n    \"        f = f.crop([p, 0, p+h, h])\\n\",\n    \"    elif h>w:\\n\",\n    \"        p = (h-w)//2\\n\",\n    \"        f = f.crop([0, p, w, p+w])\\n\",\n    \"    f = f.resize([sz, sz])\\n\",\n    \"    return f\\n\",\n    \"def remove_alter(s):  # hack expressive instruction\\n\",\n    \"    if 'ASSISTANT:' in s: s = s[s.index('ASSISTANT:')+10:].strip()\\n\",\n    \"    if '</s>' in s: s = s[:s.index('</s>')].strip()\\n\",\n    \"    if 'alternative' in s.lower(): s = s[:s.lower().index('alternative')]\\n\",\n    \"    if '[IMG0]' in s: s = s[:s.index('[IMG0]')]\\n\",\n    \"    s = '.'.join([s.strip() for s in s.split('.')[:2]])\\n\",\n    \"    if s[-1]!='.': s += '.'\\n\",\n    \"    return s.strip()\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"id\": \"df365293-3856-4c98-87dc-3569cab81700\",\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"DEFAULT_IMAGE_TOKEN = '<image>'\\n\",\n    \"DEFAULT_IMAGE_PATCH_TOKEN = '<im_patch>'\\n\",\n    \"DEFAULT_IM_START_TOKEN = '<im_start>'\\n\",\n    \"DEFAULT_IM_END_TOKEN = '<im_end>'\\n\",\n    \"PATH_LLAVA = './_ckpt/LLaVA-7B-v1'\\n\",\n    \"\\n\",\n    \"tokenizer = transformers.AutoTokenizer.from_pretrained(PATH_LLAVA)\\n\",\n    \"model = LlavaLlamaForCausalLM.from_pretrained(PATH_LLAVA, low_cpu_mem_usage=True, torch_dtype=T.float16, use_cache=True).cuda()\\n\",\n    \"image_processor = transformers.CLIPImageProcessor.from_pretrained(model.config.mm_vision_tower, torch_dtype=T.float16)\\n\",\n    \"\\n\",\n    \"tokenizer.padding_side = 'left'\\n\",\n    \"tokenizer.add_tokens(['[IMG0]', '[IMG1]', '[IMG2]', '[IMG3]', '[IMG4]', '[IMG5]', '[IMG6]', '[IMG7]'], special_tokens=True)\\n\",\n    \"model.resize_token_embeddings(len(tokenizer))\\n\",\n    \"ckpt = T.load('./_ckpt/mgie_7b/mllm.pt', map_location='cpu')\\n\",\n    \"model.load_state_dict(ckpt, strict=False)\\n\",\n    \"\\n\",\n    \"mm_use_im_start_end = getattr(model.config, 'mm_use_im_start_end', False)\\n\",\n    \"tokenizer.add_tokens([DEFAULT_IMAGE_PATCH_TOKEN], special_tokens=True)\\n\",\n    \"if mm_use_im_start_end: tokenizer.add_tokens([DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN], special_tokens=True)\\n\",\n    \"\\n\",\n    \"vision_tower = model.get_model().vision_tower[0]\\n\",\n    \"vision_tower = transformers.CLIPVisionModel.from_pretrained(vision_tower.config._name_or_path, torch_dtype=T.float16, low_cpu_mem_usage=True).cuda()\\n\",\n    \"model.get_model().vision_tower[0] = vision_tower\\n\",\n    \"vision_config = vision_tower.config\\n\",\n    \"vision_config.im_patch_token = tokenizer.convert_tokens_to_ids([DEFAULT_IMAGE_PATCH_TOKEN])[0]\\n\",\n    \"vision_config.use_im_start_end = mm_use_im_start_end\\n\",\n    \"if mm_use_im_start_end: vision_config.im_start_token, vision_config.im_end_token = tokenizer.convert_tokens_to_ids([DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN])\\n\",\n    \"image_token_len = (vision_config.image_size//vision_config.patch_size)**2\\n\",\n    \"\\n\",\n    \"_ = model.eval()\\n\",\n    \"EMB = ckpt['emb'].cuda()\\n\",\n    \"with T.inference_mode(): NULL = model.edit_head(T.zeros(1, 8, 4096).half().to('cuda'), EMB)\\n\",\n    \"print('NULL:', NULL.shape)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"id\": \"86c7224e-ac5b-4395-8e92-908aa4949b7f\",\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"pipe = diffusers.StableDiffusionInstructPix2PixPipeline.from_pretrained('timbrooks/instruct-pix2pix', torch_dtype=T.float16, safety_checker=None).to('cuda')\\n\",\n    \"pipe.set_progress_bar_config(disable=True)\\n\",\n    \"pipe.unet.load_state_dict(T.load('./_ckpt/mgie_7b/unet.pt', map_location='cpu'))\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"id\": \"bb920c1a-83fa-4746-a74a-0c72566acb73\",\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"SEED = 13331\\n\",\n    \"\\n\",\n    \"ins = ['make the frame red', 'turn the day into night', 'give him a beard', 'make cottage a mansion', \\n\",\n    \"       'remove yellow object from dogs paws', 'change the hair from red to blue', 'remove the text', 'increase the image contrast', \\n\",\n    \"       'remove the people in the background', 'please make this photo professional looking', 'darken the image, sharpen it', 'photoshop the girl out', \\n\",\n    \"       'make more brightness', 'take away the brown filter form the image', 'add more contrast to simulate more light', 'dark on rgb', \\n\",\n    \"       'make the face happy', 'change view as ocean', 'replace basketball with soccer ball', 'let the floor be made of wood']\\n\",\n    \"for i in tqdm(range(len(ins))):\\n\",\n    \"    img, txt = Image.open('_input/%d.jpg'%(i)).convert('RGB'), ins[i]\\n\",\n    \"    \\n\",\n    \"    img = image_processor.preprocess(img, return_tensors='pt')['pixel_values'][0]\\n\",\n    \"    txt = \\\"what will this image be like if '%s'\\\"%(txt)\\n\",\n    \"    txt = txt+'\\\\n'+DEFAULT_IM_START_TOKEN+DEFAULT_IMAGE_PATCH_TOKEN*image_token_len+DEFAULT_IM_END_TOKEN\\n\",\n    \"    conv = conv_templates['vicuna_v1_1'].copy()\\n\",\n    \"    conv.append_message(conv.roles[0], txt), conv.append_message(conv.roles[1], None)\\n\",\n    \"    txt = conv.get_prompt()\\n\",\n    \"    txt = tokenizer(txt)\\n\",\n    \"    txt, mask = T.as_tensor(txt['input_ids']), T.as_tensor(txt['attention_mask'])\\n\",\n    \"    \\n\",\n    \"    with T.inference_mode():\\n\",\n    \"        out = model.generate(txt.unsqueeze(dim=0).cuda(), images=img.half().unsqueeze(dim=0).cuda(), attention_mask=mask.unsqueeze(dim=0).cuda(), \\n\",\n    \"                             do_sample=False, max_new_tokens=96, num_beams=1, no_repeat_ngram_size=3, \\n\",\n    \"                             return_dict_in_generate=True, output_hidden_states=True)\\n\",\n    \"        out, hid = out['sequences'][0].tolist(), T.cat([x[-1] for x in out['hidden_states']], dim=1)[0]\\n\",\n    \"        \\n\",\n    \"        p = min(out.index(32003)-1 if 32003 in out else len(hid)-9, len(hid)-9)\\n\",\n    \"        hid = hid[p:p+8]\\n\",\n    \"\\n\",\n    \"        out = remove_alter(tokenizer.decode(out))\\n\",\n    \"        emb = model.edit_head(hid.unsqueeze(dim=0), EMB)\\n\",\n    \"        res = pipe(image=Image.open('_input/%d.jpg'%(i)).convert('RGB'), prompt_embeds=emb, negative_prompt_embeds=NULL, generator=T.Generator(device='cuda').manual_seed(SEED)).images[0]\\n\",\n    \"    \\n\",\n    \"    display(Image.open('_input/%d.jpg'%(i)).convert('RGB')), print(ins[i])\\n\",\n    \"    print('\\\\n')\\n\",\n    \"    print(out), display(res)\\n\",\n    \"    print('\\\\n\\\\n')\"\n   ]\n  }\n ],\n \"metadata\": {\n  \"kernelspec\": {\n   \"display_name\": \"Python 3 (ipykernel)\",\n   \"language\": \"python\",\n   \"name\": \"python3\"\n  },\n  \"language_info\": {\n   \"codemirror_mode\": {\n    \"name\": \"ipython\",\n    \"version\": 3\n   },\n   \"file_extension\": \".py\",\n   \"mimetype\": \"text/x-python\",\n   \"name\": \"python\",\n   \"nbconvert_exporter\": \"python\",\n   \"pygments_lexer\": \"ipython3\",\n   \"version\": \"3.10.13\"\n  }\n },\n \"nbformat\": 4,\n \"nbformat_minor\": 5\n}\n"
        },
        {
          "name": "demo.png",
          "type": "blob",
          "size": 918.904296875,
          "content": null
        },
        {
          "name": "extract_ckpt.ipynb",
          "type": "blob",
          "size": 1.611328125,
          "content": "{\n \"cells\": [\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"id\": \"f9dab9f9\",\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"#\\n\",\n    \"# For licensing see accompanying LICENSE file.\\n\",\n    \"# Copyright (C) 2024 Apple Inc. All Rights Reserved.\\n\",\n    \"#\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"id\": \"89151fde-b3c9-4265-892f-dffdccfb0494\",\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"import torch as T\\n\",\n    \"\\n\",\n    \"pt = {}\\n\",\n    \"for i in range(4): pt.update(T.load('./_snapshot/mgie/checkpoint/pytorch_model-0000%d-of-00004.bin'%(i+1), map_location='cpu'))\\n\",\n    \"print(len(pt))\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"id\": \"22503532-f4c3-4751-be4f-3250069aa179\",\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"mllm, unet = {}, {}\\n\",\n    \"for key in pt:\\n\",\n    \"    if 'embed_tokens' in key or 'lm_head' in key or 'edit_head' in key: mllm[key] = pt[key].half()\\n\",\n    \"    elif 'unet' in key: unet[key.replace('unet.', '')] = pt[key].half()\\n\",\n    \"print(len(mllm)), print(len(unet))\\n\",\n    \"T.save(mllm, './_ckpt/mgie_7b/mllm.pt'), T.save(unet, './_ckpt/mgie_7b/unet.pt')\"\n   ]\n  }\n ],\n \"metadata\": {\n  \"kernelspec\": {\n   \"display_name\": \"Python 3 (ipykernel)\",\n   \"language\": \"python\",\n   \"name\": \"python3\"\n  },\n  \"language_info\": {\n   \"codemirror_mode\": {\n    \"name\": \"ipython\",\n    \"version\": 3\n   },\n   \"file_extension\": \".py\",\n   \"mimetype\": \"text/x-python\",\n   \"name\": \"python\",\n   \"nbconvert_exporter\": \"python\",\n   \"pygments_lexer\": \"ipython3\",\n   \"version\": \"3.10.13\"\n  }\n },\n \"nbformat\": 4,\n \"nbformat_minor\": 5\n}\n"
        },
        {
          "name": "mgie.png",
          "type": "blob",
          "size": 741.015625,
          "content": null
        },
        {
          "name": "mgie_llava.py",
          "type": "blob",
          "size": 19.8916015625,
          "content": "#\n# For licensing see accompanying LICENSE file.\n# Copyright (C) 2024 Apple Inc. All Rights Reserved.\n#\n# modified from https://github.com/haotian-liu/LLaVA/blob/7ace501183c4bdec6052ec1a30039cdc3242a67c/llava/model/llava.py\n\nfrom typing import List, Optional, Tuple, Union\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.nn import CrossEntropyLoss\n\nfrom transformers import AutoConfig, AutoModelForCausalLM, \\\n                         LlamaConfig, LlamaModel, LlamaForCausalLM, \\\n                         CLIPVisionModel, CLIPImageProcessor\n\nfrom transformers.modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast\n\nimport os, diffusers\n\nDEFAULT_IMAGE_TOKEN = \"<image>\"\nDEFAULT_IMAGE_PATCH_TOKEN = \"<im_patch>\"\nDEFAULT_IM_START_TOKEN = \"<im_start>\"\nDEFAULT_IM_END_TOKEN = \"<im_end>\"\n\n\nclass LlavaConfig(LlamaConfig):\n    model_type = \"llava\"\n\n\nclass LlavaLlamaModel(LlamaModel):\n    config_class = LlavaConfig\n\n    def __init__(self, config: LlamaConfig):\n        super(LlavaLlamaModel, self).__init__(config)\n\n        if hasattr(config, \"mm_vision_tower\"):\n            # HACK: for FSDP\n            self.vision_tower = [CLIPVisionModel.from_pretrained(config.mm_vision_tower)]\n            # self.vision_tower = CLIPVisionModel.from_pretrained(config.mm_vision_tower)\n\n        if hasattr(config, \"use_mm_proj\"):\n            self.mm_projector = nn.Linear(config.mm_hidden_size, config.hidden_size)\n\n    def get_vision_tower(self):\n        vision_tower = getattr(self, 'vision_tower', None)\n        if type(vision_tower) is list:\n            vision_tower = vision_tower[0]\n        return vision_tower\n\n    def initialize_vision_modules(self, vision_tower, mm_vision_select_layer,\n                                  pretrain_mm_mlp_adapter=None, fsdp=None):\n        self.config.mm_vision_tower = vision_tower\n\n        image_processor = CLIPImageProcessor.from_pretrained(vision_tower)\n\n        if not hasattr(self, 'vision_tower'):\n            vision_tower = CLIPVisionModel.from_pretrained(vision_tower)\n        else:\n            vision_tower = self.vision_tower[0]\n        vision_tower.requires_grad_(False)\n\n        if fsdp is not None and len(fsdp) > 0:\n            self.vision_tower = [vision_tower]\n        else:\n            self.vision_tower = vision_tower\n\n        vision_config = vision_tower.config\n        num_patches = (vision_config.image_size // vision_config.patch_size) ** 2\n\n        self.config.use_mm_proj = True\n        self.config.mm_hidden_size = vision_config.hidden_size\n        self.config.mm_vision_select_layer = mm_vision_select_layer\n\n        if not hasattr(self, 'mm_projector'):\n            self.mm_projector = nn.Linear(vision_config.hidden_size, self.config.hidden_size)\n\n        if pretrain_mm_mlp_adapter is not None:\n            mm_projector_weights = torch.load(pretrain_mm_mlp_adapter, map_location='cpu')\n            self.mm_projector.load_state_dict({k.split('.')[-1]: v for k, v in mm_projector_weights.items()})\n\n        return dict(\n            image_processor=image_processor,\n            image_token_len=num_patches,\n            vision_config=vision_config\n        )\n\n    def forward(\n        self,\n        input_ids: torch.LongTensor = None,\n        attention_mask: Optional[torch.Tensor] = None,\n        past_key_values: Optional[List[torch.FloatTensor]] = None,\n        inputs_embeds: Optional[torch.FloatTensor] = None,\n        use_cache: Optional[bool] = None,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        images: Optional[torch.FloatTensor] = None,\n        return_dict: Optional[bool] = None,\n    ) -> Union[Tuple, BaseModelOutputWithPast]:\n\n        # HACK: replace back original embeddings for LLaVA pretraining\n        orig_embeds_params = getattr(self, 'orig_embeds_params', None)\n        # if orig_embeds_params is not None:\n        #     orig_embeds_params = orig_embeds_params[0]\n        #     with torch.no_grad():\n        #         self.get_input_embeddings().weight.data[:-2] = orig_embeds_params[:-2].data\n\n        if inputs_embeds is None:\n            inputs_embeds = self.embed_tokens(input_ids)\n\n        vision_tower = self.get_vision_tower()\n        if vision_tower is not None and (input_ids.shape[1] != 1 or self.training) and images is not None:\n            # TODO: this is a modified multimodal LLM -- Haotian Liu\n            with torch.no_grad():\n                if type(images) is list:\n                    # variable length images\n                    image_features = []\n                    for image in images:\n                        image_forward_out = vision_tower(image.unsqueeze(0), output_hidden_states=True)\n                        select_hidden_state_layer = getattr(self.config, \"mm_vision_select_layer\", -1)\n                        select_hidden_state = image_forward_out.hidden_states[select_hidden_state_layer]\n                        image_feature = select_hidden_state[:, 1:]\n                        image_features.append(image_feature)\n                else:\n                    image_forward_outs = vision_tower(images.to(vision_tower.dtype), output_hidden_states=True)\n                    select_hidden_state_layer = getattr(self.config, \"mm_vision_select_layer\", -1)\n                    select_hidden_state = image_forward_outs.hidden_states[select_hidden_state_layer]\n                    image_features = select_hidden_state[:, 1:].to(images.dtype)\n            if type(images) is list:\n                image_features = [self.mm_projector(image_feature)[0] for image_feature in image_features]\n            else:\n                image_features = self.mm_projector(image_features)\n            dummy_image_features = torch.zeros(256, 1024, device=inputs_embeds.device, dtype=inputs_embeds.dtype)\n            dummy_image_features = self.mm_projector(dummy_image_features)\n\n            new_input_embeds = []\n            cur_image_idx = 0\n            for cur_input_ids, cur_input_embeds in zip(input_ids, inputs_embeds):\n                if (cur_input_ids == vision_tower.config.im_patch_token).sum() == 0:\n                    # multimodal LLM, but the current sample is not multimodal\n                    cur_input_embeds = cur_input_embeds + (0. * dummy_image_features).sum()\n                    new_input_embeds.append(cur_input_embeds)\n                    cur_image_idx += 1\n                    continue\n                if vision_tower.config.use_im_start_end:\n                    cur_image_features = image_features[cur_image_idx]\n                    num_patches = cur_image_features.shape[0]\n                    if (cur_input_ids == vision_tower.config.im_start_token).sum() != (cur_input_ids == vision_tower.config.im_end_token).sum():\n                        raise ValueError(\"The number of image start tokens and image end tokens should be the same.\")\n                    image_start_tokens = torch.where(cur_input_ids == vision_tower.config.im_start_token)[0]\n                    for image_start_token_pos in image_start_tokens:\n                        cur_image_features = image_features[cur_image_idx].to(device=cur_input_embeds.device)\n                        num_patches = cur_image_features.shape[0]\n                        if cur_input_ids[image_start_token_pos + num_patches + 1] != vision_tower.config.im_end_token:\n                            raise ValueError(\"The image end token should follow the image start token.\")\n                        if orig_embeds_params is not None:\n                            cur_new_input_embeds = torch.cat((cur_input_embeds[:image_start_token_pos].detach(), cur_input_embeds[image_start_token_pos:image_start_token_pos+1], cur_image_features, cur_input_embeds[image_start_token_pos + num_patches + 1:image_start_token_pos + num_patches + 2], cur_input_embeds[image_start_token_pos + num_patches + 2:].detach()), dim=0)\n                        else:\n                            cur_new_input_embeds = torch.cat((cur_input_embeds[:image_start_token_pos+1], cur_image_features, cur_input_embeds[image_start_token_pos + num_patches + 1:]), dim=0)\n                        cur_image_idx += 1\n                    new_input_embeds.append(cur_new_input_embeds)\n                else:\n                    cur_image_features = image_features[cur_image_idx]\n                    num_patches = cur_image_features.shape[0]\n                    if (cur_input_ids == vision_tower.config.im_patch_token).sum() != num_patches:\n                        raise ValueError(\"The number of image patch tokens should be the same as the number of image patches.\")\n                    masked_indices = torch.where(cur_input_ids == vision_tower.config.im_patch_token)[0]\n                    mask_index_start = masked_indices[0]\n                    if (masked_indices != torch.arange(mask_index_start, mask_index_start+num_patches, device=masked_indices.device, dtype=masked_indices.dtype)).any():\n                        raise ValueError(\"The image patch tokens should be consecutive.\")\n                    if orig_embeds_params is not None:\n                        cur_new_input_embeds = torch.cat((cur_input_embeds[:mask_index_start].detach(), cur_image_features, cur_input_embeds[mask_index_start+num_patches:].detach()), dim=0)\n                    else:\n                        cur_new_input_embeds = torch.cat((cur_input_embeds[:mask_index_start], cur_image_features, cur_input_embeds[mask_index_start+num_patches:]), dim=0)\n                    new_input_embeds.append(cur_new_input_embeds)\n                    cur_image_idx += 1\n            inputs_embeds = torch.stack(new_input_embeds, dim=0)\n\n        return super(LlavaLlamaModel, self).forward(\n            input_ids=None, attention_mask=attention_mask, past_key_values=past_key_values,\n            inputs_embeds=inputs_embeds, use_cache=use_cache,\n            output_attentions=output_attentions, output_hidden_states=output_hidden_states,\n            return_dict=return_dict\n        )\n\nclass EditMapper(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n        self.llm2hid = nn.Linear(4096, 512)\n        self.query = nn.Parameter(torch.randn(1, 77, 512))\n        self.mapper = nn.Transformer(batch_first=True, norm_first=True,\n                                     d_model=512, nhead=4, num_encoder_layers=4, num_decoder_layers=4,\n                                     dim_feedforward=2048, dropout=0.0)\n        self.hid2feat = nn.Linear(512, 768)\n\n    def forward(self, llm, emb):\n        hid = self.llm2hid(llm+emb)\n        hid = self.mapper(hid, self.query.repeat(llm.shape[0], 1, 1))\n        feat = self.hid2feat(hid)\n\n        return feat\n\nclass LlavaLlamaForCausalLM(LlamaForCausalLM):\n    config_class = LlavaConfig\n\n    def __init__(self, config):\n        super(LlamaForCausalLM, self).__init__(config)\n        self.model = LlavaLlamaModel(config)\n\n        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n\n        self.edit_head = EditMapper()\n\n        self.scheduler, self.vae, self.unet = [diffusers.DDPMScheduler.from_pretrained('runwayml/stable-diffusion-v1-5', subfolder='scheduler'),\n                                               diffusers.AutoencoderKL.from_pretrained('runwayml/stable-diffusion-v1-5', subfolder='vae'),\n                                               diffusers.UNet2DConditionModel.from_pretrained('runwayml/stable-diffusion-v1-5', subfolder='unet')]\n        self.vae.requires_grad_(False)\n        self.unet.register_to_config(in_channels=8)\n        with torch.no_grad():\n            conv = torch.nn.Conv2d(8, self.unet.conv_in.out_channels, self.unet.conv_in.kernel_size, self.unet.conv_in.stride, self.unet.conv_in.padding)\n            conv.weight.zero_()\n            conv.weight[:, :4, :, :].copy_(self.unet.conv_in.weight)\n            self.unet.conv_in = conv\n\n        # Initialize weights and apply final processing\n        self.post_init()\n\n    def get_model(self):\n        return self.model\n\n    def get_vision_tower(self):\n        return self.get_model().get_vision_tower()\n\n    def get_vision_tower(self):\n        model = self.get_model()\n        vision_tower = model.vision_tower\n        if type(vision_tower) is list:\n            vision_tower = vision_tower[0]\n        return vision_tower\n\n    def forward(\n        self,\n        input_ids: torch.LongTensor = None,\n        attention_mask: Optional[torch.Tensor] = None,\n        past_key_values: Optional[List[torch.FloatTensor]] = None,\n        inputs_embeds: Optional[torch.FloatTensor] = None,\n        labels: Optional[torch.LongTensor] = None,\n        use_cache: Optional[bool] = None,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        images: Optional[torch.FloatTensor] = None,\n        return_dict: Optional[bool] = None,\n        p2p_inp=None, p2p_ans=None\n    ) -> Union[Tuple, CausalLMOutputWithPast]:\n        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n        output_hidden_states = (\n            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n        )\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n        # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\n        outputs = self.model(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            past_key_values=past_key_values,\n            inputs_embeds=inputs_embeds,\n            use_cache=use_cache,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n            images=images\n        )\n\n        hidden_states = outputs[0]\n        logits = self.lm_head(hidden_states)\n\n        loss = None\n        if labels is not None:\n            # Shift so that tokens < n predict n\n            shift_logits = logits[..., :-1, :].contiguous()\n            shift_labels = labels[..., 1:].contiguous()\n            # Flatten the tokens\n            loss_fct = CrossEntropyLoss()\n            shift_logits = shift_logits.view(-1, self.config.vocab_size)\n            shift_labels = shift_labels.view(-1)\n            # Enable model/pipeline parallelism\n            shift_labels = shift_labels.to(shift_logits.device)\n            loss = loss_fct(shift_logits, shift_labels)\n\n        if labels is not None:\n            llm = []\n            for i in range(labels.shape[0]):\n                try: p = labels[i].data.cpu().tolist().index(32003)-1\n                except: p = len(labels[i])-9\n                p = min(len(hidden_states[i])-9, p)\n                llm.append(hidden_states[i][p:p+8].unsqueeze(0))\n            llm = torch.cat(llm, dim=0)\n            hid_edit = self.edit_head(llm, self.model.embed_tokens.weight[-8:].unsqueeze(dim=0).repeat(labels.shape[0], 1, 1))\n\n            B, DROP = labels.shape[0], 0.05\n\n            hid_null = self.edit_head(torch.zeros(B, 8, 4096, device=labels.device),\n                                      self.model.embed_tokens.weight[-8:].unsqueeze(dim=0).repeat(labels.shape[0], 1, 1))\n\n            with torch.no_grad():\n                lat_ans, lat_inp = self.vae.encode(p2p_ans).latent_dist.sample()*self.vae.config.scaling_factor, self.vae.encode(p2p_inp).latent_dist.mode()\n                lat_ans, lat_inp = [torch.from_numpy(lat_ans.data.cpu().float().numpy()).to(lat_ans.device),\n                                    torch.from_numpy(lat_inp.data.cpu().float().numpy()).to(lat_inp.device)]\n\n            noise = torch.randn_like(lat_ans)\n            ts = torch.randint(0, self.scheduler.config.num_train_timesteps, (B, ), device=noise.device).long()\n            lat_noise = self.scheduler.add_noise(lat_ans, noise, ts)\n\n            prob = torch.rand(B, device=lat_ans.device)\n            mask = (prob<(DROP*2)).reshape(B, 1, 1)\n            hid_edit = torch.where(mask, hid_null, hid_edit)\n            mask = (1.0-((prob>=DROP).to(lat_inp.dtype)*(prob<(DROP*3)).to(lat_inp.dtype))).reshape(B, 1, 1, 1)\n            lat_inp *= mask\n\n            out = self.unet(torch.cat([lat_noise, lat_inp], dim=1), ts, hid_edit).sample\n\n            loss_ce, loss_edit = loss, nn.functional.mse_loss(out, noise, reduction='mean')\n            if int(os.environ['LOCAL_RANK'])==0: print('loss_ce:', loss_ce, '/', 'loss_edit:', loss_edit)\n            loss = loss_ce+loss_edit*0.5\n\n        if not return_dict:\n            output = (logits,) + outputs[1:]\n            return (loss,) + output if loss is not None else output\n\n        return CausalLMOutputWithPast(\n            loss=loss,\n            logits=logits,\n            past_key_values=outputs.past_key_values,\n            hidden_states=outputs.hidden_states,\n            attentions=outputs.attentions,\n        )\n\n    def prepare_inputs_for_generation(\n        self, input_ids, past_key_values=None, attention_mask=None, inputs_embeds=None, **kwargs\n    ):\n        if past_key_values:\n            input_ids = input_ids[:, -1:]\n\n        # if `inputs_embeds` are passed, we only want to use them in the 1st generation step\n        if inputs_embeds is not None and past_key_values is None:\n            model_inputs = {\"inputs_embeds\": inputs_embeds}\n        else:\n            model_inputs = {\"input_ids\": input_ids}\n\n        model_inputs.update(\n            {\n                \"past_key_values\": past_key_values,\n                \"use_cache\": kwargs.get(\"use_cache\"),\n                \"attention_mask\": attention_mask,\n                \"images\": kwargs.get(\"images\", None),\n            }\n        )\n        return model_inputs\n\n    def initialize_vision_tokenizer(self, mm_use_im_start_end, tokenizer, device,\n                                    tune_mm_mlp_adapter=False, pretrain_mm_mlp_adapter=None):\n        vision_config = self.get_vision_tower().config\n        vision_config.use_im_start_end = mm_use_im_start_end\n        tokenizer.add_tokens([DEFAULT_IMAGE_PATCH_TOKEN], special_tokens=True)\n        self.resize_token_embeddings(len(tokenizer))\n\n        if mm_use_im_start_end:\n            num_new_tokens = tokenizer.add_tokens([DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN], special_tokens=True)\n            self.resize_token_embeddings(len(tokenizer))\n            vision_config.im_start_token, vision_config.im_end_token = tokenizer.convert_tokens_to_ids([DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN])\n\n            if num_new_tokens > 0:\n                input_embeddings = self.get_input_embeddings().weight.data\n                output_embeddings = self.get_output_embeddings().weight.data\n\n                input_embeddings_avg = input_embeddings[:-num_new_tokens].mean(\n                    dim=0, keepdim=True)\n                output_embeddings_avg = output_embeddings[:-num_new_tokens].mean(\n                    dim=0, keepdim=True)\n\n                input_embeddings[-num_new_tokens:] = input_embeddings_avg\n                output_embeddings[-num_new_tokens:] = output_embeddings_avg\n\n            if tune_mm_mlp_adapter:\n                self.get_model().orig_embeds_params = [self.get_input_embeddings().weight.data.clone().to(device=device)]\n                for p in self.get_input_embeddings().parameters():\n                    p.requires_grad = True\n                for p in self.get_output_embeddings().parameters():\n                    p.requires_grad = False\n\n            if pretrain_mm_mlp_adapter:\n                mm_projector_weights = torch.load(pretrain_mm_mlp_adapter, map_location='cpu')\n                embed_tokens_weight = mm_projector_weights['model.embed_tokens.weight']\n                assert num_new_tokens == 2\n                if input_embeddings.shape == embed_tokens_weight.shape:\n                    input_embeddings[-num_new_tokens:] = embed_tokens_weight[-num_new_tokens:]\n                elif embed_tokens_weight.shape[0] == num_new_tokens:\n                    input_embeddings[-num_new_tokens:] = embed_tokens_weight\n                else:\n                    raise ValueError(f\"Unexpected embed_tokens_weight shape. Pretrained: {embed_tokens_weight.shape}. Current: {input_embeddings.shape}. Numer of new tokens: {num_new_tokens}.\")\n\n        vision_config.im_patch_token = tokenizer.convert_tokens_to_ids([DEFAULT_IMAGE_PATCH_TOKEN])[0]\n\nAutoConfig.register(\"llava\", LlavaConfig)\nAutoModelForCausalLM.register(LlavaConfig, LlavaLlamaForCausalLM)\n"
        },
        {
          "name": "mgie_train.py",
          "type": "blob",
          "size": 32.3525390625,
          "content": "#\n# For licensing see accompanying LICENSE file.\n# Copyright (C) 2024 Apple Inc. All Rights Reserved.\n#\n# modified from https://github.com/haotian-liu/LLaVA/blob/7ace501183c4bdec6052ec1a30039cdc3242a67c/llava/train/train.py\n\nimport os\nimport copy\nfrom dataclasses import dataclass, field\nimport json\nimport logging\nimport pathlib\nfrom typing import Dict, Optional, Sequence, List\n\nimport torch\n\nimport transformers\nfrom torch.utils.data import Dataset\nfrom llava.train.llava_trainer import LLaVATrainer\n\nfrom llava import conversation as conversation_lib\nfrom llava.model import *\n\nfrom PIL import Image\nimport torch.nn as nn\n\n# TODO: import and use code from ../data/dataset.py\n\nIGNORE_INDEX = -100\nDEFAULT_PAD_TOKEN = \"[PAD]\"\nDEFAULT_EOS_TOKEN = \"</s>\"\nDEFAULT_BOS_TOKEN = \"<s>\"\nDEFAULT_UNK_TOKEN = \"<unk>\"\nDEFAULT_IMAGE_TOKEN = \"<image>\"\nDEFAULT_IMAGE_PATCH_TOKEN = \"<im_patch>\"\nDEFAULT_IM_START_TOKEN = \"<im_start>\"\nDEFAULT_IM_END_TOKEN = \"<im_end>\"\n\nimport io, base64, pickle, random\nfrom tqdm import tqdm\nimport numpy as np\n\ndef b2f(b): return Image.open(io.BytesIO(base64.b64decode(b))).convert('RGB')\ndef resize(f):\n    w, h = f.size\n    if w>h:\n        p = (w-h)//2\n        f = f.crop([p, 0, p+h, h])\n    elif h>w:\n        p = (h-w)//2\n        f = f.crop([0, p, w, p+w])\n    f = f.resize([512, 512])\n    return f\ndef img2npy(f): return (2.0*np.array(f)/255.0-1.0).transpose((2, 0, 1)).astype(np.float32)\n\n@dataclass\nclass ModelArguments:\n    model_name_or_path: Optional[str] = field(default=\"facebook/opt-125m\")\n    version: Optional[str] = field(default=\"v0\")\n    freeze_backbone: bool = field(default=False)\n    tune_mm_mlp_adapter: bool = field(default=False)\n    vision_tower: Optional[str] = field(default=None)\n    mm_vision_select_layer: Optional[int] = field(default=-1)   # default to the last layer\n    pretrain_mm_mlp_adapter: Optional[str] = field(default=None)\n    mm_use_im_start_end: bool = field(default=False)\n\n\n@dataclass\nclass DataArguments:\n    data_path: str = field(default=None,\n                           metadata={\"help\": \"Path to the training data.\"})\n    lazy_preprocess: bool = False\n    is_multimodal: bool = False\n    sep_image_conv_front: bool = False\n    image_token_len: int = 0\n    image_folder: Optional[str] = field(default=None)\n    image_aspect_ratio: str = 'square'\n\n\n@dataclass\nclass TrainingArguments(transformers.TrainingArguments):\n    cache_dir: Optional[str] = field(default=None)\n    optim: str = field(default=\"adamw_torch\")\n    remove_unused_columns: bool = field(default=False)\n    freeze_mm_mlp_adapter: bool = field(default=False)\n    force_fsdp: bool = field(default=False)\n    model_max_length: int = field(\n        default=512,\n        metadata={\n            \"help\":\n            \"Maximum sequence length. Sequences will be right padded (and possibly truncated).\"\n        },\n    )\n    double_quant: bool = field(\n        default=True,\n        metadata={\"help\": \"Compress the quantization statistics through double quantization.\"}\n    )\n    quant_type: str = field(\n        default=\"nf4\",\n        metadata={\"help\": \"Quantization data type to use. Should be one of `fp4` or `nf4`.\"}\n    )\n    bits: int = field(\n        default=16,\n        metadata={\"help\": \"How many bits to use.\"}\n    )\n    lora_enable: bool = False\n    lora_r: int = 64\n    lora_alpha: int = 16\n    lora_dropout: float = 0.05\n    lora_weight_path: str = \"\"\n    lora_bias: str = \"none\"\n\n\ndef maybe_zero_3(param, ignore_status=False, name=None):\n    from deepspeed import zero\n    from deepspeed.runtime.zero.partition_parameters import ZeroParamStatus\n    if hasattr(param, \"ds_id\"):\n        if param.ds_status == ZeroParamStatus.NOT_AVAILABLE:\n            if not ignore_status:\n                logging.warning(f\"{name}: param.ds_status != ZeroParamStatus.NOT_AVAILABLE: {param.ds_status}\")\n        with zero.GatheredParameters([param]):\n            param = param.data.detach().cpu().clone()\n    else:\n        param = param.detach().cpu().clone()\n    return param\n\n\n# Borrowed from peft.utils.get_peft_model_state_dict\ndef get_peft_state_maybe_zero_3(named_params, bias):\n    if bias == \"none\":\n        to_return = {k: t for k, t in named_params if \"lora_\" in k}\n    elif bias == \"all\":\n        to_return = {k: t for k, t in named_params if \"lora_\" in k or \"bias\" in k}\n    elif bias == \"lora_only\":\n        to_return = {}\n        maybe_lora_bias = {}\n        lora_bias_names = set()\n        for k, t in named_params:\n            if \"lora_\" in k:\n                to_return[k] = t\n                bias_name = k.split(\"lora_\")[0] + \"bias\"\n                lora_bias_names.add(bias_name)\n            elif \"bias\" in k:\n                maybe_lora_bias[k] = t\n        for k, t in maybe_lora_bias:\n            if bias_name in lora_bias_names:\n                to_return[bias_name] = t\n    else:\n        raise NotImplementedError\n    to_return = {k: maybe_zero_3(v, name=k) for k, v in to_return.items()}\n    return to_return\n\n\ndef get_peft_state_non_lora_maybe_zero_3(named_params, require_grad_only=True):\n    to_return = {k: t for k, t in named_params if \"lora_\" not in k}\n    if require_grad_only:\n        to_return = {k: t for k, t in to_return.items() if t.requires_grad}\n    to_return = {k: maybe_zero_3(v, ignore_status=True).cpu() for k, v in to_return.items()}\n    return to_return\n\n\ndef find_all_linear_names(model):\n    cls = torch.nn.Linear\n    lora_module_names = set()\n    for name, module in model.named_modules():\n        if isinstance(module, cls):\n            names = name.split('.')\n            lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n\n\n    if 'lm_head' in lora_module_names: # needed for 16-bit\n        lora_module_names.remove('lm_head')\n    return list(lora_module_names)\n\n\ndef safe_save_model_for_hf_trainer(trainer: transformers.Trainer,\n                                   output_dir: str):\n    \"\"\"Collects the state dict and dump to disk.\"\"\"\n    if trainer.deepspeed:\n        torch.cuda.synchronize()\n        trainer.save_model(output_dir)\n        return\n\n    state_dict = trainer.model.state_dict()\n    if trainer.args.should_save:\n        cpu_state_dict = {\n            key: value.cpu()\n            for key, value in state_dict.items()\n        }\n        del state_dict\n        trainer._save(output_dir, state_dict=cpu_state_dict)  # noqa\n\n\ndef smart_tokenizer_and_embedding_resize(\n    special_tokens_dict: Dict,\n    tokenizer: transformers.PreTrainedTokenizer,\n    model: transformers.PreTrainedModel,\n):\n    \"\"\"Resize tokenizer and embedding.\n\n    Note: This is the unoptimized version that may make your embedding size not be divisible by 64.\n    \"\"\"\n    num_new_tokens = tokenizer.add_special_tokens(special_tokens_dict)\n    model.resize_token_embeddings(len(tokenizer))\n\n    if num_new_tokens > 0:\n        input_embeddings = model.get_input_embeddings().weight.data\n        output_embeddings = model.get_output_embeddings().weight.data\n\n        input_embeddings_avg = input_embeddings[:-num_new_tokens].mean(\n            dim=0, keepdim=True)\n        output_embeddings_avg = output_embeddings[:-num_new_tokens].mean(\n            dim=0, keepdim=True)\n\n        input_embeddings[-num_new_tokens:] = input_embeddings_avg\n        output_embeddings[-num_new_tokens:] = output_embeddings_avg\n\n\ndef _tokenize_fn(strings: Sequence[str],\n                 tokenizer: transformers.PreTrainedTokenizer) -> Dict:\n    \"\"\"Tokenize a list of strings.\"\"\"\n    tokenized_list = [\n        tokenizer(\n            text,\n            return_tensors=\"pt\",\n            padding=\"longest\",\n            max_length=tokenizer.model_max_length,\n            truncation=True,\n        ) for text in strings\n    ]\n    input_ids = labels = [\n        tokenized.input_ids[0] for tokenized in tokenized_list\n    ]\n    input_ids_lens = labels_lens = [\n        tokenized.input_ids.ne(tokenizer.pad_token_id).sum().item()\n        for tokenized in tokenized_list\n    ]\n    return dict(\n        input_ids=input_ids,\n        labels=labels,\n        input_ids_lens=input_ids_lens,\n        labels_lens=labels_lens,\n    )\n\n\ndef _mask_targets(target, tokenized_lens, speakers):\n    # cur_idx = 0\n    cur_idx = tokenized_lens[0]\n    tokenized_lens = tokenized_lens[1:]\n    target[:cur_idx] = IGNORE_INDEX\n    for tokenized_len, speaker in zip(tokenized_lens, speakers):\n        if speaker == \"human\":\n            target[cur_idx+2:cur_idx + tokenized_len] = IGNORE_INDEX\n        cur_idx += tokenized_len\n\n\ndef _add_speaker_and_signal(header, source, get_conversation=True):\n    \"\"\"Add speaker and start/end signal on each round.\"\"\"\n    BEGIN_SIGNAL = \"### \"\n    END_SIGNAL = \"\\n\"\n    conversation = header\n    for sentence in source:\n        from_str = sentence[\"from\"]\n        if from_str.lower() == \"human\":\n            from_str = conversation_lib.default_conversation.roles[0]\n        elif from_str.lower() == \"gpt\":\n            from_str = conversation_lib.default_conversation.roles[1]\n        else:\n            from_str = 'unknown'\n        sentence[\"value\"] = (BEGIN_SIGNAL + from_str + \": \" +\n                             sentence[\"value\"] + END_SIGNAL)\n        if get_conversation:\n            conversation += sentence[\"value\"]\n    conversation += BEGIN_SIGNAL\n    return conversation\n\n\ndef preprocess_multimodal(\n    sources: Sequence[str],\n    multimodal_cfg: dict,\n    cur_token_len: int,\n) -> Dict:\n    is_multimodal = multimodal_cfg['is_multimodal']\n    # image_token_len = multimodal_cfg['image_token_len']\n    image_token_len = cur_token_len\n    if not is_multimodal:\n        return sources\n\n    for source in sources:\n        if multimodal_cfg['sep_image_conv_front']:\n            assert DEFAULT_IMAGE_TOKEN in source[0]['value']\n            source[0]['value'] = source[0]['value'].replace(DEFAULT_IMAGE_TOKEN, '').strip()\n            source[0]['value'] = DEFAULT_IMAGE_TOKEN + conversation_lib.default_conversation.sep + conversation_lib.default_conversation.roles[0] + \": \" + source[0]['value']\n        for sentence in source:\n            replace_token = DEFAULT_IMAGE_PATCH_TOKEN * image_token_len\n            if multimodal_cfg['use_im_start_end']:\n                replace_token = DEFAULT_IM_START_TOKEN + replace_token + DEFAULT_IM_END_TOKEN\n            sentence[\"value\"] = sentence[\"value\"].replace(DEFAULT_IMAGE_TOKEN, replace_token)\n\n    return sources\n\n\ndef preprocess_v1(\n    sources,\n    tokenizer: transformers.PreTrainedTokenizer,\n) -> Dict:\n    conv = conversation_lib.default_conversation.copy()\n    roles = {\"human\": conv.roles[0], \"gpt\": conv.roles[1]}\n\n    # Apply prompt templates\n    conversations = []\n    for i, source in enumerate(sources):\n        if roles[source[0][\"from\"]] != conv.roles[0]:\n            # Skip the first one if it is not from human\n            source = source[1:]\n\n        conv.messages = []\n        for j, sentence in enumerate(source):\n            role = roles[sentence[\"from\"]]\n            assert role == conv.roles[j % 2], f\"{i}\"\n            conv.append_message(role, sentence[\"value\"])\n        conversations.append(conv.get_prompt())\n\n    # Tokenize conversations\n    input_ids = tokenizer(\n        conversations,\n        return_tensors=\"pt\",\n        padding=\"longest\",\n        max_length=tokenizer.model_max_length,\n        truncation=True,\n    ).input_ids\n    targets = input_ids.clone()\n\n    assert conv.sep_style == conversation_lib.SeparatorStyle.TWO\n\n    # Mask targets\n    sep = conv.sep + conv.roles[1] + \": \"\n    for conversation, target in zip(conversations, targets):\n        total_len = int(target.ne(tokenizer.pad_token_id).sum())\n\n        rounds = conversation.split(conv.sep2)\n        cur_len = 1\n        target[:cur_len] = IGNORE_INDEX\n        for i, rou in enumerate(rounds):\n            if rou == \"\":\n                break\n\n            parts = rou.split(sep)\n            if len(parts) != 2:\n                break\n            parts[0] += sep\n            round_len = len(tokenizer(rou).input_ids)\n            instruction_len = len(tokenizer(parts[0]).input_ids) - 2\n\n            target[cur_len : cur_len + instruction_len] = IGNORE_INDEX\n\n            cur_len += round_len\n        target[cur_len:] = IGNORE_INDEX\n\n        if cur_len < tokenizer.model_max_length:\n            if cur_len != total_len:\n                target[:] = IGNORE_INDEX\n                print(\n                    f\"WARNING: tokenization mismatch: {cur_len} vs. {total_len}.\"\n                    f\" (ignored)\"\n                )\n\n    return dict(\n        input_ids=input_ids,\n        labels=targets,\n    )\n\ndef preprocess_mpt(\n    sources,\n    tokenizer: transformers.PreTrainedTokenizer,\n) -> Dict:\n    conv = conversation_lib.default_conversation.copy()\n    roles = {\"human\": conv.roles[0], \"gpt\": conv.roles[1]}\n\n    # Apply prompt templates\n    conversations = []\n    for i, source in enumerate(sources):\n        if roles[source[0][\"from\"]] != conv.roles[0]:\n            # Skip the first one if it is not from human\n            source = source[1:]\n\n        conv.messages = []\n        for j, sentence in enumerate(source):\n            role = roles[sentence[\"from\"]]\n            assert role == conv.roles[j % 2], f\"{i}\"\n            conv.append_message(role, sentence[\"value\"])\n        conversations.append(conv.get_prompt())\n\n    # Tokenize conversations\n    input_ids = tokenizer(\n        conversations,\n        return_tensors=\"pt\",\n        padding=\"longest\",\n        max_length=tokenizer.model_max_length,\n        truncation=True,\n    ).input_ids\n    targets = input_ids.clone()\n    assert conv.sep_style == conversation_lib.SeparatorStyle.MPT\n\n    # Mask targets\n    sep = conv.sep + conv.roles[1]\n    for conversation, target in zip(conversations, targets):\n        total_len = int(target.ne(tokenizer.pad_token_id).sum())\n\n        rounds = conversation.split(conv.sep)\n        re_rounds = [conv.sep.join(rounds[:3])] # system + user + gpt\n        for conv_idx in range(3, len(rounds), 2):\n            re_rounds.append(conv.sep.join(rounds[conv_idx:conv_idx+2]))    # user + gpt\n        cur_len = 0\n        target[:cur_len] = IGNORE_INDEX\n        for i, rou in enumerate(re_rounds):\n            if rou == \"\":\n                break\n\n            parts = rou.split(sep)\n            if len(parts) != 2:\n                break\n            parts[0] += sep\n            round_len = len(tokenizer(rou).input_ids) + len(tokenizer(conv.sep).input_ids)\n            instruction_len = len(tokenizer(parts[0]).input_ids)\n            target[cur_len : cur_len + instruction_len] = IGNORE_INDEX\n\n            cur_len += round_len\n        target[cur_len:] = IGNORE_INDEX\n\n        if cur_len < tokenizer.model_max_length:\n            if cur_len != total_len:\n                target[:] = IGNORE_INDEX\n                print(\n                    f\"WARNING: tokenization mismatch: {cur_len} vs. {total_len}.\"\n                    f\" (ignored)\"\n                )\n\n    return dict(\n        input_ids=input_ids,\n        labels=targets,\n    )\n\n\ndef preprocess(\n    sources: Sequence[str],\n    tokenizer: transformers.PreTrainedTokenizer,\n) -> Dict:\n    \"\"\"\n    Given a list of sources, each is a conversation list. This transform:\n    1. Add signal '### ' at the beginning each sentence, with end signal '\\n';\n    2. Concatenate conversations together;\n    3. Tokenize the concatenated conversation;\n    4. Make a deepcopy as the target. Mask human words with IGNORE_INDEX.\n    \"\"\"\n    if conversation_lib.default_conversation.version == \"v1\":\n        return preprocess_v1(sources, tokenizer)\n    if conversation_lib.default_conversation.version == \"mpt\":\n        return preprocess_mpt(sources, tokenizer)\n    # add end signal and concatenate together\n    conversations = []\n    for source in sources:\n        header = f\"{conversation_lib.default_conversation.system}\\n\\n\"\n        conversation = _add_speaker_and_signal(header, source)\n        conversations.append(conversation)\n    # tokenize conversations\n    conversations_tokenized = _tokenize_fn(conversations, tokenizer)\n    input_ids = conversations_tokenized[\"input_ids\"]\n    targets = copy.deepcopy(input_ids)\n    for target, source in zip(targets, sources):\n        tokenized_lens = _tokenize_fn([header] + [s[\"value\"] for s in source],\n                                      tokenizer)[\"input_ids_lens\"]\n        speakers = [sentence[\"from\"] for sentence in source]\n        _mask_targets(target, tokenized_lens, speakers)\n\n    return dict(input_ids=input_ids, labels=targets)\n\n\nclass SupervisedDataset(Dataset):\n    \"\"\"Dataset for supervised fine-tuning.\"\"\"\n\n    def __init__(self, data_path: str,\n                 tokenizer: transformers.PreTrainedTokenizer):\n        super(SupervisedDataset, self).__init__()\n        logging.warning(\"Loading data...\")\n        list_data_dict = json.load(open(data_path, \"r\"))\n\n        logging.warning(\"Formatting inputs...\")\n        sources = [example[\"conversations\"] for example in list_data_dict]\n        data_dict = preprocess(sources, tokenizer)\n\n        self.input_ids = data_dict[\"input_ids\"]\n        self.labels = data_dict[\"labels\"]\n\n    def __len__(self):\n        return len(self.input_ids)\n\n    def __getitem__(self, i) -> Dict[str, torch.Tensor]:\n        return dict(input_ids=self.input_ids[i], labels=self.labels[i])\n\n\nclass LazySupervisedDataset(Dataset):\n\n    def __init__(self, data_path: str,\n                 tokenizer: transformers.PreTrainedTokenizer,\n                 multimodal_cfg: dict):\n        super(LazySupervisedDataset, self).__init__()\n\n        self.tokenizer, self.multimodal_cfg = tokenizer, multimodal_cfg\n\n        self.pkl, self.prompt = pickle.load(open('./_data/ipr2pr.pkl', 'rb'))['task'], json.load(open('./_data/ipr2pr_expressive.json', 'r'))\n        random.shuffle(self.pkl)\n        print('--pkl: %d--'%(len(self.pkl)))\n\n    def __len__(self):\n        return len(self.pkl)\n\n    def __getitem__(self, i) -> Dict[str, torch.Tensor]:\n        item = self.pkl[i][0]\n\n        tsv = open('./_data/ipr2pr.tsv', 'r')\n        tsv.seek(item['lineidx'])\n        b = tsv.readline().strip().split('\\t')\n        image = resize(b2f(b[0]))\n\n        processor = self.multimodal_cfg['image_processor']\n        image = processor.preprocess(image, return_tensors='pt')['pixel_values'][0]\n\n        cur_token_len = (image.shape[1]//14)*(image.shape[2]//14)\n        query = \"what will this image be like if '%s'\\n%s\"%(item['instruction'], DEFAULT_IMAGE_TOKEN)\n        ans = '%s [IMG0] [IMG1] [IMG2] [IMG3] [IMG4] [IMG5] [IMG6] [IMG7]'%(self.prompt[item['input']]['expressive'])\n        sources = preprocess_multimodal(copy.deepcopy([[{'from': 'human', 'value': query}, {'from': 'gpt', 'value': ans}]]),\n                                        self.multimodal_cfg, cur_token_len)\n\n        data_dict = preprocess(sources, self.tokenizer)\n        if isinstance(i, int): data_dict = dict(input_ids=data_dict['input_ids'][0],\n                                                labels=data_dict['labels'][0])\n        data_dict['image'] = image\n\n        p2p_inp, p2p_ans = img2npy(resize(b2f(b[0])).resize([256, 256])), img2npy(resize(b2f(b[1])).resize([256, 256]))\n        data_dict['p2p_inp'], data_dict['p2p_ans'] = p2p_inp, p2p_ans\n\n        return data_dict\n\n\n@dataclass\nclass DataCollatorForSupervisedDataset(object):\n    \"\"\"Collate examples for supervised fine-tuning.\"\"\"\n\n    tokenizer: transformers.PreTrainedTokenizer\n\n    def __call__(self, instances: Sequence[Dict]) -> Dict[str, torch.Tensor]:\n        input_ids, labels = tuple([instance[key] for instance in instances]\n                                  for key in (\"input_ids\", \"labels\"))\n        input_ids = torch.nn.utils.rnn.pad_sequence(\n            input_ids,\n            batch_first=True,\n            padding_value=self.tokenizer.pad_token_id)\n        labels = torch.nn.utils.rnn.pad_sequence(labels,\n                                                 batch_first=True,\n                                                 padding_value=IGNORE_INDEX)\n        batch = dict(\n            input_ids=input_ids,\n            labels=labels,\n            attention_mask=input_ids.ne(self.tokenizer.pad_token_id),\n        )\n\n        if 'image' in instances[0]:\n            images = [instance['image'] for instance in instances]\n            if all(x is not None and x.shape == images[0].shape for x in images):\n                batch['images'] = torch.stack(images)\n            else:\n                batch['images'] = images\n\n        batch['p2p_inp'], batch['p2p_ans'] = [torch.cat([torch.from_numpy(d['p2p_inp']).unsqueeze(dim=0) for d in instances], dim=0),\n                                              torch.cat([torch.from_numpy(d['p2p_ans']).unsqueeze(dim=0) for d in instances], dim=0)]\n\n        return batch\n\n\ndef make_supervised_data_module(tokenizer: transformers.PreTrainedTokenizer,\n                                data_args) -> Dict:\n    \"\"\"Make dataset and collator for supervised fine-tuning.\"\"\"\n    dataset_cls = (LazySupervisedDataset\n                   if data_args.lazy_preprocess else SupervisedDataset)\n    train_dataset = dataset_cls(tokenizer=tokenizer,\n                                data_path=data_args.data_path,\n                                multimodal_cfg=dict(\n                                    is_multimodal=data_args.is_multimodal,\n                                    sep_image_conv_front=data_args.sep_image_conv_front,\n                                    image_token_len=data_args.image_token_len,\n                                    image_folder=data_args.image_folder,\n                                    image_aspect_ratio=data_args.image_aspect_ratio,\n                                    use_im_start_end=getattr(data_args, 'mm_use_im_start_end', False),\n                                    image_processor=getattr(data_args, 'image_processor', None)))\n    data_collator = DataCollatorForSupervisedDataset(tokenizer=tokenizer)\n    return dict(train_dataset=train_dataset,\n                eval_dataset=None,\n                data_collator=data_collator)\n\n\ndef train():\n    parser = transformers.HfArgumentParser((ModelArguments, DataArguments, TrainingArguments))\n    model_args, data_args, training_args = parser.parse_args_into_dataclasses()\n    compute_dtype = (torch.float16 if training_args.fp16 else (torch.bfloat16 if training_args.bf16 else torch.float32))\n\n    bnb_model_from_pretrained_args = {}\n    if training_args.bits in [4, 8]:\n        from transformers import BitsAndBytesConfig\n        from peft import prepare_model_for_int8_training\n        bnb_model_from_pretrained_args.update(dict(\n            device_map={\"\": training_args.device},\n            load_in_4bit=training_args.bits == 4,\n            load_in_8bit=training_args.bits == 8,\n            quantization_config=BitsAndBytesConfig(\n                load_in_4bit=training_args.bits == 4,\n                load_in_8bit=training_args.bits == 8,\n                llm_int8_threshold=6.0,\n                llm_int8_has_fp16_weight=False,\n                bnb_4bit_compute_dtype=compute_dtype,\n                bnb_4bit_use_double_quant=training_args.double_quant,\n                bnb_4bit_quant_type=training_args.quant_type # {'fp4', 'nf4'}\n            )\n        ))\n\n    if model_args.vision_tower is not None:\n        if 'mpt' in model_args.model_name_or_path:\n            model = LlavaMPTForCausalLM.from_pretrained(\n                model_args.model_name_or_path,\n                cache_dir=training_args.cache_dir,\n                **bnb_model_from_pretrained_args\n            )\n        else:\n            model = LlavaLlamaForCausalLM.from_pretrained(\n                model_args.model_name_or_path,\n                cache_dir=training_args.cache_dir,\n                **bnb_model_from_pretrained_args\n            )\n    else:\n        model = transformers.LlamaForCausalLM.from_pretrained(\n            model_args.model_name_or_path,\n            cache_dir=training_args.cache_dir,\n            **bnb_model_from_pretrained_args\n        )\n    model.config.use_cache = False\n\n    if model_args.freeze_backbone:\n        model.model.requires_grad_(False)\n\n    if training_args.bits in [4, 8]:\n        model.config.torch_dtype=(torch.float32 if training_args.fp16 else (torch.bfloat16 if training_args.bf16 else torch.float32))\n        model = prepare_model_for_int8_training(model, use_gradient_checkpointing=training_args.gradient_checkpointing)\n\n    if training_args.gradient_checkpointing and model_args.vision_tower is None:\n        if hasattr(model, \"enable_input_require_grads\"):\n            model.enable_input_require_grads()\n        else:\n            def make_inputs_require_grad(module, input, output):\n                output.requires_grad_(True)\n            model.get_input_embeddings().register_forward_hook(make_inputs_require_grad)\n\n    if training_args.lora_enable:\n        from peft import LoraConfig, get_peft_model\n        lora_config = LoraConfig(\n            r=training_args.lora_r,\n            lora_alpha=training_args.lora_alpha,\n            target_modules=find_all_linear_names(model),\n            lora_dropout=training_args.lora_dropout,\n            bias=training_args.lora_bias,\n            task_type=\"CAUSAL_LM\",\n        )\n        if training_args.bits == 16:\n            if training_args.bf16:\n                model.to(torch.bfloat16)\n            if training_args.fp16:\n                model.to(torch.float16)\n        logging.warning(\"Adding LoRA adapters...\")\n        model = get_peft_model(model, lora_config)\n\n    if 'mpt' in model_args.model_name_or_path:\n        tokenizer = transformers.AutoTokenizer.from_pretrained(\n            model_args.model_name_or_path,\n            cache_dir=training_args.cache_dir,\n            model_max_length=training_args.model_max_length,\n            padding_side=\"right\"\n        )\n    else:\n        tokenizer = transformers.AutoTokenizer.from_pretrained(\n            model_args.model_name_or_path,\n            cache_dir=training_args.cache_dir,\n            model_max_length=training_args.model_max_length,\n            padding_side=\"right\",\n            use_fast=False,\n        )\n\n    if model_args.version == \"v0\":\n        if tokenizer.pad_token is None:\n            smart_tokenizer_and_embedding_resize(\n                special_tokens_dict=dict(pad_token=DEFAULT_PAD_TOKEN),\n                tokenizer=tokenizer,\n                model=model,\n            )\n        if \"llama\" in model_args.model_name_or_path:\n            tokenizer.add_special_tokens({\n                \"eos_token\": DEFAULT_EOS_TOKEN,\n                \"bos_token\": DEFAULT_BOS_TOKEN,\n                \"unk_token\": DEFAULT_UNK_TOKEN,\n            })\n    else:\n        tokenizer.pad_token = tokenizer.unk_token\n        if \"mpt\" in model_args.model_name_or_path:\n            conversation_lib.default_conversation = conversation_lib.conv_templates[\"mpt\"]\n        else:\n            conversation_lib.default_conversation = conversation_lib.conv_templates[\"vicuna_v1_1\"]\n\n    if model_args.vision_tower is not None:\n        model_vision_dict = model.get_model().initialize_vision_modules(\n            vision_tower=model_args.vision_tower,\n            mm_vision_select_layer=model_args.mm_vision_select_layer,\n            pretrain_mm_mlp_adapter=model_args.pretrain_mm_mlp_adapter,\n            fsdp=training_args.fsdp\n        )\n        model.get_vision_tower().to(dtype=torch.float16, device=training_args.device)\n        vision_config = model_vision_dict['vision_config']\n\n        data_args.image_token_len = model_vision_dict['image_token_len']\n        data_args.image_processor = model_vision_dict['image_processor']\n        data_args.is_multimodal = True\n\n        model.config.tune_mm_mlp_adapter = training_args.tune_mm_mlp_adapter = model_args.tune_mm_mlp_adapter\n        if model_args.tune_mm_mlp_adapter:\n            model.requires_grad_(False)\n            for p in model.get_model().mm_projector.parameters():\n                p.requires_grad = True\n\n        model.config.freeze_mm_mlp_adapter = training_args.freeze_mm_mlp_adapter\n        if training_args.freeze_mm_mlp_adapter:\n            for p in model.get_model().mm_projector.parameters():\n                p.requires_grad = False\n\n        if training_args.bits in [4, 8]:\n            model.get_model().mm_projector.to(dtype=compute_dtype, device=training_args.device)\n\n        model.config.mm_use_im_start_end = data_args.mm_use_im_start_end = model_args.mm_use_im_start_end\n        vision_config.use_im_start_end = training_args.use_im_start_end = model_args.mm_use_im_start_end\n        model.config.sep_image_conv_front = data_args.sep_image_conv_front\n        model.initialize_vision_tokenizer(mm_use_im_start_end=model_args.mm_use_im_start_end, tokenizer=tokenizer, device=training_args.device,\n                                          tune_mm_mlp_adapter=model_args.tune_mm_mlp_adapter, pretrain_mm_mlp_adapter=model_args.pretrain_mm_mlp_adapter)\n\n        params_no_grad = [n for n, p in model.named_parameters() if not p.requires_grad]\n        if len(params_no_grad) > 0:\n            if training_args.fsdp is not None and len(training_args.fsdp) > 0:\n                if len(params_no_grad) < 10:\n                    print('[WARNING] Attempting to use FSDP while {} parameters do not require gradients: {}'. format(len(params_no_grad), params_no_grad))\n                else:\n                    print('[WARNING] Attempting to use FSDP while {} parameters do not require gradients: {}...(omitted)'. format(len(params_no_grad), ', '.join(params_no_grad[:10])))\n                print(\"[WARNING] Attempting to use FSDP with partially frozen paramters, this is experimental.\")\n                print(\"[WARNING] As of 4/30/23, this feature requires PyTorch-nightly build.  See here for details: https://github.com/haotian-liu/LLaVA#experimental-use-fsdp-to-save-memory-in-pretraining\")\n\n                from torch.distributed.fsdp.fully_sharded_data_parallel import FullyShardedDataParallel as FSDP\n                def patch_FSDP_use_orig_params(func):\n                    def wrap_func(*args, **kwargs):\n                        use_orig_params = kwargs.pop('use_orig_params', True)\n                        return func(*args, **kwargs, use_orig_params=use_orig_params)\n                    return wrap_func\n\n                FSDP.__init__ = patch_FSDP_use_orig_params(FSDP.__init__)\n\n    if training_args.bits in [4, 8]:\n        from peft.tuners.lora import LoraLayer\n        for name, module in model.named_modules():\n            if isinstance(module, LoraLayer):\n                if training_args.bf16:\n                    module = module.to(torch.bfloat16)\n            if 'norm' in name:\n                module = module.to(torch.float32)\n            if 'lm_head' in name or 'embed_tokens' in name:\n                if hasattr(module, 'weight'):\n                    if training_args.bf16 and module.weight.dtype == torch.float32:\n                        module = module.to(torch.bfloat16)\n\n    # start for MGIE\n    os.makedirs('_log', exist_ok=True)\n\n    pt = {}\n    for i in tqdm(range(2)): pt.update(torch.load('./_ckpt/LLaVA-7B-v1/pytorch_model-0000%d-of-00002.bin'%(i+1), map_location='cpu'))\n    miss, unexp = model.load_state_dict(pt, strict=False)\n    print('miss:', miss), print('unexp:', unexp)\n\n    tokenizer.add_tokens(['[IMG0]', '[IMG1]', '[IMG2]', '[IMG3]', '[IMG4]', '[IMG5]', '[IMG6]', '[IMG7]'], special_tokens=True)\n    model.resize_token_embeddings(len(tokenizer))\n    print(tokenizer), json.dump(tokenizer.get_vocab(), open('_log/vocabs.json', 'w'), indent=2)\n\n    for n, p in model.named_parameters():\n        if 'embed_tokens' in n or 'lm_head' in n or 'edit_head' in n or 'unet' in n: p.requires_grad = True\n        else: p.requires_grad = False\n    with open('_log/parameters.txt', 'w') as F:\n        for n, p in model.named_parameters(): F.write('%s %s %s\\n'%(n, str(p.shape), str(p.requires_grad)))\n\n    with open('_log/args_train.txt', 'w') as F:\n        for key in vars(training_args): F.write('%s: %s\\n'%(str(key), str(vars(training_args)[key])))\n    # end for MGIE\n\n    data_module = make_supervised_data_module(tokenizer=tokenizer,\n                                              data_args=data_args)\n    trainer = LLaVATrainer(model=model,\n                    tokenizer=tokenizer,\n                    args=training_args,\n                    **data_module)\n\n    if list(pathlib.Path(training_args.output_dir).glob(\"checkpoint-*\")):\n        trainer.train(resume_from_checkpoint=True)\n    else:\n        trainer.train()\n    trainer.save_state()\n\n    if training_args.lora_enable:\n        state_dict = get_peft_state_maybe_zero_3(\n            model.named_parameters(), training_args.lora_bias\n        )\n        non_lora_state_dict = get_peft_state_non_lora_maybe_zero_3(\n            model.named_parameters()\n        )\n        if training_args.local_rank == 0 or training_args.local_rank == -1:\n            model.config.save_pretrained(training_args.output_dir)\n            model.save_pretrained(training_args.output_dir, state_dict=state_dict)\n            torch.save(non_lora_state_dict, os.path.join(training_args.output_dir, 'non_lora_trainables.bin'))\n    else:\n        safe_save_model_for_hf_trainer(trainer=trainer,\n                                       output_dir=training_args.output_dir)\n\n\nif __name__ == \"__main__\":\n    train()\n"
        },
        {
          "name": "process_data.ipynb",
          "type": "blob",
          "size": 7.4248046875,
          "content": "{\n \"cells\": [\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"id\": \"7d90bd3a\",\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"#\\n\",\n    \"# For licensing see accompanying LICENSE file.\\n\",\n    \"# Copyright (C) 2024 Apple Inc. All Rights Reserved.\\n\",\n    \"#\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"id\": \"39465df4-3ac7-4340-b71d-02c11c78d1be\",\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"import os, pickle, io, base64, json\\n\",\n    \"\\n\",\n    \"from glob import glob\\n\",\n    \"from tqdm.auto import tqdm\\n\",\n    \"\\n\",\n    \"from PIL import Image\\n\",\n    \"\\n\",\n    \"import torch as T\\n\",\n    \"import transformers\\n\",\n    \"\\n\",\n    \"from llava.conversation import conv_templates\\n\",\n    \"from llava.model import *\\n\",\n    \"\\n\",\n    \"def f2b(f):\\n\",\n    \"    b = io.BytesIO()\\n\",\n    \"    f.save(b, format='JPEG')\\n\",\n    \"    b = str(base64.b64encode(b.getvalue()))[2:-1]\\n\",\n    \"    return b\\n\",\n    \"def b2f(b):\\n\",\n    \"    return Image.open(io.BytesIO(base64.b64decode(b))).convert('RGB')\\n\",\n    \"def crop_resize(f, sz=512):\\n\",\n    \"    w, h = f.size\\n\",\n    \"    if w>h:\\n\",\n    \"        p = (w-h)//2\\n\",\n    \"        f = f.crop([p, 0, p+h, h])\\n\",\n    \"    elif h>w:\\n\",\n    \"        p = (h-w)//2\\n\",\n    \"        f = f.crop([0, p, w, p+w])\\n\",\n    \"    f = f.resize([sz, sz])\\n\",\n    \"    return f\\n\",\n    \"def remove_alter(s):  # hack expressive instruction\\n\",\n    \"    if 'ASSISTANT:' in s: s = s[s.index('ASSISTANT:')+10:].strip()\\n\",\n    \"    if '</s>' in s: s = s[:s.index('</s>')].strip()\\n\",\n    \"    if 'alternative' in s.lower(): s = s[:s.lower().index('alternative')]\\n\",\n    \"    if '[IMG0]' in s: s = s[:s.index('[IMG0]')]\\n\",\n    \"    s = '.'.join([s.strip() for s in s.split('.')[:2]])\\n\",\n    \"    if s[-1]!='.': s += '.'\\n\",\n    \"    return s.strip()\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"id\": \"7004c5ab-7ccc-4914-b1d9-0314ebfd5a6f\",\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"DEFAULT_IMAGE_TOKEN = '<image>'\\n\",\n    \"DEFAULT_IMAGE_PATCH_TOKEN = '<im_patch>'\\n\",\n    \"DEFAULT_IM_START_TOKEN = '<im_start>'\\n\",\n    \"DEFAULT_IM_END_TOKEN = '<im_end>'\\n\",\n    \"\\n\",\n    \"MODEL_NAME = './_ckpt/LLaVA-7B-v1'\\n\",\n    \"model_name = os.path.expanduser(MODEL_NAME)\\n\",\n    \"\\n\",\n    \"tokenizer = transformers.AutoTokenizer.from_pretrained(model_name)\\n\",\n    \"model = LlavaLlamaForCausalLM.from_pretrained(model_name, low_cpu_mem_usage=True, torch_dtype=T.float16, use_cache=True).cuda()\\n\",\n    \"image_processor = transformers.CLIPImageProcessor.from_pretrained(model.config.mm_vision_tower, torch_dtype=T.float16)\\n\",\n    \"\\n\",\n    \"tokenizer.padding_side = 'left'\\n\",\n    \"\\n\",\n    \"mm_use_im_start_end = getattr(model.config, 'mm_use_im_start_end', False)\\n\",\n    \"tokenizer.add_tokens([DEFAULT_IMAGE_PATCH_TOKEN], special_tokens=True)\\n\",\n    \"if mm_use_im_start_end: tokenizer.add_tokens([DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN], special_tokens=True)\\n\",\n    \"\\n\",\n    \"vision_tower = model.get_model().vision_tower[0]\\n\",\n    \"vision_tower = transformers.CLIPVisionModel.from_pretrained(vision_tower.config._name_or_path, torch_dtype=T.float16, low_cpu_mem_usage=True).cuda()\\n\",\n    \"model.get_model().vision_tower[0] = vision_tower\\n\",\n    \"vision_config = vision_tower.config\\n\",\n    \"vision_config.im_patch_token = tokenizer.convert_tokens_to_ids([DEFAULT_IMAGE_PATCH_TOKEN])[0]\\n\",\n    \"vision_config.use_im_start_end = mm_use_im_start_end\\n\",\n    \"if mm_use_im_start_end: vision_config.im_start_token, vision_config.im_end_token = tokenizer.convert_tokens_to_ids([DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN])\\n\",\n    \"image_token_len = (vision_config.image_size//vision_config.patch_size)**2\\n\",\n    \"\\n\",\n    \"_ = model.eval()\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"id\": \"8e479793-f2f9-45b6-a05b-9f5b0d09e396\",\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"summer = transformers.pipeline('summarization', 'jordiclive/flan-t5-11b-summarizer-filtered', torch_dtype=T.bfloat16, device=0)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"id\": \"25ac2e16-32a8-45ea-acc9-37b28a0a9681\",\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"pkl, tsv, ei = {'task': []}, open('./_data/ipr2pr.tsv', 'w'), {}\\n\",\n    \"\\n\",\n    \"lst = glob('_data/*/prompt.json')\\n\",\n    \"for file in tqdm(lst):\\n\",\n    \"    prompt = json.load(open(file, 'r'))\\n\",\n    \"    txt = prompt['edit']\\n\",\n    \"\\n\",\n    \"    txt = \\\"what will this image be like if '%s'  (in a short paragraph)\\\"%(txt)\\n\",\n    \"    txt = txt+'\\\\n'+DEFAULT_IM_START_TOKEN+DEFAULT_IMAGE_PATCH_TOKEN*image_token_len+DEFAULT_IM_END_TOKEN\\n\",\n    \"    conv = conv_templates['vicuna_v1_1'].copy()\\n\",\n    \"    conv.append_message(conv.roles[0], txt), conv.append_message(conv.roles[1], None)\\n\",\n    \"    txt = conv.get_prompt()\\n\",\n    \"    txt = tokenizer(txt)\\n\",\n    \"    txt, mask = T.as_tensor(txt['input_ids']), T.as_tensor(txt['attention_mask'])\\n\",\n    \"    \\n\",\n    \"    for img in glob('/'.join(file.split('/')[:-1])+'/*_0.jpg'):\\n\",\n    \"        item = file.split('/')[-2]+'_'+img.split('/')[-1].replace('.jpg', '')\\n\",\n    \"        inp, ans = Image.open(img).convert('RGB'), Image.open(img.replace('_0.jpg', '_1.jpg')).convert('RGB')\\n\",\n    \"        \\n\",\n    \"        img = image_processor.preprocess(inp, return_tensors='pt')['pixel_values'][0]\\n\",\n    \"        with T.inference_mode():\\n\",\n    \"            out = model.generate(txt.unsqueeze(dim=0).cuda(), images=img.half().unsqueeze(dim=0).cuda(), attention_mask=mask.unsqueeze(dim=0).cuda(), \\n\",\n    \"                                 do_sample=False, max_new_tokens=1024)[0].tolist()\\n\",\n    \"            \\n\",\n    \"            out = remove_alter(tokenizer.decode(out))\\n\",\n    \"            res = summer(['summarize the following paragraph in 32 words:\\\\n\\\\n%s'%(out)], num_beams=5, min_length=5, max_length=64, \\n\",\n    \"                         do_sample=False, no_repeat_ngram_size=3, truncation=True)[0]['summary_text']\\n\",\n    \"\\n\",\n    \"        pkl['task'].append([{'input': item, 'answer': item.replace('_0', '_1'), 'instruction': prompt['edit'], 'lineidx': tsv.tell()}])\\n\",\n    \"        tsv.write('%s\\\\t%s\\\\n'%(f2b(inp), f2b(ans)))\\n\",\n    \"        ei[item] = {'instruction': prompt['edit'], 'expressive': res}\\n\",\n    \"\\n\",\n    \"pickle.dump(pkl, open('./_data/ipr2pr.pkl', 'wb'))\\n\",\n    \"tsv.flush(), tsv.close()\\n\",\n    \"json.dump(ei, open('./_data/ipr2pr_expressive.json', 'w'), indent=2)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"id\": \"c36ea959-ff15-4ebd-848f-13de020886a5\",\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"pkl, tsv, ei = pickle.load(open('./_data/ipr2pr.pkl', 'rb')), open('./_data/ipr2pr.tsv', 'r'), json.load(open('./_data/ipr2pr_expressive.json', 'r'))\\n\",\n    \"for task in pkl['task']:\\n\",\n    \"    task = task[0]\\n\",\n    \"    tsv.seek(task['lineidx'])\\n\",\n    \"    b = tsv.readline().strip().split('\\\\t')\\n\",\n    \"    print(task)\\n\",\n    \"    display(b2f(b[0])), display(b2f(b[1]))\\n\",\n    \"    print(ei[task['input']])\\n\",\n    \"    print('\\\\n-----\\\\n')\"\n   ]\n  }\n ],\n \"metadata\": {\n  \"kernelspec\": {\n   \"display_name\": \"Python 3 (ipykernel)\",\n   \"language\": \"python\",\n   \"name\": \"python3\"\n  },\n  \"language_info\": {\n   \"codemirror_mode\": {\n    \"name\": \"ipython\",\n    \"version\": 3\n   },\n   \"file_extension\": \".py\",\n   \"mimetype\": \"text/x-python\",\n   \"name\": \"python\",\n   \"nbconvert_exporter\": \"python\",\n   \"pygments_lexer\": \"ipython3\",\n   \"version\": \"3.10.13\"\n  }\n },\n \"nbformat\": 4,\n \"nbformat_minor\": 5\n}\n"
        }
      ]
    }
  ]
}