{
  "metadata": {
    "timestamp": 1736559977705,
    "page": 767,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjc3MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "riffusion/riffusion-hobby",
      "stars": 3464,
      "defaultBranch": "main",
      "files": [
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 1.85546875,
          "content": "# Byte-compiled / optimized / DLL files\n__pycache__/\n*.py[cod]\n*$py.class\n\n# C extensions\n*.so\n\n# VSCode\n.vscode\n\n# Cog\n.cog/\n\n# Random stuff I don't care about\n.graveyard/\n\n# Distribution / packaging\n.Python\nbuild/\ndevelop-eggs/\ndist/\ndownloads/\neggs/\n.eggs/\nlib/\nlib64/\nparts/\nsdist/\nvar/\nwheels/\npip-wheel-metadata/\nshare/python-wheels/\n*.egg-info/\n.installed.cfg\n*.egg\nMANIFEST\n\n# OSX cruft\n.DS_Store\n\n# PyInstaller\n#  Usually these files are written by a python script from a template\n#  before PyInstaller builds the exe, so as to inject date/other infos into it.\n*.manifest\n*.spec\n\n# Installer logs\npip-log.txt\npip-delete-this-directory.txt\n\n# Unit test / coverage reports\nhtmlcov/\n.tox/\n.nox/\n.coverage\n.coverage.*\n.cache\nnosetests.xml\ncoverage.xml\n*.cover\n*.py,cover\n.hypothesis/\n.pytest_cache/\n\n# Translations\n*.mo\n*.pot\n\n# Django stuff:\n*.log\nlocal_settings.py\ndb.sqlite3\ndb.sqlite3-journal\n\n# Flask stuff:\ninstance/\n.webassets-cache\n\n# Scrapy stuff:\n.scrapy\n\n# Sphinx documentation\ndocs/_build/\n\n# PyBuilder\ntarget/\n\n# Jupyter Notebook\n.ipynb_checkpoints\n\n# IPython\nprofile_default/\nipython_config.py\n\n# pyenv\n.python-version\n\n# pipenv\n#   According to pypa/pipenv#598, it is recommended to include Pipfile.lock in version control.\n#   However, in case of collaboration, if having platform-specific dependencies or dependencies\n#   having no cross-platform support, pipenv may install dependencies that don't work, or not\n#   install all needed dependencies.\n#Pipfile.lock\n\n# PEP 582; used by e.g. github.com/David-OConnor/pyflow\n__pypackages__/\n\n# Celery stuff\ncelerybeat-schedule\ncelerybeat.pid\n\n# SageMath parsed files\n*.sage.py\n\n# Environments\n.env\n.venv\nenv/\nvenv/\nENV/\nenv.bak/\nvenv.bak/\n\n# Spyder project settings\n.spyderproject\n.spyproject\n\n# Rope project settings\n.ropeproject\n\n# mkdocs documentation\n/site\n\n# mypy\n.mypy_cache/\n.dmypy.json\ndmypy.json\n\n# Pyre type checker\n.pyre/\n"
        },
        {
          "name": "CITATION",
          "type": "blob",
          "size": 0.2099609375,
          "content": "@article{Forsgren_Martiros_2022,\n  author = {Forsgren, Seth* and Martiros, Hayk*},\n  title = {{Riffusion - Stable diffusion for real-time music generation}},\n  url = {https://riffusion.com/about},\n  year = {2022}\n}\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 1.0458984375,
          "content": "Copyright 2022 Hayk Martiros and Seth Forsgren\n\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and\nassociated documentation files (the \"Software\"), to deal in the Software without restriction,\nincluding without limitation the rights to use, copy, modify, merge, publish, distribute,\nsublicense, and/or sell copies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all copies or substantial\nportions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT\nNOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\nNONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES\nOR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN\nCONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 6.1875,
          "content": "# :guitar: Riffusion (hobby)\n\n:no_entry: This project is no longer actively maintained.\n\n<a href=\"https://github.com/riffusion/riffusion/actions/workflows/ci.yml?query=branch%3Amain\"><img alt=\"CI status\" src=\"https://github.com/riffusion/riffusion/actions/workflows/ci.yml/badge.svg\" /></a>\n<img alt=\"Python 3.9 | 3.10\" src=\"https://img.shields.io/badge/Python-3.9%20%7C%203.10-blue\" />\n<a href=\"https://github.com/riffusion/riffusion/tree/main/LICENSE\"><img alt=\"MIT License\" src=\"https://img.shields.io/badge/License-MIT-yellowgreen\" /></a>\n\nRiffusion is a library for real-time music and audio generation with stable diffusion.\n\nRead about it at https://www.riffusion.com/about and try it at https://www.riffusion.com/.\n\nThis is the core repository for riffusion image and audio processing code.\n\n * Diffusion pipeline that performs prompt interpolation combined with image conditioning\n * Conversions between spectrogram images and audio clips\n * Command-line interface for common tasks\n * Interactive app using streamlit\n * Flask server to provide model inference via API\n * Various third party integrations\n\nRelated repositories:\n* Web app: https://github.com/riffusion/riffusion-app\n* Model checkpoint: https://huggingface.co/riffusion/riffusion-model-v1\n\n## Citation\n\nIf you build on this work, please cite it as follows:\n\n```\n@article{Forsgren_Martiros_2022,\n  author = {Forsgren, Seth* and Martiros, Hayk*},\n  title = {{Riffusion - Stable diffusion for real-time music generation}},\n  url = {https://riffusion.com/about},\n  year = {2022}\n}\n```\n\n## Install\n\nTested in CI with Python 3.9 and 3.10.\n\nIt's highly recommended to set up a virtual Python environment with `conda` or `virtualenv`:\n```\nconda create --name riffusion python=3.9\nconda activate riffusion\n```\n\nInstall Python dependencies:\n```\npython -m pip install -r requirements.txt\n```\n\nIn order to use audio formats other than WAV, [ffmpeg](https://ffmpeg.org/download.html) is required.\n```\nsudo apt-get install ffmpeg          # linux\nbrew install ffmpeg                  # mac\nconda install -c conda-forge ffmpeg  # conda\n```\n\nIf torchaudio has no backend, you may need to install `libsndfile`. See [this issue](https://github.com/riffusion/riffusion/issues/12).\n\nIf you have an issue, try upgrading [diffusers](https://github.com/huggingface/diffusers). Tested with 0.9 - 0.11.\n\nGuides:\n* [Simple Install Guide for Windows](https://www.reddit.com/r/riffusion/comments/zrubc9/installation_guide_for_riffusion_app_inference/)\n\n## Backends\n\n### CPU\n`cpu` is supported but is quite slow.\n\n### CUDA\n`cuda` is the recommended and most performant backend.\n\nTo use with CUDA, make sure you have torch and torchaudio installed with CUDA support. See the\n[install guide](https://pytorch.org/get-started/locally/) or\n[stable wheels](https://download.pytorch.org/whl/torch_stable.html).\n\nTo generate audio in real-time, you need a GPU that can run stable diffusion with approximately 50\nsteps in under five seconds, such as a 3090 or A10G.\n\nTest availability with:\n\n```python3\nimport torch\ntorch.cuda.is_available()\n```\n\n### MPS\nThe `mps` backend on Apple Silicon is supported for inference but some operations fall back to CPU,\nparticularly for audio processing. You may need to set\n`PYTORCH_ENABLE_MPS_FALLBACK=1`.\n\nIn addition, this backend is not deterministic.\n\nTest availability with:\n\n```python3\nimport torch\ntorch.backends.mps.is_available()\n```\n\n## Command-line interface\n\nRiffusion comes with a command line interface for performing common tasks.\n\nSee available commands:\n```\npython -m riffusion.cli -h\n```\n\nGet help for a specific command:\n```\npython -m riffusion.cli image-to-audio -h\n```\n\nExecute:\n```\npython -m riffusion.cli image-to-audio --image spectrogram_image.png --audio clip.wav\n```\n\n## Riffusion Playground\n\nRiffusion contains a [streamlit](https://streamlit.io/) app for interactive use and exploration.\n\nRun with:\n```\npython -m riffusion.streamlit.playground\n```\n\nAnd access at http://127.0.0.1:8501/\n\n<img alt=\"Riffusion Playground\" style=\"width: 600px\" src=\"https://i.imgur.com/OOMKBbT.png\" />\n\n## Run the model server\n\nRiffusion can be run as a flask server that provides inference via API. This server enables the [web app](https://github.com/riffusion/riffusion-app) to run locally.\n\nRun with:\n\n```\npython -m riffusion.server --host 127.0.0.1 --port 3013\n```\n\nYou can specify `--checkpoint` with your own directory or huggingface ID in diffusers format.\n\nUse the `--device` argument to specify the torch device to use.\n\nThe model endpoint is now available at `http://127.0.0.1:3013/run_inference` via POST request.\n\nExample input (see [InferenceInput](https://github.com/hmartiro/riffusion-inference/blob/main/riffusion/datatypes.py#L28) for the API):\n```\n{\n  \"alpha\": 0.75,\n  \"num_inference_steps\": 50,\n  \"seed_image_id\": \"og_beat\",\n\n  \"start\": {\n    \"prompt\": \"church bells on sunday\",\n    \"seed\": 42,\n    \"denoising\": 0.75,\n    \"guidance\": 7.0\n  },\n\n  \"end\": {\n    \"prompt\": \"jazz with piano\",\n    \"seed\": 123,\n    \"denoising\": 0.75,\n    \"guidance\": 7.0\n  }\n}\n```\n\nExample output (see [InferenceOutput](https://github.com/hmartiro/riffusion-inference/blob/main/riffusion/datatypes.py#L54) for the API):\n```\n{\n  \"image\": \"< base64 encoded JPEG image >\",\n  \"audio\": \"< base64 encoded MP3 clip >\"\n}\n```\n\n## Tests\nTests live in the `test/` directory and are implemented with `unittest`.\n\nTo run all tests:\n```\npython -m unittest test/*_test.py\n```\n\nTo run a single test:\n```\npython -m unittest test.audio_to_image_test\n```\n\nTo preserve temporary outputs for debugging, set `RIFFUSION_TEST_DEBUG`:\n```\nRIFFUSION_TEST_DEBUG=1 python -m unittest test.audio_to_image_test\n```\n\nTo run a single test case within a test:\n```\npython -m unittest test.audio_to_image_test -k AudioToImageTest.test_stereo\n```\n\nTo run tests using a specific torch device, set `RIFFUSION_TEST_DEVICE`. Tests should pass with\n`cpu`, `cuda`, and `mps` backends.\n\n## Development Guide\nInstall additional packages for dev with `python -m pip install -r requirements_dev.txt`.\n\n* Linter: `ruff`\n* Formatter: `black`\n* Type checker: `mypy`\n\nThese are configured in `pyproject.toml`.\n\nThe results of `mypy .`, `black .`, and `ruff .` *must* be clean to accept a PR.\n\nCI is run through GitHub Actions from `.github/workflows/ci.yml`.\n\nContributions are welcome through pull requests.\n"
        },
        {
          "name": "VERSION",
          "type": "blob",
          "size": 0.005859375,
          "content": "0.3.1\n"
        },
        {
          "name": "cog.yaml",
          "type": "blob",
          "size": 0.9560546875,
          "content": "# Configuration for Cog ⚙️\n# Reference: https://github.com/replicate/cog/blob/main/docs/yaml.md\n\nbuild:\n  # set to true if your model requires a GPU\n  gpu: true\n\n  # a list of ubuntu apt packages to install\n  system_packages:\n    - \"ffmpeg\"\n    - \"libsndfile1\"\n\n  # python version in the form '3.8' or '3.8.12'\n  python_version: \"3.9\"\n\n  # a list of packages in the format <package-name>==<version>\n  python_packages:\n    - \"accelerate==0.15.0\"\n    - \"argh==0.26.2\"\n    - \"dacite==1.6.0\"\n    - \"diffusers==0.10.2\"\n    - \"flask_cors==3.0.10\"\n    - \"flask==1.1.2\"\n    - \"numpy==1.19.4\"\n    - \"pillow==9.1.0\"\n    - \"pydub==0.25.1\"\n    - \"scipy==1.6.3\"\n    - \"torch==1.13.0\"\n    - \"torchaudio==0.13.0\"\n    - \"transformers==4.25.1\"\n\n  # commands run after the environment is setup\n  # run:\n    # - \"echo env is ready!\"\n    # - \"echo another command if needed\"\n\n# predict.py defines how predictions are run on your model\npredict: \"integrations/cog_riffusion.py:RiffusionPredictor\"\n"
        },
        {
          "name": "integrations",
          "type": "tree",
          "content": null
        },
        {
          "name": "pyproject.toml",
          "type": "blob",
          "size": 1.6552734375,
          "content": "[tool.black]\nline-length = 100\n\n[tool.ruff]\nline-length = 100\n\n# Which rules to run\nselect = [\n    # Pyflakes\n    \"F\",\n    # Pycodestyle\n    \"E\",\n    \"W\",\n    # isort\n    \"I001\"\n]\n\nignore = []\n\n# Exclude a variety of commonly ignored directories.\nexclude = [\n    \".bzr\",\n    \".direnv\",\n    \".eggs\",\n    \".git\",\n    \".hg\",\n    \".mypy_cache\",\n    \".nox\",\n    \".pants.d\",\n    \".ruff_cache\",\n    \".svn\",\n    \".tox\",\n    \".venv\",\n    \"__pypackages__\",\n    \"_build\",\n    \"buck-out\",\n    \"build\",\n    \"dist\",\n    \"node_modules\",\n    \"venv\",\n]\nper-file-ignores = {}\n\n# Allow unused variables when underscore-prefixed.\ndummy-variable-rgx = \"^(_+|(_+[a-zA-Z0-9_]*[a-zA-Z0-9]+?))$\"\n\n# Assume Python 3.10.\ntarget-version = \"py310\"\n\n[tool.ruff.mccabe]\n# Unlike Flake8, default to a complexity level of 10.\nmax-complexity = 10\n\n[tool.mypy]\npython_version = \"3.10\"\n\n[[tool.mypy.overrides]]\nmodule = \"argh.*\"\nignore_missing_imports = true\n\n[[tool.mypy.overrides]]\nmodule = \"cog.*\"\nignore_missing_imports = true\n\n[[tool.mypy.overrides]]\nmodule = \"diffusers.*\"\nignore_missing_imports = true\n\n[[tool.mypy.overrides]]\nmodule = \"huggingface_hub.*\"\nignore_missing_imports = true\n\n[[tool.mypy.overrides]]\nmodule = \"numpy.*\"\nignore_missing_imports = true\n\n[[tool.mypy.overrides]]\nmodule = \"plotly.*\"\nignore_missing_imports = true\n\n[[tool.mypy.overrides]]\nmodule = \"pydub.*\"\nignore_missing_imports = true\n\n[[tool.mypy.overrides]]\nmodule = \"scipy.fft.*\"\nignore_missing_imports = true\n\n[[tool.mypy.overrides]]\nmodule = \"scipy.io.*\"\nignore_missing_imports = true\n\n[[tool.mypy.overrides]]\nmodule = \"torchaudio.*\"\nignore_missing_imports = true\n\n[[tool.mypy.overrides]]\nmodule = \"transformers.*\"\nignore_missing_imports = true\n"
        },
        {
          "name": "requirements.txt",
          "type": "blob",
          "size": 0.1845703125,
          "content": "accelerate\nargh\ndacite\ndemucs\ndiffusers==0.9.0\nflask\nflask_cors\nnumpy\npillow>=9.1.0\nplotly\npydub\npysoundfile\nscipy\nsoundfile\nsox\nstreamlit>=1.18.0\ntorch\ntorchaudio\ntorchvision\ntransformers\n"
        },
        {
          "name": "requirements_all.txt",
          "type": "blob",
          "size": 0.04296875,
          "content": "-r requirements.txt\n-r requirements_dev.txt\n"
        },
        {
          "name": "requirements_dev.txt",
          "type": "blob",
          "size": 0.091796875,
          "content": "black\nipdb\nmypy\nruff\ntypes-Flask-Cors\ntypes-Pillow\ntypes-requests\ntypes-setuptools\ntypes-tqdm\n"
        },
        {
          "name": "riffusion",
          "type": "tree",
          "content": null
        },
        {
          "name": "seed_images",
          "type": "tree",
          "content": null
        },
        {
          "name": "setup.py",
          "type": "blob",
          "size": 1.12890625,
          "content": "from pathlib import Path\n\nfrom setuptools import find_packages, setup\n\n# Load the version from file\n__version__ = Path(\"VERSION\").read_text().strip()\n\n# Load packages\npackages = Path(\"requirements.txt\").read_text().splitlines()\n\nsetup(\n    name=\"riffusion\",\n    packages=find_packages(exclude=[]),\n    version=__version__,\n    license=\"MIT\",\n    description=\"Riffusion - Stable diffusion for real-time music generation\",\n    author=\"Hayk Martiros\",\n    author_email=\"hayk.mart@gmail.com\",\n    long_description_content_type=\"text/markdown\",\n    url=\"https://github.com/riffusion/riffusion\",\n    keywords=[\n        \"artificial intelligence\",\n        \"audio generation\",\n        \"music\",\n        \"diffusion\",\n        \"riffusion\",\n        \"deep learning\",\n        \"transformers\",\n    ],\n    install_requires=packages,\n    package_data={\n        \"riffusion\": [\"py.typed\"],\n    },\n    classifiers=[\n        \"Development Status :: 4 - Beta\",\n        \"Intended Audience :: Developers\",\n        \"Topic :: Scientific/Engineering :: Artificial Intelligence\",\n        \"License :: OSI Approved :: MIT License\",\n        \"Programming Language :: Python :: 3.9\",\n    ],\n)\n"
        },
        {
          "name": "test",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}