{
  "metadata": {
    "timestamp": 1736560387933,
    "page": 937,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjk0MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "TMElyralab/MuseTalk",
      "stars": 3263,
      "defaultBranch": "main",
      "files": [
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.1044921875,
          "content": ".DS_Store\n*.log\n.idea/\n.vscode/\n*.pyc\n.ipynb_checkpoints\nmodels\nresults/\ndata/audio/*.wav\ndata/video/*.mp4\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 13.1123046875,
          "content": "\nMIT License\n\nCopyright (c) 2024 Tencent Music Entertainment Group\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n\n\nOther dependencies and licenses:\n\n\nOpen Source Software Licensed under the MIT License:\n--------------------------------------------------------------------\n1. sd-vae-ft-mse\nFiles：https://huggingface.co/stabilityai/sd-vae-ft-mse/tree/main\nLicense：MIT license\nFor details：https://choosealicense.com/licenses/mit/\n\n2. whisper \nFiles：https://github.com/openai/whisper\nLicense：MIT license\n              Copyright (c) 2022 OpenAI\nFor details：https://github.com/openai/whisper/blob/main/LICENSE\n\n3. face-parsing.PyTorch\nFiles：https://github.com/zllrunning/face-parsing.PyTorch\nLicense：MIT License\n\tCopyright (c) 2019 zll\nFor details：https://github.com/zllrunning/face-parsing.PyTorch/blob/master/LICENSE\n\n\n\nOpen Source Software Licensed under the Apache License Version 2.0:\n--------------------------------------------------------------------\n1. DWpose\nFiles：https://huggingface.co/yzd-v/DWPose/tree/main\nLicense：Apache-2.0\nFor details：https://choosealicense.com/licenses/apache-2.0/\n\n\nTerms of the Apache License Version 2.0:\n--------------------------------------------------------------------\nApache License \n\nVersion 2.0, January 2004\n\nhttp://www.apache.org/licenses/ \n\nTERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n1. Definitions.\n\n\"License\" shall mean the terms and conditions for use, reproduction, and distribution as defined by Sections 1 through 9 of this document.\n\n\"Licensor\" shall mean the copyright owner or entity authorized by the copyright owner that is granting the License.\n\n\"Legal Entity\" shall mean the union of the acting entity and all other entities that control, are controlled by, or are under common control with that entity. For the purposes of this definition, \"control\" means (i) the power, direct or indirect, to cause the direction or management of such entity, whether by contract or otherwise, or (ii) ownership of fifty percent (50%) or more of the outstanding shares, or (iii) beneficial ownership of such entity.\n\n\"You\" (or \"Your\") shall mean an individual or Legal Entity exercising permissions granted by this License.\n\n\"Source\" form shall mean the preferred form for making modifications, including but not limited to software source code, documentation source, and configuration files.\n\n\"Object\" form shall mean any form resulting from mechanical transformation or translation of a Source form, including but not limited to compiled object code, generated documentation, and conversions to other media types.\n\n\"Work\" shall mean the work of authorship, whether in Source or Object form, made available under the License, as indicated by a copyright notice that is included in or attached to the work (an example is provided in the Appendix below).\n\n\"Derivative Works\" shall mean any work, whether in Source or Object form, that is based on (or derived from) the Work and for which the editorial revisions, annotations, elaborations, or other modifications represent, as a whole, an original work of authorship. For the purposes of this License, Derivative Works shall not include works that remain separable from, or merely link (or bind by name) to the interfaces of, the Work and Derivative Works thereof.\n\n\"Contribution\" shall mean any work of authorship, including the original version of the Work and any modifications or additions to that Work or Derivative Works thereof, that is intentionally submitted to Licensor for inclusion in the Work by the copyright owner or by an individual or Legal Entity authorized to submit on behalf of the copyright owner. For the purposes of this definition, \"submitted\" means any form of electronic, verbal, or written communication sent to the Licensor or its representatives, including but not limited to communication on electronic mailing lists, source code control systems, and issue tracking systems that are managed by, or on behalf of, the Licensor for the purpose of discussing and improving the Work, but excluding communication that is conspicuously marked or otherwise designated in writing by the copyright owner as \"Not a Contribution.\"\n\n\"Contributor\" shall mean Licensor and any individual or Legal Entity on behalf of whom a Contribution has been received by Licensor and subsequently incorporated within the Work.\n\n2. Grant of Copyright License. Subject to the terms and conditions of this License, each Contributor hereby grants to You a perpetual, worldwide, non-exclusive, no-charge, royalty-free, irrevocable copyright license to reproduce, prepare Derivative Works of, publicly display, publicly perform, sublicense, and distribute the Work and such Derivative Works in Source or Object form.\n\n3. Grant of Patent License. Subject to the terms and conditions of this License, each Contributor hereby grants to You a perpetual, worldwide, non-exclusive, no-charge, royalty-free, irrevocable (except as stated in this section) patent license to make, have made, use, offer to sell, sell, import, and otherwise transfer the Work, where such license applies only to those patent claims licensable by such Contributor that are necessarily infringed by their Contribution(s) alone or by combination of their Contribution(s) with the Work to which such Contribution(s) was submitted. If You institute patent litigation against any entity (including a cross-claim or counterclaim in a lawsuit) alleging that the Work or a Contribution incorporated within the Work constitutes direct or contributory patent infringement, then any patent licenses granted to You under this License for that Work shall terminate as of the date such litigation is filed.\n\n4. Redistribution. You may reproduce and distribute copies of the Work or Derivative Works thereof in any medium, with or without modifications, and in Source or Object form, provided that You meet the following conditions:\n\nYou must give any other recipients of the Work or Derivative Works a copy of this License; and \n\nYou must cause any modified files to carry prominent notices stating that You changed the files; and \n\nYou must retain, in the Source form of any Derivative Works that You distribute, all copyright, patent, trademark, and attribution notices from the Source form of the Work, excluding those notices that do not pertain to any part of the Derivative Works; and \n\nIf the Work includes a \"NOTICE\" text file as part of its distribution, then any Derivative Works that You distribute must include a readable copy of the attribution notices contained within such NOTICE file, excluding those notices that do not pertain to any part of the Derivative Works, in at least one of the following places: within a NOTICE text file distributed as part of the Derivative Works; within the Source form or documentation, if provided along with the Derivative Works; or, within a display generated by the Derivative Works, if and wherever such third-party notices normally appear. The contents of the NOTICE file are for informational purposes only and do not modify the License. You may add Your own attribution notices within Derivative Works that You distribute, alongside or as an addendum to the NOTICE text from the Work, provided that such additional attribution notices cannot be construed as modifying the License. \n\nYou may add Your own copyright statement to Your modifications and may provide additional or different license terms and conditions for use, reproduction, or distribution of Your modifications, or for any such Derivative Works as a whole, provided Your use, reproduction, and distribution of the Work otherwise complies with the conditions stated in this License. \n\n5. Submission of Contributions. Unless You explicitly state otherwise, any Contribution intentionally submitted for inclusion in the Work by You to the Licensor shall be under the terms and conditions of this License, without any additional terms or conditions. Notwithstanding the above, nothing herein shall supersede or modify the terms of any separate license agreement you may have executed with Licensor regarding such Contributions.\n\n6. Trademarks. This License does not grant permission to use the trade names, trademarks, service marks, or product names of the Licensor, except as required for reasonable and customary use in describing the origin of the Work and reproducing the content of the NOTICE file.\n\n7. Disclaimer of Warranty. Unless required by applicable law or agreed to in writing, Licensor provides the Work (and each Contributor provides its Contributions) on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied, including, without limitation, any warranties or conditions of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A PARTICULAR PURPOSE. You are solely responsible for determining the appropriateness of using or redistributing the Work and assume any risks associated with Your exercise of permissions under this License.\n\n8. Limitation of Liability. In no event and under no legal theory, whether in tort (including negligence), contract, or otherwise, unless required by applicable law (such as deliberate and grossly negligent acts) or agreed to in writing, shall any Contributor be liable to You for damages, including any direct, indirect, special, incidental, or consequential damages of any character arising as a result of this License or out of the use or inability to use the Work (including but not limited to damages for loss of goodwill, work stoppage, computer failure or malfunction, or any and all other commercial damages or losses), even if such Contributor has been advised of the possibility of such damages.\n\n9. Accepting Warranty or Additional Liability. While redistributing the Work or Derivative Works thereof, You may choose to offer, and charge a fee for, acceptance of support, warranty, indemnity, or other liability obligations and/or rights consistent with this License. However, in accepting such obligations, You may act only on Your own behalf and on Your sole responsibility, not on behalf of any other Contributor, and only if You agree to indemnify, defend, and hold each Contributor harmless for any liability incurred by, or claims asserted against, such Contributor by reason of your accepting any such warranty or additional liability.\n\nEND OF TERMS AND CONDITIONS\n \n\n\nOpen Source Software Licensed under the BSD 3-Clause License:\n--------------------------------------------------------------------\n1. face-alignment\nFiles：https://github.com/1adrianb/face-alignment/tree/master\nLicense：BSD 3-Clause License\n\tCopyright (c) 2017, Adrian Bulat\n\tAll rights reserved.\nFor details：https://github.com/1adrianb/face-alignment/blob/master/LICENSE\n\n\nTerms of the BSD 3-Clause License:\n--------------------------------------------------------------------\nRedistribution and use in source and binary forms, with or without\nmodification, are permitted provided that the following conditions are met:\n\n* Redistributions of source code must retain the above copyright notice, this\n  list of conditions and the following disclaimer.\n\n* Redistributions in binary form must reproduce the above copyright notice,\n  this list of conditions and the following disclaimer in the documentation\n  and/or other materials provided with the distribution.\n\n* Neither the name of the copyright holder nor the names of its\n  contributors may be used to endorse or promote products derived from\n  this software without specific prior written permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\nAND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\nIMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\nDISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE\nFOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\nDAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\nSERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\nCAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,\nOR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\nOF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\n\nOpen Source Software：\n--------------------------------------------------------------------\n1.s3FD\nFiles：https://github.com/yxlijun/S3FD.pytorch\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 15.47265625,
          "content": "# MuseTalk\n\nMuseTalk: Real-Time High Quality Lip Synchronization with Latent Space Inpainting\n</br>\nYue Zhang <sup>\\*</sup>,\nMinhao Liu<sup>\\*</sup>,\nZhaokang Chen,\nBin Wu<sup>†</sup>,\nYubin Zeng, \nChao Zhan,\nYingjie He,\nJunxin Huang,\nWenjiang Zhou\n(<sup>*</sup>Equal Contribution, <sup>†</sup>Corresponding Author, benbinwu@tencent.com)\n\nLyra Lab, Tencent Music Entertainment\n\n**[github](https://github.com/TMElyralab/MuseTalk)**    **[huggingface](https://huggingface.co/TMElyralab/MuseTalk)**    **[space](https://huggingface.co/spaces/TMElyralab/MuseTalk)**    **[Technical report](https://arxiv.org/abs/2410.10122)**\n\nWe introduce `MuseTalk`, a **real-time high quality** lip-syncing model (30fps+ on an NVIDIA Tesla V100). MuseTalk can be applied with input videos, e.g., generated by [MuseV](https://github.com/TMElyralab/MuseV), as a complete virtual human solution.\n\n:new: Update: We are thrilled to announce that [MusePose](https://github.com/TMElyralab/MusePose/) has been released. MusePose is an image-to-video generation framework for virtual human under control signal like pose. Together with MuseV and MuseTalk, we hope the community can join us and march towards the vision where a virtual human can be generated end2end with native ability of full body movement and interaction.\n\n# Overview\n`MuseTalk` is a real-time high quality audio-driven lip-syncing model trained in the latent space of `ft-mse-vae`, which\n\n1. modifies an unseen face according to the input audio, with a size of face region of `256 x 256`.\n1. supports audio in various languages, such as Chinese, English, and Japanese.\n1. supports real-time inference with 30fps+ on an NVIDIA Tesla V100.\n1. supports modification of the center point of the face region proposes, which **SIGNIFICANTLY** affects generation results. \n1. checkpoint available trained on the HDTF dataset.\n1. training codes (comming soon).\n\n# News\n- [04/02/2024] Release MuseTalk project and pretrained models.\n- [04/16/2024] Release Gradio [demo](https://huggingface.co/spaces/TMElyralab/MuseTalk) on HuggingFace Spaces (thanks to HF team for their community grant)\n- [04/17/2024] : We release a pipeline that utilizes MuseTalk for real-time inference.\n- [10/18/2024] :mega: We release the [technical report](https://arxiv.org/abs/2410.10122). Our report details a superior model to the open-source L1 loss version. It includes GAN and perceptual losses for improved clarity, and sync loss for enhanced performance.\n\n## Model\n![Model Structure](assets/figs/musetalk_arc.jpg)\nMuseTalk was trained in latent spaces, where the images were encoded by a freezed VAE. The audio was encoded by a freezed `whisper-tiny` model. The architecture of the generation network was borrowed from the UNet of the `stable-diffusion-v1-4`, where the audio embeddings were fused to the image embeddings by cross-attention. \n\nNote that although we use a very similar architecture as Stable Diffusion, MuseTalk is distinct in that it is **NOT** a diffusion model. Instead, MuseTalk operates by inpainting in the latent space with a single step.\n\n## Cases\n### MuseV + MuseTalk make human photos alive！\n<table class=\"center\">\n  <tr style=\"font-weight: bolder;text-align:center;\">\n        <td width=\"33%\">Image</td>\n        <td width=\"33%\">MuseV</td>\n        <td width=\"33%\">+MuseTalk</td>\n  </tr>\n  <tr>\n    <td>\n      <img src=assets/demo/musk/musk.png width=\"95%\">\n    </td>\n    <td >\n      <video src=https://github.com/TMElyralab/MuseTalk/assets/163980830/4a4bb2d1-9d14-4ca9-85c8-7f19c39f712e controls preload></video>\n    </td>\n    <td >\n      <video src=https://github.com/TMElyralab/MuseTalk/assets/163980830/b2a879c2-e23a-4d39-911d-51f0343218e4 controls preload></video>\n    </td>\n  </tr>\n  <tr>\n    <td>\n      <img src=assets/demo/yongen/yongen.jpeg width=\"95%\">\n    </td>\n    <td >\n      <video src=https://github.com/TMElyralab/MuseTalk/assets/163980830/57ef9dee-a9fd-4dc8-839b-3fbbbf0ff3f4 controls preload></video>\n    </td>\n    <td >\n      <video src=https://github.com/TMElyralab/MuseTalk/assets/163980830/94d8dcba-1bcd-4b54-9d1d-8b6fc53228f0 controls preload></video>\n    </td>\n  </tr>\n  <tr>\n    <td>\n      <img src=assets/demo/sit/sit.jpeg width=\"95%\">\n    </td>\n    <td >\n      <video src=https://github.com/TMElyralab/MuseTalk/assets/163980830/5fbab81b-d3f2-4c75-abb5-14c76e51769e controls preload></video>\n    </td>\n    <td >\n      <video src=https://github.com/TMElyralab/MuseTalk/assets/163980830/f8100f4a-3df8-4151-8de2-291b09269f66 controls preload></video>\n    </td>\n  </tr>\n   <tr>\n    <td>\n      <img src=assets/demo/man/man.png width=\"95%\">\n    </td>\n    <td >\n      <video src=https://github.com/TMElyralab/MuseTalk/assets/163980830/a6e7d431-5643-4745-9868-8b423a454153 controls preload></video>\n    </td>\n    <td >\n      <video src=https://github.com/TMElyralab/MuseTalk/assets/163980830/6ccf7bc7-cb48-42de-85bd-076d5ee8a623 controls preload></video>\n    </td>\n  </tr>\n  <tr>\n    <td>\n      <img src=assets/demo/monalisa/monalisa.png width=\"95%\">\n    </td>\n    <td >\n      <video src=https://github.com/TMElyralab/MuseTalk/assets/163980830/1568f604-a34f-4526-a13a-7d282aa2e773 controls preload></video>\n    </td>\n    <td >\n      <video src=https://github.com/TMElyralab/MuseTalk/assets/163980830/a40784fc-a885-4c1f-9b7e-8f87b7caf4e0 controls preload></video>\n    </td>\n  </tr>\n  <tr>\n    <td>\n      <img src=assets/demo/sun1/sun.png width=\"95%\">\n    </td>\n    <td >\n      <video src=https://github.com/TMElyralab/MuseTalk/assets/163980830/37a3a666-7b90-4244-8d3a-058cb0e44107 controls preload></video>\n    </td>\n    <td >\n      <video src=https://github.com/TMElyralab/MuseTalk/assets/163980830/172f4ff1-d432-45bd-a5a7-a07dec33a26b controls preload></video>\n    </td>\n  </tr>\n  <tr>\n    <td>\n      <img src=assets/demo/sun2/sun.png width=\"95%\">\n    </td>\n    <td >\n      <video src=https://github.com/TMElyralab/MuseTalk/assets/163980830/37a3a666-7b90-4244-8d3a-058cb0e44107 controls preload></video>\n    </td>\n    <td >\n      <video src=https://github.com/TMElyralab/MuseTalk/assets/163980830/85a6873d-a028-4cce-af2b-6c59a1f2971d controls preload></video>\n    </td>\n  </tr>\n</table >\n\n* The character of the last two rows, `Xinying Sun`, is a supermodel KOL. You can follow her on [douyin](https://www.douyin.com/user/MS4wLjABAAAAWDThbMPN_6Xmm_JgXexbOii1K-httbu2APdG8DvDyM8).\n\n## Video dubbing\n<table class=\"center\">\n  <tr style=\"font-weight: bolder;text-align:center;\">\n        <td width=\"70%\">MuseTalk</td>\n        <td width=\"30%\">Original videos</td>\n  </tr>\n  <tr>\n    <td>\n      <video src=https://github.com/TMElyralab/MuseTalk/assets/163980830/4d7c5fa1-3550-4d52-8ed2-52f158150f24 controls preload></video>\n    </td>\n    <td>\n      <a href=\"//www.bilibili.com/video/BV1wT411b7HU\">Link</a>\n      <href src=\"\"></href>\n    </td>\n  </tr>\n</table>\n\n* For video dubbing, we applied a self-developed tool which can identify the talking person. \n\n## Some interesting videos!\n<table class=\"center\">\n  <tr style=\"font-weight: bolder;text-align:center;\">\n        <td width=\"50%\">Image</td>\n        <td width=\"50%\">MuseV + MuseTalk</td>\n  </tr>\n  <tr>\n    <td>\n      <img src=assets/demo/video1/video1.png width=\"95%\">\n    </td>\n    <td>\n      <video src=https://github.com/TMElyralab/MuseTalk/assets/163980830/1f02f9c6-8b98-475e-86b8-82ebee82fe0d controls preload></video>\n    </td>\n  </tr>\n</table>\n\n# TODO:\n- [x] trained models and inference codes.\n- [x] Huggingface Gradio [demo](https://huggingface.co/spaces/TMElyralab/MuseTalk).\n- [x] codes for real-time inference.\n- [ ] technical report.\n- [ ] training codes.\n- [ ] a better model (may take longer).\n\n\n# Getting Started\nWe provide a detailed tutorial about the installation and the basic usage of MuseTalk for new users:\n\n## Third party integration\nThanks for the third-party integration, which makes installation and use more convenient for everyone.\nWe also hope you note that we have not verified, maintained, or updated third-party. Please refer to this project for specific results.\n\n### [ComfyUI](https://github.com/chaojie/ComfyUI-MuseTalk)\n\n## Installation\nTo prepare the Python environment and install additional packages such as opencv, diffusers, mmcv, etc., please follow the steps below:\n### Build environment\n\nWe recommend a python version >=3.10 and cuda version =11.7. Then build environment as follows:\n\n```shell\npip install -r requirements.txt\n```\n\n### mmlab packages\n```bash\npip install --no-cache-dir -U openmim \nmim install mmengine \nmim install \"mmcv>=2.0.1\" \nmim install \"mmdet>=3.1.0\" \nmim install \"mmpose>=1.1.0\" \n```\n\n### Download ffmpeg-static\nDownload the ffmpeg-static and\n```\nexport FFMPEG_PATH=/path/to/ffmpeg\n```\nfor example:\n```\nexport FFMPEG_PATH=/musetalk/ffmpeg-4.4-amd64-static\n```\n### Download weights\nYou can download weights manually as follows:\n\n1. Download our trained [weights](https://huggingface.co/TMElyralab/MuseTalk).\n\n2. Download the weights of other components:\n   - [sd-vae-ft-mse](https://huggingface.co/stabilityai/sd-vae-ft-mse)\n   - [whisper](https://openaipublic.azureedge.net/main/whisper/models/65147644a518d12f04e32d6f3b26facc3f8dd46e5390956a9424a650c0ce22b9/tiny.pt)\n   - [dwpose](https://huggingface.co/yzd-v/DWPose/tree/main)\n   - [face-parse-bisent](https://github.com/zllrunning/face-parsing.PyTorch)\n   - [resnet18](https://download.pytorch.org/models/resnet18-5c106cde.pth)\n\n\nFinally, these weights should be organized in `models` as follows:\n```\n./models/\n├── musetalk\n│   └── musetalk.json\n│   └── pytorch_model.bin\n├── dwpose\n│   └── dw-ll_ucoco_384.pth\n├── face-parse-bisent\n│   ├── 79999_iter.pth\n│   └── resnet18-5c106cde.pth\n├── sd-vae-ft-mse\n│   ├── config.json\n│   └── diffusion_pytorch_model.bin\n└── whisper\n    └── tiny.pt\n```\n## Quickstart\n\n### Inference\nHere, we provide the inference script. \n```\npython -m scripts.inference --inference_config configs/inference/test.yaml \n```\nconfigs/inference/test.yaml is the path to the inference configuration file, including video_path and audio_path.\nThe video_path should be either a video file, an image file or a directory of images. \n\nYou are recommended to input video with `25fps`, the same fps used when training the model. If your video is far less than 25fps, you are recommended to apply frame interpolation or directly convert the video to 25fps using ffmpeg.\n\n#### Use of bbox_shift to have adjustable results\n:mag_right: We have found that upper-bound of the mask has an important impact on mouth openness. Thus, to control the mask region, we suggest using the `bbox_shift` parameter. Positive values (moving towards the lower half) increase mouth openness, while negative values (moving towards the upper half) decrease mouth openness.\n\nYou can start by running with the default configuration to obtain the adjustable value range, and then re-run the script within this range. \n\nFor example, in the case of `Xinying Sun`, after running the default configuration, it shows that the adjustable value rage is [-9, 9]. Then, to decrease the mouth openness, we set the value to be `-7`. \n```\npython -m scripts.inference --inference_config configs/inference/test.yaml --bbox_shift -7 \n```\n:pushpin: More technical details can be found in [bbox_shift](assets/BBOX_SHIFT.md).\n\n#### Combining MuseV and MuseTalk\n\nAs a complete solution to virtual human generation, you are suggested to first apply [MuseV](https://github.com/TMElyralab/MuseV) to generate a video (text-to-video, image-to-video or pose-to-video) by referring [this](https://github.com/TMElyralab/MuseV?tab=readme-ov-file#text2video). Frame interpolation is suggested to increase frame rate. Then, you can use `MuseTalk` to generate a lip-sync video by referring [this](https://github.com/TMElyralab/MuseTalk?tab=readme-ov-file#inference).\n\n#### :new: Real-time inference\n\nHere, we provide the inference script. This script first applies necessary pre-processing such as face detection, face parsing and VAE encode in advance. During inference, only UNet and the VAE decoder are involved, which makes MuseTalk real-time.\n\n```\npython -m scripts.realtime_inference --inference_config configs/inference/realtime.yaml --batch_size 4\n```\nconfigs/inference/realtime.yaml is the path to the real-time inference configuration file, including `preparation`, `video_path` , `bbox_shift` and `audio_clips`. \n\n1. Set `preparation` to `True` in `realtime.yaml` to prepare the materials for a new `avatar`. (If the `bbox_shift` has changed, you also need to re-prepare the materials.)\n1. After that, the `avatar` will use an audio clip selected from `audio_clips` to generate video.\n    ```\n    Inferring using: data/audio/yongen.wav\n    ```\n1. While MuseTalk is inferring, sub-threads can simultaneously stream the results to the users. The generation process can achieve 30fps+ on an NVIDIA Tesla V100.\n1. Set `preparation` to `False` and run this script if you want to genrate more videos using the same avatar.\n\n##### Note for Real-time inference\n1. If you want to generate multiple videos using the same avatar/video, you can also use this script to **SIGNIFICANTLY** expedite the generation process.\n1. In the previous script, the generation time is also limited by I/O (e.g. saving images). If you just want to test the generation speed without saving the images, you can run\n```\npython -m scripts.realtime_inference --inference_config configs/inference/realtime.yaml --skip_save_images\n```\n\n# Acknowledgement\n1. We thank open-source components like [whisper](https://github.com/openai/whisper), [dwpose](https://github.com/IDEA-Research/DWPose), [face-alignment](https://github.com/1adrianb/face-alignment), [face-parsing](https://github.com/zllrunning/face-parsing.PyTorch), [S3FD](https://github.com/yxlijun/S3FD.pytorch). \n1. MuseTalk has referred much to [diffusers](https://github.com/huggingface/diffusers) and [isaacOnline/whisper](https://github.com/isaacOnline/whisper/tree/extract-embeddings).\n1. MuseTalk has been built on [HDTF](https://github.com/MRzzm/HDTF) datasets.\n\nThanks for open-sourcing!\n\n# Limitations\n- Resolution: Though MuseTalk uses a face region size of 256 x 256, which make it better than other open-source methods, it has not yet reached the theoretical resolution bound. We will continue to deal with this problem.  \nIf you need higher resolution, you could apply super resolution models such as [GFPGAN](https://github.com/TencentARC/GFPGAN) in combination with MuseTalk.\n\n- Identity preservation: Some details of the original face are not well preserved, such as mustache, lip shape and color.\n\n- Jitter: There exists some jitter as the current pipeline adopts single-frame generation.\n\n# Citation\n```bib\n@article{musetalk,\n  title={MuseTalk: Real-Time High Quality Lip Synchorization with Latent Space Inpainting},\n  author={Zhang, Yue and Liu, Minhao and Chen, Zhaokang and Wu, Bin and Zeng, Yubin and Zhan, Chao and He, Yingjie and Huang, Junxin and Zhou, Wenjiang},\n  journal={arxiv},\n  year={2024}\n}\n```\n# Disclaimer/License\n1. `code`: The code of MuseTalk is released under the MIT License. There is no limitation for both academic and commercial usage.\n1. `model`: The trained model are available for any purpose, even commercially.\n1. `other opensource model`: Other open-source models used must comply with their license, such as `whisper`, `ft-mse-vae`, `dwpose`, `S3FD`, etc..\n1. The testdata are collected from internet, which are available for non-commercial research purposes only.\n1. `AIGC`: This project strives to impact the domain of AI-driven video generation positively. Users are granted the freedom to create videos using this tool, but they are expected to comply with local laws and utilize it responsibly. The developers do not assume any responsibility for potential misuse by users.\n"
        },
        {
          "name": "app.py",
          "type": "blob",
          "size": 15.9033203125,
          "content": "import os\nimport time\nimport pdb\nimport re\n\nimport gradio as gr\nimport spaces\nimport numpy as np\nimport sys\nimport subprocess\n\nfrom huggingface_hub import snapshot_download\nimport requests\n\nimport argparse\nimport os\nfrom omegaconf import OmegaConf\nimport numpy as np\nimport cv2\nimport torch\nimport glob\nimport pickle\nfrom tqdm import tqdm\nimport copy\nfrom argparse import Namespace\nimport shutil\nimport gdown\nimport imageio\nimport ffmpeg\nfrom moviepy.editor import *\n\n\nProjectDir = os.path.abspath(os.path.dirname(__file__))\nCheckpointsDir = os.path.join(ProjectDir, \"models\")\n\ndef print_directory_contents(path):\n    for child in os.listdir(path):\n        child_path = os.path.join(path, child)\n        if os.path.isdir(child_path):\n            print(child_path)\n\ndef download_model():\n    if not os.path.exists(CheckpointsDir):\n        os.makedirs(CheckpointsDir)\n        print(\"Checkpoint Not Downloaded, start downloading...\")\n        tic = time.time()\n        snapshot_download(\n            repo_id=\"TMElyralab/MuseTalk\",\n            local_dir=CheckpointsDir,\n            max_workers=8,\n            local_dir_use_symlinks=True,\n            force_download=True, resume_download=False\n        )\n        # weight\n        os.makedirs(f\"{CheckpointsDir}/sd-vae-ft-mse/\")\n        snapshot_download(\n            repo_id=\"stabilityai/sd-vae-ft-mse\",\n            local_dir=CheckpointsDir+'/sd-vae-ft-mse',\n            max_workers=8,\n            local_dir_use_symlinks=True,\n            force_download=True, resume_download=False\n        )\n        #dwpose\n        os.makedirs(f\"{CheckpointsDir}/dwpose/\")\n        snapshot_download(\n            repo_id=\"yzd-v/DWPose\",\n            local_dir=CheckpointsDir+'/dwpose',\n            max_workers=8,\n            local_dir_use_symlinks=True,\n            force_download=True, resume_download=False\n        )\n        #vae\n        url = \"https://openaipublic.azureedge.net/main/whisper/models/65147644a518d12f04e32d6f3b26facc3f8dd46e5390956a9424a650c0ce22b9/tiny.pt\"\n        response = requests.get(url)\n        # 确保请求成功\n        if response.status_code == 200:\n            # 指定文件保存的位置\n            file_path = f\"{CheckpointsDir}/whisper/tiny.pt\"\n            os.makedirs(f\"{CheckpointsDir}/whisper/\")\n            # 将文件内容写入指定位置\n            with open(file_path, \"wb\") as f:\n                f.write(response.content)\n        else:\n            print(f\"请求失败，状态码：{response.status_code}\")\n        #gdown face parse\n        url = \"https://drive.google.com/uc?id=154JgKpzCPW82qINcVieuPH3fZ2e0P812\"\n        os.makedirs(f\"{CheckpointsDir}/face-parse-bisent/\")\n        file_path = f\"{CheckpointsDir}/face-parse-bisent/79999_iter.pth\"\n        gdown.download(url, file_path, quiet=False)\n        #resnet\n        url = \"https://download.pytorch.org/models/resnet18-5c106cde.pth\"\n        response = requests.get(url)\n        # 确保请求成功\n        if response.status_code == 200:\n            # 指定文件保存的位置\n            file_path = f\"{CheckpointsDir}/face-parse-bisent/resnet18-5c106cde.pth\"\n            # 将文件内容写入指定位置\n            with open(file_path, \"wb\") as f:\n                f.write(response.content)\n        else:\n            print(f\"请求失败，状态码：{response.status_code}\")\n\n\n        toc = time.time()\n\n        print(f\"download cost {toc-tic} seconds\")\n        print_directory_contents(CheckpointsDir)\n\n    else:\n        print(\"Already download the model.\")\n\n\n\n\n\ndownload_model()  # for huggingface deployment.\n\n\nfrom musetalk.utils.utils import get_file_type,get_video_fps,datagen\nfrom musetalk.utils.preprocessing import get_landmark_and_bbox,read_imgs,coord_placeholder,get_bbox_range\nfrom musetalk.utils.blending import get_image\nfrom musetalk.utils.utils import load_all_model\n\n\n\n\n\n\n@spaces.GPU(duration=600)\n@torch.no_grad()\ndef inference(audio_path,video_path,bbox_shift,progress=gr.Progress(track_tqdm=True)):\n    args_dict={\"result_dir\":'./results/output', \"fps\":25, \"batch_size\":8, \"output_vid_name\":'', \"use_saved_coord\":False}#same with inferenece script\n    args = Namespace(**args_dict)\n\n    input_basename = os.path.basename(video_path).split('.')[0]\n    audio_basename  = os.path.basename(audio_path).split('.')[0]\n    output_basename = f\"{input_basename}_{audio_basename}\"\n    result_img_save_path = os.path.join(args.result_dir, output_basename) # related to video & audio inputs\n    crop_coord_save_path = os.path.join(result_img_save_path, input_basename+\".pkl\") # only related to video input\n    os.makedirs(result_img_save_path,exist_ok =True)\n\n    if args.output_vid_name==\"\":\n        output_vid_name = os.path.join(args.result_dir, output_basename+\".mp4\")\n    else:\n        output_vid_name = os.path.join(args.result_dir, args.output_vid_name)\n    ############################################## extract frames from source video ##############################################\n    if get_file_type(video_path)==\"video\":\n        save_dir_full = os.path.join(args.result_dir, input_basename)\n        os.makedirs(save_dir_full,exist_ok = True)\n        # cmd = f\"ffmpeg -v fatal -i {video_path} -start_number 0 {save_dir_full}/%08d.png\"\n        # os.system(cmd)\n        # 读取视频\n        reader = imageio.get_reader(video_path)\n\n        # 保存图片\n        for i, im in enumerate(reader):\n            imageio.imwrite(f\"{save_dir_full}/{i:08d}.png\", im)\n        input_img_list = sorted(glob.glob(os.path.join(save_dir_full, '*.[jpJP][pnPN]*[gG]')))\n        fps = get_video_fps(video_path)\n    else: # input img folder\n        input_img_list = glob.glob(os.path.join(video_path, '*.[jpJP][pnPN]*[gG]'))\n        input_img_list = sorted(input_img_list, key=lambda x: int(os.path.splitext(os.path.basename(x))[0]))\n        fps = args.fps\n    #print(input_img_list)\n    ############################################## extract audio feature ##############################################\n    whisper_feature = audio_processor.audio2feat(audio_path)\n    whisper_chunks = audio_processor.feature2chunks(feature_array=whisper_feature,fps=fps)\n    ############################################## preprocess input image  ##############################################\n    if os.path.exists(crop_coord_save_path) and args.use_saved_coord:\n        print(\"using extracted coordinates\")\n        with open(crop_coord_save_path,'rb') as f:\n            coord_list = pickle.load(f)\n        frame_list = read_imgs(input_img_list)\n    else:\n        print(\"extracting landmarks...time consuming\")\n        coord_list, frame_list = get_landmark_and_bbox(input_img_list, bbox_shift)\n        with open(crop_coord_save_path, 'wb') as f:\n            pickle.dump(coord_list, f)\n    bbox_shift_text=get_bbox_range(input_img_list, bbox_shift)\n    i = 0\n    input_latent_list = []\n    for bbox, frame in zip(coord_list, frame_list):\n        if bbox == coord_placeholder:\n            continue\n        x1, y1, x2, y2 = bbox\n        crop_frame = frame[y1:y2, x1:x2]\n        crop_frame = cv2.resize(crop_frame,(256,256),interpolation = cv2.INTER_LANCZOS4)\n        latents = vae.get_latents_for_unet(crop_frame)\n        input_latent_list.append(latents)\n\n    # to smooth the first and the last frame\n    frame_list_cycle = frame_list + frame_list[::-1]\n    coord_list_cycle = coord_list + coord_list[::-1]\n    input_latent_list_cycle = input_latent_list + input_latent_list[::-1]\n    ############################################## inference batch by batch ##############################################\n    print(\"start inference\")\n    video_num = len(whisper_chunks)\n    batch_size = args.batch_size\n    gen = datagen(whisper_chunks,input_latent_list_cycle,batch_size)\n    res_frame_list = []\n    for i, (whisper_batch,latent_batch) in enumerate(tqdm(gen,total=int(np.ceil(float(video_num)/batch_size)))):\n        \n        tensor_list = [torch.FloatTensor(arr) for arr in whisper_batch]\n        audio_feature_batch = torch.stack(tensor_list).to(unet.device) # torch, B, 5*N,384\n        audio_feature_batch = pe(audio_feature_batch)\n        \n        pred_latents = unet.model(latent_batch, timesteps, encoder_hidden_states=audio_feature_batch).sample\n        recon = vae.decode_latents(pred_latents)\n        for res_frame in recon:\n            res_frame_list.append(res_frame)\n            \n    ############################################## pad to full image ##############################################\n    print(\"pad talking image to original video\")\n    for i, res_frame in enumerate(tqdm(res_frame_list)):\n        bbox = coord_list_cycle[i%(len(coord_list_cycle))]\n        ori_frame = copy.deepcopy(frame_list_cycle[i%(len(frame_list_cycle))])\n        x1, y1, x2, y2 = bbox\n        try:\n            res_frame = cv2.resize(res_frame.astype(np.uint8),(x2-x1,y2-y1))\n        except:\n    #                 print(bbox)\n            continue\n        \n        combine_frame = get_image(ori_frame,res_frame,bbox)\n        cv2.imwrite(f\"{result_img_save_path}/{str(i).zfill(8)}.png\",combine_frame)\n        \n    # cmd_img2video = f\"ffmpeg -y -v fatal -r {fps} -f image2 -i {result_img_save_path}/%08d.png -vcodec libx264 -vf format=rgb24,scale=out_color_matrix=bt709,format=yuv420p temp.mp4\"\n    # print(cmd_img2video)\n    # os.system(cmd_img2video)\n    # 帧率\n    fps = 25\n    # 图片路径\n    # 输出视频路径\n    output_video = 'temp.mp4'\n\n    # 读取图片\n    def is_valid_image(file):\n        pattern = re.compile(r'\\d{8}\\.png')\n        return pattern.match(file)\n\n    images = []\n    files = [file for file in os.listdir(result_img_save_path) if is_valid_image(file)]\n    files.sort(key=lambda x: int(x.split('.')[0]))\n\n    for file in files:\n        filename = os.path.join(result_img_save_path, file)\n        images.append(imageio.imread(filename))\n        \n\n    # 保存视频\n    imageio.mimwrite(output_video, images, 'FFMPEG', fps=fps, codec='libx264', pixelformat='yuv420p')\n\n    # cmd_combine_audio = f\"ffmpeg -y -v fatal -i {audio_path} -i temp.mp4 {output_vid_name}\"\n    # print(cmd_combine_audio)\n    # os.system(cmd_combine_audio)\n\n    input_video = './temp.mp4'\n    # Check if the input_video and audio_path exist\n    if not os.path.exists(input_video):\n        raise FileNotFoundError(f\"Input video file not found: {input_video}\")\n    if not os.path.exists(audio_path):\n        raise FileNotFoundError(f\"Audio file not found: {audio_path}\")\n    \n    # 读取视频\n    reader = imageio.get_reader(input_video)\n    fps = reader.get_meta_data()['fps']  # 获取原视频的帧率\n    reader.close() # 否则在win11上会报错：PermissionError: [WinError 32] 另一个程序正在使用此文件，进程无法访问。: 'temp.mp4'\n    # 将帧存储在列表中\n    frames = images\n\n    # 保存视频并添加音频\n    # imageio.mimwrite(output_vid_name, frames, 'FFMPEG', fps=fps, codec='libx264', audio_codec='aac', input_params=['-i', audio_path])\n    \n    # input_video = ffmpeg.input(input_video)\n    \n    # input_audio = ffmpeg.input(audio_path)\n    \n    print(len(frames))\n\n    # imageio.mimwrite(\n    #     output_video,\n    #     frames,\n    #     'FFMPEG',\n    #     fps=25,\n    #     codec='libx264',\n    #     audio_codec='aac',\n    #     input_params=['-i', audio_path],\n    #     output_params=['-y'],  # Add the '-y' flag to overwrite the output file if it exists\n    # )\n    # writer = imageio.get_writer(output_vid_name, fps = 25, codec='libx264', quality=10, pixelformat='yuvj444p')\n    # for im in frames:\n    #     writer.append_data(im)\n    # writer.close()\n\n\n\n\n    # Load the video\n    video_clip = VideoFileClip(input_video)\n\n    # Load the audio\n    audio_clip = AudioFileClip(audio_path)\n\n    # Set the audio to the video\n    video_clip = video_clip.set_audio(audio_clip)\n\n    # Write the output video\n    video_clip.write_videofile(output_vid_name, codec='libx264', audio_codec='aac',fps=25)\n\n    os.remove(\"temp.mp4\")\n    #shutil.rmtree(result_img_save_path)\n    print(f\"result is save to {output_vid_name}\")\n    return output_vid_name,bbox_shift_text\n\n\n\n# load model weights\naudio_processor,vae,unet,pe  = load_all_model()\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ntimesteps = torch.tensor([0], device=device)\n\n\n\n\ndef check_video(video):\n    if not isinstance(video, str):\n        return video # in case of none type\n    # Define the output video file name\n    dir_path, file_name = os.path.split(video)\n    if file_name.startswith(\"outputxxx_\"):\n        return video\n    # Add the output prefix to the file name\n    output_file_name = \"outputxxx_\" + file_name\n\n    os.makedirs('./results',exist_ok=True)\n    os.makedirs('./results/output',exist_ok=True)\n    os.makedirs('./results/input',exist_ok=True)\n\n    # Combine the directory path and the new file name\n    output_video = os.path.join('./results/input', output_file_name)\n\n\n    # # Run the ffmpeg command to change the frame rate to 25fps\n    # command = f\"ffmpeg -i {video} -r 25 -vcodec libx264 -vtag hvc1 -pix_fmt yuv420p crf 18   {output_video}  -y\"\n\n    # read video\n    reader = imageio.get_reader(video)\n    fps = reader.get_meta_data()['fps']  # get fps from original video\n\n    # conver fps to 25\n    frames = [im for im in reader]\n    target_fps = 25\n    \n    L = len(frames)\n    L_target = int(L / fps * target_fps)\n    original_t = [x / fps for x in range(1, L+1)]\n    t_idx = 0\n    target_frames = []\n    for target_t in range(1, L_target+1):\n        while target_t / target_fps > original_t[t_idx]:\n            t_idx += 1      # find the first t_idx so that target_t / target_fps <= original_t[t_idx]\n            if t_idx >= L:\n                break\n        target_frames.append(frames[t_idx])\n\n    # save video\n    imageio.mimwrite(output_video, target_frames, 'FFMPEG', fps=25, codec='libx264', quality=9, pixelformat='yuv420p')\n    return output_video\n\n\n\n\ncss = \"\"\"#input_img {max-width: 1024px !important} #output_vid {max-width: 1024px; max-height: 576px}\"\"\"\n\nwith gr.Blocks(css=css) as demo:\n    gr.Markdown(\n        \"<div align='center'> <h1>MuseTalk: Real-Time High Quality Lip Synchronization with Latent Space Inpainting </span> </h1> \\\n                    <h2 style='font-weight: 450; font-size: 1rem; margin: 0rem'>\\\n                    </br>\\\n                    Yue Zhang <sup>\\*</sup>,\\\n                    Minhao Liu<sup>\\*</sup>,\\\n                    Zhaokang Chen,\\\n                    Bin Wu<sup>†</sup>,\\\n                    Yingjie He,\\\n                    Chao Zhan,\\\n                    Wenjiang Zhou\\\n                    (<sup>*</sup>Equal Contribution, <sup>†</sup>Corresponding Author, benbinwu@tencent.com)\\\n                    Lyra Lab, Tencent Music Entertainment\\\n                </h2> \\\n                <a style='font-size:18px;color: #000000' href='https://github.com/TMElyralab/MuseTalk'>[Github Repo]</a>\\\n                <a style='font-size:18px;color: #000000' href='https://github.com/TMElyralab/MuseTalk'>[Huggingface]</a>\\\n                <a style='font-size:18px;color: #000000' href=''> [Technical report(Coming Soon)] </a>\\\n                <a style='font-size:18px;color: #000000' href=''> [Project Page(Coming Soon)] </a>  </div>\"\n    )\n\n    with gr.Row():\n        with gr.Column():\n            audio = gr.Audio(label=\"Driven Audio\",type=\"filepath\")\n            video = gr.Video(label=\"Reference Video\",sources=['upload'])\n            bbox_shift = gr.Number(label=\"BBox_shift value, px\", value=0)\n            bbox_shift_scale = gr.Textbox(label=\"BBox_shift recommend value lower bound,The corresponding bbox range is generated after the initial result is generated. \\n If the result is not good, it can be adjusted according to this reference value\", value=\"\",interactive=False)\n\n            btn = gr.Button(\"Generate\")\n        out1 = gr.Video()\n    \n    video.change(\n        fn=check_video, inputs=[video], outputs=[video]\n    )\n    btn.click(\n        fn=inference,\n        inputs=[\n            audio,\n            video,\n            bbox_shift,\n        ],\n        outputs=[out1,bbox_shift_scale]\n    )\n\n# Set the IP and port\nip_address = \"0.0.0.0\"  # Replace with your desired IP address\nport_number = 7860  # Replace with your desired port number\n\n\ndemo.queue().launch(\n    share=False , debug=True, server_name=ip_address, server_port=port_number\n)\n"
        },
        {
          "name": "assets",
          "type": "tree",
          "content": null
        },
        {
          "name": "configs",
          "type": "tree",
          "content": null
        },
        {
          "name": "data",
          "type": "tree",
          "content": null
        },
        {
          "name": "entrypoint.sh",
          "type": "blob",
          "size": 0.1396484375,
          "content": "#!/bin/bash\n\necho \"entrypoint.sh\"\nwhoami\nwhich python\nsource /opt/conda/etc/profile.d/conda.sh\nconda activate musev\nwhich python\npython app.py\n"
        },
        {
          "name": "musetalk",
          "type": "tree",
          "content": null
        },
        {
          "name": "requirements.txt",
          "type": "blob",
          "size": 0.3203125,
          "content": "--extra-index-url https://download.pytorch.org/whl/cu118 \ntorch==2.0.1\ntorchvision==0.15.2 \ntorchaudio==2.0.2\ndiffusers==0.27.2\naccelerate==0.28.0\ntensorflow==2.12.0\ntensorboard==2.12.0\nopencv-python==4.9.0.80\nsoundfile==0.12.1\ntransformers==4.39.2\n\ngdown\nrequests\nimageio[ffmpeg]\n\nomegaconf\nffmpeg-python\ngradio\nspaces\nmoviepy\n"
        },
        {
          "name": "scripts",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}