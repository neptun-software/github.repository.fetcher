{
  "metadata": {
    "timestamp": 1736559896597,
    "page": 650,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjY2MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "lucasjinreal/tensorflow_poems",
      "stars": 3632,
      "defaultBranch": "master",
      "files": [
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.0390625,
          "content": "checkpoints/\nmodel/\n.idea/\n__pycache__/\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 7.08984375,
          "content": "<h1 align=\"center\">Welcome to LiBai AI Composer 👋</h1>\n<p>\n  <img alt=\"Version\" src=\"https://img.shields.io/badge/version-under-blue.svg?cacheSeconds=2592000\" />\n  <a href=\"https://github.com/jinfagang/tensorflow_poems/#Copyright\" target=\"_blank\">\n    <img alt=\"License: Apache\" src=\"https://img.shields.io/badge/License-Apache-yellow.svg\" />\n  </a>\n</p>\n\n> An ai powered automatically generats poems in Chinese.\n>\n> 很久以来，我们都想让机器自己创作诗歌，当无数作家、编辑还没有抬起笔时，AI已经完成了数千篇文章。现在，这里是第一步....\n\n### 🏠 [Homepage](https://github.com/jinfagang/tensorflow_poems)\n\n## 👍 Outcome 结果\n\n阅遍了近4万首唐诗，作出：\n\n```\n龙舆迎池里，控列守龙猱。\n几岁芳篁落，来和晚月中。\n殊乘暮心处，麦光属激羁。\n铁门通眼峡，高桂露沙连。\n倘子门中望，何妨嶮锦楼。\n择闻洛臣识，椒苑根觞吼。\n柳翰天河酒，光方入胶明。\n```\n\n这诗做的很有感觉啊，这都是勤奋的结果啊，基本上学习了全唐诗的所有精华才有了这么牛逼的能力，这一般人能做到？\n本博客讲讲解一些里面实现的技术细节，如果有未尽之处，大家可以通过微信找到我，那个头像很神奇的男人。闲话不多说，先把 github 链接放上来，这个作诗机器人我会一直维护，如果大家因为时间太紧没有时间看，可以给这个项目 star 一下或者 fork，\n我一推送更新你就能看到，主要是为了修复一些 api 问题，tensorflow 虽然到了1.0，但是 api 还是会变化。\n把星星加起来，让更多人可以看到我们创造这个作诗机器人，后期会加入更多牛逼掉渣天的功能，比如说押韵等等。\n\n## 📥 Install 安装\n\n```sh\ngit clone https://github.com/jinfagang/tensorflow_poems.git\n```\n\n## 🛠 Usage 使用\n\n```sh\n# train on poems 训练\npython3 train.py\n# compose poems 作诗\npython3 compose_poem.py\n```\n\n训练的时候，你可能会看到如下：\n\nWhen you kick it off, you will see something like this:\n\n![](https://i.loli.net/2018/03/12/5aa5fd903c041.jpeg)\n\n## 📈 Updates 更新\n\n#### 2018-8-16\n\nWe are now officially announced a new project started: **StrangeAI School** - An artificial intelligence learning school and advanced algorithm exchange platform! What we believed in is: AI should made to change people's life, rather than controlled by Gaint Companies.\nHere you can get some previews about our projects: http://ai.loliloli.pro (strangeai.pro availiable soon)\n\n#### 2018-3-12\n\n**tensorflow_poems**来诈尸了，许久没有更新这个项目，不知不觉已经有了上千个star，感觉大家对这个还是很感兴趣，在这里我非常荣幸大家关注这个项目，但是我们不能因此而停止不前，这也是我来诈尸的目的。我会向大家展示一下我最新的进展，首先非常希望大家关注一下我倾心做的知乎专栏，人工智能从入门到逆天杀神以及每周一个黑科技，我们不仅仅要关注人工智能，还有区块链等前沿技术：\n\n- 人工智能从入门到逆天杀神(知乎专栏)： https://zhuanlan.zhihu.com/ai-man\n- 每周一项目黑科技-TrackTech(知乎专栏):  https://zhuanlan.zhihu.com/tracktech\n    If you want talk about AI, visit our website (for now):  http://ai.loliloli.pro (strangeai.pro availiable soon)\n     , **subscribe** our WeChat channel: 奇异人工智能学院\n\n#### 2017-11-8\n\n貌似距离上一次更新这个repo已经很久了，这段时间很多童鞋通过微信找到了我，甚至包括一些大佬。当时这个项目只是一个练手的东西，那个时候我的手法还不是非常老道。让各位踩坑了。现在**李白**强势归来。在这次的更新中增加了这些改进：\n\n- 对数据预处理脚本进行了前所未有的简化，现在连小学生都能了解了\n- 训练只需要运行train.py，数据和预训练模型都已经备好\n- 可以直接compose_poem.py 作诗，这次不会出现死循环的情况了。\n\n#### 2017-6-1 ~~可能是最后一次更新~~\n\n我决定有时间的时候重构这个项目了，古诗，源自在下骨子里的文艺之风，最近搞得东西有点乱，所以召集大家，对这个项目感兴趣的欢迎加入扣扣群：\n\n```\n 292889553\n```\n\n\n#### 2017-3-22 重磅更新，推出藏头诗功能\n\n一波小更新，下面的问题已经解决了：\n\n* 训练完成作诗时出现一直不出现的情况，实际上是陷入了一直作诗的死循环，已修复\n* 新增pretty print功能，打印出的古诗标准，接入第三方APP或者其他平台可以直接获取到标准格式的诗词\n* Ternimal disable了tensorflow默认的debug信息\n    最后最后最重要的是： **我们的作诗机器人（暂且叫李白）已经可以根据你的指定的字作诗了哦！！**\n    欢迎大家继续来踩，没有star的快star！！保持更新！！永远开源！！！\n    让我们来看看李白做的藏头诗吧：\n\n```\n# 最近一直下雨，就作一首雨字开头的吧\n雨霁开门中，山听淮水流。\n落花遍霜霰，金壶横河湟。\n年年忽息世，径远谁论吟。\n惊舟望秋月，应柳待晨围。\n人处山霜月，萧萧广野虚。\n\n# 李白人工智能作诗机器人的作者长得比较帅，以帅开头做一首吧\n帅主何幸化，自日兼春连。\n命钱犯夕兴，职馀玄赏圣。\n君有不知益，浮于但神衍。\n（浓浓的怀才不遇之风...）\n```\n\n## 👊 它已经不仅仅能够作古诗，还能模仿周杰伦创作歌词！！\n\n这是2017-03-9更新的功能，模仿周杰伦歌曲创作歌词，大家先来感受一下它创作的歌词：\n\n```\n我的你的她\n蛾眉脚的泪花\n乱飞从慌乱\n笛卡尔的悲伤\n迟早在是石板上\n荒废了晚上\n夜你的她不是她\n....\n```\n\n怎么说，目前由于缺乏训练文本，导致我们的AI做的歌词有点....额，还好啦，有那么一点忧郁之风，这个周杰伦完全不是一种风格呀。\n然而没有关系，目前它训练的文本还太少，只有112首歌，在这里我来呼吁大家一起来整理 **中国歌手的语料文本！！！**\n如果你喜欢周杰伦的歌，可以把他的歌一首一行，每首歌句子空格分开保存到txt中，大家可以集中发到我的[邮箱](mailto:jinfagang19@163.com)：\n相信如果不断的加入训练文本我们的歌词创作机器人会越来越牛逼！当然我会及时把数据集更新到github上，大家可以 star 一下跟进本项目的更新。\n\n## 👥 Authors 作者\n\n👤 **jinfagang**\n\n* Website: http://jinfagang.github.io\n* GitHub: [@JinTian](https://github.com/JinTian)\n\n👤 **William Song**\n\n- Website: http://williamzjc.gitee.io/morninglake/\n- GitHub: [@Freakwill](https://github.com/Freakwill)\n- Twitter: [@WilliamPython](https://twitter.com/WilliamPython)\n\n👤 **Harvey Dam**\n\n- GitHub: [@damtharvey](https://github.com/damtharvey)\n\n👤 **KnowsCount**\n\n- Website: http://docs.knowscount.cc/\n- GitHub: [@KnowsCount](https://github.com/KnowsCount)\n\n## 🎉 Show your support 支持\n\n如果帮助了你，给颗 🌟 罢！\n\nGive a 🌟 if this project helped you!\n\n## 📝 License 协议\n\nCopyright 版权 © 2020 \n"
        },
        {
          "name": "compose_poem.py",
          "type": "blob",
          "size": 3.06640625,
          "content": "# -*- coding: utf-8 -*-\n# file: main.py\n# author: JinTian\n# time: 11/03/2017 9:53 AM\n# Copyright 2017 JinTian. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ------------------------------------------------------------------------\nimport tensorflow as tf\nfrom poems.model import rnn_model\nfrom poems.poems import process_poems\nimport numpy as np\n\nstart_token = 'B'\nend_token = 'E'\nmodel_dir = './model/'\ncorpus_file = './data/poems.txt'\n\nlr = 0.0002\n\n\ndef to_word(predict, vocabs):\n    predict = predict[0]       \n    predict /= np.sum(predict)\n    sample = np.random.choice(np.arange(len(predict)), p=predict)\n    if sample > len(vocabs):\n        return vocabs[-1]\n    else:\n        return vocabs[sample]\n\n\ndef gen_poem(begin_word):\n    batch_size = 1\n    print('## loading corpus from %s' % model_dir)\n    poems_vector, word_int_map, vocabularies = process_poems(corpus_file)\n\n    input_data = tf.placeholder(tf.int32, [batch_size, None])\n\n    end_points = rnn_model(model='lstm', input_data=input_data, output_data=None, vocab_size=len(\n        vocabularies), rnn_size=128, num_layers=2, batch_size=64, learning_rate=lr)\n\n    saver = tf.train.Saver(tf.global_variables())\n    init_op = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())\n    with tf.Session() as sess:\n        sess.run(init_op)\n\n        checkpoint = tf.train.latest_checkpoint(model_dir)\n        saver.restore(sess, checkpoint)\n\n        x = np.array([list(map(word_int_map.get, start_token))])\n\n        [predict, last_state] = sess.run([end_points['prediction'], end_points['last_state']],\n                                         feed_dict={input_data: x})\n        word = begin_word or to_word(predict, vocabularies)\n        poem_ = ''\n\n        i = 0\n        while word != end_token:\n            poem_ += word\n            i += 1\n            if i > 24:\n                break\n            x = np.array([[word_int_map[word]]])\n            [predict, last_state] = sess.run([end_points['prediction'], end_points['last_state']],\n                                             feed_dict={input_data: x, end_points['initial_state']: last_state})\n            word = to_word(predict, vocabularies)\n\n        return poem_\n\n\ndef pretty_print_poem(poem_):\n    poem_sentences = poem_.split('。')\n    for s in poem_sentences:\n        if s != '' and len(s) > 10:\n            print(s + '。')\n\nif __name__ == '__main__':\n    begin_char = input('## （输入 quit 退出）请输入第一个字 please input the first character: ')\n    if begin_char == 'quit':\n        exit() \n    poem = gen_poem(begin_char)\n    pretty_print_poem(poem_=poem)"
        },
        {
          "name": "data",
          "type": "tree",
          "content": null
        },
        {
          "name": "model",
          "type": "tree",
          "content": null
        },
        {
          "name": "poems",
          "type": "tree",
          "content": null
        },
        {
          "name": "test.py",
          "type": "blob",
          "size": 0.0771484375,
          "content": "user_input_str = input('input string:')\nif user_input_str == 'exit':\n    exit()"
        },
        {
          "name": "train.py",
          "type": "blob",
          "size": 3.712890625,
          "content": "# -*- coding: utf-8 -*-\n# file: main.py\n# author: JinTian\n# time: 11/03/2017 9:53 AM\n# Copyright 2017 JinTian. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ------------------------------------------------------------------------\nimport os\nimport tensorflow as tf\nfrom poems.model import rnn_model\nfrom poems.poems import process_poems, generate_batch\n\ntf.app.flags.DEFINE_integer('batch_size', 64, 'batch size.')\ntf.app.flags.DEFINE_float('learning_rate', 0.01, 'learning rate.')\ntf.app.flags.DEFINE_string('model_dir', os.path.abspath('./model'), 'model save path.')\ntf.app.flags.DEFINE_string('file_path', os.path.abspath('./data/poems.txt'), 'file name of poems.')\ntf.app.flags.DEFINE_string('model_prefix', 'poems', 'model save prefix.')\ntf.app.flags.DEFINE_integer('epochs', 50, 'train how many epochs.')\n\nFLAGS = tf.app.flags.FLAGS\n\n\ndef run_training():\n    if not os.path.exists(FLAGS.model_dir):\n        os.makedirs(FLAGS.model_dir)\n\n    poems_vector, word_to_int, vocabularies = process_poems(FLAGS.file_path)\n    batches_inputs, batches_outputs = generate_batch(FLAGS.batch_size, poems_vector, word_to_int)\n\n    input_data = tf.placeholder(tf.int32, [FLAGS.batch_size, None])\n    output_targets = tf.placeholder(tf.int32, [FLAGS.batch_size, None])\n\n    end_points = rnn_model(model='lstm', input_data=input_data, output_data=output_targets, vocab_size=len(\n        vocabularies), rnn_size=128, num_layers=2, batch_size=64, learning_rate=FLAGS.learning_rate)\n\n    saver = tf.train.Saver(tf.global_variables())\n    init_op = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())\n    with tf.Session() as sess:\n        # sess = tf_debug.LocalCLIDebugWrapperSession(sess=sess)\n        # sess.add_tensor_filter(\"has_inf_or_nan\", tf_debug.has_inf_or_nan)\n        sess.run(init_op)\n\n        start_epoch = 0\n        checkpoint = tf.train.latest_checkpoint(FLAGS.model_dir)\n        if checkpoint:\n            saver.restore(sess, checkpoint)\n            print(\"## restore from the checkpoint {0}\".format(checkpoint))\n            start_epoch += int(checkpoint.split('-')[-1])\n        print('## start training...')\n        try:\n            n_chunk = len(poems_vector) // FLAGS.batch_size\n            for epoch in range(start_epoch, FLAGS.epochs):\n                n = 0\n                for batch in range(n_chunk):\n                    loss, _, _ = sess.run([\n                        end_points['total_loss'],\n                        end_points['last_state'],\n                        end_points['train_op']\n                    ], feed_dict={input_data: batches_inputs[n], output_targets: batches_outputs[n]})\n                    n += 1\n                    print('Epoch: %d, batch: %d, training loss: %.6f' % (epoch, batch, loss))\n                if epoch % 6 == 0:\n                    saver.save(sess, os.path.join(FLAGS.model_dir, FLAGS.model_prefix), global_step=epoch)\n        except KeyboardInterrupt:\n            print('## Interrupt manually, try saving checkpoint for now...')\n            saver.save(sess, os.path.join(FLAGS.model_dir, FLAGS.model_prefix), global_step=epoch)\n            print('## Last epoch were saved, next time will start from epoch {}.'.format(epoch))\n\n\ndef main(_):\n    run_training()\n\n\nif __name__ == '__main__':\n    tf.app.run()"
        },
        {
          "name": "utils",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}