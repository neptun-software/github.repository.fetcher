{
  "metadata": {
    "timestamp": 1736559896597,
    "page": 650,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjY2MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "lucasjinreal/tensorflow_poems",
      "stars": 3632,
      "defaultBranch": "master",
      "files": [
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.0390625,
          "content": "checkpoints/\nmodel/\n.idea/\n__pycache__/\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 7.08984375,
          "content": "<h1 align=\"center\">Welcome to LiBai AI Composer ðŸ‘‹</h1>\n<p>\n  <img alt=\"Version\" src=\"https://img.shields.io/badge/version-under-blue.svg?cacheSeconds=2592000\" />\n  <a href=\"https://github.com/jinfagang/tensorflow_poems/#Copyright\" target=\"_blank\">\n    <img alt=\"License: Apache\" src=\"https://img.shields.io/badge/License-Apache-yellow.svg\" />\n  </a>\n</p>\n\n> An ai powered automatically generats poems in Chinese.\n>\n> å¾ˆä¹…ä»¥æ¥ï¼Œæˆ‘ä»¬éƒ½æƒ³è®©æœºå™¨è‡ªå·±åˆ›ä½œè¯—æ­Œï¼Œå½“æ— æ•°ä½œå®¶ã€ç¼–è¾‘è¿˜æ²¡æœ‰æŠ¬èµ·ç¬”æ—¶ï¼ŒAIå·²ç»å®Œæˆäº†æ•°åƒç¯‡æ–‡ç« ã€‚çŽ°åœ¨ï¼Œè¿™é‡Œæ˜¯ç¬¬ä¸€æ­¥....\n\n### ðŸ  [Homepage](https://github.com/jinfagang/tensorflow_poems)\n\n## ðŸ‘ Outcome ç»“æžœ\n\né˜…éäº†è¿‘4ä¸‡é¦–å”è¯—ï¼Œä½œå‡ºï¼š\n\n```\né¾™èˆ†è¿Žæ± é‡Œï¼ŒæŽ§åˆ—å®ˆé¾™çŒ±ã€‚\nå‡ å²èŠ³ç¯è½ï¼Œæ¥å’Œæ™šæœˆä¸­ã€‚\næ®Šä¹˜æš®å¿ƒå¤„ï¼Œéº¦å…‰å±žæ¿€ç¾ã€‚\né“é—¨é€šçœ¼å³¡ï¼Œé«˜æ¡‚éœ²æ²™è¿žã€‚\nå€˜å­é—¨ä¸­æœ›ï¼Œä½•å¦¨å¶®é”¦æ¥¼ã€‚\næ‹©é—»æ´›è‡£è¯†ï¼Œæ¤’è‹‘æ ¹è§žå¼ã€‚\næŸ³ç¿°å¤©æ²³é…’ï¼Œå…‰æ–¹å…¥èƒ¶æ˜Žã€‚\n```\n\nè¿™è¯—åšçš„å¾ˆæœ‰æ„Ÿè§‰å•Šï¼Œè¿™éƒ½æ˜¯å‹¤å¥‹çš„ç»“æžœå•Šï¼ŒåŸºæœ¬ä¸Šå­¦ä¹ äº†å…¨å”è¯—çš„æ‰€æœ‰ç²¾åŽæ‰æœ‰äº†è¿™ä¹ˆç‰›é€¼çš„èƒ½åŠ›ï¼Œè¿™ä¸€èˆ¬äººèƒ½åšåˆ°ï¼Ÿ\næœ¬åšå®¢è®²è®²è§£ä¸€äº›é‡Œé¢å®žçŽ°çš„æŠ€æœ¯ç»†èŠ‚ï¼Œå¦‚æžœæœ‰æœªå°½ä¹‹å¤„ï¼Œå¤§å®¶å¯ä»¥é€šè¿‡å¾®ä¿¡æ‰¾åˆ°æˆ‘ï¼Œé‚£ä¸ªå¤´åƒå¾ˆç¥žå¥‡çš„ç”·äººã€‚é—²è¯ä¸å¤šè¯´ï¼Œå…ˆæŠŠ github é“¾æŽ¥æ”¾ä¸Šæ¥ï¼Œè¿™ä¸ªä½œè¯—æœºå™¨äººæˆ‘ä¼šä¸€ç›´ç»´æŠ¤ï¼Œå¦‚æžœå¤§å®¶å› ä¸ºæ—¶é—´å¤ªç´§æ²¡æœ‰æ—¶é—´çœ‹ï¼Œå¯ä»¥ç»™è¿™ä¸ªé¡¹ç›® star ä¸€ä¸‹æˆ–è€… forkï¼Œ\næˆ‘ä¸€æŽ¨é€æ›´æ–°ä½ å°±èƒ½çœ‹åˆ°ï¼Œä¸»è¦æ˜¯ä¸ºäº†ä¿®å¤ä¸€äº› api é—®é¢˜ï¼Œtensorflow è™½ç„¶åˆ°äº†1.0ï¼Œä½†æ˜¯ api è¿˜æ˜¯ä¼šå˜åŒ–ã€‚\næŠŠæ˜Ÿæ˜ŸåŠ èµ·æ¥ï¼Œè®©æ›´å¤šäººå¯ä»¥çœ‹åˆ°æˆ‘ä»¬åˆ›é€ è¿™ä¸ªä½œè¯—æœºå™¨äººï¼ŒåŽæœŸä¼šåŠ å…¥æ›´å¤šç‰›é€¼æŽ‰æ¸£å¤©çš„åŠŸèƒ½ï¼Œæ¯”å¦‚è¯´æŠ¼éŸµç­‰ç­‰ã€‚\n\n## ðŸ“¥ Install å®‰è£…\n\n```sh\ngit clone https://github.com/jinfagang/tensorflow_poems.git\n```\n\n## ðŸ›  Usage ä½¿ç”¨\n\n```sh\n# train on poems è®­ç»ƒ\npython3 train.py\n# compose poems ä½œè¯—\npython3 compose_poem.py\n```\n\nè®­ç»ƒçš„æ—¶å€™ï¼Œä½ å¯èƒ½ä¼šçœ‹åˆ°å¦‚ä¸‹ï¼š\n\nWhen you kick it off, you will see something like this:\n\n![](https://i.loli.net/2018/03/12/5aa5fd903c041.jpeg)\n\n## ðŸ“ˆ Updates æ›´æ–°\n\n#### 2018-8-16\n\nWe are now officially announced a new project started: **StrangeAI School** - An artificial intelligence learning school and advanced algorithm exchange platform! What we believed in is: AI should made to change people's life, rather than controlled by Gaint Companies.\nHere you can get some previews about our projects: http://ai.loliloli.pro (strangeai.pro availiable soon)\n\n#### 2018-3-12\n\n**tensorflow_poems**æ¥è¯ˆå°¸äº†ï¼Œè®¸ä¹…æ²¡æœ‰æ›´æ–°è¿™ä¸ªé¡¹ç›®ï¼Œä¸çŸ¥ä¸è§‰å·²ç»æœ‰äº†ä¸Šåƒä¸ªstarï¼Œæ„Ÿè§‰å¤§å®¶å¯¹è¿™ä¸ªè¿˜æ˜¯å¾ˆæ„Ÿå…´è¶£ï¼Œåœ¨è¿™é‡Œæˆ‘éžå¸¸è£å¹¸å¤§å®¶å…³æ³¨è¿™ä¸ªé¡¹ç›®ï¼Œä½†æ˜¯æˆ‘ä»¬ä¸èƒ½å› æ­¤è€Œåœæ­¢ä¸å‰ï¼Œè¿™ä¹Ÿæ˜¯æˆ‘æ¥è¯ˆå°¸çš„ç›®çš„ã€‚æˆ‘ä¼šå‘å¤§å®¶å±•ç¤ºä¸€ä¸‹æˆ‘æœ€æ–°çš„è¿›å±•ï¼Œé¦–å…ˆéžå¸¸å¸Œæœ›å¤§å®¶å…³æ³¨ä¸€ä¸‹æˆ‘å€¾å¿ƒåšçš„çŸ¥ä¹Žä¸“æ ï¼Œäººå·¥æ™ºèƒ½ä»Žå…¥é—¨åˆ°é€†å¤©æ€ç¥žä»¥åŠæ¯å‘¨ä¸€ä¸ªé»‘ç§‘æŠ€ï¼Œæˆ‘ä»¬ä¸ä»…ä»…è¦å…³æ³¨äººå·¥æ™ºèƒ½ï¼Œè¿˜æœ‰åŒºå—é“¾ç­‰å‰æ²¿æŠ€æœ¯ï¼š\n\n- äººå·¥æ™ºèƒ½ä»Žå…¥é—¨åˆ°é€†å¤©æ€ç¥ž(çŸ¥ä¹Žä¸“æ )ï¼š https://zhuanlan.zhihu.com/ai-man\n- æ¯å‘¨ä¸€é¡¹ç›®é»‘ç§‘æŠ€-TrackTech(çŸ¥ä¹Žä¸“æ ):  https://zhuanlan.zhihu.com/tracktech\n    If you want talk about AI, visit our website (for now):  http://ai.loliloli.pro (strangeai.pro availiable soon)\n     , **subscribe** our WeChat channel: å¥‡å¼‚äººå·¥æ™ºèƒ½å­¦é™¢\n\n#### 2017-11-8\n\nè²Œä¼¼è·ç¦»ä¸Šä¸€æ¬¡æ›´æ–°è¿™ä¸ªrepoå·²ç»å¾ˆä¹…äº†ï¼Œè¿™æ®µæ—¶é—´å¾ˆå¤šç«¥éž‹é€šè¿‡å¾®ä¿¡æ‰¾åˆ°äº†æˆ‘ï¼Œç”šè‡³åŒ…æ‹¬ä¸€äº›å¤§ä½¬ã€‚å½“æ—¶è¿™ä¸ªé¡¹ç›®åªæ˜¯ä¸€ä¸ªç»ƒæ‰‹çš„ä¸œè¥¿ï¼Œé‚£ä¸ªæ—¶å€™æˆ‘çš„æ‰‹æ³•è¿˜ä¸æ˜¯éžå¸¸è€é“ã€‚è®©å„ä½è¸©å‘äº†ã€‚çŽ°åœ¨**æŽç™½**å¼ºåŠ¿å½’æ¥ã€‚åœ¨è¿™æ¬¡çš„æ›´æ–°ä¸­å¢žåŠ äº†è¿™äº›æ”¹è¿›ï¼š\n\n- å¯¹æ•°æ®é¢„å¤„ç†è„šæœ¬è¿›è¡Œäº†å‰æ‰€æœªæœ‰çš„ç®€åŒ–ï¼ŒçŽ°åœ¨è¿žå°å­¦ç”Ÿéƒ½èƒ½äº†è§£äº†\n- è®­ç»ƒåªéœ€è¦è¿è¡Œtrain.pyï¼Œæ•°æ®å’Œé¢„è®­ç»ƒæ¨¡åž‹éƒ½å·²ç»å¤‡å¥½\n- å¯ä»¥ç›´æŽ¥compose_poem.py ä½œè¯—ï¼Œè¿™æ¬¡ä¸ä¼šå‡ºçŽ°æ­»å¾ªçŽ¯çš„æƒ…å†µäº†ã€‚\n\n#### 2017-6-1 ~~å¯èƒ½æ˜¯æœ€åŽä¸€æ¬¡æ›´æ–°~~\n\næˆ‘å†³å®šæœ‰æ—¶é—´çš„æ—¶å€™é‡æž„è¿™ä¸ªé¡¹ç›®äº†ï¼Œå¤è¯—ï¼Œæºè‡ªåœ¨ä¸‹éª¨å­é‡Œçš„æ–‡è‰ºä¹‹é£Žï¼Œæœ€è¿‘æžå¾—ä¸œè¥¿æœ‰ç‚¹ä¹±ï¼Œæ‰€ä»¥å¬é›†å¤§å®¶ï¼Œå¯¹è¿™ä¸ªé¡¹ç›®æ„Ÿå…´è¶£çš„æ¬¢è¿ŽåŠ å…¥æ‰£æ‰£ç¾¤ï¼š\n\n```\n 292889553\n```\n\n\n#### 2017-3-22 é‡ç£…æ›´æ–°ï¼ŒæŽ¨å‡ºè—å¤´è¯—åŠŸèƒ½\n\nä¸€æ³¢å°æ›´æ–°ï¼Œä¸‹é¢çš„é—®é¢˜å·²ç»è§£å†³äº†ï¼š\n\n* è®­ç»ƒå®Œæˆä½œè¯—æ—¶å‡ºçŽ°ä¸€ç›´ä¸å‡ºçŽ°çš„æƒ…å†µï¼Œå®žé™…ä¸Šæ˜¯é™·å…¥äº†ä¸€ç›´ä½œè¯—çš„æ­»å¾ªçŽ¯ï¼Œå·²ä¿®å¤\n* æ–°å¢žpretty printåŠŸèƒ½ï¼Œæ‰“å°å‡ºçš„å¤è¯—æ ‡å‡†ï¼ŒæŽ¥å…¥ç¬¬ä¸‰æ–¹APPæˆ–è€…å…¶ä»–å¹³å°å¯ä»¥ç›´æŽ¥èŽ·å–åˆ°æ ‡å‡†æ ¼å¼çš„è¯—è¯\n* Ternimal disableäº†tensorflowé»˜è®¤çš„debugä¿¡æ¯\n    æœ€åŽæœ€åŽæœ€é‡è¦çš„æ˜¯ï¼š **æˆ‘ä»¬çš„ä½œè¯—æœºå™¨äººï¼ˆæš‚ä¸”å«æŽç™½ï¼‰å·²ç»å¯ä»¥æ ¹æ®ä½ çš„æŒ‡å®šçš„å­—ä½œè¯—äº†å“¦ï¼ï¼**\n    æ¬¢è¿Žå¤§å®¶ç»§ç»­æ¥è¸©ï¼Œæ²¡æœ‰starçš„å¿«starï¼ï¼ä¿æŒæ›´æ–°ï¼ï¼æ°¸è¿œå¼€æºï¼ï¼ï¼\n    è®©æˆ‘ä»¬æ¥çœ‹çœ‹æŽç™½åšçš„è—å¤´è¯—å§ï¼š\n\n```\n# æœ€è¿‘ä¸€ç›´ä¸‹é›¨ï¼Œå°±ä½œä¸€é¦–é›¨å­—å¼€å¤´çš„å§\né›¨éœå¼€é—¨ä¸­ï¼Œå±±å¬æ·®æ°´æµã€‚\nè½èŠ±ééœœéœ°ï¼Œé‡‘å£¶æ¨ªæ²³æ¹Ÿã€‚\nå¹´å¹´å¿½æ¯ä¸–ï¼Œå¾„è¿œè°è®ºåŸã€‚\næƒŠèˆŸæœ›ç§‹æœˆï¼Œåº”æŸ³å¾…æ™¨å›´ã€‚\näººå¤„å±±éœœæœˆï¼Œè§è§å¹¿é‡Žè™šã€‚\n\n# æŽç™½äººå·¥æ™ºèƒ½ä½œè¯—æœºå™¨äººçš„ä½œè€…é•¿å¾—æ¯”è¾ƒå¸…ï¼Œä»¥å¸…å¼€å¤´åšä¸€é¦–å§\nå¸…ä¸»ä½•å¹¸åŒ–ï¼Œè‡ªæ—¥å…¼æ˜¥è¿žã€‚\nå‘½é’±çŠ¯å¤•å…´ï¼ŒèŒé¦€çŽ„èµåœ£ã€‚\nå›æœ‰ä¸çŸ¥ç›Šï¼Œæµ®äºŽä½†ç¥žè¡ã€‚\nï¼ˆæµ“æµ“çš„æ€€æ‰ä¸é‡ä¹‹é£Ž...ï¼‰\n```\n\n## ðŸ‘Š å®ƒå·²ç»ä¸ä»…ä»…èƒ½å¤Ÿä½œå¤è¯—ï¼Œè¿˜èƒ½æ¨¡ä»¿å‘¨æ°ä¼¦åˆ›ä½œæ­Œè¯ï¼ï¼\n\nè¿™æ˜¯2017-03-9æ›´æ–°çš„åŠŸèƒ½ï¼Œæ¨¡ä»¿å‘¨æ°ä¼¦æ­Œæ›²åˆ›ä½œæ­Œè¯ï¼Œå¤§å®¶å…ˆæ¥æ„Ÿå—ä¸€ä¸‹å®ƒåˆ›ä½œçš„æ­Œè¯ï¼š\n\n```\næˆ‘çš„ä½ çš„å¥¹\nè›¾çœ‰è„šçš„æ³ªèŠ±\nä¹±é£žä»Žæ…Œä¹±\nç¬›å¡å°”çš„æ‚²ä¼¤\nè¿Ÿæ—©åœ¨æ˜¯çŸ³æ¿ä¸Š\nè’åºŸäº†æ™šä¸Š\nå¤œä½ çš„å¥¹ä¸æ˜¯å¥¹\n....\n```\n\næ€Žä¹ˆè¯´ï¼Œç›®å‰ç”±äºŽç¼ºä¹è®­ç»ƒæ–‡æœ¬ï¼Œå¯¼è‡´æˆ‘ä»¬çš„AIåšçš„æ­Œè¯æœ‰ç‚¹....é¢ï¼Œè¿˜å¥½å•¦ï¼Œæœ‰é‚£ä¹ˆä¸€ç‚¹å¿§éƒä¹‹é£Žï¼Œè¿™ä¸ªå‘¨æ°ä¼¦å®Œå…¨ä¸æ˜¯ä¸€ç§é£Žæ ¼å‘€ã€‚\nç„¶è€Œæ²¡æœ‰å…³ç³»ï¼Œç›®å‰å®ƒè®­ç»ƒçš„æ–‡æœ¬è¿˜å¤ªå°‘ï¼Œåªæœ‰112é¦–æ­Œï¼Œåœ¨è¿™é‡Œæˆ‘æ¥å‘¼åå¤§å®¶ä¸€èµ·æ¥æ•´ç† **ä¸­å›½æ­Œæ‰‹çš„è¯­æ–™æ–‡æœ¬ï¼ï¼ï¼**\nå¦‚æžœä½ å–œæ¬¢å‘¨æ°ä¼¦çš„æ­Œï¼Œå¯ä»¥æŠŠä»–çš„æ­Œä¸€é¦–ä¸€è¡Œï¼Œæ¯é¦–æ­Œå¥å­ç©ºæ ¼åˆ†å¼€ä¿å­˜åˆ°txtä¸­ï¼Œå¤§å®¶å¯ä»¥é›†ä¸­å‘åˆ°æˆ‘çš„[é‚®ç®±](mailto:jinfagang19@163.com)ï¼š\nç›¸ä¿¡å¦‚æžœä¸æ–­çš„åŠ å…¥è®­ç»ƒæ–‡æœ¬æˆ‘ä»¬çš„æ­Œè¯åˆ›ä½œæœºå™¨äººä¼šè¶Šæ¥è¶Šç‰›é€¼ï¼å½“ç„¶æˆ‘ä¼šåŠæ—¶æŠŠæ•°æ®é›†æ›´æ–°åˆ°githubä¸Šï¼Œå¤§å®¶å¯ä»¥ star ä¸€ä¸‹è·Ÿè¿›æœ¬é¡¹ç›®çš„æ›´æ–°ã€‚\n\n## ðŸ‘¥ Authors ä½œè€…\n\nðŸ‘¤ **jinfagang**\n\n* Website: http://jinfagang.github.io\n* GitHub: [@JinTian](https://github.com/JinTian)\n\nðŸ‘¤ **William Song**\n\n- Website: http://williamzjc.gitee.io/morninglake/\n- GitHub: [@Freakwill](https://github.com/Freakwill)\n- Twitter: [@WilliamPython](https://twitter.com/WilliamPython)\n\nðŸ‘¤ **Harvey Dam**\n\n- GitHub: [@damtharvey](https://github.com/damtharvey)\n\nðŸ‘¤ **KnowsCount**\n\n- Website: http://docs.knowscount.cc/\n- GitHub: [@KnowsCount](https://github.com/KnowsCount)\n\n## ðŸŽ‰ Show your support æ”¯æŒ\n\nå¦‚æžœå¸®åŠ©äº†ä½ ï¼Œç»™é¢— ðŸŒŸ ç½¢ï¼\n\nGive a ðŸŒŸ if this project helped you!\n\n## ðŸ“ License åè®®\n\nCopyright ç‰ˆæƒ Â© 2020 \n"
        },
        {
          "name": "compose_poem.py",
          "type": "blob",
          "size": 3.06640625,
          "content": "# -*- coding: utf-8 -*-\n# file: main.py\n# author: JinTian\n# time: 11/03/2017 9:53 AM\n# Copyright 2017 JinTian. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ------------------------------------------------------------------------\nimport tensorflow as tf\nfrom poems.model import rnn_model\nfrom poems.poems import process_poems\nimport numpy as np\n\nstart_token = 'B'\nend_token = 'E'\nmodel_dir = './model/'\ncorpus_file = './data/poems.txt'\n\nlr = 0.0002\n\n\ndef to_word(predict, vocabs):\n    predict = predict[0]       \n    predict /= np.sum(predict)\n    sample = np.random.choice(np.arange(len(predict)), p=predict)\n    if sample > len(vocabs):\n        return vocabs[-1]\n    else:\n        return vocabs[sample]\n\n\ndef gen_poem(begin_word):\n    batch_size = 1\n    print('## loading corpus from %s' % model_dir)\n    poems_vector, word_int_map, vocabularies = process_poems(corpus_file)\n\n    input_data = tf.placeholder(tf.int32, [batch_size, None])\n\n    end_points = rnn_model(model='lstm', input_data=input_data, output_data=None, vocab_size=len(\n        vocabularies), rnn_size=128, num_layers=2, batch_size=64, learning_rate=lr)\n\n    saver = tf.train.Saver(tf.global_variables())\n    init_op = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())\n    with tf.Session() as sess:\n        sess.run(init_op)\n\n        checkpoint = tf.train.latest_checkpoint(model_dir)\n        saver.restore(sess, checkpoint)\n\n        x = np.array([list(map(word_int_map.get, start_token))])\n\n        [predict, last_state] = sess.run([end_points['prediction'], end_points['last_state']],\n                                         feed_dict={input_data: x})\n        word = begin_word or to_word(predict, vocabularies)\n        poem_ = ''\n\n        i = 0\n        while word != end_token:\n            poem_ += word\n            i += 1\n            if i > 24:\n                break\n            x = np.array([[word_int_map[word]]])\n            [predict, last_state] = sess.run([end_points['prediction'], end_points['last_state']],\n                                             feed_dict={input_data: x, end_points['initial_state']: last_state})\n            word = to_word(predict, vocabularies)\n\n        return poem_\n\n\ndef pretty_print_poem(poem_):\n    poem_sentences = poem_.split('ã€‚')\n    for s in poem_sentences:\n        if s != '' and len(s) > 10:\n            print(s + 'ã€‚')\n\nif __name__ == '__main__':\n    begin_char = input('## ï¼ˆè¾“å…¥ quit é€€å‡ºï¼‰è¯·è¾“å…¥ç¬¬ä¸€ä¸ªå­— please input the first character: ')\n    if begin_char == 'quit':\n        exit() \n    poem = gen_poem(begin_char)\n    pretty_print_poem(poem_=poem)"
        },
        {
          "name": "data",
          "type": "tree",
          "content": null
        },
        {
          "name": "model",
          "type": "tree",
          "content": null
        },
        {
          "name": "poems",
          "type": "tree",
          "content": null
        },
        {
          "name": "test.py",
          "type": "blob",
          "size": 0.0771484375,
          "content": "user_input_str = input('input string:')\nif user_input_str == 'exit':\n    exit()"
        },
        {
          "name": "train.py",
          "type": "blob",
          "size": 3.712890625,
          "content": "# -*- coding: utf-8 -*-\n# file: main.py\n# author: JinTian\n# time: 11/03/2017 9:53 AM\n# Copyright 2017 JinTian. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ------------------------------------------------------------------------\nimport os\nimport tensorflow as tf\nfrom poems.model import rnn_model\nfrom poems.poems import process_poems, generate_batch\n\ntf.app.flags.DEFINE_integer('batch_size', 64, 'batch size.')\ntf.app.flags.DEFINE_float('learning_rate', 0.01, 'learning rate.')\ntf.app.flags.DEFINE_string('model_dir', os.path.abspath('./model'), 'model save path.')\ntf.app.flags.DEFINE_string('file_path', os.path.abspath('./data/poems.txt'), 'file name of poems.')\ntf.app.flags.DEFINE_string('model_prefix', 'poems', 'model save prefix.')\ntf.app.flags.DEFINE_integer('epochs', 50, 'train how many epochs.')\n\nFLAGS = tf.app.flags.FLAGS\n\n\ndef run_training():\n    if not os.path.exists(FLAGS.model_dir):\n        os.makedirs(FLAGS.model_dir)\n\n    poems_vector, word_to_int, vocabularies = process_poems(FLAGS.file_path)\n    batches_inputs, batches_outputs = generate_batch(FLAGS.batch_size, poems_vector, word_to_int)\n\n    input_data = tf.placeholder(tf.int32, [FLAGS.batch_size, None])\n    output_targets = tf.placeholder(tf.int32, [FLAGS.batch_size, None])\n\n    end_points = rnn_model(model='lstm', input_data=input_data, output_data=output_targets, vocab_size=len(\n        vocabularies), rnn_size=128, num_layers=2, batch_size=64, learning_rate=FLAGS.learning_rate)\n\n    saver = tf.train.Saver(tf.global_variables())\n    init_op = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())\n    with tf.Session() as sess:\n        # sess = tf_debug.LocalCLIDebugWrapperSession(sess=sess)\n        # sess.add_tensor_filter(\"has_inf_or_nan\", tf_debug.has_inf_or_nan)\n        sess.run(init_op)\n\n        start_epoch = 0\n        checkpoint = tf.train.latest_checkpoint(FLAGS.model_dir)\n        if checkpoint:\n            saver.restore(sess, checkpoint)\n            print(\"## restore from the checkpoint {0}\".format(checkpoint))\n            start_epoch += int(checkpoint.split('-')[-1])\n        print('## start training...')\n        try:\n            n_chunk = len(poems_vector) // FLAGS.batch_size\n            for epoch in range(start_epoch, FLAGS.epochs):\n                n = 0\n                for batch in range(n_chunk):\n                    loss, _, _ = sess.run([\n                        end_points['total_loss'],\n                        end_points['last_state'],\n                        end_points['train_op']\n                    ], feed_dict={input_data: batches_inputs[n], output_targets: batches_outputs[n]})\n                    n += 1\n                    print('Epoch: %d, batch: %d, training loss: %.6f' % (epoch, batch, loss))\n                if epoch % 6 == 0:\n                    saver.save(sess, os.path.join(FLAGS.model_dir, FLAGS.model_prefix), global_step=epoch)\n        except KeyboardInterrupt:\n            print('## Interrupt manually, try saving checkpoint for now...')\n            saver.save(sess, os.path.join(FLAGS.model_dir, FLAGS.model_prefix), global_step=epoch)\n            print('## Last epoch were saved, next time will start from epoch {}.'.format(epoch))\n\n\ndef main(_):\n    run_training()\n\n\nif __name__ == '__main__':\n    tf.app.run()"
        },
        {
          "name": "utils",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}