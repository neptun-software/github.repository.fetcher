{
  "metadata": {
    "timestamp": 1736560378439,
    "page": 919,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjkzMA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "tianzhi0549/FCOS",
      "stars": 3288,
      "defaultBranch": "master",
      "files": [
        {
          "name": ".flake8",
          "type": "blob",
          "size": 0.2412109375,
          "content": "# This is an example .flake8 config, used when developing *Black* itself.\n# Keep in sync with setup.cfg which is used for source packages.\n\n[flake8]\nignore = E203, E266, E501, W503\nmax-line-length = 80\nmax-complexity = 18\nselect = B,C,E,F,W,T4,B9\n"
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.3134765625,
          "content": "# compilation and distribution\n__pycache__\n_ext\n*.pyc\n*.so\nmaskrcnn_benchmark.egg-info/\nbuild/\ndist/\n\n# pytorch/python/numpy formats\n*.pth\n*.pkl\n*.npy\n\n# ipython/jupyter notebooks\n*.ipynb\n**/.ipynb_checkpoints/\n\n# Editor temporaries\n*.swn\n*.swo\n*.swp\n*~\n\n# Pycharm editor settings\n.idea\n\n# project dirs\n/datasets\n/models\n"
        },
        {
          "name": "ABSTRACTIONS.md",
          "type": "blob",
          "size": 2.591796875,
          "content": "## Abstractions\nThe main abstractions introduced by `maskrcnn_benchmark` that are useful to\nhave in mind are the following:\n\n### ImageList\nIn PyTorch, the first dimension of the input to the network generally represents\nthe batch dimension, and thus all elements of the same batch have the same\nheight / width.\nIn order to support images with different sizes and aspect ratios in the same\nbatch, we created the `ImageList` class, which holds internally a batch of\nimages (os possibly different sizes). The images are padded with zeros such that\nthey have the same final size and batched over the first dimension. The original\nsizes of the images before padding are stored in the `image_sizes` attribute,\nand the batched tensor in `tensors`.\nWe provide a convenience function `to_image_list` that accepts a few different\ninput types, including a list of tensors, and returns an `ImageList` object.\n\n```python\nfrom maskrcnn_benchmark.structures.image_list import to_image_list\n\nimages = [torch.rand(3, 100, 200), torch.rand(3, 150, 170)]\nbatched_images = to_image_list(images)\n\n# it is also possible to make the final batched image be a multiple of a number\nbatched_images_32 = to_image_list(images, size_divisible=32)\n```\n\n### BoxList\nThe `BoxList` class holds a set of bounding boxes (represented as a `Nx4` tensor) for\na specific image, as well as the size of the image as a `(width, height)` tuple.\nIt also contains a set of methods that allow to perform geometric\ntransformations to the bounding boxes (such as cropping, scaling and flipping).\nThe class accepts bounding boxes from two different input formats:\n- `xyxy`, where each box is encoded as a `x1`, `y1`, `x2` and `y2` coordinates, and\n- `xywh`, where each box is encoded as `x1`, `y1`, `w` and `h`.\n\nAdditionally, each `BoxList` instance can also hold arbitrary additional information\nfor each bounding box, such as labels, visibility, probability scores etc.\n\nHere is an example on how to create a `BoxList` from a list of coordinates:\n```python\nfrom maskrcnn_benchmark.structures.bounding_box import BoxList, FLIP_LEFT_RIGHT\n\nwidth = 100\nheight = 200\nboxes = [\n  [0, 10, 50, 50],\n  [50, 20, 90, 60],\n  [10, 10, 50, 50]\n]\n# create a BoxList with 3 boxes\nbbox = BoxList(boxes, image_size=(width, height), mode='xyxy')\n\n# perform some box transformations, has similar API as PIL.Image\nbbox_scaled = bbox.resize((width * 2, height * 3))\nbbox_flipped = bbox.transpose(FLIP_LEFT_RIGHT)\n\n# add labels for each bbox\nlabels = torch.tensor([0, 10, 1])\nbbox.add_field('labels', labels)\n\n# bbox also support a few operations, like indexing\n# here, selects boxes 0 and 2\nbbox_subset = bbox[[0, 2]]\n```\n"
        },
        {
          "name": "CODE_OF_CONDUCT.md",
          "type": "blob",
          "size": 0.23828125,
          "content": "# Code of Conduct\n\nFacebook has adopted a Code of Conduct that we expect project participants to adhere to.\nPlease read the [full text](https://code.fb.com/codeofconduct/)\nso that you can understand what actions will and will not be tolerated.\n"
        },
        {
          "name": "CONTRIBUTING.md",
          "type": "blob",
          "size": 1.6025390625,
          "content": "# Contributing to Mask-RCNN Benchmark\nWe want to make contributing to this project as easy and transparent as\npossible.\n\n## Our Development Process\nMinor changes and improvements will be released on an ongoing basis. Larger changes (e.g., changesets implementing a new paper) will be released on a more periodic basis.\n\n## Pull Requests\nWe actively welcome your pull requests.\n\n1. Fork the repo and create your branch from `master`.\n2. If you've added code that should be tested, add tests.\n3. If you've changed APIs, update the documentation.\n4. Ensure the test suite passes.\n5. Make sure your code lints.\n6. If you haven't already, complete the Contributor License Agreement (\"CLA\").\n\n## Contributor License Agreement (\"CLA\")\nIn order to accept your pull request, we need you to submit a CLA. You only need\nto do this once to work on any of Facebook's open source projects.\n\nComplete your CLA here: <https://code.facebook.com/cla>\n\n## Issues\nWe use GitHub issues to track public bugs. Please ensure your description is\nclear and has sufficient instructions to be able to reproduce the issue.\n\nFacebook has a [bounty program](https://www.facebook.com/whitehat/) for the safe\ndisclosure of security bugs. In those cases, please go through the process\noutlined on that page and do not file a public issue.\n\n## Coding Style  \n* 4 spaces for indentation rather than tabs\n* 80 character line length\n* PEP8 formatting following [Black](https://black.readthedocs.io/en/stable/)\n\n## License\nBy contributing to Mask-RCNN Benchmark, you agree that your contributions will be licensed\nunder the LICENSE file in the root directory of this source tree.\n"
        },
        {
          "name": "INSTALL.md",
          "type": "blob",
          "size": 2.359375,
          "content": "## Installation\n\n### Requirements:\n- PyTorch >= 1.0. Installation instructions can be found in https://pytorch.org/get-started/locally/.\n- torchvision\n- cocoapi\n- yacs\n- matplotlib\n- GCC >= 4.9,< 6.0\n- (optional) OpenCV for the webcam demo\n\n### Option 1: Step-by-step installation\n\n```bash\n# first, make sure that your conda is setup properly with the right environment\n# for that, check that `which conda`, `which pip` and `which python` points to the\n# right path. From a clean conda env, this is what you need to do\n\nconda create --name FCOS\nconda activate FCOS\n\n# this installs the right pip and dependencies for the fresh python\nconda install ipython\n\n# FCOS and coco api dependencies\npip install ninja yacs cython matplotlib tqdm\n\n# follow PyTorch installation in https://pytorch.org/get-started/locally/\n# we give the instructions for CUDA 10.2\nconda install pytorch torchvision cudatoolkit=10.2 -c pytorch\n\nexport INSTALL_DIR=$PWD\n\n# install pycocotools. Please make sure you have installed cython.\ncd $INSTALL_DIR\ngit clone https://github.com/cocodataset/cocoapi.git\ncd cocoapi/PythonAPI\npython setup.py build_ext install\n\n# install PyTorch Detection\ncd $INSTALL_DIR\ngit clone https://github.com/tianzhi0549/FCOS.git\ncd FCOS\n\n# the following will install the lib with\n# symbolic links, so that you can modify\n# the files if you want and won't need to\n# re-build it\npython setup.py build develop --no-deps\n\n\nunset INSTALL_DIR\n\n# or if you are on macOS\n# MACOSX_DEPLOYMENT_TARGET=10.9 CC=clang CXX=clang++ python setup.py build develop\n```\n\n### Option 2: Docker Image (Requires CUDA, Linux only)\n*The following steps are for original maskrcnn-benchmark. Please change the repository name if needed.* \n\nBuild image with defaults (`CUDA=9.0`, `CUDNN=7`, `FORCE_CUDA=1`):\n\n    nvidia-docker build -t maskrcnn-benchmark docker/\n    \nBuild image with other CUDA and CUDNN versions:\n\n    nvidia-docker build -t maskrcnn-benchmark --build-arg CUDA=9.2 --build-arg CUDNN=7 docker/\n    \nBuild image with FORCE_CUDA disabled:\n\n    nvidia-docker build -t maskrcnn-benchmark --build-arg FORCE_CUDA=0 docker/\n    \nBuild and run image with built-in jupyter notebook(note that the password is used to log in jupyter notebook):\n\n    nvidia-docker build -t maskrcnn-benchmark-jupyter docker/docker-jupyter/\n    nvidia-docker run -td -p 8888:8888 -e PASSWORD=<password> -v <host-dir>:<container-dir> maskrcnn-benchmark-jupyter\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 1.2958984375,
          "content": "FCOS for non-commercial purposes\n\nCopyright (c) 2019 the authors\nAll rights reserved.\n\nRedistribution and use in source and binary forms, with or without\nmodification, are permitted provided that the following conditions are met:\n\n* Redistributions of source code must retain the above copyright notice, this\n  list of conditions and the following disclaimer.\n\n* Redistributions in binary form must reproduce the above copyright notice,\n  this list of conditions and the following disclaimer in the documentation\n  and/or other materials provided with the distribution.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\nAND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\nIMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\nDISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE\nFOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\nDAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\nSERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\nCAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,\nOR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\nOF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n"
        },
        {
          "name": "MANIFEST.in",
          "type": "blob",
          "size": 0.0390625,
          "content": "graft fcos_core/csrc\ngraft fcos/configs\n"
        },
        {
          "name": "MASKRCNN_README.md",
          "type": "blob",
          "size": 10.859375,
          "content": "# Faster R-CNN and Mask R-CNN in PyTorch 1.0\n\nThis project aims at providing the necessary building blocks for easily\ncreating detection and segmentation models using PyTorch 1.0.\n\n![alt text](demo/demo_e2e_mask_rcnn_X_101_32x8d_FPN_1x.png \"from http://cocodataset.org/#explore?id=345434\")\n\n## Highlights\n- **PyTorch 1.0:** RPN, Faster R-CNN and Mask R-CNN implementations that matches or exceeds Detectron accuracies\n- **Very fast**: up to **2x** faster than [Detectron](https://github.com/facebookresearch/Detectron) and **30%** faster than [mmdetection](https://github.com/open-mmlab/mmdetection) during training. See [MODEL_ZOO.md](MODEL_ZOO.md) for more details.\n- **Memory efficient:** uses roughly 500MB less GPU memory than mmdetection during training\n- **Multi-GPU training and inference**\n- **Batched inference:** can perform inference using multiple images per batch per GPU\n- **CPU support for inference:** runs on CPU in inference time. See our [webcam demo](demo) for an example\n- Provides pre-trained models for almost all reference Mask R-CNN and Faster R-CNN configurations with 1x schedule.\n\n## Webcam and Jupyter notebook demo\n\nWe provide a simple webcam demo that illustrates how you can use `maskrcnn_benchmark` for inference:\n```bash\ncd demo\n# by default, it runs on the GPU\n# for best results, use min-image-size 800\npython webcam.py --min-image-size 800\n# can also run it on the CPU\npython webcam.py --min-image-size 300 MODEL.DEVICE cpu\n# or change the model that you want to use\npython webcam.py --config-file ../configs/caffe2/e2e_mask_rcnn_R_101_FPN_1x_caffe2.yaml --min-image-size 300 MODEL.DEVICE cpu\n# in order to see the probability heatmaps, pass --show-mask-heatmaps\npython webcam.py --min-image-size 300 --show-mask-heatmaps MODEL.DEVICE cpu\n# for the keypoint demo\npython webcam.py --config-file ../configs/caffe2/e2e_keypoint_rcnn_R_50_FPN_1x_caffe2.yaml --min-image-size 300 MODEL.DEVICE cpu\n```\n\nA notebook with the demo can be found in [demo/Mask_R-CNN_demo.ipynb](demo/Mask_R-CNN_demo.ipynb).\n\n## Installation\n\nCheck [INSTALL.md](INSTALL.md) for installation instructions.\n\n\n## Model Zoo and Baselines\n\nPre-trained models, baselines and comparison with Detectron and mmdetection\ncan be found in [MODEL_ZOO.md](MODEL_ZOO.md)\n\n## Inference in a few lines\nWe provide a helper class to simplify writing inference pipelines using pre-trained models.\nHere is how we would do it. Run this from the `demo` folder:\n```python\nfrom maskrcnn_benchmark.config import cfg\nfrom predictor import COCODemo\n\nconfig_file = \"../configs/caffe2/e2e_mask_rcnn_R_50_FPN_1x_caffe2.yaml\"\n\n# update the config options with the config file\ncfg.merge_from_file(config_file)\n# manual override some options\ncfg.merge_from_list([\"MODEL.DEVICE\", \"cpu\"])\n\ncoco_demo = COCODemo(\n    cfg,\n    min_image_size=800,\n    confidence_threshold=0.7,\n)\n# load image and then run prediction\nimage = ...\npredictions = coco_demo.run_on_opencv_image(image)\n```\n\n## Perform training on COCO dataset\n\nFor the following examples to work, you need to first install `maskrcnn_benchmark`.\n\nYou will also need to download the COCO dataset.\nWe recommend to symlink the path to the coco dataset to `datasets/` as follows\n\nWe use `minival` and `valminusminival` sets from [Detectron](https://github.com/facebookresearch/Detectron/blob/master/detectron/datasets/data/README.md#coco-minival-annotations)\n\n```bash\n# symlink the coco dataset\ncd ~/github/maskrcnn-benchmark\nmkdir -p datasets/coco\nln -s /path_to_coco_dataset/annotations datasets/coco/annotations\nln -s /path_to_coco_dataset/train2014 datasets/coco/train2014\nln -s /path_to_coco_dataset/test2014 datasets/coco/test2014\nln -s /path_to_coco_dataset/val2014 datasets/coco/val2014\n# or use COCO 2017 version\nln -s /path_to_coco_dataset/annotations datasets/coco/annotations\nln -s /path_to_coco_dataset/train2017 datasets/coco/train2017\nln -s /path_to_coco_dataset/test2017 datasets/coco/test2017\nln -s /path_to_coco_dataset/val2017 datasets/coco/val2017\n\n# for pascal voc dataset:\nln -s /path_to_VOCdevkit_dir datasets/voc\n```\n\nP.S. `COCO_2017_train` = `COCO_2014_train` + `valminusminival` , `COCO_2017_val` = `minival`\n      \n\nYou can also configure your own paths to the datasets.\nFor that, all you need to do is to modify `maskrcnn_benchmark/config/paths_catalog.py` to\npoint to the location where your dataset is stored.\nYou can also create a new `paths_catalog.py` file which implements the same two classes,\nand pass it as a config argument `PATHS_CATALOG` during training.\n\n### Single GPU training\n\nMost of the configuration files that we provide assume that we are running on 8 GPUs.\nIn order to be able to run it on fewer GPUs, there are a few possibilities:\n\n**1. Run the following without modifications**\n\n```bash\npython /path_to_maskrcnn_benchmark/tools/train_net.py --config-file \"/path/to/config/file.yaml\"\n```\nThis should work out of the box and is very similar to what we should do for multi-GPU training.\nBut the drawback is that it will use much more GPU memory. The reason is that we set in the\nconfiguration files a global batch size that is divided over the number of GPUs. So if we only\nhave a single GPU, this means that the batch size for that GPU will be 8x larger, which might lead\nto out-of-memory errors.\n\nIf you have a lot of memory available, this is the easiest solution.\n\n**2. Modify the cfg parameters**\n\nIf you experience out-of-memory errors, you can reduce the global batch size. But this means that\nyou'll also need to change the learning rate, the number of iterations and the learning rate schedule.\n\nHere is an example for Mask R-CNN R-50 FPN with the 1x schedule:\n```bash\npython tools/train_net.py --config-file \"configs/e2e_mask_rcnn_R_50_FPN_1x.yaml\" SOLVER.IMS_PER_BATCH 2 SOLVER.BASE_LR 0.0025 SOLVER.MAX_ITER 720000 SOLVER.STEPS \"(480000, 640000)\" TEST.IMS_PER_BATCH 1\n```\nThis follows the [scheduling rules from Detectron.](https://github.com/facebookresearch/Detectron/blob/master/configs/getting_started/tutorial_1gpu_e2e_faster_rcnn_R-50-FPN.yaml#L14-L30)\nNote that we have multiplied the number of iterations by 8x (as well as the learning rate schedules),\nand we have divided the learning rate by 8x.\n\nWe also changed the batch size during testing, but that is generally not necessary because testing\nrequires much less memory than training.\n\n\n### Multi-GPU training\nWe use internally `torch.distributed.launch` in order to launch\nmulti-gpu training. This utility function from PyTorch spawns as many\nPython processes as the number of GPUs we want to use, and each Python\nprocess will only use a single GPU.\n\n```bash\nexport NGPUS=8\npython -m torch.distributed.launch --nproc_per_node=$NGPUS /path_to_maskrcnn_benchmark/tools/train_net.py --config-file \"path/to/config/file.yaml\"\n```\n\n## Abstractions\nFor more information on some of the main abstractions in our implementation, see [ABSTRACTIONS.md](ABSTRACTIONS.md).\n\n## Adding your own dataset\n\nThis implementation adds support for COCO-style datasets.\nBut adding support for training on a new dataset can be done as follows:\n```python\nfrom maskrcnn_benchmark.structures.bounding_box import BoxList\n\nclass MyDataset(object):\n    def __init__(self, ...):\n        # as you would do normally\n\n    def __getitem__(self, idx):\n        # load the image as a PIL Image\n        image = ...\n\n        # load the bounding boxes as a list of list of boxes\n        # in this case, for illustrative purposes, we use\n        # x1, y1, x2, y2 order.\n        boxes = [[0, 0, 10, 10], [10, 20, 50, 50]]\n        # and labels\n        labels = torch.tensor([10, 20])\n\n        # create a BoxList from the boxes\n        boxlist = BoxList(boxes, image.size, mode=\"xyxy\")\n        # add the labels to the boxlist\n        boxlist.add_field(\"labels\", labels)\n\n        if self.transforms:\n            image, boxlist = self.transforms(image, boxlist)\n\n        # return the image, the boxlist and the idx in your dataset\n        return image, boxlist, idx\n\n    def get_img_info(self, idx):\n        # get img_height and img_width. This is used if\n        # we want to split the batches according to the aspect ratio\n        # of the image, as it can be more efficient than loading the\n        # image from disk\n        return {\"height\": img_height, \"width\": img_width}\n```\nThat's it. You can also add extra fields to the boxlist, such as segmentation masks\n(using `structures.segmentation_mask.SegmentationMask`), or even your own instance type.\n\nFor a full example of how the `COCODataset` is implemented, check [`maskrcnn_benchmark/data/datasets/coco.py`](maskrcnn_benchmark/data/datasets/coco.py).\n\nOnce you have created your dataset, it needs to be added in a couple of places:\n- [`maskrcnn_benchmark/data/datasets/__init__.py`](maskrcnn_benchmark/data/datasets/__init__.py): add it to `__all__`\n- [`maskrcnn_benchmark/config/paths_catalog.py`](maskrcnn_benchmark/config/paths_catalog.py): `DatasetCatalog.DATASETS` and corresponding `if` clause in `DatasetCatalog.get()`\n\n### Testing\nWhile the aforementioned example should work for training, we leverage the\ncocoApi for computing the accuracies during testing. Thus, test datasets\nshould currently follow the cocoApi for now.\n\nTo enable your dataset for testing, add a corresponding if statement in [`maskrcnn_benchmark/data/datasets/evaluation/__init__.py`](maskrcnn_benchmark/data/datasets/evaluation/__init__.py):\n```python\nif isinstance(dataset, datasets.MyDataset):\n        return coco_evaluation(**args)\n```\n\n## Finetuning from Detectron weights on custom datasets\nCreate a script `tools/trim_detectron_model.py` like [here](https://gist.github.com/wangg12/aea194aa6ab6a4de088f14ee193fd968).\nYou can decide which keys to be removed and which keys to be kept by modifying the script.\n\nThen you can simply point the converted model path in the config file by changing `MODEL.WEIGHT`.\n\nFor further information, please refer to [#15](https://github.com/facebookresearch/maskrcnn-benchmark/issues/15).\n\n## Troubleshooting\nIf you have issues running or compiling this code, we have compiled a list of common issues in\n[TROUBLESHOOTING.md](TROUBLESHOOTING.md). If your issue is not present there, please feel\nfree to open a new issue.\n\n## Citations\nPlease consider citing this project in your publications if it helps your research. The following is a BibTeX reference. The BibTeX entry requires the `url` LaTeX package.\n```\n@misc{massa2018mrcnn,\nauthor = {Massa, Francisco and Girshick, Ross},\ntitle = {{maskrcnn-benchmark: Fast, modular reference implementation of Instance Segmentation and Object Detection algorithms in PyTorch}},\nyear = {2018},\nhowpublished = {\\url{https://github.com/facebookresearch/maskrcnn-benchmark}},\nnote = {Accessed: [Insert date here]}\n}\n```\n\n## Projects using maskrcnn-benchmark\n\n- [RetinaMask: Learning to predict masks improves state-of-the-art single-shot detection for free](https://arxiv.org/abs/1901.03353). \n  Cheng-Yang Fu, Mykhailo Shvets, and Alexander C. Berg.\n  Tech report, arXiv,1901.03353.\n\n\n\n## License\n\nmaskrcnn-benchmark is released under the MIT license. See [LICENSE](LICENSE) for additional details.\n"
        },
        {
          "name": "MODEL_ZOO.md",
          "type": "blob",
          "size": 6.7158203125,
          "content": "## Model Zoo and Baselines\n\n### Hardware\n- 8 NVIDIA V100 GPUs\n\n### Software\n- PyTorch version: 1.0.0a0+dd2c487\n- CUDA 9.2\n- CUDNN 7.1\n- NCCL 2.2.13-1\n\n### End-to-end Faster and Mask R-CNN baselines\n\nAll the baselines were trained using the exact same experimental setup as in Detectron.\nWe initialize the detection models with ImageNet weights from Caffe2, the same as used by Detectron.\n\nThe pre-trained models are available in the link in the model id.\n\nbackbone | type | lr sched | im / gpu | train mem(GB) | train time (s/iter) | total train time(hr) | inference time(s/im) | box AP | mask AP | model id\n-- | -- | -- | -- | -- | -- | -- | -- | -- | -- | --\nR-50-C4 | Fast | 1x | 1 | 5.8 | 0.4036 | 20.2 | 0.17130 | 34.8 | - | [6358800](https://download.pytorch.org/models/maskrcnn/e2e_faster_rcnn_R_50_C4_1x.pth)\nR-50-FPN | Fast | 1x | 2 | 4.4 | 0.3530 | 8.8 | 0.12580 | 36.8 | - | [6358793](https://download.pytorch.org/models/maskrcnn/e2e_faster_rcnn_R_50_FPN_1x.pth)\nR-101-FPN | Fast | 1x | 2 | 7.1 | 0.4591 | 11.5 | 0.143149 | 39.1 | - | [6358804](https://download.pytorch.org/models/maskrcnn/e2e_faster_rcnn_R_101_FPN_1x.pth)\nX-101-32x8d-FPN | Fast | 1x | 1 | 7.6 | 0.7007 | 35.0 | 0.209965 | 41.2 | - | [6358717](https://download.pytorch.org/models/maskrcnn/e2e_faster_rcnn_X_101_32x8d_FPN_1x.pth)\nR-50-C4 | Mask | 1x | 1 | 5.8 | 0.4520 | 22.6 | 0.17796 + 0.028 | 35.6 | 31.5 | [6358801](https://download.pytorch.org/models/maskrcnn/e2e_mask_rcnn_R_50_C4_1x.pth)\nR-50-FPN | Mask | 1x | 2 | 5.2 | 0.4536 | 11.3 | 0.12966 + 0.034 | 37.8 | 34.2 | [6358792](https://download.pytorch.org/models/maskrcnn/e2e_mask_rcnn_R_50_FPN_1x.pth)\nR-101-FPN | Mask | 1x | 2 | 7.9 | 0.5665 | 14.2 | 0.15384 + 0.034 | 40.1 | 36.1 | [6358805](https://download.pytorch.org/models/maskrcnn/e2e_mask_rcnn_R_101_FPN_1x.pth)\nX-101-32x8d-FPN | Mask | 1x | 1 | 7.8 | 0.7562 | 37.8 | 0.21739 + 0.034 | 42.2 | 37.8 | [6358718](https://download.pytorch.org/models/maskrcnn/e2e_mask_rcnn_X_101_32x8d_FPN_1x.pth)\n\nFor person keypoint detection:\n\nbackbone | type | lr sched | im / gpu | train mem(GB) | train time (s/iter) | total train time(hr) | inference time(s/im) | box AP | keypoint AP | model id\n-- | -- | -- | -- | -- | -- | -- | -- | -- | -- | --\nR-50-FPN | Keypoint | 1x | 2 | 5.7 | 0.3771 | 9.4 | 0.10941 | 53.7 | 64.3 | 9981060\n\n### Light-weight Model baselines\n\nWe provided pre-trained models for selected FBNet models. \n* All the models are trained from scratched with BN using the training schedule specified below. \n* Evaluation is performed on a single NVIDIA V100 GPU with `MODEL.RPN.POST_NMS_TOP_N_TEST` set to `200`. \n\nThe following inference time is reported:\n  * inference total batch=8: Total inference time including data loading, model inference and pre/post preprocessing using 8 images per batch.\n  * inference model batch=8: Model inference time only and using 8 images per batch.\n  * inference model batch=1: Model inference time only and using 1 image per batch.\n  * inferenee caffe2 batch=1: Model inference time for the model in Caffe2 format using 1 image per batch. The Caffe2 models fused the BN to Conv and purely run on C++/CUDA by using Caffe2 ops for rpn/detection post processing.\n\nThe pre-trained models are available in the link in the model id.\n\nbackbone | type | resolution | lr sched | im / gpu | train mem(GB) | train time (s/iter) | total train time (hr) | inference total batch=8 (s/im) | inference model batch=8 (s/im) | inference model batch=1 (s/im) | inference caffe2 batch=1 (s/im) | box AP | mask AP | model id\n-- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | --\n[R-50-C4](configs/e2e_faster_rcnn_R_50_C4_1x.yaml) (reference) | Fast | 800 | 1x | 1 | 5.8 | 0.4036 | 20.2 | 0.0875 | **0.0793** | 0.0831 | **0.0625** | 34.4 | - | f35857197\n[fbnet_chamv1a](configs/e2e_faster_rcnn_fbnet_chamv1a_600.yaml) | Fast | 600 | 0.75x | 12 | 13.6 | 0.5444 | 20.5 | 0.0315 | **0.0260** | 0.0376 | **0.0188** | 33.5 | - | [f100940543](https://download.pytorch.org/models/maskrcnn/e2e_faster_rcnn_fbnet_chamv1a_600.pth)\n[fbnet_default](configs/e2e_faster_rcnn_fbnet_600.yaml) | Fast | 600 | 0.5x | 16 | 11.1 | 0.4872 | 12.5 | 0.0316 | **0.0250** | 0.0297 | **0.0130** | 28.2 | - | [f101086388](https://download.pytorch.org/models/maskrcnn/e2e_faster_rcnn_fbnet_600.pth)\n[R-50-C4](configs/e2e_mask_rcnn_R_50_C4_1x.yaml) (reference) | Mask | 800 | 1x | 1 | 5.8 | 0.452 | 22.6 | 0.0918 | **0.0848** | 0.0844 | - | 35.2 | 31.0 | f35858791\n[fbnet_xirb16d](configs/e2e_mask_rcnn_fbnet_xirb16d_dsmask_600.yaml) | Mask | 600 | 0.5x | 16 | 13.4 | 1.1732 | 29 | 0.0386 | **0.0319** | 0.0356 | - | 30.7 | 26.9 | [f101086394](https://download.pytorch.org/models/maskrcnn/e2e_mask_rcnn_fbnet_xirb16d_dsmask.pth)\n[fbnet_default](configs/e2e_mask_rcnn_fbnet_600.yaml) | Mask | 600 | 0.5x | 16 | 13.0 | 0.9036 | 23.0 | 0.0327 | **0.0269** | 0.0385 | - | 29.0 | 26.1 | [f101086385](https://download.pytorch.org/models/maskrcnn/e2e_mask_rcnn_fbnet_600.pth)\n\n## Comparison with Detectron and mmdetection\n\nIn the following section, we compare our implementation with [Detectron](https://github.com/facebookresearch/Detectron)\nand [mmdetection](https://github.com/open-mmlab/mmdetection).\nThe same remarks from [mmdetection](https://github.com/open-mmlab/mmdetection/blob/master/MODEL_ZOO.md#training-speed)\nabout different hardware applies here.\n\n### Training speed\n\nThe numbers here are in seconds / iteration. The lower, the better.\n\ntype | Detectron (P100) | mmdetection (V100) | maskrcnn_benchmark (V100)\n-- | -- | -- | --\nFaster R-CNN R-50 C4 | 0.566 | - | 0.4036\nFaster R-CNN R-50 FPN | 0.544 | 0.554 | 0.3530\nFaster R-CNN R-101 FPN | 0.647 | - | 0.4591\nFaster R-CNN X-101-32x8d FPN | 0.799 | - | 0.7007\nMask R-CNN R-50 C4 | 0.620 | - | 0.4520\nMask R-CNN R-50 FPN | 0.889 | 0.690 | 0.4536\nMask R-CNN R-101 FPN | 1.008 | - | 0.5665\nMask R-CNN X-101-32x8d FPN | 0.961 | - | 0.7562\n\n### Training memory\n\nThe lower, the better\n\ntype | Detectron (P100) | mmdetection (V100) | maskrcnn_benchmark (V100)\n-- | -- | -- | --\nFaster R-CNN R-50 C4 | 6.3 | - | 5.8\nFaster R-CNN R-50 FPN | 7.2 | 4.9 | 4.4\nFaster R-CNN R-101 FPN | 8.9 | - | 7.1\nFaster R-CNN X-101-32x8d FPN | 7.0 | - | 7.6\nMask R-CNN R-50 C4 | 6.6 | - | 5.8\nMask R-CNN R-50 FPN | 8.6 | 5.9 | 5.2\nMask R-CNN R-101 FPN | 10.2 | - | 7.9\nMask R-CNN X-101-32x8d FPN | 7.7 | - | 7.8\n\n### Accuracy\n\nThe higher, the better\n\ntype | Detectron (P100) | mmdetection (V100) | maskrcnn_benchmark (V100)\n-- | -- | -- | --\nFaster R-CNN R-50 C4 | 34.8 | - | 34.8\nFaster R-CNN R-50 FPN | 36.7 | 36.7 | 36.8\nFaster R-CNN R-101 FPN | 39.4 | - | 39.1\nFaster R-CNN X-101-32x8d FPN | 41.3 | - | 41.2\nMask R-CNN R-50 C4 | 35.8 & 31.4 | - | 35.6 & 31.5\nMask R-CNN R-50 FPN | 37.7 & 33.9 | 37.5 & 34.4 | 37.8 & 34.2\nMask R-CNN R-101 FPN | 40.0 & 35.9 | - | 40.1 & 36.1\nMask R-CNN X-101-32x8d FPN | 42.1 & 37.3 | - | 42.2 & 37.8\n\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 12.3984375,
          "content": "# FCOS: Fully Convolutional One-Stage Object Detection\n\nThis project hosts the code for implementing the FCOS algorithm for object detection, as presented in our paper:\n\n    FCOS: Fully Convolutional One-Stage Object Detection;\n    Zhi Tian, Chunhua Shen, Hao Chen, and Tong He;\n    In: Proc. Int. Conf. Computer Vision (ICCV), 2019.\n    arXiv preprint arXiv:1904.01355 \n\nThe full paper is available at: [https://arxiv.org/abs/1904.01355](https://arxiv.org/abs/1904.01355). \n\nImplementation based on Detectron2 is included in [AdelaiDet](https://github.com/aim-uofa/AdelaiDet#coco-object-detecton-baselines-with-fcos).\n\n**A real-time model with 46FPS and 40.3 in AP on COCO minival is also available [here](https://github.com/aim-uofa/AdelaiDet/blob/master/configs/FCOS-Detection/README.md#fcos-real-time-models).**\n\n## Highlights\n- **Totally anchor-free:**  FCOS completely avoids the complicated computation related to anchor boxes and all hyper-parameters of anchor boxes.   \n- **Better performance:** The very simple one-stage detector achieves much better performance (38.7 vs. 36.8 in AP with ResNet-50) than Faster R-CNN. Check out more models and experimental results [here](#models).\n- **Faster training and testing:** With the same hardwares and backbone ResNet-50-FPN, FCOS also requires less training hours (6.5h vs. 8.8h) than Faster R-CNN. FCOS also takes 12ms less inference time per image than Faster R-CNN (44ms vs. 56ms).\n- **State-of-the-art performance:** Our best model based on ResNeXt-64x4d-101 and deformable convolutions achieves **49.0%** in AP on COCO test-dev (with multi-scale testing).\n\n## Updates\n   - FCOS with Fast And Diverse (FAD) neural architecture search is avaliable at [FAD](https://github.com/MalongTech/research-fad). (30/10/2020)\n   - Script for exporting [ONNX models](https://github.com/tianzhi0549/FCOS/tree/master/onnx). (21/11/2019)\n   - New NMS (see [#165](https://github.com/tianzhi0549/FCOS/pull/165)) speeds up ResNe(x)t based models by up to 30% and MobileNet based models by 40%, with exactly the same performance. Check out [here](#models). (12/10/2019)\n   - New models with much improved performance are released. The best model achieves **49%** in AP on COCO test-dev with multi-scale testing. (11/09/2019)\n   - FCOS with VoVNet backbones is available at [VoVNet-FCOS](https://github.com/vov-net/VoVNet-FCOS). (08/08/2019)\n   - A trick of using a small central region of the BBox for training improves AP by nearly 1 point [as shown here](https://github.com/yqyao/FCOS_PLUS). (23/07/2019)\n   - FCOS with HRNet backbones is available at [HRNet-FCOS](https://github.com/HRNet/HRNet-FCOS). (03/07/2019)\n   - FCOS with AutoML searched FPN (R50, R101, ResNeXt101 and MobileNetV2 backbones) is available at [NAS-FCOS](https://github.com/Lausannen/NAS-FCOS). (30/06/2019)\n   - FCOS has been implemented in [mmdetection](https://github.com/open-mmlab/mmdetection). Many thanks to [@yhcao6](https://github.com/yhcao6) and [@hellock](https://github.com/hellock). (17/05/2019)\n\n## Required hardware\nWe use 8 Nvidia V100 GPUs. \\\nBut 4 1080Ti GPUs can also train a fully-fledged ResNet-50-FPN based FCOS since FCOS is memory-efficient.  \n\n## Installation\n#### Testing-only installation \nFor users who only want to use FCOS as an object detector in their projects, they can install it by pip. To do so, run:\n```\npip install torch  # install pytorch if you do not have it\npip install git+https://github.com/tianzhi0549/FCOS.git\n# run this command line for a demo \nfcos https://github.com/tianzhi0549/FCOS/raw/master/demo/images/COCO_val2014_000000000885.jpg\n```\nPlease check out [here](fcos/bin/fcos) for the interface usage.\n\n#### For a complete installation \nThis FCOS implementation is based on [maskrcnn-benchmark](https://github.com/facebookresearch/maskrcnn-benchmark). Therefore the installation is the same as original maskrcnn-benchmark.\n\nPlease check [INSTALL.md](INSTALL.md) for installation instructions.\nYou may also want to see the original [README.md](MASKRCNN_README.md) of maskrcnn-benchmark.\n\n## A quick demo\nOnce the installation is done, you can follow the below steps to run a quick demo.\n    \n    # assume that you are under the root directory of this project,\n    # and you have activated your virtual environment if needed.\n    wget https://huggingface.co/tianzhi/FCOS/resolve/main/FCOS_imprv_R_50_FPN_1x.pth?download=true -O FCOS_imprv_R_50_FPN_1x.pth\n    python demo/fcos_demo.py\n\n\n## Inference\nThe inference command line on coco minival split:\n\n    python tools/test_net.py \\\n        --config-file configs/fcos/fcos_imprv_R_50_FPN_1x.yaml \\\n        MODEL.WEIGHT FCOS_imprv_R_50_FPN_1x.pth \\\n        TEST.IMS_PER_BATCH 4    \n\nPlease note that:\n1) If your model's name is different, please replace `FCOS_imprv_R_50_FPN_1x.pth` with your own.\n2) If you enounter out-of-memory error, please try to reduce `TEST.IMS_PER_BATCH` to 1.\n3) If you want to evaluate a different model, please change `--config-file` to its config file (in [configs/fcos](configs/fcos)) and `MODEL.WEIGHT` to its weights file.\n4) Multi-GPU inference is available, please refer to [#78](https://github.com/tianzhi0549/FCOS/issues/78#issuecomment-526990989).\n5) We improved the postprocess efficiency by using multi-label nms (see [#165](https://github.com/tianzhi0549/FCOS/pull/165)), which saves 18ms on average. The inference metric in the following tables has been updated accordingly.\n\n## Models\nFor your convenience, we provide the following trained models (more models are coming soon).\n\n**ResNe(x)ts:**\n\n*All ResNe(x)t based models are trained with 16 images in a mini-batch and frozen batch normalization (i.e., consistent with models in [maskrcnn_benchmark](https://github.com/facebookresearch/maskrcnn-benchmark)).*\n\nModel | Multi-scale training | Testing time / im | AP (minival) | Link\n--- |:---:|:---:|:---:|:---:\nFCOS_imprv_R_50_FPN_1x | No | 44ms | 38.7 | [download](https://huggingface.co/tianzhi/FCOS/resolve/main/FCOS_imprv_R_50_FPN_1x.pth?download=true)\nFCOS_imprv_dcnv2_R_50_FPN_1x | No | 54ms | 42.3 | [download](https://huggingface.co/tianzhi/FCOS/resolve/main/FCOS_imprv_dcnv2_R_50_FPN_1x.pth?download=true)\nFCOS_imprv_R_101_FPN_2x | Yes | 57ms | 43.0 | [download](https://huggingface.co/tianzhi/FCOS/resolve/main/FCOS_imprv_R_101_FPN_2x.pth?download=true)\nFCOS_imprv_dcnv2_R_101_FPN_2x | Yes | 73ms | 45.6 | [download](https://huggingface.co/tianzhi/FCOS/resolve/main/FCOS_imprv_dcnv2_R_101_FPN_2x.pth?download=true)\nFCOS_imprv_X_101_32x8d_FPN_2x | Yes | 110ms | 44.0 | [download](https://huggingface.co/tianzhi/FCOS/resolve/main/FCOS_imprv_X_101_32x8d_FPN_2x.pth?download=true)\nFCOS_imprv_dcnv2_X_101_32x8d_FPN_2x | Yes | 143ms | 46.4 | [download](https://huggingface.co/tianzhi/FCOS/resolve/main/FCOS_imprv_dcnv2_X_101_32x8d_FPN_2x.pth?download=true)\nFCOS_imprv_X_101_64x4d_FPN_2x | Yes | 112ms | 44.7 | [download](https://huggingface.co/tianzhi/FCOS/resolve/main/FCOS_imprv_X_101_64x4d_FPN_2x.pth?download=true)\nFCOS_imprv_dcnv2_X_101_64x4d_FPN_2x | Yes | 144ms | 46.6 | [download](https://huggingface.co/tianzhi/FCOS/resolve/main/FCOS_imprv_dcnv2_X_101_64x4d_FPN_2x.pth?download=true)\n\n*Note that `imprv` denotes `improvements` in our paper Table 3. These almost cost-free changes improve the performance by ~1.5% in total. Thus, we highly recommend to use them. The following are the original models presented in our initial paper.*\n\nModel | Multi-scale training | Testing time / im | AP (minival) | AP (test-dev) | Link\n--- |:---:|:---:|:---:|:---:|:---:\nFCOS_R_50_FPN_1x | No | 45ms | 37.1 | 37.4 | [download](https://huggingface.co/tianzhi/FCOS/resolve/main/FCOS_R_50_FPN_1x.pth?download=true)\nFCOS_R_101_FPN_2x | Yes | 59ms | 41.4 | 41.5 | [download](https://huggingface.co/tianzhi/FCOS/resolve/main/FCOS_R_101_FPN_2x.pth?download=true)\nFCOS_X_101_32x8d_FPN_2x | Yes | 110ms | 42.5 | 42.7 | [download](https://huggingface.co/tianzhi/FCOS/resolve/main/FCOS_X_101_32x8d_FPN_2x.pth?download=true)\nFCOS_X_101_64x4d_FPN_2x | Yes | 113ms | 43.0 | 43.2 | [download](https://huggingface.co/tianzhi/FCOS/resolve/main/FCOS_X_101_64x4d_FPN_2x.pth?download=true)\n\n**MobileNets:**\n\n*We update batch normalization for MobileNet based models. If you want to use SyncBN, please install pytorch 1.1 or later.*\n\nModel | Training batch size | Multi-scale training | Testing time / im | AP (minival) | Link\n--- |:---:|:---:|:---:|:---:|:---:\nFCOS_syncbn_bs32_c128_MNV2_FPN_1x | 32 | No | 26ms | 30.9 | [download](https://huggingface.co/tianzhi/FCOS/resolve/main/FCOS_syncbn_bs32_c128_MNV2_FPN_1x.pth?download=true)\nFCOS_syncbn_bs32_MNV2_FPN_1x | 32 | No | 33ms | 33.1 | [download](https://huggingface.co/tianzhi/FCOS/resolve/main/FCOS_syncbn_bs32_MNV2_FPN_1x.pth?download=true)\nFCOS_bn_bs16_MNV2_FPN_1x | 16 | No | 44ms | 31.0 | [download](https://huggingface.co/tianzhi/FCOS/resolve/main/FCOS_bn_bs16_MNV2_FPN_1x.pth?download=true)\n\n[1] *1x and 2x mean the model is trained for 90K and 180K iterations, respectively.* \\\n[2] *All results are obtained with a single model and without any test time data augmentation such as multi-scale, flipping and etc..* \\\n[3] *`c128` denotes the model has 128 (instead of 256) channels in towers (i.e., `MODEL.RESNETS.BACKBONE_OUT_CHANNELS` in [config](https://github.com/tianzhi0549/FCOS/blob/master/configs/fcos/fcos_syncbn_bs32_c128_MNV2_FPN_1x.yaml#L10)).* \\\n[4] *`dcnv2` denotes deformable convolutional networks v2. Note that for ResNet based models, we apply deformable convolutions from stage c3 to c5 in backbones. For ResNeXt based models, only stage c4 and c5 use deformable convolutions. All models use deformable convolutions in the last layer of detector towers.* \\\n[5] *The model `FCOS_imprv_dcnv2_X_101_64x4d_FPN_2x` with multi-scale testing achieves 49.0% in AP on COCO test-dev. Please use `TEST.BBOX_AUG.ENABLED True` to enable multi-scale testing.*\n\n## Training\n\nThe following command line will train FCOS_imprv_R_50_FPN_1x on 8 GPUs with Synchronous Stochastic Gradient Descent (SGD):\n\n    python -m torch.distributed.launch \\\n        --nproc_per_node=8 \\\n        --master_port=$((RANDOM + 10000)) \\\n        tools/train_net.py \\\n        --config-file configs/fcos/fcos_imprv_R_50_FPN_1x.yaml \\\n        DATALOADER.NUM_WORKERS 2 \\\n        OUTPUT_DIR training_dir/fcos_imprv_R_50_FPN_1x\n        \nNote that:\n1) If you want to use fewer GPUs, please change `--nproc_per_node` to the number of GPUs. No other settings need to be changed. The total batch size does not depends on `nproc_per_node`. If you want to change the total batch size, please change `SOLVER.IMS_PER_BATCH` in [configs/fcos/fcos_R_50_FPN_1x.yaml](configs/fcos/fcos_R_50_FPN_1x.yaml).\n2) The models will be saved into `OUTPUT_DIR`.\n3) If you want to train FCOS with other backbones, please change `--config-file`.\n4) If you want to train FCOS on your own dataset, please follow this instruction [#54](https://github.com/tianzhi0549/FCOS/issues/54#issuecomment-497558687).\n5) Now, training with 8 GPUs and 4 GPUs can have the same performance. Previous performance gap was because we did not synchronize `num_pos` between GPUs when computing loss. \n\n## ONNX\nPlease refer to the directory [onnx](onnx) for an example of exporting the model to ONNX.\nA converted model can be downloaded [here](https://cloudstor.aarnet.edu.au/plus/s/38fQAdi2HBkn274/download).\nWe recommend you to use PyTorch >= 1.4.0 (or nightly) and torchvision >= 0.5.0 (or nightly) for ONNX models.\n\n## Contributing to the project\nAny pull requests or issues are welcome.\n\n## Citations\nPlease consider citing our paper in your publications if the project helps your research. BibTeX reference is as follows.\n```\n@inproceedings{tian2019fcos,\n  title   =  {{FCOS}: Fully Convolutional One-Stage Object Detection},\n  author  =  {Tian, Zhi and Shen, Chunhua and Chen, Hao and He, Tong},\n  booktitle =  {Proc. Int. Conf. Computer Vision (ICCV)},\n  year    =  {2019}\n}\n```\n```\n@article{tian2021fcos,\n  title   =  {{FCOS}: A Simple and Strong Anchor-free Object Detector},\n  author  =  {Tian, Zhi and Shen, Chunhua and Chen, Hao and He, Tong},\n  booktitle =  {IEEE T. Pattern Analysis and Machine Intelligence (TPAMI)},\n  year    =  {2021}\n}\n```\n\n\n# Acknowledgments\nWe would like to thank [@yqyao](https://github.com/yqyao) for the tricks of center sampling and GIoU.  We also thank [@bearcatt](https://github.com/bearcatt) for his suggestion of positioning the center-ness branch with box regression (refer to [#89](https://github.com/tianzhi0549/FCOS/issues/89#issuecomment-516877042)).    \n\n## License\n\nFor academic use, this project is licensed under the 2-clause BSD License - see the LICENSE file for details. For commercial use, please contact the authors. \n"
        },
        {
          "name": "TROUBLESHOOTING.md",
          "type": "blob",
          "size": 2.869140625,
          "content": "# Troubleshooting\n\nHere is a compilation if common issues that you might face\nwhile compiling / running this code:\n\n## Compilation errors when compiling the library\nIf you encounter build errors like the following:\n```\n/usr/include/c++/6/type_traits:1558:8: note: provided for ‘template<class _From, class _To> struct std::is_convertible’\n     struct is_convertible\n        ^~~~~~~~~~~~~~\n/usr/include/c++/6/tuple:502:1: error: body of constexpr function ‘static constexpr bool std::_TC<<anonymous>, _Elements>::_NonNestedTuple() [with _SrcTuple = std::tuple<at::Tensor, at::Tensor, at::Tensor, at::Tensor>&&; bool <anonymous> = true; _Elements = {at::Tensor, at::Tensor, at::Tensor, at::Tensor}]’ not a return-statement\n     }\n ^\nerror: command '/usr/local/cuda/bin/nvcc' failed with exit status 1\n```\ncheck your CUDA version and your `gcc` version.\n```\nnvcc --version\ngcc --version\n```\nIf you are using CUDA 9.0 and gcc 6.4.0, then refer to https://github.com/facebookresearch/maskrcnn-benchmark/issues/25,\nwhich has a summary of the solution. Basically, CUDA 9.0 is not compatible with gcc 6.4.0.\n\n## ImportError: No module named maskrcnn_benchmark.config when running webcam.py\n\nThis means that `maskrcnn-benchmark` has not been properly installed.\nRefer to https://github.com/facebookresearch/maskrcnn-benchmark/issues/22 for a few possible issues.\nNote that we now support Python 2 as well.\n\n\n## ImportError: Undefined symbol: __cudaPopCallConfiguration error when import _C\n\nThis probably means that the inconsistent version of NVCC compile and your conda CUDAToolKit package. This is firstly mentioned in https://github.com/facebookresearch/maskrcnn-benchmark/issues/45 . All you need to do is:\n\n```\n# Check the NVCC compile version(e.g.)\n/usr/cuda-9.2/bin/nvcc --version\n# Check the CUDAToolKit version(e.g.)\n~/anaconda3/bin/conda list | grep cuda\n\n# If you need to update your CUDAToolKit\n~/anaconda3/bin/conda install -c anaconda cudatoolkit==9.2\n```\n\nBoth of them should have the **same** version. For example, if NVCC==9.2 and CUDAToolKit==9.2, this will be fine while when NVCC==9.2 but CUDAToolKit==9, it fails.\n\n\n## Segmentation fault (core dumped) when running the library\nThis probably means that you have compiled the library using GCC < 4.9, which is ABI incompatible with PyTorch.\nIndeed, during installation, you probably saw a message like\n```\nYour compiler (g++ 4.8) may be ABI-incompatible with PyTorch!\nPlease use a compiler that is ABI-compatible with GCC 4.9 and above.\nSee https://gcc.gnu.org/onlinedocs/libstdc++/manual/abi.html.\n\nSee https://gist.github.com/goldsborough/d466f43e8ffc948ff92de7486c5216d6\nfor instructions on how to install GCC 4.9 or higher.\n```\nFollow the instructions on https://gist.github.com/goldsborough/d466f43e8ffc948ff92de7486c5216d6\nto install GCC 4.9 or higher, and try recompiling `maskrcnn-benchmark` again, after cleaning the\n`build` folder with\n```\nrm -rf build\n```\n\n\n"
        },
        {
          "name": "configs",
          "type": "tree",
          "content": null
        },
        {
          "name": "demo",
          "type": "tree",
          "content": null
        },
        {
          "name": "docker",
          "type": "tree",
          "content": null
        },
        {
          "name": "fcos",
          "type": "tree",
          "content": null
        },
        {
          "name": "fcos_core",
          "type": "tree",
          "content": null
        },
        {
          "name": "onnx",
          "type": "tree",
          "content": null
        },
        {
          "name": "requirements.txt",
          "type": "blob",
          "size": 0.078125,
          "content": "torch\ntorchvision\nninja\nyacs\ncython\nmatplotlib\ntqdm\nopencv-python\nscikit-image\n\n"
        },
        {
          "name": "setup.py",
          "type": "blob",
          "size": 2.025390625,
          "content": "#!/usr/bin/env python\n# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.\n\nimport glob\nimport os\n\nimport torch\nfrom setuptools import find_packages\nfrom setuptools import setup\nfrom torch.utils.cpp_extension import CUDA_HOME\nfrom torch.utils.cpp_extension import CppExtension\nfrom torch.utils.cpp_extension import CUDAExtension\n\n\nrequirements = [\n    \"torchvision\",\n    \"ninja\",\n    \"yacs\",\n    \"cython\",\n    \"matplotlib\",\n    \"tqdm\",\n    \"opencv-python\",\n    \"scikit-image\"\n]\n\n\ndef get_extensions():\n    extensions_dir = os.path.join(\"fcos_core\", \"csrc\")\n\n    main_file = glob.glob(os.path.join(extensions_dir, \"*.cpp\"))\n    source_cpu = glob.glob(os.path.join(extensions_dir, \"cpu\", \"*.cpp\"))\n    source_cuda = glob.glob(os.path.join(extensions_dir, \"cuda\", \"*.cu\"))\n    sources = main_file + source_cpu\n\n    extension = CppExtension\n\n    extra_compile_args = {\"cxx\": []}\n    define_macros = []\n\n    if (torch.cuda.is_available() and CUDA_HOME is not None) or os.getenv(\"FORCE_CUDA\", \"0\") == \"1\":\n        extension = CUDAExtension\n        sources += source_cuda\n        define_macros += [(\"WITH_CUDA\", None)]\n        extra_compile_args[\"nvcc\"] = [\n            \"-DCUDA_HAS_FP16=1\",\n            \"-D__CUDA_NO_HALF_OPERATORS__\",\n            \"-D__CUDA_NO_HALF_CONVERSIONS__\",\n            \"-D__CUDA_NO_HALF2_OPERATORS__\",\n        ]\n\n    include_dirs = [extensions_dir]\n\n    ext_modules = [\n        extension(\n            \"fcos_core._C\",\n            sources,\n            include_dirs=include_dirs,\n            define_macros=define_macros,\n            extra_compile_args=extra_compile_args\n        )\n    ]\n\n    return ext_modules\n\n\nsetup(\n    name=\"fcos\",\n    version=\"0.1.9\",\n    author=\"Zhi Tian\",\n    url=\"https://github.com/tianzhi0549/FCOS\",\n    description=\"FCOS object detector in pytorch\",\n    scripts=[\"fcos/bin/fcos\"],\n    packages=find_packages(exclude=(\"configs\", \"tests\",)),\n    install_requires=requirements,\n    ext_modules=get_extensions(),\n    cmdclass={\"build_ext\": torch.utils.cpp_extension.BuildExtension},\n    include_package_data=True,\n)\n"
        },
        {
          "name": "tests",
          "type": "tree",
          "content": null
        },
        {
          "name": "tools",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}