{
  "metadata": {
    "timestamp": 1736560031554,
    "page": 848,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjg1MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "tensorlayer/SRGAN",
      "stars": 3362,
      "defaultBranch": "master",
      "files": [
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.06640625,
          "content": "._*\n*.pyc\n.DS_Store\n*.npz\nsample/\nsamples/\ncheckpoint/\n__pycache__/\n"
        },
        {
          "name": ".style.yapf",
          "type": "blob",
          "size": 0.0263671875,
          "content": "[style]\ncolumn_limit = 160\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 6.8515625,
          "content": "## Super Resolution Examples\n\n- Implementation of [\"Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network\"](https://arxiv.org/abs/1609.04802)\n\n- For earlier version, please check [srgan release](https://github.com/tensorlayer/srgan/releases) and [tensorlayer](https://github.com/tensorlayer/TensorLayer).\n\n- For more computer vision applications, check [TLXCV](https://github.com/tensorlayer/TLXCV)\n\n\n### SRGAN Architecture\n\n\n<a href=\"https://github.com/tensorlayer/TensorLayerX\">\n<div align=\"center\">\n\t<img src=\"img/model.jpeg\" width=\"80%\" height=\"10%\"/>\n</div>\n</a>\n<a href=\"https://github.com/tensorlayer/TensorLayerX\">\n<div align=\"center\">\n\t<img src=\"img/SRGAN_Result3.png\" width=\"80%\" height=\"50%\"/>\n</div>\n</a>\n\n### Prepare Data and Pre-trained VGG\n\n- 1. You need to download the pretrained VGG19 model weights in [here](https://drive.google.com/file/d/1CLw6Cn3yNI1N15HyX99_Zy9QnDcgP3q7/view?usp=sharing).\n- 2. You need to have the high resolution images for training.\n  -  In this experiment, I used images from [DIV2K - bicubic downscaling x4 competition](http://www.vision.ee.ethz.ch/ntire17/), so the hyper-paremeters in `config.py` (like number of epochs) are seleted basic on that dataset, if you change a larger dataset you can reduce the number of epochs. \n  -  If you dont want to use DIV2K dataset, you can also use [Yahoo MirFlickr25k](http://press.liacs.nl/mirflickr/mirdownload.html), just simply download it using `train_hr_imgs = tl.files.load_flickr25k_dataset(tag=None)` in `main.py`. \n  -  If you want to use your own images, you can set the path to your image folder via `config.TRAIN.hr_img_path` in `config.py`.\n\n\n\n### Run\n\nðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ You need install [TensorLayerX](https://github.com/tensorlayer/TensorLayerX#installation) at first!\n\nðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ Please install TensorLayerX via source\n\n```bash\npip install git+https://github.com/tensorlayer/tensorlayerx.git \n```\n\n#### Train\n- Set your image folder in `config.py`, if you download [DIV2K - bicubic downscaling x4 competition](http://www.vision.ee.ethz.ch/ntire17/) dataset, you don't need to change it. \n- Other links for DIV2K, in case you can't find it : [test\\_LR\\_bicubic_X4](https://data.vision.ee.ethz.ch/cvl/DIV2K/validation_release/DIV2K_test_LR_bicubic_X4.zip), [train_HR](https://data.vision.ee.ethz.ch/cvl/DIV2K/DIV2K_train_HR.zip), [train\\_LR\\_bicubic_X4](https://data.vision.ee.ethz.ch/cvl/DIV2K/DIV2K_train_LR_bicubic_X4.zip), [valid_HR](https://data.vision.ee.ethz.ch/cvl/DIV2K/validation_release/DIV2K_valid_HR.zip), [valid\\_LR\\_bicubic_X4](https://data.vision.ee.ethz.ch/cvl/DIV2K/DIV2K_valid_LR_bicubic_X4.zip).\n\n```python\nconfig.TRAIN.img_path = \"your_image_folder/\"\n```\nYour directory structure should look like this:\n\n```\nsrgan/\n    â””â”€â”€ config.py\n    â””â”€â”€ srgan.py\n    â””â”€â”€ train.py\n    â””â”€â”€ vgg.py\n    â””â”€â”€ model\n          â””â”€â”€ vgg19.npy\n    â””â”€â”€ DIV2K\n          â””â”€â”€ DIV2K_train_HR\n          â”œâ”€â”€ DIV2K_train_LR_bicubic\n          â”œâ”€â”€ DIV2K_valid_HR\n          â””â”€â”€ DIV2K_valid_LR_bicubic\n\n```\n\n- Start training.\n\n```bash\npython train.py\n```\n\nðŸ”¥Modify a line of code in **train.py**, easily switch to any framework!\n\n```python\nimport os\nos.environ['TL_BACKEND'] = 'tensorflow'\n# os.environ['TL_BACKEND'] = 'mindspore'\n# os.environ['TL_BACKEND'] = 'paddle'\n# os.environ['TL_BACKEND'] = 'pytorch'\n```\nðŸš§ We will support PyTorch as Backend soon.\n\n\n#### Evaluation.\n\nðŸ”¥ We have trained SRGAN on DIV2K dataset.\nðŸ”¥ Download model weights as follows.\n\n|              | SRGAN_g | SRGAN_d | \n|------------- |---------|---------|\n| TensorFlow   | [Baidu](https://pan.baidu.com/s/118uUg3oce_3NZQCIWHVjmA?pwd=p9li), [Googledrive](https://drive.google.com/file/d/1GlU9At-5XEDilgnt326fyClvZB_fsaFZ/view?usp=sharing) |[Baidu](https://pan.baidu.com/s/1DOpGzDJY5PyusKzaKqbLOg?pwd=g2iy), [Googledrive](https://drive.google.com/file/d/1RpOtVcVK-yxnVhNH4KSjnXHDvuU_pq3j/view?usp=sharing)   |        \n| PaddlePaddle | [Baidu](https://pan.baidu.com/s/1ngBpleV5vQZQqNE_8djDIg?pwd=s8wc), [Googledrive](https://drive.google.com/file/d/1GRNt_ZsgorB19qvwN5gE6W9a_bIPLkg1/view?usp=sharing)  | [Baidu](https://pan.baidu.com/s/1nSefLNRanFImf1DskSVpCg?pwd=befc), [Googledrive](https://drive.google.com/file/d/1Jf6W1ZPdgtmUSfrQ5mMZDB_hOCVU-zFo/view?usp=sharing)   |         \n| MindSpore    | ðŸš§Coming soon!    | ðŸš§Coming soon!     |         \n| PyTorch      | ðŸš§Coming soon!    | ðŸš§Coming soon!     |\n\n\nDownload weights file and put weights under the folder srgan/models/.\n\nYour directory structure should look like this:\n\n```\nsrgan/\n    â””â”€â”€ config.py\n    â””â”€â”€ srgan.py\n    â””â”€â”€ train.py\n    â””â”€â”€ vgg.py\n    â””â”€â”€ model\n          â””â”€â”€ vgg19.npy\n    â””â”€â”€ DIV2K\n          â”œâ”€â”€ DIV2K_train_HR\n          â”œâ”€â”€ DIV2K_train_LR_bicubic\n          â”œâ”€â”€ DIV2K_valid_HR\n          â””â”€â”€ DIV2K_valid_LR_bicubic\n    â””â”€â”€ models\n          â”œâ”€â”€ g.npz  # You should rename the weigths file. \n          â””â”€â”€ d.npz  # If you set os.environ['TL_BACKEND'] = 'tensorflow',you should rename srgan-g-tensorflow.npz to g.npz .\n\n```\n\n- Start evaluation.\n```bash\npython train.py --mode=eval\n```\n\nResults will be saved under the folder srgan/samples/. \n\n### Results\n\n<a href=\"http://tensorlayer.readthedocs.io\">\n<div align=\"center\">\n\t<img src=\"img/SRGAN_Result2.png\" width=\"80%\" height=\"50%\"/>\n</div>\n</a>\n\n\n### Reference\n* [1] [Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network](https://arxiv.org/abs/1609.04802)\n* [2] [Is the deconvolution layer the same as a convolutional layer ?](https://arxiv.org/abs/1609.07009)\n\n\n\n### Citation\nIf you find this project useful, we would be grateful if you cite the TensorLayer paperï¼š\n\n```\n@article{tensorlayer2017,\nauthor = {Dong, Hao and Supratak, Akara and Mai, Luo and Liu, Fangde and Oehmichen, Axel and Yu, Simiao and Guo, Yike},\njournal = {ACM Multimedia},\ntitle = {{TensorLayer: A Versatile Library for Efficient Deep Learning Development}},\nurl = {http://tensorlayer.org},\nyear = {2017}\n}\n\n@inproceedings{tensorlayer2021,\n  title={TensorLayer 3.0: A Deep Learning Library Compatible With Multiple Backends},\n  author={Lai, Cheng and Han, Jiarong and Dong, Hao},\n  booktitle={2021 IEEE International Conference on Multimedia \\& Expo Workshops (ICMEW)},\n  pages={1--3},\n  year={2021},\n  organization={IEEE}\n}\n```\n\n### Other Projects\n\n- [Style Transfer](https://github.com/tensorlayer/adaptive-style-transfer)\n- [Pose Estimation](https://github.com/tensorlayer/openpose)\n\n### Discussion\n\n- [TensorLayer Slack](https://join.slack.com/t/tensorlayer/shared_invite/enQtMjUyMjczMzU2Njg4LWI0MWU0MDFkOWY2YjQ4YjVhMzI5M2VlZmE4YTNhNGY1NjZhMzUwMmQ2MTc0YWRjMjQzMjdjMTg2MWQ2ZWJhYzc)\n- [TensorLayer WeChat](https://github.com/tensorlayer/tensorlayer-chinese/blob/master/docs/wechat_group.md)\n\n### License\n\n- For academic and non-commercial use only.\n- For commercial use, please contact tensorlayer@gmail.com.\n"
        },
        {
          "name": "config.py",
          "type": "blob",
          "size": 1.04296875,
          "content": "from easydict import EasyDict as edict\nimport json\n\nconfig = edict()\nconfig.TRAIN = edict()\nconfig.TRAIN.batch_size = 16 # [16] use 8 if your GPU memory is small\nconfig.TRAIN.lr_init = 1e-4\nconfig.TRAIN.beta1 = 0.9\n\n## initialize G\nconfig.TRAIN.n_epoch_init = 100\n    # config.TRAIN.lr_decay_init = 0.1\n    # config.TRAIN.decay_every_init = int(config.TRAIN.n_epoch_init / 2)\n\n## adversarial learning (SRGAN)\nconfig.TRAIN.n_epoch = 2000\nconfig.TRAIN.lr_decay = 0.1\nconfig.TRAIN.decay_every = int(config.TRAIN.n_epoch / 2)\n\n## train set location\nconfig.TRAIN.hr_img_path = 'DIV2K/DIV2K_train_HR/'\nconfig.TRAIN.lr_img_path = 'DIV2K/DIV2K_train_LR_bicubic/X4/'\n\nconfig.VALID = edict()\n## test set location\nconfig.VALID.hr_img_path = 'DIV2K/DIV2K_valid_HR/'\nconfig.VALID.lr_img_path = 'DIV2K/DIV2K_valid_LR_bicubic/X4/'\n\ndef log_config(filename, cfg):\n    with open(filename, 'w') as f:\n        f.write(\"================================================\\n\")\n        f.write(json.dumps(cfg, indent=4))\n        f.write(\"\\n================================================\\n\")\n"
        },
        {
          "name": "img",
          "type": "tree",
          "content": null
        },
        {
          "name": "requirements.txt",
          "type": "blob",
          "size": 0.1005859375,
          "content": "git+https://github.com/tensorlayer/tensorlayerx.git\nnumpy>=1.16.1\neasydict==1.9\nopencv-python>=4.5.1.48"
        },
        {
          "name": "srgan.py",
          "type": "blob",
          "size": 16.2177734375,
          "content": "from tensorlayerx.nn import Module\nimport tensorlayerx as tlx\nfrom tensorlayerx.nn import Conv2d, BatchNorm2d, Elementwise, SubpixelConv2d, UpSampling2d, Flatten, Sequential\nfrom tensorlayerx.nn import Linear, MaxPool2d\n\nW_init = tlx.initializers.TruncatedNormal(stddev=0.02)\nG_init = tlx.initializers.TruncatedNormal(mean=1.0, stddev=0.02)\n\n\nclass ResidualBlock(Module):\n\n    def __init__(self):\n        super(ResidualBlock, self).__init__()\n        self.conv1 = Conv2d(\n            out_channels=64, kernel_size=(3, 3), stride=(1, 1), act=None, padding='SAME', W_init=W_init,\n            data_format='channels_first', b_init=None\n        )\n        self.bn1 = BatchNorm2d(num_features=64, act=tlx.ReLU, gamma_init=G_init, data_format='channels_first')\n        self.conv2 = Conv2d(\n            out_channels=64, kernel_size=(3, 3), stride=(1, 1), act=None, padding='SAME', W_init=W_init,\n            data_format='channels_first', b_init=None\n        )\n        self.bn2 = BatchNorm2d(num_features=64, act=None, gamma_init=G_init, data_format='channels_first')\n\n    def forward(self, x):\n        z = self.conv1(x)\n        z = self.bn1(z)\n        z = self.conv2(z)\n        z = self.bn2(z)\n        x = x + z\n        return x\n\n\nclass SRGAN_g(Module):\n    \"\"\" Generator in Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network\n    feature maps (n) and stride (s) feature maps (n) and stride (s)\n    \"\"\"\n\n    def __init__(self):\n        super(SRGAN_g, self).__init__()\n        self.conv1 = Conv2d(\n            out_channels=64, kernel_size=(3, 3), stride=(1, 1), act=tlx.ReLU, padding='SAME', W_init=W_init,\n            data_format='channels_first'\n        )\n        self.residual_block = self.make_layer()\n        self.conv2 = Conv2d(\n            out_channels=64, kernel_size=(3, 3), stride=(1, 1), padding='SAME', W_init=W_init,\n            data_format='channels_first', b_init=None\n        )\n        self.bn1 = BatchNorm2d(num_features=64, act=None, gamma_init=G_init, data_format='channels_first')\n        self.conv3 = Conv2d(out_channels=256, kernel_size=(3, 3), stride=(1, 1), padding='SAME', W_init=W_init, data_format='channels_first')\n        self.subpiexlconv1 = SubpixelConv2d(data_format='channels_first', scale=2, act=tlx.ReLU)\n        self.conv4 = Conv2d(out_channels=256, kernel_size=(3, 3), stride=(1, 1), padding='SAME', W_init=W_init, data_format='channels_first')\n        self.subpiexlconv2 = SubpixelConv2d(data_format='channels_first', scale=2, act=tlx.ReLU)\n        self.conv5 = Conv2d(3, kernel_size=(1, 1), stride=(1, 1), act=tlx.Tanh, padding='SAME', W_init=W_init, data_format='channels_first')\n\n    def make_layer(self):\n        layer_list = []\n        for i in range(16):\n            layer_list.append(ResidualBlock())\n        return Sequential(layer_list)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        temp = x\n        x = self.residual_block(x)\n        x = self.conv2(x)\n        x = self.bn1(x)\n        x = x + temp\n        x = self.conv3(x)\n        x = self.subpiexlconv1(x)\n        x = self.conv4(x)\n        x = self.subpiexlconv2(x)\n        x = self.conv5(x)\n\n        return x\n\n\nclass SRGAN_g2(Module):\n    \"\"\" Generator in Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network\n    feature maps (n) and stride (s) feature maps (n) and stride (s)\n\n    96x96 --> 384x384\n\n    Use Resize Conv\n    \"\"\"\n\n    def __init__(self):\n        super(SRGAN_g2, self).__init__()\n        self.conv1 = Conv2d(\n            out_channels=64, kernel_size=(3, 3), stride=(1, 1), act=None, padding='SAME', W_init=W_init,\n            data_format='channels_first'\n        )\n        self.residual_block = self.make_layer()\n        self.conv2 = Conv2d(\n            out_channels=64, kernel_size=(3, 3), stride=(1, 1), padding='SAME', W_init=W_init,\n            data_format='channels_first', b_init=None\n        )\n        self.bn1 = BatchNorm2d(act=None, gamma_init=G_init, data_format='channels_first')\n        self.upsample1 = UpSampling2d(data_format='channels_first', scale=(2, 2), method='bilinear')\n        self.conv3 = Conv2d(\n            out_channels=64, kernel_size=(3, 3), stride=(1, 1), padding='SAME', W_init=W_init,\n            data_format='channels_first', b_init=None\n        )\n        self.bn2 = BatchNorm2d(act=tlx.ReLU, gamma_init=G_init, data_format='channels_first')\n        self.upsample2 = UpSampling2d(data_format='channels_first', scale=(4, 4), method='bilinear')\n        self.conv4 = Conv2d(\n            out_channels=32, kernel_size=(3, 3), stride=(1, 1), padding='SAME', W_init=W_init,\n            data_format='channels_first', b_init=None\n        )\n        self.bn3 = BatchNorm2d(act=tlx.ReLU, gamma_init=G_init, data_format='channels_first')\n        self.conv5 = Conv2d(\n            out_channels=3, kernel_size=(1, 1), stride=(1, 1), act=tlx.Tanh, padding='SAME', W_init=W_init\n        )\n\n    def make_layer(self):\n        layer_list = []\n        for i in range(16):\n            layer_list.append(ResidualBlock())\n        return Sequential(layer_list)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        temp = x\n        x = self.residual_block(x)\n        x = self.conv2(x)\n        x = self.bn1(x)\n        x = x + temp\n        x = self.upsample1(x)\n        x = self.conv3(x)\n        x = self.bn2(x)\n        x = self.upsample2(x)\n        x = self.conv4(x)\n        x = self.bn3(x)\n        x = self.conv5(x)\n        return x\n\n\nclass SRGAN_d2(Module):\n    \"\"\" Discriminator in Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network\n    feature maps (n) and stride (s) feature maps (n) and stride (s)\n    \"\"\"\n\n    def __init__(self, ):\n        super(SRGAN_d2, self).__init__()\n        self.conv1 = Conv2d(\n            out_channels=64, kernel_size=(3, 3), stride=(1, 1), act=tlx.LeakyReLU(negative_slope=0.2), padding='SAME',\n            W_init=W_init, data_format='channels_first'\n        )\n        self.conv2 = Conv2d(\n            out_channels=64, kernel_size=(3, 3), stride=(2, 2), act=tlx.LeakyReLU(negative_slope=0.2), padding='SAME',\n            W_init=W_init, data_format='channels_first', b_init=None\n        )\n        self.bn1 = BatchNorm2d(gamma_init=G_init, data_format='channels_first')\n        self.conv3 = Conv2d(\n            out_channels=128, kernel_size=(3, 3), stride=(1, 1), act=tlx.LeakyReLU(negative_slope=0.2), padding='SAME',\n            W_init=W_init, data_format='channels_first', b_init=None\n        )\n        self.bn2 = BatchNorm2d(gamma_init=G_init, data_format='channels_first')\n        self.conv4 = Conv2d(\n            out_channels=128, kernel_size=(3, 3), stride=(2, 2), act=tlx.LeakyReLU(negative_slope=0.2), padding='SAME',\n            W_init=W_init, data_format='channels_first', b_init=None\n        )\n        self.bn3 = BatchNorm2d(gamma_init=G_init, data_format='channels_first')\n        self.conv5 = Conv2d(\n            out_channels=256, kernel_size=(3, 3), stride=(1, 1), act=tlx.LeakyReLU(negative_slope=0.2), padding='SAME',\n            W_init=W_init, data_format='channels_first', b_init=None\n        )\n        self.bn4 = BatchNorm2d(gamma_init=G_init, data_format='channels_first')\n        self.conv6 = Conv2d(\n            out_channels=256, kernel_size=(3, 3), stride=(2, 2), act=tlx.LeakyReLU(negative_slope=0.2), padding='SAME',\n            W_init=W_init, data_format='channels_first', b_init=None\n        )\n        self.bn5 = BatchNorm2d(gamma_init=G_init, data_format='channels_first')\n        self.conv7 = Conv2d(\n            out_channels=512, kernel_size=(3, 3), stride=(1, 1), act=tlx.LeakyReLU(negative_slope=0.2), padding='SAME',\n            W_init=W_init, data_format='channels_first', b_init=None\n        )\n        self.bn6 = BatchNorm2d(gamma_init=G_init, data_format='channels_first')\n        self.conv8 = Conv2d(\n            out_channels=512, kernel_size=(3, 3), stride=(2, 2), act=tlx.LeakyReLU(negative_slope=0.2), padding='SAME',\n            W_init=W_init, data_format='channels_first', b_init=None\n        )\n        self.bn7 = BatchNorm2d(gamma_init=G_init, data_format='channels_first')\n        self.flat = Flatten()\n        self.dense1 = Linear(out_features=1024, act=tlx.LeakyReLU(negative_slope=0.2))\n        self.dense2 = Linear(out_features=1)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.conv2(x)\n        x = self.bn1(x)\n        x = self.conv3(x)\n        x = self.bn2(x)\n        x = self.conv4(x)\n        x = self.bn3(x)\n        x = self.conv5(x)\n        x = self.bn4(x)\n        x = self.conv6(x)\n        x = self.bn5(x)\n        x = self.conv7(x)\n        x = self.bn6(x)\n        x = self.conv8(x)\n        x = self.bn7(x)\n        x = self.flat(x)\n        x = self.dense1(x)\n        x = self.dense2(x)\n        logits = x\n        n = tlx.sigmoid(x)\n        return n, logits\n\n\nclass SRGAN_d(Module):\n\n    def __init__(self, dim=64):\n        super(SRGAN_d, self).__init__()\n        self.conv1 = Conv2d(\n            out_channels=dim, kernel_size=(4, 4), stride=(2, 2), act=tlx.LeakyReLU, padding='SAME', W_init=W_init,\n            data_format='channels_first'\n        )\n        self.conv2 = Conv2d(\n            out_channels=dim * 2, kernel_size=(4, 4), stride=(2, 2), act=None, padding='SAME', W_init=W_init,\n            data_format='channels_first', b_init=None\n        )\n        self.bn1 = BatchNorm2d(num_features=dim * 2, act=tlx.LeakyReLU, gamma_init=G_init, data_format='channels_first')\n        self.conv3 = Conv2d(\n            out_channels=dim * 4, kernel_size=(4, 4), stride=(2, 2), act=None, padding='SAME', W_init=W_init,\n            data_format='channels_first', b_init=None\n        )\n        self.bn2 = BatchNorm2d(num_features=dim * 4, act=tlx.LeakyReLU, gamma_init=G_init, data_format='channels_first')\n        self.conv4 = Conv2d(\n            out_channels=dim * 8, kernel_size=(4, 4), stride=(2, 2), act=None, padding='SAME', W_init=W_init,\n            data_format='channels_first', b_init=None\n        )\n        self.bn3 = BatchNorm2d(num_features=dim * 8, act=tlx.LeakyReLU, gamma_init=G_init, data_format='channels_first')\n        self.conv5 = Conv2d(\n            out_channels=dim * 16, kernel_size=(4, 4), stride=(2, 2), act=None, padding='SAME', W_init=W_init,\n            data_format='channels_first', b_init=None\n        )\n        self.bn4 = BatchNorm2d(num_features=dim * 16, act=tlx.LeakyReLU, gamma_init=G_init, data_format='channels_first')\n        self.conv6 = Conv2d(\n            out_channels=dim * 32, kernel_size=(4, 4), stride=(2, 2), act=None, padding='SAME', W_init=W_init,\n            data_format='channels_first', b_init=None\n        )\n        self.bn5 = BatchNorm2d(num_features=dim * 32, act=tlx.LeakyReLU, gamma_init=G_init, data_format='channels_first')\n        self.conv7 = Conv2d(\n            out_channels=dim * 16, kernel_size=(1, 1), stride=(1, 1), act=None, padding='SAME', W_init=W_init,\n            data_format='channels_first', b_init=None\n        )\n        self.bn6 = BatchNorm2d(num_features=dim * 16, act=tlx.LeakyReLU, gamma_init=G_init, data_format='channels_first')\n        self.conv8 = Conv2d(\n            out_channels=dim * 8, kernel_size=(1, 1), stride=(1, 1), act=None, padding='SAME', W_init=W_init,\n            data_format='channels_first', b_init=None\n        )\n        self.bn7 = BatchNorm2d(num_features=dim * 8, act=None, gamma_init=G_init, data_format='channels_first')\n        self.conv9 = Conv2d(\n            out_channels=dim * 2, kernel_size=(1, 1), stride=(1, 1), act=None, padding='SAME', W_init=W_init,\n            data_format='channels_first', b_init=None\n        )\n        self.bn8 = BatchNorm2d(num_features=dim * 2, act=tlx.LeakyReLU, gamma_init=G_init, data_format='channels_first')\n        self.conv10 = Conv2d(\n            out_channels=dim * 2, kernel_size=(3, 3), stride=(1, 1), act=None, padding='SAME', W_init=W_init,\n            data_format='channels_first', b_init=None\n        )\n        self.bn9 = BatchNorm2d(num_features=dim * 2, act=tlx.LeakyReLU, gamma_init=G_init, data_format='channels_first')\n        self.conv11 = Conv2d(\n            out_channels=dim * 8, kernel_size=(3, 3), stride=(1, 1), act=None, padding='SAME', W_init=W_init,\n            data_format='channels_first', b_init=None\n        )\n        self.bn10 = BatchNorm2d(num_features=dim * 8, gamma_init=G_init, data_format='channels_first')\n        self.add = Elementwise(combine_fn=tlx.add, act=tlx.LeakyReLU)\n        self.flat = Flatten()\n        self.dense = Linear(out_features=1, W_init=W_init)\n\n    def forward(self, x):\n\n        x = self.conv1(x)\n        x = self.conv2(x)\n        x = self.bn1(x)\n        x = self.conv3(x)\n        x = self.bn2(x)\n        x = self.conv4(x)\n        x = self.bn3(x)\n        x = self.conv5(x)\n        x = self.bn4(x)\n        x = self.conv6(x)\n        x = self.bn5(x)\n        x = self.conv7(x)\n        x = self.bn6(x)\n        x = self.conv8(x)\n        x = self.bn7(x)\n        temp = x\n        x = self.conv9(x)\n        x = self.bn8(x)\n        x = self.conv10(x)\n        x = self.bn9(x)\n        x = self.conv11(x)\n        x = self.bn10(x)\n        x = self.add([temp, x])\n        x = self.flat(x)\n        x = self.dense(x)\n\n        return x\n\n\nclass Vgg19_simple_api(Module):\n\n    def __init__(self):\n        super(Vgg19_simple_api, self).__init__()\n        \"\"\" conv1 \"\"\"\n        self.conv1 = Conv2d(out_channels=64, kernel_size=(3, 3), stride=(1, 1), act=tlx.ReLU, padding='SAME')\n        self.conv2 = Conv2d(out_channels=64, kernel_size=(3, 3), stride=(1, 1), act=tlx.ReLU, padding='SAME')\n        self.maxpool1 = MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding='SAME')\n        \"\"\" conv2 \"\"\"\n        self.conv3 = Conv2d(out_channels=128, kernel_size=(3, 3), stride=(1, 1), act=tlx.ReLU, padding='SAME')\n        self.conv4 = Conv2d(out_channels=128, kernel_size=(3, 3), stride=(1, 1), act=tlx.ReLU, padding='SAME')\n        self.maxpool2 = MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding='SAME')\n        \"\"\" conv3 \"\"\"\n        self.conv5 = Conv2d(out_channels=256, kernel_size=(3, 3), stride=(1, 1), act=tlx.ReLU, padding='SAME')\n        self.conv6 = Conv2d(out_channels=256, kernel_size=(3, 3), stride=(1, 1), act=tlx.ReLU, padding='SAME')\n        self.conv7 = Conv2d(out_channels=256, kernel_size=(3, 3), stride=(1, 1), act=tlx.ReLU, padding='SAME')\n        self.conv8 = Conv2d(out_channels=256, kernel_size=(3, 3), stride=(1, 1), act=tlx.ReLU, padding='SAME')\n        self.maxpool3 = MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding='SAME')\n        \"\"\" conv4 \"\"\"\n        self.conv9 = Conv2d(out_channels=512, kernel_size=(3, 3), stride=(1, 1), act=tlx.ReLU, padding='SAME')\n        self.conv10 = Conv2d(out_channels=512, kernel_size=(3, 3), stride=(1, 1), act=tlx.ReLU, padding='SAME')\n        self.conv11 = Conv2d(out_channels=512, kernel_size=(3, 3), stride=(1, 1), act=tlx.ReLU, padding='SAME')\n        self.conv12 = Conv2d(out_channels=512, kernel_size=(3, 3), stride=(1, 1), act=tlx.ReLU, padding='SAME')\n        self.maxpool4 = MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding='SAME')  # (batch_size, 14, 14, 512)\n        \"\"\" conv5 \"\"\"\n        self.conv13 = Conv2d(out_channels=512, kernel_size=(3, 3), stride=(1, 1), act=tlx.ReLU, padding='SAME')\n        self.conv14 = Conv2d(out_channels=512, kernel_size=(3, 3), stride=(1, 1), act=tlx.ReLU, padding='SAME')\n        self.conv15 = Conv2d(out_channels=512, kernel_size=(3, 3), stride=(1, 1), act=tlx.ReLU, padding='SAME')\n        self.conv16 = Conv2d(out_channels=512, kernel_size=(3, 3), stride=(1, 1), act=tlx.ReLU, padding='SAME')\n        self.maxpool5 = MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding='SAME')  # (batch_size, 7, 7, 512)\n        \"\"\" fc 6~8 \"\"\"\n        self.flat = Flatten()\n        self.dense1 = Linear(out_features=4096, act=tlx.ReLU)\n        self.dense2 = Linear(out_features=4096, act=tlx.ReLU)\n        self.dense3 = Linear(out_features=1000, act=tlx.identity)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.conv2(x)\n        x = self.maxpool1(x)\n        x = self.conv3(x)\n        x = self.conv4(x)\n        x = self.maxpool2(x)\n        x = self.conv5(x)\n        x = self.conv6(x)\n        x = self.conv7(x)\n        x = self.conv8(x)\n        x = self.maxpool3(x)\n        x = self.conv9(x)\n        x = self.conv10(x)\n        x = self.conv11(x)\n        x = self.conv12(x)\n        x = self.maxpool4(x)\n        conv = x\n        x = self.conv13(x)\n        x = self.conv14(x)\n        x = self.conv15(x)\n        x = self.conv16(x)\n        x = self.maxpool5(x)\n        x = self.flat(x)\n        x = self.dense1(x)\n        x = self.dense2(x)\n        x = self.dense3(x)\n\n        return x, conv\n"
        },
        {
          "name": "train.py",
          "type": "blob",
          "size": 8.517578125,
          "content": "import os\nos.environ['TL_BACKEND'] = 'tensorflow' # Just modify this line, easily switch to any framework! PyTorch will coming soon!\n# os.environ['TL_BACKEND'] = 'mindspore'\n# os.environ['TL_BACKEND'] = 'paddle'\n# os.environ['TL_BACKEND'] = 'torch'\nimport time\nimport numpy as np\nimport tensorlayerx as tlx\nfrom tensorlayerx.dataflow import Dataset, DataLoader\nfrom srgan import SRGAN_g, SRGAN_d\nfrom config import config\nfrom tensorlayerx.vision.transforms import Compose, RandomCrop, Normalize, RandomFlipHorizontal, Resize, HWC2CHW\nimport vgg\nfrom tensorlayerx.model import TrainOneStep\nfrom tensorlayerx.nn import Module\nimport cv2\ntlx.set_device('GPU')\n\n###====================== HYPER-PARAMETERS ===========================###\nbatch_size = 8\nn_epoch_init = config.TRAIN.n_epoch_init\nn_epoch = config.TRAIN.n_epoch\n# create folders to save result images and trained models\nsave_dir = \"samples\"\ntlx.files.exists_or_mkdir(save_dir)\ncheckpoint_dir = \"models\"\ntlx.files.exists_or_mkdir(checkpoint_dir)\n\nhr_transform = Compose([\n    RandomCrop(size=(384, 384)),\n    RandomFlipHorizontal(),\n])\nnor = Compose([Normalize(mean=(127.5), std=(127.5), data_format='HWC'),\n              HWC2CHW()])\nlr_transform = Resize(size=(96, 96))\n\ntrain_hr_imgs = tlx.vision.load_images(path=config.TRAIN.hr_img_path, n_threads = 32)\n\nclass TrainData(Dataset):\n\n    def __init__(self, hr_trans=hr_transform, lr_trans=lr_transform):\n        self.train_hr_imgs = train_hr_imgs\n        self.hr_trans = hr_trans\n        self.lr_trans = lr_trans\n\n    def __getitem__(self, index):\n        img = self.train_hr_imgs[index]\n        hr_patch = self.hr_trans(img)\n        lr_patch = self.lr_trans(hr_patch)\n        return nor(lr_patch), nor(hr_patch)\n\n    def __len__(self):\n        return len(self.train_hr_imgs)\n\n\nclass WithLoss_init(Module):\n    def __init__(self, G_net, loss_fn):\n        super(WithLoss_init, self).__init__()\n        self.net = G_net\n        self.loss_fn = loss_fn\n\n    def forward(self, lr, hr):\n        out = self.net(lr)\n        loss = self.loss_fn(out, hr)\n        return loss\n\n\nclass WithLoss_D(Module):\n    def __init__(self, D_net, G_net, loss_fn):\n        super(WithLoss_D, self).__init__()\n        self.D_net = D_net\n        self.G_net = G_net\n        self.loss_fn = loss_fn\n\n    def forward(self, lr, hr):\n        fake_patchs = self.G_net(lr)\n        logits_fake = self.D_net(fake_patchs)\n        logits_real = self.D_net(hr)\n        d_loss1 = self.loss_fn(logits_real, tlx.ones_like(logits_real))\n        d_loss1 = tlx.ops.reduce_mean(d_loss1)\n        d_loss2 = self.loss_fn(logits_fake, tlx.zeros_like(logits_fake))\n        d_loss2 = tlx.ops.reduce_mean(d_loss2)\n        d_loss = d_loss1 + d_loss2\n        return d_loss\n\n\nclass WithLoss_G(Module):\n    def __init__(self, D_net, G_net, vgg, loss_fn1, loss_fn2):\n        super(WithLoss_G, self).__init__()\n        self.D_net = D_net\n        self.G_net = G_net\n        self.vgg = vgg\n        self.loss_fn1 = loss_fn1\n        self.loss_fn2 = loss_fn2\n\n    def forward(self, lr, hr):\n        fake_patchs = self.G_net(lr)\n        logits_fake = self.D_net(fake_patchs)\n        feature_fake = self.vgg((fake_patchs + 1) / 2.)\n        feature_real = self.vgg((hr + 1) / 2.)\n        g_gan_loss = 1e-3 * self.loss_fn1(logits_fake, tlx.ones_like(logits_fake))\n        g_gan_loss = tlx.ops.reduce_mean(g_gan_loss)\n        mse_loss = self.loss_fn2(fake_patchs, hr)\n        vgg_loss = 2e-6 * self.loss_fn2(feature_fake, feature_real)\n        g_loss = mse_loss + vgg_loss + g_gan_loss\n        return g_loss\n\n\nG = SRGAN_g()\nD = SRGAN_d()\nVGG = vgg.VGG19(pretrained=True, end_with='pool4', mode='dynamic')\n# automatic init layers weights shape with input tensor.\n# Calculating and filling 'in_channels' of each layer is a very troublesome thing.\n# So, just use 'init_build' with input shape. 'in_channels' of each layer will be automaticlly set.\nG.init_build(tlx.nn.Input(shape=(8, 3, 96, 96)))\nD.init_build(tlx.nn.Input(shape=(8, 3, 384, 384)))\n\n\ndef train():\n    G.set_train()\n    D.set_train()\n    VGG.set_eval()\n    train_ds = TrainData()\n    train_ds_img_nums = len(train_ds)\n    train_ds = DataLoader(train_ds, batch_size=batch_size, shuffle=True, drop_last=True)\n\n    lr_v = tlx.optimizers.lr.StepDecay(learning_rate=0.05, step_size=1000, gamma=0.1, last_epoch=-1, verbose=True)\n    g_optimizer_init = tlx.optimizers.Momentum(lr_v, 0.9)\n    g_optimizer = tlx.optimizers.Momentum(lr_v, 0.9)\n    d_optimizer = tlx.optimizers.Momentum(lr_v, 0.9)\n    g_weights = G.trainable_weights\n    d_weights = D.trainable_weights\n    net_with_loss_init = WithLoss_init(G, loss_fn=tlx.losses.mean_squared_error)\n    net_with_loss_D = WithLoss_D(D_net=D, G_net=G, loss_fn=tlx.losses.sigmoid_cross_entropy)\n    net_with_loss_G = WithLoss_G(D_net=D, G_net=G, vgg=VGG, loss_fn1=tlx.losses.sigmoid_cross_entropy,\n                                 loss_fn2=tlx.losses.mean_squared_error)\n\n    trainforinit = TrainOneStep(net_with_loss_init, optimizer=g_optimizer_init, train_weights=g_weights)\n    trainforG = TrainOneStep(net_with_loss_G, optimizer=g_optimizer, train_weights=g_weights)\n    trainforD = TrainOneStep(net_with_loss_D, optimizer=d_optimizer, train_weights=d_weights)\n\n    # initialize learning (G)\n    n_step_epoch = round(train_ds_img_nums // batch_size)\n    for epoch in range(n_epoch_init):\n        for step, (lr_patch, hr_patch) in enumerate(train_ds):\n            step_time = time.time()\n            loss = trainforinit(lr_patch, hr_patch)\n            print(\"Epoch: [{}/{}] step: [{}/{}] time: {:.3f}s, mse: {:.3f} \".format(\n                epoch, n_epoch_init, step, n_step_epoch, time.time() - step_time, float(loss)))\n\n    # adversarial learning (G, D)\n    n_step_epoch = round(train_ds_img_nums // batch_size)\n    for epoch in range(n_epoch):\n        for step, (lr_patch, hr_patch) in enumerate(train_ds):\n            step_time = time.time()\n            loss_g = trainforG(lr_patch, hr_patch)\n            loss_d = trainforD(lr_patch, hr_patch)\n            print(\n                \"Epoch: [{}/{}] step: [{}/{}] time: {:.3f}s, g_loss:{:.3f}, d_loss: {:.3f}\".format(\n                    epoch, n_epoch, step, n_step_epoch, time.time() - step_time, float(loss_g), float(loss_d)))\n        # dynamic learning rate update\n        lr_v.step()\n\n        if (epoch != 0) and (epoch % 10 == 0):\n            G.save_weights(os.path.join(checkpoint_dir, 'g.npz'), format='npz_dict')\n            D.save_weights(os.path.join(checkpoint_dir, 'd.npz'), format='npz_dict')\n\ndef evaluate():\n    ###====================== PRE-LOAD DATA ===========================###\n    valid_hr_imgs = tlx.vision.load_images(path=config.VALID.hr_img_path )\n    ###========================LOAD WEIGHTS ============================###\n    G.load_weights(os.path.join(checkpoint_dir, 'g.npz'), format='npz_dict')\n    G.set_eval()\n    imid = 0  # 0: ä¼é¹…  81: è´è¶ 53: é¸Ÿ  64: å¤å ¡\n    valid_hr_img = valid_hr_imgs[imid]\n    valid_lr_img = np.asarray(valid_hr_img)\n    hr_size1 = [valid_lr_img.shape[0], valid_lr_img.shape[1]]\n    valid_lr_img = cv2.resize(valid_lr_img, dsize=(hr_size1[1] // 4, hr_size1[0] // 4))\n    valid_lr_img_tensor = (valid_lr_img / 127.5) - 1  # rescale to ï¼»ï¼1, 1]\n\n\n    valid_lr_img_tensor = np.asarray(valid_lr_img_tensor, dtype=np.float32)\n    valid_lr_img_tensor = np.transpose(valid_lr_img_tensor,axes=[2, 0, 1])\n    valid_lr_img_tensor = valid_lr_img_tensor[np.newaxis, :, :, :]\n    valid_lr_img_tensor= tlx.ops.convert_to_tensor(valid_lr_img_tensor)\n    size = [valid_lr_img.shape[0], valid_lr_img.shape[1]]\n\n    out = tlx.ops.convert_to_numpy(G(valid_lr_img_tensor))\n    out = np.asarray((out + 1) * 127.5, dtype=np.uint8)\n    out = np.transpose(out[0], axes=[1, 2, 0])\n    print(\"LR size: %s /  generated HR size: %s\" % (size, out.shape))  # LR size: (339, 510, 3) /  gen HR size: (1, 1356, 2040, 3)\n    print(\"[*] save images\")\n    tlx.vision.save_image(out, file_name='valid_gen.png', path=save_dir)\n    tlx.vision.save_image(valid_lr_img, file_name='valid_lr.png', path=save_dir)\n    tlx.vision.save_image(valid_hr_img, file_name='valid_hr.png', path=save_dir)\n    out_bicu = cv2.resize(valid_lr_img, dsize = [size[1] * 4, size[0] * 4], interpolation = cv2.INTER_CUBIC)\n    tlx.vision.save_image(out_bicu, file_name='valid_hr_cubic.png', path=save_dir)\n\n\nif __name__ == '__main__':\n    import argparse\n\n    parser = argparse.ArgumentParser()\n\n    parser.add_argument('--mode', type=str, default='train', help='train, eval')\n\n    args = parser.parse_args()\n\n    tlx.global_flag['mode'] = args.mode\n\n    if tlx.global_flag['mode'] == 'train':\n        train()\n    elif tlx.global_flag['mode'] == 'eval':\n        evaluate()\n    else:\n        raise Exception(\"Unknow --mode\")\n"
        },
        {
          "name": "vgg.py",
          "type": "blob",
          "size": 9.7626953125,
          "content": "#! /usr/bin/python\n# -*- coding: utf-8 -*-\n\"\"\"\nVGG for ImageNet.\n\nIntroduction\n----------------\nVGG is a convolutional neural network model proposed by K. Simonyan and A. Zisserman\nfrom the University of Oxford in the paper \"Very Deep Convolutional Networks for\nLarge-Scale Image Recognition\"  . The model achieves 92.7% top-5 test accuracy in ImageNet,\nwhich is a dataset of over 14 million images belonging to 1000 classes.\n\nDownload Pre-trained Model\n----------------------------\n- Model weights in this example - vgg16_weights.npz : http://www.cs.toronto.edu/~frossard/post/vgg16/\n- Model weights in this example - vgg19.npy : https://media.githubusercontent.com/media/tensorlayer/pretrained-models/master/models/\n- Caffe VGG 16 model : https://gist.github.com/ksimonyan/211839e770f7b538e2d8#file-readme-md\n- Tool to convert the Caffe models to TensorFlow's : https://github.com/ethereon/caffe-tensorflow\n\nNote\n------\n- For simplified CNN layer see \"Convolutional layer (Simplified)\"\nin read the docs website.\n- When feeding other images to the model be sure to properly resize or crop them\nbeforehand. Distorted images might end up being misclassified. One way of safely\nfeeding images of multiple sizes is by doing center cropping.\n\n\"\"\"\n\nimport os\n\nimport numpy as np\n\nimport tensorlayerx as tlx\nfrom tensorlayerx import logging\nfrom tensorlayerx.files import assign_weights, maybe_download_and_extract\nfrom tensorlayerx.nn import (BatchNorm, Conv2d, Linear, Flatten, Input, Sequential, MaxPool2d)\nfrom tensorlayerx.nn import Module\n\n__all__ = [\n    'VGG',\n    'vgg16',\n    'vgg19',\n    'VGG16',\n    'VGG19',\n    #    'vgg11', 'vgg11_bn', 'vgg13', 'vgg13_bn', 'vgg16', 'vgg16_bn',\n    #    'vgg19_bn', 'vgg19',\n]\n\nlayer_names = [\n    ['conv1_1', 'conv1_2'], 'pool1', ['conv2_1', 'conv2_2'], 'pool2',\n    ['conv3_1', 'conv3_2', 'conv3_3', 'conv3_4'], 'pool3', ['conv4_1', 'conv4_2', 'conv4_3', 'conv4_4'], 'pool4',\n    ['conv5_1', 'conv5_2', 'conv5_3', 'conv5_4'], 'pool5', 'flatten', 'fc1_relu', 'fc2_relu', 'outputs'\n]\n\ncfg = {\n    'A': [[64], 'M', [128], 'M', [256, 256], 'M', [512, 512], 'M', [512, 512], 'M', 'F', 'fc1', 'fc2', 'O'],\n    'B': [[64, 64], 'M', [128, 128], 'M', [256, 256], 'M', [512, 512], 'M', [512, 512], 'M', 'F', 'fc1', 'fc2', 'O'],\n    'D':\n        [\n            [64, 64], 'M', [128, 128], 'M', [256, 256, 256], 'M', [512, 512, 512], 'M', [512, 512, 512], 'M', 'F',\n            'fc1', 'fc2', 'O'\n        ],\n    'E':\n        [\n            [64, 64], 'M', [128, 128], 'M', [256, 256, 256, 256], 'M', [512, 512, 512, 512], 'M', [512, 512, 512, 512],\n            'M', 'F', 'fc1', 'fc2', 'O'\n        ],\n}\n\nmapped_cfg = {\n    'vgg11': 'A',\n    'vgg11_bn': 'A',\n    'vgg13': 'B',\n    'vgg13_bn': 'B',\n    'vgg16': 'D',\n    'vgg16_bn': 'D',\n    'vgg19': 'E',\n    'vgg19_bn': 'E'\n}\n\nmodel_urls = {\n    'vgg16': 'https://git.openi.org.cn/attachments/760835b9-db71-4a00-8edd-d5ece4b6b522?type=0',\n    'vgg19': 'https://git.openi.org.cn/attachments/503c8a6c-705f-4fb6-ba18-03d72b6a949a?type=0'\n}\n\nmodel_saved_name = {'vgg16': 'vgg16_weights.npz', 'vgg19': 'vgg19.npy'}\n\n\nclass VGG(Module):\n\n    def __init__(self, layer_type, batch_norm=False, end_with='outputs', name=None):\n        super(VGG, self).__init__(name=name)\n        self.end_with = end_with\n\n        config = cfg[mapped_cfg[layer_type]]\n        self.make_layer = make_layers(config, batch_norm, end_with)\n\n    def forward(self, inputs):\n        \"\"\"\n        inputs : tensor\n            Shape [None, 224, 224, 3], value range [0, 1].\n        \"\"\"\n\n#         inputs = inputs * 255 - np.array([123.68, 116.779, 103.939], dtype=np.float32).reshape([1, 1, 1, 3])\n        inputs = inputs * 255. - tlx.convert_to_tensor(np.array([123.68, 116.779, 103.939], dtype=np.float32).reshape(-1,1,1))\n        out = self.make_layer(inputs)\n        return out\n\n\ndef make_layers(config, batch_norm=False, end_with='outputs'):\n    layer_list = []\n    is_end = False\n    for layer_group_idx, layer_group in enumerate(config):\n        if isinstance(layer_group, list):\n            for idx, layer in enumerate(layer_group):\n                layer_name = layer_names[layer_group_idx][idx]\n                n_filter = layer\n                if idx == 0:\n                    if layer_group_idx > 0:\n                        in_channels = config[layer_group_idx - 2][-1]\n                    else:\n                        in_channels = 3\n                else:\n                    in_channels = layer_group[idx - 1]\n                layer_list.append(\n                    Conv2d(\n                        out_channels=n_filter, kernel_size=(3, 3), stride=(1, 1), act=tlx.ReLU, padding='SAME',\n                        in_channels=in_channels, name=layer_name, data_format='channels_first'\n                    )\n                )\n                if batch_norm:\n                    layer_list.append(BatchNorm(num_features=n_filter, data_format='channels_first'))\n                if layer_name == end_with:\n                    is_end = True\n                    break\n        else:\n            layer_name = layer_names[layer_group_idx]\n            if layer_group == 'M':\n                layer_list.append(MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding='SAME', name=layer_name, data_format='channels_first'))\n            elif layer_group == 'O':\n                layer_list.append(Linear(out_features=1000, in_features=4096, name=layer_name))\n            elif layer_group == 'F':\n                layer_list.append(Flatten(name='flatten'))\n            elif layer_group == 'fc1':\n                layer_list.append(Linear(out_features=4096, act=tlx.ReLU, in_features=512 * 7 * 7, name=layer_name))\n            elif layer_group == 'fc2':\n                layer_list.append(Linear(out_features=4096, act=tlx.ReLU, in_features=4096, name=layer_name))\n            if layer_name == end_with:\n                is_end = True\n        if is_end:\n            break\n    return Sequential(layer_list)\n\ndef restore_model(model, layer_type):\n    logging.info(\"Restore pre-trained weights\")\n    # download weights\n    maybe_download_and_extract(model_saved_name[layer_type], 'model', model_urls[layer_type])\n    weights = []\n    if layer_type == 'vgg16':\n        npz = np.load(os.path.join('model', model_saved_name[layer_type]), allow_pickle=True)\n        # get weight list\n        for val in sorted(npz.items()):\n            logging.info(\"  Loading weights %s in %s\" % (str(val[1].shape), val[0]))\n            weights.append(val[1])\n            if len(model.all_weights) == len(weights):\n                break\n    elif layer_type == 'vgg19':\n        npz = np.load(os.path.join('model', model_saved_name[layer_type]), allow_pickle=True, encoding='latin1').item()\n        # get weight list\n        for val in sorted(npz.items()):\n            logging.info(\"  Loading %s in %s\" % (str(val[1][0].shape), val[0]))\n            logging.info(\"  Loading %s in %s\" % (str(val[1][1].shape), val[0]))\n            weights.extend(val[1])\n            if len(model.all_weights) == len(weights):\n                break\n    # assign weight values\n    if tlx.BACKEND != 'tensorflow':\n        for i in range(len(weights)):\n            if len(weights[i].shape) == 4:\n                weights[i] = np.transpose(weights[i], axes=[3, 2, 0, 1])\n    assign_weights(weights, model)\n    del weights\n\ndef vgg16(pretrained=False, end_with='outputs', mode='dynamic', name=None):\n    \"\"\"Pre-trained VGG16 model.\n\n    Parameters\n    ------------\n    pretrained : boolean\n        Whether to load pretrained weights. Default False.\n    end_with : str\n        The end point of the model. Default ``fc3_relu`` i.e. the whole model.\n    mode : str.\n        Model building mode, 'dynamic' or 'static'. Default 'dynamic'.\n    name : None or str\n        A unique layer name.\n\n    Examples\n    ---------\n    Classify ImageNet classes with VGG16, see `tutorial_models_vgg.py <https://github.com/tensorlayer/tensorlayer/blob/master/example/tutorial_models_vgg.py>`__\n    With TensorLayer\n    TODO Modify the usage example according to the model storage location\n\n    >>> # get the whole model, without pre-trained VGG parameters\n    >>> vgg = vgg16()\n    >>> # get the whole model, restore pre-trained VGG parameters\n    >>> vgg = vgg16(pretrained=True)\n    >>> # use for inferencing\n    >>> output = vgg(img)\n    >>> probs = tlx.ops.softmax(output)[0].numpy()\n\n    \"\"\"\n\n    if mode == 'dynamic':\n        model = VGG(layer_type='vgg16', batch_norm=False, end_with=end_with, name=name)\n    elif mode == 'static':\n        raise NotImplementedError\n    else:\n        raise Exception(\"No such mode %s\" % mode)\n    if pretrained:\n        restore_model(model, layer_type='vgg16')\n    return model\n\n\ndef vgg19(pretrained=False, end_with='outputs', mode='dynamic', name=None):\n    \"\"\"Pre-trained VGG19 model.\n\n    Parameters\n    ------------\n    pretrained : boolean\n        Whether to load pretrained weights. Default False.\n    end_with : str\n        The end point of the model. Default ``fc3_relu`` i.e. the whole model.\n    mode : str.\n        Model building mode, 'dynamic' or 'static'. Default 'dynamic'.\n    name : None or str\n        A unique layer name.\n\n    Examples\n    ---------\n    Classify ImageNet classes with VGG19, see `tutorial_models_vgg.py <https://github.com/tensorlayer/TensorLayerX/blob/main/examples/model_zoo/vgg.py>`__\n    With TensorLayerx\n\n    >>> # get the whole model, without pre-trained VGG parameters\n    >>> vgg = vgg19()\n    >>> # get the whole model, restore pre-trained VGG parameters\n    >>> vgg = vgg19(pretrained=True)\n    >>> # use for inferencing\n    >>> output = vgg(img)\n    >>> probs = tlx.ops.softmax(output)[0].numpy()\n\n    \"\"\"\n    if mode == 'dynamic':\n        model = VGG(layer_type='vgg19', batch_norm=False, end_with=end_with, name=name)\n    elif mode == 'static':\n        raise NotImplementedError\n    else:\n        raise Exception(\"No such mode %s\" % mode)\n    if pretrained:\n        restore_model(model, layer_type='vgg19')\n    return model\n\n\nVGG16 = vgg16\nVGG19 = vgg19\n\n"
        }
      ]
    }
  ]
}