{
  "metadata": {
    "timestamp": 1736559554152,
    "page": 161,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjE3MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "VeNoMouS/cloudscraper",
      "stars": 4592,
      "defaultBranch": "master",
      "files": [
        {
          "name": ".codacy.yml",
          "type": "blob",
          "size": 0.0439453125,
          "content": "exclude_paths:\n    - tests/*\n    - README.md\n"
        },
        {
          "name": ".coveragerc",
          "type": "blob",
          "size": 0.076171875,
          "content": "[run]\nsource = cloudscraper\nomit = tests/*,cloudscraper/interpreters/jsfuck.py"
        },
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.3818359375,
          "content": "# Byte-compiled / optimized / DLL files\n__pycache__/\n*.py[cod]\n\n# C extensions\n*.so\n\n# Distribution / packaging\n.Python\nenv/\nbuild/\ndevelop-eggs/\ndist/\ndownloads/\neggs/\n.eggs/\nlib/\nlib64/\nparts/\nsdist/\nvar/\n*.egg-info/\n.installed.cfg\n*.egg\n\n# Tests / Coverage\nreport.xml\ncoverage.xml\n.coverage\n.tox/\n.testmondata\n\n# IDE\n.idea/\n\n# Prevent unintended commits\n.env\nPipfile*\n\n# Temp files\n*.swp\n"
        },
        {
          "name": ".travis.yml",
          "type": "blob",
          "size": 0.4560546875,
          "content": "language: python\ncache: pip\ndist: bionic\nsudo: false\n\nmatrix:\n    include:\n        - python: '3.6'\n        - python: '3.7'\n        - python: '3.8'\n        - python: '3.9'\n\ninstall:\n    - pip install -r requirements.txt --use-deprecated=legacy-resolver\n    - pip install -r dev_requirements.txt --use-deprecated=legacy-resolver\n    - pip install python-coveralls --use-deprecated=legacy-resolver\n\nscript:\n    - make lint && make ci\n\nafter_success:\n    - make coverage\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 1.0654296875,
          "content": "MIT License\n\nCopyright (c) 2019 VeNoMouS\nCopyright (c) 2015 Anorov\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n"
        },
        {
          "name": "MANIFEST.in",
          "type": "blob",
          "size": 0.0625,
          "content": "include LICENSE README.md cloudscraper/user_agent/browsers.json\n"
        },
        {
          "name": "Makefile",
          "type": "blob",
          "size": 1.064453125,
          "content": "pep8-rules := E501,N806,W503,W504\n\ninit:\n\tpip install -r requirements.txt\n\ntest:\n\t# This runs all of the tests, on both Python 2 and Python 3.\n\ttox --parallel auto\n\nwatch:\n\t# This automatically selects and re-executes only tests affected by recent changes.\n\tptw -- --testmon\n\nretry:\n\t# This will retry failed tests on every file change.\n\tpy.test -n auto --forked --looponfail\n\nci:\n\tpy.test -n 8 --forked --junitxml=report.xml\n\nlint:\n\tflake8 --ignore $(pep8-rules) cloudscraper tests\n\nformat:\n\t# Automatic reformatting\n\tautopep8 -aaa --ignore $(pep8-rules) --in-place --recursive cloudscraper tests\n\ncoverage:\n\tpy.test --cov-config=.coveragerc --verbose --cov-report=term --cov-report=xml --cov=cloudscraper tests\n\tcoveralls\n\nclean:\n\trm -fr build dist .egg cloudscraper.egg-info report.xml coverage.xml\n\nbuild:\n\tmake clean\n\tpython3 setup.py sdist bdist_wheel --universal\n\npublish:\n\tmake build\n\tpip3 install 'twine>=1.5.0'\n\ttwine upload dist/*\n\tmake clean\n\npublish_test:\n\tmake build\n\tpip3 install 'twine>=1.5.0'\n\ttwine upload --repository-url https://test.pypi.org/legacy/ dist/*\n\tmake clean\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 18.298828125,
          "content": "# cloudscraper\n\n[![PyPI version](https://badge.fury.io/py/cloudscraper.svg)](https://badge.fury.io/py/cloudscraper)\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n[![image](https://img.shields.io/pypi/pyversions/cloudscraper.svg)](https://pypi.org/project/cloudscraper/)\n[![Build Status](https://travis-ci.com/VeNoMouS/cloudscraper.svg?branch=master)](https://travis-ci.com/VeNoMouS/cloudscraper)\n[![Donate](https://img.shields.io/badge/Donate-Buy%20Me%20A%20Coffee-brightgreen.svg)](https://www.buymeacoffee.com/venomous)\n\nA simple Python module to bypass Cloudflare's anti-bot page (also known as \"I'm Under Attack Mode\", or IUAM), implemented with [Requests](https://github.com/kennethreitz/requests). Cloudflare changes their techniques periodically, so I will update this repo frequently.\n\nThis can be useful if you wish to scrape or crawl a website protected with Cloudflare. Cloudflare's anti-bot page currently just checks if the client supports Javascript, though they may add additional techniques in the future.\n\nDue to Cloudflare continually changing and hardening their protection page, cloudscraper requires a JavaScript Engine/interpreter to solve Javascript challenges. This allows the script to easily impersonate a regular web browser without explicitly deobfuscating and parsing Cloudflare's Javascript.\n\nFor reference, this is the default message Cloudflare uses for these sorts of pages:\n\n```\nChecking your browser before accessing website.com.\n\nThis process is automatic. Your browser will redirect to your requested content shortly.\n\nPlease allow up to 5 seconds...\n```\n\nAny script using cloudscraper will sleep for ~5 seconds for the first visit to any site with Cloudflare anti-bots enabled, though no delay will occur after the first request.\n\n# Donations\n\nIf you feel like showing your love and/or appreciation for this project, then how about shouting me a coffee or beer :)\n\n<a href=\"https://buymeacoff.ee/venomous\" target=\"_blank\"><img src=\"https://www.buymeacoffee.com/assets/img/custom_images/orange_img.png\" alt=\"Buy Me A Coffee\" style=\"height: 41px !important;width: 174px !important;box-shadow: 0px 3px 2px 0px rgba(190, 190, 190, 0.5) !important;-webkit-box-shadow: 0px 3px 2px 0px rgba(190, 190, 190, 0.5) !important;\" ></a>\n\n# Installation\n\nSimply run `pip install cloudscraper`. The PyPI package is at https://pypi.python.org/pypi/cloudscraper/\n\nAlternatively, clone this repository and run `python setup.py install`.\n\n# Dependencies\n\n- Python 3.x\n- **[Requests](https://github.com/kennethreitz/requests)** >= 2.9.2\n- **[requests_toolbelt](https://pypi.org/project/requests-toolbelt/)** >= 0.9.1\n\n`python setup.py install` will install the Python dependencies automatically. The javascript interpreters and/or engines you decide to use are the only things you need to install yourself, excluding js2py which is part of the requirements as the default.\n\n# Javascript Interpreters and Engines\n\nWe support the following Javascript interpreters/engines.\n\n- **[ChakraCore](https://github.com/microsoft/ChakraCore):** Library binaries can also be located [here](https://www.github.com/VeNoMouS/cloudscraper/tree/ChakraCore/).\n- **[js2py](https://github.com/PiotrDabkowski/Js2Py):** >=0.67\n- **native**: Self made native python solver **(Default)**\n- **[Node.js](https://nodejs.org/)**\n- **[V8](https://github.com/sony/v8eval/):** We use Sony's [v8eval](https://v8.dev)() python module.\n\n# Usage\n\nThe simplest way to use cloudscraper is by calling `create_scraper()`.\n\n```python\nimport cloudscraper\n\nscraper = cloudscraper.create_scraper()  # returns a CloudScraper instance\n# Or: scraper = cloudscraper.CloudScraper()  # CloudScraper inherits from requests.Session\nprint(scraper.get(\"http://somesite.com\").text)  # => \"<!DOCTYPE html><html><head>...\"\n```\n\nThat's it...\n\nAny requests made from this session object to websites protected by Cloudflare anti-bot will be handled automatically. Websites not using Cloudflare will be treated normally. You don't need to configure or call anything further, and you can effectively treat all websites as if they're not protected with anything.\n\nYou use cloudscraper exactly the same way you use Requests. `cloudScraper` works identically to a Requests `Session` object, just instead of calling `requests.get()` or `requests.post()`, you call `scraper.get()` or `scraper.post()`.\n\nConsult [Requests' documentation](http://docs.python-requests.org/en/latest/user/quickstart/) for more information.\n\n## Options\n\n### Disable Cloudflare V1\n#### Description\n\nIf you don't want to even attempt Cloudflare v1 (Deprecated) solving..\n\n#### Parameters\n\n\n|Parameter|Value|Default|\n|-------------|:-------------:|:-----:|\n|disableCloudflareV1|(boolean)|False|\n\n#### Example\n\n```python\nscraper = cloudscraper.create_scraper(disableCloudflareV1=True)\n```\n\n------\n\n### Brotli\n\n#### Description\n\n[Brotli](https://en.wikipedia.org/wiki/Brotli) decompression support has been added, and it is enabled by default.\n\n#### Parameters\n\n\n|Parameter|Value|Default|\n|-------------|:-------------:|:-----:|\n|allow_brotli|(boolean)|True|\n\n#### Example\n\n```python\nscraper = cloudscraper.create_scraper(allow_brotli=False)\n```\n\n------\n\n### Browser / User-Agent Filtering\n\n#### Description\n\nControl how and which User-Agent is \"randomly\" selected.\n\n#### Parameters\n\nCan be passed as an argument to `create_scraper()`, `get_tokens()`, `get_cookie_string()`.\n\n|Parameter|Value|Default|\n|-------------|:-------------:|:-----:|\n|browser|(string) `chrome` or `firefox`|None|\n\nOr\n\n|Parameter|Value|Default|\n|-------------|:-------------:|:-----:|\n|browser|(dict)||\n\n##### `browser` *_dict_* Parameters\n|Parameter|Value|Default|\n|-------------|:-------------:|:-----:|\n|browser|(string) `chrome` or `firefox`|None|\n|mobile|(boolean)|True|\n|desktop|(boolean)|True|\n|platform|(string) `'linux', 'windows', 'darwin', 'android', 'ios'`|None|\n|custom|(string)|None|\n#### Example\n\n```python\nscraper = cloudscraper.create_scraper(browser='chrome')\n```\n\nor\n\n```python\n# will give you only mobile chrome User-Agents on Android\nscraper = cloudscraper.create_scraper(\n    browser={\n        'browser': 'chrome',\n        'platform': 'android',\n        'desktop': False\n    }\n)\n\n# will give you only desktop firefox User-Agents on Windows\nscraper = cloudscraper.create_scraper(\n    browser={\n        'browser': 'firefox',\n        'platform': 'windows',\n        'mobile': False\n    }\n)\n\n# Custom will also try find the user-agent string in the browsers.json,\n# If a match is found, it will use the headers and cipherSuite from that \"browser\",\n# Otherwise a generic set of headers and cipherSuite will be used.\nscraper = cloudscraper.create_scraper(\n    browser={\n        'custom': 'ScraperBot/1.0',\n    }\n)\n```\n------\n\n### Debug\n\n#### Description\n\nPrints out header and content information of the request for debugging.\n\n#### Parameters\n\nCan be set as an attribute via your `cloudscraper` object or passed as an argument to `create_scraper()`, `get_tokens()`, `get_cookie_string()`.\n\n|Parameter|Value|Default|\n|-------------|:-------------:|:-----:|\n|debug|(boolean)|False|\n\n#### Example\n\n```python\nscraper = cloudscraper.create_scraper(debug=True)\n```\n\n------\n\n### Delays\n\n#### Description\n\nCloudflare IUAM challenge requires the browser to wait ~5 seconds before submitting the challenge answer, If you would like to override this delay.\n\n#### Parameters\n\nCan be set as an attribute via your `cloudscraper` object or passed as an argument to `create_scraper()`, `get_tokens()`, `get_cookie_string()`.\n\n|Parameter|Value|Default|\n|-------------|:-------------:|:-----:|\n|delay|(float)|extracted from IUAM page|\n\n#### Example\n\n```python\nscraper = cloudscraper.create_scraper(delay=10)\n```\n\n------\n\n### Existing session\n\n#### Description:\n\nIf you already have an existing Requests session, you can pass it to the function `create_scraper()` to continue using that session.\n\n#### Parameters\n\n|Parameter|Value|Default|\n|-------------|:-------------:|:-----:|\n|sess|(requests.session)|None|\n\n#### Example\n\n```python\nsession = requests.session()\nscraper = cloudscraper.create_scraper(sess=session)\n```\n\n#### Note\n\nUnfortunately, not all of Requests session attributes are easily transferable, so if you run into problems with this,\n\nYou should replace your initial session initialization call\n\nFrom:\n```python\nsess = requests.session()\n```\n\nTo:\n\n```python\nsess = cloudscraper.create_scraper()\n```\n\n------\n\n### JavaScript Engines and Interpreters\n\n#### Description\ncloudscraper currently supports the following JavaScript Engines/Interpreters\n\n- **[ChakraCore](https://github.com/microsoft/ChakraCore)**\n- **[js2py](https://github.com/PiotrDabkowski/Js2Py)**\n- **native**: Self made native python solver **(Default)**\n- **[Node.js](https://nodejs.org/)**\n- **[V8](https://github.com/sony/v8eval/)**\n\n\n#### Parameters\nCan be set as an attribute via your `cloudscraper` object or passed as an argument to `create_scraper()`, `get_tokens()`, `get_cookie_string()`.\n\n|Parameter|Value|Default|\n|-------------|:-------------:|:-----:|\n|interpreter|(string)|`native`|\n\n#### Example\n\n```python\nscraper = cloudscraper.create_scraper(interpreter='nodejs')\n```\n\n------\n\n### 3rd Party Captcha Solvers\n\n#### Description\n`cloudscraper` currently supports the following 3rd party Captcha solvers, should you require them.\n\n- **[2captcha](https://www.2captcha.com/)**\n- **[anticaptcha](https://www.anti-captcha.com/)**\n- **[CapSolver](https://capsolver.com/)**\n- **[CapMonster Cloud](https://capmonster.cloud/)**\n- **[deathbycaptcha](https://www.deathbycaptcha.com/)**\n- **[9kw](https://www.9kw.eu/)**\n- **__return_response__**\n\n#### Note\n\nI am working on adding more 3rd party solvers, if you wish to have a service added that is not currently supported, please raise a support ticket on github.\n\n##### Required Parameters\n\nCan be set as an attribute via your `cloudscraper` object or passed as an argument to `create_scraper()`, `get_tokens()`, `get_cookie_string()`.\n\n|Parameter|Value|Default|\n|-------------|:-------------:|:-----:|\n|captcha|(dict)|None|\n\n------\n\n#### 2captcha\n\n##### Required `captcha` Parameters\n\n|Parameter|Value|Required|Default|\n|-------------|:-------------:|:-----:|:-----:|\n|provider|(string) `2captcha`| yes||\n|api_key|(string)| yes||\n|no_proxy|(boolean)|no|False|\n\n##### Note\n\nif proxies are set you can disable sending the proxies to 2captcha by setting `no_proxy` to `True`\n\n##### Example\n\n```python\nscraper = cloudscraper.create_scraper(\n  captcha={\n    'provider': '2captcha',\n    'api_key': 'your_2captcha_api_key'\n  }\n)\n```\n\n------\n\n#### anticaptcha\n\n##### Required `captcha` Parameters\n\n|Parameter|Value|Required|Default|\n|-------------|:-------------:|:-----:|:-----:|\n|provider|(string) `anticaptcha`|yes||\n|api_key|(string)|yes||\n|no_proxy|(boolean)|no|False|\n\n##### Note\n\nif proxies are set you can disable sending the proxies to anticaptcha by setting `no_proxy` to `True`\n\n##### Example\n\n```python\nscraper = cloudscraper.create_scraper(\n  captcha={\n    'provider': 'anticaptcha',\n    'api_key': 'your_anticaptcha_api_key'\n  }\n)\n```\n\n------\n\n#### CapSolver\n\n##### Required `captcha` Parameters\n\n|Parameter|Value|Required|Default|\n|-------------|:-------------:|:-----:|:-----:|\n|provider|(string) `captchaai`|yes||\n|api_key|(string)|yes||\n\n\n##### Example\n\n```python\nscraper = cloudscraper.create_scraper(\n  captcha={\n    'provider': 'capsolver',\n    'api_key': 'your_captchaai_api_key'\n  }\n)\n```\n\n------\n\n#### CapMonster Cloud\n\n##### Required `captcha` Parameters\n\n|Parameter|Value|Required|Default|\n|-------------|:-------------:|:-----:|:-----:|\n|provider|(string) `capmonster`| yes||\n|clientKey|(string)| yes||\n|no_proxy|(boolean)|no|False|\n\n##### Note\n\nif proxies are set you can disable sending the proxies to CapMonster by setting `no_proxy` to `True`\n\n##### Example\n\n```python\nscraper = cloudscraper.create_scraper(\n  captcha={\n    'provider': 'capmonster',\n    'clientKey': 'your_capmonster_clientKey'\n  }\n)\n```\n\n------\n\n#### deathbycaptcha\n\n##### Required `captcha` Parameters\n\n|Parameter|Value|Required|Default|\n|-------------|:-------------:|:-----:|:-----:|\n|provider|(string) `deathbycaptcha`|yes||\n|username|(string)|yes||\n|password|(string)|yes||\n\n##### Example\n\n```python\nscraper = cloudscraper.create_scraper(\n  captcha={\n    'provider': 'deathbycaptcha',\n    'username': 'your_deathbycaptcha_username',\n    'password': 'your_deathbycaptcha_password',\n  }\n)\n```\n\n------\n\n#### 9kw\n\n##### Required `captcha` Parameters\n\n|Parameter|Value|Required|Default|\n|-------------|:-------------:|:-----:|:-----:|\n|provider|(string) `9kw`|yes||\n|api_key|(string)|yes||\n|maxtimeout|(int)|no|180|\n\n##### Example\n\n```python\nscraper = cloudscraper.create_scraper(\n  captcha={\n    'provider': '9kw',\n    'api_key': 'your_9kw_api_key',\n    'maxtimeout': 300\n  }\n)\n```\n\n------\n\n#### return_response\n\nUse this if you want the requests response payload without solving the Captcha.\n\n##### Required `captcha` Parameters\n\n|Parameter|Value|Required|Default|\n|-------------|:-------------:|:-----:|:-----:|\n|provider|(string) `return_response`| yes||\n\n##### Example\n```python\nscraper = cloudscraper.create_scraper(\n  captcha={'provider': 'return_response'}\n)\n```\n\n## Integration\n\nIt's easy to integrate `cloudscraper` with other applications and tools. Cloudflare uses two cookies as tokens: one to verify you made it past their challenge page and one to track your session. To bypass the challenge page, simply include both of these cookies (with the appropriate user-agent) in all HTTP requests you make.\n\nTo retrieve just the cookies (as a dictionary), use `cloudscraper.get_tokens()`. To retrieve them as a full `Cookie` HTTP header, use `cloudscraper.get_cookie_string()`.\n\n`get_tokens` and `get_cookie_string` both accept Requests' usual keyword arguments (like `get_tokens(url, proxies={\"http\": \"socks5://localhost:9050\"})`).\n\nPlease read [Requests' documentation on request arguments](http://docs.python-requests.org/en/master/api/#requests.Session.request) for more information.\n\n------\n\n### User-Agent Handling\n\nThe two integration functions return a tuple of `(cookie, user_agent_string)`.\n\n**You must use the same user-agent string for obtaining tokens and for making requests with those tokens, otherwise Cloudflare will flag you as a bot.**\n\nThat means you have to pass the returned `user_agent_string` to whatever script, tool, or service you are passing the tokens to (e.g. curl, or a specialized scraping tool), and it must use that passed user-agent when it makes HTTP requests.\n\n------\n\n### Integration examples\n\nRemember, you must always use the same user-agent when retrieving or using these cookies. These functions all return a tuple of `(cookie_dict, user_agent_string)`.\n\n------\n\n#### Retrieving a cookie dict through a proxy\n\n`get_tokens` is a convenience function for returning a Python dict containing Cloudflare's session cookies. For demonstration, we will configure this request to use a proxy. (Please note that if you request Cloudflare clearance tokens through a proxy, you must always use the same proxy when those tokens are passed to the server. Cloudflare requires that the challenge-solving IP and the visitor IP stay the same.)\n\nIf you do not wish to use a proxy, just don't pass the `proxies` keyword argument. These convenience functions support all of Requests' normal keyword arguments, like `params`, `data`, and `headers`.\n\n```python\nimport cloudscraper\n\nproxies = {\"http\": \"http://localhost:8080\", \"https\": \"http://localhost:8080\"}\ntokens, user_agent = cloudscraper.get_tokens(\"http://somesite.com\", proxies=proxies)\nprint(tokens)\n# => {\n    'cf_clearance': 'c8f913c707b818b47aa328d81cab57c349b1eee5-1426733163-3600',\n    '__cfduid': 'dd8ec03dfdbcb8c2ea63e920f1335c1001426733158'\n}\n```\n\n------\n\n#### Retrieving a cookie string\n\n`get_cookie_string` is a convenience function for returning the tokens as a string for use as a `Cookie` HTTP header value.\n\nThis is useful when crafting an HTTP request manually, or working with an external application or library that passes on raw cookie headers.\n\n```python\nimport cloudscraper\n\ncookie_value, user_agent = cloudscraper.get_cookie_string('http://somesite.com')\n\nprint('GET / HTTP/1.1\\nCookie: {}\\nUser-Agent: {}\\n'.format(cookie_value, user_agent))\n\n# GET / HTTP/1.1\n# Cookie: cf_clearance=c8f913c707b818b47aa328d81cab57c349b1eee5-1426733163-3600; __cfduid=dd8ec03dfdbcb8c2ea63e920f1335c1001426733158\n# User-Agent: Some/User-Agent String\n```\n\n------\n\n#### curl example\n\nHere is an example of integrating cloudscraper with curl. As you can see, all you have to do is pass the cookies and user-agent to curl.\n\n```python\nimport subprocess\nimport cloudscraper\n\n# With get_tokens() cookie dict:\n\n# tokens, user_agent = cloudscraper.get_tokens(\"http://somesite.com\")\n# cookie_arg = 'cf_clearance={}; __cfduid={}'.format(tokens['cf_clearance'], tokens['__cfduid'])\n\n# With get_cookie_string() cookie header; recommended for curl and similar external applications:\n\ncookie_arg, user_agent = cloudscraper.get_cookie_string('http://somesite.com')\n\n# With a custom user-agent string you can optionally provide:\n\n# ua = \"Scraping Bot\"\n# cookie_arg, user_agent = cloudscraper.get_cookie_string(\"http://somesite.com\", user_agent=ua)\n\nresult = subprocess.check_output(\n    [\n        'curl',\n        '--cookie',\n        cookie_arg,\n        '-A',\n        user_agent,\n        'http://somesite.com'\n    ]\n)\n```\n\nTrimmed down version. Prints page contents of any site protected with Cloudflare, via curl.\n\n**Warning: `shell=True` can be dangerous to use with `subprocess` in real code.**\n\n```python\nurl = \"http://somesite.com\"\ncookie_arg, user_agent = cloudscraper.get_cookie_string(url)\ncmd = \"curl --cookie {cookie_arg} -A {user_agent} {url}\"\nprint(\n    subprocess.check_output(\n        cmd.format(\n            cookie_arg=cookie_arg,\n            user_agent=user_agent,\n            url=url\n        ),\n        shell=True\n    )\n)\n```\n\n### Cryptography\n\n#### Description\n\nControl communication between client and server\n\n#### Parameters\n\nCan be passed as an argument to `create_scraper()`.\n\n|Parameter|Value|Default|\n|-------------|:-------------:|:-----:|\n|cipherSuite|(string)|None|\n|ecdhCurve|(string)|prime256v1|\n|server_hostname|(string)|None|\n\n#### Example\n\n```python\n# Some servers require the use of a more complex ecdh curve than the default \"prime256v1\"\n# It may can solve handshake failure\nscraper = cloudscraper.create_scraper(ecdhCurve='secp384r1')\n```\n\n```python\n# Manipulate server_hostname\nscraper = cloudscraper.create_scraper(server_hostname='www.somesite.com')\nscraper.get(\n    'https://backend.hosting.com/',\n    headers={'Host': 'www.somesite.com'}\n)\n```"
        },
        {
          "name": "cloudscraper",
          "type": "tree",
          "content": null
        },
        {
          "name": "dev_requirements.txt",
          "type": "blob",
          "size": 0.3359375,
          "content": "# Development\r\npytest >= 6.1.0\r\npytest-cov >= 2.10.1\r\npytest-xdist >= 1.30.0\r\npytest-forked >= 1.3.0\r\npytest-watch >= 4.2.0\r\npytest-timeout >= 1.3.3\r\npytest-env >= 0.6.2\r\nresponses >= 0.10.8\r\nflake8 >= 3.7.9\r\ntox >= 3.14.2\r\ncoverage == 4.5.4\r\ncoveralls >= 1.9.2\r\nautopep8 >= 1.4.4\r\njs2py >= 0.60\r\npolling >= 0.3.1\r\npython_anticaptcha >= 0.4.2\r\n"
        },
        {
          "name": "pytest.ini",
          "type": "blob",
          "size": 0.091796875,
          "content": "[pytest]\naddopts = -p no:warnings\ntimeout = 2000\nenv = PYTHONHASHSEED=0\njunit_family = xunit1\n"
        },
        {
          "name": "requirements.txt",
          "type": "blob",
          "size": 0.07421875,
          "content": "# Production\nrequests >= 2.9.2\nrequests_toolbelt >= 0.9.1\npyparsing >= 2.4.7"
        },
        {
          "name": "setup.py",
          "type": "blob",
          "size": 1.7763671875,
          "content": "import os\nimport re\nfrom setuptools import setup, find_packages\nfrom io import open\n\nwith open(os.path.join(os.path.dirname(__file__), 'cloudscraper', '__init__.py')) as fp:\n    VERSION = re.match(r'.*__version__ = \\'(.*?)\\'', fp.read(), re.S).group(1)\n\nwith open('README.md', 'r', encoding='utf-8') as fp:\n    readme = fp.read()\n\nsetup(\n    name = 'cloudscraper',\n    author = 'VeNoMouS',\n    author_email = 'venom@gen-x.co.nz',\n    version=VERSION,\n    packages = find_packages(exclude=['tests*']),\n    description = 'A Python module to bypass Cloudflare\\'s anti-bot page.',\n    long_description=readme,\n    long_description_content_type='text/markdown',\n    url = 'https://github.com/venomous/cloudscraper',\n    keywords = [\n        'cloudflare',\n        'scraping',\n        'ddos',\n        'scrape',\n        'webscraper',\n        'anti-bot',\n        'waf',\n        'iuam',\n        'bypass',\n        'challenge'\n    ],\n    include_package_data = True,\n    install_requires = [\n        'requests >= 2.9.2',\n        'requests_toolbelt >= 0.9.1',\n        'pyparsing >= 2.4.7'\n    ],\n    classifiers=[\n        'Development Status :: 5 - Production/Stable',\n        'Intended Audience :: Developers',\n        'Natural Language :: English',\n        'License :: OSI Approved :: MIT License',\n        'Operating System :: OS Independent',\n        'Programming Language :: Python',\n        'Programming Language :: Python :: 3',\n        'Programming Language :: Python :: 3.6',\n        'Programming Language :: Python :: 3.7',\n        'Programming Language :: Python :: 3.8',\n        'Programming Language :: Python :: 3.9',\n        'Topic :: Internet :: WWW/HTTP',\n        'Topic :: Software Development :: Libraries :: Application Frameworks',\n        'Topic :: Software Development :: Libraries :: Python Modules'\n    ]\n)\n"
        },
        {
          "name": "tests",
          "type": "tree",
          "content": null
        },
        {
          "name": "tox.ini",
          "type": "blob",
          "size": 0.15625,
          "content": "[tox]\nenvlist = py35, py36, py37\nskip_missing_interpreters = true\n\n[testenv]\ndeps = \n    -rrequirements.txt\n    -rdev_requirements.txt\ncommands = py.test tests\n"
        }
      ]
    }
  ]
}