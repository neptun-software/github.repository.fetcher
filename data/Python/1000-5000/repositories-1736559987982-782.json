{
  "metadata": {
    "timestamp": 1736559987982,
    "page": 782,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjc5MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "POSTECH-CVLab/PyTorch-StudioGAN",
      "stars": 3444,
      "defaultBranch": "master",
      "files": [
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.1025390625,
          "content": "*__pycache__*\n*.pyc\ndb.sqlite3\n*.DS_Store\nmedia/\nres/\nlogs/\n.vscode/\nsave/\ndata/\nwandb/\ntest/\n*.csv\n*.npy"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 1.072265625,
          "content": "The MIT License (MIT)\n\nPyTorch StudioGAN: \nCopyright (c) 2020 MinGuk Kang\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in\nall copies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\nTHE SOFTWARE.\n"
        },
        {
          "name": "LICENSE-NVIDIA",
          "type": "blob",
          "size": 4.6552734375,
          "content": "Copyright (c) 2019, NVIDIA Corporation. All rights reserved.\r\n\r\n\r\nNvidia Source Code License-NC\r\n\r\n=======================================================================\r\n\r\n1. Definitions\r\n\r\n\"Licensor\" means any person or entity that distributes its Work.\r\n\r\n\"Software\" means the original work of authorship made available under\r\nthis License.\r\n\r\n\"Work\" means the Software and any additions to or derivative works of\r\nthe Software that are made available under this License.\r\n\r\n\"Nvidia Processors\" means any central processing unit (CPU), graphics\r\nprocessing unit (GPU), field-programmable gate array (FPGA),\r\napplication-specific integrated circuit (ASIC) or any combination\r\nthereof designed, made, sold, or provided by Nvidia or its affiliates.\r\n\r\nThe terms \"reproduce,\" \"reproduction,\" \"derivative works,\" and\r\n\"distribution\" have the meaning as provided under U.S. copyright law;\r\nprovided, however, that for the purposes of this License, derivative\r\nworks shall not include works that remain separable from, or merely\r\nlink (or bind by name) to the interfaces of, the Work.\r\n\r\nWorks, including the Software, are \"made available\" under this License\r\nby including in or with the Work either (a) a copyright notice\r\nreferencing the applicability of this License to the Work, or (b) a\r\ncopy of this License.\r\n\r\n2. License Grants\r\n\r\n    2.1 Copyright Grant. Subject to the terms and conditions of this\r\n    License, each Licensor grants to you a perpetual, worldwide,\r\n    non-exclusive, royalty-free, copyright license to reproduce,\r\n    prepare derivative works of, publicly display, publicly perform,\r\n    sublicense and distribute its Work and any resulting derivative\r\n    works in any form.\r\n\r\n3. Limitations\r\n\r\n    3.1 Redistribution. You may reproduce or distribute the Work only\r\n    if (a) you do so under this License, (b) you include a complete\r\n    copy of this License with your distribution, and (c) you retain\r\n    without modification any copyright, patent, trademark, or\r\n    attribution notices that are present in the Work.\r\n\r\n    3.2 Derivative Works. You may specify that additional or different\r\n    terms apply to the use, reproduction, and distribution of your\r\n    derivative works of the Work (\"Your Terms\") only if (a) Your Terms\r\n    provide that the use limitation in Section 3.3 applies to your\r\n    derivative works, and (b) you identify the specific derivative\r\n    works that are subject to Your Terms. Notwithstanding Your Terms,\r\n    this License (including the redistribution requirements in Section\r\n    3.1) will continue to apply to the Work itself.\r\n\r\n    3.3 Use Limitation. The Work and any derivative works thereof only\r\n    may be used or intended for use non-commercially. The Work or\r\n    derivative works thereof may be used or intended for use by Nvidia\r\n    or its affiliates commercially or non-commercially. As used herein,\r\n    \"non-commercially\" means for research or evaluation purposes only.\r\n\r\n    3.4 Patent Claims. If you bring or threaten to bring a patent claim\r\n    against any Licensor (including any claim, cross-claim or\r\n    counterclaim in a lawsuit) to enforce any patents that you allege\r\n    are infringed by any Work, then your rights under this License from\r\n    such Licensor (including the grants in Sections 2.1 and 2.2) will\r\n    terminate immediately.\r\n\r\n    3.5 Trademarks. This License does not grant any rights to use any\r\n    Licensor's or its affiliates' names, logos, or trademarks, except\r\n    as necessary to reproduce the notices described in this License.\r\n\r\n    3.6 Termination. If you violate any term of this License, then your\r\n    rights under this License (including the grants in Sections 2.1 and\r\n    2.2) will terminate immediately.\r\n\r\n4. Disclaimer of Warranty.\r\n\r\nTHE WORK IS PROVIDED \"AS IS\" WITHOUT WARRANTIES OR CONDITIONS OF ANY\r\nKIND, EITHER EXPRESS OR IMPLIED, INCLUDING WARRANTIES OR CONDITIONS OF\r\nMERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, TITLE OR\r\nNON-INFRINGEMENT. YOU BEAR THE RISK OF UNDERTAKING ANY ACTIVITIES UNDER\r\nTHIS LICENSE. \r\n\r\n5. Limitation of Liability.\r\n\r\nEXCEPT AS PROHIBITED BY APPLICABLE LAW, IN NO EVENT AND UNDER NO LEGAL\r\nTHEORY, WHETHER IN TORT (INCLUDING NEGLIGENCE), CONTRACT, OR OTHERWISE\r\nSHALL ANY LICENSOR BE LIABLE TO YOU FOR DAMAGES, INCLUDING ANY DIRECT,\r\nINDIRECT, SPECIAL, INCIDENTAL, OR CONSEQUENTIAL DAMAGES ARISING OUT OF\r\nOR RELATED TO THIS LICENSE, THE USE OR INABILITY TO USE THE WORK\r\n(INCLUDING BUT NOT LIMITED TO LOSS OF GOODWILL, BUSINESS INTERRUPTION,\r\nLOST PROFITS OR DATA, COMPUTER FAILURE OR MALFUNCTION, OR ANY OTHER\r\nCOMMERCIAL DAMAGES OR LOSSES), EVEN IF THE LICENSOR HAS BEEN ADVISED OF\r\nTHE POSSIBILITY OF SUCH DAMAGES.\r\n\r\n=======================================================================\r\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 30.1474609375,
          "content": "<p align=\"center\">\n  <img width=\"60%\" src=\"https://raw.githubusercontent.com/POSTECH-CVLab/PyTorch-StudioGAN/master/docs/figures/studiogan_logo.jpg\" />\n</p>\n\n--------------------------------------------------------------------------------\n\n**StudioGAN** is a Pytorch library providing implementations of representative Generative Adversarial Networks (GANs) for conditional/unconditional image generation. StudioGAN aims to offer an identical playground for modern GANs so that machine learning researchers can readily compare and analyze a new idea.\n\n**Moreover**, StudioGAN provides an unprecedented-scale benchmark for generative models. The benchmark includes results from GANs (BigGAN-Deep, StyleGAN-XL), auto-regressive models (MaskGIT, RQ-Transformer), and Diffusion models (LSGM++, CLD-SGM, ADM-G-U).\n\n# News\n- StudioGAN paper is accepted at IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2023.\n- We provide all checkpoints we used: Please visit [Hugging Face Hub](https://huggingface.co/Mingguksky/PyTorch-StudioGAN/tree/main).\n- Our new paper \"[StudioGAN: A Taxonomy and Benchmark of GANs for Image Synthesis](https://arxiv.org/abs/2206.09479)\" is made public on arXiv.\n- StudioGAN provides implementations of 7 GAN architectures, 9 conditioning methods, 4 adversarial losses, 13 regularization modules, 3 differentiable augmentations, 8 evaluation metrics, and 5 evaluation backbones.\n- StudioGAN supports both clean and architecture-friendly metrics (IS, FID, PRDC, IFID) with a comprehensive benchmark.\n- StudioGAN provides wandb logs and pre-trained models (will be ready soon).\n\n#  Release Notes (v.0.4.0)\n- We checked the reproducibility of implemented GANs.\n- We provide Baby, Papa, and Grandpa ImageNet datasets where images are processed using the anti-aliasing and high-quality resizer.\n- StudioGAN provides a dedicatedly established Benchmark on standard datasets (CIFAR10, ImageNet, AFHQv2, and FFHQ).\n- StudioGAN supports InceptionV3, ResNet50, SwAV, DINO, and Swin Transformer backbones for GAN evaluation.\n\n#  Features\n- **Coverage:** StudioGAN is a self-contained library that provides 7 GAN architectures, 9 conditioning methods, 4 adversarial losses, 13 regularization modules, 6 augmentation modules, 8 evaluation metrics, and 5 evaluation backbones. Among these configurations, we formulate 30 GANs as representatives.\n- **Flexibility:** Each modularized option is managed through a configuration system that works through a YAML file, so users can train a large combination of GANs by mix-matching distinct options.\n- **Reproducibility:** With StudioGAN, users can compare and debug various GANs with the unified computing environment without concerning about hidden details and tricks.\n- **Plentifulness:** StudioGAN provides a large collection of pre-trained GAN models, training logs, and evaluation results.\n- **Versatility:** StudioGAN supports 5 types of acceleration methods with synchronized batch normalization for training: a single GPU training, data-parallel training (DP), distributed data-parallel training (DDP), multi-node distributed data-parallel training (MDDP), and mixed-precision training.\n\n#  Implemented GANs\n\n| Method | Venue | Architecture | GC | DC | Loss | EMA |\n|:-----------|:-------------:|:-------------:|:-------------:|:-------------:|:-------------:|:-------------:|\n| [**DCGAN**](https://arxiv.org/abs/1511.06434) | arXiv'15 | DCGAN/ResNetGAN<sup>[1](#footnote_1)</sup> | N/A | N/A | Vanilla | False |\n| [**InfoGAN**](https://papers.nips.cc/paper/2016/hash/7c9d0b1f96aebd7b5eca8c3edaa19ebb-Abstract.html) | NIPS'16 | DCGAN/ResNetGAN<sup>[1](#footnote_1)</sup> | N/A | N/A | Vanilla | False |\n| [**LSGAN**](https://arxiv.org/abs/1611.04076) | ICCV'17 | DCGAN/ResNetGAN<sup>[1](#footnote_1)</sup> | N/A | N/A | Least Sqaure | False |\n| [**GGAN**](https://arxiv.org/abs/1705.02894) | arXiv'17 | DCGAN/ResNetGAN<sup>[1](#footnote_1)</sup> | N/A | N/A | Hinge | False |\n| [**WGAN-WC**](https://arxiv.org/abs/1701.04862)              |  ICLR'17   |                 ResNetGAN                  |  N/A   |  N/A   | Wasserstein  | False |\n| [**WGAN-GP**](https://arxiv.org/abs/1704.00028)              |  NIPS'17   |                 ResNetGAN                  |  N/A   |  N/A   | Wasserstein  | False |\n| [**WGAN-DRA**](https://arxiv.org/abs/1705.07215)             |  arXiv'17  |                 ResNetGAN                  |  N/A   |  N/A   | Wasserstein  | False |\n| **ACGAN-Mod**<sup>[2](#footnote_2)</sup>                     |     -      |                 ResNetGAN                  |  cBN   |   AC   |    Hinge     | False |\n| [**PDGAN**](https://arxiv.org/abs/1802.05637)                |  ICLR'18   |                 ResNetGAN                  |  cBN   |   PD   |    Hinge     | False |\n| [**SNGAN**](https://arxiv.org/abs/1802.05957)                |  ICLR'18   |                 ResNetGAN                  |  cBN   |   PD   |    Hinge     | False |\n| [**SAGAN**](https://arxiv.org/abs/1805.08318)                |  ICML'19   |                 ResNetGAN                  |  cBN   |   PD   |    Hinge     | False |\n| [**TACGAN**](https://arxiv.org/abs/1907.02690)               | Neurips'19 |                   BigGAN                   |  cBN   |  TAC   |    Hinge     | True  |\n| [**LGAN**](https://arxiv.org/abs/1902.05687)                 |  ICML'19   |                 ResNetGAN                  |  N/A   |  N/A   |   Vanilla    | False |\n| [**Unconditional BigGAN**](https://arxiv.org/abs/1809.11096) |  ICLR'19   |                   BigGAN                   |  N/A   |  N/A   |    Hinge     | True  |\n| [**BigGAN**](https://arxiv.org/abs/1809.11096) | ICLR'19 | BigGAN | cBN | PD | Hinge | True |\n| [**BigGAN-Deep-CompareGAN**](https://github.com/POSTECH-CVLab/PyTorch-StudioGAN/blob/master/src/models/big_resnet_deep_legacy.py) | ICLR'19 | BigGAN-Deep CompareGAN | cBN | PD | Hinge | True |\n| [**BigGAN-Deep-StudioGAN**](https://github.com/POSTECH-CVLab/PyTorch-StudioGAN/blob/master/src/models/big_resnet_deep_studiogan.py) | - | BigGAN-Deep StudioGAN | cBN | PD | Hinge | True |\n| [**StyleGAN2**](https://arxiv.org/abs/1912.04958)            |  CVPR' 20  |                 StyleGAN2                  | cAdaIN |  SPD   |   Logistic   | True  |\n| [**CRGAN**](https://arxiv.org/abs/1910.12027)                |  ICLR'20   |                   BigGAN                   |  cBN   |   PD   |    Hinge     | True  |\n| [**ICRGAN**](https://arxiv.org/abs/2002.04724)               |  AAAI'21   |                   BigGAN                   |  cBN   |   PD   |    Hinge     | True  |\n| [**LOGAN**](https://arxiv.org/abs/1912.00953)                |  arXiv'19  |                 ResNetGAN                  |  cBN   |   PD   |    Hinge     | True  |\n| [**ContraGAN**](https://arxiv.org/abs/2006.12681)            | Neurips'20 |                   BigGAN                   |  cBN   |   2C   |    Hinge     | True  |\n| [**MHGAN**](https://arxiv.org/abs/1912.04216)                |  WACV'21   |                   BigGAN                   |  cBN   |   MH   |      MH      | True  |\n| [**BigGAN + DiffAugment**](https://arxiv.org/abs/2006.10738) | Neurips'20 |                   BigGAN                   |  cBN   |   PD   |    Hinge     | True  |\n| [**StyleGAN2 + ADA**](https://arxiv.org/abs/2006.06676)      | Neurips'20 |                 StyleGAN2                  | cAdaIN |  SPD   |   Logistic   | True  |\n| [**BigGAN + LeCam**](https://arxiv.org/abs/2104.03310)       | CVPR'2021  |                   BigGAN                   |  cBN   |   PD   |    Hinge     | True  |\n| [**ReACGAN**](https://arxiv.org/abs/2111.01118) | Neurips'21 | BigGAN | cBN | D2D-CE | Hinge | True |\n| [**StyleGAN2 + APA**](https://arxiv.org/abs/2111.06849) | Neurips'21 | StyleGAN2 | cAdaIN | SPD | Logistic | True |\n| [**StyleGAN3-t**](https://nvlabs.github.io/stylegan3/) | Neurips'21 | StyleGAN3 | cAaIN | SPD | Logistic | True |\n| [**StyleGAN3-r**](https://nvlabs.github.io/stylegan3/) | Neurips'21 | StyleGAN3 | cAaIN | SPD | Logistic | True |\n| [**ADCGAN**](https://arxiv.org/abs/2107.10060) | ICML'22 | BigGAN | cBN | ADC | Hinge | True |\n\nGC/DC indicates the way how we inject label information to the Generator or Discriminator.\n\n[EMA](https://openreview.net/forum?id=SJgw_sRqFQ): Exponential Moving Average update to the generator.\n[cBN](https://arxiv.org/abs/1610.07629) : conditional Batch Normalization.\n[cAdaIN](https://arxiv.org/abs/1812.04948): Conditional version of Adaptive Instance Normalization.\n[AC](https://arxiv.org/abs/1610.09585) : Auxiliary Classifier.\n[PD](https://arxiv.org/abs/1802.05637) : Projection Discriminator.\n[TAC](https://arxiv.org/abs/1907.02690): Twin Auxiliary Classifier.\n[SPD](https://arxiv.org/abs/1812.04948) : Modified PD for StyleGAN.\n[2C](https://arxiv.org/abs/2006.12681) : Conditional Contrastive loss.\n[MH](https://arxiv.org/abs/1912.04216) : Multi-Hinge loss.\n[ADC](https://arxiv.org/abs/2107.10060) : Auxiliary Discriminative Classifier.\n[D2D-CE](https://arxiv.org/abs/2111.01118) : Data-to-Data Cross-Entropy.\n\n#  Evaluation Metrics\n| Method | Venue | Architecture |\n|:-----------|:-------------:|:-------------:|\n| [**Inception Score (IS)**](https://arxiv.org/abs/1606.03498) | Neurips'16 | InceptionV3 |\n| [**Frechet Inception Distance (FID)**](https://arxiv.org/abs/1706.08500) | Neurips'17 | InceptionV3 |\n| [**Improved Precision & Recall**](https://arxiv.org/abs/1904.06991) | Neurips'19 |        InceptionV3         |\n| [**Classifier Accuracy Score (CAS)**](https://arxiv.org/abs/1905.10887) | Neurips'19 |        InceptionV3         |\n| [**Density & Coverage**](https://arxiv.org/abs/2002.09797)   |  ICML'20   |        InceptionV3         |\n| **Intra-class FID**                                          |     -      |        InceptionV3         |\n| [**SwAV FID**](https://openreview.net/forum?id=NeRdBeTionN) | ICLR'21 | SwAV |\n| [**Clean metrics (IS, FID, PRDC)**](https://arxiv.org/abs/2104.11222) | CVPR'22 | InceptionV3 |\n| [**Architecture-friendly metrics (IS, FID, PRDC)**](https://arxiv.org/abs/2206.09479) | arXiv'22 | Not limited to InceptionV3 |\n\n#  Training and Inference Techniques\n\n| Method                                                 |    Venue     | Target Architecture  |\n| :----------------------------------------------------- | :----------: | :------------------: |\n| [**FreezeD**](https://arxiv.org/abs/2002.10964)        |   CVPRW'20   | Except for StyleGAN2 |\n| [**Top-K Training**](https://arxiv.org/abs/2002.06224) | Neurips'2020 |          -           |\n| [**DDLS**](https://arxiv.org/abs/2003.06060)           | Neurips'2020 |          -           |\n| [**SeFa**](https://arxiv.org/abs/2007.06600)           |  CVPR'2021   |        BigGAN        |\n\n# Reproducibility\n\nWe check the reproducibility of GANs implemented in StudioGAN  by comparing IS and FID with the original papers. We identify our platform successfully reproduces most of representative GANs except for PD-GAN, ACGAN, LOGAN, SAGAN, and BigGAN-Deep. FQ means Flickr-Faces-HQ Dataset (FFHQ). The resolutions of ImageNet, AFHQv2, and FQ datasets are 128, 512, and 1024, respectively.\n\n<p align=\"center\">\n  <img width=\"50%\" src=\"https://raw.githubusercontent.com/POSTECH-CVLab/PyTorch-StudioGAN/master/docs/figures/Reproducibility_.png\" />\n</p>\n\n# Requirements\n\nFirst, install PyTorch meeting your environment (at least 1.7):\n```bash\npip install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu116\n```\n\nThen, use the following command to install the rest of the libraries:\n```bash\npip install tqdm ninja h5py kornia matplotlib pandas sklearn scipy seaborn wandb PyYaml click requests pyspng imageio-ffmpeg timm\n```\n\nWith docker, you can use (Updated 14/DEC/2022):\n```bash\ndocker pull alex4727/experiment:pytorch113_cuda116\n```\n\nThis is our command to make a container named \"StudioGAN\".\n\n```bash\ndocker run -it --gpus all --shm-size 128g --name StudioGAN -v /path_to_your_folders:/root/code --workdir /root/code alex4727/experiment:pytorch113_cuda116 /bin/zsh\n```\nIf your nvidia driver version doesn't satisfy requirements, you can try adding below to above command.\n```bash\n--env NVIDIA_DISABLE_REQUIRE=true\n```\n\n# Dataset\n\n* CIFAR10/CIFAR100: StudioGAN will automatically download the dataset once you execute ``main.py``.\n\n* Tiny ImageNet, ImageNet, or a custom dataset:\n  1. download [Tiny ImageNet](https://gist.github.com/moskomule/2e6a9a463f50447beca4e64ab4699ac4), [Baby ImageNet](https://postechackr-my.sharepoint.com/:f:/g/personal/jaesik_postech_ac_kr/Es-M92IXeN1Dv_L6H_ScswEBxiUanxF9BVsWkH3GsazABQ?e=Bs5ROw), [Papa ImageNet](https://postechackr-my.sharepoint.com/:f:/g/personal/jaesik_postech_ac_kr/Es-M92IXeN1Dv_L6H_ScswEBxiUanxF9BVsWkH3GsazABQ?e=Bs5ROw), [Grandpa ImageNet](https://postechackr-my.sharepoint.com/:f:/g/personal/jaesik_postech_ac_kr/Es-M92IXeN1Dv_L6H_ScswEBxiUanxF9BVsWkH3GsazABQ?e=Bs5ROw), [ImageNet](http://www.image-net.org). Prepare your own dataset.\n  2. make the folder structure of the dataset as follows:\n\n```\ndata\n└── ImageNet, Tiny_ImageNet, Baby ImageNet, Papa ImageNet, or Grandpa ImageNet\n    ├── train\n    │   ├── cls0\n    │   │   ├── train0.png\n    │   │   ├── train1.png\n    │   │   └── ...\n    │   ├── cls1\n    │   └── ...\n    └── valid\n        ├── cls0\n        │   ├── valid0.png\n        │   ├── valid1.png\n        │   └── ...\n        ├── cls1\n        └── ...\n```\n\n# Quick Start\n\nBefore starting, users should login wandb using their personal API key.\n\n```bash\nwandb login PERSONAL_API_KEY\n```\nFrom release 0.3.0, you can now define which evaluation metrics to use through ``-metrics`` option. Not specifying option defaults to calculating FID only. \ni.e. ``-metrics is fid`` calculates only IS and FID and ``-metrics none`` skips evaluation.\n\n\n* Train (``-t``) and evaluate IS, FID, Prc, Rec, Dns, Cvg (``-metrics is fid prdc``) of the model defined in ``CONFIG_PATH`` using GPU ``0``.\n```bash\nCUDA_VISIBLE_DEVICES=0 python3 src/main.py -t -metrics is fid prdc -cfg CONFIG_PATH -data DATA_PATH -save SAVE_PATH\n```\n\n* Preprocess images for training and evaluation using PIL.LANCZOS filter (``--pre_resizer lanczos``). Then, train (``-t``) and evaluate friendly-IS, friendly-FID, friendly-Prc, friendly-Rec, friendly-Dns, friendly-Cvg (``-metrics is fid prdc --post_resizer clean``) of the model defined in ``CONFIG_PATH`` using GPU ``0``.\n```bash\nCUDA_VISIBLE_DEVICES=0 python3 src/main.py -t -metrics is fid prdc --pre_resizer lanczos --post_resizer clean -cfg CONFIG_PATH -data DATA_PATH -save SAVE_PATH\n```\n\n* Train (``-t``) and evaluate FID of the model defined in ``CONFIG_PATH`` through ``DataParallel`` using GPUs ``(0, 1, 2, 3)``. Evaluation of FID does not require (``-metrics``) argument!\n\n```bash\nCUDA_VISIBLE_DEVICES=0,1,2,3 python3 src/main.py -t -cfg CONFIG_PATH -data DATA_PATH -save SAVE_PATH\n```\n\n* Train (``-t``) and skip evaluation (``-metrics none``) of the model defined in ``CONFIG_PATH`` through ``DistributedDataParallel`` using GPUs ``(0, 1, 2, 3)``, ``Synchronized batch norm``, and ``Mixed precision``.\n```bash\nexport MASTER_ADDR=\"localhost\"\nexport MASTER_PORT=2222\nCUDA_VISIBLE_DEVICES=0,1,2,3 python3 src/main.py -t -metrics none -cfg CONFIG_PATH -data DATA_PATH -save SAVE_PATH -DDP -sync_bn -mpc \n```\n\nTry ``python3 src/main.py`` to see available options.\n\n# Supported Training/Testing Techniques\n\n* Load All Data in Main Memory (``-hdf5 -l``)\n  ```bash\n  CUDA_VISIBLE_DEVICES=0,...,N python3 src/main.py -t -hdf5 -l -cfg CONFIG_PATH -data DATA_PATH -save SAVE_PATH\n  ```\n\n* DistributedDataParallel (Please refer to [Here](https://yangkky.github.io/2019/07/08/distributed-pytorch-tutorial.html)) (``-DDP``)\n  ```bash\n  ### NODE_0, 4_GPUs, All ports are open to NODE_1\n  ~/code>>> export MASTER_ADDR=PUBLIC_IP_OF_NODE_0\n  ~/code>>> export MASTER_PORT=AVAILABLE_PORT_OF_NODE_0\n  ~/code/PyTorch-StudioGAN>>> CUDA_VISIBLE_DEVICES=0,1,2,3 python3 src/main.py -t -DDP -tn 2 -cn 0 -cfg CONFIG_PATH -data DATA_PATH -save SAVE_PATH\n  ```\n  ```bash\n  ### NODE_1, 4_GPUs, All ports are open to NODE_0\n  ~/code>>> export MASTER_ADDR=PUBLIC_IP_OF_NODE_0\n  ~/code>>> export MASTER_PORT=AVAILABLE_PORT_OF_NODE_0\n  ~/code/PyTorch-StudioGAN>>> CUDA_VISIBLE_DEVICES=0,1,2,3 python3 src/main.py -t -DDP -tn 2 -cn 1 -cfg CONFIG_PATH -data DATA_PATH -save SAVE_PATH\n  ```\n  \n* [Mixed Precision Training](https://arxiv.org/abs/1710.03740) (``-mpc``)\n  ```bash\n  CUDA_VISIBLE_DEVICES=0,...,N python3 src/main.py -t -mpc -cfg CONFIG_PATH -data DATA_PATH -save SAVE_PATH\n  ```\n  \n* [Change Batch Normalization Statistics](https://arxiv.org/abs/2206.09479)\n  ```bash\n  # Synchronized batchNorm (-sync_bn)\n  CUDA_VISIBLE_DEVICES=0,...,N python3 src/main.py -t -sync_bn -cfg CONFIG_PATH -data DATA_PATH -save SAVE_PATH\n  \n  # Standing statistics (-std_stat, -std_max, -std_step)\n  CUDA_VISIBLE_DEVICES=0,...,N python3 src/main.py -std_stat -std_max STD_MAX -std_step STD_STEP -cfg CONFIG_PATH -ckpt CKPT -data DATA_PATH -save SAVE_PATH\n  \n  # Batch statistics (-batch_stat)\n  CUDA_VISIBLE_DEVICES=0,...,N python3 src/main.py -batch_stat -cfg CONFIG_PATH -ckpt CKPT -data DATA_PATH -save SAVE_PATH\n  ```\n  \n* [Truncation Trick](https://arxiv.org/abs/1809.11096)\n  ```bash\n  CUDA_VISIBLE_DEVICES=0,...,N python3 src/main.py --truncation_factor TRUNCATION_FACTOR -cfg CONFIG_PATH -ckpt CKPT -data DATA_PATH -save SAVE_PATH\n  ```\n\n* [DDLS](https://arxiv.org/abs/2003.06060) (``-lgv -lgv_rate -lgv_std -lgv_decay -lgv_decay_steps -lgv_steps``)\n  ```bash\n  CUDA_VISIBLE_DEVICES=0,...,N python3 src/main.py -lgv -lgv_rate LGV_RATE -lgv_std LGV_STD -lgv_decay LGV_DECAY -lgv_decay_steps LGV_DECAY_STEPS -lgv_steps LGV_STEPS -cfg CONFIG_PATH -ckpt CKPT -data DATA_PATH -save SAVE_PATH\n  ```\n\n* [Freeze Discriminator](https://arxiv.org/abs/2002.10964) (``-freezeD``)\n  ```bash\n  CUDA_VISIBLE_DEVICES=0,...,N python3 src/main.py -t --freezeD FREEZED -ckpt SOURCE_CKPT -cfg TARGET_CONFIG_PATH -data DATA_PATH -save SAVE_PATH\n  ```\n\n# Analyzing Generated Images\n\nStudioGAN supports ``Image visualization, K-nearest neighbor analysis, Linear interpolation, Frequency analysis, TSNE analysis, and Semantic factorization``. All results will be saved in ``SAVE_DIR/figures/RUN_NAME/*.png``.\n\n* Image Visualization\n```bash\nCUDA_VISIBLE_DEVICES=0,...,N python3 src/main.py -v -cfg CONFIG_PATH -ckpt CKPT -save SAVE_DIR\n```\n\n<p align=\"center\">\n  <img width=\"95%\" src=\"https://github.com/POSTECH-CVLab/PyTorch-StudioGAN/blob/master/docs/figures/StudioGAN_generated_images.png\" />\n</p>\n\n\n* K-Nearest Neighbor Analysis (we have fixed K=7, the images in the first column are generated images.)\n```bash\nCUDA_VISIBLE_DEVICES=0,...,N python3 src/main.py -knn -cfg CONFIG_PATH -ckpt CKPT -data DATA_PATH -save SAVE_PATH\n```\n<p align=\"center\">\n  <img width=\"95%\" src=\"https://raw.githubusercontent.com/POSTECH-CVLab/PyTorch-StudioGAN/master/docs/figures/knn_1.png\" />\n</p>\n\n* Linear Interpolation (applicable only to conditional Big ResNet models)\n```bash\nCUDA_VISIBLE_DEVICES=0,...,N python3 src/main.py -itp -cfg CONFIG_PATH -ckpt CKPT -save SAVE_DIR\n```\n<p align=\"center\">\n  <img width=\"95%\" src=\"https://raw.githubusercontent.com/POSTECH-CVLab/PyTorch-StudioGAN/master/docs/figures/interpolated_images.png\" />\n</p>\n\n* Frequency Analysis\n```bash\nCUDA_VISIBLE_DEVICES=0,...,N python3 src/main.py -fa -cfg CONFIG_PATH -ckpt CKPT -data DATA_PATH -save SAVE_PATH\n```\n<p align=\"center\">\n  <img width=\"60%\" src=\"https://raw.githubusercontent.com/POSTECH-CVLab/PyTorch-StudioGAN/master/docs/figures/diff_spectrum1.png\" />\n</p>\n\n\n* TSNE Analysis\n```bash\nCUDA_VISIBLE_DEVICES=0,...,N python3 src/main.py -tsne -cfg CONFIG_PATH -ckpt CKPT -data DATA_PATH -save SAVE_PATH\n```\n<p align=\"center\">\n  <img width=\"80%\" src=\"https://raw.githubusercontent.com/POSTECH-CVLab/PyTorch-StudioGAN/master/docs/figures/TSNE_results.png\" />\n</p>\n\n* Semantic Factorization for BigGAN\n```bash\nCUDA_VISIBLE_DEVICES=0,...,N python3 src/main.py -sefa -sefa_axis SEFA_AXIS -sefa_max SEFA_MAX -cfg CONFIG_PATH -ckpt CKPT -save SAVE_PATH\n```\n<p align=\"center\">\n  <img width=\"95%\" src=\"https://raw.githubusercontent.com/POSTECH-CVLab/PyTorch-StudioGAN/master/docs/figures/fox.png\" />\n</p>\n\n\n\n#  Training GANs\n\nStudioGAN supports the training of 30 representative GANs from DCGAN to StyleGAN3-r.\n\nWe used different scripts depending on the dataset and model, and it is as follows:\n\n### CIFAR10\n```bash\nCUDA_VISIBLE_DEVICES=0 python3 src/main.py -t -hdf5 -l -std_stat -std_max STD_MAX -std_step STD_STEP -metrics is fid prdc -ref \"train\" -cfg CONFIG_PATH -data DATA_PATH -save SAVE_PATH -mpc --post_resizer \"friendly\" --eval_backbone \"InceptionV3_tf\"\n```\n\n### CIFAR10 using StyleGAN2/3\n```bash\nCUDA_VISIBLE_DEVICES=0 python3 src/main.py -t -hdf5 -l -metrics is fid prdc -ref \"train\" -cfg CONFIG_PATH -data DATA_PATH -save SAVE_PATH -mpc --post_resizer \"friendly\" --eval_backbone \"InceptionV3_tf\"\n```\n\n### Baby/Papa/Grandpa ImageNet and ImageNet\n```bash\nCUDA_VISIBLE_DEVICES=0,1,2,3 python3 src/main.py -t -hdf5 -l -sync_bn -std_stat -std_max STD_MAX -std_step STD_STEP -metrics is fid prdc -ref \"train\" -cfg CONFIG_PATH -data DATA_PATH -save SAVE_PATH -mpc --pre_resizer \"lanczos\" --post_resizer \"friendly\" --eval_backbone \"InceptionV3_tf\"\n```\n\n### AFHQv2\n```bash\nexport MASTER_ADDR=\"localhost\"\nexport MASTER_PORT=8888\nCUDA_VISIBLE_DEVICES=0,1,2,3 python3 src/main.py -t -metrics is fid prdc -ref \"train\" -cfg CONFIG_PATH -data DATA_PATH -save SAVE_PATH -mpc --pre_resizer \"lanczos\" --post_resizer \"friendly\" --eval_backbone \"InceptionV3_tf\"\n```\n\n### FFHQ\n```bash\nexport MASTER_ADDR=\"localhost\"\nexport MASTER_PORT=8888\nCUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7 python3 src/main.py -t -metrics is fid prdc -ref \"train\" -cfg CONFIG_PATH -data DATA_PATH -save SAVE_PATH -mpc --pre_resizer \"lanczos\" --post_resizer \"friendly\" --eval_backbone \"InceptionV3_tf\"\n```\n\n#  Metrics\n\nStudioGAN supports Inception Score, Frechet Inception Distance, Improved Precision and Recall, Density and Coverage, Intra-Class FID, Classifier Accuracy Score. Users can get ``Intra-Class FID, Classifier Accuracy Score`` scores using ``-iFID, -GAN_train, and -GAN_test`` options, respectively. \n\nUsers can change the evaluation backbone from InceptionV3 to ResNet50, SwAV, DINO, or Swin Transformer using ``--eval_backbone ResNet50_torch, SwAV_torch, DINO_torch, or Swin-T_torch`` option.\n\nIn addition, Users can calculate metrics with clean- or architecture-friendly resizer using ``--post_resizer clean or friendly`` option.\n\n### 1. Inception Score (IS)\nInception Score (IS) is a metric to measure how much GAN generates high-fidelity and diverse images. Calculating IS requires the pre-trained Inception-V3 network. Note that we do not split a dataset into ten folds to calculate IS ten times.\n\n### 2. Frechet Inception Distance (FID)\nFID is a widely used metric to evaluate the performance of a GAN model. Calculating FID requires the pre-trained Inception-V3 network, and modern approaches use [Tensorflow-based FID](https://github.com/bioinf-jku/TTUR). StudioGAN utilizes the [PyTorch-based FID](https://github.com/mseitzer/pytorch-fid) to test GAN models in the same PyTorch environment. We show that the PyTorch based FID implementation provides [almost the same results](https://github.com/POSTECH-CVLab/PyTorch-StudioGAN/blob/master/docs/figures/Table3.png) with the TensorFlow implementation (See Appendix F of [ContraGAN paper](https://arxiv.org/abs/2006.12681)).\n\n### 3. Improved Precision and Recall (Prc, Rec)\nImproved precision and recall are developed to make up for the shortcomings of the precision and recall. Like IS, FID, calculating improved precision and recall requires the pre-trained Inception-V3 model. StudioGAN uses the PyTorch implementation provided by [developers of density and coverage scores](https://github.com/clovaai/generative-evaluation-prdc). \n\n### 4. Density and Coverage (Dns, Cvg)\nDensity and coverage metrics can estimate the fidelity and diversity of generated images using the pre-trained Inception-V3 model. The metrics are known to be robust to outliers, and they can detect identical real and fake distributions. StudioGAN uses the [authors' official PyTorch implementation](https://github.com/clovaai/generative-evaluation-prdc), and StudioGAN follows the author's suggestion for hyperparameter selection.\n\n# Benchmark\n\n#### ※ We always welcome your contribution if you find any wrong implementation, bug, and misreported score.\n\nWe report the best IS, FID, Improved Precision & Recall, and Density & Coverage of GANs.\n\nTo download all checkpoints reported in StudioGAN, Please [**click here**](https://huggingface.co/Mingguksky/PyTorch-StudioGAN/tree/main) (Hugging face Hub).\n\nYou can evaluate the checkpoint by adding ``-ckpt CKPT_PATH`` option with the corresponding configuration path ``-cfg CORRESPONDING_CONFIG_PATH``. \n\n### 1. GANs from StudioGAN\n\nThe resolutions of CIFAR10, Baby ImageNet, Papa ImageNet, Grandpa ImageNet, ImageNet, AFHQv2, and FQ are 32, 64, 64, 64, 128, 512, and 1024, respectively.\n\nWe use the same number of generated images as the training images for Frechet Inception Distance (FID), Precision, Recall, Density, and Coverage calculation. For the experiments using Baby/Papa/Grandpa ImageNet and ImageNet, we exceptionally use 50k fake images against a complete training set as real images.\n\nAll features and moments of reference datasets can be downloaded via [**features**](https://postechackr-my.sharepoint.com/:f:/g/personal/jaesik_postech_ac_kr/ElbkH1fLidJDpzUvrZZiT6EBZgBUhi-t1xoOhnqCas2p9g?e=WfGdGT) and [**moments**](https://postechackr-my.sharepoint.com/:f:/g/personal/jaesik_postech_ac_kr/En88Meh2gJtKk-1tIM1b3YEBcUZlP_4ksAI-qAS9pja4Yw?e=3OWJ7E).\n\n<p align=\"center\">\n  <img width=\"95%\" src=\"https://raw.githubusercontent.com/POSTECH-CVLab/PyTorch-StudioGAN/master/docs/figures/StudioGAN_Benchmark_.png\"/>\n</p>\n\n### 2. Other generative models\n\nThe resolutions of ImageNet-128 and ImageNet 256 are 128 and 256, respectively.\n\nAll images used for Benchmark can be downloaded via One Drive (will be uploaded soon).\n\n<p align=\"center\">\n  <img width=\"95%\" src=\"https://raw.githubusercontent.com/POSTECH-CVLab/PyTorch-StudioGAN/master/docs/figures/Other_Benchmark.png\"/>\n</p>\n\n# Evaluating pre-saved image folders\n\n* Evaluate IS, FID, Prc, Rec, Dns, Cvg (``-metrics is fid prdc``) of image folders (already preprocessed) saved in DSET1 and DSET2 using GPUs ``(0,...,N)``.\n\n```bash\nCUDA_VISIBLE_DEVICES=0,...,N python3 src/evaluate.py -metrics is fid prdc --dset1 DSET1 --dset2 DSET2\n```\n\n* Evaluate IS, FID, Prc, Rec, Dns, Cvg (``-metrics is fid prdc``) of image folder saved in DSET2 using pre-computed features (``--dset1_feats DSET1_FEATS``), moments of dset1 (``--dset1_moments DSET1_MOMENTS``), and GPUs ``(0,...,N)``.\n\n```bash\nCUDA_VISIBLE_DEVICES=0,...,N python3 src/evaluate.py -metrics is fid prdc --dset1_feats DSET1_FEATS --dset1_moments DSET1_MOMENTS --dset2 DSET2\n```\n\n* Evaluate friendly-IS, friendly-FID, friendly-Prc, friendly-Rec, friendly-Dns, friendly-Cvg (``-metrics is fid prdc --post_resizer friendly``) of image folders saved in DSET1 and DSET2 through ``DistributedDataParallel`` using GPUs ``(0,...,N)``.\n\n```bash\nexport MASTER_ADDR=\"localhost\"\nexport MASTER_PORT=2222\nCUDA_VISIBLE_DEVICES=0,...,N python3 src/evaluate.py -metrics is fid prdc --post_resizer friendly --dset1 DSET1 --dset2 DSET2 -DDP\n```\n\n## StudioGAN thanks the following Repos for the code sharing\n\n[[MIT license]](https://github.com/POSTECH-CVLab/PyTorch-StudioGAN/blob/master/src/sync_batchnorm/LICENSE) Synchronized BatchNorm: https://github.com/vacancy/Synchronized-BatchNorm-PyTorch\n\n[[MIT license]](https://github.com/POSTECH-CVLab/PyTorch-StudioGAN/blob/master/src/utils/ops.py) Self-Attention module: https://github.com/voletiv/self-attention-GAN-pytorch\n\n[[MIT license]](https://github.com/POSTECH-CVLab/PyTorch-StudioGAN/blob/master/src/utils/diffaug.py) DiffAugment: https://github.com/mit-han-lab/data-efficient-gans\n\n[[MIT_license]](https://github.com/POSTECH-CVLab/PyTorch-StudioGAN/blob/master/src/metrics/prdc.py) PyTorch Improved Precision and Recall: https://github.com/clovaai/generative-evaluation-prdc\n\n[[MIT_license]](https://github.com/POSTECH-CVLab/PyTorch-StudioGAN/blob/master/src/metrics/prdc.py) PyTorch Density and Coverage: https://github.com/clovaai/generative-evaluation-prdc\n\n[[MIT license]](https://github.com/POSTECH-CVLab/PyTorch-StudioGAN/blob/master/src/utils/resize.py) PyTorch clean-FID: https://github.com/GaParmar/clean-fid\n\n[[NVIDIA source code license]](https://github.com/POSTECH-CVLab/PyTorch-StudioGAN/blob/master/LICENSE-NVIDIA) StyleGAN2: https://github.com/NVlabs/stylegan2\n\n[[NVIDIA source code license]](https://github.com/POSTECH-CVLab/PyTorch-StudioGAN/blob/master/LICENSE-NVIDIA) Adaptive Discriminator Augmentation: https://github.com/NVlabs/stylegan2\n\n[[Apache License]](https://github.com/POSTECH-CVLab/PyTorch-StudioGAN/blob/master/src/metrics/fid.py) Pytorch FID: https://github.com/mseitzer/pytorch-fid\n\n## License\nPyTorch-StudioGAN is an open-source library under the MIT license (MIT). However, portions of the library are avaiiable under distinct license terms: StyleGAN2, StyleGAN2-ADA, and StyleGAN3 are licensed under [NVIDIA source code license](https://github.com/POSTECH-CVLab/PyTorch-StudioGAN/blob/master/LICENSE-NVIDIA), and PyTorch-FID is licensed under [Apache License](https://github.com/POSTECH-CVLab/PyTorch-StudioGAN/blob/master/src/metrics/fid.py).\n\n## Citation\nStudioGAN is established for the following research projects. Please cite our work if you use StudioGAN.\n```bib\n@article{kang2023StudioGANpami,\n  title   = {{StudioGAN: A Taxonomy and Benchmark of GANs for Image Synthesis}},\n  author  = {MinGuk Kang and Joonghyuk Shin and Jaesik Park},\n  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)},\n  year    = {2023}\n}\n```\n\n```bib\n@inproceedings{kang2021ReACGAN,\n  title   = {{Rebooting ACGAN: Auxiliary Classifier GANs with Stable Training}},\n  author  = {Minguk Kang, Woohyeon Shim, Minsu Cho, and Jaesik Park},\n  journal = {Conference on Neural Information Processing Systems (NeurIPS)},\n  year    = {2021}\n}\n```\n\n```bib\n@inproceedings{kang2020ContraGAN,\n  title   = {{ContraGAN: Contrastive Learning for Conditional Image Generation}},\n  author  = {Minguk Kang and Jaesik Park},\n  journal = {Conference on Neural Information Processing Systems (NeurIPS)},\n  year    = {2020}\n}\n```\n---------------------------------------\n\n<a name=\"footnote_1\">[1]</a> Experiments on Tiny ImageNet are conducted using the ResNet architecture instead of CNN.\n\n<a name=\"footnote_2\">[2]</a> Our re-implementation of [ACGAN (ICML'17)](https://arxiv.org/abs/1610.09585) with slight modifications, which bring strong performance enhancement for the experiment using CIFAR10.\n"
        },
        {
          "name": "docs",
          "type": "tree",
          "content": null
        },
        {
          "name": "logs",
          "type": "tree",
          "content": null
        },
        {
          "name": "src",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}