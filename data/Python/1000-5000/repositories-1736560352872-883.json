{
  "metadata": {
    "timestamp": 1736560352872,
    "page": 883,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjg5MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "arielf/weight-loss",
      "stars": 3324,
      "defaultBranch": "master",
      "files": [
        {
          "name": "1992-ariel-dmv.png",
          "type": "blob",
          "size": 96.5185546875,
          "content": null
        },
        {
          "name": "HOWTO.md",
          "type": "blob",
          "size": 2.279296875,
          "content": "# How to run this code\n\n------------------\n## Prerequisites:\n\nThis code depends on:\n\n>- vowpal wabbit (aka vw)\n>- R\n>- ggplot2 (an R library to create charts)\n>- GNU make\n>- git (to clone this repository)\n>- bash, perl, and python (these are usually preinstalled and available on all Linux and MacOs systems)\n\n------------------\n## Installation of prerequisites:\n\n#### Linux: Ubuntu, Mint, or any Debian derivative \n\n>    sudo apt-get install make vowpal-wabbit r-base r-base-core r-cran-ggplot2 git\n\n#### Other Linux systems\n\nPackages are usually named differently.\nContributions to this section very welcome\n\n#### MacOs / OS-X\n\nUse `brew` to install the above packages\nContributions to this section very welcome\n\n#### Windows\n\nThe only sane way to run this code in a Windows environment, is to install run Ubuntu Linux on a VM (virtual machine) inside Windows, and use the Ubuntu instructions in the VM.\n\nFor instructions how to set up a VM on Windows, follow these youtube videos:\n>    - http://www.howtogeek.com/170870/5-ways-to-run-linux-software-on-windows/\n>    - https://www.youtube.com/watch?v=uzhA5p-EzqY\n\nOne you have Ubuntu on Windows you just install all the prerequisites. e.g. in a terminal:\n\n>    sudo apt-get install make r-base r-base-core r-cran-ggplot2 vowpal-wabbit git\n\ninside it, to run everything from start to finish.\n\n\n------------------\n## Running the code\n\nUsing git, you clone this repository:\n\n>    git clone https://github.com/arielf/weight-loss\n\nAnd change directory to it:\n\n>    cd weight-loss\n\nFinally type:\n\n>    make\n\nIt should produce a file `scores.txt` with your weight-loss scores.\n\nTo get a chart of the scores:\n\n>    make sc\n\nor\n\n>    make score-chart\n\n------------------\n## Changing `make` parameters\n\nThere are a few adjustable variables (which have reasonable defaults) in the\n`Makefile`, and which you may change if interested:\n\nTo change these you may call `make` with arguments changing the\nvalues, like this: \n\n>     make VarName1=Value1 VarName2=Value2 ...\n\nThe current variables and their defaults settings are:\n\n    BS = 7            # -- bootstrapping rounds\n    P = 4             # -- multiple passes over the data\n    L = 0.05          # -- learning rate\n    L2 = 1.85201e-08  # -- L2 regularization\n    NDAYS = 3         # -- Aggregate consecutive daily-data up to NDAYS days\n\n\n"
        },
        {
          "name": "Licence.md",
          "type": "blob",
          "size": 1.615234375,
          "content": "### BSD 2-Clause License\n\n    OWNER = Ariel Faigon\n    YEAR = 2016\n\n    In the original BSD license, both occurrences of the phrase\n    \"COPYRIGHT HOLDERS AND CONTRIBUTORS\" in the disclaimer read \"REGENTS\n    AND CONTRIBUTORS\".\n\n    Here is the license template:\n\n    Copyright (c) 2012-2016, Ariel Faigon\n    All rights reserved.\n\n    Redistribution and use in source and binary forms, with or without\n    modification, are permitted provided that the following conditions\n    are met:\n\n    1. Redistributions of source code must retain the above copyright\n    notice, this list of conditions and the following disclaimer.\n\n    2. Redistributions in binary form must reproduce the above copyright\n    notice, this list of conditions and the following disclaimer in\n    the documentation and/or other materials provided with the\n    distribution.\n\n    THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS\n    \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT\n    LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS\n    FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE\n    COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT,\n    INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING,\n    BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;\n    LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\n    CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT\n    LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN\n    ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\n    POSSIBILITY OF SUCH DAMAGE.\n\n\n"
        },
        {
          "name": "Longevity.md",
          "type": "blob",
          "size": 3.6044921875,
          "content": "Longevity: Lifestyle factors\n============================\n\n### _The following is a collection of factors affecting longevity_\n\nWhat better source you can have for learning what leads to a longer\nlife, than asking the longest living people themselves?\n\nMy main source in the [wikipedia page about verified longest living people](https://en.wikipedia.org/wiki/List_of_the_verified_oldest_people)\n\nIn their own opinions/words.\n\n#### [Jeanne Calment](https://en.wikipedia.org/wiki/Jeanne_Calment) (122 years, 164 days)\n\nCalment ascribed her longevity and relatively youthful appearance\nfor her age to a diet rich in *olive oil* (which she also rubbed\nonto her skin), as well as a diet of port wine, and ate nearly one\nkilogram (2.2 lb) of chocolate every week. She also credited her\ncalmness, saying, \"That's why they call me Calment.\" Calment\nreportedly remained mentally intact until her very end.\n\n#### [Sarah Knauss](https://en.wikipedia.org/wiki/Sarah_Knauss) (119 years, 97 days)\n\nHer daughter, Kathryn Sullivan, aged 96 at the time, opined that\nKnauss is \"a very tranquil person and nothing fazes her.\"\n\n#### [Misao Okawa](https://en.wikipedia.org/wiki/Misao_Okawa) (117 years, 27 days)\n\nOkawa said that \"sushi and *sleep*\" were the reasons why she lived so long.\n\n\n#### [Maria Capovilla](https://en.wikipedia.org/wiki/Mar%C3%ADa_Capovilla) (116 years 347 days)\n\nShe never smoked or drank hard liquor.\n\n#### [Susannah Mushatt Jones](https://en.wikipedia.org/wiki/Susannah_Mushatt_Jones) (116 years and 311 days)\n\nShe only took high-blood pressure medication and a multivitamin.\nJones never smoked or consumed alcohol. She *slept about ten\nhours a night* and napped throughout the day. For breakfast, she\nalways ate four strips of bacon along with *scrambled eggs* and\ngrits. She also ate bacon throughout the day.\n\n\n#### [Emma Morano](https://en.wikipedia.org/wiki/Emma_Morano) (116 years, 277 days)\nWhen asked about the secret of her longevity, she said that she had\nnever used drugs, eats *three eggs a day*, drinks a glass of homemade\nbrandy, and enjoys a chocolate sometimes, but, above all, she thinks\npositively about the future.\n\nElsewhere: Morano credits her long life to her diet of raw eggs\nand being single.\n\n\n#### [Gertrude Weaver](https://en.wikipedia.org/wiki/Gertrude_Weaver) (116 years, 276 days)\nWeaver told the Associated Press that there were three factors that\nhave contributed to her longevity: \"Trusting in the Lord, hard work\nand loving everybody.\" Weaver added a fourth factor when she\ntold Time magazine that trying to do your best is another factor\nadding: \"Just do what you can, and if we can't, we can't\"\nor, in other words, \"Kindness\"\n\n#### [Besse Cooper](https://en.wikipedia.org/wiki/Besse_Cooper) (116 years, 100 days)\n\nCooper reportedly attributed her longevity to \"minding her own business\"\nand *avoiding junk food.*\n\n\n#### [Jeralean Talley](https://en.wikipedia.org/wiki/Jeralean_Talley) (116 years, 25 days)\n\nShe lived by the Golden Rule: \"Treat others the way you want to\nbe treated\". She was known in the community for her wisdom and\nwit, she had sometimes been asked for advice. She had previously\nadvised people to use common sense, saying \"I don't have\nmuch education but what little sense I got, I try to use it.\"\n\n\n#### Coincidence?\n\nI find it fascinating that so many of my own discoveries of\nfactors affecting weight-loss have also to do with longevity.\n\nBeing calm (and reducing stress) seems to be an important\nfactor I appear to have missed. Could it be hidden in\nthe \"stayhome\" feature?\n\nChocolate appears twice above, I wonder which type of chocolate\nbecause bittersweet/dark chocolate typically has much less sugar than\nmilk chocolate.\n\n"
        },
        {
          "name": "Makefile",
          "type": "blob",
          "size": 4.8193359375,
          "content": "#\n# Makefile for diet and weight-loss monitoring\n# vim: ts=8 sw=8 noexpandtab nosmarttab\n#\n# Goal:\n#\t- Find out which lifestyle factors affect your weight the most\n#\t- Find out which foods make you gain or lose weight\n#\t- Find confidence-level (ranges) for each food/lifestyle item\n#\n# Requires you to:\n#\t- Weight yourself once a day\n#\t- Record what you do/eat daily\n#\n# How to run this code:\n#\t- Install vowpal-wabbit (vw)\n#\t- Clone this repo: https://github.com/arielf/weight-loss\n#\t- Place your data in <username>.csv\n#\t- Type 'make'\n#\n# Additional 'make' targets (make <target>):\n#\n#    c/charts\n#\tCreates optional charts\n#\n#    sc\n#\tCreates the per-item scores chart only\n#\n#    m/model\n#\tCreates a model file from the daily train-file\n#\n#    t/train\n#\tCreates the daily-delta (weight change target) train file\n#\n#    i/items\n#\tCreates 'by single-item' train file. This is a \"pretend\"\n#\tdata-file as if we only had one-item/day to see what's\n#\tits \"pretend-isolated\" effect assuming everything else is equal.\n#\n#    conf/confidence/r/range\n#\tGenerates a sorted *.range file, in which each item appears\n#\ttogether with its 'confidence range' [min max]. This can\n#\thelp you figure out how certain we are for each variable.\n#\te.g. a line like this:\n#\t\t-0.024568 carrot -0.071207 0.026108\n#\tmeans based on the given data, the machine-learning process\n#\testimates carrot makes you lose a bit of weight\n#\t(average is a negative: -0.024568) but the confidence\n#\tdaily range is from -0.071207 (loss) to 0.026108 (gain)\n#\tso there's a low confidence in this result.\n#\n#   conv\n#\tGenerates a convergence chart of the learning process\n#\n#   clean\n#\tCleans-up generated files\n#\nPATH := $(PATH)::.\nNAME = $(shell ./username)\n\n# -- scripts/programs\nVW = vw\nTOVW := lifestyle-csv2vw\nVARINFO := vw-varinfo2\nSORTABS := sort-by-abs\n\n# Adjustable parameters: to change call 'make' with NAME=Value:\n# --bootsrap rounds:\nBS = 7\n# --passes:\nP = 4\n# -- learning rate\nL = 0.05\n# L2 regularization\nL2 = 1.85201e-08\n\n# Aggregate consecutive daily-data up to this number of days\nNDAYS = 3\n\n#\n# vowpal-wabbit training args\n#\nVW_ARGS = \\\n\t-k \\\n\t--loss_function quantile \\\n\t--progress 1 \\\n\t--bootstrap $(BS) \\\n\t-l $(L) \\\n\t--l2 $(L2) \\\n\t-c --passes $(P)\n\n# -- Commented out random shuffling methods\n#    now sorting examples by abs(delta).\n#    Overfitting is countered (though not completely avoided) by:\n#\t* Aggregating on multiple partly overlapping N-day periods\n#\t* Bootstrapping each example (multiple times) via --bootstrap\n#\n# Mutliple orders via shuffling and averaging results should be\n# considered as a future option.\n#\n# SHUFFLE := shuf\n# SHUFFLE := unsort --seed $(SEED)\n#\n\n# -- data files\nMASTERDATA = $(NAME).csv\nTRAINFILE  = $(NAME).train\nITEMFILE  = $(NAME).items\nMODELFILE  = $(NAME).model\nRANGEFILE = $(NAME).range\nDWCSV := weight.2015.csv\nDWPNG := $(NAME).weight.png\nSCPNG := $(NAME).scores.png\n\n.PRECIOUS: Makefile $(MASTERDATA) $(TOVW)\n\n#\n# -- rules\n#\nall:: score\n\ns score scores.txt: $(TRAINFILE)\n\t$(VARINFO) $(VW_ARGS) -d $(TRAINFILE) | tee scores.txt\n\nc charts: weight-chart score-chart\n\n# -- Weight by date chart\nwc weight-chart $(DWPNG): date-weight.r $(DWCSV)\n\tRscript --vanilla date-weight.r $(DWCSV) $(DWPNG)\n\t@echo \"=== done: date-weight chart saved in: '$(DWPNG)'\"\n\n# -- Feature importance score chart\nsc score-chart $(SCPNG): scores.txt score-chart.r\n\t@perl -ane '$$F[5] =~ tr/%//d ;print \"$$F[0],$$F[5]\\n\"' scores.txt > scores.csv\n\t@Rscript --vanilla score-chart.r scores.csv $(SCPNG)\n\t@echo \"=== done: weight-loss factors chart saved in: '$(SCPNG)'\"\n\n# -- model\nm model $(MODELFILE): Makefile $(TRAINFILE)\n\t$(VW) $(VW_ARGS) -f $(MODELFILE) -d $(TRAINFILE)\n\n# -- train-set generation\nt train $(TRAINFILE): Makefile $(MASTERDATA) $(TOVW)\n\t$(TOVW) $(NDAYS) $(MASTERDATA) | sort-by-abs > $(TRAINFILE)\n\n# -- generate 'by single-item' train file\ni items $(ITEMFILE): $(TRAINFILE)\n\ttrain-to-items $(TRAINFILE) > $(ITEMFILE)\n\n# -- Find daily 'range' for 'per-item'\n#    This finds a ~90% confidence interval (leverages vw --bootstrap)\nconf confidence r range $(RANGEFILE): $(MODELFILE) $(ITEMFILE)\n\t$(VW) --quiet -t -i $(MODELFILE) \\\n\t\t-d $(ITEMFILE) -p /dev/stdout | sort -g > $(RANGEFILE)\n\n# -- convergence chart\nconv: $(TRAINFILE)\n\t$(VW) $(VW_ARGS) -d $(TRAINFILE) 2>&1 | vw-convergence\n\nclean:\n\t/bin/rm -f $(MODELFILE) $(ITEMFILE) $(RANGEFILE) *.cache* *.tmp*\n\n# -- more friendly error if original data doesn't exist\n$(MASTERDATA):\n\t@echo \"=== Sorry: you must provide your data in '$(MASTERDATA)'\"\n\t@exit 1\n\n# commit and push\ncp:\n\tgit commit . && git push\n\n# sync gh-pages with master & push\ngh:\n\tgit checkout gh-pages && \\\n\tgit merge master && \\\n\tgit push && \\\n\tgit checkout master\n\n#\n# Trick for introspection of this Makefile variables from the outside\n# (Needs VARNAME=<some_makefile_varname>):\n#\n# Examples:\n#       $ make VARNAME=MASTERDATA echovar\n#\n#       $ make VARNAME=TRAINFILE ev\n#\nev echovar:\n\t@echo $($(VARNAME))\n\n"
        },
        {
          "name": "QandA.md",
          "type": "blob",
          "size": 6.7001953125,
          "content": "# Frequently asked Questions, Answers, & Comments\n\n------------------\n\n#### I'm not a coder, this is too difficult for me to use; can you help?\n\n> I hear you. One reason I put this on github is so others can take it further: write a web or mail service on top, or even a smart-phone app.\n>\n> Since publishing I've become aware of one site using this work as a back-end. Please note this is very early, experimental, work in progress:\n>\n>    ***[weightbrains.com](http://weightbrains.com) alas, link is now dead***\n>\n> It would be cool to see more people taking this work further.\n>\n\n\n#### This is an example of how to not do scientific research!\n\n> Indeed, this is not scientific research.   Rather, this is:\n>\n>   - Software that can help you make sense of your own data\n>   - Sharing in hope of further work and improvement\n>   - A personal story of discovery\n>   - In the end (judging by results) - a success story\n>\n\n\n#### Isn't factor X missing (e.g. Coffee)?\n\n> Yes, many additional factors are missing.  I encourage you to use your own data, and what you personally feel is most important.\n\n\n#### Doesn't dehydration and depleting Glycogen in the liver explain most of this weight loss?\n\n> For the 1st 24 hours of a diet, and about 1 Lb of loss (approx. weight of total body glycogen) yes.  However, I can't attribute over 20 Lb loss over 1 year to just water loss.\n\n#### Doesn't caloric restriction explain all of this weight loss?\n\n> This may well be true.  I haven't counted calories and a more thorough experiment should add all the data it can use.\n>\n> However, I tried multiple times to restrict my diet and was not successful.  So even if \"caloric restriction\" is the best explanation, it comes short as \"the solution\".\n>\n> What led to success in my case is a combination of physiological and psychological factors. Most importantly, the realization that by trying a LCHF (Low Carb, High Fat) diet, I can sustain a diet regime (possibly calorie restricted, I don't know) that lead to a clear and sustainable weight loss.\n>\n> The story is about the discovery process of what worked for me in the end.\n\n#### Machine learning: aren't you over-fitting the data?\n\n> Possibly. The number of days in the data (about 120) may be too small, and my scales have low resolution. There may have other data-entry errors in it.\n>\n> OTOH: I tried many combinations, data-subsets, shuffling, bootstrapping, and while the details varied, the main conclusions were pretty consistent: longer sleep, Low carb, high fat, were leading me to losing weight.\n>\n> More data, and data from many people is always welcome.\n>\n\n#### Machine learning: how much data do I need?\n\n> The more the better. More importantly: higher resolution scales (0.1 lb or less) are especially important.\n>\n> To increase the sample size, the most recent version of the software no longer uses each day as a single data point. It augments it in two ways:\n>   - It applies a variable-length sliding window on 1..N consecutive days over the data to auto-generate additional data-points.\n>   - It applies randomized bootstrapping on each data example. This adds another multiplier towards reducing random noise and variance.\n>\n> You're welcome to play with the Makefile `NDAYS` (max length of consecutive days of the sliding window) and `BS` (number of random bootstrapping rounds multiplier for each data-set example) parameters to see how much results change, and which part remains relatively stable.\n> For example:\n>\n>      make BS=7 NDAYS=5\n>\n> My conclusions were that while some items in the middle fall into the \"too little data to conclude from\" category, items closer to the top/bottom, as a group, point to sleeping (fasting) longer and substituting carbs for fat, as likely weight-loss factors.\n\n#### What's the story on statins? How is this related to weight-loss?\n\n> [Original appeared in the HackerNews thread, some edits applied]\n>\n> Here's my personal experience with statins. I may be wrong, but I'm following my compass while always open to be proven otherwise.\n>\n> Doctor: \"Your 'bad cholesterol' is borderline, I want you to start taking \"Lipitor\"...\n\n> Observation: when the 'Lipitor' patent expired, and it became a cheap generic drug, the suggestion turned into 'Crestor' which I learned has a bit longer effective half-life, and way higher price.\n\n> ***Me: (while adopting a different diet)***\n> *\"Hmm statins would have taken my cholesterol and triglicerids  _maybe_ 3% lower and here I am 20% lower after a year of a simple, self-studied, diet change. Maybe there are better ways to lower the so called 'bad cholesterol'?*\n\n> Further study: there's always a new statin the moment the previous patent expires.\n>\n> Check out the following names on Wikipedia:\n>\n>    - Compactin\n>    - Simvastatin\n>    - Fluvastatin\n>    - Cerivastatin\n>    - Atorvastatin\n>    - Rosuvastatin\n>\n> These are all chemical names, not brand names, the last two on the list are the brands: known as \"Lipitor\" and \"Crestor\".\n>\n> So I don't know. I'm 100% sure all my doctors are well meaning and caring and I have nothing against them, but my confidence in such health suggestions, in research funded by big-pharma, and in the new great statin de-jour while America keeps getting obese and less healthy, is, how can I put it? A bit shaken.\n>\n> Again, just prove me wrong, and I'll change my view.\n\n\n#### Can you tell the story of how, and how does it feel to \"Go Viral\"?\n\n>\n> From my PoV, it was totally accidental.\n>\n> I was trying to lose weight, and since I'm into ML, I found it natural to try some machine learning to guide me, especially in the early stages. It helped.\n>\n> When I told this to a colleague of mine, she suggested \"You heave to share this more widely!\" So I dusted it up a bit, cleaned the code to make it a bit more usable by others, wrote a few pages of background and put it on github.\n>\n> On github it sat for a few months with zero forks, zero stars, zero watchers, totally ignored by the world :-)\n>\n> Then some guy named Dmitri noticed it, and posted a link to [Hacker News](http://ycombinator.com/news)\n>\n> The reaction was overwhelming.\n>\n> Some minutes later, I got an email from someone else I don't know, Victor, via github, saying \"Congratulations, you've just made it to the top of HN...\"\n>\n> It was a Friday evening. That's weekend I couldn't sleep. My mailbox was exploding too. It went viral over the \"interwebs\". Hundreds of comments, thousands of stars on the github repository, many forks, people offering me jobs, invites to present at conferences, you name it.\n>\n>  There were some follow-ups on Reddit, Quora, and some other forums, and it quickly became the top unpaid link on google when searching for a few terms combining weight-loss and machine-learning.\n>\n> In moments like this, one wishes they had two copies of oneself. I have another life too.\n>\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 21.3720703125,
          "content": "Discovering ketosis: _how to effectively lose weight_\n=====================================================\n\n### _Here is a chart of my weight vs. time in the past 16 months or so:_\n\n ![weight vs time in the past 16 months or so](weight.2015.png  \"weight loss progress\")\n\n\nThe chart was generated from a data-set [`weight.2015.csv`](weight.2015.csv) by the script [`date-weight.r`](date-weight.r) in this git repository.  It requires [`R`](http://r-project.org) and [`ggplot2`](http://ggplot2.org/).\n\n\nIn the following I'll describe the thought process, some other people ideas, and the code I used to separate signal from noise. This separation was critical to help lead me in the right direction.\n\nThis github repository includes my code, [a Q&A section](QandA.md), and links\nfor further reading.\n\n\n#### Disclaimers:\n\nThe below is what worked for me. Your situation may be different. Listen to your own body. The code here is designed to be used on your own data, not on mine.\n\nAlso: this was *not* a scientific experiment, or a \"study\"; rather, it was a personal journey of experimentation and discovery.\n\nWith these behind us, I'd like to channel [Galileo in the face of the inquisition](https://en.wikipedia.org/wiki/Galileo_affair): evolution has been hard at work for 2 billion years shaping the chemistry of all eukaryotes, multi-cellular life and eventually mammals. The Krebs cycle, glucose metabolism, insulin spikes, glycogen in the liver, carnitine, lipase, are as real for you as they are for me. We may be very different in our genes and traits, some are more insulin resistant, for example, but we cannot be too different in our most fundamental metabolic chemistry. The chemistry which drives fat synthesis and break-up.\n\n\n## Salient facts & initial observations\n\n- I used to be a pretty thin person. My 1st DMV card below, says 143 lb.\n- Unfortunately, since moving to the US, I've been gaining more and more weight. I peaked in 2015, over 50 lbs higher.\n- The US is a country where obesity is an epidemic.\n- Poorer demographics in the US have higher levels of obesity.\n\n![First DMV photo and weight (with full clothing)](1992-ariel-dmv.png \"143 pounds, sometime in the 90's\")\n\n\nDoes a US typical lifestyle has anything to do with this epidemic? After reading on the subject, I could point at a few of the main suspects:\n\n - Fast food is highly available, and is very cheap compared to most alternatives\n - Most food we buy and eat is heavily processed -- watch [Food, Inc. (documentary)](http://www.takepart.com/foodinc/film)\n - \"No Fat\" and \"Low Fat\" labels are everywhere on supermarket shelves\n - Many foods are enriched and sweetened with high-fructose corn-syrup -- watch [Sugar Coated (documentary)](http://sugarcoateddoc.com/)\n\nAs in many other instances, I realized I need to think for myself. Ignore all \"expert\" advice. Question widely accepted ideas like the FDA \"food pyramid\". Start listening to my own body, my own logic & data I can collect myself and trust.\n\nOnce I did, the results followed.\n\n## What didn't work\n\nIn the past, I tried several times to change my diet. After reading one of Atkins' books, I realized, checked, and accepted the fact that excess carbs are a major factor in gaining weight. But that realization alone has not led to success.\n\nMy will power, apparently, was insufficient. I had too much love of pizza and bread.  I would reduce my carb consumption, lose a few pounds (typically ~5 pounds), and then break-down, go back to consuming excess carbs, and gain all these pounds back, and then some. My longest diet stretch lasted just a few months.\n\nIt was obvious that something was missing in my method. I just had to find it.  I could increase my physical activity, say start training for a mini-marathon, but that's not something I felt comfortable with.\n\nI realized early on that I need to adopt a lifestyle that not just reduces carbs, or add exercise, but is also sustainable and even enjoyable so it can turn into a painless routine. Something that:\n\n> - I could do for years\n> - Never feel the urge to break habits\n> - Is not hard, or unpleasant for me to do\n\n\n## Early insights & eureka moments\n\nEarly in the process I figured I could use [machine learning](https://en.wikipedia.org/wiki/Machine_learning) to identify the factors that made me gain or lose weight. I used a simple method: every morning I would weigh myself, and record both the new weights and whatever I did in the past ~24 hours, not just the food I ate, but also whether I exercised, slept too little or too much, etc.\n\nThe file I kept was fairly simple. A CSV with 3 columns:\n\n> *Date*, *MorningWeight*, *Yesterday's lifestyle/food/actions*\n\nThe last column is a arbitrary-length list of *`word[:weight]`* items.\n\nThe (optional) numerical-weight following `:`, expresses higher/lower quantities. The default weight, when missing is 1:\n\n    #\n    # -- Comment lines (ignored)\n    #\n    Date,MorningWeight,YesterdayFactors\n    2012-06-10,185.0,\n    2012-06-11,182.6,salad sleep bacon cheese tea halfnhalf icecream\n    2012-06-12,181.0,sleep egg\n    2012-06-13,183.6,mottsfruitsnack:2 pizza:0.5 bread:0.5 date:3 dietsnapple splenda milk nosleep\n    2012-06-14,183.6,coffeecandy:2 egg mayo cheese:2 rice meat bread:0.5 peanut:0.4\n    2012-06-15,183.4,meat sugarlesscandy salad cherry:4 bread:0 dietsnapple:0.5 egg mayo oliveoil\n    2012-06-16,183.6,caprise bread grape:0.2 pasadena sugaryogurt dietsnapple:0.5 peanut:0.4 hotdog\n    2012-06-17,182.6,grape meat pistachio:5 peanut:5 cheese sorbet:5 orangejuice:2\n    # and so on ...\n\n\nThen I wrote [a script](lifestyle-csv2vw) to convert this file to [vowpal-wabbit](https://github.com/JohnLangford/vowpal_wabbit/wiki) training-set regression format. In the converted train-set the label (target feature) is the change in weight (delta) in the past 24 hours, and the input features are what I've done or ate in the ~24 hours leading to this delta -- a straight copy of the 3rd column.\n\nI was not dieting at that time. Just collecting data.\n\nThe machine learning process error-convergence after partly sorting the lines descending, by `abs(delta)` to smooth it out and try to amplify very weak signals from the data, and 4-passes over the data, looks like this:\n\n![error convergence (after partial descending sort by delta)](vw-convergence.png  \"loss convergence in 4 data passes\")\n\nYou can reproduce my work by compiling your own data-file, installing all prerequisites, and running `make` in this directory.  I wrote a [HOWTO file with more detailed instructions](HOWTO.md). Please open an issue, if anything doesn't work for you.\n\nWhen you type `make` in this directory -- some magic happens.\n\nHere's how a typical result looks like.\n\n    $ make\n\n    ... (output trimmed for brevity) ...\n\n    FeatureName       HashVal   ...   Weight RelScore\n    nosleep            143407   ...  +0.6654 90.29%\n    melon              234655   ...  +0.4636 62.91%\n    sugarlemonade      203375   ...  +0.3975 53.94%\n    trailmix           174671   ...  +0.3362 45.63%\n    bread              135055   ...  +0.3345 45.40%\n    caramelizedwalnut  148079   ...  +0.3316 44.99%\n    bun                  1791   ...  +0.3094 41.98%\n\n    ... (trimmed for brevity. Caveat: data is too noisy anyway) ...\n\n    stayhome           148879   ...  -0.2690 -36.50%\n    bacon               64431   ...  -0.2998 -40.69%\n    egg                197743   ...  -0.3221 -43.70%\n    parmesan             3119   ...  -0.3385 -45.94%\n    oliveoil           156831   ...  -0.3754 -50.95%\n    halfnhalf          171855   ...  -0.4673 -63.41%\n    sleep              127071   ...  -0.7369 -100.00%\n\nThe positive (top) relative-score values are life-style choices that make you ***gain weight***, while the negative ones (bottom) make you ***lose weight***.\n\n\n##### And here's a variable-importance chart made from a similar data-set:\n\n<a href=\"scores.png\" target=\"_blank\"><img src=\"scores.png\" width=\"900\"></a>\n\nDisclaimer: please don't read too much into the particulars of this data. Working with this particular data set, was pretty challenging, since:\n\n- The number of original data-points (a bit over 100 days) may be too small to establish enough significance.\n- Typical daily changes in body weight are very small, often ~0.1 lb.\n- My scales are not accurate: you may note that my data has 0.2 pound resolution. This is not ideal. Getting scales with 0.1 pound resolution is highly recommended.\n- You may also note that the loss-convergence chart hits a hard floor at ~0.2 even when you do multiple-passes over the data (overfit the training-set) for a similar reason.\n- Items that make you lose and gain weight, often appear together on the same line so they cancel each other. This throws the automatic learning process off-course.\n- There were some misspellings in the original data (I hope I fixed all of these by now)\n\nSo I focused mostly on the extremes (start and end) of the list as presented above, and just used the hints as general guidance for further study, experimentation, and action.\n\nDespite the noisy & insufficient data, and the inaccuracies in weighting, the machine-learning experiments made 4 facts pretty clear, pretty early:\n\n- Sleeping longer consistently appeared as *the* #1 factor in losing weight.\n- Lack of sleep did the opposite: too little sleep lead to weight gains.\n- Carbs made me gain weight. The worst were high-starch and sugary foods.\n- Fatty and oily foods tended to do the opposite: they were positively correlated with weight-loss.\n\nThe 'stayhome' lifestlye, which fell mostly on weekends, may have been a red-herring: I slept longer when I didn't have to commute to work, OTOH: my diet on stay-home days may have been different.\n\nIt took me a while to figure out the sleep part. *When we sleep we don't eat*. It is that simple.\n\nMoreover: we tend to binge and snack while not particularly hungry, but we never do it during sleep.\n\nOur sleeping time is our longest daily fasting time.\n\nPlease note that my explanations of the effects may not in fact be accurate or deeply scientific.\nThe goal of all this was incremental discovery: experiment, check effect, rinse, repeat.\n\n## Further progress\n\nYou may note that in the top (date vs. weight) chart there's a notable acceleration in the rate of weight-loss.  The cause was deeper insights and better ability to sustain the diet the more I understood the problem.\n\n***Extending the fasting time*** was one major accelerator of weight-loss rate. I did that by:\n\n> - Skipping breakfast and\n> - Stop eating earlier in the evening before going to bed.\n\nThis gave me 14-16 hours of fasting each day. Rather than the more typical 10-12 hours/day of fasting.\n\nThe 2nd accelerator was ***consuming fatty stuff*** (instead of carbs) in order to feel full.\n\nThe 3rd accelerator was understanding the concepts of [Glycemic index](https://en.wikipedia.org/wiki/Glycemic_index) and [***Glycemic Load***](https://en.wikipedia.org/wiki/Glycemic_load), and shifting whatever I chose to eat towards ***lower Glycemic loads***.\n\nI now believe and hope that I can go all the way back to my original weight when I first landed on US soil.\n\nIf I can keep the present rate, it should take 1-2 years to completely reverse the damage of the past ~20 years.\n\nIt is important to stress that I also *feel much better the more weight I lose*. As a welcome side-effect, the few borderline/high levels in my blood tests, have moved significantly towards normal averages, during the period I lost weight.\n\n### What was my data and clear improvement in health saying?\n\nLooking at my data, and reading more, convinced me that I should beware of doctors [who push statins](https://www.google.com/search?q=the+truth+about+statins) instead of suggesting a better diet. I started doubting anyone who told me I need to *reduce* fat. I run away if anyone now tells me \"high cholesterol\" in the diet is dangerous.\n\nCholesterol, by the way, is an essential building block for many essential body by-products. The liver produces as much cholesterol as we need.\n\nOur body is an amazing machine. Billions of years of evolution have made it extremely *adaptive*.\n\nIt is not our ***high fat consumption***, it is the ***storage of fat process*** that makes us acummulate fat in the tissues and become unhealthy.\n\nAn enzyme called *Lipase* breaks-up fat. Raise the levels of Lipase and our body fat gets consumed faster. To get there, we need to give the body fat as an *alternative* to carbohydrates.  When the body has depleted both the blood sugar, and the glycogen (hydrated sugar) buffer in the liver, it has no other choice but to *adapt and compensate*.  Our source of energy -- [ATP synthesis](https://en.wikipedia.org/wiki/Adenosine_triphosphate) -- switches from carbs to fats by producing more fat-breaking agents.  The body is a \"Flex Fuel\" kind of machine, that has simply replaced one fuel (carbs) with another (fat).\n\nWhen Lipase, and all other agents in the fat-to-ATP chemical path, aka [Beta oxidation](https://en.wikipedia.org/wiki/Beta_oxidation) mobilize, and their levels are elevated, we burn more fat and lose weight over time.\n\nIn a low-carb/high-fat (LCHF) regime, our night sleep (fasting time) becomes our friend.  The fat-breaking agents keep working while we sleep, breaking-up the stored fat.  This leads to weight-loss, and a healthier state.\n\nAnd when we push even further, and cut carbs to *really* low levels, we may reach a new steady state, called ketosis, in which practically all our energy comes from fat, and that's when we really win big in the weight-loss battle.\n\nThe above is a very simplified, and hopefuly easy to digest, version of what some diet books try to explain in hundreds of pages.\n\n## My bottom-line recipe:\n\n- The hardest part (especially at the beginning) is reducing carbs. The worst are starch rich foods (pizza, pasta, bread etc.), then processed foods with high sugar content (sweet sodas, no-pulp juices, etc). This doesn't mean ***no*** carbs. You may afford yourself carbs from time to time (say a pizza once a week). As it turns out, an occasional lapse isn't enough to completely reverse any steady-state.  However, you need to make sure you consume ***much less carbs*** and ***less frequently*** than before. In particular, you must avoid binging on snacks like chips, pizza, doughnuts, pasta, and bread, or drinking sugar-rich drinks.\n\n- [Look-up Glycemic index](https://en.wikipedia.org/wiki/Glycemic_index) and [Glycemic Load](https://en.wikipedia.org/wiki/Glycemic_load) on wikipedia. ***Avoid foods with high glycemic load***. This prevents the blood sugar spikes which lead to insulin spikes and tell the body chemical cycles to revert back from ketosis, or near ketosis, to fat-accumulation.  Have a sweet tooth? Eat an orange instead of drinking orange juice. The two have vastly different glycemic loads and this makes a huge difference. If you must add sweetness to your cup of tea or coffee, use a [Splenda (sucralose+dextrose) tablet](https://en.wikipedia.org/wiki/Splenda), or [a Stevia drop/tablet](https://en.wikipedia.org/wiki/Stevia) which typically weight just ~0.1 gram, rather than a tea-spoon of sugar (~4.2g, about 40x more). Result: similar sweetness effect, but much lower Glycemic load and resulting levels of blood-glucose.\n\n- High fat: I switched from milk to half-and-half and am considering heavy (and unsweetened) whipped cream. It has less carbs (lactose) and more fat; plus, it tastes better.  Eat avocados, olive oil, mayo, coconut oil, nuts.  I never worry about *natural* fat, I eat as much fat as I want. This is what makes it much easier to avoid carbs. When I stuff myself with fat I feel much less hungry and miss the carbs less. The body is very good at figuring this out: \"I have too much fat in the blood, so let's increase the amount of enzymes which break-up fat\" and this makes me lose weight in the long run.  Most importantly, I always ***avoid any products labeled \"low-fat\" or \"fat-free\"***. The food industry usually replaces fat with sugar, so it tastes better - otherwise it tastes awful. You'll often hear about \"bad\" vs \"good\" fat. My take: as long as it is natural, it is ok. The worst trans-fat is fat that's artificially hydrogenated, to increase shelf-life, by the food industry. The less saturated fat is, the better. Mono-saturated (plant) liquid oil is the best, then come the poly-unsaturated fats, and finally near saturated (but not fully saturated) fats that come from animals. My buttery-spread spectrum is:  *Margarine: no; Butter: ok; Earth Balance: no problem*. At any rate, even the most saturated fat, gets broken and depleted by the natural processes in the body.\n\n- A bit of exercise.  Of course, more is better, but for many this may prove difficult. I don't excercise too much. I just bike to work and back about 20 min each way, meaning 40 min/day, 5 out of 7 days/week. You can try walking the dog (but walk faster), or Zumba dance to music. The trick is to find something that you don't find hard to do. Or find company to do it together. Then, do a little bit of it every day.\n\n- ***Longer fasting periods:*** This is the #1 contributor to weight-loss. sleep longer, stop eating as early as possible before going to sleep and start eating as late as possible after sleeping. *Skip breakfast*, after some time you won't feel hungry in the morning anymore.  After long periods of fasting, the body chemistry adjusts. It needs ATP, but there's a too low level of glucose in the blood. The glycogen in the liver is fully consumed (this takes about 1-2 days of low or no carbs) so there's no other option, but to start looking for other sources, like stored fat. This elevates the enzymes that help with breaking up fat and the Krebs cycle reverses direction in the critical paths. Instead of transforming excess-carbs into stored fat, we break-up stored fat for energy.\n\n- Eat eggs.  They are a wonderful combo of fat and protein with no carbs at all.  I read an interview with a [Japanese woman who reached 114 years](Longevity.md) and one of her secrets was to eat eggs daily.  My favorite food is a scrambled egg with grilled onions (onions are a bit high on carbs, but too tasty to give up) and olives.\n\n- Eat slower, and chew longer... don't swallow just yet! Humans, just like dogs, tend to swallow too soon. Stop eating when you feel full. There's about 20 min delay before your brain registers that you are full so don't over-eat.\n\n***\n\n## Further reading:\n\n- [The Krebs (aka Citric acid) cycle](https://en.wikipedia.org/wiki/Citric_acid_cycle)\n- [Spikes of Insulin and their effects](https://en.wikipedia.org/wiki/Sugar_crash) -- what the body does when it has excess of sugar vs excess of fat.\n- [Glycemic Index](https://en.wikipedia.org/wiki/Glycemic_index)\n- [Glycemic Load](https://en.wikipedia.org/wiki/Glycemic_load) -- a better metric for weight-loss than Glycemic Index.\n- [Glycogen and its storage in the liver](https://en.wikipedia.org/wiki/Glycogen)\n- [Ketone bodies](https://en.wikipedia.org/wiki/Ketone_bodies)\n- [Ketosis -- not to be confused with keto-acidosis](https://en.wikipedia.org/wiki/Ketosis)\n- [Ketogenic diet](https://en.wikipedia.org/wiki/Ketogenic_diet)\n\n\n<!--\n- [The Eating Academy / Peter Attia, M.D.](http://eatingacademy.com/)\n-->\n\n- [Why We Get Fat: And What to Do About It / Gary Taubes](http://www.amazon.com/gp/product/0307272702)\n- [Summary of Good Calories, Bad Calories / Gary Taub by Lower Thought](https://lowerthought.wordpress.com/complete-notes-to-good-calories-bad-calories/)\n- [The Obesity Code: Unlocking the Secrets of Weight Loss / Jason Fung](https://www.amazon.com/Obesity-Code-Unlocking-Secrets-Weight-ebook/dp/B01C6D0LCK/)\n- [The best summary about statins I've seen](http://www.newswithviews.com/Howenstine/james23.htm)\n- [High cholesterol doesn't cause heart disease](http://www.telegraph.co.uk/science/2016/06/12/high-cholesterol-does-not-cause-heart-disease-new-research-finds/)\n- [Dr. Mark Hyman take on a good diet (a bit different than mine)](http://drhyman.com/blog/2014/08/18/one-test-doctor-isnt-save-life/)\n\n#### Documentaries:\n\n- [Food, Inc. (2008)](https://www.netflix.com/title/70108783)\n-  [Sugar Coated (2015)](https://www.netflix.com/title/80100595)\n\n#### More videos\n\n- [Reversing Type 2 diabetes starts with ignoring the guidelines | Sarah Hallberg | TEDxPurdueU](https://www.youtube.com/watch?v=da1vvigy5tQ)\n\nA nice 7:41 minute video of James McCarter in Quantified Self (an eye opener for me):\n\n- [James McCarter: The Effects of a Year in Ketosis](https://vimeo.com/147795263)\n\n#### Questions, Answers, Comments\n\n[Some questions and comments I got and tried to answer](QandA.md)\n\n<!--\n#### More friendly interface\n\n[Shyal Beardsley](http://shyal.com) has built a starter front-end for this: ***[weightbrains.com](http://weightbrains.com)***\n(Note and fair warning: this is a prototype, experimental, work in progress)\n-->\n\n## Acknowledgements\n\nBig thanks to the following people for contributing to this project in myriad ways,\ncomments, references, corrections, etc.\n\n_Anat Faigon, Ingrid Kane, Hans Lee, Steve Malmskog, Eyal Friedman, Shiri Shoham, Gabi Harel, Shingi, Noa_\n\n_Update: 2016-08-12: this project made [Hacker News](https://news.ycombinator.com/item?id=12279415) and reached the top place for a while. Thanks for some great comments by benkuhn, aab0, zzleeper, and others which helped me make it better._\n![image of this project on Hacker News 2016-08-12](hackernews-2016-08-12.png)\n\nSpecial thanks to John Langford and the many other contributors to [vowpal wabbit](https://en.wikipedia.org/wiki/Vowpal_Wabbit).\n\n\n#### License:\n\nThis code and additional material are released under a permissive and simple [2-clause BSD licence](Licence.md).  The one sentence summary of this is \"as long as you don't sue me and not claim it as your own, you should be ok.\"\n\n"
        },
        {
          "name": "ariel.csv",
          "type": "blob",
          "size": 9.880859375,
          "content": "#\n# vim: textwidth=0 nowrap\n# Diet data file: 2012-06 and on\n#\n# 'sleep' (one unit) means _at least_ 8 hours of sleep.\n#\n# Don't add 'sleep' unless you slept more than 8 hours.\n# If you slept less than 8 hours, use 'nosleep' instead.\n#\n# To emphasize any factor, i.e. make it more important/pronounced,\n# in the importance order you need to _decrease_ its weight.\n#\n# This is due to the \"less quantity drives more effect\" principle\n# when calculating _importance_ (effect on the target feature).\n#\n# e.g, if you avoid carbs, and one day you eat just one\n# slice of bread, say 0.5 of a normal portion, you could do:\n#\n#       bread:0.5\n#\n# Which (perhaps unintuitively) means double, the importance\n# of the eating bread effect, rather than halve it.\n#\n# IOW: if you eat \"more of X\", then every unit/gram of X\n# has relatively less effect given the weight-gain is the same.\n#\n# The default factor weight (when missing) is 1.0\n#\nDate,MorningWeight,YesterdayFactors\n2012-06-01,186.8,\n2012-06-10,185.9,\n2012-06-11,182.6,salad sleep:0.15 cheese egg halfnhalf:1.5\n2012-06-12,181.0,sleep:0.5 tea grape\n2012-06-13,183.6,bagel bread date:1.5 dietsnapple splenda pizza nosleep:0.2\n2012-06-14,183.7,coffeecandy cheese rice salmon peanut bread\n2012-06-15,183.4,salmon salad cherry:0.7 dietsnapple tea potato oliveoil cheese peanut\n2012-06-16,183.8,bread avocado:10 sugaryogurt dietsnapple peanut ketchup\n2012-06-17,182.6,salmon pistachio peanut cheese\n2012-06-18,182.4,bread:0 peanut bagel cheese cherry soup\n2012-06-19,182.4,cherry knish cheese salmon pistachio:0.7 peanut\n2012-06-20,183.0,sorbet granolabar peanut:8 pistachio snapple sugaryogurt\n2012-06-21,182.6,cheese salad peanut beans:0.2 soup dietsnapple salmon carrot artichokepie\n2012-06-22,182.0,cheese salad peanut:0.5 meatball tea crab cherry pistachio atkinsbar egg pizza\n2012-06-23,182.8,bread sorbet coffeecandy peanut:0.5 snapple\n2012-06-24,182.4,peanut:0.5 sugaryogurt egg pizza cheese\n2012-06-25,182.2,sugaryogurt peanut:0.5 pistachio:0.5 tea salad egg pizza\n2012-06-26,182.4,snapple rice salad meat bread peanut:0.5 pistachio:0.5 peanuts:0.4 ketchup\n2012-06-27,183.0,indian mango rice snapple danish pistachio:0.7 cheese atkinsbar peanut:0.5 chocolate\n2012-06-28,183.0,egg splenda salmon beans salad dietsnapple rice atkinsbar nectarine:0.7 peanut\n2012-06-29,183.7,egg pizza splenda sugaryogurt peanut:0.5 bagel:0.5 coffeecandy breadedchicken trailmix ketchup\n# Break, vacation in NYC, didn't keep track\n2012-07-08,181.8,\n2012-07-09,182.2,bun pistachio peanut:0.7 ketchup quinoa peas pizza egg splenda nosleep:0.5 melon bread:0.4 watermelon:0.7\n2012-07-10,181.2,carrot chicken salad dietsnapple:1 snapple plum mottsfruitsnack cheese sugaryogurt tea oliveoil\n2012-07-11,181.8,bagel:0.7 dietsnapple icecreamsandwich pistachio pizza splenda cocoa\n2012-07-12,180.8,chicken milk beans salad dietsnapple pizza cocoa splenda pistachio peanut cheese tea oliveoil potato\n2012-07-13,180.8,pizza cocoa splenda peanut cheese coffeecandy chicken olives brownie meatball sorbet stayhome\n2012-07-14,181.4,bread dietsnapple pizza cocoa splenda cheese nectarine brownie\n2012-07-15,182.4,nosleep:0.5 caramelizedwalnut friedrice sorbet:0.5 peanut pistachio meatball\n2012-07-16,182.2,frenchtoast sugaryogurt peanut:0.7 egg brownie pistachio coffeecandy:0.7\n2012-07-17,182.0,milk chicken salad tea potato oliveoil grape pistachio peanut pizza cocoa splenda mottsfruitsnack icecream:0.3\n2012-07-18,181.8,gyoza tempura sushi chicken dietsnapple pizza cocoa splenda soup\n2012-07-19,181.0,hotdog potato beans salad coleslaw dietsnapple pizza cocoa splenda icecream peanut atkinsbar tea oliveoil parmesan grape\n2012-07-20,181.2,breadedchicken veggieburger salad thaisoup sorbet:.2 pizza cocoa splenda dietsnapple indian danish beans melon grape trailmix peanut stayhome\n2012-07-21,182.0,caramelizedwalnut peanut soup avocado cheeseburger pizza cocoa splenda beet chocolate coffeecandy pistachio:0.5\n2012-07-22,181.4,salad icecream tea oliveoil soup peanut pizza chocolate coconut\n2012-07-23,180.8,egg pizza splenda sugaryogurt watermelon atkinsbar peanut pistachio cheese potato fajita\n2012-07-24,180.8,salad chicken butter bread:0.5 pizza cocoa dietsnapple splenda peanut sugaryogurt meatball cheese fajita sorbet:0.2\n2012-07-25,180.6,milk chicken salad cheese pizza cocoa splenda dietsnapple sugaryogurt beans peanut mottsfruitsnack\n2012-07-26,180.6,chicken bbqsauce milk coleslaw tea pistachio pizza cocoa splenda salad gummybears coffeecandy:0.5 brownie:.4 dietsnapple\n2012-07-27,181.0,meat bread grilledonion brownie coffeecandy sorbet cheese\n2012-07-28,181.0,peanut granola cookie salad soup splenda cocoa\n2012-07-29,181.4,salad bacon bread croissant icecreamsandwich sorbet breadedchicken bbqsauce cheese whitenectarine peanut pizza frappuccino splenda\n2012-07-30,181.2,pizza splenda whitenectarine tea pistachio peanut sugaryogurt\n2012-07-31,181.4,breadedchicken milk salad peach plum ketchup peanut pistachio cornbread\n2012-08-01,181.6,bagel peanut pizza splenda\n2012-08-02,181.2,chicken milk salad cheese pizza splenda cocoa\n2012-08-03,180.8,pizza splenda peanut sorbet stayhome\n2012-08-04,181.8,bagel beans cheeseburger whitenectarine cheese caramelizedwalnut peanut pistachio tofee coffeecandy pizza cocoa splenda chicken carrot soup pasta dietsnapple bread\n2012-08-05,181.4,pizza meatball salad coffeecandy tea oliveoil grape peanut cheese brownie pizza parmesan potato\n2012-08-06,182.0,coconut chocolate nosleep frappuccino:0.2 pizza peanut pistachio coconutbar\n2012-08-07,183.4,indian danish breadedchicken rice brownie pizza cocoa splenda dietcoke:0.3 pistachio:0.5 peanut:0.7 coffeecandy tofee:0.7 chocolate coconut driedapple\n2012-08-08,183.6,pizza cocoa splenda dietsnapple dietcoke icecream frappuccino:0.2 peanut tofee nectarine salad\n2012-08-09,182.4,pizza cocoa splenda dietsnapple salad tea potato milk tilapia peanut atkinsbar cheese sleep:0.3 exercise:0.45 strawberry\n2012-08-10,183.0,pizza frappuccino:0.2 peanuts tofee:0.7 kettlecorn\n2012-08-11,183.6,breadedshrimp beef thinkthinbar dietsnapple dietcoke\n2012-08-12,183.8,brownrice pizza shrimp whitenectarine cheese atkinsbar breadedchicken chocolate coconut peanut avocado sugaryogurt \n2012-08-13,184.2,schnitzel bagel pizza cocoa splenda dietsnapple salad icecream sugaryogurt peanut\n2012-08-14,183.4,bagel salad cocoa pizza dietsnapple splenda sugaryogurt cheese oliveoil\n2012-08-15,182.4,pizza cocoa bread cheese oliveoil frappuccino:0.2 stayhome sorbet dietsnapple sleep:0.5\n2012-08-17,181.4,sleep stayhome peanut pizza egg whitenectarine smallveggiewraps:0.7\n2012-08-18,181.4,bagel beans peanut dietsnapple salad\n2012-08-19,181.0,sleep stayhome watermelon melon meatball salad whitenectarine pizza\n2012-08-20,181.4,\n2012-08-21,182.0,sushi breadedchicken pizza cocoa dietsnapple pistachio peanut brownie atkinsbar cheese\n2012-08-22,182.0,salad cheese nuggets dietsnapple pistachio pizza frappuccino nectarine ham eggpie sausage\n2012-08-23,181.6,falafel:7 pita tea pizza frappuccino:0.2 pistachio nectarine sugaryogurt stayhome\n2012-08-24,182.0,bread dietsnapple pizza cocoa splenda kettlecorn whitenectarine\n2012-08-25,182.6,meat frenchfries ketchup sorbet peanut salad coffeecandy kettlecorn mottsfruitsnack\n2012-08-26,183.0,frappuccino pizza bagel:0.8 kettlecorn coffeecandy cheese beef\n2012-08-27,182.0,stayhome sleep:0.7 peanut salad whitenectarine pizza frappuccino\n2012-08-28,182.4,nosleep couscous pita chicken salad pizza splenda cocoa dietsnapple\n2012-08-29,181.4,sleep nectarine pizza cocoa cheeseburger cheese peanut\n2012-08-30,180.8,nectarine pizza cocoa milk salad greenbeans chocolate rizo sleep:0.7\n2012-08-31,180.0,sleep:0.5\n2012-09-01,180.9,bread:0.7 pizza cocoa splenda peanut sugaryogurt dietsnapple nectarine cheese coffeecandy:6\n2012-09-02,180.0,sleep stayhome cheese pizza tea grape icecream\n2012-09-03,181.2,peanut bun melon cheese sugarlemonade sugaryogurt pasta\n2012-09-04,,\n2012-09-05,,\n2012-09-05,,\n2012-09-07,180.0,stayhome\n2012-09-08,181.0,bread:2 peanut pizza cheese\n2012-09-09,180.2,sleep stayhome salad bacon pizza frappuccino nuggets cheese\n2012-09-10,,\n2012-09-11,181.2,\n2012-09-12,180.8,\n2012-09-13,180.8,\n2012-09-14,180.4,stayhome salad pistachio pizza egg clementine coffeecandy\n2012-09-15,181.4,falafel giro clementine:0.5 pistachio dietsnapple pizza cocoa frenchfries breadedshrimp icecream coffeecandy\n2012-09-16,181.4,\n2012-09-17,182.0,salad gefiltefish chicken wine beet kettlecorn bread\n2012-09-18,182.2,kettlecorn cheese wine chicken bread\n2012-09-19,182.2,sushi gyoza sweetjapanese cheese wine chicken dietsnapple\n2012-09-20,182.0,indian turmeric rice chicken fried spinach wine dietsnapple pomegranate sugaryogurt\n2012-09-21,181.4,stayhome musaka cheese coffeecandy clementine pizza egg pistachio nectarine:0.5 date\n2012-09-22,182.8,rogalach:0.7 nathans:0.7 hummusroll bread salad tomatosoup cocoa pizza splenda dietsnapple\n2012-09-23,183.4,bread apple hummus pita danish\n2012-09-24,182.4,chicken olives wine peanut sugaryogurt fish stayhome\n2012-09-25,181.0,sleep falafel chicken milk salad olives dietsnapple mottsfruitsnack\n2012-09-26,183.6,mottsfruitsnack:2 bagel bread nosleep:0.5\n2012-09-27,182.5,sleep tea cocoa pizza splenda dietsnapple\n2012-09-28,182.4,salad\n2012-09-29,181.6,sleep grape cheese wine salad thinpretzel\n2012-09-30,183,nosleep bread clementine quesadilla corn\n2012-10-01,182.2,salad tea peanut chocolate stayhome pizza splenda potato wine\n2012-10-02,183.0,wine cookie:0.5 pita hummus peanut bread beef\n2012-10-03,,\n2012-10-04,,\n2012-10-05,,\n2012-10-06,182.4,\n2012-10-07,181.8,sleep wine cheese\n2012-10-08,,nectarine cheese pistachio coffeecandy splenda pizza frappuccino sugaryogurt thinpretzel clementine creamcheese\n2012-10-09,183.8,bread bagel\n2012-10-10,183.2,\n2012-10-11,183.6,nosleep rice pistachio peanut greenbeans bread cheese\n2012-10-12,,\n2012-10-13,183.6,\n2012-10-14,184.2,nosleep bread beef\n2012-10-15,182.6,salad sleep bacon egg splenda icecream\n2012-10-16,184.7,bread bagel beef\n2012-10-17,183.5,tea egg oliveoil parmesan:2\n2012-10-18,182.2,earthbalance:0.7 tea havarti oliveoil bacon parmesan\n2012-10-19,,\n2012-10-20,,\n\n"
        },
        {
          "name": "cgi",
          "type": "tree",
          "content": null
        },
        {
          "name": "date-weight.r",
          "type": "blob",
          "size": 3.1630859375,
          "content": "#!/usr/bin/Rscript --vanilla\n#\n# Generate date vs. weight chart\n#\neprintf <- function(...) cat(sprintf(...), sep='', file=stderr())\n\nlibrary(ggplot2)\nlibrary(scales)     # for date_breaks()\n\nMaxMonths=20\nMaxDays=ceiling(MaxMonths*30.4375)\n\n# --- styles\nratio = 1.61803398875\nW = 6\nH = W / ratio\nDPI = 200\nFONTSIZE = 8\nMyGray = 'grey50'\n\ntitle.theme   = element_text(family=\"FreeSans\", face=\"bold.italic\",\n                            size=FONTSIZE)\nx.title.theme = element_text(family=\"FreeSans\", face=\"bold.italic\",\n                            size=FONTSIZE-1, vjust=-0.1)\ny.title.theme = element_text(family=\"FreeSans\", face=\"bold.italic\",\n                           size=FONTSIZE-1, angle=90, vjust=0.2)\nx.axis.theme  = element_text(family=\"FreeSans\", face=\"bold\",\n                            size=FONTSIZE-2, colour=MyGray)\ny.axis.theme  = element_text(family=\"FreeSans\", face=\"bold\",\n                            size=FONTSIZE-2, colour=MyGray)\nlegend.theme  = element_text(family=\"FreeSans\", face=\"bold.italic\",\n                            size=FONTSIZE-1, colour=\"black\")\n\n\nParams <- list()\nprocess.args <- function() {\n    argv <- commandArgs(trailingOnly = TRUE)\n    fileArgs <- c()\n    for (arg in argv) {\n        # Arguments can be either:\n        #       Params:   name=value\n        # or:\n        #       Files:    file arguments\n        # eprintf(\"arg: %s\\n\", arg)\n        var.val <- unlist(strsplit(arg, '='))\n        if (length(var.val) == 2) {\n            var <- var.val[1]\n            val <- var.val[2]\n            Params[[var]] <<- val\n\t    # eprintf('Params$%s=%s\\n', var, val)\n        } else {\n            fileArgs <- c(fileArgs, arg)\n        }\n    }\n    # for (n in names(Params)) {\n    #   eprintf(\"Params[[%s]]: %s\\n\", n, Params[[n]]);\n    # }\n    # Params are assigned to global array Params[]\n    # rest are returned as files\n    fileArgs\n}\n\n# --- main\nFileArgs <- process.args()\nCsvFile <- ifelse(\n                length(FileArgs) > 0 && nchar(FileArgs[1]) > 0,\n                FileArgs[1],\n                'weight.2015.csv'\n)\nPngFile <- ifelse(\n                length(FileArgs) > 1 && nchar(FileArgs[2]) > 0,\n                FileArgs[2],\n                gsub(CsvFile, pattern='.[tc]sv', replacement='.png')\n)\n\nTitle <- ifelse(length(Params$title),\n                Params$title,\n                'weight by date'\n)\n\nXlab <- Params$xlab\nYlab <- ifelse(length(Params$ylab),\n\t\t\tParams$ylab,\n\t\t\t'Lb'\n)\n\nd <- read.csv(CsvFile, h=T, colClasses=c('character', 'numeric'))\n\n# Trim data to MaxDays\nN <- nrow(d)\nif (N > MaxDays) {\n    d <- d[(N-MaxDays):N, ]\n}\n\ng <- ggplot(data=d, aes(x=as.POSIXct(Date), y=Pounds)) +\n        scale_y_continuous(breaks=150:195) +\n        scale_x_datetime(breaks = date_breaks(\"2 months\"),\n                         labels = date_format(\"%Y\\n%b\")) +\n        geom_line(aes(y=Pounds), size=0.3, col='#0077ff') +\n        geom_point(aes(y=Pounds), pch=20, size=0.8) +\n        ggtitle(Title) +\n        ylab(Ylab) + xlab(Xlab) +\n        theme(\n            plot.title=title.theme,\n            axis.title.y=y.title.theme,\n            axis.title.x=x.title.theme,\n            axis.text.x=x.axis.theme,\n            axis.text.y=y.axis.theme\n        )\n\nggsave(g, file=PngFile, width=W, height=H, dpi=DPI)\n\n"
        },
        {
          "name": "hackernews-2016-08-12.png",
          "type": "blob",
          "size": 57.8642578125,
          "content": null
        },
        {
          "name": "lifestyle-csv2vw",
          "type": "blob",
          "size": 2.1708984375,
          "content": "#!/usr/bin/perl -w\n#\n# Generate a VW training-set from our raw data CSV\n#\nuse Scalar::Util qw(looks_like_number);\n\nmy $SepPat = qr{(?:\\s*,\\s*|\\s+)};\nmy $Interactive = -t STDOUT;\n\n# Default days-window\nmy $NDays = 1;\n\nsub process_args() {\n    for my $arg (@ARGV) {\n        if (! -f $arg and $arg =~ /^\\d$/) {\n            $NDays = $arg;\n        } else {\n            push(@files, $arg);\n        }\n    }\n    @ARGV = @files;\n}\n\nsub process_input() {\n    my $prev_weight;\n    my @daily = ();\n\n    while (<>) {\n        # Skip comments or header-line\n        next if (/^[#A-Za-z]/);\n\n        chomp;\n        # Windows line endings, just in case...\n        tr/\\015//d;\n\n        my ($date, $weight, @factors) = split($SepPat);\n\n        next unless ((defined $date) and $date =~ /^\\d/);\n\n        # Only generate a training set if everything is defined and\n        # we have a prior day weight to compare to\n        unless ((defined $weight) and looks_like_number($weight)) {\n            $weight = '' unless (defined $weight);\n            if ($Interactive) {\n                warn \"$ARGV:$. weight: '$weight' is not a number - line ignored\\n\";\n            }\n            undef $prev_weight;\n            next;\n        }\n\n        \n        #\n        # -- collect daily (gain + factors) data points in @daily\n        #\n        if ((defined $prev_weight) && scalar(@factors) > 0) {\n            my $gain = $weight - $prev_weight;\n            my @day_list = ($gain, @factors);\n            push(@daily, \\@day_list);\n        }\n        $prev_weight = $weight;\n    }\n\n    #\n    # Output vw training-set\n    #\n    for (my $i = 0; $i < @daily; $i++) {\n        my $start = $i;\n        my $end = $start + $NDays - 1;\n        my $sum_gain = 0.0;\n        my @sum_factors = ();\n        for (my $j = $i; $j <= $end; $j++) {\n            #\n            # -- Aggregate consecutive days gains and factors - up to $NDays\n            #\n            my $day_list = $daily[$i];\n            my @gain_factors = @$day_list;\n            my ($gain, @factors) = @gain_factors;\n            $sum_gain += $gain;\n            push(@sum_factors, @factors);\n            printf \"%.2f | @sum_factors\\n\", $sum_gain;\n        }\n    }\n}\n\n#\n# -- main\n#\nprocess_args();\nprocess_input();\n\n"
        },
        {
          "name": "score-chart.r",
          "type": "blob",
          "size": 3.8154296875,
          "content": "#!/usr/bin/Rscript --vanilla\n#\n# Generate weight gain/loss factor chart\n#\nlibrary(ggplot2)\n\neprintf <- function(...) cat(sprintf(...), sep='', file=stderr())\n\n# --- styles\nratio = 1.61803398875\nW = 10\nH = W * ratio\nDPI = 200\nFONTSIZE = 8\nMyGray = 'grey50'\n\n# --- Favorite fonts\nFamily='FreeSans'\nFace='bold.italic'\n\ntitle.theme   = element_text(family=Family, face=Face,\n                            size=FONTSIZE)\nx.title.theme = element_text(family=Family, face=Face,\n                            size=FONTSIZE-1, vjust=-0.1)\ny.title.theme = element_text(family=Family, face=Face,\n                           size=FONTSIZE-1, angle=90, vjust=0.2)\nx.axis.theme  = element_text(family=Family, face=\"bold\",\n                            size=FONTSIZE-2, colour=MyGray)\ny.axis.theme  = element_text(family=Family, face=\"bold\",\n                            size=FONTSIZE-2, colour=MyGray)\nlegend.theme  = element_text(family=Family, face=Face,\n                            size=FONTSIZE-1, colour=\"black\")\n\nParams <- list()\nprocess.args <- function() {\n    argv <- commandArgs(trailingOnly = TRUE)\n    fileArgs <- c()\n    for (arg in argv) {\n        # Arguments can be either:\n        #       Params:   name=value\n        # or:\n        #       Files:    file arguments\n        # eprintf(\"arg: %s\\n\", arg)\n        var.val <- unlist(strsplit(arg, '='))\n        if (length(var.val) == 2) {\n            var <- var.val[1]\n            val <- var.val[2]\n            Params[[var]] <<- val\n\t    # eprintf('Params$%s=%s\\n', var, val)\n        } else {\n            fileArgs <- c(fileArgs, arg)\n        }\n    }\n    # for (n in names(Params)) {\n    #   eprintf(\"Params[[%s]]: %s\\n\", n, Params[[n]]);\n    # }\n    # Params are assigned to global array Params[]\n    # rest are returned as files\n    fileArgs\n}\n\n# --- main\nFileArgs <- process.args()\nCsvFile <- ifelse(\n    length(FileArgs) > 0 && nchar(FileArgs[1]) > 0,\n    FileArgs[1],\n    'scores.csv'\n)\nPngFile <- ifelse(\n    length(FileArgs) > 1 && nchar(FileArgs[2]) > 0,\n    FileArgs[2],\n    gsub(CsvFile, pattern='.[tc]sv', replacement='.png')\n)\n\nTitle <- ifelse(length(Params$title),\n    Params$title,\n   'Relative weight-loss factor importance\\n(negative/green means causing weight-loss\\npositive/red means causing weight-gain)'\n)\n\n# -- Color weight-gains in red and weigh-losses in green for effect\n#    (this is one uncommon case where a 'positive' quantity is\n#     actually undesired/negative)\nMyGreen = '#00cc00'\nMyRed = '#ff0000'\n\nd <- read.csv(CsvFile, h=T, sep=',', colClasses=c('character', 'numeric'))\n\nN <- nrow(d)\nCrossIdx = which.min(abs(d$RelScore))\n\nd <- transform(d,\n    FeatureNo = 1:N,\n    TextOffset = (ifelse(d$RelScore > 0, -2, +2)),\n    TextJust = d$RelScore > 0,\n    FillColor = (ifelse(d$RelScore > 0, MyRed, MyGreen)),\n    FeatureLabels = sprintf(\"%s (%.1f%%)\", d$FeatureName, d$RelScore)\n)\n\n\ng <- ggplot(\n        data=d,\n        aes(\n            x=FeatureNo,\n            y=RelScore\n        ),\n        xlim(-100, 100)\n    ) +\n    geom_bar(\n        stat='identity',\n        position='identity',\n        width=0.8,\n        fill=d$FillColor,\n    ) +\n    geom_text(label=d$FeatureLabels,\n                y=d$TextOffset, x=d$FeatureNo,\n                size=2.0, angle=0, hjust=d$TextJust) +\n    ggtitle(Title) +\n    ylab('Relative Importance (%pct)') +\n    xlab(NULL) +\n    annotate(\"text\", x=CrossIdx+20, y=+35, label='Weight\\nGain',\n                angle=0, colour=MyRed, size=9,\n                family=Family, fontface=Face) +\n    annotate(\"text\", x=CrossIdx-20, y=-35, label='Weight\\nLoss',\n                angle=0, colour=MyGreen, size=9,\n                family=Family, fontface=Face) +\n    coord_flip() +\n    theme(\n        plot.title=title.theme,\n        axis.title.y=y.title.theme,\n        axis.title.x=x.axis.theme,\n        axis.text.x=x.axis.theme,\n        axis.text.y=element_blank()\n    )\n\nggsave(g, file=PngFile, width=W, height=H, dpi=DPI)\n\n"
        },
        {
          "name": "scores.png",
          "type": "blob",
          "size": 396.37109375,
          "content": null
        },
        {
          "name": "sort-by-abs",
          "type": "blob",
          "size": 0.376953125,
          "content": "#!/usr/bin/perl -w\n# vim: sw=4 ts=4\n#\n\n#\n# Convert every line to a pair: absolute-value, as-is-line\n#\nmy @LinesInMem = ();\n\nwhile (<>) {\n    my $first_field = (split(' ', $_))[0];\n    push(@LinesInMem, [abs($first_field), $_]);\n}\n\n#\n# In the end print original lines in descending order of the\n# absolute value\n#\nprint map $_->[1], sort {\n        $b->[0] <=> $a->[0] \n} @LinesInMem;  \n\n"
        },
        {
          "name": "train-to-items",
          "type": "blob",
          "size": 0.6552734375,
          "content": "#!/usr/bin/perl -w\n# vim: ts=4 sw=4 expandtab\n#\n# Convert a regular train file (one day, many-items, per line)\n# to a one line per-item train file so we can easily calculate\n# confidence intervals for each element separately by predicting.\n#\nmy %Items = ();\n\n# Collect items\nwhile (<>) {\n    # remove label, leave items (input features) only\n    s/^.*\\|\\s*//;\n    # Loop on all items\n    while (/(\\S+)/g) {\n        my $item = $1;\n        # Remove (optional) weights if any\n        $item =~ s/:.*$//;\n        $Items{$item} = 1;\n    }\n}\n\n#\n# Print items, one per-line (test-file for prediction)\n#\nfor my $item (sort keys %Items) {\n    printf \" '%s| %s\\n\", $item, $item;\n}\n\n"
        },
        {
          "name": "username",
          "type": "blob",
          "size": 0.1123046875,
          "content": "#!/usr/bin/perl\n\n$user = $ENV{'USER'} || $ENV{'LOGNAME'} || getlogin || (getpwuid($>))[0];\n\nprintf \"%s\\n\", $user;\n\n"
        },
        {
          "name": "vw-convergence.png",
          "type": "blob",
          "size": 74.322265625,
          "content": null
        },
        {
          "name": "vw-varinfo2",
          "type": "blob",
          "size": 12.845703125,
          "content": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n# vim: ts=4 sw=4 expandtab\n\"\"\"\n    vw-varinfo2: vw dataset summary & variable importance\n\n    This is new & simpler implementation of the original vw-varinfo\n\n    It is designed to be simpler and faster. There's less dependence\n    on command line options so it is much more robust against future\n    changes and new options in vowpal-wabbit.\n\n    This implementation is in python (original was in perl)\n\n    TODO: multi-class support is not implemented!\n\n    Author: Ariel Faigon (2016)\n\"\"\"\nimport sys\nimport os\nimport subprocess\nimport re\nimport itertools\nimport tempfile\nimport traceback\n\nfrom operator import itemgetter\n\nVerbose = False\nARGV0 = os.path.basename(sys.argv[0])\n\n# Default vw executable program to call\nVW = 'vw'\n# Additional VW args which should be reused from 1st to 2nd pass\nVWARGS = []\n\n# Hash mappings for per-feature (min, max, hash-value, weight)\nF_MIN = {}\nF_MAX = {}\nF_HASH = {}\nF_WEIGHT = {}\n\n# We need to have a model at the end to load all weights\n# If it is not supplied on the command line, we add it ourselves\nModelName = ''\nCleanupModel = False\n\n# A global switch and list of all seen labels to support MultiClass\nMultiClass = False\nMCLabels = None\n\ndef v(msg):\n    \"\"\"print message to stderr\"\"\"\n    sys.stderr.write(\"%s\\n\" % msg)\n    sys.stderr.flush()\n\ndef d(msg):\n    \"\"\"Verbose/debugging message, activated with '-v' option.\"\"\"\n    if not Verbose:\n        return\n    v(msg)\n\ndef fatal(msg):\n    \"\"\"fatal (can't continue) situation error message\"\"\"\n    v(\"== FATAL: %s\" % msg)\n    sys.exit(1)\n\ndef usage(msg):\n    \"\"\"Print usage message and exit\"\"\"\n    if msg:\n        v(msg)\n    v(\"Usage: %s [-v] [<vw>] [<vw_options>] <vw_args>...\" % ARGV0)\n    v(\"    Notes:\\n\"\n      \"\\t- You may omit the <vw> argument (default is 'vw')\\n\"\n      \"\\t- You may use a different <vw> executable as the 1st arg\\n\"\n      \"\\t- <vw_args> are all the vw arguments, as you would call vw directly\\n\"\n      \"\\t- If <vw_args> is just a dataset-file - vw defaults will be used\\n\"\n      \"\\t- To lose the constant (intercept), use vw's '--noconstant' option\\n\"\n      \"\\t  However the constant may be useful to show if there's a bias\")\n    sys.exit(1)\n\n\ndef which(program):\n    \"\"\"\n    Find a program in $PATH\n    If found, return its full path, otherwise return None\n    \"\"\"\n    def is_exe(fpath):\n        \"\"\"Return True if fpath is executable, False otherwise\"\"\"\n        return os.path.isfile(fpath) and os.access(fpath, os.X_OK)\n\n    fpath, _ = os.path.split(program)\n    if fpath:\n        if is_exe(program):\n            return program\n    else:\n        for path in os.environ[\"PATH\"].split(os.pathsep):\n            path = path.strip('\"')\n            exe_file = os.path.join(path, program)\n            if is_exe(exe_file):\n                return exe_file\n\n    return None\n\n\ndef all_features_dicts():\n    \"\"\"\n    Returns two dict of all features in a structured way:\n    1st dict is individual features: scalar keys with a value of 1\n    2nd dict is for features within name-spaces, key is the name-space\n    first dict is:\n    {\n        # individual (not in a name-space) features:\n        \"f1\": 1,\n        \"f2\": 1,\n        \"fN\": 1\n    }\n    second dict is:\n    {\n        # features in name-spaces:\n        \"namespace1\": { \"f1\":1, \"f2\":1, ... },\n        \"namespace2\": {\"f1\":1, \"f2\":1, ... },\n    }\n    \"\"\"\n    d1 = {}\n    d2 = {}\n    for k in F_HASH:\n        if '^' in k:\n            ns, fname = k.split('^', 1)\n            if ns not in d2:\n                d2[ns] = {}\n            d2[ns][fname] = 1\n        else:\n            # Constant feature should never be added as a regular\n            # feature. vw adds it by itself as needed.\n            # TODO: multiclass uses separate Constant_<N> per class\n            if k != 'Constant':\n                d1[k] = 1\n\n    return d1, d2\n\n\ndef all_features_example():\n    \"\"\"Return a equal-weight vw line with all features present\"\"\"\n    # TODO: implement multi-class: needs per-class internal data-structs\n    d1, d2 = all_features_dicts()\n    individual_features = ' | ' + ' '.join(d1.keys())\n    ns_features = []\n    for ns in d2:\n        fnames = d2[ns].keys()\n        one_ns_features = \" |%s %s\" % (ns, ' '.join(fnames))\n        ns_features.append(one_ns_features)\n\n    example = '1' + individual_features + ' '.join(ns_features) + '\\n'\n    d(\"all_features_example: %s\" % example)\n    return example\n\n\ndef process_audit_line(line):\n    \"\"\"\n    Process an audit line coming from 'vw'\n    Track min/max/hash-value/weight for each feature\n    \"\"\"\n    features = line.split(\"\\t\")\n    features.pop(0)\n    for f in features:\n        fields = f.split(':')\n        fname = fields[0]\n\n        if fname == '':\n            # don't process 'empty' features\n            continue\n\n        fhash = int(fields[1])\n        fval = float(fields[2])\n        fweight = float(fields[-1].split('@')[0])\n\n        F_WEIGHT[fname] = fweight\n        F_HASH[fname] = fhash\n\n        if fname not in F_MIN:\n            # feature seen for 1st time\n            F_MIN[fname] = fval\n            F_MAX[fname] = fval\n\n        if fval < F_MIN[fname]:\n            F_MIN[fname] = fval\n        if fval > F_MAX[fname]:\n            F_MAX[fname] = fval\n\n\ndef vw_audit(vw_cmd, our_input=None):\n    \"\"\"\n    Generator for vw audit-lines\n    (Each example is mapped to its audit-line)\n\n    vw_cmd is a list of args to run vw with (vw command line)\n\n    There are two modes of running:\n        1) Normal: input provided directly to vw from command line\n        2) 2nd pass: input provided by vw-varinfo as a string\n           This mode is activated when our_input=\"some string...\"\n    \"\"\"\n    if our_input:\n        # Input comes from our_input (string)\n        # which is sent via stdin to the vw subprocess\n        vw_proc = subprocess.Popen(\n            vw_cmd,\n            stdout=subprocess.PIPE,\n            stderr=subprocess.STDOUT,\n            stdin=subprocess.PIPE,\n            bufsize=1048576\n        )\n        # python3 expects a bytes-like object\n        # Hence encoding the string our_input\n        vw_proc.stdin.write(our_input.encode())\n        vw_proc.stdin.close()\n    else:\n        # By default, vw reads from a training-set\n        # which is provided on the command line\n        vw_proc = subprocess.Popen(\n            vw_cmd,\n            stdout=subprocess.PIPE,\n            stderr=subprocess.STDOUT,\n            close_fds=False,\n            bufsize=1048576\n        )\n\n    example_no = 0\n\n    while True:\n        # Since encoded string was written\n        # therefore it needs to be decoded\n        vw_line = vw_proc.stdout.readline().decode()\n        if not vw_line:\n            # End of input\n            vw_proc.stdout.close()\n            vw_proc.wait()\n            if vw_proc.returncode:\n                # non-zero exit code, print the full command that\n                # failed to help user reproduce/understand it\n                fatal(\"vw subprocess failed (status=%s):\\n\\t%s\\n\"\n                      \"(Run the command above to reproduce the failure)\" %\n                      (vw_proc.returncode, ' '.join(vw_cmd)))\n            else:\n                # everything looks cool, support debugging anyway\n                d(\"%s: %s examples, exit status: %s\" %\n                  (vw_cmd, example_no, vw_proc.returncode))\n\n            return\n\n        # An audit line is recognized by a leading-tab\n        if vw_line[0] == '\\t':\n            # An audit line (what we're looking for)\n            example_no += 1\n            d(vw_line)\n            yield vw_line\n            continue\n\n        # print(\"vw_line=[%s] len=%d not(vw_line)=%s\" % (vw_line, len(vw_line), (not vw_line)))\n        # time.sleep(0.0001)\n\n        # Q: anything we want to do with other lines?\n        # A: for now no, we just read the next line from vw\n\n    return\n\ndef run_vw(vw_cmd, our_input=None):\n    \"\"\"Track all variables and their weights via vw --audit lines\"\"\"\n    for line in vw_audit(vw_cmd, our_input):\n        process_audit_line(line)\n\n\ndef is_vw_arg(arg):\n    \"\"\"\n    Return True iff the arg looks like a 'vw' argument\n    Side effect: modifies the VW global variable iff user uses\n    a different vw\n    \"\"\"\n    global VW\n    if arg == VW:\n        return True\n    if re.search(r'(?:^|[\\\\/])vw[-_.0-9]*(\\.exe)?$', arg):\n        VW = arg\n        return True\n    return False\n\n\ndef already_has_audit(args):\n    \"\"\"Return True iff args already include --audit (or -a)\"\"\"\n    if '-a' in args or '--audit' in args:\n        return True\n    return False\n\n\ndef is_multiclass(args):\n    \"\"\"\n    Check args for any hint of a multiclass problem\n    (Check is option dependent and may be incomplete)\n    \"\"\"\n    # Not sure if --wap, --ect multi-class are actually right\n    for mc_opt in ('--oaa', '--csoaa', '--ect', '--wap', '--sequence'):\n        if mc_opt in args:\n            return True\n    return False\n\n\ndef model_arg(args):\n    \"\"\"Return the model arg if any\"\"\"\n    f_idx = None\n    try:\n        f_idx = args.index('-f')\n    except:\n        # not there\n        return None\n\n    try:\n        f_idx += 1\n        model = args[f_idx]\n    except:\n        fatal(\"Oops! -f withot an arg - can't continue\")\n\n    return model\n\n\ndef get_vw_cmd(args):\n    \"\"\"\n    Return the vw command we want to run\n    This means stripping our own (vw-varinfo) name from the list\n    and making sure:\n        1) That we have 'vw' at the beginning\n        2) That -a is added for auditing\n    \"\"\"\n    global ModelName, CleanupModel, Verbose\n\n    if len(args) <= 1:\n        usage('')\n\n    # -- move ourselves (vw-varinfo arg) out of the way\n    args.pop(0)\n\n    # 1st arg can be '-v' for debugging this script\n    if len(args) > 0 and args[0] == '-v':\n        Verbose = True\n        args.pop(0)\n\n    vw_args = []\n\n    if len(args) < 1:\n        usage('Too few args: %s' % args)\n\n    if not is_vw_arg(args[0]):\n        if os.name == 'nt':\n            args.insert(0, 'vw.exe')\n        else:\n            args.insert(0, 'vw')\n\n    if not already_has_audit(args):\n        args.insert(1, '--audit')\n\n    if '--noconstant' in args:\n        VWARGS.append('--noconstant')\n\n    model = model_arg(args)\n    if model:\n        ModelName = model\n    else:\n        ModelName = tempfile.mktemp(suffix='.vwmodel')\n        args.insert(1, ModelName)\n        args.insert(1, '-f')\n        CleanupModel = True\n\n    # TODO: skip leading options that are intended for vw-varinfo itself\n    for arg in args:\n        vw_args.append(arg)\n\n    d(\"vw_cmd is: %s\" % vw_args)\n    vw_exe = vw_args[0]\n    if which(vw_exe) is None:\n        fatal(\"Sorry: can't find %s (vowpal wabbit executable) in $PATH\\n\"\n              \"PATH=%s\" % (vw_exe, os.environ[\"PATH\"]))\n\n    return vw_args\n\n\ndef minmax(data):\n    \"\"\"\n    Return a pair (min, max) of list arg\n    \"\"\"\n    lo = 0\n    hi = 0\n    for i in data:\n        if i > hi:\n            hi = i\n        if i < lo:\n            lo = i\n\n    return lo, hi\n\n\ndef summarize():\n    \"\"\"Output summary of variables\"\"\"\n    wmin, wmax = minmax(F_WEIGHT.values())\n    w_absmax = max(abs(wmin), abs(wmax))\n\n    # Print a header\n    print((\"%-16s\\t%10s\\t%s\\t%s\\t%s\\t%s\" %\n          ('FeatureName', 'HashVal', 'MinVal', 'MaxVal', 'Weight', 'RelScore')))\n\n    # TODO: implement multi-class\n    # multi-class needs per-class internal data-structs\n\n    # To reverse-order add: 'reverse=True' arg to 'sorted'\n    # itemgetter: (0): sort-by-key, (1): sort by value\n    sorted_tuples = sorted(F_WEIGHT.items(), key=itemgetter(1))\n    for fname, _ in sorted_tuples:\n        fmin = float(F_MIN[fname])\n        fmax = float(F_MAX[fname])\n        fweight = float(F_WEIGHT[fname])\n        fhash = F_HASH[fname]\n        relscore = 100.0 * (fweight/w_absmax if w_absmax > 0 else 0.0)\n        print((\"%-16s\\t%10s\\t%.2f\\t%.2f\\t%.2f\\t%7.2f\" %\n              (fname, fhash, fmin, fmax, fweight, relscore)))\n\ndef pass1(vw_cmd):\n    \"\"\"In pass1 we run 'vw' as in the original command\"\"\"\n    d(\"Starting PASS 1 ...\")\n    run_vw(vw_cmd, None)\n\ndef pass2():\n    \"\"\"\n    Run a 2nd pass with all features and stored model\n    To get the final weights for all features\n    \"\"\"\n    vw_cmd = [VW, '--quiet', '-t', '-a', '-i', ModelName]\n    if len(VWARGS) > 0:\n        vw_cmd += VWARGS\n    all_features = all_features_example()\n    d(\"Starting PASS 2 ...\")\n    run_vw(vw_cmd, all_features)\n\n\n#\n# -- main\n#\ndef main():\n    \"\"\"Main func for vw-varinfo2: dataset feature information summary\"\"\"\n\n    global MultiClass, MCLabels\n\n    try:\n        vw_cmd = get_vw_cmd(sys.argv)\n\n        if is_multiclass(vw_cmd):\n            # multi-class needs per-class internal data-structs\n            MultiClass = True\n            MCLabels = []\n\n        # Run 1st pass to collect data on all features:\n        pass1(vw_cmd)\n\n        # Run second pass:\n        #   with -i ModelName and single example w/ all-features present\n        pass2()\n\n        summarize()\n\n        if CleanupModel:\n            d(\"removing tempfile: %s\" % ModelName)\n            os.remove(ModelName)\n\n    except Exception as estr:\n        # catch-all to cover all unhandled exceptions\n        fatal(\"%s\\n%s\" % (estr, traceback.format_exc()))\n\n    return 0\n\nif __name__ == \"__main__\":\n    sys.exit(main())\n\n"
        },
        {
          "name": "weight.2015.csv",
          "type": "blob",
          "size": 0.509765625,
          "content": "Date,Pounds\n2015-02-10,194.0\n2015-10-10,187.0\n2016-01-10,184.0\n2016-04-10,182.0\n2016-05-10,180.0\n2016-05-20,178.0\n2016-05-22,178.6\n2016-05-24,177.8\n2016-05-26,180.0\n2016-05-29,180.0\n2016-05-30,179.0\n2016-05-31,177.6\n2016-06-02,178.4\n2016-06-02,177.6\n2016-06-10,178.0\n2016-06-16,177.4\n2016-06-21,176.4\n2016-06-22,177.0\n2016-06-23,176.6\n2016-06-25,176.0\n2016-06-27,175.6\n2016-06-28,175.4\n2016-06-30,175.8\n2016-07-01,175.8\n2016-07-02,175.4\n2016-07-03,175.6\n2016-07-04,174.6\n2016-07-07,175.2\n2016-07-08,174.6\n2016-07-09,174.0\n"
        },
        {
          "name": "weight.2015.png",
          "type": "blob",
          "size": 75.4033203125,
          "content": null
        }
      ]
    }
  ]
}